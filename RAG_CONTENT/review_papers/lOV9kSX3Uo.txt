Optimizing over Multiple Distributions under
Generalized Quasar-Convexity Condition
Shihong Ding1
dingshihong@stu.pku.edu.cnLong Yang1
YANGLONG001@pku.edu.cn
Luo Luo2,4
luoluo@fudan.edu.cnCong Fang1,3†
fangcong@pku.edu.cn
1State Key Lab of General AI, School of Intelligence Science and Technology, Peking University
2School of Data Science, Fudan University
3Institute for Artificial Intelligence, Peking University
4Shanghai Key Laboratory for Contemporary Applied Mathematics
Abstract
We study a typical optimization model where the optimization variable is com-
posed of multiple probability distributions. Though the model appears frequently in
practice, such as for policy problems, it lacks specific analysis in the general setting.
For this optimization problem, we propose a new structural condition/landscape
description named generalized quasar-convexity (GQC) beyond the realms of con-
vexity. In contrast to original quasar-convexity [ 24], GQC allows an individual
quasar-convex parameter γifor each variable block iand the smaller of γiimplies
less block-convexity. To minimize the objective function, we consider a generalized
oracle termed as the internal function that includes the standard gradient oracle as
a special case. We provide optimistic mirror descent (OMD) for multiple distri-
butions and prove that the algorithm can achieve an adaptive ˜O((Pd
i=11/γi)ε−1)
iteration complexity to find an ε-suboptimal global solution without pre-known
the exact values of γiwhen the objective admits “polynomial-like” structural.
Notably, it achieves iteration complexity that does not explicitly depend on the
number of distributions and strictly faster (Pd
i=11/γiv.s.dmax i∈[1:d]1/γi)than
mirror decent methods. We also extend GQC to the minimax optimization problem
proposing the generalized quasar-convexity-concavity (GQCC) condition and a
decentralized variant of OMD with regularization. Finally, we show the appli-
cations of our algorithmic framework on discounted Markov Decision Processes
problem and Markov games, which bring new insights on the landscape analysis of
reinforcement learning.
1 Introduction
We study a common class of generic minimization problem
min
x∈Xf(x), (1)
where the optimization variable xis composed of dprobability distributions {xi}d
i=1andXdenotes
the product space of the dprobability simplexes. Problem (1)meets widespread applications in
†Corresponding author.
38th Conference on Neural Information Processing Systems (NeurIPS 2024).reinforcement learning optimization [ 62,2,35], multi-class classification [ 53] and model selection
type aggregation [ 29]. In this paper, we are particularly interested in the case where dis reasonably
large and we manage to obtain complexities dependent of dnon-explicitly.
When fis convex with respect to x, many efficient algorithms can be powerful tools for solving
Problem (1). One well-known algorithm is mirror descent (MD) [ 5] which is based on Bregman
divergence. The wide choices of Bregman divergence enable the algorithm to iterate and converge
under specifically constrained region [ 34]. In particular, if one applies the usual Euclidean distance,
the algorithm reduces to project gradient descent [ 37]. One common and more sophisticated selection
is the Kullback-Leibler (KL) divergence, the algorithm thereby becoming the variant of multiplicative
weights update (MWU) [41] over probability distribution.
Turning to the non-convex world, specific analysis for Problem (1)is rare. In general, finding an
approximate global solution suffers from the curse of dimensionality [ 51,46]. And one interesting
direction is to consider suitable relaxations for the desired solutions, such as an approximate local
stationary point of smooth functions [ 31,19]. However, for many cases, local solutions may not
be sufficient. Moreover, the algorithms often converge much faster in practice than the theoretic
lower bounds in non-convex optimization suggest. This observed discrepancy can be attributed
to the fairly weak assumptions underpinning these generic bounds. For example, many generic
non-convex optimization theories, e.g. Carmon et al. [7,8]only focus on the consideration of
Lipschitz continuity of the gradient and some higher-order derivatives. In practice, the objective is
often more “structured”. For example, the recent progress in neural networks shows that systems of
neural networks approximate convex kernel systems when the model is overparameterized [ 28]. As
pointed out by Hinder et al. [24], much more research is needed to characterize structured sets of
functions for which minimizers can be efficiently found; It was also noted by Yurii Nesterov [ 47] that
lots of functions are essentially convex; Our work follows this research line.
We propose generalized quasar-convexity (GQC) for the class of “structure”. The original quasar-
convex functions [ 22] is parameterized by a constant γ∈(0,1]and requires f(x)−f(x∗)≤
1
γ⟨∇f(x),x−x∗⟩. These functions are unimodal on all lines that pass through a global minimizer
and so all critical points are minimizers. We extend quasar-convexity by introducing individual
quasar-convex parameter γifor each distribution xi. Therefore GQC is parameterized by dconstants
{γi}d
i=1and implies quasar-convexity in the case d= 1. The main intuition of the generalization is
the observation that d/mini∈[1:d]γioften depends on the number of distributions din real problems,
whereas,Pd
i=11/γimay not. That is to say, the hardness for distribution idiverges according to the
magnitude of γi. The larger of γiimplies more convexity and the simpler to solve xi. In general,
one always havePd
i=11/γi≤dmax i∈[1:d]1/γi. In the worst case,Pd
i=11/γican be dtimes
smaller than dmax i∈[1:d]1/γi(see discussions in Section 3.3), which motivates us to study the GQC
condition.
We then study designing efficient algorithms to solve (1). One simple case is when {γi}m
i=1is
pre-known by the algorithms. The possible direction is to impose a γi-dependent update rule, such as
by non-uniform sampling. However, in general cases, {γi}m
i=1is not known and determining {γi}m
i=1
require non-negligible costs.
In this paper, we consider a generalized oracle, which we refer to as the internal function. Here
the standard gradient oracle can be viewed as a special case of the internal function. We pro-
vide the optimistic mirror descent algorithm for multiple distributions, which makes sure that
each probability distribution is updated according to its own internal function. We first establish
anO((dγmax)1/2(Pd
i=1γ−1
i)3/2Lε−1log(N))complexity with N= max i∈[1:d]niandγmax=
max i∈[1:d]γiwhen γmax<∞. However, such an complexity depends on dγmaxand requires the step
size rely on pre-known γmaxPd
i=1γ−1
i. We then consider fsatisfies “polynomial-like” structural (see
Assumption 3.3). We show the assumption can be achieved in a variety of function classes and impor-
tant machine learning problems. Under the assumption, we show the algorithm can adapt to the values
of{γi}m
i=1and guarantees an reduced iteration complexity O((Pd
i=11/γi)ε−1log(N) log4.5(ε−1)).
In the following, the eO(·)notation hides factors that are polynomial in log(ε−1)andlog(N).
We also extend our framework to the minimax optimization
min
x∈Xmax
y∈Yf(x,y), (2)
2Solution
typeRelated
workIteration
complexitySingle
loop
ε-approximate NECen et al. [9]
Chen et al. [12]eO
1
(1−θ)2ε
✗
Wei et al. [67] eO
|S|3
(1−θ)8ε2
✓
Cen et al. [10] eO
|S|
(1−θ)4ε
✓
This Work eO
1
(1−θ)2.5ε
✓
Table 1: Comparison of policy optimization methods for finding an ε-approximate NE of infinite
horizon two-player zero-sum Markov games in terms of the max-min gap (see Eq. (4)). Since the
iteration complexity of several research works (such as Zhao et al. [75], Alacaoglu et al. [3]and Zeng
et al. [72]) involve concentrability coefficient and initial distribution mismatch coefficient, we will
not delve into them here.
where both xandyare composed of dprobability distributions, and Z=X × Y is a joint region. In
the general non-convex and non-concave setting, it is known that finding even an approximated local
solution for (2)is computationally intractable [ 16]. We introduce the generalized quasar-convexity-
concavity (GQCC) condition analogous to GQC and demonstrate the feasibility of obtaining an
ε-approximate Nash equilibrium with O((1−θ)−2.5maxz∈Z(Pd
i=1ψi(z))ε−1log(M) log( ε−1))
iteration complexities, where maxz∈Z(Pd
i=1ψi(z))is analogous to (Pd
i=11/γi)withψi(z)defined
in the GQCC condition; θis the discount parameter; M= max i∈[1:d]{mi+ni}. Intuitively, the
GQCC condition can be viewed as the generalization of convexity-concavity condition. Similarly, the
eO(·)notation hides factors that are polynomial in log(ε−1)andlog(M).
Finally, we demonstrate the applications of our framework. For problem (1), we consider both infinite
horizon discounted and finite horizon MDPs problem. For problem (2), we study the infinite horizon
two-player zero-sum Markov games. We prove the learning objectives admit the GQC and GQCC
conditions, respectively. This provides new landscape description for RL problems, thereby bringing
new insights. Accordingly, our algorithms achieve state-of-the-art iteration complexities up to loga-
rithmic factors. We provide eO(ε−1)iteration bound for finding an ε-approximate Nash equilibrium
of infinite horizon two-player zero-sum Markov games, which outperforms the eO(|S|3ε−2)bound of
Wei et al. [67] and the eO(|S|ε−1)bound of Cen et al. [10] by factors of |S|3ε−1and|S|, respectively,
up to a logarithmic factor.
1.1 Contribution
(A)We introduce new structural conditions GQC for minimization problems and GQCC for
minimax problems over multiple distributions.
(B)We provide adaptive algorithm that achieves eO((Pd
i=11/γi)ε−1)iteration complexities to
find an ε-suboptimal global minimum of “polynomial-like” function under GQC. We also
provide an implementable minimax algorithm, given a generalized quasar-convex-concave
function with proper conditions, uses eO((1−θ)−2.5maxz∈Z(Pd
i=1ψi(z))ε−1)iterations
to find an ε-approximate Nash equilibrium.
(C)We show that discounted MDP and infinite horizon two-player zero-sum Markov games
admit the GQC and GQCC conditions, respectively, and also satisfy our mild assumptions. In
addition, we provide eO((1−θ)−2.5ε−1)iteration bound for finding an ε-approximate Nash
equilibrium of infinite horizon two-player zero-sum Markov games. Detailed comparisons
between our method and prior arts are provided in Table 1.
1.2 Related Works
Minimization: Convexity condition has been studied at length and plays a critical role in optimizing
minimization problems [ 59,44,25,60,6,49]. Several other “convexity-like” conditions have
3attracted considerable attention, which provide opportunity for designing algorithmic framework to
achieve global convergence. Star-convexity [ 47] is a typical example that relaxes convexity, showing
potential in machine learning recently [ 32,76]. Quasi-convexity, which admits that the highest
point along any line segment is one of the endpoints, is also an important condition [ 6]. Following
this, the concept of weak quasi-convexity is proposed by Hardt et al. [22] which is an extension of
star-convexity in the differentiable case, and Hinder et al. [24] provides lower bound for the number
of gradient evaluations to find an ε-minimizer of a quasar-convex function (a linguistically clearer
redefinition of weak quasi-convex function claimed by Hinder et al. [24] ).
Minimax Optimization: Minimax problem attracted considerable attention in machine learning.
There exist a variety of algorithms to find the approximate Nash equilibrium points [ 63,43,48,
45,40,33,55,66,27] or stationary points [ 71] for convex-concave functions. Without convex-
concave assumption, there exist related work considered specific structures in objective, including
nonconvex-(strongly-)concave assumption [ 39,73,50], Kurdyka–Lojasiewicz condition (or specific
PL condition) [ 68,11,69,38], interaction dominant condition [ 21] and negative comonotonicity
[17, 36].
RL Landscape Descriptions: For the policy gradient based model of infinite horizon reinforcement
learning problems, Agarwal et al. [2]provides a convergence proof for the natural policy gradient
descent, which is the same as the mirror descent-modified policy iteration algorithm [ 20] with
negative entropy as the Bregman divergence. Subsequently, Lan [35] focuses on exploring the
structural properties of infinite horizon reinforcement learning problems with convex regularizers.
For two-player zero-sum Markov games [ 61,42] under full information setting, there are various
algorithms [ 26,54,64,18,42,67,9,74,70] have been proposed. Specifically, Cen et al. [9]focus on
finding approximate minimax soft Q-function in regularized infinite horizon setting; Zhao et al. [74]
focus on finding one-sided approximate Nash equilibrium in standard infinite horizon setting with
˜O(ε−1)iteration bound which depends on the concentrability coefficient; Yang and Ma [70] focus on
finding approximate Nash equilibrium in standard finite horizon setting with ˜O(ε−1)iteration bound.
Related Works on Optimistic Mirror Descent (OMD) and Optimistic Multiplicative Weights
Update (OMWU): The connection between online learning and game theory [ 58,4,23,1] has
since led to the discovery of broad learning algorithms such as multiplicative weights update (MWU)
[41]. Rakhlin and Sridharan [57] introduces an optimistic variant of online mirror descent [ 56,14]–
optimistic mirror descent. Daskalakis et al. [15] shows that the external regret of each player achieves
near-optimal growth in multi-player general-sum games, with all players employ the optimistic
multiplicative weights update.
2 Preliminary
Notation: Letx= (x1,···,xd)∈RPd
i=1nibe the joint vector variable, for every vector
variable xi∈Rni. Letα= (α(1),···,α(n))be the multi-indices, where α(i)∈Z+, we define
|α|=Pn
i=1α(i)andα! =α(1)!···α(n)!. For any vector u= (u(1),···,u(n))∈Rn, we
define uα=u(1)α(1)···u(n)α(n). Let f:Rn→Rbe a smooth function, we expand its Taylor
expansion with Lagrange remainder Rf
K,w(u)as follows,
Rf
K,w(u) =f(u)−KX
i=0X
|α|=iDαf(w)
α!·(u−w)α. (3)
Given matrices QandPinRℓ1×ℓ2we claim that Q≤Pif[Q]i,j−[P]i,j≤0for every i, j.
For a sequence of vector-valued functions {Fi}d
i=1, we say that {Fi}d
i=1is uniformly L-Lipschitz
continuous with respect to ∥·∥′under∥·∥if∥Fi(xi)−Fi(ui)∥′≤L∥xi−ui∥for every i∈[1 :d]
and any x,u∈ X. We denote by ∥·∥∗the dual norm of ∥·∥. LetP:Rℓ1×ℓ2→Rn1×n2be a matrix
function, we say that Pis aθ-contraction mapping under ∥·∥if∥P(Q1)−P(Q2)∥∞≤θ∥Q1−Q2∥
for any Q1,Q2∈Rℓ1×ℓ2. For matrix-valued function P:Rn→Rℓ1×ℓ2,we define DP(x,x′) =
P(x)−P(x′)for any x,x′∈Rn. The KL divergence KL(p∥q) =Pn
j=1p(j)·log
p(j)
q(j)
between distributions pandqis defined on probability simplex ∆n. And the variance of xoverp
is defined by Varp(x) =Pn
j=1p(j)·(x(j)−Ej′∼p[x(j′)])2. We define max-min gap of function
4f:X × Y → Ras follows,
Gf(x,y) := max
y′∈Yf(x,y′)−min
x′∈Xf(x′,y). (4)
We claim that (x,y)is an ε-approximate Nash equilibrium ( ε-approximate NE) if Gf(x,y)≤ε.
When ε= 0,(x,y)is a Nash equilibrium.
Infinite Horizon Discounted Markov Decision Process: We consider the setting of an infinite
horizon discounted Markov decision process (MDP), denoted by M:= (S,A,P, σ, θ,ρ0).Sis
a finite state space; Ais a finite action space; P(s|s′, a′)denotes the probability of transitioning
from stos′under playing action a′;σ:S × A → [0,1]is a cost function, which quantifies the
cost associated with taking action ain state s;θ∈[0,1)is a discount factor; ρ0is an initial state
distribution over S.
π:S → ∆A(where ∆Ais the probability simplex over A) denotes a stochastic policy, i.e., the
agent play actions according to a∼π(·|s). We use Prπ
t(s′|s) =Prπ(st=s′|s0=s)to denote
the probability of visiting the state s′from the state safterttime steps according to policy π. Let
trajectory τ={(st, at)}∞
t=0, where s0∼ρ0, and, for all subsequent time steps t,at∼π(·|st)and
st+1∼P(·|st, at). The value function Vπ:S →Ris defined as the discounted sum of future cost
starting at state sand executing π, i.e.
Vπ(s) = (1 −θ)E"∞X
t=0θtσ(st, at)π, s0=s#
.
Moreover, we define the action-value function Qπ:S × A → Rand the advantage function
Aπ:S × A → Ras follows:
Qπ(s, a) = (1 −θ)E"∞X
t=0θtσ(st, at)π, s0=s, a0=a#
, Aπ(s, a) =Qπ(s, a)−Vπ(s).
It’s also useful to define the discounted state visitation distribution dπ
s0of a policy πasdπ
s0(s) =
(1−θ)P∞
t=0θtPrπ
t(s|s0). In order to simplify notation, we write dπ
ρ0(s) =Es0∼ρ0[dπ
s0(s)], where
dπ
ρ0is the discounted state visitation distribution under initial distribution ρ0.
3 Minimization Optimization
In this section, we propose the generalized quasar-convexity (GQC) condition, and analyze a related
algorithmic framework for minimization over X=Qd
i=1∆ni, under mild assumptions.
3.1 Generalized Quasar-Convexity (GQC)
We provide a novel depiction of function structure–generalized quasar-convexity, which is defined as
follows:
Definition 3.1 (Generalized Quasar-Convexity (GQC)) .Letx∗∈ X ⊂ RPd
i=1nibe a minimizer of
the function f:X →R. We say that fis generalized quasar-convex on Xwith respect to x∗if for
allx∈ X, there exist a sequence of vector-valued functions {Fi:X →Rni}d
i=1and a sequence of
positive scalars {γi}d
i=1such that
f(x∗)≥f(x) +dX
i=11
γi⟨Fi(x),x∗
i−xi⟩. (5)
If Eq. (5)holds, we say that F= (F⊤
1,···,F⊤
d)⊤is the internal function of f. Given i∈[1 :d]we
say that Fiis the internal function of ffor variable block xi.
Our proposed GQC condition concerns the multi-variable generalized extension of the quasar-
convexity condition. In the case d= 1, the GQC condition degenerates into the γ-quasar-convexity
condition as studied in Hinder et al. [24] with the gradient ∇f(x)belongs to the internal functions
off. In the case d >1, the GQC condition is instrumental in capturing the crucial characteristic of
those optimization applications with each variable block has difficulty to be optimized.
5Algorithm 1 Optimistic Mirror Descent for Multi-Distributions
Input:
g0
i=x0
i= (1/ni,···,1/ni)	d
i=1,ηandT.
Output: Randomly pick up t∈ {1,···, T}following the probability P[t] = 1/Tand return xt.
1:while t≤Tdo
2: for all i∈[1 :d]do
3: xt
i= argmin
xi∈∆niη
Fi(xt−1),xi
+ KL 
xi∥gt−1
i
,
4: gt
i= argmin
gi∈∆niη
Fi(xt),gi
+ KL 
gigt−1
i
.
5: end for
6: t←t+ 1.
7:end while
3.2 Main Results
Recall that GQC condition provides a perspective to bound function error f(x)−f(x∗)based on
internal function, which is different from that based on gradient oracle. We therefore aim to provide
an algorithmic framework for finding an approximate suboptimal global solution using internal
function. Given an objective function f:X →Rwith internal function F, our algorithm (Algorithm
1) independently computes points gt
iandxt
ifollowing OMD over each block. If max i∈[1:d]γi<∞
and internal function Fhas Lipschitz continuity, we have following basic and primary convergence
result of Algorithm 1,
Theorem 3.2. Assuming that FisL-Lipschitz continuous with respect to ∥ · ∥∗under ∥ · ∥ and
γmax= max i∈[1:d]γi<∞, and setting η= (L2dγmaxPd
i=1γ−1
i)−1/2/2, we have
1
TTX
t=1(f(xt)−f(x∗))≤2Lmax i∈[1:d]log(ni) (dγmax)1/2Pd
i=1γ−1
i3/2
T. (6)
However, the estimation provided by Theorem 3.2 depends on dγmax. And the step size relying on
γmaxPd
i=1γ−1
i
might be difficult to set when {γi}d
i=1is unknown.
We then hope to propose an alternative analytical method that can adapt to unknown {γi}d
i=1and
obtain complexity which does not depends on block dimension dexplicitly. The challenges includes:
1) The algorithm does not know the weight 1/γi; 2) every Fihas dependence on the joint variable
xinstead of depending on xi. Before we present the details of convergence analysis, we need the
following notations and assumptions:
Denote Pf
K,y(x)) =PK
i=0P
|α|=i|Dαf(y)|
α!·(|x|+|y|)αand let Pϕ
K,y(x) = ( Pϕ(1)
K,y(x),···,
Pϕ(ℓ)
K,y(x))for any vector-valued function ϕ:Rn→Rℓ. Recalling the definition of Rf
K,win Eq. (3),
we shall also define Rϕ
K,y(x) = (Rϕ(1)
K,y(x),···, Rϕ(ℓ)
K,y(x)).
Assumption 3.3. LetFbe the internal function of f. There exists Θ1,Θ2>0,K0∈Z+, and
θ∈[0,1), and a fixed y∈RPd
i=1nisuch that
[A1]RF
K,y(x)
∞≤Θ1θKfor any integer K > K 0andx∈ X.
[A2]PF
K,y(x)
∞≤Θ2for any integer K∈Z+andx∈ X.
Assumption 3.3 is a characterization of “polynomial-like” functions. We clarify this view as follows.
For a standard polynomial function p, it’s clear that psatisfies Assumption 3.3, since the Taylor
expansion of pafter order K0is always equal to 0 ( [A1]in Assumption 3.3 holds) and Xis a
bounded and closed set ( [A2]in Assumption 3.3 holds). Assumption 3.3 is easy to achieve. Shown in
Proposition B.2 and Remark B.3 in Appendix B, Assumption 3.3 can be satisfied by many smooth
functions defined on bounded region X. In addition, we introduce a simple machine learning example:
learning one single neuron network over a simplex in the realizable setting.
6Example 3.4.The objective function is written as f(p,P) =1
2Ex,y(Pm
i=1piσ(x⊤Pi)−y)2, where
p∈∆mandP= (P1,···,Pm)∈Qm
i=1∆dand the target ygiven x∈[−C, C]dadmits y=
σ(x⊤P∗
1)for some P∗
1∈∆d. For activation function σ(x) = exp {x},fsatisfies GQC condition
and Assumption 3.3 with the internal functions Fp={E[(Pm
j=1pjσ(x⊤Pj)−y)σ(x⊤Pi)]}m
i=1
for block pandFPi=E[(σ(x⊤Pi)−y)x]for block Pi.
Note previous work [ 65] studies single neuron learning by considering P∗
1in the sphere and assuming
xfollows from a Gaussian distribution. To our knowledge, there is no evidence shows that objective
function of Example 3.4 has quasar-convexity. This example demonstrates the advantage of studying
the GQC framework over the previous approach. The proof of Example 3.4 is in Section B.2.
Parameter Setting Before stating the convergence result, we set the parameters as follows:
Θ = Θ 1+ Θ 2+ 1, H =⌈log(T)⌉, β 0= (4H)−1, β = min(p
β0/8
H3,1
2Θ(H+ 3))
,
Γ =e2+O(Θ2),ˆK= maxHlog(4β−1) + log(Θ 1)
log(θ−1), K0
, η = min(
β
6e3ˆKΓ max{Θ,1},β4
0
O(Θ))
.
(7)
Theorem 3.5. Letfsatisfies the GQC condition and denote N= max i∈[1:d]{ni}. Under Assumption
3.3, the following estimation holds for Algorithm 1’s output {xt}T
t=1
1
TTX
t=1(f(xt)−f(x∗))≤ dX
i=11/γi!1
ηlog(N) +ηΘ3(6 + 330240Θ H5)
T−1, (8)
Theorem 3.5 implies that for any generalized quasar-convex function fsatisfies Assumption 3.3,
theT-step random solution outputted by Algorithm 1 is a O((Pd
i=11/γi)T−1log(N) log4.5(T))-
suboptimal solution. Ignoring the logarithmic factor, the iteration complexity of our algorithm
is competitive to the state-of-the-art algorithm when applied to specific application (i.e. policy
optimization of reinforcement learning [ 2]). Moreover, our algorithm makes iteration complexity
depend onPd
i=11/γilinearly. In some common applications,Pd
i=11/γihas no dependence on d,
which is the number of variable blocks (see discussions in Section 3.3).
3.3 Application to Reinforcement Learning
This section reveals that GQC condition provides a novel analytical approach to reinforcement
learning. We show how to leverage Algorithm 1 to find ε-suboptimal global solution for infinite
horizon reinforcement learning problem. And in Appendix B.3.2, we show how to leverage Algorithm
1 to minimize finite horizon reinforcement learning problem.
The infinite horizon reinforcement learning is formulated as the following policy optimization
problem:
min
π∈XJπ(ρ0), (9)
where Jπ(ρ0) =Es0∼ρ0[Vπ(s0)]andX=Q|S|
i=1∆Adenotes |S|probability simplexes. We write
S={si}|S|
i=1and denote the action-value vector on state sibyQπ(si,·). The next Proposition 3.6
states that Jπ(ρ0)satisfies the GQC condition for any initial state distribution ρ0.
Proposition 3.6. Let{π∗(·|s)∈∆A}s∈Sdenote the optimal global solution of problem (9). We
have that Jπ(ρ0)satisfies the GQC condition in Eq. (5)with internal function Fi(π) =Qπ(si,·)
for variable block πiandFsatisfies Assumption 3.3 with Θ1=θ,Θ2= 1andK0= 1.
According to Theorem 3.5, if we apply Algorithm 1 to the infinite horizon reinforcement learning
basing action-value vector Qπwith parameter selection Eq. (7), which is actually a simple variant of
natural policy gradient descent [ 2], then the iteations Twe need to find an ε-suboptimal global solution
is upper-bounded by O(max{1,log−1(θ−1)}(1−θ)−1ε−1log4.5(ε−1) log(|A|))under Agarwal et al.
[2]’s setting. Therefore, the iteration complexity of Algorithm 1 does not depend on the size of states,
since the summation of dπ∗
ρ0overS(P|S|
i=11/γi=P|S|
i=1dπ∗
ρ0(si) = 1) mollifies the accumulation
7of the maximum of dπ∗
ρ0overSwith|S|times. Specifically, if we take into account the loosest upper
bound |S|max i∈[1:|S|]dπ∗
ρ0(si), then the iteration complexity of algorithm may suffer from the linear
dependence on |S|, since max i∈[1:|S|]dπ∗
ρ0(si)≥(1−θ) max i∈[1:|S|]ρ0(si). Previous research
[2, Theorem 5.3] has demonstrated that utilizing the information of joint variables to separately
update each variable block ensures global convergence for problem (9)withO((1−θ)−2ε−1)
iteration complexity. However, their analytical approach is carefully designed for infinite horizon
reinforcement learning problems.
4 Minimax Optimization
In this section, we introduce the generalized quasar-convexity-concavity (GQCC) condition, which
can be verified in real applications such as two-player zero-sum Markov games. We provide a
related algorithm for minimax optimization (minimizing Gf(x,y)has been defined in Eq. (4))
overZ=Qd
i=1Zi=Qd
i=1(∆ni×∆mi), under proper assumptions. We specify the divergence-
generating function vasv(x) =Ei∼x(·)[log(x(i))]in probability simplexes setting. We also provide
a framework for minimax problem over the general compact convex regions in Appendix C.
4.1 Generalized Quasar-Convexity-Concavity (GQCC)
We provide a new notion called generalized quasar-convexity-concavity for nonconvex-nonconcave
minimax optimization, which is defined as follows:
Definition 4.1 (Generalized Quasar-Convexity-Concavity (GQCC)) .Denote Zi=Xi× Yifor any
i∈[1 :d], and let f:Z →Rbe the objective function. We say that fis generalized quasar-convex-
concave on Zif for all z= (x,y)∈ Z, there exist a sequence of functions {fi:Rℓ×d× Zi→
R}d
i=1, a sequence of non-negative functions {ψi:Z →R+∪0}d
i=1and a matrix-valued function
P= (P1,···,Pd) :Z →Rℓ×dwhere every Piis aℓ-dimensional vector-valued function, such
that
Gf(x,y)≤dX
i=1ψi(z)Gfi(P(z),·,·)(xi,yi), (10)
where each fi(Q,·,·)is convex-concave for a fixed Q= (Q1,···,Qd)∈Rℓ×d. We denote the in-
ternal operator of ffor variable block zibyFiwhere Fi(Q,zi) = ((∇xifi(Q,zi))⊤,(−∇yifi(Q,
zi))⊤)⊤. Moreover, we say that F= (F⊤
1,···,F⊤
d)⊤is the internal operator of f.
The GQCC condition is an extension of the GQC condition in minimax optimization setting. The
specific connection between them can be found in Appendix C. The GQCC condition can be viewed
as an extension of the convexity-concavity condition in multi-variable optimization; it seamlessly
reduces to the convexity-concavity condition with f1(P(z),z) =f(z)andψ1(z)≡1, in the case
d= 1. Assuming every ψiis bounded, fi(P(z),zi)≡fi(0,zi)with Lipschitz continuous gradient
and is convex-concave with respect to zi, then finding the Nash equilibrium point of fis reduced to
finding the Nash equilibrium points of dindependent convex-concave minimax problems. However,
how to find the approximate Nash equilibrium points in more general case has not been well-studied.
Most of existing work for minimax optimization without convex-concave assumption are focused on
finding the approximate stationary points.
4.2 Main Results
For simplicity, we denote by Fx
iandFy
ithe projection of Fiin thexiandyidirections, respectively,
i.e.,F⊤
i= 
(Fx
i)⊤,(Fy
i)⊤
. Given an objective function f:Z →Rwith internal operator F, our
algorithm (Algorithm 2) employs regularized OMD over each distribution independently basing on
Fiand updates matrix Qtto track the behavior of function Piteratively. It’s worth noting that each
iteration of Algorithm 2 provides explicit expressions for xt
iandgt
i(see the proof of Theorem 3.5 in
Appendix B). Consequently, Algorithm 2 essentially operates as a single-loop algorithm.
Assumption 4.2. In Definition 4.1, we assume that matrix-valued function Phas the form of
P(Qz,z)where Qz∈Rℓ×ddepends on z, and Psatisfies the following properties on region
Q∈Rℓ×d∥Q∥∞≤C} × Z for some constant C >0:
8Algorithm 2 Optimistic Mirror Descent with Regularization for Multiple Distributions
Input:
z0
i	d
i=1=
g0
i	d
i=1={(1/ni,···,1/ni),(1/mi,···,1/mi)}d
i=1,{αt≥0}T
t=1withPT
t=1αt= 1,
{γt≥0}T
t=1,{λt≥0}T
t=1,ηandQ0=0.
Output: ¯zT=PT
t=1αtzt.
1:while t≤Tdo
2:Qt= (1−βt−1)Qt−1+βt−1P(Qt−1,zt−1).
3: for all i∈[1 :d]do
4: xt
i= argmin
xi∈Xiη
Fx
i(Qt−1,zt−1
i),xi
+γtKL 
xi(gx
i)t−1
+λtv(xi),
5: yt
i= argmin
yi∈Yiη
Fy
i(Qt−1,zt−1
i),yi
+γtKL 
yi(gy
i)t−1
+λtv(yi),
6: (gx
i)t= argmin
gx
i∈Xiη
Fx
i(Qt,zt
i),gx
i
+γtKL 
gx
i(gx
i)t−1
+λtv(gx
i),
7: (gy
i)t= argmin
gy
i∈Yiη
Fy
i(Qt,zt
i),gy
i
+γtKL 
gy
i(gy
i)t−1
+λtv(gy
i).
8: end for
9: t←t+ 1.
10:end while
[A1]There exist constants L1, L2≥0such that Fi(·,zi)is uniformly L1-Lipschitz continuous
with respect to ∥·∥∞under∥·∥∞, andFi(Q,·)is uniformly L2-Lipschitz continuous with
respect to ∥ · ∥∞under∥ · ∥ 1.
[A2]There are a positive constant γ > 0and a set of non-negative constant matrices
{Bi,Ci}d
i=1satisfyingPd
i=1(Bi+Ci)
∞≤γ, such that DP(Q,·,y)(x,x′)≤Pd
i=1Ci⟨Fx
i(Q,zi),xi−x′
i⟩andDP(Q,x,·)(y,y′)≥Pd
i=1Bi⟨Fy
i(Q,zi),y′
i−yi⟩.
[A3]There exists θ∈[0,1)such that P(·,z)is aθ-contraction mapping under ∥ · ∥∞, and
∥P(Q,z)∥∞≤Cfor any z∈ Z.
We present Lemma 4.3 to demonstrate that there exist Q∗∈Rℓ×d,x∗∈ X andy∗∈ Y satisfy the
saddle point and fixed point conditions of function P, i.e., Eq. (11), under proper assumptions.
Lemma 4.3. Assuming that Assumption 4.2 holds, [P(Q,·,·)]k,jis continuous, convex with respect
tox, concave with respect to yfor any (k, j), and mink,j,imin{[Ci]k,j,[Bi]k,j}
[Ci]k,j+[Bi]k,j≥C′for some C′>0,
then there exist Q∗∈Rℓ×dandz∗∈ Z such that
Q∗=P(Q∗,x∗,y∗),Q∗≤P(Q∗,x,y∗),and Q∗≥P(Q∗,x∗,y). (11)
For Algorithm 2, we let βT,t=βtQT
j=t+1(1−βj)for any T≥tandβT,T=βT, and set parameters
c= 2(1−θ)−1, η≤(1−θ)1/2
16L2((γL1)1/2+ 1), βt=c
c+t, αt=βT,t, γt=αt−1
αt, λt= 1−γt. (12)
Then we have the following convergence result by denoting M= max i∈[1:d]{mi+ni}.
Theorem 4.4. For any generalized quasar-convex-concave function fwhich satisfies Assumption 4.2
withP≡Q∗, where Q∗satisfies Eq. (11). Algorithm 2’s output ¯zT= (¯xT,¯yT)satisfies
Gf(¯xT,¯yT)≤60 max
z∈Z dX
i=1ψi(z)!
(1−θ)−12
ηlog(M) +ηL2
1+L1Yη
T
T−1,
where Yη
T= 8(c+ 1)[4γ
ηlog(M) + 160 γL2+ 2ηγL2
1(1 + 64 C2)](log( c+T) + 1) .
Similar to minimization Algorithm 1, the iteration complexity of minimax Algorithm 2 linearly
depends on the upper bound ofPd
i=1ψioverZ. Generally, the upper bound ofPd
i=1ψionZ
is related to d. In specific problems of multi-variable optimization (such as two-player zero-sum
Markov games), one can uniformly boundPd
i=1ψionZby a constant.
94.3 Application to Infinite Horizon Two-Player Zero-Sum Markov Games
In this section, we show how to leverage Algorithm 2 to achieve accelerated rates for optimizing
infinite horizon two-player zero-sum Markov games. Our algorithm use ˜O(ε−1)iteration bound to
find an ε-approximate Nash equilibrium of infinite horizon two-player zero-sum Markov games.
As similar as the definition of discounted MDP in Preliminary, we utilize M= (S,A,B,P, σ, θ,ρ0)
to define a infinite horizon two-player zero-sum Markov game. The difference here compared to
Section 3.3 is that the cost function σis defined on S × A × B with values in [0,1], and the transition
model P(s|s′, a′, b′)denotes the probability of transitioning into state supon player 1 taking action
a′and player 2 taking action b′in state s′. We can define the value function Vzand action-value
function Qzon the joint distribution z= (x,y)∈ Z=Q|S|
i=1∆A×Q|S|
i=1∆B. The infinite horizon
two-player zero-sum Markov games consider the following policy optimization problem:
min
x∈Xmax
y∈YJx,y(ρ0), (13)
where Jz(ρ0) =Es0∼ρ0[Vz(s0)]. The following proposition indicates that Jzis general quasar
convex-concave, and satisfies Assumption 4.2 and the condition of Theorem 4.4,
Proposition 4.5. For any Q= (Q1,···,Q|S|)with every Qi∈R|A|×|B|, define function
fi(Q,zi) :=x⊤
iQiyifor any i∈[1 :|S|]. There exists a tensor-valued function Psuch that
Jz(ρ0)satisfies GQCC condition with fi(P(z),zi) =fi(Q∗,zi)for any ρ0∈∆S, where Q∗
satisfies the conditions mentioned in Eq. (11). Moreover, Psatisfies Assumption 4.2.
According to Proposition 4.5 and Theorem 4.4, if we apply Algorithm 2 to the infinite horizon
two-player Markov games basing internal operator Fi(Q,z) = (y⊤
iQ⊤
i,−x⊤
iQi)⊤for block zi
with parameter selection Eq. (12), which is actually a variant of optimistic gradient descent/ascent for
Markov games [ 67], then the iterations Twe need to find an ε-approximate Nash equilibrium is upper-
bounded by ˜O((1−θ)−2.5ε−1). To the best of our knowledge, our iteration bound matches state-of-
the-art iteration bound and is a factor of (1−θ)−1.5|S|better than ˜O((1−θ)−4|S|ε−1)bound of Cen
et al. [10]. Since the upper bound ofP|S|
i=1ψiover feasible region Zin infinite horizon two-player
zero-sum Markov games’ setting satisfiesP|S|
i=1ψi(z)≤P|S|
i=1[dx,y∗(x)
ρ0(si) +dx∗(y),y
ρ0(si)]≤2
for any z∈ Z, our algorithm’s iteration bound does not depend on the size of states.
5 Conclusion
In this work, we introduce two function structures: GQC and GQCC and provide related algorithmic
frameworks with convergence result. To complement our result, we also show that discounted MDP
and infinite horizon two-player zero-sum Markov games admit the GQC and GQCC condition,
respectively, and satisfy our mild assumptions.
6 Acknowledgements
C. Fang was supported by National Key R&D Program of China (2022ZD0114902) and the NSF
China (No.62376008). L. Luo was supported by National Natural Science Foundation of China
(No. 62206058), Shanghai Sailing Program (22YF1402900), Shanghai Basic Research Program
(23JC1401000), and the Major Key Project of PCL under Grant PCL2024A06.
References
[1]Jacob D. Abernethy, Peter L. Bartlett, and Elad Hazan. Blackwell approachability and no-regret
learning are equivalent. arXiv preprint arXiv:1011.1936 , 2010.
[2]Alekh Agarwal, Sham M. Kakade, Jason D. Lee, and Gaurav Mahajan. On the theory of
policy gradient methods: Optimality, approximation, and distribution shift. Journal of Machine
Learning Research , 2021.
[3]Ahmet Alacaoglu, Luca Viano, Niao He, and V olkan Cevher. A natural actor-critic framework
for zero-sum Markov games. In International Conference on Machine Learning . PMLR, 2022.
10[4]David Blackwell. An analog of the minimax theorem for vector payoffs. Pacific Journal of
Mathematics , 1956.
[5] Charles E. Blair. Problem complexity and method efficiency in optimization (a. s. nemirovsky
and d. b. yudin). Siam Review , 1985.
[6]Stephen P. Boyd and Lieven Vandenberghe. Convex optimization. Journal of the American
Statistical Association , 2005.
[7]Yair Carmon, John C. Duchi, Oliver Hinder, and Aaron Sidford. Lower bounds for finding
stationary points I. Mathematical Programming , 2017.
[8]Yair Carmon, John C. Duchi, Oliver Hinder, and Aaron Sidford. Lower bounds for finding
stationary points II: first-order methods. Mathematical Programming , 2017.
[9]Shicong Cen, Yuting Wei, and Yuejie Chi. Fast policy extragradient methods for competitive
games with entropy regularization. Advances in Neural Information Processing Systems , 2021.
[10] Shicong Cen, Yuejie Chi, Simon Shaolei Du, and Lin Xiao. Faster last-iterate convergence
of policy optimization in zero-sum markov games. International Conference on Learning
Representations , 2023.
[11] Lesi Chen, Boyuan Yao, and Luo Luo. Faster stochastic algorithms for minimax optimization
under Polyak Lojasiewicz condition. Advances in Neural Information Processing Systems ,
2022.
[12] Ziyi Chen, Shaocong Ma, and Yi Zhou. Sample efficient stochastic policy extragradient
algorithm for zero-sum markov game. 2021.
[13] Ching-An Cheng, Remi Tachet des Combes, Byron Boots, and Geoff Gordon. A reduction
from reinforcement learning to no-regret online learning. Proceedings of the Twenty Third
International Conference on Artificial Intelligence and Statistics , 2020.
[14] Chao-Kai Chiang, Tianbao Yang, Chia-Jung Lee, Mehrdad Mahdavi, Chi-Jen Lu, Rong Jin, and
Shenghuo Zhu. Online optimization with gradual variations. Conference on Learning Theory ,
2012.
[15] Constantinos Daskalakis, Maxwell Fishelson, and Noah Golowich. Near-optimal no-regret
learning in general games. Advances in Neural Information Processing Systems , 2021.
[16] Constantinos Daskalakis, Stratis Skoulakis, and Manolis Zampetakis. The complexity of
constrained min-max optimization. Proceedings of the 53rd Annual ACM SIGACT Symposium
on Theory of Computing , 2021.
[17] Jelena Diakonikolas, Constantinos Daskalakis, and Michael Jordan. Efficient methods for
structured nonconvex-nonconcave min-max optimization. Proceedings of The 24th International
Conference on Artificial Intelligence and Statistics , 2021.
[18] Jerzy A. Filar and Boleslaw Tolwinski. On the algorithm of Pollatschek and Avi-ltzhak. 1991.
[19] Anders Forsgren, Philip E. Gill, and Margaret H. Wright. Interior methods for nonlinear
optimization. SIAM Rev. , 2002.
[20] Matthieu Geist, Bruno Scherrer, and Olivier Pietquin. A theory of regularized markov decision
processes. International Conference on Machine Learning , 2019.
[21] Benjamin Grimmer, Haihao Lu, Pratik Worah, and Vahab Mirrokni. The landscape of the
proximal point method for nonconvex–nonconcave minimax optimization. Mathematical
Programming , 2023.
[22] Moritz Hardt, Tengyu Ma, and Benjamin Recht. Gradient descent learns linear dynamical
systems. The Journal of Machine Learning Research , 2018.
[23] Sergiu Hart and Andreu Mas-Colell. A simple adaptive procedure leading to correlated equilib-
rium. Econometrica , 2000.
11[24] Oliver Hinder, Aaron Sidford, and Nimit Sohoni. Near-optimal methods for minimizing
star-convex functions and beyond. Conference on learning theory , 2020.
[25] Jean-Baptiste Hiriart-Urruty and Claude Lemaréchal. Convex analysis and minimization
algorithms. 1993.
[26] Alan J. Hoffman and Richard M. Karp. On nonterminating stochastic games. Management
Science , 1966.
[27] Feihu Huang, Xidong Wu, and Heng Huang. Efficient mirror descent ascent methods for
nonsmooth minimax problems. Advances in Neural Information Processing Systems , 2021.
[28] Arthur Jacot, Franck Gabriel, and Clément Hongler. Neural tangent kernel: Convergence and
generalization in neural networks. Advances in neural information processing systems , 2018.
[29] Anatoli B. Juditsky, Philippe Rigollet, and A. Tsybakov. Learning by mirror averaging. Annals
of Statistics , 2005.
[30] Sham M. Kakade and John Langford. Approximately optimal approximate reinforcement
learning. International Conference on Machine Learning , 2002.
[31] Alexander Kaplan and Rainer Tichatschke. Proximal point methods and nonconvex optimization.
Journal of Global Optimization , 1998.
[32] Robert D. Kleinberg, Yuanzhi Li, and Yang Yuan. An alternative view: When does SGD escape
local minima? International Conference on Machine Learning , 2018.
[33] GM Korpelevich. Extragradient method for finding saddle points and other problems. Matekon ,
1977.
[34] Guanghui Lan. First-order and Stochastic Optimization Methods for Machine Learning .
Springer Cham, 2020.
[35] Guanghui Lan. Policy mirror descent for reinforcement learning: Linear convergence, new
sampling complexity, and generalized problem classes. Mathematical Programming , 2022.
[36] Sucheol Lee and Donghwan Kim. Fast extra gradient methods for smooth structured nonconvex-
nonconcave minimax problems. Advances in Neural Information Processing Systems , 2021.
[37] E.S. Levitin and Boris Polyak. Constrained minimization methods. USSR Computational
Mathematics and Mathematical Physics , 1966.
[38] Jiajin Li, Linglingzhi Zhu, and Anthony Man-Cho So. Nonsmooth composite nonconvex-
concave minimax optimization. arXiv preprint arXiv:2209.10825 , 2022.
[39] Tianyi Lin, Chi Jin, and Michael I. Jordan. On gradient descent ascent for nonconvex-concave
minimax problems. International Conference on Machine Learning , 2020.
[40] Tianyi Lin, Chi Jin, and Michael I. Jordan. Near-optimal algorithms for minimax optimization.
Proceedings of Thirty Third Conference on Learning Theory , 2020.
[41] Nick Littlestone and Manfred K. Warmuth. The weighted majority algorithm. 30th Annual
Symposium on Foundations of Computer Science , 1989.
[42] Michael L. Littman. Markov games as a framework for multi-agent reinforcement learning.
International Conference on Machine Learning , 1994.
[43] Arkadi Nemirovski. Prox-method with rate of convergence O(1/t)for variational inequali-
ties with lipschitz continuous monotone operators and smooth convex-concave saddle point
problems. SIAM Journal on Optimization , 2004.
[44] Yurii Nesterov. A method of solving a convex programming problem with convergence rate
O 
k−2
. InDoklady Akademii Nauk . Russian Academy of Sciences, 1983.
[45] Yurii Nesterov. Dual extrapolation and its applications to solving variational inequalities and
related problems. Mathematical Programming , 2007.
12[46] Yurii Nesterov. Introductory lectures on convex optimization - a basic course. In Applied
Optimization , 2014.
[47] Yurii Nesterov and Boris T Polyak. Cubic regularization of newton method and its global
performance. Mathematical Programming , 2006.
[48] Yurii Nesterov and Laura Rosa Maria Scrimali. Solving strongly monotone variational and
quasi-variational inequalities. Econometrics eJournal , 2006.
[49] Jorge Nocedal and Stephen J. Wright. Numerical optimization. In Fundamental Statistical
Inference , 2018.
[50] Maher Nouiehed, Maziar Sanjabi, Tianjian Huang, Jason D Lee, and Meisam Razaviyayn.
Solving a class of non-convex min-max games using iterative first order methods. In H. Wallach,
H. Larochelle, A. Beygelzimer, F. d 'Alché-Buc, E. Fox, and R. Garnett, editors, Advances in
Neural Information Processing Systems . Curran Associates, Inc., 2019.
[51] Erich Novak. Deterministic and stochastic error bounds in numerical analysis. 1988.
[52] Yuyuan Ouyang and Yangyang Xu. Lower complexity bounds of first-order methods for
convex-concave bilinear saddle-point problems. Mathematical Programming , 2018.
[53] Sam Patterson and Yee Whye Teh. Stochastic gradient riemannian langevin dynamics on the
probability simplex. In Neural Information Processing Systems , 2013.
[54] Moshe Asher Pollatschek and Benjamin Avi-Itzhak. Algorithms for stochastic games with
geometrical interpretation. Management Science , 1969.
[55] Leonid Denisovich Popov. A modification of the arrow-hurwicz method for search of saddle
points. Mathematical notes of the Academy of Sciences of the USSR , 1980.
[56] Alexander Rakhlin and Karthik Sridharan. Online learning with predictable sequences. arXiv
preprint arXiv:1208.3728 , 2012.
[57] Sasha Rakhlin and Karthik Sridharan. Optimization, learning, and games with predictable
sequences. Advances in Neural Information Processing Systems , 2013.
[58] Julia Jean Robinson. An iterative method of solving a game. Classics in Game Theory , 1951.
[59] R. Tyrrell Rockafellar. Convex analysis: (pms-28). 1970.
[60] R. Tyrrell Rockafellar, Roger J.-B. Wets, and Maria Wets. Variational analysis. In Grundlehren
der mathematischen Wissenschaften , 1998.
[61] Lloyd S. Shapley. Stochastic games*. Proceedings of the National Academy of Sciences , 1953.
[62] Richard S Sutton, David McAllester, Satinder Singh, and Yishay Mansour. Policy gradient meth-
ods for reinforcement learning with function approximation. Advances in neural information
processing systems , 1999.
[63] Paul Tseng. On linear convergence of iterative methods for the variational inequality problem.
Journal of Computational and Applied Mathematics , 1995.
[64] Jan van der Wal. Discounted markov games: Generalized policy iteration method. Journal of
Optimization Theory and Applications , 1978.
[65] Gal Vardi, Gilad Yehudai, and Ohad Shamir. Learning a single neuron with bias using gradient
descent. ArXiv , 2021.
[66] Yuanhao Wang and Jian Li. Improved algorithms for convex-concave minimax optimization.
Advances in Neural Information Processing Systems , 2020.
[67] Chen-Yu Wei, Chung-Wei Lee, Mengxiao Zhang, and Haipeng Luo. Last-iterate convergence of
decentralized optimistic gradient descent/ascent in infinite-horizon competitive markov games.
InAnnual Conference Computational Learning Theory , 2021.
13[68] Junchi Yang, Negar Kiyavash, and Niao He. Global convergence and variance-reduced optimiza-
tion for a class of nonconvex-nonconcave minimax problems. arXiv preprint arXiv:2002.09621 ,
2020.
[69] Junchi Yang, Antonio Orvieto, Aurelien Lucchi, and Niao He. Faster single-loop algorithms
for minimax optimization without strong concavity. In International Conference on Artificial
Intelligence and Statistics , 2022.
[70] Yuepeng Yang and Cong Ma. O(T−1) convergence of optimistic-follow-the-regularized-leader
in two-player zero-sum markov games. arXiv preprint arXiv:2209.12430 , 2022.
[71] TaeHo Yoon and Ernest K Ryu. Accelerated algorithms for smooth convex-concave minimax
problems with O(1/k2)rate on squared gradient norm. International Conference on Machine
Learning , 2021.
[72] Sihan Zeng, Thinh T. Doan, and Justin Romberg. Regularized gradient descent ascent for
two-player zero-sum Markov games. Advances in Neural Information Processing Systems ,
2022.
[73] Jiawei Zhang, Peijun Xiao, Ruoyu Sun, and Zhi-Quan Luo. A single-loop smoothed gradient
descent-ascent algorithm for nonconvex-concave min-max problems. Advances in Neural
Information Processing Systems , 2020.
[74] Yulai Zhao, Yuandong Tian, Jason D. Lee, and Simon Shaolei Du. Provably efficient policy
optimization for two-player zero-sum markov games. In International Conference on Artificial
Intelligence and Statistics , 2021.
[75] Yulai Zhao, Yuandong Tian, Jason Lee, and Simon Du. Provably efficient policy optimization
for two-player zero-sum markov games. In International Conference on Artificial Intelligence
and Statistics . PMLR, 2022.
[76] Yi Zhou, Junjie Yang, Huishuai Zhang, Yingbin Liang, and Vahid Tarokh. SGD converges to
global minimum in deep learning via star-convex path. arXiv preprint arXiv:1901.00451 , 2019.
14Contents
1 Introduction 1
1.1 Contribution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3
1.2 Related Works . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3
2 Preliminary 4
3 Minimization Optimization 5
3.1 Generalized Quasar-Convexity (GQC) . . . . . . . . . . . . . . . . . . . . . . . . 5
3.2 Main Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6
3.3 Application to Reinforcement Learning . . . . . . . . . . . . . . . . . . . . . . . 7
4 Minimax Optimization 8
4.1 Generalized Quasar-Convexity-Concavity (GQCC) . . . . . . . . . . . . . . . . . 8
4.2 Main Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8
4.3 Application to Infinite Horizon Two-Player Zero-Sum Markov Games . . . . . . . 10
5 Conclusion 10
6 Acknowledgements 10
A Preliminary 16
A.1 Supplemental Notation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16
A.2 Finite Differences . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16
A.3 Finite Horizon Markov Decision Process . . . . . . . . . . . . . . . . . . . . . . . 16
B Minimization Optimization 17
B.1 Proof of Theorem 3.5 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19
B.1.1 Part I . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19
B.1.2 Part II . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21
B.1.3 The Last Step . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24
B.2 Simple Example . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24
B.3 Application to Reinforcement Learning . . . . . . . . . . . . . . . . . . . . . . . 25
B.3.1 Analysis of Infinite Horizon Reinforcement Learning . . . . . . . . . . . . 25
B.3.2 Analysis of Finite Horizon Reinforcement Learning . . . . . . . . . . . . . 26
C Minimax Optimization 26
C.1 Preparatory Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26
C.2 Theorem C.7 and Relate Proof . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29
C.2.1 Part I . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30
C.2.2 Part II: Estimation of Approximation Error ∥Qt−Q∗∥. . . . . . . . . . . 32
C.2.3 The Last Step . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35
15C.3 Application to Minimax Problems . . . . . . . . . . . . . . . . . . . . . . . . . . 36
C.3.1 Infinite Horizon Two-Player Zero-Sum Markov Games . . . . . . . . . . . 36
C.3.2 Convex-Concave Minimax Problems . . . . . . . . . . . . . . . . . . . . 37
D Auxiliary Lemma 37
E Limitation 41
A Preliminary
A.1 Supplemental Notation
For simplicity, we denote g(Γ) :=P∞
k=1Γ−k[k7+ (k+ 1) exp {2k}], the chi-squared divergence
between p,qasχ2(p∥q) :=Pn
j=1(p(j)−q(j))2
q(j),Ep(x) :=Pn
j=1p(j)x(j)andVarp(x) :=
Pn
j=1p(j)·(x(j)−Ep(x))2for any p,q∈∆nandx∈Rn. For ζ > 0, n∈Z+, we say
that a sequence of distributions p1,···,pT∈∆nisζ-consecutively close if for each 1≤t < T , it
holds that maxnpt
pt+1,pt+1
pto
≤1 +ζ. For positive scalar θ∈[0,1), non-negative integers t
andT, we define βθ
T,t:=βtQT−1
j=t(1−βj+θβj), and βθ
T,T= 1.
A.2 Finite Differences
Definition A.1 (Finite Differences) .For a sequence of vectors L= (L0,···,LT)where each
Lt∈Rn, and integers h∈Z+, the order- hfinite difference sequence for the sequence Lis denoted
byDhL:= 
(DhL)0,···,(DhL)T−h
recursively with (D0L)t:=Ltfor all t∈[0 :T], and
(DhL)t:= (Dh−1L)t+1−(Dh−1L)t, (14)
for all h≥1andt∈[1 :T−h].
As stated in [15, Remark 4.3], we have
(DhL)t=hX
s=0
h
s
(−1)h−sLt+s. (15)
To guarantee the coherence of the analysis’s structure, we introduce the definition of the shift operator
Esas follows:
Definition A.2 (Shift Operator) .For a sequence of vectors L= (L0,···,LT)where each Lt∈Rn,
and integers s∈Z+, thes-shift sequence for the sequence Lis denoted by EsL:= 
(EsL)0,···,
(EsL)T−h
with(EsL)t=Lt+sfort∈[1 :T−s].
A.3 Finite Horizon Markov Decision Process
We also consider the following finite horizon Markov decision process (MDP), denoted by M:=
(H,S1:H,A1:H,P2:H, σ,ρ1).H∈Z+denotes the number of horizon; S1:H= (S1,···,SH)is a
sequence of Hfinite state spaces; A1:H= (A1,···,AH)is a sequence of Hfinite action spaces;
Ph(sh|sh−1, ah−1)denotes the probability of transitioning from sh−1toshunder playing action
ah−1at horizon h−1;σ:S1:H× A 1:H→[0,1]is a cost function; ρ1is a initial state distribution
overS1.
π= (π1,···,πH) :S1:H→∆A1× ··· × ∆AHdenotes a stochastic policy. Similarly, we
usePrπ1:h−1
h(s′|s) =Prπ1:h−1
h(sh=s′|s1=s)to denote the probability of visiting the state s′
from the state sat horizon haccording to policy π1:h−1. Let trajectory τ= (sh, ah)H
h=1, where
s1∼ρ1, and, for all subsequent horizon h,ah∼πh(·|sh)andsh+1∼Ph+1(·|sh, ah). The value
function Vπh:H
h:Sh→Ris defined as the sum of future cost starting at state shand executing
16Algorithm 3 Optimistic Mirior Descent for Multi-Variables
Input:
g0
i=x0
i	d
i=1,ηandT.
Output: Randomly pick up t∈ {1,···, T}following the probability P[t] = 1/Tand return xt.
1:while t≤Tdo
2: for all i∈[1 :d]do
3: xt
i= argmin
xi∈Xiη
Fi(xt−1),xi
+V 
xi,gt−1
i
,
4: gt
i= argmin
gi∈Xiη⟨Fi(xt),gi⟩+V 
gi,gt−1
i
.
5: end for
6: t←t+ 1.
7:end while
πh:H= (πh,···,πH), i.e.,
Vπh:H
h(sh) =E"HX
h′=hσ(sh′, ah′)πh:H, sh#
.
For convenience, we define Vπ
1(s1) =Vπ1:H
1(s1). Moreover, we define the action-value function
Qπh+1:H
h:Sh× A h→[0,1 +H−h]as follows:
Qπh+1:H
h(sh, ah) =σ(sh, ah) +E"HX
h′=h+1σ(sh′, ah′)πh+1:H, sh, ah#
.
B Minimization Optimization
We begin with a general version of Theorem 3.2 basing Algorithm 3 in this part.
Theorem B.1. [General Version of Theorem 3.2] We consider the divergence-generating function
vwith Bregman’s divergence V(xi,ui) =v(xi)−v(ui)− ⟨∇ v(ui),xi−ui⟩for any block Xi
and any xi,ui∈ Xi. Assuming that FisL-Lipschitz continuous with respect to ∥ · ∥∗under∥ · ∥,
V(xi,ui)≥ ∥xi−ui∥2for any xi,ui∈ Xiandγmax= max i∈[1:d]γi<∞, we have
1
TTX
t=1(f(xt)−f(x∗))≤2L(dγmax)1/2Pd
i=1γ−1
i3/2
Tmax
i∈[1:d]
max
xi∈XiV(xi,g0
i)
, (16)
with setting η= (L2dγmaxPd
i=1γ−1
i)−1/2/2.
Proof. According to GQC condition (Definition 3.1), we have the following estimation
TX
t=1(f(xt)−f(x∗))≤dX
i=11
γiTX
t=1
Fi(xt),xt
i−x∗
i
. (17)
For any fixed i∈[1 :d], we obtain that
⟨Fi(xt),xt
i−x∗
i⟩=⟨Fi(xt)−Fi(xt−1),xt
i−gt
i⟩| {z }
I+⟨Fi(xt−1),xt
i−gt
i⟩| {z }
II
+⟨Fi(xt),gt
i−x∗
i⟩| {z }
III(18)
SinceFisL-Lipschitz continuous with respect to ∥ · ∥∗under∥ · ∥, we have following estimation of
Iby using Cauchy-Schwarz inequality
I ≤L2η
2xt−xt−12+1
2ηxt
i−gt
i2. (19)
17In addition, utilizing the result of [Lemma 3.4, [34]] on step-3 and step-4 of Algorithm 3, we have
II ≤1
η
V 
gt
i,gt−1
i
−V 
gt
i,xt
i
−V 
xt
i,gt−1
i
, (20)
III ≤1
η
V 
x∗
i,gt−1
i
−V 
x∗
i,gt
i
−V 
gt
i,gt−1
i
. (21)
Therefore, by applying Eq. (19), (20) and (21) into Eq. (18), we obtain
TX
t=1⟨Fi(xt),xt
i−x∗
i⟩ ≤1
ηV(x∗
i,g0
i) +TX
t=1L2η
2xt−xt−12+1
2ηxt
i−gt
i2
−1
ηTX
t=1V(gt
i,xt
i)−1
ηTX
t=1V(xt
i,gt−1
i)
≤
(a)1
ηV(x∗
i,g0
i) +L2η
2TX
t=1xt−xt−12
−1
2ηTX
t=1gt
i−xt
i2−1
2ηTX
t=1xt
i−gt−1
i2
≤
(b)1
ηV(x∗
i,g0
i) +1
2ηg0
i−x0
i2+L2η
2TX
t=1xt−xt−12
−1
4ηTX
t=1xt
i−xt−1
i2, (22)
where (a) is derived from the assumption that V(xi,ui)≥ ∥xi−ui∥2for any xi,ui∈ Xiand (b)
follows from the convexity of ∥ · ∥. Applying Eq. (22) to Eq. (17), we have
TX
t=1(f(xt)−f(x∗))≤
(c)1
ηdX
i=1V(x∗
i,g0
i)
γi+L2η
2 dX
i=1γ−1
i!TX
t=1xt−xt−12
−1
4ηTX
t=1"dX
i=1xt
i−xt−1
i2
γi#
≤
(d)Pd
i=1γ−1
i
ηmax
i∈[1:d]
max
xi∈XiV(xi,g0
i)
− 
1
4dηγ max−L2η
2dX
i=1γ−1
i!TX
t=1xt−xt−12, (23)
where (c) is derived from the fact that g0
i=x0
ifor any i∈[1 :d]and (d) follows from the convexity
of∥ · ∥ (1
dPd
i=1∥xi∥2≤ ∥1
dPd
i=1xi∥2).
Since KL divergence satisfies KL(xi∥ui)≥ ∥xi−ui∥2
1(Pinsker’s inequality), Theorem 3.2 can be
directly derived from Theorem B.1. Next, we propose Proposition B.2 and provide related proof.
Proposition B.2. We denote N=Pd
i=1niand let a smooth vector-valued function F:RN→Rℓ
satisfies:
1. There is a point y∈RNsuch that ∥DαF(y)∥∞≤γkwith|α|=kfor all k∈[0 :K],
2.For any positive integer kgreater than K,∥DαF∥∞≤γkwith|α|=kuniformly over X,
with a positive constant γand a positive integer K, thenFsatisfies Assumption 3.3.
Proof of Proposition B.2. For any k∈Z+andj∈[1 :l], we have
PF(j)
k,y(x)≤kX
i=0X
|α|=iγi
α!·(|x|+|y|)α=kX
i=0[γ(d+∥y∥1)]i
i!≤exp{γ(d+∥y∥1)}, (24)
18using the fact that ∥DαF(y)∥∞≤γkfor any k∈Z+and|α|=k. In addition, by the Taylor
expansion of F(j)with Lagrange remainder formula for any j∈[1 :l]andk >1, we can obtain
RF(j)
k,y(x)=X
|α|=kDαF(j)(y+t(x−y))
α!(x−y)α≤[γ(d+∥y∥1)]k
k!, (25)
where t∈[0,1]depends on F(j),xandy. Letting k0=⌈3γ(d+∥y∥1)⌉and supposing k≥
k0
1 +log(1+ γ(d+∥y∥1))
log(3/2)
, we derive that
[γ(d+∥y∥1)]k
k!≤3k0−k. (26)
Therefore, in the light of Eq. (24), Eq. (25) and Eq. (26), it’s direct to derive that Fstatisfies
Assumption 3.3 with K0=k0
1 +log(1+ γ(d+∥y∥1))
log(3/2)
,θ=1
3,Θ1= 3k0andΘ2= exp {γ(d+
∥y∥1)}.
The following remark discusses the reasonability of Proposition B.2 conditions, which supports the
reasonability of Assumption 3.3.
Remark B.3.Since region X=Qd
i=1∆niis bounded, it’s reasonable to assume that the growth rate
of the upper bound of internal function’s high-order derivatives is not faster than linear growth rate.
For example, the upper bounds of high-order derivatives of sin(Cx),cos(Cx)andexp{Cx}have
linear growth rate over Xfor fixed constant C. Therefore, if the internal function Fcan be generated
by the linear combination of {sin(Ckx)}K
k=1and{cos(Ckx)}K
k=1(or{exp{Ckx}}K
k=1) with finite
K,Fsatisfies Assumption 3.3 by using Proposition B.2.
B.1 Proof of Theorem 3.5
We briefly introduce our techniques to make the proof of Theorem 3.5 more comprehensible
in this part. Our proof consists of two ingredients. The first is applying Lemma B.4 to con-
struct a variant upper bound of average function error1
TPT
t=1(f(xt)−f(x∗))that is differ-
ent from the upper bound derived from the classical OMD algorithm. This bound is com-
posed of a) O
1
ηT
invariant error and b) weighted sum of the variance for finite difference
sequence {(D1Fi(xt−1)}T
t=1and{(D0Fi(xt−1)}T
t=1overi∈[1 :d], which has the form ofPd
i=11
γi[O(1)
TPT
t=1Varxt
i(D1Fi(xt−1))−O(1)
TPT
t=1Varxt
i(Fi(xt−1))]. The second is applying
Lemma D.7 (refer to it as control lemma) on each {Fi(xt)}T
t=1to bound (b) by a quantity that grows
poly-logarithmically in T. Therefore, it’s necessary to leverage Theorem B.5 and Lemma B.7 to show
that every sequence {Fi(xt)}T
t=0outputted by Algorithm 1 satisfies the preconditions of Lemma
D.7.
B.1.1 Part I
The next Lemma B.4 provides a variant convergence proof of the OMD algorithm. In this Lemma,
basing on KL divergence, an explicit expression for the optimal solution of the OMD sub-problem is
utilized to provide an upper bound ofPT
t=1(f(xt)−f(x∗)).
Lemma B.4. Suppose ∥F(x)∥∞≤Θ(Θ≥1) for any x∈ X and policy set {xt}T
t=1follows the
iteration of Algorithm 1 with step size η∈(0,1
32Θ). Then, it holds that
TX
t=1 
f(xt)−f(x∗)
≤dX
i=11
γi"
log(ni)
η+ ˆg1(ηΘ)ηΘ2TX
t=1Varxt
i 
Fi(xt)−Fi(xt−1)
−ˆg2(ηΘ)ηΘ2TX
t=1Varxt
i(Fi(xt−1))#
, (27)
where ˆg1(η) :=1
2+ 64
1
3(1−16η)+ 2
ηandˆg2(η) :=1
2−16
1
3(1−16η)+ 2
η.
19Proof. As claimed by Definition 3.1, we have the following estimation
TX
t=1(f(xt)−f(x∗))≤dX
i=11
γiTX
t=1
Fi(xt),xt
i−x∗
i
. (28)
In the following, considering a fixed i∈[1 :d], it’s easy to obtain that
⟨Fi(xt),xt
i−x∗
i⟩=⟨Fi(xt)−Fi(xt−1),xt
i−gt
i⟩| {z }
I+⟨Fi(xt−1),xt
i−gt
i⟩| {z }
II
+⟨Fi(xt),gt
i−x∗
i⟩| {z }
III(29)
Recall the update of Algorithm 1 can be devided into two parts:
gt
i= arg min
gi∈∆niη
Fi(xt),gi
+ KL( gi∥gt−1
i), (30)
xt+1
i= arg min
xi∈∆niη
Fi(xt),xi
+ KL( xi∥gt
i), (31)
for any i∈[1 :d]where g0
i∝x0
i·exp{η(Fi(x0)−Fi(x−1))}andx−1
i=x0
i=
1
ni,···,1
ni⊤
.
According to Cauchy-Schwarz inequality, we can evaluate Ias follows
I ≤gt
i−xt
i∗
xt
i·q
Varxt
i(Fi(xt)−Fi(xt−1)). (32)
In addition, utilizing the result of Lemma D.2, we have
II=1
η
KL 
gt
i||gt−1
i
−KL 
gt
i∥xt
i
−KL 
xt
i∥gt−1
i
, (33)
III=1
η
KL 
x∗
i∥gt−1
i
−KL 
x∗
i∥gt
i
−KL 
gt
i∥gt−1
i
. (34)
Therefore, by applying Eq. (32), (33) and (34) into Eq. (29), we obtain
TX
t=1⟨Fi(xt),xt
i−x∗
i⟩ ≤1
ηKL(x∗
i∥g0
i) +TX
t=1∥gt
i−xt
i∥∗
xt
i·q
Varxt
i(Fi(xt)−Fi(xt−1))
−1
ηTX
t=1KL(gt
i∥xt
i)−1
ηTX
t=1KL(xt
i∥gt−1
i). (35)
Since there is a vector Fi(xt)−Fi(xt−1)such that for any j∈[1 :ni]
gt
i(j) =xt
i(j) exp
η 
Fi(j)(xt)−Fi(j)(xt−1)	
Pni
j′=1xt
i(j′) exp{η(Fi(j′)(xt)−Fi(j′)(xt−1))}, (36)
we have that
max
i∈[1:d]gt
i
xt
i
∞≤exp{2ηFi(xt)−Fi(xt−1)
∞} ≤exp{4ηΘ} ≤1 + 8 ηΘ, (37)
and
max
i∈[1:d]xt
i
gt−1
i
∞≤exp{2η∥Fi(xt−1)∥∞} ≤exp{2ηΘ} ≤1 + 4 ηΘ,
with combining Eq. (31) and choosing proper ηsuch that ηΘ≤1
4. According to Lemma D.3, we
have
KL(gt
i∥xt
i)≥1−8ηΘ
2−16ηΘ
3(1−8ηΘ)
X2(gt
i,xt
i),
KL(xt
i∥gt−1
i)≥1−4ηΘ
2−8ηΘ
3(1−4ηΘ)
X2(xt
i,gt−1
i),(38)
20for any i∈[1 :d]. Noting that X2(ρ, µ) =
∥ρ−µ∥∗
µ2
, in the light of Lemma D.4, we derive that
X2(gt
i,xt
i)≤
1 + 321
3(1−16ηΘ)+ 2
ηΘ
(ηΘ)2Varxt
i 
Fi(xt)−Fi(xt−1)
,
X2(gt
i,xt
i)≥
1−321
3(1−16ηΘ)+ 2
ηΘ
(ηΘ)2Varxt
i 
Fi(xt)−Fi(xt−1)
,(39)
as long as ηΘ≤1
32. There exists a similar lower bound with respect to X2(xt
i,gt−1
i)
X2(xt
i,gt−1
i)≥
1−161
3(1−8ηΘ)+ 2
ηΘ
(ηΘ)2Vargt−1
i(Fi(xt−1))
≥
1−161
3(1−8ηΘ)+ 2
ηΘ
(ηΘ)2exp{−2ηΘ}Varxt
i(Fi(xt−1))
≥
(a)
1−161
3(1−8ηΘ)+ 3
ηΘ
(ηΘ)2Varxt
i(Fi(xt−1)), (40)
where (a) is derived from exp{−2ηΘ} ≥1−4ηΘfor any ηΘ≤1
32. Relying on Eq. (35), Eq. (38)-
(40), we conclude that
TX
t=1
Fi(xt),xt
i−x∗
i
≤log(ni)
η+
1 + 321
3(1−16ηΘ)+ 2
ηΘ
ηΘ2TX
t=1Varxt
i 
Fi(xt)−Fi(xt−1)
−1
2−32
3(1−16ηΘ)+ 36
ηΘ
ηΘ2TX
t=1Varxt
i 
Fi(xt)−Fi(xt−1)
−1
2−16
3(1−8ηΘ)+ 27
ηΘ
ηΘ2TX
t=1Varxt
i(Fi(xt−1))
≤log(ni)
η+1
2+ 641
3(1−16ηΘ)+ 2
ηΘ
ηΘ2TX
t=1Varxt
i 
Fi(xt)−Fi(xt−1)
−1
2−161
3(1−16ηΘ)+ 2
ηΘ
ηΘ2TX
t=1Varxt
i(Fi(xt−1)). (41)
Finally, applying the estimation Eq. (41) to Eq. (28), we complete the proof.
B.1.2 Part II
Basing on the conclusion of Lemma B.4, if the finite sum of Varxt
i(Fi(xt)−Fi(xt−1))can be
controlled by the finite sum of Varxt
i(Fi(xt−1))with aO(poly(log( T)))constant for each i∈[1 :d],
the final convergence result can be obtained directly. Hence, to demonstrate this relationship, we
require the assistance of auxiliary Lemma D.7. Our initial step is to prove that Fi(xt)satisfies the
first condition in Lemma D.7 for any i∈[1 :d].
Theorem B.5. Assuming fsatisfies GQC condition and Assumption 3.3 holds, xtfollows the
iteration of Algorithm 1, we set β∈
0,1
(Θ1+Θ2+1)(H+3)
,Γ≥e2+ 322560Θ 2,ˆK≥max{K0,
Hlog(4β−1)+log(Θ 1)
log(θ−1)}andη=β
6e3ˆKΓ max{Θ,1}. Then, the following finite difference bound with
respect to {Fi(xt)}d
i=1holds
max
i∈[1:d](DhFi(x))t0
∞≤βhh3h+1, (42)
for all h∈[1 :H]andt0∈[0 :T−h]. Without loss of generality, we require that Hdoes not
exceed T.
21Proof of Theorem B.5. According to the Taylor expansion of each component kofFiaty, one can
notice that
(DhFi(k)(x))t0≤ˆKX
j=0X
|α|=j|DαFi(k)(y)|
α!(Dh(x−y)α)t0+
DhRFi(k)
ˆK,y(x)t0,(43)
for any ˆK∈Z+. Therefore, setting ˆK≥maxn
Hlog(4β−1)+log(Θ 1)
log(θ−1), K0o
and combining the remark
Eq. (15) of operator Dhin Appendix A.2, we can guarantee the validity of the following estimation
DhRFi(k)
ˆK,y(x)t0≤2hmax
x∈XRFi(k)
ˆK,y(x)≤Θ12hθˆK≤1
2βH≤1
2βHhBh+1, (44)
for any h∈[1 :H]. Moreover, as stated by Assumption 3.3, we obtain max
i∈[1:d]∥Fi(x)∥∞≤Θ1+ Θ 2
for any x∈ X. Suppose that max
i∈[1:d]∥(Dh′Fi(x))t0∥∞≤βh′h′Bh′+1holds for any h′∈[1 :h]and
t0∈[0 :T−h′], we deduce
(Dh+1Fi(k)(x))t0≤g(Γ)βh+1(h+ 1)B(h+1)+1PFi(k)
ˆK,y(xt0) +1
2βh+1(h+ 1)B(h+1)+1
≤1
2+g(Γ)Θ 2
βh+1(h+ 1)B(h+1)+1≤βh+1(h+ 1)B(h+1)+1,(45)
by using Lemma B.6 with p(x) :=|DαFi(k)(y)|
α!(x−y)αand the fact that g(Γ)Θ 2≤1
2(which
can be derived from Lemma D.1). Therefore, to apply mathematical induction, it suffices to
prove that max
i∈[1:d]∥(Dh′Fi(x))t0∥∞≤βh′h′Bh′+1holds when h′= 1. Observe that Lemma
B.6 holds in the case h= 0. Thus, we can obtain Eq. (45) forh= 0 as well. Hence, we have
max
i∈[1:d]∥(D1Fi(x))t0∥∞≤β.
The proof of Theorem B.5 relies on the next Lemma B.6.
Lemma B.6. Assume max
i∈[1:d]∥Fi(x)∥∞≤Θfor any x∈ X and each element in utbelongs to
one of the dprobability distributions generated by Algorithm 1 with η≤β
6e3ΓˆKmax{Θ,1}for some
Γ>1,ˆK≥Kin iteration t, and consider positive constants B≥3,β∈
0,1
(Θ+1)( H+3)
and polynomial function p(u) := CQK
k=1(u(k)−y(k))where u:= (u(1),···,u(K))⊤and
y:= (y(1),···,y(K))⊤∈RKis a fixed point. Given h∈[1 :H−1], we derive that
(Dh+1p(u))t0≤g(Γ)CKY
k=1(ut0(k) +|y(k)|)βh+1(h+ 1)B(h+1)+1, (46)
if the condition max
i∈[1:d]∥(Dh′Fi(x))t0∥∞≤βh′h′Bh′+1holds for any h′∈[1 :h]andt0∈[0 :
T−h′].
Proof. Drawing on the premises outlined in the lemma, we assume that each u(k)corresponds to a
unique xi(k)(j(k)). According to the iteration of Algorithm 1, we can obtain
ut+1(k) =xt
i(k)(j(k))·exp
η· 
2Fi(k)(j(k)) (xt)−Fi(k)(j(k)) 
xt−1	
Pni(k)
j=1xt
i(k)(j)·exp
η· 
2Fi(k)(j) (xt)−Fi(k)(j) (xt−1)	,
=ut(k)·exp
η· 
2Fi(k)(j(k)) (xt)−Fi(k)(j(k)) 
xt−1	
Pni(k)
j=1xt
i(k)(j)·exp
η· 
2Fi(k)(j) (xt)−Fi(k)(j) (xt−1)	, (47)
for any k∈[1 :K]andt∈[1 :T−1]. Given the sequence x1,···,xt0+hgenerated by Algorithm
1, it is straightforward to derive that
ut0+t+1(k) = (Nk
u)−1ut0(k)·exp{η·(Fi(k)(j(k))(xt0+t) +tX
t′=0Fi(k)(j(k))(xt0+t′)
−Fi(k)(j(k))(xt0−1))}, (48)
22for any k∈[1 :K],t0∈[1 :T−h−1]andt∈[1 :h], where Nk
u=Pni(k)
j=1xt0
i(k)(j)·exp{η·
(Fi(k)(j)(xt0+t) +Pt
t′=0Fi(k)(j)(xt0+t′)−Fi(k)(j)(xt0−1))}. We write
rt
t0,k:=Fi(k)(xt0+t−1) +t−1X
t′=0Fi(k)(xt0+t′)−Fi(k)(xt0−1). (49)
Also, for a vector z∈Rni(k)and an index j∈[1 :ni(k)], define
ψj
t0,k(z) =exp{z(j)}
Pni(k)
j′=1xi(k)
t0(j′)·exp{z(j′)}, (50)
so that ut0+t(k) =xt0
i(k)(j(k))·ψj(k)
t0,k
ηrt
t0,k
=ut0(k)·ψj(k)
t0,k
ηrt
t0,k
fort≥1. For conve-
nience, we denote that D:=
α∈NK|α(i)∈ {0,1},∀i∈[1 :K]	
ande:= (1 ,···,1)∈NK. In
particular, for any α∈ D, we have 
Dh′(ue−α)t0≤(ut0)e−α 
Dh′(ψt0(ηrt0))e−α0
| {z }
I(α,h′,t0), (51)
where ψt0(ηrt
t0) :=
ψj(1)
t0,1(ηrt
t0,1),···,ψj(K)
t0,K(ηrt
t0,K)
,h′∈[1 :h+1] andt0∈[1 :T−h−1].
It is important to observe that the finite difference in Eq. (51) pertains specifically to ψj(k)
t0,k(ηrt
t0,k).
Notice that
(D1rt0,k)t= 2(E1Fi(k)(x))t0+t−1−Fi(k)(xt0+t−1), (52)
for any t∈[0 :h]. Therefore, for any h′∈[1 :h+ 1], we obtain
(Dh′rt0,k)t= 2 
E1Dh′−1 
Fi(k)(x)t0+t−1− 
Dh′−1 
Fi(k)(x)t0+t−1, (53)
for any t∈[0 :h+ 1−h′]. Because the step size ηsatisfies η≤β
6e3ΓˆKmax{Θ,1},(D0ηrt0)t
∞≤
ηHΘ≤1
6e3ΓˆKandmax
i∈[1:d](Dh′Fi(x))t0
∞≤βh′h′Bh′+1for all h′∈[1 :h], the following
estimation holds(Dh′+1ηrt0)0
∞≤1
2e2ΓˆKβh′+1(h′+ 1)B(h′+1), (54)
for any h′∈[0 :h]by using Eq. (53) where rt
t0:= 
rt
t0,1(j(1)),···,rt
t0,K(j(K))
. By Lemma
D.5 and Lemma D.6, we have
I(α, h+ 1, t0)≤g(Γ)βh+1(h+ 1)B(h+1)+1. (55)
Noting that
p(u) =CKX
i=0(−1)i
X
α∈D:|α|=iyαue−α
, (56)
and applying bound Eq. (55) to Eq. (50), we can derive
(Dh+1p(u))t0=
(a)|C|KX
i=0(−1)i
X
α∈D:|α|=iyαh+1X
h′=1h+ 1
h′
(−1)h′(ut0+h′)e−α

≤|C|KX
i=0X
α∈D:|α|=i|y|α 
Dh+1 
ue−αt0
≤|C|KX
i=0X
α∈D:|α|=i|y|α(ut0)e−αg(Γ)βh+1(h+ 1)B(h+1)+1
≤g(Γ)βh+1(h+ 1)B(h+1)+1CKY
k=1(ut0(k) +|y(k)|), (57)
for any t0∈[1 :T−h−1]where (a) is derived from the equivalent expression Eq. (56) of the
polynomial p(u)and Eq. (15) of the finite difference (Dhf(x))t0w.r.t function frespectively.
23Recalling that we set parameters as follows
T≥4, H:=⌈log(T)⌉, β=1
8(Θ1+ Θ 2+ 1)H7/2,Γ =e2+ 322560Θ 2,
ˆK= maxHlog(4β−1) + log(Θ 1)
log(θ−1), K0
, η=β
6e2ˆKΓ, B≥3,(58)
According to Theorem B.5, we have max
i∈[1:d](DhFi(x))t
∞≤βhH3h+1for each h∈[0 :H]and
t∈[1 :T−h]. We are now prepared to prove that xt
isatisfies the second condition of Lemma D.7.
Lemma B.7. The sequence {xt
i}T
t=1which has been generated from Algorithm 1 is 7η(Θ1+ Θ 2)−
consecutively close when H≥1,β0= (4H)−1andη∈(0, β4
0(Θ1+ Θ 2+ 1)−1/57792] .
Proof. According to the iteration of Algorithm 1, we have
xt+1
i(k) =xt
i(k)·exp{η·(2Fi(k)(xt)−Fi(k)(xt−1))}Pni
k′=1xt
i(k′)·exp{η·(2Fi(k′)(xt)−2Fi(k′)(xt−1))}, (59)
for any i∈[1 :d]andk∈[1 :ni]. Therefore, for any i∈[1 :d]andt∈[1 :T−1], we obtain
max(xt
i
xt+1
i
∞,xt+1
i
xt
i
∞)
≤exp{6η(Θ1+ Θ 2)}=
(a)(1 + 7 η(Θ1+ Θ 2)), (60)
where (a) is derived from the fact that exp(x)≤1 +7
6xforx∈[0,1/24].
B.1.3 The Last Step
With the preparatory work for proving Theorem 3.5 is completed, we now turn to providing the final
proof:
Proof of Theorem 3.5. Applying Theorem B.5 and Lemma B.7 to Lemma D.7, we have
TX
t=1Varxt
i(Fi(xt)−Fi(xt−1))≤2β0TX
t=1Varxt
i(Fi(xt−1)) + 165120Θ2(1 + 7 ηΘ)H5+ 2.
(61)
According to the result of Lemma B.4, we obtain
TX
t=1(f(xt)−f(x∗))≤dX
i=11
γi"
log(ni)
η− 
ˆg2(ηΘ)ηΘ2−2β0ˆg1(ηΘ)ηΘ2TX
t=1Varxt
i(Fi(xt))#
+ ˆg1(ηΘ)ηΘ2dX
i=11
γi[8β0Θ2+ 165120Θ2(1 + 7 ηΘ)H5+ 2]. (62)
Combining Assumptions 3.3 and parameters selection Eq. (7), we complete the proof.
B.2 Simple Example
In this section, we provide the proof of Example 3.4 which satisfies GQC condition and Assumption
3.3.
Proof of Example 3.4. Recalling the objective function f(p,P) =1
2Ex,y(Pm
i=1piσ(x⊤Pi)−y)2,
we have
f(p∗,P)−f(p,P)≥ ⟨Fp(p,P),p∗−p⟩, (63)
24since f(·,P)is convex for any fixed P. In addition, we obtain
f(p∗,P∗)−f(p∗,P) =−1
2E
(σ(x⊤P1)−σ(x⊤P∗
1))2
(a)
≥BC
2E
⟨σ(x⊤P1)−σ(x⊤P∗
1),x⊤(P∗
1−P1)⟩
=BC
2E
(σ(x⊤P∗
1)−y)x,P∗
1−P1
, (64)
where BCis a constant depends on Cand (a) is derived from the fact that BC⟨exp{x1} −
exp{x2}, x1−x2⟩ ≥ | exp{x1} −exp{x2}|2for any x1, x2∈[−C, C]. Therefore, summing
up Eq. (63) and Eq. (64), we have that fsatisfies GQC condition with the internal functions
Fp={E[(Pm
j=1pjσ(x⊤Pj)−y)]σ(x⊤Pi)}m
i=1for block pandFPi=E
(σ(x⊤Pi)−y)x
for block Pi. Notice that γp= 1,γP1=BC
2andγPi= 0 for any i̸= 1. Furthermore, we
have∥DαFp(·)∥∞≤2 exp{C}(2C)|α|and∥DαFPi(·)∥∞≤exp{C}C|α|+1by using and
x∈[−C, C]d. According to Proposition B.2, we complete the proof.
There is also a toy example satisfying GQC condition and Assumption 3.3.
Example B.8.Assuming (p1,p2)∈∆m×∆n, the function f(p1,p2) =1
2∥p1p⊤
2∥2
Fsatisfies
GQC condition and Assumption 3.3 with the internal functions Fp1=∥p2∥2p1for block p1and
Fp2=p2for block p2.
Proof. We have1
2∥(p∗
1)⊤p2∥F−1
2∥p⊤
1p2∥F≥ ∥p2∥2p⊤
1(p∗
1−p1)and1
2∥(p∗
1)⊤p∗
2∥F−
1
2∥(p∗
1)⊤p2∥F≥ ∥p∗
1∥2p⊤
2(p∗
2−p2). Therefore, we have that fsatisfies GQC condition with
the internal functions Fp1=∥p2∥2p1for block p1andFp2=p2for block p2. Notice that γp1= 1
andγp2=∥p∗
1∥2. Since both ∥p2∥2p1andp2are polynomials with respect to (p1,p2), we derive
that the internal function of fsatisfies Assumption 3.3.
B.3 Application to Reinforcement Learning
B.3.1 Analysis of Infinite Horizon Reinforcement Learning
Proof of Proposition 3.6. The following performance difference lemma [ 30,13,2,35] plays an
important role in the policy gradient based model of infinite horizon reinforcement learning problems,
Vπ∗(ρ0)−Vπ(ρ0) =Es∼dπ∗
ρ0⟨Aπ(s,·),π∗(·|s)−π(·|s)⟩. (65)
Letd=|S|,S={si}d
i=1and write 1/γi=dπ∗
ρ0(si),Fi(π) = Qπ(si,·). According
to Eq. (65) whose proof is given in Cheng et al. [13] and⟨Aπ(si,·),π′(·|si)−π(·|s)⟩=
⟨Qπ(si,·),π′(·|si)−π(·|s)⟩for any policy πandπ′, we obtain that
Vπ∗(ρ0)−Vπ(ρ0) =dX
i=11
γi⟨Fi(π),π∗(·|si)−π(·|si)⟩. (66)
Eq.(66) implies that Vπ(ρ0)satisfies GQC condition. For every a∈ A, the Taylor expansion of
Qπ(si, a)up to K-th order at origin is the same as its truncation at horizon K, which indicates
RQπ(si,a)
K,0 (π) =θK+1EsK+1[Vπ(sK+1)|s0=si, a0=a]≤θK+1.
Therefore, according to the fact that
PQπ(si,a)
K,0 (π)≤Qπ(si, a)≤1,
we have that Qπ(si,·)satisfies Assumption 3.3 with Θ1=θ,Θ2= 1andK0= 1.
25B.3.2 Analysis of Finite Horizon Reinforcement Learning
The function structure of finite horizon reinforcement learning on policy is strictly polynomial.
Moreover, since the action-value functions on horizon his only dependent of policy πh+1:H, we may
therefore verify that the objective function of finite horizon reinforcement learning satisfies GQC
condition by utilizing finite difference expansion on function error Jπ
1(ρ1)−Jπ∗
1(ρ1).
The finite horizon reinforcement learning considers the following policy optimization problem:
min
π∈XJπ
1(ρ1), (67)
where Jπ
1(ρ1) =Es1∼ρ1[Vπ
1(s1)], andX=X1× ··· × X H, and each Xhdenotes |Sh|probability
simplexes. We write Sh={sh,ih}|Sh|
ih=1for any h∈[1 :H]and denote the action-value vector on
state sh,ihat horizon hbyQπh+1:H
h(sh,ih,·). According to the definition of finite horizon value
function Vπh:H
h, we obtain the observation as Eq. (68).
Jπ∗
1(ρ1)−Jπ
1(ρ1) =HX
h=1h
Jπ∗
1:h,πh+1:H
1 (ρ1)−Jπ∗
1:h−1,πh:H
1 (ρ1)i
=HX
h=1E
sh∼Es1∼ρ1Prπ∗
1:h−1
h(·|s1)
Qπh+1:H
h(sh,·),π∗
h(·|sh)−πh(·|sh)
,
(68)
Since Qπh+1:H
h(sh, ah)is a polynomial with respect to policy π1:H, whose value is bounded by
1 +H−hfor any sh∈ Shandah∈ A h, we derive that Jπ
1(ρ1)satisfies GQC condition with
internal function Fh,ih(π) =Qπh+1:H
h(sh,ih,·)for variable block xh,ihwhere ih∈[1 :|Sh|],
andFsatisfies Assumption 3.3 with θ= 0,Θ1= 0,Θ2=HandK0=H. Therefore, for
finite horizon reinforcement learning, it follows from Theorem 3.5 that Algorithm 1 with parameter
selection Eq. (7)finds an ε-suboptimal global solution in a number of iterations that is at most
O(Hmax h∈[0:H]log(|Ah|)ε−1log4(ε−1)).
C Minimax Optimization
We begin with showing the connection between GQCC condition and GQC condition. Without loss
of generality, we assume ni=nandmi=mfor any i∈[1 :d], and let ℓ=n+m. Iff(·,y)and
−f(x,·)satisfy GQC condition with respect to a pair of minimizers x∗(y)andy∗(x), respectively,
then we have the following estimations of function error
f(x,y)−f(x∗(y),y)≤dX
i=11
γi(y)(fi(P(z),xi,yi)−fi(P(z),x∗(y)i,yi)), (69)
f(x,y∗(x))−f(x,y)≤dX
i=11
τi(x)(fi(P(z),xi,y∗(x)i)−fi(P(z),xi,yi)), (70)
where fi(Q,zi) =⟨Qi,zi⟩for any Q∈Rℓ×dandzi∈Rn+m, and each Piincludes the internal
function of f(·,y)for varriable block xiand the internal function of −f(x,·)for variable block yi. It
follows from Eq. (69) and Eq. (70) that Eq. (10) holds for fwithψi(z) = max {1/γi(y),1/τi(x)}.
C.1 Preparatory Discussion
In this section, we provide the convergence analysis of general version of Algorithm 2, i.e., Algorithm
4. We consider the divergence-generating function vwith Bregman’s divergence V(i.e.,V(x,u) =
v(x)−v(u)− ⟨∇ v(u),x−u⟩for any x,u) over general compact convex regions Z=X × Y ⊂
RPd
i=1ni×RPd
i=1mi. Before we introduce the main theorem, we need the following assumptions:
Assumption C.1. There exists positive constants A, D such that
[A1]max
i∈[1:d]{∥zi∥} ≤ Auniformly on Z.
26[A2]max(
max
zi∈Zi
i∈[1:d] 
V 
xi,(gx
i)0
+V 
yi,(gy
i)0
,max
zi∈Zi
i∈[1:d](v(xi) +v(yi)))
≤D.
[A3]vmodulus 2 with respect to ∥ · ∥ (i.e.,∀i∈[1 :d],V(xi,ui)≥ ∥xi−ui∥2for any
xi,ui∈ XiandV(yi,wi)≥ ∥yi−wi∥2for any yi,wi∈ Yi).
If we choose v(x) =Pn
j=1x(j) log(x(j))and∥ · ∥=∥ · ∥1, then (1) in Assumption C.1 holds with
A= 2; (2) in Assumption C.1 holds with D= 2 max i∈[1:d]{log(ni) + log( mi)}; (3) in Assumption
C.1 holds following Pinsker’s inequality. According to Remark C.2, we state that there exist some
compact convex regions in RPd
i=1(ni+mi)with proper divergence-generating function vand proper
choice of g0satisfy Assumption C.1.
Remark C.2.If the feasible region Zis a compact set of Euclidean space, then it is reasonable
that assuming the divergence-generating function v(i.e., v(x) =Pn
j=1x(j) log(x(j))over the
probability simplex or v(x) =∥x∥2
2over the standard compact set) and the norm ∥ · ∥ are uniformly
bounded on every Zi. For some Bregman divergences, if x0is a fixed point, V(·,x0)can be
bounded by a constant (may depend on the dimension of space) on a compact feasible region,
such as V(·,x0) =∥ · −x0∥2
2withx0=0on the closed ball BR(0)for radius R∈(0,∞)and
V(·,x0) = KL( ·∥x0)withx0= (1/n,···,1/n)on the probability simplex ∆n.
Assumption C.3. In Definition 4.1, let matrix-valued function Phas the form of P(Qz,z)
where Qz∈Rℓ×ddepends on z, and assume that Psatisfies the following properties on region 
Q∈Rℓ×d∥Q∥∞≤C} × Z for some constant C >0:
[A4]There exist constants L1, L2≥0such that Fi(·,zi)is uniformly L1-Lipschitz continuous
with respect to ∥ · ∥∗under∥ · ∥∞, andFi(P,·)is uniformly L2-Lipschitz continuous with
respect to ∥ · ∥∗under∥ · ∥.
[A5]There are a positive constant γ >0and a pair of sets of matrices
{Bi}d
i=1,{Ci}d
i=1	
⊂
Rℓ×d
+∪0satisfyingPd
i=1(Bi+Ci)
∞≤γ, such that the following bounds hold
DP(Q,·,y)(x,x′)≤dX
i=1Ci⟨Fx
i(Q,zi),xi−x′
i⟩,
DP(Q,x,·)(y′,y)≤dX
i=1Bi⟨−Fy
i(Q,zi),y′
i−yi⟩,
for any y,y′∈ Y andx,x′∈ X.
[A6]There exists θ∈[0,1)such that P(·,z)is aθ-contraction mapping under ∥ · ∥∞, and
∥P(Q,z)∥∞≤Cfor any z∈ Z.
Lemma C.4 (General Version of Lemma 4.3) .Assuming that Assumption C.1 and C.3 hold, [P(Q,·,
·)]k,jis continuous, and convex with respect to x, and concave with respect to yfor any (k, j), and
min
k,j
i∈[1:d]min{[Ci]k,j,[Bi]k,j}
[Ci]k,j+[Bi]k,j≥C′for some C′>0, then we claim that there exist Q∗∈Rℓ×dand
z∗∈ Z such that
Q∗=P(Q∗,x∗,y∗), (71)
Q∗≤P(Q∗,x,y∗), (72)
Q∗≥P(Q∗,x∗,y). (73)
Proof. We shall begin the proof by proving the following lemma.
Lemma C.5. Under the conditions of Lemma C.4, it can be proven that for any Q∈Rℓ×d, there
exists a pair of x∗,y∗that satisfy the following
P(Q,x∗,y)≤P(Q,x∗,y∗)≤P(Q,x,y∗). (74)
27Proof. Considering the following iteration
zt
i= argmin
zi∈Ziη
Fi(Q,zt−1
i),zi
+V 
xi,(gx
i)t−1
+V 
yi,(gy
i)t−1
,
gt
i= argmin
gi∈Ziη
Fi(Q,zt
i),gi
+V 
gx
i,(gx
i)t−1
+V 
gy
i,(gy
i)t−1
,(75)
for any i∈[1 :d]and combining [57, Lemma 1], we have
[Ci]k,jTX
t=1
Fx
i(Q,zt
i),xt
i−x′
i
+ [Bi]k,jTX
t=1
Fy
i(Q,zt
i),yt
i−y′
i
≤([Ci]k,j+ [Bi]k,j)η−1D+ [Ci]k,jTX
t=1Fx
i(Q,zt
i)−Fx
i(Q,zt−1
i)
∗xt
i−(gx
i)t
+ [Bi]k,jTX
t=1Fy
i(Q,zt
i)−Fy
i(Q,zt−1
i)
∗yt
i−(gy
i)t
−[Ci]k,j
ηTX
t=1xt
i−(gx
i)t2+(gx
i)t−1−xt
i2
−[Bi]k,j
ηTX
t=1yt
i−(gy
i)t2+(gy
i)t−1−yt
i2
≤
(a)([Ci]k,j+ [Bi]k,j)η−1D+ηL2
2([Ci]k,j+ [Bi]k,j)
2TX
t=1zt
i−zt−1
i2
−min{[Ci]k,j,[Bi]k,j}
ηTX
t=11
4zt
i−gt
i2+1
2gt−1
i−zt
i2
, (76)
for any i∈[1 :d],(k, j)∈[1 :ℓ]×[1 :d], andz′
i∈ Z i, where (a) is derived from A4in
Assumption C.3 and Cauchy-Schwarz inequality. Therefore, by setting η=√
C′
2L2and1
TPT
t=1zt=
¯zT= (¯xT,¯yT), the following estimation holds for any (k, j)
max
z′=(x′,y′)∈Z[P(Q,¯xT,y′)−P(Q,x′,¯yT)]k,j
≤
(b)1
TTX
t=1
P(Q,xt,¯y∗
T)−P(Q,¯x∗
T,yt)
k,j
≤
(c)1
TdX
i=1TX
t=1 
[Ci]k,j
Fi(Q,zt
i),xt
i−(¯x∗
T)i
+ [Bi]k,j
Fi(Q,zt
i),yt
i−(¯y∗
T)i
≤2γη−1D+ 4ηγA2L2
2
T, (77)
where the convexity of function [P(Q,·,w)−P(Q,u,·)]k,jfor fixed Qandv= (u,w)implies
(b), and (c) is derived from Eq. (76) and the definition that (¯x∗
T,¯y∗
T) := argmax
z′∈Z[P(Q,¯xT,y′)−
P(Q,x′,¯yT)]k,j. Since Zis a compact set, the sequence {(¯xT,¯yT)}∞
T=1must have a convergent
subsequence. Therefore, all accumulation points of the sequence {(¯xT,¯yT)}∞
T=1satisfy Eq. (74) by
using the continuity of P(Q,·,·).
Now, we define the iterately update as follows
Qt+1=P(Qt,x∗
t,y∗
t), (78)
28where (x∗
t,y∗
t)satisfies Eq. (74) in Lemma C.5 w.r.t P(Qt,·,·). It’s direct to derive that
Qt+1−Qt≤P(Qt,x∗
t−1,y∗
t)−P(Qt−1,x∗
t−1,y∗
t)
≤θQt−Qt−1
∞, (79)
Qt+1−Qt≥P(Qt,x∗
t,y∗
t−1)−P(Qt−1,x∗
t,y∗
t−1)
≥ −θQt−Qt−1
∞. (80)
Finally, according to the contraction mapping principle, we complete the proof.
Corollary C.6. Assuming preconditions of Lemma C.4 hold, and letting {fi(Q,·) :Rni+mi→
R}d
i=1be a sequence of continuous convex-concave functions which satisfies ∇fi(Q,·) = (Fx
i(Q,·),
−Fy
i(Q,·))for any fixed Q∈Rℓ×dandi∈[1 :d], then there exist a matrix Q∗and a pair of
(x∗,y∗)which satisfy Eq. (71)-Eq. (73) and
fi(Q∗,x∗
i,y∗
i)≥fi(Q∗,x∗
i,yi),
fi(Q∗,x∗
i,y∗
i)≤fi(Q∗,xi,y∗
i),
for any zi∈ Ziandi∈[1 :d].
Proof. With proper selection of η, we have the following bound which is similar to that derived from
Eq.(77)
max
y′
i∈Yifi(Q,(¯xT)i,y′
i)−min
x′
i∈Xifi(Q,x′
i,(¯yT)i)
≤TX
t=1[fi(Q,xt
i,(¯y∗
T)i)−fi(Q,(¯x∗
T)i,yt
i)]
≤TX
t=1
Fi(Q,zt
i),xt
i−(¯x∗
T)i
+
Fi(Q,zt
i),yt
i−(¯y∗
T)i
≤4η−1D+ 8ηA2L2
2
T, (81)
for every i∈[1 :d], where {zt= (xt,yt)}T
t=1follows from the iteration (75) and(¯z∗
T)i=
((¯x∗
T)i,(¯y∗
T)i)denotes argmax
z′
i∈Zi[fi(Q,(¯xT)i,y′
i)−fi(Q,x′
i,(¯yT)i)]. Hence, by directly leveraging
the result of Lemma 4.3, we obtain the result.
Before stating the general version of Theorem 4.4 as follows, we define
Yη
T= 8(c+ 1)
γD1
η+ 16ηL2
+ 40η3γA2L4
2+ 2ηγL2
1(1 + 64 η2L2
2C2)
(log(c+T) + 1) .
(82)
C.2 Theorem C.7 and Relate Proof
Theorem C.7. [General Version of Theorem 4.4] For any generaized quasar-convex-concave function
fwhich satisfies Assumption C.1 and C.3 with constant matrix function P≡Q∗, where Q∗is
unknown and satisfies Eq. (71)-Eq. (73), with parameter configuration in Eq. (12), the weighted
average of Algorithm 2’s outputs {zt}T
t=1satisfies the following inequality
Gf(¯xT,¯yT)≤6
max
z∈ZPd
i=1ψi(z)
(1−θ)−1
3D
η+ 10ηL2
1+ 5AL1Yη
T+ 4ηA2L2
2
T+ 3.(83)
For a generalized quasar-convex-concave function satisfying smoothness and recurrence conditions,
the iteration complexity of our algorithm matches the lower bound [ 52] for solving ε-approximate
Nash equilibrium points in the smooth convex-concave setting, up to a logarithmic factor. Furthermore,
we prove that standard smooth convex-concave functions satisfy the preconditions of Theorem C.7
(as discussed in Appendix C.3.2).
29Algorithm 4 Optimistic Mirror Descent with Regularization for Multi-Variables
Input:
z0
i	d
i=1=
g0
i	d
i=1,{αt≥0}T
t=1(PT
t=1αt= 1) ,{γt≥0}T
t=1,{λt≥0}T
t=1,ηandQ0=0.
Output: ¯zT=PT
t=1αtzt.
1:while t≤Tdo
2:Qt= (1−βt−1)Qt−1+βt−1P(Qt−1,zt−1).
3: for all i∈[1 :d]do
4: xt
i= argmin
xi∈Xiη
Fx
i(Qt−1,zt−1
i),xi
+γtV 
xi,(gx
i)t−1
+λtv(xi),
5: yt
i= argmin
yi∈Yiη
Fy
i(Qt−1,zt−1
i),yi
+γtV 
yi,(gy
i)t−1
+λtv(yi),
6: (gx
i)t= argmin
gx
i∈Xiη
Fx
i(Qt,zt
i),gx
i
+γtV 
gx
i,(gx
i)t−1
+λtv(gx
i),
7: (gy
i)t= argmin
gy
i∈Yiη
Fy
i(Qt,zt
i),gy
i
+γtV 
gy
i,(gy
i)t−1
+λtv(gy
i).
8: end for
9: t←t+ 1.
10:end while
Our analysis relies on the connection between Fi(Qt,zt
i)andFi(Q∗,zt
i). Theorem C.8 combines
a) classical O(log(T)/T)bound derived from regularized OMD, and b) the weighted average of
iteration error ∥Qt−Qt−1∥2
∞overt∈[1 :T]which has the form ofPT
t=1αt∥Qt−Qt−1∥2
∞,
with magnitude O(T−1log(T)), and c) weighted average of approximation error ∥Qt−Q∗∥∞over
t∈[1 :T]which has the form ofPT
t=1αt∥Qt−Q∗∥∞to bound the max-min gap of fat¯zT. Next,
we leverage Lemma C.9 to show the decreasing trend of approximation error ∥Qt−Q∗∥∞and
boundPT
t=1αt∥Qt−Q∗∥∞by a quantity that grows only logarithmically in T. We may therefore
obtain the result of Theorem C.7 by applying the estimation of weighted average of approximation
errorPT
t=1αt∥Qt−Q∗∥∞to Theorem C.8.
C.2.1 Part I
Theorem C.8. Assuming that Assumption C.1 holds, we set the hyper-parameters for Algorithm 2
carefully such that
αt(γt+λt)≥αt+1γt+1, (84)
η≤min
t∈[1:T]p
γt(γt+λt)
4L2. (85)
Suppose that vmodulus 2 w.r.t ∥ · ∥,∥z∥ ≤Afor any z∈ Z, and{zt}T
t=1follows the iterations of
Algorithm 4, then we can show that
max
y′∈Yf(¯xT,y′)−min
x′∈Xf(x′,¯yT)≤B(ψ) max
i∈[1:d]α1γ1
ηmax
zi∈Zi 
V 
xi,(gx
i)0
+V 
yi,(gy
i)0
−1
2ηTX
t=1αt
γt∥zt
i−gt−1
i∥2+γt+λt
2∥gt
i−zt
i∥2
+2PT
t=1αtλt
ηmax
zi∈Zi(v(xi) +v(yi)))
+ 2B(ψ)ηL2
1TX
t=1αt
γt+λt∥Qt−Qt−1∥2
∞
+ 2AB(ψ)L1TX
t=1αt∥Qt−Q∗∥∞+8A2B(ψ)L2
2α1η
γ1+λ1,
(86)
where B(ψ) := max
z∈ZPd
i=1ψi(z)and¯zT:=PT
t=1αtzt.
30Proof. Recalling the definition of GQCC, we derive that
max
y′∈Yf(x,y′)−min
x′∈Xf(x′,y)≤B(ψ) max
i∈[1:d]
max
wi∈Yifi(Q∗,xi,wi)−min
ui∈Xifi(Q∗,ui,yi)
,(87)
for any z= (x,y),v= (u,w)∈ Z and
fi(Q∗,(¯xT)i,wi)−fi(Q∗,ui,(¯yT)i)≤TX
t=1αt
fi(Q∗,xt
i,wi)−fi(Q∗,ui,yt
i)
(88)
≤TX
t=1αt
Fi(Q∗,zt
i),zt
i−vi
≤TX
t=1αt
Fi(Qt,zt
i),zt
i−vi
+ 2AL1TX
t=1αt∥Qt−Q∗∥∞,
for any vi∈ Zi. Using the optimality condition, we obtain

Fi(Qt−1,zt−1
i),zt
i−gt
i
≤γt
η 
V 
(gx
i)t,(gx
i)t−1
+V 
(gy
i)t,(gy
i)t−1
−γt
η 
V 
xt
i,(gx
i)t−1
+V 
yt
i,(gy
i)t−1
−γt+λt
η 
V 
(gx
i)t,xt
i
+V 
(gy
i)t,yt
i
+λt
η 
v 
(gx
i)t
+v 
(gy
i)t
−v 
xt
i
−v 
yt
i
, (89)

Fi(Qt,zt
i),gt
i−vi
≤γt
η 
V 
ui,(gx
i)t−1
+V 
wi,(gy
i)t−1
−γt
η 
V 
(gx
i)t,(gx
i)t−1
+V 
(gy
i)t,(gy
i)t−1
−γt+λt
η 
V 
ui,(gx
i)t
+V 
wi,(gy
i)t
+λt
η 
v(ui) +v(wi)−v 
(gx
i)t
−v 
(gy
i)t
. (90)
For each t∈[1 :T], we can apply Eq. (89) and Eq. (90) to the following equation
αt
Fi(Qt,zt
i),zt
i−vi
=αt
Fi(Qt,zt
i),gt
i−vi
+
Fi(Qt−1,zt−1
i),zt
i−gt
i
+
Fi(Qt,zt
i)−Fi(Qt−1,zt−1
i),zt
i−gt
i
,
≤αtγt
η 
V 
ui,(gx
i)t−1
+V 
wi,(gy
i)t−1
−γt+λt
η 
V 
ui,(gx
i)t
+V 
wi,(gy
i)t
−γt
η 
V 
xt
i,(gx
i)t−1
+V 
yt
i,(gy
i)t−1
−γt+λt
η 
V 
(gx
i)t,xt
i
+V 
(gy
i)t,yt
i
+αtλt
η(v(ui) +v(wi)
−v 
(gx
i)t
−v 
(gy
i)t
+αt
Fi(Qt,zt
i)−Fi(Qt−1,zt−1
i),zt
i−gt
i
.
(91)
31Therefore, by summing Eq.(91) from t= 1tot=Tand utilizing Eq. (84), we have
TX
t=1αt
Fi(Qt,zt
i),zt
i−vi
≤α1γ1
η 
V 
ui,(gx
i)0
+V 
wi,(gy
i)0
+2PT
t=1αtλt
ηmax
zi∈Zi(v(xi) +v(yi))
−1
ηTX
t=1αt
γt∥zt
i−gt−1
i∥2+γt+λt
2∥gt
i−zt
i∥2
+ηTX
t=1αt
γt+λtFi(Qt,zt
i)−Fi(Qt−1,zt−1
i)2
∗
.(92)
According to the Lipschitz continuity of Fi, we derive that
ηTX
t=1αt
γt+λtFi(Qt,zt
i)−Fi(Qt−1,zt−1
i)2
∗
≤2ηL2
2TX
t=1αt
γt+λt∥zt
i−zt−1
i∥2
+ 2ηL2
1TX
t=1αt
γt+λt∥Qt−Qt−1∥2
∞
. (93)
It follows from parameter setting Eq. (84) and Cauchy-Schwarz inequality that
αtγt
2∥zt
i−gt−1
i∥2+αt−1(γt−1+λt−1)
2∥gt−1
i−zt−1
i∥2≥αtγt
4∥zt
i−zt−1
i∥2. (94)
Combining Eq. (85) and Eq. (94), we may therefore obtain
−1
ηTX
t=1αtγt
2∥zt
i−gt−1
i∥2+γt+λt
4∥gt
i−zt
i∥2
+ 2ηL2
2TX
t=1αt
γt+λt∥zt
i−zt−1
i∥2
≤2ηL2
2α1
γ1+λ1∥z1
i−z0
i∥2. (95)
Applying Eq. (93) and Eq. (95) to Eq. (92) and utilizing Eq. (87), Eq. (88), we complete the proof.
C.2.2 Part II: Estimation of Approximation Error ∥Qt−Q∗∥
According to the iterately update of Qt, we can derive the upper bound of weighted average of
∥Qt−Qt−1∥2
∞. Next, we aim to bound ∥Qt−Q∗∥for each iteration t. In this section, we select
the following parameter settings:
c= 2(1 −θ)−1, η≤(1−θ)1/2
8(γAL 1)1/2L2, βt=c
c+t, αt=βT,t, γt=αt−1
αt, λt= 1−γt.(96)
Lemma C.9. Consider the settings: γt=αt−1
αt≤1, λt= 1−γt, and η≤(1−θ)1/2
8(γAL 1L2)1/2. Then, we
obtain the estimation of ∥Qt−Q∗∥as follows,
∥Qt−Q∗∥∞≤tX
j=2β(1+θ)/2
t,j Hj, (97)
for any t≥2where
Hj:=γD1
η+ 16ηL2 
βj−1,1γ1+ 2j−1X
κ=1βj−1,κλκ!
+ 2ηγL2
1(1 + 64 η2L2
2C2)j−1X
κ=1βj−1,κβ2
κ−1+ 128 η3γA2L4
2βj−1,1. (98)
32Proof. According to the fact that Q∗is a fixed point of function P, we have
Qt−Q∗=t−1X
κ=1βt−1,κ[P(Qκ,zκ)−P(Q∗,z∗)]
≤
(a)t−1X
κ=1βt−1,κ{[P(Qκ,xκ,yκ)−P(Qκ,x∗,yκ)] + [P(Qκ,x∗,yκ)−P(Q∗,x∗,yκ)]}
≤dX
i=1 t−1X
κ=1βt−1,κ⟨Fx
i(Qκ,zκ
i),xκ
i−x∗
i⟩!
Ci+θ t−1X
κ=1βt−1,κ∥Qκ−Q∗∥∞!
ede⊤
d.
(99)
Where (a) is derived from the maximizer’s property of y∗for matrix-valued function P(Q∗,x∗,·).
Similarly, we can obtain
Qt−Q∗≥ −dX
i=1 t−1X
κ=1βt−1,κ⟨Fy
i(Qκ,zκ
i),yκ
i−y∗
i⟩!
Bi−θ t−1X
κ=1βt−1,κ∥Qκ−Q∗∥∞!
ede⊤
d.
(100)
Hence, we derive
∥Qt−Q∗∥∞≤γmax
i∈[1:d]βt−1,1γ1
ηmax
zi∈Zi 
V 
xi,(gx
i)0
+V 
yi,(gy
i)0
+2Pt−1
κ=1βt−1,κλκ
ηmax
zi∈Zi(v(xi) +v(yi))
+2ηL2
2t−1X
κ=1βt−1,κ
γκ+λκzκ
i−zκ−1
i2)
+ 2ηγL2
1t−1X
κ=1βt−1,κ
γκ+λκ∥Qκ−Qκ−1∥2
∞+θ t−1X
κ=1βt−1,κ∥Qκ−Q∗∥∞!
,
(101)
by combining βt−1,κQT
j=t(1−βj) = ακand Eq. (99), and using the proof technique of
Theorem C.8. Next, for any i∈[1 : d], we can obtain an upper bound estimation of
max
vi∈ZiPt−1
κ=1βt−1,κ⟨Fi(Qκ,zκ
i),zκ
i−vi⟩as follows
max
vi∈Zit−1X
κ=1βt−1,κ⟨Fi(Qκ,zκ
i),zκ
i−vi⟩ ≤D
η 
βt−1,1γ1+ 2t−1X
κ=1βt−1,κλκ!
+ 8ηA2L2
2βt−1,1
+ 8ηL2
1C2t−1X
κ=1βt−1,κβ2
κ−1−1
8ηt−1X
κ=1βt−1,κzk
κ−zk
κ−12.
(102)
Furthermore, we also have a lower bound estimation of it
max
vi∈Zit−1X
κ=1βt−1,κ⟨Fi(Qκ,zκ
i),zκ
i−vi⟩ ≥max
vi∈Zit−1X
κ=1βt−1,κ⟨Fi(Qκ,vi),zκ
i−vi⟩
≥max
vi∈Zit−1X
κ=1βt−1,κ⟨Fi(Q∗,vi),zκ
i−vi⟩
−2AL1t−1X
κ=1βt−1,κ∥Qκ−Q∗∥∞
≥ −2AL1t−1X
κ=1βt−1,κ∥Qκ−Q∗∥∞. (103)
33Therefore, combining Eq. (102) and Eq. (103), we derive the following result
t−1X
κ=1βt−1,κzκ
i−zκ−1
i2≤16ηAL 1t−1X
κ=1βt−1,κ∥Qκ−Q∗∥∞+ 64η2A2L2
2βt−1,1
+ 8D 
βt−1,1γ1+ 2t−1X
κ=1βt−1,κλκ!
+ 64η2C2L2
1t−1X
κ=1βt−1,κβ2
κ−1,
(104)
for any i∈[1 :d].
∥Qt−Q∗∥∞≤γD1
η+ 16ηL2
2 
βt−1,1γ1+ 2t−1X
κ=1βt−1,κλκ!
+ 128 η3γA2L4
2βt−1,1
+ 2ηγL2
1 
1 + 64 η2C2L2
2t−1X
κ=1βt−1,κβ2
κ−1
+ (32 η2γAL 1L2
2+θ)t−1X
κ=1βt−1,κ∥Qκ−Q∗∥∞. (105)
Finally, by applying [67, Lemma 33] to Eq. (105), we complete the proof.
Under parameter settings Eq. (96), the following auxiliary Lemma C.10 provides both lower bound
and upper bound of βT,t.
Lemma C.10. Assuming that βt=c′
c+tandc≥c′, we can obtain the following result:
exp
−(c′+c′c)2
2c(c+t)c′−1
(c+T)c′≤βT,t≤(1 +c)(c+t+ 1)c′−1
(c+T+ 1)c′ , (106)
for any T≥t≥1.
Proof. Recalling that
βT,t=c′
c+tTY
k=t+1
1−c′
c+k
=c′
c+texp(TX
k=t+1log
1−c′
c+k)
, (107)
we have
βT,t≤c′
c+tc+t+ 1
c+T+ 1c′
≤(1 +c)(c+t+ 1)c′−1
(c+T+ 1)c′ , (108)
and
βT,t≥exp
−(c′+c′c)2
2c(c+t)c′−1
(c+T)c′, (109)
by combining the result of Lemma D.9.
Corollary C.11. Assuming that βt=c′
c+t,c≥1andc′(1−θ)≥1, we can obtain
βθ
T,t≤c′
c+T, (110)
for any T≥t≥1.
By utilizing the result of Lemma C.10, we notice that
j−1X
κ=1βj−1,κλκ≤j−1X
κ=1(1 +c)2(c+κ+ 1)c−2
(c+j)c
≤
(a)(1 +c)2
(c+j)cZj−1
1(c+x+ 1)c−2dx+(1 +c)2
(c+j)2
≤(1 +c)2
(c−1)(c+j)+(1 +c)2
(c+j)2, (111)
34and
j−1X
κ=1βj−1,κβ2
κ−1≤j−1X
κ=1(1 +c)4(c+κ+ 1)c−3
c(c+j)c
≤
(b)(1 +c)3
c(c+j)cZj−1
1(c+x+ 1)c−2dx+(1 +c)4
c(c+j)3
≤(1 +c)3
c(c−1)(c+j)+(1 +c)4
c(c+j)3, (112)
where (a) and (b) are derived from the fact thatPj−2
κ=1(c+κ+ 1)c−2≤Rj−1
1(c+x+ 1)c−2dx.
Next, we have
Hj≤γD1
η+ 16ηL22(c+ 2)c−1
(c+j)c+2(1 + c)2
(c−1)(c+j)+2(1 + c)2
(c+j)2
+ 2ηγL2
1(1 + 64 η2L2
2C2)(1 +c)3
c(c−1)(c+j)+(1 +c)4
c(c+j)3
+ 128 η3γA2L4
2(c+ 2)c
(c+j)c, (113)
and
tX
j=2β(1+θ)/2
t,j Hj≤
cc
c+t
γD1
η+ 16ηL2Zt
22(c+ 2)c−1
(c+x)cdx
+Zt
12(1 + c)2
(c−1)(c+x)+2(1 + c)2
(c+x)2
dx+ 1
+2ηγL2
1(1 + 64 η2L2
2C2)Zt
1(1 +c)3
c(c−1)(c+x)+(1 +c)4
c(c+x)3
dx
+ 128 η3γA2L4
2
1 +Zt
2(c+ 2)c
(c+x)cdx
≤c
c+t
γD1
η+ 16ηL22(c+ 1)2
c−1log(c+t) + 5 + 2 c
+ 640 η3γA2L4
2
+ 2ηγL2
1(1 + 64 η2L2
2C2)2(c+ 1)2
c−1log(c+t) +(c+ 1)2
2c
,
(114)
where (c) follows from Corollary C.11. For simplicity, we denote
Yη
T:= 8( c+ 1)
γD1
η+ 16ηL2
+ 40η3γA2L4
2+ 2ηγL2
1(1 + 64 η2L2
2C2)
(log(c+T) + 1) .
(115)
Therefore, it follows from Eq. (114) and Lemma C.9 that
∥Qt−Q∗∥∞≤tX
j=2β(1+θ)/2
t,j Hj≤c
c+tYη
t. (116)
C.2.3 The Last Step
According to Eq. (116) and the initial Q0satisfies ∥Q0∥∞≤C, we have ∥Q∗∥ ≤C. We are ready
to complete the proof of Theorem C.7.
Proof of Theorem C.7. It is noteworthy that the hyper-parameters selected in Eq. (96) satisfies the
preconditions of Theorem C.8. Combining the conclusion of Theorem C.8, Eq. (116) and the
35estimation of αt(i.e.βT,t) in Lemma C.10, we obtain:
Gf(x,y)≤Bmax
i∈[1:d]α1γ1
ηmax
zi∈Zi 
V 
xi,(gx
i)0
+V 
yi,(gy
i)0
+2PT
t=1αtλt
ηmax
zi∈Zi(v(xi) +v(yi)))
+ 2ηBL2
1TX
t=1αtβ2
t−1
+ 2ABcL 1Yη
TTX
t=1αt
c+t+ 8ηA2BL2
2α1
≤
a2BD
η(c+ 2)c−1
(c+T+ 1)c+c(c+ 2)
(c+T+ 1)2+2(c+ 1)
c+T+ 1
+ 2ηBL2
1(c+ 1)3
c(c−1)(c+T+ 1)+(c+ 1)4
c(c+T+ 1)3
+ 2ABL 1Yη
T4c
c+T+ 1+c(c+ 2)
(c+T+ 1)2
+ 8ηA2BL2
2(1 +c)(c+ 2)c−1
(c+T+ 1)c
≤2B3D
η+ 10ηL2
1+ 5AL1Yη
T+ 4ηA2L2
2c+ 1
c+T+ 1, (117)
when T≥1, where Bdenotes maxz∈ZPd
i=1ψi(z)and (a) is derived from parameter settings
Eq. (96) and the result of Lemma C.10.
C.3 Application to Minimax Problems
C.3.1 Infinite Horizon Two-Player Zero-Sum Markov Games
To simplify notations, in the following discussion, we write S={si}|S|
i=1, and denote by Qz= 
Qz(s1,·,·),···,Qz(s|S|,·,·)
the joint action-value matrix where Qz(si,·,·)∈R|A|×|B|is an
action-value matrix on state si. According to the connection between value function and action-value
function
Vz(si) =Ea∼x(·|si)
b∼y(·|si)[Qz(si, a, b)], Qz(si, a, b) = (1 −θ)σ(si, a, b) +θEsi′∼P(·|si,a,b)[Vz(si′)],
we provide the following proof for Proposition 4.5.
Proof of Proposition 4.5. By defining
[Pi(Q,z)]a,b:= (1−θ)σ(si, a, b) +θEsi′∼P(·|si,a,b)[⟨Qi′yi′,xi′⟩], (118)
we derive that Qz=P(Qz,z). We can notice that
[Pi(Q,x,y)−Pi(Q,x′,y)]a,b=θEsi′∼P(·|si,a,b)[⟨Qi′yi′,xi′−(x′)i′⟩],
[Pi(Q,x,y′)−Pi(Q,x,y)]a,b=θEsi′∼P(·|si,a,b)
⟨−Q⊤
i′xi′,yi′−(y′)i′⟩
.(119)
Therefore, for any Qsatisfies ∥Q∥∞≤1, it’s easy to verify that
1.Fi(·,zi)is uniformly 2-Lipschitz continuous with respect to ∥ · ∥∞under∥ · ∥∞for any
zi∈ Zi, andFi(Q,·)is uniformly 1-Lipschitz continuous with respect to ∥ · ∥∞under
∥ · ∥ 1, since F(Q,zi) = 
y⊤
iQ⊤
i,−x⊤
iQi⊤,
2.Psatisfies [A2]in Assumptions 4.2 with [Bi]s,a,b= [Ci]s,a,b=θP(si|s, a, b )andγ= 2θ,
since Eq. (119),
3.P(·,z)is aθ-contraction mapping under ∥ · ∥∞, and∥P(·,·)∥∞≤1, since the definition
ofP,
4.[Pi(Q,·,·)]a,bis bi-linear with respect to xandy, andmin{[Ci]s,a,b,[Bi]s,a,b}
[Ci]s,a,b+[Bi]s,a,b≡1/2for any
iands, a, b .
36Therefore, according to Lemma 4.3, there exist a tensor Q∗and a pair of (x∗,y∗)satisfy Eq. (11).
Furthermore, the (x∗,y∗)mentioned above is a Nash equilibrium of Jx,y(ρ0)by utilizing Corollary
C.6. We may therefore derive that Q∗≡Qz∗. Leveraging Eq. (65) for any Nash equilibrium
(x∗,y∗)∈ Z and denoting Q∗
i=Qx∗,y∗(si,·,·), we have
Jx∗,y∗(ρ0)−Jx∗(y),y(ρ0) =X
s∈Sdx∗(y),y
ρ0(s)h
⟨Qx∗,y∗(s,·,·)y∗(·|s),x∗(·|s)⟩ (120)
−⟨Qx∗,y∗(s,·,·)y(·|s),x∗(y)(·|s)⟩i
≤|S|X
i=1dx∗(y),y
ρ0(si)
⟨Q∗
iy∗
i,x∗
i⟩ −min
ui∈Xi⟨Q∗
iyi,ui⟩
, (121)
where x∗(y) = argmin
u∈XJu,y(ρ0)andy∗(x) = argmax
w∈YJx,w(ρ0). Similarly, we have
Jx,y∗(x)(ρ0)−Jx∗,y∗(ρ0)≤|S|X
i=1dx,y∗(x)
ρ0(si)
max
wi∈Yi⟨(Q∗
i)⊤xi,wi⟩ − ⟨(Q∗
i)⊤x∗
i,y∗
i⟩
.
(122)
By setting ψi(z) := max {dx,y∗(x)
ρ0(si),dx∗(y),y
ρ0(si)}and combining the facts that fi(Q∗,x∗
i,y∗
i)−
min
ui∈Xifi(Q∗,ui,yi)≥0andmax
wi∈Yifi(Q∗,xi,wi)−fi(Q∗,x∗
i,y∗
i)≥0derived from Corollary
C.6, we have
Jx,y∗(x)(ρ0)−Jx∗(y),y(ρ0)≤dX
i=1ψi(z)
max
wi∈Yifi(Q∗,xi,wi)−min
ui∈Xifi(Q∗,ui,yi)
.
(123)
C.3.2 Convex-Concave Minimax Problems
In this section, we consider convex-concave minimax problem over compact concave region Z=
X ×Y ⊂ RPd
i=1ni×RPd
i=1miwhich satisfies Assumption C.1 with divergence-generating function
v. The standard convex-concave minimax problem is formulated as follows:
min
x∈Xmax
y∈Yf(x,y), (124)
where fis convex with respect to xand concave with respect to y. Therefore, we obtain that
f(x,y∗(x))−f(x∗(y),y)≤ ⟨∇ xf(z),x−x∗(y)⟩+⟨−∇ yf(z),y−y∗(x)⟩, (125)
for any z= (x,y)∈ Z. We may therefore derive that fsatisfies GQCC condition with g(z)≡1
andf(P(z),z) =f(z). Furthermore, assuming ∇fisL-lipschitz continuous (i.e., ∥∇f(z)−
∇f(v)∥∗≤L∥z−v∥for any z,v∈ Z ) and choosing P≡0, then verifying that fsatisfies
the preconditions of general version of Theorem C.7 is reduced to verifying that fsatisfies (1) in
Assumption C.3. Since F=∇fonly depends on variable z, it is evident that fsatisfies (1) in
Assumption C.3 when ∇fis L-Lipschitz. Therefore, under the smoothness condition of f, Theorem
C.7 implies that O(ε−1)iterations Algorithm 4 needs to find an ε-approximate Nash equilibrium
offmatches the lower bounds of Ω(ε−1)[52] for the number of iterations that any deterministic
first-order method requires to find an ε-approximate Nash equilibrium of a smooth convex-concave
function.
D Auxiliary Lemma
Lemma D.1. ForΓ≥17, the function g(Γ)can be bounded by80640
Γ−1+2
Γe−2−1. Letg(Γ)be defined
asP∞
k=1Γ−k[k7+ (k+ 1) exp {2k}].
37Proof.
g(Γ)≤∞X
k=1"
Γ−k(k+ 7)!
k!+e2
Γk
(k+ 1)#
=d7
dα7α8
1−α
α=Γ−1+d
dαα2
1−α
α=e2Γ−1
≤
(a)80640
Γ−1+2
Γe−2−1, (126)
(a) can be deduced based on the following inequality
d7
dα7α8
1−α
=7X
k=0(−1)k
7
k8!k!
(k+ 1)!α
1−αk+1
≤
(b)7!8X
k=1
8
kα
1−αk
=7!"
1 +α
1−α8
−1#
≤7!
exp8α
1−α
−1
≤
(c)80640 α
1−α, (127)
where (b) and (c) are derived from Leibniz equation, and the inequality ex−1≤2xholds for
0≤x≤1/2respectively.
Lemma D.2. For any n∈N,r∈Rn,p∈∆n, if it holds that p∗= arg min
p∈∆nη⟨p,r⟩+ KL( p∥q),
then we have
⟨p∗−p,r⟩=1
η(KL(p∥q)−KL(p∥p∗)−KL(p∗∥q)). (128)
Proof. We just need to prove p∗(i)≡p′(i) :=q(i) exp{−ηr(i)}Pn
j=1q(j) exp{−ηr(j)}for any i∈[n]which satisfies
⟨p−p′, ηr+ log( p′)−log(q)⟩= 0, (129)
for any p∈∆n. Assume that F(p) :=η⟨p,r⟩+ KL( p∥q)and define E(p) =Pn
i=1p(i) log(p(i))
for any p∈∆n. Clearly, p′∈∆n. Hence, for all p∈∆n,
F(p) =η⟨p,r⟩+ KL( p∥q)
=η⟨p′,r⟩+ KL( p′∥q) +⟨p−p′, ηr−log(q)⟩+E(p)− E(p′)
=
(a)η⟨p′,r⟩+ KL( p′∥q) +E(p)− E(p′) +⟨p−p′,−log(p′)⟩
=
(b)F(p′)−KL(p∥p′), (130)
where (a) is derived from Eq. (129) . Therefore, we obtain that p∗≡p′. By using equality (b), we
finish the proof.
Lemma D.3. Suppose that for τ∈(0,1), we havep
q
∞≤1 +τ. Then
1−τ
2−2τ
3(1−τ)
X2(p,q)≤KL(p∥q).
38Proof. We consider the Taylor expansion of the function log(1 + x) =P∞
k=1(−1)k−1
kxkand define
Qτ,D(x) :=x− 1
2+Dτ
x2. According to
log(1 + x)−Qτ,D(x)≥Dτx2−|x|3
3(1−τ), (131)
for any x∈[−τ, τ], we have log(1 + x)≥Qτ,D(x)when D≥1
3(1−τ)andx∈[−τ, τ]. Therefore,
we obtain
KL(p∥q) =nX
j=1p(j) logp(j)
q(j)
≥nX
j=1p(j)"p(j)
q(j)−1
−1
2+Dτp(j)
q(j)−12#
=X2(p,q)−1
2+DτnX
j=1p(j)
q(j)q(j)p(j)
q(j)−12
≥X2(p,q)−1 +τ
2+Dτ(1 +τ)
X2(p,q)
=1−τ
2−Dτ(1 +τ)
X2(p,q). (132)
We complete the proof if D=1
3(1−τ).
Lemma D.4. Suppose that r∈Rn, τ∈(0,1/2),∥r∥∞≤τ
2,andp,˜p∈∆nsatisfy, for each
j∈[n],
˜p(j) =p(j)·exp{r(j)}P
j′∈[n]p(j′)·exp{r(j′)}. (133)
Then
1−2
3(1−τ)+ 4
τ
Varp(r)≤ X2(˜p,p)≤
1 +2
3(1−τ)+ 4
τ
Varp(r).(134)
Proof. Without loss of generality, we consider the case ⟨p,r⟩= 0. If not, redefine ˜r:=r− ⟨p,r⟩ ·
e(∥˜r∥∞≤τ)and analyze ˜rwhere e∈Rnis an all 1 vector. It’s clear that
X2(˜p,p) =−1 +nX
j=1p(j)˜p(j)
p(j)2
=−1 +Epexp{r}
Ep[exp{r}]2
. (135)
We define F1
D(x) := 1 + x+1−Dτ
2x2, F2
D(x) := 1 + x+1+Dτ
2x2and note that for any x∈[−τ, τ]
exp{x} −F1
D(x)≥Dτ
2x2−x3
6, (136)
F2
D(x)−exp{x} ≥Dτ
2x2−|x|3
6(1−τ), (137)
where Eq. (136) is derived from the summation of the 2k-th and 2k+ 1-th (k≥2) terms in the Taylor
expansion of exp{x}is always non-negative, Eq. (137) is derived fromP∞
k=3xk
k!≤|x|3
6(1−x)≤|x|3
6(1−τ)
for any x∈[−τ, τ]. Therefore, we have exp{x} −F1
D(x)≥0andF2
D(x)−exp{x} ≥0for all
x∈[−τ, τ]ifD≥1
3(1−τ). Then, we have
1 + 2 x+ (2−(D+ 2)τ)x2≤(exp{x})2≤1 + 2 x+ (2 + ( D+ 2)τ)x2, (138)
when Dτ≤1
2. In addition, by ⟨p,r⟩= 0, it’s obvious that
1 +1−Dτ
2Ep[r2]≤Ep[exp{r}]≤1 +1 +Dτ
2Ep[r2]. (139)
39Combining Eq. (138) and (139), we derived that
1 + (1 −(D+ 1)τ)Ep[r2]≤(Ep[exp{r}])2≤1 + (1 + ( D+ 1)τ)Ep[r2], (140)
1 + (2 −(D+ 2)τ)Ep[r2]≤Ep
(exp{r})2
≤1 + (2 + ( D+ 2)τ)Ep[r2], (141)
forDτ≤1
2. According to Eq. (135) ,(140) and (141), we have
−1 +Epexp{r}
Ep[exp{r}]2
≥(1−(2D+ 3)τ)Ep[r2]
1 + (1 + ( D+ 1)τ)Ep[r2]≥(1−(2D+ 4)τ)Ep[r2],
−1 +Epexp{r}
Ep[exp{r}]2
≤(1 + (2 D+ 3)τ)Ep[r2]
1 + (1 −(D+ 1)τ)Ep[r2]≤(1−(2D+ 4)τ)Ep[r2].
We derive Eq. (134) by setting D=1
3(1−τ).
Lemma D.5 (Lemma B.6, [15]) .Letϕ1,···, ϕlbe softmax-type functions.
ϕi(x) =exp{x(ji)}Pn
k=1τikexp{x(k)}, (142)
where ji∈[1,···, n],Pn
k=1τik= 1for any i∈[1,···, l]. LetP(x) =P
k=0P
|α|=kDαP(0)
α!xα
denote the Taylor series ofQl
i=1ϕi. Then for any integer k,
X
|α|=k|DαP(0)|
α!≤(e3l)k. (143)
We introduce the conception of (Q, R)-bounded function briefly. Suppose ϕ:Rn→Ris real-
analytic in a neighborhood of the origin. For real numbers Q, R > 0, we say that ϕis(Q, R)-bounded
if the Taylor expansion of ϕat0, denoted Pϕ(x) =P∞
k=0P
|α|=kDαf(0)
α!xα, satisfies, for each
integer i≥0,P
|α|=k|Dαϕ(0)|
α!≤Q·Rk.
Lemma D.6 (Detailed version of Lemma 4.5, [ 15]).Suppose that h, n∈N, ϕ:Rn→Ris a
(Q, R)-bounded function such that the radius of convergence of its power series at 0is at least
ν > 0, and Z={Z0,···,ZT} ⊂Rnis a sequence of vectors satisfyingZt
∞≤νfor
t∈[0,···, T]. Suppose for some β∈(0,1), for each 0≤h′≤handt∈[0,···, T−h′], it holds
thatDh′Zt
∞≤1
ΓRβh′(h′)Bh′for some B≥3,Γ≥e3. Then for all t∈[0,···, T−h],(Dh(ϕ◦Z))t≤Q·g(Γ)·βhhBh+1, (144)
where g(Γ)is a bounded function with respect to Γ.
Proof. Without loss of generality, we assume ϕ(0) = 0 . We define (ϕ◦Z)t=P
γ∈Zn
≥0:|γ|=kaγ 
Ztγand obtain
(Dh(ϕ◦Z))t=∞X
k=1X
γ∈Zn
≥0:|γ|=kaγ(DhZγ)t
≤∞X
k=1X
γ∈Zn
≥0:|γ|=k|aγ|
X
x:[h]→[k]kY
j=1
Et′
x,jDh′
x,jZ(l′
x,j)t

≤∞X
k=1X
γ∈Zn
≥0:|γ|=k|aγ| ·βh
(ΓR)k·
X
x:[h]→[k]kY
j=1(h′
x,j)Bh′
x,j

≤∞X
k=1X
γ∈Zn
≥0:|γ|=k|aγ| ·βh
(ΓR)khBhmax
k7,(hk+ 1) exp2k
hB−1
≤
(c)∞X
k=1QR
ΓRk
·max
k7,(k+ 1) exp {2k}	
·βhhBh+1
≤Q·g(Γ)·βhhBh+1, (145)
40where (c) is derived from (Q, R)-bounded condition.
Lemma D.7 (Lemma C.4, [ 15]).Let{n, T} ⊂Z+withn≥2andT≥4, we select H:=
⌈log(T)⌉, β0=1
4H, and β=√
β0/8
H3. Assume that {zt}T
t=1⊂[0,1]nand{pt}T
t=1⊂∆nsatisfy the
following condition
1. For each 0≤h≤Hand1≤t≤T−h, it holds that ∥(Dhz)t∥∞≤βhH3h+1.
2. The sequence {pt}T
t=1isζ−consecutively close for some ζ∈
(2T)−1, β4
0/8256
.
Then, we have
TX
t=1Varpt(zt−zt−1)≤2β0TX
t=1Varpt(zt−1) + 165120(1 + ζ)H5+ 2. (146)
Proposition D.8. Given a constant c >0, we have
tX
k=1c
c+k2
≤c. (147)
Lemma D.9. For a constant c≥c′>0, the following inequality holds
c′logc+t−1
c+T
−(c′+c′c)2
2c≤TX
k=tlog
1−c′
c+k
≤c′logc+t
c+ 1 + T
, (148)
when T > t ≥1.
Proof. Accoding to the Taylor expansion of log(1−x)when x <1, we obtain the estimation of
log
1−c′
c+k
for any k≥1as follows
log
1−c′
c+k
≤ −c′
c+k, (149)
log
1−c′
c+k
≥ −c′
c+k−(c′+cc′)2
21
c+k2
. (150)
Next, we have
TX
k=t−c′
c+k≤ −ZT+1
tc′
c+xdx=c′logc+t
c+ 1 + T
, (151)
TX
k=t"
−c′
c+k−(c′+cc′)2
21
c+k2#
≥ −ZT
t−1"
c′
c+x+(c′+cc′)2
21
c+x2#
dx
≥c′logc+t−1
c+T
−(c′+cc′)2
2c. (152)
E Limitation
For objectives with GQC condition (GQCC condition) and general smooth internal function (i.e.
Lipschitz continuous internal function), our analytical method might not provide similar iteration
complexity. We leave the related algorithmic analysis on more generalized smoothness conditions as
a future work.
41NeurIPS Paper Checklist
1.Claims
Question: Do the main claims made in the abstract and introduction accurately reflect the
paper’s contributions and scope?
Answer: YES
Justification: We have a detailed explanation in the contribution section of the introduction.
Guidelines:
•The answer NA means that the abstract and introduction do not include the claims
made in the paper.
•The abstract and/or introduction should clearly state the claims made, including the
contributions made in the paper and important assumptions and limitations. A No or
NA answer to this question will not be perceived well by the reviewers.
•The claims made should match theoretical and experimental results, and reflect how
much the results can be expected to generalize to other settings.
•It is fine to include aspirational goals as motivation as long as it is clear that these goals
are not attained by the paper.
2.Limitations
Question: Does the paper discuss the limitations of the work performed by the authors?
Answer: YES
Justification: See Section E in appendix.
Guidelines:
•The answer NA means that the paper has no limitation while the answer No means that
the paper has limitations, but those are not discussed in the paper.
• The authors are encouraged to create a separate "Limitations" section in their paper.
•The paper should point out any strong assumptions and how robust the results are to
violations of these assumptions (e.g., independence assumptions, noiseless settings,
model well-specification, asymptotic approximations only holding locally). The authors
should reflect on how these assumptions might be violated in practice and what the
implications would be.
•The authors should reflect on the scope of the claims made, e.g., if the approach was
only tested on a few datasets or with a few runs. In general, empirical results often
depend on implicit assumptions, which should be articulated.
•The authors should reflect on the factors that influence the performance of the approach.
For example, a facial recognition algorithm may perform poorly when image resolution
is low or images are taken in low lighting. Or a speech-to-text system might not be
used reliably to provide closed captions for online lectures because it fails to handle
technical jargon.
•The authors should discuss the computational efficiency of the proposed algorithms
and how they scale with dataset size.
•If applicable, the authors should discuss possible limitations of their approach to
address problems of privacy and fairness.
•While the authors might fear that complete honesty about limitations might be used by
reviewers as grounds for rejection, a worse outcome might be that reviewers discover
limitations that aren’t acknowledged in the paper. The authors should use their best
judgment and recognize that individual actions in favor of transparency play an impor-
tant role in developing norms that preserve the integrity of the community. Reviewers
will be specifically instructed to not penalize honesty concerning limitations.
3.Theory Assumptions and Proofs
Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?
Answer: YES
42Justification: We provide the assumptions and the associated theoretical results in Section 3
and Section 4, respectively.
Guidelines:
• The answer NA means that the paper does not include theoretical results.
•All the theorems, formulas, and proofs in the paper should be numbered and cross-
referenced.
•All assumptions should be clearly stated or referenced in the statement of any theorems.
•The proofs can either appear in the main paper or the supplemental material, but if
they appear in the supplemental material, the authors are encouraged to provide a short
proof sketch to provide intuition.
•Inversely, any informal proof provided in the core of the paper should be complemented
by formal proofs provided in appendix or supplemental material.
• Theorems and Lemmas that the proof relies upon should be properly referenced.
4.Experimental Result Reproducibility
Question: Does the paper fully disclose all the information needed to reproduce the main ex-
perimental results of the paper to the extent that it affects the main claims and/or conclusions
of the paper (regardless of whether the code and data are provided or not)?
Answer: NA
Justification:
Guidelines:
• The answer NA means that the paper does not include experiments.
•If the paper includes experiments, a No answer to this question will not be perceived
well by the reviewers: Making the paper reproducible is important, regardless of
whether the code and data are provided or not.
•If the contribution is a dataset and/or model, the authors should describe the steps taken
to make their results reproducible or verifiable.
•Depending on the contribution, reproducibility can be accomplished in various ways.
For example, if the contribution is a novel architecture, describing the architecture fully
might suffice, or if the contribution is a specific model and empirical evaluation, it may
be necessary to either make it possible for others to replicate the model with the same
dataset, or provide access to the model. In general. releasing code and data is often
one good way to accomplish this, but reproducibility can also be provided via detailed
instructions for how to replicate the results, access to a hosted model (e.g., in the case
of a large language model), releasing of a model checkpoint, or other means that are
appropriate to the research performed.
•While NeurIPS does not require releasing code, the conference does require all submis-
sions to provide some reasonable avenue for reproducibility, which may depend on the
nature of the contribution. For example
(a)If the contribution is primarily a new algorithm, the paper should make it clear how
to reproduce that algorithm.
(b)If the contribution is primarily a new model architecture, the paper should describe
the architecture clearly and fully.
(c)If the contribution is a new model (e.g., a large language model), then there should
either be a way to access this model for reproducing the results or a way to reproduce
the model (e.g., with an open-source dataset or instructions for how to construct
the dataset).
(d)We recognize that reproducibility may be tricky in some cases, in which case
authors are welcome to describe the particular way they provide for reproducibility.
In the case of closed-source models, it may be that access to the model is limited in
some way (e.g., to registered users), but it should be possible for other researchers
to have some path to reproducing or verifying the results.
5.Open access to data and code
Question: Does the paper provide open access to the data and code, with sufficient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?
43Answer: NA
Justification:
Guidelines:
• The answer NA means that paper does not include experiments requiring code.
•Please see the NeurIPS code and data submission guidelines ( https://nips.cc/
public/guides/CodeSubmissionPolicy ) for more details.
•While we encourage the release of code and data, we understand that this might not be
possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
including code, unless this is central to the contribution (e.g., for a new open-source
benchmark).
•The instructions should contain the exact command and environment needed to run to
reproduce the results. See the NeurIPS code and data submission guidelines ( https:
//nips.cc/public/guides/CodeSubmissionPolicy ) for more details.
•The authors should provide instructions on data access and preparation, including how
to access the raw data, preprocessed data, intermediate data, and generated data, etc.
•The authors should provide scripts to reproduce all experimental results for the new
proposed method and baselines. If only a subset of experiments are reproducible, they
should state which ones are omitted from the script and why.
•At submission time, to preserve anonymity, the authors should release anonymized
versions (if applicable).
•Providing as much information as possible in supplemental material (appended to the
paper) is recommended, but including URLs to data and code is permitted.
6.Experimental Setting/Details
Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?
Answer: NA
Justification:
Guidelines:
• The answer NA means that the paper does not include experiments.
•The experimental setting should be presented in the core of the paper to a level of detail
that is necessary to appreciate the results and make sense of them.
•The full details can be provided either with the code, in appendix, or as supplemental
material.
7.Experiment Statistical Significance
Question: Does the paper report error bars suitably and correctly defined or other appropriate
information about the statistical significance of the experiments?
Answer: NA
Justification:
Guidelines:
• The answer NA means that the paper does not include experiments.
•The authors should answer "Yes" if the results are accompanied by error bars, confi-
dence intervals, or statistical significance tests, at least for the experiments that support
the main claims of the paper.
•The factors of variability that the error bars are capturing should be clearly stated (for
example, train/test split, initialization, random drawing of some parameter, or overall
run with given experimental conditions).
•The method for calculating the error bars should be explained (closed form formula,
call to a library function, bootstrap, etc.)
• The assumptions made should be given (e.g., Normally distributed errors).
•It should be clear whether the error bar is the standard deviation or the standard error
of the mean.
44•It is OK to report 1-sigma error bars, but one should state it. The authors should
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
of Normality of errors is not verified.
•For asymmetric distributions, the authors should be careful not to show in tables or
figures symmetric error bars that would yield results that are out of range (e.g. negative
error rates).
•If error bars are reported in tables or plots, The authors should explain in the text how
they were calculated and reference the corresponding figures or tables in the text.
8.Experiments Compute Resources
Question: For each experiment, does the paper provide sufficient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?
Answer: NA
Justification:
Guidelines:
• The answer NA means that the paper does not include experiments.
•The paper should indicate the type of compute workers CPU or GPU, internal cluster,
or cloud provider, including relevant memory and storage.
•The paper should provide the amount of compute required for each of the individual
experimental runs as well as estimate the total compute.
•The paper should disclose whether the full research project required more compute
than the experiments reported in the paper (e.g., preliminary or failed experiments that
didn’t make it into the paper).
9.Code Of Ethics
Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines ?
Answer: YES
Justification:
Guidelines:
•The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
•If the authors answer No, they should explain the special circumstances that require a
deviation from the Code of Ethics.
•The authors should make sure to preserve anonymity (e.g., if there is a special consid-
eration due to laws or regulations in their jurisdiction).
10.Broader Impacts
Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?
Answer: NA
Justification: Since this paper is a theoretical paper, it may not have other social impacts.
Guidelines:
• The answer NA means that there is no societal impact of the work performed.
•If the authors answer NA or No, they should explain why their work has no societal
impact or why the paper does not address societal impact.
•Examples of negative societal impacts include potential malicious or unintended uses
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
(e.g., deployment of technologies that could make decisions that unfairly impact specific
groups), privacy considerations, and security considerations.
•The conference expects that many papers will be foundational research and not tied
to particular applications, let alone deployments. However, if there is a direct path to
any negative applications, the authors should point it out. For example, it is legitimate
to point out that an improvement in the quality of generative models could be used to
45generate deepfakes for disinformation. On the other hand, it is not needed to point out
that a generic algorithm for optimizing neural networks could enable people to train
models that generate Deepfakes faster.
•The authors should consider possible harms that could arise when the technology is
being used as intended and functioning correctly, harms that could arise when the
technology is being used as intended but gives incorrect results, and harms following
from (intentional or unintentional) misuse of the technology.
•If there are negative societal impacts, the authors could also discuss possible mitigation
strategies (e.g., gated release of models, providing defenses in addition to attacks,
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
feedback over time, improving the efficiency and accessibility of ML).
11.Safeguards
Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?
Answer: NA
Justification:
Guidelines:
• The answer NA means that the paper poses no such risks.
•Released models that have a high risk for misuse or dual-use should be released with
necessary safeguards to allow for controlled use of the model, for example by requiring
that users adhere to usage guidelines or restrictions to access the model or implementing
safety filters.
•Datasets that have been scraped from the Internet could pose safety risks. The authors
should describe how they avoided releasing unsafe images.
•We recognize that providing effective safeguards is challenging, and many papers do
not require this, but we encourage authors to take this into account and make a best
faith effort.
12.Licenses for existing assets
Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?
Answer: NA
Justification:
Guidelines:
• The answer NA means that the paper does not use existing assets.
• The authors should cite the original paper that produced the code package or dataset.
•The authors should state which version of the asset is used and, if possible, include a
URL.
• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
•For scraped data from a particular source (e.g., website), the copyright and terms of
service of that source should be provided.
•If assets are released, the license, copyright information, and terms of use in the package
should be provided. For popular datasets, paperswithcode.com/datasets has
curated licenses for some datasets. Their licensing guide can help determine the license
of a dataset.
•For existing datasets that are re-packaged, both the original license and the license of
the derived asset (if it has changed) should be provided.
•If this information is not available online, the authors are encouraged to reach out to
the asset’s creators.
13.New Assets
Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?
46Answer: NA
Justification:
Guidelines:
• The answer NA means that the paper does not release new assets.
•Researchers should communicate the details of the dataset/code/model as part of their
submissions via structured templates. This includes details about training, license,
limitations, etc.
•The paper should discuss whether and how consent was obtained from people whose
asset is used.
•At submission time, remember to anonymize your assets (if applicable). You can either
create an anonymized URL or include an anonymized zip file.
14.Crowdsourcing and Research with Human Subjects
Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as
well as details about compensation (if any)?
Answer: NA
Justification:
Guidelines:
•The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
•Including this information in the supplemental material is fine, but if the main contribu-
tion of the paper involves human subjects, then as much detail as possible should be
included in the main paper.
•According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
or other labor should be paid at least the minimum wage in the country of the data
collector.
15.Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects
Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?
Answer: NA
Justification:
Guidelines:
•The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
•Depending on the country in which research is conducted, IRB approval (or equivalent)
may be required for any human subjects research. If you obtained IRB approval, you
should clearly state this in the paper.
•We recognize that the procedures for this may vary significantly between institutions
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
guidelines for their institution.
•For initial submissions, do not include any information that would break anonymity (if
applicable), such as the institution conducting the review.
47