Published in Transactions on Machine Learning Research (07/2023)
Stochasticgradientupdatesyielddeepequilibriumkernels
Russell Tsuchida russell.tsuchida@data61.csiro.au
Data61, CSIRO
Cheng Soon Ong chengsoon.ong@anu.edu.au
Data61, CSIRO
Australian National University
Reviewed on OpenReview: https: // openreview. net/ forum? id= p7UTv2hWgM
Abstract
Implicit deep learning allows one to compute with implicitly defined features, for exam-
ple features that solve optimisation problems. We consider the problem of computing
with implicitly defined features in a kernel regime. We call such a kernel a deep equilib-
rium kernel ( DEKer). Specialising on a stochastic gradient descent (SGD) update rule
applied to features (not weights) in a latent variable model, we find an exact determin-
istic update rule for the DEKer in a high dimensional limit. This derived update rule
resembles previously introduced infinitely wide neural network kernels. To perform our
analysis, we describe an alternative parameterisation of the link function of exponen-
tial families, a result that may be of independent interest. This new parameterisation
allows us to draw new connections between a statistician’s inverse link function and a
machine learner’s activation function. We describe an interesting property of SGD in
this high dimensional limit: even though individual iterates are random vectors, inner
products of any two iterates are deterministic, and can converge to a unique fixed point
as the number of iterates increases. We find that the DEKer empirically outperforms
related neural network kernels on a series of benchmarks.1
1 Kernel methods, deep learning and implicit deep learning
Kernel methods are a classical paradigm for analysing representational capacity, bias, generalisation
performance and practical algorithms for nonparametric prediction (Schölkopf et al., 2002). Many
classical nonparametric models can be seen as extensions of parametric models (Saunders, 1998; Ras-
mussen & Williams, 2006, § 2.2) that allow for increased representational capacity while retaining some
statistical model-based properties. Examples of model-based qualities may include the smoothness,
stationarity or periodicity of the predictor (Duvenaud, 2014, § 2) or the statistical interpretation of the
learning procedure (Sollich, 2002; Rasmussen & Williams,§ 3), which may be understood by examining
the kernel or the loss function (Banerjee et al., 2005, Theorem 4).
Despite early successes of kernel methods, when data is plentiful and/or modelling is hard, over-
parameterised and under-regularised deep learning is now seen as the dominant paradigm for practical
nonparametric-style prediction (OpenAI et al., 2019; Adiwardana et al., 2020; Rombach et al., 2022).
Unlike parametric and classical nonparametric approaches, the architecture and loss functions of many
explicit neural networks are driven purely from the perspective of representational power or predictive
performance (either empirical (Vaswani et al., 2017) or mathematical (Raghu et al., 2017)) rather than
model-based qualities.
A fruitful direction is to analyse deep learning predictors through the reductionist lens of kernel methods
through sufficiently well-behaved neural networks in certain large parameter count regimes (Neal, 1995).
1Code available at https://github.com/RussellTsuchida/dek.git .
1Published in Transactions on Machine Learning Research (07/2023)
Deep
learningKernel
methods
Implicit
functionsNeal (1995)
Lee et al. (2018)
Jacot et al. (2018)This paper
Chen et al. (2018)
Bai et al. (2019)
Gould et al. (2021)
Figure 1: We establish links between kernel methods and implicit functions to design a kernel with
corresponding statistical assumptions about a latent variable model.
However, to the best of our knowledge, no current theory describes architectural properties of neural
networks in the kernel regime such as choice of activation function, depth and skip connections, in
terms of model-based properties. It is desirable to motivate predictive deep learning architectures from
a more fundamental, statistical model-based perspective (Rudin, 2019; Efron, 2020) in a kernel regime.
Implicit neural networks are an emerging approach to model-based deep learning. We describe such net-
works as model-based because the layers are defined and guaranteed to implicitly satisfy the solution to
aproblemarisingfromamodelthatisnotonlypurelypredictive, butalsoconceptuallyelegant. Implicit
networks use solutions to problems as feature representations. For example, deep declarative networks
(DDNs) (Gould et al., 2021) solve optimisation problems, deep equilibrium models (DEQs) (Bai et al.,
2019) solve fixed point (algebraic) problems and neural ODEs solve differential equations (Chen et al.,
2018). Example applications of implicit neural network layers include layers that model optimal trans-
port layers (Campbell et al., 2020; Eisenberger et al., 2022) and layers that perform point estimation
of specific statistical models (Tsuchida et al., 2022; Tsuchida & Ong, 2023). Such problems are usu-
ally computed numerically via the (approximate) fixed point of an iterative procedure. This leads to
the view that implicit layers are themselves a composition of infinitely many functions. Owing to the
complexity of deep learning algorithms, theory falls short of explaining the empirically demonstrated
successes of both implicit and explicit models. To the best of our knowledge, no general notion of an
implicit kernel is currently described in the literature.
We define a new type of kernel called a deep equilibrium kernel. This kernel is defined as the inner
product of features, where features are taken to be solutions to a given problem depending on inputs,
using a given algorithm, at a given iteration of the algorithm. We focus on the special case where the
problem is an optimisation problem for point estimates of a particular latent variable model and the
algorithm is stochastic gradient descent. We find that in the limit as the dimensionality of the features
goes to infinity, the deep equilibrium kernel at any iterate can be represented as a function of the deep
equilibrium kernel at the previous iterate. We describe our contributions in more detail in § 1.1. Our
analysis brings together elements of implicit functions, kernel methods, and deep learning (Figure 1).
1.1 Our contributions: an implicit kernel and an update rule in kernel space
Updatesinfeaturespace Solutionstooptimisation, fixedpointordifferentialequationproblemsare
in practice most often obtained via a possibly stochastic iterative update procedure. Let X1∈X⊆Rl
be an input to the problem and ψX1∈ψψ⊆Rmbe the solution to the problem. Note that ψX1is
the evaluation of an implicit function of X1. The function is implicit because the mapping is defined
through the solution to a problem, rather than an explicit closed-form expression. Let ψ(0)
X1be an initial
guess for solution at iterate 0. Suppose there exists some possibly stochastic function g(t)(·;X) :ψψ→
ψψthat maps solutions at iterate tto solutions at iterate t+ 1, so that
ψ(t+1)
X =g(t)/parenleftbig
ψ(t)
X;X/parenrightbig
. (1)
2Published in Transactions on Machine Learning Research (07/2023)
We callψ(t)
Xfeatures and g(t)the update rule in feature space. We emphasise that we consider the
problem where features are updated, not weight parameters as in some other settings. This is unlike
typical settings for the neural tangent kernel (Jacot et al., 2018), where weight and parameter updates
are performed for a fixed depth.
Deep equilibrium kernels We find helpful the notion of an implicitly defined kernel, which we
call a deep equilibrium kernel ( DEKer). This allows us to draw parallels between infinitely wide
implicit neural networks and implicitly defined kernel machines. Let ψ(t+1)
X1andψ(t+1)
X2be two features
corresponding to inputs X1andX2. Recall that mis the dimension of the feature mapping and tis the
iteration of the solver. We consider three kernel evaluations in terms of the implicit updates in feature
space,
Ψ(t+1)
12≜ψ(t+1)
X1⊤ψ(t+1)
X2,
/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
finite feature DEKer(ffDEKer)Ψ(t+1)
12≜plim
m→∞Ψ(t+1)
12,
/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
DEKerand Ψ12≜lim
τ→∞Ψ(τ)
12,
/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
limiting DEKer(ℓDEKer)(2)
wheredefined, where plimdenotesconvergenceinprobability. Notetheorderofthelimits. Wewillsimi-
larly write Ψ11andΨ22to represent evaluations of such DEKers at(X1,X1)and(X2,X2)respectively.
We write
Ψ(t+1)=/parenleftigg
Ψ(t+1)
11 Ψ(t+1)
12
Ψ(t+1)
21 Ψ(t+1)
22/parenrightigg
for the corresponding 2×2PSD matrices (and likewise for the ff DEKer andℓDEKer). The dimen-
sionality of features are allowed to grow to infinity, but only after taking the inner product, resulting
in a scalar value for examination. The mathematical construction of 2×2matrices suffices for our
purposes to analyse algorithms that utilise NexamplesX1,...,XN, since every element of an N×N
kernel matrix is an element of a corresponding 2×2matrix. Defining such a kernel allows one to build
predictive algorithms that operate on kernel matrices instead of feature space, avoiding the necessity of
describing, analysing and building algorithms involving infinite dimensional feature spaces.
Updates in kernel space LetS2
+={K∈R2×2|K=K⊤,K⪰0}denote the space of
2×2PSD matrices. Our central questions are as follows. Firstly, as m→∞, does there exist
an update that may be performed on 2×2PSDDEKer matrices instead of 2m-dimensional
feature space? Secondly, can we write a closed form for the update? Finally, do repeated
iterations of the update converge? That is, does there exist a closed-form update rule in kernel
space G(·;X1,X2) :S2
+→S2
+such that
Ψ(t+1)=G(Ψ(t);X1,X2) ? (3)
And does Ψ=G(Ψ) = lim
τ→∞G◦...◦G/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
τcompositions(Ψ(0);X1,X2) ? (4)
For convenience, we notationally decompose Ginto components via a function Gsatisfying for each
ij∈{11,22,12}
G(Φ;X1,X2) =/parenleftbiggG11G12
G12G22/parenrightbigg
,where
Gij≜G(Φii,Φjj,Φij;Xi,Xj)≜/parenleftig
G(Φ;X1,X2)/parenrightig
ij.(5)
Here Φ∈S2
+is a local dummy variable whose sole purpose is to define GandG.
Contributions We study an important special case of a DEKer where we answer equation 3
and equation 4 positively, one in which the features are iteratively updated using SGD applied to
a latent variable model. The model is over-parameterised (the number of parameters grows much faster
than the amount of data), but shallow (the motivation for the model more closely resembles exponential
3Published in Transactions on Machine Learning Research (07/2023)
family PCA than a deep neural network). The objective to which we apply SGD is an under-regularised
variant of an expected negative log posterior for point estimates of latent variables. Our main result
(stated precisely in § 3) is a constructive proof of the existence of a closed form update rule in kernel
space G.
Surprisingly, despite the feature space of the DEKer having seemingly no direct relation with deep
learning predictors, deep learning structures emerge as part of our analysis. Our DEKer is a kernel
which satisfies a fixed point equation, constructed via a latent variable model optimisation problem
solved using SGD. Our DEKer may be understood (but is not constructed) as an infinitely wide DEQ
whose iterates are computed with stochastic mapping that is resampled at every iterate rather than as
a deterministic fixed point iteration. Further, the kernel iterates of our DEKer resemble previously
introduced NNKs and NTKs and may be computed with deterministic fixed point solvers.
Informally, our main result (Theorem 4) states that when the features ψ(t)
X1andψ(t)
X2are point estimates
obtained by SGD, we can construct a deterministic update rule, G, for the DEKer,Ψ(t+1). Further,
Corollary 5 says that repeated applications of Gconverge to a fixed point, the ℓDEKer. We further
quantify the degree to which the ℓDEKer is an invariant of SGD (i.e. does not change as iterates of
SGD increase) when treated as an ff DEKer (Theorem 7).
2 Background
Our analysis requires combining fixed point theory (optimisation), the exponential family (statistics),
and neural network kernels (machine learning). We briefly describe elements of these topics here.
2.1 Notation
Numerical subscripts are used to extract (groups of) indices of a vector or matrix. Parenthesised
superscripts indicate a layer or iteration of a naive fixed point solver, both of which turn out to be the
same in our constructions, as is consistent with other DEQ works. We index objects by iteration by
superscript (t), so thatψ(t)represents a feature in the tth iteration. Sans serif fonts are used to denote
matrices, and serif fonts are used to denote vectors (so Wis a matrix and Wis a vector). We use ϕ
andΦfor arbitrary vectors and inner products that are not necessarily obtained by iterations of SGD.
We will use ψandΨfor feature mappings and inner products of feature mappings that are obtained
by iterations of SGD.
We assume that we are given access to a dataset X∈RN×lofNexamples of datapoints Xi∈X⊆Rl.
We denote by X1andX2any two elements of this dataset.
There are two types of function signatures we associate with PSD kernels. The first is for a usual
PSD kernel k:X×X→R, so that an evaluation is written k(X,X′)for any two X,X′∈X. We
call this form a k-form kernel. The second is for a PSD kernel whose evaluation depends on ϕ1,ϕ2∈
ψψonly through evaluations Φ11=⟨ϕ1,ϕ1⟩,Φ12=⟨ϕ1,ϕ2⟩,Φ22=⟨ϕ2,ϕ2⟩of some suitably defined
inner product⟨·,·⟩:ψψ×ψψ→ΨΨ. We represent such a kernel through κ:ΨΨ3→Rwith evaluations
κ/parenleftbig
⟨ϕ1,ϕ1⟩,⟨ϕ2,ϕ2⟩,⟨ϕ1,ϕ2⟩/parenrightbig
. An example of this second form is the NNK (equation 11). We call this
form aκ-form kernel.
Our notation is summarised in Table 3 in Appendix A.
2.2 Fixed points and infinite compositions
Letf:F→Ffor some set Fequipped with a norm ∥·∥and norm-induced metric. A fixed point of
fis anyZ∗∈FsatisfyingZ∗=f(Z∗). Banach’s fixed point theorem (BFPT) (Goebel & Kirk, 1990,
Theorem 2.1) gives sufficient conditions for the existence and uniqueness of such a fixed point.
Theorem 1 (BFPT).Let(F,∥·∥)be a non-empty complete normed space. A mapping f:F→Fis
called a contraction mapping if there exists some q∈[0,1)such that∥f(Z)−f(Z′)∥≤q∥Z−Z′∥for
4Published in Transactions on Machine Learning Research (07/2023)
everyZ,Z′∈F. Every contraction mapping fadmits a unique fixed point Z∗∈F. Furthermore, for
any initial element Z(1)∈F, the sequence Z(t+1)=f(Z(t))fort≥1converges to Z∗ast→∞.
It is worth noting that BFPT not only provides a mathematical condition for well-posedness, but also
describes an algorithm for approximating fixed points of contraction mappings. We call this algorithm
thenaive fixed point solver , which simply involves applying a τ-fold composition of fto some starting
valueZ(1), with a linear rate of convergence immediate from the definition of contraction mapping, i.e.
∥Z∗−Z(t+1)∥≤qt
1−q∥Z(2)−Z(1)∥. Other solvers for fixed point problems are available, many of which
are approximate Newton methods for root finding (Kelley, 1995).
Deep equilibrium models (DEQs) (Bai et al., 2019) are neural network predictors constructed of pa-
rameterised layers that output the solution to fixed point equations Z∗=fU(Z∗), where Uis a general
parameter object. These layers draw upon earlier works on recurrent backpropagation (Pineda, 1987;
Almeida, 1990), leveraging the modern machinery of deep learning architectures, optimisers and heuris-
tics. The unsupervised learning problem for a DEQ, an example of which is considered by (Tsuchida &
Ong, 2023), is
min
UN/summationdisplay
i=1L/parenleftbig
Xi,Z∗
i,U/parenrightbig
/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
Empirical risk minimisation for parameters Usubject to Z∗
i=fU(Z∗
i,Xi),
/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
Fixed point solution for DEQ predictions Z∗
i
whereLis some loss function, Uis a parameter object, and {Xi}N
i=1is a collection of input examples
(a supervised setting might also involve a set of output examples). Derivatives∂Z∗
i
∂Uof outputs of these
layerswithrespecttotheirparameters Ucanbecomputedwithoutbackpropagatingthroughtheiterates
of the fixed point solver using the implicit function theorem (Bai et al., 2019). This allows first-order
stochastic gradient methods that are popular with explicit deep learning architectures to be applied to
DEQs.
In general, it is not guaranteed that a function necessarily admits a unique fixed point; various works
discuss dealing with multiple fixed points or ensuring or encouraging that exactly or at least one fixed
pointexists(Winston&Kolter,2020;Revayetal.,2020;ElGhaouietal.,2021). Interestingly, ifasingle
DEQlayerinvolvesfindingthefixedpointofacontractionmapping, byTheorem1theoutputcomputed
by a DEQ has the interpretation of an infinitely deep neural network with shared parameters in each
layer. More generally, mappings computed by the naive fixed point solver have interpretations as very
deep neural networks with shared parameters in each layer. Since zeros of the gradient of sufficiently
well behaved objectives are stationary points of the objectives, DEQ layers share a connection with
optimisation-based implicit layers (Gould et al., 2021), as explored in various works (Revay et al.,
2020; Xie et al., 2021; Tsuchida et al., 2022; Riccio et al., 2022; Tsuchida & Ong, 2023). Our current
investigation concerns a connection more specific than optimisation, since it considers the special case
of applying SGD.
2.3 Exponential families
Exponential families The feature mappings that we use to build our kernel are estimates obtained
using SGD applied to certain variants of exponential family likelihoods and Gaussian priors. We now
defineminimal and regular exponential families in canonical form . Lethbe a probability density (mass)
function supported on data space Y⊆R. LetT:Y→Rbe a function called the sufficient statistic .
Given some canonical parameter ηbelonging to an open set H⊆R, we may construct a probability
density (mass) function by normalising the nonnegative function h(·) exp/parenleftbig
T(·)η/parenrightbig
. The normalising
constant is called the partition function, and its strictly convex and infinitely differentiable logarithm
Ais called the log partition function (Wainwright et al., 2008, Proposition 3.1). We write
p(y|η) =h(y) exp/parenleftbig
T(y)η−A(η)/parenrightbig
, A (η) = log/integraldisplay
Yh(y) exp/parenleftbig
T(y)η/parenrightbig
dy
5Published in Transactions on Machine Learning Research (07/2023)
for the evaluation of a probability density (mass) function of an exponential family. The log partition
functionAacts as a cumulant generating function for the conditional distribution of the sufficient
statisticT. In particular the expected value of the sufficient statistic (often called the expectation
parameter (Nielsen & Garcia, 2009)) is the gradient of the log partition function A. That is,
E/bracketleftbig
T(y)|η/bracketrightbig
=A′(η). (6)
We consider factorised exponential families in the following sense. Let y1,...ydbe distributed according
to the same exponential family and define data vector Y= (y1,...,yd)⊤and canonical parameter vector
H= (η1,...,ηd). Then the joint distribution of data Yconditioned on canonical parameters His the
product of the individual elements
p(Y|H) =d/productdisplay
i=1p(yi|ηi) =/parenleftigd/productdisplay
r=1h(yr)/parenrightig
exp/parenleftbig
T(Y)⊤H−A(H)⊤1/parenrightbig
, (7)
where we write T(Y) =/parenleftbig
T(y1),...,T (yd)/parenrightbig⊤,A(H) =/parenleftbig
A(η1),...,A (ηd)/parenrightbig⊤and1= (1,..., 1)⊤.
Link functions and canonical link functions Exponential families are used in generalised linear
models (GLMs) (McCullagh & Nelder, 1989). In GLMs, the conditional expectation equation 6 of an
exponential family is set to be the result of applying an (invertible) inverse link function s−1to the
result of a linear transformation of features ϕ∈Rm(classically called parameters). That is, for some
linear basis V∈Rd×m(classically called covariates),
A′(H) =E/bracketleftbig
T(Y)|H/bracketrightbig
=s−1(Vϕ). (8)
The conditional expectation is then mapped to the canonical parameter Hthrough
H= (A′)−1◦s−1(Vϕ), noting that A′is invertible because Ais strictly convex. In the case
wheres−1is chosen to be A′,s≡(A′)−1is called the canonical link function , and we observe
from equation 8 that the canonical parameter and conditional expectation satisfy
H=Vϕ, E[T(Y)|Vϕ] =A′(Vϕ) =s−1(Vϕ). (9)
There are two main and sometimes conflicting reasons why one might be interested in using a non-
canonical link function. The first is computational; if the link function were canonical, for some dis-
tributions such as Gamma or exponential one would need a constrained optimisation method over the
open set Hinstead of R. Ifs−1were allowed to be non-canonical — that is, we are free to choose
s−1different from A′— we could map the conditional expectation to the appropriate constraint set
and unconstrained optimisation procedures could be applied. In a Bayesian context, sampling from the
posterior over ϕcan be made easier by convenient choices of s. For example, the probit model admits
an efficient Gibb’s sampler for the posterior (Albert & Chib, 1993). The second, and arguably more
important consideration is modelling; we might have reason to suspect that the conditional expectation
is constrained. For example, if the observations should have a positive expectation, the power family
of link functions might be used (McCullagh & Nelder, 1989, equation 2.9a). Alternative link functions
can lead to exploiting particular properties of interest; for example, Wiemann et al. (2021) use the
softplus function for positive conditional expectations to exploit its identity-like behaviour at large pos-
itive values. In weighing up the possibly conflicting aims of computational convenience and modelling
suitability, we highlight the view of Efron & Hastie (2021, page 68); while classical exponential families
and link functions may lead to closed-form expressions, modern computer technology allows us more
flexible models.
Point estimation When using a canonical inverse link function s≡A′, the negative logarithm of the
likelihood (equation 7) is strictly convex in H, sinceAis strictly convex and linear functions are convex.
IfHis chosen to be H=Vϕ, this translates to convexity in ϕ, and maximum likelihood estimates can
be computed using first or (more typically, in a classical setting) second order optimisation methods.
Whens−1is not a canonical inverse link function, convexity does not necessarily hold. Nevertheless,
6Published in Transactions on Machine Learning Research (07/2023)
local estimates are practically useful, so pre-implemented link functions and the option to implement
custom link functions is available in a number of software frameworks including R(R Core Team, 2021,
family) and Stata(Hardin & Hilbe, 2018, glm).
2.4 Kernels arising from neural networks
Our main result describes the DEKer update rule as a composite function involving evaluations of
kernels of a particular form. These are kernels that are constructed from neural network models. In
this section, we describe such kernels.
The neural network kernel was first investigated as the covariance function of a certain neural network
with random parameters and a single hidden fully connected layer (Neal, 1995). Under mild conditions,
as the width of the hidden layer goes to infinity the neural network converges to a Gaussian process.
This analysis has since been extended to handle multiple layers (Matthews et al., 2018; Lee et al.,
2018), other layer types including convolutional layers (Mairal et al., 2014; Garriga-Alonso et al., 2018;
Novak et al., 2019; Yang, 2019a;b), and training under gradient flow via the neural tangent kernel
(NTK) (Jacot et al., 2018) . Since our motivation is better described in terms of inner products of
the features, we favour the view of the neural network kernel as an inner product in an infinitely wide
hidden layer rather than a covariance function of a Gaussian process. We note that connections between
Bayesian Gaussian processes and kernel methods exist (Kanagawa et al., 2018) and apply to some but
not all infinitely wide neural networks.
Neural network kernel, single hidden layer LetW(1)∈Rd×nbe the weights of a fully connected
hidden layer with activation function ζdefined over the reals. Suppose each entry of W(1)is i.i.d. with
distributionN(0,1)2. Given an input feature ϕ1∈Rn×1(we take the convention that vectors are
column vectors), the signal in the hidden layer is h(1)≜ζ(W(1)ϕ1)3. Here and throughout the paper
the symbol ≜means that the object on the left hand side is defined to be the expression on the right
hand side. By a strong law of large numbers, a suitably normalised inner product in the hidden layer
converges almost surely as d→∞to an expectation,
1
dh(1)
1⊤h(1)
2=1
dζ(W(1)ϕ1)⊤ζ(W(1)ϕ2)a.s.→EW/bracketleftbig
ζ(W⊤ϕ1)ζ(W⊤ϕ2)/bracketrightbig
,
assuming the right hand side is finite, since the inner product is a sum of i.i.d. random variables. Here
W⊤∈R1×mis a vector with i.i.d. entries drawn from N(0,1). We define
kζ(ϕ1,ϕ2)≜EW/bracketleftbig
ζ(W⊤ϕ1)ζ(W⊤ϕ2)/bracketrightbig
, (10)
and callkζa single hidden layer neural network kernel (NNK) with activation function ζ. The PSD
kernelkζuniquely defines an RKHS by the Moore–Aronszajn theorem. Closed-form expressions of kζ
for different ζare available (Williams, 1997; Le Roux & Bengio, 2007; Cho & Saul, 2009; Tsuchida et al.,
2018; Pearce et al., 2019; Tsuchida, 2020; Meronen et al., 2020; Tsuchida et al., 2021; Han et al., 2022).
Define (χ1,χ2)⊤≜/parenleftbig
W⊤ϕ1,W⊤ϕ2/parenrightbig⊤, which is a zero mean bivariate Gaussian with a covariance matrix
Σ(1). Note that kζ(ϕ1,ϕ2)depends on the input features ϕ1andϕ2only through the covariance
matrix Σ(1). It is helpful to explicate this dependence structure through a special notation. We have
that equation 10 is equal to
κζ/parenleftbig
Σ(1)
11,Σ(1)
22,Σ(1)
12/parenrightbig
≜kζ(ϕ1,ϕ2) =E(χ1,χ2)⊤∼N(0,Σ(1))/bracketleftbig
ζ/parenleftbig
χ1/parenrightbig
ζ/parenleftbig
χ2/parenrightbig/bracketrightbig
,Σ(1)≜/parenleftbiggϕ⊤
1ϕ1ϕ⊤
1ϕ2
ϕ⊤
2ϕ1ϕ⊤
2ϕ2/parenrightbigg
.(11)
With an abuse of terminology, we refer to both kζandκζas PSD single hidden layer NNKs. For a
more detailed description of the NNK, see Appendix C.
2The effect of non-unit weight variance may be obtained by scaling all inputs ϕ1by a hyperparameter. Similarly,
arbitrary covariance structures inside rows of W(1)can be reflected as linear transformations of all inputs ϕ1.
3The effect of zero mean Gaussian biases may be obtained by augmenting inputs with an additional coordinate. The
magnitude of this coordinate is equivalent to the quotient of the standard deviation of the weights to the biases.
7Published in Transactions on Machine Learning Research (07/2023)
Neuralnetworkkernel, τhiddenlayers Onemaycomposeequation11multipletimesbyapplying
a sequence of kernels to a 3-dimensional state represented by a 2×2PSD matrix Σ(t), in place of the
infinitely wide signals. This 3-dimensional state represents the two squared norms and inner product
in each hidden layer. For t= 1,...,τandij∈{11,22,12},
Σ(t+1)
ij≜E(χi,χj)⊤∼N(0,Σ(t))/bracketleftbig
ζ/parenleftbig
χi/parenrightbig
ζ/parenleftbig
χj/parenrightbig/bracketrightbig
=κζ/parenleftbig
Σ(t)
ii,Σ(t)
jj,Σ(t)
ij/parenrightbig
, (12)
where Σ(t)
ijdenotes the ijth element of Σ(t). This iteration appears in deep infinitely wide
NNKs (Matthews et al., 2018; Lee et al., 2018). We will refer to this kernel as the τlayer NNK. Eval-
uations Σ(τ+1)
12of the PSD kernel are determined entirely by the activation function ζ, and uniquely
define an RKHS.
Neural tangent kernel, τhidden layers This kernel describes the limiting behaviour of randomly
initialisedneuralnetworksthataretrainedundergradientflow(Jacotetal.,2018). Thekerneliterations
are similar to equation 12, but also contain components involving the derivative ˙ζofζ. Let⊙denote
elementwise product. In addition to the iteration equation 12, let Θ(1)=Σ(1)and define
Θ(t+1)≜Θ(t)⊙˙Σ(t+1)+ Σ(t+1),where (13)
˙Σ(t+1)
ij≜E(χi,χj)⊤∼N(0,Σ(t))/bracketleftbig˙ζ/parenleftbig
χi/parenrightbig˙ζ/parenleftbig
χj/parenrightbig/bracketrightbig
=κ˙ζ/parenleftbig
Σ(t)
ii,Σ(t)
jj,Σ(t)
ij/parenrightbig
to obtain the evaluation of the PSD NTK in the last iteration Θ(τ+1)
12. Once again, the kernel is
determined entirely by the ζ, and uniquely defines an RKHS. The notion of an NTK for DEQ models
has been explored (Feng & Kolter, 2021), which results in a kernel that is in some sense a composition
of many layers of kernels. In contrast, our current investigation is about building a compositional kernel
from a latent variable model, rather than a neural network model.
While the DEKer we will derive shares only superficial similarities with the NTK, special cases of the
DEKer can recover NNKs. We further discuss this at the end of § 3.
3 Main results
Our results are most clearly described in terms of an alternative parameterisation of exponential fam-
ilies and link functions, which are perhaps of independent interest. We first describe this alternative
parameterisation in § 3.1, before moving onto the setup for our main analysis in § 3.2. We then in § 3.3
provide a special case (Corollary 3) of our main and most general result (Theorem 4) in § 3.4. Finally,
quantification of error between the DEKer and ff DEKer is described in § 3.5. We give examples of
our resulting updates in Appendix G.2.
3.1 An alternative view of link functions in exponential families
Instead of computing via the conditional expectation resulting from the application of an inverse link
function equation 8, we follow Tsuchida & Ong (2023) and learn the canonical parameter via a non-
linearityH=R(Vϕ), for some once-differentiable R:R→Hcalled the canonical nonlinearity . This
means that the conditional likelihood equation 7 is now
p(Y|V,ϕ) =d/productdisplay
i=1p(yi|ηi) =/parenleftigd/productdisplay
r=1h(yr)/parenrightig
exp/parenleftbig
T(Y)⊤R(Vϕ)−A/parenleftbig
R(Vϕ)/parenrightbig⊤1/parenrightbig
. (14)
Such a parameterisation is rich enough to recover the (non-canonical) inverse link function view of
the statistician (see Proposition 2). It can therefore be considered to be a change of notation, placing
emphasis on the canonical nonlinearity Rinstead of the inverse link function s−1. In our setting, one
advantage of such a notation is that it avoids more complicated function compositions involving inverses
and derivatives. For example, instead of writing A◦(A′)−1◦s−1(Vϕ)we may write A◦R(Vϕ). The
value of these simple compositions become more evident in Proposition 2.
8Published in Transactions on Machine Learning Research (07/2023)
Exponential family A(η) s−1(a) R(a) ρ(a) σ(a)
Gaussian η2/2 s−1(a) s−1(a) (s−1)′(a)s−1(a) (s−1)′(a)
Gaussian η2/2 a a 1 a
Gaussian η2/2 erf(a/√
2) erf(a/√
2) 2p(a) erf(a/√
2)2p(a)
Gaussian η2/2 ReLU(a) ReLU(a) u(a) ReLU(a)
Poisson exp(η) s−1(a) logs−1(a)(s−1)′(a)
s−1(a)(s−1)′(a)
Poisson exp(η) exp(a) a 1 exp(a)
Poisson exp(η) log(1 + exp a) log log(1 + exp a)exp(a)
(1+expa) log(1+exp a)exp(a)//parenleftbig
1 + exp(a)/parenrightbig
Bernoulli log(1 + exp( η)) s−1(a)(s−1)′(a)
s−1(a)/parenleftbig
1−s−1(a)/parenrightbig(s−1)′(a)
1−s−1(a)
Bernoulli log(1 + exp( η))exp(a)//parenleftbig
1 + exp(a)/parenrightbig
a 1 exp(a)//parenleftbig
1 + exp(a)/parenrightbig
Bernoulli log(1 + exp( η)) P(a) log/parenleftig
P(a)
1−P(a)/parenrightig
p(a)
P(a)P(−a)p(a)
P(−a)
Table 1: These examples are obtained by plugging the desired log partition function Aand inverse
link function s−1into expressions equation 24, equation 25 and equation 26. Canonical link functions
are shown in blue. General inverse link function settings are shown in red. Here Pandprespectively
denote the cdf and pdf of the univariate standard Gaussian and erfdenotes the error function.
Nonlinearities and activation functions The derivatives of the log likelihood equation 14 (a score
function) play a central role in numerical procedures associated with estimation. In our setting, such
derivatives involve terms derived from AandR. These terms are expressed in terms of functions we call
factor activations ρ(a)≜R′(a)andchain activations σ(a)≜(A◦R)′(a). The following identities show
how one may map between choices of (A,s)and choices of (A,R), and additionally how these induce
activation functions σandρwhich appear in gradient-based optimisers and our later derivations. Note
that we may choose the inverse link function s−1to be non-canonical (not A′).
Proposition 2. Consider a regular and minimal exponential family with log partition function A:
H→R. Suppose the conditional expectation belongs to a set A′, that is,A′(η)∈A′for all canonical
parameters η∈H. Lets−1:B→A′be an inverse link function, for some B⊆R. That is, for every
η∈Hthere exists some a∈Bsuch thatA′(η) =s−1(a). Then equivalently, η=R(a), whereR:B→H
is defined by R(a)≜/parenleftbig
(A′)−1◦s−1/parenrightbig
(a). Furthermore,
ρ(a)≜R′(a) =(s−1)′(a)
A′′◦(A′)−1◦s−1(a)andσ(a)≜(A◦R)′(a) =s−1(a) (s−1)′(a)
A′′◦(A′)−1◦s−1(a).
The proof is given in Appendix B. In practice we will take B=R. We observe that Ris the identity
if and only if sis a canonical link function (which is to say that s−1(a) =A′(a)). For the special case
of a Gaussian with known variance, A′is the identity and Ris the inverse link function s−1. Further
cases are listed in Table 1.
Nonlinear parameterisation framed in terms of Rinstead ofs−1are often used (McCullagh & Nelder,
1989, Chapter 11.4 and references therein), but their general relationship to s−1does not appear to be
discussed. As our setting is equivalent to using an arbitrary link function, we inherit the motivation
of using a non-identity Rfrom the motivation for using a non-canonical link function. We also inherit
the usual difficulties in estimation and sample complexity due to using not necessarily canonical link
functions.
Recall that the choice of (A,s)should be informed by both modelling and numerical convenience
(sampling, optimisation) considerations. Motivated by neural network kernels, we find a different set of
(A,s)pairs convenient to work with compared with the generalised linear model setting. Convenience
here translates to being able to compute certain Gaussian integrals of the form equation 11 in closed
form. For example, we find it easy to work with a Gaussian with non-negative conditional expectation,
parameterised by A(η) =η2/2ands−1(a) = ReLU( a), where ReLUis the popular rectified linear
unit (Efron & Hastie, 2021, page 362). Another convenient setting is a Gaussian likelihood and probit
inverse link function (in contrast with the often seen Bernoulli and probit inverse link function). Our
theory holds for general (A,s)pairs, but its practical efficiency is contingent upon the existence of
efficient numerical routines for computing the integral equation 11. Such numerical routines in the
9Published in Transactions on Machine Learning Research (07/2023)
absence of closed-forms are available in other works (Zandieh et al., 2021; Han et al., 2022), but we do
not study their application here.
3.2 Setup
Stochastic gradient descent We apply SGD (Robbins & Monro, 1951; Wright & Recht, 2022,
Chapter 5) to a minimisation objective L(ϕ;X) =EVL/parenleftbig
ϕ;X,V/parenrightbig
with decision variable ϕ, inputX
and random linear transformation V. The exact minimisation objective relates to a continuous latent
variable model, and is described shortly. Given two inputs X1andX2, thet+ 1th iterates are
ψ(t+1)
X1=ψ(t)
X1−α(t)∂
∂ψ(t)
X1L/parenleftbig
ψ(t)
X1;X1,V(t)/parenrightbig
andψ(t+1)
X2=ψ(t)
X2−α(t)∂
∂ψ(t)
X2L/parenleftbig
ψ(t)
X2;X2,V(t)/parenrightbig
(15)
with initial features ψ(0)
X1andψ(0)
X2, a sequence{α(t)}tof step sizes, and a sequence of {V(t)}tof iid
samples of V. We use the features ψ(t)
X1andψ(t)
X2in equation 15 to define the ff DEKer,DEKer and
ℓDEKer via equation 2. We stress that we are updating features, not weight parameters.
Continuous latent variable model We work with a data generating process which is a slight
nonlinear generalisation (Tsuchida & Ong, 2023) of exponential family PCA (Collins et al., 2001),
allowing for nonlinear R(or equivalently, non-canonical link functions as in Proposition 2). This model
describes data Yas being drawn from an exponential family distribution with a canonical parameter
that is a function of a latent ϕ. The number of conditionally independent observations of exponential
family distributed random variables is d, and the dimensionality of the latent is m.
More concretely, let X∈X⊆Rlbe an input and suppose that data Y= Γ(X)follows a factorised
exponential family equation 7 for some realisation of a random mapping Γ :X→Yd. LetR:R→H
be a once-differentiable function. Choose the canonical parameter H=R(Vϕ)to be the composition
ofRand a linear transformation Vof a latent input variable ϕ∈ΨΨ =Rm. The linear transformation
Vform the model parameters. Place an i.i.d. N(0,1)prior over each entry of V∈Rd×m, independent
ofΓ. Place an i.i.d.N(0,λ−1/m)prior overϕ. This results in a pre-nonlinearity parameter Vϕhaving
components with variance which stays constant in dandm. For some constant Cnot depending on ϕ,
we have
−logp/parenleftbig
ϕ|Γ(X),V/parenrightbig
=−/parenleftigg
logp/parenleftbig
Γ(X)|R(Vϕ)/parenrightbig
/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
Log likelihood−mλ
2∥ϕ∥2
/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
-Log prior/parenrightigg
+C.
As discussed in Proposition 2, the derivative of the negative log-posterior with log-partition function
A, which appears in our later optimisation procedure, induces two functions ρ(a)≜R′(a)andσ(a)≜
(A◦R)′(a)which we call factor activations and chain activations respectively.
Objective function The expected negative log posterior
L(ϕ;X)≜EVL/parenleftbig
ϕ;X,V/parenrightbig
,whereL/parenleftbig
ϕ;X,V/parenrightbig
≜1
d/parenleftig
−logp/parenleftbig
Γ(X)|R(Vϕ)/parenrightbig
+mλ
2∥ϕ∥2/parenrightig
,(16)
is a commonly used minimisation objective to find point estimates of ϕ. See Appendix H.1 for a
discussion on this objective. The division by dis introduced to account for the natural numerical
scaling of the likelihood term, which is a sum of dparts. Following recent deep learning trends, we
consider an over-parameterised andunder-regularised variant
L(ϕ;X)≜EVL/parenleftbig
ϕ;X,V/parenrightbig
,whereL/parenleftbig
ϕ;X,V/parenrightbig
≜1
d/parenleftig
−logp/parenleftbig
Γ(X)|R(Vϕ)/parenrightbig
+√
mdλ
2∥ϕ∥2/parenrightig
,(17)
whered<m. Wecallthesettingover-parameterisedbecausetherearemoreparametersthandatapoints
(m>d) and under-regularised because the regularisation strength√
mdλ
2is less than what it would be
under a typical Gaussian prior, mλ
2. This expected negative log posterior may be obtained by choosing
an overly broad i.i.d. prior N/parenleftbig
0,λ−1/√
md/parenrightbig
overϕ. We will take dto be a well-behaved function of m
such thatd→∞asm→∞(see Assumption 1).
10Published in Transactions on Machine Learning Research (07/2023)
Assumptions We now describe two assumptions common to both settings. Our first assumption says
thatthedimensionality mofthefeaturespaceshouldbecomelargermuchfasterthanthedimensionality
dof the number of conditionally independent observations in the exponential family.
Assumption 1. Consider any limit path (m,d)→(∞,∞)such that lim
m→∞d
m= 0.
Our second assumption describes how the step size αshould depend on m,d, and the SGD iteration t.
Recallthenomenclature“fixed”and“decreasing”asqualifiersforstep-size, whichdescribeadependency
ont(but notd). Recall that λis the regularisation parameter.
Assumption 2 (a).lim
m→∞α(t)λ/radicalbigm
d= 1.
Assumption 2 (a) allows for fixed or decreasing step sizes, such as1
λ√
d√m+r(t)for increasing but finite
r. Assumption 2 (a) suffices for our limiting result to hold. If we want to additionally quantify the
distance between the limit and finite-dimensional kernels, we use the stronger Assumption 2 (b), which
in particular requires a fixed step-size. Both variants 2 (a) and 2 (b) result in a limiting step size of 0,
under Assumption 1.
Assumption 2 (b). We have a fixed step-size α(t)=1
λ/radicalig
d
m.
We will find that in our setup, the DEKer is a composite function involving NNK building blocks.
3.3 Error function inverse link and Gaussian likelihood to match a random mapping
We will find that the update rule Gof the DEKer is a composite function involving NNK building
blocks. In order to clearly highlight the role of these NNK building blocks, we first present a special
case before presenting our more general Theorem 4. This provides a clear link between the statistical
likelihood model and closed form expressions for the DEKer update rule.
We choose an exponential family, canonical nonlinearity and random mapping Γ. This particular setup
leads to closed-form expressions for the NNKs involved in the update rule. As an activation function,
we choose the error function erf(z) = 2/√π/integraltextz
0e−v2dv(closely related to the Probit function), and rely
on a closed-form NNK derived in Williams (1997),
κerf(·/√
2)(Σ11,Σ22,Σ12) =2
πsin−1 Σ12/radicalbig
(1 + Σ 11)(1 + Σ 22). (18)
Weshowherethestatisticalmodellingchoicesandtheircorrespondingeffectonthe DEKer updaterule.
In this special case, our main result (Theorem 4) implies Corollary 3, as proven in Appendix G.1. Recall
from § 2.3, that the designer needs to choose a log partition function A, and a canonical nonlinearity
R. We map input X∈Rlto dataY∈Rdthrough a random mapping Y= erf( WX/√
2) +Q,
where W∈Rd×landQ∈Rdcontain i.i.d. standard Gaussian elements. The distribution of Y
given erf(WX/√
2)isconditionallyGaussian,withconditionalexpectation erf(WX/√
2)havingelements
between−1and1. We therefore choose a matching inverse link function, to represent the conditional
expectation as a function of features ψX. The inverse link function is s−1(a) = erf(a/√
2). The log
partition function is A(η) =η2/2and the sufficient statistic is T(y) =y. Since the likelihood is
Gaussian, the canonical nonlinearity Rand inverse link function s−1are the same, as shown in the first
row of Table 1. In this particular case, the activations ρandσare shown in the third row of Table 1.
Corollary 3. Suppose input Xis mapped to data YbyY= erf( WX/√
2) +Q, where erfis the
error function and W∈Rd×landQ∈Rdcontain i.i.d. standard Gaussian elements. Choose the log
partition function A(η) =η2/2. Choose the canonical nonlinearity R(a) = erf(a/√
2), or equivalently,
choose the inverse link function to be s−1(a) = erf(a/√
2). This implies that ρ(a) = 2p(a)andσ(a) =
2p(a) erf(a/√
2), wherepis the pdf of the standard Gaussian. Then κρandκσare given by
κρ(Φ11,Φ22,Φ12) =2
π/radicalbig
(1 + Φ 11)(1 + Φ 22)−Φ2
12,
κσ(Φ11,Φ22,Φ12) =κρ(Φ11,Φ22,Φ12)κerf(·/√
2)(F11,F22,F12),where F=/parenleftbig
Φ−1+I/parenrightbig−1.
11Published in Transactions on Machine Learning Research (07/2023)
LetCij=κerf(·/√
2)(X⊤
iXi,X⊤
jXj,X⊤
iXj). Suppose Assumptions 1 and 2 (a) hold. Then applying SGD
to objective equation 17, the update rule Gequation 3 exists and can be decomposed into Gequation 5
satisfying
G(Φii,Φjj,Φij;Xi,Xj) =1
λ2/parenleftigg
Cijκρ/parenleftbig
Φii,Φjj,Φij/parenrightbig
+κσ/parenleftbig
Φii,Φjj,Φij/parenrightbig/parenrightigg
.
Note that in this case the component Gof the update rule Gcan be computed entirely in closed
form. The 2×2matrix F= (Φ−1+I)−1has a simple closed-form in terms of Φ(see Appendix G.1)4.
Recall that the decomposition of GintoGsays that, by plugging equation 5 into equation 3, for each
ij∈{11,22,12},
Ψ(τ+1)
ij =G/parenleftbig
Ψ(τ)
ii,Ψ(τ)
jj,Ψ(τ)
ij;Xi,Xj/parenrightbig
.That is, Ψ(τ+1)=G(Ψ(τ);X1,X2).
3.4 General case
We now consider the general setting, allowing for arbitrary (A,s)pairs and random mappings Γ. In
order to analyse this generalised setting, we require one additional definition equation 19 and two
additional assumptions 3 and 4.
In the most general setting, the DEKer includes some non-symmetric (hence not PSD and not a kernel)
cross terms. Given two activations ζ1andζ2,
κζ1,ζ2/parenleftbig
Σ11,Σ22,Σ12/parenrightbig
≜E(χ1,χ2)⊤∼N(0,Σ)/bracketleftbig
ζ1/parenleftbig
χ1/parenrightbig
ζ2/parenleftbig
χ2/parenrightbig/bracketrightbig
. (19)
The third assumption says that if the inner products were empirical estimates of an expectation, the
resulting expectation is real valued and finite. Recall that Tis the sufficient statistic of the exponential
family, Γis the random mapping from input space to data space, and σ(a) = (A◦R)′(a)
Assumption 3. The expectation K(a) =EZ/bracketleftig/parenleftig
T/parenleftbig
Γ(X)/parenrightbig
⊙ρ/parenleftbig
aZ/parenrightbig
−σ(aZ/parenrightig2/bracketrightig
is finite for all X∈X
anda∈R, whereZis a standard Gaussian random variable.
The fourth assumption describes the properties of the random mapping Γ :X→Ydasd→∞. In
order to understand what happens to the solutions found by SGD as dbecomes large, we need the
inputs which are passed through Γto be well-behaved. It suffices that a kernel and average defined
byΓconverges. We call the limiting kernel ctheexplicit kernel , which contrasts with our implicitly
defined DEKer. We give examples in Appendix F.
Assumption 4. The PSD kernel cdefined by c(X1,X2)≜ lim
m→∞1
dT/parenleftbig
Γ(X1)/parenrightbig⊤T/parenleftbig
Γ(X2)/parenrightbig
=
ET/parenleftbig
Γ(X1)/parenrightbig⊤T/parenleftbig
Γ(X2)/parenrightbig
is finite. Similarly, the mean function defined by µ(X1)≜lim
m→∞1
dT/parenleftbig
Γ(X1)/parenrightbig⊤1 =
ET/parenleftbig
Γ(X1)/parenrightbig⊤1is finite.
We are now ready to state our main result.
Theorem 4. Suppose Assumptions 1, 2 (a), 3, and 4 hold. Let Cij=c(Xi,Xj)andµi=µ(Xi)be
as defined in Assumption 4. Then applying SGD to objective equation 17, the update rule Gequation 3
exists and can be decomposed into Gequation 5 satisfying
G(Φii,Φjj,Φij;Xi,Xj)
=1
λ2/parenleftigg
Cijκρ/parenleftbig
Φii,Φjj,Φij/parenrightbig
−κσ,ρ/parenleftbig
Φii,Φjj,Φij/parenrightbig
µi−κρ,σ/parenleftbig
Φii,Φjj,Φij/parenrightbig
µj+κσ/parenleftbig
Φii,Φjj,Φij/parenrightbig/parenrightigg
.
Hereκσ,κρ,κσ,ρandκρ,σare as defined by equation 11, equation 19 and Proposition 2.
4The matrix Farises because the activation functions ρandσinvolve the pdf of a standard normal distribution pwith
covariance matrix I. Whenpis multiplied with the pdf of the bivariate normal distribution with covariance Φ, this has
an effect of computing harmonic means of covariances Φand I, resulting in F.
12Published in Transactions on Machine Learning Research (07/2023)
Proof sketch Our main result is a constructive proof for the existence of an update rule G, as posed
in equation 3. In order to prove our main result, we will need to prove a series of lemmas, as detailed
in Appendix E. The intuition behind these lemmas is as follows. The stochastic gradient∂
∂ϕL/parenleftbig
ϕ;X,V/parenrightbig
evaluated at an arbitrary point ϕ∈ψψfor inputXand random Vis the sum of the gradient of the
negative log prior and the stochastic gradient of the negative log likelihood,
∂
∂ϕL/parenleftbig
ϕ;X,V/parenrightbig
=/radicalbiggm
dλϕ
/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
Gradient of negative log prior−1
dV⊤/parenleftbig
T/parenleftbig
Γ(X)/parenrightbig
⊙ρ(Vϕ)−σ(Vϕ)/parenrightbig
/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
Stochastic gradient of log likelihood. (20)
Assumption 2 (a) means that if the limit were allowed to be applied, the gradient of the negative log
prior term in equation 20 multiplied by the step size would look like ϕ. This means that the update of
SGD would reduce to the stochastic gradient of the log likelihood. To derive the kernel update rule,
we then examine the inner product of the stochastic gradient of the log likelihood. We first convert
the inner product of the stochastic gradient of the log likelihood to an approximate form that is easier
to deal with (Lemma 15). We then confirm that the kernel update only involves the inner product of
the stochastic gradients of the log likelihood (Lemma 16). Finally, we show that the inner products of
the approximate form converges to a closed form update rule G(Lemma 17). Assembling these lemmas
together yields Theorem 4. The detailed proof is given in Appendix E.
Recall again that the decomposition of GintoGsays that, by plugging equation 5 into equation 3, for
eachij∈{11,22,12},
Ψ(τ+1)
ij =G/parenleftbig
Ψ(τ)
ii,Ψ(τ)
jj,Ψ(τ)
ij;Xi,Xj/parenrightbig
.That is, Ψ(τ+1)=G(Ψ(τ);X1,X2).
Note the cross terms involving κσ,ρandµin Theorem 4, which were not present in the special case of
Corollary 3. These cross-terms arise from random mappings Γwith an average element that is non-zero.
In the case of Corollary 3, these cross-terms cancel out.
Theorem 4 implies a fixed point condition by Theorem 1, providing a positive answer for equation 4.
Corollary 5. Suppose the same setting as Theorem 4. If Gis a contraction mapping, then the DEKer
converges to a unique fixed point as t→∞. That is, for each ij∈{11,22,12},
Ψij=1
λ2/parenleftigg
Cijκρ/parenleftbig
Ψii,Ψjj,Ψij/parenrightbig
−κσ,ρ/parenleftbig
Ψii,Ψjj,Ψij/parenrightbig
µi−κρ,σ/parenleftbig
Ψii,Ψjj,Ψij/parenrightbig
µj+κσ/parenleftbig
Ψii,Ψjj,Ψij/parenrightbig/parenrightigg
.
(21)
Whether Gis a contraction can be determined by a derivative test and an identity given in Theorem 19,
as we demonstrate in § G.2. Note that even if a unique fixed point does not exist (which may be the
case if Gis not a contraction), one may still compute with finite- titerates of SGD via Theorem 4.
We may compute iterates of SGD in the limit via the update rule for any τto obtain Ψ(τ)
ij, which is
the naive fixed point algorithm applied to equation 21. Alternatively, we may compute the ℓDEKer
by solving equation 21 for each ij∈{11,22,12}using any other fixed point solver.
Notable special cases and relation to NNK and NTK Some further examples arising from
special choices of A,RandΓ(inducing corresponding ρ,σ,candµ) are discussed in Appendix G.2.
We find that the linear Gaussian ( A(η) =η2/2,R(a) =a) results in a DEKer that is a scale multiple of
c(Appendix G.2.1). We can recover an NNK with activation σwhenAandRare allowed to be general
andCandµare set to zero (Appendix G.2.2). This means that any NNK with activaiton σmay be
expressed as a special case of a DEKer. The setting we found useful for our experiments (§ 4) is a
nonlinearly parameterised Gaussian ( A(η) =η2/2andR(a) =ReLU (a)) with a first-order arc-cosine
kernel forcand a corresponding mean function µ. This setting admits a closed-form update rule for
G. While this setting is distinct from the NTK with ReLU activations, it shares superficial similarities
in that both updates involve repeated iterations of kernels κρandκσ. See Appendices G.2.4 and G.2.5
for details.
13Published in Transactions on Machine Learning Research (07/2023)
3.5 Sensitivity
In order to quantify the rate at which the infinite-dimensional, infinite-iteration DEKer converges
with respect to the dimension, we need the feature mapping to be well-behaved. Lipschitzness and
boundedness allows concentration inequalities to be applied.
Assumption 5. Supposeρis bounded or Lipschitz. Suppose σis bounded or Lipschitz.
We may quantify the degree to which the infinite-dimensional, infinite-iteration DEKer is an invariant
of SGD. We define specific values of finite dimensional ϕandϕ′using the infinite dimensional kernel
fixed point Ψ11,Ψ22andΨ12. This definition will serve as a good approximation of an invariant.
Definition 6. Define
r1=/radicalbig
Ψ11/parenleftbig
1,0,...0/parenrightbig⊤∈Rm, r 2=/radicalbig
Ψ22/parenleftbig
cosω,sinω,0,...0/parenrightbig⊤∈Rm,
where cosω=Ψ12√Ψ11Ψ22.Thenr⊤
1r2= Ψ 12,r⊤
1r1= Ψ 11, andr⊤
2r2= Ψ 22.
We bound the residual of the finite dimensional kernel evaluated at an initial guess that is the solution
of the infinite (m,d)system. When this bound is small, intuitively speaking, the limiting solution Ψis
“almost” an invariant of the finite-dimensional system. By invariant of SGD, we mean that Ψdoes not
change as iterates of SGD increase. In other words, if we intitialise SGD close to the solution Ψ, future
iterates of kernels will stay close to the initialisation with high probability.
Theorem 7. Suppose Assumptions 1, 2 (b), 3, 4 and 5 hold. Let initial guesses be ψ(0)
X1=r1and
ψ(0)
X2=r2as in Definition 6. Then there exist constants Q2,Q3,c2,c3>0such that for all δ>0,ϵ2>0
andε2,
P/parenleftig/vextendsingle/vextendsingleΨ(1)
12−Ψ12/vextendsingle/vextendsingle≤ε1+ε2/parenrightig
≥1−δ1−δ2,
where
ε1=K+ϵ2
λ2(2ϵ1+ϵ2
1), δ 1= 2 exp/parenleftig
−c2dM2/parenrightig
+ exp/parenleftbig
−mδ2/2/parenrightbig
andδ2= 2 exp/parenleftbig
−dc3M3/parenrightbig
andϵ1=/radicalig
d
m+δ,M2= min/braceleftig
ϵ2
2
Q2
2,ϵ2
Q2/bracerightig
andM3= min/braceleftig
ε2
2
Q2
3,ε2
Q3/bracerightig
andc3>0is some absolute constant.
The proof is given in Appendix E. The probability decays to 1exponentially in the minimum of mand
d, where we recall that dis the number of conditionally independent observations in the exponential
family and mis the dimensionality of the latent variable. The closeness ε1+ε2may be configured to
be close to zero by choosing ε2,δandϵ2to be small, thereby trading off against constants resulting in
a slower exponential decay of probabilities.
4 Experiments
Recallthattheff DEKer isdefinedforfiniteSGDiteration τasaninnerproductoffinite m-dimensional
features. The DEKer is defined for finite SGD iteration τas a limit as m→∞of an inner product of
m-dimensional features. The ℓDEKer is defined as a limit as SGD iteration τ→∞of the DEKer.
Although the DEKer andℓDEKer are defined in terms of infinite dimensional features, evaluations
of the DEKer andℓDEKer are scalar values and can be used to form matrices with a finite number
of rows and columns. These matrices can be used in kernel methods to build predictive algorithms.
In our analysis in the previous section, we considered 2×2kernel matrices. Such analysis extends to
N×Nkernel matrices, where each element of the N×Nkernel matrices may be related to an element of
a2×2kernel matrix. All our implementations are vectorised, so that they operate on N×Nmatrices.
14Published in Transactions on Machine Learning Research (07/2023)
4.1 Measuring finite-width effects
We empirically measure the similarity of (finite- τ, finite-d) ffDEKer matrices and (infinite- τ, infinite-
d)ℓDEKer matrices using the centered variant (Cortes et al., 2012) of kernel alignment (Cristianini
et al., 2001), abbreviated CKA, as dincreases. We vary dbetween 5 and 500 in steps of 5 and choose
m=d3/2. For control, we also measure the CKA between the (finite- τ, finite-d) ffDEKer and the
squared exponential kernel (SEK). See Figure 2, and Appendix J.1 for full details on the experimental
setup. As expected (Theorem 4), the CKA between the DEKer matrices becomes larger as dandm
increase, but not between the SEK and finite DEKer.
Gaussian
 Bernoulli
 Gaussian (rectified parameter)
Figure 2: CKA between kernel matrices consisting of entries Ψijandk(t)
d(Xi,Xj)(Blue) and squared
exponential kernel for control and k(t)
d(Xi,Xj)(Orange) for three choices of AandR. (Left) Gaussian
exponential family, A(η) =η2/2andR(η) =η. (Middle) Bernoulli exponential family, A(η) = log(1 +
expη)andR(η) =η. (Right) Gaussian exponential family with rectified parameter, A(η) =η2and
R(η) = ReLU(η). The dashed red horizontal line at 1represents the maximum value of CKA.
4.2 Inference using the DEKer
We use the DEKer for kernel ridge regression (Saunders, 1998) (KRR) ( cfGaussian process regres-
sion (Rasmussen & Williams, 2006)) on a suite of benchmarks. For each dataset, we first partition
the data into an 80−20train-test split. Using the training set, we perform 5-fold cross-validation
for hyperparameter selection using the default settings of sci-kit learn’s GridSearchCV , which performs
model selection based on the coefficient of determination. The hyperparameter grid we search over is
described in Table 4, Appendix J.2. We then compute the RMSE on the held out test set using all
training data. We repeat this procedure for 100different random shuffles of dataset, and find the sample
average and standard deviation RMSE over the random shuffles. The results are reported in Table 2.
The inputXis preprocessed by subtracting the sample average and dividing by the sample standard
deviation of each feature. Additionally, the target data yis mean-centered and scaled by the sample
standard deviation. The reported RMSE is after conversion of yback to original units.
Sincethe DEKer isastrictgeneralisationoftheNNK,weexpectthe DEKer tostrictlyout-performthe
NNK. Any empirical performance result that may be obtained by previous investigations into the NNK
with activations σ(Lee et al., 2020) can be reproduced by a DEKer with the correct hyperparameter
choice. We do not empirically explore uncertainty properties of corresponding Gaussian process models
as others do (Adlam et al., 2020), but note that the same principle applies. We find that GridSearchCV
sometimes picks out settings that correspond with an NNK, but often does not. The number of times
GridSearchCV collapses the DEKer to the NNK is indicated in the last column of Table 2. Our
results are consistent with the previously established observation that “NNKs frequently outperform
NTKs” (Lee et al., 2020). More interestingly, we find that for each dataset, the DEKer performs as
well or better than every other kernel, including the NNK.
15Published in Transactions on Machine Learning Research (07/2023)
Data DEKer orℓDEKer NNK NTK SEK
yacht 0.65±0.21 2.13±0.57 2.75±0.58 3.62±0.670
diabetes 54.51±3.29 54.58±3.30 55.05±3.38 54.75±3.3270
energy1 1.00±0.11 1.01±0.11 1.67±0.14 1.08±0.1378
energy2 1.58±0.15 1.58±0.15 2.10±0.18 1.58±0.1668
concrete 4.94±0.47 4.97±0.47 5.05±0.48 5.65±0.3960
wine 0.57±0.02 0.61±0.02 0.54±0.02 0.62±0.021
Table 2: RMSE of KRR models ( ±one standard deviation over 100random seeds). We use the DEKer
described in § G.2.5, which outperforms other kernels according to the sample average of the RMSE.
Often the difference in performance is small compared with the standard deviation. The final column
is the number of times the best DEKer found using GridSearchCV was an NNK. The datasets are
UCI benchmarks — yacht (Gerritsma et al., 2013), diabetes (Kahn), energy (first and second value
independently) (Tsanas & Xifara, 2012), concrete (Yeh, 2007) and wine (Cortez et al., 2009).
5 Conclusion
We introduced the DEKer, a kernel analogue of implicit neural network models. The DEKer is
defined as the limiting inner product between two features computed using a feature update procedure
as the dimensionality of the features goes to infinity.
We considered the problem of whether a deterministic update procedure for the DEKer exists (equa-
tion 3), and whether this update rule converges (equation 4). We focused on the special case where the
features are latent variables in an exponential family PCA model (with not necessarily canonical link
function) learnt using SGD. Leveraging the connection between infinitely wide explicit neural networks
and kernel methods, we showed how in such a setting an explicit update rule can be computed. The
update rule is a composition of functions involving NNK building blocks. While this type of update
rule is not the one that is typically encountered in deep learning, which usually updates weights using
SGD, it results in an interesting limiting model that shares connections to the NNK.
TheDEKer has a number of interesting properties. The DEKer is able to recover instances of the
NNK, and also resembles the NTK. Importantly, unlike the NNK and NTK, the deep layer structure
of the DEKer is motivated entirely from an optimisation perspective. The activation functions (and
thus kernels) involved in the computation of the DEKer can be related back to statistical modelling
assumptions on the data through the exponential family. In particular, the activation functions share a
connection to the log partition function and inverse link function of the exponential family. On a series
of benchmarks, the DEKer performs as well as or outperforms the NNK, NTK and SEK.
Our work admits several natural extensions. The matrix Vwhich represents a linear transformation or
fullyconnectedlayermaybeconstrainedtoresembleaconvolutionallayer,andweexpectaconvolutional
variant of the DEKer to be tractable (Novak et al., 2019). Since our construction is probabilistic, the
Laplace approximation may yield a tractable means of obtaining principled uncertainty estimates for
kernel methods beyond the regular Gaussian process framework. Since the DEKer satisfies a fixed
point equation, implicit differentiation may be used to compute derivatives of the DEKer with respect
to its hyperparameters, mirroring the neural network counterpart (Bai et al., 2019).
We considered the special case where the DEKer equation 2 is defined using features that are solu-
tions to (latent variable model) optimisation problems using SGD. The resulting DEKer satisfies a
fixed point equation. In future, other work might consider other types of problems and algorithms,
resulting in DEKers which satisfy other types of conditions. Concurrent work (Cirone et al., 2023)
considers solving neural controlled differential equations using Euler discretisations. Their resulting
kernel satisfies a certain type of partial differential equation. We believe these two results, along with
classical descriptions of Gaussian processes as solutions to stochastic differential equations (Rasmussen
& Williams, 2006, Appendix B), might represent special instances of a more general framework.
16Published in Transactions on Machine Learning Research (07/2023)
We hope that our optimisation view and deterministic kernel update rule stimulates new research in
both deep learning and kernel methods.
Acknowledgments
Both authors would like to acknowledge the support of CSIRO’s Machine Learning and Artificial Intel-
ligence Future Science Platform.
References
Daniel Adiwardana, Minh-Thang Luong, David R So, Jamie Hall, Noah Fiedel, Romal Thoppilan,
Zi Yang, Apoorv Kulshreshtha, Gaurav Nemade, Yifeng Lu, et al. Towards a human-like open-
domain chatbot. arXiv preprint arXiv:2001.09977 , 2020.
Ben Adlam, Jaehoon Lee, Lechao Xiao, Jeffrey Pennington, and Jasper Snoek. Exploring the un-
certainty properties of neural networks’ implicit priors in the infinite-width limit. arXiv preprint
arXiv:2010.07355 , 2020.
James H Albert and Siddhartha Chib. Bayesian analysis of binary and polychotomous response data.
Journal of the American statistical Association , 88(422):669–679, 1993.
Luís B. Almeida. A learning rule for asynchronous perceptrons with feedback in a combinatorial envi-
ronment. 1990.
SanjeevArora, SimonSDu, ZhiyuanLi, RuslanSalakhutdinov, RuosongWang, andDingliYu. Harness-
ing the power of infinitely wide deep nets on small-data tasks. nternational Conference on Learning
Representations , 2020.
Shaojie Bai, J Zico Kolter, and Vladlen Koltun. Deep equilibrium models. Advances in Neural Infor-
mation Processing Systems , 32, 2019.
Arindam Banerjee, Srujana Merugu, Inderjit S Dhillon, Joydeep Ghosh, and John Lafferty. Clustering
with Bregman divergences. Journal of machine learning research , 6(10), 2005.
DylanCampbell, LiuLiu, andStephenGould. Solvingtheblindperspective-n-pointproblemend-to-end
with robust differentiable geometric optimization. In Computer Vision–ECCV 2020: 16th European
Conference , pp. 244–261. Springer, 2020.
Ricky TQ Chen, Yulia Rubanova, Jesse Bettencourt, and David K Duvenaud. Neural ordinary differ-
ential equations. Advances in Neural Information Processing Systems , 31, 2018.
Lenaic Chizat, Edouard Oyallon, and Francis Bach. On lazy training in differentiable programming.
Advances in Neural Information Processing Systems , 32, 2019.
Youngmin Cho and Lawrence K. Saul. Kernel methods for deep learning. In Advances in Neural
Information Processing Systems , pp. 342–350. 2009.
Nicola Muca Cirone, Maud Lemercier, and Cristopher Salvi. Neural signature kernels as infinite-width-
depth-limits of controlled ResNets. International Conference on Machine Learning , 2023.
Michael Collins, Sanjoy Dasgupta, and Robert E Schapire. A generalization of principal components
analysis to the exponential family. Advances in Neural Information Processing Systems , 14, 2001.
Corinna Cortes, Mehryar Mohri, and Afshin Rostamizadeh. Algorithms for learning kernels based on
centered alignment. The Journal of Machine Learning Research , 13:795–828, 2012.
P Cortez, A Cerdeira, F Almeida, T Matos, , and J Reis. Wine Quality. UCI Machine Learning
Repository, 2009. DOI: https://doi.org/10.24432/C56S3T.
17Published in Transactions on Machine Learning Research (07/2023)
Nello Cristianini, John Shawe-Taylor, Andre Elisseeff, and Jaz Kandola. On kernel-target alignment.
Advances in Neural Information Processing Systems , 14, 2001.
Ricky Der and Daniel Lee. Beyond Gaussian processes: On the distributions of infinite networks. In
Advances in Neural Information Processing Systems , volume 18. MIT Press, 2005.
David Duvenaud. Automatic model construction with Gaussian processes . PhD thesis, University of
Cambridge, 2014.
Bradley Efron. Prediction, estimation, and attribution. International Statistical Review , 88:S28–S59,
2020.
Bradley Efron and Trevor Hastie. Computer Age Statistical Inference: Algorithms, Evidence, and Data
Science, volume 6. Cambridge University Press, 2021.
Marvin Eisenberger, Aysim Toker, Laura Leal-Taixé, Florian Bernard, and Daniel Cremers. A unified
framework for implicit sinkhorn differentiation. In IEEE/CVF Conference on Computer Vision and
Pattern Recognition , pp. 509–518, 2022.
Laurent El Ghaoui, Fangda Gu, Bertrand Travacca, Armin Askari, and Alicia Tsai. Implicit deep
learning. SIAM Journal on Mathematics of Data Science , 3(3):930–958, 2021.
Stefano Favaro, Sandra Fortini, and Stefano Peluchetti. Deep stable neural networks: large-width
asymptotics and convergence rates. arXiv preprint arXiv:2108.02316 , 2021.
Stefano Favaro, Sandra Fortini, and Stefano Peluchetti. Neural tangent kernel analysis of shallow
alpha-stable ReLU neural networks. arXiv preprint arXiv:2206.08065 , 2022.
Zhili Feng and J Zico Kolter. On the neural tangent kernel of equilibrium models. preprint, 2021.
Adrià Garriga-Alonso, Carl Edward Rasmussen, and Laurence Aitchison. Deep convolutional networks
as shallow Gaussian processes. In International Conference on Learning Representations , 2018.
J Gerritsma, R Onnink, and A Versluis. Yacht Hydrodynamics. UCI Machine Learning Repository,
2013. DOI: https://doi.org/10.24432/C5XG7R.
Kazimierz Goebel and William A Kirk. Topics in metric fixed point theory . Number 28. Cambridge
university press, 1990.
Stephen Gould, Richard Hartley, and Dylan Campbell. Deep declarative networks. IEEE Transactions
on Pattern Analysis and Machine Intelligence , 44(8):3988–4004, 2021.
Insu Han, Amir Zandieh, Jaehoon Lee, Roman Novak, Lechao Xiao, and Amin Karbasi. Fast neural
kernel embeddings for general activations. Advances in Neural Information Processing Systems , 2022.
James W Hardin and Joseph W M Hilbe. Generalized Linear Models and Extensions . College Station
Tex: StataCorp Press, 4th edition, 2018.
Arthur Jacot, Franck Gabriel, and Clément Hongler. Neural tangent kernel: Convergence and general-
ization in neural networks. In Advances in Neural Information Processing Systems , pp. 8571–8580,
2018.
M Kahn. Diabetes. UCI Machine Learning Repository. DOI: https://doi.org/10.24432/C5T59G.
Motonobu Kanagawa, Philipp Hennig, Dino Sejdinovic, and Bharath K Sriperumbudur. Gaus-
sian processes and kernel methods: A review on connections and equivalences. arXiv preprint
arXiv:1807.02582 , 2018.
C. T. Kelley. Iterative Methods for Linear and Nonlinear Equations . Society for Industrial and Applied
Mathematics, 1995. doi: 10.1137/1.9781611970944.
18Published in Transactions on Machine Learning Research (07/2023)
Diederik P Kingma and Max Welling. Auto-encoding variational Bayes. 2014.
Nicolas Le Roux and Yoshua Bengio. Continuous neural networks. In Artificial Intelligence and Statis-
tics, pp. 404–411, 2007.
Jaehoon Lee, Yasaman Bahri, Roman Novak, Samuel S Schoenholz, Jeffrey Pennington, and Jascha
Sohl-Dickstein. Deep neural networks as Gaussian processes. In International Conference on Learning
Representations , 2018.
Jaehoon Lee, Samuel Schoenholz, Jeffrey Pennington, Ben Adlam, Lechao Xiao, Roman Novak, and
Jascha Sohl-Dickstein. Finite versus infinite neural networks: an empirical study. Advances in Neural
Information Processing Systems , 33:15156–15172, 2020.
David JC MacKay. Introduction to Gaussian processes . 1998.
Julien Mairal, Piotr Koniusz, Zaid Harchaoui, and Cordelia Schmid. Convolutional kernel networks.
Advances in Neural Information Processing Systems , 27, 2014.
Alexander G de G Matthews, Mark Rowland, Jiri Hron, Richard E Turner, and Zoubin Ghahramani.
Gaussian process behaviour in wide deep neural networks. In The International Conference on
Learning Representations , 2018.
P. McCullagh and J.A. Nelder. Generalized Linear Models, Second Edition . Monographs on Statistics
and Applied Probability Series. Chapman & Hall, 1989.
Lassi Meronen, Christabella Irwanto, and Arno Solin. Stationary activations for uncertainty calibration
in deep learning. Advances in Neural Information Processing Systems , 33:2338–2350, 2020.
Shakir Mohamed, Zoubin Ghahramani, and Katherine A Heller. Bayesian exponential family PCA.
Advances in Neural Information Processing Systems , 21, 2008.
Radford M Neal. Bayesian learning for neural networks . PhD thesis, University of Toronto, 1995.
Frank Nielsen and Vincent Garcia. Statistical exponential families: A digest with flash cards. arXiv
preprint arXiv:0911.4863 , 2009.
Roman Novak, Lechao Xiao, Jaehoon Lee, Yasaman Bahri, Greg Yang, Jiri Hron, Daniel A Abolafia,
Jeffrey Pennington, and Jascha Sohl-Dickstein. Bayesian deep convolutional networks with many
channels are Gaussian processes. In The International Conference on Learning Representations ,
2019.
OpenAI, Christopher Berner, Greg Brockman, Brooke Chan, Vicki Cheung, Przemyslaw Debiak,
Christy Dennison, David Farhi, Quirin Fischer, Shariq Hashme, Chris Hesse, Rafal Jozefowicz, Scott
Gray,CatherineOlsson,JakubPachocki,MichaelPetrov,HenriquePondedeOliveiraPinto,Jonathan
Raiman, Tim Salimans, Jeremy Schlatter, Jonas Schneider, Szymon Sidor, Ilya Sutskever, Jie Tang,
Filip Wolski, and Susan Zhang. Dota 2 with large scale deep reinforcement learning. 2019.
Tim Pearce, Russell Tsuchida, Mohamed Zaki, Alexandra Brintrup, and Andy Neely. Expressive priors
in Bayesian neural networks: Kernel combinations and periodic functions. In Uncertainty in Artificial
Intelligence , 2019.
Stefano Peluchetti, Stefano Favaro, and Sandra Fortini. Stable behaviour of infinitely wide deep neural
networks. In International Conference on Artificial Intelligence and Statistics , pp. 1137–1146. PMLR,
2020.
Fernando Pineda. Generalization of back propagation to recurrent and higher order neural networks.
InNeural information processing systems , 1987.
R Core Team. R: A Language and Environment for Statistical Computing . R Foundation for Statistical
Computing, Vienna, Austria, 2021. URL https://www.R-project.org/ .
19Published in Transactions on Machine Learning Research (07/2023)
MaithraRaghu, BenPoole, JonKleinberg, SuryaGanguli, andJaschaSohl-Dickstein. Ontheexpressive
power of deep neural networks. In International Conference on Machine Learning , pp. 2847–2854.
PMLR, 2017.
Ali Rahimi and Benjamin Recht. Random features for large-scale kernel machines. Advances in Neural
Information Processing Systems , 20, 2007.
Carl Edward Rasmussen and Christopher K. I. Williams. Gaussian processes for machine learning.
Adaptive computation and machine learning. MIT Press, 2006.
Max Revay, Ruigang Wang, and Ian R Manchester. Lipschitz bounded equilibrium networks. arXiv
preprint arXiv:2010.01732 , 2020.
Danilo Riccio, Matthias J Ehrhardt, and Martin Benning. Regularization of inverse problems: Deep
equilibrium models versus bilevel learning. arXiv preprint arXiv:2206.13193 , 2022.
Herbert Robbins and Sutton Monro. A stochastic approximation method. The Annals of Mathematical
Statistics , pp. 400–407, 1951.
Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-
resolution image synthesis with latent diffusion models. In IEEE/CVF Conference on Computer
Vision and Pattern Recognition , pp. 10684–10695, 2022.
Cynthia Rudin. Stop explaining black box machine learning models for high stakes decisions and use
interpretable models instead. Nature Machine Intelligence , 1(5):206–215, 2019.
C Saunders. Ridge regression learning algorithm in dual variables. In International Conference on
Machine Learning , 1998.
Bernhard Schölkopf, Alexander J Smola, Francis Bach, et al. Learning with kernels: support vector
machines, regularization, optimization, and beyond . MIT press, 2002.
William Fleetwood Sheppard. On the application of the theory of error to cases of normal distribu-
tion and normal correlation. Philosophical Transactions of the Royal Society of London. Series A,
Containing Papers of a Mathematical or Physical Character , (192):140, 1899.
Jascha Sohl-Dickstein, Roman Novak, Samuel S Schoenholz, and Jaehoon Lee. On the infinite width
limit of neural networks with a standard parameterization. arXiv preprint arXiv:2001.07301 , 2020.
Peter Sollich. Bayesian methods for support vector machines: Evidence and predictive class probabili-
ties.Machine learning , 46(1):21–52, 2002.
Athanasios Tsanas and Angeliki Xifara. Energy efficiency. UCI Machine Learning Repository, 2012.
DOI: https://doi.org/10.24432/C51307.
Russell Tsuchida. Results on infinitely wide multi-layer perceptrons . PhD thesis, The University of
Queensland, 2020.
Russell Tsuchida and Cheng Soon Ong. Deep equilibrium models as estimators for continuous latent
variables. In International Conference on Artificial Intelligence and Statistics , 2023.
Russell Tsuchida, Fred Roosta, and Marcus Gallagher. Invariance of weight distributions in rectified
MLPs. In International Conference on Machine Learning , pp. 5002–5011, 2018.
Russell Tsuchida, Tim Pearce, Chris van der Heide, Fred Roosta, and Marcus Gallagher. Avoiding
kernel fixed points: Computing with ELU and GELU infinite networks. In AAAI Conference on
Artificial Intelligence , volume 35, pp. 9967–9977, 2021.
Russell Tsuchida, Suk Yee Yong, Mohammad Ali Armin, Lars Petersson, and Cheng Soon Ong. Declar-
ative nets that are equilibrium models. In International Conference on Learning Representations ,
2022.
20Published in Transactions on Machine Learning Research (07/2023)
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. Attention is all you need. Advances in Neural Information Processing
Systems, 30, 2017.
Roman Vershynin. High-dimensional probability: An introduction with applications in data science ,
volume 47. Cambridge university press, 2018.
Martin J. Wainwright. High-Dimensional Statistics: A Non-Asymptotic Viewpoint . Cambridge Se-
ries in Statistical and Probabilistic Mathematics. Cambridge University Press, 2019. doi: 10.1017/
9781108627771.
Martin J Wainwright, Michael I Jordan, et al. Graphical models, exponential families, and variational
inference. Foundations and Trends ®in Machine Learning , 1(1–2):1–305, 2008.
Paul FV Wiemann, Thomas Kneib, and Julien Hambuckers. Using the softplus function to construct
alternative link functions in generalized linear models and beyond. arXiv preprint arXiv:2111.14207 ,
2021.
Christopher KI Williams. Computing with infinite networks. In Advances in Neural Information
Processing Systems , pp. 295–301, 1997.
Ezra Winston and J Zico Kolter. Monotone operator equilibrium networks. Advances in Neural Infor-
mation Processing Systems , 33:10718–10728, 2020.
Blake Woodworth, Suriya Gunasekar, Jason D Lee, Edward Moroshko, Pedro Savarese, Itay Golan,
Daniel Soudry, and Nathan Srebro. Kernel and rich regimes in overparametrized models. In Confer-
ence on Learning Theory , pp. 3635–3673. PMLR, 2020.
Stephen J. Wright and Benjamin Recht. Optimization for Data Analysis . Cambridge University Press,
2022. doi: 10.1017/9781009004282.
Xingyu Xie, Qiuhao Wang, Zenan Ling, Xia Li, Yisen Wang, Guangcan Liu, and Zhouchen Lin. Opti-
mization induced equilibrium networks. arXiv preprint arXiv:2105.13228 , 2021.
Greg Yang. Wide feedforward or recurrent neural networks of any architecture are Gaussian processes.
InAdvances in Neural Information Processing Systems , pp. 9947–9960. 2019a.
Greg Yang. Scaling limits of wide neural networks with weight sharing: Gaussian process behavior,
gradient independence, and neural tangent kernel derivation. In arXiv preprint arXiv:1902.04760 ,
2019b.
I-Cheng Yeh. Concrete Compressive Strength. UCI Machine Learning Repository, 2007. DOI:
https://doi.org/10.24432/C5PK67.
Amir Zandieh, Insu Han, Haim Avron, Neta Shoham, Chaewon Kim, and Jinwoo Shin. Scaling neural
tangent kernels via sketching and random features. Advances in Neural Information Processing
Systems, 34:1062–1073, 2021.
21