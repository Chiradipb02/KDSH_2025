Under review as submission to TMLR
Towards Prototype Conformity Loss Functions for Better
Out of Distribution Detection in Traffic Sign Image
Classification
Anonymous authors
Paper under double-blind review
Abstract
Deep neural networks (DNNs) generate overconfident outputs even in case of miss-detections
caused by abnormal data. Consequently, this can lead to unreliable classifications and,
thus, potentially lead to issues in safety-critical applications such as automated driving
systems. Recent works propose to detect such anomalous data based on probabilistic
methods derived from the DNN’s internal activation functions, such as the convolutional
neural networks (CNN) backbones. This paper shows that such CNNs cannot semantically
disentangle similar classes when trained with conventional cross-entropy loss functions,
leading to poor out-of-distribution (OOD) detection while applying probabilistic methods
for such a purpose. Therefore, we propose to apply the prototype conformity loss (PCL)
function from the literature and show that such a contrastive learning method leads to
better OOD detection for traffic sign classification. Furthermore, we propose two novel
variations of the PCL, namely weighted PCL (WPCL) and multi-scale PCL (MSPCL),
which group similar classes and force the DNN to disentangle them from each other. In
contrast to existing contrastive OOD detection literature, we do not rely on complex input
transformations or augmentations. We perform our experiments on multiple DNNs and two
traffic sign classification datasets, which we test against multiple OOD data sources, such as
adversarial and non-adversarial augmentation and real-world OOD data. Based on that, we
demonstrate that our PCL variations can achieve superior results in OOD detection when
the training dataset includes various similar classes.
1 Introduction
The safety of deep neural networks (DNNs) has been a significant topic within automated driving research
in recent years Aravantinos & Schlicht (2020); Sämann et al. (2020); Schwalbe et al. (2020); Schwalbe &
Schels (2020); Willers et al. (2020). DNNs often appear as black-box modules and therefore pose a challenge
to the safety evaluation of the overall system that relies on their function. To ensure the safe functionality
of DNNs, one can study corner cases in which the DNNs fail to predict correctly. However, identifying such
corner cases for DNN-based modules, especially at runtime, is not easily feasible due to their aforementioned
complexity and resulting decision uncertainty.
Furthermore, DNNs are sensitive to unseen data. Concisely; this means that any small change in the input
data can cause a covariance shift from the data the DNN is trained with and can potentially lead to unreliable
predictions by the DNN, which can be extremely difficult to track in practice Yang et al. (2021). Such a
covariance shift in inputs could stem from different sources such as adversarial data generated by adversarial
attack methods Goodfellow et al. (2014; 2015); Fawzi et al. (2016); Eykholt et al. (2017); Samangouei et al.
(2018); Sitawarin et al. (2018); Brown et al. (2018); Soll (2019); Feng et al. (2021), which aim at deviating
the DNN decision in certain directions. Moreover, other unseen changes in the input data include different
natural causes or sensor failures, e.g., destruction and color changes of the objects in the real world due to
vandalism or aging. In the case of traffic sign classification, different adversarial attacks are introduced that
are deployable to the real world in the form of adversarial stickers to be put on the traffic signs. Despite
1Under review as submission to TMLR
CE
 PCL
WPCL
 MSPCL
Speed Limit
Prohibition ClassesTriangular Warning
End of ProhibitionRound Blue Direction
Right of WayYield
StopNo Entry
OOD
Figure 1: The t-SNE van der Maaten & Hinton (2008) visualization of different optimization methods based
on the filter activations extracted from the WideResNet50 Zagoruyko & Komodakis (2016) on the GTSRB
dataset Stallkamp et al. (2011). The optimization methods include cross-entropy (CE), original prototype
conformity loss (PCL) Mustafa et al. (2021) and our proposed two new losses i.e, weighted prototype
conformity loss (WPCL), and multi-scale prototype conformity loss (MSPCL). The colors represent the
grouping that has been done to the GTSRB dataset presented in sub-section 4.1. Accordingly, multiple
instances of a color represent all the classes belonging to the same group. The plot indicates that our
proposed methods achieved a more sparse projection of the activations, leading to improved OOD detection.
In the plot, we refer to "Clean" samples as the original data of each class, and "OOD" refers to the attacked
data of certain classes.
2Under review as submission to TMLR
their performance, DNNs often generate false predictions with high confidence, which may lead to hazardous
events if not detected and compensated by other mechanisms of the safety critical systems. Therefore, it is
essential for the system to actively recognize when to rely on the decisions made by the DNN module and
when not to, thereby increasing the overall robustness of the system.
Accordingly, werefertoout-of-distribution(OOD)basedonthedefinitionprovidedbyYang et al. Yangetal.
(2021) as the covariance shift occurred to the input data ( x) while having the same semantic meaning ( y),
and any other distributional shift in the input data causing a semantic shift as well, such as unforeseen traffic
sign classes. Therefore, our catalog of OOD data includes real-world traffic sign samples with occlusions due
to different factors such as dirt, snow, etc., adversarial augmentations leading to the miss-classification of
the input data, and non-adversarial augmentations resembling vandalized traffic signs. Based on that, our
motivation is to optimize the OOD detection methods based on latent space activations generated from
the clean training data sets as their primary operational design domain (ODD) and consider any shift in
the input space as OOD. To better achieve this, we aim to encourage the DNNs to provide better latent
activations to help the OOD detectors perform better.
In this paper, we utilize a prototype conformity loss (PCL) alongside the conventional cross-entropy loss to
train traffic sign classifiers for better out-of-distribution (OOD) detection. We show that when combined
with the standard OOD detection methods from the literature, training the classifiers with PCL would lead
to better OOD detection while maintaining, if not outperforming, the baseline accuracy. Furthermore, we
show that in particular classification tasks, such as traffic sign image classification, where there are various
classes with strong visual similarity, the PCL can also be improved to tackle such a similarity and lead to
even better OOD detection. Such visual similarities can be found in the speed limit sign classes, i.e., 30, 50,
70, and 80 km/h, wherein the overall shape of the traffic signs is identical except for one or two digits that
distinguish one sign from the others.
An illustration of our PCL training approach can be found in Figure 2. As shown in the figure, during the
DNNtraining, thefeaturerepresentationsareextractedfromoneormorelayersoftheDNN,whereindifferent
PCL losses are calculated. These losses are then aggregated and combined with the main cross-entropy (CE)
loss of the classifier to force it to better disentanglement the embedded space representations. A hypothetical
illustrationofthepossiblefeaturescanbefoundinFigure3.ItshowsthattheDNNstrainedwithconventional
CE loss would generate feature space representations for various classes that fall very close to each other. We
show that this can be improved by utilizing the PCL while training the DNNs for the common classification
tasks.
Input Conv
Layer 1
Layer N
Pooling
Flatten
Fully
ConnectedLPCPer-Classp-norm balls
RepresentationsPer-Classp-norm balls
RepresentationsLPC
L=LCE+LPC. . .
Figure 2: The overview of a common DNN utilized with PCL loss along with the CE loss. During the
training, we first compute the conformity losses at two different locations (before and after flattening the
embedded representations) and then average them together in the final CE loss. Our WPCL and MSPCL
employ the same representations for their computation but differ compared to the original PCL loss, further
improving the disentanglement for similar classes.
3Under review as submission to TMLR
CE PCL MSPCL
Figure 3: A simplified illustration of filter activations in different training setups including CE: the
conventional cross-entropy loss; PCL: the original PCL Mustafa et al. (2021) which can disentangle dissimilar
classes, while similar classes would still lead to having overlapping p-norm balls; MSPCL: our MSPCL as a
variation of PCL, where a p-norm ball is defined for each group of similar classes (light gray), and individual
classes p-norm balls (dark gray). Different colors depict different classes, dots depict clean data samples,
and stars depict adversarial or other outliers, leading to miss-classifications.
However, as traffic sign classification involves distinguishing classes with strong visual similarity, we propose
two novel variations of the original PCL, namely weighted PCL (WPCL) and multi-scale PCL (MSPCL) to
force the DNNs to generate sparse feature space representations for such similar classes. In our experiments,
we show that such an approach outperforms the CE and PCL across multiple DNNs and OOD detection
methods. An actual illustration of our hypothesis can be found in Figure 1. This figure illustrates the
activations extracted from the WideResNet50 Zagoruyko & Komodakis (2016) classifier while trained with
all four training methods. The activations include clean test samples with colorful dots and attacked images
as OOD with red star markers. Each color in this figure represents either a group of similar classes or a
single class if it does not belong to any group. The groups are discussed in detail in Section 4.1. It shows
that the DNN generated activations for all the clean samples for the common CE training, which fall very
close to each other. Furthermore, in contrast to the aforementioned problem, our proposed PCL variations
lead to better feature disentanglements compared among similar classes to CE and the original PCL training
method. It can be observed that the clusters of similar classes that share the same colors fall very close to
each other for both the CE and PCL training methods. This is mitigated in our training approaches by
pushing such clusters away from each other, which is the purpose of such a grouping.
Accordingly, our contributions are as follows:
•Propose to apply the contrastive PCL loss function from literature to enhance feature-based OOD
detection algorithms.
•Propose weighted prototype conformity loss WPCL and multi-scale prototype conformity
loss MSPCL to tackle the problem of very similar classes disentanglement in the embedded space.
•Conducted experiments over six DNNs, two traffic sign classification datasets, four image
augmentation techniques, and six OOD detection methods. Our MSPCL outperforms the CE and
PCL by 3.36% and 3.20% in AUROC and 5.15% and 1.00% in FPR across six DNNs and five ODD
methods in our challenging GTSRB benchmark.
The rest of this paper is organized as follows: we review the related work in Section 2; our proposed methods
are introduced in Section 3; the experiment setup is explained in detail in Section 4; the results are discussed
in Section 5; and the paper is concluded with the key achievements and possible future work in Section 6.
2 Related Work
During recent years, ensuring the safety of DNN within safety-critical systems has received great attention
from the research community Aravantinos & Schlicht (2020); Sämann et al. (2020); Schwalbe et al. (2020);
Schwalbe & Schels (2020); Willers et al. (2020); Heyn et al. (2023); Goodloe (2022). This is due to
4Under review as submission to TMLR
the fact that the output of DNNs is sensitive to the changes in the input data including adversarial
augmentations Goodfellow et al. (2014); Kurakin et al. (2017); Eykholt et al. (2017); Moosavi-Dezfooli
et al. (2017); Samangouei et al. (2018); Brown et al. (2018); Soll (2019); Feng et al. (2021); Hu et al. (2022)
or other factors including damage, vandalism, etc. Bielik et al. (2020); Magnussen et al. (2020); Wali et al.
(2019). This poses a significant safety concern to DNNs deployment to the real world applications.
The aforementioned adversarial attacks can be divided into two categories. In the first category, attacks
aim to introduce malicious content to all images (with or without any specific target class). Such methods
require access to the images before DNN inference to augment them with the adversarial content Madry et al.
(2018); Soll (2019). On the other hand, the second category includes patch-based attacks to the detectable
objects (e.g., traffic signs in our case), where the attacks are optimized over only a small patch of the image
by adding adversarial stickers to partially occlude such objects Eykholt et al. (2017); Brown et al. (2018);
Feng et al. (2021); Sitawarin et al. (2018).
Eykholtet al. Eykholt et al. (2017) introduced Robust Physical Perturbations (RP2), which is a one-to-one
attack (one source to one target class) for targeting traffic sign classifier i.e., DNN. RP2 is optimized to
mislead the classifier from the source class to a target class. The generated patches were applied to traffic
signs of the real world. Similarly, Brown et al. Brown et al. (2018) introduced the Adversarial Patch (AP),
which is a many-to-one attack (multiple source classes to a single target class). The goal of this attack is
to deviate the attacked DNN’s detection from all source classes to a single target class. Unlike RP2, AP is
location, rotation, and scale-invariant, which allows it to be applied anywhere on the object with different
rotations and scales. Similar to Eykholt et al. (2017), Sitawarin et al. Sitawarin et al. (2018) proposed an
OOD adversarial attack for traffic sign dataset. Recently, Bayzidi et al. Bayzidi et al. (2022) discussed the
main challenges of applying realistic stickers to traffic signs with augmentation techniques. They proposed
a multi-step approach of conventional image processing techniques to adjust the sticker overlays to the
individual traffic sign images with varying properties.
While adversarial attacks and other augmentation methods have proven to be able to mislead the
state-of-the-artDNNsintohighlyconfidentwrongpredictions,detectingexamplesdeviatingfromthetraining
distribution as outliers have been extensively studied in the literature as well Feinman et al. (2017); Lee et al.
(2018); Ma et al. (2018); Pang et al. (2018); Xu et al. (2018); Papernot & Mcdaniel (2018); Carrara et al.
(2019); Cohen et al. (2020). We can divide these approaches into two different branches. The first branch
consists of the post-hoc approaches, which mainly rely on an outlier scoring function based on the output
of a trained DNN. Such approaches include confidence scores, uncertainty-based approaches, or functions
like the energy applied on the DNN outputs. A simple baseline (MSP) Hendrycks & Gimpel (2017) by
Hendrycks et al. detects outliers based on the maximum SoftMax value. The ODIN Liang et al. (2018)
method by Liang et al. extends MSP by perturbing the DNN inputs. As an alternative to the maximum
SoftMax criterion Hendrycks et al. Hendrycks et al. (2022) evaluated the maximum logit value (MaxLogit)
and the KL divergence computed on the mean class-conditional logits. Lee et al. Lee et al. (2018) suggested
the Mahalanobis distance as a detection metric that one can use to discover the outlier data points in latent
space. Liu et al. Liu et al. (2020) propose the energy function as a metric based on the logits. The ReAct
approach Sun et al. (2021) by Sun et al. extends the energy method by rectifying activations prior to logit
computation. In ViM Wang et al. (2022) by Wang et al. , a statistical subspace projection is applied on the
last layer weights to enhance energy-based detection. Recently, Sun et al. Sun et al. (2022) investigated the
distance function of k nearest neighbors to score outliers.
The second branch consists of training-based detection introducing additional regularization objectives.
These aim to either optimize a specific detection function or enhance post-hoc algorithms. This can be
implemented by adding outlier data during training and enforcing separable score distributions for such
samples and the training distribution Hendrycks et al. (2019), Liu et al. (2020), Tao et al. (2023). Moreover,
others fit the features of a DNN to probabilistic models during training to better distinguish inlier and outlier
feature embeddings at test time Du et al. (2022b), Du et al. (2022a). Following the recent advances in
contrastive learning, Tack et al. Tack et al. (2020) proposed a training framework based on SimCLR Chen
et al. (2020) using transformed images and a specific scoring criterion. Sehwag et al. Sehwag et al. (2021)
similarly detect outliers by employing contrastive learning on transformed images and the Mahalanobis
distance as a score. Ming et al. Ming et al. (2023) combine probabilistic modeling of features and contrastive
5Under review as submission to TMLR
losses to create separable embeddings in the hyperspherical space. This leads to distant and compact latent
embeddings for each class. One contrastive training objective that eliminates the need for identifying and
applying suitable input transformations is the conformity loss Kapoor et al. (2020; 2021); Mustafa et al.
(2021). Mustafa et al. Mustafa et al. (2021) proposed the prototype conformity loss (PCL) combined
with the conventional cross-entropy loss function for training to increase robustness against adversarial and
non-adversarial perturbations of DNNs. This is achieved by forcing the latent activations of different classes
to be maximally separated from each other, a desirable property for OOD detection. Nevertheless, to the
best of our knowledge, there are no OOD detection approaches that benefit from such an optimization based
on only clean input samples without the need for additional transformations. Therefore, we extend the PCL
to enhance OOD detection algorithms and propose two novel variations specifically designed for the OOD
detection task.
3 Method
DNNs often produce confident output scores, even when misclassifying data. However, it is possible to detect
if OOD data is inferred into the DNN under test. This is done by evaluating the distance of the bespoken
data sample with the group of clean data samples from the training dataset using the latent activations of the
DNN. Accordingly, multiple feature-based OOD detection methods have been proposed in recent literature.
We want to emphasize that the DNN is trained only with clean data, and no augmented data, such as
adversarial or noisy data, is involved during the training.
Furthermore,inmostreal-worldapplications,therecanbeoverlapsorcloseactivationsofthetrainingsamples
of different classes due to their semantic similarity. For example, in the case of traffic sign classification, the
activations of speed limit classes would be close to each other, as their only distinguishing features are mostly
one or two digits, such as 30km/h and 80km/h. This can lead to sensitivity of the DNNs against OOD and
adversarial data on one side and poor OOD detection in latent space on the other side, as anomalous data
samples might be projected very close to clean ones in latent space. Therefore, we argue that there is a
need to alter the training process of such DNNs to also clearly separate the activations of different classes
from each other. Consequently, different classes should have prototype p-norm balls with minimum or no
overlaps.
To achieve this, we propose two novel loss functions based on the PCL Mustafa et al. (2021) to force the
activations of different classes to be separated, which would support better OOD detection based on such
activations. Theseincludeweightedprototypeconformityloss(WPCL)andmulti-scaleprototypeconformity
loss (MSPCL). Firstly, we discuss the details of the conventional PCL in this section. Following that, we
discuss the problems of applying PCL to real-world applications such as traffic sign classification, which
involves various similar classes. Finally, we introduce our novel prototype conformity losses, which can
outperform the standard PCL in OOD detection capability. For our ablation study, we apply multiple OOD
detection methods to latent activations for different training variations, including normal cross-entropy, PCL,
WPCL, and MSPCL training.
To make things easier to understand in the subsequent sections, let’s first discuss some mathematical
notations, which this section will consistently follow.
Notation: Letxxx= (xixixi)∈RC×H×Wandyyy={1,...,c}denote an input-label pair, where cdenotes the
number of classes in the dataset. We denote the DNN which consists of Llayers by Fθ:xxx→O, whereθare
the trainable parameters and Orepresents the classification output of the DNN. Further, the DNN outputs a
feature representation i.e., fff=OL−1∈Rd, which are used by the classification layer to perform multi-class
classification and produce O. The parameters of Fcan be represented as WWW= [w1w1w1,...,wcwcwc]∈Rd×c.
Prototype Conformity Loss (PCL) Mustafa et al. (2021) : As mentioned earlier, the goal of such a
loss is to disentangle different classes in latent space. Specifically, it considers a p-norm ball Pfor each class,
wherein the goal of PCL would be to 1) minimize the possible overlaps among the hyper-balls of different
classes, and 2) encourage all the data samples activations to fall into their respective class hyper-balls. This
way, the centroids of the p-norm balls are learned automatically to maintain a maximum distance from each
other, eventually leading to no overlap among them. Thus, for calculating the PCL loss, the proximity pis
6Under review as submission to TMLR
first calculated as follows:
p(xxxi,yyyi) =||fffi−wwwc
yi||2
2, (1)
wherewwwc∈Rdis the p-norm ball optimizable centroid of class c. Furthermore, the contrastive proximity
i.e.,cpbetween two classes can be calculated as follows:
cp(i,j) =||fffi−wwwc
j||2
2+||wwwc
yi−wwwc
j||2
2. (2)
Finally, the PCL is calculated as follows:
LPC(xxx,yyy) =/summationdisplay
i/braceleftig
p−1
c−1/summationdisplay
j̸=yi/parenleftbig
cp(i,j)/parenrightbig/bracerightig
,(3)
As shown in the Equations 2 and 3, the second part for calculating the contrastive proximity of class iwith
all the other classes is averaged over all the classes in the dataset. In the case of real-world applications
such as traffic sign classification, this would lead to the minimum effect of such loss functions for classes
with strong visual similarity, such as speed limit signs. For example, if xibelongs to the class 30km/h, it
could lead to a high cpwith similar classes, such as 50km/h or 80km/h. However, as the cpis averaged
over all the classes, including dis-similar ones, it would be decreased and lead to projecting such similar
classes close to each other in the latent space, despite being trained with such a proximity loss. Therefore,
it is essential to distinguish the proximity loss calculated for similar classes from dis-similar ones so that the
DNN is penalized more when projecting such similar classes close to each other. Based on this proposition,
we propose two novel alternations of PCL to overcome this problem in the following.
Weighted Prototype Conformity Loss (WPCL) : As mentioned earlier, the visual similarity of certain
classes would minimize the effect of the contrastive proximity cpdepicted in the Equation 2, when they are
averaged over all classes. Therefore, we propose to alter the cpwith a hyper-parameter to force the similar
classes with a stronger cp. Based on that:
WLPC(xxx,yyy) =/summationdisplay
i/braceleftig
p−1
c−1/summationdisplay
j̸=yi

λ∗cp j∈Si
cp j̸∈Si/bracerightig
, (4)
whereλ >1andSiare the sets of all the similar classes that the current image xxxfalls into, which can be
defined manually for each dataset. This is specifically simple for traffic signs as they can be grouped based
on their structural and visual similarities, such as shape, color, texture, etc. This way, if the activation fffi
of imagexxxifalls into the p-norm balls of a similar class, it will be additionally penalized for the respective
similar class compared to the other, non-similar, classes.
Multi-Scale Prototype Conformity Loss (MSPCL) : Similar to WPCL, we propose the MSPCL as
an alternative to the original PCL to tackle the problem of overlapping hyper-balls of similar classes. In
this method, we also enforce class similarity based on groups defined similarly to WPCL while considering
that there might be single classes that do not belong to any group. For example, in the case of traffic sign
classification, there can be groups of speed limit signs, triangular danger signs, and single-class groups, e.g.
the stop sign. Derived from that, the PCL will be calculated in two scales: group scale and single class scale.
Accordingly, if a class belongs to a group, such as the speed limit, or is a single class, such as a stop sign,
then a proximity pgand a contrastive proximity cpgwill be calculated on the group level as follows:
pg(xxx,yyy) =||fffi−wwwg
yi||2
2 (5)
and
cpg(i,j) =||fffi−wwwg
j||2
2+||wwwg
yi−wwwg
j||2
2, (6)
wherewwwgis the p-norm ball for the group that the image class xxxibelongs to. If the class does not belong to
any group, it will only have a p-norm ball in the group scale. Otherwise, another PCL is calculated within
each group with more than one class. This is determined based on Equations 1, 2 and 3 with the difference
that the centers of the hyper-balls in this scale are subtracted from their respective group center so that
7Under review as submission to TMLR
they are forced to adopt themselves to their respective group centroids. Finally, the LMSPCis calculated as
follows:
MSLPC(xxx,yyy) =
/summationdisplay
i/braceleftig
pc+pg−1
c−1/summationdisplay
j̸=yi/parenleftbig
cpc(i,j) +cpg(i,j)/parenrightbig/bracerightig
,(7)
wherepcandcpcare calculated for each class, and pgandcpgare calculated for each group of classes. If
a class does not belong to any group, only pgandcpgare calculated; therefore, pcandcpcwill be zero. A
simplified example of different training methods and their effect on encoding is illustrated in Figure 3. As
shown in sub-figure (a) of this figure, the cross entropy training would lead to many overlapping clusters,
makingfindinganomalousdatasampleschallenging. Thereasonisthattheprobabilisticmethodsrelymostly
on data distribution to compare novel data samples with and define them as inliers or OOD. In sub-figure (b),
one can see an example of training with PCL, which leads to the better disentanglement of activation clusters
ofdissimilarclassesbutwouldleadtohyper-ballsofclasseswithsimilarshapesstilloverlapping. Insub-figure
(c), our MSPCL aims to disentangle in different scales and achieve better hyper-ball disentanglement and,
thus, better OOD detection capabilities.
Combined WPCL and MSPCL : We also performed experiments on combining both of our proposed
approaches to provide a better understanding of the joint effect on the training and the OOD detection
results. To do so, we considered a similar weighting of the combined loss as follows:
COMPC(xxx,yyy) = 0.5∗MSLPC(xxx,yyy) + 0.5∗WLPC(xxx,yyy), (8)
whereMSLPC(xxx,yyy)andWLPC(xxx,yyy)are calculated based on Equations 7 and 4, respectively.
4 Experiments
In our experiments, we utilized six DNNs, namely ResNet18 and ResNet50 He et al.
(2016), WideResNet50 Zagoruyko & Komodakis (2016), ResNeXt50 Xie et al. (2017), VGG-16 and
VGG-19 Simonyan & Zisserman (2015). Furthermore, we evaluate our two novel variants i.e., WPCL and
MSPCL, and their combination on two automated driving datasets including two traffic sign classification
datasets Stallkamp et al. (2011) and Temel et al. (2017). Moreover, we also used various OOD methods
i.e., two variants including Isolation Forest (IF) Liu et al. (2008), Virtual Logit Matching (ViM) Wang
et al. (2022), KL Matching Hendrycks et al. (2022), Max Softmax Probability (MSP) Hendrycks &
Gimpel (2017), Energy-based methods Liu et al. (2020), ODINLiang et al. (2018), ReAct Sun et al. (2021),
Maximum Logit Value (MaxLogit) Hendrycks et al. (2022) and k-NN based approaches Sun et al. (2022).
In the following, we explain our experiments per each category. This includes our training setup for the
DNNs and the datasets, as well as our grouping of similar classes needed for our WPCL and MSPCL
approaches. Followed by that, we explain the adversarial and non-adversarial augmentation methods used
for generating OOD data on the GTSRB dataset. Finally, we explain the probabilistic methods used for
OOD detection at the end of this section.
4.1 Datasets
We used the German Traffic Sign Recognition Benchmark (GTSRB) dataset Stallkamp et al. (2011) for
training the mentioned DNNs that includes 50,000images with 43classes. The image sizes used for training
are299×299pixels. This dataset is then grouped into five different groups as well as four single classes.
The groups include 1) speed limit signs with 9classes; 2) triangular warning signs with 15classes; 3) round
blue direction signs with 8classes, 4) no passing, no passing trucks, no trucks, and no vehicles; 5) end of
all speed and passing limits, end of passing limit and end of passing truck limit. Individual classes include
yield, stop, no entry, and right of way.
Furthermore, weextendedourexperimentstotheCURE-TSRrealdataset, whichincludes 49videosequences
with 14classes. We followed the same image size for this dataset as the GTSRB. The groups in this dataset
8Under review as submission to TMLR
include 1) goods vehicles and no overtaking; 2) no stopping and no parking; and 3) no left, no right, and
priority. Individual classes include speed limits, stop, bicycle, hump, no entry, yield, and parking.
4.2 Training Parameters
As mentioned earlier, we have trained four DNNs including the ResNet18 and ResNet50 He et al.
(2016), WideResNet50 Zagoruyko & Komodakis (2016), ResNeXt50 Xie et al. (2017), and VGG16 and
VGG19 Simonyan & Zisserman (2015). Each of these DNNs is trained four times including with the
conventional cross-entropy loss (CE), the prototype conformity loss (PCL) Mustafa et al. (2021), our WPCL,
and our MSPCL. For all these types of training, we have used the ADAM optimizer with a learning rate
of1e−4. Furthermore, for optimizing the p-norm balls in all the PCL-based training, we have used the
stochastic gradient descent SGD optimizer with a learning rate of 5e−1for the proximity optimizer and
1e−4for the contrastive proximity optimizer.
4.3 OOD Data
Figure 4: Examples of different OOD samples from both of the GTSRB Stallkamp et al. (2011) and the
CURE-TSR Temel et al. (2017) datasets. The OOD data based on the GTSRB dataset is generated as part
of the experiments conducted for this paper, while the OOD data based on the CURE-TSR dataset are part
of the dataset itself.
We applied different augmentation methods to the GTSRB dataset to generate OOD data. These OOD
datasets were then used to test the OOD detection performance. Based on that, we applied the RP2 Eykholt
et al. (2017) and AP Brown et al. (2018) from the literature for attacking the image classifiers above. The
RP2 attacks are optimized on five classes out of the 43classes present in the GTSRB dataset, which include
30km/h, yield, STOP sign, no entry, and children crossing. Besides the two adversarial methods above, we
appliedtheMCSAfromBayzidietal.(2022)tothementionedfiveattackedclasses. Asalltheaforementioned
augmentation methods introduced occlusion patterns to the input images, we also applied the well-known
projected gradient descent (PGD) Madry et al. (2018) attack from the literature, which is an adversarial
imperceptible attack. Moreover, we collected 261images from the internet as real OOD data which include
classes that are not presented in the dataset, heavily occluded images due to vandalism, snow, dirt, etc. .
For the CURE-TSR dataset, there are 12augmentations per video sequence with five severity levels to
reproduce real-world conditions that might challenge the traffic sign detectors, which are introduced by the
authors. There we did not apply any further augmentation methods or adversarial attacks to this dataset
and used the available augmentations. However, such OOD data are not used during the training of the
DNNs and are solely used for testing the OOD detection methods during the test phase.
Examples of different augmentation methods leading to OOD data samples are illustrated in Figure 4, which
include examples generated based on the GTSRB dataset (top row) and examples from the CURE-TSR
dataset (bottom row). The first column includes clean images, while the rest are OOD data. These GTSRB
examples in columns two to four are all generated for the ResNet18 DNN. The last column on the top row
includes a real-world traffic sign covered with snow.
9Under review as submission to TMLR
4.4 OOD Detection Benchmark
We evaluate all trained DNNs using the publicly available benchmark PyTorch-OOD Kirchheim et al. (2022).
The complete benchmark consists Mahalanobis Lee et al. (2018), ViM Wang et al. (2022), ReAct Sun et al.
(2021), KNN Sun et al. (2022), and IsolationForest Liu et al. (2008). We employ the same parameters
for every evaluation, setting ϵ= 0.002for Mahalanobis, the projection dimension of d= 64for ViM, and
k= 1000classes for KNN. Feature-based approaches are fit on the penultimate layer activations for all
architectures. For our benchmark, we report two widely used metrics in the OOD detection domain. One
is the AUROC which is the area under the ROC curve, describing the relationship of the TPR and FPR.
Here, the higher the better. In addition, we report the FPR@95% metric measuring the false positive rate
at a true positive rate threshold of 95%. Here, lower values are better.
5 Results and Discussion
In this section, we review the results of our experiments along with our observations, which are conducted
over multiple DNNs and adversarial and non-adversarial augmentations to generate OOD data. The
training methods include the conventional cross-entropy loss (CE), the original prototype conformity
loss (PCL) Mustafa et al. (2021), our weighted version of the PCL method (WPCL), and our multi-scale
version of the PCL method (MSPCL), and their combination (COM). The augmentation methods include
the adversarial patch (AP) Brown et al. (2018), PGD Madry et al. (2018), RP2 Eykholt et al. (2017) and
MCSA Bayzidi et al. (2022). The results are averaged over all the classes regardless of whether the OOD
data samples inferred into those DNNs are classified correctly or not. The reason for such a decision is that
although the OOD data might be classified correctly by the DNN, it is still OOD data and, therefore, needs
to be classified as OOD as well.
We first show the classification results on GTSRB and CURE-TSR datasets in Table 5. Given GTSRB
dataset, on average, CE outperforms its counterparts PCL, WPCL, and MSPCL in F1-score by a very small
margin of 0.19%,0.15%and0.72%and in accuracy by 0.19%,0.09%and0.46%respectively. However, on
CURE-TSR dataset, our proposed PCL variant, i.e., WPCL outperforms CE, PCL, and MSPCL in F1-score
by a significant margins of 3.36%,1.63%and1.97%but in accuracy, PCL exceeds the performance of CE,
WPCL, and MSPCL by a subtle margins of 0.22%,0.17%and20.97%respectively. Furthermore, we would
like to emphasize that our goal in this paper is not to increase the classification performance or the robustness
of the studied DNNs against the OOD data. On the other hand, by reporting these results, one can conclude
that, except for certain DNNs, our proposed training approaches not only do not lead to a significant
classification performance drop but also lead to an increase in classification performance for certain DNNs,
such as ResNet18 and ResNet50 and datasets, such as the CURE-TSR. Examples of noticeable drops in
performance can be observed in specific training setups, such as VGG16 and ResNeXt50 on the CURE-TSR
dataset. This aligns with our motivation to introduce our training methods, which encourage the dispersed
projection of latent activations for similar classes, which are not as well annotated in CURE-TSR compared
to GTSRB. In fact, instead of precise annotation of classes such as speed limit signs, they are all grouped
into singular classes, which then neutralizes the effectiveness of our approach.
The overall results of the OOD detection on the GTSRB dataset are presented in Table 2. The results are
presented based on each DNN optimized four times with the different optimization methods (i.e., CE, PCL,
WPCL, MSPCL) and tested with six of the OOD detection methods (IF, class-based IF, KNN, Mahalanobis
distance, ReAct, and ViM). The reported metrics are AUROC and false positive rate. This table’s last
two columns represent each row’s average results per metric. According to this table, it is observable that
optimizing the DNNs with different versions of PCL leads to better OOD detection compared to conventional
cross-entropy training.
It can be observed that except for the ResNet18 and ResNet50, all of the other tested DNNs achieved
better OOD detection results on both of the reported metrics while they were optimized using either of our
proposed PCL variations. Based on that, averaged over all the tested DNNs and OOD detection methods,
the optimization with CE loss achieved 55.29in AUROC and 79.36in FPR. Moreover, the optimization with
the original PCL achieved 55.45and75.21on both of the metrics, respectively. However, with our proposed
10Under review as submission to TMLR
Model GTSRB CURE-TSR
Accuracy Precision Recall F1-Score Accuracy Precision Recall F1-Score
ResNet18CE 99.26 98.99 99.32 99.12 95.74 74.81 81.53 75.44
PCL 99.17 98.96 99.02 98.96 97.21 74.77 75.47 75.06
WPCL 99.35 99.15 99.32 99.22 97.09 81.08 81.36 81.15
MSPCL 99.52 99.37 99.50 99.43 94.12 78.83 81.14 79.73
COM 99.03 98.72 98.91 98.77 - - - -
ResNet50CE 99.02 97.95 98.59 98.13 95.77 68.64 68.12 68.30
PCL 98.97 97.68 98.66 97.89 95.95 80.00 85.64 78.94
WPCL 99.18 98.21 98.97 98.39 94.09 71.97 74.19 72.76
MSPCL 98.76 97.78 98.78 98.14 96.49 74.30 75.20 74.68
COM 98.98 98.62 98.33 98.38 - - - -
WideResNet50CE 99.28 98.89 99.05 98.90 95.02 69.26 74.84 69.49
PCL 98.67 98.40 97.83 98.02 95.47 73.57 74.68 74.02
WPCL 99.12 98.80 98.89 98.81 96.04 80.66 81.46 80.99
MSPCL 98.92 97.46 98.46 97.62 95.17 79.55 80.75 79.94
COM 98.06 96.99 97.39 96.90 - - - -
ResNeXt50CE 99.06 99.01 98.60 98.74 96.88 81.57 82.20 81.83
PCL 99.27 99.00 98.95 98.94 97.72 83.84 90.16 85.47
WPCL 99.06 98.14 98.98 98.40 96.97 79.58 82.73 80.88
MSPCL 99.11 98.91 98.45 98.65 96.97 76.78 76.72 76.67
COM 98.94 98.49 98.03 98.16 - - - -
VGG16CE 98.99 97.51 98.54 97.74 95.11 73.53 74.37 73.80
PCL 98.69 97.38 98.62 97.78 93.79 72.41 72.09 72.07
WPCL 98.90 97.87 98.13 97.82 94.51 68.09 68.55 68.22
MSPCL 97.59 96.93 96.66 96.66 92.50 64.67 68.95 66.52
COM 98.46 98.46 98.19 98.29 - - - -
VGG19CE 99.21 98.76 98.78 98.71 94.42 72.74 73.87 73.18
PCL 98.95 98.89 98.48 98.63 94.12 67.71 66.41 66.91
WPCL 98.69 98.39 97.48 97.80 94.54 78.23 78.60 78.22
MSPCL 98.19 96.76 96.70 96.52 93.37 72.98 72.97 72.87
COM 98.03 98.23 96.92 97.44 - - - -
AverageCE 99.14 98.52 98.81 98.56 95.49 73.43 75.82 73.68
PCL 98.95 98.39 98.59 98.37 95.71 75.38 77.41 75.41
WPCL 99.05 98.43 98.63 98.41 95.54 76.60 77.82 77.04
MSPCL 98.68 97.87 98.09 97.84 94.77 74.52 75.96 75.07
COM 98.58 98.25 97.96 97.99 - - - -
Table 1: Training classification performance results of all the six DNNs with all four training methods
on both of the clean test datasets. The trained DNNs include ResNet18 and ResNet50 He et al.
(2016), WideResNet50 Zagoruyko & Komodakis (2016), ResNeXt50 Xie et al. (2017), and the VGG16
and VGG19 Simonyan & Zisserman (2015). The training methods include training with the conventional
cross-entropy (CE) loss, the PCL Mustafa et al. (2021), our WPCL, and our MSPCL. The performance
evaluation metrics include accuracy, precision, recall, and F1-score.
11Under review as submission to TMLR
ModelIF KNN Mahal. ReAct ViM Average
AUROC↑FPR↓AUROC↑FPR↓AUROC↑FPR↓AUROC↑FPR↓AUROC↑FPR↓AUROC↑FPR↓
ResNet18CE 63.66 86.28 68.16 67.32 34.99 97.95 68.41 75.97 69.26 75.89 60.9 80.68
PCL 73.63 57.28 39.49 79.53 33.99 88.02 85.77 43.25 51.56 65.14 56.89 66.64
WPCL 60.31 98.63 61.91 67.62 48.56 75.95 58.45 79.66 57.94 82.12 57.43 80.80
MSPCL 70.21 76.01 59.43 82.3 27.49 96.19 56.43 87.0 66.37 78.92 55.99 84.08
COM. 70.9 62.25 48.6 74.79 34.71 80.68 83.14 52.48 73.74 55.07 62.22 65.05
ResNet50CE 55.69 88.95 55.96 85.14 44.31 93.74 59.04 77.69 65.84 66.42 56.17 82.39
PCL 61.5 81.87 39.05 93.79 45.11 91.23 70.06 76.55 58.89 81.12 54.92 84.91
WPCL 45.69 93.45 44.84 94.69 19.5 64.59 59.49 80.27 46.78 88.51 43.26 84.30
MSPCL 74.99 61.21 53.66 91.25 21.23 99.85 49.82 76.65 51.98 74.5 50.34 80.69
COM. 64.23 71.2 49.48 75.45 2.94 97.07 57.39 87.59 61.45 71.65 47.1 80.59
WideResNet50CE 58.77 86.81 59.7 79.96 38.26 94.29 61.16 78.52 66.89 70.89 56.96 82.09
PCL 51.73 80.05 36.76 74.92 6.71 91.06 55.03 75.08 58.69 77.04 41.78 79.63
WPCL 61.08 93.4 52.81 89.57 43.09 98.53 49.87 88.46 49.5 90.99 51.27 92.19
MSPCL 74.77 67.6 66.09 79.93 44.85 85.82 49.8 79.47 61.00 74.40 59.30 77.44
COM. 67.71 73.83 56.32 85.41 17.37 96.44 61.43 85.59 66.04 72.49 53.77 82.75
ResNeXt50CE 66.88 81.43 63.97 79.77 3.16 99.82 62.06 84.35 26.96 99.52 44.61 88.98
PCL 58.94 82.32 47.3 78.5 58.55 80.8 74.62 69.96 52.73 91.6 58.43 80.64
WPCL 43.62 92.01 48.22 93.84 52.79 96.05 49.26 88.8 44.82 94.94 47.74 93.13
MSPCL 81.7 56.15 67.77 80.37 28.38 98.02 49.82 76.89 50.15 77.86 55.56 77.86
COM. 67.97 73.63 48.84 76.21 14.63 97.32 69.72 85.93 67.46 75.26 53.72 81.67
VGG16CE 70.32 55.61 34.95 86.23 37.76 78.29 82.13 56.33 81.03 72.75 61.24 69.84
PCL 70.53 55.44 34.3 83.26 61.69 62.41 83.94 51.93 80.65 69.85 62.27 64.58
WPCL 70.35 51.7 44.52 79.22 40.78 78.92 84.59 54.59 69.61 56.83 61.97 64.26
MSPCL 72.44 50.23 44.39 81.3 30.86 79.33 79.99 44.2 89.68 46.18 63.47 60.25
COM. 70.43 60.59 36.47 82.27 21.69 83.99 81.8 49.37 71.83 61.24 56.44 67.49
VGG19CE 69.4 58.55 37.49 80.18 34.88 79.72 50.29 75.19 67.15 67.2 51.84 72.17
PCL 66.04 74.25 35.1 80.79 76.39 50.24 48.23 77.53 46.77 91.5 54.51 74.86
WPCL 75.15 48.61 35.57 77.15 68.02 62.86 83.15 52.07 78.56 52.91 68.09 58.72
MSPCL 70.29 67.36 34.19 84.74 70.78 58.92 86.26 52.65 74.81 61.26 67.27 64.99
COM. 71.11 53.77 38.17 92.45 22.54 84.68 77.79 58.54 68.99 73.2 55.72 72.53
AverageCE 64.12 76.27 53.37 79.77 32.23 90.63 63.85 74.67 62.85 75.45 55.29 79.36
PCL 63.73 71.87 38.67 81.8 47.07 77.29 69.61 65.72 58.22 79.38 55.45 75.21
WPCL 59.37 79.63 47.98 83.68 45.46 79.47 64.14 73.97 57.87 77.72 54.96 75.65
MSPCL 74.07 63.09 54.25 83.31 37.27 86.35 62.02 69.48 65.67 68.85 58.65 74.21
COM. 68.73 65.88 46.31 81.1 18.98 90.03 71.88 69.92 68.25 68.15 54.83 75.01
Table2: TheOODdetectionresultsontheGTSRBdataset,thesixtrainedDNNs,fourtrainingmethods,and
the six OOD detection methods. The OOD data used for this evaluation is generated with five augmentation
methods and a dataset consisting of 260 OOD images collected from the internet. The optimization methods
include 1) the common cross-entropy (CE) loss; 2) the original PCL; 3) our WPCL; 4) and our MSPCL.
The OOD detection methods include IF, class-based IF, KNN, Mahalanobis distance, ReAct, and ViM. The
reported metrics are AUROC and false positive rate at 95percept true positive rate. The last two columns on
the right indicate the average over all the OOD detection methods on each row and each metric. Please note
that the OOD detection methods results are averaged over 5for different random seeds used for optimizing
the OOD detection methods.
12Under review as submission to TMLR
CE PCL WPCL MSPCL COM303540455055606570AUROC57.2763.8867.72
65.34
61.0263.9966.16
61.9065.27
58.59
34.6235.27
31.0441.62
36.3471.08
68.34
64.7667.39
58.8765.05
61.86
55.7168.27
55.9458.4059.10
56.2361.58
54.15
CE PCL WPCL MSPCL COM65707580859095FPR95TPR91.19
79.00
69.6676.57
74.4281.60
70.5478.65
76.73
73.2191.86
88.6894.6397.16
89.92
68.1169.9081.34
69.3070.8883.23
79.91
78.77
63.9677.2483.20
77.6180.61
76.7477.13AP
MCSA
PGD
RP2
RealOoD
Mean
Figure 5: OOD detection results across different training methods and OOD data types.
CE PCL WPCL MSPCL COM3040506070AUROC64.12 63.73
59.3774.07
66.58
53.37
38.6747.9854.25
45.99
31.1244.1546.39
38.55
25.9963.8569.61
64.14
62.0269.04
62.85
58.22 57.8765.67 65.64
55.06 54.87 55.1558.91
54.65
CE PCL WPCL MSPCL COM657075808590FPR95TPR76.27
71.8779.63
63.0969.5679.7781.8083.6883.31
82.5693.10
80.2779.6087.7688.09
74.67
65.7273.97
69.4873.7575.4579.38
77.72
68.8571.6879.85
75.8178.92
74.5077.13
IsolationForest
KNN
Mahalanobis
ReAct
ViM
Mean
Figure 6: OOD detection results across different training and OOD detection methods.
novel variants of PCL, i.e., WPCL and MSPCL, we comprehensively outperformed both the CE and PCL
with MSPCL on average. For WPCL, we achieved an AUROC of 54.96and an FPR of 75.65. Furthermore,
for our MSPCL, we achieved an AUROC of 58.65and an FPR of 74.21. Accordingly, compared to all the
other DNNs, the VGG16 has benefitted the most from our proposed training methodology, achieving a boost
of16.25in AUROC and 13.45in FPR. This indicates that in use cases such as traffic sign classification,
which includes a variety of similar classes, disentangling them using our proposed methods would be a more
promising approach to achieve better outlier detection.
The Figures 7 and 8 depict the predicted scores of an IsolationForest from our detection benchmark on the
clean versus RP2 attacked, as well as clean versus real OOD test samples. The IsolationForest was fitted
on a ResNet50 trained on CE, PCL, and our WPCL and MSPCL. It can be observed that the predicted
scores have a significant overlap for the CE-trained network, which makes distinguishing inliers and outliers
difficult. One can also observe that defining a threshold to separate in-distribution and OOD would only be
effectively possible for MSPCL due to its strong separability of clean and OOD data. These results are also
in coordination with the detailed results presented in Tables S.2 and S.1 in Supplementary Section A.
13Under review as submission to TMLR
DensityCE PCL
ID
RP2DensityMSPCL WPCL
Figure 7: Distributions of predicted scores on the clean test samples and RP2 attacked test samples by
Isolation Forest fitted on a ResNet50 He et al. (2016) trained with CE, PCL Mustafa et al. (2021), our
WPCL and MSPCL. The vertical line indicates the threshold, whereby 95 percent of the OOD data would
be selected as OOD.
Figures 5 and 6 illustrate the results across different training methods, OOD data types, and OOD detection
methods. One can observe from these plots that different OOD data types including different adversarial
attacks, real-world OOD data, and other augmentations, can achieve different results across different training
approaches. However, one can observe that except for RP2, in all the other OOD types, either of the PCL
variants outperforms the OOD detection when the DNNs are trained with cross-entropy loss. This indicates
a clear conclusion that training with such an approach is beneficial for OOD detection. On the other
hand, one can also observe that all the OOD detection methods benefited from either variant of the PCL
training. However, different results might also indicate inconsistency in selecting the best method for this
task. Therefore, it is important to note that each DNN architecture might benefit differently from such
training approaches, and one would need to perform extensive analysis for each architecture to select the
best configuration.
As mentioned earlier, we have also extended our experiments to the CURE-TSR dataset, which includes only
14classes. ThisledtothedefinitionofsmallergroupsforboththeWPCLandMSPCL.Therefore, theresults
of the different OOD detection performances on different training approaches on the CURE-TSR dataset are
presented in Table 3. The results are presented based on each DNN optimized four times with the different
optimization methods (i.e. CE, PCL, WPCL, MSPCL) and tested with five of the OOD detection methods
(IF, KNN, Mahalanobis distance, ReAct, and ViM). The reported metrics are AUROC and false positive
rate. The last two columns of this table represent the average results of each row per metric. Moreover, as
the dataset includes 12augmentation with 5severity levels each, we observed that up to severity level 3, the
augmentations did not lead to a major drop in the classification performance of the networks. In fact, levels
4 and 5 led to a more than 20percent decrease in the classification performance of the evaluated DNNs
on average. Therefore, we considered the data samples of these two levels as OOD data and evaluated the
benchmarked OOD detection methods only on these two levels. Accordingly, the results presented in Table 3
include only these two levels. According to these results, our proposed approaches can outperform the other
evaluated ones in some of the DNNs, such as VGG19, WideResNet50 (only on AUROC), and ResNet18.
However, it can be observed that OOD detection methods such as Mahalanobis distance benefit greatly from
our proposed approaches, in particular the WPCL.
We performed a dispersion analysis on the activations generated from the WideResNet50 DNN trained on
the GTSRB dataset. The results of this analysis are presented in Figure 9. In order to quantify how far the
14Under review as submission to TMLR
ModelIF KNN Mahal. ReAct ViM Average
AUROC↑FPR↓AUROC↑FPR↓AUROC↑FPR↓AUROC↑FPR↓AUROC↑FPR↓AUROC↑FPR↓
ResNet18CE 53.44 94.19 49.17 91.38 60.7 76.32 64.51 84.82 68.93 74.85 59.35 84.31
PCL 35.54 95.22 53.66 87.64 70.24 66.58 66.03 87.56 72.67 76.08 59.63 82.62
WPCL 38.82 90.76 53.88 86.19 70 67.64 65.79 86.73 71.5 78.24 60.0 81.91
MSPCL 60.21 87.86 64.29 83.46 63.94 73.72 65.32 85.39 69.66 78.84 64.68 81.85
ResNet50CE 55.43 92.68 53.64 92.7 55.89 63.06 65.05 83.7 71.08 74.95 60.22 81.42
PCL 39.82 93.95 51.33 88.06 69.1 63.65 63.3 90.61 68.49 84.65 58.41 84.18
WPCL 40.75 92.87 51.4 89.8 69.51 65.65 61.58 91.07 59.56 91.42 56.56 86.16
MSPCL 48.96 91.95 56.2 90.77 43 98.4 60.09 90.86 62.33 90.51 54.12 92.5
WideResNet50CE 57.87 91.03 53.76 90.35 17.09 73.22 64.3 64.3 67.63 80.34 52.13 83.67
PCL 44.14 92.95 51.27 86.98 60.58 67.63 67.7 86.92 69.31 83.66 58.56 84.23
WPCL 42.49 94.08 49.45 91.47 67.27 70.3 61.85 91.59 52.46 95.63 58.6 83.63
MSPCL 57.82 89.94 62.7 84.85 45.95 72.04 64.44 84.94 61.92 89.36 54.7 88.61
ResNeXt50CE 55.41 91.22 52.61 90.72 42.75 71.22 64.93 82.29 68.78 76.87 56.9 82.46
PCL 40.22 93.3 53.21 88.72 65.41 67.34 64.59 89.98 68.62 80.12 58.41 83.89
WPCL 37.9 96.65 48 92.59 67.65 68.37 56.22 91.49 35.74 95.61 49.1 88.94
MSPCL 39.26 94.12 50.2 91.46 66.18 71.6 62.38 90.62 50 94.88 53.6 88.54
VGG16CE 62.95 80.33 52.97 86.45 36.61 97.38 69.61 77.05 64.71 79.95 57.37 84.23
PCL 58.96 84.11 50.79 82.68 54.92 90.35 75.05 71.47 73.49 73.64 62.64 80.45
WPCL 54.09 89 51.64 86.98 61.3 85.25 71.94 72.06 71.99 70.17 62.19 80.69
MSPCL 53.45 86.38 46.3 90.39 63.42 82.56 70.4 76.07 71.5 74.04 61.01 81.89
VGG19CE 65.03 78.24 50.74 85.31 36.3 97.81 70.15 72.3 65.48 76.39 57.54 82.01
PCL 57.16 84 53.72 79.78 61.3 89.31 75.2 70.75 74.26 72.34 64.33 79.23
WPCL 53.44 87.24 55.27 76.33 68.11 79.03 77.98 60.69 77.33 62.01 66.43 73.06
MSPCL 48.44 91.91 49.12 87.5 53.84 90.01 71.32 73.51 69.07 75.36 58.36 83.66
AverageCE 58.35 87.95 52.15 89.48 41.56 79.83 66.42 80.59 67.77 77.22 57.25 83.02
PCL 45.97 90.59 52.33 85.64 63.59 74.14 68.65 82.88 71.14 78.41 60.34 82.33
WPCL 44.58 91.77 51.61 87.23 67.31 72.7 65.89 82.27 61.43 82.18 58.16 83.23
MSPCL 51.35 90.36 54.8 88.07 56.06 81.39 65.66 83.56 64.08 83.83 58.39 85.44
Table3: TheOODdetectionresultsontheCURE-TSRdataset, thesixtrainedDNNs, fourtrainingmethods,
and the six OOD detection methods. The OOD data used for this evaluation is generated with twelve
augmentation methods with five severity levels included in the dataset, out of which severity levels four and
five are considered OOD, and used for this evaluation. The optimization of the DNNs is done on only clean
data. The optimization methods include 1) the common cross-entropy (CE) loss; 2) the original PCL; 3) our
WPCL; 4) and our MSPCL. The OOD detection methods include IF, class-based IF, KNN, Mahalanobis
distance, ReAct, and ViM. The reported metrics are AUROC and false positive rate at 95percept true
positive rate. The last two columns on the right indicate the average over all the OOD detection methods
on each row and each metric.
DensityCE PCL
ID
Real OODDensityMSPCL WPCL
Figure 8: Distributions of predicted scores on the clean test samples and real OOD test samples by Isolation
Forest fitted on a ResNet50 He et al. (2016) trained with CE, PCL Mustafa et al. (2021), our WPCL and
MSPCL. The vertical line indicates the threshold, whereby 95 percent of the OOD data would be selected
as OOD.
15Under review as submission to TMLR
Speed Limits Warnings Directions Prohibitions End of Limits0510152025Inter-class Distance
Speed Limits Warnings Directions Prohibitions End of Limits0.000.020.040.060.080.10Intra-class Variance
Speed Limits Warnings Directions Prohibitions End of Limits0.000.050.100.150.200.250.30Intra-class Standard Deviation
CE
PCL
WPCL
MSPCL
Figure 9: Dispersion analysis based on inter-class distance, intra-class variance, and intra-class standard
deviation of the activations extracted from the WideResNet50 DNN trained on the GTSRB dataset. The
results are calculated and averaged among the grouped classes.
similar classes within each group are projected, we calculated the distance of activations generated for each
class with the other classes. To compare the compactness of each activation class, we measured the variance
and standard deviation, which are averaged over each of the groups. As shown in this figure, our WPCL
method achieved a bigger inter-class distance for all the groups while also achieving the lowest numbers in
intra-class variance and standard deviation for most of the groups. This indicates that the WPCL could
achieve more compact activations within classes while pushing them from other classes.
Despite the encouraging results showcasing the effectiveness of our proposed approaches in training the
studied DNNs for traffic sign image classification, we consider the following limitations to be addressed as
potential future work. While we only evaluated our methods on traffic sign image classification, we did not
extend our studies to other image classification tasks that might benefit from this approach. Therefore,
we propose to extend our methods to datasets containing multiple similar classes that could cause naive
disentanglement by the encoder. Moreover, as our experiments are conducted on the ResNet, WideResNet,
ResNeXt, and VGG DNNs, we propose to extend the experiments to other architectures to study their
differences and similarities as well. Due to the extent of our experiments, we did not perform any fine-tuning
of already trained DNNs, and therefore, we would encourage further research in this regard. On the other
hand, we only utilized one contrastive training approach, PCL, to showcase the challenges and potential
solutions when adopting SOTA solutions to real-world problems. Therefore, we also encourage further
research with other contrastive or similar training approaches that can benefit from such an adaptation
technique. Conducting experiments on different OOD methods, different attacks, and other augmentation
techniques to generate OOD data would also enrich this work. To increase the DNN’s robustness, we would
encourage further research using our proposed approaches and adversarial training. Finally, we would like to
emphasize that our work showcases an example solution for closing the gap between SOTA machine learning
and the safety assurance of such methods in safety-critical applications, wherein active monitoring of all the
components is essential to ensure safe operation in all scenarios including unforeseen malfunction of such
components. The authors, therefore, highly encourage further research in this area.
6 Conclusion
In this paper, we discuss the problem of OOD detection for traffic sign image classification methods. This
refers to the problem of CNN-based encoders where the projection of data samples of different classes lie
very close to each other in latent space when optimized solely with CE loss function, this leads to poor OOD
detection results based on CNN’s latent activations. Therefore, we first proposed to apply the prototype
conformity loss (PCL) function from the literature to the traffic sign classification problem. However,
due to very similar groups in the dataset e.g., 30km/h and 80km/h, PCL failed to disentangle them
properly. Therefore, to solve this problem, we introduced two novel variations of the PCL function, namely
weighted prototype conformity loss WPCL and multi-scale prototype conformity loss MSPCL, to enhance
the OOD detection rate for both clean data as well as OOD data. Finally, we have shown in our experiments
that our proposed variants, MSPCL outperforms the CE and PCL by 3.36%and3.20%in AUROC and
16Under review as submission to TMLR
5.515%and1.00%in FPR, across six DNN’s and five OOD detection methods on the GTSRB dataset which
includes various similar classes that can benefit from our proposed methodology. These results indicate that
it is possible to introduce optimization goals besides the main classification task that are tailored to the need
for monitoring DNNs within safety-critical applications during their runtime operations without disrupting
their classification capability.
References
Vincent Aravantinos and Peter Schlicht. Making the relationship between uncertainty estimation and safety
less uncertain. In 2020 Design, Automation Test in Europe Conference Exhibition (DATE) , pp. 1139–1144,
2020.
Yasin Bayzidi, Alen Smajic, Fabian Hüger, Ruby Moritz, Serin Varghese, Peter Schlicht, and Alois Knoll.
Traffic sign classifiers under physical world realistic sticker occlusions: A cross analysis study. In 33rd
IEEE Intelligent Vehicles Symposium (IV) , Jun 2022.
Pavol Bielik, Petar Tsankov, Andreas Krause, and Martin Vechev. Reliability Assessment of Traffic Sign
Classifiers . Federal Office for Information Security, Bonn, Germany, 2020. URL https://www.bsi.bund.
de.
Tom B. Brown, Dandelion Mané, Aurko Roy, Martín Abadi, and Justin Gilmer. Adversarial patch, 2018.
Fabio Carrara, Rudy Becarelli, Roberto Caldelli, Fabrizio Falchi, and Giuseppe Amato. Adversarial
examples detection in features distance spaces. In Laura Leal-Taixé and Stefan Roth (eds.), Computer
Vision – ECCV 2018 Workshops , pp. 313–327, Cham, 2019. Springer International Publishing. ISBN
978-3-030-11012-3.
Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive
learning of visual representations. In ICML, 2020.
Gilad Cohen, Guillermo Sapiro, and Raja Giryes. Detecting adversarial samples using influence functions
and nearest neighbors. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR) , June 2020.
Xuefeng Du, Gabriel Gozum, Yifei Ming, and Yixuan Li. Siren: Shaping representations for detecting
out-of-distribution objects. In Advances in Neural Information Processing Systems , 2022a.
Xuefeng Du, Zhaoning Wang, Mu Cai, and Yixuan Li. Vos: Learning what you don’t know by virtual outlier
synthesis. ICLR, 2022b.
Kevin Eykholt, Ivan Evtimov, Earlence Fernandes, Tadayoshi Kohno, Bo Li, Atul Prakash, Amir Rahmati,
and Dawn Song. Robust physical-world attacks on machine learning models. CoRR, abs/1707.08945, 2017.
URL http://arxiv.org/abs/1707.08945 .
Kevin Eykholt, Ivan Evtimov, Earlence Fernandes, Bo Li, Amir Rahmati, Chaowei Xiao, Atul Prakash,
Tadayoshi Kohno, and Dawn Song. Robust physical-world attacks on deep learning visual classification.
InProceedings of the IEEE conference on computer vision and pattern recognition , pp. 1625–1634, 2018.
Alhussein Fawzi, Seyed-Mohsen Moosavi-Dezfooli, and Pascal Frossard. Robustness of classifiers: from
adversarial to random noise. In Proc. of NIPS , pp. 1632–1640, Barcelona, Spain, December 2016.
Reuben Feinman, Ryan R. Curtin, Saurabh Shintre, and Andrew B. Gardner. Detecting adversarial samples
from artifacts, 2017.
Ryan Feng, Jiefeng Chen, Earlence Fernandes, Somesh Jha, and Atul Prakash. Robust physical hard-label
attacks on deep learning visual classification, 2021.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron
Courville, and Yoshua Bengio. Generative Adversarial Nets. In Proc. of NIPS , pp. 2672–2680, Montréal,
Canada, December 2014.
17Under review as submission to TMLR
Ian Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and Harnessing Adversarial Examples.
InProc. of ICLR , pp. 1–10, San Diego, CA, USA, May 2015.
Alwyn E. Goodloe. Assuring safety-critical machine learning enabled systems: Challenges and promise.
In2022 IEEE International Symposium on Software Reliability Engineering Workshops (ISSREW) , pp.
326–332, 2022. doi: 10.1109/ISSREW55968.2022.00088.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition.
In2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , pp. 770–778, 2016. doi:
10.1109/CVPR.2016.90.
Dan Hendrycks and Kevin Gimpel. A baseline for detecting misclassified and out-of-distribution examples
in neural networks. In ICLR, 2017.
Dan Hendrycks, Mantas Mazeika, and Thomas Dietterich. Deep anomaly detection with outlier exposure.
ICLR, 2019.
Dan Hendrycks, Steven Basart, Mantas Mazeika, Andy Zou, Joe Kwon, Mohammadreza Mostajabi, Jacob
Steinhardt, and Dawn Song. Scaling out-of-distribution detection for real-world settings. In ICML, 2022.
Hans-MartinHeyn,EricKnauss,IswaryaMalleswaran,andShruthiDinakaran. Aninvestigationofchallenges
encountered when specifying training data and runtime monitors for safety critical ml applications. arXiv
preprint arXiv:2301.13476 , 2023.
Zhanhao Hu, Siyuan Huang, Xiaopei Zhu, Fuchun Sun, Bo Zhang, and Xiaolin Hu. Adversarial texture for
fooling person detectors in the physical world. In Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition (CVPR) , pp. 13307–13316, June 2022.
Plotly Technologies Inc. Collaborative data science, 2015. URL https://plot.ly .
Nikhil Kapoor, Chun Yuan, Jonas Löhdefink, Roland Zimmerman, Serin Varghese, Fabian Hüger, Nico
Schmidt, Peter Schlicht, and Tim Fingscheidt. A self-supervised feature map augmentation (fma) loss and
combined augmentations finetuning to efficiently improve the robustness of cnns. In Computer Science
in Cars Symposium , CSCS ’20, New York, NY, USA, 2020. Association for Computing Machinery. ISBN
9781450376211. doi: 10.1145/3385958.3430477. URL https://doi.org/10.1145/3385958.3430477 .
Nikhil Kapoor, Andreas Bär, Serin Varghese, Jan David Schneider, Fabian Hüger, Peter Schlicht, and Tim
Fingscheidt. From a fourier-domain perspective on adversarial examples to a wiener filter defense for
semantic segmentation. In 2021 International Joint Conference on Neural Networks (IJCNN) , pp. 1–8,
2021. doi: 10.1109/IJCNN52387.2021.9534145.
Konstantin Kirchheim, Marco Filax, and Frank Ortmeier. Pytorch-ood: A library for out-of-distribution
detection based on pytorch. In CVPRW , 2022.
Alexey Kurakin, Ian Goodfellow, and Samy Bengio. Adversarial Examples in the Physical World. In Proc.
of ICLR - Workshops , pp. 1–14, Toulon, France, April 2017.
Kimin Lee, Kibok Lee, Honglak Lee, and Jinwoo Shin. A simple unified framework for detecting
out-of-distribution samples and adversarial attacks. In Proceedings of the 32nd International Conference
on Neural Information Processing Systems , NIPS’18, pp. 7167–7177, Red Hook, NY, USA, 2018. Curran
Associates Inc.
Shiyu Liang, Yixuan Li, and R. Srikant. Enhancing the reliability of out-of-distribution image detection in
neural networks. In ICLR, 2018.
Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou. Isolation forest. In 2008 Eighth IEEE International
Conference on Data Mining , pp. 413–422, 2008. doi: 10.1109/ICDM.2008.17.
WeitangLiu, XiaoyunWang, JohnOwens, andYixuanLi. Energy-basedout-of-distributiondetection. NIPS,
2020.
18Under review as submission to TMLR
Xingjun Ma, Bo Li, Yisen Wang, Sarah M. Erfani, Sudanthi Wijewickrema, Grant Schoenebeck,
Michael E. Houle, Dawn Song, and James Bailey. Characterizing adversarial subspaces using local
intrinsic dimensionality. In International Conference on Learning Representations , 2018. URL https:
//openreview.net/forum?id=B1gJ1L2aW .
AleksanderMadry, AleksandarMakelov, LudwigSchmidt, DimitrisTsipras, andAdrianVladu. Towardsdeep
learning models resistant to adversarial attacks. In International Conference on Learning Representations ,
2018. URL https://openreview.net/forum?id=rJzIBfZAb .
Angelica F. Magnussen, Nathan Le, Linghuan Hu, and W. Eric Wong. A survey of the inadequacies in traffic
sign recognition systems for autonomous vehicles. International Journal of Performability Engineering ,
16(10):1588, 2020. doi: 10.23940/ijpe.20.10.p10.15881597. URL http://www.ijpe-online.com/EN/
abstract/article_4490.shtml .
Yifei Ming, Yiyou Sun, Ousmane Dia, and Yixuan Li. How to exploit hyperspherical embeddings for
out-of-distribution detection? In ICLR, 2023.
Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, Omar Fawzi, and Pascal Frossard. Universal Adversarial
Perturbations. In Proc. of CVPR , pp. 1765–1773, Honolulu, HI, USA, July 2017.
Aamir Mustafa, Salman H. Khan, Munawar Hayat, Roland Goecke, Jianbing Shen, and Ling Shao. Deeply
supervised discriminative learning for adversarial defense. IEEE Transactions on Pattern Analysis and
Machine Intelligence , 43(9):3154–3166, 2021. doi: 10.1109/TPAMI.2020.2978474.
Tianyu Pang, Chao Du, Yinpeng Dong, and Jun Zhu. Towards robust detection of adversarial examples.
InProceedings of the 32nd International Conference on Neural Information Processing Systems , NIPS’18,
pp. 4584–4594, Red Hook, NY, USA, 2018. Curran Associates Inc.
Nicolas Papernot and Patrick Mcdaniel. Deep k-nearest neighbors: Towards confident, interpretable and
robust deep learning. ArXiv, abs/1803.04765, 2018.
Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish
Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning
transferable visual models from natural language supervision, 2021.
Pouya Samangouei, Maya Kabkab, and Rama Chellappa. Defense-GAN: Protecting Classifiers Against
Adversarial Attacks Using Generative Models. In Proc. of ICLR , pp. 1–17, Vancouver, BC, Canada, April
2018.
Gesina Schwalbe and Martin Schels. A survey on methods for the safety assurance of machine learning based
systems. 2020.
Gesina Schwalbe, Bernhard Knie, Timo Sämann, Timo Dobberphul, Lydia Gauerhof, Shervin Raafatnia,
and Vittorio Rocco. Structuring the safety argumentation for deep neural network based perception in
automotive applications. In António Casimiro, Frank Ortmeier, Erwin Schoitsch, Friedemann Bitsch,
and Pedro Ferreira (eds.), Computer Safety, Reliability, and Security. SAFECOMP 2020 Workshops , pp.
383–394, Cham, 2020. Springer International Publishing. ISBN 978-3-030-55583-2.
Vikash Sehwag, Mung Chiang, and Prateek Mittal. Ssd: A unified framework for self-supervised outlier
detection. In International Conference on Learning Representations , 2021. URL https://openreview.
net/forum?id=v5gjXpmR8J .
Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition.
In Yoshua Bengio and Yann LeCun (eds.), 3rd International Conference on Learning Representations,
ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings , 2015. URL http:
//arxiv.org/abs/1409.1556 .
Chawin Sitawarin, Arjun Nitin Bhagoji, Arsalan Mosenia, Mung Chiang, and Prateek Mittal. DARTS:
deceiving autonomous cars with toxic signs. CoRR, abs/1802.06430, 2018. URL http://arxiv.org/abs/
1802.06430 .
19Under review as submission to TMLR
Marcus Soll. Informaticup competition 2019: Fooling traffic sign recognition. In Christoph Benzmüller and
Heiner Stuckenschmidt (eds.), KI 2019: Advances in Artificial Intelligence , pp. 325–332, Cham, 2019.
Springer International Publishing.
Johannes Stallkamp, Marc Schlipsing, Jan Salmen, and Christian Igel. The german traffic sign recognition
benchmark: A multi-class classification competition. In The 2011 International Joint Conference on
Neural Networks , pp. 1453–1460, 2011. doi: 10.1109/IJCNN.2011.6033395.
Yiyou Sun, Chuan Guo, and Yixuan Li. React: Out-of-distribution detection with rectified activations. In
NIPS, 2021.
Yiyou Sun, Yifei Ming, Xiaojin Zhu, and Yixuan Li. Out-of-distribution detection with deep nearest
neighbors. ICML, 2022.
Timo Sämann, Peter Schlicht, and Fabian Hüger. Strategy to increase the safety of a dnn-based perception
for had systems, 2020.
Jihoon Tack, Sangwoo Mo, Jongheon Jeong, and Jinwoo Shin. Csi: Novelty detection via contrastive learning
on distributionally shifted instances. In NIPS, 2020.
Leitian Tao, Xuefeng Du, Jerry Zhu, and Yixuan Li. Non-parametric outlier synthesis. In The Eleventh
International Conference on Learning Representations , 2023. URL https://openreview.net/forum?id=
JHklpEZqduQ .
Dogancan Temel, Gukyeong Kwon, Mohit Prabhushankar, and Ghassan AlRegib. CURE-TSR: Challenging
unreal and real environments for traffic sign recognition. In Neural Information Processing Systems
(NeurIPS) Workshop on Machine Learning for Intelligent Transportation Systems , 2017.
Laurens van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of Machine Learning
Research , 9(86):2579–2605, 2008. URL http://jmlr.org/papers/v9/vandermaaten08a.html .
Safat B. Wali, Majid A. Abdullah, Mahammad A. Hannan, Aini Hussain, Salina A. Samad, Pin J. Ker, and
Muhamad Bin Mansor. Vision-based traffic sign detection and recognition systems: Current trends and
challenges. Sensors, 19(9), 2019. ISSN 1424-8220. doi: 10.3390/s19092093. URL https://www.mdpi.
com/1424-8220/19/9/2093 .
Haoqi Wang, Zhizhong Li, Litong Feng, and Wayne Zhang. Vim: Out-of-distribution with virtual-logit
matching. In CVPR, 2022.
OliverWillers, SebastianSudholt, ShervinRaafatnia, andStephanieAbrecht. Safetyconcernsandmitigation
approaches regarding the use of deep learning in safety-critical perception tasks. In SAFECOMP
Workshops , 2020.
Saining Xie, Ross Girshick, Piotr Dollar, Zhuowen Tu, and Kaiming He. Aggregated residual transformations
for deep neural networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition (CVPR) , July 2017.
Weilin Xu, David Evans, and Yanjun Qi. Feature squeezing: Detecting adversarial examples in deep neural
networks. In 25th Annual Network and Distributed System Security Symposium, NDSS 2018, San Diego,
California, USA, February 18-21, 2018 . The Internet Society, 2018. URL http://wp.internetsociety.
org/ndss/wp-content/uploads/sites/25/2018/02/ndss2018_03A-4_Xu_paper.pdf .
Jingkang Yang, Kaiyang Zhou, Yixuan Li, and Ziwei Liu. Generalized out-of-distribution detection: A
survey.ArXiv, abs/2110.11334, 2021. URL https://api.semanticscholar.org/CorpusID:239049401 .
Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. In Edwin R. Hancock Richard C. Wilson
and William A. P. Smith (eds.), Proceedings of the British Machine Vision Conference (BMVC) , pp.
87.1–87.12. BMVA Press, September 2016. ISBN 1-901725-59-6. doi: 10.5244/C.30.87. URL https:
//dx.doi.org/10.5244/C.30.87 .
20Under review as submission to TMLR
A Appendix
Supplementary Information
In this section, the extra information and details about our experiments and findings are presented.
Accordingly, the training details of all the DNNs and training methods are discussed in Section S.1.
Following that, the extra results of two individual OOD datasets from the GTSRB experiments are presented
in Section S.2. Finally, as different training methods lead to the different encoding of the image data
samples, especially in latent spaces of the DNNs, we have visualized and discussed the activations for the
WideResNet50 DNN in Section S.4, which include an attached video for better clarification.
S.1 Training Details
As mentioned earlier, we have trained four DNNs including the ResNet18 and ResNet50 He et al.
(2016), WideResNet50 Zagoruyko & Komodakis (2016), ResNeXt50 Xie et al. (2017), and VGG16 and
VGG19 Simonyan & Zisserman (2015). Each of these DNNs is trained four times including with the
conventional cross-entropy loss (CE), the prototype conformity loss (PCL) Mustafa et al. (2021), our WPCL,
and our MSPCL. For all these types of training, we have used the ADAM optimizer with a learning rate
of1e−4. Furthermore, for optimizing the p-norm balls in all the PCL-based training, we have used the
stochastic gradient descent SGD optimizer with a learning rate of 0.5for the proximity optimizer and 1e−4
for the contrastive proximity optimizer.
ModelIF KNN Mahal. ReAct ViM Average
AUROC↑ FPR↓ AUROC↑ FPR↓ AUROC↑ FPR↓ AUROC↑ FPR↓ AUROC↑ FPR↓ AUROC↑FPR↓
ResNet18CE 73.41±0.078.87±0.079.93±0.063.38±0.029.56±0.095.77±0.079.43±0.3484.71±2.4680.76±0.3676.1±1.7268.61 79.77
PCL 78.16±0.083.56±0.035.23±0.099.05±0.035.12±0.097.25±0.080.72±0.1975.31±1.7263.77±0.6272.13±0.9258.60 85.46
WPCL 47.7±0.0100.0±0.057.8±0.083.92±0.067.55±0.079.41±0.060.6±1.3397.62±0.5542.06±0.7394.27±0.3755.14 91.04
MSPCL 74.78±0.071.53±0.050.45±0.090.05±0.037.56±0.096.35±0.046.18±0.5696.93±0.5359.12±0.2690.95±0.5853.60 89.16
ResNet50CE 62.07±0.087.07±0.070.93±0.086.08±0.037.58±0.095.5±0.069.68±0.2674.94±0.4878.2±0.3459.09±1.1962.94 81.03
PCL 63.64±0.079.64±0.048.02±0.072.21±0.036.34±0.095.9±0.075.9±0.4770.74±0.365.63±0.0488.25±0.2757.33 83.27
WPCL 29.39±0.093.24±0.036.54±0.099.91±0.05.43±0.028.29±0.055.34±0.4488.8±1.2424.33±0.1693.58±0.0630.14 80.87
MSPCL 93.09±0.021.58±0.071.83±0.069.14±0.00.36±0.099.86±0.050.0±0.076.97±1.4155.34±0.1947.7±1.5154.67 64.2
WideResNet50CE 69.99±0.084.05±0.071.04±0.076.22±0.031.23±0.091.89±0.073.52±0.379.43±0.6878.97±0.377.33±1.1464.95 81.78
PCL 50.98±0.076.49±0.039.54±0.069.1±0.074.31±0.0386.95±0.3858.29±0.6971.17±0.4659.53±0.572.59±0.7148.53 75.58
WPCL 55.61±0.093.24±0.044.11±0.097.97±0.045.53±0.0299.91±0.050.0±0.095.89±1.7947.91±0.1492.66±0.148.63 95.93
MSPCL 96.15±0.011.98±0.083.91±0.046.58±0.021.03±0.0178.02±0.0150.0±0.060.61±1.6385.54±0.5439.46±0.467.33 47.33
ResNeXt50CE 72.39±0.084.82±0.072.09±0.077.7±0.00.0±0.099.95±0.064.2±0.4992.26±2.522.02±0.1199.95±0.0346.14 90.94
PCL 45.54±0.099.86±0.047.47±0.083.24±0.074.34±0.089.46±0.077.34±0.573.77±1.4348.92±0.2393.24±0.058.72 87.91
WPCL 23.09±0.092.21±0.027.86±0.099.23±0.047.01±0.096.71±0.050.07±0.0996.4±0.5828.86±0.0797.06±0.2335.38 96.32
MSPCL 95.94±0.08.83±0.075.57±0.057.7±0.03.64±0.0100.0±0.050.0±0.069.49±1.950.77±0.038.72±2.1455.18 54.95
VGG16CE 93.66±0.1829.57±1.8431.98±0.2491.05±0.6712.3±0.0499.86±0.081.15±0.182.18±3.2179.13±0.8476.81±1.4871.48 69.9
PCL 97.44±0.1212.89±1.4334.43±0.0985.52±0.6849.14±0.4383.32±2.9488.44±0.0959.48±1.6391.55±0.1747.72±0.5677.97 51.4
WPCL 91.72±0.2726.68±1.9951.21±0.1470.47±0.1928.44±0.3798.6±0.389.42±0.237.81±1.1789.52±0.0734.7±2.2480.47 42.42
MSPCL 93.82±0.8127.41±2.4251.73±0.3171.79±0.428.67±0.3799.68±0.1597.33±0.0211.48±0.4692.86±0.3931.47±2.2483.94 35.54
VGG19CE 91.73±0.530.98±5.2132.15±0.1583.9±1.4910.6±0.2998.77±0.1435.64±0.8690.04±0.8962.31±0.2359.58±0.8146.49 72.65
PCL 87.45±0.9462.83±4.0737.56±0.3370.55±0.3876.03±0.2139.3±0.9646.24±0.5298.41±0.5971.24±0.2888.81±0.763.7 71.98
WPCL 93.59±0.2629.45±3.2435.26±0.0477.44±0.857.24±0.5882.11±2.6191.59±0.0726.41±0.2891.21±0.0927.95±0.9473.78 48.67
MSPCL 74.84±0.8179.46±1.9632.43±0.0294.65±0.573.33±0.2559.4±1.5494.46±0.126.67±2.7887.59±0.2555.27±1.8972.53 63.09
CLIP - 53.87±3.8186.44±0.4043.60±6.3995.99±4.0156.10±12.1791.46±5.2961.34±0.093.91±0.050.00±0.088.38±4.4152.98 91.24
Table S.1: The OOD detection results on the GTSRB dataset for the real-world OOD cases, the six trained
DNNs, four training methods, and the six OOD detection methods. The OOD data used for this evaluation
is a dataset consisting of 260 OOD images collected from the internet. The optimization methods include
1) the common cross-entropy (CE) loss; 2) the original PCL; 3) our WPCL; 4) and our MSPCL. The OOD
detection methods include IF, class-based IF, KNN, Mahalanobis distance, ReAct, and ViM. The reported
metrics are AUROC and false positive rate at 95percept true positive rate. The last two columns on the
right indicate the average over all the OOD detection methods on each row and each metric. Please note
that the OOD detection methods results are averaged over 5different random seeds used for optimizing the
OOD detection methods, and the elevated numbers besides them are the standard deviation over the 5run.
S.2 Extra Details On The GTSRB Dataset
The results of detecting the real-world OOD samples on different training approaches on the GTSRB dataset
are presented in Table S.1. This includes the AUROC and FPR for six individual OOD detection methods
21Under review as submission to TMLR
ModelIF KNN Mahal. ReAct ViM Average
AUROC↑ FPR↓ AUROC↑ FPR↓ AUROC↑ FPR↓ AUROC↑ FPR↓ AUROC↑ FPR↓ AUROC↑FPR↓
ResNet18CE 80.51±0.079.32±0.088.97±0.050.54±0.023.07±0.098.6±0.089.91±0.2339.2±0.9789.05±0.0851.14±1.9974.3 63.76
PCL 97.61±0.07.52±0.063.41±0.064.77±0.05.87±0.099.82±0.097.27±0.128.64±0.5521.77±0.0997.23±0.0857.19 55.6
WPCL 87.31±0.0100.0±0.091.69±0.030.59±0.014.13±0.099.73±0.077.93±0.6562.35±1.6490.97±0.2241.1±1.7972.41 66.75
MSPCL 89.42±0.036.58±0.077.48±0.067.16±0.07.88±0.099.82±0.076.04±0.3671.91±0.7990.05±0.1943.65±0.3568.17 63.82
ResNet50CE 76.44±0.075.18±0.085.15±0.056.8±0.028.95±0.094.5±0.081.02±0.0457.38±0.5190.35±0.0836.88±0.5471.77 65.24
PCL 85.44±0.070.45±0.058.51±0.065.86±0.016.75±0.097.75±0.085.78±0.2859.39±0.5883.17±0.0264.47±0.7565.29 73.95
WPCL 72.79±0.093.24±0.062.02±0.092.34±0.00.0±0.099.28±0.074.45±0.3955.62±0.7279.34±0.2369.42±1.3857.63 81.82
MSPCL 87.57±0.045.99±0.074.69±0.080.77±0.01.02±0.099.64±0.050.0±0.060.9±1.2153.91±0.0756.52±0.7152.62 69.65
WideResNet50CE 78.27±0.077.12±0.084.5±0.063.33±0.018.61±0.096.62±0.084.35±0.0658.46±0.5891.35±0.0736.37±0.6471.42 66.38
PCL 67.36±0.066.26±0.039.24±0.066.76±0.023.89±0.0693.11±0.1766.07±0.2858.99±0.8684.79±0.0855.82±0.4254.84 68.3
WPCL 82.34±0.092.93±0.072.48±0.073.56±0.044.1±0.0196.12±0.0450.0±0.070.59±1.2551.76±0.2478.5±0.360.14 82.34
MSPCL 84.5±0.056.8±0.072.07±0.073.92±0.044.7±0.081.97±0.0350.0±0.066.18±0.6160.96±0.2169.71±0.3962.45 69.72
ResNeXt50CE 85.87±0.057.25±0.085.16±0.058.83±0.00.0±0.0100.0±0.080.79±0.0556.63±1.2126.74±0.1399.12±0.1155.71 74.37
PCL 94.57±0.041.04±0.064.96±0.065.45±0.014.35±0.099.28±0.094.16±0.1229.56±0.7291.08±0.0878.68±0.6271.82 62.8
WPCL 59.04±0.086.31±0.054.33±0.093.42±0.050.88±0.097.43±0.050.21±0.0479.77±0.4543.04±0.1694.13±0.1151.5 90.21
MSPCL 95.24±0.013.47±0.085.48±0.059.77±0.00.62±0.0100.0±0.050.0±0.055.9±0.6450.0±0.0158.09±0.3856.27 57.45
VGG16CE 94.19±0.1421.2±0.934.95±0.0472.89±0.7115.12±0.0998.35±0.0893.31±0.1421.29±0.4392.3±0.1423.76±0.8578.69 34.78
PCL 81.58±0.7172.19±1.9539.84±0.1286.67±0.2236.9±0.2998.05±0.3265.34±0.1690.65±0.4471.45±0.0881.15±0.5564.55 82.67
WPCL 76.07±0.7975.75±0.6157.9±0.182.35±0.3525.77±0.198.47±0.271.0±0.1283.72±0.2472.51±0.1480.43±0.2569.37 80.56
MSPCL 95.34±0.4924.74±3.6557.54±0.270.07±0.136.29±0.0799.93±0.0297.68±0.039.03±0.1592.79±0.2127.96±1.4485.84 32.95
VGG19CE 91.77±0.226.2±1.7736.13±0.0569.57±0.1210.39±0.0698.94±0.148.48±0.283.65±0.771.52±0.2444.08±0.3251.66 64.49
PCL 83.22±0.5560.75±0.7940.12±0.1278.47±0.3450.6±0.4898.64±0.3343.62±0.1599.24±0.2464.99±0.2878.3±0.9556.51 83.08
WPCL 89.77±0.3657.29±1.9941.32±0.0571.18±0.1241.28±0.2197.25±0.2383.79±0.0676.66±0.2783.4±0.177.21±1.267.91 75.92
MSPCL 79.04±1.0370.16±1.8540.53±0.1279.59±0.7245.09±0.2587.2±0.7676.7±0.0491.71±0.7877.67±0.1779.33±0.6563.81 81.6
Table S.2: The OOD detection results on the GTSRB dataset for the RP2 adversarial OOD cases, the six
trained DNNs, four training methods, and the six OOD detection methods. The OOD data used for this
evaluation of the attacked images using the RP2 real-world adversarial sticker attack Eykholt et al. (2018).
The optimization methods include 1) the common cross-entropy (CE) loss; 2) the original PCL; 3) our
WPCL; 4) and our MSPCL. The OOD detection methods include IF, class-based IF, KNN, Mahalanobis
distance, ReAct, and ViM. The reported metrics are AUROC and false positive rate at 95percept true
positive rate. The last two columns on the right indicate the average over all the OOD detection methods on
each row and each metric. Please note that the OOD detection methods results are averaged over 5different
random seeds used for optimizing the OOD detection methods, and the elevated numbers beside them are
the standard deviation over the 5run.
and their averages on each training type of each DNN, wherein we have implemented and evaluated a
class-wise isolation forest method to better understand the clustering capability of the standard isolation
forest, which is not presented in the main paper. Similarly, similar results are presented for the OOD data
generated by the RP2 adversarial attack in Table S.2. For both of these tables, the OOD detectors are
fitted 5times. Therefore, the results are the average results achieved over the 5iterations, along with their
standard deviation, as shown on the side of each number.
S.3 Comparison With Contrastive Language–Image Pre-training (CLIP)
We have also conducted a comparison between our trained DNNs and the Contrastive Language–Image
Pre-training (CLIP) Radford et al. (2021), which is a language vision model trained on web-scale data.
However, as it is not specially trained in traffic sign classification, we limited the OOD detection to our
real-world OOD data subset. For this comparison, we utilized a CLIP with a ResNet50 backbone. The
results are presented in Table S.1.
S.4 Videos
Although visualizing latent space activations of DNNs accurately is not easily feasible due to the high number
of dimensions of such data, we have visualized a 3dimensional version of the activations to analyze the effect
of different training methods on the image encoding done by the CNN backbones of our DNNs. Accordingly,
we have extracted the activations generated by the WideResNet50 DNN on the whole GTSRB test dataset,
which consists of more than 12000images. These activations are taken from the latest average pool layer
from the WideResNet50 DNN. Following that, we have reduced the number of dimensions of these data to
3using the principal component analysis (PCA). Furthermore, we have visualized this using the Plotly Inc.
(2015) visualization tool, which leads to web-based interactive plots. Finally, we have captured these plots in
our supplementary video, which consists of four parts for the four training methods we conducted, including
22Under review as submission to TMLR
the cross-entropy loss, PCL, our WPCL, and our MSPCL. In this video, the colors indicate classes, wherein
due to a high number of classes, some are visualized with similar colors while still being distinguishable due
to their distances.
Moreover, we would like to emphasize the fact that our videos are generated solely for visualization purposes
and do not serve as the basis for qualitative analysis. However, one can observe that different training
methods do affect the way the image data samples are encoded by the CNN backbone.
23