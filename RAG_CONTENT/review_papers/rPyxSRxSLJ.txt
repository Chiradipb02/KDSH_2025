Under review as submission to TMLR
Towards Understanding Label Smoothing
Anonymous authors
Paper under double-blind review
Abstract
Label smoothing regularization (LSR) has a great success in training deep neural networks
by stochastic algorithms such as stochastic gradient descent and its variants. However, the
theoretical understanding of its power from the view of optimization is still rare. This study
opens the door to a deep understanding of LSR by initiating the analysis. In this paper,
we analyze the convergence behaviors of stochastic gradient descent with label smoothing
regularization for solving non-convex problems and show that an appropriate LSR can help
to speed up the convergence by reducing the variance. More interestingly, we proposed a
simple yet effective strategy, namely Two-StageLAbel smoothing algorithm (TSLA), that
uses LSR in the early training epochs and drops it off in the later training epochs. We
observe from the improved convergence result of TSLA that it benefits from LSR in the first
stage and essentially converges faster in the second stage. To the best of our knowledge, this
is the first work for understanding the power of LSR via establishing convergence complexity
of stochastic methods with LSR in non-convex optimization. We empirically demonstrate
the effectiveness of the proposed method in comparison with baselines on training ResNet
models over benchmark data sets.
1 Introduction
In training deep neural networks, one common strategy is to minimize cross-entropy loss with one-hot label
vectors, which may lead to overfitting during the training progress that would lower the generalization
accuracy (Müller et al., 2019). To overcome the overfitting issue, several regularization techniques such as
ℓ1-norm orℓ2-norm penalty over the model weights, Dropout which randomly sets the outputs of neurons
to zero (Hinton et al., 2012b), batch normalization (Ioffe & Szegedy, 2015), and data augmentation (Simard
et al., 1998), are employed to prevent the deep learning models from becoming over-confident. However,
these regularization techniques conduct on the hidden activations or weights of a neural network. As an
output regularizer, label smoothing regularization (LSR) (Szegedy et al., 2016) is proposed to improve the
generalization and learning efficiency of a neural network by replacing the one-hot vector labels with the
smoothed labels that average the hard targets and the uniform distribution of other labels. Specifically, for a
K-class classification problem, the one-hot label is smoothed by yLS= (1−θ)y+θ/hatwidey, where yis the one-hot
label,θ∈(0,1)is the smoothing strength and /hatwidey=1
Kis a uniform distribution for all labels. Extensive
experimental results have shown that LSR has significant successes in many deep learning applications
including image classification (Zoph et al., 2018; He et al., 2019), speech recognition (Chorowski & Jaitly,
2017; Zeyer et al., 2018), and language translation (Vaswani et al., 2017; Nguyen & Salazar, 2019).
Due to the importance of LSR, researchers try to explore its behavior in training deep neural networks.
Müller et al. (2019) have empirically shown that the LSR can help improve model calibration, however, they
also have found that LSR could impair knowledge distillation, that is, if one trains a teacher model with LSR,
then a student model has worse performance. Yuan et al. (2019a) have proved that LSR provides a virtual
teacher model for knowledge distillation. As a widely used trick, Lukasik et al. (2020) have shown that LSR
works since it can successfully mitigate label noise. However, to the best of our knowledge, it is unclear, at
least from a theoretical viewpoint, how the introduction of label smoothing will help improve the training of
deep learning models, and to what stage, it can help. In this paper, we aim to provide an affirmative answer
to this question and try to deeply understand why and how the LSR works from the view of optimization.
1Under review as submission to TMLR
Our theoretical analysis will show that an appropriate LSR can essentially reduce the variance of stochastic
gradient in the assigned class labels and thus it can speed up the convergence. Moreover, we will propose a
novel strategy of employing LSR that tells when to use LSR. We summarize the main contributions of this
paper as follows.
•It is thefirst work that establishes improved iteration complexities of stochastic gradient descent
(SGD) (Robbins & Monro, 1951) with LSR for finding an ϵ-approximate stationary point (Defini-
tion 1) in solving a smooth non-convex problem in the presence of an appropriate label smoothing.
The results theoretically explain why an appropriate LSR can help speed up the convergence. (Sub-
section 4.1)
•We propose a simple yet effective strategy, namely Two-StageLAbel smoothing (TSLA) algorithm,
where in the first stage it trains models for certain epochs using a stochastic method with LSR while
inthesecondstageitrunsthesamestochasticmethodwithoutLSR.TheproposedTSLAisageneric
strategy that can incorporate many stochastic algorithms. With an appropriate label smoothing, we
show that TSLA integrated with SGD has an improved iteration complexity (Subsection 4.3) and
bettertesting error (Subsection 4.4), compared to the SGD with LSR and the SGD without LSR.
2 Related Work
In this section, we introduce some related work. A closely related idea to LSR is confidence penalty proposed
by Pereyra et al. (2017), an output regularizer that penalizes confident output distributions by adding its
negative entropy to the negative log-likelihood during the training process. The authors (Pereyra et al.,
2017) presented extensive experimental results in training deep neural networks to demonstrate better gen-
eralization comparing to baselines with only focusing on the existing hyper-parameters. They have shown
that LSR is equivalent to confidence penalty with a reversing direction of KL divergence between uniform
distributions and the output distributions.
DisturbLabel introduced by Xie et al. (2016) imposes the regularization within the loss layer, where it
randomly replaces some of the ground truth labels as incorrect values at each training iteration. Its effect
is quite similar to LSR that can help to prevent the neural network training from overfitting. The authors
have verified the effectiveness of DisturbLabel via several experiments on training image classification tasks.
Recently, many works (Zhang et al., 2018; Bagherinezhad et al., 2018; Goibert & Dohmatob, 2019; Shen
et al., 2019; Li et al., 2020b) explored the idea of LSR technique. Ding et al. (2019) extended an adaptive
label regularization method, which enables the neural network to use both correctness and incorrectness
during training. Pang et al. (2018) used the reverse cross-entropy loss to smooth the classifier’s gradients.
Wang et al. (2020) proposed a graduated label smoothing method that uses the higher smoothing penalty for
high-confidence predictions than that for low-confidence predictions. They found that the proposed method
can improve both inference calibration and translation performance for neural machine translation models.
By contrast, we will try to understand the power of LSR from an optimization perspective and try to study
how and when to use LSR.
3 Preliminaries and Notations
We first present some notations. Let ∇wFS(w)denote the gradient of a function FS(w). When the variable
to be taken a gradient is obvious, we use ∇FS(w)for simplicity. We use ∥·∥to denote the Euclidean norm.
Let⟨·,·⟩be the inner product.
In classification problem, we aim to seek a classifier to map an example x∈ Xonto one of Klabels
y∈Y⊂ RK, where y= (y1,y2,...,yK)is a one-hot label, meaning that yiis “1" for the correct class and
“0" for the rest. Suppose the example-label pairs are drawn from a distribution P, i.e., (x,y)∼P= (Px,Py).
LetS={(x1,y1),..., (xn,yn)}denotes a set of nexamples drawn from P. We denote by E(x,y)[·]the
expectation that takes over a random variable (x,y). When the randomness is obvious, we write E[·]for
simplicity. Our goal is to learn a prediction function f(w;x) :W×X → RKthat is as close as possible
2Under review as submission to TMLR
toy, where w∈Wis the parameter and Wis a closed convex set. To this end, we want to minimize the
following expected loss under P:
min
w∈WFS(w) :=1
nn/summationdisplay
i=1ℓ(yi,f(w;xi)) (1)
whereℓ:Y×RK→R+is a cross-entropy loss function given by
ℓ(y,f(w;x)) =K/summationdisplay
i=1−yilog/parenleftigg
exp(fi(w;x))/summationtextK
j=1exp(fj(w;x))/parenrightigg
. (2)
The objective function FS(w)is not convex since f(w;x)is non-convex in terms of w. To solve the prob-
lem (1), one can simply use some iterative methods such as stochastic gradient descent (SGD). Specifically,
at each training iteration t, SGD updates solutions iteratively by wt+1=wt−η∇wℓ(yt,f(wt;xt)),where
η>0is a learning rate.
Next, we present some notations and assumptions that will be used in the convergence analysis. Throughout
this paper, we also make the following assumptions for solving the problem (1).
Assumption 1. Assume the following conditions hold:
(i) The stochastic gradient of FS(w)is unbiased, i.e., E(x,y)[∇ℓ(y,f(w;x))] =∇FS(w), and the
variance of stochastic gradient is bounded, i.e., there exists a constant σ2>0, such that
E(x,y)/bracketleftig
∥∇ℓ(y,f(w;x))−∇FS(w)∥2/bracketrightig
=σ2.
(ii)FS(w)is smooth with an L-Lipchitz continuous gradient, i.e., it is differentiable and there exists a
constantL>0such that∥∇FS(w)−∇FS(u)∥≤L∥w−u∥,∀w,u∈W.
Remark. Assumption 1 (i) and (ii) are commonly used assumptions in the literature of non-convex opti-
mization (Ghadimi & Lan, 2013; Yan et al., 2018; Yuan et al., 2019b; Wang et al., 2019; Li et al., 2020a).
Assumption 1 (ii) says the objective function is L-smooth, and it has an equivalent expression (Nesterov,
2004) which is FS(w)−FS(u)≤⟨∇FS(u),w−u⟩+L
2∥w−u∥2,∀w,u∈W.For a classification problem,
the smoothed label yLSis given by
yLS= (1−θ)y+θ/hatwidey, (3)
whereθ∈(0,1)is the smoothing strength, yis the one-hot label, /hatwideyis an introduced label. For example, one
can simply use /hatwidey=1
K(Szegedy et al., 2016) for K-class problems. Similar to label y, we suppose the label
/hatwideyis drawn from a distribution P/hatwidey. We introduce the variance of stochastic gradient using label /hatwideyas follows.
E(x,/hatwidey)/bracketleftig
∥∇ℓ(/hatwidey,f(w;x))−∇FS(w)∥2/bracketrightig
=/hatwideσ2:=δσ2. (4)
whereδ>0is a constant and σ2is defined in Assumption 1 (i). We make several remarks for (4).
Remark. (a) We do not require that the stochastic gradient ∇ℓ(/hatwidey,f(w;x))is unbiased, i.e., it could be
E[∇ℓ(/hatwidey,f(w;x))]̸=∇FS(w). (b) The variance /hatwideσ2is defined based on the label /hatwideyrather than the smoothed
labelyLS. (c) We do not assume the variance /hatwideσ2is bounded since δcould be an arbitrary value, however, we
will discuss the different cases of δin our analysis. If δ≥1, then/hatwideσ2≥σ2; while if 0<δ< 1, then/hatwideσ2<σ2.
It is worth mentioning that δcould be small when an appropriate label is used in the label smoothing.
For example, one can smooth labels by using a teacher model (Hinton et al., 2014) or the model’s own
distribution (Reed et al., 2014). In the first paper of label smoothing (Szegedy et al., 2016) and the following
related studies (Müller et al., 2019; Yuan et al., 2019a), researchers consider a uniform distribution over all K
classes of labels as the label /hatwidey, i.e., set/hatwidey=1
K. Due to the space limitation, we include more discussions
and the evaluations of δin real-world applications in Appendix A.1. For example, we show that
δ<1for CIFAR-100 in practice.
We now introduce an important property regarding FS(w), i.e. Polyak-Łojasiewicz (PL) condition (Polyak,
1963). More specifically, the following assumption holds.
3Under review as submission to TMLR
Algorithm 1 SGD with Label Smoothing Regularization
1:Initialize :w0∈W,θ∈(0,1), setηas the value in Theorem 3.
2:fort= 0,1,...,T−1do
3:sample (xt,yt), setyLS
t= (1−θ)yt+θ/hatwideyt
4:update wt+1=wt−η∇wℓ(yLS
t,f(wt;xt))
5:end for
6:Output: wR, whereRis uniformly sampled from {0,1,...,T−1}.
Assumption 2. There exists a constant µ > 0such that 2µ(FS(w)−FS(w∗
S))≤∥∇FS(w)∥2,∀w∈W,
where w∗
S∈minw∈WFS(w)is a optimal solution.
Remark. This property has been theoretically and empirically observed in training deep neural net-
works (Allen-Zhu et al., 2019; Yuan et al., 2019b). This condition is widely used to establish convergence in
the literature of non-convex optimization, please see (Yuan et al., 2019b; Wang et al., 2019; Karimi et al.,
2016; Li & Li, 2018; Charles & Papailiopoulos, 2018; Li et al., 2020a) and references therein.
Tomeasuretheconvergenceofnon-convexandsmoothoptimizationproblemsasin(Nesterov,1998;Ghadimi
& Lan, 2013; Yan et al., 2018), we need the following definition of the first-order stationary point.
Definition 1 (First-order stationary point) .For the problem of minw∈WFS(w), a point w∈Wis called a
first-order stationary point if ∥∇FS(w)∥= 0. Moreover, if∥∇FS(w)∥≤ϵ, then the point wis said to be an
ϵ-stationary point, where ϵ∈(0,1)is a small positive value.
4 TSLA: A Two-Stage Label Smoothing Algorithm
In this section, we present our main method with its convergence analysis. As a warm-up, we first show
the convergence analysis of SGD with LSR to understand LS from theoretical side. Then we will introduce
the proposed TSLA and study its convergence results both in training error (optimization) and testing error
(generalization).
4.1 Convergence Analysis of SGD with LSR
To understand LSR from the optimization perspective, we consider SGD with LSR in Algorithm 1 for the
sake of simplicity. The only difference between Algorithm 1 and standard SGD is the use of the output
label for constructing a stochastic gradient. The following theorem shows that Algorithm 1 converges to an
approximate stationary point in expectation under some conditions. We include its proof in Appendix A.3.
Theorem 3. Under Assumption 1, run Algorithm 1 with η=1
Landθ=1
1+δ, then ER[∥∇FS(wR)∥2]≤
2FS(w0)
ηT+ 2δσ2, whereRis uniformly sampled from {0,1,...,T−1}. Furthermore, given a target accuracy
levelϵ, we have the following two results.
(1) whenδ≤ϵ2
4σ2, if we setT=4FS(w0)
ηϵ2, then Algorithm 1 converges to an ϵ-stationary point in expectation,
i.e.,ER[∥∇FS(wR)∥2]≤ϵ2. The total sample complexity is T=O/parenleftbig1
ϵ2/parenrightbig
.
(2) whenδ>ϵ2
4σ2, if we setT=FS(w0)
ηδσ2, then Algorithm 1 does not converge to an ϵ-stationary point, but we
have ER[∥∇FS(wR)∥2]≤4δσ2≤O(δ).
Remark. We observe that the variance term is 2δσ2, instead of ηLσ2for standard analysis of SGD without
LSR (i.e.,θ= 0, please see the detailed analysis of Theorem 1 in Appendix A.4). For the convergence
analysis, the difference between SGD with LSR and SGD without LSR is that ∇ℓ(/hatwidey,f(w;x))is not an
unbiased estimator of ∇FS(w)when using LSR. The convergence behavior of Algorithm 1 heavily depends
on the parameter δ. Whenδis small enough, say δ≤O(ϵ2)with a small positive value ϵ∈(0,1), then
Algorithm 1 converges to an ϵ-stationary point with the total sample complexity of O/parenleftbig1
ϵ2/parenrightbig
. Recall that the
total sample complexity of standard SGD without LSR for finding an ϵ-stationary point is O/parenleftbig1
ϵ4/parenrightbig
((Ghadimi
& Lan, 2016; Ghadimi et al., 2016), please also see the detailed analysis of Theorem 1 in Appendix A.4).
The convergence result shows that if we could find a label /hatwideythat has a reasonably small amount of δ, we will
4Under review as submission to TMLR
Algorithm 2 The TSLA algorithm
1:Initialize :w0∈W,T1,θ∈(0,1),η1,η2>0
// First stage: SGD with LSR
2:fort= 0,1,...,T 1−1do
3:setyLS
t= (1−θ)yt+θ/hatwideyt
4:update wt+1=wt−η1∇ℓ(yLS
t,f(wt;xt))
5:end for
// Second stage: SGD without LSR
6:fort=T1,1,...,T 1+T2−1do
7:update wt+1=wt−η2∇ℓ(yt,f(wt;xt)).
8:end for
9:Output: wR, whereRis uniformly sampled from
{T1,...,T 1+T2−1}.
Figure 1: Loss on ResNet-18 over CUB-2011.
be able to reduce sample complexity for training a machine learning model from O/parenleftbig1
ϵ4/parenrightbig
toO/parenleftbig1
ϵ2/parenrightbig
. Thus, the
reduction in variance will happen when an appropriate label smoothing with δ∈(0,1)is introduced. We
will find in the empirical evaluations that different label /hatwideylead to different performances and an appropriate
selection of label /hatwideyhas a better performance (see the performances of LSR and LSR-pre in Table 2). On
the other hand, when the parameter δis large such that δ >Ω(ϵ2), that is to say, if an inappropriate label
smoothing is used, then Algorithm 1 does not converge to an ϵ-stationary point, but it converges to a worse
level ofO(δ).
4.2 The TSLA Algorithm
Previous subsection have shown that LSR could not find an ϵ−stationary point under some situations. These
motivate us to investigate a strategy that combines the algorithm with and without LSR during the training
progress. Let think in this way, one possible scenario is that training one-hot label is “easier" than training
smoothed label. Taking the cross entropy loss in (2) for an example, one need to optimize a single loss
function−log/parenleftig
exp(fk(w;x))//summationtextK
j=1exp(fj(w;x))/parenrightig
when one-hot label (e.g, yk= 1andyi= 0for alli̸=k)
is used, but need to optimize all Kloss functions−/summationtextK
i=1yLS
ilog/parenleftig
exp(fi(w;x))//summationtextK
j=1exp(fj(w;x))/parenrightig
when
smoothed label (e.g., yLS= (1−θ)y+θ1
Kso thatyLS
k= 1−(K−1)θ/KandyLS
i=θ/Kfor alli̸=k) is
used. Nevertheless, training deep neural networks is gradually focusing on hard examples with the increase
of training epochs. It seems that training with smoothed label in the late epochs makes the learning progress
more difficult. In addition, after LSR, we focus on optimizing the overall distribution that contains the minor
classes, which are probably not important at the end of training progress. Please see the blue dashed line
(trained by SGD with LSR) in Figure 11, and it shows that there is almost no change in loss after 30 epochs.
One question is whether LSR helps at the early training epochs but it has less (even negative) effect during
the later training epochs? This question encourages us to propose and analyze a simple strategy with LSR
dropping that switches a stochastic algorithm with LSR to the algorithm without LSR.
A natural choice could be just dropping off LSR after certain epochs. For example, one can run SGD with
LSR in the first 50 epochs and then run SGD without LSR after that (see red solid line in Figure 1). From
Figure 1, we can see that the drop of LSR after 50 epochs can further decrease the loss function.
With this in mind in this subsection, we propose a generic framework that consists of two stages, wherein
the first stage it runs SGD with LSR for T1iterations and the second stage it runs SGD without LSR up
toT2iterations. This framework is referred to as Two-StageLAbel smoothing (TSLA) algorithm, whose
updating details are presented in Algorithm 2. Although SGD is considered as the pipeline (Line 4 and
Line 8 in Algorithm 2) in the convergence analysis, in practice, SGD can be replaced by any stochastic
algorithms such as momentum SGD (Polyak, 1964), Stochastic Nesterov’s Accelerated Gradient (Nesterov,
1The blue dashed line is trained by SGD with LSR. The red solid line is trained by SGD with LSR for the first 50 epochs
and trained by SGD without LSR (i.e., using one-hot label) for the last 40 epochs.
5Under review as submission to TMLR
1983), and adaptive algorithms including AdaGrad (Duchi et al., 2011), RMSProp (Hinton et al., 2012a),
AdaDelta (Zeiler, 2012), Adam (Kingma & Ba, 2015), Nadam (Dozat, 2016) and AMSGrad (Reddi et al.,
2018). In this paper, we will not study the theoretical guarantees and empirical evaluations of other opti-
mizers, which can be considered as future work. Please note that the algorithm can use different learning
ratesη1andη2during the two stages. The last solution of the first stage will be used as the initial solution
of the second stage. If T1= 0, then TSLA reduces to the baseline, i.e., SGD without LSR; while if T2= 0,
TSLA becomes to LSR method, i.e., SGD with LSR.
4.3 Optimization Result of TSLA
In this subsection, we will give the convergence result of the proposed TSLA algorithm. For simplicity, we
use SGD as the subroutine algorithm Ain the analysis. The convergence result in the following theorem
shows the power of LSR from the optimization perspective. Its proof is presented in Appendix A.5. It is
easy to see from the proof that by using the last output of the first stage as the initial point of the second
stage, TSLA can enjoy the advantage of LSR in the second stage with an improved convergence.
Theorem 4. Under Assumptions 1, 2, suppose σ2δ/µ≤F(w0), run Algorithm 2, θ=1
1+δ,η1=1
L,
T1= log/parenleftig
2µF(w0)(1+δ)
2δσ2/parenrightig
/(η1µ),η2=ϵ2
2Lσ2andT2=8δσ2
µη2ϵ2, then ER[∥∇FS(wR)∥2]≤ϵ2, whereRis
uniformly sampled from {T1,...,T 1+T2−1}.
Remark. It is obvious that the learning rate η2in the second stage is roughly smaller than the learning rate
η1in the first stage, which matches the widely used stage-wise learning rate decay scheme in training neural
networks. To explore the total sample complexity of TSLA, we consider different conditions on δ. For a fixed
the target convergence level ϵ∈(0,1), let us discuss the total sample complexities of finding ϵ-stationary
points for SGD with TSLA (TSLA), SGD with LSR (LSR), and SGD without LSR (baseline), where we
only consider the orders of the complexities but ignore all constants. When Ω(ϵ2)< δ < 1, LSR does not
converge to an ϵ-stationary point, while TSLA reduces sample complexity from O/parenleftbig1
ϵ4/parenrightbig
toO/parenleftbigδ
ϵ4/parenrightbig
, compared
to the baseline. When δ < O (ϵ2), the total complexity of TSLA is between log(1/ϵ)and1/ϵ2, which is
always better than LSR and the baseline. In summary, TSLA achieves the best total sample complexity
by enjoying the good property of an appropriate label smoothing (i.e., when 0< δ < 1). However, when
δ≥1, baseline has better convergence than TSLA, meaning that the selection of label /hatwideyis not appropriate.
SinceT1contains unknown parameters, it is difficulty to know its ground-truth value. However, we can tune
different values in practice.
4.4 Generalization Result of TSLA
In this subsection, we study the generalization result of TSLA. First, we give some notations, where most of
them are followed by (Hardt et al., 2015; Yuan et al., 2019b). Let wS=A(S)be a solution that generated
by a random algorithm Abased on dataset S. Recall that problem (1) is called empirical risk minimization
in the literature, and the true risk minimization is given by
min
w∈WF(w) := E (x,y)[ℓ(y,f(w;x))]. (5)
Then the testing error is dedined as
EA,S[F(wS)]−ES[FS(w∗
S)] (6)
We notice that there are several works (Hardt et al., 2015; Yuan et al., 2019b) study the testing er-
ror result of SGD for non-convex setting under different conditions such as bounded stochastic gradient
∥∇wℓ(y,f(w;x))∥≤Gand decaying learning rate ηt≤c
twith a constant c>0, wheretis the optimization
iteration. In this paper, we are not interested in establishing fast rate under different conditions, but we
want to explore the generalization ability of TSLA with the fewest possible modifications when building
a bridge between theory and practice. For example, weight decay is a widely used trick when training
deep neural networks. With the use of weight decay, the empirical risk minimization in practice becomes
minw∈W{/hatwideFS(w) :=FS(w) +λ
2∥w∥2}. We present the ERB of TSLA in the following Theorem 5, whose
proofs can be found in the Appendix A.6.
6Under review as submission to TMLR
Theorem 5. Under Assumption 1, assume that ℓ(y,f(w,x))isL-smooth and B-Lipschitz, suppose /hatwideFS(w)
satisfies Assumption 2 and minw∈W/hatwideFS(w)≤FS(w∗
S)+λ
2∥wt∥2withλ= 2L, where wtis the intermediate so-
lution in the second stage of Algorithm 2 by running with θ=1
1+δ,η1<1
3L,T1= log/parenleftig
2µFS(w0)(1+δ)
2δσ2/parenrightig
/(η1µ),
η2=O(1/√n)andT2=O(δn)then the testing error ER,A,S[F(wR)]−ES[FS(w∗
S)]≤O(1/√n), whereA
is TSLA.
Remark. Theorem 5 shows that in order to have an bound of testing error in the order of O(1/√n), TSLA
needs to run T=O(δn)iterations. For the standard analysis of SGD with/without LSR (please see the
remarks in Appendix A.6) under the same conditions, SGD without LSR needs to run T=O(n)iterations
to obtain an O(1/√n)bound of testing error. Thus, if δis small enough, the testing error of TSLA is better
than that of SGD without LSR, otherwise, SGD without LSR has better testing error than TSLA. For SGD
with LSR, when δ >Ω(1/√n), the bound of testing error for LSR can not be bounded by O(1/√n); when
δ≤Ω(1/√n), the bound for LSR can be bounded by O(1/√n)inT=O(√n)iterations. This shows that
TSLA bas better testing error than LSR.
5 Experiments
To further evaluate the performance of the proposed TSLA method, we trained deep neural networks on
three benchmark data sets, CIFAR-100 (Krizhevsky & Hinton, 2009), Stanford Dogs (Khosla et al., 2011)
and CUB-2011 (Wah et al., 2011), for image classification tasks. CIFAR-1002has 50,000 training images
and 10,000 testing images of 32 ×32 resolution with 100 classes. Stanford Dogs data set3contains 20,580
images of 120 breeds of dogs, where 100 images from each breed is used for training. CUB-20114is a birds
image data set with 11,788 images of 200 birds species. The ResNet-18 model (He et al., 2016) is applied as
the backbone in the experiments.
We compare the proposed TSLA incorporating with SGD (TSLA) with two baselines, SGD with LSR (LSR)
and SGD without LSR (baseline). The mini-batch size of training instances for all methods is 256as
suggested by (He et al., 2019; 2016). The momentum parameter is fixed as 0.9. We will include more details
of experimental settings in Appendix A.1.
5.1 Stanford Dogs and CUB-2011
We separately train ResNet-18 (He et al., 2016) up to 90 epochs over two data sets Stanford Dogs and
CUB-2011. We use weight decay with the parameter value of 10−4. For all algorithms, the initial learning
rates for FC are set to be 0.1, while that for the pre-trained backbones are 0.001 and 0.01 for Standford
Dogs and CUB-2011, respectively. The learning rates are divided by 10 every 30 epochs. For LSR, we fix
the value of smoothing strength θ= 0.4for the best performance, and the label /hatwideyused for label smoothing
is set to be a uniform distribution over all Kclasses, i.e.,/hatwidey=1
K. The same values of the smoothing strength
θand the same /hatwideyare used during the first stage of TSLA. For TSLA, we drop off the LSR (i.e., let θ= 0)
aftersepochs during the training process, where s∈{20,30,40,50,60,70,80}.
We first report the highest top-1 and top-5 accuracy on the testing data sets for different methods. All
top-1 and top-5 accuracy are averaged over 5 independent random trails with their standard deviations.
The results of the comparison are summarized in Table 1, where the notation “TSLA( s)" means that the
TSLA algorithm drops off LSR after epoch s. It can be seen from Table 1 that under an appropriate
hyperparameter setting the models trained using TSLA outperform that trained using LSR and baseline,
which supports the convergence result in Section 4.2. We notice that the best top-1 accuracy of TSLA are
TSLA(40) and TSLA(50) for Stanford Dogs and CUB-2011, respectively, meaning that the performance of
TSLA(s) is not monotonic over the dropping epoch s. For CUB-2011, the top-1 accuracy of TSLA(20) is
smaller than that of LSR. This result matches the convergence analysis of TSLA showing that it can not
drop off LSR too early. For top-5 accuracy, we found that TSLA(80) is slightly worse than baseline. This
2https://www.cs.toronto.edu/~kriz/cifar.html
3http://vision.stanford.edu/aditya86/ImageNetDogs/
4http://www.vision.caltech.edu/visipedia/CUB-200.html
7Under review as submission to TMLR
Table 1: Comparison of Testing Accuracy for Different Methods (mean ±standard deviation, in %).
Stanford Dogs CUB-2011
Algorithm∗Top-1 accuracy Top-5 accuracy Top-1 accuracy Top-5 accuracy
baseline 82.31 ±0.18 97.76±0.06 75.31±0.25 93.14±0.31
LSR 82.80 ±0.07 97.41±0.09 76.97±0.19 92.73±0.12
TSLA(20) 83.15 ±0.02 97.91±0.08 76.62±0.15 93.60±0.18
TSLA(30) 83.89 ±0.16 98.05±0.08 77.44±0.19 93.92±0.16
TSLA(40) 83.93±0.13 98.03±0.05 77.50±0.20 93.99±0.11
TSLA(50) 83.91 ±0.1598.07±0.0677.57±0.21 93.86±0.14
TSLA(60) 83.51 ±0.11 97.99±0.06 77.25±0.2994.43±0.18
TSLA(70) 83.38 ±0.09 97.90±0.09 77.21±0.15 93.31±0.12
TSLA(80) 83.14 ±0.09 97.73±0.07 77.05±0.14 93.05±0.08
∗TSLA( s): TSLA drops off LSR after epoch s.
2030405060708090
# ep ch798081828384T p-1 AccuracyTSLA (30)
TSLA (40)
TSLA (50)
TSLA (60)TSLA (70)
TSLA (80)
LSR
ba#elineStanf rd D g#
2030405060708090
# e och74757677To -1 Acc#racyTSLA (30)
TSLA (40)
TSLA (50)
TSLA (60)TSLA (70)
TSLA (80)
LSR
baselineCUB-2011
020 40 60 80
# e och0.60.81.01.21.4LossTSLA (30)
TSLA (40)
TSLA (50)
TSLA (60)TSLA (70)
TSLA (80)
LSR
baselineS#anford Dogs
020 40 60 80
# e och1.01.52.02.53.0LossTSLA (30)
TSLA (40)
TSLA (50)
TSLA (60)TSLA (70)
TSLA (80)
LSR
baselineCUB-2011
Figure 2: Testing Top-1 and Loss on ResNet-18 over Stanford Dogs and CUB-2011. TSLA( s) means TSLA
drops off LSR after epoch s.
8Under review as submission to TMLR
is because of dropping LSR too late so that the update iterations (i.e., T2) in the second stage of TSLA is
too small to converge to a good solution. We also observe that LSR is better than baseline regarding top-1
accuracy but the result is opposite as to top-5 accuracy.
We then plot the averaged top-1 accuracyand averaged loss among 5 trails of different methods in Figure 2.
We remove the results for TSLA(20) since it dropped off LSR too early as mentioned before. The figure
shows TSLA improves the top-1 testing accuracy immediately once it drops off LSR. Although TSLA may
not converge if it drops off LSR too late, see TSLA(60), TSLA(70), and TSLA(80) from the third column
of Figure 2, it still has the best performance compared to LSR and baseline. TSLA(30), TSLA(40), and
TSLA(50) can converge to lower objective levels, comparing to LSR and baseline.
5.2 CIFAR-100
6080100120140160180200
# epoch7475767778Top-1 Accurac TSLA (180)
TSLA (160)
TSLA (140)
TSLA (120)
LSR
TSLA-pre (180)TSLA-pre (160)
TSLA-pre (140)
TSLA-pre (120)
LSR-pre
baselinecifar100
0 25 50 75 100 125 150 175 200
# epoch0.81.01.21.41.61.82.0LossTSLA (180)
TSLA (160)
TSLA (140)
TSLA (120)
LSR
TSLA-pre (180)TSLA-pre (160)
TSLA-pre (140)
TSLA-pre (120)
LSR-pre
baselinecifar100
Figure 3: Testing Top-1 and Loss on ResNet-18 over CIFAR-100. TSLA( s)/TSLA-pre( s)
meansTSLA/TSLA-pre drops off LSR/LSR-pre after epoch s.
The total epochs of training ResNet-18 (He et al., 2016) on CIFAR-100 is set to be 200.The weight decay
with the parameter value of 5×10−4is used. We use 0.1as the initial learning rates for all algorithms and
divide them by 10 every 60 epochs as suggested in (He et al., 2016; Zagoruyko & Komodakis, 2016). For
LSR and the first stage of TSLA, the value of smoothing strength θis fixed asθ= 0.1, which shows the best
performance for LSR. We use two different labels /hatwideyto smooth the one-hot label, the uniform distribution over
all labels and the distribution predicted by an ImageNet pre-trained model which is downloaded directly
from PyTorch5(Paszke et al., 2019). For TSLA, we try to drop off the LSR after sepochs during the
training process, where s∈{120,140,160,180}.
All top-1 and top-5 accuracy on the testing data set are averaged over 5 independent random trails with
their standard deviations. We summarize the results in Table 2, where LSR-pre and TSLA-pre indicate LSR
and TSLA use the label /hatwideybased on the ImageNet pre-trained model. The results show that LSR-pre/TSLA-
pre has a better performance than LSR/TSLA. The reason might be that the pre-trained model-based
prediction is closer to the ground truth than the uniform prediction and it has lower variance (smaller δ).
Then, TSLA (LSR) with such pre-trained model-based prediction converges faster than TSLA (LSR) with
uniform prediction, which verifies our theoretical findings in Section 4.2 (Section 4.1). This observation also
empirically tells us the selection of the prediction function /hatwideyused for smoothing label is the key to the
success of TSLA as well as LSR. Among all methods, the performance of TSLA-pre is the best. For top-1
accuracy, TSLA-pre(160) outperforms all other algorithms, while for top-5 accuracy, TSLA-pre(180) has the
best performance.
5https://pytorch.org/docs/stable/torchvision/models.html
9Under review as submission to TMLR
Table 2: Comparison of Testing Accuracy for Different Methods (mean ±standard deviation, in %).
CIFAR-100
Algorithm∗Top-1 accuracy Top-5 accuracy
baseline 76.87 ±0.04 93.47±0.15
LSR 77.77 ±0.18 93.55±0.11
TSLA(120) 77.92 ±0.21 94.13±0.23
TSLA(140) 77.93 ±0.19 94.11±0.22
TSLA(160) 77.96 ±0.20 94.19±0.21
TSLA(180) 78.04 ±0.27 94.23±0.15
LSR-pre 78.07 ±0.31 94.70±0.14
TSLA-pre(120) 78.34 ±0.31 94.68±0.14
TSLA-pre(140) 78.39 ±0.25 94.73±0.11
TSLA-pre(160) 78.55±0.28 94.83±0.08
TSLA-pre(180) 78.53 ±0.2394.96±0.23
∗TSLA( s)/TSLA-pre( s): TSLA/TSLA-pre drops off LSR/LSR-pre after epoch s.
Table 3: Comparison of Testing Top-1 Accuracy for Different θof LSR and TSLA (in %).
θ 0.2 0.4 0.9
LSR 77.75 77.72 76.40
TSLA(120) 77.92 77.68 76.05
TSLA(140) 78.06 77.61 76.27
TSLA(160) 78.09 77.54 76.37
TSLA(180) 78.10 77.89 76.60
∗TSLA( s): TSLA drops off LSR after epoch s.
Finally, we observe from Figure 3 that both TSLA and TSLA-pre converge, while TSLA-pre converges to
the lowest objective value. Similarly, the results of top-1 accuracy show the improvements of TSLA and
TSLA-pre at the point of dropping off LSR.
We conduct an ablation study for the smoothing parameter θand the dropping epoch sin TSLA(s).
We follow the same settings in Subsection 5.2 but use different values of θin LSR and TSLA. Specifically,
θ∈{0.2,0.4,0.9}. We use the uniform distribution over all labels to smooth the one-hot label. The results
are summarized in Table 3, showing that the different values of θandscan affect the performances of LSR
and TSLA. With the increase of θ, the performances become worse. However, with different values of θ,
TSLA can always outperform LSR when an appropriate dropping epoch sis selected. Besides, TSLA(180)
has the best performance for each value of θ.
6 Conclusions
In this paper, we have studied the power of LSR in training deep neural networks by analyzing SGD with
LSR in different non-convex optimization settings. The convergence results show that an appropriate LSR
with reduced label variance can help speed up the convergence. We have proposed a simple and efficient
strategy so-called TSLA that can incorporate many stochastic algorithms. The basic idea of TSLA is to
switch the training from smoothed label to one-hot label. Integrating TSLA with SGD, we observe from
its improved convergence result that TSLA benefits from LSR in the first stage and essentially converges
faster in the second stage. Our result also show that TSLA has better testing error than LSR in theory.
Throughout extensive experiments, we have shown that TSLA improves the generalization accuracy of deep
models on benchmark data sets.
10Under review as submission to TMLR
References
Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via over-
parameterization. In International Conference on Machine Learning , pp. 242–252, 2019.
Hessam Bagherinezhad, Maxwell Horton, Mohammad Rastegari, and Ali Farhadi. Label refinery: Improving
imagenet classification through label progression. arXiv preprint arXiv:1805.02641 , 2018.
Zachary Charles and Dimitris Papailiopoulos. Stability and generalization of learning algorithms that con-
verge to global optima. In International Conference on Machine Learning , pp. 745–754, 2018.
Jan Chorowski and Navdeep Jaitly. Towards better decoding and language model integration in sequence
to sequence models. Proc. Interspeech 2017 , pp. 523–527, 2017.
Qianggang Ding, Sifan Wu, Hao Sun, Jiadong Guo, and Shu-Tao Xia. Adaptive regularization of labels.
arXiv preprint arXiv:1908.05474 , 2019.
Timothy Dozat. Incorporating nesterov momentum into adam. 2016.
JohnDuchi, EladHazan, andYoramSinger. Adaptivesubgradientmethodsforonlinelearningandstochastic
optimization. Journal of Machine Learning Research , 12:2121–2159, 2011.
Saeed Ghadimi and Guanghui Lan. Stochastic first-and zeroth-order methods for nonconvex stochastic
programming. SIAM Journal on Optimization , 23(4):2341–2368, 2013.
Saeed Ghadimi and Guanghui Lan. Accelerated gradient methods for nonconvex nonlinear and stochastic
programming. Math. Program. , 156(1-2):59–99, 2016.
Saeed Ghadimi, Guanghui Lan, and Hongchao Zhang. Mini-batch stochastic approximation methods for
nonconvex stochastic composite optimization. Mathematical Programming , 155(1-2):267–305, 2016.
Morgane Goibert and Elvis Dohmatob. Adversarial robustness via adversarial label-smoothing. arXiv
preprint arXiv:1906.11567 , 2019.
Moritz Hardt, Benjamin Recht, and Yoram Singer. Train faster, generalize better: Stability of stochastic
gradient descent. arXiv preprint arXiv:1509.01240 , 2015.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In
Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pp. 770–778, 2016.
Tong He, Zhi Zhang, Hang Zhang, Zhongyue Zhang, Junyuan Xie, and Mu Li. Bag of tricks for image
classification with convolutional neural networks. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition , pp. 558–567, 2019.
Geoffrey Hinton, Nitish Srivastava, and Kevin Swersky. Neural networks for machine learning lecture 6a
overview of mini-batch gradient descent. 2012a.
Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. In NeurIPS
Deep Learning Workshop , 2014.
Geoffrey E Hinton, Nitish Srivastava, Alex Krizhevsky, Ilya Sutskever, and Ruslan R Salakhutdinov. Im-
proving neural networks by preventing co-adaptation of feature detectors. arXiv preprint arXiv:1207.0580 ,
2012b.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing
internal covariate shift. arXiv preprint arXiv:1502.03167 , 2015.
Hamed Karimi, Julie Nutini, and Mark Schmidt. Linear convergence of gradient and proximal-gradient
methods under the polyak-łojasiewicz condition. In Joint European Conference on Machine Learning and
Knowledge Discovery in Databases , pp. 795–811. Springer, 2016.
11Under review as submission to TMLR
Aditya Khosla, Nityananda Jayadevaprakash, Bangpeng Yao, and Fei-Fei Li. Novel dataset for fine-grained
image categorization: Stanford dogs. In Proc. CVPR Workshop on Fine-Grained Visual Categorization
(FGVC), volume 2, 2011.
DiederikPKingmaandJimmyBa. Adam: Amethodforstochasticoptimization. In International Conference
on Learning Representations , 2015.
Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. Master’s thesis,
Technical report, University of Tronto , 2009.
Xiaoyu Li, Zhenxun Zhuang, and Francesco Orabona. Exponential step sizes for non-convex optimization.
arXiv preprint arXiv:2002.05273 , 2020a.
Xingjian Li, Haoyi Xiong, Haozhe An, Dejing Dou, and Chengzhong Xu. Colam: Co-learning of deep neural
networks and soft labels via alternating minimization. arXiv preprint arXiv:2004.12443 , 2020b.
Zhize Li and Jian Li. A simple proximal stochastic gradient method for nonsmooth nonconvex optimization.
InAdvances in Neural Information Processing Systems , pp. 5564–5574, 2018.
Michal Lukasik, Srinadh Bhojanapalli, Aditya Krishna Menon, and Sanjiv Kumar. Does label smoothing
mitigate label noise? arXiv preprint arXiv:2003.02819 , 2020.
Rafael Müller, Simon Kornblith, and Geoffrey E Hinton. When does label smoothing help? In Advances in
Neural Information Processing Systems , pp. 4696–4705, 2019.
Yurii Nesterov. A method of solving a convex programming problem with convergence rate O(1/k2).Soviet
Mathematics Doklady , 27:372–376, 1983.
Yurii Nesterov. Introductory lectures on convex programming volume i: Basic course. 1998.
Yurii Nesterov. Introductory lectures on convex optimization : a basic course . Applied optimization. Kluwer
Academic Publ., 2004. ISBN 1-4020-7553-7.
Toan Q Nguyen and Julian Salazar. Transformers without tears: Improving the normalization of self-
attention. arXiv preprint arXiv:1910.05895 , 2019.
Tianyu Pang, Chao Du, Yinpeng Dong, and Jun Zhu. Towards robust detection of adversarial examples. In
Advances in Neural Information Processing Systems , pp. 4579–4589, 2018.
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen,
Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance
deep learning library. In Advances in Neural Information Processing Systems , pp. 8024–8035, 2019. URL
https://pytorch.org/docs/stable/torchvision/models.html .
Gabriel Pereyra, George Tucker, Jan Chorowski, Lukasz Kaiser, and Geoffrey Hinton. Regularizing neural
networks by penalizing confident output distributions. arXiv preprint arXiv:1701.06548 , 2017.
Boris T Polyak. Some methods of speeding up the convergence of iteration methods. USSR Computational
Mathematics and Mathematical Physics , 4(5):1–17, 1964.
Boris Teodorovich Polyak. Gradient methods for minimizing functionals. Zhurnal Vychislitel’noi Matematiki
i Matematicheskoi Fiziki , 3(4):643–653, 1963.
Sashank J Reddi, Satyen Kale, and Sanjiv Kumar. On the convergence of adam and beyond. In International
Conference on Learning Representations , 2018.
Scott Reed, Honglak Lee, Dragomir Anguelov, Christian Szegedy, Dumitru Erhan, and Andrew Rabinovich.
Training deep neural networks on noisy labels with bootstrapping. arXiv preprint arXiv:1412.6596 , 2014.
Herbert Robbins and Sutton Monro. A stochastic approximation method. The annals of mathematical
statistics , pp. 400–407, 1951.
12Under review as submission to TMLR
Chaomin Shen, Yaxin Peng, Guixu Zhang, and Jinsong Fan. Defending against adversarial attacks by
suppressing the largest eigenvalue of fisher information matrix. arXiv preprint arXiv:1909.06137 , 2019.
Patrice Y Simard, Yann A LeCun, John S Denker, and Bernard Victorri. Transformation invariance in
pattern recognition—tangent distance and tangent propagation. In Neural networks: tricks of the trade ,
pp. 239–274. Springer, 1998.
Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethinking the
inception architecture for computer vision. In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition , pp. 2818–2826, 2016.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser,
and Illia Polosukhin. Attention is all you need. In Advances in Neural Information Processing Systems ,
pp. 5998–6008, 2017.
C. Wah, S. Branson, P. Welinder, P. Perona, and S. Belongie. The Caltech-UCSD Birds-200-2011 Dataset.
Technical Report CNS-TR-2011-001, California Institute of Technology, 2011.
Shuo Wang, Zhaopeng Tu, Shuming Shi, and Yang Liu. On the inference calibration of neural machine
translation. arXiv preprint arXiv:2005.00963 , 2020.
Zhe Wang, Kaiyi Ji, Yi Zhou, Yingbin Liang, and Vahid Tarokh. Spiderboost and momentum: Faster
variance reduction algorithms. In Advances in Neural Information Processing Systems , pp. 2403–2413,
2019.
Lingxi Xie, Jingdong Wang, Zhen Wei, Meng Wang, and Qi Tian. Disturblabel: Regularizing cnn on the loss
layer. InProceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pp.4753–4762,
2016.
Yan Yan, Tianbao Yang, Zhe Li, Qihang Lin, and Yi Yang. A unified analysis of stochastic momentum
methods for deep learning. In International Joint Conference on Artificial Intelligence , pp. 2955–2961,
2018.
LiYuan, FrancisEHTay, GuilinLi, TaoWang, andJiashiFeng. Revisitknowledgedistillation: ateacher-free
framework. arXiv preprint arXiv:1909.11723 , 2019a.
Zhuoning Yuan, Yan Yan, Rong Jin, and Tianbao Yang. Stagewise training accelerates convergence of testing
error over sgd. In Advances in Neural Information Processing Systems , pp. 2604–2614, 2019b.
Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. British Machine Vision Conference , 2016.
Matthew D Zeiler. Adadelta: an adaptive learning rate method. arXiv preprint arXiv:1212.5701 , 2012.
Albert Zeyer, Kazuki Irie, Ralf Schlüter, and Hermann Ney. Improved training of end-to-end attention
models for speech recognition. Proc. Interspeech 2018 , pp. 7–11, 2018.
Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond empirical risk
minimization. In International Conference on Learning Representations , 2018.
Barret Zoph, Vijay Vasudevan, Jonathon Shlens, and Quoc V Le. Learning transferable architectures for
scalable image recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition , pp. 8697–8710, 2018.
13