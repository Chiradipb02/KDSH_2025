Published in Transactions on Machine Learning Research (04/2023)
UncertaINR: Uncertainty Quantiﬁcation of End-to-End
Implicit Neural Representations for Computed Tomography
Francisca Vasconcelos∗†francisca@berkeley.edu
Department of Electrical Engineering and Computer Science
University of California, Berkeley
Bobby He∗bobby.he@stats.ox.ac.uk
Department of Statistics
University of Oxford
Nalini Singh nmsingh@mit.edu
Computer Science and Artiﬁcial Intelligence Laboratory
Massachusetts Institute of Technology
Yee Whye Teh y.w.teh@stats.ox.ac.uk
Department of Statistics
University of Oxford
Reviewed on OpenReview: https: // openreview. net/ forum? id= jdGMBgYvfX
Abstract
Implicit neural representations (INRs) have achieved impressive results for scene reconstruc-
tion and computer graphics, where their performance has primarily been assessed on recon-
struction accuracy. As INRs make their way into other domains, where model predictions
inform high-stakes decision-making, uncertainty quantiﬁcation of INR inference is becoming
critical. To that end, we study a Bayesian reformulation of INRs, UncertaINR, in the con-
text of computed tomography, and evaluate several Bayesian deep learning implementations
in terms of accuracy and calibration. We ﬁnd that they achieve well-calibrated uncertainty,
whileretainingaccuracycompetitivewithotherclassical, INR-based, andCNN-basedrecon-
structiontechniques. ContrarytocommonintuitionintheBayesiandeeplearningliterature,
we ﬁnd that INRs obtain the best calibration with computationally eﬃcient Monte Carlo
dropout, outperforming Hamiltonian Monte Carlo and deep ensembles. Moreover, in con-
trast to the best-performing prior approaches, UncertaINR does not require a large training
dataset, but only a handful of validation images.
1 Introduction
Implicitneuralrepresentations(INRs)arearecenttechniqueforcapturingcomplex, coordinate-basedsignals,
achievingimpressiveresultsinnovelviewsynthesis(Mildenhalletal.,2020;Niemeyeretal.,2020;Saitoetal.,
2019; Sitzmann et al., 2019), shape representation (Chen & Zhang, 2019; Deng et al., 2020; Genova et al.,
2019; 2020; Jiang et al., 2020; Park et al., 2019), and texture synthesis (Henzler et al., 2020; Oechsle et al.,
2019). In the context of image reconstruction, INRs represent images as continuous functions mapping
coordinates to pixel values, f: (x,y)→[0,1]. Typically fis parameterized by a small neural network (NN)
and trained with gradient-based optimization, given observations S. This continuous INR formulation oﬀers
several beneﬁts to discrete array representations, e.g. enabling signal processing at arbitrary resolutions
(thereby avoiding the so-called “curse of discretization” (Mescheder, 2020)) and improved memory eﬃciency
(Dupont et al., 2021).
∗Equal contribution.
†This work was done while the author was at the University of Oxford.
1Published in Transactions on Machine Learning Research (04/2023)
(a)UncertaINR Framework
 (b)Computed Tomography Setup
Figure1: (a)FromanoisyCTscan, UncertaINRreconstructsahigh-qualityimageofthe2Dimagingsubject
cross-section. Further, UncertaINR achieves well-calibrated UQ over the reconstructed image, enabling
doctors to use the predicted variance to determine regions where the reconstruction may have failed. (b)A
CT scan of attenuation-coeﬃcient image f∗(x,y)results in sinogram Sφ(r).
Despite these appealing properties, existing INR applications have primarily focused their scope on either: 1)
predictive representation accuracy or 2) visual plausibility of predictions extrapolated from a given training
signalS. However, there are settings – e.g. Magnetic Resonance Imaging (MRI) and Computed Tomography
(CT) (Tancik et al., 2020) – in which INRs have proven promising and for which uncertainty quantiﬁcation
(UQ) is highly desirable. In such applications, only an underdetermined set of observations Sis available,
which is exacerbated by the high-dimensional, non-convex nature of NN parameter landscapes. As a result,
many diﬀerent INR parameter values may ﬁt the observed data equally well, but yield vastly diﬀerent
predictions (some of which may be poor, e.g. Figure 4 of Tancik et al. (2021)) when extrapolating to
unobserved regions of coordinate space. Hence, in such scenarios, calibrated predictive uncertainty is crucial
alongside high reconstruction accuracy.
This work assesses the UQ capabilities of INRs in the applied setting of medical imaging, speciﬁcally com-
puted tomography (CT). This setting is apt for INR UQ due to its high-stakes nature – even a small image
artifact could result in misdiagnosis – and underdetermination of CT sinogram observations. Moreover, UQ
could be leveraged to reduce healthcare costs via automated triage, e.g. by assigning images with varying
degrees ofuncertainty to healthcare providersof relevantexpertise. Furthermore, eachCT scan measurement
exposes the patient to harmful radiation1, meaning calibrated model uncertainty could enable techniques,
such as active learning (Cohn et al., 1996), to inform more eﬃcient measurement collection and reduce neg-
ative radiation exposure eﬀects. Although uncertainty has been quantiﬁed in deep-learning based medical
imaging (Barbano et al., 2021; Laves et al., 2022), medical datasets are often small and apparatus-speciﬁc,
posing signiﬁcant generalization challenges for large data-driven approaches (Abramoﬀ et al., 2018; De Fauw
et al., 2018; Hosny et al., 2018; Yasaka & Abe, 2018; Zhang et al., 2020). Meanwhile, INRs require little to
no training data and have previously demonstrated decent CT reconstruction accuracy (Tancik et al., 2020),
which we now complement with well-calibrated UQ.
In light of the aforementioned need for well-calibrated INRs, we propose UncertaINR (Figure 1a): a Bayesian
reformulation of INR-based image reconstruction. We demonstrate eﬀective application of Bayesian deep-
learning (BDL) principles to INRs, in the underdetermined CT reconstruction context, and study the perfor-
mance of various established BDL approximate inference approaches: Bayes-by-backprop (BBB) (Blundell
et al., 2015), Monte Carlo dropout (MCD) (Gal & Ghahramani, 2016), and deep ensembles (DEs) (Laksh-
minarayanan et al., 2017), as well as the ‘gold-standard’ but computationally intensive Hamiltonian Monte
Carlo (HMC) (Neal et al., 2011). We ﬁnd that the simple, yet eﬃcient MCD procedure is highly eﬀective at
1An estimated 29,000 current or future cancer cases are linked to CT scans performed in the United States of America in
2007 alone (De González et al., 2009)
2Published in Transactions on Machine Learning Research (04/2023)
achieving calibrated INR UQ. MCD outperforms deep ensembles, countering ﬁndings in other BDL setups,
like image classiﬁcation – where ensembling is considered a state-of-the-art NN UQ approach (Ovadia et al.,
2019). MCD also rivals (sometimes outperforming) HMC, which is far more computationally demanding
and diﬃcult to hypertune. Ensembling INRs with MCD achieves further performance gains. Overall, Un-
certaINR attains well-calibrated uncertainty estimates without sacriﬁcing reconstruction quality relative to
other classical, INR-based, and CNN-based reconstruction techniques on realistic, noisy, and underdeter-
mined data.
2 Problem Setting and Related Works
2.1 CT Image Reconstruction
We are interested in reconstructing 2D images, represented as functions f(x,y)of tissue attenuation coef-
ﬁcients at pixel locations (x,y)within an imaging subject cross-section. As visualized in Figure 1b, CT
scanners rotate X-ray emitters and detectors around the subject, collecting measurements of the Radon
transformSφ(r)of the desired image frather than actual image values f(x,y),
Sφ(r) =/integraldisplay /integraldisplay
f(x,y)δ(xcosφ+ysinφ−r)dxdy, (1)
for view-angles φand X-ray detector radii r.Sφ(r)is also known as a sinogram and is not directly human-
interpretable. The reconstruction of f(x,y)fromSφ(r)thus constitutes an inverse problem, governed by
the inverse Radon transform. Appendix A further describes the CT measurement physics and image re-
construction problem. While the Projection Slice Theorem (Bracewell, 1956) ensures that f∗can be fully
reconstructed from the complete sinogram, practical measurement data is ﬁnite and noisy, making the image
reconstruction problem underdetermined and reconstruction UQ desireable.
We assume that image f(x,y)is reconstructed on a ﬁnite set of pixels X×Y(assumed to be grid-spaced for
comparison of INRs with classical grid-based reconstruction techniques), while sinogram measurements are
observed on a ﬁnite set of view-angles Φand X-ray detector radii R. For notational convenience, we denote
theith sinogram measurement Sifori∈{1,...,|Φ×R|}andjth pixel value fjforj∈{1,...,|X×Y|} .
In a slight abuse of notation, we also denote resulting vectors of pixel values and sinogram measurements f
andS, respectively. Thus, the discretization of Eq. 1 is
|X×Y|/summationdisplay
j=1Aijfj=Si,∀i; equivalently, Af=S, (2)
where theijth entry of the discretized Radon transform matrix, A, represents pixel j’s contribution to the
ith prediction measurement, e.g. Aijis zero when pixel jis not along the ray measured by Si.
2.1.1 Classical CT Reconstruction Methods
Among classical approaches for CT reconstruction, ﬁltered backprojection (FBP) is one of the simplest ana-
lytical reconstruction techniques, providing a simple closed-form estimate of the image. By upweighting high
frequencies, FBP enables reconstruction of detailed image structures, but can also emphasize high-frequency
noise, resulting in poor image quality. Iterative reconstruction techniques – e.g. the algebraic reconstruction
technique (ART) (Gordon et al., 1970), simultaneous iterative reconstruction technique (SIRT) (Gilbert,
1972), simultaneous algebraic reconstruction technique (SART) (Andersen & Kak, 1984), and conjugate
gradient for least squares (CGLS) (Yuan & Iusem, 1996) – can mitigate these noise eﬀects.
Another algorithm class frames the reconstruction problem as minimization of a regularized objective,
min
f/bardblAf−S/bardbl2+λT(f), (3)
3Published in Transactions on Machine Learning Research (04/2023)
whereT(f)is a regularizer encoding a prior on f, with regularization strength λ. A regularizer typically
used in medical imaging is the total-variation (TV),
TTV(f) =/summationdisplay
x,y∈X×Y/radicalBig/parenleftbig
f(x+ 1,y)−f(x,y)/parenrightbig2+/parenleftbig
f(x,y+ 1)−f(x,y)/parenrightbig2, (4)
which removes unwanted image noise and artifacts, while preserving important details such as edges (Rudin
et al., 1992). The minimization problem in Eq. 3 with TV regularization is solvable via proximal gradient-
based techniques, e.g. the fast iterative shrinkage-thresholding algorithm (FISTA-TV) (Beck & Teboulle,
2009). Alternatively, expectation-maximization (EM) iteratively maximizes the log likelihood of the pro-
jections given the estimated image (Dong, 2007). While these regularized methods typically outperform
analytic methods, iterative solutions may be computationally slow and their reconstruction quality can be
further improved. Appendix B provides detailed descriptions of these classical techniques.
2.1.2 Deep-Learning CT Reconstruction Methods
Challenges for classical methods have motivated signiﬁcant recent interest in NNs trained with large-scale
datasets to reconstruct high quality CT images from low-dose acquisitions. Because reconstruction requires
only a single NN forward pass, rather than a large number of optimization updates, these methods are
faster than purely iterative methods. Some deep-learning (DL) approaches input analytic low-dose recon-
structed CT images into an NN trained to directly produce artifact-free reconstructions from higher-dose
acquisitions (Chen et al., 2017a;b; Liu & Zhang, 2018; Yang et al., 2018). Alternatively, “unrolled" network
architectures (Adler & Öktem, 2018; Jin et al., 2017; Wu et al., 2019) solve Eq. 3 by chaining together NN
layers such that each layer computes one optimization update. In this work, we compare our methods to the
FBP-Unet (Jin et al., 2017). We also compare to GM-RED (Sun et al., 2021), which uses a deep denoiser
trained on the acquired dataset to deﬁne a reconstructed image prior lying on a manifold of natural images.
We note that while FBP-Unet and GM-RED require large training datasets, our method – leveraging INRs
– requires only a few validation images for hyperparameter tuning.
Although DL has enabled advances in CT reconstruction (Lell & Kachelrieß, 2020), progress has been ham-
pered by the speciﬁc,expensive , andsmallnature of CT datasets. This follows from the existence of various
CT-imaging modalities (e.g. helical, spiral, electron beam, and perfusion imaging) and need for individual
calibration of CT-imaging apparatuses, which compromises the transferability of DL models. CT data collec-
tion is also expensive , requiring long hours on costly machines (exposing patients to harmful radiation) and
labelsbyexpertpractitioners(makinglarge-scaleannotationvirtuallyimpossible). Inresult, CTdatasetsare
small relative to those of other areas, such as ImageNet (Deng et al., 2009), whose size has proven crucial in
enabling state-of-the-art DL performance. In light of these challenges and the aforementioned radiation risk,
there is clear need for NN methods that achieve high-quality CT image reconstruction from small datasets
consisting of few measurements . Combined with our desire for calibrated UQ, this motivates the study of
INRs in our UncertaINR framework.
2.2 Implicit Neural Representations
Implicit neural representations (INRs), implemented via NNs, are functions fθ(·)mapping coordinates xto a
coordinate-wise feature fθ(x)of interest, with parameters θtrained to match some observed signal S. Due to
their general and scalable formulation, INRs have been applied to a wide range of data modalities including:
3D scenes (Sitzmann et al., 2019), voxel grids (Dupont et al., 2022a; Mescheder et al., 2019), video (Li et al.,
2021), and audio (Sitzmann et al., 2020). In this work, we focus on UQ of INRs for CT reconstruction of
a 2D image f, with pixel inputs x= (x,y)∈R2and observed sinogram S. We leave exploration of more
complex data modalities, such as 3D or 4D CT reconstruction, for future work.
In recent years, INRs have grown popular both in computer graphics and unrelated ﬁelds like medical
imaging (Tancik et al., 2020), arguably inspired by state-of-the-art novel view synthesis results achieved by
neural radiance ﬁelds (NeRF) (Mildenhall et al., 2020). Since NeRF, INRs have been successfully applied to:
high-resolution 3D scenes from unstructured collections of 2D images (Martin-Brualla et al., 2021); scalable
large scene view synthesis (Tancik et al., 2022); generative modelling (Dupont et al., 2022b); meta-learning
4Published in Transactions on Machine Learning Research (04/2023)
(Tancik et al., 2021); and image segmentation of medical scans (Khan & Fang, 2022). Meanwhile, several
improvements in INR architectures have accompanied these empirical gains, such as random Fourier feature
(RFF) (Rahimi & Recht, 2007) encodings (Tancik et al., 2020) and periodic activation functions (Sitzmann
et al., 2020), both of which have a tunable frequency hyperparameter Ω0that enables INRs to represent
high frequency functions. In UncertaINR, we adopted RFF encodings, detailed in Appendices F.3 and F.4,
and similarly found Ω0critical for decent reconstruction accuracy (Appendix F.5.1). One key challenge for
INRs is their long evaluation times. However, recent literature has focused on addressing this challenge,
successfully leveraging sparse voxel models (Fridovich-Keil et al., 2022) and multiresolution hash-encodings
(Müller et al., 2022) to reduce evaluation times by orders of magnitude.
Recent work has also demonstrated the applicability of INRs to CT image reconstruction. Reed et al.
(2021) utilize parametric motion ﬁeld warped INRs to perform limited view 4D-CT reconstruction of rapidly
deforming scenes. Sun et al. (2021) propose Coordinate-based Internal Learning (CoIL) and Zang et al.
(2021) propose IntraTomo, which use INRs to boost the performance of classical reconstruction algorithms,
such as those discussed in Section 2.1. In CoIL, an INR learns a functional form of the sinogram, receiving
sinogram location (φ,r)as input and outputting projection measurement Sφ(r). This functional sinogram
generates artiﬁcial measurements from view angles not included in the original measurement sinogram.
The reconstruction algorithm leverages this artiﬁcially INR-enlarged measurement set to achieve improved
performance reconstructing image f, over the same algorithm trained on the original, smaller measurement
dataset.
We note that no existing INR works have addressed the aforementioned need for calibrated UQ, motivating
our proposed UncertaINR framework.
3 UncertaINR: Uncertainty Quantiﬁcation of INRs for CT
To quantify the uncertainty in reconstructing image f, given sinogram measurements S, we reformulate the
CT reconstruction problem, Eq. 3, as one of Bayesian inference. We assume a Gaussian measurement model,
Si|find.∼ N (Aif,σ2)∀i∈{1,...,|Φ×R|}, (5)
whereσ2is an assumed known observation noise, and Aiis theith row of the discretized Radon transform
A.2Once a prior distribution is placed over f, the posterior distribution over fcan be computed. The
posterior distribution captures both plausible reconstructions of f(e.g. via the posterior mean), as well as
the uncertainty over reconstructions (e.g. via the posterior standard deviation).
An important practical consideration is to choose how the image fis parameterized. In the following we
compare two alternative parameterizations: a classical grid-based baseline and our INR-based proposal. For
each of these parameterizations we will also discuss appropriate priors for f.
Grid-of-Pixels Baseline As a baseline, we parameterize the image fusing a pixel-wise grid representation;
speciﬁcally, we take f∈R|X×Y|. A sensible prior, which prefers smoothness in images while allowing for
important details such as edges, is p(f)∝exp(−λTTV(f)), whereTTV(f)is the total-variation of f. In this
case the posterior distribution is
p(f|S)∝exp

−1
2σ2|Φ×R|/summationdisplay
i=1/parenleftbig
Si−Aif/parenrightbig2−λTTV(f)

. (6)
This is a Bayesian extension to the grid-based methods of Section 2.1.1, with Eq. 3 corresponding to the
maximum a posteriori (MAP) solution to Eq. 6. In our experiments we compare INR-based inferences to
this discretized baseline, which we refer to as Grid-of-Pixels (GOP).
UncertaINR Alternatively, in this work, we parameterize fas an INR fθwith parameters θ∈Rp, map-
ping from pixel coordinates (x,y)to pixel values fθ(x,y). A reasonable prior for θis an independent and
2The end-to-end INR approach introduced in Tancik et al. (2020) can thus be viewed as maximum likelihood using Eq. 5’s
measurement model.
5Published in Transactions on Machine Learning Research (04/2023)
Figure 2: UncertaINR Architecture : An end-to-end INR with UQ (BBB, MCD, HMC, and/or DEs) is
sampled across all image pixels, generating a distribution of predicted images, {fθn}N
n=1. The predicted
sinogram,generatedbytheRadontransformofpredictedmeanimage ˆf,iscomparedtothetruemeasurement
data in the INR loss, L. Once training completes, ˆfis reported as the reconstructed image and the predictive
distribution across INR samples is used to quantify uncertainty.
identically-distributed zero-mean Gaussian prior, pN(θ). This gives rise to an implicit prior distribution over
functionsfθ. Such a prior is standard in the Bayesian Deep Learning (BDL) literature (Blundell et al., 2015;
Graves, 2011), and is well-known to yield a Gaussian Process (GP) limit over fθfor wide NNs (Neal, 1996;
Matthews et al., 2018; Lee et al., 2018). However, properties of implicit parameter priors are less understood
for ﬁnite-width NNs, even in standard BDL applications like image classiﬁcation (due to the uninformative
nature of NN parameter spaces), and much less so for CT reconstruction. Thus, we choose a composite prior
for UncertaINR,
p(θ)∝pN(θ)pI(θ)
pI(θ) = exp(−λT(fθ))(7)
combining pN(θ), which constrains the NN parameter values, with T(fθ), which imposes a smooth regu-
larization constraint on implicit images fθparameterized by θ. We adopt the common medical imaging
practice of using TV regularization, T=TTV(Eq. 4). To the best of our knowledge, this constitutes the
ﬁrst application of TV regularization to INRs, which can be seen in Appendix G.2 Table 7 to noticeably
improve UncertaINR reconstruction performance.
Inference Our overall framework for UncertaINR is illustrated in Figure 2. Given parameter prior p(θ),
modelfθ, and likelihood model p(S|fθ)from Eq. 5, we apply Bayes’ rule to p(θ|S)∝p(S|fθ)p(θ), deriving
the parameter space posterior distribution p(θ|S). Given a set of sinogram measurements S, we ideally seek
to sample from the posterior
p(θ|S)∝exp

−1
2σ2|Φ×R|/summationdisplay
i=1/parenleftbig
Si−Aifθ/parenrightbig2−1
2τ2||θ||2
2−λTTV(fθ)

, (8)
which assigns high probability to images that strike a balance between: 1) a small error between reconstruc-
tionfθ’s Radon transform and observed measurements Si, and 2) low regularization cost under prior p(θ).
τ2is a variance hyperparameter for the zero-mean Gaussian parameter prior pN(θ), in Eq. 8.
GivenNposterior INR parameter samples, {θn}N
n=1, from Eq. 8, we can then use the induced posterior
image samples{fθn}N
n=1to infer both image reconstruction and UQ over the reconstruction. For example,
the reconstructed image can be estimated, e.g. by the posterior mean over locations (x,y)as
ˆf(x,y) =1
NN/summationdisplay
n=1fθn(x,y), (9)
6Published in Transactions on Machine Learning Research (04/2023)
and the posterior predictive uncertainty can be characterized, e.g. through the predictive variance as
ˆσ2(x,y) =1
N−1N/summationdisplay
n=1/parenleftbig
fθn(x,y)−ˆf(x,y)/parenrightbig2. (10)
This can be used to visualize, as in Figures 1a and 3b, regions of varying model uncertainty and calculate
uncertainty metrics such as coverage and calibration (Section 4.1).
Approximate Inference Unfortunately, exact inference in BDL is usually intractable – due in part to
the high-dimensional and complicated nature of NN posterior landscapes – meaning one must resort to
approximate inference methods. Several BDL algorithms have been proposed in the literature, based on
approximations with varying levels of sophistication and implementation complexity. In our experiments,
we consider four approaches: Bayes-By-Backprop (BBB),Monte Carlo Dropout (MCD),Hamiltonian Monte
Carlo(HMC), and deep ensembles (DEs).BBBis a variational evidence lower bound minimization proce-
dure based on stochastic gradient descent (Blundell et al., 2015). MCDuses samples at test-time from NNs
trained with dropout (Srivastava et al., 2014) (motivated as a variational approximation of a deep Gaussian
process (Gal & Ghahramani, 2016)). HMC(Neal et al., 2011) is a gold-standard yet more computationally
expensive Markov Chain Monte Carlo (MCMC) procedure leveraging Hamiltonian dynamics (via a time-
reversible and volume-preserving integrator) to better explore the full distribution typical set, decreasing
consecutive sample correlation and reducing the number of samples required for convergence to the pos-
terior (Betancourt, 2017; Duane et al., 1987). Finally, DEsquantify predictive uncertainty by ensembling
the outputs of several NN “base learners” trained for the same task with diﬀerent random seeds (Laksh-
minarayanan et al., 2017), and is generally regarded as a state-of-the-art approach for UQ in NNs (Ovadia
et al., 2019). Despite a non-Bayesian motivation in Lakshminarayanan et al. (2017), the relationship be-
tween Bayesian inference and DEs is an active area of research in the BDL community. Wilson & Izmailov
(2020) argue that DEs provide a more compelling approximation to the true posterior than many standard
BDL approaches, whilst others have adapted DEs to provide a Bayesian interpretation (Ciosek et al., 2019;
D’Angelo & Fortuin, 2021; Pearce et al., 2020; He et al., 2020).
Implementation Details We note that ensembling can be combined with any of the three prior BDL meth-
ods to further improve performance and, in particular, we found that ensembling MCD base learners provides
additional gains, in practice, with UncertaINR. Surprisingly, our experiments show that MCD, arguably the
simplest BDL approach, produces high-quality reconstructions, outperforming DEs and comparable results
to the complex HMC procedure. MCD trains a model fθto minimize the unnormalized version of Eq. 8 with
dropout (Srivastava et al., 2014) before every weight layer. The samples {fθn}nof Eqs. 9-10 are obtained
by simply performing dropout at inference time, i.e. disabling each internal network node according to a
pre-set probability pfor diﬀerent random seeds and applying a forward pass over image coordinates (x,y)to
obtain the pixel values fθn(x,y). Hence, the MCD samples from the approximate posterior can be obtained
very eﬃciently after training. For HMC, we used the No-U-Turn-Sampler (Hoﬀman et al., 2014) sampling
scheme in NumPyro (Phan et al., 2019), both for UncertaINR, and also to obtain UQ with GOP. Further
details of the approximate inference algorithms we consider are given in Appendix D.
4 Experimental Results
Project code is available at: https://github.com/bobby-he/uncertainr .
4.1 Experimental set up
Datasets Two datasets were considered. The ﬁrst consists of artiﬁcial 256×256pixel Shepp-Logan phan-
tom (Shepp & Logan, 1974) brain images and was used for ablations. Given the data’s simple nature,
ablations were performed in the extremely low measurement settings of 5- and 20-view sinograms. We tuned
hyperparameters on 5 validation images and evaluated performance on 5 test images. The second dataset,
used for UncertaINR baseline comparisons, contains 512×512pixel abdominal CT scan images, provided by
7Published in Transactions on Machine Learning Research (04/2023)
the Mayo Clinic for the 2016 Low-Dose CT AAPM Grand Challenge (McCollough et al., 2017). 3 validation
images and 8 test images were used to generate noisy 60- and 120-view sinograms3
Performance Metrics We assessed reconstructed image samples {fθn}N
n=1via four metrics: peak-signal-
to-noise ratio (PSNR), signal-to-noise ratio (SNR),negative log-likelihood (NLL), and expected calibration
error(ECE).PSNRandSNRare common measures of predictive accuracy, whose equations we present in
Appendix C. NLLis a common probabilistic model quality metric, assessing both predictive accuracy and
uncertainty calibration. Under the assumption of an independent Gaussian model, each pixel’s averaged
prediction ˆf(x,y)(Eq. 9) is sampled from a Gaussian distribution with the ground truth pixel value f∗(x,y)
as mean and calculated predicted variance of pixel responses, ˆσ2(x,y)from Eq. 10, as variance. The NLL is
the negative log-likelihood of the pixel values under this model,
NLL({fθ}n,f∗)=/summationdisplay
x,y1
2ˆσ2(x,y)/parenleftbigˆf(x,y)−f∗(x,y)/parenrightbig2+1
2log(2πˆσ2(x,y)).
Ideally, the model maximizes the likelihood, meaning NLL is minimized when ˆf(x,y) =f∗(x,y)for all (x,y)
and variance ˆσ2(x,y)is small.
Finally,ECEassesses model prediction of its outcome probabilities, gauging reliability of the model’s pre-
diction conﬁdence. Speciﬁcally, it describes the discrepancy between the target coverage (TC) and achieved
coverage (AC). Given Nimage samples{fθn}N
n=1, each pixel (x,y)has an empirical distribution of Npre-
dicted values, ˆFN(x,y) ={fθn(x,y)}N
n=1. Ideally, the median of this distribution is the ground truth pixel
value,f∗(x,y). For a given TC pwe deﬁne,
CˆFN,f∗,p(x,y) =I/braceleftbigg
Q50−p
2/parenleftBig
ˆFN(x,y)/parenrightBig
≤f∗(x,y)≤Q50+p
2/parenleftBig
ˆFN(x,y)/parenrightBig/bracerightbigg
, (11)
whereQk(F)denotes the kth quantile of distribution F. AC is thus deﬁned as the percentage of pixel
distributions containing f∗(x,y)in that quantile,
AC(f∗,ˆFN,p) =1
|X×Y|/summationdisplay
x,yCˆFN,f∗,p(x,y). (12)
If a model is perfectly calibrated, p%of the reconstructed pixel distributions will contain f∗(x,y)in their
p%quantile, meaning AC =TC for all quantiles. Given a ﬁnite set, P, of percentages evenly spaced in [0,1],
ECE is deﬁned as the average diﬀerence between AC and TC,
ECE(f∗,ˆFN) =1
|P|/summationdisplay
p∈P|AC(f∗,ˆFN,p)−p|. (13)
Areliability curve , asvisualizedinFigure3b, plotsACasafunctionofTC.Bettermodelcalibrationproduces
curves similar to the identity function, AC (f∗,ˆFN,p) =p. Furthermore, akin to inverse transform sampling,
the marginal distribution of the inverse quantiles of calibrated pixel predictive distributions ˆFN(x,y), at
ground truth pixel values f∗(x,y), should be uniformly distributed in [0,1]. We visualize such coverage
histograms forUncertaINRinFigure3b. Forafurtherdiscussionofcalibration,coverage,andimplementation
details, see Appendix C.
4.2 Experiments on Artiﬁcial (Shepp-Logan) Data
Ablations Experiments were performed on the artiﬁcial Shepp-Logan dataset, described in Section 4.1, to
ablate diﬀerent UncertaINR hyperparameters across BDL approaches. We ablated the activation function
(Tanh, SoftPlus, Sine, SiLU, and ReLU), depth, width, and random Fourier feature (RFF) embedding
frequency. For MCD we also assessed sensitivity to the dropout probability p, while for BBB we ablated prior
standarddeviationandKLfactor. Forthesakeofbrevity, weonlyreportmainﬁndingshere(detailedanalysis
3Gaussian noise was added to achieve a 40dB SNR relative to the original, noiseless sinogram.
8Published in Transactions on Machine Learning Research (04/2023)
Table 1:Ablation Study : UncertaINR accuracy, relative to classical approaches, and calibration results on
the Shepp-Logan phantom dataset. Results are averaged across 5 validation and 5 test images, with the best
result for each metric (PSNR, NLL, and ECE) bolded.
Reconstruction 5-ViewValidation Set 5-ViewTest Set 20-View Validation Set 20-View Test Set
Type PSNR ( ↑) NLL (↓) ECE (↓) PSNR (↑) NLL (↓) ECE (↓) PSNR (↑) NLL (↓) ECE (↓) PSNR (↑) NLL (↓) ECE (↓)
FBP 7.68 – – 5.15 – – 17.35 – – 15.71 – –
CGLS 16.38 – – 14.62 – – 21.85 – – 20.82 – –
EM 21.39 – – 19.88 – – 30.22 – – 29.11 – –
SART 21.12 – – 19.75 – – 31.97 – – 30.45 – –
SIRT 21.12 – – 21.12 – – 31.98 – – 30.44 – –
BBB UINR 23.26 -1.190 0.152 22.52 0.138 0.203 28.25 1.650 0.121 28.16 0.562 0.119
MCD UINR 26.15 -1.473 0.111 24.45 -1.572 0.083 33.74 0.701 0.135 33.08 1.093 0.113
DE-2 MCD UINR 26.31 -1.730 0.091 24.49 -1.774 0.069 33.96 0.005 0.136 33.44 -0.372 0.102
DE-5 MCD UINR 26.44 -1.737 0.085 24.88 -1.751 0.067 34.31 -0.364 0.134 34.02 -0.625 0.101
DE-10 MCD UINR 26.36 -2.226 0.075 24.67 -1.969 0.068 34.38 -0.529 0.131 33.86 -0.774 0.096
is provided in Appendix F.5). Activation function and RFF frequency were found to be the most critical
hyperparameters. Sineactivationproducedthebest-performingmodels, butresultingnetworksweresensitive
tohyperparameterchoice. SiLU,ReLU,andTanhachievedslightlylower, butmoreconsistentreconstruction
accuracies. In line with recent work demonstrating RFF importance for learning high-frequency image
components (Tancik et al., 2020), we found that RFF frequency signiﬁcantly aﬀected model performance.
Speciﬁcally, RFF frequency must be consistent with the number of view angles – too low (high) an RFF
frequency leads to blurry images (high-frequency image artifacts).
Model comparison We also used the Shepp-Logan dataset to compare low-complexity reconstruction and
UQ methods – including UncertaINR with BBB, MCD, and DEs of (2, 5, and 10) MCD base learners, as
well as classical reconstruction baselines (FBP, CGLS, EM, SART, and SIRT). For the latter, without UQ,
only reconstruction accuracy is reported. Several conclusions can be drawn from the experimental results
summarized in Table 1, with further analysis on variability and signiﬁcance presented in Appendix F.7. First,
MCD UINRs signiﬁcantly outperform BBB UINRs, e.g. with PSNR gains up to 5dB (20-view test set) and
ECE reductions by 2/3(5-view test set). Due to this poor performance (more detail in Appendix F.6.1), BBB
UINRs were not considered in later experiments. Second, all MCD-based methods signiﬁcantly outperformed
classical methods in terms of reconstruction accuracy, with gains up to ∼4dB PSNR in both test sets.
Third, among UncertaINR methods, DE-5 and DE-10 MCD UINRs achieved the best performance, but the
simple MCD UINR was surprisingly close (within 1dB PSNR of the best ensemble in all cases). Similar
conclusions can be drawn with respect to UQ. Furthermore, despite small validation set size (5 images),
UINRs generalized well to the test set. For 20-views, validation and test accuracies were comparable, with
improved test set calibration. For 5-views, despite slight PSNR and NLL degradation, ECE improved in the
test set. These results suggest that small validation sets are suﬃcient for tuning UncertaINRs.
4.3 Experiments on Real-World (AAPM Grand Challenge) Data
We next compared UncertaINR to state-of-the-art reconstruction approaches, on the real-world AAPM
dataset, described in Section 4.1. Results are reported in Table 2, with further analysis on variability and
signiﬁcance in Appendix G.3. UncertaINRs were trained with MCD, DEs of INRs, DEs of MCD UINRs, and
HMC–allimplementedwithTVregularization. Moreinformationaboutdatasetandmodelhyperparameters
is given in Appendix G. To understand the eﬀect of UQ on reconstruction accuracy, we implemented our
end-to-end INR without UQ (“INR” in Table 2), similar to Tancik et al. (2020)’s proposal. For 60-views,
MCD UINRs, DE UINRs and DE MCD UINRs outperformed INRs, whereas HMC UINRs were competitive,
but underperformed. Overall, adding UQ did not harm INR reconstruction accuracy.
The lowest block of Table 2 presents results of CNNs trained on large datasets: FBP-UNet, GM-RED4,
and these methods with CoIL. We emphasize that these methods are trained on largetraining datasets,
while INRs require only a handful of images for hyperparameter tuning. Thus, the results of these methods
4Note that, while the COIL work leveraged GM-RED trained with deep image denoisers, RED can be used with denoisers
that require less training data.
9Published in Transactions on Machine Learning Research (04/2023)
provide an upper bound on the reconstruction accuracy expected of UncertaINRs, but the two are not
directly comparable.
Nevertheless, all (U)INRs achieve competitive performance, reaching accuracies within 1dB (60-views) or
1.5dB (120-views) of the best (large training dataset) CNN. In fact, in the low-measurement regime (60-
views), all (U)INRs achieved competitive performance with the highest reconstruction accuracy UINR (DE-5
MCD UINR) – which outperformed all methods, except FBP-UNET with CoIL.
Table2:PerformanceAssessment : Accuracyandcal-
ibration results of all approaches are presented for the
AAPM dataset, with noise added to achieve a 40dB
SNR sinogram. The table is divided into 4 method
types: classical, GOP, INRs, and DL. (*) denotes re-
sults taken directly from (Sun et al., 2021). Results
are averaged over all 8 test set images and the best re-
sult for each metric (SNR, NLL, and ECE) is bolded
(excluding italicized DL methods, which rely on sub-
stantially more training data, constituting an unfair
UINR comparison).
Reconstruction 60-Views 120-Views
Method SNR NLL ECE SNR NLL ECE
FBP 10.58 – – 14.11 – –
EM 14.47 – – 15.55 – –
CGLS 20.08 – – 21.94 – –
SIRT 20.89 – – 21.36 – –
SART 21.54 – – 21.77 – –
FISTA-TV* 26.08 – – 27.59 – –
FBP CoIL* 23.48 – – 24.52 – –
FISTA-TV CoIL* 26.95 – – 28.95 – –
GOP-TV 25.97 – – 27.40 – –
HMC GOP-TV 25.10 -2.604 0.102 26.82 -3.367 0.102
INR 27.25 – – 28.81 – –
DE-2 UINR 27.29 24.55 0.224 28.83 18.86 0.222
DE-5 UINR 27.30 8.427 0.176 28.83 8.804 0.183
DE-10 UINR 27.28 6.882 0.144 28.82 6.346 0.162
MCD UINR 27.38 -3.447 0.078 28.65 -3.759 0.071
DE-2 MCD UINR 27.44 -3.573 0.063 28.68 -3.819 0.056
DE-5 MCD UINR 27.48-3.660 0.051 28.70 -3.876 0.043
DE-10 MCD UINR 27.46 -3.689 0.04528.74 -4.090 0.053
HMC UINR 27.10 -3.963 0.074 28.50 -4.021 0.085
FBP-UNet* 27.08 – – 29.18 – –
GM-RED* 27.12 – – 29.30 – –
FBP-UNet CoIL* 27.93 – – 29.71 – –
GM-RED CoIL* 27.42 – – 29.79 – –The top 3 blocks of Table 2 present results for meth-
ods that can be fairly compared to the (U)INRs: 1)
classical FBP, CGLS, EM, SART, and SIRT meth-
ods, 2) Sun et al. (2021)’s results for FISTA-TV,
FBP with CoIL, and FISTA-TV with CoIL, and
3) TV regularized GOP with and without HMC.
Similarly to the previously presented results for
synthetic Shepp-Logan data, most classical meth-
ods underperform the INRs by more than 5dB on
AAPM. Only FISTA-TV is competitive, albeit still
more than 1dB away from the INRs. While adding
CoILtoFBPandFISTA-TVimprovesperformance,
these methods still perform worse than all 60-view
(U)INRs. Furthermore, unlike CoIL-based meth-
ods, UncertaINR is end-to-end and does not require
pre-processing. Finally, we found the discretized
GOP instatiation signiﬁcantly underperformed all
(U)INR methods – highlighting the power of INRs
relative to classical grid/voxel approaches.
In terms of uncertainty calibration, Table 2’s results
are surprising. Contrary to common BDL intuition
thatDEsachievethebestmodelcalibration(Ovadia
et al., 2019), we found MCD to be more eﬀective for
INR UQ calibration. Speciﬁcally, MCD UINRs and
DEs of MCD UINRs achieved signiﬁcantly better
model calibration than DEs of INRs without uncer-
tainty. For example, introducing MCD to a DE-10
UINR reduced ECE from 0.144 to 0.045 (60-views)
and from 0.162 to 0.053 (120-views). Although in-
creasing ensemble sizes generally improved model
calibration, the performance boost was not as sig-
niﬁcant as that of using MCD. We do not currently
have a full understanding of why DEs perform poorly relative to MCD for UINRs, in contrast to standard
BDL applications. However, in Appendix F.6, we hypothesize that this may be due to the model capacity
of an individual ensemble member, which is dictated through the RFF encoding frequency Ω0.
Furthermore, although HMC is often considered a “golden standard” of approximate Bayesian inference (Iz-
mailov et al., 2021), MCD UINRs achieved better performance than HMC GOP and HMC UINRs. While
HMC UINRs achieved NLL competitive with the best DEs of MCD UINRs, they only achieved ECE compet-
itive with the MCD UINR and noticeably lower SNR than all (U)INR approaches. Similar to observations in
the Bayesian deep learning literature (Wenzel et al., 2020) and discussed in Appendix G, we found a related
cold posterior eﬀect in which modifying posterior temperature enables either improved SNR across HMC
samples or improved UQ calibration, but not both. In all, given the large computational overhead in tuning
and training HMC relative to MCD, MCD DEs appear to oﬀer the best compromise between computational
speed, reconstruction accuracy, and well-calibrated uncertainty.
10Published in Transactions on Machine Learning Research (04/2023)
0.00 0.01 0.02 0.03HMC
GOP
DE-2
DE-5
DE-10
UINR
MCD
UINR
DE-2
MCD
UINR
DE-5
MCD
UINR
DE-10
MCD
UINR
HMC
UINRAbs. Error
Std. Dev.
(a)Calibration
HMC GOPReconstructed Mean
DE-10 UINR
 MCD UINR
 DE-5 MCD UINR
 DE-10 MCD UINR
 HMC UINR
0.00.20.40.60.81.0
Absolute Error
0.000.010.020.030.040.05
Variance
0.00000.00010.00020.00030.00040.0005Coverage
0.0 0.5 1.0
Target CoverageReliability
0.0 0.5 1.0
Target Coverage
0.0 0.5 1.0
Target Coverage
0.0 0.5 1.0
Target Coverage
0.0 0.5 1.0
Target Coverage
0.0 0.5 1.0
Target Coverage0.00.51.0Achieved Coverage
 (b)Reconstruction and Uncertainty Quantiﬁcation on AAPM
Figure 3: For an AAPM test image and the UQ approaches of this work: (a)Violin plots of pixel-wise
absolute error and predicted standard deviation distributions indicate model calibration. (b)Visualizations
of the reconstructed mean, absolute error, predicted variance, coverage, and reliability.
ThebeneﬁtsofMCDforINRcalibrationarefurtherillustratedbytheviolinplotsofFigure3a, comparingthe
pixel-wise absolute error versus predicted standard deviation distributions across GOP and UINR models.
For non-MCD DEs, the standard deviation distribution skews towards smaller values than the absolute
error distribution, indicating that the model is overconﬁdent. The opposite is true for HMC GOP, which is
underconﬁdent. Meanwhile, HMC and MCD UINRs predicted standard deviation distributions most closely
resembling those of the absolute error, indicating decent calibration.
Finally, Figure 3b visualizes the model output, calibration diagnostics, and uncertainty on a single test image
for GOP and the diﬀerent UINRs proposed in this work. Similar ﬁgures are presented for the remaining
test images in Appendix G. As reﬂected in the absolute error images, the GOP reconstructed mean is blurry
relative to those of the UINRs. Meanwhile, the reliability curves and coverage plots of both HMC GOP and
the DE-10 UINR are quite poor relative to the nearly uniform coverage plots and ideal reliability curves
of the (DE) MCD UINRs and HMC UINR. However, these absolute error, coverage, and reliability metrics
require the ground truth image to compute and thus would not be available to a doctor.
In real-world scenarios, the only visualizations available are the reconstructed mean and variance images.
Given that a well-calibrated model reports larger variance in regions of larger absolute error, variance can
be used as a proxy for reconstruction error. For example, the Figure 3b absolute error images show that the
models underperform at predicting boundaries between diﬀerent tissues, which is reﬂected in corresponding
higher uncertainties in the variance images. When presented to a doctor, our UncertaINR variance images
could inform more cautious diagnoses based on perceived issues in those regions.
11Published in Transactions on Machine Learning Research (04/2023)
5 Limitations, Broader Impact, and Future Work
Despite its decent performance, UncertaINR should not be considered as a replacement for professional
medical diagnosis but simply as a tool to aid it. In this work, UncertaINR was thoroughly studied on
two datasets, with AAPM data representing a retrospectively-simulated low-dose acquisition. While these
experiments oﬀer a promising proof-of-concept of uncertainty quantiﬁcation for INR-based low-dose CT
reconstruction, additional evaluation should be performed on larger, real acquired low-dose CT data before
this strategy is deployed in medical settings.
Possible future work includes evaluating UncertaINR in other modalities, such as MRI or 3D/4D imaging
settings. In such extensions it would be beneﬁcial to further improve training and memory eﬃciency of
UncertaINR, and to do so UncertaINR could be combined with recent INR-related advances, such as meta-
learning (Tancik et al., 2021) and compression (Dupont et al., 2021). Another possibility is to leverage the
predictive uncertainty achieved by UINR in an active learning (Cohn et al., 1996) setting, enabling eﬃcient
measurement procedures and reducing harmful patient radiation exposure. Finally, our ﬁndings raise some
fundamental questions about UQ in the INR setting, such as the poor performance of deep ensembles and
the eﬀectiveness of MCD, which counter common beliefs in, and should be of interest to, the Bayesian deep
learning community.
6 Conclusion
In this work, we proposed UncertaINR: a Bayesian reformulation of INR-based image reconstruction. In
the high-stakes and well-motivated application of CT image reconstruction, UncertaINR attained calibrated
uncertainty estimates without sacriﬁcing reconstruction quality relative to other classical, INR-based, and
CNN-based reconstruction techniques on realistic, noisy, and underdetermined data. In the context of
INR UQ, contrary to common intuition, we found that simple and eﬃcient MC Dropout rivaled (even
outperformed)thepopulardeepensemblesandthesophisticated, yetcomputationally-expensiveHamiltonian
Monte Carlo methods. UncertaINR’s strategic use of INRs outperformed classical reconstruction approaches,
while alleviating key challenges faced by state-of-the-art DL methods – namely generalizability and small-
scale medical datasets. In addition to informing doctor diagnoses, UncertaINR’s well-calibrated uncertainty
estimates could pave the way for reduced healthcare costs, via methods like automated triage, and reduced
patient radiation exposure, via methods like active learning.
7 Acknowledgements
Francisca Vasconcelos primarily carried out this work at the University of Oxford, while supported by the
RhodesTrustviaaRhodesScholarship. SheisnowatUCBerkeley, supportedbyanNSFGraduateResearch
Fellowship under grant number DGE 2146752. Bobby He is supported by the EPSRC and MRC through
the OxWaSP CDT programme (EP/L016710/1). Nalini Singh is supported by a Google PhD Fellowship.
References
Michael D Abramoﬀ, Philip T Lavin, Michele Birch, Nilay Shah, and James C Folk. Pivotal Trial of an
Autonomous AI-based Diagnostic System for Detection of Diabetic Retinopathy in Primary Care Oﬃces.
NPJ digital medicine , 1(1):1–8, 2018.
Ben Adlam, Jasper Snoek, and Samuel L Smith. Cold Posteriors and Aleatoric Uncertainty. arXiv preprint
arXiv:2008.00029 , 2020.
Jonas Adler and Ozan Öktem. Learned Primal-Dual Reconstruction. IEEE transactions on medical imaging ,
37(6):1322–1332, 2018.
Laurence Aitchison. A Statistical Theory of Cold Posteriors in Deep Neural Networks. In International
Conference on Learning Representations , 2020.
12Published in Transactions on Machine Learning Research (04/2023)
Anders H. Andersen and Avinash C. Kak. Simultaneous Algebraic Reconstruction Technique (SART): A
Superior Implementation of the ART Algorithm. Ultrasonic Imaging , 6(1):81–94, 1984.
Riccardo Barbano, Javier Antoran, José Miguel Hernández-Lobato, and Bangti Jin. A Probabilistic Deep
Image Prior over Image Space. In Fourth Symposium on Advances in Approximate Bayesian Inference ,
2021.
Amir Beck and Marc Teboulle. A Fast Iterative Shrinkage-Thresholding Algorithm for Linear Inverse Prob-
lems.SIAM journal on imaging sciences , 2(1):183–202, 2009.
M.J. Berger, J.S. Coursey, M.A. Zucker, and J. Chang. ESTAR, PSTAR, and ASTAR: Computer Programs
for Calculating Stopping-Power and Range Tables for Electrons, Protons, and Helium Ions . National
Institute of Standards and Technology, Gaithersburg, MD, 2004. (version 1.2.3) Availabile: http://
physics.nist.gov/Star [2021, 08, 22]. Originally published as: Berger, M.J., NISTIR 4999, National
Institute of Standards and Technology, Gaithersburg, MD (1993).
Michael Betancourt. A Conceptual Introduction to Hamiltonian Monte Carlo. arXiv preprint
arXiv:1701.02434 , 2017.
Lukas Biewald. Experiment Tracking with Weights and Biases, 2020. URL https://www.wandb.com/ .
Software available from wandb.com.
Charles Blundell, Julien Cornebise, Koray Kavukcuoglu, and Daan Wierstra. Weight Uncertainty in Neural
Network. In International Conference on Machine Learning , pp. 1613–1622. PMLR, 2015.
Ronald N Bracewell. Strip Integration in Radio Astronomy. Australian Journal of Physics , 9(2):198–217,
1956.
James Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal Maclaurin,
George Necula, Adam Paszke, Jake VanderPlas, Skye Wanderman-Milne, and Qiao Zhang. JAX: compos-
able transformations of Python+NumPy programs, 2018. URL http://github.com/google/jax .
David J. Brenner and Eric J. Hall. Computed Tomography — An Increasing Source of Radiation Exposure.
New England Journal of Medicine , 357(22):2277–2284, 2007.
David R. Bull. Chapter 4 - Digital Picture Formats and Representations. In David R. Bull (ed.), Commu-
nicating Pictures , pp. 99–132. Academic Press, Oxford, 2014. ISBN 978-0-12-405906-1.
GaryS.BustandCathrynN.Mitchell. History, CurrentState, andFutureDirectionsofIonosphericImaging.
Reviews of Geophysics , 46(1), 2008.
Hu Chen, Yi Zhang, Mannudeep K Kalra, Feng Lin, Yang Chen, Peixi Liao, Jiliu Zhou, and Ge Wang.
Low-dose CT with a Residual Encoder-Decoder Convolutional Neural Network. IEEE transactions on
medical imaging , 36(12):2524–2535, 2017a.
Hu Chen, Yi Zhang, Weihua Zhang, Peixi Liao, Ke Li, Jiliu Zhou, and Ge Wang. Low-dose CT via Convo-
lutional Neural Network. Biomedical optics express , 8(2):679–694, 2017b.
Zhiqin Chen and Hao Zhang. Learning Implicit Fields for Generative Shape Modeling. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition , pp. 5939–5948, 2019.
Siddhartha Chib and Edward Greenberg. Understanding the Metropolis-Hastings Algorithm. The american
statistician , 49(4):327–335, 1995.
Kamil Ciosek, Vincent Fortuin, Ryota Tomioka, Katja Hofmann, and Richard Turner. Conservative un-
certainty estimation by ﬁtting prior networks. In International Conference on Learning Representations ,
2019.
David A. Cohn, Zoubin Ghahramani, and Michael I. Jordan. Active Learning with Statistical Models.
Journal of Artiﬁcial Intelligence Research , 4:129–145, 1996.
13Published in Transactions on Machine Learning Research (04/2023)
Beau Coker, Wessel P Bruinsma, David R Burt, Weiwei Pan, and Finale Doshi-Velez. Wide Mean-Field
Bayesian Neural Networks Ignore the Data. In International Conference on Artiﬁcial Intelligence and
Statistics , pp. 5276–5333. PMLR, 2022.
GeorgeCybenko. ApproximationbySuperpositionsofaSigmoidalFunction. Mathematics of Control, Signals
and Systems , 2(4):303–314, 1989.
Francesco D’Angelo and Vincent Fortuin. Repulsive Deep Ensembles are Bayesian. In A. Beygelzimer,
Y. Dauphin, P. Liang, and J. Wortman Vaughan (eds.), Advances in Neural Information Processing Sys-
tems, 2021.
Jeﬀrey De Fauw, Joseph R Ledsam, Bernardino Romera-Paredes, Stanislav Nikolov, Nenad Tomasev, Sam
Blackwell, Harry Askham, Xavier Glorot, Brendan O’Donoghue, Daniel Visentin, et al. Clinically Ap-
plicable Deep Learning for Diagnosis and Referral in Retinal Disease. Nature medicine , 24(9):1342–1350,
2018.
Amy Berrington de Gonzalez and Sarah Darby. Risk of Cancer from Diagnostic X-Rays: Estimates for the
UK and 14 Other Countries. The Lancet , 363(9406):345–351, 2004.
AmyBerringtonDeGonzález,MahadevappaMahesh,Kwang-PyoKim,MythreyiBhargavan,RebeccaLewis,
Fred Mettler, and Charles Land. Projected Cancer Risks from Computed Tomographic Scans Performed
in the United States in 2007. Archives of Internal Medicine , 169(22):2071–2077, 2009.
Stanley R. Deans. The Radon Transform and Some of Its Applications . Courier Corporation, 2007.
Boyang Deng, John P Lewis, Timothy Jeruzalski, Gerard Pons-Moll, Geoﬀrey Hinton, Mohammad Norouzi,
and Andrea Tagliasacchi. NASA: Neural Articulated Shape Approximation. In Computer Vision–ECCV
2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part VII 16 , pp. 612–
628. Springer, 2020.
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A Large-Scale Hierarchical
Image Database. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition ,
pp. 248–255, 2009.
Armen Der Kiureghian and Ove Ditlevsen. Aleatory or Epistemic? Does It Matter? Structural Safety , 31
(2):105–112, 2009.
Bao-Yu Dong. Image Reconstruction using EM Method in X-Ray CT. In 2007 International Conference on
Wavelet Analysis and Pattern Recognition , volume 1, pp. 130–134. IEEE, 2007.
Simon Duane, A.D. Kennedy, Brian J. Pendleton, and Duncan Roweth. Hybrid Monte Carlo. Physics Letters
B, 195(2):216–222, 1987. ISSN 0370-2693.
Charles Dugas, Yoshua Bengio, François Bélisle, Claude Nadeau, and René Garcia. Incorporating Second-
Order Functional Knowledge for Better Option Pricing. Advances in Neural Information Processing Sys-
tems, pp. 472–478, 2001.
Emilien Dupont, Adam Golinski, Milad Alizadeh, Yee Whye Teh, and Arnaud Doucet. COIN: COmpression
with Implicit Neural representations. In Neural Compression: From Information Theory to Applications
– Workshop @ ICLR 2021 , 2021.
Emilien Dupont, Hyunjik Kim, S. M. Ali Eslami, Danilo Jimenez Rezende, and Dan Rosenbaum. From
data to functa: Your data point is a function and you can treat it like one. In Proceedings of the 39th
International Conference on Machine Learning , volume 162 of Proceedings of Machine Learning Research ,
pp. 5694–5725. PMLR, 17–23 Jul 2022a.
Emilien Dupont, Yee Whye Teh, and Arnaud Doucet. Generative Models as Distributions of Functions. In
The 25th International Conference on Artiﬁcial Intelligence and Statistics , 2022b.
14Published in Transactions on Machine Learning Research (04/2023)
Piero Esposito. BLiTZ - Bayesian Layers in Torch Zoo (a Bayesian Deep Learing library for Torch). https:
//github.com/piEsposito/blitz-bayesian-deep-learning/ , 2020.
Vincent Fortuin, Adrià Garriga-Alonso, Sebastian W. Ober, Florian Wenzel, Gunnar Ratsch, Richard E
Turner, Mark van der Wilk, and Laurence Aitchison. Bayesian Neural Network Priors Revisited. In
International Conference on Learning Representations , 2022.
Sara Fridovich-Keil, Alex Yu, Matthew Tancik, Qinhong Chen, Benjamin Recht, and Angjoo Kanazawa.
Plenoxels: Radiance Fields Without Neural Networks. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition , pp. 5501–5510, 2022.
Yarin Gal and Zoubin Ghahramani. Dropout as a Bayesian Approximation: Representing Model Uncertainty
in Deep Learning. In International Conference on Machine Learning , pp. 1050–1059. PMLR, 2016.
Kyle Genova, Forrester Cole, Daniel Vlasic, Aaron Sarna, William T. Freeman, and Thomas Funkhouser.
Learning Shape Templates with Structured Implicit Functions. In Proceedings of the IEEE International
Conference on Computer Vision , pp. 7154–7164, 2019.
Kyle Genova, Forrester Cole, Avneesh Sud, Aaron Sarna, and Thomas Funkhouser. Local Deep Implicit
Functions for 3D Shape. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pp. 4857–4866, 2020.
Peter Gilbert. Iterative Methods for the Three-Dimensional Reconstruction of an Object from Projections.
Journal of theoretical biology , 36(1):105–117, 1972.
Walter R. Gilks, Sylvia Richardson, and David Spiegelhalter. Markov Chain Monte Carlo in Practice . CRC
Press, 1995.
Xavier Glorot, Antoine Bordes, and Yoshua Bengio. Deep Sparse Rectiﬁer Neural Networks. In Proceedings
of the Fourteenth International Conference on Artiﬁcial Intelligence and Statistics , pp. 315–323. JMLR
Workshop and Conference Proceedings, 2011.
Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep Learning . MIT Press, 2016.
Richard Gordon, Robert Bender, and Gabor T. Herman. Algebraic Reconstruction Techniques (ART) for
Three-Dimensional Electron Microscopy and X-Ray Photography. Journal of Theoretical Biology , 29(3):
471–481, 1970.
AlexGraves. PracticalVariationalInferenceforNeuralNetworks. Advances in Neural Information Processing
Systems, 24, 2011.
Chuan Guo, Geoﬀ Pleiss, Yu Sun, and Kilian Q. Weinberger. On Calibration of Modern Neural Networks.
In Doina Precup and Yee Whye Teh (eds.), Proceedings of the 34th International Conference on Machine
Learning , volume 70 of Proceedings of Machine Learning Research , pp. 1321–1330. PMLR, 2017.
Boris Hanin. Universal Function Approximation by Deep Neural Nets with Bounded Width and ReLU
Activations. Mathematics , 7(10), 2019. ISSN 2227-7390.
Kazuyuki Hara, Daisuke Saito, and Hayaru Shouno. Analysis of Function of Rectiﬁed Linear Unit used in
Deep Learning. In 2015 International Joint Conference on Neural Networks (IJCNN) , pp. 1–8, 2015.
Bobby He, Balaji Lakshminarayanan, and Yee Whye Teh. Bayesian Deep Ensembles via the Neural Tangent
Kernel. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin (eds.), Advances in Neural
Information Processing Systems , volume 33, pp. 1010–1022. Curran Associates, Inc., 2020.
Sigurdur Helgason. Groups & Geometric Analysis: Radon Transforms, Invariant Diﬀerential Operators and
Spherical Functions: Volume 1 . Academic Press, 1984.
PhilippHenzler,NiloyJ.Mitra,andTobiasRitschel. LearningaNeural3dTextureSpacefrom2DExemplars.
InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pp. 8356–8364,
2020.
15Published in Transactions on Machine Learning Research (04/2023)
Matthew D Hoﬀman, Andrew Gelman, et al. The No-U-Turn sampler: adaptively setting path lengths in
Hamiltonian Monte Carlo. J. Mach. Learn. Res. , 15(1):1593–1623, 2014.
Kurt Hornik. Approximation Capabilities of Multilayer Feedforward Networks. Neural Networks , 4(2):
251–257, 1991.
Kurt Hornik, Maxwell Stinchcombe, and Halbert White. Multilayer Feedforward Networks are Universal
Approximators. Neural Networks , 2(5):359–366, 1989.
Ahmed Hosny, Chintan Parmar, John Quackenbush, Lawrence H Schwartz, and Hugo JWL Aerts. Artiﬁcial
Intelligence in Radiology. Nature Reviews Cancer , 18(8):500–510, 2018.
Jiang Hsieh. Computed Tomography: Principles, Design, Artifacts, and Recent Advances , volume 114. SPIE
Press, 2003.
J.H. Hubbell and S.M. Seltzer. Tables of X-Ray Mass Attenuation Coeﬃcients and Mass Energy-Absorption
Coeﬃcients . National Institute of Standards and Technology, Gaithersburg, MD, 2004. (version 1.4) Avail-
abile: http://physics.nist.gov/xaamdi [2021, 08, 22]. Originally published as NISTIR 5632, National
Institute of Standards and Technology, Gaithersburg, MD (1995).
PavelIzmailov, DmitriiPodoprikhin, TimurGaripov, DmitryVetrov, andAndrewGordonWilson. Averaging
WeightsLeadstoWiderOptimaandBetterGeneralization. In 34th Conference on Uncertainty in Artiﬁcial
Intelligence 2018, UAI 2018 , pp. 876–885, 2018.
Pavel Izmailov, Sharad Vikram, Matthew D Hoﬀman, and Andrew Gordon Gordon Wilson. What are
Bayesian neural network posteriors really like? In International Conference on Machine Learning , pp.
4629–4640. PMLR, 2021.
Arthur Jacot, Franck Gabriel, and Clément Hongler. Neural Tangent Kernel: Convergence and General-
ization in Neural Networks. In Proceedings of the 53rd Annual ACM SIGACT Symposium on Theory of
Computing , pp. 6–6, 2021.
Chiyu Jiang, Avneesh Sud, Ameesh Makadia, Jingwei Huang, Matthias Nießner, Thomas Funkhouser, et al.
Local Implicit Grid Representations for 3D Scenes. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition , pp. 6001–6010, 2020.
Kyong Hwan Jin, Michael T McCann, Emmanuel Froustey, and Michael Unser. Deep Convolutional Neural
Network for Inverse Problems in Imaging. IEEE Transactions on Image Processing , 26(9):4509–4522,
2017.
Alex Kendall and Yarin Gal. What Uncertainties Do We Need in Bayesian Deep Learning for Computer
Vision?Advances in Neural Information Processing Systems , 30:5574–5584, 2017.
Muhammad Osama Khan and Yi Fang. Implicit Neural Representations for Medical Imaging Segmentation.
InInternational Conference on Medical Image Computing and Computer-Assisted Intervention , pp. 433–
443. Springer, 2022.
Patrick Kidger and Terry Lyons. Universal Approximation with Deep Narrow Networks. In Conference on
Learning Theory , pp. 2306–2327. PMLR, 2020.
Diederik P Kingma and Max Welling. Auto-Encoding Variational Bayes. stat, 1050:1, 2014.
Volodymyr Kuleshov, Nathan Fenner, and Stefano Ermon. Accurate Uncertainties for Deep Learning using
Calibrated Regression. In International Conference on Machine Learning , pp. 2796–2804. PMLR, 2018.
Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. Simple and Scalable Predictive Un-
certainty Estimation using Deep Ensembles. Advances in Neural Information Processing Systems , 30,
2017.
16Published in Transactions on Machine Learning Research (04/2023)
Max-Heinrich Laves, Malte Tölle, Alexander Schlaefer, and Sandy Engelhardt. Posterior Temperature Op-
timized Bayesian Models for Inverse Problems in Medical Imaging. Medical image analysis , 78:102382,
2022.
Jaehoon Lee, Yasaman Bahri, Roman Novak, Samuel S Schoenholz, Jeﬀrey Pennington, and Jascha Sohl-
Dickstein. Deep Neural Networks as Gaussian Processes. In International Conference on Learning Repre-
sentations , 2018.
Michael M Lell and Marc Kachelrieß. Recent and Upcoming Technological Developments in Computed
Tomography: High Speed, Low Dose, Deep Learning, Multienergy. Investigative radiology , 55(1):8–19,
2020.
Zhengqi Li, Simon Niklaus, Noah Snavely, and Oliver Wang. Neural Scene Flow Fields for Space-Time
View Synthesis of Dynamic Scenes. In Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition , pp. 6498–6508, 2021.
Eugene C Lin. Radiation Risk from Medical Imaging. Mayo Clinic proceedings , 85 12:1142–6; quiz 1146,
2010.
David B. Lindell, Julien NP Martel, and Gordon Wetzstein. AutoInt: Automatic Integration for Fast
Neural Volume Rendering. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pp. 14556–14565, 2021.
Yan Liu and Yi Zhang. Low-dose CT restoration via stacked sparse denoising autoencoders. Neurocomputing ,
284:80–89, 2018.
LuShin Lu, Yanhui YeonjongSu, and George Em Karniadakis. Dying ReLU and Initialization: Theory and
Numerical Examples. Communications in Computational Physics , 28(5):1671–1706, 2020.
Zhou Lu, Hongming Pu, Feicheng Wang, Zhiqiang Hu, and Liwei Wang. The Expressive Power of Neural
Networks: A View from the Width. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus,
S. Vishwanathan, and R. Garnett (eds.), Advances in Neural Information Processing Systems , volume 30.
Curran Associates, Inc., 2017.
Ricardo Martin-Brualla, Noha Radwan, Mehdi S.M. Sajjadi, Jonathan T. Barron, Alexey Dosovitskiy, and
Daniel Duckworth. NeRF in the Wild: Neural Radiance Fields for Unconstrained Photo Collections. In
Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pp. 7210–7219,
2021.
Alexander G de G Matthews, Jiri Hron, Mark Rowland, Richard E Turner, and Zoubin Ghahramani. Gaus-
sian Process Behaviour in Wide Deep Neural Networks. In International Conference on Learning Repre-
sentations , 2018.
Cynthia H McCollough, Adam C Bartley, Rickey E Carter, Baiyu Chen, Tammy A Drees, Phillip Edwards,
David R Holmes III, Alice E Huang, Farhana Khan, Shuai Leng, et al. Low-dose CT for the detection and
classiﬁcation of metastatic liver lesions: results of the 2016 low dose CT grand challenge. Medical physics ,
44(10):e339–e352, 2017.
Lars Mescheder, Michael Oechsle, Michael Niemeyer, Sebastian Nowozin, and Andreas Geiger. Occupancy
Networks: Learning 3D Reconstruction in Function Space. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pp. 4460–4470, 2019.
L.M. Mescheder. Stability and Expressiveness of Deep Generative Models . Eberhard Karls Universität
Tübingen, 2020.
Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, and Ren
Ng. NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis. In European Conference
on Computer Vision , pp. 405–421. Springer, 2020.
17Published in Transactions on Machine Learning Research (04/2023)
Thomas Müller, Alex Evans, Christoph Schied, and Alexander Keller. Instant Neural Graphics Primitives
with a Multiresolution Hash Encoding. ACM Trans. Graph. , 41(4):102:1–102:15, July 2022.
Seth Nabarro, Stoil Ganev, Adrià Garriga-Alonso, Vincent Fortuin, Mark van der Wilk, and Laurence
Aitchison. Data augmentation in Bayesian neural networks and the cold posterior eﬀect. In Proceedings
of the Thirty-Eighth Conference on Uncertainty in Artiﬁcial Intelligence , volume 180 of Proceedings of
Machine Learning Research , pp. 1434–1444. PMLR, 01–05 Aug 2022.
RadfordMNeal. PriorsforInﬁniteNetworks. In Bayesian Learning for Neural Networks , pp.29–53.Springer,
1996.
Radford M. Neal et al. MCMC using Hamiltonian Dynamics. Handbook of Markov Chain Monte Carlo , 2
(11):2, 2011.
Behnam Neyshabur, Zhiyuan Li, Srinadh Bhojanapalli, Yann LeCun, and Nathan Srebro. Towards Un-
derstanding the Role of Over-Parametrization in Generalization of Neural Networks. In International
Conference on Learning Representations (ICLR) , 2019.
Michael Niemeyer, Lars Mescheder, Michael Oechsle, and Andreas Geiger. Diﬀerentiable Volumetric Ren-
dering: Learning Implicit 3D Representations without 3D Supervision. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition , pp. 3504–3515, 2020.
Jeremy Nixon, Michael W. Dusenberry, Linchuan Zhang, Ghassen Jerfel, and Dustin Tran. Measuring
Calibration in Deep Learning. In CVPR Workshops , volume 2(7), 2019.
Lorenzo Noci, Kevin Roth, Gregor Bachmann, Sebastian Nowozin, and Thomas Hofmann. Disentangling
the Roles of Curation, Data-Augmentation and the Prior in the Cold Posterior Eﬀect. Advances in Neural
Information Processing Systems , 34, 2021.
Michael Oechsle, Lars Mescheder, Michael Niemeyer, Thilo Strauss, and Andreas Geiger. Texture Fields:
Learning Texture Representations in Function Space. In Proceedings of the IEEE/CVF International
Conference on Computer Vision , pp. 4531–4540, 2019.
Yaniv Ovadia, Emily Fertig, Jie Ren, Zachary Nado, D Sculley, Sebastian Nowozin, Joshua Dillon, Balaji
Lakshminarayanan, and Jasper Snoek. Can you trust your model’s uncertainty? Evaluating predictive
uncertainty under dataset shift. Advances in Neural Information Processing Systems , 32:13991–14002,
2019.
Xiaochuan Pan, Emil Y. Sidky, and Michael Vannier. Why Do Commercial CT Scanners Still Employ
Traditional, Filtered Back-Projection for Image Reconstruction? Inverse Problems , 25(12):123009, 2009.
Jeong Joon Park, Peter Florence, Julian Straub, Richard Newcombe, and Steven Lovegrove. DeepSDF:
Learning Continuous Signed Distance Functions for Shape Representation. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition , pp. 165–174, 2019.
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen,
Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary
DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai,
and Soumith Chintala. PyTorch: An Imperative Style, High-Performance Deep Learning Library. In
H. Wallach, H. Larochelle, A. Beygelzimer, F. d /quotesingle.ts1Alché-Buc, E. Fox, and R. Garnett (eds.), Advances in
Neural Information Processing Systems 32 , pp. 8024–8035. Curran Associates, Inc., 2019.
Tim Pearce, Felix Leibfried, and Alexandra Brintrup. Uncertainty in Neural Networks: Approximately
Bayesian Ensembling. In Proceedings of the Twenty Third International Conference on Artiﬁcial Intelli-
gence and Statistics , volume 108 of Proceedings of Machine Learning Research , pp. 234–244, 26–28 Aug
2020.
Daniël M Pelt, Doga Gürsoy, Willem Jan Palenstijn, Jan Sijbers, Francesco De Carlo, and Kees Joost
Batenburg. Integration of TomoPy and the ASTRA Toolbox for Advanced Processing and Reconstruction
of Tomographic Synchrotron Data. Journal of Synchrotron Radiation , 23(3):842–849, 2016.
18Published in Transactions on Machine Learning Research (04/2023)
Du Phan, Neeraj Pradhan, and Martin Jankowiak. Composable Eﬀects for Flexible and Accelerated Prob-
abilistic Programming in NumPyro. arXiv preprint arXiv:1912.11554 , 2019.
Eugenio Picano. Sustainability of Medical Imaging. Bmj, 328(7439):578–580, 2004.
S.E. Pryse, L. Kersley, D.L. Rice, C.D. Russell, and I.K. Walker. Tomographic Imaging of the ionospheric
Mid-Latitude Trough. Annales Geophysicae , 11(2-3):144–149, 1993.
Maithra Raghu, Ben Poole, Jon Kleinberg, Surya Ganguli, and Jascha Sohl-Dickstein. On the Expressive
PowerofDeepNeuralNetworks. In International Conference on Machine Learning , pp.2847–2854.PMLR,
2017.
Ali Rahimi and Benjamin Recht. Random Features for Large-Scale Kernel Machines. In Proceedings of the
20th International Conference on Neural Information Processing Systems , pp. 1177–1184, 2007.
Prajit Ramachandran, Barret Zoph, and Quoc V. Le. Searching for Activation Functions. arXiv preprint
arXiv:1710.05941 , 2017.
Albert W Reed, Hyojin Kim, Rushil Anirudh, K Aditya Mohan, Kyle Champley, Jingu Kang, and Suren
Jayasuriya. Dynamic CT Reconstruction from Limited Views with Implicit Neural Representations and
ParametricMotionFields. In Proceedings of the IEEE/CVF International Conference on Computer Vision ,
pp. 2258–2268, 2021.
Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic Backpropagation and Approxi-
mateInferenceinDeepGenerativeModels. In Proceedings of the 31st International Conference on Machine
Learning , Proceedings of Machine Learning Research, pp. 1278–1286, 2014.
C.A.Roobottom, G.Mitchell, andG.Morgan-Hughes. Radiation-ReductionStrategiesinCardiacComputed
Tomographic Angiograph. Clinical Radiology , 65(11):859–867, 2010. ISSN 0009-9260.
Leonid I Rudin, Stanley Osher, and Emad Fatemi. Nonlinear Total Variation Based Noise Removal Algo-
rithms.Physica D: nonlinear phenomena , 60(1-4):259–268, 1992.
Shunsuke Saito, Zeng Huang, Ryota Natsume, Shigeo Morishima, Angjoo Kanazawa, and Hao Li. PIFu:
Pixel-Aligned Implicit Function for High-Resolution Clothed Human Digitization. In Proceedings of the
IEEE International Conference on Computer Vision , pp. 2304–2314, 2019.
LawrenceASheppandBenjaminFLogan. TheFourierReconstructionofaHeadSection. IEEE Transactions
on Nuclear Science , 21(3):21–43, 1974.
Vincent Sitzmann, Michael Zollhoefer, and Gordon Wetzstein. Scene Representation Networks: Continuous
3D-Structure-Aware Neural Scene Representations. Advances in Neural Information Processing Systems ,
32:1121–1132, 2019.
VincentSitzmann, JulienMartel, AlexanderBergman, DavidLindell, andGordonWetzstein. ImplicitNeural
Representations with Periodic Activation Functions. Advances in Neural Information Processing Systems ,
33, 2020.
Rebecca Smith-Bindman, Jaﬁ Lipson, Ralph Marcus, Kwang-Pyo Kim, Mahadevappa Mahesh, Robert
Gould, Amy Berrington De González, and Diana L. Miglioretti. Radiation Dose Associated with Common
Computed Tomography Examinations and the Associated Lifetime Attributable Risk of Cancer. Archives
of Internal Medicine , 169(22):2078–2086, 2009.
Nitish Srivastava, Geoﬀrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout:
A Simple Way to Prevent Neural Networks from Overﬁtting. Journal of Machine Learning Research , 15
(56):1929–1958, 2014.
Yu Sun, Jiaming Liu, Mingyang Xie, Brendt Wohlberg, and Ulugbek S. Kamilov. CoIL: Coordinate-Based
Internal Learning for Tomographic Imaging. IEEE Transactions on Computational Imaging , 7:1400–1412,
2021.
19Published in Transactions on Machine Learning Research (04/2023)
Matthew Tancik, Pratul Srinivasan, Ben Mildenhall, Sara Fridovich-Keil, Nithin Raghavan, Utkarsh Singhal,
Ravi Ramamoorthi, Jonathan Barron, and Ren Ng. Fourier Features Let Networks Learn High Frequency
Functions in Low Dimensional Domains. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and
H. Lin (eds.), Advances in Neural Information Processing Systems , volume 33, pp. 7537–7547, 2020.
Matthew Tancik, Ben Mildenhall, Terrance Wang, Divi Schmidt, Pratul P. Srinivasan, Jonathan T. Bar-
ron, and Ren Ng. Learned Initializations for Optimizing Coordinate-Based Neural Representations. In
Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pp. 2846–2855,
2021.
Matthew Tancik, Vincent Casser, Xinchen Yan, Sabeek Pradhan, Ben Mildenhall, Pratul P Srinivasan,
Jonathan T Barron, and Henrik Kretzschmar. Block-NeRF: Scalable Large Scene Neural View Synthesis.
InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pp. 8248–8258,
2022.
Florian Wenzel, Kevin Roth, Bastiaan Veeling, Jakub Swiatkowski, Linh Tran, Stephan Mandt, Jasper
Snoek, Tim Salimans, Rodolphe Jenatton, and Sebastian Nowozin. How Good is the Bayes Posterior
in Deep Neural Networks Really? In International Conference on Machine Learning , pp. 10248–10259.
PMLR, 2020.
Andrew Gordon Wilson and Pavel Izmailov. Bayesian Deep Learning and a Probabilistic Perspective of
Generalization. Advances in Neural Information Processing Systems , 2020-December, 2020. ISSN 1049-
5258.
Dufan Wu, Kyungsang Kim, and Quanzheng Li. Computationally Eﬃcient Deep Neural Network for Com-
puted Tomography Image Reconstruction. Medical physics , 46(11):4763–4776, 2019.
Omry Yadan. Hydra - A Framework for Elegantly Conﬁguring Complex Applications. Github, 2019. URL
https://github.com/facebookresearch/hydra .
Qingsong Yang, Pingkun Yan, Yanbo Zhang, Hengyong Yu, Yongyi Shi, Xuanqin Mou, Mannudeep K Kalra,
Yi Zhang, Ling Sun, and Ge Wang. Low-dose CT image denoising using a generative adversarial network
with Wasserstein distance and perceptual loss. IEEE transactions on medical imaging , 37(6):1348–1357,
2018.
Koichiro Yasaka and Osamu Abe. Deep Learning and Artiﬁcial Intelligence in Radiology: Current Applica-
tions and Future Directions. PLoS medicine , 15(11):e1002707, 2018.
Jin-Yun Yuan and Alfredo Noel Iusem. Preconditioned Conjugate Gradient Method for Generalized Least
Squares Problems. Journal of Computational and Applied Mathematics , 71(2):287–297, 1996.
Sheheryar Zaidi, Arber Zela, Thomas Elsken, Chris C Holmes, Frank Hutter, and Yee Teh. Neural Ensemble
Search for Uncertainty Estimation and Dataset Shift. Advances in Neural Information Processing Systems ,
34:7898–7911, 2021.
Guangming Zang, Ramzi Idoughi, Rui Li, Peter Wonka, and Wolfgang Heidrich. IntraTomo: Self-supervised
Learning-based Tomography via Sinogram Synthesis and Prediction. In 2021 IEEE/CVF International
Conference on Computer Vision (ICCV) , pp. 1940–1950, 2021.
Ling Zhang, Xiaosong Wang, Dong Yang, Thomas Sanford, Stephanie Harmon, Baris Turkbey, Bradford J.
Wood, Holger Roth, Andriy Myronenko, Daguang Xu, and Ziyue Xu. Generalizing deep learning for
medical image segmentation to unseen domains via deep stacked transformation. IEEE Transactions on
Medical Imaging , 39(7):2531–2540, 2020.
20Published in Transactions on Machine Learning Research (04/2023)
8 Appendices
A Computed Tomography
Diagnostic X-rays constitute the largest man-made source of radiation exposure to the general popula-
tion (de Gonzalez & Darby, 2004; Picano, 2004). In 2010, 5 billion medical imaging studies were performed
worldwide, two-thirds of which employed ionizing radiation (Roobottom et al., 2010), and the use of radiol-
ogy has only grown since (Lin, 2010). Computed tomography (CT), also known as computed axial/assisted
tomography (CAT), is a noninvasive medical imaging technique frequently used in radiology to generate
detailed images of the body and comprises the majority of this radiation exposure (Brenner & Hall, 2007),
with an estimated 29,000 current or future cancer cases linked to CT scans performed in the United States
of America in 2007 alone (De González et al., 2009). Since its original development in the 1970s, CT has
become widespread in medical imaging – with over 70 million CT scans taken and reported annually in the
United States, since 2007 (Smith-Bindman et al., 2009). There are multiple types of CT scanners, such as
spiral CT, electron beam CT, and CT perfusion imaging. In this work, we focus on spiral, also know as
spinning tube or helical, CT.
In spiral CT, illustrated in Fig. 4, the patient lies along the central axis of a cylindrical measurement tube.
As the scan is performed, an X-ray generator rotates around the patient while moving along the axis of
measurement. X-rays are emitted, which pass through the patient and are attenuated at various rates
by the diﬀerent types of tissues in the body, as described in Appendix A.1. After exiting the body, the
attenuated X-rays are measured by X-ray detectors positioned and moving opposite to the X-ray source.
These measurements are used to create a sinogram, as described in Section A.2, which is not understandable
by doctors. This sinogram is then input to a reconstruction algorithm, which solves an under-determined
inverse problem, described in Section A.3, to generate a human-understandable 2D or 3D image of the organ
of interest. This image can then be used by the doctor for medical diagnosis.
A.1 X-Ray Attenuation
X-rays produced by CT scanners can interact with matter via the photoelectric eﬀect, the Compton eﬀect,
and coherent scattering. Through these interactions, some of the emitted X-ray photons are absorbed or
scattered when passing through diﬀerent tissues in the body. The attenuation is described by the Beer-
Figure 4: Illustration of a CT scan-
ning device. Reproduced with permission
from (Brenner & Hall, 2007), Copyright
Massachusetts Medical Society.
0 20 40 60 80 100 120 140
Energy (keV)10−1100101102103104f(cm−1)Adipose Tissue
Bone Cortical
Eye Lens
Mercuric Iodine
Soft TissueFigure 5: The attenuation coeﬃcient ( f) of diﬀerent tis-
sues found in the body, as a function of energy (keV). The
plot was generated using X-ray mass attenuation coeﬃcients
(f/ρ) and densities ( ρ) from the NIST Standard Reference
Database (Hubbell & Seltzer, 2004; Berger et al., 2004).
21Published in Transactions on Machine Learning Research (04/2023)
Lambert Law,
J=J0e−fL, (14)
whereJandJ0are the incident and transmitted X-ray intensities; Lis the material thickness; and fis the
linear attenuation coeﬃcient of the material,
f=τ1+τ2+τ3, (15)
with photoelectric ( τ1), Compton ( τ2), and coherent scattering ( τ3) attenuation coeﬃcients. Attenuation
coeﬃcients for common materials in the body – iodine, bone, water, and soft-tissue – are plotted, in Fig. 5,
over the range of incident X-ray energies used in CT imaging. It is clear that the attenuation of X-ray
photons can be used to distinguish and, thus, image various tissues in the body (Hsieh, 2003).
A.2 CT Projection Measurements
As previously described, the CT scan relies on an X-ray generator which rotates around the patient, emitting
X-ray photons. In this work, we only consider a restricted case of the spiral CT setup, in which there is
no motion along the patient axis. Instead, we focus on reconstructing singular 2D image cross-sections and
assume a parallel-beam geometry, in which photons are emitted and detected with the linear geometry of
Figure 1b.
We begin by considering the measurements of a single detector, measuring at angle φ. Assume that the
X-ray generator outputs monoenergetic X-rays of intensity J0. If the patient were simply a homogeneous
block of tissue, with length ∆/lscriptand attenuation coeﬃcient f∗, we could directly apply the Beer-Lambert law
(Eq. 14),
J=J0e−f∗∆/lscript, (16)
to solve for the output attenuated X-ray intensity J. In reality, several blocks of tissue will be present in
the patient, each with its own attenuation coeﬃcient. However, since the exit X-ray ﬂux from one block of
tissue is the entrance X-ray ﬂux to its neighboring block, we can simply apply the Beer-Lambert law in a
cascading fashion over intervals of length ∆/lscriptand attenuation coeﬃcients (f∗
1,f∗
2,...,f∗
n),
J=J0e−f∗
1∆/lscripte−f∗
2∆/lscript...e−f∗
n∆/lscript=J0e−/summationtextn
i=1f∗
i∆/lscript. (17)
As∆/lscript→0, the summation term becomes an integration over the length, L, of the patient,
J=J0e−/integraltext
Lf∗(/lscript)d/lscript. (18)
Finally, dividing both sides of the expression by J0and taking a negative logarithm, we deﬁne the projection
measurement term,
Sφ=−ln/parenleftbiggJ
J0/parenrightbigg
=/integraldisplay
Lf∗(/lscript)d/lscript. (19)
As illustrated in Figure 1b, a CT scanner with a parallel beam geometry contains several detectors side-by-
side, collectively measuring a plane of attenuated X-ray photons. In this 2D imaging space, the projection
measurement becomes a function of detector position, r. Thus, Eq. 19 is re-expressed as the line integral,
Sφ(r) =−ln/parenleftbiggJ
J0/parenrightbigg
=/integraldisplay
f∗(φ,r)ds, (20)
known as a forward-projection (FP). It follows from the coordinate system of Figure 1b that, for measure-
ments at angle φ, point (x,y)within the patient cross-section is projected onto detector position
r/prime=xcosφ+ysinφ. (21)
Combining this with Eq. 20, we derive the Radon transform (Deans, 2007) of the patient cross-section,
Sφ(r) =−ln/parenleftbiggJ
J0/parenrightbigg
(22)
=/integraldisplay
Y/integraldisplay
Xf(x,y)δ(xcosφ+ysinφ−r)dxdy,
22Published in Transactions on Machine Learning Research (04/2023)
whereδis the Dirac delta function and X×Yis the set of image pixels (x,y). Sweeping over all the
angles, these projective measurements are stacked to form a Radon transform image. As depicted in Fig-
ure 1b, the projection measurements of the blue point across several angles produces a sinusoidal curve. The
representation of all the CT scan measurements is thus known as a sinogram.
A.3 CT Image Reconstruction Problem
Sinograms are not human-interpretable. They depict the integrated attenuation coeﬃcients, or projection
measurements ( Sφ(r)), of the patient cross-section from several angles ( φ) over all detector positions ( r). In-
stead, the desired outcome of a CT scan is a reconstructed image of the cross-section itself. This corresponds
to the attenuation coeﬃcient function, f(x,y), which is the inverse of the Radon transform of Eq. 22, or
f(x,y) =1
2π/integraldisplayπ
0uφ(xcosφ+ysinφ)dφ, (23)
whereuφis the derivative of the Hilbert transform of Sφ(r)(Helgason, 1984). The Projection-Slice the-
orem (Bracewell, 1956) ensures that f∗can be fully reconstructed with inﬁnite measurement angles, φ.
In practice, however, it is not possible to acquire inﬁnite measurements. Typically, reconstruction quality
improves with number of measurements, but this increases radiation exposure. In practice, hundreds of
measurements are performed in a CT scan, but there is interest in reducing this number. In this work, we
study algorithm performance in the very low measurement data regime, where uncertainty quantiﬁcation
over the value of f(x,y)becomes especially important. The combination of limited and noisy real-world
data renders the reconstruction of the desired image much more complex than simply evaluating the integral
of Eq. 23. Leveraging assumptions or data-driven insights about the measurements and physics at play,
several statistical models have been developed and used to derive various image reconstruction algorithms,
as discussed in the next section.
23Published in Transactions on Machine Learning Research (04/2023)
B Classical Reconstruction Algorithms
In this appendix, we provide brief descriptions of the classical approaches implemented in this work via the
TomoPy Astra (Pelt et al., 2016) software package. These algorithms are clinically approved and widely
used in medical imaging, serving as a basis of comparison for the methods developed in this work. Note that
although our approach used NNs, it was not a data-driven approach, but rather learned image functions
from small amounts of data. Thus, we do not discuss data-driven techniques. Further, note that we did not
implement the FISTA-TV algorithm, which was also used as a classical baseline, but cited results from the
CoIL work (Sun et al., 2021), where a description of the algorithm can be found.
B.1 Filtered Back-Projection (FBP)
Filtered back-projection (FBP) (Pan et al., 2009) is an analytic algorithm which calculates a stable, dis-
cretized version of the inverse Radon transform. As the name implies, there are two key steps: ﬁltering and
back-projection.
The forward-projection of Eq. 20 describes how X-rays passing through the object domain create a measure-
ment. In back-projection (BP), this measurement is integrated back along the X-ray path across the object
domain. This is done over all projection angles φ, using
fBP(x,y) =/integraldisplay
Sφ(xcosφ+ysinφ)dφ (24)
to reconstruct the object attenuation coeﬃcient image. As the number of projection angles increases, the
image reconstruction improves. However, as shown in Figure 6, this back-projection is insuﬃcient to guaran-
tee a clear image. While information about the low frequencies of the object are captured in measurements
at several view angles, that of high frequencies may only be captured in a few view-angles. Thus, the low
frequencies are sampled far more densely than the higher frequencies, resulting in a blurry image. This can
be corrected by suppressing the lower frequencies with ﬁltering, by applying to each projective measurement,
Sφ(r), the sequence of a Fourier transform (FFT), high-pass ﬁlter, and an inverse Fourier transform (iFFT).
While several high-pass ﬁlters can be used, a popular choice is the Ram-Lak ﬁlter, which generates the
ﬁltered projective measurement
˜Sφ(r) =/integraldisplay
F[Sφ](ω)|ω|ei2πωrdω, (25)
whereF[Sφ](ω)is the Fourier transform of Sφ(r)and|ω|the frequency response of the ﬁlter. Performing
back-projections of all the ﬁltered projective measurements,
fFBP(x,y) =/integraldisplay
˜Sφ(xcosφ+ysinφ)dφ, (26)
results in a sharper object attenuation coeﬃcient image. Figure 6 visualizes the diﬀerence in reconstruc-
tion performance of BP and FBP for increasing measurement view angles, φ. Although the analytic FBP
algorithm is fast and numerically stable, it suﬀers from poor resolution-noise trade-oﬀ.
B.2 Algebraic and Iterative Reconstruction
The reconstruction problem can be formulated as a system of linear equations
W/vectorf=/vectorS, (27)
where/vectorSis anm×1vector of the mprojective measurement values in the sinogram; /vectorfis ann×1vector of the
nattenuation coeﬃcient pixel values in the reconstruction image; and Wis a, typically sparse, m×nweight
matrix representing the contribution of each of the msinogram values to each of the nimage pixel values.
Given the vector /vectorS, the goal is to solve for /vectorf. IfWwere invertible, /vectorfwould simply be W−1/vectorS. However,
becausenis usually much larger than m, the system of equations of Eq. 27 is underconstrained. In algebraic
24Published in Transactions on Machine Learning Research (04/2023)
BP1 view
 2 views
 8 views
 16 views
 64 views
 128 views
FBP
0.00.20.40.60.81.0
Figure 6: Comparison of the reconstruction quality, as a function of the number of views, of the BP (top)
and FBP (bottom) algorithms.
reconstruction, this problem is addressed by using iterative algorithms that pose the reconstruction of /vectorfas
the solution of a constrained optimization problem,
/vectorf∗= arg min
/vectorf||/vectorS−W/vectorf||,subject tofi≥0∀i. (28)
Several families of iterative solvers can be used to solve this optimization, such as Landweber, Krylov
subspaces, and expectation maximization (EM). The key beneﬁt of iterative methods is that prior system
knowledge can be integrated, via the cost function and initialization of W. Their down-side is that they are
not necessarily stable, may not converge, and are much slower than analytic techniques, such as FBP.
B.3 Simultaneous Iterative Reconstruction Technique (SIRT)
The simultaneous iterative reconstruction technique (SIRT) (Pryse et al., 1993; Bust & Mitchell, 2008) is a
Landweber iterative method that updates the image reconstruction using all available sinogram projection
data,/vectorS, simultaneously. The optimization update at step kis deﬁned as
/vectorf(k+1)=/vectorf(k)+BWTD/parenleftbig/vectorS−W/vectorf(k)/parenrightbig
, (29)
whereD∈Dm×mis a diagonal matrix containing the inverse row sums, dii= (/summationtextn−1
j=0wij)−1, andB∈Rn×n
is a diagonal matrix containing the inverse column sums, bii= (/summationtextm−1
i=0wij)−1. The weighted projection
diﬀerence, D/parenleftbig/vectorS−W/vectorf(k)/parenrightbig
, corresponds to the inverse of the length each X-rays passes through the volume.
Shorterrayshaveahighercontribution,withtheweightingrequiredtoguaranteeconvergence. Thisdiﬀerence
is forward-passed back to the image domain, using the weighted back-projection term, BWT, where it can
be used to update the reconstruction. These updates iteratively solve the problem
/vectorf∗= arg min
/vectorf||/vectorS−W/vectorf||D= arg min
/vectorf/parenleftbig/vectorS−W/vectorf/parenrightbigTD/parenleftbig/vectorS−W/vectorf/parenrightbig
, (30)
converging to a weighted least-squares solution, with weights given by the inverse row sums of W.
B.4 Simultaneous Algebraic Reconstruction Technique (SART)
The algebraic reconstruction technique (ART) (Gordon et al., 1970) was one of the ﬁrst proposed algebraic
iterative algorithms for CT image reconstruction. It is a Landweber technique almost identical to the SIRT
algorithm. However, a single projective measurement is used to update the reconstruction image per update
step. Generally, the ART algorithm reaches a solution much faster than SIRT, but does not have stable
convergence if the system of equations is inconsistent, for example due to measurement noise.
The simultaneous algebraic reconstruction technique (SART) (Andersen & Kak, 1984) was proposed in 1984,
as an improvement to ART, and is also a Landweber algebraic iterative algorithm. It combines the reduced
25Published in Transactions on Machine Learning Research (04/2023)
runtimes of ART with the improved convergence of SIRT, by using all the projective measurements from a
single view angle in each optimization iteration. The update of image vector index iat stepnis deﬁned as
f(n+1)
i =f(n)
i+λn/summationtextn−1
j=0wijφnL+L/summationdisplay
j=φnL+1wijSj−ˆSj/summationtextm−1
g=0wgj, (31)
whereλn<<1is a, potentially dynamic, relaxation parameter; φnis the (nmodN)thmeasurement angle
of the sinogram, assuming Ntotal measurement angles; and Lis the number of projective measurements
taken at each angle. SART typically converges to a good reconstruction within a few iterations.
B.5 Conjugate Gradient Least Squares (CGLS)
The conjugate gradient least squares (CGLS) (Yuan & Iusem, 1996) algorithm is a Krylov subspace iter-
ative method. Since it requires a positive-deﬁnite system matrix, the CT image reconstruction problem is
reformulated in terms of the set of normal equations
WTW/vectorf=WT/vectorS. (32)
Due to the positive-deﬁniteness of WTW, there exists a set of conjugate normal vectors Q={/vector q1,...,/vector qn},
where/vector qT
iWTW/vector qj= 0,∀i/negationslash=j∈(1,n). SinceQforms a basis for Rn, the image vector /vectorfcan be rexpressed
as a linear combination of these conjugate normal vectors,
/vectorf=n/summationdisplay
i=1αi/vector qi. (33)
Thus, solving for /vectorfbecomes a problem of solving for the conjugate normal basis vector directions, /vector qi, and
their corresponding weights, αi. This can be achieved iteratively by expressing the problem as a quadratic
least-squares minimization of the function
L(/vectorf) =1
2/vectorfTWTW/vectorf−/vectorfTWT/vectorS, (34)
which has gradient ∇L(/vectorf) =WTW/vectorf−WT/vectorSand a guaranteed unique minimizer because the Hessian
∇2L(/vectorf) =WTWis symmetric positive-deﬁnite. The name conjugate gradient least squares comes from the
fact that, in each iteration, a conjugate basis vector and its weight are found by taking a gradient step in
the direction that minimizes the least-squares function, L(/vectorf), as
/vectorf(k+1)=/vectorf(k)+αk/vector qk (35)
/vector ek=WT/vectorS−WTW/vectorf(k)(36)
/vector qk=/vector ek−/summationdisplay
i<k/vector qT
iWTW/vector ek
/vector qT
iWTW/vector qi/vector qi (37)
αk=/vector qT
k/vector ek
/vector qT
kWTW/vector qk(38)
where/vector ekis the residual at step k. Thus, the main diﬀerence between SIRT/SART and CGLS is that the
search direction in SIRT/SART is determined only by the projection diﬀerence at that point, while in CGLS
the search directions of all the previous iterations are also taken into account. CGLS typically converges
much faster than SIRT, but has a large memory footprint.
B.6 Expectation Maximization (EM)
The ﬁnal classical approach to CT image reconstruction that we consider is a statistical iterative method
known as expectation maximization (EM) (Dong, 2007). This technique explicitly encodes prior knowledge
26Published in Transactions on Machine Learning Research (04/2023)
about the X-ray physics at hand. Each projective measurement, Sjis modeled as a Poisson distribution,
Sj∼Sj=Poisson (λj) =λSj
je−λj
pj!, (39)
where the distribution mean λj=E[Sj]is the function
λj=/summationdisplay
iwijfi (40)
of the probability wijthat an X-ray photon penetrating image pixel iwas measured at detector location j;
and the underlying attenuation coeﬃcient function fto reconstruct.
The measurement sinogram is modeled as the likelihood
p(/vectorS|/vectorf) =/productdisplay
jλSj
je−λj
Sj!=/productdisplay
j(/summationtext
iwijfi)Sje−(/summationtext
iwijfi)
Sj!. (41)
The EM algorithm computes the maximum likelihood estimate of f,
ˆfMLE= argmax
f/braceleftbigg
log/parenleftbig
p(S|f)/parenrightbig/bracerightbigg
, (42)
by alternating between expectation and maximization steps. These can be combined into the update-step
ˆf(k+1)
i =ˆf(k)
i/summationtext
jwij/summationdisplay
jwijSj/summationtext
iwijˆf(k)
i. (43)
The EM algorithm is computationally intensive, but guaranteed to converge to a local optimum of the
likelihood. Further, although a Poisson distribution was assumed for Sjin this discussion, further knowledge
of the detector noise can be easily incorporated into the model.
B.7 Performance of Classical Reconstruction Techniques on Shepp-Logan
Finally, we evaluate the performance of these classical reconstruction algorithms on the Shepp-Logan vali-
dation set, depicted in Figure 12. Figure 7 shows average PSNR (for the 5 validation images) as a function
of view angle, for each of the 5 reconstruction methods. Also shown are the values below which PSNR
is usually considered unacceptable (20dB), at which reconstruction is considered lossy (30dB), and above
which has high quality (40dB). FBP has the worst performance, which is particularly poor in the low-view
regime (<30views). The iterative reconstruction algorithms perform better. CGLS has slow convergence
to high quality image reconstruction. EM, SIRT, and SART all converge with far fewer views, achieving
lossy compression with only ∼20 views. EM levels out and requires around 180 views to achieve high-quality
reconstruction. SIRT and SART have near identical performance, passing the high-quality reconstruction
threshold with only ∼75 views and SIRT performing slightly better for larger view numbers.
Table 3: Classical reconstruction PSNR, averaged across the ﬁve validation set images, in the 5-, 20-, and
180-view cases. The best achieved PSNR for each view-# is bolded.
# Views FBP CGLS EM SART SIRT
5 7.68 16.38 21.39 21.12 21.12
20 17.35 21.85 30.22 31.98 31.97
180 36.74 38.6 40.46 42.51 42.76
Table 3 reports the PSNR values obtained for 5, 20, and 180 views. EM is the best performer for 5views,
SART for 20, and SIRT for 180. However, while the performance of the three algorithms is similar for 5,
27Published in Transactions on Machine Learning Research (04/2023)
0 25 50 75 100 125 150 175 200
# Views510152025303540PSNR
SIRT
SART
EM
CGLS
FBP
Figure 7: PSNR as a function of number of views for the classical reconstruction algorithms.
SARTandSIRTperformbetterthanEMforlargernumberofviews. Inthelow-viewregime, roughlyanorder
of magnitude is required to achieve a 10dB improvement in average PSNR. Figure 8 shows a reconstructed
image for each of these algorithm-view combinations. With 5 views the reconstruction algorithms are able
to capture low-frequency object structure, but the image would not be useful for medical diagnosis. With 20
views it is clear that the algorithms are already capturing high-frequency components of the object, but the
images have many artifacts. By 180 views, the reconstructed images are nearly identical to the ground truth
images of Figure 12, with any discrepancies in PSNR due mostly to minor image reconstruction artifacts or
imprecisions.
5 viewsFBP
 CGLS
 EM
 SART
 SIRT
20 views
 180 views
0.00.20.40.60.81.0
Figure 8: Reconstructions, generated with varying view angles, by the 5 classical reconstruction algorithms
on the Shepp-Logan validation set.
28Published in Transactions on Machine Learning Research (04/2023)
C Metrics and Uncertainty
C.1 PSNR & SNR
PSNR is deﬁned as
PSNR/parenleftbig
f∗,f/parenrightbig
= 10 log10/parenleftbiggmax(f∗)2
MSE (f∗,f)/parenrightbigg
, (44)
where MSE/parenleftbig
f∗,f/parenrightbig
=1
|X×Y|/summationdisplay
x,y/parenleftbig
f∗(x,y)−f(x,y)/parenrightbig2
while SNR is deﬁned as
SNR/parenleftbig
f∗,f/parenrightbig
= 20 log10/parenleftbigg||f∗||2
||f∗−f||2/parenrightbigg
, (45)
wheref∗denotes the ground truth image, fthe noisy/reconstructed image, and ||·|| 2the/lscript2-norm. SNR
is strictly less than PSNR, with higher SNR and PSNR corresponding to better image reconstruction. In
the absence of any noise, f∗andfare identical,making SNR and PSNR inﬁnite. For lossy images, PSNR is
typically between 30-50dB, with values over 40dB considered very good, and values below 20dB considered
unacceptable (Bull, 2014).
C.2 Types of Uncertainty
Bayesian modeling can address two distinct types of uncertainty: aleatoric and epistemic (Der Kiureghian
& Ditlevsen, 2009; Kendall & Gal, 2017). Aleatoric uncertainty is due to measurement noise , such as X-ray
detector noise. This type of uncertainty cannot be reduced, even if more measurements are taken, since it
is inherent to the measurement. To see why, think of rolling an unbiased die. Irrespective of how many
times you roll the die, you will always be uncertain of the outcome of the next roll, since each outcome has
a1
6thprobability. On the other hand, epistemic or model uncertainty accounts for uncertainty in the model
parameters . This type of uncertainty can be reduced with more measurement data. To see why, imagine
a model that aims to predict the outcome of a biased die roll, with no prior information about the bias.
As more data is taken, the variance in the model parameters decreases, and the model output distribution
better approximates the true biased die distribution. Even in the presence of aleatoric uncertainty, this work
primarily focuses on quantifying epistemic/model uncertainty. Namely, we consider how well the model
reconstructs the ground truth attenuation coeﬃcient image from the sinogram data.
C.3 Calibration and Coverage
Calibration is a metric that assesses a model’s ability to predict the probabilities of its outcomes, gauging
the reliability of the model’s conﬁdence in its predictions. For example, a model performing class predictions
is considered calibrated if it assigns a class 50% probability and that class actually appears 50% of the time
in prediction. For further information on calibration of class prediction models, we refer the reader to (Guo
et al., 2017; Nixon et al., 2019). Since this work focuses on regression models, the remaining discussion is
centered on calibrated regression (Kuleshov et al., 2018).
LetFbe the cumulative distribution function (CDF) of model predictions f(x,y), that seek to approximate
ground truth image f∗∈F, whereFdenotes the functional space of possible images. Letting fx:=f(x,y)
We denote the corresponding quantile function as
F−1
x(˜p) = inf{fx: ˜p≤F(fx)}, (46)
whereF−1performs the mapping F−1: [0,1]→Fand˜pis a conﬁdence interval. For calibrated regression,
ground truth pixel f(x,y)should fall in a, say, 90%conﬁdence interval 90%of the time. Thus, the regression
model is calibrated for conﬁdence interval ˜pif
lim
X→∞1
XT/summationdisplay
x=1I/braceleftbig
fx≤F−1
x(˜p)/bracerightbig
= ˜p ,∀˜p∈[0,1], (47)
29Published in Transactions on Machine Learning Research (04/2023)
Ground Truth
 Mean
 Variance
 MSE
 Coverage
 NLL
0.00.20.40.60.81.0
0.000.020.040.060.080.10
−20246810
Figure 9: The ground truth image is plotted alongside diﬀerent metrics for assessing the reconstructed
predicted distribution generated by an UINR model. From left to right, we show the ground truth, predicted
mean image, predicted variance image, mean squared error, coverage quantile of each pixel, and negative
log-likelihood of each pixel. Note that PSNR is calculated using the ground truth and predicted mean image.
In a real medical setting, the ground truth is unknown, the doctor would be given the predicted mean image
and the predicted variance image could be provided as supplementary information to help the doctor reach
a diagnosis. Further note that white regions in the coverage image denote that the ground truth pixel value
did not fall in the range of the predicted distribution.
as the number of pixel samples approaches inﬁnity, X→∞. Iff∗
Xdenotes the ground truth value for i.i.d.
random pixel (x,y)∈X, a suﬃcient condition for calibrated regression is
p/parenleftbig
f∗
X≤F−1
X(˜p)/parenrightbig
= ˜p,∀˜p∈[0,1]. (48)
Since practical dataset sizes are ﬁnite, preventing perfect calibration, diﬀerent metrics have been developed
to assess empirical model calibration.
Reliability diagrams serve as a visual representation of model calibration, plotting expected sample accuracy
as a function of average model conﬁdence. Ideally these would be continuous plots, but, in practice, samples
are binned into Mbins according to their prediction conﬁdence. Let Bmbe the set of indices, i, of samples
with prediction conﬁdence, ˆpiin the interval Im= (m−1
M,m
M]. The expected accuracy (acc) and conﬁdence
(conf) are approximations to the terms of Eq. 48, namely
p/parenleftbig
f∗
X≤F−1
X(˜p)/parenrightbig
≈acc(Bm) =1
|Bm|/summationdisplay
i∈BmI/braceleftbig
f∗
i≤F−1
i(˜p)/bracerightbig
(49)
˜p≈conf(Bm) =1
|Bm|/summationdisplay
i∈Bmˆpi. (50)
The calibration error (CE) is the discrepancy
CE(˜p) =|p/parenleftbig
f∗
X≤F−1
X(˜p)/parenrightbig
−˜p|≈|acc(Bm)−conf(Bm)|=CE(Bm). (51)
It can be measured on a reliability diagram as the diﬀerence between the expected accuracy curve and the
ideal acc (Bm) =conf(Bm)line. The expected calibration error (ECE) quantiﬁes the calibration error of the
full distribution as
ECE(f∗,F−1
X,˜p) =1
MM/summationdisplay
m=1|acc(Bm)−conf(Bm)|. (52)
The model is considered calibrated if ECE (x,f) = 0.
In practice, modiﬁcations were made to the previously described theory of reliability curves and ECE. You
may notice in Figure 10 that, instead of plotting accuracy andconﬁdence , we instead plot analogous target
coverage andachieved coverage . Typically, a coverage value, ¯p, refers to a quantile of data points lying within
±¯q
2%of the median ( 50%quantile). Speciﬁcally, in our setup, we use diﬀerent uncertainty quantiﬁcation
methods (BBB, MCD, and DE) to sample Ndiﬀerent model weights for our INR, each set of weights
corresponding to a diﬀerent model output. Given that each output corresponds to an image, for each pixel,
(x,y), we have a distribution of Npredicted values, FN(x,y). Ideally, the median of the pixel distribution
would be equivalent to the ground truth pixel value, f∗(x,y). However, this is unrealistic to expect in
30Published in Transactions on Machine Learning Research (04/2023)
0.0 0.2 0.4 0.6 0.8 1.0
Target Coverage0.00.20.40.60.81.0Achieved Coverage
Reliability Curve
Withδ
Withoutδ
10−1410−1210−1010−810−610−410−2
δ0.050.100.150.200.250.300.350.40ECE
δSelection Curve
Figure 10: Both plots were made with an MCD UINR model trained on 20 views, achieving a PSNR of 19.
Left)A plot of the model reliability curves, with the grey dashed line indicating a perfectly calibrated model.
The blue curve is the empirical reliability curve of the model when a small δterm is added symmetrically
to the target coverage, in order to slightly widen the quantile ranges. Although this δterm has nearly
negligible magnitude, it signiﬁcantly improves the model reliability curve, as illustrated by the orange curve
of reliability without the added δterm. Right)The added δterm was not chosen arbitrarily, but selected
to minimize ECE.
practice. Thus, we check whether the ground truth pixel lies within the predicted pixel distribution quantile,
Qn, speciﬁed by coverage value, ¯p,
Q50−¯p
2/parenleftbig
FN(x,y)/parenrightbig
≤f∗(x,y)≤Q50+¯p
2/parenleftbig
FN(x,y)/parenrightbig
. (53)
If a model is perfectly calibrated, ¯p%of reconstructed pixels distributions will contain the ground truth pixel
in their ¯p%-th quantile, corresponding to the grey dashed line in Figure 10. Thus, model conﬁdence can be
seen as a pre-selected quantile for each pixel (target coverage), p, while accuracy is the percentage of pixel
distributions containing the ground truth that quantile (achieved coverage),
AC(f∗,FN,¯q) =1
|X|1
|Y|/summationdisplay
x∈X/summationdisplay
y∈YI/braceleftbigg
Q50−¯q
2/parenleftbig
FN(x,y)/parenrightbig
≤f∗(x,y)≤Q50+¯q
2/parenleftbig
FN(x,y)/parenrightbig/bracerightbigg
.(54)
The ECE is thus implemented as,
ECE(f∗,FN) =1
|P|/summationdisplay
¯p∈P|AC(f∗,FN,¯p)−¯p|, (55)
wherePis a ﬁnite set of percentages evenly spaced in [0,1], separated by percentage interval i<< 1. The
ﬁfth image of Figure 9, plots the smallest quantile of each pixel containing the corresponding ground truth
pixel, for an example UINR reconstruction with N= 50. Note that white regions indicate that the ground
truth value does not fall within the minimum and maximum predicted pixel values.
There is one ﬁnal modiﬁcation made in implementing the reliability curves, in order to eﬀectively assess
model calibration. Since the ﬁnal layer of all the NNs used for the INR have a sigmoid activation, ensuring
that the model output is in the range (0,1). However, the sigmoid function only approaches 0and1in the
inﬁnite limit, meaning that in practice our model will never output 0or1exactly. However, our images
contain a large percentage of pixels with exactly 0value, especially for noiseless artiﬁcial data, which in the
context of medical imaging is regions containing air and no tissue. This is problematic for calibration, since
all of predicted pixel values will be near-zero, but will not actually contain the ground truth value of 0. This
is illustrated by the orange reliability curve in Figure 10, for which only 40%of pixels contain the ground
31Published in Transactions on Machine Learning Research (04/2023)
truth in their full range of predicted pixel values, for an MCD model trained on 20 views with N= 50. Our
proposed solution to this issue is slightly widening the quantile range by adding a negligible δterm. Thus,
for coverage value ¯p, we now check if the ground truth pixel lies in,
Q50−¯q
2/parenleftbig
FN(x,y)/parenrightbig
−δ≤f∗(x,y)≤Q50+¯q
2/parenleftbig
FN(x,y)/parenrightbig
+δ, (56)
where 0< δ << 1. In this case, if our predicted pixel values are slightly larger than 0, theδoﬀset can
widen the quantile range to include 0, enabling these pixels to contribute to the achieved calibration (this
also applies to pixels with exact value of 1). The improvement in using a delta oﬀset is illustrated by the
blue reliability curve in Figure 10, which is much closer to the ideal grey dashed line than the orange curve
withδ. It should be noted that the value of δis not assigned arbitrarily, but instead optimized to minimize
overall ECE. For too small a δ, the quantiles will not be widened suﬃciently to capture ground truth 0
pixels. However, for too large of δ, the quantiles will be widened too much, reducing overall calibration, as
achieved coverage is much higher than target coverage for low coverage values. Thus, ECE as a function of
δis expected to have a unique minima, as illustrated by the example in Figure 10.
C.4 Assessing Model Quality
Section 4.1 describes how PSNR and SNR quantify image reconstruction quality, coverage metrics (such as
ECE) gauge the uncertainty calibration, and NLL encapsulates both. In this work, we aim to optimize both
reconstruction and calibration quality, meaning the best metric would, naively, be NLL. However, there is
often a trade-oﬀ between calibration and prediction quality. Specﬁcally, NN overﬁtting to NLL manifests in
probabilistic error rather than prediction error (Guo et al., 2017). Furthermore, while this work focuses on
uncertainty quantiﬁcation of INRs for medical imaging, little prior work has addressed this problem. Most
existing techniques only quantify reconstruction PSNR and SNR. Hence, for fair comparison, we assess and
optimize our models primarily according to PSNR and SNR. However, for similarly performing models, we
use NLL and ECE as secondary selectors for the best model. Note that initial attempts at optimizing models
according to coverage metrics resulted in preference for the lowest capacity models possible. This indicates
that optimal performance according to coverage favors blurry image reconstruction, with as little certainty
as possible in the ﬁnal image.
32Published in Transactions on Machine Learning Research (04/2023)
D Bayesian Deep-Learning Methods and Implementations
Building oﬀ of the high-level descriptions of the diﬀerent BDL methods discussed in Section 3, we provide
more detailed algorithms descriptions and implementation speciﬁcs.
D.1 Bayes-by-Backprop
A popular method for variational approximation to exact Bayesian updates is Bayes-by-Backprop
(BBB) (Blundell et al., 2015). BBB aims to ﬁnd the optimal parameters ψof an approximate distribu-
tion on the NN weights, q(θ|ψ), also known as the variational posterior. This is achieved by maximizing the
variational free energy/evidence lower bound (ELBO),
L(S,ψ) =Eq(θ|ψ)[logp(S|θ)]−KL[q(θ|ψ)||p(θ) ], (57)
with respect to the variational parameters ψ. The ELBO can be optimized with respect to ψusing stochastic
gradients estimated by Monte Carlo,
∇ψL(S,ψ)≈∇ψ/parenleftbig
logp(S|θ) + logp(θ)−logq(θ|ψ)/parenrightbig
, (58)
whereθ∼q(·|ψ)is a sample drawn from the variational posterior, and the gradient is taken through θusing
the reparameterization trick (Rezende et al., 2014).
In this work, the BBB variational posterior is treated as a Gaussian distribution, N(µψ,σψ). The elements
ofσψcomprise a diagonal covariance matrix, meaning weights are assumed to be uncorrelated. A Gaussian
prior,p(θ) =N(θ|σ2), with tunable σis used to initialize the network. Training the network requires
computing a forward-pass and backward-pass. Although the network is parameterized by a distribution
of weights, in each forward pass a single sample is drawn from the variational posterior and propagated
through the network to perform updates. A re-parameterization trick (Kingma & Welling, 2014), in which
the sample /epsilon1is transformed by the function µψ+σψ⊙/epsilon1, is used to ensure a gradient can be calculated for
backpropagation. Finally, to aid learning, it is common to modify the ELBO as
˜L(y,ψ) =Eq(θ|ψ)[logp(S|θ)]−ξ·KL[q(θ|ψ)||p(θ) ], (59)
whereξ>0is an added hyperparameter, known as the Kullback-Leibler (KL) factor. This is beneﬁcial for
training because it puts greater emphasis on the training data in the loss, through the Eq(θ|ψ)[logp(S|θ)]In
the context of medical imaging with INRs, this re-weights the importance of obtaining a Radon transform
of network outputs close to the sinogram measurement data.
D.2 Monte Carlo Dropout
Another popular approach is Monte Carlo dropout (MCD) (Gal & Ghahramani, 2016). There, the authors
argue that optimizing a NN regularized with dropout (Srivastava et al., 2014) applied to every layer can
be interpreted as variational approximation for a deep GP. The ﬁrst two moments of the corresponding
variational posterior can be approximated using Monte Carlo, with Nsamples from NNs sampled with
dropout. The predictive mean is thus calculated as,
Eq(ˆfθ|y)(ˆfθ)≈1
NN/summationdisplay
n=1ˆf(θ(n)), (60)
which is equivalent to averaging the results of Nstochastic forward passes through the network. Details of
the MCD implementation are provided in the “Implementation Details" of main paper Section 3.
D.3 Hamiltonian Monte Carlo
A ﬁnal means of BNN inference which we explore in this work, is Hamiltonian Monte Carlo (HMC) (Duane
et al., 1987; Neal et al., 2011; Betancourt, 2017). Originally proposed for calculations in lattice quantum
33Published in Transactions on Machine Learning Research (04/2023)
Figure 11: An illustration of concentration of measure, in which the expectation, p(θ|S)dθ, of the BNN’s
high-dimensional weight posterior is concentrated in the typical set. This can be attributed to the fact that,
while most of the distribution density p(θ|S)is concentrated about the distribution modes, the volume dθ
is concentrated at the tails. Unlike other MCMC methods, HMC eﬃciently explores and samples from the
entire distribution typical set. (Figure inspired by (Betancourt, 2017).)
chromodynamics (Duane et al., 1987), HMC is an instance of the Metropolis-Hastings algorithm (Chib &
Greenberg, 1995) for Markov Chain Monte Carlo (MCMC) (Gilks et al., 1995). In the context of quantifying
NN uncertainty, HMC can be used to obtain a sequence of random samples that converge in distribution
to samples from the BNN posterior distribution, p(θ|S)(Izmailov et al., 2021). For the high-dimensional
distributions associated with BNNs, the density p(θ|S)is concentrated at the distribution mode, while the
volumedθis concentrated at the distribution tails. The resulting distribution expectation – a product
p(θ|S)dθof distribution volume and density – concentrates in a nearly-singular neighborhood, known as the
typical set, as illustrated in Figure 11. Traditional Metropolis-Hastings MCMC, using a Gaussian random
walk proposal distribution, typically fails to explore the full typical set of these distributions. HMC, however,
leverages the physics of Hamiltonian dynamics via a time-reversible and volume-preserving integrator, to
simulate and propose points scattered around the typical set. This signiﬁcantly improves exploration of the
full distribution and decreases the correlation between consecutive samples, reducing the total number of
required MCMC samples. In this work, HMC was implemented via the NumPyro Python package (Phan
et al., 2019), using a leapfrog integrator and No-U-Turn sampler (Hoﬀman et al., 2014). For a more detailed
description of the HMC algorithm, in the UncertaINR context, we refer the reader to Appendix E.
D.4 Deep Ensembles
Alternatively, predictive uncertainty can be quantiﬁed by aggregating the outputs of several NN “base learn-
ers” trained for the same task from diﬀerent initializations, a method known as deep ensembles (DEs) (Lak-
shminarayanan et al., 2017). Averaging predictions over multiple NNs consistent with the training data leads
to better predictive performance, enables uncertainty quantiﬁcation, makes DEs robust to model misspeciﬁ-
cation, and presents a strong baseline in out-of-distribution detection (Ovadia et al., 2019). Ensembling can
also be applied on top of other methods to further improve performance. In principle, more base learners
results in better performance, but, in practice, training large ensembles is computationally expensive and
diminishing returns are observed after ∼10 base learners. In this work, UncertaINR leverages ensembles of
MMonte Carlo dropout base learners, such that Eqs. 9 and 10 are updated to,
ˆfDE(x,y) =1
NM/summationdisplay
m=1N/M/summationdisplay
n=1fθm,n(x,y) (61)
VDE(x,y) =1
N−1M/summationdisplay
m=1N/M/summationdisplay
n=1/parenleftbig
fθm,n(x,y)−ˆfDE(x,y)/parenrightbig2.
34Published in Transactions on Machine Learning Research (04/2023)
The relationship between Bayesian inference and DEs is an active area of research in the BDL community.
Wilson & Izmailov (2020) argue that DEs provide a more compelling approximation to the true posterior
than many standard BDL approaches, whilst others have adapted DEs to provide a Bayesian interpretation
(Ciosek et al., 2019; D’Angelo & Fortuin, 2021; Pearce et al., 2020). He et al. (2020) characterize how DEs
relate to posterior inference in the limit of inﬁnite NN width.
Typically, DEsinducerandomnessbytrainingthesamenetworkseveraltimeswithrandomizedinitializations
and data order. However, recent work (Zaidi et al., 2021) has shown that ensembling over architectures
can outperform the more common single-architecture DEs for uncertainty estimation. For our large-scale
hyperparameter study (on Shepp-Logan phantom data), in which thousands of BBB and MCD UncertaINR
base learners were trained, we were able to easily implement architecture-ensembled DEs, by selecting the
best-performing UncertaINRs as base learners. However, for the high-resolution AAPM-Mayo data, in
which UncertaINR training times were extended signiﬁcantly, we found it too computationally demanding
to hypertune multiple architectures to ensemble. Thus, for the ﬁnal results presented, DEs are created by
ensembling the same network trained with randomized weight initialization.
D.5 Hardware and Software Notes
The experiments presented in this paper were computationally intensive, requiring hundreds of compute
hours on parallelized GPUs. Runtimes for each of the methods developed and implemented in this work on
the AAPM dataset are presented in Table 4. Non-HMC methods were run on a cluster of 4 GPU nodes
consisting of 8 GPUs each, containing a mixture of GTX 1080, GTX 1080Ti, and GeForce RTX 2080 Ti
cards. HMC methods (*) were run on two Titan RTX cards, which were faster and had double the memory.
Note that the HMC models were further intialized to a pre-trained check-point, which helped the models
converge much faster. The time required to obtain this pre-trained checkpoint is not reﬂected in the HMC
runtimes. Furthermore, while overall runtimes of the ensemble methods (†) appear long, each of the base
models could be trained in parallel.
Table 4:Computation Times : Most experiments were run on a GPU cluster consisting of four GPU nodes
with 8 GPUs each, consisting of a mixture of GTX 1080, GTX 1080Ti, and GeForce RTX 2080 Ti cards.
Experiments denoted with a (*) were run on faster Titan RTX cards, with increased memory. (†) is used to
denote ensemble experiments, which in practice were run in parallel to achieve faster runtimes.
Reconstruction Method 60-View Runtime 120-View Runtime Compute Type
GOP-TV <10min ∼10min GTX/GeForce
HMC GOP-TV 25+min* 45+min* Titan RTX*
INR ∼2 hrs ∼2.75 hrs GTX/GeForce
DE-2 UINR ∼4 hrs†∼5.5 hrs†GTX/GeForce
DE-5 UINR ∼10 hrs†∼11 hrs†GTX/GeForce
DE-10 UINR ∼20 hrs†∼27.5 hrs†GTX/GeForce
MCD UINR ∼2.25 hrs ∼2.8 hrs GTX/GeForce
DE-2 MCD UINR ∼5 hrs†∼5.6 hrs†GTX/GeForce
DE-5 MCD UINR ∼11.25 hrs†∼14 hrs†GTX/GeForce
DE-10 MCD UINR ∼22.5 hrs†∼28 hrs†GTX/GeForce
HMC UINR 8+hrs* 11.5+hrs* Titan RTX*
Overall, we do not forsee computation as a limitation of UncertaINR. In fact, this work demonstrates that
MCD (one of the least computationally intensive means of UQ) is highly eﬀective for INR UQ, while adding
minimal overhead to the INR evaluation time. Furthermore, as mentioned in Section 2.2 recent work on
INR optimization has found ways to reduce INR evaluation times. These approaches could be implemented
directly within the UncertaINR framework to reduce training time, without aﬀecting the network results or
uncertainty quantiﬁcation. This, however, would neither add any novelty or change the fundamental claims
of this paper, which are about UQ and not computational speed.
The project codebase was developed in Python, mostly using Pytorch (Paszke et al., 2019), Hydra (Yadan,
2019), and Weights & Biases (Biewald, 2020) to implement the NN functionality and Blitz (Esposito, 2020)
for BNN functionality. For HMC, we used the No-U-Turn-Sampler (Hoﬀman et al., 2014) sampling scheme
in NumPyro (Phan et al., 2019), which is based in JAX (Bradbury et al., 2018).
35Published in Transactions on Machine Learning Research (04/2023)
E HMC Sampling Algorithm for UncertaINR
Our goal in using HMC with UncertaINR is to sample weight parameters, {θ(1),...,θ(N)}, from the BNN
weight posterior, p(θ|S). This problem is reformulated in terms of physics-inspired dynamics. These dynam-
ics are governed by Hamiltonian
H(θ,Pθ) =U(θ) +1
2PT
θM−1Pθ, (62)
whereθare the ‘position’ terms, Pθare the ‘momentum’ terms, U(θ) =−lnp(θ|S)is the system potential
energy,1
2PT
θM−1Pθis the system kinetic energy, and Mis a symmetric positive deﬁnite mass matrix.
HMC starts by initializing parameters, θ(0)∼N(0,1
τ), by sampling from the Gaussian distribution of prior
precisionτ. At HMC iteration n, the parameters are initialized to θ(n)(0) =θ(n)while the momentum is
initialized by sampling from the normal distribution P(n)
θ(0)∼N (0,M)of variance deﬁned by the mass
matrix. The leapfrog iterative algorithm is then used to simulate system dynamics for time L∆t, whereLis
the number of leapfrog steps and ∆tthe step size. In each step, the leapfrog algorithm alternates between
momentum and position updates, using
P(n)
θ/parenleftbigg
t+∆t
2/parenrightbigg
=P(n)
θ(t)−∆t
2∇U(θ)|θ=θ(n)(t) (63)
θ(n)(t+ ∆t) =θ(n)(t) + ∆tM−1P(n)
θ/parenleftbigg
t+∆t
2/parenrightbigg
(64)
P(n)
θ(t+ ∆t) =P(n)
θ/parenleftbigg
t+∆t
2/parenrightbigg
−∆t
2∇U(θ)|θ=θ(n)(t+∆t), (65)
so as to solve Hamilton’s equations
dθ
dt=∂H
∂Pθ(66)
dPθ
dt=∂H
∂θ. (67)
Since the leapfrog iterative algorithm is a discretized numerical approximation to the true integral, the limit
∆t→0would be needed to solve Hamilton’s equations exactly. To correct for bad proposals from the
leapfrog method, an HMC Metropolis-Hastings acceptance ratio is deﬁned as
αHMC(θ(n)(0),θ(n)(L∆t)) = min/braceleftBigg
1,exp[−H(θ(n)(L∆t),P(n)
θ(L∆t))]
exp[−H(θ(n)(0),P(n)
θ(0))]/bracerightBigg
, (68)
whereHis the Hamiltonian deﬁned in Eq. 62. In result, the parameter sample returned by iteration n,
which is also the parameter initialization of iteration n+ 1, is
θ(n+1)(0)|θ(n)(0),θ(n)(L∆t) =/braceleftBigg
θ(n)(L∆t),with probability αHMC(θ(n)(0),θ(n)(L∆t))
θ(n)(0),otherwise.(69)
This process is repeated for T/primeHMC iterations. Since the prior weight initialization does not usually lie
within the typical set of the distribution, the samples initially output by HMC are poor indicators of the
typical set region. To address this problem, a burn-in period of Binitial iterations is deﬁned and all burn-in
samples are discarded after the algorithm is complete.
36Published in Transactions on Machine Learning Research (04/2023)
F Preliminary Ablation Study
Our preliminary ablation study presents a large-scale study of uncertainty quantiﬁcation for INRs. Specif-
ically, we test the relative ability of UncertaINR (using MCD, BBB, and/or DEs) to reconstruct artiﬁcial
noiseless CT brain images. From this study, we present guiding principles for eﬀective UncertaINR hyper-
parameter selection and compare to traditional CT reconstruction techniques.
F.1 Dataset
Given the long INR training times required to reconstruct large, high-frequency images (Fridovich-Keil et al.,
2022), we opted for a dataset of simple images, enabling large-scale hyperparameter sweeps. Speciﬁcally,
the Shepp-Logan phantom (Shepp & Logan, 1974) approach was used to generate ( 256×256pixel) artiﬁcial
brain images, with corresponding measurement sinograms generated via the Radon transform. No noise
was added to the measurement sinograms. In all, 10 ground truth images, depicted in Figure 12, and 20
corresponding sinograms were generated: 5 validation and 5 test set sinograms each for the 5- and 20-view
(φ) cases.
Validation SetImage #1
 Image #2
 Image #3
 Image #4
 Image #5
Test Set
0.00.20.40.60.81.0
Figure 12: The ground truth images used in tuning and assessing our preliminary Shepp-Logan hyper-
paramter exploration. The ﬁve validation set images were used to optimized model hyperparameters, while
test set images were used to assess the ﬁnalized models.
F.2 Baselines
In this study, we compared UncertaINR to the classical medical image reconstruction algorithms – FBP,
CGLS, EM, SART, and SIRT – described in Section 2.1.1 and Appendix B, implemented using the TomoPy
Astra wrapper (Pelt et al., 2016). A detailed analysis of the classical reconstruction methods on the Shepp-
Logan validation set is presented in Appendix B.7.
F.3 Tuneable INR Model Parameters
ThereareseveraldegreesoffreedomindesigninganINR,includingitssize, embeddings, activationfunctions,
and optimizer. These design choices are critical in determining model performance, but there is little
theoretical understanding of how to best select most of these model parameters. In this section, we lay
out the diﬀerent parameters we considered in designing our INRs and provide any known insights as to
how they aﬀect model performance. These insights informed the hyperparameter sweeps described in the
following section.
F.3.1 Width and Depth
The size of an NN is determined by both its width (number of nodes per layer) and depth (number of
layers). Universal approximation theorems (Hornik et al., 1989) have been derived in both the arbitrary-
width (Cybenko, 1989; Hornik, 1991) and arbitrary-depth (Hanin, 2019; Kidger & Lyons, 2020; Lu et al.,
37Published in Transactions on Machine Learning Research (04/2023)
2017) cases, demonstrating that NNs are theoretically guaranteed universal function approximators in the
inﬁnite limit. In practice, however, neural networks have ﬁnite width and depth. Recent work has empirically
demonstrated and theoretically suggested that, in this regime, increased-depth networks generally perform
better than increased-width networks (Lu et al., 2017; Raghu et al., 2017). It is also known that, while neural
networks are overparametrized relative to the amount of training data, this overparametrization is key for
their generalization ability (Jacot et al., 2021; Neyshabur et al., 2019). However, for INRs speciﬁcally,
it has been shown that relatively small networks can typically be used to learn decent functional image
encodings (Dupont et al., 2021). Thus, we tend to sweep over smaller widths and depths than standard
deep-learning networks.
F.3.2 Fourier Feature Mappings
Random Fourier features (RFF) were ﬁrst introduced in 2007 by (Rahimi & Recht, 2007) as a means of
accelerating kernel methods. The key idea is to map the input data to a randomized low-dimensional feature
space, while maintaining the kernel of the original data. Given input /vector x∈Rn, the RFF mapping takes the
form
γRFF(/vector x) = [cos(2πB/vector x),sin(2πB/vector x)]T, (70)
whereBis anm×nmatrix, with each entry sampled from N(0,Ω2
0). The standard deviation, Ω0, is a
tuneable hyperparameter, but remains static after initialization – i.e. it is not modiﬁed with NN weights
during the MLP training. There exist other types of Fourier feature mappings, such as positional encodings ,
in which
γPE(/vector x) = [...,cos(2πΩj/m
0/vector x),sin(2πΩj/m
0/vector x),...]T, (71)
forj= 0,...,m−1.
In 2018, it was theoretically demonstrated that NNs can be approximated by kernel regression via the neural
tangent kernel (NTK) (Jacot et al., 2021). Using this intuition, in 2020, it was argued that applying a simple
Fourier feature mapping to input data enables MLPs to learn high-dimensional functions rapidly, even in low-
dimensional problem domains (Tancik et al., 2020), making the technique particularly well-suited for INRs.
In fact, positional encodings have been shown to have key importance in the success of NeRF (Mildenhall
et al., 2020) and Fourier feature mappings have been shown to boost the performance of the CoIL network
for medical image reconstruction (Sun et al., 2021). In this work, all networks apply an RFF mapping, γRFF,
to the input data. The standard deviation, Ω0, is tuned among other hyperparameters.
F.3.3 Activation Functions
Activation functions are key to the success of neural networks, transforming what would otherwise be simple
linear systems into complex, non-linear universal function representers. We performed hyperparameter
sweeps with ﬁve activations widely used in the MLP and INR literature – ReLU, SiLU, Sine, SoftPlus, and
Tanh. We now brieﬂy review these activation functions, as well as their use in deep learning.
The rectiﬁed linear unit (ReLU), plotted in blue in Figure 13, was introduced as early as the 1960s for visual
feature extraction in hierarchical NNs (Hara et al., 2015). The ReLU is deﬁned as
ReLU (x) = max{0,x}, (72)
returning its input if greater than zero and otherwise returning zero. Despite its hard non-linearity at zero,
non-diﬀerentiability at zero, and vanishing gradient challenge (Lu et al., 2020), the ReLU was shown in 2011
to enable better training than previously used activation functions, such as Sigmoid and Tanh, by inducing
sparse representations (Glorot et al., 2011). As of 2017, the ReLU was the most popular activation function
for deep NNs (Ramachandran et al., 2017).
The sigmoid-weighted linear unit (SiLU), plotted in orange in Figure 13, is a speciﬁc instance of the Swish ac-
tivation function family and was proposed in 2017 as a continuous, ‘undershooting’ version of the ReLU (Ra-
machandran et al., 2017). The Swish family, parameterized by β, is deﬁned as
Swishβ(x) =x·σ(βx) =x
1 +e−βx, (73)
38Published in Transactions on Machine Learning Research (04/2023)
−2−1 0 1 2
x−1.0−0.50.00.51.01.52.02.5Activation (x)ReLU
SiLU
Sine
SoftPlus
Tanh
Figure 13: The ﬁve diﬀerent activation functions – ReLU, SiLU, Sine, SoftPlus, and Tanh – tested in our
preliminary hypertuning experiment.
whereσ(x)isthesigmoidfunction. Bysetting βtodiﬀerentvaluesin [0,∞), Swishβnon-linearlyinterpolates
smooth functions between the linear function and ReLU. In 2017, Swish was empirically shown to outperform
ReLU, a result theoretically attributed to its bounded, smooth, and non-monotonic nature (Ramachandran
et al., 2017). More recently, Swish has been shown to outperform both ReLU and Sine in the context of CT
image reconstruction via Automatic Integration (AutoInt) (Lindell et al., 2021). The SiLU is the speciﬁc
instance of Swish where β= 1,
SiLU (x) =x·σ(x) =x
1 +e−x. (74)
The Sine activation function, plotted in green in Figure 13, is the sinusoid
Sineω0= sin(ω0·x). (75)
In the 2020 SIREN paper (Sitzmann et al., 2020), INRs with sinusoidal activation functions and random
Fourier features were empirically demonstrated to outperform ReLU-based INRs. Theoretically, it was
argued that these periodic activations are better suited to capturing naturally complex signals and their
derivatives. However, the performance of these activations depends strongly on the choice of frequency, ω0,
which needs to be tuned.
The SoftPlus activation function, plotted in red in Figure 13, has continuous and diﬀerentiable form
Softplus (x) = ln(1 +ex). (76)
It was introduced in 2001 (Dugas et al., 2001) as the primitive of the sigmoid function. It is primarily used
as a smooth approximation to the ReLU activation and to constrain to positive outputs, since Softplus (x)∈
(0,∞).
The hyperbolic tangent Tanh, plotted in purple in Figure 13, has form
Tanh (x) =ex−e−x
ex+e−x, (77)
and is both diﬀerentiable and monotonic. It has a form similar to the sigmoid function, with Tanh (x) =
2σ(2x)−1, but lies in the range (-1,1) instead of (0,1), meaning it does not constrain to positive values.
Before the ReLU became popular, the sigmoid and Tanh were two of the most common activation functions.
Tanh was easier to train and typically outperformed the sigmoid as an activation function. However, because
thesesigmoidalactivationfunctionssaturateforlargeinputs, theirderivativesvanishfortheseinputs, leading
to slow convergence of learning algorithms. This has motivated the increased use of ReLU-like activation
functions (Goodfellow et al., 2016), which ameliorate the vanishing derivative problem.
39Published in Transactions on Machine Learning Research (04/2023)
F.4 Experimental Design
The goal of this hyperparameter study was to understand the relative performance of UncertaINR with
BBB, MCD, and DEs. In all these cases, well-tuned hyperparameters were needed to achieve decent model
reconstruction accuracy and uncertainty calibration. Large hyperparameter sweeps were used to ﬁnd the
optimal parameters for BBB and MCD. The best performing MCD UncertaINRs were used as base learners
for the DEs.
F.4.1 BBB & MCD
ForbothBBBandMCD,MLPdesignchoiceshadalargeeﬀectonINRreconstructionperformance. Carefully
designed hyperparameter sweeps were thus run to strategically search the MLP parameter space for four
diﬀerent settings: (1) MCD 5-view, (2) MCD 20-view, (3) BBB 5-view, and (4) BBB 20-view.
The model selection process began with a coarse grid search to eﬃciently prune across the wide range of
possible parameter combinations. Speciﬁcally, we considered model activation type, depth, width, RFF
embedding frequency Ω0, and dropout probability. Among these, activation type was the only categorical
parameter, using the ﬁve activation types plotted in Figure 13: Tanh, SoftPlus, Sine, SiLU, and ReLU. For
the remaining parameters, this initial coarse grid search was used to get a sense of orders of magnitude, for
the sake of computational feasibility. We swept over model depths of 3, 6, and 9; widths of 16, 64, 256,
and 1024; and RFF Ω0’s of 1, 5, 10, and 15. Three values - 0.2, 0.5, and 0.8 - were considered for the ﬁnal
parameter, dropout probability, which is speciﬁc to MCD. For BBB, we instead swept over the Gaussian
prior standard deviation (values 10, 100, and 1000)5and KL factor (values 1e-10, 1e-5, and 1e-1)6. For
these coarse grid searches, all networks were trained using the Adam optimizer with no weight decay and the
default learning rate of 3e-4. For each set of parameters, three individual INRs were trained, one for each of
the three validation images, Image #1-#3, shown in Figure 127. All reported metrics are averaged across the
three test images, in an eﬀort to ensure model generalization and prevent overﬁtting to a particular image.
For both the 5- and 20-view experiments, 2,160 (2,512) models were trained and tested for the MCD (BBB)
coarse grid sweeps. This resulted in a total of 9,344 models trained and tested during these initial grid
searches. Since the performance metrics are calculated from a distribution of predictions, sampled according
to the uncertainty method, all hyperparameter sweeps used 50 prediction samples to enable uncertainty
quantiﬁcation, mean output prediction, and metric calculation.
For now, we conclude our discussion of methodology, with the second hyperparameter sweep – a ﬁne Bayesian
search used to generate the ﬁnal models. This Bayesian sweep leveraged a reduced search space, informed by
the ﬁrst coarse grid search. It was found that Bayesian sweeps do not perform well with categorical variables,
so independent sweeps of ∼200 runs were performed for each of the three best performing activation functions
from the grid search. Since only 600 models were trained in total in each uncertainty-view setting, all ﬁve
validation images of Figure 12 were used as the validation set, to improve model generalization ability.
Further, AdamW was used to optimize the models, with a weight decay hyperparameter added to the sweep.
InthecaseofMCDwith20-views, threesweepswereperformed, oneforeachofthethreeactivationfunctions:
Sine, SiLU, and Tanh. A log uniform distribution, in the range 1e-16 to 1e-1, was swept for the weight decay,
while uniform distributions U(min, max, q), whereqis the discrete interval, were generated and swept over
for the remaining numerical parameters: depth ∈U(2,12,1), width∈U(200,1000,100),Ω0∈U(3,15,1),
andp(dropout )∈U(0.1,0.6,0.1). The top performing model according to PSNR, across all three Bayesian
sweeps, was selected as the ﬁnal model.
F.4.2 Deep Ensembles
Given the robust and computationally intensive nature of DEs, we did not perform large-scale hyperparam-
eter sweeps, as was the case for BBB and MCD. DEs combine the outputs of multiple base learner models
5We originally swept over BBB standard deviation (values 0.2, 0.5, and 0.8), which are more on par with theoretical
expectations. However, we found that increasing prior standard deviation signiﬁcantly improved ﬁnal model performance.
6Given the added BBB uncertainty hyperparameter, we reduced relative number of search values for the remaining sweep
parameters.
7Again, for the sake of computationally eﬃciency, only validation Images #1 and #2 were used for BBB.
40Published in Transactions on Machine Learning Research (04/2023)
to improve uncertainty calibration. Assuming each base learner makes a reasonable prediction, even if not
optimized, adding more base learners should only maintain or improve uncertainty calibration. In order to
create a DE of size M(DE-M), the top-Mperforming models identiﬁed by the MCD hyperparameter sweeps
were used as base learners. If Ntotal samples were desired for uncertainty quantiﬁcation, each of the MCD
baselearners was sampled repeatedly to generateN
Mpredictions. These predictions were pooled together to
create a sample of size N, from which uncertainty was quantiﬁed, the mean prediction generated, and model
metrics calculated. In order to remain consistent with the BBB and MCD experiments, we used N= 50.
F.5 Uncertainty Quantiﬁcation Method Performance Analysis
F.5.1 MCD Hyperparameter Analysis
For MCD hyperparameter sweeps, metrics were averaged across the INR model reconstruction of the ﬁrst
three validation set images of Figure 12. However, we also recorded the PSNR of the INRs trained on each
individual image. Box plots for the individual distributions of PSNR for validation Images #1, #2, and #3
are shown in Figure 14. Ideally, PSNR should be consistent across the three images. This is roughly the case
(barringafewoutlierpoints)formodelsusingtheTanh, SoftPlus, Sine, andSiluactivationfunctions. Notice,
however, that in all cases the distribution is broadest for Image #3. This eﬀect is exacerbated for models
using the ReLU activation function, for which PSNR of Image #3 ranges all the way from approximately
0dB to nearly the maximum achieved PSNR, in both the 5- and 20-view cases. This indicates that ReLU in
particular, but all the all activation functions to some extent, struggle to capture features of Image #3.
1 2 3
Image #05101520253035Individual Image PSNR
ReLU
1 2 3
Image #SiLU
1 2 3
Image #
Sine
1 2 3
Image #
Softplus
1 2 3
Image #Tanh
Views
5
20
Figure 14: Reconstruction PSNR for Images #1-3 of Figure 14, for MCD INRs with diﬀerent activations
and diﬀerent numbers of views.
To understand what is unique about Image #3, the image reconstructions of the best overall performing
model are shown, for each activation function and 20-views, in Fig. 15. It is apparent that Sine and SiLU
produce smoother images, while Tanh, Softplus, and ReLU produce spottier images. Except for SoftPlus,
all activation functions manage to capture low-frequency image information and strong edges. However, all
activations struggle to capture the small, low-intensity ellipses in the center of Image #3. Overall, it is clear
that, irrespective of activation function, 20-views are insuﬃcient to robustly capture ﬁne CT image details.
This individual image analysis also suggests that the Sine activation performs the best for MCD, achieving
the highest overall PSNR.
Figure 16 shows boxplots of the average PSNR distributions for MCD models trained in the 5- and 20-view
cases. Each column contains all the models trained with one of the ﬁve activation functions, while each row
shows how the PSNR distribution changes as a function of hyperparameter – depth, width, probability of
dropout, or RFF frequency Ω0. Ideally, PSNR would be consistently large across hyperparameter values,
indicatingthatthearchitectureisrobustanddoesnotrequiremuchtuning. Inpractice, however, weﬁndthat
the activation functions are either consistent or high-performing, but not both. As previously mentioned,
Sine achieves the best overall PSNR. However, it is also the least consistent activation function, with its
highest performing models typically being outliers (indicated by diamonds in Figure 16). Softplus, on the
other extreme, is very consistent across hyperparameter values, but performs consistently poorly. Tanh, Silu,
and ReLU have less extreme variations. Their top models perform slightly worse than the best Sine models,
but they perform much more consistently across hyperparameter values (ReLU is a bit inconsistent in width
and probability of dropout). This suggests that the Sine network is potentially the best reconstruction
41Published in Transactions on Machine Learning Research (04/2023)
Image #1Tanh
 SoftPlus
 Sine
 SiLU
 ReLU
Image #2
 Image #3
0.00.20.40.60.81.0
Figure 15: Best image reconstructions obtained with each activation function, for 20-view MCD.
network, but signiﬁcant tuning (in terms of hyperparameter search) eﬀort may be needed to achieve that
solution. For practitioners inclined to perform less tuning, the Tanh and SiLU networks may be a preferred
solution, due to their robustness and competitive top performance.
For ReLU, Tanh, SiLU, and Sine, it should also be noted that the PSNR distributions behave similarly in
the 5- and 20-view cases (with 5 views performing consistently worse than 20 views, as expected) for all
hyperparameters except RFF Ω0. This is consistent with recent results (Tancik et al., 2020) suggesting that
RFF embeddings enable NNs to learn higher frequency image information. In the 5-view case, where the
available data is insuﬃcient for the INR to conﬁdently learn high-frequency image features, performance is
poor for increasing Ω0. In the 20-view case, the increased data enables the network to learn higher frequency
image components. However, because a higher-frequency Ω0is required to ensure the network can actually
learn those frequencies, best performance tends to occur for larger Ω0. This eﬀect can also be observed
in Figure 17. For 5 views, the reconstruction is blurry for Ω0= 1but the network starts to learn smaller
ellipses for Ω0= 5. By Ω0= 10, however, the network is trying to learn higher-frequencies than the data
cannot specify, resulting in artifacts, which are exacerbated for Ω0= 30. In the 20-view case, a similar trend
of reduced blurriness and increasingly sharper images can be seen between Ω0= 1anΩ0= 10. However,
the reconstructed images have much more recognizable details and less artifacts than those obtained with 5
views. It is only for Ω0= 30that artifacts start to appear.
42Published in Transactions on Machine Learning Research (04/2023)
3 6 9
Depth1015202530Avg PSNRReLU
3 6 9
DepthSiLU
3 6 9
Depth
Sine
3 6 9
Depth
Softplus
3 6 9
DepthTanh
Views
5
20
16 64 256 1024
Width1015202530Avg PSNR
ReLU
16 64 256 1024
WidthSiLU
16 64 256 1024
Width
Sine
16 64 256 1024
Width
Softplus
16 64 256 1024
WidthTanh
Views
5
20
0.2 0.5 0.8
P(Dropout)1015202530Avg PSNR
ReLU
0.2 0.5 0.8
P(Dropout)SiLU
0.2 0.5 0.8
P(Dropout)
Sine
0.2 0.5 0.8
P(Dropout)
Softplus
0.2 0.5 0.8
P(Dropout)
Tanh
Views
5
20
1 5 10 30
RFF Ω01015202530Avg PSNRReLU
1 5 10 30
RFF Ω0SiLU
1 5 10 30
RFF Ω0
Sine
1 5 10 30
RFF Ω0
Softplus
1 5 10 30
RFF Ω0
Tanh
Views
5
20
Figure 16: Boxplots of the average PSNR of MCD models trained in the coarse grid search hyperparameter
sweep, for both 5 and 20 views. Each column corresponds to a diﬀerent activation function and each row
to a sweep over one of the remaining hyperparameters - depth, width, probability of dropout, and RFF
frequency Ω0. Individual diamond points are outliers.
43Published in Transactions on Machine Learning Research (04/2023)
Ω0= 1ReLU
 SiLU
 Sine
 SoftPlus
 Tanh
Ω0= 5
 Ω0= 10
 Ω0= 30
0.00.20.40.60.81.0MCDropout 5-Views
Ω0= 1ReLU
 SiLU
 Sine
 SoftPlus
 Tanh
Ω0= 5
 Ω0= 10
 Ω0= 30
0.00.20.40.60.81.0MCDropout 20-Views
Figure 17: MCD image reconstruction, in both the 5- and 20-view cases, for each activation function and
RFF frequency Ω0value. Note that in the 5-view case, Ω0= 5enables the network to learn low-frequency
image features, without many artifacts. Smaller Ω0causes the network to produce overly simple output,
whereas larger Ω0induces high-frequency artifacts. In the 20-view case, similar observations are made, but
the optimal Ω0= 10. In this case, the reconstruction has higher frequencies without signiﬁcant artifacts, for
most activation functions.
44Published in Transactions on Machine Learning Research (04/2023)
F.6 Comparing the eﬀects of RFF frequency for DEs and MCD
In Section 4, we saw that MCD is an eﬀective approach to obtain calibrated UQ in INRs, compared to DEs.
This is a surprising observation, particularly in relation to the typical belief, in BDL settings (e.g. image
classiﬁcation), that deep ensembles are a very strong or state-of-the-art baseline (Ovadia et al., 2019).
While we do not have a full justiﬁcation for this observation, we suspect that the underperformance of DEs
(relative to MCD), in the context of UncertaINR, is related to the capacity that each individual ensemble
member has. In particular, we observed that the RFF frequency Ω0is a very important hyperparameter.
This frequency is related to the ‘capacity’ of the model, in the sense that higher Ω0implies ability to learn
higher frequency details of the image, as shown in Fig. 10 of Tancik et al. (2020).
In Fig. 18, we plot the eﬀect of RFF frequency Ω0for diﬀerent numbers of sinogram views (5 and 20), for
both MCD and DEs. For 20-view Shepp-Logan data, we observe that an ensemble (blue) achieves roughly
the same PSNR at both Ω0= 8andΩ0= 40(top right). However, the NLL at Ω0= 40is signiﬁcantly
better (lower) than the lower capacity Ω0= 8(bottom right). This indicates that the models with higher
RFF frequencies generate more diverse, but still accurate, predictions and hence obtain better uncertainty
estimates. On the other hand, MCD introduces an alternative eﬀective way of encouraging diverse yet
accurate predictions through diﬀerent dropout seeds at a set dropout rate p. Thus, dropout combined with
RFF encodings allows the individual models to have suﬃcient capacity to produce diverse yet accurate
predictions. In contrast, deep ensembles rely solely on the RFF encodings to regulate the model capacity
(and thus, diversity in predictions). This is also corroborated in Fig. 18 by the fact that MCD achieves
better NLL (on Shepp-Logan phantoms) at lower Ω0(i.e. lower capacity) than deep ensembles.
A more formal understanding of the settings in which deep ensembles are eﬀective, as well as interactions
with MCD, is important future work both for UncertaINR and BDL generally.
Figure 18: The eﬀect of changing RFF frequency Ω0for PSNR and NLL with MC dropout and Deep
Ensembles.
45Published in Transactions on Machine Learning Research (04/2023)
F.6.1 BBB Hyperparameter Analysis
The BBB model selection analysis is presented in a similar fashion to that of MCD. The main diﬀerences to
MCD are that, in the BBB grid search, metrics are averaged over only the ﬁrst two validation set images; the
width and RFF search spaces are reduced; and the uncertainty parameters are the KL factor and Gaussian
prior standard deviation (not probability of dropout). Figure 19 shows boxplots of average PSNR as a
function of diﬀerent hyperparameter values. Unlike MCD, model performance is extremely consistent across
activation functions for every hyperparameter, except for width. We note that Tanh has some variation for
RFF Ω0, following trends similar to those discussed in the case of MCD. Overall, SiLU performs the best,
with ReLU and Tanh following closely behind. Interestingly, Sine performs the worst, failing to reach even
20db. However, greater performance consistency across hyperparameter conﬁgurations comes at the price
of weaker top-performing models, which achieve lower PSNR values than those of MCD. Because the image
sets are not identical, these comparisons across approaches should be taken with some reservation.
To understand why there is so much variation in BBB performance as a function of width, consider Figure 20,
where the average PSNR obtained for each width is plotted as a function of prior standard deviation. It
can be seen that the PSNR values are extremely consistent for each network width, across prior standard
deviations. From a Bayesian perspective, the fact that the prior does not aﬀect inference suggests that
the latter is dominated by the model likelihood. However, mean performance decreases as a function of
model width, indicating that the model becomes increasingly misspeciﬁed for larger widths. Given the
Gaussian assumptions made in the variational inference speciﬁcations of BBB, this suggests that the true
posterior distribution becomes less Gaussian as network width increases, and the variational approximation
deteriorates. When combined with the low performance of BBB relative to other uncertainty quantiﬁcation
methods, reported in Table 1, this indicates that BBB may not be well suited for uncertainty quantiﬁcation
of INRs in the medical imaging context.
Furthermore, the overall BBB ablation results appear to mimic Coker et al. (2022)’s observation of mean-
ﬁeld BNN performance degradation with increased width. In all, BBB’s poor performance is unsurprising
and well-established in the BDL literature (Ovadia et al., 2019) (here BBB=SVI).
Table 5: Top 10 performing MCD models and their performances, for the 5-view case ( Top) and 20-view
case ( Bottom ). Models are ranked by PSNR, but NLL and ECE are also reported.
RankActivation Depth Width RFF Ω0PD W. Decay PSNR NLL ECE
1 Sine 4 800 2 0.4 0.001 26.15 -1.437 0.122
2 Sine 3 600 4 0.4 0.157 25.79 -1.799 0.008
3 Sine 3 600 8 0.7 0.366 25.76 -1.579 0.009
4 Sine 3 700 2 0.4 4.46e-4 25.75 -1.533 0.117
5 Sine 4 700 3 0.5 9.36e-4 25.72 -1.390 0.011
6 Sine 3 500 5 0.7 0.077 25.63 2.622 0.161
7 Sine 5 700 3 0.6 0.012 25.62 -1.149 0.123
8 Sine 3 500 5 0.7 0.068 25.62 -1.643 0.007
9 SiLU 8 300 3 0.2 2.68e-7 25.62 0.219 0.389
10 Sine 3 500 4 0.7 0.170 25.59 -1.79 0.083
RankActivation Depth Width RFF Ω0PD W. Decay PSNR NLL ECE
1 Sine 3 400 9 0.4 2.06e-5 33.74 0.701 0.134
2 Sine 4 300 12 0.4 2.63e-7 33.41 1.076 0.149
3 Sine 3 500 8 0.5 0.006 33.37 -0.502 0.137
4 Sine 5 500 11 0.4 3.74e-6 33.29 -0.288 0.137
5 Sine 6 400 10 0.2 5.85e-6 33.28 4.654 0.152
6 Sine 6 500 8 0.2 1.117e-4 33.24 2.622 0.161
7 Sine 4 400 9 0.5 0.001 33.21 -0.798 0.143
8 Sine 3 500 10 0.5 2.53e-4 33.20 -0.716 0.129
9 Sine 5 500 11 0.2 7.87e-7 33.18 1.973 0.163
10 Sine 4 500 8 0.5 4.56e-5 33.17 -1.257 0.114
46Published in Transactions on Machine Learning Research (04/2023)
3 6 9
Depth0510152025Avg PSNR
ReLU
3 6 9
Depth
SiLU
3 6 9
DepthSine
3 6 9
Depth
Softplus
3 6 9
Depth
Tanh
Views
5
20
32 256 1024
Width0510152025Avg PSNR
ReLU
32 256 1024
Width
SiLU
32 256 1024
WidthSine
32 256 1024
Width
Softplus
32 256 1024
Width
Tanh
Views
5
20
3 10 30
RFF Ω00510152025Avg PSNR
ReLU
3 10 30
RFF Ω0
SiLU
3 10 30
RFF Ω0Sine
3 10 30
RFF Ω0
Softplus
3 10 30
RFF Ω0Tanh
Views
5
20
1e-10 1e-05 0.1
KL Factorξ0510152025Avg PSNR
ReLU
1e-10 1e-05 0.1
KL Factorξ
SiLU
1e-10 1e-05 0.1
KL FactorξSine
1e-10 1e-05 0.1
KL Factorξ
Softplus
1e-10 1e-05 0.1
KL FactorξTanh
Views
5
20
10 100 1000
Prior Std. Dev. σ0510152025Avg PSNR
ReLU
10 100 1000
Prior Std. Dev. σ
SiLU
10 100 1000
Prior Std. Dev. σSine
10 100 1000
Prior Std. Dev. σ
Softplus
10 100 1000
Prior Std. Dev. σTanh
Views
5
20
Figure 19: Boxplots of the average PSNR of BBB models trained in the coarse grid search hyperparameter
sweep, for both 5 and 20 views. Each column corresponds to a diﬀerent activation function and each row to
a sweep over one of the remaining hyperparameters - depth, width, RFF Ω0, KL factor ξ, and prior standard
deviationσ.
F.6.2 DE Performance Analysis
As described in Appendix F.4.2, DEs of Mbase learners were created by combining the top- Mperforming
NNs, according to average PSNR. As reported in Table 1, the best MCD model outperforms the best
BBB model signiﬁcantly, with at least a 2dB increase in PSNR, as well as reduced NLL and ECE, for
both the 5- and 20-view cases. Thus, we ensembled the top MCD models produced by the second ﬁne
Bayesian hyperparameter sweeps of Section F.6.1. The parameterizations and performance of the 10 best
performing models for both the 5- and 20-view cases are listed in Table 5. Note that, in both cases, the
model architectures vary greatly across models. While the Sine activation function is fairly consistent, the
remaining parameters vary greatly. For example, in the 5-view case, model depths range from 3 to 8, widths
47Published in Transactions on Machine Learning Research (04/2023)
32 256 1024
Width0510152025Avg PSNR
Prior Sigma = 10
32 256 1024
Width
Prior Sigma = 100
32 256 1024
Width
Prior Sigma = 1000
Views
5
20
Figure 20: Boxplots of average PSNR of BBB models of varying width for each value of prior standard
deviationσ, for both 5- and 20-view cases.
0 10 20 30 40 50
# Base Learners26.1026.1526.2026.2526.3026.3526.40PSNR
0 10 20 30 40 50
# Base Learners−2.6−2.4−2.2−2.0−1.8−1.6−1.4NLL
0 10 20 30 40 50
# Base Learners0.040.050.060.070.08ECE
0 10 20 30 40 50
# Base Learners2.3502.3752.4002.4252.4502.4752.500MSE×10−3 Deep Ensemble 5-View
0 10 20 30 40 50
# Base Learners33.834.034.2PSNR
0 10 20 30 40 50
# Base Learners−0.75−0.50−0.250.000.250.500.75NLL
0 10 20 30 40 50
# Base Learners0.0800.0850.0900.0950.100ECE
0 10 20 30 40 50
# Base Learners3.73.83.94.04.14.24.3MSE×10−4 Deep Ensemble 20-View
Figure 21: PSNR, NLL, ECE, and MSE plotted as a function of the number of base-learners used in DEs,
for both the 5- and 20-view cases. Note that each added base learner performs slightly worse in terms of
image reconstruction quality than the network preceding it.
from 300 to 800, RFF Ω0from 2 to 8, and probability of dropout (PD) from 0.2 to 0.7, all with a variety of
weight decays. These variations increase base learner diversity well beyond diﬀerent weight values.
DEcombinesvariedbaselearnerstoimproveuncertaintycalibration. Inprinciple, ifallbaselearnersachieved
the same PSNR, model performance should only increase (or plateau) and variance should only decrease
(or plateau) as more base-learners are added. In our case, however, each new added base learner had a
slightly lower PSNR on the validation set. To verify how this aﬀected model performance and calibration, we
generated plots of each metric as a function of #of DE base learners. As shown in Figure 21, both PSNR and
MSE improve signiﬁcantly as the ﬁrst base learners are added to the ensemble, but begin to worsen for larger
ensembles. Considering that models of worse PSNR are being added with each increase in #of base learners,
it is unsurprising that performance eventually degrades. However, ensembling never reduces performance
below that of using the single best model. NLL and ECE are more sensitive to the number of base learners.
NLL demonstrates the overall best performance gain as a function of baselearners. ECE, however, does not
change consistently with DE size, with large ensembles sometimes even harming performance relative to the
single best model. In all, it is clear that ensembling improves model performance and calibration. However,
larger ensembles have no gains over smaller ensembles and are far more computationally expensive. Thus,
for the ﬁnal results, presented in Table 1, only ensembles of sizes 2, 5, and 10 are considered.
48Published in Transactions on Machine Learning Research (04/2023)
PSNR: 23.49No Ensembling
PSNR: 24.75DE-2
PSNR: 25.06DE-5
PSNR: 23.41DE-10
ECE: 0.061
 ECE: 0.050
 ECE: 0.057
 ECE: 0.076
0.0 0.2 0.4 0.6 0.8 1.00.00.20.40.60.81.0
Withδ
Withoutδ
0.0 0.2 0.4 0.6 0.8 1.00.00.20.40.60.81.0
Withδ
Withoutδ
0.0 0.2 0.4 0.6 0.8 1.00.00.20.40.60.81.0
Withδ
Withoutδ
0.0 0.2 0.4 0.6 0.8 1.00.00.20.40.60.81.0
Withδ
Withoutδ
0.00.20.40.60.81.0
PSNR: 33.03No Ensembling
PSNR: 33.68DE-2
PSNR: 33.97DE-5
PSNR: 33.76DE-10
ECE: 0.090
 ECE: 0.069
 ECE: 0.080
 ECE: 0.061
0.0 0.2 0.4 0.6 0.8 1.00.00.20.40.60.81.0
Withδ
Withoutδ
0.0 0.2 0.4 0.6 0.8 1.00.00.20.40.60.81.0
Withδ
Withoutδ
0.0 0.2 0.4 0.6 0.8 1.00.00.20.40.60.81.0
Withδ
Withoutδ
0.0 0.2 0.4 0.6 0.8 1.00.00.20.40.60.81.0
Withδ
Withoutδ
0.00.20.40.60.81.0
Figure 22: Image reconstruction and calibration performance of the ﬁnal DE models on test set Image #3 for
5-views (top) and 20-views (bottom). Diﬀerent columns show the results of diﬀerent ensemble sizes, ranging
from 1 to 10. Top row shows the reconstructed image, middle row the pixelwise coverage, and bottom row
the reliability curve.
Figure 22 illustrates image reconstruction performance for the diﬀerent ensemble types on test set Image
#3. In the 5-view case, DEs achieve impressive performance improvements, with a PSNR increase of over
1.5dB for DE-5 and an ECE reduction of 0.011 for DE-2. In the 20-view case the baseline is much better
performing. Hence, although there are gains in PSNR, these are not very noticeable in the reconstructed
image. However, the improvements in calibration are larger, with an ECE drop of 0.2 for DE-2 and a clear
improvement in the image reliability curve.
F.7 Final Results Conclusions
The primary goal of this study was to understand the relative performance of diﬀerent uncertainty quantiﬁ-
cation techniques for UncertaINR, on the Shepp-Logan dataset. Table 1, in the main text, presents metrics
assessingthebest-performingMCD,DE-2MCD,DE-5MCD,andDE-10MCDUncertaINRs. Thesemethods
consistently outperformed the classical reconstruction techniques in terms of image quality, while producing
reasonably well calibrated uncertainty estimates. BBB was found to be the worst performing uncertainty
quantiﬁcation approach, generally producing the poorest calibrated uncertainty estimates and worse image
49Published in Transactions on Machine Learning Research (04/2023)
BBBMean
Variance
MSE
Coverage
0.00.51.00.51.0Reliability
MCD
0.00.51.00.51.0
DE-2
0.00.51.00.51.0
DE-5
0.00.51.00.51.0
DE-10
0.00.51.00.51.0
HMC
0.00.51.00.51.0With±Without±
Mean/Cov0.00.20.40.60.81.0
Var/MSE0.000.020.040.060.080.10
BBBMean
Variance
MSE
Coverage
0.00.51.00.51.0Reliability
MCD
0.00.51.00.51.0
DE-2
0.00.51.00.51.0
DE-5
0.00.51.00.51.0
DE-10
0.00.51.00.51.0
HMC
0.00.51.00.51.0With±Without±
Mean/Cov0.00.20.40.60.81.0
Var/MSE0.000.020.040.060.080.10
Figure 23: Validation results of all approaches for 5-view Shepp-Logan data. From left to right: mean image
reconstruction, variance, MSE, coverage, and reliability diagram.
BBBMean
Variance
MSE
Coverage
0.00.51.00.51.0Reliability
MCD
0.00.51.00.51.0
DE-2
0.00.51.00.51.0
DE-5
0.00.51.00.51.0
DE-10
0.00.51.00.51.0
HMC
0.00.51.00.51.0With±Without±
Mean/Cov0.00.20.40.60.81.0
Var/MSE0.000.020.040.060.080.10
BBBMean
Variance
MSE
Coverage
0.00.51.00.51.0Reliability
MCD
0.00.51.00.51.0
DE-2
0.00.51.00.51.0
DE-5
0.00.51.00.51.0
DE-10
0.00.51.00.51.0
HMC
0.00.51.00.51.0With±Without±
Mean/Cov0.00.20.40.60.81.0
Var/MSE0.000.020.040.060.080.10
Figure 24: Validation results of all approaches for 20-view Shepp-Logan data. From left to right: mean
image reconstruction, variance, MSE, coverage, and reliability diagram.
50Published in Transactions on Machine Learning Research (04/2023)
Table6:AblationStudyDiﬀerences : TestsetresultsfromTable1arepresentedagain, intermsofdiﬀerences
from the average metric value for each image. Speciﬁcally, the table reports the average and standard error
of these diﬀerences across the 5 test images for each metric.
Reconstruction 5-ViewTest Set Differences 20-View Test Set Differences
Type PSNR ( ↑) NLL (↓) ECE (↓) PSNR ( ↑) NLL (↓) ECE (↓)
FBP−14.862±0.239 – – −13.203±0.168 – –
CGLS −5.397±0.201 – – −8.086±0.272 – –
EM−0.136±0.093 – – 0.199±0.364 – –
SART −0.270±0.167 – – 1.536±0.260 – –
SIRT−0.270±0.167 – – 1.533±0.261 – –
BBB UINR 2.504±0.378 1.524±0.383 0.105±0.003−0.749±0.152 0.782±2.137 0.013±0.008
MCD UINR 4.437±0.244−0.186±0.094−0.015±0.005 4.175±0.218 0.329±0.532 0.007±0.012
DE-2 MCD UINR 4.473±0.267−0.389±0.063−0.028±0.006 4.529±0.276−0.153±0.580−0.005±0.011
DE-5 MCD UINR 4.866±0.402−0.366±0.070−0.032±0.006 5.109±0.329−0.405±0.511−0.005±0.006
DE-10 MCD UINR 4.655±0.303−0.583±0.262−0.030±0.006 4.957±0.144−0.554±0.519−0.010±0.011
reconstruction than classical techniques in the 20-view regime. MCD consistently outperformed classical
approaches and was generally better calibrated than BBB. Ensembling over MCD base learners, however,
was the most successful approach, outperforming the best classical approach by ∼4dB in the 5-view case
and∼3dB in the 20-view case, as well as achieving the lowest overall NLL and ECE values.
Given the varying diﬃculty of images in the Shepp-Logan test set, we did not believe that reporting standard
deviationsinTable1wouldaccuratelyreﬂectreconstructionvariability. Instead, Table6presentstheaverage
and standard error over the diﬀerence from the mean metric value for each image. Calculating the diﬀerence
from mean for each image reduces the eﬀect of variability in image diﬃculty. Notice that the diﬀerences
are signiﬁcant with respect to the standard error across distinct methods, although not as signiﬁcant across
ensemble sizes for the MCD UINRs.
One further remark regarding Table 1 is that, although the classical reconstruction procedures did not use
the validation set images as a validation set (since no hyperparameters were tuned), image reconstruction
quality still deteriorated in the test set. This indicates that the test set data is actually more challenging
than the validation set. Given that UncertaINR performance did not signiﬁcantly decline on the test set, we
have strong reason to believe that none of the UncertaINR approaches over-ﬁtted to the validation set.
Figures 23 and 24 visually illustrate the diﬀerence in the uncertainty quantiﬁcation of the diﬀerent methods,
for the 5- and 20-view cases respectively. Beginning with the 5-view case, the mean predicted image was
fairlyblurryforallmethods, onlycapturinglow-frequencyimagecomponentsandexhibitinghighuncertainty
surrounding edges. Furthermore, note that all reliability curves are very well calibrated in this 5-view case
(after aδ-selection adjustment), except for BBB. Similar trends are present in the 20-view case, but it is
visually harder to distinguish diﬀerences in the image output due to the higher quality of all reconstructions.
51Published in Transactions on Machine Learning Research (04/2023)
G UncertaINR AAPM Performance Assessment
G.1 Dataset
In order to assess the performance of UncertaINR in a realistic setting, we used data from the American
Association of Physicists in Medicine (AAPM) and Mayo Clinic 2016 Low-Dose CT grand challenge (McCol-
lough et al., 2017). Speciﬁcally, we use the 8 reconstructed images used as a test set in the CoIL work (Sun
et al., 2021), shown in Figure 25, so as to directly compare to their results. These ground truth images were
reconstructed from real CT scan measurement data.
Figure 25: The 8 AAPM-Mayo test set images used to assess UncertaINR performance.
In order to make the dataset more realistic and compare to the results presented in CoIL, noise was added to
the image sinograms before being input to the models. This artiﬁcial data generation pipeline is illustrated
in Figure 26. More speciﬁcally, uniformly distributed Gaussian white noise was added to the sinogram, so
as to achieve a desired SNR relative to the original, noise-less sinogram. In this work, we chose to present
results for a noise level achieving sinograms of 40dB SNR.
Figure 26: A ﬂowchart of the artiﬁcial data generation pipeline used in this work. (a)The Radon transform
is used to generate a sinogram, corresponding to measurement data, from the ground truth data. (b)In the
noiseless case, this sinogram would be directly used to train our model. Note that even though the sinogram
is noiseless, the reconstruction is not perfect. (c)In a more realistic scenario, Gaussian white noise is added
to the sinogram. (d)This noisy sinogram data is then fed into the model, which produces a reconstructed
image of the ground truth, of lower quality that produced in the noiseless case.
G.2 Final Model Hyperparameters
Given the increased computational costs in running UncertaINR on the AAPM-Mayo dataset, it was not
feasibletoperformlarge-scalehyperparametersweeps, likethoseperformedintheablationstudypresentedin
Appendix F. However, coarse searches were performed for each hyperparameter and new training approaches
were used in order to achieve the competitively performing models presented in Table 2. For reproducibility,
we provide the ﬁnal hyperparameters and training procedures.
52Published in Transactions on Machine Learning Research (04/2023)
Beyond training insights learned from the hyperparameter study, further improvements were made to Un-
certaINR training in order to achieve competitive reconstruction accuracy on the higher-resolution, higher-
frequency, and noisy Mayo-AAPM data. Speciﬁcally, images were zero-padded to ensure that all image
content was contained in the projections when calculating the Radon transform. All UncertaINR were opti-
mized using Adam (not AdamW, to eliminate the computational costs of tuning the weight decay parameter)
and we found that longer training times were needed (on the order of 15,000 epochs). If that many training
epochs cannot be used for some reason, we found that stochastic weight averaging (Izmailov et al., 2018) can
be used to improve reconstruction accuracy by a few decibel for UncertaINRs trained with few epochs. With
the increased number of training iterations, the Sine activation proved too unstable and we found SiLU to be
the best performing activation function, with Tanh a close second. Noting the signiﬁcance of RFFs from the
ablation study, we found it crucial that the width of the RFF layer be at least as large as the padded image
width. Finally, before adding a regularization term to the UncertaINR loss, our models were not competitive
with state-of-the-art reconstruction techniques. While adding an isotropic TV regularization,
˜TISO(f) =/summationdisplay
x,y∈X×Y(f(x+ 1,y)−f(x,y))2+ (f(x,y+ 1)−f(x,y))2,
boosted performance by a few decibel, the anisotropic approximation of the TV regularizer,
˜TANISO (f) =/summationdisplay
x,y∈X×Y|f(x+ 1,y)−f(x,y)|+|f(x,y+ 1)−f(x,y)|,
achieved better reconstruction accuracy, especially in the low-measurement (60-view) regime. (at the slight
cost of uncertainty calibration). The relative performance of these two regularizers relative to no TV reg-
ularizer are presented for both GOP and MCD UINR in Table 7. While we do not have a justiﬁcation for
why the anisotropic approximation performs better than isotropic for reconstruction accuracy, we believe
that regularization/prior exploration would be interesting future work.
Table 7: The aﬀect of diﬀerent regularizers on image reconstruction performance.
Reconstruction Method Regularization Type 60-View SNR 120-View SNR
None 18.33 18.85
GOP Isotropic 23.62 25.60
Anisotropic 25.97 27.40
None 25.75 27.35
MCD Uinr Isotropic 25.78 28.06
Anisotropic 27.38 28.65
Given all these training insights, the hyperparameters of the top-performing (MCD and HMC) UINRs,
reported in Table 2, are presented in Table 8.
Table 8: Hyperparameters of the top-performing MCD UINRs, HMC UINRs, and INRs reported in Table 2.
Additional HMC-speciﬁc hyperparameters, such as burn-in periods, are provided in Appendix G.2.1.
Model # Views Act. Func. Depth Width RFF Ω0Reg Type Reg Coeff p(Dropout) # Epochs
INR 60 SiLU 5 280 48 Aniso 0.05 0 15,000
MCD UINR 60 SiLU 5 280 35 Aniso 0.03 0.2 15,000
HMC UINR 60 SiLU 4 200 48 Aniso 0.03 N/A N/A
INR 120 SiLU 5 280 60 Aniso 0.05 0 15,000
MCD UINR 120 SiLU 5 280 40 Aniso 0.008 0.2 15,000
HMC UINR 120 SiLU 4 200 60 Aniso 0.03 N/A N/A
G.2.1 HMC-speciﬁc hyperparameters
As mentioned in Section 3, all our HMC experiments used the NUTS (Hoﬀman et al., 2014) implementation
in NumPyro (Phan et al., 2019). All HMC runs ran for 1000 samples and had a burn-in period of 500
53Published in Transactions on Machine Learning Research (04/2023)
samples, with a thinning factor of 5, so that every ﬁfth post burn-in sample was used at evaluation time. We
initialized the model parameters from a trained model for both GOP and UINRs (corresponding to models in
theGOPandINRrows of Table 2 respectively) to enable faster mixing. For NUTS speciﬁc hyperparameters,
we set the max_tree_depth to 8 and target a Metropolis-Hastings acceptance rate of 0.8. During the burn-in
period, NUTS adaptively sets the step-size and (diagonal) mass matrix hyperparameters which are crucial
to the eﬀectiveness of HMC; more details can be found in Hoﬀman et al. (2014).
HMC-GOP hyperparameters For 60 and 120 views we set the TV regularisation strength to 0.5 and 0.3
respectively. Both values were tuned on a small grid of hyperparameter values.
HMC-UNIR hyperparameters We set the parameter variance, τ2in Eq. 8, to1√
γ×width, withγ= 0.2and
0.15respectively for 60 and 120 views. γ, and the regularisation coeﬃcients in Table 8, were tuned on a small
grid of values due to the high computational cost of HMC. Due to the non-convex nature of NN parameter
space, we aggregated the samples from two independent chain runs (from two independently trained MAP
initializations) for HMC-UINR, the maximum number of chains that ﬁt into memory on a 24GB VRAM
Titan RTX GPU.
G.3 Results Analysis
Given the varying diﬃculty of images in the AAPM test set, we did not believe that reporting standard
deviations in Table 2 would accurately reﬂect variability. Instead, Table 9 presents the average and standard
error over the diﬀerence from the mean metric value for each image. Calculating the diﬀerence from mean
for each image reduces the eﬀect of variability in image diﬃculty. Notice that, generally speaking, our
conclusions from Table 2 regarding the diﬀerent methods (e.g. MCD provides more calibrated UQ than
ensembling and our INR-based methods outperform their classical counterparts) are statistically signiﬁcant
relative to standard error across diﬀerent images.
Table 9:AAPM Diﬀerences : AAPM results from Table 2 are presented again, in terms of diﬀerences from
the average metric value for each image. Speciﬁcally, the table reports the average and standard error of
these diﬀerences across the 8 test images for each metric.
Reconstruction 60-Views 120-Views
Method SNR ( ↑) NLL (↓) ECE (↓) SNR (↑) NLL (↓) ECE (↓)
FBP−13.46±0.46 – – −11.38±0.49 – –
EM−9.57±0.82 – – −9.94±0.37 – –
CGLS −3.96±0.22 – – −3.55±0.26 – –
SIRT−3.15±0.22 – – −3.81±0.40 – –
SART −2.50±0.21 – – −3.72±0.19 – –
GOP-TV 1.93±0.10 – – 1.91±0.07 – –
HMC GOP-TV 1.06±0.13−4.70±0.93−0.004±0.011 1.33±0.12−4.60±0.81−0.007±0.010
INR 3.21±0.09 – – 3.32±0.10 – –
DE-2 UINR 3.25±0.09 22.45±3.41 0.118±0.004 3.34±0.10 17.62±2.30 0.114±0.004
DE-5 UINR 3.26±0.10 6.32±0.93 0.070±0.006 3.34±0.09 7.57±1.29 0.075±0.007
DE-10 UINR 3.25±0.10 4.78±0.93 0.038±0.010 3.33±0.09 5.11±1.20 0.054±0.008
MCD UINR 3.34±0.11−5.55±0.72−0.029±0.005 3.14±0.13−4.98±0.61−0.040±0.006
DE-2 MCD UINR 3.41±0.11−5.68±0.77−0.044±0.004 3.20±0.13−5.05±0.64−0.052±0.006
DE-5 MCD UINR 3.44±0.11−5.76±0.80−0.055±0.005 3.22±0.13−5.11±0.66−0.065±0.007
DE-10 MCD UINR 3.42±0.10−5.79±0.81−0.061±0.004 3.25±0.13−5.32±0.77−0.055±0.005
HMC UINR 3.06±0.09−6.07±0.93−0.033±0.010 3.01±0.11−5.25±0.75−0.023±0.008
G.4 Observed Cold Posterior Type Eﬀect
We observe in Table 2 that there is a decrease in SNR performance of HMC GOP-TV relative to its determin-
isticcounterpartGOP-TV(25.10vs25.97for60views), thelattercorrespondingtothemaximum-a-posterior
(MAP) point estimate for the posterior, Eq. 6, from which we seek to sample.
This evokes similarities with the recently observed “cold-posterior” eﬀect (CPE) in BDL (Wenzel et al.,
2020). In this setting, one considers a tempered posterior pT(f|S)∝exp/parenleftbig−U(f)
T/parenrightbig
, where the Bayes’ posterior
corresponds to temperature T= 1and “cold” posteriors ( T<1) place more weight on parameter regions
54Published in Transactions on Machine Learning Research (04/2023)
of high posterior probability. In the cold limit T → 0+, the tempered posterior becomes exactly the MAP
point estimate.
For context, the tempered version of Eq. 6 for GOP yields:
pT(f|S)∝exp

−1
2σ2T|Φ×R|/summationdisplay
i=1/parenleftbig
Si−Aif/parenrightbig2−λ
TT(f)

, (78)
TheCPEistheobservationthatcolderposteriors( T<1)outperformsthestandardBayes’posterior( T= 1)
in terms of generalization, and understanding its causes has been a topic of recent study in BDL (Wenzel
et al., 2020; Adlam et al., 2020; Aitchison, 2020; Fortuin et al., 2022; Wilson & Izmailov, 2020; Izmailov
et al., 2021; Noci et al., 2021; Nabarro et al., 2022).
Inspired by this line of work, as well as our observation that HMC GOP-TV underperforms MAP GOP-TV
(point estimate) in terms of SNR in Table 2, we studied the eﬀect of tempering posteriors in GOP with HMC
in 27. We ﬁnd a similar eﬀect to the cold-posterior whereby SNR performance is improved at cold temper-
atures, and even allows tempered GOP-HMC to outperform its deterministic MAP counterpart. However,
we also observe that the colder temperatures are detrimental to the metrics which consider UQ: ECE (which
is purely about UQ) and NLL (which considers both UQ and reconstruction accuracy), presumably because
the tempered sample only samples around the MAP estimate with little diversity in samples.
There are three commonly presumed causes for the cold-posterior eﬀect (Noci et al., 2021): data-curation;
data-augmentation; and prior mispeciﬁcation. Given that our observation model (Eq. 5) is constructed
by design, and that we use noisy data exactly matching our likelihood model, we rule out data-curation
(proposed by Aitchison (2020) who argue that the CPE is caused by the fact that standard DL datasets,
e.g. CIFAR-10, are well-curated and that standard likelihoods like cross-entropy do not account for this)
as the cause for our observed eﬀect in 27. Likewise, our CT reconstruction setting is very diﬀerent to
standard DL supervised learning and there is no natural analogue for data-augmentation. This leaves prior
misspeciﬁcation, in the form of the TV-regularization term in Eq. 78, as a potential cause for our observed
cold-posterior like eﬀect in CT reconstruction.
We note that the grid-of-pixels model uses a simple grid-based pixel-by-pixel dimension array of parameters
without any mention of NNs, and enjoys a convex log-posterior compared to the more complex non-convex
parameter loss landscapes that exist in (B)DL. In this sense, to the best of our knowledge 27 represents the
ﬁrst suggestion that a cold-posterior eﬀect can occur outside of NN-based models.
102
101
100
T emperature1.0
0.8
0.6
0.4
0.2
0.00.2Metric value
SNR difference vs MAP GOP-TV ()
MAP GOP-TV
102
101
100
T emperature0255075100125150
NLL ()
102
101
100
T emperature0.1000.1250.1500.1750.2000.2250.250
ECE ()
Figure 27: A cold-posterior like eﬀect observed for GOP-TV with HMC for 60-views AAPM.
G.5 UncertaINR AAPM Test Set Outputs
In our main ﬁgure of results, Figure 3b, we present the model outputs and metrics for only 1 out of the 8
AAPM test set images, as illustrated in Figure 25. We conclude the paper by presenting the ﬁgures for all
55Published in Transactions on Machine Learning Research (04/2023)
test set images. Note that the observed trends and analysis, as presented in main paper Section 4.3, hold
for the remaining, following test set image results.
HMC GOPReconstructed Mean
DE-10 UINR
 MCD UINR
 DE-5 MCD UINR
 DE-10 MCD UINR
 HMC UINR
0.00.20.40.60.81.0
Absolute Error
0.000.010.020.030.040.05
Variance
0.00000.00010.00020.00030.00040.0005Coverage
0.0 0.5 1.0
Target CoverageReliability
0.0 0.5 1.0
Target Coverage
0.0 0.5 1.0
Target Coverage
0.0 0.5 1.0
Target Coverage
0.0 0.5 1.0
Target Coverage
0.0 0.5 1.0
Target Coverage0.00.51.0Achieved Coverage
HMC GOPReconstructed Mean
DE-10 UINR
 MCD UINR
 DE-5 MCD UINR
 DE-10 MCD UINR
 HMC UINR
0.00.20.40.60.81.0
Absolute Error
0.000.010.020.030.040.05
Variance
0.00000.00010.00020.00030.00040.0005Coverage
0.0 0.5 1.0
Target CoverageReliability
0.0 0.5 1.0
Target Coverage
0.0 0.5 1.0
Target Coverage
0.0 0.5 1.0
Target Coverage
0.0 0.5 1.0
Target Coverage
0.0 0.5 1.0
Target Coverage0.00.51.0Achieved Coverage
HMC GOPReconstructed Mean
DE-10 UINR
 MCD UINR
 DE-5 MCD UINR
 DE-10 MCD UINR
 HMC UINR
0.00.20.40.60.81.0
Absolute Error
0.000.010.020.030.040.05
Variance
0.00000.00010.00020.00030.00040.0005Coverage
0.0 0.5 1.0
Target CoverageReliability
0.0 0.5 1.0
Target Coverage
0.0 0.5 1.0
Target Coverage
0.0 0.5 1.0
Target Coverage
0.0 0.5 1.0
Target Coverage
0.0 0.5 1.0
Target Coverage0.00.51.0Achieved Coverage
HMC GOPReconstructed Mean
DE-10 UINR
 MCD UINR
 DE-5 MCD UINR
 DE-10 MCD UINR
 HMC UINR
0.00.20.40.60.81.0
Absolute Error
0.000.010.020.030.040.05
Variance
0.00000.00010.00020.00030.00040.0005Coverage
0.0 0.5 1.0
Target CoverageReliability
0.0 0.5 1.0
Target Coverage
0.0 0.5 1.0
Target Coverage
0.0 0.5 1.0
Target Coverage
0.0 0.5 1.0
Target Coverage
0.0 0.5 1.0
Target Coverage0.00.51.0Achieved Coverage
56Published in Transactions on Machine Learning Research (04/2023)
HMC GOPReconstructed Mean
DE-10 UINR
 MCD UINR
 DE-5 MCD UINR
 DE-10 MCD UINR
 HMC UINR
0.00.20.40.60.81.0
Absolute Error
0.000.010.020.030.040.05
Variance
0.00000.00010.00020.00030.00040.0005Coverage
0.0 0.5 1.0
Target CoverageReliability
0.0 0.5 1.0
Target Coverage
0.0 0.5 1.0
Target Coverage
0.0 0.5 1.0
Target Coverage
0.0 0.5 1.0
Target Coverage
0.0 0.5 1.0
Target Coverage0.00.51.0Achieved Coverage
HMC GOPReconstructed Mean
DE-10 UINR
 MCD UINR
 DE-5 MCD UINR
 DE-10 MCD UINR
 HMC UINR
0.00.20.40.60.81.0
Absolute Error
0.000.010.020.030.040.05
Variance
0.00000.00010.00020.00030.00040.0005Coverage
0.0 0.5 1.0
Target CoverageReliability
0.0 0.5 1.0
Target Coverage
0.0 0.5 1.0
Target Coverage
0.0 0.5 1.0
Target Coverage
0.0 0.5 1.0
Target Coverage
0.0 0.5 1.0
Target Coverage0.00.51.0Achieved Coverage
HMC GOPReconstructed Mean
DE-10 UINR
 MCD UINR
 DE-5 MCD UINR
 DE-10 MCD UINR
 HMC UINR
0.00.20.40.60.81.0
Absolute Error
0.000.010.020.030.040.05
Variance
0.00000.00010.00020.00030.00040.0005Coverage
0.0 0.5 1.0
Target CoverageReliability
0.0 0.5 1.0
Target Coverage
0.0 0.5 1.0
Target Coverage
0.0 0.5 1.0
Target Coverage
0.0 0.5 1.0
Target Coverage
0.0 0.5 1.0
Target Coverage0.00.51.0Achieved Coverage
HMC GOPReconstructed Mean
DE-10 UINR
 MCD UINR
 DE-5 MCD UINR
 DE-10 MCD UINR
 HMC UINR
0.00.20.40.60.81.0
Absolute Error
0.000.010.020.030.040.05
Variance
0.00000.00010.00020.00030.00040.0005Coverage
0.0 0.5 1.0
Target CoverageReliability
0.0 0.5 1.0
Target Coverage
0.0 0.5 1.0
Target Coverage
0.0 0.5 1.0
Target Coverage
0.0 0.5 1.0
Target Coverage
0.0 0.5 1.0
Target Coverage0.00.51.0Achieved Coverage
57