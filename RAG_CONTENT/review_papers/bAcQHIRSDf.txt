Under review as submission to TMLR
A Theoretical Insight of Histogram Binning and Extending
to Multi-label Classification
Anonymous authors
Paper under double-blind review
Abstract
Learning well-calibrated probabilistic predictions is crucial as neural networks and machine
learning models are increasingly employed in critical tasks nowadays. While there exist
several post-processing methods aimed at calibrating output probabilities, most lack proper
theoretical justification; in other words, they have typically only been validated on lim-
ited datasets and models to report empirical results. This work is divided into two parts.
In the first part, we analyze some post-processing calibration methods from a geometrical
perspective and demonstrate that calibrated outcomes consistently reduce Expected Cali-
bration Error (ECE) while increasing accuracy. In the second part, we present a previously
unexplored framework for calibrating the outcomes of multi-label problems by addressing
multiple binary calibration problems. To achieve this, we introduce a novel concept of ECE
for multi-label problems and provide substantial theoretical rationale for our approach. Ex-
perimental results demonstrate the feasibility and efficacy of our method in practice.
1 Introduction
Deep learning methods have dramatically improved prediction accuracy. This leads the usage of these
models in complex decision making tasks like medical image processing(Litjens et al., 2017; Shen et al.,
2017), disease diagnosis(Caruana et al., 2015; Jiang et al., 2012), autonomous driving(Bojarski et al., 2016)
etc and Neural network is an essential part of these systems. Though the ability of neural networks has
increased in last decade, it is essential for a model to have the ability to indicate when the prediction
is incorrect. The confidence for each prediction does this job for us- the higher the confidence the
more we can trust the result of the model. For intriguing tasks mentioned above confidence plays a
key role. As for example we can think of self-driven cars(Bojarski et al., 2016; Tian et al., 2018) which
uses neural network based object detection module to determine whether there is some pedestrian or
some blockage in front of the car. If the module cannot predict these with a very high confidence then
the car must rely on other sensors for braking. Unfortunately, for many modern deep learning models
the algorithms mainly focuses on accuracy rather than producing well calibrated results. In fact it is
very surprising that modern neural networks have become less calibrated(Guo et al., 2017) though their
accuracy has increased. (Guo et al., 2017) has showed that a 5 layer Le-Net(LeCun et al., 1998) is
more calibrated than a 110 layer Res-Net(He et al., 2016). On the other hand Res-Net is more accu-
rate than Le-Net. This also establishes an intuition that accuracy is independent or orthogonal to confidence.
Several post processing calibration methods are used widely to calibrate the outputs of the neural networks
and other machine learning models. These methods can be divided mostly into two parts: Parametric and
Non-parametric. An example of parametric method is Platt’s scaling(Platt et al., 1999) which applies a
sigmoidal transformation to predictive probabilities to give more calibrated output. Some Non-parametric
method consists Histogram Binning(Zadrozny & Elkan, 2001), Isotonic Regression(Zadrozny & Elkan, 2002)
etc. In Histogram Binning the outputs of the Binary Classifier are sorted then they are divided into B
numbers of equally spaced bins and a binning parameter( θmfor binBm) is assigned to each and every bin.
The predictive probability now becomes θmif it falls into the bin Bm.
1Under review as submission to TMLR
Despite these methods give better calibrated results in a few studies(Naeini et al., 2015; Zadrozny & Elkan,
2001), we pointed out that these results are mostly empirical, based on some very well known data sets
and models. In the following sections we will see that Histogram Binning changes the output probability
to give us calibrated results. But there is no theoretical background which suggests that calibrating using
Histogram Binning will surely give us better calibrated and more accurate results(for any data sets and
models). We find this question very important as these methods can be used to calibrate the outcomes of
very delicate tasks.
Although traditional neural networks and supervised learning algorithms work successfully for multi-class
classification, the real world is much more complicated and the assumption of only one class label associated
with each input does not fit well. In most of the real world objects there will be multiple semantics
associated with it. To name a few, a single medical image(Bustos et al., 2020) is related to multiple
radiographic findings; In text classification(Minaee et al., 2021) one single piece of text might cover different
genres like thriller, drama, romance; In self driving cars there will be several different labels like pedestrians,
roads, humans are often present in one single frame. To tackle multiple semantics of a single object we
resort to multi-label classification methods. Medical Image analysis, Self Driving cars etc are very delicate
multi-label classification task for which precise confidence of the classifiers are needed keenly. Unfortunately,
there are not much studies with proper theoretical justification regarding calibration of multi-label problems.
This study presents a theoretical rationale of Histogram Binning along with a framework to apply this
method to calibrate multi-label classification problems. Our approach to Histogram Binning is from a
geometric and functional point of view. Usage of Large Sample Theory endorses the validity of this method.
For multi-label classification framework, firstly two metrics are proposed to measure the Total and Average
Expected Calibration Error(ECE) as ECE(Naeini et al., 2015) proposed in previous works is not designed to
measure the calibration error for Multi-Label classification problems. Then we have proposed a previously
unexplored method which combines multiple binary Histogram Binning problems to work for multi-label
problems. Theoretical rationale behind our method is also provided along with. The experimental results
presented for multi-label problems endorses our claims and verifies that out method works well in practice.
2 Definitions and Background
LetX={X1,...,Xn}is the set of input data and the set of output labels are Y={1,2,...,K}.
Both of them are random variables following a joint distribution which can be written by using
Bayes Rule f(X,Y ) =f(Y|X)f(X), whereX∈ XandY∈ Y. Assume that his a Neural Net-
work(NN) such that h:X → RKandσSMis the softmax function. Define σSM:RK→ PKwhere
PK={(p1,...,pK)∈RK
+|p1+···+pK= 1}and ˆy(Xi) = arg max 1≤k≤K{σSM(h(Xi))k}. An example
of perfect calibration can be seen in a weather prediction model forecasting 80%chance of rain next day
consistently throughout the year, resulting in rain occurring exactly 80%of the days during the time period.
In short, whenever predicted class is ˆYand predicted probability is ˆPfor a neural network h, perfect
calibration implies Ph(ˆY=y|ˆP=p) =p. As we can see from the definition, with finite number of sample
points it is impossible to calculate so we resort to empirical approaches.
2.1 Expected Calibration Error
Let us define ˆy1,..., ˆynas the predicted labels and ˆp1,..., ˆpnbe the predictive probabilities of a binary
classification problem. ˆpi’s of such NNs is the confidence of ithinstance to belong to the positive class.
We will assume that these are sorted in ascending order, that means ˆp1≤...≤ˆpn. The total number of
bins in between [0,1]is M. If there are nmnumber of output probabilities inside the bin number mthen
n=/summationtextM
j=1nj. For binm, the predicted probabilities are defined by ˆpk
mand the corresponding response
byˆyk
mfork= 1,2,...nm. The responses are independent of each other. Under the assumption of null
hypothesis which is perfect calibration, we can say that ˆyk
mis the outcome of a Bernoulli trial with an
2Under review as submission to TMLR
expected value of ˆpk
m. We sort the predicted probabilities such that ˆpi
m≤ˆpj
mwheni≤jandˆpl
m1≤ˆpl
m2
whenm1<m 2. For convenience we define ˆp1
0= 0;ˆp1
m+1= 1.
To define the confidence of a bin Bm, we take the average of all predicted probabilities lying inside that bin.
Then we also define the fraction of positive instances( posm) and the expected accuracy of the mthbin.
confm=1
nmnm/summationdisplay
i=1pi
m
posm=1
nmnm/summationdisplay
i=11(ˆyi
m= 1)
and
accm=1
nmnm/summationdisplay
i=11(ˆyi
m=yi
m)
Expected Calibration Error(ECE) metric is commonly used to measure the calibration error, first proposed
by (Naeini et al., 2015) to calibrate binary classification problems. ECE measures the discrepancy between
the fraction of positive instances and confidence of a model thus captures the miscalibration. To compute this
measure predicted probabilities are sorted into Mequally spaced bins, Bm, each of size1
M. The empirical
estimate can be written as:
ECE =M/summationdisplay
m=1nm
n|posm−confm|. (1)
3 Trustworthiness of calibration
We are going to use calibrated results in very critical tasks like medical image classifcation, disease
detection, self driving cars etc. as a well calibrated NNs should reflect the true confidence of the prediction
being correct. But till now only a few data sets were used to validate the methods of calibration. In
real-world scenarios where data sets can be intricate, it’s pertinent to question whether calibration methods
consistently yield improved calibration while maintaining overall accuracy, or if they might occasionally
compromise the performance. In this work we will analyze a model agnostic method Histogram Binning(HB)
and show analytically with some minor assumptions that HB does not compromise the performance of NNs.
3.1 Histogram Binning
Histogram Binning(Zadrozny & Elkan, 2001) uses bins to calibrate the outcomes of a NN. Probabilities
predicted by NN, ˆpis are sorted and divided into Mmutually exclusive bins namely B1,...,BM. Each
bin is assigned a score say θmfor binm. Whenever the predicted probability ˆptefor a test input falls
inside the bin Bm, the calibrated probability ˆqtetakes the value θm. The bin edges can be defined as
0 =a1<...<a M+1= 1henceBm= (am,am+1]. The bins can be chosen such that they are equally spaced
or there are equal numbers of points inside each bin. In this work we have considered equally spaced bins.
As the bins are equally spaced, the mthbin is going to be Bm= (m−1
M,m
M]. To choose the scores for every
bin we will resort to the following equation:
(θ1,...,θM) = arg min
θ1,...,θMM/summationdisplay
m=1n/summationdisplay
i=11(am≤ˆpi≤am+1)(θm−yi)2(2)
3Under review as submission to TMLR
whereyi∈{0,1}is the ground truth of ithinput.
Below we will state and prove a theorem in which we will see association of HB with the scores of each bin.
Theorem 1. The binning scores are the ratio of number of positive classes and the total number of instances
in the bin. In short, θm=n(1)
m
nmwheren(1)
mis the number of positive instances inside the bin m.
Proof.The equation 2 is sum of squares so the right hand side cannot be negative. So we have
M/summationdisplay
m=1n/summationdisplay
i=11(am≤ˆpi≤am+1)(θm−yi)2= 0
By differentiation we get
nm/summationdisplay
i=1(θm−yi) = 0 (3)
yi= 0 or 1. From equation 3 it can be inferred that θm=n(1)
m
nm.
From this theorem we get to see that the scores actually corresponds to the amount of positive instances
in a particular bin. But it is still questionable whether these calibrated scores are actually accurate or
not or whether we can trust these new predicted probabilities over the uncalibrated outputs produced by
neural networks because the results in previous studies are all empirical. These issues are not yet addressed
analytically and hinders a user to use the calibrated probabilities produced by HB safely. In the next lemma
and theorem we will see that HB always produces outputs which are better calibrated and the accuracy is
also maintained.
Lemma 1. Let(X,Y)be a binary classification dataset, where Yi∈{0,1}∀Yi∈Yand a neural network
h:X→R. LetσSGbe a sigmoid function and the interval [0,1]is divided into Mequally spaced bins; Imbe
themthbin such that Bm= (m−1
M,m
M]. Assume that his injective function. Let Cmbe the cluster centre of all
pointsXi∈Xsuch thatσSG(h(Xi))∈Bm. Then we can say that for new instances X,X′ifσSG(h(X))∈
BmandσSG(h(X′))∈Bj; thend(Cm,X)<d(Cm,X′)for allj∈{1,2,...,m−1,m+ 1,...,M}whered
is the distance metric.
Proof.his an injective(if we use any non-polynomial analytic activation function then we have that
neural network is injective and some other conditions of injectivity is described in (Puthawala et al.,
2022)) function, thus σSG◦h:X → [0,1]is injective too. Let fnn=σSG◦h. For any new instance X,
fnn(X)∈Bm=⇒X∈f−1
nn(Bm);Bmis a convex set. We need to show that f−1
nn(Bm)is a convex set.
Let, x,y∈f−1
nn(Bm). Then αfnn(x) + (1−α)fnn(y)∈Bmfor anyα∈[0,1]. So
f−1
nn(αfnn(x) + (1−α)fnn(y))∈f−1
nn(Bm). Asfnnis injective and continuous so by the property of
inverse mapping αx+ (1−α)y∈f−1
nn(Bm). So,Sm=f−1
nn(Bm)andSj=f−1
nn(Bj)are disjoint convex sets.
AsSmandSjare disjoint convex sets, by Hyperplane Separation Theorem there exists a non-zero vector v
andc∈Rsuch that⟨x,v⟩≥cand⟨y,v⟩≤cfor allx∈Smandy∈Sj. Now,X′∈Sjwhich is outside of
Smand between SmandSjthere is hyperplane that is separating these two. So d(Cm,X)<d(Cm,X′).
The consequence of the lemma 1 is that the inputs for which the neural network produces the output
probability within same interval are tend to be similar than the points for which the output probability lies
outside the specific interval. So, for any input if the output probability lies inside the said interval then the
points are expected to have properties more similar to the cluster centre of inverse image of that interval
than the cluster centre of the inverse image of the other intervals.
4Under review as submission to TMLR
Theorem 2. A calibrated probability obtained by solving the equation 2 is always going to be more well
calibrated and accurate compared to uncalibrated outputs.
Proof.Xbe a input and fnn(X)∈Bm. We have θm=n(1)
m
nm.limnm→∞θm= Θmby the Law of Large
Numbers. Θmis representing the true fraction of samples to be of positive class for very large nm. So for
anyX∈f−1
nn(Bm), randomly chosen(this also represent any element used as a input of the neural network)
the output probability ideally should be Θm. In this way it is actually representing the true probability of
being an element belong to the positive class. So, for bin Im;|posm−confm|>|posm−Θm|making it
better calibrated.
IfXte∈f−1
nn(Bm)thenXtewill be assigned to the positive class with probability θm. By lemma 2
d(Xte,Cm)<d(Xte,Cj)implies that Xtewill be more similar to the points in f−1
nn(Bm). In case of wrongly
classified points to the zero(positive) class but pretty high(low) θmsuggests that for most of the data points
inf−1
nn(Bm)the true class will be the positive(zero) class. So the accuracy will increase too.
4 Calibration for Multi-class Classification(MCC)
Now we will present a version of the histogram binning method for multi-class classification by mainly
following (Zadrozny & Elkan, 2002). Let h:X→RKbe a neural network where Kis the number of classes.
σSMbe the softmax function as defined previously. As the output of every class label will be in between
[0,1], we will divide the outputs of every class label into Mequally spaces bins. The binning parameter
of themthbin of the ithclass isθ(i)
m. If the output probability of ithclass of the jthinput,p(i)
j, falls
inside the bin B(i)
mthen calibrated probability q(i)
j=θ(i)
m. The calibrated output probability is going to be
{q(1)
j,...,q(K)
j}. But/summationtextK
l=1q(l)
j̸= 1in general. So, after normalization the output probability is going to be
{q(1)
j/summationtextK
l=1q(l)
j,...,q(K)
j/summationtextK
l=1q(l)
j}and the confidence of the correct class is going to be max{q(1)
j/summationtextK
l=1q(l)
j,...,q(K)
j/summationtextK
l=1q(l)
j}.
5 Calibration for Multi-label Classification(MLC) Problems
Multi-label classification is a machine learning task where a classifier learns to associate multiple labels to
an output instead of a single one. For label set /tildewideY={1,...,K}and an input /tildewideX∈/tildewideX, the classifier tries to
predict ˜ y⊆/tildewideY.˜ ycan be any subset of /tildewideYin case of multi-label classification.
Let˜hbe a neural network for multi-label classification such that ˜h:/tildewideX →RKandσSGbe the sigmoid
function. Now, ˜h= (˜h1,..., ˜hK)where ˜h1:/tildewideX → Rand ˜p(/tildewideX) ={˜p1(/tildewideX),..., ˜pK(/tildewideX)}where ˜pl(/tildewideX) =
σSG(˜hl(/tildewideX)). Ifˆ˜ y⊆/tildewideYis the predicted set of labels for the input /tildewideXthenl∈ˆ˜ yif˜hl(/tildewideX)>0. In this case
of multi-label classification, each of ˜hl(/tildewideX)can be thought of as an individual neural network involved in
predicting whether a particular label lis in the input. In other words, Multi-label classification consists
of multiple binary classifiers and the predicted label is going to be a multi-hot encoded vector instead of a
one-hot-encoded vector in case of multi-class classification.
5.1 Calibrating the Probabilities
In multi-label classification, predicting each label is a task of one binary classifier. So we will calibrate each
of these classifiers separately and define two types of ECE. We will produce first results of calibrating a
multi-label classifier in this way. Also we will provide a theoretical insight and produce the first theoretical
justification for calibrating the multi-label classification.
5Under review as submission to TMLR
5.2 Calibrating with Histogram Binning
We will treat the multi-label problem as multiple binary classification problem. For this we need to divide
each binary output to Mequally spaced bins. Let (/tildewideXj,/tildewideYj)n
j=1be data points and
Yi= (Y(1)
i,...,Y(K)
i); Y(l)
i=/braceleftigg
1,ifl∈/tildewideYi
0,o.w.
To calibrate the NNs for multi-label classification problems we will calibrate each ˜hl,l∈{1,...,K}sepa-
rately. We assume that all labels are independent so calibrating them in this way will not cause any loss
of information. The uncalibrated predicted probabilities related to ˜hl,ˆp(l)
i, are divided into Mlmutually
exclusive bins{B(l)
1,...,B(l)
Ml}. The bin edges for label lis defined as 0 =a(l)
1< a(l)
2< ... < a(l)
Ml+1= 1.
The bin edges can be chosen different for different labels but in this work we have considered equal length
and equal number of bins for all labels. For each bin in each label, B(l)
m, a binning score θ(l)
mis assigned.
For a new prediction ˆp(l)
te∈B(l)
m, the calibrated outcome ˆq(l)
te=θ(l)
m.θ(l)
mis chosen to minimize the following
equation:
(θ(l)
1,...,θ(l)
M) = arg min
θ(l)
1,...θ(l)
MM/summationdisplay
m=1n/summationdisplay
i=11(a(l)
m≤ˆp(l)
i≤a(l)
m+1)(θ(l)
m−Y(l)
i)2(4)
We found that the ECE metric in equation1 that is orginally defined by (Naeini et al., 2015) is only for
calibrating binary classification problems; not suitable for multi-label problems. Hence, we define the two
measures of ECE for multi-label problems calculating total and weighted average of calibration error denoted
byECEtotandECEwavgrespectively. Mathematically they are defined as follows:
ECEtot=K/summationdisplay
l=1M/summationdisplay
m=1nml
n|pos(l)
m−confidence(l)
m| (5)
ECEwavg =1
nK/summationdisplay
l=1/parenleftbigg
(n/summationdisplay
i=1Y(l)
i)·(M/summationdisplay
m=1nml
n|pos(l)
m−confidence(l)
m|)/parenrightbigg
(6)
pos(l)
mandconfidence(l)
misthenumberofpositiveinstancesandconfidenceof lthlabelforwhichthepredictive
probabilities falls under mthbin. Equation 5 describes the total ECE of all labels. This notion is useful as we
have used binary relevance in our multi-label classification problem and the labels are mutually independent.
In equation 6 we have chosen weighted average instead of simple average because there might be a scarcity
of particular label in the inputs. This kind of setting is quite common in various real world situations. If
there are not much presence of sample for a label in the inputs, calibrating that particular label might cause
a big change in calibration which from our point of view does not reflect the truth. This might be more
problematic if there are very less number of labels present(say 3-5). Weighted average of ECE mitigates the
problem by multiplying weights(fractions of each label present in the input) to every label.
Theorem 3. The binning score for mthbin at label l,θl
m, is going to ben(1)
ml
nml, wheren(1)
ml=|D(1)
ml|;D(1)
ml=
{/tildewideYi|l∈/tildewideYiandam≤ˆpl
i≤am+1}andnml=|Dml|;Dml={/tildewideYi|am≤ˆp(l)
i≤am+1}.
We are going to present a generalized version of lemma 1 for the multi-label classification. First we will
prove it for each individual binary classifier then we will extend it to the multi-label problems.
Lemma 2. Let(/tildewideX,Y)be a multi-label dataset where Yi∈{0,1}K∀Yi∈Y. The interval [0,1]is divided
intomequally spaced bins B(l)
m= (m−1
M,m
M]for each label l. Assume that ˜h:/tildewideX→RKis a neural network
which is injective. Let C(l)
mbe the cluster centre of all points /tildewideXi∈/tildewideXsuch thatσSG(˜hl(/tildewiderXi))∈B(l)
m. It can
be said that for any new instances /tildewideX,/tildewiderX′ifσSG(˜hl(/tildewideX))∈B(l)
mandσSG(˜hl(/tildewiderX′))∈B(l)
jrespectively, then
d(C(l)
m,/tildewideX)<d(C(l)
m,/tildewiderX′)for anyj∈[M]\mwheredis a distance metric.
6Under review as submission to TMLR
Proof.Letf(l)
nn=σSG◦˜hl. Hencef(l)
nnis a continuous and injective function. B(l)
mis a convex set. By
the properties of continuous and injective function, S(l)
m=f(l)
nn−1(B(l)
m)andS(l)
j=f(l)
nn−1(B(l)
j)are disjoint
convex sets. C(l)
mbe the cluster centre of the set Sl
m
As the sets are convex and disjoint, again by Hyperplane Separation Theorem, the existence of a hyperplane
betweenS(l)
mandS(l)
jguarantees that d(C(l)
m,/tildewideX)<d(C(l)
j,/tildewideX′).
With this lemma we are now ready to show the validity of our extension theoretically. Experimental results
will be provided to endorse our claims.
Theorem 4. For any given neural network and training set, the calibrated probabilities of label ‘l‘obtained by
solving the equation 4 is going to be better calibrated and more or equally accurate compared to an uncalibrated
one.
Proof.Let/tildewideXbe input to the model and fnn(/tildewideX∈B(l)
m. By the Law of Large numbers θ(l)
mnm→∞−−−−−→ Θ(l)
m.
Θ(l)
mis representing the true fraction of inputs that belongs to the positive class for the interval I(l)
mfor
very largenmlideally. For anyX∈fnn−1(B(l)
m), the output probability should ideally be Θl
m. Hence, for
binB(l)
m;|pos(l)
m−Θ(l)
m|<|pos(l)
m−confidence(l)
m|. Byusingtheineqalityin7and8wehaveprovedourclaim.
LetXte∈fnn−1(B(l)
m)then Lemma 2 implies that Xtewill be more similar to the points in B(l)
m. In case
of wrongly classified points to zero class and very high θ(l)
mthe correct class is more probable to be actually
positive which indicates the increase in accuracy.
So we have established some genralized results which guarantees the effectiveness of Histogram Binning for
both multi-label and multi-class problems. Theorem 4 is an extended theorem of calibration in case of
multi-label classification problems. We have proved that under some conditions the calibrated probabilities
obtained by using Histogram Binning are going to give us better estimates. Also this result makes the
calibrated estimates trustworthy.
6 Experiments and Results
This section describes some experiments that we have performed to check the validation of our theory in
case of Multi-label image and text datasets. We have used ECEtotandECEwavgfor multi-label calibration
as a measure to check the calibration error. To check the accuracy we have used Hamming Loss(HL) which
is described below:
Hamming Loss: IfˆYiandYibe the predicted labels and true labels respectively for the ithinput, then
the hamming loss is defined as:
HL=1
NLN/summationdisplay
l=1N/summationdisplay
i=1ˆY(l)
i⊕Y(l)
i (7)
Datasets
We have used two image datasets and one text dataset mainly for our experiments. The first one is a
dataset(shr) from Kaggle, publicly available for multi-label classification problems. There are 7843images in
total and each image has one or more than one label among 10labels. The dataset is originally developed to
find the personality of people based on the pictures they share in facebook and other social media. Another
is the famous PASCAL-VOC(Everingham et al.) dataset which consista several images from flicker mainly.
It has in total 20labels and each and every image contains one or more number of labels. The third one is
7Under review as submission to TMLR
Dataset and
ModelUncalibrated Calibrated
ECEwavgECEtotECEwavgECEtot
Kaggle Image
Dataset;
ResNet 500.0406 0.20 0.014 0.12
±0.0052±0.02±0.0024±0.008
PASCAL-
VOC;
ResNet 500.03819 0.2851 .22.207
±0.0048±0.016±0.0169±0.017
Propaganda
PRC;
BERT0.037 0.202 0.0214 0.11
±0.0052±0.07±0.00152±0.009
Table 1:ECEwavgandECEtotforM= 10bins for kaggle image dataset, PASCAL-VOC dataset and
propaganda PRC dataset before and after calibration
Dataset of Propaganda Techniques of the State-Sponsored Information Operation of the People’s Republic of
China(PTSI-PRC). This dataset consists 9950tweets in Mandarin with 20different propaganda techniques.
We have used several randomized partitions for each of the dataset in our experiments to get the variability
of calibration measures.
To conduct experiments on image datasets we have used ResNet50 CNN, a variant of ResNet with 50
layers. As it has high performance in ImageNet competition, we decided to choose this network as our
backbone in our experiments. We have tweaked the model a bit to use it in multi-label classification.
The softmax layer is removed and instead of that sigmoid function is used to represent probability of
presence of each label in an input. Then the output is calibrated with the calibrating functions and
ECE is measured for both uncalibrarted and calibrated datasets. This procedure is followed in all of our
experiments which contains Images. To conduct our experiment in text datasets we have used BERT(Devlin
et al., 2018) model as in (Chang et al., 2021), an open source framework for different tasks in NLP.
BERT enhances machine understanding of ambiguous language by leveraging surrounding texts. All of the
calibration related results are presented in the table 1. The code for PASCAL-VOC can be found here:
https://github.com/drunksailors/multi-label-calibration
It can be seen that our experimental result is on par with our theoretical results. For all three datasets
that are presented here, both ECEwavgandECEtotare decreasing significantly after calibration. Our
method works for text data as well. In theorem 4 we have claimed that the accuracy will improve along with
better calibrated results. To endorse our claim of betterment of accuracy, we used the same datasets and
calculated the Hamming Loss to measure the discrepancy between predicted and actual labels before and
after calibration. The results in table 2 establishes out claim. So, we have devised a method for calibration
of Multi-label problems which not only increases the calibration also does not hamper the accuracy.
7 Conclusion
In this work we have provided theoretical insight of using Histogram Binning. We have proved that His-
togram Binning will always give us better calibration and more accurate results rather than an uncalibrated
one. Also a framework for calibrating multi-label classification problems is provided along with theoretical
rationale. We have presented experimental results which endorses that our theoretical formulation will work
in practice.
In this work the bin numbers are fixed at M= 10. One future direction of research can be to be able to
choose the optimal number of bins and types of bins(equally spaced or equal frequency or any other type).
Bayesian Binning Quantile(Naeini et al., 2015) can be extended to calibrate multi-label problems. Also some
train time methods(Hebbalaguppe et al., 2022; Müller et al., 2019) can be combined with the model agnostic
post-hoc methods such as ours to improve the calibration further more. Our assumption was neural networks
8Under review as submission to TMLR
Dataset and
modelUncalibrated Calibrated
Kaggle Image
Dataset;
ResNet 500.0379 0.03552
PASCAL-
VOC;
ResNet 500.0711 0.07
Propaganda
PRC;
ResNet 500.0242 0.0229
Table 2: Hamming Loss before and after calibration
are injective in out theoretical results. In future one can try to exclude the condition of injecticvity and try
to prove more general results by showing the convergence of binning scores.
References
Multi label image classification dataset. URL https://www.kaggle.com/dsv/3943866 .
Mariusz Bojarski, Davide Del Testa, Daniel Dworakowski, Bernhard Firner, Beat Flepp, Prasoon Goyal,
Lawrence D Jackel, Mathew Monfort, Urs Muller, Jiakai Zhang, et al. End to end learning for self-driving
cars.arXiv preprint arXiv:1604.07316 , 2016.
Aurelia Bustos, Antonio Pertusa, Jose-Maria Salinas, and Maria De La Iglesia-Vaya. Padchest: A large chest
x-ray image dataset with multi-label annotated reports. Medical image analysis , 66:101797, 2020.
Rich Caruana, Yin Lou, Johannes Gehrke, Paul Koch, Marc Sturm, and Noemie Elhadad. Intelligible models
for healthcare: Predicting pneumonia risk and hospital 30-day readmission. In Proceedings of the 21th
ACM SIGKDD international conference on knowledge discovery and data mining , pp. 1721–1730, 2015.
Rong-Ching Chang, Chun-Ming Lai, Kai-Lai Chang, and Chu-Hsing Lin. Dataset of propaganda tech-
niques of the state-sponsored information operation of the people’s republic of china. arXiv preprint
arXiv:2106.07544 , 2021.
JacobDevlin,Ming-WeiChang,KentonLee,andKristinaToutanova. Bert: Pre-trainingofdeepbidirectional
transformers for language understanding. arXiv preprint arXiv:1810.04805 , 2018.
M. Everingham, L. Van Gool, C. K. I. Williams, J. Winn, and A. Zisserman. The
PASCAL Visual Object Classes Challenge 2012 (VOC2012) Results. http://www.pascal-
network.org/challenges/VOC/voc2012/workshop/index.html.
Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q Weinberger. On calibration of modern neural networks. In
International conference on machine learning , pp. 1321–1330. PMLR, 2017.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In
Proceedings of the IEEE conference on computer vision and pattern recognition , pp. 770–778, 2016.
Ramya Hebbalaguppe, Jatin Prakash, Neelabh Madan, and Chetan Arora. A stitch in time saves nine: A
train-time regularizing loss for improved neural network calibration. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition , pp. 16081–16090, 2022.
XiaoqianJiang, MelanieOsl, JihoonKim, andLucilaOhno-Machado. Calibratingpredictivemodelestimates
to support personalized medicine. Journal of the American Medical Informatics Association , 19(2):263–
274, 2012.
9Under review as submission to TMLR
Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to docu-
ment recognition. Proceedings of the IEEE , 86(11):2278–2324, 1998.
Geert Litjens, Thijs Kooi, Babak Ehteshami Bejnordi, Arnaud Arindra Adiyoso Setio, Francesco Ciompi,
Mohsen Ghafoorian, Jeroen Awm Van Der Laak, Bram Van Ginneken, and Clara I Sánchez. A survey on
deep learning in medical image analysis. Medical image analysis , 42:60–88, 2017.
Shervin Minaee, Nal Kalchbrenner, Erik Cambria, Narjes Nikzad, Meysam Chenaghlu, and Jianfeng Gao.
Deep learning–based text classification: a comprehensive review. ACM computing surveys (CSUR) , 54(3):
1–40, 2021.
Rafael Müller, Simon Kornblith, and Geoffrey E Hinton. When does label smoothing help? Advances in
neural information processing systems , 32, 2019.
Mahdi Pakdaman Naeini, Gregory Cooper, and Milos Hauskrecht. Obtaining well calibrated probabilities
using bayesian binning. In Proceedings of the AAAI conference on artificial intelligence , volume 29, 2015.
John Platt et al. Probabilistic outputs for support vector machines and comparisons to regularized likelihood
methods. Advances in large margin classifiers , 10(3):61–74, 1999.
Michael Puthawala, Konik Kothari, Matti Lassas, Ivan Dokmanić, and Maarten De Hoop. Globally injective
relu networks. Journal of Machine Learning Research , 23(105):1–55, 2022.
Dinggang Shen, Guorong Wu, and Heung-Il Suk. Deep learning in medical image analysis. Annual review
of biomedical engineering , 19:221–248, 2017.
Yuchi Tian, Kexin Pei, Suman Jana, and Baishakhi Ray. Deeptest: Automated testing of deep-neural-
network-driven autonomous cars. In Proceedings of the 40th international conference on software engi-
neering, pp. 303–314, 2018.
Bianca Zadrozny and Charles Elkan. Obtaining calibrated probability estimates from decision trees and
naive bayesian classifiers. In Icml, volume 1, pp. 609–616, 2001.
Bianca Zadrozny and Charles Elkan. Transforming classifier scores into accurate multiclass probability
estimates. In Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery
and data mining , pp. 694–699, 2002.
10