Under review as submission to TMLR
Variance Reduced Smoothed Functional REINFORCE Policy
Gradient Algorithms
Anonymous authors
Paper under double-blind review
Abstract
We revisit the REINFORCE policy gradient algorithm from the literature. This algorithm
typically works with reward (or cost) returns obtained over episodes or trajectories. We pro-
pose a major enhancement to the basic algorithm where we estimate the policy gradient us-
ing a smoothed functional (random perturbation) gradient estimator requiring one function
measurement over a perturbed parameter. Subsequently, we also propose a two-simulation
counterpart of the algorithm that has lower estimator bias. Like REINFORCE, our algo-
rithms are trajectory-based Monte-Carlo schemes and usually suffer from high variance. To
handle this issue, we propose two independent enhancements to the basic scheme: (i) use the
sign of the increment instead of the original (full) increment that results in smoother albeit
possibly slower convergence and (ii) use clipped costs or rewards as proposed in the Prox-
imal Policy Optimization (PPO) based scheme. We analyze the asymptotic convergence
of the algorithm in the one-simulation case as well as the case where signed updates are
used and briefly discuss the changes in analysis when two-simulation estimators are used.
Finally, we show the results of several experiments on various Grid-World settings wherein
we compare the performance of the various algorithms with REINFORCE as well as PPO
and observe that both our one and two simulation SF algorithms show better performance
than these algorithms. Further, the versions of these algorithms with clipped gradients and
signed updates show good performance with lower variance.
Key Words: REINFORCE policy gradient algorithm, smoothed functional gradient esti-
mation, one and two simulation algorithms, stochastic gradient search, stochastic shortest
path Markov decision processes, signed updates, objective function clipping.
1 Introduction
Policy gradient methods, see Sutton & Barto (2018); Sutton et al. (1999), are a popular class of approaches
in reinforcement learning (Bertsekas (2019); Meyn (2022)). Randomized policy is normally used in these
approaches that is however parameterized and one updates the policy parameter along the gradient search
direction. The policy gradient theorem, cf. Sutton et al. (1999); Marbach & Tsitsiklis (2001); Cao (2007),
which is a fundamental result in these approaches relies on an interchange of the gradient and expectation
operators and in such cases turns out to be the expectation of the gradient of noisy performance functions
much like the earlier studied perturbation analysis based sensitivity approaches for simulation optimization,
see Ho & Cao (1991); Chong & Ramadge (1993).
The REINFORCE algorithm, cf. Williams (1992); Sutton & Barto (2018) is a noisy gradient scheme for
which the expectation of the gradient is the policy gradient, i.e., the gradient of the expected objective
w.r.t. the policy parameters. The updates of the policy parameter are however obtained once after the full
return on an episode has been found. Actor-critic algorithms, see Sutton & Barto (2018); Konda & Borkar
(1999); Konda & Tsitsiklis (2003); Bhatnagar et al. (2009; 2007), have been presented in the literature as
alternatives to the REINFORCE algorithm as they perform incremental parameter updates at every instant
but do so using two-timescale stochastic approximation algorithms.
1Under review as submission to TMLR
In this paper, we revisit the REINFORCE algorithm and present new algorithms for the case of episodic
tasks, also referred to as the stochastic shortest path setting. Our algorithms perform parameter updates
upon termination of episodes, that is when the goal or terminal states are reached. As with REINFORCE,
parameter updates are performed only at instants of visit to a prescribed recurrent state, see Cao (2007);
Marbach & Tsitsiklis (2001). Our first algorithm is based on a single function measurement or simulation
at a perturbed parameter value where the perturbations are obtained using independent Gaussian random
variates. The problem, however, is that it suffers from a large bias in the gradient estimator. We show
analytically the reason for the large bias here. Subsequently, we present the two-function measurement
variant of this scheme which we show has lower bias. Our algorithms rely on a diminishing sensitivity
parameter sequence {δn}that appears in the denominator of an increment term in our algorithms. This can
result in high variance in the iterates at least in the initial iterates. To tackle this problem, we introduce the
signed analogs of these algorithms where we only consider the sign of the increment terms (the ones that
multiply the learning rates in the updates). We analyze fully the asymptotic convergence of these schemes
including the ones with signed updates. Subsequently, for our experiments, we also incorporate variants
that use gradient clipping as with the proximal policy optimization (PPO), see Schulman et al. (2017a).
A similar scheme as our first (single-measurement) algorithm is briefly presented in Bhatnagar (2023) that
however does not present any analysis of convergence or experiments. Our paper, on the other hand, not only
provides a detailed analysis and experiments with the one-measurement scheme, but also analyzes several
other related algorithms both for their convergence as well as empirical performance.
The REINFORCE algorithm relies on an interchange between the expectation and gradient operators. In
other words, it is assumed that the gradient of the expectation of the noisy performance objective equals the
expectation of the gradient of the same. While such an interchange is easily justifiable in the case of finite
state systems, it is not easy to justify this interchange when the number of states is infinite resulting in an
infinite summation or an integral when computing the expectation. The approaches we present bypass this
problem altogether by directly estimating the gradient of the expectation of the noisy objective.
Gradient estimation in our algorithm is performed using the smoothed functional (SF) technique for gradient
estimation(Rubinstein,1981;Bhatnagar&Borkar,2003;Bhatnagar,2007;Bhatnagaretal.,2013). Thebasic
problem in this setting is the following: Given an objective function J:Rd→Rsuch thatJ(θ) =Eξ[h(θ,ξ)],
whereθ∈Rdis the parameter to be tuned and ξis the noise element, the goal is to find θ∗∈Rdsuch that
J(θ∗) = min
θ∈RdJ(θ). Since the objective function J(·)can be highly nonlinear, one often settles for a lesser
goal – that of finding a local instead of a global minimum. In this setting, the Kiefer-Wolfowitz (Kiefer &
Wolfowitz, 1952) finite difference estimates for the gradient of Jrequire 2dfunction measurements. This
approach does not perform well in practice when dis large.
Random search methods such as simultaneous perturbation stochastic approximation (SPSA) (Spall, 1992;
1997; Bhatnagar, 2005), smoothed functional (SF) (Katkovnik & Kulchitsky, 1972; Bhatnagar & Borkar,
2003; Bhatnagar, 2007) or random directions stochastic approximation (RDSA) (Kushner & Clark, 1978;
Prashanth et al., 2017) typically require much less number of simulations. For instance, the gradient based
algorithms in these approaches require only one or two simulations regardless of the parameter dimension
d. A textbook treatment of random search approaches (including both gradient and Newton algorithms) for
stochastic optimization is available in Bhatnagar et al. (2013).
Before we proceed further, we present the basic Markov decision process (MDP) setting and recall the
REINFORCE algorithm that we consider for the episodic setting. We remark here that there are not many
analyses of REINFORCE type algorithms in the literature in the episodic or stochastic shortest path setting.
2 The Basic MDP Setting
By a Markov decision process (MDP), we mean a controlled stochastic process {Xn}whose evolution is
governed by an associated control-valued sequence {Zn}. It is assumed that Xn,n≥0takes values in a set
Scalled the state-space. Let A(s)be the set of feasible actions in state s∈SandA△=∪s∈SA(s)denote the
set of all actions. When the state is say sand a feasible action ais chosen, the next state seen is s′with a
2Under review as submission to TMLR
probability p(s′|s,a)△=P(Xn+1=s′|Xn=s,Zn=a),∀n. Such a process satisfies the controlled Markov
property, i.e., P(Xn+1=s′|Xn,Zn,...,X 0,Z0) =p(s′|Xn,Zn)a.s.,∀n≥0.
By an admissible policy or simply a policy, we mean a sequence of functions π={µ0,µ1,µ2,...}, with
µk:S→A,k≥0, such that µk(s)∈A(s),∀s∈S. When following policy π, a decision maker selects action
µk(s)at instantk, when the state is s. A stationary policy πis one for which µk=µl△=µ(a time-invariant
function),∀k,l= 0,1,.... Associated with any transition to a state s′from a state sunder action a, is a
‘single-stage’ cost g(s,a,s′)whereg:S×A×S→Ris called the cost function. The goal of the decision
maker is to select actions ak,k≥0in response to the system states sk,k≥0, observed one at a time, so as
to minimize a long-term cost objective. We assume here that the number of states and actions is finite.
2.1 The Episodic or Stochastic Shortest Path Setting
We consider here the episodic or the stochastic shortest path problem where decision making terminates
once a goal or terminal state is reached. We let 1,...,pdenote the set of non-terminal or regular states and
tbe the terminal state. Thus, S={1,2,...,p,t}denotes the state space for this problem Bertsekas (2019).
Our basic setting here is similar to Chapter 3 of Bertsekas (2012) (see also Bertsekas (2019)), where it is
assumed that under any policy there is a positive probability of hitting the goal state tin at most psteps
starting from any initial (non-terminal) state, that would in turn signify that the problem would terminate
in a finite though random amount of time.
Under a given policy π, define
Vπ(s) =Eπ/bracketleftiggT/summationdisplay
k=0g(Xk,µk(Xk),Xk+1)|X0=s/bracketrightigg
, (1)
whereT > 0is a finite random time at which the process enters the terminal state t. HereEπ[·]indicates
that all actions are chosen according to policy πdepending on the system state at any instant. We assume
that there is no action that is feasible in the state tand so the process terminates once it reaches t.
LetΠdenote the set of all admissible policies. The goal here is to find the optimal value function V∗(i),i∈S,
where
V∗(i) = min
π∈ΠVπ(i) =Vπ∗(i), i∈S, (2)
withπ∗being the optimal policy. A related goal then would be to search for the optimal policy π∗. It turns
out that in these problems, there exist stationary policies that are optimal, and so it is sufficient to restrict
the search to the class of stationary policies.
A stationary policy πis called a proper policy (cf. pp.174 of Bertsekas (2012)) if
ˆpπ△= max
s=1,...,pP(Xp̸=t|X0=s,π)<1.
In other words, regardless of the initial state i, there is a positive probability of termination after at most p
stages when using a proper policy πand moreover P(T <∞) = 1under such a policy. An admissible policy
(and so also a stationary policy) can be randomized as well. A randomized admissible policy or simply a
randomized policy is the sequence ψ={ϕ0,ϕ1,...}with eachϕi:S→P(A). In other words, given a state
s, a randomized policy would provide a distribution ϕi(s) = (ϕi(s,a),a∈A(s))for the action to be chosen
in theith stage. A stationary randomized policy is one for which ϕj=ϕk△=ϕ,∀j,k= 0,1,.... Here and
in the rest of the paper, we shall assume that the policies are stationary randomized and are parameterized
via a certain parameter θ∈C⊂Rd, a compact and convex set. We make the following assumption:
Assumption 1 All stationary randomized policies ϕθparameterized by θ∈Care proper.
In practice, one might beable to relax this assumption (as with the model-based analysis ofBertsekas (2012))
by (a) assuming that for policies that are not proper, Vπ(i) =∞for at least one non-terminal state iand
3Under review as submission to TMLR
(b) there exists a proper policy. The optimal value function satisfies the following Bellman equation: For
s= 1,...,p,
V∗(s) = min
a∈A(s)
¯g(s,a) +p/summationdisplay
j=1p(j|s,a)V∗(j)
, (3)
where ¯g(s,a) =p/summationdisplay
j=1p(j|s,a)g(s,a,j ) +p(t|s,a)g(s,a,t )is the expected single-stage cost in a non-terminal
stateswhen a feasible action ais chosen. It can be shown, see Bertsekas (2012), that an optimal stationary
proper policy exists.
2.2 The Policy Gradient Theorem
Policy gradient methods perform a gradient search within the prescribed class of parameterized policies.
Letϕθ(s,a)denote the probability of selecting action a∈A(s)when the state is s∈Sand the policy
parameter is θ∈C. We assume that ϕθ(s,a)is continuously differentiable in θ. A common example
here is of the parameterized Boltzmann or softmax policies. Let ϕθ(s)△= (ϕθ(s,a),a∈A(s)),s∈Sand
ϕθ△= (ϕθ(s),s∈S).
We assume that trajectories of states and actions are available either as real data or from a simulation. Let
Gk=T−1/summationdisplay
j=kgjdenote the sum of costs until termination (likely when a goal state is reached) on a trajectory
starting from instant k. Note that if all actions are chosen according to a policy ϕ, then the value and
Q-value functions (under ϕ) would be Vϕ(s) =Eϕ[Gk|Xk=s]andQϕ(s,a) =Eϕ[Gk|Xk=s,Zk=a],
respectively. In what follows, for ease of notation, we let Vθ≡VϕθandQθ≡Qϕθ, respectively.
The policy gradient theorem for episodic problems has the following form, cf. Chapter 13 of Sutton & Barto
(2018):
∇Vθ(s0) =/summationdisplay
s∈Sµ(s)/summationdisplay
a∈A(s)∇θπ(s,a)Qθ(s,a), (4)
whereµ(s),s∈S, is defined as µ(s) =η(s)/summationtext
s′∈Sη(s′)whereη(s) =∞/summationdisplay
k=0pk(s|s0,ϕθ),s∈S, withpk(s|s0,ϕθ)
being thek-step transition probability of going to state sfroms0under the policy ϕθ. Proving the policy
gradient theorem when the state-action spaces are finite is relatively straight forward (Sutton et al. (1999);
Sutton & Barto (2018)). However, one would require strong regularity assumptions on the system dynamics
and performance function as with infinitesimal perturbation analysis (IPA) or likelihood ratio approaches
(Chong & Ramadge (1994); Ho & Cao (1991)) if the state-action spaces are either countably infinite or
continuously-valued sets.
The REINFORCE algorithm (Sutton & Barto (2018); Williams (1992)) makes use of the policy gradient
theorem as the latter is based on an interchange between the gradient and expectation operators (since the
value function is an expectation over noisy cost returns). In what follows, we present an alternative algo-
rithm based on REINFORCE that incorporates a one-measurement SF gradient estimator. Our algorithm
does not incorporate the policy gradient theorem and thus does not require an interchange between the
aforementioned operators. Our basic algorithm incorporates a zero-order gradient approximation using the
smoothed functional method and like the REINFORCE algorithm, requires one sample trajectory. However,
since our algorithm caters to episodic tasks, it performs updates whenever a certain prescribed recurrent
state is visited, see Cao (2007); Marbach & Tsitsiklis (2001). We refer to our one-simulation algorithm as
the One-SF-REINFORCE (SFR-1) algorithm.
4Under review as submission to TMLR
3 The One-Simulation SF REINFORCE (SFR-1) Algorithm
We assume that data on the mth trajectory is represented in the form of the tuples (sm
k,am
k,gm
k,sm
k+1),
k= 0,1,...,TmwithTmbeing the termination instant on the mth trajectory, m≥1. Also,sm
jis the state at
instantjin themth trajectory. Further, am
kandgm
kare the action chosen and the cost incurred, respectively,
at instant kin themth trajectory. Let Γ :Rd→Cdenote a projection operator that projects any
x= (x1,...,xd)T∈Rdto its nearest point in C. For ease of exposition, we assume that Cis ad-dimensional
rectangle having the form C=d/productdisplay
i=1[ai,min,ai,max], where−∞< ai,min< ai,max<∞,∀i= 1,...,d. Then
Γ(x) = (Γ 1(x1),..., Γd(xd))Twith Γi:R→ [ai,min,ai,max]such that Γi(xi) = min(ai,max,max(ai,min,xi)),
i= 1,...,d. Also, letC(C)denote the space of all continuous functions from CtoRd.
In what follows, we present a procedure that incrementally updates the parameter θ. Letθ(n)denote the
parameter value obtained after the nth update of this procedure which depends on the nth episode and which
is run using the policy parameter Γ(θ(n)+δn∆(n)), forn≥0, whereθ(n) = (θ1(n),...,θd(n))T∈Rd,δn>0
∀nwithδn→0asn→∞and∆(n) = (∆ 1(n),..., ∆d(n))T,n≥0, where ∆i(n),i= 1,...,d,n≥0are
independent random variables distributed according to the N(0,1)distribution.
Algorithm (5) below is used to update the parameter θ∈C⊂Rd. Letχndenote the nth state-action
trajectoryχn={sn
0,an
0,sn
1,an
1,...,sn
T−1,an
T−1,sn
T},n≥0where the actions an
0,...,an
T−1inχnare obtained
using the policy parameter θ(n) +δn∆(n). The instant Tdenotes the termination instant in the trajectory
χnand corresponds to the instant when the terminal or goal state tis reached. Note that the various actions
in the trajectory χnare chosen according to the policy ϕ(θ(n)+δn∆(n)). The initial state is assumed to be
sampled from a given initial distribution ν= (ν(i),i∈S)over states. Let Gn=T−1/summationdisplay
k=0gn
kdenote the sum of
costs until termination on the trajectory χnwithgn
k≡g(sn
k,an
k,sn
k+1). The update rule that we consider
here is the following: For n≥0,i= 1,...,d,
θi(n+ 1) = Γi/parenleftbigg
θi(n)−a(n)/parenleftbigg
∆i(n)Gn
δn/parenrightbigg/parenrightbigg
. (5)
Assumption 2 The step-size sequence {a(n)}satisfiesa(n)>0,∀n. Further,
/summationdisplay
na(n) =∞,/summationdisplay
n/parenleftbigga(n)
δn/parenrightbigg2
<∞.
After the (n−1)st episode, θ(n)is computed using (5). The perturbed parameter θ(n) +δn∆(n)is then
obtained after sampling ∆(n)from the multivariate Gaussian distribution as explained previously and there-
afteranewtrajectorygovernedbythisperturbedparameterisgeneratedwiththeinitialstateineachepisode
sampled according to a given distribution ν.
4 Variants for Improved Performance
We present here two variants of this algorithm that result in improved performance. The first variant reduces
the bias in the estimator by using two simulations instead of one, while the second variant uses the sign of
the increments instead of the increments themselves and this helps reduces the estimator variance. When
applied on two-simulation SF, the second variant helps reduce both the bias and variance.
4.1 Two-Sided SF REINFORCE Algorithm
The idea here is to use two system simulations instead of one in order to reduce the estimator bias. As with
the one-simulation SF algorithm, we assume that we have access to trajectories of data that are used for
performing the parameter updates.
5Under review as submission to TMLR
Letχn+andχn−denote two state-action trajectories or episodes generated after the nth update of the
parameter. These correspond to χn+={sn+
0,an+
0,sn+
1,an+
1,...,sn+
T−1,an+
T−1,sn+
T},n≥0where the ac-
tionsan+
0,...,an+
T−1inχn+are obtained using the policy parameter θ(n) +δn∆(n). Likewise, the actions
an−
0,...,an−
T−1inχn−are obtained using the policy parameter θ(n)−δn∆(n). As before, a new random
vector ∆(n)is generated after θ(n)is obtained using the algorithm but the same ∆(n)is used in both the
policy parameters used to generate the two trajectories.
The initial state in both these episodes is independently sampled from the same initial distribution ν=
(ν(i),i∈S)over states. Let Gn+=T−1/summationdisplay
k=0gn+
kdenote the return or the sum of costs until termination on the
trajectoryχn+, withgn+
k≡g(sn+
k,an+
k,sn+
k+1). Similarly, we let Gn−=T−1/summationdisplay
k=0gn−
kdenote the return or the
sum of costs until termination on the trajectory χn−, withgn−
k≡g(sn−
k,an−
k,sn−
k+1).
The update rule that we consider here is the following: For n≥0,i= 1,...,d,
θi(n+ 1) = Γi/parenleftbigg
θi(n)−a(n)/parenleftbigg
∆i(n)(Gn+−Gn−)
2δn/parenrightbigg/parenrightbigg
. (6)
4.2 SF REINFORCE with Signed Updates
As expected and (also) reported in the literature (Sutton & Barto (2018)), the REINFORCE algorithm
typically suffers from high iterate-variance. We observe this problem even when SF-REINFORCE is used.
To counter the problem of high iterate-variance, we use the sign function sgn(·)in the updates defined as
follows:sgn(x) = +1ifx>0andsgn(x) =−1otherwise. The update rules for the one and two simulation
SF with signed updates are as follows:
4.2.1 One-SF with Signed Updates
The update rule is exactly the same as (5) except that only the sign of the increment is used in the update:
∀i= 1,...,d,
θi(n+ 1) = Γi/parenleftbigg
θi(n)−a(n)sgn/parenleftbigg
∆i(n)Gn
δn/parenrightbigg/parenrightbigg
. (7)
4.2.2 Two-SF with Signed Updates
As with the One-SF case, the update rule here is the same as (6) except that the update rule involves the
sign of the update increment. Thus, we have ∀i= 1,...,d,
θi(n+ 1) = Γi/parenleftbigg
θi(n)−a(n)sgn/parenleftbigg
∆i(n)(Gn+−Gn−)
2δn/parenrightbigg/parenrightbigg
. (8)
5 Convergence Analysis
We present here the main convergence results for the algorithms: one-simulation SF, two-simulation SF, and
two-simulation signed SF, respectively. The proofs of all these results are provided in Appendix A.
5.1 Convergence of One-Simulation SF
We begin by rewriting the recursion (5) as follows:
θi(n+ 1) = Γi/parenleftbigg
θi(n)−a(n)E/bracketleftbigg
∆i(n)Gn
δn|Fn/bracketrightbigg
+Mi
n+1/parenrightbigg
, (9)
whereMi
n+1= ∆i(n)Gn
δn−E/bracketleftig
∆i(n)Gn
δn|Fn/bracketrightig
,n≥0.Here, we letFn△=σ(θ(m),m≤n,∆(m),χm,m <
n),n≥1as a sequence of increasing sigma fields with F0=σ(θ(0)). LetMn△= (M1
n,...,Md
n)T,n≥0.
6Under review as submission to TMLR
Lemma 1 (Mn,Fn),n≥0is a martingale difference sequence.
Proposition 1 We have
E/bracketleftbigg
∆i(n)Gn
δn|Fn/bracketrightbigg
=/summationdisplay
s∈Sν(s)∇iVθ(n)(s) +o(δn)a.s.
In the light of Proposition 1, we can rewrite (5) as follows:
θ(n+ 1) = Γ(θ(n)−a(n)(/summationdisplay
sν(s)∇Vθ(n)(s) +Mn+1
+β(n))), (10)
whereβi(n) =E/bracketleftbigg
∆i(n)Gn
δ|Fn/bracketrightbigg
−/summationtext
sν(s)∇iVθ(n)(s)andβ(n) = (β1(n),...,βd(n))T. FromProposition1,
it follows that β(n) =o(δn).
Lemma 2 The function∇Vθ(s)is Lipschitz continuous in θ. Further,∃a constant K1>0such that
∥∇Vθ(s)∥≤K1(1+∥θ∥).
Lemma 3 The sequence (Mn,Fn),n≥0satisfiesE[∥Mn+1∥2|Fn]≤ˆL
δ2n,for some constant ˆL>0.
Define now a sequence Zn,n≥0according to Zn=n−1/summationdisplay
m=0a(m)Mm+1,n≥1, withZ0= 0.
Lemma 4 (Zn,Fn),n≥0is an almost surely convergent martingale sequence.
Consider now the following ODE:
˙θ(t) =¯Γ(−/summationdisplay
sν(s)∇Vθ(s)), (11)
where ¯Γ :C(C)→C(Rd)is defined according to
¯Γ(v(x)) = lim
η→0/parenleftbiggΓ(x+ηv(x))−x
η/parenrightbigg
. (12)
LetH△={θ|¯Γ(−/summationtext
sν(s)∇Vθ(s)) = 0}denote the set of all equilibria of (11). By Lemma 11.1 of Borkar
(2022), the only possible ω-limit sets that can occur as invariant sets for the ODE (11) are subsets of H. Let
¯H⊂Hbe the set of all internally chain recurrent points of the ODE (11). Our main result below is based
on Theorem 5.3.1 of Kushner & Clark (1978) for projected stochastic approximation algorithms. We state
this theorem in Appendix A along with the assumptions needed there that we verify for our analysis.
Theorem 1 The iterates θ(n),n≥0governed by (5) converge almost surely to ¯H.
5.2 Convergence of Two-Simulation SF
The analysis proceeds in a similar manner here as for the one-simulation SF. Let
Hi(θ(n),∆(n)) = ∆i(n)/bracketleftbiggVθ(n)+δ(n)∆(n)−Vθ(n)−δ(n)∆(n))
2δ(n)/bracketrightbigg
.
Proposition 2
E/bracketleftbigg
∆i(n)/parenleftbiggGn+−Gn−
2δn/parenrightbigg
|Fn/bracketrightbigg
=/summationdisplay
sν(s)E[Hi(θ(n),∆(n))|Fn] =/summationdisplay
s∈Sν(s)∇iVθ(n)(s) +o(δn)a.s.
7Under review as submission to TMLR
The main result on convergence of the stochastic recursions is the following:
Theorem 2 The iterates θ(n),n≥0governed by (6) converge almost surely to ¯H.
5.3 Convergence of Two-Simulation Signed SF REINFORCE
We present here the convergence analysis of the two-simulation signed SF REINFORCE algorithm. The
analysis of the one-simulation counterpart is similar and hence is not provided.
Define now ei(n) =Hi(θ(n),∆(n))−∇iVθ(n), and letFi(e|θ) =P(ei(n)≤e|θ(n) =θ)be the conditional
distribution of ei(n)givenθ(n) =θ. We make the following assumptions:
(A1)P(ei(n)≤e|θ(m),m≤n) =Fi(e|θ(n))independent of n.
(A2) The maps (e,θ)∝⇕⊣√∫⊔≀→Fi(e|θ)andθ∝⇕⊣√∫⊔≀→∇iVθare Lipschitz continuous.
(A3) For all θandi= 1,...,N,Fi(0|θ) = 1/2.
(A4)a(n)>0,∀n,/summationdisplay
na(n) =∞,/summationdisplay
n/parenleftbigga(n)
δ(n)/parenrightbigg2
<∞.
Consider the following ODE associated with the above recursion:
˙θi(t) =¯Γi(−(1−2Fi(−/summationdisplay
sν(s)∇iVθ(s)|θ))), t≥0, i= 1,...,N. (13)
Forx= (x1,...,xd)T, let ¯Γ(x) = ( ¯Γ1(x1),..., ¯Γd(xd))T. Also, letF(−∇Vθ)|θ)△= (F1(−/summationtext
sν(s)∇1Vθ(s)|θ),
...,Fd(−/summationtext
sν(s)∇NVθ(s)|θ))and letK={θ|(¯Γ(−(1−2F(−/summationtext
sν(s)∇Vθ(s)|θ)) = 0) denote the set of
equilibria of (13). Further, let ¯K⊂K⊂{θ|¯Γ(⟨(1−2F(−/summationtext
sν(s)∇Vθ(s)|θ)),/summationtext
sν(s)∇Vθ(s)⟩) = 0}denote
the largest invariant set contained in K.
Theorem 3 (Convergence of Signed SFR-2) {θ(n)}governed as per (8) converges as n→∞almost
surely to ¯K.
Remark 1 Supposeθ∈Kis such that θis in the interior of the constraint set. Then, from Assump-
tions (A2)-(A3) and Theorem 3,/summationtext
sν(s)∇Vθ(s) = 0. Forθon the boundary of the constraint set, either/summationtext
sν(s)∇Vθ(s) = 0or/summationtext
sν(s)∇Vθ(s)̸= 0but in the latter case, ¯Γ(/summationtext
sν(s)∇Vθ(s) = 0). The latter are
spurious fixed points that occur at the boundary of the constraint set, see Kushner & Yin (1997).
6 Numerical Results
We investigate the performance of our proposed SF-REINFORCE algorithms (both one and two sided
variants: SFR-1 and SFR-2) on a stochastic grid world setting. The specifics of the environment setup,
policy estimator and the algorithm parameters are discussed in Appendix B.1, B.2 and B.3, respectively.
Initially, we experiment with various grid sizes and compare our SFR-1 and SFR-2 with REINFORCE and
PPO. To ensure the results are reproducible, we run every setting 10 times with different seeds, and we
plot the mean as the dark line, and shade using the standard deviation around it. Although our algorithms
show good performance, they also display large variance. We refer the reader to Appendix B.4 for these
experiments. We further try out various schedules for perturbations and learning rates, as well as gradient
clipping techniques. In the process of trying out methods to improve on the variance of the objective, we
illustrate the tradeoff between the performance of the iterates and their variance.
For brevity, we show the best performance on our largest (most difficult) grid in Figure 1 and also compare
how fast these algorithms achieve a performance threshold, by counting the number of updates in Table 2.
Table 1 summarizes the best performance of variants of SFR-1 and SFR-2 on 50×50grid. Notably, SFR-2
8Under review as submission to TMLR
Figure 1: Best runs across all algorithms on 50×50grid
algo vanilla const_delta signed clip_by_value clip_by_norm
SFR-1 376.29±401.4 677.55±233.1 424.93±254.5483.68±215.3395.38±279.6
SFR-2 562.89±387.7665.5±229.1 823.68±3.1824.96±5.6 830.4±3.7
Table 1: Best Performance of variants, on large grid, L= 50
with gradient clipping by norm and SFR-1 with constant perturbation size emerge as the leaders in their
respective best runs. Both algorithms require substantially fewer updates compared to PPO. While neither
SFR-1 nor PPO reach thresholds of 700 and 800 (resp.), SFR-2 achieves a target of 800 within just 11,000
updates. This demonstrates the superior efficacy of SFR-2, particularly when incorporating techniques like
gradient clipping.
6.1 Perturbation sizes
Since our algorithm uses stochastic perturbations to estimate the gradient of the objective function, we
expect the dynamics of our policy weights to be intimately related to the perturbation size. To investigate
this, we experiment with both decaying and constant perturbation schedules δ(n).
6.1.1 Decaying perturbations
Since we are using δ(n) =δ0(1
50000+n)d, we setδ0= 1,α0= 2×10−6and varyd∈{0.15,0.25,0.35,0.45}.
These runs are made on the medium grid size L= 10. Note that for a fixed δ0, lower values of dcorrespond
to higher values of δ(n). It is clear that for larger perturbations ( d∈{0.101,0.15,0.25}), there is lower
variance, both across runs (in terms of the converged value) and lower variance within the run (compare
the shaky blue lines from Figures 2c and 2d with the steady lines from Figures 2a and 2b). It is clear from
Figure 2 that we can conclude that SFR-1 is more sensitive to decay parameter since it affects the variance
in the iterates within the same run too.
Algorithm / Threshold 200400600700 800
SFR-2 1000100020003000 11000
SFR-1 90001900063000- -
PPO 510006800099000134000 -
Table 2: Number of updates from different algorithms required to cross reward threshold on 50×50grid
9Under review as submission to TMLR
(a) |S|= 100 ,d= 0.101
 (b) |S|= 100 ,d= 0.15
 (c) |S|= 100 ,d= 0.35
 (d) |S|= 100 ,d= 0.45
Figure 2: Iterate performance for different decay exponents dfor perturbations δ(n)on 10×10 grid. Other
parameters are set to δ0= 1,α0= 2×10−6.
(a) |S|= 100 ,δ(n) = 0.175
 (b) |S|= 100 ,δ(n) = 0.5
Figure 3: Runs showing constant schemes for δ(n), for grid size L= 10.
(a) |S|= 2500 ,δ(n) = 0.175
 (b) |S|= 2500 ,δ(n) = 0.5
Figure 4: Runs showing constant schemes for δ(n), for the large grid size L= 50, capturing the tradeoff
between performance and variance.
Algorithm d= 0.101d= 0.15d= 0.25d= 0.35d= 0.45
SFR-1 68.72±1.574.07±0.759.45±35.062.60±28.431.33±34.0
SFR-2 68.10±1.374.18±0.668.49±26.360.40±35.460.46±35.4
Table3: Performancefordifferentdecayexponents dforperturbations δ(n)on10×10grid. Otherparameters
are set toδ0= 1,α0= 2×10−6.
10Under review as submission to TMLR
Algorithm / Decay 0.175 0.500
SFR-1 72.66±3.652.60±2.8
SFR-2 73.29±1.953.10±3.2
Table 4: Performance for different constant values of δ(n)on 10×10 grid over 10 seeded runs
Algorithm δ(n) = 0.175δ(n) = 0.5δ(n) = 0.7
SFR-1 677.55±233.1446.65±26.8254.74±35.6
SFR-2 665.50±229.1437.26±21.7247.09±33.0
Table 5: Performance for different constant values of δ(n)on 50×50 grid over 10 seeded runs
6.1.2 Constant Perturbations
We also tried experiments with constant values of δ(n) = 0.175andδ(n) = 0.5(they correspond to d= 0
andδ0∈{0.175,0.5}). We run on both medium and large grids ( L∈{10,50}). For both grids, SFR-1 and
SFR-2 show similar results, see Figures 3 and 4. Larger value of δ(n)shows stable iterates that however do
not reach a high enough reward compared to those with a smaller value of δ(n). Tables 4 and 5 capture the
trade-off between the mean and variance of the converged values of this setting. Since δ(n) = 0.175gives the
best performance on the large grid-size, we try experimenting with signed and clipped gradients to improve
on the consistency of the total reward across runs.
6.2 On transforming updates
To control the dynamics of the weights, we employ variants of the proposed scheme geared towards trans-
forming the gradients to mitigate the effects of exploding or vanishing gradients problem. The effect of
signed as well as clipped gradients is investigated.
6.2.1 Signed Updates
We experiment with the signed update scheme described and analyzed previously. For this, we maintain
a constant value for δ(n) = 0.175, while varying the step-sizes ( α(n)) as shown in Figure 5 and Table 6.
Since each component is changed by a magnitude of α(n), the step size controls how far the iterates can go
in the span of fixed iterations, from a given random initialization. As expected, for very small step-sizes,
the performance is not good, but improves with increasing step-sizes. We note that for large step-sizes, the
two-sided variant does extremely well and shows good reward performance with low standard deviation as
well. In the case of the one-sided algorithm, large step-sizes also improve the discounted reward performance
but at the same time increase variance.
6.2.2 Gradient clipping
To maintain stability of the iterates, we need to ensure that gradients do not explode. One way is to clip
each component of the gradient by value, whereas the other method involves normalizing the gradient such
that its norm does not exceed a previously agreed value. Figure 6 shows the best runs in both cases. Results
of more detailed experiments on different gridsizes are provided in Appendix B.4.
Algorithm α0= 2×10−6α0= 2×10−4α0= 2×10−3α0= 2×10−2
SFR-1 -26.03±8.2141.32±144.6392.66±234.3424.93±254.5
SFR-2 -22.87±6.4456.54±161.1761.28±6.7823.68±3.1
Table 6: Performance of signed updates on large grids by varying step-sizes
11Under review as submission to TMLR
(a) |S|= 2500 ,α(n) =10
50000+ n
(b) |S|= 2500 ,α(n) =10
50000+ n
(c) |S|= 2500 ,α(n) =1000
50000+ n
Figure 5: Runs for signed updates with different values of step-sizes, α(n)
(a) |S|= 2500 , clip by value
 (b) |S|= 2500 , clip by norm
Figure 6: Runs for gradients clipped by value and norm
7 Conclusions
We presented model-free smoothed functional algorithms as suitable Monte-Carlo based alternatives to the
popular REINFORCE algorithm for the setting of episodic tasks. We first presented the one-simulation SF
REINFORCE algorithm followed by its two-simulation counterpart. We also presented the signed variant
of these algorithms. Subsequently, we provided a complete convergence analysis of the one-simulation SF
REINFORCE algorithm, and described the changes in the analysis needed for the two-simulation variant.
Next, we also provided the convergence analysis of the two-simulation signed variant, the one with one-
simulation being analogous.
We showed detailed empirical results of the various algorithms on different grid world settings and under
various choices of the setting parameters. We also compared the performance of our algorithms with the
REINFORCE algorithm as well as PPO, and observed that both one-sided and two-sided SF REINFORCE
algorithms show better performance than these other algorithms. We also found that signed updates and
gradient clipping are effective procedures that help significantly alleviate the problems of high variance in
Monte-Carlo algorithms such as REINFORCE. As future work, it would be of interest to theoretically study
the convergence rate results of the algorithms presented here.
References
D. P. Bertsekas. Dynamic Programming and Optimal Control, Vol.II . Athena Scientific, 2012.
Dimitri Bertsekas. Reinforcement learning and optimal control , volume 1. Athena Scientific, 2019.
S. Bhatnagar. Adaptive multivariate three-timescale stochastic approximation algorithms for simulation
based optimization. ACM Transactions on Modeling and Computer Simulation , 15(1):74–107, 2005.
12Under review as submission to TMLR
S. Bhatnagar. Adaptive Newton-based smoothed functional algorithms for simulation optimization. ACM
Transactions on Modeling and Computer Simulation , 18(1):2:1–2:35, 2007.
S. Bhatnagar. The reinforce policy gradient algorithm revisited. In 2023 Ninth Indian Control Conference
(ICC), pp. 177–177. IEEE, 2023.
S. Bhatnagar and V.S. Borkar. Multiscale chaotic SPSA and smoothed functional algorithms for simulation
optimization. Simulation , 79(10):568–580, 2003.
S. Bhatnagar, R.S. Sutton, M. Ghavamzadeh, and M. Lee. Incremental natural actor-critic algorithms.
Advances in neural information processing systems , 20, 2007.
S. Bhatnagar, R.S. Sutton, M. Ghavamzadeh, and M. Lee. Natural actor–critic algorithms. Automatica , 45
(11):2471–2482, 2009.
S Bhatnagar, H. L. Prasad, and L. A. Prashanth. Stochastic Recursive Algorithms for Optimization: Simul-
taneous Perturbation Methods (Lecture Notes in Control and Information Sciences) , volume 434. Springer,
2013.
V. S. Borkar. Probability Theory: An Advanced Course . Springer, New York, 1995.
V. S. Borkar. Stochastic Approximation: A Dynamical Systems Viewpoint, 2’nd Edition . Cambridge Uni-
versity Press, 2022.
X.-R. Cao. Stochastic Learning and Optimization: A Sensitivity-Based Approach . Springer, 2007.
E.K.P.ChongandP.J.Ramadge. Optimizationofqueuesusinganinfinitesimalperturbationanalysis-based
stochastic algorithm with general update times. SIAM J. Cont. and Optim. , 31(3):698–732, 1993.
Edwin KP Chong and Peter J Ramadge. Stochastic optimization of regenerative systems using infinitesimal
perturbation analysis. IEEE transactions on automatic Control , 39(7):1400–1410, 1994.
T. Furmston, G. Lever, and D. Barber. Approximate Newton methods for approximate policy search in
Markov decision processes. Journal of Machine Learning Research , 17:1–51, 2016.
Y. C. Ho and X. R. Cao. Perturbation Analysis of Discrete Event Dynamical Systems . Kluwer, Boston,
1991.
V. Ya Katkovnik and Yu Kulchitsky. Convergence of a class of random search algorithms. Automation
Remote Control , 8:1321–1326, 1972.
J. Kiefer and J. Wolfowitz. Stochastic estimation of the maximum of a regression function. Ann. Math.
Statist., 23:462–466, 1952.
V.R. Konda and V.S. Borkar. Actor-critic–type learning algorithms for markov decision processes. SIAM
Journal on control and Optimization , 38(1):94–123, 1999.
V.R. Konda and J.N. Tsitsiklis. Onactor-critic algorithms. SIAM journal on Control and Optimization , 42
(4):1143–1166, 2003.
H. J. Kushner and D. S. Clark. Stochastic Approximation Methods for Constrained and Unconstrained
Systems. Springer Verlag, 1978.
H. J. Kushner and G. G. Yin. Stochastic Approximation Algorithms and Applications . Springer Verlag, New
York, 1997.
P. Marbach and J.N. Tsitsiklis. Simulation-based optimization of Markov reward processes. IEEE Transac-
tions on Automatic Control , 46(2):191–209, 2001.
Sean Meyn. Control systems and reinforcement learning . Cambridge University Press, 2022.
13Under review as submission to TMLR
L A Prashanth, S. Bhatnagar, M.C. Fu, and S.I. Marcus. Adaptive system optimization using random
directions stochastic approximation. IEEE Transactions on Automatic Control , 62(5):2223–2238, 2017.
Antonin Raffin, Ashley Hill, Adam Gleave, Anssi Kanervisto, Maximilian Ernestus, and Noah Dormann.
Stable-baselines3: Reliable reinforcement learning implementations. Journal of Machine Learning Re-
search, 22(268):1–8, 2021. URL http://jmlr.org/papers/v22/20-1364.html .
R. Y. Rubinstein. Simulation and the Monte Carlo Method . Wiley, New York, 1981.
J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov. Proximal policy optimization algorithms.
arXiv preprint arXiv:1707.06347 , 2017a.
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimiza-
tion algorithms, 2017b.
J.C. Spall. Multivariate stochastic approximation using a simultaneous perturbation gradient approximation.
IEEE Transactions on Automatic Control , 37(3):332–341, 1992.
J.C. Spall. A one-measurement form of simultaneous perturbation stochastic approximation. Automatica ,
33(1):109–112, 1997.
R. S. Sutton and A. W. Barto. Reinforcement Learning, 2’nd Edition . MIT Press, 2018.
R. S. Sutton, D. A. McAllester, S. P. Singh, and Y. Mansour. Policy gradient methods for reinforcement
learning with function approximation. In NIPS, volume 99, pp. 1057–1063, 1999.
R.J. Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning.
Reinforcement learning , pp. 5–32, 1992.
A Details of the Convergence Analysis
We present here the details of the convergence analysis and give the proofs of the various results. We begin
first with the results for the One-Simulation SF algorithm. We will subsequently sketch the analysis of the
two-simulation SF algorithm. Finally, we shall discuss the convergence analysis of the algorithms with signed
updates.
A.1 Convergence of One-Simulation SF
Proof of Lemma 1: Notice that
Mi
n= ∆i(n−1)Gn−1
δn−1−E/bracketleftbigg
∆i(n−1)Gn−1
δn−1|Fn−1/bracketrightbigg
.
The first term on the RHS above is clearly measurable Fnwhile the second term is measurable Fn−1and
hence measurable Fnas well. Further, from Assumption 1, each Mnis integrable. Finally, it is easy to verify
that
E[Mi
n+1|Fn] = 0,∀i.
The claim follows.
Proof of Proposition 1: Note that
E/bracketleftbigg
∆i(n)Gn
δn|Fn/bracketrightbigg
=E/bracketleftbigg
E/bracketleftbigg
∆i(n)Gn
δn|Gn/bracketrightbigg
|Fn/bracketrightbigg
,
whereGn△=σ(θ(m),∆(m),m≤n,χm,m < n ),n≥1is a sequence of increasing sigma fields with G0=
σ(θ(0),∆(0)). It is clear thatFn⊂Gn,∀n≥0. Now,
E/bracketleftbigg
∆i(n)Gn
δn|Gn/bracketrightbigg
=∆i(n)
δnE[Gn|Gn].
14Under review as submission to TMLR
Letsn
0=sdenote the initial state in the trajectory χn. Recall that the initial state sis chosen randomly
from the distribution ν. Thus,
E[Gn|Gn] =/summationdisplay
sν(s)E[Gn|sn
0=s,ϕθ(n)+δn∆(n)]
=/summationdisplay
sν(s)Vθ(n)+δn∆(n)(s).
Thus, with probability one,
E/bracketleftbigg
∆i(n)Gn
δn|Gn/bracketrightbigg
=/summationdisplay
sν(s)/parenleftbigg
∆i(n)Vθ(n)+δn∆(n)(s)
δn/parenrightbigg
.
Hence, it follows almost surely that
E/bracketleftbigg
∆i(n)Gn
δn|Fn/bracketrightbigg
=/summationdisplay
sν(s)E/bracketleftbigg
∆i(n)Vθ(n)+δn∆(n)(s)
δn|Fn/bracketrightbigg
.
Using a Taylor’s expansion of Vθ(n)+δn∆(n)(s)aroundθ(n)gives us
Vθ(n)+δn∆(n)(sn) =Vθ(n)(sn) +δn∆(n)T∇Vθ(n)(sn)
+δ2
n
2∆(n)T∇2Vθ(n)(sn)∆(n) +o(δ2
n).
Now recall that ∆(n) = (∆i(n),i= 1,...,d )T. Thus,
∆(n)Vθ(n)+δn∆(n)(sn)
δn=1
δn∆(n)Vθ(n)(sn)
+∆(n)∆(n)T∇Vθ(n)(sn)
+δn
2∆(n)∆(n)T∇2Vθ(n)(sn)∆(n) +o(δn).
Now observe from the properties of ∆i(n),∀i,n,that
(i)E[∆(n)] = 0(the zero-vector), ∀n, since ∆i(n)∼N(0,1),∀i,n.
(ii)E[∆(n)∆(n)T] =I(the identity matrix), ∀n.
(iii)E
d/summationdisplay
i,j,k=1∆i(n)∆j(n)∆k(n)
= 0.
Property (iii) follows from the facts that (a) E[∆i(n)∆j(n)∆k(n)] = 0,∀i̸=j̸=k, (b)E[∆i(n)∆2
j(n)] = 0,
∀i̸=j(this pertains to the case where i̸=jbutj=kabove) and (c) E[∆3
i(n)] = 0(for the case when
i=j=kabove). These properties follow from the independence of the random variables ∆i(n),i= 1,...,d
andn≥0, as well as the fact that they are all distributed N(0,1). The claim now follows from (i)-(iii)
above.
Recall that from Proposition 1, it follows that β(n) =o(δn).
Proof of Lemma 2 : It can be seen from (4) that Vθ(s)is continuously differentiable in θ. It can also be
shown as in Theorem 3 of Furmston et al. (2016) that ∇2Vθ(s)exists and is continuous. Since θtakes values
inC, a compact set, it follows that ∇2Vθ(s)is bounded and thus ∇Vθ(s)is Lipschitz continuous.
Finally, let Ls
1>0denote the Lipschitz constant for the function ∇Vθ(s). Then, for a given θ0∈C,
∥∇Vθ(s)∥−∥∇Vθ0(s)∥≤∥∇Vθ(s)−∇Vθ0(s)∥
≤Ls
1∥θ−θ0∥
15Under review as submission to TMLR
≤Ls
1∥θ∥+Ls
1∥θ0∥.
Thus,∥ ∇Vθ(s)∥≤∥ ∇Vθ0(s)∥+Ls
1∥θ0∥+Ls
1∥θ∥.LetKs△=∥∇Vθ0(s)∥+Ls
1∥θ0∥andK1△=
max(Ks,Ls
1,s∈S). Thus,∥ ∇Vθ(s)∥≤K1(1+∥θ∥). Note here that since |S|<∞,K1<∞as
well. The claim follows.
Proof of Lemma 3: Note that
∥Mn+1∥2=d/summationdisplay
i=1(Mi
n+1)2
=d/summationdisplay
i=1/parenleftig
∆2
i(n)(Gn)2
δ2n+1
δ2nE[∆i(n)Gn|Fn]2
−2∆i(n)Gn
δ2nE[∆i(n)Gn|Fn]/parenrightig
.
Thus,
E[∥Mn+1∥2|Fn] =1
δ2nd/summationdisplay
i=1/parenleftig
E[∆2
i(n)(Gn)2|Fn]
−E2[∆i(n)Gn|Fn]/parenrightig
.
The claim now follows from Assumption 1 and the fact that all single-stage costs are bounded (cf. pp.174,
Chapter 3 of Bertsekas (2012)).
Proof of Lemma 4: It is easy to see that ZnisFn-measurable∀n. Further, it is integrable for each nand
moreoverE[Zn+1|Fn] =Znalmost surely since (Mn+1,Fn),n≥0is a martingale difference sequence by
Lemma 1. It is also square integrable from Lemma 3. The quadratic variation process of this martingale
will be convergent almost surely if
∞/summationdisplay
n=0E[∥Zn+1−Zn∥2|Fn]<∞a.s. (14)
Note that
E[∥Zn+1−Zn∥2|Fn] =a(n)2E[∥Mn+1∥2|Fn].
Thus,
∞/summationdisplay
n=0E[∥Zn+1−Zn∥2|Fn] =∞/summationdisplay
n=0a(n)2E[∥Mn+1∥2|Fn]
≤ˆL∞/summationdisplay
n=0/parenleftbigga(n)
δn/parenrightbigg2
,
by Lemma 3. (14) now follows as a consequence of Assumption 2. Now (Zn,Fn),n≥0can be seen to be
convergent from the martingale convergence theorem for square integrable martingales Borkar (1995).
Our main result below is based on Theorem 5.3.1 of Kushner & Clark (1978) for projected stochastic
approximation algorithms. Before we proceed further, we recall that result below.
LetC⊂Rdbe a compact and convex set as before and Γ :Rd→Cdenote the projection operator that
projects any x= (x1,...,xd)T∈Rdto its nearest point in C.
Consider now the following the d-dimensional stochastic recursion
Xn+1= Γ(Xn+a(n)(h(Xn) +ξn+βn)), (15)
under the assumptions listed below. Also, consider the following ODE associated with (15):
˙X(t) =¯Γ(h(X(t))). (16)
16Under review as submission to TMLR
LetC(C)denote the space of all continuous functions from CtoRd. The operator ¯Γ :C(C)→C(Rd)is
defined according to
¯Γ(v(x)) = lim
η→0/parenleftbiggΓ(x+ηv(x))−x
η/parenrightbigg
, (17)
for any continuous v:C→Rd. The limit in (17) exists and is unique since Cis a convex set. In case this
limit is not unique, one may consider the set of all limit points of (17). Note also that from its definition,
¯Γ(v(x)) =v(x)ifx∈Co(the interior of C). This is because for such an x, one can find η >0sufficiently
small so that x+ηv(x)∈Coas well and hence Γ(x+ηv(x)) =x+ηv(x). On the other hand, if x∈∂C
(the boundary of C) is such that x+ηv(x)̸∈C, for any small η>0, then ¯Γ(v(x))is the projection of v(x)
to the tangent space of ∂Catx.
Consider now the assumptions listed below.
(B1) The function h:Rd→Rdis continuous.
(B2) The step-sizes a(n),n≥0satisfy
a(n)>0∀n,/summationdisplay
na(n) =∞, a(n)→0asn→∞.
(B3) The sequence βn,n≥0is a bounded random sequence with βn→0almost surely as n→∞.
(B4) There exists T >0such that∀ϵ>0,
lim
n→∞P
sup
j≥nmax
t≤T/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsinglem(jT+t)−1/summationdisplay
i=m(jT)a(i)ξi/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle≥ϵ
= 0.
(B5) The ODE (16) has a compact subset KofRNas its set of asymptotically stable equilibrium points.
Lett(n),n≥0be a sequence of positive real numbers defined according to t(0) = 0 and forn≥1,
t(n) =n−1/summationdisplay
j=0a(j). ByAssumption(B2), t(n)→∞asn→∞. Letm(t) = max{n|t(n)≤t}. Thus,m(t)→∞
ast→∞. Assumptions (B1)-(B3) correspond to A5.1.3-A5.1.5 of Kushner & Clark (1978) while (B4)-(B5)
correspond to A5.3.1-A5.3.2 there.
(Kushner & Clark, 1978, Theorem 5.3.1 (pp. 191-196)) essentially says the following:
Theorem 4 (Kushner and Clark Theorem:) Under Assumptions (B1)–(B5), almost surely, Xn→K
asn→∞.
Finally, we come to the proof of our main result.
Proof of Theorem 1: In lieu of the foregoing, we rewrite (5) according to
θi(n+ 1) = Γi/parenleftig
θi(n)−a(n)/summationdisplay
sν(s)∇iVθ(n)(s)
−a(n)βi(n) +Mi
n+1/parenrightig
, (18)
whereβi(n)is as in (10). We shall proceed by verifying Assumptions (B1)-(B5) and subsequently appeal
to Theorem 5.3.1 of Kushner & Clark (1978) (i.e., Theorem 1 above) to claim convergence of the scheme.
Note that Lemma 2 ensures Lipschitz continuity of ∇Vθ(s)implying (B1). Next, from (B2), since δn→0,
it follows that a(n)→0asn→∞. Thus, Assumption (B2) holds as well. Now from Lemma 2, it follows
that/summationtext
sν(s)∇Vθ(s)is uniformly bounded since θ∈C, a compact set. Assumption (B3) is now verified from
Proposition 1. Since Cis a convex and compact set, Assumption (B4) holds trivially. Finally, Assumption
17Under review as submission to TMLR
(B5) is also easy to see as a consequence of Lemma 4. Now note that for the ODE (11), F(θ) =/summationtext
sν(s)Vθ(s)
serves as an associated Lyapunov function and in fact
∇F(θ)T¯Γ(−/summationdisplay
sν(s)∇Vθ(s))
= (/summationdisplay
sν(s)∇θVθ(s))T¯Γ(−/summationdisplay
sν(s)∇Vθ(s))≤0.
Forθ∈Co(the interior of C), it is easy to see that ¯Γ(−/summationtext
sν(s)∇Vθ(s)) =−/summationtext
sν(s)∇Vθ(s), and
∇F(θ)T¯Γ(−/summationdisplay
sν(s)∇Vθ(s))<0ifθ∈Hc∩C
= 0o.w.
Forθ∈δC(the boundary of C), there can additionally be spurious attractors, see Kushner & Yin (1997),
that are also contained in H. The claim now follows from Theorem 5.3.1 of Kushner & Clark (1978).
A.2 Convergence of Two-Simulation SF
The analysis proceeds in a similar manner as for the one-simulation SF except withGn+−Gn−
2δnin place of
Gn
δn.
Proof of Proposition 2:
A similar calculation as with the proof of Proposition 1 would show that
E/bracketleftbigg
∆i(n)/parenleftbiggGn+−Gn−
2δn/parenrightbigg
|Fn/bracketrightbigg
=/summationdisplay
sν(s)E/bracketleftbigg
∆i(n)(Vθ(n)+δn∆(n)(s)−Vθ(n)−δn∆(n)(s)
2δn|Fn/bracketrightbigg
.
Using Taylor’s expansions of Vθ(n)+δn∆(n)(s)andVθ(n)−δn∆(n)(s)aroundθ(n)gives us
∆(n)/parenleftbiggVθ(n)+δn∆(n)(sn)−Vθ(n)−δn∆(n)(sn)
2δn/parenrightbigg
= ∆(n)∆(n)T∇Vθ(n)(sn) +o(δn).
The zero order and second order terms directly cancel above instead of being zero-mean, thereby resulting
in lower gradient estimator bias. The rest follows from properties (i)-(iii) mentioned previously for the
one-simulation gradient SF. In particular, E[∆(n)∆(n)T] =I.
Proof of Theorem 2:
In the light of Proposition 2, the proof here follows in a similar manner as one-simulation SF.
A.3 Convergence of Two-Simulation Signed SF REINFORCE
Recall that we have
Hi(θ(n),∆(n)) = ∆i(n)/bracketleftbiggVθ(n)+δ(n)∆(n)−Vθ(n)−δ(n)∆(n))
2δ(n)/bracketrightbigg
.
As explained previously,
E[Hi(θ(n),∆(n)|Fn] =∇iVθ(n)+o(δ(n)).
Also, recall the ‘error’ in the ith component is given by
ei(n) =Hi(θ(n),∆(n))−∇iVθ(n)=/summationdisplay
j̸=i∆i(n)∆j(n)∇jVθ(n)+o(δ(n)).
18Under review as submission to TMLR
Proof of Theorem 3:
We rewrite the algorithm as follows:
θi(n+ 1) = Γi(θi(n)−a(n)sgn(Hi(θ(n),∆(n))))
= Γi(θi(n)−a(n)(I(Hi(θ(n),∆(n))>0)−I(Hi(θ(n),∆(n))≤0))),
whereI(·)is the indicator function. Thus, we have
θi(n+ 1) = Γi(θi(n)−a(n)(1−2I(Hi(θ(n),∆(n))≤0)))
= Γi(θi(n)−a(n)(1−2P(Hi(θ(n),∆(n))≤0|Fn) +Mi(n+ 1))),
where
Mi(n+ 1) = 2P(Hi(θ(n),∆(n))≤0|Fn)−2I(Hi(θ(n),∆(n))≤0),
= 2P(ei(n)≤−/summationdisplay
sν(s)∇iVθ(n)(s)|Fn)−2I(ei(n)≤−/summationdisplay
sν(s)∇iVθ(n)(s))
= 2P(ei(n)≤−/summationdisplay
sν(s)∇iVθ(n)(s)|θ(n))−2I(ei(n)≤−/summationdisplay
sν(s)∇iVθ(n)(s)),
by (A1). It is easy to see that (Mi(n),Fn),n≥0is a martingale difference sequence. Since
supn|Mi(n)|≤1, and under (A4), it follows from an application of the martingale convergence theorem
thatn−1/summationdisplay
m=0a(m)Mm+1,n≥1is an almost surely convergent martingale.
It is easy to verify that W(θ) =/summationtext
sν(s)Vθ(s)itself is a Lyapunov function for the ODE (13) since
dW(θ)
dt=−¯Γ(⟨(1−2F(−/summationdisplay
sν(s)∇Vθ(s)|θ)),/summationdisplay
sν(s)∇Vθ(s)⟩)
=−N/summationdisplay
i=1¯Γi((1−2Fi(−/summationdisplay
sν(s)∇iVθ(s)|θ))/summationdisplay
sν(s)∇iVθ(s)).
From (A3), if/summationtext
sν(s)∇iVθ(s)>0,(1−2Fi(−/summationtext
sν(s)∇iVθ(s)|θ))≥0anddW(θ)
dt≤0. Similarly, if
∇iVθ<0,(1−2Fi(−∇iVθ|θ))≤0anddVθ
dt≤0. Further, when/summationtext
sν(s)∇iVθ(s) = 0,dW(θ)
dt= 0.
From Lasalle’s invariance theorem in conjunction with Theorem 2 of Chapter 2 of Borkar (2022), it
follows that θ(n),n≥0converges almost surely to the largest invariant set ¯K⊂K⊂ {θ|¯Γ(⟨(1−
2F(−/summationtext
sν(s)∇Vθ(s)|θ)),/summationtext
sν(s)∇Vθ(s)⟩) = 0}. The claim follows.
B Numerical Results
B.1 The Stochastic Grid-World Environment
We conduct experiments on a 2-dimensional grid-world environment with configurable sizes denoted by L.
The agent starts at the top-left corner of the grid-world and aims to reach the terminal state located at
the bottom-right corner. The reward function, based on Manhattan distance, penalizes for not reaching the
goal, with a gradual decrease in penalty as the agent approaches the terminal state, as visualized in Figure
7. The penalty here is specified by a negative single-stage reward that is high for states that are farther
from the goal than the nearby states. This is designed so as to provide guidance to the agent to move in the
desired direction towards the goal state.
The agent is allowed to move in all four directions in the grid. However, we introduce an uncertainty in
the environment. When an agent executes any of these actions, there is a probability 1−pthat the agent
will move in the intended direction, and with probability p/2, it will move in one of the two perpendicular
19Under review as submission to TMLR
Figure 7: Stochastic grid world problem
Grid size Max episode length
4×4 50
8×8 80
10×10 100
20×20 150
50×50 200
Table 7: Maximum steps allowed in an episode for a given grid-size (L)
directions. This probabilistic element makes the objective function stochastic. We set p= 0.1in all our
experiments. The episode ends after the agent reaches the terminal state. If the agent is unable to reach the
goal state, to ensure finite total rewards, we terminate the episode after a specified number of steps. Table
7 shows these specific parameters for the available grid-sizes. Note, however, that only the goal state has
a positive reward while all the other states carry a negative (though varying) reward. Thus, only episodes
that carry a negative return are the ones that were terminated.
B.2 Policy function
The input to the policy consists of polynomial and modulo features of the agent’s position in the grid. Both
algorithms utilize the same policy function, which computes logits for the available actions using a linear
function. The logits are then inputted into the Softmax or Boltzmann function to obtain a probability
distribution. Figure 8 illustrates the same.
B.3 Algorithm Parameters
For the two SF-REINFORCE algorithms we chose the sensitivity and step-size parameters as δ(n) =
δ0(1
50000+n)dandα(n) =α0∗50000
50000+n, respectively, where nis the iteration or episode number. Note that
the iteration and episode numbers are the same in our setup. We set α(0) =α0= 2×10−6. We require
d < 0.5to prove convergence theoretically to satisfy the requirement that/summationdisplay
n/parenleftbigga(n)
δ(n)/parenrightbigg2
<∞. Hence, we
restrict these quantities as such. We experiment with both (a) decay schemes by varying dand setting
δ(0) =δ0= 1and also (b) a constant scheme with d= 0and varying δ0. To reduce the variance, we measure
Gnas the average over 10 trials. The two-sided version uses 5 trials for each side, so as to match the net
computational effort in comparison to the one-sided scheme, and thereby allowing for a fair comparison.
20Under review as submission to TMLR
(a) Features from state
 (b) Policy architecture
Figure 8: Boltzmann Policy used across all algorithms
Algorithm / Gridsize 4x4 8x8 10x10 20x20 50x50
REINFORCE 13.90±0.150.74±2.376.90±4.9144.89±113.8-18.79±8.4
PPO 12.72±0.249.72±0.275.38±0.3235.09±1.9682.97±12.9
SFR-1 13.23±0.145.40±17.759.80±35.0220.24±78.8376.29±401.4
SFR-2 13.26±0.145.52±17.860.39±35.0170.68±127.4562.89±387.7
Table 8: Total reward (mean ±standard error) of algorithms on different grid sizes. Parameters used:
δ0= 1,d= 0.25andα0= 2×10−6
We also include the Proximal Policy Optimization (PPO) algorithm from Schulman et al. (2017b); Raffin
et al. (2021) for our numerical comparison. We use the same policy architecture along with a linear layer
for the value function. For REINFORCE, and PPO the policy is updated using the policy gradient via an
ADAM optimizer with its learning rate set to 0.0003.
B.4 Gridsizes
Figure 9 illustrates the dynamics of the REINFORCE, SF-REINFORCE and PPO algorithms on various
gridsizes. For this, we set the parameters: δ0= 1,d= 0.25andα0= 2×10−6, and vary the gridsize L= 4,
8, 10, 20 and 50. For the largest grid-size, we end up having |S|= 2500states. We limit L≤50since this
is quite a lot for a Boltzmann policy of 30trainable parameters. Table 8 shows the convergence values.
From Table 8, it can be seen that REINFORCE does only slightly better than our algorithms on smaller
gridsizesL∈4,8,10and performs decently for L= 20. For smaller gridsizes, since REINFORCE uses the
analytical gradient, there is no noise in the gradient due to perturbations unlike SF-REINFORCE. As a
result, as seen in figures 9a, 9c and 9d, it is able to quickly converge to the optimal linear policy.
PPO regularizes the objective function by clipping it. Such methods are reliable for policies with deeper neu-
ral networks. Yet, we note that PPO is able to work with linear policy and produces consistent performances
across seeds.
Compared to REINFORCE, our methods are more robust since it shows reasonable performance on all
grid sizes, especially for higher L∈{20,50}. The two-sided SF REINFORCE (SFR-2) attains the best
performance for L= 50. As expected, the one-sided SF REINFORCE (SFR-1) shows similar or higher
variance across seeds than SFR-2.
21Under review as submission to TMLR
(a) |S|= 16
 (b) |S|= 64
(c) |S|= 100
 (d) |S|= 400
(e) |S|= 2500
Figure 9: Plots showing performance of iterates of algorithms on various gridsizes. Parameters used: δ0= 1,
d= 0.25andα0= 2×10−6
22Under review as submission to TMLR
Although our algorithms converge to competent averages for bigger grid sizes, the variance is still quite large.
This motivates us to experiment with various perturbation schedules and gradient clipping, and study their
effects not only on the performance (mean ±standard error) but also the dynamics of iterates in Sections
6.1 and 6.2.
23