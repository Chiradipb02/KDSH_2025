Published in Transactions on Machine Learning Research (06/2023)
POMRL: No-Regret Learning-to-Plan with Increasing
Horizons
Khimya Khetarpal∗†khimya.khetarpal@mail.mcgill.ca
Department of Computer Science
McGill University
Claire Vernade∗‡claire.vernade@gmail.com
University of Tuebingen
Brendan O’Donoghue bodonoghue@google.com
Google Deepmind
Satinder Singh Baveja baveja@google.com
Google Deepmind
Tom Zahavy tomzahavy@google.com
Google Deepmind
Reviewed on OpenReview: https: // openreview. net/ forum? id= brGgOAXYtr
Abstract
We study the problem of planning under model uncertainty in an online meta-reinforcement
learning (RL) setting where an agent is presented with a sequence of related tasks with
limited interactions per task. The agent can use its experience in each task andacross
tasks to estimate both the transition model and the distribution over tasks. We propose an
algorithm to meta-learn the underlying relatedness across tasks, utilize it to plan in each
task, and upper-bound the regret of the planning loss. Our bound suggests that the average
regret over tasks decreases as the number of tasks increases and as the tasks are more similar.
In the classical single-task setting, it is known that the planning horizon should depend on
the estimated model’s accuracy, that is, on the number of samples within task. We generalize
this ﬁnding to meta-RL and study this dependence of planning horizons on the number of
tasks. Based on our theoretical ﬁndings, we derive heuristics for selecting slowly increasing
discount factors, and we validate its signiﬁcance empirically.
1 Introduction
Meta-learning (Caruana, 1997; Baxter, 2000; Thrun & Pratt, 1998; Finn et al., 2017; Denevi et al., 2018)
oﬀers a powerful paradigm to leverage past experience to reduce the sample complexity of learning future
related tasks. Online meta-learning considers a sequential setting, where the agent progressively accumulates
knowledge and uses past experience to learn good priors and to quickly adapt within each task Finn et al.
(2019); Denevi et al. (2019). Robots acting in real world for instance need to be responsive to and robust
against perturbation inherent in the environment dynamics and their decision making. When the tasks share
a structure i.e. have similar transition dynamics and are related, such approaches enable progressively faster
convergence, or equivalently better model accuracy with better sample complexity (Schmidhuber & Huber,
1991; Thrun & Pratt, 1998; Baxter, 2000; Finn et al., 2017; Balcan et al., 2019).
∗Equal Contribution
†Work partially done during an internship at DeepMind. Now at Deepmind.
‡Work partially done at DeepMind.
1Published in Transactions on Machine Learning Research (06/2023)
In model-based reinforcement learning (RL), the agent uses an estimated model of the environment to plan
actions ahead towards the goal of maximizing rewards. A key component in the agent’s decision making is
the horizon used during planning. In general, an evaluation horizon is imposed by the task itself, but the
learner may want to use a diﬀerent and potentially shorter guidance horizon . In the discounted setting, the
size of the evaluation horizon is of order (1−γeval)−1, for some discount factor γeval∈(0,1), and the agent
may useγ/negationslash=γevalfor planning. For instance, a classic result known as Blackwell Optimality (Blackwell,
1962) states there exists a discount factor γ⋆and a corresponding optimal policy such that the policy is also
optimal for any greater discount factor γ≥γ⋆.Thus, an agent that plans with γ=γ⋆will be optimal for
anyγeval>γ⋆.In the Arcade Learning Environment (Bellemare et al., 2013) a discount factor of γeval= 1is
used for evaluation, but typically a smaller γis used for training (Mnih et al., 2015). Using a smaller discount
factor acts as a regularizer (Amit et al., 2020; Petrik & Scherrer, 2008; Van Seijen et al., 2009; François-Lavet
et al., 2019; Arumugam et al., 2018) and reduces planner over-ﬁtting in random MDPs (Arumugam et al.,
2018). Indeed, the choice of planning horizon plays a signiﬁcant role in computation (Kearns et al., 2002),
optimality (Kocsis & Szepesvári, 2006), and on the complexity of the policy class (Jiang et al., 2015). In
addition, meta-learning discount factors has led to signiﬁcant improvements in performance (Xu et al., 2018;
Zahavy et al., 2020; Flennerhag et al., 2021; 2022; Luketina et al., 2022).
When doing model-based RL with a learned model, the optimal guidance planning horizon, called eﬀective
horizon by Jiang et al. (2015), depends on the accuracy of the model, and so on the amount of data used to
estimate it. Jiang et al. (2015) show that when data is scarce, a guidance discount factor γ <γ evalshould be
preferred for planning. The reason for this is straightforward; if the model used for planning is inaccurate,
then errors will tend to accumulate along the planned trajectory. A shorter eﬀective planning horizon will
accumulate less error and may lead to better performance, even when judged using the true γeval. While
that work treated only the batch, single-task setting, the question of eﬀective planning horizon remains open
in the online meta-learning setting where the agent accumulates knowledge from many tasks, with limited
interactions within each task.
In this work, we consider a meta-reinforcement-learning problem made of a sequence of related tasks . We
leverage this structural task similarity to obtain model estimators with faster convergence as more tasks are
seen. The central question of our work is:
Can we meta-learn the model across tasks and adapt the eﬀective planning horizon accordingly?
We take inspiration from the Average Regret-Upper-Bound Analysis [ARUBA] framework (Khodak et al., 2019)
to generalize planning loss bounds to the meta-RL setting. A high-level, intuitive outline of our approach is
presented in Fig. 1. Our main contributions are as follows:
•We formalize planning in a model-based meta-RL setting as an average planning loss minimization
problem, and we propose an algorithm to solve it.
•Under a structural task-similarity assumption, we prove a novel high-probability task-averaged regret
upper-bound on the planning loss of our algorithm, inspired by ARUBA. We also demonstrate a
way to learn the task-similarity parameter σon-the-ﬂy. To the best of our knowledge, this is a ﬁrst
formal (ARUBA-style) analysis to show that meta-RL can be more eﬃcient than RL.
•Our theoretical result highlights a new dependence of the planning horizon on the size of the within-
task datamandon the number of tasks T. This observation allows us to propose two heuristics to
adapt the planning horizon given the overall sample-size.
2 Preliminaries
ReinforcementLearning. We consider tabular Markov Decision Processes (MDPs) M=/angbracketleftS,A,R,P,γ eval/angbracketright,
whereSis a ﬁnite set of states, Ais a ﬁnite set of actions and we denote the set cardinalities as S=|S|and
A=|A|. For each state s∈S, and for each available action a∈A, the probability vector P(·|s,a)deﬁnes a
transition model over the state space and is a probability distribution in a set of feasible models DP⊂∆S,
where ∆Sthe probability simplex of dimension S−1. We denote Σ≤1the diameter ofDP. A policy is a
functionπ:S→Aand it characterizes the agent’s behavior.
2Published in Transactions on Machine Learning Research (06/2023)
Gr o wing Planning Horizon Online Me ta-L earning K e y- T ak ea w a y s 
T ask-Similarity 
m  s amples  
per task 
T T ask s 
Figure 1:Eﬀective Planning Horizons in Meta-Reinforcement Learning. The agent faces a sequence
of tasks with transition vector (Pt)t∈[T](probability vectors represented by blue dots) all close to each other
(σ<Σ = 1). The agent builds a transition model for each task and plans with these inaccurate models. By
using data from previous tasks, the agent meta-learns an initialization of the model ( ˆPo,t), which leads to
better planning in new related but unseen tasks. We show an improved average regret upper bound that
scales with task-similarity parameter σand inversely with the number of tasks T: as knowledge accumulates,
uncertainty diminishes, and the agent can plan with longer horizons. All tasks Pt∼Pare centered at some
ﬁxed but unknown Po, depicted here by the shaded red dot and pointed by the arrow.
We consider the bounded reward setting, i.e.,R∈[0,Rmax]and without loss of generality we set Rmax= 1
(unless stated otherwise). Given an MDP, or task, M, for any policy π, letVπ
M,γ∈RSbe the value
function when evaluated in MDP Mwith discount factor γ∈(0,1)(potentially diﬀerent from γeval);
deﬁned asVπ
M,γ(s) =E/summationtext∞
t=0(γtRst|s0=s). The goal of the agent is to ﬁnd an optimal policy, π⋆
M,γ=
arg maxπEs∼ρVπ
M,γ(s)whereρ>0is any positive measure, denoted π⋆when there is no ambiguity. For given
state and action spaces and reward function (S,A,R), we denote Πγthe set of potentially optimal policies for
discount factor γ:Πγ={π|∃Ps.t.π=π⋆
M,γwhereM=/angbracketleftS,A,R,P,γ/angbracketright}. We use Big-O notation, O(·)and
˜O(·), to hide respectively universal constants and poly-logarithmic terms in T,S,Aandδ>0(the conﬁdence
level).
Model-based Reinforcement Learning. In practice, the true model of the world is unknown and must
be estimated from data. One approach to approximately solve the optimization problem above is to construct
a model,/angbracketleftˆR,ˆP/angbracketrightfrom data, then ﬁnd π⋆
ˆM,γfor the corresponding MDP ˆM=/angbracketleftS,A,ˆR,ˆP,γ/angbracketright. This approach is
calledmodel-based RL orcertainty-equivalence (CE) control .
Planning with inaccurate models. In this setting, Jiang et al. (2015) deﬁne the planning loss as the gap
in expected return in MDP Mwhen using γ≤γevaland the optimal policy for an approximate model ˆM:
L(ˆM,γ|M,γ eval) =/bardblVπ⋆
M,γ eval
M,γ eval−Vπ⋆
ˆM,γ
M,γ eval/bardbl∞.
Thus, theoptimaleﬀectiveplanninghorizon (1−γ⋆)−1is deﬁned using the discount factor that minimizes
the planning loss, i.e.,γ⋆:= min 0≤γ≤γevalL(ˆM,γ|M,γ eval).
Theorem 1. (Jiang et al. (2015)) Let Mbe an MDP with non-negative bounded rewards and evaluation
discount factor γeval. Let ˆMbe the approximate MDP comprising the true reward function of M and the
approximate transition model ˆP, estimated from m > 0samples for each state-action pair. Then, with
probability at least 1−δ,
/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleVπ⋆
M,γ eval
M,γ eval−Vπ⋆
ˆM,γ
M,γ eval/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
∞≤γeval−γ
(1−γeval)(1−γ)+2γRmax
(1−γ)2/parenleftBigg/radicalbigg
Σ
2mlog2SA|Πγ|
δ/parenrightBigg
(1)
where Σis upper-bounded by 1 as P,ˆP∈∆S.
This result holds for a count-based model estimator (i.e, empirical average of observed transitions) given
by a generator model for each pair (s,a). It gives an upper-bound on the planning loss as a function of the
3Published in Transactions on Machine Learning Research (06/2023)
 m = 5
 m = 10 m = 20 m = 50
Figure 2: On the role of incorporating a ground truth prior of transition model on planning
horizon. The planning loss is a function of the discount factor γand is impacted by incorporating prior
knowledge. The learner has m= 5,10,20,50samples per task to estimate the model, corresponding to the
curves in each sub ﬁgure. Inspecting any of the sub ﬁgures, we observe that larger values of mlead to lower
planning loss and a larger eﬀective discount factor. Besides, inspecting one value of macross tasks (e.g.,
m= 5), we see that the same eﬀect (lower planning loss and larger eﬀective discount) occurs when the
learner puts more weight on the ground truth prior through α.
guidance discount factor γ <1. The result decomposes the loss into two terms: the constant bias which
decreases as γtends toγeval, and the variance (or uncertainty) term which increases with γbut decreases as
1/√m. Asm→∞that second factor vanishes, but in the low-sample regime the optimal eﬀective planning
horizon should trade-oﬀ both terms.
Illustration. These eﬀects are illustrated in Fig. 2 on a simple 10-state, 2-action random MDP. The leftmost
plot uses the simple count-based model estimator and reproduces the results from Jiang et al. (2015). We
then incorporate the true prior (mean model Poas in Fig 1 and deﬁned above Eq. 3 in Assumption 1) in the
estimator with a growing mixing factor α∈(0,1):ˆP(m) =αPo+ (1−α)/summationtextiXi
m. We observe that increasing
the weight α∈(0,1)on good prior knowledge enables longer planning horizons and lower planning loss.
Online Meta-Learning and Regret. We consider an online meta-RL problem where an agent is presented
with a sequence of tasks M1,M2,...,MT, where for each t∈[T],Mt=/angbracketleftS,A,Pt,R,γ eval/angbracketright, that is, the MDPs
only diﬀer from each other by the transition matrix (dynamics model) Pt. The learner must sequentially
estimate the model ˆPtfor each task tfrom a batch of mtransitions simulated for each state-action pair1.
Its goal is to minimize the average planning loss also expressed in the form of task averaged regret suﬀered in
planning and deﬁned as
¯L(ˆM1:T,γ|M1:T,γeval) =1
TT/summationdisplay
t=1L(ˆMt,γ|Mt,γeval) =1
TT/summationdisplay
t=1/bardblVπ⋆
Mt,γeval
Mt,γeval−Vπ⋆
ˆMt,γ
Mt,γeval/bardbl∞ (2)
Note that the reference MDP for each term is the true Mt, and the discount factor γis the same in all tasks.
One can see this objective as a stochastic dynamic regret: at each task t∈[T], the learner competes against
the optimal policy for the currenttrue model, as opposed to competing against the best ﬁxed policy in
hindsight used in classical deﬁnitions of regret.
Note that our dynamic regret is diﬀerent from the one considered in ARUBA (Khodak et al.,
2019). They consider the fully online setting where the data is observed as an arbitrary stream within each
task, and each comparator is simply the minimum of the within-task loss in hindsight. In our model, however,
given access to a simulator (See Sec. 2) allows us to get i.i.d transition samples as a batch at the beginning of
each task, and consequently we deﬁne our regret with respect to the true generating parameter. One key
consequence of this diﬀerence is that their regret bounds cannot be directly applied to our setting, and we
prove new results further below.
1So a total of mSAsamples.
4Published in Transactions on Machine Learning Research (06/2023)
3 Planning with Online Meta-Reinforcement Learning
We here formalize planning in a model-based meta-RL setting. We start by specifying all our assumptions in
Sec 3.1 including our main assumption about task relatedness in Sec. 1, present our approach and explain the
proposed algorithms POMRLandada-POMRL in Sec. 3.2. Our main result is a high-probability upper bound on
the average planning loss under the assumed task relatedness, presented as Theorem 2.
3.1 Assumptions
In many real world scenarios such as robotics, it is required to be responsive to changes in the environment
and, at the same time, to be robust against perturbation inherent in the environment and their decision
making. In such practical scenarios, the key reason to employ meta-learning is for the learner to leverage
task-similarity (or task variance) across tasks. Bounded task similarity is becoming a core assumption in
the analysis of recent meta learning (Khodak et al., 2019) and multi-task (Cesa-Bianchi et al., 2021) online
learning algorithms.
Assumption 1 (Structural Assumption Across Tasks: Task Relatedness) .In this work, we exploit the
structural assumption that for all t∈[T],Pt∼Pcentered at some ﬁxed but unknown Po∈∆S×A
Sand such
that for any (s,a),
/bardblPt
s,a−Po
s,a/bardbl∞≤σ= max
(s,a)σ(s,a)a.s. (3)
This also implies that maxt,t/prime/bardblPt
s,a−Pt/prime
s,a/bardbl∞≤2σ, and that the meta-distribution Pis bounded within a
small subset of the simplex. It is immediate to extend our results under a high-probability assumption instead
of the almost sure statement above. In our experiments, we will use Gaussian or Dirichlet priors over the
simplex, whose moments are bounded with high-probability, not almost surely. Importantly, we will say that
a multi-task environment is strongly structured whenσ<Σ,i.e.when the eﬀective diameter of the models is
smaller than that of the entire feasible space.
Assumption 2 (Access to a Simulator) .We assume that for each task t∈[T]we have access to a simulator
of transitions (Kearns et al., 2002) providing mi.i.d. samples (Xt,i
s,a)i=1..m∈Sm∼Pt(·|s,a)(categorical
distribution).
Next, for simplicity we assume throughout that the rewards are known and focus on learning and planning
with an approximate dynamics model. Additionally estimating the reward is a straightforward extension of
our analysis and would not change the implications of our main result.
Assumption 3 (Known Rewards) .Given a distribution of tasks, we assume that the rewards are known.
3.2 Our Approach
With access to a simulator (Assumption 2); for each (s,a), we can compute an empirical estimator for
eachs/prime∈[S]:¯Pt
s,a(s/prime) =/summationtextm
i=11{Xt,i
s,a=s/prime}/m, with naturally/summationtext
s/prime¯Pt
s,a(s/prime) = 1. We perform meta-RL via
alternating minimizing a batch within-task regularized least-squares loss, and an outer-loop step where we
optimize the regularization to optimally balance bias and variance of the next estimator.
Estimating dynamics model via regularized least squares. We adapt the standard technique of
meta-learned regularizer (see e.g. Baxter (2000); Cella et al. (2020) for supervised learning and bandit
respectively) to this model estimation problem. At each round t, thecurrent model ˆPt
(s,a)is estimated
by minimizing a regularized least square loss : for a given regularizer ht(to be speciﬁed below)2and
parameterλt>0for each (s,a)∈S×A we solve
ˆPt
(s,a)= arg min
P(s,a)∈∆S/vextenddouble/vextenddouble/vextenddouble1
mm/summationdisplay
i=11{Xt,i
s,a}
/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
empirical transition prob.−P(s,a)/vextenddouble/vextenddouble/vextenddouble2
2+λt/bardblP(s,a)−ht/bardbl2
2, (4)
2In principle, this loss is well deﬁned for any regularizer htbut we specify a meta-learned one and prove that it induces good
performance.
5Published in Transactions on Machine Learning Research (06/2023)
where we use 1{Xt,i
s,a}to denote the one-hot encoding of the state into a vector in RS. Importantly, htand
λtare meta-learned in the outer-loop (see below) and aﬀect the bias and variance of the resulting estimator.
The solution of equation 4 can be computed in closed form as a convex combination of the empirical average
(count-based) and the prior: ˆPt=αtht+ (1−αt)¯Ptwhereαt=λt
1+λtis the current mixing parameter.
Outer-loop: Meta-learning the regularization. At the beginning of task 1<t≤T, the learner has
already observed t−1related but diﬀerent tasks. We deﬁne htas anaverage of Means (AoM) :
ht
(s,a)←ˆPo,t
(s,a)=1
t−1t−1/summationdisplay
j=1/summationtextm
i=11{Xj,i
(s,a)}
m:=1
t−1t−1/summationdisplay
j=1¯Pj
(s,a). (5)
Deriving the mixing rate. To setαt, we compute the Mean Squared Error (MSE) of ˆPt
(s,a), and minimize
an upper bound (see details in Appendix B): MSE (ˆPt
(s,a))≤α2
tσ2(1 +1
t) + (1−αt)21
m, which leads to
αt=1
σ2(1+1/t)m+1.
Algorithm 1 depicts the complete pseudo code. We note here that POMRL(σ) assumes, for now, that the
underlying task-similarity parameter σis known, and we discuss a fully empirical extension further below
(See Sec. 4). The learner does not know the number of tasks a priori and tasks are faced sequentially online.
The learner performs meta-RL alternating between within-task estimation of the dynamics model ˆPtvia a
batch ofmsamples for that task, and an outer loop step to meta-update the regularizer ˆPo,t+1alongside the
mixing rate αt+1. For each task, we use a γ-Selection-Procedure to choose planning horizon γ∗≤γeval.
We defer the details of this step to Sec. 6 as it is non-trivial and only a partial consequence of our theoretical
analysis. Next, the learner performs planning with an imperfect model ˆPt. For planning, we use dynamic
programming, in particular policy iteration (a combination of policy evaluation, and improvement), and value
iteration to obtain the optimal policy π⋆
ˆPt,γ∗for the corresponding MDP ˆMt.
Algorithm 1: POMRL(σ) – Planning with Online Meta-Reinforcement Learning
Input:Given task-similarity (σ(s,a))a matrix of size S×A. Initialize ˆPo,1to uniform, α1= 0.
fortaskt∈[T]do
fortthbatch ofmsamplesdo
ˆPt(m) = (1−αt)1
m/summationtextm
i=1Xi+αtˆPo,t//regularized least squares minimizer.
γ⋆← −γ-Selection-Procedure (m,αt,σ,T,S,A )
π⋆
ˆPt,γ∗←Planning (ˆPt(m))//
Output:π⋆
ˆPt,γ∗
Update ˆPo,t+1,αt+1=1
σ2(1+1/t)m+1//meta-update AoM (Eq. 5) and mixing rate
3.3 Average Regret Bound for Planning with Online-meta-learning
Our main theoretical result below controls the average regret of POMRL (σ), a version of Alg. 1 with additional
knowledge of the underlying task relatedness, i.e., the trueσ>0.
Theorem 2. Using the notation of Theorem 1, we bound the average planning loss equation 2 for POMRL (σ):
¯L≤γeval−γ
(1−γeval)(1−γ)+2γS
(1−γ)2˜O
σ+/radicalBig
1
T/parenleftbigg
σ+/radicalBig
σ2+Σ
m/parenrightbigg
σ2m+ 1+σ2m/radicalBig
Σ
m
σ2m+ 1
(6)
with probability at least 1−δ, whereσ2<1is the measure of the task-similarity and σ= max (s,a)σ(s,a).
The proof of this result is provided in Appendix D and relies on a new concentration bound for the meta-
learned model estimator. The last term on the r.h.s. corresponds to the uncertainty on the dynamics.
6Published in Transactions on Machine Learning Research (06/2023)
First we verify that if T= 1andmgrows large, the second term dominates and is equivalent to ˜O(/radicalBig
Σ
m)
(asσ2/(σ2m+ 1)→0), which is similar to that of Jiang et al. (2015) as there is no meta-learning, with
an additional O(1
m)but second order term due to the introduced bias. Then, if mis ﬁxed and small, for
small enough values of σ2(typicallyσ < 1/√m), the ﬁrst term dominates and the r.h.s. boils down to
˜O/parenleftBig
(σ+1√m)/√
T/parenrightBig
. This highlights the interplay of our structural assumption parameter σand the amount
of datamavailable at each round. The regimes of the bound for various similarity levels are explored
empirically in Sec. 5 (Q3). We also show the dependence of the regret upper bound on mandTfor a ﬁxedσ,
in Appendix Fig. F3.
Implications for degree of task-similarity i.e.,σvalues. Our bound suggests that the degree of
improvement you can get from meta learning scales with the task similarity σinstead of the set size Σ. Thus,
forσ≤Σ, performing meta learning with Algorithm 1 guarantees better learning measured via our improved
regret bound when there is underlying structure in the problem space which we formalize through Eq. 3.
Shouldσbe large, the techniques will still hold and our bounds will simply scale accordingly.
Whenσ= 0, all tasks are exactly the same. Indeed, the mixing rate αt≈1for allt, so our algorithm
boils down to returning the average of means ˆPo,tfor each task, which simply corresponds to solving the
tasks as a continuous, uninterrupted stream of batches from the nearly same model that ˆPo,taggregates.
Unsurprisingly, our bound recovers that of (Jiang et al., 2015, Theorem 1): the bound below reﬂects that we
have to estimate only one model in a space of “size” ΣwithmTsamples.
¯L≤γeval−γ
(1−γeval)(1−γ)+2γS
(1−γ)2˜O/parenleftBigg/radicalbigg
Σ
mT/parenrightBigg
(7)
Whenσ= 1, thenσ= Σ = 1, then the meta-learning assumption is not relevant but our bound
remains valid and gracefully degrades to reﬂect it. We need to estimate Tmodels each with m
samples. Then the second term1√mreﬂects the usual estimation error for each task while the ﬁrst term is an
added bias (second order in1
m) due to our regularization to our mean prior Pothat is not relevant here.
¯L≤γeval−γ
(1−γeval)(1−γ)+2γS
(1−γ)2˜O/parenleftBig1
m/parenleftBig
1 +1√
T(1 +/radicalbigg
1 +1
m)/parenrightBig
+1√m/parenrightBig
(8)
Connections to ARUBA. As explained earlier, our metric is not directly comparable to that of
ARUBA (Khodak et al., 2019) but it is interesting to make a parallel with the high-probability aver-
age regret bounds proved in their Theorem 5.1. They also obtain an upper bound in ˜O(1/√m+ 1/√
mT)if
one upper bounds their average within-task regret ¯U≤B√m.
Remark 1 (Role of the task similarity σin Eq. 2).Whenσ>0,POMRLnaturally integrates each new data
batch into the model estimation. The knowledge of σis necessary to obtain this exact and intuitive update
rule, and our theory only covers POMRLequipped with this prior knowledge, but we discuss how to learn and
plug-in ˆσtin practice. Note that it would be possible to extend our result to allow for using the empirical
variance estimator with tools like the Bernstein inequality, but we believe this it out of the scope of this work
as it would essentially give a similar bound as obtained in Theorem 2 with an additional lower order term in
O(1/T), and it would not provide much further intuition on the meta-planning problem we study.
4 Practical Considerations: Adaption On-The-Fly
In this section we propose a variant of POMRLthat meta learns the task similarity parameter, which we call
ada-POMRL . We compare the two algorithms empirically in a 10 state, 2 action MDP with closely related
tasks with a total of T= 15tasks (details of the experiment setup are deferred to Sec. 5).
Performance of POMRL.Recall that POMRLis primarily learning the regularizer and assumes the knowledge
of the underlying task similarity (i.e. σ). We observe in Fig. 3 that with each round t∈TPOMRLis able to
plan better as it learns and adapts the regularizer to the incoming tasks. The convergence rate and ﬁnal
performance corroborates with our theory.
7Published in Transactions on Machine Learning Research (06/2023)
Can we also meta-learn the task-similarity parameter? In practice, the parameter σ
may not be known and must be estimated online and plugged in (see Appendix C for details).
123456789101112131415
Task (T)0.00.51.01.52.02.53.03.54.0Planning Loss°=0:99=°eval 
 ada-POMRL
POMRL(¾)
Figure 3: ada-POMRL enables
meta-learning the task-similarity
on-the-ﬂywithaperformancegap
for the initial set of tasks as com-
pared to the oracle POMRL, but
improves with more tasksAlg. 2 ada-POMRL uses Welford’s algorithm to compute an online estimate
of the variance after every task using the model estimators, and simply
plugs-in this estimate wherever POMRLwas using the true value. From
the perspective of ada-POMRL ,POMRLis an "oracle", i.e. the underlying
task-similarity is known. However, in most practical scenarios, the learner
does not have this information a priori.
We compare empirically POMRLand ada-POMRL on a strongly structured
problem (σ≈0.01) in Fig. 3 and observe that meta-learning the under-
lying task relatedness allows ada-POMRL to adapt to the incoming tasks
accordingly. Adaptation on-the-ﬂy with ada-POMRL comes at a cost i.e., the
performance gap in comparison to POMRLbut eventually converges albeit
with a slower rate. This is intuitive and a similar theoretical guarantee
applies (See Remark 1).
This online estimation of σmeans that ada-POMRL now requires an initial
value for ˆσ1, which is a choice left to the practitioner, but will only aﬀect the results of a ﬁnite number of
tasks at the beginning. Using ˆσ1too small will give a slightly increased weight to the prior in initial tasks,
which is not desirable as the latter is not yet learned and will result in an increased bias. On the other
hand, setting ˆσ1too large (i.e close to 1/2) will decrease the weight of the prior and increase the variance of
the returned solution; in particular, in cases where the true σis small, a large initialization will slow down
convergence and we observe empirical larger gaps between POMRLandada-POMRL . In the extreme case where
σ≈0, a large initialization will drastically slow down ada-POMRL as it will take many tasks before it discovers
that the optimal behavior is essentially to aggregate the batches.
Algorithm 2: ada-POMRL – Planning with Online Meta-Reinforcement Learning
Input:Initialize ˆPo,1to uniform, (ˆσ)1as a matrix of size S×A,α1= 0.
fortaskt∈[T]do
fortthbatch ofmsamplesdo
ˆPt(m) = (1−αt)1
m/summationtextm
i=1Xi+αtˆPo,t//regularized least squares minimizer.
γ⋆← −γ-Selection-Procedure (m,αt,σt,T,S,A )
π⋆
ˆPt,γ⋆←Planning (ˆPt(m))
Output:π⋆
ˆPt,γ⋆
Update ˆPo,t+1,ˆσt+1← −Welford’s online algorithm/parenleftBig
( ˆσo)t,ˆPo,t+1,ˆPo,t/parenrightBig
//meta-update AoM
(Eq. 5) and task-similarity parameter.
Updateαt+1=1
ˆσt+12(1+1/t)m+1//meta-update mixing rate, plug max(σS×A)
Tasks vary only in certain states and actions. Thus far, we considered a uniform notion of task
similarity as Eq. 3 holds for any (s,a). However, in many practical settings the transition distribution might
remains the same for most part of the state space but only vary on some states across diﬀerent tasks. These
scenarios are hard to analyse in general because local changes in the model parameters do not always imply
changes in the optimal value function nor necessarily modify the optimal policy. Our Theorem 2 still remains
valid, but it may not be tight when the meta-distribution has non-uniform noise levels. More precisely
Theorem 1 in Appendix D remains locally valid for each (s,a)pair and one could easily replace the uniform
σwith localσ(s,a), but this cannot directly imply a stronger bound on the average planning loss. Indeed, in
our experiments, in both POMRLandada-POMRL , the parameter σandˆσrespectively, are S×Amatrices of
state-action dependent variances resulting in state-action dependent mixing rate αt.
8Published in Transactions on Machine Learning Research (06/2023)
5 Experiments
We now study the empirical behavior of planning with online meta-learning in order to answer the following
questions: Q1.Does meta-learning a good initialization of the dynamics model facilitate improved planning
accuracy for the choice of γ=γeval? (Sec. 5.1) Q2.Does meta-learning a good initialization of the dynamics
model enables longer planning horizons? (Sec. 5.2) Q3.How does performance depend on the amount of
shared structure across tasks i.e.,σ? (Sec. 5.3) Source code is provided in the supplementary material.
Setting: For each experiment, we ﬁx a mean model Po∈∆S×A
S(see below how), and for each new task
t∈[T], we sample Ptfrom a Dirichlet distribution3centered at Po. As prescribed by theory (see Sec.3.2),
we set4σ≈0.01.1/S√munless otherwise speciﬁed (see Q3). Note that σandˆσrespectively, are S×A
matrices of state-action dependent variances that capture the directional variance as we used Dirichlet
distributions as priors and these have non-uniform variance levels in the simplex, depending on how close
to the simplex boundary the mean is located. Aligned with our theory, we use the max of the σmatrices
resulting in the aforementioned single scalar value. As in Jiang et al. (2015), Po(and eachPt) characterizes
a random chain MDP with S= 10states5andA= 2actions, which is drawn such that, for each state–action
pair, the transition function P(s,a,s/prime)is constructed by choosing randomly k= 5states whose probability is
set to 0. Then we draw the value of the S−kremaining states uniformly in [0,1]and normalize the resulting
vector.
5.1 Meta-reinforcement learning leads to improved planning accuracy for [ γeval]. [Q1.]
We consider the aforementioned problem setting with a total of T= 15closely related tasks and focus on the
planning loss gains due to improved model accuracy. We ﬁx γ=γeval, a rather naive γ-Selection-Procedure
and show the planning loss of POMRL(Alg. 1) with the following baselines : 1)Oracle Prior Knowledge
knows a priori the underlying task structure ( Po,σ) and uses an estimator (Eq. 4) with exact regularizer
Poand optimal mixing rate αt=1
σ2(1+1/t)m+1, 2)Without Meta-Learning simply uses ˆPt=¯Pt, the
count-based estimated model using the msamples seen in each task, 3) POMRL(Alg. 1) meta-learns the
regularizer but knows apriori the underlying task structure, and 4) ada-POMRL (Alg. 2) meta-learns not only
the regularizer, but also the underlying task-similarity online. The oracle is a strong baseline that provides
a minimally inaccurate model and should play the role of an "empirical lower bound". For all baselines,
the number of samples per task m= 5. Results are averaged over 100independent runs. Besides, we also
propose and empirically validate competitive heuristics for γ-Selection-Procedure in Sec. 6. Besides, we
also run another baseline called Aggregating( α= 1), that simply ignores the meta-RL structure and just
plans assuming there is a single task (See Appendix F.2).
Inspecting Fig. 4(a) , we can see that our approach ada-POMRL (green) results in decreasing per-task
planning loss as more tasks are seen, and decreasing variance as the estimated model gets more stable and
approaches the optimal value returned by the oracle prior knowledge baseline (blue). On the contrary, without
meta-learning (red), the agent struggles to cope as it faces new tasks every round, and its performance does not
improve. ada-POMRL gradually improves as more tasks are seen whilst adaptation to learned task-similarity
on-the-ﬂy which is the primary cause of the performance gap in ada-POMRL and POMRL. Importantly, no
prior knowledge about the underlying task relatedness enables a more practical algorithm with the same
theoretical guarantees (See Sec. 4). Recall that oracle prior knowledge is a strong baseline as it corresponds
to both known task relatedness and regularizer.
5.2 Meta-learning the underlying task relatedness enables longer planning horizons. [Q2.]
We run ada-POMRL forT= 15(withσ≈0.01)as above and report planning losses for a range of values of
guidanceγfactors. Results are averaged over 100independent runs and displayed on Fig. 4(b). We observe
3The variance of this distribution is controlled by its coeﬃcient parameters α1:S: the larger they are, the smaller is the variance.
More details on our choices are given in Appendix F.1. Dirichlet distributions with small variance satisfy the high-probability
version of our structural assumption 3 for σ= max iσi
4Our priors are multivariate Dirichlet distribution in dimension Sso we divide the theoretical rate by Sto ensure the max
bounded by 1/√m. See App. F for implementation details.
5We provide additional experiments with varying size of the state space in Appendix Fig. F5.
9Published in Transactions on Machine Learning Research (06/2023)
123456789101112131415
Task (T)0.00.51.01.52.02.53.03.54.0Planning Loss°=0:99=°eval 
ada-POMRL
Oracle Prior Knowledge
Without Meta-Learning
POMRL(¾)
(a)
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.91.0
°0.00.51.01.52.02.53.03.54.0Planning LossT=1 T=3 T=6 T=15 (b)
123456789101112131415
Task T0.00.20.40.60.81.0Planning Loss
°eval=0:99 Planning Loss 
0.00.20.40.60.81.0
Empircally optimal guidance 
 discount factorOptimal Gamma (°⋆) (c)
Figure 4: Planning with Online Meta-Learning. (a) Per-task planning loss of our algorithms
POMRLandada-POMRL compared to an Oracle, and Without Meta-learning baselines. All methods use a ﬁxed
γ=γeval= 0.99.(b)ada-POMRL ’s planning loss decreases as more tasks are seen. Markers denote the
γthat minimizes the planning loss in respective tasks. Error bars show standard error. (c)ada-POMRL ’s
empirically optimal guidance discount factor (right y axis) depicts the eﬀective planning horizon, i.e.,
one that minimizes the planning loss. Optimal γaka the eﬀective planning horizon is larger with online
meta-learning. Planning loss (left y axis) shows the minimum planning loss achieved by the agent in that
roundT. Results are averaged over 100independent runs and error bars represent 1-standard deviation.
in Fig. 4(b) when the agent has seen fewer tasks T, an intermediate value of the discount is optimal, i.e., one
that minimizes the task-averaged planning loss (γ⋆<0.5). In the presence of strong underlying structure
across tasks, as the agent sees more tasks, the eﬀective planning horizon (γ⋆>0.7)shifts to a
larger value - one that is closer to the gamma used for evaluation (γeval= 0.99).
As we incorporate the knowledge of the underlying task distribution, i.e., meta-learned initialization of
the dynamics model, we note that the adaptive mixing rate αtputs increasing amounts of weight on the
shared task-knowledge. Note that this conforms to the eﬀect of increasing weight on the model initialization
that we observed in Fig. 2. As predicted by theory, the per-task planning loss decreases as Tgrows and
is minimized for progressively larger values of γ, meaning for longer planning horizons (See Fig. 4(c)). In
addition, Appendix Fig. F4 depicts the eﬀective planning horizon individually for ada-POMRL , Oracle and
without meta learning baselines.
5.3 POMRL and ada-POMRL perform consistently well for varying task-similarity. [Q3.]
We have thus far studied scenarios where the learner can exploit strong task relatedness, i.e.,σ≈0.01<
1/(S√m)(for low data per task i.e.,m= 5) is small and we now illustrate the other regimes discussed in
Section 3.2. We show that our algorithms remain consistently good for all amounts of task-similarity.
We letσvary to cover the three regimes :σ≈0.01corresponding to fast convergence, σ= 0.025is in the
intermediate regime (needs longer T), andσ= 0.047is the loosely structured case where we don’t expect
much meta-learning to help improve model accuracy. The small inset ﬁgures in Fig. 5 represent the task
distribution in the simplex. In all cases, ada-POMRL estimatesσonline and we report the planning losses for
a range ofγ’s. Inspecting Fig. 5, we observe that while in the presence of closely related tasks (Fig. 5(a))
all methods perform well (except without meta-learning). As the underlying task relatedness decreases (for
intermediate regime in Fig. 5(b)), both POMRLand ada-POMRL remain consistent in their performance as
compared to the Oracle Prior Knowledge baseline. When the underlying tasks are loosely related (as in
Fig. 5(c)), ada-POMRL andPOMRLcan still perform well in comparison to other baselines.
Next, we report and discuss the planning loss plot for ada-POMRL for the three cases are shown in Figures 5(d),
5(e), and 5(f) respectively. An intermediate value of task-similarity (Fig. 5(e)) still leads to gains, albeit at a
lower speed of convergence. In contrast, a large value of σ= 0.047indicates little relatedness across tasks
resulting in minimal gains from meta-learning here as seen in Fig. 5(f). The learner struggles to learn a good
initialization of the model dynamics as there is no natural one. All planning loss curves remain U-shaped and
overall higher with an intermediate optimal guidance γvalue ( 0.5). However, ada-POMRL does not do worse
overall than the initial run T= 1, meaning that while there is not a signiﬁcant improvement, our method
10Published in Transactions on Machine Learning Research (06/2023)
Strong Structure
123456789101112131415
Task (T)0.00.51.01.52.02.53.03.54.0Planning Loss°=0:99=°eval 
ada-POMRL
Oracle Prior Knowledge
Without Meta-Learning
POMRL(¾)
(a)Medium Structure
123456789101112131415
Task (T)0.00.51.01.52.02.53.03.54.0Planning Loss°=0:99=°eval 
ada-POMRL
Oracle Prior Knowledge
Without Meta-Learning
POMRL(¾)
(b)Loosely Structured
123456789101112131415
Task (T)0.00.51.01.52.02.53.03.54.0Planning Loss°=0:99=°eval 
ada-POMRL
Oracle Prior Knowledge
Without Meta-Learning
POMRL(¾)
(c)
3ODQQLQJ/RVV7 7 7 7 7 7 7 7 
(d)
3ODQQLQJ/RVV7 7 7 7 7 7 7 7 
 (e)
3ODQQLQJ/RVV7 7 7 7 7 7 7 7 
 (f)
Figure 5: POMRLand ada-POMRL are robust to varying task-similarity σfor a small ﬁxed amount of
datam= 5available at each round t∈T. A small value of σreﬂects the fact that tasks are closely related to
each other and share a good amount of structure whereas a much larger value indicates loosely related tasks
(simplex plots illustrate the meta-distribution in dimension 2). In the former case, meta-learning the shared
structure alongside a good model initialization leads to most gains. In the latter, the learner struggles to cope
with new unseen tasks which diﬀer signiﬁcantly. Error bars represent 1-standard deviation of uncertainty
across 100independent runs.
does not hurt performance in loosely related tasks6. Recall that ada-POMRL has no apriori knowledge of the
number of tasks ( T), or the underlying task relatedness ( σ)i.e., adaptation is on-the-ﬂy.
6 Adaptation of Planning Horizon γ
We now propose and empirically validate two heuristics to design an adaptive schedule for γbased on existing
work (Sec. 6.1) and on our average regret upper bound (Sec. 6.2).
6.1 Schedule adapted from Dong et al. (2021) [ γ=f(m,αt,σt,T)]
Dong et al. (2021) study a continuous, never-ending RL setting. They divide the time into growing phases
(Tt)t≥0, and tune a discount factor γt= 1−1/T1/5
t. We adapt their schedule to our problem, where the time
is already naturally divided into tasks: for each t≥0, we deﬁne the phase size Ttand the corresponding γtas
T0=m, Tt=SA
L/parenleftbig
(1−αt)m+αtm(t−1)/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
eﬃcient sample size/parenrightbig
, γt= 1−1
T1/5
t,
whereLis the maximum trajectory length. The size of each Tt,t≥1, is controlled by an "eﬃcient sample
size" which includes a combination of the current task’s samples and of the samples observed so far, as used
to construct our estimator in POMRL.
6.2 Using the upper bound to guide the schedule [ γ= min{1,γ0+ ˜γ}]
Having a second look at Theorem 2, we see that the r.h.s. is a function of γof the form
U:γ/mapsto→1
1−γeval+1
γ−1+Cm,T,S,A,σ,δγ
(1−γ)2,
6The theoretical bound may lead to think that the average planning loss is higher due to the introduced bias, but in practice
we do not observe that, which means our bound is pessimistic on the second order terms.
11Published in Transactions on Machine Learning Research (06/2023)
where the ﬁrst term is positive and monotonically decreasing on (0,γeval)and the second term is positive and
monotonically increasing on (0,1). We simplify and scale this constant, keeping only problem-related terms:
Ct= (1√
t(σ+1√m)/(σ2m+ 1) +σ2m1√m/(σ2m+ 1), which is of the order of the constant in equation 6.
Optimizing γby using the function Uwith constant Cdoes not lead to a principled analytical value strictly
speaking because Uis derived from an upper bound that may be loose and may not reﬂect the true shape
of the loss w.r.t. γ, but we may use the resulting growth schedule to guide our choices online. In general,
the existence of a strict minimum for Uin(0,1)is not always guaranteed: depending on the values of
C≈Cm,T,S,A,σ , the function may be monotonic and the minimum may be on the edges. We give explicit
ranges in the proposition below, proved in Appendix E.
Proposition 1. The existence of a strict minimum in (0,1)is determined by C=Cm,T,S,A,σ,δ (which can
be computed) as follows:
˜γ=

0ifC≥1
1ifC < 1/2
1−C
1+Cotherwise, i.e if 1/2<C < 1
We use these values as a guide. Typically, when T= 1andmis small, the multiplicative term Cis large and
the bound is not really informative (concentration has not happened yet), and γshould be small, potentially
close to but not equal to zero. As a heuristic, we propose to simply oﬀset ˜γby an additional γ0such that the
guidance discount factor is γ=min{1,γ0+˜γ}, whereγ0should be reasonably chosen by the practitioner to
allow for some short-horizon planning at the beginning of the interaction. Empirically, γ0=∈(0.25,0.50)
seems reasonable for our random MDP setting as it corresponds to the empirical minima on Fig 4(b).
123456789101112131415
Number of tasks (T)100Planning LossBest Fixed
Dynamic Best
°eval=0:99°=f(m;®t;¾t;T) 
°=min©
1;°0+~°ª
 
(a)
123456789101112131415
Number of tasks (T)100Planning LossBest Fixed
Dynamic Best°=f(m;®t;¾t;T) 
°=min©
1;°0+~°ª
 (b)
123456789101112131415
Number of tasks (T)100Planning Loss°=min©
1;°0+~°ª°0=0:35
°0=0:25°0=0:45
°0=0:50 (c)
Figure 6:Adapting the planning horizon during online meta-learning reduces planning loss. (a)
Planning with online-meta learning shows that allbaselines outperform using a constant discount factor.
(b)Zoomed in plot of average planning loss over the progression of tasks T shows competitive performance
with the proposed schedule of γ=f(m,αt,σt,T)beating best-ﬁxed as more tasks are seen. The γschedule
γ=min{1,γ0+˜γ}using the upper bound as a guidance beats the best-ﬁxed and is very competitive to the
dynamic-best baseline. (c)Using the upper bound to guide the schedule signiﬁcantly outperforms γevaland
is shown for γ0∈(0.25,0.50). Error bars depict 1-standard error for 600independent runs.
6.3 Empirical Validation
Next, we empirically test the proposed schedules for adaptation of discount factors. We consider the setup
described in Sec. 5 with 15tasks in a 10-state, 2-action random MDP distribution of tasks with σ≈0.01.
In Fig. 6, we plot the planning loss obtained by POMRLwith our schedules, a ﬁxed γevaland two strong
baselines: best ﬁxed which considers the best ﬁxed value of discount over all tasks estimated in hindsight and
dynamic best which considers the best choice if we had used the optimal γ⋆in each round as in Fig. 4(c). It
is important to note that dynamic best is a lower bound that we cannot outperform.
We observe in Fig. 6(a) that γevalresults in a very high loss, potentially corresponding to trying to plan too far
ahead despite model uncertainty. Upon inspecting Fig. 6(b), we observe that the proposed γ=f(m,αt,σt,T)
obtains similar performance to best ﬁxed and is within the signiﬁcance range of the lower bound. Our second
heuristic,γ=min{1,γ0+˜γ}obtains similarly good performance, as seen in Fig. 6(b). Fig. 6(c) shows the
eﬀect of diﬀerent values of γ0in the prescribed range. These results provide evidence that it is possible to
adapt the planning horizon as a function of the problem’s structure (meta-learned task-similarity) and sample
sizes. Adapting the planning horizon online is an open problem and beyond the scope of our work.
12Published in Transactions on Machine Learning Research (06/2023)
7 Discussion and Future Work
We presented connections between planning with inaccurate models and online meta-learning via a high-
probability task-averaged regret upper-bound on the planning loss that primarily depends on task-similarity
σas opposed to the entire search space Σ. Algorithmically, we demonstrate that the agent can use its
experience in each task andacross tasks to estimate both the transition model and the distribution over
tasks. Meta-learning the underlying task similarity and a good initialization of transition model across tasks
enables longer planning horizons.
Beyond the tabular case: Function approximation is at the heart of practical RL so a natural question
is how to extend our work to parametrized models. For linear MDPs, Müller & Pacchiano (2022) recently
derived regret bounds in the ﬁxed-horizon setting for an algorithm using meta-regularizers similar to ours.
One question is whether this idea could be extended to inﬁnite horizons and further to non-linear, richer
representations. Another, and perhaps deeper question, is around designing and evaluating better planning
strategies. Should we revisit such line of work under the light of the planning loss rather than the regret? On-
or Oﬀ- Policy Meta-Learning without a simulator: Realistic problem settings in RL involve using
sequentially learnt policies to collect data instead of the simulator. One direction could be to extend our
approach to model-based RL algorithms via meta-gradient updates as in ARUBA or MAML, and seek regret
guarantees induced by our concentration results. Non-stationary meta-distribution : Many real-world
scenarios have (slow or sudden) drifts in the underlying distribution itself, e.g. weather. A promising future
direction is to consider non-stationary environments where the optimal initialization varies over time.
Acknowledgments
The authors would like to thank Zheng Wen, Andras Gyorgy, and anonymous reviewers for valuable feedback
on a draft of this paper, Sebastian Flennerhag, David Abel, and Benjamin Van Roy for insightful discussions
during the course of this project. C.Vernade is funded by the Deutsche Forschungsgemeinschaft (DFG) under
both the project 468806714 of the Emmy Noether Programme and under Germany’s Excellence Strategy –
EXC number 2064/1 – Project number 390727645. CV also thanks the international Max Planck Research
School for Intelligent Systems (IMPRS-IS).
References
Ron Amit, Ron Meir, and Kamil Ciosek. Discount factor as a regularizer in reinforcement learning. In
International conference on machine learning , pp. 269–278. PMLR, 2020.
Dilip Arumugam, David Abel, Kavosh Asadi, Nakul Gopalan, Christopher Grimm, Jun Ki Lee, Lucas Lehnert,
and Michael L Littman. Mitigating planner overﬁtting in model-based reinforcement learning. arXiv
preprint arXiv:1812.01129 , 2018.
Maria-Florina Balcan, Mikhail Khodak, and Ameet Talwalkar. Provable guarantees for gradient-based
meta-learning. In International Conference on Machine Learning , pp. 424–433. PMLR, 2019.
Jonathan Baxter. A model of inductive bias learning. Journal of artiﬁcial intelligence research , 12:149–198,
2000.
Marc G Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling. The arcade learning environment: An
evaluation platform for general agents. Journal of Artiﬁcial Intelligence Research , 47:253–279, 2013.
David Blackwell. Discrete dynamic programming. The Annals of Mathematical Statistics , pp. 719–726, 1962.
Rich Caruana. Multitask learning. Machine learning , 28(1):41–75, 1997.
Leonardo Cella, Alessandro Lazaric, and Massimiliano Pontil. Meta-learning with stochastic linear bandits.
InInternational Conference on Machine Learning , pp. 1360–1370. PMLR, 2020.
Nicolò Cesa-Bianchi, Pierre Laforgue, Andrea Paudice, and Massimiliano Pontil. Multitask online mirror
descent. arXiv preprint arXiv:2106.02393 , 2021.
13Published in Transactions on Machine Learning Research (06/2023)
Giulia Denevi, Carlo Ciliberto, Dimitris Stamos, and Massimiliano Pontil. Learning to learn around a common
mean.Advances in Neural Information Processing Systems , 31, 2018.
Giulia Denevi, Dimitris Stamos, Carlo Ciliberto, and Massimiliano Pontil. Online-within-online meta-learning.
Advances in Neural Information Processing Systems , 32, 2019.
Shi Dong, Benjamin Van Roy, and Zhengyuan Zhou. Simple agent, complex environment: Eﬃcient reinforce-
ment learning with agent state. arXiv preprint arXiv:2102.05261 , 2021.
William Fedus, Carles Gelada, Yoshua Bengio, Marc G Bellemare, and Hugo Larochelle. Hyperbolic
discounting and learning over multiple horizons. arXiv preprint arXiv:1902.06865 , 2019.
Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of deep
networks. In International Conference on Machine Learning , pp. 1126–1135. PMLR, 2017.
Chelsea Finn, Aravind Rajeswaran, Sham Kakade, and Sergey Levine. Online meta-learning. In International
Conference on Machine Learning , pp. 1920–1930. PMLR, 2019.
Sebastian Flennerhag, Yannick Schroecker, Tom Zahavy, Hado van Hasselt, David Silver, and Satinder Singh.
Bootstrapped meta-learning. arXiv preprint arXiv:2109.04504 , 2021.
Sebastian Flennerhag, Tom Zahavy, Brendan O’Donoghue, Hado van Hasselt, András György, and Satinder
Singh. Optimistic meta-gradients. In Sixth Workshop on Meta-Learning at the Conference on Neural
Information Processing Systems , 2022.
Vincent François-Lavet, Guillaume Rabusseau, Joelle Pineau, Damien Ernst, and Raphael Fonteneau. On
overﬁtting and asymptotic bias in batch reinforcement learning with partial observability. Journal of
Artiﬁcial Intelligence Research , 65:1–30, 2019.
Nan Jiang, Alex Kulesza, Satinder Singh, and Richard Lewis. The dependence of eﬀective planning horizon
on model accuracy. In Proceedings of the 2015 International Conference on Autonomous Agents and
Multiagent Systems , pp. 1181–1189. International Foundation for Autonomous Agents and Multiagent
Systems, 2015.
Michael Kearns, Yishay Mansour, and Andrew Y Ng. A sparse sampling algorithm for near-optimal planning
in large markov decision processes. Machine learning , 49(2):193–208, 2002.
Mikhail Khodak, Maria-Florina Balcan, and Ameet Talwalkar. Adaptive gradient-based meta-learning
methods. arXiv preprint arXiv:1906.02717 , 2019.
Levente Kocsis and Csaba Szepesvári. Bandit based monte-carlo planning. In European conference on
machine learning , pp. 282–293. Springer, 2006.
Jelena Luketina, Sebastian Flennerhag, Yannick Schroecker, David Abel, Tom Zahavy, and Satinder Singh.
Meta-gradients in non-stationary environments. In ICLR Workshop on Agent Learning in Open-Endedness ,
2022.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex
Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control through
deep reinforcement learning. Nature, 518(7540):529, 2015.
Robert Müller and Aldo Pacchiano. Meta learning mdps with linear transition models. In International
Conference on Artiﬁcial Intelligence and Statistics , pp. 5928–5948. PMLR, 2022.
Junhyuk Oh, Matteo Hessel, Wojciech M Czarnecki, Zhongwen Xu, Hado van Hasselt, Satinder Singh, and
David Silver. Discovering reinforcement learning algorithms. arXiv preprint arXiv:2007.08794 , 2020.
Marek Petrik and Bruno Scherrer. Biasing approximate dynamic programming with a lower discount factor.
Advances in neural information processing systems , 21, 2008.
Joelle Pineau. The machine learning reproducibility checklist. arxiv, 2019.
14Published in Transactions on Machine Learning Research (06/2023)
Juergen Schmidhuber and Rudolf Huber. Learning to generate artiﬁcial fovea trajectories for target detection.
International Journal of Neural Systems , 2(01n02):125–134, 1991.
Terence Tao and Van Vu. Random matrices: universality of local spectral statistics of non-hermitian matrices.
The Annals of Probability , 43(2):782–874, 2015.
Sebastian Thrun and Lorien Pratt. Learning to learn: Introduction and overview. In Learning to learn , pp.
3–17. Springer, 1998.
Harm Van Seijen, Hado Van Hasselt, Shimon Whiteson, and Marco Wiering. A theoretical and empirical
analysis of expected sarsa. In 2009 ieee symposium on adaptive dynamic programming and reinforcement
learning, pp. 177–184. IEEE, 2009.
Zhongwen Xu, Hado van Hasselt, and David Silver. Meta-gradient reinforcement learning. arXiv preprint
arXiv:1805.09801 , 2018.
Tom Zahavy, Zhongwen Xu, Vivek Veeriah, Matteo Hessel, Junhyuk Oh, Hado P van Hasselt, David Silver,
and Satinder Singh. A self-tuning actor-critic algorithm. Advances in neural information processing systems ,
33, 2020.
15Published in Transactions on Machine Learning Research (06/2023)
A Additional Related Work
Discount Factor Adaptation. For almost all real-world applications, RL agents operate in a much larger
environment than the agent capacity in the context of both the computational and memory complexity (e.g.
the internet). Inevitably, it becomes crucial to adapt the planning horizon over time as opposed to using a
relatively longer planning horizon from the start (which can be both expensive and sub-optimal). This has
been extensively studied in the context of planning with inaccurate models in reinforcement learning (Jiang
et al., 2015; Arumugam et al., 2018).
Dong et al. (2021) introduced a schedule for γthat we take inspiration from in Section 6.1. They consider a
’never-ending RL’ problem in the inﬁnite-horizon, average-regret setting in which the true horizon is 1, but
show that adopting a diﬀerent smaller discount value proportional to the time in the agent’s life results in
signiﬁcant gains. Their focus and contributions are diﬀerent from ours as they are interested in asymptotic
rates, but we believe the connection between our ﬁndings is an interesting avenue for future research.
Meta-Learning and Meta-RL, orlearning-to-learn has shown tremendous success in online discovery of
diﬀerent aspects of an RL algorithm, ranging from hyper-parameters (Xu et al., 2018) to complete objective
functions (Oh et al., 2020). In recent years, many deep RL agents (Fedus et al., 2019; Zahavy et al., 2020)
have gradually used higher discounts moving away from the traditional approach of using a ﬁxed discount
factor. However, to the best of our knowledge, existing works do not provide a formal understanding of why
this is helping the agents in better performance, especially across varied tasks. Our analysis is motivated
by the aforementioned empirical success of adapting the discount factor. While there has been signiﬁcant
progress in meta-learning-inspired meta-gradients techniques in RL (Xu et al., 2018; Zahavy et al., 2020;
Flennerhag et al., 2021), they are largely focused on empirical analysis with lot or room for in-depth insights
about the source of underlying gains.
B Closed-form solution of the regularized least squares
We note that each ˆPshould be understood as ˆP(s,a)(s/prime).
∇/lscript(P|h) =−2
mm/summationdisplay
i=1(Xi−P) + 2λ(P−h)
∇/lscript(P|h) = 0⇐⇒P(1 +λ) =/summationtext
iXi
m+λh
ˆP(s,a)(s/prime|h) =1
1 +λ/summationtext
iXi
m+λ
1 +λh (9)
=αh+ (1−α)/summationtextiXi
mwhereα=λ
1 +λ(10)
Derivation of Mixing Rate αt:To chooseαt, we want to minimize the MSE of the ﬁnal estimator.
EX∼Pt/parenleftBig
ˆPt−Pt/parenrightBig2
=EX∼Pt/parenleftbig
αtht+ (1−αt)¯Pt−Pt/parenrightbig2
=EX∼Pt/parenleftbig
αt(ht−Pt) + (1−αt)(¯Pt−Pt)/parenrightbig2
=α2
t(ht−Pt)2+ (1−αt)2EX∼Pt/parenleftbig
(¯Pt−Pt)/parenrightbig2
where the cross term 2α)t(ht−Pt)(1−αt)EX∼PtE/bracketleftbig¯Pt−Pt/bracketrightbig
= 0sinceE[¯Pt] =Pt. This is the classic
bias-variance decomposition of an estimator and we see that the choice of htplays a role as well as the
variance of ¯Pt, which is upper bounded by 1/m(because each Xi,tis bounded in (0,1)). For instance, for
the choiceht=Po, by our structural assumption 3 we get:
EX∼Pt/parenleftBig
ˆPt−Pt/parenrightBig2
≤α2σ2+ (1−α)21
m,
16Published in Transactions on Machine Learning Research (06/2023)
and we minimize this upper bound in αto obtain the mixing coeﬃcient with smallest MSE: α∗=1
σ2m+1, or
equivalently λ∗=1
σ2m. Recall this is the within-task estimator’s variance where we consider the true Po.
In practice, however, we meta-learn the prior, so for t>1,ht=ˆPo,t=1
t−1/summationtextt−1
j=1¯Pj. Intuitively, as mandt
grow large, ˆPo,t→Poand we retrieve the result above (we show this formally to prove Eq. 20 in the proof of
our main theorem). To obtain a simple expression for αt, we minimize the "meta-MSE" of our estimator:
EPt∼Po/parenleftBig
ˆPt−Pt/parenrightBig2
=α2
tEPt∼PoEX∼Pt/parenleftbig
ht−Pt/parenrightbig2+ (1−αt)2EPt∼PoEX∼Pt/parenleftbig
(¯Pt−Pt)/parenrightbig2
≤α2
tEPt∼Po
1
t−1t−1/summationdisplay
j=1Pj−Po+Po−Pt
2
+ (1−αt)21
m
≤α2
tσ2(1 + 1/t) + (1−αt)21
m,
where in the last inequality, we upper bounded the variance of1
t−1/summationtextt−1
j=1Pj(the "denoised" ˆP0,t) byσ2/t
since each Ptis bounded in [Po−σ,Po+σ]by our structural assumption. Minimizing that last upper
bound inαtleads toαt=1
(σ2)(1+1/t)m+1≤
→tα∗, whent→∞. This means that the uncertainty on the prior
implies that its weight in the estimator is smaller, but eventually converges at a fast rate to the optimal value
(when the exact optimal prior is known). This inequality holds with probability 1−δbecause we use the
concentration of ˆPo,t(see proof of Theorem 17 below)
C Online Estimation
Online Estimation of Prior. At each task, the learner gets minteractions per state-action pair. At task
t= 1, learner can compute the prior based on the samples seen so far, i.e.:
ˆPt=1
o(s/prime|s,a) ={/summationtextm
i=1Xi}t=1
m
At subsequent tasks,
ˆPt=2
o(s/prime|s,a) ={/summationtext2m
i=1Xi}t=1:2
2m=1
2/parenleftBig{/summationtextm
i=1Xi}
m+{/summationtext2m
i=m+1Xi}
m/parenrightBig
=1
2/parenleftBig
ˆPt=1
o(s/prime|s,a) +{/summationtext2m
i=m+1Xi}
m/parenrightBig
Similarly,
ˆPt=3
o(s/prime|s,a) ={/summationtext3m
i=1Xi}t=1:3
3m={/summationtext2m
i=1Xi}t=1:2+{/summationtext3m
i=2m+1Xi}t=3
3m
=1
3/parenleftBig2{/summationtext2m
i=1Xi}t=1:2
2m+{/summationtext3m
i=2m+1Xi}t=3
m/parenrightBig
=⇒ˆPt
o(s/prime|s,a) =1
t/parenleftBig
(t−1)ˆPt−1
o(s/prime|s,a) +/summationtexttm
i=(t−1)m+1Xi
m/parenrightBig
Therefore,
ˆPt
o(s/prime|s,a) =/parenleftBig
1−1
t/parenrightBig
ˆPt−1
o(s/prime|s,a) +/parenleftBig1
t/parenrightBig/summationtexttm
i=(t−1)m+1Xi
m(11)
Online Estimation of Variance. Similarly, we can derive the online estimate of the variance:
(ˆσ2
o)t= (ˆσ2
o)t−1+(Xmt−ˆPt−1
o)(Xmt−ˆPt
o)−(ˆσ2
o)t−1
t(12)
17Published in Transactions on Machine Learning Research (06/2023)
Since the above method is numerically unstable, we will employ Welford’s online algorithm for variance
estimate.
D Concentration bounds and Proof of Theorem 2
D.1 Proof of Theorem 2
We begin the proof by decomposing each term of the loss:
Lemma 1. For a task t denoted by M, and its estimate denoted by ˆM,∀s∈S,
Vπ∗
Pt,γeval
Pt,γeval(s)−Vπ∗
ˆPt,γ
Pt,γeval(s) =/parenleftBig
Vπ∗
Pt,γeval
Pt,γeval(s)−Vπ∗
Pt,γeval
Pt,γ(s)/parenrightBig
/bracehtipupleft/bracehtipdownright/bracehtipdownleft /bracehtipupright
At+/parenleftBig
Vπ∗
Pt,γeval
Pt,γ(s)−Vπ∗
ˆPt,γ
Pt,γeval(s)/parenrightBig
/bracehtipupleft/bracehtipdownright/bracehtipdownleft /bracehtipupright
Bt
We are going to bound each term separately. The term ( At) corresponds to the bias constant due to using γ
instead ofγevaland was already bounded by Jiang et al. (2015):
Lemma 2. Jiang et al. (2015) For any MDP ˆMwith rewards in [0,Rmax],∀π:S→Aandγ≤γeval,
Vπ
Pt,γ≤Vπ
Pt,γeval≤Vπ
Pt,γ+γeval−γ
(1−γeval)(1−γ)Rmax (13)
We denote C(γ) =γeval−γ
(1−γeval)(1−γ)Rmaxand notice that/summationtext
tAt/T=C(γ)so that bounds the ﬁrst part of the
average loss.
To bound the second term Bt, we ﬁrst use Lemma 3 (Equation 18) in Jiang et al. (2015) to upper bound
Vπ∗
Pt,γeval
Pt,γ(s)−Vπ∗
ˆPt,γ
Pt,γeval(s)≤2 max
s∈S,π∈ΠR,γ|VπPt,γeval
Pt,γ(s)−VπˆPt,γ
ˆPt,γeval(s)| (14)
≤2 max
s∈S,a∈A,
π∈ΠR,γ|QπPt,γeval
Pt,γ(s,a)−QπˆPt,γ
ˆPt,γeval(s,a)| (15)
Using Lemma 4 from Jiang et al. (2015) and noticing that in our setting we do not estimate RsoˆR=R,
Qπ
Pt,γ(s,a) =R(s,a) +γ/angbracketleftPt(s,a,; ),Vπ
Pt,γ/angbracketrightandQπ
ˆPt,γ(s,a) =R(s,a) +γ/angbracketleftˆPt(s,a,; ),Vπ
ˆPt,γ/angbracketright, we have
max
s∈S,a∈A,
π∈ΠR,γ|QπPt,γeval
Pt,γ(s,a)−QπˆPt,γ
ˆPt,γeval(s,a)|≤1
(1−γ)max
s∈S,a∈A,
π∈ΠR,γ/vextendsingle/vextendsingle/vextendsingleγ/angbracketleftˆPt(s,a,; ),Vπ
Pt,γ/angbracketright−γ/angbracketleftPt(s,a,; ),Vπ
Pt,γ/angbracketright/vextendsingle/vextendsingle/vextendsingle
(16)
Notice that we are comparing the value functions of two diﬀerent MDPs which is non-trivial and we leverage
the result of Jiang et al. (2015). We refer the reader to the proof of Lemma 4 therein for intermediate steps.
Now summing over tasks, we have
/summationtext
t(B)t
T≤1
TT/summationdisplay
t=12
(1−γ)max
s∈S,a∈A,
π∈ΠR,γ/vextendsingle/vextendsingle/vextendsingleγ/angbracketleftˆPt(s,a,; ),Vπ
Pt,γ/angbracketright−γ/angbracketleftPt(s,a,; ),Vπ
Pt,γ/angbracketright/vextendsingle/vextendsingle/vextendsingle
≤2γ
(1−γ)1
TT/summationdisplay
t=1max
s∈S,a∈A,
π∈ΠR,γ/vextendsingle/vextendsingle/vextendsingle/angbracketleftˆPt(s,a,; )−Pt(s,a,; ),Vπ
Pt,γ/angbracketright/vextendsingle/vextendsingle/vextendsingle
≤2Rmax
(1−γ)1
TT/summationdisplay
t=1/summationdisplay
s/prime∈[S]max
s∈S,a∈A,
π∈ΠR,γ/vextendsingle/vextendsingle/vextendsingleˆPt(s,a,s/prime)−Pt(s,a,s/prime)/vextendsingle/vextendsingle/vextendsingle|Vπ
Pt,γ|
≤2Rmaxγ
(1−γ)2S
TT/summationdisplay
t=1max
s,s/prime∈S,a∈A/vextendsingle/vextendsingle/vextendsingleˆPt(s,a,s/prime)−Pt(s,a,s/prime)/vextendsingle/vextendsingle/vextendsingle
18Published in Transactions on Machine Learning Research (06/2023)
where we upper-bounded the value function by Rmax/(1−γ)and one sum over SbyS×maxs/prime∈S.... Note
that this step diﬀers from Jiang et al. (2015) and allows us to boil down to an average (worst-case) estimation
error of the transition model. We ﬁnally upper bound the r.h.s using Theorem 1 stated and proved below.
Remark 2. In Jiang et al. (2015), the argument is slightly more direct and involves directly controlling
the deviations of the scalar random variables R(s,a) +γ/angbracketleftˆPt(s,a,; ),Vπ
ˆPt,γ/angbracketright, arguing that it is bounded and
centered at Qπ
Pt,γ(s,a). This approach is followed by taking a union bound over the policy space ΠRγand
results in a factor log(ΠR,γ)under the square root. We could have followed this approach and obtained a
similar result but we made the alternative choice above as we believe it is informative. In our case, this
factor is replaced (and upper bounded) by the extra Sterm. As a result, we lose the direct dependence on
the size of the policy class, which is a function of γand should play a role in the bound. In turn, and at
the price of this extra looseness, we get a slightly more "exploitable" bound (see our heuristic for a gamma
schedule in Section 6). It is easy and straightforward to adapt our concentration bound below to directly bound
R(s,a) +γ/angbracketleftˆPt(s,a,; ),Vπ
ˆPt,γ/angbracketright−Qπ
Pt,γ(s,a)as in Jiang et al. (2015), and one would obtain a similar bound as
Eq. equation 6 without the factor S, but with an extra log(ΠR,γ).
D.2 Concentration of the model estimator
To avoid clutter in the notation of this section , we drop the (s,a,s/prime)everywhere, as we did in Appendix B
above. All deﬁnitions of ˆPand ˆP0are as stated in the latter section.
Theorem 1. with probability 1−δ:
max
s,a,s/prime|ˆPt−Pt|≤1
σ2m+ 1
/radicalBigg
log(6T
δ) log(TS2A
δ)(σ2+Σ log2(6T2
δ)
m)
T+σ/radicalBigg
log(3TS2A
δ)
T+ 2σ

+σ2m
2σ2m+ 1/radicalBigg
Σ log(3TS2A
δ)
2m(17)
For anyt∈[T],s,a,s/primeandπ∈ΠR,γ, deﬁne ˆPt,∗=αtPo+ (1−αt)¯Pt
mthe optimally regularized estimator
(using the true unknown Pofor eacht). We have
/vextendsingle/vextendsingle/vextendsingleˆPt−Pt/vextendsingle/vextendsingle/vextendsingle≤|ˆPt−ˆPt,∗|+|ˆPt,∗−Pt|
≤αt|ˆPo,t−Po|/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
(A)+ (1−αt)|¯Pt
m−Pt|/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
(B)+αt|Po−Pt|/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
≤2·σby assum.(18)
Bounding Term A
Substituting the estimator ˆPt=αt1
m/summationtextm
iXi+ (1−αt)ˆPt
o,
A≤αt
|ˆPt
o−1
t−1t−1/summationdisplay
j=1Pj|+|1
t−1t−1/summationdisplay
j=1Pj−Po|

≤1
σ2m+ 1
|ˆPt
o−1
t−1t−1/summationdisplay
j=1Pj|
/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
(A1)+|1
t−1t−1/summationdisplay
j=1Pj−Po|
/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
A2

whereαtis simply upper bounded by its initial value1
σ2m+1and we introduced the denoised (expected)
average1
t−1/summationtextt−1
j=1Pj=EP1,...Pt−1ˆPo,t. Indeed, by assumption, EP∼P1
t−1/summationtextt−1
j=1Pj=Poand the variance of
19Published in Transactions on Machine Learning Research (06/2023)
this estimator is bounded by σ2/(t−1)by our structure assumption. This allows to naturally bound A2
using Hoeﬀding’s inequality for bounded random variables: with probability at least 1−δ/3,
max
s,a,s/primeA2≤σ/radicalbigg
log(6S2AT/δ )
T(19)
We now bound A1
A1=/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle1
t−1/summationdisplay
j(¯Pj
m−Pj)/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
We note here that the ﬁrst term in A1 is indeed a martingale Mt=/summationtextt−1
j=1Zj, whereZj=¯Pj
m−Pj, such that
each increment is bounded with high probability: for each j,|Zj|≤cjw.p1−δ
6, wherecj=/radicalBig
Σ
mlog(6T2
δ).
Moreover, the diﬀerences |Zj−Zj+1|are also bounded with high probability:
|Zj−Zj+1|≤|Pj−Pj+1|+|¯Pj−¯Pj+1|<2σ+ 2cj=Dj= 2/parenleftBigg
σ+√
Σ log(6T2
δ)√m/parenrightBigg
Then by (Tao & Vu, 2015, Prop. 34), for any /epsilon1>0,
P/parenleftBig/vextendsingle/vextendsingle/vextendsingleMt
t−1/vextendsingle/vextendsingle/vextendsingle≥/epsilon1
t−1/radicaltp/radicalvertex/radicalvertex/radicalbtt−1/summationdisplay
j=1D2
j/parenrightBig
≤2 exp(−2/epsilon12) +t−1/summationdisplay
j=1δ
6T2
Choosing/epsilon1=/radicalBig
1
2log(12T
δ), we get
P
/vextendsingle/vextendsingle/vextendsingle1
t−1t−1/summationdisplay
j=1¯Pj
m−Pj/vextendsingle/vextendsingle/vextendsingle≥/radicaltp/radicalvertex/radicalvertex/radicalbt(σ+√
Σ log(6T2
δ)√m)2log(6T
δ)
T
≤δ
6T+δ
6T=δ
3T
With a union bound as before, we get that with probability at least 1−δ/3,
A1≤/radicalBigg
log(6T
δ) log(TS2A
δ)(σ2+Σ log2(6T2
δ)
m)
T(20)
because (σ+/radicalBig
Σ
m)2≥σ2+Σ
m.
By combining equation 20 and equation 19, we get:
max
s,a,s/primeαt|Po,t−Po|≤1
σ2m+ 1
/radicalBigg
log(6T
δ) log(TS2A
δ)(σ2+log2(6T2
δ)
m)
T+σ/radicalbigg
log(3S2AT/δ )
T
(21)
Bounding Term B
Term B is simply the concentration of the average of bounded variables ¯Pt
m=1
m/summationtext
iXi, whose variance is
bounded by 1. So by Hoeﬀding’s inequality, and a union bound, with probability at least 1−δ/4
max
s,a,s/prime|¯Pt
m−Pt|≤/radicalbigg
Σ log(4TS2A/δ)
2m
20Published in Transactions on Machine Learning Research (06/2023)
To bound term B, it remains to upper bound 1−αtfor allt∈[T]:
1−αt=σ2(1 +1
t)m
σ2(1 +1
t)m+ 1≤σ2m
2σ2m+ 1
We get that with probability 1−δ/3
max
s,a,s/prime(B)≤σ2m
2σ2m+ 1/radicalbigg
Σ log(3TS2A/δ)
2m(22)
Combining all bounds
To conclude, we combine the bounds on the terms in equation 18, replacing with equation 21,equation 22,
and with a union bound, we get that with probability 1−δ,
max
s,a,s/prime|ˆPt−Pt|≤1
σ2m+ 1
/radicalBigg
log(6T
δ) log(TS2A
δ)(σ2+Σ log2(6T2
δ)
m)
T+σ/radicalbigg
log(3S2AT/δ )
T+ 2σ

+σ2m
2σ2m+ 1/radicalbigg
Σ log(3TS2A/δ)
2m(23)
Discussion
The bound has 4 main terms respectively in ˜O(/radicalBig
1
mT),˜O(/radicalBig
1
T),˜O(1
m)and ˜O(/radicalBig
1
m), all scaled by some factor
depending on σ2andm. A ﬁrst remark is that when mis large and T= 1, the last part in ˜O(/radicalBig
1
m)dominates
due to the factorσ2m
σ2m+1→1, while the coeﬃcient of the ﬁrst two terms goes to 0 fast (in 1/(σ2m)).
E Proof of Proposition 1
We study the function Udeﬁned by
U:γ/mapsto→1
1−γeval+1
γ−1+Cm,T,S,A,σ,δγ
(1−γ)2,
whereγevalis a ﬁxed constant and C:=Cm,T,S,A,σ,δ is seen as a parameter whose value controls the general
"shape" of the function. We diﬀerentiate with respect to γ:
dU
dγ=−−C(γ+ 1) + (1−γ)
(1−γ)3.
We see that the sign of the derivative is aﬀected by the value of the parameter C:
•If∀γ∈(0,1),−C(γ+ 1) + (1−γ)>0thenUis monotonically decreasing on (0,1)and the minimum
is reached for γ= 1,
∀γ∈(0,1),−C(γ+ 1) + (1−γ)>0⇐⇒ − 2C+ 1>0⇐⇒C < 1/2.
•Similarly, if Cis really large, Umay be monotonically increasing on (0,1):
∀γ∈(0,1),−C(γ+ 1) + (1−γ)<0⇐⇒C≥1;
•Finally, ifC∈(1/2,2), the minimum exists inside (0,1)and is reached for
−Cγ−C+ 1−γ= 0⇐⇒γ=γ∗=1−C
1 +C
21Published in Transactions on Machine Learning Research (06/2023)
F Experiments: Implementation Details, Ablations & Additional Results
F.1 Implementation details
We consider a Dirichlet distribution of tasks such that all tasks t∈[T],Pt∼Pare centered at some ﬁxed
meanPo∈∆S×A
Sas shown in Figure F1. The mean of the task distribution Pois chosen as a sampled
random MDP and variance of this distribution is determined such that /bardblPt
s,a−Po
s,a/bardbl∞≤σ<1. Next, we
compute the variance of this distribution σi=˜αi(1−˜αi)
α0+1, where ˜αi=αi
α0andα0=/summationtextS
iαi.
α = (15.000, 15.000, 15.000)
Figure F1: Dirichlet Task Distribution forS= 3states, with Dir(α) whereα= [15,15,15], resulting in
our task-similarity measure approximately to be σ= 0.0129.
F.2 Ablations
We also run ablations with Aggregating( α= 1), a naive baseline that simply ignores the meta-RL structure
and just plans assuming there is a single task. We observe in Fig. F2 the aggregating baseline works at-par
with our method POMRLwhich is intuitive when the tasks are strongly related to each other in this case.
However, as the underlying task structure decreases, we note that Aggregating( α= 1) as though it is one
single task is problematic and suﬀers from a non-vanishing bias due to which for each new task there is on
average an error which does not go to zero. More importantly, the Aggregating( α= 1) baseline cannot have
the same guarantees as POMRLandada-POMRL .
Strong Structure
123456789101112131415
Task (T)0.00.51.01.52.02.53.03.54.0Planning Loss°=0:99=°eval 
ada-POMRL
Oracle Prior Knowledge
Without Meta-Learning
Aggregating(®=1)
POMRL(¾)
(a)Medium Structure
123456789101112131415
Task (T)0.00.51.01.52.02.53.03.54.0Planning Loss°=0:99=°eval 
ada-POMRL
Oracle Prior Knowledge
Without Meta-Learning
Aggregating(®=1)
POMRL(¾)
(b)Loosely Structured
123456789101112131415
Task (T)0.00.51.01.52.02.53.03.54.0Planning Loss°=0:99=°eval 
ada-POMRL
Oracle Prior Knowledge
Without Meta-Learning
Aggregating(®=1)
POMRL(¾)
(c)
Figure F2: Ablations for Eﬃcacy of POMRLand ada-POMRL for varying task-similarity. depicts the
eﬀect of the task-similarity parameter σfor a small ﬁxed amount of data m= 5available at each round. We
run another baseline called Aggregating (orange) that simply ignores the meta-RL structure and acts as if
it is all one single task. In the presence of strong structure, meta-learning the shared structure alongside
a good model initialization leads to most gains and even naively aggregating the tasks transitions might
seem to work well. However, such a naive method is not reliable as the underlying task similarity decreases -
the learner struggles to cope with new unseen tasks which diﬀer signiﬁcantly and the planning loss doesn’t
improve. Error bars represent 1-standard deviation of uncertainty across 100independent runs.
22Published in Transactions on Machine Learning Research (06/2023)
0.00 0.10 0.20 0.30 0.40 0.50 0.60 0.70 0.80 0.900.99
γ0.00.51.01.52.02.53.03.54.0Planning LossT=1 T=2 T=3 T=4 T=5
(a)m= 5,T= 5
0.00 0.10 0.20 0.30 0.40 0.50 0.60 0.70 0.80 0.900.99
γ0.00.51.01.52.02.53.03.54.0Planning LossT=1
T=2T=3 T=4 T=5 (b)m= 20,T= 5
0.00 0.10 0.20 0.30 0.40 0.50 0.60 0.70 0.80 0.900.99
γ0.00.51.01.52.02.53.03.54.0Planning LossT=1
T=2
T=3T=4
T=5
T=12T=15
T=17
T=20T=25
T=27
T=30 (c)m= 5,T= 30
Figure F3: Eﬀect of m and T on Average Regret Upper Bound on Planning: for a ﬁxed value of
task similarity σ, depends on the number of samples per task mand the number of tasks T. (a) For m=T,
smaller loss is obtained with very small discount factor. This implies that with a lot of uncertainty it is not
interesting to plan far too ahead, (b) For m>>T, each task has enough samples to inform itself resulting in
slightly larger eﬀective discount factors. Not a lot is gained in this scenario from meta-learning, (c) m/lessmuchT
is the most interesting case as samples seen in each individual task are very limited due to small m. However,
the number of tasks are much more resulting in huge gains from leveraging shared structure across tasks.
F.3 Additional Experiments
We examine more properties of ada-POMRL , namelyEﬀect ofm, andTon Planning Loss in Fig. F3,
Individual Baseline’s Performance in Fig. F4, and Varying State Space |S|,m, andTin Fig. F5.
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.91.0
°0.00.51.01.52.02.53.03.54.0Planning LossT=1 T=3 T=6 T=15
(a)ada-POMRL
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.91.0
°0.00.51.01.52.02.53.03.54.0Planning LossT=1 T=3 T=6 T=15 (b) Oracle Prior Knowledge
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.91.0
°0.00.51.01.52.02.53.03.54.0Planning LossT=1 T=3 T=6 T=15 (c) Without Meta-Learning
Figure F4: Planning with Online Meta Learning - Baselines. (a)ada-POMRL . Meta updates include
learningPo,σ,αas a function of tasks. (b) Oracle Prior Knowledge considers the optimal α, true
mean of the task distribution Poand actual underlying task similarity σas known apriori, (c) Without
Meta-Learning estimates the transition kernel in each round Twithout any meta-learning. All baselines
are obtained with T= 15tasks andm= 5samples per task.
F.4 Reproducibility
We follow the reproducibility checklist by Pineau (2019) to ensure this research is reproducible. For all
algorithms presented, we include a clear description of the algorithm and source code is included with these
supplementary materials. For any theoretical claims, we include: a statement of the result, a clear explanation
of any assumptions, and complete proofs of any claims. For all ﬁgures that present empirical results, we
include: the empirical details of how the experiments were run, a clear deﬁnition of the speciﬁc measure or
statistics used to report results, and a description of results with the standard error in all cases.
F.5 Computing and Open source libraries.
All experiments were conducted using Google Colab instances7.
7https://colab.research.google.com/
23Published in Transactions on Machine Learning Research (06/2023)
|S|= 20
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.91.0
°0.00.51.01.52.02.53.03.54.0Planning LossT=1
T=2T=3 T=4 T=5
(a)m=T= 5
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.91.0
°0.00.51.01.52.02.53.03.54.0Planning LossT=1
T=2T=3 T=4 T=5 (b)m= 20,T= 5
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.91.0
°0.00.51.01.52.02.53.03.54.0Planning LossT=1 T=3 T=6 T=15 (c)m= 5,T= 30
|S|= 30
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.91.0
°0.00.51.01.52.02.53.03.54.0Planning LossT=1
T=2T=3 T=4 T=5
(d)m=T= 5
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.91.0
°0.00.51.01.52.02.53.03.54.0Planning LossT=1
T=2T=3 T=4 T=5 (e)m= 20,T= 5
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.91.0
°0.00.51.01.52.02.53.03.54.0Planning LossT=1
T=3T=6
T=16T=17
T=18T=19
T=20 (f)m= 5,T= 15
Figure F5: Varying the size of state-space S, number of samples per task m, and number of
tasksT, on Task-averaged Regret Upper Bound on Planning: for a ﬁxed value of task similarity σ,
We note that despite larger state-space we observe the same eﬀect i.e. (a,d,g) For m=T, smaller loss is
obtained with very small discount factor i.e. a lot of uncertainty and inability to plan far too ahead, (b,e,h)
Form>>T, each task has enough samples to inform itself resulting in slightly larger eﬀective discount
factors. Not a lot is gained in this scenario from meta-learning. (c,f,i) m/lessmuchTis the most interesting case as
samples seen in each individual task are very limited due to small m. Meta-learning has most signiﬁcant
gains in this case by leveraging the structure across tasks. Results are averaged over 20independent runs and
error bars represent 1-standard deviation.
24