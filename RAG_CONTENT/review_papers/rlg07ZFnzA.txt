Under review as submission to TMLR
A Unified Linear Speedup Analysis
of Stochastic and Nesterov Federated Averaging
Anonymous authors
Paper under double-blind review
Abstract
Federated learning (FL) learns a model jointly from a set of participating devices without
sharing each other’s privately held data. The characteristics of non- i.i.d.data across
the network, low device participation, high communication costs, and the mandate that
data remain private bring challenges in understanding the convergence of FL algorithms,
particularly regarding how convergence scales with the number of participating devices. In
this paper, we focus on Federated Averaging (FedAvg), one of the most popular and effective
FL algorithms in use today, and conduct a systematic study of how its convergence scales with
the number of participating devices under non- i.i.d.data and partial participation in convex
settings. We provide a unified analysis that establishes convergence guarantees for FedAvg
under strongly convex, convex, and overparameterized strongly convex problems. We show
that FedAvg enjoys linear speedup in each case, although with different convergence rates and
communication efficiencies. For strongly convex and convex problems, we also characterize
the corresponding convergence rates for the Nesterov accelerated FedAvg algorithm, which
are the first linear speedup guarantees for momentum variants of FedAvg in convex settings.
Empirical studies of the algorithms in various settings have supported our theoretical results.
1 Introduction
Federated learning (FL) is a machine learning paradigm where many clients (e.g., mobile devices or organiza-
tions) collaboratively train a model under the orchestration of a central server (e.g., service provider), while
keeping the training data decentralized in order to preserve privacy (Konečn` y et al., 2016; McMahan et al.,
2017; Smith et al., 2017; Li et al., 2020a; Kairouz et al., 2021; Wang et al., 2021). In recent years, FL has
swiftly emerged as an important learning paradigm, enjoying widespread success in such diverse applications
as personalized recommendation (Chen et al., 2018), virtual assistant (Lam et al., 2019), keyboard prediction
(Hard et al., 2018), and digital health (Rieke et al., 2020), to name a few. There are at least three reasons
for its popularity: First, the rapid proliferation of smart devices that are equipped with both computing
power and data-capturing capabilities provided the infrastructure core for FL. Second, the rising awareness
of privacy and the explosive growth of computational power in mobile devices have made it increasingly
attractive to push the computation to the edge. Third, the empirical success of communication-efficient FL
algorithms has enabled increasingly larger-scale parallel computing and learning with less communication
overhead.
Despite its promise and broad applicability, the potential value FL delivers is coupled with the unique
challenges it brings. In particular, when FL learns a single statistical model using data from across all
the devices while keeping each individual device’s data isolated, it faces two challenges that are absent in
centralized optimization and distributed (stochastic) optimization (Zhou & Cong, 2018; Woodworth et al.,
2018; Jiang & Agrawal, 2018; Woodworth et al., 2020b; Charles & Konečn` y, 2021):
1)Data heterogeneity (non- i.i.d. data):data distributions on local devices/servers are different, and
data cannot be shared across devices;
1Under review as submission to TMLR
hhhhhhhhhhhhhhParticipationObjective functionStrongly Convex Convex Overparameterized
Full O(1
NT+E2
T2)O/parenleftig
1√
NT+NE2
T/parenrightig
O(exp(−NT
Eκ))
Partial O/parenleftig
E
KT+E2
T2/parenrightig
O/parenleftig
E√
KT+KE2
T/parenrightig
O(exp(−KT
Eκ))
Table 1: Our convergence results for FedAvg and accelerated FedAvg in this paper. Throughout the paper, Nis the
total number of local devices, and K≤Nis the number of devices that are accessible to the central server during
each communication. Tis the total number of stochastic updates performed by each local device, Eis the local steps
between two consecutive server communications (and hence T/Eis the number of communications).†Depending on
the particular overparameterized setting, κis a type of condition number defined in Section 5 and Appendix G.
2)System heterogeneity (partial participation): only a subset of devices may access the central server
at each time, which happens because the communication bandwidth profiles vary across devices and there is
no central server that has control over when a device is active (the presence of “stragglers”).
To address these challenges, Federated Averaging (FedAvg) (McMahan et al., 2017) was proposed as a
particularly effective heuristic, which has enjoyed great empirical success. This success has since motivated a
growing line of research efforts into understanding its theoretical convergence guarantees in various settings
(Stich, 2019; Khaled et al., 2019; Haddadpour & Mahdavi, 2019; Li et al., 2020b; Wang et al., 2019; Yu
et al., 2019a;b; Wang & Joshi, 2018; Koloskova et al., 2020; Woodworth et al., 2020a; Khaled et al., 2020;
Yang et al., 2021). Of these, Li et al. (2020b) was among the first to establish an O(1
T)convergence rate for
FedAvg for strongly convex smooth FL problems with both data and system heterogeneities. When only data
heterogeneity is present, Khaled et al. (2020) provides tight convergence results with linear speedup analysis
in convex settings. In non-convex settings, Yang et al. (2021) obtained linear speedup convergence results for
FedAvg under both non- i.i.d.data and partial participation.
Despite the recent fruitful efforts to understand the theoretical convergence properties of FedAvg, the question
of how the number of participating devices affects the convergence speed remains to be answered fully when
both data and system heterogeneity are present. In particular, is linear speedup of FedAvg a universal
phenomenon across different settings and for any number of devices? What about when FedAvg is accelerated
with momentum updates? Does the presence of both data and system heterogeneity in FL imply different
communication complexities and require technical novelties over results in distributed and decentralized
optimization? Linear speedup is a desirable property of distributed optimization systems, including FedAvg,
as it characterizes the impact of scale on such systems. Here we provide affirmative answers to these questions.
Our Contributions. First, we establish an O(1/KT )convergence rate for FedAvg for strongly convex
and smooth problems and an O(1/√
KT)convergence rate for convex and smooth problems, where Kis
the number of participating devices at each communication round (can take lower bound if not fixed),
confirming that FedAvg enjoys the desirable linear speedup property with both non- i.i.d.data and partial
participation. In previous works, the best and most related convergence analyses are given by Li et al.
(2020b), which established an O(1
T)convergence rate for strongly convex smooth problems under FedAvg,
and by Khaled et al. (2020), which established linear speedup in the number of participating local servers
under data heterogeneity. Our rate matches the same (and optimal) dependence on T, but also establishes
the linear speedup dependence on K, for anyK≤N, whereNis the total number of devices, whereas Li
et al. (2020b) does not have linear speedup analysis, and Khaled et al. (2020) focuses on full participation
K=N. The concurrent work of Karimireddy et al. (2020) also established linear speedup convergence
under partial participation, using a modified version of the FedAvg with distinct learning rates for local steps
and communication rounds. Compared to their work, our analyses are carried out for the original FedAvg
algorithm that utilizes a decaying rate independent of local vs. communication rounds. Our unified analysis
highlights the common elements and distinctions between the strongly convex and convex settings, as well as
the communication complexity differences between the full and partial participation settings.
Second, we establish the same convergence rates– O(1/KT )for strongly convex and smooth problems and
O(1/√
KT)for convex and smooth problems–for Nesterov accelerated FedAvg. We analyze the accelerated
version of FedAvg here because empirically it tends to perform better; yet, its theoretical convergence
guarantee is unknown. To the best of our knowledge, these are the first results that provide a linear speedup
2Under review as submission to TMLR
characterization of Nesterov accelerated FedAvg in the two convex problem classes. The fact that FedAvg
and Nesterov accelerated FedAvg share the same convergence rate is to be expected: this is the case even
for general centralized stochastic optimization problems. Prior to our results, the most relevant results only
concern the non-convex setting (Yu et al., 2019a; Li et al., 2020a; Huo et al., 2020), where convergence is
measured with respect to stationary points (vanishing of gradient norms, rather than optimality gaps). Our
unified analysis of Nesterov FedAvg also illustrates the technical similarities and distinctions compared to the
original FedAvg algorithm, whereas prior works in the non-convex setting used different frameworks with
distinct proof techniques.
Third, we study a subclass of strongly convex smooth problems where the objective is over-parameterized and
establish a faster O(exp(−KT
κ))geometric convergence rate for FedAvg, in contrast to the O(exp(−T
κ))rate
for individual solvers (Ma et al., 2018). Within this class, we further consider the linear regression problem
and establish an even sharper rate for FedAvg. To our knowledge, these bounds are among the first to extend
the geometric convergence results in the non-distributed overparameterized setting to the federated learning
setting with a linear speedup in the number of local servers.
Connections with Distributed and Decentralized Optimization Federated learning is closely related to
distributed and decentralized optimization, and as such it is important to discuss connections and distinctions
between our work and related results from that literature. First, when there is neither system heterogeneity,
i.e., all devices participate in parameter averaging during a communication round, nor data heterogeneity,
i.e., all devices have access to a common set of stochastic gradients, FedAvg coincides with the “Local SGD”
of Stich (2019), which showed the linear speedup rate O(1/NT)for strongly convex and smooth functions.
Woodworth et al. (2020b;a) further improved the communication complexity that guarantees the linear
speedup rate. When there is only data heterogeneity, some works such as Khaled et al. (2020) have continued
to use the term Local SGD to refer to FedAvg, while others subsume it in more general frameworks that
include decentralized model averaging based on a network topology or a mixing matrix. They have provided
linear speedup analyses for strongly convex and convex problems, e.g., Khaled et al. (2020); Koloskova
et al. (2020) as well as non-convex problems, e.g., Jiang & Agrawal (2018); Yu et al. (2019b); Wang &
Joshi (2018). However, most of these results do not consider system heterogeneity, i.e., the presence of
stragglers in the device network. Even with decentralized model averaging, the assumptions usually imply
that model averages over all devices is the same as decentralized model averages based on network topology
(e.g., Koloskova et al. (2020) Proposition 1), which precludes system heterogeneity as defined in this paper
and prevalent in FL problems. For momentum accelerated FedAvg, Yu et al. (2019a) provided linear speedup
analysis for non-convex problems, while results for strongly convex and convex settings are entirely lacking,
even without system heterogeneity. In contrast, our linear speedup analyses for FedAvg and consider both
types of heterogeneity present in the full federated learning setting, and are valid for almost any number of
participating devices. We also highlight a distinction in communication efficiency when system heterogeneity
is present. Moreover, our results for Nesterov accelerated FedAvg completes the picture for strongly convex
and convex problems. For a detailed comparison with related works, please refer to Table 2 in Appendix
Section B.
2 Setup
In this paper, we study the following federated learning problem:
min
w/braceleftbigg
F(w)≜/summationdisplayN
k=1pkFk(w)/bracerightbigg
, (1)
whereNis the number of local devices (users/nodes/workers) and pkis thek-th device’s weight satisfying
pk≥0and/summationtextN
k=1pk= 1. In thek-th local device, there are nkdata points: x1
k,x2
k,...,xnk
k. The local
objectiveFk(·)is defined as: Fk(w)≜1
nk/summationtextnk
j=1ℓ/parenleftig
w;xj
k/parenrightig
, whereℓdenotes a user-specified loss function. Each
device only has access to its local data, which gives rise to its own local objective Fk. Note that we do not
make any assumptions on the data distributions of each local device. The local minimum F∗
k=minwFk(w)
can be far from the global minimum of Eq (1) (data heterogeneity).
3Under review as submission to TMLR
2.1 The Federated Averaging (FedAvg) Algorithm
WefirstintroducethestandardFederatedAveraging(FedAvg)algorithmwhichwasfirstproposedbyMcMahan
et al. (2017). FedAvg updates the model in each device by local Stochastic Gradient Descent (SGD) and sends
the latest model to the central server every Esteps. The central server conducts a weighted average over
the model parameters received from active devices and broadcasts the latest averaged model to all devices.
Formally, the updates of FedAvg at round tis described as follows:
vk
t+1=wk
t−αtgt,k,wk
t+1=/braceleftbiggvk
t+1 ift+ 1/∈IE,/summationtext
k∈St+1qkvk
t+1ift+ 1∈IE,(2)
where wk
tis the local model parameter maintained in the k-th device at the t-th iteration and gt,k:=
∇Fk(wk
t,ξk
t)is the stochastic gradient based on ξk
t, the data point sampled from k-th device’s local data
uniformly at random. IE={E,2E,...}is the set of global communication steps, when local parameters
from a set of active devices are averaged and broadcast to all devices. We use St+1to represent the (random)
set of active devices at t+ 1.qkis a set of averaging weights that are specific to the sampling procedure used
to obtain the set of active devices St+1.
Since federated learning usually involves an enormous amount of local devices, it is often more realistic to
assume only a subset of local devices is active at each communication round (system heterogeneity). In this
work, we consider both the case of full participation where the model is averaged over all devices at each
communication round, in which case qk=pkfor allkandwk
t+1=/summationtextN
k=1pkvk
t+1ift+ 1∈IE, and the case of
partial participation where|St+1|<N.
With partial participation, we follow Li et al. (2020a); Karimireddy et al. (2020); Li et al. (2020b) and assume
thatSt+1is obtained by one of two types of sampling schemes to simulate practical scenarios. One scheme
establishesSt+1byi.i.d.sampling the devices with probability pkwith replacement, and uses qk=1
K, where
K=|St+1|, while the other scheme samples St+1uniformly i.i.d.from all devices without replacement, and
usesqk=pkN
K. Both schemes guarantee that gradient updates in FedAvg are unbiased stochastic versions
of updates in FedAvg with full participation, which is important in the theoretical analysis of convergence.
Because the original sampling scheme and weights proposed by McMahan et al. (2017) lacks this desirable
property, it is not considered in this paper. An interesting recent work (Chen et al., 2022) proposes a new
client selection procedure based on importance sampling that achieves better communication complexities
thani.i.d.sampling. For more details on the notations and setup as well as properties of the two sampling
schemes, please refer to Section A in the appendix.
2.2 Assumptions
We make the following standard assumptions on the objective function F1,...,FN. Assumptions 1 and 2 are
commonly satisfied by a range of popular objective functions, such as ℓ2-regularized logistic regression and
cross-entropy loss functions.
Assumption 1 (L-smooth) .F1,···,FNare allL-smooth: for all vandw,Fk(v)≤Fk(w) + (v−
w)T∇Fk(w) +L
2∥v−w∥2
2.
Assumption 2 (Strongly-convex) .F1,···,FNare allµ-strongly convex: for all v and w,Fk(v)≥Fk(w) +
(v−w)T∇Fk(w) +µ
2∥v−w∥2
2
Assumption 3 (Bounded local variance) .Letξk
tbe sampled from the k-th device’s local data uniformly at
random. The variance of stochastic gradients in each device is bounded: E/vextenddouble/vextenddouble∇Fk/parenleftbig
wk
t,ξk
t/parenrightbig
−∇Fk/parenleftbig
wk
t/parenrightbig/vextenddouble/vextenddouble2≤σ2
k,
fork= 1,···,Nand any wk
t. Letσ2:=/summationtextN
k=1pkσ2
k.
Assumption 4 (Bounded local gradient) .The expected squared norm of stochastic gradients is uniformly
bounded. i.e., E/vextenddouble/vextenddouble∇Fk/parenleftbig
wk
t,ξk
t/parenrightbig/vextenddouble/vextenddouble2≤G2, for allk= 1,...,Nandt= 0,...,T−1.
Assumptions 3 and 4 have been used in many prior works, e.g., Yu et al. (2019b); Li et al. (2020b); Stich
(2019). Some more recent works (Khaled et al., 2020; Karimireddy et al., 2020) have relaxed Assumption 4 to
only requiring the bound at a minimizer w∗∈arg min wF(w)of the global objective instead of everywhere.
4Under review as submission to TMLR
This is to address the issue that for unconstrained optimization problems, gradients may not be bounded
everywhere. For example, Karimireddy et al. (2020) assume a bound similar to
/summationdisplay
kpkE∥∇Fk(w,ξk
t)∥2≤G2+ 2βB2(F(w)−F(w∗))
While it is true that in unconstrained optimization, gradients can become unbounded, and the above bound
is formally weaker than Assumption 4 due to the optimality gap in the upper bound, we argue that under
convexity,Fcan be assumed to be bounded due to the boundedness of local updates, so that Assumption 4 is
not substantially stronger. First, if Fis itself bounded above, then the bound above would imply the bound
/summationdisplay
kpkE∥∇Fk(w,ξk
t)∥2≤G′2
with a larger G′, which is essentially equivalent to Assumption 4. Even if Fis unbounded, convexity implies
that in expectation, local parameters gravitate towards minima of local objectives during local updates, and
thus stay bounded in a ball around w∗. Once local parameters are bounded, Fcan essentially be assumed
to be bounded in that region, yielding a bound Gin Assumption 4 that depends on the initialization and
local objective functions. Therefore, for convexproblems, Assumption 4 is not fundamentally more restrictive
than assuming the bounds at w∗only. Furthermore, compared to assuming bounded gradient diversity as in
related works Haddadpour & Mahdavi (2019); Li et al. (2020a), Assumption 4 is much less restrictive. When
the optimality gap converges to zero, bounded gradient diversity restricts local objectives to have the same
minimizer as the global objective, contradicting the heterogeneous data setting. For detailed discussions of
our assumptions, please refer to Appendix Section B.
3 Linear Speedup Analysis of Federated Averaging
In this section, we provide convergence analyses of FedAvg for convex objectives in the general setting with
both heterogeneous data (statistical heterogeneity) and partial participation (system heterogeneity). We
show that for strongly convex and smooth objectives, the convergence of the optimality gap of averaged
parameters across devices is O(1/KT ), while for convex and smooth objectives, the rate is O(1/√
KT). Our
results improve upon Li et al. (2020b) by showing linear speedup for any number of participating devices,
and upon Khaled et al. (2020); Koloskova et al. (2020) by allowing system heterogeneity. The proofs also
highlight similarities and distinctions between the strongly convex and convex settings. Detailed proofs are
deferred to Appendix Section E.
3.1 Strongly Convex and Smooth Objectives
We first show that FedAvg has an O(1/KT )convergence rate for µ-strongly convex and L-smooth objectives.
The result relies on a technical improvement over the analysis in Li et al. (2020b). Moreover, it implies a
distinction in communication efficiency that guarantees this linear speedup for FedAvg with full and partial
device participation. With full participation, Ecan be chosen as large as O(/radicalbig
T/N)without degrading the
linear speedup in the number of workers. On the other hand, with partial participation, Emust beO(1)to
guaranteeO(1/KT )convergence.
Theorem 1. LetwT=/summationtextN
k=1pkwk
Tin FedAvg,νmax=maxkNpk, and set decaying learning rates αt=4
µ(γ+t)
withγ= max{32κ,E}andκ=L
µ. Then under Assumptions 1 to 4 with full device participation,
EF(wT)−F∗=O/parenleftbiggκνmaxσ2/µ
NT+κ2E2G2/µ
T2/parenrightbigg
,
and with partial device participation with at least Ksampled devices at each communication round,
EF(wT)−F∗=O/parenleftbiggκEG2/µ
KT+κνmaxσ2/µ
NT+κ2E2G2/µ
T2/parenrightbigg
.
5Under review as submission to TMLR
Proof sketch. Because our unified analyses of results in the main text follow the same framework with
variations in technical details, we first give an outline of proof for Theorem 1 to illustrate the main ideas. For
full participation, the main ingredient is a recursive contraction bound
E∥wt+1−w∗∥2≤(1−µαt)E∥wt−w∗∥2+α2
t1
Nνmaxσ2+ 6α3
tLE2G2
where theO(α3
tE2G2)term is the key improvement over the bound in Li et al. (2020b), which has O(α2
tE2G2)
instead. We then use induction to obtain a non-recursive bound on E∥wT−w∗∥2, which is converted to a
bound on EF(wT)−F∗usingL-smoothness. For partial participation, an additional term O(1
Kα2
tE2G2)of
leading order resulting from sampling variance is added to the contraction bound, but only every Esteps. To
facilitate the understanding of our analysis, please refer to a high-level summary in Appendix C.
Linear speedup. We compare our bound with that in Li et al. (2020b), which is O(1
NT+E2
KT+E2G2
T).
Because the termE2G2
Tis alsoO(1/T)without a dependence on N, for any choice of Etheir bound cannot
achieve linear speedup. The improvement of our bound comes from the termκ2E2G2/µ
T2, which now is
O(E2/T2)and so is not of leading order. As a result, all leading terms scale with 1/Nin the full device
participation setting, and with 1/Kin the partial participation setting. This implies that in both settings,
there is a linear speedup in the number of active workers during a communication round. We also emphasize
that the reason one cannot recover the full participation bound by setting K=Nin the partial participation
bound is due to the variance generated by sampling.
Discussion on νmax.The parameter νmaxis a measure of how unbalanced different local servers are, and
is also discussed in Li et al. (2020b). Recall that νmax=Nmaxkpk, wherepkis the weight of the local
objective of server kin the FL objective. Often, pkis the proportion of data stored on server krelative
to the total amount of data across all servers, and is therefore small, i.e., O(1/N). This is a reasonable
assumption in many FL applications, e.g., mobile computing. In this case, νmax=O(1)and linear speedup
in the number of local servers is guaranteed. However, when some local servers dominate the FL objective,
i.e.,maxkpk= Θ(1), those local servers will become bottlenecks in the convergence of FedAvg, and linear
speedup is not guaranteed. This is already observed in Li et al. (2020b), where the convergence of FedAvg is
shown empirically to slow down significantly when νmaxis large. Thus, linear speedup convergence depends
on the balance parameter νmax, and is only guaranteed when νmax=O(1).
Communication Complexity. Our bound implies a distinction in the choice of Ebetween the full and
partial participation settings. With full participation, the term involving E,O(E2/T2), is not of leading order
O(1/T), so we can increase Eand reduce the number of communication rounds without degrading the linear
speedup in iteration complexity O(1/NT), as long as E=O(/radicalbig
T/N), since thenO(E2/T2) =O(1/NT)
matches the leading term. This corresponds to a communication complexity of T/E =O(√
NT). In contrast,
the bound in Li et al. (2020b) does not allow Eto scale with√
Tto preserveO(1/T)rate, even for full
participation. On the other hand, with partial participation,κEG2/µ
KTis also a leading term, and so Emust
beO(1). In this case, our bound still yields a linear speedup in K, which is also confirmed by experiments.
The requirement that E=O(1)in order to achieve linear speedup in partial participation cannot be removed
for our sampling schemes, as the termκEG2/µ
KTcomes from variance in the sampling process.
Comparison with related works. To better understand the significance of the obtained bound, we
compare our rates to the best-known results in related settings. Haddadpour & Mahdavi (2019) proves a
linear speedupO(1/KT )result for strongly convex and smooth objectives1, withO(K1/3T2/3)communication
complexity with non- i.i.d.data and partial participation. However, their results build on the bounded
gradient diversity assumption, which implies the existence of w∗that minimizes all local objectives (see
discussions in Section 2.2 and Appendix B), effectively removing statistical heterogeneity. The bound in
Koloskova et al. (2020) matches our bound in the full participation case, but their framework excludes partial
participation (Koloskova et al., 2020, Proposition 1). Karimireddy et al. (2020) considers both types of
heterogeneities for FL and establishes linear speedup using a modified version of FedAvg with distinct learning
rates for local steps and communication rounds that are O(1/T). In contrast, our linear speedup result is for
1Their result applies to a larger class of non-convex objectives that satisfy the Polyak-Lojasiewicz condition.
6Under review as submission to TMLR
the standard FedAvg that does not use different learning rates for local and aggregation steps. Moreover, our
learning rate decays with the iteration number, and is thus generally larger in practice. When there is no
data heterogeneity, i.e. in the classical distributed optimization paradigm, communication complexity can be
further improved, e.g. Woodworth et al. (2020a;b), but such results are not directly comparable to ours since
we consider the setting where individual devices have access to different datasets. Yang et al. (2021) obtains
linear speedup results under both data and system heterogeneity for non-convex problems, so can be viewed
as complementary results.
3.2 Convex Smooth Objectives
Next we provide linear speedup analysis of FedAvg with convex and smooth objectives and show that the
optimality gap is O(1/√
KT). This result complements the strongly convex case in the previous part, as well
as the non-convex smooth setting in Jiang & Agrawal (2018); Yu et al. (2019b); Haddadpour & Mahdavi
(2019), whereO(1/√
KT)results are given in terms of averaged gradient norm, and it also extends the result
in Khaled et al. (2020), which has the best linear speedup result in the convex setting with full participation.
Theorem 2. Under Assumptions 1,3,4 and constant learning rate αt=O(/radicalig
N
T), FedAvg satisfies
min
t≤TF(wt)−F(w∗) =O/parenleftbiggνmaxσ2
√
NT+NE2LG2
T/parenrightbigg
with full participation, and with partial device participation with Ksampled devices at each communication
round and learning rate αt=O(/radicalig
K
T),
min
t≤TF(wt)−F(w∗) =O/parenleftbiggνmaxσ2
√
KT+EG2
√
KT+KE2LG2
T/parenrightbigg
.
The analysis again relies on a recursive bound, but without contraction:
E∥wt+1−w∗∥2+αt(F(wt)−F(w∗))≤E∥wt−w∗∥2+α2
t1
Nνmaxσ2+ 6α3
tE2LG2
which is then summed over time steps to give the desired bound, with αt=O(/radicalig
N
T).
Choice of Eand linear speedup. With full participation, as long as E=O(T1/4/N3/4), the convergence
rate isO(1/√
NT)withO(N3/4T3/4)communication rounds. In the partial participation setting, Emust
beO(1)in order to achieve linear speedup of O(1/√
KT). This is again due to the fact that the sampling
variance E∥wt−vt∥2=O(α2
tE2G2)cannot be made independent of E, as illustrated by Proposition 1. See
also the proof in Section E for how the sampling variance and the term EG2/√
KTare related. Our result
again demonstrates the difference in communication complexities between full and partial participation.
4 Linear Speedup Analysis of Nesterov Accelerated Federated Averaging
A natural extension of the FedAvg algorithm is to use momentum-based local updates instead of local SGD
updates in order to accelerate FedAvg. As we know from standard stochastic optimization settings, Nesterov
and other momentum updates fail to provably accelerate over SGD in general (Liu & Belkin, 2020; Kidambi
et al., 2018; Yuan & Ma, 2020). This is in contrast to the classical acceleration result of Nesterov-accelerated
gradient descent over GD. See, however, Jain et al. (2017); Even et al. (2021) for acceleration results for
quadratic objectives. Thus in the FL setting, the best provable convergence rate, in terms of dependence
onT, for FedAvg with Nesterov updates is the same as FedAvg with SGD updates. Nevertheless, Nesterov
and other momentum updates are frequently used in practice, in both non-FL and FL settings, and are
observed to perform better empirically. In fact, previous works such as Stich (2019) use FedAvg with Nesterov
or other momentum updates in their experiments to achieve target accuracy. Because of the popularity of
Nesterov and other momentum-based methods, understanding the linear speedup behavior of FedAvg with
momentum updates is important. In addition, the communication complexity required to guarantee such a
7Under review as submission to TMLR
linear speedup convergence is also a relevant question with practical implications. To our knowledge, the
majority of convergence analyses of FedAvg with momentum-based stochastic updates focus on the non-convex
smooth case (Huo et al., 2020; Yu et al., 2019a; Li et al., 2020a). In convex smooth settings, the results of
Even et al. (2021) can be adapted to prove acceleration, in terms of dependence on T, of Nesterov FedAvg
with full participation for quadratic objectives. The work of Yang et al. (2022b) establishes a O(1/√
T)rate
for Nesterov FedAvg for general convex smooth objectives under full participation. However, their convergence
result does not have linear speedup in the number of participating servers. In this section, we complete
the picture by providing the first O(1/KT )andO(1/√
KT)convergence results for Nesterov-accelerated
FedAvg for general convex objectives that match the rates for FedAvg with SGD updates. Detailed proofs of
convergence results in this section are deferred to Appendix Section F.
4.1 Strongly Convex and Smooth Objectives
The Nesterov Accelerated FedAvg algorithm follows the updates:
vk
t+1=wk
t−αtgt,k,wk
t+1=/braceleftigg
vk
t+1+βt(vk
t+1−vk
t) ift+ 1/∈IE,/summationtext
k∈St+1qk/bracketleftbig
vk
t+1+βt(vk
t+1−vk
t)/bracketrightbig
ift+ 1∈IE,
where gt,k:=∇Fk(wk
t,ξk
t)is the stochastic gradient sampled on the k-th device at time t, andqkagain
depends on participation and sampling schemes.
Theorem 3. LetvT=/summationtextN
k=1pkvk
Tin Nesterov accelerated FedAvg, and set learning rates αt=6
µ1
t+γ,
βt−1=3
14(t+γ)(1−6
t+γ) max{µ,1}. Then under Assumptions 1,2,3,4 with full device participation,
EF(vT)−F∗=O/parenleftbigg
κνmaxσ2/µ
NT+κ2E2G2/µ
T2/parenrightbigg
,
and with partial device participation with Ksampled devices at each communication round,
EF(vT)−F∗=O/parenleftbigg
κνmaxσ2/µ
NT+κEG2/µ
KT+κ2E2G2/µ
T2/parenrightbigg
.
Similar to FedAvg, the key step in the proof of this result is a recursive contraction bound, but different in
that it involves three time steps, due to the update format of Nesterov SGD (see Lemma 7 in Appendix F.1).
Then we can again use induction and L-smoothness to obtain the desired bound. To our knowledge, this is
the first convergence result for Nesterov accelerated FedAvg in the strongly convex and smooth setting. The
same discussion about linear speedup of FedAvg applies to the Nesterov accelerated variant. In particular, to
achieveO(1/NT )linear speedup, Titerations of the algorithm require only O(√
NT)communication rounds
with full participation.
To our knowledge, this is the first work that establishes linear speedup convergence of Nesterov-accelerated
FedAvg in the convex setting under both non- i.i.d.data and partial participation. Recently, there have
been significant efforts to develop novel acceleration algorithms for Federated Learning. A notable work
among these is Yuan & Ma (2020), which developed a new momentum-accelerated variant of FedAvg called
FedAc, based on the generalized accelerated SGD of Ghadimi & Lan (2012). They provided linear speedup
convergence rates under fullparticipation that match our O(1/KT )andO(1/√
KT)complexities in the
leading terms, but with an improved dependence on Ein the non-leading terms. This results in a better
communication complexity that guarantees linear speedup. However, this improvement is only present in
thefullparticipation setting. Under partial participation, the sampling variance dominates the convergence,
resulting in the same communication complexity requirements for Nesterov-accelerated FedAvg and FedAcin
order to achieve linear speedup.
4.2 Convex Smooth Objectives
We now show that the optimality gap of Nesterov-accelerated FedAvg has O(1/√
KT)rate for convex and
smooth objectives. This result complements the strongly convex case in the previous part, as well as the
8Under review as submission to TMLR
non-convex smooth setting in Huo et al. (2020); Yu et al. (2019a); Li et al. (2020a), where a similar O(1/√
KT)
rate is given in terms of averaged gradient norm. A later work (Yang et al., 2022b) establishes an O(1/√
T)
rate in the convex smooth setting for FedAvg with Nesterov updates, but only under full participation and
without linear speedup in N. In contrast, we establish linear speedup convergence for both the full and
partial participation settings.
Theorem 4. Set learning rates αt=βt=O(/radicalig
N
T). Then under Assumptions 1,3,4 Nesterov accelerated
FedAvg with full device participation has rate
min
t≤TF(vt)−F∗=O/parenleftbigg
νmaxσ2
√
NT+NE2LG2
T/parenrightbigg
,
and with partial device participation with Ksampled devices at each communication round and learning rates
αt=βt=O(/radicalbigK
T),
min
t≤TF(vt)−F∗=O/parenleftbigg
νmaxσ2
√
KT+EG2
√
KT+KE2LG2
T/parenrightbigg
.
We emphasize again that in the stochastic optimization setting with general objectives, the optimal convergence
rate that FedAvg with Nesterov udpates can achieve is the same as FedAvg with SGD updates. When
objectives are quadratic, Jain et al. (2017); Even et al. (2021) provide acceleration results for Nesterov
SGD in the centralized and decentralized settings, but acceleration with Nesterov is impossible in general.
Nevertheless, due to the popularity and superior performance of momentum methods in practice, it is still
important to understand the linear speedup behavior of such FedAvg variants. Our results in this section fill
exactly this gap, and is to our knowledge the first work to establish such results.
5 Geometric Convergence of FedAvg in Overparameterized Settings
Overparameterization is a prevalent machine learning setting where the statistical model has much more
parameters than the number of training samples and the existence of parameter choices with zero training loss
is ensured (Allen-Zhu et al., 2019; Zhang et al., 2021). This is also called the interpolating regime. Due to the
property of automatic variance reduction in the overparameterized setting, a line of recent works have proved
that SGD and accelerated methods achieve geometric convergence (Ma et al., 2018; Moulines & Bach, 2011;
Needell et al., 2014; Schmidt & Roux, 2013; Strohmer & Vershynin, 2009). A natural question is whether such
a result still holds in the Federated Learning setting. In this section, we establish the geometric convergence
of FedAvg for overparameterized strongly convex and smooth problems, and show that it preserves linear
speedup at the same time. We then sharpen this result in the special case of linear regression. Detailed proofs
are deferred to Section G. In particular, we do not need Assumptions 3 and 4 and use modified versions of
Assumptions 1 and 2 detailed in this section.
5.1 Geometric Convergence of FedAvg in the Overparameterized Setting
Recall the FL problem minw/summationtextN
k=1pkFk(w)withFk(w) =1
nk/summationtextnk
j=1ℓ(w;xj
k). In this section, we consider
the standard Empirical Risk Minimization (ERM) setting where ℓis non-negative, l-smooth, and convex,
and as before, each Fk(w)isL-smooth and µ-strongly convex. Note that l≥L. This setup includes many
important problems in practice. In the overparameterized setting, there exists w∗∈arg minw/summationtextN
k=1pkFk(w)
such thatℓ(w∗;xj
k) = 0for all xj
k. We first show that FedAvg achieves geometric convergence with linear
speedup in the number of workers.
Theorem 5. In the overparameterized setting with full participation, FedAvg with communication every E
iterations and constant step size α=O(1
EN
lνmax+L(N−νmin))has geometric convergence:
EF(wT)≤L
2(1−α)T∥w0−w∗∥2=O/parenleftbigg
Lexp/parenleftbigg
−µ
ENT
lνmax+L(N−νmin)/parenrightbigg
·∥w0−w∗∥2/parenrightbigg
.
9Under review as submission to TMLR
Linear speedup and Communication Complexity The linear speedup factor is on the order of O(N/E )
forN≤O(l
L), i.e. FedAvg with Nworkers and communication every Eiterations provides a geometric
convergence speedup factor of O(N/E ), forN≤O(l
L). In this regime, the convergence rate is of order
O(exp(−N
EκT))whereκ=l
µis the “condition number” of the objective. When Nis above this threshold,
however, the speedup is almost constant in the number of workers. This matches the findings in Ma et al.
(2018). Our result also illustrates that Ecan be takenO(Tβ)for anyβ <1to achieve geometric convergence,
achieving better communication efficiency than the standard FL setting. We emphasize again that compared
to the single-server results in Ma et al. (2018), the difference of our result lies in the factor of Nin the
speedup, which cannot be obtained if one simply applied the single-server result to each device in our problem.
Recently, Qin et al. (2022) provided a convergence bound independent of the number Eof local steps by
using a larger learning rate and under a strong growth condition on the local gradients. However, their bound
does not exhibit linear speedup in the number of local servers.
We also remark that similar geometric convergence results have been established for decentralized SGD
(also called gossip averaging), which only allows communications between connected local servers, where
the network topology is given by a possibly time-varying graph. This setting includes FedAvg with full
participation (local SGD) as a special case. See the work of Koloskova et al. (2020) for details. In comparison,
our convergence result includes a linear speedup term when N≤O(l
L).
5.2 Overparameterized Linear Regression Problems
We now turn to quadratic problems and show that the bound in Theorem 5 can be improved to
O(exp(−N
Eκ1T))for a larger range of N. The local device objectives are now given by the sum of squares
Fk(w) =1
2nk/summationtextnk
j=1(wTxj
k−zj
k)2, and there exists w∗such thatF(w∗)≡0. A notion of condition number
is important in our result: κ1which is based on local Hessians (Liu & Belkin, 2020). See Section G for a
detailed definition of κ1. The larger range of Nfor which linear speedup holds is due to κ1>κwhereκis
the condition number used in Theorem 5.
Theorem 6. For the overparamterized linear regression problem with full participation, FedAvg with commu-
nication every Eiterations with constant step size α=O(1
EN
lνmax+µ(N−νmin))has geometric convergence:
EF(wT)≤O/parenleftbigg
Lexp(−NT
E(νmaxκ1+ (N−νmin)))∥w0−w∗∥2/parenrightbigg
.
WhenN=O(κ1), the convergence rate is O((1−N
Eκ1)T) =O(exp(−NT
Eκ1)), which exhibits linear speedup in
the number of workers, as well as a 1/κ1dependence on the condition number κ1.
6 Numerical Experiments
In this section, we empirically examine the linear speedup convergence of FedAvg and Nesterov accelerated
FedAvg in various settings, including strongly convex function, convex smooth function, and overparameterized
objectives.
Setup. Following the experimental setting in Stich (2019), we conduct experiments on both synthetic
datasets and real-world dataset w8a (Platt, 1999) (d= 300,n= 49749) . We consider the distributed
objectivesF(w) =/summationtextN
k=1pkFk(w), and the objective function on the k-th local device includes three
cases: 1) Strongly convex objective : the regularized binary logistic regression problem, Fk(w) =
1
Nk/summationtextNk
i=1log(1+exp(−yk
iwTxk
i)+λ
2∥w∥2. The regularization parameter is set to λ= 1/n≈2e−5. 2)Convex
smooth objective : the binary logistic regression problem without regularization. 3) Overparameterized
setting: the linear regression problem without adding noise to the label, Fk(w) =1
Nk/summationtextNk
i=1(wTxk
i+b−yk
i)2.
Linear speedup of FedAvg and Nesterov accelerated FedAvg. To verify the linear speedup convergence
as shown in Theorems 1 2 3 4, we evaluate the number of iterations needed to reach ϵ-accuracy in three
objectives. We initialize all runs with w0=0dand measure the number of iterations to reach the target
accuracyϵ. For each configuration (E,K ), we extensively search the learning rate from min(η0,nc
1+t), where
10Under review as submission to TMLR
100101
Number of workers (N)103104Number of iterations (T)E=1
E=4
E=16
100101
Number of workers (N)103Number of iterations (T)E=1
E=4
E=16
100101
Number of workers2×1033×103Number of iterations
E=1
E=4
E=16
101
Number of active workers (K)103104Number of iterations (T)
E=1
E=4
101
Number of active workers (K)103Number of iterations (T)
E=1
E=4
101
Number of active workers (K)2×103Number of iterations (T)
E=1
E=4
100101
Number of workers (N)103104Number of iterations (T)E=1
E=4
E=16
100101
Number of workers (N)103104Number of iterations (T)E=1
E=4
E=16
100101
Number of workers (N)2×1033×103Number of iterations (T)E=1
E=4
E=16
(a) Strongly convex objective (b) Convex smooth objective (c) Linear regression
Figure 1: The linear speedup of FedAvg in full participation, partial participation, and the linear speedup of
Nesterov accelerated FedAvg, respectively.
η0∈{0.1,0.12,1,32}according to different problems and ccan take the values c= 2i∀i∈Z. As the results
shown in Figure 1, the number of iterations decreases as the number of (active) workers increasing, which
is consistent for FedAvg and Nesterov accelerated FedAvg across all scenarios. For additional experiments
on the impact of E, detailed experimental setup, and hyperparameter setting, please refer to the Appendix
Section H.
Partial participation. We verify the linear speedup in the partial participation settings, where we set 50%
of devices are active. As the results are shown in Figure 1 (2nd row), FedAvg enjoys linear speedup in various
settings even with partial device participation.
Nesterov accelerated FedAvg. In the third row of Figure 1, we report the last iteration to converge to
ϵ-accuracy of Nesterov accelerated FedAvg. The empirical observations align with Theorems 3 4 that the
accelerated version of FedAvg can also achieve the linear speedup w.r.t the number of workers.
7 Concluding Remarks
In this paper, we provided a unified linear speedup analysis of the convergence of stochastic FedAvg and
Nesterov accelerated FedAvg in convex smooth, strongly convex smooth, and overparameterized regimes in the
presence of both system and data heterogeneity, while also highlighting the distinct communication efficiency
11Under review as submission to TMLR
differences between full and partial participation of local devices. It is well known that Nesterov and other
momentum variants fail to accelerate over SGD in both the overparameterized and convex settings. Thus in
general one cannot hope to obtain theoretical acceleration results for the FedAvg algorithm with stochastic
Nesterov updates, unless objectives are quadratic (Even et al., 2021). We refer to recent works such as Yuan
& Ma (2020) for new federated learning algorithms that achieve linear speedup with better communication
complexities. Lastly, we remark that the desirable linear speedup property has been studied in other federated
versions of classical learning environments, such as federated reinforcement learning (Khodadadian et al.,
2022), and in entirely new federated learning regimes, such as the so-called anarchic federated learning (Yang
et al., 2022a).
12Under review as submission to TMLR
References
Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via over-
parameterization. In International Conference on Machine Learning , pp. 242–252. PMLR, 2019.
Zachary Charles and Jakub Konečn` y. Convergence and accuracy trade-offs in federated learning and meta-
learning. In International Conference on Artificial Intelligence and Statistics , pp. 2575–2583. PMLR,
2021.
Fei Chen, Zhenhua Dong, Zhenguo Li, and Xiuqiang He. Federated meta-learning for recommendation. arXiv
preprint arXiv:1802.07876 , 2018.
Wenlin Chen, Samuel Horváth, and Peter Richtárik. Optimal client sampling for federated learning. Transac-
tions on Machine Learning Research , 2022. URL https://openreview.net/forum?id=8GvRCWKHIL .
Mathieu Even, Raphaël Berthier, Francis Bach, Nicolas Flammarion, Hadrien Hendrikx, Pierre Gaillard,
Laurent Massoulié, and Adrien Taylor. Continuized accelerations of deterministic and stochastic gradient
descents, and of gossip algorithms. Advances in Neural Information Processing Systems , 34:28054–28066,
2021.
Saeed Ghadimi and Guanghui Lan. Optimal stochastic approximation algorithms for strongly convex
stochastic composite optimization i: A generic algorithmic framework. SIAM Journal on Optimization , 22
(4):1469–1492, 2012.
Farzin Haddadpour and Mehrdad Mahdavi. On the convergence of local descent methods in federated learning.
arXiv preprint arXiv:1910.14425 , 2019.
Andrew Hard, Chloé M Kiddon, Daniel Ramage, Francoise Beaufays, Hubert Eichner, Kanishka Rao,
Rajiv Mathews, and Sean Augenstein. Federated learning for mobile keyboard prediction, 2018. URL
https://arxiv.org/abs/1811.03604 .
Zhouyuan Huo, Qian Yang, Bin Gu, Lawrence Carin Huang, et al. Faster on-device training using new
federated momentum algorithm. arXiv preprint arXiv:2002.02090 , 2020.
Prateek Jain, Sham M Kakade, Rahul Kidambi, Praneeth Netrapalli, and Aaron Sidford. Accelerating
stochastic gradient descent. In Proc. STAT , volume 1050, pp. 26, 2017.
Peng Jiang and Gagan Agrawal. A linear speedup analysis of distributed deep learning with sparse and
quantized communication. In Advances in Neural Information Processing Systems , pp. 2525–2536, 2018.
Peter Kairouz, H Brendan McMahan, Brendan Avent, Aurélien Bellet, Mehdi Bennis, Arjun Nitin Bhagoji,
Kallista Bonawitz, Zachary Charles, Graham Cormode, Rachel Cummings, et al. Advances and open
problems in federated learning. Foundations and Trends ®in Machine Learning , 14(1–2):1–210, 2021.
Sai Praneeth Karimireddy, Satyen Kale, Mehryar Mohri, Sashank Reddi, Sebastian Stich, and
Ananda Theertha Suresh. Scaffold: Stochastic controlled averaging for federated learning. In Inter-
national Conference on Machine Learning , pp. 5132–5143. PMLR, 2020.
A Khaled, K Mishchenko, and P Richtárik. Tighter theory for local sgd on identical and heterogeneous data.
InThe 23rd International Conference on Artificial Intelligence and Statistics (AISTATS 2020) , 2020.
Ahmed Khaled, Konstantin Mishchenko, and Peter Richtárik. First analysis of local gd on heterogeneous
data.NeurIPS Workshop on Federated Learning for Data Privacy and Confidentiality , 2019.
Sajad Khodadadian, Pranay Sharma, Gauri Joshi, and Siva Theja Maguluri. Federated reinforcement
learning: Linear speedup under markovian sampling. In International Conference on Machine Learning ,
pp. 10997–11057. PMLR, 2022.
Rahul Kidambi, Praneeth Netrapalli, Prateek Jain, and Sham Kakade. On the insufficiency of existing
momentum schemes for stochastic optimization. In 2018 Information Theory and Applications Workshop
(ITA), pp. 1–9. IEEE, 2018.
13Under review as submission to TMLR
Anastasia Koloskova, Nicolas Loizou, Sadra Boreiri, Martin Jaggi, and Sebastian Stich. A unified theory
of decentralized sgd with changing topology and local updates. In International Conference on Machine
Learning , pp. 5381–5393. PMLR, 2020.
Jakub Konečn` y, H Brendan McMahan, Felix X Yu, Peter Richtárik, Ananda Theertha Suresh, and Dave Bacon.
Federated learning: Strategies for improving communication efficiency. arXiv preprint arXiv:1610.05492 ,
2016.
Monica S Lam, Giovanni Campagna, Silei Xu, Michael Fischer, and Mehrad Moradshahi. Protecting privacy
and open competition with almond: an open-source virtual assistant. XRDS: Crossroads, The ACM
Magazine for Students , 26(1):40–44, 2019.
Tian Li, Anit Kumar Sahu, Manzil Zaheer, Maziar Sanjabi, Ameet Talwalkar, and Virginia Smith. Federated
optimization in heterogeneous networks. Proceedings of Machine Learning and Systems , 2:429–450, 2020a.
Xiang Li, Kaixuan Huang, Wenhao Yang, Shusen Wang, and Zhihua Zhang. On the convergence of fedavg on
non-iid data. ICLR, 2020b.
Chaoyue Liu and Mikhail Belkin. Accelerating sgd with momentum for over-parameterized learning. ICLR,
2020.
Siyuan Ma, Raef Bassily, and Mikhail Belkin. The power of interpolation: Understanding the effectiveness
of sgd in modern over-parametrized learning. In International Conference on Machine Learning , pp.
3325–3334. PMLR, 2018.
H Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, et al. Communication-efficient learning
of deep networks from decentralized data. Proceedings of the 20 th International Conference on Artificial
Intelligence and Statistics (AISTATS) , 2017.
Eric Moulines and Francis R Bach. Non-asymptotic analysis of stochastic approximation algorithms for
machine learning. In Advances in Neural Information Processing Systems , pp. 451–459, 2011.
Deanna Needell, Rachel Ward, and Nati Srebro. Stochastic gradient descent, weighted sampling, and the
randomized kaczmarz algorithm. In Advances in neural information processing systems , pp. 1017–1025,
2014.
John C Platt. Sequential minimal optimization: A fast algorithm for training support vector machines. In
Advances in Kernel Methods-Support Vector Learning , 1999.
Tiancheng Qin, S Rasoul Etesami, and César A Uribe. Faster convergence of local sgd for over-parameterized
models.arXiv preprint arXiv:2201.12719 , 2022.
Nicola Rieke, Jonny Hancox, Wenqi Li, Fausto Milletari, Holger R Roth, Shadi Albarqouni, Spyridon Bakas,
Mathieu N Galtier, Bennett A Landman, Klaus Maier-Hein, et al. The future of digital health with
federated learning. NPJ digital medicine , 3(1):1–7, 2020.
R Tyrrell Rockafellar. Convex analysis . Number 28. Princeton university press, 1970.
Mark Schmidt and Nicolas Le Roux. Fast convergence of stochastic gradient descent under a strong growth
condition. arXiv preprint arXiv:1308.6370 , 2013.
Virginia Smith, Chao-Kai Chiang, Maziar Sanjabi, and Ameet S Talwalkar. Federated multi-task learning.
InAdvances in Neural Information Processing Systems , pp. 4424–4434, 2017.
Sebastian U Stich. Local sgd converges fast and communicates little. ICLR, 2019.
Thomas Strohmer and Roman Vershynin. A randomized kaczmarz algorithm with exponential convergence.
Journal of Fourier Analysis and Applications , 15(2):262, 2009.
Jianyu Wang and Gauri Joshi. Cooperative sgd: A unified framework for the design and analysis of
communication-efficient sgd algorithms. arXiv preprint arXiv:1808.07576 , 2018.
14Under review as submission to TMLR
Jianyu Wang, Zachary Charles, Zheng Xu, Gauri Joshi, H Brendan McMahan, Maruan Al-Shedivat, Galen
Andrew, Salman Avestimehr, Katharine Daly, Deepesh Data, et al. A field guide to federated optimization.
arXiv preprint arXiv:2107.06917 , 2021.
Shiqiang Wang, Tiffany Tuor, Theodoros Salonidis, Kin K Leung, Christian Makaya, Ting He, and Kevin
Chan. Adaptive federated learning in resource constrained edge computing systems. IEEE Journal on
Selected Areas in Communications , 37(6):1205–1221, 2019.
Blake Woodworth, Kumar Kshitij Patel, Sebastian Stich, Zhen Dai, Brian Bullins, Brendan Mcmahan, Ohad
Shamir, and Nathan Srebro. Is local sgd better than minibatch sgd? In International Conference on
Machine Learning , pp. 10334–10343. PMLR, 2020a.
Blake E Woodworth, Jialei Wang, Adam Smith, Brendan McMahan, and Nati Srebro. Graph oracle models,
lower bounds, and gaps for parallel stochastic optimization. In Advances in neural information processing
systems, pp. 8496–8506, 2018.
Blake E Woodworth, Kumar Kshitij Patel, and Nati Srebro. Minibatch vs local sgd for heterogeneous
distributed learning. Advances in Neural Information Processing Systems , 33:6281–6292, 2020b.
Haibo Yang, Minghong Fang, and Jia Liu. Achieving linear speedup with partial worker participation in
non-iid federated learning. arXiv preprint arXiv:2101.11203 , 2021.
Haibo Yang, Xin Zhang, Prashant Khanduri, and Jia Liu. Anarchic federated learning. In International
Conference on Machine Learning , pp. 25331–25363. PMLR, 2022a.
Zhengjie Yang, Wei Bao, Dong Yuan, Nguyen H Tran, and Albert Y Zomaya. Federated learning with
nesterov accelerated gradient. IEEE Transactions on Parallel and Distributed Systems , 33(12):4863–4873,
2022b.
Hao Yu, Rong Jin, and Sen Yang. On the linear speedup analysis of communication efficient momentum sgd
for distributed non-convex optimization. In International Conference on Machine Learning , pp. 7184–7193.
PMLR, 2019a.
Hao Yu, Sen Yang, and Shenghuo Zhu. Parallel restarted sgd with faster convergence and less communication:
Demystifying why model averaging works for deep learning. In Proceedings of the AAAI Conference on
Artificial Intelligence , volume 33, pp. 5693–5700, 2019b.
Honglin Yuan and Tengyu Ma. Federated accelerated stochastic gradient descent. Advances in Neural
Information Processing Systems , 33, 2020.
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep
learning (still) requires rethinking generalization. Communications of the ACM , 64(3):107–115, 2021.
Fan Zhou and Guojing Cong. On the convergence properties of a k-step averaging stochastic gradient descent
algorithm for nonconvex optimization. IJCAI, 2018.
15Under review as submission to TMLR
A Additional Notations and Bounds for Sampling Schemes
In this section, we introduce additional notations that are used throughout the proofs. Following common
practice, e.g. Stich (2019); Li et al. (2020b), we define two virtual sequences vt=/summationtextN
k=1pkvk
tandwt=/summationtextN
k=1pkwk
t, where we recall the FedAvg updates from (2):
vk
t+1=wk
t−αtgt,k,wk
t+1=/braceleftbiggvk
t+1 ift+ 1/∈IE,/summationtext
k∈St+1qkvk
t+1ift+ 1∈IE.
The following observations apply to FedAvg updates, while Nesterov accelerated FedAvg requires modifications.
For full device participation or partial participation with t /∈IE, note that vt=wt=/summationtextN
k=1pkvk
t. For partial
participation with t∈IE,wt̸=vtsince vt=/summationtextN
k=1pkvk
twhile wt=/summationtext
k∈Stqkwk
t. However, we can use
unbiased sampling strategies such that EStwt=vt. Note that vt+1is one-step SGD from wt.
vt+1=wt−αtgt, (3)
where gt=/summationtextN
k=1pkgt,kis the one-step stochastic gradient averaged over all devices.
gt,k=∇Fk/parenleftbig
wk
t,ξk
t/parenrightbig
,
Similarly, we denote the expected one-step gradient gt=Eξt[gt] =/summationtextN
k=1pkEξk
tgt,k, where
Eξk
tgt,k=∇Fk/parenleftbig
wk
t/parenrightbig
, (4)
andξt={ξk
t}N
k=1denotes random samples at all devices at time step t.
Since in this work we also consider the case of partial participation, the sampling strategy to approximate
the system heterogeneity can also affect the convergence. Here we follow the prior works Li et al. (2020b)
and Li et al. (2020a) and consider two types of sampling schemes that guarantee EStwt=vt. The sampling
scheme I establishes St+1byi.i.d.sampling the devices according to probabilities pkwith replacement, and
settingqk=1
K. In this case the upper bound of expected square norm of wt+1−vt+1is given by (Li et al.,
2020b, Lemma 5):
ESt+1∥wt+1−vt+1∥2≤4
Kα2
tE2G2. (5)
The sampling scheme II establishes St+1by uniformly sampling all devices without replacement and setting
qk=pkN
K, in which case we have
ESt+1∥wt+1−vt+1∥2≤4(N−K)
K(N−1)α2
tE2G2. (6)
We summarize these upper bounds as follows:
ESt+1∥wt+1−vt+1∥2≤4
Kα2
tE2G2. (7)
and this bound will be used in the convergence proof of the partial participation result.
B Comparison of Convergence Rates with Related Works
In this section, we compare our convergence rate with the best-known results in the literature (see Table 2).
In Haddadpour & Mahdavi (2019), the authors provide O(1/NT)convergence rate of non-convex problems
under Polyak-Łojasiewicz (PL) condition, which means their results can directly apply to the strongly convex
problems. However, their assumption is based on bounded gradient diversity, defined as follows:
Λ(w) =/summationtext
kpk∥∇Fk(w)∥2
2
∥/summationtext
kpk∇Fk(w)∥2
2≤B
16Under review as submission to TMLR
This is a more restrictive assumption comparing to assuming bounded gradient under the case of target
accuracyϵ→0and PL condition. To see this, consider the gradient diversity at the global optimal w∗, i.e.,
Λ(w∗) =/summationtext
kpk∥∇Fk(w)∥2
2
∥/summationtext
kpk∇Fk(w)∥2
2. For Λ(w∗)to be bounded, it requires ∥∇Fk(w∗)∥2
2= 0,∀k. This indicates w∗is
also the minimizer of each local objective, which contradicts to the practical setting of heterogeneous data.
Therefore, their bound is not effective for arbitrary small ϵ-accuracy under general heterogeneous data while
our convergence results still hold in this case.
Reference Convergence rate E NonIID Participation Extra Assumptions Setting
FedAvgLi et al. (2020b) O(E2
T)O(1) ✓ Partial Bounded gradient Strongly convex
FedAvgHaddadpour & Mahdavi (2019) O(1
KT)O(K−1/3T2/3)†✓‡‡Partial Bounded gradient diversity Strongly convex§
FedAvgKoloskova et al. (2020) O(1
NT)O(N−1/2T1/2) ✓ Full Bounded gradient Strongly convex
FedAvgKarimireddy et al. (2020) O(1
KT)O(N−1/2T1/2) ✓ Partial Bounded gradient dissimilarity Strongly convex
FedAvg/N-FedAvg (our analysis) O(1
KT)O(N−1/2T1/2)‡✓ Partial Bounded gradient Strongly convex
FedAvgKhaled et al. (2020) O(1√
NT)O(N−3/2T1/2) ✓ Full Bounded gradient Convex
FedAvgKoloskova et al. (2020) O(1√
NT)O(N−3/4T1/4) ✓ Full Bounded gradient Convex
FedAvgKarimireddy et al. (2020) O(1√
KT)O(N−3/4T1/4) ✓ Partial Bounded gradient dissimilarity Convex
FedAvg/N-FedAvg (our analysis) O/parenleftbig
1√
KT/parenrightbig
O(N−3/4T1/4)‡✓ Partial Bounded gradient Convex
FedAvg (our analysis) O/parenleftbig
exp(−NT
Eκ1)/parenrightbig
O(Tβ) ✓ Partial Bounded gradient Overparameterized
Table 2: A high-level summary of the convergence results in this paper compared to prior state-of-the-art FL
algorithms. This table only highlights the dependence on T(number of iterations), E(the maximal number
of local steps), N(the total number of devices), and K≤Nthe number of participated devices. κis the
condition number of the system and β∈(0,1). We denote Nesterov accelerated FedAvg as N-FedAvg in this
table.
†ThisEis obtained under i.i.d. setting.
‡ThisEis obtained under full participation setting.
§In Haddadpour & Mahdavi (2019), the convergence rate is for non-convex smooth problems with PL
condition, which also applies to strongly convex problems. Therefore, we compare it with our strongly convex
results here.
‡‡The bounded gradient diversity assumption is not applicable for general heterogeneous data when
converging to arbitrarily small ϵ-accuracy (see discussions in Sec B).
C A High-level Summary of FedAvg analysis
To facilitate the understanding of our analysis and highlight the improvement of our work comparing to prior
arts, we summarize the general steps used in the proofs across the various settings. In this section, we take
the strongly convex case as an example to illustrate our analysis. The corresponding proof for general convex
functions follows the same framework.
Algorithm 1 FedAvg : Federated Averaging
1:Server input: initial model w0, initial step size α0, local steps E.
2:Client input:
3:foreach round r= 0,1,...,R, wherer=t∗Edo
4:Sample clientsSt⊆{1,...,N}
5:Broadcast wto all clients k∈St
6:foreach client k⊆Stdo
7:initialize local model wk
t=w
8: fort=r∗E+ 1,..., (r+ 1)∗Edo
9: wk
t+1=wk
t−αtgt,k
10: end for
11: end for
12:Average the local models at server end: wt=/summationtext
k∈Stwk
t.
13:end for
17Under review as submission to TMLR
One step progress bound
This step establishes the progress of distance ( ∥wt−w∗∥2) to optimal solution after one step SGD update
(see line 9, Alg 1), as the following equation shows:
E∥wt+1−w∗∥2≤O(ηtE∥wt−w∗∥2+α2
tσ2/N+α3
tE2G2).
The above bound consists of three main ingredients, the distance to optima in previous step (with ηt∈(0,1)
to obtained a contraction bound), the variance of stochastic gradients in local clients (second term), the
variance across different clients (third term). Notice that the third term in this bound is the primary source
of improvement in the rate. Comparing to the bound in Li et al. (2020b), we improve the third term from
O(α2
tE2G2)toO(α2
tE2G2), which enables the linear speedup in the convergence rate.
Iterating the one-step bound
This step uses the one step progress bound iteratively to connect the the current distance to optimal solution
with the initial distance ( ∥w0−w∗∥2), as follows:
E∥wt+1−w∗∥2≤O(E∥w0−w∗∥21
T).
Then we can use the distance to optima to upper bound the optimality gap ( F(wt)−F∗≤O(1/T)), as
follows:
E(F(wt))−F∗≤O(E∥wt−w∗∥2).
The convergence rate of the optimality gap is equally obtained as the convergence rate of the distance to
optima.
From full participation to partial participation
There are three sources of variances that affect the convergence rate. The first two sources come from
the variances of within local clients and across clients (second and third term in one step progress bound).
The partial participation, which involves a sampling procedure, is the third source of variance. Therefore,
comparing to the rate in full participation, this will add another term of variance into the convergence rate,
where we follow a similar derivation as in Li et al. (2020b).
D Technical lemmas
To facilitate reading, we first summarize some basic properties of L-smooth and µ-strongly convex functions,
found in e.g. Rockafellar (1970), which are used in various steps of proofs in the appendix.
Lemma 1. LetFbe a convex L-smooth function. Then we have the following inequalities:
1. Quadratic upper bound: 0≤F(w)−F(w′)−⟨∇F(w′),w−w′⟩≤L
2∥w−w′∥2.
2. Coercivity:1
L∥∇F(w)−∇F(w′)∥2≤⟨∇F(w)−∇F(w′),w−w′⟩.
3. Lower bound: F(w)≥F(w′) +⟨∇F(w′),w−w′⟩+1
2L∥∇F(w)−∇F(w′)∥2. In particular,∥∇F(w)∥2≤
2L(F(w)−F(w∗)).
4. Optimality gap: F(w)−F(w∗)≤⟨∇F(w),w−w∗⟩.
Lemma 2. LetFbe aµ-strongly convex function. Then
F(w)≤F(w′) +⟨∇F(w′),w−w′⟩+1
2µ∥∇F(w)−∇F(w′)∥2
F(w)−F(w∗)≤1
2µ∥∇F(w)∥2
E Proof of Convergence Results for FedAvg
E.1 Strongly Convex Smooth Objectives
To organize our proofs more effectively and highlight the significance of our results compared to prior works,
we first state the following key lemmas used in proofs of main results and defer their proofs to later.
18Under review as submission to TMLR
Lemma 3 (One step progress, strongly convex ).Letwt=/summationtextN
k=1pkwk
t, and suppose our functions
satisfy Assumptions 1,2,3,4, and set step size αt=4
µ(γ+t)withγ=max{32κ,E}andκ=L
µ, then the updates
of FedAvg with full participation satisfy
E∥wt+1−w∗∥2≤(1−µαt)E∥wt−w∗∥2+α2
t1
Nνmaxσ2+ 6E2Lα3
tG2.
We emphasize that the above lemma is the key step that allows us to obtain a bound that improves on the
convergence result of Li et al. (2020b) with linear speedup. Its proof will make use of the following two
results.
Lemma 4 (Bounding gradient variance (Lemma 2 Li et al. (2020b)) ).Given Assumption 3, the
upper bound of gradient variance is given as follows,
E∥gt−gt∥2≤N/summationdisplay
k=1p2
kσ2
k.
Lemma 5 (Bounding the divergence of wk
t(Lemma 3 Li et al. (2020b)) ).Given Assumption 4,
and assume that αtis non-increasing and αt≤2αt+Efor allt≥0, we have
E/bracketleftiggN/summationdisplay
k=1pk∥wt−wk
t∥2/bracketrightigg
≤4E2α2
tG2.
We now restate Theorem 1 from the main text and then prove it using Lemma 3.
Theorem 1. LetwT=/summationtextN
k=1pkwk
Tin FedAvg,νmax=maxkNpk, and set decaying learning rates αt=4
µ(γ+t)
withγ= max{32κ,E}andκ=L
µ. Then under Assumptions 1,2,3,4 with full device participation,
EF(wT)−F∗=O/parenleftbiggκνmaxσ2/µ
NT+κ2E2G2/µ
T2/parenrightbigg
and with partial device participation with at most Ksampled devices at each communication round,
EF(wT)−F∗=O/parenleftbiggκEG2/µ
KT+κνmaxσ2/µ
NT+κ2E2G2/µ
T2/parenrightbigg
Proof.The road map of the proof for full device participation contains three steps. First, we establish a
recursive relationship between E∥wt+1−w∗∥2andE∥wt−w∗∥2, upper bounding the progress of FedAvg
from steptto stept+ 1. Second, we show that E∥wt−w∗∥2=O(νmaxσ2/µ
tN+E2LG2/µ2
t2 )by induction using
the recursive relationship from the previous step. Third, we use the property of L-smoothness to bound the
optimality gap by E∥wt−w∗∥2.
By Lemma 3, we have the following upper bound for the one step progress:
E∥wt+1−w∗∥2≤(1−µαt)E∥wt−w∗∥2+α2
t1
Nνmaxσ2+ 6E2Lα3
tG2.
We show next that E∥wt−w∗∥2=O(νmaxσ2/µ
tN+E2LG2/µ2
t2 )using induction. To simplify the presentation, we
denoteC≡6E2LG2andD≡1
Nνmaxσ2. Suppose that we have the bound E∥wt−w∗∥2≤b·(αtD+α2
tC)
for some constant band learning rates αt. Then the one step progress from Lemma 3 becomes:
E∥wt+1−w∗∥2≤(b(1−µαt) +αt)αtD+ (b(1−µαt) +αt)α2
tC
19Under review as submission to TMLR
To establish the result at step t+ 1, it remains to choose αtandbsuch that (b(1−µαt) +αt)αt≤bαt+1and
(b(1−µαt) +αt)α2
t≤bα2
t+1. If we letαt=4
µ(t+γ)whereγ=max{E,32κ}(choice ofγrequired to guarantee
the one step progress) and set b=4
µ, we have:
(b(1−µαt) +αt)αt=/parenleftbigg
b(1−4
t+γ) +4
µ(t+γ)/parenrightbigg4
µ(t+γ)≤b4
µ(t+γ+ 1)=bαt+1
(b(1−µαt) +αt)α2
t=b(t+γ−2
t+γ)16
µ2(t+γ)2≤b16
µ2(t+γ+ 1)2=bα2
t+1
where we have used the following inequalities:
t+γ−1
(t+γ)2≤1
(t+γ+ 1)t+γ−2
(t+γ)3≤1
(t+γ+ 1)2∀γ≥1
Thus we have established the result at step t+ 1assuming the result is correct at step t:
E∥wt+1−w∗∥2≤b·(αt+1D+α2
t+1C)
At stept= 0, we can ensure the following inequality by scaling bwithc∥w0−w∗∥2for a sufficiently large
constantc:
∥w0−w∗∥2≤b·(α0D+α2
0C) =b·(4
µγD+16
µ2γ2C)
It follows that
E∥wt−w∗∥2≤c∥w0−w∗∥24
µ(Dαt+Cα2
t) (8)
for allt≥0.
Finally, the L-smoothness of Fimplies
E(F(wT))−F∗≤L
2E∥wT−w∗∥2
≤L
2c∥w0−w∗∥24
µ(DαT+Cα2
T)
= 2c∥w0−w∗∥2κ(DαT+Cα2
T)
≤2c∥w0−w∗∥2κ/bracketleftbigg4
µ(T+γ)·1
Nνmaxσ2+ 6E2LG2·(4
µ(T+γ))2/bracketrightbigg
=O(κ
µ1
Nνmaxσ2·1
T+κ2
µE2G2·1
T2)
where in the first line, we use the property of L-smooth function (see Lemma 1), and in the second line, we
use the conclusion in Eq (8).
With partial participation, the update at each communication round is now given by weighted averages over
a subset of sampled devices. When t+ 1/∈IE,vt+1=wt+1, while when t+ 1∈IE, we have Ewt+1=vt+1
by design of the sampling schemes (Li et al. (2020b), Lemma 4).
Lett+ 1andt+ 1 +Ebe two consecutive communication rounds. Using Ewt+E+1=vt+E+1, we can write
E∥wt+E+1−w∗∥2=E∥wt+E+1−vt+E+1+vt+E+1−w∗∥2
=E∥vt+E+1−w∗∥2+E∥wt+E+1−vt+E+1∥2
We note that applying the one step progress consecutively to the sequence vtwith full participation setting
starting from round t+ 1and stopping at round t+ 1 +E, we can bound the first term as
E∥vt+E+1−w∗∥2≤(1−µαt+1)E∥wt+1−w∗∥2+Eα2
t+11
Nνmaxσ2+ 6E3Lα3
t+1G2
20Under review as submission to TMLR
The bound for E∥wt+E+1−vt+E+1∥2for the two sampling schemes we consider is provided in Eq (7) as
E∥wt+E+1−vt+E+1∥2≤E∥wt+E+1−wt+1∥2
≤4
Kα2
tE2G2
which yields
E∥wt+E+1−w∗∥2≤(1−µαt+1)E∥wt+1−w∗∥2+Eα2
t+11
Nνmaxσ2+ 6E3Lα3
t+1G2+4
Kα2
tE2G2
We note that this is similar to the one-step progress bound in Karimireddy et al. (2020) for two consecutive
communication rounds. From here, using the same induction argument (effectively T/Etimes instead of T
times) and L-smoothness as the full participation case implies
EF(wT)−F∗=O(κνmaxσ2/µ
NT+κEG2/µ
KT+κ2E2G2/µ
T2)
The advantage of bounding the square distance to optimum between consecutive communication rounds
is that it results in bounding the sampling variance E∥wt−vt∥2T/Einstead ofTtimes, which gives an
O(E/KT )term instead ofO(E2/KT )in the convergence result.
E.1.1 Deferred proofs of key lemmas
Here we first rewrite the proofs of lemmas 4 and 5 from Li et al. (2020b) with slight modifications for the
consistency and completeness of this work, since later we will use modified versions of these results in the
convergence proof for Nesterov accelerated FedAvg.
Proof of lemma 4.
E∥gt−gt∥2=E∥gt−Egt∥2=N/summationdisplay
k=1p2
k∥gt,k−Egt,k∥2≤N/summationdisplay
k=1p2
kσ2
k
Proof of lemma 5. Now we bound E/summationtextN
k=1pk∥wt−wk
t∥2following Li et al. (2020b). Since communication
is done every Esteps, for any t≥0, we can find a t0≤tsuch thatt−t0≤E−1andwk
t0=wt0for allk.
Moreover, using αtis non-increasing and αt0≤2αtfor anyt−t0≤E−1, we have
EN/summationdisplay
k=1pk∥wt−wk
t∥2
=EN/summationdisplay
k=1pk∥wk
t−wt0−(wt−wt0)∥2
≤EN/summationdisplay
k=1pk∥wk
t−wt0∥2
=EN/summationdisplay
k=1pk∥wk
t−wk
t0∥2
=EN/summationdisplay
k=1pk∥−t−1/summationdisplay
i=t0αigi,k∥2
≤2N/summationdisplay
k=1pkEt−1/summationdisplay
i=t0Eα2
i∥gi,k∥2
21Under review as submission to TMLR
≤2N/summationdisplay
k=1pkE2α2
t0G2
≤4E2α2
tG2
Based on the results of Lemma 4, 5, we now prove the upper bound of one step SGD progress. This proof
improves on the previous work Li et al. (2020b) and reveals the linear speedup of convergence of FedAvg.
Proof of lemma 3. We have
∥wt+1−w∗∥2=∥(wt−αtgt)−w∗∥2=∥(wt−αtgt−w∗)−αt(gt−gt)∥2
=∥wt−w∗−αtgt∥2
/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
A1+ 2αt⟨wt−w∗−αtgt,gt−gt⟩/bracehtipupleft/bracehtipdownright/bracehtipdownleft /bracehtipupright
A2+α2
t∥gt−gt∥2
/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
A3
where we denote:
A1=∥wt−w∗−αtgt∥2
A2= 2αt⟨wt−w∗−αtgt,gt−gt⟩
A3=α2
t∥gt−gt∥2
By definition of gtandgt(see Eq (4)), we have EA2= 0. ForA3, we have the following upper bound (see
Lemma 4):
α2
tE∥gt−gt∥2≤α2
tN/summationdisplay
k=1p2
kσ2
k
Next we bound A1:
∥wt−w∗−αtgt∥2=∥wt−w∗∥2+ 2⟨wt−w∗,−αtgt⟩+∥αtgt∥2
and we will show that the third term ∥αtgt∥2can be canceled by an upper bound of the second term, which
is one of major improvement comparing to prior art Li et al. (2020b). The upper bound of second term can
be derived as follows, using the strong convexity and L-smoothness of Fk:
−2αt⟨wt−w∗,gt⟩
=−2αtN/summationdisplay
k=1pk⟨wt−w∗,∇Fk(wk
t)⟩
=−2αtN/summationdisplay
k=1pk⟨wt−wk
t,∇Fk(wk
t)⟩−2αtN/summationdisplay
k=1pk⟨wk
t−w∗,∇Fk(wk
t)⟩
≤−2αtN/summationdisplay
k=1pk⟨wt−wk
t,∇Fk(wk
t)⟩+ 2αtN/summationdisplay
k=1pk(Fk(w∗)−Fk(wk
t))−αtµN/summationdisplay
k=1pk∥wk
t−w∗∥2
≤2αtN/summationdisplay
k=1pk/bracketleftbigg
Fk(wk
t)−Fk(wt) +L
2∥wt−wk
t∥2+Fk(w∗)−Fk(wk
t)/bracketrightbigg
−αtµ∥N/summationdisplay
k=1pkwk
t−w∗∥2
=αtLN/summationdisplay
k=1pk∥wt−wk
t∥2+ 2αtN/summationdisplay
k=1pk[Fk(w∗)−Fk(wt)]−αtµ∥wt−w∗∥2
We record the bound we have obtained so far, as it will also be used in the proof for convex case:
E∥wt+1−w∗∥2≤E(1−µαt)∥wt−w∗∥2+αtLN/summationdisplay
k=1pk∥wt−wk
t∥2
22Under review as submission to TMLR
+ 2αtN/summationdisplay
k=1pk[Fk(w∗)−Fk(wt)] +α2
tN/summationdisplay
k=1p2
kσ2
k+α2
t∥gt∥2(9)
For the term 2αt/summationtextN
k=1pk[Fk(w∗)−Fk(wt)], which is negative, we can ignore it, but this yields a suboptimal
bound that fails to provide the desired linear speedup. Instead, we upper bound it using the following
derivation:
2αtN/summationdisplay
k=1pk[Fk(w∗)−Fk(wt)]
≤2αt[F(wt+1)−F(wt)]
≤2αtE⟨∇F(wt),wt+1−wt⟩+αtLE∥wt+1−wt∥2
=−2α2
tE⟨∇F(wt),gt⟩+α3
tLE∥gt∥2
=−2α2
tE⟨∇F(wt),gt⟩+α3
tLE∥gt∥2
=−α2
t/bracketleftbig
∥∇F(wt)∥2+∥gt∥2−∥∇F(wt)−gt∥2/bracketrightbig
+α3
tLE∥gt∥2
=−α2
t/bracketleftigg
∥∇F(wt)∥2+∥gt∥2−∥∇F(wt)−/summationdisplay
kpk∇F(wk
t)∥2/bracketrightigg
+α3
tLE∥gt∥2
≤−α2
t/bracketleftigg
∥∇F(wt)∥2+∥gt∥2−/summationdisplay
kpk∥∇F(wt)−∇F(wk
t)∥2/bracketrightigg
+α3
tLE∥gt∥2
≤−α2
t/bracketleftigg
∥∇F(wt)∥2+∥gt∥2−L2/summationdisplay
kpk∥wt−wk
t∥2/bracketrightigg
+α3
tLE∥gt∥2
≤−α2
t∥gt∥2+α2
tL2/summationdisplay
kpk∥wt−wk
t∥2+α3
tLE∥gt∥2−α2
t∥∇F(wt)∥2
where we have used the smoothness of Ftwice.
Note that the term −α2
t∥gt∥2exactly cancels the α2
t∥gt∥2in the bound in Eq (9), so that plugging in the
bound for−2αt⟨wt−w∗,gt⟩, we have so far proved
E∥wt+1−w∗∥2≤E(1−µαt)∥wt−w∗∥2+αtLN/summationdisplay
k=1pk∥wt−wk
t∥2+α2
tN/summationdisplay
k=1p2
kσ2
k
+α2
tL2N/summationdisplay
k=1pk∥wt−wk
t∥2+α3
tLE∥gt∥2−α2
t∥∇F(wt)∥2(10)
Under Assumption 4, we have E∥gt∥2≤G2. Furthermore, we can check that our choice of αtsatisfiesαt
is non-increasing and αt≤2αt+E, so we may plug in the bound E/summationtextN
k=1pk∥wt−wk
t∥2≤4E2α2
tG2to the
above inequality (see Lemma 5).
Therefore, we can conclude that, with νmax:=N·maxkpkandνmin:=N·minkpk,
E∥wt+1−w∗∥2
≤E(1−µαt)∥wt−w∗∥2+ 4E2Lα3
tG2+ 4E2L2α4
tG2+α2
tN/summationdisplay
k=1p2
kσ2
k+α3
tLG2
=E(1−µαt)∥wt−w∗∥2+ 4E2Lα3
tG2+ 4E2L2α4
tG2+α2
t1
NN/summationdisplay
k=1(pkN)pkσ2
k+α3
tLG2
≤E(1−µαt)∥wt−w∗∥2+ 4E2Lα3
tG2+ 4E2L2α4
tG2+α2
t1
NνmaxN/summationdisplay
k=1pkσ2
k+α3
tLG2
23Under review as submission to TMLR
≤E(1−µαt)∥wt−w∗∥2+ 6E2Lα3
tG2+α2
t1
Nνmaxσ2
where in the last inequality we use σ2=/summationtextN
k=1pkσ2
k, and that by construction αtsatisfiesLαt≤1
8.
One may ask whether the dependence on Ein the termκE2G2/µ
KTcan be removed, or equivalently whether/summationtext
kpk∥wk
t−wt∥2=O(1/T2)can be independent of E. We provide a simple counterexample that shows
that this is not possible in general.
Proposition 1. There exists a dataset such that if E=O(Tβ)for anyβ >0then/summationtext
kpk∥wk
t−wt∥2=
Ω(1
T2−2β).
Proof.Suppose that we have an even number of devices and each Fk(w) =1
nk/summationtextnk
j=1(xj
k−w)2contains data
points xj
k=w∗,k, withnk≡n. Moreover, the w∗,k’s come in pairs around the origin. As a result, the global
objectiveFis minimized at w∗= 0. Moreover, if we start from w0= 0, then by design of the dataset the
updates in local steps exactly cancel each other at each iteration, resulting in wt= 0for allt. On the other
hand, ifE=Tβ, then starting from any t=O(T)with constant step size O(1
T), afterEiterations of local
steps, the local parameters are updated towards w∗,kwith∥wk
t+E∥2= Ω((Tβ·1
T)2) = Ω(1
T2−2β). This implies
that
/summationdisplay
kpk∥wk
t+E−wt+E∥2=/summationdisplay
kpk∥wk
t+E∥2
= Ω(1
T2−2β)
which is at a slower rate than1
T2for anyβ > 0. Thus the sampling variance E∥wt+1−vt+1∥2=
Ω(/summationtext
kpkE∥wk
t+1−wt+1∥2)decays at a slower rate than1
T2, resulting in a convergence rate slower than O(1
T)
with partial participation.
E.2 Convex Smooth Objectives
In this section we provide the proof of the convergence result for FedAvg with convex and smooth objectives.
The key step is a one step progress result analogous to that in the strongly convex case, and their proofs
share identical components as well.
Lemma 6 (One step progress, convex case ).Letwt=/summationtextN
k=1pkwk
tin FedAvg. Under assumptions 1,3,4,
the following bound holds for all t:
E∥wt+1−w∗∥2+αt(F(wt)−F(w∗))≤E∥wt−w∗∥2+α2
t1
Nνmaxσ2+ 6α3
tE2LG2
Proof.The first part of the proof follows directly from Eq (9) in the proof of Lemma 3. Setting µ= 0in
Eq (9) (since we are in the convex setting instead of strongly convex), we obtain
∥wt+1−w∗∥2≤∥wt−w∗∥2+αtLN/summationdisplay
k=1pk∥wt−wk
t∥2
+ 2αtN/summationdisplay
k=1pk[Fk(w∗)−Fk(wt)] +α2
t∥gt∥2+α2
tN/summationdisplay
k=1p2
kσ2
k
The difference of this bound with that in the strongly convex case is that we no longer have a contraction
factor of 1−µαtin front of∥wt−w∗∥2. In the strongly convex case, we were able to cancel α2
t∥gt∥2with
2αt/summationtextN
k=1pk[Fk(w∗)−Fk(wt)]and obtain only lower order terms. In the convex case, we use a different
strategy and preserve/summationtextN
k=1pk[Fk(w∗)−Fk(wt)]in order to obtain the desired optimality gap.
24Under review as submission to TMLR
More precisely, we have
∥gt∥2=∥/summationdisplay
kpk∇Fk(wk
t)∥2
=∥/summationdisplay
kpk∇Fk(wk
t)−/summationdisplay
kpk∇Fk(wt) +/summationdisplay
kpk∇Fk(wt)∥2
≤2∥/summationdisplay
kpk∇Fk(wk
t)−/summationdisplay
kpk∇Fk(wt)∥2+ 2∥/summationdisplay
kpk∇Fk(wt)∥2
≤2L2/summationdisplay
kpk∥wk
t−wt∥2+ 2∥/summationdisplay
kpk∇Fk(wt)∥2
= 2L2/summationdisplay
kpk∥wk
t−wt∥2+ 2∥∇F(wt)∥2
using∇F(w∗) = 0. Now using the Lsmoothness of F, we have∥∇F(wt)∥2≤2L(F(wt)−F(w∗)), so that
∥wt+1−w∗∥2
≤∥wt−w∗∥2+αtLN/summationdisplay
k=1pk∥wt−wk
t∥2+ 2αtN/summationdisplay
k=1pk[Fk(w∗)−Fk(wt)]
+ 2α2
tL2/summationdisplay
kpk∥wk
t−wt∥2+ 4α2
tL(F(wt)−F(w∗)) +α2
tN/summationdisplay
k=1p2
kσ2
k
=∥wt−w∗∥2+ (2α2
tL2+αtL)N/summationdisplay
k=1pk∥wt−wk
t∥2+αtN/summationdisplay
k=1pk[Fk(w∗)−Fk(wt)]
+α2
tN/summationdisplay
k=1p2
kσ2
k+αt(1−4αtL)(F(w∗)−F(wt))
SinceF(w∗)≤F(wt), as long as 4αtL≤1, we can ignore the last term, and rearrange the inequality to
obtain
∥wt+1−w∗∥2+αt(F(wt)−F(w∗))
≤∥wt−w∗∥2+ (2α2
tL2+αtL)N/summationdisplay
k=1pk∥wt−wk
t∥2+α2
tN/summationdisplay
k=1p2
kσ2
k
≤∥wt−w∗∥2+3
2αtLN/summationdisplay
k=1pk∥wt−wk
t∥2+α2
tN/summationdisplay
k=1p2
kσ2
k
The same argument as before yields E/summationtextN
k=1pk∥wt−wk
t∥2≤4E2α2
tG2which gives
∥wt+1−w∗∥2+αt(F(wt)−F(w∗))≤∥wt−w∗∥2+α2
tN/summationdisplay
k=1p2
kσ2
k+ 6α3
tE2LG2
≤∥wt−w∗∥2+α2
t1
Nνmaxσ2+ 6α3
tE2LG2
With the one step progress result, we can now prove the convergence result in the convex setting, which we
restate below.
Theorem 2. Under assumptions 1,3,4 and constant learning rate αt=O(/radicalig
N
T), FedAvg satisfies
min
t≤TF(wt)−F(w∗) =O/parenleftbiggνmaxσ2
√
NT+NE2LG2
T/parenrightbigg
25Under review as submission to TMLR
with full participation, and with partial device participation with Ksampled devices at each communication
round and learning rate αt=O(/radicalig
K
T),
min
t≤TF(wt)−F(w∗) =O/parenleftbiggνmaxσ2
√
KT+EG2
√
KT+KE2LG2
T/parenrightbigg
Proof.We first prove the bound for full participation. Applying Lemma 6, we have
∥wt+1−w∗∥2+αt(F(wt)−F(w∗))≤∥wt−w∗∥2+α2
t1
Nνmaxσ2+ 6α3
tE2LG2
Summing the inequalities from t= 0tot=T, we obtain
T/summationdisplay
t=0αt(F(wt)−F(w∗))≤∥w0−w∗∥2+T/summationdisplay
t=0α2
t·1
Nνmaxσ2+T/summationdisplay
t=0α3
t·6E2LG2
so that
min
t≤TF(wt)−F(w∗)≤1/summationtextT
t=0αt/parenleftigg
∥w0−w∗∥2+T/summationdisplay
t=0α2
t·1
Nνmaxσ2+T/summationdisplay
t=0α3
t·6E2LG2/parenrightigg
By setting the constant learning rate αt≡/radicalig
N
T, we have
min
t≤TF(wt)−F(w∗)≤1√
NT·∥w0−w∗∥2+1√
NTT·N
T·1
Nνmaxσ2+1√
NTT(/radicalbigg
N
T)36E2LG2
≤1√
NT·∥w0−w∗∥2+1√
NTT·N
T·1
Nνmaxσ2+N
T6E2LG2
= (∥w0−w∗∥2+νmaxσ2)1√
NT+N
T6E2LG2
=O(νmaxσ2
√
NT+NE2LG2
T)
For partial participation, the one step progress bound in Lemma 6 is updated in a similar manner as the
strongly convex case to incorporate the sampling variance. More precisely, with partial participation, we
consider two consecutive communication rounds t+ 1andt+ 1 +E. Using Ewt+E+1=vt+E+1, we can write
E∥wt+E+1−w∗∥2=E∥wt+E+1−vt+E+1+vt+E+1−w∗∥2
=E∥vt+E+1−w∗∥2+E∥wt+E+1−vt+E+1∥2
Again, the first term can be bounded by applying the one-step bound Etimes and summing it up, giving
E∥vt+E+1−w∗∥2+t+E/summationdisplay
t+1αt(F(vt)−F(w∗))≤E∥wt+1−w∗∥2+t+E/summationdisplay
t+1α2
t1
Nνmaxσ2+ 6t+E/summationdisplay
t+1α3
tE2LG2
The bound for E∥wt+E+1−vt+E+1∥2for the two sampling schemes we consider is again provided in Eq (7),
giving the following E-step progress bound
E∥wt+E+1−w∗∥2+t+E/summationdisplay
t+1αt(F(vt)−F(w∗))≤E∥wt+1−w∗∥2+t+E/summationdisplay
t+1α2
t1
Nνmaxσ2+ 6t+E/summationdisplay
t+1α3
tE2LG2+4
Kα2
t+1E2G2
26Under review as submission to TMLR
Summing up the above bounds T/Etimes,
min
t≤TF(wt)−F(w∗)≤1/summationtextT
t=0αt
∥w0−w∗∥2+T/summationdisplay
t=0α2
t1
Nνmaxσ2+/summationdisplay
t=E,2E,...4
Kα2
t+1E2G2+T/summationdisplay
t=0α3
t·6E2LG2
,
so that with αt=/radicalig
K
T, we have
min
t≤TF(wt)−F(w∗) =O(νmaxσ2
√
KT+EG2
√
KT+KE2LG2
T).
F Proof of Convergence Results for Nesterov Accelerated FedAvg
F.1 Strongly Convex Smooth Objectives
Recall that the Nesterov accelerated FedAvg follows the updates
vk
t+1=wk
t−αtgt,k,wk
t+1=/braceleftigg
vk
t+1+βt(vk
t+1−vk
t) ift+ 1/∈IE,/summationtext
k∈St+1qk/bracketleftbig
vk
t+1+βt(vk
t+1−vk
t)/bracketrightbig
ift+ 1∈IE.
The proofs of convergence results for Nesterov Accelerated FedAvg consists of components that are direct
analogues of the FedAvg case. We first state these analogue results before proving the main theorem. Like
before, the proofs of the lemmas are deferred to after the main proof.
Lemma 7 (One step progress, Nesterov ).Letvt=/summationtextN
k=1pkvk
tin Nesterov accelerated FedAvg, and
suppose our functions satisfy Assumptions 1,2,3,4, and set step sizes αt=6
µ1
t+γ,βt−1=3
14(t+γ)(1−6
t+γ) max{µ,1}
withγ= max{32κ,E}andκ=L
µ, the updates of Nesterov accelerated FedAvg satisfy
E∥vt+1−w∗∥2≤E(1−µαt)(1 +βt−1)2∥vt−w∗∥2+ 20E2Lα3
tG2+ (1−αtµ)β2
t−1∥(vt−1−w∗)∥2
+α2
t1
Nνmaxσ2+ 2βt−1(1 +βt−1)(1−αtµ)∥vt−w∗∥·∥vt−1−w∗∥
The one step progress result makes use of the same bound on the gradient variance in Lemma 4, as well as a
divergence bound analogous to Lemma 5, which we state below.
Lemma 8 (Bounding the divergence of wk
t, Nesterov ).Given Assumption 4, and assume that αtis
non-increasing, αt≤2αt+E, and 2β2
t−1+ 2α2
t≤1/2for allt≥0,wt=/summationtextN
k=1pkwk
tin Nesterov accelerated
FedAvg satisfies
E/bracketleftiggN/summationdisplay
k=1pk∥wt−wk
t∥2/bracketrightigg
≤16(E−1)2α2
tG2.
Theorem 3. LetvT=/summationtextN
k=1pkvk
Tin Nesterov accelerated FedAvg and set learning rates αt=6
µ1
t+γ,
βt−1=3
14(t+γ)(1−6
t+γ) max{µ,1}. Then under Assumptions 1,2,3,4 with full device participation,
EF(vT)−F∗=O/parenleftbiggκνmaxσ2/µ
NT+κ2E2G2/µ
T2/parenrightbigg
,
and with partial device participation with Ksampled devices at each communication round,
EF(vT)−F∗=O/parenleftbiggκνmaxσ2/µ
NT+κE2G2/µ
KT+κ2E2G2/µ
T2/parenrightbigg
.
27Under review as submission to TMLR
Proof.We first prove the result for full participation. Applying the one step progress bound in Lemma 7, we
have
E∥vt+1−w∗∥2≤E(1−µαt)(1 +βt−1)2∥vt−w∗∥2+ 20E2Lα3
tG2+ (1−αtµ)β2
t−1∥(vt−1−w∗)∥2
+α2
t1
Nνmaxσ2+ 2βt−1(1 +βt−1)(1−αtµ)∥vt−w∗∥·∥vt−1−w∗∥
Recall that we require αt0≤2αtfor anyt−t0≤E−1,Lαt≤1
5, and 2β2
t−1+ 2α2
t≤1/2in order for
Lemmas 8 and 7 to hold, which we can check by definition of αtandβt.
We show next that E∥vt−w∗∥2=O(νmaxσ2/µ
tN+E2LG2/µ2
t2 )by induction. Assume that we have shown
E∥vt−w∗∥2≤b(Cα2
t+Dαt)
for all iterations until t, whereC= 20E2LG2,D=1
Nνmaxσ2, andbis some constant to be chosen later. For
step sizes recall that we choose αt=6
µ1
t+γandβt−1=3
14(t+γ)(1−6
t+γ) max{µ,1}whereγ=max{32κ,E}, so
thatβt−1≤αtand
(1−µαt)(1 + 14βt−1)≤(1−6
t+γ)(1 +3
(t+γ)(1−6
t+γ))
= 1−6
t+γ+3
t+γ= 1−3
t+γ= 1−µαt
2
Moreover, E∥vt−1−w∗∥2≤b(Cα2
t−1+Dαt−1)≤4b(Cα2
t+Dαt)with the chosen step sizes. Therefore the
bound for E∥vt+1−w∗∥2can be further simplified with
2βt−1(1 +βt−1)(1−αtµ)E∥vt−w∗∥·∥vt−1−w∗∥≤4βt−1(1 +βt−1)(1−αtµ)·b(Cα2
t+Dαt)
and
(1−αtµ)β2
t−1E∥(vt−1−w∗)∥2≤4(1−αtµ)β2
t−1·b(Cα2
t+Dαt)
so that
E∥vt+1−w∗∥2≤(1−µαt)((1 +βt−1)2+ 4βt−1(1 +βt−1) + 4β2
t−1)·b(Cα2
t+Dαt)
+ 20E2Lα3
tG2+α2
t1
Nνmaxσ2
≤E(1−µαt)(1 + 14βt−1)·b(Cα2
t+Dαt) + 20E2Lα3
tG2+α2
t1
Nνmaxσ2
≤b(1−µαt
2)(Cα2
t+Dαt) +Cα3
t+Dα2
t
= (b(1−µαt
2) +αt)α2
tC+ (b(1−µαt
2) +αt)αtD
and so it remains to choose bsuch that
(b(1−µαt
2) +αt)αt≤bαt+1
(b(1−µαt
2) +αt)α2
t≤bα2
t+1
from which we can conclude E∥vt+1−w∗∥2≤α2
t+1C+αt+1D.
Withb=6
µ, we have
(b(1−µαt
2) +αt)αt= (b(1−(3
t+γ) +6
µ(t+γ))6
µ(t+γ)
28Under review as submission to TMLR
= (bt+γ−3
t+γ+6
µ(t+γ))6
µ(t+γ)
≤b(t+γ−1
t+γ)6
µ(t+γ)
≤b6
µ(t+γ+ 1)=bαt+1
where we have usedt+γ−1
(t+γ)2≤1
t+γ+1.
Similarly
(b(1−µαt
2) +αt)α2
t= (b(1−(3
t+γ) +6
µ(t+γ))(6
µ(t+γ))2
= (bt+γ−3
t+γ+6
µ(t+γ))(6
µ(t+γ))2
=b(t+γ−2
t+γ)(6
µ(t+γ))2
≤b36
µ2(t+γ+ 1)2=bα2
t+1
where we have usedt+γ−2
(t+γ)3≤1
(t+γ+1)2.
Finally, to ensure ∥v0−w∗∥2≤b(Cα2
0+Dα0), we can rescale bbyc∥v0−w∗∥2for somec.It follows that
E∥vt−w∗∥2≤b(Cα2
t+Dαt)for allt≥0. Using the L-smooothness of F,
E(F(vT))−F∗=E(F(vT)−F(w∗))
≤L
2E∥vT−w∗∥2≤L
2c∥v0−w∗∥26
µ(DαT+Cα2
T)
= 3c∥v0−w∗∥2κ(DαT+Cα2
T)
≤3c∥v0−w∗∥2κ/bracketleftbigg6
µ(T+γ)·1
Nνmaxσ2+ 20E2LG2·(6
µ(T+γ))2/bracketrightbigg
=O(κ
µ1
Nνmaxσ2·1
T+κ2
µE2G2·1
T2)
With partial participation, the same argument as in the FedAvg case in Theorem 1 by adding a term for
sampling error every Esteps yields
EF(wT)−F∗=O(κνmaxσ2/µ
NT+κEG2/µ
KT+κ2E2G2/µ
T2)
F.1.1 Deferred proofs of key lemmas
Proof of lemma 8. The proof of bound for E/summationtextN
k=1pk∥wt−wk
t∥2in the Nesterov accelerated FedAvg follows
a similar logic as in Lemma 5, but requires extra reasoning. Since communication is done every Esteps,
for anyt≥0, we can find a t0≤tsuch thatt−t0≤E−1andwk
t0=wt0for allk. Moreover, using αtis
non-increasing, αt0≤2αt, andβt≤αtfor anyt−t0≤E−1, we have
EN/summationdisplay
k=1pk∥wt−wk
t∥2=EN/summationdisplay
k=1pk∥wk
t−wt0−(wt−wt0)∥2
≤EN/summationdisplay
k=1pk∥wk
t−wt0∥2
29Under review as submission to TMLR
=EN/summationdisplay
k=1pk∥wk
t−wk
t0∥2
=EN/summationdisplay
k=1pk∥t−1/summationdisplay
i=t0βi(vk
i+1−vk
i)−t−1/summationdisplay
i=t0αigi,k∥2
≤2N/summationdisplay
k=1pkEt−1/summationdisplay
i=t0(E−1)α2
i∥gi,k∥2+ 2N/summationdisplay
k=1pkEt−1/summationdisplay
i=t0(E−1)β2
i∥(vk
i+1−vk
i)∥2
≤2N/summationdisplay
k=1pkEt−1/summationdisplay
i=t0(E−1)α2
i(∥gi,k∥2+∥(vk
i+1−vk
i)∥2)
≤4N/summationdisplay
k=1pkEt−1/summationdisplay
i=t0(E−1)α2
iG2
≤4(E−1)2α2
t0G2≤16(E−1)2α2
tG2
where we have used E∥vk
t−vk
t−1∥2≤G2. To see this identity for appropriate αt,βt, note the recursion
vk
t+1−vk
t=wk
t−wk
t−1−(αtgt,k−αt−1gt−1,k)
wk
t+1−wk
t=−αtgt,k+βt(vk
t+1−vk
t)
so that
vk
t+1−vk
t=−αt−1gt−1,k+βt−1(vk
t−vk
t−1)−(αtgt,k−αt−1gt−1,k)
=βt−1(vk
t−vk
t−1)−αtgt,k
Since the identity vk
t+1−vk
t=βt−1(vk
t−vk
t−1)−αtgt,kimplies
E∥vk
t+1−vk
t∥2≤2β2
t−1E∥vk
t−vk
t−1∥2+ 2α2
tG2
as long asαt,βt−1satisfy 2β2
t−1+2α2
t≤1/2, we can guarantee that E∥vk
t−vk
t−1∥2≤G2for allkby induction.
This together with Jensen’s inequality also gives E∥vt−vt−1∥2≤G2for allt.
Now we are ready to prove the one step progress result for Nesterov accelerated FedAvg. The first part of the
proof is identical to that of the FedAvg case, while the main recursion takes a different form.
Proof of lemma 7. We again have
∥vt+1−w∗∥2=∥(wt−αtgt)−w∗∥2
and using exactly the same derivation as the FedAvg case, we can obtain the following bound (same as Eq (10)
in the proof of Lemma 3):
E∥wt+1−w∗∥2≤E(1−µαt)∥wt−w∗∥2+αtLN/summationdisplay
k=1pk∥wt−wk
t∥2+α2
tN/summationdisplay
k=1p2
kσ2
k
+α2
tL2N/summationdisplay
k=1pk∥wt−wk
t∥2+α3
tLE∥gt∥2−α2
t∥∇F(wt)∥2
Different from the FedAvg case, we no longer have wt=vt. Instead,
∥wt−w∗∥2=∥vt+βt−1(vt−vt−1)−w∗∥2
=∥(1 +βt−1)(vt−w∗)−βt−1(vt−1−w∗)∥2
30Under review as submission to TMLR
= (1 +βt−1)2∥vt−w∗∥2−2βt−1(1 +βt−1)⟨vt−w∗,vt−1−w∗⟩+β2
t−1∥(vt−1−w∗)∥2
≤(1 +βt−1)2∥vt−w∗∥2+ 2βt−1(1 +βt−1)∥vt−w∗∥·∥vt−1−w∗∥+β2
t−1∥(vt−1−w∗)∥2
which gives a recursion involving both vtandvt−1:
∥vt+1−w∗∥2≤(1−αtµ)(1 +βt−1)2∥vt−w∗∥2+ 2(1−αtµ)βt−1(1 +βt−1)∥vt−w∗∥·∥vt−1−w∗∥+α2
tN/summationdisplay
k=1p2
kσ2
k
+β2
t−1(1−αtµ)∥(vt−1−w∗)∥2+αtLN/summationdisplay
k=1pk∥wt−wk
t∥2+α2
tL2/summationdisplay
kpk∥wt−wk
t∥2+α3
tLG2
and we will using this recursive relation to obtain the desired bound.
We can check that our choice of αtandβtsatisfyαtis non-increasing, αt≤2αt+E, and 2β2
t−1+ 2α2
t≤1/2
for allt≥0, so that we can apply the bound from Lemma 8 on E/summationtextN
k=1pk∥wt−wk
t∥2to conclude that, with
νmax:=N·maxkpk,
E∥vt+1−w∗∥2≤E(1−µαt)(1 +βt−1)2∥vt−w∗∥2+ 16E2Lα3
tG2+ 16E2L2α4
tG2+α3
tLG2
+ (1−αtµ)β2
t−1∥(vt−1−w∗)∥2+α2
tN/summationdisplay
k=1p2
kσ2
k+ 2βt−1(1 +βt−1)(1−αtµ)∥vt−w∗∥·∥vt−1−w∗∥
≤E(1−µαt)(1 +βt−1)2∥vt−w∗∥2+ 20E2Lα3
tG2+ (1−αtµ)β2
t−1∥(vt−1−w∗)∥2
+α2
t1
Nνmaxσ2+ 2βt−1(1 +βt−1)(1−αtµ)∥vt−w∗∥·∥vt−1−w∗∥
where we have used σ2=/summationtext
kpkσ2
k, and by construction our αtsatisfiesLαt≤1
5.
F.2 Convex Smooth Objectives
In this section we provide proof of the convergence result for Nesterov accelerated FedAvg with convex
and smooth objectives. Unlike with the FedAvg algorithm, where convex and strongly convex results share
identical components, the proof for the convergence result in the convex setting for Nesterov FedAvg uses
a change of variables, although the general ideas are in the same vein: we have a one step progress bound
forE∥wt+1−w∗∥2+ηt(F(wt)−F(w∗)), which is then used to form a telescoping sum that gives an upper
bound on mint≤TF(wt)−F(w∗).
Lemma 9 (One step progress, convex case, Nesterov ).Letwt=/summationtextN
k=1pkwk
tin Nesterov accelerated
FedAvg, and define ηt=αt
1−βt. Under assumptions 1,3,4, the following bound holds for all t:
E∥wt+1−w∗∥2+ηt(F(wt)−F(w∗))≤E∥wt−w∗∥2+ 32LE2α2
tηtG2+η2
tνmax1
Nσ2+ 2ηtβ2
t
1−βtG2
Theorem 4. Set learning rates αt=βt=O(/radicalig
N
T). Then under Assumptions 1,3,4 Nesterov accelerated
FedAvg with full device participation has rate
min
t≤TF(wt)−F∗=O/parenleftbiggνmaxσ2
√
NT+NE2LG2
T/parenrightbigg
,
and with partial device participation with Ksampled devices at each communication round and learning rates
αt=βt=O(/radicalig
K
T),
min
t≤TF(wt)−F∗=O/parenleftbiggνmaxσ2
√
KT+EG2
√
KT+KE2LG2
T/parenrightbigg
.
31Under review as submission to TMLR
Proof.Applying the bound from Lemma 9, with ηt=αt
1−βtwe have
E∥wt+1−w∗∥2+ηt(F(wt)−F(w∗))≤E∥wt−w∗∥2+ 32LE2α2
tηtG2+η2
tνmax1
Nσ2+ 2ηtβ2
t
1−βtG2
Summing the inequalities from t= 0tot=T, we obtain
T/summationdisplay
t=0ηt(F(wt)−F(w∗))≤∥w0−w∗∥2+T/summationdisplay
t=0η2
t·1
Nνmaxσ2+T/summationdisplay
t=0ηtα2
t·32LE2G2+T/summationdisplay
t=02ηtβ2
t
1−βtG2
so that
min
t≤TF(wt)−F(w∗)≤1/summationtextT
t=0ηt/parenleftigg
∥w0−w∗∥2+T/summationdisplay
t=0η2
t·1
Nνmaxσ2+T/summationdisplay
t=0ηtα2
t·32LE2G2+T/summationdisplay
t=02ηtβ2
t
1−βtG2/parenrightigg
By setting the constant learning rates αt≡/radicalig
N
Tandβt≡c/radicalig
N
Tso thatηt=αt
1−βt=√
N
T
1−c√
N
T≤2/radicalig
N
T, we
have
min
t≤TF(wt)−F(w∗)
≤1
2√
NT·∥w0−w∗∥2+2√
NTT·N
T·1
Nνmaxσ2+1√
NTT(/radicalbigg
N
T)332LE2G2+2√
NTT(/radicalbigg
N
T)3G2
= (1
2∥w0−w∗∥2+ 2νmaxσ2)1√
NT+N
T(32LE2G2+ 2G2)
=O(νmaxσ2
√
NT+NE2LG2
T)
Similarly, for partial participation, using the same argument to get the E-step bound in the proof of Theorem
2, we have
min
t≤TF(wt)−F(w∗) =O(νmaxσ2
√
KT+EG2
√
KT+KE2LG2
T)
F.2.1 Deferred proofs of key lemmas
Proof of lemma 9. Define pt:=βt
1−βt[wt−wt−1+αtgt−1]=β2
t
1−βt(vt−vt−1)fort≥1and 0 fort= 0. We
can check that
wt+1+pt+1=wt+pt−αt
1−βtgt
Now we define zt:=wt+ptandηt=αt
1−βtfor allt, so that we have the recursive relation
zt+1=zt−ηtgt
Now
∥zt+1−w∗∥2=∥(zt−ηtgt)−w∗∥2
=∥(zt−ηtgt−w∗)−ηt(gt−gt)∥2
=A1+A2+A3
where
A1=∥zt−w∗−ηtgt∥2
32Under review as submission to TMLR
A2= 2ηt⟨zt−w∗−ηtgt,gt−gt⟩
A3=η2
t∥gt−gt∥2
where again EA2= 0andEA3≤η2
t/summationtext
kp2
kσ2
k. ForA1we have
∥zt−w∗−ηtgt∥2=∥zt−w∗∥2+ 2⟨zt−w∗,−ηtgt⟩+∥ηtgt∥2
Using the convexity and L-smoothness of Fk,
−2ηt⟨zt−w∗,gt⟩
=−2ηtN/summationdisplay
k=1pk⟨zt−w∗,∇Fk(wk
t)⟩
=−2ηtN/summationdisplay
k=1pk⟨zt−wk
t,∇Fk(wk
t)⟩−2ηtN/summationdisplay
k=1pk⟨wk
t−w∗,∇Fk(wk
t)⟩
=−2ηtN/summationdisplay
k=1pk⟨zt−wt,∇Fk(wk
t)⟩−2ηtN/summationdisplay
k=1pk⟨wt−wk
t,∇Fk(wk
t)⟩−2ηtN/summationdisplay
k=1pk⟨wk
t−w∗,∇Fk(wk
t)⟩
≤−2ηtN/summationdisplay
k=1pk⟨zt−wt,∇Fk(wk
t)⟩−2ηtN/summationdisplay
k=1pk⟨wt−wk
t,∇Fk(wk
t)⟩+ 2ηtN/summationdisplay
k=1pk(Fk(w∗)−Fk(wk
t))
≤2ηtN/summationdisplay
k=1pk/bracketleftbigg
Fk(wk
t)−Fk(wt) +L
2∥wt−wk
t∥2+Fk(w∗)−Fk(wk
t)/bracketrightbigg
−2ηtN/summationdisplay
k=1pk⟨zt−wt,∇Fk(wk
t)⟩
=ηtLN/summationdisplay
k=1pk∥wt−wk
t∥2+ 2ηtN/summationdisplay
k=1pk[Fk(w∗)−Fk(wt)]−2ηtN/summationdisplay
k=1pk⟨zt−wt,∇Fk(wk
t)⟩
which results in
E∥wt+1−w∗∥2≤E∥wt−w∗∥2+ηtLN/summationdisplay
k=1pk∥wt−wk
t∥2+ 2ηtN/summationdisplay
k=1pk[Fk(w∗)−Fk(wt)]
+η2
t∥gt∥2+η2
tN/summationdisplay
k=1p2
kσ2
k−2ηtN/summationdisplay
k=1pk⟨zt−wt,∇Fk(wk
t)⟩
As before,∥gt∥2≤2L2/summationtext
kpk∥wk
t−wt∥2+ 4L(F(wt)−F(w∗)), so that
η2
t∥gt∥2+ηtN/summationdisplay
k=1pk[Fk(w∗)−Fk(wt)]≤2L2η2
t/summationdisplay
kpk∥wk
t−wt∥2+ηt(1−4ηtL)(F(w∗)−F(wt))
≤2L2η2
t/summationdisplay
kpk∥wk
t−wt∥2
forηt≤1/4L. Using/summationtextN
k=1pk∥wt−wk
t∥2≤16E2α2
tG2and/summationtextN
k=1p2
kσ2
k≤νmax1
Nσ2, it follows that
E∥wt+1−w∗∥2+ηt(F(wt)−F(w∗))≤E∥wt−w∗∥2+ (ηtL+ 2L2η2
t)N/summationdisplay
k=1pk∥wt−wk
t∥2+η2
tN/summationdisplay
k=1p2
kσ2
k
−2ηtN/summationdisplay
k=1pk⟨zt−wt,∇Fk(wk
t)⟩
33Under review as submission to TMLR
≤E∥wt−w∗∥2+ 32LE2α2
tηtG2+η2
tνmax1
Nσ2
−2ηtN/summationdisplay
k=1pk⟨zt−wt,∇Fk(wk
t)⟩
ifηt≤1
2L. It remains to bound E/summationtextN
k=1pk⟨zt−wt,∇Fk(wk
t)⟩. Recall that zt−wt=
βt
1−βt[wt−wt−1+αtgt−1] =β2
t
1−βt(vt−vt−1)andE∥vt−vt−1∥2≤G2,E∥∇Fk(wk
t)∥2≤G2.
Cauchy-Schwarz gives
EN/summationdisplay
k=1pk⟨zt−wt,∇Fk(wk
t)⟩≤N/summationdisplay
k=1pk/radicalbig
E∥zt−wt∥2·/radicalig
E∥∇Fk(wk
t)∥2
≤β2
t
1−βtG2
Thus
E∥wt+1−w∗∥2+ηt(F(wt)−F(w∗))≤E∥wt−w∗∥2+ 32LE2α2
tηtG2+η2
tνmax1
Nσ2+ 2ηtβ2
t
1−βtG2
G Proof of Geometric Convergence Results for Overparameterized Problems
G.1 Geometric Convergence of FedAvg for general strongly convex and smooth objectives
Theorem 5. For the overparameterized setting with general strongly convex and smooth objectives, FedAvg
with local SGD updates and communication every Eiterations with constant step size α=1
2EN
lνmax+L(N−νmin)
gives the exponential convergence guarantee
EF(wt)≤L
2(1−µα)t∥w0−w∗∥2=O(exp(−µ
2EN
lνmax+L(N−νmin)t)·∥w0−w∗∥2)
Proof.To illustrate the main ideas of the proof, we first present the proof for E= 2. Lett−1be a
communication round, so that wk
t−1=wt−1. We show that
∥wt+1−w∗∥2≤(1−αtµ)(1−αt−1µ)∥wt−1−w∗∥2
for appropriately chosen constant step sizes αt,αt−1. We have
∥wt+1−w∗∥2=∥(wt−αtgt)−w∗∥2
=∥wt−w∗∥2−2αt⟨wt−w∗,gt⟩+α2
t∥gt∥2
and the cross term can be bounded as usual using µ-convexity and L-smoothness of Fk:
−2αtEt⟨wt−w∗,gt⟩
=−2αtN/summationdisplay
k=1pk⟨wt−w∗,∇Fk(wk
t)⟩
=−2αtN/summationdisplay
k=1pk⟨wt−wk
t,∇Fk(wk
t)⟩−2αtN/summationdisplay
k=1pk⟨wk
t−w∗,∇Fk(wk
t)⟩
34Under review as submission to TMLR
≤−2αtN/summationdisplay
k=1pk⟨wt−wk
t,∇Fk(wk
t)⟩+ 2αtN/summationdisplay
k=1pk(Fk(w∗)−Fk(wk
t))−αtµN/summationdisplay
k=1pk∥wk
t−w∗∥2
≤2αtN/summationdisplay
k=1pk/bracketleftbigg
Fk(wk
t)−Fk(wt) +L
2∥wt−wk
t∥2+Fk(w∗)−Fk(wk
t)/bracketrightbigg
−αtµ∥N/summationdisplay
k=1pk(wk
t−w∗)∥2
=αtLN/summationdisplay
k=1pk∥wt−wk
t∥2+ 2αtN/summationdisplay
k=1pk[Fk(w∗)−Fk(wt)]−αtµ∥wt−w∗∥2
=αtLN/summationdisplay
k=1pk∥wt−wk
t∥2−2αtN/summationdisplay
k=1pkFk(wt)−αtµ∥wt−w∗∥2
and so
E∥wt+1−w∗∥2≤E(1−αtµ)∥wt−w∗∥2−2αtF(wt) +α2
t∥gt∥2+αtLN/summationdisplay
k=1pk∥wt−wk
t∥2
Applying this recursive relation to ∥wt−w∗∥2and using∥wt−1−wk
t−1∥2≡0, we further obtain
E∥wt+1−w∗∥2≤E(1−αtµ)/parenleftbig
(1−αt−1µ)∥wt−1−w∗∥2−2αt−1F(wt−1) +α2
t−1∥gt−1∥2/parenrightbig
−2αtF(wt) +α2
t∥gt∥2+αtLN/summationdisplay
k=1pk∥wt−wk
t∥2
Now instead of bounding/summationtextN
k=1pk∥wt−wk
t∥2using the arguments in the general convex case, we follow Ma
et al. (2018) and use the fact that in the overparameterized setting, w∗is a minimizer of each ℓ(w,xj
k)
and that each ℓisl-smooth to obtain ∥∇Fk(wt−1,ξk
t−1)∥2≤2l(Fk(wt−1,ξk
t−1)−Fk(w∗,ξk
t−1)), where recall
Fk(w,ξk
t−1) =ℓ(w,ξk
t−1), so that
N/summationdisplay
k=1pk∥wt−wk
t∥2=N/summationdisplay
k=1pk∥wt−1−αt−1gt−1−wk
t−1+αt−1gt−1,k∥2
=N/summationdisplay
k=1pkα2
t−1∥gt−1−gt−1,k∥2
=α2
t−1N/summationdisplay
k=1pk(∥gt−1,k∥2−∥gt−1∥2)
=α2
t−1N/summationdisplay
k=1pk∥∇Fk(wt−1,ξk
t−1)∥2−α2
t−1∥gt−1∥2
≤α2
t−1N/summationdisplay
k=1pk2l(Fk(wt−1,ξk
t−1)−Fk(w∗,ξk
t−1))−α2
t−1∥gt−1∥2
again using wt−1=wk
t−1. Taking expectation with respect to ξk
t−1’s and using the fact that F(w∗) = 0, we
have
Et−1N/summationdisplay
k=1pk∥wt−wk
t∥2≤2lα2
t−1N/summationdisplay
k=1pkFk(wt−1)−α2
t−1∥gt−1∥2
= 2lα2
t−1F(wt−1)−α2
t−1∥gt−1∥2
Note also that
∥gt−1∥2=∥N/summationdisplay
k=1pk∇Fk(wt−1,ξk
t−1)∥2
35Under review as submission to TMLR
while
∥gt∥2=∥N/summationdisplay
k=1pk∇Fk(wk
t,ξk
t)∥2≤2∥N/summationdisplay
k=1pk∇Fk(wt,ξk
t)∥2+ 2∥N/summationdisplay
k=1pk(∇Fk(wt,ξk
t)−∇Fk(wk
t,ξk
t))∥2
≤2∥N/summationdisplay
k=1pk∇Fk(wt,ξk
t)∥2+ 2N/summationdisplay
k=1pkl2∥wt−wk
t∥2
Substituting these into the bound for ∥wt+1−w∗∥2, we have
E∥wt+1−w∗∥2≤E(1−αtµ)((1−αt−1µ)∥wt−1−w∗∥2−2αt−1F(wt−1) +α2
t−1∥gt−1∥2)
−2αtF(wt) + 2α2
t∥N/summationdisplay
k=1pk∇Fk(wt,ξk
t)∥2+/parenleftbig
2l2α2
t−1α2
t+αtα2
t−1L/parenrightbig/parenleftbig
2lF(wt−1)−∥gt−1∥2/parenrightbig
=E(1−αtµ)(1−αt−1µ)∥wt−1−w∗∥2
−2αt(F(wt)−αt∥N/summationdisplay
k=1pk∇Fk(wt,ξk
t)∥2)
−2αt−1(1−αtµ)/parenleftigg
(1−lαt−1(2l2α2
t+αtL)
1−αtµ)F(wt−1)−αt−1
2∥N/summationdisplay
k=1pk∇Fk(wt−1,ξk
t−1)∥2/parenrightigg
from which we can conclude that
E∥wt+1−w∗∥2≤(1−αtµ)(1−αt−1µ)E∥wt−1−w∗∥2
if we can choose αt,αt−1to guarantee
E(F(wt)−αt∥N/summationdisplay
k=1pk∇Fk(wt,ξk
t)∥2)≥0
E/parenleftigg
(1−lαt−1(2l2α2
t+αtL)
1−αtµ)F(wt−1)−αt−1
2∥N/summationdisplay
k=1pk∇Fk(wt−1,ξk
t−1)∥2/parenrightigg
≥0
Note that
Et∥N/summationdisplay
k=1pk∇Fk(wt,ξk
t)∥2=Et⟨N/summationdisplay
k=1pk∇Fk(wt,ξk
t),N/summationdisplay
k=1pk∇Fk(wt,ξk
t)⟩
=N/summationdisplay
k=1p2
kEt∥∇Fk(wt,ξk
t)∥2+N/summationdisplay
k=1/summationdisplay
j̸=kpjpkEt⟨∇Fk(wt,ξk
t),∇Fj(wt,ξj
t)⟩
=N/summationdisplay
k=1p2
kEt∥∇Fk(wt,ξk
t)∥2+N/summationdisplay
k=1/summationdisplay
j̸=kpjpk⟨∇Fk(wt),∇Fj(wt)⟩
=N/summationdisplay
k=1p2
kEt∥∇Fk(wt,ξk
t)∥2+N/summationdisplay
k=1N/summationdisplay
j=1pjpk⟨∇Fk(wt),∇Fj(wt)⟩−N/summationdisplay
k=1p2
k∥∇Fk(wt)∥2
≤N/summationdisplay
k=1p2
kEt∥∇Fk(wt,ξk
t)∥2+∥/summationdisplay
kpk∇Fk(wt)∥2−1
Nνmin∥/summationdisplay
kpk∇Fk(wt)∥2
=N/summationdisplay
k=1p2
kEt∥∇Fk(wt,ξk
t)∥2+ (1−1
Nνmin)∥∇F(wt)∥2
36Under review as submission to TMLR
and so following Ma et al. (2018) if we let αt=min{qN
2lνmax,1−q
2L(1−1
Nνmin)}for aq∈[0,1]to be optimized later,
we have
Et(F(wt)−αt∥N/summationdisplay
k=1pk∇Fk(wt,ξk
t)∥2)
≥EtN/summationdisplay
k=1pkFk(wt)−αt/bracketleftiggN/summationdisplay
k=1p2
kEt∥∇Fk(wt,ξk
t)∥2+ (1−1
Nνmin)∥∇F(wt)∥2/bracketrightigg
≥EtN/summationdisplay
k=1pk(qFk(wt,ξk
t)−αt1
Nνmax∥∇Fk(wt,ξk
t)∥2) + ((1−q)F(wt)−αt(1−1
Nνmin)∥∇F(wt)∥2)
≥qEtN/summationdisplay
k=1pk(Fk(wt,ξk
t)−1
2l∥∇Fk(wt,ξk
t)∥2) + (1−q)(F(wt)−1
2L∥∇F(wt)∥2)
≥0
again using w∗optimizesFk(w,ξk
t)withFk(w∗,ξk
t) = 0.
Maximizing αt=min{qN
2lνmax,1−q
2L(1−1
Nνmin)}overq∈[0,1], we see that q=lνmax
lνmax+L(N−νmin)results in
the fastest convergence, and this translates to αt=1
2N
lνmax+L(N−νmin). Next we claim that αt−1=
c1
2N
lνmax+L(N−νmin)also guarantees
E(1−lαt−1(2l2α2
t+αtL)
1−αtµ)F(wt−1)−αt−1
2∥N/summationdisplay
k=1pk∇Fk(wt−1,ξk
t−1)∥2≥0
Note that by scaling αt−1by a constant c≤1if necessary, we can guaranteelαt−1(2l2α2
t+αtL)
1−αtµ≤1
2, and so the
condition is equivalent to
F(wt−1)−αt−1∥N/summationdisplay
k=1pk∇Fk(wt−1,ξk
t−1)∥2≥0
which was shown to hold with αt−1≤1
2N
lνmax+L(N−νmin).
For the proof of general E≥2, we use the following two identities:
∥gt∥2≤2∥N/summationdisplay
k=1pk∇Fk(wt,ξk
t)∥2+ 2N/summationdisplay
k=1pkl2∥wt−wk
t∥2
EN/summationdisplay
k=1pk∥wt−wk
t∥2≤E2(1 + 2l2α2
t−1)N/summationdisplay
k=1pk∥wt−1−wk
t−1∥2+ 8α2
t−1lF(wt−1)−2α2
t−1∥gt−1∥2
where the first inequality has been established before. To establish the second inequality, note that
N/summationdisplay
k=1pk∥wt−wk
t∥2=N/summationdisplay
k=1pk∥wt−1−αt−1gt−1−wk
t−1+αt−1gt−1,k∥2
≤2N/summationdisplay
k=1pk/parenleftbig
∥wt−1−wk
t−1∥2+∥αt−1gt−1−αt−1gt−1,k∥2/parenrightbig
and
/summationdisplay
kpk∥gt−1,k−gt−1∥2=/summationdisplay
kpk(∥gt−1,k∥2−∥gt−1∥2)
37Under review as submission to TMLR
=/summationdisplay
kpk∥∇Fk(wt−1,ξk
t−1) +∇Fk(wk
t−1,ξk
t−1)−∇Fk(wt−1,ξk
t−1)∥2−∥gt−1∥2
≤2/summationdisplay
kpk/parenleftbig
∥∇Fk(wt−1,ξk
t−1)∥2+l2∥wk
t−1−wt−1∥2/parenrightbig
−∥gt−1∥2
so that using the l-smoothness of ℓ,
EN/summationdisplay
k=1pk∥wt−wk
t∥2
≤E2(1 + 2l2α2
t−1)N/summationdisplay
k=1pk∥wt−1−wk
t−1∥2+ 4α2
t−1/summationdisplay
kpk∥∇Fk(wt−1,ξk
t−1)∥2−2α2
t−1∥gt−1∥2
≤E2(1 + 2l2α2
t−1)N/summationdisplay
k=1pk∥wt−1−wk
t−1∥2+ 4α2
t−12l/summationdisplay
kpk(Fk(wt−1,ξk
t−1)−Fk(w∗,ξk
t−1))−2α2
t−1∥gt−1∥2
=E2(1 + 2l2α2
t−1)N/summationdisplay
k=1pk∥wt−1−wk
t−1∥2+ 8α2
t−1lF(wt−1)−2α2
t−1∥gt−1∥2
Using the first inequality, we have
E∥wt+1−w∗∥2≤E(1−αtµ)∥wt−w∗∥2
−2αtF(wt) + 2α2
t∥N/summationdisplay
k=1pk∇Fk(wt,ξk
t)∥2
+ (2α2
tl2+αtL)N/summationdisplay
k=1pk∥wt−wk
t∥2
and we choose αtandαt−1such that E(F(wt)−αt∥/summationtextN
k=1pk∇Fk(wt,ξk
t)∥2)≥0and(2α2
tl2+αtL)≤
(1−αtµ)(2α2
t−1l2+αt−1L)/3. This gives
E∥wt+1−w∗∥2≤E(1−αtµ)[(1−αt−1µ)∥wt−1−w∗∥2−2αt−1F(wt−1) + 2α2
t−1∥N/summationdisplay
k=1pk∇Fk(wt−1,ξk
t−1)∥2
+ (2α2
t−1l2+αt−1L)(N/summationdisplay
k=1pk∥wt−1−wk
t−1∥2+N/summationdisplay
k=1pk∥wt−wk
t∥2)/3]
Using the second inequality
N/summationdisplay
k=1pk∥wt−wk
t∥2≤E2(1 + 2l2α2
t−1)N/summationdisplay
k=1pk∥wt−1−wk
t−1∥2+ 8α2
t−1lF(wt−1)−2α2
t−1∥gt−1∥2
and that 2(1 + 2l2α2
t−1)≤3,2α2
t−1l2+αt−1L≤1, we have
E∥wt+1−w∗∥2≤E(1−αtµ)[(1−αt−1µ)∥wt−1−w∗∥2
−2αt−1F(wt−1) + 2α2
t−1∥N/summationdisplay
k=1pk∇Fk(wt−1,ξk
t−1)∥2+ 8α2
t−1lF(wt−1)
+ (2α2
t−1l2+αt−1L)(2N/summationdisplay
k=1pk∥wt−1−wk
t−1∥2)]
38Under review as submission to TMLR
and ifαt−1is chosen such that
(F(wt−1)−4αt−1lF(wt−1))−αt−1∥N/summationdisplay
k=1pk∇Fk(wt−1,ξk
t−1)∥2≥0
and
(2α2
t−1l2+αt−1L)(1−αt−1µ)≤(2α2
t−2l2+αt−2L)/3
we again have
E∥wt+1−w∗∥2≤E(1−αtµ)(1−αt−1µ)[∥wt−1−w∗∥2+ (2α2
t−2l2+αt−2L)·(2N/summationdisplay
k=1pk∥wt−1−wk
t−1∥2)/3]
Applying the above derivation iteratively τ <Etimes, we have
E∥wt+1−w∗∥2≤E(1−αtµ)···(1−αt−τ+1µ)[(1−αt−τµ)∥wt−τ−w∗∥2
−2αt−τF(wt−τ) + 2α2
t−τ∥N/summationdisplay
k=1pk∇Fk(wt−τ,ξk
t−τ)∥2+ 8τα2
t−τlF(wt−τ)
+ (2α2
t−τl2+αt−τL)((τ+ 1)N/summationdisplay
k=1pk∥wt−τ−wk
t−τ∥2)]
as long as the step sizes αt−τare chosen such that the following inequalities hold
(2α2
t−τl2+αt−τL)(1−αt−τµ)≤(2α2
t−τ−1l2+αt−τ−1L)/3
2(1 + 2l2α2
t−τ)≤3
2α2
t−τl2+αt−τL≤1
(F(wt−τ)−4ταt−τlF(wt−τ))−αt−τ∥N/summationdisplay
k=1pk∇Fk(wt−τ,ξk
t−τ)∥2≥0
We can check that setting αt−τ=c1
τ+1N
lνmax+L(N−νmin)for some small constant csatisfies the requirements.
Since communication is done every Eiterations, wt0=wk
t0for somet0>t−E, from which we can conclude
that
E∥wt−w∗∥2≤(t−t0−1/productdisplay
τ=1(1−µαt−τ))∥wt0−w∗∥2
≤(1−cµ
EN
lνmax+L(N−νmin))t−t0∥wt0−w∗∥2
and applying this inequality to iterations between each communication round,
E∥wt−w∗∥2≤(1−cµ
EN
lνmax+L(N−νmin))t∥w0−w∗∥2
=O(exp(µ
EN
lνmax+L(N−νmin)t))∥w0−w∗∥2
With partial participation, we note that
E∥wt+1−w∗∥2=E∥wt+1−vt+1+vt+1−w∗∥2
39Under review as submission to TMLR
=E∥wt+1−vt+1∥2+E∥vt+1−w∗∥2
=1
K/summationdisplay
kpkE∥wk
t+1−wt+1∥2+E∥vt+1−w∗∥2
and so the recursive identity becomes
E∥wt+1−w∗∥2≤E(1−αtµ)···(1−αt−τ+1µ)[(1−αt−τµ)∥wt−τ−w∗∥2
−2αt−τF(wt−τ) + 2α2
t−τ∥N/summationdisplay
k=1pk∇Fk(wt−τ,ξk
t−τ)∥2+ 8τα2
t−τlF(wt−τ)
+ (2α2
t−τl2+αt−τL+1
K)((τ+ 1)N/summationdisplay
k=1pk∥wt−τ−wk
t−τ∥2)]
which requires
(2α2
t−τl2+αt−τL+1
K)(1−αt−τµ)≤(2α2
t−τ−1l2+αt−τ−1L+1
K)/3
2(1 + 2l2α2
t−τ)≤3
2α2
t−τl2+αt−τL+1
K≤1
(F(wt−τ)−4ταt−τlF(wt−τ))−αt−τ∥N/summationdisplay
k=1pk∇Fk(wt−τ,ξk
t−τ)∥2≥0
to hold. Again setting αt−τ=c1
τ+1N
lνmax+L(N−νmin)for a possibly different constant from before satisfies the
requirements.
Finally, using the L-smoothness of F,
F(wT)−F(w∗)≤L
2E∥wT−w∗∥2=O(Lexp(−µ
EN
lνmax+L(N−νmin)T))∥w0−w∗∥2
G.2 Geometric Convergence of FedAvg for Overparameterized Linear Regression
We first provide details on quantities used in the proof of results on linear regression in Section 5. Recall
that the local device objectives are now given by the sum of squares Fk(w) =1
2nk/summationtextnk
j=1(wTxj
k−zj
k)2, and
there exists w∗such thatF(w∗)≡0. Define the local Hessian matrix as Hk:=1
nk/summationtextnk
j=1xj
k(xj
k)T, and
the stochastic Hessian matrix as ˜Hk
t:=ξk
t(ξk
t)T, whereξk
tis the stochastic sample on the kth device at
timet. Definelto be the smallest positive number such that E∥ξk
t∥2ξk
t(ξk
t)T⪯lHkfor allk. Note that
l≤maxk,j∥xj
k∥2. LetLandµbe lower and upper bounds of non-zero eigenvalues of Hk. Defineκ1:=l/µ
andκ:=L/µ. The condition number κ1is important in the characterization of convergence rates for FedAvg
algorithms. Note that κ1>κ.
LetH=/summationtext
kpkHk. In general Hhas zero eigenvalues. However, because the null space of Hand range of H
are orthogonal, in our subsequence analysis it suffices to project wt−w∗onto the range of H, thus we may
restrict to the non-zero eigenvalue of H.
A useful observation is that we can use w∗Txj
k−zj
k≡0to rewrite the local objectives as Fk(w) =
1
2⟨w−w∗,Hk(w−w∗)⟩≡1
2∥w−w∗∥2
Hk:
Fk(w) =1
2nknk/summationdisplay
j=1(wTxk,j−zk,j−(w∗Txk,j−zk,j))2=1
2nknk/summationdisplay
j=1((w−w∗)Txk,j)2
=1
2⟨w−w∗,Hk(w−w∗)⟩=1
2∥w−w∗∥2
Hk
40Under review as submission to TMLR
so thatF(w) =1
2∥w−w∗∥2
H.
Finally, note that E˜Hk
t=1
nk/summationtextnk
j=1xj
k(xj
k)T=Hkandgt,k=∇Fk(wk
t,ξk
t) = ˜Hk
t(wk
t−w∗)while
gt=/summationtextN
k=1pk∇Fk(wk
t,ξk
t) =/summationtextN
k=1pk˜Hk
t(wk
t−w∗)andgt=/summationtextN
k=1pkHk(wk
t−w∗)
Theorem 6. For the overparamterized linear regression problem, FedAvg with communication every E
iterations with constant step size α=O(1
EN
lνmax+µ(N−νmin))has geometric convergence:
EF(wT)≤O/parenleftbigg
Lexp(−NT
E(νmaxκ1+ (N−νmin)))∥w0−w∗∥2/parenrightbigg
.
Proof.We again show the result first when E= 2andt−1is a communication round. We have
∥wt+1−w∗∥2=∥(wt−αtgt)−w∗∥2
=∥wt−w∗∥2−2αt⟨wt−w∗,gt⟩+α2
t∥gt∥2
and
−2αtEt⟨wt−w∗,gt⟩
=−2αtN/summationdisplay
k=1pk⟨wt−w∗,∇Fk(wk
t)⟩
=−2αtN/summationdisplay
k=1pk⟨wt−wk
t,∇Fk(wk
t)⟩−2αtN/summationdisplay
k=1pk⟨wk
t−w∗,∇Fk(wk
t)⟩
=−2αtN/summationdisplay
k=1pk⟨wt−wk
t,∇Fk(wk
t)⟩−2αtN/summationdisplay
k=1pk⟨wk
t−w∗,Hk(wk
t−w∗)⟩
=−2αtN/summationdisplay
k=1pk⟨wt−wk
t,∇Fk(wk
t)⟩−4αtN/summationdisplay
k=1pkFk(wk
t)
≤2αtN/summationdisplay
k=1pk(Fk(wk
t)−Fk(wt) +L
2∥wt−wk
t∥2)−4αtN/summationdisplay
k=1pkFk(wk
t)
=αtLN/summationdisplay
k=1pk∥wt−wk
t∥2−2αtN/summationdisplay
k=1pkFk(wt)−2αtN/summationdisplay
k=1pkFk(wk
t)
=αtLN/summationdisplay
k=1pk∥wt−wk
t∥2−αtN/summationdisplay
k=1pk⟨(wt−w∗),Hk(wt−w∗)⟩−2αtN/summationdisplay
k=1pkFk(wk
t)
and
∥gt∥2=∥N/summationdisplay
k=1pk˜Hk
t(wk
t−w∗)∥2
=∥N/summationdisplay
k=1pk˜Hk
t(wt−w∗) +N/summationdisplay
k=1pk˜Hk
t(wk
t−wt)∥2
≤2∥N/summationdisplay
k=1pk˜Hk
t(wt−w∗)∥2+ 2∥N/summationdisplay
k=1pk˜Hk
t(wk
t−wt)∥2
which gives
E∥wt+1−w∗∥2≤E∥wt−w∗∥2−αtN/summationdisplay
k=1pk⟨wt−w∗,Hkwt−w∗⟩+ 2α2
t∥N/summationdisplay
k=1pk˜Hk
t(wt−w∗)∥2
41Under review as submission to TMLR
+αtLN/summationdisplay
k=1pk∥wt−wk
t∥2+ 2α2
t∥N/summationdisplay
k=1pk˜Hk
t(wk
t−wt)∥2−2αtN/summationdisplay
k=1pkFk(wk
t)
following Ma et al. (2018) we first prove that
E∥wt−w∗∥2−αtN/summationdisplay
k=1pk⟨(wt−w∗),Hk(wt−w∗)⟩+ 2α2
t∥N/summationdisplay
k=1pk˜Hk
t(wt−w∗)∥2
≤(1−N
8(νmaxκ1+ (N−νmin)))E∥wt−w∗∥2
with appropriately chosen αt. Compared to the rate O(µN
lνmax+L(N−νmin)) =O(N
νmaxκ1+(N−νmin)κ)for general
strongly convex and smooth objectives, this is an improvement as linear speedup is now available for a larger
range ofN.
We have
Et∥N/summationdisplay
k=1pk˜Hk
t(wt−w∗)∥2
=Et⟨N/summationdisplay
k=1pk˜Hk
t(wt−w∗),N/summationdisplay
k=1pk˜Hk
t(wt−w∗)⟩
=N/summationdisplay
k=1p2
kEt∥˜Hk
t(wt−w∗)∥2+N/summationdisplay
k=1/summationdisplay
j̸=kpjpkEt⟨˜Hk
t(wt−w∗),˜Hj
t(wt−w∗)⟩
=N/summationdisplay
k=1p2
kEt∥˜Hk
t(wt−w∗)∥2+N/summationdisplay
k=1/summationdisplay
j̸=kpjpkEt⟨Hk(wt−w∗),Hj(wt−w∗)⟩
=N/summationdisplay
k=1p2
kEt∥˜Hk
t(wt−w∗)∥2+N/summationdisplay
k=1N/summationdisplay
j=1pjpkEt⟨Hk(wt−w∗),Hj(wt−w∗)⟩−N/summationdisplay
k=1p2
k∥Hk(wt−w∗)∥2
=N/summationdisplay
k=1p2
kEt∥˜Hk
t(wt−w∗)∥2+∥/summationdisplay
kpkHk(wt−w∗)∥2−N/summationdisplay
k=1p2
k∥Hk(wt−w∗)∥2
≤N/summationdisplay
k=1p2
kEt∥˜Hk
t(wt−w∗)∥2+∥/summationdisplay
kpkHk(wt−w∗)∥2−1
Nνmin∥/summationdisplay
kpkHk(wt−w∗)∥2
≤1
NνmaxN/summationdisplay
k=1pkEt∥˜Hk
t(wt−w∗)∥2+ (1−1
Nνmin)∥/summationdisplay
kpkHk(wt−w∗)∥2
≤1
NνmaxlN/summationdisplay
k=1pk⟨(wt−w∗),Hk(wt−w∗)⟩+ (1−1
Nνmin)∥/summationdisplay
kpkHk(wt−w∗)∥2
=1
Nνmaxl⟨(wt−w∗),H(wt−w∗)⟩+ (1−1
Nνmin)⟨wt−w∗,H2(wt−w∗)⟩
using∥˜Hk
t∥≤l.
Now we have
E∥wt−w∗∥2−αtN/summationdisplay
k=1pk⟨(wt−w∗),Hk(wt−w∗)⟩+ 2α2
t∥N/summationdisplay
k=1pk˜Hk
t(wt−w∗)∥2=
⟨wt−w∗,(I−αtH+ 2α2
t(νmaxl
NH+N−νmin
NH2))(wt−w∗)⟩
42Under review as submission to TMLR
and it remains to bound the maximum eigenvalue of
(I−αtH+ 2α2
t(νmaxl
NH+N−νmin
NH2))
and we bound this following Ma et al. (2018). If we choose αt<N
2(νmaxl+(N−νmin)L), then
−αtH+ 2α2
t(νmaxl
NH+N−νmin
NH2)≺0
and the convergence rate is given by the maximum of 1−αtλ+ 2α2
t(νmaxl
Nλ+N−νmin
Nλ2)maximized over
the non-zero eigenvalues λofH. To select the step size αtthat gives the smallest upper bound, we then
minimize over αt, resulting in
min
αt<N
2(νmaxl+(N−νmin)L)max
λ>0:∃v,Hv=λv/braceleftbigg
1−αtλ+ 2α2
t(νmaxl
Nλ+N−νmin
Nλ2)/bracerightbigg
Since the objective is quadratic in λ, the maximum is achieved at either the largest eigenvalue λmaxofHor
the smallest non-zero eigenvalue λminofH.
WhenN≤4νmaxl
L−λmin+ 4νmin, i.e. when N=O(l/λmin) =O(κ1), the optimal objective value is achieved at
λminand the optimal step size is given by αt=N
4(νmaxl+(N−νmin)λmin). The optimal convergence rate (i.e. the
optimal objective value) is equal to 1−1
8Nλmin
(νmaxl+(N−νmin)λmin)= 1−1
8N
(νmaxκ1+(N−νmin)). This implies that
whenN=O(κ1), the optimal convergence rate has a linear speedup in N. WhenNis larger, this step size is
no longer optimal, but we still have 1−1
8N
(νmaxκ1+(N−νmin))as an upper bound on the convergence rate.
Now we have proved
E∥wt+1−w∗∥2≤(1−1
8N
(νmaxκ1+ (N−νmin)))E∥wt−w∗∥2
+αtLN/summationdisplay
k=1pk∥wt−wk
t∥2+ 2α2
t∥N/summationdisplay
k=1pk˜Hk
t(wk
t−wt)∥2−2αtN/summationdisplay
k=1pkFk(wk
t)
Next we bound terms in the second line using a similar argument as the general case. We have
2α2
t∥N/summationdisplay
k=1pk˜Hk
t(wk
t−wt)∥2≤2α2
tl2N/summationdisplay
k=1pk∥wt−wk
t∥2
and
EN/summationdisplay
k=1pk∥wt−wk
t∥2≤E2(1 + 2l2α2
t−1)N/summationdisplay
k=1pk∥wt−1−wk
t−1∥2+ 8α2
t−1lF(wt−1)
= 4α2
t−1l⟨wt−1−w∗,H(wt−1−w∗)⟩
and ifαt,αt−1satisfy
αtL+ 2α2
t≤(1−1
8N
(νmaxκ1+ (N−νmin)))(αt−1L+ 2α2
t−1)/3
2(1 + 2l2α2
t−1)≤3
αtL+ 2α2
t≤1
we have
E∥wt+1−w∗∥2
43Under review as submission to TMLR
≤(1−1
8N
(νmaxκ1+ (N−νmin)))[E∥wt−1−w∗∥2−αt⟨wt−1−w∗,Hwt−1−w∗⟩+ 2α2
t∥N/summationdisplay
k=1pk˜Hk
t(wt−w∗)∥2
+ (αt−1L+ 2α2
t−1)·2N/summationdisplay
k=1pk∥wt−1−wk
t−1∥2+ 4α2
t−1l⟨wt−1−w∗,H(wt−1−w∗)⟩]
and again by choosing αt−1=cN
8(νmaxl+(N−νmin)λmin)for a small constant c, we can guarantee that
E∥wt−1−w∗∥2−αt−1⟨wt−1−w∗,Hwt−1−w∗⟩
+2α2
t−1∥N/summationdisplay
k=1pk˜Hk
t−1(wt−1−w∗)∥2+ 4α2
t−1l⟨wt−1−w∗,H(wt−1−w∗)⟩
≤(1−cN
16(νmaxl+ (N−νmin)λmin))E∥wt−1−w∗∥2
For general E, we have the recursive relation
E∥wt+1−w∗∥2≤E(1−c1
8N
(νmaxκ1+ (N−νmin)))···(1−c1
8τN
(νmaxκ1+ (N−νmin)))[∥wt−τ−w∗∥2
−αt−τ⟨wt−τ−w∗,Hwt−τ−w∗⟩+ 2α2
t−τ∥N/summationdisplay
k=1pk˜Hk
t−τ(wt−τ−w∗)∥2
+ 4τα2
t−1l⟨wt−1−w∗,H(wt−1−w∗)⟩
+ (2α2
t−τl2+αt−τL)((τ+ 1)N/summationdisplay
k=1pk∥wt−τ−wk
t−τ∥2)]
as long as the step sizes are chosen αt−τ=cN
4τ(νmaxl+(N−νmin)λmin)such that the following inequalities hold
(2α2
t−τl2+αt−τL)≤(1−αt−τµ)(2α2
t−τ−1l2+αt−τ−1L)/3
2(1 + 2l2α2
t−τ)≤3
2α2
t−τl2+αt−τL≤1
and
∥wt−τ−w∗∥2−αt−τ⟨wt−τ−w∗,Hwt−τ−w∗⟩
+ 2α2
t−τ∥N/summationdisplay
k=1pk˜Hk
t−τ(wt−τ−w∗)∥2+ 4τα2
t−1l⟨wt−1−w∗,H(wt−1−w∗)⟩
≤(1−cN
8(τ+ 1)(νmaxκ1+ (N−νmin)))E∥wt−τ−w∗∥2
which gives
E∥wt−w∗∥2≤(1−c1
8EN
(νmaxκ1+ (N−νmin)))t∥w0−w∗∥2
=O(exp(−1
EN
(νmaxκ1+ (N−νmin))t))∥w0−w∗∥2.
H Details on Experiments and Additional Results
We describe the precise procedure to reproduce the results in this paper. As we mentioned in Section 6,
we empirically verified the linear speed up on various convex settings for both FedAvg and its accelerated
44Under review as submission to TMLR
variants. For all the results, we set random seeds as 0,1,2and report the best convergence rate across the
three folds. For each run, we initialize w0=0and measure the number of iteration to reach the target
accuracyϵ. We use the small-scale dataset w8a Platt (1999), which consists of n= 49749 samples with
feature dimension d= 300. The label is either positive one or negative one. The dataset has sparse binary
features in{0,1}. Each sample has 11.15 non-zero feature values out of 300features on average. We set the
batch size equal to four across all experiments. In the next following subsections, we introduce parameter
searching in each objective separately.
H.1 Strongly Convex Objectives
We first consider the strongly convex objective function, where we use a regularized binary logistic regression
with regularization λ= 1/n≈2e−5. We evenly distributed on 1,2,4,8,16,32devices and report the number
of iterations/rounds needed to converge to ϵ−accuracy, where ϵ= 0.005. The optimal objective function value
f∗is set asf∗= 0.126433176216545 . This is determined numerically and we follow the setting in Stich (2019).
The learning rate is decayed as the ηt=min(η0,nc
1+t), where we extensively search the best learning rate
c∈{2−1c0,2−2c0,c0,2c0,22c0}. In this case, we search the initial learning rate η0∈{1,32}andc0= 1/8.
H.2 Convex Smooth Objectives
We also use binary logistic regression without regularization. The setting is almost same as its regularized
counter part. We also evenly distributed all the samples on 1,2,4,8,16,32devices. The figure shows the
number of iterations needed to converge to ϵ−accuracy, where ϵ= 0.02. The optiaml objective function
value is set as f∗= 0.11379089057514849 , determined numerically. The learning rate is decayed as the
ηt=min(η0,nc
1+t), where we extensively search the best learning rate c∈{2−1c0,2−2c0,c0,2c0,22c0}. In this
case, we search the initial learning rate η0∈{1,32}andc0= 1/8.
H.3 Linear regression
For linear regression, we use the same feature vectors from w8a dataset and generate ground truth [w∗,b∗]
from a multivariate normal distribution with zero mean and standard deviation one. Then we generate label
based onyi=xt
iw∗+b∗. This procedure will ensure we satisfy the over-parameterized setting as required
in our theorems. We also evenly distributed all the samples on 1,2,4,8,16,32devices. The figure shows
the number of iterations needed to converge to ϵ−accuracy, where ϵ= 0.02. The optiaml objective function
value isf∗= 0. The learning rate is decayed as the ηt=min(η0,nc
1+t), where we extensively search the best
learning rate c∈{2−1c0,2−2c0,c0,2c0,22c0}. In this case, we search the initial learning rate η0∈{0.1,0.12}
andc0= 1/256.
H.4 Partial Participation
To examine the linear speedup of FedAvg in partial participation setting, we evenly distributed data on
4,8,16,32,64,128devices and uniformly sample 50%devices without replacement. All other hyperparameters
are the same as previous sections.
H.5 Nesterov accelerated FedAvg
The experiments of Nesterov accelerated FedAvg (the update formula is given as follows) uses the same
setting as previous three sections for vanilia FedAvg.
yk
t+1=wk
t−αtgt,k
wk
t+1=/braceleftigg
yk
t+1+βt(yk
t+1−yk
t) ift+ 1/∈IE/summationtext
k∈St+1/parenleftbig
yk
t+1+βt(yk
t+1−yk
t)/parenrightbig
ift+ 1∈IE
We setβt= 0.1and search αtin the same way as ηtin FedAvg.
45Under review as submission to TMLR
100101
Local steps (E)103Number of iterations (T)K=4
K=8
K=16
K=32
100101
Local steps (E)103Number of iterations (T)K=4
K=8
K=16
K=32
100101
Local steps (E)Number of iterations (T)K=4
K=8
K=16
K=32
100101
Local steps (E)103Number of rounds (T/E)K=4
K=8
K=16
K=32
100101
Local steps (E)102103Number of rounds (T/E)K=4
K=8
K=16
K=32
100101
Local steps (E)102103Number of rounds (T/E)K=4
K=8
K=16
K=32
(a) Strongly convex objective (b) Convex smooth objective (c) Linear regression
Figure 2: The convergence of FedAvg w.r.t the number of local steps E.
H.6 The impact of E.
In this subsection, we further examine how does the number of local steps ( E) affect convergence. As shown
in Figure 2, the number of iterations increases as Eincrease, which slow down the convergence in terms of
gradient computation. However, it can save communication costs as the number of rounds decreased when
theEincreases. This showcases that we need a proper choice of Eto trade-off the communication cost and
convergence speed.
46