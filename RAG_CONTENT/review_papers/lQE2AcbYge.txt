Published in Transactions on Machine Learning Research (04/2024)
Online Continuous Hyperparameter Optimization for Gener-
alized Linear Contextual Bandits
Yue Kang yuekang@ucdavis.edu
University of California, Davis
Cho-Jui Hsieh chohsieh@cs.ucla.edu
Google and University of California, Los Angeles
Thomas C. M. Lee tcmlee@ucdavis.edu
University of California, Davis
Reviewed on OpenReview: https: // openreview. net/ forum? id= lQE2AcbYge
Abstract
In stochastic contextual bandits, an agent sequentially makes actions from a time-dependent
action set based on past experience to minimize the cumulative regret. Like many other
machine learning algorithms, the performance of bandits heavily depends on the values of
hyperparameters, and theoretically derived parameter values may lead to unsatisfactory
results in practice. Moreover, it is infeasible to use offline tuning methods like cross-validation
to choose hyperparameters under the bandit environment, as the decisions should be made in
real-time. To address this challenge, we propose the first online continuous hyperparameter
tuning framework for contextual bandits to learn the optimal parameter configuration in
practice within a search space on the fly. Specifically, we use a double-layer bandit framework
named CDT (Continuous Dynamic Tuning) and formulate the hyperparameter optimization
as a non-stationary continuum-armed bandit, where each arm represents a combination
of hyperparameters, and the corresponding reward is the algorithmic result. For the top
layer, we propose the Zooming TS algorithm that utilizes Thompson Sampling (TS) for
exploration and a restart technique to get around the switching environment. The proposed
CDT framework can be easily utilized to tune contextual bandit algorithms without any
pre-specified candidate set for multiple hyperparameters. We further show that it could
achieve a sublinear regret in theory and performs consistently better than all existing methods
on both synthetic and real datasets.
1 Introduction
The contextual bandit is a powerful framework for modeling sequential learning problems under uncertainty,
with substantial applications in recommendation systems (Li et al., 2010), clinical trials (Woodroofe, 1979),
personalized medicine (Bastani & Bayati, 2020), etc. At each round t, the agent sequentially interacts with
the environment by pulling an arm from a feasible arm set AtofKarms (Kmight be infinite), where every
arm could be represented by a d-dimensional feature vector, and only the reward of the selected arm is
revealed. HereAtis drawn IID from an unknown distribution. In order to maximize the cumulative reward,
the agent would update its strategy on the fly to balance the exploration-exploitation tradeoff.
Generalized linear bandit (GLB) was first proposed in Filippi et al. (2010) and has been extensively studied
under various settings over the recent years (Jun et al., 2017; Kang et al., 2022), where the stochastic payoff
of an arm follows a generalized linear model (GLM) of its associated feature vector and some fixed, but
initially unknown parameter θ∗. Note that GLB extends the linear bandit (Abbasi-Yadkori et al., 2011)
in representation power and has greater applicability in the real-world applications, e.g. logistic bandit
1Published in Transactions on Machine Learning Research (04/2024)
algorithms Zhang et al. (2016) can achieve improvement over linear bandit when the rewards are binary.
Upper Confidence Bound (UCB) (Auer et al., 2002a; Filippi et al., 2010; Li et al., 2010) and Thompson
Sampling (TS) (Agrawal & Goyal, 2012; 2013) are the two most popular ideas to solve the GLB problem.
Both of these methods could achieve the optimal regret bound of order ˜O(√
T)1under some mild conditions,
whereTstands for the total number of rounds (Agrawal & Goyal, 2013).
However, the empirical performance of these bandit algorithms significantly depends on the configuration of
hyperparameters, and simply using theoretical optimal values often yields unsatisfactory practical results,
not to mention some of them are unspecified and need to be learned in reality. For example, in both
LinUCB (Li et al., 2010) and LinTS (Abeille & Lazaric, 2017; Agrawal & Goyal, 2013) algorithms, there are
hyperparameters called exploration rates that govern the tradeoff and hence the learning process. But it
has been empirically verified that the best exploration rate to use is always instance-dependent and may
vary at different iterations Bouneffouf & Claeys (2020); Ding et al. (2022b). Note it is inherently impossible
to use any state-of-the-art offline hyperparameter tuning methods such as cross validation (Stone, 1974) or
Bayesian optimization (Frazier, 2018) since decisions in bandits should be made in real time. To choose the
best hyperparameters, some previous works use grid search in their experiments (Ding et al., 2021; Jun et al.,
2019), but obviously, this approach is infeasible when it comes to reality, and how to manually discretize the
hyperparameter space is also unclear. Conclusively, this limitation has already become a bottleneck for bandit
algorithms in real-world applications, but unfortunately, it has rarely been studied in the previous literature.
The problem of hyperparameter optimization for contextual bandits was first studied in Bouneffouf & Claeys
(2020), where the authors proposed two methods named OPLINUCB and DOPLINUCB to learn the practically
optimal exploration rate of LinUCB in a finite candidate set by viewing each candidate as an arm and then
using multi-armed bandit to pull the best one. However, 1) the authors did not provide any theoretical support,
and 2) we believe the best exploration parameter in practice would vary during iterations – more exploration
may be preferred at the beginning due to the lack of observations, while more exploitation would be favorable
in the long run when the model estimate becomes more accurate. Furthermore, 3) they only consider tuning
one single hyperparameter. To tackle these issues, Ding et al. (2022b) proposed TL and Syndicated framework
by using a non-stationary multi-armed bandit for the hyperparameter set. However, their approach still
requires a pre-defined set of hyperparameter candidates. In practice, choosing the candidates requires domain
knowledge and plays a crucial role in the performance. Also, using a piecewise-stationary setting instead of a
complete adversarial bandit (e.g. EXP3) for hyperparameter tuning is more efficient since we expect a fixed
hyperparameter setting would yield indistinguishable results in a period of time. Conclusively, it would be
more efficient to use a continuous space for bandit hyperparameter tuning.
We propose an efficient bandit-over-bandit (BOB) framework (Cheung et al., 2019) named Continuous
Dynamic Tuning (CDT) framework for bandit hyperparameter tuning in the continuous hyperparameter
space, without requiring a pre-defined set of hyperparameter candidate configurations. For the top layer
bandit we formulate the online hyperparameter tuning as a non-stationary Lipschitz continuum-arm bandit
problem with noise where each arm represents a hyperparameter configuration and the corresponding reward
is the performance of the GLB, and the expected reward is a time-dependent Lipschitz function of the
arm with some biased noise. Here the bias depends on the previous observations since the history could
also affect the update of bandit algorithms. It is also reasonable to assume the Lipschitz functions are
piecewise stationary since we believe the expected reward would be stationary with the same hyperparameter
configuration over a period of time (i.e. switching environment). Specifically, for the top layer of our CDT
framework, we propose the Zooming TS algorithm with Restarts, and the key idea is to adaptively refine
the hyperparameter space and zoom into the regions with more promising reward (Kleinberg et al., 2019)
by using the TS methodology (Chapelle & Li, 2011). Moreover, we demonstrate that a simple restart trick
could handle the piecewise changes of the bandit environments in both theory and practice. To sum up, we
summarize our contributions as follows:
1) We propose an online continuous hyperparameter optimization framework for contextual bandits called
CDT that handles all aforementioned issues of previous methods with theoretical guarantees. To the best
of our knowledge, CDT is the first hyperparameter tuning method (even model selection method) with
1˜O(·)ignores the poly-logarithmic factors.
2Published in Transactions on Machine Learning Research (04/2024)
continuous candidates in the bandit community. 2) For the top layer of CDT, we propose the Zooming TS
algorithm with Restarts for Lipschitz bandits under the switching environment. To the best of our knowledge,
our work is the first one to consider the Lipschitz bandits under the switching environment, and the first one
to utilize TS methodology in Lipschitz bandits. 3) Experiments on both synthetic and real datasets with
various GLBs validate the efficiency of our method.
Notations: For a vector x∈Rd, we use∥x∥to denote its l2norm and∥x∥A:=√
x⊤Axfor any positive
definite matrix A∈Rd×d. We also denote [T] ={1,...,T}forT∈N+.
2 Related Work
There has been extensive literature on contextual bandit algorithms, and most of them are based on the
UCB or TS techniques. For example, several UCB-type algorithms have been proposed for GLB, such as
GLM-UCB (Filippi et al., 2010) and UCB-GLM (Li et al., 2017) that achieve the optimal ˜O(√
T)regret
bound. Another rich line of work on GLBs follows the TS idea, including Laplace-TS (Chapelle & Li, 2011),
SGD-TS (Ding et al., 2021), etc. In this paper, we focus on the hyperparameter tuning of contextual bandits,
which is a practical but under-explored problem. For related work, Sharaf & Daumé III (2019) first studied
how to learn the exploration parameters in contextual bandits via a meta-learning method. However, this
algorithm fails to adjust the learning process based on previous observations and hence can be unstable in
practice. Bouneffouf & Claeys (2020) then proposed OPLINUCB and DOPLINUCB to choose the exploration
rate of LinUCB from a candidate set, and moreover Ding et al. (2022b) formulates the hyperparameter
tuning problem as a non-stochastic multi-armed bandit and utilizes the classic EXP3 algorithm. However,
as we mentioned in Section 1, both works have several limitations that could be decently fixed. Note that
hyperparameter tuning could be regarded as a branch of model selection in bandit algorithms. To name a
few for this general problem, Agarwal et al. (2017) proposed a master algorithm that could combine multiple
bandit algorithms, while Foster et al. (2019) initiated the study of model selection tradeoff in contextual
bandits and proposed the first model selection algorithm for contextual linear bandits. Pacchiano et al. (2020)
further considered the confidence tuning in OFUL and model selection in reinforcement learning. However,
these general model selection methods may fail for the bandit hyperparameter tuning task. To clarify this
point, we take the state-of-the-art corralling idea Agarwal et al. (2017) as an example: in theory, it has regret
bound or order O(√
MT+MR max)whereMis the number of base models (number of hyperparameter
combinations in our setting) and Rmaxis the regret of the worst candidate model in the tuning set. Therefore,
on the one hand, Mis infinitely large in our problem setting with a continuous candidate set, which means
the regret bound would also be infinitely large. On the other hand, in order to achieve sub-linear regret in
hyperparameter tuning, the corralling idea requires that all hyperparameter candidates yield sub-linear regret
in theory, which is a very unrealistic assumption. On the contrary, our work only assumes the existence of a
hyperparameter candidate in the tuning set which yields good theoretical regret in theory. In experiments, it
is also costly to use since it requires updating all base models at each round, and we have infinitely many
base models under our setting. Ding et al. (2022b) includes the corralling idea in their experiments, and we
can observe that it achieves almost linear regret in each setting since it has no sub-linear regret guarantee
for the bandit hyperparameter tuning problem. In conclusion, the only existing methods that focus on
hyperparameter tuning of bandits are OP and TL (Syndicated), and we use both of them in our paper as
baselines. And we propose the first continuous hyperparameter tuning framework for contextual bandits,
which doesn’t require a pre-defined set of candidates. Note it is doable to finely discretize the continuous
space and then implement an algorithm with discrete candidate sets (e.g. Syndicated) in methodology, but
we highlight the inefficiency of this idea on both the empirical and theoretical side in Appendix A.4.
We also briefly review the literature on Lipschitz bandits that follows two key ideas. One is uniformly
discretizing the action space into a mesh (Kleinberg, 2004; Magureanu et al., 2014) so that any learning
process like UCB could be directly utilized. Another more popular idea is adaptive discretization on the
action space by placing more probes in more encouraging regions (Bubeck et al., 2008; Kleinberg et al., 2019;
Lu et al., 2019; Valko et al., 2013), and UCB could be used for exploration. Furthermore, the Lipschitz bandit
under adversarial corruption was recently studied in Kang et al. (2023). In addition, (Podimata & Slivkins,
2021) proposed the first fully adversarial Lipschitz bandit in an adaptive refinement manner and derived
instance-dependent regret bounds, but their algorithm relies on some unspecified hyperparameters and is
3Published in Transactions on Machine Learning Research (04/2024)
computationally infeasible. Since the expected reward function for hyperparameters would not drastically
change every time, it is also inefficient to use a fully adversarial algorithm here. Therefore, we introduce a
new problem of Lipschitz bandits under the switching environment, and propose the Zooming TS algorithm
with a restart trick to deal with the “almost stationary” nature of the bandit hyperparameter tuning problem.
3 Preliminaries
We first review the problem setting of contextual bandit algorithms. Denote Tas the total number of
rounds and Kas the number of arms we could choose at each round, where Kcould be infinite. At
each round t∈[T]:={1,...,T}, the player is given Karms represented by a set of feature vectors
Xt={xt,a|a∈[K]}⊆Rddrawn from some unknown distribution, where xt,ais ad-dimensional vector
containing information of arm aat roundt. The player selects an action at∈[K]based on the current Xt
and previous observations, and only receives the payoff of the pulled arm at. Denotext:=xt,atas the feature
vector of the chosen arm atandytas the corresponding reward. We assume the reward ytfollows a canonical
exponential family with minimal representation, a.k.a. generalized linear bandits (GLB) with some mean
functionµ(·). In addition, one can represent this model by yt=µ(x⊤
tθ∗) +ϵt, whereϵtfollows a sub-Gaussian
distribution with parameter σ2independent with the information filtration Ft=σ({as,Xs,ys}t−1
s=1)andσ(Xt)
up to round t, andθ∗is some unknown coefficient. Denote at,∗:=arg maxa∈[K]µ(x⊤
t,aθ∗)as the optimal arm
at roundtandxt,∗as its corresponding feature vector. The goal is to minimize the expected cumulative
regret defined as:
R(T) =T/summationdisplay
t=1/bracketleftbig
µ(xt,∗⊤θ∗)−E/parenleftbig
µ(x⊤
tθ∗)/parenrightbig/bracketrightbig
. (1)
Note that all state-of-the-art contextual GLB algorithms depend on at least one hyperparameter to balance
the well-known exploration-exploitation tradeoff. For example, LinUCB (Li et al., 2010), the most popular
UCB linear bandit, uses the following rule for arm selection at round t:
at= arg max
a∈[K]x⊤
t,aˆθt+α1(t)∥xt,a∥V−1
t. (LinUCB)
Here the model parameter ˆθtis estimated at each round tvia ridge regression, i.e. ˆθt=V−1
t/summationtextt−1
s=1xsyswhere
Vt=λIr+/summationtextt−1
s=1xsx⊤
s. And it considers the standard deviation of each arm with an exploration parameter
α1(t), where with a larger value of α1(t)the algorithm will be more likely to explore uncertain arms. Note
that the regularization parameter λis only used to ensure Vtis invertible and hence its value is not crucial and
commonly set to 1. In theory we can choose the value of α1(t)asα1(t) =σ/radicalbig
rlog ((1 +t/λ)/δ)+∥θ∗∥√
λ,
to achieve the optimal /tildewideO(√
T)bound of regret: However, in practice, the values of σand∥θ∗∥are unspecified,
and hence this theoretical value of α1(t)is inaccessible. Furthermore, it has been shown that this is a very
conservative choice that would lead to unsatisfactory practical performance, and the practically optimal
hyperparameter values to use are distinct and far from the theoretical ones under different algorithms or
settings. We also conduct a series of simulations with several state-of-the-art GLB algorithms to validate
this fact, which is deferred to Appendix A.1. Conclusively, the best exploration parameter to use in practice
should always be chosen dynamically based on the specific scenario and past observations. In addition, many
GLB algorithms depend on some other hyperparameters, which may also affect the performance. For example,
SGD-TS also involves a stepsize parameter for the stochastic gradient descent besides the exploration rate,
and it is well known that a decent stepsize could remarkably accelerate the convergence (Loizou et al., 2021).
To handle all these cases, we propose a general framework that can be used to automatically tune multiple
continuous hyperparameters for a contextual bandit.
For a certain contextual bandit, assume there are pdifferent hyperparameters α(t) ={αi(t)}p
i=1, and each
hyperparameter αi(t)could take values in an interval [ai,bi],∀t. Denote the parameter space A=/circlemultiplytextp
i=1[ai,bi],
and the theoretical optimal values as α∗(t). Given the observations Ftup to round t, we writeat(α(t)|Ft)as
the arm we pulled when the hyperparameters are set to α(t), andxt(α(t)|Ft)as the corresponding feature
vector.
Motivated by the success of Bayesian optimization (Frazier, 2018) on the hyperparameter tuning of the offline
machine learning algorithms, the main idea of our algorithm is to formulate the hyperparameter optimization
4Published in Transactions on Machine Learning Research (04/2024)
Algorithm 1 Zooming TS algorithm with Restarts
Input:Time horizon T, spaceA, epoch size H.
1:fort= 1toTdo
2:ift∈{τH+ 1:τ=0,1,...}then
3: Initialize the total candidate space A0=Aand the active set J⊆A0s.t.A0⊆∪v∈JB(v,r1(v))
andn1(v)←1,∀v∈J. ▷Restart
4:else if ˆft(v)−ˆft(u)>rt(v) + 2rt(u)for some pair of u,v∈Jthen
5: SetJ=J\{u}andA0=A0\B(u,rt(u)). ▷Removal
6:end if
7:ifA0⊈∪v∈JB(v,rt(v))then ▷Activation
8: Activate and pull some point v∈A0that has not been covered: J=J∪{v},vt=v.
9:else
10:vt= arg maxv∈JIt(v), break ties arbitrarily. ▷Selection
11:end if
12:Observe the reward ˜yt+1, and then update components in the Zooming TS algorithm:
nt+1(v),ˆft+1(v),rt+1(v),st+1(v)for the chosen vt∈J:
nt+1(vt) =nt(vt) + 1,ˆft+1(vt) = ( ˆft(vt)nt(vt) + ˜yt+1)/nt+1(vt).
13:end for
as a (another layer of) non-stationary Lipschitz bandit in the continuous space A⊆Rp, i.e. the agent chooses
an arm (hyperparameter combination) α∈Ain roundt∈[T], and then we decompose µ(xt(α|Ft)⊤θ∗)as
µ(xt(α|Ft)⊤θ∗) =gt(α) +ηFt,α. (2)
Heregtis some time-dependent Lipschitz function that formulates the performance of the bandit algorithm
under the hyperparameter combination αat roundt, since the bandit algorithm tends to pull similar arms if
the chosen values of hyperparameters are close at round t. In other words, we expect close hyperparameter
values to yield similar results with other conditions fixed, as in Bayesian optimization on offline hyperparameter
tuning. To demonstrate that our Lipschitz assumption w.r.t. the hyperparameter values in Eqn. equation 3
is reasonable, we conduct simulations with LinUCB and LinTS, and defer it to Appendix A due to the space
limit. Moreover, (ηFt,α−E[ηFt,α])is IID sub-Gaussian with parameter τ2, and to be fair we assume E[ηFt,α]
could also depend on the history Ftsince past observations and action sets would explicitly influence the
model parameter estimation and hence the decision making at each round. In addition to Lipschitzness, we
also suppose gtfollows a switching environment: gtis piecewise stationary with some change points, i.e.
|gt(α1)−gt(α2)|≤∥α1−α2∥,∀α1,α2∈A; (3)
T−1/summationdisplay
t=11[∃α∈A:gt(α)̸=gt+1(α)] =c(T), c(T)∈N. (4)
Since after sufficient exploration, the expected reward should be stable with the same hyperparameter setting,
we could assume that c(T) =˜O(1). Detailed justification on this piecewise Lipschitz assumption is deferred
to Remark B.1 in Appendix B. Although numerous research works have considered the switching environment
(a.k.a.abruptly-changing environment) for multi-armed or linear bandits (Auer et al., 2002b; Wei et al., 2016),
our work is the first to introduce this setting into the continuum-armed bandits. In Section 4.1, we will show
that by combining our proposed Zooming TS algorithm for Lipschitz bandits with a simple restarted strategy,
a decent regret bound could be achieved under the switching environment.
4 Main Results
In this section, we present our novel online hyperparameter optimization framework that could be easily
adapted to most contextual bandit algorithms. We first introduce the continuum-arm Lipschitz bandit
problem under the switching environment, and propose the Zooming TS algorithm with Restarts which
5Published in Transactions on Machine Learning Research (04/2024)
Figure 1: Illustration of the restarted strategy.
modifies the traditional Zooming algorithm (Kleinberg et al., 2019) to make it more efficient and also adaptive
to theswitching environment. Subsequently, we propose our bandit hyperparameter tuning framework named
Continuous Dynamic Tuning (CDT) by making use of our proposed Zooming TS algorithm with Restarts
and the Bandit-over-Bandit (BOB) idea.
W.l.o.g. we assume that there exists a positive constant Ssuch that∥θ∗∥≤Sand∥xt,a∥≤1,∀t,a, and
each hyperparameter space has been shifted and scaled to [0,1]. We also assume that the mean reward
µ(x⊤
t,aθ∗)∈[0,1], and hence naturally gt(α)∈[0,1],∀α∈A= [0,1]p,t∈[T].
4.1 Zooming TS Algorithm with Restarts
For simplicity and consistency, we will reload and introduce a new system of notations in this subsection.
Consider the non-stationary Lipschitz bandit problem on a compact space Aunder some metric Dist(·,·)≥0,
where the covering dimension is denoted by pc. The learner pulls an arm vt∈Aat roundt∈[T]and
subsequently receives a reward ˜ytsampled independently of Pvtas˜yt=ft(vt) +ηv, wheret= 1,...,Tandηv
is IID zero-mean error with sub-Guassian parameter τ2
0, andftis the expected reward function at round tand
is Lipschitz with respect to Dist(·,·). Theswitching environment assumes the time horizon Tis partitioned
intoc(T) + 1intervals, and the bandit stays stationary within each interval, i.e.
|ft(m)−ft(n)|≤Dist(m,n), m,n∈A;andT−1/summationdisplay
t=11[∃m∈A:ft(m)̸=ft+1(m)] =c(T).
Here in this section c(T) =o(T)could be any integer. The goal of the learner is to minimize the expected
(dynamic) regret that is defined as:
RL(T) =T/summationdisplay
t=1max
v∈Aft(v)−/summationdisplayT
t=1E(ft(vt)).
At each round t,v∗
t:=arg maxv∈Aft(v)denotes the maximal point (w.l.o.g. assume it’s unique), and
∆t(v) =ft(v∗)−ft(v)is the “badness” of each arm v. We also denote Ar,tas ther-optimal region at the
scaler∈(0,1], i.e.Ar,t={v∈A:r/2<∆t(v)≤r}at timet. Then the r-zooming number Nz,t(r)of
(A,ft)is defined as the minimal number of balls of radius no more than rrequired to cover Ar,t. (Note the
subscriptzstands for zooming here.) Next, we define the zooming dimension pz,t(Kleinberg et al., 2019) at
timetas the smallest q≥0such that for every r∈(0,1]ther-zooming number can be upper bounded by
cr−qfor some multiplier c>0free ofr:
pz,t= min{q≥0 :∃c>0,Nz,t(r)≤cr−q,∀r∈(0,1]}.
It’s obvious that 0≤pz,t≤pc,∀t∈[T]. (Notepz,tis fixed under the stationary environment.) On the other
hand, the zooming dimension could be much smaller than pcunder some mild conditions. For example, if
6Published in Transactions on Machine Learning Research (04/2024)
the payoff function ftdefined on Rpcis greater than∥v∗
t−v∥βin scale for some β≥1aroundv∗in the
spaceA, i.e.ft(v∗
t)−ft(v) = Ω(∥v∗
t−v∥β), then it holds that pz,t≤(1−1/β)pc. Note that we have β= 2
(i.e.pz,t≤pc/2) whenft(·)isC2-smooth and strongly concave in a neighborhood of v∗. More details are
presented in Appendix C. Since the expected reward Lipschitz function ft(·)is fixed in each time interval
under the switching environment, the zooming number and zooming dimension pz,twould also stay identical.
And we also write pz,∗= maxt∈[T]pz,t≤pc.
Our proposed Algorithm 1 extends the classic Zooming algorithm (Kleinberg et al., 2019), which was used
under the stationary Lipschitz bandit environment, by adding several new ingredients for better efficiency
and adaptivity to non-stationary environment: on the one hand, we employ the TS methodology and
propose a novel removal step. Here we utilize TS since it was shown that TS is more robust than UCB in
practice (Chapelle & Li, 2011; Wang & Chen, 2018), and the removal procedure in line 5 of Algorithm 1
could adaptively subtract regions that are prone to yield low rewards. Both of these two ideas could enhance
the algorithmic efficiency, which coincides with the practical orientation of our work. On the other hand, the
restarted strategy proceeds our proposed Zooming TS in epochs and refreshes the algorithm after every H
rounds, as displayed in Figure 1. The epoch size His fixed through the total time horizon and controls the
tradeoff between non-stationarity and stability. Note that Hin our algorithm does not need to match the
actual length of stationary intervals of the environment, and we would discuss its selection later. At each
epoch, we maintain a time-varying active arm set St⊆A, which is initially empty and updated every time.
For each arm v∈Aand timet, denotent(v)as the number of times arm vhas been played before time t
since the last restart, and ˆft(v)as the corresponding average sample reward. We let ˆft(v) = 0whennt(v) = 0.
Define the confidence radius and the TS standard deviation of active arm vat timetrespectively as
rt(v) =/radicaligg
13τ2
0lnT
2nt(v), st(v) =s0/radicaligg
1
nt(v), (5)
wheres0=/radicalbig
52πτ2
0ln(T). We callB(v,rt(v)) ={u∈Rp:Dist(u,v)≤rt(v)}as the confidence ball of arm
vat timet∈[T]. We construct a randomized algorithm by choosing the best active arm according to the
perturbed estimate mean It(·):
It(v) =ˆft(v) +st(v)Zt,v, (6)
whereZt,vis i.i.d. drawn from the clipped standard normal distribution: we first sample ˜Zt,vfrom the
standard normal distribution and then set Zt,v=max{1/√
2π,˜Zt,v}. This truncation was also used in TS
multi-armed bandits (Jin et al., 2021), and our algorithm clips the posterior samples with a lower threshold
to avoid underestimation of good arms. Moreover, the explanations of the TS update is deferred to Appendix
D due to the space limit.
The regret analysis of Algorithm 1 is very challenging since the active arm set is constantly changing and the
optimal arm v∗cannot be exactly recovered under the Lipschitz bandit setting. Thus, existing theory on
multi-armed bandits with TS is not applicable here. We overcome these difficulties with some innovative use
of metric entropy theory, and the regret bound of Algorithm 1 is given as follows.
Theorem 4.1. WithH= Θ/parenleftbig
(T/c(T))(pz,∗+2)/(pz,∗+3)]/parenrightbig
, the total regret of our Zooming TS algorithm with
Restarts under the switching environment over time Tis bounded as
RL(T)≤˜O/parenleftig
(c(T))1/(pz,∗+3)T(pz,∗+2)/(pz,∗+3)/parenrightig
,
whenc(T)>0. In addition, if the environment is stationary (i.e. c(T) = 0,ft=f,pz,t=pz,∗:=pz,∀t∈[T]),
then by using H=T(i.e. no restart), our Zooming TS algorithm could achieve the optimal regret bound for
Lipschitz bandits up to logarithmic factors:
RL(T)≤˜O/parenleftig
T(pz+1)/(pz+2)/parenrightig
.
We also present empirical studies to further evaluate the performance of our Algorithm 1 compared with
stochastic Lipschitz bandit algorithms in Appendix A.3. A potential drawback of Theorem 4.1 is that
7Published in Transactions on Machine Learning Research (04/2024)
Algorithm 2 Continuous Dynamic Tuning (CDT)
Input:T1,T2,{Xt}T
t=1,A=/circlemultiplytextp
i=1[ai,bi].
1:Randomly choose at∈[K]and observe xt,yt, t≤T1.
2:Initialize the hyperparameter active set Js.t.A⊆∪v∈JB(v,r1(v))wherenT1(v)←1,∀v∈J.
3:fort= (T1+ 1)toTdo
4:Run thet-th iteration of Algorithm 1 with initial input horizon T−T1, input space Aand restarting
epoch length T2. Denote the pulled arm at round tasα(it)∈A. ▷Top
5:Run the contextual bandit algorithm with hyperparameter α(it)to pull an arm at.▷Bottom
6:Obtainytand update components in the contextual bandit algorithm. ▷Bottom Update
7:Update components in Algorithm 1 by treating ytas the reward of arm α(it)▷Top Update
8:end for
the optimal epoch size Hunderswitching environment relies on the value of c(T)andpz,∗, which are
unspecified in reality. However, this problem could be solved in theory by using the BOB idea (Cheung
et al., 2019; Zhao et al., 2020) to adaptively choose the optimal epoch size with a meta algorithm (e.g.
EXP3 (Auer et al., 2002b)) in real time. In this case, we prove the expected regret can be bounded by the
order of ˜O/parenleftig
Tpc+2
pc+3·max/braceleftig
c(T)1
pc+3,T1
(pc+3)(pc+4)/bracerightig/parenrightig
in general, and some better regret bounds in problem-
dependent cases. More details are presented in Theorem F.1 with its proof in Appendix F. However, in the
following Section 4.2 we could simply set H=T(2+p)/(3+p)in our CDT framework where pis the number of
hyperparameters to be tuned after assuming c(T) =˜O(1)is of constant scale up to logarithmic terms. The
value ofτ0can be determined by assuring the observed rewards are bounded. Note our work introduces a
new problem on Lipschitz bandits under the switching environment. One potential limitation of our work is
how to deduce a regret lower bound under this problem setting is unclear, and we leave it as a future work.
4.2 Online Continuous Hyperparameter Optimization for Contextual Bandits
Based on the proposed algorithm in the previous subsection, we introduce our online double-layer Continuous
Dynamic Tuning (CDT) framework for hyperparameter optimization of contextual bandit algorithms. We
assume the arm to be pulled follows a fixed distribution given the hyperparameters to be used and the history
at each round. The detailed algorithm is shown in Algorithm 2. Our method extends the bandit-over-bandit
(BOB) idea that was first proposed for non-stationary stochastic bandit problems (Cheung et al., 2019), where
it adjusts the sliding-window size dynamically based on the changing model. In our work, for the top layer we
use our proposed Algorithm 1 to tune the best hyperparameter values from the admissible space, where each
arm represents a hyperparameter configuration and the corresponding reward is the algorithmic result. T2is
the length of each epoch (i.e. Hin Algorithm 1), and we would refresh our Zooming TS Lipschitz bandit
after every T2rounds as shown in Line 5 of Algorithm 2 due to the non-stationarity. The bottom layer is the
primary contextual bandit and would run with the hyperparameter values α(it)chosen from the top layer
at each round t. We also include a warming-up period of length T1in the beginning to guarantee sufficient
exploration as in Li et al. (2017); Ding et al. (2021). Despite the focus of our CDT framework is on the
practical aspect, we also present a novel theoretical analysis in the following for the completeness of our work.
Although there has been a rich line of work on regret analysis of UCB and TS GLB algorithms, most literature
certainly requires that some hyperparameters, e.g. exploration rate, always take their theoretical values. It is
challenging to study the regret bound of GLB algorithms when their hyperparameters are synchronously tuned
in real time, since the chosen hyperparameter values may be far from the theoretical ones in practice, not to
mention that previous decisions would also affect the current update cumulatively. Moreover, there is currently
no existing literature and regret analysis on hyperparameter tuning (or model selection) for bandit algorithms
with an infinite number of candidates in a continuous space. Recall that we denote Ft=σ/parenleftbig
{as,Xs,ys}t−1
s=1/parenrightbig
as the past information before round tunder our CDT framework, and at,xtare the chosen arm and its
corresponding feature vector at time t, which implies that at=at(α(it)|Ft),xt=xt(α(it)|Ft). Furthermore,
we denoteα∗(t)as the theoretical optimal value at round tandF∗
tas the past information filtration by
always using the theoretical optimal α∗(t). Since the decision at each round talso depends on the history
observe by time t, the pulled arm with the same hyperparameter α(t)might be different under FtorF∗
t. To
8Published in Transactions on Machine Learning Research (04/2024)
analyze the cumulative regret R(T)of our Algorithm 2, we first decompose it into four quantities:
R(T) =E/bracketleftiggT1/summationdisplay
t=1/parenleftbig
µ(x⊤
t,∗θ∗)−µ(xt⊤θ∗)/parenrightbig/bracketrightigg
/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
Quantity (A)+E/bracketleftiggT/summationdisplay
t=T1+1/parenleftbig
µ(x⊤
t,∗θ∗)−µ(xt(α∗(t)|F∗
t)⊤θ∗)/parenrightbig/bracketrightigg
/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
Quantity (B)
+E/bracketleftiggT/summationdisplay
t=T1+1(µ/parenleftbig
xt(α∗(t)|F∗
t)⊤θ∗)−µ(xt(α∗(t)|Ft)⊤θ∗)/parenrightbig/bracketrightigg
/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
Quantity (C)
+E/bracketleftiggT/summationdisplay
t=T1+1(µ/parenleftbig
xt(α∗(t)|Ft)⊤θ∗)−µ(xt(α(it)|Ft)⊤θ∗)/parenrightbig/bracketrightigg
/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
Quantity (D).
Intuitively, Quantity (A) is the regret paid for pure exploration during the warming-up period and could be
controlled by the order O(T1). Quantity (B) is the regret of the contextual bandit algorithm that runs with the
theoretical optimal hyperparameters α∗(t)all the time, and hence it could be easily bounded by the optimal
scale ˜O(√
T)based on the literature. Quantity (C) is the difference of cumulative reward with the same α∗(t)
under two separate lines of history. Quantity (D) is the extra regret paid to tune the hyperparameters on
the fly. By using the same line of history Ftin Quantity (D), the regret of our Zooming TS algorithm with
Restarts in Theorem 4.1 can be directly used to bound Quantity (D). Conclusively, we deduce the following
theorem for the regret bound:
Theorem 4.2. Under our problem setting in Section 3, for UCB and TS GLB algorithms with explo-
ration hyperparameters (e.g. LinUCB, UCB-GLM, GLM-UCB, LinTS), by taking T1=O(T2/(p+3)),T2=
O(T(p+2)/(p+3))wherepis the number of hyperparameters, and let the theoretically optimal hyperparameter
combination α∗(T)∈A, it holds that
E[R(T)]≤˜O(T(p+2)/(p+3)).
The detailed proof of Theorem 4.2 is presented in Appendix G. Note that this regret bound could be further
improved to ˜O(T(p0+2)/(p0+3))wherep0is any constant that is no smaller than the zooming dimension of
(A,gt),∀t. For example, from Figure 3 in Appendix A we can observe that in practice gtwould beC2-smooth
and strongly concave, which implies that E[R(T)]≤˜O(T(p+4)/(p+6)).
Note our work is the first one to consider model selection for bandits with a continuous candidate set,
and the regret analysis for online model selection in the bandit setting (Foster et al., 2019) is intrinsically
difficult. For example, regret bounds of the algorithm CORRAL (Agarwal et al., 2017) for model selection
and Syndicated (Ding et al., 2022b) for bandit hyperparameter tuning are (sub)linearly dependent on the
number of candidates, which would be infinitely large and futile in our case. Furthermore, given the fact
that Syndicated in Ding et al. (2022b) fails to recover the optimal O(√
T)bound of regret without stringent
assumptions under the easier setting with finite hyperparameter candidates, it would be substantially difficult
to deduce a feasible regret bound under our more complicated problem setting. Moreover, the non-stationarity
under the switching environment would further deteriorate the optimal order of cumulative regret Cheung et al.
(2019). And it is intrinsically more difficult to consider the continuum-armed bandit over the multi-armed
bandit. Therefore, we believe our theoretical result is non-trivial and significant. Our work stands as the first
seminal attempt in bandit hyperparameter tuning (or even bandit model selection) with an infinite number
of candidates. An extensive study on this new problem will be an interesting future direction.
5 Experimental Results
In this section, we show by experiments that our hyperparameter tuning framework outperforms the theoretical
hyperparameter setting and other tuning methods with various (generalized) linear bandit algorithms. We
9Published in Transactions on Machine Learning Research (04/2024)
3000 6000 9000 12000
Iterations0100200300400500600700Cumulative RegretSimulations for LinUCB
Theory
CDT
OP
TL
3000 6000 9000 12000
Iterations020040060080010001200Cumulative RegretMovielens for LinUCB
Theory
CDT
OP
TL
3000 6000 9000 12000
Iterations02505007501000125015001750Cumulative RegretSimulations for LinTS
Theory
CDT
OP
TL
3000 6000 9000 12000
Iterations02505007501000125015001750Cumulative RegretMovielens for LinTS
Theory
CDT
OP
TL
3000 6000 9000 12000
Iterations0100200300400Cumulative RegretSimulations for UCB-GLM
Theory
CDT
OP
TL
3000 6000 9000 12000
Iterations0100200300400500600700Cumulative RegretMovielens for UCB-GLM
Theory
CDT
OP
TL
3000 6000 9000 12000
Iterations0100200300400500600Cumulative RegretSimulations for GLM-TSL
Theory
CDT
OP
TL
3000 6000 9000 12000
Iterations0200400600800Cumulative RegretMovielens for GLM-TSL
Theory
CDT
OP
TL
3000 6000 9000 12000
Iterations0100200300400500600700Cumulative RegretSimulations for Laplace-TS
Theory
CDT
OP
TL
3000 6000 9000 12000
Iterations0250500750100012501500Cumulative RegretMovielens for Laplace-TS
Theory
CDT
OP
TL
3000 6000 9000 12000
Iterations0100200300400500Cumulative RegretSimulations for GLOC
CDT
OP
Syndicated
3000 6000 9000 12000
Iterations0100200300400500600700Cumulative RegretMovielens for GLOC
CDT
OP
Syndicated
3000 6000 9000 12000
Iterations0100200300400500600Cumulative RegretSimulations for SGD-TS
Theory
CDT
OP
Syndicated
3000 6000 9000 12000
Iterations020040060080010001200Cumulative RegretMovielens for SGD-TS
Theory
CDT
OP
Syndicated
Figure 2: Cumulative regret curves of our CDT framework compared with existing hyperparameter selection
methods under multiple (generalized) linear bandit algorithms on the simulations and Movielens dataset.
utilize seven state-of-the-art bandit algorithms: two of them (LinUCB (Li et al., 2010), LinTS (Agrawal &
Goyal, 2013)) are linear bandits, and the other five algorithms (UCB-GLM (Li et al., 2017), GLM-TSL (Kveton
et al., 2020), Laplace-TS (Chapelle & Li, 2011), GLOC (Jun et al., 2017), SGD-TS (Ding et al., 2021)) are
GLBs. Note that all these bandit algorithms except Laplace-TS contain an exploration rate hyperparameter,
while GLOC and SGD-TS further require an additional learning parameter. And Laplace-TS only depends
on one stepsize hyperparameter for a gradient descent optimizer. We compare our CDT framework with the
theoretical setting, OP (Bouneffouf & Claeys, 2020) and TL (Ding et al., 2022b) (one hyperparameter) and
Syndicated (Ding et al., 2022b) (multiple hyperparameters) algorithms. Their details are given as follows:
(1)Theoretical setting : We implement the theoretical exploration rate and stepsize for each algorithm.
For the stepsize of gradient descent used in SGD-TS and Laplace-TS, we set it as 1 instead. (We observe
the algorithmic performance is not sensitive to this stepsize.)
(2)OP: (Bouneffouf & Claeys, 2020) proposes OPLINUCB to tune the exploration rate of LinUCB. Here
we modify it so that it could be used in other bandit algorithms. Note that OP is only applicable to
algorithms with one hyperparameter, and hence we fix the learning parameter of GLOC and SGD-TS as
their theoretical values instead, and only tune the exploration rates.
(3)TL(Ding et al., 2022b) (one hyperparameter): For algorithms with only one hyperparameter, TL is used.
(4)Syndicated (Ding et al., 2022b) (multiple hyperparameters): For GLOC and SGD-TS (two hyperparam-
eters), the Syndicated framework is utilized for comparison.
We run comprehensive experiments on both simulations and real-world datasets. Specifically, for the real
data, we use the benchmark Movielens 100K dataset along with the Yahoo News dataset:
10Published in Transactions on Machine Learning Research (04/2024)
Table 1: Running time (seconds) for different algorithms under settings shown in Figure 2.
Algorithm Setting Theory TL OP CDT
Simulation 2.11 4.01 3.70 6.89LinUCBMovielens 2.17 3.87 2.95 7.31
Simulation 2.21 4.10 3.95 7.63LinTSMovielens 2.04 4.09 3.45 7.71
Simulation 7.74 9.84 9.71 12.67UCB-GLMMovielens 7.89 9.64 9.35 13.05
Simulation 304.28 306.98 306.02 309.54GLM-TSLMovielens 294.17 295.87 294.83 298.81
Simulation 523.62 526.31 526.24 531.15Laplace-TSMovielens 500.19 503.45 503.91 509.65
Simulation 486.58 489.31 490.24 593.15GLOCMovielens 474.87 476.52 477.16 481.03
Simulation 67.42 70.62 69.42 73.68SGD-TSMovielens 62.09 66.48 64.68 67.31
(1)Simulation : In each repetition, we simulate all the feature vectors {xt,a}and the model parameter
θ∗according to Uniform( −1/√r,1/√r) elementwisely, and hence we have ∥xt,a∥≤1. We setd=25,
K=120 andT=14,000. For linear model, the expected reward of arm ais formulated as x⊤
t,aθ∗and
random noise is sampled from N(0,0.25); for Logistic model, the mean reward of arm ais defined as
p= 1/(1 + exp(−x⊤
t,aθ∗)), and the output is drawn from a Bernoulli distribution.
(2)Movielens 100K dataset : This dataset contains 100K ratings from 943 users on 1,682 movies. For
data pre-processing, we utilize LIBPMF (Yu et al., 2014) to perform matrix factorization and obtain the
feature matrices for both users and movies with d=20, and then normalize all feature vectors into unit
r-dimensional ball. In each repetition, the model parameter θ∗is defined as the average of 300 randomly
chosen users’ feature vectors. And for each time t, we randomly choose K= 300movies from 1,682
available feature vectors as arms {xt,a}300
a=1. The time horizon Tis set to 14,000. For linear models, the
expected reward of arm ais formulated as x⊤
t,aθ∗and random noise is sampled from N(0,0.5); for Logistic
model, the output of arm ais drawn from the Bernoulli distribution with p= 1/(1 + exp(−x⊤
t,aθ∗)).
(3)Yahoo News dataset: We downloaded the Yahoo Recommendation dataset R6A, which contains Yahoo
data from May 1 to May 10, 2009 with T= 2881timestamps. For each user’s visit, the module will select
one article from a pool of 20articles for the user, and then the user will decide whether to click. We
transform the contextual information into a 6-dimensional vector based on the processing in (Chu et al.,
2009). We build a Logistic bandit on this data, and the observed reward is simulated from a Bernoulli
distribution with a probability of success equal to its click-through rate at each time.
We first present the results on simulations and Movielens datasets: since all the existing tuning algo-
rithms require a user-defined candidate set, we design the tuning set for all potential hyperparameters as
{0.1,1,2,3,4,5}. And for our CDT framework, which is the first algorithm for tuning hyperparameters in an
interval, we simply set the interval as [0.1,5]for all hyperparameters. Each experiment is repeated for 20
times, and the average regret curves with standard deviation are displayed in Figure 2. We further explore
the existing methods after enlarging the hyperparameter candidate set to fairly validate the superiority of
our proposed CDT in Appendix A.4.1. The results in Appendix A.4.1 further lead to discussion on why it is
inefficient to first discretize the continuous space and then implement an algorithm (e.g. Syndicated) with
discrete candidate sets.
We believe a large value of warm-up period T1may abandon some useful information in practice, and hence
we useT1=T2/(p+3)according to Theorem 4.2 in experiments. And we would restart our hyperparameter
tuning layer after every T2= 3T(p+2)/(p+3)rounds. An ablation study on the role of T1,T2in our CDT
framework is also conducted and deferred to Appendix A.4.2, where we demonstrate that the performance of
CDT is pretty robust to the choice of T1,T2in practice.
11Published in Transactions on Machine Learning Research (04/2024)
From Figure 2, we observe that our CDT framework outperforms all existing hyperparameter tuning methods
for most contextual bandit algorithms. It is also clear that CDT performs stably and soundly with the smallest
standard deviation across most datasets (e.g. experiments for LinTS, UCB-GLM), indicating that our method
is highly flexible and robustly adaptive to different datasets. Moreover, when tuning multiple hyperparameters
(GLOC, SGD-TS), we can see that the advantage of our CDT is also evident since our method is intrinsically
designed for any hyperparameter space. It is also verified that the theoretical hyperparameter values are too
conservative and would lead to terrible performance (e.g. LinUCB, LinTS). Note that all tuning methods
exhibit similar results when applied to Laplace-TS. We believe it is because Laplace-TS only relies on
an insensitive hyperparameter that controls the stepsize in gradient descent loops, which mostly affects
the convergence speed. To further validate the high efficiency of our proposed CDT, we also report the
computational running time for the 14 cases corresponding to Figure 2. Specifically, we display the average
running time on each method in Table 1. We can observe that all existing methods and our CDT can run
very fast in practice, and our CDT is only slightly more expensive than TL and OP in computation (CDT
only takes about four more seconds) since the procedure of removal, restarting and activation checks at each
round would take some extra computation. In addition, we can conclude that the main computation time
comes from the contextual bandit algorithm we want to tune on, as is shown that, e.g. GLM-TSL requires
much more time than all other methods under different tuning methods. Therefore, we can conclude that our
CDT significantly outperforms all existing baselines without increasing computational time.
For the Yahoo News Recommendation dataset, since it is a logistic bandit, we only output the cumulative
rewards of GLBs in Table 2. From the table, we can observe that our proposed CDT also performs the
best overall. Specifically, it is only slightly worse than TL for GLM-TSL and GLOC, and yields the best
results among all hyperparameter tuning frameworks for UCB-GLM, GLM-TSL, and SGD-TS. And the
theoretical hyperparameter setting is very unstable again as in Figure 2. Conclusively, our proposed CDT
yields uniformly the best performances compared with existing baselines in both large-scale and mild-scale
experiments with multiple contextual bandit algorithms. This fact also validates the rationality of Lipschitz
continuity assumption on the bandit hyperparameter tuning problem in Section 3.
Method UCB-GLM GLM-TSL Laplace-TS GLOC SGD-TS
Theory 221.51 214.67 217.38 206.73
CDT 221.69 218.27 217.05 217.95 218.35
OP 217.25 217.08 213.95 216.28 215.58
TL/Syndicated 218.95 219.36 214.42 218.19 215.02
Table 2: Comparisons of cumulative rewards from different algorithms on Yahoo dataset.
6 Conclusion
In this paper, we propose the first online continuous hyperparameter optimization method for contextual
bandit algorithms named CDT given the continuous hyperparameter search space. Our framework can attain
sublinear regret bound in theory, and is general enough to handle the hyperparameter tuning task for most
contextual bandit algorithms. Multiple synthetic and real experiments with multiple GLB algorithms validate
the remarkable efficiency of our framework compared with existing methods in practice. In the meanwhile,
we propose the Zooming TS algorithm with Restarts, which is the first work on Lipschitz bandits under the
switching environment.
Limitations and future works: Beyond the hyperparameter selection, our work paves the way for
exploring the broader problem of bandit model selection within a continuous candidate space. Another
promising avenue for future investigation involves conducting a comprehensive study on the non-stationary
Lipschitz bandit problem. The examination of lower bounds for these two novel directions falls outside the
purview of our study but remains intriguing for further exploration.
12Published in Transactions on Machine Learning Research (04/2024)
Acknowledgments
WeappreciatetheinsightfulcommentsfromtheTMLRreviewersandtheactioneditor. Thisworkwaspartially
supported by the National Science Foundation under grants CCF-1934568, DMS-1916125, DMS-2113605,
DMS-2210388, IIS-2008173 and IIS-2048280.
References
Yasin Abbasi-Yadkori, Dávid Pál, and Csaba Szepesvári. Improved algorithms for linear stochastic bandits.
Advances in neural information processing systems , 24, 2011.
Marc Abeille and Alessandro Lazaric. Linear thompson sampling revisited. In Artificial Intelligence and
Statistics , pp. 176–184. PMLR, 2017.
Alekh Agarwal, Haipeng Luo, Behnam Neyshabur, and Robert E Schapire. Corralling a band of bandit
algorithms. In Conference on Learning Theory , pp. 12–38. PMLR, 2017.
Shipra Agrawal and Navin Goyal. Analysis of thompson sampling for the multi-armed bandit problem. In
Conference on learning theory , pp. 39–1. JMLR Workshop and Conference Proceedings, 2012.
Shipra Agrawal and Navin Goyal. Thompson sampling for contextual bandits with linear payoffs. In
International conference on machine learning , pp. 127–135. PMLR, 2013.
Peter Auer, Nicolo Cesa-Bianchi, and Paul Fischer. Finite-time analysis of the multiarmed bandit problem.
Machine learning , 47(2):235–256, 2002a.
Peter Auer, Nicolo Cesa-Bianchi, Yoav Freund, and Robert E Schapire. The nonstochastic multiarmed bandit
problem. SIAM journal on computing , 32(1):48–77, 2002b.
Hamsa Bastani and Mohsen Bayati. Online decision making with high-dimensional covariates. Operations
Research , 68(1):276–294, 2020.
Djallel Bouneffouf and Emmanuelle Claeys. Hyper-parameter tuning for the contextual bandit. arXiv preprint
arXiv:2005.02209 , 2020.
Sébastien Bubeck, Gilles Stoltz, Csaba Szepesvári, and Rémi Munos. Online optimization in x-armed bandits.
Advances in Neural Information Processing Systems , 21, 2008.
Olivier Chapelle and Lihong Li. An empirical evaluation of thompson sampling. Advances in neural
information processing systems , 24, 2011.
Wang Chi Cheung, David Simchi-Levi, and Ruihao Zhu. Learning to optimize under non-stationarity. In The
22nd International Conference on Artificial Intelligence and Statistics , pp. 1079–1087. PMLR, 2019.
Wei Chu, Seung-Taek Park, Todd Beaupre, Nitin Motgi, Amit Phadke, Seinjuti Chakraborty, and Joe
Zachariah. A case study of behavior-driven conjoint analysis on yahoo! front page today module. In
Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining ,
pp. 1097–1104, 2009.
Qin Ding, Cho-Jui Hsieh, and James Sharpnack. An efficient algorithm for generalized linear bandit: Online
stochastic gradient descent and thompson sampling. In International Conference on Artificial Intelligence
and Statistics , pp. 1585–1593. PMLR, 2021.
Qin Ding, Cho-Jui Hsieh, and James Sharpnack. Robust stochastic linear contextual bandits under adversarial
attacks. In International Conference on Artificial Intelligence and Statistics , pp. 7111–7123. PMLR, 2022a.
Qin Ding, Yue Kang, Yi-Wei Liu, Thomas Chun Man Lee, Cho-Jui Hsieh, and James Sharpnack. Syndicated
bandits: A framework for auto tuning hyper-parameters in contextual bandit algorithms. Advances in
Neural Information Processing Systems , 35:1170–1181, 2022b.
13Published in Transactions on Machine Learning Research (04/2024)
Sarah Filippi, Olivier Cappe, Aurélien Garivier, and Csaba Szepesvári. Parametric bandits: The generalized
linear case. In NIPS, volume 23, pp. 586–594, 2010.
Dylan J Foster, Akshay Krishnamurthy, and Haipeng Luo. Model selection for contextual bandits. Advances
in Neural Information Processing Systems , 32, 2019.
Peter I Frazier. A tutorial on bayesian optimization. arXiv preprint arXiv:1807.02811 , 2018.
Tianyuan Jin, Pan Xu, Jieming Shi, Xiaokui Xiao, and Quanquan Gu. Mots: Minimax optimal thompson
sampling. In International Conference on Machine Learning , pp. 5074–5083. PMLR, 2021.
Kwang-Sung Jun, Aniruddha Bhargava, Robert Nowak, and Rebecca Willett. Scalable generalized linear
bandits: Online computation and hashing. Advances in Neural Information Processing Systems , 30, 2017.
Kwang-Sung Jun, Rebecca Willett, Stephen Wright, and Robert Nowak. Bilinear bandits with low-rank
structure. In International Conference on Machine Learning , pp. 3163–3172. PMLR, 2019.
Yue Kang, Cho-Jui Hsieh, and Thomas Chun Man Lee. Efficient frameworks for generalized low-rank matrix
bandit problems. Advances in Neural Information Processing Systems , 35:19971–19983, 2022.
Yue Kang, Cho-Jui Hsieh, and Thomas Chun Man Lee. Robust lipschitz bandits to adversarial corruptions.
InThirty-seventh Conference on Neural Information Processing Systems , 2023. URL https://openreview.
net/forum?id=6RiqluMFNz .
Robert Kleinberg. Nearly tight bounds for the continuum-armed bandit problem. Advances in Neural
Information Processing Systems , 17, 2004.
Robert Kleinberg, Aleksandrs Slivkins, and Eli Upfal. Bandits and experts in metric spaces. Journal of the
ACM (JACM) , 66(4):1–77, 2019.
Branislav Kveton, Manzil Zaheer, Csaba Szepesvari, Lihong Li, Mohammad Ghavamzadeh, and Craig Boutilier.
Randomized exploration in generalized linear bandits. In International Conference on Artificial Intelligence
and Statistics , pp. 2066–2076. PMLR, 2020.
Lihong Li, Wei Chu, John Langford, and Robert E Schapire. A contextual-bandit approach to personalized
news article recommendation. In Proceedings of the 19th international conference on World wide web , pp.
661–670, 2010.
Lihong Li, Yu Lu, and Dengyong Zhou. Provably optimal algorithms for generalized linear contextual bandits.
InInternational Conference on Machine Learning , pp. 2071–2080. PMLR, 2017.
Nicolas Loizou, Sharan Vaswani, Issam Hadj Laradji, and Simon Lacoste-Julien. Stochastic polyak step-size
for sgd: An adaptive learning rate for fast convergence. In International Conference on Artificial Intelligence
and Statistics , pp. 1306–1314. PMLR, 2021.
Shiyin Lu, Guanghui Wang, Yao Hu, and Lijun Zhang. Optimal algorithms for lipschitz bandits with
heavy-tailed rewards. In International Conference on Machine Learning , pp. 4154–4163. PMLR, 2019.
Stefan Magureanu, Richard Combes, and Alexandre Proutiere. Lipschitz bandits: Regret lower bound and
optimal algorithms. In Conference on Learning Theory , pp. 975–999. PMLR, 2014.
AldoPacchiano, ChristophDann, ClaudioGentile, andPeterBartlett. Regretboundbalancingandelimination
for model selection in bandits and rl. arXiv preprint arXiv:2012.13045 , 2020.
Chara Podimata and Alex Slivkins. Adaptive discretization for adversarial lipschitz bandits. In Conference
on Learning Theory , pp. 3788–3805. PMLR, 2021.
Amr Sharaf and Hal Daumé III. Meta-learning for contextual bandit exploration. arXiv preprint
arXiv:1901.08159 , 2019.
14Published in Transactions on Machine Learning Research (04/2024)
Mervyn Stone. Cross-validatory choice and assessment of statistical predictions. Journal of the royal statistical
society: Series B (Methodological) , 36(2):111–133, 1974.
Michal Valko, Alexandra Carpentier, and Rémi Munos. Stochastic simultaneous optimistic optimization. In
International Conference on Machine Learning , pp. 19–27. PMLR, 2013.
Siwei Wang and Wei Chen. Thompson sampling for combinatorial semi-bandits. In International Conference
on Machine Learning , pp. 5114–5122. PMLR, 2018.
Chen-YuWei, Yi-TeHong, andChi-JenLu. Trackingthebestexpertinnon-stationarystochasticenvironments.
Advances in neural information processing systems , 29, 2016.
Michael Woodroofe. A one-armed bandit problem with a concomitant variable. Journal of the American
Statistical Association , 74(368):799–806, 1979.
Hsiang-Fu Yu, Cho-Jui Hsieh, Si Si, and Inderjit S Dhillon. Parallel matrix factorization for recommender
systems. Knowledge and Information Systems , 41(3):793–819, 2014.
Lijun Zhang, Tianbao Yang, Rong Jin, Yichi Xiao, and Zhi-Hua Zhou. Online stochastic linear optimization
under one-bit feedback. In International Conference on Machine Learning , pp. 392–401. PMLR, 2016.
Peng Zhao, Lijun Zhang, Yuan Jiang, and Zhi-Hua Zhou. A simple approach for non-stationary linear bandits.
InInternational Conference on Artificial Intelligence and Statistics , pp. 746–755. PMLR, 2020.
15Published in Transactions on Machine Learning Research (04/2024)
A Supportive Experimental Details
A.1 Simulations on the Optimal Hyperparameter Value in Grid Search
To further validate the necessity of dynamic hyperparameter tuning, we conduct a simulation for UCB
algorithms LinUCB, UCB-GLM, GLOC and TS algorithms LinTS, GLM-TSL with a grid search of exploration
parameter in{0.1,0.5,1,1.5,2,..., 10}and then report the best parameter value under different settings.
Specifically, we set d= 10,T= 8000,K= 60,120, and choose arm xt,aandθ∗randomly in{x:∥x∥≤1}.
Rewards are simulated from N(x⊤
t,aθ∗,0.5)for LinUCB, LinTS, and from Bernoulli( 1/(1 +exp (−x⊤
t,aθ∗))) for
UCB-GLM, GLOC and GLM-TSL. The results are displayed in Table 3, where we can see that the optimal
hyperparameter values are distinct and far from the theoretical ones under different algorithms or settings.
Moreover, the theoretical optimal exploration rate should be identical under different values of Kfor most
algorithms shown here, but in practice the best hyperparameter to use depends on K, which also contradicts
with the theoretical result.
Bandit type Linear bandit Generalized linear bandit
Algorithm LinUCB LinTS UCB-GLM GLOC GLM-TSL
K= 60 2.5 1 1.5 4.5 1.5
K= 120 3 1.5 2.5 5 2
Table 3: The optimal exploration parameter value in grid search for LinUCB, LinTS, UCB-GLM, GLOC and
GLM-TSL based on average cumulative regret of 5 repeated simulations.
A.2 Simulations to Validate the Lipschitzness of Hyperparameter Configuration
Wealsoconductanothersimulationtoshowitisreasonableandfairtoassumetheexpectedrewardisanalmost-
stationary Lipschitz function w.r.t. hyperparameter values. Specifically, we set d= 6,T= 3000,K= 60,
and for each time we run LinUCB and LinTS by using our CDT framework, but also obtain the results
by choosing the exploration hyperparameter in the set {0.3,0.45,0.6,..., 8.85,9}respectively. For the first
200rounds we use the random selection for sufficient exploration, and hence we omit the results for the
first 200rounds. After the warming-up period, we divide the rest of iterations into 140groups uniformly,
where each group contains 20consecutive iterations. Then we calculate the mean of the obtained reward
of each hyperparameter value in the adjacent 20rounds, and centralize the mean reward across different
hyperparameters in each group (we call it group mean reward). Afterward, we can calculate the mean and
standard deviation of the group mean reward for different hyperparameter values across all groups. The
results are shown in Figure 3, where we can see the group mean reward can be decently represented by a
stationary Lipschitz continuous function w.r.t hyperparameter values. Conclusively, we could formulate the
hyperparameter optimization problem as a stationary Lipschitz bandit after sufficient exploration in the long
run. And in the very beginning we can safely believe there is also only finite number of change points. This
fact firmly authenticates our problem setting and assumptions.
A.3 Simulations for Algorithm 1
We also conduct empirical studies to evaluate our proposed Zooming TS algorithm with Restarts (Algorithm
1) in practice. Here we generate the dataset under the switching environment, and abruptly change the
underlying mean function for several times within the time horizon T. The methods used for comparison as
well as the simulation setting are elaborated as follows:
Methods. We compare our Algorithm 1 (we call it Zooming TS-R for abbreviation) with two contenders:
(1) Zooming algorithm (Kleinberg et al., 2019): this algorithm is designed for the static Lipschitz bandit,
and would fail in theory under the switching environment; (2) Oracle: we assume this algorithm knows the
exact time for all switching points, and would renew the Zooming algorithm when reaching a new stationary
environment. Although this algorithm could naturally perform well, but it is infeasible in reality. Therefore,
16Published in Transactions on Machine Learning Research (04/2024)
0 1 3 5
Iterations0.04
0.02
0.000.020.040.06Centralized mean regret 1.5Simulations for LinUCB
0 1 3 5
Iterations0.04
0.02
0.000.020.040.06Centralized mean regret 1.05Simulations for LinTS
Figure 3: Average cumulative regret and its standard deviation of group mean reward for different hyperpa-
rameter values across all groups.
we would just use Oracle as a skyline here, and a direct comparison between Oracle and our Algorithm 1 is
inappropriate.
Settings. Assume the set of arm is [0,1]. The unknown mean function ft(x)is cho-
sen from two classes of reward functions with different smoothness around their maximum:
(1){0.9−0.9|x−a|,x∈[0,1] :a= 0.05,0.25,0.45,0.70,0.95}(triangle function); (2)/braceleftbig2
3πsin/parenleftbig3π
2(x−a+1
3)/parenrightbig
,x∈[0,1] :a= 0.05,0.25,0.45,0.70,0.95/bracerightbig
. We setT= 90,000andc(T) = 3, and
choose the location of changing points at random in the very beginning. The random noise is generated
according to N(0,0.1). The value of epoch size His set as suggested by our theory H= 10⌈(T/c(T))3/4⌉.
For each class of reward functions, we run the simulations for 20times and report the average cumulative
regret as well as the standard deviation for each contender in Figure 4. (The change points are fixed for each
repetition to make the average value meaningful.)
Figure 4 shows the performance comparisons of three different methods under the switching environment
measured by the average cumulative regret. We can see that Oracle is undoubtedly the best since it knows the
exact times for all change points and hence restart our Zooming TS algorithm accordingly. The traditional
Zooming algorithm ranks the last w.r.t both mean and standard deviation since it doesn’t take the non-
stationarity issue into account at all, and would definitely fail when the environment changes. This fact
also coincides with our expectation precisely. Our proposed algorithm has an obvious advantage over the
traditional Zooming algorithm when the change points exist, and we can see that our algorithm could adapt
to the environment change quickly and smoothly.
A.4 Additional Details and Results for Section 5
A.4.1 Baselines with A Large Candidate Set
To further make a fair comparison and validate the high superiority of our proposed CDT framework over
the existing OP, TL (or Syndicated) which relies on a user-defined hyperparameter candidate set, we explore
whether CDT will consistently outperform if baselines are running with a large tuning set. Here we replace the
original tuning set C1={0.1,1,2,3,4,5}with a finer set C2={0.1,0.25,0.5,0.75,1,1.5,2,2.5,3,3.5,4,4.5,5}.
And the new results are shown in the following Table 4 (original results in Section 5 are in gray).
Therefore, we can observe that the performance overall becomes worse under C2compared with the original
C1. In other words, adding lots of elements to the tuning set will not help improve the performance of existing
algorithms. We believe this is because the theoretical regret bound of TL (Syndicated) also depends on the
number of candidates kin terms of√
k(Ding et al., 2022b). There is no theoretical guarantee for OP. After
introducing so many redundant values in the candidate set, the TL (Syndicated) and OP algorithms would
get disturbed and waste lots of concentration on those unnecessary candidates.
17Published in Transactions on Machine Learning Research (04/2024)
Candidate Set C1 C2
Algorithm Setting TL/Syndicated OP TL/Syndicated OP
Simulations 343.14 383.62 356.23 389.91LinUCBMovielens 346.16 390.10 359.10 408.67
Simulations 828.41 869.30 874.34 925.29LinTSMovielens 519.09 666.35 516.62 667.77
Simulations 271.45 350.85 298.68 367.97UCB-GLMMovielens 381.00 397.58 406.29 412.62
Simulations 433.27 445.43 448.21 458.71GLM-TSLMovielens 446.74 678.91 458.23 718.46
Simulations 510.03 568.81 530.29 567.10Laplace-TSMovielens 949.51 1063.92 958.10 1009.23
Simulations 406.28 417.30 414.82 427.05GLOCMovielens 571.36 513.90 568.91 520.72
Simulations 448.29 551.63 458.09 557.04SGD-TSMovielens 1016.72 1084.13 1038.94 1073.91
Table 4: Cumulative regrets of baselines under different hyperparameter tuning sets.
In conclusion, we believe the existing algorithms relying on user-tuned candidate sets would perform well
if the size of the candidate set is reasonable and the candidate set contains some value very close to the
optimal hyperparameter value. However, in practice, finding the unknown optimal hyperparameter value is
a black-box problem, and it’s impossible to construct a candidate set satisfying the above requirements at
the beginning. If we discretize the interval finely, then the large size of the candidate set would hurt the
performance as well. On the other hand, our proposed CDT could adaptively “zoom in” on the regions
containing this optimal hyperparameter value automatically, without the need of pre-specifying a “good” set
of hyperparameters. And CDT could always yield robust results according to the extensive experiments we
did in Section 5.
On the other hand, these results also imply an interesting fact. Note it is doable to first discretize the
continuous space and then implement an algorithm with discrete candidate sets, such as Syndicated (Ding
et al., 2022b). However, we observe that finely discretizing the hyperparameter space will significantly hurt
the practical performance and hence is wasteful and inefficient. Intuitively, it is inefficient to place lots
of “probes” in other regions that do not contain the optimal point, and we should place probes in more
promising regions via adaptive discretization methodology. In theory, the uniform discretization idea will
lead to regret bound of order Td+1
d+2with covering dimension dand the zooming idea will incur Tdz+1
dz+2regret
with zooming dimension dz, and we know dz≤danddzcould be significantly smaller than dunder various
cases. Therefore, we believe the same phenomena will occur in the non-stationary Lipschitz bandits and also
our hyperparameter tuning framework as well.
A.4.2 Ablation Study on the Choice of T1andT2
ForT1, we set it to T2/(p+3)wherepstands for the number of hyperparameters according to Theorem 4.2.
Specifically, for LinUCB, LinTS, UCB-GLM, GLM-TSL and Laplace-TS, we choose it to be 118. For GLOC
and SGD-TS, we set it as 45. Here we also rerun our experiments in Section 5 with T1= 0(no warm-up)
since we believe a long warm-up period will abandon lots of useful information, and then we report the results
after this change:
We can observe that the results are almost identical from Table 5. For T2, Theorem 4.2 suggests that
T2=O/parenleftbig
T(p+2)/(p+3)/parenrightbig
. In our original experiments, we choose T2= 3T(p+2)/(p+3). To take an ablation study
onT2we takeT2=kT(p+2)/(p+3)fork= 1,2,3in each experiment, and to see whether our CDT framework
is robust to the choice of k.
18Published in Transactions on Machine Learning Research (04/2024)
Algorithm Setting T1= 0T1=T2/(p+3)
LinUCBSimulation 298.28 303.14
Movielens 313.29 307.19
LinTSSimulation 677.03 669.45
Movielens 343.18 340.85
UCB-GLMSimulation 299.74 300.54
Movielens 314.41 311.72
GLM-TSLSimulation 339.49 333.07
Movielens 428.82 432.47
Laplace-TSSimulation 520.29 520.35
Movielens 903.16 900.10
GLOCSimulation 414.70 418.05
Movielens 455.39 461.78
SGD-TSSimulation 430.05 425.98
Movielens 843.91 838.06
Table 5: Ablation study on the role of T1in our CDT framework.
Algorithm Setting k= 1k= 2k= 3
Simulation 328.28 300.62 298.28LinUCBMovielens 310.06 303.10 313.29
Simulation 717.77 670.90 677.03LinTSMovielens 360.12 352.19 343.18
Simulation 314.01 316.95 299.74UCB-GLMMovielens 347.92 325.58 314.41
Simulation 320.21 331.43 339.49GLM-TSLMovielens 439.98 428.91 428.82
Simulation 565.15 540.61 520.29Laplace-TSMovielens 948.10 891.91 903.16
Simulation 417.05 414.70 415.05GLOCMovielens 441.85 455.39 462.24
Simulation 450.14 430.05 414.57SGD-TSMovielens 852.98 843.91 830.35
Table 6: Ablation study on the role of T2in our CDT framework.
According to Table 6, we can observe that overall k= 2andk= 3perform better than k= 1. We believe it
is because, in the long run, the optimal hyperparameter would tend to be stable, and hence some restarts
are unnecessary and inefficient. Note by choosing k= 1our proposed CDT still outperforms the existing
TL and OP tuning algorithms overall. For k= 2andk= 3, we can observe that their performances are
comparable, which implies that the choice of kis quite robust in practice. We believe it is due to the fact
that our proposed Zooming TS algorithm could always adaptively approximate the optimal point. Although
it is unknown which one is better in practice under different cases, our comprehensive simulations show
that choosing either one in practice will work well and outperform all the existing methods. In conclusion,
these results suggest that we have a universal way to set the values of T1andT2according to the theoretical
bounds, and we do not need to tune them for each particular dataset. In other words, the performance of our
CDT tuning framework is robust to the choice of T1,T2under different scenarios.
19Published in Transactions on Machine Learning Research (04/2024)
B Supportive Remarks
Remark B.1.(Justifications on assumptions) We further explain the motivations of the Lipschitzness and
piecewise stationarity assumptions of the expected reward function for hyperparameter tuning of bandit
algorithms.
For Lipschitzness, we get the motivation of our formulation shown in Eqn. 3 and Eqn. 4 from the
hyperparameter tuning work on the offline machine learning algorithms. Specifically, Bayesian optimization
is widely considered as the state-of-the-art and most popular hyperparameter tuning method, which assumes
that the underlying function is sampled from a Gaussian process in the given space. By selecting a value xin
the space and obtaining the corresponding reward, Bayesian optimization could update its estimation of the
underlying function, especially in the neighbor of xsequentially. And it also relies on a user-defined kernel
function, whose selection is also purely empirical and lacks theoretical support. In our work, we use a similar
idea as Bayesian optimization: close hyperparameters tend to yield similar values with other conditions fixed.
And this natural extension motivates the Lipschitz assumption made in our paper. Therefore, it is fair to
make a similar and analogous assumption (close hyperparameters yield similar results given other conditions
fixed) for the hyperparameter tuning of bandit algorithms in our work. We validate this assumption using a
suite of simulations in Appendix A.
For the piecewise stationarity, as we mention in Section 3, it is inappropriate to assume the strict stationarity
of the bandit algorithm performance under the same hyperparameter value setting across time T. As an
example, for most UCB and TS-based bandit algorithms (e.g. LinUCB, LinTS, UCB-GLM, GLM-UCB,
GLM-TSL, etc.), the exploration degree of an arm is a multiplier of the exploration rate and the uncertainty
of an arm. In the beginning, a moderate value of the exploration rate may lead to a large exploration degree
for the arm since the uncertainty is large. On the contrary, in the long run, a moderate value of exploration
rate will lead to a minor exploration degree for the arm since its value has been well estimated with small
uncertainty. Therefore, a fixed hyperparameter setting may suggest different results across different stages
of time, and hence it is unreasonable to expect the strong stationarity of the hyperparameter tuning for
bandit algorithms at all time steps. On the other hand, it would be very inefficient to assume a completely
non-stationary environment as in Ding et al. (2022b) which uses EXP3. In very close time steps, we could
anticipate that the same hyperparameter setting would yield a very similar result in expectation since the
uncertainty of any arm would be close. And using a non-stationary environment will totally waste this
information and hence is inefficient. Therefore, it is very well motivated to use a partial non-stationarity
assumption that lies in the middle ground between the above two extremes. Note our proposed tuning
method yields very promising results in extensive experiments under our formulations. And the stationary
environment can be regarded as a special case of our switching environment setting where the functions in
between all change points are the same.
Finally, we will explain why it is excessively difficult to present theoretical validation regarding these
assumptions in our paper. As we mentioned, our formulation is motivated by Bayesian optimization, arguably
the most popular method for hyperparameter tuning for offline machine learning algorithms. And we use a
similar idea: similar hyperparameters tend to yield similar values while other conditions are fixed. However,
people could hardly provide any theory backing for the analogous assumption of Bayesian optimization
for any offline machine learning algorithms (e.g. regression, classification), and hyperparameter tuning is
widely considered as a black-box problem for offline machine learning algorithms. Not to mention that the
theoretical analysis of hyperparameter tuning for any bandit algorithm is much more challenging than that of
offline machine learning algorithms since historical observations along with hyperparameter values will affect
the online selection simultaneously for the bandit algorithms, and we can use different hyperparameters in
different rounds for bandit algorithms. Conclusively, our formulation is natural and well-motivated.
C Detailed Proof on the Zooming Dimension
In the beginning, we would reload some notations for simplicity. Here we could omit the time subscript (or
superscript) tsince the following result could be identically proved for each round t. Assume the Lipschitz
functionfis defined on Rpc, andv∗:=arg maxv∈Af(v)denotes the maximal point (w.l.o.g. assume it’s
20Published in Transactions on Machine Learning Research (04/2024)
unique), and ∆(v) =f(v∗)−f(v)is the “badness” of the arm v. We then naturally denote Aras the
r-optimal region at the scale r∈(0,1], i.e.Ar={v∈A:r/2<∆(v)≤r}. Ther-zooming number could be
denoted as Nz(r). And the zooming dimension could be naturally denoted as pz. Note that by the Assouad’s
embedding theorem, any compact doubling metric space (A,Dist(·,·))can be embedded into the Euclidean
space with some type of metric. Therefore, for all compact doubling metric spaces with cover dimension pc,
it is sufficient to study on the metric space ([0,1]pc,∥·∥l)for somel∈(0,+∞]instead.
We will rigorously prove the following two facts regarding the r-zooming number Nz(r)of(A,f)for arbitrary
compact set A⊆Rpcand Lipschitz function f(·)defined onA:
•0≤pz≤pc.
•The zooming dimension could be much smaller than pcunder some mild conditions. For example,
if the payoff function fis greater than∥v∗−v∥βin scale in a (non-trivial) neighborhood of v∗for
someβ≥1, i.e.f(v∗)−f(v)≥C(∥v∗−v∥β)as∥v∗−v∥≤rfor someC > 0andr= Θ(1), then it
holds thatpz≤(1−1/β)pc. Noteβ= 2when we have f(·)isC2-smooth and strongly concave in a
neighborhood of v∗, which subsequently implies that pz≤pc/2.
Proof.Due to the compactness of A, it suffices to prove the results when A= [0,1]pc. By the definition of
the zooming dimension pz, it naturally holds that pz≥0. On the other side, since the space Ais a closed
and bounded set in Rpc, we assume the radius of Ais no more than S, which consequently implies that the
r/16-covering number of Ais at most the order of
/parenleftbiggS
r
16/parenrightbiggpc
= (16S)pc·r−pc.
Since we know Ar⊆A, it holds that pz≤p. Secondly, if the payoff function fis locally greater than
∥v∗−v∥βin scale for some β≥1, i.e.f(v∗)−f(v)≥C(∥v∗−v∥β), then there exists C∈Randδ>0such
that as long as C∥v−v∗∥β≤δwe havef(v∗)−f(v)≥C∥v−v∗∥β. Therefore, for 0<r<δ, it holds that,
{v:r≥f(v∗)−f(v)>r/2}⊆{v:C∥v−v∗∥β≤r}=/braceleftigg
v:∥v−v∗∥≤/parenleftigr
C/parenrightig1
β/bracerightigg
It holds that the r-covering number of the Euclidean ball with center v∗and radius (r/c)(1/β)is of the order
of
/parenleftbigr
C/parenrightbig1
β
r
16
pc
=/parenleftbigg16
C1
β/parenrightbiggpc
·r−(1−1
β)pc
which explicitly implies that pz≤(1−1/β)pc.
D Intuition of our Thompson Sampling update
Intuitively, we consider a Gaussian likelihood function and Gaussian conjugate prior to design our Thompson
Sampling version of zooming algorithm, and here we would ignore the clipping step for explanation. Suppose
the likelihood of reward ˜ytat timet, given the mean of reward I(vt)for our pulled arm vt, follows a Gaussian
distribution N(I(vt),s2
0). Then, if the prior of I(vt)at timetis given byN(ˆft(vt),s2
0/nt(vt)), we could easily
compute the posterior distribution at time t+ 1,
Pr(I(vt)|˜yt)∝Pr(˜yt|I(vt))Pr(I(vt)),
asN(ˆft+1(vt),s2
0/nt+1(vt)). We can see this result coincides with our design in Algorithm 1 and its proof is
as follows:
21Published in Transactions on Machine Learning Research (04/2024)
Figure 4: Cumulative regret plots of Zooming TS-R, Zooming and Oracle algorithms under the switching
environment.
20000 40000 60000 80000
Iterations0100020003000400050006000Cumulative RegretSimulations for triangle functions
Zooming
Zooming TS-R
Oracle
20000 40000 60000 80000
Iterations05001000150020002500Cumulative RegretSimulations for sine functions
Zooming
Zooming TS-R
Oracle
Proof.
Pr(I(vt)|˜yt)∝Pr(˜yt|I(vt))Pr(I(vt))
∝exp/braceleftbigg
−1
2s2
0[(I(vt)−˜yt)2+nt(vt)(I(vt)−ft(vt))2]/bracerightbigg
∝exp/braceleftbigg
−1
2s2
0[(nt(vt) + 1)I(vt)2−2(˜yt+nt(vt)ft(vt))I(vt)]/bracerightbigg
∝exp/braceleftbigg
−nt+1(vt)
2s2
0/bracketleftbigg
I(vt)2−2(˜yt+nt(vt)ft(vt))
nt+1(vt)I(vt)/bracketrightbigg/bracerightbigg
∝exp/braceleftbigg
−nt+1(vt)
2s2
0(I(vt)−ft+1(vt))2/bracerightbigg
Therefore, the posterior distribution of I(vt)at timet+ 1isN(ft+1(vt),s2
01
nt+1(vt)).
This gives us an intuitive explanation why our Zooming TS algorithm works well when we ignore the clipped
distribution step. And we have stated that this clipping step is inevitable in Lipschitz bandit setting in
our main paper since (1) we’d like to avoid underestimation of good active arms, i.e. avoid the case when
their posterior samples are too small. (2) We could at most adaptively zoom in the regions which contains
v∗instead of exactly detecting v∗, and this inevitable loss could be mitigated by setting a lower bound for
TS posterior samples. Note that although the intuition of our Zooming TS algorithm comes from the case
where contextual bandit rewards follow a Gaussian distribution, we also prove that our algorithm can achieve
a decent regret bound under the switching environment and the optimal instance-dependent regret bound
under the stationary Lipschitz bandit setting.
E Proof of Theorem 4.1
E.1 Stationary Environment Case
To prove Theorem 4.1, we will first focus on the stationary case, where ft:=f,∀t∈[T]. When the
environment is stationary, we could omit the subscript (or superscript) tin some notations as in Section C
for simplicity: Assume the Lipschitz function is f, andv∗:=arg maxv∈Af(v)denotes the maximal point
(w.l.o.g. assume it’s unique), and ∆(v) =f(v∗)−f(v)is the “badness” of the arm v. We then naturally
denoteAras ther-optimal region at the scale r∈(0,1], i.e.Ar={v∈A:r/2<∆(v)≤r}. Ther-zooming
number could be denoted as Nz(r). And the zooming dimension could be naturally denoted as pz. Note we
could omit the subscript (or superscript) tfor the notations just mentioned above since all these values would
be fixed through all rounds under the stationary environment.
22Published in Transactions on Machine Learning Research (04/2024)
E.1.1 Useful Lemmas and Corollaries
Recall that ˆft(v)is the average observed reward for arm v∈Aby timet. And we call all the observations
(pulled arms and observed rewards) over Ttotal rounds as a process.
Definition E.1. We call it a clean process, if for each time t∈[T]and each strategy v∈Athat has been
played at least once at any time t, we have|ˆft(v)−f(v)|≤rt(v).
Lemma E.2. The probability that, a process is clean, is at least 1−1/T.
Proof.Fix some arm v. Recall that each time an algorithm plays arm v, the reward is sampled IID from
some distribution Pv. Define random variables Uv,sfor1≤s≤Tas follows: for s≤nT(v),Uv,sis the
reward from the s-th time arm vis played, and for s>nT(v)it is an independent sample from Pv. For each
k≤Twe can apply Chernoff bounds to {Uv,s: 1≤s≤k}and obtain that:
Pr/parenleftigg/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle1
kk/summationdisplay
s=1Uv,s−f(v)/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle≥/radicalbigg
13τ2
0lnT
2k/parenrightigg
≤2·exp/parenleftbigg
−k
2τ2
013τ2
0lnT
2k/parenrightbigg
= 2 exp/parenleftbigg13
4lnT/parenrightbigg
= 2T−3.25≤T−3, (7)
since we can trivially assume that T≥16. LetNbe the number of arms activated all over rounds T; note
thatN≤T. DefineX-valued random variables {xi}T
i=1as follows: xjis the min(j,N)-th arm activated by
timeT. For anyx∈Aandj≤T, the event{x=xj}is independent of the random variables {Ux,s}: the
former event depends only on payoffs observed before xis activated, while the latter set of random variables
has no dependence on payoffs of arms other than x. Therefore, Eqn. equation 7 is still valid if we replace the
probability on the left side with conditional probability, conditioned on the event {x=xj}. Taking the union
bound over all k≤T, it follows that:
Pr(∀t≤T,|f(v)−ˆft(v)|≤rt(v)|xj=v)≥1−T−2,∀v∈A,j∈[T],
Integrating over all arms vwe get
Pr(∀t≤T,|f(xj)−ˆft(xj)|≤rt(xj))≥1−T−2,∀j∈[T].
Finally, we take the union bound over all j≤T, and it holds that,
Pr(∀t≤T,j≤T,|f(xj)−ˆft(xj)|≤rt(xj))≥1−T−1,
and this obviously implies the result.
Lemma E.3. If it is a clean process, then B(v,rt(v))could never be eliminated from Algorithm 1 for any
t∈[T]and armvthat is active at round t, given that v∗∈B(v,rt(v)).
Proof.Recall that from Algorithm 1, at round tthe ballB(u,rt(u))would be permanently removed if we
have for some active arm vs.t.
ˆft(v)−rt(v)>ˆft(u) + 2rt(u).
If we have that v∗= arg maxx∈Af(x)∈B(u,rt(u)), then it holds that
ˆft(u) + 2rt(u)≥f(u) +rt(u)≥f(u) +Dist(u,v∗)≥f(v∗),
where the first inequality is due to the clean process and the last one comes from the fact that fis a Lipschitz
function. On the other hand, we have that for any active arm v,
f(v)≥ˆft(v)−rt(v), f(v∗)≥f(v).
Therefore, it holds that
ˆft(v)−rt(v)≤ˆft(u) + 2rt(u).
And this inequality concludes our proof.
23Published in Transactions on Machine Learning Research (04/2024)
Lemma E.4. If it is a clean process, then for any time tand any active strategy vthat has been played at
least once before time twe have ∆(v)≤5E[rt(v)]. Furthermore, it holds that E(nt(v))≤O(ln (T)/∆(v)2).
Proof.LetStbe the set of all arms that are active at time t. Suppose an arm vtis played at time tand was
previously played at least twice before time t. Firstly, We would claim that
f(v∗)≤It(vt)≤f(vt) + 3rt(vt)
holds uniformly for all twith probability at least 1−δ, which directly implies that ∆(vt)≤3rt(vt)with high
probability uniformly. First we show that It(vt)≥f(v∗). Indeed, recall that all arms are covered at time t,
so there exists an active arm v∗
tthat covers v∗, meaning that v∗is contained in the confidence ball of v∗
t.
And based on Lemma E.3 the confidence ball containing v∗could never be eliminated at round twhen it’s a
clean process. Recall Zt,vis the i.i.d. standard normal random variable used for any arm vin roundt(Eqn.
equation 6). Since arm vtwas chosen over v∗
t, we haveIt(vt)≥It(v∗
t). Since this is a clean process, it follows
that
It(v∗
t) =ˆft(v∗
t) +s0/radicaligg
1
nt(v∗
t)Zt,v∗
t≥f(v∗
t) +s0/radicaligg
1
nt(v∗
t)Zt,v∗
t−rt(v∗
t) (8)
Furthermore, according to the Lipschitz property we have
f(v∗
t)≥f(v∗)−Dist(v∗
t,v∗)≥f(v∗)−rt(v∗
t). (9)
Combine Eqn. equation 8 and equation 9, we have
It(vt)≥It(v∗
t)≥f(v∗) +s0/radicaligg
1
nt(v∗
t)Zt,v∗
t−2rt(v∗
t)
=f(v∗) +/radicaligg
52πτ2
0ln (T)
nt(v∗
t)/parenleftbigg
Zt,v∗
t−1√
2π/parenrightbigg
≥f(v∗), (10)
where we get the last inequality since we truncate the random variable Zt,v∗
tby the lower bound 1/√
2π
according to the definition. On the other hand, we have
It(vt)≤f(vt) +rt(vt) +s0/radicaligg
1
nt(vt)Zt,vt=f(vt) +/parenleftig
1 + 2√
2πZt,vt/parenrightig
rt(vt) (11)
Therefore, by combining Eqn. equation 10 and equation 11 we have that
∆(vt)≤/parenleftig
1 + 2√
2πZt,vt/parenrightig
rt(vt). (12)
And we know that Zt,:is defined as Zt,:=max{1/√
2π,˜Zt,:}where ˜Zt,:is IID drawn from standard normal
distribution. In other words, Zt,vtfollows a clipped normal distribution with the following PDF:
f(x) =

ϕ(x) + (1−Φ(x))δ/parenleftig
x−1√
2π/parenrightig
, x≥1√
2π;
0, x<1√
2π;
Hereϕ(·)andΦ(·)denote the PDF and CDF of standard normal distribution. And we have
E(Zt,vt)≤1√
2π+/integraldisplay+∞
1√
2πxϕ(x)dx≤1√
2π+1√
2πe−1
4π≤/radicalbigg
2
π
By taking expectation on Eqn. equation 12, we have ∆(vt)≤5E(rt(vt)). Next, we would show that
E(nt(vt))≤O(ln (T))/∆(vt)2. Based on Eqn. equation 11 and the definition of rt(·), we could deduce that
/radicalbig
nt(vt)≤/radicalbigg
13
2τ2
0ln (T)(1 + 2√
2πZt,vt)1
∆(vt),
24Published in Transactions on Machine Learning Research (04/2024)
which thus implies that
nt(vt)≤13
2τ2
0ln (T)(1 + 2√
2πZt,vt)21
∆(vt)2=O(ln (T))(1 + 2√
2πZt,vt)21
∆(vt)2. (13)
By simple calculation, we could show that
E(Z2
t,vt)≤1
2π+/integraldisplay+∞
1√
2πx2ϕ(x)dx≤1
π+1
2≤1
⇒E/bracketleftig
(1 + 2√
2πZt,vt)2/bracketrightig
≤1 + 4√
2π/radicalbigg
2
π+ 8π<+∞.
After revisiting Eqn. equation 13, we can show that E(nt(vt))≤O(ln (T))/∆(vt)2. Now suppose arm vis
only played once at time t, thenrt(v)>1and thus the lemma naturally holds. Otherwise, let sbe the last
time armvhas been played according to the selection rule, where we have rt(v) =rs(v), and then based on
Eqn. equation 11 it holds that
It(v)≤f(v) +/parenleftig
1 + 2√
2πZs,v/parenrightig
rt(v).
And then we could show that ∆(v)≤5E(rt(v)). By using an identical argument as before, we could show
thatE(nt(v))≤O(ln (T))/∆(v)2.
Lemma E.5. LetX1,...,Xnbe independent σ2-sub-Gaussian random variables. Then for every t>0,
P/parenleftbigg
max
1,≤,nXi≥/radicalbig
2σ2(ln (T) +t)/parenrightbigg
≤e−t.
Proof.Letu=/radicalbig
2σ2(ln (n) +t), we have
P/parenleftbigg
max
1,≤,nXi≥u/parenrightbigg
=P(∃i,Xi≥u)≤n/summationdisplay
i=1P(Xi≥u)≤ne−u2
2σ2=e−t.
E.1.2 Proof of Theorem 4.1 under stationary environment
Proof.By Lemma E.2 we know that it is a clean process with probability at least 1−1
T. In other words,
denote the event Ω:={clean process}, and then we have that P(Ω)≥1−1
T. And according to Lemma E.3
we’re aware that the active confidence balls containing the best arm can’t be removed in a clean process.
Remember that we use STas the set of all arms that are active in the end, and denote
Bi,T=/braceleftbigg
v∈ST: 2i≤1
∆(v)<2i+1/bracerightbigg
,whereST=+∞/uniondisplay
i=0Bi,T,
wherei≥0. Then, under the event Ω, by using Corollary E.4 we have E(nT(v)|Ω)≤O(lnT)/∆(v)2, and
hence it holds that
/summationdisplay
v∈Bi,T∆(v)E(nT(v)|Ω)≤O(lnT)/summationdisplay
v∈Bi,t1
∆(v)≤O(lnT)·2i|Bi,t|
Denoteri= 2−i, we have
/summationdisplay
v∈Bi,T∆(v)E(nT(v)|Ω)≤O(lnT)·1
ri|Bi,t|
25Published in Transactions on Machine Learning Research (04/2024)
Next, we would show that for any active arms u,vwe have
Dist(u,v)>1
4/radicalbig
2πln (T)min{∆(u),∆(v)} (14)
with probability at least 1−1
T. W.l.o.g assume uhas been activated before v. Letsbe the time when vhas
been activated. Then by the philosophy of our algorithm we have that Dist(u,v)>rs(v). Then according to
Eqn. equation 12 in the proof Lemma E.4, it holds that rs(v)≥1
2√
2πZ∆(v)for some random variable Z
following the clipped standard normal distribution. Define the event Υ ={Zt,vt<2/radicalbig
ln (T)for allt∈[T]},
then based on Lemma E.5 we have P(Υ)≥1−1
T. Then under the event Υ, we havers(v)≥1
4√
2πln (T)∆(v),
which then implies that Eqn. equation 14 holds under Υ. Since for arbitrary x,y∈Bi,Twe have
ri
2<∆(x)≤ri,ri
2<∆(y)≤ri,
which implies that under the event Υ
Dist(x,y)>1
4/radicalbig
2πln (T)min{∆(x),∆(y)}>ri
8/radicalbig
2πln (T).
Therefore, xandyshould belong to different sets of (ri/8/radicalbig
2πln (T))-diameter-covering. It follows that
|Bi,T|≤Nz(ri/8/radicalbig
2πln (T))≤O(ln(T)p)crpz
i≤˜O(crpz
i). RecallNz(r)is defined as the minimal number of
balls of radius no more than rrequired to cover Ar. As a result, under the events ΩandΥ, it holds that
/summationdisplay
v∈Bi,T∆(v)E(nT(v)|Ω∩Υ)≤O(lnT)·1
riNz(ri) (15)
Therefore, based on Eqn. equation 15, we have
RL(T) =/summationdisplay
v∈ST∆(v)E(nT(v))
=P(Ω∩Υ)/summationdisplay
v∈ST∆(v)E(nT(v)|Ω∩Υ) +P(Ωc∪Υc)/summationdisplay
v∈ST∆(v)E(nT(v)|Ωc∪Υc)
≤/summationdisplay
v∈ST:∆(v)≤ρ∆(v)E(nT(v)|Ω∩Υ) +/summationdisplay
v∈ST:∆(v)>ρ∆(v)E(nT(v)|Ω∩Υ) +2
T·T
≤ρT+/summationdisplay
i<log2(1
ρ)1
ri˜O(cr−pz
i) + 2
≤ρT+˜O(1)/summationdisplay
i<log2(1
ρ)1
ricr−pz
i+ 2
≤ρT+˜O(1)⌊log1/22ρ⌋/summationdisplay
k=0c2k(pz+1)+ 2
≤ρT+˜O(1)·2·2⌊log1/22ρ⌋(pZ+1)+ 2
≤ρT+˜O(1)/parenleftbigg1
2ρ/parenrightbiggpz+1
+ 2
By choosing ρin the scale of
ρ=˜O/parenleftbigg1
T/parenrightbigg1
pz+2
,
it holds that
RL(T) =˜O/parenleftbigg
Tpz+1
pz+2/parenrightbigg
.
26Published in Transactions on Machine Learning Research (04/2024)
E.2 Switching (Non-stationary) Environment Case
Since there are c(T)change points for the environment Lipschitz functions ft(·), i.e.
T−1/summationdisplay
t=11[∃m∈A:ft(m)̸=ft+1(m)] =c(T).
Given the length of epochs as H, we would have ⌈T/H⌉epochs overall. And we know that among these
⌈T/H⌉different epochs, at most c(T)of them contain the change points. For the rest of epochs that are free
of change points, the cumulative regret could be bounded by the result we just deduced for the stationary
case above. And the cumulative regret in any epoch with stationary environment could be bounded as
H(pz,∗+1)/(pz,∗+2). Specifically, we could partition the Trounds into m=⌈T/H⌉epochs:
[T1+ 1,T] = [ω0=T1+ 1,ω1)∪[ω1,ω2)∪···∪ [ωm−1,ωm=T+ 1),
whereωi+1=ωi+Hfori= 0,...,m−2. Denote all the change points as T1≤ρ1<···<ρc(T)≤T, and
then define
Ω ={∪[ωi,ωi+1) :ρj∈[ωi,ωi+1),∃j= 1,...c ;i= 0,...,m−1}.
Then it holds that |Ω|≤Hc(T). Therefore, it holds that
RL(T)≤˜O/parenleftigg
Hc(T) +/parenleftbiggT
H+ 1/parenrightbigg
Hpz,∗+1
pz,∗+2/parenrightigg
≤˜O/parenleftigg
Hc(T) +T
H·Hpz,∗+1
pz,∗+2/parenrightigg
,
where the first part bound the regret of non-stationary epochs and the second part bound that of stationary
ones. By taking H= (T/c(T))(pz,∗+2)/(pz,∗+3), it holds that
RL(T)≤˜O/parenleftigg
Tpz,∗+2
pz,∗+3c(T)1
pz,∗+3/parenrightigg
.
And this concludes our proof for Theorem 4.1.
F Algorithm 1 with unknown c(T)andpz,∗
F.1 Introduction of Algorithm 3
When both the number of change points c(T)over the total time horizon Tand the zooming dimension pz,∗
are unknown, we could adapt the BOB idea used in Cheung et al. (2019); Zhao et al. (2020) to choose the
optimal epoch size Hbased on the EXP3 meta algorithm. In the following, we first describe how to use the
EXP3 algorithm to choose the epoch size dynamically even if c(T)andpz,∗are unknown. Then we present
the regret analysis in Theorem F.1 and its proof.
Although the zooming dimension pz,∗is unknown, it holds that pz,∗≤pc, and hence we could simply use the
upper bound of pz,∗(denoted as pu) aspcinstead (recall pcis the covering dimension). Note that the upper
boundpz,∗could be more specific when we have some prior knowledge of the reward Lipschitz function f(·):
for example, as we mentioned in Appendix C, if the function f(·)is known to be C2−smooth and strongly
concave in a neighborhood of its maximum defined in Rpc, it holds that pz,∗≤pc/2and then we could
usepu=pc/2as the upper bound. Note that we also use the BOB mechanism in the CDT framework for
hyperparameter tuning in Algorithm 2, where we treat the zooming TS algorithm with Restarts as the meta
algorithm to select the hyperparameter setting in the upper layer, and then use the selected configuration for
the bandit algorithm in the lower layer. However, here we would use BOB mechanism differently: we firstly
divide the total horizon Tinto several epochs of the same length H0(named top epoch), where in each top
epoch we would restart the Algorithm 1. And in the i−th top epoch the restarting length Hi(named bottom
epoch) of Algorithm 1 could be chosen from the set J={Ji:=⌈k⌉:k≥1,k=H0/2i−1,i= 1,2,...}, where
the chosen bottom epoch size could be adaptively tuned by using EXP3 as the meta algorithm. Here we
27Published in Transactions on Machine Learning Research (04/2024)
Figure 5: An illustration of Zooming TS algorithm with double restarts when c(T)is agnostic.
restart the zooming TS algorithm from two perspectives, where we first restart the zooming TS algorithm
with Restarts (Algorithm 1) in each top epoch of some fixed length H0, and then for each top epoch the
restarting length Hifor Algorithm 1 would be tuned on the fly based on the previous observations (Cheung
et al., 2019). Therefore, we would name this method Zooming TS algorithm with Double Restarts.
As for how to choose the bottom epoch size Hiin each top epoch of length H0, we implement a two-layer
framework: In the upper layer, we use the adversarial MAB algorithm EXP3 to pull the candidate from
J={Ji}. And then in the lower layer we use it as the bottom epoch size for Algorithm 1. When a top epoch
ends, we would update the components in EXP3 based on the rewards witnessed in this top epoch. The
illustration of this double restarted strategy is depicted in Figure 5. And the detailed procedure is shown in
Algorithm 3.
Theorem F.1. By using the (top) epoch size as H0=⌈T(pu+2)/(pu+4)⌉, the expected total regret of our
Zooming TS algorithm with Double Restarts (Algorithm 3) under the switching environment over time T
could be bounded as
RL(T)≤˜O/parenleftbigg
Tpu+2
pu+3·max/braceleftbigg
c(T)1
pu+3,T1
(pu+3)(pu+4)/bracerightbigg/parenrightbigg
.
Specifically, it holds that
RL(T)≤

Tpu+2
pu+3c(T)1
pu+3, c(T)≥T1
pu+4,
Tpu+3
pu+4, c (T)<T1
pu+4,
wherepu≤pcis the upper bound of pz,∗.
Therefore, we observe that if c(T)is large enough, we could obtain the same regret bound as in Theorem 4.1
givenpz,∗.
F.2 Proof of Theorem F.1
Proof.The proof of Theorem F.1 relies on the recent usage of the BOB framework that was firstly introduced
in Cheung et al. (2019) and then widely used in various bandit-based model selection work (Ding et al.,
2022a; Zhao et al., 2020). To be consistent we would use the notations in Algorithm 3 in this proof, and we
would also recall these notations here for readers’ convenience: for the i-th bottom epoch, we assume the
candidateHjiis pulled from the set Jin the beginning, where jiis the index of the pulled candidate. At
roundt, given the current bottom epoch length Hjifor somei, we pull the arm vt(Hji)∈Aand then collect
the stochastic reward Yt. We also define ci(T)as the number of change points during each top epoch, and
hence it naturally holds that/summationtext⌈T/H 0⌉
i=1ci(T) =c(T). Given these notations, the expected cumulative regret
28Published in Transactions on Machine Learning Research (04/2024)
could be decomposed into the following two parts:
RL(T) =E/bracketleftiggT/summationdisplay
t=1ft(v∗
t)−ft(vt)/bracketrightigg
=E
⌈T/H 0⌉/summationdisplay
i=1min{T,iH 0}/summationdisplay
t=(i−1)H0+1ft(v∗
t)−ft(vt(Hji))

=E
⌈T/H 0⌉/summationdisplay
i=1min{T,iH 0}/summationdisplay
t=(i−1)H0+1ft(v∗
t)−ft(vt(H∗))

/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
Quantity (I)
+E
⌈T/H 0⌉/summationdisplay
i=1min{T,iH 0}/summationdisplay
t=(i−1)H0+1ft(vt(H∗))−ft(vt(Hji))

/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
Quantity (II), (16)
whereH∗could be any restarting period in J, and we expect it could approximate the optimal choice
Hopt= (T/c(T))(pu+2)/(pu+3)in Theorem 4.1. (Here we replace pz,∗bypuin Theorem 4.1 since the
underlying pz,∗is mostly unspecified in reality.) According to the proof of Theorem 4.1 in Appendix G, the
Quantity (I) could be bounded as:
E
⌈T/H 0⌉/summationdisplay
i=1min{T,iH 0}/summationdisplay
t=(i−1)H0+1ft(v∗
t)−ft(vt(H∗))
≤⌈T/H 0⌉/summationdisplay
i=1H∗ci(T) +H0
H∗(H∗)pu+2
pu+4
=H∗c(T) +T(H∗)−1
pu+2
However, itisclearthateachcandidatein Jcouldatmostbethelengthoftopepochsize H0, whichwesettobe
⌈T(pu+2)/(pu+4)⌉, and hence it would be more challenging if the optimal choice Hopt= (T/c(T))(pu+2)/(pu+3)
is larger than H0. To deal with this issue, we bound the expected cumulative regret in two different cases
separately:
(1) IfHopt= (T/c(T))(pu+2)/(pu+3)≤H0, which is equivalent to
/parenleftbiggT
c(T)/parenrightbiggpu+2
pu+3
≤H0⇔/parenleftbiggT
c(T)/parenrightbiggpu+2
pu+3
≤Tpu+2
pu+4⇔c(T)≥T1
pu+4,
then we know that there exists some H+∈Jsuch thatH+≤(T/c(T))(pu+2)/(pu+3)≤2H+. By setting
H∗=H+, the Quantity (I) could be bounded as:
Quantity (I) =˜O/parenleftbigg
H+c(T) +T(H∗)−1
pu+2/parenrightbigg
=˜O/parenleftbigg
Hoptc(T) +T(Hopt)−1
pu+2/parenrightbigg
=˜O/parenleftbigg
Tpu+2
pu+3c(T)1
pu+3/parenrightbigg
.
For the Quantity (II), we could bound it based on the results in Auer et al. (2002b). Specifically, from
Corollary 3.2 in Auer et al. (2002b), the expected cumulative regret of EXP3 could be upper bounded by
2Q/radicalbig
(e−1)LKln(K), whereQis the maximum absolute sum of rewards in any epoch, Lis the number
of rounds and Kis the number of arms. Under our setting, we can set Q=H0,L=⌈T/H 0⌉and
K=|J|=O(ln(H0)). So we could bound Quantity (II) as:
E
⌈T/H 0⌉/summationdisplay
i=1min{T,iH 0}/summationdisplay
t=(i−1)H0+1ft(vt(H∗))−ft(vt(Hji))
≤2√
e−1H0/radicalbigg
T
H0|J|ln(|J|) =˜O(/radicalbig
TH0)
=˜O/parenleftbigg
Tpu+3
pu+4/parenrightbigg
=˜O/parenleftbigg
Tpu+2
pu+3T1
(pu+3)(pu+4)/parenrightbigg
=˜O/parenleftbigg
Tpu+2
pu+3c(T)1
pu+3/parenrightbigg
, (17)
29Published in Transactions on Machine Learning Research (04/2024)
Algorithm 3 Zooming TS algorithm with Double Restarts
Input:Time horizon T, spaceA, upper bound pu≤pc.
Initialization: the (top) epoch size H0=⌈T(pu+2)/(pu+4)⌉,N=⌈log2(H0)⌉+ 1,J={Hi=⌈H0/2i−1⌉}N
i=1.
1:Initialize the exponential weights wj(1) = 1forj= 1,...,|J|.
2:Initialize the exploration parameter for the EXP3 algorithm as α= min/braceleftig
1,/radicalig
|J|log(|J|)
(e−1)⌈T/H 0⌉/bracerightig
.
3:fori= 1to⌈T/H 0⌉do
4:Update probability distribution for selecting candidates in Jbased on EXP3 as:
pj(i) =α
|J|+ (1−α)wj(i)
/summationtext|J|
k=1wk(i), j= 1,...,|J|.
5:Pulljifrom{1,2,...,|J|}according to the probability distribution {pj(i)}|J|
j=1.
6:Run Zooming TS algorithm with Restarts using the (bottom) epoch size Hjifort= (i−1)H0+ 1to
min{T,iH 0}, and collect the pulled arm vt(Hji)and reward Ytat each iteration.
7:Update components in EXP3: rj(i) = 0for allj̸=ji;rj(i) =/summationtextmin{T,iH 0}
k=(i−1)H0+1Yk/pj(i)ifj=ji, and
then
wj(i+ 1) =wj(i) exp/parenleftbiggα
|J|rj(i)/parenrightbigg
, j= 1,...,|J|.
8:end for
where we have the last equality since we assume that c(T)≥T1/(pu+4). Therefore, we have finished the proof
for this case. (2) If Hopt= (T/c(T))(pu+2)/(pu+3)>H 0, which is equivalent to
/parenleftbiggT
c(T)/parenrightbiggpu+2
pu+3
>H 0⇔/parenleftbiggT
c(T)/parenrightbiggpu+2
pu+3
>Tpu+2
pu+4⇔c(T)<T1
pu+4,
then we know that Hoptis greater than all candidates in J, which means that we could not bound the
Quantity (I) based on the previous argument. By simply using H∗=H0, it holds that
Quantity (I) =˜O/parenleftigg
H0c(T) +T·H−1
pu+2
0/parenrightigg
=˜O/parenleftbigg
Tpu+3
pu+4/parenrightbigg
.
For Quantity (II), based on Eqn. equation 17, we have
Quantity (II) =˜O/parenleftbigg
Tpu+3
pu+4/parenrightbigg
.
Combining the case (1) and (2), it holds that
RL(T)≤

Tpu+2
pu+3c(T)1
pu+3, c(T)≥T1
pu+4,
Tpu+3
pu+4, c (T)<T1
pu+4.
And this concludes our proof.
G Analysis of Theorem 4.2
G.1 Additional Lemma
Lemma G.1 (Proposition 1 in Li et al. (2017)) .DefineVn+1=/summationtextn
t=1XtXT
t, whereXtis drawn IID from
some distribution in unit ball Bd. Furthermore, let Σ :=E[XtXT
t]be the second moment matrix, let B,δ2>0
be two positive constants. Then there exists positive, universal constants C1andC2such thatλmin(Vn+1)≥B
30Published in Transactions on Machine Learning Research (04/2024)
with probability at least 1−δ2, as long as
n≥/parenleftigg
C1√
d+C2/radicalbig
log(1/δ2)
λmin(Σ)/parenrightigg2
+2B
λmin(Σ).
Lemma G.2 (Theorem 2 in Abbasi-Yadkori et al. (2011)) .For anyδ <1, under our problem setting in
Section 3, it holds that for all t>0,
/vextenddouble/vextenddouble/vextenddoubleˆθt−θ∗/vextenddouble/vextenddouble/vextenddouble
Vt≤βt(δ),
∀x∈Rd,|x⊤(ˆθt−θ∗)|≤∥x∥v−1
tβt(δ),
with probability at least 1−δ, where
βt(δ) =σ/radicaligg
log/parenleftbigg(λ+t)d
δ2λd/parenrightbigg
+√
λS.
In this subsection we denote α∗(δ):=βT(δ).
Lemma G.3 (Filippi et al. (2010)) .Letλ>0, and{xi}t
i=1be a sequence in Rdwith∥xi∥≤1, then we have
t/summationdisplay
s=1∥xs∥2
V−1
s≤2 log/parenleftbiggdet(Vt+1)
det(λI)/parenrightbigg
≤2dlog/parenleftbigg
1 +t
λ/parenrightbigg
,
t/summationdisplay
s=1∥xs∥V−1
s≤/radicaltp/radicalvertex/radicalvertex/radicalbtT/parenleftiggt/summationdisplay
s=1∥xs∥2
V−1
s/parenrightigg
≤/radicaligg
2dtlog/parenleftbigg
1 +t
λ/parenrightbigg
.
Lemma G.4 (Agrawal & Goyal (2013)) .For a Gaussian random variable Zwith meanmand variance σ2,
for anyz≥1,
P(|Z−m|≥zσ)≤1√πze−z2/2.
Lemma G.5 (Adapted from Lemma G.2) .For anyδ<1, under our problem setting in Section 3 with the
regularization hyper-parameter λ∈[λmin,λmax] (λmin>0), it holds that for all t>0,
/vextenddouble/vextenddouble/vextenddoubleˆθt−θ∗/vextenddouble/vextenddouble/vextenddouble
Vt≤βt(δ),
∀x∈Rd,|x⊤(ˆθt−θ∗)|≤∥x∥V−1
tβt(δ),
with probability at least 1−δ, where
βt(δ) =σ/radicaligg
log/parenleftbigg(λmin+t)d
δ2λd
min/parenrightbigg
+/radicalbig
λmaxS.
Proof.The proof of this Lemma is trivial given Lemma G.2. For any λ∈[λmin,λmax], according to Lemma
G.2 it holds that, for all t>0,
/vextenddouble/vextenddouble/vextenddoubleˆθt−θ∗/vextenddouble/vextenddouble/vextenddouble
Vt≤βt(δ),
∀x∈Rd,|x⊤(ˆθt−θ∗)|≤∥x∥V−1
tβt(δ),
with probability at least 1−δ, where
βt(δ) =σ/radicaligg
log/parenleftbigg(λ+t)d
δ2λd/parenrightbigg
+√
λS≤σ/radicaligg
log/parenleftbigg(λmin+t)d
δ2λd
min/parenrightbigg
+/radicalbig
λmaxS.
31Published in Transactions on Machine Learning Research (04/2024)
G.2 Proof of Theorem 4.2
Recall the partition of the cumulative regret as:
R(T) =E/bracketleftiggT1/summationdisplay
t=1/parenleftbig
µ(x⊤
t,∗θ∗)−µ(xt⊤θ∗)/parenrightbig/bracketrightigg
/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
Quantity (A)+E/bracketleftiggT/summationdisplay
t=T1+1/parenleftbig
µ(x⊤
t,∗θ∗)−µ(xt(α∗(t)|F∗
t)⊤θ∗)/parenrightbig/bracketrightigg
/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
Quantity (B)
+E/bracketleftiggT/summationdisplay
t=T1+1(µ/parenleftbig
xt(α∗(t)|F∗
t)⊤θ∗)−µ(xt(α∗(t)|Ft)⊤θ∗)/parenrightbig/bracketrightigg
/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
Quantity (C)
+E/bracketleftiggT/summationdisplay
t=T1+1(µ/parenleftbig
xt(α∗(t)|Ft)⊤θ∗)−µ(xt(α(it)|Ft)⊤θ∗)/parenrightbig/bracketrightigg
/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
Quantity (D).
For Quantity (A), it could be easily bounded by the length of warming up period as:
E/bracketleftiggT1/summationdisplay
t=1/parenleftbig
µ(x⊤
t,∗θ∗)−µ(xt⊤θ∗)/parenrightbig/bracketrightigg
≤T1=O/parenleftbigg
T2
p+3/parenrightbigg
≤O/parenleftbigg
Tp+2
p+3/parenrightbigg
. (18)
For Quantity (B), it depicts the cumulative regret of the contextual bandit algorithm that runs with the
theoretical optimal hyperparameter α∗(t)all the time. Therefore, if we implement any state-of-the-arm
contextual generalized linear bandit algorithms (e.g. Filippi et al. (2010); Li et al. (2010; 2017)), it holds that
E/bracketleftiggT/summationdisplay
t=T1+1/parenleftbig
µ(x⊤
t,∗θ∗)−µ(xt(α∗(t)|F∗
t)⊤θ∗)/parenrightbig/bracketrightigg
≤˜O(/radicalbig
T−T1) =˜O(√
T). (19)
ForQuantity(C),itrepresentsthecumulativedifferenceofregretunderthetheoreticaloptimalhyperparameter
combination α∗(t)with two lines of history FtandF∗
t. Note for most GLB algorithms, the most significant
hyperparameter is the exploration rate, which directly affect the decision-making process. Regarding the
regularization hyperparameter λ, it is used to make Vtinvertible and hence would be set to 1 in practice.
And in the long run it would not be influential. Moreover, there is commonly no theoretical optimal value for
λ, and it could be set to an arbitrary constant in order to obtain the ˜O(√
T)bound of regret. For theoretical
proof, this hyperparameter ( λ) is also not significant: for example, if the search interval for λis[λmin,λmax],
then we can easily modify the Lemma G.3 as:
t/summationdisplay
s=1∥xs∥2
V−1
s≤2 log/parenleftbiggdet(Vt+1)
det(λI)/parenrightbigg
≤2dlog/parenleftbigg
1 +t
λmin/parenrightbigg
,
t/summationdisplay
s=1∥xs∥V−1
s≤/radicaltp/radicalvertex/radicalvertex/radicalbtT/parenleftiggt/summationdisplay
s=1∥xs∥2
V−1
s/parenrightigg
≤/radicaligg
2dtlog/parenleftbigg
1 +t
λmin/parenrightbigg
.
We will offer a more detailed explanation to this fact in the following proof of bounding Quantity (C).
Furthermore, other parameters such as the stepsize in a loop of gradient descent will not be crucial either
since the final result would be similar after the convergence criterion is met. Therefore, w.l.o.g we would only
assume there is only one exploration rate hyperparameter here to bound Quantity (C). Recall that α(t)is
the combination of all hyperparameters, and hence we could denote this exploration rate hyperparameter as
α(t)in this part since there is no more other hyperparameter. Here we would use LinUCB and LinTS for the
detailed proof, and note that regret bound of all other UCB and TS algorithms could be similarly deduced.
32Published in Transactions on Machine Learning Research (04/2024)
We first reload some notations: recall we denote Vt=λI+/summationtextt−1
i=1xix⊤
i,ˆθt=V−1
t/summationtextt−1
i=1xiyiwherextis the
arm we pulled at round tby using our tuned hyperparameter α(it)and the history based on our framework
all the time. And we denote
Xt= arg max
x∈Xtx⊤ˆθt+α∗(t)∥x∥V−1
t
Similarly, we denote ˜Vt=λI+/summationtextt−1
i=1˜Xi˜X⊤
i,˜θt=˜V−1
t/summationtextt−1
i=1˜Xi˜yi, where ˜Xtis the arm we pulled by using
the theoretical optimal hyperparameter α∗(t)under the history of always using {α∗(s)}t−1
s=1, and ˜ytis the
corresponding payoff we observe at round t. Therefore, it holds that,
˜Xt= arg max
x∈Xtx⊤˜θt+α∗(t)∥x∥˜V−1
t.
By using these new definitions, the Quantity (C) could be formulated as:
E/bracketleftiggT/summationdisplay
t=T1+1(µ/parenleftbig
xt(α∗(t)|F∗
t)⊤θ∗)−µ(xt(α∗(t)|Ft)⊤θ∗)/parenrightbig/bracketrightigg
/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
Quantity (C)=E/bracketleftiggT/summationdisplay
t=T1+1µ(˜X⊤
tθ∗)−µ(X⊤
tθ∗)/bracketrightigg
For LinUCB, since the Lemma G.2 holds for any sequence (x1,...,xt), and hence we have that with probability
at least 1−δ,
/vextenddouble/vextenddouble/vextenddoubleˆθ−θ/vextenddouble/vextenddouble/vextenddouble
Vt≤βt(δ)≤α∗(T,δ), (20)
where
βt(δ) =σ/radicaligg
log/parenleftbigg(λ+t)d
δ2λd/parenrightbigg
+√
λS=α∗(t).
And we will omit δfor simplicity. For LinUCB, we have that
X⊤
tˆθt+α∗(t)∥Xt∥V−1
t≥˜X⊤
tˆθt+α∗(t)/vextenddouble/vextenddouble˜Xt/vextenddouble/vextenddouble
V−1
t
≥˜X⊤
tθ∗+α∗(t)/vextenddouble/vextenddouble˜Xt/vextenddouble/vextenddouble
V−1
t+˜X⊤
t(ˆθt−θ∗)≥˜X⊤
tθ∗.
Therefore, it holds that
X⊤
tθ∗+α∗(t)∥Xt∥V−1
t+X⊤
t(ˆθt−θ∗)≥˜X⊤
tθ∗
X⊤
tθ∗+ 2α∗(t)∥Xt∥V−1
t≥˜X⊤
tθ∗,
which implies that
(˜Xt−Xt)⊤θ∗≤2α∗(T)∥Xt∥V−1
t.
By Lemma G.3 and choosing T1=T2/(p+3), it holds that,
T/summationdisplay
t=T1+1∥Xt∥V−1
t≤T/summationdisplay
t=T1+1∥Xt∥/radicalbig
λmin(Vt) =O(T×T−1/(p+3)) =O(T(p+2)/(p+3)).
And then it holds that,
T/summationdisplay
t=T1+1/parenleftbig˜XT
tθ−Xtθ/parenrightbig
=˜O/parenleftigg
α∗(T)T/summationdisplay
t=T1+1/vextenddouble/vextenddouble˜Xt/vextenddouble/vextenddouble
V−1
t/parenrightigg
=˜O(T(p+2)/(p+3)). (21)
Noteβt(δ)contain the regularizer parameter λ, and it’s often set to some constant (e.g. 1) in practice. If we
tuneλin the search interval [λmin,λmax], then we can still have the identical bound as in Eqn. equation 20
by using the fact that
βt(δ) =σ/radicaligg
log/parenleftbigg(λ+t)d
δ2λd/parenrightbigg
+√
λS≤σ/radicaligg
log/parenleftbigg(λmin+t)d
δ2λd
min/parenrightbigg
+/radicalbig
λmaxS.
33Published in Transactions on Machine Learning Research (04/2024)
This result is deduced in our Lemma G.5, which implies that tuning the regularizer hyperparameter would
not affect the order of final regret bound in Eqn. equation 21. Therefore, as we mentioned earlier, we could
only consider the exploration rate as the unique hyperparameter for theoretical analysis.
For LinTS, we have that
X⊤
tˆθt+α∗(T)∥Xt∥V−1
tZt≥˜X⊤
tˆθt+α∗(T)/vextenddouble/vextenddouble˜Xt/vextenddouble/vextenddouble
V−1
t˜Zt
≥˜X⊤
tθ∗+α∗(T)/vextenddouble/vextenddouble˜Xt/vextenddouble/vextenddouble
V−1
t˜Zt+˜X⊤
t(ˆθt−θ∗)
≥˜X⊤
tθ∗+α∗(T)/vextenddouble/vextenddouble˜Xt/vextenddouble/vextenddouble
V−1
t˜Zt+/vextenddouble/vextenddouble˜Xt/vextenddouble/vextenddouble
V−1
t/vextenddouble/vextenddouble/vextenddoubleˆθt−θ∗/vextenddouble/vextenddouble/vextenddouble
Vt
≥˜X⊤
tθ+ (α∗(T)˜Zt−α∗(T))/vextenddouble/vextenddouble˜Xt/vextenddouble/vextenddouble
V−1
t,
whereZtandZt,∗are IID normal random variables, ∀t. And then we could deduce that
X⊤
tθ∗+α∗(T)∥Xt∥V−1
tZt+X⊤
t(ˆθt−θ∗)≥˜X⊤
tθ+ (α∗(T)˜Zt−α∗(T))/vextenddouble/vextenddouble˜Xt/vextenddouble/vextenddouble
V−1
t
X⊤
tθ∗+α∗(T)∥Xt∥V−1
tZt+α∗(T)∥Xt∥V−1
t≥˜X⊤
tθ+ (α∗(T)˜Zt−α∗(T))/vextenddouble/vextenddouble˜Xt/vextenddouble/vextenddouble
V−1
t
(˜Xt−Xt)⊤θ∗≤(α∗(T)−α∗(T)˜Zt)/vextenddouble/vextenddouble˜Xt/vextenddouble/vextenddouble
V−1
t+ (α∗(T) +α∗(T)Zt)∥Xt∥V−1
t:=Kt
whereKtis normal random variable with
E(Kt)≤2α(T)T−1/(p+3),SD(Kt)≤√
2α∗T−1/(p+3).
Consequently, we have
T/summationdisplay
t=T1+1/parenleftbig˜XT
tθ−XT
tθ/parenrightbig
≤T/summationdisplay
t=T1+1Kt:=K
E(K) = 2α∗(T)T(p+2)/(p+3)=˜O(Tp+2
p+3),SD(K)≤√
2α∗Tp+1
2p+6=O(Tp+1
2p+6).
Based on Lemma G.4, we have
P/parenleftiggT/summationdisplay
t=T1+1/parenleftbig˜XT
tθ−XT
tθ/parenrightbig
≥K > (2α∗+√
2)Tp+2
p+3/parenrightigg
≤1
c√π√
Te−c2T/2. (22)
This probability upper bound is minimal and negligible, which means the bound on its expected value
(Quantity (C)) could be easily deduced. Note we could use this procedure to bound the regret for other
UCB and TS bandit algorithms, since most of the proof for GLB algorithms are closely related to the rate
of/summationtextT
t=T1+1∥Xt∥V−1
tand the consistency of ˆθt. In conclusion, we have that Quantity (C) could be upper
bounded by the order ˜O(Tp+2
p+3).
For Quantity (D), which is the extra regret we paid for hyperparameter tuning in theory. Recall we assume
µ(xt(α|Ft)⊤θ∗) =gt(α) +ηFt,αfor some time-dependent Lipschitz function gt. And (ηFt,α−E[ηFt,α])is IID
sub-Gaussian with parameter τ2where E[ηFt,α]depends on the history Ft. DenoteνFt,α=ηFt,α−E[ηFt,α]
is the IID sub-Gaussian random variable with parameter τ2, then we have that
yt=gt(α(it)) +νFt,α(it)+E[ηFt,α(it)] +ϵt
SinceνFt,α(it),ϵtis IID sub-Gaussian random variable independent with Ft, we denote ˜ϵFt,α(it)=νFt,α(it)+ϵt
as the IID sub-Gaussian noise with parameter τ2+σ2. And then we have
yt=gt(α(it)) +E[ηFt,α(it)] + ˜ϵFt,α(it),E(yt) =gt(α(it)) +E[ηFt,α(it)]
µ(xt(α|Ft)⊤θ∗) =gt(α) +E[ηFt,α].
34Published in Transactions on Machine Learning Research (04/2024)
For Quantity (D), recall it could be formulated as:
E/bracketleftiggT/summationdisplay
t=T1+1(µ/parenleftbig
xt(α∗(t)|Ft)⊤θ∗)−µ(xt(α(it)|Ft)⊤θ∗)/parenrightbig/bracketrightigg
/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
Quantity (D).
Since both terms in Quantity (D) are based on the same line of history Ftat iteration t, and the value of
E[ηFt,α]only depends on the history filtration Ftbut not the value of α. Therefore, it holds that
E/bracketleftiggT/summationdisplay
t=T1+1(µ/parenleftbig
xt(α∗(t)|Ft)⊤θ∗)−µ(xt(α(it)|Ft)⊤θ∗)/parenrightbig/bracketrightigg
/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
Quantity (D)=T/summationdisplay
t=T1+1gt(α∗(t))−E[gt(α(it))]
≤T/summationdisplay
t=T1+1sup
α∈Agt(α)−E[gt(α(it))].
Therefore, Quantity (D) could be regarded as the cumulative regret of a non-stationary Lipschitz bandit
and the noise is IID sub-Gaussian with parameter τ2
0= (τ2+σ2). We assume that, under the switching
environment, the Lipschitz function gt(·)would be piecewise stationary and the number of change points
is of scale ˜O(1). Therefore, Quantity (D) can be upper bounded the cumulative regret of our Zooming TS
algorithm with restarted strategy given c(T) =˜O(1). By choosing T2= (T−T1)(p+2)/(p+3)= Θ(T(p+2)/(p+3)),
and according to Theorem 4.1, it holds that,
T/summationdisplay
t=T1+1sup
α∈Agt(α)−E[gt(α(it))]≤˜O/parenleftbigg
Tp+2
p+3/parenrightbigg
. (23)
By combining the results deduced in Eqn. equation 18, Eqn. equation 19, Eqn. equation 21 (or Eqn.
equation 22) and Eqn. equation 23, we finish the proof of Theorem 4.2 for linear bandits. For generalized
linear bandits, under the default and standard assumption in the generalized linear bandit literature that the
derivative of µ(·)could be upper bounded by some constant given |x|≤S, the regret could be bounded by
further multiplying a constant in the same order.
35