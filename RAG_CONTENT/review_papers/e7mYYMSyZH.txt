Published in Transactions on Machine Learning Research (08/2022)
On the Convergence of Shallow Neural Network Training
with Randomly Masked Neurons
Fangshuo Liao Fangshuo.Liao@rice.edu
Department of Computer Science
Rice University
Anastasios Kyrillidis anastasios@rice.edu
Department of Computer Science
Rice University
Reviewed on OpenReview: https: // openreview. net/ forum? id= e7mYYMSyZH
Abstract
With the motive of training all the parameters of a neural network, we study why and when
one can achieve this by iteratively creating, training, and combining randomly selected sub-
networks. Such scenarios have either implicitly or explicitly emerged in the recent literature:
see e.g., the Dropout family of regularization techniques, or some distributed ML training
protocols that reduce communication/computation complexities, such as the Independent
Subnet Training protocol. While these methods are studied empirically and utilized in prac-
tice, they often enjoy partial or no theoretical support, especially when applied on neural
network-based objectives.
In this manuscript, our focus is on overparameterized single hidden layer neural networks
withReLUactivationsinthelazytrainingregime. Bycarefullyanalyzing i)thesubnetworks’
neural tangent kernel, ii)the surrogate functions’ gradient, and iii)how we sample and
combine the surrogate functions, we prove linear convergence rate of the training error –up
to a neighborhood around the optimal point– for an overparameterized single-hidden layer
perceptron with a regression loss. Our analysis reveals a dependency of the size of the
neighborhood around the optimal point on the number of surrogate models and the number
of local training steps for each selected subnetwork. Moreover, the considered framework
generalizes and provides new insights on dropout training, multi-sample dropout training,
as well as Independent Subnet Training; for each case, we provide convergence results as
corollaries of our main theorem.
1 Introduction
Overparameterized neural networks have led to both unexpected empirical success in deep learning (Zhang
et al., 2021; Goodfellow et al., 2016; Arpit et al., 2017; Recht et al., 2019; Toneva et al., 2018) , and new
techniques in analyzing neural network training (Kawaguchi et al., 2017; Bartlett et al., 2017; Neyshabur
et al., 2017; Golowich et al., 2018; Liang et al., 2019; Arora et al., 2018; Dziugaite & Roy, 2017; Neyshabur
et al., 2018; Zhou et al., 2018; Soudry et al., 2018; Shah et al., 2020; Belkin et al., 2019; 2018; Feldman,
2020; Ma et al., 2018; Spigler et al., 2019; Belkin, 2021; Bartlett et al., 2021; Jacot et al., 2018) . While
theoretical work in this field has led to a diverse set of new overparameterized neural network architectures
(Frei et al., 2020; Fang et al., 2021; Lu et al., 2020; Huang et al., 2020; Allen-Zhu et al., 2019a; Gu et al.,
2020; Cao et al., 2020) and training algorithms (Du et al., 2018; Zou et al., 2020; Soltanolkotabi et al., 2018;
Oymak & Soltanolkotabi, 2019; Li et al., 2020; Oymak & Soltanolkotabi, 2020) , most efforts fall under the
following scenario: in each iteration, we perform a gradient-based update that involves all parameters of the
neural network in both the forward and backward propagation. Yet, advances in regularization techniques
1Published in Transactions on Machine Learning Research (08/2022)
(Srivastava et al., 2014; Wan et al., 2013; Gal & Ghahramani, 2016; Courbariaux et al., 2015; Labach et al.,
2019), computationally-efficient (Shazeer et al., 2017; Fedus et al., 2021; Lepikhin et al., 2020; LeJeune
et al., 2020; Yao et al., 2021; Yu et al., 2018; Mohtashami et al., 2021; Yuan et al., 2020; Dun et al., 2021;
Wolfe et al., 2021) and communication-efficient distributed training methods (Vogels et al., 2019; Wang
et al., 2021; Yuan et al., 2020) favor a different narrative: one would –explicitly or implicitly– train smaller
and randomly-selected models within a large model, iteratively. This brings up the following question:
“Can one meaningfully train an overparameterized ML model by iteratively training
and combining together smaller versions of it? ”
Figure 1: Training a single hidden-layer perceptron using multiple randomly masked subnetworks. Here,
f(W,·)denotes the full model with Wparameters, and fml
k(W,·)denotes the surrogate model (subnetwork)
with only active neurons as dictated by the mask ml
kat thek-th iteration for subnetwork l. Moreover, Wk
denotes the parameter at the start of the iteration, while Wl
k,τis the trained parameter of subnetwork l.
This question closely relates to multiple existing training algorithms, as we discuss below; our goal is to work
towards a unified training scheme, and seek for rigorous theoretical analysis of such a framework. Focusing
on this objective based on shallow feedforward neural networks, we provide a positive answer, accompanied
with theoretical guarantees that are supported by observations in practical scenarios.
To be more specific, the training scheme we consider is depicted in Figure 1. Given a dense neural network,
as in Fig.1 (a), we sample masks within one training step; see Fig.1 (b). Each of these masks deactivates a
subset of the neurons in the original network’s hidden layer. In a way, each mask defines a surrogate model ,
as shown in Fig.1 (c), based on the original network, leading to a collection of subnetworks . These surrogate
subnetworks independently update their own parameters (possibly on different data shards), by performing
(stochastic)gradientdescent((S)GD)steps. Lastly,weaggregatetheparametersoftheindependentlytrained
subnetworks (Fig.1( d)) to update the weights of the original network, before the next iteration starts; see
Fig.1(e). Note that multiple masks could share active neurons. When aggregating the updates, we take the
weighted sum of the updated parameters across all subnetworks, with the aggregation weights computed on
the masks of the current iteration.
2Published in Transactions on Machine Learning Research (08/2022)
We mathematically illustrate the difference between traditional training (first expression below) and the
considered methodology (second expression below):
Wk+1=(S)GD (f,Wk,τ)vs.
Wk+1=Reassemble/parenleftig
(S)GD/parenleftig
fm1
k,W1
k,τ/parenrightig
,(S)GD/parenleftig
fm2
k,W2
k,τ/parenrightig
,···,(S)GD/parenleftig
fmp
k,Wp
k,τ/parenrightig/parenrightig
Here, the acronym (S)GD (f,W,τ)indicates the application of (S)GD on function fforτiterations, starting
from initial parameters W. Consequently, (S)GD/parenleftig
fml
k,Wl
k,τ/parenrightig
indicates the application of (S)GD for τ
iterations on the surrogate function fml
k, based on the mask ml
kand using only the subset of parameters
Wl
k. The function Reassemble involves both aggregation and reassembly of the whole model Wk+1.
In this work, we perform a theoretical analysis of this framework, based on a single-hidden layer perceptron
with ReLU activations. This is a non-trivial, non-convex setting, that has been used extensively in studying
the behavior of training algorithms on neural networks (Du et al., 2018; Zou et al., 2020; Soltanolkotabi
et al., 2018; Oymak & Soltanolkotabi, 2019; Li et al., 2020; Oymak & Soltanolkotabi, 2020; Song & Yang,
2020; Ji & Telgarsky, 2020; Mianjy & Arora, 2020) .
Challenges. Much work has been devoted to analyzing the convergence of neural networks based on the
Neural Tangent Kernel (NTK) perspective (Jacot et al., 2018) ; see the Related Works section below. The
literature in this direction notice that the NTK remains roughly stable throughout training. Therefore, the
neural network output can be approximated well by the linearization defined by the NTK. Yet, training
with randomly masked neurons poses additional challenges: i)With a randomly generated mask, the NTK
changes even with the same set of weights, leading to more instability of the kernel; ii)the gradient of
the subnetworks introduces both randomness and bias towards optimizing the loss of the full network; and,
iii)the non-linear activation makes the aggregated network function no longer a linear combination of the
subnetwork functions. The three challenges complicate the analysis, driving us to treat the NTK, gradient,
andcombinednetworkfunctionwithspecialcare. Wewilltacklethesedifficultiesintheproofofthetheorems.
Motivation and connection to existing methods. The study of partial models/subnetworks that reside
in a large dense network have drawn increasing attention.
Dropout regularization . Dropout (Srivastava et al., 2014; Wan et al., 2013; Gal & Ghahramani, 2016;
Courbariaux et al., 2015) is a widely-accepted technique against overfitting in deep learning. In each training
step, a random mask is generated from some pre-defined distribution, and used to mask-out part of the
neurons in the neural network. Later variants of dropout include the drop-connect (Wan et al., 2013) , multi-
sample dropout (Inoue, 2019) , Gaussian dropout (Wang & Manning, 2013) , and the variational dropout
(Kingma et al., 2015) . Here, we restrict our attention to the vanilla dropout, and the multi-sample dropout.
The vanilla dropout corresponds to our framework, if in the latter we sample only one mask per iteration,
and let the subnetwork perform only one gradient descent update. The multi-sample dropout extends the
vanilla dropout in that it samples multiple masks per iteration. For regression tasks, our theoretical result
implies convergence guarantees for these two scenarios on a single hidden-layer perceptron.
Distributed ML training. Recent advances in distributed model/parallel training have led to variants of
distributed gradient descent protocols (Mcdonald et al., 2009; Zinkevich et al., 2010; Zhang & Ré, 2014;
Zhang et al., 2016) . Yet, all training parameters are updated per outer step, which could be computationally
and communication inefficient, especially in cases of high communication costs per round. The Independent
Subnetwork Training (IST) protocol (Yuan et al., 2020) goesonestepfurther: ISTsplitsthemodelvertically,
where each machine contains all layers of the neural network, but only with a (non-overlapping) subset of
neurons being active in each layer. Multiple local SGD steps can be performed without the workers having
to communicate. Methods in this line of work achieves higher communication efficiency and accuracy that is
comparabletocentralizedtraining. (Wolfe et al., 2021; Dun et al., 2021; Yuan et al., 2020)Yet, the theoretical
understanding of IST is currently missing. Our theoretical result implies convergence guarantees for IST for
a single hidden-layer perceptron under the simplified assumption that every worker has full data access, and
provides insights on how the number of compute nodes affects the performance of the overall protocol.
3Published in Transactions on Machine Learning Research (08/2022)
Contributions. The present training framework naturally generalizes the approaches above. Yet, current
literature –more often than not– omits any theoretical understanding for these scenarios, even for the case
of shallow MLPs. While handling multiple layers is a more desirable scenario (and is, indeed, considered
as future work), our presented theory illustrates how training and combining multiple randomly masked
surrogate models behaves. Our findings can be summarized as follows:
•We provide convergence rate guarantees for i)dropout regularization (Srivastava et al., 2014) ,ii)multi-
sample dropout (Inoue, 2019) ,iii)and multi-worker IST (Yuan et al., 2020) , given a regression task on
a single-hidden layer perceptron.
•We show that the NTK of surrogate models stays close to the infinite width NTK, thus being positive
definite. Consequently, our work shows that training over surrogate models still enjoys linear convergence.
•For subnetworks defined by Bernoulli masks with a fixed distribution parameter, we show that aggregated
gradient in the first local step is a biased estimator of the desirable gradient of the whole network, with
the bias term decreasing as the number of subnetworks grows. Moreover, all aggregated gradients during
local training stays close to the aggregated gradient of the first local step. This finding leads to linear
convergence of the above training framework with an error term under Bernoulli masks.
•For masks sampled from categorical distribution, we provide tight bounds i)on the average loss increase,
when sampling a subnetwork from the whole network; ii)on the loss decrease, when the independently
trained subnetworks are combined into the whole model. This finding leads to linear convergence with a
slightly different error term than the Bernoulli mask scenario.
Summarizing the contributions above, the main objective of our work is to provide theoretical support for
the following statement:
Main statement (Informal ).Consider the training scheme shown in Figure 1 and described precisely in
Algorithm 1. If the masks are generated from a Bernoulli distribution or categorical distribution, under
sufficiently large over-parameterization coefficient, and sufficiently small learning rate, training the large
model via surrogate subnetworks still converges linearly, up to an neighborhood around the optimal point.
2 Related Works
Convergence of Neural Network Training. Recent study on the properties of over-parameterized
neural networks enabled their training error analysis. The NTK-based analysis studies the dynamics of
the parameters in the so-called kernel regime under a particular scaling option (Jacot et al., 2018; Du
et al., 2018; Oymak & Soltanolkotabi, 2020; Song & Yang, 2020; Ji & Telgarsky, 2020; Su & Yang, 2019;
Arora et al., 2019; Mianjy & Arora, 2020; Huang et al., 2021) . NTKs can be viewed as the reproducing
kernels of the function space defined by the neural network structure, and are constructed using the inner
product between gradients of pairs of data points. With the observation of the NTK’s stability under
sufficient over-parameterization, recent work has shown that (S)GD achieves zero training loss on shallow
neural networks for regression task, even if when the data-points are randomly labeled (Du et al., 2018;
Oymak & Soltanolkotabi, 2020; Song & Yang, 2020) . To study how the labeling of the data affects the
convergence, (Arora et al., 2019) characterizes the loss update in terms of the NTK-induced inner-product
of the label vector, and notices that, when the label vector aligns with the top eigenvectors of the NTK,
training achieves a faster convergence rate. (Su & Yang, 2019) analyze the convergence of training from a
functional approximation perspective, and obtains a meaningful result under infinite sample size limit, where
the minimum eigenvalue of the NTK matrix goes to zero.
Later works start to deviate from the NTK-based analysis and aim at reducing the over-parametereization
requirement. By using a more refined analysis on the evolution of the Jacobian matrix, (Oymak &
Soltanolkotabi, 2019) reduce the required hidden-layer width to n2, withnbeing the sample size. (Nguyen,
2021)leverages a property of the gradient that resembles the PL-condition, and provides convergence guar-
antees for deep neural network under proper initialization. When applied to neural networks with one
hidden layer, their over-parameterization requirement also reduces to n2.(Song et al., 2021) further study
4Published in Transactions on Machine Learning Research (08/2022)
the PL-condition along the path of the optimization and show that a subquadratic over-parameterization is
sufficient to guarantee convergence. While reducing the over-parameterization is ideal, the focus of our work
is to extend the analysis on regular neural network training to a more general training scheme.
A different line of work explores the structure of the data-distribution in classification tasks, by assuming
separabilitywhenmappedtotheHilbertspaceinducedbythepartialapplicationoftheNTK (Ji & Telgarsky,
2020; Mianjy & Arora, 2020) . Rather than depending on the stability of NTK, the crux of these works
relies on the small change in the linearization of the network function. This line of work requires milder
overparameterization, and can be easily extended to training stochastic gradient descent without changing
theover-parameterizationrequirement. The above literature assumes all parameters are updated per iteration.
Analysis of Dropout. There is literature devoted to the analysis of dropout training. For shallow linear
neural networks, (Senen-Cerda & Sanders, 2020a) give asymptotic convergence rate by carefully character-
izing the local minima. For deep neural networks with ReLU activations, (Senen-Cerda & Sanders, 2020b)
shows that the training dynamics of dropout converge to a unique stationary set of a projected system of
differential equations. Under NTK assumptions, (Mianjy & Arora, 2020) shows sublinear convergence rate
for an online version for dropout in classification tasks. Recently, (LeJeune et al., 2021) study the duality of
Dropout in a linear regression problem and transform the dropout into a penalty term in the loss. Our main
theorem implies linear convergence rate of the training loss dynamic for the regression task on a shallow
neural network with ReLU activations.
Federated Learning and Distributed Training. Traditional analysis on distributed training methods
assumes that the objective can be written as a sum or average of a sequence sub-objectives (Stich, 2018;
Li et al., 2019; Haddadpour & Mahdavi, 2019; Khaled et al., 2019a;b) . Each worker/client performs some
variant of local (stochastic) gradient descent on a subset of the sub-objectives. This line of work deviates
greatly from the scenario we are considering. First, the assumption that the objective can be broken down
into linear combination of sub-objectives corresponds to the data-parallel training, in stark contrast to
our proposed scheme: the loss computed on the whole network not necessarily equals to the mean of the
losses computed on the subnetworks. Second, this line of work assumes that the objective is smooth and
usually convex or strongly convex. Moreover, there is recent work on training partially masked neural
networks that deviates from the assumption that the objective can be written as a linear combination of
sub-objectives (Mohtashami et al., 2021) . In particular, in this work (Mohtashami et al., 2021) , the authors
consideroptimizingamoregeneralclassofdifferentiableobjectivefunctions. However, assumptionsincluding
Lipschitzness and bounded perturbation made in their work cannot be easily checked for a concrete neural
network, especially for the problem of minimizing mean squared error. Lastly, recent advances in the NTK
theory also facilitated the theoretical work on Federated Learning (FL) on neural network training. FL-NTK
(Huang et al., 2021) characterize the asymmetry of the NTK matrix due to the partial data knowledge. For
non-i.i.d. data distribution, (Deng & Mahdavi, 2021) proves convergence for a shallow neural network by
analyzing the semi-Lipschitzness of the hidden layer. Our work differs since we consider training a partial
model with the whole dataset. We consider the more frequently used setting of a one-hidden layer perceptron
with non-differentiable activation.
3 Training with Randomly Masked Neurons
We use bold lower-case letters (e.g., a) to denote vectors, bold upper-case letters (e.g., A) to denote matrices,
and standard letters (e.g., a) for scalars.∥a∥2stands for the ℓ2(Euclidean) vector norm, ∥A∥2stands for
the spectral matrix norm, and ∥A∥Fstands for the Frobenius norm. For an integer a, we use [a]to
denote the enumeration set {1,2,···,a}. Unless otherwise stated, pdenotes the number of subnetworks,
andl∈[p]its index;Kdenotes the number of global iterations and k∈[K]its index;τis used for the
number of local iterations and t∈[τ]its index. We use Mkto denote the mask at global iteration k, and
E[Mk][·] =EM0,...,Mk[·]to denote the total expectation over masks M0,...,Mk. We use P(·)to denote the
probability of an event, and I{·}to denote the indicator function of an event. For distributions, we use
N(µ,Σ)to denote the Gaussian distribution with mean µand variance Σ. We use Bern (ξ)to denote the
Bernoulli distribution with mean ξ, and we use Unif (S)to denote the uniform distribution over the set S.
For a complete list of notation, see Table 1 in the Appendix.
5Published in Transactions on Machine Learning Research (08/2022)
3.1 Single Hidden-Layer Neural Network with ReLU activations
Algorithm 1 Randomly Masked Training
Input:Mask Distribution D, local step-size η,
global aggregation weight ηk,r
1:Initialize W0,a
2:fork= 0,...,K−1do
3:Sample mask Mk∼D
4:forl= 1,...,pdo
5: Wl
k,0←Wk
6:fort= 0,...,τ−1do
7: Wl
k,t+1←Wl
k,t−η∂Lml
k(Wl
k,t)
∂W
8:end for
9: ∆Wl
k←Wl
k,τ−Wk
10:end for
11:forr= 1,...,mdo
12: wk+1,r←wk,r+ηk,r/summationtextp
l=1∆wl
k,r
13:end for
14:end forWe consider the single hidden-layer neural network with
ReLU activations, as in:
f(W,a,x) =1√mm/summationdisplay
r=1arσ(⟨wr,x⟩) :=f(W,x).
Here, W=/bracketleftbigw1,...,wm/bracketrightbig⊤∈Rm×dis the weight ma-
trix of the first layer, and a=/bracketleftbiga1,...,am/bracketrightbig⊤∈Rm
is the weight vector of the second layer. We assume
that each wris initialized based on N(0,κ2I). Each
weight entry arin the second layer is initialized uni-
formly at random from {−1,1}. As in (Du et al.,
2018; Zou et al., 2020; Soltanolkotabi et al., 2018; Oy-
mak & Soltanolkotabi, 2019; Li et al., 2020; Oymak &
Soltanolkotabi, 2020) ,ais fixed.
Consider a subnetwork computing scheme with pworkers.
In thek-th global iteration, we consider each binary mask
Mk∈{0,1}m×pto be composed of subnetwork masks
ml
k∈{0,1}mforl∈[p]. Ther-th entry of ml
kis denoted
asml
k,r, withml
k,r= 1indicating that neuron ris active in subnetwork lin thekth global iteration, and
ml
k,r= 0otherwise. We assume that the sampling of the masks for each neuron is independent of other
neurons, and further impose the condition that the event ml
k,r= 1happens with a fixed probability for
allk,randl. We denote this probability with ξ=P/parenleftig
ml
k,r= 1/parenrightig
. The surrogate function defined by a
subnetwork mask ml
kis given by:
fml
k(W,x) =1√mm/summationdisplay
r=1arml
k,rσ(⟨wr,x⟩).
With colored text, we highlight the differences between the full model and the surrogate functions. Consider
the dataset given by (X,y) ={(xi,yi)}n
i=1. We make the following assumption on the dataset:
Assumption 1. For anyi∈[n], it holds that∥xi∥2= 1and|yi|≤C−1for some constant C≥1.
Moreover, for any j̸=iit holds that the points xi,xjare not co-aligned, i.e., xi̸=ζxjfor anyζ∈R.
This assumption is quite standard as in previous literature (Du et al., 2018; Arora et al., 2019; Song & Yang,
2020).We consider training the neural network using the regression loss. Given a dataset (X,y), the function
output on the whole dataset is denoted as f(W,X) =/bracketleftbig
f(W,x1),...,f (W,xn)/bracketrightbig
. Then, the (scaled) mean
squared error (MSE) of a surrogate model is given by:
Lml
k(W) =/vextenddouble/vextenddouble/vextenddoubley−fml
k(W,X)/vextenddouble/vextenddouble/vextenddouble2
2.
The surrogate gradient is computed as:
∂Lml
k(W)
∂wr=1√mn/summationdisplay
i=1arml
k,r/parenleftig
fml
k(W,xi)−yi/parenrightig
xiI{⟨wr,xi⟩≥0}.
Letηbe a constant subnetwork training learning rate, and let the aggregation weight ηk,rbe zero if neuron
ris active in no subnetwork in the kth iteration; otherwise ηk,ris set to the inverse of the number of subnets
in which it is active. Within this setting, the general training algorithm is given by Algorithm 1.
4 Convergence on Two-Layer ReLU Neural Network
We assume that ml
k,r= 1happens with a fixed probability for all k,randl, and such probability is denoted
byξ. Consequently, the forward pass of the surrogate function is a linear combination of ξ-proportion of
6Published in Transactions on Machine Learning Research (08/2022)
the neurons’ output. To keep the pre-activation of the hidden layer at the same scale for both the whole
network and the subnetwork, we multiply the weight of the whole network with a factor of ξ. Due to the
homogeneity of the ReLU activation, this is equivalent to scaling the output of each neuron, as in (Mianjy
& Arora, 2020) . For notation clarity, we define:
u(i)
k=1√mm/summationdisplay
r=1arξσ(⟨wk,r,xi⟩) =ξ√mm/summationdisplay
r=1arσ(⟨wk,r,xi⟩).
Adding this scaling factor gives the property that EMk/bracketleftig
fml
k(Wk,xi)/bracketrightig
=u(i)
k, meaning that the sampled
subnetworksintheglobaliteration kareunbiasedestimatorsoftheaggregatednetworkintheglobaliteration
k−1. Hereu(i)
kis both the initial whole network output in global iteration kand the aggregated network
output in global iteration k−1. We focus on the behavior of the following loss, computed on the scaled
whole network over iterations k:
Lk=∥y−uk∥2
2,where uk=/bracketleftig
u(1)
k,...,u(n)
k/bracketrightig
.
This is the regression loss over iterations kbetween observations yand the learned model uk.
Properties of subnetwork NTK. Recent works on analyzing the convergence of gradient descent for
neural networks consider approximating the function output ukwith the first order Taylor expansion (Du
et al., 2018; Arora et al., 2019; Song & Yang, 2020) . For constant step size η, taking the gradient descent’s
(i.e.,Wk+1=Wk−η∇WL(Wk)) first-order Taylor expansion, we get:
u(i)
k+1≈u(i)
k+/angbracketleftig
∇Wu(i)
k,Wk+1−Wk/angbracketrightig
≈u(i)
k−ξηn/summationdisplay
j=1H(k)ij(u(j)
k−yj), (1)
where H(k)∈Rn×nis the finite-width NTK matrix of iteration k, given by
H(k)ij=ξ
m⟨xi,xj⟩m/summationdisplay
r=1I{⟨wk,r,xi⟩≥0,⟨wk,r,xj⟩≥0}. (2)
Compared with the previous definition of finite-width NTK, we have an additional scaling factor ξ. This is
because, based on our later definition of masked-NTK, we would like the masked-NTK to be an unbiased
estimator of the finite-width NTK. In the overparameterized regime, the change of the network’s weights is
controlled in a small region around initialization. Therefore, the change of H(k)is small, staying close to
the NTK at initialization. Moreover, the latter can be well approximated by the infinite-width NTK:
H∞
ij=ξ·Ew∼N(0,I)[⟨xi,xj⟩I{⟨w,xi⟩≥0,⟨w,xj⟩≥0}].
(Du et al., 2018) shows that H∞is positive definite.
Theorem 1. (Du et al., 2018) Denote λ0:=λmin(H∞), the minimum eigenvalue of H∞. Then we have
λ0>0as long as assumption (1) holds.
With H(k)staying sufficiently close to H∞,(Du et al., 2018; Arora et al., 2019; Song & Yang, 2020) show
thatλmin(H(k))≥λ0
2>0. Moreover, Equation 1 implies that
uk+1−uk≈−ξηH(k)(uk−y),
that further leads to linear convergence rate:
Lk+1≈Lk+⟨∇ukLk,uk+1−uk⟩≈Lk−ξη⟨uk−y,H(k)(uk−y)⟩≈(1−ξηλ0)Lk.
In NTK analysis, the Taylor expansion for both ukandLkproduces an error term that improves the
convergence rate from ηλ0toγηλ 0withγ∈(0,1)being a constant.
For our scenario, the randomly sampled subnetworks bring a trickier situation onto the table: in each
iteration, due to the different masks, the NTK changes even when the weights stay the same. To tackle this
difficulty, we provide a generalization of the definition of the finite-width NTK that takes both the mask and
the weight into consideration:
7Published in Transactions on Machine Learning Research (08/2022)
Definition 1. Letml
k′be the mask of subnetwork lin iteration k′. We define the masked-NTK in global
iterationkand local iteration tinduced by ml
k′as:
/parenleftbig
ml
k′◦H(k,t)/parenrightbig
ij=1
m⟨xi,xj⟩m/summationdisplay
r=1ml
k′,rI{⟨wk,t,r,xi⟩≥0,⟨wk,t,r,xi⟩≥0}.
Here, with colored text we highlight the main differences to the common NTK definition. Although we
are only interested in the masked-NTK with k=k′, to facilitate our analysis on the minimum eigenvalue
of masked-NTK, we also allow k̸=k′. We point out two connections between our masked-NTK and the
vanilla NTK: i)the masked-NTK is an unbiased estimator of the whole network’s NTK; ii)whenξ= 1,
the masked-NTK reduce to the vanilla NTK as in equation (2). Throughout iterations of the algorithm, the
following theorem shows that all masked-NTKs stay sufficiently close to the infinite-width NTK.
Theorem 2. Suppose the number of hidden nodes satisfies m= Ω (n2log(Kpn/δ )/ξλ2
0). If for all k,tit holds
that∥wk,t,r−w0,r∥2≤R:=κλ0
8n, then with probability at least 1−δ, for allk,k′∈[K]we have:
λmin(ml
k′◦H(k,t))≥λ0
2.
The above theorem relies on the small weight change in iteration (k,t). Such assumption is also made in
previous work (Du et al., 2018; Song & Yang, 2020) to show the positive definiteness of the NTK matrix.
In order to guarantee each subnetwork’s loss decrease, we need to ensure that the i)the weight change is
bounded up to global iteration k(this implies that when a subnetwork is sampled from the whole network,
its weights do not deviate much from the initialization); and, ii)the weight change during the local training
of the subnetwork is also bounded. The following hypothesis establishes these two conditions, and sets up
the “skeleton” to construct different theorems, based on problems considered. The aim of this work is to
prove this hypothesis for several cases.
Hypothesis 1. Fix the number of global iterations K. Suppose the number of hidden nodes satisfies m=
Ω (n2log(Kpn/δ )/ξλ2
0), and suppose we use a constant step size η=O(λ0/n2). If the weight perturbation before
iterationkis bounded by
∥wk,r−w0,r∥2+ 2ητ/radicalig
nK
mδE[Mk−1],W0,a[∥y−uk∥2] + (K−k)κ/radicalbig
ξ(1−ξ)pn≤R, (3)
then, for all t∈[τ], with probability at least 1−4δ, we have:
/vextenddouble/vextenddouble/vextenddoubley−fml
k/parenleftbig
Wl
k,t+1,X/parenrightbig/vextenddouble/vextenddouble/vextenddouble2
2≤/parenleftbigg
1−ηλ0
2/parenrightbigg/vextenddouble/vextenddouble/vextenddoubley−fml
k/parenleftbig
Wl
k,t,X/parenrightbig/vextenddouble/vextenddouble/vextenddouble2
2, (4)
and the local weight perturbation satisfies:
∥wk,t,r−wk,r∥≤ητ√
2nK√
mδE[Mk−1],W0,a[∥y−uk∥2] + 2ηκn/radicalig
2ξ(1−ξ)pK
mδ. (5)
The hypothesis above states that, in a given global step k, given a small weight perturbation guarantee
(Equation 3) up to the current global iterations, each subnetwork’s local loss also decreases linearly (Equa-
tion 4), as well as the weight perturbation remains bounded (Equation 5). Yet, the above hypothesis does not
connect the subnetwork’s loss with the whole network’s loss through the sampling and aggregation process.
Our aim is to turn Hypothesis 1 into a series of specific theorems that cover different cases. In particular,
we prove using induction the condition for which Hypothesis (1) holds under: i)masks with i.i.d. Bernoulli;
andii)masks with i.i.d categorical rows. Utilizing these results, we provide convergence results for the two
scenarios. This is the goal in the following section.
4.1 Generic Convergence Result under Bernoulli Mask
While the local gradient descent for each subnetwork is guaranteed to make progress with high probability,
when a large network is split into small subnetworks, the expected loss on the dataset increases. Since
EMk/bracketleftig
fml
k(Wk,xi)/bracketrightig
=u(i)
k, simply expanding the MSE reveals that:
EMk/bracketleftbigg/vextenddouble/vextenddouble/vextenddoubley−fml
k(Wk,xi)/vextenddouble/vextenddouble/vextenddouble2
2/bracketrightbigg
=∥y−uk∥2
2+EMk/bracketleftbigg/vextenddouble/vextenddouble/vextenddoublefml
k(Wk,xi)−uk/vextenddouble/vextenddouble/vextenddouble2
2/bracketrightbigg
.
8Published in Transactions on Machine Learning Research (08/2022)
When analyzing the convergence, the second term on the right-hand side needs to be carefully dealt with.
It is non-trivial to show that, when combining the updated network of the local steps, the loss computed on
the whole network is smaller than or equal to the error of each sub-network. We will solve these technical
difficulties for the training procedure with subnetworks created using masks sampled from two types of
distribution. In this section, we focus on masks satisfies the following Bernoulli assumption:
Assumption 2. (Bernoulli Mask) Each mask entry ml
k,ris independently from a Bernoulli distribution
with meanξ, i.e.,ml
k,r∼Bern (ξ).
Masks sampled in this fashion allow a neuron to be active in more than one subnetworks, or none of the
subnetworks. For convenience, we denote the probability that a neuron is active in at least one subnetwork
withθ= 1−(1−ξ)p. In the meantime, subnetworks created using Bernoulli masks enjoy full independence,
and thus have nice concentration properties. By carefully analyzing the aggregated gradient of each local
step, we arrive at the following generic convergence theorem, under the Bernoulli mask assumption.
Theorem 3. Let assumptions (1) and (2) hold. Then λ0>0. Fix the number of global iterations to Kand
the number of local iterations to τ. Let the number of hidden neurons satisfy:
m= Ω/parenleftbiggK
δmax/braceleftbiggn4
κ2ξθλ4
0,nK2B1
κ2θλ2
0,K2p/bracerightbigg/parenrightbigg
. (6)
Then Algorithm (1) with a constant step-size η=O/parenleftig
λ0
max{n,p}nτ/parenrightig
converges with probability at least 1−δ,
according to:
E[Mk−1]/bracketleftbig
∥y−uk∥2
2/bracketrightbig
≤/parenleftbig
1−1
4ηθτλ 0/parenrightbigk∥y−u0∥2
2+B1, (7)
for some error region level B1>0, defined as:
B1=O/parenleftigg
(1−ξ)2n3d
mλ2
0+(θ−ξ2)nκ2
p+/parenleftbigg
1−1
τ/parenrightbigg2
θ2(1−ξ)nκ2/parenrightigg
. (8)
Overall, given the overparameterization requirement in Equation 6, the neural network training error, as
expressed in Equation 7, drops linearly up to a neighborhood around the optimal point, defined by B1in
Equation 8. We notice that B1has three terms that all reduce to zero when ξ= 1. In the first term of B1,
mappears in the denominator, implying that this term can be arbitrarily decreased as the cost of increasing
the number of hidden neurons. The second term is kept at a constant scale, as long as the initialization
scaleκis small enough. The third term disappears when the number of local steps is one. However, the loss
decreases more in each global iteration, when τis larger, since the convergence rate is 1−O(ηθτλ 0). In the
case ofξ= 1,p= 1andτ= 1, the proposed framework reduces to the whole network training. Choosing
κ= 1, Theorem 3 reduces to a form similar to (Song & Yang, 2020) , with the same convergence rate and
over-parameterization requirement.
Remark. Compared to (Du et al., 2018; Song & Yang, 2020) , the scenario considered in our work involves
an additional randomness introduced by the mask. Thus, our convergence result is based on the expectation
oftheloss: wederivetheboundofthelossfromtheboundofitsexpectation, usingconcentrationinequalities,
and apply a union bound over all iterations k∈[K]. Therefore, the required over-parameterization on m
grows as we increase the number of global iterations, meaning that, under a fixed m, the convergence is
only guaranteed for a bounded number of iterations. This is not a concern in general since to guarantee ϵ
small training error we only need Kto belogϵ−1+logn
log(1−O(ηθτλ 0)−1). This is termed as early-stopping, and is used
in previous literature (Su & Yang, 2019; Allen-Zhu et al., 2018) .
The complete proof of this theorem is defered to Appendix E, and we sketch the proof below:
1. LetXk,r=/summationtextp
l=1ml
k,rdenote the number of subnetworks that update neuron rin global iteration
k. LetNk,r= max{Xk,r,1}to be the normalizer of the aggregated gradient, N⊥
k,r= min{Xk,r,1}
9Published in Transactions on Machine Learning Research (08/2022)
to be the indicator of whether a neuron is selected by at least one subnetwork. Then, the update of
each weight vector can be written as:
wk+1,r=wk,r−η·N⊥
k,r
Nk,rτ−1/summationdisplay
t=0p/summationdisplay
l=1∂L/parenleftig
Wl
k,t,r/parenrightig
∂wr. (9)
2. We first focus on the aggregated gradient of the first local stepN⊥
k,r
Nk,r/summationtextp
l=1∂Lml
k(Wl
k,t,r)
∂wr, and show
thatthisaggregatedgradientsatisfiesaconcentrationpropertyaroundapointneartheidealgradient
∂L(Wk)
∂wr, and such concentration is closer if pis larger.
3. We notice that the difference between the aggregated gradient in the later local steps and the
aggregated gradient in the first local step depends on how much the local weight of each subnetwork
in the later local step deviates from the weight of the first local step. We then show that the local
weight change is bounded, implying that the aggregated gradient in all local steps lie near to the
aggregated gradient in the first local step.
4. Lastly, we use the standard NTK technique to show that the aggregated gradient update in equation
(9) leads to linear convergence per each global step, with an additional error term.
To interpret the theorem, we choose κ=n−1
2, make mild assumptions and simplify the key messages of the
form in Theorem (3). Note that this choice of κis the same as in (Arora et al., 2019).
Assumption 3. For the simplicity of our theorem, we assume that max{K,d,p}≤nandλ0≤1.
Notice that for all mthat satisfies equation (6), the first term in B1is upper bounded by O(1). Moreover,
sincep≥1andτ≥1, by choosing κ=n−1
2, the second and third term are also upper bounded by
O(1). Therefore, B1is upper bounded by O(1). Moreover, since λ0≥1andmax{K,p}≤n, we have that
bothnK2B1
κ2θλ2
0andK2pare smaller thann4
κ2ξθλ4
0, so the over-parameterization requirement in equation (6)
reduces tom=n5K
δξθλ4
0. For different choice of τandp, our considered scenario reduces to different existing
algorithms. In the following, we provide convergence results of these algorithms, as corollaries of Theorem
(3), by considering different τandpvalues.
Dropout. The dropout algorithm (Srivastava et al., 2014) corresponds to the case τ= 1,p= 1. For this
assignment, we arrive at the following corollary.
Corollary 1. Let assumptions (1), (2), and (3) holds. Fix the number of dropout iterations to K, the step
size toη=O(λ0/n2), and let the number of hidden neurons satisfies m= Θ (n5K/ξ2λ4
0δ). Then, the dropout
algorithm on a two-layer ReLU neural network converges with probability at least 1−δ, according to:
E[Mk−1]/bracketleftbig
∥y−uk∥2
2/bracketrightbig
≤/parenleftbigg
1−1
4ηξλ0/parenrightbiggk
∥y−u0∥2
2+O(1−ξ).
Typically, 1−ξisusuallyreferredtoasthe“dropoutrate”. Inourresult, as ξapproaches 0, whichcorresponds
to the scenario that no neurons are selected, the convergence rate approaches 1, meaning that the loss hardly
decreases. In the mean time, the error term remains constant. On the contrary, as ξapproaches 1, which
corresponds to the scenario that all neurons are selected, we get the same convergence rate of 1−O(ηλ0)as
in previous literature (Du et al., 2018; Song & Yang, 2020) , and the error term decreases to 0. Moreover,
we should note that the over-parameterization requirement also depends on ξ. In particular, as ξbecomes
smaller, we need a larger number of hidden neurons to guarantee convergence.
Multi-Sample Dropout. The multi-sample dropout (Inoue, 2019) corresponds to the scenario where
τ= 1,p≥1. Our corollary below indicates how increasing phelps the convergence.
Corollary 2. Let assumptions (1), (2), and (3) hold. Fix the number of dropout iterations to K, the step
size toη=O(λ0/n2), and let the number of hidden neurons satisfy m= Θ (n5K/ξθλ4
0δ). Then the p-sample
10Published in Transactions on Machine Learning Research (08/2022)
dropout algorithm on a two-layer ReLU neural network converges with probability at least 1−δ, according
to:
E[Mk′−1]/bracketleftbig
∥y−uk′∥2
2/bracketrightbig
≤/parenleftbigg
1−1
4ηθλ0/parenrightbiggk′
∥y−u0∥2
2+O/parenleftbigg(1−ξ)2
nK+θ−ξ2
p/parenrightbigg
.
Recall that θ= 1−(1−ξ)pdenotes the probability that a neuron is selected by at least one subnetwork.
Based on this corollary, increasing the number of subnetworks pimprove the convergence rate and the
over-parameterization requirement since θincreases as pincreases. Moreover, increasing the number of
subnetworks help decreasing the error term even when the dropout rate ξis fixed. After pis as large as
nK, the error term stops decreasing, dominated by the term O/parenleftig
(1−ξ)2
nK/parenrightig
. Lastly, compared with the result
of dropout, the over-parameterization depends not only on ξ, but also on θ.
Multi-Worker IST. The multi-worker IST algorithm (Yuan et al., 2020) is very similar to the general
scheme with p≥1andτ≥1, but with the additional assumption that max{K,d,p}≤n, and a special
choice of initialization κ=n−1
2.
Corollary 3. Let assumptions (1), (2), and (3) hold. Fix the number of dropout iterations to K, the step
size toη=O(λ0/nτmax{n,p}), and let the number of hidden neurons satisfy m= Θ (n5K/ξθλ4
0δ). Then the
IST algorithm on a two-layer ReLU neural network converges with probability at least 1−δ, according to:
E[Mk−1]/bracketleftbig
∥y−uk∥2
2/bracketrightbig
≤/parenleftbigg
1−1
4ηθτλ 0/parenrightbiggk
∥y−u0∥2
2+O/parenleftbigg(1−ξ)2
nK+θ−ξ2
p+/parenleftbigg
1−1
τ/parenrightbigg
θ2(1−ξ)/parenrightbigg
While IST with subnetworks constructed using Bernoulli masks , it allows a neuron to be active in more
than one or none of the subnetworks. In the next section, we consider another mask sampling approach that
fits better into the scenario of the original IST, where each hidden neuron is distributed to one and only one
subnetwork with uniform probability.
4.2 Multi-Subnetwork Convergence Result for Categorical Mask
We consider masks sampled from categorical distribution, as explained by the assumption below:
Assumption 4. We assume that Mk∼Categorical (p). To be specific, for each r∈[m], letl′
r∼Unif ([p]),
and we define ml
k,r= 1ifl=l′
randml
k,r= 0otherwise.
In this way, the masks endorsed by each worker are non-overlapping (as stated in (Yuan et al., 2020) ), and
the union of the masks covers the whole set of hidden neurons. However, we note that the subnetworks
created by the masks sampled according to this fashion are no longer independent. The following theorem
presents the convergence result under this setting.
Theorem 4. Let assumptions (1) and (4) hold. Then λ0>0. Moreover, let λmaxdenote the maximum
eigenvalue of H∞. Fix the number of global iterations to Kand the number of local iterations to τ. Let
the number of hidden neurons be m= Ω/parenleftig
n5τ2Kλmax
λ6
0δ/parenrightig
, and choose the initialization scale κ=√nλmaxλ−1
0.
Letγ=/parenleftbig
1−p−1/parenrightbig1
3. Then, Algorithm (1) with a constant step-size η=O/parenleftig
λ0
n2min/braceleftig
p
γ2τ,1/bracerightig/parenrightig
converges with
probability at least 1−δ, according to:
E[Mk−1]/bracketleftbig
∥y−uk∥2
2/bracketrightbig
≤/parenleftbigg
γ+ (1−γ)/parenleftbigg
1−ηλ0
2/parenrightbiggτ/parenrightbiggk
∥y−u0∥2
2+O/parenleftbiggγτnκ2λmax
λ2
0/parenrightbigg
.
We defer the proof of this theorem to Appendix F. This theorem has a couple noticeable properties. First,
when the number of workers p= 1, i.e., the scenario of multi-worker IST reduces to the full-network training,
we achieve γ= 0, which implies that the error term disappears, driving further connections between regular
and IST training. Second, when the number of subnetworks pincreases,γalso increases, leading to a slower
decreasing of the training MSE and convergence to a bigger error neighborhood. In particular, as we increase
11Published in Transactions on Machine Learning Research (08/2022)
the number of subnetworks, the number of active neurons in each subnetwork becomes smaller, which makes
the subnetworks both harder to train and harder to synchronize. We defer the complete proof of this theorem
to Appendix F, and sketch the proof below:
1. Let ˆul
k,t=fml
k/parenleftig
Wl
k,τ,X/parenrightig
. We notice that f=1
p/summationtextp
l=1fml
k. Using this property, we show that
Lk+1=1
p/summationtextp
l=1∥y−ˆul
k,τ∥2
2−1
p/summationtextp
l=1∥uk+1−ˆul
k,τ∥2
2. The first term here enjoys linear convergence
starting from an initial value of ∥y−ˆul
k,0∥2
2.
2. Itthenfollowsthat EMk[1
p/summationtextp
l=1∥y−ˆul
k∥2
2] =Lk+EMk[1
p/summationtextp
l=1∥uk−ˆul
k∥2
2]. Puttingthingstogether,
we have that EMk[Lk+1]≤(1−α)τLk+ιk, whereιk=EMk[1
p/summationtextp
l=1∥uk−ˆul
k∥2
2−∥uk+1−ˆul
k,τ∥2
2],
whereα∈(0,1)is some convergence rate achieved by invoking Hypothesis (1).
3. We then use the small weight perturbation induced by over-parameterization, which means that
∥ˆul′
k−ˆul′
k,τ∥2is small. This allows us to bound the term ιk, and arrive at the final convergence.
5 Experiments
(a)
 (b)
(c)
 (d)
Figure 2: Validation experiments on a single hidden layer perceptron.
We empirically validate our main theorems on a one-hidden-layer, ReLU- activated neural network. With
the purpose of performing the experiments on a task that is both widely used and representative, and possess
some simplicity to be solved by the one-hidden-layer MLP, we use the features/embeddings extracted from
a convolutional-based neural network (Krizhevsky, 2009) . This is a common practice: E.g., the recent work
in Chowdhury et al. (2021) proposes the use of a library of pre-trained networks to extract useful features,
which later on are processed by added topmost layers used for classification. This results in a procedure
that takes an image, creates an embedding, and then uses that embedding to build a classifier, by feeding
the embedding into a multi-layer perceptron with a single/multiple hidden layers. In this work, we take a
12Published in Transactions on Machine Learning Research (08/2022)
ResNet-50 model (He et al., 2015) pretrained on ImageNet as our feature extractor and concatenate it with
two fully-connected layers. We then train this combined models on the CIFAR-10 dataset, and take the
outputs of the re-trained ResNet-50 model as the input features, and use the logits output of the combined
model as the labels. The obtained input feature has a dimension of 2048, and we choose a constant learning
rate and a sample size of 1000.
In Figure 2a, we plot the logarithm of the mean and variance of the training error dynamic with respect
to theK(for clarity, we only plot the first 160 iterations), which includes the sampling step, local training
steps, as well as the gradient aggregation step. Since the algorithm converges with a stable/similar manner
across all trials, the variance is too small to be observed from the figure. Notice that there are three types
of dynamics, as annotated in the figure: (1) A smooth decrease of training error: This corresponds to
subnetworks’ local training, which is supported by our theory that each subnetwork makes local progress.
(2)A sudden decrease of training error: This corresponds to the aggregation of locally-trained subnetworks,
and is consistent with our proof in Theorem 4. (3) A sudden increase of training error: This corresponds
to re-sampling subnetworks; according to our theory, the expected average training error increases after
sampling.
Figure 2b provides heatmap results that demonstrate the change of the error term as we vary the number
of subnetworks, and the selection probability. In Figure 2b, the subnetworks are generated using Bernoulli
masks, and the training process assumes a fixed number of local steps. Note that, as we fix the number of
subnetworks and increase the selection probability, the error decreases (lighter colors in heatmap). Moreover,
if we fix the number of selection probability and increase the number of subnetworks, the training error also
decreases. This is consistent with Theorem 3.
Figure 2c studies how the error term changes as we increase the width of the neural network. In Theorem
(3), we notice that the error term decreases as we choose a smaller initialization scale κ, while a smaller
κwould require a larger over-parameterization. This is consistent with our experiments in figure 2c: as
we increase the number of hidden neurons and adjust the initialization scale, we observe that the training
converges to a smaller error.
Figure 2d shows how the convergence rate changes as we increase the number of subnetworks under the
categorical mask assumption. In particular, the y-axis denotes the training error improvement in the
first, second, and third global step, respectively. The training error improvement is defined to be the
training error in last step −training error in current step
training error in last step. We observe that the training error improvement decreases
consistently across the first three global steps, as we increase the number of subnetworks. This corresponds
to what we have shown in Theorem (4), where 1−γdecreases as we increase p.
6 Conclusion
We prove linear convergence up to a neighborhood around the optimal point when training and combining
subnetworks in a single hidden-layer perceptron scenario. Our work extends results on dropout, multi-sample
dropout, and the Independent Subnet Training, and has broad implications on how the sampling method,
the number of subnetworks, and the number of local steps affect the convergence rate and the size of the
neighborhood around the optimal point. While our work focus on the single hidden-layer perceptron, we
considermulti-layerperceptronsasaninterestingdirection: weconjecturethatamorerefinedanalysisofeach
layer’s output is required (Du et al., 2019; Allen-Zhu et al., 2019b) . Moreover, focusing on the convergence
of a stochastic algorithm for our framework, as well as considering different losses (e.g., classification tasks or
even generic generalization losses) are interesting future research directions. Lastly, training with randomly
sampled subnetwork may result into a regularization benefit. Theoretically studying how the proposed
training scheme affects the generalization ability of the neural network is an interesting next step.
References
Zeyuan Allen-Zhu, Yuanzhi Li, and Yingyu Liang. Learning and generalization in overparameterized neural
networks, going beyond two layers, 2018. URL https://arxiv.org/abs/1811.04918 .
13Published in Transactions on Machine Learning Research (08/2022)
Zeyuan Allen-Zhu, Yuanzhi Li, and Yingyu Liang. Learning and generalization in overparameterized neural
networks, going beyond two layers. Advances in neural information processing systems , 2019a.
Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via over-
parameterization. In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.), Proceedings of the 36th Inter-
national Conference on Machine Learning , volume 97 of Proceedings of Machine Learning Research , pp.
242–252. PMLR, 09–15 Jun 2019b. URL https://proceedings.mlr.press/v97/allen-zhu19a.html .
Sanjeev Arora, Rong Ge, Behnam Neyshabur, and Yi Zhang. Stronger generalization bounds for deep nets
via a compression approach. In International Conference on Machine Learning , pp. 254–263. PMLR, 2018.
Sanjeev Arora, Simon S. Du, Wei Hu, Zhiyuan Li, and Ruosong Wang. Fine-grained analysis of optimization
and generalization for overparameterized two-layer neural networks, 2019.
Devansh Arpit, Stanisław Jastrzębski, Nicolas Ballas, David Krueger, Emmanuel Bengio, Maxinder S Kan-
wal, Tegan Maharaj, Asja Fischer, Aaron Courville, Yoshua Bengio, et al. A closer look at memorization
in deep networks. In International Conference on Machine Learning , pp. 233–242. PMLR, 2017.
Peter L Bartlett, Dylan J Foster, and Matus J Telgarsky. Spectrally-normalized margin bounds for neural
networks. Advances in Neural Information Processing Systems , 30:6240–6249, 2017.
Peter L Bartlett, Andrea Montanari, and Alexander Rakhlin. Deep learning: a statistical viewpoint. arXiv
preprint arXiv:2103.09177 , 2021.
Mikhail Belkin. Fit without fear: remarkable mathematical phenomena of deep learning through the prism
of interpolation. arXiv preprint arXiv:2105.14368 , 2021.
Mikhail Belkin, Daniel J Hsu, and Partha Mitra. Overfitting or perfect fitting? risk bounds for classification
and regression rules that interpolate. Advances in Neural Information Processing Systems , 31:2300–2311,
2018.
Mikhail Belkin, Daniel Hsu, Siyuan Ma, and Soumik Mandal. Reconciling modern machine-learning practice
and the classical bias–variance trade-off. Proceedings of the National Academy of Sciences , 116(32):15849–
15854, 2019.
Jinming Cao, Yangyan Li, Mingchao Sun, Ying Chen, Dani Lischinski, Daniel Cohen-Or, Baoquan
Chen, and Changhe Tu. Do-conv: Depthwise over-parameterized convolutional layer. arXiv preprint
arXiv:2006.12030 , 2020.
Arkabandhu Chowdhury, Mingchao Jiang, Swarat Chaudhuri, and Chris Jermaine. Few-shot image classi-
fication: Just use a library of pre-trained feature extractors and a simple classifier. In Proceedings of the
IEEE/CVF International Conference on Computer Vision , pp. 9445–9454, 2021.
Matthieu Courbariaux, Yoshua Bengio, and Jean-Pierre David. Binaryconnect: Training deep neural net-
works with binary weights during propagations. In Advances in neural information processing systems ,
pp. 3123–3131, 2015.
Yuyang Deng and Mehrdad Mahdavi. Local sgd optimizes overparameterized neural networks in polynomial
time.arXiv preprint arXiv:2107.10868 , 2021.
Simon Du, Jason Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai. Gradient descent finds global minima of
deep neural networks. In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.), Proceedings of the 36th
International Conference on Machine Learning , volume 97 of Proceedings of Machine Learning Research ,
pp. 1675–1685. PMLR, 09–15 Jun 2019. URL https://proceedings.mlr.press/v97/du19c.html .
Simon S Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh. Gradient descent provably optimizes over-
parameterized neural networks. In International Conference on Learning Representations , 2018.
Chen Dun, Cameron R. Wolfe, Christopher M. Jermaine, and Anastasios Kyrillidis. Resist: Layer-wise
decomposition of resnets for distributed training, 2021.
14Published in Transactions on Machine Learning Research (08/2022)
Gintare Karolina Dziugaite and Daniel M Roy. Computing nonvacuous generalization bounds for
deep (stochastic) neural networks with many more parameters than training data. arXiv preprint
arXiv:1703.11008 , 2017.
Cong Fang, Jason Lee, Pengkun Yang, and Tong Zhang. Modeling from features: a mean-field framework
for over-parameterized deep neural networks. In Conference on Learning Theory , pp. 1887–1936. PMLR,
2021.
William Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion parameter models
with simple and efficient sparsity. arXiv preprint arXiv:2101.03961 , 2021.
Vitaly Feldman. Does learning require memorization? a short tale about a long tail. In Proceedings of the
52nd Annual ACM SIGACT Symposium on Theory of Computing , pp. 954–959, 2020.
Spencer Frei, Yuan Cao, and Quanquan Gu. Algorithm-dependent generalization bounds for overparame-
terized deep residual networks. Advances in neural information processing systems , 2020.
Yarin Gal and Zoubin Ghahramani. Dropout as a bayesian approximation: Representing model uncertainty
in deep learning. In international conference on machine learning , pp. 1050–1059. PMLR, 2016.
Noah Golowich, Alexander Rakhlin, and Ohad Shamir. Size-independent sample complexity of neural net-
works. In Conference On Learning Theory , pp. 297–299. PMLR, 2018.
Ian Goodfellow, Yoshua Bengio, Aaron Courville, and Yoshua Bengio. Deep learning , volume 1. MIT Press,
2016.
Yihong Gu, Weizhong Zhang, Cong Fang, Jason D. Lee, and Tong Zhang 0001. How to characterize the
landscape of overparameterized convolutional neural networks. In Hugo Larochelle, Marc’Aurelio Ran-
zato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin (eds.), Advances in Neural Information
Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS
2020, December 6-12, 2020, virtual , 2020. URL https://proceedings.neurips.cc/paper/2020/hash/
2794f6a20ee0685f4006210f40799acd-Abstract.html .
Farzin Haddadpour and Mehrdad Mahdavi. On the convergence of local descent methods in federated
learning, 2019. URL https://arxiv.org/abs/1910.14425 .
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition,
2015. URL https://arxiv.org/abs/1512.03385 .
Baihe Huang, Xiaoxiao Li, Zhao Song, and Xin Yang. Fl-ntk: A neural tangent kernel-based framework for
federated learning convergence analysis, 2021.
Kaixuan Huang, Yuqing Wang, Molei Tao, and Tuo Zhao. Why do deep residual networks generalize better
than deep feedforward networks?—a neural tangent kernel perspective. Advances in Neural Information
Processing Systems , 33, 2020.
Hiroshi Inoue. Multi-sample dropout for accelerated training and better generalization. arXiv preprint
arXiv:1905.09788 , 2019.
Arthur Jacot, Franck Gabriel, and Clément Hongler. Neural tangent kernel: convergence and generalization
in neural networks. In Proceedings of the 32nd International Conference on Neural Information Processing
Systems, pp. 8580–8589, 2018.
Ziwei Ji and Matus Telgarsky. Polylogarithmic width suffices for gradient descent to achieve arbitrarily small
test error with shallow relu networks, 2020.
Kenji Kawaguchi, Leslie Pack Kaelbling, and Yoshua Bengio. Generalization in deep learning. arXiv preprint
arXiv:1710.05468 , 2017.
15Published in Transactions on Machine Learning Research (08/2022)
Ahmed Khaled, Konstantin Mishchenko, and Peter Richtárik. First analysis of local gd on heterogeneous
data, 2019a. URL https://arxiv.org/abs/1909.04715 .
Ahmed Khaled, Konstantin Mishchenko, and Peter Richtárik. Tighter theory for local sgd on identical and
heterogeneous data, 2019b. URL https://arxiv.org/abs/1909.04746 .
Durk P Kingma, Tim Salimans, and Max Welling. Variational dropout and the local reparameterization
trick.Advances in neural information processing systems , 28:2575–2583, 2015.
Alex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, 2009.
Alex Labach, Hojjat Salehinejad, and Shahrokh Valaee. Survey of dropout methods for deep neural networks.
arXiv preprint arXiv:1904.13310 , 2019.
Daniel LeJeune, Hamid Javadi, and Richard Baraniuk. The implicit regularization of ordinary least squares
ensembles. In International Conference on Artificial Intelligence and Statistics , pp. 3525–3535. PMLR,
2020.
Daniel LeJeune, Hamid Javadi, and Richard Baraniuk. The flip side of the reweighted coin: Dual-
ity of adaptive dropout and regularization. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang,
and J. Wortman Vaughan (eds.), Advances in Neural Information Processing Systems , volume 34, pp.
23401–23412. Curran Associates, Inc., 2021. URL https://proceedings.neurips.cc/paper/2021/
file/c4b8bb990423f770dd7f26ff79168416-Paper.pdf .
Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang, Maxim
Krikun, Noam Shazeer, and Zhifeng Chen. Gshard: Scaling giant models with conditional computation
and automatic sharding. arXiv preprint arXiv:2006.16668 , 2020.
Mingchen Li, Mahdi Soltanolkotabi, and Samet Oymak. Gradient descent with early stopping is provably
robust to label noise for overparameterized neural networks. In International conference on artificial
intelligence and statistics , pp. 4313–4324. PMLR, 2020.
Xiang Li, Kaixuan Huang, Wenhao Yang, Shusen Wang, and Zhihua Zhang. On the convergence of fedavg
on non-iid data, 2019. URL https://arxiv.org/abs/1907.02189 .
Tengyuan Liang, Tomaso Poggio, Alexander Rakhlin, and James Stokes. Fisher-Rao metric, geometry,
and complexity of neural networks. In The 22nd International Conference on Artificial Intelligence and
Statistics , pp. 888–896. PMLR, 2019.
Yiping Lu, Chao Ma, Yulong Lu, Jianfeng Lu, and Lexing Ying. A mean field analysis of deep ResNet and
beyond: Towards provably optimization via overparameterization from depth. In International Conference
on Machine Learning , pp. 6426–6436. PMLR, 2020.
Siyuan Ma, Raef Bassily, and Mikhail Belkin. The power of interpolation: Understanding the effectiveness
of SGD in modern over-parametrized learning. In International Conference on Machine Learning , pp.
3325–3334. PMLR, 2018.
Ryan Mcdonald, Mehryar Mohri, Nathan Silberman, Dan Walker, and Gideon S Mann. Efficient large-
scale distributed training of conditional maximum entropy models. In Advances in Neural Information
Processing Systems , pp. 1231–1239, 2009.
Poorya Mianjy and Raman Arora. On convergence and generalization of dropout training, 2020.
Amirkeivan Mohtashami, Martin Jaggi, and Sebastian U Stich. Simultaneous training of partially masked
neural networks. arXiv preprint arXiv:2106.08895 , 2021.
Behnam Neyshabur, Srinadh Bhojanapalli, David Mcallester, and Nati Srebro. Exploring generalization in
deep learning. Advances in Neural Information Processing Systems , 30:5947–5956, 2017.
16Published in Transactions on Machine Learning Research (08/2022)
Behnam Neyshabur, Srinadh Bhojanapalli, and Nathan Srebro. A PAC-Bayesian approach to spectrally-
normalized margin bounds for neural networks. In International Conference on Learning Representations ,
2018.
Quynh Nguyen. On the proof of global convergence of gradient descent for deep relu networks with linear
widths, 2021. URL https://arxiv.org/abs/2101.09612 .
Samet Oymak and Mahdi Soltanolkotabi. Overparameterized nonlinear learning: Gradient descent takes the
shortest path? In International Conference on Machine Learning , pp. 4951–4960. PMLR, 2019.
Samet Oymak and Mahdi Soltanolkotabi. Toward moderate overparameterization: Global convergence
guarantees for training shallow neural networks. IEEE Journal on Selected Areas in Information Theory ,
1(1):84–105, 2020.
BenjaminRecht, RebeccaRoelofs, LudwigSchmidt, andVaishaalShankar. DoImageNetclassifiersgeneralize
to ImageNet? In International Conference on Machine Learning , pp. 5389–5400. PMLR, 2019.
Albert Senen-Cerda and Jaron Sanders. Asymptotic convergence rate of dropout on shallow linear neural
networks, 2020a.
Albert Senen-Cerda and Jaron Sanders. Almost sure convergence of dropout algorithms for neural networks,
2020b.
VatsalShah, SoumyaBasu, AnastasiosKyrillidis, andSujaySanghavi. Ongeneralizationofadaptivemethods
for over-parameterized linear regression. arXiv preprint arXiv:2011.14066 , 2020.
Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff
Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. arXiv preprint
arXiv:1701.06538 , 2017.
Mahdi Soltanolkotabi, Adel Javanmard, and Jason D Lee. Theoretical insights into the optimization land-
scape of over-parameterized shallow neural networks. IEEE Transactions on Information Theory , 65(2):
742–769, 2018.
ChaehwanSong, AliRamezani-Kebrya, ThomasPethick, ArminEftekhari, andVolkanCevher. Subquadratic
overparameterization for shallow neural networks, 2021. URL https://arxiv.org/abs/2111.01875 .
Zhao Song and Xin Yang. Quadratic suffices for over-parametrization via matrix chernoff bound, 2020.
Daniel Soudry, Elad Hoffer, Mor Shpigel Nacson, Suriya Gunasekar, and Nathan Srebro. The implicit bias
of gradient descent on separable data. The Journal of Machine Learning Research , 19(1):2822–2878, 2018.
Stefano Spigler, Mario Geiger, Stéphane d’Ascoli, Levent Sagun, Giulio Biroli, and Matthieu Wyart. A
jamming transition from under-to over-parametrization affects generalization in deep learning. Journal of
Physics A: Mathematical and Theoretical , 52(47):474001, 2019.
Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: a
simple way to prevent neural networks from overfitting. The journal of machine learning research , 15(1):
1929–1958, 2014.
Sebastian U. Stich. Local sgd converges fast and communicates little, 2018. URL https://arxiv.org/abs/
1805.09767 .
Lili Su and Pengkun Yang. On learning over-parameterized neural networks: A functional approximation
perspective, 2019.
Mariya Toneva, Alessandro Sordoni, Remi Tachet des Combes, Adam Trischler, Yoshua Bengio, and Ge-
offrey J Gordon. An empirical study of example forgetting during deep neural network learning. In
International Conference on Learning Representations , 2018.
17Published in Transactions on Machine Learning Research (08/2022)
Thijs Vogels, Sai Praneeth Karinireddy, and Martin Jaggi. PowerSGD: Practical low-rank gradient compres-
sion for distributed optimization. Advances In Neural Information Processing Systems 32 (Nips 2019) , 32
(CONF), 2019.
Li Wan, Matthew Zeiler, Sixin Zhang, Yann Le Cun, and Rob Fergus. Regularization of neural networks
using dropconnect. In International conference on machine learning , pp. 1058–1066. PMLR, 2013.
Hongyi Wang, Saurabh Agarwal, and Dimitris Papailiopoulos. Pufferfish: Communication-efficient models
at no extra cost. Proceedings of Machine Learning and Systems , 3, 2021.
Sida Wang and Christopher Manning. Fast dropout training. In international conference on machine
learning, pp. 118–126. PMLR, 2013.
Cameron R Wolfe, Jingkang Yang, Arindam Chowdhury, Chen Dun, Artun Bayer, Santiago Segarra, and
Anastasios Kyrillidis. GIST: Distributed training for large-scale graph convolutional networks. arXiv
preprint arXiv:2102.10424 , 2021.
Tianyi Yao, Daniel LeJeune, Hamid Javadi, Richard G Baraniuk, and Genevera I Allen. Minipatch learning
as implicit ridge-like regularization. In 2021 IEEE International Conference on Big Data and Smart
Computing (BigComp) , pp. 65–68. IEEE, 2021.
Jiahui Yu, Linjie Yang, Ning Xu, Jianchao Yang, and Thomas Huang. Slimmable neural networks. In
International Conference on Learning Representations , 2018.
Binhang Yuan, Cameron R. Wolfe, Chen Dun, Yuxin Tang, Anastasios Kyrillidis, and Christopher M.
Jermaine. Distributed learning of deep neural networks using independent subnet training, 2020.
Ce Zhang and Christopher Ré. Dimmwitted: A study of main-memory statistical analytics. Proceedings of
the VLDB Endowment , 7(12):1283–1294, 2014.
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep
learning (still) requires rethinking generalization. Communications of the ACM , 64(3):107–115, 2021.
Jian Zhang, Christopher De Sa, Ioannis Mitliagkas, and Christopher Ré. Parallel SGD: When does averaging
help?arXiv preprint arXiv:1606.07365 , 2016.
Wenda Zhou, Victor Veitch, Morgane Austern, Ryan P Adams, and Peter Orbanz. Non-vacuous generaliza-
tion bounds at the ImageNet scale: a PAC-Bayesian compression approach. In International Conference
on Learning Representations , 2018.
Martin Zinkevich, Markus Weimer, Lihong Li, and Alex J Smola. Parallelized stochastic gradient descent.
InAdvances in neural information processing systems , pp. 2595–2603, 2010.
Difan Zou, Yuan Cao, Dongruo Zhou, and Quanquan Gu. Gradient descent optimizes over-parameterized
deep ReLU networks. Machine Learning , 109(3):467–492, 2020.
18Published in Transactions on Machine Learning Research (08/2022)
A Notation
Table 1: Notations
SYMBOL DESCRIPTION MATHEMATICAL DEFINITION
KNumber of global iterations K∈N+
kIndex of global iterations k∈[K]
τNumber of local iterations τ∈N+
tIndex of local iterations t∈[τ]
pNumber of subnetworks p∈N+
lIndex of subnetworks l∈[p]
ξProbability of selecting a neuron ξ∈(0,1]
ξ Vector probability of selection a
neuron by each workerξ∈(0,1]p
η Constant step size for local
gradient updateη∈R
MkBinary mask in iteration k Mk∈{0,1}p×m
mk,r Binary mask for neuron r in
iterationkmk,r∈{0,1}p, the vector of rth column of Mk
ml
kBinary mask for subnetwork lin
iterationkml
k∈{0,1}m, the vector of lth row of Mk
ml
k,rBinary mask for neuron rin
subnetwork lin iteration kml
k,r∈{0,1}the(l,r)th entry of Mk
Xk,r Number of subnetworks
selecting neuron rin iteration kXk,r=/summationtextp
l=1ml
k,r
Nk,r Aggregated gradient normalizer
for neuronrin iteration kNk,r= max{Xk,r,1}
N⊥
k,rIndicator of gradient existing for
neuronrin iteration kN⊥
k,r= min{Xk,r,1}
ηk,r Global gradient aggregation step
size for neuron rin iteration kηk,r=N⊥
k,r/Nk,r
u(i)
kOutput of the whole network at
global iteration kfor sampleiu(i)
k=ξ√m/summationtextm
r=1arσ(⟨wk,r,xi⟩)
ukOutput of the whole network at
global iteration kfor all Xuk=/bracketleftig
u(1)
k,...,u(n)
k/bracketrightig
ˆul(i)
k,tOutput of subnetwork lat
iteration (k,t)for sampleiˆul(i)
k,t=1√m/summationtextm
r=1arml
k,rσ/parenleftig/angbracketleftig
wl
k,t,r,xi/angbracketrightig/parenrightig
ˆul
k,tOutput of subnetwork lat
iteration (k,t)for all Xˆul
k,t=/bracketleftig
ˆul(1)
k,t,..., ˆul(n)
k,t/bracketrightig
ˆul(i)
kOutput of subnetwork lat
iteration (k,0)for sampleiˆul(i)
k= ˆul(i)
k,0
ˆul
kOutput of subnetwork lat
iteration (k,0)for all Xˆul
k=ˆul
k,0
LkGlobal loss at iteration k Lk=∥y−uk∥2
2
Lml
k(Wl
k,t)Local loss for subnetwork lat
iteration (k,t)Lml
k(Wl
k,t) =∥y−fml
k(Wl
k,t)∥2
2
19Published in Transactions on Machine Learning Research (08/2022)
B Preliminary and Definition
In the proofs of our theorems, we use extensively the following tools.
Definition 2. (Sub-Gaussian Random Variable) A random variable Xisκ2-sub-Gaussian if
E[etx]≤eκ2t2
2
Definition 3. (Sub-Exponential Random Variable) A random variable with mean E[X] =µis(κ′,α)-sub-
exponential if there exists non-negative (κ′,α)such that for all t≤α−1
E[et(X−µ)]≤e−t2κ′2
2
Property 1. (Sub-Exponential Tail Bound) For a (κ′,α)-sub-exponential random variable XwithE[X] =µ
we have
P(X >µ +t)≤/braceleftigg
e−t2
2κ′2if0≤t≤κ′2
α
e−t2
2αift>κ′2
α
Property 2. (Markov’s Inequality) For a non-negative random variable X, we have
P(X≥a)≤1
aE[X]
Property3. (Hoeffding’s Inequality for Bounded Random Variables) Let X1,...,Xnbe independent random
variables bounded by |Xi|≤1for alli∈[n]. Then we have
P/parenleftigg/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle1
nn/summationdisplay
i=1Xi/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle≥t/parenrightigg
≤e−2nt2
Property 4. (Berstein’s Inequality) Let X1,...,Xnbe random variables with E[Xi] = 0for alli∈[n]. If
|Xi|≤Malmost surely, then
P/parenleftiggn/summationdisplay
i=1Xi>t/parenrightigg
≤e−t2/2/summationtextn
j=1E[X2
j]+Mt/3
Property 5. (Jensen’s Inequality for Expectation) For a non-negative random variable X, we have
E/bracketleftig
X1
2/bracketrightig
≤(E[X])1
2
Apart from the properties above, we also need the following definitions to facilitate our analysis. First, we
note that, in the following proofs, we let R=κλ0
192n. Define
Air={∃w∈B(w0,r,R) :I{⟨w,xi⟩≥0}̸=I{⟨w0,r,xi⟩≥0}}
to denote the event that for sample xi, the activation pattern of neuron rmay change through training if
the weight vector change is bounded in the R-ball centered at initialization. Moreover, let
Si={r∈[m] :¬Air}
S⊥
i= [m]\Si
to be the set of neurons whose activation pattern does not change for sample xiif the weight vector change
is bounded by the R-ball centered at initialization. Moreover, since we are interested in the loss dynamic
computed on the following function
u(i)
k=ξ√mm/summationdisplay
r=1arσ(⟨wk,r,xi⟩).
we denote the full gradient of loss with respect to each weight vector wras
∂L(Wk)
∂wr=ξ√mn/summationdisplay
i=1arxi(u(i)
k−yi)I{⟨wk,r,xi⟩≥0}
20Published in Transactions on Machine Learning Research (08/2022)
C Proof of Theorem 2
Recall the definition of masked-NTK
(ml
k′◦H(k,t))ij=1
m⟨xi,xj⟩m/summationdisplay
r=1ml
k′,rI{⟨wk,t,r,xi⟩≥0,⟨wk,t,r,xi⟩≥0}
To start with, we fix k′∈[K],l∈[p], andi,j∈[n]. In this case, let
hr=ml
k′,r⟨xi,xj⟩I{⟨w0,r,xi⟩≥0,⟨w0,r,xi⟩≥0}
Then we have
(ml
k′◦H(0,0))ij=1
mm/summationdisplay
r=1hr
Also we have
EMk,W[hr] =Ew∼N(0,I)[EMk[hr]] =H∞
ij
Note that for all rwe have|hr|≤1. Thus we apply Hoeffding’s inequality for bounded random variables
and get
P/parenleftbig/vextendsingle/vextendsingle(ml
k′◦H(0,0))ij−H∞
ij/vextendsingle/vextendsingle≥t/parenrightbig
=P/parenleftigg/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle1
mm/summationdisplay
r=1hr−H∞
ij/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle≥t/parenrightigg
≤2e−2mt2
Apply a union bound over i,jgives that with probability at least 1−2n2e−2mt2it holds that
/vextendsingle/vextendsingle(ml
k′◦H(0,0))ij−H∞
ij/vextendsingle/vextendsingle≤t
for alli,j∈[n]. Therefore,
/vextenddouble/vextenddoubleml
k′◦H(0,0)−H∞/vextenddouble/vextenddouble2
2≤/vextenddouble/vextenddoubleml
k′◦H(0,0)−H∞/vextenddouble/vextenddouble2
F
≤n/summationdisplay
i,j=1/vextendsingle/vextendsingle(ml
k′◦H(0,0))ij−H∞
ij/vextendsingle/vextendsingle2
≤n2t2
Lett=λ0
4ngives
/vextenddouble/vextenddoubleml
k′◦H(0,0)−H∞/vextenddouble/vextenddouble
2≤λ0
4
holds with probability at least 1−2n2e−mλ2
0
8n2. Next we show that for all k∈[k]andt∈[τ], as long as
∥wk,t,r−w0,r∥2≤Rfor allr∈[m], then it holds that
/vextenddouble/vextenddoubleml
k′◦H(k,t)−ml
k′◦H(0,0)/vextenddouble/vextenddouble
2≤2nκ−1R
Following the argument of (Song & Yang, 2020) , lemma 3.2, we have
/vextenddouble/vextenddoubleml
k′◦H(k,t)−ml
k′◦H(0,0)/vextenddouble/vextenddouble2
F≤1
m2n/summationdisplay
i,j=1/parenleftiggm/summationdisplay
r=1sr,i,j/parenrightigg2
with
sr,i,j=ml
k′,r(I{⟨w0,r,xi⟩≥0;⟨w0,r,xi⟩≥0}−I{⟨wr,k,t,xj⟩≥0;⟨wr,k,t,xj⟩≥0})
21Published in Transactions on Machine Learning Research (08/2022)
Thensr,i,j= 0if¬Airand¬Ajrhappend. In other cases we have |sr,i,j|≤1. Thus we have that for all
i,j∈[n]
EMk,W0[sr,i,j] =ξP(Air∪Ajr)≤4ξR
κ√
2π≤2ξκ−1R
and
EMk,W0/bracketleftig
(sr,i,j−EMk,W0[sr,i,j])2/bracketrightig
≤EMk,w0,r[s2
r,i,j]≤4ξR
κ√
2π≤2ξκ−1R
Thus applying Bernstein inequality with t=ξκ−1Rgives
P/parenleftigg
1
m2m/summationdisplay
r=1sr,i,j≥3ξκ−1R/parenrightigg
≤exp/parenleftbigg
−mξR
10κ/parenrightbigg
Therefore, taking a union bound gives that, with probability at least 1−n2e−mξR
10κwe have that
/vextenddouble/vextenddoubleml
k′◦H(k,t)−ml
k′◦H(0,0)/vextenddouble/vextenddouble
2≤/vextenddouble/vextenddoubleml
k′◦H(k,t)−ml
k′◦H(0,0)/vextenddouble/vextenddouble
F≤3ξnκ−1R
UsingR≤κλ0
12ngives∥ml
k′◦H(k,t)−ml
k′◦H(0,0)∥2≤ξλ0
4≤λ0
4with probability at least 1−n2e−mξλ0
12n.
Therefore, we have
/vextenddouble/vextenddoubleml
k′◦H(k,t)−H∞/vextenddouble/vextenddouble
2≤λ0
2
which implies that λmin/parenleftbig
ml
k′◦H(k,t)/parenrightbig
≥λ0
2holds with probability at least 1−n2/parenleftbigg
e−mξλ0
12n−2e−mλ2
0
8n2/parenrightbigg
for a fixed k′∈[K]andl∈[p]. Taking a union bound over all k′andland plugging in the requirement
m= Ω/parenleftig
n2logKpn/δ
ξλ0/parenrightig
gives the desired result.
22Published in Transactions on Machine Learning Research (08/2022)
D Proof of Hypothesis 1
In this proof, we follow the idea of (Du et al., 2018) . However, the difference is that i)we use our masked-
NTK during the analysis, and ii)we use a different technique for bounding the weight perturbation. We
repeat the key requirement stated in the theorem here: for all r∈[m]
∥wk,r−w0,r∥2+ 2ητ/radicalbigg
nK
mδE[Mk−1],W0,a[∥y−uk∥2] + (K−k)κ/radicalbig
ξ(1−ξ)pn≤R (10)
To start, we notice that, using the required over-parameterization, lemma 23 holds with probability at least
1−O(δ). We use induction on the following two conditions to prove the theorem:
/vextenddouble/vextenddoubley−ul
k,t+1/vextenddouble/vextenddouble2
2≤/parenleftbigg
1−ηλ0
2/parenrightbigg/vextenddouble/vextenddoubley−ul
k,t/vextenddouble/vextenddouble2
2(11)
/vextenddouble/vextenddoublewl
k,t,r−wk,r/vextenddouble/vextenddouble≤ητ√
2nK√
mδE[Mk−1],W0,a[∥y−uk∥2] + 2ηκn/radicalbigg
2ξ(1−ξ)pK
mδ(12)
∥wl
k,t,r−w0,r∥≤R (13)
Base Case : For the case of t= 0, we notice that equation (11) and (12) naturally holds. Moreover, equation
(10) implies equation (12).
Inductive Case : the inductive case is divided into three parts.
(12)→(13): assume that equation (12) holds in local iteration t. Combine the result with equation (10)
gives that equation (13) holds in iteration t.
(13→(11): assume that equation (13) holds in local iteration t. We are going to prove that equation (11)
holds. In particular, we are interested in
∥y−ˆul
k,t+1∥2
2=∥y−ˆul
k,t∥2
2−2/angbracketleftbig
y−ul
k,t,ˆul
k,t+1−ˆul
k,t/angbracketrightbig
+∥ˆul
k,t+1−ˆul
k,t∥2
2
We define ˆul(i)
k,t+1−ˆul(i)
k,t=Il(i)
1,k,t+Il(i)
2,k,twith
Il(i)
1,k,t=1√m/summationdisplay
r∈Siarml
k,r/parenleftbig
σ/parenleftbig/angbracketleftbig
wl
k,t+1,r,xi/angbracketrightbig/parenrightbig
−σ/parenleftbig/angbracketleftbig
wl
k,t,r,xi/angbracketrightbig/parenrightbig/parenrightbig
Il(i)
2,k,t=1√m/summationdisplay
r∈S⊥
iarml
k,r/parenleftbig
σ/parenleftbig/angbracketleftbig
wl
k,t+1,r,xi/angbracketrightbig/parenrightbig
−σ/parenleftbig/angbracketleftbig
wl
k,t,r,xi/angbracketrightbig/parenrightbig/parenrightbig
and notice that, with the 1-Lipchitzness of ReLU,
/vextendsingle/vextendsingle/vextendsingleIl(i)
2,k,t/vextendsingle/vextendsingle/vextendsingle≤1√m/summationdisplay
r∈S⊥
i/vextendsingle/vextendsingleσ/parenleftbig/angbracketleftbig
wl
k,t+1,r,xi/angbracketrightbig/parenrightbig
−σ/parenleftbig/angbracketleftbig
wl
k,t,r,xi/angbracketrightbig/parenrightbig/vextendsingle/vextendsingle
≤1√m/summationdisplay
r∈S⊥
i/vextenddouble/vextenddoublewl
k,t+1,r−wk,t,r/vextenddouble/vextenddouble
2
≤η√m/summationdisplay
r∈S⊥
i/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble∂Lml
k/parenleftig
Wl
k,t/parenrightig
∂wr/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble
2
≤η√n
m/summationdisplay
r∈S⊥
i/vextenddouble/vextenddoubley−ˆul
k,t/vextenddouble/vextenddouble
2
≤4ηκ−1√nR∥y−ˆul
k,t∥2
23Published in Transactions on Machine Learning Research (08/2022)
where the last inequality uses |S⊥
i|≤4mκ−1Rfrom Lemma 16. Therefore,
/vextendsingle/vextendsingle/angbracketleftbig
y−ˆul
k,t,Il
2,k,t/angbracketrightbig/vextendsingle/vextendsingle≤√nmax
i∈[n]/vextendsingle/vextendsingle/vextendsingleIl(i)
2,k,t/vextendsingle/vextendsingle/vextendsingle·∥y−ˆul
k,t∥2≤4ηκ−1nR∥y−ˆul
k,t∥2
2
Similarly, we have
/parenleftig
ˆul(i)
k,t+1−ˆul(i)
k,t/parenrightig2
≤1
m/parenleftiggm/summationdisplay
r=1/vextenddouble/vextenddoublewl
k,t+1,r−wk,t,r/vextenddouble/vextenddouble
2/parenrightigg2
≤m/summationdisplay
r=1/vextenddouble/vextenddoublewl
k,t+1,r−wk,t,r/vextenddouble/vextenddouble2
2
≤η2m/summationdisplay
r=1/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble∂Lml
k/parenleftig
Wl
k,t/parenrightig
∂wr/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
2
≤η2n2∥y−ˆul
k,t∥2
2
Lastly, we define ml
k◦H(k,t)⊥with
/parenleftbig
ml
k◦H(k,t)⊥/parenrightbig
ij=1
m⟨xi,xj⟩/summationdisplay
r∈S⊥
iml
k,rI{⟨wk,r,xi⟩≥0,⟨wk,r,xj⟩≥0}
and we have
Il(i)
1,k,t=1√m/summationdisplay
r∈Siarml
k,r/angbracketleftbig
wl
k,t+1,r−wl
k,t,r,xi/angbracketrightbig
I{⟨wk,r,xi⟩≥0}
=−η√m/summationdisplay
r∈Siarml
k,r/angbracketleftigg∂Lml
k/parenleftig
Wl
k,t/parenrightig
∂wr,xi/angbracketrightigg
I{⟨wk,r,xi⟩≥0}
=η
m/summationdisplay
r∈Sin/summationdisplay
j=1ml
k,r/parenleftig
yj−ˆul(j)
k,t/parenrightig
⟨xi,xj⟩I{⟨wk,r,xi⟩≥0,⟨wk,r,xj⟩≥0}
=ηn/summationdisplay
j=1/parenleftbig
ml
k◦H(k,t)−ml
k◦H(k,t)⊥/parenrightbig
ij/parenleftig
yj−ˆul(j)
k,t/parenrightig
Therefore,
/angbracketleftbig
y−ˆul
k,t,Il
1,k,t/angbracketrightbig
=ηn/summationdisplay
i,j=1/parenleftig
yi−ˆul(i)
k,t/parenrightig/parenleftbig
ml
k◦H(k,t)−ml
k◦H(k,t)⊥/parenrightbig
ij/parenleftig
yj−ˆul(j)
k,t/parenrightig
=η/angbracketleftbig
y−ˆuk,t,/parenleftbig
ml
k◦H(k,t)−ml
k◦H(k,t)⊥/parenrightbig
(y−ˆuk,t)/angbracketrightbig
≥ηλ0
2∥y−ˆul
k,t∥2
2−η∥ml
k◦H(k,t)⊥∥2∥y−ˆul
k,t∥2
2
≥/parenleftbiggηλ0
2−4ηκ−1nR/parenrightbigg
∥y−ˆul
k,t∥2
2
where the last inequality follows from the fact that
∥ml
k◦H(k,t)⊥∥2
2≤∥ml
k◦H(k,t)⊥∥2
F
=1
m2n/summationdisplay
i,j=1
⟨xi,xj⟩/summationdisplay
r∈S⊥
iI{⟨wk,r,xi⟩≥0,⟨wk,r,xj⟩≥0}
2
≤n2
m2|S⊥
i|2
= 16n2κ−2R2
24Published in Transactions on Machine Learning Research (08/2022)
Putting things together gives
∥y−ˆul
k,t+1∥2
2≤/parenleftbig
1−ηλ0+ 16ηκ−1nR+η2n2/parenrightbig
∥y−ˆul
k,t∥2
2
ChooseR≤κλ0
64nandη≤λ0
4n2gives
∥y−ˆul
k,t+1∥2
2≤/parenleftbigg
1−ηλ0
2/parenrightbigg
∥y−ˆul
k,t∥2
2
(11)→(12): Assume that equation (11) holds for local iteration 0,...,t. We are going to prove equation
(12) for local iteration t+ 1. We start by noticing that equation (11) implies that for all local iteration
t′∈[t], we have that∥y−ˆul
k,t′∥2≤∥y−ˆul
k∥2. Moreover, we notice that
/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble∂Lmk/parenleftig
Wl
k,t/parenrightig
∂wr/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble
2≤1√mn/summationdisplay
i=1/vextenddouble/vextenddouble/vextenddouble/parenleftig
ˆul,(i)
k,t−yi/parenrightig
arml
k,rxiI{/angbracketleftbig
wl
k,t,r,xi/angbracketrightbig
}/vextenddouble/vextenddouble/vextenddouble
2
≤1√mn/summationdisplay
i=1/vextendsingle/vextendsingle/vextendsingleˆul,(i)
k,t−yi/vextendsingle/vextendsingle/vextendsingle
≤√n√m/vextenddouble/vextenddoubley−ˆul
k,t/vextenddouble/vextenddouble
2
Therefore,
∥wk,t,r−wk,r∥2≤ηt−1/summationdisplay
t′=0/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble∂Lmk/parenleftig
Wl
k,t/parenrightig
∂wr/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble
2
≤η√n√mt−1/summationdisplay
t′=0∥y−ˆul
k,t′∥2
≤ητ√n√m∥y−ˆul
k∥2
≤ητ√n√m/parenleftbig
∥y−uk∥2+∥uk−ˆul
k∥2/parenrightbig
Applying Markov’s inequality to the global convergence, with probabiltiy at least 1−δ
2K, it holds that
∥y−uk∥2≤/radicalbig
2K/δE[Mk−1],W0,a[∥y−uk∥2]
By Lemma 25, we have
EMk/bracketleftbig
∥uk−ˆul
k∥2
2/bracketrightbig
≤4ξ(1−ξ)nκ2
Thus with probability at least 1−δ
2pKit holds that
∥uk−ˆul
k∥2≤2κ/radicalbig
2ξ(1−ξ)npK/δ
Plugging in gives
∥wk,t,r−wk,r∥≤ητ√
2nK√
mδE[Mk−1],W0,a[∥y−uk∥2] + 2ητκn/radicalbigg
2ξ(1−ξ)pK
mδ
which completes the proof.
25Published in Transactions on Machine Learning Research (08/2022)
E Proof of Theorem 3
Before we start the proof, we introduce several notations. Define
I(i)
1,k=ξ√m/summationdisplay
r∈Siar(σ(⟨wk+1,r,xi⟩)−σ(⟨wk,r,xi⟩))
I(i)
2,k=ξ√m/summationdisplay
r∈S⊥
iar(σ(⟨wk+1,r,xi⟩)−σ(⟨wk,r,xi⟩))
Let
I1,k=/bracketleftig
I(1)
1,k,...,I(n)
1,k/bracketrightig
and similarly,
I2,k=/bracketleftig
I(1)
2,k,...,I(n)
2,k/bracketrightig
Then we have u(i)
k+1−u(i)
k=I(i)
1+I(i)
2anduk+1−uk=I1,k+I2,k. Also, we define H(k)⊥to be
H(k)⊥
ij=ξ
m/summationdisplay
r∈S⊥
i⟨xi,xj⟩I{⟨wk,r,xi⟩≥0,⟨wk,r,xj⟩≥0}
Fork-th global iteration, first local iteration, we define the mixing gradient as
gk,r=ηk,rp/summationdisplay
l=1∂Lml
k/parenleftig
Wl
k,0/parenrightig
∂wr
=ηk,rp/summationdisplay
l=1∂Lml
k(Wk)
∂wr
=ηk,r√mp/summationdisplay
l=1n/summationdisplay
i=1ml
k,r(ˆul(i)
k−yi)arxiI{⟨wk,r,xi⟩≥0}
=1√mm/summationdisplay
i=1(f(i)
k,r−N⊥
k,ryi)arxiI{⟨wk,r,xi⟩≥0}
where we define the mixing function as
f(i)
k,r=ηk,rp/summationdisplay
l=1ml
k,rˆul(i)
k
As usual, we let fk,r=/bracketleftig
f(1)
k,r,...,f(n)
k,r/bracketrightig
. We note that f(i)
k,rhas the form
f(i)
k,r=ηk,rp/summationdisplay
l=1ml
k,rˆu(l(i)
k
=1√mm/summationdisplay
r′=1ar/parenleftigg
ηk,rp/summationdisplay
l=1ml
k,rml
k,r′/parenrightigg
σ(⟨wk,r,xi⟩)
Letνk,r,r′=ηk,r/summationtextp
l=1ml
k,rml
k,r′. The mixing function reduce to the form
f(i)
k,r=1√mm/summationdisplay
r=1arνk,r,r′σ(⟨wk,r,xi⟩)
26Published in Transactions on Machine Learning Research (08/2022)
Also, note that if N⊥
k,r= 0, we haveνk,r,r′= 0. We prove Theorem 3 by a fashion of induction, with the
two conditions we consider stated below:
E[Mk−1]/bracketleftbig
∥y−uk∥2
2/bracketrightbig
≤/parenleftbigg
1−1
4ηθτλ 0/parenrightbiggk
∥y−u0∥2
2+B1 (14)
∥wk,r−w0,r∥2+ 2ητ/radicalbigg
2nK
mδ/parenleftbigg4
ηθτλ 0E[Mk−1][∥y−uk∥2] + (K−k)B/parenrightbigg
≤R (15)
/vextenddouble/vextenddoublewl
k,t,r−w0,r/vextenddouble/vextenddouble
2≤R (16)
with
B=/radicaligg
4B1
ηθτλ 0+κ/radicalbig
ξ(1−ξ)pn
Base Case: Note that equation (14) and equation (16) holds naturally for t= 0. To show that equation
(15) holds, we need to use the over-parameterization property. In particular, we want to show that
2ητ/radicalbigg
2nK
mδ/parenleftbigg4
ηθτλ 0EW0,a[∥y−u0∥2] +KB/parenrightbigg
≤R≤κλ0
144n
Apply lemma 26 and move the factor on the left hand side of the equation to the right. Then we equivalently
want
κλ0
ητ/radicalbigg
mδ
n3K= Ω/parenleftbigg
max/braceleftbigg4C2n
ηθτλ 0,KB/bracerightbigg/parenrightbigg
Plugging in the value of Band the requirement of η, and solve for mto get that
m= Ω/parenleftbiggK
δmax/braceleftbiggn4
κ2ξθλ4
0,nK2B1
κ2θλ2
0,K2p/bracerightbigg/parenrightbigg
Inductive Case: again the inductive case is divided into three parts.
(15)→(16)Observe that B≥κ/radicalbig
ξ(1−ξ)pn. Thus, if equation (15) is satisfied and the over-
parameterization requirement holds, then Hypothesis (1) holds. Thus equation (16) holds naturally.
(14)→(15). Assume that equation (14) holds, and (15)holds for global iteration k, we want to show that
equation (15) holds for global iteration t+ 1. In particular, we would like to show that
∥wk+1,r−w0,r∥2+ 2ητ/radicalbigg
2nK
mδ/parenleftbigg4
ηθτλ 0E[Mk],W0,a[∥y−uk+1∥2] + (K−k−1)B/parenrightbigg
≤R
it suffice to show that
∥wk+1,r−wk,r∥2≤ητ/radicalbigg
2nK
mδE[Mk−1],W0,a[∥y−uk∥2] + 2ητ/radicalbigg
2nK
mδB−2ητ/radicaligg
8nKB 1
mδηθτλ 0
Recall the definition of Bas
B=/radicalbigg
B1
α+κ/radicalbig
ξ(1−ξ)pn
It then suffice to show that
∥wk+1,r−w0,r∥2≤ητ/radicalbigg
2nK
mδE[Mk−1],W0,a[∥y−uk∥2] + 2ητκn/radicalbigg
2ξ(1−ξ)pK
mδ
27Published in Transactions on Machine Learning Research (08/2022)
Note that under these two conditions and the over-parameterization requirement, Hypothesis (1) holds.
Thus, we have
∥wl
k,t,r−wk,r∥2≤ητ/radicalbigg
2nK
mδE[Mk−1],W0,a[∥y−uk∥2] + 2ητκn/radicalbigg
2ξ(1−ξ)pK
mδ
for alll∈[p]andt∈[τ]. Using the definition that ηk,r=N⊥
k,r
Nk,r, we have
∥wk+1,r−wk,r∥2≤ηk,rp/summationdisplay
l=1ml
k,r∥wl
k,τ,r−wk,r∥2
≤ητ/radicalbigg
2nK
mδE[Mk−1],W0,a[∥y−uk∥2] + 2ητκn/radicalbigg
2ξ(1−ξ)pK
mδ
(16), (15)→(14)Assume that equation (16) and (15) holds for iteration k. We want to show (14) for up
to iteration k+ 1. Under these conditions, we have that with probability at least 1−4δ, Hypothesis 1 holds.
Throughout this proof, we assume that
n/summationdisplay
i=1m/summationdisplay
r′=1⟨w0,r,xi⟩2≤2mnκ2−mnR2
and that
∥W0∥F≤√
2md−√mR
Note that Lemma 22 and Lemma 23 shows that, as long as m= Ω/parenleftbig
logn
δ/parenrightbig
, the above assumption holds with
probability at least 1−δover initialization. Moreover, Lemma 16 shows that as long as m=/parenleftignlogn
δ
ξλ0/parenrightig
, with
probability at least 1−δover initialization we have
|S⊥
i|≤4mκ−1R
To start, expanding the loss at iteration k+ 1gives
EMk/bracketleftbig
∥y−uk+1∥2
2/bracketrightbig
=∥y−uk∥2
2−2⟨y−uk,EMk[uk+1−uk]⟩+EMk/bracketleftbig
∥uk+1−uk∥2
2/bracketrightbig
=∥y−uk∥2
2−2⟨y−uk,EMk[I1,k]⟩−2⟨y−uk,EMk[I2,k]⟩+
EMk/bracketleftbig
∥uk+1−uk∥2
2/bracketrightbig
Following previous work, we bound the second, third, and fourth term separately. However, the second term
requires a more detailed analysis. In particular, we let
I′
1,k=I1,k−ηθτH(k)(y−uk)
Then the loss at iteration k+ 1has the form
EMk/bracketleftbig
∥y−uk+1∥2
2/bracketrightbig
=∥y−uk∥2
2−2ηθτ⟨y−uk,H(k)(y−uk)⟩+EMk/bracketleftbig
∥uk+1−uk∥2
2/bracketrightbig
−
2/angbracketleftbig
y−uk,EMk/bracketleftbig
I′
1,k/bracketrightbig/angbracketrightbig
−2⟨y−uk,EMk[I2,k]⟩
≤(1−ηθτλ 0)∥y−uk∥2
2+ 2/vextendsingle/vextendsingle/angbracketleftbig
y−uk,EMk/bracketleftbig
I′
1,k/bracketrightbig/angbracketrightbig/vextendsingle/vextendsingle+
2|⟨y−uk,EMk[I2,k]⟩|+EMk/bracketleftbig
∥uk+1−uk∥2
2/bracketrightbig
where in the last inequality we use λmin(H(k))≥λ0
2from(Du et al., 2018) , Assumption 3.1. Moreover,
Lemma 6, Lemma 9, and Lemma 10 shows that under the given assumption, with η=O/parenleftig
λ0
nτmax{n,p}/parenrightig
we
have
/vextendsingle/vextendsingle/angbracketleftbig
y−uk,EMk/bracketleftbig
I′
1,k/bracketrightbig/angbracketrightbig/vextendsingle/vextendsingle≤1
8ηθτλ 0∥y−uk∥2
2+16ηθτξ2(1−ξ)2κ2n3d
mλ0+2η3ξ2τ(τ−1)2n4pC1
θλ0
28Published in Transactions on Machine Learning Research (08/2022)
|⟨y−uk,EMk[I2,k]⟩|≤1
8ηθτλ 0∥y−uk∥2
2+ηλ0ξ2(θ−ξ2)nκ2
24pτ+ηλ0ξ2(τ−1)2pC1
96τθ
EMk/bracketleftbig
∥uk+1−uk∥2
2/bracketrightbig
≤1
4ηθτλ 0∥y−uk∥2
2+17η2ξ2τ2θ(θ−ξ2)n3κ2
p+η2ξ2λ0(τ−1)2pnC 1
Putting things together gives
EMk/bracketleftbig
∥y−uk+1∥2
2/bracketrightbig
≤/parenleftbigg
1−1
4ηθτλ 0/parenrightbigg
∥y−uk∥2
2+16ηθτξ2(1−ξ)2n3d
mλ0+2η3ξ2τ(τ−1)2n4pC1
θλ0
ηλ0ξ2(θ−ξ2)nκ2
24τp+ηλ0ξ2(τ−1)2pC1
96τθ+17η2ξ2τ2θ(θ−ξ2)n3κ2
p+
η2ξ2λ0τ(τ−1)pnC 1
≤/parenleftbigg
1−1
4ηθτλ 0/parenrightbigg
∥y−uk∥2
2+1
4ηθτλ 0B1
Therefore, we have
E[Mk−1]/bracketleftbig
∥y−uk∥2
2/bracketrightbig
≤/parenleftbigg
1−1
4ηθτλ 0/parenrightbiggk
∥y−u0∥2
2+B1
This completes the proof.
29Published in Transactions on Machine Learning Research (08/2022)
F Proof of Theorem 4
Again we use induction to prove the theorem. Consider the following conditions
EMk/bracketleftbig
∥y−uk∥2
2/bracketrightbig
≤(1−α)∥y−uk∥2
2+B1 (17)
∥wk,r−w0,r∥2+ 2ητ/radicalbigg
2nK
mδ/parenleftbigg1
αE[Mk−1],W0,a[∥y−uk∥2] + (K−k)B/parenrightbigg
≤R (18)
withαandBdefined as below
α=/parenleftig
1−/parenleftbig
1−p−1/parenrightbig1
3/parenrightig/parenleftbigg
1−/parenleftbigg
1−ηλ0
2/parenrightbiggτ/parenrightbigg
;B=B1
α+κ/radicalbig
ξ(1−ξ)pn;B1=O/parenleftbigg/parenleftbig
1−p−1/parenrightbig2
3nκ2ητλmax
λ0/parenrightbigg
withξ=p−1. For notation clarity we will not plug in the value of ξfor now. Notice that equation (17)
implies the convergence result as in the theorem statement as long as γ=1
2and that max{K,d,p}≥n.
Thus, as long as we establish the inductive relations as above we are done.
Base Case: Notice by lemma 26, equation (17) holds. We use the over-parameterization requirement to
show equation (18). In particular, we show
2ητ/radicalbigg
2nK
mδ/parenleftbigg1
αEW0,a[∥y−u0∥2] +KB/parenrightbigg
≤R=O/parenleftbiggκλ0
n/parenrightbigg
As before, using Lemma 26, we have
EW0,a[∥y−u0∥2]≤/parenleftbig
EW0,a/bracketleftbig
∥y−u0∥2
2/bracketrightbig/parenrightbig1
2=C√n
Plugging in this value and the value of κ,Bandα, we arrive at the over-parameterization requirement
m= Ω/parenleftbiggn5τ2Kλmax
λ6
0δ/parenrightbigg
Inductive Case: the proof is again divided into two parts.
(17)→(18)Assume that equation (17) holds, and equation (18) holds for global iteration k. Similar to the
proof of Theorem 3, it suffice to show that
∥wk+1,r−w0,r∥2≤ητ/radicalbigg
2nK
mδE[Mk−1],W0,a[∥y−uk∥2] + 2ητ/radicalbigg
2nK
mδB−2ητ/radicalbigg
2nKB 1
mδα
Plugging in the value of Bandα, it then suffice to show that
∥wk+1,r−w0,r∥2≤ητ/radicalbigg
2nK
mδE[Mk−1],W0,a[∥y−uk∥2] + 2ητκn/radicalbigg
2ξ(1−ξ)pK
mδ
By Hypothesis 1 we have
∥wl
k,t,r−wk,r∥2≤ητ/radicalbigg
2nK
mδE[Mk−1],W0,a[∥y−uk∥2] + 2ητκn/radicalbigg
2ξ(1−ξ)pK
mδ
for alll∈[p]andt∈[τ]. Then we have
∥wk+1,r−wk,r∥2≤ηk,rp/summationdisplay
l=1ml
k,r∥wl
k,τ,r−wk,r∥2
≤ητ/radicalbigg
2nK
mδE[Mk−1],W0,a[∥y−uk∥2] + 2ητκn/radicalbigg
ξ(1−ξ)pK
mδ
30Published in Transactions on Machine Learning Research (08/2022)
(18)→(17)Now, assume that equation (18) holds. Then the result of Hypothesis (D) holds. Our target
is to show equation (17). As in previous theorem, we start by studying ∥y−uk+1∥2
2. In the case of a
categorical mask, we have the nice property that the average of the sub-networks equals to the full network
uk+1=1
p/summationtextp
l=1ˆul
k,τ. Using this property, Lemma 13 characterize ∥y−uk+1∥2
2as
∥y−uk+1∥2
2=1
pp/summationdisplay
l=1∥y−ˆul
k,τ∥2
2−1
p2p/summationdisplay
l=1l−1/summationdisplay
l′=1∥ˆul
k,τ−ˆul′
k,τ∥2
2
We start by assuming the condition of Hypothesis 1 holds. We proceed by proving the convergence, then we
prove the weight perturbation bound with a fashion of induction. Hypothesis 1 implies that
∥y−ˆul
k,τ∥2
2≤/parenleftbigg
1−ηλ0
2/parenrightbiggτ
∥y−ˆul
k∥2
2
=∥y−ˆul
k∥2
2−ηλ0
2τ−1/summationdisplay
t=0/parenleftbigg
1−ηλ0
2/parenrightbiggt
∥y−ˆul
k∥2
2
Using the fact that EMk/bracketleftbig
ˆul
k/bracketrightbig
=uk, we have that
EMk/bracketleftbig
∥y−ˆuk∥2
2/bracketrightbig
=∥y−uk∥2
2+EMk/bracketleftbig
∥uk−ˆul
k∥2
2/bracketrightbig
Therefore, we have
EMk/bracketleftbig
∥y−uk+1∥2
2/bracketrightbig
=1
pp/summationdisplay
l=1EMk/bracketleftbig
∥y−ˆul
k∥2
2/bracketrightbig
−ηλ0
2pτ−1/summationdisplay
t=0p/summationdisplay
l=1/parenleftbigg
1−ηλ0
2/parenrightbiggt
EMk/bracketleftbig
∥y−ˆul
k∥2
2/bracketrightbig
−
1
p2p/summationdisplay
l=1l/summationdisplay
l′=1EMk/bracketleftig
∥ˆul
k,τ−ˆul′
k,τ∥2
2/bracketrightig
=∥y−uk∥2
2−ηλ0
2pτ−1/summationdisplay
t=0p/summationdisplay
l=1/parenleftbigg
1−ηλ0
2/parenrightbiggt
EMk/bracketleftbig
∥y−ˆul
k∥2
2/bracketrightbig
+
1
pp/summationdisplay
l=1EMk/bracketleftbig
∥uk−ˆul
k∥2
2/bracketrightbig
−1
p2p/summationdisplay
l=1l/summationdisplay
l′=1EMk/bracketleftig
∥ˆul
k,τ−ˆul′
k,τ∥2
2/bracketrightig
Lemma 14 studies the error term ∥uk−ˆul
k∥2
2and gives
p/summationdisplay
l=1∥uk−ˆul
k∥2
2=1
pp/summationdisplay
l=1l−1/summationdisplay
l′=1∥ˆul
k−ˆul′
k∥2
2
Plugging in we have
EMk/bracketleftbig
∥y−uk+1∥2
2/bracketrightbig
≤∥y−uk∥2
2−−ηλ0
2pτ−1/summationdisplay
t=0/parenleftbigg
1−ηλ0
2/parenrightbiggtp/summationdisplay
l=1EMk/bracketleftbig
∥y−ˆul
k∥2
2/bracketrightbig
+
1
p2p/summationdisplay
l=1l/summationdisplay
l′=1EMk/bracketleftig
∥ˆul
k−ˆul′
k∥2
2−∥ˆul
k,τ−ˆul′
k,τ∥2
2/bracketrightig
Denote the last term in the right hand side as ιk. Lemma 15 shows bound of the expectation of ιkwith
respect to the initialization. In particular, if lemma 23 holds for some R≥0, and the weight perturbation
31Published in Transactions on Machine Learning Research (08/2022)
is bounded by∥wl
k,r−w0,r∥2≤Rfor allr∈[m], then we have
ιk≤/parenleftbig
1−p−1/parenrightbig1
2ηλ0
2pp/summationdisplay
l=1τ−1/summationdisplay
t=0EMk/bracketleftig/vextenddouble/vextenddoubley−ˆul
k,t/vextenddouble/vextenddouble2
2/bracketrightig
+ 24/parenleftbig
1−p−1/parenrightbig1
2nκ2ητλmax
λ0
Using this result, we have that
EMk/bracketleftbig
∥y−uk+1∥2
2/bracketrightbig
=∥y−uk∥2
2−−ηλ0
2pτ−1/summationdisplay
t=0/parenleftbigg
1−ηλ0
2/parenrightbiggtp/summationdisplay
l=1EMk/bracketleftbig
∥y−ˆul
k∥2
2/bracketrightbig
+ιk
=∥y−uk∥2
2−ηλ0
2pp/summationdisplay
l=1τ−1/summationdisplay
t=0/parenleftbigg
1−ηλ0
2/parenrightbiggt
EMk/bracketleftbig
∥y−ˆul
k∥2
2/bracketrightbig
+
/parenleftbig
1−p−1/parenrightbig1
3ηλ0
4pp/summationdisplay
l=1τ−1/summationdisplay
t=0/parenleftbigg
1−ηλ0
2/parenrightbiggt
EMk/bracketleftbig
∥y−ˆul
k∥2
2/bracketrightbig
+ 48/parenleftbig
1−p−1/parenrightbig2
3nκ2ητλmax
λ0
=∥y−uk∥2
2−/parenleftbig
1−p−1/parenrightbig1
3ηλ0
4pp/summationdisplay
l=1τ−1/summationdisplay
t=0/parenleftbigg
1−ηλ0
2/parenrightbiggt
EMk/bracketleftbig
∥y−ˆul
k∥2
2/bracketrightbig
+
48/parenleftbig
1−p−1/parenrightbig2
3nκ2ητλmax
λ0
≤∥y−uk∥2
2−/parenleftbig
1−p−1/parenrightbig1
3ηλ0
4τ−1/summationdisplay
t=0/parenleftbigg
1−ηλ0
2/parenrightbiggt
∥y−uk∥2
2+
48/parenleftbig
1−p−1/parenrightbig2
3nκ2ητλmax
λ0
=/parenleftbigg/parenleftbig
1−p−1/parenrightbig1
3+/parenleftig
1−/parenleftbig
1−p−1/parenrightbig1
3/parenrightig/parenleftbigg
1−ηλ0
2/parenrightbiggτ/parenrightbigg
∥y−uk∥2
2+ 48/parenleftbig
1−p−1/parenrightbig2
3nκ2ητλmax
λ0
Therefore,
EMk/bracketleftbig
∥y−uk+1∥2
2/bracketrightbig
≤(1−α)∥y−uk∥2
2+B1
This is the same as the form in equation (17). Thus we arrive at the convergence
E[Mk]/bracketleftbig
∥y−uk∥2
2/bracketrightbig
≤(1−α)k∥y−u0∥2
2+B1
α
This shows the convergence result in the theorem with αandB1plugged in.
32Published in Transactions on Machine Learning Research (08/2022)
G Lemmas for Theorem 3
Lemma 1. The expectation of the mixing function satisfies
EMk/bracketleftig
f(i)
k,r/bracketrightig
=θu(i)
k+θ(1−ξ)√marσ(⟨wk,r′,xi⟩)
Proof.Note that if N⊥
k,r= 0, then we have f(i)
k,r= 0for alli∈[n]. Thus
EMk/bracketleftig
f(i)
k,r|N⊥
k,r= 0/bracketrightig
= 0
Moreover, if N⊥
k,r= 1, the expectation can be computed as
EMk/bracketleftig
f(i)
k,r|N⊥
k,r= 1/bracketrightig
=EMk/bracketleftigg
ηk,r√mp/summationdisplay
l=1m/summationdisplay
r′=1ml
k,rml
k,r′arσ(⟨wk,r,xi⟩)|N⊥
k,r= 1/bracketrightigg
=1√mm/summationdisplay
r′=1EMk/bracketleftigg
ηk,rp/summationdisplay
l=1ml
k,rml
k,r′|N⊥
k,r= 1/bracketrightigg
ar′σ(⟨wk,r′,xi⟩)
=ξ√mm/summationdisplay
r′=1ar′σ(⟨wk,r′,xi⟩) +1−ξ√marσ(⟨wk,r′,xi⟩)
by using Lemma 20. Combining the two conditions above gives that
EMk/bracketleftig
f(i)
k,r/bracketrightig
=P(N⊥
k,r= 1)EMk/bracketleftig
f(i)
k,r|N⊥
k,r= 1/bracketrightig
+P(N⊥
k,r= 0)EMk/bracketleftig
f(i)
k,r|N⊥
k,r= 0/bracketrightig
=θξ√mm/summationdisplay
r′=1ar′σ(⟨wk,r′,xi⟩) +θ(1−ξ)√marσ(⟨wk,r′,xi⟩)
=θu(i)
k+θ(1−ξ)√marσ(⟨wk,r′,xi⟩)
Lemma 2. The expectation of the mixing gradient satisfies
EMk[gk,r] =θ
ξ∂L(Wk)
∂wr+θ(1−ξ)
mn/summationdisplay
i=1xiσ(⟨wk,r,xi⟩)
Proof.With the result from Lemma 1, we have
EMk[gk,r] =1√mn/summationdisplay
i=1/parenleftig
EMk/bracketleftig
f(i)
k,r/bracketrightig
−yiEMk/bracketleftbig
N⊥
k,r/bracketrightbig/parenrightig
arxiI{⟨wk,r,xi⟩≥0}
=θ√mn/summationdisplay
i=1/parenleftbigg
u(i)
k−yi+1−ξ√marσ(⟨wk,r,xi⟩)/parenrightbigg
arxiI{⟨wk,r,xi⟩≥0}
=θ
ξ∂L(Wk)
∂wr+θ(1−ξ)
mn/summationdisplay
i=1xiσ(⟨wk,r,xi⟩)
Lemma 3. Supposem≥p. If for some R> 0and allr∈[m]the initialization satisfies
m/summationdisplay
r=1⟨w0,r,xi⟩2≤2mnκ2−mnR2
33Published in Transactions on Machine Learning Research (08/2022)
and for all r∈[m], it holds that∥wk,r−w0,r∥2≤R, the expected norm of the difference between the mixing
function and u(i)
ksatisfies
EMk/bracketleftbig
∥fk,r−uk∥2
2|N⊥
k,r= 1/bracketrightbig
≤8(θ−ξ2)nκ2
p
Proof.Since EMk/bracketleftig
νk,r,r′|N⊥
k,r= 1/bracketrightig
=ξforr′̸=r, we have for r1̸=r2, there is at least one of r1,r2that
is notr. Thus
EMk/bracketleftbig
(νk,r,r 1−ξ)(νk,r,r 2−ξ)|N⊥
k,r= 1/bracketrightbig
= 0
and forr̸=r′
VarMk/parenleftbig
νk,r,r′|N⊥
k,r= 1/parenrightbig
=EMk/bracketleftbig
(νk,r,r′−ξ)2|N⊥
k,r= 1/bracketrightbig
Moreover, for r=r′, Lemma 19
EMk/bracketleftbig
(νk,r,r,−ξ)2/bracketrightbig
≤θ−ξ2
Therefore, using Lemma 21 we have
EMk/bracketleftbigg/parenleftig
f(i)
k,r−u(i)
k/parenrightig2
|N⊥
k,r= 1/bracketrightbigg
=1
mEMk

m/summationdisplay
r′̸=rar(νk,r,r′−ξ)σ(⟨wk,r′,xi⟩)
2
|N⊥
k,r= 1

=1
mm/summationdisplay
r′=1VarMk/parenleftbig
νk,r,r′|N⊥
k,r= 1/parenrightbig
σ(⟨wk,r′,xi⟩)2+
1
mEMk/bracketleftbig
(νk,r,r,−ξ)2/bracketrightbig
σ(⟨wk,r,xi⟩)2
≤θ−ξ2
pmm/summationdisplay
r′̸=r⟨wk,r′,xi⟩2+θ−ξ2
mσ(⟨wk,r,xi⟩)2
≤2(θ−ξ2)
pm/parenleftiggm/summationdisplay
r′=1⟨w0,r,xi⟩2+mR2/parenrightigg
+2(θ−ξ2)
p/parenleftbig
⟨wk,r,xi⟩+R2/parenrightbig
≤8(θ−ξ2)κ2
p
Plugging this in gives
EMk/bracketleftbig
∥fk,r−uk∥2
2|N⊥
k,r= 1/bracketrightbig
≤8(θ−ξ2)nκ2
p
Lemma 4. Under the condition of Lemma 3, the expected norm and squared-norm of the mixing gradient
is bounded by
EMk/bracketleftbig
∥gk,r∥2
2/bracketrightbig
≤2nθ
m∥y−uk∥2
2+16θ(θ−ξ2)n2κ2
pm
EMk[∥gk,r∥2]≤√nθ√m∥y−uk∥2+ 4nκ/radicaligg
θ(θ−ξ2)
pm
Proof.Using Lemma 3, we have
EMk/bracketleftbig
N⊥
k,r∥fk,r−uk∥2
2/bracketrightbig
=P(N⊥
k,r= 1)EMk/bracketleftbig
∥fk,r−uk∥2
2/bracketrightbig
≤8θ(θ−ξ2)nκ2
p
34Published in Transactions on Machine Learning Research (08/2022)
According to Jensen’s inequality, we also have
EMk/bracketleftbig
N⊥
k,r∥fk,r−uk∥2/bracketrightbig
≤2κ·/radicaligg
2θ(θ−ξ2)n
p
Moreover, we have
gk,r=1√mn/summationdisplay
i=1/parenleftig
f(i)
k,r−yi/parenrightig
arN⊥
k,rxiI{⟨wk,r,xi⟩≥0}
=1√mn/summationdisplay
i=1/parenleftig
f(i)
k,r−u(i)
k/parenrightig
arN⊥
k,rxiI{⟨wk,r,xi⟩≥0}+N⊥
k,r
ξ·∂L(Wk)
∂wr
Therefore,
EMk/bracketleftbig
∥gk,r∥2
2/bracketrightbig
≤2EMk/bracketleftig
N⊥
k,r/bracketrightig
ξ2/vextenddouble/vextenddouble/vextenddouble/vextenddouble∂L(Wk)
∂wr/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
2+2
mEMk
/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoublen/summationdisplay
i=1/parenleftig
f(i)
k,r−u(i)
k/parenrightig
arN⊥
k,rxiI{⟨wk,r,xi⟩≥0}/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
2

≤2n
mEMk/bracketleftigg
N⊥
k,rn/summationdisplay
i=1/parenleftig
f(i)
k,r−u(i)
k/parenrightig2/bracketrightigg
+2nθ
m∥y−uk∥2
2
≤2n
m/parenleftbig
EMk/bracketleftbig
N⊥
k,r∥fk,r−uk∥2
2/bracketrightbig
+θ∥y−uk∥2
2/parenrightbig
≤2nθ
m∥y−uk∥2
2+16θ(θ−ξ2)n2κ2
pm
This shows the first inequality. To show the second, similarly we have
EMk[∥gk,r∥2]≤EMk/bracketleftig
N⊥
k,r/bracketrightig
ξ/vextenddouble/vextenddouble/vextenddouble/vextenddouble∂L(Wk)
∂wr/vextenddouble/vextenddouble/vextenddouble/vextenddouble
2+1√mEMk/bracketleftigg/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoublen/summationdisplay
i=1/parenleftig
f(i)
k,r−u(i)
k/parenrightig
arN⊥
k,rxiI{⟨wk,r,xi⟩≥0}/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble
2/bracketrightigg
≤1√mEMk/bracketleftigg
N⊥
k,rn/summationdisplay
i=1/vextendsingle/vextendsingle/vextendsinglef(i)
k,r−u(i)
k/vextendsingle/vextendsingle/vextendsingle/bracketrightigg
+√nθ√m∥y−uk∥2
≤/radicalbiggn
mEMk/bracketleftbig
N⊥
k,r∥fk,r−uk∥2/bracketrightbig
+√nθ√m∥y−uk∥2
≤√nθ√m∥y−uk∥2+ 4nκ/radicaligg
θ(θ−ξ2)
pm
Lemma 5. Under the condition of Theorem 3, we have
/vextendsingle/vextendsingle/vextendsingleˆul(i)
k,t−ˆul(i)
k/vextendsingle/vextendsingle/vextendsingle≤ηt√n∥y−ˆul
k∥2
and therefore,
/vextendsingle/vextendsingle/vextendsingleˆul(i)
k,t−ˆul(i)
k/vextendsingle/vextendsingle/vextendsingle≤ηt√n/parenleftbig
∥y−uk∥2+∥uk−ˆul
k∥2/parenrightbig
/parenleftig
ˆul(i)
k,t−ˆul(i)
k/parenrightig2
≤2η2t2n/parenleftbig
∥y−uk∥2
2+∥uk−ˆul
k∥2
2/parenrightbig
35Published in Transactions on Machine Learning Research (08/2022)
Proof.We have
/vextendsingle/vextendsingle/vextendsingleˆul(i)
k,t−ˆul(i)
k/vextendsingle/vextendsingle/vextendsingle=1√m/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsinglem/summationdisplay
r=1arml
k,r/parenleftbig
σ/parenleftbig/angbracketleftbig
wl
k,t,r,xi/angbracketrightbig/parenrightbig
−σ(⟨wk,r,xi⟩)/parenrightbig/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
≤1√mm/summationdisplay
r=1/vextendsingle/vextendsingleσ/parenleftbig/angbracketleftbig
wl
k,t,r,xi/angbracketrightbig/parenrightbig
−σ(⟨wk,r,xi⟩)/vextendsingle/vextendsingle
≤1√mm/summationdisplay
r=1/vextenddouble/vextenddoublewl
k,t,r−wk,r/vextenddouble/vextenddouble
2
≤η√mm/summationdisplay
r=1t−1/summationdisplay
t′=0/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble∂Lml
k/parenleftig
Wl
k,t/parenrightig
∂wr/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble
2
≤η√nt−1/summationdisplay
t′=0∥y−ˆul
k,t∥2
≤ηt√n∥y−ˆul
k∥2
Therefore,
/vextendsingle/vextendsingle/vextendsingleˆul(i)
k,t−ˆul(i)
k/vextendsingle/vextendsingle/vextendsingle≤ηt√n/parenleftbig
∥y−uk∥2+∥uk−ˆul
k∥2/parenrightbig
Moreover,
/parenleftig
ˆul(i)
k,t−ˆul(i)
k/parenrightig2
=/vextendsingle/vextendsingle/vextendsingleˆul(i)
k,t−ˆul(i)
k/vextendsingle/vextendsingle/vextendsingle2
≤2η2t2n/parenleftbig
∥y−uk∥2
2+∥uk−ˆul
k∥2
2/parenrightbig
Lemma 6. Under the condition of Lemma 3, with η≤λ0
16(τ−1)n2, we have
/vextendsingle/vextendsingle/angbracketleftbig
y−uk,EMk/bracketleftbig
I′
1,k/bracketrightbig/angbracketrightbig/vextendsingle/vextendsingle≤1
8ηθτλ 0∥y−uk∥2
2+16ηθτξ2(1−ξ)2κ2n3d
mλ0+2η3ξ2τ(τ−1)2n4pC1
θλ0
Proof.We start by analyzing wk+1,r−wk,r. Taking expectation, we have
EMk[wk+1,r−wk,r] =−ηEMk
ηk,rp/summationdisplay
l=1τ−1/summationdisplay
t=0∂Lml
k/parenleftig
Wl
k,t/parenrightig
∂wr

=−ηEMk
ηk,rτp/summationdisplay
l=1∂Lml
k(Wk)
∂wr+ηk,rp/summationdisplay
l=1τ−1/summationdisplay
t=1
∂Lml
k/parenleftig
Wl
k,t/parenrightig
∂wr−∂Lml
k(Wk)
∂wr


=−ητEMk[gk,r]−ηEMk
ηk,rp/summationdisplay
l=1τ−1/summationdisplay
t=1
∂Lml
k/parenleftig
Wl
k,t/parenrightig
∂wr−∂Lml
k(Wk)
∂wr


=−ηθτ
ξ∂L(Wk)
∂wr−ηθ(1−ξ)τ
mn/summationdisplay
i=1xiσ(⟨wk,r,xi⟩)−
ηEMk
ηk,rp/summationdisplay
l=1τ−1/summationdisplay
t=1
∂Lml
k/parenleftig
Wl
k,t/parenrightig
∂wr−∂Lml
k(Wk)
∂wr


36Published in Transactions on Machine Learning Research (08/2022)
Therefore
EMk/bracketleftig
I(i)
1,k/bracketrightig
=ξ√m/summationdisplay
r∈SiarEMk[σ(⟨wk+1,r,xi⟩)−σ(⟨wk,r,xi⟩)]
=ξ√m/summationdisplay
r∈Siar⟨EMk[wk+1,r−wk,r],xi⟩I{⟨wk,r,xi⟩≥0}
=−ηθτ√m/summationdisplay
r∈Siar/angbracketleftbigg∂L(Wk)
∂wr,xi/angbracketrightbigg
I{⟨wk,r,xi⟩≥0}−η/parenleftig
E(i)
1,k+E(i)
2,k/parenrightig
=ηξθτ
m/summationdisplay
r∈Sin/summationdisplay
j=1(yi−u(i)
k)⟨xi,xj⟩I{⟨wk,r,xi⟩≥0,⟨wk,r,xj⟩≥0}−η/parenleftig
E(i)
1,k+E(i)
2,k/parenrightig
=ηθτn/summationdisplay
j=1/parenleftbig
H(k)ij−H(k)⊥
ij/parenrightbig
(yj−u(j)
k)−η/parenleftig
E(i)
1,k+E(i)
2,k/parenrightig
where
E(i)
1,k=θξ(1−ξ)τ
m3
2/summationdisplay
r∈Sin/summationdisplay
j=1ar⟨xi,xj⟩σ(⟨wk,r,xj⟩)
E(i)
2,k=ξ√m/summationdisplay
r∈Siar/angbracketleftigg
EMk
ηk,rp/summationdisplay
l=1τ−1/summationdisplay
t=1
∂Lml
k/parenleftig
Wl
k,t/parenrightig
∂wr−∂Lml
k(Wk)
∂wr

,xi/angbracketrightigg
I{⟨wk,r,xi⟩≥0}
LetE1,k=/bracketleftig
E(1)
1,k,...,E(n)
1,k/bracketrightig
, andE2,k=/bracketleftig
E(1)
2,k,...,E(n)
2,k/bracketrightig
. Then we have
EMk[I1,k] =ηθτ/parenleftbig
H(k)−H(k)⊥/parenrightbig
(y−uk)−η(E1,k+E2,k)
Thus,
EMk/bracketleftbig
I′
1,k/bracketrightbig
=EMk[I1,k]−ηθτH(k)(y−uk)
=ηθτH(k)⊥(y−uk) +η(E1,k+E2,k)
According to Lemma 7 and Lemma 8, we have the bound of E(i)
1,kandE(i)
2,kas
/vextendsingle/vextendsingle/vextendsingleE(i)
1,k/vextendsingle/vextendsingle/vextendsingle≤θξ(1−ξ)τnκ/radicalbigg
2d
m
/vextendsingle/vextendsingle/vextendsingleE(i)
2,k/vextendsingle/vextendsingle/vextendsingle≤ηξτ(τ−1)n3
2
2/parenleftig
θ∥y−uk∥2+/radicalbig
pC1/parenrightig
Moreover, according to Lemma 17, we have
∥H(k)⊥∥2≤4ξnκ−1R
LetR≤κλ0
128n, we have
∥H(k)⊥∥2≤λ0
32
37Published in Transactions on Machine Learning Research (08/2022)
Therefore, we have
/vextendsingle/vextendsingle/angbracketleftbig
y−uk,EMk/bracketleftbig
I′
1,k/bracketrightbig/angbracketrightbig/vextendsingle/vextendsingle≤ηθτ/vextendsingle/vextendsingle/angbracketleftbig
y−uk,H(k)⊥(y−uk)/angbracketrightbig/vextendsingle/vextendsingle+ηn/summationdisplay
i=1/parenleftig/vextendsingle/vextendsingle/vextendsingle/parenleftig
yi−u(i)
k/parenrightig
E(i)
1,k/vextendsingle/vextendsingle/vextendsingle+/vextendsingle/vextendsingle/vextendsingle/parenleftig
yi−u(i)
k/parenrightig
E(i)
2,k/vextendsingle/vextendsingle/vextendsingle/parenrightig
=ηθτ∥H(k)⊥∥2∥y−uk∥2
2+ max
i∈[n]/parenleftig/vextendsingle/vextendsingle/vextendsingleE(i)
1,k/vextendsingle/vextendsingle/vextendsingle+/vextendsingle/vextendsingle/vextendsingleE(i)
2,k/vextendsingle/vextendsingle/vextendsingle/parenrightig
ηn/summationdisplay
i=1/vextendsingle/vextendsingle/vextendsingleyi−u(i)
k/vextendsingle/vextendsingle/vextendsingle
≤1
32ηθτλ 0∥y−uk∥2
2+ max
i∈[n]/parenleftig/vextendsingle/vextendsingle/vextendsingleE(i)
1,k/vextendsingle/vextendsingle/vextendsingle+/vextendsingle/vextendsingle/vextendsingleE(i)
2,k/vextendsingle/vextendsingle/vextendsingle/parenrightig
η√n∥y−uk∥2
≤1
32ηθτλ 0∥y−uk∥2
2+η2θξτ(τ−1)n2
2∥y−uk∥2
2+
/parenleftigg
ηθξ(1−ξ)τκ/radicalbigg
2n3d
m+η2ξτ(τ−1)n2
2/radicalbig
pC1/parenrightigg
∥y−uk∥2
Using the general inequality that ab≤1
2(a2+b2), andη≤λ0
16(τ−1)n2, we get
/vextendsingle/vextendsingle/angbracketleftbig
y−uk,EMk/bracketleftbig
I′
1,k/bracketrightbig/angbracketrightbig/vextendsingle/vextendsingle≤1
8ηθτλ 0∥y−uk∥2
2+16ηθτξ2(1−ξ)2κ2n3d
mλ0+2η3ξ2τ(τ−1)2n4pC1
θλ0
Lemma 7. Under the assumption of Theorem 3 we have that for all k∈[K],i∈[n], it holds that
/vextendsingle/vextendsingle/vextendsingleE(i)
1,k/vextendsingle/vextendsingle/vextendsingle≤θξ(1−ξ)τnκ/radicalbigg
2d
m
Proof.We have
/vextendsingle/vextendsingle/vextendsingleE(i)
1,k/vextendsingle/vextendsingle/vextendsingle≤θξ(1−ξ)τ
m3
2/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/summationdisplay
r∈Sin/summationdisplay
j=1ar⟨xi,xj⟩σ(⟨wk,r,xj⟩)/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
≤θξ(1−ξ)τ
m3
2/summationdisplay
r∈Sin/summationdisplay
j=1|⟨wk,r,xi⟩|
≤θξ(1−ξ)τn
m3
2/summationdisplay
r∈Si∥wk,r∥2
≤θξ(1−ξ)τn
m3
2/summationdisplay
r∈Si(∥w0,r∥2+R)
≤θξ(1−ξ)τn
m∥W0∥F+θξ(1−ξ)τnR√m
≤θξ(1−ξ)τnκ/radicalbigg
2d
m
where for the bound of ∥W0∥Fwe use Lemma 22.
Lemma 8. Suppose∥wk,t,r−w0,r∥2≤Rfor allr∈[m]. Then we have
/vextendsingle/vextendsingle/vextendsingleE(i)
2,t/vextendsingle/vextendsingle/vextendsingle≤ηξτ(τ−1)n3
2
2/parenleftig
θ∥y−uk∥2+/radicalbig
pC1/parenrightig
38Published in Transactions on Machine Learning Research (08/2022)
Proof.Sincer∈Si, the difference between the surrogate gradients of a sub-network has the form
/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble∂Lml
k/parenleftig
Wl
k,t/parenrightig
∂wr−∂Lml
k(Wk)
∂wr/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble
2=1√m/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoublen/summationdisplay
j=1arml
k,rxj/parenleftig
ˆul(j)
k,t−ˆul(j)
k/parenrightig
I{⟨wk,r,xj⟩≥0}/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble
2
≤ml
k,r√mn/summationdisplay
j=1/vextendsingle/vextendsingle/vextendsingleˆul(j)
k,t−ˆul(j)
k/vextendsingle/vextendsingle/vextendsingle
Therefore, using the convexity of ℓ2-norm,
/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoubleEMk
ηk,rp/summationdisplay
l=1
∂Lml
k/parenleftig
Wl
k,t/parenrightig
∂wr−∂Lml
k(Wk)
∂wr

/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble
2≤EMk
ηk,r/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoublep/summationdisplay
l=1
∂Lml
k/parenleftig
Wl
k,t/parenrightig
∂wr−∂Lml
k(Wk)
∂wr
/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble
2

≤EMk
ηk,rp/summationdisplay
l=1/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble∂Lml
k/parenleftig
Wl
k,t/parenrightig
∂wr−∂Lml
k(Wk)
∂wr/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble
2

≤EMk
ηk,r√mp/summationdisplay
l=1mk,rn/summationdisplay
j=1/vextendsingle/vextendsingle/vextendsingleˆul(j)
k,t−ˆul(j)
k/vextendsingle/vextendsingle/vextendsingle

By Lemma 5, we have
/vextendsingle/vextendsingle/vextendsingleˆul(i)
k,t−ˆul(i)
k/vextendsingle/vextendsingle/vextendsingle≤ηt√n/parenleftbig
∥y−uk∥2+∥uk−ˆul
k∥2/parenrightbig
Therefore,
/vextendsingle/vextendsingle/vextendsingleE(i)
2,t/vextendsingle/vextendsingle/vextendsingle≤ξ√m/summationdisplay
r∈Si/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoubleEMk
ηk,rp/summationdisplay
l=1τ−1/summationdisplay
t=1
∂Lml
k/parenleftig
Wl
k,t/parenrightig
∂wr−∂Lml
k(Wk)
∂wr

/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble
2
≤ξ√mτ−1/summationdisplay
t=1/summationdisplay
r∈Si/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoubleEMk
ηk,rp/summationdisplay
l=1
∂Lml
k/parenleftig
Wl
k,t/parenrightig
∂wr−∂Lml
k(Wk)
∂wr

/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble
2
≤ξ√mτ−1/summationdisplay
t=1/summationdisplay
r∈SiEMk
ηk,r√mp/summationdisplay
l=1mk,rn/summationdisplay
j=1/vextendsingle/vextendsingle/vextendsingleˆul(j)
k,t−ˆul(j)
k/vextendsingle/vextendsingle/vextendsingle

≤ηξn3
2
mτ−1/summationdisplay
t=1t/summationdisplay
r∈SiEMk/bracketleftigg
ηk,rp/summationdisplay
l=1mk,r/parenleftbig
∥y−uk∥2+∥y−ˆul
k∥2/parenrightbig/bracketrightigg
≤ηξτ(τ−1)n3
2
2/parenleftigg
]θ∥y−uk∥2+EMk/bracketleftigg
ηk,rp/summationdisplay
l=1ml
k,r∥uk−ˆul
k∥2/bracketrightigg/parenrightigg
≤ηξτ(τ−1)n3
2
2/parenleftig
θ∥y−uk∥2+/radicalbig
pC1/parenrightig
where the last inequality follows from Lemma 24.
Lemma 9. Under the condition of Theorem 3, we have
|⟨y−uk,EMk[I2]⟩|≤1
8ηθτλ 0∥y−uk∥2
2+ηλ0ξ2(θ−ξ2)nκ2
24pτ+ηλ0ξ2(τ−1)2pC1
96τθ
39Published in Transactions on Machine Learning Research (08/2022)
Proof.To start, we notice that Using the 1-Lipschitzness of ReLU, we have
EMk/bracketleftig/vextendsingle/vextendsingle/vextendsingleI(i)
2,k/vextendsingle/vextendsingle/vextendsingle/bracketrightig
=ξ√mEMk
/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/summationdisplay
r∈S⊥
iar(σ(⟨wk+1,r,xi⟩−σ(⟨wk+1,r,xi⟩)/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle

≤ξ√m/summationdisplay
r∈S⊥
iEMk[|σ(⟨wk+1,r,xi⟩−σ(⟨wk+1,r,xi⟩|]
≤ξ√m/summationdisplay
r∈S⊥
iEMk[|⟨wk+1,r−wk,r,xi⟩|]
≤ξ√m/summationdisplay
r∈S⊥
iEMk[∥wk+1,r−wk,r∥2]
≤ηξ√m/summationdisplay
r∈S⊥
iEMk
/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoubleηk,rτ−1/summationdisplay
t=0p/summationdisplay
l=1∂Lml
k/parenleftig
Wl
k,t/parenrightig
∂wr/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble
2

≤ηξ√m/summationdisplay
r∈S⊥
i
EMk[∥gk,r∥2] +EMk
ηk,rτ−1/summationdisplay
t=1p/summationdisplay
l=1/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble∂Lmk/parenleftig
Wl
k,t/parenrightig
∂wr/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble
2


≤ηθξ√n
m|S⊥
i|∥y−uk∥2+4ηξκn|S⊥
i|
m/radicaligg
θ(θ−ξ2)
p+
ηξ√n
m/summationdisplay
r∈S⊥
iEMk/bracketleftigg
ηk,rτ−1/summationdisplay
t=1p/summationdisplay
l=1ml
k,r∥y−ˆul
k,t∥2/bracketrightigg
≤ηθξ√n
m|S⊥
i|∥y−uk∥2+4ηξκn|S⊥
i|
m/radicaligg
θ(θ−ξ2)
p+
ηξ√n
m/summationdisplay
r∈S⊥
iEMk/bracketleftigg
ηk,rτ−1/summationdisplay
t=1p/summationdisplay
l=1ml
k,r∥y−ˆul
k∥2/bracketrightigg
≤ηθξτ√n
m|S⊥
i|∥y−uk∥2+4ηξκn|S⊥
i|
m/radicaligg
θ(θ−ξ2)
p+
ηξ√n
m/summationdisplay
r∈S⊥
iEMk/bracketleftigg
ηk,rτ−1/summationdisplay
t=1p/summationdisplay
l=1ml
k,r∥uk−ˆul
k∥2/bracketrightigg
where in the seventh inequality we use the bound on EMk[∥gk,r∥2]from Lemma 4. Moreover, using Lemma
24 we have
EMk/bracketleftigg
ηk,rp/summationdisplay
l=1ml
k,r∥uk−ˆul
k∥2/bracketrightigg
≤/radicalbig
pC1
Then we have
EMk/bracketleftig/vextendsingle/vextendsingle/vextendsingleI(i)
2,k/vextendsingle/vextendsingle/vextendsingle/bracketrightig
≤ηθξτ√n
m|S⊥
i|∥y−uk∥2+4ηξκn|S⊥
i|
m/radicaligg
θ(θ−ξ2)
p+ηξ(τ−1)
m/radicalbig
npC 1|S⊥
i|
≤8ηθξτ√nκ−1R∥y−uk∥2+ 16ηξnR/radicaligg
θ(θ−ξ2)
p+ 4ηξ(τ−1)κ−1R/radicalbig
npC 1
40Published in Transactions on Machine Learning Research (08/2022)
where in the last inequality we use |S⊥
i|≤4mκ−1R. Therefore,
|⟨y−uk,EMk[I2,k]⟩|=/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsinglen/summationdisplay
i=1(yi−u(i)
k)EMk/bracketleftig
I(i)
2,k/bracketrightig/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
≤n/summationdisplay
i=1/vextendsingle/vextendsingle/vextendsingleyi−u(i)
k/vextendsingle/vextendsingle/vextendsingle·/vextendsingle/vextendsingle/vextendsingleEMk/bracketleftig
I(i)
2,k/bracketrightig/vextendsingle/vextendsingle/vextendsingle
≤max
i∈[n]/vextendsingle/vextendsingle/vextendsingleEMk/bracketleftig
I(i)
2,k/bracketrightig/vextendsingle/vextendsingle/vextendsinglen/summationdisplay
i=1/vextendsingle/vextendsingle/vextendsingleyi−u(i)
k/vextendsingle/vextendsingle/vextendsingle
≤√nmax
i∈[n]/vextendsingle/vextendsingle/vextendsingleEMk/bracketleftig
I(i)
2,k/bracketrightig/vextendsingle/vextendsingle/vextendsingle∥y−uk∥2
≤8ηθξτκ−1nR∥y−uk∥2
2+ 16ηξR/radicaligg
θ(θ−ξ2)n3
p∥y−uk∥2+
4ηξ(τ−1)κ−1nR/radicalbig
pC1∥y−uk∥2
≤1
8ηθτλ 0∥y−uk∥2
2+ηλ0ξ2(θ−ξ2)nκ2
24pτ+ηλ0ξ2(τ−1)2pC1
96τθ
where in the last inequality we use R≤κλ0
192nandab≤1
2(a2+b2).
Lemma 10. Under the condition of Theorem 3, with η≤λ0
48nτmax{n,p}, we have
EMk/bracketleftbig
∥uk+1−uk∥2
2/bracketrightbig
≤1
4ηθτλ 0∥y−uk∥2
2+17η2ξ2τ2θ(θ−ξ2)n3κ2
p+η2ξ2λ0(τ−1)2pnC 1
Proof.As in previous lemma, we use the Lipschitzness of ReLU to get
EMk/bracketleftbigg/parenleftig
u(i)
k+1−u(i)
k/parenrightig2/bracketrightbigg
≤ξ2
mEMk
/parenleftiggm/summationdisplay
r=1ar(σ(⟨wk+1,r,xi⟩)−σ(⟨wk,r,xi⟩))/parenrightigg2

≤ξ2m/summationdisplay
r=1EMk/bracketleftig
(σ(⟨wk+1,r,xi⟩)−σ(⟨wk,r,xi⟩))2/bracketrightig
≤ξ2m/summationdisplay
r=1EMk/bracketleftig
⟨wk+1,r−wk,r,xi⟩2/bracketrightig
≤ξ2m/summationdisplay
r=1EMk/bracketleftig
∥wk+1,r−wk,r∥2
2/bracketrightig
=ξ2(D1,k+D2,k)
where
D1,k=/summationdisplay
r∈SiEMk/bracketleftig
∥wk+1,r−wk,r∥2
2/bracketrightig
D2,k=/summationdisplay
r∈S⊥
iEMk/bracketleftig
∥wk+1,r−wk,r∥2
2/bracketrightig
Using Lemma 11 and Lemma 12 we have
D1,k≤/parenleftbig
4η2τ2nθ+ 4η4θn3τ3(τ−1)p/parenrightbig
∥y−uk∥2
2+16η2τ2θ(θ−ξ2)n2κ2
p+ 4η4n3τ2(τ−1)2pC1
D2,k≤η2θτλ 0
18(1 + (τ−1)p)∥y−uk∥2
2+4η2λ0θ(θ−ξ2)τnκ2
9p+η2τ(τ−1)λ0pC1
18
41Published in Transactions on Machine Learning Research (08/2022)
Therefore we have
EMk/bracketleftbig
∥uk+1−uk∥2
2/bracketrightbig
≤ξ2n(D1,k+D2,k)
≤/parenleftbigg
4η2ξ2τ2n2θ+ 4η4θξ2n4τ3(τ−1)p+η2θτnλ 0
18(1 + (τ−1)p)/parenrightbigg
∥y−uk∥2
2+
16η2ξ2τ2θ(θ−ξ2)n3κ2
p+ 4η4ξ2n4τ2(τ−1)2pC1+4η2λ0ξ2θ(θ−ξ2)τn2κ2
9p+
η2ξ2τ(τ−1)nλ0pC1
18
Withη≤λ0
48nτmax{n,p}, we have
EMk/bracketleftbig
∥uk+1−uk∥2
2/bracketrightbig
≤1
4ηθτλ 0∥y−uk∥2
2+17η2ξ2τ2θ(θ−ξ2)n3κ2
p+η2ξ2λ0(τ−1)2pnC 1
Lemma 11.
D1,k≤/parenleftbig
4η2τ2nθ+ 4η4θn3τ3(τ−1)p/parenrightbig
∥y−uk∥2
2+16η2τ2θ(θ−ξ2)n2κ2
p+ 4η4n3τ2(τ−1)2pC1
Proof.We have
D1,k=η2/summationdisplay
r∈SiEMk
/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoubleηk,rτ−1/summationdisplay
t=0p/summationdisplay
l=1∂Lml
k/parenleftig
Wl
k,t/parenrightig
∂wr/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
2

≤η2/summationdisplay
r∈SiEMk
/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoubleτgk,r+ηk,rτ−1/summationdisplay
t=1p/summationdisplay
l=1
∂Lml
k/parenleftig
Wl
k,t/parenrightig
∂wr−∂Lml
k(Wk)
∂wr
/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
2

≤2η2τ2/summationdisplay
r∈SiEMk/bracketleftbig
∥gk,r∥2
2/bracketrightbig
+ 2η2(τ−1)p/summationdisplay
r∈Siτ−1/summationdisplay
t=1EMk
η2
k,rp/summationdisplay
l=1/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble∂Lml
k/parenleftig
Wl
k,t/parenrightig
∂wr−∂Lml
k(Wk)
∂wr/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
2

Note that for r∈Si, we have
/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble∂Lml
k/parenleftig
Wl
k,t/parenrightig
∂wr−∂Lml
k(Wk)
∂wr/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
2=mk,r
m/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoublen/summationdisplay
i=1arxi/parenleftig
ˆul(i)
k,t−ˆul(i)
k/parenrightig
I{⟨wk,r,xi⟩}≥ 0/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
2
≤nmk,r
mn/summationdisplay
i=1/parenleftig
ˆul(i)
k,t−ˆul(i)
k/parenrightig2
≤n2ml
k,r
m/parenleftbig
2η2t2n∥y−uk∥2
2+ 2η2t2n∥uk−ˆul
k∥2
2/parenrightbig
where in the last inequality we use Lemma 5. Plugging in the bound above and the bound on EMk/bracketleftbig
∥gk∥2
2/bracketrightbig
from Lemma 4 gives
D1,k≤4η2τ2nθ∥y−uk∥2
2+16η2τ2θ(θ−ξ2)n2κ2
p+ 4η4θn3τ3(τ−1)p∥y−uk∥2
2+
4η4n3τ3(τ−1)pEMk/bracketleftigg
η2
k,rp/summationdisplay
l=1ml
k,r∥uk−ˆul
k∥2
2/bracketrightigg
≤/parenleftbig
4η2τ2nθ+ 4η4θn3τ3(τ−1)p/parenrightbig
∥y−uk∥2
2+16η2τ2θ(θ−ξ2)n2κ2
p+ 4η4n3τ2(τ−1)2pC1
42Published in Transactions on Machine Learning Research (08/2022)
Lemma 12.
D2,k≤η2θτλ 0
18(1 + (τ−1)p)∥y−uk∥2
2+4η2λ0θ(θ−ξ2)τnκ2
9p+η2τ(τ−1)λ0pC1
18
Proof.
D2,k=η2/summationdisplay
r∈S⊥
iEMk
/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoubleηk,rτ−1/summationdisplay
t=0p/summationdisplay
l=1∂Lml
k/parenleftig
Wl
k,t/parenrightig
∂wr/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
2

≤η2τ/summationdisplay
r∈S⊥
i
EMk/bracketleftbig
∥gk,r∥2
2/bracketrightbig
+τ−1/summationdisplay
t=1EMk
/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoubleηk,rp/summationdisplay
l=1∂Lml
k/parenleftig
Wl
k,t/parenrightig
∂wr/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
2


≤η2τ/summationdisplay
r∈S⊥
i
EMk/bracketleftbig
∥gk,r∥2
2/bracketrightbig
+pτ−1/summationdisplay
t=1EMk
η2
k,rp/summationdisplay
l=1/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble∂Lmk/parenleftig
Wl
k,t/parenrightig
∂wr/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
2


≤2η2τnθ
m|S⊥
i|∥y−uk∥2
2+8η2θ(θ−ξ2)τn2κ2
pm|S⊥
i|+η2τnp
m|S⊥
i|τ−1/summationdisplay
t=1EMk/bracketleftigg
η2
k,rp/summationdisplay
l=1ml
k,r∥y−ˆul
k,t∥2
2/bracketrightigg
≤2η2τnθ
m|S⊥
i|∥y−uk∥2
2+8η2θ(θ−ξ2)τn2κ2
pm|S⊥
i|+2η2θτ(τ−1)np
m|S⊥
i|∥y−uk∥2
2+
2η2τ(τ−1)np
m|S⊥
i|EMk/bracketleftigg
η2
k,rp/summationdisplay
l=1ml
k,r∥uk−ˆul
k∥2
2/bracketrightigg
Using|S⊥
i|≤4mκ−1RwithR≤ξκλ0
144ngives
D2,k≤η2θτλ 0
18(1 + (τ−1)p)∥y−uk∥2
2+4η2λ0θ(θ−ξ2)τnκ2
9p+
η2τ(τ−1)λ0p
18EMk/bracketleftigg
η2
k,rp/summationdisplay
l=1ml
k,r∥uk−ˆul
k∥2
2/bracketrightigg
≤η2θτλ 0
18(1 + (τ−1)p)∥y−uk∥2
2+4η2λ0θ(θ−ξ2)τnκ2
9p+η2τ(τ−1)λ0pC1
18
43Published in Transactions on Machine Learning Research (08/2022)
H Lemmas for Theorem 4
Lemma 13. Thekth global step produce the squared error satisfying
∥y−uk+1∥2
2=1
pp/summationdisplay
l=1∥y−ˆul
k,τ∥2
2−1
p2p/summationdisplay
l=1l−1/summationdisplay
l′=1∥ˆul
k,τ−ˆul′
k,τ∥2
2
Proof.We have
∥y−uk+1∥2
2=/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoubley−1
pp/summationdisplay
l=1ˆul
k,τ/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
2
=1
p2p/summationdisplay
l=1p/summationdisplay
l′=1/angbracketleftig
y−ˆul
k,τ,y−ˆul′
k,τ/angbracketrightig
=1
pp/summationdisplay
l=1∥y−ˆul
k,τ∥2
2−1
pp/summationdisplay
l=1∥y−ˆul
k,τ∥2
2+1
p2p/summationdisplay
l=1p/summationdisplay
l′=1/angbracketleftig
y−ˆul
k,τ,y−ˆul′
k,τ/angbracketrightig
=1
pp/summationdisplay
l=1∥y−ˆul
k,τ∥2
2−
1
2p2/parenleftiggp/summationdisplay
l=1p/summationdisplay
l′=1/parenleftig
∥y−ˆul
k,τ∥2
2+∥y−ˆul′
k,τ∥2
2/parenrightig
−p/summationdisplay
l=1p/summationdisplay
l′=1/angbracketleftig
y−ˆul
k,τ,y−ˆul′
k,τ/angbracketrightig/parenrightigg
=1
pp/summationdisplay
l=1∥y−ˆul
k,τ∥2
2−1
p2p/summationdisplay
l=1l−1/summationdisplay
l′=1∥ˆul
k,τ−ˆul′
k,τ∥2
2
Lemma 14. We have
1
pp/summationdisplay
l=1l−1/summationdisplay
l′=1∥ˆul
k,τ−ˆul′
k,τ∥2
2=p/summationdisplay
l=1∥uk,τ−ˆul
k,τ∥2
2
Proof.Using uk=1
p/summationtextp
l=1ˆul
kwe have
p/summationdisplay
l=1∥uk,τ−ˆul
k,τ∥2
2=p/summationdisplay
l=1/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble1
pp/summationdisplay
l′=1ˆul
k,τ−ˆul
k,τ/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
2
=1
p2p/summationdisplay
l=1/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoublep/summationdisplay
l′=1/parenleftig
ˆul′
k,τ−ˆul
k,τ/parenrightig/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
2
=1
p2p/summationdisplay
l=1p/summationdisplay
l1=1p/summationdisplay
l2=1/angbracketleftig
ˆul1
k,τ−ˆul
k,τ,ˆul2
k,τ−ˆul
k,τ/angbracketrightig
=p/summationdisplay
l=1∥ˆul
k,τ∥2
2−1
pp/summationdisplay
l=1p/summationdisplay
l′=1/angbracketleftig
ˆuk,τkl,ˆul′
k/angbracketrightig
=1
2p/parenleftiggp/summationdisplay
l=1p/summationdisplay
l′=1/parenleftig
∥ˆul
k,τ∥2
2+∥ˆul′
k,τ∥2
2/parenrightig
−p/summationdisplay
l=1p/summationdisplay
l′=12/angbracketleftig
ˆul
k,τ,ˆul′
k,τ/angbracketrightig/parenrightigg
=1
pp/summationdisplay
l=1l−1/summationdisplay
l′=1∥ˆul
k,τ−ˆul′
k,τ∥2
2
44Published in Transactions on Machine Learning Research (08/2022)
Lemma 15. Suppose the condition of lemma 25 holds and the step size satisfies η=O/parenleftbigg
λ0p
n2(1−p−1)2
3τ/parenrightbigg
, then
with probability at least 1−δit holds for all k∈[K]that
ιk≤/parenleftbig
1−p−1/parenrightbig1
3ηλ0
2pp/summationdisplay
l=1τ−1/summationdisplay
t=0EMk/bracketleftig/vextenddouble/vextenddoubley−ˆul
k,t/vextenddouble/vextenddouble2
2/bracketrightig
+ 24/parenleftbig
1−p−1/parenrightbig2
3nκ2ητλmax
λ0
Proof.Recall the definition of ιk, expanding the quadratic form gives
ιk=1
pp/summationdisplay
l=1EMk/bracketleftig/vextenddouble/vextenddoubleuk−ˆul
k/vextenddouble/vextenddouble2
2−/vextenddouble/vextenddoubleuk,τ−ˆul
k,τ/vextenddouble/vextenddouble2
2/bracketrightig
=1
pp/summationdisplay
l=1EMk/bracketleftig/vextenddouble/vextenddoubleuk−ˆul
k/vextenddouble/vextenddouble2
2−/vextenddouble/vextenddouble/parenleftbig
uk−ˆul
k/parenrightbig
+/parenleftbig
uk−uk+1−ˆul
k+ˆul
k,τ/parenrightbig/vextenddouble/vextenddouble2
2/bracketrightig
=1
pp/summationdisplay
l=1EMk/bracketleftig
2/angbracketleftbig
uk−ˆul
k,uk−uk+1−ˆul
k+ˆul
k,τ/angbracketrightbig
+/vextenddouble/vextenddoubleuk−uk+1−ˆul
k+ˆul
k,τ/vextenddouble/vextenddouble2
2/bracketrightig
=1
pp/summationdisplay
l=1EMk/bracketleftbig
2/angbracketleftbig
uk−ˆul
k,ˆul
k,τ−ˆul
k/angbracketrightbig/bracketrightbig
+1
pp/summationdisplay
l=1EMk/bracketleftig/vextenddouble/vextenddoubleuk−uk+1−ˆul
k+ˆul
k,τ/vextenddouble/vextenddouble2
2/bracketrightig
where in the third inequality, we use the fact that
p/summationdisplay
l=1/angbracketleftbig
uk−ˆul
k,uk−uk+1/angbracketrightbig
=/angbracketleftigg
puk−p/summationdisplay
l=1ˆul
k,uk−uk+1/angbracketrightigg
= 0
For convenience, we denote
σ(i)
k,r=σ(⟨Wk,r,xi⟩) ;σl(i)
k,τ,r=σ/parenleftbig/angbracketleftbig
wl
k,τ,r,xi/angbracketrightbig/parenrightbig
Noticing that1
p/summationtextp
l=1/parenleftig
ˆul
k−ˆul
k,τ/parenrightig
=uk−uk+1, we apply a trick similar to lemma 14 to get that
1
pp/summationdisplay
l=1EMk/bracketleftig/vextenddouble/vextenddoubleuk−uk+1−ˆul
k+ˆul
k,τ/vextenddouble/vextenddouble2
2/bracketrightig
=1
p2p/summationdisplay
l=1l−1/summationdisplay
l′=1EMk/bracketleftig/vextenddouble/vextenddoubleˆul
k−ˆul
k,τ−ˆul
k−ˆul
k,τ/vextenddouble/vextenddouble2
2/bracketrightig
=1
mp2p/summationdisplay
l=1l−1/summationdisplay
l′=1n/summationdisplay
i=1/parenleftiggm/summationdisplay
r=1arml
k,r/parenleftig
σ(i)
k,r−σl(i)
k,τ,r/parenrightig
−ml′
k,r/parenleftig
σ(i)
k,r−σk,τ,rl′(i)/parenrightig/parenrightigg2
2
≤1
mp2p/summationdisplay
l=1l−1/summationdisplay
l′=1n/summationdisplay
i=1m/summationdisplay
r,r′=1ml
k,r/parenleftig
σ(i)
k,r−σl(i)
k,τ,r/parenrightig2
+ml′
k,r/parenleftig
σ(i)
k,r−σk,τ,rl′(i)/parenrightig2
≤η2τn2
mp2p/summationdisplay
l=1l−1/summationdisplay
l′=1m/summationdisplay
r=1τ−1/summationdisplay
t=0/parenleftbigg
ml
k,r/vextenddouble/vextenddoubley−ˆul
k,t/vextenddouble/vextenddouble2
2+ml′
k,r/vextenddouble/vextenddouble/vextenddoubley−ˆul′
k,t/vextenddouble/vextenddouble/vextenddouble2
2/parenrightbigg
≤η2τn2(p−1)
2mpp/summationdisplay
l=1m/summationdisplay
r=1τ−1/summationdisplay
t=0ml
k,r/vextenddouble/vextenddoubley−ˆul
k,t/vextenddouble/vextenddouble2
2
Notice that fixing an l∈[p], we have that ml
k,r’s are independent for all r∈[m]. Apply Hoeffding’s inequality
to get that
P/parenleftiggm/summationdisplay
r=1ml
k,r≥2mp−1/parenrightigg
≤exp/parenleftbig
−2mp−2/parenrightbig
45Published in Transactions on Machine Learning Research (08/2022)
Apply the union bound over all k∈[K]andl∈[p], and apply the over-parameterization requirement to get
that, with probability at least 1−δ, it holds that
1
pp/summationdisplay
l=1EMk/bracketleftig/vextenddouble/vextenddoubleuk−uk+1−ˆul
k+ˆul
k,τ/vextenddouble/vextenddouble2
2/bracketrightig
≤η2τn2(p−1)
p2p/summationdisplay
l=1τ−1/summationdisplay
t=0/vextenddouble/vextenddoubley−ˆul
k,t/vextenddouble/vextenddouble2
2
≤/parenleftbig
1−p−1/parenrightbig1
3ηλ0
4pp/summationdisplay
l=1τ−1/summationdisplay
t=0/vextenddouble/vextenddoubley−ˆul
k,t/vextenddouble/vextenddouble2
2
by choosing η=O/parenleftbigg
λ0p
n2(1−p−1)2
3τ/parenrightbigg
The first term can be bounded by
∆1=/angbracketleftbig
uk−ˆul
k,ˆul
k,τ−ˆul
k/angbracketrightbig
≤/vextenddouble/vextenddoubleuk−ˆul
k/vextenddouble/vextenddouble/vextenddouble/vextenddoubleˆul
k,τ−ˆul
k/vextenddouble/vextenddouble
We study the term/vextenddouble/vextenddouble/vextenddoubleˆul
k,t+1−ˆul
k,t/vextenddouble/vextenddouble/vextenddouble
2. Following analysis in the proof of hypothesis 1, we write ˆul
k,t+1−ˆul
k,t=
Il
1,k,t+Il
2,k,t. We first study the magnitude of Il
1,k,t. Itsith entry has
Il(i)
1,k,t=ηn/summationdisplay
j=1/parenleftbig
ml
k◦H(k,t)−ml
k◦H(k,t)⊥/parenrightbig
ij/parenleftig
yi−ˆul(i)
k,t/parenrightig
As we have shown in the proof of theorem 2
/vextenddouble/vextenddoubleml
k◦H(k,t)−H∞/vextenddouble/vextenddouble≤λ0
2
Therefore,λmax/parenleftbig
ml
k◦H(k,t)/parenrightbig
≤λmax+λ0
2≤2λmax. Moreover, in the proof of hypothesis 1 we have shown
that
/vextenddouble/vextenddoubleml
k◦H(k,t)⊥/vextenddouble/vextenddouble
2≤4nκ−1R
Therefore, we have
/vextenddouble/vextenddoubleIl
1,k,t/vextenddouble/vextenddouble
2=η/parenleftbig
ml
k◦H(k,t)−ml
k◦H(k,t)⊥/parenrightbig/parenleftbig
y−ˆul
k,t/parenrightbig
≤/parenleftbig
2ηλmax+ 4ηκ−1nR/parenrightbig/vextenddouble/vextenddoubley−ˆul
k,t/vextenddouble/vextenddouble
2
Using the bound of/vextendsingle/vextendsingle/vextendsingleIl(i)
2,k,t/vextendsingle/vextendsingle/vextendsinglein the proof of hypothesis 1, we have that
/vextenddouble/vextenddoubleIl
2,k,t/vextenddouble/vextenddouble
2≤/parenleftiggn/summationdisplay
i=1/vextendsingle/vextendsingle/vextendsingleIl(i)
2,k,t/vextendsingle/vextendsingle/vextendsingle2/parenrightigg1
2
4ηκ−1nR/vextenddouble/vextenddoubley−ˆul
k,t/vextenddouble/vextenddouble
2
Therefore,
/vextenddouble/vextenddoubleˆul
k,t+1−ˆul
k,t/vextenddouble/vextenddouble
2≤/vextenddouble/vextenddoubleIl
1,k,t/vextenddouble/vextenddouble
2+/vextenddouble/vextenddoubleIl
2,k,t/vextenddouble/vextenddouble
2≤/parenleftbig
2ηλmax+ 8ηκ−1nR/parenrightbig/vextenddouble/vextenddoubley−ˆul
k,t/vextenddouble/vextenddouble
2≤3ηλmax/vextenddouble/vextenddoubley−ˆul
k,t/vextenddouble/vextenddouble
2
by our choice of R. Thus, we have that
∆1≤3ηλmaxτ−1/summationdisplay
t=0/vextenddouble/vextenddoubley−ˆul
k,t/vextenddouble/vextenddouble
2/vextenddouble/vextenddoubleuk−ˆul
k/vextenddouble/vextenddouble
2≤/parenleftbig
1−p−1/parenrightbig1
3ηλ0
8τ−1/summationdisplay
t=0/vextenddouble/vextenddoubley−ˆul
k,t/vextenddouble/vextenddouble2
2+ 12ητλ max
(1−p−1)1
3λ0/vextenddouble/vextenddoubleuk−ˆul
k/vextenddouble/vextenddouble2
2
Therefore, by applying lemma 25 we have that
ιk≤/parenleftbig
1−p−1/parenrightbig1
3ηλ0
2pp/summationdisplay
l=1τ−1/summationdisplay
t=0EMk/bracketleftig/vextenddouble/vextenddoubley−ˆul
k,t/vextenddouble/vextenddouble2
2/bracketrightig
+ 24/parenleftbig
1−p−1/parenrightbig2
3nκ2ητλmax
λ0
46Published in Transactions on Machine Learning Research (08/2022)
I Auxiliary Results
Lemma 16. With probability at least 1−ne−mκ−1Rwe have|Si|≤4mκ−1Rfor alli∈[n].
Proof.Note that I{r∈S⊥
i}=I{I{Air}̸= 0}=I{Air}. Therefore, we have
|S⊥
i|=m/summationdisplay
r=1I{r∈S⊥
i}=I{Air}.
Since Ew0,r[I{Air}] =P(Air)≤2R
κ√
2π≤κ−1R, we also have
Ew0,r/bracketleftig/parenleftbig
I{Air}−Ew0,r[I{Air}]/parenrightbig2/bracketrightig
≤Ew0,r/bracketleftbig
I{Air}2/bracketrightbig
=2R
κ√
2π≤κ−1R
Again apply Bernstein inequality over the random variable I{Air}−Ew0,r[I{Air}]witht= 3mκ−1Rgives
P/parenleftbig
|S⊥
i|≤4mκ−1R/parenrightbig
=P/parenleftiggm/summationdisplay
r=1I{Air}≥4mκ−1R/parenrightigg
≤exp/parenleftbig
−mκ−1R/parenrightbig
Lemma 17. Define H⊥∈Rn×nsuch that
H⊥
ij=ξ
m⟨xi,xj⟩/summationdisplay
r∈S⊥
iI{⟨wr,xi⟩≥0;⟨wr,xi⟩≥0}
If|S⊥
i|≤4mκ−1R, then we have
∥H⊥∥2≤4nξκ−1R
Proof.We note that
∥H⊥∥2
2≤∥H⊥∥2
F=n/summationdisplay
i,j=1|H⊥
ij|2
For eachi,jpair we have
|H⊥
ij|≤ξ
m|S⊥
i|= 4ξκ−1R
Thus
∥H⊥∥2≤/parenleftbig
∥H⊥∥2
F/parenrightbig−1
2≤/parenleftbig
16n2κ−2∆2/parenrightbig−1
2= 4nξκ−1R
Lemma 18. For i.i.d Bernoulli masks with parameter ξ,N⊥
k,r∼Bern (θ)with
θ=P(N⊥
k,r= 1) = 1−(1−ξ)p
Proof.We have
P(N⊥
k,r= 1) = 1−P(N⊥
k,r= 0) = 1−p/productdisplay
l=1P(ml
k,r= 0) = 1−(1−ξ)p
47Published in Transactions on Machine Learning Research (08/2022)
Lemma 19. We have
EMk/bracketleftbig
(νk,r,r−ξ)2/bracketrightbig
≤θ−ξ2
Proof.To start, we notice that νk,r,r=ηk,r/summationtextp
l=1ml2
k,r=ηk,r/summationtextp
l=1ml
k,r=N⊥
k,r. Therefore EMk[νk,r,r] =
EMk/bracketleftig
N⊥
k,r/bracketrightig
=θ. Moreover, since N⊥2
k,r=N⊥
k,r, we have EMk/bracketleftig
ν2
k,r,r/bracketrightig
=θ. Thus, using θ≥ξ, we have
EMk/bracketleftbig
(νk,r,r−ξ)2/bracketrightbig
=EMk/bracketleftbig
ν2
k,r,r/bracketrightbig
−2ξEMk[νk,r,r] +ξ2
=θ−2ξθ+ξ2≤θ−ξ2
Lemma 20. For i.i.d Bernoulli masks with parameter ξ, we have
EMk/bracketleftbig
νk,r,r′|N⊥
k,r= 1/bracketrightbig
=/braceleftigg
ξifr̸=r′
1ifr=r′
Proof.Ifr=r′, we have
EMk/bracketleftbig
νk,r,r′|N⊥
k,r= 1/bracketrightbig
=EMk/bracketleftigg
ηk,rp/summationdisplay
l=1ml
k,r|N⊥
k,r= 1/bracketrightigg
=EMk/bracketleftbiggXk,r
Nk,r|N⊥
k,r= 1/bracketrightbigg
=EMk/bracketleftbig
N⊥
k,r|N⊥
k,r= 1/bracketrightbig
= 1
Ifr′̸=r, then we have that ml
k,r′is independent from ml
k,randNk,r. Therefore,
EMk/bracketleftbig
νk,r,r′|N⊥
k,r= 1/bracketrightbig
=EMk/bracketleftigg
ηk,rp/summationdisplay
l=1ml
k,r|N⊥
k,r= 1/bracketrightigg
EMk/bracketleftbig
ml
k,r′/bracketrightbig
=ξEMk/bracketleftbiggXk,r
Nk,r|N⊥
k,r= 1/bracketrightbigg
=ξ
Lemma 21. The variance follows
VarMk/parenleftbig
νk,r,r′|N⊥
k,r= 1/parenrightbig
=/braceleftigg
θ−ξ2
pifr̸=r′
0ifr=r′
Proof.Forr̸=r′, the expectation of ν2
k,r,r′givenN⊥
k,r= 1is
EMk/bracketleftbig
ν2
k,r,r′|N⊥
k,r= 1/bracketrightbig
=EMk/bracketleftigg/summationtextp
l=1/summationtextp
l′=1ml
k,rml′
k,rml
k,r′ml′
k,r′
Xk,r|N⊥
k,r= 1/bracketrightigg
=p/summationdisplay
l=1/summationdisplay
l′̸=lEMk[ml
k,r′]EMk[ml′
k,r′]EMk/bracketleftigg
ml
k,r
Xk,r|N⊥
k,r= 1/bracketrightigg
EMk/bracketleftigg
ml′
k,r
Xk,r|N⊥
k,r= 1/bracketrightigg
+
p/summationdisplay
l=1EMk[ml
k,r′]EMk/bracketleftigg
ml
k,r
X2
k,r|N⊥
k,r= 1/bracketrightigg
=ξ2p/summationdisplay
l=1/summationdisplay
l′̸=lEMk/bracketleftigg
ml
k,r
Xk,r|N⊥
k,r= 1/bracketrightigg
EMk/bracketleftigg
ml′
k,r
Xk,r|N⊥
k,r= 1/bracketrightigg
+
ξp/summationdisplay
l=1EMk/bracketleftigg
ml
k,r
X2
k,r|N⊥
k,r= 1/bracketrightigg
=ξ2+ξEMk/bracketleftbigg1
Xk,r|N⊥
k,r= 1/bracketrightbigg
−ξ2p/summationdisplay
l=1EMk/bracketleftigg
ml
k,r
Xk,r|N⊥
k,r= 1/bracketrightigg2
48Published in Transactions on Machine Learning Research (08/2022)
Therefore, the variance of νk,r,r′givenN⊥
k,r= 1has the form
Var/parenleftbig
νk,r,r′|gN⊥
k,r= 1/parenrightbig
=EMk/bracketleftbig
ν2
k,r,r′|N⊥
k,r= 1/bracketrightbig
−EMk/bracketleftbig
νk,r,r′|N⊥
k,r= 1/bracketrightbig2
=ξEMk/bracketleftbigg1
Xk,r|N⊥
k,r= 1/bracketrightbigg
−ξ2p/summationdisplay
l=1EMk/bracketleftigg
ml
k,r
Xk,r|N⊥
k,r= 1/bracketrightigg2
LetX(p) =/summationtextp
l=1ml
·∼B(p,ξ), then we have
EMk/bracketleftbigg1
Xk,r|N⊥
k,r= 1/bracketrightbigg
=EMk/bracketleftbigg1
1 +X(p−1)/bracketrightbigg
EMk/bracketleftigg
ml
k,r
Xr|N⊥
k,r= 1/bracketrightigg
=P(ml
k,r= 1|N⊥
k,r= 1)EMk/bracketleftbigg1
1 +X(p−1)/bracketrightbigg
=ξ
θEMk/bracketleftbigg1
1 +X(p−1)/bracketrightbigg
Moreover, using reciprocal moments we have
EMk/bracketleftbigg1
1 +X(p−1)/bracketrightbigg
=θ
pξ
Therefore
VarMk/parenleftbig
νk,r,r′|N⊥
k,r= 1/parenrightbig
=ξEMk/bracketleftbigg1
1 +X(p−1)/bracketrightbigg
−ξ4
θ2pEMk/bracketleftbigg1
1 +X(p−1)/bracketrightbigg2
=θ−ξ2
p
Ifr=r′, the variance is
VarMk(νr,r|gr= 1) =VarMk(gr|gr= 1) = 0
Lemma 22. Supposeκ≤1,R≤κ/radicalig
d
32. With probability at least 1−emd/32we have that
∥W0∥F≤κ√
2md−√mR
Proof.For allr∈[m],d1∈[d], we have EMk/bracketleftbig
w2
rd1/bracketrightbig
=κ2. Moreover, each w2
rd1is a (2κ2,2κ2)-sub-
exponential random variable
E/bracketleftig
et(w2
rd1−κ2)/bracketrightig
=1
κ√
2π/integraldisplay∞
−∞et(w2
rd1−κ2e−w2
rd1
2κ2dwrd1
=1
κ√
2π/integraldisplay∞
−∞e−(1
2κ2−t)w2
rd1−tκ2dwrd1
=1
κ√
2π·/radicalbiggπ
(2κ)−1−t·e−tκ2
=e−tκ2
√
1−2tκ2≤e2t2κ4
witht≤1
2κ2. Thus, using independence between entries of W0gives
E/bracketleftig
et(∥W0∥2
F−mdκ2)/bracketrightig
≤m/productdisplay
r=1d/productdisplay
d1=1E/bracketleftig
et(w2
rd1−κ2)/bracketrightig
≤e2mdt2κ4
49Published in Transactions on Machine Learning Research (08/2022)
Invoking the tail bound of sub-exponential random variable gives
P/parenleftbig
∥W0∥2
F≥mdκ2+t/parenrightbig
≤

e−t2
8mdκ 4if0≤t≤2mdκ2
e−t2
4κ2ift>2mdκ2
Lett=mdκ2−2mκR√
2d+mR2. Then
∥W0∥2
F≤2mdκ2+mR2−2mκR√
2d= (κ√
2md−√mR)2
with probability at least 1−e−t2
8mdκ 4. UsingR≤κ/radicalig
d
32we havet≥1
2mdκ2. Thus with probability at least
1−e−md
32we have
∥W0∥F≤κ√
2md−√mR
Lemma 23. Assumeκ≤1andR≤κ√
2. With probability at least 1−ne−m
32over initialization, it holds for
alli∈[n]that
m/summationdisplay
r=1⟨w0,r,xi⟩2≤2mκ2−mR2
n/summationdisplay
i=1m/summationdisplay
r=1⟨w0,r,xi⟩2≤2mnκ2−mnR2
Proof.It suffice to prove the first inequality, and the second follows by summing over n. To begin, we show
that each⟨w0,xi⟩are Gaussian with zero mean and variance κ2. Using independence between entries of
w0,r, we have
E/bracketleftig
e−t⟨w0,r,xi⟩/bracketrightig
=E
d/productdisplay
j=1e−tw0,r,jxi,j
=d/productdisplay
j=1E/bracketleftbig
e−tw0,r,jxi,j/bracketrightbig
=d/productdisplay
j=1e−t2x2
i,jκ2=e−t2κ2/summationtextd
j=1x2
i,j=e−t2κ2
where the last equality follows from our assumption that ∥xi∥2= 1. Next, we treat each ωr,i=⟨w0,r,xi⟩2
as a random variable. First, we compute the mean of ωr,i
E[ωr,i] =EW0/bracketleftig
⟨w0,r,xi⟩2/bracketrightig
=EW0
/parenleftiggd/summationdisplay
d1=1w0,r,dxi,d/parenrightigg2

=d/summationdisplay
d1=1EW0/bracketleftbig
w2
0,r,d/bracketrightbig
x2
i,d=κ2d/summationdisplay
d1=1x2
i,d=κ2
Then, we show that each ωr,iis sub-exponential with parameter (2κ2,2κ2).
E/bracketleftig
et(ωr,i−κ2)/bracketrightig
=1
κ√
2π/integraldisplay∞
−∞et(ωr,i−κ2)e−ωr,i
2κ2d√ωr,i
=1
κ√
2π/integraldisplay∞
−∞e−(1
2κ2−t)(√ωr,i)2−tκ2d√ωr,i
=1
κ√
2π·/radicalbiggπ
(2κ2)−1−t·e−tκ2
=e−tκ2
1−2tκ2≤e2tκ4
50Published in Transactions on Machine Learning Research (08/2022)
fort≤1
2κ2. Since each w0,ris independent, we have that each ωr,iis independent for a fixed i. Thus
E/bracketleftig
et/summationtextm
r=1(ωr,i−κ2)/bracketrightig
=m/productdisplay
r=1E/bracketleftig
et(ωr,i−κ2)/bracketrightig
≤e2mtκ4
Thus we have
P/parenleftiggm/summationdisplay
r=1ωr,i≥mκ2+t/parenrightigg
≤

e−t2
8mκ4if0≤t≤2mκ2
e−t2
2κ2ift≥2mκ2
We choose t=mκ2−mR2. SinceR≤κ√
2, we have thatmκ2
2≤t≤mκ2. Thus
P/parenleftiggm/summationdisplay
r=1ωr,i≥2mκ2−mR2/parenrightigg
≤e−m
8
Apply a union bound over all i∈[n]gives that with probability at least 1−ne−m
32, it holds for all i∈[n]
that
m/summationdisplay
r=1ωr,i≤2mκ2−mR2
Lemma 24. If for some R> 0and allr∈[m]the initialization satisfies
m/summationdisplay
r=1⟨w0,r,xi⟩2≤2mnκ2−mnR2
and for all r∈[m], it holds that∥wk,r−w0,r∥2≤R. Then with C1=4θ2(1−ξ)nκ2
p, we have
EMk/bracketleftigg
η2
k,rp/summationdisplay
l=1ml
k,r∥uk−ˆul
k∥2
2/bracketrightigg
≤C1;EMk/bracketleftigg
ηk,rp/summationdisplay
l=1ml
k,r∥uk−ˆul
k∥2/bracketrightigg
≤/radicalbig
pC1
Proof.Using reciprocal moments, we have
EMk[ηk,r] =P/parenleftbig
N⊥
k,r= 1/parenrightbig
EMk/bracketleftbig
ηk,r|N⊥
k,r= 1/bracketrightbig
=θ2
pξ
To start, we compute that for r̸=r′. Using the independence of ml
k,randml
k,r′, we have
EMk/bracketleftigg
η2
k,rp/summationdisplay
l=1ml
k,r(ξ−ml
k,r′)2/bracketrightigg
=p/summationdisplay
l=1EMk/bracketleftbig
η2
k,rmk,r(ξ−mk,r′)2/bracketrightbig
=p/summationdisplay
l=1EMk/bracketleftbig
η2
k,rml
k,r/bracketrightbig
EMk/bracketleftbig
(ξ−ml
k,r′)2/bracketrightbig
=ξ(1−ξ)EMk/bracketleftigg
η2
k,rp/summationdisplay
l=1ml
k,r/bracketrightigg
=ξ(1−ξ)EMk[ηk,r]
Forr=r′, we use the idempotent
EMk/bracketleftigg
η2
k,rp/summationdisplay
l=1ml
k,r(ξ−ml
k,r)2/bracketrightigg
= (1−ξ)2EMk/bracketleftigg
η2
k,rp/summationdisplay
l=1ml
k,r/bracketrightigg
= (1−ξ)2EMk[ηk,r]
51Published in Transactions on Machine Learning Research (08/2022)
Therefore
EMk/bracketleftigg
η2
k,rp/summationdisplay
l=1ml
k,r/parenleftig
u(i)
k−ˆul(i)
k/parenrightig2/bracketrightigg
≤1
mEMk
η2
k,rp/summationdisplay
l=1ml
k,r/parenleftiggm/summationdisplay
r′=1ar(ξ−ml
k,r′)σ(⟨wk,r′,xi⟩)/parenrightigg2

≤1
mEMk/bracketleftigg
η2
k,rp/summationdisplay
l=1ml
k,rm/summationdisplay
r′=1(ξ−ml
k,r′)2σ(⟨wk,r′,xi⟩)2/bracketrightigg
≤1
mm/summationdisplay
r′=1EMk/bracketleftigg
η2
k,rp/summationdisplay
l=1ml
k,r(ξ−ml
k,r′)2/bracketrightigg
σ(⟨wk,r′,xi⟩)2
≤ξ(1−ξ)
mEMk[ηk,r]m/summationdisplay
r′=1σ(⟨wk,r′,xi⟩)2+
(1−ξ)(1−2ξ)
mEMk[ηk,r]σ(⟨wk,r,xi⟩)2
≤θ2(1−ξ)
mpm/summationdisplay
r′=1⟨wk,r′,xi⟩2+(1−ξ)2θ2
mpξ⟨wk,r,xi⟩2
≤2θ2(1−ξ)κ2
p+2θ2(1−ξ)2κ2
mpξ
≤4θ2(1−ξ)κ2
p
where in the last inequality we use m≥ξ−1. Thus, we have
EMk/bracketleftigg
η2
k,rp/summationdisplay
l=1ml
k,r∥uk−ˆul
k∥2
2/bracketrightigg
=n/summationdisplay
i=1EMk/bracketleftigg
η2
k,rp/summationdisplay
l=1ml
k,r/parenleftig
u(i)
k−ˆul(i)
k/parenrightig2/bracketrightigg
≤C1
Also, we have
EMk/bracketleftigg
ηk,rp/summationdisplay
l=1ml
k,r∥uk−ˆul
k∥2/bracketrightigg
≤EMk
/parenleftigg
ηk,rp/summationdisplay
l=1ml
k,r∥uk−ˆul
k∥2/parenrightigg2
1
2
≤√pEMk/bracketleftigg
η2
k,rp/summationdisplay
l=1ml
k,r∥uk−ˆul
k∥2
2/bracketrightigg1
2
Plugging in the previous bound gives the desired result.
Lemma 25. If for some R> 0and allr∈[m]the initialization satisfies
m/summationdisplay
r=1⟨w0,r,xi⟩2≤2mnκ2−mnR2
and for all r∈[m], it holds that∥wk,r−w0,r∥2≤R. Then we have
EMk/bracketleftbig
∥uk−ˆul
k∥2
2/bracketrightbig
≤4ξ(1−ξ)nκ2
52Published in Transactions on Machine Learning Research (08/2022)
Proof.To start, we have
EMk/bracketleftbigg/parenleftig
u(i)
k−ˆul(i)
k/parenrightig2/bracketrightbigg
=1
mEMk
/parenleftiggm/summationdisplay
r=1ar(ξ−ml
k,r)σ(⟨wk,r,xi⟩)/parenrightigg2

≤1
mm/summationdisplay
r=1EMk/bracketleftbig
(ξ−ml
k,r)2/bracketrightbig
σ(⟨wk,r,xi⟩)2
≤ξ(1−ξ)
mm/summationdisplay
r=1⟨wk,r,xi⟩2
≤2ξ(1−ξ)
mm/summationdisplay
r=1⟨w0,r,xi⟩2+ 2ξ(1−ξ)R2
≤4ξ(1−ξ)κ2
Therefore,
EMk/bracketleftbig
∥uk−ˆul
k∥2
2/bracketrightbig
=n/summationdisplay
i=1EMk/bracketleftbigg/parenleftig
u(i)
k−ˆul(i)
k/parenrightig2/bracketrightbigg
≤4ξ(1−ξ)nκ2
Lemma 26. Assume that for all i∈[n],yisatisfies|yi|≤C−1for someC≥1. Then, we have
EW0,a/bracketleftbig
∥y−u0∥2
2/bracketrightbig
≤C2n
Proof.It is easy to see that EW0,a/bracketleftig
u(i)
0/bracketrightig
= 0. Now, note that
EW0,a/bracketleftbigg/parenleftig
u(i)
0/parenrightig2/bracketrightbigg
=ξ2
mEW0,a
/parenleftiggm/summationdisplay
r=1arσ(⟨w0,r,xi⟩/parenrightigg2

=ξ2
mm/summationdisplay
r=1EW0/bracketleftig
⟨w0,r,xi⟩2/bracketrightig
=ξ2
mm/summationdisplay
r=1EW0
/parenleftiggd/summationdisplay
d′=1w0,r,d′xi,d′/parenrightigg2

=ξ2
mm/summationdisplay
r=1d/summationdisplay
d′=1EW0/bracketleftbig
w2
0,r,d′x2
i,d′/bracketrightbig
=ξ2
mm/summationdisplay
r=1d/summationdisplay
d′=1x2
i,d′
=ξ2
Therefore,
EW0,a/bracketleftbigg/parenleftig
yi−u(i)
0/parenrightig2/bracketrightbigg
=y2
i−2yiEW0,a/bracketleftig
u(i)
0/bracketrightig
+EW0,a/bracketleftbigg/parenleftig
u(i)
0/parenrightig2/bracketrightbigg
=y2
i+ξ2
Thus,
EW0,a/bracketleftbig
∥y−u0∥2
2/bracketrightbig
=n/summationdisplay
i=1EW0,a/bracketleftbigg/parenleftig
yi−u(i)
0/parenrightig2/bracketrightbigg
=n/summationdisplay
i=1y2
i+ξ2n≤C2n
53