Published in Transactions on Machine Learning Research (10/2022)
Deep Policies for Online Bipartite Matching:
A Reinforcement Learning Approach
Mohammad Ali Alomrani mohammad.alomrani@mail.utoronto.ca
Department of Electrical & Computer Engineering
University of Toronto
Reza Moravej mreza.moravej@mail.utoronto.ca
Department of Mechanical & Industrial Engineering
University of Toronto
Elias B. Khalil khalil@mie.utoronto.ca
Department of Mechanical & Industrial Engineering
SCALE AI Research Chair in Data-Driven Algorithms for Modern Supply Chains
University of Toronto
Reviewed on OpenReview: https: // openreview. net/ forum? id= mbwm7NdkpO
Abstract
The challenge in the widely applicable online matching problem lies in making irrevocable
assignments while there is uncertainty about future inputs. Most theoretically-grounded
policies are myopic or greedy in nature. In real-world applications where the matching
process is repeated on a regular basis, the underlying data distribution can be leveraged
for better decision-making. We present an end-to-end Reinforcement Learning framework
for deriving better matching policies based on trial-and-error on historical data. We de-
vise a set of neural network architectures, design feature representations, and empirically
evaluate them across two online matching problems: Edge-Weighted Online Bipartite
Matching and Online Submodular Bipartite Matching. We show that most of the learning
approaches perform consistently better than classical baseline algorithms on four synthetic
and real-world datasets. On average, our proposed models improve the matching quality
by 3–10% on a variety of synthetic and real-world datasets.Our code is publicly available
athttps://github.com/lyeskhalil/CORL .
1 Introduction
Originally introduced by Karp et al. (1990), the Online Bipartite Matching (OBM) problem is a simple
formulation of sequential resource allocation. A fixed set Uof known entities (e.g., ads, tasks, servers) are
to be dynamically assigned to at most one of a discrete stream Vof (apriori unknown) entities (e.g., ad-
slots, job candidates, computing job) upon their arrival, so as to maximize the size of the final matching.
Matching decisions are irrevocable and not matching is allowed at all times. Despite its simplicity, finding
better algorithms for OBM and its variants remains an active area of research. The uncertainty about
future inputs makes online problems inherently challenging. While practical exact methods (e.g., using in-
teger programming formulations and solvers) exist for many offline combinatorial problems, the restriction
to irrevocable and instant decision-making makes the use of such algorithmic tools impractical. Existing
algorithms for online matching are thus typically myopic and greedy in nature (Mehta, 2013).
In practice, however, the underlying (bipartite) graph instances may come from the same unknown distri-
bution (Borodin et al., 2020). In many applications, a sufficiently large collection of samples from the data
can represent the often implicit statistical properties of the entire underlying data-generating distribution.
1Published in Transactions on Machine Learning Research (10/2022)
It is often the case that corporations, for example, have access to a wealth of information that is repre-
sented as a large graph instance capturing customer behaviour, job arrivals, etc. Thus, it is sensible for an
algorithm to use historical data to derive statistical information about online inputs in order to perform
better on future instances. However, the majority of non-myopic hand-designed algorithms depend on esti-
mating the arrival distribution of the incoming nodes (Borodin et al., 2020; Mehta, 2013). The downside
of this approach is that imperative information such as the graph sparsity, ratio of incoming nodes to fixed
nodes, existence of community structures, degree distribution, and the occurrence of structural motifs are
ignored. Ideally, a matching policy should be able to leverage such information to refine its decisions based
on the observed history.
In this work, we formulate online matching as a Markov Decision Process (MDP) for which a neural net-
work is trained using Reinforcement Learning (RL) on past graph instances to make near-optimal match-
ings on unseen test instances. We design six unique models, engineer a set of generic features, and test
their performance on two variations of OBM across two synthetic datasets and two real-world datasets.
Our contributions can be summarized as follows:
Automating matching policy design: Motivated by practical applications, other variants of OBM
have been introduced with additional constraints (such as fairness constraints) or more complex objective
functions than just matching size. Our method reduces the reliance on human handcrafting of algorithms
for each individual variant of OBM since the RL framework presented herein can flexibly model them;
this will be demonstrated for the novel Online Submodular Bipartite Matching problem (Dickerson et al.,
2019).
Deriving tailored policies using past graph instances: We show that our method is capable of tak-
ing advantage of past instances to learn a near-optimal policy that is tailored to the problem instance.
Unlike “pen-and-paper” algorithms, our use of historical information is not limited to estimating the ar-
rival distribution of incoming nodes. Rather, our method takes advantage of additional statistics such as
the existing (partial) matching, graph sparsity, the |U|-to-|V|ratio, and the graph structure. Taking in a
more comprehensive set of statistics into account allows for fine-grained decision-making. For example, the
RL agent can learn to skip matching a node strategically based on the observed statistical properties of
the current graph. Our results on synthetic and real world datasets demonstrate this.
Leveraging Node Attributes: In many variants of OBM, nodes have identities, e.g., the nodes in V
could correspond to social media users whose demographic information could be used to understand their
preferences. Existing algorithms are limited in considering such node features that could be leveraged to
obtain better solutions. For instance, the RL agent may learn that connecting a node vwith a particular
set of attributes to a specific node in Uwould yield high returns. The proposed framework can naturally
account for such attributes, going beyond simple greedy-like policies. We will show that accounting for
node attributes yields improved results on a real-world dataset for Online Submodular Bipartite Matching.
2 Problem Setting
In a bipartite graph G= (U,V,E ),UandVare disjoint sets of nodes and Eis the set of edges connecting
a node inUto one inV. In the online bipartite matching problem, the vertex set Uis fixed and at each
timestep a new node v∈Vand its edges{(u,v) :u∈U}arrive. The algorithm must make an instan-
taneousandirrevocable decision to match vto one of its neighbors or not match at all. Nodes in Ucan
be matched to at most one node in V. The time horizon T=|V|is finite and assumed to be known in
advance. The simplest generalization of OBM is the edge-weighted OBM (E-OBM), where a non-negative
weight is associated with each edge. Other well-known variants include Adwords, Display Ads and Online
Submodular Welfare Maximization (Mehta, 2013). We will focus our experiments on E-OBM and Online
Submodular Bipartite Matching (OSBM), a new variation of the problem introduced by Dickerson et al.
(2019); together, the two problems span a wide range in problem complexity. The general framework can
be extended with little effort to address other online matching problems with different constraints and
objectives; see Appendix F for a discussion and results on Adwords.
2Published in Transactions on Machine Learning Research (10/2022)
2.1 Edge-weighted OBM (E-OBM)
Each edgee∈Ehas a predefined weight we∈R+and the objective is to select a subset Sof the incoming
edges that maximizes/summationtext
e∈Swe. Note that in the offline setting, where the whole graph is available, this
problem can be solved in polynomial time using existing algorithms (Kuhn, 1955). However, the online
setting involves reasoning under uncertainty, making the design of optimal online algorithms non-trivial.
2.2 Online Submodular Bipartite Matching (OSBM)
We first define some relevant concepts:
Submodular function : A function f: 2U→R+,f(∅) = 0issubmodular iff∀S,T⊆U:
f(S∪T) +f(S∩T)≤f(S) +f(T).
Some common examples of submodular functions include the coverage function, piecewise linear functions,
and budget-additive functions. In our experiments, we will focus on the weighted coverage function follow-
ing Dickerson et al. (2019):
Coverage function : Given a universe of elements Uand a collection of gsubsetsA1,A2,...,Ag⊆U, the
functionf(M) =|∪i∈MAi|is called the coverage function for M⊆{1,...,g}. Given a non-negative,
monotone weight function w: 2U→R+, the weighted coverage function is defined analogously as f(M) =
w(∪i∈MAi)and is known to be submodular.
In this setting, each edge e∈Eincident to arriving node vthas the weight f(Mt∪{e})−f(Mt)where
Mtis the matching at timestep t. The objective in OSBM is to find Msuch thatf(M) =/summationtext
e∈Mweis
maximized; fis a submodular function.
An illustrative application of the OSBM problem, identified by Dickerson et al. (2019), can be found in
movie recommendation systems. There, the goal is to match incoming users to a set of movies that are
both relevant and diverse (genre-wise). A user can login to the platform multiple times and may be rec-
ommended (matched to) a movie or left unmatched. Since we have historical information on each user’s
average ratings for each genre, we can quantify diversity as the weighted coverage function over the set
of genres that were matched to the user. The goal is to maximize the sum of the weighted coverage
functions for all users. More concretely, if we let Ube the universe of genres, then any movie ibelongs
to a subset of genres Ai. LetLbe the set of all users, Mlbe the set of movies matched to user l, and
fl(Ml) =w(∪i∈MAi)be the weighted coverage function defined as the sum of the weights of all genres
matched to the user, where the weight of a genre kis the average rating given by user lto movies of genre
k. Each user’s weighted coverage function is submodular. The objective of OSBM is to maximize the (sub-
modular) sum of these user functions: f(M) =/summationtext
l∈Lfl(Ml).
2.3 Arrival Order
Online problems are studied under different input models that allow the algorithm to access varying
amounts of information about the arrival distribution of the vertices in V. Theadversarial order setting is
often used to study the worst-case performance of an algorithm, positing that an imaginary adversary can
generate the worst possible graph and input order to make the algorithm perform poorly. More optimistic
is theknown i.i.d distribution (KIID) setting, where the algorithm knows Uas well as a distribution D
on the possible typesof vertices in V. Each arriving vertex vbelongs to one type and vertices of a given
type have the same neighbours in U. This assumption, i.e., that the arrival distribution Dis given, is too
optimistic for complex real-world applications.
In this work, we study the unknown i.i.d distribution (UIID) setting, which lies between the adver-
sarialand theKIIDsettings in terms of how much information is given about the arrival distribution
(Karande et al., 2011). The unknown i.i.d setting best captures real-world applications, where a base
graph is provided from an existing data set, but an explicit arrival distribution Dis not accessible. For
example, a database of past job-to-candidate or item-to-customer relationships can represent a base graph.
3Published in Transactions on Machine Learning Research (10/2022)
It is thus safe to assume that the arriving graph will follow the same distribution. The arrival distribution
is accessible on through sample instances drawn from D. More details on data generation are provided in
Section 5.1 and Appendix C.
3 Related Work
Traditional Algorithms for OBM: Generally, the focus of algorithm design for OBM has been on
worst-case approximation guarantees for “pen-and-paper” algorithms via competitive analysis, rather than
average-case performance in a real-world application. We refer the reader to (Karande et al., 2011; Mehta,
2013) for a summary of the many results for OBM under various arrival models. On the empirical side,
an extensive set of experiments by Borodin et al. (2020) showed that the naive greedy algorithm performs
similarly to more sophisticated online algorithms on synthetic and real-world graphs in the KIID setting.
Though the experiments were limited to OBM with |U|=|V|, they were conclusive in that ( i) greedy is
a strong baseline in practical domains, and ( ii) having better proven lower bounds does not necessarily
translate into better performance in practice.
The main challenge in online problems is decision-making in the face of uncertainty. Many traditional algo-
rithms under the KIID setting aim to overcome this challenge by explicitly approximating the distribution
over node types via a type graph. The algorithms observe past instances and estimate the frequency of
certain types of online nodes, i.e., for each type i, the algorithm predicts a probability piof a node of this
type arriving. We refer the reader to Borodin et al. (2020) for detailed explanation on algorithms under
the KIID setting. As noted earlier, the KIID setting is rather simplistic compared to the more realistic
UIID setting that we tackle here. Other non-myopic algorithms have been proposed which do not rely
on estimating the arrival distribution. For example, Awasthi & Sandholm (2009) solve stochastic kidney
exchange, a generalization of OBM, by sampling a subset of future trajectories, solving the offline problem
on each of them, and assigning a score to each action. The algorithm then selects the action that is the
best overall. To the best of our knowledge, our presented method is the first which learns a custom policy
from data based on both explicit and implicit patterns (such as graph sparsity, the graph structure, degree
distribution, etc.).
Learning in Combinatorial Optimization: There has recently been substantial progress in using
RL for finding better heuristics for offline, NP-complete graph problems. Dai et al. (2017) presented an
RL-based approach combined with graph embeddings to learn greedy heuristics for some graph problems.
Barrett et al. (2020) take a similar approach but start with a non-empty solution set and allow the policy
to explore by removing nodes/edges from the solution. Chen & Tian (2019) learn a secondary policy to
pick a particular region of the current solution to modify and incrementally improve the solution. The
work by Kool et al. (2019) uses an attention based encoder-decoder approach to find high-quality solutions
for TSP and other routing problems. We refer to the following surveys for a more comprehensive view of
the state of this area (Mazyavkina et al., 2021; Bengio et al., 2021).
Prior work on using predictive approaches for online problems has been fairly limited. Wang et al. (2019)
overlook the real-time decision making condition and use Q-learning for a batch of the arriving nodes. The
matching process, however, is not done by an RL agent but using an offline matching algorithm. Con-
sequently, their method is not practical for OBM variants that are NP-hard in the offline setting (e.g.,
Adwords) and where instantaneous decision making is paramount, e.g., in ad placement on search engines.
The work by Kong et al. (2019) is one of few to apply RL to online combinatorial optimization. Their
work differs from ours in three main ways: ( i) the question raised there is whether RL can discover al-
gorithms which perform best on worst-case inputs. They show that the RL agent will eventually learn a
policy which follows the “pen-and-paper” algorithm with the best worst-case guarantee. Our work, on the
other hand, asks if RL can outperform hand-crafted algorithms on average; ( ii) The MDP formulation
introduced in (Kong et al., 2019), unlike ours, does not consider the entire past (nodes that have previ-
ously arrived and the existing matching) which can help an RL policy better reason about the future; ( iii)
our family of invariant neural network architectures apply to graphs of arbitrary sizes |V|and|U|. More
details about the advantages of our method are provided in the next section. Zuzic et al. (2020) propose a
GAN-like adversarial training approach to learn robust OBM algorithms. However, just like (Kong et al.,
4Published in Transactions on Machine Learning Research (10/2022)
Figure 1: The MDP formulation of E-OBM. The agent is trained on different graph instances sampled
from a distribution D. At each timestep t, the agent picks a node to match or skip. In this 3x3 graph,
the optimal matching has weight 22; following the greedy policy would yield a matching of weight 7. The
illustrated policy ends up with a matching of weight 18.
2019), they are more concerned with learning algorithms that are robust to hard distributions rather than
real-world graphs, and do not utilize historical information accumulated in the previous steps within the
same instance.
Online Algorithm Design via Learned Advice: A hybrid paradigm has been recently introduced
where the predictive and competitive-analysis approaches are combined to tackle online problems. Such
algorithms take advantage of the predictions made by a model to obtain an improved competitive ratio
while still guaranteeing a worst-case bound when the predictions are inaccurate. Work in this area has
resulted in improvements over traditional algorithms for the secretary, ski rental and online matching prob-
lems (Antoniadis et al., 2020; Wang et al., 2020; Diakonikolas et al., 2021; Purohit et al., 2018). Unlike
our approach, the model does not get to construct a solution. Rather, its output is used as advice to a sec-
ondary algorithm. Since competitive analysis is of concern in this line of work, the algorithm is not treated
as a black box and must be explicitly handcrafted for each different online problem. On the other hand,
we introduce a general end-to-end framework that can handle online matching problems with different
objectives and constraints, albeit without theoretical performance guarantees.
4 Learning Deep Policies for Matching
We now formalize online matching as a Markov Decision Process (MDP). We then present a set of neural
network architectures with different representational capabilities, numbers of parameters, and assumptions
on the size of the graphs. An extensive set of features have been designed to facilitate the learning of high-
performance policies. We conclude this section by mentioning the RL training algorithm we use as well as
a supervised behavioral cloning baseline.
4.1 MDP Formulation
The online bipartite matching problem can be formulated in the RL setting as a Markov Decision Pro-
cess as follows; see Fig. 1 for a high-level illustration. Each instance of the online matching problem is
drawn uniformly at random from an unknown distribution D. The following MDP captures the sequential
decision-making task at hand:
5Published in Transactions on Machine Learning Research (10/2022)
–State: A stateSis a set of selected edges (a matching) and the current (partial) bipartite graph
G. A terminal state ˆSis reached when the final node in Varrives. The length of an episode is
T=|V|.
–Action: The agent has to pick an edge to match or skip. At each timestep t, a nodevtarrives
with its edges. The agent can choose to match vtto one of its neighbors in Uor leave it un-
matched. Therefore, |At|, the maximum number of possible actions at time tis|Ngbr (v)|+ 1,
whereNgbr (v)is the set of Unodes with edges to v. Note that there can exist problem-specific
constraints on the action space, e.g., a fixed node can only be matched once in E-OBM. Unlike the
majority of domains where RL is applied, the uncertainty is exogenous here. Thus, the transition
isdeterministic regardless of the action picked. That is, the (random) arrival of the next node is
independent of the previous actions.
–Reward function : The reward r(s,a)is defined as the weight of the edge selected with action
a. Hence, the cumulative reward Rat the terminal state ˆSrepresents the total weight of the final
matching solution:
R=/summationdisplay
e∈ˆSwe.
–Policy: A solution (matching) is a subset of the edges in E,π=¯E⊂E. A stochastic policy,
parameterized by θ, outputs a solution πwith probability
pθ(π|G) =|V|/productdisplay
t=1pθ(πt|st),
wherestrepresents the state at timestep t,Grepresents the full graph, and πtrepresents the
action picked at timestep tin solution π.
4.2 Deep Learning Architectures
In this section, we propose a number of architectures that can be utilized to learn effective matching poli-
cies. Unless otherwise stated, the models are trained using RL.
Feed-Forward (ff): When node vtarrives, the ffpolicy will take as input a vector
(w0,...,w|U|,m0,...,m|U|)∈R2(|U|+1)1, wherewuis the weight of the edge from vtto fixed node
u(withwu= 0ifvis not a neighbor of u), andmuis a binary mask representing the availability of node
ufor matching. The policy will output a vector of probabilities of size |U|+ 1, where the additional action
represents skipping. ffis similar to the architecture presented in Kong et al. (2019).
Feed-Forward with history (ff-hist): This model is similar to ffbut takes additional historical in-
formation about the current graph to better reason about future input. That is, ff-hist will take in
a vector consisting of five concatenated vectors, (w,m,ht,gt,nt). The vectors wandmare the same as
those in ff. The feature vectors h,n,gcontain a range of node-level features such as average weights seen
so far per fixed node and solution-level features such as maximum weight in current solution; see Table 1
for details.
Invariant Feed-Forward (inv-ff): We present an invariant architecture, inspired by Andrychow-
icz et al. (2016), which processes each of the edges and their fixed nodes independently using the same
(shared) feed-forward network; see Fig. 2 for an illustration. That is, inv-ffwill take as input a 3-
dimensional vector, (wu,su,wmean), wherewmeanis the mean of the edge weights incident to incoming
nodevt, andsuis a binary flag set to 1 if uis the “skip” node. The output for each potential edge is a
single number ou. The vector ois normalized using the softmax to output a vector of probabilities.
1The extra input represents the skip node, which is not needed for ffand ff-hist , but we add it to make the input
consistent across models.
6Published in Transactions on Machine Learning Research (10/2022)
Figure 2: Invariant ( inv-ff) Architecture. A shared feed-forward neural network takes in node-specific
features and outputs a single number for each node in U. The outputs are then fed into the softmax func-
tion to give a vector of probabilities. The red node represents skipping.
Invariant Feed-Forward with history (inv-ff-hist ): An invariant model, like inv-ff, which uti-
lizes historical information. It is important to note that inv-ff-hist will only look at historical fea-
tures of one node at a time, in addition to solution-level features. Therefore, the node-wise input is
(wu,mu,su,wmean,nt,gt,u,ht).
Supervised Feed-Forward with history (ff-supervised ): To test the advantage of using RL meth-
ods, we train ff-hist in a supervised learning fashion. In other words, each incoming node is considered a
data sample with a target (the optimal Unode to match, in hindsight). During training, after all Vnodes
have arrived, we minimize the cross-entropy loss across all the samples. This setup is equivalent to behav-
ior cloning (Ross & Bagnell, 2010) where expert demonstrations are divided into state-action pairs and
treated as i.i.d. samples.
Graph Neural Network (gnn-hist ): In this model, we employ the encoder-decoder architecture used in
many combinatorial optimization problems, see (Cappart et al., 2021). At each timestep t, the graph en-
coder consumes the current graph and produces embeddings for all nodes. The decoder feed-forward neu-
ral network, which also takes node-wise inputs, will take in (wu,t/|V|,mu,su,pt,evt,eu,emean,es)where
the last four inputs represent the embedding of the incoming node vt, embedding of the fixed node ubeing
considered, mean embedding of all fixed nodes, and mean solution embedding, respectively. Our graph
encoder is a MPNN (Gilmer et al., 2017) with the weights as edge features. The mean solution embedding
is defined as the sum of a learnable linear transformation of the concatenation of the embeddings of the
vertices of the edges in the solution S:
es=1
|S|/summationdisplay
(u,v)∈SΘe([eu;ev]), (1)
where “;” represents horizontal concatenation, Θeis a learnable parameter, and Sis the set of all match-
ings made so far. The mean of the embeddings of all fixed nodes is calculated simply as:
emean =1
|¯U|/summationdisplay
u∈¯Ueu. (2)
where ¯U=U∪{uskip}anduskiprepresents the skip node, i.e., matching to this node means skipping. The
graph encoder also takes in problem-specific node features if available; see Appendix B.2 for details. The
output of the encoder is fed into a feed-forward network which outputs a distribution over available edges.
7Published in Transactions on Machine Learning Research (10/2022)
Table 1: Features used in ff-hist and inv-ff-hist .dt
urepresents degree of node uat timet.uskiprep-
resents the skip node, i.e., matching to this node means choosing to skip.
Feature type Description Equation Size
Average weight per fixed node u
up to time tµw=1
dtu/summationtext
(u,vi)∈E:
vi∈V,
1≤i<tw(u,vi)|U|+ 1
Graph-Level
FeaturesgtVariance of weights per fixed node u
up to time tσw=1
dtu/summationtext
(u,vi)∈E:
vi∈V,
1≤i<t(w(u,vi)−µw)2|U|+ 1
Average degree per fixed node u
up to time t1
t|{(u,vi)∈E:i≤t}||U|+ 1
Incoming Node
FeaturesntPercentage of fixed nodes incident
to incoming vt(For invariant models only)1
|U||{(u,vt)∈E:u∈U}| 1
Normalized step size at time tt
|V|1
Solution-Level
FeatureshtMaximum weight in current
matching solutionmax e∈Swe 1
Minimum weight in current
matching solutionmine∈Swe 1
Mean weight in current
matching solutionµS=1
|S|/summationtext
e∈Swe 1
Variance of weights in current
matching solutionσS=1
|S|/summationtext
e∈S(we−µS)21
Ratio of already matched nodes in U1
|U||{(u,v)∈S,u̸=uskip}| 1
Ratio of skips made up to time t1
t|{(u,v)∈S,u=uskip}| 1
The normalized size of
current matching solutionpt=1
|U|/summationtext
e∈Swe 1
The models outlined above are designed based on a set of desirable properties for matching. Table 2 sum-
marizes the properties that are satisfied by each model:
•Graph Size Invariance : Training on large graph instances may be infeasible and costly. Thus, it
would be ideal to train a model on small graphs if it generalizes well to larger graphs with a simi-
lar generating distribution. We utilize normalization in a way to make sure that each statistic (fea-
ture) that we compute lies within a particular range, independently of the graph size. Moreover,
the invariant architectures allow us to train small networks that only look at node-wise inputs
and share parameters across all fixed nodes. It is also worth noting that the invariance property
can be key to OBM variants where Uis not fixed, e.g., 2-sided arrivals (Dickerson et al., 2018), an
application that is left for future work.
•Permutation Invariance : In most practical applications, such as assigning jobs to servers or
web advertising, the ordering of nodes in the set Ushould not affect the solution. The invariant
architectures ensure that the model outputs the same solution regardless of the permutation of the
nodes inU. On the other hand, the non-invariant models such as ffwould predict differently for
the same graph instance if the Unodes were permuted.
•History-Awareness : A state space defined based on the entire current graph and the current
matching will allow the model to learn smarter strategies that reason about the future based on
the observed past. Historical and graph-based information within the current graph gives the
models an “identity” for each fixed node which may be lost due to node-wise input. Contextual
features such as incoming node features nt(see Table 1) and the ratio of already matched nodes
help the learned policies to generalize to different graph sizes and U-to-Vratios.
8Published in Transactions on Machine Learning Research (10/2022)
Table 2: Important model characteristics. L: Number of hidden layers, H: Hidden layer size, E: Embedding
dimension.
ModelGraph size
InvariancePermutation
InvarianceHistory
AwarenessNode-feature
AwarenessLearnable
Parameters
inv-ff ✓ ✓ O(LH2)
ff O(LH2+|U|H)
ff-hist ✓ O(LH2+|U|H)
ff-supervised ✓ O(LH2+|U|H)
inv-ff-hist ✓ ✓ ✓ O(LH2)
gnn-hist ✓ ✓ ✓ ✓ O(LH2+EH +E2)
•Node-feature Awareness : In real-world scenarios, nodes in UandVrepresent entities with
features that can be key to making good matching decisions. For example, incoming nodes can be
users with personal information such as age, gender, and occupation. The node features can be
leveraged to obtain better matchings. Our GNN model supports node features. Other models can
be modified to take such additional features but would need to be customized to the problem at
hand.
4.3 Training Algorithms
RL Models : Because our focus is on flexible modeling of OBM-type problems with deep learning ar-
chitectures, we have opted to leverage existing training algorithms with little modification. We use the
REINFORCE algorithm (Sutton & Barto, 2018), both for its effectiveness and simplicity:
∇L(θ|s) =Epθ(π|s)[(L(π)−b(s))∇logpθ(π|s)].
To reduce gradient variance and noise, we add a baseline b(s)which is the exponential moving average,
b(s) =M, whereMis the negative episode reward , L(π), in the first training iteration and the update
step isb(s) =βM+ (1−β)L(π)(Sutton & Barto, 2018).
Supervised Models : All incoming nodes are treated as independent samples with targets. Therefore, for
a batch ofNbipartite graphs with Tincoming nodes, we minimize the weighted cross entropy loss:
1
N×TN/summationdisplay
i=1T/summationdisplay
j=1loss(pi
j,ti
j,c)
wherepi
jis output of the policy for graph instance iat timestep j,ti
jis the target which is generated by
solving an integer programming formulation on the full graph in hindsight (see Appendix D for details),
andcis the weight vector of size |U|+ 1. All classes are given a weight of 1 except the skipping class which
is given a weight of|U|
|V|. This is to prevent overfitting when most samples belong to the skipping class, i.e.,
when|V|≫|U|and most incoming nodes are left unmatched.
Masking is utilized to prevent all models from picking non-existent edges or already matched nodes.
5 Experimental Setup
5.1 Dataset Preparation
We train and test our models across two synthetically generated datasets from the Erdos-Renyi (ER)
(Erdos & Renyi, 1960) and Barabasi-Albert (BA) (Albert & Barabási, 2002) graph families. In addition,
we use two datasets generated from real-world base graphs. The gMission base graph (Chen et al., 2014)
comes from crowdsourcing data for assigning workers to tasks. We also use MovieLens (Harper & Konstan,
2015), which is derived from data on users’ ratings of movies based on Dickerson et al. (2019). Table 3
9Published in Transactions on Machine Learning Research (10/2022)
summarizes the datasets and their key properties. In our experiments, we generate two versions of each
real-world dataset: one where the same fixed nodes are used for all graph instances (gMission, MovieLens),
and one where a new set of fixed nodes is generated for each graph instance (gMission-var, MovieLens-
var).
To generate a bipartite graph instance of size |U|by|V|from the real-world base graph, we sample |U|
nodes uniformly at random without replacement from the nodes on the left side of the base graph and
sample|V|nodes with replacement from the right side of the base graph. A 10x30 graph is one with |U|=
10,|V|= 30, a naming convention we will adopt throughout. We note that our framework could be used in
the non-i.i.d arrival settings. However, the graph generation process depends on the i.i.d assumption, since
we sample nodes from the base graph at random.
Erdos-Renyi (ER): We generate bipartite graph instances for the E-OBM problem using the Erdos-
Renyi (ER) scheme (Erdos & Renyi, 1960). Edge weights are sampled from the uniform distribution
U(0,1]. For each graph size, e.g., 10x30, we generate datasets for a number of values of p, the probabil-
ity of an edge being in the graph.
Barabasi-Albert (BA): We follow the same process described by (Borodin et al., 2020) for generating
preferential attachment bigraphs. To generate a bigraph in this model, start with |U|offline nodes and
introduce online nodes Vone at a time. The model has a single parameter pwhich is the average degree
of an online node. Upon arrival of a new online node v∈V, we sample nv∼Bionomial (|U|,p/|U|)to
decide the number of the neighbours of v. Letµbe a probability distribution over the nodes in Udefined
byµ(u) =1+degree (u)
|U|+/summationtext
u∈Udegree (u). We sample offline nodes according to µfromUuntilnvneighbours are
selected.
gMission : In this setting, we have a set of workers available offline and incoming tasks which must be
matched to compatible workers (Chen et al., 2014). Every worker is associated with a location in Euclid-
ian space, a range within which they can service tasks, and a success probability with which they will
complete any task. Tasks are represented by a Euclidean location and a payoff value for being completed.
We use the same strategy as in (Dickerson et al., 2018) to pre-process the dataset. That is, workers that
share similar locations are grouped into the same “type”, and likewise for tasks. An edge is drawn between
a worker and a task if the task is within the range of the worker. The edge weight is calculated by multi-
plying the payoff for completing the task with the success probability. In total, we have 532 worker types
and 712 task types.
To generate a bipartite graph instance of size |U|by|V|, we sample|U|workers uniformly at random with-
out replacement from the 532 types and sample |V|tasks with replacement from D. We setDto be uni-
form. That is, the graph generation process involves sampling node from Vin the base graph uniformly.
MovieLens : The dataset consists of a set of movies each belonging to some genres and a set of users
which can arrive and leave the system at any time. Once a user arrives, they must be matched to an avail-
able movie or left unmatched if no good movies are available. We have historical information about the
average ratings each user has given for each genre. The goal is to recommend movies which are relevant
and diverse genre-wise. This objective is measured using the weighted coverage function over the set of
genres (see Section 2). Therefore, we must maximize the sum of the weighted coverage functions of all
users which have arrived.
The MovieLens dataset contains a total of 3952 movies, 6040 users, and 100,209ratings of the movies
by the users. As in (Dickerson et al., 2019), we choose 200 users who have given the most ratings and
sample 100 movies at random. We then remove any movies that have no neighbors with the 200 users to
get a total of 94 movies. These sets of movies and users will be used to generate all bipartite graphs. We
calculate the average ratings each user has given for each genre. These average ratings will be used as the
weights in the coverage function (see section 2.1). To generate an instance of size |U|by|V|, we sample
|U|movies uniformly at random without replacement from the 94 movies and |V|users with replacement
according to the uniform arrival distribution D. The full graph generation procedure for gMission and
MovieLens can be seen in Algorithm 3 of Appendix C.
10Published in Transactions on Machine Learning Research (10/2022)
Table 3: Datasets used for our experiments. pis the average node degree in BA graphs.
Type Problem Base Graph Size Node Attributes? Weight Generation
ER E-OBM w(u,v)∼U(0,1]
BA E-OBM w(u,v)∼N(degree (u),p/5)
gMission E-OBM 532 jobs ×712 workers payoff for computing the task ×the success probability
MovieLens OSBM 94 movies ×200 users ✓average ratings each user has given for
each genre is used as the weights in the coverage function
5.2 Evaluation
Evaluation Metric: We use the optimality ratio averaged over the set of test instances. The optimality
ratio of a solution Son a graph instance Gis defined as O(S,G) =c(S)
OPT (G), wherec(S)is the objective
value ofSandOPT (G)is the optimal value on graph instance G, which is computed in hindsight using
integer programming; see Appendix D.
Baselines: For E-OBM, we compare our models to the greedybaseline, which simply picks the
maximum-weight edge, and greedy-rt (Ting & Xiang, 2014), a randomized algorithm which is near-
optimal in the adversarial setting. In an effort to compare to strong tunable baselines, we implemented
greedy-t , which picks the maximum-weight edge with weight above a dataset-specific threshold wTthat
is tuned on the training set. If no weight is above the threshold, then we skip (see Appendix B.4 for de-
tails). To best of our knowledge, this is the first application of such a baseline to real-world datasets. For
OSBM, we only use greedyas (Dickerson et al., 2019) find that it achieves a better competitive ratio than
their algorithms when movies cannot be matched more than once and the incoming user can be matched
to one movie at a time, which is the setting we study here.
5.3 Hyperparameter Tuning and Training Protocol
In a nutshell, around 400 configurations, varying four hyperparameters, are explored using Bayesian opti-
mization (Biewald, 2020) on a small validation set consisting of small graphs (10x30) from the gMission
dataset. We have found the models to be fairly robust to hyperparameter values. In fact, most configura-
tions with low learning rates (under 0.01) result in satisfactory performance as seen in Fig 3. The model
with the best average optimality ratio on the validation set is selected for final evaluation, the results of
which will be shown in the next section. Some hyperparameters are fixed throughout, particularly the
depths/widths of the feed-forward networks (2-3 layers, 100-200 neurons), and the use of the ReLU as acti-
vation function. Training often takes less than 6 hours on a NVIDIA v100 GPU. Full details are deferred
to appendices B.3 and B.1.
Figure 3: Top 200 hyperparameter tuning results for ff-hist on gMission 10x30. Each curve represents a
hyperparameter configuration. Lighter color means better average optimality ratio on the validation set.
11Published in Transactions on Machine Learning Research (10/2022)
0.05 0.1 0.15 0.2
p0.30.40.50.60.70.80.91.0 Optimality RatioE-OBM ER 10x30
greedy
greedy-rt
greedy-t
ff-supervised
ff
ff-hist
inv-ff
inv-ff-hist
gnn-hist
0.05 0.1 0.15 0.2
p0.30.40.50.60.70.80.91.0 Optimality RatioE-OBM ER 10x60
0.05 0.1 0.15 0.2
p0.50.60.70.80.9 Optimality RatioE-OBM ER 100x100
Figure 4: Distributions of the Optimality Ratios for E-OBM on ER graphs. The graph family parameter p
is the probability of a random edge existing in the graph.
6 Experimental Results
6.1 Edge-Weighted Online Bipartite Matching
For E-OBM, we will analyze the performance of the models across ER and BA graphs as well as the gMis-
sion datatset. The edges and weights in the ER graphs are generated from a uniform distribution. Thus,
ER graphs do not have special graph properties such as the existence of community structures or the
occurrence of structural motifs. As a result, the ER dataset is hard to learn from as the models would
merely be able to leverage the |U|-to-|V|ratio and the density of the graph (the graph family parameter is
proportional to the expected node degree in a graph). Unlike ER graphs, explicit structural patterns are
found in BA graphs. The BA graph generation process captures heterogeneous and varied degree distribu-
tions which are often observed in real world graphs (Barabási & Pósfai, 2016). For example, graphs with
many low-degree nodes and a few high-degree nodes occur in practical domains where the rich gets richer
phenomenon is witnessed. The BA graph generation process is described in Appendix C. In our experi-
ments, nodes with higher degrees also have higher weights in average. We also study the models under the
gMission dataset. Like many real-world datasets, the exact properties of the graphs are unknown. Thus,
the models may derive policies based on implicit graph properties. The results will demonstrate that the
models have taken advantage of some existing patterns in the dataset.
Trends in decisions with respect to the |U|-to-|V|ratio and graph sparsity: When|U|<|V|,
the models outperform the greedy strategies since they learn that skipping would yield a better result in
hindsight, despite missing a short-term reward. This is apparent for the 10x30 and 10x60 graphs in Fig-
ure 4 for ER and Figure 5 (b) for gMission. To substantiate this and other hypotheses about the behavior
of various policies, we use “agreement plots” such as those in Figure 6. An agreement plot shows how fre-
quently the different policies agree with a reference policy, e.g., with a hindsight-optimal solution or with
the greedy method. Appendix E includes agreement plots w.r.t. greedy: most disagreements between the
learned policies and greedy happen in the beginning but all methods (including greedy) imitate the opti-
mum towards the end, when most actions consist in skipping due to the fixed nodes having been matched
already.
Outperforming greedy on 100x100 (3rd plot in Fig. 4) ER graphs is quite a difficult task, as there is not
much besides the graph density for the models to take advantage of. Since |V|=|U|, skipping is also
rarely feasible. Hence, the models perform similarly to greedy.
12Published in Transactions on Machine Learning Research (10/2022)
0.840.860.880.90.920.940.96 Optimality RatioE-OBM BA 100x100
0.80.820.840.860.880.90.920.940.96 Optimality RatioE-OBM BA 100x100
greedy
greedy-rt
greedy-t
ff-supervised
ff
ff-hist
inv-ff
inv-ff-hist
gnn-hist
(a) Distributions of the Optimality Ratios for E-OBM on BA graphs with average node degree 5, and weights
w(u,v)∼N(deg(u),1), and BA with average node degree 15, and weights w(u,v)∼N(deg(u),3).
0.60.70.80.91.0 Optimality RatioE-OBM gMission 10x30
0.50.60.70.80.91.0 Optimality RatioE-OBM gMission 10x60
0.7750.80.8250.850.8750.90.9250.950.975 Optimality RatioE-OBM gMission 100x100
(b) Distributions of the Optimality Ratios for E-OBM on the gMission dataset.
0.60.70.80.91.0 Optimality RatioE-OBM gMission-var 10x30
0.50.60.70.80.91.0 Optimality RatioE-OBM gMission-var 10x60
0.7750.80.8250.850.8750.90.9250.95Optimality RatioE-OBM gMission-var 100x100
(c) Distributions of the Optimality Ratios for E-OBM on gMission-var.
Figure 5: Distributions of the Optimality Ratios for E-OBM on BA and gMission.
As the ER graphs get denser (moving to the right within the first three plots of Fig. 4), the gap between
the models and the greedy baselines increases as there is a higher chance of encountering better future
options in denser graphs. Hence, the models learn to skip as they find it more rewarding over the long
term. This can be further seen in Fig. 6, where the models agree less with greedy on denser ER graphs.
For gMission (right side of Fig. 6), most disagreements happen in the beginning but all models imitate the
optimum towards the end when most actions consist in skipping; Appendix E has more agreement plots.
Model-specific Results: Models with history, namely inv-ff-hist (gray) and ff-hist (brown), consis-
tently outperform their history-less counterparts, ffandinv-ff, across all three datasets (Figure 5).
inv-ffreceives the same information as greedyand performs fairly similar to it on gMission and ER
graphs. In fact, inv-fflearns to be exactly greedy on gMission 100x100 (see Appendix E). However,
inv-ffperforms better than other non-invariant models on the BA dataset. The ideal policy on BA
13Published in Transactions on Machine Learning Research (10/2022)
4060800.05
4060800.1
4060800.15
0 5 10 15 20 25 30
Timestep4060800.2Agreement with Optimal for ER 10x30Agreement per Timestep %
0 5 10 15 20 25 30
Timestep304050607080
greedy
ff-supervised
ff
ff-hist
inv-ff
inv-ff-hist
gnn-histAgreement with Optimal for gMission 10x30
Agreement per Timestep %
Figure 6: Percent agreement with the optimal solution per timestep. A point (timestep t, agreement a)
on this plot can be read as: at timestep t, this method makes the same matching decision as the optimal
solution on a%of the test instances.
graphs would discover that matching to a high-degree node is not wise since the node will likely receive
a lot more edges in the future. Similarly, matching to a low-degree node is desired since the node will
probably not have many edges in the future. The node-wise reasoning of the invariant models is effective
at learning to utilize such node-specific graph property and following a more feasible policy. Armed with
historical context, inv-ff-hist outperforms all other models on BA graphs (Fig. 5a).
The best performance on ER and gMission is achieved by ff-hist since the model gets all informa-
tion (weights) at once (Fig. 5). However, when Unodes are permuted, inv-ff-hist vastly outperforms
ff-hist, as shown in Appendix G.
ff-supervised performs well but not as good as RL models since supervised learning comes with its own
disadvantages, i.e, overfitting when there are more skip actions than match, and being unable to reason
sequentially when it makes a wrong decision early on. The latter is a well-known fatal flaw in behavior
cloning, as observed by Ross & Bagnell (2010) and others.
greedy-t performs well compared to greedy, which shows the advantage of strategically skipping if
weights do not pass a tuned threshold. However, it is still outperformed by the learned models, especially
on BA graphs where the graphs exhibit explicit patterns and gMission 100x100.
In general, the choice of the best model is dependent on the problem, but we provide some empirical ev-
idence on how this choice should be made. The invariant models with history ( inv-ff-hist ,gnn-hist )
are the best performing models and most recommended to be used in practice as they are invariant to
|U|, can support more general OBM variants such as 2-sided arriving nodes, and can take advantage of
node/edge features. For settings where |U|is always fixed (e.g., scheduling jobs to servers), ff-hist is the
best as it can see all arriving edges at once and takes advantage of history.
Some advantages to using invariant models: We also experiment with a variation on the gMission
dataset, where a new set of fixed nodes is generated for each graph instance. We see the same pattern as
in gMission (where the same fixed nodes in |U|existed across all instances), except non-invariant models
14Published in Transactions on Machine Learning Research (10/2022)
greedygreedy-t
ff-supervisedff
ff-hist inv-ff
inv-ff-histgnn-hist10x30
10x60
100x100
100x2000.76 0.83 0.85 0.84 0.86 0.84 0.87 0.87
0.69 0.81 0.86 0.81 0.86 0.82 0.87 0.87
0.88 0.81 - - -0.80 0.82 0.84
0.76 0.88 - - -0.90 0.91 0.91Graph Transferability Trained On 10x30
greedygreedy-t
ff-supervisedff
ff-hist inv-ff
inv-ff-histgnn-hist10x30
10x60
100x100
100x2000.76 0.80 0.80 0.80 0.85 0.79 0.84 0.84
0.69 0.84 0.86 0.85 0.89 0.85 0.89 0.89
0.88 0.71 - - -0.62 0.73 0.69
0.76 0.85 - - -0.86 0.90 0.88Graph Transferability Trained On 10x60
Figure 7: Graph Transferability on gMission-var: Average optimality ratios for models trained on graphs
of size 10x30 (left) & 10x60 (right) and tested on graphs of different sizes. Missing values are denoted with
a dash for models that are not invariant to the number of Unodes of the training graphs.
degrade substantially for 100x100. This is because the input size increased substantially but the model’s
hidden layer sizes were kept constant. The same issue is seen for BA graphs. A significant disadvantage
of non-invariant models is that they are not independent of |U|, so model size needs to be increased with
|U|. Invariant models are unaffected by this issue as they reason node-wise . We notice that this problem is
not seen in gMission. One explanation for this is that fixed nodes are the same across all instances so mod-
els have a good idea of the weight distribution for each fixed node which is the same during testing and
training. Therefore, even though the model size is not increased as the input size increases, the models can
in some sense “guess” the incoming weights and so do not need as much of an increase in capacity. Once
again, models with history display better performance as history helps the models build a better “identity”
of each fixed node as more nodes come in even if never seen during training.
Do models trained on small graphs transfer to larger graphs? In Fig. 7, we train all models on
10x30 and 10x60 graphs separately and test their transferability to graphs with different |U|-to-|V|ratios
up to size 100x200. gnn-hist and inv-ff-hist perform especially well on graphs with similar |U|-to-|V|
ratio. For 100x100 graphs, inv-ffand greedy-t perform poorly as they do not receive any features that
give them context within a new graph size such as number of available fixed nodes.
6.2 Online Submodular Bipartite Matching (OSBM)
The inherent complexity of the OSBM problem and the real-world datasets provide a learning-based ap-
proach with more information to leverage. As such, the models tend to discover policies which do signifi-
cantly better than greedyas shown in Fig. 8.
The benefit of RL models is apparent here when compared to ff-supervised , particularly for 10x30 and
94x100 graphs. The relative complexity of OSBM compared to E-OBM will require the model to be more
generalizable as the reasoning involved is more complex and mere imitation is not enough. ff-supervised
also underperforms because the edge weights depend on the current solution and can change on the same
graph instance if previous matches are different, causing a great mismatch with the training data.
A similar trend to E-OBM results is observed here: models with history outperform their history-less coun-
terparts. The context provided by history is particularly helpful as the edge weights depend on previous
matches. Furthermore, we notice that gnn-hist has the best performance on 10x30 and 94x100 graphs as
gnn-hist is the only model that uses user attributes as node features.
We witness the same issue seen in gMission-Var (5). The non-invariant models degrade on 94x100 graphs
due to having the same number of hidden layer despite processing larger graphs. The invariant models
15Published in Transactions on Machine Learning Research (10/2022)
remain unaffected by the graph size. Interestingly, the invariant models even slightly outperform their
non-invariant counterparts on 10x30 and 10x60 MovieLens-var.
0.750.80.850.90.951.0Optimality RatioOSBM MovieLens 10x30
0.750.80.850.90.951.0Optimality RatioOSBM MovieLens 10x60
0.860.880.90.920.940.960.981.0Optimality RatioOSBM MovieLens 94x100
0.50.60.70.80.91.0 Optimality RatioOSBM MovieLens-var 10x30
0.650.70.750.80.850.90.951.0Optimality RatioOSBM MovieLens-var 10x60
0.80.850.90.951.0Optimality RatioOSBM MovieLens-var 94x100
greedy
ff-supervisedff
ff-histinv-ff
inv-ff-histgnn-hist
Figure 8: Distributions of the Optimality Ratios for OSBM on three graph sizes for MovieLens and
MovieLens-var. Higher is better.
7 Conclusion and Future Work
Through extensive experiments, we have demonstrated that deep reinforcement learning with appropri-
ately designed neural network architectures and feature engineering can produce high-performance online
matching policies across two problems spanning a wide range of complexity. In particular, we make the
following concluding observations:
•A basic reinforcement learning formulation and training scheme are sufficient to produce good
learned policies for online matching, are typically not sensitive to the choice of hyperparameters,
and are advantageous compared to a supervised learning approach;
•Compared to greedy policies, RL-based policies are more effective, a result that can be partially
explained by a stronger agreement with the optimal solution (in hindsight) in the early timesteps
of the process when greedy is too eager to match. RL policies tend to perform particularly well
when trained and tested on dense graphs or ones with a strong structural pattern;
•Models that are invariant to the number of nodes are more advantageous than fully-connected
models in terms of how well they generalize to test instances that are slightly perturbed compared
to the training instances, either in graph size or in the identities of the fixed nodes;
•Feature engineering at the node and graph levels can help model the history of the matching
process up to that timestep, resulting in improved solutions compared to models that use only
weight information from the current timestep;
16Published in Transactions on Machine Learning Research (10/2022)
•Graph Neural Network models are a viable alternative to feed-forward models as they can leverage
node features and their dependencies across nodes more naturally.
Future avenues of research include:
•A more extensive experimental analysis of different RL training algorithms beyond basic policy
gradient;
•Extensions to new real-world datasets with rich node and edge features that could benefit even
more from highly expressive models such as GNNs;
•Extensions to other online combinatorial optimization problems, which can leverage our frame-
work, models, and code as a starting point.
17Published in Transactions on Machine Learning Research (10/2022)
References
Réka Albert and Albert-László Barabási. Statistical mechanics of complex networks. Rev. Mod. Phys. , 74:47–
97, Jan 2002. doi:10.1103/RevModPhys.74.47. URL https://link.aps.org/doi/10.1103/RevModPhys.
74.47.
Marcin Andrychowicz, Misha Denil, Sergio Gomez, Matthew W Hoffman, David Pfau, Tom Schaul, Bren-
dan Shillingford, and Nando De Freitas. Learning to learn by gradient descent by gradient descent. In
Advances in neural information processing systems , pp. 3981–3989, 2016.
Antonios Antoniadis, Themis Gouleakis, Pieter Kleer, and Pavel Kolev. Secretary and online match-
ing problems with machine learned advice. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Bal-
can, and H. Lin (eds.), Advances in Neural Information Processing Systems , volume 33, pp. 7933–
7944. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/file/
5a378f8490c8d6af8647a753812f6e31-Paper.pdf .
Pranjal Awasthi and Tuomas Sandholm. Online stochastic optimization in the large: Application to kidney
exchange. In IJCAI, 2009.
Albert-László Barabási and Márton Pósfai. Network science . Cambridge University Press, Cambridge, 2016.
ISBN 9781107076266 1107076269. URL http://barabasi.com/networksciencebook/ .
Thomas D. Barrett, William R. Clements, Jakob N. Foerster, and A. I. Lvovsky. Exploratory combi-
natorial optimization with reinforcement learning. In The Thirty-Fourth AAAI Conference on Ar-
tificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence
Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelli-
gence, EAAI 2020, New York, NY, USA, February 7-12, 2020 , pp. 3243–3250. AAAI Press, 2020. URL
https://aaai.org/ojs/index.php/AAAI/article/view/5723 .
Yoshua Bengio, Andrea Lodi, and Antoine Prouvost. Machine learning for combinatorial optimization: a
methodological tour d’horizon. European Journal of Operational Research , 290(2):405–421, 2021.
Lukas Biewald. Experiment tracking with weights and biases, 2020. URL https://www.wandb.com/ .
Software available from wandb.com.
Allan Borodin, Christodoulos Karavasilis, and Denis Pankratov. An experimental study of algo-
rithms for online bipartite matching. ACM J. Exp. Algorithmics , 25, March 2020. ISSN 1084-6654.
doi:10.1145/3379552. URL https://doi.org/10.1145/3379552 .
Quentin Cappart, Didier Chételat, Elias B. Khalil, Andrea Lodi, Christopher Morris, and Petar Velickovic.
Combinatorial optimization and reasoning with graph neural networks. CoRR, abs/2102.09544, 2021.
URL https://arxiv.org/abs/2102.09544 .
Xinyun Chen and Yuandong Tian. Learning to perform local rewriting for combinatorial optimiza-
tion. In Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d’Alché-Buc, Emily B. Fox,
and Roman Garnett (eds.), Advances in Neural Information Processing Systems 32: Annual Confer-
ence on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancou-
ver, BC, Canada , pp. 6278–6289, 2019. URL https://proceedings.neurips.cc/paper/2019/hash/
131f383b434fdf48079bff1e44e2d9a5-Abstract.html .
Zhao Chen, Rui Fu, Ziyuan Zhao, Zheng Liu, Leihao Xia, Lei Chen, Peng Cheng, Caleb Chen Cao,
Yongxin Tong, and Chen Jason Zhang. Gmission: A general spatial crowdsourcing platform. Proc.
VLDB Endow. , 7(13):1629–1632, August 2014. ISSN 2150-8097. doi:10.14778/2733004.2733047. URL
https://doi.org/10.14778/2733004.2733047 .
Hanjun Dai, Elias B. Khalil, Yuyu Zhang, Bistra Dilkina, and Le Song. Learning combinatorial op-
timization algorithms over graphs. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus,
S. Vishwanathan, and R. Garnett (eds.), Advances in Neural Information Processing Systems , vol-
ume 30. Curran Associates, Inc., 2017. URL https://proceedings.neurips.cc/paper/2017/file/
d9896106ca98d3d05b8cbdf4fd8b13a1-Paper.pdf .
18Published in Transactions on Machine Learning Research (10/2022)
Ilias Diakonikolas, Vasilis Kontonis, Christos Tzamos, Ali Vakilian, and Nikos Zarifis. Learning online
algorithms with distributional advice. In Marina Meila and Tong Zhang (eds.), Proceedings of the 38th In-
ternational Conference on Machine Learning , volume 139 of Proceedings of Machine Learning Research ,
pp. 2687–2696. PMLR, 18–24 Jul 2021. URL http://proceedings.mlr.press/v139/diakonikolas21a.
html.
John P. Dickerson, Karthik Abinav Sankararaman, Aravind Srinivasan, and Pan Xu. Assigning tasks to
workers based on historical data: Online task assignment with two-sided arrivals. In Proceedings of
the 17th International Conference on Autonomous Agents and MultiAgent Systems , AAMAS ’18, pp.
318–326, Richland, SC, 2018. International Foundation for Autonomous Agents and Multiagent Systems.
John P. Dickerson, Karthik Abinav Sankararaman, Aravind Srinivasan, and Pan Xu. Balancing relevance
and diversity in online bipartite matching via submodularity. Proceedings of the AAAI Conference on
Artificial Intelligence , 33(01):1877–1884, Jul. 2019. doi:10.1609/aaai.v33i01.33011877. URL https:
//ojs.aaai.org/index.php/AAAI/article/view/4013 .
Paul Erdos and Alfred Renyi. On the evolution of random graphs. Publ. Math. Inst. Hungary. Acad. Sci. , 5:
17–61, 1960.
Matthias Fey and Jan E. Lenssen. Fast graph representation learning with PyTorch Geometric. In ICLR
Workshop on Representation Learning on Graphs and Manifolds , 2019.
Justin Gilmer, Samuel S. Schoenholz, Patrick F. Riley, Oriol Vinyals, and George E. Dahl. Neural message
passing for quantum chemistry. In Proceedings of the 34th International Conference on Machine Learning
- Volume 70 , ICML’17, pp. 1263–1272. JMLR.org, 2017.
Gurobi Optimization, LLC. Gurobi Optimizer Reference Manual, 2021. URL https://www.gurobi.com .
Aric A. Hagberg, Daniel A. Schult, and Pieter J. Swart. Exploring Network Structure, Dynamics, and
Function using NetworkX. In Gaël Varoquaux, Travis Vaught, and Jarrod Millman (eds.), Proceedings
of the 7th Python in Science Conference , pp. 11 – 15, Pasadena, CA USA, 2008.
F. Maxwell Harper and Joseph A. Konstan. The movielens datasets: History and context. ACM Trans.
Interact. Intell. Syst. , 5(4), December 2015. ISSN 2160-6455. doi:10.1145/2827872. URL https://doi.
org/10.1145/2827872 .
Eric Jones, Travis Oliphant, Pearu Peterson, et al. SciPy: Open source scientific tools for Python, 2001–.
URL http://www.scipy.org/ .
Chinmay Karande, Aranyak Mehta, and Pushkar Tripathi. Online bipartite matching with unknown
distributions. In Proceedings of the Forty-Third Annual ACM Symposium on Theory of Computing , STOC
’11, pp. 587–596, New York, NY, USA, 2011. Association for Computing Machinery. ISBN 9781450306911.
doi:10.1145/1993636.1993715. URL https://doi.org/10.1145/1993636.1993715 .
Richard Karp, Umesh Vazirani, and Vijay Vazirani. An optimal algorithm for on-line bipartite matching. In
STOC ’90 , 1990.
Seyed Mehran Kazemi, Rishab Goel, Kshitij Jain, Ivan Kobyzev, Akshay Sethi, Peter Forsyth, and Pascal
Poupart. Representation learning for dynamic graphs: A survey, 2020.
Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Yoshua Bengio and
Yann LeCun (eds.), 3rd International Conference on Learning Representations, ICLR 2015, San Diego,
CA, USA, May 7-9, 2015, Conference Track Proceedings , 2015. URL http://arxiv.org/abs/1412.6980 .
Weiwei Kong, Christopher Liaw, Aranyak Mehta, and D. Sivakumar. A new dog learns old tricks: Rl finds
classic optimization algorithms. 2019. URL https://openreview.net/pdf?id=rkluJ2R9KQ .
Wouter Kool, Herke van Hoof, and Max Welling. Attention, learn to solve routing problems! In Inter-
national Conference on Learning Representations , 2019. URL https://openreview.net/forum?id=
ByxBFsRqYm .
19Published in Transactions on Machine Learning Research (10/2022)
H. W. Kuhn. The hungarian method for the assignment problem. Naval Research Logistics Quarterly , 2
(1-2):83–97, 1955. doi:https://doi.org/10.1002/nav.3800020109. URL https://onlinelibrary.wiley.
com/doi/abs/10.1002/nav.3800020109 .
Nina Mazyavkina, Sergey Sviridov, Sergei Ivanov, and Evgeny Burnaev. Reinforcement learning for combi-
natorial optimization: A survey. Computers & Operations Research , 134:105400, 2021.
A. Mehta, A. Saberi, U. Vazirani, and V. Vazirani. Adwords and generalized on-line matching. In
46th Annual IEEE Symposium on Foundations of Computer Science (FOCS’05) , pp. 264–273, 2005.
doi:10.1109/SFCS.2005.12.
Aranyak Mehta. Online matching and ad allocation. Foundations and Trends in Theoretical Computer
Science, 8 (4):265–368, 2013. URL http://dx.doi.org/10.1561/0400000057 .
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor
Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Ed-
ward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit
Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-
performance deep learning library. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d 'Alché-
Buc, E. Fox, and R. Garnett (eds.), Advances in Neural Information Processing Systems 32 ,
pp. 8024–8035. Curran Associates, Inc., 2019. URL http://papers.neurips.cc/paper/
9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf .
Manish Purohit, Zoya Svitkina, and Ravi Kumar. Improving online algorithms via ml predictions. In
S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett (eds.), Advances
in Neural Information Processing Systems , volume 31. Curran Associates, Inc., 2018. URL https://
proceedings.neurips.cc/paper/2018/file/73a427badebe0e32caa2e1fc7530b7f3-Paper.pdf .
Stephane Ross and Drew Bagnell. Efficient reductions for imitation learning. In Yee Whye Teh and Mike
Titterington (eds.), Proceedings of the Thirteenth International Conference on Artificial Intelligence and
Statistics , volume 9 of Proceedings of Machine Learning Research , pp. 661–668, Chia Laguna Resort,
Sardinia, Italy, 13–15 May 2010. PMLR. URL http://proceedings.mlr.press/v9/ross10a.html .
Jasper Snoek, Hugo Larochelle, and Ryan P. Adams. Practical bayesian optimization of machine learning
algorithms. In Proceedings of the 25th International Conference on Neural Information Processing Systems
- Volume 2 , NIPS’12, pp. 2951–2959, Red Hook, NY, USA, 2012. Curran Associates Inc.
Richard S. Sutton and Andrew G. Barto. Reinforcement Learning: An Introduction . The MIT Press, second
edition, 2018. URL http://incompleteideas.net/book/the-book-2nd.html .
Hingfung Ting and Xiangzhong Xiang. Near optimal algorithms for online maximum weighted b-matching.
In Jianer Chen, John E. Hopcroft, and Jianxin Wang (eds.), Frontiers in Algorithmics , pp. 240–251,
Cham, 2014. Springer International Publishing. ISBN 978-3-319-08016-1.
Shufan Wang, Jian Li, and Shiqiang Wang. Online algorithms for multi-shop ski rental with machine learned
advice. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin (eds.), Advances in Neural
Information Processing Systems , volume 33, pp. 8150–8160. Curran Associates, Inc., 2020. URL https:
//proceedings.neurips.cc/paper/2020/file/5cc4bb753030a3d804351b2dfec0d8b5-Paper.pdf .
Yansheng Wang, Yongxin Tong, Cheng Long, Pan Xu, Ke Xu, and Weifeng Lv. Adaptive dynamic bipartite
graph matching: A reinforcement learning approach. In 2019 IEEE 35th International Conference on
Data Engineering (ICDE) , pp. 1478–1489, 2019. doi:10.1109/ICDE.2019.00133.
Goran Zuzic, Di Wang, Aranyak Mehta, and D. Sivakumar. Learning robust algorithms for online allocation
problems using adversarial training, 2020.
20Published in Transactions on Machine Learning Research (10/2022)
A Implementation Details
All environments are implemented in Pytorch (Paszke et al., 2019). We use NetworkX (Hagberg et al.,
2008) to generate synthetic graphs and find optimal solutions for E-OBM problems. Optimal solutions
for OSBM problems are found using Gurobi (Gurobi Optimization, LLC, 2021); see Appendix D. Pytorch
Geometric (Fey & Lenssen, 2019) is used for handling graphs during training and implementing graph
encoders. Our code is attached to the submission as supplementary material.
B Training and Evaluation
B.1 Training Protocol
We train our models for 300 epochs on datasets of 20000 instances using the Adam optimizer (Kingma &
Ba, 2015). We use a batch size of 200 except for MovieLens, where batch size 100 is used on graphs bigger
than 10x60 due to memory constraints.
Training often takes less than 6 hours on a NVIDIA v100 GPU. gnn-hist takes less than a day to train
on small graphs but consumes more time for bigger graphs and more complicated environments such as
MovieLens 94x100. This is due to re-embedding the graph at every timestep which consumes more mem-
ory and computation as the graph size grows. We believe this can be improved with more efficient embed-
ding schemes for dynamic graphs (Kazemi et al., 2020) but leave this for future work.
0 5000 10000 15000 20000 25000 30000
Step4.04.24.44.64.85.0RewardAverage Validation Reward
ff-supervised
ff
ff-hist
inv-ff
inv-ff-hist
gnn-hist
Figure 9: Average Validation reward during training on gMission 10x30. Note that ff-supervised starts
with a reward of approximately 1.9 at batch 0.
B.2 Node Features
Since gnn-hist supports node features by default, we leverage this property in order for incoming nodes
to have “identities” that can be helpful for the learning of effective strategies.
For the E-OBM problem, the ER and gMission datasets do not have any helpful node attributes that
are not encoded in the edge weights. Therefore, we only provide node attributes that help the encoder
differentiate between different types of nodes. That is, the skip node will get node feature −1, any fixed
nodeiwill get feature (ji)which is 1 if iis already matched and 0 otherwise, and incoming nodes have
feature 2. These features simply help the model differentiate between incoming nodes, fixed nodes, and the
node that represents the skipping action.
21Published in Transactions on Machine Learning Research (10/2022)
In the original MovieLens dataset, the incoming nodes are users while the fixed nodes are movies, both
of which have helpful features. A fixed node ihas feature vector (ji,gi)wheregiis a binary vector that
represents the genres which the movie belongs to. The skip nodes will have feature vector ⃗0. Incoming
users have attributes (gender,occupation,age), each of which is mapped to a number and normalized to
between 0 and 1. See Table 4 for details.
Table 4: User features in the MovieLens dataset.
Attribute Categories Feature Range
Gender Male, Female 0≤i≤1
Age•Under 18
•18-24
•25-34
•35-44
•45-49
•50-55
•56+0≤i≤6
Occupation•Other
•academic/educator
•artist
•clerical/admin
•college/grad student
•customer service
•doctor/health care
•executive/managerial
•farmer
•homemaker
•K-12 student
•lawyer
•programmer
•retired
•sales/marketing
•scientist
•self-employed
•technician/engineer
•tradesman/craftsman
•unemployed
•writer0≤i≤20
B.3 Hyperparameter Tuning
We tune 4 training hyperparameters for each RL model using a held-out validation set of size 1000. The
hyperparameters are the learning rate, learning rate decay, exponential decay parameter, and entropy reg-
ularization rate. For the supervised-ff model, only the learning rate and learning rate decay are tuned.
Hyperparameters are optimized using the Bayesian search method (Snoek et al., 2012) in the Weights &
Biases library (Biewald, 2020) with default parameters. We conduct around 400 runs for each model.
All models are tuned on small bipartite graphs (10 ×30) from the gMission dataset; the same hyper-
parameters are used for bigger graphs at evaluation time. We also use the same hyper-parameters for
all datasets. We have found the models to be fairly robust to different hyperparameters. As can be seen in
Figure 3, most configurations with low learning rates (under 0.01) result in satisfactory performance.
Fixed Hyperparameters : The ffand ff-hist models have 3 hidden layers with 100 neurons each.
inv-ffand inv-ff-hist have 2 hidden layers of size 100. The gnn-hist ’s decoder feed-forward neural
22Published in Transactions on Machine Learning Research (10/2022)
Table 5: Hyperparameter Grid
Hyperparameter Range
Learning Rate {j×10−i|2≤i≤6,1≤j≤9}
Learning Rate Decay {1.0,0.99,0.98,0.97,0.96,0.95}
Exponential Decay β{1.0,0.95,0.9,0.85,0.8,0.7,0.65,0.6}
Entropy Rate γ{j×10−i|1≤i≤4,1≤j≤9}
Algorithm 1 greedy-rt
1:Choose an integer Kuniformly at random in the set N={0,1,...,⌈ln(wmax+ 1)⌉−1}
2:Setτ=eK
3:while a new vertex v∈Varrives do
4:A={u|uisv’s unmatched neighbor in U and w(u,v)≥τ}
5: ifA=ϕthen
6: leavevunmatched
7: else
8: matchvto an arbitrary vertex in A
network has 2 hidden layers of size 200 and the encoder uses embedding dimension 30 with one embedding
layer. All models use the ReLU non-linearity.
Each RL model is tuned on 4 hyperparameters, as seen in Table 5, using a held-out validation set of size
1000. The figure below shows the top 200 hyperparameter search results for ff-hist. Each curve repre-
sents a hyperparameter configuration. Evidently, most configurations with small learning rates result in a
high average optimality ratio. Other models also show similar results in being insensitive to minor hyper-
parameter changes. All experiments are done with a constant random seed. Unlike other RL domains, we
have not found the models to be sensitive to the seed and never had to restart training due to a bad run.
This could be explained by the fact that the transitions in OBM are deterministic regardless of the action
(skip or match). Therefore, the MDP is not highly stochastic.
B.4 Evaluation
greedy-rt baseline : As shown in Algorithm 1, greedy-rt works by randomly picking a threshold be-
tweeneande⌈ln(wmax+1)⌉, wherewmaxis the maximum possible weight. When a new node comes in, we
arbitrarily pick any edge whose weight is at least the threshold, or skip if none exist. Surprisingly, this
simple strategy is near-optimal in the adversarial setting, with a competitive ratio of1
2e⌈ln(wmax+1)⌉(Ting
& Xiang, 2014).
Since greedy-rt does not support weights between 0 and 1, we re-normalize the edge weights in all E-
OBM datasets. For ER and BA graphs, we divide all weights by the minimum weight in the dataset.
For gMission graphs, we re-normalize by multiplying all weights by the maximum weight in the original
dataset.
greedy-t baseline : Gaining intuition from greedy-rt , we implement a baseline where the threshold is
tuned rather than randomly picked. That is, we find a threshold wT∈{0.01,0.02,..., 1.}that achieves
the best average reward on the training set. Then, we use wTas fixed threshold for all test graphs. See
Algorithm 2 for pseudo-code.
C Dataset Generation Details
We provide high-level pseudocode for graph generation in Algorithm 3. Kis the number of fixed nodes, T
is the time horizon and also the number of incoming Vnodes,Nis the number of instances to be gener-
ated, and “type” can be set to “var” to ask for graphs with varying sets of fixed nodes, with the default
being to use the same set of fixed nodes.
23Published in Transactions on Machine Learning Research (10/2022)
Algorithm 2 greedy-t
1:InputwT.
2:while a new vertex v∈Varrives do
3:A={u|uisv’s unmatched neighbor in U and w(u,v)≥wT}
4: ifA=ϕthen
5: leavevunmatched
6: else
7: Matchvwith the maximum-weighted edge to a node in A
Algorithm 3 Graph Generation
1:procedure Generate (K,T,D,G(U∗,V∗,E∗), type,N)
2:D={}
3:iftype != “var” then ▷if all graphs should have same fixed nodes
4:U=u1,...,uK∼Uniform (U∗)▷Sample K fixed nodes without replacement from base graph
5:whilei<Ndo ▷GenerateNgraphs
6:iftype = “var” then
7: U=u1,...,uK∼Uniform (U∗) ▷Re-sample for every graph
8:V,E= {}, {}
9:whilej <Tdo ▷AddTnodes toV
10: v∼D(V∗) ▷Sample according to arrival distribution Dfrom base graph
11: e={(u,v) :u∈U}
12: ife=ϕthen
13: Go to Step 10 ▷Re-sample if incoming node has no neighbors
14: V=V∪{v}
15: E=E∪e
16: j+= 1
17:D=D∪{G(U,V,E )} ▷Add graph instance to dataset
18:i+= 1
returnD ▷ ReturnNgraphs of size KbyT
D Finding Optimal Solutions Offline
To find the optimal solutions for E-OBM, we use the linear_sum_assignment function in the SciPy Li-
brary (Jones et al., 2001–). For OSBM (with the coverage function), we borrow the IP formulation from
(Dickerson et al., 2019) defined below, where [g]is the set of genres and zis used to index into each genre.
Every edge is associated with a binary feature vector q(u,v)of dimension g:
Maximize/summationdisplay
v∈V/summationdisplay
z∈[g]wzvγzv
Subject to/summationdisplay
u∈Ngbr (v)xuv≤rv,∀v∈V
/summationdisplay
v∈Ngbr (u)xuv≤1,∀u∈U
γzv≤/summationdisplay
(u,v)∈E:q(u,v)[z]=1x(u,v),∀z∈[g],v∈V
γzv≤1,∀z∈[g],v∈V
xuv∈{0,1}
24Published in Transactions on Machine Learning Research (10/2022)
E Agreement Plots
In Figure 10a, greedyis closer to optimal for sparse graphs but gets increasingly different as the graphs
get denser. This suggests that sparse graphs require a more greedy strategy as there not many options
available in the future, while dense graphs with more incoming nodes than fixed nodes require a smarter
strategy as the future may hold many better options.
For gMission 100x100, invariant models are closer to greedyin the beginning only. Non-invariant models
learn very different strategies from greedy. Interestingly, while ff-supervised does not have the best
performance on gMission 100x100, it is the closest to optimal in Figure 10c. This is because the supervised
model is learning to copy the optimal actions but lacks the sequential reasoning that RL models possess.
It is often the case that there are many different solutions besides optimal that give high reward, so RL
models may not necessarily learn the same strategy as ff-supervised .
F Adwords
In order to demonstrate that our RL framework can also tackle other variations of OBM, we test the
performance of our models on the Adwords Mehta et al. (2005) problem. Adwords is a variation of OBM
with applications in real-time ad allocation. Each vertex u∈Uhas a budget Bu, and each edge (u,v)
has a bid (weight) biduv. Upon matching the edge (v,u), the nodeudepletesbiduvamount of its budget.
When a vertex is out of budget ( Bu= 0), then the vertex becomes unavailable. The goal is to maximize
the total amount of budget spent.
To give the hist-models ( inv-ff-hist ,ff-hist) more context about the problem state, we input an ad-
ditional vector of the remaining and original budgets of nodes (r0,r2,...r|U|,B0,B2,...,B|U|), whereriis
the remaining budget of node iat time t2.
We train and test our models on two hard distributions, Thick-z and Triangular, used in the work by
Zuzic et al. (2020). The instances in the datasets are generated by permuting the nodes in U, all edges
have the same bidsampled from U[0.1,0.4). The budget of each fixed node is bid|V|
|U|. The order in which
the nodes in Varrive is the same across all instances.
We find that ffand inv-ffconverge to one of the existing policies (greedy and MSVV), or outper-
form them by a small margin. This result is coherent with the findings in Zuzic et al. (2020). However,
we can see that inv-ff-hist and ff-hist outperform the baselines on thick-z graphs. In particular,
inv-ff-hist is able to achieve 0.91 average optimality ratio.
It is noteworthy that, in such hard graph datasets, only the permutation of the U nodes and bid∼
U[0.1,0.4)change for each instance. If one discovers the real identities (order) of the U nodes, then achiev-
ing the optimal matching is trivial Zuzic et al. (2020). Therefore, the historical data coupled with the
invariance property gives inv-ff-hist better inductive bias to discover the identities and achieve near-
optimal matching performance.
In traditional RL applications, a sub-optimal action can result in the agent entirely changing its trajectory
ahead. In the E-OBM setting, however, a sub-optimal action is not guaranteed to affect all the future ac-
tions (edge selections), especially on sparse graphs. In Adwords, the impact of a single sub-optimal action
is even less significant on the total budget spent (and the cumulative reward). That is, a sub-optimal ac-
tion is very unlikely to affect all the future actions (edge selections) in the future. Hence, there will be less
need to be strategic as the penalty for a sub-optimal action is negligible. Thus, discovering a new policy
on real-world graphs will be quite hard. In general, the existing greedy algorithms for Adwords perform
quite well and are hard to beat (especially when the bids are significantly smaller than budgets).
2Models without history are only given the current budget.
25Published in Transactions on Machine Learning Research (10/2022)
60801000.05
60801000.1
60801000.15
0 5 10 15 20 25 30
Timestep60801000.2Agreement with Greedy for ER 10x30Agreement per Timestep %
4060801000.05
4060801000.1
4060801000.15
0 10 20 30 40 50 60
Timestep4060801000.2Agreement with Greedy for ER 10x60Agreement per Timestep %
4060800.05
4060800.1
4060800.15
0 10 20 30 40 50 60
Timestep4060800.2Agreement with Optimal for ER 10x60Agreement per Timestep %
(a)
0 5 10 15 20 25 30
Timestep30405060708090
ff-supervised
ff
ff-hist
inv-ff
inv-ff-hist
gnn-histAgreement with Greedy for gMission 10x30
Agreement per Timestep %
0 10 20 30 40 50 60
Timestep2030405060708090100
ff-supervised
ff
ff-hist
inv-ff
inv-ff-hist
gnn-histAgreement with Greedy for gMission 10x60
Agreement per Timestep %
0 20 40 60 80 100
Timestep20406080100ff-supervised
ff
ff-hist
inv-ff
inv-ff-hist
gnn-histAgreement with Greedy for gMission 100x100
Agreement per Timestep %
(b)
0 10 20 30 40 50 60
Timestep102030405060708090
greedy
ff-supervised
ff
ff-hist
inv-ff
inv-ff-hist
gnn-histAgreement with Optimal for gMission 10x60
Agreement per Timestep %
0 20 40 60 80 100
Timestep10152025303540greedy
ff-supervised
ff
ff-hist
inv-ff
inv-ff-hist
gnn-histAgreement with Optimal for gMission 100x100
Agreement per Timestep %
(c)
Figure 10: Percent agreement with the optimal solution per timestep. A point (timestep t, agreement a)
on this plot can be read as: at timestep t, this method makes the same matching decision as the optimal
solution on a%of the test instances.
26Published in Transactions on Machine Learning Research (10/2022)
Table 6: Mean/std optimality ratio on 2 hard Adwords datasets Zuzic et al. (2020). All graphs in the
training and test set have the same structure except the fixed nodes are permuted and the bids are sam-
pled uniformly between 0.1 and 0.4 for all edges.
Thick-z Triangular
10x60 10x100 10x60 10x100
greedy 0.59±.03 0.58±.02 0.66±.03 0.66±.02
MSVV 0.7±.01 0.7±0 0.66±0 0.66±0
ff 0.7±.08 0.7±.09 0.65±.06 0.65±.06
ff-hist 0.77±.07 0.71±.09 0.65±.06 0.65±.06
inv-ff 0.7±.01 0.7±0 0.66±0 0.66±0
inv-ff-hist 0.91±0 0.91±0 0.66±0 0.66±0
G Permutation Invariance
In Figure 11, we show the results on gMission-perm where we train all models on the gMission dataset and
test on the same dataset but with the fixed nodes permuted for each graph instance independently. We
can see that the performance of the non-invariant models degrades significantly compared to the gMission
results. The invariant models are unaffected by the permutation as they receive node-wise input. ff-hist
is less affected by this permutation in the 10x60 plot, this suggests that historical features help the model
learn “identities” for each fixed node even if the nodes are permuted at test time. However, more incoming
nodes need to be observed for the features to be statistically significant.
0.40.50.60.70.80.91.0 Optimality RatioE-OBM gMission-perm 10x30
0.50.60.70.80.91.0 Optimality RatioE-OBM gMission-perm 10x60
Figure 11: Distribution of Optimality Ratios for gMission-perm: gMission with Fixed Nodes Permuted at
Test Time
27