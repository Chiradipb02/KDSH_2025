Published in Transactions on Machine Learning Research (07/2024)
Convergence Analysis and Trajectory Comparison of Gradi-
ent Descent for Overparameterized Deep Linear Networks
Hongru Zhao zhao1118@umn.edu
School of Statistics
University of Minnesota
Jinchao Xu jinchao.xu@kaust.edu.sa
Computer, Electrical and Mathematical Science and Engineering Division
King Abdullah University of Science and Technology
Reviewed on OpenReview: https: // openreview. net/ forum? id= jG7ndW7UHp
Abstract
Thispaperpresentsaconvergenceanalysisandtrajectorycomparisonofthegradientdescent
(GD) method for overparameterized deep linear neural networks with different random ini-
tializations, demonstrating that the GD trajectory for these networks closely matches that
of the corresponding convex optimization problem. This study touches upon one major
open theoretical problem in machine learning–why deep neural networks trained with GD
methods are efficient in many practical applications? While the solution of this problem is
still beyond the reach of general nonlinear deep neural networks, extensive efforts have been
invested in studying relevant questions for deep linear neural networks, and many interest-
ing results have been reported to date. For example, recent results on loss landscape show
that even though the loss function of deep linear neural networks is non-convex, every local
minimizer is also a global minimizer. We focus on the trajectory of GD when applied to deep
linear networks and demonstrate that, with appropriate initialization and sufficient width
of the hidden layers, the GD trajectory closely matches that of the corresponding convex
optimization problem. This result holds regardless of the depth of the network, providing
insight into the efficiency of GD in the training of deep neural networks. Furthermore, we
show that the GD trajectory for an overparameterized deep linear network automatically
avoids bad saddle points.
1 Introduction
Deep linear neural networks, as a class of toy models, are frequently used to understand loss surfaces and
gradient-based optimization methods related to non-convex problems. Dauphin et al. (2014) and Choro-
manska et al. (2015a) explored the loss function of deep non-linear networks based on random matrix theory
(such as a spherical spin-glass model). This theory essentially converts the loss surface of deep nonlinear
neural networks into that of deep linear neural networks under certain assumptions, some of which are un-
realistic. Choromanska et al. (2015b) suggested an open problem to establish a connection between the loss
function of neural networks and the Hamiltonian of spherical spin-glass models under milder assumptions.
Later, Kawaguchi (2016) successfully discarded most of these assumptions by analyzing the loss surface of
the deep linear neural networks.
The landscape for deep linear neural network (Kawaguchi, 2016; Kawaguchi & Lu, 2017; Laurent & Brecht,
2018) focuses on several properties of the critical points: (i) every local minimum is a global minimum; (ii)
every critical point that is not a local minimum is a saddle point; and (iii) there exists a saddle such that all
eigenvalues of its Hessian are zeros if the network is deeper than three layers. Thus, for deep linear neural
networks, convergence to a global minimum is impeded by the existence of poor saddles.
1Published in Transactions on Machine Learning Research (07/2024)
Lee et al. (2016) showed that the gradient method almost surely never converges to a strict saddle point,
although the time cost can depend exponentially on the dimension (Du et al., 2017). Gradient descent (GD)
with perturbations (Ge et al., 2015; Jin et al., 2017) can find a local minimizer in polynomial time. Thus,
the trajectory approach combined with random initialization or random algorithm circumvents the obstacle
of existence of poor saddles. According to studies on continuous time dynamics of a gradient flow (Du et al.,
2018; Arora et al., 2018b), the balance property of deep linear network is preserved if the initialization is
balanced. Arora et al. (2018a;b), Du & Hu (2019), and Hu et al. (2020) successfully proved that GD with its
corresponding initialization schemes converges to a global minimizer of deep linear neural networks with high
probability. Furthermore, the rate of convergence is linear, and behaves like GD for a convex optimization
problem.
Hu et al. (2020) established that the convergence for Gaussian initialization can be very slow for deep linear
neural networks with large depths. For efficient convergence of Gaussian networks, the width needs to scale
linearly with the depth. They also showed that orthogonal initialization in deep linear neural networks
accelerate the convergence. Thus, the convergence behavior of the GD method, for training deep linear
neural networks, crucially networks depends on the initialization.
Recent studies have demonstrated the connection between deep learning and kernel methods (Daniely, 2017;
Arora et al., 2019a;b; Chizat et al., 2019; Lee et al., 2019; Du et al., 2019; Cao & Gu, 2019; Woodworth
et al., 2020), especially the neural tangent kernel (NTK), introduced by Jacot et al. (2018). For most
common neural networks, the NTK becomes constant (Jacot et al., 2018; Liu et al., 2020) and remains so
throughout the training in the limit of a large layer width. Throughout the training, the neural networks are
well described by their first-order Taylor expansion around their parameters at the initialization (Lee et al.,
2019).
In this paper, we first study the convergence region, the set of initializations which lead to the linear
convergenceofGDfordeeplinearneuralnetworks(seeLemma3). Next, wedemonstratethatiftheminimum
widthamongallhiddenlayersissufficientlylarge, thentherandominitializationwillfallintotheconvergence
region with high probability (see Theorem 1, Theorem 4, Theorem 5, and Theorem 6). Furthermore, the
worst-case convergence rate of GD for deep linear neural networks is almost the same as the original convex
optimization problem with a corresponding learning rate. The most significant finding of our work is that
the trajectories of gradient descent for deep linear neural networks are arbitrarily close to those of the
corresponding convex optimization problem, establishing a strong connection between their optimization
dynamics and providing insights into gradient descent behavior in more complex neural networks. The
precise statement is related to Remark 3, Theorem 2, Lemma 6, and Corollary 3.
1.1 Our contributions
This work is inspired by recent work by Du & Hu (2019) and Hu et al. (2020), who carefully constructed the
upper and lower bounds of eigenvalues of the Gram matrix along GD and established linear convergence. In
this paper, we generalize their results to strongly convex loss functions with layer-varying widths, and obtain
sharp results. We show that our rate of convergence for GD in deep linear neural networks matches the
worst-case convergence rate for the original convex optimization problem. Next, we show that the optimal
rate depends neither on the types of random initialization nor on the depth of the network, provided that
the width of each hidden layer is appropriately large.
The most significant contribution of our work is the trajectory comparison result, which demonstrates that
the trajectories of gradient descent for deep linear neural networks can be made arbitrarily close to those
of the original convex optimization problem (1). To the best of our knowledge, this trajectory comparison
result is the first of its kind and has the potential to greatly influence our understanding of optimization in
deep learning.
2Published in Transactions on Machine Learning Research (07/2024)
2 Preliminaries
2.1 Problem Setup
Letx∈Rnxandy∈Rnybe an input vector and a target vector, respectively. Define {(xi,yi)}m
i=1as a
training dataset of size m, and letX= [x1,x2,···,xm]̸= 0andY= [y1,y2,···,ym]. Denote the weight
parameters by W∈Rny×nx.
Consider the well studied convex optimization problem:
minimize
WL(W) :=1
mm/summationdisplay
i=1l(Wxi,yi), (1)
wherel(·,y)is strongly convex and has a smooth gradient with respect to its first argument uniformly in y.
The GD for convex optimization problem (1) with learning rate η∗is given by:
W(t+ 1) =W(t)−η∗∇L(W(t)),t= 0,1,2,···. (2)
For any matrix A, letσmax(A)andσmin(A)be the largest and smallest singular values of Arespectively.
In this paper, we consider two types of matrix norms and one type of semi-norm for A,∥A∥:=
σmax(A),∥A∥2
F:= tr(AAT),and∥A∥X:=∥APX∥F, wherePX=X(XTX)†XTis the orthogonal pro-
jection matrix onto the column space of X, and (XTX)†is the Moore–Penrose inverse.
For two real matrices A,Bwith the same size, we consider their Frobenius inner product as well as their
semi-inner product, ⟨A,B⟩=⟨A,B⟩F:= tr(ATB)and⟨A,B⟩X:=⟨APX,BPX⟩.
The following lemma illustrates why semi-norm ∥·∥Xis crucial in our analysis.
Lemma 1 Assume that l(·,y)isα(l)−strongly convex. The following statements hold.
1. IfXis not full row rank, then L(W)is neither strictly convex nor strongly convex with respect to
∥·∥F.
2.L(W)isα(l)λmin(XXT)
m−strongly convex with respect to ∥·∥X, whereλmin(XXT)is the smallest
non-zero eigenvalue of XXT.
The proof of the lemma above can be found in Appendix A. From now on, if we do not specify the inner
product, we will always consider the semi-inner product.
Assume that Lisα−strongly convex ( α > 0), and∇Lisβ−Lipschitz (with respect to semi-norm ∥·∥X),
that means, for any W,V∈Rny×nx,
L(W)≥L(V) +⟨∇L(V),W−V⟩X+α
2∥W−V∥2
X,
∥∇L(W)−∇L(V)∥X=∥∇L(W)−∇L(V)∥F≤β∥W−V∥X.
Without loss of generality, we assume that αandβare the best constants. Then, Lemma 1 implies α≥
α(l)λmin(XXT)
m. Similarly, we can also show that β≤β(l)λmax(XXT)
m, where∇l(·,y)isβ(l)−Lipschitz and
λmax(XXT)is the largest eigenvalue of XXT.
Define the effective condition number of the convex function Lbyκ=κ(L) =β
α<∞.κappears naturally
in the rate of convergence of GD. Let W∗be a global minimizer of L(W), that is,L(W∗) = minWL(W).
Notice that W∗might not be unique, but W∗PXis unique, that is, if W′is another global minimizer of
L(W), thenW∗PX=W′PX(see Lemma 7 for the precise statement).
3Published in Transactions on Machine Learning Research (07/2024)
By applying the well-known results on the rate of convergence, specifically Theorem 3.9 from Garrigos &
Gower (2023) and Theorem 2.1.15 from Nesterov (2003), for GD (2), we have
η∗=1
β=⇒ E (t)≤/parenleftbigg
1−1
κ/parenrightbiggt
E(0),t= 1,2,···,as well as, (3)
η∗=2
α+β=⇒ E (t)≤β
2/parenleftbigg
1−4κ
(1 +κ)2/parenrightbiggt
∥W(0)−W∗∥2
X,t= 1,2,···, (4)
whereE(t) =L(W(t))−L(W∗).
2.2 Deep Linear Network Setup
LetN−1be the number of hidden layers. Assume rank (X) =r. Denote the weight parameters by
Wk∈Rnk×nk−1,k= 1,2···,N, withnN=ny,n0=nx, where the nkis the width of the k-th layer. Set
nmin= min{n1,n2,···,nN−1}andnmax= max{n1,n2,···,nN−1}. For notational convenience, we denote
nj:i=/producttext
i≤k≤jnkand denote Wj:i=WjWj−1···Wifor each 1≤i≤j≤N. Defineni−1:i= 1and
Wi−1:i=I(of appropriate dimension) for completeness.
Consider the implicit regularization on W=WN:1for the convex optimization problem (1). Thus, we obtain
the following non-convex optimization problem of deep linear neural networks:
minimize
W1,···,WNL(WN:1) =1
mm/summationdisplay
i=1l(WN:1xi,yi). (5)
Example 1 Specifically, if we set the loss to be l(Wxi,yi) =∥Wxi−yi∥2
2, thenL(W) =1
m∥WX−Y∥2
Fis
2λmin(XXT)
m−strongly convex, and ∇Lis2λmax(XXT)
m−Lipschitz.
Example 2 The deep linear neural networks with regularization λ∥WN···W1PX∥2
Fcan be converted into
a new optimization problem
minimize
W1,···,WNL(WN:1) +λ∥WN:1∥2
X.
LetLλ(W) =L(W) +λ∥W∥2
X. ThenLλ(·)isα+ 2λ-strongly convex, and ∇Lλ(·)isβ+ 2λ-Lipschitz.
More general, if we consider regularization with form R(W) =λ·g(WPX),g(·)isα′-strongly convex, and
β′-Lipschitz, then for the optimization problem
minimize
W1,···,WNL(WN:1) +R(WN···W1) =:LR(WN···W1),
we know that LR(·)isα+λα′-strongly convex, and ∇LR(·)isβ+λβ′-Lipschitz.
2.3 Initialization Schemes
In literature, researchers consider the following form of the deep linear network instead of (5):
minimize
W1,···,WNL(aNWN:1) =1
mm/summationdisplay
i=1l(aNWN:1xi,yi), (6)
whereaN= 1/√n1n2···nNis a normalization constant.
Applying GD on (6), where we update Wjsimultaneously for j,
Wj(t+ 1) =Wj(t)−η·aN(WN:j+1(t))T∇L(aNWN:1(t)) (Wj−1:1(t))T,j= 1,···,N. (7)
Recent work consider GD (7), initializing Wj(0)with Gaussian initialization (Du & Hu, 2019) or scaled
orthogonal initialization (Hu et al., 2020).
4Published in Transactions on Machine Learning Research (07/2024)
In this paper, we consider the following three kinds of random initialization which generalizes their idea.
•Gaussian Initialization: LetW1(0),···,WN(0)be the weight matrices at initialization. We assume that
all entries of Wj,1≤j≤Nare independent Gaussian random variables with zero mean and unit variance.
ThenaNis a normalization constant in the sense that for any x∈Rn0, we have
E/bracketleftig
∥aNWN:1(0)x∥2
2/bracketrightig
=∥x∥2
2. (8)
In fact, all initializations we discussed in this paper satisfy (8); see Lemma 9, Lemma 14 and the proof of
Theorem 6.
Remark 1 LetVi(t) =1√niWi(t), for 1≤i≤N. Then, GD (7) with unit variance Gaussian initialization
is equivalent to
Vj(t+ 1) =Vj(t)−η
nj(VN:j+1(t))T∇L(VN:1(t)) (Vj−1:1(t))T, (9)
with mean zero and variance1
niGaussian initialization for Vi,i= 1,···,N. The GD (9) for the loss (5)
is equivalent to GD (7) for the loss (6). From now on, we will only consider GD (7) for deep linear neural
networks (6).
•Orthogonal Initialization: We consider a so-called one peak random orthogonal projections and em-
beddings initialization, which generalizes the idea of orthogonal initialization (Hu et al., 2020).
Definition 1 A initialization WN:1(0) =WN(0)WN−1(0)···W1(0)is said to be one peak random orthogonal
projections and embeddings initialization if there exists 1≤p < N, such that n0≤n1≤n2≤···≤np,
np≥np+1≥np+2≥···nN−1≥nN, andW1(0),W2(0),···,Wp(0),Wp+1(0),Wp+2(0),···,WnN(0)are
independent and uniformly distributed over rectangular matrices which satisfy
/braceleftigg
WT
i(0)Wi(0) =niIni−1,1≤i≤p,
Wj(0)WT
j(0) =nj−1Inj,p+ 1≤j≤N.
Remark 2 In this definition,/radicalig
1
niWi(0),1≤i≤pare random embeddings and/radicalig
1
nj−1Wj(0),p+1≤j≤N
are random orthogonal projections. Notice that, Ais a random orthogonal projection if and only if ATis a
random embedding.
Arora et al. (2018a) studied the rate of convergence to global optimum for GD to train deep linear neural
network for balanced initialization. Here, we will consider a special case of balanced initialization which is
described as follows.
•Special Balanced Initialization: Assumen1=···=nN−1=n. Consider initialization WN(0) =√nUN[Iny,0ny×(n−ny)]VT
N,W1(0) =√nU1[Inx,0nx×(n−nx)]TVT
1andWi(0) =√nUiInVT
i,2≤i≤N−1,
whereUN−1,UN,V1,Vi=Ui−1,2≤i≤N−1are orthogonal matrices (random or deterministic), and VN
has uniform distribution over orthogonal matrices. Notice that only VNis required to be random.
A simple estimation of the loss at the initialization is given by the following lemma.
Lemma 2 If the initialization satisfies (8) for all x, then with probability at least 1−δ
2, we have
L(aNWN:1(0))−L(W∗)≤βBδ,whereBδ=/parenleftbigg2·rank (X)
δ+∥W∗∥2
X/parenrightbigg
.
Note that using sharp concentration inequality, the bound Bδcan be improved.
5Published in Transactions on Machine Learning Research (07/2024)
3 Main Results
For the rest of this paper, assume the thinnest layer is either the input layer or the output layer, that is,
nmin≥max{n0,nN}and the ratio between the width of any hidden layer is bounded from above, precisely
we havenmax
nmin≤C0<∞. Define some quantities as follows:
q=/braceleftigg
1−αη∗(2−η∗α), 0<η∗≤2
α+β,
1−βη∗(2−η∗β),2
α+β<η∗<2
β,(10)
C1=nNκ2BδC0+ lnN,
C2=nNκ2BδC0+C0ln(N),
C3=nNκ2Bδ,(11)
whereNdenotes the number of distinct elements in the set {n1,···,nN−1}.
Here, the quantity qis related to the rate of convergence of gradient descent for strongly convex function
L(W). Setting the learning rate η∗=2
α+β, we can rewrite the well-known rate in (4) in terms of q, due to
q= 1−αη∗(2−η∗α) = 1−4κ
(1 +κ)2.
The parameter δ∈(0,1
2)is a confidence threshold, such that the conclusions in Theorem 1 (or Theorem 2)
hold with a probability of at least 1−δ, given an extra inequality constraint between the minimum width
of hidden layers, nmin, and the depth, N, as quantified by C1,C2, andC3.
For notational convenience, we denote
E(t) =L(W(t))−L(W∗),andEDLN(t) =L(aNWN:1(t))−L(W∗).
Our assumptions and notation are now in place. We next state our main theorems in this section.
3.1 Linear Convergence of Deep Linear Neural Networks
In Appendix B, we present an analysis of the linear convergence of GD for deep linear neural networks, in
Theorem 4 for Gaussian initialization, Theorem 5 for orthogonal initialization, and Theorem 6 for special
balanced initialization, respectively. These results extend the work of Du & Hu (2019), and Hu et al. (2020),
which proved similar convergence rates with ℓ2loss. In particular, with the special choice of learning rate
η=nN
βN, our Theorem 4 and Theorem 5 lead us to obtain the following optimum convergence rate.
Theorem 1 Consider GD for the deep linear neural networks (7) with learning rate η=nN
βN. Given any
δ,ε∈(0,1
2)andC0∈[1,∞), there exists a constant C:=C(ε),such that if one of the following two
overparameterization condition holds:
1.nmin≥C·C1·Nwith the Gaussian initialization,
2.nmin≥C·C2with the one peak random orthogonal projections and embeddings initialization,
with probability at least 1−δwe have
EDLN(t)≤/parenleftbigg
1−1−ε
κ/parenrightbiggt
EDLN(0),t= 1,2,···.
Remark 3 Consider GD (2) with learning rate η∗=1
β, and initialization W(0) =aNWN:1(0). The well-
known result on the rate of convergence (3) for GD (2) of the convex optimization problem (1) matches the
rates in Theorem 1.
Remark 4 If we setC0= 1and consider ℓ2loss, then we can recover the main results in Du & Hu (2019),
and Hu et al. (2020), that the number of iterations needed to reach precision εisO/parenleftbig
κlog1
ϵ/parenrightbig
forℓ2loss. We
generalized their results to any strongly convex loss with varying width.
6Published in Transactions on Machine Learning Research (07/2024)
3.2 Trajectory Comparison
Theorem 1 and Remark 3 establish that the rate of convergence to a global optimum for GD to train
a deep linear neural networks is almost the same as the trajectories for GD to train the corresponding
convex optimization problem with high probability, if the width is large enough. Moreover, GD for the
fully-connected deep linear neural networks (7) and that for GD (2) have almost the same trajectories.
Letη1=2nN
βNbe an upper bound of learning rate η. We can show that the trajectories of GD (7) for deep
linear neural networks (6) with learning rate η<η 1are close to those of GD (2) with learning rate η∗=N
nNη
for the corresponding convex optimization problem (1) with high probability, if the width of each hidden
layer is sufficiently large. The precise statement is as follows.
Theorem 2 (Trajectory Comparison) Consider GD for the deep linear neural networks (7) with learning
rateη < η 1foraNWN:1(t),t= 0,1,···, and GD (2) with learning rate η∗=N
nNηforW(t),t= 0,1,···.
Givenτ,δ∈(0,1)andC0∈[1,∞), there exists a constant C:=C(τ,η/η 1)such that if one of the following
three overparameterization conditions holds:
1.nmin≥C·C1·Nwith the Gaussian initialization,
2.nmin≥C·C2with the one peak random orthogonal projections and embeddings initialization,
3.nmin≥C·C3with the special balanced initialization,
then with probability at least 1−δ, we have
∥aNWN:1(t)−W(t)∥2
X≤D(τ,q,t )∥aNWN:1(0)−W∗∥2
X, (12a)
|EDLN(t)−E(t)|≤β/parenleftbigg
qt/2/radicalbig
D(τ,q,t ) +1
2D(τ,q,t )/parenrightbigg
∥aNWN:1(0)−W∗∥2
X, (12b)
EDLN(t)≤3β(q+τ)t∥aNWN:1(0)−W∗∥2
X, (12c)
whereD(τ,q,t ) = min/braceleftig
τ
1−q,2(q+τ)t/bracerightig
, with 0<q< 1defined in (10).
Remark 5 To our knowledge, we are the first who showed that the trajectory of the overparameterized
deep linear neural networks is close to the original convex optimization problem with appropriately rescaled
learning rate.
Corollary 3 Under the setting of Theorem 2, if we set η=2nN
(α+β)N, the following inequality hold with high
probability,
EDLN(t)≤3β/parenleftbigg
1−4κ
(1 +κ)2+τ/parenrightbiggt
∥aNWN:1(0)−W∗∥2
X. (13)
We notice that the rate of convergence in (13), matching (4), is better than that in Theorem 1. It is because
ifκ>1, we can choose τsufficiently small, so that the following inequality holds:
1−4κ
(1 +κ)2+τ <1−1
κ.
Roughly speaking, Theorem 1, Theorem 2, Theorem 4, together with Theorem 5 illustrate that the GD for
a convex optimization problem recovers the convex problem itself in terms of optimization, at the cost of
linear convergence only with high probability for random initialization.
Remark 6 In constants C1,C2,C3defined in (11), the termrank (X)
δis not optimal, since our concentration
inequality only depends on the second moment. By using stronger concentration inequalities for our Lemma
2, similar to the proof of proposition 6.5 (Du & Hu, 2019) and Lemma 4.2 (Hu et al., 2020), therank (X)
δ
can be improved to 1 + log(rank (X)
δ).
TheC1is proportional to κ2, which is slightly better than the constant in Du & Hu (2019) that is proportional
toκ3. The C2is also slightly better than the constant in Hu et al. (2020), since we do not have the extra
term∥X∥2
F
∥X∥2. The improvement of the constant is mainly due to introducing the semi-norm ∥·∥X.
7Published in Transactions on Machine Learning Research (07/2024)
3.3 Proofs Overview
Now we will give an overview of the proofs for all theorems in the main results. Since Theorem 1 in the main
results are special cases of general theorems with non-optimal learning rate (see Theorem 4 and Theorem 5),
we only need to focus on the proofs of the general theorems (see Theorem 2, Theorem 4, Theorem 5, and
Theorem 6).
We start with the convergence region of deep linear neural networks. Basically, the convergence region is the
set of initialization which lead to the convergence of GD for the deep linear neural networks. The precise
definition can be found in the Definition 2. Lemma 3 and Lemma 6 prove that this convergence region
satisfies the following properties: if the initialization falls into the convergence region, then
(i) GD is guaranteed to converge to a global minimizer of the deep linear neural networks,
(ii) the worst-case rate of convergence of GD for the deep linear neural networks, which is a non-convex
problem, is almost the same as the corresponding convex optimization problem with a corresponding
learning rate, and,
(iii) the trajectories of GD for the deep linear neural networks are arbitrarily close to those for the
corresponding convex optimization problem.
More precisely, Lemma 3 establishes the convergence region for deterministic initialization, and it demon-
strates the first two properties, (i) and (ii). Additionally, in Appendix E and Appendix F we also proved that
the spectral properties of products of random matrices partially reveal the mystery of overparameterization,
that is, overparameterization by adding width of each hidden layer guarantees that the random initialization
will fall into the convergence region with high probability. These results provide a foundation to establish
the main linear convergence theorem for random initialization (see Theorem 4, 5, and 6).
On the other hand, Lemma 4 shows that if the initialization falls into the convergence region, the update
rule for the product of weight matrices in GD for deep linear neural networks is more or less that given in
(2). This result can be used to establish both Theorem 2, and Lemma 6, which is precisely the property (iii)
of the convergence region for deterministic initialization and non-deterministic initialization, respectively.
It is worth mentioning that property (iii) of the convergence region cannot be directly obtained from the
results of Du & Hu (2019) and Hu et al. (2020), highlighting the contribution of our work in establishing
this property.
4 Convergence Analysis and Trajectory Comparison
4.1 Initialization and Convergence Region
Arora et al. (2018a) showed that if the initialization is approximately balanced, and the product matrix
WN:1(0)is very close to a global minimizer, then GD linearly converges to the global minimum for the deep
linear network, without any requirement on the width. However, the convergence region in (Arora et al.,
2018a) is very small, since WN:1(0)has to be very close to W∗. Later, several papers by Du & Hu (2019), and
Hu et al. (2020) successfully proved that GD with Gaussian, or orthogonal initialization linearly converges
to a global minimizer of the overparameterized deep linear neural networks with high probability. They
introduced a technique to analyze trajectories of GD with large widths for any deterministic initialization.
We introduce the following lemma which generalizes the idea from recent work (Du & Hu, 2019; Hu et al.,
2020), describes the linear convergence result for deep linear networks with deterministic initialization.
DefineA|R(X)=AXT(XXT)−X=APX, and view A|R(X)as a linear operator on R(X), the column
space ofX. For notational convenience, we denote Wj:i(t) =Wj(t)···Wi(t),Lt=L(aNWN:1(t)), and
∇Lt=∇L(aNWN:1(t)), etc.
8Published in Transactions on Machine Learning Research (07/2024)
Lemma 3 Assume the initialization satisfies the following conditions simultaneously:


σmax(WN:i+1(0))≤ec1/2(nN−1:i)1/2,1≤i≤N−1,
σmin(WN:i+1(0))≥e−c2/2(nN−1:i)1/2,1≤i≤N−1,
σmax(Wi−1:1(0)|R(X))≤ec1/2(ni−1:1)1/2,2≤i≤N,
σmin(Wi−1:1(0)|R(X))≥e−c2/2(ni−1:1)1/2,2≤i≤N,
∥Wj:i(0)∥≤M/2·Nθ(/producttext
i≤k≤j−1nk·max{ni−1,nj})1/2,1<i≤j <N,
L0−L(W∗)≤βB0=:B,(14)
wherec1,c2,Mare positive constant and θ≥0. Note that B0is a proper upper bound for ∥aNWN:1(0)∥2
X+
∥W∗∥2
X.
Set the learning rate η=(1−ε)2nN
e6c1+3c2βN, where 0<ε< 1. Defineγ=2e6c1εαN
nN.Assume that
nmin≥C(c1,c2)M2κ2B0
ε2N2θnN. (15)
Then GD (7) satisfies
Lt−L(W∗)≤(1−ηγ)t(L0−L(W∗)),t= 1,2,···.
Definition 2 For givenc1,c2,M,B 0>0, andθ≥0, we define the convergence region R(c1,c2,θ,M,B 0)
by the set of initialization that satisfies the inequality system (14).
Remark 7 Condition (14) describes the convergence region for initialization and the condition (15) de-
scribes the overparameterization for deep linear neural networks. At this time, it is not clear how large this
convergence region is. Later, we will show that the properly scaled random initialization with some extra mild
overparameterization conditions will fall into this convergence region with high probability.
Our convergence region (see (14) in Lemma 3 and Definition 2) originated from Du & Hu (2019), and Hu
et al. (2020). This convergence region can be view as a neighbourhood of special balanced initialization, if
n1=n2=···=nN−1. Both the Gaussian and orthogonal initializations are approximately balanced.
For theℓ2loss, we assume without loss of generality that Xis of full rank and L(W∗) = 0, due to the
decomposition method in Claim B.1 from Du & Hu (2019). However, when considering the general strongly
convex loss, we have to directly confront the low rank Xin our analysis. Thus, the ∥·∥Xappears naturally
and helps us to achieve the sharp rate of convergence in our main theorems.
4.2 Dynamic of GD for Overparameterized Deep Linear Neural Networks
We will explain why trajectories of GD for overparameterized deep linear neural networks with approximate
balanced initialization are close to trajectories for convex problems. Although the recent result (Ziyin et al.,
2022) can describe the exact global minimizer for the deep linear network (with a regularization term such
asℓ2), the evolution of each Wjis still hard to track. Instead, we consider the discrete dynamics for product
matricesWN:1(t):
aNWN:1(t+ 1) =aNWN:1(t)−η·P(t)[∇L(aNWN:1(t)PX)] +aNE(t),
where linear operator
P(t)[A] =a2
NN/summationdisplay
i=1WN:i+1(t)WT
N:i+1(t)(APX)(Wi−1:1(t)|R(X))TWi−1:1(t)|R(X),
for anyA∈RnN×n0.
Du & Hu (2019) showed for their linear operator Ptthatλmax(Pt)≤O(N
nN)·λmax(XTX)andλmin(Pt)≥
Ω(N
nN)·λmin(XTX). We prove that for our operator P(t)[·]≈N
nNI(also see (43)), where Iis the identity
operator.E(t)is negligible, which leads to the following lemma on discrete dynamics.
9Published in Transactions on Machine Learning Research (07/2024)
4.3 Trajectory Comparison
We can summarize the above result in the following dynamic comparison lemma.
Lemma 4 Assume that all the assumptions in Lemma 3 hold. For any τ >0, we can choose new constants
c1,c2as well asC:=C(c1,c2)such that the overparameterization assumption (15) in Lemma 3 hold and
∥R(t)∥X≤τ∥aNWN:1(t)−W∗∥X, (16)
where
aNWN:1(t+ 1) =aNWN:1(t)−N
nNη∇L(aNWN:1(t)) +R(t).
Without the R(t)term, the discrete dynamics is exactly GD (2) for a convex function (1). To control the
distance between the two trajectories, we introduce the following lemma, which coincided with Theorem 6
in Hacohen & Weinshall (2022).
Next, we introduce the following lemma for convergence analysis for a dynamical system V(t+ 1) =V(t)−
η∗∇L(V(t)) +R(t), withV(t) =aNWN:1(t),t= 0,1,···.
Lemma 5 Consider a discrete dynamical system V(t)such that,
V(t+ 1) =V(t)−η∗∇L(V(t)) +R(t),t≥0,
where∥R(t)∥X≤τ∥V(t)−W∗∥X, andτ∈[0,1). Ifη∗≤2/β, we have
∥V(t)−W∗∥2
X≤(q+ 7τ)t∥V(0)−W∗∥2
X,
where 0<q< 1is defined in (10).
Proof of Lemma 5 Set∆(t) =V(t)−W∗andτ′=τ∥∆(t)∥X. Notice that
∆(t+ 1) = ∆(t)−η∗(∇L(V(t))−∇L(W∗)) +R(t),
and
∥∆(t+ 1)∥2
X≤η2
∗∥∇L(V(t))−∇L(W∗)∥2
X−2η∗⟨∆(t),∇L(V(t))−∇L(W∗)⟩X
+∥∆(t)∥2
X+ (2∥∆(t)∥X+ 2η∗∥∇L(V(t))−∇L(W∗)∥X+τ′)τ′.
By inequality (28) in Appendix C, we have
∥∆(t+ 1)∥2
X≤∥∆(t)∥2
X−2η∗⟨∆(t),∇L(V(t))−∇L(W∗)⟩X
+η2
∗∥∇L(V(t))−∇L(W∗)∥2
X+ 7τ∥∆(t)∥2
X
=(1 + 7τ)∥∆(t)∥2
X−2η∗⟨∆(t),∇L(V(t))−∇L(W∗)⟩X
+η2
∗∥∇L(V(t))−∇L(W∗)∥2
X
≤(1 + 7τ)∥∆(t)∥2
X−2η∗αβ
α+β∥∆(t)∥2
X
+/parenleftbigg
η2
∗−2η∗
α+β/parenrightbigg
∥∇L(V(t))−∇L(W∗)∥2
X.
Case 1:2
α+β<η∗<2
β.
In this case, we have
∥∆(t+ 1)∥2
X≤(1 + 7τ)∥∆(t)∥2
X−2η∗αβ
α+β∥∆(t)∥2
X+/parenleftbigg
η2
∗−2η∗
α+β/parenrightbigg
∥∇L(V(t))−∇L(W∗)∥2
X
≤(1 + 7τ)∥∆(t)∥2
X−2η∗αβ
α+β∥∆(t)∥2
X+/parenleftbigg
η2
∗−2η∗
α+β/parenrightbigg
β2∥∆(t)∥2
X
≤(1 + 7τ−βη∗(2−η∗β))∥∆(t)∥2
X
=(q+ 7τ)∥∆(t)∥2
X.
10Published in Transactions on Machine Learning Research (07/2024)
Case 2:0<η∗≤2
α+β.
Similarly, we have
∥∆(t+ 1)∥2
X≤(1 + 7τ−αη∗(2−η∗α))∥∆(t)∥2
X= (q+ 7τ)∥∆(t)∥2
X.
In both cases, we have ∥∆(t+ 1)∥2
X≤(q+ 7τ)∥∆(t)∥2
X.Thus,∥∆(t)∥2
X≤(q+ 7τ)t∥∆(0)∥2
X. □
With the help of this lemma, we further obtain the following trajectories comparison lemma, which leads
to the main conclusions of Theorem 2. We will show that the trajectories of GD (7) for deep linear neural
networks (6) are close to those of GD (2) for the corresponding convex optimization problem (1). Now, we
introduce the following technical lemma for trajectory comparison.
Lemma 6 Consider GD for deep linear neural networks (7) with learning rate η < η 1=2nN
Nβfor
aNWN:1(t),t= 0,1,···,and GD (2) with learning rate η∗=N
nNηforW(t),t= 0,1,···.
Assume that C(c1,c2)exists in Lemma 3 for any c1,c2>0. For anyτ∈(0,1), andη <η 1, we can choose
c1,c2>0and the constant C=C(c1,c2) =C′(τ,η/η 1), such that inequality (16) holds, given initialization
condition (14), and overparameterization condition
nmin≥CM2κ2B0N2θnN. (17)
Then, we have
∥aNWN:1(t)−W(t)∥2
X≤D(τ,q,t )∥aNWN:1(0)−W∗∥2
X, (18a)
|EDLN(t)−E(t)|≤β/parenleftbigg
qt/2/radicalbig
D(τ,q,t ) +1
2D(τ,q,t )/parenrightbigg
∥aNWN:1(0)−W∗∥2
X, (18b)
EDLN(t)≤3β(q+τ)t∥aNWN:1(0)−W∗∥2
X, (18c)
whereD(τ,q,t ) = min/braceleftig
τ
1−q,2(q+τ)t/bracerightig
, withqdefined in (10).
The proof of Lemma 6 requires analyzing the dynamical system
∆(t+ 1) = ∆(t)−η∗(∇L(V(t))−∇L(W(t))) +R(t),t≥0,
with ∆(t) =V(t)−W(t)andV(t) =aNWN:1(t). The analysis is essentially the same as the proof of
Lemma 5.
4.4 Random Initialization Fall into the Convergence Region
In Appendices E and F, we establish that when the width of each hidden layer is sufficiently large, both
Gaussian and one peak random orthogonal projections and embeddings initializations will fall into the
convergence region, as defined in Definition 2. Moreover, our analysis treats the balanced initialization as a
specific instance of orthogonal initialization.
To analyze the product of Gaussian random matrices, it only requires analyzing the concentration inequality
(Lemma 10) for the product of the χ2
nidistribution random variables, which has already been considered in
Du & Hu (2019). To analyze the product of one peak random orthogonal projections and embeddings, it
only requires analyzing the concentration inequality (Lemma 16) for the product of the beta distribution,
which has not been considered in Hu et al. (2020).
Given this foundation, and when combined with Lemma 3 and Lemma 6, we can corroborate Theorems 1,
2, 4, and 5.
4.5 Navigating Away from Bad Saddles
A critical point x∗offis a bad saddle if λmin(∇2f(x∗)) = 0. Kawaguchi (2016) showed that the deep linear
network has bad saddles, thus in general vanishing Hessian can hinder optimization. Now, we are going to
explain why do bad saddles not affect GD for overparameterized deep linear neural networks.
11Published in Transactions on Machine Learning Research (07/2024)
Theorem 2.3 in Kawaguchi (2016) showed that all bad saddles satisfy that WN−1:2is a non-full rank ma-
trix. Thus, to show that trajectories of GD are away from bad saddle points, it suffices to show that
inftσmin(WN−1:2(t))>0. In literature, there are two main ways to avoid bad saddles for GD to train the
deep linear network.
On the one hand, under the setting in Arora et al. (2018b), it has been showed that if the approximate
balanced initialization satisfies ∥WN:1(0)−W∗∥F≤σmin(W∗)−c, for some 0< c < σ min(W∗), then
σmin(WN:1(t))≥cthrough the training as well as ∥W1(t)∥≤(4∥W∗∥F)1/N, and∥WN(t)∥≤(4∥W∗∥F)1/N.
Thus,
σmin(WN−1:2(t))≥σmin(WN:1(t))
∥W1(t)∥∥WN(t)∥≥c
(4∥W∗∥)2/N.
On the other hand, if we assume our rescaled and overparameterized weight initialization falls into the
convergence region (14), we can show that (see B(t)in the proof of Lemma 3)
σmin(WN−1:2(t))≥max/braceleftbiggσmin(WN:2(t))
σmax(WN(t)),σmin(WN−1:1(t))
σmax(W1(t))/bracerightbigg
.
Thus,σmin(WN−1:2
(nN−1:2)1/2)≥e−c1−c2max{n1
nN−1,nN−1
n1}≥e−c1−c2>0. For the overparameterized deep linear
network, GD initialized in the convergence region will force the trajectories away from all bad saddles.
Numerical Experiments: In Appendix G, we will discuss some empirical evidence to support the main
results shown in Section 3. The figures compare the trajectories of the logarithm of loss for gradient descent
in deep linear networks and the corresponding convex optimization problem. The numerical experiments
demonstrate that increasing the minimal width of the hidden layers in deep linear networks leads to opti-
mization trajectories that closely resemble those of the corresponding convex problem, irrespective of the
initialization scheme. This suggests that sufficient network width stabilizes the optimization process, sup-
porting the main theoretical results presented in the paper.
5 Conclusion
In this paper, we present a comprehensive convergence analysis and trajectory comparison of the gradient
descent method for overparameterized deep linear neural networks with different random initializations.
A key contribution of our work is the trajectory comparison between gradient descent for deep linear neural
networks and the corresponding convex optimization problem. We showed that, with appropriate initial-
ization and sufficient width of the hidden layers, the trajectories of gradient descent for deep linear neural
networks closely match those of the convex optimization problem, regardless of the depth of the network.
This finding provides valuable insights into the optimization dynamics of deep linear networks and their
relation to their convex counterparts.
The convergence analysis and trajectory comparison presented in this paper contribute to the growing body
of work on understanding the optimization landscape and dynamics of deep neural networks. Our findings on
the efficiency of gradient descent in the overparameterized regime and the role of initialization in shaping the
optimization trajectories provide valuable insights that can potentially extend to more general deep neural
network architectures.
There could be a debate on whether the techniques and insights for linear networks can be extended to deep
non-linear networks. On the one hand, works by Ding et al. (2022) and He et al. (2020) have indicated
that the loss landscapes of non-linear neural networks differ significantly from those of linear networks.
Specifically, non-linear networks may contain spurious local minima, while linear networks do not. Hence,
the training dynamics of non-linear networks may be quite different from those of linear networks. On the
other hand, Du et al. (2019) has already demonstrated that gradient descent finds a global minimum in
training for sufficiently wide deep neural networks, including multilayer fully-connected neural networks,
ResNet, and convolutional ResNet. In these cases, the linear rate of convergence in terms of loss has been
established for overparameterized neural networks. However, the result of the trajectory comparison has not
been established.
12Published in Transactions on Machine Learning Research (07/2024)
Future research directions include exploring the generalization of our findings to nonlinear deep neural net-
works and investigating the impact of different optimization algorithms and regularization techniques on
the convergence and trajectory properties of deep networks. Furthermore, our work motivates the develop-
ment of theoretical frameworks that can provide a more comprehensive understanding of the optimization
dynamics of deep neural networks.
References
Sanjeev Arora, Nadav Cohen, Noah Golowich, and Wei Hu. A convergence analysis of gradient descent for
deep linear neural networks. CoRR, abs/1810.02281, 2018a.
Sanjeev Arora, Nadav Cohen, and Elad Hazan. On the optimization of deep networks: Implicit acceleration
by overparameterization. In International Conference on Machine Learning , pp. 244–253. PMLR, 2018b.
SanjeevArora,SimonDu, WeiHu, ZhiyuanLi, andRuosongWang. Fine-grainedanalysisofoptimizationand
generalization for overparameterized two-layer neural networks. In International Conference on Machine
Learning , pp. 322–332. PMLR, 2019a.
Sanjeev Arora, Simon S Du, Wei Hu, Zhiyuan Li, Russ R Salakhutdinov, and Ruosong Wang. On exact
computation with an infinitely wide neural net. Advances in Neural Information Processing Systems , 32,
2019b.
Yuan Cao and Quanquan Gu. Generalization bounds of stochastic gradient descent for wide and deep neural
networks. Advances in neural information processing systems , 32, 2019.
Djalil Chafaı, Djalil Chafä, Olivier Guédon, Guillaume Lecue, and Alain Pajor. Singular values of random
matrices. Lecture Notes , 2009.
Lénaïc Chizat, Edouard Oyallon, and Francis R. Bach. On lazy training in differentiable programming. In
NeurIPS , 2019.
AnnaChoromanska, MikaelHenaff, MichaelMathieu, GérardBenArous, andYannLeCun. Thelosssurfaces
of multilayer networks. In Artificial intelligence and statistics , pp. 192–204. PMLR, 2015a.
Anna Choromanska, Yann LeCun, and Gérard Ben Arous. Open problem: The landscape of the loss surfaces
of multilayer networks. In Conference on Learning Theory , pp. 1756–1760. PMLR, 2015b.
Amit Daniely. Sgd learns the conjugate kernel class of the network. Advances in Neural Information
Processing Systems , 30, 2017.
Yann N Dauphin, Razvan Pascanu, Caglar Gulcehre, Kyunghyun Cho, Surya Ganguli, and Yoshua Bengio.
Identifyingandattackingthesaddlepointprobleminhigh-dimensionalnon-convexoptimization. Advances
in neural information processing systems , 27, 2014.
Tian Ding, Dawei Li, and Ruoyu Sun. Suboptimal local minima exist for wide neural networks with smooth
activations. Mathematics of Operations Research , 47(4):2784–2814, 2022.
Simon Du and Wei Hu. Width provably matters in optimization for deep linear neural networks. In Inter-
national Conference on Machine Learning , pp. 1655–1664. PMLR, 2019.
Simon Du, Jason Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai. Gradient descent finds global minima of
deep neural networks. In International conference on machine learning , pp. 1675–1685. PMLR, 2019.
Simon S Du, Chi Jin, Jason D Lee, Michael I Jordan, Aarti Singh, and Barnabas Poczos. Gradient descent
can take exponential time to escape saddle points. Advances in neural information processing systems , 30,
2017.
Simon S Du, Wei Hu, and Jason D Lee. Algorithmic regularization in learning deep homogeneous models:
Layers are automatically balanced. Advances in Neural Information Processing Systems , 31, 2018.
13Published in Transactions on Machine Learning Research (07/2024)
Morris L. Eaton. Group invariance applications in statistics. Regional Conference Series in Probability and
Statistics , 1:i–133, 1989. ISSN 19355912.
Sam Elder. Bayesian adaptive data analysis guarantees from subgaussianity. CoRR, abs/1611.00065, 2016.
Guillaume Garrigos and Robert M Gower. Handbook of convergence theorems for (stochastic) gradient
methods. arXiv preprint arXiv:2301.11235 , 2023.
Rong Ge, Furong Huang, Chi Jin, and Yang Yuan. Escaping from saddle points—online stochastic gradient
for tensor decomposition. In Conference on learning theory , pp. 797–842. PMLR, 2015.
Guy Hacohen and Daphna Weinshall. Principal components bias in over-parameterized linear models, and
its manifestation in deep neural networks. The Journal of Machine Learning Research , 23(1):6973–7018,
2022.
Fengxiang He, Bohan Wang, and Dacheng Tao. Piecewise linear activations substantially shape the loss
surfaces of neural networks. arXiv preprint arXiv:2003.12236 , 2020.
Wei Hu, Lechao Xiao, and Jeffrey Pennington. Provable benefit of orthogonal initialization in optimizing
deep linear networks. In International Conference on Learning Representations , 2020.
Arthur Jacot, Franck Gabriel, and Clément Hongler. Neural tangent kernel: Convergence and generalization
in neural networks. Advances in neural information processing systems , 31, 2018.
GJO Jameson. Inequalities for gamma function ratios. The American Mathematical Monthly , 120(10):
936–940, 2013.
Chi Jin, Rong Ge, Praneeth Netrapalli, Sham M Kakade, and Michael I Jordan. How to escape saddle points
efficiently. In International Conference on Machine Learning , pp. 1724–1732. PMLR, 2017.
Kenji Kawaguchi. Deep learning without poor local minima. In Advances In Neural Information Processing
Systems, pp. 586–594, 2016.
Kenji Kawaguchi and Haihao Lu. Deep creates no bad local minima. arXiv: 1702.08580 , 2017.
Thomas Laurent and James Brecht. Deep linear networks with arbitrary loss: All local minima are global.
International Conference on Machine Learning , pp. 2908–2913, 2018.
Jaehoon Lee, Lechao Xiao, Samuel Schoenholz, Yasaman Bahri, Roman Novak, Jascha Sohl-Dickstein, and
Jeffrey Pennington. Wide neural networks of any depth evolve as linear models under gradient descent.
Advances in neural information processing systems , 32, 2019.
Jason D Lee, Max Simchowitz, Michael I Jordan, and Benjamin Recht. Gradient descent only converges to
minimizers. In Conference on learning theory , pp. 1246–1257. PMLR, 2016.
Chaoyue Liu, Libin Zhu, and Misha Belkin. On the linearity of large non-linear models: when and why the
tangent kernel is constant. Advances in Neural Information Processing Systems , 33:15954–15964, 2020.
Olivier Marchal and Julyan Arbel. On the sub-gaussianity of the beta and dirichlet distributions. Electronic
Communications in Probability , 22(none):1 – 14, 2017.
Yurii Nesterov. Introductory lectures on convex optimization: A basic course , volume 87. Springer Science
& Business Media, 2003.
Feng Qi, Bai-Ni Guo, and C. Chen. The best bounds in gautschi-kershaw inequalities. Mathematical
Inequalities & Applications , pp. 427–436, 2006.
Blake Woodworth, Suriya Gunasekar, Jason D Lee, Edward Moroshko, Pedro Savarese, Itay Golan, Daniel
Soudry, and Nathan Srebro. Kernel and rich regimes in overparametrized models. In Conference on
Learning Theory , pp. 3635–3673. PMLR, 2020.
Liu Ziyin, Botao Li, and Xiangming Meng. Exact solutions of a deep linear network. arXiv preprint
arXiv:2202.04777 , 2022.
14Published in Transactions on Machine Learning Research (07/2024)
A Proofs of Basic Properties of Semi-norm
Lemma 7 The loss function L(W)defined in (1) satisfies the following properties: for any W,V∈Rny×nx,
1.L(W) =L(WPX),
2.∇L(W) =∇L(WPX)PX,
3.⟨∇L(W),V⟩F=⟨∇L(W),V⟩X,
4.∥∇L(W)∥F=∥∇L(W)∥X,
5.∥W∥X≡∥W∥Fif and only if Xis full row rank.
6. IfL(W)isα-strongly convex (α > 0)with respect to the semi-norm ∥·∥X, thenW∗PXis unique,
that is, ifW′is another global minimizer of L(W), thenW∗PX=W′PX.
Proof of Lemma 7 The first property is a direct consequence of the definition of the projection matrix
PX.
Notice that1
ε(L(W+ε∆W)−L(W)) =1
ε(L(WPX+ε∆WPX)−L(WPX)).
By lettingε→0, the definition of the directional derivative implies
⟨∇L(W),∆W⟩F=⟨∇L(WPX),∆WPX⟩F=⟨∇L(WPX)PX,∆W⟩F,∀∆W∈Rny×nx,
sincePX=PT
X. This completes the proof of the second property.
The third property is obtained based on the fact that the orthogonal projection matrix satisfies PX=PT
X=
P2
X, since
⟨∇L(W),V⟩F=⟨∇L(WPX)PX,V⟩F
=⟨∇L(WPX)P2
X,VPX⟩F=⟨∇L(WPX)PX,V⟩X=⟨∇L(W),V⟩X.
SetV=∇L(W). Then the fourth property is implied by the third property.
For the fifth property, first recall that ∥W∥X=∥WPX∥FandPX=X(XTX)†XT.Xis of full row rank if
and only if PXis identity matrix, which completes the proof.
To prove the last property, let W′be another global minimizer of L(W).
SinceL(W)is strongly convex, we have
L(W′) =L(W∗)≥L(W′) +⟨∇L(W′),W∗−W′⟩X+α
2∥W′−W∗∥2
X. (19)
BecauseL(W∗) =L(W′), we can show that
⟨∇L(tW∗+ (1−t)W′),W∗−W′⟩F= 0. (20)
Applying property 3 and setting t= 0in (20), inequality (19) implies ∥W′−W∗∥X= 0.
By the definition of semi-norm ∥·∥X, we obtain that
∥W′PX−W∗PX∥F=∥W′−W∗∥X= 0,
which implies that W′PX=W∗PXfor any global minimizer W′. Thus,W∗PXis unique.
□
15Published in Transactions on Machine Learning Research (07/2024)
Proof of Lemma 1 BecauseXis not full row rank, we know that I−PX̸= 0. There exists Wsuch that
W(I−PX)̸= 0. Applying the first property in Lemma 7, we have
L(1
2W+1
2WPX) =L((1
2W+1
2WPX)PX) =L(WPX) =1
2L(W) +1
2L(WPX),
providedW̸=WPX.
Hence,Lis not strictly convex, which implies Lis not strongly convex.
To prove the second property, it suffices to show that g(W) =L(W)−α(l)λmin(XXT)
m∥W∥2
Xis convex. It is
obvious that
g(W) =L(W)−α(l)
mm/summationdisplay
i=1∥Wxi−yi∥2
2+α(l)
m(∥WX−Y∥2
F−λmin(XTX)∥W∥2
X). (21)
L(W)−α(l)
m/summationtextm
i=1∥Wxi,yi∥2
Fis convex, since l(·,yi)is strongly convex. The Hessian of ∥WX−Y∥2
F−
λmin(WTW)∥WPX∥2
Fhas no negative eigenvalue, thus the second term in (21) is also convex. This com-
pletes the proof. □
Proof of Lemma 2 Setr=rank (X), andu1,···,urbe an orthonormal basis of the column space of X.
Then,PX=/summationtextr
i=1uiuT
i.
Notice that
∥anWN:1(0)∥2
X=∥anWN:1(0)PX∥2
F=r/summationdisplay
i=1∥anWN:1(0)ui∥2
2.
By assumption, we have
E∥anWN:1(0)∥2
X=Er/summationdisplay
i=1∥anWN:1(0)ui∥2
2=r.
The Markov inequality implies
P(∥anWN:1(0)∥2
X≥2r
δ)≤δ
2.
Therefore, we can bound the initial loss value as
L0−L(W∗)≤⟨∇L(W∗),aNWN:1(0)X−W∗⟩+β
2∥aNWN:1(0)−W∗∥2
X
=β
2∥aNWN:1(0)−W∗∥2
X
≤β(∥aNWN:1(0)∥2
X+∥W∗∥2
X)
≤β(2r
δ+∥W∗∥2
X),
with probability at least 1−δ/2. □
B The Exact Statements of the Main Theorems
Other than the quantities in (11), we define
C4=nNκ2Bδ1
(η0−η)2/η2
0,
C5=nNκ2BδC0
(η0−η)2/η2
0+ lnN,
C6=nNκ2BδC0
(η0−η)2/η2
0+C0ln(N),(22)
whereη0=2nN
e2cNβwithc>0. Recall that η1=2nN
Nβ.
16Published in Transactions on Machine Learning Research (07/2024)
Theorem 4 Given anyc>0, and 0<δ < 1/2, defineη0=2nN
e2cNβ, and consider the learning rate η <η 0.
There exists a constant C:=C(c),such that if
nmin≥C·C5·N, (23)
then with probability at least 1−δover the random Gaussian initialization, we have
EDLN(t)≤/parenleftigg
1−4e−cη
η0(1−η
η0)
κ/parenrightiggt
EDLN(0).
Theorem 5 Given anyc>0, and 0<δ < 1/2, defineη0=2nN
e2cβN, and consider the learning rate η <η 0.
There exists a constant C:=C(c), such that if
nmin≥C·C5, (24)
then with probability at least 1−δover the random one peak projections and embeddings initialization, we
have
EDLN(t)≤/parenleftigg
1−4e−cη
η0(1−η
η0)
κ/parenrightiggt
EDLN(0).
Specially, if n1=n2=···=nN−1=n≥min{nN,n0}, then the requirement (24) can be replaced by
n≥C·C4. (25)
Remark 8 AssumeL(aNWN···W1) =1
2∥aNWN···W1X−Y∥2
F, andn1=···=nN−1=n. Then for
Gaussian initialization, our Theorem 4 leads to Theorem 4.1 in Du & Hu (2019). Similarly, for orthogonal
initialization, our Theorem 5 leads to Theorem 4.1 in Hu et al. (2020).
Next, we present a version of the theorem related to balanced initialization.
Theorem 6 Assumen1=···=nN−1=n. Given any c >0, and 0< δ < 1/2, defineη0=2nN
e2cβN, and
consider the learning rate η<η 0. There exists a constant C:=C(c),such that as long as
n≥C·C4. (26)
then with probability at least 1−δover special balanced initial, we have
EDLN(t)≤/parenleftigg
1−4e−cη
η0(1−η
η0)
κ/parenrightiggt
EDLN(0).
C Inequalities in Convex Optimization
Convex optimization has been studied for about a century. Recall the definitions and basic inequalities for
α−strongly convex and β−Lipschitz functions.
Definition 3 A continues differentiable function fis said to be β−Lipschitz if the gradient ∇fisβ−
Lipschitz, that is if for all x,y,
∥∇f(y)−∇f(x)∥≤β∥y−x∥,
fis said to be α−strongly convex if for all x,y, we have
f(y)≥f(x) +⟨∇f(x),y−x⟩+α
2∥y−x∥2.
17Published in Transactions on Machine Learning Research (07/2024)
Proposition 1 Iffisα−strongly convex and ∇fisβ−Lipschitz with respect to a (semi-)norm, then α≤β
and
⟨∇f(x),y−x⟩+α
2∥y−x∥2≤f(y)−f(x)≤⟨∇f(x),y−x⟩+β
2∥y−x∥2, (27)
⟨∇f(x)−∇f(y),x−y⟩≥αβ
α+β∥x−y∥2+1
α+β∥∇f(x)−∇f(y)∥2, (28)
∥∇f(x)−∇f(y)∥≥α∥x−y∥, (29)
f(x)−f(y)≤⟨∇f(x),x−y⟩−1
2β∥∇f(x)−∇f(y)∥2. (30)
Proof of Proposition 1 We only proof the last inequality.
Letz=y−1
β(∇f(y)−∇f(x)). Sincefis convexβ−Lipschitz, we have
f(z)−f(x)≥⟨∇f(x),z−x⟩
and
f(z)−f(y)≤⟨∇f(y),z−y⟩+β
2∥z−y∥2.
Thus,
f(x)−f(y) =f(x)−f(z) +f(z)−f(y)
≤⟨∇f(x),x−z⟩+⟨∇f(y),z−y⟩+β
2∥z−y∥2
=⟨∇f(x),x−y⟩−1
2β∥∇f(x)−∇f(y)∥2.
□
Before we start to prove Lemma 3, let us first include and prove the following result.
Lemma 8 1.AssumeLisα−strongly convex, α > 0. Denote a global minimizer of LbyW∗. Then for
anyW,
L(W∗)−L(W)≥−1
2α∥∇L(W)∥2
X. (31)
2.Assume∇Lisβ−Lipschitz, then
L(W∗)−L(W)≤−1
2β∥∇L(W)∥2
X. (32)
Proof of Lemma 8 1. First, we know that ∇L(W∗) = 0.Lisα−strongly convex, which implies the
inequality (27) holds. Thus
L(V)−L(W)≥⟨∇L(W),V−W⟩X+α
2∥V−W∥2
X=:g(V).
Minimizing both sides in terms of Vgives (31).
Now we focus on minimizing g(V). Sinceg(V)∈C1and the global minimizer exits, we have
∇g(V∗) =∇L(W)PX+α(V∗−W)PX= 0,
whereV∗is a global minimizer for g(V). Thus,
g(V∗) =−1
2α∥∇L(W)∥2
X.
18Published in Transactions on Machine Learning Research (07/2024)
2.Applying proposition 1 to a β−Lipschitz function ∇L, we obtain
L(W∗)−L(W)
≤⟨∇L(W∗),W∗−W⟩X−1
2β∥∇L(W)−∇L(W∗)∥2
X
=−1
2β∥∇L(W)∥2
X.
□
D Proofs Related to Convergence Region
Proof of Lemma 3 To prove Lemma 3, it suffices to show that the following three properties hold A(t),
B(t), andC(t)for allt= 0,1,···.
1.A(t):
Lt−L(W∗)≤(1−ηγ)t(L0−L(W∗)).
2.B(t):

σmax(WN:i+1(t))≤ec1(nN−1:i)1/2,1≤i≤N−1,
σmin(WN:i+1(t))≥e−c2(nN−1:i)1/2,1≤i≤N−1,
σmax(Wi−1:1(t)|R(X))≤ec1(ni−1:1)1/2,2≤i≤N,
σmin(Wi−1:1(t)|R(X))≥e−c2(ni−1:1)1/2,2≤i≤N,
∥Wj:i(t)∥≤M·Nθ(1
nmin/producttext
i−1≤k≤jnk)1/2,1<i≤j <N.
3.C(t):
∥Wi(t)−Wi(0)∥F≤2e2c1√2βB√nNγ=:R,1≤i≤N.
Using simultaneous induction, the proof of Lemma 3 is divided into the following 3 claims.
Claim 1A(0),···,A(t),B(0),···,B(t) =⇒ C (t+ 1).
Claim 2C(t) =⇒ B (t), ifnmin≥C(c1,c2)M2κ2B0
ε2N2θnN, whereC(c1,c2)is a positive constant only depend
onc1,c2.
Claim 3A(t),B(t) =⇒ A (t+ 1), ifnmin≥C(c1,c2)M2B0N2θnN,whereC(c1,c2)is a positive constant
only depend on c1,c2.
□
Proof of Claim 1 As a consequence of Lemma 8 and Lemma 7, and A(s), s≤t, we have
∥∇L(aNWN:1(s))∥2
F=∥∇Ls−∇L(W∗PX)∥2
X
≤2β[Ls−L(W∗)]
≤2β(1−ηγ)sB.(33)
FromA(0),···,A(t),B(0),···,B(t), we have for any 0≤s≤t,
/vextenddouble/vextenddouble/vextenddouble/vextenddouble∂L
∂Wi(s)/vextenddouble/vextenddouble/vextenddouble/vextenddouble
F≤aN∥WN:i+1(s)∥∥∇L(aNWN:1(s))∥F/vextenddouble/vextenddoubleWi−1:1(s)|R(X)/vextenddouble/vextenddouble
≤e2c1
√nN∥∇L(aNWN:1(s))∥F
≤e2c1
√nN/radicalig
2β(1−ηγ)sB.(34)
19Published in Transactions on Machine Learning Research (07/2024)
Then,
∥Wi(t+ 1)−Wi(0)∥F≤t/summationdisplay
s=0∥Wi(s+ 1)−Wi(s)∥F
=t/summationdisplay
s=0/vextenddouble/vextenddouble/vextenddouble/vextenddoubleη∂L
∂Wi(s)/vextenddouble/vextenddouble/vextenddouble/vextenddouble
F
≤ηe2c1
√nN/radicalbig
2βBt/summationdisplay
s=0(1−ηγ)s/2
≤ηe2c1
√nN/radicalbig
2βBt/summationdisplay
s=0(1−ηγ/2)s
≤2e2c1√2βB√nNγ=R.
This provesC(t+ 1). □
Proof of Claim 2 Letδi=Wi(t)−Wi(0),1≤i≤N. UsingC(t), we have∥δi∥F≤R,1≤i≤N. Set
ε1=e−c1/2min{ec1−ec1/2,e−c2/2−e−c2,1/2}.
It is suffices to show that
∥WN:i(t)−WN:i(0)∥≤ec1/2ε1(nN−1nN−1···ni−1)1/2,1<i≤N, (35)
/vextenddouble/vextenddouble(Wi:1(t)−Wi:1(0))|R(X)/vextenddouble/vextenddouble≤ec1/2ε1(n1n2···ni−1)1/2,1≤i<N, (36)
and
∥Wj:i(t)−Wj:i(0)∥≤M/2·Nθ
1
nmin/productdisplay
i−1≤k≤jnk
1/2
,1<i≤j <N, (37)
becauseσmin(A+B)≥σmin(A)−σmax(B) =σmin(A)−∥B∥andσmax(A+B)≤σmax(A) +σmax(B) =
∥A∥+∥B∥(e.g. see Theorem 1.3 in Chafaı et al. (2009)).
Case 1. We first prove (37).
For1≤i<j≤N, we can write Wj:i(t) = (Wj(0) +δj)···(Wi(0) +δi).
Expanding the above product, each term has the form:
Wj:(ks+1)(0)·δks·W(ks−1):(ks−1+1)(0)·δks−1···δk1·W(k1−1):i(0), (38)
wherei≤k1<···<ks≤jare positions at which perturbation terms δklare taken out.
Notice that the convergence region assumptions (14) implies that for any 1<i≤j <N,
∥Wj:i(0)∥≤M/2·Nθ
/productdisplay
i≤k≤j−1nk·max{ni−1,nj}
1/2
≤M·Nθ/parenleftbigg/producttext
i−1≤k≤jnk
nmin/parenrightbigg1/2
.(39)
WLOG, assume M≥1. Ifi=j+ 1, then
∥Wj:i(0)∥=∥I∥≤M·Nθ(nj/nmin)1/2.
Assumei>1,j <N, applying inequality (39) as well as the following inequality
j−i+1/summationdisplay
s=1/parenleftbiggj−i+ 1
s/parenrightbigg
xs= (1 +x)j−i+1−1≤(1 +x)N−1,∀x≥0,
20Published in Transactions on Machine Learning Research (07/2024)
we obtain that
∥Wj:i(t)−Wj:i(0)∥
≤j−i+1/summationdisplay
s=1/parenleftbiggj−i+ 1
s/parenrightbigg
Rs(M·Nθ)s+1n−s/2
min(ni−1···nj/nmin)1/2
≤M·Nθ(ni−1···nj/nmin)1/2[(1 +R·M·Nθ/√nmin)N−1]
≤ε1M·Nθ(ni−1···nj/nmin)1/2.
The last line holds due to the following reasons:
there exists absolute constant A1,A2>0such that
(1 +x)N−1≤A2xN,
ifx≥0,N≥1, andxN≤A1. Since there exists positive constant C(c1,c2), which only depends on c1,c2,
such that when
nmin≥C(c1,c2)M2κ2B0
ε2N2θnN (40)
we can have
R·M·Nθ+1/√nmin≤A1,
as well as
[(1 +R·M·Nθ/√nmin)N−1]≤A2·M·R·Nθ+1/√nmin≤ε1=ε1(c1,c2).
Case 2. The proof of (35) is similar. Set j=N, we can save the factor M·Nθfrom previous calculation,
which means
∥WN:i(t)−WN:i(0)∥
≤ec1/2N−i+1/summationdisplay
s=1/parenleftbiggN−i+ 1
s/parenrightbigg
Rs(M·Nθ)sn−s/2
min(ni−1···nN−1)1/2
≤ec1/2(ni−1···nN−1)1/2[(1 +R·M·Nθ/√nmin)N−1]
≤ec1/2ε1(ni−1···nN−1)1/2,i≥2,
where the last line is implied by equation (40).
Case 3. Similarly, we have
/vextenddouble/vextenddoubleWj:1(t)|R(X)−Wj:1(0)|R(X)/vextenddouble/vextenddouble
≤ec1/2j/summationdisplay
s=1/parenleftbiggj
s/parenrightbigg
Rs(M·Nθ)sn−s/2
min(n1···nj)1/2
≤ec1/2(n1···nj)1/2[(1 +R·M·Nθ/√nmin)N−1]
≤ec1/2ε1(n1···nj)1/2,j≤N−1
This provesB(t).
□
Proof of Claim 3 The GD (7) implies
WN:1(t+ 1)
=/parenleftbigg
WN(t)−η∂LN
∂WN(t)/parenrightbigg/parenleftbigg
WN−1(t)−η∂LN
∂WN−1(t)/parenrightbigg
···/parenleftbigg
W1(t)−η∂LN
∂W1(t)/parenrightbigg
=WN:1(t)−η·aNN/summationdisplay
i=1WN:i+1(t)WT
N:i+1(t)∇L(aNWN:1(t))(Wi−1:1(t))T(Wi−1:1(t)) +E(t),
21Published in Transactions on Machine Learning Research (07/2024)
whereE(t)contains all high-order terms (those with η2or higher). Define a linear operator
P(t)[A] =a2
NN/summationdisplay
i=1WN:i+1(t)WT
N:i+1(t)(APX)(Wi−1:1(t)|R(X))TWi−1:1(t)|R(X), (41)
for anyA∈RnN×n0.
Now we have
aNWN:1(t+ 1) =aNWN:1(t)−η·P(t)[∇L(aNWN:1(t)PX)] +aNE(t). (42)
Easy to check that P(t)[·]is a sum of positive semidefinite linear operator.
The following proposition describes the eigenvalues of the linear operator P(t)[·].
Proposition 2 LetS1,S2be symmetric matrices. Suppose S1=UΛ1UT,S2=VΛ2VT, whereU=
[u1,u2,···,um], andV= [v1,v2,···,vn]are othogonal matrices, and Λ1=diag(λ1,λ2,···,λm)and
Λ2=diag(µ1,µ2,···,µn)are diagonal matrices. Then the linear operator L(A) :=S1AS2is orthogo-
nally diagonalizable, and L(Aij) =λiµjAij, whereλiµjrepresent all eigenvalues corresponding to their
eigenvectors Aij=uivT
j.
Applying this proposition and the assumption B(t), we obtain the upper bound and lower bound for the
maximum and minimum eigenvalues of positive definite operator P(t), respectively,
λmax(P(t))≤a2
NN/summationdisplay
i=1σ2
max(Wi−1:1(t)|R(X))·σ2
max(WN:i+1(t))≤N
nNe2c1,
and
λmin(P(t))≥a2
NN/summationdisplay
i=1σ2
min(Wi−1:1(t)|R(X))·σ2
min(WN:i+1(t))≥N
nNe−2c2.
In conclusion, we have
λmax(P(t))≤N
nNe2c1,andλmin(P(t))≥N
nNe−2c2. (43)
With learning rate η=ηε=(1−ε)2nN
e6c1+3c2βN,0<ε< 1, we have
Lt+1−Lt
≤⟨∇Lt,−ηP(t)[∇Lt]⟩X+⟨∇Lt,aNE(t)⟩X+β
2∥ηP(t)[∇Lt]−aNE(t)∥2
X
=⟨∇Lt,−ηP(t)[∇Lt]⟩+β
2η2∥P(t)[∇Lt]∥2
X+F(t)
≤−/parenleftbigg
ηλmin(P(t))−β
2η2λ2
max(P(t))/parenrightbigg
∥∇Lt∥2
X+F(t)
≤−e−2c2N
nNη/parenleftbigg
1−e4c1+2c2β
2ηN
nN/parenrightbigg
∥∇Lt∥2
X+F(t),
where
F(t) =⟨∇Lt,aNE(t)⟩X+β
2∥ηP(t)[∇Lt]−aNE(t)∥2
X−β
2η2∥P(t)[∇Lt]∥2
X.
We claim that F(t)is small enough, such that
Lt+1−Lt
≤−e−2c2N
nNη/parenleftbigg
1−e4c1+2c2β
2ηN
nN/parenrightbigg
∥∇Lt∥2
X+F(t)
≤−e−3c2N
nNη/parenleftbigg
1−e6c1+3c2β
2ηN
nN/parenrightbigg
∥∇Lt∥2
X
=−e−6(c1+c2)2ε(1−ε)
β∥∇Lt∥2
X.(44)
22Published in Transactions on Machine Learning Research (07/2024)
Assuming this claim for the moment, we complete the proof. Combining (31) and (44), we have
/braceleftigg
Lt+1−Lt≤−e−6(c1+c2)2ε(1−ε)
β∥∇Lt∥2
X,
L(W∗)−Lt≥−1
2α∥∇Lt∥2
X,
which implies
Lt+1−L(W∗)≤/parenleftbigg
1−e−6(c1+c2)4ε(1−ε)
κ/parenrightbigg
(Lt−L(W∗)),
that is
Lt−L(W∗)≤/parenleftbigg
1−e−6(c1+c2)4ε(1−ε)
κ/parenrightbiggt
(L0−L(W∗)) = (1−ηγ)t(L0−L(W∗)).
EstimateF(t)
Notice that
|F(t)|
≤∥∇Lt∥X∥aNE(t)∥X+β
2(2ηλmax(P(t))∥∇Lt∥X∥aNE(t)∥X+∥aNE(t)∥2
X)
= :I1+I2.
From (34), we have
/vextenddouble/vextenddouble/vextenddouble/vextenddouble∂L
∂Wi(t)/vextenddouble/vextenddouble/vextenddouble/vextenddouble
F≤e2c1
√nN∥∇L(aNWN:1(t))∥F=e2c1
√nN∥∇L(aNWN:1(t))∥X=:K.
Expanding the product
WN:1(t+ 1) =/parenleftbigg
WN(t)−η∂LN
∂WN(t)/parenrightbigg/parenleftbigg
WN−1(t)−η∂LN
∂WN−1(t)/parenrightbigg
···/parenleftbigg
W1(t)−η∂LN
∂W1(t)/parenrightbigg
,
each term has the form:
∆ =WN:(ks+1)(t)·η∂L
∂Wks(t)·W(ks−1):(ks−1+1)(t)·η∂L
∂Wks−1(t)···η∂L
Wk1(t)·W(k1−1):1(t),
where 1≤k1<k2<···<ks≤N.
As a direct consequence of inequality B(t)and inequality (39), we obtain
∥∆∥X=∥∆PX∥F≤1
aN√nNe2c1(ηK)s/parenleftbiggM·Nθ
√nmin/parenrightbiggs−1
,
Recall that E(t)contains all high-order terms (those with η2or higher) in the expansion of the product.
Thus,E(t)can be expressed as follows:
N/summationdisplay
s=2/summationdisplay
1≤k1<k2<···<ks≤NWN:(ks+1)(t)·η∂L
∂Wks(t)·W(ks−1):(ks−1+1)(t)·η∂L
∂Wks−1(t)···η∂L
Wk1(t)·W(k1−1):1(t).
23Published in Transactions on Machine Learning Research (07/2024)
Setξ= min{(e−2c2−e−3c2)/e4c1+1,1
4(e6c1−e4c1)/e6c1+1,1
2(e6c1−e4c1)1/2/e4c1+1,1}.
Recall the inequality/parenleftbiggN
s/parenrightbigg
≤(eN)s. Thus, we have
aN∥E(t)∥X
≤1√nNe2c1N/summationdisplay
s=2/parenleftbiggN
s/parenrightbigg
(ηK)s/parenleftbiggM·Nθ
√nmin/parenrightbiggs−1
≤1√nN/parenleftbiggM·Nθ
√nmin/parenrightbigg−1
e2c1N/summationdisplay
s=2(eN)s(ηK)s/parenleftbiggM·Nθ
√nmin/parenrightbiggs
≤1√nNe2c1(ηeKN )ηeKM·Nθ+1/√nmin
1−ηeKM·Nθ+1/√nmin
≤ξN
nNη·e4c1+1∥∇L(aNWN:1(t))∥X(ifηeKM·Nθ+1/√nmin<ξ/(1 +ξ))
=ξ·e4c1+1/parenleftbigg
ηN
nN/parenrightbigg
∥∇L(aNWN:1(t))∥X.(45)
Using (33) and the upper bound of η, we know that there exists constant C(c1,c2), such that
nmin≥C(c1,c2)M2·B0N2θnN,
and
ηeKM·Nθ+1/√nmin≤2√
2M·e1+2c1√B0Nθ√nN√nmin=1
C′(c1,c2)≤ξ
2≤ξ
1 +ξ.
Using (45), we have
I1≤ξ·e4c1+1/parenleftbigg
ηN
nN/parenrightbigg
∥∇Lt∥2
X≤(e−2c2−e−3c2)/parenleftbigg
ηN
nN/parenrightbigg
∥∇Lt∥2
X,
and
I2
≤β
2/parenleftbigg
2ξ·e6c1+1/parenleftbigg
η2N2
n2
N/parenrightbigg
∥∇Lt∥2
X+ξ2·e8c1+2/parenleftbigg
η2N2
n2
N/parenrightbigg
∥∇Lt∥2
X/parenrightbigg
≤(e6c1−e4c1)β
2η2N2
n2
N∥∇Lt∥2
X.
Thus, (44) valid.
This provesA(t).
□
Proof of Lemma 4 Due to (33), (42), (43), (45), and lemma 8, we have
∥R(t)∥X=/vextenddouble/vextenddouble/vextenddouble/vextenddoubleaNE(t) +η/parenleftbiggN
nN∇Lt−P(t)[∇Lt]/parenrightbigg/vextenddouble/vextenddouble/vextenddouble/vextenddouble
X
≤∥aNE(t)∥X+ηmax/braceleftbigg
λmax(P(t))−N
nN,N
nN−λmin(P(t))/bracerightbigg
∥∇Lt∥X
≤(C′·ξ+ max{e2c1−1,1−e−2c2})·ηN
nN·∥∇Lt∥X
≤2/radicalbig
2β(Lt−L(W∗))
e6c1+3c2·β·(C′·ξ+ max{e2c1−1,1−e−2c2}).
24Published in Transactions on Machine Learning Research (07/2024)
Due to the fact that Lt−L(W∗)is non-increasing in t, andC′is a constant only depend on c1,c2, we can
choose small enough positive c1,c2andξ, which depends on τ, such that
∥R(t)∥X≤τ/radicalbig
2β(Lt−L(W∗))
β≤τ∥aNWN:1(t)−W∗∥X.
□
Proof of Lemma 6 Using Lemma 4, we obtain that for any τ∈(0,1)andη < η 1, we can find small
enough positive constant c1,c2, which are only depend on τ,η/η 1, and constant C=C(c1,c2) =C′′(τ,η/η 1)
mentioned in Lemma 4, such that
η=(1−ε)2nN
e6c1+3c2βN,
where 0<ε< 1, as well as
V(t+ 1) =V(t)−η∗∇L(V(t)) +R(t),
whereV(t) =aNWN:1(t),η∗=N
nNη, and∥R(t)∥X≤τ′=τ∥V(t)−W∗∥X.
Notice that θ0:=η/η1=1−ε
e6c1+3c2andη/η0= 1−ε, whereη0=2nN
e6c1+3c2βN.
For the right hand side of inequality (15), we have
C(c1,c2)M2κ2B0
ε2N2θnN=C′′(τ,η/η 1)M2κ2B0
ε2N2θnN.
To show that inequality (15) is equivalent to inequality (17), it suffices to show that εonly depend on τ,η/η 1.
Notice that
ε= 1−η/η0= 1−θ0e6c1+3c2,
andc1,c2only depend on τandη/η1, which implies εonly depend on τ,η/η 1.
Now, we start to prove the three inequalities in (18).
Recall GD (2) for W(t). Define ∆(t) =V(t)−W(t) =aNWN:1(t)−W(t). Notice that
∆(t+ 1) = ∆(t)−η∗(∇L(V(t))−∇L(W(t))) +R(t),
and
∥∆(t+ 1)∥2
X
≤η2
∗∥∇L(V(t))−∇L(W(t))∥2
X−2η∗⟨∆(t),∇L(V(t))−∇L(W(t))⟩X
+∥∆(t)∥2
X+ (2∥∆(t)∥X+ 2η∗∥∇L(V(t))−∇L(W(t))∥X+τ′)τ′.
Letlt= 2∥∆(t)∥X+ 2η∗∥∇L(V(t))−∇L(W(t))∥X+τ′.
Now, we aim to find an upper bound for lt.
Applying lemma 8 with the assumption 0<η∗=N
nNη<2
β, we know that
lt≤(6∥∆(t)∥X+τ′)≤7(∥W(t)−W∗∥X+∥V(t)−W∗∥X). (46)
Thus
ltτ′≤7τ∥V(t)−W∗∥X(∥V(t)−W∗∥X+∥W(t)−W∗∥X) =:Utτ.
25Published in Transactions on Machine Learning Research (07/2024)
By inequality (28),
∥∆(t+ 1)∥2
X
≤∥∆(t)∥2
X−2η∗⟨∆(t),∇L(V(t))−∇L(W(t))⟩X
+η2
∗∥∇L(V(t))−∇L(W(t))∥2
X+Utτ
=∥∆(t)∥2
X−2η∗⟨V(t)−W(t),∇L(V(t))−∇L(W(t))⟩X
+η2
∗∥∇L(V(t))−∇L(W(t))∥2
X+Utτ
≤∥∆(t)∥2
X−2η∗αβ
α+β∥∆(t)∥2
X
+/parenleftbigg
η2
∗−2η∗
α+β/parenrightbigg
∥∇L(V(t))−∇L(W(t))∥2
X+Utτ.
Case 1:2
α+β<η∗<2
β.
In this case, we have
∥∆(t+ 1)∥2
X
≤∥∆(t)∥2
X−2η∗αβ
α+β∥∆(t)∥2
X+/parenleftbigg
η2
∗−2η∗
α+β/parenrightbigg
∥∇L(V(t))−∇L(W(t))∥2
X+Utτ
≤∥∆(t)∥2
X−2η∗αβ
α+β∥∆(t)∥2
X+/parenleftbigg
η2
∗−2η∗
α+β/parenrightbigg
β2∥∆(t)∥2
X+Utτ
≤(1−βη∗(2−η∗β))∥∆(t)∥2
X+Utτ
= :q∥∆(t)∥2
X+Utτ.
Case 2:0<η∗≤2
α+β.
Similarly, we have
∥∆(t+ 1)∥2
X≤(1−αη∗(2−η∗α))∥∆(t)∥2
X+Utτ=:q∥∆(t)∥2
X+Utτ. (47)
In both cases, we have 0<q< 1.
First of all, since Ut≤U0and∥∆(0)∥X= 0, we obtain that
∥∆(t)∥2
X≤U0τ
1−q+qt/parenleftbigg
∥∆(0)∥2
X−U0τ
1−q/parenrightbigg
≤U0τ
1−q≤14τ
1−q∥V(0)−W∗∥2
X.
Applying Lemma 5 for V(t)andW(t), we obtain∥V(t)−W∗∥2
X≤(1 +ε)tqt∥V(0)−W∗∥2
Xand
∥W(t)−W∗∥2
X≤qt∥W(0)−W∗∥2
X, respectively. Thus,
|L(W(t))−L(aNWN:1(t))|
≤|⟨∇L(W(t)),∆(t)⟩X|+β
2∥∆(t)∥2
X
≤β∥W(t)−W∗∥X·∥∆(t)∥X+β
2∥∆(t)∥2
X
≤β/parenleftbigg
qt/2/radicalbigg14τ
1−q+7τ
1−q/parenrightbigg
∥V(0)−W∗∥2
X.
Generally speaking, (47) implies
∥∆(t)∥2
X≤τt−1/summationdisplay
j=0qt−1−jUj.
26Published in Transactions on Machine Learning Research (07/2024)
We have
∥∆(t)∥2
X≤14τt−1/summationdisplay
j=0(q+ 7τ)jqt−1−j∥V(0)−W∗∥2
X
≤2(q+ 7τ)t/parenleftbigg
1−/parenleftbigq
q+ 7τ/parenrightbigt/parenrightbigg
∥V(0)−W∗∥2
X
Thus, we have
∥aNWN:1(t)−W(t)∥2
X≤min/braceleftbigg14τ
1−q,2(q+ 7τ)t/bracerightbigg
∥V(0)−W∗∥2
X,
as well as
|L(W(t))−L(aNWN:1(t))|
≤β∥W(t)−W∗∥X·∥∆(t)∥X+β
2∥∆(t)∥2
X
≤β/parenleftigg/radicaligg
min/braceleftbigg14τ
1−q,2(q+ 7τ)t/bracerightbigg
·qt/2+1
2min/braceleftbigg14τ
1−q,2(q+ 7τ)t/bracerightbigg/parenrightigg
∥V(0)−W∗∥2
X.
By triangle inequality as well as L(W(t))−L(W∗)≤β
2qt∥V(0)−W∗∥2
X, we have
|L(aNWN:1(t))−L(W∗)|≤3β(q+ 7τ)t∥V(0)−W∗∥2
X.
Without loss of generality, we replace all 14τand7τbyτ, which completes the proof. □
E Gaussian Initialization Fall into the Convergence Region
In this section, we first establish some spectral properties of the products of random Gaussian matrices. The
spectral properties lead to the conclusion that overparameterization guarantees that the random initializa-
tion will fall into the convergence region with high probability. Denote by N(0,1)the standard Gaussian
distribution, and χ2
kthe chi square distribution with kdegrees of freedom. Let Sd−1={x∈Rd;∥x∥2= 1}
be the unit sphere in Rd.
The scaling factor aN=1√n1n2···nNensures that the networks at initialization preserves the norm of every
input in expectation.
Lemma 9 For anyx∈Rn0, the Gaussian initialization satisfies
E/bracketleftig
∥aNWN:1(0)x∥2
2/bracketrightig
=∥x∥2
2.
Proof of Lemma 9 For random matrix A∈Rni×ni−1with i.i.dN(0,1)entries and any vector 0̸=v∈
Rni−1, the distribution of∥Av∥2
2
∥v∥2
2isχn2
i. We rewrite
∥WN:1(0)x∥2
2/∥x∥2
2=ZNZN−1···Z1,
whereZi=∥Wi:1(0)x∥2/∥Wi−1:1(0)x∥2.
Then we know that the distribution of random variable Z1∼χ2
n1, and conditional distribution of random
variablesZi|(Z1,···,Zi−1)∼χ2
ni(1< i≤N). Thus,Z1,···,Zniare independent. By law of iterated
expectations, we have
E[∥WN:1(0)x∥2
2/∥x∥2
2] =N/productdisplay
j=1nj.
□
27Published in Transactions on Machine Learning Research (07/2024)
Define ∆1=/summationtextN−1
j=11/nj. Now, we introduce a new notation Ω/parenleftig
1
∆1/parenrightig
, which means that there exists k>0,
such that Ω/parenleftig
1
∆1/parenrightig
≥k
∆1.
Lemma 10 Consider real random matrix Aj∈Rnj×nj−1,1≤j≤qwith i.i.dN(0,1)entries and any vector
0̸=x∈Rn1.
Define ∆1(q) =/summationtextq
j=11
njandnmin= min 1≤j≤qnj. Then
P(∥AqAq−1···A1x∥2
2/∥x∥2
2>ecn1···nq)≤exp/braceleftbigg
−c2
8∆1(q)/bracerightbigg
=:f1(c),∀c>0. (48)
When 0<c≤3 ln 2,∆1(q)≤c/(12 ln 2), we have
P(∥AqAq−1···A1x∥2
2/∥x∥2
2<e−cn1···nq)≤exp/braceleftbigg
−c2
36 ln(2)∆ 1(q)/bracerightbigg
=:f2(c). (49)
Hence, for any x∈Sn0−1with probability at least 1−e−Ω(1
∆1(q)), we have
e−c2/2(n1···nq)1/2≤∥Aq···A1x∥2≤ec1/2(n1···nq)1/2,
when 0<c2≤3 ln 2,∆1(q)≤c2/(12 ln 2).
Proof of Lemma 10 For random matrix Ai∈Rni×ni−1with i.i.dN(0,1)entries and any vector 0̸=v∈
Rni−1, the random variable∥Aiv∥2
2
∥v∥2
2is distributed as χ2
ni. We rewrite
∥Aq···A1x∥2
2/∥x∥2
2=ZqZq−1···Z1,
whereZi=∥Ai:1x∥2/∥Ai−1:1x∥2. We haveZ1∼χ2
n1,Zi|(Z1,···,Zi−1)∼χ2
ni(1<i≤q).
Recall the moments of Z∼χ2
m:
E[Zλ] =2λΓ(m
2+λ)
Γ(m
2),∀λ>−m
2.
Now, we aim to find the Chernoff type bound.
Case 1: We define ratio of Gamma function
R(x,λ) =Γ(x+λ)
Γ(x),λ> 0,x> 0.
In Jameson (2013), we have
R(x,λ)≤x(x+λ)λ−1≤(x+λ)λ,λ> 0,x> 0. (50)
Fixedc>0, for anyλ>0we have
P(Zq···Z1>ecn1···nq)≤P((Zq···Z1)λ>eλc(n1···nq)λ)
≤e−λc(n1···nq)−λE[(Zq···Z1)λ] (Markov inequality)
= exp{−λ(c+ ln(n1···nq))}q/productdisplay
j=12λR(nj/2,λ)(Law of total expectation)
≤exp{−λ(c+ ln(n1···nq)) +qλln 2 +q/summationdisplay
j=1λln(nj
2+λ)}(Inequality (50))
= exp{−λc+λq/summationdisplay
j=1ln(1 +2λ
nj)}
≤exp{−λc+ 2λ2q/summationdisplay
j=11
nj}.
28Published in Transactions on Machine Learning Research (07/2024)
Define constant ∆1(q) =/summationtextq
j=11
nj. Setλ=c
4∆1(q), we obtain (48).
Case 2: Letnmin= min 1≤j≤qnj.
P(Zq···Z1<e−cn1···nq)≤P((Zq···Z1)λ>e−λc(n1···nq)λ)
≤exp{λ(c−ln(n1···nq)) +qλln 2 +q/summationdisplay
j=1lnR(nj
2,λ)}.
Define
f(λ) =λ(c−ln(n1···nq)) +qλln 2 +q/summationdisplay
j=1lnR(nj
2,λ),−nmin
2<λ≤0.
Notice that f(0) = 0. Define digamma function,
ψ(x) =d
dxln(Γ(x)) =Γ′(x)
Γ(x).
Qi et al. (2006) proved the following sharp inequality of digamma function,
ln(x+1
2)−1
x<ψ(x)<ln(x+e−γ)−1
x,x> 0,
whereγis the Euler-Mascheroni constant, and e−γ≈0.561459.
Thus,
f′(λ) =c+q/summationdisplay
j=1/bracketleftig
−ln(nj
2) +ψ(nj
2+λ)/bracketrightig
≥c+q/summationdisplay
j=1ln(1 +λ+ 1/2
nj/2)−q/summationdisplay
j=11
nj/2 +λ.
Since ln(1 +x)is concave, we have
ln(1 +x)≥2 ln(2)x,x∈[−1/2,0].
If−nmin
4≤λ≤0, then
f(λ) =f(0)−/integraldisplay0
λf′(x)dx
≤cλ+/integraldisplayλ
0
q/summationdisplay
j=1ln(1 +x+ 1/2
nj/2)−q/summationdisplay
j=11
nj/2 +x
dx
=cλ+q/summationdisplay
j=1/bracketleftbigg
λln(1 +λ+ 1/2
nj/2) + (nj/2 + 1/2) ln(1 +λ
nj/2 + 1/2)−λ−ln(1 +λ
nj/2)/bracketrightbigg
≤cλ+q/summationdisplay
j=1(λ−1) ln(1 +λ
nj/2)
≤cλ+ 4 ln(2)λ(λ−1)∆ 1(q).
Assume 0<c≤3 ln 2. LetA= 12 ln 2, andλ∗=−c
A∆1(q). Sincenmin∆1(q)≥1, we haveλ∗≥−nmin/4.
Assume ∆1(q)≤c/(12 ln 2).
Thus
f(λ∗)≤−c2
A∆1(q)+ 4 ln 2c2
A∆1(q)/parenleftbigg∆1(q)
c+1
A/parenrightbigg
≤−c2
36 ln(2)∆ 1(q).
Thus, we obtain (49). □
Lemma 11 There exists a positive constant C(c1,c2)which only depends on c1,c2, such that if nN∆1≤
C(c1,c2), then for any fixed 1<i≤N, with probability at least 1−exp/braceleftig
−Ω/parenleftig
1
∆1/parenrightig/bracerightig
we have
σmax(WN:i(0))≤ec1(ni−1ni···nN−1)1/2,
and
σmin(WN:i(0))≥e−c2(ni−1ni···nN−1)1/2.
29Published in Transactions on Machine Learning Research (07/2024)
Proof of Lemma 11 LetA=WT
N:i(0). We know that
σmax(A) =∥A∥= sup
v∈SnN−1∥Av∥2
and
σmin(A) = inf
v∈SnN−1∥Av∥2.
Applying lemma 10, we know that with probability at least 1−exp/braceleftig
−Ω/parenleftig
1
∆1/parenrightig/bracerightig
,
∥Av∥2/∥v∥2∈[e−c2/2P,ec1/2P],
whereP= (ni−1···nN−1)1/2.
Setϕ= min{1−e−c1/2,(e−c2/2−e−c2)/(e−c2/2+ec1)}. Take aϕ-netNϕforSnN−1with size|Nϕ|≤(3/ϕ)nN.
Notice that with this size we can actually cover the unit ball, not only the unit sphere.
Thus, with probability at least 1−|Nϕ|exp/braceleftig
−Ω/parenleftig
1
∆1/parenrightig/bracerightig
, for allu∈Nϕsimultaneously we have
∥Au∥2/∥u∥2∈[e−c2/2P,ec1/2P].
Fixedv∈SnN−1, there exists u∈Nϕsuch that∥u−v∥2≤ϕ. WLOG, we assume 1−ϕ≤∥u∥2≤1. We
obtain
∥Av∥2≤∥Au∥2+∥A(u−v)∥2≤ec1/2P+ϕ∥A∥.
Taking supereme over ∥v∥2= 1, we obtain
σmax(A) =∥A∥≤ec1/2
1−ϕP≤ec1P.
For the lower bound, we have
∥Av∥2≥∥Au∥2−∥A(u−v)∥2≥e−c2/2P∥u∥−ϕ∥A∥≥/bracketleftig
(1−ϕ)e−c2/2−ϕec1/bracketrightig
P≥e−c2P.
Taking the infimum over ∥v∥2= 1, we get
σmin(A)≥e−c2P.
The conclusions hold with probability at least
1−|Nϕ|exp/braceleftbigg
−Ω/parenleftbigg1
∆1/parenrightbigg/bracerightbigg
≥1−exp{nNln(3/ϕ)}exp/braceleftbigg
−Ω/parenleftbigg1
∆1/parenrightbigg/bracerightbigg
≥1−exp/braceleftbigg
−Ω/parenleftbigg1
∆1/parenrightbigg/bracerightbigg
,
sincenN∆1≤C(c1,c2). □
Lemma 12 There exists a positive constant C(c1,c2)which only depends on c1,c2, such that if
rank (X)∆1≤C(c1,c2), then for any fixed 1≤j < N, with probability at least 1−exp{−Ω/parenleftig
1
∆1/parenrightig
}we
have
σmax(Wj:1(0)|R(X))≤ec1(n1n2···nj)1/2,
and
σmin(Wj:1(0)|R(X))≥e−c2(n1n2···nj)1/2.
30Published in Transactions on Machine Learning Research (07/2024)
Proof of Lemma 12 The proof is similar to that of previous lemma. The only difference is that now we
consider the ϕ−net to cover the unit sphere in R(X)∩Rn0, with dimR(X)∩Rn0=rank (X), whereR(X)
represents the column space of X. □
Lemma 13 SetC=nmax/nmin<∞,θ= 1/2. Assume Ω(1/∆1)≥k
∆1, where 0<k< 1is a constant and
∆1satisfies

∆1≤min/braceleftig
k
5 ln(6),k
5 ln(5 ln(6)e/k)/bracerightig
∆1ln(C)≤min/braceleftig
k
5 ln(5 ln(6)e/k),k
5/bracerightig
∆1ln(N2θ)≤k/5.
Given 1<i≤j <N, with probability at least 1−2e−k/(5∆1)= 1−e−Ω(1/∆1)we have
∥Wj:i(0)∥≤Mk√
CNθ(ni···nj−1·max{ni−1,nj})1/2,
whereMkis a positive constant that only depends on k.
Proof of Lemma 13 WLOG, assume ni−1≤nj. LetA=Wj:i(0). From lemma 10, we know that fixed
v∈Sni−1−1, with probability at least 1−e−Ω(1/∆1)we have∥Av∥2≤4/3(ni···nj)1/2. .
Take a small constant c=kN2θ
5 ln(6)∆ 1ni−1≥k
5 ln(6)C. Letv1,···,vni−1be an orthonormal basis for Rni−1.
Partition the index set {1,2,···,vni−1}=S1∪S2∪···∪S⌈N2θ/c⌉, where|Sl|≤ ⌈cni−1/N2θ⌉for each
1≤l≤⌈N2θ/c⌉.
The following discussion is similar to the proof of lemma 11, hence we omit some details. For each l, taking
a1/2−netNlfor the setVSl={v∈Sni−1−1;v∈span{vi;i∈Sl}}, we can get
∥Au∥2≤4(ni···nj)1/2,u∈VSl,
with probability at least
1−|Nl|e−k/∆1≥1−exp{−k/∆1+ (cni−1/N+ 1) ln 6}≥1−e−3k/(5∆1),
since ∆1≤k
5 ln(6).
Therefore, for any v∈Rni−1, we can write it as the sum v=/summationtext
lalvl, whereαl∈Randvl∈VSlfor eachl.
We also know that ∥v∥2
2=/summationtext
l≥1|αl|2.
Then we have
∥Av∥2≤/summationdisplay
l|αl|∥Avl∥2≤4(ni···nj)1/2/radicaligg
⌈N2θ/c⌉/summationdisplay
l|al|2≤Mk√
CNθ(ni···nj)1/2∥v∥2.
Thus,
∥A∥≤Mk√
CNθ(ni···nj)1/2.
Notice that when C≤e,∆1≤k
5 ln(5 ln(6)e/k)≤k
5 ln(5 ln(6)·C/k),and whenC >e, we have
∆1ln(C)≤min/braceleftbiggk
5 ln(5 ln(6)e/k),k/5/bracerightbigg
≤kln(C)
5 ln(5 ln(6)·C/k).
The success probability is at least
1−⌈N2θ/c⌉·e−3k/(5∆1)
≥1−exp/braceleftbigg
ln/parenleftbigg5 ln(6)·C
k/parenrightbigg
+ ln(N2θ)−3k/(5∆ 1)/bracerightbigg
−e−3k/(5∆1)
≥1−2e−k/(5∆1),
since
∆1≤k
5 ln (5 ln(6)·C/k)and∆1ln(N2θ)≤k/5.
□
31Published in Transactions on Machine Learning Research (07/2024)
Proof of Theorem 4 The requirement on size {n1,n2,···,nN−1,N}in (23) makes sure that lemma 11,
12, 13, 2, and 3 hold.
WLOG, we set c1=c/6,c2=c/3,M= 2Mk√C0,B0=Bδ,andη=:(1−ε)2nN
e2cβN, then with probability at
least
1−N2e−Ω(1/∆1)−δ/2≥1−δ,since ∆1≤1
C(c)min/braceleftbigg1
lnN,1
ln(1/δ)/bracerightbigg
,
therandominitializationsatisfiestheinitializationassumption(14)andtheoverparameterizationassumption
(15). Applying Lemma 3, we complete the proof. □
F Orthogonal Initialization Fall into the Convergence Region
There are some basic facts for random projections and embeddings. Most of the following properties can be
found in Eaton (1989).
Proposition 3
1.Ais a random embedding if and only if ATis a random projection.
2. IfAis a square matrix, then random projection, random embedding and random orthogonal matrix
are equivalent.
3. The uniform distribution on the group is a left and right invariant probability measure, that is, if
Ais a random orthogonal matrix, then A,UA,AU are all random orthogonal matrix, where Uis a
non-random orthogonal matrix.
4. Assume Xis an×q(q≤n)random matrix whose entries are i.i.d. N(0,1)random variables.
ThenA:=X(XTX)−1/2is a random embedding, since ATA=Iqand the distribution of Ais
left invariant, which means that AandUAhave the same distribution, where Uis a non-random
orthogonal matrix.
5. IfAis a uniform distribution over an orthogonal group of order nandAis partitioned as A=
(A1,A2), whereA1isn×qandA2isn×(n−q), thenAT
1andAT
2are both random orthogonal
matrix.
6. The columns of uniform distribution over orthogonal group of order n, and
(ξ1,···,ξn)/radicalbig
ξ2
1+ξ2
2+···+ξ2n
have the same distribution, where ξ1,···,ξnare i.i.d.N(0,1)random variables.
7. Assume A=An×p,n≤pis a random orthogonal projection. For any v∈Sp−1,∥Av∥2
2and
(/summationtextn
i=1ξ2
i)/(/summationtextp
j=1ξ2
j)are both following beta distribution with α=n/2,β= (p−n)/2, where
ξ1,···,ξnare i.i.d.N(0,1)random variables.
Remark 9 There are several ways to construct random matrix A= (aij)q×n, q≤n, which is uniformly
distributed over rectangular matrices with AAT=c2Iq,c > 0. LetOnbe uniformly distributed over real
orthogonal group of order n, and Onis partitioned as On= (AT
1,AT
2)T, whereA1isq×n. AssumeX=
(xij)q×n, andxijare independent standard normal random variables. Then A,cA1, andc(XXT)−1/2X
have the same distribution.
Lemma 14 For anyx∈Rn0, the one peak random projections and embedding initiation satisfies
E/bracketleftig
∥aNWN:1(0)x∥2
2/bracketrightig
=∥x∥2
2.
32Published in Transactions on Machine Learning Research (07/2024)
Proof of Lemma 14 LetD=Wp:1(0)/√n1n2···np. ThenDis an embedding matrix. Thus, ∥Dx∥2
2=
∥x∥2
2. LetAi=Wi:p+1(0)/√npnp+1···ni−1, wherei≥p+ 1, andAp=I.
SetBi=∥AiDx∥2
2/∥Ai−1Dx∥2
2,i≥p+ 1. Then,Bifollows beta distribution B(ni/2,(ni−1−ni)/2)given
Bi−1,Bi−2,···,Bp+1,i≥p+ 1. Ifni=ni−1, thenBi|(Bi−1,Bi−2,···,Bp+1) = 1, a.s.
IfB∼B(a,b), then the expectation is given by the following equation,
EB=a
a+b.
Thus, by law of total expectation, we have
nN
npE∥aNWN:1(0)x∥2
2=E∥ANDx∥2
2=EBNBN−1···Bp+1∥Dx∥2
2=nN
np∥x∥2
2.
This completes the proof. □
Next, we introduce sub-Gaussian random variables, associated with bounds on how a random variables
deviate their expected value.
Definition 4 A random variable Xwith finite mean µ=EXis sub-Gaussian if there is a positive number
σsuch that:
E[exp(λ(X−µ))]≤exp/parenleftbiggλ2σ2
2/parenrightbigg
for allλ∈R
Such a constant σ2is called a proxy variance, and we say that Xisσ2-sub-Gaussian, and we write X∼
SG(σ2).
Example 3 Normal distribution N(µ,σ2)of course is σ2sub-Gaussian.
For beta distribution, Elder (2016) showed that B(a,b)is1
4(a+b)+2-sub-Gaussian and later, Marchal & Arbel
(2017) concluded1
4(a+b+1)-sub-Gaussian.
The Hoeffding bound for random variable Xwith mean µand sub-Gaussian parameter σis given by,
P[|X−µ|≥t]≤2 exp/braceleftbigg
−t2
2σ2/bracerightbigg
,∀t≥0. (51)
Simply applying the Chernoff bound for B(a,b), we obtain the following lemma.
Lemma 15 Assume random variable Bdistributed as beta distribution B(a,b)with two positive shape pa-
rametersaandb. Then
P(/vextendsingle/vextendsingle/vextendsingle/vextendsingleB−a
a+b/vextendsingle/vextendsingle/vextendsingle/vextendsingle≥y)≤2 exp/braceleftbig
−2(a+b)y2/bracerightbig
,y≥0.
Hence,
P/parenleftbigg/vextendsingle/vextendsingle/vextendsingle/vextendsingleB−a
a+b/vextendsingle/vextendsingle/vextendsingle/vextendsingle≤εa
a+b/parenrightbigg
≥1−exp{−Ω(a2/(a+b))},
where Ω(·)only depend on ε.
For the upper tail, we can obtain a better bound,
P/parenleftbigg
B≥(1 +ε)a
a+b/parenrightbigg
≤exp{−(ε−ln(ε+ 1))a}. (52)
Proof of Lemma 15 We only need to prove the third inequality. Assume random variable B∼B(a,b).
Setv=a+b,(1 +t)a
v≤y<1,t> 0, andr>0.
We are going to estimate the Chernoff bound for B, which is
P(B≥y)≤e−(ry−lnEerB)=:e−Ir(y).
33Published in Transactions on Machine Learning Research (07/2024)
The moment generating function of Bis given by
EerB= 1 +∞/summationdisplay
k=1a(a+ 1)···(a+k−1)
v(v+ 1)···(v+k−1)rk
k!≤1 +∞/summationdisplay
k=1a(a+ 1)···(a+k−1)
vkrk
k!,r> 0.
Recall that the Maclaurin series of (1−r/v)−aover (−v,v), is given by equation
(1−r/v)−a= 1 +∞/summationdisplay
k=1a(a+ 1)···(a+k−1)
vkrk
k!.
Thus,
Ir(y) =ry−lnEerB≥ry+aln(1−r/v).
Setr=v−a/y∈(0,v). We obtain
P(B≥y)≤exp{−(vy−a+aln(a/(vy)))}=: exp{−vy·g(a/(vy))},(1 +t)a
v≤y<1
whereg(x) = 1−x+xln(x),x=a/(vy)∈(0,1/(1 +t)]. Notice that g(1) = 0andg′(x) = ln(x)<0over
x∈(0,1).
We know that
g(x)≥g(1/(1 +t)) =t−ln(1 +t)
t+ 1,t> 0.
Thus,
P(B≥y)≤exp/braceleftbigg
−vy·t−ln(1 +t)
t+ 1/bracerightbigg
= exp{−(t−ln(1 +t))a},y= (1 +t)a
v<1.
Sety= (1 +ε)a
a+b. We obtain the inequality (52). □
Remark 10 It is trivial to check
∥Wj:i(0)∥= (nini+1···nj)1/2,1≤i≤j≤p,
∥Wj:i(0)∥= (ni−1ni···nj−1)1/2,p+ 1≤i≤j≤N,
∥Wj:i(0)∥≤(nini+1···nj−1)1/2(np)1/2
≤/parenleftbiggnmax
nmin/parenrightbigg1/2
(nini+1···nj−1·max{ni−1,nj})1/2,1≤i<p<j≤N,(i,j)̸= (1,N).
Remark 11 As a special case, if n1=n2=···=nN−1=n, we know that ∥Wj:i(0)∥=
(ni−1ni···nN−1)1/2=n(N−i+1)/2.
Lemma 16 Assumenp/min{n1,nN−1}≤C0<∞. Setε>0. LetC(ε)represent the constant depend only
onε. Ifn1/C0≥C(ε)nN, then with probability at least 1−e−Ω(n1/C0)
σmax(WN:i(0))≤(1 +ε)(ni−1ni···nN−1)1/2,2≤i≤p
σmin(WN:i(0))≥(1−ε)(ni−1ni···nN−1)1/2,2≤i≤p.
Similarly, if nN−1/C0≥C(ε)rank (X), then with probability at least 1−e−Ω(nN−1/C0)
σmax(Wj:1(0)|R(X))≤(1 +ε)(n1n2···nj)1/2,p+ 1≤j≤N
σmin(Wj:1(0)|R(X))≥(1−ε)(n1n2···nj)1/2,p+ 1≤j≤N.
34Published in Transactions on Machine Learning Research (07/2024)
Proof of Lemma 16 LetD= (nN−1nN−2···np)−1/2WT
N:p+1(0)and
Ai= (npnp−1···ni)−1/2WT
p:i(0). Assumev∈SnN−1. Easy to see that Aiis a product of random orthogonal
projections and Dis a random embedding.
Lete1= (1,0,0,···,0)T∈Rnp.There exists orthogonal matrix Tsuch thatTDv =e1,∥e1∥2=∥TDv∥2=
∥v∥2= 1.
Since random orthogonal projections are right invariant, we have
P(∥AiDv∥2≥y) =E/bracketleftig
E/parenleftig
I{∥AiTTe1∥2≥y}/vextendsingle/vextendsingle/vextendsingleD/parenrightig/bracketrightig
=E/bracketleftbig
E/parenleftbig
I{∥Aie1∥2≥y}/vextendsingle/vextendsingleD/parenrightbig/bracketrightbig
=P(∥Aie1∥2≥y).
This proves that ∥AiDv∥2
2and∥Aie1∥2
2have the same distribution.
Claim: Ifv̸= 0, then∥AiDv∥2
2/∥v∥2
2=/vextenddouble/vextenddouble(nini+1···n2
p···nN−1)−1/2WT
N:iv/vextenddouble/vextenddouble2
2/∥v∥2
2follows beta distribu-
tionB(ni−1/2,(np−ni−1)/2).
DefineBp=∥Ape1∥2
2,Bi=∥Aie1∥2
2/∥Ai+1e1∥2
2,i=p−1,p−2,···,1.
ThenBp∼B(np−1/2,(np−np−1)/2),Bp−1|Bp∼B(np−2/2,(np−1−np−2)/2),···,Bi|(Bp,···,Bi+1)∼
B(ni−1/2,(ni−ni−1)/2).
Ifni+1=ni, we know that Bi|(Bp,···,Bi+1) = 1,a.s.
IfB∼B(a,b), then the moments are given by the following equations,
EB=a
a+b,andEBk=a
a+ba+ 1
a+b+ 1···a+k−1
a+b+k−1.
By law of total expectation, we have
EBiBi+1···Bp=ni−1
nini
ni+1···np−1
np=ni−1
np,
as well as
E(BiBi+1···Bp)k=ni−1/2
np/2ni−1/2 + 1
np/2 + 1···ni−1/2 +k−1
np/2 +k−1.
Notice that all integer moments of BiBi+1···Bpmatch those of B(ni−1/2,(np−ni−1)/2). We can verify that
betadistributionsatisfiesCarleman’scondition,whichimpliesthat BiBi+1···Bp∼B(ni−1/2,(np−ni−1)/2).
Thus,∥AiDv∥2
2/∥v∥2
2∼B(ni−1/2,(np−ni−1)/2), which proves the claim.
With probability at least 1−exp{−Ω(n1/C0)}, we have
(1−ε)2ni−1
np≤∥ADv∥2
2≤(1 +ε)2ni−1
np,∥v∥2= 1.
Using theϕ−net technique which has already been used to prove lemma 11, we know that
σmin(AD)≥(1−ε)/parenleftbiggni−1
np/parenrightbigg1/2
,
and
σmax(AD)≤(1 +ε)/parenleftbiggni−1
np/parenrightbigg1/2
,
with probability at least 1−exp{nNln(3/ϕ(ε))}exp{−Ω(n1/C0)}≥ 1−exp{−Ω(n1/C0), sincen1/C0≥
C(ε)nN, for 2≤i≤p.
Hence, with probability at least 1−e−Ω(n1/C0), we have
σmin(WN:i(0))≥(1−ε) (ni−1···nN−1)1/2,
and
σmax(WN:i(0))≤(1 +ε) (ni−1···nN−1)1/2.
The other part of the proof is similar to that of lemma 12, so we omit it.
□
35Published in Transactions on Machine Learning Research (07/2024)
Proof of Theorem 5 Setc > 0,c1=c/6,c2=c/3. In lemma 16, we can pick a ε > 0, such that
1 +ε≤ec1/2and1−ε≥e−c2/2. SetM= 2√C0,θ= 0,B0=Bδ, andη=(1−ε)2nN
e2cβN.
The requirement on size {n1,n2,···,nN−1,N}in (24) make sure that the remark 10, lemma 16, lemma 2,
and lemma 3 all hold.
Notice that even though we need the conclusions in lemma 16 simultaneously hold for 2≤i≤p,p+ 1≤
j≤N, it suffices to apply lemma 16 over i∈Iandj∈J, such that{ni;i∈I}and{nj;j∈I}both have
distinct values. Since |I|≤Nand|J|≤N, with probability at least
1−2Ne−Ω(nmin/C0)−δ/2≥1−δ,
the one peak random orthogonal projections and embeddings initialization satisfies the initialization assump-
tion (14) and the overparameterization assumption (15).
Under assumption n1=n2=···=nN−1, we can use remark 11 to replace lemma 16. Thus, with probability
at least 1−δ/2≥1−δ, (14) holds. Applying lemma 2 and 3, we complete the proof. □
Proof of Theorem 6 LetWN(0) =√nUN[Iny,0]VT
N,···,Wi(0) =√nUiInVT
i,2≤i≤N−1, and
W1(0) =√nU1[Inx,0]TVT
1. Now, we want to verify (14). By simply calculation, we have


σmax(WN:i+1(0)) =σmin(WN:i+1(0)) =n(N−i)/2,1≤i≤N−1,
σmax(Wi−1:1(0)|R(X)) =σmax(Wi−1:1(0)|R(X)) =n(i−1)/2,2≤i≤N,
∥Wj:i(0)∥=n(j−i+1)/2,1<i≤j <N.
Notice that for any 1≤p≤m
∥aNWN:1(0)x∥2
2=n
nN/vextenddouble/vextenddoubleUN[Iny,0]VT
NUN[Inx,0]TVT
1x/vextenddouble/vextenddouble2
2=n
nN/vextenddouble/vextenddoubleUN[Iny,0]VT
Nx′/vextenddouble/vextenddouble2
2,
wherex′=UN[Inx,0]TVT
1x,∥x∥2=∥x′∥2.
Since the distribution of UN[Iny,0]VT
Nis right invariant under multiplying orthogonal matrices, we have
/vextenddouble/vextenddoubleUN[Iny,0]VT
Nx′/vextenddouble/vextenddouble2
2/∥x∥2
2∼B(ny
2,n−ny
2).
Thus,
E/bracketleftig
∥aNWN:1(0)x∥2
2/bracketrightig
=∥x∥2
2.
Applying lemma 2, we have
L0−L(W∗)≤β/parenleftbigg2·rank (X)
δ+∥W∗∥2
X/parenrightbigg
,
with probability at least 1−δ/2.
Applying Lemma 3 with c>0,c1=c/6,c2=c/3,θ= 0, we complete the proof. □
Proof of Theorem 1 Theorem 1 is a special case of Theorem 4 and Theorem 5. Hence, we omit the
proof. □
Proof of Theorem 2 In Theorem 4, 5, and 6, we proved that for given constant c1,c2>0and0<
ε,δ/2<1/2as well as learning rate η, there exists constant C=C(c1,c2)such that all three kinds of
random initializations will fall into the convergence region defined in Section 4.1 with probability at least
1−δ.
This implies that with probability at least 1−δ, we obtain (16) by Lemma 4. Applying Lemma 6, we
complete the proof. □
36Published in Transactions on Machine Learning Research (07/2024)
G Numerical Experiments
We will discuss some empirical evidence to support the main results in Section 3. We aim to show how the
trajectories of the non-convex deep linear neural networks are related to a convex optimization problem for
GD under different initialization schemes. Consider the following procedures for plots of the logarithm of
loss as a function of number of iterations:
a) We choose X∈R128×1000andW∗∈R10×128and setY=W∗X+ε, where the entries in X,W∗and
εare drawn i.i.d. from N(0,1).
b) We consider the loss function1
2∥aNWN:1X−Y∥2
F.
c) For the given linear networks, we apply the Gaussian initialization and the one peak random or-
thogonal projections and embeddings initialization, which are denoted as Wj(0),1≤j≤N= 3.
d) For the convex optimization problem (1), we set the initialization to be W(0) =aNWN(0)···W1(0).
e) We set the learning rate η=nN
N·∥X∥2andη∗=N
nNηfor the deep linear neural networks and the
convex optimization problem, respectively.
f) We draw the loss function through 25iterations.
Figure 1 compare the trajectories of the logarithm of loss for gradient descent in deep linear networks and
the corresponding convex optimization problem. The left panel shows the comparison with Gaussian initial-
ization, while the right panel shows the comparison with orthogonal initialization, both without averaging.
Recall that the main theorems in Section 3 require nmin≥max{n0,nN}, and thus, in the simulation, we
requirenmin≥128. Thus, for the top panels, the minimal width of the hidden layers nmin= 128is small,
and the trajectories of loss for deep linear networks exhibit non-monotonic behavior, with the loss increasing
in some iterations. However, this does not contradict the main theorems in Section 3, because the minimal
width of the hidden layers is insufficient.
As the minimal width of the hidden layers increases, the trajectories of loss for deep linear networks become
increasingly similar to those of the corresponding convex optimization problem. This suggests that increasing
the minimal width of the hidden layers helps to stabilize the optimization process and makes it more closely
resemble the convex counterpart, which is consistent with Theorem 2.
The choice of initialization scheme, whether Gaussian or orthogonal, does not significantly impact the overall
convergence behavior and the relationship between the trajectories of deep linear networks and the convex
optimization problem, when the minimal width of the hidden layers is large enough.
37Published in Transactions on Machine Learning Research (07/2024)
Figure 1: Plot of logarithm of loss as a function of number of iterations with n1=n2=n3= 128(Top),
200(Middle), 2000(Bottom) for Gaussian initialization (left panel) and n1=n2=n3= 128(Top), 200
(Middle), 5000(Bottom) for Orthogonal initialization (right panel), respectively.
38