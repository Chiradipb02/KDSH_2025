Published in Transactions on Machine Learning Research (10/2024)
Nonlinear Behaviour of Critical Points for a Simple Neural
Network
G. Welper gerrit.welper@ucf.edu
Department of Mathematics
University of Central Florida
Orlando, FL 32816, USA
Reviewed on OpenReview: https: // openreview. net/ forum? id= wfdG2PEOHS
Abstract
In severely over-parametrized regimes, neural network optimization can be analyzed by lin-
earizationtechniquesastheneuraltangentkernel, whichshowsgradientdescentconvergence
to zero training error, and landscape analysis, which shows that all local minima are global
minima. Practical networks are often much less over-parametrized, and training behaviour
becomes more nuanced and nonlinear. This paper contains a fine grained analysis of the
nonlinearity for a simple shallow network in one dimension. We show that the networks
have unfavourable critical points, which can be mitigated by sufficiently high local resolu-
tion. Given this resolution, all critical points satisfy L2loss bounds of optimal adaptive
approximation in Sobolev and Besov spaces on convex and concave subdomains of the tar-
get function. These bounds cannot be matched by linear approximation methods and show
nonlinear and global behaviour of the critical point’s inner weights.
1 Introduction
In this paper, we analyze nonlinear aspects of neural network training for a simple model problem in su-
pervised learning: For samples xiand datayi=f(xi)generated by some unknown target function f, find
a neural network fθwith weights θby minimizing the least squares loss. To motivate the results, we first
review some common approaches in the literature.
Landscape Analysis Gradient descent can easily get stuck in local minima. That this fact does not
harm neural network training is the purview of landscape analysis. It aims to demonstrate that either
the loss has no local minima, in favour of saddle points, or all local minima have small loss value and
therefore provide good trained networks. Indeed, the papers Soudry & Carmon (2016); Kawaguchi (2016);
Nguyen & Hein (2017); Ge et al. (2018); Du & Lee (2018); Soltanolkotabi et al. (2019); Venturi et al. (2019);
Kawaguchi et al. (2019); Kawaguchi & Huang (2019) show that local minima are global minima, either under
strong assumptions, or over-parametrization with more network width than number of samples. Absent such
assumptions, one needs to be more careful, e.g. the papers Swirszcz et al. (2017); Safran & Shamir (2018);
He et al. (2020); Ding et al. (2022); Jentzen & Riekert (2024), find local minima that are not global. A more
fine grained analysis of the landscape is included in He et al. (2020), which finds valleys of path connected
local minima.
Since these results are mixed, matching local and global minima may be to strong a goal and on may be
content with a simpler question:
(Q1) Do critical points have favorable properties and what are these?
To address this question, first note that ultimately we are not interested in a good training error, but rather
in a good generalization error infθ∥fθ−f∥2
L2(P)for some probability measure Pthat generates the input
1Published in Transactions on Machine Learning Research (10/2024)
samplesxi. In general, it is difficult to understand the exact nature of the global optimum, but it is much
more feasible to understand upper bounds of the form
inf
θ∥fθ−f∥≲n(θ)−r, f∈K, (1)
wheren(θ)is an indicator for the network size, like width, depth or total number of weights and r >0an
asymptotic rate. Similar to the no-free-lunch theorem, such bounds cannot work for arbitrary f, which is
why we restrict them to some compact set K. Typically, it bounds Sobolev, Besov or Barron norms or other
smoothness properties of the permissible targets f. Inequalities of type (1) are common in approximation
theory and have been studied extensively for neural networks. A literature overview is given later in the
introduction.
We use this perspective to ease the characterization of local minima. If they do not match the global
minimum, can they match their scaling behaviour
∥fθ−f∥≲n(θ)−r, f∈K, θis a critical point of the training loss ? (2)
Such results are well established for partial differential equations (PDEs), where fθis a nonlinear approxi-
mation method like adaptive finite elements or wavelets and fthe solution of a PDE Cohen et al. (2002);
Morin et al. (2002); Binev et al. (2004). Similar results also exits for shallow neural networks, when trained
with greedy algorithms Siegel & Xu (2022b); Siegel et al. (2023) instead of gradient descent.
Linearization Arguments In over-parametrized regimes, typically with more network width than train-
ingsamples, gradientdescenttrainingdoesnotmovethenetworkweightsfarfromtheirrandominitialization.
As a result, one can obtain accurate descriptions of the training dynamics by linearising the network at the
initial value. Careful analysis then provides exponential gradient descent convergence to zero training loss.
A common representative of this approach is the neural tangent kernel (NTK) introduced in Jacot et al.
(2018); Li & Liang (2018); Allen-Zhu et al. (2019); Du et al. (2019b;a), and refined in Zou et al. (2020); Arora
et al. (2019a;b); Su & Yang (2019); Lee et al. (2019); Song & Yang (2019); Zou & Gu (2019); Kawaguchi
& Huang (2019); Chizat et al. (2019); Oymak & Soltanolkotabi (2020); Ji & Telgarsky (2020); Nguyen &
Mondelli (2020); Bai & Lee (2020); Cao & Gu (2020); Chen et al. (2021); Song et al. (2021); Lee et al. (2022);
Gentile & Welper (2022); Welper (2024b;a); Keene & Welper (2024).
Contrary to this analysis, much of the promise of neural networks relies on their severe non-linearity, leading
to e.g. high expressivity and excellent function approximation properties, even in high dimensions. Can
these be exploited by gradient descent training? If we consider less over-parametrization, or even slightly
under-parametrized regimes, the weights can move farther from their initial and break the linear dominance
in the training dynamics. Empirical studies Vyas et al. (2023) (see also Lee et al. (2020); Seleznova &
Kutyniok (2022)) on image classification datasets show that in such regimes networks perform better than
extremely wide networks with dominantly linear behaviour. A theoretical understanding of these regimes is
still largely unknown. This leads to a second question:
(Q2) Can training in under-parametrized or only slightly over-parametrized regimes exploit the nonlinear
nature of neural networks?
To this end, it is instructive to look at classical approximation methods, where fθis replaced by e.g. splines,
finite elements or wavelets. These depend nonlinearly on θif adaptivity is used and linearly if not. The non-
linear variations strictly include the linear ones so that infθ∥fnonlinear
θ−f∥≤infθ∥flinear
θ−f∥. Nonetheless,
in the upper error bounds (1) this does neither change the number of degrees of freedom (weights) n(θ)nor
the (maximal) rate r. It does change, however, the size of the compact sets Klinear⊂Knonlinearfor which
the given rates can be achieved, with the latter being significantly larger.
Insummary, ifwewanttoestablishapproximationresults(2)forneuralnetworkcriticalpointswithnonlinear
compact sets Knonlinear, we have to carefully exploit the nonlinear nature of the networks and can no longer
rely on vanilla NTK analysis.
2Published in Transactions on Machine Learning Research (10/2024)
New Contributions In this paper, we address questions (Q1) and (Q2) for the very simple model problem
fθ(x) :=m/summationdisplay
r=1wrσ(x−br), (3)
with ReLUactivation, in a one dimensional interval x∈D⊂R, trained on the L2loss∥fθ−f∥L2(D).
This is probably the simplest choice with nonlinear weight dependence (of the br), non-convex loss and fully
understood approximation behaviour both in linear ( bruntrained) and non-linear ( brtrained) cases. The
continuous loss simplifies the analysis and places the problem in an under-parametrized regime, independent
of the width m. Empirical losses, with large numbers of samples, are expected to show similar behaviour by
classical arguments in statistics and machine learning, different from the over-parametrized regime, where
their application is more complicated.
Although this setup may seem simple, it contains two challenges:
1. The problem does have bad local minima.
2. Large compact sets Knonlinearin the approximation bounds (2) cannot be achieved by linear ap-
proximation methods and require careful global placement of the nonlinear inner weights br.
The first result shows this global placement for all critical points of the loss function in the infinite width
limit: If we order the inner weights b0≤···≤bmthe normalized grid size satisfies
limm→∞
x∈[br−1,br]m(br−br−1) =constant|f′′(x)−2/5|, (4)
with possibly a different constant for each interval on which fis strictly convex or concave. The factor
m∼1/his reciprocal to the uniform grid size hand used for normalization. The right hand side shows
that the breakpoints brare close wherever the second derivative f′′(x), and hence the local approximation
difficulty, is large. Generally, this requires global movement of breakpoints brdependent on f, from initial
locations independent of f. For finite m, analogous arguments show that at critical points of the loss the
breakpoints equidistribute the local smoothness
∥f′′∥L2/5([br−1,br])=constant,
again on intervals DIwherefis convex or concave. With standard approximation theory, this leads to
approximation errors of the type
∥fθ−f∥L2(DI)≲|I|−2∥f′′∥L2/5(DI),
where|I|is the number of breakpoints in the respective intervals. To avoid bad local minima, these results
require the critical points to have sufficient local resolution |br−br−1|so thatfdoes not have highly
oscillatory features between breakpoints that are imperceptible to the gradient. The rigorous statements are
in Theorems 3.3 and 3.4 and an example for the conditions is given in Section 5.
The results demonstrate approximation errors (2) on subdomains where fis convex or concave with K:=
K2/5:={f∈L2(D) :∥f′′∥L2/5(D)≤1}. A subtle, but crucial, observation is that f′′is measured in the very
weakL2/5(quasi-) norm (or Besov spaces in Section B.3), which allows us to achieve high approximation
orders for fairly rough functions f. These are not possible for purely linear approximation methods (by
Kolmogorov n-width lower bounds) and therefore demonstrate that finding local critical points of the loss
landscape allows us to exploit some nonlinearity of the neural networks.
Infinite Width Limit Mean field theory of neural networks Chizat & Bach (2018); Mei et al. (2018);
Rotskoff & Vanden-Eijnden (2018); Sirignano & Spiliopoulos (2020) takes the infinite width limit
1
mm/summationdisplay
r=1wrσ(vT
rx)→/integraldisplay
wσ(vTx)dµ(v,w)
for some limiting measure µand then analyzes training of the infinite networks. For comparison, the limits
of the gird size (4) are taken in different order: We first compute the gradient, decouple the computation of
brfromwrand then take the limit afterwards.
3Published in Transactions on Machine Learning Research (10/2024)
Beyond Linearization Some recent papers analyze neural network training beyond the NTK regime. For
example, Damian et al. (2022); Lee et al. (2024) demonstrate results for two layer networks that cannot be
achieved by kernel methods for polynomials g(Ux)that depend only on a few dimension by the inner matrix
U∈Rr×dwithr≪d.
Approximation Universal approximation theorems Cybenko (1989); Hornik et al. (1989); Barron (1993);
Zhou(2020);Luetal.(2017);Hanin&Sellke(2017)showthatneuralnetworkscanapproximateanyfunction
arbitrarily well. Since this is true for virtually all approximation methods in practical use, it is important
to quantify the approximation error more closely. This usually leads to errors bounds of type (1), which are
studied extensively for neural networks. If the compact set Kconsists of functions with bounded Sobolev
or Besov smoothness, results can be found in Gribonval et al. (2022); Gühring et al. (2020); Opschoor et al.
(2020); Li et al. (2019); Suzuki (2019), or for improved rates that surpass classical methods for the price
of discontinuous weight assignments in Yarotsky (2017; 2018); Yarotsky & Zhevnerchuk (2020); Daubechies
et al. (2022); Shen et al. (2019); Lu et al. (2021). Compact sets Kspecifically tailored to neural networks
include Barron and related spaces Bach (2017); Klusowski & Barron (2018); Weinan et al. (2022); Li et al.
(2020);Siegel&Xu(2020;2022a);Bresler&Nagaraj(2020);Parhi&Nowak(2021);Unser(2023). Overviews
are in Pinkus (1999); DeVore et al. (2021); Weinan et al. (2020); Berner et al. (2022).
In the majority of neural network approximation results, the weights are hand-picked and only few papers
show approximation properties of gradient descent trained neural networks Jentzen & Riekert (2022); Ibrag-
imov et al. (2022); Drews & Kohler (2022); Kohler & Krzyzak (2022); Gentile & Welper (2022); Welper
(2024b;a). These heavily rely on the outermost linear layer, or a NTK linearization and therefore show ap-
proximation guarantees only for compact sets Kthat can be well approximated by linear methods. Larger,
nonlinear classes Kcan, to best of our knowledge, so far only be proven for greedy training algorithms Siegel
& Xu (2022b); Siegel et al. (2023) which rely on another non-convex optimization problem in each step.
Notations We usecfor generic constants that can be different in each occurrence, but are independent
offand the network width m. We abbreviate a≤cb,a≥bandca≤b≤cbbya≲b,a≳banda∼b,
respectively. We define [m] :={1,...,m}andPras all polynomials of degree at most r. We use Sobolev
∥·∥Ws,p(D)and Besov∥·∥Bsq(Lp(D)norms with their usual definitions, stated in Section B.1. For any interval
I, we denote the corresponding L2inner product by ⟨·,·⟩I.
2 Approximation By Piecewise Linear Functions
Before we state the main results of the paper, we review relevant approximation properties of the neural
networks. The set of all networks of type 3
Υm:=/braceleftigg
fθ(·) =m/summationdisplay
r=1wrσ(·−br)/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsinglewr,br∈R/bracerightigg
.
corresponds exactly to continuous piecewise linear (CPwL) functions in one dimension
Σm:={f|fis continuous piecewise linear with mbreakpoints},
often referred to as first order free knot splines or finite elements. To discuss the benefits of nonlinearity, we
compare them with the simpler linear class
Σu
m:={f∈Σm|uniform distance between neighbouring breakpoints },
corresponding to networks with untrained inner biases brand hence convex loss. Notice that the lat-
ter set Σu
mis linear, while the former Υm= Σmis nonlinear and hence we refer to them as linearand
nonlinear approximation methods . Their approximation errors are precisely understood:
inf
ϕ∈Σum∥ϕ−f∥L2≲Cm−2|f|B2
2(L2),
inf
ϕ∈Σm∥ϕ−f∥L2≲Cm−2|f|B2
2/5(L2/5).(5)
4Published in Transactions on Machine Learning Research (10/2024)
These correspond exactly to the approximation bound in the introduction if we define Ks,p:={f∈L2(D) :
|f|B2p(Lp)≤1}withp= 2andp= 2/5.
Up to minor differences, the Besov norms |f|Bsp(Lp)≈∥f(s)∥Lpare equivalent to Sobolev norms, which
bound the s-th derivative of finLp. The former are technical, but usually preferred in approximation
theory because they are well behaved for p<1, as used in the nonlinear bound above. For orientation, these
spaces are often arranged as in Figure 1. The sets Ks,pbecome larger with decreasing sand decreasing p.
By Sobolev embedding theorems, one may trade some pfor somesso that all spaces (s,p)above the dashed
line in the figure are contained in L2and thusKs,p⊂K0,2. See Section B.1 for definitions and DeVore &
Lorentz (1993); DeVore (1998) for more details.
1/ps
L2L1B2
2(L2)B2
2/5(L2/5)
Figure 1: Diagram of Besov spaces. Each point (1/p,s)corresponds to one space Bs
p(Lp)fors>0andLp
fors= 0.
Let us compare the linear approximation Σu
mwith the nonlinear approximation Σm. First observe that in
(5) the rate m−2is identical for both methods. Generally, piecewise linear approximation does not achieve
higher rates, even if fadmits more smoothness. However, since ∥·∥L2/5(D)≲∥·∥L2(D), the smoothness
conditions for nonlinear approximation are much weaker. For example fϵ= sigmoid(x/ϵ)has norms
∥f′′
ϵ∥Lp(D)∼ϵ1
p−2,∥f′′
ϵ∥L2(D)∼ϵ−3
2,∥f′′
ϵ∥L2/5(D)∼ϵ1
2. (6)
Indeed, the second derivative is of size ϵ−2in a region [−cϵ,cϵ ]and negligible outside. Thus ∥f′′
ϵ∥Lp(D)≈
ϵ−2∥1∥Lp([−cϵ,cϵ])∼ϵ−2ϵ1
p. Asϵgoes to zero and fϵconverges to a jump function, the L2norm blows up,
whereas the L2/5norm remains bounded. This provides significantly better approximation bounds in (5)
for nonlinear approximation. If we use Besov spaces instead of the second derivative, this extends to the
jump function itself, which can be approximated by nonlinear methods up to order m−2, whereas linear
approximation only achieves order <m−1/2.
While the linear approximation has a fixed number of breakpoints brnear the jump or sharp gradients of
fϵ, the adaptive approximation can allocate more resources where fis complicated. Indeed, algorithms and
proofs for the approximation bounds (5), aim for breakpoints that equidistribute the local errors
∥fθ−f∥L2([br,br−1])=constant for all r
or closely related the local smoothness
∥f′′∥L2/5([br,br−1])=constant for all r. (7)
Finally note that the bounds (5) are sharp in several ways. For example the best possible rate linear
approximation methods can achieve for functions fin the class K2,p⊂K2,2/5with 2<p≤1ism−2+1
p−1
2<
m−2, seeLorentzetal.(1996), Chapter14, Theorem1.1. Therefore, ifwecanfindcriticalpointsoftheneural
network (9) loss that achieves second order m−2error on the class K2/5, it must exploit the nonlinearity of
the network.
5Published in Transactions on Machine Learning Research (10/2024)
3 Main Result
Setup Maybe the simplest neural network with non-convex training objective and fully understood ap-
proximation properties is
FW,V,B (x) :=B0+W0x+m/summationdisplay
r=1Wrσ(Vrx−Br) (8)
in one dimension, which generates the piecewise linear functions Σmof the last section. By the property
σ(ax) =aσ(x)fora≥0of ReLU activations, the separation of WrandVris redundant and we simplify the
network to
fw,b:=b0+w0x+m/summationdisplay
r=1wrσ(x−br),
with appropriately redefined weights. The case Vr<0requires a slight technicality and is considered in the
following lemma. The lemma also shows that critical points of the loss carry over to the simplified network
Lemma 3.1. LetDbe an interval. Assume FW,V,Bis a critical point of the loss minW,V,B∥FW,V,B−f∥2
L2(D).
ThenFW,V,Bis piecewise linear with ¯m≤mbreakpoints
br=Br/Vr, for allrwithWr,Vr̸= 0.
Furthermore, there is a network fw,b(x) =FW,V,B (x)of width ¯mwith the same breakpoints that is a critical
point of the loss minw,b∥fw,b−f∥2
L2(D).
The proof is in Section B.4. Since our main theorems characterize critical points, it suffices to consider the
simplified variant of the networks. Finally, the term b0+w0xcan be generated by two ReLUs σ(x−br), one
with breakpoint at the left boundary and one more left of the domain. Therefore, we drop b0andw0and
arrive at the network
fθ(x) :=m/summationdisplay
r=1wrσ(x−br), (9)
that we consider throughout the paper.
We train the network with loss
ℓ(θ) :=1
2∥fθ−f∥2
L2(D) (10)
on some finite domain D⊂Rand some target function f∈L2(D). This matches the infinite sample limit of
the least squares loss and places us in an under-parametrized regime similar to classical statistics. Although
theReLUactivation has kinks, this loss is strongly differentiable for all weights θ. Indeed, it suffices to
consider the network as a map θ→fθ(·)from parameters to L2(D)functions. This topology is sufficiently
weak to render the map differentiable, unlike the regular pointwise topology, which does not. See Gentile &
Welper (2022) for details.
Cleanup Gradient descent and related methods converge to critical points,
∇ℓ(θ) = 0, (11)
which we examine more closely in the following. To ease the theoretical analysis, we start with some
notational cleanup, which does not alter the actual network. First, we drop inactive neurons with wr= 0.
Second, we join neurons with identical bias brinto one neuron and adjust the outer weights wraccordingly.
Third, we drop neurons for biases broutside of the domain D, except for the largest brleft of the domain,
which influences the left boundary value of fθ. Finally, we add one artificial breakpoint b¯mat the right end
ofD, which does not change fθinsideD, but avoids technicalities. This yields ¯m≤mneurons, which we
reorder according to
b0<···<b¯m (12)
and denote as cleaned critical breakpoints . These define intervals Ir:= [br−1,br]of lengthhr:=|Ir|. We
denote two consecutive intervals by Ir+:=Ir∪Ir+1andhr+:=|Ir+|.
6Published in Transactions on Machine Learning Research (10/2024)
Equidistribution We have seen in the last section that optimal asymptotic approximation rates are
achieved by equidistributing local errors or smoothness via careful placement of the breakpoints br. It is
instructive to start with an informal discussion of the infinite width limit. To this end, we define the grid
size limit
h(x) := limm→∞
x∈Irmhr,
where for every mwe choose the interval Irwith width hrthat contains x. For a uniform grid, we have
grid sizehr=|D|/mand therefore h(x) =mhr=|D|. For non-uniform grids, h(x)measures the ratio hr/h
between the actual local grid size hrand the uniform grid size h=|D|/m, up to a global factor. If the limit
exists,h(x)is given by the following lemma.
Lemma 3.2. Letfbe smooth and for every mletbr,r∈[ ¯m]be cleaned critical breakpoints (11),(12). If
the limith(x)exists, it satisfies
h(x) =cIf′′(x)−2/5,
with possibly a different constant cIon each interval Ifor whichf′′(x)is non-zero.
For finitem, a careful perturbation analysis leads to the first main theorem.
Theorem 3.3. Letθbe a critical point (11), with cleaned breakpoints in ascending order 12. For r,s∈
{2,..., ¯m}, letI={r,r+ 1,...,s}be a set of consecutive neurons with DI:=/uniontext
k∈IIkand
max/braceleftbigg
h1
2−1
p
k+∥f(3)∥Lp(Ik+), h1−1
q
k+∥f(4)∥Lq(Ik+),/bracerightbigg
≤Cmin
x∈Ik+|f′′(x)| (13)
for some 1< q,p≤∞and some sufficiently small constant C > 0independent of fandhk. Then for
l,k∈DIwe have equidistribution
∥f′′∥L2/5(Il)∼∥f′′∥L2/5(Ik).
This is precisely the equidistribution (7) used in the proofs of CPwL approximation bounds (5). We discuss
the result and the major assumption (13) after the approximation theorem below. In short, high oscillations
strictly contained in one interval Irare imperceptible to the gradient and therefore can lead to bad critical
points. The assumption ensures that we have enough breakpoints to fully resolve such oscillations.
Approximation Once we have established equidistribution, approximation results can be obtained along
standard lines.
Theorem 3.4. Letθbe a critical point (11), with cleaned breakpoints in ascending order 12. For r,s∈
{2,..., ¯m}, letI={r,r+ 1,...,s}be a set of consecutive neurons with DI:=/uniontext
k∈IIkand
max/braceleftbigg
h1
2−1
p
k+∥f(3)∥Lp(Ik+), h1−1
q
k+∥f(4)∥Lq(Ik+),/bracerightbigg
≤Cmin
x∈Ik+|f′′(x)| (14)
for some 1<q,p≤∞and some sufficiently small constant C > 0independent of fandhk. Then
∥fθ−f∥L2(DI)≲|I|−2∥f′′∥L2/5(DI). (15)
Discussion Approximation Result: Note that|I|is the number of breakpoints in DIand therefore is
analogous to the number of breakpoints ¯mon the full domain. Therefore, any critical point subject to the
given conditions achieves asymptotically optimal CPwL approximation on subdomains DI⊂Dand can
properly utilize the nonlinearity of the network. If we have multiple subdomains DI1andDI2, the total
allocation of breakpoints on each of these may be suboptimal.
Resolution: The main purpose of assumption 13 is to prevent bad critical points: Oscillations of the target
fthat are strictly contained inside one interval Irdo not change the gradient and can cause unfavorable
critical points. The assumption is satisfied if the network’s local resolution hris sufficiently high to capture
all fine grained features of f. For comparison classical adaptive CPwL approximation methods contain “data
oscillation” terms in their error bounds. See Section 5 for a more careful discussion.
7Published in Transactions on Machine Learning Research (10/2024)
Smoothness: Thefourthordersmoothnessin (13)ishigherthanthesmoothnessintheapproximationbounds
andseemstocontradictthediscussioninSection2. Indeed, itdoesruleoutlimitingcaseslikejumpfunctions.
This is also the reason why we can use Sobolev type norms instead of Besov norms. However, functions with
large gradients as in example (6) are permissible. In this case assumption (13) provides some a-posteriori
bounds on the resolution necessary to achieve equidistribution and adaptive approximation errors. However,
even if this requires large networks none of the constants enters the approximation error itself: As soon as the
networks are big enough, we obtain nonlinear approximation bounds with small constants only dependent
on the favourable ∥f′′∥L2/5smoothness bound.
Convex/Concave: Assumption (13) implicitly entails that fis concave or convex on each subdomain DI.
While it is not clear if this is strictly necessary, longer stretches with f′′(x) = 0seem problematic: In these
fis linear and can be approximated with zero local error. Then any breakpoint in this region has no good
gradient information for its placement.
Left Boundary: The networks are zero fθ(x) = 0for allx < b 0left of the leftmost breakpoint. To avoid
dealing with this boundary condition, we exclude the corresponding interval I1, by requiring s,r≥2in the
definition ofI.
Comparison with NTK Theory: In over-parametrized NTK regimes the weights do not move far from their
initialization during gradient descent training. This implies that uniformly initialized breakpoints brremain
uniform and can generally not equidistribute local errors. Accordingly, the approximation error achieved
after training is bounded by ∥fθ−f∥L2(D)≲m−α∥f∥Bs
2(L2(D)in Gentile & Welper (2022) for some constants
α≥0and0≤s≤1/2. In particular, the smoothness is measured in the L2norm as for uniform CPwL
approximation and not in a suitable larger Lpnorm as for adaptive CPwL approximation.
Inner Weights: The equidistribution results carry over to more standard networks FW,V,B, with inner weights
Vr, as defined in (8). Indeed, by Lemma B.2, the breakpoints of a critical point FW,V,Bmatch the breakpoints
of a critical point fw,b, for which Theorem (3.3) provides equidistribution on convex/concave sub-domains.
4 Proof Idea
This section contains a short overview over the proof. The optimization of the outer weights wris convex
and therefore fairly simple. On the other hand, the optimization of the inner weights bris non-convex and
the main objective of the prove is to demonstrate their equidistribution in Theorem 3.3. Once this property
is established, the approximation Theorem 3.4 follows by standard arguments DeVore (1998). The proof
proceeds in several steps.
1.Critical Points: Define the spaces
X:= span{∂wrfθ|r∈[m]}= span{σ(x−br)|r∈[m]},
˙X:= span{∂brfθ|r∈[m]}= span{wr˙σ(x−br)|r∈[m]}
of the partial derivatives and the residual κ:=fθ−f. Then, by taking linear combinations, it is
easy to see that the critical point conditions
∂θrℓ(θ) =⟨κ,∂θrfθ⟩= 0,
are equivalent to
⟨κ,v⟩= 0, v ∈X+˙X. (16)
2.Eliminatewr:In the critical point conditions the residual κdepends on both wrandbr. To show
equidistribution, which depends on bronly, we eliminate wrfrom the equations. To this end, define
theL2-orthogonal complement space X⊥so that
X+˙X=X⊕X⊥, X ⊥X⊥.
8Published in Transactions on Machine Learning Research (10/2024)
SinceXisthespanofallneurons σ(x−br),wehavefθ∈Xandtherefore⟨κ,v⟩=⟨fθ−f,v⟩=⟨f,v⟩.
for allv∈X⊥by orthogonality. Thus, the critical point condition implies
⟨f,v⟩= 0, v∈X⊥,
This condition does not depend on wrany longer and guarantees equidistribution, as we show in
the following steps of the proof.
3.Characterization of the Complement space X⊥:We construct basis functions
φr(x) =

hr´ϕ′′(x), x∈Ir
−hr+1`ϕ′′(x), x∈Ir+1
0, else
forX⊥supported on two consecutive intervals IrandIr+1. The critical point condition then yields
⟨f,φ⟩= 0and integration by parts
hr⟨f′′,´ϕr⟩Ir=hr+1⟨f′′,`ϕr+1⟩Ir+1. (17)
Since the functions ´ϕand`ϕare non-negative bump functions, the smoothness conditions of the main
theorems imply that
∥f∥L1/2(Ir)∼hr⟨f′′,´ϕr⟩Ir=hr+1⟨f′′,`ϕr+1⟩Ir+1∼∥f∥L1/2(Ir+1). (18)
4.Refined Analysis: While the last two equations provide equidistribution on two neighbouring in-
tervals, they are insufficient: (18) has the wrong norm, L1/2instead ofL2/5, and is too inaccurate
when chaining over large numbers of intervals. (17) cannot be chained directly because the functions
´ϕ̸=`ϕare asymmetric. A more refined analysis of the asymmetry and passing to the limit m→∞
shows that the grid size limit h(x) = limm→∞mhrforx∈Irsatisfies the differential equation
[h2f′′]′=1
5h2f′′′,
where the left hand side originates from (17) and the right hand side from the asymmetry. This
is a first order linear differential equation for h2. Solving it with an integrating factor leads to
[h2(f′′)4/5]′= 0and the extra power 4/5leads to proper grid size limit for L2/5equidistribution.
The main result Theorem 3.3 follows from a perturbation analysis of this ODE for finite hr.
5 Unfavourable Critical Points
5.1 Critical Points with High Oscillations
Recall from (16) that critical points are given by the condition
⟨fθ−f,v⟩= 0, v ∈X+˙X.
To construct unfavourable critical points, we merely need a perturbation φthat is orthogonal to X+˙X.
Thenfθis also a critical point for f+φ:
⟨fθ−(f+φ),v⟩= 0, v ∈X+˙X.
It is easy to construct φso thatfθis a bad approximation. To provide a simple example, let f= 0so that
fθ= 0must also be zero. Now choose two neighbouring breakpoints brandbr+1and define an oscillation
φsupported inside Ir= [br−1,br], with some margin to the boundary and orthogonal to all linear functions
P1. Then, we have φ⊥X+˙Xandfθ= 0is a critical point for approximating 0 +cφwith arbitrarily large
approximation error c∥φ∥. On the other hand, the network fθmay have an arbitrary number of breakpoints
outside ofIr. With optimal placement, they can all be used to approximate φand make the approximation
9Published in Transactions on Machine Learning Research (10/2024)
error arbitrarily small. This simple example is sufficient for our purposes, but the literature contains many
more Swirszcz et al. (2017); Safran & Shamir (2018); He et al. (2020); Ding et al. (2022); Jentzen & Riekert
(2024).
Note that by construction any small perturbation of the outer weights wror the breakpoints brdoes not
change the loss for approximating 0+cφso that the large error is imperceptible to gradient based optimizers.
In the main theorems, assumption (13) ensures that the network has sufficient resolution so that the target
fcannot have any severe sub-grid oscillations. For comparison, classical adaptive CPwL approximation
algorithms use error indicators to steer the approximation towards error equidistribution. Theoretical results
then include extra “data oscillation” error terms to capture sub-grid oscillations Cohen et al. (2002); Binev
et al. (2004); Morin et al. (2002).
5.2 Avoiding Critical Points with High Oscillations
This section provides an informal motivation how assumption (13) rules out bad local minima with sub-
grid oscillations. To this end, first note that in case the target fis a simple function, like e.g. a second
degree polynomial, it clearly cannot oscillate as φin the counter example. Of course this is too strong an
assumption, but what if fis close to a second order polynomial? Since second order approximation converges
faster than first order approximation, for sufficiently small intervals Ir= [br−1,br], we can expect
inf
p2∈P2∥f−p2∥L2(Ir)≤Cinf
p1∈P1∥f−p1∥L2(Ir)≤C∥f−fθ∥L2(Ir) (19)
for some arbitrarily small constant C, with the last implied by the linearity of fθonIr. Denoting the
minimizer on the left by p2,r, with the triangle inequality, it is easy to show (see e.g. (36)) that
(1 +C)−1∥p2,r−fθ∥L2(Ir)≤∥f−fθ∥L2(Ir).(1−C)−1∥p2,r−fθ∥L2(Ir), (20)
i.e. the error of approximating fand of approximating the simple function p2,ris about the same. Hence,
if we can prove error equidistribution for locally simple functions p2,r, we also obtain error equidistribution
forf
∥f−fθ∥L2(Ir)∼∥p2,r−fθ∥L2(Ir)∼∥p2,s−fθ∥L2(Is)∼∥f−fθ∥L2(Is)
for any two intervals rands. This is a slightly stronger alternative to the equidistribution of smooth-
ness in Theorem 3.3. In summary, given (19), we can reduce equidistribution from arbitrary fto simpler
equidistribution of piecewise second degree polynomials.
Before we discuss (19) more carefully, let us apply our observations to the counter example from the last
section. If the oscillation φfrom the example is orthogonal to P1andP2, the best approximations and the
network from the example satisfy p1=p2=fθ= 0and we have
∥p2,r−fθ∥L2(Ir)= 0, ∥f−fθ∥L2(Is)=∥φ∥L2(Is)̸= 0,
which contradicts the norm equivalence (20). Hence, if we can establish (19), severe sub-grid oscillations as
in the counter example are not possible.
Let us now consider the condition (19) more carefully. Informally, it follows directly from assumption (13)
of the main theorems. Indeed, standard direct approximation inequalities (41) provide the first degree
approximation error
inf
p1∈P1∥f−p1∥L2(Ir)≲h2
r∥f(2)∥L2(I),
and the second degree approximation error
inf
p2∈P2∥f−p2∥L2(Ir)≤ch3
r∥f(3)∥L2(I).
Notice that the second degree approximation has a higher power on hrand therefore is much smaller for
small intervals. Assumption (13) contains a concrete criterion when this happens (replacing hr+withhrfor
10Published in Transactions on Machine Learning Research (10/2024)
0.3 0.50.8
0.6
0.4
0.2
0.00.20.40.60.8
(a) Coarse interval Ir
0.41 0.450.30.40.50.60.7f
degree 1
degree 2 (b) Fine interval Ir
Figure 2: Approximation of a oscillatory f=φ=L5−L3constructed from Legendre basis polynomials
Li. On the coarse interval (left), the first and second degree approximation do not capture the function
correctly. On the zoom in to a fine interval (right), the second degree approximation captures the bulk of
the function, whereas the first degree approximation does not.
simplicity). It implies
inf
p2∈P2∥f−p2∥L2(Ir)≤ch3
r∥f(3)∥L2(I)(13)
≤cCh5
2rmin
x∈Ir|f(2)(x)|
=cCh5
2r/parenleftbigg1
hr/integraldisplay
Irmin
x∈Ir|f(2)(x)|2/parenrightbigg1
2
=cCh2
r∥f(2)∥L2(I),
for some constant Cthat is assumed to be sufficiently small. Direct approximation results tend to be sharp
asymptotically, in which case we can directly compare the first and second order approximation to obtain
(19):
inf
p2∈P2∥f−p2∥L2(Ir)≲Ch2
r∥f(2)∥L2(I)≲Cinf
p1∈P1∥f−p1∥L2(Ir).
A rigorous argument is e.g. in Lemma A.27, applied to f′′and constant polynomials p′′
2. The lemma also
includesreplacements ⟨fθ−f,v⟩Ir≈⟨fθ−p2,r,v⟩Iroffbylocalpolynomialsforthecriticalpointconditions
(16), which require a little more care because of missing absolute values and potential cancellation.
A simple illustration is given in Figure 2. We can see that the approximation on a coarse interval does not
capture the oscillatory fcorrectly. On the fine interval, the second degree approximation is fairly accurate,
and captures the bulk of f, as required by (20).
5.3 Feasibility of Assumptions
When is the main assumption (13) satisfied? A simple guarantee can be given if
•fis smooth∥f(4)∥Lp(D),∥f(4)∥Lp(D)≤Sfor someS>0andpfrom (13).
•fis uniformly convex (or concave) f′′(x)≥Cfor some constant C>0and allx∈D(or a convex
region as in the main theorems).
In this case, with the crude bound ∥f(s)∥Lp(Ir)≤∥f(s)∥Lp(D),s= 3,4, the assumption reduces to
h1
2−1
p
k+S≤CC
for some sufficiently small C. This is clearly satisfied if the breakpoints are close so that hr=|br−1−br|
is small. Note that practically the constants SandC−1may be large so that we need many breakpoints to
11Published in Transactions on Machine Learning Research (10/2024)
1.5
 1.0
 0.5
 0.0 0.5 1.0 1.50.000.050.100.150.200.250.300.35
(a) Uniform initial bias.
1.5
 1.0
 0.5
 0.0 0.5 1.0 1.50.000.050.100.150.200.250.300.35
 (b) Random initial bias.
Figure 3: Cusp function (21) (red) and neural network approximation (blue) at each 20000-th epoch (light
to dark blue). The circle markers show the location of the breakpoints.
apply Theorems 3.3 and 3.4. However, once we can, the resulting equidistribution and approximation errors
have standard constants that are independent of CandS.
We have argued that the condition (13) is simple to verify for a given critical point. A more difficult question
is to what extent we can guarantee that these favourable critical points are the limit of gradient descent. We
can choose hrarbitrarily small at the initial breakpoints. By stability of gradient descent, one may argue
that two breakpoints that are close at the initial may remain so during training. This would entail (13) also
for the gradient descent limit. However, a rigorous argument requires a full understanding of the gradient
descent dynamics, which is not considered in this paper and left for future research.
6 Numerical Experiments
This section contains some preliminary numerical experiments. We train the target function
f(x) = 1−|x|0.1, x ∈[−1,1], (21)
which has a cusp at the origin. This entails that f′′is not contained in L2([−1,1]), but is contained in
L2/5([−1,1])so that we expect a better performance of nonlinear approximation. Intuitively, this is achieved
by moving more breakpoints to the vicinity of the cusp. Figure 3 shows the evolution of the networks and
breakpoints during gradient descent training with the following setup
•Network architecture (9).
•Network width: 16.
•Learning rate: 0.01.
•1280samples in 10batches.
•100000epochs, with a plot of the network every 20000epochs.
The results show that gradient descent moves the breakpoints towards the cusp at the origin, as expected.
However, theconvergenceisveryslowandrequiresasubstantialnumberofepochsinrelationtothesimplicity
of the problem.
References
Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via over-
parameterization. In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.), Proceedings of the 36th
12Published in Transactions on Machine Learning Research (10/2024)
International Conference on Machine Learning , volume 97 of Proceedings of Machine Learning Re-
search, pp. 242–252, Long Beach, California, USA, 09–15 Jun 2019. PMLR. Full version available at
https://arxiv.org/abs/1811.03962 .
Sanjeev Arora, Simon Du, Wei Hu, Zhiyuan Li, and Ruosong Wang. Fine-grained analysis of optimization
and generalization for overparameterized two-layer neural networks. In Kamalika Chaudhuri and Ruslan
Salakhutdinov (eds.), Proceedings of the 36th International Conference on Machine Learning , volume 97 of
Proceedings of Machine Learning Research , pp. 322–332, Long Beach, California, USA, 09–15 Jun 2019a.
PMLR.
Sanjeev Arora, Simon S Du, Wei Hu, Zhiyuan Li, Russ R Salakhutdinov, and Ruosong Wang. On exact
computation with an infinitely wide neural net. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d’Alché
Buc, E. Fox, and R. Garnett (eds.), Advances in Neural Information Processing Systems , volume 32.
Curran Associates, Inc., 2019b.
Francis Bach. Breaking the curse of dimensionality with convex neural networks. Journal of Machine
Learning Research , 18(19):1–53, 2017.
Yu Bai and Jason D. Lee. Beyond linearization: On quadratic and higher-order approximation of wide neural
networks. In International Conference on Learning Representations , 2020.
A.R.Barron. Universalapproximationboundsforsuperpositionsofasigmoidalfunction. IEEE Transactions
on Information Theory , 39(3):930–945, 1993.
Julius Berner, Philipp Grohs, Gitta Kutyniok, and Philipp Petersen. The Modern Mathematics of Deep
Learning. In Philipp Grohs and Gitta Kutyniok (eds.), Mathematical Aspects of Deep Learning , pp. 1–111.
Cambridge University Press, 1 edition, 2022. ISBN 9781009025096 9781316516782.
Peter Binev, Wolfgang Dahmen, and Ron DeVore. Adaptive Finite Element Methods with convergence rates.
Numerische Mathematik , 97(2):219–268, April 2004.
GuyBreslerandDheerajNagaraj. SharprepresentationtheoremsforReLUnetworkswithprecisedependence
on depth. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (eds.), Advances in Neural
Information Processing Systems , volume 33, pp. 10697–10706. Curran Associates, Inc., 2020.
Yuan Cao and Quanquan Gu. Generalization Error Bounds of Gradient Descent for Learning Over-
Parameterized Deep ReLU Networks. Proceedings of the AAAI Conference on Artificial Intelligence ,
34(04):3349–3356, April 2020. ISSN 2374-3468, 2159-5399.
Zixiang Chen, Yuan Cao, Difan Zou, and Quanquan Gu. How much over-parameterization is sufficient to
learn deep re{lu} networks? In International Conference on Learning Representations , 2021.
Lénaïc Chizat and Francis Bach. On the global convergence of gradient descent for over-parameterized
models using optimal transport. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi,
and R. Garnett (eds.), Advances in Neural Information Processing Systems , volume 31. Curran Associates,
Inc., 2018. https://arxiv.org/abs/1805.09545 .
Lénaïc Chizat, Edouard Oyallon, and Francis Bach. On lazy training in differentiable programming. In
H. Wallach, H. Larochelle, A. Beygelzimer, F. d’Alché Buc, E. Fox, and R. Garnett (eds.), Advances in
Neural Information Processing Systems , volume 32. Curran Associates, Inc., 2019.
A. Cohen, W. Dahmen, and R. DeVore. Adaptive Wavelet Methods II—Beyond the Elliptic Case. Founda-
tions of Computational Mathematics , 2(3):203–202, August 2002. ISSN 16153375.
G. Cybenko. Approximation by superpositions of a sigmoidal function. Math. Control Signal Systems , 2:
303–314, 1989.
Alexandru Damian, Jason Lee, and Mahdi Soltanolkotabi. Neural networks can learn representations with
gradient descent. In Po-Ling Loh and Maxim Raginsky (eds.), Proceedings of Thirty Fifth Conference on
Learning Theory , volume 178 of Proceedings of Machine Learning Research , pp. 5413–5452. PMLR, 02–05
Jul 2022.
13Published in Transactions on Machine Learning Research (10/2024)
I. Daubechies, R. DeVore, S. Foucart, B. Hanin, and G. Petrova. Nonlinear Approximation and (Deep) ReLU
Networks. Constructive Approximation , 55(1):127–172, February 2022. ISSN 0176-4276, 1432-0940.
R.A. DeVore and G.G. Lorentz. Constructive Approximation . Grundlehren der mathematischen Wis-
senschaften. Springer Berlin Heidelberg, 1993. ISBN 9783540506270.
Ronald DeVore, Boris Hanin, and Guergana Petrova. Neural network approximation. Acta Numerica , 30:
327–444, 2021.
Ronald A. DeVore. Nonlinear approximation. Acta Numerica , 7:51–150, 1998.
Tian Ding, Dawei Li, and Ruoyu Sun. Suboptimal Local Minima Exist for Wide Neural Networks with
Smooth Activations. Mathematics of Operations Research , 47(4):2784–2814, November 2022. ISSN 0364-
765X, 1526-5471.
Selina Drews and Michael Kohler. On the universal consistency of an over-parametrized deep neural network
estimate learned by gradient descent, 2022. https://arxiv.org/abs/2208.14283 .
SimonDuandJasonLee. Onthepowerofover-parametrizationinneuralnetworkswithquadraticactivation.
In Jennifer Dy and Andreas Krause (eds.), Proceedings of the 35th International Conference on Machine
Learning , volume 80 of Proceedings of Machine Learning Research , pp. 1329–1338. PMLR, 10–15 Jul 2018.
https://arxiv.org/abs/1803.01206 .
Simon Du, Jason Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai. Gradient descent finds global minima of
deep neural networks. In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.), Proceedings of the 36th
International Conference on Machine Learning , volume 97 of Proceedings of Machine Learning Research ,
pp. 1675–1685, Long Beach, California, USA, 09–15 Jun 2019a. PMLR.
Simon S. Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh. Gradient descent provably optimizes over-
parameterized neural networks. In International Conference on Learning Representations , 2019b.
Rong Ge, Jason D. Lee, and Tengyu Ma. Learning one-hidden-layer neural networks with landscape design.
InInternational Conference on Learning Representations , 2018. https://arxiv.org/abs/1711.00501 .
R. Gentile and G. Welper. Approximation results for gradient descent trained shallow neural networks in
1d, 2022. https://arxiv.org/abs/2209.08399 .
Rémi Gribonval, Gitta Kutyniok, Morten Nielsen, and Felix Voigtlaender. Approximation Spaces of Deep
Neural Networks. Constructive Approximation , 55(1):259–367, February 2022. ISSN 0176-4276, 1432-0940.
Ingo Gühring, Gitta Kutyniok, and Philipp Petersen. Error bounds for approximations with deep ReLU
neural networks in ws,p norms. Analysis and Applications , 18(05):803–859, 2020.
Boris Hanin and Mark Sellke. Approximating continuous functions by ReLU nets of minimal width. https:
//arxiv.org/abs/1710.11278 , 2017.
Fengxiang He, Bohan Wang, and Dacheng Tao. Piecewise linear activations substantially shape the loss
surfaces of neural networks. In International Conference on Learning Representations , 2020.
Kurt Hornik, Maxwell Stinchcombe, and Halbert White. Multilayer feedforward networks are universal
approximators. Neural Networks , 2(5):359–366, 1989. ISSN 0893-6080.
Shokhrukh Ibragimov, Arnulf Jentzen, and Adrian Riekert. Convergence to good non-optimal critical points
inthetrainingofneuralnetworks: Gradientdescentoptimizationwithonerandominitializationovercomes
all bad non-global local minima with high probability, 2022. https://arxiv.org/abs/2212.13111 .
Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural tangent kernel: Convergence and generalization
in neural networks. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett
(eds.),Advances in Neural Information Processing Systems , volume 31. Curran Associates, Inc., 2018.
14Published in Transactions on Machine Learning Research (10/2024)
Arnulf Jentzen and Adrian Riekert. A proof of convergence for the gradient descent optimization method
with random initializations in the training of neural networks with relu activation for piecewise linear
target functions. Journal of Machine Learning Research , 23(260):1–50, 2022.
Arnulf Jentzen and Adrian Riekert. Non-convergence to global minimizers for adam and stochastic gradient
descent optimization and constructions of local minimizers in the training of artificial neural networks,
2024. https://arxiv.org/abs/2402.05155 .
Ziwei Ji and Matus Telgarsky. Polylogarithmic width suffices for gradient descent to achieve arbitrarily small
test error with shallow ReLU networks. In International Conference on Learning Representations , 2020.
Kenji Kawaguchi. Deep learning without poor local minima. In D. Lee, M. Sugiyama, U. Luxburg, I. Guyon,
and R. Garnett (eds.), Advances in Neural Information Processing Systems , volume 29. Curran Associates,
Inc., 2016.
Kenji Kawaguchi and Jiaoyang Huang. Gradient descent finds global minima for generalizable deep neural
networks of practical sizes. In 2019 57th Annual Allerton Conference on Communication, Control, and
Computing (Allerton) , pp. 92–99, 2019.
Kenji Kawaguchi, Jiaoyang Huang, and Leslie Pack Kaelbling. Every Local Minimum Value Is the Global
Minimum Value of Induced Model in Nonconvex Machine Learning. Neural Computation , 31(12):2293–
2323, 12 2019. ISSN 0899-7667.
B. Keene and G. Welper. Approximation , estimation and optimization errors for a deep neural network,
2024. Submitted, under double blind review.
Jason M. Klusowski and Andrew R. Barron. Approximation by combinations of ReLU and squared ReLU
ridge functions with ℓ1andℓ0controls. IEEE Transactions on Information Theory , 64(12):7649–7656,
2018.
Michael Kohler and Adam Krzyzak. Analysis of the rate of convergence of an over-parametrized deep neural
network estimate learned by gradient descent, 2022. https://arxiv.org/abs/2210.01443 .
Jaehoon Lee, Lechao Xiao, Samuel Schoenholz, Yasaman Bahri, Roman Novak, Jascha Sohl-Dickstein, and
Jeffrey Pennington. Wide neural networks of any depth evolve as linear models under gradient descent.
In H. Wallach, H. Larochelle, A. Beygelzimer, F. d 'Alché-Buc, E. Fox, and R. Garnett (eds.), Advances in
Neural Information Processing Systems , volume 32. Curran Associates, Inc., 2019.
Jaehoon Lee, Samuel Schoenholz, Jeffrey Pennington, Ben Adlam, Lechao Xiao, Roman Novak, and Jascha
Sohl-Dickstein. Finite versus infinite neural networks: an empirical study. In H. Larochelle, M. Ranzato,
R.Hadsell,M.F.Balcan,andH.Lin(eds.), Advances in Neural Information Processing Systems ,volume33,
pp. 15156–15172. Curran Associates, Inc., 2020.
Jason D. Lee, Kazusato Oko, Taiji Suzuki, and Denny Wu. Neural network learns low-dimensional polyno-
mials with sgd near the information-theoretic limit, 2024. https://arxiv.org/abs/2406.01581 .
Jongmin Lee, Joo Young Choi, Ernest K Ryu, and Albert No. Neural tangent kernel analysis of deep narrow
neural networks. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and
Sivan Sabato (eds.), Proceedings of the 39th International Conference on Machine Learning , volume 162
ofProceedings of Machine Learning Research , pp. 12282–12351. PMLR, 17–23 Jul 2022.
Bo Li, Shanshan Tang, and Haijun Yu. Better approximations of high dimensional smooth functions by deep
neural networks with rectified power units. Communications in Computational Physics , 27(2):379–411,
2019. ISSN 1991-7120.
Yuanzhi Li and Yingyu Liang. Learning overparameterized neural networks via stochastic gradient descent
onstructureddata. InS.Bengio, H.Wallach, H.Larochelle, K.Grauman, N.Cesa-Bianchi, andR.Garnett
(eds.),Advances in Neural Information Processing Systems 31 , pp. 8157–8166. Curran Associates, Inc.,
2018.
15Published in Transactions on Machine Learning Research (10/2024)
Zhong Li, Chao Ma, and Lei Wu. Complexity measures for neural networks with general activation functions
using path-based norms, 2020. https://arxiv.org/abs/2009.06132 .
George G. Lorentz, Manfred v. Golitschek, and Yuly Makovoz. Constructive Approximation: Advanced
Problems . Springer-Verlag Berlin Heidelberg, 1996.
Jianfeng Lu, Zuowei Shen, Haizhao Yang, and Shijun Zhang. Deep network approximation for smooth
functions. SIAM Journal on Mathematical Analysis , 53(5):5465–5506, 2021.
Zhou Lu, Hongming Pu, Feicheng Wang, Zhiqiang Hu, and Liwei Wang. The expressive power of neural
networks: A view from the width. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus,
S. Vishwanathan, and R. Garnett (eds.), Advances in Neural Information Processing Systems 30 , pp.
6231–6239. Curran Associates, Inc., 2017.
Song Mei, Andrea Montanari, and Phan-Minh Nguyen. A mean field view of the landscape of two-layer
neural networks. Proceedings of the National Academy of Sciences , 115(33):E7665–E7671, 2018. ISSN
0027-8424. https://arxiv.org/abs/1804.06561 .
PedroMorin,RicardoH.Nochetto,andKunibertG.Siebert. Convergenceofadaptivefiniteelementmethods.
SIAM Review , 44(4):631–658, 2002.
Quynh Nguyen and Matthias Hein. The loss surface of deep and wide neural networks. In Doina Precup and
Yee Whye Teh (eds.), Proceedings of the 34th International Conference on Machine Learning , volume 70
ofProceedings of Machine Learning Research , pp. 2603–2612. PMLR, 06–11 Aug 2017.
Quynh N Nguyen and Marco Mondelli. Global convergence of deep networks with one wide layer followed by
pyramidal topology. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (eds.), Advances
in Neural Information Processing Systems , volume 33, pp. 11961–11972. Curran Associates, Inc., 2020.
Joost A. A. Opschoor, Philipp C. Petersen, and Christoph Schwab. Deep ReLU networks and high-order
finite element methods. Analysis and Applications , 18(05):715–770, 2020.
S. Oymak and M. Soltanolkotabi. Toward moderate overparameterization: Global convergence guarantees
for training shallow neural networks. IEEE Journal on Selected Areas in Information Theory , 1(1):84–105,
2020.
Rahul Parhi and Robert D. Nowak. Banach space representer theorems for neural networks and ridge splines.
Journal of Machine Learning Research , 22(43):1–40, 2021.
Allan Pinkus. Approximation theory of the mlp model in neural networks. Acta Numerica , 8:143–195, 1999.
Grant M. Rotskoff and Eric Vanden-Eijnden. Neural networks as interacting particle systems: Asymptotic
convexity of the loss landscape and universal scaling of the approximation error. CoRR, abs/1805.00915,
2018. https://arxiv.org/abs/1805.00915 .
Itay Safran and Ohad Shamir. Spurious local minima are common in two-layer ReLU neural networks. In
Jennifer Dy and Andreas Krause (eds.), Proceedings of the 35th International Conference on Machine
Learning , volume 80 of Proceedings of Machine Learning Research , pp. 4433–4441, Stockholmsmässan,
Stockholm Sweden, 10–15 Jul 2018. PMLR.
Mariia Seleznova and Gitta Kutyniok. Analyzing finite neural networks: Can we trust neural tangent kernel
theory? In Joan Bruna, Jan Hesthaven, and Lenka Zdeborova (eds.), Proceedings of the 2nd Mathematical
and Scientific Machine Learning Conference , volume 145 of Proceedings of Machine Learning Research ,
pp. 868–895. PMLR, 16–19 Aug 2022.
ZuoweiShen, HaizhaoYang, andShijunZhang. Nonlinearapproximationviacompositions. Neural Networks ,
119:74–84, 2019. ISSN 0893-6080.
Jonathan W. Siegel and Jinchao Xu. Approximation rates for neural networks with general activation
functions. Neural Networks , 128:313–321, 2020. ISSN 0893-6080.
16Published in Transactions on Machine Learning Research (10/2024)
Jonathan W. Siegel and Jinchao Xu. High-order approximation rates for shallow neural networks with cosine
and ReLUkactivation functions. Applied and Computational Harmonic Analysis , 58:1–26, 2022a. ISSN
1063-5203.
Jonathan W. Siegel and Jinchao Xu. Optimal convergence rates for the orthogonal greedy algorithm. IEEE
Transactions on Information Theory , 68(5):3354–3361, 2022b.
Jonathan W. Siegel, Qingguo Hong, Xianlin Jin, Wenrui Hao, and Jinchao Xu. Greedy training algorithms
for neural networks and applications to PDEs. Journal of Computational Physics , 484:112084, July 2023.
ISSN 00219991.
Justin Sirignano and Konstantinos Spiliopoulos. Mean field analysis of neural networks: A law of large
numbers. SIAM Journal on Applied Mathematics , 80(2):725–752, 2020.
Mahdi Soltanolkotabi, Adel Javanmard, and Jason D. Lee. Theoretical insights into the optimization land-
scape of over-parameterized shallow neural networks. IEEE Transactions on Information Theory , 65(2):
742–769, 2019. https://arxiv.org/abs/1707.04926 .
ChaeHwan Song, Ali Ramezani-Kebrya, Thomas Pethick, Armin Eftekhari, and Volkan Cevher. Sub-
quadratic overparameterization for shallow neural networks. In M. Ranzato, A. Beygelzimer, Y. Dauphin,
P.S. Liang, and J. Wortman Vaughan (eds.), Advances in Neural Information Processing Systems , vol-
ume 34, pp. 11247–11259. Curran Associates, Inc., 2021.
Zhao Song and Xin Yang. Quadratic suffices for over-parametrization via matrix chernoff bound, 2019.
https://arxiv.org/abs/1906.03593 .
Daniel Soudry and Yair Carmon. No bad local minima: Data independent training error guarantees for
multilayer neural networks, 2016. https://arxiv.org/abs/1605.08361 .
Lili Su and Pengkun Yang. On learning over-parameterized neural networks: A functional approximation
perspective. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d 'Alché-Buc, E. Fox, and R. Garnett (eds.),
Advances in Neural Information Processing Systems , volume 32. Curran Associates, Inc., 2019.
Taiji Suzuki. Adaptivity of deep reLU network for learning in besov and mixed smooth besov spaces: optimal
rate and curse of dimensionality. In International Conference on Learning Representations , 2019.
Grzegorz Swirszcz, Wojciech Marian Czarnecki, and Razvan Pascanu. Local minima in training of neural
networks, 2017. https://arxiv.org/abs/1611.06310v2 .
Michael Unser. Ridges, neural networks, and the radon transform. Journal of Machine Learning Research ,
24(37):1–33, 2023.
Luca Venturi, Afonso S. Bandeira, and Joan Bruna. Spurious valleys in one-hidden-layer neural network
optimization landscapes. Journal of Machine Learning Research , 20(133):1–34, 2019. https://arxiv.
org/abs/1802.06384 .
Nikhil Vyas, Yamini Bansal, and Preetum Nakkiran. Empirical limitations of the NTK for understanding
scaling laws in deep learning. Transactions on Machine Learning Research , 2023. ISSN 2835-8856.
E Weinan, Ma Chao, Wu Lei, and Stephan Wojtowytsch. Towards a mathematical understanding of neural
network-based machine learning: What we know and what we don’t. CSIAM Transactions on Applied
Mathematics , 1(4):561–615, 2020. ISSN 2708-0579.
E Weinan, Chao Ma, and Lei Wu. The Barron Space and the Flow-Induced Function Spaces for Neural
Network Models. Constructive Approximation , 55(1):369–406, February 2022. ISSN 0176-4276, 1432-0940.
G. Welper. Approximation and gradient descent training with neural networks, 2024a. https://arxiv.
org/abs/2405.11696 .
17Published in Transactions on Machine Learning Research (10/2024)
Gerrit Welper. Approximation results for gradient flow trained neural networks. Journal of Machine Learn-
ing, 3(2):107–175, 2024b.
Dmitry Yarotsky. Error bounds for approximations with deep ReLU networks. Neural Networks , 94:103–114,
2017. ISSN 0893-6080.
Dmitry Yarotsky. Optimal approximation of continuous functions by very deep ReLU networks. In Sébastien
Bubeck, Vianney Perchet, and Philippe Rigollet (eds.), Proceedings of the 31st Conference On Learning
Theory, volume 75 of Proceedings of Machine Learning Research , pp. 639–649. PMLR, 06–09 Jul 2018.
Dmitry Yarotsky and Anton Zhevnerchuk. The phase diagram of approximation rates for deep neural
networks. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (eds.), Advances in Neural
Information Processing Systems , volume 33, pp. 13005–13015. Curran Associates, Inc., 2020.
Ding-Xuan Zhou. Universality of deep convolutional neural networks. Applied and Computational Harmonic
Analysis, 48(2):787–794, 2020. ISSN 1063-5203.
Difan Zou and Quanquan Gu. An improved analysis of training over-parameterized deep neural networks.
In H. Wallach, H. Larochelle, A. Beygelzimer, F. d 'Alché-Buc, E. Fox, and R. Garnett (eds.), Advances in
Neural Information Processing Systems , volume 32. Curran Associates, Inc., 2019.
Difan Zou, Yuan Cao, Dongruo Zhou, and Quanquan Gu. Gradient descent optimizes over-parameterized
deep ReLU networks. Machine Learning , 109(3):467 – 492, 2020.
18Published in Transactions on Machine Learning Research (10/2024)
A Proof of the Main Results
We follow roughly the steps in the proof overview in Section 4:
1. Section A.1 characterizes the critical points, constructs the complement space X⊥and provides some
of its properties.
2. Section A.2 proves the distribution of the grid size limit for infinite width in Lemma 3.2.
3. Section A.3 proves equidistribution in Theorem 3.3.
4. Section A.4 proves the approximation properties of the critical points in Theorem 3.4.
A.1 Critical Points and the Complement Space X⊥
A.1.1 Critical Points
We have already seen in the proof overview that the weights θare a critical point if and only if ⟨κ,v⟩= 0for
allv∈X+˙X, with residual κ:=fθ−f. For reference, this is stated again in the following lemma, together
with a characterization of the spaces Xand ˙X.
Lemma A.1. Letθbe a critical point (11), with cleaned breakpoints in ascending order 12. Then
⟨κ,v⟩= 0, v∈X={v∈L2(D)|v(b0) = 0, v|Ir∈P1, r= 1,..., ¯m},
⟨κ,v⟩= 0, v∈˙X={v∈L2(D)|v|Ir∈P0, r= 1,..., ¯m},
Proof.In (16), we have seen that the critical points are given by the condition ⟨κ,v⟩= 0for allvin the
spanXof the partial derivatives ∂wrℓand in the span ˙Xof the partial derivatives ∂brℓ. Hence, it suffices to
show that the span of the derivatives matches the two sets in the lemma. Indeed, one readily computes
∂wrfθ=⟨κ,σ(·−br)⟩, ∂ brfθ=⟨κ,wr˙σ(·−br)⟩.
Clearly ReLUactivations σ(·−br)span piecewise linear functions and their derivatives wr˙σ(·−br)span
piecewise constants because in the cleanup before Theorem 3.4 we have already dropped all neurons with
wr= 0. Finally, by the same cleanup, the leftmost breakpoint maybe inside or outside of D, but anyways,
we must have v(b0) = 0because no partial derivative has support left of this point.
Note that the network itself is continuous piecewise linear (CPwL) so that fθ∈Xand the first critical point
condition
⟨κ,v⟩= 0, v∈X
is merely a best L2projection for the outer weights wr. Together with the second condition
⟨κ,v⟩= 0, v∈X+˙X (22)
this formally matches a best L2projection onto discontinuous piecewise linear (DPwL) functions. However,
fθis not discontinuous and instead we have to move the breakpoints brto satisfy all conditions.
Recall from the proof overview that we split X+˙XintoXand theL2-orthogonal complement X⊥:
X+˙X=X⊕X⊥, X ⊥X⊥.
Since the neural network fθis contained in X, this implies⟨f,v⟩=⟨fθ−f,v⟩=⟨κ,v⟩for allv∈X⊥and
therefore at a critical point ⟨κ,v⟩= 0we have
⟨f,v⟩= 0, v∈X⊥. (23)
Unlike the residual κ=fθ−f, the target fdoes not depend on the outer weights wrand therefore the last
condition decouples the computation of the inner weights brfrom the outer weights wr. This will be crucial
to prove equidistribution, which also depends on the inner weights br, only.
19Published in Transactions on Machine Learning Research (10/2024)
A.1.2 The Complement Space X⊥
In this section, we construct an explicit basis for the complement space X⊥. As is customary for e.g. finite
elements, we first define suitable functions on the reference interval ˆI:= [0,1]and then use the bijective
affine linear transform
Tr:ˆI→Ir, T r(ˆx) = (br−br−1)ˆx+br−1, T′
r=hr, (T−1
r)′=h−1
r
to define corresponding functions on the interval Ir. We use hat ˆ·to emphasize that a certain quantity is
defined on the reference interval.
The construction starts with the four functions
´ϕ:ˆI→R,´ϕ(x) =−x3+x2,
`ϕ:ˆI→R,`ϕ(x) =x3−2x2+x,
¯ϕ:ˆI→R,¯ϕ(x) =´ϕ(x) +`ϕ(x),
˜ϕ:ˆI→R,¯ϕ(x) =´ϕ(x)−`ϕ(x),(24)
defined on the reference interval ˆIand shown in Figure 4. The corresponding functions on the interval Ir
are defined with the affine transform
˚ϕr(x) :=/braceleftbigg˚ϕ◦T−1
r(x), x∈Ir
0 x̸∈Ir,(25)
extended by zero outside of Ir, for any ˚∈{´,`,¯,˜}.
0.0 0.2 0.4 0.6 0.8 1.00.000.020.040.060.080.100.120.14
(a)´ϕ
0.0 0.2 0.4 0.6 0.8 1.00.000.020.040.060.080.100.120.14 (b)`ϕ
0.0 0.2 0.4 0.6 0.8 1.00.000.050.100.150.200.25 (c)¯ϕ
0.0 0.2 0.4 0.6 0.8 1.00.100
0.075
0.050
0.025
0.0000.0250.0500.0750.100 (d)˜ϕ
Figure 4: Functions defined in (24).
From ´ϕrand `ϕr, we can construct all functions in X⊥that we need for the proof of the main results, as
given in the next lemma.
Lemma A.2. Let´ϕrand`ϕrbe defined by (25). Then
hr´ϕ′′
r−hr+1`ϕ′′
r+1∈X⊥, r = 2,..., ¯m−1.
Although the second derivative might seem artificial at first, the function ´ϕrwill be more useful than ´ϕ′′
r
down the road.
Proof.We abbreviate φr:=hr´ϕ′′
r−hr+1`ϕ′′
r. Clearly `ϕ′′
rand´ϕ′′
rare piecewise linear so that φr∈X+˙Xand
it is sufficient to show ⟨φr,Hs⟩= 0for a basisHs,s= 0,..., ¯mofX. We choose hat functions centered at
bs, i.e.
Hs(x) =

T−1
s(x)x∈Is,
1−T−1
s+1(x)x∈Is+1,
0 else.
Using the compact supports of ´ϕr,`ϕr,Hs, the derivatives T′
r=hr,(T−1
r)′=h−1
r, the chain rule ´ϕ′′
r=
(´ϕ′′◦T−1
r)h−2
rand transforming to the reference interval, we compute
⟨φr,Hr−1⟩=h−1
r/integraldisplay
Ir´ϕ′′◦T−1
r(x)(1−T−1
r)(x)dx=/integraldisplay
ˆI´ϕ′′(ˆx)(1−ˆx)dˆx=/integraldisplay1
0(−6ˆx+ 2)(1−ˆx)dˆx= 0
20Published in Transactions on Machine Learning Research (10/2024)
and
⟨φr,Hr+1⟩=h−1
r+1/integraldisplay
Ir+1`ϕ′′◦T−1
r+1(x)T−1
r+1(x)dx =/integraldisplay
ˆI`ϕ′′(ˆx)ˆxdˆx=/integraldisplay1
0(6ˆx−4)ˆxdˆx= 0
and
⟨φ,Hr⟩=h−1
r/integraldisplay
Ir´ϕ′′◦T−1
r(x)T−1
r(x)dx−h−1
r+1/integraldisplay
Ir+1`ϕ′′◦T−1
r+1(x)(1−T−1
r+1)(x)dx
=/integraldisplay1
0(−6ˆx+ 2)ˆxdˆx−/integraldisplay1
0(6ˆx−4)(1−ˆx)dˆx= (−1)−(−1) = 0
All other⟨φr,Hs⟩have non-overlapping support and therefore evaluate to zero. In conclusion ⟨φr,Hs⟩= 0
for a basisHsofX⊥. Together with φr∈X⊥, this concludes the proof.
The following lemma is the cornerstone for showing equidistribution in Lemma 3.2 and Theorem 3.3.
Lemma A.3. Letbr,r∈[ ¯m]be cleaned critical breakpoints (11),(12). Let ´ϕr,`ϕr,¯ϕr,˜ϕrbe defined by
(25). Then
hr⟨f′′,´ϕr⟩Ir=hr+1⟨f′′,`ϕr+1⟩Ir+1.
Since ´ϕrand`ϕrare non-negative bump functions, under the conditions of the main theorem, one can show
thath2−1
qr∥f′′∥Lq(Ir)∼hr⟨f′′,´ϕr⟩∼hr⟨f′′,`ϕr⟩(with an argument analogous to Lemma A.27). Therefore,
repeated application of the Lemma yields equidistribution
h2−1
qr∥f′′∥Lq(Ir)∼hr⟨f′′,´ϕr⟩=hr+1⟨f′′,`ϕr+1⟩∼h2−1
q
r+1∥f′′∥Lq(Ir+1)
between neighbouring intervals. However, repeated application for distant intervals IrandIsaccumulates
too much error. For the main equidistribution theorem we chain the identity of the lemma directly, which
requires careful consideration of the difference ˜ϕr=`ϕr−`ϕr.
Proof.Recall from (23), that at critical points, we have ⟨f,v⟩= 0for allv∈X⊥. By Lemma A.2, the
functionhr´ϕ′′
r−hr+1`ϕ′′
r+1is contained in X⊥and therefore we may substitute it for vto obtain
⟨f,hr´ϕ′′
r−hr+1`ϕ′′
r+1⟩= 0,⇔ hr⟨f,´ϕ′′
r⟩=hr+1⟨f,`ϕ′′
r+1⟩. (26)
This shows the lemma, except that the two derivatives are on the wrong side of the inner product, which we
correct with integration by parts:
hr⟨f,´ϕ′′
r⟩Ir=−f(br)−hr⟨f′,´ϕ′
r⟩Ir=−f(br) +hr⟨f′′,´ϕr⟩Ir
where in the first step we have used that ´ϕ′
r(br−1) = 0and ´ϕ′
r(br) = ´ϕ′◦T−1
r(br)h−1
r=−h−1
rand in
the second step that ´ϕris zero on the boundary, by the boundary values in Lemma A.4 in the technical
supplements. Analogously, we obtain
hr+1⟨f,`ϕ′′
r+1⟩Ir+1=−f(br)−hr+1⟨f′,`ϕ′
r+1⟩Ir+1=−f(br) +hr+1⟨f′′,`ϕr+1⟩Ir+1
Plugging into (26), we conclude that
−f(br) +hr⟨f′′,´ϕr⟩Ir=−f(br) +hr+1⟨f′′,`ϕr+1⟩Ir+1,
which yields the lemma upon cancelling f(br).
21Published in Transactions on Machine Learning Research (10/2024)
A.1.3 Technical Properties of ´ϕr,`ϕr,¯ϕr,˜ϕr
This section contains several technical properties of the functions ´ϕr,`ϕr,¯ϕr,˜ϕrdefined in (25).
Lemma A.4. Let´ϕrand`ϕrbe defined by (25). Then
´ϕ(0) = 0, ´ϕ(1) = 0, ´ϕ′(0) = 0, ´ϕ′(1) =−1,
`ϕ(0) = 0, `ϕ(1) = 0, `ϕ′(0) = 1, `ϕ′(1) = 0,
Proof.Follows directly from the explicit formulas for ´ϕand`ϕin (24).
Lemma A.5. Let´ϕrand`ϕrbe defined by (25). Then for all x∈ˆI
´ϕ(x)≥0, `ϕ(x)≥0.
Proof. ´ϕand`ϕare third order polynomials for which the lemma follows by elementary computation.
Lemma A.6. Let´ϕrand`ϕrbe defined by (25). Then
⟨1,´ϕ⟩Ir=hr
12,⟨T−1
r,´ϕ⟩Ir=hr
20,⟨x,´ϕ⟩Ir=h2
r
20+hr
12br−1,
⟨1,`ϕ⟩Ir=hr
12,⟨T−1
r,`ϕ⟩Ir=hr
30,⟨x,`ϕ⟩Ir=h2
r
30+hr
12br−1.
Proof.For any functions ˆvandˆϕdefined on the reference interval ˆIand corresponding functions v:= ˆv◦T−1
r
andϕ:=ˆϕ◦T−1
ron the interval Ir, we have
⟨v,ϕ⟩Ir=hr/integraldisplay
ˆIˆv(ˆx)ˆϕ(ˆx)dˆx.
Therefore, for ˆv= 1andˆϕ=´ϕandˆϕ=`ϕwe have
⟨1,´ϕr⟩Ir=hr⟨1,´ϕ⟩ˆI=hr/integraldisplay
ˆI−ˆx3+ ˆx2dˆx=hr
12,
⟨1,`ϕr⟩Ir=hr⟨1,`ϕ⟩ˆI=hr/integraldisplay
ˆIˆx3−2ˆx2+ ˆxdˆx=hr
12.
Likewise, since the transformation of ˆx→ˆxto the interval IrisT−1
r, we have
⟨T−1
r,´ϕr⟩Ir=hr⟨ˆx,´ϕ⟩ˆI=hr/integraldisplay
ˆIˆx(−ˆx3+ ˆx2)dˆx=hr
20,
⟨T−1
r,`ϕr⟩Ir=hr⟨ˆx,`ϕ⟩ˆI=hr/integraldisplay
ˆIˆx(ˆx3−2ˆx2+ ˆx)dˆx=hr
30.
Finally, the function xtransformed to the reference interval is Tr(ˆx) = (br−br−1)ˆx+br−1=hrˆx+br−1.
Thus, together with the identities above
⟨x,´ϕr⟩Ir=hr⟨Tr,´ϕ⟩ˆI=h2
r⟨ˆx,´ϕ⟩ˆI+hrbr−1⟨1,´ϕ⟩ˆI=h2
r
20+hr
12br−1,
⟨x,`ϕr⟩Ir=hr⟨Tr,`ϕ⟩ˆI=h2
r⟨ˆx,`ϕ⟩ˆI+hrbr−1⟨1,`ϕ⟩ˆI=h2
r
30+hr
12br−1,
which completes the proof.
22Published in Transactions on Machine Learning Research (10/2024)
A.2 Equilibration for the Limit h→0
A.2.1 Results
We have seen in Section 2 that we can achieve optimal approximation rates by equilibrating the smoothness
∥f′′∥L2/5(Ir)on all intervals Ir. In this section, we provide an argument that in the limit of small intervals
hr→0, the equidistribution and the critical point conditions yield the same adapted grids. This section is
kept informal, put provides intuition and guidelines for a rigorous analysis for finite hrin Section A.3.
To provide meaningful limits, we consider the grid size limit
h(x) := lim
m→∞mhr,
wherehris the size of the interval Irthat contains xand the network width ma normalization factor. On
a uniform grid, this normalization yields h(x) =|D|. Throughout this section, we assume that the limit
exists. We first consider the limit of equidistribution in the following lemma.
Lemma A.7. Assume that the limit h(x)exists. Let fbe smooth and let the intervals be equilibrated
∥f′′∥L2/5(Ir)=∥f′′∥L2/5(Is)for allr,sand for all m. Then for m→∞the grid size limit satisfies
h(x) =c|f′′(x)|−2/5
for some constant c.
This limit confirms the expectation that we want a fine grid wherever the function fhas little smoothness,
here expressed by a large second derivative |f′′(x)|. In comparison, the limiting grid of the critical points
satisfy the following lemma.
Lemma A.8 (Lemma 3.2 restated) .Letfbe smooth and for every mletbr,r∈[ ¯m]be cleaned critical
breakpoints (11),(12). If the limit h(x)exists, it satisfies
h(x) =cIf′′(x)−2/5,
with possibly a different constant cIon each interval Ifor whichf′′(x)is non-zero.
We observe that the grid size limit is identical to the density of the smoothness norm equidistribution in
Lemma 3.2, up to a global factor on each interval for which f′′is non-zero. Thus, in the limit, critical points
have a proper grid distribution on every strictly convex or concave stretch of the target function f, but may
be imbalanced between these stretches.
The proof relies on the observation that in the limit the grid size satisfies the ODE in the following lemma.
Lemma A.9. Letfbe smooth and for every mletbr,r∈[ ¯m]be cleaned critical breakpoints (11),(12). If
the limith(x)exists, it satisfies the differential equation
[h(x)2f′′(x)]′=1
5h(x)2f′′′(x).
The proofs of all lemmas are given in the following section.
A.2.2 Proofs
We first prove the limit for norm equidistribution.
Proof of Lemma 3.2. Sincefis smooth, we have
m5/2∥f′′∥L2/5(Ir)= (mhr)5/2/parenleftbigg1
hr/integraldisplay
Ir|f′′(x)|2/5dx/parenrightbigg5/2
→h(x)5/2|f′′(x)|.
23Published in Transactions on Machine Learning Research (10/2024)
Since by equidistribution the left hand side is independent of the interval Ircontaining x, the right hand
side is independent of xand thus
h(x)5/2|f′′(x)|=const ⇒ h(x) =const|f′′(x)|−2/5,
which concludes the proof.
To prove the analogous result for critical points, recall that from Lemma A.3 that hr⟨f′′,´ϕr⟩Ir=
hr+1⟨f′′,`ϕr+1⟩Ir+1. We show that in the limit this reduces to an ODE for the grid size h(x). To this
end, we first need some technical lemmas.
Lemma A.10. Letvbe smooth and ´ϕr,`ϕrbe defined by (25). Then
lim
m→∞1
hr⟨v,´ϕr⟩Ir=1
12v(x),
lim
m→∞1
h2r⟨v,´ϕr−`ϕr⟩Ir=1
60v′(x).
Proof.Sincevis smooth and ´ϕrnon-negative (Lemma A.5), by the mean value theorem and normalization
⟨1,´ϕ⟩Ir=hr/12(Lemma A.6), we have
1
hr⟨v,´ϕr⟩Ir=v(ξ)1
hr⟨1,´ϕr⟩Ir=1
12v(ξ)
for someξ∈Ir. In the limit m→∞and intervals Irthat contain x, this yields the first formula of the
lemma.
To show the second limit, define
Φ(x) :=/integraldisplayx
0´ϕ(y)−`ϕ(y)dy=−1
2(x4−2x3+x2)
andΦr:= Φ◦(Tr)−1. Then Φ′
r= Φ′◦T−1
rh−1
r= (´ϕr−`ϕr)h−1
r. Furthermore, Φhas boundary values
Φ(0) = Φ(1) = 0 , is non-positive on ˆIand
⟨1,Φr⟩Ir=hr/integraldisplay
ˆIΦ(ˆx)dˆx=−hr
60
by transforming to the reference interval with TrandT′
r=hr. Using integration by parts, Φ≤0and the
mean value theorem, it follows that
1
h2r⟨v,´ϕr−`ϕr⟩Ir=−1
hr⟨v′,Φr⟩Ir=−v′(ξ)1
hr⟨1,Φr⟩Ir=v′(ξ)1
60.
Taking the limit m→∞, this yields the second formula of the lemma.
Next, we show that the grid size limit satisfies the ODE in Lemma A.9.
Proof of Lemma A.9. By a telescopic sum, we have
hs⟨f′′,´ϕs⟩Is−hr⟨f′′,´ϕr⟩Ir=s/summationdisplay
k=r+1hk⟨f′′,´ϕk⟩Ik−hk−1⟨f′′,´ϕk−1⟩Ik−1.
24Published in Transactions on Machine Learning Research (10/2024)
Since the intervals Iroriginate from are a critical point, by Lemma A.3 we have hk−1⟨f′′,´ϕk−1⟩Ik−1=
hk⟨f′′,`ϕk⟩Ikand therefore
hs⟨f′′,´ϕs⟩Is−hr⟨f′′,´ϕr⟩Ir=s/summationdisplay
k=r+1hk⟨f′′,´ϕk⟩Ik−hk⟨f′′,`ϕk⟩Ik
=s/summationdisplay
k=r+1hk⟨f′′,´ϕk−`ϕk⟩Ik.
Multiplying by m2yields
(mhs)21
hs⟨f′′,´ϕs⟩Is−(mhr)21
hr⟨f′′,´ϕr⟩Ir=s/summationdisplay
k=r+1(mhk)21
h2
k⟨f′′,´ϕk−`ϕk⟩Ikhk.
By Lemma A.10, in the limit m→∞this converges to
1
12h(x)2f′′(x)−1
12h(y)2f′′(y) =1
60/integraldisplayx
yh(z)2f′′′(z)dz,
where we have used that the terms in the Riemann sum are constant on each interval Irso that the extra
hrat the end converges to dz. Multiplying by 12and differentiation with respect to xyields the lemma.
It remains to solve the ODE in the last Lemma.
Proof of Lemma 3.2, A.8. We abbreviate g:=f′′. By Lemma A.9, the grid size limit hsatisfies the ODE
[h2g]′=1
5h2g′,
which is first order linear in h2, whenever g(x)̸= 0. To solve it, define the integrating factor µby the ODE
gµ′=−1
5g′µ,
with the explicit solution
gµ′=−1
5g′µ⇒µ′
µ=−1
5g′
g⇒ ln(µ)′=−1
5ln(g)′⇒µ=g−1/5,
up to a global multiplicative factor. Multiplying the original ODE with µ, we obtain
[h2g]′µ=1
5h2g′µ=−h2gµ′⇒ [h2g]′µ+h2gµ′= 0⇒ [h2gµ]′= 0.
Henceh2gµis constant. Plugging in g=f′′and the explicit solution µ=g−1/5yieldsh2(f′′)4/5=cfor
some constant c. Solving for hyields the lemma.
A.3 Equilibrium for Finite h
In this section, we prove the main equidistribution theorem, restated here for convenience:
Theorem A.11 (Theorem 3.3, restated) .Letθbe a critical point (11), with cleaned breakpoints in ascending
order 12. For r,s∈{2,..., ¯m}, letI={r,r+ 1,...,s}be a set of consecutive neurons with DI:=/uniontext
k∈IIk
and
max/braceleftbigg
h1
2−1
p
k+∥f(3)∥Lp(Ik+), h1−1
q
k+∥f(4)∥Lq(Ik+),/bracerightbigg
≤Cmin
x∈Ik+|f′′(x)| (27)
for some 1< q,p≤∞and some sufficiently small constant C > 0independent of fandhk. Then for
l,k∈DIwe have equidistribution
∥f′′∥L2/5(Il)∼∥f′′∥L2/5(Ik).
25Published in Transactions on Machine Learning Research (10/2024)
A.3.1 Notations
Throughout this section, we abbreviate
´ar(v) :=h−1
r⟨v,´ϕr⟩Ir, `ar(v) :=h−1
r⟨v,`ϕr⟩Ir, (28)
¯ar(v) :=h−1
r⟨v,¯ϕr⟩Ir, ˜ar(v) :=h−1
r⟨v,˜ϕr⟩Ir (29)
and in case v=f′′, even shorter
´ar:= ´ar(f′′), `ar:= `ar(f′′), ¯ar:= ¯ar(f′′), ˜ar:= ˜ar(f′′).(30)
We will also repeatedly use the integrating factor
µr:=|¯ar|α, α :=−1
5. (31)
analogous to the one that was used in the solution of the ODE from Lemma A.9, in the infinite width limit.
A.3.2 Overview
Recall from Lemma A.9 that for m→∞, the grid size limit h(x)and the integrating factor µ(x)satisfy the
two ODEs
[h2f′′]2=1
5h2f′′′, f′′µ′=−1
5f′′′µ,
respectively. It follows that
[h2f′′µ]′= [h2f′′]′µ+h2f′′µ′
=1
5h2f′′′µ−1
5h2f′′′µ
= 0,(32)
where in the first step we have used the product rule and in the third the two ODEs for handµ. For finite
hr, we follow a similar argument:
1. Lemma A.12 replaces the derivative on the left hand side with a difference and the zero on the right
hand side with perturbation terms (I)−(IV)that we prove to be small in subsequent sections.
As result, the terms h2
r¯arµrare almost constant between neighbouring intervals.
2. Lemmas A.13 and A.14 bound error accumulation when comparing h2
r¯arµr= [h5
2r¯as]4
5over multiple
intervals, resulting in equidistribution of this quantity.
3. Theorem 3.3 then follows from [h5
2r¯as]4
5∼∥f′′∥4
5
L2/5(Ir),by Lemma A.27 at the end of this section.
A.3.3 Proof of the Main Result
The assumptions for the main theorems confine the results to regions where fis convex or concave. For the
time being, we make this assumption explicit by assuming ¯ar≥0, which will be removed later.
Lemma A.12. Letbr,r∈[ ¯m]be cleaned critical breakpoints (11),(12). Let ¯ar,˜arandµrbe defined by
(28),(31),¯ar≥0andα:=−1/5. Then
h2
r+1¯ar+1µr+1−h2
r¯arµr= (I) + (II) + (III) + (IV),
with
(I) =h2
r[˜ar+1+ ˜ak+α[¯ar+1−¯ar]]µr+1,
(II) = [h2
r+1−h2
r]˜ar+1µr+1,
(III) =α2h2
r¯aα−1
r[¯ar+1−¯ar]2,
(IV) =αh2
r[¯ar+1−¯ar]Rr+h2
r¯arRr.
26Published in Transactions on Machine Learning Research (10/2024)
and for some 0≤ξr≤1
Rr:=α(α−1)[ξr¯ar+ (1−ξr)¯ar+1]α−2[¯ar+1−¯ar]2.
Proof.We mimic the steps in the motivation (32), starting with the product rule:
h2
r+1¯ar+1µr+1−h2
r¯arµr= [h2
r+1¯ar+1−h2
r¯ar]µr+1+h2
r¯ar[µr+1−µr]. (33)
The next step in the motivation is to invoke the ODEs for handµ. The former is based on Lemma
A.3, which we can invoke directly (twice in the second step below) together with the observation that
h2
r´ar=hr⟨f′′,´ϕr⟩Ir, etc., to obtain
h2
r+1¯ar+1−h2
r¯ak= (h2
r+1´ar+1−h2
k´ak) + (h2
r+1`ar+1−h2
k`ak)
= (h2
r+1´ar+1−h2
r+1`ar+1) + (h2
r´ar−h2
k`ak)
=h2
r+1˜ar+1+h2
r˜ak.
For the second term, we don’t invoke the µODE directly, but instead compute the derivative, or rather
difference, using the explicit formula µr= ¯aα
r. Applying a Taylor expansion for z→zα, we obtain
µr+1−µr= ¯aα
r+1−¯aα
r=α¯aα−1
r[¯ar+1−¯ar] +Rr (34)
and Taylor remainder
Rr:=α(α−1)[ξr¯ar+ (1−ξr)¯ar+1]α−2[¯ar+1−¯ar]2(35)
for some 0<ξr<1. Plugging these identities into (33) and using ¯aα
r=µr, we obtain
h2
r+1¯ar+1µr+1−h2
r¯arµr= [h2
r+1˜ar+1+h2
r˜ak]µr+1+h2
r¯ar[α¯aα−1
r[¯ar+1−¯ar] +Rr]
= [h2
r+1˜ar+1+h2
r˜ak]µr+1+αh2
rµr[¯ar+1−¯ar] +h2
r¯arRr.
In the continuous motivation (32), the terms on the right hand side cancel to zero. In the discrete case, we
rearrange the right hand side into summands that we prove to be small later:
h2
r+1¯ar+1µr+1−h2
r¯arµr= [h2
r+1−h2
r]˜ar+1µr+1
+h2
r[˜ar+1+ ˜ak]µr+1+αh2
rµr+1[¯ar+1−¯ar]
−αh2
r[µr+1−µr][¯ar+1−¯ar] +h2
r¯arRr
= [h2
r+1−h2
r]˜ar+1µr+1
+h2
r[˜ar+1+ ˜ak+α[¯ar+1−¯ar]]µr+1
−αh2
r[µr+1−µr][¯ar+1−¯ar] +h2
r¯arRr.
We will see later that the third but last and last lines contain small perturbation terms and the second but
last line is zero for f′′∈P1(Lemma A.16) and close to zero for general f′′. For now, we eliminate the
differenceµr+1−µrby Taylor expansion (34), (35) to obtain
αh2
r[µr+1−µr][¯ar+1−¯ar] =α2h2
r¯aα−1
r[¯ar+1−¯ar]2+αh2
r[¯ar+1−¯ar]Rr
and therefore
h2
r+1¯ar+1µr+1−h2
r¯arµr= [h2
r+1−h2
r]˜ar+1µr+1
+h2
r[˜ar+1+ ˜ak+α[¯ar+1−¯ar]]µr+1
−α2h2
r¯aα−1
r[¯ar+1−¯ar]2+αh2
r[¯ar+1−¯ar]Rr+h2
r¯arRr
This concludes the proof, upon reordering terms.
27Published in Transactions on Machine Learning Research (10/2024)
The previous lemma shows that h2
r¯arµris comparable on two neighbouring intervals IrandIr+1. Applying
the argument repeatedly, allows us to compare h2
r¯arµracross multiple intervals. The following lemma bounds
the compound error.
Lemma A.13. Letzk∈Randhk≥0fork= 1,...,m. Assume
|zk+1−zk|≤cd[hkzk+hk+1zk+1]
and the conditions
cdhk≤1
2, 1 + 2cdCdm/summationdisplay
k=1hk≤Cd,1
Cd≤1−2cdm/summationdisplay
k=1hk
for the two constants cd,Cd≥0. Then for all r,s∈[m]we have
1
Cdzr≤zs≤Cdzr.
Proof.We first show that zkdoes not change sign. To this end, assume zk+1≥0andzk≤0. Then, by
assumption
|zk+1|+|zk|≤cd[hk|zk|+hk+1|zk+1|]≤1
2|zk+1|+|zk|,
which directly implies zk=zk+1= 0. Therefore, in the following we assume without loss of generality that
zk≥0.
By symmetry it suffices to show zs≤Cdzr. To this end, assume without loss of generality r≤sand by
induction that the statement is true for all r≤k≤s−1. We show the statement for k=s. With a
telescopic sum, we have
zs−zr=s−1/summationdisplay
k=rzk+1−zk≤cds−1/summationdisplay
k=rhkzk+hk+1zk+1≤2cds/summationdisplay
k=rhkzk
≤2cd/parenleftiggs/summationdisplay
k=rhk/parenrightigg
max{zs,max
r≤k<szk}≤2cd/parenleftiggs/summationdisplay
k=rhk/parenrightigg
max{zs,Cdzr},
where in the second step we have used the first assumption of the lemma, in the third an index shift on the
hk+1zk+1summands and in the last the induction hypothesis.
We proceed with the two options for the maximum separately. In case max{zs,Cdzr}=zs, we have
zs−zr≤2cd/parenleftiggs/summationdisplay
k=rhk/parenrightigg
zs⇒ zs≤/bracketleftigg
1−2cd/parenleftiggs/summationdisplay
k=rhk/parenrightigg/bracketrightigg−1
zr≤Cdzr,
where we have solved the first inequality for zsand then estimated the bracket by the given assumptions.
By the same reasoning, in the case max{zs,Cdzr}=Cdzrwe have
zs−zr≤2cd/parenleftiggs/summationdisplay
k=rhk/parenrightigg
Cdzr⇒ zs≤/bracketleftigg
1 + 2cdCd/parenleftiggs/summationdisplay
k=rhk/parenrightigg/bracketrightigg
zr≤Cdzr,
Thus, in any case we have zs≤Cdzrand the lemma follows by induction.
Combining the last two lemmas shows that h2
r¯arµris equidistributed across multiple intervals Ir. This
requires us to bound the terms (I)−(IV), which is technical and deferred to Section A.3.5 later.
28Published in Transactions on Machine Learning Research (10/2024)
Lemma A.14. Letbr,r∈[ ¯m]be cleaned critical breakpoints (11),(12). Assume that
max/braceleftbigg
h1
2−1
p
k+∥f(3)∥Lp(Ik+), h1−1
q
k+∥f(4)∥Lq(Ik+),/bracerightbigg
≤Cmin
x∈Ik+|f′′(x)|
for some 1<q,p≤∞and some sufficiently small constant C > 0, independent of fandhr, andrcontained
in some consecutive indices I⊂[ ¯m]. Then
h2
s|¯as|µs∼h2
r|¯ar|µr
for allr,s∈I.
Proof.First note that by Lemma A.20 in the technical supplements we have ¯ar∼¯ar+1so that they cannot
change sign. Upon eventually replacing fwith−f, we may assume without loss of generality that ¯ar≥0.
Then, the result follows from Lemma A.13 with the choice zk=h2
k¯akµk. To prove its assumptions, we have
to show
h2
r+1¯ar+1µr+1−h2
r¯arµr≤cd(h2
r¯arµr)hr+cd(h2
r+1¯ar+1µr+1)hr+1
for some sufficiently small cdso that the lemmas restrictions on the constants are satisfied. By Lemma A.12,
we have
h2
r+1¯ar+1µr+1−h2
r¯arµr= (I) + (II) + (III) + (IV),
≲C(h2
r¯arµr)hr+C(h2
r+1¯ar+1µr+1)hr+1
for some terms (I)−(IV)that are bounded by Lemmas A.23, A.24, A.25, A.26. These lemmas require
h1
2−1
p
r+∥f(3)∥Lp(Ir+)≤Cmin
x∈Ir+|f′′(x)|,
h1−1
q
r+∥f(4)∥Lq(Ir+)≤Cmin
x∈Ir+|f′′(x)|,
possibly with different 1≤p,q≤∞and for sufficiently small constant C. This matches the given assumption
and concludes the proof.
The technical Lemma A.27 below shows that h5
2r|¯as|∼∥f′′∥L2/5(Ir),which allows us to conclude the proof
of the main Theorem 3.3.
Proof of Theorem 3.3. From Lemma A.14 together with the definition µr:=|¯ar|−1/5, we have the equidis-
tribution
h2
s|¯as|4
5∼h2
r|¯ar|4
5
and by Lemma A.27, we have the equivalence
h2
r|¯as|4
5= [h5
2r|¯as|]4
5∼∥f′′∥4
5
L2/5(Ir).
Combining these equivalences, the norms ∥f′′∥4
5
L2/5(Ir)are equilibrated and the result follows.
A.3.4 Technical Lemmas
This Section contains a collection of technical lemmas that are used to bound (I)−(IV)in Lemma A.12.
29Published in Transactions on Machine Learning Research (10/2024)
Lemma A.15. Let´ϕr,`ϕr,¯ϕr,˜ϕrbe defined by (25)and¯arby(28). Then
˜ar(1) = 0, ˜ar(x) =hr
60,
¯ar(1) =1
6, ¯ar(x) =hr
12+1
6br−1,
¯ar+1(1)−¯ar(1) = 0, ¯ar+1(x)−¯ar(x) =hr+1+hr
12.
Proof.From ˜ϕr=´ϕr−`ϕrand Lemma A.6, we have
⟨1,˜ϕr⟩Ir=⟨1,´ϕr⟩Ir−⟨1,`ϕr⟩Ir=hr
12−hr
12= 0,
⟨x,˜ϕr⟩Ir=⟨x,´ϕr⟩Ir−⟨x,`ϕr⟩Ir=/parenleftbiggh2
r
20+hr
12br−1/parenrightbigg
−/parenleftbiggh2
r
30+hr
12br−1/parenrightbigg
=h2
r
60.
Dividing by hrand plugging in the definition of ˜aron the left hand side shows the first two identities of the
lemma. Likewise, with the definition ¯ϕr=´ϕr+`ϕr, we have
⟨1,¯ϕr⟩Ir=⟨1,´ϕr⟩Ir+⟨1,`ϕr⟩Ir=hr
12+hr
12=hr
6,
⟨x,¯ϕr⟩Ir=⟨x,´ϕr⟩Ir+⟨x,`ϕr⟩Ir=/parenleftbiggh2
r
20+hr
12br−1/parenrightbigg
+/parenleftbiggh2
r
30+hr
12br−1/parenrightbigg
=h2
r
12+hr
6br−1,
which shows the second two identities of the lemma. It follows that
h−1
r+1⟨1,¯ϕr+1⟩Ir+1−h−1
r⟨1,¯ϕr⟩Ir=1
6−1
6= 0,
h−1
r+1⟨x,¯ϕr+1⟩Ir+1−h−1
r⟨x,¯ϕr⟩Ir=/parenleftbigghr+1
12+1
6br/parenrightbigg
−/parenleftbigghr
12+1
6br−1/parenrightbigg
=hr+1−hr
12+hr
6=hr+1+hr
12,
where we have used that br−br−1=hr. Again, plugging in the definitions of ¯aron the left hand side shows
the remaining identities of the lemma.
Lemma A.16. Let¯arand˜arbe defined by (28)andα:=−1/5. Then
˜ar+1(p) + ˜ar(p) +α[¯ar+1(p)−¯ar(p)] = 0.
for all linear p∈P1.
Proof.Since ¯ar(·)and˜ar(·)are linear, it suffices to show the lemma for p= 1andp=x. For the former by
Lemma A.15 we have
˜ar+1(1) + ˜ar(1) +α[¯ar+1(1)−¯ar(1)] = 0 + 0−α[0] = 0.
For the latter, we have
˜ar+1(x) + ˜ar(x) +α[¯ar+1(x)−¯ar(x)] =hr+1
60+hr
60+α/bracketleftbigghr+1+hr
12/bracketrightbigg
= 0,
becauseα=−1/5.
30Published in Transactions on Machine Learning Research (10/2024)
Lemma A.17. Let´ϕrand`ϕrbe defined by (25). Then for any 0<p<∞and integer s≥0theLpnorms
of thes-th derivatives are bounded by
∥´ϕ(s)
r∥Lp(Ir)∼h1
p−s
r, ∥`ϕ(s)
r∥Lp(Ir)∼h1
p−s
r,
with constants that depend on pands.
Proof.On the reference interval ˆI, we have∥´ϕ(s)∥Lp(ˆI)∼1form some constants that depend on pands.
Hence, we only need to check the scaling for the transform to the interval Irby integral substitution:
∥´ϕ(s)
r∥p
Lp(Ir)=/integraldisplay
Ir|´ϕ(s)◦T−1
r(x)h−s
r|pdx=h1−sp
r/integraldisplay
Ir|´ϕ(ˆx)|pdˆx∼h1−sp
r.
Taking the p-th root shows the claimed equivalences for ´ϕr. The result for `ϕrfollows analogously.
Lemma A.18. Let¯ar(v)and˜ar(v)be defined by (28)and1≤p≤∞. Define the joint interval Ir+:=
Ir∪Ir+1of sizehr+:=|Ir+|. Then
1.|¯ar(v)|≤h−1
pr∥v∥Lp(Ir).
2.|˜ar(v)|≤h1−1
pr∥v′∥Lp(Ir).
3.|¯ar+1(v)−¯ar(v)|≤hr+/parenleftbigg
h−1
p
r+1+h−1
pr/parenrightbigg
∥v′∥Lp(Ir+).
4.¯ar(v)≥1
6minx∈Irv(x)and□r(v)≥1
12minx∈Irv(x)for□∈{´a,`a}.
Proof.Throughout the proof define qby1/p+ 1/q= 1so that−1 + 1/q=−1/p.
1. By Hölder’s inequality and Lemma A.17 we have
|¯ar(v)|=h−1
r⟨v,´ϕr+`ϕr⟩Ir≤h−1
r∥v∥Lp(Ir)∥´ϕr+`ϕr∥Lq(Ir)
≤h−1
r∥v∥Lp(Ir)h1
qr≤h−1
pr∥v∥Lp(Ir).
2. By Lemma A.15 we have ˜ar(1) = 0. Hence, if cis the best Lpconstant approximation to v, with
standard direct approximation inequalities ((41) in the supplementary material), we obtain
|˜ar(v)|=|˜ar(v−c)|=h−1
r⟨v−c,´ϕr−`ϕr⟩Ir
≤h−1
r∥v−c∥Lp(Ir)∥´ϕr−`ϕr∥Lq(Ir)≤h−1
rhr∥v′∥Lp(Ir)h1
qr
≤h1−1
pr∥v′∥Lp(Ir).
3. By Lemma A.15 we have ¯ar+1(1)−¯ar(1) = 0. Hence, ifcis the bestLpconstant approximation to
von the joint interval Ir+:=Ir+1∪Ir, we have
|¯ar+1(v)−¯ar(v)|=|¯ar+1(v−c)−¯ar(v−c)|
≤h−1
r+1|⟨v−c,¯ϕr+1⟩Ir+1|+h−1
r|⟨v−c,¯ϕr⟩Ir|.
31Published in Transactions on Machine Learning Research (10/2024)
With standard direct approximation inequalities ((41) in the supplementary material), we estimate
the second term as before:
h−1
r|⟨v−c,¯ϕr⟩Ir|≤h−1
r∥v−c∥Lp(Ir)∥´ϕr−`ϕr∥Lq(Ir)
≤h−1
r∥v−c∥Lp(Ir+)h1
qr
≤h−1
rhr+∥v′∥Lp(Ir+)h1
qr
≤hr+h−1
pr∥v′∥Lp(Ir+).
With an analogous argument on the interval Ir+1, we obtain
|¯ar+1(v)−¯ar(v)|≤hr+/parenleftbigg
h−1
p
r+1+h−1
pr/parenrightbigg
∥v′∥Lp(Ir+).
4. By Lemma A.5 the function ¯ϕr=´ϕr+`ϕris non-negative. Therefore, by the mean value theorem
for someξ∈Ir
¯ar(v) =h−1
k⟨v,¯ϕr⟩Ir=h−1
kv(ξ)⟨1,¯ϕr⟩Ir≥h−1
kmin
x∈Irv(x)⟨1,¯ϕr⟩Ir= min
x∈Irv(x)¯ar(1) =1
6min
x∈Irv(x),
where in the last step we have used Lemma A.15. Analogously, one can show that ´ar(v)≥
1
12minx∈Irv(x)and`ar(v)≥1
12minx∈Irv(x), using that ´ar(1) =1
12by normalization Lemma A.6.
Lemma A.19. Letbr,r∈[ ¯m]be cleaned critical breakpoints (11),(12). Then
|h2
r+1−h2
r|≤12
minx∈Ir+|f′′(x)|hr+/parenleftbigg
h2−1
p
r+1+h2−1
pr/parenrightbigg
∥f′′′∥Lp(Ir+).
Proof.From Lemma A.3 we have hr⟨f′′,´ϕr⟩Ir=hr+1⟨f′′,`ϕr+1⟩Ir+1or equivalently h2
r´ar=h2
r+1`ar+1and
thereforeh2
r+1=h2
r´ar/`ar+1. This implies
h2
r+1−h2
r=/bracketleftbigg´ar
`ar+1−1/bracketrightbigg
h2
r=−1
`ar+1[`ar+1−´ar]h2
r.
We first bound [`ar+1−´ar]. To this end, note that
`ar+1(1)−´ar(1) =h−1
r+1⟨1,`ϕr+1⟩Ir+1−h−1
r⟨1,´ϕr⟩Ir=1
12−1
12= 0,
where in the second but last step we have used the normalization properties Lemma A.6. Hence, we obtain
|`ar+1−´ar|≤hr+/parenleftbigg
h−1
p
r+1+h−1
pr/parenrightbigg
∥f′′′∥Lp(Ir+),
with a proof that is identical to the same bound for |¯ar+1−¯ar|in Lemma A.18. Next, we bound
1
´ar+1≤12
minx∈Ir+1|f′′(x)|
by Lemma A.18. We conclude that
|h2
r+1−h2
r|≤12
minx∈Ir+1|f′′(x)|hr+h2
r/parenleftbigg
h−1
p
r+1+h−1
pr/parenrightbigg
∥f′′′∥Lp(Ir+),
32Published in Transactions on Machine Learning Research (10/2024)
Starting with h2
r=h2
r+1`ar+1/´arinstead ofh2
r+1=h2
r´ar/`ar+1at the beginning of the proof, we obtain the
same inequality with the term h2
rreplaced by h2
r+1. Thus, we can simplify to
|h2
r+1−h2
r|≤12
minx∈Ir+|f′′(x)|hr+/parenleftbigg
h2−1
p
r+1+h2−1
pr/parenrightbigg
∥f′′′∥Lp(Ir+),
which completes the proof.
Lemma A.20. Letbr,r∈[ ¯m]be cleaned critical breakpoints (11),(12). Assume that
h1−1
p
r+∥f′′′∥Lp(Ir+)≤Cmin
x∈Ir+|f′′(x)|
for a sufficiently small constant Cindependent of fandhr. Then
hr∼hr+1, ¯ar∼¯ar+1, µ r∼µr+1.
Proof.All equivalences in this lemma are based on the following observation: For two numbers a,b∈Rwe
have
|a−b|≤1
2max{|a|,|b|} ⇒1
2a≤b≤2a. (36)
First note that aandbhave same sign. Indeed, if a≥0andb≤0, we have
|a|+|b|≤1
2max{|a|,|b|}≤1
2(|a|+|b|),
which implies a=b= 0. Thus, without loss of generality assume a,b≥0. In case min{a,b}=a, we have
b=|b|≤|a|+|b−a|≤a+1
2a≤3
2a,
b=|b|≥|a|−|b−a|≥a−1
2a≥1
2a
and thus1
2a≤b≤3
2a. In case min{a,b}=b, analogously we have1
2b≤a≤3
2b. Rearranging this is
equivalent to2
3a≤b≤2a. Using the worst of the two cases yields the claim.
We now turn to the statements of the lemma.
1. By Lemma A.19 and the given assumptions, we have
|h2
r+1−h2
r|≤c12
minx∈Ir+|f′′(x)|hr+/parenleftbigg
h2−1
p
r+1+h2−1
pr/parenrightbigg
∥f′′′∥Lp(Ir+).
≤c12
minx∈Ir+|f′′(x)|h2
r+h1−1
p
r+∥f′′′∥Lp(Ir+)
≤cCh2
r+≤1
2max{hr,hr+1}hr+.
for sufficiently small constant C. It follows that
|hr+1−hr|=/vextendsingle/vextendsingle/vextendsingle/vextendsingleh2
r+1−h2
r
hr+1+hr/vextendsingle/vextendsingle/vextendsingle/vextendsingle≤1
2max{hr,hr+1}
and thus with (36) we obtain1
2hr≤hr+1≤hr.
33Published in Transactions on Machine Learning Research (10/2024)
2. By the first part of the lemma we have hr∼hr+1and therefore h−1/p
r +h−1/p
r+1≲h−1/p
r+. Thus, by
Lemma A.18 and the given assumptions we have
|¯ar+1−¯ar|≤chr+/parenleftbigg
h−1
p
r+1+h−1
pr/parenrightbigg
∥f′′′∥Lp(Ir+)≤ch1−1
p
r+∥f′′′∥Lp(Ir+)≤cCmin
x∈Ir+|f′′(x)|≤1
2¯ar.
for sufficiently small constant C. With (36) this implies1
2¯ar≤¯ar+1≤¯ar.
3. Sinceµr=|¯ar|α, the previous part of the lemma directly implies µr∼µr+1.
Lemma A.21. LetP∈Ps−1,1≤s∈Nbe the best linear approximation of f′′inLpon some interval
J⊃Irand¯ar,˜arbe defined in 28, 30. Then for 1≤p≤∞
|¯ar−¯ar(P)|≲|J|sh−1
pr∥f(s+2)∥Lp(J),|˜ar−˜ar(P)|≲|J|sh−1
pr∥f(s+2)∥Lp(J).
Proof.Let1/p+ 1/q= 1so that−1 + 1/q=−1/p. Then
|¯ar−¯ar(P)|=|¯ar(f′′−P)|=h−1
r⟨f′′−P,´ϕr+`ϕr⟩Ir
≤h−1
r∥f′′−P∥Lp(J)∥´ϕr−`ϕr∥Lq(Ir)≲h−1
r|J|s∥f(s+2)∥Lp(J)h1
qr
≤|J|sh−1
pr∥f(s+2)∥Lp(J).
The result for ˜arfollows analogously.
Lemma A.22. Let¯ar(v)≥0and for some β∈Rassume
hβ
r∥v′∥Lp(Ir)≤Cmin
x∈Ir+|v(x)|.
Then
min
x∈Ir|v(x)|≤¯ar.
Proof.First assume that minx∈Ir+|v(x)|= 0. With the given assumption, this implies v′(x) = 0onIrso
thatvis constant and thus zero everywhere. In this case we have ¯ar(v) = 0and the result follows.
Ifminx∈Ir+|v(x)|̸= 0, the function v(x)does not change sign and since ¯ar(v)≥0, we must have v(x)≥0
for allx∈Irbecause ¯ϕr≥0(Lemma A.5). Then the result follows directly from Lemma A.18.
A.3.5 Bounds for (I)−(IV)in Lemma A.12
In this section, we bound the terms (I)-(IV) in Lemma A.12. The bounds are of the form (I)−(IV)≤
[h2
r¯arµr]hr=:zrhr, all with one extra factor hr, which allows us to control the cumulative error for equidis-
tribution over longer distances by Lemma A.13.
Lemma A.23. Letbr,r∈[ ¯m]be cleaned critical breakpoints (11),(12)and¯ar,¯ar+1≥0. Assume that
h1−1
p
r+∥f(4)∥Lp(Ir+)≤Cmin
x∈Ir+|f′′(x)|
for some constant C > 0independent of fandhr. Then
(I) =h2
r[˜ar+1+ ˜ak+α[¯ar+1−¯ar]]µr+1≲C[h2
r¯arµr]hr+1
8[h2
r+1¯ar+1µr+1]hr+1.
34Published in Transactions on Machine Learning Research (10/2024)
Proof.First note that by Lemma A.16 for all P∈P1we have
˜ar+1(P) + ˜ar(P) +α[¯ar+1(P)−¯ar(P)] = 0.
Hence, in case minx∈Ir+|f′′(x)|= 0the given assumption implies f(4)= 0so thatf′′∈P1and(I) = 0. In
case minx∈Ir+|f′′(x)|̸= 0, it follows that with ¯ar= ¯ar(f′′), etc.,
(I) =h2
r[˜ar+1+ ˜ak+α[¯ar+1−¯ar]]µr+1
−h2
r[˜ar+1(P) + ˜ak(P) +α[¯ar+1(P)−¯ar(P)]]µr+1
=h2
r[˜ar+1(f′′−P) + ˜ak(f′′−P) +α[¯ar+1(f′′−P)−¯ar(f′′−P)]]µr+1.(37)
We choose the best Lp(Ir+)approximation for Pand estimate all terms separately. First, we have
h2
k¯ak(f′′−P)µk=h2
k[¯ak−¯ak(P)]µk≲h2
r+h2−1
pr∥f(4)∥Lp(Ir+)µk
≲Ch2
r+hrmin
x∈Ir+|f′′(x)|µr≲Ch2
r+hr¯arµr=C(h2
r¯arµr)hr,
where in the second step we have used Lemma A.21, in the third our assumptions, in the fourth |f′′(x)|≲¯ar
analogous to Lemma A.22 with minx∈Ir+|f′′(x)|̸= 0and in the second but last hr∼hr+1by Lemma A.20.
Analogously, we obtain
h2
k˜ak(f′′−P)µk≲C(h2
r¯arµr)hr,
as well as for all other combination of indices randr+ 1becauseµr∼µr+1by Lemma A.20. Using these
estimates for all four terms in (37), we obtain
(I)≲C/bracketleftbig
(h2
r¯arµr)hr+ (h2
r+1¯ar+1µr+1)hr+1/bracketrightbig
,
which shows the lemma.
Lemma A.24. Letbr,r∈[ ¯m]be cleaned critical breakpoints (11),(12)and¯ar+1≥0. Assume
h1
2−1
p
r+∥f′′′∥Lp(Ir+)≤Cmin
x∈Ir+|f′′(x)|
for some constant C > 0independent of fandhr. Then
(II) = [h2
r+1−h2
r]˜ar+1µr+1≲C[h2
r+1¯ar+1µr+1]hr+1.
Proof.By Lemma A.19 and the given assumptions, we have
|h2
r+1−h2
r|≲12
minx∈Ir+|f′′(x)|hr+/parenleftbigg
h2−1
p
r+1+h2−1
pr/parenrightbigg
∥f′′′∥Lp(Ir+)≲Ch5
2
r+1,
where in the last step we have used hr+1∼hrby Lemma A.20. From Lemma A.18, the given assumptions,
and|f′′(x)|≤¯ar+1(Lemma A.22), we have
|˜ar+1|≲h1−1
p
r+1∥f′′′∥Lp(Ir+1)≲Ch1
2
r+1min
x∈Ir+|f′′(x)|≲Ch1
2
r+1¯ar+1.
Combining these two inequalities yields the lemma.
Lemma A.25. Letbr,r∈[ ¯m]be cleaned critical breakpoints (11),(12)and¯ar≥0. Assume
h1
2−1
p
r+∥f′′′∥Lp(Ir+)≤Cmin
x∈Ir+|f′′(x)|
for some constant 0<C≤1independent of fandhr. Then
(III) =α2h2
r¯aα−1
r[¯ar+1−¯ar]2≲C[h2
r¯arµr]hr.
35Published in Transactions on Machine Learning Research (10/2024)
Proof.From Lemma A.18, the given assumptions, hr+1∼hr(Lemma A.20) and |f′′(x)]≤¯ar(Lemma
A.22), we have
|¯ar+1−¯ar|≲hr+/parenleftbigg
h−1
p
r+1+h−1
pr/parenrightbigg
∥f′′′∥Lp(Ir+)≲Cmin
x∈Ir+|f′′(x)|≲Ch1
2r¯ar.
Thus, with µr= ¯aα
r, we have
(III) =α2h2
r¯aα−1
r[¯ar+1−¯ar]2≲α2h2
r¯aα−1
rC2hr¯a2
r≲Ch2
r¯ar¯aα
rhr=C(h2
r¯arµr)hr.
This completes the proof.
Lemma A.26. Letbr,r∈[ ¯m]be cleaned critical breakpoints (11),(12)and¯ar,¯ar+1≥0. Assume
h1
2−1
p
r+∥f′′′∥Lp(Ir+)≤Cmin
x∈Ir+|f′′(x)|
for some constant 0≤C≤1independent of fandhr. Then
(IV) =αh2
r[¯ar+1−¯ar]Rr+h2
r¯arRr≲C(hr¯arµr)hr.
withRrdefined in Lemma A.12.
Proof.Recall that Rris defined by
Rr:=α(α−1)[ξr¯ar+ (1−ξr)¯ar+1]α−2[¯ar+1−¯ar]2
for some 0≤ξr≤1. From Lemma A.18, the given assumptions, hr+1∼hr(Lemma A.20) and |f′′(x)]≤¯ar
(Lemma A.22), we have
|¯ar+1−¯ar|≲hr+/parenleftbigg
h−1
p
r+1+h−1
pr/parenrightbigg
∥f′′′∥Lp(Ir+)≲Ch1
2rmin
x∈Ir+|f′′(x)|≲Ch1
2r¯ar.
From Lemma A.20 we have ¯ar∼¯ar+1and thus
ξr¯ar+ (1−ξr)¯ar+1∼¯ar.
Combining these estimates, with µr= ¯aα
r, we obtain
h2
r¯arRr≲h2
r¯ar¯aα−2
rC2hr¯a2
r≲C2[h2
r¯arµr]hr.
as well as
h2
r[¯ar+1−¯ar]Rr≲h2
rCh1
2r¯ar¯aα−2
rC2hr¯a2
r≲C3[h2
r¯arµr]h3
2r,
SinceC≤1, we haveC2,C3≤C, which proves the lemma.
A.3.6 Norm Equivalences
This section contains the equivalences of h2
r¯arµrandLpnorms.
Lemma A.27. Let1< p≤∞and0< q≤∞. LetP∈P0be theLp(Ir)best constant approximation of
some function gand assume
h1−1
p
r+∥g′∥Lp(Ir+)≤Cmin
x∈Ir+|g(x)|
for some sufficiently small constant C > 0independent of fandhr. Then
1.¯ar(g)∼¯ar(P).
36Published in Transactions on Machine Learning Research (10/2024)
2.∥g∥Lq(Ir)∼∥P∥Lq(Ir).
3.∥g∥Lq(Ir)∼h1
qr|¯ar(g)|.
Proof.Recall from the proof of Lemma A.20 that for two numbers a,b∈Rwe have
|a−b|≤1
2max{a,b} ⇒ a∼b. (38)
1. In case ¯ar≥0, by Lemma A.21 and |g(x)|≤¯ar(Lemma A.22) we have
|¯ar(g)−¯ar(P)|≤h1−1
pr∥g′∥Lp(Ir)≲Cmin
x∈Ir+|g(x)|≲C¯ar
For sufficiently small C, with (38) this implies ¯ar(g)∼¯ar(P). The case ¯ar(g)≤0follows by
replacinggwith−g.
2. We first consider the case 1≤q <∞. Using direct approximation inequalities ((41) in the supple-
mentary material), we have
/vextendsingle/vextendsingle∥g∥Lq(Ir)−∥P∥Lq(Ir)/vextendsingle/vextendsingle≤∥g−P∥Lq(Ir)≲h1+1
q−1
pr∥g′∥Lp(Ir)
≤Ch1
qrmin
x∈Ir+|g(x)|≤C/bracketleftbigg/integraldisplay
Ir|g(x)|qdx/bracketrightbigg1
q
≤C∥g∥Lq(Ir),
which implies∥g∥Lq(Ir)∼∥P∥Lq(Ir)with (38) for sufficiently small C.
In caseq<1, by Taylor’s theorem, we have uq−vq=q(ξu+ (1−ξ)v)q−1[u−v]for some 0≤ξ≤1.
Withu=|g(x)|andv=|P|, this implies
∥g∥q
Lq(Ir)−∥P∥q
Lq(Ir)=/integraldisplay
Ir|g(x)|q−|P(x)|qdx
=/integraldisplay
Irq[ξ(x)|g(x)|−(1−ξ(x))|P(x)|]q−1[|g(x)|−|P(x)|]dx
≲min
x∈Ir|g(x)|q−1∥g−P∥L1(Ir)
≤min
x∈Ir|g(x)|q−1h1−1
pr∥g−P∥Lp(Ir)
≤min
x∈Ir|g(x)|q−1h2−1
pr∥g′∥Lp(Ir)
≤Cmin
x∈Ir|g(x)|q−1hrmin
x∈Ir|g(x)|
≤Chrmin
x∈Ir|g(x)|q
≤C/integraldisplay
Ir|g(x)|qdx
≤C∥g(x)∥q
Lq(Ir),
where in the third step we have used q−1<0and thatP=g(η)⇒|P|≥minx∈Ir|g(x)|for some
η∈Ir, as can easily be seen by by first order optimality criteria and the mean value theorem. In
the fourth step we have used that ∥·∥L1(Ir)≤h1−1
pr∥·∥Lp(Ir)by Hölder’s inequality and in the sixth
the given assumptions. Again with (38) this implies ∥g∥q
Lq(Ir)∼∥P∥q
Lq(Ir)for sufficiently small C
and thus the statement of the lemma by taking the q-th root.
37Published in Transactions on Machine Learning Research (10/2024)
3. We first show the desired identities for Pinstead ofg. Indeed, we have
∥P∥Lq(Ir)=h1
qr|P|,
h1
qr¯ar(P) =h1
qr[h−1
r⟨P,¯ϕr⟩Ir] =1
12h1
qrP.
With the first two equivalences of this lemma, this implies
∥g∥Lq(Ir)∼∥P∥Lq(Ir)=h1
qr|P|∼h1
qr|¯ar(P)|∼h1
qr|¯ar(g)|,
which concludes the proof.
A.4 Approximation
In this section, we prove the main approximation result, restated here for convenience:
Theorem A.28 (Theorem 3.4, restated) .Letθbe a critical point (11), with cleaned breakpoints in ascending
order 12. For r,s∈{2,..., ¯m}, letI={r,r+ 1,...,s}be a set of consecutive neurons with DI:=/uniontext
k∈IIk
and
max/braceleftbigg
h1
2−1
p
k+∥f(3)∥Lp(Ik+), h1−1
q
k+∥f(4)∥Lq(Ik+),/bracerightbigg
≤Cmin
x∈Ik+|f′′(x)| (39)
for some 1<q,p≤∞and some sufficiently small constant C > 0independent of fandhk. Then
∥fθ−f∥L2(DI)≲|I|−2∥f′′∥L2/5(DI). (40)
Since we have already established equidistribution in Theorem 3.3, the approximation results is standard.
Proof.We first split the L2norm
∥fθ−f∥2
L2(DI)=/summationdisplay
r∈I∥fθ−f∥2
L2(Ir)
=/summationdisplay
r∈I/bracketleftig
∥fθ−f∥2
5
L2(Ir)/bracketrightig5
=/summationdisplay
r∈I/bracketleftigg
1
|I|/summationdisplay
s∈I∥fθ−f∥2
5
L2(Ir)/bracketrightigg5
,
where in the last step we have inserted an artificial sum for later use. By (22) and the discussion thereafter
on each interval Irthe neural network is a best linear approximation and therefore by standard direct
approximation results ((41) in the supplementary material), we have
∥fθ−f∥2
L2(DI)≲/summationdisplay
r∈I/bracketleftigg
1
|I|/summationdisplay
s∈Ih2
5(2+1
2−1)
r∥f′′∥2
5
L1(Ir)/bracketrightigg5
.
By Lemma A.27, we have
h3
5∥f∥2
5
L1(Ir)∼h3
5[hr|¯ar|]2
5= [h5
2r|¯as|]2
5∼∥f′′∥2
5
L2/5(Ir)
and therefore
∥fθ−f∥2
L2(DI)≲/summationdisplay
r∈I/bracketleftigg
1
|I|/summationdisplay
s∈I∥f′′∥2
5
L2/5(Ir)/bracketrightigg5
.
38Published in Transactions on Machine Learning Research (10/2024)
As a side remark, we could have used ∥fθ−f∥L2(Ir)≲∥f∥B2
2/5(L2/5(Ir))directly if we would use Besov
norms. Anyways, note that the sum depends on s, but the summands depend on r, which we fix with
equidistribution∥f′′∥L2/5(Ir)∼∥f′′∥L2/5(Is)from Theorem 3.3. Then
∥fθ−f∥2
L2(DI)≲/summationdisplay
r∈I/bracketleftigg
1
|I|/summationdisplay
s∈I∥f′′∥2
5
L2/5(Is)/bracketrightigg5
=/summationdisplay
r∈I/bracketleftbigg1
|I|∥f′′∥2
5
L2/5(DI)/bracketrightbigg5
=1
|I|4∥f′′∥2
L2/5(DI),
which concludes the proof.
B Technical Supplements
B.1 Besov Spaces
For integer s≥0and1≤p≤∞, Sobolev norms are defined by
∥f∥p
Ws,p(D):=s/summationdisplay
r=0|f|p
Ws,p(D), |f|Ws,p(D):=∥f(r)∥L2(D)
For Besov norms, define the difference operators (∆1
hf)(x) :=f(x+h)−f(x)and∆r
h:= ∆1
h∆r−1
h, extended
by zero in case x+h̸∈D, and ther-th order modulus of smoothness
ωr(f,t)p:= sup
|h|≤t∥∆r
hf∥Lp(D).
Then for 0<p,q<∞, and the smallest integer r>s, the Besov norms are defined by
∥f∥Bsq(Lp(Ω)):=∥f∥Lp(D)+|f|Bsq(Lp(Ω)),|f|Bsq(Lp(Ω)):=/braceleftbigg/integraldisplay∞
0/bracketleftbig
t−sωr(f,t)p/bracketrightbigqdt
t/bracerightbigg1
q
.
See DeVore & Lorentz (1993); DeVore (1998) for details.
B.2 Direct Approximation Estimates
For the best Lpapproximation with polynomials Pr−1of degree at most r−1on interval Iit is well known
that
inf
p∈Pr−1∥f−p∥Lp(I)≲|I|r+1
p−1
q∥f(r)∥Lq(I) (41)
for allr>0and1≤p,q≤∞withr+1
p−1
q>0. See e.g. DeVore (1998), (6.9).
B.3 Main Results with Besov Norms
In the main Theorem 3.4 we use the Sobolev type norm ∥f′′∥Lq(DI), which is unusual for q:= 2/5<1.
This is permissible, because the assumptions (13) requires higher weak derivatives in regular Lpnorms with
1≤p≤∞. In this section, we consider a similar result in Besov norms. These allow a larger range of q,p< 1
in the assumptions. Up to an arbitrarily small discrepancy in smoothness, the approximation bounds use
the same norms than classical adaptive approximation in (5).
39Published in Transactions on Machine Learning Research (10/2024)
Theorem B.1. Letθbe a critical point (11), with cleaned breakpoints in ascending order 12. For r,s∈
{2,..., ¯m}, letI={r,r+ 1,...,s}be a set of consecutive neurons with DI:=/uniontext
k∈IIkand assume
h1
2−1
o
k∥(∆2
tf)′∥Lo(Ik+)≤Cmin
x∈Ik+|(∆2
tf)(x)|, (42)
h1
2−1
p
k+|f′′|B1p(Lp(Ik+))≤Cmin
x∈Ik+|f′′(x)|̸= 0, (43)
h1−1
q
k+|f′′|B2q(Lq(Ik+)),≤Cmin
x∈Ik+|f′′(x)|̸= 0, (44)
uniformly for all t >0,o= 2/5, some 1≤o≤∞, some1
2≤p≤∞, some1
3≤q≤∞and a sufficiently
small constant C > 0independent of fandhk. Then
∥fθ−f∥L2(DI)≲|I|−2|f|Bsq(Lq(Ir))
for everys<2.
Proof.The result is proven analogously to Theorem 3.4 with a few small changes that we point out in the
following.
1.Assumptions: Assumptions (43), (44) yield
max/braceleftbigg
h1
2−1
p
k+|f′′|B1p(Lp(Ik+)), h1−1
q
k+|f′′|B2q(Lq(Ik+)),/bracerightbigg
≤Cmin
x∈Ik+|f′′(x)|̸= 0,
analogous to 13 with Sobolev norms replaced by Besov norms, which allow the larger ranges1
2≤
p≤∞and1
3≤q≤∞. Reconsidering the proof of Theorem 3.4, the non-zero condition on the
left hand side ensures the second case in the proof of Lemma A.22. Then, we replace the use of the
approximation inequality (41) with
inf
p∈Pr−1∥f−p∥Lp(I)≲|I|r+1
p−1
q∥f∥Brq(Lq(I))
withr >0andr+1
p−1
q>0, which remains true in case q <1, see e.g. DeVore (1998), (6.8).
We make this replacement in the proofs of Lemmas A.18, A.19, A.21 and A.27, where we obtain
minimalp,qif we approximate in the L1norm after applying Hölder’s inequality.
2.Conclusion: By assumption (42) and Lemma A.27, for any 0<ρ<∞ands<2by we have
h1
qrω2(f,t)q∼h1
prω2(f,t)p⇒ h1
qr|f|Bsρ(Lq(Ω))∼h1
pr|f|Bsρ(Lp(Ω)).
For1≤p≤∞, it is well known that Sobolev and Besov spaces are closely related. Using this in
the second step below and Hölder’s inequality in the first, we have
∥f′′∥Lq(Ir)≤h1
q−1
pr∥f′′∥Lp(Ir)≤h1
q−1
pr|f|Bsq(Lp(Ir))∼|f|Bsq(Lq(Ir)).
Plugging this into the approximation bounds of Theorem 3.4, we obtain
∥fθ−f∥L2(DI)≲|I|−2∥f′′∥L2/5(DI)≲|I|−2|f|Bsq(Lq(Ir))
for anys<2, which concludes the proof.
40Published in Transactions on Machine Learning Research (10/2024)
B.4 Networks in Inner Weights
In this section we prove Lemma 3.1. Without loss of generality, we use the domain D= [−1,1]. We first
show an abstract characterization of critical points, similar to the proof sketch in Section 4.
Lemma B.2. For an arbitrary function θ→fθ∈L2(D), the weights θ∈Rmare a critical point of the loss
∥fθ−f∥2
L2(D)if and only if
⟨fθ−f,v⟩= 0, v ∈span{∂θrfθ:r∈[m]}.
Proof.The critical points are given by
⟨fθ−f,∂θrfθ⟩= 0, r∈[m].
Clearly, the condition of the lemma implies the critical point condition. In the other direction, taking linear
combinations of the critical point condition directly implies the condition of the lemma.
Proof of Lemma 3.1. We first show that fW,V,Bandfw,brepresent the same functions and then relate their
critical points.
1.Construction of fw,b:We use the property σ(ax) =aσ(x)for alla≥0of ReLU activations to
rewriteFW,V,Bas
FW,V,B :=/parenleftigg
B0+/summationdisplay
Vr=0Wrσ(Br)/parenrightigg
+W0x+m/summationdisplay
Wr,Vr̸=0Wr|Vr|σ/parenleftbigg
sign(Vr)x−Br
|Vr|/parenrightbigg
.
This is already in the same format as fw,b, except for the term sign(Vr)inside the activation. We
can easily eliminate it with the formula
σ(−x+b) =σ(x−b)−(x−b) (45)
and obtain
FW,V,B :=/parenleftigg
B0+/summationdisplay
Vr=0Wrσ(Br)−/summationdisplay
Vr<0WrBr/parenrightigg
+/parenleftigg
W0−/summationdisplay
Vr<0Wr|Vr|/parenrightigg
x+m/summationdisplay
Wr,Vr̸=0Wr|Vr|σ/parenleftbigg
x−Br
Vr/parenrightbigg
.
after rearranging terms. Note that the first two parenthesis are constant and thus, we can find
fw,bby matching terms. The last formula also shows that the breakpoints are Br/Vrfor allrwith
nonzeroWrandVr.
2.Critical Points: LetFW,V,Bbe a critical point and fw,bbe the corresponding network constructed
above. We show that the latter is also a critical point for optimization of wandb. To this end,
define the linear spaces
Xf:= span{∂□fw,b:□∈{wr,br}, r∈[m]}.
XF:= span{∂□fW,V,B :□∈{Wr,Vr,Br}, r∈[m]}.
Sincefw,b=FW,V,B, by Lemma B.2 it suffices to show that Xf⊂XF. First note that
∂b0fw,b= 1 =∂B0FW,V,B∈XF
∂w0fw,b=x=∂W0FW,V,B∈XF.
Forr>0, we have
∂wrfw,b=σ/parenleftbigg
x−Br
Vr/parenrightbigg
∈span/braceleftbigg
1,x,1
|Vr|σ(Vrx−Br)/bracerightbigg
= span{∂B0FW,V,B,∂W0FW,V,B,∂WrFW,V,B}⊂XF,
41Published in Transactions on Machine Learning Research (10/2024)
where in the first span we have used (45) if Vr<0and by construction we know that Wr,Vr̸= 0.
Analogously, we obtain
∂brfw,b=wr˙σ/parenleftbigg
x−Br
Vr/parenrightbigg
∈span{1,˙σ(Vrx−Br)}= span{∂B0FW,V,B,∂BrFW,V,B} ⊂XF,
Thus, all partial derivatives of fw,bare contained in XFso thatXf⊂XF, which concludes the
proof
42