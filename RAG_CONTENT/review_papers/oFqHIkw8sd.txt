Under review as submission to TMLR
Multi-Grid Tensorized Fourier Neural Operator for High-
Resolution PDEs
Anonymous authors
Paper under double-blind review
Abstract
Memory complexity and data scarcity have so far prohibited learning solution operators of
partial differential equations ( PDE) at high resolutions. We address these limitations by
introducing a new data efficient and highly parallelizable operator learning approach with
reduced memory requirement and better generalization, called multi-grid tensorized neu-
ral operator ( MG-TFNO ).MG-TFNO scales to large resolutions by leveraging local and
global structures of full-scale, real-world phenomena, through a decomposition of both the
input domain and the operator’s parameter space. Our contributions are threefold: i) we
enable parallelization over input samples with a novel multi-grid-based domain decompo-
sition, ii) we represent the parameters of the model in a high-order latent subspace of the
Fourier domain, through a global tensor factorization, resulting in an extreme reduction in
the number of parameters and improved generalization, and iii) we propose architectural
improvements to the backbone FNO. Our approach can be used in any operator learning
setting. We demonstrate superior performance on the turbulent Navier-Stokes equations
where we achieve less than half the error with over 150 ×compression. The tensorization
combined with the domain decomposition, yields over 150 ×reduction in the number of
parameters and 7×reduction in the domain size without losses in accuracy, while slightly
enabling parallelism.
1 Introduction
Real-worldscientificcomputingproblemsoftentimerequirerepeatedlysolvinglarge-scaleandhigh-resolution
partial differential equations ( PDEs). For instance, in weather forecasts, large systems of differential equa-
tions are solved to forecast the future state of the weather. Due to internal inherent and aleatoric uncertain-
ties, multiple repeated runs are carried out by meteorologists every day to quantify prediction uncertainties.
Conventional PDEsolvers constitute the mainstream approach used to tackle such computational problems.
However, these methods are known to be slow and memory-intensive. They require an immense amount of
computingpower, areunabletolearnandadaptbasedonobserveddata, andoftentimesrequiresophisticated
tuning (Slingo & Palmer, 2011; Leutbecher & Palmer, 2008; Blanusa et al., 2022).
Neural operators are a new class of models that aim at tackling these challenging problems (Li et al.,
2020a). They are maps between function spaces whose trained models emulate the solution operators of
PDEs (Kovachki et al., 2021b). In the context of PDEs, these deep learning models are orders of magnitude
faster than conventional solvers, can easily learn from data, can incorporate physically relevant information,
and recently enabled solving problems deemed to be unsolvable with the current state of available PDE
methodologies (Liu et al., 2022; Li et al., 2021c). Among neural operator models, Fourier neural operators
(FNOs), in particular, have seen successful application in scientific computing for the task of learning the
solution operator to PDEs as well as in computer vision for classification, in-painting, and segmentation
(Li et al., 2021b; Kovachki et al., 2021a; Guibas et al., 2021). By leveraging spectral theory, FNOs have
successfully advanced frontiers in weather forecasts, carbon storage, and seismology (Pathak et al., 2022;
Wen et al., 2022; Yang et al., 2021).
1Under review as submission to TMLR
FNO
FNO [Ours]
TFNO [Ours]67.14M
67.18M
3.56M
0.4% 0.6% 0.8% 1.0% 1.2% 1.4%1.0%1.5%2.0%2.5%3.0%3.5%4.0%4.5%5.0%
l2 (lower is better)h1 (lower is better)
Figure 1:Comparison of the performance on the relative L2andH1test errors (lower is better)
onalog-scale ofourapproach,comparedwithbothourimprovedbackbone( FNO)andtheoriginalFNO,on
Navier-Stokes. Our approach enables large compression for both input and parameter, while outperforming
regular FNO.
While FNOs have shown tremendous speed-up over classical numerical methods, their efficacy can be limited
due to the rapid growth in memory needed to represent complex operators. In the worst case, large memory
complexity is required and, in fact, is unavoidable due to the need for resolving fine-scale features globally.
However, many real-world problems, possess a local structure not currently exploited by neural operator
methods. For instance, consider a weather forecast where predictions for the next hour are heavily dependent
on the weather conditions in local regions and minimally on global weather conditions. Incorporating and
learningthislocalstructureoftheunderlying PDEsisthekeytoovercomingthecurseofmemorycomplexity.
In this work, we propose a new, scalable neural operator that addresses these issues by leveraging the
structure in both the domain space and the parameter space, Figure 2. Specifically, we introduce the multi-
grid tensor operator ( MG-TFNO ), a model that exploits locality in physical space by a novel multi-grid
domain decomposition approach to compress the input domain size by up to 7×while leveraging the global
interactions of the model parameters to compress them by over 100×without any loss of accuracy.
In the input space , to predict the solution in any region of the domain, MG-TFNO decomposes the input
domain into small local regions to which hierarchical levels of global information are added in a multi-grid
fashion. Since a local prediction depends most strongly on its immediate spatial surroundings, the farther
field information is downsampled to lower resolutions, progressively, based on its distance from the region of
interest. Thus, MG-TFNO allows parallelization over the input domain as it relies on high-resolution data
only locally and coarse-resolution data globally. Due to its state-of-the-art performance on PDEproblems
and efficient FFT-based implementation, we use the FNOas the backbone architecture for our method. It
is worth noting that the multi-grid approach is readily amendable to neural network settings and, moreover,
any other neural operator architecture can be used in place of FNOas a backbone.
In the parameter space , we exploit the spatiotemporal structure of the underlying PDEsolution operator
by parameterizing the convolutional weights within the Fourier domain with a low-rank tensor factorization.
2Under review as submission to TMLR
Figure 2: Overview of our approach . First (left), a multi-grid approach is used to create coarse to fine
inputs that capture high-resolution details in a local region while still encoding global context. The resulting
regions are fed to a tensorized Fourier operator (middle), the parameters of which are jointly represented
in a single latent space via a low-rank tensor factorization (here, a Tucker form). Here Fdenotes Fourier
transform. Finally, the outputs (right) are stitched back together to form the full result. Smoothness in the
output is ensured via the choice of the loss function.
Specifically, we impose a coupling between all the weights in the Fourier space by jointly parameterizing them
withasingletensor, learnedinafactorizedformsuchasTuckerorCanonical-Polyadic(Kolda&Bader,2009).
This coupling allows us to limit the number of parameters in the model without limiting its expressivity. On
the contrary, this low-rank regularization on the model mitigates over-fitting and improves generalization.
Intuitively, our method can be thought of as a fully-learned implicit scheme capable of converging in a small,
fixed number of iterations. Due to the global nature of the integral kernel transform, the FNOavoids the
Courant–Friedrichs–Lewy (CFL) condition plaguing explicit schemes, allowing convergence in only a few
steps (Courant et al., 1928). Our weight coupling ensures maximum communication between the steps,
mitigating possible redundancies in the learned kernels and reducing the complexity of the optimization
landscape.
In summary, we make the following contributions:
•We propose architectural improvements to the backbone which we validated through thor-
ough ablations.
•We propose MG-TFNO , a novel neural operator parameterized in the spectral domain by a single
low-rank factorized tensor, allowing its size to grow linearly with the size of the problem.
•Our tensor operator achieves better performance with a fraction of the parameters :
we outperform FNOon solving the turbulent Navier Stokes equations with more than 400×weight
compression ratio, Figure 6.
•Our method overfits less and does better in the low-data regime . In particular, it outper-
forms FNOwith less than half the training samples, Figure 8.
•We introduce a novel multi-grid domain decomposition approach , a technique which allows
the operator to predict the output only on local portions of the domain, thus reducing the memory
usage by an order of magnitude with no performance degradation.
•Combining tensorization with multi-grid domain decomposition leads to MG-TFNO ,
which is more efficient in terms of task performance, computation, and memory. MG-TFNO achieves
2.5×lower error with 10×model weight compression, and 1.8×domain compression.
3Under review as submission to TMLR
•A unified codebase to run all configurations and variations of FNOandMG-TFNO will be
released, along with the Navier-Stokes data used in this paper.
2 Background
Here, we review related works and introduce the background necessary to explain our approach.
Many physical phenomena are governed by PDEs and a wide range of scientific and engineering computation
problems are based on solving these equations. In recent years, a new perspective to PDEs dictates to
formulate these problems as machine learning problems where solutions to PDEs are learned. Prior works
mainly focused on using neural networks to train for the solution map of PDEs (Guo et al., 2016; Zhu &
Zabaras, 2018; Adler & Oktem, 2017; Bhatnagar et al., 2019; Gupta et al., 2021). The use of neural networks
in the prior works limits them to a fixed grid and narrows their applicability to PDEs where maps between
function spaces are desirable. Multiple attempts have been made to address this limitation. For example
mesh free methods are proposed that locally output mesh-free solution (Lu et al., 2019; Esmaeilzadeh et al.,
2020), but they are still limited to fixed input gird.
A new deep learning paradigm, neural operators, are proposed as maps between function spaces (Li et al.,
2020a; Kovachki et al., 2021b). They are discretization invariants maps. The input functions to neural
operators can be presented in any discretization, mesh, resolution, or basis. The output functions can be
evaluatedatanypointinthedomain. VariantsofneuraloperatorsdeployavarietyofNyströmapproximation
to develop new neural operator architecture. Among these, multi-pole neural operators (Li et al., 2020b)
utilize the multi-pole approach to develop computationally efficient neural operator architecture. Inspired by
thespectralmethod,Fourier-basedneuraloperatorsshowsignificantapplicabilityinpracticalapplications(Li
et al., 2021b; Yang et al., 2021; Wen et al., 2022; Rahman et al., 2022a), and the architectures have been
used in neural networks for vision and text tasks (Guibas et al., 2021; Dao et al., 2022). Principle component
analysis and u-shaped methods are also considered (Bhattacharya et al., 2020; Liu et al., 2022; Rahman et al.,
2022b; Yang et al., 2022). It is also shown that neural operators can solely be trained using PDEs, resulting
in physics-informed neural operators, opening new venues for hybrid data and equation methods (Li et al.,
2021c) to tackle problems in scientific computing.
Decomposing the domain in smaller subdomains is at the core of many methods in computational sci-
ences(Chan & Mathew, 1994) and extensively developed in deep learning (Dosovitskiy et al., 2020). Prior
deeplearningmethodsonneuralnetworksproposetodecomposetheinputfinitedimensionvectortomultiple
patches, accomplish local operations, and aggregate the result of such process in the global sense (Dosovitskiy
et al., 2020; Guibas et al., 2021). Such methods do not decompose the output domain and directly predict
the entire output vector. In contrast, MG-TFNO works on function spaces, and not only decomposes the
input domain, but also decomposes the domain of the output functions, and separately predicts the output
at each subdomain.
As we move beyond learning from simple structures to solving increasingly complex problems, the data we
manipulate becomes more structured. To efficiently manipulate these structures, we need to go beyond
matrix algebra and leverage the spatiotemporal structure. For all purposes of this paper, tensors are multi-
dimensional arrays and generalize the concept of matrices to more than 2 modes (dimensions). For instance,
RGB images are encoded as third-order (three-dimensional) tensors, videos are 4thorder tensors and so on
and so forth. Tensor methods generalize linear algebraic methods to these higher-order structures. They
have been very successful in various applications in computer vision, signal processing, data mining and
machine learning (Panagakis et al., 2021; Janzamin et al., 2019; Sidiropoulos et al., 2017; Papalexakis et al.,
2016).
Using tensor decomposition Kolda & Bader (2009), previous works have been able to compress and improve
deep networks for vision tasks. Either a weight matrix is tensorized and factorized Novikov et al. (2015), or
tensor decomposition is directly to the convolutional kernels before fine-tuning to recover-for lost accuracy,
which also allows for an efficient reparametrization of the network (Lebedev et al., 2015; Kim et al., 2016;
Gusak et al., 2019). There is a tight link between efficient convolutional blocks and tensor factorization and
factorized higher-order structures (Kossaifi et al., 2020). Similar strategies have been applied to multi-task
4Under review as submission to TMLR
Variable Meaning Dimensionality
T Tensor of weights in the Fourier domain Covα×···×α×m×n
W Weight tensor parameterizing the entire operator Covα×···×α×n×n×2d−1L
A Input function space Infinite
U output function space Infinite
a Input function Infinite
u Output function Infinite
DA Domain of function a d
DU Domain of function u d
dADimension of the co-domain of the input functions 1
dUDimension of the co-domain of the output functions 1
F Fourier transform Infinite
F−1Fourier transform Infinite
L Number of integral operation layers In N
l Layer index Between 1andL
σ Point-wise activation operation Infinite
b Bias vector
v Function at each layer Infinite
α Number of kept frequencies in Fourier space Between 1and1
2min{s1,···,sd}
Table 1:Table of notation
learning (Bulat et al., 2020a) and NLP (Papadopoulos et al., 2022; Cordonnier et al., 2020). Of all these
prior works, none has been applied to neural operator. In this work, we propose the first application of
tensor compression to learning operators and propose a Tensor OPerator ( TFNO).
3 Methodology
Here, we briefly review operator learning as well as the Fourier Neural Operator, on which we build to
introduce our proposed Tensor OPerator ( TFNO) as well as the Multi-Grid Domain Decomposition, which
together form our proposed MG-TFNO .
3.1 Operator Learning
LetA:={a:DA→RdA}andU:={u:DU→RdU}denote two input and output function spaces
respectively. Each function a, in the input function space A, is a map from a bounded, open set DA⊂Rd
to thedA-dimensional Euclidean space. Any function in the output function space Uis a map from a
bounded open set DU⊂Rdto thedU-dimensional Euclidean space. In this work we consider the case
D=DA=DU⊂Rd.
We aim to learn an operator G:A→Uwhich is a mapping between the two function spaces. In particular,
given a dataset of Npoints{(aj,uj)}N
j=1, where the pair (aj,uj)are functions satisfying G(aj) =uj, we build
an approximation of the operator G. As a backbone operator learning model, we use neural operators as they
are consistent and universal learners in function spaces. For an overview of theory and implementation, we
refer the reader to Kovachki et al. (2021b). We specifically use the FNOand give details in the forthcoming
section (Li et al., 2021b).
3.2 Notation
We summarize the notation used throughout the paper in Table 1.
5Under review as submission to TMLR
3.3 Fourier Neural Operators
For simplicity, we will work on the d-dimensional unit torus Tdand first describe a single, pre-activation
FNOlayer mapping Rm-valued functions to Rn-valued functions. Such a layer constitutes the mapping
G:L2(Td;Rm)→L2(Td;Rn)defined as
G(v) =F−1/parenleftbig
F(κ)·F(v)/parenrightbig
,∀v∈L2(Td;Rm) (1)
whereκ∈L2(Td;Rn×m)isafunctionconstitutingthelayerparametersand F,F−1aretheFouriertransform
and its inverse respectively. The Fourier transform of the function κis parameterized directly by some fixed
number of Fourier nodes denoted α∈N.
To implement equation 1, F,F−1are replaced by the discrete fast Fourier transforms ˆF,ˆF−1. Let ˆv∈
Rs1×···×sd×mdenote the evaluation of the function von a uniform grid discretizing Tdwithsj∈Npoints
in each direction. We replace F(κ)with a weight tensor T∈Covs1×···×sd×n×mconsisting of the Fourier
modes ofκwhich are parameters to be learned. To ensure that κis parameterized as a Rn×m-valued
function with a fixed, maximum amount of wavenumbers α <1
G2min{s1,···,sd}that is independent of
the discretization of Td, we leave as learnable parameters only the first αentries of Tin each direction and
enforce that Thave conjugate symmetry. In particular, we parameterize half the corners of the d-dimensional
hyperrectangle with 2d−1hypercubes with length size α. That is, Tis made up of the free-parameter tensors
˜T1,···,˜T2d−1∈Covα×···×α×n×msituated in half of the corners of T. Each corner diagonally opposite of
a tensor ˜Tjis assigned the conjugate transpose values of ˜Tj. All other values of Tare set to zero. This
is illustrated in the middle-top part of Figure 2 for the case d= 2with ˜T1and˜T2. We will use the
notation T(k,···) =˜Tkfor anyk∈[2d−1]. The discrete version of equation 1 then becomes the mapping
ˆG:Rs1×···×sd×m→Rs1×···×sd×ndefined as
ˆG(ˆv) =ˆF−1/parenleftbig
T·ˆF(ˆv)/parenrightbig
,∀ˆv∈Rs1×···×sd×m(2)
where the·operation is simply the matrix multiplication contraction along the last dimension. Specifically,
we have
/parenleftbig
T·ˆF(ˆv)/parenrightbig
(l1,...,ld,j) =m/summationdisplay
i=1T(l1,...,ld,j,i)/parenleftbigˆF(ˆv)/parenrightbig
(l1,...,ld,i). (3)
From equation 2, a full FNO layer is build by adding a point-wise linear action to ˆv, a bias term, and applying
a non-linear activation. In particular, from an input ˆv∈Rs1×···×sd×m, the output ˆq∈Rs1×···×sd×nis given
as
ˆq(l1,···,ld,:) =σ/parenleftbig
Qˆv(l1,···,ld,:) +ˆG(ˆv) +b/parenrightbig
withσ:R→Ra fixed, non-linear activation, and b∈Rn,Q∈Rn×m,˜T1,···,˜T2d−1∈Covα×···×α×n×mare
the learnable parameters of the layer. The full FNO model consists of L∈Nsuch layers each with weight
tensors T1,···,TLthat have learnable parameters ˜T(l)
k=Tl(k,···)for anyl∈[L]andk∈[2d−1]. In the
casen=mfor all layers, we introduce the joint parameter tensor W∈Covα×···×α×n×n×2d−1Lso that
W/parenleftbig
...,2d−1(l−1) +k+ 1/parenrightbig
=˜T(l)
k.
A perusal of the above discussion reveals that there are (2dαd+ 1)mn+ntotal parameters in each FNO
layer. Note that, since mandnconstitute the respective input and output channels of the layer, the number
of parameters can quickly explode due to the exponential scaling factor 2dαdif many wavenumbers are
kept. Preserving a large number of modes could be crucial for applications where the spectral decay of the
input or output functions is slow such as in image processing or the modeling of multi-scale physics. In the
following section, we describe a tensorization method that is able to mitigate this growth without sacrificing
approximation power.
6Under review as submission to TMLR
(a)Original FNO.
 (b)Double Skip.
 (c)Improved backbone - pre-
activation.
(d)Improved backbone.
Figure 3: Original FNO and Improved Backbone Architecture. The original FNO architecture (Li
et al., 2021b) is composed of simply a Spectral Convolution, with a (linear) skip connection to recover
high-frequency information and handle non-periodic inputs (3(a)). We improve the architecture as detailed
in section 3.4. In particular, we have a version with a double (sequential) skip connection (3(b)), while
our best architecture uses nested skip connections, and can be made both with and without preactivation
(subfigures 3(c) and 3(d), respectively). The latter, subfigure 3(d), is our best architecture.
3.4 Architectural improvements
Our proposed approach uses FNOas a backbone. To improve its performance, we first study various aspects
of the Fourier Neural Architecture and perform thorough ablation to validate each aspect. In particular, we
propose improvements to the base architecture that improve performance.
Normalization in neural operators While normalization techniques, such as Batch-Normalization Ioffe
& Szegedy (2015), have proven very successful in training neural networks, additional consideration must
be given when applying those to neural operators in order to preserve its properties, notably discretization
invariance. Specifically, it cannot depend on the spatial variables and therefore has to be either a global or
a function-wise normalization. We investigate several configurations using instance normalization Ulyanov
et al. (2016) and layer-normalization Ba et al. (2016), in conjunction with the use-of preactivation He et al.
(2016).
Channel mixing FNOrelies on a global convolution realized in the spectral domain. Inspired by previous
works, e.g. Guibas et al. (2021), we propose adding an MLP in the original space, after each Spectral
convolution. In practice, we found that two-layer bottleneck MLP works well, e.g. we decrease the co-
dimension by half in the first linear layer before restoring it in the second one.
Boundary conditions Fourier neural operators circumvent the limitation of traditional Fourier methods
to inputs with periodic boundaries only. This is achieved through a local linear transformation added to the
spectral convolution. This can be seen as a linear skip connection. We investigate replacing these with an
identity skip-connection and a soft-gated skip-connection Bulat et al. (2020b).
We also investigate the impact of domain-padding, found by Li et al. (2021b) to improve results, especially
for non-periodic inputs, and padding for the multi-grid decomposition.
7Under review as submission to TMLR
We represent in Figure. 3 the original FNO architecture (Li et al., 2021b), subfigure 3(a), the improved
version with double (sequential) skip connections (subfigure 3(b)) and our best architecture, both with and
without preactivation (subfigures 3(c) and 3(d), respectively).
3.5 Tensor Fourier Neural Operators
Intheprevioussection, weintroducedaunifiedformulationofFNOwherethewholeoperatorisparametrized
by a single parameter tensor W. This enables us to introduce the tensor operator, which parameterizes
efficiently Wwith a low-rank, tensor factorization. We introduce the method for the case of a Tucker de-
composition, for its flexibility. Other decompositions, such as Canonical Polyadic, can be readily integrated.
This joint parametrization has several advantages: i) it applies a low-rank constraint on the entire tensor
W, thus regularizing the model. These advantages translate into i) a huge reduction in the number of pa-
rameters, ii) better generalization and an operator less prone to overfitting. We show superior performance
for low-compression ratios (up to 200×) and very little performance degradation when largely compressing
(>450×) the model, iii) better performance in a low-data regime.
In practice, we express Win a low-rank factorized form, e.g. Tucker or CP. In the case of a Tucker
factorization with rank (R1,···,Rd,RL,RI,RO), whereRLcontrols the rank across layers, RI=ROcontrol
the rank across the input and output co-dimension, respectively, and R1,···,Rdcontrol the rank across the
dimensions of the operator:
W=R1/summationdisplay
r1=1···Rd/summationdisplay
rd=1RI/summationdisplay
ri=1RO/summationdisplay
ro=1RL/summationdisplay
rl=1G(r1,···,rd,ri,ro,rl)·U(1)(:,r1)· ··· · U(d)(:,rd)·U(I)(:,ri)··U(O)(:,ro)·U(L)(:,rl).
(4)
Here, Gis the core of size RL×RI×RO×R1×···×RdandU(L),U(I),U(O),U(1),···,U(d)are factor
matrices of size (RL×L),(RI×I),(RO×O),(R1×α),···,(Rd×α), respectively.
Note that the mode (dimension) corresponding to the co-dimension can be left uncompressed by setting
RL=LandU(L)= Id. This leads to layerwise compression. Also note that having a rank of 1along any of
the modes would mean that the slices along that mode differ only by a (multiplicative) scaling parameter.
Also note that during the forward pass, we can pass Tdirectly in factorized form to each layer by selecting
the corresponding rows in U(L). While the contraction in equation 3 can be done using the reconstructed
tensor, it can also be done directly by contracting ˆF(ˆv)with the factors of the decomposition. For small,
adequately chosen ranks, this can result in computational speedups.
Figure 4: Illustration of a Tucker
decomposition. For clarity , we
show Was a3rd-order tensor weight.A visualization of the Tucker decomposition of a third-order tensor
can be seen in Figure 4). Note that we can rewrite the entire weight
parameter for this Tucker case, equivalently, using the more compact
n-mode product as:
W=G×1U(1)···×dU(d)×d+1U(I)×d+2U(O)×d+3U(L)
We can efficiently perform an iFFT after contraction with the ten-
sorized kernel. For any layer l, the (j1,j2)coordinate of the matrix-
valued convolution function κ(x)is as follows,
[κl(x)]j1,j2=m1/summationdisplay
i1=1···md/summationdisplay
id=1RL/summationdisplay
rl=1RI/summationdisplay
ri=1RO/summationdisplay
ro=1R1/summationdisplay
r1=1···Rd/summationdisplay
rd=1G(r1,···,rd,ri,ro,rl)·
U(1)(i1,r1)···U(d)(id,rd)·U(I)(j1,ri)·U(O)(j2,ro)·U(L)(l,rl)·exp(2πd/summationdisplay
k=1ixkik)
8Under review as submission to TMLR
This joint factorization along the entire operator allows us to leverage redundancies both locally and across
the entire operator. This leads to a large reduction in the memory footprint, with only a fraction of the pa-
rameter. It also acts as a low-rank regularizer on the operator, facilitating training. Finally, through global
parametrization, we introduce skip connections that allow gradients to flow through the latent parametriza-
tion to all the layers jointly, leading to better optimization.
(a)Predicting with padded regions. Local region in the input is
padded and used to predict the corresponding region in the output.
(b)MG-Domain Decomposition. Progres-
sively larger spatial regions are added to a local
region by subsampling.
Figure 5: Domain decomposition in space (5(a)) and our Multi-Grid based approach. (5(b)) .
White squares represent the region of interest while yellow squares the larger embeddings.
Importantly, this formulation is general and works with any tensor factorization. For instance, we also
explore a Canonical-Polyadic decomposition (CP) which can be seen as a special case of Tucker with a
super-diagonal core. In that case, we set a single rank Rand express the weights as a weighted sum of R
rank-1 tensors. Concretely:
W=R/summationdisplay
r=1λrU(1)(:,r)· ··· · U(d)(:,r)· (5)
U(I)(:,r)·U(O)(:,r)·U(L)(:,r).
where U(L),U(I),U(O),U(1),···,U(d)arefactormatricesofsize (R×L),(R×I),(R×O),(R×α),···,(R×α),
respectively and λ∈RR. Note that the CP, contrarily to the Tucker, has a single rank parameter, shared
between all the dimensions. This means that to maintain the number of parameters the same, Rneeds to
be very high, which leads to memory issues. This makes CP more suitable for large compression ratios,
and indeed, we found it leads to better performance at high-compression / very low-rank. In this paper,
we also explore the tensor-train decomposition Oseledets (2011). A rank- (1,R1,···,RN,RI,RO,RL,1)TT
factorization expresses Was:
W(i1,···,id,ic,io,il) =G1(i1)·×GN(id)GI(ic)×···GO(io)×···GL(il).
Where each of the factors of the decompositions Gkare third order tensors of size Rk×Ik×Rk+1.
In the experimental section 4.3, we show results of TFNOtrained with a Tucker, TT and CP factorization.
Separable Fourier Convolution The proposed tensorization approach introduces a factorization of the
weights in the spectral domain. When a CP Kolda & Bader (2009) is used, this induces separability over
the learned kernel. We propose to make this separability explicit by not performing any channel mixing in
the spectral domain and relying on the MLP introduced above to do so. The separable Spectral convolution
can be thought of as a depthwise convolution performed in the Fourier domain, e.g. without any channel
mixing. The mixing between channels is instead done in the spatial domain. This results in a significant
reduction in the number of parameters while having minimal impact on performance (we found it necessary
to increase the depth of the network, however, to ensure the network retained enough capacity).
9Under review as submission to TMLR
3.6 Multi-Grid Domain Decomposition
Having introduced our decomposition in the operator’s parameter space, we now introduce our novel multi-
grid approach to decompose the problem domain.
Domain decomposition is a method commonly used to parallelize classical solvers for time-dependent
PDEs that is based on the principle that the solution for a fixed local region in space depends mostly on
the input at the same local region (Chan & Mathew, 1994). In particular, since the time-step h > 0of
the numerical integrator is small, the solution u(x,t+h), for any point x∈Dandt∈R+, depends most
strongly on the points u(y,t)for ally∈B/parenleftbig
x,r(h))whereB/parenleftbig
x,r(h)/parenrightbig
denotes the ball centered at xwith
radiusr(h). This phenomenon is easily seen for the case of the heat equation where, in one dimension, the
solution satisfies
u(x,t+h)∝/integraldisplay∞
−∞exp/parenleftbigg−(x−y)2
4h/parenrightbigg
u(y,t)dy
≈/integraldisplayx+4h
x−4hexp/parenleftbigg−(x−y)2
4h/parenrightbigg
u(y,t)dy
with the approximation holding since 99.9937%of the kernel’s mass is contained within B(x,4h). While
some results exist, there is no general convergence theory for this approach, however, its empirical success
has made it popular for various numerical methods (Albin & Bruno, 2011).
To exploit this localization , the domain Dis split inq∈Npairwise-disjoint regions D1,···,Dqso that
D=∪q
j=1Dj. Each region Djis then embedded into a larger one Zj⊃Djso that points away from the
center ofDjhave enough information to be well approximated. A model can then be trained so that the
approximationG(a|Zj)|Dj≈u|Djholds for all j∈[q]. This idea is illustrated in Figure 5(a) where D= [0,1]2
and allDj,Zjare differently sized squares. This allows the model to be ran fully in parallel hence its time
and memory complexities are reduced linearly in q.
Multi-Grid. Domain decomposition works well in classical solvers when the time step h > 0is small
because the mapping u(·,t)∝⇕⊣√∫⊔≀→u(·,t+h)is close to the identity. However, the major advancement made by
machine learning-based operator methods for PDEs is that a model can approximate the solution, in one
shot, for very large times i.e. h>1. But, for larger h, the size of Zjrelative toDjmust increase to obtain
the same approximation accuracy, independently of model capacity. This causes any computational savings
made by the decomposition approach to be lost.
To mitigate this, we propose a multi-grid based domain decomposition approach where global information
is added hierarchically at different resolutions. While our approach is inspired by the classical multi-grid
method, it is not based on the V-cycle algorithm (McCormick, 1985). For ease of presentation, we describe
this concept when a domain D=T2is uniformly discretized by 2s×2spoints, for some s∈N, but note
that generalizations can readily be made. Given a final level L∈N, we first sub-divide the domain into 22L
total regions each of size 2s−L×2s−Land denote them D(0)
1,···,D(0)
22L. We call this the zeroth level. Then,
around each D(0)
j, for anyj∈[22L], we consider the square D(1)
jof size 2s−L+1×2s−L+1that is equidistant,
in every direction, from each boundary of D(0)
j. We then subsample the points in D(1)
juniformly by a factor
of1
2in each direction, making D(1)
jhave 2s−L×2s−Lpoints. We call this the first level. We continue this
process by considering the squares D(2)
jof size 2s−L+2×2s−L+2around each D(1)
jand subsample them
uniformly by a factor of1
4in each direction to again yield squares with 2s−L×2s−Lpoints. The process is
repeated until the Lth level is reached wherein D(L)
jis the entire domain subsampled by a factor of 2−Lin
each direction. The process is illustrated for the case L= 2in Figure 5(b). Since we work with the torus,
the region of the previous level is always at the center of the current level.
The intuition behind this method is that since the dependence of points inside a local region diminishes the
further we are from that region, it is enough to have coarser information, as we go farther. We combine
this multi-grid method with the standard domain decomposition approach by building appropriately padded
squaresZ(l)
jof size 2s−L+ 2p×2s−L+ 2paround each D(l)
jwherep∈Nis the amount of padding to be
10Under review as submission to TMLR
Figure 6: Tensorization: error in logscale as a function of the compression ratio. We compare the
tensor neural operator with an FNO with the same number of parameters ( trimmed).We achieve over
100x compression ratio with better performance that the original FNO
Figure 7: MG-Domain Decomposition: error as a function of the domain compression ratio.
We compare MG-TFNO with different numbers of multigrid regions both with and without weight tensor
compression to a full field FNO model. We achieve over 7x input space compression, 10x parameter
space compression ratios and better performance than the original FNO.
added in each direction. We then take the evaluations of the input function aat each level and concatenate
them as channels. In particular, we train a model so that ˆG/parenleftbig
(a|Z(0)
j,···,a|Z(L)
j)/parenrightbig
|D(0)
j≈u|D(0)
j.Since the
model only operates on each padded region separately, we reduce the total number of grid points used from
22sto(2s−L+ 2p)2and define the domain compression ratio as the quotient of these numbers. Furthermore,
11Under review as submission to TMLR
note that, assuming aisRdA-valued, a model that does not employ our multi-grid domain decomposition
uses inputs with dAchannels while our approach builds inputs with (L+ 1)dAchannels. In particular, the
number of input channels scales only logarithmically in the number of regions hence global information is
added at very little additional cost. Indeed, FNO models are usually trained with internal widths much
larger than dAhence the extra input channels cause almost no additional memory overhead.
4 Experiments
Inthissection, wefirstintroducethedata,experimentalsettingandimplementationdetailsbeforeempirically
validating our approach through thorough experiments and ablations.
4.1 Data.
We experiment on a dataset of 10K training samples and 2K test samples of the two-dimensional Navier-
Stokes equation with Reynolds number 500. We also experiment with the one-dimensional viscous Burgers’
equation.
Navier-Stokes. We consider the vorticity form of the two-dimensional Navier-Stokes equation,
∂tω+∇⊥ϕ·ω=1
Re∆ω+f, x∈T2, t∈(0,T]
−∆ϕ=ω,/integraldisplay
T2ϕ= 0, x∈T2, t∈(0,T](6)
with initial condition ω(0,·) = 0where T2∼=[0,2π)2is the torus, f∈˙L2(T2;R)is a forcing function, and
Re>0is the Reynolds number. Then ω(t,·)∈˙Hs(T2;R)for anyt∈(0,T]ands > 0, is the unique
weak solution to equation 6 (Temam, 1988). We consider the non-linear operator mapping f∝⇕⊣√∫⊔≀→ω(T,·)with
T= 5and fix the Reynolds number Re = 500. We define the Gaussian measure µ=N(0,C)on the forcing
functions where we take the covariance C= 27(−∆ + 9I)−4, following the setting in (De Hoop et al., 2022).
Input data is obtained by generating i.i.d. samples from µby a KL-expansion onto the eigenfunctions of C
(Powell et al., 2014). Solutions to equation 6 are then obtained by a pseudo-spectral scheme (Chandler &
Kerswell, 2013).
Burgers’ Equation. We consider the one-dimensional Burgers’ equation on the torus,
∂tu+uux=νuxx, x∈T, t∈(0,T]
u|t=0=u0, x∈T(7)
for initial condition u0∈L2(T;R)and viscosity ν >0. Thenu(t,·)∈Hs(T;R), for anyt∈R+ands>0,
is the unique weak solution to 7 (Evans, 2010). We consider the non-linear operator u0∝⇕⊣√∫⊔≀→u(T,·)with
T= 0.5or1and fixν= 0.01. We define the Gaussian measure µ=N(0,C)where we take the covariance
C= 35/2(−d2
dx2+ 9I)−3. Input data is obtained by generating i.i.d. samples from µby a KL-expansion onto
the eigenfunctions of C. Solutions to equation 7 are then obtained by a pseudo-spectral solver using Heun’s
method. We use 8K samples for training and 2K for testing.
4.2 Implementation details
Implementation We use PyTorch Paszke et al. (2017) for implementing all the models. The tensor
operations are implemented using TensorLy Kossaifi et al. (2019) and TensorLy-Torch Kossaifi (2021). Our
code was released under the permissive MIT license, as a Python package that is well-tested and comes with
extensive documentation, to encourage and facilitate downstream scientific applications. It is available at
https://github.com/neuraloperator/neuraloperator .
12Under review as submission to TMLR
Figure 8: Error as a function of the number of training samples (left) and training VS testing
loss.We compare TFNOwith a regular FNO. Note that on the left we show the testing L2error while,
for training, the H1loss is used and that is compared with the H1test error on the right. Our approach
generalizes better while requiring fewer training samples.
Hyper-parameters We train all models via gradient backpropagation using a mini-batch size of 16, the
Adam optimizer, with a learning rate of 1e−3, weight decay of 1e−4, for 500epochs, decreasing the learning
rate every 100epochs by a factors of1
2. The model width is set in all cases to 64except when specified
otherwise (for the Trimmed FNO), meaning that the input was first lifted (with a linear layer) from the
number of input channels to that width. The projection layer projects from the width to 256and a prediction
linear layer outputs the predictions. 10000samples were used for training, as well as a separate set of 2000
samples for testing. All experiments are done on a NVIDIA Tesla V100 GPU.
To disentangle the effect of each of our components, the comparisons between the original FNO, the MG-
FNO, TFNO, and the MG-TFNO were conducted in the same setting, with a mini-batch size of 32, modes
of 42 and 21 for the height and width, respectively, and an operator width of 64.
For the comparison between our best models, we use all the modes (64 and 32) and a mini-batch size of
16, which leads to improved performance for all models but longer training times. For each comparison, the
same setting and hyper-parameters were used for all models.
Training the operator. Since MG-TFNO predicts local regions which are then stitched together to form
a global function without any communication, aliasing effects can occur where one output prediction does
not flow smoothly into the next. To prevent this, we train our model using the H1Sobolev norm (Czarnecki
et al., 2017; Li et al., 2021a). By matching derivatives, training with this loss prevents any discontinuities
from occurring and the output prediction is smooth.
4.3 Experimental results
In this section, we compare our approach with both the regular FNO Li et al. (2021b) and the Factorized-
FNO Tran et al. (2023), which separately applied FFT along each mode before combining the results. In
all cases, our approach achieves superior performance with a fraction of the parameters, as can be seen in
Table 4.4.4.
Tensorizing: better compression. In Figure 6, we show the performance of our approach (TNO) com-
pared to the original FNO, for varying compression ratios. In the Trimmed- FNO, we adjust the width in
order to match the number of parameters in our TNO. We focus on the width of the network as it was
shown to be the most important parameter (De Hoop et al., 2022). Our method massively outperforms the
Trimmed- FNOat every single fixed parameter amount. Furthermore, even for very large compression ratios,
our FNO outperforms the full-parameter FNO model. This is likely due to the regularizing effect of the
tensor factorization on the weight, showing that many of the ones in the original model are redundant.
13Under review as submission to TMLR
Table 2:Comparing the performance of MG-TFNO with previous works on Navier-Stokes . Our
methodachievessuperiorperformancewithafractionoftheparameterswhilelargelycompressingtheweights
(TFNO) and the input-domain ( MG-TFNO ).
Method L2test error (%) # Params Model CR Input CR
FNOLi et al. (2021b) 1.34% 67M - -
FFNOTran et al. (2023) 1.15 % 1M 67× -
TFNO(CP) 0.29% 890K 75× -
TFNO(CP) 0.47% 447K 150× -
MG-TFNO (CP) 0.49 % 447K 40× 1.9×
MG-TFNO (Tucker) 0.42 % 447K 19× 1.9×
Tensorizing: better generalization. Figure 8 (left) shows that our TNO generalizes better with less
training samples. Indeed, at every fixed amount of training samples, the TNO massively outperforms the
full-parameter FNO model. Even when only using half the samples, our TNO outperforms the FNO trained
on the full dataset. Furthermore, Figure 8 (right) shows that our TNO overfits significantly less than FNO,
demonstratingtheregularizingeffectofthetensordecomposition. Thisresultisinvaluableinthe PDEsetting
where very few training samples are typically available due to the high computational cost of traditional
PDEsolvers.
Multi-Grid Domain Decomposition. In Table??, we compare our MG-TFNO with the baseline FNO
and the TFNO, respectively. MG-TFNO enables compressing both the weight tensor but also the input
domain. On the other hand, preserving resolution invariance requires padding the patches, which decreases
performance, resulting in a tradeoff between input domain compression and prediction accuracy.
We also show the impact of multi-grid domain decomposition on performance in Figure 7. We find that
lower compression ratios (corresponding to a larger amount of padding in the decomposed regions) perform
better which is unsurprising since more information is incorporated into the model. More surprisingly, we
find that using a larger number of regions (16) performs consistently better than using a smaller number (4)
and both can outperform the full-field FNO. This can be due to the fact that: i) the domain decomposition
acts as a form of data augmentation, exploiting the translational invariance of the PDEand more regions
yield larger amounts of data, and ii) the output space of the model is simplified since a function can have
high frequencies globally but may only have low frequencies locally. Consistently, we find that the tensor
compression in the weights acts as a regularizer and improves performance across the board.
Architectural improvements to the backbone In addition to the ablation performed on our MG-
TFNO, we also investigate architectural improvements to the FNObackbone, see Sec 3.4 for details. In
particular, we find that, while instance normalization decreases performance, layer normalization helps, es-
pecially when used in conjunction with a pre-activation. Adding an MLP similarly improves performance, we
found that a bottleneck (expansion factor of 0.5) works well in practice, resulting in an absolute improvement
of0.87%in relativeL2error. We found the ordering of normalization, activation, and weights (including
preactivation), did not have a significant impact on performance. Finally, when not using multi-grid domain
decomposition, the inputs are periodic and padding is not necessary. In that case, not padding the input im-
proves performance. We use all these improvements for the backbone of the best version of our MG-TFNO ,
Fig 1 where we show that our improved backbone significantly outperforms the original FNO, while our
approach significantly outperforms both, with a small fraction of the parameters, opening the door to the
application of MG-TFNO to high-resolution problems.
4.4 Ablation studies
In this section, we further study the properties of our model through ablation studies. We first look at how
TFNOsuffers less from overfitting thanks to the low-rank constraints before comparing its performance with
14Under review as submission to TMLR
Table 3:Impact of our architectural improvements .
Method Layers L2test error H1test error # Params Model CR
FNOLi et al. (2021b) 4 1.34% 3 .78% 67,142,657 -
FNOLi et al. (2021b) 6 0.90% 2 .59% 100,705,409 0.7×
FNOLi et al. (2021b) 8 0.73% 2 .09% 134,268,161 0.5×
TFNO(CP) 4 0.47% 1 .20% 447,105 150×
TFNO(CP) 6 0.27% 0 .74% 662,081 101×
TFNO(CP) 8 0.22% 0 .59% 877,057 77×
Table 4:Resolution invariance of TFNO.Since the model is an operator, it is resolution invariant. In
particular, here, we trained our model in resolution 128×128and test it on unseen samples in various
resolutions and show it generalizes, with virtually no loss of performance to higher resolutions unseen during
training.
Method128×128 256 ×256 512 ×512 1024 ×1024
L2error H1error L2error H1error L2error H1error L2error H1error
CPTFNO 0.3% 0.87% 0.3% 0.93% 0.3% 0.93% 0.3% 0.93%
CPMG-TFNO 0.49% 1.2% 0.49% 1.3% 0.49% 1.5% 0.49% 1.6%
various tensor decompositions. Finally, we perform ablation studies for our multi-grid domain decomposition
on Burger’s equation.
4.4.1 Resolution invariance
TFNOis resolution invariant, meaning that it can be trained on one resolution and tested on a different
one. To illustrate this, we show zero-shot super-resolution results: we trained our best model (Table ??) on
images of resolution 128×128and tested it on unseen samples at higher resolutions ( 256×256and512×512),
Table 4. As can be seen, our method does as well on unseen, higher-resolution unseen testing samples as it
does on the training resolution, confirming the resolution invariance property of our neural operator.
4.4.2 Training on higher-resolution with Multi-grid
One important advantage of our multi-grid domain decomposition is that it enables training much larger
models on large inputs by distributing over patches. We demonstrate this, by training on larger resolution
(512x512 discretization) and using the largest FNO and TFNO that fits in memory, on a V100 GPU. For
the original FNO, this corresponds to a width of 12, first row in table 5. We then compare its performance
with the multigrid approach with a neural operator as large as fits into the same V100 GPUs i.e. each width
in the table has been optimized to be as large as memory allows. As we can see, our approach allows to fit
a larger model and reaches a much lower relative L2error.
4.4.3 Overfitting and Low-Rank Constraint
Here, we show that lower ranks (higher compressions) lead to reduced overfitting. In Figure ??, we show
the training and testing H1errors for our TOP with Tucker decomposition at varying compression ratios
(2x, 49x and 172x). We can see how, while the test error does not vary much, the gap between training and
test errors reduces as we decrease the rank. As we can see, while being the most flexible, Tucker does not
perform as well at higher compression ratios. In those extreme cases, CP and Tensor-Train lead to lower
errors.
15Under review as submission to TMLR
Table 5: Training on 512x512 . Multi-grid domain decomposition allows us to fit larger models into
memory by distributing patches in the domain space, thus reaching a lower relative error.
Model Width Patches Padding L2error
FNO 12 0 0 6.1
MG-FNO 42 4 70 2.9
MG-FNO 66 4 53 2.4
MG-FNO 88 16 40 1.8
Tucker MG-TFNO 80 16 46 1.3
(a)Train VS Test error over time for a TOP with a CP
factorization).
(b)Train VS Test error over time for a TOP with a TT
factorization.
Figure 9: Train/test curve for a TOP-CP (9(a)) and TOP-TT (9(b))
4.4.4 Tensor-Train and TOP
Our approach is independent of the choice of tensor decomposition. We already showed how Tucker is most
flexible and works well across all ranks. We also showed that while memory demanding for high rank, a CP
decomposition leads to better performance and low rank. Our method can also be used in conjunction with
other decompositions, such as tensor-train. To illustrate this, we show the convergence behavior of TNO
with a Tensor-Train decomposition for a compression ratio of 178, figure 9(b).
We also compare in Table 4.4.4 our TFNOwith different tensor decompositions.
Table 6:RelativeL2test error of our MG-TFNO approach for different tensor decompositions .
WeempiricallyfoundthatTuckerworksbestforsmallcompressionratio, CPexcelsatlargecompressionratio
(≈100×) but becomes computationally heavy for smaller ones. TT tends to be unstable at low-compression
ratios but preserves a good performance for extreme compression ratio ( >500×).
Method L2test error # Params Model CR
FNOLi et al. (2021b) 1.12% 67 M 0×
TFNO[Tucker] 0.37% 28 M 2.3×
TFNO[CP] 0.46% 808 K 83×
TFNO[TT] 1.18% 117 K 574×
16Under review as submission to TMLR
Table 7:Ablation comparing the performance on the relative L2test error of our MG-TFNO ap-
proach, compared with its parts TFNOand MG-FNO and the regular FNO, on Navier-Stokes.
CR stands for compression ratio. Tensorization and multi-grid domain decomposition both individually im-
prove performance while enabling space savings. The two techniques combined lead to further improvements,
enabling large compression for both input and parameter, while outperforming regular FNO.
Method L2test error # Params Model CR Domain CR
FNO(Li et al., 2021b) 2.54% 58 M 0× 0×
TFNO[Tucker] 1.39% 41 M 1.5× 0×
TFNO[CP] 2.24% 130 K 482× 0×
MG-FNO 1.43% 58 M 0× 1.4×
MG-TFNO [Tucker] 0.85% 5 .5M 10× 1.78×
MG-TFNO [Tucker] 1.89% 5 .5M 10× 7×
4.4.5 Decomposing domain and weights: MG-TFNO .
Tensorization and multi-grid domain decomposition not only improve performance individually, but their
advantages compound and lead to a strictly better algorithm that scales well to higher-resolution data by
decreasing the number of parameters in the model as well as the size of the inputs thereby improving
performance as well as memory and computational footprint. Table 7 compares FNO with Tensorization
alone, multi-grid domain decomposition alone, and our joint approach combining the two, MG-TFNO . In
all cases, for α, we keep 40Fourier coefficients for height and 24for the width and use an operator width of
64. Our results imply that, under full parallelization, the memory footprint of the model’s inference can be
reduced by 7×and the size of its weights by 10×while also improving performance.
Consistently with our other experiments, we find that the tensor compression in the weights acts as a
regularizer and improves performance across the board. Our results imply that, under full parallelization,
the memory footprint of the model’s inference can be reduced by 7×and its weight size by 10×while also
improving performance.
4.4.6 Burgers’ Equation
Figure 10: Error on Burgers’ equation with T= 0.5(left) and T= 1(right) as a function
of domain compression ratio using standard domain decomposition without our multi-grid
approach. We evaluate the performance of the standard domain decomposition approach. The radius
indicates the size, in physical space, of the padding added to each region.
We test the efficacy of the standard domain decomposition approach by training on two separate Burgers
problems: one with a final time T= 0.5and one with T= 1. As described in Section 3.6, we expect that
17Under review as submission to TMLR
forT= 1, each region requires more global information thus significantly more padding need to be used in
order to reach the same error. The results of Figure 10 indeed confirm this. The domain compression ratios
needed for the approach to reach the performance of the full-field model are higher, indicating the need for
incorporating global information. These results motivate our multi-grid domain decomposition approach.
5 Conclusion
In this work, we introduced i) a novel tensor operator ( TFNO) as well as a multi-grid domain decomposition
approachwhichtogetherform MG-TFNO ,ii)anoperatormodelthatoutperformsthe FNOwithafractionof
the parameters and memory complexity requirements, and iii) architectural improvements to the FNO. Our
method scales better, generalizes better, and requires fewer training samples to reach the same performance;
while the multi-grid domain decomposition enables parallelism over huge inputs. This paves the way to
applications on very high-resolution data and in our future work, we plan to deploy MG-TFNO to large-
scale weather forecasts for which existing deep learning models are prohibitive.
References
Jonas Adler and Ozan Oktem. Solving ill-posed inverse problems using iterative deep neural networks.
Inverse Problems , nov 2017.
Nathan Albin and Oscar P. Bruno. A spectral fc solver for the compressible navier–stokes equations in
general domains i: Explicit time-stepping. Journal of Computational Physics , 230(16):6248–6270, 2011.
Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint
arXiv:1607.06450 , 2016.
Saakaar Bhatnagar, Yaser Afshar, Shaowu Pan, Karthik Duraisamy, and Shailendra Kaushik. Prediction of
aerodynamic flow fields using convolutional neural networks. Computational Mechanics , pp. 1–21, 2019.
Kaushik Bhattacharya, Bamdad Hosseini, Nikola B Kovachki, and Andrew M Stuart. Model reduction and
neural networks for parametric pdes. arXiv preprint arXiv:2005.03180 , 2020.
Mackenzie L Blanusa, Carla J López-Zurita, and Stephan Rasp. The role of internal variability in global
climate projections of extreme events. arXiv preprint arXiv:2208.08275 , 2022.
Adrian Bulat, Jean Kossaifi, Georgios Tzimiropoulos, and Maja Pantic. Incremental multi-domain learning
with network latent tensor factorization. In Proceedings of the AAAI Conference on Artificial Intelligence ,
volume 34, pp. 10470–10477, 2020a.
Adrian Bulat, Jean Kossaifi, Georgios Tzimiropoulos, and Maja Pantic. Toward fast and accurate human
poseestimationviasoft-gatedskipconnections. In 2020 15th IEEE International Conference on Automatic
Face & Gesture Recognition , 2020b.
Tony F. Chan and Tarek P. Mathew. Domain decomposition algorithms. Acta Numerica , 3:61–143, 1994.
Gary J. Chandler and Rich R. Kerswell. Invariant recurrent solutions embedded in a turbulent two-
dimensional kolmogorov flow. Journal of Fluid Mechanics , 722:554–595, 2013.
Jean-Baptiste Cordonnier, Andreas Loukas, and Martin Jaggi. Multi-head attention: Collaborate instead of
concatenate. arXiv preprint arXiv:2006.16362 , 2020.
R. Courant, K. Friedrichs, and H. Lewy. Uber die partiellen differenzengleichungen der mathematischen
physik.Mathematische annalen , 100(1):32–74, 1928.
Wojciech M Czarnecki, Simon Osindero, Max Jaderberg, Grzegorz Swirszcz, and Razvan Pascanu. Sobolev
training for neural networks. Advances in Neural Information Processing Systems , 30, 2017.
18Under review as submission to TMLR
Tri Dao, Beidi Chen, Nimit S Sohoni, Arjun Desai, Michael Poli, Jessica Grogan, Alexander Liu, Aniruddh
Rao, Atri Rudra, and Christopher Ré. Monarch: Expressive structured matrices for efficient and accurate
training. In International Conference on Machine Learning , pp. 4690–4721. PMLR, 2022.
Maarten De Hoop, Daniel Zhengyu Huang, Elizabeth Qian, and Andrew M Stuart. The cost-accuracy
trade-off in operator learning with neural networks. arXiv preprint arXiv:2203.13181 , 2022.
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Un-
terthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth
16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929 , 2020.
Soheil Esmaeilzadeh, Kamyar Azizzadenesheli, Karthik Kashinath, Mustafa Mustafa, Hamdi A Tchelepi,
Philip Marcus, Mr Prabhat, Anima Anandkumar, et al. Meshfreeflownet: A physics-constrained deep
continuous space-time super-resolution framework. In SC20: International Conference for High Perfor-
mance Computing, Networking, Storage and Analysis , pp. 1–15. IEEE, 2020.
Lawrence C. Evans. Partial differential equations . American Mathematical Society, 2010.
John Guibas, Morteza Mardani, Zongyi Li, Andrew Tao, Anima Anandkumar, and Bryan Catanzaro. Adap-
tive fourier neural operators: Efficient token mixers for transformers. arXiv preprint arXiv:2111.13587 ,
2021.
Xiaoxiao Guo, Wei Li, and Francesco Iorio. Convolutional neural networks for steady flow approximation.
InProceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data
Mining, 2016.
Gaurav Gupta, Xiongye Xiao, and Paul Bogdan. Multiwavelet-based operator learning for differential equa-
tions.Advances in Neural Information Processing Systems , 34:24048–24062, 2021.
JuliaGusak, MaksymKholiavchenko, EvgenyPonomarev, LarisaMarkeeva, PhilipBlagoveschensky, Andrzej
Cichocki, and Ivan Oseledets. Automated multi-stage compression of neural networks. Oct 2019.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual networks. In
Computer Vision–ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11–14,
2016, Proceedings, Part IV 14 , pp. 630–645. Springer, 2016.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing
internal covariate shift. In International conference on machine learning , pp. 448–456. pmlr, 2015.
Majid Janzamin, Rong Ge, Jean Kossaifi, Anima Anandkumar, et al. Spectral learning on matrices and
tensors.Found. and Trends ®in Mach. Learn. , 12(5-6):393–536, 2019.
Yong-Deok Kim, Eunhyeok Park, Sungjoo Yoo, Taelim Choi, Lu Yang, and Dongjun Shin. Compression of
deep convolutional neural networks for fast and low power mobile applications. 2016.
Tamara G Kolda and Brett W Bader. Tensor decompositions and applications. SIAM Rev. , 51(3):455–500,
2009.
Jean Kossaifi. Tensorly-torch. https://github.com/tensorly/torch , 2021.
Jean Kossaifi, Yannis Panagakis, Anima Anandkumar, and Maja Pantic. Tensorly: Tensor learning in
python.Journal of Machine Learning Research (JMLR) , 20(26), 2019.
Jean Kossaifi, Antoine Toisoul, Adrian Bulat, Yannis Panagakis, Timothy M Hospedales, and Maja Pantic.
Factorized higher-order CNNs with an application to spatio-temporal emotion estimation. pp. 6060–6069,
2020.
Nikola Kovachki, Samuel Lanthaler, and Siddhartha Mishra. On universal approximation and error bounds
for fourier neural operators. Journal of Machine Learning Research , 22(290):1–76, 2021a.
19Under review as submission to TMLR
Nikola Kovachki, Zongyi Li, Burigede Liu, Kamyar Azizzadenesheli, Kaushik Bhattacharya, Andrew Stuart,
and Anima Anandkumar. Neural operator: Learning maps between function spaces. arXiv preprint
arXiv:2108.08481 , 2021b.
VadimLebedev, YaroslavGanin, MaksimRakhuba, IvanV.Oseledets, andVictorS.Lempitsky. Speeding-up
convolutional neural networks using fine-tuned CP-decomposition. 2015.
Martin Leutbecher and Tim N Palmer. Ensemble forecasting. Journal of computational physics , 227(7):
3515–3539, 2008.
Zongyi Li, Nikola Kovachki, Kamyar Azizzadenesheli, Burigede Liu, Kaushik Bhattacharya, Andrew Stuart,
and Anima Anandkumar. Neural operator: Graph kernel network for partial differential equations. arXiv
preprint arXiv:2003.03485 , 2020a.
Zongyi Li, Nikola Kovachki, Kamyar Azizzadenesheli, Burigede Liu, Andrew Stuart, Kaushik Bhattacharya,
and Anima Anandkumar. Multipole graph neural operator for parametric partial differential equations.
Advances in Neural Information Processing Systems , 33:6755–6766, 2020b.
Zongyi Li, Nikola Kovachki, Kamyar Azizzadenesheli, Burigede Liu, Kaushik Bhattacharya, Andrew Stu-
art, and Anima Anandkumar. Markov neural operators for learning chaotic systems. arXiv preprint
arXiv:2106.06898 , 2021a.
Zongyi Li, Nikola Borislavov Kovachki, Kamyar Azizzadenesheli, Burigede liu, Kaushik Bhattacharya, An-
drewStuart, andAnimaAnandkumar. Fourierneuraloperatorforparametricpartialdifferentialequations.
InInternational Conference on Learning Representations , 2021b.
Zongyi Li, Hongkai Zheng, Nikola Kovachki, David Jin, Haoxuan Chen, Burigede Liu, Kamyar Azizzade-
nesheli, and Anima Anandkumar. Physics-informed neural operator for learning partial differential equa-
tions.arXiv preprint arXiv:2111.03794 , 2021c.
Burigede Liu, Nikola Kovachki, Zongyi Li, Kamyar Azizzadenesheli, Anima Anandkumar, Andrew M Stuart,
and Kaushik Bhattacharya. A learning-based multiscale method and its application to inelastic impact
problems. Journal of the Mechanics and Physics of Solids , 158:104668, 2022.
Lu Lu, Pengzhan Jin, and George Em Karniadakis. Deeponet: Learning nonlinear operators for identi-
fying differential equations based on the universal approximation theorem of operators. arXiv preprint
arXiv:1910.03193 , 2019.
S. F. McCormick. Multigrid methods for variational problems: General theory for the v- cycle. SIAM
Journal on Numerical Analysis , 22(4):634–643, 1985.
Alexander Novikov, Dmitry Podoprikhin, Anton Osokin, and Dmitry Vetrov. Tensorizing neural networks.
pp. 442–450, 2015.
I. V. Oseledets. Tensor-train decomposition. SIAM J. Sci. Comput. , 33(5):2295–2317, September 2011.
Yannis Panagakis, Jean Kossaifi, Grigorios G. Chrysos, James Oldfield, Mihalis A. Nicolaou, Anima Anand-
kumar, and Stefanos Zafeiriou. Tensor methods in computer vision and deep learning. Proceedings of the
IEEE, 109(5):863–890, 2021. doi: 10.1109/JPROC.2021.3074329.
Christos Papadopoulos, Yannis Panagakis, Manolis Koubarakis, and Mihalis Nicolaou. Efficient learning of
multiple nlp tasks via collective weight factorization on bert. In Findings of the Association for Compu-
tational Linguistics: NAACL 2022 , pp. 882–890, 2022.
Evangelos E Papalexakis, Christos Faloutsos, and Nicholas D Sidiropoulos. Tensors for data mining and data
fusion: Models, applications, and scalable algorithms. ACM Trans. Intell. Syst. and Technol. (TIST) , 8
(2):1–44, 2016.
Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito, Zeming Lin,
Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in PyTorch. 2017.
20Under review as submission to TMLR
Jaideep Pathak, Shashank Subramanian, Peter Harrington, Sanjeev Raja, Ashesh Chattopadhyay, Morteza
Mardani, Thorsten Kurth, David Hall, Zongyi Li, Kamyar Azizzadenesheli, et al. Fourcastnet: A
global data-driven high-resolution weather model using adaptive fourier neural operators. arXiv preprint
arXiv:2202.11214 , 2022.
Catherine E. Powell, Gabriel Lord, and Tony Shardlow. An Introduction to Computational Stochastic PDEs .
Texts in Applied Mathematics. Cambridge University Press, United Kingdom, 1 edition, August 2014.
ISBN 9780521728522.
Md Ashiqur Rahman, Manuel A Florez, Anima Anandkumar, Zachary E Ross, and Kamyar Azizzadenesheli.
Generative adversarial neural operators. arXiv preprint arXiv:2205.03017 , 2022a.
Md Ashiqur Rahman, Zachary E Ross, and Kamyar Azizzadenesheli. U-no: U-shaped neural operators.
arXiv preprint arXiv:2204.11127 , 2022b.
Nicholas D Sidiropoulos, Lieven De Lathauwer, Xiao Fu, Kejun Huang, Evangelos E Papalexakis, and
Christos Faloutsos. Tensor decomposition for signal processing and machine learning. Transactions Signal
Processing , 65(13):3551–3582, 2017.
Julia Slingo and Tim Palmer. Uncertainty in weather and climate prediction. Philosophical Transactions of
the Royal Society A: Mathematical, Physical and Engineering Sciences , 369(1956):4751–4767, 2011.
Roger Temam. Infinite-dimensional dynamical systems in mechanics and physics . Applied mathematical
sciences. Springer-Verlag, New York, 1988.
Alasdair Tran, Alexander Mathews, Lexing Xie, and Cheng Soon Ong. Factorized fourier neural operators.
InThe Eleventh International Conference on Learning Representations , 2023. URL https://openreview.
net/forum?id=tmIiMPl4IPa .
Dmitry Ulyanov, Andrea Vedaldi, and Victor Lempitsky. Instance normalization: The missing ingredient
for fast stylization. arXiv preprint arXiv:1607.08022 , 2016.
Gege Wen, Zongyi Li, Kamyar Azizzadenesheli, Anima Anandkumar, and Sally M Benson. U-fno—an
enhanced fourier neural operator-based deep-learning model for multiphase flow. Advances in Water
Resources , 163:104180, 2022.
Yan Yang, Angela F Gao, Jorge C Castellanos, Zachary E Ross, Kamyar Azizzadenesheli, and Robert W
Clayton. Seismicwavepropagationandinversionwithneuraloperators. The Seismic Record , 1(3):126–134,
2021.
Yan Yang, Angela F Gao, Jorge C Castellanos, Zachary E Ross, Kamyar Azizzadenesheli, and Robert W
Clayton. Accelerated full seismic waveform modeling and inversion with u-shaped neural operators. arXiv
preprint arXiv:2209.11955 , 2022.
Yinhao Zhu and Nicholas Zabaras. Bayesian deep convolutional encoder–decoder networks for surrogate
modeling and uncertainty quantification. Journal of Computational Physics , 2018. ISSN 0021-9991.
21