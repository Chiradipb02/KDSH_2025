Published in Transactions on Machine Learning Research (10/2024)
Stochastic Bandits for Egalitarian Assignment
Eugene Lim elimwj@comp.nus.edu.sg
Department of Computer Science,
National University of Singapore
Vincent Y. F. Tan vtan@nus.edu.sg
Department of Mathematics,
Department of Electrical and Computer Engineering,
National University of Singapore
Harold Soh harold@comp.nus.edu.sg
Department of Computer Science,
National University of Singapore
Reviewed on OpenReview: https: // openreview. net/ forum? id= kmKVJl2JWo
Abstract
We study EgalMAB, an egalitarian assignment problem in the context of stochastic multi-
armed bandits. In EgalMAB, an agent is tasked with assigning a set of users to arms. At
each time step, the agent must assign exactly one arm to each user such that no two users
are assigned to the same arm. Subsequently, each user obtains a reward drawn from the
unknown reward distribution associated with its assigned arm. The agent’s objective is to
maximize the minimum expected cumulative reward among all users over a fixed horizon.
Thisproblemhasapplicationsinareassuchasfairnessinjobandresourceallocations, among
others. We design and analyze a UCB-based policy EgalUCB and establish upper bounds on
the cumulative regret. In complement, we establish an almost-matching policy-independent
impossibility result.
1 Introduction
The multi-armed bandit (MAB) problem serves as a model for online decision-making under uncertainty,
finding applications in diverse domains (Shen et al., 2015; Durand et al., 2018; Ding et al., 2019; Mueller
et al., 2019; Forouzandeh et al., 2021). In the classical stochastic MAB problem, an agent is provided with
a set ofKarms, each associated with an unknown distribution. At each round, the agent plays an arm
and receive a reward drawn from its distribution. The agent’s goal is to maximize the expected cumulative
reward obtained over a fixed number of time steps T.
In our work, we study the problem of egalitarian assignment in the context of stochastic MABs, which we
refer to as EgalMAB. In this scenario, the agent is provided with a set of U < Kusers. At each time step,
the agent must assign exactly one arm to each user such that no two users are assigned to the same arm.
Subsequently, each user obtain a reward drawn from the reward distribution associated with its assigned
arm. The agent’s objective is to maximize the minimum expected cumulative reward among all users.
EgalMAB finds applications in various domains, including ensuring fairness in job and resource allocations.
Consider the job assignment problem depicted in Figure 1. In this scenario, there are Uusers with recurring
jobs of equal load (e.g., hourly database updates) and Kshared cloud computing resources with fluctuating
computationalpower(e.g., duetounrelatedloadsthatarerunningonneighbouringcoresofthesamephysical
node (Kousiouris et al., 2011)). The agent’s objective is to distribute these jobs fairlyamong the available
resources such that over Ttrials, the user who has to wait the longest across all their jobs (i.e., receives the
least cumulative reward) is not significantly worse off compared to other users.
1Published in Transactions on Machine Learning Research (10/2024)
minuE[Su,T]
U= 7 users
K= 8 armsE[Su,T]
Figure 1: An illustration with K= 8arms andU= 7users. After time step T, each user u∈[U]has some
expected cumulative reward E[Su,T]. The agent’s objective is to maximize minu∈[U]E[Su,T], which is the
minimum expected cumulative reward across all users.
Similarly, ride-hailing services present another scenario where fair assignment is desirable. In this scenario,
there areUpassengers and Kdrivers who offer varying degrees of experience to passengers (e.g., due to
vehicle condition and driver behavior). The agent’s objective is to allocate users fairlyto vehicles such that
overTtrips, no passenger faces a significantly worse overall experience than others.
As illustrated in both examples, EgalMAB embodies the principle of egalitarianism, which is also known as
the Rawlsian maximin principle (Rawls, 1971), in its notion of fairness. Egalitarianism is a fundamental
notion of justice based on the difference principle , which seeks to maximize the welfare of those in society
who are the worst-off . Likewise, our agent’s objective is to maximize the cumulative reward of the user
receiving the lowest cumulative reward among all Uusers. Given our examples above, solving the EgalMAB
problem yields a policy that minimizes the overall waiting time of the user who has to wait the longest and
maximizes the experience of the passenger who has the most negative encounters. This is in contrast to the
classic MAB setting where only overall utility is considered. This prompts the fundamental question that
our work seeks to answer:
How can we design an agent’s assignment policy that optimizes overall utility for individual
users while also ensuring that no user within a group consistently encounters substantially
sub-optimal outcomes?
Our contributions are summarized as follows. We formally define the EgalMAB problem in Section 3 and
propose EgalUCB, a UCB-based solution to EgalMAB, in Section 4. In Section 5 and 6, we establish that
EgalUCB achieves an expected regret of at most
O/parenleftigg/radicalbigg
Tln(T)·(K−U)
U/parenrightigg
.
Wealsoprovideapolicy-independentlowerboundthatmatchestheupperbounduptoamultiplicativefactor
of/radicalbig
1/Uand a term logarithmic in T. Proof sketches for these results are included in Section 6. Lastly,
empirical validations for these results using both synthetic and real-world data are presented in Section 7.
2 Related Works
MAB with Multiple Plays. The multiple-play multi-armed bandit (MP-MAB) problem (Anantharam
et al., 1987; Gai et al., 2012; Chen et al., 2016; Komiyama et al., 2015) expands upon the classical MAB
framework by allowing the agent to play U≥2distinct arms. The MP-MAB problem has been extended
in various ways, including combinatorial bandits (Cesa-Bianchi & Lugosi, 2012; Kveton et al., 2015a; Chen
et al., 2016), cascading bandits (Kveton et al., 2015b; Wen et al., 2017), and MP-MAB with shareable
arms (Wang et al., 2022). An adjacent problem to ours is identifying the top Uarms while minimizing
regret, which can be framed as a matroid bandit problem (Kveton et al., 2014) with a uniform matroid of
rankU. Although similar to EgalMAB in that the agent has to select multiple arms, unlike EgalMAB, these
2Published in Transactions on Machine Learning Research (10/2024)
t= 3 t= 6 t= 9 t=T
1
2
3
4
5b= 1 b= 2 b= 3 b=T/3
· · ·
Figure 2: A trace of EgalUCB withK= 5arms andU= 3users. When b= 2, the three arms with the
highest UCBa,bvalues area∈{2,4,5}, which are then assigned in a round-robin fashion across time steps
t∈{4,5,6}. Asbincreases, the estimates for µafor all arms aimproves. By b=T/3for largeT, the three
arms with the highest UCBa,bvalues are most likely a∈{1,2,3}, which are then assigned over time steps
t∈{T−2,T−1,T}.
formulations of the MP-MAB problem do not involve multiple users ; that is, the reward is with respect to
the agent instead of users.
MAB with Multiple Users. The extension of multiple users into the classic MAB problem has been
extensively explored in the context of cognitive radio networks (Jouini et al., 2009; Liu & Zhao, 2010; Avner
& Mannor, 2014). Unlike EgalMAB where acentralized agent assigns the arms to the users, these works have
focused on scenarios where multiple decentralized users interact with a single MAB instance. As a result
of this decentralization, it is possible for multiple users to play the same arm simultaneously, resulting in a
collision that can negatively affect the received reward.
Fairness in MAB. The introduction of fairness considerations into the classical MAB problem has gar-
nered significant interest. Much attention has been directed towards addressing fairness concerns that
typically involve ensuring that each arm is played a minimum number of times, known as fairness in ex-
posure (Claure et al., 2020; Chen et al., 2020; Li et al., 2020; Wang et al., 2021), or with a probability
proportional to the arm’s merit, known as meritocratic fairness (Joseph et al., 2016; 2018). While EgalMAB
emphasizes fairness among users, the aforementioned works deal with fairness among arms. However, there
are alternative approaches that maintain a focus on fairness among users. One such approach involves maxi-
mizing the Nash social welfare (NSW) function. This function is defined as the product of rewards obtained
by all users, which intuitively encodes the notion of fairness as it increases only when most users achieve
high rewards. Hossain et al. (2021) investigated a scenario involving Uusers andKarms, in which each
arm’s reward varies for each user due to different perceived utilities. In each time step, only one arm is
played, and all users obtain rewards according to their perceived utilities. Their primary objective is to
maximize the NSW. Sawarni et al. (2023) examined an alternative fairness framework within the context
of linear bandits. In their scenario, a new user is introduced at each time step, and fairness is ensured by
considering the product of rewards across all time steps. Both of these works assume that only a singlearm
is played at each time step, while the agent in EgalMAB simultaneously selects multiple arms. Additionally,
instead of aiming to maximize the NSW function ,EgalMAB focuses on maximizing the cumulative reward of
the worst-off user .
3EgalMAB Problem
In this section, we will formally define the components involved in an EgalMAB problem. We will start by
presenting a working definition for each component. When relevant, we will supplement these definitions
with the measure-theoretic details necessary for the proofs of lower bounds.
3Published in Transactions on Machine Learning Research (10/2024)
Environment. LetKbe the number of arms, U < K be the number of users, and Tbe the time
horizon. For each arm a∈[K], letpadenote the reward density. An instance of the EgalMAB problem is
represented by the tuple (ν,U,T )whereν:= (p1,...,pK). When the context is clear, we also refer to νas
anEgalMAB instance. We assume that the expected reward µa:=EX∼pa[X]obtained for playing arm ais
finite. Moreover, for convenience, we assume that the arms are indexed in such a way that µ1≥···≥µK.
However, the agent is unaware of this ordering.
More formally, for each arm a∈[K], letPabe the probability law for the reward obtained after playing
arma. For any Pa-measurable set B, we have Pa(B) =/integraltext
Bpadλ, whereλis the Lebesgue measure and
pa= dPa/dλis the Radon–Nikodym derivative for Pa.
Agent Policy. A solution to the EgalMAB problem is characterized by an agent policy π:= (πt)t. During
eachtimestep t∈[T],themapπtconsiderstheactionsandrewardshistory(whichwewilldefineshortly,after
introducing relevant notations) and assigns each user u∈[U]to an armAu,t∈[K]such that no two users
are assigned the same arm. Subsequently, each user ureceives a reward Xu,tdrawn independently from the
densitypAu,t. Using these notations, we will denote the history that πtconsiders as (A1,X1,...,At−1,Xt−1)
whereAt:= (Au,t)uandXt:= (Xu,t)u.
More formally, let P(S)denote the power set of a set S. For each time step t∈[T], the stochastic map
πt: (([K]×R)U)t−1×P([K]U)→[0,1]
is a probability kernel. We denote Pπνas the probability law for the interaction between the policy πand
theEgalMAB instanceνoverTtime steps. Thus, the density of Pπνis
pπν(A1,X1,...,AT,XT) =T/productdisplay
t=1πt(At|A1,X1,...,At−1,Xt−1)U/productdisplay
u=1pAu,t(Xu,t).
Egalitarian Objective. To achieve egalitarian fairness, we want to design a policy πthat maximizes the
expected cumulative reward of the least-rewarded user. Let
Su,t:=t/summationdisplay
s=1Xu,s
be the cumulative reward for user uup to time t. Formally, our egalitarian objective is to maximize
minu∈[U]E[Su,T]. However, in line with most works in MAB, we will frame our problem as regret minimiza-
tion. We define the expected cumulative regret as
RT:=Tµ∗
U−min
u∈[U]E[Su,T]
whereµ∗:=µ1+···+µUis the sum of the expected reward of the top Uarms. The choice to compare with
Tµ∗/Uis natural because the maximum expected cumulative reward obtained by the user with the least
reward is at most Tµ∗/U, which is obtained by pulling the best arms in a round robin. To see this, observe
that the sum of cumulative rewards for all Uusers is/summationtext
uE[Su,T]≤Tµ∗. Suppose, to the contrary, that
minuE[Su,T]>Tµ∗/U, then/summationtext
uE[Su,T]>Tµ∗, which is a contradiction.
4EgalUCB Policy
In this section, we describe our policy EgalUCB that achieves near-optimal regret for the EgalMAB problem.
TheEgalUCB policyisbasedonthe UCB1policy(Aueretal.,2002)forsolvingclassicMABproblems; however,
it differs from the UCB1in several key aspects. We present the pseudocode for EgalUCB in Algorithm 1,
complemented by a visual guide in Figure 2. We also provide an alternate version of the pseudocode with
more implementation details in Appendix B.
4Published in Transactions on Machine Learning Research (10/2024)
Algorithm 1: EgalUCB
1let number of blocks Ba,0= 0, cumulative reward Sa,0= 0, and UCBa,0=∞for eacha∈[K]
2forb= 1,2,...,T/U do
3letAb⊆U[K]be a set of Uarms with highest UCBa,b−1
4play armsAbin a round robin fashion for the next Usteps
5updateBa,bandSa,bUaccordingly for each a∈Ab
6update UCBa,b=Sa,bU
Ba,bU+/radicaligg
6 ln(bU)
Ba,bUfor eacha∈Ab
7end
EgalUCB partitions the horizon into B:=T/Ublocks, each with Usteps. We assume, without loss of
generality, that Tis divisible by U. In cases where this is not true, the difference in expected regret between
the best and worst user is at most (µ1+···+µ⌈U/2⌉)−(µK−⌊U/2⌋+···+µU), which is independent of T.
LetA⊆USdenote that set Ais a subset of set Swith sizeU.EgalUCB begins by initializing some statistics
(Line 1). At the start of each block b∈[B], it selects any set Ab⊆U[K]consisting of the highest-ranked U
distinct arms as determined by their upper confidence bounds UCBa,b(Line 3). Over a block with Usteps,
these arms are then assigned to the Uusers in a round-robin fashion (Line 4). After observing the rewards,
EgalUCB updates its statistics (Lines 5–6).
Since each user is assigned to every arm in Abexactly once, we have E[Su,T] =E[Su′,T]for allu,u′∈[U].
From a technical standpoint, this simplifies the regret by eliminating the minoperator in the regret RT:
RT=Tµ∗
U−min
u∈[U]E[Su,T] =Tµ∗
U−E[Su,T],∀u∈[U].
5 Main Results
In this section, we present our main theoretical results. We first state some necessary definitions and
notations. Then, we present the regret upper bounds for the EgalUCB policy. Following that, we discuss the
policy-independent regret lower bound for EgalMAB.
LetXa,tbe the reward obtained from playing arm afor thet-th time across all users. Note that if user u
plays armaat time step t, then the reward obtained is Xu,t=Xa,Ta,twhereTa,tis the number of times
armais played up till time step t.
LetBa,bbe the number of blocks that arm ais played up till block b. Note that Ba,b=Ta,bU. Furthermore,
let
ˆµa,b:=1
bUbU/summationdisplay
t=1Xa,t
be the empirical estimate of µaafter playing arm aforbblocks. As the policy observe more rewards, it gains
confidence about its estimate of µa. This level of confidence is captured by
ϵb,b′:=/radicalbigg
6 ln(bU)
b′U,
which is the confidence radius of playing an arm for b′blocks after block b.
DenoteA∗:= [U]⊆U[K]as the set of best Uarms. For any set of arms A⊆[K]of not necessarily size U,
letA−:=A\A∗be the setAwith the best Uarms removed. Furthermore, let the expected reward of Abe
µA:=/summationdisplay
a∈Aµa.
5Published in Transactions on Machine Learning Research (10/2024)
Thesub-optimalitygapof Aisdefinedas ∆A:=µA∗−µA. Intheextremecases, themaximumsub-optimality
gap
∆max:= (µ1+···+µU)−(µK−U+1+···+µK)
is obtained by selecting the worst set of Uarms, and the minimum non-zero sub-optimality gap ∆min:=
µU−µU+1is obtained by replacing arm UinA∗with armU+ 1, assuming that µU̸=µU+1.
Letν= (p1,...,pK)be an instance of EgalMAB. We say that νis a1-subgaussian EgalMAB if for all arms
a∈[K],X∼pais a1-subgaussian random variable. Theorems 1 and 2 respectively provide problem-
dependent and problem-independent upper bounds for the expected cumulative regret of running EgalUCB
on a 1-subgaussian EgalMAB.
Theorem 1 (Problem-Dependent Upper Bound) .Let(ν,T,U )a1-subgaussian EgalMAB. After running
EgalUCB forTtime steps, we have
RT≤2136(K−U) ln(T)
∆min+4K∆max
U.
Theorem 2 (Problem-Independent Upper Bound) .Let(ν,T,U )be a1-subgaussian EgalMAB withµa∈[0,1]
for all arms a∈[K]. After running EgalUCB forTtime steps, we have
RT≤/radicalbigg
8544(K−U)Tln(T)
U+4Kmin{U,K−U}
U.
When the number of arms Kand usersUare fixed, the problem-independent upper bound increases with
the number of time steps Tat a rate of O(/radicalbig
Tln(T)). Notably, when U= 1, the EgalMAB instance and the
EgalUCB policy reduce to the classic MAB instance and the UCB1policy (Auer et al., 2002). Consequently,
both the problem-dependent and problem-independent upper bounds can be reduced to the bound for classic
UCB1with minimal effort1.
Next, we examine how the number of users Uaffects performance. Consider some fixed time horizon Tand
number of arms K. SinceT≫K, the first terms in both the problem-independent and problem-dependent
upper bounds dominates the regrets. Furthermore, when K=U, every user would have played every arm
exactly once after each block, yielding an expected cumulative reward of bµ∗after block b. By definition,
this implies that the expected cumulative regret RT= 0. This behavior is reflected in both upper bounds,
sinceK−U= 0and∆max= 0whenK=U.
Additionally, the problem-independent upper bound decreases as Uapproaches K, and this reduction scales
withO(1/√
U). There are two reasons for this. Firstly, if we fix some EgalMAB instanceνand varyU,
then increasing Uresults in decreasing Tµ∗/U. Secondly, since EgalUCB assigns the arms in a round-robin
fashion during each block, as long as the UCB values of the top arms are consistently among the highest
regardless of their order, EgalUCB will also consistently select a good set of arms. This implies that as U
increases, this problem becomes more statistically robust to the variability inherent in the estimates of the
arms. Loosely speaking, the more users we have, the easier it is to match the performance of a policy always
plays round-robin the set of arms A∗.
We further consider the scope for algorithmic improvement by deriving a policy-independent lower bound.
Letν= (p1,...,pK)be an EgalMAB instance and πbe any policy. We denote Rπνas the expected cumulative
regret of running πonνforTtime steps. Theorem 3 provides a policy-independent lower bound for the
regretRπν. This bound applies to the class Vof all Gaussian EgalMAB instancesν= (p1,...,pK)where, for
alla∈[K], the reward density pa=N(µa,1)andµa∈[0,1].
Theorem 3 (Policy-Independent Lower Bound) .SupposeK≥2U. For any policy π, there exist an EgalMAB
instanceν∈Vwith regret
Rπν≥/radicalbig
(K−U)T
76U.
1Refer to the notes at the end of Lemma 5 for the details.
6Published in Transactions on Machine Learning Research (10/2024)
The lower bound suggests that EgalUCB is tight in Tup to logarithmic factors. This factor is expected
when using a UCB-based policy due to the choice of confidence radius ϵb,b′. Drawing parallels to the MOSS
policy (Audibert & Bubeck, 2009) in classic K-armed MAB problems, we also conjecture that there exists a
MOSS-based policy that can shave away the√logTterm in the regret bound.
Furthermore, there is a multiplicative gap of 1/√
Ubetween the lower bound and the problem-independent
upper bound. Our experimental results in Section 7 suggest that our upper bound analysis is not tight, as
we empirically observe the O(1/U)behavior when running EgalUCB.
6 Regret Analysis
In this section, we provide a proof sketch for the main results in Section 5. Our analysis relies on techniques
developed in the combinatorial semi-bandits literature (Kveton et al., 2015a). Detailed proofs for the upper
bounds and lower bound can be found in Appendices C and D respectively.
6.1 Problem-Dependent Upper Bound
The regret upper bound in Theorem 1 consists of a sum of two terms: the first term is dependent on T
and the second is independent of T. The first (resp. second) term arises from the regret accumulated over
blocks where some goodeventEboccurs (resp. did not occur). This goodeventEbis the event that µaand
its estimate ˆµaare at a distance of at most ϵb−1,Ba,b−1at the beginning of block bfor alla∈[K]. Formally,
we define
Eb:=K/intersectiondisplay
a=1/braceleftig/vextendsingle/vextendsingleˆµa,Ba,b−1−µa/vextendsingle/vextendsingle≤ϵb−1,Ba,b−1/bracerightig
.
Notethatwewilloftenabusethesetnotationwhendefiningevents. Inparticular, whenwehaveaproposition
P, we useE={P}to signify thatEis the set of all outcomes in the underlying probability space where P
holds. Lemma 1 shows that Eboccurs with high probability.
Lemma 1. Let(ν,T,U )be a 1-subgaussian EgalMAB. Then, for all blocks b∈[B],
P(Ec
b)≤2K
b2U3.
To facilitate analysis, we then consider another goodeventFbfor which the set of arms Ab⊆U[K]selected
during block bis sub-optimal but “not too bad”. This is defined as
Fb:=/braceleftigg
0<∆Ab≤2/summationdisplay
a∈A−
bϵB,Ba,b−1/bracerightigg
.
Lemma 2 shows that if the set of arms Abplayed during block bis sub-optimal and Eboccurs, thenFbmust
follow.
Lemma 2. Letb∈[B]. If the set of arms Abplayed during block bis sub-optimal and Eboccurs, thenFb
also occurs.
We proceed by partitioning the regret into two terms: one conditioned on the high-probability event Fbusing
Lemma 2 and another conditioned on the low-probability event Ec
busing Lemma 1. This result is formally
stated in Lemma 3.
Lemma 3. Let(ν,T,U )be a 1-subgaussian EgalMAB. Then, after Ttime steps, for all users u∈[U], we
have
Ru,T≤B/summationdisplay
b=1E[∆AbI{Fb}] +π2K∆max
3U3.
7Published in Transactions on Machine Learning Research (10/2024)
Let us now focus on bounding the contributions made by the high-probability term (i.e., the first term
above). We first partition Fbinto countably many mutually exclusive events {Gb,i}iso that we can write
∆AbI{Fb}=∞/summationdisplay
i=1∆AbI{Gb,i},almost surely .
To define the events {Gb,i}i, letα= 0.13,β= 0.22, and
γ= 24/parenleftbigg1−β√α−β/parenrightbigg2
be constants that are carefully chosen, and define
mb,i:=γαiUln(BU)
∆2
Ab
for alli∈N. Let
Lb,i:={a∈A−
b:Ba,b−1≤mb,i}
to be the set of arms in A−
bthat are played for fewer than mb,iblocks at the beginning of block b. For
convenience, let Lb,0=A−
b. For eachb∈[B]andi∈N, the eventGb,iis then defined as
Gb,i:=i−1/intersectiondisplay
j=1/braceleftbig
|Lb,j|<βjU/bracerightbig
∩/braceleftbig
|Lb,i|≥βiU/bracerightbig
∩/braceleftbig
∆Ab>0/bracerightbig
.
It is clear that at most one of {Gb,i}ican occur. However, to show that it is a partition for Fb, we also need
to show that at least one of {Gb,i}imust happen. This is shown in Lemma 4.
Lemma 4. Assume that b>K/U . On the eventFb, exactly one of the events in {Gb,i}ioccurs.
Using the newly-defined events {Gb,i}b,i, we bound the contributions of the the high-probability term in
Lemma 5. The intuition behind the events {Gb,i}b,i, which is a common construction used in the proof of
combinatorial semi-bandits Kveton et al. (2015a), is that it serves to upper bound the high probability term
in the regret by the number of times the set of arms Abis played. This allows us to introduce the reciprocal
of the gap term for individual arms ∆a,Nawhich then serves to derive a meaningful problem-dependent
bound on the regret.
Lemma 5. Letν= (p1,...,pK). Suppose that pais the density for a 1-subgaussian distribution for all
a∈[K]. Then, after Ttime steps,
B/summationdisplay
b=1E[∆AbI{Fb}] =∞/summationdisplay
i=1B/summationdisplay
b=b0∆AbI{Gb,i}+b0−1/summationdisplay
b=1∆AbI{Fb}
≤2136 ln(BU)/summationdisplay
a∈Λ1
∆a,Na+K∆max
U
≤2136(K−U) ln(BU)
∆min+K∆max
U.
for all users u∈[U].
The proof of Theorem 1 simply involves substituting the result of 5 into Lemma 3.
Proof of Theorem 1. We can bound the regret by
Ru,T≤B/summationdisplay
b=1E[∆AbI{Fb}] +π2K∆max
3U3≤2136(K−U) ln(BU)
∆min+K∆max
U+π2K∆max
3U3
≤2136(K−U) ln(T)
∆min+4K∆max
U
where the first inequality holds due to Lemma 3 and the second inequality holds due to Lemma 5.
8Published in Transactions on Machine Learning Research (10/2024)
6.2 Problem-Independent Upper Bound
Much like the expression in Theorem 1, the first term of the regret bound in Theorem 2 arises from the
cumulative regret accumulated in blocks where the high-probability event Eboccurs. Conversely, the second
term arises from the regret accumulated in the initial blocks and the blocks where Ebdid not occur. The
proof of Theorem 2 can be found in Appendix C.
6.3 Policy-Independent Lower Bound
Our proof for Theorem 3 consists of constructing two EgalMAB instancesνandν′that are “close” enough
that it is difficult for any policy to distinguish between them statistically, yet “far” enough that a sequence
of actions that is good for one instance is bad for the other. Specifically, let
∆ =/radicalbigg
K−U
8TU2
and setν= (p1,...,pK)∈Vto be an EgalMAB instance with
µa=/braceleftigg
∆,ifa∈[U]
0,otherwise.(1)
Assuming that K≥2U, let
A′= arg min
A⊆U[K]\[U]/summationdisplay
a∈AEπν[Ta,T]
be the set of Uarms that are sub-optimal under νhave been played the fewest number of times under the
distribution Pπν. Setν′∈Vto be an EgalMAB instance with
µ′
a=/braceleftigg
2∆,ifa∈A′
µa,otherwise.(2)
LetΓbe the set of size- Usubsets of [K]that contains at least U/2arms from [U], and let
H:=/braceleftigg/summationdisplay
A∈ΓTA,T≤T
2/bracerightigg
be the event that, for at least T/2time steps, the policy πselects a set in which at least half of it consists
of arms from [U]. Lemma 6 uses the Bretagnolle–Huber inequality with Hto reduce the lower bound
computation to evaluating the KL-divergence between PπνandPπν′.
Lemma 6. Letνandν′be the EgalMAB instances defined by equation 1 and equation 2. Under the assump-
tions of Theorem 3, we have
Rπν+Rπν′>∆T
4/parenleftbig
Pπν(H) +Pπν′(Hc)/parenrightbig
≥∆T
8exp/parenleftbig
−DKL(Pπν∥Pπν′)/parenrightbig
.
PartofthenoveltyinouranalysisinvolvesreducingthecomputationofthisKL-divergencetoacombinatorial
problem. Lemma 7 decompose the KL-divergence from Lemma 6 into an expression that involves counting
the number of times each arm a∈A′is played, and Lemma 8 shows, using a counting argument, that the
number of times an arm a∈A′is played is at most TU2/(K−U).
Lemma 7. Letνandν′be the EgalMAB instances defined by equation 1 and equation 2. Under the assump-
tions of Theorem 3, we have
DKL(Pπν∥Pπν′)≤4∆2/summationdisplay
a′∈A′Eπν[Ta′,T].
9Published in Transactions on Machine Learning Research (10/2024)
0 50 100 150
Timestep (in thousands)0369121518Regret (in thousands)Gaussian (σ2= 1.0)
0 50 100 150
Timestep (in thousands)0369121518Gaussian (σ2= 0.1)
0 50 100 150
Timestep (in thousands)0369121518Bernoulli
1 users 2 users 3 users 4 users 5 users
Figure 3: Expected regret incurred by EgalUCB overT= 150,000time steps on simulated data with K= 10.
Each line corresponds to a different U. The lighter region around each line represents the range between the
minimum and maximum expected regrets observed over a total of 30 independent runs.
Lemma 8. Letνandν′be the EgalMAB instances defined in equation 1 and equation 2. Under the assump-
tions of Theorem 3, we have
/summationdisplay
a∈A′Ta,T≤TU2
K−U
almost surely.
The proof of Theorem 3 simply involves substituting the results of Lemma 7 and Lemma 8 into Lemma 6.
Proof of Theorem 3. We have
RT,π,ν +RT,π,ν′≥∆T
8exp/parenleftigg
−4∆2/summationdisplay
a′∈A′Eπν[Ta′,T]/parenrightigg
≥∆T
8exp/parenleftigg
−4∆2TU2
K−U/parenrightigg
=∆T
8exp(−1/2)>/radicalbig
T(K−U)
38U.
Since 2 max{RT,π,ν,RT,π,ν′}≥RT,π,ν +RT,π,ν′, dividing by 2concludes the proof.
7 Experiments
In this section, we present the results of our numerical experiments to validate the analysis of EgalUCB. These
experimentsincludebothasyntheticenvironment(Section7.1)andreal-worlddatasets(Sections7.2and7.3).
The code for these experiments can be found in the supplementary materials.
7.1 Synthetic Experiments
To empirically verify the problem-independent upper bound in Theorem 2, we conducted experiments on
three synthetic datasets: Gaussian bandits with variance σ2= 1.0, Gaussian bandits with variance σ2= 0.1
and Bernoulli bandits.
In each environment, we fixed K= 10and variedUfrom 1to5. For each choice of U, we ran the experiment
30times. In each run, we randomly generated the ground truth expected reward µafor each arm ausing
a uniform distribution with support [0.01,0.99]. Figure 3 shows the expected regret incurred by EgalUCB
overT= 150,000time steps. As predicted by Theorem 2, the expected regret RTis sub-linear in Tand
diminishes with U.
10Published in Transactions on Machine Learning Research (10/2024)
2122232425262728
Number of users210212214216Regret
Figure 4: Expected regret incurred by EgalUCB over
T= 218time steps on Bernoulli bandits with K=
210arms.
2468101214161820
Number of users061218Regret
(in hundreds)
Figure 5: Expected regret incurred by EgalUCB over
T= 126,000time steps on Bernoulli bandits with
K= 20arms.
0 30 60 90 120 150
Timestep (in thousands)00.61.21.82.4Regret (in thousands)5 users
10 users
15 users
20 users
25 users
Figure 6: Expected regret incurred by EgalUCB over
T= 150,000timestepsontheGoogleClusterUsage
Trace dataset with K= 100. Each line is associated
with a different number of users U.
0 30 60 90 120 150
Timestep (in thousands)02468Regret (in thousands)10 users
20 users
30 users
40 users
50 usersFigure 7: Expected regret incurred by EgalUCB over
T= 150,000time steps on the MovieLens 25M
dataset with K= 500. Each line is associated with
a different number of users U.
We conducted another experiment to verify the rate at which RTdiminishes with UwhenK≫U. We fixed
K= 210andT= 218while varying U∈{21,..., 28}. We ran EgalUCB on instances of Bernoulli EgalMAB
whereµa= 0.8ifa∈[U]and0.5otherwise. This choice of µaensures that µ∗is kept constant for all
choices ofU. Figure 4 shows the log-log plot of RTagainstU. We observe that RTdiminishes with Uat
a rate ofO(U−c)for some constant c≈1.0. This corroborates with our policy-independent lower bound in
Theorem 3 and suggests that our problem-independent upper bound in Theorem 2 may be loose.
To assess the rate at which RTdiminishes with UwhenU→K, we ran a similar experiment with T=
126,000,K= 20, andU∈ {2,4,..., 18,20}on Bernoulli EgalMAB instances. For each U, we ran the
experiment 30times. Figure 5 shows the plot of RTagainstU. We observe that as Uapproaches K, the
regret decreases to 0, and specifically when U=K, the regret is exactly 0. This observation aligns with the
problem-independent upper bound in Theorem 2.
7.2 Google Cluster Usage Trace Dataset
The Google Cluster Usage Traces dataset comprises 2.4 TiB of compressed traces that record the workloads
executed on Google compute cells (Wilkes, 2020). These traces are organized into tables that contain
information about the machines and the instances running on them.
In our experiment, we focus on the InstanceUsage table from the clusterdata_2019 trace. This table
contains traces of both processor and memory usage during instance execution. To adapt to the EgalMAB
setting, we designate each arm aas a machine, uniquely identifiable using the machine_id field in the
table. We implicitly construct its reward distribution Paby drawing an entry uniformly from the trace that
corresponds to the machine and return the negative of the cycles_per_instruction field for its reward.
11Published in Transactions on Machine Learning Research (10/2024)
Given the substantial size of the dataset, we will limit our analysis to the initial 4million entries from the
table. We pick K= 100machines that contain the most amount of trace entries. Then, we formulate
scenarios involving U∈{5,10,15,20,25}unseenusers. At each time step t∈[T]whereT= 150,000, we
employ the EgalUCB policy to assign machines to these users.
Figure 6 shows the expected regret RTover time. Since the distribution Pahas bounded support, it is a
subgaussian distribution. This empirical finding supports the O(/radicalbig
Tln(T))growth rate that Theorem 2
predicts.
7.3 MovieLens 25M Dataset
The MovieLens 25M dataset is widely used in recommender systems (Kużelewska, 2014; Forouzandeh et al.,
2021) and collaborative filtering (He et al., 2017; Álvaro González et al., 2022) research. This dataset
encompasses a substantial collection of 25,000,000user ratings contributed by 162,000users for a repository
of62,000movies.
Toadapttothe EgalMAB setting, werandomlyselect K= 500moviesandtreatthemasarms. Foreachmovie
a∈[K], we implicitly construct its reward distribution Pausing the user ratings provided by existing users in
the dataset. This empirical distribution is categorical and has support residing within {0.5,1.0,..., 4.5,5.0}.
Then, we formulate a scenario involving U∈{10,20,30,40,50}unseenusers. At each time step t∈[T]
whereT= 150,000, we employ the EgalUCB policy to assign movies to these users.
Figure 7 shows the expected regret RTover time. Similar to the Google Cluster Usage Trace dataset, the
distribution Pais subgaussian for all machines a. As such, this empirical finding aligns with the O(/radicalbig
Tln(T))
growth rate that Theorem 2 predicts.
8 Discussion
In this work, we introduced EgalMAB, an extension to the MAB framework with egalitarian considerations.
The EgalUCB policy was proposed and shown to achieve an expected regret of O/parenleftbig/radicalbig
Tln(T)·(K−U)·U−1/parenrightbig
.
We also derived a lower bound that matches the upper bound up to a multiplicative gap of 1/√
Uand a
term logarithmic in T. Our experiments on simulated and real-world data validated the theoretical analysis.
Our empirical results lead us to conjecture that EgalUCB is indeed tight with respect to U. This gap could
potentially be reconciled with a more refined analysis of the upper bound in future work. Other future works
include:
Adversarial semi-bandits. It is natural to consider the adversarial variant of our setup. After choosing
a randomized assignment at each time step, an adaptive online adversary chooses the reward for each arm.
Since that the set of all randomized assignments B(a.k.a. the Birkhoff polytope) is a convex set and the
expected reward given a randomized assignment is a convex function, we conjecture that a modification
ofComponent Hedge (Koolen et al., 2010) and PermELearn (Helmbold & Warmuth, 2009) can achieve an
asymptotically near-optimal solution under the egalitarian consideration.
Thompson Sampling. Another possible direction is to develop a Thompson sampling approach for the
EgalMAB problem, potentially by adapting Combinatorial Thompson Sampling (Wang & Chen, 2018).
Arms with capacity. Suppose that at each time step, we can assign at most Cusers to each arm.
When multiple users are assigned to the same arm at a given time step, we assume that they receive the
same reward. This scenario particularizes to our setting when C= 1. Since the optimal assignment is to
round-robin the top U/Carms, we can redefine the regret with µ∗=µ1+···+µU/C. We can show that a
modified version of EgalUCB, in which we split the horizon into B=TC/Ublocks and play the U/Carms
with the highest UCB value, achieves a regret of R(T)≤2136(K−U/C) ln(T)∆−1
min+KCU−1∆maxwhere
∆minand∆maxare redefined by replacing UbyU/C. We conjecture that this bound may be improved by
dividing the horizon into Cphases in which we eliminate all except K/carms in phase c∈[C]and play the
U/carms with the highest UCB values in a round-robin fashion. If one can prove that the top U/carms
survive the elimination after each phase with high probability, it is possible to achieve a factor of C−1in the
first term in the upper bound on R(T).
12Published in Transactions on Machine Learning Research (10/2024)
Acknowledgments
This research/project is supported by the National Research Foundation, Singapore under its AI Singapore
Programme (AISG Award No: AISG2-PhD/2021-08-011). This research/project is also supported by a
Ministry of Education Tier 2 grant under grant number A-9000423-00-00.
References
Álvaro González, Fernando Ortega, Diego Pérez-López, and Santiago Alonso. Bias and unfairness of collab-
orative filtering based recommender systems in movielens dataset. IEEE Access , 10:68429–68439, 2022.
doi: 10.1109/ACCESS.2022.3186719.
V. Anantharam, P. Varaiya, and J. Walrand. Asymptotically efficient allocation rules for the multiarmed
bandit problem with multiple plays-part i: I.i.d. rewards. IEEE Transactions on Automatic Control , 32
(11):968–976, 1987. doi: 10.1109/TAC.1987.1104491.
Jean-Yves Audibert and Sébastien Bubeck. Minimax policies for adversarial and stochastic bandits. In
Proceedings of the 22nd Annual Conference on Learning Theory , Berlin, Germany, 2009. Springer-Verlag.
Peter Auer, Nicolò Cesa-Bianchi, and Paul Fischer. Finite-time analysis of the multiarmed bandit problem.
Mach. Learn. , 47(2–3):235–256, May 2002. doi: 10.1023/A:1013689704352.
Orly Avner and Shie Mannor. Concurrent bandits and cognitive radio networks, 2014.
Nicolò Cesa-Bianchi and Gábor Lugosi. Combinatorial bandits. Journal of Computer and System Sciences ,
78(5):1404–1422, 2012. doi: https://doi.org/10.1016/j.jcss.2012.01.001.
Wei Chen, Yajun Wang, Yang Yuan, and Qinshi Wang. Combinatorial multi-armed bandit and its extension
to probabilistically triggered arms. J. Mach. Learn. Res. , 17(1):1746–1778, Jan 2016.
Yifang Chen, Alex Cuellar, Haipeng Luo, Jignesh Modi, Heramb Nemlekar, and Stefanos Nikolaidis. The
fair contextual multi-armed bandit. In Proceedings of the 19th International Conference on Autonomous
Agents and MultiAgent Systems , AAMAS’20, pp.1810–1812, California, UnitedStates, 2020.International
Foundation for Autonomous Agents and Multiagent Systems.
Houston Claure, Yifang Chen, Jignesh Modi, Malte Jung, and Stefanos Nikolaidis. Multi-armed bandits
with fairness constraints for distributing resources to human teammates. In Proceedings of the 2020
ACM/IEEE International Conference on Human-Robot Interaction , HRI ’20, pp. 299–308, Cambridge,
United Kingdom, 2020. Association for Computing Machinery. doi: 10.1145/3319502.3374806. URL
https://doi.org/10.1145/3319502.3374806 .
KaizeDing, JundongLi, andHuanLiu. Interactiveanomalydetectiononattributednetworks. In Proceedings
of the Twelfth ACM International Conference on Web Search and Data Mining , WSDM ’19, pp. 357–365,
New York, United States, 2019. Association for Computing Machinery. doi: 10.1145/3289600.3290964.
AudreyDurand, CharisAchilleos, DemetrisIacovides, KaterinaStrati, GeorgiosD.Mitsis, andJoellePineau.
Contextual bandits for adapting treatment in a mouse model of de novo carcinogenesis. In Finale Doshi-
Velez, Jim Fackler, Ken Jung, David Kale, Rajesh Ranganath, Byron Wallace, and Jenna Wiens (eds.),
Proceedings of the 3rd Machine Learning for Healthcare Conference , volume 85 of Proceedings of Machine
Learning Research , pp. 67–82, California, United States, 2018. PMLR. URL https://proceedings.mlr.
press/v85/durand18a.html .
Saman Forouzandeh, Kamal Berahmand, and Mehrdad Rostami. Presentation of a recommender system
with ensemble learning and graph embedding: A case on movielens. Multimedia Tools and Applications ,
80(5):7805–7832, Feb 2021. doi: 10.1007/s11042-020-09949-5.
Yi Gai, Bhaskar Krishnamachari, and Rahul Jain. Combinatorial network optimization with unknown
variables: Multi-armed bandits with linear rewards and individual observations. IEEE/ACM Transactions
on Networking , 20(5):1466–1478, 2012. doi: 10.1109/TNET.2011.2181864.
13Published in Transactions on Machine Learning Research (10/2024)
Xiangnan He, Lizi Liao, Hanwang Zhang, Liqiang Nie, Xia Hu, and Tat-Seng Chua. Neural collaborative
filtering. In Proceedings of the 26th International Conference on World Wide Web , WWW’17, pp.173–182,
Geneva, Switzerland, 2017.InternationalWorldWideWebConferencesSteeringCommittee. doi: 10.1145/
3038912.3052569.
David P. Helmbold and Manfred K. Warmuth. Learning permutations with exponential weights. Journal of
Machine Learning Research , 10(58):1705–1736, 2009. URL http://jmlr.org/papers/v10/helmbold09a.
html.
Safwan Hossain, Evi Micha, and Nisarg Shah. Fair algorithms for multi-agent multi-armed bandits. In
M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan (eds.), Advances
in Neural Information Processing Systems , volume 34, pp. 24005–24017, New York, United States,
2021. Curran Associates, Inc. URL https://proceedings.neurips.cc/paper_files/paper/2021/
file/c96ebeee051996333b6d70b2da6191b0-Paper.pdf .
Matthew Joseph, Michael Kearns, Jamie Morgenstern, and Aaron Roth. Fairness in learning: Clas-
sic and contextual bandits. In D. Lee, M. Sugiyama, U. Luxburg, I. Guyon, and R. Garnett (eds.),
Advances in Neural Information Processing Systems , volume 29, New York, United States, 2016.
Curran Associates, Inc. URL https://proceedings.neurips.cc/paper_files/paper/2016/file/
eb163727917cbba1eea208541a643e74-Paper.pdf .
Matthew Joseph, Michael Kearns, Jamie Morgenstern, Seth Neel, and Aaron Roth. Meritocratic fairness for
infinite and contextual bandits. In Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and
Society, AIES ’18, pp. 158–163, New York, United States, 2018. Association for Computing Machinery.
doi: 10.1145/3278721.3278764.
Wassim Jouini, Damien Ernst, Christophe Moy, and Jacques Palicot. Multi-armed bandit based policies for
cognitive radio’s decision making issues. In 2009 3rd International Conference on Signals, Circuits and
Systems (SCS) , Medenine, Tunisia, 2009. Institute of Electrical and Electronics Engineers (IEEE). doi:
10.1109/ICSCS.2009.5412697.
Junpei Komiyama, Junya Honda, and Hiroshi Nakagawa. Optimal regret analysis of thompson sampling
in stochastic multi-armed bandit problem with multiple plays. In Proceedings of the 32nd International
Conference on International Conference on Machine Learning , volume 37 of ICML’15 , pp. 1152–1161,
Lille, France, 2015. JMLR.org.
Wouter M. Koolen, Manfred K. Warmuth, and Jyrki Kivinen. Hedging structured concepts. Virtual Reality ,
2010. URL https://api.semanticscholar.org/CorpusID:70290123 .
George Kousiouris, Tommaso Cucinotta, and Theodora Varvarigou. The effects of scheduling, workload
type and consolidation scenarios on virtual machine performance and their prediction through optimized
artificial neural networks. Journal of Systems and Software , 84(8):1270–1291, 2011. doi: https://doi.org/
10.1016/j.jss.2011.04.013.
Urszula Kużelewska. Clustering algorithms in hybrid recommender system on movielens data. Studies in
Logic, Grammar and Rhetoric , 37:125–139, Jan 2014. doi: 10.2478/slgr-2014-0021.
Branislav Kveton, Zheng Wen, Azin Ashkan, Hoda Eydgahi, and Brian Eriksson. Matroid bandits: fast
combinatorial optimization with learning. In Proceedings of the Thirtieth Conference on Uncertainty in
Artificial Intelligence , UAI’14, pp. 420–429, 2014.
Branislav Kveton, Zheng Wen, Azin Ashkan, and Csaba Szepesvari. Tight regret bounds for stochas-
tic combinatorial semi-bandits. In Guy Lebanon and S. V. N. Vishwanathan (eds.), Proceedings of
the Eighteenth International Conference on Artificial Intelligence and Statistics , volume 38 of Pro-
ceedings of Machine Learning Research , pp. 535–543, California, United States, 2015a. PMLR. URL
https://proceedings.mlr.press/v38/kveton15.html .
14Published in Transactions on Machine Learning Research (10/2024)
Branislav Kveton, Zheng Wen, Azin Ashkan, and Csaba Szepesvári. Combinatorial cascading bandits. In
Proceedings of the 28th International Conference on Neural Information Processing Systems , volume 1 of
NIPS’15, pp. 1450–1458, Massachusetts, United States, 2015b. MIT Press.
Fengjiao Li, Jia Liu, and Bo Ji. Combinatorial sleeping bandits with fairness constraints. IEEE Transactions
on Network Science and Engineering , 7(3):1799–1813, 2020. doi: 10.1109/TNSE.2019.2954310.
Keqin Liu and Qing Zhao. Distributed learning in multi-armed bandit with multiple players. IEEE Trans-
actions on Signal Processing , 58(11):5667–5681, 2010. doi: 10.1109/TSP.2010.2062509.
Jonas Mueller, Vasilis Syrgkanis, and Matt Taddy. Low-rank bandit methods for high-dimensional dynamic
pricing. In Proceedings of the 33rd International Conference on Neural Information Processing Systems ,
New York, United States, 2019. Curran Associates Inc.
John Rawls. A Theory of Justice: Original Edition . Harvard University Press, Massachusetts, United States,
1971. URL http://www.jstor.org/stable/j.ctvjf9z6v .
Ayush Sawarni, Soumybrata Pal, and Siddharth Barman. Nash regret guarantees for linear bandits, 2023.
Weiwei Shen, Jun Wang, Yu-Gang Jiang, and Hongyuan Zha. Portfolio choices with orthogonal bandit
learning. In Proceedings of the 24th International Conference on Artificial Intelligence , IJCAI’15, pp.
974–980, California, United States, 2015. AAAI Press.
Lequn Wang, Yiwei Bai, Wen Sun, and Thorsten Joachims. Fairness of exposure in stochastic bandits, 2021.
Siwei Wang and Wei Chen. Thompson sampling for combinatorial semi-bandits. In Jennifer Dy and
Andreas Krause (eds.), Proceedings of the 35th International Conference on Machine Learning , vol-
ume 80 of Proceedings of Machine Learning Research , pp. 5114–5122. PMLR, Jul 2018. URL https:
//proceedings.mlr.press/v80/wang18a.html .
Xuchuang Wang, Hong Xie, and John C. S. Lui. Multiple-play stochastic bandits with shareable finite-
capacity arms. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and
Sivan Sabato (eds.), Proceedings of the 39th International Conference on Machine Learning , volume 162
ofProceedings of Machine Learning Research , pp. 23181–23212, Maryland, United States, 2022. PMLR.
URL https://proceedings.mlr.press/v162/wang22af.html .
Zheng Wen, Branislav Kveton, Michal Valko, and Sharan Vaswani. Online influence maximization under
independent cascade model with semi-bandit feedback. In Proceedings of the 31st International Confer-
ence on Neural Information Processing Systems , NIPS’17, pp. 3026–3036, New York, USA, 2017. Curran
Associates Inc.
John Wilkes. Yet more Google compute cluster trace data. Google research blog, Apr 2020. Posted at
https://ai.googleblog.com/2020/04/yet-more-google-compute-cluster-trace.html .
15