The Empirical Impact of Neural Parameter
Symmetries, or Lack Thereof
Derek Lim∗
MIT CSAIL
dereklim@mit.eduTheo (Moe) Putterman∗
UC Berkeley
moeputterman@berkeley.edu
Robin Walters
Northeastern UniversityHaggai Maron
Technion, NVIDIAStefanie Jegelka
TU Munich, MIT
Abstract
Many algorithms and observed phenomena in deep learning appear to be affected
by parameter symmetries — transformations of neural network parameters that
do not change the underlying neural network function. These include linear mode
connectivity, model merging, Bayesian neural network inference, metanetworks,
and several other characteristics of optimization or loss-landscapes. However, theo-
retical analysis of the relationship between parameter space symmetries and these
phenomena is difficult. In this work, we empirically investigate the impact of neural
parameter symmetries by introducing new neural network architectures that have re-
duced parameter space symmetries. We develop two methods, with some provable
guarantees, of modifying standard neural networks to reduce parameter space sym-
metries. With these new methods, we conduct a comprehensive experimental study
consisting of multiple tasks aimed at assessing the effect of removing parameter
symmetries. Our experiments reveal several interesting observations on the empiri-
cal impact of parameter symmetries; for instance, we observe linear mode connec-
tivity between our networks without alignment of weight spaces, and we find that
our networks allow for faster and more effective Bayesian neural network training.
Our code is available at https://github.com/cptq/asymmetric-networks .
1 Introduction
Neural networks have found profound empirical success, but have many associated behaviors and
phenomena that are difficult to understand. One important property of neural networks is that they
generally have many parameter space symmetries — for any set of parameters, there are typically
many other choices of parameters that correspond to the same exact neural network function [ 24].
For instance, permutations of hidden neurons in a multi-layer perceptron (MLP) induce permutations
of weights that leave the overall input-output relationship unchanged. These parameter symmetries
are a type of (not-necessarily detrimental) redundancy in the parameterization of neural networks,
that adds much non-Euclidean structure to parameter space.
Parameter space symmetries appear to influence several phenomena observed in neural networks.
For example, when linearly interpolating between the parameters of two independently trained
networks with the same architecture, the intermediate networks typically perform poorly [ 59,13].
However, if we first align the two networks via a permutation of parameters that does not affect
the network function, then the intermediate networks can perform just as well as the unmerged
networks [ 59,1]. In some sense, this suggests that neural network loss landscapes are more convex
∗Equal contribution
38th Conference on Neural Information Processing Systems (NeurIPS 2024).Standard -Asymmetric -Asymmetric
Trained
FixedFiGLUFigure 1: (Left) Standard MLP. The hidden nodes (grey hatches) can be freely permuted, which
induces permutation parameter symmetries. Black edges denote trainable parameters. (Middle) Our
W-Asymmetric MLP, which fixes certain weights to be constant and untrainable (colored dashed
lines) to break parameter symmetries. (Right) Our σ-Asymmetric MLP, which uses our FiGLU
nonlinearity involving a fixed matrix F(colored dashed lines) to break parameter symmetries.
or well-behaved after removing permutation symmetries. Other areas that parameter symmetries
play a role in include interpretability of neurons [ 20], optimization [ 48,84,80], model merging [ 60],
learned equivariance [ 7], Bayesian deep learning [ 34], loss landscape geometry [ 54], processing
neural network weights as input data using metanetworks [ 39], and generalization measures [ 49,11].
To rigorously study the effect of parameter symmetries, we study the effect of removing them. In
particular, we introduce two ways of modifying neural network architectures to remove parameter
space symmetries (see Figure 1):
(1)W-Asymmetric networks fix certain elements of each linear map to break symmetries in
the computation graph.
(2)σ-Asymmetric networks use a new nonlinearity (FiGLU) that does not act elementwise, and
hence does not induce symmetries such as permutations.
These two approaches are inspired by previous work, which shows that both symmetries of computa-
tion graphs [ 39] and equivariances of nonlinearities [ 20] induce parameter symmetries in standard
neural networks. We theoretically prove that both of our approaches remove parameter symmetries
under certain conditions. Our Asymmetric networks are similar structurally to standard networks
and can be trained with standard backpropagation and first-order optimization algorithms like Adam.
Thus, they are a reasonable “counterfactual” system for studying neural networks that are similar to
standard neural networks, but that do not have as many parameter symmetries.
With our Asymmetric networks, we run a suite of experiments to study the effects of removing
parameter symmetries on several base architectures, including MLPs, ResNets, and graph neural
networks. We investigate linear mode connectivity, Bayesian deep learning, metanetworks, and
monotonic linear interpolation. Through the lenses of linear mode connectivity and monotonic linear
interpolation, we see that the loss landscapes of our Asymmetric networks are remarkably more
well-behaved and closer to convex than the loss landscapes of standard neural networks. When using
our Asymmetric networks as the base model in a Bayesian neural network, we find faster training
and better performance than using standard neural networks that have many parameter symmetries.
When using metanetworks to predict properties such as test accuracy of an input neural network, we
see that all tested metanetworks more accurately predict the accuracy of Asymmetric networks than
standard networks. Overall, our Asymmetric networks provide valuable insights for empirical study
and hold promise for advancing our understanding of the impact of neural parameter symmetries.
2 Background and Definitions
LetΘbe the space of parameters of a fixed neural network architecture. For any choice of parameters
θ∈Θ, we have a neural network function fθ:X → Y from an input space Xto an output space Y.
We call a function ϕ:Θ→Θaparameter space symmetry iffθ(x) =fϕ(θ)(x)for all inputs xand
parameters θ∈Θ(i.e. if fθandfϕ(θ)are always the same function).
For instance, consider a two-layer MLP with no biases, parameterized by matrices θ= (W2,W1)
with an elementwise nonlinearity σ. Then fθ(x) =W2σ(W1x). Let Pbe a permutation matrix,
2and let ϕ(θ) = (W2P⊤, PW1). Then for any input x,
fϕ(θ)(x) =W2P⊤σ(PW1x) =W2P⊤Pσ(W1x) =W2σ(W1x) =fθ(x), (1)
soϕis a parameter space symmetry. A key step is the second equality, which holds because
Pσ(x) =σ(Px): any elementwise nonlinearity σis permutation equivariant. Any other equivariance
ofσalso induces a parameter symmetry; for instance, if σ(x) = max(0 , x)is the ReLU function, then
ασ(x) =σ(αx)for any α >0, so there is a positive-scaling-based parameter symmetry [ 49,11,20].
3 Related Work
Characterizing parameter space symmetries. While many works spanning several decades have
noted specific parameter space symmetries in neural networks [ 24,61], some works take a more
systematic approach to deriving parameter space symmetries. Godfrey et al. [20] characterize all
global linear symmetries induced by the nonlinearity for two-layer multi-layer perceptrons with
pointwise nonlinearities. Zhao et al. [78] study several types of symmetries, and derive nonlinear,
data-dependent parameter space symmetries. Lim et al. [39] show that graph automorphisms of the
computation graph of a neural network induce permutation parameter symmetries, which captures
hidden neuron permutations in MLPs and hidden channel permutations in CNNs.
Constraints and post-processing to break parameter space symmetries. Several works develop
methods for constraining or post-processing the weights of a single neural network to remove
ambiguities from parameter space symmetries. This includes methods to remove scaling symmetries
induced by normalization layers or positively-homogeneous nonlinearities such as ReLU [6,55,
54,36], methods to remove permutation symmetries [ 55,54,71,36], and methods to remove sign
symmetries induced by odd activation functions [71].
Unlike these previous works, we develop neural network architectures that have reduced parameter
space symmetries. Our models are optimized using standard unconstrained gradient-descent based
methods like Adam. Hence, our networks do not require any non-standard optimization algorithms
such as manifold optimization or projected gradient descent [ 6,55], nor do they require post-training-
processing to remove symmetries or special care during analysis of parameters (such as geodesic
interpolation in a Riemannian weight space [ 54]). These methods based on constraining weights or
post-processing have significantly different optimization and loss landscape properties (for instance,
linear interpolation is not even well-defined on general nonlinear parameter manifolds), so they are
less suitable than our Asymmetric networks for studying phenomena that may generalize to standard
neural networks.
Aligning multiple networks for relative invariance to parameter symmetries. One way to
reduce the impact of parameter symmetries in certain settings, especially for model merging, is to
align the parameters of one network to another. Several methods have been proposed for choosing
permutations that align the parameters of two neural networks of the same architecture, via efficient
heuristics or learned methods [ 4,69,63,13,1,53,47,67]. Other approaches relax the exact
permutation-parameter-symmetry constraint or do additional postprocessing to achieve effective
merging of models in parameter space [ 59,28,29,60,56]. As our Asymmetric networks have
removed parameter symmetries, they can often be successfully merged and linearly interpolated
between without any alignment.
4 Asymmetric Networks
We develop two methods of parameterizing neural network architectures without parameter sym-
metries, both of which are justified by theoretical results. We can prove that our σ-Asym networks
have permutation and scale symmetries removed, and that our W-Asym networks have permutation
symmetries removed. Although we have not formally proven that W-Asym networks have scale
symmetries removed, we believe that they do (intuitively, the fixed weights fix a scale).
We first focus on the case of fully-connected MLPs with no biases, which take the form fθ(x) =
WLσ(WL−1···σ(W1x))for weights θ= (WL, . . . ,W1)and nonlinearity σ. Then in Section 4.3,
we discuss how we use these approaches to remove parameter symmetries in other architectures (e.g.
CNNs, GNNs) as well.
3Standard
Standard-Asymmetric
(Fix entries) -Asymmetric
(Fix filters) -Asymmetric
Linear
ConvolutionFigure 2: Depiction of our W-Asymmetric approach to removing parameter symmetries. Entries
with a black outline are untrained. Note that the W-Asym linear map has 2 nonzeros per row, the
W-Asym convolution with fixed entries has 8 fixed entries for its single output channel, and the
W-Asym convolution with fixed filters has a single input filter fixed. We often use a constant number
of fixed entries per row or output channel in our experiments.
4.1 Computation Graph Approach ( W-Asymmetric Networks)
Our first approach to developing neural networks with greatly reduced parameter space symmetries
relies on their computation graph. In particular, we can write a feedforward neural network archi-
tecture as a DAG G= (V, E)with neurons as nodes Vand connections between them as edges E.
For a choice of parameters θ∈R|E|, we get a function fθfrom input neuron space to output neuron
space [19, 49, 39]
Lim et al. [39] showed that neural DAG automorphisms ϕ, which are graph automorphisms of the
DAG Gthat preserve types of nodes and weight-sharing constraints, induce permutation parameter
symmetries ϕthat leave the function unchanged: fθ=fϕ(θ). Thus, any feedforward neural network
architecture that has no permutation parameter symmetries must necessarily have a computation
graph with no nontrivial neural DAG automorphisms.
To modify MLPs so they have no nontrivial neural DAG automorphisms, we mask edges in the
computation graph, by setting certain edge weights to constant values that are not updated during
training. For an MLP, we can do this by enforcing that every linear layer T:Rd1→Rd2takes
the form of a matrix W∈Rd2×d1, where each row has a unique pattern of untrained weights. To
achieve this, define a mask M∈ {0,1}d2×d1such that Wijis a trainable parameter if and only if
Mij= 1, and the rows of Mare pairwise distinct binary vectors in {0,1}d1. We call any neural
network with linear maps masked as such a W-Asymmetric neural network. In Appendix B.1, we
show that masking these entries so that they are not trained is sufficient to remove all nontrivial neural
DAG automorphisms.
Theorem 1. If each mask matrix Mhas unique nonzero rows, then W-Asymmetric MLPs with fixed
entries set to zero have no nontrivial neural DAG automorphisms.
In practice, we generate a binary mask Mby randomly selecting a subset of nfixfixed elements for
each row. For the fixed entries, we sample them from a normal distribution N(0, κI)with standard
deviation κ >0that is a hyperparameter that we tune. Our asymmetric linear layer can be written as
W′=M⊙W+ (1−M)⊙F, (2)
where W∈Rd2×d1is a matrix of trainable parameters, and F∈Rd2×d1is a matrix of fixed elements,
sampled from N(0, κI). The only trainable parameters are the unmasked entries of M⊙W, of which
there are d2·(d1−nfix). We empirically find that having κbe significantly larger than the standard
deviation of typical initializations for weight matrices (e.g. κ= 1while the trained coefficients have
standard deviation about 1/√
1000 ) is important for breaking parameter symmetries.
44.2 Nonlinearity Approach ( σ-Asymmetric Networks)
Another approach for removing parameter symmetries is to change the nonlinearity. As studied
by Godfrey et al. [20], equivariances of the nonlinearity induce parameter symmetries in MLPs
with elementwise nonlinearities. Recall that an elementwise nonlinearity acts by using the same
function on each coordinate of the input; σ:Rd→Rdis elementwise if it takes the form
σ(x) = (σ1(x1), . . . , σ 1(xd))for some real function σ1:R→R. Any elementwise nonlinearity is
permutation equivariant, and hence induces a permutation parameter symmetry.
Thus, in contrast to most neural network architectures, for Asymmetric networks we must use
a nonlinearity that does not act elementwise. Likewise, the nonlinearity cannot have any linear
symmetry itself, since if σ◦A=B◦σforA, B∈GL(d), then for a two-layer network:
W2◦σ◦W1=W2B−1B◦σ◦W1=W2B−1◦σ◦AW1. (3)
So(W2,W1)and(W2B−1, AW1)give the same neural network function. Thus, in order to define
a model class without parameter symmetries, it is necessary for σto have no linear equivariances ,
i.e. we desire that if σ◦A=B◦σforA, B∈GL(d), then A=B=I. For two-layer MLPs with
square invertible weights, this is in fact sufficient to remove all parameter symmetries: we prove this
in Appendix B.2.
Proposition 1. Let the parameter space Θbe all pairs of square invertible matrices θ= (W2,W1)
forW2,W1∈GL(d), and let fθ(x) =W2σ(W1x). Ifσhas no linear equivariances, then
fθ1=fθ2if and only if θ1=θ2. In other words, there are no nontrivial parameter space symmetries.
4.2.1 FiGLU: the Fixed Gated Linear Unit Nonlinearity
Motivated by Proposition 1, we define a non-elementwise nonlinearity that does not have the equiv-
ariances of standard nonlinearities. Letting ηbe the sigmoid function η(x) =1
1+e−x, we define our
nonlinearity as
σ(x) =η(Fx)⊙x, (4)
for a randomly sampled, untrained matrix F. Similarly to W-Asym nets, we sample Fas an i.i.d
Gaussian matrix with variance that we tune. This nonlinearity is similar to Swish / SiLU [ 57,26] with
an additional matrix Fto mix feature dimensions (to break permutation equivariance), and it is also
similar to a gated linear unit (GLU) with no trainable parameters [ 9]. Thus, we call our nonlinearity
FiGLU: the Fixed Gated Linear Unit.
In Appendix B.2.1, we prove that FiGLU does not have permutation equivariances or diagonal
equivariances, which are the only equivariances for most elementwise nonlinearities [20].
Proposition 2. With probability 1over the sampling of F, FiGLU has no permutation equivariances
or diagonal equivariances.
We call any network with our symmetry-breaking FiGLU nonlinearity a σ-Asymmetric Network.
4.3 Extension to Other Architectures
The graph-based approach ( W-Asymmetric Networks) works naturally for neural network archi-
tectures with “channel” dimensions, such as convolutional neural networks (CNNs), graph neural
networks (GNNs) [ 22], Transformers [ 66], and equivariant neural networks based on equivariant
linear maps [ 16]. In these types of networks, permutations of entire channels induce permutation
parameter symmetries [ 39]. For such networks, we thus mask entire connections between channels,
e.g. entire filters in CNNs. For CNNs, we also experiment with randomly masking some number of
entries in each filter (instead of masking entire filters), and find that this also works well in removing
parameter symmetries. For neural networks with linear layers that include bias terms, we do not
modify the biases in any way, as they do not introduce new computation graph automorphisms [ 39].
The nonlinearity-based approach ( σ-Asymmetric Networks) can be straightforwardly applied to many
general architectures as well. Though, the fixed matrix Fmay have to be changed to a structured
linear map; for instance, in CNNs we take Fto be a 1D convolution.
50.0 0.2 0.4 0.6 0.8 1.0
Interpolation0.050.100.150.200.250.300.350.40T est LossMLP MNIST
Standard
Rebasin
Asym 
Asym W
0.0 0.2 0.4 0.6 0.8 1.0
Interpolation0.51.01.52.02.53.0T est LossResNet 8x width CIFAR-10
Standard
Rebasin
Asym 
Asym W
0.0 0.2 0.4 0.6 0.8 1.0
Interpolation1.001.251.501.752.002.252.502.75T est LossGNN ogbn-arXiv
Standard
Rebasin
Asym 
Asym WFigure 3: Linear mode connectivity: test loss curves along linear interpolations between trained
networks. (Left) MLP on MNIST. (Middle) ResNet with 8×width on CIFAR-10. (Right) GNN
on ogbn-arXiv. W-Asymmetric networks interpolate the best, followed by networks aligned with
Git-Rebasin, then σ-Asymmetric networks, and finally standard networks.
4.4 Universal Approximation
Our two approaches remove parameter symmetries from standard neural networks, but still intu-
itively retain much of the structure of standard networks. One important property of widely-used
neural network architectures is universal approximation — for any target function of a certain type,
there exists a neural network of the given architecture that approximates the target to an arbitrary
accuracy [ 8,25,42,75]. In Appendix B.3, we show that W-Asymmetric MLPs retain this property:
Theorem 2 (Informal) .Fornfix∈o(n1
4), where nis the hidden dimension, W-Asymmetric MLPs
are Universal Approximators with probability 1over the choice of hardwired entries.
We have not been able to prove a similar result for σ-Asymmetric networks. Classical universal
approximation results for standard neural networks do not apply to σ-Asym nets, as they tend to
assume elementwise nonlinearities.
5 Experiments
5.1 Linear Mode Connectivity without Permutation Alignment
Background. Under certain conditions, neural networks have been found to exhibit linear mode
connectivity, which is when all networks on the line segment in parameter space between two
well-performing trained networks are also well-performing. Starting with Frankle et al. [18], who
coined the term and provided the first in-depth analysis, many works have studied this phenomenon
[44,13,76,14]. When the two networks are randomly initialized and trained independently, linear
mode connectivity generally does not hold [ 13,1]. However, if one of the two networks is permuted
with a parameter symmetry that does not change its function, but that aligns its parameters with the
other network, then linear mode connectivity empirically and theoretically holds for many more model
/ task combinations [ 13,1,79,14]. In fact, Entezari et al. [13] conjectures that if all permutation
symmetries are accounted for, then linear mode connectivity generally holds. Since our Asymmetric
networks remove parameter space symmetries, we may expect linear mode connectivity to hold,
without any post-processing or alignment step.
Hypothesis. Asymmetric networks are more linearly mode connected than standard networks, and
do not require post-processing or alignment of pairs of networks before merging.
Experimental Setup. We consider several networks and tasks: MLPs on MNIST, ResNets [ 23] on
CIFAR-10, and Graph Neural Networks [ 73] on ogbn-arXiv [ 27]. For each architecture and task,
we compute the midpoint test loss barrier: L(1
2θ1+1
2θ2)−1
2(L(θ1) +L(θ2)). This measures how
much worse the interpolated network with parameters1
2θ1+1
2θ2is than the original networks with
parameters θ1andθ2. We measure this barrier for standard networks, pairs of networks aligned with
Git-Rebasin [ 1], and networks with our two approaches ( σ-Asym and W-Asym) applied. To be clear,
whenever we interpolate between the weights of two Asymmetric networks, they have the same exact
fixed weights F(for both W-Asym and σ-Asym) and the same exact mask M(forW-Asym).
Results. Figure 3 plots interpolation curves and Table 1 displays midpoint test loss barriers of various
methods. Our σ-Asymmetric approach lowers the test loss barrier compared to standard networks,
but falls short of the alignment approach of Git-Rebasin. On the other hand, our W-Asymmetric
6approach achieves strong (and sometimes perfect) interpolation, and interpolates better than standard
networks aligned via Git-ReBasin. This may be caused by failure of the Git-ReBasin approaches
to find the optimal permutations, importance of other parameter symmetries besides layer-wise
permutations, or other properties of W-Asymmetric networks.
Table 1: Test loss interpolation barriers at midpoint: L(1
2θ1+1
2θ2)−1
2(L(θ1) +L(θ2)). We use
different methods of breaking symmetries in each column; from left to right: no symmetry breaking,
Git-Rebasin [ 1], our σ-Asym approach, and our W-Asym approach. We report mean and standard
deviation of the barrier across at least 5 pairs of networks, and bold lowest barriers.
Standard Git-ReBasin σ-Asym (ours) W-Asym (ours)
MLP (MNIST) 0.188±.12−.006±.00 0 .117±.01 −0.012±.00
ResNet (CIFAR-10) 3.287±.32 2 .041±.21 2 .521±.46 0.934±.72
ResNet 8x width (CIFAR-10) 2.640±.24 0 .509±.45 1 .492±.15 0.031±.05
GNN (ogbn-arXiv) 1.475±.24 0 .269±.02 0 .901±.11 0.095±.03
5.2 Bayesian Neural Networks
Background. Bayesian deep learning is a promising approach to improve several deficits of main-
stream deep learning methods, such as uncertainty quantification and integration of priors [ 30,51].
However, parameter symmetries are problematic in Bayesian neural networks, as they are a major
source of statistical nonidentifiability [ 30]. Parameter symmetries introduce modes in the posterior
p(θ|D)that make the posterior harder to approximate [ 2,34,72], sample from [ 50,71], and otherwise
analyze [ 36]. For instance, one common technique for training Bayesian neural networks is varia-
tional inference via fitting a Gaussian distribution to the true posterior p(θ|D). This approach suffers
because the Gaussian distribution has only one mode, whereas the true posterior has at least one mode
for every parameter symmetry. As such, some approaches treat kernel matrices are random variables,
which has less symmetries than treating features or weights as random variables, and allows better
approximation by unimodal posteriors [ 74,43]. Instead, we consider traditional, commonly-used
Bayesian deep learning techniques applied on the features or weights of our Asymmetric networks.
Table 2: Bayesian neural network results. Reported loss is the negative log likelihood loss. All
results (except for last column) are after 50 epochs of training. W-Asymmetric networks tend to
improve over their standard counterparts, especially early in training. 16-layer MLPs fail to train,
but 16-layer W-Asymmetric MLPs successfully train. Standard or Asymmetric networks better than
their counterpart by a standard deviation are bolded.
Model Train Loss ↓ Test Loss ↓ ECE↓ Test Acc ↑ Test Acc (25 Epochs) ↑CIFAR-10MLP-8 1.34±.00 1 .24±.01 .039±.009 56 .37±.31 52 .87±0.2
W-Asym MLP-8 1.31±.011.22±.01 .042±.009 57.08±.50 54.15±0.2
MLP-16 2.29±.02 2 .28±.03 .026±.017 13 .54±2.0 13 .34±2.7
W-Asym MLP-16 1.39±.011.27±.01 .045±.009 55.16±.44 51.42±0.3CIFAR-10ResNet20 .596±.01 .535±.03 .045±.007 81 .98±1.2 72 .37±1.0
W-Asym ResNet20 .600±.02 .535±.01 .044±.004 81 .94±0.6 73 .64±1.5
ResNet110 .803±.08 .706±.08 .052±.007 75 .71±2.8 59 .85±3.9
W-Asym ResNet110 .745±.07 .658±.06 .049±.004 77 .40±2.4 63.20±3.0CIFAR-100ResNet20 (BN) 1.68±.03 1 .57±.02 .078±.004 56 .83±.62 46 .80±0.9
W-Asym ResNet20 (BN) 1.62±.021.50±.03 .076±.006 58.40±.62 49.29±0.4
ResNet20 (LN) 1.97±.02 1 .88±.02 .090±.007 50 .02±.54 37 .24±1.1
W-Asym ResNet20 (LN) 1.91±.031.82±.02 .086±.006 51.20±.47 39.03±1.0
Hypothesis. Using Asymmetric networks as the base model improves Bayesian neural networks, as
the posterior will have less modes.
Experimental setup. We train Standard Bayesian and Asymmetric Bayesian Networks for image
classification using variational inference. We use the method of [ 64] for variational inference, which
fits a Gaussian approximate posterior with a diagonal plus low-rank covariance. We train 10 instances
of each model and then report train loss, test loss, test accuracy, and Expected Calibration Error
(ECE) [45], which is a measure of calibration.
Results. See training curves in Figure 4, and quantiative results in Table 2. Using W-Asymmetric
networks as a base for Bayesian deep learning improves training speed and convergence. Most
7strikingly, Bayesian MLPs of depth 16 cannot train at all, while W-Asymmetric Bayesian MLPs
train well. In general, the W-Asymmetric approach improves training and test accuracy across the
several models (MLPs, ResNets of varying sizes, and ResNets with either batch norm or layer norm).
0 10 20 30 40 50
Epoch101
100Train LossMLP MNIST
Standard
W-Asym
0 10 20 30 40 50
Epoch1002×1003×100Train LossResNet110 CIFAR-10
Standard
W-Asym
0 10 20 30 40 50
Epoch2×1003×1004×100Train LossResNet20 BN CIFAR-100
Standard
W-Asym
Figure 4: Bayesian neural network training loss over time for depth 8 MLPs on MNIST (left),
ResNet110 on CIFAR-10 (middle), and ResNet20 with BatchNorm on CIFAR-100 (right). W-
Asymmetric networks train more quickly, and achieve lower training loss.
5.3 Metanetworks
Table 3: Metanetwork performance for predicting the test accuracy of small ResNets and our W-
Asym ResNets. Each row is a different metanetwork. Reported are R2and Kendall τon the test set
— higher is better.
ResNet W-Asym ResNet
R2τ R2τ
MLP .330±.04 .389±.03 .594±.12 .864±.01
DMC [12] .950±.01 .787±.02 .967±.01 .911±.01
DeepSets [77] .855±.01 .617±.03 .936±.00 .858±.00
StatNN [65] .976±.00 .866±.00 .978±.00 .935±.01
Background. Metanetworks [ 39] — also referred to as deep weight-space networks [ 46,58],
meta-models [ 35], or neural functionals [ 81,82,83] — are neural networks that take as inputs the
parameters of other neural networks. Recent work has found that making metanetworks invariant or
equivariant to parameter-space symmetries of the input neural networks can substantially improve
metanetwork performance [46, 81, 39, 32].
Hypothesis. Asymmetric networks are easier to train metanetworks on because they do not have to
explicitly account for symmetries.
Experimental setup. We experiment with metanetworks on the task of predicting the CIFAR-10 test
accuracy of an input image classifier, which many metanetworks have been tested on [ 65,12,81,39].
We use metanetworks based on simple MLPs, 1D-CNN metanetworks [ 12], and metanetworks that
are exactly invariant to permutation parameter symmetries: DeepSets [ 77] and StatNN [ 65]. We
train two separate datasets of 10,000 image classifiers: one dataset of small ResNet models, and one
dataset of W-Asymmetric ResNet models. More information on the data, metanetworks, and training
details are in Appendix F.3.
Results. In Table 3, we see that metanetworks are signficantly better at predicting the performance
of our W-Asymmetric ResNets than standard ResNets. Interestingly, simple MLP metanetworks,
which view the input parameters as a flattened vector, can predict the test accuracy of Asymmetric
Networks quite well, but fail on standard networks. Also, the permutation equivariant metanetworks
(DeepSets and StatNN) both improve on W-Asym ResNets compared to on ResNets, even though
the permutation symmetries of standard ResNets do not affect these metanetworks; thus, it may
be possible that other symmetries in standard ResNets (but not Asym-ResNets) harm metanetwork
performance, or they may be other factors besides symmetries that improve metanetwork performance
for Asym-ResNets.
80.0 0.2 0.4 0.6 0.8 1.0
Interpolation12345Train LossStandard ResNet
0.0 0.2 0.4 0.6 0.8 1.0
Interpolation0.51.01.52.02.5Train Loss-Asymmetric ResNet
0.0 0.2 0.4 0.6 0.8 1.0
Interpolation2468Train LossW-Asymmetric ResNetFigure 5: Train loss against interpolation coefficient αfor the interpolation (1−α)θ0+αθTbetween
initial parameters θ0and trained parameters θT. Trajectories for the 20 (θ0, θT)pairs of lowest train
loss for each architecture are plotted. The trajectories for Asymmetric ResNets appear significantly
more monotonic and convex.
Table 4: Monotonic linear interpolation: properties of linear interpolations between 300 pairs
of initialization and trained parameters. Arrows denote behavior that is more similar to convex
optimization, e.g. there is a downarrow (↓)next to ∆because convex objectives have nonpositive
∆, while nonconvex can have positive ∆. For both types of Asymmetric networks, all differences
from Standard ResNets are statistically significant ( p < .001) under a two-sided T-test: Asymmetric
networks have significantly more monotonic and convex linear interpolations from initialization.
∆↓ Percent Monotonic ↑Local Convexity ↑Global Convexity ↑
Standard ResNet .079±.109 26 .3% .548±.139 .823±.229
σ-Asym ResNet .004±.047 87 .3% .675±.143 .976±.098
W-Asym ResNet −.027±.026 100% .769±.165 1.00±.000
5.4 Monotonic Linear Interpolation
Background. One common method of studying the loss landscapes of neural networks is by
studying the one-dimensional line segment of parameters attained by linear interpolation between
parameters at initialization and parameters after training. Many works have observed monotonic
linear interpolation (MLI), which is when the training loss monotonically decreases along this
line segment [ 21,17,41,68]. Loss landscapes of convex problems have this property as well, so
presence of the monotonic linear interpolation property has been used as a rough measure of how
well-behaved the loss landscape is. However, with many types of models, tasks, or hyperparameter
settings, monotonic linear interpolation does not hold [ 41,68], or there is a large plateau where the
loss barely changes for much of the line segment [ 17,70]; neither of these properties can happen for
convex objectives trained to completion. To the best of our knowledge, there has been little work on
the role of parameter symmetries — or lack thereof — in monotonic linear interpolation (besides
one minor experiment in [ 41] Appendix C.9). Nonetheless, since removing parameter symmetries
substantially improves linear interpolation between trained networks (Section 5.1), one may expect
removing parameter symmetries to improve monotonic linear interpolation.
Hypothesis. The training loss along the line segment between initialization and trained parameters is
more monotonic and convex for Asymmetric networks.
Experimental setup. For the learning task, we follow the setup used for creating the dataset
of image classifiers in Section 5.3. In particular, we train 300 standard ResNets and 300 W-
Asymmetric ResNets with varying hyperparameters sampled from the same distributions as used for
the dataset of image classifiers (see Appendix Table 13). For each of these networks, we linearly
interpolate between its initial parameters θ0and its final trained parameters θT:(1−α)θ0+αθT
for 25 uniformly spaced values 0 =α1< α 2< . . . < α 25= 1. To measure monotonicity, we
record the maximum increase between adjacent networks ∆ = max( L(αi+1)−L(αi)), and the
percentage of networks that have ∆≤0i.e. the percentage of networks that satisfy monotonic linear
interpolation. To measure convexity, we consider a local convexity measure (the proportion of αi
where the centered difference second derivative approximation is nonnegative) and a global convexity
measure (the proportion of αisuch that L(αi)lies below the line segment between the endpoints, i.e.
L(αi)≤(1−αi)L(0) + αiL(1)).
Results. Table 4 shows the measures of monotonicity and convexity for standard, σ-Asymmetric, and
W-Asymmetric ResNets. Remarkably, every single one of the 300 W-Asymmetric ResNets satisfies
9monotonic linear interpolation and has a trajectory that lies underneath the line segment between
the endpoints. Qualitatively, we can see in Figure 5 that W-Asymmetric ResNets do not have any
clear loss barriers from initialization, nor any loss plateaus that indicate nonconvexity. In contrast, the
majority of standard ResNets have non-monotonic trajectories, and the monotonic trajectories seem to
be more nonconvex. σ-Asymmetric network trajectories are signficantly more convex and monotonic
than standard network trajectories, but there are some non-monotonic or nonconvex trajectories still.
5.5 Other Optimization and Loss Landscape Properties
In Appendix A, we note other interesting differences in optimization and loss landscape properties of
Asymmetric and standard neural networks. These can be summarized as:
1.Even though Asymmetric networks interpolate much better than standard networks, the
parameters of trained Asymmetric networks are often basically the same distance away from
each other in weight space as standard networks.
2.Asymmetric networks do not tend to overfit as much: the difference in train performance
and test performance can be substantially lower than that of standard networks.
3.Asymmetric networks can take longer to train, especially when choosing hyperparameters
that make them more dissimilar to standard networks.
6 Discussion
While many properties of Asymmetric networks are in line with our hypotheses and intuition about
the impact of removing parameter symmetries, there are many unexpected effects and unanswered
questions that are promising to further investigate. For instance, we did not extensively explore
Asymmetric networks in the context of model interpretability, generalization measures in weight
spaces, or optimization improvements, all of which are known to be influenced to some extent by
parameter symmetries. Further studying the properties in Section 5.5, the dependence of behavior on
the choices of Asymmetry-inducing hyperparameters, and other design choices in making networks
asymmetric could also bring more insights into parameter space symmetries.
Also, it is interesting that our σ-Asymmetric networks do not appear to break parameter symmetries
as well as our W-Asymmetric networks. We have run preliminary empirical tests on several variants
ofσ-Asym networks, such as: σ◦F◦σas the nonlinearity, sparsifying F, adding instead of
multiplying the gate, using cosine instead of sigmoid, squaring instead of using sigmoid, and putting
a LayerNorm in the nonlinearity. However, none of these approaches worked well. We believe
that such failures may be because σ-Asymmetry breaks symmetries at the activation / neuron level,
whereas W-Asymmetry breaks symmetries in the larger space of weights (for more evidence, see
Appendix E, where we show that fixing biases at neurons also fails to effectively remove symmetries).
These curiosities provide interesting directions for future work.
All in all, we believe that future theoretical and empirical study of Asymmetric networks could garner
many insights into the role of parameter symmetries in deep learning.
Acknowledgements
We would like to thank Kwangjun Ahn, Benjamin Banks, Nima Dehmamy, Nikos Karalias, Jinwoo
Kim, Marc Law, Hannah Lawrence, Thien Le, Jonathan Lorraine, James Lucas, Behrooz Tahmasebi,
and Logan Weber for discussions at various points of this project. DL is supported by an NSF
Graduate Fellowship. RW is supported in part by NSF award 2134178. HM is the Robert J. Shillman
Fellow, and is supported by the Israel Science Foundation through a personal grant (ISF 264/23) and
an equipment grant (ISF 532/23). This research was supported in part by Office of Naval Research
grant N00014-20-1-2023 (MURI ML-SCOPE), NSF AI Institute TILOS (NSF CCF-2112665), NSF
award 2134108, and the Alexander von Humboldt Foundation.
References
[1]Samuel Ainsworth, Jonathan Hayase, and Siddhartha Srinivasa. Git re-basin: Merging models
modulo permutation symmetries. In The Eleventh International Conference on Learning
Representations , 2023. URL https://openreview.net/forum?id=CQsmMYmlP5T .
10[2]Laurence Aitchison, Adam Yang, and Sebastian W Ober. Deep kernel processes. In International
Conference on Machine Learning , pages 130–140. PMLR, 2021.
[3]Gul Sena Altintas, Gregor Bachmann, Lorenzo Noci, and Thomas Hofmann. Disentangling
linear mode-connectivity. arXiv preprint arXiv:2312.09832 , 2023.
[4]Stephen Ashmore and Michael Gashler. A method for finding similarity between multi-layer
perceptrons by forward bipartite alignment. In 2015 International Joint Conference on Neural
Networks (IJCNN) , pages 1–7. IEEE, 2015.
[5]Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint
arXiv:1607.06450 , 2016.
[6]Vijay Badrinarayanan, Bamdev Mishra, and Roberto Cipolla. Understanding symmetries in
deep networks. arXiv preprint arXiv:1511.01029 , 2015.
[7]Georg Bökman and Fredrik Kahl. Investigating how reLU-networks encode symmetries. In
Thirty-seventh Conference on Neural Information Processing Systems , 2023. URL https:
//openreview.net/forum?id=8lbFwpebeu .
[8]George Cybenko. Approximation by superpositions of a sigmoidal function. Mathematics of
control, signals and systems , 2(4):303–314, 1989.
[9]Yann N Dauphin, Angela Fan, Michael Auli, and David Grangier. Language modeling with
gated convolutional networks. In International conference on machine learning , pages 933–941.
PMLR, 2017.
[10] Terrance DeVries and Graham W Taylor. Improved regularization of convolutional neural
networks with cutout. arXiv preprint arXiv:1708.04552 , 2017.
[11] Laurent Dinh, Razvan Pascanu, Samy Bengio, and Yoshua Bengio. Sharp minima can generalize
for deep nets. In International Conference on Machine Learning , pages 1019–1028. PMLR,
2017.
[12] Gabriel Eilertsen, Daniel Jönsson, Timo Ropinski, Jonas Unger, and Anders Ynnerman.
Classifying the classifier: dissecting the weight space of neural networks. arXiv preprint
arXiv:2002.05688 , 2020.
[13] Rahim Entezari, Hanie Sedghi, Olga Saukh, and Behnam Neyshabur. The role of permutation
invariance in linear mode connectivity of neural networks. In International Conference on
Learning Representations , 2022. URL https://openreview.net/forum?id=dNigytemkL .
[14] Damien Ferbach, Baptiste Goujaud, Gauthier Gidel, and Aymeric Dieuleveut. Proving linear
mode connectivity of neural networks via optimal transport. In International Conference on
Artificial Intelligence and Statistics , pages 3853–3861. PMLR, 2024.
[15] Matthias Fey and Jan Eric Lenssen. Fast graph representation learning with pytorch geometric.
arXiv preprint arXiv:1903.02428 , 2019.
[16] Marc Finzi, Max Welling, and Andrew Gordon Wilson. A practical method for constructing
equivariant multilayer perceptrons for arbitrary matrix groups. In International conference on
machine learning , pages 3318–3328. PMLR, 2021.
[17] Jonathan Frankle. Revisiting "qualitatively characterizing neural network optimization prob-
lems", 2020.
[18] Jonathan Frankle, Gintare Karolina Dziugaite, Daniel Roy, and Michael Carbin. Linear mode
connectivity and the lottery ticket hypothesis. In International Conference on Machine Learning ,
pages 3259–3269. PMLR, 2020.
[19] Cedric Gegout, Bernard Girau, and Fabrice Rossi. A mathematical model for feed-forward
neural networks: theoretical description and parallel applications. PhD thesis, Laboratoire de
l’informatique du parallélisme, 1995.
[20] Charles Godfrey, Davis Brown, Tegan Emerson, and Henry Kvinge. On the symmetries of deep
learning models and their internal representations. Advances in Neural Information Processing
Systems , 35:11893–11905, 2022.
[21] Ian J Goodfellow, Oriol Vinyals, and Andrew M Saxe. Qualitatively characterizing neural
network optimization problems. ICLR , 2015.
[22] William L Hamilton. Graph representation learning . Morgan & Claypool Publishers, 2020.
11[23] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition ,
pages 770–778, 2016.
[24] Robert Hecht-Nielsen. On the algebraic structure of feedforward network weight spaces. In
Advanced Neural Computers , pages 129–135. Elsevier, 1990.
[25] Robert Hecht-Nielsen. Theory of the backpropagation neural network. In Neural networks for
perception , pages 65–93. Elsevier, 1992.
[26] Dan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus). arXiv preprint
arXiv:1606.08415 , 2016.
[27] Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele
Catasta, and Jure Leskovec. Open graph benchmark: Datasets for machine learning on graphs.
Advances in neural information processing systems , 33:22118–22133, 2020.
[28] Moritz Imfeld, Jacopo Graldi, Marco Giordano, Thomas Hofmann, Sotiris Anagnostidis, and
Sidak Pal Singh. Transformer fusion with optimal transport. arXiv preprint arXiv:2310.05719 ,
2023.
[29] Keller Jordan, Hanie Sedghi, Olga Saukh, Rahim Entezari, and Behnam Neyshabur. REPAIR:
REnormalizing permuted activations for interpolation repair. In The Eleventh International
Conference on Learning Representations , 2023. URL https://openreview.net/forum?
id=gU5sJ6ZggcX .
[30] Laurent Valentin Jospin, Hamid Laga, Farid Boussaid, Wray Buntine, and Mohammed Ben-
namoun. Hands-on bayesian neural networks—a tutorial for deep learning users. IEEE
Computational Intelligence Magazine , 17(2):29–48, 2022.
[31] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980 , 2014.
[32] Miltiadis Kofinas, Boris Knyazev, Yan Zhang, Yunlu Chen, Gertjan J Burghouts, Efstratios
Gavves, Cees GM Snoek, and David W Zhang. Graph neural networks for learning equivariant
representations of neural networks. arXiv preprint arXiv:2403.12143 , 2024.
[33] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images,
2009.
[34] Richard Kurle, Ralf Herbrich, Tim Januschowski, Yuyang Bernie Wang, and Jan Gasthaus. On
the detrimental effect of invariances in the likelihood for variational inference. Advances in
Neural Information Processing Systems , 35:4531–4542, 2022.
[35] Lauro Langosco, Neel Alex, William Baker, David John Quarel, Herbie Bradley, and
David Krueger. Towards meta-models for automated interpretability, 2024. URL https:
//openreview.net/forum?id=fM1ETm3ssl .
[36] Olivier Laurent, Emanuel Aldea, and Gianni Franchi. A symmetry-aware exploration of
bayesian neural network posteriors. In The Twelfth International Conference on Learning
Representations , 2024. URL https://openreview.net/forum?id=FOSBQuXgAq .
[37] Guillaume Leclerc, Andrew Ilyas, Logan Engstrom, Sung Min Park, Hadi Salman, and Alek-
sander M ˛ adry. Ffcv: Accelerating training by removing data bottlenecks. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 12011–12020,
2023.
[38] Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning
applied to document recognition. Proceedings of the IEEE , 86(11):2278–2324, 1998.
[39] Derek Lim, Haggai Maron, Marc T. Law, Jonathan Lorraine, and James Lucas. Graph metanet-
works for processing diverse neural architectures. In The Twelfth International Conference on
Learning Representations , 2024. URL https://openreview.net/forum?id=ijK5hyxs0n .
[40] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International
Conference on Learning Representations , 2018.
[41] James Lucas, Juhan Bae, Michael R Zhang, Stanislav Fort, Richard Zemel, and Roger Grosse.
Analyzing monotonic linear interpolation in neural network loss landscapes. arXiv preprint
arXiv:2104.11044 , 2021.
12[42] Haggai Maron, Ethan Fetaya, Nimrod Segol, and Yaron Lipman. On the universality of invariant
networks. In International conference on machine learning , pages 4363–4371. PMLR, 2019.
[43] Edward Milsom, Ben Anson, and Laurence Aitchison. Convolutional deep kernel machines.
InThe Twelfth International Conference on Learning Representations , 2024. URL https:
//openreview.net/forum?id=1oqedRt6Z7 .
[44] Seyed Iman Mirzadeh, Mehrdad Farajtabar, Dilan Gorur, Razvan Pascanu, and Hassan
Ghasemzadeh. Linear mode connectivity in multitask and continual learning. In Interna-
tional Conference on Learning Representations , 2021. URL https://openreview.net/
forum?id=Fmg_fQYUejf .
[45] Mahdi Pakdaman Naeini, Gregory Cooper, and Milos Hauskrecht. Obtaining well calibrated
probabilities using bayesian binning. In Proceedings of the AAAI conference on artificial
intelligence , volume 29, 2015.
[46] Aviv Navon, Aviv Shamsian, Idan Achituve, Ethan Fetaya, Gal Chechik, and Haggai Maron.
Equivariant architectures for learning in deep weight spaces. In International Conference on
Machine Learning , pages 25790–25816. PMLR, 2023.
[47] Aviv Navon, Aviv Shamsian, Ethan Fetaya, Gal Chechik, Nadav Dym, and Haggai Maron.
Equivariant deep weight space alignment. arXiv preprint arXiv:2310.13397 , 2023.
[48] Behnam Neyshabur, Russ R Salakhutdinov, and Nati Srebro. Path-sgd: Path-normalized
optimization in deep neural networks. Advances in neural information processing systems , 28,
2015.
[49] Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro. Norm-based capacity control in neural
networks. In Conference on learning theory , pages 1376–1401. PMLR, 2015.
[50] Theodore Papamarkou, Jacob Hinkle, M Todd Young, and David Womble. Challenges in
markov chain monte carlo for bayesian neural networks. Statistical Science , 37(3):425–442,
2022.
[51] Theodore Papamarkou, Maria Skoularidou, Konstantina Palla, Laurence Aitchison, Julyan
Arbel, David Dunson, Maurizio Filippone, Vincent Fortuin, Philipp Hennig, Aliaksandr Hubin,
et al. Position paper: Bayesian deep learning in the age of large-scale ai. arXiv preprint
arXiv:2402.00809 , 2024.
[52] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan,
Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative
style, high-performance deep learning library. Advances in neural information processing
systems , 32, 2019.
[53] Fidel A Guerrero Peña, Heitor Rapela Medeiros, Thomas Dubail, Masih Aminbeidokhti, Eric
Granger, and Marco Pedersoli. Re-basin via implicit sinkhorn differentiation. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 20237–20246,
2023.
[54] Fabrizio Pittorino, Antonio Ferraro, Gabriele Perugini, Christoph Feinauer, Carlo Baldassi, and
Riccardo Zecchina. Deep networks on toroids: removing symmetries reveals the structure of flat
regions in the landscape geometry. In International Conference on Machine Learning , pages
17759–17781. PMLR, 2022.
[55] Arya A Pourzanjani, Richard M Jiang, and Linda R Petzold. Improving the identifiability of
neural networks for bayesian inference. In NIPS workshop on bayesian deep learning , volume 4,
page 31, 2017.
[56] Xingyu Qu and Samuel Horvath. Rethink model re-basin and the linear mode connectivity.
arXiv preprint arXiv:2402.05966 , 2024.
[57] Prajit Ramachandran, Barret Zoph, and Quoc V . Le. Searching for activation functions, 2017.
[58] Aviv Shamsian, Aviv Navon, David W Zhang, Yan Zhang, Ethan Fetaya, Gal Chechik, and
Haggai Maron. Improved generalization of weight space networks via augmentations. arXiv
preprint arXiv:2402.04081 , 2024.
[59] Sidak Pal Singh and Martin Jaggi. Model fusion via optimal transport. Advances in Neural
Information Processing Systems , 33:22045–22055, 2020.
13[60] George Stoica, Daniel Bolya, Jakob Brandt Bjorner, Pratik Ramesh, Taylor Hearn, and Judy
Hoffman. Zipit! merging models from different tasks without training. In The Twelfth
International Conference on Learning Representations , 2024. URL https://openreview.
net/forum?id=LEYUkvdUhq .
[61] Héctor J Sussmann. Uniqueness of the weights for minimal feedforward nets with a given
input-output map. Neural networks , 5(4):589–593, 1992.
[62] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Re-
thinking the inception architecture for computer vision. In Proceedings of the IEEE conference
on computer vision and pattern recognition , pages 2818–2826, 2016.
[63] Norman Tatro, Pin-Yu Chen, Payel Das, Igor Melnyk, Prasanna Sattigeri, and Rongjie Lai. Op-
timizing mode connectivity via neuron alignment. Advances in Neural Information Processing
Systems , 33:15300–15311, 2020.
[64] Marcin Tomczak, Siddharth Swaroop, and Richard Turner. Efficient low rank gaussian varia-
tional inference for neural networks. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and
H. Lin, editors, Advances in Neural Information Processing Systems , volume 33, pages 4610–
4622. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper_
files/paper/2020/file/310cc7ca5a76a446f85c1a0d641ba96d-Paper.pdf .
[65] Thomas Unterthiner, Daniel Keysers, Sylvain Gelly, Olivier Bousquet, and Ilya Tolstikhin.
Predicting neural network accuracy from weights. arXiv preprint arXiv:2002.11448 , 2020.
[66] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information
processing systems , 30, 2017.
[67] Neha Verma and Maha Elbayad. Merging text transformer models from different initializations.
arXiv preprint arXiv:2403.00986 , 2024.
[68] Tiffany J. Vlaar and Jonathan Frankle. What can linear interpolation of neural network loss
landscapes tell us? ICML , 2022.
[69] Hongyi Wang, Mikhail Yurochkin, Yuekai Sun, Dimitris Papailiopoulos, and Yasaman Khazaeni.
Federated learning with matched averaging. arXiv preprint arXiv:2002.06440 , 2020.
[70] Xiang Wang, Annie N. Wang, Mo Zhou, and Rong Ge. Plateau in monotonic linear interpo-
lation — a ”biased” view of loss landscape for deep networks. In The Eleventh International
Conference on Learning Representations , 2023. URL https://openreview.net/forum?
id=z289SIQOQna .
[71] Jonas Gregor Wiese, Lisa Wimmer, Theodore Papamarkou, Bernd Bischl, Stephan Günnemann,
and David Rügamer. Towards efficient mcmc sampling in bayesian neural networks by exploiting
symmetry. In Joint European Conference on Machine Learning and Knowledge Discovery in
Databases , pages 459–474. Springer, 2023.
[72] Tim Z Xiao, Weiyang Liu, and Robert Bamler. A compact representation for bayesian neural
networks by removing permutation symmetry. arXiv preprint arXiv:2401.00611 , 2023.
[73] Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural
networks? In International Conference on Learning Representations , 2019. URL https:
//openreview.net/forum?id=ryGs6iA5Km .
[74] Adam X Yang, Maxime Robeyns, Edward Milsom, Ben Anson, Nandi Schoots, and Laurence
Aitchison. A theory of representation learning gives a deep generalisation of kernel methods. In
International Conference on Machine Learning , pages 39380–39415. PMLR, 2023.
[75] Chulhee Yun, Srinadh Bhojanapalli, Ankit Singh Rawat, Sashank Reddi, and Sanjiv Kumar.
Are transformers universal approximators of sequence-to-sequence functions? In International
Conference on Learning Representations , 2020. URL https://openreview.net/forum?
id=ByxRM0Ntvr .
[76] David Yunis, Kumar Kshitij Patel, Pedro Henrique Pamplona Savarese, Gal Vardi, Jonathan
Frankle, Matthew Walter, Karen Livescu, and Michael Maire. On convexity and linear mode
connectivity in neural networks. In OPT 2022: Optimization for Machine Learning (NeurIPS
2022 Workshop) , 2022.
14[77] Manzil Zaheer, Satwik Kottur, Siamak Ravanbakhsh, Barnabas Poczos, Russ R Salakhutdinov,
and Alexander J Smola. Deep sets. Advances in neural information processing systems , 30,
2017.
[78] Bo Zhao, Iordan Ganev, Robin Walters, Rose Yu, and Nima Dehmamy. Symmetries, flat minima,
and the conserved quantities of gradient flow. arXiv preprint arXiv:2210.17216 , 2022.
[79] Bo Zhao, Nima Dehmamy, Robin Walters, and Rose Yu. Understanding mode connectivity via
parameter space symmetry. In UniReps: the First Workshop on Unifying Representations in
Neural Models , 2023.
[80] Bo Zhao, Robert M. Gower, Robin Walters, and Rose Yu. Improving convergence and gener-
alization using parameter symmetries. In The Twelfth International Conference on Learning
Representations , 2024. URL https://openreview.net/forum?id=L0r0GphlIL .
[81] Allan Zhou, Kaien Yang, Kaylee Burns, Adriano Cardace, Yiding Jiang, Samuel Sokota, J Zico
Kolter, and Chelsea Finn. Permutation equivariant neural functionals. Advances in Neural
Information Processing Systems , 36, 2023.
[82] Allan Zhou, Kaien Yang, Yiding Jiang, Kaylee Burns, Winnie Xu, Samuel Sokota, J Zico Kolter,
and Chelsea Finn. Neural functional transformers. Advances in Neural Information Processing
Systems , 36, 2023.
[83] Allan Zhou, Chelsea Finn, and James Harrison. Universal neural functionals. arXiv preprint
arXiv:2402.05232 , 2024.
[84] Liu Ziyin. Symmetry leads to structured constraint of learning. arXiv preprint arXiv:2309.16932 ,
2023.
150 ~10000 ~20000 ~30000 ~40000
Num Fixed0.0 0.75 1.5 2.25 3.0Standard Deviation11 10 11 12 11
11 12 15 19 22
11 19 25 31 38
11 24 36 44 53
11 30 46 60 72Number of Epochs until 70%
203040506070Figure 6: Epochs until reaching 70% training accuracy on CIFAR-10 when varying the hyperparame-
ters of W-Asymmetric ResNets; we vary number of fixed entries nfixand standard deviation κof the
fixed entries F. Entries further to the bottom and right are more asymmetric, while the entries further
to the top and left are more like standard networks (the leftmost column are all standard networks).
We see that more-asymmetric networks need more time to train.
A Additional Observations on Asymmetric Networks
There are several other interesting differences in the optimization and loss landscape properties of
Asymmetric and standard neural networks. For one, even though Asymmetric networks generally
interpolate significantly better than standard networks, this cannot be seen by measuring distances
in parameter space. For instance, in GNN experiments following the setup of Section 5.1, pairs
of standard GNNs have a distance per parameter of .000174 on average, whereas W-Asymmetric
GNNs have .000159, which is only slightly lower. However, the average test loss barrier is 1.448 for
standard GNNs while it is only 0.069 for W-Asymmetric GNNs. Likewise, in our datasets of 10,000
standard and W-Asymmetric ResNets, the average distance per parameter between the weights
of trained standard classifiers is .0034, which is actually lower than the distance per parameter of
.0051 for W-Asymmetric ResNets (estimated on 20,000 pairs of networks). Thus, although we
sometimes imagine well-interpolating pairs of networks to lie in the same local basin of parameter
space, W-Asymmetric networks are actually rather far apart in parameter space, but nonetheless
have linear segments of low loss between them.
We also find that Asymmetric networks often do not overfit as much as standard networks. For
instance, in the GNN setup of Section 5.1, standard GNNs have a max training accuracy of 84.6%on
average, with a validation accuracy of 71.6%. On the other hand, σ-Asym GNNs have 70.8%/70.1%
train/validation accuracy, while W-Asym GNNs have 70.7%/70.06% train/validation accuracy. This
difference does not show as much in our datasets of 10,000 standard ResNets and W-Asym ResNets,
possibly because of the substantial regularization (data augmentation, weight decay, and label
smoothing) used for training (standard gets 74.8%/73.8%train/test accuracy while W-Asymmetric
gets64.0%/64.0%).
Further, in Figure 6, we see that training speed is slower for W-Asymmetric ResNets when we
increase the amount of asymmetry (by increasing the number of fixed entries and the standard
deviation of the fixed entries). While standard ResNets take on average 11epochs to reach 70%
training accuracy on CIFAR-10, W-Asymmetric ResNets with the most extreme hyperparameters
take up to 72epochs.
16B Proofs of Theoretical Results
B.1 Graph-based approach
Here, we prove that as long as each mask matrix Min ourW-Asymmetric MLPs with fixed entries set
to zero has unique nonzero rows, then our architecture has no nontrivial neural DAG automorphisms.
In practice, we find that setting the standard deviation κof the fixed entries Fto be positive (and
in fact orders of magnitude larger than the standard deviation that we typically initialize trainable
weights with) is important to achieve properties such as linear mode connectivity that Asymmetric
networks have but standard networks do not have. When κ= 0(i.e. when fixed entries are set to
zero), we can directly work in the framework of Lim et al. [39] that connects parameter symmetries
to computation graph automorphisms. To work towards generalizing our result to κ >0, we would
have to modify the definitions and results of Lim et al. [39]; for instance, we would need to add edges
associated to untrainable parameters in the computation graph, and redefine the concept of neural
DAG automorphisms. We leave such exploration to future work.
Theorem 3. If each mask matrix Mhas unique nonzero rows, then W-Asymmetric MLPs with κ= 0
have no nontrivial neural DAG automorphisms.
Proof. Consider an L-layer W-Asymmetric MLP with fixed entries set to zero. Denote its weights
asWL, . . . ,W1and the corresponding binary masks as ML, . . . , M 1. The forward pass of such a
network on an input xis then
[WL⊙ML]σ(···σ([W1⊙M1]x)···), (5)
for some elementwise nonlinearity σ. The dimension of Wiisdi×di−1. In the framework of Lim
et al. [39], this is a feedforward neural network with a computation graph defined as follows.
The node set is V0×V1×. . .×VL, where Vihasdinodes, and no nodes are shared between different
Vi. If a node vis inVi, then we say that layer( v) =i.V0contains the input nodes and VLcontains
the output nodes. The adjacency matrix can be written as
A=
0
M10
M2
...0
ML0
. (6)
Every block besides the ones containing masks is zero. There are L+ 1×L+ 1blocks, and the (i, j)
block is of size di×dj.
Recall that a neural DAG automorphism is a relabelling of nodes τ:V→Vsuch that τis bijective,
(i, j)∈Eif and only if (τ(i), τ(j))∈E, and every input node and output node is a fixed point of τ.
Now, let τ:V→Vbe a neural DAG automorphism. Further, let Pbe the corresponding permutation
matrix. We will show that τis the identity, i.e. that P=I. By Lemma 1, we know that τpreserves
layer number of nodes, meaning layer( τ(i)) = layer( i). Thus, Pis a block diagonal permutation
matrix:
P=
P0
P1
...
PL
, (7)
where Piisdi×di. Morever, P0=IandPL=Ibecause input nodes and output nodes are fixed
points. Applying this to the adjacency matrix, we see that
τ(A) =PAP⊤=
0
P1M1P⊤
00
...
PLMLP⊤
L−10
. (8)
Since τis a neural DAG automorphism, we have that τ(A) =A. Equating blocks, this means that
P1M1P⊤
0=M1. AsP0=I, we have P1M1=M1. But M1has unique rows, so P1=Ias well.
17For the inductive step, assume Pi=Ifor some i. Then Pi+1Mi+1P⊤
i=Pi+1Mi+1=Mi+1, so
since Mi+1has unique rows, we have that Pi+1=I. As this holds for any iby induction, this means
thatP=I, soτis a trivial neural DAG automorphism and we are done.
Lemma 1. Neural DAG automorphisms preserve layer number in W-Asymmetric MLPs that have
masks with nonzero rows.
Proof. Letτbe a neural DAG automorphism. This means that PAP⊤=A, where Pis the
permutation matrix associated to τ. Then, using PAP⊤=Afor the first equality and the definition
ofPin the second, we have that
Aτ(i),τ(j)= (PAP⊤)τ(i),τ(j)=Ai,j. (9)
We proceed by induction on layer number l. For any input node iwe know that τ(i) =i, so of course
layer( τ(i)) = layer( i).
Now, suppose that layer( τ(i)) = layer( i)for any iin layer l≥1. If node jis in layer l+ 1, then
there is some iin layer lsuch that (i, j)∈Ebecause Ml+1has no nonzero rows. We have that
Aτ(i),τ(j)=Ai,j, soτ(i)is connected to τ(j). As we know that τ(i)is in layer l, we have that τ(j)
is in layer l+ 1.
B.2 Symmetry Breaking via Nonlinearities
Proposition 3. Let the parameter space Θbe all pairs of square invertible matrices θ= (W2,W1)
forW2,W1∈GL(d), and let fθ(x) =W2σ(W1x). Ifσhas no linear equivariances, then
fθ1=fθ2if and only if θ1=θ2. In other words, there are no nontrivial parameter space symmetries.
Proof. Ifθ1=θ2, then clearly fθ1=fθ2. For the other direction, suppose fθ1=fθ2, and denote
θ1= (W2,W1)andθ2= (fW2,fW1). Then for any input z∈Rn, we have
W2σ(W1z) =fW2σ(fW1z) (10)
fW−1
2W2σ(W1z) =σ(fW1z). (11)
Now, choose an arbitrary x∈Rn. We let zin the above equation (11) be W−1
1x, so we have
fW−1
2W2σ(x) =σ(fW1W−1
1x). (12)
This holds for any x, sofW−1
2W2◦σ=σ◦fW1W−1
1, i.e. we have found a linear equivariance of σ.
Since σhas no linear equivariances,
fW−1
2W2=I=fW1W−1
1, (13)
meaning that fW2=W2andfW1=W1, i.e.θ1=θ2, so we are done.
B.2.1 FiGLU nonlinearity proofs (Proposition 2)
Now, we study the properties of our FiGLU nonlinearity σ(x) =η(Fx)⊙x, where ηis the sigmoid
function η(x) =1
1+e−x. For proving Proposition 2, we want to prove that with probability 1 over
samples of F,σhas no permutation or diagonal equivariances.
We say that σhasno permutation equivariances if whenever P2◦σ=σ◦P1for permutation matrices
P1andP2, then P1=P2=I. Likewise, we say that σhasno diagonal equivariances if whenever
B◦σ=σ◦Afor invertible diagonal matrices AandB, then A=B=I.
We will show that these two properties hold for any Fthat has no permutation symmetries and no
zero entries. We say that Fhasno permutation symmetries ifP2FP1=Ffor permutation matrices
P1andP2implies that P1=P2=I. Note that if Fhas distinct entries, then it has no permutation
symmetries. Thus, Fsatisfies both of these conditions with probability 1, since the set of matrices
with nondistinct entries or with at least one zero entry are of Lebesgue measure zero, so they have
zero probability under the Gaussian distribution. We now proceed to show that σhas no permutation
or diagonal equivariances under these conditions on F.
18Proposition 4. IfFis a square matrix with no permutation symmetries, then σ(x) =η(Fx)⊙xhas
no permutation equivariances.
Proof. Suppose σ◦P1=P2◦σfor permutation matrices P1, P2. We will show that P1=P2=I.
For any input x, we have
η(FP1x)⊙P1x=P2
η(Fx)⊙x
(14)
P⊤
2
η(FP1x)⊙P1x
=η(Fx)⊙x (15)
η(P⊤
2FP1x)⊙P⊤
2P1x=η(Fx)⊙x, (16)
where we used permutation equivariance of η, which acts elementwise. Let x=ei, the standard
basis vector that is 1in the ith coordinate and 0elsewhere. If iis not a fixed point of the permutation
P⊤
2P1, then let jbe the index that it is mapped to. Then equation (16) gives that
η(P⊤
2FP1ei)⊙P⊤
2P1ei=η(Fei)⊙ei (17)
η(P⊤
2FP1ei)⊙ej=η(Fei)⊙ei. (18)
In the ith coordinate of this equality of vectors, we see that η(Fei) = 0 , which is impossible, since
ηis the sigmoid function. Thus, icannot be a fixed point of P⊤
2P1, soP⊤
2P1=Iis the identity
permutation. Now, let xbe an arbitrary vector with no zero entries. Equation (16) gives that
η(P⊤
2FP1x)⊙x=η(Fx)⊙x. (19)
Since xis nonzero, we can divide by xiin the ith coordinate of this vector equality for each ito get
that
η(P⊤
2FP1x) =η(Fx). (20)
Asηis bijective,
P⊤
2FP1x=Fx. (21)
Because this holds for all xwith no zero entries (and in particular for a basis of the input space), we
know that
P⊤
2FP1=F (22)
as matrices. But since Fhas no permutation symmetries, we have that P1=P2=I, so we are
done.
Proposition 5. IfFis a square matrix with no zero entries, then σ(x) =η(Fx)⊙xhas no diagonal
equivariances.
Proof. LetA= Diag( α)andB= Diag( β)be invertible diagonal matrices, and suppose that
σ◦A=B◦σ. We will show that A=B=I. For any input x, we have
η(F[α⊙x])⊙(α⊙x) =β⊙
η(Fx)⊙x
. (23)
Letx=cei, where eiis the ith standard basis vector and c̸= 0is any nonzero number. Then
η(Fcαiei)⊙cαiei=β⊙
η(cFei)⊙cei
. (24)
At the ith coordinate of this equality, we have
η(Fcαiei)icαi=βiη(cFei)ic (25)
αi
βi=η(cFei)i
η(αicFei)i(26)
Thus, the right hand side is constant in c. We must have that αi>0, because if not, then increasing c
would increase either the numerator or denominator and decrease the other, hence contradicting the
equality (here we use that Fhas no zero entries, so cFeiis nonzero in every entry). Thus, letting
c→ ∞ , we see thatαi
βi= 1, soαi=βi. Plugging this back into Equation (26), we have
1 =η(cFei)i
η(αicFei)i(27)
η(αicFei)i=η(cFei)i (28)
αic(Fei)i=c(Fei)i (29)
αi= 1, (30)
where in the third line we used the fact that ηis invertible. We have shown that αi=βi= 1for each
i, soA=B=Iand we are done.
19We note that the proofs of these two results about FiGLU are reminiscent of some proof techniques
from Godfrey et al. [20], such as those used in their analysis of GELU nonlinearities.
B.3 Proofs for Universal Approximation
Here, we prove the universal approximation result for our W-Asymmetric MLPs.
Theorem 4. Letηbe any nonpolynomial elementwise nonlinearity with η(x)−η(−x) =x(e.g.
ReLU ,GELU ,swish ), letΩ⊆RDbe a compact domain, and let ftarget :Ω→Rbe a continuous
target function. Fix ε >0andδ >0.
There exists a width n′such that for all n > n′, with probability 1−δ, for a randomly sampled 4-layer
W-Asymmetric MLP fwithηnonlinearity, hidden dimensions 24n→n→24n, and nfix∈o(n1/4)
hardwired entries per neuron, there will exist θ∈Θsuch that the W-Asymmetric MLP f:Rn→R
approximates ftarget toε:
fθ([x; 0])−ftarget(x)< εfor all x∈Ω. (31)
Importantly, we require that the input to fθbe padded with n−Dzeroes, so [x; 0]∈Rn.
B.3.1 Proof sketch
To approximate ftarget toε >0, we will leverage the universal approximation for standard MLPs
with nonlinearity ηto first obtain a standard 2-layer MLP that approximates ftarget to within ε,
meaningftarget(x)−fMLP(x)< εfor all x∈Ω. Then we will exactly represent fMLP using an
Asymmetric Network f.
This will be done by approximating each linear map WoffMLP by two layers of an Asymmetric
network: W′
2◦η◦W′
1=Wfor Asymmetric linear maps W′
2andW′
1. For the sake of exposition,
we will show how to do this first when both W′
2andW′
1have no Asymmetric mask (i.e. fitting a
linear map Wusing a standard two-layer η-MLP), then when only W′
2has an Asymmetric mask, and
finally when both W′
2andW′
1have an Asymmetric mask.
B.3.2 Fitting a Linear Map with a Two-Layer Standard MLP
LetW∈Rn×nbe the target linear map, and let B∈R2n×nandA∈Rn×2nbe parameters of a
two-layer MLP, defined by fA,B(x) =Aη(Bx). We will choose AandBsuch that fA,B(x) =Wx
for all x∈Rn.
Denote the ith row of WbyWi, so that
W=
W0
...
Wn−1
, Wx =
W0·x
...
Wn−1·x
 (32)
We set AandBas follows, where Inis the n×nidentity matrix:
A=In⊗1−1
=
1−1
1−1
......
1−1
B=
W0
−W0
W1
−W1
...
Wn−1
−Wn−1
. (33)
Then we can see that fA,Bexactly computes the linear transformation Wx.
Aη(Bx) =
η(W0·x)−η(−W0·x)
...
η(Wn−1·x)−η(−Wn−1·x)
=
W0·x
...
Wn−1·x
=Wx. (34)
20B.3.3 Fitting a Linear Map with One Asymmetric and One Standard Linear Map
Letnfix>0and let each row of N∈ {0,1}n×6nhavenfixentries equal to 0, selected at random. Let
B∈R6n×nandA∈Rn×6n. Define A′to be an Asymmetric linear map: A′=A⊙N+(1−N)⊙P,
where Nis a randomly sampled binary mask, and Pa randomly sampled Gaussian matrix. We
consider a two-layer network with one Asymmetric and one standard linear map: fA,B(x) =
A′η(Bx). We want fA,B(x) =Wx for all x. For the remainder of this proof, we will assume that N
never has three consecutive entries in a row set to zero; we will later show that this holds with high
probability over the sampling of N.
First, we define Bin a similar way to the purely linear setting, but with additional copies of entries to
allow for error correction of the random noisy entries fixed in A′.
B=
W0
W0
W0
−W0
−W0
−W0
...
Wn−1
Wn−1
Wn−1
−Wn−1
−Wn−1
−Wn−1
. (35)
Recall that each row A′
iofA′hasnfixentries that are randomly hardwired to constants. Ideally, we
would want A′
ito pick out η(Wi·x)−η(−Wi·x) =Wi·x, but because of the hardwired constants,
A′
imight randomly add c∗η(Wj·x). However, since there are three copies of η(Wj·x)inη(Bx), as
long as not all three corresponding entries in A′
iare fixed, one of the un-fixed copies can be changed
such that the coefficients sum to 0. Since by our assumption Nnever has three consecutive entries all
set to 0, the coefficients of Acan be picked such that A′η(Bx) =Wx. For example, a possible A′
matrix would be
A′=
1 0 0 −1 0 0 |1.1.37−1.47 0 0 0 |. . .|0 0 0 0 0 0
.89−.89 0 0 0 0 |.37 .63 0 −1 0 0 |. . .|0 0 0 0 0 0
0 0 0 0 0 0 |0 0 0 0 0 0 |. . .|1 0 0 −1 0 0

Thus we have shown that under the assumption that Nnever has three consecutive en-
tries equal to 0, Acan be picked such that A′η(Bx) = Wx. We will now show that
P(Nnever has three consecutive entries equal to 0 )can be made arbitrarily small by increasing the
width nwhile keeping nfixto be o(n1/3).
The probability there are three consecutive entries in a given row of Nthat are zero is O(n3
fix
n2). By
the union bound, the probability that any row has 3 consecutive hardwired entries is O(n3
fix
n). For any
nfix∈o(n1/3), this tends towards 0. Thus, with probability ≥1−O(n3
fix
n),Acan be picked such
thatA′η(Bx) =Wx.
B.3.4 Fitting a Linear Map with Two Asymmetric Linear Maps
Once again let nfix>0, and let each row of Mhavenfixentries equal to 0, selected at random. Let
B∈R24n×nandA∈Rn×24n. Further, define Asymmetric maps
A′=A⊙N+ (1−N)⊙P, B′=B⊙M+ (1−M)⊙Q, (36)
where N,Mare randomly sampled masks, and P,Qare normal matrices. Then we let fA,B(x) =
A′η(B′(x)), and we once again desire choices of parameters AandBsuch that fA,B(x) =Wx.
21Constructing B
Consider the randomly drawn mask M∈ {0,1}24n×n, and denote the ith row by Mi.
M=
M0
M1
...
M24n−1
(37)
where Mi∈ {0,1}n. We partition M’s rows into nblocks of 24rows. β1={M0. . . M 23}, . . . β i=
{M24i. . . M 24i+23}. Now, consider β1, the first 24rows of M.
Definition B.1. We say two rows Mj, Mkareintersecting if there is some column index αsuch
thatMj,α=Mk,α= 0. That is, two rows of are intersecting if they share a 0 at the same index.
Note that for any two given rows of M, the probability that they share a 0in the same location is
≤n2
fix
n.
We assume that βicontains no more than one pair of intersecting rows; later, we show this to hold
with high probability. Then, every βican be broken into two disjoint sets of 12 rows, βi,0andβi,1,
such that neither set of 12 contains a single pair of intersecting rows. Intuitively, this means that each
row in Bcorresponding to βi,0will have unique fixed indices.
Our goal will be for the rows in βito mimic the row Wi. We will show how to do this for each i. Fix
an arbitrary index i∈ {0, . . . , n −1}.
Without loss of generality, assume βi,0andβi,1are continguous, so βi,0=M24i:24i+11andβi,1=
M24i+11:24i+23. By our assumption, for j, k∈βi,0(i.e. j, k∈ {0, . . . , 11}), the mask rows
M24i+jandM24i+kare never 0in the same two column indices. Similarly, for j, k∈βi,1(i.e.
j, k∈ {12, . . . , 23}), the mask rows M24i+jandM24i+kare never 0in the same two column
indices.
Next, we define ci,jas the difference between B′andBin the (24i+j)th row:
ci,j=B′
24i+j−B24i+j. (38)
In particular, we have that
ci,j=−B24i+j⊙(1−M24i+j) + (1 −M24i+j)⊙Q24i+j. (39)
Lemma 2. For any indices j̸=ksuch that j, k∈βi,0orj, k∈βi,1, we have that
ci,j⊙M24i+k=ci,j. (40)
Proof. By the definition of ci,j, we know that ci,jis only nonzero at indices where M24i+jis equal
to zero. Since j, kare either both in βi,0orβi,1, we know that M24i+kcannot also be zero at
indices where M24i+jis zero. Thus, M24i+kis equal to 1at every index where ci,jis nonzero, so
ci,j⊙M24i+k=ci,jas desired.
Next, we construct B, by constructing this block of 24 rows. Let [ci,0,ci,1,ci,2,ci,3]be continguous
sums of length-3 segments of ci,::
ci,0=ci,0+ci,1+ci,2 (41)
ci,1=ci,3+ci,4+ci,5 (42)
ci,2=ci,6+ci,7+ci,8 (43)
ci,3=ci,9+ci,10+ci,11 (44)
We assign the first 12 rows of Bas follows.
(0≤j <3)→B24i+j=Wi+ci,0−ci,1+ci,2−ci,3−cij (45)
(3≤j <6)→B24i+j=−Wi−ci,0+ci,1−ci,2+ci,3−cij (46)
(6≤j <9)→B24i+j= +ci,0−ci,1+ci,2−ci,3−cij (47)
(9≤j <12)→B24i+j=−ci,0+ci,1−ci,2+ci,3−cij (48)
22Defining c=ci,0−ci,1+ci,2−ci,3, we have the nice property:
(0≤j <3)→B′
24i+j=Wi+c (49)
(3≤j <6)→B′
24i+j=−Wi−c (50)
(6≤j <9)→B′
24i+j= +c (51)
(9≤j <12)→B′
24i+j=−c (52)
By the construction above, B′
24i=−B′
24i+3, and B′
24i+6=−B′
24i+9. This means that
η(B′
24i·x)−η(B′
24i+3·x) =B′
24i·x= (Wi+c)·x (53)
and likewise that
η(B′
24i+6·x)−η(B′
24i+9·x) =c·x (54)
So that a simple linear map gives our desired output:
η(B′
24i·x)−η(B′
24i+3·x)−[η(B′
24i+6·x)−η(B′
24i+9·x)] = ( Wi+c−c)·x (55)
=Wi·x. (56)
In the next part, we will construct A′to compute this linear map, which will follow the method of
Appendix B.3.3 (because A′has certain fixed entries).
What remains is to define the rows of Bcorresponding to βi,1in an error correctible manner. This
can be done easily by defining
d=23X
j=12cij (57)
and then defining
(12≤j <24)→B24i+j=d−cij (58)
By similar reasoning to before, this means that
(12≤j <24)→B′
24i+j=d. (59)
Recall that we constructed B′under the assumption that no βihas at most one pair of intersecting
rows. We now show that the βieach have at most one pair of intersecting rows with high probability.
Within the 24rows of any given βi, the probability that more than one pair of rows are intersecting is
≤Cn4
fix
n2for some constant C. So, by the union bound, the probability over Mthat any of the βihave
more than one pair of intersecting rows is ≤Cn4
fix
n. Thus, we can construct Bin this manner with
probability ≥1−Cn4
fix
n. For sufficiently large nandnfix∈o(n1/4), this probability approaches 1.
23Construction of A
With our above construction, each block of the 24 rows in βiofη(B′x)is of the form

η((Wi+c)·x)
η((Wi+c)·x)
η((Wi+c)·x)
η((−Wi−c)·x)
η((−Wi−c)·x)
η((−Wi−c)·x)
η(c·x)
η(c·x)
η(c·x)
η(−c·x)
η(−c·x)
η(−c·x)
η(d·x)
η(d·x)
η(d·x)
η(d·x)
η(d·x)
η(d·x)
η(d·x)
η(d·x)
η(d·x)
η(d·x)
η(d·x)
η(d·x)
(60)
Importantly, each row here is nwide. Recall that A′∈Rn×24n.Denote the ith row of A′by
A′
i∈R24nwithA′
i∈R24n. IfA′had 0 hardwired entries, then setting Ai= 124i− 124i+3−
( 124i+6− 124i+9)would give Aiη(B′·x) =Wi·x, by the same argument as in Appendix B.3.2.
Unfortunately this is not the case, so we have to use the construction in Appendix B.3.3. Recall, that
A′hasnfixfixed entries in each row. This means that Nihasnfixentries equal to 0. Since every entry
ofB′xhas three copies, as long as Nidoes not have three elements set to 0in a row, A′
ican be made
equivalent to Ai= 124i− 124i+3−( 124i+6− 124i+9). This is because, as in Appendix B.3.3, if at
most 2elements out of any 3three copies are hardwired, then the third can be changed arbitrarily to
offset the hardwiring.
Further, just as in Appendix B.3.3, the probability that a given row of A′has three items hardwired
in a row is O(n3
fix
n2). Thus, by the union bound, the probability that some row of A′has three
items hardwired in a row is O(n3
fix
n). So, with large enough width n,Acan be chosen such that
A′η(B′x) =Wx.
Similarly, any linear map in R˜n×nfor˜n < n can also be fit using this method.
Conclusion
We have shown that a W-Asymmetric MLP with hidden dimension 24ncan exactly fit an n×n
linear map with high probability over the choice of Asymmetric masks M. It is known by [ 8] that for
any continuous function ftarget :Ω⊆RD→Rand any ε >0, there exists a width k′such that
2-layer MLPs of width k′can approximate fto within ε.
Letkbe sufficiently big so that the probability that the masks do not satisfy the conditions of
Appendix B.3.4 is less than δ. Such a kexists as long as nfix∈o(k1
4). Letm≥max( k, k′, D).
Importantly, if a 2-layer MLP of width k′can approximate ftarget to within ε, a 2-layer MLP of width
mwithm≥k′can also approximate fto within ε. LetfMLP be a width mMLP that approximates
ftarget to within ε.
24We now pad the input xtoftarget , with m−Dzeros. This allows us to define a new function
f0
target :Rm→Rbyf0
target([x; 0]) = f(x). Clearly f0
target can also be approximated by a width m
MLP.
Letf0
MLP denote the width mMLP that approximates f0to within ε. Now, f0
MLP has dimensions
m→m→1, with corresponding linear maps W1∈Rm×m, W 2∈R1×m. Each of these maps
can be exactly fit using a 2-layer W-Asymmetric MLP, since their corresponding matrices have at
least as many columns as rows. Concatenating these two exact fits yields an asymmetric MLP whose
output exactly matches f0
MLP and thus approximates f0to within ε.
Thus, setting n′=m, there exists a width n′such that for all n > n′, with probability 1−δ,
for a randomly sampled 4-layer W-Asymmetric MLP fwithηnonlinearity, hidden dimensions
24n→n→24n, and nfix∈o(n1/4)hardwired entries per neuron, there will exist θ∈Θsuch that
theW-Asymmetric MLP f:Rn→Rapproximates ftarget toε.
On parameter symmetries of the 4-layer W-Asymmetric Network
As an aside, this procedure of mapping 2-layer standard MLPs to 4-layer W-Asymmetric MLPs
implies that these 4-layer W-Asymmetric MLPs have at least as many symmetries as 2 layer standard
MLPs. To fix this, we may want to consider a nonlinearity ηsuch that η(x)−η(−x)̸=x.
C Limitations
Although our W-Asymmetric and σ-Asymmetric networks are motivated by removing parameter
space symmetries, their distinct empirical behavior may be caused by other factors besides just
parameter space symmetries. For instance, the fixed entries Ffor the W-Asymmetric approach are
taken to be much larger than the standard initialization of linear maps, which could cause several
changes to optimization and loss landscapes besides just parameter symmetry breaking.
Also, our theoretical results could be strengthened by future work in several ways. For instance, for
theσ-Asymmetric approach, Proposition 1 only gives a guarantee of no parameter symmetries in the
two-layer network case with square invertible weights. Future work could also give tighter analysis
of the required width and depth for universal approximation using our W-Asymmetric architecture.
D Broader Impacts
This work does not focus on any particular application area. Instead, we study fundamental phenom-
ena and theory of deep learning in general. Our work has potential to improve known deficits of
neural networks: by making neural network loss landscapes more similar to convex landscapes, we
can improve our understanding of them, and by improving Bayesian neural networks we advance
one paradigm for bettering uncertainty quantification in neural networks. However, unlike standard
neural networks, which have millions of papers studying them, we have only scratched the surface
of Asymmetric networks. Important properties such as generalization, robustness to distribution
shifts, and adversarial robustness have not been extensively studied for Asymmetric networks, and
the interaction of parameter symmetries with these properties is not clear. Future research should
further explore these important properties.
E Experimental Ablations
Here, we present experiments with various ablations. These were in-part suggested by anonymous
reviewers for the NeurIPS 2024 conference (thanks!).
E.1 Matching Learnable Parameters
For the experiments in the main text, our W-Asym networks often had less learnable parameters
than the standard networks that they were compared against (since we take them to have the same
architecture, but the W-Asym approach fixes certain learnable parameters to constants). See Table 5
for the learnable parameter counts.
25Table 5: Number of learnable parameters of Standard / σ-Asym and W-Asym nets for our experi-
ments.
Experiment / Architecture Standard / σ-Asym W-Asym
5.1 MLP 935,434 834,570
5.1 ResNet 1x 272,474 230,024
5.1 ResNet 8x 17,289,866 16,273,946
5.1 GNN 176,424 171,576
5.2 MLP-8 3,242,146 3,324,466
5.2 MLP-16 5,960,242 5,796,002
5.2 ResNet-20 1x 1,356,098 1,143,858
5.2 ResNet-20 2x 5,410,386 5,044,756
5.2 ResNet-110 1x 8,620,418 7,371,378
5.2 ResNet-110 2x 34,512,276 32,014,996
5.2 ResNet-20 2x 5,410,386 5,044,756
5.3 ResNet 78,042 60,634
5.4 MLI ResNet 78,042 60,634
Table 6: Metanetwork performance, including results with smaller standard networks (Smaller
ResNet) at same number of parameters at W-Asym. There is no substantial difference — W-Asym
ResNets are still substantially easier to predict performance of.
ResNet Smaller ResNet W-Asym ResNet
R2τ R2τ R2τ
MLP .330±.04 .389±.03 .348±.07 .400±.02 .594±.12 .864±.01
DMC .950±.01 .787±.02 .943±.01 .779±.01 .967±.01 .911±.01
DeepSets .855±.01 .617±.03 .849±.01 .627±.01 .936±.00 .858±.00
StatNN .976±.00 .866±.00 .976±.00 .869±.00 .978±.00 .935±.01
In this section, we control for this, by decreasing the width of the standard networks so that they have
the same number of parameters as the W-Asymmetric networks they are compared against. Results
are shown in Tables 6 and 7. We see that there is little to no change in metanetwork performance or
Bayesian neural network performance for these smaller standard networks, so our results from the
main text all still hold.
E.2 Changing Number of Warmup Steps
Altintas et al. [3]showed that amount of learning rate warmup can affect the extent of linear mode
connectivity between training runs. In Table 8, we see that varying the number of warmup epochs
between 1 and 20 does not change the qualitative results much on test loss barrier for our linear mode
connectivity experiments.
E.3 Failure to Break Symmetries by Fixing Biases
Intuitively, one way to break permutation symmetries of an MLP is to order the hidden neurons in
some way. One possible way to do this is to fix the biases (so that they are untrained). As shown
in Table 9, a basic attempt at this fails, and has a much larger test barrier than our W-Asym and
σ-Asym networks.
F Experimental Details
F.1 Linear Mode Connectivity Experimental Details
F.1.1 Image Classifier Interpolation
For the image classification experiments, we use two types of models.
26Table 7: Bayesian NN test accuracy after 25 epochs. Decreasing standard ResNet20 parameters to
match that of W-Asym ResNet20 does not substantially change performance.
Base Network Test Accuracy
W-Asym ResNet20 49.3±0.4
Standard ResNet20 46.8±0.9
Smaller Standard ResNet20 46.5±1.1
Table 8: Test loss barrier when changing warmup steps. Results are very similar when lowering
number of warmup epochs ( W-Asym interpolates significantly better than Git-ReBasin). Adam
optimizer with learning rate 1e-2 (ResNet20) and 1e-3 (GNN) is used.
ResNet20 (ReBasin) W-Asym ResNet20 GNN (ReBasin) W-Asym GNN
1 Epoch Warmup 4.2±.80 .673±.29 .249±.04 .075±.04
20 Epoch Warmup 2.0±.21 .934±.72 .292±.04 .074±.02
1.ResNet We train ResNet20s with LayerNorm of width 64and8·64. We use a batch size of
128 and a learning rate that warms up from .0001 to.01over20epochs. In the width 8×
multiplier case we train for 50 epochs, and in the width 1×multiplier case we train for 100.
Forσ-Asymmetric ResNets, we warm up to a learning rate of .001instead of .01due to
training instability.
2.MLP We train MLPs with 4 layers, LayerNorm, and width 512. For MNIST we tuned the
hyperparameters (epochs, learning rate, weight decay) of both the Asymmetric and Standard
models to minimize loss barrier. We use a batch size of 64.
For MNIST we use no data augmentation, and for CIFAR-10 we use random cropping and horizontal
flipping. For the Git-ReBasin tests, we use the weight matching algorithm from [ 1]. For MLPs
on MNIST, we used the Asymmetry hyperparameters in Table 10. Table 11 gives the Asymmetric
hyperparameters for ResNet20 on CIFAR-10, and Table 12 lists the same for ResNet20 with 8x larger
width.
F.1.2 Graph Neural Network Interpolation
For the GNN experiments, we use a GNN architecture similar to GIN [ 73] with mean aggregation.
The base GNN has three message passing layers and a hidden dimension of 256, which gives 176,424
trainable parameters. The dataset is ogbn-arXiv [ 27], which is a citation network of computer science
arXiv papers with 169,343 nodes and 1,166,243 edges. The task is transductive node classification,
where the label of each paper node is the primary subject area of the paper.
As is common in transductive node classification on modestly sized graphs, we train each network
with full-batch gradient on the whole graph. Thus, the randomness in training is purely from the
initialization — there is no noise from minibatch selection in SGD. We use the Adam optimizer [ 31]
with a peak learning rate of .001. The learning rate is linearly warmed up for 25 epochs to the peak,
and then is held constant. Each network is trained for 500 epochs.
For the Git-ReBasin alignment, we implement the activation matching approach. For the σ-
Asymmetric GNN, we take σto be FiGLU, in which we randomly initialize each fixed matrix
Fas a standard normal matrix with standard deviation .01/√
dwhere dis the number of hidden
channels; we found that having small standard deviation helped with training and interpolation. For
theW-Asymmetric GNN, we fix 6 constants in each row of each linear map, and randomly initialize
these constants from a normal distribution with standard deviation .5.
F.2 Bayesian Neural Network Experimental Details
For training Bayesian neural networks, we use the variational inference approach of Tomczak et al.
[64], which fits an approximate posterior that is Gaussian with a diagonal plus rank-4 covariance
matrix structure. For the W-Asym ResNet tests, we train ResNet20s with the same Asymmetric
hyperparameters as in Table 11, though with κ=.5. For the CIFAR-100 experiments, we use a
27Table 9: Loss barrier when fixing biases to attempt to break symmetries for standard ResNet20 on
CIFAR-10. Biases are between [−k, k]. Barriers are much larger than for W-Asymmetric Nets
(.934±.72) and σ-Asym Nets ( 2.521±.46).
k Loss Barrier
15.81±3.67
33.76±0.38
94.62±1.03
27 10.6±4.29
Table 10: W-Asymmetric network hyperparameters for depth 4 MLPs. nfixrefers to the number of
weights we randomly fix per neuron. κrefers to the standard deviation of the normal distribution that
the fixed entries Fare drawn from.
Layer nfix κ
Linear-1 64 1
Linear-2 64 1
Linear-3 641
2
Linear-4 2561
4
Table 11: W-Asymmetric network hyperparameters for ResNet20s with width multiplier 1. nfix
refers to the number of weights we randomly fix per output channel (for convolutional layers) or
neuron (for linear layers). κrefers to the standard deviation of the normal distribution that the fixed
entries Fare drawn from.
Block nfixκ
First Conv 12 2
Block 1 - Conv 36 2
Block 1 - Skip 4 2
Block 2 - Conv 54 2
Block 2 - Skip 6 2
Block 3 - Conv 72 2
Block 3 - Skip 8 2
Linear 8 2
Table 12: W-Asymmetric network hyperparameters for ResNet20s with width multiplier 8 on
CIFAR-10. We use 3 times more fixed entries per output channel or neuron than for Table 11.
Block nfixκ
First Conv 27 2
Block 1 - Conv 108 2
Block 1 - Skip 12 2
Block 2 - Conv 162 2
Block 2 - Skip 18 2
Block 3 - Conv 216 2
Block 3 - Skip 24 2
Linear 24 2
standard linear layer instead of hardwiring weights for the last fully-connected linear layer. On
CIFAR-100 we also use a width multiplier of 2 for our ResNets. For the ResNet experiments, we use
a learning rate of .001. We train with a batch size of 250 for 50 epochs.
For the MLP experiments, we use κ=.5,8hardwired entries per neuron, and a learning rate of
.0005. A batch size of 250 is used for 50 epochs again.
We use standard data augmentation (horizontal flips and random crops) on CIFAR-10 and CIFAR-100,
and no data augmentations for MNIST. All training is done with the Adam optimizer [31].
28F.3 Metanetwork Experimental Details
F.3.1 Dataset Details
We trained two datasets of image classifiers on CIFAR-10: one consisting of 10,000 small ResNet-like
convolutional neural networks, and one consisting of 10,000 networks with a similar architecture,
that use our graph-based approach to removing parameter symmetries. For fast training of many
image classifiers, we use the FFCV package [ 37]. In particular, we use their CIFAR-10 sample script
https://github.com/libffcv/ffcv/tree/main/examples/cifar , which includes data aug-
mentation (random horizontal flips, random translations, and Cutout [ 10]), label smoothing [ 62], and
a linear learning rate warmup and decay. In total, training all 20,000 classifiers takes just under 400
GPU hours (about 2 GPU-weeks) on NVIDIA RTX 2080 Ti GPUs.
See Table 13 for the hyperparameters and ranges that we varied across the networks in our datasets.
In each dataset, the trained networks all have the same architecture.
Each ResNet has 78,042 trainable parameters, and each W-Asym ResNet has 60,634 trainable
parameters. Both have the same architecture, except the W-Asym ResNet has certain filters that are
fixed to constants to break the parameter symmetries. The ResNets each have 8convolution layers,
LayerNorm [ 5], and a final fully-connected linear classification layer after average pooling across
spatial dimensions.
Table 13: Hyperparameters and distributions we sampled from for the datasets of image classifiers
that we trained on CIFAR-10. Unif( a, b)is the uniform distribution over [a, b], and RandInt( a, b)is
the uniform distribution over integers in [a, b](inclusive of endpoints).
Hyperparameter Distribution
Learning rate .5·10−Unif(0 ,2)
Weight decay 10−Unif(1 ,5)
Label smoothing Unif(0 , .2)
Epochs RandInt(10 ,40)
F.3.2 Metanetwork Details
Table 14: Learning rate and number of parameters for each type of metanetwork trained in Table 3.
ResNet W-Asym ResNet
LR # Params LR # Params
MLP 10−44,994,945 10−43,880,833
DMC [12] 10−3105,357 5 ·10−3105,357
DeepSets [77] 10−28,897 5 ·10−38,897
StatNN [65] 10−3119,297 10−2119,297
We trained several types of metanetworks for our experiments. All of these metanetworks are trained
for 50 epochs using the AdamW optimizer [40]. For each metanetwork, on each dataset, we choose
the learning rate in {10−5,10−4,5·10−4,10−3,5·10−3,10−2}that gives the best validation R2
performance on one training run. Then we run train each type of metanetwork 5 times on each dataset,
and report the mean and standard deviation for each metric in Table 3.
F.4 Monotonic Linear Interpolation Experimental Details
For the monotonic linear interpolation experiments, we used the same setup as in the training of the
datasets of CIFAR-10 image classifiers in Section 5.3. For each architecture, we sample 300 sets
of hyperparameters from the distributions in Table 13, and train one network for each set of these
sampled hyperparameters. When evaluating training loss, we include the labeling smoothing term.
For the σ-Asymmetric networks, we initialize the FiGLU Fwith a standard deviation of 1/√
d,
where dis the number of channels in the layer. Note that this is considerably larger than the standard
29deviation of .01/√
dused in the GNN experiments of Section 5.1; we found this setting to train better
(note that this initialization is in line with standard initializations of trainable parameters). Further,
for the σ-Asymmetric networks, 24out of the 300networks diverged during training (giving NaNs ),
so we exclude them from the computation of statistics in Table 4. From manual inspection, this
divergence seems to happen when the learning rate is high (greater than .1). In contrast, none of the
standard or W-Asymmetric networks diverged.
F.5 Miscellaneous Experimental Details
The datasets we use are MNIST [ 38], CIFAR-10 [ 33], CIFAR-100 [ 33], and ogbn-arXiv [ 27], which
are all widely used in machine learning research. The first three appear to not have licenses and are
open to use, while the last dataset is from the Open Graph Benchmark, which has an MIT License in
the Github repository.
We use software packages including PyTorch [ 52] (for all neural network experiments), FFCV [ 37]
(for building our dataset in Section 5.3), and PyTorch Geometric [15] (for GNN experiments).
We ran our experiments on several types of NVIDIA GPUs and compute systems, including 2080 Ti,
3090 Ti, 4090 Ti, and V100 GPUs. Every training run was conducted on at most one GPU.
30NeurIPS Paper Checklist
1.Claims
Question: Do the main claims made in the abstract and introduction accurately reflect the
paper’s contributions and scope?
Answer: [Yes]
Justification: In Sections 4 and 5, we include theoretical proofs, a description of the methods
and experimental setup, and experimental results to support the claims.
Guidelines:
•The answer NA means that the abstract and introduction do not include the claims
made in the paper.
•The abstract and/or introduction should clearly state the claims made, including the
contributions made in the paper and important assumptions and limitations. A No or
NA answer to this question will not be perceived well by the reviewers.
•The claims made should match theoretical and experimental results, and reflect how
much the results can be expected to generalize to other settings.
•It is fine to include aspirational goals as motivation as long as it is clear that these goals
are not attained by the paper.
2.Limitations
Question: Does the paper discuss the limitations of the work performed by the authors?
Answer: [Yes]
Justification: In Appendix C, we detail empirical and theoretical limitations of our work.
Guidelines:
•The answer NA means that the paper has no limitation while the answer No means that
the paper has limitations, but those are not discussed in the paper.
• The authors are encouraged to create a separate "Limitations" section in their paper.
•The paper should point out any strong assumptions and how robust the results are to
violations of these assumptions (e.g., independence assumptions, noiseless settings,
model well-specification, asymptotic approximations only holding locally). The authors
should reflect on how these assumptions might be violated in practice and what the
implications would be.
•The authors should reflect on the scope of the claims made, e.g., if the approach was
only tested on a few datasets or with a few runs. In general, empirical results often
depend on implicit assumptions, which should be articulated.
•The authors should reflect on the factors that influence the performance of the approach.
For example, a facial recognition algorithm may perform poorly when image resolution
is low or images are taken in low lighting. Or a speech-to-text system might not be
used reliably to provide closed captions for online lectures because it fails to handle
technical jargon.
•The authors should discuss the computational efficiency of the proposed algorithms
and how they scale with dataset size.
•If applicable, the authors should discuss possible limitations of their approach to
address problems of privacy and fairness.
•While the authors might fear that complete honesty about limitations might be used by
reviewers as grounds for rejection, a worse outcome might be that reviewers discover
limitations that aren’t acknowledged in the paper. The authors should use their best
judgment and recognize that individual actions in favor of transparency play an impor-
tant role in developing norms that preserve the integrity of the community. Reviewers
will be specifically instructed to not penalize honesty concerning limitations.
3.Theory Assumptions and Proofs
Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?
Answer: [Yes]
31Justification: We give the full set of assumptions and proofs for each result in Appendix B.
Guidelines:
• The answer NA means that the paper does not include theoretical results.
•All the theorems, formulas, and proofs in the paper should be numbered and cross-
referenced.
•All assumptions should be clearly stated or referenced in the statement of any theorems.
•The proofs can either appear in the main paper or the supplemental material, but if
they appear in the supplemental material, the authors are encouraged to provide a short
proof sketch to provide intuition.
•Inversely, any informal proof provided in the core of the paper should be complemented
by formal proofs provided in appendix or supplemental material.
• Theorems and Lemmas that the proof relies upon should be properly referenced.
4.Experimental Result Reproducibility
Question: Does the paper fully disclose all the information needed to reproduce the main ex-
perimental results of the paper to the extent that it affects the main claims and/or conclusions
of the paper (regardless of whether the code and data are provided or not)?
Answer: [Yes]
Justification: We give high-level experimental setup information in the main paper (Sec-
tion 5), and more detailed information about experiments in the Appendix F.
Guidelines:
• The answer NA means that the paper does not include experiments.
•If the paper includes experiments, a No answer to this question will not be perceived
well by the reviewers: Making the paper reproducible is important, regardless of
whether the code and data are provided or not.
•If the contribution is a dataset and/or model, the authors should describe the steps taken
to make their results reproducible or verifiable.
•Depending on the contribution, reproducibility can be accomplished in various ways.
For example, if the contribution is a novel architecture, describing the architecture fully
might suffice, or if the contribution is a specific model and empirical evaluation, it may
be necessary to either make it possible for others to replicate the model with the same
dataset, or provide access to the model. In general. releasing code and data is often
one good way to accomplish this, but reproducibility can also be provided via detailed
instructions for how to replicate the results, access to a hosted model (e.g., in the case
of a large language model), releasing of a model checkpoint, or other means that are
appropriate to the research performed.
•While NeurIPS does not require releasing code, the conference does require all submis-
sions to provide some reasonable avenue for reproducibility, which may depend on the
nature of the contribution. For example
(a)If the contribution is primarily a new algorithm, the paper should make it clear how
to reproduce that algorithm.
(b)If the contribution is primarily a new model architecture, the paper should describe
the architecture clearly and fully.
(c)If the contribution is a new model (e.g., a large language model), then there should
either be a way to access this model for reproducing the results or a way to reproduce
the model (e.g., with an open-source dataset or instructions for how to construct
the dataset).
(d)We recognize that reproducibility may be tricky in some cases, in which case
authors are welcome to describe the particular way they provide for reproducibility.
In the case of closed-source models, it may be that access to the model is limited in
some way (e.g., to registered users), but it should be possible for other researchers
to have some path to reproducing or verifying the results.
5.Open access to data and code
Question: Does the paper provide open access to the data and code, with sufficient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?
32Answer: [Yes]
Justification: We have open sourced the code to reproduce our experiments in https:
//github.com/cptq/asymmetric-networks .
Guidelines:
• The answer NA means that paper does not include experiments requiring code.
•Please see the NeurIPS code and data submission guidelines ( https://nips.cc/
public/guides/CodeSubmissionPolicy ) for more details.
•While we encourage the release of code and data, we understand that this might not be
possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
including code, unless this is central to the contribution (e.g., for a new open-source
benchmark).
•The instructions should contain the exact command and environment needed to run to
reproduce the results. See the NeurIPS code and data submission guidelines ( https:
//nips.cc/public/guides/CodeSubmissionPolicy ) for more details.
•The authors should provide instructions on data access and preparation, including how
to access the raw data, preprocessed data, intermediate data, and generated data, etc.
•The authors should provide scripts to reproduce all experimental results for the new
proposed method and baselines. If only a subset of experiments are reproducible, they
should state which ones are omitted from the script and why.
•At submission time, to preserve anonymity, the authors should release anonymized
versions (if applicable).
•Providing as much information as possible in supplemental material (appended to the
paper) is recommended, but including URLs to data and code is permitted.
6.Experimental Setting/Details
Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?
Answer: [Yes]
Justification: We describe high-level experimental setup in Section 5, and fine-grained
experimental details in Appendix F.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The experimental setting should be presented in the core of the paper to a level of detail
that is necessary to appreciate the results and make sense of them.
•The full details can be provided either with the code, in appendix, or as supplemental
material.
7.Experiment Statistical Significance
Question: Does the paper report error bars suitably and correctly defined or other appropriate
information about the statistical significance of the experiments?
Answer: [Yes]
Justification: We generally do several runs of experiments, and report error bars that measure
the variance in the results. We also conduct statistical significance tests in Table 4.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The authors should answer "Yes" if the results are accompanied by error bars, confi-
dence intervals, or statistical significance tests, at least for the experiments that support
the main claims of the paper.
•The factors of variability that the error bars are capturing should be clearly stated (for
example, train/test split, initialization, random drawing of some parameter, or overall
run with given experimental conditions).
•The method for calculating the error bars should be explained (closed form formula,
call to a library function, bootstrap, etc.)
33• The assumptions made should be given (e.g., Normally distributed errors).
•It should be clear whether the error bar is the standard deviation or the standard error
of the mean.
•It is OK to report 1-sigma error bars, but one should state it. The authors should
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
of Normality of errors is not verified.
•For asymmetric distributions, the authors should be careful not to show in tables or
figures symmetric error bars that would yield results that are out of range (e.g. negative
error rates).
•If error bars are reported in tables or plots, The authors should explain in the text how
they were calculated and reference the corresponding figures or tables in the text.
8.Experiments Compute Resources
Question: For each experiment, does the paper provide sufficient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?
Answer: [No]
Justification: While we are not fully comprehensive, we include some details on compute
in Appendix F.5, and estimate the compute required for generating our datasets of image
classifiers in Appendix F.3. All other experiments follow popular training paradigms and
are small in scale.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The paper should indicate the type of compute workers CPU or GPU, internal cluster,
or cloud provider, including relevant memory and storage.
•The paper should provide the amount of compute required for each of the individual
experimental runs as well as estimate the total compute.
•The paper should disclose whether the full research project required more compute
than the experiments reported in the paper (e.g., preliminary or failed experiments that
didn’t make it into the paper).
9.Code Of Ethics
Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines ?
Answer: [Yes]
Justification: We have read and complied with the NeurIPS Code of Ethics.
Guidelines:
•The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
•If the authors answer No, they should explain the special circumstances that require a
deviation from the Code of Ethics.
•The authors should make sure to preserve anonymity (e.g., if there is a special consid-
eration due to laws or regulations in their jurisdiction).
10.Broader Impacts
Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?
Answer: [Yes]
Justification: Yes, we discuss both potential positive and negative societal impacts in
Appendix D.
Guidelines:
• The answer NA means that there is no societal impact of the work performed.
•If the authors answer NA or No, they should explain why their work has no societal
impact or why the paper does not address societal impact.
34•Examples of negative societal impacts include potential malicious or unintended uses
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
(e.g., deployment of technologies that could make decisions that unfairly impact specific
groups), privacy considerations, and security considerations.
•The conference expects that many papers will be foundational research and not tied
to particular applications, let alone deployments. However, if there is a direct path to
any negative applications, the authors should point it out. For example, it is legitimate
to point out that an improvement in the quality of generative models could be used to
generate deepfakes for disinformation. On the other hand, it is not needed to point out
that a generic algorithm for optimizing neural networks could enable people to train
models that generate Deepfakes faster.
•The authors should consider possible harms that could arise when the technology is
being used as intended and functioning correctly, harms that could arise when the
technology is being used as intended but gives incorrect results, and harms following
from (intentional or unintentional) misuse of the technology.
•If there are negative societal impacts, the authors could also discuss possible mitigation
strategies (e.g., gated release of models, providing defenses in addition to attacks,
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
feedback over time, improving the efficiency and accessibility of ML).
11.Safeguards
Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?
Answer: [NA]
Justification: We do not train models or use data that have a high risk of misuse.
Guidelines:
• The answer NA means that the paper poses no such risks.
•Released models that have a high risk for misuse or dual-use should be released with
necessary safeguards to allow for controlled use of the model, for example by requiring
that users adhere to usage guidelines or restrictions to access the model or implementing
safety filters.
•Datasets that have been scraped from the Internet could pose safety risks. The authors
should describe how they avoided releasing unsafe images.
•We recognize that providing effective safeguards is challenging, and many papers do
not require this, but we encourage authors to take this into account and make a best
faith effort.
12.Licenses for existing assets
Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?
Answer: [Yes]
Justification: We credit code and data used in Appendix F.5. For other types of models, we
cite papers in which they were introduced.
Guidelines:
• The answer NA means that the paper does not use existing assets.
• The authors should cite the original paper that produced the code package or dataset.
•The authors should state which version of the asset is used and, if possible, include a
URL.
• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
•For scraped data from a particular source (e.g., website), the copyright and terms of
service of that source should be provided.
35•If assets are released, the license, copyright information, and terms of use in the
package should be provided. For popular datasets, paperswithcode.com/datasets
has curated licenses for some datasets. Their licensing guide can help determine the
license of a dataset.
•For existing datasets that are re-packaged, both the original license and the license of
the derived asset (if it has changed) should be provided.
•If this information is not available online, the authors are encouraged to reach out to
the asset’s creators.
13.New Assets
Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?
Answer: [Yes]
Justification: We have open-sourced new assets here: https://github.com/cptq/
asymmetric-networks . The documentation can be found at the link.
Guidelines:
• The answer NA means that the paper does not release new assets.
•Researchers should communicate the details of the dataset/code/model as part of their
submissions via structured templates. This includes details about training, license,
limitations, etc.
•The paper should discuss whether and how consent was obtained from people whose
asset is used.
•At submission time, remember to anonymize your assets (if applicable). You can either
create an anonymized URL or include an anonymized zip file.
14.Crowdsourcing and Research with Human Subjects
Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as
well as details about compensation (if any)?
Answer: [NA]
Justification: No crowdsourcing or human subjects were used.
Guidelines:
•The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
•Including this information in the supplemental material is fine, but if the main contribu-
tion of the paper involves human subjects, then as much detail as possible should be
included in the main paper.
•According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
or other labor should be paid at least the minimum wage in the country of the data
collector.
15.Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects
Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?
Answer: [NA]
Justification: No crowdsourcing or human subjects were used.
Guidelines:
•The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
•Depending on the country in which research is conducted, IRB approval (or equivalent)
may be required for any human subjects research. If you obtained IRB approval, you
should clearly state this in the paper.
36•We recognize that the procedures for this may vary significantly between institutions
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
guidelines for their institution.
•For initial submissions, do not include any information that would break anonymity (if
applicable), such as the institution conducting the review.
37