Published in Transactions on Machine Learning Research (6/2024)
Koopman Spectrum Nonlinear Regulators and Efficient On-
line Learning
Motoya Ohnishi mohnishi@cs.washington.edu
Paul G. Allen School of Computer Science & Engineering
University of Washington
Isao Ishikawa ishikawa.isao.zx@ehime-u.ac.jp
Ehime University
RIKEN Center for Advanced Intelligence Project
Kendall Lowrey kendall.lowrey@gmail.com
Et Cetera Robotics
Masahiro Ikeda masahiro.ikeda@riken.jp
RIKEN Center for Advanced Intelligence Project
Keio University
Sham Kakade sham@seas.harvard.edu
Harvard University
Yoshinobu Kawahara kawahara@ist.osaka-u.ac.jp
Graduate School of Information Science and Technology, Osaka University
RIKEN Center for Advanced Intelligence Project
Reviewed on OpenReview: https: // openreview. net/ forum? id= thfoUZugvS
Abstract
Most modern reinforcement learning algorithms optimize a cumulative single-step cost along
a trajectory. The optimized motions are often ‘unnatural’, representing, for example, be-
haviors with sudden accelerations that waste energy and lack predictability. In this work,
we present a novel paradigm of controlling nonlinear systems via the minimization of the
Koopman spectrum cost : a cost over the Koopman operator of the controlled dynamics.
This induces a broader class of dynamical behaviors that evolve over stable manifolds such
as nonlinear oscillators, closed loops, and smooth movements. We demonstrate that some
dynamics characterizations that are not possible with a cumulative cost are feasible in this
paradigm, which generalizes the classical eigenstructure and pole assignments to nonlinear
decision making. Moreover, we present a sample efficient online learning algorithm for our
problem that enjoys a sub-linear regret bound under some structural assumptions.
1 Introduction
Reinforcement learning (RL) has been successfully applied to diverse domains, such as robot control (Kober
et al. (2013); Todorov et al. (2012); Ibarz et al. (2021)) and playing video games (Mnih et al. (2013; 2015)).
Most modern RL problems modeled as Markov decision processes consider an immediate (single-step) cost
(reward) that accumulates over a certain time horizon to encode tasks of interest. Although such a cost
can encode any single realizable dynamics, which is central to inverse RL problems (Ng & Russell (2000)),
the generated motions often exhibit undesirable properties such as high jerk, sudden accelerations that
waste energy, and unpredictability. Intuitively, the motion specified by the task-oriented cumulative cost
formulation may ignore “how to” achieve the task unless careful design of cumulative cost is in place,
1Published in Transactions on Machine Learning Research (6/2024)
necessitating a systematic approach that effectively regularizes or constrains the dynamics to guarantee
predictable global property such as stability.
Meanwhile, many dynamic phenomena found in nature are known to be represented as simple trajectories,
such as nonlinear oscillators, on low-dimensional manifolds embedded in high-dimensional spaces that we
observe (Strogatz, 2018). Its mathematical concept is known as phase reduction (Winfree, 2001; Nakao,
2017), and recently its connection to the Koopman operator has been attracted much attention in response
to the growing abundance of measurement data and the lack of known governing equations for many systems
of interest (Koopman, 1931; Mezić, 2005; Kutz et al., 2016).
In this work, we present a novel paradigm of controlling nonlinear systems based on the spectrum of the
Koopmanoperator. Tothisend, weexploittherecenttheoreticalandpracticaldevelopmentsoftheKoopman
operators (Koopman, 1931; Mezić, 2005), and propose the Koopman spectrum cost as the cost over the
Koopman operator of controlled dynamics, defining a preference of the dynamical system in the reduced
phase space. The Koopman operator, also known as the composition operator, is a linear operator over an
observable space of a (potentially nonlinear) dynamical system, and is used to extract global properties of
the dynamics such as its dominating modes and eigenspectrum through spectral decomposition. Controlling
nonlinear systems via the minimization of the Koopman spectrum cost induces a broader class of dynamical
behaviors such as nonlinear oscillators, closed loops, and smooth movements.
Although working in the spectrum (or frequency) domain has been standard in the control community (e.g.
Andry et al. (1983); Hemati & Yao (2017)), the use of the Koopman spectrum cost together with function
approximation and learning techniques enables us to generate rich class of dynamics evolving over stable
manifolds (cf. Strogatz (2018)).
Our contributions. The contributions of this work are three folds: First, we propose the Koopman spec-
trum cost that complements the (cumulative) single-step cost for nonlinear control. Our problem, which we
refer to as Koopman Spectrum Nonlinear Regulator (KSNR), is to find an optimal parameter (e.g. policy
parameter) that leads to a dynamical system associated to the Koopman operator that minimizes the sum of
both the Koopman spectrum cost and the cumulative cost. Note that “Regulator” in KSNR means not only
controller in control problems but a more broad sense of regularization of dynamical systems for attaining
specific characteristics. Second, we show that KSNR paradigm effectively encodes/imitates some desirable
agent dynamics such as limit cycles, stable loops, and smooth movements. Note that, when the underlying
agent dynamics is known, KSNR may be approximately solved by extending any nonlinear planning heuris-
tics, including population based methods. Lastly, we present a (theoretical) learning algorithm for online
KSNR, which attains the sub-linear regret bound (under certain condition, of order ˜O(√
T)). Our algo-
rithm (Koopman-Spectrum LC3(KS-LC3)) is a modification of Lower Confidence-based Continuous Control
(LC3) (Kakade et al., 2020) to KSNR problems with several technical contributions. We need structural
assumptions on the model to simultaneously deal with the Koopman spectrum cost and cumulative cost.
Additionally, we present a certain Hölder condition of the Koopman spectrum cost that makes regret analysis
tractable for some cost such as the spectral radius.
Key Takeaways 
KSNRconsidersthe spectrum cost thatis notsubsumedby aclassicalsingle-stepcost oranepisodiccost,
and is beyond the MDP framework, which could be viewed as a generalization of eigenstructure/pole
assignments to nonlinear decision making problems. The framework systematically deals with the
shapingof behavior (e.g., ensuring stability, smoothness, and adherence to a target modeof behavior).
Because we employ this new cost criterion, we strongly emphasize that this work is not intended to
compete against the MDP counterparts, but rather to illustrate the effectiveness of the generalization
we make for systematic behavior shaping . For online learning settings under unknown dynamics, the
spectrum cost is unobservable because of inaccessible system models; it thus differs from other costs such
as a policy cost. As such, we need specific structural assumptions that are irrelevant to the Kernelized
Nonlinear Regulator (Kakade et al., 2020) to devise sample efficient (if not computationally efficient)
algorithm. Proof techniques include some operator theoretic arguments that are novel in this context.
 
2Published in Transactions on Machine Learning Research (6/2024)
Notation. Throughout this paper, R,R≥0,N,Z>0, and Cdenote the set of the real numbers, the non-
negative real numbers, the natural numbers ( {0,1,2,...}), the positive integers, and the complex numbers,
respectively. Also, Πis a set of dynamics parameters, and [H] :={0,1,...H−1}forH∈Z>0. The set of
bounded linear operators from AtoBis denoted byL(A;B), and the adjoint of the operator Ais denoted
byA†. We let det(·)be the functional determinant. Finally, we let ∥x∥Rd,∥x∥1,∥A∥, and∥A∥HSbe the
Euclidean norm of x∈Rd, the 1-norm (sum of absolute values), the operator norm of A∈L(A;B), and the
Hilbert–Schmidt norm of a Hilbert-Schmidt operator A, respectively.
2 Related work
Koopman operator was first introduced in Koopman (1931); and, during the last two decades, it has gained
traction, leading to the developments of theory and algorithm (e.g. Črnjarić-Žic et al. (2019); Kawahara
(2016); Mauroy & Mezić (2016); Ishikawa et al. (2018); Iwata & Kawahara (2020); Burov et al. (2021))
partially due to the surge of interests in data-driven approaches. The analysis of nonlinear dynamical
system with Koopman operator has been applied to control (e.g. Korda & Mezić (2018); Mauroy et al.
(2020); Kaiser et al. (2021); Li et al. (2019); Korda & Mezić (2020)), using model predictive control (MPC)
framework and linear quadratic regulator (LQR) although nonlinear controlled systems in general cannot
be transformed to LQR problem even by lifting to a feature space. For unknown systems, active learning
of Koopman operator has been proposed (Abraham & Murphey (2019)). Note our framework applied to
stability regularization considers the solely different problem than that of (Mamakoukas et al., 2023). In
the context of stability regularization, our framework is not for learning the stable Koopman operator or
to construct a control Lyapunov function from the learned operator but to solve the regulator problem to
balance the (possibly task-based) cumulative cost and the spectrum cost that enforces stability. We will
revisit this perspective in Section 3.3.
The line of work that is most closely related to ours is the eigenstructure/pole assignments problem (e.g.
Andry et al. (1983); Hemati & Yao (2017)) classically considered in the control community particularly
for linear systems. In fact, in the literature on control, working in frequency domain has been standard
(e.g. Pintelon & Schoukens (2012); Sabanovic & Ohnishi (2011)). These problems aim at computing a
feedback policy that generates the dynamics whose eigenstructure matches the desired one; we note such
problems can be naturally encoded by using the Koopman spectrum cost in our framework as well. In
connection with the relation of the Koopman operator to stability, these are in principle closely related to
the recent surge of interest in neural networks to learn dynamics with stability (Manek & Kolter, 2019;
Takeishi & Kawahara, 2021).
In the RL community, there have been several attempts of using metrics such as mutual information for
acquisitions of the skillsunder RL settings, which are often referred to as unsupervised RL (e.g. Eysenbach
et al. (2018)). These work provide an approach of effectively generating desirable behaviors through com-
positions of skills rather than directly optimizing for tasks, but are still within cumulative (single-step) cost
framework. Historically, the motor primitives investigated in, for example, (Peters & Schaal, 2008; Ijspeert
et al., 2002; Stulp & Sigaud, 2013) have considered parametric nonlinear dynamics having some desirable
properties such as stability, convergence to certain attractor etc., and it is related to the Koopman spectrum
regularization in the sense both aim at regulating the global dynamical properties. Those primitives may be
discovered by clustering (cf. Stulp et al. (2014)), learned by imitation learning (cf. Kober & Peters (2010)),
and coupled with meta-parameter learning (e.g. Kober et al. (2012)).
Finally, as related to the sample efficient algorithm we propose, provably correct methods (e.g. Jiang et al.
(2017); Sun et al. (2019)) have recently been developed for continuous control problems (Kakade et al., 2020;
Mania et al., 2020; Simchowitz & Foster, 2020; Curi et al., 2020).
Below, we present our control framework, KSNR, in Section 3 with several illustrative numerical examples
based on population based policy search (e.g. genetic algorithm), followed by an introduction of its example
online learning algorithm (Section 4) with its theoretical insights on sample complexity and on reduction of
the model to that of eigenstructure assignments problem as a special case. For more details about population
based search that repeatedly evaluates the sampled actions to update the sampling distribution of action
sequence so that the agent can achieve lower cost, see for example (Beheshti & Shamsuddin, 2013).
3Published in Transactions on Machine Learning Research (6/2024)
3 Koopman Spectrum Nonlinear Regulator
In this section, we present the dynamical system model and our main framework.
3.1 Dynamical system model
LetX ⊂RdXbe the state space, and Πa set of parameters each of which corresponds to one random
dynamical system (RDS) as described below. Given a parameter Θ∈Π, let(ΩΘ,PΘ)be a probability space,
where ΩΘis a measurable space and PΘis a probability measure. Let µΘ:= (µΘ(r))r∈Nbe a semi-group
of measure preserving measurable maps on ΩΘ(i.e.,µΘ(r) : Ω Θ→ΩΘ). This work studies the following
nonlinear regulator (control) problem: for each parameter Θ∈Π, the corresponding nonlinear random
dynamical system is given by
FΘ:N×ΩΘ×X→X,
that satisfies
FΘ(0,ω,x ) =x,FΘ(r+s,ω,x ) =FΘ(r,µΘ(s)ω,FΘ(s,ω,x )),∀r,s∈N, ω∈ΩΘ, x∈X.(3.1)
The above definition of random dynamical system is standard in the community studying dynamical systems
(refer to (Arnold, 1998), for example). Roughly speaking, an RDS consists of the following two models:
•A model of the noise;
•A function representing the physicaldynamics of the system.
RDSs subsume many practical systems including solutions to stochastic differential equations and additive-
noise systems, i.e.,
xh+1=f(xh) +ηh, x0∈Rd, h∈[H],
wheref:RdX→RdXrepresents the dynamics, and ηh∈RdXis the zero-mean i.i.d. additive noise vector.
LetΩ0be a probability space of the noise, and one could consider Ω := ΩN
0(andµis the shift) to express the
system as an RDS. Also, Markov chains can be described as an RDS by regarding them as a composition of
i.i.d. random transition and by following similar arguments to the above (see Arnold (1998, Theorem 2.1.6)).
As an example, consider the discrete states {s1,s2}and the transition matrix
Transition Matrix :=/bracketleftbigg0.8 0.2
0.1 0.9/bracketrightbigg
= 0.7/bracketleftbigg1 0
0 1/bracketrightbigg
+ 0.2/bracketleftbigg0 1
0 1/bracketrightbigg
+ 0.1/bracketleftbigg1 0
1 0/bracketrightbigg
,
where the right hand side shows a decomposition of the transition matrix to deterministic transitions. For
thisexample, onecannaturelytreatthisMarkovchainasanRDSthatgenerateseachdeterministictransition
with corresponding probability at every step.
More intuitively, dynamical systems with an invariant noise-generating mechanism could be described as an
RDS by an appropriate translation (see Figure 1 (Left) for an illustration of an RDS).
Koopman operator For the random dynamical systems being defined above, we define the operator-
valued map Kbelow, using the dynamical system model.
Definition 3.1 (Koopman operator) .LetHbe a function space on XoverCand let{FΘ}Θ∈Πbe a
dynamical system model. We define an operator-valued map KbyK: Π→L(H,H)such that for any
Θ∈Πandg∈H,
[K(Θ)g](x) :=EΩΘ/bracketleftbig
g◦FΘ(1,ω,x )/bracketrightbig
, x∈X.
We will choose a suitable Hto define the map K, andK(Θ)is the Koopman operator for FΘ. Essentially,
the Koopman operator represents a nonlinear dynamics as a linear (infinite-dimensional) operator that
describes the evolution of observables in a lifted space (see Figure 1 Right).
4Published in Transactions on Machine Learning Research (6/2024)
Figure 1: Left: Random dynamical system consists of a model of the noiseand a function representing the
physicalphase space (the illustration is inspired by Arnold (1998); Ghil et al. (2008)). The RDS flows over
sample space and phase space for each realization ωand for initial state x0. Right: By lifting the state space
to a space of observables, a nonlinear dynamical system over the state space is represented by the linear
operator in a lifted space.
Remark 3.1 (Choice ofHand existence of K).In an extreme (but useless) case, one could choose H
to be a one dimensional space spanned by a constant function, and Kcan be defined. In general, the
properties of the Koopman operator depend on the choice of the space on which the operator is defined. As
more practical cases, if one employs a Gaussian RKHS for example, the only dynamics inducing bounded
Koopman operators are those of affine (e.g. Ishikawa (2023); Ikeda et al. (2022)). However, some practical
algorithms have recently been proposed for an RKHS to approximate the eigenvalues of so-called “extended”
Koopman operator through some appropriate computations under certain conditions on the dynamics and
on the RKHS (cf. Ishikawa et al. (2024)).
With these settings in place, we propose our framework.
3.2 Koopman Spectrum Nonlinear Regulator
FixasetX0:={(x0,0,H0),(x0,1,H1),..., (x0,N−1,HN−1)}⊂X× Z>0, forN∈Z>0, anddefine c:X→R≥0
be a cost function. The Koopman Spectrum Nonlinear Regulator (KSNR), which we propose in this paper,
is the following optimization problem:
Find Θ⋆∈arg min
Θ∈Π/braceleftbig
Λ[K(Θ)] +JΘ(X0;c)/bracerightbig
, (3.2)
where Λ :L(H;H)→R≥0is a mapping that takes a Koopman operator as an input and returns its cost;
and
JΘ(X0;c) :=N−1/summationdisplay
n=0EΩΘ/bracketleftiggHn−1/summationdisplay
h=0c(xh,n)/vextendsingle/vextendsingle/vextendsingleΘ,x0,n/bracketrightigg
,
wherexh,n(ω) :=FΘ(h,ω,x 0,n). In control problem setting, the problem (3.2) can be read as finding a
control policy Θ∗, which minimizes the cost; note, each control policy Θgenerates a dynamical system that
gives the costs Λ[K(Θ)]andJΘ(X0;c)in this case. However, we mention that the parameter Θcan be the
physics parameters used to design the robot body for automated fabrication or any parameter that uniquely
determines the dynamics.
Example 3.1 (Examples of Λ).Some of the examples of Λare:
1.Λ[A] = max{1,ρ(A)}, whereρ(A)is the spectral radius of A, prefers stable dynamics.
5Published in Transactions on Machine Learning Research (6/2024)
Figure2: Comparisonsofseveralcostsfordecisionmakingproblems. TheKoopmanspectrumcostisthecost
over the global properties of the dynamical system itself which is typically unknown for learning problems,
and is unobservable.
2.Λ[A] =ℓA⋆(A)can be used for imitation learning, where ℓA⋆(A) :L(H;H)→R≥0is a loss
function measuring the gap between Aand the given A⋆∈L(H;H).
3.Λ[A] =/summationtext
i|λi(A)|, prefers agent behaviors described by fewer dominating modes. Here, {λi}i∈Z>0
is the set of eigenvalues of the operator A(assuming that the operator has discrete spectrum).
AssumingthattheKoopmanoperatorisdefinedoverafinite-dimensionalspace, aneigenvalueoftheoperator
correspondingtoeacheigenfunctioncanbegivenbythatofthematrixrealizationoftheoperator. Inpractice,
one employs a finite-dimensional space even if it is not invariant under the Koopman operator; in such cases,
thereareseveralanalysesthathaverecentlybeenmadeforprovidingestimatesofthespectraoftheKoopman
operator through computations over such a finite-dimensional space. Interested readers may be referred to
(Ishikawaetal.,2024;Colbrook&Townsend,2024;Colbrooketal.,2023)forexample. Inparticular, avoiding
spectral pollution , which refers to the phenomena where discretizations of an infinite-dimensional operator
to a finite matrix create spurious eigenvalues, and approximating the continuous spectra have been actively
studied with some theoretical convergence guarantees for the approximated spectra. In order to obtain an
estimate of the spectral radius through computations on a matrix of finite rank, the Koopman operator may
need to be compact (see the very recent work Akindji et al. (2024) for example).
Remark 3.2 (Remarks on how the choice of Haffects the Koopman spectrum cost) .As we have discussed
in Remark 3.1, the mathematical properties of the Koopman operator depend on the function space H, and
information of the background dynamics implied by this operator depends on the choice of H; however their
spectral properties typically capture global characterstics of the dynamics through its linearity.
We mention that the Koopman operator over Hmayfully represents the dynamics in the sense that it can
reproduce the dynamical system over the state space (i.e., the original space) under certain conditions (see
Ishikawa et al. (2024) for example on detailed discussions); however, depending on the choice of H, it is
often the case that the Koopman operator does not uniquely reproduce the dynamics. The extreme case
of such an example is the function space of single dimension spanned by a constant function, for which
the Koopman operator does not carry any information about the dynamics. Even for the cases where the
Koopman operator over the chosen space Honly captures partial information on the dynamics, the spectrum
cost is still expected to act as a regularizer of such partial spectral properties of the dynamics.
As also mentioned in Remark 3.2, the Koopman spectrum cost regularizes certain global properties of the
generated dynamical system; as such, it is advantageous to employ this cost Λover sums of single-step
costsc(x)that are the objective in MDPs especially when encoding stability of the system for example. To
illustrate this perspective more, we consider generating a desirable dynamical system (or trajectory) as a
solution to some optimization problem.
6Published in Transactions on Machine Learning Research (6/2024)
Figure 3: While single-step costs (taking the current and next states as input) could be used to specify every
transition, acting as a “local” cost, the Koopman spectrum cost regularizes “global” characteristics of the
dynamics through specifying its spectral properties (e.g., by forcing the dynamics to have some given mode
m∗as its top mode). The regularization incurred by the Koopman spectrum cost may not be implemented
by the cumulative cost formulation in a straightforward manner. We mention it has some relations to the skill
learning with motor primitives (see Section 2) in the sense that both aim at regulating the global dynamical
properties.
LetX=R,υ∈(0,1], and letc:X →R≥0be nonconstant cost function. We consider the following loss
function for a random dynamical system F:N×Ω×X→X :
ℓ(F,x) :=EΩ∞/summationdisplay
h=0υhc(F(h,ω,x 0)).
Now consider the dynamical system F(1,ω,x ) =−x,∀x∈X,∀ω∈Ω, for example. Then, it holds that,
for any choice of υandc, there exists another random dynamical system Gsatisfying that for all x∈X,
ℓ(G,x)<ℓ(F,x).
This fact indicates that there exists a dynamical system that cannot be described by a solution to the above
optimization. Note, however, that given any deterministic map f⋆:X→X, if one employs a (different form
of) cost c:X×X→ R≥0, where c(x,y)evaluates to 0only ify=f⋆(x)and otherwise evaluates to 1, it is
straightforward to see that the dynamics f⋆is the one that simultaneously optimizes c(x,y)for anyx∈X.
In other words, this form of single-step cost uniquely identifies the (deterministic) dynamics by defining its
evaluation at each state (see Figure 3 (Left)).
In contrast, when one wishes to constrain or regularize the dynamics globally in the (spatio-temporal)
spectrum domain, to obtain the set of stable dynamics for example, single-step costs become powerless.
Intuitively, while cumulative cost can effectively determine or regularize one-step transitions towards certain
state, the Koopman spectrum cost can characterize the dynamics globally (see Figure 3 (Right)). Refer to
Appendix C for more formal arguments.
Although aforementioned facts are simple, they give us some insights on the limitation of the use of (cumu-
lative) single-step cost for characterizing dynamical systems, and imply that enforcing certain constraints
on the dynamics requires the Koopman spectrum perspective. This is similar to the problems treated in the
Fourier analysis; where the global (spectral) characteristics of the sequential data are better captured in the
frequency domain.
Lastly, we depicted how the spectrum cost differs from other types of costs used in decision making problems
in Figure 2. The figure illustrates that the spectrum cost is not incurred on a single-step or on one trajectory
7Published in Transactions on Machine Learning Research (6/2024)
Figure 4: Left: We minimize solely for Koopman spectrum cost Λ(A) =∥m−m⋆∥1to imitate the top mode
of a reference spectrum to recover a desired limit cycle behavior for the single-integrator system. Right: By
regularizing the spectral radius of Cartpole with a cumulative cost that favors high velocity, the cartpole
performs a stable oscillation rather than moving off to infinity.
Figure 5: The joint angle trajectories generated by a combination of linear and RFF policies. Left: when
only cumulative reward is maximized. Right: when both the cumulative cost and the spectrum cost Λ(A) =
5/summationtextdϕ
i=1|λi(A)|are used, where the factor 5is multiplied to balance between the spectrum cost and the
cumulative cost.
(episode), but is over a (part of) the global properties of the dynamical system model (see also Remark
3.2). The dynamics typically corresponds to a policy, but a policy regularization cost used in, for example
(Haarnoja et al., 2018), is computable when given the current policy while the spectrum cost is unobservable
if the dynamics is unknown (and hence is not directly computable).
3.3 Simulated experiments
We illustrate how one can use the costs in Example 3.1. See Appendix H for detailed descriptions and
results of the experiments. Throughout, we used Julia language (Bezanson et al., 2017) based robotics
control package, Lyceum (Summers et al., 2020), for simulations and visualizations. Also, we use Cross-
Entropy Method (CEM) based policy search (Kobilarov (2012); one of the population based policy search
techniques) to optimize the policy parameter Θto minimize the cost in (3.2). Specifically, at each iteration
of CEM, we generate many parameters ( Θs) to compute the loss (i.e., the sum of the Koopman spectrum
cost and negative cumulative reward). This is achieved by fitting the transition data to the chosen feature
space to estimate its (finite-dimensional realization of) Koopman operator (see Appendix H.1); here the data
are generated by the simulator which we assume to have access to.
8Published in Transactions on Machine Learning Research (6/2024)
Imitating target behaviors through the Koopman operators We consider the limit cycle dynamics
˙r=r(1−r2),˙θ= 1,
describedbythepolarcoordinates, andfindtheKoopmanoperatorforthisdynamicsbysamplingtransitions,
assumingHis the span of Random Fourier Features (RFFs) (Rahimi & Recht, 2007). We illustrate how
KSNR is used to imitate the dynamics; in particular, by imitating the Koopman modes , we expect that some
physically meaningful dynamical behaviors of the target system can be effectively reconstructed.
To define Koopman modes, suppose that the Koopman operator A⋆induced by the target dynamics has
eigenvalues λi∈Cand eigenfunctions ξi:X→Cfori∈{1,2,...,dϕ}, i.e.,
A⋆ξi=λiξi.
If the set of observables ϕis satisfies
ϕx=dϕ/summationdisplay
i=1ξi(x)m⋆
i,
form⋆
i∈Cdϕ, whereϕx:= [ϕ1(x),ϕ2(x),...,ϕdϕ(x)]⊤∈Rdϕ, then m⋆
is are called the Koopman modes.
The Koopman modes are closely related to the concept isostable; interested readers are referred to (Mauroy
et al., 2013) for example.
In this simulated example, the target system is being imitated by forcing the generated dynamics to have
(approximately) the same top mode (i.e., the Koopman modes corresponding to the largest absolute eigen-
value) that dominates the behavior. To this end, with Πa space of RFF policies that define ˙rand ˙θas a
single-integrator model, we solve KSNR for the spectrum cost Λ(A) =∥m−m⋆∥1, where m∈Cdϕandm⋆
are the top modes of the Koopman operator induced by the generated dynamics and of the target Koopman
operator found previously, respectively.
Figure 4 (Left) plots the trajectories (of the Cartesian coordinates) generated by RFF policies that minimize
this cost; it is observed that the agent successfully converged to the desired limit cycle of radius one by
imitating the dominant mode of the target spectrum.
Generating stable loops (Cartpole) We consider Cartpole environment (where the rail length is ex-
tended from the original model). The single-step reward (negative cost) is 10−3|v|wherevis the velocity of
the cart, plus the penalty −1when the pole falls down (i.e., directing downward). This single-step reward
encourages the cartpole to maximize its velocity while preferring not to let the pole fall down. The additional
spectrum cost considered in this experiment is Λ(A) = 104max(1,ρ(A)), which highly penalizes spectral
radius larger than one; it therefore regularizes the dynamics to be stable, preventing the velocity from ever
increasing its magnitude.
Figure 4 (Right) plots the cart velocity trajectories generated by RFF policies that (approximately) solve
KSNR with/without the spectrum cost. It is observed that spectral regularization led to a back-and-forth
motion while the non-regularized policy preferred accelerating to one direction to solely maximize velocity.
When the spectrum cost was used, the cumulative rewards were 0.072and the spectral radius was 0.990,
while they were 0.212and1.003when the spectrum cost was not used; limiting the spectral radius prevents
the ever increasing change in position.
We mention that this experiment is not intended to force the dynamics to have this oscillation, but rather to
let the dynamics be stable in the sense that the Koopman operator over a chosen space has spectral radius
that is less than or equal to one (see Figure 3 to review the difference of concepts and roles of cumulative
cost and Koopman spectrum cost). See more experimental results in Appendix I.
Remark 3.3 (Remarks on the stability regularization) .As mentioned in Section 2, this stability regular-
ization is not meant to learn the stable Koopman operator but to find an RDS that balances the cumulative
cost and the spectrum cost that enforces stability; in other words, the task is to find a dynamical system
that minimizes the cumulative cost under the hard stability constraint encoded by the spectrum cost. Here,
9Published in Transactions on Machine Learning Research (6/2024)
Figure 6: Examples of attractor dynamics that are stable; from the left, they are a fixed-point attractor,
limit cycle attractor, and strange attractor. Those should be included in the set of stable dynamics.
we emphasize that the set of stable dynamics should include a variety of dynamics ranging from a fixed-point
attractor to a strange attractor as portrayed in Figure 6. On the other hand, constraining dynamics by a
single control Lyapunov function (cf. Freeman & Primbs (1996)) will form a strictly smaller set of stable
dynamics (see the discussions in Section 3.2 as well). Also, in this simulation example, the stability does
not necessarily mean the behaviors converging to the zero state as long as the velocity does not increase
indefinitely, andthepreferenceofkeepingthepolestraightupisencodedbythecumulativecostterminstead.
Generating smooth movements (Walker) We use the Walker2d environment and compare movements
with/without the spectrum cost. The single-step reward (negative cost) is given by v−0.001∥a∥2
R6, wherevis
the velocity and ais the action vector of dimension 6. The spectrum cost is given by Λ(A) = 5/summationtextdϕ
i=1|λi(A)|,
wherethefactor 5ismultiplied tobalancebetweenthespectrumand cumulativecost. The single-step reward
encourages the walker to move to the right as fast as possible with a small penalty incurred on the action
input, and the spectrum cost regularizes the dynamics to have fewer dominating modes, which intuitively
leads tosmoother motions.
Figure 5 plots typical joint angles along a trajectory generated by a combination of linear and RFF policies
that (approximately) solves KSNR with/without the spectrum cost. It is observed that the spectrum cost
led to simpler (i.e., some joint positions converge) and smoother dynamics while doing the task sufficiently
well. With the spectrum cost, the cumulative rewards and the spectrum costs averaged across four random
seeds (plus-minus standard deviation) were 584±112and196±8.13. Without the spectrum cost, they were
698±231and310±38.6. We observe that, as expected, the spectrum cost is lower for KSNR while the
classical cumulative reward is higher for the behavior generated by the optimization without spectrum cost.
Again, we emphasize that we are notcompeting against the MDP counterparts in terms of the cumulative
reward, but rather showing an effect of additional spectrum cost. Please also refer to Appendix H and I for
detailed results.
4 Theoretical algorithm of online KSNR and its insights on the complexity
In this section, we present a (theoretical) learning algorithm for online KSNR. Although the KSNR itself
is a general regulator framework, we need some structural assumptions to the problem in order to devise
a sample efficient (if not computation efficient) algorithm. Nevertheless, despite the unobservability of the
spectrum cost, KSNR admits a sample efficient model-based algorithm through operator theoretic arguments
under those assumptions. Here, “model-based” simply means we are not directly learning the spectrum cost
itself but the Koopman operator model. We believe the algorithm and its theoretical analysis clarify the
intrinsic difficulty of considering the spectrum cost, and pave the way towards future research.
The learning goal is to find a parameter Θ⋆tthat satisfies (3.2) at each episode t∈[T]. We employ
episodic learning, and let Θtbe a parameter employed at the t-th episode. Adversary chooses Xt
0:=
{(xt
0,0,Ht
0),(xt
0,1,Ht
1),..., (xt
0,Nt−1,Ht
Nt−1)}⊂X× Z>0, whereNt∈Z>0, and the cost function ctat the
beginning of each episode. Let ct
h,n:=ct(xt
h,n).ω∈ΩΘtis chosen according to PΘt.
10Published in Transactions on Machine Learning Research (6/2024)
Algorithmevaluation. Inthiswork, weemploythefollowingperformancemetric, namely, thecumulative
regret:
RegretT:=T−1/summationdisplay
t=0
Λ[K(Θt)] +Nt−1/summationdisplay
n=0Ht
n−1/summationdisplay
h=0ct
h,n
−T−1/summationdisplay
t=0min
Θ∈Π/parenleftbig
Λ[K(Θ)] +JΘ(Xt
0;ct)/parenrightbig
.
Note the minimum on the second term on the right hand side is taken at every episode. Below, we present
model assumptions and an algorithm with a regret bound.
4.1 Models and algorithms
We make the following modeling assumptions.
Assumption 1. LetK(Θ)be the Koopman operator corresponding to a parameter Θ. Then, assume that
there exists a finite-dimensional subspace H0onXoverRand its basis ϕ1,...,ϕdϕsuch that the random
dynamical system ( 3.1) satisfies the following:
∀Θ∈Π,∀x∈X:ϕi(FΘ(1,ω,x )) = [K(Θ)ϕi](x) +ϵi(ω),
where the additive noise ϵi(ω)∼N(0,σ2)is assumed to be independent across timesteps, parameters Θ, and
indicesi.
Remark 4.1 (On Assumption 1) .Although the added noise term is expected to deal with stochasticity and
misspecification to some extent in practice, Assumption 1 is strong to ask for; in fact, claiming existence
of the Koopman operator over a useful RKHS (e.g., with Gaussian kernel) is not trivial for most of the
practical problems. Studying misspecified case with small error margin is an important future direction of
research; however, as our regulator problem is novel, we believe this work guides the future attempts of
further algorithmic research.
Assumption 2 (Function-valued RKHS (see Appendix A for details)) .K(·)ϕforϕ∈His assumed to live
in a known function valued RKHS with the operator-valued reproducing kernel K(·,·) : Π×Π→L(H;H),
or equivalently, there exists a known map Ψ : Π→L(H;H′)for a specific Hilbert space H′satisfying for any
ϕ∈H, there exists ψ∈H′such that
K(·)ϕ= Ψ(·)†ψ. (4.1)
Remark 4.2 (On Assumption 2) .The assumption intuitively states that the structure on how the closeness
of parameters Θs relates to that of the resulting Koopman operators is known. One can always consider an
expressive function-valued RKHS in practice, but it may lead to increased (effective) dimensions that will
require more samples to learn.
When Assumption 2 holds, we obtain the following claim which is critical for our learning framework.
Lemma 4.3. Suppose Assumption 2 holds. Then, there exists a linear operator M⋆:H→H′such that
K(Θ) = Ψ(Θ)†◦M⋆.
In the reminder of this paper, we work on the invariant subspace H0in Assumption 1 and thus we regard
H=H0∼=Rdϕ,L(H;H)∼=Rdϕ×dϕ, and, by abuse of notations, we view K(Θ)as the realization of the
operator over Rdϕ, i.e.,
ϕFΘ(1,ω,x)=K(Θ)ϕx+ϵ(ω) = [Ψ(Θ)†◦M⋆]ϕx+ϵ(ω),
whereϕx:= [ϕ1(x),ϕ2(x),...,ϕdϕ(x)]⊤∈Rdϕ, andϵ(ω) := [ϵ1(ω),ϵ2(ω),...,ϵdϕ(ω)]⊤∈Rdϕ.
Finally, we assume the following.
Assumption3 (Realizabilityofcosts) .For allt, the single-step cost ctis known and satisfies ct(x) =wt(ϕx)
for some known map wt:Rdϕ→R≥0.
11Published in Transactions on Machine Learning Research (6/2024)
Algorithm 1 Koopman-Spectrum LC3(KS-LC3)
Require: Parameter set Π; regularizer λ
1:Initialize Ball0
Mto be a set containing M⋆.
2:fort= 0...T−1do
3:Adversary chooses Xt
0.
4: Θt,ˆMt= arg minΘ∈Π, M∈Ballt
MΛ[Ψ(Θ)†◦M] +JΘ(Xt
0;M;ct)
5:Under the dynamics FΘt, sample transition data τt:={τt
n}Nt−1
n=0, whereτt
n:={xt
h,n}Ht
n
h=0
6:Update Ballt+1
M.
7:end for
Remark 4.4 (On Assumption 3) .As mentioned in Remark 3.2, the space H0over which the Koopman
operator is acting should be properly chosen so that the Koopman operator exists and that its spectrum cost
has desirable regularization effect over the dynamics. At the same time, Assumption 3 requires that H0is
sufficiently expressive in the sense that it can capture the immediate cost ct.
For later use, we define, for all t∈[T],n∈[Nt], andh∈[Ht
n];At
h,n∈ L(L(H;H′);H)andBt∈
L(L(H;H′);L(H;H))by
At
h,n(M) =/bracketleftbig
Ψ(Θt)†◦M/bracketrightbig/parenleftig
ϕxt
h,n/parenrightig
,Bt(M) = Ψ(Θt)†◦M.
Remark 4.5 (Hilbert-Schmidt operators) .BothAt
h,nandBtare Hilbert-Schmidt operators because the
rangesHandL(H;H)are of finite dimension. Note, in case H′is finite, we obtain
ϕFΘ(1,ω,x)= Ψ(Θ)†M⋆ϕx+ϵ(ω) = (ϕ†
x⊗Ψ(Θ)†)vec(M⋆) +ϵ(ω), (4.2)
where vecis the vectorization of matrix.
With these preparations in mind, our proposed information theoretic algorithm, which is an extension of
LC3to KSNR problem (estimating the true operator M⋆) is summarized in Algorithm 1.1This algorithm
assumes the following oracle.
Definition 4.1 (Optimal parameter oracle) .Define the oracle, OptDynamics , that computes the minimiza-
tion problem (3.2) for any K,X0,Λandctsatisfying Assumption 3.
4.2 Information theoretic regret bounds
Here, we present the regret bound analysis. To this end, we make the following assumptions.
Assumption 4. The operator Λsatisfies the following (modified) Hölder condition:
∃L∈R≥0,∃α∈(0,1],∀A∈L(H,H),∀Θ∈Π,
|Λ[A]−Λ[K(Θ)]|≤L·max/braceleftig
∥A−K(Θ)∥2,∥A−K(Θ)∥α/bracerightig
.
Further, we assume there exists a constant Λmax≥0such that, for any Θ∈Πand for any M∈Ball0
M,
/vextendsingle/vextendsingleΛ[Ψ(Θ)†◦M]/vextendsingle/vextendsingle≤Λmax.
Remark 4.6 (On Assumption 4) .This assumption does not preclude practical examples such as matrix
norms (because of the triangle inequality) and the spectral radius as described below. However, we note
that the spectrum cost in Section 3.3 used for top mode imitation, namely, Λ(A) =∥m1−m⋆
1∥1, does not
satisfy this (modified) Hölder condition in general; therefore this cost might not be used for KS-LC3.
For example, for spectral radius ρ, the following proposition holds using the result from (Song, 2002,
Corollary 2.3).
1See Appendix B for the definitions of the values in this algorithm.
12Published in Transactions on Machine Learning Research (6/2024)
Proposition 4.7. Assume there exists a constant Λmax≥0such that, for any Θ∈Πand for any M∈
Ball0
M,ρ(Ψ(Θ)†◦M)≤Λmax. Let the Jordan condition number of K(Θ)be the following:
κ:= sup
Θ∈Πinf
Q(Θ)/braceleftbig
∥Q(Θ)∥∥Q(Θ)−1∥:Q(Θ)−1K(Θ)Q(Θ) =J(Θ)/bracerightbig
,
whereJ(Θ)is a Jordan canonical form of K(Θ)transformed by a nonsingular matrix Q(Θ). Also, letm
be the order of the largest Jordan block. Then, if κ<∞, the cost Λ(A) =ρ(A)satisfies the Assumption 4
for
L:= (1 +κ)d2
ϕ(1 +/radicalbig
dϕ−1), α =1
m.
Note when K(Θ)is diagonalizable for all Θ,α= 1.
Assumption 5. For everyt, adversary chooses Nttrajectories such that {ϕxt
0,n}satisfies that the smallest
eigenvalue of/summationtextNt−1
n=0ϕxt
0,nϕ†
xt
0,nis bounded below by some constant C > 0. Also, there exists a constant
H∈Z>0such that for every t,/summationtextNt−1
n=0Ht
n≤H.
Remark 4.8 (On Assumption 5) .In practice, user may wait to end an episode until a sufficient number of
trajectories is collected in order for the assumption to hold. Intuitively, this condition ensures that fitting the
transitiondataoverthefeaturespaceisnotill-posed. Note, thisassumptiondoesnotprecludethenecessityof
exploration: If the sole purpose is just to fit the data for single parameter Θ, we may not need a well-designed
exploration strategy in this case; however, because the smallest eigenvalue of/summationtextNt−1
n=0/summationtextHt
n−1
h=0At
h,n†At
h,nis in
general not bounded below by a positive constant, we still need careful design of exploration.
We mention that this assumption plays a role in our regret analysis to simultaneously manage the cumulative
cost and the spectrum cost through the use of common confidence balls; note the former cares about each
single-step transition while the latter deals with the global dynamical properties represented as the Koopman
operator realized over a given space.
The direct use of this assumption is highlighted by Lemma F.2 in Appendix F (derived from our positive
operator norm bounding lemma ; see Lemma F.1) which is used to basically bound the norm of difference
between the true Koopman operator and the estimated one at each episode by the multiple of radiusof the
confidence ball.
We hope that this assumption can be relaxed by assuming the bounds on the norms of ϕx0,nandK(Θ)and
by using matrix Bernstein inequality under additive Gaussian noise assumption.
Lastly, the following assumption is the modified version of the one made in (Kakade et al., 2020).
Assumption 6. [Modified version of Assumption 2 in (Kakade et al., 2020)] Assume there exists a constant
Vmax>0such that, for every t,
sup
Θ∈ΠNt−1/summationdisplay
n=0EΩΘ

Ht
n−1/summationdisplay
h=0ct(xt
h,n)
2/vextendsingle/vextendsingle/vextendsingle/vextendsingleΘ,xt
0,n
≤Vmax.
Remark 4.9 (On Assumption 6) .This is a slight modification of Assumption 2 in (Kakade et al., 2020) to
adjust to our problem settings; this assumption does not state that the cost function is bounded over the
state space but the second moment of realizedcumulative cost is bounded, and the complexity depends on
this bound.
Theorem 4.10. Suppose Assumption 1 to 6 hold. Set λ=σ2
∥M⋆∥2. Then, there exists an absolute constant
C1such that, for all T, KS-LC3(Algorithm 1) using OptDynamics enjoys the following regret bound:
EKS−LC3[RegretT]≤C1(˜dT,1+˜dT,2)T1−α
2,
13Published in Transactions on Machine Learning Research (6/2024)
where
˜d2
T,1:= (1 +γT,B)/bracketleftbig
L2(1 +C−1)2˜β2,T+ (L2+ ΛmaxL)(1 +C−1)˜β1,T+ Λ2
max+L2/bracketrightbig
,
˜d2
T,2:=γT,AHVmax(γT,A+dϕ+ log(T) +H),
˜β1,T:=σ2(dϕ+ log(T) +γT,A),
˜β2,T:=σ4((dϕ+ log(T) +γT,A)2+γ2,T,A).
Here,γT,A,γ2,T,A, andγT,Bare the expected maximum information gains that scale (poly-)logarithmically
withTunder practical settings (see Appendix B for details).
Remark 4.11 (On the order) .We note that, when α= 1, we obtain the order of ˜O(√
T).
Remark 4.12 (On the adversary) .In our setting, the adversary only chooses the initial states, their time
horizons, and the immediate cost function. The trajectories themselves are generated by the learner’s
parameter Θ. Although the regret bound given above is valid if the assumptions hold regardless of the
adversary, a caveat here is the bound HandVmaxappearing in the regret bound can potentially depend on
the choice of the adversary.
The proof techniques include our positive operator norm bounding lemma (see Lemma F.1 in Appendix F),
which is another crucial operator theoretic lemma in this work in addition to Lemma 4.3.
4.3 Relations to the kernelized nonlinear regulator and to eigenstructure assignments
First, wementionthatourtheoreticalresultsapplysomeofthetechniquesdevelopedintheworkofKNR;and
in fact, additional theoretical arguments that are necessitated for KSNR framework highlight the essential
difference of our framework from the MDP counterpart.
As mentioned in Remark 4.1, for a given dynamics described by the system model studied in the KNR (i.e.,
the transition map from the current state and control to the next state is in a known RKHS), the existence
of a (finite-dimensional) space H0in Assumption 1 that can represent the given cumulative cost is in general
not guaranteed. As such, the system model considered in this section is no more general than that of the
KNR (although our spectrum cost formulation is indeed a generalization of that of the KNR). Now, we
consider the finite dimensional description (see (4.2)) for simplicity. The equation (4.2) can be rewritten by
ϕFΘ(1,ω,x)=/parenleftbig
I⊗vec(M⋆)⊤/parenrightbig
vec/bracketleftbigg/parenleftig
ϕ†
x⊗Ψ(Θ)†/parenrightig⊤/bracketrightbigg
+ϵ(ω),
and is a special case of the system model of the KNR.
We see that the model associated with our spectrum cost formulation reduces to the eigenstructure assign-
ment problem for the linear systems described by
xh+1=Axh+Buh+ϵ, A∈RdX×dX, B∈RdX×dU, ϵ∼N(0,σ2I),
whereu∈dUis a control input. In particular, considering the feedback policy in the form of uh=Kxh
whereK∈RdU×dX(orΘ =Kin this case), the system becomes xh+1= (A+BK)xh+ϵ. By lettingH0be
RdXwhere the canonical basis is taken and by properly designing Ψ(Θ)(see Appendix G), our system model
reduces to the linear case. The spectrum cost may be designed not only to constrain the eigenstructure but
also to balance with the cumulative single-step cost in our framework as well.
4.4 Simulated experiments
Linear case: as eigenstructure assignment problem First, to demonstrate the correctness of our the-
oretical algorithm, we consider a linear system case. In particular, the following simple system is considered:
xh+1=xh+ ∆t·uh,
14Published in Transactions on Machine Learning Research (6/2024)
Figure7: Linearsystemimitation(aseigenstructureassignment)byKS-LC3usingthespectrumcost Λ(A) =
∥(I+ 0.05K∗)−(A+BK)∥2
HS(whereAandBare extracted from A). Left: ground-truth linear system
position trajectory and a trajectory generated by the learned model with KS-LC3. We see both overlap
exactly, indicating that the algorithm learned the feedback matrix properly. Middle: The spectrum cost
curve evaluated approximately using the current trajectory data. Right: The estimated spectrum cost curve
obtained by using the current estimate ˆM.
where ∆t= 0.05. The target linear system is given by
xh+1= (I+ 0.05K∗)xh,
where
K∗:=/bracketleftbigg−2 4
−6−2/bracketrightbigg
.
The spectrum cost given by ∥(I+0.05K∗)−(A+BK)∥2
HSenforces that the parameter Θ(which is a feedback
matrixKin this case) generates the dynamical system that follows this target linear dynamics; where A
andBare matrices that are part of the Koopman operator to learn (see Section 4.3). Figure 7 plots the
result showing that the trajectories generated by the ground-truth target linear system and by the learned
model match almost exactly, and that the spectrum cost decreases along episodes. The spectrum cost curve
evaluated approximately using the current trajectory data is given in Figure 7 (Middle). Note the curve
is averaged across four random seeds and across episode window of size four. Additionally, the estimated
spectrum cost curve obtained by using the current estimate ˆMis plotted in Figure 8 (Right).
Cartpole problem with reduced policy space We again consider a Cartpole environment for KS-LC3.
To reduce the policy search space, we compose three pre-trained policies to balance the pole while 1) moving
the cart position pto−0.3and then to 0.3, 2) moving the cart with velocity v=−1.5, 3) moving cart with
velocityv= 1.5. We also extract the Koopman operator A⋆for the first policy. Then, we let Πto be the
space of linear combinations of the three policies, reducing the dimension of Θ. We let the first four features
ϕ1(x)toϕ4(x)bex1tox4, wherex= [x1,...,x 4]⊤is the state, and the rest be RFFs.
The single-step cost is given by 10−4(|v|−1.5)2plus the large penalty when the pole falls down (i.e.,
directing downward), and the spectrum cost is Λ(A) =∥A[1 : 4,:]−A⋆[1 : 4,:]∥2
HS+ 0.01/summationtextdϕ
i=1|λi(A)|,
whereA[1 : 4,:]∈R4×dϕis the first four rows of A; this imitates the first policy while also regularizing the
spectrum.
When CEM is used to (approximately) solve KSNR, the resulting cart position trajectories are given by
Figure 8 (Left). With the spectrum cost, the cumulative costs and the spectrum costs averaged across four
random seeds were 0.706±0.006and2.706±0.035. Without the spectrum cost, they were 0.428±0.045and
3.383±0.604.
We then use the Thompson sampling version of KS-LC3; the resulting cart position trajectories are given
by Figure 8 (Left) as well, and the spectrum cost curves are also shown. It is observed that the addition
of the spectrum cost favored the behavior corresponding to the target Koopman operator to oscillate while
balancing the pole, and achieved similar performance to the ground-truth model optimized with CEM.
15Published in Transactions on Machine Learning Research (6/2024)
Figure 8: Cartpole imitation by KS-LC3using the spectrum cost Λ(A) =∥A[1 : 4,:]−A⋆[1 : 4,:]∥2
HS+
0.01/summationtextdϕ
i=1|λi(A)|. Left: Cart position trajectories with/without the spectrum cost with CEM on a known
model versus the learned model with KS-LC3. Middle: The spectrum cost curve evaluated approximately
using the current trajectory data. Right: The estimated spectrum cost curve obtained by using the current
estimate ˆM.
5 Discussion and conclusion
This work proposed a novel paradigm of regulating (controlling) dynamical systems, which we refer to as
Koopman Spectrum Nonlinear Regulator, and presented an information theoretic algorithm that achieves a
sublinear regret bound under model assumptions. We showed that behaviors such as limit cycles of interest,
stable motion, and smooth movements are effectively realized within our framework, which is an effective
generalization of classical eigenstructure (pole) assignments. In terms of learning algorithms, we stress
that there is a significant potential of future work for inventing sophisticated and scalable methods that
elicit desired and interesting behaviors from dynamical systems. Our motivation of this work stems from
the fact that some preferences over dynamical systems cannot be encoded by cumulative single-step cost
based control/learning algorithms; we believe that this work enables a broader representation of dynamical
properties that can enable more intelligent and robust agent behaviors.
6 Limitations
First, when the dynamical systems of interests do not meet certain conditions, Koopman operators might
not be well defined over functional spaces that can be practically managed (e.g. RKHSs). Studying such
conditions is an on-going research topic, and we will need additional studies of misspecified cases where only
an approximately invariant Koopman space is employed with errors.
Second, to solve KSNR, one needs heuristic approximations when the (policy) space Πor the state space
Xis continuous. Even when the dynamical system model is known, getting better approximation results
would require certain amount of computations, and additional studies on the relation between the amount
of computations (or samples) and approximation errors would be required. Also, studying a better heuristic
algorithm that is well suited to our problem to robustly scale the methodology to more complicated domains
is indeed an interesting direction of research.
Third, KS-LC3requires several assumptions for tractable regret analysis. It is again a somewhat strong
to assume that one can find a Koopman invariant space H. Further, additive Gaussian noise assumption
may not be satisfied exactly in some practical applications, and robustness against the violations of the
assumptions is an important future work. However, we stress that we believe the analysis of provably correct
methods deepens our understandings of the problem. Eventually, we hope our framework will match the
maturity of the MDP counterpart, for example by studying gradient-based algorithms that scale better to
the more complicated domains and their theoretical analysis with a sample complexity guarantee.
16Published in Transactions on Machine Learning Research (6/2024)
Lastly, we note that our empirical experiment results are only conducted on simulators; to apply to real
robotics problems, we need additional studies, such as computation-accuracy trade-off, safety/robustness
issues, and simulation-to-real, if necessary. Also, balancing between cumulative cost and the Koopman
spectrum cost is necessary to avoid unexpected negative outcomes.
7 Broader impact statement
The work, along with other robotics/control literature, encourages the further automation of robots; which
would lead to loss of jobs that are currently existent, or may lead to developments of harmful military
robots. Further, when our proposed framework requires additional (computational or other) resources, it
would benefit entities with larger amount of such resources. Also, there is always a risk of robots misbehaving
under partially unknown environments or under adversarial attacks.
However, overall, we believe our work alone does not immediately carry a significant risk of harm. Moreover,
we stress that the use of the Koopman spectrum cost has no implications on “good” and “bad” about any
particular human behaviors.
Acknowledgments
We thank the constructive comments by anonymous reviewers for improving this work. Motoya Ohnishi
thanks Colin Summers for instructions on Lyceum. Kendall Lowrey and Motoya Ohnishi thank Emanuel
Todorov for valuable discussions and Roboti LLC for computational supports. This work of Motoya Ohnishi,
Isao Ishikawa, Masahiro Ikeda, and Yoshinobu Kawahara was supported by JST CREST Grant Number
JPMJCR1913, including AIP challenge program, Japan. Also, Motoya Ohnishi is supported in part by
Funai Overseas Fellowship. Sham Kakade acknowledges funding from the ONR award N00014-18-1-2247.
References
I. Abraham and T. D. Murphey. Active learning of dynamics for data-driven control using Koopman oper-
ators.IEEE Trans. Robotics , 35(5):1071–1083, 2019.
E. Akindji, J. Slipantschuk, O. F. Bandtlow, and W. Just. Convergence properties of dynamic mode decom-
position for analytic interval maps. arXiv preprint arXiv:2404.08512 , 2024.
A. N. Andry, E. Y. Shapiro, and J. C. Chung. Eigenstructure assignment for linear systems. IEEE Trans.
Aerospace and Electronic Systems , (5):711–729, 1983.
L. Arnold. Random dynamical systems . Springer, 1998.
Z. Beheshti and S. M. H. Shamsuddin. A review of population-based meta-heuristic algorithms. Int. j. adv.
soft comput. appl , 5(1):1–35, 2013.
J. Bezanson, A. Edelman, S. Karpinski, and V. B. Shah. Julia: A fresh approach to numerical computing.
SIAM review , 59(1):65–98, 2017.
R. Brault, M. Heinonen, and F. Buc. Random Fourier features for operator-valued kernels. In Proc. ACML ,
pp. 110–125, 2016.
G. Brockman, V. Cheung, L. Pettersson, J. Schneider, J. Schulman, J. Tang, and W. Zaremba. OpenAi
Gym.arXiv preprint arXiv:1606.01540 , 2016.
D. Burov, D. Giannakis, K. Manohar, and A. Stuart. Kernel analog forecasting: Multiscale test problems.
Multiscale Modeling & Simulation , 19(2):1011–1040, 2021.
M. J. Colbrook and A. Townsend. Rigorous data-driven computation of spectral properties of Koopman
operators for dynamical systems. Communications on Pure and Applied Mathematics , 77(1):221–283,
2024.
17Published in Transactions on Machine Learning Research (6/2024)
M. J. Colbrook, L. J. Ayton, and M. Szőke. Residual dynamic mode decomposition: robust and verified
Koopmanism. Journal of Fluid Mechanics , 955:A21, 2023.
N. Črnjarić-Žic, S. Maćešić, and I. Mezić. Koopman operator spectrum for random dynamical systems.
Journal of Nonlinear Science , pp. 1–50, 2019.
S. Curi, F. Berkenkamp, and A. Krause. Efficient model-based reinforcement learning through optimistic
policy search and planning. arXiv preprint arXiv:2006.08684 , 2020.
B. Eysenbach, A. Gupta, J. Ibarz, and S. Levine. Diversity is all you need: Learning skills without a reward
function. arXiv preprint arXiv:1802.06070 , 2018.
R. A. Freeman and J. A. Primbs. Control Lyapunov functions: New ideas from an old source. In IEEE
Proc. CDC , volume 4, pp. 3926–3931, 1996.
M. Ghil, M. D. Chekroun, and E. Simonnet. Climate dynamics and fluid mechanics: Natural variability and
related uncertainties. Physica D: Nonlinear Phenomena , 237(14-17):2111–2126, 2008.
L. Grafakos. Classical Fourier analysis , volume 2. Springer, 2008.
T. Haarnoja, A. Zhou, P. Abbeel, and S. Levine. Soft actor-critic: Off-policy maximum entropy deep
reinforcement learning with a stochastic actor. In Proc. ICML , pp. 1861–1870, 2018.
M. Hemati and H. Yao. Dynamic mode shaping for fluid flow control: New strategies for transient growth
suppression. In 8th AIAA Theoretical Fluid Mechanics Conference , pp. 3160, 2017.
J. Ibarz, J. Tan, C. Finn, M. Kalakrishnan, P. Pastor, and S. Levine. How to train your robot with deep
reinforcement learning: lessons we have learned. International Journal of Robotics Research , 40(4-5):
698–721, 2021.
A. Ijspeert, J. Nakanishi, and S. Schaal. Learning attractor landscapes for learning motor primitives.
Proc. NeurIPS , 15, 2002.
M. Ikeda, I. Ishikawa, and C. Schlosser. Koopman and Perron–Frobenius operators on reproducing kernel
Banach spaces. Chaos: An Interdisciplinary Journal of Nonlinear Science , 32(12), 2022.
I. Ishikawa. Bounded composition operators on functional quasi-Banach spaces and stability of dynamical
systems. Advances in Mathematics , 424:109048, 2023.
I. Ishikawa, K. Fujii, M. Ikeda, Y. Hashimoto, and Y. Kawahara. Metric on nonlinear dynamical systems
with Perron-Frobenius operators. In Proc. NeurIPS , pp. 2856–2866. 2018.
I. Ishikawa, Y. Hashimoto, M. Ikeda, and Y. Kawahara. Koopman operators with intrinsic observables in
rigged reproducing kernel Hilbert spaces. arXiv preprint arXiv:2403.02524 , 2024.
T. Iwata and Y. Kawahara. Neural dynamic mode decomposition for end-to-end modeling of nonlinear
dynamics. arXiv:2012.06191 , 2020.
N. Jiang, A. Krishnamurthy, A. Agarwal, J. Langford, and R. E. Schapire. Contextual decision processes
with low Bellman rank are PAC-learnable. In Proc. ICML , pp. 1704–1713, 2017.
H. Kadri, E. Duflos, P. Preux, S. Canu, A. Rakotomamonjy, and J. Audiffren. Operator-valued kernels for
learning from functional response data. Journal of Machine Learning Research , 17(20):1–54, 2016.
E.Kaiser, J.N.Kutz, andS.Brunton. Data-drivendiscoveryofKoopmaneigenfunctionsforcontrol. Machine
Learning: Science and Technology , 2021.
S. Kakade, A. Krishnamurthy, K. Lowrey, M. Ohnishi, and W. Sun. Information theoretic regret bounds for
online nonlinear control. Proc. NeurIPS , 2020.
Y. Kawahara. Dynamic mode decomposition with reproducing kernels for Koopman spectral analysis.
Proc. NeurIPS , 29:911–919, 2016.
18Published in Transactions on Machine Learning Research (6/2024)
J. Kober and J. Peters. Imitation and reinforcement learning. IEEE Robotics & Automation Magazine , 17
(2):55–62, 2010.
J. Kober, A. Wilhelm, E. Oztop, and J. Peters. Reinforcement learning to adjust parametrized motor
primitives to new situations. Autonomous Robots , 33:361–379, 2012.
J. Kober, J. A. Bagnell, and J. Peters. Reinforcement learning in robotics: A survey. International Journal
of Robotics Research , 32(11):1238–1274, 2013.
M. Kobilarov. Cross-entropy motion planning. International Journal of Robotics Research , 31(7):855–871,
2012.
B. O. Koopman. Hamiltonian systems and transformation in Hilbert space. Proc. National Academy of
Sciences of the United States of America , 17(5):315, 1931.
M. Korda and I. Mezić. Linear predictors for nonlinear dynamical systems: Koopman operator meets model
predictive control. Automatica , 93:149–160, 2018.
M. Korda and I. Mezić. Optimal construction of Koopman eigenfunctions for prediction and control. IEEE
Trans. Automatic Control , 65(12):5114–5129, 2020.
J. N. Kutz, S. L. Brunton, B. W. Brunton, and J. L. Proctor. Dynamic mode decomposition: Data-driven
modeling of complex systems . SIAM, 2016.
Y.Li, H.He, J.Wu, D.Katabi, andA.Torralba. LearningcompositionalKoopmanoperatorsformodel-based
control.arXiv preprint arXiv:1910.08264 , 2019.
G. Mamakoukas, I. Abraham, and T. D. Murphey. Learning stable models for prediction and control. IEEE
Trans. Robotics , 2023.
G. Manek and J Zico Kolter. Learning stable deep dynamics models. Proc. NeurIPS , 2019.
H. Mania, M. I. Jordan, and B. Recht. Active learning for nonlinear system identification with guarantees.
arXiv preprint arXiv:2006.10277 , 2020.
A. Mauroy and I. Mezić. Global stability analysis using the eigenfunctions of the Koopman operator. IEEE
Trans. Automatic Control , 61(11):3356–3369, 2016.
A. Mauroy, I. Mezić, and J. Moehlis. Isostables, isochrons, and Koopman spectrum for the action–angle
representation of stable fixed point dynamics. Physica D: Nonlinear Phenomena , 261:19–30, 2013.
A. Mauroy, Y. Susuki, and I. Mezić. The Koopman operator in systems and control . Springer, 2020.
I. Mezić. Spectral properties of dynamical systems, model reduction and decompositions. Nonlinear Dy-
namics, 41:309–325, 2005.
V. Mnih, K. Kavukcuoglu, D. Silver, A. Graves, I. Antonoglou, D. Wierstra, and M. Riedmiller. Playing
Atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602 , 2013.
V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves, M. Riedmiller,
A. K. Fidjeland, G. Ostrovski, et al. Human-level control through deep reinforcement learning. Nature,
518(7540):529–533, 2015.
H. Nakao. Phase reduction approach to synchronization of nonlinear oscillators. Contemporary Physics , 57
(2):188–214, 2017.
A. Y. Ng and S. J. Russell. Algorithms for inverse reinforcement learning. In Proc. ICML , volume 1, 2000.
J. Peters and S. Schaal. Reinforcement learning of motor skills with policy gradients. Neural networks , 21
(4):682–697, 2008.
19Published in Transactions on Machine Learning Research (6/2024)
R. Pintelon and J. Schoukens. System identification: a frequency domain approach . John Wiley & Sons,
2012.
A. Rahimi and B. Recht. Random features for large-scale kernel machines. Proc. NeurIPS , 2007.
A. Rajeswaran, K. Lowrey, E. Todorov, and S. Kakade. Towards generalization and simplicity in continuous
control.Proc. NeurIPS , 2017.
A. Sabanovic and K. Ohnishi. Motion control systems . John Wiley & Sons, 2011.
M. Simchowitz and D. Foster. Naive exploration is optimal for online LQR. In Proc. ICML , pp. 8937–8948,
2020.
Y. Song. A note on the variation of the spectrum of an arbitrary matrix. Linear algebra and its applications ,
342(1-3):41–46, 2002.
S. H. Strogatz. Nonlinear dynamics and chaos with student solutions manual: With applications to physics,
biology, chemistry, and engineering . CRC press, 2018.
F. Stulp and O. Sigaud. Robot skill learning: From reinforcement learning to evolution strategies. Paladyn,
Journal of Behavioral Robotics , 4(1):49–61, 2013.
F. Stulp, L. Herlant, A. Hoarau, and G. Raiola. Simultaneous on-line discovery and improvement of robotic
skill options. In IEEE/RSJ IROS , pp. 1408–1413, 2014.
C. Summers, K. Lowrey, A. Rajeswaran, S. Srinivasa, and E. Todorov. Lyceum: An efficient and scalable
ecosystem for robot learning. In Learning for Dynamics and Control , pp. 793–803. PMLR, 2020.
W. Sun, N. Jiang, A. Krishnamurthy, A. Agarwal, and J. Langford. Model-based RL in contextual decision
processes: PAC bounds and exponential improvements over model-free approaches. In Proc. COLT , pp.
2898–2933, 2019.
N. Takeishi and Y. Kawahara. Learning dynamics models with stable invariant sets. In Proc. AAAI , 2021.
Y. Tassa, Y. Doron, A. Muldal, T. Erez, Y. Li, D. L. Casas, D. Budden, A. Abdolmaleki, J. Merel,
A. Lefrancq, et al. DeepMind Control Suite. arXiv preprint arXiv:1801.00690 , 2018.
E. Todorov, T. Erez, and Y. Tassa. MuJoCo: A physics engine for model-based control. In IEEE/RSJ
IROS, pp. 5026–5033, 2012.
G. Williams, A. Aldrich, and E. A. Theodorou. Model predictive path integral control: From theory to
parallel computation. Journal of Guidance, Control, and Dynamics , 40(2):344–357, 2017.
A. T. Winfree. The Geometry of Biological Time . Springer, 2001.
20Published in Transactions on Machine Learning Research (6/2024)
A Function-valued RKHS
Function-valued reproducing kernel Hilbert spaces (RKHSs) are defined below.
Definition A.1 (Kadri et al. (2016)) .A Hilbert space (HK,⟨·,·⟩HK)of functions from Πto a Hilbert space
(H,⟨·,·⟩H)is called a reproducing kernel Hilbert space if there is a nonnegative L(H;H)-valued kernel Kon
Π×Πsuch that:
1.Θ∝⇕⊣√∫⊔≀→K(Θ′,Θ)ϕbelongs toHKfor all Θ′∈Πandϕ∈H,
2. for every G∈HK,Θ∈Πandϕ∈H,⟨G,K(Θ,·)ϕ⟩HK=⟨G(Θ),ϕ⟩H.
For function-valued RKHSs, the following proposition holds.
Proposition A.1 (Feature map (Brault et al., 2016)) .LetH′be a Hilbert space and Ψ : Π→L(H;H′).
Then the operator W:H′→HΠdefined by [Wψ](Θ) := Ψ(Θ)†ψ,∀ψ∈H′and∀Θ∈Π, is a partial
isometry fromH′onto the reproducing kernel Hilbert space HKwith a reproducing kernel K(Θ2,Θ1) =
Ψ(Θ 2)†Ψ(Θ 1),∀Θ1,Θ2∈Π.
Remark A.2 (Decomposable kernel) .In practice, one can use decomposable kernel (Brault et al., 2016);
when the kernel Kis given by K(Θ1,Θ2) =k(Θ1,Θ2)Afor some scalar-valued kernel k(Θ1,Θ2)and for
positive semi-definite operator A∈L(H,H), the kernel Kis called decomposable kernel. For an RKHS of a
decomposable kernel K, (4.1) becomes
K(Θ)ϕ= (ζ(Θ)†⊗B)ψ,
whereζ: Π→H′′is known (H′′is some Hilbert space), and A=BB†. Further, to use RFFs, one considers
a shift-invariant kernel k(Θ1,Θ2).
B Some definitions of the values
The valueJΘ(Xt
0;M;ct)in Algorithm 1 is defined by
JΘ(Xt
0;M;ct) :=Nt−1/summationdisplay
n=0EΩΘ
Ht
n−1/summationdisplay
h=0ct(xt
h,n)/vextendsingle/vextendsingle/vextendsingleΘ,M,xt
0,n
,
where the expectation is taken over the trajectory following ϕxh+1= [Ψ(Θ)†◦M]ϕxh+ϵ(ω). Also the
confidence ball at tis given by
Ballt
M:=/braceleftbigg
M/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextenddouble/vextenddouble/vextenddouble(Σt
A)1
2/parenleftig
M−Mt/parenrightig/vextenddouble/vextenddouble/vextenddouble2
≤βt
M/bracerightbigg
∩Ball0
M,Σt
A:=λI+t−1/summationdisplay
τ=0Nτ−1/summationdisplay
n=0Hτ
n−1/summationdisplay
h=0Aτ
h,n†Aτ
h,n,
andΣ0
A:=λI, whereβt
M:= 20σ2/parenleftig
dϕ+ log/parenleftig
tdet(Σt
A)
det(Σ0
A)/parenrightig/parenrightig
and
Mt:= arg min
M
t−1/summationdisplay
τ=0Nτ−1/summationdisplay
n=0Hτ
n−1/summationdisplay
h=0/vextenddouble/vextenddouble/vextenddoubleϕxτ
h+1,n−Aτ
h,n(M)/vextenddouble/vextenddouble/vextenddouble2
Rdϕ+λ∥M∥2
HS
.
Similar to the work (Kakade et al., 2020), we define the expected maximum information gains as:
γT,A(λ) := 2 max
AEA/bracketleftigg
log/parenleftigg
det/parenleftbig
ΣT
A/parenrightbig
det/parenleftbig
Σ0
A)/parenrightbig/parenrightigg/bracketrightigg
.
21Published in Transactions on Machine Learning Research (6/2024)
Here, detis a properly defined functional determinant of a bounded linear operator. Further,
γ2,T,A(λ) := 2 max
AEA
/parenleftigg
log/parenleftigg
det/parenleftbig
ΣT
A/parenrightbig
det/parenleftbig
Σ0
A)/parenrightbig/parenrightigg/parenrightigg2
.
Also, we define
Σt
B:=λI+t−1/summationdisplay
τ=0Bτ†Bτ,Σ0
B:=λI, γT,B(λ) := 2 max
AEA/bracketleftigg
log/parenleftigg
det/parenleftbig
ΣT
B/parenrightbig
det/parenleftbig
Σ0
B)/parenrightbig/parenrightigg/bracketrightigg
.
Lemma B.1. Assume that Ψ(Θ)∈RdΨ×dϕand that∥Bt∥HS≤BB,∥At
h,n∥HS≤BAfor allt∈[T],
n∈[Nt], andh∈[Ht
n]and someBB≥0andBA≥0. Then,γT,A(λ) =O(dϕdΨlog(1 +THB2
A/λ)), and
γT,B(λ) =O(dϕdΨlog(1 +TB2
B/λ)).
Proof.ForγT,A(λ), from the definition of Hilbert-Schmidt norm, we have
tr
T−1/summationdisplay
t=0Nt−1/summationdisplay
n=0Ht
n−1/summationdisplay
h=0At
h,n†At
h,n
≤THB2
A,
and the result follows from (Kakade et al., 2020, Lemma C.5). The similar argument holds for γT,B(λ)
too.
C Formal argument on the limitation of cumulative costs
In this section, we present a formal argument on the limitation of cumulative costs, mentioned in Section
3.2. To this end, we give the following proposition.
Proposition C.1. LetX=R. Consider the set Sfof dynamics converging to some point in X. Then, for
any choice of υ∈(0,1], set-valued map S:X→ 2X\∅, and cost cof the form
c(x,y) =/braceleftigg
0 (y∈S(x)),
1 (otherwise) ,
we have{F}̸ =Sf. Here,{F}are given by
/intersectiondisplay
x0∈X/braceleftigg
arg min
F:r.d.s.EΩ∞/summationdisplay
h=0υhc(F(h,ω,x 0),F(h+ 1,ω,x 0))/bracerightigg
.
Proof.Assume there exist υ, a set-valued map S, and csuch that the set {FΘ}=Sf. Then, becauseSfis
strictly smaller than the set of arbitrary dynamics, it must be the case that ∃x0∈X,∃y0∈X, y0/∈S(x0).
However, the dynamics f⋆, wheref⋆(x0) =y0andf⋆(x) =x,∀x∈X\{x0}, is an element of Sf. Therefore,
the proposition is proved.
Remark C.2 (Interpretation of Proposition C.1) .Proposition C.1 intuitively says that the set of “stable
dynamics” cannot be determined by specifying every single transition. This is because, the dynamics that
does not follow any pre-specified transition can always be “stable”.
D Proof of Lemma 4.3
It is easy to see that one can define a map M0so that it satisfies M0(ϕ) =ψsuch that Assumption 2 holds.
Define
M⋆:=PM 0,
22Published in Transactions on Machine Learning Research (6/2024)
wherePis the orthogonal projection operator onto the sum space of the orthogonal complement of the null
space of Ψ(Θ)†for all Θ∈Π, namely,
/summationdisplay
Θ∈Πker(Ψ(Θ)†)⊥.
Also, define PΘby the orthogonal projection onto
ker(Ψ(Θ)†)⊥.
Then, for any Θ∈Π, we obtain
PΘM0= Ψ(Θ)†+K(Θ),
whereA+is the pseudoinverse of the operator A, andPΘM0is linear. Let a,b∈Randϕ1,ϕ2∈H, and
define ˜ψby
˜ψ:=PM 0(aϕ1+bϕ2)−aPM 0(ϕ1)−bPM 0(ϕ2).
BecausePΘPM 0=PΘM0, it follows that PΘ˜ψ= 0for all Θ∈Π, which implies
˜ψ∈/intersectiondisplay
Θ∈ΠkerPΘ=/intersectiondisplay
Θ∈Πker(Ψ(Θ)†).
On the other hand, we have
˜ψ∈/summationdisplay
Θ∈Πker(Ψ(Θ)†)⊥.
Therefore, it follows that ˜ψ= 0, which proves that M⋆is linear.
E Proof of Proposition 4.7
FixΘ∈Πand suppose that the eigenvalues λi(i∈I:={1,2,...,dϕ}) ofK(Θ)are in descending order
according to their absolute values, i.e., |λ1|≥|λ2|≥...≥|λdϕ|. Given A∈L(H;H), suppose also that the
eigenvalues µi(i∈I) ofAare in descending order according to their absolute values. Let
κΘ:= inf
Q(Θ)/braceleftbig
∥Q(Θ)∥∥Q(Θ)−1∥:Q(Θ)−1K(Θ)Q(Θ) =J(Θ)/bracerightbig
,
whereJ(Θ)is a Jordan canonical form of K(Θ)transformed by a nonsingular matrix Q(Θ). Also, let
κΘ:= max
m∈{1,...,dϕ}/braceleftig
κ1
m
Θ/bracerightig
.
Then, by (Song, 2002, Corollary 2.3), we have
||µi|−|λi||≤|µi−λi|≤/summationdisplay
i|µi−λi|≤/summationdisplay
i|µπ(i)−λi|
≤dϕ/radicalbig
dϕ(1 +/radicalbig
dϕ−p) max/braceleftig
κΘ/radicalbig
dϕ∥A−K(Θ)∥,(κΘ/radicalbig
dϕ)1
m∥A−K(Θ)∥1
m/bracerightig
,
for anyi∈Iand for some permutation π, wherep∈{1,2,...,dϕ}is the number of the Jordan block of
Q(Θ)−1K(Θ)Q(Θ)andm∈{1,2,...,dϕ}is the order of the largest Jordan block. Because/radicalbig
dϕ−p≤/radicalbig
dϕ−1for anyp∈{1,2,...,dϕ}, and because
max
m∈{1,...,dϕ}/bracketleftig/radicalbig
dϕ·max{/radicalbig
dϕ,(/radicalbig
dϕ)1
m}·max/braceleftig
κΘ,κ1
m
Θ/bracerightig/bracketrightig
≤dϕκΘ,
it follows that
|ρ(A)−ρ(K(Θ))|=||µ1|−|λ1||
≤κΘd2
ϕ(1 +/radicalbig
dϕ−1) max/braceleftig
∥A−K(Θ)∥,∥A−K(Θ)∥1
m/bracerightig
.
SinceκΘ<1 +κ, simple computations complete the proof.
23Published in Transactions on Machine Learning Research (6/2024)
F Regret analysis
Throughout this section, suppose Assumptions 1 to 6 hold. Note that Assumption 3 is required for
OptDynamics . We first give the positive operator norm bounding lemma followed by another lemma based
on it.
Lemma F.1 (Positive operator norm bounding lemma) .LetHbe a Hilbert space and Ai, Bi∈L(H;H) (i∈
{1,2,...,n}). Assume, for all i∈{1,2,...,n}, thatAiis positive definite. Also, assume B1is positive
definite, and for all i∈{2,3,...,n},Biis positive semi-definite. Then,
/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/parenleftigg/summationdisplay
iB1
2
iAiB1
2
i/parenrightigg−1
2/parenleftigg/summationdisplay
iBi/parenrightigg/parenleftigg/summationdisplay
iB1
2
iAiB1
2
i/parenrightigg−1
2/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble≤max
i/vextenddouble/vextenddoubleA−1
i/vextenddouble/vextenddouble.
IfAiBi=BiAifor alli∈{1,2,...,n}, then
/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/parenleftigg/summationdisplay
iAiBi/parenrightigg−1
2/parenleftigg/summationdisplay
iBi/parenrightigg/parenleftigg/summationdisplay
iAiBi/parenrightigg−1
2/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble≤max
i/vextenddouble/vextenddoubleA−1
i/vextenddouble/vextenddouble.
Proof.Letc:= maxi/vextenddouble/vextenddoubleA−1
i/vextenddouble/vextenddouble. Then, we have, for all i,
I⪯/vextenddouble/vextenddoubleA−1
i/vextenddouble/vextenddoubleAi⪯cAi,
from which it follows that
Bi⪯cB1
2
iAiB1
2
i.
Therefore, we obtain
/summationdisplay
iBi⪯c/summationdisplay
iB1
2
iAiB1
2
i.
From the assumptions,/parenleftig/summationtext
iB1
2
iAiB1
2
i/parenrightig−1
exists and
/parenleftigg/summationdisplay
iB1
2
iAiB1
2
i/parenrightigg−1
2/parenleftigg/summationdisplay
iBi/parenrightigg/parenleftigg/summationdisplay
iB1
2
iAiB1
2
i/parenrightigg−1
2
⪯cI,
from which we obtain
/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/parenleftigg/summationdisplay
iB1
2
iAiB1
2
i/parenrightigg−1
2/parenleftigg/summationdisplay
iBi/parenrightigg/parenleftigg/summationdisplay
iB1
2
iAiB1
2
i/parenrightigg−1
2/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble≤c.
The second claim follows immediately.
Lemma F.2. Suppose Assumptions 1, 2, and 5 hold. Then, it follows that, for all t∈[T],
/vextenddouble/vextenddouble/vextenddouble(Σt
B)1
2/parenleftig
M−Mt/parenrightig/vextenddouble/vextenddouble/vextenddouble2
≤(1 +C−1)/vextenddouble/vextenddouble/vextenddouble(Σt
A)1
2/parenleftig
M−Mt/parenrightig/vextenddouble/vextenddouble/vextenddouble2
.
Proof.Under Assumptions 1 and 2, define Ct∈L(L(H;H′);L(H;H′))by
Ct(M) =M◦
Nt−1/summationdisplay
n=0Ht
n−1/summationdisplay
h=0ϕxt
h,nϕ†
xt
h,n
.
24Published in Transactions on Machine Learning Research (6/2024)
Also, define Xt:=/summationtextNt−1
n=0/summationtextHt
n−1
h=0At
h,n†At
h,nandYt:=Bt†Bt. We have CtYt=YtCt=Xt(and thus
XtYt=YtXt), and
Σt
A=λI+t−1/summationdisplay
τ=0Xτ, Σt
B=λI+t−1/summationdisplay
τ=0Yτ.
From Assumption 5, we obtain, for all t∈[T],(Ct)−1exists and
∥(Ct)−1∥≤/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble
Nt−1/summationdisplay
n=0Ht
n−1/summationdisplay
h=0ϕxt
h,nϕ†
xt
h,n
−1/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble≤/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble
Nt−1/summationdisplay
n=0ϕxt
0,nϕ†
xt
0,n
−1/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble≤C−1.
Therefore, using Lemma F.1 by substituting IandCtoA,λIandYtoB, it follows that
/vextenddouble/vextenddouble/vextenddouble(Σt
A)−1
2Σt
B(Σt
A)−1
2/vextenddouble/vextenddouble/vextenddouble≤max/braceleftbigg
1,max
τ∈[t]{∥(Cτ)−1∥}/bracerightbigg
≤1 +C−1,
and
/vextenddouble/vextenddouble/vextenddouble(Σt
B)1
2/parenleftig
M−Mt/parenrightig/vextenddouble/vextenddouble/vextenddouble2
=/vextenddouble/vextenddouble/vextenddouble(Σt
B)1
2(Σt
A)−1
2(Σt
A)1
2/parenleftig
M−Mt/parenrightig/vextenddouble/vextenddouble/vextenddouble2
≤/vextenddouble/vextenddouble/vextenddouble(Σt
A)1
2/parenleftig
M−Mt/parenrightig/vextenddouble/vextenddouble/vextenddouble2/vextenddouble/vextenddouble/vextenddouble(Σt
A)−1
2(Σt
B)1
2/vextenddouble/vextenddouble/vextenddouble2
=/vextenddouble/vextenddouble/vextenddouble(Σt
A)−1
2Σt
B(Σt
A)−1
2/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble(Σt
A)1
2/parenleftig
M−Mt/parenrightig/vextenddouble/vextenddouble/vextenddouble2
≤(1 +C−1)/vextenddouble/vextenddouble/vextenddouble(Σt
A)1
2/parenleftig
M−Mt/parenrightig/vextenddouble/vextenddouble/vextenddouble2
.
Here, the second equality used ∥A∥2=∥A A†∥.
Now, letEtbe the event M⋆∈Ballt
M. Assume/intersectiontextT−1
t=0Et. Then, using Assumption 4,
/parenleftig
Λ[K(Θt)] +JΘt(Xt
0;ct)/parenrightig
−/parenleftig
Λ[K(Θ⋆t)] +JΘ⋆t(Xt
0;ct)/parenrightig
=/parenleftbig
Λ[K(Θt)]−Λ[K(Θ⋆t)]/parenrightbig
+/parenleftig
JΘt(Xt
0;ct)−JΘ⋆t(Xt
0;ct)/parenrightig
≤/parenleftig
Λ[K(Θt)]−Λ[Bt◦ˆMt]/parenrightig
+/parenleftig
JΘt(Xt
0;ct)−JΘt/parenleftig
Xt
0;ˆMt;ct/parenrightig/parenrightig
≤/parenleftig/vextendsingle/vextendsingle/vextendsingleΛ[K(Θt)]−Λ[Bt◦ˆMt]/vextendsingle/vextendsingle/vextendsingle/parenrightig
+/parenleftig
JΘt(Xt
0;ct)−JΘt/parenleftig
Xt
0;ˆMt;ct/parenrightig/parenrightig
≤min/braceleftbigg
L·max/braceleftbigg/vextenddouble/vextenddouble/vextenddoubleBt/parenleftig
M⋆−ˆMt/parenrightig/vextenddouble/vextenddouble/vextenddouble2
,/vextenddouble/vextenddouble/vextenddoubleBt/parenleftig
M⋆−ˆMt/parenrightig/vextenddouble/vextenddouble/vextenddoubleα/bracerightbigg
,2Λmax/bracerightbigg
/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
term 1+/parenleftig
JΘt(Xt
0;ct)−JΘt/parenleftig
Xt
0;ˆMt;ct/parenrightig/parenrightig
/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
term 2.
(F.1)
Here, the first inequality follows because we assumed Etand because the algorithm selects ˆMtandΘtsuch
that
Λ[Bt◦ˆMt] +JΘt/parenleftig
Xt
0;ˆMt;ct/parenrightig
≤Λ[Bt◦M] +JΘ/parenleftbig
Xt
0;M;ct/parenrightbig
for anyM∈Ballt
Mand for any Θ∈Π. The third inequality follows from Assumption 4.
Using Lemma F.2, we have
/vextenddouble/vextenddouble/vextenddoubleBt/parenleftig
M⋆−ˆMt/parenrightig/vextenddouble/vextenddouble/vextenddouble≤/vextenddouble/vextenddouble/vextenddouble(Σt
B)1
2/parenleftig
M⋆−ˆMt/parenrightig/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoubleBt(Σt
B)−1
2/vextenddouble/vextenddouble/vextenddouble
≤/radicalbig
(1 +C−1)/vextenddouble/vextenddouble/vextenddouble(Σt
A)1
2/parenleftig
M⋆−ˆMt/parenrightig/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoubleBt(Σt
B)−1
2/vextenddouble/vextenddouble/vextenddouble
≤/radicalbig
(1 +C−1)/parenleftig/vextenddouble/vextenddouble/vextenddouble(Σt
A)1
2/parenleftig
M⋆−Mt/parenrightig/vextenddouble/vextenddouble/vextenddouble+/vextenddouble/vextenddouble/vextenddouble(Σt
A)1
2/parenleftig
Mt−ˆMt/parenrightig/vextenddouble/vextenddouble/vextenddouble/parenrightig/vextenddouble/vextenddouble/vextenddoubleBt(Σt
B)−1
2/vextenddouble/vextenddouble/vextenddouble
≤2/radicalig
(1 +C−1)βt
M/vextenddouble/vextenddouble/vextenddoubleBt(Σt
B)−1
2/vextenddouble/vextenddouble/vextenddouble (∵Et).
25Published in Transactions on Machine Learning Research (6/2024)
Therefore, ifEt, it follows that
term 1≤min/braceleftbigg
L/braceleftbig
4(1 +C−1)βt
M+ 1/bracerightbig
max/braceleftbigg/vextenddouble/vextenddouble/vextenddoubleBt(Σt
B)−1
2/vextenddouble/vextenddouble/vextenddouble2
,/vextenddouble/vextenddouble/vextenddoubleBt(Σt
B)−1
2/vextenddouble/vextenddouble/vextenddoubleα/bracerightbigg
,2Λmax/bracerightbigg
.(F.2)
Then, we use the following lemma which is an extension of (Kakade et al., 2020, Lemman B.6) to our Hölder
condition.
Lemma F.3. For any sequence of Btand for any α∈(0,1], we have
T−1/summationdisplay
t=0min/braceleftbigg/vextenddouble/vextenddouble/vextenddoubleBt(Σt
B)−1
2/vextenddouble/vextenddouble/vextenddouble2α
,1/bracerightbigg
≤2T1−α/bracketleftigg
1 + log/parenleftigg
det/parenleftbig
ΣT
B/parenrightbig
det/parenleftbig
Σ0
B/parenrightbig/parenrightigg/bracketrightigg
.
Proof.Usingx≤2 log(1 +x)forx∈[0,1],
T−1/summationdisplay
t=0min/braceleftbigg/vextenddouble/vextenddouble/vextenddoubleBt(Σt
B)−1
2/vextenddouble/vextenddouble/vextenddouble2α
,1/bracerightbigg
≤T−1/summationdisplay
t=0min/braceleftbigg/vextenddouble/vextenddouble/vextenddoubleBt(Σt
B)−1
2/vextenddouble/vextenddouble/vextenddouble2α
HS,1/bracerightbigg
(∵∥A∥≤∥A∥HS)
=T−1/summationdisplay
t=0/parenleftbigg
min/braceleftbigg/vextenddouble/vextenddouble/vextenddoubleBt(Σt
B)−1
2/vextenddouble/vextenddouble/vextenddouble2
HS,1/bracerightbigg/parenrightbiggα
≤T−1/summationdisplay
t=0/bracketleftbigg
2 log/parenleftbigg
1 +/vextenddouble/vextenddouble/vextenddoubleBt(Σt
B)−1
2/vextenddouble/vextenddouble/vextenddouble2
HS/parenrightbigg/bracketrightbiggα
=T−1/summationdisplay
t=0/bracketleftig
2 log/parenleftig
1 + tr/braceleftig
(Σt
B)−1
2Bt†Bt(Σt
B)−1
2/bracerightig/parenrightig/bracketrightigα
≤2αT−1/summationdisplay
t=0/bracketleftig
log det/parenleftig
I+ (Σt
B)−1
2Bt†Bt(Σt
B)−1
2/parenrightig/bracketrightigα
≤2αT1−α/bracketleftiggT−1/summationdisplay
t=0log det/parenleftig
I+ (Σt
B)−1
2Bt†Bt(Σt
B)−1
2/parenrightig/bracketrightiggα
≤2αT1−α/bracketleftiggT−1/summationdisplay
t=0/parenleftbig
log det/parenleftbig
Σt+1
B/parenrightbig
−log det/parenleftbig
Σt
B/parenrightbig/parenrightbig/bracketrightiggα
≤2T1−α/bracketleftigg
log/parenleftigg
det/parenleftbig
ΣT
B/parenrightbig
det/parenleftbig
Σ0
B/parenrightbig/parenrightigg/bracketrightiggα
≤2T1−α/parenleftigg
1 + log/parenleftigg
det/parenleftbig
ΣT
B/parenrightbig
det/parenleftbig
Σ0
B/parenrightbig/parenrightigg/parenrightigg
.
Here, the fifth inequality follows from (Grafakos, 2008, Exercise 1.1.4).
Corollary F.4. For any sequence of Btand for any α∈(0,1], we have
T−1/summationdisplay
t=0min/braceleftbigg
max/braceleftbigg/vextenddouble/vextenddouble/vextenddoubleBt(Σt
B)−1
2/vextenddouble/vextenddouble/vextenddouble4
,/vextenddouble/vextenddouble/vextenddoubleBt(Σt
B)−1
2/vextenddouble/vextenddouble/vextenddouble2α/bracerightbigg
,1/bracerightbigg
≤2T1−α/bracketleftigg
1 + log/parenleftigg
det/parenleftbig
ΣT
B/parenrightbig
det/parenleftbig
Σ0
B/parenrightbig/parenrightigg/bracketrightigg
.
26Published in Transactions on Machine Learning Research (6/2024)
From (F.2) and Corollary F.4, we obtain
E/bracketleftiggT−1/summationdisplay
t=0term 1/vextendsingle/vextendsingle/vextendsingle/vextendsingleT−1/intersectiondisplay
t=0Et/bracketrightigg
≤E/bracketleftiggT−1/summationdisplay
t=0min/braceleftbigg
L/braceleftbig
4(1 +C−1)βt
M+ 1/bracerightbig
max/braceleftbigg/vextenddouble/vextenddouble/vextenddoubleBt(Σt
B)−1
2/vextenddouble/vextenddouble/vextenddouble2
,/vextenddouble/vextenddouble/vextenddoubleBt(Σt
B)−1
2/vextenddouble/vextenddouble/vextenddoubleα/bracerightbigg
,2Λmax/bracerightbigg/bracketrightigg
≤E/bracketleftiggT−1/summationdisplay
t=0/braceleftbig
L/braceleftbig
4(1 +C−1)βt
M+ 1/bracerightbig
+ 2Λ max/bracerightbig
min/braceleftbigg
max/braceleftbigg/vextenddouble/vextenddouble/vextenddoubleBt(Σt
B)−1
2/vextenddouble/vextenddouble/vextenddouble2
,/vextenddouble/vextenddouble/vextenddoubleBt(Σt
B)−1
2/vextenddouble/vextenddouble/vextenddoubleα/bracerightbigg
,1/bracerightbigg/bracketrightigg
≤T−1/summationdisplay
t=0/radicalig
E[ [L{4(1 +C−1)βt
M+ 1}+ 2Λ max]2]/radicaligg
E/bracketleftbigg
min/braceleftbigg
max/braceleftbigg/vextenddouble/vextenddouble/vextenddoubleBt(Σt
B)−1
2/vextenddouble/vextenddouble/vextenddouble4
,/vextenddouble/vextenddouble/vextenddoubleBt(Σt
B)−1
2/vextenddouble/vextenddouble/vextenddouble2α/bracerightbigg
,1/bracerightbigg/bracketrightbigg
≤/radicaltp/radicalvertex/radicalvertex/radicalbtT/summationdisplay
t=0E[ [L{4(1 +C−1)βt
M+ 1}+ 2Λ max]2]/radicaltp/radicalvertex/radicalvertex/radicalbtE/bracketleftiggT−1/summationdisplay
t=0min/braceleftbigg
max/braceleftbigg/vextenddouble/vextenddouble/vextenddoubleBt(Σt
B)−1
2/vextenddouble/vextenddouble/vextenddouble4
,/vextenddouble/vextenddouble/vextenddoubleBt(Σt
B)−1
2/vextenddouble/vextenddouble/vextenddouble2α/bracerightbigg
,1/bracerightbigg/bracketrightigg
≤√
T·value/radicalig
T1−α(2 +γT,B(λ)). (F.3)
Here, the first inequality is due to (F.2); the third inequality uses E[ab]≤/radicalbig
E[a2]E[b2]; the forth inequality
uses the Cauchy-Schwartz inequality; the last is from Corollary F.4. Also,
value := 16L2(1 +C−1)2E[(βT
M)2] + (8L2+ 16Λ maxL)(1 +C−1)E[βT
M] + 4Λ maxL+L2+ 4Λ2
max
≤C′/braceleftbig
L2(1 +C−1)2E[(βT
M)2] + (L2+ ΛmaxL)(1 +C−1)E[βT
M] + Λ2
max+L2/bracerightbig
,
for some constant C′.
Next, we turn to bound the latter term term 2of (F.1); our analysis is based on that of (Kakade et al.,
2020). Simple calculations show that
M⋆−Mt=λ(Σt
A)−1M⋆−(Σt
A)−1t−1/summationdisplay
τ=0Nτ−1/summationdisplay
n=0Hτ
n−1/summationdisplay
h=0Aτ
h,n†ϵτ
h,n,
whereϵτ
h,nis the sampled noise at τ-th episode, h-th timestep, and n-th trajectory. Now, by introducing
a Hilbert space containing an operator of L(L(H;H′);R), which is a Hilbert-Schmidt operator, because of
Assumption 1, we can apply Lemma C.4 in (Kakade et al., 2020) to our problem too. Therefore, with
probability at least 1−δt, it holds that
/vextenddouble/vextenddouble/vextenddouble(Σt
A)1
2/parenleftig
M−Mt/parenrightig/vextenddouble/vextenddouble/vextenddouble2
≤λ∥M⋆∥2+σ2(8dϕlog(5) + 8 log(det(Σt
A) det(Σ0
A)−1)/δt),
and properly choosing δtleads toβt
Mdefined in Section B, and we obtain the result
Pr/parenleftiggT−1/uniondisplay
t=0Et/parenrightigg
≤1
2.
In our algorithm, transition data are chosen from any initial states and the horizon lengths vary; however,
slight modification of the analysis of LC3will give the following lemma.
Lemma F.5 (Modified version of Theorem 3.2 in (Kakade et al., 2020)) .Suppose Assumptions 1, 2, 3, 5,
and 6 hold. Then, the term term 2is bounded by
E/bracketleftiggT−1/summationdisplay
t=0term 2/vextendsingle/vextendsingle/vextendsingle/vextendsingleT−1/intersectiondisplay
t=0Et/bracketrightigg
≤/radicalbig
HVmax/radicalig
64T(dϕ+ log(T) +γT,A(λ) +H)/radicalig
γT,A(λ).
27Published in Transactions on Machine Learning Research (6/2024)
Combining all of the above results, we prove Theorem 4.10:
Proof of Theorem 4.10. Using (F.3) (which requires Assumptions 1, 2, 4, and 5), Lemma F.5 (which requires
Assumptions 1, 2, 3, 5, and 6), and Pr/parenleftig/uniontextT−1
t=0Et/parenrightig
≤1
2, it follows that
EKS−LC3[RegretT]
≤/radicalig
T/braceleftbig
C′/braceleftbig
L2(1 +C−1)2E[(βT
M)2] + (L2+ ΛmaxL)(1 +C−1)E[βT
M] + Λ2max+L2/bracerightbig/bracerightbig/radicalig
T1−α(2 +γT,B(λ))
+/radicalbig
HVmax/radicalig
64T(dϕ+ log(T) +γT,A(λ) +H)/radicalig
γT,A(λ)
+1
2·(Λmax+/radicalbig
Vmax)
≤C1T1−α
2(˜dT,1+˜dT,2),
for some absolute constant C1. Therefore, the theorem is proved.
G Reduction to eigenstructure assignment problem for linear systems
To see how our system model studied in (4.2) reduces to linear system case, take RdXasH0with the
canonical basis (i.e., ϕx=x) and let
Φ(Θ) = [IdX⊗k⊤
1,IdX⊗k⊤
2,...,IdX⊗k⊤
dX,IdX]⊤,
where the feedback matrix is given by K:= [k1,k2,...,kdX], and let
M∗=/bracketleftbig
[b1,a1]⊤,..., [bdX,adX]⊤/bracketrightbig
,
where the entries of the row vector biare all zero except for the entries from the index (i−1)dXdU+ 1to
the indexidXdUgiven by vec/parenleftbig
B⊤/parenrightbig
, andA= [a⊤
1,a⊤
2,...,a⊤
dX].
28Published in Transactions on Machine Learning Research (6/2024)
H Setups and results of simulations in the main body
Throughout the main body of this paper, we used the following version of Julia; for each experiment, the
running time was less than around 10minutes.
Julia Version 1.5.3
Platform Info:
OS: Linux (x86_64-pc-linux-gnu)
CPU: AMD Ryzen Threadripper 3990X 64-Core Processor
WORD_SIZE: 64
LIBM: libopenlibm
LLVM: libLLVM-9.0.1 (ORCJIT, znver2)
Environment:
JULIA_NUM_THREADS = 12
The licenses of Julia, OpenAI Gym, DeepMind Control Suite, Lyceum, and MuJoCo, are [The MIT Li-
cense; Copyright (c) 2009-2021: Jeff Bezanson, Stefan Karpinski, Viral B. Shah, and other contribu-
tors: https://github.com/JuliaLang/julia/contributors], [The MIT License; Copyright (c) 2016 OpenAI
(https://openai.com)], [Apache License Version 2.0, January 2004 http://www.apache.org/licenses/], [The
MIT License; Copyright (c) 2019 Colin Summers, The Contributors of Lyceum], and [MuJoCo Pro Lab
license], respectively.
In this section, we provide simulation setups, including the details of environments (see also Figure 9) and
parameter settings.
Figure 9: Illustration of some dynamical systems we have used in this work. Left: Simple limit cycle
represented effectively by the Koopman modes. Middle: DeepMind Control Suite (Tassa et al., 2018)
Cartpole showing stable cycle with spectral radius regularization. Right: OpenAI Gym (Brockman et al.,
2016) walker2d showing simpler movement cycle when the Koopman eigenvalues are regularized.
H.1 Cross-entropy method
Throughout, we used CEM for dynamics parameter (policy) selection to approximately solve KSNR. Here,
we present the setting of CEM.
First, we prepare some fixed feature (e.g. RFFs) for ϕ. Then, at each iteration of CEM, we generate many
parameters to compute the loss (i.e., the sum of the Koopman spectrum cost and negative cumulative reward)
by fitting the transition data generated by each parameter to the feature to estimate its Koopman operator
A. In particular, we used the following regularized fitting:
A=YX⊤(XX⊤+I)−1,
29Published in Transactions on Machine Learning Research (6/2024)
Table 1: Hyperparameters used for limit cycle generation.
CEM hyperparameter Value Training target Koopman operator Value
samples 200training iteration 500
elite size 20RFF bandwidth for ϕ 3.0
iteration 50RFF dimension dϕ 80
planning horizon 80horizon for each iteration 80
policy RFF dimension 50
policy RFF bandwidth 2.0
whereY:= [ϕxh1+1,1,ϕxh2+1,2,...,ϕxhn+1,n]andX:= [ϕxh1,1,ϕxh2,2,...,ϕxhn,n].
If the feature spans a Koopman invariant space and the deterministic dynamical systems are considered, and
if no regularization (i.e., the identity matrix I) is used, any sufficiently rich trajectory data may be used to
exactly compute K(Θ)forΘ. However, in practice, the estimate depends on transition data although the
regularization mitigates this effect. In our simulations, at each iteration, we randomly reset the initial state
according to some distribution, and computed loss for each parameter generating trajectory starting from
that state.
H.2 Setups: imitating target behaviors through Koopman operators
The discrete-time dynamics
rh+1=rh+vr,h∆t, θh+1=θh+vθ,h∆t
is considered and the policy returns vr,handvθ,hgivenrhandθh. In our simulation, we used ∆t= 0.05.
Note the ground-truth dynamics
˙r=r(1−r2),˙θ= 1,
is discretized to
rh+1=rh+rh(1−r2
h)∆t, θh+1=θh+ ∆t.
Figure 10 plots the ground-truth trajectories of observations and x-ypositions.
We trained the target Koopman operator using the ground-truth dynamics with random initializations; the
hyperparameters used for training are summarized in Table 1.
Then, we used CEM to select policy so that the spectrum cost is minimized; the hyperparameters are also
summarized in Table 1.
We tested two forms of the spectrum cost; Λ1(A) =∥m−m⋆∥1andΛ2(A) =∥A−A⋆∥2
HS. The resulting
trajectories are plotted in Figure 11 and 12, respectively. It is interesting to observe that the top mode imi-
tation successfully converged to the desirable limit cycle while Frobenius norm imitation did not. Intuitively,
the top mode imitation focuses more on reconstructing the practically and physically meaningful behavior
while minimizing the error on the Frobenius norm has no immediately clear physical meaning.
H.3 Setups: Generating stable loops (Cartpole)
We used DeepMind Control Suite Cartpole environment with modifications; specifically, we extended the
cart rail to [−100,100]from the original length [−5,5]to deal with divergent behaviors. Also, we used a
combination of linear and RFF features; the first elements of the feature are simply the observation (state)
vector, and the rest are Gaussian RFFs. That way, we found divergent behaviors were well-captured in terms
of spectral radius. The hyperparemeters used for CEM are summarized in Table 2.
30Published in Transactions on Machine Learning Research (6/2024)
Figure 10: The ground-truth trajectory of the limit cycle ˙r=r(1−r2),˙θ= 1. Left: Observations r,cos(θ),
andsin(θ). Right:x-ypositions.
Figure 11: The trajectory generated by RFF policies that minimize Λ(A) =∥m−m⋆∥1. Left: Observations
r,cos(θ), and sin(θ). Right:x-ypositions.
H.4 Setups: Generating smooth movements (Walker)
Because of the complexity of the dynamics, we used four random seeds in this simulation, namely, 100,
200,300, and 400. We used a combination of linear and RFF features for both ϕand the policy. Note,
accordingtothework(Rajeswaranetal.,2017), linearpolicyisactuallysufficientforsometasksforparticular
environments. The hyperparemeters used for CEM are summarized in Table 3.
The resulting trajectories of Walker are illustrated in Figure 13. The results are rather surprising; because
we did not specify the height in reward, the dynamics with only cumulative cost showed rolling behavior
(Up figure) to go right faster most of the time. On the other hand, when the spectrum cost was used, the
hopping behavior (Down figure) emerged. Indeed this hopping behavior moves only one or two joints most
of the time while fixing other joints, which leads to lower (absolute values of) eigenvalues.
The eigenspectrums of the resulting dynamics with/without the spectrum cost are plotted in Figure 14. In
fact, it is observed that the dynamics when the spectrum cost was used showed consistently lower (absolute
values of) eigenvalues; for the hopping behavior, most of the joint angles converged to some values and stayed
there.
31Published in Transactions on Machine Learning Research (6/2024)
Figure12: ThetrajectorygeneratedbyRFFpoliciesthatminimize Λ(A) =∥A−A⋆∥2
HS. Left: Observations
r,cos(θ), and sin(θ). Right:x-ypositions.
Table 2: Hyperparameters used for stable loop generation.
Hyperparameters Value Hyperparameters Value
samples 200elite size 20
iteration 100planning horizon 100
dimensiondϕ 50RFF bandwidth for ϕ 2.0
policy RFF dimension 100policy RFF bandwidth 2.0
Algorithm 2 Practical Algorithm for KS-LC3
Require: Parameter set Π; prior parameter λ; covariance scale ι∈R≥0.
1:Prior distribution over M′is given by (Σ0
M′)−1where Σ0
M′:=λI
2:fort= 0...T−1do
3:Adversary chooses Xt
0.
4:Sample ˆM′tfromN/parenleftig
M′t,(Σt
M′)−1/parenrightig
5:Solve Θt= arg minΘ∈ΠΛ[ˆKt(Θ)] +JΘ/parenleftig
Xt
0;ˆM′t;ct/parenrightig
(e.g., using CEM)
6:Under the dynamics FΘt, sample transition data τt:={τt
n}Nt−1
n=0, whereτt
n:={xt
h,n}Ht
n
h=0
7:Update Σt
M′
8:end for
H.5 Setups: Koopman-Spectrum LC3
Decomposable kernels case We explain how to reduce the memory size in a special decomposable kernel
case. Assume that we employ decomposable kernel with B=Ifor simplicity. Also, suppose Ψ(Θ)∈RdΨ×dϕ
is of finite dimension. For such a case, the dimension dΨ=dζ·dϕwheredζis the dimension of H′′;
however, we do not need to store a covariance matrix of size d2
ϕdζ×d2
ϕdζbut only require to update
a matrix of size dϕdζ×dϕdζwhich significantly reduces the memory size. Specifically, we consider the
modelϕxh+1=M′(ϕxh⊗ζ(Θ)); then using M′′:= reshape( M′,dϕ,dζ,dϕ), we obtain K(Θ) = [M′′[:
,:,1]ζ(Θ),...,M′′[:,:,dϕ]ζ(Θ)]. Here,ζis the realization of ζ(Θ)over some basis. Now, we note that
the dimension of ϕxh⊗ζ(Θ)isdζ·dϕ. Our practical (Thompson sampling version) algorithm is thus
given by Algorithm 2. In the algorithm, we used ˆKt(Θ) = [ ˆM′′t[:,:,1]ζ(Θ),..., ˆM′′t[:,:,dϕ]ζ(Θ)], where
ˆM′′t:= reshape( ˆM′t,dϕ,dζ,dϕ).
32Published in Transactions on Machine Learning Research (6/2024)
Table 3: Hyperparameters used for Walker.
Hyperparameters Value Hyperparameters Value
samples 300elite size 20
iteration 50planning horizon 300
dimension of dϕ 200RFF bandwidth for ϕ 5.0
policy RFF dimension 300policy RFF bandwidth 30.0
Figure 13: Walker trajectories visualized via Lyceum. Up: When only (single-step) reward v−0.001∥a∥2
R6
is used, showing rolling behavior. Down: When the spectrum cost Λ(A) = 5/summationtextdϕ
i=1|λi(A)|is used together
with the reward, showing simple hopping behavior.
Simple linear system experiment The hyperparameters used for KS-LC3in the simple linear system
experiment are summarized in Table 4.
Cartpole pretraining policies For training three policies, we used Model Predictive Path Integral Con-
trol (MPPI) (Williams et al., 2017) with the rewards (p+ 0.3)2,(p−0.3)2,(v+ 1.5)2, and (v−1.5)2, where
pis the cart position and vis the cart velocity. Also, for all of the cases, the penalty −100is added when
cos(θ)<0, whereθis the pole angle, which aims at preventing the pole from falling down.
Becauseweneedtohaveonemorestatedimensiontospecifywhichrewardtogenerate, weusedtheanalytical
model of cartpole specified in OpenAI Gym.
Starting from random initial state, we first use MPPI to move to p=−0.3, then from there move to p= 0.3;
and we learn an RFF policy for this movement along with the Koopman operator. Then, we randomly
33Published in Transactions on Machine Learning Research (6/2024)
Figure 14: Eigenspectrums showing absolute values of eigenvalues for the dynamics with/without the spec-
trum cost.
Table 4: Hyperparameters used for KS-LC3in the simple linear case.
Hyperparameters Value Hyperparameters Value
prior parameter λ 0.05covariance scale ι0.0001
planning horizon 50
initialized the state to accelerate to v=−1.5, followed by a random initialization again to accelerate to
v= 1.5; we then learned two policies for those two movements. We used the planning horizons of 100for
every movement except for the movement going to p= 0.3where we used 120because it is following the
previous movement. We repeated this for 20iterations.
The parameter space Πis a space of linear combinations of those three policies. We summarized the
hyperparemeters used for MPPI/pretraining in Table 5.
Cartpole learning For learning, we used four random seeds, namely, 100,200,300, and 400. The esti-
mated spectrum cost curve represents the cost Λ[ˆKt(Θt)]. The hyperparameters used for CEM and KS-LC3
are summarized in Table 6. We note that for KS-LC3we added additional cost on the policy parameter
exceeding its ℓ∞norm above 2.0.
Table 5: Hyperparameters of MPPI and pretraining.
MPPI hyperparameters Value Pretraining hyperparameters Value
variance of controls 0.42iteration 20
temperature parameter 0.1policy RFF dimension 2000
planning horizon 100/120policy RFF bandwidth 1.5
number of planning samples 524 dimension of dϕ 60
RFF bandwidth for ϕ 1.5
34Published in Transactions on Machine Learning Research (6/2024)
Table 6: Hyperparameters used for CEM/KS-LC3.
Hyperparameters for CEM Value Hyperparameters for KS-LC3Value
samples 200dimension of dζ 50
elite size 20RFF bandwidth for ζ 5.0
planning horizon 500prior parameter λ 1.0
iteration 50covariance scale ι 0.0001
I Further experimental analysis
In this section, we provide additional experiments. Throughout this section, we used the following version of
Julia as our computational resource has changed when conducting the experiments presented in this section.
Julia Version 1.5.3
Platform Info:
OS: Linux (x86_64-pc-linux-gnu)
CPU: Intel(R) Xeon(R) CPU E5-2620 v3 @ 2.40GHz
WORD_SIZE: 64
LIBM: libopenlibm
LLVM: libLLVM-9.0.1 (ORCJIT, haswell)
Environment:
JULIA_NUM_THREADS = 12
MuJoCo version 2.0 is used (license: MuJoCo Pro Individual license).
I.1 Variations on stable Cartpole motions
In our simulation of generating stable Cartpole motion, we have seen that it shows oscillating behavior when
stability is enforced through the spectrum cost in addition to the reward that encourages the cartpole to
have larger velocity. To see this phenomenon more in details, we have conducted the same experiments
for different time horizons, namely, 50,100and200. Here, we use Λ(A) = 105max(1,ρ(A))(10times
more weight than that in the simulation experiment in the main body). The resulting velocity trajectories
with/without the spectrum cost are plotted in Figure 15. Also, the angle trajectories are plotted in Figure
16, where the zero lines are the threshold for adding penalty costs. Note for the case of time horizon 200, we
used more intensive CEM search whose parameters are summarized in Table 7, but could not find a policy
parameter that can keep the pole straight up. Studying more sophisticated heuristic search algorithm will
be an important direction of future research. From Figure 15, it is observed that the longer horizon may
not indicate more oscillation “cycles”. In fact, our spectrum cost only regularizes the dynamics to be stable,
which may include a motion where the velocity converges to some fixed value. The spectrum radius for the
cases of time horizon 50,100and200without the spectrum cost is given by 1.003,1.00006and1.002, while
that with the spectrum cost is 0.992,0.999and0.997. While all of them show stable Koopman spectrum
when the spectrum cost is used, the case for the time horizon 100shows particularly interesting behavior.
Please recall that the failure of keeping the pole straight up is not necessarily regarded as unstable dynamics
over the specified state space where the angle representation is bounded but costs the learner within the
cumulative cost term.
I.2 Variations on smooth Walker motions
To investigate smooth motion generations studied in the main body of this paper more, we conducted
additional experiments.
35Published in Transactions on Machine Learning Research (6/2024)
Figure 15: Velocity trajectories of Cartpole for different time horizons ( 50,100and200) with/without the
spectrum cost.
Table 7: Hyperparameters used for stabilized Cartpole for time horizon 200.
Hyperparameters Value Hyperparameters Value
samples 200elite size 20
iteration 1000planning horizon 200
dimensiondϕ 50RFF bandwidth for ϕ 2.0
policy RFF dimension 300policy RFF bandwidth 2.0
I.2.1 Smoothness comparison with increased action cost
Especially, we also compare our KSNR for smoothness enhancements to the use of action costs in the Walker
environment. In this experiment, we used the hyperparameters summarized in Table 8. We again used a
combination of linear and RFF features for both ϕand the policy. Recall the default immediate reward is
v−0.001∥a∥2
R6, wherevis the velocity and ais the action vector of dimension 6. Here, in addition to KSNR,
we tested increased action cost scenarios where the immediate rewards are v−0.01∥a∥2
R6andv−0.1∥a∥2
R6
respectively. Across the six seed runs (of seed numbers of 100,200,300,400,500, and 600), we obtained the
mean of the cumulative reward and the cumulative action cost (which is computed for the trajectories using
0.001∥a∥2
R6for all of the cases), and the mean and standard deviation of the spectrum cost, all of which
are summarized in Table 9. As observed, increased action cost in our scenarios shows lower spectrum cost;
while KSNR shows better cumulative reward with better spectrum cost. However, the motion generated
by the increased action cost shows lower action penalty cost; which implies that the spectrum cost and the
action cost have some correlation while they qualitatively prefer different motions. We also measured the
smoothness by another metric than the spectrum cost itself, which is defined by
Smoothness( τ) :=1
dXHH−1/summationdisplay
h=0∥xh+1−xh∥1,
36Published in Transactions on Machine Learning Research (6/2024)
Figure 16: Angle trajectories of Cartpole for different time horizons ( 50,100and200) with/without the
spectrum cost.
Table 8: Hyperparameters used for additional Walker smoothness experiments.
Hyperparameters Value Hyperparameters Value
samples 300elite size 20
iteration 120planning horizon 300
dimension of dϕ 200RFF bandwidth for ϕ 5.0
policy RFF dimension 300policy RFF bandwidth 30.0
whereτ:={xh}H
h=0is a trajectory. The mean smoothness values across the runs for the motions generated
by the CEM algorithms with default action cost, 10times more action cost, 100times more action cost, and
with the spectrum cost are 0.082,0.033,0.007, and 0.028respectively, and they appear to be consistent to
the spectrum cost in this case. The motions are visualized in Figure 17; their joint trajectories are plotted in
Figure 18 and the eigenspectrums are given in Figure 19. Note those motions are of those showing median
values of the spectrum cost within the seed runs.
I.2.2 Different feature space H0
Next, we examine what happens when different feature space H0is employed in practice. In particular, we
used a polynomial feature (spanned by x,x2,x3,x4,x5) forϕwhile the policy is again a combination of linear
and RFF features. The hyperparameters are the same except for the dimension of dϕwhich is now 5dX.
Now, the spectrum costs of the generated motions of the CEM algorithms with default action cost, 10times
more action cost, 100times more action cost are given by 232.9±13.6,194.5±19.5,152.1±42.8, and
217.9±24.7; and the mean of cumulative reward and action cost (penalty) of the one with spectrum cost
defined over the polynomial feature space is 900.1and143.3. It is observed that 10times more action cost
led to lower spectrum cost in this case than KSNR while the cumulative reward of KSNR is much higher.
The smoothness measure for KSNR is now 0.064, which is higher than the case with RFF features.
37Published in Transactions on Machine Learning Research (6/2024)
Table 9: Cumulative reward, cumulative action cost (penalty), and spectrum cost comparisons.
Method (Env. setting) Reward Penalty Spectrum cost (mean) Spectrum cost (std)
CEM (default action cost) 1011.5 177.3 317.0 ±33.2
CEM (×10action cost) 596.4 10.9 213.2 ±50.0
CEM (×100action cost) 63.4 0.5 88.8 ±46.6
CEM (with spectrum cost) 737.8 78.1 186.6 ±88.4
In fact, when visualizing the motion and joint trajectory Figure 20, we observe that the walker also shows
similar rolling behavior but with slightly better periodicity than that one without spectrum cost. Eigenspec-
trums are shown in Figure 21; where we see less difference among the ones with/without spectrum cost and
with×10action cost.
These results imply that the feature space selection influences the qualitative behavior difference in practice;
please also see Remark 3.2.
38Published in Transactions on Machine Learning Research (6/2024)
Figure 17: Visualizations of Walker motions generated by the CEM algorithm with default action cost, 10
times more action cost, 100times more action cost, and with the spectrum cost. The motions are of those
showing median values of the spectrum cost within the seed runs. It is observed that the motion generated
by the one with 10times more action cost is smooth but uses two feet to hop, which would reduce the
magnitudes of actions applied to the joints. The motion generated by the one with the spectrum cost again
lifts one foot and hops; this specific visualized motion then shows a bit of rotation at the last moment.
39Published in Transactions on Machine Learning Research (6/2024)
Figure 18: Joint trajectories of Walker motions generated by the CEM algorithm with default action cost, 10
times more action cost, 100times more action cost, and with the spectrum cost. They are of those showing
median values of the spectrum cost within the seed runs.
Figure 19: Averaged eigenspectrums showing absolute values of eigenvalues for the dynamics with/without
the spectrum cost and with 10times more action cost and 100times more action cost.
40Published in Transactions on Machine Learning Research (6/2024)
Figure 20: Up: Visualization of Walker motions generated by the CEM algorithm with the spectrum cost
where the feature space is spanned by x,x2,x3,x4,x5. The motion is of that showing median value of the
spectrum cost within the seed runs. Down: Its joint trajectory.
41Published in Transactions on Machine Learning Research (6/2024)
Figure 21: Averaged eigenspectrums over the polynomial feature space showing absolute values of eigenvalues
for the dynamics with/without the spectrum cost and with 10times more action cost and 100times more
action cost.
42