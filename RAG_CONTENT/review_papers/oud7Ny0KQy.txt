Published in Transactions on Machine Learning Research (09/2023)
RIFLE: Imputation and Robust Inference from Low Order
Marginals
Sina Baharlouei baharlou@usc.edu
University of Southern California
Kelechi Ogudu kogudu@usc.edu
University of Southern California
Sze-chuan Suen ssuen@usc.edu
University of Southern California
Meisam Razaviyayn razaviya@usc.edu
University of Southern California
Reviewed on OpenReview: https: // openreview. net/ forum? id= oud7Ny0KQy
Abstract
The ubiquity of missing values in real-world datasets poses a challenge for statistical infer-
ence and can prevent similar datasets from being analyzed in the same study, precluding
many existing datasets from being used for new analyses. While an extensive collection
of packages and algorithms have been developed for data imputation, the overwhelming
majority perform poorly if there are many missing values and low sample sizes, which are
unfortunately common characteristics in empirical data. Such low-accuracy estimations ad-
versely aﬀect the performance of downstream statistical models. We develop a statistical
inference framework for regression and classiﬁcation in the presence of missing data without
imputation . Our framework, RIFLE (Robust InFerence via Low-order moment Estima-
tions), estimates low-order moments of the underlying data distribution with corresponding
conﬁdence intervals to learn a distributionally robust model. We specialize our framework
to linear regression and normal discriminant analysis, and we provide convergence and per-
formance guarantees. This framework can also be adapted to impute missing data. In
numerical experiments, we compare RIFLE to several state-of-the-art approaches (includ-
ing MICE, Amelia, MissForest, KNN-imputer, MIDA, and Mean Imputer) for imputation
and inference in the presence of missing values. Our experiments demonstrate that RIFLE
outperforms other benchmark algorithms when the percentage of missing values is high
and/or when the number of data points is relatively small. RIFLE is publicly available at
https://github.com/optimization-for-data-driven-science/RIFLE .
1 Introduction
Machine learning algorithms have shown promise when applied to various problems, including healthcare,
ﬁnance, social data analysis, image processing, and speech recognition. However, this success mainly relied
on the availability of large-scale, high-quality datasets, which may be scarce in many practical problems,
especially in medical and health applications (Pedersen et al., 2017; Sterne et al., 2009; Beaulieu-Jones et al.,
2018). Moreover, many experiments and datasets suﬀer from the small sample size in such applications.
Despite the availability of a small number of data points in each study, an increasingly large number of
datasets are publicly available. To fully and eﬀectively utilize information on related research questions from
diverse datasets, information across various datasets (e.g., diﬀerent questionnaires from multiple hospitals
with overlapping questions) must be combined in a reliable fashion.
1Published in Transactions on Machine Learning Research (09/2023)
Figure 1: Consider the problem of predicting the trait yfrom feature vector (x1,...,x100). Suppose that
we have access to three data sets: The ﬁrst dataset includes the measurements of (x1,x2,...,x40,y)for
n1individuals. The second dataset collects data from another n2individuals by measuring (x30,...,x80)
with no measurements of the target variable yin it; and the third dataset contains the measurements
from the variables (x70,...,x100,y)forn3number of individuals. How one should learn the predictor
ˆy=h(x1,...,x100)from these three datasets?
After integrating data from diﬀerent studies, the obtained dataset can contain large blocks of missing values,
as they may not share the same features (Figure 1).
There are three general approaches for handling missing values in statistical inference (classiﬁcation and
regression) tasks. A Naïve method is to remove the rows containing missing entries. However, such an
approach is not an option when the percentage of missingness in a dataset is high. For instance, as demon-
strated in Figure 1, the entire dataset will be discarded if we eliminate the rows with at least one missing
entry.
The most common methodology for handling missing values in a learning task is to impute them in a pre-
processing stage. The general idea behind data imputation is that the missing values can be predicted using
the available data entries and correlated features. Imputation algorithms cover a wide range of methods,
including imputing missing entries with the columns means Little & Rubin (2019, Chapter 3) (or median),
least-square and linear regression-based methods (Raghunathan et al., 2001; Kim et al., 2005; Zhang et al.,
2008; Cai et al., 2006; Buuren & Groothuis-Oudshoorn, 2010), matrix completion and expectation maxi-
mization approaches Dempster et al. (1977); Ghahramani & Jordan (1994); Honaker et al. (2011), KNN
based (Troyanskaya et al., 2001), Tree based methods (Stekhoven & Bühlmann, 2012; Xia et al., 2017), and
methods using diﬀerent neural network structures. Appendix A presents a comprehensive review of these
methods.
The imputation of data allows practitioners to run standard statistical algorithms requiring complete data.
However, the prediction model’s performance can be highly reliant on the accuracy of the imputer. High
error rates in the prediction of missing values by the imputer can lead to the catastrophic performance of
the downstream statistical methods executed on the imputed data.
Another class of methods for inference in the presence of missing values relies on robust optimization over the
uncertainty sets on missing entries. Shivaswamy et al. (2006) and Xu et al. (2009) adopt robust optimization
to learn the parameters of a support vector machine model. They consider uncertainty sets for the missing
entries in the dataset and solve a min-max problem over those sets. The obtained classiﬁers are robust
to the uncertainty of missing entries within the uncertainty regions. In contrast to the imputation-based
approaches, the robust classiﬁcation formulation does not carry the imputation error to the classiﬁcation
phase. However, ﬁnding appropriate intervals for each missing entry is challenging, and it is unclear how to
determine the uncertainty range in many real datasets. Moreover, their proposed algorithms are limited to
the SVM classiﬁer.
2Published in Transactions on Machine Learning Research (09/2023)
Figure 2: Prediction of the target variable without imputation. RIFLE estimates conﬁdence intervals for
low-order (ﬁrst and second-order) marginals from the input data containing missing values. Then, it solves
a distributionally robust problem over the set of all distributions whose low-order marginals are within the
estimated conﬁdence intervals.
In this paper, we propose RIFLE (Robust InFerence via Low-order moment Estimations) for the direct
inference of a target variable based on a set of features containing missing values. The proposed framework
does not require the data to be imputed in a pre-processing stage. However, it can also be used as a
pre-processing tool for imputing data. The main idea of the proposed framework is to estimate the ﬁrst
and second-order moments of the data and their conﬁdence intervals by bootstrapping on the available
data matrix entries. Then, RIFLE ﬁnds the optimal parameters of the statistical model for the worst-case
distribution with the low-order moments (mean and variance) within the estimated conﬁdence intervals (See
Figure 2). Compared to Shivaswamy et al. (2006); Xu et al. (2009), we estimate uncertainty regions for the
low-order marginals using the Bootstrap technique. Furthermore, our framework is not restricted to any
particular machine learning model, such as support vector machines (Xu et al., 2009).
Contributions: Our main contributions are as follows:
1. We present a distributionally robust optimization framework over the low-order marginals of the
training data distribution for inference in the presence of missing values. The proposed framework
does not require data imputation as a pre-processing stage. In Section 3 and Section 4, we specialize
the framework to ridge regression and classiﬁcation models as two case studies respectively. The
proposed framework provides a novel strategy for inference in the presence of missing data, especially
for datasets with large proportions of missing values.
2. We provide theoretical convergence guarantees and the iteration complexity analysis of the presented
algorithms for robust formulations of ridge linear regression and normal discriminant analysis. More-
over, we show the consistency of the prediction under mild assumptions and analyze the asymptotic
statistical properties of the solutions found by the algorithms.
3. While the robust inference framework is primarily designed for direct statistical inference in the
presence of missing values without performing data imputation, it can also be adopted as an impu-
tation tool. To demonstrate the quality of the proposed imputer, we compare its performance with
several widely-used imputation packages such as MICE (Buuren & Groothuis-Oudshoorn, 2010),
Amelia (Honaker et al., 2011), MissForest (Stekhoven & Bühlmann, 2012), KNN-Imputer (Troyan-
skaya et al., 2001), MIDA (Gondara & Wang, 2018), GAIN (Yoon et al., 2018) on real and synthetic
datasets. Generally speaking, our method outperforms all of the mentioned packages when the
number of missing entries is large.
2 Robust Inference via Estimating Low-order Moments
RIFLE is based on a distributionally robust optimization (DRO) framework over low-order marginals. As-
sume that (x,y)∈Rd×Rfollows a joint probability distribution P∗. A standard approach for predicting
the target variable ygiven the input vector xis to ﬁnd the parameter θthat minimizes the population risk
with respect to a given loss function /lscript:
min
θE(x,y)∼P∗/bracketleftBig
/lscript/parenleftBig
x,y;θ/parenrightBig/bracketrightBig
. (1)
Since the underlying distribution of data is rarely available in practice, the above problem cannot be directly
solved. The most common approach for approximating (1) is to minimize the empirical risk with respect to
3Published in Transactions on Machine Learning Research (09/2023)
ngiven i.i.d samples (x1,y1),..., (xn,yn)drawn from the joint distribution P∗:
min
θ1
nn/summationdisplay
i=1/lscript(xi,yi;θ).
The above empirical risk formulation assumes that all entries of xiandyiare available. Thus, to utilize
the empirical risk minimization (ERM) framework in the presence of missing values, one can either remove
or impute the missing data points in a pre-processing stage. Training via robust optimization is a natural
alternative in the presence of missing data. Shivaswamy et al. (2006); Xu et al. (2009) suggest the following
optimizationproblemthatminimizesthelossfunctionfortheworst-casescenariooverthedeﬁneduncertainty
sets per data points:
min
θmax
{δi∈Ni}n
i=11
nn/summationdisplay
i=1/lscript(xi−δi,yi;θ), (2)
whereNirepresents the uncertainty region of data point i. Shivaswamy et al. (2006) obtains the uncertainty
sets by assuming a known distribution on the missing entries of datasets. The main issue in their approach is
thattheconstraintsdeﬁnedondatapointsaretotallyuncorrelated. Xuetal.(2009)ontheotherhanddeﬁnes
Nias a “box” constraint around the data point isuch that they can be linearly correlated. For this speciﬁc
case, they show that solving the corresponding robust optimization problem is equivalent to minimizing
a regularized reformulation of the original loss function. Such an approach has several limitations: First,
it can only handle a few special cases (SVM loss with linearly correlated perturbations on data points).
Furthermore, Xu et al. (2009) is primarily designed for handling outliers and contaminated data. Thus, they
do not oﬀer any mechanism for the initial estimation of xiwhen several vector entries are missing. In this
work, we instead take a distributionally robust approach by considering uncertainty on the data distribution
instead of deﬁning an uncertainty set for each data point. In particular, we aim to ﬁt the best parameters
of a statistical learning model for the worst distribution in a given uncertainty set by solving the following:
min
θmax
P∈PE(x,y)∼P[/lscript(x,y;θ)], (3)
wherePis an uncertainty set over the underlying distribution of data. A key observation is that deﬁning the
uncertaintysetPin(3)iseasierandcomputationallymoreeﬃcientthandeﬁningtheuncertaintysets {Ni}n
i=1
in(2). Inparticular,theuncertaintyset Pcanbeobtainednaturallybyestimatinglow-ordermomentsofdata
distribution using only available entries . To explain this idea and to simplify the notations, let z= (x,y),
¯µz,E[z], and ¯Cz,E[zzT].While ¯µzand ¯Czare typically not known exactly, one can estimate them
(within certain conﬁdence intervals) from the available data by simply ignoring missing entries (assuming
the missing value pattern is completely at random, e.g., MCAR). Moreover, we can estimate the conﬁdence
intervals via bootstrapping. Particularly, we can estimate µz
min,µz
max,Cz
min, and Cz
maxfrom data such that
µz
min≤¯µz≤µz
maxandCz
min≤¯Cz≤Cz
maxwith high probability (where the inequalities for matrices and
vectors denote component-wise relations). In Appendix B, we show how a bootstrapping strategy can be
used to obtain the conﬁdence intervals described above. Given these estimated conﬁdence intervals from
data, (3) can be reformulated as
min
θmax
PEP[/lscript(z;θ)]
s.t.µz
min≤EP[z]≤µz
max,
Cz
min≤EP[zzT]≤Cz
max.(4)
Gao & Kleywegt (2017) utilize the distributionally robust optimization as (3) over the set of positive semi-
deﬁnite (PSD) cones for robust inference under uncertainty. While their formulation considers /lscript2balls for the
constraints on low order moments of the data, we use /lscript∞constraints that are computationally more natural
in the presence of missing entries when combined with bootstrapping. Furthermore, while it can be applied
to general convex losses, their method relies on the ellipsoid and the existence of oracles for performing
the steps of the ellipsoid method, which is not applicable in modern high-dimensional problems. Moreover,
they assume concavity in data (the existence of some oracle to return the worst-case data points) that is
4Published in Transactions on Machine Learning Research (09/2023)
practically unavailable even in convex loss functions (including linear regression and normal discriminant
analysis studied in our work).
In Section 3, we study the proposed distributionally robust framework described in (4) for the ridge linear
regression. We design eﬃcient ﬁrst-order convergent algorithms to solve the problem and show how we
can use the algorithms for both inference and imputation in the presence of missing values. Further, in
Appendix F, we study the proposed distributionally robust framework for the classiﬁcation problems under
the normality assumption of features. In particular, we show how Framework (4) can be specialized to the
robust normal discriminant analysis in the presence of missing values.
3 Robust Linear Regression in the Presence of Missing Values
Let us specialize our framework to the ridge linear regression model. In the absence of missing data, ridge
regression ﬁnds optimal regressor parameter θby solving
min
θ/bardblXθ−y/bardbl2
2+λ/bardblθ/bardbl2
2,
or equivalently by solving:
min
θθTXTXθ−2θTXTy+λ/bardblθ/bardbl2
2. (5)
Thus, having the second-order moments of the data C=XTXandb=XTyis suﬃcient for ﬁnding the
optimal solution. In other words, it suﬃces to compute the inner product of any two column vectors ai,aj
ofX, and the inner product of any column aiofXwith vector y. Since the matrix Xand vector yare not
fully observed due to the existence of missing values, one can use the available data (see (24) for details) to
compute the point estimators C0andb0. These point estimators can be highly inaccurate, especially when
the number of non-missing rows for two given columns is small. In addition, if the pattern of missing entries
does not follow the MCAR assumption, the point estimators are not unbiased estimators of Candb.
3.1 A Distributionally Robust Formulation of Linear Regression
As we mentioned above, to solve the linear regression problem, we only need to estimate the second-order
moments of the data ( XTXandXTy). Thus, the distributionally robust formulation described in (4) is
equivalent to the following optimization problem for the linear regression model:
min
θmax
C,bθTCθ−2bTθ+λ/bardblθ/bardbl2
2
s.t. C0−c∆≤C≤C0+c∆,
b0−cδ≤b≤b0+cδ,
C/followsequal0,(6)
where the last constraint guarantees that the covariance matrix is positive and semi-deﬁnite. We dicuss the
procedure of estimating the conﬁdence intervals ( b0,C0,δ,and∆) in Appendix B.
3.2 RIFLE for Ridge Linear Regression
Since the objective function in (6) is convex in θ(ridge regression) and concave in bandC(linear), the min-
imization and maximization sub-problems are interchangeable (Sion et al., 1958). Thus, we can equivalently
rewrite Problem (6) as:
max
C,bg(C,b)
s.t.C0−c∆≤C≤C0+c∆,
b0−cδ≤b≤b0+cδ,
C/followsequal0,(7)
5Published in Transactions on Machine Learning Research (09/2023)
whereg(b,C) = min θθTCθ−2bTθ+λ/bardblθ/bardbl2. Function gcan be computed in closed-form given any pair
of(C,b)by settingθ= (C+λI)−1b. Thus, using Danskin’s Theorem (Danskin, 2012), we can apply
projected gradient ascent to function gto ﬁnd an optimal solution of (7) as described in Algorithm 1. At
each iteration of the algorithm, we ﬁrst perform one step of projected gradient ascent on matrix Cand
vector b; then we update θin closed-form for the obtained Candb. We initialize Candbusing entriwise
point estimation on the available rows (see Equation (24) in Appendix B). The projection of bto the box
Algorithm 1 RIFLE for Ridge Linear Regression in the Presence of Missing Values
1:Input:C0,b0,∆,δ,T
2:Initialize :C=C0,b=b0.
3:fori= 1,...,Tdo
4:Update C= Π∆+/bracketleftbig
C+αθθT/bracketrightbig
5:Update b= Π δ(b−2αθ)
6:Setθ= (C+λI)−1b
constraint b0−cδ≤b≤b0+cδcan be done entriwise and has the following closed-form
Πδ(bi) =

bi ifb0i−cδi≤bi≤b0i+cδi,
b0i−cδiifbi<b0i−cδi,
b0i+cδiifb0i+cδi<bi.
Theorem 1. Let(˜θ,˜C,˜b)be the optimal solution of (6),θ∗(b,C) = arg minθθTCθ−2bTθ+λ/bardblθ/bardbl2, and
D=/bardblC0−˜C/bardbl2
F+/bardblb0−˜b/bardbl2
2. Assume that for any given bandC, within the uncertainty (constraint) sets
described in (6),/bardblθ∗(b,C)/bardbl≤τ. Then Algorithm 1 computes an /epsilon1-optimal solution of the objective function
in(7)inO/parenleftBig
D(τ+1)2
λ/epsilon1/parenrightBig
iterations.
Proof.The proof is relegated to Appendix H.
In Appendix C, we show how using the acceleration method of Nesterov can improve the convergence rate
of Algorithm 1 to O/parenleftBig/radicalBig
D(τ+1)2
/epsilon1λ/parenrightBig
. A technical issue of Algorithm 1 and its accelerated version presented in
Appendix C is that projection of Cto the intersection of box constraints and the set of positive semideﬁnite
matrices ( Π∆+[C]) is challenging and cannot be done in closed-form. In the implementation of Algorithm 1,
we relax the problem by removing the PSD constraint on Cto avoid this complexity and time-consuming
singular value decomposition at each iteration. This relaxation does not drastically change the algorithm’s
performance, as our experiments show in Section 5. A more systematic approach is to write the dual
problem of the maximization problem and handle the resulting constrained minimization problem with the
Alternating Direction Method of Multipliers (ADMM). The detailed procedure of such an approach can be
found in Appendix D. All these algorithms are provably convergent to the optimal points of Problem (6). In
addition to theoretical convergence, we have numerically evaluated the convergence of resulting algorithms
in Appendix K. Further, the proposed algorithms are consistent , as discussed in Appendix J.
3.3 Performance Guarantees for RIFLE
Thus far, we have discussed how to eﬃciently solve the robust linear regression problem in the presence of
missing values. A natural question in this context is the statistical performance of the obtained optimal
solution in the previous section on the unseen test data points. Theorem 2 answers this question from two
perspectives: Assuming that the missing values are distributed completely at random, our estimators are
consistent. Moreover, for the ﬁnite case, Theorem 2 part (b) states that with the proper choice of conﬁdence
intervals, with high probability, the test loss of the obtained solution is bounded by the training loss of the
estimator. Note that the results regarding the performance of the robust estimator generally hold for MCAR
missing pattern. However, we perform several experiments on datasets with MNAR patterns to show how
RIFLE works in practice on such datasets in Section 5.
6Published in Transactions on Machine Learning Research (09/2023)
Theorem2. Assume the data domain is bounded and that the missing pattern of the data follows MCAR. Let
Xn×d,ybe the training data drawn i.i.d. from the ground-truth distribution P∗with low-order moments C∗
andb∗. Further, assume that each entry of Xandyis missing with probability p<1. Let (˜θn,˜Cn,˜bn)be
the solution of Problem (6).
(a) Consistency of the Covariance Estimator : As the number of data points goes to inﬁnity, the
estimated low-order marginals converge to the ground-truth values, almost surely. More precisely,
lim
n→∞˜Cn=EP∗[xxT], a.s., (8)
lim
n→∞˜bn=EP∗[xy], a.s. (9)
(b)Deﬁning
Ltrain(˜θn) =˜θT
n˜Cn˜θn−2˜bn˜θn+λ/bardbl˜θn/bardbl2
2
Ltest(˜θn) =˜θT
nC∗˜θn−2b∗T˜θn+λ/bardbl˜θn/bardbl2
2,
where C∗=E(x,y)∼P∗[xxT]andb∗=E(x,y)∼P∗[xy]are the ground-truth second-order moments. Given
V= maxi,jVar(XiXj)(maximum variance of pairwise feature products), with the probability of at least
1−d2V
2c2∆2n(1−p), we have:
Ltest(˜θ)≤Ltrain(˜θ), (10)
where ∆ = min{∆ij}andcis the hyper-parameter for controlling the size of the conﬁdence intervals as
presented in (6)
Proof.The proof is relegated to Appendix H.
3.4 Imputation of Missing Values and Going Beyond Linear Regression
RIFLE can be used for imputing missing data. To this end, we impute diﬀerent features of a given dataset
independently. More precisely, to impute each feature containing missing values, we consider it as a target
variable yand the rest of the features as the input Xin our methodology. Then, we train a model to predict
the feature ygiven Xvia Algorihm 1 (or its ADMM version, Algorithm 7, in the appendix). Let the obtained
optimal solutions be C∗,b∗,andθ∗. For a given missing entry, we can use θ∗only if all other features in the
row of that missing entry are available. However, that is not usually the case in practice, as each row can
contain more than one missing entry. Therefore, one can learn a separate model for each missing pattern
in the dataset. Let us clarify this point through the example in Figure 1. In this example, we have three
diﬀerent missing patterns (one missing pattern for each dataset). For missing entries in Dataset 1, the ﬁrst
forty features are available. Let rjdenote the vector of the ﬁrst 40features in row j. Assume that we aim
to impute entry i∈{41,..., 100}in rowjwhereidenoted by xji. To this end, we restrict Xto the ﬁrst
40features. Moreover, we consider y=xias the target variable. Then, we run Algorithm 1 on Xandyto
obtain the optimal C∗,b∗
i, andθ∗
i. Consequently, we impute xjias follows:
xji=rT
jθ∗
i
We can use the same methodology for imputing missing entries in each feature for missing patterns in
Dataset 2 and Dataset 3. While this approach is reasonable for the missing pattern observed in Figure 1,
in many practical problems, diﬀerent rows can have distinct missing patterns. Thus, in the worst case,
Algorithm 1 must be executed once for each missing entry. Such an approach is computationally expensive
and might be infeasible in large-scale datasets containing large amounts of missing entries. Alternatively, one
can perform Algorithm 1 only once to obtain C∗andb∗(considered the “worst-case/pessimistic” estimation
of the moments). Then to impute each missing entry, C∗andb∗are restricted to the features available in
that missing entry’s row. Having the restricted C∗andb∗, the regressor θ∗can be obtained in closed-form
(line 6 in Algorithm 1). In this approach, we perform algorithm 1 once and ﬁnd the optimal θ∗for each
missing entry based on the estimated C∗andb∗. This approach can lead to sub-optimal solutions compared
to the former approach, but it is much faster and more scalable.
7Published in Transactions on Machine Learning Research (09/2023)
Beyond Linear Regression: While the developed methods are primarily designed for ridge linear regres-
sion, one can apply non-linear transformations (kernels) to obtain models beyond linear. In Appendix E, we
show how to extend the developed algorithms to quadratic models. The RIFLE framework applied to the
quadratically transformed data is called QRIFLE .
4 Robust Classiﬁcation Framework
In this section, we study the proposed framework in (4) for the classiﬁcation tasks in the presence of missing
values. Since the target variable y∈ Y ={1,...,M}takes discrete values in classiﬁcation tasks, we
consider the uncertainty sets over the data’s ﬁrst- and second-order marginals given each target value (label)
separately. Therefore, the distributionally robust classiﬁcation over low-order marginals can be described as:
min
wmax
PEP[/lscript(x,y,w)]
s.t.µmin,y≤EP[x|y]≤µmax,y∀y∈Y
Σmin,y≤EP[xxT|y]≤Σmax,y∀y∈Y(11)
whereµmin,µmax,Σmin,andΣmaxare the estimated conﬁdence intervals for the ﬁrst and second order of
the data distribution. Unlike the robust linear regression task in Section 3, the evaluation of the objective
function in (11) might depend on higher-order marginals (beyond second-order) due to the nonlinearity of
the loss function. As a result, Problem (11) is a non-convex non-concave intractable min-max optimization
problem in general. For the sake of computational traceability, we restrict the distribution in the inner
maximization problem to the set of normal distributions. In the following section, we specialize (11) to
the quadratic discriminant analysis as a case study. The methodology can be extended to other popular
classiﬁcation algorithms, such as support vector machines and multi-layer neural networks.
4.1 Robust Quadratic Discriminant Analysis
Learning a logistic regression model on datasets containing missing values has been studied extensively
in the literature (Fung & Wrobel, 1989; Abonazel & Ibrahim, 2018). Besides deleting missing values and
imputation-based approaches, Fung & Wrobel (1989) models the logistic regression task in the presence
of missing values as a linear discriminant analysis problem where the underlying assumption is that the
predictors follow normal distribution conditional on the labels. Mathematically speaking, they assume that
the data points assigned to a speciﬁc label follow a Gaussian distribution, i.e., x|y=i∼N(µi,Σ). They use
the available data to estimate the parameters of each Gaussian distribution. Therefore, the parameters of the
logistic regression model can be assigned based on the estimated parameters of the Gaussian distributions for
diﬀerent classes. Similar to the linear regression case, the estimations of means and covariances are unbiased
only when the data satisﬁes the MCAR condition. Moreover, when the number of data points in the dataset
is small, the variance of the estimations can be very high. Thus, to train a logistic regression model that is
robust to the percentage and diﬀerent types of missing values, we specialize the general robust classiﬁcation
framework formulated in Equation (11) to the logistic regression model. Instead of considering a common
covariance matrix for the conditional distributions of xgiven labels y(linear discriminant analysis), we
assume a more general case where each conditional distribution has its own covariance matrix (quadratic
discriminant analysis). Assume that x|y∼N(µy,Σy)fory= 0,1. We aim to ﬁnd the optimal solution to
the following problem:
min
wmax
µ0,µ1,Σ0,Σ1Ex|y=1∼N(µ1,Σ1)/bracketleftBig
−log/parenleftBig
σ(wTx)/parenrightBig/bracketrightBig
P(y= 1) +
Ex|y=0∼N(µ0,Σ0)/bracketleftBig
−log/parenleftBig
1−σ(wTx)/parenrightBig/bracketrightBig
P(y= 0)
s.t. µmin 0≤µ0≤µmax 0
µmin 1≤µ1≤µmax 1
Σmin 0≤Σ0≤Σmax 0
Σmin 1≤Σ1≤Σmax 1(12)
Whereσ(x) = 1//parenleftBig
1 + exp(−x)/parenrightBig
is the sigmoid function.
8Published in Transactions on Machine Learning Research (09/2023)
To solve Problem (12), ﬁrst, we focus on the scenario when the target variable has no missing values. In this
case, each data point contributes to the estimation of either (µ1,Σ1)or(µ0,Σ0), depending on its label.
Similar to the robust linear regression case, we can apply Algorithm 4 to estimate the conﬁdence intervals
forµi,Σiusing data points whose target variable equals i(y=i).
Obviously,theobjectivefunctionisconvexin wsincethelogisticregressionlossisconvex,andtheexpectation
of loss can be seen as a weighted summation, which is convex. Thus, ﬁxing µ,Σthe outer minimization
problem can be solved with respect to wusing standard ﬁrst-order methods such as gradient descent.
Although the robust reformulation of logistic regression stated in (12) is convex in wand concave in µ0and
µ1, the inner maximization problem is intractable with respect to Σ0andΣ1. We approximate Problem (12)
in the following manner:
min
wmax
µ0,Σ0,µ1,Σ1π1Ex|y=1∼N(µ1,Σ1)/bracketleftBig
−log/parenleftBig
σ(wTx)/parenrightBig/bracketrightBig
+π0Ex|y=0∼N(µ0,Σ0)/bracketleftBig
−log/parenleftBig
1−σ(wTx)/parenrightBig/bracketrightBig
,
s.t. µmin 0≤µ0≤µmax 0
µmin 1≤µ1≤µmax 1
Σ0∈{Σ01,Σ02,...,Σ0k}
Σ1∈{Σ11,Σ12,...,Σ1k},(13)
whereπ1=P(y= 1)andπ0=P(y= 0). To compute optimal µ0andµ1, we have:
max
µ1Ex∼N(µ1,Σ1)/bracketleftBig
−log/parenleftBig
σ(wTx)/parenrightBig/bracketrightBig
s.t.µmin≤µ1≤µmax (14)
Theorem 3. Leta[i]be thei-th element of vector a. The optimal solution of Problem (14)has the following
form:
µ∗
1[i] =/braceleftBigg
µmax[i],ifw[i]≤0
µmin[i],ifw[i]>0.(15)
Notethatwerelaxed(12)bytakingthemaximizationproblemoveraﬁnitesetof Σestimations. Weestimate
eachΣby bootstrapping on the available data using Algorithm 4. Deﬁne fi(w)as:
fi(w) =π1Ex∼N(µ∗
1,Σi1)/bracketleftBig
−log/parenleftBig
σ(wTx)/parenrightBig/bracketrightBig
(16)
Similarly, we can deﬁne:
gi(w) =π0Ex∼N(µ∗
0,Σi0)/bracketleftBig
−log/parenleftBig
1−σ(wTx)/parenrightBig/bracketrightBig
(17)
Since the maximization problem is over a ﬁnite set, we can rewrite Problem (13) as:
min
wmax
i,j∈{1,...,k}fi(w) +gj(w) = min
wmax
p1,...,pk,q1,...,qkk/summationdisplay
i=1pifi(w) +k/summationdisplay
j=1pigj(w)
s.t./summationtextk
i=1pi= 1, pi≥0
/summationtextk
j=1qj= 1, qj≥0(18)
Since the maximum of several functions is not necessarily smooth (diﬀerentiable), we add a quadratic reg-
ularization term to the maximization problem, accelerating the convergence rate (Nouiehed et al., 2019) as
follows:
min
wmax
p1,...,pk,q1,...,qkk/summationdisplay
i=1pifi(w)−δk/summationdisplay
i=1p2
i+k/summationdisplay
j=1qjgj(w)−δk/summationdisplay
j=1q2
j
s.t./summationtextk
i=1pi= 1, pi≥0
/summationtextk
j=1qj= 1, qj≥0(19)
9Published in Transactions on Machine Learning Research (09/2023)
First, we show how to solve the inner maximization problem. Note that the pi’s andqi’s are independent. We
show how to ﬁnd optimal pi’s. Optimizing with respect to qi’s is similar. Since the maximization problem
is a constrained quadratic program, we can write the Lagrangian function as follows:
max
p1,...,pkk/summationdisplay
i=1pifi(w)−δk/summationdisplay
i=1p2
i−λ(k/summationdisplay
i=1pi−1)
s.t. pi≥0(20)
Having the optimal λ, the above problem has a closed-form solution with respect to each pi, which can be
written as:
p∗
i=/bracketleftbigg−λ+fi
2δ/bracketrightbigg
+
Sincep∗
iis a non-increasing function with respect to λ, we can ﬁnd the optimal value of λusing the following
bisection algorithm. Algorithm 2 demonstrates how to ﬁnd an /epsilon1-optimalλandp∗
i’s eﬃciently using the
bisection idea.
Algorithm 2 Finding the optimal λandpi’s using the bisection idea
1:Initialize :λlow= 0,λhigh= maxifi,pi= 0∀i∈{1,2,...,k}.
2:while|/summationtextn
i=1pk−1|>/epsilon1do
3:λ=λlow+λhigh
2
4:Setpi= [−λ+fi
2δ]+∀i∈{1,2,...,k}
5:if/summationtextk
i=1pi<1then
6:λhigh=λ
7:else
8:λlow=λ
9:returnλ,p1,p2,...,pk.
Remark 4. An alternative method for ﬁnding optimal λ, andpi’s is to sort fivalues inO(klogk)ﬁrst, and
then ﬁnding the smallest fisuch that if we set λ=fi, the sum of pi’s is bigger than 1(letjbe the index
of that value). Without loss of generality, assume that f1≤···≤fk. Then,/summationtextk
i=j−λ+fi
2δ= 1, which has a
closed-form solution with respect to λ.
To update w, we need to solve the following optimization problem:
min
wk/summationdisplay
i=1p∗
ifi(w) +k/summationdisplay
j=1q∗
jgi(w), (21)
Similar to the standard statistical learning framework, we solve the following empirical risk minimization
problem by applying the gradient descent to won a ﬁnite data sample. Deﬁne ˆfias follows:
ˆfi(w) =π1n/summationdisplay
t=1/bracketleftBig
−log/parenleftBig
σ(wTxt)/parenrightBig/bracketrightBig
, (22)
where x1,...,xnare generated from the distribution N(µ∗
1,Σ1i). The empirical risk minimization problem
can be written as follows:
min
wk/summationdisplay
i=1p∗
iˆfi(w) +k/summationdisplay
j=1q∗
jˆgi(w), (23)
Algorithm 3 summarizes the robust linear discriminant analysis method for the case where the label of all
data points is available. Theorem 5 demonstrates the convergence of gradient descent algorithm applied
to (23) inO/parenleftBig
k
/epsilon1log(M
/epsilon1)/parenrightBig
iterations to an /epsilon1-optimal solution.
10Published in Transactions on Machine Learning Research (09/2023)
Algorithm 3 Robust Quadratic Discriminant Analysis in the Presence of Missing Values
1:Input:X0,X1: matrix of data points with labels 0and1respectively, T:Number of iterations, α:
Step-size.
2:Estimateµmin 0andµmax 0using the available entries of X0.
3:Estimateµmin 1andµmax 1using the available entries of X1.
4:Estimate Σ01,...,Σ0kusing bootstrap estimator on the available data of X0.
5:Estimate Σ11,...,Σ1kusing bootstrap estimator on the available data of X1.
6:fori= 1,...,Tdo
7:Computeµ∗
1andµ∗
0by Equation (15).
8:Find optimal p1,...,pk, andq1,...,qkusing Algorithm 2.
9:w=w−α/parenleftBigg
/summationtextk
i=1p∗
i∇ˆfi(w) +/summationtextk
j=1q∗
j∇ˆgi(w)/parenrightBigg
Theorem 5. Assume that M= maxifi. Gradient descent algorithm requires O/parenleftBig
k
/epsilon1log(M
/epsilon1)/parenrightBig
gradient evalu-
ations for converging to an /epsilon1-optimal saddle point of the optimization problem (23).
In Appendix F, we extend the methodology to the case where ycontains missing entries.
5 Experiments
In this section, we evaluate RIFLE’s performance on a diverse set of inference tasks in the presence of missing
values. We compare RIFLE’s performance to several state-of-the-art approaches for data imputation on
synthetic and real-world datasets. The experiments are designed in a manner that the sensitivity of the
model to factors such as the number of samples, data dimension, types, and proportion of missing values
can be evaluated. The description of all datasets used in the experiments can be found in Appendix I.
5.1 Evaluation Metrics
We need access to the ground-truth values of the missing entries to evaluate RIFLE and other state-of-the-art
imputation approaches. Hence, we artiﬁcially mask a proportion of available data entries and predict them
with diﬀerent imputation methods. A method performs better than others if the predicted missing entries
are closer to the ground-truth values. To measure the performance of RIFLE and the existing approaches on
a regression task for a given test dataset consisting of Ndata points, we use normalized root mean squared
error (NRMSE), deﬁned as:
NRMSE =/radicalBig
1
N/summationtextN
i=1(yi−ˆyi)2
/radicalBig
1
N/summationtextN
i=1(yi−¯y)2
whereyi,ˆyi, and ¯yrepresent the true value of the i-th data point, the predicted value of the i-th data point,
and the average of true values of data points, respectively. In all experiments, generated missing entries
follow either a missing completely at random (MCAR) or a missing not at random (MNAR) pattern. A
discussion on the procedure of generating these patterns can be found in Appendix G.
5.2 Tuning Hyper-parameters of RIFLE
Thehyper-parameter cin(7)controlstherobustnessofthemodelbyadjustingthesizeofconﬁdenceintervals.
This parameter is tuned by performing a cross-validation procedure over the set {0.1,0.25,0.5,1,2,5,10,20,
50,100}, and the one with the lowest NMRSE is chosen. The default value in the implementation is c= 1
since it consistently performs well over diﬀerent experiments. Furthermore, λ, the hyper-parameter for
the ridge regression regularizer, is tuned by choosing 20%of the data as the validation set from the set
{0.01,0.1,0.5,1,2,5,10,20,50}. To tuneK, the number of bootstrap samples for estimating the conﬁdence
intervals, we tried 10,20,50, and 100. No signiﬁcant diﬀerence is observed in terms of the test performance
for the above values.
11Published in Transactions on Machine Learning Research (09/2023)
Furthermore, we tune the hyper-parameters of the competing packages as follows. For KNN-Imputer (Troy-
anskaya et al., 2001), we try {2,10,20,50}for the number of neighbors ( K) and pick the one with the highest
performance. For MICE (Buuren & Groothuis-Oudshoorn, 2010) and Amelia (Honaker et al., 2011), we gen-
erate 5diﬀerent imputed data and pick the one with the highest performance on the test data. MissForest
has multiple hyper-parameters. We keep the criterion as “MSE” since our performance evaluation measure
is NRMSE. Moreover, we tune the number of iterations and number of estimations (number of trees) by
checking values from {5,10,20}and{50,100,200}, respectively. We do not change the structure of the neural
networks for MIDA (Gondara & Wang, 2018) and GAIN (Yoon et al., 2018), and the default versions are
performed for imputing datasets.
5.3 RIFLE Consistency
InTheroem2Part(a), wedemonstratedthatRIFLEisconsistent. InFigure3, weinvestigatetheconsistency
Figure 3: Comparing the consistency of RIFLE, MissForest, KNN Imputer, MICE, Amelia, and Expectation
Maximization methods on a synthetic dataset containing 40%of missing values.
of RIFLE on synthetic datasets with diﬀerent proportions of missing values. The synthetic data has 50input
features following a jointly normal distribution with the mean whose entries are randomly chosen from the
interval (−100,100). Moreover, thecovariancematrixequals Σ =SSTwhereSelementsarerandomlypicked
from (−1,1). The dimension of Sis50×20. The target variable is a linear function of input features added
to a mean zero normal noise with a standard deviation of 0.01. As depicted in Figure 3, RIFLE requires
fewer samples to recover the ground-truth parameters of the model compared to MissForest, KNN Imputer,
Expectation Maximization (Dempster et al., 1977), and MICE. Amelia’s performance is signiﬁcantly good
sincethepredictorshaveajoint normaldistributionandthelinearunderlyingmodel. Notethatby increasing
the number of samples, the NRMSE of our framework converges to 0.01, which is the standard deviation of
the zero-mean Gaussian noise added to each target value (the dashed line).
5.4 Data Imputation via RIFLE
As explained in Section 3, while the primary goal of RIFLE is to learn a robust regression model in the
presence of missing values, it can also be used as an imputation tool. We run RIFLE and several state-of-
the-art approaches on ﬁve datasets from the UCI repository (Dua & Graﬀ, 2017) (Spam, Housing, Clouds,
Breast Cancer, and Parkinson datasets) with diﬀerent proportions of MCAR missing values (the description
of the datasets can be found in Appendix I). Then, we compute the NMRSE of imputed entries. Table 1
shows the performance of RIFLE compared to other approaches for the datasets where the proportion of
missing values are relatively high/parenleftBig
n(1−p)
d≈O(1)/parenrightBig
. RIFLE outperforms these methods in almost all cases
andperformsslightlybetterthanMissForest, whichusesahighlynon-linearmodel(randomforest)toimpute
missing values.
12Published in Transactions on Machine Learning Research (09/2023)
Dataset Name RIFLE QRIFLE MICE Amelia GAIN MissForest MIDA EM
Spam (30%) 0.87±0.0090.82±0.0091.23±0.0121.26±0.0070.91±0.0050.90±0.0130.97±0.008 0.94±0.004
Spam (50%) 0.90±0.0130.86±0.0141.29±0.0181.33±0.0240.93±0.0150.92±0.0110.99±0.011 0.97±0.008
Spam (70%) 0.92±0.0170.91±0.0191.32±0.0281.37±0.0320.97±0.0140.95±0.0160.99±0.018 0.98±0.017
Housing (30%) 0.86±0.0150.89±0.0181.03±0.0241.02±0.0160.82±0.0150.84±0.0180.93±0.025 0.95±0.011
Housing (50%) 0.88±0.0210.90±0.0241.14±0.0291.09±0.0270.88±0.0190.88±0.0180.98±0.029 0.96±0.016
Housing (70%) 0.92±0.0260.95±0.0281.22±0.0361.18±0.0380.95±0.0270.93±0.0241.02±0.037 0.98±0.017
Clouds (30%) 0.81±0.0180.79±0.0190.98±0.0241.04±0.0270.76±0.0210.71±0.0110.83±0.022 0.86±0.013
Clouds (50%) 0.84±0.0260.84±0.0281.10±0.0411.13±0.0460.82±0.0270.75±0.0230.88±0.033 0.89±0.018
Clouds (70%) 0.87±0.0290.90±0.0331.16±0.0441.19±0.0480.89±0.0350.81±0.0310.93±0.044 0.92±0.023
Breast Cancer (30%) 0.52±0.0230.54±0.0270.74±0.0310.81±0.0320.58±0.0240.55±0.0160.70±0.026 0.67±0.014
Breast Cancer (50%) 0.56±0.0260.59±0.0270.79±0.0290.85±0.0330.64±0.0250.59±0.0220.76±0.035 0.69±0.022
Breast Cancer (70%) 0.59±0.0310.65±0.0340.86±0.0420.92±0.0440.70±0.0370.63±0.0280.82±0.035 0.67±0.014
Parkinson (30%) 0.57±0.0160.55±0.0160.71±0.0190.67±0.0210.53±0.0150.54±0.0100.62±0.017 0.64±0.011
Parkinson (50%) 0.62±0.0220.64±0.0250.77±0.0290.74±0.0340.61±0.0220.65±0.0140.71±0.027 0.69±0.022
Parkinson (70%) 0.67±0.0270.74±0.0330.85±0.0380.82±0.0370.69±0.0310.73±0.0220.78±0.038 0.75±0.029
Table 1: Performance comparison of RIFLE, QRIFLE (Quadratic RIFLE), and state-of-the-art methods on
several UCI datasets. We applied to impute methods on three diﬀerent missing-value proportions for each
dataset. The best imputer is highlighted with bold font, and the second-best imputer is underlined. Each
experiment is done 5times, and the average and the standard deviation of performances are reported.
5.5 Sensitivity of RIFLE to the Number of Samples and Proportion of Missing Values
In this section, we analyze the sensitivity of RIFLE and other state-of-the-art approaches to the number of
samples and the proportion of missing values. In the experiment in Figure 4, we create 5datasets containing
Figure 4: Performance Comparison of RIFLE, MICE, and MissForest on four UCI datasets: Parkinson,
Spam, Wave Energy Converter, and Breast Cancer. For each dataset, we count the number of features that
each method outperforms the others.
40%,50%,60%,70%, and 80%ofMCARmissingvalues, respectively, forfourrealdatasets(Spam, Parkinson,
Wave Energy Converter, and Breast Cancer) from UCI Repository (Dua & Graﬀ, 2017) (the description of
the datasets can be found in Appendix I). Given a feature in a dataset containing missing values, we say an
imputer wins that feature if the imputation error in terms of NRMSE for that imputer is less than the error
13Published in Transactions on Machine Learning Research (09/2023)
of the other imputers. Figure 4 reports the number of features won by each imputer on the created datasets
described above. As we observe, the number of wins for RIFLE increases as we increase the proportion of
missing values. This observation shows that the sensitivity of RIFLE as an imputer to the proportion of
missing values is less than MissForest and MICE in general.
Figure 5: Sensitivity of RIFLE, MissForest, Amelia, KNN Imputer, MIDA, and Mean Imputer to the
percentage of missing values on the Drive dataset. Increasing the percentage of missing value entries degrades
the benchmarks’ performance compared to RIFLE. KNN-imputer implementation cannot be executed on
datasets containing 80%(or more) missing entries. Moreover, Amelia and MIDA do not converge to a
solution when the percentage of missing value entries is higher than 70%.
Figure 4 does not show how the increase in the proportion of missing values changes the NRMSE of imputers.
Next, we analyze the sensitivity of RIFLE and several imputers to change in missing value proportions.
Fixing the proportion of missing values, we generate 10random datasets containing missing values in random
locations on the Drive dataset (the description of datasets is available in Appendix I). We impute the missing
values for each dataset with RIFLE, MissForest, Mean Imputation, and MICE. Figure 5 shows the average
and the standard deviation of these 4imputers’ performances for diﬀerent proportions of missing values
(10%to90%). Figure 5 depicts the sensitivity of MissForest and RIFLE to the proportion of missing values
in the Drive dataset. We select 400 data points for each experiment with diﬀerent proportions of missing
values (from 10%to90%) and report the average NRMSE of imputed entries. Finally, in Figure 6, we have
evaluated RIFLE and other methods on the BlogFeedback dataset (see Appendix I) containing 40%missing
values. The results show that RIFLE’s performance is less sensitive to decreasing the number of samples.
5.6 Performance Comparison on Real Datasets
In this section, we compare the performance of RIFLE to several state-of-the-art approaches, including
MICE (Buuren & Groothuis-Oudshoorn, 2010), Amelia (Honaker et al., 2011), MissForest (Stekhoven &
Bühlmann, 2012), KNN Imputer (Raghunathan et al., 2001), and MIDA (Gondara & Wang, 2018). There
are two primary ways to do this. One method to predict a continuous target variable in a dataset with
many missing values is ﬁrst to impute the missing data with a state-of-the-art package, then run a linear
regression. An alternative approach is to directly learn the target variable, as we discussed in Section 3.
Table 2 compares the performance of mean imputation, MICE, MIDA, MissForest, and KNN to that of
RIFLE on three datasets: NHANES, Blog Feedback, and superconductivity. Both Blog Feedback and
Superconductivity datasets contain 30%of MNAR missing values generated by Algorithm 9, with 10000and
20000training samples, respectively. The description of the NHANES data and its distribution of missing
values can be found in Appendix I.
14Published in Transactions on Machine Learning Research (09/2023)
Figure 6: Sensitivity of RIFLE, MissForest, MICE, Amelia, Mean Imputer, KNN Imputer, and MIDA to the
number of samples for the imputations of Blog Feedback dataset containing 40%of MCAR missing values.
When the number of samples is limited, RIFLE outperforms other methods, and its performance is very
close to the non-linear imputer MissForest for larger samples.
Eﬃciency of RIFLE: We perform RIFLE for 1000iterations and the step size of 0.01in the above
experiments. At each iteration, the main operation is to ﬁnd the optimal θfor any given bandC. The
average time of each method on each dataset is reported in Table 5 in Appendix L. The main reason for
the time eﬃciency of RIFLE compared to MICE, MissForest, MIDA, and KNN Imputer is that it directly
predicts the target variable without imputation of all missing entries.
MethodsDatasets
Super Conductivity Blog Feedback NHANES
Regression on Complete Data 0.4601 0.7432 0.6287
RIFLE 0.4873±0.0036 0.8326±0.00850.6304±0.0027
Mean Imputer + Regression 0.6114±0.0006 0.9235±0.0003 0.6329±0.0008
MICE + Regression 0.5078±0.0124 0.8507±0.0325 0.6612±0.0282
EM + Regression 0.5172±0.0162 0.8631±0.0117 0.6392±0.0122
MIDA Imputer + Regression 0.5213±0.0274 0.8394±0.0342 0.6542±0.0164
MissForest 0.4925±0.00730.8191±0.0083 0.6365±0.0094
KNN Imputer 0.5438±0.0193 0.8828±0.0124 0.6427±0.0135
Table 2: Normalized RMSE of RIFLE and several state-of-the-art Methods on Superconductivity, blog
feedback, and NHANES datasets. The ﬁrst two datasets contain 30%Missing Not At Random (MNAR)
missing values in the training phase generated by Algorithm 9. Each method applied 5times to each
dataset, and the result is reported as the average performance ±standard deviation of experiments in terms
of NRMSE.
Since MICE and MIDA cannot predict values during the test phase without data imputation, we use them
in a pre-processing stage to impute the data. Then we apply the linear regression to the imputed dataset.
On the other hand, RIFLE, KNN imputer, and MissForest can predict the target variable without imputing
the training dataset. Table 2 shows that RIFLE outperforms all other state-of-the-art approaches executed
on the three mentioned datasets. In particular, RIFLE outperforms MissForest, while the underlying model
RIFLE uses is simpler (linear) compared to the nonlinear random forest model utilized by Missforest.
15Published in Transactions on Machine Learning Research (09/2023)
Number of Training Data PointsMethod
LDA Robust LDA Robust QDA
50 52.38%±3.91% 62.14%±1.78% 61.36%±1.62%
100 61.24%±1.89% 68.46%±1.04% 70.07%±0.95%
200 73.49%±0.97% 73.35%±0.67% 73.51%±0.52%
Table 3: Sensitivity of Linear Discriminant Analysis, Robust LDA (Common Covariance Matrices), and
Robust QDA (Diﬀerent Covariance matrices for two groups) to the number of training samples.
5.6.1 Performance of RIFLE on Classiﬁcation Tasks
In Section 4, we discussed how to specialize RIFLE to robust normal discriminant analysis in the presence
of missing values. Since the maximization problem over the second moments of the data ( Σ) is intractable,
we solved the maximization problem over a set of kcovariance matrices estimated by bootstrap sampling.
To investigate the eﬀect of choosing kon the performance of the robust classiﬁer, we train robust normal
discriminant analysis models for diﬀerent values of kon two training datasets (Avila and Magic) containing
40%MCAR missing values. The description of the datasets can be found in Appendix I. For k= 1, there is
no maximization problem, and thus, it is equivalent to the classiﬁer proposed in Fung & Wrobel (1989). As
shown in Figure 7, increasing the number of covariance estimations generally enhances the accuracy of the
classiﬁer in the test phase. However, as shown in Theorem 5, the required time for completing the training
phase grows linearly regarding the number of covariance estimations.
Figure 7: Eﬀect of the number of covariance estimations on the performance (left) and run time (right) of
robust LDA on Avila and Magic datasets. Increasing the number of covariance estimations ( k) improves the
model’s accuracy on the test data. However, it takes longer training time.
5.6.2 Comparison of Robust Linear Regression and Robust QDA
An alternative approach to the robust QDA presented in Section 4 is to apply the robust linear regression
algorithm (Section 3) and mapping the solutions to each one of the classes by thresholding (positive value
maps to Label 1and negative values to label −1).
Table 4 compares the performance of two classiﬁers on three diﬀerent datasets. As demonstrated in the table,
when all features are continuous, quadratic discriminant analysis has a better performance. It shows the
QDA model relies highly on the normality assumption, while robust linear regression handles the categorical
features better than robust QDA.
Limitations and Future Directions: The proposed framework for robust regression in the presence of
missingvaluesislimitedtolinearmodels. WhileinAppendixE,weusepolynomialkernelstoapplynon-linear
transformations on the data, such an approach can potentially increase the number of missing values in the
16Published in Transactions on Machine Learning Research (09/2023)
Accuracy of Methods
Dataset Feature Type RIFLE Robust QDA MissForest MICE KNN Imputer EM
Glass Identiﬁcation Continuous 67.12%±1.84% 69.54%±1.97% 65.76%±1.49% 62.48%±2.45% 60.37% +±1.12% 68.21% +±0.94%
Annealing Mixed 63.41%±2.44% 59.51%±2.21% 64.91%±1.35%60.66%±1.59% 57.44%±1.44% 59.43% +±1.29%
Abalone Mixed 68.41%±0.74% 63.27%±0.76% 69.40%±0.42%63.12%±0.98% 62.43%±0.38% 62.91% +±0.37%
Lymphography Discrete 66.32%±1.05% 58.15%±1.21% 66.11%±0.94% 55.73%±1.24 57.39%±0.88% 59.55% +±0.68%
Adult Discrete 72.42%±0.06% 60.36%±0.08 70.34%±0.03% 63.30%±0.14% 60.14%±0.00 60.69% +±0.01%
Table 4: Accuracy of RIFLE, MICE, KNN-Imputer, Expectation Maximization (EM), and Robust QDA on
diﬀerent discrete, mixed, and continuous datasets. Robust QDA can perform better than other methods
when the input features are continuous, and the target variable is discrete. However, RIFLE results in higher
accuracy in mixed and discrete settings.
kernel space generated by the composition of the original features. A future direction is to develop eﬃcient
algorithms for non-linear regression models such as multi-layer neural networks, decision tree regressors,
gradient boosting regressors, and support vector regression models. In the case of robust classiﬁcation, the
methodology is extendable to any loss beyond quadratic discriminant analysis. Unlike the regression case,
a limitation of the proposed method for robust classiﬁcation is its reliance on the Gaussianity assumption
of data distribution (conditioned on each data label). A natural extension is to assume the underlying data
distribution follows a mixture of Gaussian distributions.
Conclusion: In this paper, we proposed a distributionally robust optimization framework over the distri-
butions with the low-order marginals within the estimated conﬁdence intervals for inference and imputation
of datasets in the presence of missing values. We developed algorithms for regression and classiﬁcation
with convergence guarantees. The method’s performance is evaluated on synthetic and real datasets with
diﬀerent numbers of samples, dimensions, missing value proportions, and types of missing values. In most
experiments, RIFLE consistently outperforms other existing methods.
Acknowledgments
This work was supported by the NIH/NSF Grant 1R01LM013315-01, the NSF CAREER Award CCF-
2144985, and the AFOSR Young Investigator Program Award FA9550-22-1-0192.
17