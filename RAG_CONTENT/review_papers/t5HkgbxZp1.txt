Published in Transactions on Machine Learning Research (09/2022)
On the Near-Optimality of Local Policies in Large Coopera-
tive Multi-Agent Reinforcement Learning
Washim Uddin Mondal wmondal@purdue.edu
School of IE and CE, Purdue University
Vaneet Aggarwal vaneet@purdue.edu
School of IE and ECE, Purdue University
Satish V. Ukkusuri sukkusur@purdue.edu
Lyles School of Civil Engineering, Purdue University
Reviewed on OpenReview: https: // openreview. net/ forum? id= t5HkgbxZp1
Abstract
We show that in a cooperative N-agent network, one can design locally executable policies
for the agents such that the resulting discounted sum of average rewards (value) well ap-
proximates the optimal value computed over all (including non-local) policies. Specifically,
we prove that, if |X|,|U|denote the size of state, and action spaces of individual agents,
then for sufficiently small discount factor, the approximation error is given by O(e)where
e≜1√
N/bracketleftig/radicalbig
|X|+/radicalbig
|U|/bracketrightig
. Moreover, in a special case where the reward and state transition
functions are independent of the action distribution of the population, the error improves
toO(e)wheree≜1√
N/radicalbig
|X|. Finally, we also devise an algorithm to explicitly construct a
local policy. With the help of our approximation results, we further establish that the con-
structed local policy is within O(max{e,ϵ})distance of the optimal policy, and the sample
complexity to achieve such a local policy is O(ϵ−3), for anyϵ>0.
1 Introduction
Multi-agent system (MAS) is a powerful abstraction that models many engineering and social science prob-
lems. For example, in a traffic control network, each controller at a signalised intersection can be depicted as
an agent that decides the duration of red and green times at the adjacent lanes based on traffic flows (Chen
et al., 2020). As flows at different neighboring intersections are inter-dependent, the decisions taken by one
intersection have significant ramifications over the whole network. How can one come up with a strategy
that steers this tangled mesh to a desired direction is one of the important questions in the multi-agent
learning literature. Multi-agent reinforcement learning (MARL) has emerged as one of the popular solutions
to this question. Cooperative MARL, which is the main focus of this paper, constructs a policy(decision rule)
for each agent that maximizes the aggregate cumulative rewardof all the agents by judiciously executing
exploratory and exploitative trials. Unfortunately, the size of the joint state-space of the network increases
exponentially with the number of agents. This severely restricts the efficacy of MARL in the large population
regime.
There have been many attempts in the literature to circumvent this curse of dimensionality . For example,
one of the approaches is to restrict the policies of each agent to be local. This essentially means that each
agent ought to take decisions solely based on its locally observable state. In contrast, the execution of a global
policy requires each agent to be aware of network-wide state information. Based on how these local policies
are learnt, the existing MARL algorithms can be segregated into two major categories. In independent
Q-learning (IQL) (Tan, 1993), the local policies of each agent are trained independently. On the other hand,
in Centralised Training with Decentralised Execution (CTDE) based paradigm, the training of local policies
1Published in Transactions on Machine Learning Research (09/2022)
are done in a centralised manner (Oliehoek et al., 2008; Kraemer and Banerjee, 2016). Despite the empirical
success for multiple applications (Han et al., 2021; Feriani and Hossain, 2021), no theoretical convergence
guarantees have been obtained for either of these methods.
Recently, mean-fieldcontrol(MFC)(Ruthottoetal.,2020)isgainingpopularityasanothersolutionparadigm
to large cooperative multi-agent problems with theoretical optimality guarantees. The idea of MFC hinges
on the assertion that in an infinite pool of homogeneous agents, the statistics of the behaviour of the whole
population can be accurately inferred by observing only one representative agent. However, the optimal
policy given by MFC is, in general, non-local. The agents, in addition to being aware of their local states,
must also know the distribution of states in the whole population to execute these policies.
To summarize, on one hand, we have locally executable policies that are often empirically sound but have no
theoretical guarantees. On the other hand, we have MFC-based policies with optimality guarantees but those
require global information to be executed. Local executability is desirable for many practical application
scenarios where collecting global information is either impossible or costly. For example, in a network where
the state of the environment changes rapidly (e.g., vehicle-to-vehicle (V2V) type communications (Chen
et al., 2017)), the latency to collect network-wide information might be larger than the time it takes for
the environment to change. Consequently, by the time the global information are gathered, the states are
likely to transition to new values, rendering the collected information obsolete. The natural question that
arises in this context is whether it is possible to come up with locally executable policies with optimality
guarantees. Answering this question may provide a theoretical basis for many of the empirical studies in the
literature (Chu et al., 2019) that primarily use local policies as the rule book for strategic choice of actions.
But, in general, how close are these policies to the optimal one? In this article, we provide an answer to this
question.
1.1 Our Contribution
We consider a network of Ninteracting agents with individual state, and action space of size |X|, and
|U|respectively. We demonstrate that, given an initial state distribution, µ0, of the whole population, it
is possible to obtain a non-stationary locally executable policy-sequence ˜π={˜πt}t∈{0,1,···}such that the
average time-discounted sum of rewards (value) generated by ˜πclosely approximates the value generated
by the optimal policy-sequence, π∗
MARL. In fact, we prove that the approximation error is O(e)where
e≜1√
N/bracketleftig/radicalbig
|X|+/radicalbig
|U|/bracketrightig
(Theorem 1). We would like to clarify that the optimal policy-sequence, π∗
MARL, is,
in general, notlocally executable. In a special case where reward and transition functions are independent of
theactiondistributionofthepopulation, weshowthattheerrorcanbeimprovedto O(e)wheree≜1√
N/radicalbig
|X|
(Theorem 2).
Our suggested local policy-sequence, ˜πis built on top of the optimal mean-field policy sequence, π∗
MFthat
maximizes the infinite-agent value function. It is worth mentioning that π∗
MF, in general, is not local −agents
require the state-distribution of the N-agent network at each time-step in order to execute it. Our main
contribution is to show that if each agent uses infinite-agent state distributions (which can be locally and
deterministically computed if the initial distribution, µ0is known) as proxy for the N-agent distribution,
then the resulting time-discounted sum of rewards is not too far off from the optimal value generated by
π∗
MARL.
Finally, we devise a Natural Policy Gradient (NPG) based procedure (Algorithm 1) that approximately
computes the optimal mean-field policy-sequence, π∗
MF. Subsequently, in Algorithm 2, we exhibit how the
desired local policy can be extracted from π∗
MF. Applying the result from (Liu et al., 2020), we prove that
the local policy generated from Algorithm 2 yields a value that is at most O(max{ϵ,e})distance away from
the optimal value, and it requires at most O(ϵ−3)samples to arrive at the intended local policy for any ϵ>0.
1.2 Related Works
Single Agent RL: Tabular algorithms such as Q-learning (Watkins and Dayan, 1992), and SARSA (Rum-
mery and Niranjan, 1994) were the first RL algorithms adopted in single agent learning literature. Due to
their scalability issues, however, these algorithms could not be deployed for problems with large state-space.
2Published in Transactions on Machine Learning Research (09/2022)
Recently, neural network based Q-iteration (Mnih et al., 2015), and policy-iteration (Mnih et al., 2016)
algorithms have gained popularity as a substitute for tabular procedures. However, they are still inadequate
for large scale multi-agent problems due to the exponential blow-up of joint state-space.
Multi-Agent Local Policy: As discussed before, one way to introduce scalability into multi-agent learning
is to restrict the policies to be local. The easiest and perhaps the most popular way to train the local policies
is via Independent Q-learning (IQL) procedure which has been widely adapted in many application scenarios.
For example, many adaptive traffic signal control algorithms apply some form of IQL (Wei et al., 2019; Chu
et al., 2019). Unlike IQL, centralised training with decentralised execution (CTDE) based algorithms train
local policies in a centralised manner. One of the simplest CTDE based training procedure is obtained
via value decomposition network (VDN) (Sunehag et al., 2018) where the goal is to maximize the sum of
local Q-functions of the agents. Later, QMIX (Rashid et al., 2018) was introduced where the target was to
maximize a weighted sum of local Q-functions. Other variants of CTDE based training procedures include
WQMIX (Rashid et al., 2020), QTRAN (Son et al., 2019) etc. As clarified before, none of these procedures
provide any theoretical guarantee. Very recently, there have been some efforts to theoretically characterize
the performance of localised policies. However, these studies either assume the reward functions themselves
to be local, thereby facilitating no agent interaction in the reward model (Qu et al., 2020), or allow the
policies to take state-information from neighbouring agents as an input, thereby expanding the definition of
local policies (Lin et al., 2020; Koppel et al., 2021).
Mean-Field Control: An alternate way to bring scalability into multi-agent learning is to apply the con-
cept of mean-field control (MFC). Empirically, MFC based algorithms have been applied in many practical
scenarios ranging from congestion control (Wang et al., 2020), ride-sharing (Al-Abbasi et al., 2019) and
epidemic management (Watkins et al., 2016). Theoretically, it has been recently shown that both in homo-
geneous (Gu et al., 2021), and heterogeneous (Mondal et al., 2022a) population of agents, and in population
with non-uniform interactions (Mondal et al., 2022b), MFC closely approximates the optimal policy. More-
over, various model-free (Angiuli et al., 2022) and model-based (Pasztor et al., 2021) algorithms have been
proposed to solve MFC problems. Alongside standard MARL problems, the concept of MFC based solutions
has also been applied to other variants of reinforcement learning (RL) problems. For example, (Gast and
Gaujal, 2011) considers a system comprising of a single controller and Nbodies. The controller is solely
responsible for taking actions and the bodies are associated with states that change as functions of the chosen
action. At each time instant, the controller receives a reward that solely is a function of the current states.
The objective is to strategize the choice of actions as a function of states such that the cumulative reward
of the controller is maximized. It is proven that the above problem can be approximated via a mean-field
process.
2 Model for Cooperative MARL
We consider a collection of Ninteracting agents. The state of i-th agent,i∈ {1,···,N}at timet∈
{0,1,···,∞}is symbolized as xi
t∈XwhereXdenotes the collection of all possible states (state-space).
Each agent also chooses an action at each time instant from a pool of possible actions, U(action-space).
The action of i-th agent at time tis indicated as ui
t. The joint state and action of all the agents at time t
are denoted as xN
t≜{xi
t}N
i=1anduN
t≜{ui
t}N
i=1respectively. The empirical distributions of the joint states,
µN
tand actions,νN
tat timetare defined as follows.
µN
t(x) =1
NN/summationdisplay
i=1δ(xi
t=x),∀x∈X (1)
νN
t(u) =1
NN/summationdisplay
i=1δ(ui
t=u),∀u∈U (2)
whereδ(·)denotes the indicator function.
At timet, thei-th agent receives a reward ri(xN
t,uN
t)and its state changes according to the following state-
transition law: xi
t+1∼Pi(xN
t,uN
t). Note that the reward and the transition function not only depend on the
3Published in Transactions on Machine Learning Research (09/2022)
state and action of the associated agent but also on the states, and actions of other agents in the population.
This dependence makes the MARL problem difficult to handle. In order to simplify the problem, we assume
the reward and transition functions to be of the following form for some r:X×U×P (X)×P(U)→R, and
P:X×U×P (X)×P(U)→P(X)whereP(·)is the collection of all probability measures defined over its
argument set,
ri(xN
t,uN
t) =r(xi
t,ui
t,µN
t,νN
t) (3)
Pi(xN
t,uN
t) =P(xi
t,ui
t,µN
t,νN
t) (4)
∀i∈{1,···,N},∀xN
t∈XN, and∀uN
t∈UN. Two points are worth mentioning. First, (3)and(4)suggest
that the reward and transition functions of each agent take the state and action of that agent, along with
empirical state and action distributions of the entire population as their arguments. As a result, the agents
can influence each other only via the distributions µN
t,νN
twhich are defined by (1),(2)respectively. Second,
the functions r,Pare taken to be the same for every agent. This makes the population homogeneous.
Such assumptions are common in the mean-field literature and holds when the agents are identical and
exchangeable.
We define a policy to be a (probabilistic) rule that dictates how a certain agent must choose its action
for a given joint state of the population. Mathematically, a policy πis a mapping of the following form,
π:XN→P(U). Letπi
tdenote the policy of i-th agent at time t. Due to (3)and(4), we can, without loss
of generality, write the following equation for some π:X×P (X)→U.
πi
t(xN
t) =π(xi
t,µN
t) (5)
In other words, one can equivalently define a policy to be a rule that dictates how an agent should choose
its action given its state and the empirical state-distribution of the entire population. Observe that, due
to agent-homogeneity, the policy function, π, of each agent are the same. With this revised definition,
let the policy of any agent at time tbe denoted as πt, and the sequence of these policies be indicated as
π≜{πt}t∈{0,1,···}. For a given joint initial state, xN
0, the population-average value of the policy sequence π
is defined as follows.
vMARL (xN
0,π)≜1
NN/summationdisplay
i=1E/bracketleftigg∞/summationdisplay
t=0γtri(xN
t,uN
t)/bracketrightigg
=1
NN/summationdisplay
i=1E/bracketleftigg∞/summationdisplay
t=0γtr(xi
t,ui
t,µN
t,νN
t)/bracketrightigg
(6)
where the expectation is computed over all state-action trajectories induced by the policy-sequence, πand
γ∈[0,1)is the discount factor. The target of MARL is to maximize vMARL (xN
0,·)over all policy sequences,
π. Let the optimal policy sequence be π∗
MARL≜{π∗
t,MARL}t∈{0,1,···}. Note that, in order to execute the
policyπ∗
t,MARL, in general, the agents must have knowledge about their own states at time tas well as the
empirical state-distribution of the entire population at the same instant. As stated previously, the collection
of population-wide state information is a costly process in many practical scenarios. In the subsequent
sections of this article, our target, therefore, is to identify (a sequence of) local policies that the agents can
execute solely with the knowledge of their own states. Additionally, the policy must be such that its value
(expected time-discounted cumulative reward) is close to that generated by the optimal policy-sequence,
π∗
MARL.
3 The Mean-Field Control (MFC) Framework
Mean-Field Control (MFC) considers a scenario where the agent population size is infinite. Due to the
homogeneity of the agents, in such a framework, it is sufficient to track only one representative agent. We
denote the state and action of the representative agent at time tasxt∈Xandut∈U, respectively, while
the state and action distributions of the infinite population at the same instance are indicated as µ∞
t∈P(X)
andν∞
t∈P(U), respectively. For a given sequence of policies π≜{πt}t∈{0,1,···}and the state distribution
µ∞
tat timet, the action distribution ν∞
tat the same instant can be computed as follows.
ν∞
t=νMF(µ∞
t,πt)≜/summationdisplay
x∈Xπt(x,µ∞
t)µ∞
t(x) (7)
4Published in Transactions on Machine Learning Research (09/2022)
In a similar fashion, the state-distribution at time t+ 1can be evaluated as shown below.
µ∞
t+1=PMF(µ∞
t,πt)≜/summationdisplay
x∈X/summationdisplay
u∈UP(x,u,µ∞
t,νMF(µ∞
t,πt))πt(x,µ∞
t)(u)µ∞
t(x) (8)
Finally, the average reward at time tcan be expressed as follows.
rMF(µ∞
t,πt)≜/summationdisplay
x∈X/summationdisplay
u∈Ur(x,u,µ∞
t,νMF(µ∞
t,πt))πt(x,µ∞
t)(u)µ∞
t(x) (9)
For an initial state distribution, µ0, the value of a policy sequence π={πt}t∈{0,1,···}is computed as shown
below.
vMF(µ0,π) =∞/summationdisplay
t=0γtrMF(µ∞
t,πt) (10)
The goal of Mean-Field Control is to maximize vMF(µ0,·)over all policy sequences π. Let the optimal policy
sequence be denoted as π∗
MF. (Gu et al., 2021) recently established that if the agents execute π∗
MFin theN-
agent MARL system, then the N-agent value function generated by this policy-sequence well-approximates
the value function generated by π∗
MARLwhenNis large. However, to execute the sequence π∗
MFin an
N-agent system, each agent must be aware of empirical state distributions, {µN
t}t∈{0,1,···}, along with its
own states at each time instant. In other words, when the i-th agent in an N-agent system executes the
policy sequence π∗
MF≜{π∗
t,MF}t∈{0,1,···}, it chooses action at time taccording to the following distribution,
ui
t∼π∗
t,MF(xi
t,µN
t). As stated in Section 2, the computation of empirical state distribution of the whole
population at each instant is a costly procedure. In the following subsection, we discuss how we can design
a near-optimal policy that does not require {µN
t}t∈{1,2,···}its execution.
Designing Local Policies: Note from Eq. (8) that the evolution of infinite-population state distribution,
µ∞
tis a deterministic equation. Hence, if the initial distribution µ0is disseminated among the agents, then
each agent can locally compute the distributions, {µ∞
t}fort >0. Consider the following policy sequence,
denoted as ˜π∗
MF≜{˜π∗
t,MF}t∈{0,1,···}.
˜π∗
t,MF(x,µ)≜π∗
t,MF(x,µ∞
t),∀x∈X,∀µ∈P(X),∀t∈{0,1,···} (11)
The sequence, π∗
MF≜{π∗
t,MF}t∈{0,1,···}, as mentioned before, is the optimal policy-sequence that maximizes
the mean-field value function, vMF(µ∞
0,·)for a given initial state distribution, µ0. Note that, in order to
execute the policy-sequence, ˜π∗
t,MFin anN-agent system, each agent must be aware of its own state as well as
the state distribution of an infinite agent system at each time instant −both of which can be obtained locally
provided it is aware of the initial distribution, µ0. In other words, the policy-sequence ˜π∗
MFcompletely
disregards the empirical distributions, {µN
t}t∈{1,2,···}(instead depends on {µ∞
t}t∈{1,2,···}), and therefore is
locally executable for t>0.
The above discussion well establishes ˜π∗
MFas a locally executable policy. However, it is not clear at this
point how well ˜π∗
MFperforms in comparison to the optimal policy, π∗
MARLin anN-agent system. Must
one embrace a significant performance deterioration to enforce locality? In the next section, we provide an
answer to this crucial question.
4 Main Result
Before stating the main result, we shall state a few assumptions that are needed to establish it. Our
first assumption is on the reward function, r, and the state-transition function, P, defined in (3)and(4),
respectively.
5Published in Transactions on Machine Learning Research (09/2022)
Assumption 1. The reward function, r, is bounded and Lipschitz continuous with respect to the mean-field
arguments. Mathematically, there exists MR,LR>0such that the following holds
(a)|r(x,u,µ,ν)|≤MR
(b)|r(x,u,µ1,ν1)−r(x,u,µ2,ν2)|≤LR[|µ1−µ2|1+|ν1−ν2|1]
∀x∈X,∀u∈U,∀µ1,µ2∈P(X), and∀ν1,ν2∈P(U). The function|·|denotesL1-norm.
Assumption 2. The state-transition function, P, is Lipschitz continuous with respect to the mean-field
arguments. Mathematically, there exists LP>0such that the following holds
(a)|P(x,u,µ1,ν1)−P(x,u,µ2,ν2)|≤LP[|µ1−µ2|1+|ν1−ν2|1]
∀x∈X,∀u∈U,∀µ1,µ2∈P(X), and∀ν1,ν2∈P(U).
Assumptions 1 and 2 ensure the boundedness and the Lipschitz continuity of the reward and state transition
functions. The very definition of transition function makes it bounded. So, it is not explicitly mentioned in
Assumption 2. These assumptions are commonly taken in the mean-field literature (Mondal et al., 2022a;
Gu et al., 2021). Our next assumption is on the class of policy functions.
Assumption 3. The class of admittable policy functions, Π, is such that every π∈Πsatisfies the following
for someLQ>0,
|π(x,µ1)−π(x,µ2)|1≤LQ|µ1−µ2|1
∀x∈X,∀µ1,µ2∈P(X).
Assumption 3 states that every policy is presumed to be Lipschitz continuous w. r. t. the mean-field state
distribution argument. This assumption is also common in the literature (Carmona et al., 2018; Pasztor
et al., 2021) and typically satisfied by Neural Network based policies with bounded weights. Corresponding
to the admittable policy class Π, we define Π∞≜Π×Π×···as the set of all admittable policy-sequences.
We now state our main result. The proof of Theorem 1 is relegated to Appendix A.
Theorem 1. LetxN
0be the initial states in an N-agent system and µ0be its associated empirical distribution.
Assumeπ∗
MF≜{π∗
t,MF}t∈{0,1,···}, andπ∗
MARL≜{π∗
t,MARL}t∈{0,1,···}to be policy sequences that maximize
vMF(µ0,·), andvMARL (xN
0,·)respectively within the class, Π∞. Let ˜π∗
MFbe a localized policy sequence
corresponding to π∗
MFwhich is defined by (11). If Assumptions 1 and 2 hold, then the following is true
wheneverγSP<1.
|vMARL (xN
0,π∗
MARL )−vMARL (xN
0,˜π∗
MF)|≤/parenleftbigg2
1−γ/parenrightbigg/bracketleftbiggMR√
N+LR√
N/radicalbig
|U|/bracketrightbigg
+1√
N/bracketleftig/radicalbig
|X|+/radicalbig
|U|/bracketrightig/parenleftbigg2SRCP
SP−1/parenrightbigg/bracketleftbigg1
1−γSP−1
1−γ/bracketrightbigg (12)
The parameters are defined as follows CP≜2 +LP,SR≜(MR+ 2LR) +LQ(MR+LR), andSP≜
(1 + 2LP) +LQ(1 +LP)whereMR,LR,LP, andLQare defined in Assumptions 1−3.
Theorem 1 accomplishes our goal of showing that, for large N, there exists a locally executable policy whose
N-agent value function is at most O/parenleftig
1√
N/bracketleftig/radicalbig
|X|+/radicalbig
|U|/bracketrightig/parenrightig
distance away from the optimal N-agent value
function. Note that the optimality gap decreases with increase in N. We, therefore, conclude that for
large population, locally-executable policies are near-optimal. Interestingly, this result also shows that the
optimality gap increases with |X|,|U|, the sizes of state, and action spaces. In the next section, we prove
that, if the reward, and transition functions do not depend on the action distribution of the population, then
it is possible to further improve the optimality gap.
6Published in Transactions on Machine Learning Research (09/2022)
5 Optimality Error Improvement in a Special Case
The key to improving the optimality gap of Theorem 1 hinges on the following assumption.
Assumption 4. The reward function, r, and transition function, Pare independent of the action distribution
of the population. Mathematically,
(a)r(x,u,µ,ν) =r(x,u,µ)
(b)P(x,u,µ,ν) =P(x,u,µ)
∀x∈X,∀u∈U,∀µ∈P(X), and∀ν∈P(U).
Assumption4considersascenariowheretherewardfunction, r, andthetransitionfunction, Pisindependent
of the action distribution. In such a case, the agents can influence each other only through the state
distribution. Clearly, this is a special case of the general model considered in Section 2. We would like to
clarify that although randPare assumed to be independent of the action distribution of the population,
they still might depend on the action taken by the individual agents. Assumption 4 is commonly assumed
in many mean-field related articles (Tiwari et al., 2019).
Theorem 2 (stated below) formally dictates the optimality gap achieved by the local policies under Assump-
tion 4. The proof of Theorem 2 is relegated to Appendix B.
Theorem 2. LetxN
0be the initial states in an N-agent system and µ0be its associated empirical distribution.
Assumeπ∗
MF≜{π∗
t,MF}t∈{0,1,···}, andπ∗
MARL≜{π∗
t,MARL}t∈{0,1,···}to be policy sequences that maximize
vMF(µ0,·), andvMARL (xN
0,·)respectively within the class, Π∞. Let ˜π∗
MFbe a localized policy sequence
corresponding to π∗
MFwhich is defined by (11). If Assumptions 1, 2, and 4 hold, then the following is true
wheneverγSP<1.
|vMARL (xN
0,π∗
MARL )−vMARL (xN
0,˜π∗
MF)|≤/parenleftbigg2
1−γ/parenrightbigg/bracketleftbiggMR√
N/bracketrightbigg
+1√
N/radicalbig
|X|/parenleftbigg4SR
SP−1/parenrightbigg/bracketleftbigg1
1−γSP−1
1−γ/bracketrightbigg (13)
The parameters are same as in Theorem 1.
Theorem 2 suggests that, under Assumption 4, the optimality gap for local policies can be bounded as
O(1√
N/radicalbig
|X|). Although the dependence of the optimality gap on Nis same as in Theorem 1, its dependence
on the size of state, action spaces has been reduced to O(/radicalbig
|X|)fromO(/radicalbig
|X|+/radicalbig
|U|)stated previously.
This result is particularly useful for applications where Assumption 4 holds, and the size of action-space is
large.
A natural question that might arise in this context is whether it is possible to remove the dependence of the
optimality gap on the size of state-space, |X|by imposing the restriction that r,Pare independent of the
state-distribution. Despite our effort, we could not arrive at such a result. This points towards an inherent
asymmetry between the roles played by state, and action spaces in mean-field approximation.
6 Roadmap of the Proof
In this section, we provide an outline of the proof of Theorem 1. The proof of Theorem 2 is similar. The
goal in the proof of Theorem 1 is to establish the following three bounds.
G0≜|vMARL (xN
0,π∗
MARL )−vMF(µ0,π∗
MARL )|=O/parenleftigg/radicalbig
|X|+/radicalbig
|U|√
N/parenrightigg
(14)
G1≜|vMARL (xN
0,π∗
MF)−vMF(µ0,π∗
MF)|=O/parenleftigg/radicalbig
|X|+/radicalbig
|U|√
N/parenrightigg
(15)
G2≜|vMARL (xN
0,˜π∗
MF)−vMF(µ0,π∗
MF)|=O/parenleftigg/radicalbig
|X|+/radicalbig
|U|√
N/parenrightigg
(16)
7Published in Transactions on Machine Learning Research (09/2022)
wherevMARL (·,·)andvMF(·,·)are value functions defined by (6)and(10), respectively. Once these bounds
are established, (13)can be proven easily. For example, observe that,
vMARL (xN
0,π∗
MARL )−vMARL (xN
0,˜π∗
MF)
=vMARL (xN
0,π∗
MARL )−vMF(µ0,π∗
MF) +vMF(µ0,π∗
MF)−vMARL (xN
0,˜π∗
MF)
(a)
≤vMARL (xN
0,π∗
MARL )−vMF(µ0,π∗
MARL ) +G2
≤G0+G2=O/parenleftigg/radicalbig
|X|+/radicalbig
|U|√
N/parenrightigg
(17)
Inequality (a) uses the fact that π∗
MFis a maximizer of vMF(µ0,·). Following a similar argument, the term
vMARL (xN
0,˜π∗
MF)−vMARL (xN
0,π∗
MARL )can be bounded by G1+G2=O/parenleftbigg√
|X|+√
|U|√
N/parenrightbigg
. Combining with
(17), we can establish that |vMARL (xN
0,π∗
MARL )−vMARL (xN
0,˜π∗
MF)|=O/parenleftbigg√
|X|+√
|U|√
N/parenrightbigg
.
To prove (14),(15), and (16), it is sufficient to show that the following holds
J0≜|vMARL (xN
0,¯π)−vMF(µ0,π)|=O/parenleftigg/radicalbig
|X|+/radicalbig
|U|√
N/parenrightigg
(18)
for suitable choice of policy-sequences, ¯π, andπ. This is achieved as follows.
•Note that the value functions are defined as the time-discounted sum of average rewards. To bound
J0definedin (18), wethereforefocusonthedifferencebetweentheempirical N-agentaveragereward,
1
N/summationtext
ir(¯xi
t,¯ui
t,¯µN
t,¯νN
t)attimetgeneratedfromthepolicy-sequence ¯π, andtheinfiniteagentaverage
reward,rMF(µ∞
t,πt)atthesameinstantgeneratedby π. Theparameters ¯xi
t,¯ui
t,¯µN
t,¯νN
trespectively
denote state, action of i-th agent, and empirical state, action distributions of N-agent system at
timetcorresponding to the policy-sequence ¯π. Also, by ¯µ∞
t,µ∞
t, we denote the infinite agent state
distributions at time tcorresponding to ¯π,πrespectively.
•The difference between1
N/summationtext
ir(¯xi
t,¯ui
t,¯µN
t,¯νN
t)andrMF(µ∞
t,πt)can be upper bounded by three
terms.
•The first term is|1
N/summationtext
ir(¯xi
t,¯ui
t,¯µN
t,¯νN
t)−rMF(¯µN
t,¯πt)|which is bounded as O(1
N/radicalbig
|U|)by Lemma
7 (stated in Appendix A.2).
•The second term is |rMF(¯µN
t,¯πt)−rMF(¯µ∞
t,¯πt)|which can be bounded as O(|¯µN
t−¯µ∞
t|)by using
the Lipschitz continuity of rMFestablished in Lemma 4 in Appendix A.1. The term |¯µN
t−¯µ∞
t|can
be further bounded as O/parenleftig
1√
N/bracketleftig/radicalbig
|X|+/radicalbig
|U|/bracketrightig/parenrightig
using Lemma 8 stated in Appendix A.2.
•Finally, the third term is |rMF(¯µ∞
t,¯πt)−rMF(µ∞
t,πt)|. Clearly, if ¯π=π, then this term is zero.
Moreover, if ¯πis localization of π, defined similarly as in (11), then the term is zero as well. This
is due to the fact that the trajectory of state and action distributions generated by ¯π,πare exactly
the same in an infinite agent system. This, however, may not be true in an N-agent system.
•For the above two cases, i.e., when ¯π,πare the same or one is localization of another, we can bound
|1
N/summationtext
ir(¯xi
t,¯ui
t,¯µN
t,¯νN
t)−rMF(µ∞
t,πt)|asO/parenleftig
1√
N/bracketleftig/radicalbig
|X|+/radicalbig
|U|/bracketrightig/parenrightig
by taking a sum of the above
three bounds. The bound obtained is, in general, tdependent. By taking a time discounted sum of
these bounds, we can establish (18).
•Finally, (14),(15),(16)are established by injecting the following pairs of policy-sequences in (18),
(¯π,π) = (π∗
MARL,π∗
MARL ),(π∗
MF,π∗
MF),(˜π∗
MF,π∗
MF). Note that ¯π,πare equal for first two pairs
whereas in the third case, ¯πis a localization of π.
8Published in Transactions on Machine Learning Research (09/2022)
7 Algorithm to obtain Near-Optimal Local Policy
In section 3, we discussed how near-optimal local policies can be obtained if the optimal mean-field policy
sequenceπ∗
MFis known. In this section, we first describe a natural policy gradient (NPG) based algorithm to
approximatelyobtain π∗
MF. Later, wealsoprovideanalgorithmtodescribehowtheobtainedpolicy-sequence
is localised and executed in a decentralised manner.
Recall from section 3 that, in an infinite agent system, it is sufficient to track only one representative agent
which, at instant t, takes an action uton the basis of its observation of its own state, xt, and the state
distribution, µtof the population. Hence, the evaluation of π∗
MFcan be depicted as a Markov Decision
Problem with state-space X×P (X), and action space U. One can, therefore, presume π∗
MFto be stationary
i.e.,π∗
MF={π∗
MF,π∗
MF,···}(Puterman, 2014). We would like to point out that the same conclusion may not
hold for the localised policy-sequence ˜π∗
MF. Stationarity of the sequence, π∗
MFreduces our task to finding an
optimal policy, π∗
MF. This facilitates a drastic reduction in the search space. With slight abuse of notation,
in this section, we shall use πΦto denote a policy parameterized by Φ, as well as the stationary sequence
generated by it.
Let the collection of policies be denoted as Π. We shall assume that Πis parameterized by Φ∈Rd. Consider
an arbitrary policy πΦ∈Π. The Q-function associated with this policy be defined as follows ∀x∈X,
∀µ∈P(X), and∀u∈U.
QΦ(x,µ,u)≜E/bracketleftigg∞/summationdisplay
t=0γtr(xt,ut,µt,νt)/vextendsingle/vextendsingle/vextendsinglex0=x,µ0=µ,u0=u/bracketrightigg
(19)
whereut+1∼πΦ(xt+1,µt+1),xt+1∼P(xt,ut,µt,νt), andµt,νtare recursively obtained ∀t >0using
(8),(7)respectively from the initial distribution µ0=µ. The advantage function associated with πΦis
defined as shown below.
AΦ(x,µ,u)≜QΦ(x,µ,u)−E[QΦ(x,µ,¯u)] (20)
The expectation is evaluated over ¯u∼πΦ(x,µ). To obtain the optimal policy, π∗
MF, we apply the NPG
update (Agarwal et al., 2021; Liu et al., 2020) as shown below with learning parameter, η. This generates a
sequence of parameters {Φj}J
j=1from an arbitrary initial choice, Φ0.
Φj+1= Φj+ηwj,wj≜arg minw∈RdLζΦj
µ0(w,Φj) (21)
The termζΦj
µ0is the occupancy measure defined as,
ζΦj
µ0(x,µ,u)≜∞/summationdisplay
τ=0γτP(xτ=x,µτ=µ,uτ=u|x0=x,µ0=µ,u0=u,πΦj)(1−γ)
whereas the function LζΦj
µ0is given as follows.
LζΦj
µ0(w,Φ)≜E(x,µ,u)∼ζΦj
µ0/bracketleftig/parenleftig
AΦ(x,µ,u)−(1−γ)wT∇ΦlogπΦ(x,µ)(u)/parenrightig2/bracketrightig
Note that in j-th iteration in (21), the gradient direction wjis computed by solving another minimization
problem. We employ a stochastic gradient descent (SGD) algorithm to solve this sub-problem. The update
equation for the SGD is as follows: wj,l+1=wj,l−αhj,l(Liu et al., 2020) where αis the learning parameter
and the gradient direction, hj,lis defined as follows.
hj,l≜/parenleftigg
wT
j,l∇ΦjlogπΦj(x,µ)(u)−1
1−γˆAΦj(x,µ,u)/parenrightigg
∇ΦjlogπΦj(x,µ)(u) (22)
9Published in Transactions on Machine Learning Research (09/2022)
Algorithm 1 Natural Policy Gradient Algorithm to obtain the Optimal Policy
Input:η,α: Learning rates, J,L: Number of execution steps
w0,Φ0: Initial parameters, µ0: Initial state distribution
Initialization: Φ←Φ0
1:forj∈{0,1,···,J−1}do
2:wj,0←w0
3:forl∈{0,1,···,L−1}do
4:Sample (x,µ,u)∼ζΦj
µ0and ˆAΦj(x,µ,u)using Algorithm 3
5:Compute hj,lusing (22)
wj,l+1←wj,l−αhj,l
6:end for
7:wj←1
L/summationtextL
l=1wj,l
8: Φj+1←Φj+ηwj
9:end for
Output:{Φ1,···,ΦJ}: Policy parameters
where (x,µ,u)is sampled from the occupancy measure ζΦj
µ0, and ˆAΦjis a unbiased estimator of AΦj. We
detail the process of obtaining the samples and the estimator in Algorithm 3 in the Appendix M. We would
like to clarify that Algorithm 3 of (Agarwal et al., 2021) is the foundation for Algorithm 3. The NPG process
is summarised in Algorithm 1.
In Algorithm 2, we describe how the policy obtained from Algorithm 1 can be localised and executed by
the agents in a decentralised manner. This essentially follows the ideas discussed in section 3. Note that, in
order to execute line 3 in Algorithm 2, the agents must be aware of the transition function, P. However, the
knowledge of the reward function, r, is not required.
Algorithm 2 Decentralised Execution of the Policy generated from Algorithm 1
Input: Φ: Policy parameter from Algorithm 1, T: Number of Execution Steps
µ0: Initialstatedistribution, x0: Initialstateoftheagent.
1:fort∈{0,1,···,T−1}do
2:Executeut∼πΦ(xt,µt)
3:Computeµt+1via mean-field dynamics (8).
4:Observe the next state xt+1(Updated via N-agent dynamics: xt+1∼P(xt,ut,µN
t,νN
t))
5:Update:µt←µt+1
6:Update:xt←xt+1
7:end for
Theorem 1 showed that the localization of the optimal mean-field policy π∗
MFis near-optimal for large
N. However, Algorithm 1 can provide only an approximation of π∗
MF. One might naturally ask: is the
decentralised version of the policy given by Algorithm 1 still near-optimal? We provide an answer to this
question in Theorem 3. Lemma 1, which follows from Theorem 4.9 of (Liu et al., 2020), is an essential
ingredient of this result. The proof of Lemma 1, however, hinges on the assumptions stated below. These
are similar to Assumptions 2.1, 4.2, and 4.4 respectively in (Liu et al., 2020).
Assumption 5. ∀Φ∈Rd,∀µ0∈P(X), for some χ > 0,Fµ0(Φ)−χIdis positive semi-definite where
Fµ0(Φ)is given as follows.
Fµ0(Φ)≜E(x,µ,u)∼ζΦµ0/bracketleftig
{∇ΦπΦ(x,µ)(u)}×{∇ ΦlogπΦ(x,µ)(u)}T/bracketrightig
Assumption 6. ∀Φ∈Rd,∀µ∈P(X),∀x∈X,∀u∈U,
|∇ΦlogπΦ(x,µ)(u)|1≤G
for some positive constant G.
10Published in Transactions on Machine Learning Research (09/2022)
Assumption 7. ∀Φ1,Φ2∈Rd,∀µ∈P(X),∀x∈X,∀u∈U,
|∇Φ1logπΦ1(x,µ)(u)−∇ Φ2logπΦ2(x,µ)(u)|1≤M|Φ1−Φ2|1
for some positive constant M.
Assumption 8. ∀Φ∈Rd,∀µ0∈P(X),
LζΦ∗
µ0(w∗
Φ,Φ)≤ϵbias,w∗
Φ≜arg minw∈RdLζΦµ0(w,Φ)
where Φ∗is the parameter of the optimal policy.
The parameter ϵbiasindicates the expressive power of the parameterized policy class, Π. For example,
ϵbias= 0for softmax policies, and is small for rich neural network based policies.
Lemma 1. Assume that{Φj}J
j=1is the sequence of policy parameters generated from Algorithm 1. If
Assumptions 5−8 hold, then the following inequality holds for some choice of η,α,J,L, for arbitrary initial
parameter Φ0and initial state distribution µ0∈P(X).
sup
Φ∈RdvMF(µ0,πΦ)−1
JJ/summationdisplay
j=1vMF(µ0,πΦj)≤√ϵbias
1−γ+ϵ, (23)
The sample complexity of Algorithm 1 to achieve (23)isO(ϵ−3).
We now state the following result.
Theorem 3. LetxN
0be the initial states in an N-agent system and µ0be its associated empirical distribution.
Assume that{Φj}J
j=1are the sequences of policy parameters obtained from Algorithm 1, and the set of policies,
Πobeys Assumption 3. If Assumptions 1,2,5−8hold, then for arbitrary ϵ>0, the following relation holds
for certain choices of η,α,J,L, and arbitrary initial distribution, µ0∈P(X), and initial parameter, Φ0,
/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsinglesup
Φ∈RdvMARL (µ0,πΦ)−1
JJ/summationdisplay
j=1vMARL (µ0,˜πΦj)/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle≤√ϵbias
1−γ+Cmax{ϵ,e},
e≜1√
N/bracketleftig/radicalbig
|X|+/radicalbig
|U|/bracketrightig(24)
wheneverγSP<1whereSPis given in Theorem 1. The term ˜πΦjdenotes the localization of the policy
πΦjdefined similarly as in (11), andCis a constant. The sample complexity of the process is O(ϵ−3).
Additionally, if Assumption 4 is satisfied, then ein(24)can be reduced to e=/radicalbig
|X|/√
N.
The proof of Theorem 3 is relegated to Appendix N. It states that for any ϵ>0, Algorithm 1 can generate
a policy such that if it is localised and executed in a decentralised manner in an N-agent system, then the
value generated from this localised policy is at most O(max{ϵ,e})distance away from the optimal value
function. The sample complexity to obtain such a policy is O(ϵ−3).
8 Experiments
The setup considered for the numerical experiment is taken from (Subramanian and Mahajan, 2019) with
slight modifications. We consider a network of Ncollaborative firms that yield the same product but with
varying quality. The product quality of i-th firm,i∈{1,···,N}at timet∈{0,1,···}is denoted as xi
tthat
can take values from the set Q≜{0,···,Q−1}. At each instant, each firm has two choices to make. Either
it can remain unresponsive (which we denote as action 0) or can invest some money to improve the quality
of its product (indicated as action 1). If the action taken by i-th firm at time tis denoted as ui
t, then its
state-transition law is described by the following equation.
xi
t+1=

xi
t ifui
t= 0
xi
t+/floorleftbigg
χ/parenleftbig
Q−1−xi
t/parenrightbig/parenleftbigg
1−¯µN
t
Q/parenrightbigg/floorrightbigg
elsewhere(25)
11Published in Transactions on Machine Learning Research (09/2022)
whereχis a uniform random variable in [0,1], and ¯µN
tis the mean of the empirical state distribution, µN
t.
The intuition behind this transition law can be stated as follows. If the firm remain unresponsive i.e., ui
t= 0,
the product quality does not improve and the state remains the same. In contrast, if the firm invests some
money, the quality improves probabilistically. However, if the average quality, ¯µN
tin the market is high, a
significant improvement is difficult to achieve. The factor (1−¯µN
t
Q)describes the resistance to improvement
due to higher average quality. The reward function of the i-th firm is defined as shown below.
r(xi
t,ui
t,µN
t,νN
t) =αRxi
t−βR¯µN
t−λRui
t (26)
The first term, αRxi
tis due to the revenue earned by the firm; the second term, βR¯µN
tis attributed to the
resistance to improvement imparted by high average quality; the third term, λRui
tis due to the cost incurred
for the investment. Following our previous convention, let π∗
MF, and ˜π∗
MFdenote the optimal mean-field
policy-sequence and its corresponding local policy-sequence respectively. Using these notations, we define
the error for a given joint initial state, xN
0as follows.
error≜/vextendsingle/vextendsinglevMARL (xN
0,π∗
MF)−vMARL (xN
0,˜π∗
MF)/vextendsingle/vextendsingle (27)
Fig. 1 plots erroras a function of NandQ. Evidently, errordecreases with Nand increases with Q. We
would like to point out that π∗
MF, in general, is not a maximizer of vMARL (xN
0,·). However, for the reasons
stated in Section 1, it is difficult to evaluate the N-agent optimal policy-sequence, π∗
MARL, especially for
largeN. On the other hand, π∗
MFcan be easily computed via Algorithm 1 and can act as a good proxy
forπ∗
MARL. Fig. 1 therefore essentially describes how well the local policies approximate the optimal value
function obtained over all policy-sequences in an N-agent system.
(a)
 (b)
Figure 1: Error defined by (27)as a function of the population size, N(Fig. 1a) and the size of individual
state space, Q(Fig. 1b). The solid line, and the half-width of the shaded region respectively denote mean,
and standard deviation of errorobtained over 25random seeds. The values of different system parameters
are:αR= 1,βR=λR= 0.5. We useQ= 10for Fig. 1a whereas N= 50for Fig. 1b. Moreover, the values
of different hyperparameters used in Algorithm 1 are as follows: α=η= 10−3,J=L= 102. We use a feed
forward (FF) neural network (NN) with single hidden layer of size 128as the policy approximator.
The code used for the numerical experiment can be accessed at: https://github.itap.purdue.edu/Clan-
labs/NearOptimalLocalPolicy
Before concluding, we would like to point out that both reward and state transition functions considered in
our experimental set up are independent of the action distributions. Therefore, the setting described in this
section satisfies Assumption 4. Moreover, due to the finiteness of individual state space Q, and action space
{0,1}, the reward function is bounded. Also, it is straightforward to verify the Lipschitz continuity of both
reward and transition functions. Hence, Assumptions 1 and 2 are satisfied. Finally, we use neural network
(NN) based policies (with bounded weights) in our experiment. Thus, Assumption 3 is also satisfied.
12Published in Transactions on Machine Learning Research (09/2022)
9 Conclusions
In this article, we show that, in an N-agent system, one can always choose localised policies such that
the resulting value function is close to the value function generated by (possibly non-local) optimal policy.
We mathematically characterize the approximation error as a function of N. Furthermore, we devise an
algorithm to explicitly obtain the said local policy. One interesting extension of our problem would be to
considerthecasewheretheinteractionsbetweentheagentsarenon-uniform. Provingnear-optimalityoflocal
policies appears to be difficult in such a scenario because, due to the non-uniformity, the notion of mean-field
is hard to define. Although our results are pertinent to the standard MARL systems, similar results can also
be established for other variants of reinforcement learning problems, e.g., the model described in (Gast and
Gaujal, 2011).
Acknowledgments
W. U. M. and S. V. U. were partially funded by NSF Grant No. 1638311 CRISP Type 2/Collaborative Re-
search: Critical Transitions in the Resilience and Recovery of Interdependent Social and Physical Networks.
References
Alekh Agarwal, Sham M Kakade, Jason D Lee, and Gaurav Mahajan. On the theory of policy gradient
methods: Optimality, approximation, and distribution shift. Journal of Machine Learning Research , 22
(98):1–76, 2021.
Abubakr O Al-Abbasi, Arnob Ghosh, and Vaneet Aggarwal. Deeppool: Distributed model-free algorithm for
ride-sharing using deep reinforcement learning. IEEE Transactions on Intelligent Transportation Systems ,
20(12):4714–4727, 2019.
Andrea Angiuli, Jean-Pierre Fouque, and Mathieu Laurière. Unified reinforcement q-learning for mean field
game and control problems. Mathematics of Control, Signals, and Systems , pages 1–55, 2022.
René Carmona, François Delarue, et al. Probabilistic theory of mean field games with applications I-II .
Springer, 2018.
Chacha Chen, Hua Wei, Nan Xu, Guanjie Zheng, Ming Yang, Yuanhao Xiong, Kai Xu, and Zhenhui Li.
Toward a thousand lights: Decentralized deep reinforcement learning for large-scale traffic signal control.
InProceedings of the AAAI Conference on Artificial Intelligence , volume 34, pages 3414–3421, 2020.
Shanzhi Chen, Jinling Hu, Yan Shi, Ying Peng, Jiayi Fang, Rui Zhao, and Li Zhao. Vehicle-to-everything
(v2x) services supported by lte-based systems and 5g. IEEE Communications Standards Magazine , 1(2):
70–76, 2017.
Tianshu Chu, Jie Wang, Lara Codecà, and Zhaojian Li. Multi-agent deep reinforcement learning for large-
scale traffic signal control. IEEE Transactions on Intelligent Transportation Systems , 21(3):1086–1095,
2019.
Amal Feriani and Ekram Hossain. Single and multi-agent deep reinforcement learning for AI-enabled wireless
networks: A tutorial. IEEE Communications Surveys & Tutorials , 23(2):1226–1252, 2021.
Nicolas Gast and Bruno Gaujal. A mean field approach for optimization in discrete time. Discrete Event
Dynamic Systems , 21(1):63–101, 2011.
HaotianGu, XinGuo, XiaoliWei, andRenyuanXu. Mean-fieldcontrolswithq-learningforcooperativemarl:
convergence and complexity analysis. SIAM Journal on Mathematics of Data Science , 3(4):1168–1196,
2021.
ChenchenHan, HaipengYao, TianleMai, NiZhang, andMohsenGuizani. Qmixaidedroutinginsocial-based
delay-tolerant networks. IEEE Transactions on Vehicular Technology , 71(2):1952–1963, 2021.
13Published in Transactions on Machine Learning Research (09/2022)
Alec Koppel, Amrit Singh Bedi, Bhargav Ganguly, and Vaneet Aggarwal. Convergence rates of
average-reward multi-agent reinforcement learning via randomized linear programming. arXiv preprint
arXiv:2110.12929 , 2021.
Landon Kraemer and Bikramjit Banerjee. Multi-agent reinforcement learning as a rehearsal for decentralized
planning. Neurocomputing , 190:82–94, 2016.
Yiheng Lin, Guannan Qu, Longbo Huang, and Adam Wierman. Multi-agent reinforcement learning in
stochastic networked systems. arXiv preprint arXiv:2006.06555 , 2020.
Yanli Liu, Kaiqing Zhang, Tamer Basar, and Wotao Yin. An improved analysis of (variance-reduced) policy
gradient and natural policy gradient methods. Advances in Neural Information Processing Systems , 33:
7624–7636, 2020.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex
Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control through
deep reinforcement learning. nature, 518(7540):529–533, 2015.
Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim Harley,
David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement learning. In Inter-
national conference on machine learning , pages 1928–1937. PMLR, 2016.
Washim Uddin Mondal, Mridul Agarwal, Vaneet Aggarwal, and Satish V. Ukkusuri. On the approximation
of cooperative heterogeneous multi-agent reinforcement learning (MARL) using mean field control (MFC).
Journal of Machine Learning Research , 23(129):1–46, 2022a.
Washim Uddin Mondal, Vaneet Aggarwal, and Satish Ukkusuri. Can mean field control (mfc) approxi-
mate cooperative multi agent reinforcement learning (marl) with non-uniform interaction? In The 38th
Conference on Uncertainty in Artificial Intelligence , 2022b.
Frans A Oliehoek, Matthijs TJ Spaan, and Nikos Vlassis. Optimal and approximate q-value functions for
decentralized pomdps. Journal of Artificial Intelligence Research , 32:289–353, 2008.
Barna Pasztor, Ilija Bogunovic, and Andreas Krause. Efficient model-based multi-agent mean-field reinforce-
ment learning. arXiv preprint arXiv:2107.04050 , 2021.
Martin L Puterman. Markov decision processes: discrete stochastic dynamic programming . John Wiley &
Sons, 2014.
Guannan Qu, Yiheng Lin, Adam Wierman, and Na Li. Scalable multi-agent reinforcement learning for
networked systems with average reward. Advances in Neural Information Processing Systems , 33:2074–
2086, 2020.
Tabish Rashid, Mikayel Samvelyan, Christian Schroeder, Gregory Farquhar, Jakob Foerster, and Shimon
Whiteson. Qmix: Monotonic value function factorisation for deep multi-agent reinforcement learning. In
International conference on machine learning , pages 4295–4304. PMLR, 2018.
Tabish Rashid, Gregory Farquhar, Bei Peng, and Shimon Whiteson. Weighted qmix: Expanding monotonic
value function factorisation for deep multi-agent reinforcement learning. Advances in neural information
processing systems , 33:10199–10210, 2020.
Gavin A Rummery and Mahesan Niranjan. On-line Q-learning using connectionist systems , volume 37.
Citeseer, 1994.
Lars Ruthotto, Stanley J Osher, Wuchen Li, Levon Nurbekyan, and Samy Wu Fung. A machine learning
framework for solving high-dimensional mean field game and mean field control problems. Proceedings of
the National Academy of Sciences , 117(17):9183–9193, 2020.
14Published in Transactions on Machine Learning Research (09/2022)
Kyunghwan Son, Daewoo Kim, Wan Ju Kang, David Earl Hostallero, and Yung Yi. Qtran: Learning
to factorize with transformation for cooperative multi-agent reinforcement learning. In International
conference on machine learning , pages 5887–5896. PMLR, 2019.
Jayakumar Subramanian and Aditya Mahajan. Reinforcement learning in stationary mean-field games. In
Proceedings of the 18th International Conference on Autonomous Agents and MultiAgent Systems , pages
251–259, 2019.
Peter Sunehag, Guy Lever, Audrunas Gruslys, Wojciech Marian Czarnecki, Vinicius Zambaldi, Max Jader-
berg, Marc Lanctot, Nicolas Sonnerat, Joel Z Leibo, Karl Tuyls, et al. Value-decomposition networks for
cooperative multi-agent learning based on team reward. In Proceedings of the 17th International Confer-
ence on Autonomous Agents and MultiAgent Systems , pages 2085–2087, 2018.
Ming Tan. Multi-agent reinforcement learning: Independent vs. cooperative agents. In Proceedings of the
tenth international conference on machine learning , pages 330–337, 1993.
Nilay Tiwari, Arnob Ghosh, and Vaneet Aggarwal. Reinforcement learning for mean field game. arXiv
preprint arXiv:1905.13357 , 2019.
Xiaoqiang Wang, Liangjun Ke, Zhimin Qiao, and Xinghua Chai. Large-scale traffic signal control using a
novel multiagent reinforcement learning. IEEE transactions on cybernetics , 51(1):174–187, 2020.
Christopher JCH Watkins and Peter Dayan. Q-learning. Machine learning , 8(3):279–292, 1992.
Nicholas J Watkins, Cameron Nowzari, Victor M Preciado, and George J Pappas. Optimal resource alloca-
tion for competitive spreading processes on bilayer networks. IEEE Transactions on Control of Network
Systems, 5(1):298–307, 2016.
Hua Wei, Chacha Chen, Guanjie Zheng, Kan Wu, Vikash Gayah, Kai Xu, and Zhenhui Li. Presslight:
Learning max pressure control to coordinate traffic signals in arterial network. In Proceedings of the 25th
ACM SIGKDD International Conference on Knowledge Discovery & Data Mining , pages 1290–1298, 2019.
15Published in Transactions on Machine Learning Research (09/2022)
A Proof of Theorem 1
Inordertoestablishthetheorem, thefollowingLemmasarenecessary. TheproofoftheLemmasarerelegated
to Appendix C−I.
A.1 Lipschitz Continuity Lemmas
In the following three lemmas (Lemma 2−4),π,¯π∈Πare arbitrary admittable policies, and µ,¯µ∈P(X)
are arbitrary state distributions. Moreover, the following definition is frequently used.
|π(·,µ)−¯π(·,¯µ)|∞≜sup
x∈X|π(x,µ)−¯π(x,¯µ)|1 (28)
Lemma 2. IfνMF(·,·)is defined by (7), then the following holds.
|νMF(µ,π)−νMF(¯µ,¯π)|1≤|µ−¯µ|1+|π(·,µ)−¯π(·,¯µ)|∞ (29)
Lemma 3. IfPMF(·,·)is defined by (8), then the following holds.
|PMF(µ,π)−PMF(¯µ,¯π)|1≤˜SP|µ−¯µ|1+¯SP|π(·,µ)−¯π(·,¯µ)|∞ (30)
where ˜SP≜1 + 2LP, and ¯SP≜1 +LP.
Lemma 4. IfνMF(·,·)is defined by (9), then the following holds.
|rMF(µ,π)−rMF(¯µ,¯π)|≤˜SR|µ−¯µ|1+¯SR|π(·,µ)−¯π(·,¯µ)|∞ (31)
where ˜SR≜MR+ 2LR, and ¯SR≜MR+LR.
Lemma 2−4essentially dictate that the state and action evolution functions, ( PMF(·,·), andνMF(·,·)
respectively), and the average reward function, rMF(·,·)demonstrate Lipschitz continuity property w. r. t.
the state distribution and policy arguments. Lemma 2 is an essential ingredient in the proof of Lemma 3,
and 4.
A.2 Large Population Approximation Lemmas
In the following four lemmas (Lemma 5−8),π≜{πt}t∈{0,1,···}∈Π∞is an arbitrary admissible policy-
sequence, and{µN
t,νN
t}t∈{0,1,···}are theN-agent empirical state, and action distributions induced by it
from the initial state distribution, µ0. Similarly,{xi
t,ui
t}t∈{0,1,···}are the states, and actions of i-th agent
evolved from the initial distribution, µ0via the policy-sequence, π. The joint states, and actions at time t
are denoted by xN
t,uN
trespectively.
Lemma 5. The following inequality holds ∀t∈{0,1,···}.
E/vextendsingle/vextendsingleνN
t−νMF(µN
t,πt)/vextendsingle/vextendsingle
1≤1√
N/radicalbig
|U| (32)
Lemma 6. The following inequality holds ∀t∈{0,1,···}.
E/vextendsingle/vextendsingleµN
t+1−PMF(µN
t,πt)/vextendsingle/vextendsingle
1≤CP√
N/bracketleftig/radicalbig
|X|+/radicalbig
|U|/bracketrightig
(33)
whereCP≜2 +LP.
Lemma 7. The following inequality holds ∀t∈{0,1,···}.
E/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle1
N/summationdisplay
i=1r(xi
t,ui
t,µN
t,νN
t)−rMF(µN
t,πt)/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle≤MR√
N+LR√
N/radicalbig
|U|
16Published in Transactions on Machine Learning Research (09/2022)
Finally, ifµ∞
tindicates the state distribution of an infinite agent system at time tinduced by the policy-
sequence,πfrom the initial distribution, µ0then the following result can be proven invoking Lemma 3, and
6.
Lemma 8. The following inequality holds ∀t∈{0,1,···}.
E|µN
t−µ∞
t|≤CP√
N/bracketleftig/radicalbig
|X|+/radicalbig
|U|/bracketrightig/parenleftbiggSt
P−1
SP−1/parenrightbigg
whereSP≜˜SP+LQ¯SP. The terms ˜SP,¯SPare defined in Lemma 3 while CPis given in Lemma 6.
A.3 Proof of the Theorem
Letπ≜{πt}t∈{0,1,···}, and ¯π≜{¯πt}t∈{0,1,···}be two arbitrary policy-sequences in Π∞. Denote by
{µN
t,νN
t},{µ∞
t,ν∞
t}the state, and action distributions induced by policy-sequence πat timetin anN-
agent system, and infinite agent systems respectively. Also, the state, and action of i-th agent at time t
corresponding to the same policy sequence are indicated as xi
t, andui
trespectively. The same quantities
corresponding to the policy-sequence ¯πare denoted as{¯µN
t,¯νN
t,¯µ∞
t,¯ν∞
t,¯xi
t,¯ui
t}. Consider the following
difference,
|vMARL (xN
0,¯π)−vMF(µ0,π)|
(a)
≤∞/summationdisplay
t=0γt/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle1
NN/summationdisplay
i=1E/bracketleftbig
r(¯xi
t,¯ui
t,¯µN
t,¯νN
t)/bracketrightbig
−rMF(µ∞
t,πt)/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
≤∞/summationdisplay
t=0γtE/vextendsingle/vextendsingle/vextendsingle/vextendsingle1
Nr(¯xi
t,¯ui
t,¯µN
t,¯νN
t)−rMF(¯µN
t,¯πt)/vextendsingle/vextendsingle/vextendsingle/vextendsingle
/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
≜J1+∞/summationdisplay
t=0γtE/vextendsingle/vextendsinglerMF(¯µN
t,¯πt)−rMF(¯µ∞
t,¯πt)/vextendsingle/vextendsingle
/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
≜J2
+∞/summationdisplay
t=0γt/vextendsingle/vextendsinglerMF(¯µ∞
t,¯πt)−rMF(µ∞
t,πt)/vextendsingle/vextendsingle
/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
≜J3
Inequality (a) follows from the definition of the value functions vMARL (·,·),vMF(·,·)given in (6), and (10)
respectively. The first term, J1can be bounded using Lemma 7 as follows.
J1≤/parenleftbigg1
1−γ/parenrightbigg/bracketleftbiggMR√
N+LR√
N/radicalbig
|U|/bracketrightbigg
The second term, J2, can be bounded as follows.
J2≜∞/summationdisplay
t=0γtE|rMF(¯µN
t,¯πt)−rMF(¯µ∞
t,¯πt)|
(a)
≤∞/summationdisplay
t=0γtE/braceleftbig˜SR|¯µN
t−¯µ∞
t|1+¯SR/vextendsingle/vextendsingle¯πt(·,¯µN
t)−¯πt(·,¯µ∞
t)/vextendsingle/vextendsingle
∞/bracerightbig
(b)
≤SR∞/summationdisplay
t=0γtE|¯µN
t−¯µ∞
t|
(c)
≤1√
N/bracketleftig/radicalbig
|X|+/radicalbig
|U|/bracketrightig/parenleftbiggSRCP
SP−1/parenrightbigg/bracketleftbigg1
1−γSP−1
1−γ/bracketrightbigg
whereSR≜˜SR+LQ¯SR. Inequality (a) follows from Lemma 4, whereas (b) is a consequence of Assumption
3. Finally, (c) follows from Lemma 8. It remains to bound J3. Note that, if ¯π=π, thenJ3= 0. Hence,
|vMARL (xN
0,π∗
MARL )−vMF(µ0,π∗
MARL )|≤J0, (34)
|vMARL (xN
0,π∗
MF)−vMF(µ0,π∗
MF)|≤J0 (35)
17Published in Transactions on Machine Learning Research (09/2022)
whereJ0is given as follows,
J0≜/parenleftbigg1
1−γ/parenrightbigg/bracketleftbiggMR√
N+LR√
N/radicalbig
|U|/bracketrightbigg
+1√
N/bracketleftig/radicalbig
|X|+/radicalbig
|U|/bracketrightig/parenleftbiggSRCP
SP−1/parenrightbigg/bracketleftbigg1
1−γSP−1
1−γ/bracketrightbigg
Moreover, if π=π∗
MF, and ¯π=˜π∗
MF(or vice versa), then J3= 0as well. This is precisely because the
trajectory of state, and action distributions generated by the policy-sequences π∗
MF,˜π∗
MFare identical in an
infinite agent system. Hence, we have,
|vMARL (xN
0,˜π∗
MF)−vMF(µ0,π∗
MF)|≤J0 (36)
Consider the following inequalities,
vMARL (xN
0,π∗
MARL )−vMARL (xN
0,˜π∗
MF)
=vMARL (xN
0,π∗
MARL )−vMF(µ0,π∗
MF) +vMF(µ0,π∗
MF)−vMARL (xN
0,˜π∗
MF)
(a)
≤vMARL (xN
0,π∗
MARL )−vMF(µ0,π∗
MARL ) +J0(b)
≤2J0(37)
Inequality (a) follows from (36), and the fact that π∗
MFmaximizes vMF(µ0,·). Inequality (b) follows from
(34). Moreover,
vMARL (xN
0,˜π∗
MF)−vMARL (xN
0,π∗
MARL )
=vMARL (xN
0,˜π∗
MF)−vMF(µ0,π∗
MF) +vMF(µ0,π∗
MF)−vMARL (xN
0,π∗
MARL )
(a)
≤J0+vMF(µ0,π∗
MF)−vMARL (xN
0,π∗
MF)(b)
≤2J0(38)
Inequality (a) follows from (36), and the fact that π∗
MARLmaximizes vMARL (x0,·). Inequality (b) follows
from (35). Combining (37), and (38), we conclude that,
|vMARL (xN
0,π∗
MARL )−vMARL (xN
0,˜π∗
MF)|≤2J0
B Proof of Theorem 2
The proof of Theorem 2 is similar to that of Theorem 1, however, with subtle differences. The following
Lemmas are needed to establish the theorem.
B.1 Auxiliary Lemmas
In the following (Lemma 9−11),π≜{πt}t∈{0,1,···}∈Π∞denotes an arbitrary admissible policy-sequence.
The terms{µN
t,νN
t}t∈{0,1,···}denote the N-agent empirical state, and action distributions induced by π
from the initial state distribution, µ0whereas{µ∞
t,ν∞
t}t∈{0,1,···}indicate the state, and action distributions
induced by the same policy-sequence in an infinite agent system. Similarly, {xi
t,ui
t}t∈{0,1,···}denote the
states, and actions of i-th agent evolved from the initial distribution, µ0via the policy-sequence, π. The
joint states, and actions at time tare denoted by xN
t,uN
trespectively.
Lemma 9. The following inequality holds ∀t∈{0,1,···}.
E/vextendsingle/vextendsingleµN
t+1−PMF(µN
t,πt)/vextendsingle/vextendsingle
1≤2√
N/radicalbig
|X| (39)
Lemma 10. The following inequality holds ∀t∈{0,1,···}.
E/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle1
N/summationdisplay
i=1r(xi
t,ui
t,µN
t)−rMF(µN
t,πt)/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle≤MR√
N
18Published in Transactions on Machine Learning Research (09/2022)
Lemma 11. The following inequality holds ∀t∈{0,1,···}.
E|µN
t−µ∞
t|≤2√
N/radicalbig
|X|/parenleftbiggSt
P−1
SP−1/parenrightbigg
whereSP≜˜SP+LQ¯SP. The terms ˜SP,¯SPare defined in Lemma 3.
The proofs of the above lemmas are relegated to Appendix J−L.
B.2 Proof of the Theorem
We use the same notations as in Appendix A.3. Consider the following difference,
|vMARL (xN
0,¯π)−vMF(µ0,π)|
(a)
≤∞/summationdisplay
t=0γt/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle1
NN/summationdisplay
i=1r(¯xi
t,¯ui
t,¯µN
t)/bracketrightigg
−E/bracketleftbig
rMF(µ∞
t,πt)/vextendsingle/vextendsingle
≤∞/summationdisplay
t=0γtE/vextendsingle/vextendsingle/vextendsingle/vextendsingle1
Nr(¯xi
t,¯ui
t,¯µN
t)−rMF(¯µN
t,¯πt)/vextendsingle/vextendsingle/vextendsingle/vextendsingle
/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
≜J1+∞/summationdisplay
t=0γtE/vextendsingle/vextendsinglerMF(¯µN
t,¯πt)−rMF(¯µ∞
t,¯πt)/vextendsingle/vextendsingle
/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
≜J2
+∞/summationdisplay
t=0γt/vextendsingle/vextendsinglerMF(¯µ∞
t,¯πt)−rMF(µ∞
t,πt)/vextendsingle/vextendsingle
/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
≜J3
Inequality (a) follows from the definition of the value functions vMARL (·,·),vMF(·,·)given in (6), and (10)
respectively. The first term, J1can be bounded using Lemma 7 as follows.
J1≤/parenleftbigg1
1−γ/parenrightbigg/bracketleftbiggMR√
N/bracketrightbigg
The second term, J2, can be bounded as follows.
J2≜∞/summationdisplay
t=0γtE|rMF(¯µN
t,¯πt)−rMF(¯µ∞
t,¯πt)|
(a)
≤∞/summationdisplay
t=0γtE/braceleftbig˜SR|¯µN
t−¯µ∞
t|1+¯SR/vextendsingle/vextendsingle¯πt(·,¯µN
t)−¯πt(·,¯µ∞
t)/vextendsingle/vextendsingle
∞/bracerightbig
(b)
≤SR∞/summationdisplay
t=0γtE|¯µN
t−¯µ∞
t|
(c)
≤1√
N/radicalbig
|X|/parenleftbigg2SR
SP−1/parenrightbigg/bracketleftbigg1
1−γSP−1
1−γ/bracketrightbigg
whereSR≜˜SR+LQ¯SR. Inequality (a) follows from Lemma 4, whereas (b) is a consequence of Assumption
3. Finally, (c) follows from Lemma 8. It remains to bound J3. Note that, if ¯π=π, thenJ3= 0. Hence,
|vMARL (xN
0,π∗
MARL )−vMF(µ0,π∗
MARL )|≤J0, (40)
|vMARL (xN
0,π∗
MF)−vMF(µ0,π∗
MF)|≤J0 (41)
whereJ0is given as follows,
J0≜/parenleftbigg1
1−γ/parenrightbigg/bracketleftbiggMR√
N/bracketrightbigg
+1√
N/bracketleftig/radicalbig
|X|+/radicalbig
|U|/bracketrightig/parenleftbigg2SR
SP−1/parenrightbigg/bracketleftbigg1
1−γSP−1
1−γ/bracketrightbigg
19Published in Transactions on Machine Learning Research (09/2022)
Moreover, if π=π∗
MF, and ¯π=˜π∗
MF(or vice versa), then J3= 0as well. This is precisely because the
trajectory of state, and action distributions generated by the policy-sequences π∗
MF,˜π∗
MFare identical in an
infinite agent system. Hence, we have,
|vMARL (xN
0,˜π∗
MF)−vMF(µ0,π∗
MF)|≤J0 (42)
Following the same set of arguments as is used in (37), and (38), we conclude that,
|vMARL (xN
0,π∗
MARL )−vMARL (µ0,˜π∗
MF)|≤2J0
C Proof of Lemma 2
Note the chain of inequalities stated below.
|νMF(µ,π)−νMF(¯µ,¯π)|1
(a)=/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/summationdisplay
x∈Xπ(x,µ)µ(x)−/summationdisplay
x∈X¯π(x,¯µ)¯µ(x)/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
1
=/summationdisplay
u∈U/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/summationdisplay
x∈Xπ(x,µ)(u)µ(x)−/summationdisplay
x∈X¯π(x,¯µ)(u)¯µ(x)/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
≤/summationdisplay
x∈X/summationdisplay
u∈U|π(x,µ)(u)µ(x)−¯π(x,¯µ)(u)¯µ(x)|
≤/summationdisplay
x∈X|µ(x)−¯µ(x)|/summationdisplay
u∈Uπ(x,µ)(u)
/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
=1+/summationdisplay
x∈X¯µ(x)/summationdisplay
u∈U|π(x,µ)(u)−¯π(x,¯µ)(u)|
≤|µ−¯µ|1+/summationdisplay
x∈X¯µ(x)
/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
=1/bracketleftbigg
sup
x∈X|π(x,µ)−¯π(x,¯µ)|1/bracketrightbigg
(b)=|µ−¯µ|1+|π(·,µ)−¯π(·,¯µ)|∞
Inequality (a) follows from the definition of νMF(·,·)as given in (7). On the other hand, equation (b) is a
consequence of the definition of |·|∞over the space of all admissible policies, Π. This concludes the result.
D Proof of Lemma 3
Observe that,
|PMF(µ,π)−PMF(¯µ,¯π)|1
(a)=/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/summationdisplay
x∈X/summationdisplay
u∈UP(x,u,µ,νMF(µ,π))π(x,µ)(u)µ(x)−P(x,u,¯µ,νMF(¯µ,¯π))¯π(x,¯µ)(u)¯µ(x)/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
1
≤J1+J2
20Published in Transactions on Machine Learning Research (09/2022)
Equality (a) follows from the definition of PMF(·,·)as depicted in (8). The term J1satisfies the following
bound.
J1≜/summationdisplay
x∈X/summationdisplay
u∈U/vextendsingle/vextendsingle/vextendsingleP(x,u,µ,νMF(µ,π))−P(x,u,¯µ,νMF(¯µ,¯π))/vextendsingle/vextendsingle/vextendsingle
1×π(x,µ)(u)µ(x)
(a)
≤LP/bracketleftbig
|µ−¯µ|1+|νMF(µ,π)−νMF(¯µ,¯π)|1/bracketrightbig
×/summationdisplay
x∈Xµ(x)/summationdisplay
u∈Uπ(x,µ)(u)
/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
=1
(b)
≤2LP|µ−¯µ|1+LP|π(·,µ)−¯π(·,¯µ)|∞
Inequality (a)is a consequence of Assumption 2 whereas (b)follows from Lemma 2, and the fact that π(x,µ),
µare probability distributions. The second term, J2obeys the following bound.
J2≜/summationdisplay
x∈X/summationdisplay
u∈U|P(x,u,¯µ,νMF(¯µ,¯π))|1/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
=1×|π(x,µ)(u)µ(x)−¯π(x,¯µ)(u)¯µ(x)|
≤/summationdisplay
x∈X|µ(x)−¯µ(x)|/summationdisplay
u∈Uπ(x,µ)(u)
/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
=1+/summationdisplay
x∈X¯µ(x)/summationdisplay
u∈U|π(x,µ)(u)−¯π(x,¯µ)(u)|
(a)
≤|µ−¯µ|1+/summationdisplay
x∈X¯µ(x)
/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
=1/bracketleftbigg
sup
x∈X|π(x,µ)−¯π(x,¯µ)|1/bracketrightbigg
=|µ−¯µ|1+|π(·,µ)−¯π(·,¯µ)|∞
Inequality (a)results from the fact that π(x,µ)is a probability distribution while (b)utilizes the definition
of|·|∞. This concludes the result.
E Proof of Lemma 4
Observe that,
|rMF(µ,π)−rMF(¯µ,¯π)|
(a)=/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/summationdisplay
x∈X/summationdisplay
u∈Ur(x,u,µ,νMF(µ,π))π(x,µ)(u)µ(x)−r(x,u,¯µ,νMF(¯µ,¯π))¯π(x,¯µ)(u)¯µ(x)/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
≤J1+J2
Equality (a) follows from the definition of rMF(·,·)as given in (9). The first term obeys the following bound.
J1≜/summationdisplay
x∈X/summationdisplay
u∈U/vextendsingle/vextendsingle/vextendsingler(x,u,µ,νMF(µ,π))−r(x,u,¯µ,νMF(¯µ,¯π))/vextendsingle/vextendsingle/vextendsingle×π(x,µ)(u)µ(x)
(a)
≤LR/bracketleftbig
|µ−¯µ|1+|νMF(µ,π)−νMF(¯µ,¯π)|1/bracketrightbig
×/summationdisplay
x∈Xµ(x)/summationdisplay
u∈Uπ(x,µ)(u)
/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
=1
(b)
≤2LR|µ−¯µ|1+LR|π(·,µ)−¯π(·,¯µ)|∞
21Published in Transactions on Machine Learning Research (09/2022)
Inequality (a)is a consequence of Assumption 1(b) whereas inequality (b)follows from Lemma 2, and the
fact thatπ(x,µ),µare probability distributions. The second term, J2satisfies the following.
J2≜/summationdisplay
x∈X/summationdisplay
u∈U|r(x,u,¯µ,νMF(¯µ,¯π))|×|π(x,µ)(u)µ(x)−¯π(x,¯µ)(u)¯µ(x)|
(a)
≤MR/summationdisplay
x∈X/summationdisplay
u∈U|π(x,µ)(u)µ(x)−¯π(x,¯µ)(u)¯µ(x)|
≤MR/summationdisplay
x∈X|µ(x)−¯µ(x)|/summationdisplay
u∈Uπ(x,µ)(u)
/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
=1+MR/summationdisplay
x∈X¯µ(x)/summationdisplay
u∈U|π(x,µ)(u)−¯π(x,¯µ)(u)|
≤MR|µ−¯µ|1+MR/summationdisplay
x∈X¯µ(x)
/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
=1/bracketleftbigg
sup
x∈X|π(x,µ)−¯π(x,¯µ)|1/bracketrightbigg
(b)=MR|µ−¯µ|1+MR|π(·,µ)−¯π(·,¯µ)|∞
Inequality (a)resultsfromAssumption1(a). Ontheotherhand, equality (b)isaconsequenceofthedefinition
of|·|∞, and the fact that ¯µis a probability distribution. This concludes the result.
F Proof of Lemma 5
The following Lemma is required to prove the result.
Lemma 12. If∀m∈{1,···,M},{Xmn}n∈{1,···,N}are independent random variables that lie in [0,1], and
satisfy/summationtext
m∈{1,···,M}E[Xmn]≤1,∀n∈{1,···,N}, then the following holds,
M/summationdisplay
m=1E/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleN/summationdisplay
n=1(Xmn−E[Xmn])/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle≤√
MN (43)
Lemma 12 is adapted from Lemma 13 of (Mondal et al., 2022a).
Notice the following relations.
E/vextendsingle/vextendsingleνN
t−νMF(µN
t,πt)/vextendsingle/vextendsingle
1
=E/bracketleftig
E/bracketleftig/vextendsingle/vextendsingleνN
t−νMF(µN
t,πt)/vextendsingle/vextendsingle
1/vextendsingle/vextendsingle/vextendsinglexN
t/bracketrightig/bracketrightig
(a)=E/bracketleftigg
E/bracketleftigg/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleνN
t−/summationdisplay
x∈Xπt(x,µN
t)µN
t(x)/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
1/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsinglexN
t/bracketrightigg/bracketrightigg
=E/bracketleftigg
E/bracketleftigg/summationdisplay
u∈U/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleνN
t(u)−/summationdisplay
x∈Xπt(x,µN
t)(u)µN
t(x)/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsinglexN
t/bracketrightigg/bracketrightigg
(b)=E/bracketleftigg/summationdisplay
u∈UE/bracketleftigg
1
N/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleN/summationdisplay
i=1δ(ui
t=u)−1
N/summationdisplay
x∈Xπt(x,µN
t)(u)N/summationdisplay
i=1δ(xi
t=x)/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsinglexN
t/bracketrightigg/bracketrightigg
=E/bracketleftigg/summationdisplay
u∈UE/bracketleftigg/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle1
NN/summationdisplay
i=1δ(ui
t=u)−1
NN/summationdisplay
i=1πt(xi
t,µN
t)(u)/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsinglexN
t/bracketrightigg/bracketrightigg
(c)
≤1√
N/radicalbig
|U|
Equality (a) can be established using the definition of νMF(·,·)as depicted in (7). Similarly, relation (b) is a
consequence of the definitions of µN
t,νN
t. Finally, (c) uses Lemma 12. Specifically, it utilises the facts that,
22Published in Transactions on Machine Learning Research (09/2022)
{ui
t}i∈{1,···,N}are conditionally independent given xN
t, and the followings hold
E/bracketleftig
δ(ui
t=u)/vextendsingle/vextendsingle/vextendsinglexN
t/bracketrightig
=πt(xi
t,µN
t)(u),
/summationdisplay
u∈UE/bracketleftig
δ(ui
t=u)/vextendsingle/vextendsingle/vextendsinglexN
t/bracketrightig
= 1
∀i∈{1,···,N},∀u∈U. This concludes the lemma.
G Proof of Lemma 6
Notice the following decomposition.
E/vextendsingle/vextendsingleµN
t+1−PMF(µN
t,πt)/vextendsingle/vextendsingle
1
(a)=/summationdisplay
x∈XE/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle1
NN/summationdisplay
i=1δ(xi
t+1=x)−/summationdisplay
x′∈X/summationdisplay
u∈UP(x′,u,µN
t,νMF(µN
t,πt))(x)πt(x′,µN
t)(u)1
NN/summationdisplay
i=1δ(xi
t=x′)/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
=/summationdisplay
x∈XE/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle1
NN/summationdisplay
i=1δ(xi
t+1=x)−1
NN/summationdisplay
i=1/summationdisplay
u∈UP(xi
t,u,µN
t,νMF(µN
t,πt))(x)πt(xi
t,µN
t)(u)/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
≤J1+J2+J3
Equality (a) uses the definition of PMF(·,·)as shown in (8). The term, J1obeys the following bound.
J1≜1
N/summationdisplay
x∈XE/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleN/summationdisplay
i=1δ(xi
t+1=x)−N/summationdisplay
i=1P(xi
t,ui
t,µN
t,νN
t)(x)/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
=1
N/summationdisplay
x∈XE/bracketleftigg
E/bracketleftigg/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleN/summationdisplay
i=1δ(xi
t+1=x)−N/summationdisplay
i=1P(xi
t,ui
t,µN
t,νN
t)(x)/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsinglexN
t,uN
t/bracketrightigg/bracketrightigg
(a)
≤1√
N/radicalbig
|X|
Inequality (a)is obtained applying Lemma 12, and the facts that {xi
t+1}i∈{1,···,N}are conditionally inde-
pendent given{xN
t,uN
t}, and the following relations hold
E/bracketleftig
δ(xi
t+1=x)/vextendsingle/vextendsingle/vextendsinglexN
t,uN
t/bracketrightig
=P(xi
t,ui
t,µN
t,νN
t)(x),
/summationdisplay
x∈XE/bracketleftig
δ(xi
t+1=x)/vextendsingle/vextendsingle/vextendsinglexN
t,uN
t/bracketrightig
= 1
∀i∈{1,···,N}, and∀x∈X. The second term satisfies the following bound.
J2≜1
N/summationdisplay
x∈XE/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleN/summationdisplay
i=1P(xi
t,ui
t,µN
t,νN
t)(x)−N/summationdisplay
i=1P(xi
t,ui
t,µN
t,νMF(µN
t,πt))(x)/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
≤1
NN/summationdisplay
i=1E/vextendsingle/vextendsingleP(xi
t,ui
t,µN
t,νN
t)−P(xi
t,ui
t,µN
t,νMF(µN
t,πt))/vextendsingle/vextendsingle
1
(a)
≤LPE/vextendsingle/vextendsingleνN
t−νMF(µN
t,πt)/vextendsingle/vextendsingle
1(b)
≤LP√
N/radicalbig
|U|
23Published in Transactions on Machine Learning Research (09/2022)
Inequality (a) is a consequence of Assumption 2 while (b) follows from Lemma 5. Finally, the term, J3can
be upper bounded as follows.
J3≜1
N/summationdisplay
x∈XE/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleN/summationdisplay
i=1P(xi
t,ui
t,µN
t,νMF(µN
t,πt))(x)−N/summationdisplay
i=1/summationdisplay
u∈UP(xi
t,u,µN
t,νMF(µN
t,πt))(x)πt(xi
t,µN
t)(u)/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
(a)
≤1√
N/radicalbig
|X|
Inequality (a) is a result of Lemma 12. In particular, it uses the facts that, {ui
t}i∈{1,···,N}are conditionally
independent given xN
t, and the following relations hold
E/bracketleftig
P(xi
t,ui
t,µN
t,νMF(µN
t,πt))(x)/vextendsingle/vextendsingle/vextendsinglexN
t/bracketrightig
=/summationdisplay
u∈UP(xi
t,u,µN
t,νMF(µN
t,πt))(x)πt(xi
t,µN
t)(u),
/summationdisplay
x∈XE/bracketleftig
P(xi
t,ui
t,µN
t,νMF(µN
t,πt))(x)/vextendsingle/vextendsingle/vextendsinglexN
t/bracketrightig
= 1
∀i∈{1,···,N}, and∀x∈X. This concludes the Lemma.
H Proof of Lemma 7
Observe the following decomposition.
E/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle1
N/summationdisplay
i=1r(xi
t,ui
t,µN
t,νN
t)−rMF(µN
t,πt)/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
(a)=E/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle1
NN/summationdisplay
i=1r(xi
t,ui
t,µN
t,νN
t)−/summationdisplay
x∈X/summationdisplay
u∈Ur(x,u,µN
t,νMF(µN
t,πt))πt(x,µN
t)(u)1
NN/summationdisplay
i=1δ(xi
t=x)/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
=E/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle1
NN/summationdisplay
i=1r(xi
t,ui
t,µN
t,νN
t)−1
NN/summationdisplay
i=1/summationdisplay
u∈Ur(xi
t,u,µN
t,νMF(µN
t,πt))πt(xi
t,µN
t)(u)/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
≤J1+J2
Equation (a) uses the definition of rMF(·,·)as depicted in (9). The term, J1, obeys the following bound.
J1≜1
NE/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleN/summationdisplay
i=1r(xi
t,ui
t,µN
t,νN
t)−N/summationdisplay
i=1r(xi
t,ui
t,µN
t,νMF(µN
t,πt))/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
≤1
NEN/summationdisplay
i=1/vextendsingle/vextendsingler(xi
t,ui
t,µN
t,νN
t)−r(xi
t,ui
t,µN
t,νMF(µN
t,πt))/vextendsingle/vextendsingle
(a)
≤LRE/vextendsingle/vextendsingleνN
t−νMF(µN
t,πt)/vextendsingle/vextendsingle
1
(b)
≤LR√
N/radicalbig
|U|
24Published in Transactions on Machine Learning Research (09/2022)
Inequality (a) results from Assumption 1, whereas (b) is a consequence of Lemma 5. The term, J2, satisfies
the following.
J2≜1
NE/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleN/summationdisplay
i=1r(xi
t,ui
t,µN
t,νMF(µN
t,πt))−N/summationdisplay
i=1/summationdisplay
u∈Ur(xi
t,u,µN
t,νMF(µN
t,πt))πt(xi
t,µN
t)(u)/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
=1
NE/bracketleftigg
E/bracketleftigg/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleN/summationdisplay
i=1r(xi
t,ui
t,µN
t,νMF(µN
t,πt))−N/summationdisplay
i=1/summationdisplay
u∈Ur(xi
t,u,µN
t,νMF(µN
t,πt))πt(xi
t,µN
t)(u)/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsinglexN
t/bracketrightigg/bracketrightigg
=MR
NE/bracketleftigg
E/bracketleftigg/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleN/summationdisplay
i=1r0(xi
t,ui
t,µN
t,νMF(µN
t,πt))−N/summationdisplay
i=1/summationdisplay
u∈Ur0(xi
t,u,µN
t,νMF(µN
t,πt))πt(xi
t,µN
t)(u)/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsinglexN
t/bracketrightigg/bracketrightigg
(a)
≤MR√
N
wherer0(·,·,·,·)≜r(·,·,·,·)/MR. Inequality (a) follows from Lemma 12. In particular, it utilises the fact
that{ui
t}i∈{1,···,N}are conditionally independent given xt, and the following relations hold.
|r0(xi
t,ui
t,µN
t,νMF(µN
t,πt))|≤1,
E/bracketleftig
r0(xi
t,ui
t,µN
t,νMF(µN
t,πt))/vextendsingle/vextendsingle/vextendsinglexN
t/bracketrightig
=/summationdisplay
u∈Ur0(xi
t,u,µN
t,νMF(µN
t,πt))πt(xi
t,µN
t)(u)
∀i∈{1,···,N},∀u∈U.
I Proof of Lemma 8
Observe that,
E|µN
t−µ∞
t|1≤E/vextendsingle/vextendsingleµN
t−PMF(µN
t−1,πt−1)/vextendsingle/vextendsingle
1+E/vextendsingle/vextendsinglePMF(µN
t−1,πt−1)−µ∞
t/vextendsingle/vextendsingle
1
(a)
≤CP√
N/bracketleftig/radicalbig
|X|+/radicalbig
|U|/bracketrightig
+E/vextendsingle/vextendsinglePMF(µN
t−1,πt−1)−PMF(µ∞
t−1,πt−1)/vextendsingle/vextendsingle
1
Inequality (a) follows from Lemma 6, and relation (8). Using Lemma 3, we get
/vextendsingle/vextendsinglePMF(µN
t−1,πt−1)−PMF(µ∞
t−1,πt−1)/vextendsingle/vextendsingle
1
≤˜SP|µN
t−1−µ∞
t−1|1+¯SP|πt−1(·,µN
t−1)−πt−1(·,µ∞
t−1)|∞
(a)
≤SP|µN
t−1−µ∞
t−1|1
whereSP≜˜SP+LQ¯SP. Inequality (a) follows from Assumption 3. Combining, we get,
E|µN
t−µ∞
t|1≤CP√
N/bracketleftig/radicalbig
|X|+/radicalbig
|U|/bracketrightig
+SPE/vextendsingle/vextendsingleµN
t−1−µ∞
t−1/vextendsingle/vextendsingle
1(44)
Recursively applying the above inequality, we finally obtain,
E|µN
t−µ∞
t|1≤CP√
N/bracketleftig/radicalbig
|X|+/radicalbig
|U|/bracketrightig/parenleftbiggSt
P−1
SP−1/parenrightbigg
25Published in Transactions on Machine Learning Research (09/2022)
J Proof of Lemma 9
Note that,
E/vextendsingle/vextendsingleµN
t+1−PMF(µN
t,πt)/vextendsingle/vextendsingle
1
(a)=/summationdisplay
x∈XE/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle1
NN/summationdisplay
i=1δ(xi
t+1=x)−/summationdisplay
x′∈X/summationdisplay
u∈UP(x′,u,µN
t)(x)πt(x′,µN
t)(u)1
NN/summationdisplay
i=1δ(xi
t=x′)/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
=/summationdisplay
x∈XE/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle1
NN/summationdisplay
i=1δ(xi
t+1=x)−1
NN/summationdisplay
i=1/summationdisplay
u∈UP(xi
t,u,µN
t)(x)πt(xi
t,µN
t)(u)/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
≤J1+J2
Equality (a) follows from the definition of PMF(·,·)as depicted in (8). The first term, J1, can be upper
bounded as follows.
J1≜1
N/summationdisplay
x∈XE/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleN/summationdisplay
i=1δ(xi
t+1=x)−N/summationdisplay
i=1P(xi
t,ui
t,µN
t)(x)/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
=1
N/summationdisplay
x∈XE/bracketleftigg
E/bracketleftigg/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleN/summationdisplay
i=1δ(xi
t+1=x)−N/summationdisplay
i=1P(xi
t,ui
t,µN
t)(x)/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsinglexN
t,uN
t/bracketrightigg/bracketrightigg
(a)
≤1√
N/radicalbig
|X|
Inequality (a)can be derived using Lemma 12, and the facts that {xi
t+1}i∈{1,···,N}are conditionally inde-
pendent given{xN
t,uN
t}, and,
E/bracketleftig
δ(xi
t+1=x)/vextendsingle/vextendsingle/vextendsinglexN
t,uN
t/bracketrightig
=P(xi
t,ui
t,µN
t)(x),
/summationdisplay
x∈XE/bracketleftig
δ(xi
t+1=x)/vextendsingle/vextendsingle/vextendsinglexN
t,uN
t/bracketrightig
= 1
∀i∈{1,···,N}, and∀x∈X. The second term can be bounded as follows.
J2≜1
N/summationdisplay
x∈XE/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleN/summationdisplay
i=1P(xi
t,ui
t,µN
t)(x)−N/summationdisplay
i=1/summationdisplay
u∈UP(xi
t,u,µN
t)(x)πt(xi
t,µN
t)(u)/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
(a)
≤1√
N/radicalbig
|X|
Inequality (a) is a consequence of Lemma 12. Specifically, it uses the facts that, {ui
t}i∈{1,···,N}are condi-
tionally independent given xN
t, and
E/bracketleftig
P(xi
t,ui
t,µN
t)(x)/vextendsingle/vextendsingle/vextendsinglexN
t/bracketrightig
=/summationdisplay
u∈UP(xi
t,u,µN
t)(x)πt(xi
t,µN
t)(u),
/summationdisplay
x∈XE/bracketleftig
P(xi
t,ui
t,µN
t)(x)/vextendsingle/vextendsingle/vextendsinglexN
t/bracketrightig
= 1
∀i∈{1,···,N}, and∀x∈X. This concludes the Lemma.
26Published in Transactions on Machine Learning Research (09/2022)
K Proof of Lemma 10
Note that,
E/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle1
N/summationdisplay
i=1r(xi
t,ui
t,µN
t)−rMF(µN
t,πt)/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
(a)=E/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle1
NN/summationdisplay
i=1r(xi
t,ui
t,µN
t)−/summationdisplay
x∈X/summationdisplay
u∈Ur(x,u,µN
t)πt(x,µN
t)(u)1
NN/summationdisplay
i=1δ(xi
t=x)/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
=E/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle1
NN/summationdisplay
i=1r(xi
t,ui
t,µN
t)−1
NN/summationdisplay
i=1/summationdisplay
u∈Ur(xi
t,u,µN
t)πt(xi
t,µN
t)(u)/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
=1
NE/bracketleftigg
E/bracketleftigg/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleN/summationdisplay
i=1r(xi
t,ui
t,µN
t)−N/summationdisplay
i=1/summationdisplay
u∈Ur(xi
t,u,µN
t)πt(xi
t,µN
t)(u)/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsinglexN
t/bracketrightigg/bracketrightigg
=MR
NE/bracketleftigg
E/bracketleftigg/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleN/summationdisplay
i=1r0(xi
t,ui
t,µN
t)−N/summationdisplay
i=1/summationdisplay
u∈Ur0(xi
t,u,µN
t)πt(xi
t,µN
t)(u)/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsinglexN
t/bracketrightigg/bracketrightigg
(a)
≤MR√
N
wherer0(·,·,·,·)≜r(·,·,·,·)/MR. Inequality (a) follows from Lemma 12. Specifically, it uses the fact that
{ui
t}i∈{1,···,N}are conditionally independent given xN
t, and
|r0(xi
t,ui
t,µN
t)|≤1,
E/bracketleftig
r0(xi
t,ui
t,µN
t)/vextendsingle/vextendsingle/vextendsinglexN
t/bracketrightig
=/summationdisplay
u∈Ur0(xi
t,u,µN
t)πt(xi
t,µN
t)(u)
∀i∈{1,···,N},∀u∈U.
L Proof of Lemma 11
Observe that,
E|µN
t−µ∞
t|1≤E/vextendsingle/vextendsingleµN
t−PMF(µN
t−1,πt−1)/vextendsingle/vextendsingle
1+E/vextendsingle/vextendsinglePMF(µN
t−1,πt−1)−µ∞
t/vextendsingle/vextendsingle
1
(a)
≤2√
N/radicalbig
|X|+E/vextendsingle/vextendsinglePMF(µN
t−1,πt−1)−PMF(µ∞
t−1,πt−1)/vextendsingle/vextendsingle
1
Inequality (a) follows from Lemma 6, and relation (8). Using Lemma 3, we get
/vextendsingle/vextendsinglePMF(µN
t−1,πt−1)−PMF(µ∞
t−1,πt−1)/vextendsingle/vextendsingle
1
≤˜SP|µN
t−1−µ∞
t−1|1+¯SP|πt−1(·,µN
t−1)−πt−1(·,µ∞
t−1)|∞
(a)
≤SP|µN
t−1−µ∞
t−1|1
whereSP≜˜SP+LQ¯SP. Inequality (a) follows from Assumption 3. Combining, we get,
E|µN
t−µ∞
t|1≤2√
N/radicalbig
|X|+SPE/vextendsingle/vextendsingleµN
t−1−µ∞
t−1/vextendsingle/vextendsingle
1(45)
Recursively applying the above inequality, we finally obtain,
E|µN
t−µ∞
t|1≤2√
N/radicalbig
|X|/parenleftbiggSt
P−1
SP−1/parenrightbigg
This concludes the Lemma.
27Published in Transactions on Machine Learning Research (09/2022)
M Sampling Algorithm
Algorithm 3 Sampling Algorithm
Input:µ0,πΦj,P,r
1:Samplex0∼µ0.
2:Sampleu0∼πΦj(x0,µ0)
3:ν0←νMF(µ0,πΦj)
4:t←0
5:FLAG←FALSE
6:while FLAG is FALSE do
7: FLAG←TRUEwith probability 1−γ.
8:Execute Update
9:end while
10:T←t
11:Accept (xT,µT,uT)as a sample.
12:ˆVΦj←0,ˆQΦj←0
13:FLAG←FALSE
14:SumRewards←0
15:while FLAG is FALSE do
16: FLAG←TRUEwith probability 1−γ.
17:Execute Update
18: SumRewards←SumRewards + r(xt,ut,µt,νt)
19:end while
20:With probability1
2,ˆVΦj←SumRewards . Otherwise ˆQΦj←SumRewards .
21:ˆAΦj(xT,µT,uT)←2(ˆQΦj−ˆVΦj).
Output:(xT,µT,uT)and ˆAΦj(xT,µT,uT)
Procedure Update:
1:xt+1∼P(xt,ut,µt,νt).
2:µt+1←PMF(µt,πΦj)
3:ut+1∼πΦj(xt+1,µt+1)
4:νt+1←νMF(µt+1,πΦj)
5:t←t+ 1
EndProcedure
28Published in Transactions on Machine Learning Research (09/2022)
N Proof of Theorem 3
Note that,
/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsinglesup
Φ∈RdvMARL (µ0,πΦ)−1
JJ/summationdisplay
j=1vMARL (µ0,˜πΦj)/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
≤/vextendsingle/vextendsingle/vextendsingle/vextendsinglesup
Φ∈RdvMARL (µ0,πΦ)−sup
Φ∈RdvMF(µ0,πΦ)/vextendsingle/vextendsingle/vextendsingle/vextendsingle+/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsinglesup
Φ∈RdvMF(µ0,πΦ)−1
JJ/summationdisplay
j=1vMF(µ0,πΦj)/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
≜J2
+1
JJ/summationdisplay
j=1/vextendsingle/vextendsinglevMF(µ0,πΦj)−vMARL (µ0,˜πΦj)/vextendsingle/vextendsingle
/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
≜J3
≤sup
Φ∈Rd|vMARL (µ0,πΦ)−vMF(µ0,πΦ)|
/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
≜J1+J2+J3
Using the same argument as used in Appendix A.3, we can conclude that,
J1≤C1√
N/bracketleftig/radicalbig
|X|+/radicalbig
|U|/bracketrightig
=C1e≤C1max{e,ϵ},
J3≤C3√
N/bracketleftig/radicalbig
|X|+/radicalbig
|U|/bracketrightig
=C3e≤C2max{e,ϵ}
for some constants C1,C3. Moreover, Lemma 1 suggests that,
J2≤√ϵbias
1−γ+ϵ≤√ϵbias
1−γ+ max{e,ϵ}
TakingC=C1+C3+ 1, we conclude the result. Under Assumption 4, using the same argument as is used
in Appendix B.2, we can improve the bounds on J1,J3as follows.
J1≤C1√
N/radicalbig
|X|,
J3≤C3√
N/radicalbig
|X|
This establishes the improved result.
29