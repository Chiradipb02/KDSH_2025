Published in Transactions on Machine Learning Research (10/2022)
Nonstationary Reinforcement Learning with Linear Function
Approximation
Huozhi Zhou hzhou35@illinois.edu
Department of Electrical and Computer Engineering
University of Illinois Urbana-Champaign
Jinglin Chen jinglinc@illinois.edu
Department of Computer Science
University of Illinois Urbana-Champaign
Lav R. Varshney varshney@illinois.edu
Department of Electrical and Computer Engineering
University of Illinois Urbana-Champaign
Ashish Jagmohan ashishja@us.ibm.com
IBM Research
Reviewed on OpenReview: https: // openreview. net/ forum? id= nS8A9nOrqp
Abstract
We consider reinforcement learning (RL) in episodic Markov decision processes (MDPs)
with linear function approximation under drifting environment. Specifically, both the re-
ward and state transition functions can evolve over time but their total variations do not
exceed a variation budget . We first develop LSVI-UCB-Restart algorithm, an optimistic
modification of least-squares value iteration with periodic restart, and bound its dynamic
regret when variation budgets are known. Then we propose a parameter-free algorithm
Ada-LSVI-UCB-Restart that extends to unknown variation budgets. We also derive the
first minimax dynamic regret lower bound for nonstationary linear MDPs and as a byprod-
uct establish a minimax regret lower bound for linear MDPs unsolved by Jin et al. (2020).
Finally, we provide numerical experiments to demonstrate the effectiveness of our proposed
algorithms.
1 Introduction
Reinforcement learning (RL) is a core control problem in which an agent sequentially interacts with an
unknown environment to maximize its cumulative reward (Sutton & Barto, 2018). RL finds enormous
applications in real-time bidding in advertisement auctions (Cai et al., 2017), autonomous driving (Shalev-
Shwartz et al., 2016), gaming-AI (Silver et al., 2018), and inventory control (Agrawal & Jia, 2019), among
others. Due to the large dimension of sequential decision-making problems that are of growing interest,
classical RL algorithms designed for finite state space such as tabular Q-learning (Watkins & Dayan,
1992) no longer yield satisfactory performance. Recent advances in RL rely on function approximators such
as deep neural nets to overcome the curse of dimensionality, i.e., the value function is approximated by
a function which is able to predict the value function for unseen state-action pairs given a few training
samples. This function approximation technique has achieved remarkable success in various large-scale
decision-making problems such as playing video games (Mnih et al., 2015), the game of Go (Silver et al.,
2017), and robot control (Akkaya et al., 2019). Motivated by the empirical success of RL algorithms with
function approximation, there is growing interest in developing RL algorithms with function approximation
that are statistically efficient (Yang & Wang, 2019; Cai et al., 2020; Jin et al., 2020; Modi et al., 2020; Wang
1Published in Transactions on Machine Learning Research (10/2022)
et al., 2020; Wei et al., 2021; Neu & Olkhovskaya, 2021; Jiang et al., 2017; Wang et al., 2020; Jin et al., 2021;
Du et al., 2021). The focus of this line of work is to develop statistically efficient algorithms with function
approximation for RL in terms of either regretorsample complexity . Such efficiency is especially crucial in
data-sparse applications such as medical trials (Zhao et al., 2009).
However, all of the aforementioned empirical and theoretical works on RL with function approximation
assume the environment is stationary, which is insufficient to model problems with time-varying dynamics.
For example, consider online advertising. The instantaneous reward is the payoff when viewers are redirected
to an advertiser, and the state is defined as the the details of the advertisement and user contexts. If the
targetusers’preferencesaretime-varying, time-invariantrewardandtransitionfunctionareunabletocapture
the dynamics. In general nonstationary random processes naturally occur in many settings and are able to
characterize larger classes of problems of interest (Cover & Pombra, 1989). Can one design a theoretically
sound algorithm for large-scale nonstationary MDPs? In general it is impossible to design algorithm to
achieve sublinear regret for MDPs with non-oblivious adversarial reward and transition functions in the
worst case (Yu et al., 2009). Then what is the maximum nonstationarity a learner can tolerate to adapt
to the time-varying dynamics of an MDP with potentially infinite number of states? This paper addresses
these two questions.
We consider the setting of episodic RL with nonstationary reward and transition functions. To measure
the performance of an algorithm, we use the notion of dynamic regret , the performance difference between
an algorithm and the set of policies optimal for individual episodes in hindsight. For nonstationary RL,
dynamic regret is a stronger and more appropriate notion of performance measure than static regret, but is
also more challenging for algorithm design and analysis. To incorporate function approximation, we focus on
a subclass of MDPs in which the reward and transition dynamics are linear in a known feature map (Melo
& Ribeiro, 2007), termed linear MDP . For any linear MDP, the value function of any policy is linear in the
known feature map since the Bellman equation is linear in reward and transition dynamics (Jin et al., 2020).
Since the optimal policy is greedy with respect to the optimal value function, linear function approximation
suffices to learn the optimal policy. For nonstationary linear MDPs, we show that one can design a near-
optimal statistically-efficient algorithm to achieve sublinear dynamic regret as long as the total variation
of reward and transition dynamics is sublinear. Let Tbe the total number of time steps, Bbe the total
variation of reward and transition function throughout the entire time horizion, dbe the ambient dimension
of the features, and Hbe the planning horizon.
The contribution of our work is summarized as follows.
•We prove a Ω(B1/3d2/3H1/3T2/3)minimax regret lower bound for nonstationary linear MDP, which shows
that it is impossible for any algorithm to achieve sublinear regret on any nonstationary linear MDP with
total variation linear in T. As a byproduct, we also derive the minimax regret lower bound for stationary
linear MDP on the order of Ω(d√
HT), which is unsolved in Jin et al. (2020).
•We develop the LSVI-UCB-Restart algorithm and analyze the dynamic regret bound for both cases
that local variations are known or unknown, assuming the total variations are known. We define local
variations (Eq. (2)) as the change in the environment between two consecutive epochs instead of the
total changes over the entire time horizon. When local variations are known, LSVI-UCB-Restart achieves
˜O(B1/3d4/3H4/3T2/3)dynamic regret, which matches the lower bound in BandT, up to polylogarithmic
factors. When local variations are unknown, LSVI-UCB-Restart achieves ˜O(B1/4d5/4H5/4T3/4)dynamic
regret.
•We propose a parameter-free algorithm called Ada-LSVI-UCB-Restart , an adaptive version of
LSVI-UCB-Restart , and prove that it can achieve ˜O(B1/4d5/4H5/4T3/4)dynamic regret without knowing
the total variations.
•We conduct numerical experiments on synthetic nonstationary linear MDPs to demonstrate the effective-
ness of our proposed algorithms.
2Published in Transactions on Machine Learning Research (10/2022)
1.1 Related Works
Nonstationary bandits Bandit problems can be viewed as a special case of MDP problems with unit
planning horizon. It is the simplest model that captures the exploration-exploitation tradeoff, a unique
feature of sequential decision-making problems. There are several ways to define nonstationarity in the
bandit literature. The first one is piecewise-stationary (Garivier & Moulines, 2011), which assumes the
expected rewards of arms change in a piecewise manner, i.e., stay fixed for a time period and abruptly
change at unknown time steps. The second one is to quantify the total variations of expected rewards of
arms (Besbes et al., 2014). The general strategy to adapt to nonstationarity for bandit problems is the
forgetting principle: run the algorithm designed for stationary bandits either on a sliding window or in small
epochs. This seemingly simple strategy is successful in developing near-optimal algorithms for many variants
of nonstationary bandits, such as cascading bandits (Wang et al., 2019), combinatorial semi-bandits (Zhou
et al., 2020) and linear contextual bandits (Cheung et al., 2019; Zhao et al., 2020; Russac et al., 2019). Other
nonstationary bandit models include the nonstationary rested bandit, where the reward of each arm changes
only when that arm is pulled (Cortes et al., 2017), and online learning with expert advice (Mohri & Yang,
2017a;b), where the qualities of experts are time-varying. However, reinforcement learning is much more
intricate than bandits. Note that naïvely adapting existing nonstationary bandit algorithms to nonstationary
RL leads to regret bounds with exponential dependence on the planing horizon H.
RL with function approximation Motivated by empirical success of deep RL, there is a recent line of
work analyzing the theoretical performance of RL algorithms with function approximation (Yang & Wang,
2019; Cai et al., 2020; Jin et al., 2020; Modi et al., 2020; Ayoub et al., 2020; Wang et al., 2020; Zhou et al.,
2021; Wei et al., 2021; Neu & Olkhovskaya, 2021; Huang et al., 2021; Modi et al., 2021; Jiang et al., 2017;
Agarwal et al., 2020; Dong et al., 2020; Jin et al., 2021; Du et al., 2021; Foster et al., 2021a; Chen et al.,
2022). Recent work also studies the instance-dependent sample complexity bound for RL with function
approximation, which adapts to the complexity of the specific MDP instance (Foster et al., 2021b; Dong &
Ma, 2022). All of these works assume that the learner is interacting with a stationary environment. In sharp
contrast, this paper considers learning in a nonstationary environment. As we will show later, if we do not
properly adapt to the nonstationarity, linear regret is incurred.
Nonstationary RL The last relevant line of work is on dynamic regret analysis of nonstationary MDPs
mostly without function approximation (Auer et al., 2010; Ortner et al., 2020; Cheung et al., 2019; Fei et al.,
2020;Cheungetal.,2020). TheworkofAueretal.(2010)considersthesettinginwhichtheMDPispiecewise-
stationary and allowed to change in ltimes for the reward and transition functions. They show that UCRL2
with restart achieves ˜O(l1/3T2/3)dynamic regret, where Tis the time horizon. Later works (Ortner et al.,
2020; Cheung et al., 2020; Fei et al., 2020) generalize the nonstationary setting to allow reward and transition
functions vary for any number of time steps, as long as the total variation is bounded. Specifically, the work
of (Ortner et al., 2020) proves that UCRLwith restart achieves ˜O((Br+Bp)1/3T2/3)dynamic regret (when
the variation in each epoch is known), where BrandBpdenote the total variation of reward and transition
functionsoveralltimesteps. Cheungetal.(2020)proposesanalgorithmbasedon UCRL2bycombiningsliding
windows and a confidence widening technique. Their algorithm has slightly worse dynamic regret bound
˜O((Br+Bp)1/4T3/4)without knowing the local variations. Further, Fei et al. (2020) develops an algorithm
which directly optimizes the policy and enjoys near-optimal regret in the low-variation regime. A different
model of nonstationary MDP is proposed by Lykouris et al. (2021), which smoothly interpolates between
stationary and adversarial environments, by assuming that most episodes are stationary except for a small
number of adversarial episodes. Note that Lykouris et al. (2021) considers linear function approximation,
but their nonstationarity assumption is different from ours. In this paper, we assume the variation budget
for reward and transition function is bounded, which is similar to the settings in Ortner et al. (2020);
Cheung et al. (2020); Mao et al. (2021). Concurrently to our work, Touati & Vincent (2020) propose an
algorithm combining weighted least-squares value iteration and the optimistic principle, achieving the same
˜O(B1/4d5/4H5/4T3/4)regret as we do with knowledge of the total variation B. They do not have a dynamic
regret bound when the knowledge of local variations is available. Their proposed algorithm uses exponential
weights to smoothly forget data that are far in the past. By contrast, our algorithm periodically restarts
theLSVI-UCB algorithm from scratch to handle the non-stationarity and is much more computationally
3Published in Transactions on Machine Learning Research (10/2022)
efficient. Another concurrent work by Wei & Luo (2021) follows a substantially different approach to achieve
the optimal T2/3regret. The key idea of their algorithm is to run multiple base algorithms for stationary
instances with different duration simultaneously, under a carefully designed random schedule. Compared
with them, our algorithm has a slightly worse rate, but a much better computational complexity, since we
only require to maintain one instance of the base algorithm. Both of these two concurrent works do not
have empirical results, and we are also the first one to conduct numerical experiments on online exploration
for non-stationary MDPs (Section 6). Other related and concurrent works investigate online exploration
in different classes of non-stationary MDPs, including linear kernal MDP (Zhong et al., 2021), constrained
tabular MDP (Ding & Lavaei, 2022), and stochastic shorted path problem (Chen & Luo, 2022).
The rest of the paper is organized as follows. Section 2 presents our problem definition. Section 3 establishes
the minimax regret lower bound for nonstationary linear MDPs. Section 4 and Section 5 present our
algorithms LSVI-UCB-Restart ,Ada-LSVI-UCB-Restart and their dynamic regret bounds. Section 6 shows
our experiment results. Section 7 concludes the paper and discusses some future directions. All detailed
proofs can be found in Appendices.
Notation We use⟨·,·⟩to denote inner products in Euclidean space, ∥v∥2to denote the L2norm of vector
v, and∥v∥Λto denote the norm induced by a positive definite matrix Afor vectorv, i.e.,∥v∥Λ=√
v⊤Λv.
For an integer N, we denote the set of positive integers {1,2,...,N}as[N].
2 Preliminaries
We consider the setting of a nonstationary finite-horizon episodic Markov decision process (MDP), specified
by a tuple (S,A,H,K, P={Pk
h}h∈[H],k∈[K],r={rk
h}h∈[H],k∈[K]), where the setSis the collection of states,
Ais the collection of actions, His the length of each episode, Kis the total number of episodes, and P
andrare the transition kernel and deterministic reward functions respectively. Moreover, Pk
h(·|s,a)denotes
the transition kernel over the next states if the action ais taken for state sat stephin thek-th episode,
andrk
h:S×A→ [0,1]is the deterministic reward function at step hin thek-th episode. Note that we
are considering a nonstationary setting, thus we assume the transition kernel Pand reward function rmay
change in different episodes. We will explicitly quantify the nonstationarity later.
The learning protocol proceeds as follows. In episode k, the initial state sk
1is chosen by an adversary.
Then at each step h∈[H], the agent observes the current state sh∈S, takes an action ah∈A, receives
an instantaneous reward rk
h(sh,ah), then transitions to the next state sh+1according to the distribution
Pk
h(·|sh,ah). This process terminates at step Hand then the next episode begins. The agent interacts with
the environment for Kepisodes, which yields T=KHtime steps in total.
A policyπis a collection of functions πh:S→A,∀h∈[H]. We define the value function at step hin
thek-th episode for policy πas the expected value of the cumulative rewards received under policy πwhen
starting from an arbitrary state s:
Vπ
h,k(s) =Eπ/bracketleftiggH/summationdisplay
h′=hrk
h′(sh′,ah′)|sh=s/bracketrightigg
,∀s∈S,h∈[H],k∈[K].
Notethatthevaluefunctionalsodependsontheepisode ksincetheMarkovdecisionprocessisnonstationary.
Similarly, we can also define the action-value function (a.k.a. Qfunction) for policy πat stephin thek-th
episode, which gives the expected cumulative reward starting from an arbitrary state-action pair:
Qπ
h,k(s,a) =rk
h(s,a) +Eπ/bracketleftiggH/summationdisplay
h′=h+1rk
h′(sh′,ah′)|sh=s,ah=a/bracketrightigg
,∀(s,a)∈S×A,h∈[H],k∈[K].
We define the optimal value function and optimal action-value function for step hink-th episode as
V∗
h,k(s) = supπVπ
h,k(s)andQ∗
h,k(s,a) = supπQπ
h,k(s,a)respectively, which always exist (Puterman, 2014).
For simplicity, we denote Es′∼Pk
h(·|s,a)[Vh+1(s′)] = [Pk
hVh+1](s,a). Using this notation, we can write down the
4Published in Transactions on Machine Learning Research (10/2022)
Bellman equation for any policy π,
Qπ
h,k(s,a) = (rk
h+Pk
hVπ
h+1,k)(s,a),Vπ
h,k(s) =Qπ
h,k(s,πh(s)),Vπ
H+1,k(s) = 0,∀(s,a,k )∈S×A× [K].
Similarly, the Bellman optimality equation is
Q∗
h,k(s,a) = (rk
h+Pk
hV∗
h+1,k)(s,a),V∗
h,k(s) = max
a∈AQ∗
h,k(s,a),V∗
H+1,k(s) = 0,∀(s,a,k )∈S×A× [K].
This implies that the optimal policy π∗is greedy with respect to the optimal action-value function Q∗(·,·).
Thus in order to learn the optimal policy it suffices to estimate the optimal action-value function.
Recall that the agent is learning the optimal policy via interactions with the environment, despite the
uncertainty of randP. In thek-th episode, the adversary chooses the initial state sk
1, then the agent decides
its policyπkfor this episode based on historical interactions. To measure the convergence to optimality, we
consider an equivalent objective of minimizing the dynamic regret (Cheung et al., 2020),
Dyn-Reg (K) =K/summationdisplay
k=1/bracketleftig
V∗
1,k(sk
1)−Vπk
1,k(sk
1)/bracketrightig
. (1)
2.1 Linear Markov Decision Process
We consider a special class of MDPs called linear Markov decision process (Melo & Ribeiro, 2007; Bradtke
& Barto, 1996; Jin et al., 2020), which assumes both transition function Pand reward function rare linear
in a known feature map ϕ(·,·). The formal definition is as follows.
Definition 1. (Linear MDP). The MDP (S,A,H,K, P,r)is a linear MDP with the feature map ϕ:S×A→
Rd, if for any (h,k)∈[H]×[K], there exist dunknown measures µh,k= (µ1
h,k,...,µd
h,k)⊤onSand a vector
θh,k∈Rdsuch that
Pk
h(s′|s,a) =ϕ(s,a)⊤µh,k(s′), rk
h(s,a) =ϕ(s,a)⊤θh,k.
Without loss of generality, we assume ∥ϕ(s,a)∥2≤1for all (s,a)∈S×A, and max{∥µh,k(S)∥2,∥θh,k∥2}≤√
dfor all (h,k)∈[H]×[K].
Note that the transition function Pand reward function rare determined by the unknown measures
{µh,k}h∈[H],k∈[K]and latent vectors {θh,k}h∈[H],k∈[K]. The quantities µh,kandθh,kvary across time in
general, which leads to change in transition function Pand reward function r. Following Besbes et al.
(2014); Cheung et al. (2019; 2020), we quantify the total variation on µandθin terms of their respective
variation budget BθandBµ, and define the total variation budget Bas the summation of these two variation
budgets:
Bθ=K/summationdisplay
k=2H/summationdisplay
h=1∥θh,k−θh,k−1∥2, Bµ=K/summationdisplay
k=2H/summationdisplay
h=1∥µh,k(S)−µh,k−1(S)∥2, B =Bθ+Bµ,
whereµh,k(S)is the concatenation of µh,k(s)for all states.
Remark 1. One may find the definition of Bµto be restrictive, since Bµsums over all possible states and
the number of states can be infinite in linear MDPs. However, as indicated by Theorem 1, we cannot remove
the dependency of Bµin the worst case. It is possible to define a nonstationarity measure that does not
depend on all states if we impose additional constraints on linear MDPs. For example, we can add some
reachability constraints on the state space to avoid the dependency on all states to some extent. But such
reaching probability in general would also change in the nonstationary environment; therefore, it is hard to
define and capture this. Future work may define a new environment change measure.
3 Minimax Regret Lower Bound
Inthissection, wederiveminimaxregretlowerboundsfornonstationarylinearMDPsinbothinhomogeneous
and homogeneous settings, which quantify the fundamental difficulty when measured by the dynamic regret
5Published in Transactions on Machine Learning Research (10/2022)
in nonstationary linear MDPs. More specifically, we consider inhomogeneous setting in this paper, where
the transition function Pk
h(as introduced in Section 1) can be different for different h. In contrast, for the
homogeneous setting, the transition function Pk
hwill be the same within an episode, i.e., for any k,Pk
h≡Pk
for anyh={1,...,H}. All of the detailed proofs for this section are in Appendix A.
For the homogeneous setting, the minimax dynamic regret lower bound is the following.
Theorem 1. For any algorithm, the dynamic regret is at least Ω(B1/3d2/3H1/3T2/3)for one nonstationary
homogeneous linear MDP instance, if d≥4,T≥64(d−3)2H.
Sketch of Proof. TheconstructionofthelowerboundinstanceisbasedontheconstructionusedinTheorem8
in Appendix A. Nature divides the whole time horizon into ⌈K
N⌉intervals of equal length Nepisodes (the
last episode may be shorter). For each interval, nature initiates a new stationary linear MDP parametrized
byv, which is drawn from a set {±√
d−3/√
N}d−3. Note that nature chooses the parameters for the linear
MDP for each interval only depending on the learner’s policy, and the worst-case regret for each interval is at
least Ω(d√
H2N). Since there are at least ⌈K
N⌉−1intervals, the total regret is at least Ω(d√
H2K2N−1/2).
By checking the total variation budget B, we can obtain the lower bound for N, which is Ω(B−2/3d2/3K2/3).
Then we can obtain the desired regret lower bound. For the detailed proof, please refer to Appendix A.
For the inhomogeneous setting, the minimax dynamic regret lower bound is the following.
Theorem 2. For any algorithm, the dynamic regret is at least Ω(B1/3d5/6HT2/3)for one nonstationary
inhomogenous linear MDP instance, if d≥4,H≥3,T≥(d−1)2H2/2.
Sketch of Proof. The proof idea is similar to that of Theorem 1. The only difference is that within each
piecewise-stationary segment, we use the hard instance constructed by Zhou et al. (2021); Hu et al. (2022)
for inhomogenous linear MDPs. Optimizing the length of each piecewise-stationary segment Nand the
variation magnitude between consecutive segments (subject to the constraints of the total variation budget)
leads to our lower bound.
Comparing these lower bounds, we can see that learning in inhomogeneous settings is indeed harder since
there is more freedom in the transition function.
4LSVI-UCB-Restart Algorithm
In this section, we describe our proposed algorithm LSVI-UCB-Restart , and discuss how to tune the hyper-
parameters for cases when local variation is known or unknown. For both cases, we present their respective
regret bounds. Detailed proofs are deferred to Appendix B. Note that our algorithms are all designed for
inhomogeneous setting.
4.1 Algorithm Description
Our proposed algorithm LSVI-UCB-Restart has two key ingredients: least-squares value iteration with
upper confidence bound to properly handle the exploration-exploitation trade-off (Jin et al., 2020), and
restart strategy to adapt to the unknown nonstationarity. Our algorithm is summarized in Algorithm 1.
From a high-level point of view, our algorithm runs in epochs. At each epoch, we first estimate the action-
value function by solving a regularized least-squares problem from historical data, then construct the upper
confidence bound for the action-value function, and update the policy greedily w.r.t. action-value function
plus the upper confidence bound. Finally, we periodically restart our algorithm to adapt to the nonstationary
nature of the environment.
Next, we delve into more details of the algorithm design. Note that in a linear MDP, for any policy π, the
Q-function is linear in the feature embedding ϕ(·,·). As a preliminary, we briefly introduce least-squares
value iteration (Bradtke & Barto, 1996; Osband et al., 2016), which is the key tool to estimate w, the latent
vector used to form the optimal action-value function, Q∗
h,k(·,·) =⟨ϕ(·,·),w⟩. Least-squares value iteration
6Published in Transactions on Machine Learning Research (10/2022)
is a natural extension of classical value iteration algorithm (Sutton & Barto, 2018), which finds the optimal
action-value function by recursively applying Bellman optimality equation,
Q∗
h,k(s,a) = [rh,k+Pk
hmax
a′∈AQ∗
h+1,k−1(·,a′)](s,a).
In practice, the transition function Pis unknown, and the state space might be so large that it is impossible
for the learner to fully explore all states. If we parametrize the action-value function in a linear form as
⟨ϕ(·,·),w⟩, it is natural to solve a regularized least-squares problems using collected data inspired by classical
value iteration. Specifically, the update formula of wk
hin Algorithm 1 (line 8) is the analytic solution of the
following regularized least-squares problem:
wk
h= arg min
wk−1/summationdisplay
l=τ[rh,l(sl
h,al
h) + max
a∈AQk−1
h+1(sl
h+1,a)−⟨ϕ(sl
h,al
h),w⟩]2+∥w∥2.
One might be skeptical since simply applying least-squares method to solve wdoes not take the distribution
drift in Pandrinto account and hence, may lead to non-trivial estimation error. However, we show that
the estimation error can gracefully adapt to the nonstationarity, and it suffices to restart the estimation
periodically to achieve good dynamic regret.
In addition to least-squares value iteration, the inner loop of Algorithm 1 also adds an additional quadratic
bonus term β∥ϕ(·,·)∥(Λk
h)−1(line 9) to encourage exploration, where βis a scalar and Λk
his the Gram matrix
of the regularized least-squares problem. Intuitively, 1/∥ϕ∥−1
(Λk
h)−1is the effective sample number of the
agent observed so far in the direction of ϕ, thus the quadratic bonus term can quantify the uncertainty of
estimation. We will show later if we tune βkproperly, then our action-value function estimate Qk
hcan be an
optimistic upper bound or an approximately optimistic upper bound of the optimal action-value function,
so we can adapt the principle of optimism in the face of uncertainty (Auer et al., 2002a) to explore.
Finally, we use epoch restart strategy to adapt to the drifting environment, which achieves near-optimal
dynamic regret notwithstanding its simplicity. Specifically, we restart the estimation of wafterW
Hepisodes,
all illustrated in the outer loop of Algorithm 1. Note that in general epoch size Wcan vary for different
epochs, but we find that a fixed length is sufficient to achieve near-optimal performance.
To sum up, our algorithm follows the algorithmic principles of LSVI-UCB (Jin et al., 2020). The key differ-
ence and challenge is to set the confidence parameter properly and use periodic restart strategy to handle
nonstationarity.
Algorithm 1 LSVI-UCB-Restart Algorithm
Require: time horizon T, epoch size W
1:Set epoch counter j= 1.
2:whilej≤⌈T
W⌉do
3:setτ= (j−1)W
H
4:for allk=τ,τ+ 1,..., min(τ+W
H−1,K)do
5:Receive the initial state sk
1.
6:for allsteph=H,..., 1do
7: Λk
h←/summationtextk−1
l=τϕ(sl
h,al
h)ϕ(sl
h,al
h)⊤+I
8:wk
h←(Λk
h)−1/summationtextk−1
l=τϕ(sl
h,al
h)[rh,l(sl
h,al
h) + maxaQk−1
h+1(sl
h+1,a)]
9:Qk
h(·,·)←min{(wk
h)⊤ϕ(·,·) +βk∥ϕ(·,·)∥(Λk
h)−1,H}
10:end for
11:for allsteph= 1,...,Hdo
12: take action ak
h←arg maxaQk
h(sk
h,a), and observe sk
h+1
13:end for
14:end for
15:setj=j+ 1
16:end while
7Published in Transactions on Machine Learning Research (10/2022)
4.2 Regret Analysis
Now we derive the dynamic regret bounds for LSVI-UCB-Restart , first introducing additional notation for
local variations. We let
Bθ,E=/summationdisplay
k∈EH/summationdisplay
h=1∥θh,k−θh,k−1∥2andBµ,E=/summationdisplay
k∈EH/summationdisplay
h=1∥µh,k(S)−µh,k−1(S)∥2(2)
be the local variation for θandµin epochE. To derive the dynamic regret lower bounds, we need the
following lemma to control the fluctuation of least-squares value iteration.
Lemma 1. (Modified from Jin et al. (2020)) Denote τto be the first episode in the epoch which contains
episodek. There exists an absolute constant Csuch that the following event E,
/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoublek−1/summationdisplay
l=τϕl
h[Vk
h+1(sl
h+1)−Pl
hVk
h+1(sl
h,al
h)]/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble
(Λk
h)−1≤CdH/radicalig
log[2(cβ+ 1)dW/p ],∀(k,h)∈E× [H].
happens with probability at least 1−p/2.
Next we proceed to derive the dynamic regret bounds for two cases: (1) local variations are known, and (2)
local variations are unknown.
4.2.1 Known Local Variations
Forthecaseofknownlocalvariations, underevent EdefinedinLemma1,theestimationerrorofleast-squares
estimation scales with the quadratic bonus ∥ϕ(·,·)∥(Λk
h)−1uniformly for any policy π, which is detailed in
the following lemma.
Lemma 2. Under event Edefined in Lemma 1, we have for any policy π,∀s,a,h,k∈S×A× [H]×E,
|⟨ϕ(s,a),wk
h⟩−Qπ
h,k(s,a)−Pk
h(Vk
h+1−Vπ
h+1,k)(s,a)|≤βk∥ϕ(s,a)∥(Λk
h)−1,
whereβk=C0dH/radicalbig
log(2dW/p )+Bθ,E/radicalbig
d(k−τ)+Bµ,EH/radicalbig
d(k−τ)andτis the first episode in the current
epoch.
The proof of Lemma 2 is included in Appendix B. Based on Lemma 2, we can show that if we set βk
properly with knowledge of local variations Bθ,EandBµ,E, the action-value function estimate maintained in
Algorithm 1 is an upper bound of the optimal action-value function under event E.
Lemma 3. Under event Edefined in Lemma 1, for episode k, if we set βk=cdH/radicalbig
log(2dW/p ) +
Bθ,E/radicalbig
d(k−τ) +Bµ,EH/radicalbig
d(k−τ), we have
Qk
h(s,a)≥Q∗
h,k,∀(s,a,h,k )∈S×A× [H]×E.
Sketch of Proof. For the last step Hin each episode, the results hold due to Lemma 2 since after step H
there is no reward and the episode terminates. We then prove Qk
his indeed the upper bound for the optimal
action-value function Q∗
h,kfor remaining h∈[H−1]by induction. Please see Appendix B for details.
After showing the action-value function estimate is the optimistic upper bound of the optimal action-value
function, we can derive the dynamic regret bound within one epoch via recursive regret decomposition. The
dynamic regret within one epoch for Algorithm 1 with the knowledge of Bθ,EandBµ,Eis as follows, and
the proof is deferred to Appendix B.
Theorem 3. For each epochEwith epoch size W, setβin thek-th episode as βk=cdH/radicalbig
log(2dW/p ) +
Bθ,E/radicalbig
d(k−τ) +Bµ,EH/radicalbig
d(k−τ), wherecis an absolute constant and p∈(0,1). Then the dynamic regret
within that epoch is ˜O(H3/2d3/2W1/2+Bθ,EdW+Bµ,EdHW )with probability at least 1−p.
8Published in Transactions on Machine Learning Research (10/2022)
By summing over all epochs and applying the union bound, we can obtain the dynamic regret upper bound
forLSVI-UCB-Restart for the whole time horizon.
Theorem 4. If we setβk=cdH/radicalbig
log(2dT/p ) +Bθ,E/radicalbig
d(k−τ) +Bµ,EH/radicalbig
d(k−τ), the dynamic regret of
LSVI-UCB-Restart is˜O(H3/2d3/2TW−1/2+BθdW+BµdHW ), with probability at least 1−p.
By properly tuning the epoch size W, we can obtain a tight dynamic regret upper bound.
Corollary 1. LetW=⌈B−2/3T2/3d1/3H−2/3⌉H, andβk=cdH/radicalbig
log(2dW/p ) +Bθ,E/radicalbig
d(k−τ) +
Bµ,EH/radicalbig
d(k−τ)for each epoch. LSVI-UCB-Restart achieves ˜O(B1/3d4/3H4/3T2/3)dynamic regret, with
probability at least 1−p.
Remark 2. Corollary 1 shows that if local variations are known, we can achieve near-optimal dependency
on the the total variation Bθ,Bµand time horizon Tcompared to the lower bound provided in Theorem 1.
However, the dependency on dandHis worse. The dependency on dis unlikely to improve unless there is
an improvement to LSVI-UCB.
Remark 3. The definition of total variation Bis related to the misspecification error defined by Jin et al.
(2020). One can apply the Cauchy-Schwarz inequality to show that our total variation bound implies that
misspecification in Eq. (4) of Jin et al. is also bounded (but not vice versa). However, the regret analysis in
the misspecified linear MDP of Jin et al. (2020) is restricted to static regret, so we cannot directly borrow
their analysis for the misspecified setting (Jin et al., 2020) to handle our dynamic regret (as defined in Eq.
(1)).
Remark 4. For the case when the environment changes abruptly Ltimes, our algorithm enjoys an
˜O(L1/3T2/3)dynamic regret bound, which is sub-optimal compared to Wei & Luo (2021). The reason is
that periodic restart is not a suitable strategy to handle abrupt changes since the passive nature indicates
that we cannot guarantee detecting the abrupt environment change within a reasonably short delay. Wei &
Luo (2021) overcome this issue by running two tests on top of multiple base instances with different scales to
detect the environmental change. Similar ideas have also been used in the piecewise-stationary bandit liter-
ature (Besson & Kaufmann, 2019), where a change detection subroutine is run to detect the environmental
change, so the regret incurred by the environmental drift can be better controlled.
4.2.2 Unknown Local Variation
If the local variations are unknown, the proof is similar to the case of known local variations. We only
highlight the differences compared to the previous case. The key difference is that without knowledge of
local variations BθandBµ, we set the hyper-parameter β=cdH/radicalbig
log(2dW/p ). As a result, the action-
value function estimate Qk
hmaintained in Algorithm 1 is no longer the optimistic upper bound of the optimal
action-value function, but only approximately, up to some error, proportional to the local variation. The
rigorous statement is as follows.
Lemma 4. Under event Edefined in Lemma 1, if we set β=cdH/radicalbig
log(2dW/p ), we have
∀(s,a,h,k )∈S×A× [H]×E,
Qk
h(s,a)≥Q∗
h,k−(H−h+ 1)(Bθ,E/radicalbig
d(k−τ) +Bµ,EH/radicalbig
d(k−τ)).
Sketch of Proof. For the case when local variations are unknown, the least -squares estimation error have
some additional terms that are linear in the local variations Bθ,EandBµ,E(See Lemma 10 in Appendix B).
Then we can prove that Qk
his an approximate upper bound of Q∗
h,kvia induction. For details, please see
Appendix B.
By applying a similar proof technique as Theorem 3, we can derive the dynamic regret within one epoch
when local variations are unknown.
Theorem 5. For each epochEwith epoch size W, if we setβk=cdH/radicalbig
log(2dW/p ), wherecis an absolute
constant and p∈(0,1), then the dynamic regret within that epoch is ˜O(√
d3H3W+Bθ,E/radicalbig
d/HW3/2+
Bµ,E√
dHW3/2)with probability at least 1−p, whereBθ,EandBµ,Eare the total variation within that
epoch.
9Published in Transactions on Machine Learning Research (10/2022)
By summing regret over epochs and applying a union bound over all epochs, we obtain the dynamic regret
ofLSVI-UCB-Restart for the whole time horizon.
Theorem 6. If we set β=cdH/radicalbig
log(2dT/p ), then the dynamic regret of LSVI-UCB-Restart is
˜O(d3/2H3/2TW−1/2+Bθd1/2H−1/2W3/2+Bµd1/2H1/2W3/2), with probability at least 1−p.
By properly tuning the epoch size W, we can obtain a tight regret bound for the case of unknown local
variations as follows.
Corollary 2. LetW=⌈B−1/2T1/2d1/2H−1/2⌉Handβk=cdH/radicalbig
log(2dW/p ). Then LSVI-UCB-Restart
achieves ˜O(B1/4d5/4H5/4T3/4)dynamic regret, with probability at least 1−p.
Remark 5. Our algorithm has a slightly worse regret bound compared with Wei & Luo (2021). However, our
algorithm has a much better better computational complexity, since we only require to maintain one instance
of the base algorithm. We are also the first one to conduct numerical experiments on online exploration for
non-stationary MDPs (Section 6). How to achieve the ˜O(T2/3)dynamic regret bound without prior knowledge
and with only one base instance is still an open problem.
5Ada-LSVI-UCB-Restart : a Parameter-free Algorithm
In practice, the total variations BθandBµare unknown. To mitigate this issue, we present a parameter-free
algorithm Ada-LSVI-UCB-Restart and its dynamic regret bound.
5.1 Algorithm Description
Inspired by bandit-over-bandit mechanism (Cheung et al., 2019), we develop a new parameter-free algorithm.
The key idea is to use LSVI-UCB-Restart as a subroutine (set β=cdH/radicalbig
log(2dT/p )since we assume total
variations are unknown), and periodically update the epoch size based on the historical data under the time-
varying Pandr(potentially adversarial). More specifically, Ada-LSVI-UCB-Restart (Algorithm 2) divides
the whole time horizon into ⌈T
HM⌉blocks of equal length Mepisodes (the length of the last block can be
smaller than Mepisodes), and specifies a set JWfrom which epoch size is drawn. For each block i∈[⌈T
HM⌉],
Ada-LSVI-UCB runs a master algorithm to select the epoch size Wiand runs LSVI-UCB-Restart withWi
for the current block. After the end of this block, the total reward of this block is fed back to the master
algorithm, and the posteriors of the parameters are updated accordingly.
For the detailed master algorithm, we select EXP3-P(Bubeck & Cesa-Bianchi, 2012) since it is able to deal
with non-oblivious adversary. Now we present the details of Ada-LSVI-UCB-Restart . We set the length of
each block Mand the feasible set of epoch size JWas follows:
M=⌈5T1/2d1/2H−1/2⌉,JW={H,2H,4H,...,MH}.
The intuition of designing the feasible set for epoch size JWis to guarantee it can well-approximate the
optimal epoch size with the knowledge of total variations while on the other hand make it as small as
possible, so the learner do not lose much by adaptively selecting the epoch size from JW. This intuition is
more clear when we derive the dynamic regret bound of Ada-LSVI-UCB-Restart . Denoting|JW|= ∆, the
master algorithm EXP3-Ptreats each element of JWas an arm and updates the probabilities of selecting
each feasible epoch size based on the reward collected in the past. It begins by initializing
α= 0.95/radicaligg
ln ∆
∆⌈T/MH⌉, β=/radicaligg
ln ∆
∆⌈T/MH⌉, γ= 1.05/radicaligg
ln ∆
∆⌈T/MH⌉, ql,1= 0, l∈[∆], (3)
whereα,β,γare parameters used in EXP3-Pandql,1,l∈[∆]are the initialization of the estimated total
reward of running different epoch lengths. At the beginning of the block i, the agent first sees the initial
states(i−1)H
1, and updates the probability of selecting different epoch lengths for block ias
ul,i= (1−γ)exp(αql,i)/summationtext
l∈[∆]exp(αql,i)+γ
∆. (4)
10Published in Transactions on Machine Learning Research (10/2022)
Then the master algorithm samples li∈[∆]according to the updated distribution {ul,i}i∈[∆]; the epoch
sizeWifor the block iis chosen as li-th element in JW,⌊Mli/⌊lnM⌋⌋H. After selecting the epoch size
Wi,Ada-LSVI-UCB runs a new copy of LSVI-UCB-Restart with that epoch size. By the end of each block,
Ada-LSVI-UCB-Restart observes the total reward of the current block, denoted as Ri(Wi,s(i−1)H
1 ), then the
algorithm updates the estimated total reward of running different epoch sizes (divide Ri(Wi,s(i−1)H
1 )by
MHto normalize):
ql,i+1=ql,i+β+1{l=li}Ri(Wi,s(i−1)H
1 )/MH
ul,i. (5)
Algorithm 2 ADA-LSVI-UCB-Restart Algorithm
Require: time horizon T, block length M, feasible set of epoch size Jw
1:Initializeα,β,γand{ql,1}l∈[∆]according to Eq. 3.
2:for alli= 1,2,...,⌈T/HM⌉do
3:Receive the initial state s(i−1)H
1
4:Update the epoch size selection distribution {ul,i}l∈[∆]according to Eq. 4
5:Sampleli∈[∆]from the updated distribution {ul,i}l∈[∆], then set the epoch size for block ias
Wi=⌊Mli/⌊lnM⌋⌋H.
6:for allt= (i−1)MH + 1,..., min(iMH,T )do
7:Run LSVI-UCB-Restart algorithm with epoch size Wi
8:end for
9:After observing the total reward for block i,Ri(Wi,s(i−1)H
1 ), update the estimated total reward of
running different epoch sizes {ql,i+1}l∈[∆]according to Eq. 5
10:end for
5.2 Regret Analysis
Now we present the dynamic regret bound achieved by Ada-LSVI-UCB-Restart .
Theorem 7. The dynamic regret of Ada-LSVI-UCB-Restart is˜O(B1/4d5/4H5/4T3/4).
Sketch of Proof. To analyze the dynamic regret of Ada-LSVI-UCB-Restart , we decompose the dynamic
regret into two terms. The first term is regret incurred by always selecting the best W†fromJW, and the
second term is regret incurred by adaptively selecting the epoch size from JWviaEXP3-Prather than always
selectingW†. For the second term, we reduce to an adversarial bandit problem and directly use the regret
bound of EXP3-P(Bubeck & Cesa-Bianchi, 2012; Auer et al., 2002b). For the first term, we show that W†
can well-approximate the optimal epoch size with the knowledge of BµandBθ, up to constant factors. Thus
we can use Theorem 6 to bound the first term. For details, see Appendix C.
Remark6. Using the master algorithm to select the window size is reminiscent of model selection approaches
to online RL (Agarwal et al., 2017; Pacchiano et al., 2020; Lee et al., 2021; Abbasi-Yadkori et al., 2020).
Typically model selection approaches achieve worse rates compared to the best base algorithm. However, in
our setting we can discretize the feasible set for epoch size JWat a proper granularity, so we can control the
additional regret incurred by the master algorithm with respect to the best epoch size in JWto be reasonably
small. That is, we have more freedom to choose the base algorithms compared to some problem settings in
the model selection literature. As a result, the regret bound does not lose much compared to the case where
we have knowledge of total variation B.
6 Experiments
In this section, we perform empirical experiments on synthetic datasets to illustrate the effectiveness
ofLSVI-UCB-Restart and Ada-LSVI-UCB-Restart . We compare the cumulative rewards of the pro-
posed algorithms with five baseline algorithms: Epsilon-Greedy (Watkins, 1989), Random-Exploration ,
11Published in Transactions on Machine Learning Research (10/2022)
LSVI-UCB (Jin et al., 2020), OPT-WLSVI (Touati & Vincent, 2020), and MASTER(Wei & Luo, 2021). As dis-
cussed before, we are the first one to perform numerical experiments on online exploration for non-stationary
MDPs and demonstrate the effectiveness of proposed algorithms.
The agent takes actions uniformly in Random-Exploration . In Epsilon-Greedy , instead of adding a
bonus term as in LSVI-UCB , the agent takes the greedy action according to the current estimate of Q
function with probability 1−ϵ, and takes the action uniformly at random with probability ϵ, where
we setϵ= 0.05. For LSVI-UCB and LSVI-UCB-Restart , we setβ= 0.001cdH/radicalbig
log(200dT). In addi-
tion, for LSVI-UCB-Restart we test the performance of two cases: (1) known global variation, where we
setW=⌈B−1/2T1/2d1/2H−1/2⌉H; (2) unknown global variation (denoted LSVI-UCB-Unknown ), where
we setW=⌈T1/2d1/2H−1/2⌉H(the dynamic regret bound is ˜O(Bd5/4H5/4T3/4)for this case). For
ADA-LSVI-UCB-Restart , we set the length of each block M=⌈0.2T1/2d1/2H1/2⌉. Note that the tuning
of hyperparameters is different from our theoretical derivations by some constant factors. The reason is that
the worst-case analysis is pessimistic and we ignore the constant factor in the derivation.
Settings We consider an MDP with S= 15states,A= 7actions,H= 10,d= 10, andT= 20000. In the
abruptly-changing environment, the linear MDP changes abruptly every 100 episodes to another linear MDP
with different transition function and reward function. The changes happen periodically by cycling through
different linear MDPs; we have 5 different linear MDPs in total. In the gradually-changing environment, we
consider the same set of 5 linear MDPs, {M0,M1,...,M 4}. The environment changes smoothly from Mito
M(i+1) mod 5 over every 100 episodes. To be more specific, at episode 100i(iis a non-negative integer), the
MDP model is Mimod 5parameterized by latent vectors {θimod 5
h}H
h=1and{µimod 5
h (S)}H
h=1. The latent
vectors of the MDP from episode 100ito100(i+ 1)are the linear interpolations of those of Mimod 5and
M(i+1) mod 5 , i.e., atepisode k(100i≤k≤100(i+1)),θh,k= (1−k−100i
100)θimod 5
h +k−100i
100θ(i+1) mod 5
h,∀h∈
[H],µh,k(S) = (1−k−100i
100)µimod 5
h (S) +k−100i
100µ(i+1) mod 5
h(S),∀h∈[H]. To make the environment
challenging for exploration, our construction falls into the category of combination lock (Koenig & Simmons,
1993). For each of these 5 linear MDPs, there is only one good (and different) chain that contains a huge
reward at the end, but 0 reward for the rest of the chain. Further, any sub-optimal action has small positive
rewards that would attract the agent to depart from the optimal route. Therefore, the agent must perform
“deep exploration” (Osband et al., 2019) to obtain near-optimal policy. The details of the constructions are
in Appendix E. Here we report the cumulative rewards and the running time of all algorithms averaged over
10 trials.
From Figure 1, we see LSVI-UCB-Restart with the knowledge of global variation drastically outperforms
all other methods designed for stationary environments , in both abruptly-changing and gradually-changing
environments, since it restarts the estimation of the Qfunction with knowledge of the total variations.
Ada-LSVI-UCB-Restart also outperforms the baselines because it also takes the nonstationarity into ac-
count by periodically updating the epoch size for restart. In addition, Ada-LSVI-UCB-Restart has a
huge gain compared to LSVI-UCB-Unknown , which agrees with our theoretical analysis. This suggests that
Ada-LSVI-UCB-Restart works well when the knowledge of global variation is unavailable. Our proposed
algorithms not only perform systemic exploration, but also adapt to the environment change.
Compared to OPT-WLSVI and MASTER, our proposed algorithms achieve comparable empirical performance.
More specifically, MASTERoutperforms our proposed algorithm which agrees with its dynamic regret upper
bound. However, the variance of MASTERis larger due to the random scheduling of multiple base algorithms.
Our algorithm outperforms OPT-WLSVI in the abrupt change setting, but has worse performance in the
gradual change setting, which agrees with the empirical findings in the nonstationary contextual bandit
literature (Zhao et al., 2020). The main advantage of our algorithm compared to OPT-WLSVI andMASTERis
its computational efficiency, as demonstrated by Figure 1. The reason is that our algorithm only requires
the most recent data to estimate the Q-function, while the other two require the entire history. MASTER
additionally requires maintaining multiple base instances at different scales, which further increases the
computational burden.
From Figure 1, we find that the restart strategy works better under abrupt changes than under gradual
changes, since the gap between our algorithms and the baseline algorithms designed for stationary environ-
12Published in Transactions on Machine Learning Research (10/2022)
0 2500 5000 7500 10000 12500 15000 17500 20000
Total timestep025050075010001250150017502000Cumulative rewardAbrupt change
Epsilon-Greedy
Random-Exploration
LSVI-UCB
LSVI-UCB-Restart
ADA-LSVI-UCB-Restart
LSVI-UCB-Unknown
MASTER
OPT-WLSVI
0 2500 5000 7500 10000 12500 15000 17500 20000
Total timestep0500100015002000250030003500Cumulative rewardGradual change
Epsilon-Greedy
Random-Exploration
LSVI-UCB
LSVI-UCB-Restart
ADA-LSVI-UCB-Restart
LSVI-UCB-Unknown
MASTER
OPT-WLSVI
Figure 1: Comparisons of different methods on cumulative reward under two different environments. The
results are averaged over 10 trials and the error bars show the standard deviations. The environment changes
abruptly in the left subfigure, whereas the environment changes gradually in the right subfigure.
ments is larger in this setting. The reason is that the algorithms designed to explore in stationary MDPs are
generally insensitive to abrupt change in the environment. For example, UCB-type exploration does not have
incentive to take actions other than the one with the largest upper confidence bound of Q-value, and if it has
collected sufficient number of samples, it very likely never explores the new optimal action thereby taking
the former optimal action forever. On the other hand, in gradually-changing environment, LSVI-UCB and
Epsilon-Greedy can perform well in the beginning when the drift of environment is small. However, when
the change of environment is greater, they no longer yield satisfactory performance since their Qfunction
estimate is quite off. This also explains why LSVI-UCB and Epsilon-Greedy outperform ADA-LSVI-UCB at
the beginning in the gradually-changing environment, as shown in Figure 1.
Figure 2 shows that the running times of LSVI-UCB-Restart and Ada-LSVI-UCB-Restart are roughly the
same. They are much less compared with MASTER,OPT-WLSVI ,LSVI-UCB ,Epsilon-Greedy . This is because
LSVI-UCB-Restart andAda-LSVI-UCB-Restart can automatically restart according to the variation of the
environment and thus have much smaller computational burden since it does not need to use the entire
history to compute the current policy at each time step. The running time of LSVI-UCB-Unknown is larger
than LSVI-UCB-restart since the epoch larger is larger due to the lack of the knowlege of total variation
B, but it still does not use the entire history to compute its policy. Although Random-Exploration takes
the least time, it cannot find the near-optimal policy. This result further demonstrates that our algorithms
are not only sample-efficient, but also computationally tractable.
7 Conclusion and Future Work
In this paper, we studied nonstationary RL with time-varying reward and transition functions. We focused
on the class of nonstationary linear MDPs such that linear function approximation is sufficient to realize
any value function. We first incorporated the epoch start strategy into LSVI-UCB algorithm (Jin et al.,
2020) to propose the LSVI-UCB-Restart algorithm with low dynamic regret when the total variations are
known. We then designed a parameter-free algorithm Ada-LSVI-UCB-Restart that enjoys a slightly worse
dynamic regret bound without knowing the total variations. We derived a minimax regret lower bound
for nonstationary linear MDPs to demonstrate that our proposed algorithms are near-optimal. Specifically,
when the local variations are known, LSVI-UCB-Restart is near order-optimal except for the dependency
on feature dimension d, planning horizon H, and some poly-logarithmic factors. Numerical experiments
demonstrates the effectiveness of our algorithms.
A number of future directions are of interest. An immediate step is to investigate whether the dependence
on the dimension dand planning horizon Hin our bounds can be improved, and whether the minimax regret
13Published in Transactions on Machine Learning Research (10/2022)
0200400600800100012001400Time (seconds)1281.1
213.5177.9145.2
44.87.38.21.11373.9
282.9
202.0
162.2
59.8
10.1 11.31.3
Abrupt Change Gradual ChangeComparison of running time
MASTER
OPT-WLSVI
LSVI-UCB
Epsilon-Greedy
LSVI-UCB-Unknown
ADA-LSVI-UCB-Restart
LSVI-UCB-Restart
Random-Exploration
Figure 2: Comparisons of different methods on running time for two different environments. The results are
averaged over 10 trials and the error bars show the standard deviations. See Appendix E.2 for details on
hardware.
lower bound can also be improved. It would also be interesting to investigate the setting of nonstationary
RL under general function approximation (Wang et al., 2020; Du et al., 2021; Jin et al., 2021), which is
closer to modern RL algorithms in practice. Recall that our algorithm is more computationally efficient than
other works. Another closely related and interesting direction is to study the low-switching cost (Gao et al.,
2021) or deployment efficient (Huang et al., 2021) algorithm in the nonstationary RL setting. Finally, our
algorithm is based on the Optimism in Face of Uncertainty. There is another broad category of algorithms
called Thompson Sampling (TS) (Agrawal & Jia, 2017; Russo, 2019; Agrawal et al., 2021; Xiong et al.,
2021; Ishfaq et al., 2021; Dann et al., 2021; Zhang, 2022). It would be an interesting avenue to see whether
empirically appealing TS algorithms are also suitable in nonstationary RL settings.
References
Yasin Abbasi-Yadkori, Dávid Pál, and Csaba Szepesvári. Improved algorithms for linear stochastic bandits.
InProc. 25th Annu. Conf. Neural Inf. Process. Syst. (NeurIPS) , pp. 2312–2320, December 2011.
Yasin Abbasi-Yadkori, Aldo Pacchiano, and My Phan. Regret balancing for bandit and rl model selection.
arXiv preprint arXiv:2006.05491 , 2020.
Alekh Agarwal, Haipeng Luo, Behnam Neyshabur, and Robert E Schapire. Corralling a band of bandit
algorithms. In Conference on Learning Theory , pp. 12–38. PMLR, 2017.
Alekh Agarwal, Sham Kakade, Akshay Krishnamurthy, and Wen Sun. FLAMBE: Structural complexity
and representation learning of low rank MDPs. In Proc. 34th Annu. Conf. Neural Inf. Process. Syst.
(NeurIPS) , December 2020.
Priyank Agrawal, Jinglin Chen, and Nan Jiang. Improved worst-case regret bounds for randomized least-
squares value iteration. In Proceedings of the AAAI Conference on Artificial Intelligence , volume 35, pp.
6566–6573, 2021.
Shipra Agrawal and Randy Jia. Optimistic posterior sampling for reinforcement learning: worst-case regret
bounds. In Advances in Neural Information Processing Systems , pp. 1184–1194, 2017.
14Published in Transactions on Machine Learning Research (10/2022)
Shipra Agrawal and Randy Jia. Learning in structured MDPs with convex cost functions: Improved regret
bounds for inventory management. In Proc. 20th ACM Conf. Electron. Commer. (EC’19) , pp. 743–744,
June 2019.
Ilge Akkaya, Marcin Andrychowicz, Maciek Chociej, Mateusz Litwin, Bob McGrew, Arthur Petron, Alex
Paino, Matthias Plappert, Glenn Powell, Raphael Ribas, Jonas Schneider, Nikolas Tezak, Jerry Tworek,
Peter Welinder, Lilian Weng, Qiming Yuan, Wojciech Zaremba, and Lei Zhang. Solving rubik’s cube with
a robot hand. arXiv:1910.07113 [cs.LG]., October 2019.
Peter Auer, Nicolo Cesa-Bianchi, and Paul Fischer. Finite-time analysis of the multiarmed bandit problem.
Machine Learning , 47(2-3):235–256, May 2002a.
Peter Auer, Nicolo Cesa-Bianchi, Yoav Freund, and Robert E Schapire. The nonstochastic multiarmed
bandit problem. SIAM J. Comput. , 32(1):48–77, 2002b.
Peter Auer, Thomas Jaksch, and Ronald Ortner. Near-optimal regret bounds for reinforcement learning.
Journal of Machine Learning Research , 11(51):1563–1600, 2010.
Alex Ayoub, Zeyu Jia, Csaba Szepesvari, Mengdi Wang, and Lin Yang. Model-based reinforcement learning
with value-targeted regression. In International Conference on Machine Learning , 2020.
Omar Besbes, Yonatan Gur, and Assaf Zeevi. Stochastic multi-armed-bandit problem with non-stationary
rewards. In Proc. 28th Annu. Conf. Neural Inf. Process. Syst. (NeurIPS) , pp. 199–207, December 2014.
Lilian Besson and Emilie Kaufmann. The generalized likelihood ratio test meets klucb: an improved al-
gorithm for piece-wise non-stationary bandits. Proceedings of Machine Learning Research vol XX , 1:35,
2019.
Steven J Bradtke and Andrew G Barto. Linear least-squares algorithms for temporal difference learning.
Machine Learning , 22(1-3):33–57, March 1996.
J Bretagnolle and C Huber. Estimation des densités: risque minimax. Zeitschrift für Wahrscheinlichkeits-
theorie und verwandte Gebiete , 47(2):119–137, 1979.
Sébastien Bubeck and Nicolo Cesa-Bianchi. Regret analysis of stochastic and nonstochastic multi-armed
bandit problems. Foundations and Trends in Machine Learning , 5(1):1–122, 2012.
Han Cai, Kan Ren, Weinan Zhang, Kleanthis Malialis, Jun Wang, Yong Yu, and Defeng Guo. Real-time
bidding by reinforcement learning in display advertising. In Proc. 10th ACM Int. Conf. Web Search Data
Min. (WSDM ’17) , pp. 661–670, February 2017.
Qi Cai, Zhuoran Yang, Chi Jin, and Zhaoran Wang. Provably efficient exploration in policy optimization.
InProc. 37th Int. Conf. Mach. Learn. (ICML 2020) , July 2020.
Jinglin Chen and Nan Jiang. Information-theoretic considerations in batch reinforcement learning. In
International Conference on Machine Learning , 2019.
Jinglin Chen, Aditya Modi, Akshay Krishnamurthy, Nan Jiang, and Alekh Agarwal. On the statistical
efficiency of reward-free exploration in non-linear rl. arXiv preprint arXiv:2206.10770 , 2022.
Liyu Chen and Haipeng Luo. Near-optimal goal-oriented reinforcement learning in non-stationary environ-
ments.arXiv preprint arXiv:2205.13044 , 2022.
Wang Chi Cheung, David Simchi-Levi, and Ruihao Zhu. Learning to optimize under non-stationarity. In
Proc. 22nd Int. Conf. Artif. Intell. Stat. (AISTATS 2019) , pp. 1079–1087, April 2019.
WangChiCheung, DavidSimchi-Levi, andRuihaoZhu. Non-stationaryreinforcementlearning: Theblessing
of (more) optimism. In Proc. 37th Int. Conf. Mach. Learn. (ICML 2020) , July 2020.
Corinna Cortes, Giulia DeSalvo, Vitaly Kuznetsov, Mehryar Mohri, and Scott Yang. Discrepancy-based
algorithms for non-stationary rested bandits. arXiv preprint arXiv:1710.10657 , 2017.
15Published in Transactions on Machine Learning Research (10/2022)
Thomas M Cover and Sandeep Pombra. Gaussian feedback capacity. IEEE Transactions on Information
Theory, 35(1):37–43, January 1989.
Varsha Dani, Thomas P Hayes, and Sham M Kakade. Stochastic linear optimization under bandit feedback.
InProc. 21st Annual Conf. Learning Theory (COLT 2008) , pp. 355–366, July 2008.
Christoph Dann, Mehryar Mohri, Tong Zhang, and Julian Zimmert. A provably efficient model-free posterior
sampling method for episodic reinforcement learning. Advances in Neural Information Processing Systems ,
34:12040–12051, 2021.
Yuhao Ding and Javad Lavaei. Provably efficient primal-dual reinforcement learning for cmdps with non-
stationary objectives and constraints. arXiv preprint arXiv:2201.11965 , 2022.
Kefan Dong and Tengyu Ma. Asymptotic instance-optimal algorithms for interactive decision making. arXiv
preprint arXiv:2206.02326 , 2022.
Kefan Dong, Jian Peng, Yining Wang, and Yuan Zhou.√n-regret for learning in Markov decision processes
with function approximation and low Bellman rank. In Proc. Conf. Learning Theory (COLT) , pp. 1554–
1557, July 2020.
Simon Du, Sham Kakade, Jason Lee, Shachar Lovett, Gaurav Mahajan, Wen Sun, and Ruosong Wang.
Bilinear classes: A structural framework for provable generalization in RL. In International Conference
on Machine Learning , 2021.
Damien Ernst, Pierre Geurts, and Louis Wehenkel. Tree-based batch mode reinforcement learning. Journal
of Machine Learning Research , 6:503–556, 2005.
Yingjie Fei, Zhuoran Yang, Zhaoran Wang, and Qiaomin Xie. Dynamic regret of policy optimization in
non-stationary environments. In Proc. 33rd Annu. Conf. Neural Inf. Process. Syst. (NeurIPS) , December
2020.
DylanJFoster, ShamMKakade, JianQian, andAlexanderRakhlin. Thestatisticalcomplexityofinteractive
decision making. arXiv preprint arXiv:2112.13487 , 2021a.
Dylan J Foster, Alexander Rakhlin, David Simchi-Levi, and Yunzong Xu. Instance-dependent complexity
of contextual bandits and reinforcement learning: A disagreement-based perspective. In Conference on
Learning Theory , 2021b.
Minbo Gao, Tianle Xie, Simon S Du, and Lin F Yang. A provably efficient algorithm for linear markov
decision process with low switching cost. arXiv preprint arXiv:2101.00494 , 2021.
Aurélien Garivier and Eric Moulines. On upper-confidence bound policies for switching bandit problems. In
Proc. Int. Conf. Algorithmic Learning Theory (ALT 2011) , pp. 174–188, October 2011.
Pihe Hu, Yu Chen, and Longbo Huang. Nearly minimax optimal reinforcement learning with linear function
approximation. In International Conference on Machine Learning , pp. 8971–9019. PMLR, 2022.
Jiawei Huang, Jinglin Chen, Li Zhao, Tao Qin, Nan Jiang, and Tie-Yan Liu. Towards deployment-efficient
reinforcement learning: Lower bound and optimality. In International Conference on Learning Represen-
tations, 2021.
Haque Ishfaq, Qiwen Cui, Viet Nguyen, Alex Ayoub, Zhuoran Yang, Zhaoran Wang, Doina Precup, and Lin
Yang. Randomized exploration in reinforcement learning with general value function approximation. In
International Conference on Machine Learning , pp. 4607–4616. PMLR, 2021.
Nan Jiang, Akshay Krishnamurthy, Alekh Agarwal, John Langford, and Robert E Schapire. Contextual
decision processes with low Bellman rank are PAC-learnable. In Proc. 34th Int. Conf. Mach. Learn.
(ICML 2017) , pp. 1704–1713, August 2017.
16Published in Transactions on Machine Learning Research (10/2022)
Chi Jin, Zhuoran Yang, Zhaoran Wang, and Michael I Jordan. Provably efficient reinforcement learning with
linear function approximation. In Proc. Conf. Learning Theory (COLT) , pp. 2137–2143, July 2020.
Chi Jin, Qinghua Liu, and Sobhan Miryoosefi. Bellman eluder dimension: New rich classes of rl problems,
and sample-efficient algorithms. In Advances in Neural Information Processing Systems , 2021.
Sven Koenig and Reid G Simmons. Complexity analysis of real-time reinforcement learning. In Proc. 11th
Nat. Conf. Artificial Intell. (AAAI) , pp. 99–107, 1993.
Tor Lattimore and Csaba Szepesvári. Bandit Algorithms . Cambridge University Press, Cambridge, UK,
2020.
Jonathan Lee, Aldo Pacchiano, Vidya Muthukumar, Weihao Kong, and Emma Brunskill. Online model
selection for reinforcement learning with function approximation. In International Conference on Artificial
Intelligence and Statistics , pp. 3340–3348. PMLR, 2021.
Thodoris Lykouris, Max Simchowitz, Aleksandrs Slivkins, and Wen Sun. Corruption robust exploration in
episodic reinforcement learning. In Conference on Learning Theory . PMLR, 2021.
Weichao Mao, Kaiqing Zhang, Ruihao Zhu, David Simchi-Levi, and Tamer Başar. Near-optimal regret
bounds for model-free rl in non-stationary episodic mdps. In Proc. 38th Int. Conf. Mach. Learn. (ICML
2021), 2021.
Francisco S Melo and M Isabel Ribeiro. Q-learning with linear function approximation. In Proc. Int. Conf.
Computational Learning Theory (COLT 2007) , pp. 308–322, June 2007.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex
Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, Stig Petersen, Charles Beattie, Amir
Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wierstra, Shane Legg, and Demis
Hassabis. Human-level control through deep reinforcement learning. Nature, 518(7540):529–533, February
2015.
Aditya Modi, Nan Jiang, Ambuj Tewari, and Satinder Singh. Sample complexity of reinforcement learning
using linearly combined model ensembles. In Proc. 23rd Int. Conf. Artif. Intell. Stat. (AISTATS 2020) ,
pp. 2010–2020, August 2020.
Aditya Modi, Jinglin Chen, Akshay Krishnamurthy, Nan Jiang, and Alekh Agarwal. Model-free representa-
tion learning and exploration in low-rank mdps. arXiv preprint arXiv:2102.07035 , 2021.
Mehryar Mohri and Scott Yang. Online learning with transductive regret. In Proc. 31st Annu. Conf. Neural
Inf. Process. Syst. (NeurIPS) , pp. 5220–5230, 2017a.
Mehryar Mohri and Scott Yang. Online learning with automata-based expert sequences. arXiv preprint
arXiv:1705.00132 , 2017b.
Rémi Munos and Csaba Szepesvári. Finite-time bounds for fitted value iteration. Journal of Machine
Learning Research , 9(5), 2008.
Gergely Neu and Julia Olkhovskaya. Online learning in MDPs with linear function approximation and bandit
feedback. In Proc. 34th Annu. Conf. Neural Inf. Process. Syst. (NeurIPS) , December 2021.
Ronald Ortner, Pratik Gajane, and Peter Auer. Variational regret bounds for reinforcement learning. In
Proc. 36th Annu. Conf. Uncertainty in Artificial Intelligence (UAI’20) , pp. 81–90, August 2020.
Ian Osband, Benjamin Van Roy, and Zheng Wen. Generalization and exploration via randomized value
functions. In Proc. 33rd Int. Conf. Mach. Learn. (ICML 2016) , pp. 2377–2386, June 2016.
Ian Osband, Benjamin Van Roy, Daniel J Russo, and Zheng Wen. Deep exploration via randomized value
functions. Journal of Machine Learning Research , 20(124):1–62, 2019.
17Published in Transactions on Machine Learning Research (10/2022)
Aldo Pacchiano, My Phan, Yasin Abbasi Yadkori, Anup Rao, Julian Zimmert, Tor Lattimore, and Csaba
Szepesvari. Model selection in contextual stochastic bandit problems. Advances in Neural Information
Processing Systems , 33:10328–10337, 2020.
Martin L Puterman. Markov Decision Processes: Discrete Stochastic Dynamic Programming . John Wiley
& Sons, Hoboken, NJ, USA, 2014.
Yoan Russac, Claire Vernade, and Olivier Cappé. Weighted linear bandits for non-stationary environments.
InProc. 33rd Annu. Conf. Neural Inf. Process. Syst. (NeurIPS) , 2019.
Daniel Russo. Worst-case regret bounds for exploration via randomized value functions. In Advances in
Neural Information Processing Systems , pp. 14410–14420, 2019.
Shai Shalev-Shwartz, Shaked Shammah, and Amnon Shashua. Safe, multi-agent, reinforcement learning for
autonomous driving. arXiv:1610.03295 [cs.AI]., October 2016.
David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez, Thomas
Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, Yutian Chen, Timothy Lillicrap, Fan Hui, Laurent
Sifre, George van den Driessche, Thore Graepel, and Demis Hassabis. Mastering the game of Go without
human knowledge. Nature, 550(7676):354–359, October 2017.
David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez, Marc
Lanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel, Timothy Lillicrap, Karen Simonyan, and
Demis Hassabis. A general reinforcement learning algorithm that masters chess, shogi, and Go through
self-play. Science, 362(6419):1140–1144, December 2018. doi: 10.1126/science.aar6404.
Richard S Sutton and Andrew G Barto. Reinforcement Learning: An Introduction . MIT Press, Cambridge,
MA, USA, 2018.
Ahmed Touati and Pascal Vincent. Efficient learning in non-stationary linear markov decision processes.
arXiv preprint arXiv:2010.12870 , 2020.
Lingda Wang, Huozhi Zhou, Bingcong Li, Lav R Varshney, and Zhizhen Zhao. Nearly optimal algorithms
for piecewise-stationary cascading bandits. arXiv:1909.05886v1 [cs.LG]., September 2019.
Ruosong Wang, Ruslan Salakhutdinov, and Lin F Yang. Provably efficient reinforcement learning with
general value function approximation. In Proc. 34th Annu. Conf. Neural Inf. Process. Syst. (NeurIPS) ,
May 2020.
Christopher J. C. H. Watkins and Peter Dayan. Q-learning. Machine Learning , 8(3-4):279–292, May 1992.
Christopher John Cornish Hellaby Watkins. Learning from delayed rewards . PhD thesis, University of
Combrdge, 1989.
Chen-Yu Wei and Haipeng Luo. Non-stationary reinforcement learning without prior knowledge: An optimal
black-box approach. In Conference on Learning Theory , pp. 4300–4354. PMLR, 2021.
Chen-Yu Wei, Mehdi Jafarnia-Jahromi, Haipeng Luo, and Rahul Jain. Learning infinite-horizon average-
reward MDPs with linear function approximation. In Proc. 24th Int. Conf. Artif. Intell. Stat. (AISTATS
2021), April 2021.
Zhihan Xiong, Ruoqi Shen, and Simon S Du. Randomized exploration is near-optimal for tabular mdp.
arXiv preprint arXiv:2102.09703 , 2021.
Lin F Yang and Mengdi Wang. Sample-optimal parametric Q-learning using linearly additive features. In
Proc. 36th Int. Conf. Mach. Learn. (ICML 2019) , June 2019.
Jia Yuan Yu, Shie Mannor, and Nahum Shimkin. Markov decision processes with arbitrary reward processes.
Math. Oper. Res. , 34(3):737–757, August 2009.
18Published in Transactions on Machine Learning Research (10/2022)
Tong Zhang. Feel-good thompson sampling for contextual bandits and reinforcement learning. SIAM Journal
on Mathematics of Data Science , 4(2):834–857, 2022.
Peng Zhao, Lijun Zhang, Yuan Jiang, and Zhi-Hua Zhou. A simple approach for non-stationary linear
bandits. In Proc. 23rd Int. Conf. Artif. Intell. Stat. (AISTATS 2020) , pp. 746–755, August 2020.
Yufan Zhao, Michael R Kosorok, and Donglin Zeng. Reinforcement learning design for cancer clinical trials.
Statistics in Medicine , 28(26):3294–3315, November 2009.
Han Zhong, Zhuoran Yang, and Zhaoran Wang Csaba Szepesvári. Optimistic policy optimization is provably
efficient in non-stationary mdps. arXiv preprint arXiv:2110.08984 , 2021.
Dongruo Zhou, Jiafan He, and Quanquan Gu. Provably efficient reinforcement learning for discounted mdps
with feature mapping. In International Conference on Machine Learning , pp. 12793–12802. PMLR, 2021.
Huozhi Zhou, Lingda Wang, Lav R Varshney, and Ee-Peng Lim. A near-optimal change-detection based
algorithm for piecewise-stationary combinatorial semi-bandits. In Proc. 34th AAAI Conf. Artif. Intell. ,
pp. 6933–6940, February 2020.
19Published in Transactions on Machine Learning Research (10/2022)
A Proofs in Section 3
To prove the minimax dynamic regret lower bound for the homogeneous setting, we first prove the minimax
regret bound for linear MDP with homogeneous transition function.
Theorem 8. For any algorithm, if d≥4andT≥64(d−3)2H, then there exists at least one stationary
linear MDP instance that incurs regret at least Ω(d√
HT).
The key step of this proof is to construct the hard-to-learn MDP instances. Inspired by the lower bound
construction for stochastic contextual bandits (Dani et al., 2008; Lattimore & Szepesvári, 2020), we construct
an ensemble of hard-to-learn 3-state linear MDPs, which is illustrated in Figure 3. This construction can be
viewed as a generalization of the lower bound construction for linear contextual bandits (Dani et al., 2008;
Lattimore & Szepesvári, 2020). The intuition is that the reward distributions under optimal and suboptimal
policies for these instances are close: thus it is statistically hard for any learner to identify the optimal policy.
s0s1
s2δ+/angbracketlefta1,v/angbracketright
δ+/angbracketleftai,v/angbracketright
1−δ−/angbracketlefta1,v/angbracketright
1−δ−/angbracketleftai,v/angbracketright1
1. . .
. . .
1
Figure 3: Graphical illustration of the hard-to-learn linear MDP instances with deterministic reward.
Each linear MDP instance in this ensemble has three states s0,s1,s2(s1ands2are absorbing states), and
it is characterized by a unique (d−3)-dimensional vector {±/radicalbig
(d−3)H/√
T}d−3. Specifically, the vector v
defines the transition function of the corresponding MDP, as illustrated in Figure 3. Each action aof this
MDP instance is encoded by a (d−3)dimensional vector a∈/braceleftbig
±1/√
d−3/bracerightbigd−3. The reward functions for
the three states are fixed regardless of the actions, specifically, r(s0,a) =r(s2,a) = 0,r(s1,a) = 1,∀a∈A.
For each episode, the agent starts at s0, and ends at step H. The transition functions of the linear MDP
parametrized by vare defined as follows,
P(s1|s0,a) =δ+⟨a,v⟩,P(s2|s0,a) = 1−δ−⟨a,v⟩,P(s1|s1,a) = 1,P(s2|s2,a) = 1,
whereδ=1
4. NoticethattheoptimalpolicyfortheMDPinstanceparametrizedby vistakingtheactionthat
maximizes the probability to reach s1, which is equivalent to taking the action such that its corresponding
vectorasatisfies sgn (ai) =sgn(vi),∀i∈[d−3]. Furthermore, it can be verified that the above MDP instance
is indeed a linear MDP, by setting:
ϕ(s0,a) = (0,1,δ,a),ϕ(s1,a) = (1,0,0,⃗0),ϕ(s2,a) = (0,1,0,⃗0)
µ(s0) = (0,0,0,⃗0),µ(s1) = (1,0,1,v),µ(s2) = (0,1,−1,−v),θ= (1,0,0,0).
Remark 7. Note that the above parameters violate the normalization assumption in Definition 1, but it is
straightforward to normalize them. We ignore the additional rescaling to clarify the presentation.
After constructing the ensemble of hard instances, we can derive the minimax regret lower bound for sta-
tionary linear MDP for the homogeneous setting.
20Published in Transactions on Machine Learning Research (10/2022)
Proof of Theorem 8. LetPπ
t,v(assumetis a multiple of H) be the probability distribution of
{a1
1,/summationtextH
h=1r1
h,a2
1,/summationtextH
h=1r2
h,...,at/H
1,/summationtextH
h=1rt/H
h}of running algorithm πon linear MDP parametrized by v.
First note that by the Markov property of π, we can decompose DKL(Pπ
t,v||Pπ
t,v′)as
t/H/summationdisplay
l=1EDKL/parenleftigg
P/parenleftiggH/summationdisplay
h=1rl
h|al
1,v/parenrightigg
||P/parenleftiggH/summationdisplay
h=1rl
h|al
1,v′/parenrightigg/parenrightigg
.
Recall that due to our hard cases construction, the first step in every episode determines the distribution of
the total reward of that episode, thus
DKL/parenleftigg
P/parenleftiggH/summationdisplay
h=1rl
h|al
1,v/parenrightigg
||P/parenleftiggH/summationdisplay
h=1rl
h|al
1,v′/parenrightigg/parenrightigg
=/parenleftbig
δ+⟨al
1,v⟩/parenrightbig
logδ+⟨al
1,v⟩
δ+⟨al
1,v′⟩+/parenleftbig
1−δ−⟨al
1,v⟩/parenrightbig
log1−δ−⟨al
1,v⟩
1−δ−⟨al
1,v′⟩(6)
We bound the KL divergence in (6) applying the following lemma.
Lemma 5. (Auer et al., 2010) If 0≤δ′≤1/2andϵ′≤1−2δ′, then
δ′logδ′
δ′+ϵ′+ (1−δ′) log1−δ′
1−δ′−ϵ′≤2(ϵ′)2
δ′.
To apply Lemma 5, we let ⟨al
1,v⟩+δ=δ′,⟨v−v′,al
1⟩=ϵ′. Thus we must ensure the following inequalities
hold for anya,v,v′:
⟨a,v⟩+δ≤(d−3)√
H√
T+δ≤1/2
⟨v−v′,a⟩≤2(d−3)√
H√
T≤1−2/parenleftigg
(d−3)√
H√
T+δ/parenrightigg
≤1−2δ′.
To guarantee the above inequalities hold, we can set δ=1
4and let(d−3)√
H√
T≤1
8. Now we get back to
bounding Eq. 6. Let ∆ =(d−3)√
H√
Tand suppose vandv′only differ in one coordinate. Then
DKL/parenleftigg
P/parenleftiggH/summationdisplay
h=1rl
h|al
1,v/parenrightigg
||P/parenleftiggH/summationdisplay
h=1rl
h|al
1,v′/parenrightigg/parenrightigg
≤8∆21
(d−3)2
δ−∆≤16∆2
δ(d−3)2≤64H
T.
Furthermore, let Ei,bbe the the following event:
|{l∈[K] :sgn(al
1)i̸=sgn(b)}|≥1
2K.
Letqi,v=P[Ei,vi|v], the probability that the agent is taking sub-optimal action for the i-th coordinate for
at least half of the episodes given that the underlying linear MDP is parameteriezed by v. We can then
lower bound the regret of any algorithm when running on linear MDP parameterized by vas:
Regv(T)≥d−3/summationdisplay
i=1qi,vK(H−1)/radicalbigg
H
T
≥/parenleftig√
TH−√
K/parenrightigd−3/summationdisplay
i=1qi,v, (7)
since whenever the learner takes a sub-optimal action that differs from the optimal action by one coordinate,
it will incur 2/radicalig
H
T(H−1)expected regret. Next we take the average over 2d−3linear MDP instances to
21Published in Transactions on Machine Learning Research (10/2022)
show that on average it incurs Ω(d√
HT)regret, thus there exists at least one instance incurring Ω(d√
HT)
regret. Before that, we need to bound the summation of bad events under two close linear MDP instances.
Denote the vector which is only different from vini-th coordinate as v⊕i. Then we have
qi,v+qi,v⊕i=P[Ei,vi|v] +P[Ei,v⊕i
i|v⊕i]
=P[Ei,vi|v] +P[¯Ei,vi|v⊕i]
≥1
2exp(−DKL(PT,v||PT,v⊕i))
≥1
2exp(−64), (8)
where the inequality is due to Bretagnolle-Huber inequality (Bretagnolle & Huber, 1979). Now we are ready
to lower bound the average regret over all linear MDP instances.
1
2d−3/summationdisplay
vRegv(T)≥√
HT−√
K
2d−3/summationdisplay
vd−3/summationdisplay
i=1qi,v
≥√
HT−√
K
2d−3d−3/summationdisplay
i=1/summationdisplay
vqi,v+qi,v⊕i
2
≥√
HT−√
K
2d−32d−31
4e−64(d−3)
≳Ω(d√
HT)
where the first inequality is due to (7), and the third inequality is due to (8).
Based on Theorem 8, we can derive the minimax dynamic regret for nonstationary linear MDP.
Proof of Theorem 1. Weconstructthehardinstanceasfollows: Wefirstdividethewholetimehorizon Tinto
⌈K
N⌉intervals, where each interval has ⌈K
N⌉episodes (the last interval might be shorter if Kis not a multiple
ofN). For each interval, the linear MDP is fixed and parameterized by a v∈{±√
(d−3)√
N}d−3which we define
when constructing the hard instances in Theorem 8. Note that different intervals are completely decoupled,
thus information is not passed across intervals. For each interval, it incurs regret at least Ω(d√
H2N)by
Theorem 8. Thus the total regret is at least
Dyn-Reg (T)≳(⌈K
N⌉−1)Ω(d√
H2N)
≳Ω(d√
H2K2N−1/2). (9)
Intuitively, we would like Nto be as small as possible to obtain a tight lower bound. However, due to our
construction, the total variation for two consecutive blocks is upper-bounded by
/radicaltp/radicalvertex/radicalvertex/radicalbtd−3/summationdisplay
i=14(d−3)
N=2(d−3)√
N.
Note that the total time variation for the whole time horizon is Band by definition B≥2(d−3)√
N(⌊K
N⌋−1),
which implies N≳Ω(B−2/3d2/3K2/3). Substituting the lower bound of Ninto (9), we have
Dyn-Reg (T)≳Ω(B1/3d2/3K2/3H)≳Ω(B1/3d2/3H1/3T2/3)
which concludes the proof.
Finally, we provide the formal proof for Theorem 2.
22Published in Transactions on Machine Learning Research (10/2022)
Proof of Theorem 2. We construct the hard instance as follows. We first divide the whole time horizon T
into⌈K
N⌉intervals, where each interval has ⌈K
N⌉episodes (the last interval might be shorter if Kis not a
multiple of N). For each interval, the linear MDP is fixed and parameterized by a v∈{±1
4√
2/radicalig
1
NH}d−1,
defined in Lemma E.1 in Hu et al. (2022). Note that different intervals are completely decoupled, thus
information is not passed across intervals. For each interval, it incurs regret at least Ω(d√
H3N)by Lemma
E.1 in Hu et al. (2022). Thus the total regret is at least
Dyn-Reg (T)≳(⌈K
N⌉−1)Ω(d√
H3N)
≳Ω(d√
H3K2N−1/2). (10)
Intuitively, we would like Nto be as small as possible to obtain a tight lower bound. However, due to our
construction, the total variation for two consecutive blocks is upper-bounded by
/radicaltp/radicalvertex/radicalvertex/radicalbtd−1/summationdisplay
i=11
32NH=√
d−1
4√
2√
NH.
Note that the total time variation for the whole time horizon is Band by definition B≳
Ω(d1/2KN−3/2H−1/2), which implies N≳Ω(B−2/3d1/3K2/3H−1/3). Substituting the lower bound of N
into Eq. (10), we have
Dyn-Reg (T)≳Ω(B1/3d5/6HT2/3)
which concludes the proof.
B Proofs in Section 4
Here we provide the proofs in Section 4. We first want to comment that our algorithm builds on LSVI-UCB .
LSVIcan be seen as a specialization of the regression-based Fitted Q-Iteration algorithm (Ernst et al., 2005;
Munos & Szepesvári, 2008; Chen & Jiang, 2019) to the linear case, and LSVI-UCB (Jin et al., 2020) further
adds the bonus term on top of that to handle exploration.
Now, we introduce some notations we use throughout the proof. We let wk
h,Λk
handQk
has the parameters
and action-value function estimate in episode kfor steph. Denote value function estimate as Vk
h(s) =
maxaQk
h(s,a). For any policy π, we letwπ
h,k,Qπ
h,kbe the ground-truth parameter and action-value function
for that policy in episode kfor steph. We also abbreviate ϕ(sl
h,al
h)asϕl
hfor notational simplicity.
We first work on the case when local variation is known and then consider the case when local variation is
unknown.
B.1 Case 1: Known Local Variation
Before we prove the regret upper bound within one epoch (Theorem 5), we need some additional lemmas.
The first lemma is used to control the fluctuations in least-squares value iteration, when performed on the
value function estimate Vk
h(·)maintained in Algorithm 1.
Proof of Lemma 1. The lemma is slightly different than Jin et al. (2020, Lemma B.3), since they assume
Phis fixed for different episodes. It can be verified that the proof for stationary case still holds in our case
without any modifications since the results in Jin et al. (2020) holds for least-squares value iteration for
arbitrary function in the function class of our interest, i.e., {V|V={ϕ(·,·),w},w∈Rd}. Let us first restate
Lemma B.3 in Jin et al. (2020) to compare against our Lemma 1.
23Published in Transactions on Machine Learning Research (10/2022)
Lemma 6. (Lemma B.3 in Jin et al. (2020)) There exists an absolute constant Cthat is independent of cβ
such that for any fixed p∈[0,1], if we let the event Ebe the following,
∀(k,h)∈[K]×[H],/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoublek−1/summationdisplay
l=1ϕl
h[Vk
h+1(sl
h+1)−PhVk
h+1(sl
h,al
h)]/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble
(Λk
h)−1≤CdH/radicalig
log[2(cβ+ 1)dW/p ],
then event Ehappens with probability at least 1−p/2.
To prove Lemma 1, we need the following technical lemmas.
Lemma 7. (Lemma D.4 in Jin et al. (2020)) Let {xτ}∞
τ=1be a stochastic process on state space Swith
corresponding filtration {Fτ}∞
τ=1. Let{ϕτ}∞
τ=1be aRd-valued stochastic process where ϕτ∈Fτ−1, and
∥ϕτ∥≤1. Let Λk=λI+/summationtextk
τ=1ϕτϕ⊤
τ. Then for any δ>0, with probability at least 1−δ, for allk>0, and
V∈Vso that supx|V(x)|≤H, we have
/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoublek/summationdisplay
τ=1ϕτ{V(xτ)−E[V(xτ)|Fτ−1]}/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
Λ−1
k≤4H2[d
2log(k+λ
λ) + logNϵ
δ] +8k2ϵ2
λ,
whereNϵis the covering number for V.
Lemma 8. (Lemma D.6 in Jin et al. (2020)) For function class with the following form,
V(·) = min{max
aw⊤ϕ(·,a) +β/radicalig
ϕ(·,a)⊤Λ−1ϕ(·,a),H}
satisfying the constraints given by Jin et al. (2020), the log covering number logNϵfor this function class is
upper bounded by dlog(1 + 8H√
d/ϵ) +d2log[1 + 8d1/2β2/(λϵ2)].
By applying Lemma 7 and Lemma 8, and setting λ= 1,ϵ=dH/k,β =CdH log(2dT/p ), we prove Lemma 1.
We then proceed to derive the error bound for the action-value function estimate maintained in the algorithm
for any policy.
Proof of Lemma 2. Note thatQπ
h,k(s,a) =⟨ϕ(s,a),wπ
h,k⟩. First we can decompose wk
h−wπ
h,kas
wk
h−wπ
h,k= (Λk
h)−1k−1/summationdisplay
l=τϕl
h[rl
h(sl
h,al
h) +Vk
h+1(sl
h+1)]−wπ
h,k
= (Λk
h)−1{−wπ
h,k+k−1/summationdisplay
l=τϕl
h[Vk
h+1(sl
h+1)−Pk
hVπ
h+1,k(sl
h,al
h)] +k−1/summationdisplay
l=τϕl
h[rl
h(sl
h,al
h)−rk
h(sl
h,al
h)]}
=−(Λk
h)−1wπ
h,k/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
1+ (Λk
h)−1k−1/summationdisplay
l=τϕl
h[Vk
h+1(sl
h+1)−Pl
hVk
h+1(sl
h,al
h)]
/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
2
+ (Λk
h)−1k−1/summationdisplay
l=τϕl
h[(Pl
h−Pk
h)Vk
h+1(sl
h,al
h)]
/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
3+ (Λk
h)−1k−1/summationdisplay
l=τϕl
hPk
h(Vk
h+1−Vπ
h+1,k)(sl
h,al
h)
/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
4
+ (Λk
h)−1k−1/summationdisplay
l=τϕl
h[rl
h(sl
h,al
h)−rk
h(sl
h,al
h)]
/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
5.
24Published in Transactions on Machine Learning Research (10/2022)
We bound the individual terms on right side one by one. For the first term,
|⟨ϕ(s,a),1⟩|=|⟨ϕ(s,a),(Λk
h)−1wπ
h,k⟩|
≤/vextenddouble/vextenddoublewπ
h,k/vextenddouble/vextenddouble∥ϕ(s,a)∥(Λk
h)−1
≤2H√
d∥ϕ(s,a)∥(Λk
h)−1,
where the last inequality is due to Lemma 12. For the second term, we know that under event Edefined in
Lemma 1,
|⟨ϕ(s,a),2⟩|≤CdH/radicalig
log[2(cβ+ 1)dW/p ]∥ϕ(s,a)∥(Λk
h)−1.
For the third term,
⟨ϕ(s,a),3⟩=⟨ϕ(s,a),(Λk
h)−1k−1/summationdisplay
l=τϕl
h[(Pl
h−Pk
h)Vk
h+1(sl
h,al
h)]⟩
≤k−1/summationdisplay
l=τ|ϕ(s,a)⊤(Λk
h)−1ϕl
h|[(Pl
h−Pk
h)Vk
h+1(sl
h,al
h)]
≤Bµ,EHk−1/summationdisplay
l=τ|ϕ(s,a)⊤(Λk
h)−1ϕl
h|
≤Bµ,EH/radicaltp/radicalvertex/radicalvertex/radicalbtk−1/summationdisplay
l=τ∥ϕ(s,a)∥2
(Λk
h)−1/radicaltp/radicalvertex/radicalvertex/radicalbtk−1/summationdisplay
l=τ(ϕl
h)⊤(Λk
h)−1ϕl
h
≤/radicalbig
d(k−τ)Bµ,EH∥ϕ(s,a)∥(Λk
h)−1,
where the first three inequalities are due to Cauchy-Schwarz inequality and boundedness of Pl
h−Pk
hand
Vk
h+1, and the last inequality is due to Lemma 13.
For the fourth term,
⟨ϕ(s,a),4⟩=⟨ϕ(s,a),(Λk
h)−1k−1/summationdisplay
l=τϕl
hPk
h(Vk
h+1−Vπ
h+1,k)(sl
h,al
h)⟩
=⟨ϕ(s,a),(Λk
h)−1k−1/summationdisplay
l=τϕl
h(ϕl
h)⊤/integraldisplay
(Vk
h+1(s′)−Vπ
h+1,k(s′))dµh,k(s′)⟩
=⟨ϕ(s,a),/integraldisplay
(Vk
h+1−Vπ
h+1,k)(s′)dµh,k(s′)⟩
/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
6⟩−⟨ϕ(s,a),(Λk
h)−1/integraldisplay
(Vk
h+1−Vπ
h+1,k)(s′)dµh,k(s′)⟩
/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
7,
where6= [Pk
h(Vk
h+1−Vπ
h+1,k)](s,a)and7≤2H√
d∥ϕ(s,a)∥(Λk
h)−1due to Cauchy-Schwarz inequality.
For the fifth term,
⟨ϕ(s,a),5⟩=⟨ϕ(s,a),(Λk
h)−1k−1/summationdisplay
l=τϕl
h[rl
h(sl
h,al
h)−rk
h(sl
h,al
h)]⟩
≤k−1/summationdisplay
l=τ|ϕ(s,a)⊤(Λk
h)−1ϕl
h||rl
h(sl
h,al
h)−rk
h(sl
h,al
h)|
≤/radicalbig
d(k−τ)Bθ,E∥ϕ(s,a)∥(Λk
h)−1,
25Published in Transactions on Machine Learning Research (10/2022)
where the inequalities are derived similarly as bounding the third term. After combining all the upper
bounds for these individual terms, we have
|⟨ϕ(s,a),wk
h⟩−Qπ
h,k(s,a)−Pk
h/parenleftbig
Vk
h+1−Vπ
h+1,k/parenrightbig
(s,a)|
≤4H√
d∥ϕ(s,a)∥(Λk
h)−1+CdH/radicalig
log[2(cβ+ 1)dW/p ]∥ϕ(s,a)∥(Λk
h)−1
+Bθ,E/radicalbig
d(k−τ)∥ϕ(s,a)∥(Λk
h)−1+Bµ,EH/radicalbig
d(k−τ)∥ϕ(s,a)∥(Λk
h)−1
≤C0dH/radicalbig
log[2dW/p ]∥ϕ(s,a)∥(Λk
h)−1+Bθ,E/radicalbig
d(k−τ)∥ϕ(s,a)∥(Λk
h)−1
+Bµ,EH/radicalbig
d(k−τ)∥ϕ(s,a)∥(Λk
h)−1.
The second inequality holds if we choose a sufficiently large absolute constant C0.
Lemma2impliesthattheaction-valuefunctionestimatewemaintainedinAlgorithm1isalwaysanoptimistic
upper bound of the optimal action-value function with high confidence, if we know the local variation.
Proof of Lemma 3. We prove this by induction. First prove the base case when h=H. According to
Lemma 2, we have
|⟨ϕ(s,a),wk
H⟩−Q∗
H,k(s,a)|≤βk∥ϕ(s,a)∥(Λk
H)−1,
which implies
Qk
H(s,a) = min{⟨wk
H,ϕ(s,a)⟩+βk∥ϕ(s,a)∥(Λk
H)−1,H}≥Q∗
H,k(s,a).
Now suppose the statement holds true at step h+ 1, then for step h, due to Lemma 2, we have
|⟨ϕ(s,a),wk
h⟩−Qπ
h,k(s,a)−Pk
h(Vk
h+1−V∗
h+1,k)(s,a)|≤βk∥ϕ(s,a)∥(Λk
h)−1.
By the induction hypothesis, we have Pk
h(Vk
h+1−V∗
h+1,k)(s,a)≥0, thus
Qk
h(s,a) = min{⟨wk
h,ϕ(s,a)⟩+βk∥ϕ(s,a)∥(Λk
H)−1,H}≥Q∗
h,k(s,a).
Next we derive the bound for the gap between the value function estimate and the ground-truth value
function for the executing policy πk,δk
h=Vk
h(sk
h)−Vπk
h,k(sk
h), in a recursive manner.
Lemma 9. Letδk
h=Vk
h(sk
h)−Vπk
h,k(sk
h),ζk
h+1=E[δk
h+1|sk
h,ak
h]−δk
h+1. Under event Edefined in Lemma 1,
we have for all (k,h)∈E× [H],
δk
h≤δk
h+1+ζk
h+1+ 2βk/vextenddouble/vextenddoubleϕk
h/vextenddouble/vextenddouble
(Λk
h)−1.
Proof.By Lemma 2, for any (s,a,h,k )∈S×A× [H]×E,
Qk
h(s,a)−Qπk
h(s,a)≤Pk
h(Vk
h+1−Vπk
h+1,k)(s,a) + 2βk∥ϕ(s,a)∥(Λk
H)−1.
Note thatQk
h(sk
h,ak
h) = maxaQk
h(sk
h,a) =Vk
h(sk
h)according to Algorithm 1, and Qπk
h,k(sk
h,ak
h) =Vπk
h,k(sk
h)by
the definition. Thus,
δk
h≤δk
h+1+ζk
h+1+ 2βk/vextenddouble/vextenddoubleϕk
h/vextenddouble/vextenddouble
(Λk
h)−1.
Now we are ready to derive the regret bound within one epoch.
26Published in Transactions on Machine Learning Research (10/2022)
Proof of Theorem 3. We denote the dynamic regret within that epoch as Dyn-Reg (E). We define δk
h=
Vk
h(sk
h)−Vπk
h,k(sk
h)andζk
h+1=E[δk
h+1|sk
h,ak
h]−δk
h+1as in Lemma 11. We derive the dynamic regret within
a epochE(the length of this epoch is Wwhich is equivalent toW
Hepisodes) conditioned on the event E
defined in Lemma 1 which happens with probability at least 1−p/2.
Dyn-Reg (E) =/summationdisplay
k∈E/bracketleftig
V∗
1,k(sk
1)−Vπk
1,k/bracketrightig
≤/summationdisplay
k∈E/bracketleftig
Vk
1(sk
1)−Vπk
1,k/bracketrightig
≤/summationdisplay
k∈Eδk
1
≤/summationdisplay
k∈EH/summationdisplay
h=1ζk
h+ 2/summationdisplay
k∈KβkH/summationdisplay
h=1/vextenddouble/vextenddoubleϕk
h/vextenddouble/vextenddouble
(Λk
h)−1, (11)
where the first inequality is due to Lemma 3, the third inequality is due to Lemma 9. For the first term in
the right side, since Vk
his independent of the new observation sk
h,{ζk
h}is a martingale difference sequence.
Applying the Azuma-Hoeffding inequlity, we have for any t>0,
P/parenleftigg/summationdisplay
k∈EH/summationdisplay
h=1ζk
h≥t/parenrightigg
≥exp(−t2/(2WH2)).
Hence with probability at least 1−p/2, we have
/summationdisplay
k∈EH/summationdisplay
h=1ζk
h≤2H/radicalbig
Wlog(2dW/p ). (12)
For the second term, we bound via Cauchy-Schwarz inequality:
2/summationdisplay
k∈EβkH/summationdisplay
h=1/vextenddouble/vextenddoubleϕk
h/vextenddouble/vextenddouble
(Λk
h)−1= 2C0dH/radicalbig
log 2(dW/p )/summationdisplay
k∈EH/summationdisplay
h=1/vextenddouble/vextenddoubleϕk
h/vextenddouble/vextenddouble
(Λk
h)−1+ 2/summationdisplay
k∈EBθ,E/radicalbig
d(k−τ)H/summationdisplay
h=1/vextenddouble/vextenddoubleϕk
h/vextenddouble/vextenddouble
(Λk
h)−1
+ 2/summationdisplay
k∈EBµH/radicalbig
d(k−τ)H/summationdisplay
h=1/vextenddouble/vextenddoubleϕk
h/vextenddouble/vextenddouble
(Λk
h)−1
≤2C0dH/radicalbig
log 2(dW/p )H/summationdisplay
h=1/radicalbig
W/H (/summationdisplay
k∈E/vextenddouble/vextenddoubleϕk
h/vextenddouble/vextenddouble2
(Λk
h)−1)1/2
+ 2H/summationdisplay
h=1(/summationdisplay
k∈EBθ,E/radicalbig
d(k−τ))1/2(/summationdisplay
k∈E/vextenddouble/vextenddoubleϕk
h/vextenddouble/vextenddouble2
(Λk
h)−1)1/2
+ 2H/summationdisplay
h=1(/summationdisplay
k∈EBµ,EH/radicalbig
d(k−τ))1/2(/summationdisplay
k∈E/vextenddouble/vextenddoubleϕk
h/vextenddouble/vextenddouble2
(Λk
h)−1)1/2
≤2C0dH/radicalbig
log 2(dW/p )H/summationdisplay
h=1/radicalbig
W/H (/summationdisplay
k∈E/vextenddouble/vextenddoubleϕk
h/vextenddouble/vextenddouble2
(Λk
h)−1)1/2
+ 2H/summationdisplay
h=1Bθ,E√
dW
H(/summationdisplay
k∈E/vextenddouble/vextenddoubleϕk
h/vextenddouble/vextenddouble2
(Λk
h)−1)1/2
+ 2H/summationdisplay
h=1Bθ,E√
dW(/summationdisplay
k∈E/vextenddouble/vextenddoubleϕk
h/vextenddouble/vextenddouble2
(Λk
h)−1)1/2(13)
27Published in Transactions on Machine Learning Research (10/2022)
By Lemma 14, we have
(/summationdisplay
k∈E/vextenddouble/vextenddoubleϕk
h/vextenddouble/vextenddouble2
(Λk
h)−1)1/2≤/radicaligg
dlog/parenleftbiggW
H+ 1/parenrightbigg
. (14)
Finally, by combining Eq. 11–14, we obtain the regret bound within the epoch Eas:
Dyn-Reg (E)≲˜O(H3/2d3/2W1/2+Bθ,EdW+Bµ,EdHW ).
By summing over all epochs and applying a union bound, we obtain the regret bound for the whole time
horizon.
Theorem 9. If we setβ=βk=cdH/radicalbig
log(2dT/p )+Bθ,E/radicalbig
d(k−τ)+Bµ,EH/radicalbig
d(k−τ), the dynamic regret
ofLSVI-UCB-Restart is˜O(H3/2d3/2TW−1/2+BθdW+BµdHW ), with probability at least 1−p.
Proof.In total there are N=⌈T
W⌉epochs. For each epoch Eiif we setδ=p
N, then it will incur regret
˜O(d3/2H3/2W1/2+Bθ,EidW+Bµ,EidHW )with probability at least 1−p
N. By summing over all epochs
and applying the union bound over them, we can obtain the regret upper bound for the whole time horizon.
With probability at least 1−p,
Dyn-Reg (T) =/summationdisplay
EiDyn-Reg (Ei)
≲/summationdisplay
Ei˜O(d3/2H3/2W1/2+Bθ,EidW+Bµ,EidHW )
≲˜O(H3/2d3/2TW−1/2+BθdW+BµdHW ).
B.2 Case 2: Unknown Local Variation
Similar to the case of known local variation, we first derive the error bound for the action-value function
estimate maintained in the algorithm for any policy, which is the following technical lemma.
Lemma 10. Under event Edefined in Lemma 1, we have for any policy π,∀s,a,h,k∈S×A× [H]×E,
|⟨ϕ(s,a),wk
h⟩−Qπ
h,k(s,a)−Pk
h(Vk
h+1−Vπ
h+1,k)(s,a)|≤β∥ϕ(s,a)∥(Λk
h)−1+Bθ,E/radicalbig
d(k−τ) +Bµ,EH/radicalbig
d(k−τ),
whereβ=C0dH/radicalbig
log(2dW/p )andτis the first episode in the current epoch.
Proof.This lemma is a looser upper bound implied by Lemma 2. By Lemma 2, we have
|⟨ϕ(s,a),wk
h⟩−Qπ
h,k(s,a)−Pk
h(Vk
h+1−Vπ
h+1,k)(s,a)|
≤CodH/radicalbig
log(2dW/p )∥ϕ(s,a)∥(Λk
h)−1+Bθ,E/radicalbig
d(k−τ)∥ϕ(s,a)∥(Λk
h)−1
+Bµ,EH/radicalbig
d(k−τ)∥ϕ(s,a)∥(Λk
h)−1
≤CodH/radicalbig
log(2dW/p )∥ϕ(s,a)∥(Λk
h)−1+Bθ,E/radicalbig
d(k−τ)
+Bµ,EH/radicalbig
d(k−τ),
where the second inequality is due to ∥ϕ(s,a)∥≤1andλmin(Λk
h)≥1, thus∥ϕ(s,a)∥(Λk
h)−1≤1.
Different from Lemma 3, when the local variation is unknown, the action-value function estimate we main-
tained in Algorithm 1 is no longer an optimistic upper bound of the optimal action-value function, but
approximately up to some error proportional to the local variation.
28Published in Transactions on Machine Learning Research (10/2022)
Proof of Lemma 4. We prove this by induction. First prove the base case when h=H. According to
Lemma 10, we have
|⟨ϕ(s,a),wk
H⟩−Q∗
H,k(s,a)|≤β∥ϕ(s,a)∥(Λk
H)−1+Bθ,E/radicalbig
d(k−τ) +Bµ,EH/radicalbig
d(k−τ),
which implies
Qk
H(s,a) = min{⟨wk
H,ϕ(s,a)⟩+β∥ϕ(s,a)∥(Λk
H)−1,H}
≥Q∗
H,k(s,a)−(Bθ,E/radicalbig
d(k−τ) +Bµ,EH/radicalbig
d(k−τ)).
Now suppose the statement holds true at step h+ 1, then for step h, due to Lemma 10, we have
|⟨ϕ(s,a),wk
h⟩−Qπ
h,k(s,a)−Pk
h(Vk
h+1−V∗
h+1,k)(s,a)|
≤β∥ϕ(s,a)∥(Λk
h)−1+Bθ,E/radicalbig
d(k−τ) +Bµ,EH/radicalbig
d(k−τ).
By the induction hypothesis, we have [Pk
h(Vk
h+1−V∗
h+1,k)](s,a)≥ − (H−h+ 2)(Bθ,E/radicalbig
d(k−τ) +
Bµ,EH/radicalbig
d(k−τ)), thus
Qk
h(s,a) = min{⟨wk
h,ϕ(s,a)⟩+β∥ϕ(s,a)∥(Λk
H)−1,H}
≥Q∗
h,k(s,a)−(H−h+ 1)(Bθ,E/radicalbig
d(k−τ) +Bµ,EH/radicalbig
d(k−τ)).
Similar to Lemma 9, next we derive the bound for the gap between the value function estimate and the
ground-truth value function for the executing policy πk,δk
h=Vk
h(sk
h)−Vπk
h,k(sk
h), in a recursive manner,
when the local variation is unknown.
Lemma 11. Letδk
h=Vk
h(sk
h)−Vπk
h,k(sk
h),ζk
h+1=E[δk
h+1|sk
h,ak
h]−δk
h+1. Under event Edefined in Lemma 1,
we have for all (k,h)∈E× [H],
δk
h≤δk
h+1+ζk
h+1+ 2β/vextenddouble/vextenddoubleϕk
h/vextenddouble/vextenddouble
(Λk
h)−1+Bθ,E/radicalbig
d(k−τ) +Bµ,EH/radicalbig
d(k−τ).
Proof.By Lemma 10, for any (s,a,h,k )∈S×A× [H]×E,
Qk
h(s,a)−Qπk
h(s,a)≤Pk
h(Vk
h+1−Vπk
h+1,k)(s,a) + 2β∥ϕ(s,a)∥(Λk
H)−1+Bθ,E/radicalbig
d(k−τ) +Bµ,EH/radicalbig
d(k−τ).
Note thatQk
h(sk
h,ak
h) = maxaQk
h(sk
h,a) =Vk
h(sk
h)according to Algorithm 1, and Qπk
h,k(sk
h,ak
h) =Vπk
h,k(sk
h)by
the definition. Thus,
δk
h≤δk
h+1+ζk
h+1+ 2β/vextenddouble/vextenddoubleϕk
h/vextenddouble/vextenddouble
(Λk
h)−1+Bθ,E/radicalbig
d(k−τ) +Bµ,EH/radicalbig
d(k−τ).
Now we are ready to prove Theorem 5, which is the regret upper bound within one epoch.
Proof of Theorem 5. We denote the dynamic regret within an epoch as Dyn-Reg (E). We define δk
h=
Vk
h(sk
h)−Vπk
h,k(sk
h)andζk
h+1=E[δk
h+1|sk
h,ak
h]−δk
h+1as in Lemma 11. We derive the dynamic regret within
a epochE(the length of this epoch is Wwhich is equivalent toW
Hepisodes) conditioned on the event E
29Published in Transactions on Machine Learning Research (10/2022)
defined in Lemma 1 which happens with probability at least 1−p/2.
Dyn-Reg (E)
=/summationdisplay
k∈E/bracketleftig
V∗
1,k(sk
1)−Vπk
1,k(sk
1)/bracketrightig
≤/summationdisplay
k∈E[Vk
1(sk
1) +Bθ,EH/radicalbig
d(k−τ) +Bµ,EH2/radicalbig
d(k−τ)−Vπk
1,k(sk
1)]
≤/summationdisplay
k∈E[δk
1+Bθ,EH/radicalbig
d(k−τ) +Bµ,EH2/radicalbig
d(k−τ)]
≤/summationdisplay
k∈EH/summationdisplay
h=1ζk
h+ 2β/summationdisplay
k∈EH/summationdisplay
h=1/vextenddouble/vextenddoubleϕk
h/vextenddouble/vextenddouble
(Λk
h)−1+ 2/summationdisplay
k∈EBθ,EH/radicalbig
d(k−τ) + 2/summationdisplay
k∈EBµ,EH2/radicalbig
d(k−τ)
≤/summationdisplay
k∈EH/summationdisplay
h=1ζk
h+ 2β/summationdisplay
k∈EH/summationdisplay
h=1/vextenddouble/vextenddoubleϕk
h/vextenddouble/vextenddouble
(Λk
h)−1+Bθ,EW/radicalbig
2d(W/H + 1) +Bµ,EW/radicalbig
2d(WH +H)(15)
where the first inequality is due to Lemma 4, the third inequality is due to Lemma 11, and the last inequality
is due to Jensen’s inequality. Now we need to bound the first two terms in the right side. Note that {ζk
h}is
a martingale difference sequence satisfying |ζk
h|≤2Hfor all (k,h). By Azuma-Hoeffding inequality we have
for anyt>0,
P/parenleftigg/summationdisplay
k∈EH/summationdisplay
h=1ζk
h≥t/parenrightigg
≥exp(−t2/(2WH2)).
Hence with probability at least 1−p/2, we have
/summationdisplay
k∈EH/summationdisplay
h=1ζk
h≤2H/radicalbig
Wlog(2dW/p ). (16)
For the second term, note that by Lemma 14 for any h∈[H], we have
/summationdisplay
k∈E(ϕk
h)⊤(Λk
h)−1ϕk
h≤2 log/bracketleftigg
det(Λk+1
h)
det(Λ1
h)/bracketrightigg
≤2dlog/parenleftbiggW
H+ 1/parenrightbigg
.
By Cauchy-Schwarz inequality, we have
/summationdisplay
k∈EH/summationdisplay
h=1/vextenddouble/vextenddoubleϕk
h/vextenddouble/vextenddouble
(Λk
h)−1≤H/summationdisplay
h=1/radicalbig
W/H/bracketleftigg/summationdisplay
k∈E(ϕk
h)⊤(Λk
h)−1ϕk
h/bracketrightigg1/2
≤H/radicaligg
2dW
Hlog/parenleftbiggW
H+ 1/parenrightbigg
≤H/radicalbigg
2dW
Hlog [2dW/p ]. (17)
Finally, combining Eq. 15–17, we have with probability at least 1−p,
Dyn-Reg (E)≤2H/radicalbig
Wlog(2dW/p ) +C0dH2/radicalbig
log(2dW)/p/radicalbigg
2dW
Hlog[2dW/p ]
+Bθ,EW/radicalbig
2d(W/H + 1) +Bµ,EW/radicalbig
2d(WH +H)
≲˜O(√
d3H3W+Bθ,E/radicalbig
d/HW3/2+Bµ,E√
dHW3/2).
30Published in Transactions on Machine Learning Research (10/2022)
Now we can derive the regret bound for the whole time horizon by summing over all epochs and applying a
union bound. We restate the regret upper bound and provide its detailed proof.
Theorem 10. If we setβ=cdH/radicalbig
log(2dT/p ), the dynamic regret of LSVI-UCB-Restart algorithm is
˜O(W−1/2Td3/2H3/2+Bθd1/2H−1/2W3/2+Bµd1/2H1/2W3/2), with probability at least 1−p.
Proof.In total there are N=⌈T
W⌉epochs. For each epoch Eiif we setδ=p
N, then it will incur regret
˜O(√
d3H3W+Bθ,Ei/radicalbig
d/HW3/2+Bµ,Ei√
dHW3/2)with probability at least 1−p
N. By summing over all
epochs and applying a union bound over them, we can obtain the regret upper bound for the whole time
horizon. With probability at least 1−p,
Dyn-Reg (T) =/summationdisplay
EiDyn-Reg (Ei)≲/summationdisplay
Ei˜O(√
d3H3W+Bθ,Ei/radicalbig
d/HW3/2+Bµ,Ei√
dHW3/2)
≲˜O(d3/2H3/2TW−1/2+Bθd1/2H−1/2W3/2+Bµd1/2H1/2W3/2).
C Proofs in Section 5
In this section, we derive the regret bound for Ada-LSVI-UCB-Restart algorithm.
Proof of Theorem 7. LetRi(W,s(i−1)H
1 )be the totol reward recieved in i-th block by running proposed
LSVI-UCB-Restart with window size Wstarting at state s(i−1)H
1, we can first decompose the regret as
follows:
Dyn-Reg (T) =K/summationdisplay
k=1V∗
1,k(s1
k)−⌈T/MH⌉/summationdisplay
i=1Ri(W†,s(i−1)H
1 )
/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
1+⌈T/MH⌉/summationdisplay
i=1(Ri(W†,s(i−1)H
1 )−Ri(Wi,s(i−1)H
1 )
/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
2,
where term 1is the regret incurred by always selecting the best epoch size for restart in the feasible set
JW, and term 2is the regret incurred by adaptively tuning epoch size by EXP3-P. We denote the optimal
epoch size in this case as W∗=⌈(Bθ+Bµ+ 1)−1/2d1/2H1/2T1/2⌉H. It is straightforward to verify that
1≤W∗≤MH, thus there exists a W†∈JWsuch thatW†≤W∗≤2W†, which well-approximates the
optimal epoch size up to constant factors. Denote the total variation of θandµin blockiasBθ,iandBµ,i
respectively. Now we can bound the regret. For the first term, we have
1≲⌈T/MH⌉/summationdisplay
i=1˜O(d3/2H3/2MH(W†)−1/2+Bθ,id1/2H−1/2(W†)3/2+Bµ,id1/2H1/2(W†)3/2)
≲˜O(d3/2H3/2T(W†)−1/2+Bθd1/2H−1/2(W†)3/2+Bµd1/2H1/2(W†)3/2)
≲˜O(d3/2H3/2T(W∗)−1/2+Bθd1/2H−1/2(W∗)3/2+Bµd1/2H1/2(W∗)3/2)
≲˜O((Bθ+Bµ+ 1)1/4d5/4H5/4T3/4),
where the first inequality is due to Theorem 6, and the third inequality is due to W†differs from W∗up to
constant factor. For the second term, we can directly apply the regret bound of EXP3-Palgorithm (Bubeck
& Cesa-Bianchi, 2012). In this case there are ∆ = lnM+ 1arms, number of equivalent time steps is ⌈T
MH⌉,
and loss per equivalent time step is bounded within [0,MH ]. Thus we have
2≲˜O(MH/radicalbig
∆T/MH )≤˜O(d1/4H3/4T3/4).
Combining the bound of 1and2yields the regret bound of Ada-LSVI-UCB-Restart ,
Dyn-Reg (T)≲˜O((Bθ+Bµ+ 1)1/4d5/4H5/4T3/4).
31Published in Transactions on Machine Learning Research (10/2022)
D Auxiliary Lemmas
In this section, we present some useful auxiliary lemmas.
Lemma 12. For any fixed policy π, let{wπ
h,k}h∈[H],k∈[K]be the corresponding weights such that Qπ
h,k(s,a) =
⟨ϕ(s,a),wπ
h,k⟩for all (s,a,h,k )∈S×A× [H]×[K]. Then we have
∀(k,h)∈[K]×[H],/vextenddouble/vextenddoublewπ
h,k/vextenddouble/vextenddouble≤2H√
d.
Proof.By the Bellman equation, we know that for any (h,k)∈[H]×[K],
Qπ
h,k(s,a) = (rk
h+Pk
hVπ
h+1,k)(s,a)
=⟨θh,k+/integraldisplay
Vπ
h+1,kdµh,k(s′),ϕ(s,a)⟩
=⟨wπ
h,k,ϕ(s,a)⟩,
where the second equality holds due to the linear MDP assumption. Under the normalization assumption
in Definition 1, we have ∥θh,k∥≤√
d,Vπ
h+1,k≤Hand∥µh,k(s′)∥≤√
d. Thus,
wπ
h,k≤√
d+H√
d≤2H√
d.
Lemma 13. LetΛt=I+/summationtextt
i=1ϕ⊤
iϕi, whereϕt∈Rd, then
t/summationdisplay
i=1ϕ⊤
i(Λt)−1ϕi≤d.
Proof.We have/summationtextt
i=1ϕ⊤
i(Λt)−1ϕi=/summationtextt
i=1Tr(ϕ⊤
i(Λt)−1ϕi) = Tr((Λ t)−1/summationtextt
i=1ϕiϕ⊤
i). After apply eigen-
value decomposition, we have/summationtextt
i=1ϕiϕ⊤
i=Udiag(λ1,...,λd)andΛt=Udiag(λ1+ 1,...,λd+ 1). Thus/summationtextt
i=1ϕ⊤
i(Λt)−1ϕi=/summationtextd
i=1λi
λi≤d.
Lemma 14. (Abbasi-Yadkori et al., 2011) Let {ϕt}t≥0be a bounded sequence in Rdsatisfying supt≥0∥ϕt∥≤
1. Let Λ0∈Rd×dbe a positive definite matrix. For any t≥0, we define Λt= Λ 0+/summationtextt
j=1ϕ⊤
jϕj. Then if the
smallest eigenvalue of Λ0satisfiesλmin(Λ0)≥1, we have
log/bracketleftbiggdet(Λt)
det(Λ 0)/bracketrightbigg
≤t/summationdisplay
j=1ϕ⊤
jΛ−1
j−1ϕj≤2 log/bracketleftbiggdet(Λt)
det(Λ 0)/bracketrightbigg
.
E Details of the Experiments
E.1 Synthetic Linear MDP Construction
The MDP has S= 15states,A= 7actions,H= 10,d= 10, andT= 20000, and 5 special chains. The
states are denoted s1,...,sS, and the actions are denoted a1,...,aA. We first construct the known feature ϕ.
Intuitively,ϕrepresents the transition from S×Aspace tod-dim space. We let the special chains have the
correct transition to the d-dim space while other parts have random transition. This special transition will
later be connected with µh,k, (i.e., the transition from d-dim space toSspace) to form the chain. A special
property of the construction is at any episode k, the transition function only has one connected chain and
such chain is similar to combination lock. The agent must find this unique chain to achieve good behavior.
We let feature ϕhave “one-hot” form (each (s,a)deterministically transits to a latent state in d-dim space),
and satisfy the following:
32Published in Transactions on Machine Learning Research (10/2022)
1. For special chain i= 1,..., 5, we haveϕ(si,ai)[i] = 1andϕ(si,ai)[n] = 0,n̸=i. Fori= 1,..., 5
andj∈[A],j̸=i, we haveϕ(si,aj)[n] = 1andϕ(si,aj)[l] = 0,l∈[S],l̸=n, wherekis uniformly
drawn from 1,...,i−1,i+ 1,...,d.
2. Fornormalchain i= 6,...,S,andj∈[A]wehaveϕ(si,aj)[n] = 1,andϕ(si,aj)[l] = 0,l∈[S],l̸=n,
wherenis uniformly drawn from 1,...,d.
Now consider designing µh,k. As mentioned in the main text, we have a set of 5 different MDPs, and they
have unique but different good chains. The abruptly-changing environment abruptly switches the good chain
(or linear MDP) periodically every 100 episodes, whereas the gradually-changing environment switches the
good chain (or linear MDP) continuously from one to another within every 100 episodes. In the following
construction, we only design those 5 different MDPs for the abruptly-changing environment since we only
need to take convex combination of different MDPs in the gradually-changing case. For each of those 5 linear
MDPs, we only let one special chain be connected, and it becomes the good chain. Other special chains are
broken in the µh,kpart (i.e., the part transits from d-dim space toSspace). When the good chain is g∈[5]
in episodek, we letµh,ksatisfy the following:
1. For good chain g,∀h∈[H], we haveµh,k(sg)[g] = 0.99,µh,k(sg+1)[g] = 0.01, andµh,k(sn)[g] =
0,n̸=g,g+ 1.
2. For other special but not good chain ˇg∈[5],ˇg̸=g,∀h∈[H], we haveµh,k(sˇg)[ˇg] = 0.01,
µh,k(sˇg+1)[ˇg] = 0.99, andµh,k(sn)[ˇg] = 0,n∈[S],n̸= ˇg,ˇg+ 1.
3. For all normal chain i= 6,...,d,∀h∈[H], we randomly sample two states n1andn2, and let
µh,k(sn1)[i] = 0.8,µh,k(sn2)[i] = 0.2, andµh,k(sn)[i] = 0,n∈[S],n̸=n1,n2.
Finally we construct θh,k, which is related to the reward function. To ensure g∈[5]is a good chain, we
place a huge reward at the end of the chain but 0 reward for the rest of the chain. In addition, we put small
intermediate rewards on sub-optimal actions. Specifically, when gis the good chain at episode k,θh,kis as
follows:
1. For good chain g, we haveθh,k[g] = 0,∀h∈[H−1], andθH,k[g] = 1.
2. For all other chains i∈[d],i̸=g,∀h∈[H], we letθh,k[i]uniformly sample from [0.005,0.008].
In our construction, it is straightforward to verify that we have a valid transition function, and the transition
function and reward function together satisfy the combination lock type construction. Notice that sometimes
we refer to the normal chain in the Sspace and sometimes in the d-dim space. The reason is that a special
chain must be connected in both parts ( S×Aspace tod-dim space and d-dim space toSspace), so we can
break any part to make it a normal chain.
E.2 Hardware Details
All experiments are performed on a Macbook Pro with 8 cores, 16 GB of RAM.
33