Under review as submission to TMLR
Learning Trees of ℓ0-Minimization Problems
Anonymous authors
Paper under double-blind review
Abstract
The problem of computing minimally sparse solutions of under-determined linear systems
Ax=bisNPhard in general. Subsets with extra properties, may allow efficient algorithms,
most notably problems with the restricted isometry property (RIP) can be solved by convex
ℓ1-minimization. Whiletheseclasseshavebeenverysuccessful,theyleaveoutmanypractical
applications. Alternative sub-classes, can be based on the prior information that x=Xzis
in the (sparse) span of some suitable matrix X. The prior knowledge allows us to reduce
assumptions on Afrom RIP to stable rank and by means of choosing Xmake the classes
flexible. However, in order to utilize these classes in a solver, we need explicit knowledge
ofX, which, in this paper, we learn form related samples, Aandbl,l= 1,.... During
training, we do not know Xyet and need other mechanisms to circumvent the hardness of
the problem. We do so by organizing the samples in a hierarchical curriculum tree with a
progression from easy to harder problems.
1 Introduction
We consider efficiently solvable subclasses of NPhard problems, signed extensions of 1-in-3-SAT at the end
of the paper and sparse solutions of linear systems in its main part: For matrix A∈Rm×nand right hand
sideb∈Rm, we wish to find the sparsest solution of
min
x∈Rn∥x∥0subject to Ax=b, (1)
where∥x∥0denotesthenumberofnon-zeroentriesof x. Infullgenerality, thisproblemis NP-hardNatarajan
(1995); Ge et al. (2011) but as many hard problems it contains tractable subclasses. Some of these are
uninteresting, at least from the perspective of sparsity, e.g. problems with zero kernel ker(A) = 0and unique
solution, which renders the ℓ0-minimization trivial. Other tractable subclasses have been extensively studied
in the literature, most notably problems that satisfy the (s,ϵ)-Restricted Isometry property (RIP)
(1−ϵ)∥x∥≤∥Ax∥≤(1 +ϵ)∥x∥for alls-sparsex∈Rn, (2)
withstrictrequirements ϵ<4/√
41≈0.6246ontheRIPconstantsandmoregenerallythe null space property
(NSP) of order s
∥vS∥1<∥v¯S∥1for all 0̸=v∈kerAand|S|≤s,
wherevSis the restriction of vto an index set Sand ¯Sits complement. In both cases, the sparsest solution
of (1) is found by the relaxation of the sparsity ∥·∥ 0to the convex∥·∥ 1-norm
min
x∈Rn∥x∥1subject to Ax=b,
see Candes et al. (2006); Donoho (2006); Candès et al. (2006); Foucart & Rauhut (2013) for details.
All of these tractable subclasses are completely rigid: A problem is either contained in the class or we are
out of luck. Alternatively, there are subclasses based on prior knowledge. Trivially, if we know that the
solutionx=Xzis in the column span of a matrix X∈Rn×p, we can simplify the search space to
min
z∈Rp∥Xz∥0subject to AXz =b,
1Under review as submission to TMLR
which, however, is no longer a standard compressed sensing problem in the variable z. In order to utilize
compressed sensing results, we confine Xto sparse matrices and consider the simpler problem to find sparse
z
min
z∈Rp∥z∥0subject to AXz =b. (3)
so that also the product x=Xzis necessarily sparse. In general this modified problem does not provide
the globally sparsest solution x, but does so in many scenarios: E.g. if ∥Xz∥0sparse solutions are unique
(which is much weaker than the RIP Foucart & Rauhut (2013)), if Xhas sufficiently sparse columns so that
cancellation of non-zero entries in their span are unlikely or the compressed sensing problem admits some
extra structure as in the SAT experiments at the end of the paper.
Besides the global optima question, the variant (3) has the advantage that it can be analyzed by available
compressed sensing theory. Indeed, we can uniquely recover zifAXis RIP, which is the case for (partially)
randomXand only mild rank conditions on A, see Kasiviswanathan & Rudelson (2019) and Welper (2020;
2021) in our context.
In summary, we can define tractable and adaptable subclasses by properly chosen X, but the algorithms
require explicit knowledge of it. Since it is implausible that we just happen to know a good X, we learn
it. While one may try to automatically uncover interesting or useful classes X, in this paper, we analyze
the simpler option of a teacher-student setup: The teacher knows Xand can generate samples from the
class, i.e. compressed sensing problems consisting of the measurement matrix Aand a right hand side
b=Ax. The student observes only the compressed sensing problems (A,b), without having the answers x
and reconstructs the class. On first sight, this is a cyclic problem, where the student has to solve intractable
problems to uncover a class that helps her to solve otherwise intractable problems. This conundrum is
solved by differentiating the problems into easy and hard ones: The former are used during training, can be
solved without prior knowledge of Xand have sparser zthan the hard problems that the student can solve
after training. For details, see Welper (2021) or its summary in Section 2, included to keep this paper self
contained.
New Contributions In Welper (2021), it is difficult to find easy problems that do not require prior
knowledge. This paper addresses this issue by organizing problems classes into a tree, similar to a university
curriculum. Each node is a problem class, arranged so that the hard problems on the children match the
easy problems on the parent. This setup allows the student to use the child prior to learn a tree node and
thus inductively iterate through the tree. We prove two main theorems stated informally as follows:
1.Theorem 3.5, Corollary 3.6: If a tree is learnable (see Definition 3.4), the student can learn to solve
all hard problems in the tree. As for compressed sensing, this is a recovery result; the student learns
the knowledge of the teacher. If this constitutes ℓ0minimizers is verified separately.
2.Theorem 4.2, Corollary 4.4: Given several assumptions, learnable trees exist, consisting of one
deterministic solution and further random solutions.
These two results mimic the theory of classical compressed sensing: 1. RIP conditions ensure sparse recovery
(viaℓ1minimization) and 2. RIP matrices exist (e.g. i.i.d. random matrices). Further results include the
following:
3. In Welper (2021) the problem class, defined by Xis completely random, because it is the most favor-
able setup for compressed sensing. As a first step towards low randomness classes, our construction
allows to embed one fully deterministic problem into the root of the curriculum tree. While a single
problem is not yet practical, it shows that some determinism is permissible.
4. In Section 5, we apply the learning method to a signed generalization of NPcomplete 1-in-3-SAT
problems.
In summary, unlike traditional tractable subclasses of ℓ0minimization, we aim for subclasses that are adapt-
able and learnable by some matrix X. Overlaps in the classes organized into a tree together with training
2Under review as submission to TMLR
samples form each class allow a student to follow a trail from easier to harder problems and avoid the
NP-hardness of the problems alone. See Figure 1.
ℓ0-min
ker(A) = 0
Null Space
PropertyAX
Figure 1: Tractable subclasses inside the NP-hard problem of sparse solutions of linear systems.
Human Learning The prior knowledge informed subclasses, together with an iterative learning curricu-
lum, are intended as a hypothetical model for human problem solving, or more concretely theorem proving.
IfP̸=NP, and human brains have no fundamental superiority to computers, humans cannot effectively
solve arbitrary instances of computationally hard problems. Yet, we routinely prove theorems and have built
up a rich trove of results. But we only do so in our respective areas of expertise. Hence, one may argue
that within these areas, and equipped with prior knowledge and experience, theorem proving is tractable.
If so, can we program corresponding solvers into a computer? The history of artificial intelligence provides
some caution. Hand coded rules in expert systems and natural language processing have proven difficult
due to their immense complexity, while learned approaches are currently superior. Likewise, instead of hand
crafting tractable subclasses, it seems more promising to learn them.
As a mathematical model for tractable subclasses, we consider sparse solutions of linear systems. These
areNP-hard and in (3), we have already identified some adaptable and tractable subclasses. The solution
vectorxis a model for a proof, as both are hard to compute. The linear combination x=Xz, together with
the non-linear minimal sparsity, composes a candidate solution xfrom elementary pieces in the columns of
X, similar to assembling a proof from known tricks, techniques, lemmas and theorems.
Of course, this solution strategy is of no use if we do not know X. Likewise, humans need to acquire their
expertise, either through training or research. An important component of both, is the solution of many
related and often simplified problems. For a student, these are split into episodes, ordered by prerequisites
into a curriculum tree. Likewise, for our mathematical model, we learn a tree of subclasses Xifrom simple
samples, i.e. pairs (Ak,bk)generated form solutions in the respective classes.
As we will see (Remark 3.3), the combined knowledge of all leaf nodes [X1,X2,...]in the curriculum tree
is not sufficient to solve all problems in the root node X0because in an expansion x=X0z0=/summationtext
iXizi, the
zicombined generally have less sparsity than z0and are thus more difficult to find. Therefore, at each tree
node we compress our knowledge into matrices with fewer columns and more sparse z. This step is similar
to summarizing reoccurring proof steps into a lemma and the using it as a black box in subsequent classes.
Unlike human curricula, the model curricula in this this paper are substantially random. This is remi-
niscent of compressed sensing and phase retrieval, whose theory is also more random than many practical
applications. In the latter areas much effort has been invested in low randomness results, yielding partially
structured measurement matrices like e.g. random samples from bounded orthonormal systems. Likewise,
in this paper, in an effort towards lower randomness, we allow one deterministic solution embedded into
otherwise random classes. Further effort towards more realistic low randomness models is left for future
work.
3Under review as submission to TMLR
Greedy Search and Heuristics Similar toℓ1minimization, greedy algorithms like orthogonal matching
pursuit
jn+1= argmax
j/vextendsingle/vextendsingleAT
·j(Axn−b)/vextendsingle/vextendsingle
Sn+1=Sn∪{jn+1}
xn+1= argmin
supp(x)⊂Sn+1∥Ax−b∥2
2,
also find global ℓ0-minimizers under RIP assumptions Foucart & Rauhut (2013). Instead of systematically
searching through an exponentially large set of candidate supports S, the first line provides a criterion to
greedily select the next support index, based on the correlation of a column A·jwith the residual Axn−b.
Applied to the modified problem (3) with prior knowledge X, the method changes to
jn+1= argmax
j/vextendsingle/vextendsingleXT
·jAT(AXzn−b)/vextendsingle/vextendsingle
Sn+1=Sn∪{jn+1}
zn+1= argmin
supp(z)⊂Sn+1∥AXz−b∥2
2.
In the first row, the learned knowledge Xmodifies the index selection and thus provides a learned greedy
criterion or heuristic. The learning of X, however, implicitly depends on a meta-heuristic as explained in
Remark 3.3 below. From this perspective, the proposed methods are related to greedy and heuristic search
methods in AI Russell et al. (2010); Sutton & Barto (2018); Holden (2021).
1.1 Related Work
ℓ0-Minimization without RIP This paper is mainly concerned with minimally sparse solutions of sys-
tems with non-NSP or non-RIP matrices A. A common approach in the literature for these systems is
ℓp-minimization with p < 1, which resembles the ℓ0-norm more closely than the convex ℓ1norm. While
sparse recovery can be guaranteed for weaker variants of the RIP Candès et al. (2008); Chartrand & Staneva
(2008); Foucart & Lai (2009); Sun (2012); Shen & Li (2012), these problems are again NPhard Ge et al.
(2011). Nonetheless, iterative solvers for ℓp-minimization or non-RIP Aoften show good results Candès et al.
(2008); Chartrand & Wotao Yin (2008); Foucart & Lai (2009); Daubechies et al. (2010); Lai et al. (2013);
Woodworth & Chartrand (2016).
ℓ0-Minimization with Learning Similar to our approach, many papers study prior information for
under-determined linear systems Ax=b. Similar to this paper, ℓ1synthesis März et al. (2022) considers
solutionsoftheform x=Xz, incasexisnotsparseinthestandardbasisandforrandom A. ThepapersBora
et al. (2017); Hand & Voroninski (2018); Huang et al. (2018); Dhar et al. (2018); Wu et al. (2019b) assume
that the solution xis in the range of a neural network x=G(z;w), with weights pre-trained on relevant
data, and then minimize minz∥AG(z;w)−b∥2. Alternatively, the deep image prior Ulyanov et al. (2020)
and compressed sensing applications Veen et al. (2020); Jagatap & Hegde (2019); Heckel & Soltanolkotabi
(2020) use the architecture of an untrained network as prior and minimize the weights minw∥AG(z;w)−b∥2
for some latent input z. These papers assume i.i.d. Gaussian Aor the Restricted Eigenvalue Condition
(REC) and use the prior to select a suitable candidate among all non-unique solutions. In contrast, in the
present paper, we aim for the sparsest solution and use the prior to address the hardness of the problem for
difficultA.
The paper Wu et al. (2019a) considers an auto-encoder mechanism to find measurement matrices A, not only
X, as in our case. Several other papers that combine compressed sensing with machine learning approximate
the right hand side to solution map b→xby neural networks Mardani et al. (2018); Shi et al. (2017).
Teaching Dimension A teacher/student setup is also considered in the teaching dimension . It measures
how many samples a teacher needs to provide for a learner to distinguish all concepts in a concept class
C⊂{ 0,1}Xfor some finite domain X, see Goldman & Kearns (1995). The recursive teaching dimension
4Under review as submission to TMLR
refines the idea to teaching plans, i.e. sequences of concepts and corresponding samples Zilles et al. (2011);
Doliwa et al. (2014); Kirkpatrick et al. (2019). The teaching dimension is closely related to the VC-dimension
Chen et al. (2016); Hu et al. (2017).
While we also learn problems in a curriculum imposing a sequential order, the goal is different: In the
terminology of supervised learning, the student learns the problem to solution map (A,b)→xfor(A,b)is
some problem class. Unlike supervised learning, this map is known to the student from the outset. The
problemisratherthatinitiallythestudentdoesnothaveanefficientalgorithmtocomputeitandthelearning
shall help her to reduce the “problem to solution map” to a convex optimization problem.
Knowledge Distillation Another area that relies on a teacher/student setup is knowledge distillation
Hinton et al. (2015), where a large teacher neural network is used to train a smaller student network. See
Gou et al. (2021) for an overview.
Transfer Learning The progression through a tree splits the learning problem into separate episodes on
different but related data sets. This is reminiscent of empirical studies on transfer- Donahue et al. (2014);
Yosinski et al. (2014) and meta-learning Hospedales et al. (2020) in neural networks.
1.2 Notations
We usecandCfor generic constants, independent of dimension, variance or ψ2norms that can change in
each formula. We write a≲b,a≳banda∼bfora≤cb,a≥cbandca≤b≤Ca, respectively. We denote
index sets by [n] ={1,...,n}and restrictions of vectors, matrix rows and matrix columns to J⊂[n]byvJ,
MJ·andM·J, respectively.
2 Easy and Hard Problems
In this section, we summarize an easy to hard progression from Welper (2021) that allows us to progress
from one node to the next, in the curriculum tree below.
2.1ℓ0-Minimization with Prior Knowledge
For given matrix A∈Rm×nand vector b∈Rm, we consider the ℓ0-minimization problem
min
x∈Rn∥x∥0,s.t.Ax=b
from the introduction. We have seen that this problem is NP-hard in general, but tractable for suitable
subclasses. While the RIP and NSP conditions are rigid classes, fully determined by the matrix A, we now
consider some more flexible ones, based on the prior knowledge that the solution is in some subset
C<t:={x∈Rn:x=Xz, zist-sparse},
parametrized by some matrix X∈Rn×pand with only mild assumptions on A, to be determined below.
Remark 2.1. This definition does not enforce that the x∈C<tareℓ0-minimizers and likewise, the main
results in this paper show recovery of x∈C<t, notℓ0minimization. In order to obtain ℓ0minimizers, we
need extra assumptions:
1. If the columns of Xares-sparse, all solutions in class C<tarest-sparse and global ℓ0minimization
can be guaranteed by uniqueness of st-sparse solutions. In classical compressed sensing this is implied
by the RIP condition, but can also be enforced by much weaker conditions, see Foucart & Rauhut
(2013).
2. For specific applications one may find alternative arguments. E.g. For the SAT type problems in
Section 5, Lemmas 5.4 and 5.5 show global ℓ0minimization of class members.
5Under review as submission to TMLR
We may regard X’s columns as solution components and hence assume that they are s-sparse, as well, for
somes>0, so that the solutions x=Xzin class are stsparse. Although the condition seems linear on first
sight, the sparsity requirement of zcan lead to non-linear behavior as explored in detail in Welper (2021).
As usual, we relax the ℓ0toℓ1norm and solve the convex optimization problem
min
x∈Rn∥z∥1,s.t.AXz =b. (4)
Of course any solver requires explicit knowledge of X, which we discuss in detail in Section 2.2. For now, let
us assumeXis known. Two extreme cases are noteworthy. First, without prior knowledge X=I, we retain
standardℓ1-minimization
min
x∈Rn∥x∥1,s.t.Ax=b,
which provides correct solutions for the ℓ0-minimization problem if Asatisfies the null-space property (NSP)
or the restricted isometry property (RIP), typically for sufficiently random A.
Second, if instead of the matrix A, the prior knowledge Xis sufficiently random, we can reduce the null-
space property of Ato a much weaker stable rank condition on A. In that case, the product AXsatisfies a
RIP with high probability (Kasiviswanathan & Rudelson (2019) and Theorem 2.5 below) and hence we can
recover a unique sparse z. SinceXis also sparse, this leads to a sparse solution x=Xzof the linear system
Ax=b. In order to show that xis the sparest possible solution, we need some extra structure, as discussed
in Remark 2.1.
2.2 Learning Prior Knowledge
We have seen that subclasses C<tofℓ0-minimization problems may be tractable, given suitable prior knowl-
edge encoded in the matrix X. Hence, we need a plausible model to acquire this knowledge. To this end,
we consider a teacher - student scenario, with a teacher that provides sample problems and a student that
infers knowledge Xfrom the samples.
The training samples must be chosen with care. Indeed, to be plausible for a variety of machine learning
scenarios, we assume that the student receives samples (A,bi), but not the corresponding solutions xi. On
first sight, this poses a cyclic problem: We need Xto efficiently solve for xi, but we need xito findX.
To resolve this issue, we train only on a subset of easy problems Ceasy⊂C<t. These must be sufficient to fully
recoverXand at the same time solvable by the student, without prior knowledge of X, by some method
Solve (A,b): Given an easy problem (A,Ax ), withx∈Ceasy, returnx.
Throughout this paper, easy problems are given by b=AXzfor random samples zwith expected sparsity
¯t<t, which is strictly less than the sparsity of class C≤t, see Assumption (A1) for details. These samples are
provided by the teacher, who has access to X, in contrast of the student, who has not. If this class is indeed
easy, depends on the existence of Solveand requires a delicate balance because we want the easy problems
solvable but the hard ones not (otherwise training is not necessary). At this point, we do not consider the
implementation of Solve. It will arise naturally out of the tree construction in Section 3, which also resolves
the balancing issue. For comparison, the presence of easy problems may also play a role in gradient descent
training of neural networks Allen-Zhu & Li (2020).
In order to recover the matrix Xfrom the easy samples Ceasy, the student combines the corresponding
solutions into a matrix Y(as columns). Since Ceasyis contained inC<t, they must be of the form Y=XZ
for some matrix Zwitht-sparse columns. Given that Ycontains sufficiently many independent samples form
the classC<t, sparse factorization algorithms Aharon et al. (2006); Gribonval & Schnass (2010); Spielman
et al. (2012); Agarwal et al. (2014); Arora et al. (2014b;a); Neyshabur & Panigrahy (2014); Arora et al.
(2015); Barak et al. (2015); Schnass (2015); Sun et al. (2017a;b); Rencker et al. (2019); Zhai et al. (2020)
can recover the matrices XandZup to scaling Γand permutation P.
SparseFactor (Y): Factorize Yinto ¯X=XPΓand ¯Z= Γ−1P−1Zfor some permutation Pand
diagonal scaling Γ.
Scale: Scale the columns of ¯Xso thatA¯Xsatisfies the RIP.
6Under review as submission to TMLR
The permutation is irrelevant, but we need proper scaling for ℓ1minimizers to work, computed by Scale,
which is a simple normalization in Welper (2021) and an application dependent function in the experiments
in Section 5. We combine the discussion into Traindefined in Algorithm 1.
Algorithm 1 Training of easy problems Ceasy.
function Train(A,b1,...,bq)
For alll∈[q], computeyl=Solve (A,bl).
Combine all ylinto the columns of a matrix ¯Y.
Compute ¯X,¯Z=SparseFactor (¯Y)
return Scale(¯X).
end function
Remark 2.2. In general ¯Yand ¯Xhave the same column span and thus every x∈C<tis given by
x=¯Xz=¯Yu.
Why don’t we skip the sparse factorization? While zist-sparse by construction, u=Y+xis generally not.
Hence, even if Yis sufficiently random for AYto satisfy an RIP, it is not clear that it allows us to recover
uby the modified ℓ1-minimization (4).
2.3 Results
This section contains rigorous results for the algorithms of the last sections.
2.3.1 Learning Prior Knowledge
We need a suitable model of random matrices, where as usual the ψ2norm is defined by ∥X∥ψ2:=
supp≥1p−1/2E[|X|p]1/p.
Definition 2.3. A matrixM∈Rn×piss/n-Bernoulli-Subgaussian ifMjk= ΩjkRjk, where Ωis an i.i.d.
Bernoulli matrix and Ris an i.i.d. Subgaussian matrix with
E[Ωjk] =s
n,E[Rjk] = 0,E/bracketleftbig
R2
jk/bracketrightbig
=ν2,∥Rjk∥ψ2≤νCψ (5)
for some variance ν >0. We callMrestricteds/nBernoulli-Subgaussian if in addition
Pr [Rjk= 0] = 0,E[|Rjk|]∈/bracketleftbigg1
10,1/bracketrightbigg
,E/bracketleftbig
R2
jk/bracketrightbig
≤1,Pr [|Rjk|>τ]≤2e−τ2
2. (6)
Next, we define the easy class Ceasyas a slightly sparser version of C<tand generate the training data by
drawing random samples.
(A1) The easy class Ceasyconsists of solutions for xl=Xzlwith columns zlof¯t/2prestricted Bernoulli-
Subgaussian matrix Z∈Rp×qwith
clogq≤¯t≤t, q>cp2log2p,2
p≤¯t
p≤c√p. (7)
Thematrix Xisonlyknowntotheteacher,whilethestudentreceivessamples (A,bl)withbl=AXzl.
The first and last inequalities pose mild conditions on the sparsities tand¯t, while the middle inequality can
always be satisfied by providing sufficiently many samples. The vectors zlhave expected sparsity ¯tand
thus the corresponding solutions Xzlhave expected sparsity s¯t. In order for them be easier than the full
classC<t, we generally choose ¯t<t.
Next, we require the student to be accurate on easy problems, with a safety margin√
2on sparsity:
7Under review as submission to TMLR
(A2) For all√
2¯tsparse columns zlofZ, we have Solve (A,AXzl) =XzlforSolveas defined at the
beginning of Section 2.2.
This assumption, used in Welper (2021), is delicate and will be lifted in the reminder of the paper, see
Section 2.4 for more details. Since the student shall only recover the class X, at this point, it is not strictly
necessary that the solutions Xzlare globalℓ0minimizers, which can, however, be ensured by the teacher in
selecting the class X, see Remark 2.1. Finally, we need the following technical assumption.
(A3)Xhas full column rank.
Although this implies that Xhas more rows than columns, that is generally not true for AXused in the
sparse recovery (4). The assumption results from the sparse factorization Spielman et al. (2012), where X
represents a basis. Newer results Agarwal et al. (2014); Arora et al. (2014b;a; 2015); Barak et al. (2015)
consider over-complete bases with less rows than columns and coherence conditions and may eventually allow
a weaker assumption. Anyways, as is, the assumption can be enforced by shrinking the problem class, i.e.
removing columns in X, at the expense of being less expressive. Such a procedure is not necessary for the
choices ofXin this paper, which have strong random components and thus full rank with high probability.
With the given setup, we can recover Xfrom easy training samples as claimed in the previous sections.
Theorem 2.4 (Welper (2021), Theorem 4.2) .Assume that (A1), (A2) and (A3) hold. Then there are
constantsc >0andC≥0independent of the probability model, dimensions and sparsity, and a tractable
implementation of SparseFactor (see Section 2.2) so that with probability at least
1−Cp−c
the output ¯XofTrain(Algorithm 1) is a scaled permutation permutation ¯X=XPΓof the matrix Xthat
defines the classC<t.
The result follows from Theorem 4.2in Welper (2021) with some minor modifications described in Appendix
A.1.
2.3.2ℓ0-Minimization with Prior Knowledge
After we have learned X, we need to ensure that we can solve all problems in class C<tby (4), not only the
easy ones. We do so here for random X, which is clearly a idealization but common in compressed sensing,
phase retrieval and related fields. Section 4 makes some progress towards more realistic classes by allowing
some deterministic component. For this review, we assume
(A4) The matrix X∈Rn×pis(s/n√
2)-Bernoulli-Subgaussian with
∥A∥2
F
∥A∥2≥CC4
ψnt
sϵ2log/parenleftbigg3p
ϵt,/parenrightbigg
(8)
ψ2-norm bound Cψin the Bernoulli-Subgaussian model (5) and arbitrary constant 0<ϵ< 4/√
41≈
0.6246with the same bounds as the RIP constant in (2).
The left hand side ∥A∥2
F/∥A∥2is the stable rank of A. With the scaling
Scale (¯X) =√n
∥A∥F, (9)
we obtain the following result, with some minor modifications from the reference described in Appendix A.1.
Theorem 2.5 (Welper (2021), Theorem 4.2) .Assume we choose (9)forScaleand that (A1) and (A4)
hold. Then there are constants c > 0andC≥0independent of the probability model, dimensions and
sparsity, and a tractable implementation of SparseFactor (see Section 2.2) so that with probability at least
1−Cp−c
8Under review as submission to TMLR
the matrixXhas full column rank, s-sparse columns and AXand satisfies the RIP
(1−ϵ)∥v∥2≤∥A¯Xv∥2≤(1 +ϵ)∥v∥2 (10)
for all 2t-sparse vectors v∈Rp. Hence, for ϵ <4/√
41≈0.6246, we can solve all problems in C<tbyℓ1
minimization (4).
As discussed in Remark 2.1, this is a recovery result and ℓ0-optimality of the class C<tmust be shown
separately. In conclusion, if we train on easy samples in Ceasy, we can recover Xand thus with the modified
ℓ1-minimization (4) solve all problems in class C<t, even the ones which we could not solve before training.
2.4 Implementation of the Student Solver?
While most assumptions are of technical nature the two critical ones are:
1. Implementation of Solve? If we implement Solveby plainℓ1-minimization, Amust satisfy the s¯t-
NSP. This poses strong assumptions on Aand if it satisfies the slightly stronger st-NSP, all problems
inC<tcan be solved by ℓ1-minimization, rendering the training of Xobsolete. We resolve the issue
in the next section by a hierarchy of problem classes, which allow us to use prior knowledge from
lower level classes to implement Solve.
2. Can we learn classes Xthat are not fully random? Some partially deterministic cases are considered
in Section 4.
3 Iterative Learning
3.1 Overview
We have seen that we can learn to solve all problems in a class C<t, if we are provided with samples from an
easier subclassCeasy. The easy class must be sufficiently rich and at the same time its sample problems must
be solvable without prior training. This results in a delicate set of assumptions, which we have hidden in the
existence of Solve, in the last section. The situation becomes much more favorable if we do not try to learn
C<tat once, but instead iteratively proceed from easy to harder and harder problems. This way, we can
implement Solveby the outcomes of previous learning episodes, instead of uninformed plain ℓ1minimizers.
To this end, we order multiple problem classes into a curriculum, similar to a human student who progresses
from easy to hard classes ordered by a set of prerequisites. Likewise, we consider a collection of problem
classesCi, indexed by some index set i∈Iand organized in a tree, e.g.
C1
C2
C4C5C3
C6
with root node C0and where each class Cihas finitely many children Cj,j∈child(i).
(T1) Each tree node has at most γchildren for some γ≥0.
The student starts learning the leafs and may proceed to a class Cionly if all prerequisite or child classes
have been successfully learned. As before each class is given by a matrix Xiwithsisparse columns and
sparsityt
Ci:={x∈Rn:x=Xiz, zist-sparse}.
AsinRemark2.1, wedonotenforcethatclassmembersare ℓ0minimizers, whichhastobeensuredseparately.
The difficulty of each class roughly corresponds to the sparsity, with the easiest at the leafs and then less and
9Under review as submission to TMLR
less sparsity towards the root of the tree. In order to learn each class Ci, the corresponding easy problems
are constructed as in Assumption (A1) in the last section
(T2) Oneachnode i, theteacherprovideseasyproblemsconsistingofsolutionsfor xl=Xizlwithcolumns
zlof¯t/2prestricted Bernoulli-Subgaussian matrix Zi∈Rp×qwith
clogq≤¯t≤t, q>cp2log2p,2
p≤¯t
p≤c√p. (11)
The matrix Xiis only known to the teacher, while the student receives samples (A,bl)withbl=
AXizl.
Thus, the easy samples are x=Xizwith random zof expected sparsity ¯t. For reference, we define
Ceasy,i :={x∈Rn:x=Xiz, zis a column of Zi},
which differs slightly from Welper (2021) and its summary in Section 2 and is only used for the following
motivation.
In order to progress through the curriculum, we have to carefully connect each parent to its children. First,
we assume:
(T3) The combined knowledge of all children contains the knowledge of the parent, i.e.
Xi=/summationdisplay
j∈child(i)XjWj=:Xchild(i)Wchild(i)(12)
for some matrices Wjand combined matrices Xchild(i)= [Xj1,...,Xjr]and[WT
j1,...,WT
jr]Twith
child(i) ={j1,...,jr}.
Next, we carefully calibrate the sparsity of all matrices to obtain a proper easy/hard split.
(T4) Assume that the columns of Xiaresisparse and the columns of Wchild(i)aret/¯tsparse with
t/¯t≥1, s i¯t≤sjt, j ∈child(i). (13)
Then every element in the parent class satisfies x=Xizi=Xchild(i)Wchild(i)zi=:Xchild(i)zchild(i). Hence,
if it is easy for the parent ∥zi∥0≤¯t, it is hard for the combined knowledge of the children ∥zchild(i)∥0≤t.
But given our prerequisites, we can already solve all hard children problems and implement Solveby the
ℓ1minimization (4) with prior knowledge Xchild(i). Technically, this requires that AXchild(i)ist-NSP, not
only allAXj,j∈child(i)separately. This is a relatively mild extra assumption because the NSP typically
depends only logarithmically on the number of columns in X·.
(T5) For all tree nodes ithe matrix product A[Scale (Xchild(i))]as well as the root node A[Scale (X0)]
satisfy the null space property of order√
2t.
With the implementation of Solve, we can now learn the full parent class Ciby Algorithm 1 and then
proceed through the full tree by induction. The split (12), roughly models a set of university courses,
where higher level courses recombine concepts from multiple prerequisite courses. In summary, we have the
sparsities (ignoring probabilities in Ceasy,i)
x∈Child problems ⇝x=Xchild(i)zchild(i),∥zchild(i)∥0≤t,∥x∥0≤sjt,
x∈Ceasy,i⇝ x=Xizi, ∥zi∥0≤¯t,∥x∥0≤si¯t,
x∈Ci⇝ x=Xizi, ∥zi∥0≤t,∥x∥0≤sit.
10Under review as submission to TMLR
It remains to learn the leafs, for which we cannot rely on any prior knowledge. To this end, note that by
construction (12), we can expect the columns of the parent Xito be a factor t/¯t >1less sparse than the
columns of the children Xj,j∈child(i). Hence, in a carefully constructed curriculum, the tree nodes’ Xi
become more sparse towards the bottom of the tree and ideally have unit sparsity O(1)at the leafs. This
ensures that the leaf node classes can be solved by brute force in sub-exponential time.
(T6) We have a solver SolveL for the leaf nodes, satisfying Assumption (A2).
For some applications this may be costly, while for others, like SAT reductions to compressed sensing and
related problems discussed in Section 5, this is routinely done for moderately sized problems Holden (2021).
We need the following two technical assumptions
(T7) For each tree node i, the matrix Xihas full column rank.
(T8) On each tree node, we have implementations of Scale.
These match (A3) and Scalein Section 2, where they are discussed in more detail.
Remark 3.1. All problems xin classCiaret2/¯t-sparse linear combinations of Xchild(i). Hence, if AXchild(i)
satisfies the t2/¯tinstead of only a t-NSP, the student can solve all problems in Ci, without training Algorithm
1. Practically, she can jump a class, but it is increasingly difficult to jump all classes, which would render
the entire learning procedure void.
Remark 3.2. The easy/hard split is achieved by some matrix satisfying a ¯tbut not atRIP. In Section 2
this matrix is A, so that this setup is very limiting. In this section, this is the matrix AXchild(i)and therefore
at the digression of the teacher and to a large extend independent on the problem matrix A.
Remark 3.3. The sparse factorization in Algorithm 1 condenses the knowledge Xchild(i)intoXi, allowing
more sparse zithanzchild(i)and as a consequence to tackle more difficult, or less sparse, problems x. This
condensation is crucial to progress in the curriculum, but is in itself a meta-heuristic to consolidate knowledge.
It is comparable to Occam’s razor and the human preference for simple solutions. More flexible meta-
heuristics are left for future research.
3.2 Learnable Trees
ThealgorithmofthelastsectionissummarizedinAlgorithm2andallassumptionsinthefollowingdefinition.
Definition 3.4. We call a tree of problem classes Ci,i∈Ilearnable if it satisfies (T1)–(T8).
Deferring existence of learnable trees to Section 4 below, for now we assume that a teacher has already
constructed such a tree. Then, as reasoned in the last section, we can recover the knowledge X0of the root
classC0, up to permutation and scaling in polynomial time. For a formal proof, see Appendix A.3.
Theorem 3.5. LetCi,i∈Ibe learnable according to Definition 3.4. Then, there exists an implementation
ofSparseFactor and constants c >0andC≥0independent of the probability model, dimensions and
sparsity, so that with probability at least
1−Cγslogγ
log(cst/¯t)
0p−c
the output ¯Xi=TreeTrain (Ci)of Algorithm 2 is a scaled permutation Scale (¯Xi) =Scale (XiP)ofXi
for some permutation matrix P.
Knowing all Xiup to permutation and scaling allows the student to solve all problems in the tree. The proof
for the following corollary is in Appendix A.3.
11Under review as submission to TMLR
Corollary 3.6. Assume that the event in Theorem 3.5 is true so that the student has computed
TreeTrain (Ci) =¯Xifor all three nodes i∈I, which are scaled permutations Scale (¯Xi) =Scale (XiP)of
Xifor some permutation matrix P. Then, the student can solve all problems in class Ci,i∈Iby the convex
optimization problem (3).
Remark 3.7. As for compressed sensing, the last corollary is a recovery result: After training the student
can find the same solutions xin classCias the teacher. The corollary does not state that these are ℓ0-
minimizers, which has to be verified separately. In classical compressed sensing this follows from uniqueness
of sparse solutions, which is not required for the last corollary, but may be assumed in addition (and is much
weaker than the RIP, see e.g. Foucart & Rauhut (2013)). Alternative verification of global ℓ0minimization
are also possible as e.g. Lemmas 5.4, 5.5 for reductions of SAT type problems to compressed sensing.
The biggest problem with learning hard problems C<tfrom easy problems Ceasyin Theorem 2.4 is the need
for a solver for the easy problems, as discussed in Section 2.4. The hierarchical structure of Theorem 3.5
completely eradicates this assumption, except for the leaf nodes, which ideally have sparsity O(1)so that
brute force solvers are a viable option.
Algorithm 2 Tree training
SolveX: Solve the modified ℓ1-minimization (4) with the given matrix X
SolveL: Solver for leaf nodes.
Train (A,b1,...,bq,Solve ): Algorithm 1 using the given solver subroutine.
function TreeTrain (classCi)
Get matrix Aand training samples b1,...,bqfrom teacher.
ifCihas children then
ComputeXj=TreeTrain (Cj)forj∈child(i)
Concatenate all child matrices X= [Xj]j∈child(i)
returnXi=Train (A,b1,...,bq,SolveX)
else ifCihas no children then
returnXi=Train (A,b1,...,bq,SolveL )
end if
end function
3.3 Cost
Let us consider the cost of learnable trees from Definition 3.4. The number of nodes grows exponentially in
the depth of the tree, but the depth only grows logarithmically with regard to the sparsity s0of the root
node, given that we advance the sparsities sias fast as (13) allows.
Lemma 3.8. Lets0be the sparsity of the root node of the tree. Assume that each node of the tree has at
mostγchildren and that si¯t≳csjtforc≥0and allj∈child(i). Then the tree has at most
γN+1=γslogγ
log(ct/¯t)
0
nodes.
The proof is given in Appendix A.2. Since on each node, the number of training samples and the runtime
of the training algorithm are both polynomial, this lemma ensures that the entire curriculum is learned in
polynomial time, with an exponent depending on γ, and the ratio t/¯t.
4 A tree Construction
In the last section, we have seen that we can learn difficult classes, given a suitable training curriculum. In
this section, we argue that such curricula exist. Definition 3.4 and Theorem 3.5 state several conditions on
12Under review as submission to TMLR
classesCiand their matrices Xithat allow the student to successfully learn the entire tree. While these are
mainly simple dimensional requirements, the most severe is the NSP condition of A[Scale (Xchild(i))]. By
Kasiviswanathan & Rudelson (2019) or Theorem 2.5 this is expected for random Xi. For a more realistic
model scenario, we add a deterministic component.
The deterministic part guarantees that every global ℓ0-minimizer
min
x∈Rn∥x∥0,s.t.Ax=b (14)
canbeembeddedintoadedicatedcurriculum,forarbitraryrighthandside bandonlyminorrankassumptions
onA. The random part is a placeholder for further solutions in class, to obtain a more realistic model.
Remark 4.1. The model shall demonstrate that learning of any deterministic problem is possible, but is not
intended as a practical curriculum design.
4.1 Tree Result
GivenAandx, we construct a partially random learnable tree whose root class contains xand eachXihas
pcolumns for some p>0. To this end, we first partition the support supp(x)into non-overlapping patches
{J1,...,Jq}=Jand then place the corresponding sub-vectors of xintoqcolumns of the matrix
Sjl:=/braceleftbiggxjj∈Jl
0else.(15)
The columns are spread into the leaf classes of the following learnable tree, were κ(·)denotes the condition
number.
Theorem 4.2. LetA∈Rm×nand splitx∈Rnintoq= 2L,L≥1components Sgiven by (15). If
1.AShas full column rank.
2. On each tree node, we have implementations of Scale.
3.SolveL satisfies Assumption (A2) on the leaf nodes.
4.
t≳logp2+ log3p, 1≲t≲√p (16)
5.
min
J∈J∥A·J∥2
F
∥A·J∥2≳tκ(AS)L+tκ(AS) logcqp
t(17)
for some generic constant c, with probability at least
1−2 exp/parenleftigg
−c1
κ(AS)min
J∈J∥A·J∥2
F
∥A·J∥2/parenrightigg
there is a learnable binary tree of problem classes Ci,i∈Iof depthL, given by matrices Xiand sparsity t
so that
1. The root class C0containsx.
2. The parents are constructed from the children Xi=Xchild(i)Wchild(i), whereWchild(i)hast/¯t= 2
sparse columns.
3. The columns of the leaf nodes’ Xiare|J|sparse.
13Under review as submission to TMLR
4. Each class’ matrix Xicontainspcolumns, consisting of columns of S, i.e. pieces of x, in the leafs
and sums thereof in the interior nodes. All other entries are random (dependent between classes) or
zero.
In short, curricula that allow us to learn the root class do exist, even if we add some deterministic structure
to ensure that the classes contain some meaningful result. More sophisticated classes are left for future
research.
Note thatxcan be recovered even if it is not a global ℓ0minimizer. This has to be ensured separately by
the designer of the curriculum, see Remarks 2.1 and 3.7. The only restriction on xis Assumption 1 that AS
has full column rank. In case xis indeed a global ℓ0minimizer, this assumption is automatically satisfied
by the following lemma, with z= [1,1,...]T. The proof is in Appendix A.4.
Lemma 4.3. Assume the columns of S∈Rn×qhave non-overlapping support and z∈Rqwith non-zero
entries. If the vector x=Szis the solution of the ℓ0-minimization problem 14, then the columns of ASare
linearly independent.
Theorem 4.2 leaves the implementation of Scaleopen. The function is necessary because the sparse
factorization of Y=XZintoXandZin Algorithm 1 is not unique up to permutation and scaling. Two
options are as follows:
1. IfAXsatisfies the RIP, all columns of AXmust have unit size up to the RIP constants. Hence a
reasonable scaling of Xensures equality∥(AX)·i∥= 1. However, the proof only shows that TAX
is RIP for some preconditioner T, depending on the condition of the deterministic part AS. This
implies the NSP (without preconditioning) since it is invariant under left preconditioning and hence
ensuresℓ1recovery. However, this is not informative for scaling X, unless the teacher provides the
preconditioned matrix TAinstead ofA.
2. The teacher can ensure that the training samples Zare scaled, e.g. by sampling entries from a
discrete set{−1,0,1}, which allows the student to recover the scaling.
Another major assumption in Theorem 4.2 is the existence of a leaf node solver SolveL. We can use a
brute force approach if we manage to achieve enough sparsity |J|in the leaf nodes, which we estimate in the
following corollary.
Corollary 4.4. LetA∈Rm×n. Letx∈Rnbe a sparse, sx:=∥x∥0and for some qlet
J={J1,...,Jq},sx
q≤|Ji|≤2sx
q
be a quasi-uniform partition of its support and Sbe the corresponding component split defined in (15).
1. Assume that the following sub-matrices have uniform condition number and full stable rank
κ(AS)≲1,∥A·J∥2
F
∥A·J∥2≳|J|, J ∈J.
with
|J|≳logsxlogp+ (logp)2. (18)
2. On each tree node, we have implementations of Scale.
3.SolveL satisfies Assumption (A2) on the leaf nodes.
Then for some generic constant c, with probability at least
1−2 min/braceleftbig
s−c
x,p−c/bracerightbig
there is a learnable binary tree of problem classes Ci,i∈I, given by matrices Xiso that
14Under review as submission to TMLR
1. The root class C0containsx.
2. The columns of the leaf nodes’ Xiare|J|sparse.
3. Each class’ matrix Xicontainspcolumns
Proof.We apply Theorem 4.2. Using κ(AS)≲1and∥A·J∥2
F
∥A·J∥2≳|J|and choosing the most favorable t∼logp
in (16), assumption (17) reduces to
|J|≳Lt+tlog(2Lp)∼Lt+tlogp∼Llogp+ (logp)2, (19)
posing a limit on the minimal support size we can achieve at the leafs of the tree. In order to eliminate L,
the corollary assumes that all Jare of equivalent size. Since the tree has q= 2Lleafs, this implies that
sx∼|J|2Land thus logsx∼log|J|+L≥L. Thus, condition (19) reduces to
|J|≳logsxlogp+ (logp)2. (20)
Thus, the corollary follows from Theorem 4.2 with probability at least
1−2 exp/parenleftigg
−c1
κ(AS)min
J∈J∥A·J∥2
F
∥A·J∥2/parenrightigg
≤1−2 exp (−c|J|),
for allJ∈J, which directly shows the result.
Hence, on the leaf nodes, a brute force SolveL search of|J|sparse solutions, considers about n|J|≥nlogs
possible supports (ignoring pfor the time being, which is at the teachers discretion). While significantly
better than nspossible supports for finding xdirectly, the former number is not of polynomial size. In order
to drive down the search size to O(1), we can iterate the tree construction and build new trees designed to
enable the student to find every column in the leaf nodes Xiwith one full tree per column. At the break
between curricula, this requires the teacher to provide the samples (A,bk)withbk=A(Xi)·kfor every leaf
node column (Xi)·k, which is a much stronger requirement than just providing arbitrary samples from the
child classes in the interior nodes. Since this is more costly, we calculate in the next section that this still
leads to a total tree of polynomial size.
4.2 Tree Extension
The curriculum in Theorem 4.2 shrinks the support size from stologs. In order to reduce the size further,
we may build a new curriculum for every column in every leaf Xi, if these columns can be split with full
rank ofAS, yieldingp2L≤psnew curricula. The assumption seems plausible for the random parts and is
justified for the deterministic part by the following lemma (together with Lemma 4.3), proven in Appendix
A.4.
Lemma 4.5. Assume the columns of S∈Rn×qhave non-overlapping support and z∈Rqwith non-zero
entries. If the vector x=Szis the solution of the ℓ0-minimization problem 14, then the columns S·k,k∈[q]
are globalℓ0optimizers of
S·k∈min
x∈Rn∥x∥0subject to Ax=AS·k.
Remark 4.6. Within each curriculum, the teacher provides samples from each class. At the break between
different curricula, the teacher must provide the more restrictive samples b=Axwith columns xof leaf node
Xi. Weather this can be avoided in a more careful tree construction is left for future research.
Since we aim for leaf column support size |J|∼1and its lower bound (18) contains the number pof columns
in eachXi, which is at the teachers disposal, we shrink it together with the initial (sub-)curriculum support
sizesby choosing p∼s.
15Under review as submission to TMLR
Remark 4.7. By choosing a large constant in p∼sor alternatively p∼sα, for the more difficult curricula,
pcan be larger than m. But by (19), towards the simpler curricula pmust become small so that eventually
p≤mand the matrix AXihas more rows that columns. Depending on the kernel of AXi, this may void ℓ0
orℓ1-minimization and allow simpler constructions towards the bottom of the curriculum tree.
We iteratively repeat the procedure until the leaf support |J|∼O (1)is of unit size. The total number #(s)
of required (sub-)curricula for initial support size ssatisfies the recursive formula
#(s)∼ps#/parenleftbig
logslogp+ (logp)2/parenrightbig
≥s2#/parenleftbig
(logs)2/parenrightbig
By induction, one easily verifies that #(s)≲s3, so that we use only a polynomial number of curricula,
each of which can be learned in polynomial time. In conclusion, combining all problem classes into one
single master tree, this yields a curriculum for a student to learn the root C0in polynomial time,
including a predetermined solution x.The problem classes can be fairly large at the top of the tree
and must be small at the leafs. At the breaks between different curricula, the training samples must be of
unit size containing only one column of the next tree.
4.3 Construction Idea
In Theorem 4.2, all class matrices Xiare derived from the single matrix
X:=SZT+DR(I−ZZT). (21)
The first summand is the deterministic part, with components Sofxdefined in (15) and arbitrary matrix
Zwith sparse orthogonal columns that boosts the number of columns from qto the desired p. The second
summand is the random part with sparse random matrix R. The projector (I−ZZT)ensures that it does
not interfere with the deterministic part and Dis a scaling matrix to balance both parts.
We choose Zand the support of Rso that, upon permutation of rows and columns Xis a block matrix
X=
B1
...
Bq

with each block containing one piece xJ. The tree is constructed out of these blocks as follows in case q= 4
and analogously for larger cases.
X0=
B1
B2
B3
B4

X1=
B1
B2


B1

B2
X2=
B3
B4


B3


X4

See Appendices A.5.1 and A.6 for details.
5 Applications
5.1 3SAT and 1-in-3-SAT
For an example applications, we consider reductions from the NP-complete 3SAT and 1-in-3-SAT to sparse
linear systems (The paper Ayanzadeh et al. (2019) considers the other direction). The problems are defined
as follows.
16Under review as submission to TMLR
•Literal:boolean variable or its negation, e.g. : xor¬x.
•Clause:disjunction of one or more literals, e.g.: x1∨¬x2∨x3.
•3SAT:satisfiability of conjunctions of clauses with three literals. For a positive result, at least one
literal in each clause must be true.
•1-in-3-SAT: As 3SAT, but for a positive result, exactly one literal in each clause must be true.
Both problems are NP-complete and can easily be transformed into each other. In this section, we reduce
a 1-in-3-SAT problem with clauses ck,k∈[m]and boolean variables xi,i∈[n]to a sparse linear system,
following techniques from Ge et al. (2011). For each boolean variable xi, we introduce two variables yi∈R
corresponding to xiandzi∈Rcorresponding to ¬xifori∈[n]. For each clause ck, we define a pair of
vectorsCk, Dk. The vector Ckhas a one in each entry ifor which the corresponding literal (not variable) xi
is contained in the clause ckand likewise Dkhas a one in each entry ifor which the literal ¬xiis contained
inck. All other entries of CkandDkare zero. It is easy to see that
y∈{0,1}nandzi=¬yi
⇒Exactly one literal in ckis true if and only if CT
ky+DT
kz= 1. (22)
We combine the linear conditions into the linear system
A:=
···CT
1··· ···DT
1···
......
···CT
m··· ···DT
m···
......
Inn Inn
......
, b :=
1
...
1
1
...
1...
(23)
with some extra identity blocks that together with the ℓ0-minimization
min
y,z∈Rn∥y∥0+∥z∥0subject to A/bracketleftbiggy
z/bracketrightbigg
=b. (24)
ensure that y∈{0,1}n, when possible.
Lemma 5.1. The clauses ckcorresponding to CkandDk,k∈[m]are 1-in-3 satisfiable if and only if (24)
has ansparse solution.
Proof.Thei-th row of the identity blocks is yi+zi= 1. The solution is either 2-sparse or 1-sparse with
yi= 1, zi= 0oryi= 0, zi= 1. Hence the solution is at most nsparse. The latter two cases are true for all
iif and only if yandzcombined are nsparse. Then the pair (yi,zi)matches a boolean variable (xi,¬xi)
and the result follows from (22).
5.2 Model Class
Note that RIP proofs typically rely on random mean zero matrix entries, while reductions of random 1-in-3-
SAT subclasses to compressed sensing have matrices and solution vectors with non-negative entries and thus
non-zero mean. As a result, our theory is not immediately applicable. We avoid the problem by considering
a larger problem class with signed solutions x∈Rnorx∈{− 1,0,1}n, which we can sample by mean-zero
Bernoulli-Subgaussian distributions required for our theory. While in our 1-in-3-SAT reduction the first rows
ofAhave exactly three non-zero entries, for simplicity, we sample sparse rows
A=/bracketleftbiggA11A12
In/2In/2/bracketrightbigg
∈Rm×n, b =/bracketleftbiggb1
b2/bracketrightbigg
∈Rn
17Under review as submission to TMLR
for two sparse matrices A1j∈{0,1}(m−n/2)×(n/2). As in Lemma 5.1, the two identity blocks ensure that
any solution xofAx=bmust have support at least ∥x∥0≥ ∥b2∥0. In the 1-in-3-SAT case, equality
corresponds to satisfiable problems. Likewise, we ensure that all training problems satisfy ∥x∥0=∥b2∥0,
which automatically implies that they are global ℓ0optimizers.
Remark 5.2. If∥x∥0=∥b2∥0, thenxis a globalℓ0minimizer.
5.3 Curricula
We consider several example curricula. The first is a realization of the construction in Theorem 4.2. The
following two add some extra structure to ensure global ℓ0minimization properties. The Second for all
columns of each Xiin the curriculum and the third for all sparse linear combinations of columns of Xi, i.e.
all elements in the corresponding problem class Ci.
5.3.1 Curriculum I
We first consider a realization of the curriculum in Theorem 4.2, as shown in Figure 2. The curriculum is
constructed from a single matrix Xat the root node, where ∗entries are mean-zero random ±1and thex
entries are (different per entry) random {0,1}. The latter have non-zero mean, which is not amenable to RIP
conditions and used as a model for the deterministic part of the theory. The children Xiare constructed from
the parent by zeroing out selected rows, as shown in the figure. The tree is learnable, proven in Appendix
C.
Lemma 5.3. Assume that the dimensions m,n,p, sparsities t, deterministic solution xand matrix Asatisfy
all assumptions of Theorem 4.2. Then the tree in Figure 2 is learnable and all other conclusions in Theorem
4.2 hold.

x∗...∗
x∗...∗
x∗...∗
x∗...∗


x∗...∗
x∗...∗

......
x∗...∗
x∗...∗

......
Figure 2:Ximatrices for a Curriculum I. xcan be different in each row and ∗are random entries.
5.3.2 Curriculum II
For none of the solutions in the problem classes in Curriculum I we know if they are global ℓ0minimizers.
While this is not necessarily an issue for the tree construction, as outlined in Remark 3.7, it is not fully
satisfactory and global minimizers can be obtained as follows. First, we split the columns according to the
identity blocks in A, as shown in Figure 3. Each component in the upper block yor∗, has exactly one
corresponding component in the lower block zor+so that for each pair at most one entry is non-zero. As
a result each column has the required sparsity to guarantee that it is a global ℓ0minimum by Remark 5.2.
Lemma 5.4. For all nodes i∈Iin Curriculum II, all columns of Xiare globalℓ0minimizers.
Since the random parts of the curriculum have dependencies, we can no longer apply Theorem 4.2 to show
that this tree is learnable, although it is conceivable that similar results still apply.
18Under review as submission to TMLR

y∗...∗
y∗...∗
y∗...∗
y∗...∗
z+... +
z+... +
z+... +
z+... +


y∗...∗
y∗...∗
z+... +
z+... +

......
y∗...∗
y∗...∗
z+... +
z+... +

......
Figure 3:Ximatrices for Curriculum II with ℓ0minimal columns.
5.3.3 Curriculum III
In Curriculum II the columns of Xiare globalℓ0minimizers, but their linear combinations in the classes
Cior the training samples are generally not, which can be fixed by the modification in Figure 4. All blocks
individually work as before, but instead of allowing all possible sparse linear combinations of the columns,
we only allow one non-zero contribution from each block column. This ensures the sparsity requirements in
Remark 5.2 so that all problems in class are global ℓ0minimizers.
Lemma 5.5. For all nodes i∈Iin Curriculum III, define the classes by
Ci:={x∈Rn:x=Xiz, zist-sparse
zhas at most one non-zero entry per block column of Xi}
Then all elements of Ciare global are global ℓ0minimizers.
As for Curriculum II Theorem 4.2 is not applicable to show that this tree is learnable. Since the yandz
entries are non-negative, this allows us to build a curriculum to learn one arbitrary 1-in-3-SAT problem in
a larger class of random signed problems. If we can build an entire curriculum that is fully contained in
1-in-3-SAT itself remains open.
5.4 Numerical Experiments
Table 1 contains results for Curricula II and III. All ℓ1-minimizations problems are solved by gradient descent
in the kernel of Ax=band the sparse factorization is implemented by ℓ4-maximization Zhai et al. (2020).
Solutions on the leaf nodes are given instead of brute force solved. As in Welper (2021), Algorithm 1 contains
an additional grader that sorts out wrong solutions from Solve, which often depend on the gradient descent
accuracy. Scaleis implemented by snapping the output of SparseFactor to the discrete values {−1,0,1},
which allows exact recovery of all nodes Xi, without numerical errors. Further details are given in Appendix
C.
•Curriculum II: We train three tree nodes on two levels. Grader tests to accuracy 10−4. The results
are the average of 5independent runs.
19Under review as submission to TMLR

y∗
y∗
y∗
y∗
...
z+
z+
z+
z+
...


y∗
y∗
...
z+
z+
...

......
y∗
y∗
...
z+
z+
...

......
Figure 4:Ximatrices for Curriculum III with ℓ0minimal classes.
Curr. I Curr. II
Depth 0 1 0
m 96 96 121
n 128 128 162
p/parenleftbig
Xchild(i)/parenrightbig
102 102 459
Rank/parenleftbig
AXchild(i)/parenrightbig
96.00 62.80 113.00
#Samples 10000 10000 90000
%Validate 0.55 0.91 0.98
#(Xstudent =X)5/5 7/10 2/2
Table 1: Results of numerical experiments, Section 5.4, averaged over all runs and all nodes of given depth.
The second but last row shows the percentage of successful training solutions, according to the grader. The
last row shows the number of successfully recovered Xifor the given level out of the total number of trials.
•Curriculum III: We train one tree node. The training sample matrices (23) are preconditioned per
node, not globally as in Theorem 4.2, below. Grader tests to accuracy 10−3. The results are the
average of 2independent runs.
Table 1 contains the results. It includes average ranks to show that the systems AXare non-trivial with non-
zero kernel and the row %Validate shows the percentage of correctly recovered training samples according
to the grader. A major bottleneck is the number of training samples for each node, which scales quadratically
forℓ4maximization (but only linear for unique factorization without algorithm Spielman et al. (2012)), up
to log factors. The last line shows that in the majority of cases we can recover the tree nodes Xi. The misses
depend on solver parameters as e.g. iteration numbers and the size of random matrices.
20Under review as submission to TMLR
6 Conclusion
Although sparse solutions of linear systems are generally hard to compute, many subclasses are tractable. In
particular, the prior knowledge x=Xzwith sparse zallows us to solve problems with only mild assumptions
onA. We learnXfrom a curriculum of easy samples and condensation of knowledge at every tree node. The
problems in each class must be compatible so that AXsatisfies the null space property. To demonstrate the
feasibility of the approach, we show that the algorithms can learn a class Xof non-trivial size that contains
an arbitrary solution x.
The results provide a rigorous mathematical model for some hypothetical principles in human reasoning,
including expert knowledge and its training in a curriculum. To be applicable in practice, further research
is required, e.g.:
•The mapping of SAT type problems into sparse linear problems lacks several invariances, e.g. a
simple reordering of terms may invalidate acquired knowledge. The reduction of SAT or other
problems to sparse linear solvers is similar to feature engineering in machine learning.
•For sparse factorization, the required number of samples scales quadratically, up to a log factor,
which is the biggest computational bottleneck in the numerical experiments. However, the current
implementationusesastandardmethodanddoesnotusethattheparentclass Xicanbeconstructed
from its children (12).
•The curriculum is designed so that knowledge can be condensed by sparse factorization, which in
itselfisameta-heuristic. Onemayneedtodynamicallyadaptthecondensationheuristictorealdata.
Since sparse factorization algorithms themselves often rely on ℓ1minimization, similar approaches
as discussed in the paper are conceivable.
•Not all relevant knowledge can be combined into one root class X0so thatAX0satisfies the null
space property. Hence, one may need several roots or rather a knowledge graph, together with a
decision criterion which node to use for a given problem.
21Under review as submission to TMLR
References
AlekhAgarwal, AnimashreeAnandkumar, PrateekJain, PraneethNetrapalli, andRashishTandon. Learning
sparsely used overcomplete dictionaries. In Maria Florina Balcan, Vitaly Feldman, and Csaba Szepesvári
(eds.),Proceedings of The 27th Conference on Learning Theory , volume 35 of Proceedings of Machine
Learning Research , pp. 123–137, Barcelona, Spain, 13–15 Jun 2014. PMLR. URL http://proceedings.
mlr.press/v35/agarwal14a.html .
Michal Aharon, Michael Elad, and Alfred M. Bruckstein. On the uniqueness of overcomplete dictionaries,
and a practical way to retrieve them. Linear Algebra and its Applications , 416(1):48–67, 2006. ISSN
0024-3795. doi: 10.1016/j.laa.2005.06.035. URL http://www.sciencedirect.com/science/article/
pii/S0024379505003459 . Special Issue devoted to the Haifa 2005 conference on matrix theory.
Zeyuan Allen-Zhu and Yuanzhi Li. Backward feature correction: How deep learning performs deep learning,
2020. URL https://arxiv.org/abs/2001.04413 .https://arxiv.org/abs/2001.04413 .
SanjeevArora,AdityaBhaskara,RongGe,andTengyuMa. Morealgorithmsforprovabledictionarylearning,
2014a. URL https://arxiv.org/abs/1401.0579 .https://arxiv.org/abs/1401.0579 .
Sanjeev Arora, Rong Ge, and Ankur Moitra. New algorithms for learning incoherent and overcomplete
dictionaries. In Maria Florina Balcan, Vitaly Feldman, and Csaba Szepesvári (eds.), Proceedings of The
27th Conference on Learning Theory , volume35of Proceedings of Machine Learning Research , pp.779–806,
Barcelona, Spain, 13–15 Jun 2014b. PMLR. URL http://proceedings.mlr.press/v35/arora14.html .
Sanjeev Arora, Rong Ge, Tengyu Ma, and Ankur Moitra. Simple, efficient, and neural algorithms for sparse
coding. In Peter Grünwald, Elad Hazan, and Satyen Kale (eds.), Proceedings of The 28th Conference on
Learning Theory , volume 40 of Proceedings of Machine Learning Research , pp. 113–149, Paris, France,
03–06 Jul 2015. PMLR. URL http://proceedings.mlr.press/v40/Arora15.html .
Ramin Ayanzadeh, Milton Halem, and Tim Finin. Sat-based compressive sensing, 2019. URL https:
//arxiv.org/abs/1903.03650 .
Boaz Barak, Jonathan A. Kelner, and David Steurer. Dictionary learning and tensor decomposition via the
sum-of-squares method. In Proceedings of the Forty-Seventh Annual ACM Symposium on Theory of Com-
puting, STOC ’15, pp. 143–151, New York, NY, USA, 2015. Association for Computing Machinery. ISBN
9781450335362. doi: 10.1145/2746539.2746605. URL https://doi.org/10.1145/2746539.2746605 .
Richard Baraniuk, Mark Davenport, Ronald DeVore, and Michael Wakin. A simple proof of the restricted
isometry property for random matrices. Constructive Approximation , 28(3):253–263, Dec 2008. ISSN
1432-0940. doi: 10.1007/s00365-007-9003-x. URL https://doi.org/10.1007/s00365-007-9003-x .
Ashish Bora, Ajil Jalal, Eric Price, and Alexandros G. Dimakis. Compressed sensing using generative
models. In Doina Precup and Yee Whye Teh (eds.), Proceedings of the 34th International Conference on
Machine Learning , volume 70 of Proceedings of Machine Learning Research , pp. 537–546, International
Convention Centre, Sydney, Australia, 06–11 Aug 2017. PMLR. URL http://proceedings.mlr.press/
v70/bora17a.html .
E. J. Candes, J. Romberg, and T. Tao. Robust uncertainty principles: exact signal reconstruction from
highly incomplete frequency information. IEEE Transactions on Information Theory , 52(2):489–509, Feb
2006. ISSN 1557-9654. doi: 10.1109/TIT.2005.862083.
Emmanuel J. Candès, Justin K. Romberg, and Terence Tao. Stable signal recovery from incomplete and
inaccurate measurements. Communications on Pure and Applied Mathematics , 59, 08 2006. doi: 10.1002/
cpa.20124.
Emmanuel J. Candès, Michael B. Wakin, and Stephen P. Boyd. Enhancing sparsity by reweighted ℓ1
minimization. Journal of Fourier Analysis and Applications , 14(5):877–905, Dec 2008. ISSN 1531-5851.
doi: 10.1007/s00041-008-9045-x. URL https://doi.org/10.1007/s00041-008-9045-x .
22Under review as submission to TMLR
R. Chartrand and Wotao Yin. Iteratively reweighted algorithms for compressive sensing. In 2008 IEEE
International Conference on Acoustics, Speech and Signal Processing , pp. 3869–3872, March 2008. doi:
10.1109/ICASSP.2008.4518498.
Rick Chartrand and Valentina Staneva. Restricted isometry properties and nonconvex compressive sensing.
Inverse Problems , 24(3):035020, may 2008. doi: 10.1088/0266-5611/24/3/035020. URL https://doi.
org/10.1088%2F0266-5611%2F24%2F3%2F035020 .
Xi Chen, Xi Chen, Yu Cheng, and Bo Tang. On the recursive teaching dimension of vc classes. In D. Lee,
M. Sugiyama, U. Luxburg, I. Guyon, and R. Garnett (eds.), Advances in Neural Information Processing
Systems, volume 29. Curran Associates, Inc., 2016. URL https://proceedings.neurips.cc/paper_
files/paper/2016/file/69a5b5995110b36a9a347898d97a610e-Paper.pdf .
Ingrid Daubechies, Ronald DeVore, Massimo Fornasier, and C. Sinan Güntürk. Iteratively reweighted least
squares minimization for sparse recovery. Communications on Pure and Applied Mathematics , 63(1):1–38,
2010. doi: 10.1002/cpa.20303. URL https://onlinelibrary.wiley.com/doi/abs/10.1002/cpa.20303 .
Manik Dhar, Aditya Grover, and Stefano Ermon. Modeling sparse deviations for compressed sensing using
generative models, 2018. https://arxiv.org/abs/1807.01442 .
Thorsten Doliwa, Gaojian Fan, Hans Ulrich Simon, and Sandra Zilles. Recursive teaching dimension, vc-
dimension and sample compression. Journal of Machine Learning Research , 15(89):3107–3131, 2014. URL
http://jmlr.org/papers/v15/doliwa14a.html .
Jeff Donahue, Yangqing Jia, Oriol Vinyals, Judy Hoffman, Ning Zhang, Eric Tzeng, and Trevor Darrell.
Decaf: A deep convolutional activation feature for generic visual recognition. In Eric P. Xing and
Tony Jebara (eds.), Proceedings of the 31st International Conference on Machine Learning , volume 32
ofProceedings of Machine Learning Research , pp. 647–655, Bejing, China, 22–24 Jun 2014. PMLR. URL
http://proceedings.mlr.press/v32/donahue14.html .
D. L. Donoho. Compressed sensing. IEEE Transactions on Information Theory , 52(4):1289–1306, April
2006. ISSN 1557-9654. doi: 10.1109/TIT.2006.871582.
Simon Foucart and Ming-Jun Lai. Sparsest solutions of underdetermined linear systems via ℓq-minimization
for0< q≤1.Applied and Computational Harmonic Analysis , 26(3):395–407, 2009. ISSN 1063-
5203. doi: 10.1016/j.acha.2008.09.001. URL http://www.sciencedirect.com/science/article/pii/
S1063520308000882 .
Simon Foucart and Holger Rauhut. A Mathematical Introduction to Compressive Sensing . Birkhäuser, 2013.
Dongdong Ge, Xiaoye Jiang, and Yinyu Ye. A note on the complexity of lpminimization. Mathematical
Programming , 129(2):285–299, Oct 2011. ISSN 1436-4646. doi: 10.1007/s10107-011-0470-2. URL https:
//doi.org/10.1007/s10107-011-0470-2 .
S.A. Goldman and M.J. Kearns. On the complexity of teaching. Journal of Computer and System Sciences ,
50(1):20–31, 1995. ISSN 0022-0000. doi: 10.1006/jcss.1995.1003. URL https://www.sciencedirect.
com/science/article/pii/S0022000085710033 .
Jianping Gou, Baosheng Yu, Stephen J. Maybank, and Dacheng Tao. Knowledge Distillation: A Survey.
International Journal of Computer Vision , 129(6):1789–1819, 2021. ISSN 0920-5691, 1573-1405. doi:
10.1007/s11263-021-01453-z. URL https://link.springer.com/10.1007/s11263-021-01453-z .
R. Gribonval and K. Schnass. Dictionary identification—sparse matrix-factorization via ℓ1-minimization.
IEEE Transactions on Information Theory , 56(7):3523–3539, 2010. doi: 10.1109/TIT.2010.2048466. URL
https://arxiv.org/abs/0904.4774 .
PaulHandandVladislavVoroninski. Globalguaranteesforenforcingdeepgenerativepriorsbyempiricalrisk.
In Sébastien Bubeck, Vianney Perchet, and Philippe Rigollet (eds.), Proceedings of the 31st Conference
On Learning Theory , volume 75 of Proceedings of Machine Learning Research , pp. 970–978. PMLR, 06–09
Jul 2018. URL http://proceedings.mlr.press/v75/hand18a.html .
23Under review as submission to TMLR
Reinhard Heckel and Mahdi Soltanolkotabi. Compressive sensing with un-trained neural networks: Gradient
descent finds a smooth approximation. In Hal Daumé, III and Aarti Singh (eds.), Proceedings of the
37th International Conference on Machine Learning , volume 119 of Proceedings of Machine Learning
Research , pp. 4149–4158, Virtual, 13–18 Jul 2020. PMLR. URL http://proceedings.mlr.press/v119/
heckel20a.html .
Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network, 2015.
Sean B. Holden. Machine learning for automated theorem proving: Learning to solve sat and qsat. Founda-
tions and Trends ®in Machine Learning , 14(6):807–989, 2021. ISSN 1935-8237. doi: 10.1561/2200000081.
URL http://dx.doi.org/10.1561/2200000081 .
Timothy Hospedales, Antreas Antoniou, Paul Micaelli, and Amos Storkey. Meta-learning in neural networks:
A survey, 2020. URL https://arxiv.org/abs/2004.05439 .https://arxiv.org/abs/2004.05439 .
Lunjia Hu, Ruihan Wu, Tianhong Li, and Liwei Wang. Quadratic upper bound for recursive teaching
dimension of finite vc classes. In Satyen Kale and Ohad Shamir (eds.), Proceedings of the 2017 Conference
on Learning Theory , volume 65 of Proceedings of Machine Learning Research , pp. 1147–1156. PMLR,
07–10 Jul 2017. URL https://proceedings.mlr.press/v65/hu17a.html .
Wen Huang, Paul Hand, Reinhard Heckel, and Vladislav Voroninski. A provably convergent scheme for
compressive sensing under random generative priors, 2018. URL https://arxiv.org/abs/1812.04176 .
https://arxiv.org/abs/1812.04176 .
Gauri Jagatap and Chinmay Hegde. Algorithmic guarantees for inverse imaging with untrained
network priors. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d’ Alché-Buc, E. Fox,
and R. Garnett (eds.), Advances in Neural Information Processing Systems , volume 32, pp.
14832–14842. Curran Associates, Inc., 2019. URL https://papers.nips.cc/paper/2019/hash/
831b342d8a83408e5960e9b0c5f31f0c-Abstract.html .
Shiva Prasad Kasiviswanathan and Mark Rudelson. Restricted isometry property under high correlations,
2019. URL https://arxiv.org/abs/1904.05510 .https://arxiv.org/abs/1904.05510 .
David Kirkpatrick, Hans U. Simon, and Sandra Zilles. Optimal collusion-free teaching. In Aurélien Garivier
and Satyen Kale (eds.), Proceedings of the 30th International Conference on Algorithmic Learning Theory ,
volume 98 of Proceedings of Machine Learning Research , pp. 506–528. PMLR, 22–24 Mar 2019. URL
https://proceedings.mlr.press/v98/kirkpatrick19a.html .
Ming-Jun. Lai, Yangyang. Xu, and Wotao. Yin. Improved iteratively reweighted least squares for uncon-
strained smoothed ℓqminimization. SIAM Journal on Numerical Analysis , 51(2):927–957, 2013. doi:
10.1137/110840364. URL https://doi.org/10.1137/110840364 .
Morteza Mardani, Qingyun Sun, David Donoho, Vardan Papyan, Hatef Monajemi, Shreyas Vasanawala,
and John Pauly. Neural proximal gradient descent for compressive imaging. In S. Bengio, H. Wallach,
H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett (eds.), Advances in Neural Information Pro-
cessing Systems , volume 31, pp. 9573–9583. Curran Associates, Inc., 2018. URL https://proceedings.
neurips.cc/paper/2018/hash/61d009da208a34ae155420e55f97abc7-Abstract.html .
Maximilian März, Claire Boyer, Jonas Kahn, and Pierre Weiss. Sampling Rates for $$\ell ^1$$-Synthesis.
Foundations of Computational Mathematics , August 2022. ISSN 1615-3375, 1615-3383. doi: 10.
1007/s10208-022-09580-w. URL https://link.springer.com/10.1007/s10208-022-09580-w;https:
//arxiv.org/abs/2004.07175 .
B. K. Natarajan. Sparse approximate solutions to linear systems. SIAM Journal on Computing , 24(2):
227–234, 1995. doi: 10.1137/S0097539792240406. URL https://doi.org/10.1137/S0097539792240406 .
Behnam Neyshabur and Rina Panigrahy. Sparse matrix factorization, 2014. URL https://arxiv.org/abs/
1311.3315 .https://arxiv.org/abs/1311.3315 .
24Under review as submission to TMLR
Lucas Rencker, Francis Bach, Wenwu Wang, and Mark D. Plumbley. Sparse recovery and dictionary learning
from nonlinear compressive measurements. IEEE Transactions on Signal Processing , 67(21):5659–5670,
2019. doi: 10.1109/TSP.2019.2941070. URL https://arxiv.org/abs/1809.09639 .
StuartJ.Russell,PeterNorvig, andErnestDavis. Artificial intelligence: a modern approach . PrenticeHallse-
ries in artificial intelligence. Prentice Hall, Upper Saddle River, 3rd ed edition, 2010. ISBN 9780136042594.
Karin Schnass. Local identification of overcomplete dictionaries. Journal of Machine Learning Research , 16
(35):1211–1242, 2015. URL http://jmlr.org/papers/v16/schnass15a.html .
Yi Shen and Song Li. Restricted p–isometry property and its application for nonconvex compressive sensing.
Advances in Computational Mathematics , 37:441–452, 2012. doi: 10.1007/s10444-011-9219-y.
W. Shi, F. Jiang, S. Zhang, and D. Zhao. Deep networks for compressed image sensing. In 2017 IEEE
International Conference on Multimedia and Expo (ICME) , pp. 877–882, 2017. doi: 10.1109/ICME.2017.
8019428. URL https://arxiv.org/abs/1707.07119 .
Daniel A. Spielman, Huan Wang, and John Wright. Exact recovery of sparsely-used dictionaries. volume 23
ofProceedings of Machine Learning Research , pp.37.1–37.18, Edinburgh, Scotland, 25–27Jun2012.JMLR
Workshop and Conference Proceedings. URL https://arxiv.org/abs/1206.5882 .
J. Sun, Q. Qu, and J. Wright. Complete dictionary recovery over the sphere i: Overview and the geomet-
ric picture. IEEE Transactions on Information Theory , 63(2):853–884, 2017a. doi: 10.1109/TIT.2016.
2632162. URL https://arxiv.org/abs/1511.03607 .
J. Sun, Q. Qu, and J. Wright. Complete dictionary recovery over the sphere ii: Recovery by riemannian
trust-region method. IEEE Transactions on Information Theory , 63(2):885–914, 2017b. doi: 10.1109/
TIT.2016.2632149. URL https://arxiv.org/abs/1511.04777 .
Qiyu Sun. Recovery of sparsest signals via ℓq-minimization. Applied and Computational Harmonic
Analysis, 32(3):329–341, 2012. ISSN 1063-5203. doi: 10.1016/j.acha.2011.07.001. URL http://www.
sciencedirect.com/science/article/pii/S1063520311000790 .
Richard S. Sutton and Andrew G. Barto. Reinforcement learning: an introduction . Adaptive computation
and machine learning series. The MIT Press, Cambridge, Massachusetts, second edition edition, 2018.
ISBN 9780262039246.
DmitryUlyanov,AndreaVedaldi,andVictorLempitsky. Deepimageprior. Int J Comput Vis ,128:1867–1888,
2020. doi: 10.1007/s11263-020-01303-4. URL https://arxiv.org/abs/1711.10925 .
DaveVanVeen,AjilJalal,MahdiSoltanolkotabi,EricPrice,SriramVishwanath,andAlexandrosG.Dimakis.
Compressed sensing with deep image prior and learned regularization, 2020. https://arxiv.org/abs/
1806.06438 .
Roman Vershynin. High-dimensional probability: an introduction with applications in data science . Num-
ber 47 in Cambridge series in statistical and probabilistic mathematics. Cambridge University Press,
Cambridge ; New York, NY, 2018. ISBN 9781108415194.
G. Welper. A relaxation argument for optimization in neural networks and non-convex compressed sensing,
2020. URL https://arxiv.org/abs/2002.00516 .https://arxiv.org/abs/2002.00516 .
G. Welper. Non-convex compressed sensing with training data, 2021. URL https://arxiv.org/abs/2101.
08310.https://arxiv.org/abs/2101.08310 .
Joseph Woodworth and Rick Chartrand. Compressed sensing recovery via nonconvex shrinkage penalties.
Inverse Problems , 32(7):075004, may 2016. doi: 10.1088/0266-5611/32/7/075004. URL https://doi.
org/10.1088%2F0266-5611%2F32%2F7%2F075004 .
25Under review as submission to TMLR
Shanshan Wu, Alex Dimakis, Sujay Sanghavi, Felix Yu, Daniel Holtmann-Rice, Dmitry Storcheus, Afshin
Rostamizadeh, and Sanjiv Kumar. Learning a compressed sensing measurement matrix via gradient
unrolling. In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.), Proceedings of the 36th International
Conference on Machine Learning , volume 97 of Proceedings of Machine Learning Research , pp. 6828–6839.
PMLR, 09–15 Jun 2019a. URL https://proceedings.mlr.press/v97/wu19b.html;https://arxiv.
org/abs/1806.10175 .
Yan Wu, Mihaela Rosca, and Timothy Lillicrap. Deep compressed sensing. volume 97 of Proceedings of
Machine Learning Research , pp. 6850–6860, Long Beach, California, USA, 09–15 Jun 2019b. PMLR. URL
https://arxiv.org/abs/1905.06723 .
Jason Yosinski, Jeff Clune, Yoshua Bengio, and Hod Lipson. How transferable are features in deep neural
networks? In Proceedings of the 27th International Conference on Neural Information Processing Systems
- Volume 2 , NIPS’14, pp. 3320–3328, Cambridge, MA, USA, 2014. MIT Press. URL https://papers.
nips.cc/paper/5347-how-transferable-are-features-in-deep-neural-networks.pdf .
Yuexiang Zhai, Zitong Yang, Zhenyu Liao, John Wright, and Yi Ma. Complete dictionary learning via
ℓ4-norm maximization over the orthogonal group. Journal of Machine Learning Research , 21(165):1–68,
2020. URL https://arxiv.org/abs/1906.02435 .
Sandra Zilles, Steffen Lange, Robert Holte, and Martin Zinkevich. Models of cooperative teaching and
learning. Journal of Machine Learning Research , 12(11):349–384, 2011. URL http://jmlr.org/papers/
v12/zilles11a.html .
A Details and Proofs
A.1 Easy and Hard Problems: Theorems 2.4, 2.5
Theorem 2.4 contains some small changes to the original reference Welper (2021). In the original version
(A1) contains two extra inequalities
n≥¯c1plogp,1
p≤s
n≤¯c2,
which are used to ensure that Xhas full rank Welper (2021), Proof of Theorem 4.2 with (A3), Item 4. We
assume this directly in (A3) and leave out the inequalities.
For Theorem 2.5, the reference Welper (2021) requires the extra assumption that Ax=bhas uniquestsparse
solutions, which is only used to verify that solutions of Solveare correct . In our case, this is implicitly
contained in (A2), instead.
A.2 Tree Size: Lemma 3.8
Lemma A.1 (Lemma 3.8 restated) .Lets0be the sparsity of the root node of the tree. Assume that each
node of the tree has at most γchildren and that si¯t≳csjtforc≥0and allj∈child(i). Then the tree has
at most
γN+1=γslogγ
log(ct/¯t)
0
nodes.
Proof.Letℓibe the level of a node, i.e. the distance to the root node, and Nthe maximal level of all nodes.
Each level has at most γN−inodes and thus the full tree has at most
N/summationdisplay
i=0γN−i=γN+1−1
γ−1≤γγN
26Under review as submission to TMLR
nodes.
It remains to estimate N. By induction on the assumption si¯t≥csjtwe have
sj≤/parenleftbigg¯t
ct/parenrightbiggℓj
s0
and thus, since necessarily sj≥1, we conclude that
s0≥/parenleftbiggct
¯t/parenrightbiggN
.
Plugging in γN=/parenleftbigct
¯t/parenrightbigNlogγ
logct/¯tthe number of nodes is bounded by
γγN=γ/parenleftbiggct
¯t/parenrightbiggNlogγ
logct/¯t
≤γslogγ
logct/¯t
0.
A.3 Learnable Trees: Theorem 3.5
Theorem A.2 (Theorem 3.5 restated) .LetCi,i∈Ibe learnable according to Definition 3.4. Then, there
exists an implementation of SparseFactor and constants c>0andC≥0independent of the probability
model, dimensions and sparsity, so that with probability at least
1−Cγslogγ
log(cst/¯t)
0p−c
the output ¯Xi=TreeTrain (Ci)of Algorithm 2 is a scaled permutation Scale (¯Xi) =Scale (XiP)ofXi
for some permutation matrix P.
Proof.The result follows from inductively applying Theorem 2.4 on each node of the tree, starting at its
leafs. The assumptions of Theorem 2.4 are easily matched with the given ones, except for (A2), which we
verify separately for leaf and non-leaf nodes.
1.Leave Nodes: For the leaf nodes (A2) is assumed. This is required because the globally sparsest
solution of Ax=bmay not be unique, in which case (A2) ensures that we pick an in class solution.
2.Non-Leave Nodes: Letzbe a column of the training sample Zandx=Xiz. By (12), we have
x=Xiz=Xchild(i)Wchild(i)z=:Xchild(i)w
with√
2tsparsewbecauseWchild(i)hast/¯tsparse columns and zis√
2¯tsparse, with probability at
least 1−2p−c(see the proof of Theorem 2.4, Item 2, in Welper (2021)). Since AXchild(i)satisfies
the√
2t-RIP, the correct solution xis recovered by the modified ℓ1-minimization (4) and hence by
SolveXi.
Finally, we add up the probabilities. By Theorem 2.4, the probability of failure on each node is at most
Cp−c. By Lemma 3.8, there are at most γslogγ
log(ct/¯t)
0nodes and thus the result follows from a union bound.
Proof of Corollary 3.6. By assumption, the student knows the matrices ¯Xi, which are scaled permutations
ofXi, i.e.Scale (¯Xi) =XiSPfor some scaling matrix Sand permutation matrix P. Since sub-matrices of
NSP matrices are also NSP, by Assumption (T5) of learnable trees and removing contributions from siblings,
27Under review as submission to TMLR
for all tree nodes i∈I, the scaled product A[Scale (¯Xi)]satisfies the null space property of order . Hence,
for every problem x=Xiz= (XiSP)(P−1S−1z) =:Scale (¯Xi)¯zwitht-sparsezin classCi, the convex
ℓ1-minimization problem
min
¯z∈Rp∥¯z∥1subject to AScale (¯Xi)¯z=b,
recovers ¯zand thus the solution x=Sclae (¯Xi)¯z.
A.4 Split of Global ℓ0Minimizers
This section contains two lemmas that state the splits of ℓ0minimizers are again ℓ0minimizers and that
they are linearly independent.
Lemma A.3 (Lemma 4.3 restated) .Assume the columns of S∈Rn×qhave non-overlapping support and
z∈Rqwith non-zero entries. If the vector x=Szis the solution of the ℓ0-minimization problem 14, then
the columns of ASare linearly independent.
Proof.Letxibe the columns of Sand assume that the Axi,i∈[t]are linearly dependent. Then there exists
a non-zero y∈Rtsuch that/summationtextt
i=1Axiyi= 0. Without loss of generality, let y1̸= 0so that
Ax1=−At/summationdisplay
i=2xiyi
y1.
We use this identity to eliminate x1:
b=Ax=At/summationdisplay
i=1xizi,=Ax1z1+At/summationdisplay
i=2xizi,=At/summationdisplay
i=2xizi/parenleftbigg
1−yi
y1z0/parenrightbigg
=:A¯x.
Since allxihave disjoint support and all ziare non-zero, we have ∥¯x∥0<∥x∥0, which contradicts the
assumption that xis aℓ0minimizer and thus all Axi,i∈[n]must be linearly independent.
Lemma A.4 (Lemma 4.5 restated) .Assume the columns of S∈Rn×qhave non-overlapping support and
z∈Rqwith non-zero entries. If the vector x=Szis the solution of the ℓ0-minimization problem 14, then
the columns S·k,k∈[q]are globalℓ0optimizers of
S·k∈min
x∈Rn∥x∥0subject to Ax=AS·k.
Proof.Assume the statement is wrong. Then for some k∈[q]there is aykwith
∥yk∥0≤∥S·k∥0, Ayk=AS·k.
Define
¯x:=ykzk+/summationdisplay
l̸=kS·lzl.
Then, we have
A¯x=Aykzk+A/summationdisplay
l̸=kS·lzl.=A/summationdisplay
lS·lzl=ASz =Ax
and since all S·lhave disjoint support and zl̸= 0
∥¯x∥0=∥yk∥0+/summationdisplay
l̸=k∥S·l∥0</summationdisplay
l∥S·l∥0=∥x∥0.
This contradicts the assumption that xis a globalℓ0minimiser and hence all S·kmust beℓ0minimizers as
well.
28Under review as submission to TMLR
A.5 Tree Nodes for Theorem 4.2
This section contains the construction of the matrices Xin the tree nodes used in Theorem 4.2.
A.5.1 Construction of X
We follow the idea outlined in Section 4.3. For given matrix Aand vector x, we construct a decomposition
matrixX∈Rn×pandzso thatx=Xzfort-sparsezandAXsatisfies the null space property. The
first condition ensures that xis contained in the class C<tand the second provides solvers Solve. This
construction will be used in subsequent sections to define nodes in the curriculum tree. We start with some
simple definitions
(M1) BySm×nwe denote all matrices in Rm×nwhose columns have non-overlapping support.
(M2) 1:=/bracketleftbig1··· 1/bracketrightbigTwith dimensions derived from context.
We splitxintoqnon-overlapping components, which we combine into the columns of a matrix S∈Sn×qso
thatx=S1. The matrix Shasqcolumns, which is generally less than the pcolumns we desire for a rich
class given by X. A convenient way out is to choose some matrix Z∈Rp×qwith orthonormal columns so
thatx=SZTZ1=SZTzwithz:=Z1. To ensure sparsity of zand for later tree construction, we confine
ZtoSp×q.
(M3)S∈Sn×qwith non-zero columns.
(M4)Z∈Sp×qwithℓ2-normalized columns.
While the matrix SZThas the same dimensions as X, it is generally low rank and cannot satisfy the NSP.
Furthermore, we want a rich class matrix Xwith further possible random solutions. To this end, we add in
a random matrix R, but only on blocks of SZTthat are non-zero to keep sparsity. We define Ras follows
(M5) Note that upon permutation of rows and columns SZTis a block diagonal matrix with l∈[q]blocks
with row indices supp(S·l)and column indices supp(Z·l). If the blocks do not contain all rows and
columns, we may enlarge them to some disjoint sets JlandKl, respectively so that
supp(S·l)⊂Jl, supp(Z·l)⊂Kl, l ∈[q].
Then each set Jl×Klcorresponds to one diagonal block that contains one component of xin the
columns of S. We also use the index free notation
J:={Jl:l∈[q]},K:={Kl:l∈[q]},JK:={Jl×Kl:l∈[q]},
(M6)R∈Rn×pis block matrix
Rjk=/braceleftbiggi.i.d random j,k∈[J,K]∈JK
0 else,
whose random entries satisfy
E[Rjk] = 0, E/bracketleftbig
R2
jk/bracketrightbig
= 1,∥Rjk∥ψ2≤Cψ
for some constant Cψand are absolutely continuous with respect to the Lebesgue measure.
Finally, we need a scaling matrix that will be determined below.
(M7)D∈Rn×nis a diagonal scaling matrix to be determined below.
29Under review as submission to TMLR
Then, we define the following class matrix
(M8)
X:=SZT+DR(I−ZZT), (25)
which is random on the kernel of ZTand matches the previously constructed SZTon the orthogonal
complement.
The following lemma summarises several elementary properties of the matrices and vectors in (M1) - (M8)
that are used in the proofs below. In particular, they satisfy x=Xzforz=Z1.
Lemma A.5. For the construction (M1) - (M8) we have:
1.ZTZ=I.
2.ZZTis an orthogonal projector.
3. Let supp(Z·l)⊂K∈Kfor some column l. Then
(ZZT)KL=/braceleftbigg
ZKlZT
KlifK=L
0 else.
4.(ZZT)KL= 0for allK̸=L∈K.
5.(ZZT)KKis an orthogonal projector for all K∈K.
6. For allu∈Rpwe have /summationdisplay
K∈K/vextenddouble/vextenddouble(ZZT)KKuK/vextenddouble/vextenddouble2=/vextenddouble/vextenddoubleZTu/vextenddouble/vextenddouble2.
7. For allu∈Rpwe have /summationdisplay
K∈K/vextenddouble/vextenddouble(I−ZZT)K·u/vextenddouble/vextenddouble2≤∥u∥2.
8. Forz=Z1, we haveZZTz=z.
9. Forx=S1andz=Z1, we haveSZTz=x.
10. Forx=S1andz=Z1, we haveXz=x.
Proof. 1. SinceZis normalized and Z∈Sp×q, all columns are orthonormal.
2.ZZTis symmetric and with Item 1 we have (ZZT)(ZZT) =Z(ZTZ)ZT=ZZT.
3. We have (ZZT)KL=/summationtextq
l=1(Z·lZT
·l)KL=/summationtextq
l=1ZKlZT
Ll,which reduces to the formula in the lemma
becauseK̸=Lare disjoint and suppZ·l⊂K.
4. Follows directly from Item 3.
5. Follows directly from Item 3 because the vectors ZKlis normalized.
6. For every K∈K, letl∈[q]be the corresponding index with supp(Z·l)⊂K. Then, we have
/summationdisplay
K∈K/vextenddouble/vextenddouble(ZZT)KKuK/vextenddouble/vextenddouble2=q/summationdisplay
K,l=1/vextenddouble/vextenddoubleZKlZT
KluK/vextenddouble/vextenddouble2
=q/summationdisplay
K,l=1(ZT
KluK)2=q/summationdisplay
l=1(ZT
·lu)2=/vextenddouble/vextenddoubleZTu/vextenddouble/vextenddouble2,
where in the first equality we have used Item 3, in the second that all ZKlare normalized and in
the third that supp(ZKl)⊂K.
30Under review as submission to TMLR
7. From Item 3, we have
(I−ZZT)K·u=uK−/summationdisplay
L∈K(ZZT)KLuL=uK−(ZZT)KKuK.
Since by Item 5 the matrix (I−ZZT)KKis a projector, it follows that
/summationdisplay
K∈K/vextenddouble/vextenddouble(I−ZZT)K·u/vextenddouble/vextenddouble2=/summationdisplay
K∈K/vextenddouble/vextenddouble(I−ZZT)KKuK/vextenddouble/vextenddouble2
≤/summationdisplay
K∈K/vextenddouble/vextenddouble(I−ZZT)KK/vextenddouble/vextenddouble2∥uK∥2≤∥u∥2.
8. With Item 1 we have ZZTz=ZZTZ1=Z1=z.
9. With Item 1 we have SZTz=SZTZ1=S1=x.
10. Follows directly from the previous items.
A.5.2 Expectation and Concentration
For the proof of RIP and null space properties, we need expectation and concentration results for ∥AXu∥
for an arbitrary u.
Lemma A.6. Letu∈Rp,A∈Rm×nandXbe the matrix defined in (25). Then
E/bracketleftbig
∥AXu∥2/bracketrightbig
=/vextenddouble/vextenddoubleASZTu/vextenddouble/vextenddouble2+/summationdisplay
[J,K]∈JK∥AD·J∥2
F/bracketleftig
∥uK∥2−/vextenddouble/vextenddouble(ZZT)KKuK/vextenddouble/vextenddouble2/bracketrightig
.
Proof.SinceRis zero outside of the blocks RJKfor[J,K]∈JK, we have
Xu= [SZT+DR(I−ZZT)]u=SZTu+/summationdisplay
[J,K]∈JKD·JRJK(I−ZZT)K·u
and thus
E/bracketleftbig
∥AXu∥2/bracketrightbig
=E
/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoubleASZTu+/summationdisplay
[J,K]∈JKAD·JRJK(I−ZZT)K·u/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2

=/vextenddouble/vextenddoubleASZTu/vextenddouble/vextenddouble2+/summationdisplay
[J,K]∈JK/vextenddouble/vextenddoubleAD·JRJK(I−ZZT)K·u/vextenddouble/vextenddouble2
=/vextenddouble/vextenddoubleASZTu/vextenddouble/vextenddouble2+/summationdisplay
[J,K]∈JK∥AD·J∥2
F/vextenddouble/vextenddouble(I−ZZT)K·u/vextenddouble/vextenddouble2,
where in the third line we have used Lemma B.1 and in the second that the cross terms
/angbracketleftigg
ASZTu,/summationdisplay
[J,K]∈JKAD·JE[RJK] (I−ZZT)K·u/angbracketrightigg
= 0
and/angbracketleftbig
AD·¯JE[R¯J¯K] (I−ZZT)¯K·u, AD·JE[RJK] (I−ZZT)K·u/angbracketrightbig
= 0
31Under review as submission to TMLR
vanish because E[RJK] = 0and in the last equation we have split the expectation because RJKandR¯J¯K
are independent for all cross terms (¯J,¯K)̸= (J,K). We simplify the last term
/vextenddouble/vextenddouble(I−ZZT)K·u/vextenddouble/vextenddouble2=/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoubleuK−/summationdisplay
L∈K(ZZT)KLuL/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
=/vextenddouble/vextenddoubleuK−(ZZT)KKuK/vextenddouble/vextenddouble2
=∥uK∥2−/vextenddouble/vextenddouble(ZZT)KKuK/vextenddouble/vextenddouble2,
where the second and third lines follow from Items 4 and 5 in Lemma A.5, respectively. Hence, we obtain
E/bracketleftbig
∥AXu∥2/bracketrightbig
=/vextenddouble/vextenddoubleASZTu/vextenddouble/vextenddouble2+/summationdisplay
[K,J]∈JK∥AD·K∥2
F/bracketleftig
∥uK∥2−/vextenddouble/vextenddouble(ZZT)KKuK/vextenddouble/vextenddouble2/bracketrightig
.
IfAShas orthonormal columns, we can simplify the expectation. Since this is generally not true, we rename
A→M, which will be a preconditioned variant of Alater.
Lemma A.7. Letu∈RpandM∈Rm×n. WithX,SandDdefined in (25), assume that MShas
orthonormal columns and the diagonal scaling is chosen as Dj=∥M·J∥−1
Ffor alljin blockJ∈J. Then
E/bracketleftig
∥MXu∥2/bracketrightig
=∥u∥2.
Proof.The result follows from Lemma A.6 after simplifying several terms. First, since MShas orthonormal
columns, we have (MS)T(MS) =Iand thus
/vextenddouble/vextenddoubleMSZTu/vextenddouble/vextenddouble2=uTZ(MS)T(MS)ZTu=uTZZTu=/vextenddouble/vextenddoubleZTu/vextenddouble/vextenddouble2.
Second, for arbitrary j∈J, by definition of the scaling D, we have
∥MD·J∥2
F=∥M·J∥2
F|Dj|2=∥M·J∥2
F∥M·J∥−2
F= 1.
Finally, from Lemma A.5 Item 6, we have
/summationdisplay
K∈K/vextenddouble/vextenddouble(ZZT)KKuK/vextenddouble/vextenddouble2=/vextenddouble/vextenddoubleZTu/vextenddouble/vextenddouble2.
Plugging into Lemma A.6, we obtain
E/bracketleftig
∥MXu∥2/bracketrightig
=/vextenddouble/vextenddoubleMSZTu/vextenddouble/vextenddouble2+/summationdisplay
[J,K]∈JK∥MD·J∥2
F/bracketleftig
∥uK∥2−/vextenddouble/vextenddouble(ZZT)KKuK/vextenddouble/vextenddouble2/bracketrightig
.
=/vextenddouble/vextenddoubleZTu/vextenddouble/vextenddouble2+
/summationdisplay
[J,K]∈JK∥uK∥2
−/vextenddouble/vextenddoubleZTu/vextenddouble/vextenddouble2
=∥u∥2.
Next, we prove concentration inequalities for the random matrix X.
Lemma A.8. Letu∈RpandM∈Rm×n. WithX,SandDdefined in (25), assume that MShas
orthonormal columns and the diagonal scaling is chosen as Dj=∥M·J∥−1
Ffor alljin blockJ∈J. Then
/vextenddouble/vextenddouble/vextenddouble∥MXu∥2−∥u∥/vextenddouble/vextenddouble/vextenddouble
ψ2≤CC2
ψmax
J∈J∥M·J∥
∥M·J∥F∥u∥.
32Under review as submission to TMLR
Proof.The result follows from Lemma B.4 after we have vectorized R. To this end, let vec(·)be the
vectorization, which identifies a matrix Ra×bwith a vector in (Ra)⊗(Rb)′for any dimensions a,b. Then,
since for all matrices ABu = (A⊗uT) vec(B), we have
MD·JRJK(I−(ZZT)K·u=/bracketleftbig
MD·J⊗uT(I−(ZZT)T
K·/bracketrightbig
vec (RJK)
so that
MXu = [MSZT+MDR (I−ZZT)]u
=MSZTu+/summationdisplay
[J,K]∈JKMD·JRJK(I−ZZT)K·u
=MSZTu+/summationdisplay
[J,K]∈JK/bracketleftbig
MD·J⊗uT(I−ZZT)T
K·/bracketrightbig
vec (RJK)
=:B+AR,
with the block matrix and vectors
A:=/bracketleftbig
MD·J⊗uT(I−ZZT)T
K·/bracketrightbig
[J,K]∈JK
R:= [vec (RJK)][J,K]∈JK
B:=MSZTu.
Using Lemma B.2 in the fist equality and Lemma A.7 in the last, we have
∥A∥2
F+∥B∥2=E/bracketleftig
∥AR +B∥2/bracketrightig
=E/bracketleftig
∥MXu∥2/bracketrightig
=∥u∥2.
Furthermore, we have
∥A∥≤
/summationdisplay
[J,K]∈JK/vextenddouble/vextenddoubleMD·J⊗uT(I−ZZT)T
K·/vextenddouble/vextenddouble2
1/2
=
/summationdisplay
[J,K]∈JK∥MD·J∥2/vextenddouble/vextenddouble(I−ZZT)K·u/vextenddouble/vextenddouble2
1/2
= max
J∈J∥MD·J∥/parenleftigg/summationdisplay
K∈K/vextenddouble/vextenddouble(I−ZZT)K·u/vextenddouble/vextenddouble2/parenrightigg1/2
≤max
J∈J∥MD·J∥∥u∥,
where in the last inequality we have used Lemma A.5, Item 7. Thus, with Lemma B.4, we have
∥∥MXu∥−∥u∥∥ψ2=/vextenddouble/vextenddouble/vextenddouble/vextenddouble∥AR +B∥−/parenleftig
∥A∥2
F+∥B∥2/parenrightig1/2/vextenddouble/vextenddouble/vextenddouble/vextenddouble
ψ2
≤CC2
ψ∥A∥≤CC2
ψmax
J∈J∥MD·J∥∥u∥.
We can further estimate the right hand side with the definition of diagonal scaling D
∥MD·J∥=∥M·JDJJ∥=∥M·J∥
∥M·J∥F,
which completes the proof.
33Under review as submission to TMLR
A.5.3 RIP of MX
We do not show the RIP for AXdirectly, but for a preconditioned variant. Since we determine the pre-
conditioner later, we first state results for a generic matrix MX. With the expectation and concentration
inequalities from the previous section, the proof of the RIP is standard, see e.g. Baraniuk et al. (2008);
Foucart & Rauhut (2013); Kasiviswanathan & Rudelson (2019). We first show a technical lemma.
Lemma A.9. LetA∈Rm×nand assume that there is aϵ
4coverN⊂Sn−1of the unit sphere Sn−1with
|∥Axi∥−1|≤ϵ
2for allxi∈N.
Then
(1−ϵ)∥x∥≤∥Ax∥≤(1 +ϵ)∥x∥for allx∈Rn.
Proof.Letx∈Sn−1be the maximizer of the norm so that ∥Ax∥=∥A∥. Then, there is a element xi∈N
in the cover with ∥x−xi∥≤ϵ
4and we obtain the upper bound
∥A∥=∥Ax∥≤∥Axi∥+∥A(x−xi)∥≤∥Axi∥+∥A∥ϵ
4
⇒/parenleftig
1−ϵ
4/parenrightig
∥A∥≤∥Axi∥
⇒∥A∥≤1 +ϵ/2
1−ϵ/4≤1 +ϵ.
With the upper bound and the given assumptions, for arbitrary x∈Sn−1, we estimate the lower bound by
∥Ax∥≥∥Axi∥−∥A(x−xi)∥≥∥Axi∥−(1 +ϵ)∥x−xi∥
≥/parenleftig
1−ϵ
2/parenrightig
−(1 +ϵ)ϵ
4= 1−ϵ
2−ϵ
4−ϵ2
4≥1−ϵ.
The bounds extend from the sphere to all x∈Rnby scaling.
For the following RIP result, we add in an isometry W∈Rp×p′, with∥W·∥=∥·∥, which allows us to
construct tree nodes Xifrom its children by (12) below.
Lemma A.10. LetW∈Rp×p′be an isometry and for M∈Rm×n, withX,SandDdefined in (25), assume
thatMShas orthonormal columns and the diagonal scaling is chosen as Dj=∥M·J∥−1
Ffor alljin block
J∈J. IfminJ∈J∥M·J∥2
F
∥M·J∥2≥2tC4
ψ
cϵ2log12ep
tϵ, then with probability at least 1−2 exp/parenleftig
−c
2ϵ2
C4
ψminJ∈J∥M·J∥2
F
∥M·J∥2/parenrightig
the matrixMXWsatisfies the RIP
(1−ϵ)∥z∥≤∥MXWz∥≤(1 +ϵ)∥z∥for allzwith∥z∥0≤t.
Proof.Fix a support T⊂[p′]with|T|=tand let ΣT⊂Rp′be the subspace of all vectors supported on T.
By standard volumetric estimates Baraniuk et al. (2008); Vershynin (2018) there is aϵ
4coverNof the unit
sphere in ΣTof cardinality
|N|≤/parenleftbigg12
ϵ/parenrightbiggt
.
Since∥Wzi∥=∥zi∥,zi∈N, by Lemma A.8 and a union bound, we obtain
Pr [∃zi∈N :|∥MXWzi∥−1|≥ϵ]≤2/parenleftbigg12
ϵ/parenrightbiggt
exp/parenleftigg
−cϵ2
C4
ψmin
J∈J∥M·J∥2
F
∥M·J∥2/parenrightigg
.
Let us assume that the event fails and thus |∥MXWzi∥−1|≤ϵfor allzi∈N. Then, by Lemma A.9, we
have
(1−ϵ)∥z∥≤∥MXWz∥≤(1 +ϵ)∥z∥for allz∈ΣT.
34Under review as submission to TMLR
There are/parenleftbigp
t/parenrightbig
≤/parenleftbigep
t/parenrightbigtsupportsTof sizetand thus, by a union bound we obtain
(1−ϵ)∥z∥≤∥MXWz∥≤(1 +ϵ)∥z∥for allzwith∥z∥0≤t
with probability of failure bounded by
2/parenleftigep
t/parenrightigt/parenleftbigg12
ϵ/parenrightbiggt
exp/parenleftigg
−cϵ2
C4
ψmin
J∈J∥M·J∥2
F
∥M·J∥2/parenrightigg
= 2 exp/parenleftigg
−cϵ2
C4
ψmin
J∈J∥M·J∥2
F
∥M·J∥2+tlog12ep
tϵ/parenrightigg
≤2 exp/parenleftigg
−c
2ϵ2
C4
ψmin
J∈J∥M·J∥2
F
∥M·J∥2/parenrightigg
if
tlog12ep
tϵ≤c
2ϵ2
C4
ψmin
J∈J∥M·J∥2
F
∥M·J∥2⇔min
J∈J∥M·J∥2
F
∥M·J∥2≥2tC4
ψ
cϵ2log12ep
tϵ.
A.5.4 Null Space Property of AX
The matrix MSin the RIP results must have orthonormal columns, which is not generally true for M=A.
However, this is true with a suitable preconditioner that we construct next. The null space property is
invariant under preconditioning, which allows us to eliminate it, later.
Lemma A.11. LetM∈Rm×qwithm≥qhave full column rank. Then there is a matrix T∈Rm×mwith
condition number κ(T) =κ(M)such thatTMhas orthonormal columns.
Proof.LetM=UΣVTbe the singular value decomposition of M. Define
T:=DUT, D−1:= diag[σ1,...,σq,σ,...,σ ]
forq≤msingular values σiand remaining m−qvaluesσin the interval [σ1,...,σq]. Then, we have
MTTTTM= (VΣTUT)(UDT)(DUT)(UΣVT) =VΣTDTDΣVT=VVT=I,
where we have used that ΣTDTDΣ =I. By construction, Thas singular values σ1,...,σqand one extra
valueσbounded by the former so that
κ(T) =σ1
σq=κ(M).
Lemma A.12. LetA∈Rm×nandT∈Rm×mbe invertible. Then
∥A∥F
∥A∥≤κ(T)∥TA∥F
∥TA∥.
Proof.We first show that
∥TA∥F≥/vextenddouble/vextenddoubleT−1/vextenddouble/vextenddouble−1∥A∥F.
Indeed∥x∥≤/vextenddouble/vextenddoubleT−1/vextenddouble/vextenddouble∥Tx∥implies∥Tx∥≥/vextenddouble/vextenddoubleT−1/vextenddouble/vextenddouble−1∥x∥and thus applied to the columns ajofA, we have
∥TA∥2
F=n/summationdisplay
j=1∥Taj∥2≥n/summationdisplay
j=1/vextenddouble/vextenddoubleT−1/vextenddouble/vextenddouble−2∥aj∥2=/vextenddouble/vextenddoubleT−1/vextenddouble/vextenddouble−2∥A∥2
F.
35Under review as submission to TMLR
With this estimate, we obtain
κ(T)∥TA∥F
∥TA∥≥∥T∥/vextenddouble/vextenddoubleT−1/vextenddouble/vextenddouble/vextenddouble/vextenddoubleT−1/vextenddouble/vextenddouble−1∥A∥F
∥T∥∥A∥=∥A∥F
∥A∥.
Corollary A.13. LetW∈Rp×p′be an isometry and for X,SandDdefined in (25), assume that AS
has full column rank and minJ∈J∥A·J∥2
F
∥A∥2
·J≥2tC4
ψ
cϵ2κ(AS) log12ep
tϵ. Then there is an invertible matrix T∈
Rm×mso that with the diagonal scaling Dj=∥TA·J∥−1
Ffor alljin blockJ∈Jwith probability at least
1−2 exp/parenleftig
−c
2ϵ2
C4
ψ1
κ(AS)minJ∈J∥A·J∥2
F
∥A·J∥2/parenrightig
the matrixTAXWsatisfies the RIP
(1−ϵ)∥z∥≤∥TAXWz∥≤(1 +ϵ)∥z∥for allzwith∥z∥0≤t.
Proof.Since the matrix AShas full column rank by Lemmas A.11 and A.12, there is an invertible matrix T
such that
κ(T) =κ(AS), TAS has orthogonal columns
∥A·J∥F
∥A·J∥≤κ(T)∥TA·J∥F
∥TA·J∥for allJ∈J.
Thus, the corollary follows from Lemma A.10 with M=TA.
The last corollary allows us to recover x=S1byℓ1-minimization
min
x∈Rn∥x∥1subject to TAx =b,
preconditioned by some matrix T. This problem is not yet solvable by the student, who generally has no
access to the matrix T, which is only used by the teacher for the construction of X. However, the matrix Tis
unnecessary for ℓ1recovery because the RIP implies the null space property, which is sufficient for recovery
and independent of left preconditioning.
Corollary A.14. LetW∈Rp×p′be an isometry and for X,SandDdefined in (25), assume that AS
has full column rank and minJ∈J∥A·J∥2
F
∥A·J∥2≥2tC4
ψ
cϵ2κ(AS) log12ep
tϵ. Then there is an invertible matrix T∈
Rm×mso that with the diagonal scaling Dj=∥TA·J∥−1
Ffor alljin blockJ∈Jwith probability at least
1−2 exp/parenleftig
−c
2ϵ2
C4
ψ1
κ(AS)minJ∈J∥A·J∥2
F
∥A·J∥2/parenrightig
the matrixAXWsatisfies the null space property of order t
∥zT∥1<∥z¯T∥1for allz∈ker(AXW )andT⊂[p],|T|≤t.
with complement ¯TofT.
Proof.Settingϵ=1
3, changing t→2tand adjusting the constants accordingly, with the given conditions
and probabilities, the matrix TAXsatisfies the/parenleftbig
2t,1
3/parenrightbig
-RIP. Thus, by Foucart & Rauhut (2013), proof of
Theorem 6.9,TAXsatisfies
∥zT∥1<1
2∥z∥1 for allz∈ker(TAX )andT⊂[p],|T|≤t.
This directly implies the null space property of order t
∥zT∥1<∥z¯T∥1for allz∈ker(TAX )andT⊂[p],|T|≤t.
SinceTis invertible, ker(TAX ) = ker(AX), so that also AXsatisfies the null space property.
36Under review as submission to TMLR
Remark A.15. For Corollaries A.13 and A.14, we are particularly interested in applications where x=S1
is the global ℓ0-minimizer of Ax=bin 14. Then the full column rank condition of ASis automatically
satisfied by Lemma A.3.
A.6 Model Tree: Theorem 4.2
Recall that κ(·)denotes the condition number.
Theorem A.16 (Theorem 4.2 restated) .LetA∈Rm×nand splitx∈Rnintoq= 2L,L≥1components
Sgiven by (15). If
1.AShas full column rank.
2. On each tree node, we have implementations of Scale.
3.SolveL satisfies Assumption (A2) on the leaf nodes.
4.
t≳logp2+ log3p, 1≲t≲√p (26)
5.
min
J∈J∥A·J∥2
F
∥A·J∥2≳tκ(AS)L+tκ(AS) logcqp
t(27)
for some generic constant c, with probability at least
1−2 exp/parenleftigg
−c1
κ(AS)min
J∈J∥A·J∥2
F
∥A·J∥2/parenrightigg
there is a learnable binary tree of problem classes Ci,i∈Iof depthL, given by matrices Xiand sparsity t
so that
1. The root class C0containsx.
2. The parents are constructed from the children Xi=Xchild(i)Wchild(i), whereWchild(i)hast/¯t= 2
sparse columns.
3. The columns of the leaf nodes’ Xiare|J|sparse.
4. Each class’ matrix Xicontainspcolumns, consisting of columns of S, i.e. pieces of x, in the leafs
and sums thereof in the interior nodes. All other entries are random (dependent between classes) or
zero.
Proof.We build a matrix Xaccording to (M1) - (M8) and use the extra matrix Win Corollary A.14 to
build a tree out of it. In the following, we denote by ¯pthe number of columns in Xand bypthe number
of columns in the class matrices Xithat we are going to construct. By assumption, the support of xis
partitioned into patches {J1,...,Jq}=Jfor which we define a corresponding partition K={K1,...,Kq}
of[¯p]with allKiof equal size and Zby
Zkl:=/braceleftbigg1k=kl
0else
for some choices kl∈Kl. The index sets JandKare naturally combined by their indices to obtain the
pairsJK. With these choices, the matrix Xis given by (M1) - (M8).
Xis non-zero only on blocks [J,K]∈JK, which allows us to build a tree, whose nodes we index by iin a
suitable index set Iwith leaf nodes i∈[q]. Each node iis associated with a subset Ki⊂[q]that is a union
of two children Ki=/uniontext
j∈child(i)Kj, starting with leaf nodes Ki∈K,i∈[q], e.g.
37Under review as submission to TMLR
{1,2,3,4}
{1,2}
{1}{2}{3,4}
{3}{4}
We now define matrices Xion each node, starting with the leafs
Xi:=X·Ki
for leafiand then inductively by joining the two child matrices
Xi:=/bracketleftbigXj1Xj2/bracketrightbig¯Wi, ¯Wi=1√
2/bracketleftbiggIKj1,Kj1
IKj2,Kj2/bracketrightbigg
forchild(i) ={j1,j2}and identity matrix I·,·on the given index sets. It is easy to join all ¯Wimatrices
leading up to node iinto a single isometry Wiso that
Xi=/bracketleftbigX1···Xq/bracketrightbig
Wi.
which implies
Xchild(i)=/bracketleftbigXj1Xj2/bracketrightbig
=/bracketleftbigX1···Xq/bracketrightbig
Wchild(i), W child(i)=/bracketleftbigWj1Wj2/bracketrightbig
,
where again Wchild(i)is an isometry because the columns of Wj1andWj2have non-overlapping support. By
Lemma 3.8 the tree has at most 2L+1nodes and thus, if
min
J∈J∥A·J∥2
F
∥A·J∥2≥2tC4
ψ
cϵ2κ(AS) log12e¯p
tϵ(28)
by Corollary A.14 and union bound over all tree nodes, with probability at least
1−42Lexp/parenleftigg
−c
2ϵ2
C4
ψ1
κ(AS)min
J∈J∥A·J∥2
F
∥A·J∥2/parenrightigg
all nodesXchild(i)satisfy thet-NSP. For this probability to be close to one, log 2Lmust be smaller than say
half the exponent
L≳log 2L≤−c
4ϵ2
C4
ψ1
κ(AS)min
J∈J∥A·J∥2
F
∥A·J∥2⇔ min
J∈J∥A·J∥2
F
∥A·J∥2≳tC4
ψ
ϵ2κ(AS) logs.
Combining this with the NSP condition (28), if
min
J∈J∥A·J∥2
F
∥A·J∥2≳tC4
ψ
ϵ2κ(AS)L+tC4
ψ
ϵ2κ(AS) log12e¯p
tϵ,
with probability at least
1−2 exp/parenleftigg
−c
2ϵ2
C4
ψ1
κ(AS)min
J∈J∥A·J∥2
F
∥A·J∥2/parenrightigg
all nodesXchild(i)satisfy the t-NSP. This yields the statements in the proposition if we choose ϵ∼1and
Cψ∼1, without loss of generality.
38Under review as submission to TMLR
Let us verify the remaining properties of learnable trees. By construction, we have t/¯t= 2andγ= 2and
¯p=qp. Since all random samples in Xare absolutely continuous with respect to the Lebesgue measure, the
probability of rank deficit Xiis zero. The remaining assumptions are given, with the exception of the first
two inequalities in (A1). Renaming the number of training samples q, whose name is already used otherwise
here, tor, they state that t≥clograndr > cp2log2pand thus imply that t≥logp2+ log3p, which is
sufficient since the number of training samples ris at the disposal of the teacher.
B Technical Supplements
Lemma B.1. LetR∈Rn×pbe a i.i.d. random matrix with mean zero entries of variance one. Then for
anyA∈Rm×nandu∈Rpwe have
E/bracketleftbig
∥ARu∥2/bracketrightbig
=∥A∥2
F∥u∥2.
Proof.Since E[RikRjl] =δijδkl, we have
E/bracketleftbig
∥ARu∥2/bracketrightbig
=E[⟨ARu,ARu⟩]
=E
/summationdisplay
ijklukRik(ATA)ijRjlul

=/summationdisplay
ijkl(ATA)ijukulE[RikRjl]
=/summationdisplay
ik(ATA)iiukuk
=∥A∥2
F∥u∥2.
Lemma B.2. LetA∈Rm×nbe a matrix, b∈Rmbe a vector and x∈Rna i.i.d. random vector with
E[xj] = 0,E/bracketleftbig
x2
j/bracketrightbig
= 1. Then
E/bracketleftig
∥Ax+b∥2/bracketrightig
=∥A∥2
F+∥b∥2.
Proof.Sincebis not random, we have
E/bracketleftig
∥Ax+b∥2/bracketrightig
=E/bracketleftig
∥Ax∥2/bracketrightig
+∥b∥2=∥A∥2
F+∥b∥2,
where in the last equality we have used Lemma B.1 with Rn×1matrixR=xandu= [1]∈R1.
The following result is a slight variation of Vershynin (2018), Theorem 6.3.2.
Lemma B.3. LetA∈Rm×nbe a matrix, b∈Rmbe a vector and x∈Rna i.i.d. random vector with
E[xj] = 0,E/bracketleftbig
x2
j/bracketrightbig
= 1and∥x∥ψ2≤Cψ. Then
Pr/bracketleftig/vextendsingle/vextendsingle/vextendsingle∥Ax+b∥2−∥A∥2
F−∥b∥2/vextendsingle/vextendsingle/vextendsingle≥ϵ/parenleftig
∥A∥2
F+∥b∥2/parenrightig/bracketrightig
≤8 exp/bracketleftigg
−cmin(ϵ2,ϵ)∥A∥2
F+∥b∥2
C4
ψ∥A∥2/bracketrightigg
.
39Under review as submission to TMLR
Proof.We decompose
∥Ax+b∥2−∥A∥2
F−∥b∥2=∥Ax∥2+ 2⟨Ax,b⟩+∥b∥2−∥A∥2
F−∥b∥2
=/parenleftig
∥Ax∥2−∥A∥2
F/parenrightig
+ 2⟨Ax,b⟩
so that
Pr/bracketleftig
±/parenleftig
∥Ax+b∥2−∥A∥2
F−∥b∥2/parenrightig
≥ϵ/parenleftig
∥A∥2
F+∥b∥2/parenrightig/bracketrightig
≤Pr/bracketleftig
±/parenleftig
∥Ax∥2−∥A∥2
F/parenrightig
±2⟨Ax,b⟩≥ϵ/parenleftig
∥A∥2
F+∥b∥2/parenrightig/bracketrightig
≤Pr/bracketleftig
±/parenleftig
∥Ax∥2−∥A∥2
F/parenrightig
≥ϵ∥A∥2
F/bracketrightig
+ Pr/bracketleftig
±2⟨Ax,b⟩≥ϵ∥b∥2/bracketrightig
.
It remains to estimate the two probabilities on the right hand side. Since E/bracketleftbig
x2
j/bracketrightbig
= 1, we haveCψ≳1and
thus from the proof of Theorem 6.3.2in Vershynin (2018), we have
Pr/bracketleftbig
±/parenleftbig
∥Ax∥2−∥A∥2
F/parenrightbig
≥ϵ∥A∥2
F/bracketrightbig
≤2 exp/bracketleftigg
−cmin(ϵ2,ϵ)∥A∥2
F
C4
ψ∥A∥2/bracketrightigg
and from Hoeffding’s inequality, we have
Pr/bracketleftbig
±2⟨Ax,b⟩≥ϵ∥b∥2/bracketrightbig
≤2 exp/bracketleftigg
−cϵ2∥b∥4
C2
ψ∥ATb∥2/bracketrightigg
≤2 exp/bracketleftigg
−cϵ2∥b∥2
C4
ψ∥AT∥2/bracketrightigg
.
The following result is a slight variation of Vershynin (2018), Theorem 6.3.2.
Lemma B.4. LetA∈Rm×nbe a matrix, b∈Rmbe a vector and x∈Rna i.i.d. random vector with
E[xj] = 0,E/bracketleftbig
x2
j/bracketrightbig
= 1and∥x∥ψ2≤Cψ. Then
/vextenddouble/vextenddouble/vextenddouble/vextenddouble∥Ax+b∥−/parenleftig
∥A∥2
F+∥b∥2/parenrightig1/2/vextenddouble/vextenddouble/vextenddouble/vextenddouble
ψ2≤CC2
ψ∥A∥
for some constant C≥0.
Proof.Weuseastandardargument,e.g. fromtheproofofTheorem 6.3.2inVershynin(2018). Anelementary
computation shows that for δ2= min(ϵ2,ϵ)and anya,b∈R, we have
|a−b|≥δb,⇒ |a2−b2|≥ϵb2.
Witha=∥Ax+b∥andb=/parenleftig
∥A∥2
F+∥b∥2/parenrightig1/2
and Lemma B.3, this implies
Pr/bracketleftbigg/vextendsingle/vextendsingle/vextendsingle/vextendsingle∥Ax+b∥−/parenleftig
∥A∥2
F−∥b∥2/parenrightig1/2/vextendsingle/vextendsingle/vextendsingle/vextendsingle≥δ/parenleftig
∥A∥2
F+∥b∥2/parenrightig1/2/bracketrightbigg
≤8 exp/bracketleftigg
−cδ2∥A∥2
F+∥b∥2
C4
ψ∥A∥2/bracketrightigg
.
This shows Subgaussian concentration and thus the ψ2-norm of the lemma.
40Under review as submission to TMLR
C Implementation Details
Details for Curriculum I in Section 5.3.1
Proof of Lemma 5.3. The curriculum satisfies (M1) – (M8) with the index sets
/bracketleftig
1,...,|J|/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
J1, ... ,n−|J|,...,n/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
Jq/bracketrightig
,/bracketleftig
1,...,|K|/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
K1, ... ,p−|K|,...,p/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
Kq/bracketrightig
andZ=/bracketleftbige1e|K|+1e2|K|+1.../bracketrightbig
with unit basis vectors ekfor the first index in each block Ki. Hence,
it is a special case of the construction in the proof of Theorem 4.2 and all conclusions of the theorem are
applicable.
Details for the implementation in Section 5.4:
1. The teacher provides a left preconditioned matrix TAin every tree node. This allows RIP instead of
weaker NSP conditions, as in Corollary A.13 versus Corollary A.14. For Curriculum II Tis uniform
for all tree nodes, for Curriculum III, it is computed individually for each node.
2. Unlike (21) in the split X:=SZT+DR(I−ZZT)between deterministic and random part, we use
no balancing Din the experiments.
3. As a result, all tree node Xihave entries in{−1,0,1}so that we implement Scaleby snapping to
these discrete values.
D Glossary
Algorithms
Solve (A,b) ℓ0minimizer for easy problems, Section 2.2.
SparseFactor (Y)Sparse matrix factorization Y=XZ, Section 2.2.
Scale Rescaling after matrix factorization, Section 2.2, Definition 3.4.
Train(A,b1,...,bq) Find class Xfrom samples, Algorithm 1.
SolveL ℓ0minimizer for easy problems in leaf nodes, Definition 3.4.
TreeTrain (Ci) Find Xifor all tree nodes ifrom samples, Algorithm 2.
Dimensions
A∈Rm×n
X∈Rn×p
Z∈Rp×q
Sparsities
s Sparsity of the columns of X.
t (Expected) Sparsity of the columns of Zfor problems class C.
¯t (Expected) Sparsity of the columns of Zfor easy problems in class Ceasy⊂C.
Tree
I Indices of tree nodes, Section 3.1.
child(i) Children of node i, Section 3.1.
Wchild(i) (13).
Xchild(i) (13).
41