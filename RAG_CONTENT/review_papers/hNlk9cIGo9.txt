Faster Algorithms for User-Level Private Stochastic
Convex Optimization∗
Andrew Lowy
Wisconsin Institute for Discovery
University of Wisconsin-Madison
alowy@wisc.eduDaogao Liu
Department of Computer Science
University of Washington
liudaogao@gmail.com
Hilal Asi
Apple Machine Learning Research
hilal.asi94@gmail.com
Abstract
We study private stochastic convex optimization (SCO) under user-level differential
privacy (DP) constraints. In this setting, there are nusers (e.g., cell phones), each
possessing mdata items (e.g., text messages), and we need to protect the privacy
of each user’s entire collection of data items. Existing algorithms for user-level DP
SCO are impractical in many large-scale machine learning scenarios because: (i)
they make restrictive assumptions on the smoothness parameter of the loss function
and require the number of users to grow polynomially with the dimension of the
parameter space; or (ii) they are prohibitively slow, requiring at least (mn)3/2
gradient computations for smooth losses and (mn)3computations for non-smooth
losses. To address these limitations, we provide novel user-level DP algorithms with
state-of-the-art excess risk and runtime guarantees, without stringent assumptions.
First, we develop a linear-time algorithm with state-of-the-art excess risk (for a
non-trivial linear-time algorithm) under a mild smoothness assumption. Our second
algorithm applies to arbitrary smooth losses and achieves optimal excess risk in
≈(mn)9/8gradient computations. Third, for non-smooth loss functions, we obtain
optimal excess risk inn11/8m5/4gradient computations. Moreover, our algorithms
do not require the number of users to grow polynomially with the dimension.
1 Introduction
The increasing ubiquity of machine learning (ML) systems in industry and society has sparked serious
concerns about the privacy of the personal data used to train these systems. Much work has shown
that ML models may violate individuals’ privacy by leaking their sensitive training data [ SSSS17 ,
LLL+24a,LLL+24b]. For instance, large language models (LLMs) are vulnerable to black-box
attacks that extract individual training examples [ CTW+21].Differential privacy (DP) [ DMNS06 ]
prevents ML models from leaking their training data.
The classical definition of differential privacy— item-level differential privacy [DMNS06 ]—is ill-
suited for many modern applications. Item-level DP ensures that the inclusion or exclusion of any
one training example has a negligible impact on the model’s outputs. If each person (a.k.a. user)
contributes only one piece of training data , then item-level DP provides a strong guarantee that each
user’s data cannot be leaked. However, in many modern ML applications, such as training LLMs on
users’ data in federated learning, each user contributes a large number of training examples [XZ24].
∗Authors are listed reverse alphabetically.
38th Conference on Neural Information Processing Systems (NeurIPS 2024).In such scenarios, the privacy protection that item-level DP provides for each user is insufficiently
weak.
User-level differential privacy is a stronger privacy notion that addresses the above shortcoming of
item-level DP. Informally, user-level DP ensures that the inclusion or exclusion of any one user’s
entire training data (msamples) has a negligible impact on the model’s outputs. Thus, user-level
DP provides a strong guarantee that no user’s data can be leaked, even when users contribute many
training examples.
A fundamental problem in (private) machine learning is stochastic convex optimization (SCO): given
a data set D= (Z1, . . . , Z n)from ni.i.d. users, each possessing mi.i.d. samples from an unknown
distribution Zi∼Pmour goal is to approximately minimize the expected population loss
F(x) :=Ez∼P[f(x, z)].
Here, f:X × Z → Ris a loss function (e.g., cross-entropy loss), X ⊂Rdis the parameter domain,
andZis the data universe. We require that the output of the optimization algorithm A:Zmn→ X
satisfies user-level DP (Definition 1.3). We measure the accuracy of Aby its excess (population) risk
EF(A(D))−F∗:=EA,D∼PnmF(A(D))−min
x∈XF(x).
Given the practical importance of user-level DP SCO, it is unsurprising that many prior works have
studied this problem. The work of [ LSA+21] initiated this line of work, and provided an excess risk
lower bound of Ω(1/√nm+√
d/(εn√m)), where εis the privacy parameter. However, their upper
bound was suboptimal and required strong assumptions. The work of [ BS23 ] gave an algorithm
that achieves optimal risk for β-smooth losses with β < (n/√
md∧n3/2/(d√m)), provided
thatn≥√
d/εandm≤max(√
d, nε2/√
d).These assumptions are restrictive in large-scale
applications with a large number of examples per user mor when the number of model parameters d
is large. For example, in deep learning, we often have d≫nand an enormous smoothness parameter
β≫1. Moreover, their algorithm requires mn3/2gradient evaluations, making it slow when the
number of users nis large.2The work of [ GKK+23] gave another user-level DP algorithm that only
requires n≥log(d)/ε, but unfortunately their algorithm does not run in polynomial-time.
To address the deficiencies of previous works on user-level DP SCO, the recent work [ AL24 ]
provided an algorithm that achieves optimal excess risk in polynomial-time, while also only requiring
n≥log(md)/εusers. Moreover, their algorithm also works for non-smooth losses. The drawback
of [AL24 ] is that it is even slower than the algorithm of [ BS23 ]: for β-smooth losses, their algorithm
requires β·(nm)3/2gradient evaluations; for non-smooth losses, their algorithm requires (nm)3
evaluations.
Evidently, the runtime requirements and parameter restrictions of existing algorithms for user-level
DP SCO are prohibitive in many important ML applications . Thus, an important question is:
Question 1. Can we develop faster user-level DP
algorithms that achieve optimal excess risk without
restrictive assumptions ?
Contribution 1. We give a positive answer to Question 1, providing a novel algorithm that achieves
optimal excess risk using max{β1/4(nm)9/8, β1/2n1/4m5/4}gradient computations for β-smooth
loss functions, with any β <∞(Theorem 3.2). For non-smooth loss functions, our algorithm
achieves optimal excess risk using n11/8m5/4gradient evaluations for non-smooth loss functions
(Theorem 4.1). Our runtime bounds dominate those of all prior works in every applicable parameter
regime, by polynomial factors in n, m , andd. Moreover, our results only require n1−o(1)≥log(d)/ε
users. See Table 1 for a comparison of our results vs. prior works. For example, for non-smooth loss
functions, our optimal algorithm is faster than the previous state-of-the-art [ AL24 ] by a multiplicative
factor of n13/8m7/4.For smooth loss functions, our optimal algorithm is faster than [ AL24 ] by a
factor of (nm)3/8β3/4(in the typical parameter regime when n7≥m).
2In the introduction, whenever εdoes not appear, we are assuming ε= 1to ease readability. For runtime
bounds, we also assume n=dto further simplify.
2Loss FunctionReferenceGradient complexityAssumptions  -Smooth [BS23]  &[AL24]NoneOur Algorithm 3NoneNon-Smooth[AL24]        NoneOur Algorithm 3 (smoothed)Noneββ⋅(mn)3/2mn3/2β1/4⋅(mn)9/8+β1/2n1/4m5/4(mn)3n11/8m5/4β≤n/mm≤d≤nFigure 1: Optimal algorithms for user-level DP SCO. We omit logarithms, fix L=R= 1 = εandn=d.
Linear-Time Algorithms The “holy grail” of DP SCO is a linear-time algorithm with optimal
excess risk, which is unimprovable both in terms of runtime and accuracy. In the item-level DP
setting, such algorithms are known to exist for smooth loss functions [ FKT20 ,ZTOH22 ]. [AL24 ]
posed an interesting open question: is there a user-level DP algorithm that achieves optimal excess
risk in linear time for smooth functions? For our second contribution, we make progress towards
answering this question.
Existing techniques for user-level DP SCO are not well-suited for linear-time algorithms. Indeed,
the only prior non-trivial linear-time algorithm is the user-level LDP algorithm of [ BS23 , Algorithm
5].3Their algorithm can achieve excess risk ≈1/√nmε +√
d/(√nmε). Unfortunately, however,
their algorithm requires a very stringent assumption on the smoothness parameter β <p
n3/(md3),
which is unlikely to hold for large-scale ML problems. Further, the result of [ BS23 ] requires the
number of users queried in each round to grow polynomially with the dimension d, and it assumes
m < d < n .These assumptions severely limit the applicability of [ BS23 , Algorithm 5] in practical
ML scenarios . This leads us to:
Question 2. Can we develop a linear-time user-
level DP algorithm with state-of-the-art excess risk,
without restrictive assumptions ?
Contribution 2. We answer Question 2 affirmatively in Theorem 2.1: under a very mild requirement
on the smoothness parameter β <√
nmd , our novel linear-time algorithm achieves excess risk of
≈1/√nmε +√
d/(√nmε). Moreover, our algorithm does not require the number of users to grow
polynomially in the dimension d, and our result holds for any values of m, d, andn. Thus, our
algorithm has excess risk matching that of [BS23], but is much more widely applicable.
1.1 Techniques
We develop novel techniques and algorithms to achieve new state-of-the-art results in user-level DP
SCO. Before discussing our techniques, let us review the key ideas from prior works that we build on.
The goal of prior works [ BS23 ,AL24 ] was to develop user-level analogs of DP-SGD [ BFTT19 ],
which is optimal in the item-level setting. To do so, they observed that each user i’s gradient
1
mPm
j=1∇f(x, Zi,j)lies in a ball of radius ≈1/√maround the population gradient ∇F(x)with
high probability, if the data is i.i.d ( Zi∼Pm). Consequently, if the data is i.i.d., then replacing
one user Zi∈ D by another user Z′
i∈ D′will not change the empirical gradient ∇FD(x)by
too much: ∥∇FD(x)− ∇FD′(x)∥≲1/(n√m)with high probability. Thus, one would hope
for a method to privatize ∇FD(x)by adding noise that scales with 1/(n√m)—rather than 1/n—
which would allow for optimal excess risk. [ AL24 ] devised such a method, which was inspired by
FriendlyCore [ TCK+22]. Their method privately detects and removes “outlier” user gradients, and
then adds noise to the average of the “inlier” user gradients. This outlier-removal procedure ensures
privacy with noise scaling with 1/(n√m), provided n≳1/ε. Moreover, when the data is i.i.d., no
3It is trivial to achieve excess risk ≈1/√nm +√
d/(εn)with (ε, δ)-user-level, e.g. by applying group
privacy to an optimal item-level DP algorithm such as [ FKT20 ]. The error due to privacy in this bound does not
decrease with m.
3outliers will be removed with high probability, leading to a nearly unbiased estimator of the empirical
gradient.
Our algorithms apply variations of the outlier-removal idea of [AL24] in novel ways.
Our linear-time Algorithm 1 takes a different approach to outlier removal, compared to prior works.
Instead of removing outlier gradients , we aim to detect and remove outlier SGD iterates .4The
high-level idea of our algorithm is to partition the nusers into C≈1/εgroups, with each group
containing ≈nεusers. For each group of users, we run T≈mnε steps of online SGD using the
samples in this group and obtain the average iterate of each group: {˜xj}C
j=1. We then privately
identify and remove the outlier iterates from{˜xj}C
j=1. In order to successfully do so, we need to
argue that if we run online SGD independently on user Zand user Z′to obtain ˜xand˜x′respectively,
then∥˜x−˜x′∥≲η√
Twith high probability, where ηis the SGD step size. We prove such a stability
bound in Lemma 2.3, which we hope will be of independent interest. By repeating the above process
log(n)times and using iterative localization [FKT20 ], we obtain our state-of-the-art linear-time
result.
Our second algorithm, Algorithm 3, builds on [ AL24 ] in a different way. In Algorithm 3, we
apply an outlier-removal procedure to users’ gradients. However, unlike [ AL24 ], we draw random
minibatches of users in each iteration and apply outlier-removal to these minibatches. To make this
procedure private while also achieving optimal excess risk, we combine AboveThreshold [DR14 ] with
privacy amplification by subsampling [BBG18 ]. We then develop an accelerated [GL12 ] user-level
DP algorithm that solves a carefully chosen sequence of regularized ERM problems, and applies
localization in the spirit of [ KLL21 ,AFKT21 ]. An obstacle that arises when we try to extend the
ERM-based localization framework to the user-level DP setting is getting a tight bound on the
variance of our minibatch stochastic gradient estimator that scales with 1/m. We overcome this
obstacle in Lemma 3.5, by appealing to the stability of user-level DP [BS23 ]. To handle non-smooth
loss functions, we apply randomized smoothing to our accelerated algorithm.
1.2 Preliminaries
We consider loss functions f:X × Z → R, where Xis a convex parameter domain and Zis a data
universe. Let Pbe an unknown data distribution and F(x) :=Ez∼P[f(x, z)]be the population loss
function. Denote F∗:= min x∈XF(x). The SCO problem is minx∈XF(x). Let∥ · ∥ denote the ℓ2
norm. ΠX(u) := argminx∈X∥u−x∥2denotes projection onto X.
Assumptions and Notation. Function g:X →RisL-Lipschitz if|g(x)−g(x′)| ≤L∥x−x′∥2
for all x, x′∈ X . Function g:X → Risβ-smooth ifgis differentiable and has β-Lipschitz
gradient: ∥∇g(x)− ∇g(x′)∥2≤β∥x−x′∥2. Function g:X → Risµ-strongly convex if
g(αx+ (1−α)x′)≤αg(x) + (1−α)g(x′)−α(1−α)µ
2∥x−x′∥2for all α∈[0,1]and all x, x′∈ X.
Ifµ= 0,we say gisconvex .
Assumption 1.1. 1. The convex set Xis compact with ∥x−x′∥ ≤Rfor all x, x′∈ X.
2. The loss function f(·, z)isL-Lipschitz and convex for all z∈ Z.
In all of the paper except for Section 4 , we will also assume:
Assumption 1.2. The loss function f(·, z)isβ-smooth for all z∈ Z.
Denote a∧b:= min( a, b). For functions fandgof input parameters θ, we write f≲gif there is an
absolute constant C >0such that f(θ)≤Cg(θ)for all permissible values of θ. We use eOto hide
logarithmic factors. Write a≤poly(b)if there exists some large J >1for which a≤bJ.
Differential Privacy.
Definition 1.3 (User-Level Differential Privacy) .Letε≥0, δ∈[0,1).Randomized algorithm A:
Znm→ X is(ε, δ)-user-level differentially private (DP) if for any two datasets D= (Z1, . . . , Z n)
andD′= (Z′
1, . . . , Z′
n)that differ in one user’s data (say Zi̸=Z′
ibutZj=Z′
jforj̸=i), we have
P(A(D)∈S)≤eεP(A(D′)∈S) +δ,
4The reason that this innovation is necessary is discussed in the last paragraph of Section 2.
4for all measurable subsets S⊂ X .
Definition 1.3 prevents any adversary from learning much more about an individual’s data set than if
that data had not been used for training. Appendix A contains the necessary background on DP.
1.3 Roadmap
We begin with our state-of-the-art linear-time algorithm in Section 2. In Section 3, we present our
error-optimal algorithm with state-of-the-art runtime for smooth loss functions. Section 4 extends our
fast optimal algorithm to non-smooth loss functions. We conclude in Section 5 with a discussion and
guidance on future research directions stemming from our work.
2 A state-of-the-art linear-time algorithm for user-level DP SCO
In this section, we develop a new algorithm (Algorithm 1) for user-level DP SCO that runs in
linear time and has state-of-the-art excess risk, without requiring any impractical assumptions. The
algorithm can be seen as a user-level DP variation of the localized phased SGD of [ FKT20 ]: we
execute a sequence of SGD trajectories with geometrically decaying step sizes, shrinking both the
expected distance to the population minimizer and the privacy noise over a logarithmic number of
phases.
In each phase i, we first re-set algorithmic parameters and draw a disjoint set of niusers Di⊂ D
(lines 4-5). We further partition DiintoCdisjoint subsets {Di,j}C
j=1. For each j∈[C], we pool
together all of the nimsamples in Di,jand run one-pass online SGD on Di,jwith initial point xi−1
given to us from the previous phase. Next, in lines 10-20, we privately detect and remove “outliers”
from{˜xi,j}C
j=1. That is, our goal is to privately select a subset Si⊂ {˜xi,j}C
j=1, such that for any two
points ˜xi,j,˜xi,j′∈ Si,∥˜xi,j−˜xi,j′∥ ≤τi=eO(ηiL√Ti). This will enable us to add noise scaling
withτiin line 22, rather than with the much larger worst-case sensitivity (that scales linearly with Ti).
In order to privately select such a subset Si, we first compute (and privatize) the concentration score
for{˜xi,j}C
j=1in line 10. A small concentration score indicates that outlier removal is doomed to fail
and we must halt the algorithm to avoid breaching the privacy constraint. A large concentration score
indicates that {˜xi,j}C
j=1is nearly τi-concentrated and we may proceed with outlier removal in lines
12-15.
Theorem 2.1 (Privacy and utility of Algorithm 1 - Informal) .Letε≤10,n1−o(1)≳log(n/δ)
ε,
β≤(L/R)√
dmnε , and m≲poly(n). Then, Algorithm 1 is (ε, δ)-user-level DP . Further,
EF(xl)−F∗≤LR·eO 
1√nmε+p
dlog(1/δ)√nε√m!
.
The gradient complexity of Algorithm 1 is ≤nm.
Remark 2.2 (State-of-the-art excess risk in linear time, without the restrictive assumptions) .Under the
assumptions that β <(Lε3/R)p
n3/md3andm≤d/ε2≤n, [BS23 ] gave a linear-time algorithm
with similar excess risk to Algorithm 1. However, their assumptions are very restrictive in practice:
For example, in the canonical regime n≈d, their assumption on βrules out essentially every
(non-linear) loss function. By contrast, our result holds even if the smoothness parameter is huge
(β≈√
nmd ) and we only require a logarithmic number of users. Thus, our algorithm and result is
applicable to many practical ML problems.
To prove that Algorithm 1 is private, we essentially argue that for any phase i, theℓ2-sensitivity of
˜xiis upper bounded by eO(τi/C)with probability at least 1−δ/2. The argument goes roughly as
follows: First, the Laplace noise added to si(τi)ensures that si(τi)isε/4-user-level DP. Now, it
suffices to assume bsi(τi)≥4C/5, since otherwise the algorithm halts and outputs 0independently
of the data. Next, conditional on the high probability event that the Laplace noise is smaller than
eO(1/ε), we know that bsi(τi)≥4C/5 =⇒si(τi)≥2C/3with high probability by our choice of
C. In this case, an argument along the lines of [ AL24 , Lemma 3.5] shows that ˜xihas sensitivity
bounded by eO(τi/C)with probability at least 1−δ/2. See Appendix B for the detailed proof.
To prove the excess risk bound in Algorithm 1, the key step is to show that if the data is i.i.d.,
then with high probability, no points are removed from {˜xi,j}C
j=1during the outlier-removal phase
5Algorithm 1: User-Level DP Phased SGD with Outlier Iterate Removal and Output Perturbation
1Input: Dataset D= (Z1, . . . , Z n), privacy parameters (ε, δ), parameters p, q > 0, stepsize η;
2Setl=⌊log2(n)⌋,C:= 100 log(20 nmeε/δ)/ε;
3fori= 1,···, ldo
4 Setni= (1−(1/2)q)n/2iq,ηi=η/2pi,Ni=ni/C,Ti=Nim,
τi= 1000 ηiL√Tilog(ndm);
5 Draw disjoint users Diof size nifromD;
6 Divide DiintoCdisjoint subsets {Di,j}C
j=1, each containing |Di,j|=Niusers;
7 forj= 1,···, Cdo
8 ˜xi,j←SGD (Di,j, ηi, Ti, xi−1) =average iterate of Tisteps of one-pass projected SGD
with data Di,j, stepsize ηi, and initial point xi−1;
9 end
10 Compute the concentration score for Di:
si(τi) :=1
CX
j,j′∈[C]1(∥˜xi,j−˜xi,j′∥ ≤τi)
Letbsi(τi) =si(τi) + Lap(20 /ε);
11 ifbsi(τi)≥4C
5then
12 Si=∅;
13 forj= 1,···, Cdo
14 Compute the score function of ˜xi,j:hi,j=PC
j′=11(∥˜xi,j−˜xi,j′∥ ≤2τi);
15 Add˜xi,jtoSiwith probability pi,jforpi,j=

0 hi,j< C/ 2
1 hi,j≥2C/3
hi,j−C/2
C/6o.w.
16 end
17 end
18 else
19 Halt; Output 0
20 end
21 Let˜xi=1
|Si|P
˜xi,j∈Si˜xi,j;
22 xi←˜xi+ζi, where ζi∼ N(0, σ2
iId)withσi=100τilog2(n/δ)
εC;
23end
24Output: xl.
(i.e.|Si|=C). If|Si|=Cholds, then we can use the convergence guarantees of SGD and the
localization arguments in [ FKT20 ] to establish the excess risk guarantee. In order to prove that
|Si|=Cwith high probability, we need the following novel stability lemma:
Lemma 2.3. Assume f(·, z)is convex, L-Lipschitz, and β-smooth on Xwithη≤1/β. Let
˜x←SGD (D, η, T, x 0)and˜y←SGD (D′, η, T, x 0)be two independent runs of projected SGD,
where D, D′∼PNare i.i.d. Then, with probability at least 1−ζ, we have
∥˜x−˜y∥≲ηLp
Tlog(dT/ζ ).
We prove Lemma 2.3 via induction on t, using non-expansiveness of gradient descent on smooth
losses [HRS16], subgaussian concentration bounds, and a union bound.
Lemma 2.3 implies that if the data is i.i.d., then the following events hold with high probability:
∥˜xi,j−˜xi,j′∥ ≤τifor all j, j′∈[Ci]and hence si(τi) =C. Further, conditional on si(τi) =C,
we know that bsi(τi)≥4C/5with high probability, so that the algorithm does not halt. Also,
∥˜xi,j−˜xi,j′∥ ≤τifor all j, j′implies pi,j= 1for all jand hence |Si|=Cfor all j. The detailed
excess risk proof is provided in Appendix B.
Challenges of getting optimal excess risk in linear time: In the item-level DP setting, there
are several (nearly) linear time algorithms that achieve optimal excess risk for smooth DP SCO
6under mild smoothness conditions, such as snowball SGD [ FKT20 ], phased SGD [ FKT20 ], and
phased ERM with output perturbation [ ZTOH22 ]. Extending these approaches into optimal nearly
linear-time user-level DP algorithms is challenging. First, the user-level DP implementation of output
perturbation in [ GKK+23] is computationally inefficient. Second, snowball SGD relies on privacy
amplification by iteration , which does not extend nicely to the user-level DP case due to instability of
the outlier-detection procedure in [ AL24 ]. Specifically, since amplification by iteration intermediate
only provides DP for the last iterate xTbut not the intermediate iterates xt(t < T ), the sensitivity of
the concentration score function is not O(1), which impairs DP outlier-detection. A similar instability
issue arises if one tries to naively extend phased SGD to be user-level DP by applying [ AL24 ] to user
gradients. This issue motivates our Algorithm 1, which extends phased SGD in an alternative way:
by applying outlier-detection/removal to the SGD iterates instead of the gradients, we can control the
sensitivity of the concentration score and thus prove that our algorithm is DP. However, since the
bound in Lemma 2.3 scales polynomially with T(and we believe this dependence on Tis necessary),
Algorithm 1 adds excessive noise and has suboptimal excess risk. We believe that obtaining optimal
risk in linear time will require a fundamentally different user-level DP mean estimation procedure
that does not suffer from the instability issue.
3An optimal algorithm with ≈(mn )9/8gradient complexity for smooth losses
In this section, we provide an algorithm that achieves optimal excess risk using ≈(mn)9/8stochastic
gradient evaluations. Our Algorithm 3 is inspired by the item-level accelerated phased ERM algorithm
of [KLL21 ]. It applies iterative localization [ FKT20 ] to the user-level DP accelerated minibatch
SGD Algorithm 2. Algorithm 2 is a user-level DP variation of the accelerated minibatch SGD
of [GL12, GL13].
Our Algorithm 2 applies a DP outlier-removal procedure to the users’ gradients in each iteration. We
useAbove Threshold [DR14 ] to privatize the concentration scores s(t)
iand determine whether or not
most of the gradients of users in minibatch Dt
iare2τ-close to each other. If bst
i≥b∆i, indicating that
the gradients of users in Dt
iare nearly 2τ-concentrated, then we proceed with outlier removal in lines
8-12. We then invoke privacy amplification by subsampling [BBG18 ] and the advanced composition
theorem [KOV15 ] to privatize the average of the “inlier” gradients with additive Gaussian noise. By
properly choosing algorithmic parameters, we obtain the following results, proved in Appendix C:
Theorem 3.1 (Privacy of Algorithm 3) .Letε≤10,q >0such that n1−q>100 log(20 nmdeε/δ)
ε(1−(1/2)q).
Then, Algorithm 3 is (ε, δ)-DP .
Theorem 3.2 (Utility & runtime of Algorithm 3 - Informal) .Letε≤10andδ <1/(mn). Then,
Algorithm 3 yields optimal excess risk:
EF(xl)−F∗≤LR·eO 
1√mn+p
dlog(1/δ)
εn√m!
.
The gradient complexity of this algorithm is upper bounded by
mn 
1 +εβR
L1/4 
(mn)1/8∧ε2n2m
d1/8!!
+r
βR
Ln1/4m5/4
ε+n1/2m5/4
d1/4ε1/2
.
Ifn=d,ε= 1, and βR=Lthen the gradient complexity bound in Theorem 3.2 simplifies to
(mn)9/8+n1/4m5/4. Typically, n7≥m, so that the dominant term in this bound is (mn)9/8.
Remark 3.3 (State-of-the-art runtime) .The gradient complexity bound in Theorem 3.2 is superior
to the runtime bounds of all existing near-optimal algorithms by polynomial factors inn, m , and
d[BS23 ,GKK+23,AL24 ]. Note that while the mn3/2gradient complexity bound of [ BS23 ] may
appear to be better than β1/4(nm)9/8in certain parameter regimes (e.g. m > n3orβ≫nm), this
is not the case: the result of [BS23] requires m < n andβ <p
n/m .
Remark 3.4 (Mild assumptions) .Note that Theorems 3.1 and 3.2 do not require any bound on the
smoothness parameter β, and only require the number of users to grow logarithmically: n1−o(1)≥
eΩ(1/ε). Contrast this with the results of previous works (e.g. [BS23]).
7Algorithm 2: User-Level DP Accelerated Minibatch SGD (bFi, Ti, Ki, xi−1, τ, ε, δ )
1Initialize x1
i−1←xi−1;
2fort= 1,···, Tido
3 Draw Kirandom users Dt
i={Zt
i,j}Ki
j=1from Diuniformly with replacement;
4 Set noisy threshold b∆i:=4Ki
5+ξi, where ξi∼Lap 8
ε
;
5 Letqt(Z) :=1
mP
z∈Z∇f(xt
i−1, z)for user Z;
6 Compute the concentration score of Dt
i:
st
i(τ) :=1
KiX
Z,Z′∈Dt
i1(∥qt(Z)−qt(Z′)∥ ≤2τ)
Letbst
i(τ) =st
i(τi) +vt
i, where vt
i∼Lap 16
ε
;
7 ifbst
i(τ)≥b∆ithen
8 St
i=∅;
9 forEach User Z∈Dt
ido
10 Setht
i(Z) =P
Z′∈Dt
i1(∥qt(Z)−qt(Z′)∥ ≤2τ);
11 AddZtoSt
iwith probability pt
i(Z) :=

0 ht
i(Z)< K i/2
1 ht
i(Z)≥2Ki/3
ht
i(Z)−Ki/2
Ki/6o.w.
12 end
13 gt
i=1
|St
i|P
Z∈St
i∇bF(xt
i−1, Z);
14 bgt
i=gt
i+ζt
i, where ζt
i∼ N(0, σ2
i)withσi=1000τ√Tilog(ndeε/δ)
εni;
15 Do1iteration of Accelerated Minibatch SGD (AC-SA) [GL12] on bFi, using gradient
estimator bgt
i+λi(xt
i−1−xi−1)to obtain xt+1
i−1.
16 end
17 else
18 Halt; Return 0
19 end
20end
21Output xTi
i−1.
Algorithm 3: User-Level DP Accelerated Phased ERM with Outlier Gradient Removal
1Input: Dataset D= (Z1, . . . , Z n), privacy parameters (ε, δ), parameters p, q, λ > 0;
2Setl=⌊log2(n)⌋andτ=O(Llog(ndm)/√m), choose any initial point x0∈ X;
3fori= 1,···, ldo
4 Setni= (1−(1/2)q)n/2iq,λi=λ·2pi,Ti=eΘ(1 +p
β/λi),
Ki= 500 log( n2
im2eε/δ)
1
ε+niε√
Tilog(1/δ)
;
5 Draw disjoint users Diof size nifromD;
6 LetbFi(x) :=1
niP
Zi,j∈DibF(x, Zi,j)+λi
2∥x−xi−1∥2, where bF(x, Zi,j)is user Zi,j’s
empirical loss;
7 xi←User-Level DP Accelerated Minibatch SGD (bFi, Ti, Ki, xi−1, τ, ε, δ ). ;
8end
9Output xl.
8A challenge in proving Theorem 3.2 is getting a tight bound on the variance of the the noisy minibatch
stochastic gradients bgt
ithat are used in Algorithm 2 (lines 12-14). Conditional on St
i=Dt
i, it is easy
to obtain a variance bound of the form E∥bgt
i− ∇bFi(xt
i)∥2≲dσ2
i+L2
Ki, since we are sampling Ki
users uniformly at random. However, this bound is too weak to obtain Theorem 3.2, since it does not
scale with m. To prove Theorem 3.2, we need the following stronger result:
Lemma 3.5 (Variance Bound for Algorithm 2) .Letδ≤1/(nm), ε≲1. Denote eFi(x) :=
1
niP
Zi,j∈DibF(x, Zi,j). Then, conditional on St
i=Dt
ifor all i∈[l], t∈[Ti], we have
E∥gt
i− ∇eFi(xt
i−1)∥2≲L2log(ndm)
Km
for all i∈[l], t∈[Ti], where the expectation is over both the random i.i.d. draw of D=
(Z1, . . . , Z n)∼Pnmand the randomness in Algorithm 3.
The difficulty in proving Lemma 3.5 comes from the fact that the iterates xt
iand the data Dare not
independent. To overcome this difficulty, we use the stability of user-level DP [BS23 ] to argue that
for all Z∈Di,∇bF(xt
i−1, Z)is≈L/√m-close to ∇F(xt
i−1)with high probability, since xt
i−1is
user-level DP. A detailed proof is given in Appendix C.
Remark 3.6 (Strongly convex losses: Optimal excess risk with state-of-the-art runtime) .Iff(·, z)is
µ-strongly convex, then Algorithm 3 can be combined with the meta-algorithm of [ FKT20 , Section
5.1] to obtain optimal excess risk
L2
µ·eO1
nm+dln(1/δ)
ε2n2m
with the same gradient complexity stated in Theorem 3.2. This improves over the previous state-of-
the-art gradient complexity ≈β(mn)3/2of [AL24].
4 An optimal algorithm with subquadratic gradient complexity for
non-smooth losses
In this section, we extend our accelerated algorithm from the previous section to non-smooth loss
functions. To accomplish this with minimal computational cost, we apply randomized (convolution)
smoothing [YNS12 ,DBW12 ] to approximate non-smooth fby aβ-smooth ˜f. We can then apply Al-
gorithm 3 to ˜f. Since convolution smoothing is by now a standard optimization technique, we defer
the details and proof to Appendix D.
Theorem 4.1 (Privacy and utility of smoothed Algorithm 3 for non-smooth loss - informal) .Let
ε≤10,δ <1/(mn), and q >0such that n1−q>100 log(20 nmdeε/δ)
ε(1−(1/2)q). Then, applying Algorithm 3
to the smooth approximation of fyields optimal excess risk:
EF(xl)−F∗≤LR·eO 
1√mn+p
dlog(1/δ)
εn√m!
.
The gradient complexity of this algorithm is upper bounded by
mn
1 +n3/8m1/4ε1/4
.
Remark 4.2 (State-of-the-art gradient complexity) .The only previous polynomial-time algorithm
that can achieve optimal excess risk for non-smooth loss functions is due to [ AL24 ]. The algorithm
of [AL24 ] required (nm)3+ (mn)2√
dgradient evaluations. Thus, the gradient complexity of the
smoothed version of Algorithm 3 offers a significant improvement over the previous state-of-the-art .
For example, if ε= 1, then our algorithm is faster than the previous state-of-the-art by a multiplicative
factor of at least n13/8m7/4.
5 Conclusion
In this paper, we developed new user-level DP algorithms with improved runtime and excess risk
guarantees for stochastic convex optimization without the restrictive assumptions made in prior
9works. Our accelerated Algorithm 3 achieves optimal excess risk for both smooth and non-smooth
loss functions, with significantly smaller computational cost than the previous state-of-the-art. Our
linear-time Algorithm 1 achieves state-of-the-art excess risk under much milder, more practical
assumptions than existing linear-time approaches.
Our work paves the way for several intriguing future research directions. First, the question of
whether there exists a linear-time algorithm that can attain the user-level DP lower bound for smooth
losses remains open. In light of our improved gradient complexity bound ( ≈(nm)9/8), we are now
optimistic that the answer to this question is “yes.” We believe that our novel techniques will be
key to the development of an optimal linear-time algorithm. Specifically, utilizing Lemma 2.3 to
apply outlier removal to the iterates instead of the gradients appears to be pivotal. Second, the study
of user-level DP SCO has been largely limited to approximate (ε, δ)-DP. What rates are achievable
under the stronger notion of pure ε-user-level DP? Third, it would be useful to develop fast and
optimal algorithms that are tailored to federated learning environments [ MRTZ18 ,GLZW24 ], where
only a small number of users may be available to communicate with the server in each iteration. We
hope our work inspires and guides further research in this exciting and practically important area.
Acknowledgements
AL’s research is supported by NSF grant 2023239 and the AFOSR award FA9550-21-1-0084. We
thank the anonymous NeurIPS reviewers for their helpful feedback.
10References
[AFKT21] Hilal Asi, Vitaly Feldman, Tomer Koren, and Kunal Talwar. Private stochastic convex
optimization: Optimal rates in ℓ1geometry. In ICML , 2021.
[AL24] Hilal Asi and Daogao Liu. User-level differentially private stochastic convex optimiza-
tion: Efficient algorithms with optimal rates. In International Conference on Artificial
Intelligence and Statistics , pages 4240–4248. PMLR, 2024.
[ALT24] Hilal Asi, Daogao Liu, and Kevin Tian. Private stochastic convex optimization with
heavy tails: Near-optimality from simple reductions. arXiv preprint arXiv:2406.02789 ,
2024.
[BBG18] Borja Balle, Gilles Barthe, and Marco Gaboardi. Privacy amplification by subsampling:
Tight analyses via couplings and divergences. In S. Bengio, H. Wallach, H. Larochelle,
K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, Advances in Neural Information
Processing Systems , volume 31. Curran Associates, Inc., 2018.
[BFTT19] Raef Bassily, Vitaly Feldman, Kunal Talwar, and Abhradeep Thakurta. Private stochastic
convex optimization with optimal rates. In Advances in Neural Information Processing
Systems , volume 32, 2019.
[BS23] Raef Bassily and Ziteng Sun. User-level private stochastic convex optimization with
optimal rates. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt,
Sivan Sabato, and Jonathan Scarlett, editors, Proceedings of the 40th International
Conference on Machine Learning , volume 202 of Proceedings of Machine Learning
Research , pages 1838–1851. PMLR, 23–29 Jul 2023.
[CTW+21]Nicholas Carlini, Florian Tramer, Eric Wallace, Matthew Jagielski, Ariel Herbert-V oss,
Katherine Lee, Adam Roberts, Tom B Brown, Dawn Song, Ulfar Erlingsson, et al.
Extracting training data from large language models. In USENIX Security Symposium ,
volume 6, pages 2633–2650, 2021.
[DBW12] John C Duchi, Peter L Bartlett, and Martin J Wainwright. Randomized smoothing for
stochastic optimization. SIAM Journal on Optimization , 22(2):674–701, 2012.
[DMNS06] Cynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam Smith. Calibrating noise
to sensitivity in private data analysis. In Theory of cryptography conference , pages
265–284. Springer, 2006.
[DR14] Cynthia Dwork and Aaron Roth. The Algorithmic Foundations of Differential Privacy ,
volume 9. Now Publishers, Inc., 2014.
[FKT20] Vitaly Feldman, Tomer Koren, and Kunal Talwar. Private stochastic convex optimization:
optimal rates in linear time. In Proceedings of the 52nd Annual ACM SIGACT Symposium
on Theory of Computing , pages 439–449, 2020.
[GKK+23]Badih Ghazi, Pritish Kamath, Ravi Kumar, Pasin Manurangsi, Raghu Meka, and Chiyuan
Zhang. On user-level private convex optimization. In International Conference on
Machine Learning , pages 11283–11299. PMLR, 2023.
[GL12] Saeed Ghadimi and Guanghui Lan. Optimal stochastic approximation algorithms for
strongly convex stochastic composite optimization i: A generic algorithmic framework.
SIAM Journal on Optimization , 22(4):1469–1492, 2012.
[GL13] Saeed Ghadimi and Guanghui Lan. Optimal stochastic approximation algorithms for
strongly convex stochastic composite optimization, ii: Shrinking procedures and optimal
algorithms. SIAM Journal on Optimization , 23(4):2061–2089, 2013.
[GLZW24] Changyu Gao, Andrew Lowy, Xingyu Zhou, and Stephen J Wright. Private het-
erogeneous federated learning without a trusted server revisited: Error-optimal and
communication-efficient algorithms for convex losses. arXiv preprint arXiv:2407.09690 ,
2024.
11[HRS16] Moritz Hardt, Ben Recht, and Yoram Singer. Train faster, generalize better: Stability of
stochastic gradient descent. In Maria Florina Balcan and Kilian Q. Weinberger, editors,
Proceedings of The 33rd International Conference on Machine Learning , volume 48 of
Proceedings of Machine Learning Research , pages 1225–1234, New York, New York,
USA, 20–22 Jun 2016. PMLR.
[JNG+19]Chi Jin, Praneeth Netrapalli, Rong Ge, Sham M Kakade, and Michael I Jordan. A short
note on concentration inequalities for random vectors with subgaussian norm. arXiv
preprint arXiv:1902.03736 , 2019.
[KLL21] Janardhan Kulkarni, Yin Tat Lee, and Daogao Liu. Private non-smooth erm and sco in
subquadratic steps. Advances in Neural Information Processing Systems , 34:4053–4064,
2021.
[KOV15] Peter Kairouz, Sewoong Oh, and Pramod Viswanath. The composition theorem for
differential privacy, 2015.
[LJCJ17] Lihua Lei, Cheng Ju, Jianbo Chen, and Michael I Jordan. Non-convex finite-sum
optimization via scsg methods. In Proceedings of the 31st International Conference on
Neural Information Processing Systems , pages 2345–2355, 2017.
[LLL+24a] Zhuohang Li, Andrew Lowy, Jing Liu, Toshiaki Koike-Akino, Kieran Parsons, Bradley
Malin, and Ye Wang. Analyzing inference privacy risks through gradients in machine
learning. arXiv preprint arXiv:2408.16913 , 2024.
[LLL+24b] Andrew Lowy, Zhuohang Li, Jing Liu, Toshiaki Koike-Akino, Kieran Parsons, and
Ye Wang. Why does differential privacy with large epsilon defend against practical
membership inference attacks? arXiv preprint arXiv:2402.09540 , 2024.
[LR22] Andrew Lowy and Meisam Razaviyayn. Private stochastic optimization with large
worst-case lipschitz parameter, 2022.
[LSA+21]Daniel Levy, Ziteng Sun, Kareem Amin, Satyen Kale, Alex Kulesza, Mehryar Mohri,
and Ananda Theertha Suresh. Learning with user-level privacy. Advances in Neural
Information Processing Systems , 34:12466–12479, 2021.
[LUW24] Andrew Lowy, Jonathan Ullman, and Stephen Wright. How to make the gradients
small privately: Improved rates for differentially private non-convex optimization. In
Forty-first International Conference on Machine Learning , 2024.
[McS09] Frank D McSherry. Privacy integrated queries: an extensible platform for privacy-
preserving data analysis. In Proceedings of the 2009 ACM SIGMOD International
Conference on Management of data , pages 19–30, 2009.
[MRTZ18] Brendan McMahan, Daniel Ramage, Kunal Talwar, and Li Zhang. Learning differ-
entially private recurrent language models. In International Conference on Learning
Representations (ICLR) , 2018.
[SSSS17] Reza Shokri, Marco Stronati, Congzheng Song, and Vitaly Shmatikov. Membership
inference attacks against machine learning models. In 2017 IEEE symposium on security
and privacy (SP) , pages 3–18. IEEE, 2017.
[SSSSS09] Shai Shalev-Shwartz, Ohad Shamir, Nathan Srebro, and Karthik Sridharan. Stochastic
convex optimization. In COLT , volume 2, page 5, 2009.
[TCK+22]Eliad Tsfadia, Edith Cohen, Haim Kaplan, Yishay Mansour, and Uri Stemmer. Friend-
lycore: Practical differentially private aggregation. In International Conference on
Machine Learning , pages 21828–21863. PMLR, 2022.
[Ull17] Jonathan Ullman. CS7880: rigorous approaches to data privacy, 2017.
[XZ24] Zheng Xu and Yanxiang Zhang. Advances in private training for pro-
duction on-device language models. https://research.google/blog/
advances-in-private-training-for-production-on-device-language-models/ ,
2024. Google Research Blog.
12[YNS12] Farzad Yousefian, Angelia Nedi ´c, and Uday V Shanbhag. On stochastic gradient and
subgradient methods with adaptive steplength sequences. Automatica , 48(1):56–67,
2012.
[ZTOH22] Liang Zhang, Kiran K Thekumparampil, Sewoong Oh, and Niao He. Bring your own
algorithm for optimal differentially private stochastic minimax optimization. Advances
in Neural Information Processing Systems , 35:35174–35187, 2022.
13Appendix
A More Preliminaries
A.1 Tools from Differential Privacy
Additive Noise Mechanisms Additive noise mechanisms privatize a query by adding noise to its
output, with the scale of the noise calibrated to the sensitivity of the query.
Definition A.1 (Sensitivity) .Given a function q:ZN→Rkand a norm ∥·∥ponRk, theℓp-sensitivity
ofqis defined as
sup
D∼D′∥q(D)−q(D′)∥p,
where the supremum is taken over all pairs of datasets that differ in one user’s data.
Definition A.2 (Laplace Distribution) .We say X∼Lap(b)if the density of Xisf(X=x) =
1
2bexp(−|x|
b).
Definition A.3 (Laplace Mechanism) .Letε >0. Given a function q:ZN→RkonRkwith
ℓ1-sensitivity ∆, the Laplace Mechanism Mis defined by
M(D) :=q(D) + (Y1, . . . , Y k),
where {Yi}k
i=1are i.i.d., Yi∼Lap ∆
ε
.
Lemma A.4 (Privacy of Laplace Mechanism [DR14]) .The Laplace Mechanism is ε-DP .
Definition A.5 (Gaussian Mechanism) .Letε >0,δ∈(0,1). Given a function q:ZN→Rkwith
ℓ2-sensitivity ∆, the Gaussian Mechanism Mis defined by
M(D) :=q(D) +G
where G∼ N k 
0, σ2Ik
andσ2=2∆2log(2/δ)
ε2 .
Lemma A.6 (Privavcy of Gaussian Mechanism [DR14]) .The Laplace Mechanism is (ε, δ)-DP .
Advanced Composition If we adaptively query a data set Ttimes, then the privacy guarantees of
theT-th query is still DP and the privacy parameters degrade gracefully:
Lemma A.7 (Advanced Composition Theorem [ DR14 ]).Letε≥0, δ, δ′∈[0,1). Assume
A1,···,AT, with At:Zn× X → X , are each (ε, δ)-DP∀t= 1,···, T. Then, the adaptive
composition A(D) :=AT(D,AT−1(D,AT−2(X,···)))is(ε′, Tδ+δ′)-DP for
ε′=p
2Tln(1/δ′)ε+Tε(eε−1).
Privacy Amplification by Subsampling
Lemma A.8 ([Ull17 ]).LetM:ZM→ X be(ε, δ)-DP . Let M′:ZN→ X that first selects a
random subsample D′of size Mfrom the data set D ∈ ZNand then outputs M(D′). Then, M′is
(ε′, δ′)-DP , where ε′=(eε−1)M
Nandδ′=δM
N.
AboveThreshold: AboveThreshold algorithm [ DR14 ] which is a key tool in differential privacy to
identify whether there is a query qi:Z →Rin a stream of queries q1, . . . , q Tthat is above a certain
threshold ∆. The AboveThreshold Algorithm 4 has the following guarantees:
Lemma A.9 ([DR14 ], Theorem 3.24) .Letγ >0andα=8 log(2 T/γ)
ε,k∈[T+ 1].AboveThreshold
is(ε,0)-DP . Moreover, with probability at least 1−γ, for all t≤k, we have:
•ifat=⊤, then qt(D)≥∆−α; and
•ifat=⊥, then qt(D)≤∆ +α.
14Algorithm 4: AboveThreshold
1Input: Dataset D= (Z1, . . . , Z n), threshold ∆∈R, privacy parameter ε, sequence of Tqueries
q1,···, qT:Zn→R, each with ℓ1-sensitivity 1;
2Letb∆ := ∆ + Lap(2
ε);
3fort= 1toTdo
4 Receive a new query qt:Zn→R;
5 Sample νi∼Lap(4
ε);
6 ifqt(D) +νi≥b∆then
7 Output: at=⊤;
8 Halt ;
9 end
10 else
11 Output: at=⊥;
12 end
13end
A.2 SubGaussian and Norm-SubGuassian Random Vectors
Definition A.10. Letζ >0. We say a random vector XisSubGaussian (SG(ζ)) with parameter ζ
ifE[e⟨v,X−EX⟩]≤e∥v∥2ζ2/2for any v∈Rd. Random vector X∈RdisNorm-SubGaussian with
parameter ζ(nSG( ζ)) ifP[∥X−EX∥ ≥t]≤2e−t2
2ζ2for all t >0.
Theorem A.11 (Hoeffding-type inequality for norm-subGaussian, [ JNG+19]).LetX1,···, Xk∈
Rdbe random vectors, and let Fi=σ(x1,·, xi)fori∈[k]be the corresponding filtration. Suppose
for each i∈[k],Xi| Fi−1is zero-mean nSG( ζi). Then, there exists an absolute constant c >0, for
anyγ >0,
P
X
i∈[k]Xi≥cs
log(d/γ)X
i∈[k]ζ2
i
≤γ.
Lemma A.12 ([JNG+19]).There exists an absolute constant c, such that if XisnSG( ζ), then for
any fixed unit vector v∈Rd,⟨v, X⟩iscζnorm-SubGaussian.
B Proof of Theorem 2.1
Theorem B.1 (Formal statement of Theorem 2.1) .Suppose n1−q≥(100/(1−1/2q)) log( n/δ)/εfor
some small q >0, and m≤nJfor some large J >0. Choose p=J+ 3/2andη=R/(L√
dmnε ).
in Algorithm 1. Then, Algorithm 1 is (ε, δ)-user-level DP and achieves excess risk
EF(xl)−F∗≤LR·eO 
1√nmε+p
dlog(1/δ)√nε√m!
,
using nmgradient evaluations, provided β≤(L/R)√
dmnε .
The gradient complexity is clear by inspection of the algorithm: The number of stochastic gradients
computed during the algorithm is
lX
i=1TiC=lX
i=1NimC=lX
i=1nim≤nm.
Next, we will prove the privacy statement in Theorem B.1. The following lemma ensures that if the
Laplace noise added in Algorithm 1 is sufficiently small and outlier detection succeeds, then the
sensitivity of ˜xiiseO(τi/C)with high probability.
15Lemma B.2. [AL24 , Slight modification of Lemma 3.5] Let i∈[l]andζ >0. Suppose Diand
D′
idiffer in the data of one user and we are in phase iof Algorithm 1. Let Eibe the event that the
Laplace noise added to the concentration score si(τi)forDihas absolute value less than 2C/15
and define E′
isimilarly for data D′
i. Denote ai:=1(bsi(τi)≥4C/5)anda′
i:=1(bs′
i(τi)≥4C/5),
wherebsi(τi)andbs′
i(τi)are the noisy concentration scores that we get when running phase iof
Algorithm 1 on neighboring DiandD′
i, respectively. Then, conditional on ai=a′
iandEiTE′
i,
there is a coupling Γiover˜xiand˜x′
isuch that for (yi, y′
i)drawn from Γi, we have
∥yi−y′
i∥≲τilog(1/ζ)
C
with probability at least 1−ζ.
With Lemma B.2 in hand, we proceed to prove that Algorithm 1 is (ε, δ)-user-level DP:
Proof of Theorem B.1 - Privacy. Privacy: Since the {Di}l
i=1are disjoint, parallel composition of
DP [ McS09 ] implies that it suffices to prove that phase iis(ε, δ)-user-level-DP for any fixed iand
fixed xi−1. To that end, let DandD′be adjacent datasets differing in the data of one user, say
Zi,1̸=Z′
i,1without loss of generality. We will show that the outputs of phase iwhen run on Dand
D′,xi:=xi(D)andx′
i:=xi(D′)respectively, are (ε, δ)-indistinguishable.
LetEibe the event that the Laplace noise added in phase i(for data set D) has absolute value less
than2C/15and define E′
ianalogously for data set D′. Note that EiandE′
iare independent and
P(Ei, E′
i)≥1−δ/10eε. Denote ζ:=δ/10eε. Let ai:=1(bsi(τi)≥4C/5)anda′
i:=1(bs′
i(τi)≥
4C/5), where bsi(τi)andbs′
i(τi)are the noisy concentration scores that we get when running phase
iof Algorithm 1 on neighboring DiandD′
i, respectively. By Lemma B.2 and our choice of C, we
know that, conditional on EiTE′
iand on ai=a′
i, there exists a coupling Γover(˜xi,˜x′
i)such that
for(yi, y′
i)drawn from Γ, we have
∥yi−y′
i∥≲τilog(1/ζ)
C(1)
with probability at least 1−ζ.
Note that the sensitivity of siis less than or equal to 2. Thus, by the privacy guarantees of the Laplace
mechanism (Lemma A.4), we have
P(ai=b)≤eε/4P(a′
i=b) (2)
for any b∈ {0,1}. Further, this implies
P(ai=b, Ei)≤eε/4[P(a′
i=b, E′
i) +ζ]. (3)
By the bound (1), the privacy guarantee of the Gaussian mechanism (Lemma A.6), our choice of σi,
and independence of the Laplace and Gaussian noises that we add in Algorithm 1, we have
P(xi∈ O | Ei, ai= 1)≤eε/4P(x′
i∈ O | E′
i, a′
i= 1) +δ
n+ζ, (4)
for any event O ⊂ X .
Moreover, since the algorithm halts and returns xi= 0ifai= 0, we know that
P(xi∈ O | Ei, ai= 0) = P(x′
i∈ O | E′
i, a′
i= 0) (5)
for any event O ⊂ X .
Therefore,
P(xi∈ O) =P(xi∈ O | Ei)P(Ei) +P(xi∈ O | Ec
i)P(Ec
i)
≤P(xi∈ O | Ei, ai= 1)P(Ei, ai= 1) + P(xi∈ O | Ei, ai= 0)P(Ei, ai= 0) + ζ
(i)
≤eε/4P(x′
i∈ O | E′
i, a′
i= 1)eε/4[P(E′
i, a′
i= 1) + ζ]
+P(x′
i∈ O | E′
i, a′
i= 0)eε/4[P(E′
i, a′
i= 0) + ζ] +ζ
16≤eε/2P(x′
i∈ O, E′
i) +ζ
2eε/2+ 1
≤eεP(x′
i∈ O) +δ,
where (i)follows from inequalities (3),(4), and (5). Thus, xiis(ε, δ)-user-level-DP. This completes
the privacy proof.
Next, we turn to the excess risk proof. The following lemma is immediate from [ FKT20 , Lemma
4.5]:
Lemma B.3. Letηi≤1/β. Then, for any y∈ X and all i, j, we have
E[F(˜xi,j)−F(y)]≤E∥y−xi−1∥2
ηiTi+ηiL2.
The next novel lemma is crucial in our analysis:
Lemma B.4 (Re-statement of Lemma 2.3) .Assume f(·, z)is convex, L-Lipschitz, and β-smooth
onXwithη≤1/β. Let ˜x←SGD (D, η, T, x 0)and˜y←SGD (D′, η, T, x 0)be two independent
runs of projected SGD, where D, D′∼PNare i.i.d. Then, with probability at least 1−ζ, we have
∥˜x−˜y∥≲ηLp
Tlog(dT/ζ ).
Proof. Letgt:=∇f(xt, zt)forztdrawn uniformly from Dwithout replacement and g′
t:=
∇f(yt, z′
t)forz′
tdrawn uniformly from D′without replacement. Let F(x) :=Ez∼P[f(x, z)].
We will prove that ∥xt−yt∥≲ηLp
Tlog(dT/ζ )with probability at least 1−ζ/tfor all t∈[T].
Note that this implies the lemma. We proceed by induction. The base case, when t= 0, is trivially
true since x0=y0. For the inductive hypothesis, suppose there is an absolute constant c >0such
that with probability at least 1−tζ/T , we have
∥xi−yi∥ ≤cηLp
i·log(dT/ζ ) + 2ηL,
∀i≤t. Then, for the inductive step, we have by non-expansiveness of projection onto convex sets,
that
∥xt+1−yt+1∥2≤ ∥xt−ηgt−(yt−ηg′
t)∥2
=∥xt−η∇F(xt)−(yt−η∇F(yt))−η(gt− ∇F(xt)−g′
t+∇F(yt))∥2
=∥xt−η∇F(xt)−(yt−η∇F(yt))∥2
−2η⟨xt−η∇F(xt)−(yt−η∇F(yt)), gt− ∇F(xt)−g′
t+∇F(yt)⟩
+η2∥gt− ∇F(xt)−g′
t+∇F(yt)∥2
(i)
≤ ∥xt−yt∥2−2η⟨xt−η∇F(xt)−(yt−η∇F(yt)), gt− ∇F(xt)−g′
t+∇F(yt)⟩
+ 4η2L2, (6)
where (i)follows from the non-expansive property of gradient descent on smooth convex function
forη≤1/β[HRS16].
Define at:=−2η⟨xt−η∇F(xt)−(yt−η∇F(yt)), gt−∇F(xt)−g′
t+∇F(yt)⟩. By Inequality (6)
and the inductive hypothesis, we obtain
∥xt+1−yt+1∥2≤4η2L2t+tX
i=1at.
It remains to boundPt
i=1ai. Note that E[ai|a1,···, ai−1] = 0 , and by Lemma A.12 we know
there is a constant c >0such that aiisnSG( cηL∥xi−yi∥)for all i. Hence by Theorem A.11, we
know
P
tX
i=1ai≥cηLs
log(dT/γ )X
i≤t∥xi−yi∥2
≤1−ζ/T.
17Conditional on the event that ∥xi−yi∥ ≤cp
log(dT/ζ )ηL√
ifor all i≤t(which happens with
probability 1−tζ/T by the inductive hypothesis), we know
P"tX
i=1ai≥c2(t+ 1)L2η2log(dT/ζ )∥xi−yi∥ ≤clog(dT/ζ )ηL√
i,∀i≤t#
≤1−ζ/T.
Hence we know
Ph
∥xt+1−yt+1∥2≥c2log(dT/ζ )η2L2(t+ 1)∥xi−yi∥ ≤clog(dT/ζ )ηL√
i,∀i≤ti
≤1−ζ/T.
Combining the above pieces completes the inductive step, showing that ∥xt+1−yt+1∥ ≤
cp
(t+ 1) log( dT/ζ )ηL+ 2ηLwith probability at least 1−(t+ 1)ζ/T. This completes the proof.
By combining Lemmas 2.3 and B.3 with the localization proof technique of [ FKT20 ], we can now
prove the excess risk guarantee of Theorem B.1:
Proof of Theorem 2.1 - Excess risk. Excess Risk: First, we will argue that ˜xi=1
CPC
j=1˜xi,jfor all
iwith high probability ≥1−3/nm . Lemma 2.3 implies that
∥˜xi,j−˜xi,j′∥ ≤τi
for all i∈[l],j, j′∈[C]with probability at least 1−1/nm . Thus, si(τi) =Cwith probability at
least1−1/nm . Now, conditional on si(τi) =C, we have bsi(τi)≥4C/5for all iwith probability at
least1−1/nm by Laplace concentration and a union bound. Moroever, if ∥˜xi,j−˜xi,j′∥ ≤τifor all
j, j′, then pi,j= 1for all jand hence there are no outliers: Si={˜xi,j}j∈[C]. By a union bound, we
conclude that Si={˜xi,j}j∈[C]and hence ˜xi=1
SiP
Di,j∈Si˜xi,jfor all iwith probability at least
≥1−3/nm . By the law of total expectation and Lipschitz continuity, it suffices to condition on this
high probability good event that ˜xi=1
CPC
j=1˜xi,jfor all i: the total expected excess risk can only
be larger than the conditional excess risk by an additive factor of at most 3LR/nm .
Now, conditional on ˜xi=1
SiP
Di,j∈Si˜xi,j, Lemma B.3 and Jensen’s inequality implies
E[F(˜xi)−F(˜xi−1)]≲E∥˜xi−1−xi−1∥2
ηiTi+ηiL2=dσ2
i−1
ηiTi+ηiL2. (7)
Next, let x∗
0:=x∗=argminx∈XF(x), and write
E[F(xl)−F∗] =lX
i=1E[F(x∗
i)−F(x∗
i−1)] +E[F(xl)−F(x∗
l)]
≲R2
ηT1+ηL2+lX
i=2
ηi−1L2d+ηiL2
+L2√
dηlp
Tl
≲R2
ηT1+dηL2+L2√
dηlp
Tl.
Plugging in the prescribed algorithmic parameters completes the excess risk proof.
C Proofs of Results in Section 3
Theorem C.1 (Re-statement of Theorem 3.1) .Letε≤10,q > 0such that n1−q>
100 log(20 nmdeε/δ)
ε(1−(1/2)q). Then, Algorithm 3 is (ε, δ)-DP .
We require the following lemma, which is a direct consequence of [AL24, Lemma 3.5]:
18Lemma C.2. Consider Algorithm 2. Let D′
iandD′
ibe two data sets that differ in the data of one
user. Let Ei={|vt
i| ≤Ki/20∀t∈[Ti]∩ |ξi| ≤Ki/20}. Define E′
isimilarly for independent
draws of random Laplace noise: E′
i={|(vt
i)′| ≤Ki/20∀t∈[Ti]∩ |ξ′
i| ≤Ki/20}. Let at
i=
1(bst
i(Di)≥4Ki/5)andbt
i=1(bst
i(D′
i)≥4Ki/5)denote the concentration scores in iteration t.
Then, conditional on EiTE′
iand conditional on at
i=bt
i, there exists a coupling Γt
iovergt
i(Di)and
gt
i(D′
i)such that for (h, h′)drawn from Γi, we have
∥h−h′∥≲τlog(1/ζ)
Ki
with probability at least 1−ζ.
Proof of Theorem C.1. Note that our assumption on n1−qbeing sufficiently large implies that ni≳
log(nmd/δ )
εfor all i∈[l]. By parallel composition [ McS09 ] and post-processing, it suffices to show
that{bgt
i}Ti
t=1satisfies (ε, δ)-user-level DP for any i∈[l]. To that end, fix any i∈[l]and let D
andD′be adjacent datasets that differ in the data of one user such that Di̸=D′
i. We will show
that{bgt
i(D)}Ti
t=1and{bgt
i(D′)}Ti
t=1are(ε, δ)-indistinguishable, which will imply that Algorithm 3 is
(ε, δ)-user-level DP.
LetEi={|vt
i| ≤Ki/20∀t∈[Ti]∩ |ξi| ≤Ki/20}. Define E′
isimilarly for independent draws
of random Laplace noise: E′
i={|(vt
i)′| ≤Ki/20∀t∈[Ti]∩ |ξ′
i| ≤Ki/20}. Our choice of Ki≥
500 log( nmdeε/δ)
εensures that
P
Ei\
E′
i
≥1−δ/(10eε),
by Laplace concentration and a union bound. Let ζ:=δ/(10Tieε).
Letat
i=1(bst
i(D)≥4Ki/5)andbt
i=1(bst
i(D′)≥4Ki/5). Note that if at
i=bt
i= 0, then
bgt
i(D) = 0 = bgt
i(D′).
Conditional on the good event that at
i=bt
ifor all tand conditional on EiTE′
i, we can bound the
ℓ2-sensitivity of gt
iwith high probability, via Lemma C.2 and a union bound:
∥gt
i(D)−gt
i(D′)∥≲τlog(1/ζ)
Ki≲τlog(nmeε/δ)
Ki(8)
for all t∈[Ti]with probability at least 1−Tiζ= 1−δ/(10eε).
Note that {bst
i(D)}Ti
t=1and{bst
i(D′)}areε/2-indistinguishable by the DP guarantees of AboveThresh-
old in Lemma A.9, since the sensitivity of st
iis upper bounded by 2. Therefore,
P({at
i}Ti
t=1=v, Ei)≤eε/2h
P({bt
i}Ti
t=1=v, E′
i) +ζi
(9)
for any v∈ {0,1}Ti.
Now, by the sensitivity bound (8), the privacy guarantee of the Gaussian mechanism (Lemma A.6)
and our choice of σi, the advanced composition theorem (Lemma A.7), and privacy amplification by
subsampling (Lemma A.8), we have
P({bgt
i(D)}Ti
t=1∈ O | Ei{at
i}Ti
t=1=v)≤eε/2P({bgt
i(D′)}Ti
t=1∈ O | E′
i,{bt
i}Ti
t=1=v) + (Ti+ 1)ζ,
(10)
for any event O ⊂ X . Here we also used the fact that Ki≥niε√Ti.
For short-hand, write {at
i}Ti
t=1= 1 ifat
i= 1 for all t∈[Ti]and{at
i}Ti
t=1= 0 ifat
i= 0 for some
t∈[Ti]; similarly for bt
i. Then since the algorithm halts and returns {bgt
i(D)}Ti
t=1= 0if{at
i}Ti
t=1= 0,
we know that
P({bgt
i(D)}Ti
t=1∈ O | Ei,{at
i}Ti
t=1= 0) = P({bgt
i(D′)}Ti
t=1∈ O | E′
i,{bt
i}Ti
t=1= 0), (11)
for any event O ⊂ X .
Combining the above pieces, we have
P({bgt
i(D)}Ti
t=1∈ O) =P({bgt
i(D)}Ti
t=1∈ O | Ei)P(Ei) +P({bgt
i(D)}Ti
t=1∈ O | Ec
i)P(Ec
i)
19≤P({bgt
i(D)}Ti
t=1∈ O | Ei,{at
i}Ti
t=1= 1)P(Ei,{at
i}Ti
t=1= 1)
+P({bgt
i(D)}Ti
t=1∈ O | Ei,{at
i}Ti
t=1= 0)P(Ei,{at
i}Ti
t=1= 0) + 2 Tζ
(i)
≤eε/2P({bgt
i(D′)}Ti
t=1∈ O | E′
i,{bt
i}Ti
t=1= 1)eε/4h
P(E′
i,{bt
i}Ti
t=1= 1) + Tζi
+P({bgt
i(D′)}Ti
t=1∈ O | E′
i,{bt
i}Ti
t=1= 0)eε/2h
P(E′
i,{bt
i}Ti
t=1= 0) + Tζi
+Tζ
≤eε/2P({bgt
i(D′)}Ti
t=1∈ O, E′
i) + 4Tζ
2eε/2+ 1
≤eεP({bgt
i(D′)}Ti
t=1∈ O) +δ,
where (i)follows from inequalities (9),(10), and (11). Thus, {bgt
i(D)}Ti
t=1is(ε, δ)-user-level-DP,
which implies the result.
Theorem C.3 (Formal statement of Theorem 3.2) .Letε≤10andδ <1/(mn). Then, choosing
λ=L
R
1√nm+√
d
εn√m
andp≥3q+ 2.5 + logn(√m)in Algorithm 3 yields optimal excess risk:
EF(xl)−F∗≤LR·eO 
1√mn+p
dlog(1/δ)
εn√m!
.
The gradient complexity of this algorithm is upper bounded by
mn 
1 +εβR
L1/4 
(mn)1/8∧ε2n2m
d1/8!!
+r
βR
Ln1/4m5/4
ε+n1/2m5/4
d1/4ε1/2
.
We will need the following bound on the excess empirical risk of accelerated (noisy) SGD for the
proof of Theorem C.3:
Lemma C.4. [GL13 , Proposition 7] Let xTbe computed by Tsteps of (multi-stage) Accelerated
Minibatch SGD on λ-strongly convex, β-smooth bFwith unbiased stochastic gradient estimator gt
such that E∥gt− ∇bF(xt)∥2≤V2for all t∈[T]. Then,
E[bF(xT)−min
x∈XbF(x)]≲[bF(x0)−min
x∈XbF(x)] exp 
−Ts
λ
β!
+V2
λT.
Remark C.5.As noted in Lemma C.4, we technically need to call a multi-stage implementation
of Algorithm 2 in line 7 of Algorithm 3 (as in [ GL13 ]) to get the desired excess risk bound for
minimizing the regularized ERM problem in each iteration. For improved readability, we omitted
these details in the main body.
Next, we obtain a bound on the variance of the noisy stochastic minibatch gradient estimator bgt
i
in Algorithm 2, which can then be plugged in for V2in Lemma C.4.
Lemma C.6 (Re-statement of Lemma 3.5) .Letδ≤1/(nm), ε≲1. Denote eFi(x) :=
1
niP
Zi,j∈DibF(x, Zi,j). Then, conditional on St
i=Dt
ifor all i∈[l], t∈[Ti], we have
E∥gt
i− ∇eFi(xt
i−1)∥2≲L2log(ndm)
Km
for all i∈[l], t∈[Ti], where the expectation is over both the random i.i.d. draw of D=
(Z1, . . . , Z n)∼Pnmand the randomness in Algorithm 3.
Proof. By [LJCJ17 , Lemma A.1], we know that, conditional on the draw of the data Diand for fixed
xt
i−1, the variance of the minibatch estimator of the gradient of the empirical loss is
E"gt
i− ∇eFi(xt
i−1)2Di, xt
i−1#
=E{il}K
l=1∼Unif([n])
1
KmKX
l=1mX
j=1∇f(xt
i−1, zt
il,j))− ∇eFi(xt
i−1)2Di, xt
i−1

20≤1(K=n)
K1
nnX
i=11
mmX
j=1[∇f(xt
i−1, zt
i,j)− ∇eFi(xt
i−1)]2
.
(12)
Recall eFi(x) :=1
nimP
z∈Dif(x, z)is the empirical loss of Di.
Now, for any fixed xand any Z∈Di, Hoeffding’s inequality implies that
∥∇bF(x, Z)− ∇F(x)∥ ≤τ=O 
Lp
log(nd/γ )√m!
with probability at least 1−γ, where bF(x, Z) :=1
mP
z∈Zf(x, z)is user Z’s empirical loss. Thus,
by the stability of user-level DP (see [BS23, Theorem 3.4]), for any i∈[l], t∈[t], we have that
∥∇bF(xt
i−1, Z)− ∇F(xt
i−1)∥ ≤τ (13)
for all Z∈ D with probability at least 1−γ′=n(e2εγ+δ), since xt
i−1is(ε, δ)-user-level DP. To
make γ′≲1/m, we choose γ= 1/mn and use the assumptions that ε≲1andδ≤1/mn . Thus,
for any fixed i, twe have
∥∇bF(xt
i−1, Z)− ∇F(xt
i−1)∥≲Lp
log(n2md)√m
for all Z∈ D with probability at least 1−1/m, which implies
E∥∇bF(xt
i−1, Z)− ∇F(xt
i−1)∥2≲L2log(nmd)
m.
This also implies
E∥∇bFD(xt
i−1)− ∇F(xt
i−1)∥2≲L2log(nmd)
m,
by Jensen’s inequality, where bFD(x)is the empirical loss over the entire data set D.
Plugging the above bounds into (12) then yields
E∥gt
i− ∇eFi(xt
i−1)∥2≲L2log(ndm)
Km=ED∼Pnm,{il}K
l=1∼Unif([n])
1
KmKX
l=1mX
j=1∇f(xt
i−1, zt
il,j)− ∇bFD(xt
i−1)2

≲1(K=n)
K·L2log2(nmd)
m,
completing the proof.
We are now ready to prove Theorem C.3:
Proof of Theorem C.3. Excess risk: Note that the assumption in the theorem ensures that Ki≤ni
for all i. By similar arguments to those used in [ AL24 , Proposition 3.7], we will show that with high
probability ≥1−2/(nm), for all i∈[l], t∈[Ti],St
i=Dt
iand hence gt
iis an unbiased estimator of
∇bFDt
i(xt
i). To show this, first note that for any γ >0and any fixed x,
∥∇bF(x, Zj)− ∇F(x)∥ ≤Llog(nd/γ )√m
with probability at least 1−γ/K iby Hoeffding’s inequality (see [ AL24 , Lemma 4.3]). Next, we
invoke the stability of differential privacy to show that for all t∈[Ti],(qt(Zt
i,1), . . . , q t(Zt
i,Ki))
isτ-concentrated (i.e. there exists q∗∈Rdsuch that ∥qt(Zt
i,j−q∗∥ ≤ τ) with probability
at least 1−Ti(e2εγ+δ)(see [ BS23 , Theorem 4.3]). By a union bound and the choice of
21γ= 1/[(nm)5/4log(ndm)e2ε], we have that (qt(Zt
i,1), . . . , q t(Zt
i,Ki))isτ-concentrated for all
i∈[l], t∈[Ti]with probability at least 1−1/nm . Now, τ-concentration of (qt(Zt
i,1), . . . , q t(Zt
i,Ki))
implies st
i(τ) =Ki. Further, st
i(τ) =Kiimplies bst
i(τ)≥4Ki/5with probability at least 1−ζif
Ki≥500 log( nm/ζ ), by Laplace concentration and a union bound. Next, note that τ-concentration
of(qt(Zt
i,1), . . . , q t(Zt
i,Ki))implies pt
i(Zt
i,j) = 1 for all j∈[Ki]andSt
i=Dt
i. Thus, St
i=Dt
ifor
alli, twith probability at least 1−1/nm . Setting ζ= 1/nm and using a union bound shows that
with probability at least 1−2/nm , we have St
i=Dt
iandgt
i=∇bFDt
i(xt
i)for all i, t.
Now, Lemma C.4 implies that, if the outlier-removal procedure in Algorithm 2 leads to an unbiased
gradient estimator gt
i(line 12) for all t∈[Ti=eΘ(1 +p
β/λi)], then
E[bFi(xTi
i)−min
x∈XbFi(x)]≲V2
i
λiTi, (14)
where V2
i= max t∈[ti]E∥bgt
i− ∇bFi(xt
i)∥2≲dσ2
i+log(ndm)L2
Kim(unconditionally, after taking
expectation over the random draw of D ∼ Pnm, by Lemma 3.5). We have shown that the event
GOOD :={gt
i=∇bFDt
i(xt
i)for all i∈[l], t∈[Ti]}occurs with probability at least 1−2/nm . We
will condition on GOOD for the rest of the proof: note that the Lipschitz assumption implies that the
total (unconditional) excess risk will only by larger than the conditional (on GOOD) excess risk by
an additive factor of at most 2LR/√nm.
By stability of regularized ERM (see [SSSSS09]), we have
E[F(x∗
i)−F(y)]≲L2
λinim+λiE[∥xi−1−y∥2] (15)
for all i, where x∗
i:=argminxbFi(x). By strong convexity and (14), we have
(λi/2)E∥xi−x∗
i∥2≤EbFi(xi)−bF∗
i≲dσ2
i
λiTi+L2log(ndm)
λiTiKim. (16)
Thus,
E∥xi−x∗
i∥2≲dσ2
i
λiTi+L2log(ndm)
λiTiKim≲dτ2log(1/δ)
λ2
iε2n2
i+L2log(ndm)
λ2
iTiKim(17)
Now, letting x∗
0:=x∗=argminx∈XF(x)and hiding logarithmic factors, we have:
E[F(xl)−F∗] =lX
i=1E[F(x∗
i)−F(x∗
i−1)] +E[F(xl)−F(x∗
l)]
≲L2
λ1n1m+λ1R2+lX
i=2EL2
λinim+λi∥xi−1−x∗
i−1∥2
+LE∥xl−x∗
l∥
≲L2
λnm+λR2+lX
i=2L2
λinim+λidτ2log(1/δ)
λ2
i−1ε2n2
i−1+L2
λ2
i−1Ti−1Ki−1m
+L√
dτp
Tllog(1/δ)
λlεnl,
where the first inequality used (15) and Lipschitz continuity, the second inequality used (17).
Note that KiTi≥ni. Further, our choice of sufficiently large pmakes λllarge enough that
L√
dτ√
Tllog(1/δ)
λlεnl≤LR√
d
εn√m. Therefore, upper bounding the sum by it’s corresponding geometric
series gives us
E[F(xl)−F∗]≲LR√
d
εn√m+L2
λ1
nm+dτ2log(1/δ)
ε2n2
+λR2. (18)
Plugging in λcompletes the excess risk proof.
Gradient Complexity: The gradient complexity isPl
i=1TiKim. Plugging in the prescribed choices
ofTiandKicompletes the proof.
22D Details on the non-smooth algorithm and the proof of Theorem 4.1
For any loss function f(·, z), we define the convolution function fr(·, z) := f(·, z)∗nrwhere
nris the uniform density in the ℓ2ball of radius rcentered at the origin in Rd. Specifically,
nr(y) =Γ(d
2+1)
πd
2rdfor∥y∥ ≤r, and nr(y) = 0 otherwise. For simplicity, we omit the dependence on
zin the following Lemma:
Lemma D.1 (Randomized Smoothing, [ YNS12 ,DBW12 ]).For any r >0, letXr:=X+{x∈
Rd:∥x∥ ≤r}. Iffis convex and L-Lipschitz over Xr, then the convolution function frhas the
following properties:
•fr(x)≤f(x)≤fr(x) +Lr, for all x∈ X.
•frisL-Lipschitz and convex.
•frisL√
d
r-smooth.
•For random variables y∼nr, we have Ey[∇f(x+y)] =∇fr(x).
The following lemma can be easily seen from the proofs of Theorems 3.1 and 3.2:
Lemma D.2 (Privacy and utility of Algorithm 3 for general Ki, Ti).Letε≤10,q >0such that
n1−q>100 log(20 nmdeε/δ)
ε(1−(1/2)q).
•IfKi≳niε√Ti+log(nmdeε/δ)
ε, then Algorithm 3 is (ε, δ)-user-level DP .
•IfTiKi≥niandTi≳(1 +p
β/λi) log( ndm)for all i, then Algorithm 3 achieves optimal
excess risk.
Theorem D.3 (Formal statement of Theorem 4.1) .Letε≤10,δ <1/(mn), and q >0such that
n1−q>100 log(20 nmdeε/δ)
ε(1−(1/2)q). Suppose that for any z,f(, z)is convex and L-Lipschitz over Xrfor
Xr:=X+{x∈Rd:∥x∥ ≤r}where r=√
d
εn√mR. Then, running Algorithm 3 with functions
{fr(x;z)}z∈Dyields optimal excess risk:
EF(xl)−F∗≤LR·eO 
1√mn+p
dlog(1/δ)
εn√m!
.
The gradient complexity of this algorithm is upper bounded by
mn
1 +n3/8m1/4ε1/4
.
Proof. By Lemma D.1 and our choice of r, we have |fr(x, z)−f(x, z)| ≤Lr=O(LR√
d
εn√m). Set
λ=1√mn. Then we know that
EF(xl)−F∗≤E[Fr(xl)−F∗
r] +O(LR√
d
εn√m).
Further, Frisβ-smooth for β≤L
Rεn√m. Set Ti= (1 +p
β/λi) log( ndm) = 1 +
n3/4
im1/2ε1/2log(ndm)andKi=niε√Ti+log(nmdeε/δ)
ε. Then Lemma D.2 implies that Algorithm 3
is(ε, δ)user-level DP, and yields the excess risk bound
EFr(xl)−F∗
r≤LR·˜O 
1√mn+p
dlog(1/δ)
εn√m!
,
as desired. The number of gradient evaluations is
lX
i=1TiKim≲mn
1 +n3/8m1/4ε1/4
.
This completes the proof.
23E Limitations
Our work weakens the assumptions on the smoothness parameter and the number of users that are
needed for user-level DP SCO. Nevertheless, our results still require certain assumptions that may
not always hold in practice. For example, we assume convexity of the loss function. In deep learning
scenarios, this assumption does not hold and our algorithms should not be used. Thus, user-level
DPnon-convex optimization is an important direction for future research [ LUW24 ]. Furthermore,
the assumption that the loss function is convex and uniformly Lipschitz continuous may not hold in
certain applications, motivating the future study of user-level DP stochastic optimization with heavy
tails [LR22, ALT24].
Our algorithms are also faster than the previous state-of-the-art, including a linear-time Algorithm 1
with state-of-the-art excess risk. However, our error-optimal accelerated Algorithm 3 runs in super-
linear time. Thus, in certain applications where a linear-time algorithm is needed due to strict
computational constraints, Algorithm 1 should be used instead.
F Broader Impacts
Our work on differentially private optimization for machine learning advances the field of privacy-
preserving ML by developing techniques that protect the privacy of individuals (users) who contribute
data. The significance of privacy cannot be overstated, as it is a fundamental right enshrined in
various legal systems worldwide. However, the implications of our work extend beyond its intended
benefits, and it is essential to consider both potential positive and negative impacts.
Positive Impacts:
1. Enhanced Privacy Protections: By incorporating differential privacy into machine learning
models, we can provide strong privacy guarantees for individuals, mitigating the risk of
personal data being exposed or misused.
2.Ethical Data Utilization: DP ML enables organizations to leverage data while adhering to
ethical standards and privacy regulations, fostering trust among users and stakeholders.
3.Broad Applications: The techniques we develop can be applied across diverse domains,
including healthcare, finance, and social sciences, where sensitive data is prevalent. This
broad applicability can drive innovations while maintaining privacy.
4.Educational Advancement: Our research contributes to the growing body of knowledge
in privacy-preserving technologies, serving as a valuable resource for future studies and
fostering an environment of continuous improvement in privacy practices.
Potential Negative Impacts:
1.Misuse by Corporations and Governments: There is a risk that our algorithms could be
exploited by entities to justify the unauthorized collection of personal data under the guise
of privacy compliance. Vigilant oversight and clear regulatory frameworks are necessary to
prevent such abuses.
2.Decreased Model Accuracy: While DP ML provides privacy benefits, it can also lead to
reduced model accuracy compared to non-private models. This trade-off might have adverse
consequences, such as less accurate medical diagnoses or flawed economic forecasts. For
example, an overly optimistic prediction of environmental impacts due to lower accuracy
could be misused to weaken environmental protections.
While recognizing the potential for misuse and the trade-offs involved, we firmly believe that the
advancement and dissemination of differentially private machine learning algorithms offer a net
benefit to society. By addressing privacy concerns head-on and advocating for responsible use, we
aim to contribute positively to the field of machine learning and uphold the fundamental right to
privacy. Through ongoing research, collaboration, and education, we strive to enhance both the
capabilities and ethical foundations of machine learning technologies.
24NeurIPS Paper Checklist
1.Claims
Question: Do the main claims made in the abstract and introduction accurately reflect the
paper’s contributions and scope?
Answer: [Yes]
Justification: We state our results in Sections 2-4 and prove them in the Appendix.
Guidelines:
•The answer NA means that the abstract and introduction do not include the claims
made in the paper.
•The abstract and/or introduction should clearly state the claims made, including the
contributions made in the paper and important assumptions and limitations. A No or
NA answer to this question will not be perceived well by the reviewers.
•The claims made should match theoretical and experimental results, and reflect how
much the results can be expected to generalize to other settings.
•It is fine to include aspirational goals as motivation as long as it is clear that these goals
are not attained by the paper.
2.Limitations
Question: Does the paper discuss the limitations of the work performed by the authors?
Answer: [Yes]
Justification: See Appendix E.
Guidelines:
•The answer NA means that the paper has no limitation while the answer No means that
the paper has limitations, but those are not discussed in the paper.
• The authors are encouraged to create a separate "Limitations" section in their paper.
•The paper should point out any strong assumptions and how robust the results are to
violations of these assumptions (e.g., independence assumptions, noiseless settings,
model well-specification, asymptotic approximations only holding locally). The authors
should reflect on how these assumptions might be violated in practice and what the
implications would be.
•The authors should reflect on the scope of the claims made, e.g., if the approach was
only tested on a few datasets or with a few runs. In general, empirical results often
depend on implicit assumptions, which should be articulated.
•The authors should reflect on the factors that influence the performance of the approach.
For example, a facial recognition algorithm may perform poorly when image resolution
is low or images are taken in low lighting. Or a speech-to-text system might not be
used reliably to provide closed captions for online lectures because it fails to handle
technical jargon.
•The authors should discuss the computational efficiency of the proposed algorithms
and how they scale with dataset size.
•If applicable, the authors should discuss possible limitations of their approach to
address problems of privacy and fairness.
•While the authors might fear that complete honesty about limitations might be used by
reviewers as grounds for rejection, a worse outcome might be that reviewers discover
limitations that aren’t acknowledged in the paper. The authors should use their best
judgment and recognize that individual actions in favor of transparency play an impor-
tant role in developing norms that preserve the integrity of the community. Reviewers
will be specifically instructed to not penalize honesty concerning limitations.
3.Theory Assumptions and Proofs
Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?
Answer: [Yes]
25Justification: All assumptions are provided in theorem statements and proofs are provided
in the Appendix.
Guidelines:
• The answer NA means that the paper does not include theoretical results.
•All the theorems, formulas, and proofs in the paper should be numbered and cross-
referenced.
•All assumptions should be clearly stated or referenced in the statement of any theorems.
•The proofs can either appear in the main paper or the supplemental material, but if
they appear in the supplemental material, the authors are encouraged to provide a short
proof sketch to provide intuition.
•Inversely, any informal proof provided in the core of the paper should be complemented
by formal proofs provided in appendix or supplemental material.
• Theorems and Lemmas that the proof relies upon should be properly referenced.
4.Experimental Result Reproducibility
Question: Does the paper fully disclose all the information needed to reproduce the main ex-
perimental results of the paper to the extent that it affects the main claims and/or conclusions
of the paper (regardless of whether the code and data are provided or not)?
Answer: [NA]
Justification: This is a theoretical paper without experiments.
Guidelines:
• The answer NA means that the paper does not include experiments.
•If the paper includes experiments, a No answer to this question will not be perceived
well by the reviewers: Making the paper reproducible is important, regardless of
whether the code and data are provided or not.
•If the contribution is a dataset and/or model, the authors should describe the steps taken
to make their results reproducible or verifiable.
•Depending on the contribution, reproducibility can be accomplished in various ways.
For example, if the contribution is a novel architecture, describing the architecture fully
might suffice, or if the contribution is a specific model and empirical evaluation, it may
be necessary to either make it possible for others to replicate the model with the same
dataset, or provide access to the model. In general. releasing code and data is often
one good way to accomplish this, but reproducibility can also be provided via detailed
instructions for how to replicate the results, access to a hosted model (e.g., in the case
of a large language model), releasing of a model checkpoint, or other means that are
appropriate to the research performed.
•While NeurIPS does not require releasing code, the conference does require all submis-
sions to provide some reasonable avenue for reproducibility, which may depend on the
nature of the contribution. For example
(a)If the contribution is primarily a new algorithm, the paper should make it clear how
to reproduce that algorithm.
(b)If the contribution is primarily a new model architecture, the paper should describe
the architecture clearly and fully.
(c)If the contribution is a new model (e.g., a large language model), then there should
either be a way to access this model for reproducing the results or a way to reproduce
the model (e.g., with an open-source dataset or instructions for how to construct
the dataset).
(d)We recognize that reproducibility may be tricky in some cases, in which case
authors are welcome to describe the particular way they provide for reproducibility.
In the case of closed-source models, it may be that access to the model is limited in
some way (e.g., to registered users), but it should be possible for other researchers
to have some path to reproducing or verifying the results.
5.Open access to data and code
Question: Does the paper provide open access to the data and code, with sufficient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?
26Answer: [NA]
Justification: This is a theoretical paper without experiments.
Guidelines:
• The answer NA means that paper does not include experiments requiring code.
•Please see the NeurIPS code and data submission guidelines ( https://nips.cc/
public/guides/CodeSubmissionPolicy ) for more details.
•While we encourage the release of code and data, we understand that this might not be
possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
including code, unless this is central to the contribution (e.g., for a new open-source
benchmark).
•The instructions should contain the exact command and environment needed to run to
reproduce the results. See the NeurIPS code and data submission guidelines ( https:
//nips.cc/public/guides/CodeSubmissionPolicy ) for more details.
•The authors should provide instructions on data access and preparation, including how
to access the raw data, preprocessed data, intermediate data, and generated data, etc.
•The authors should provide scripts to reproduce all experimental results for the new
proposed method and baselines. If only a subset of experiments are reproducible, they
should state which ones are omitted from the script and why.
•At submission time, to preserve anonymity, the authors should release anonymized
versions (if applicable).
•Providing as much information as possible in supplemental material (appended to the
paper) is recommended, but including URLs to data and code is permitted.
6.Experimental Setting/Details
Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?
Answer: [NA]
Justification: This is a theoretical paper without experiments.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The experimental setting should be presented in the core of the paper to a level of detail
that is necessary to appreciate the results and make sense of them.
•The full details can be provided either with the code, in appendix, or as supplemental
material.
7.Experiment Statistical Significance
Question: Does the paper report error bars suitably and correctly defined or other appropriate
information about the statistical significance of the experiments?
Answer: [Yes]
Justification: This is a theoretical paper without experiments.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The authors should answer "Yes" if the results are accompanied by error bars, confi-
dence intervals, or statistical significance tests, at least for the experiments that support
the main claims of the paper.
•The factors of variability that the error bars are capturing should be clearly stated (for
example, train/test split, initialization, random drawing of some parameter, or overall
run with given experimental conditions).
•The method for calculating the error bars should be explained (closed form formula,
call to a library function, bootstrap, etc.)
• The assumptions made should be given (e.g., Normally distributed errors).
•It should be clear whether the error bar is the standard deviation or the standard error
of the mean.
27•It is OK to report 1-sigma error bars, but one should state it. The authors should
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
of Normality of errors is not verified.
•For asymmetric distributions, the authors should be careful not to show in tables or
figures symmetric error bars that would yield results that are out of range (e.g. negative
error rates).
•If error bars are reported in tables or plots, The authors should explain in the text how
they were calculated and reference the corresponding figures or tables in the text.
8.Experiments Compute Resources
Question: For each experiment, does the paper provide sufficient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?
Answer: [Yes]
Justification: This is a theoretical paper without experiments.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The paper should indicate the type of compute workers CPU or GPU, internal cluster,
or cloud provider, including relevant memory and storage.
•The paper should provide the amount of compute required for each of the individual
experimental runs as well as estimate the total compute.
•The paper should disclose whether the full research project required more compute
than the experiments reported in the paper (e.g., preliminary or failed experiments that
didn’t make it into the paper).
9.Code Of Ethics
Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines ?
Answer: [Yes]
Justification: The research conforms to the Code of Ethics.
Guidelines:
•The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
•If the authors answer No, they should explain the special circumstances that require a
deviation from the Code of Ethics.
•The authors should make sure to preserve anonymity (e.g., if there is a special consid-
eration due to laws or regulations in their jurisdiction).
10.Broader Impacts
Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?
Answer: [Yes]
Justification: See Appendix F.
Guidelines:
• The answer NA means that there is no societal impact of the work performed.
•If the authors answer NA or No, they should explain why their work has no societal
impact or why the paper does not address societal impact.
•Examples of negative societal impacts include potential malicious or unintended uses
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
(e.g., deployment of technologies that could make decisions that unfairly impact specific
groups), privacy considerations, and security considerations.
•The conference expects that many papers will be foundational research and not tied
to particular applications, let alone deployments. However, if there is a direct path to
any negative applications, the authors should point it out. For example, it is legitimate
to point out that an improvement in the quality of generative models could be used to
28generate deepfakes for disinformation. On the other hand, it is not needed to point out
that a generic algorithm for optimizing neural networks could enable people to train
models that generate Deepfakes faster.
•The authors should consider possible harms that could arise when the technology is
being used as intended and functioning correctly, harms that could arise when the
technology is being used as intended but gives incorrect results, and harms following
from (intentional or unintentional) misuse of the technology.
•If there are negative societal impacts, the authors could also discuss possible mitigation
strategies (e.g., gated release of models, providing defenses in addition to attacks,
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
feedback over time, improving the efficiency and accessibility of ML).
11.Safeguards
Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?
Answer: [NA]
Justification: This is a theoretical paper without experiments.
Guidelines:
• The answer NA means that the paper poses no such risks.
•Released models that have a high risk for misuse or dual-use should be released with
necessary safeguards to allow for controlled use of the model, for example by requiring
that users adhere to usage guidelines or restrictions to access the model or implementing
safety filters.
•Datasets that have been scraped from the Internet could pose safety risks. The authors
should describe how they avoided releasing unsafe images.
•We recognize that providing effective safeguards is challenging, and many papers do
not require this, but we encourage authors to take this into account and make a best
faith effort.
12.Licenses for existing assets
Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?
Answer: [NA]
Justification: This is a theoretical paper without experiments.
Guidelines:
• The answer NA means that the paper does not use existing assets.
• The authors should cite the original paper that produced the code package or dataset.
•The authors should state which version of the asset is used and, if possible, include a
URL.
• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
•For scraped data from a particular source (e.g., website), the copyright and terms of
service of that source should be provided.
•If assets are released, the license, copyright information, and terms of use in the
package should be provided. For popular datasets, paperswithcode.com/datasets
has curated licenses for some datasets. Their licensing guide can help determine the
license of a dataset.
•For existing datasets that are re-packaged, both the original license and the license of
the derived asset (if it has changed) should be provided.
•If this information is not available online, the authors are encouraged to reach out to
the asset’s creators.
13.New Assets
Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?
29Answer: [NA]
Justification: This is a theoretical paper without experiments.
Guidelines:
• The answer NA means that the paper does not release new assets.
•Researchers should communicate the details of the dataset/code/model as part of their
submissions via structured templates. This includes details about training, license,
limitations, etc.
•The paper should discuss whether and how consent was obtained from people whose
asset is used.
•At submission time, remember to anonymize your assets (if applicable). You can either
create an anonymized URL or include an anonymized zip file.
14.Crowdsourcing and Research with Human Subjects
Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as
well as details about compensation (if any)?
Answer: [NA]
Justification: This is a theoretical paper without experiments.
Guidelines:
•The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
•Including this information in the supplemental material is fine, but if the main contribu-
tion of the paper involves human subjects, then as much detail as possible should be
included in the main paper.
•According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
or other labor should be paid at least the minimum wage in the country of the data
collector.
15.Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects
Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?
Answer: [NA]
Justification: This is a theoretical paper without experiments.
Guidelines:
•The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
•Depending on the country in which research is conducted, IRB approval (or equivalent)
may be required for any human subjects research. If you obtained IRB approval, you
should clearly state this in the paper.
•We recognize that the procedures for this may vary significantly between institutions
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
guidelines for their institution.
•For initial submissions, do not include any information that would break anonymity (if
applicable), such as the institution conducting the review.
30