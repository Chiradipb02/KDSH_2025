Published in Transactions on Machine Learning Research (01/2024)
Bandits with Stochastic Corruption:
Lower Bounds on Regret and Robust Optimistic Algorithms
Timothée Mathieu timothee.mathieu@inria.fr
Debabrota Basu debabrota.basu@inria.fr
Odalric-Ambrym Maillard odalric.maillard@inria.fr
Université de Lille, Inria, CNRS, Centrale Lille UMR 9189 – CRIStAL, F-59000 Lille, France
Reviewed on OpenReview: https: // openreview. net/ forum? id= oGIR0ic3jU
Abstract
We study the Bandits with Stochastic Corruption problem, i.e. a stochastic multi-armed
bandit problem with kunknown reward distributions, which are heavy-tailed and corrupted
by a history-independent stochastic adversary or Nature. To be specific, the reward ob-
tained by playing an arm comes from corresponding heavy-tailed reward distribution with
probability 1−ε∈(0.5,1]and an arbitrary corruption distribution of unbounded support
with probability ε∈[0,0.5). First, we provide a problem-dependent lower bound on the
regretof any corrupted bandit algorithm. The lower bounds indicate that the Bandits with
Stochastic Corruption problem is harder than the classical stochastic bandit problem with
sub-Gaussian or heavy-tail rewards. Following that, we propose a novel UCB-type algorithm
for Bandits with Stochastic Corruption, namely HuberUCB , that builds on Huber’s estimator
for robust mean estimation. Leveraging a novel concentration inequality of Huber’s estima-
tor, we prove that HuberUCB achieves a near-optimal regret upper bound. Since computing
Huber’s estimator has quadratic complexity, we further introduce a sequential version of
Huber’s estimator that exhibits linear complexity. We leverage this sequential estimator to
design SeqHuberUCB that enjoys similar regret guarantees while reducing the computational
burden. Finally, we experimentally illustrate the efficiency of HuberUCB andSeqHuberUCB in
solving Bandits with Stochastic Corruption for different reward distributions and different
levels of corruptions.
1 Introduction
The multi-armed bandit problem is an archetypal setting to study sequential decision-making under incom-
plete information (Lattimore & Szepesvári, 2020). In the classical setting of stochastic multi-armed bandits,
the decision maker or agent has access to k∈Nunknown reward distributions or arms. At every step, the
agent plays an arm and obtains a reward. The goal of the agent is to maximize the expected total reward
accumulated by a given horizon T∈N.
In this paper, we are interested in a challenging extension of the classical multi-armed bandit problem, where
the reward at each step may be corrupted by Nature, which is a stationary mechanism independent of the
agent’s decisions and observations. This setting is often referred as the Corrupted Bandits . Specifically, we
extend the existing studies of corrupted bandits (Lykouris et al., 2018; Bogunovic et al., 2020; Kapoor et al.,
2019) to the more general case, where the ‘true’ reward distribution might be heavy-tailed (i.e. with a finite
number of finite moments) and the corruption can be unbounded.
BanditswithStochasticCorruption. Specifically, wemodelacorruptedrewarddistributionas (1−ε)P+
εH, wherePis the distribution of inliers with a finite variance, His the distributions of outliers with possibly
1Published in Transactions on Machine Learning Research (01/2024)
unbounded support, and ε∈[0,1/2)is the proportion of outliers. Thus, in the corresponding stochastic
bandit setting, an agent has access to karms of corrupted reward distributions {(1−ε)Pi+εHi}k
i=1. Here,
Pi’s are uncorrupted reward distributions with heavy-tails and bounded variances, and Hi’s are corruption
distributions with possibly unbounded corruptions. The goal of the agent is to maximize the expected total
reward accumulated oblivious to the corruptions. This is equivalent to considering a setting where at every
step Nature flips a coin with success probability ε. The agent obtains a corrupted reward if Nature obtains
1and otherwise, an uncorrupted reward. We call this setting Bandits with Stochastic Corruption as the
corruption introduced in each step does not depend on the present or previous choices of arms and observed
rewards. Our setting encompasses both heavy-tailed rewards and unbounded corruptions. We formally
define the setting and corresponding regret definition in Section 3.
Though this article primarily focuses on the theoretical understanding of the interplay between corruption,
heavytailedness and decision making, we find it relevant to pinpoint at a few applications for which this
setting may apply. Note that heavy-tail distributions are naturally motivated by applications in economy and
financial markets (Agrawal et al., 2021), while corrupted distributions are naturally motivated by robustness
issuesinlifesciencesandapplicationsingamesorsecurity, orwhendealingwithamisspecifiedmodel(Hotho,
2022; Golrezaei et al., 2021; Singh & Upadhyaya, 2012). Hence the combination of corrupted and heavy-tail
distributions naturally appears at the crossing of these classical application domains.
Bandits with Stochastic Corruption is different from the adversarial bandit setting (Auer et al., 2002). The
adversarial bandit assumes existence of a non-stochastic adversary that can return at each step the worst-
case reward to the agent depending on its history of choices. Incorporating corruptions in this setting,
Lykouris et al. (2018) and Bogunovic et al. (2020) consider settings where the rewards can be corrupted by
a history-dependent adversary but the total amount of corruption and also the corruptions at each step are
bounded. In contrast to the adversarial corruption setting in the literature, we consider a non-adversarial
proportion of corruptions ( ε∈[0,1/2)) at each step, which are stochastically generated from unbounded
corruption distributions/parenleftbig
{Hi}k
i=1/parenrightbig
. To the best of our knowledge, only Kapoor et al. (2019) have studied
similar non-adversarial corruption setting with a history-independent proportion of corruption at each step
for regret minimization. But they assume that the probable corruptions at each step are bounded, and the
uncorrupted rewards are sub-Gaussian. On the other hand, Altschuler et al. (2019) study the same stochastic
unbounded corruption that we consider but they focus on best arm identification using the median of the
arms as a goal making this a different problem. Hence, we observe that there is a gap in the literature in
studying unbounded stochastic corruption for bandits with possibly heavy-tailed rewards and this article aims
to fill this gap. Specifically, we aim to deal with unbounded corruption and heavy-tails simultaneously, which
requires us to develop a novel sensitivity analysis of the robust estimator in lieu of a worst-case (adversarial
bandits) analysis.
Our Contributions. Specifically, in this paper, we aim to investigate three main questions:
1. Is the setting of Bandits with Stochastic Corruption with unbounded corruptions and heavy tails
fundamentally harder (in terms of the regret lower bound) than the classical sub-Gaussian and
uncorrupted bandit setting?
2. Isitpossibletodesignan efficient and robust algorithm thatachievesanorder-optimalperformance
(logarithmic regret) in the stochastic corruption setting?
3. Are robust bandit algorithms efficient in practice ?
These questions have led us to the following contributions:
1. Hardness of Bandits with Stochastic Corruption with unbounded corruptions and heavy tails. In order to
understand the fundamental hardness of the proposed setting, we use a suitable notion of regret (Kapoor
et al., 2019), denoted by Rn(Definition 1), that extends the traditional pseudo-regret (Lattimore &
Szepesvári, 2020) to the corrupted setting. Then, in Section 4, we derive lower bounds on regret that reveal
increased difficulties of corrupted bandits with heavy tails in comparison with the classical non-corrupted
and light-tailed Bandits. (a) In the heavy-tailed regime (3), we show that even when the suboptimality gap
2Published in Transactions on Machine Learning Research (01/2024)
∆i1for armiis large, the regret increases with ∆ibecause of the difficulty to distinguish between two arms
when the rewards are heavy-tailed. (b) Our lower bounds indicate that when ∆iis large, the logarithmic
regret is asymptotically achievable, but the hardness depends on the corruption proportion ε, variance of Pi,
denoted by σ2
i, and the suboptimality gap ∆i. Specifically, if∆i
σi’s are small, i.e. we are in low distinguisha-
bility/high variance regime, the hardness is dictated byσ2
i
∆2
i,ε. Here, ∆i,ε≜∆i(1−ε)−2εσiis the ‘corrupted
suboptimality gap ’ that replaces the traditional suboptimality gap ∆iin the lower bound of non-corrupted
and light-tailed bandits (Lai & Robbins, 1985). Since ∆i,ε≤∆i, it is harder to distinguish the optimal and
suboptimal arms in the corrupted settings. They are the same when the corruption proportion ε= 0. In
this article, we exclude the case when ∆i,ε≤0as it essentially corresponds to the case when corruption is
large enough to render reward distributions hard to distinguish. Hence we limit our study to the case when
∆i,ε>0.
Additionally, our analysis partially addresses an open problem in heavy-tailed bandits. Works on heavy-
tailed bandits (Bubeck et al., 2013; Agrawal et al., 2021) rely on the assumption that a bound on the
(1+η)-moment, i.e. E[|X|1+η], is known for some η >0. We do not assume such a restrictive bound, as
knowing a bound on E[|X|1+η]implies the knowledge of a bound on the differences between the means of
the reward of the different arms. Instead, we assume that the centered moment, specifically the variance,
is bounded by a known constant. Thus, we partly address the open problem of Agrawal et al. (2021) by
relaxing the classical bounded (1+η)-moment assumption with the bounded centered moment one, for η≥1.
2. Robust and Efficient Algorithm Design. In Section 5, we propose a robust algorithm, called HuberUCB ,
that leverages the Huber’s estimator for robust mean estimation using the knowledge of εand a bound on
the variances of inliers. We derive a novel concentration inequality on the deviation of empirical Huber’s
estimate that allows us to design robust and tight confidence intervals for HuberUCB . In Theorem 3, we show
that HuberUCB achieves the logarithmic regret, and also the optimal rate when the sub-optimality gap ∆is
not too large. We show that for HuberUCB ,Rncan be decomposed according to the respective values of ∆i
andσi:
Rn≤ O
/summationdisplay
i:∆i>σilog(n)σi

/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
Error due to heavy-tail+O
/summationdisplay
i:∆i≤σilog(n)∆iσ2
i
∆2
i,ε

/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
σ2/∆error with corrupted sub-optimality gaps.
Thus, our upper bound allows us to segregate the errors due to heavy-tail, corruption, and corruption-
correction with heavy tails. The error incurred by HuberUCB can be directly compared to the lower bounds
obtained in Section 4 and interpreted in both the high distinguishibility regime and the low distinguishibility
regime as previously mentioned.
3. Empirically Efficient and Robust Performance. To the best of our knowledge, we present the first
robust mean estimator that can be computed in a linear time in a sequential setting (Section 6). Existing
robust mean estimators, such as Huber’s estimator, need to be recomputed at each iteration using all the
data, which implies a quadratic complexity. Our proposal recomputes Huber’s estimator only when the
iteration number is a power of 2and computes a sequential approximation on the other iterations. We
use the Sequential Huber’s estimator to propose SeqHuberUCB . We theoretically show that SeqHuberUCB
achieves similar order of regret as HuberUCB , while being computationally efficient. In Section 7, we also
experimentally illustrate that HuberUCB and SeqHuberUCB achieve the claimed performances for corrupted
Gaussian and Pareto environments.
We further elaborate on the novelty of our results and position them in the existing literature in Section 2.
For brevity, we defer the detailed proofs and the parameter tuning to Appendix.
1The suboptimality gap of an arm is the difference in mean rewards of an optimal arm and that arm. In our context
suboptimality gap refer to the gap between the inlier distributions which can be compared to suboptimality gap in heavy-tail
setting, as opposed to the corrupted gap that we define later.
3Published in Transactions on Machine Learning Research (01/2024)
2 Related Work
Due to the generality of our setting, this work either extends or relates to the existing approaches in both
the heavy-tailed and corrupted bandits literature. While designing the algorithm, we further leverage the
literature of robust mean estimation. In this section, we connect to these three streams of literature. Table 1
summarizes the previous works and posits our work in lieu.
Algorithms Settings Corruption Type of outliers Heavy-tailed Adversarial/
Stochastic
Our work MAB Yes Unbounded Yes Stochastic
Bubeck et al. (2013);
Agrawal et al. (2021); Lee
et al. (2020)MAB No x Yes Stochastic
Lykouris et al. (2018) MAB Yes Bounded No Stochastic
Bogunovic et al. (2020) GP Bandits Yes Bounded No Adversarial
Kapoor et al. (2019)MAB &
Linear
BanditsYes Bounded No Stochastic
Medina & Yang (2016);
Shao et al. (2018)Linear
BanditsNo x Yes Stochastic
Bouneffouf (2021)Contextual
Banditscontext only Unbounded No Stochastic
Agarwal et al. (2019) Control Yes Bounded x Adversarial
Hajiesmaili et al. (2020);
Auer et al. (2002); Pogodin
& Lattimore (2020)MAB Yes Bounded x Adversarial
Table 1: Comparison of existing results on Corrupted and Heavy-tailed Bandits.
Heavy-tailed bandits. Bubeck et al. (2013) are one of the first to study robustness in multi-armed bandits by
studying the heavy-tailed rewards. They use robust mean estimator to propose the RobustUCB algorithms.
They show that under assumptions on the raw moments of the reward distributions, a logarithmic regret
is achievable. It sprouted research works leading to either tighter rates of convergence (Lee et al., 2020;
Agrawal et al., 2021), or algorithms for structured environments (Medina & Yang, 2016; Shao et al., 2018).
Our article uses Huber’s estimator which was already discussed in Bubeck et al. (2013). However, the chosen
parameters in Bubeck et al. (2013) were suited for heavy-tailed distributions, and thus, render their proposed
estimator non-robust to corruption. We address this gap in this work.
Corrupted bandits. The existing works on Corrupted Bandits (Lykouris et al., 2018; Bogunovic et al., 2020;
Kapoor et al., 2019) are restricted to bounded corruption. When dealing with bounded corruption, one can
use techniques similar to adversarial bandits (Auer et al., 2002) to deal with an adversary that can’t corrupt
an arm too much. The algorithms and proof techniques are fundamentally different in our article because the
stochastic (or non-adversarial) corruption by Nature allows us to learn about the inlier distribution on the
condition that corresponding estimators are robust. Thus, our bounds retain the problem-dependent regret,
while successfully handling probably unbounded corruptions with robust estimators .
Robust mean estimation . Our algorithm design leverages the rich literature of robust mean estimation,
specifically the influence function representation of Huber’s estimator. The problem of robust mean estima-
tion in a corrupted and heavy-tailed setting stems from the work of Huber (Huber, 1964; 2004). Recently,
in tandem with machine learning, there have been numerous advances both in the heavy-tailed (Devroye
et al., 2016; Catoni, 2012; Minsker, 2019), and in the corrupted settings (Lecué & Lerasle, 2020; Minsker
& Ndaoud, 2021; Prasad et al., 2019; 2020; Depersin & Lecué, 2022; Lerasle et al., 2019; Lecué & Lerasle,
2020). Our work, specifically the novel concentration inequality for Huber’s estimator, enriches this line of
work with a result of parallel interest. We also introduce a sequential version of Huber’s estimator achieving
linear complexity.
4Published in Transactions on Machine Learning Research (01/2024)
3 Bandits with Stochastic Corruption: Problem formulation
In this section, we present the corrupted bandits setting that we study, together with the corresponding
notion of regret. Similarly to the classical bandit setup, the regret decomposition lemma allows us to focus
ontheexpectednumberofpullsofasuboptimalarmasthecentralquantitytocontrolalgorithmicstandpoint.
Notations. We denote byPthe set of probability distributions on the real line Rand byP[q](M)≜{P∈
P:EP[|X−EP[X]|q]≤M}the set of distributions with qthmoment,q≥1, bounded by M > 0.1{A}is
the indicator function for the event Abeing true. We denote the mean of a distribution Piasµi≜EPi[X].
For anyD⊂P, we denoteD(ε)≜{(1−ε)P+εH:P∈D,H∈P}the set of corrupted distributions from D.
Problem Formulation. In the setting of Bandits with Stochastic Corruption , a bandit algorithm faces
an environment with k∈Nmany reward distributions in the form νε= (νε
i)k
i=1whereνε
i= (1−ε)Pi+εHi
denotes the distribution of rewards of arm i. HerePi,Hiare real-valued distributions and εis a mixture
parameter assumed to be in [0,1/2), that isPiis given more weights than Hiin the mixture of arm i. For
this reason, the{Pi}k
i=1are called the inlierdistributions and the {Hi}k
i=1theoutlierdistributions. We
assume the inlier distributions have at least 2finite moments that is P1,...,Pk∈P[2](M)for someM > 0,
while no restriction is put on the outlier distributions, that is H1,...,Hk∈P. For this reason, we also refer
to the outlier distributions as the corrupted distributions, and to the inlier distributions as the non-corrupted
ones.εis called the level of corruption. For convenience, we further denote by νin lieu ofν0= (Pi)k
i=1the
reward distributions of the non-corrupted environment.
The game proceed as follows: At each step t∈{0,...,n}, the agent policy πinteracts with the corrupted
environmentbychoosinganarm Atandobtainingastochasticallycorruptedreward. Togeneratethisreward,
Nature first draws a random variable Ct∈{0,1}from a Bernoulli distribution with mean ε∈[0,1/2).
IfCt= 1, it generates a corrupted reward Ztfrom distribution HAtcorresponding to the chosen arm
At∈{1,...,k}. Otherwise, it generates a non-corrupted X′
tfrom distribution PAt. More formally, Nature
generates reward Xt=X′
t1{Ct= 0}+Zt1{Ct= 1}which the learner observes. The learner leverages
this observation to choose another arm at the next step in order to maximize the total cumulative reward
obtained after nsteps. In Algorithm 1, we outline a pseudocode of this framework.
Algorithm 1 Bandits with Stochastic Corruption
Require:ε∈[0,1/2),q≥2andM > 0
1:Input:P1,...,Pk∈ P [q](M)be the uncorrupted reward distributions and H1,...,Hk∈ Pbe the
corrupted reward distributions.
2:fort= 1,...,ndo
3:Player plays an arm At∈{1,...,k}
4:Nature draws a Bernoulli Ct∼Ber(ε)
5:Generate a corrupted reward Zt∼HAtand an uncorrupted reward X′
t∼PAt
6:Player observe the reward Xt=X′
t1{Ct= 0}+Zt1{Ct= 1}
7:end for
Remark 1 (Non-adversarial corruption.) In the setting of Bandits with Stochastic Corruption, we con-
sider that the reward received by the learner is corrupted when Ct= 1and non-corrupted otherwise. Since
the law ofCtis a Bernoulli Ber(ε), the corruption is stochastic, and independent on other variables. This
is in contrast with adversarial setups, where corruption is typically chosen by an opponent and possibly de-
pending on other variables. Assuming a non-adversarial behavior of the Nature seems more justified than
assuming an adversarial setup in applications, such as agriculture where corruption is often due to external
disturbances, such as pests appearance or weather hazards, whose occurrence are typically non-adversarial.
Now when corruption happens, we do not put restriction on the level of corruption. For example, we can
imagine a pest outburst or hail, that may have huge impact on a crop but does not occur adversarially.
Remark 2 (Weak assumption on inliers) Let us highlight that we do not assume sub-Gaussian behavior
for the inlier distributions Pi. Instead, we consider only a weak moment assumption, i.e. the inlier distri-
5Published in Transactions on Machine Learning Research (01/2024)
butionsPihave a finite variance . Thus, our setting is capable of modeling both the moderately heavy-tailed
setting and the corrupted settings. We highlight this generality in the regret lower bounds and empirical
performance analysis in Section 4 and 7.
Corrupted regret. In this setting, we observe that a corrupted reward distribution ( (1−ε)Pi+εHi) might
not have a finite mean, unlike the true Pi’s. Thus, the classical notion of regret with respect to the corrupted
reward distributions might fail to quantify the goodness of the policy and its immunity to corruption while
learning. On the other hand, in this setup, the natural notion of expected regret is measured with respect
to the mean of the non-corrupted environment νspecified by{Pi}k
i=1.
Definition 1 (Corrupted Regret) In Bandits with Stochastic Corruption, we define the regret of a learn-
ing algorithm playing strategy πafternsteps of interaction with the environment νεagainst constantly
playing an optimal arm ⋆∈arg min
iEPi[X′]as
Rn(π,νε)≜nmax
iEPi[X′]−E/bracketleftiggn/summationdisplay
t=1X′
t/bracketrightigg
. (Corrupted regret)
We call this quantity the pseudo-regret under corrupted observation, or for short, the corrupted regret .
The expectation is crucially taken on X′
i∼PiandX′
t∼PAtbut not on XiandXt. The expectation on the
right also incorporates possible randomization from the learner. Thus, (Corrupted regret) quantifies the loss
in the rewards accumulated by policy πfrom the inliers while learning only from the corrupted rewards and
also not knowing the arm with the best true reward distribution. Thus, this definition of corrupted regret
quantifies the rate of learning of a bandit algorithm as regret does for non-corrupted bandits. A similar
notion of regret is considered in (Kapoor et al., 2019) that deals with bounded stochastic corruptions.
Due to the non-adversarial nature of the corruption, the regret can be decomposed, as in classical stochastic
bandits, to make appear the expected number of pulls of suboptimal arms Eνε[Ti(n)], which allow us to
focus the regret analysis on bounding these terms.
Lemma 1 (Decomposition of corrupted regret) In a corrupted environment νε, the regret writes
Rn(π,νε) =k/summationdisplay
i=1∆iEνε[Ti(n)],
whereTi(n)≜/summationtextn
t=11{At=i}denotes the number of pulls of arm iuntil timenand the problem-dependent
quantity ∆i≜max
jµj−µiis called the suboptimality gap of arm i.
4 Lower bounds for uniformly good policies under heavy-tails and corruptions
In order to derive the lower bounds, it is classical to consider uniformly good policies on some family of envi-
ronments, Lai & Robbins (1985). We introduce below the corresponding notion for corrupted environments
with the set of laws D⊗k=D1⊗···⊗D k, whereDi⊂Pfor eachi∈{1,...,k}.
Definition 2 (Robust uniformly good policies) LetD⊗k(ε) =D1(ε)⊗···⊗D k(ε)be a family of cor-
rupted bandit environments on R. For a corrupted environment νε∈D⊗k(ε)with corresponding uncorrupted
environment ν, letµi(ν)denote the mean reward of arm iin the uncorrupted setting and µ⋆(ν)≜max
aµi(ν)
denote the maximum mean reward. A policy πis uniformly good on D⊗k(ε)if for anyα∈(0,1],
∀ν∈D⊗k(ε),∀i∈{1,...,k},µi(ν)<µ⋆(ν)⇒Eνε[Ti(n)] =o(nα).
Since the corrupted setup is a special case of stochastic bandits, a lower bound can be immediately recovered
with classical results, such as Lemma 2 below, that is a version of the change of measure argument (Burnetas
& Katehakis, 1997), and can be found in (Maillard, 2019, Lemma 3.4).
6Published in Transactions on Machine Learning Research (01/2024)
Lemma 2 (Lower bound for uniformly good policies) LetD⊗k=D1⊗···⊗D k, whereDi⊂Pfor
eachi∈{1,...,k}and letν∈D⊗k. Then, any uniformly good policy on D⊗kmust pull arms such that,
∀i∈{1,...,k}, µ(νi)≤µ⋆(ν)⇒ lim inf
n→∞Eν[Ti(n)]
log(n)≥1
Ki(νi,µ(P⋆)).
withKi(νi,µ(P⋆)) = inf{DKL(νi,P∗) :νi∈Di,µ(νi)≥µ(P⋆)}whereDKLdenotes the Kullback-Leibler
divergence between distributions.
Lemma 2 is used in the traditional bandit literature to obtain lower bound on the regret using the decom-
position of regret from Lemma 1. In our setting, however, the lower bound is more complex as it involves
optimization onP[2](M), the set of distributions with a variance bounded by M > 0, and this set is not
convex. Indeed, for example taking the convex combination of two Dirac distributions, both distributions
have variance 0but depending on where the Dirac distribution are located the variance of the convex combi-
nation is arbitrary. It also involves an optimization in both the first and second term of the KL because we
consider the worst-case corruption in both the optimal arm distribution ν⋆and non-optimal arm distribution
νi. In this section, we do not solve these problems, but we propose lower bounds derived from the study
of a specific class of heavy-tailed distributions on one hand (Lemma 3) and the study of a specific class of
corrupted (but not heavy-tailed) distributions on the other hand (Lemma 4).
Using the fact that Ki(νi,µ(P⋆))is an infimum that is smaller than the DKLfor the choice ν=P⋆, Lemma 2
induces the following weaker lower-bound:
∀i∈{1,...,k}, µ(νi)≤µ⋆(ν)⇒ lim inf
n→∞Eν[Ti(n)]
log(n)≥1
DKL(νi,P⋆). (1)
Equation (1) shows that it is sufficient to have an upper bound on the DKL-divergence of the reward distri-
butions interacting with the policy to get a lower bound on the number of pulls of a suboptimal arm .
In order to bound the DKL-divergence, we separately focus on two families of reward distributions, namely
Student’s distribution without corruption (Lemma 3) and corrupted Bernoulli distribution (Lemma 4), that
reflect the hardness due to heavy-tails and corruptions, respectively. Applying Lemma 3 and Lemma 4 in
Equation (1) yields the final regret lower bound in Theorem 1.
Shifted Student’s distribution without corruption. To obtain a lower bound in the heavy-tailed case
we use shifted Student distributions. Student distribution are well adapted because they exhibit a finite
number of finite moment which makes them heavy-tailed, and we can easily change the mean of Student
distribution by adding a shift without changing its shape parameter d. We denote byTdthe set of shifted
Student distributions with ddegrees of freedom,
Td=/braceleftigg
P∈P:∃µ∈RPhas distribution p:t∈R∝⇕⊣√∫⊔≀→Γ(d+1
2)
Γ(d/2)√
dπ/parenleftbigg
1 +(t−µ)2
d/parenrightbigg−d+1
2/bracerightigg
.
Lemma 3 (Control of KL-divergence for Heavy-tails) LetP1,P2be two shifted Student distributions
withd>1degrees of freedom with EP1[X] = 0andEP2[X] = ∆>0. Then,
DKL(P1,P2)≤

3d−1(d+1)2∆2
5√
dif∆≤1,
(d+ 1) log (∆) + log/parenleftig
3d(d+1)2
5√
d/parenrightig
if∆>1.(2)
Corrupted Bernoulli distributions. We denoteBp(ε) ={(1−ε)P+εH;H∼Ber(p′)andP∼
Ber(p),p′∈[0,1]}the corrupted neighborhood of the Bernoulli distribution Ber(p). LetP0∈Ber(p0)
andP1∈Ber(p1)for somep0,p1∈(0,1)be two Bernoulli distributions. We corrupt both P0andP1
with a proportion ε > 0to getQ0∈ Bp0(ε)andQ1∈ Bp1(ε). We obtain Lemma 4 that illustrates
three bounds on DKL(Q0,Q1)as functions of the sub-optimality gap ∆≜EP0[X]−EP1[X], variance
σ2≜VarP0(X) = VarP1(X), and corruption proportion ε.
7Published in Transactions on Machine Learning Research (01/2024)
Figure 1: Visualizing the KL and the corresponding bounds in Lemma 4 for σ= 1andε= 0.2(xaxis is in
log scale).
Lemma 4 (Control of KL-divergence for Corruptions) LetP0∈Ber(p0)andP1∈Ber(p1)be two
Bernoulli probability distributions with means p0,p1∈(0,1), such that ∆ =EP0[X]−EP1[X] =p0−p1,
σ2= VarP0(X) = VarP1(X)>0, and ∆≥2σε
1−ε. Then, there exists Q0∈Bp0(ε)andQ1∈Bp1(ε), that have
corrupted suboptimality gap given by ∆ε=EQ0[X]−EQ1[X] = ∆(1−ε)−2εσ, such that
•Uniform Bound. Without further assumptions on ∆andσ, we have
DKL(Q0,Q1)≤(1−2ε) log/parenleftbigg
1 +1−2ε
ε/parenrightbigg
. (3)
•High Distinguishability/Low Variance Regime. If2σε√1−2ε<∆<2σ, we get
DKL(Q0,Q1)≤∆ε
2σlog/parenleftigg
1 +∆ε
2σ−∆ε/parenrightigg
. (4)
•Low Distinguishability/High Variance Regime. If∆≤2σε√1−2ε, there exists ε′≤εandQ′
0∈
Bp0(ε′),Q′
1∈Bp1(ε′)such thatDKL(Q′
0,Q′
1) = 0.
Note that the corrupted sub-optimality gap ∆εshould not be confused with the sub-optimality gap ∆. Note
also that due to the assumption on the variance in Lemma 4, we must have p0= 1−p1≥1/2. The specific
pair of distributions mentioned Q0,Q1,Q′
0,Q′
1can be found in the proof of the lemma, Section B.1.3.
Consequences of Lemma 4. We illustrate the bounds of Lemma 4 in Figure 1. The three upper bounds
on the KL-divergence of corrupted Bernoullis provide us some insights regarding the impact of corruption.
1.Three Regimes of Corruption: We observe that, depending on ∆/σ, we can categorize the corrupted
environment in three categories. For ∆/σ∈[2,+∞), we observe that the KL-divergence between corrupted
distributions Q0andQ1is upper bounded by a function of only corruption proportion εand is independent
of the uncorrupted distributions. Whereas for ∆/σ∈(2ε/√1−2ε,2), the distinguishability of corrupted
distributions depend on the distinguishibility of uncorrupted distributions and also the corruption level. We
call this the High Distinguishability/Low Variance Regime. For ∆/σ∈[0,2ε/√1−2ε], we observe that the
KL-divergence can always go to zero. We refer to this setting as the Low Distinguishability/High Variance
Regime.
8Published in Transactions on Machine Learning Research (01/2024)
2.High Distinguishability/Low Variance Regime: InLemma4,weobservethattheeffectivegaptodistinguish
the optimal arm to the closest suboptimal arm that dictates hardness of a bandit instance has shifted from
the uncorrupted gap ∆to acorrupted suboptimality gap :∆ε≜∆(1−ε)−2εσ.
3.Low Distinguishability/High Variance Regime: We notice also that there is a limit for ∆below which the
corruption can make the two distributions Q0andQ1indistinguishable, this is a general phenomenon in the
setting of testing in corruption neighborhoods (Huber, 1965).
4.Feasibility of the Bandits with Stochastic Corruption problem and ∆ε:In Lemma 4, we have assumed ∆ε
to be positive. If ∆εis negative or zero, i.e.∆
2σ≤ε
1−ε, we cannot achieve better than linear regret in the
corresponding Bandits with Stochastic Corruption problem. Lemma 4 additionally shows that we have to
concede linear regret even when ∆εis positive but∆
2σ≤ε√1−2ε.
From KL Upper bounds to Regret Lower Bounds. Substituting the results of Lemma 3 and 4 in
Equation (1) yield the lower bounds on regret of any uniformly good policy in heavy-tailed and corrupted
settings, where reward distributions either belong to the class of corrupted student distributions or the class
of corrupted Bernoulli distributions, respectively. We denote
D⊗k
T2≜T2⊗···⊗T 2,
whereT2is the set of Student distributions with more than 2degrees of freedoms. We also define
D⊗k
B(ε)≜B(ε)⊗···⊗B (ε),
whereB(ε) ={(1−ε)P+εH;H∼Ber(p)andP∼Ber(p′),p,p′∈[0,1]}is the set of corrupted Bernoulli
distributions.
Theorem 1 (Lower bound for heavy-tailed and corrupted bandit) Letibe a suboptimal arm such
thatEPi[X]≤max
aEPa[X]and denote ∆i≜max
aEPa[X]−EPi[X]and∆i,ε≜∆i(1−ε)−2εσi, suppose
∆i,ε>0.
Student’s distributions. Suppose that the arms are pulled according to a policy that is uniformly good on
D⊗k
T2. Then, for all ν∈D⊗k
T2,
lim inf
n→∞Eν[Ti(n)]
log(n)≥σ2
i
51∆2
i∨1
4 log(∆i/σi) + 22. (5)
Corrupted Bernoulli distributions : Suppose that the arms are pulled according to a policy that is uni-
formly good on D⊗k
B(ε). Then, for all νε∈D⊗k
B(ε)such that 2σiε√1−2ε<∆i<2σi, then
lim inf
n→∞Eνε[Ti(n)]
log(n)≥2σi
∆i,εlog/parenleftig
1 +∆i,ε
2σi−∆i,ε/parenrightig, (6)
and for ∆i>2σi,
lim inf
n→∞Eνε[Ti(n)]
log(n)≥1
(1−2ε) log/parenleftbig1−ε
ε/parenrightbig. (7)
For brevity, the detailed proof is deferred to Appendix A.1.
Small gap versus large gap regimes. Due to the restriction in the family of distributions considered
in Theorem 1, the lower bounds are not tight and may not exhibit the correct rate of convergence for all
families of distributions. However, this theorem provides some insights about the difficulties that one may
encounter in corrupted and heavy-tail bandits problems, including the logarithmic dependence on n.
In Theorem 1, if ∆iis small, we see that in the heavy-tailed case (Student’s distribution), we recover a term
very similar to the lower bound when the arms are from a Gaussian distribution. Now, in the case where
9Published in Transactions on Machine Learning Research (01/2024)
∆iis large, the number of suboptimal pulls in the heavy-tail setting is Ω/parenleftig
1/log/parenleftig
∆i
σi/parenrightig/parenrightig
. This is the price to
pay for heavy-tails.
If we are in the high distiguishability/low variance regime, i.e.∆i,ε
2σi∈(ε√1−2ε,1), we recover a logarithmic
lower bound which depends on a corrupted gap between means ∆i,ε= ∆i(1−ε)−2εσi. Since the corrupted
gap is always smaller than the true gap ∆i, this indicates that a corrupted bandit ( ε>0) must incur higher
regret than a uncorrupted one ( ε= 0). Forε= 0, this lower bound coincides with the lower bound for
Gaussians with uncorrupted gap of means ∆iand variance σ2
i. On the other hand, if∆i,ε
2σiis larger than 1,
we observe that we can still achieve logarithmic regret but the hardness depends on only the corruption level
ε, specifically1
(1−2ε) log(1−ε
ε).
5 Robust bandit algorithm: Huber’s estimator and upper bound on the regret
Inthissection, weproposeanUCB-typealgorithm, namely HuberUCB ,addressingtheBanditswithStochastic
Corruption problem (Algorithm 2). This algorithm uses primarily a robust mean estimator called Huber’s
estimator (Section 5.1) and corresponding confidence bound to develop HuberUCB (Section 5.2). We further
provide a theoretical analysis in Theorem 3 leading to upper bound on regret of HuberUCB . We observe that
the proposed upper bound matches the lower bound in Theorem 1 under some settings.
5.1 Robust mean estimation and Huber’s estimator
We begin with a presentation of the Huber’s estimator of mean (Huber, 1964).
As we aim to design a UCB-type algorithm, the main focus is to obtain an empirical estimate of the mean
rewards. Since the rewards are heavy-tailed and corrupted in this setting, we have to use a robust estimator
of mean. We choose to use Huber’s estimator (Huber, 1964), an M-estimator that is known for its robustness
properties and have been extensively studied (e.g. the concentration properties (Catoni, 2012)).
Huber’sestimatorisanM-estimator, whichmeansthatitcanbederivedasaminimizerofsomelossfunction.
Given access to ni.i.d. random variables Xn
1≜{X1,...,Xn}, we define Huber’s estimator as
Hubβ(Xn
1)∈arg min
θ∈Rn/summationdisplay
i=1ρβ(Xi−θ), (8)
whereρβis Huber’s loss function with parameter β >0.ρβis a loss function that is quadratic near 0and
linear near infinity, with βthresholding between the quadratic and linear behaviors.
In the rest of the paper, rather than using the aforementioned definition, we represent the Huber’s estimator
as a root of the following equation (Mathieu, 2022):
n/summationdisplay
i=1ψβ(Xi−Hubβ(Xn
1)) = 0. (9)
Here,ψβ(x)≜x1{|x|≤β}+βsign(x)1{|x|>β}is called the influence function. Though the representations
inEquation(8)and(9)areequivalent, weprefertouserepresentationEquation(9)asweprovetheproperties
of Huber’s estimator using those of ψβ.
βplays the role of a scaling parameter. Depending on β, Huber’s estimator exhibits a trade-off between the
efficiency of the minimizer of the square loss, i.e. the empirical mean, and the robustness of the minimizer
of the absolute loss, i.e. the empirical median.
5.2 Concentration of Huber’s estimator in corrupted setting
Let use denote the true Huber mean for a distribution PasHubβ(P). This means that, for a random
variableYwith lawP,Hubβ(P)satisfies E[ψβ(Y−Hubβ(P))] = 0.
10Published in Transactions on Machine Learning Research (01/2024)
We now state our first key result on the concentration of Huber’s estimator around Hubβ(P)in a corrupted
and heavy-tailed setting.
Theorem 2 (Concentration of Empirical Huber’s estimator) Suppose that X1,...,Xnare i.i.d.
with law (1−ε)P+εHfor someP,H∈Pand proportion of outliers ε∈(0,1/2), andPhas a finite
varianceσ2. Then, with probability larger than 1−5δ,
|Hubβ(Xn
1)−Hubβ(P)|≤σ/radicalig
2 ln(1/δ)
n+βln(1/δ)
3n+ 2βε/radicalig
ln(1/δ)
n+ 2βε
/parenleftbigg
p−/radicalig
ln(1/δ)
2n−ε/parenrightbigg
+.
Here,p=PP(|Y−EP[Y]|≤β/2)withp>5ε,β >4σ,ε=/radicalbigg
(1−2ε)
log(1−ε
ε), andδ≥exp/parenleftbigg
−n128(p−5ε)2
49(1+2ε√
2)2/parenrightbigg
.
Theorem 2 gives us the concentration of Hubβ(Xn
1)around Hubβ(P), i.e. the Huber functional of the
inlierdistribution P. This theorem allows us to construct a UCB-type algorithm to solve the Bandits with
Stochastic Corruption.
For convenience of notation, hereafter, we denote the rate of convergence of Hubβ(Xn
1)toHubβ(P)as
rn(δ)≜σ/radicalig
2 ln(1/δ)
n+βln(1/δ)
3n+ 2βε/radicalig
ln(1/δ)
n+ 2βε
/parenleftbigg
p−/radicalig
ln(1/δ)
2n−ε/parenrightbigg
+. (10)
Discussion. Now, we provide a brief discussion on the implications of Theorem 2.
1. Value of p:For most laws that exhibit concentration properties, the constant pis close to 1asβ≥4σ.
One might also use Markov inequality to lower bound p, depending on the number of finite moments Phas.
Boundingpthen becomes a trade-off on the value of β, where large values of βimplies that pis close to 1.
But largerβalso leads to a less robust estimator, since the error bound in Theorem 2 increases with β.
2. Tightness of constants: If there are no outliers ( ε= 0), the optimal rate of convergence in such a setting
is at least of order σ/radicalbig
2 ln(1/δ)/ndue to the central limit theorem. Theorem 2 shows that we are very close
to attaining this optimal constant in the leading 1/√nterm. This result for Huber’s estimator echoes the
one presented in Catoni (2012).
3. Value of β:βis a parameter that achieve a trade-off between accuracy in the light-tailed uncorrupted
setting and robustness. For our result, βmust be at least of the order of 4σ. We provide a detailed discussion
on the choice of βin Section 5.4.
4. Restriction on the values of δ:In Theorem 2, δmust be at least of order e−n. This restriction may seem
arbitrary but it is in fact unavoidable as shown in Theorem 4.3 of Devroye et al. (2016). This is a limitation
of robust mean estimation that enforces our algorithm to perform a forced exploration in the beginning.
5. Restriction on the values of ε:In Theorem 2, εcan be at most p/5, which implies that it is smaller
than 1/5. This restriction is common in robustness literature. In particular, in Kapoor et al. (2019), εis
supposed smaller than ∆/σ. In robustness literature, Lecué & Lerasle (2020) and Dalalyan & Thompson
(2019) assumed that ε≤1/768and1/400respectively. In contrast, our analysis can handle εup to 0.2,
which is significantly higher than the existing restrictions.
Bias of Huber’s Estimate. IfPis symmetric, we have Hubβ(P) =E[X]. WhenPis non-symmetric,
we need to control the distance of the Huber’s estimate from the true mean, i.e. |Hubβ(P)−E[X]|. We call
it the bias of Huber’s estimate. We need to bound this bias to get a concentration of the empirical Huber’s
estimate Hubβ(Xn
1)around the true mean E[X]. We control the bias using the following lemma, which is a
direct consequence of Lemma 4 from Mathieu (2022).
11Published in Transactions on Machine Learning Research (01/2024)
Lemma 5 (Bias of Huber’s estimator) LetYbe a random variable with E[|Y|q]<∞forq≥2and
suppose that β2≥9Var(Y). Then
|E[Y]−Hubβ(P)|≤2E[|Y−E[Y]|q]
(q−1)βq−1.
Using Lemma 5 and Theorem 2, we can control the deviations of Hubβ(Xn
1)fromE[X]. This allows us to
formulate an index-based algorithm (UCB-type algorithm) for corrupted Bandits. We present this algorithm
in Section 5.3.
5.3 HuberUCB : Algorithm and regret bound
In this section, we describe a robust, UCB-type algorithm called HuberUCB . We denote µias the mean of arm
iand its variance as σ2
i. We assume that we know the variances of the reward distributions, i.e. {σ2
i}k
i=1,
and hence, we define by construction M≜max
iσ2
i. We refer to Section 5.4 for a discussion on the choice of
the parameters when the reward distributions are unknown.
HuberUCB: The algorithm. In order to deploy the Huber’s estimator in the multi-armed bandits setting,
we need to estimate the mean of the rewards of each arm separately. We do that by defining a parameter βi
for each arm and estimating separately each µiusing
Hubi,s= Hubβi(Xt,1≤t≤ssuch that At=i,).
Now, at each step t, we define a confidence bound for arm iwithsnumber of pulls as
Bi(s,t)≜/braceleftigg
rs(1/t2) +biifs≥slim(t)
∞ ifs<slim(t), (11)
wherers(1/t2)isdefinedbyEquation(10), slim(t) = log(t)98
128(p−5ε)2/parenleftig
1 + 2√
2/parenleftig
ε∨9
14√
2/parenrightig/parenrightig2
,ε=/radicalbigg
(1−2ε)
log(1−ε
ε),
andbiis a bound on the bias |E[X]−Hubβi(Pi)|. Hencebican be set to zero if Piis known to be symmetric
and controlled by Lemma 5 otherwise. Here, we assign bi= 2σ2
i/βias a conservative choice by imposing
q= 2, i.e. finite second moment, in Lemma 5.
Now, we propose HuberUCB that selects an arm atat steptbased on the index
IHuberUCB
i (t) = Hubi,Ti(t−1)+Bi(Ti(t−1),t). (12)
The index of HuberUCB together with the confidence bound defined in Equation (11) dictates that if an arm
is less explored, i.e. Ti(t−1)<slim(t), we choose that arm, and if multiple arms satisfy this, we break the
tie randomly. As tgrows and for all the arms Ti(t−1)≥slim(t)is satisfied, we choose the arms according
to the adaptive bonus. Thus, HuberUCB induces an initial forced exploration to obtain confident-enough
robust estimates, followed by a time-adaptive selection of arms. We present a pseudocode of HuberUCB in
Algorithm 2. We discuss the choices of the hyperparameters and the computational details in Section 5.4.
Algorithm 2 HuberUCB
Require: Parameterε∈[0,1/2)andβi>4σifor alli≤K
1:fort= 1,...,ndo
2:Compute index IHuberUCB
i (t)(Equation (12)) for i∈{1,...,k}usingX1,...,Xt−1.
3:Choose arm at∈arg maxiIi(t).
4:Observe a reward Xt.
5:end for
Regret Analysis. Now, we provide a regret upper bound for HuberUCB .
12Published in Transactions on Machine Learning Research (01/2024)
Theorem 3 (Upper Bound on number of pulls of suboptimal arms with HuberUCB)Let us con-
sider a set of kreward distributions {Pi}k
i=1with known and finite variances {σ2
i}k
i=1, i.e. for all
i∈{1,...,k},Pi∈P [2](M)such thatM≜ max
i∈{1,...,k}σ2
i. Let us also consider some βi≥4σiandp≜
inf1≤i≤kPPi(|X−EPi[X]|≤βi/2)such thatp>5εandε<1/5. We denote/tildewide∆i,ε≜(∆i−2bi)(p−ε)−8βiε,
which we assume positive and/radicalbigg
(1−2ε)
log(1−ε
ε)≤ε.
•If/tildewide∆i,ε>12σ2
i
βi/parenleftig√
2 + 2βi
σiε/parenrightig2
, then HuberUCB pulls in expectation arm iat most
E[Ti(n)]≤log(n) max/parenleftigg
32βi
3/tildewide∆i,ε,4
(p−5ε)2/parenleftbigg
1 + 2√
2/parenleftbigg
ε∨9
14√
2/parenrightbigg/parenrightbigg2/parenrightigg
+ 10(log(n)+1)
•If/tildewide∆i,ε≤12σ2
i
βi/parenleftig√
2 + 2βi
σiε/parenrightig2
, then HuberUCB pulls in expectation arm iat most
E[Ti(n)]≤log(n) max/parenleftigg
50σ2
i
9/tildewide∆2
i,ε/parenleftbigg√
2+2βi
σiε/parenrightbigg2
,4
(p−5ε)2/parenleftbigg
1+2√
2/parenleftbigg
ε∨9
14√
2/parenrightbigg/parenrightbigg2/parenrightigg
+ 10(log(n)+1).
Using Theorem 3 and Lemma 1, a bound on the corrupted regret of HuberUCB follows immediately.
We now state a simplified version of Theorem 3 with worse but explicit constants for easier comprehension.
Let us fixβ2
i= 16σ2
iandε≤1/10such thatε= 4/(5/radicalbig
ln(9))≃0.54, andp≥1−4σ2
i
β2
i≥3
4≥5ε+1
4. Now, if
we further assume that Pisymmetric leading to bi=0, it yields the following upper bounds.
Corollary 1 (Simplified version of Theorem 3) Suppose that for all i,Piis a symmetric distribution
with finite variance σ2
i. Let also denote /tildewide∆i,ε≜∆i(p−ε)−32σiεwhich is assumed to be positive and let
ε<1/10.
•If/tildewide∆i,ε>6σi/parenleftbig
1 + 4√
2ε/parenrightbig2, then HuberUCB pulls in expectation arm iat most
E[Ti(n)]≤43 log(n) max/parenleftigg
σi
/tildewide∆i,ε,10/parenrightigg
+ 10(log(n) + 1).
•If/tildewide∆i,ε≤6σi/parenleftbig
1 + 4√
2ε/parenrightbig2, then HuberUCB pulls in expectation arm iat most
E[Ti(n)]≤23 log(n) max/parenleftigg
σ2
i
/tildewide∆2
i,ε/parenleftbig
1 + 32ε2/parenrightbig
,18/parenrightigg
+ 10(log(n) + 1).
Remark that in this corollary, we replaced some occurrences of εby its upper bound, which is also an upper
bound onε. Thus, the presented result is loose up to constants but lend itself to easier comprehension.
Discussions on the Upper Bound. Here, we discuss how this proposed upper bound of HuberUCB
matches and mismatches with the lower bounds in Theorem 1.
1.Order-optimality of Upper Bound. HuberUCB achieves the logarithmic regret prescribed by the lower
bound (Theorem 1) plus some additive error due to the fact that this is a UCB-type algorithm. Thus,
HuberUCB is order optimal with respect to n.
2.Two Regimes of Upper Bound. When ∆iis small compared to σi, we obtain an upper bound E[Ti(n)] =
n→∞
O/parenleftbigg
log(n)/parenleftbigg
σ2
i
/tildewide∆2
i,εε2/parenrightbigg/parenrightbigg
from Corollary 1. ε2is of the same order of magnitude as Equation (7) because we
takeεstrictly smaller than 1/2.ε2acts as an indicator of the corruption level. The termσ2
i
/tildewide∆2
i,εindicates
13Published in Transactions on Machine Learning Research (01/2024)
the hardness due to the corrupted gaps /tildewide∆i,εand echoes the hardness termσ2
i
∆2
ithat appears in regret upper
bound of UCB for uncorrupted bandits. The hardness termσ2
i
/tildewide∆2
i,εalso appears in the corrupted lower bound
(Equation (6)) as well as the heavy-tailed lower bound (Equation (5)) for ∆i≪σi2.
On the other hand, if ∆iis larger than σi, we get that E[Ti(n)] =O/parenleftbigg
log(n)/parenleftbigg
σi
/tildewide∆i,ε∨ε2∨1/parenrightbigg/parenrightbigg
. This upper
bound reflects the lower bound in Equation (7) that holds for ∆i>2σi. This reinstates the fact that for
large enough suboptimality gaps, the regret of HuberUCB depends solely on the corruption level than the
suboptimality gap.
3.Deviation from the Lower Bound. The two regimes defined in the upper bound does not follow the exact
distinctions made in the lower bounds. We observe that in the upper bound, the distinction between regimes
depend on a corrupted suboptimality gap /tildewide∆i,ε≜∆i(p−ε)−32σiε, while the lower bound depends on
the corrupted suboptimality gap ∆i,ε≜∆i(1−ε)−2σiε. This difference in constants hinder the hardness
regimes and corresponding constants in upper and lower bounds to match for all ∆i,σi,andε. This deviation
also comes from the fact that the lower bounds proposed in Theorem 1 consider effects of heavy-tails and
corruptions separately, while the upper bound of HuberUCB consider them in a coupled manner.
Additionally, we observe that regret of HuberUCB is suboptimal due to the constant additive error, which
appears due to the initial forced exploration of HuberUCB up toslim(t). Our concentration bounds and
corresponding regret analysis shows that this forced exploration phase is unavoidable in order to be able to
handle the case ∆i≤σiwith HuberUCB . Removing this discrepancy between the lower and upper bounds
would constitute an interesting future work.
5.4 Computational Details
Here, we discuss the three hyperparameters that HuberUCB depends on and also its computational cost.
Choice ofσandε.In Theorem 3, we assume to know the σandε. In practice, these are unknown and
we estimate σ2with a robust estimator of the variance, such as the median absolute deviation. In contrast,
estimatingεis hard. There exists some heuristics, for example using the proportion of point larger than 1.5
times the inter-quartile range or using more complex algorithms like Isolation Forest algorithm but these
methods work in general using the hypothesis that outliers are in some way points that are located outside of
“the bulk of the data" which conflicts with the fact that we don’t suppose anything on the outliers. Moreover
even though there are heuristics, the problem of finding what constitute “the bulk of the data" is closely
linked to problems such as finding a “Robust minimum volume ellipsoid" which is NP-hard in general (Mittal
& Hanasusanto, 2022). We refer to Appendix C.1 for an ablation study on the choice of ε.
Choice ofβi.Ideally,βishould be larger than 4σi. We recommend using an estimator of σito estimate a
good value of βi. The choice of βireflects the difference between heavy-tailed bandits and corrupted bandits.
When the data are heavy-tailed but not corrupted, Catoni (2012) shows that βi≃σi√nis a good choice for
the scaling parameter. However, this choice is not robust to outliers and yields a linear regret in our setup
(see Section 7) and a trade-off between Heavy-tailed and corrupted setting would dictate βi≃σ√n∧ε−1/2
(see Proposition 2 in Mathieu (2022)). In Appendix C.1, we present an ablation study on the choice of ε.
Computational Cost. Huber’sestimatorhaslinearcomplexityduetotheinvolvedIteratedRe-weightingLeast
Squares algorithm, which is not sequential. We have to do this at every iteration, which leads HuberUCB to
have a quadratic time complexity. This is the computational cost of using a robust mean estimator, i.e. the
Huber’s estimator.
2We observe that the lower bound in Equation (5) depends onσ2
i
∆i,ε2for∆i≪σi, since the first order approximation of
log(1 +x)isxasx→0.
14Published in Transactions on Machine Learning Research (01/2024)
6SeqHuberUCB : A Faster Robust Bandit Algorithm
In this section, we present a sequential approximation of the Huber’s estimator, and we leverage it further
to create a robust bandit algorithm with linear-time complexity algorithm. Here, we describe the algorithm
(SeqHuberUCB ) and its theoretical properties.
A sequential approximation of Huber’s estimator. The central idea is to compute the Huber’s
estimator using the full historical data only in logarithmic number of steps than at every step, and in between
two of these re-computations, update the estimator using only the samples observed at that step. This allows
us to propose a sequential approximation of Huber’s estimator, i.e. SeqHubt, with lower computational
complexity.
By fixing the update step P2(t) = 2/floorleftbiglog(t)
log(2)/floorrightbig
before a given step t >0, we define the estimator SeqHubtby
SeqHub0= 0and
SeqHubt=

Ht ift=P2(t),
Ht+/summationtextt
i=P2(t)ψ(Xi−Ht)/summationtextt
i=1ψ′(Xi−Ht)otherwise.(13)
Here,Ht≜Hub(XP2(t)
1)andψis the influence function defined in Equation (9). SeqHubtcan be conceptu-
alized as a first order Taylor approximation of Hub(Xt
1)around Hub(XP2(t)
1).
One might argue that SeqHubtis not fully sequential rather a phased estimator as we still recompute the
Huber’s estimator following a geometric schedule. Thus, we still need to keep all the data in memory, leading
to linear space complexity as the non-sequential Huber’s estimator. But it features the good property of
having a linear time complexity when computed using the prescribed geometric schedule. This implies that
theSeqHuberUCB algorithm leveraging the sequential Huber’s estimator achieves a linear time complexity.
Concentration Properties of SeqHub.Now, in order to propose SeqHuberUCB we first aim to derive the
rate of convergence of SeqHubttowards the true Huber’s mean Hub(P).
Theorem 4 If the assumptions of Theorem 2 hold true, with probability larger than 1−14δ, we have
|SeqHubt−Hub(P)|≤rt(δ) +
1
p−/radicalig
log(1/δ)
2t−ε−1
rP2(t)(δ) (14)
for anyt>0, andδ≥exp/parenleftbigg
−P2(t)128(p−5ε)2
49(1+2ε√
2)2/parenrightbigg
. Here,rt(δ)is defined as in Equation (10).
We observe that the confidence bound of SeqHubtincludes the confidence bound of Hubt, i.e.rt(δ),
and an additive term proportional to rP2(t)(δ). SincerP2(t)(δ)≥rt(δ)fort≥P2(t), we can show that
|SeqHubt−Hub(P)| ≤/parenleftbigg
p−/radicalig
log(1/δ)
2t−ε/parenrightbigg−1
rP2(t)(δ). Thus, we obtain larger confidence bounds for
SeqHubthan that of Hub, and they differ approximately by a multiplicative constant (p−ε)−1ast→∞.
SeqHuberUCB : The algorithm. Now, we plug-in the sequential Huber’s estimator, SeqHub, and the cor-
responding confidence bound (Equation (14)), instead of the Huber’s estimator and the corresponding con-
fidence bound in the HuberUCB algorithm. This allows us to construct the SeqHuberUCB algorithm that we
present hereafter.
Specifically, we define the index of SeqHuberUCB as
ISeqHuberUCB
i (t) = SeqHubi,Ti(t−1)+BSeqHuberUCB
i (Ti(t−1),t). (15)
where
SeqHubi,s= SeqHub ( Xt,1≤t≤ssuch that At=i,),
15Published in Transactions on Machine Learning Research (01/2024)
and a confidence bound for arm iwithsnumber of pulls is
BSeqHuberUCB
i (s,t)≜

rs(1/t2) +/parenleftbigg
1
p−/radicalbig
log(1/δ)
2s−ε−1/parenrightbigg
rP2(s)(1/t2) +biifP2(s)≥slim(t)
∞ ifP2(s)<slim(t).
Here,slim(t),εandbiare same as defined for HuberUCB .
SimilartoCorollary1, wenowpresentasimplifiedregretupperboundfor SeqHuberUCB .Retainingthesetting
of Corollary 1, we assume that β2
i=16σ2
i,ε≤1/10implyingε= 4/(5/radicalbig
ln(9))≃0.54,p≥1−4σ2
i
β2
i≥3
4≥5ε+1
4,
andPisymmetric so that bi=0. Further simplifying the constants yields the following regret upper bound
forSeqHuberUCB .
Lemma 6 (Simplified Upper Bound on Regret of SeqHuberUCB )Suppose that for all i,Piis a dis-
tribution with finite variance σ2
i. Let us also denote /tildewide∆i,ε= ∆i(p−ε)−32σiε,
•If/tildewide∆i,ε>18σi/parenleftbig
1 + 4√
2ε/parenrightbig2, then
E[Ti(n)]≤128 log(n) max/parenleftigg
σi
/tildewide∆i,ε,2/parenrightigg
+ 28(log(n) + 1).
•If/tildewide∆i,ε≤18σi/parenleftbig
1 + 4√
2ε/parenrightbig2, then
E[Ti(n)]≤80 log(n) max/parenleftigg
σ2
i
/tildewide∆2
i,ε/parenleftbig
1 + 32ε2/parenrightbig
,3/parenrightigg
+ 28(log(n) + 1).
Comparison between Regrets of HuberUCB and SeqHuberUCB .Lemma 6 yields similar regret bounds
forSeqHuberUCB as the ones obtained for HuberUCB in Corollary 1. We observe that the regrets of these two
algorithms only differ in n-independent constants. Specifically, regret of SeqHuberUCB can be approximately
3−4timeshigherthanthatof HuberUCB .Forsimplicityofexposition,wepresentapproximateconstantsinour
results. A more careful analysis might yield more fine-tuned constants. Theorem 4 and experimental results
(Figure 2) indicate that it is possible to have very close performances with SeqHuberUCB andHuberUCB .
7 Experimental Evaluation
In this section, we assess the experimental efficiency of HuberUCB andSeqHuberUCB by plotting the empirical
regret. Contrary to the uncorrupted case, we cannot really estimate the corrupted regret in (Corrupted
regret) only using the observed rewards. Instead, we use the true uncorrupted gaps that we know because
we are in a simulated environment, and we estimate the corrupted regret Rnusing/summationtextk
i=1∆iTi/logicalandtext
(n), where
Ti/logicalandtext
(n) =1
M/summationtextM
m=1(Ti(n))mis a Monte-Carlo estimation of Eνε[Ti(n)]overMexperiments. We use rlberry
library (Domingues et al., 2021) and Python3 for the experiments. We run the experiments on an 8 core
Intel(R) Core(TM) i7-8665U CPU@1.90GHz. For each algorithm, we perform each experiment 100times to
get a Monte-Carlo estimate of regret.
Comparison with Bandit Algorithms for Heavy-tailed and Adversarial Settings. To the best of
our knowledge, there is no existing bandit algorithm for handling unbounded stochastic corruption prior
to this work. Hence, we focus on comparing ourselves to the closest settings, i.e. bandits in heavy-tailed
setting and adversarial bandit algorithms. We empirically and competitively study five different algorithms:
HuberUCB ,SeqHuberUCB , two RobustUCB algorithms with Catoni-Huber estimator and Median of Means
(MOM) (Bubeck et al., 2013). In particular, we compare to algorithms assuming bounded centered moments
and not bounded raw moment such as Truncated Mean from Bubeck et al. (2013), and KL inf-UCB from
Agrawal et al. (2021). See also Appendix C for further experimental results.
HuberUCB is closely related to the RobustUCB with Catoni Huber estimator, which also uses Huber’s esti-
mator but with another set of parameters and confidence intervals. The RobustUCB algorithms are tuned
16Published in Transactions on Machine Learning Research (01/2024)
RegretBernoulli=0
 =0.03
 =0.05
RegretStudent
0 5000 10000 15000
nRegretPareto
0 5000 10000 15000
n
SeqHuberUCB HuberUCB CatoniUCB MOMUCB0 5000 10000 15000
n
Figure 2: Cumulative regret plot of the algorithms on a corrupted Bernoulli (above), Student’s (middle)
and Pareto (below) reward distributions with various corruption levels ε. Lower corrupted regret indicates
better performance for an algorithm.
for uncorrupted heavy-tails. Hence, they incur linear regret in a corrupted setting. This is reflected in the
experiments. We also improve upon Bubeck et al. (2013) as we can handle arm-dependent variances .
Corrupted Bernoulli setting: In Figure 2 (above), we study a 3-armed bandits with corrupted Bernoulli
distributions with means 0.1,0.97,0.99. The corruption applied to this bandit problem are Bernoulli dis-
tributions with means 0.999,0.999,0.001, respectively. For HuberUCB and SeqHuberUCB , we choose to use
βi= 0.1σi, which seems to work better despite the theory presented before. We plot the mean plus/minus
the standard error of the result in Figure 2. We do that for the three corruption proportions εequal to 0%,
3%and5%. We notice that there is a short linear regret phase at the beginning due to the forced exploration
performed by the algorithms. Followed by that, HuberUCB and SeqHuberUCB incur logarithmic regret. On
the other hand, Catoni Huber Agent and MOM Agent incur logarithmic regret only in the uncorrupted
setting. When the data are corrupted, i.e. ε>0, their regret grow linearly.
Corrupted Student setting: In Figure 2 (middle), we study a 3-armed bandits with corrupted Student’s
distributions with 3degrees of freedom (finite second moment) and with means 0.1,0.95,1. The corruption
applied to this bandit problem are Gaussians with variance 1, and means 100,100,−1000respectively. For
HuberUCB and SeqHuberUCB , we choose to use βi=σi. The results echo the observations for the Bernoulli
case except that the corruption is more drastic and affect the performance even more.
Corrupted Pareto setting: In Figure 2 (bottom), we illustrate the results for a 3-armed bandits with
corrupted Pareto distributions having shape parameters 3,3,2.1(i.e. they have finite second moments),
and scale parameters 0.1,0.2,0.3respectively. Thus, the corresponding means are 0.15,0.3and0.57and
the standard deviations are 0.09,0.17,1.25, respectively. The corruption applied to this bandit problem are
17Published in Transactions on Machine Learning Research (01/2024)
Gaussians with variance 1, and centered at 100,100,−1000respectively. For HuberUCB and SeqHuberUCB ,
we choose to use β= 1.5σiand we also bound the bias bibyσ2
i/βi. The results echo the observations for
the Student’s distributions.
Thus, weconcludethat HuberUCB incurthelowestregretamongthecompetingalgorithmsintheBanditswith
Stochastic Corruption setting, specially for higher corruption levels ε. Also, performances of SeqHuberUCB
andHuberUCB are very close, except for the Pareto distributions with high corruption level.
8 Conclusion
In this paper, we study the setting of Bandits with Stochastic Corruption that encompasses both the heavy-
tailed rewards with bounded variance and unbounded corruptions in rewards. In this setting, we prove
lower bounds on the regret that shows the heavy-tailed bandits and corrupted bandits are strictly harder
than the usual sub-Gaussian bandits. Specifically, in this setting, the hardness depends on the suboptimality
gap/variance regimes. If the suboptimality gap is small, the hardness is dictated by σ2
i/∆2
i,ε. Here, ∆i,εis the
corrupted sub-optimality gap, which is smaller than the uncorrupted gap ∆and thus, harder to distinguish.
To complement the lower bounds, we design a robust algorithm HuberUCB that uses Huber’s estimator for
robustmeanestimationandanovelconcentrationboundonthisestimatortocreatetightconfidenceintervals.
HuberUCB achieves logarithmic regret that matches the lower bound for low suboptimality gap/high variance
regime. We also present a sequential Huber estimator that could be of independent interest and we use it
to state a linear-time robust bandit algorithm, SeqHuberUCB , that presents the same efficiency as HuberUCB .
Unlike existing literature, we do not need any assumption on a known bound on corruption and a known
bound on the (1 +η)-uncentered moment, which was posed as an open problem in Agrawal et al. (2021).
Since our upper and lower bounds disagree in the high gap/low variance regime, it will be interesting to
investigate this regime further. From multi-armed bandits, we know that the tightest lower and upper
bounds depend on the KL-divergence between optimal and suboptimal reward distributions. Thus, it would
be imperative to study KL-divergence with corrupted distributions to better understand the Bandits with
Stochastic Corruption problem. In this paper, we have focused on a problem-dependent regret analysis for
a givenε. In future, it would be interesting to get some insight on how to adapt to an unknown ε, and to
performaproblem-independent“worst-case"analysis. Also, followingthereinforcementlearningliterature, it
will be natural to extend HuberUCB to contextual and linear bandit settings with corruptions and heavy-tails.
This will facilitate its applicability to practical problems, such as choosing treatments against pests.
18Published in Transactions on Machine Learning Research (01/2024)
References
Naman Agarwal, Brian Bullins, Elad Hazan, Sham Kakade, and Karan Singh. Online control with adver-
sarial disturbances. In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.), Proceedings of the 36th
International Conference on Machine Learning , volume 97 of Proceedings of Machine Learning Research ,
pp. 111–119. PMLR, 09–15 Jun 2019. URL https://proceedings.mlr.press/v97/agarwal19c.html .
Shubhada Agrawal, Sandeep K Juneja, and Wouter M Koolen. Regret minimization in heavy-tailed bandits.
InConference on Learning Theory , pp. 26–62. PMLR, 2021.
Jason Altschuler, Victor-Emmanuel Brunel, and Alan Malek. Best arm identification for contaminated
bandits. Journal of Machine Learning Research , 20(91):1–39, 2019.
Peter Auer, Nicolo Cesa-Bianchi, Yoav Freund, and Robert E Schapire. The nonstochastic multiarmed
bandit problem. SIAM journal on computing , 32(1):48–77, 2002.
Ilija Bogunovic, Andreas Krause, and Jonathan Scarlett. Corruption-tolerant gaussian process bandit op-
timization. In International Conference on Artificial Intelligence and Statistics , pp. 1071–1081. PMLR,
2020.
Djallel Bouneffouf. Corrupted contextual bandits: Online learning with corrupted context. In ICASSP
2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) , pp.
3145–3149. IEEE, 2021.
Hippolyte Bourel, Odalric-Ambrym Maillard, and Mohammad Sadegh Talebi. Tightening Exploration in
Upper Confidence Reinforcement Learning. In International Conference on Machine Learning , Vienna,
Austria, July 2020. URL https://hal.archives-ouvertes.fr/hal-03000664 .
Sébastien Bubeck, Nicolo Cesa-Bianchi, and Gábor Lugosi. Bandits with heavy tail. IEEE Transactions on
Information Theory , 59(11):7711–7717, 2013.
Apostolos N Burnetas and Michael N Katehakis. Optimal adaptive policies for markov decision processes.
Mathematics of Operations Research , 22(1):222–255, 1997.
Olivier Catoni. Challenging the empirical mean and empirical variance: a deviation study. In Annales de
l’IHP Probabilités et statistiques , volume 48, pp. 1148–1185, 2012.
Arnak Dalalyan and Philip Thompson. Outlier-robust estimation of a sparse linear model using l1-penalized
huber’s m-estimator. Advances in neural information processing systems , 32, 2019.
Jules Depersin and Guillaume Lecué. Robust sub-gaussian estimation of a mean vector in nearly linear time.
The Annals of Statistics , 50(1):511–536, 2022.
Luc Devroye, Matthieu Lerasle, Gabor Lugosi, and Roberto I Oliveira. Sub-gaussian mean estimators. The
Annals of Statistics , 44(6):2695–2725, 2016.
Omar Darwiche Domingues, Yannis Flet-Berliac, Edouard Leurent, Pierre Ménard, Xuedong Shang, and
Michal Valko. rlberry - A Reinforcement Learning Library for Research and Education, 10 2021. URL
https://github.com/rlberry-py/rlberry .
Negin Golrezaei, Vahideh Manshadi, Jon Schneider, and Shreyas Sekar. Learning product rankings robust
to fake users. In Proceedings of the 22nd ACM Conference on Economics and Computation , pp. 560–561,
2021.
Mohammad Hajiesmaili, Mohammad Sadegh Talebi, John Lui, Wing Shing Wong, et al. Adversarial bandits
withcorruptions: Regretlowerboundandno-regretalgorithm. Advances in Neural Information Processing
Systems, 33:19943–19952, 2020.
19Published in Transactions on Machine Learning Research (01/2024)
Andreas Hotho. Anomaly detection in beehives: An algorithm comparison. In Sensor Networks: 9th Inter-
national Conference, SENSORNETS 2020, Valletta, Malta, February 28–29, 2020, and 10th International
Conference, SENSORNETS 2021, Virtual Event, February 9–10, 2021, Revised Selected Papers , pp. 1.
Springer Nature, 2022.
Peter J. Huber. Robust estimation of a location parameter. Annals of Mathematical Statistics , 35:492–518,
1964.
Peter J Huber. A robust version of the probability ratio test. The Annals of Mathematical Statistics , pp.
1753–1758, 1965.
Peter J Huber. Robust statistics , volume 523. John Wiley & Sons, 2004.
Sayash Kapoor, Kumar Kshitij Patel, and Purushottam Kar. Corruption-tolerant bandit learning. Machine
Learning , 108(4):687–715, 2019.
T.L Lai and Herbert Robbins. Asymptotically efficient adaptive allocation rules. Advances in Applied
Mathematics , 6(1):4–22, 1985. ISSN 0196-8858. doi: https://doi.org/10.1016/0196-8858(85)90002-8. URL
https://www.sciencedirect.com/science/article/pii/0196885885900028 .
Tor Lattimore and Csaba Szepesvári. Bandit algorithms . Cambridge University Press, 2020.
Guillaume Lecué and Matthieu Lerasle. Robust machine learning by median-of-means: theory and practice.
The Annals of Statistics , 48(2):906–931, 2020.
Kyungjae Lee, Hongjun Yang, Sungbin Lim, and Songhwai Oh. Optimal algorithms for stochastic multi-
armed bandits with heavy tailed rewards. Advances in Neural Information Processing Systems , 33:8452–
8462, 2020.
MatthieuLerasle, ZoltánSzabó, TimothéeMathieu, andGuillaumeLecué. Monkoutlier-robustmeanembed-
ding estimation by median-of-means. In International Conference on Machine Learning , pp. 3782–3793.
PMLR, 2019.
Thodoris Lykouris, Vahab Mirrokni, and Renato Paes Leme. Stochastic bandits robust to adversarial corrup-
tioreferences 1ns. In Proceedings of the 50th Annual ACM SIGACT Symposium on Theory of Computing ,
pp. 114–122, 2018.
Odalric-Ambrym Maillard. Mathematics of Statistical Sequential Decision Making . Habilitation à diriger des
recherches, Université de Lille Nord de France, February 2019. URL https://hal.archives-ouvertes.
fr/tel-02077035 .
Timothée Mathieu. Concentration study of m-estimators using the influence function, 2022.
Andres Munoz Medina and Scott Yang. No-regret algorithms for heavy-tailed linear bandits. In International
Conference on Machine Learning , pp. 1642–1650. PMLR, 2016.
Stanislav Minsker. Distributed statistical estimation and rates of convergence in normal approximation.
Electronic Journal of Statistics , 13(2):5213–5252, 2019.
Stanislav Minsker and Mohamed Ndaoud. Robust and efficient mean estimation: an approach based on the
properties of self-normalized sums. Electronic Journal of Statistics , 15(2):6036–6070, 2021.
Areesh Mittal and Grani A Hanasusanto. Finding minimum volume circumscribing ellipsoids using general-
ized copositive programming. Operations Research , 70(5):2867–2882, 2022.
Roman Pogodin and Tor Lattimore. On first-order bounds, variance and gap-dependent bounds for adver-
sarial bandits. In Uncertainty in Artificial Intelligence , pp. 894–904. PMLR, 2020.
Adarsh Prasad, Sivaraman Balakrishnan, and Pradeep Ravikumar. A unified approach to robust mean
estimation. arXiv preprint arXiv:1907.00927 , 2019.
20Published in Transactions on Machine Learning Research (01/2024)
Adarsh Prasad, Sivaraman Balakrishnan, and Pradeep Ravikumar. A robust univariate mean estimator is
all you need. In International Conference on Artificial Intelligence and Statistics , pp. 4034–4044. PMLR,
2020.
Han Shao, Xiaotian Yu, Irwin King, and Michael R Lyu. Almost optimal algorithms for linear stochastic
bandits with heavy-tailed payoffs. Advances in Neural Information Processing Systems , 31, 2018.
Karanjit Singh and Shuchita Upadhyaya. Outlier detection: applications and techniques. International
Journal of Computer Science Issues (IJCSI) , 9(1):307, 2012.
James G. Wendel. Note on the gamma function. American Mathematical Monthly , 55:563, 1948.
21Published in Transactions on Machine Learning Research (01/2024)
Appendix
Table of Contents
A Proof of Theorems 23
A.1 Proof of Theorem 1: Regret Lower Bound . . . . . . . . . . . . . . . . . . . . . . . . . . . 23
A.2 Proof of Theorem 2: Concentration of Huber’s Estimator . . . . . . . . . . . . . . . . . . 23
A.3 Proof of Theorem 4: Concentration of Sequential Huber’s Estimator . . . . . . . . . . . . 25
A.4 Proof of Theorem 3: Regret Upper bound of HuberUCB . . . . . . . . . . . . . . . . . . . 27
B Proof of Technical Lemmas and Corollaries 31
B.1 Preliminary lemmas . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31
B.2 Lemmas for Regret upper bound . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34
B.3 Lemmas for concentration of robust estimators . . . . . . . . . . . . . . . . . . . . . . . . 38
C Additional experimental results 43
C.1 Sensitivity to βandε. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43
C.2 Corrupted bandits with adversarial algorithms . . . . . . . . . . . . . . . . . . . . . . . . 43
22Published in Transactions on Machine Learning Research (01/2024)
A Proof of Theorems
A.1 Proof of Theorem 1: Regret Lower Bound
The theorem is a consequence of Lemmas 2, 3 and 4.
From Lemma 2, we have
lim inf
n→∞Eν[Ti(n)]
log(n)≥1
DKL(P0,P1)(16)
Student distributions
LetP0,P1be student distributions with parameter d= 3and gap ∆ias in Lemma 3. From Lemma 3, we
get
DKL(P0,P1)≤/braceleftigg
17∆2
i if∆i≤1
4 log (∆i) + log (50) if∆i>1(17)
Then, using that log(50)≤17,
DKL(P0,P1)≤17∆2
i∧4 log (∆i) + 17.
Finally, use that the variance of a student with three degrees of freedom is σ2
i= 3to get that
DKL(P0,P1)≤51∆2
i
σ2
i∧4 log/parenleftbigg∆i
σi/parenrightbigg
+ 22.
Bernoulli distributions
LetP0,P1be as in Lemma 4 with gap ∆iand variance σi. If2σiε√1−2ε<∆i<2σi, then
DKL(P0,P1)≤∆i,ε
2σilog/parenleftigg
1 +∆i,ε
2σi−∆i,ε/parenrightigg
∧(1−2ε) log/parenleftbigg
1 +1−2ε
ε/parenrightbigg
(18)
Use Equation (16) to conclude.
A.2 Proof of Theorem 2: Concentration of Huber’s Estimator
First, we control the deviations of Huber’s estimator using the deviations of ψβ(X−Hubβ(Xn
1)). We will
need the following lemma to control the variance of ψβ(X−Hubβ(Xn
1)), which will in turn allow us to
control its deviation with Lemma 8.
Lemma 7 (Controlling Variance of Influence of Huber’s Estimator) Suppose that Y1,...,Ynare
i.i.d with law P. Then
Var(ψβ(Y−Hubβ(P)))≤Var(Y) =σ2
Lemma 8 (Concentrating Huber’s Estimator by Concentrating the Influence) Suppose that X1,
...,Xnare i.i.d with law (1−ε)P+εHfor someH∈Pand proportion of outliers ε∈(0,1/2). Then, for
anyη>0andλ∈(0,β/2], we have
P(|Hubβ(Xn
1)−Hubβ(P)|≥λ)≤P/parenleftigg/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle1
nn/summationdisplay
i=1ψβ(Xi−Hubβ(P))/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle≥λ(p−η−ε)+/parenrightigg
+ 2e−2nη2
wherep=P(|Y−E[X]|≤β/2).
Then, using these Lemmas, we can prove the theorem.
23Published in Transactions on Machine Learning Research (01/2024)
Step 1. For anyδ∈(0,1), with probability larger than 1−3δ,
/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle1
nn/summationdisplay
i=1ψβ(Xi−Hubβ(P))/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle≤σ/radicalbigg
2 log(1/δ)
n+βlog(1/δ)
2n+ 2βε+ 2β/radicaligg
log(1/δ)(1−2ε)
nlog/parenleftbig1−ε
ε/parenrightbig.(19)
Proof: Write that Xi= (1−Wi)Yi+WiZiwhereW1,...,Wnare i.i.d{0,1}Bernoulli random variable
with mean ε,Y1,...,Ynare i.i.d∼PandZ1,...,Znare i.i.d with law H, we have
/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle1
nn/summationdisplay
i=1ψβ(Xi−Hubβ(P))/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
=/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle1
nn/summationdisplay
i=1ψβ(Yi−Hubβ(P)) +1
nn/summationdisplay
i=11{Wi= 1}(ψβ(Zi−Hubβ(P))−ψβ(Yi−Hubβ(P)))/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
≤/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle1
nn/summationdisplay
i=1ψβ(Yi−Hubβ(P))/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle+ 2β1
nn/summationdisplay
i=11{Wi= 1}
Remark that by definition of Hubβ(P), it is defined as the root of the equation E[ψβ(Y−Hubβ(P))] = 0.
From Bernstein’s inequality, for any δ∈(0,1),
P/parenleftigg/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle1
nn/summationdisplay
i=1ψβ(Yi−Hubβ(P))/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle≥/radicalbigg
2Vψβlog(1/δ)
n+βlog(1/δ)
3n/parenrightigg
≤2δ
whereVψβ= Var(ψβ(Yi−Hubβ(P))).
Then, using that Bernoulli random variables with mean εare sub-Gaussian with variance parameter
1−2ε
2 log((1−ε)/ε)(see Lemma 6 of Bourel et al. (2020)),
P/parenleftigg
1
nn/summationdisplay
i=11{Wi= 1}≤ε+/radicaligg
log(1/δ)(1−2ε)
nlog/parenleftbig1−ε
ε/parenrightbig/parenrightigg
≥1−δ.
Then, using Lemma 7 we get for any δ∈(0,1), with probability larger than 1−3δ,
/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle1
nn/summationdisplay
i=1ψβ(Xi−Hubβ(P))/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle≤σ/radicalbigg
2 log(1/δ)
n+βlog(1/δ)
2n+ 2βε+ 2β/radicaligg
log(1/δ)(1−2ε)
nlog/parenleftbig1−ε
ε/parenrightbig.(20)
Step 2. Usingη=/radicalig
log(1/δ)
2n, the hypotheses of Lemma 8 are verified.
Proof: To apply Lemma 8, it is sufficient that
σ/radicalbigg
2t
n+βlog(1/δ)
3n+ 2βε+ 2β/radicaligg
log(1/δ)(1−2ε)
nlog/parenleftbig1−ε
ε/parenrightbig≤β
2/parenleftigg
p−/radicalbigg
log(1/δ)
2n−ε/parenrightigg
(21)
and using that 4σ≤β, we have that it is sufficient that
/radicalbigg
log(1/δ)
2n+log(1/δ)
3n+ 2/radicaligg
log(1/δ)(1−2ε)
nlog/parenleftbig1−ε
ε/parenrightbig≤1
2(p−5ε). (22)
This is a polynomial in/radicalbig
log(1/δ)/nthat we need to solve. We use the following elementary algebra lemma.
Lemma 9 (2nd order polynomial root bound) leta,b,cbe three positive constants and xverifyax2+
bx−c≤0. Suppose that4ac
b2≤d, thenxmust verify
x≥2c(√
d+ 1−1)
db.
24Published in Transactions on Machine Learning Research (01/2024)
Observe that we have
2 (p−5ε)
3/parenleftbigg
1√
2+2√1−2ε/radicalbig
log(1−ε
ε)/parenrightbigg2≤4
3
and (/radicalbig
4/3 + 1−1)/(4/3)≥8/7, hence, from Lemma 9, we get the following sufficient condition for
Equation (22) to hold:
/radicalbig
log(1/δ)/n≤8√
2 (p−5ε)
7/parenleftbigg
1 +2√
2(1−2ε)/radicalbig
log(1−ε
ε)/parenrightbigg.
Hence, taking this to the square,
log(1/δ)≤n128 (p−5ε)2
49/parenleftbigg
1 +2√
2(1−2ε)/radicalbig
log(1−ε
ε)/parenrightbigg2.
Step 3. Using Lemma 8 and Step 1 prove that the theorem is true.
Proof: The hypotheses of Lemma 8 are verified and we can use its result and together with Equation (19)
we get with probability larger than 1−5δ,
|Hubβ(Xn
1)−Hubβ(P)|≤σ/radicalig
2 log(1/δ)
n+βlog(1/δ)
3n+ 2β/radicalbigg
log(1/δ)(1−2ε)
nlog(1−ε
ε)+ 2βε
/parenleftbigg
p−/radicalig
log(1/δ)
2n−ε/parenrightbigg
+.
A.3 Proof of Theorem 4: Concentration of Sequential Huber’s Estimator
In this proof, we denote
rt(δ) :=σ/radicalig
2 log(1/δ)
t+βlog(1/δ)
3t+ 2βε/radicalig
log(1/δ)
t+ 2βε
/parenleftbigg
p−/radicalig
log(1/δ)
2t−ε/parenrightbigg
+
this is the rate of convergence of Hubβ(Xt
1)toHubβ(P), as stated by Theorem 2.
LetP2(t)<t<P 2(t+ 1), define
ft(u) =1
tt/summationdisplay
i=1ψβ(Xi−u).
ftis a continuous function, we take its derivative in distribution to get that
ft(Hubβ(P)) =ft(Ht) + (Hubβ(P)−Ht)f′
t(Ht) +/integraldisplayHubβ(P)
Htf′′
t(u) (Hubβ(P)−u)du
Then, by definition of SeqHubt, we also have
0 =ft(Ht) + (SeqHubt−Ht)f′
t(Ht).
Hence,
ft(Hub(P)) = (Hub β(P)−SeqHubt)f′
t(Ht) +/integraldisplayHubβ(P)
Htf′′
t(u) (Hubβ(P)−u)du. (23)
wheref′
t(u) =−1
t/summationtextt
i=11{|Xi−u|≤β}andf′′
t(u) =−1
t/summationtextt
i=1(δXi−u−β−δXi−u+β)whereδxis the Dirac
mass inx.
25Published in Transactions on Machine Learning Research (01/2024)
f′
t(Ht)is a sum of indicator functions and should be close to P(|X−E[X]|≤β), which is close to 1.
Bound on f′
t(Ht)
We bound|f′
t(Ht)|. We have
|f′
t(Ht)|=1
tt/summationdisplay
i=11{|Xi−Ht|≤β}
≥1
tt/summationdisplay
i=11{|Xi−Hubβ(P)|≤β−|Ht−Hubβ(P)|}.
Choose the limiting δwhich isδ= exp/parenleftbigg
−P2(t)128(p−5ε)2
49(1+2ε√
2)2/parenrightbigg
, from Equation (21), we get that rt(δ)≤β/2.
Then, we have from Theorem 2, with probability larger than 1−5 exp/parenleftbigg
−P2(t)128(p−5ε)2
49(1+2ε√
2)2/parenrightbigg
, that|Ht−
Hubβ(P)|≤β/2, and then,
|f′
t(Ht)|≥1
tt/summationdisplay
i=11{|Xi−Hubβ(P)|≤β/2}. (24)
Bound on the integral of f′′
t.
We have,
/integraldisplayHubβ(P)
Htf′′
t(u) (Hubβ(P)−u)du
=1
tt/summationdisplay
i=1/integraldisplayHubβ(P)
Ht(δXi−u−β−δXi−u+β) (Hubβ(P)−u)du
=1
tt/summationdisplay
i=1(Hubβ(P)−Xi−β)1{Xi∈I−}−(Hubβ(P)−Xi+β)1{Xi∈I+}
whereI−andI+are the two undirected intervals
I−= [Ht−β,Hubβ(P)−β]andI+= [Ht+β,Hubβ(P) +β].
Hubβ(P)−βHt−β Hub(P)Ht Hubβ(P) +βHt+β
I− I+
Figure 3: Illustration I−andI+
Having that|Hubβ(Xt
1)−Ht| ≤β/2, we have that I−∩I+=∅. Then, choosing either the sum
1
t/summationtextt
i=1(Hubβ(P)−Xi−β)1{Xi∈I−}or1
t/summationtextt
i=1(Hubβ(P)−Xi−β)1{Xi∈I+}according to which one is
larger. IfXi∈I+, we have|Hubβ(P)−Xi+β|≤|Hubβ(P)−Ht|and ifXi∈I−,|Hubβ(P)−Xi−β|≤
|Hubβ(P)−Ht|, hence we have
/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle1
tt/summationdisplay
i=1/integraldisplayHubβ(P)
Ht(δXi−u−β−δXi−u+β) (Hubβ(P)−u)du/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
≤|Hubβ(P)−Ht|max/parenleftigg
1
tt/summationdisplay
i=11{Xi∈I−},1
tt/summationdisplay
i=11{Xi∈I+}/parenrightigg
.
26Published in Transactions on Machine Learning Research (01/2024)
Now, remark that by Equation (24), we have,
t/summationdisplay
i=11{Xi−Ht}=|f′
t(Ht)|≥1
tt/summationdisplay
i=11{|Xi−Hubβ(P)|≤β/2}
Let us denote pt(β) =1
t/summationtextt
i=11{|Xi−Hubβ(P)|≤β/2}.
There cannot be more than 1−pt(β)fraction of the Xi’s that are outside [Ht−β,Ht+β]. Similarly, there
cannot be more than 1−pt(β)fraction of the X′
isthat are outside [Hubβ(P)−β/2,Hubβ(P) +β/2]. Hence,
ifHt≤Hubβ(P), thenI−⊂[Ht−β,Ht+β]cand the proportion of Xi’s inI−can’t be larger than 1−pt(β).
IfHubβ(P)≤Ht, thenI−⊂[Hubβ(P)−β,Hubβ(P) +β]cwhich is itself a subset of [Hubβ(P)−
β/2,Hubβ(P) +β/2]cand the proportion of Xi’s included in [Hubβ(P)−β/2,Hubβ(P) +β/2]ccannot
be larger than 3/10.
In both cases,1
t/summationtextt
i=11{Xi∈I−}≤1−pt(β). A similar reasoning holds for I+, hence
/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/integraldisplayHubβ(P)
Htf′′
t(u) (Hubβ(P)−u)du/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle≤(1−pt(β))|Hubβ(P)−Ht|
Then, using Equation (24) and Equation (23), we get with probability larger than 1−
5 exp/parenleftbigg
−P2(t)128(p−5ε)2
49(1+2ε√
2)2/parenrightbigg
,
|Hubβ(P)−SeqHubt|≤ft(Hubβ(P)) +/vextendsingle/vextendsingle/vextendsingle/integraltextHubβ(P)
Htf′′
t(u) (Hubβ(P)−u)du/vextendsingle/vextendsingle/vextendsingle
f′
t(Ht)
≤ft(Hubβ(P)) + (1−pt(β))|Hubβ(P)−Ht|
pt(β). (25)
Then letδ≤exp/parenleftbigg
−P2(t)128(p−5ε)2
49(1+2ε√
2)2/parenrightbigg
, we use Equation (20) to say that with probability larger than 1−3δ,
we have
ft(Hubβ(P))≤σ/radicalbigg
2 log(1/δ)
t+βlog(1/δ)
2t+ 2βε+ 2β/radicaligg
log(1/δ)(1−2ε)
tlog/parenleftbig1−ε
ε/parenrightbig.
Then, using Hoeffding’s inequality after taking out the outliers, we get with probability larger than 1−δ,
that
pt(β) =1
tt/summationdisplay
i=11{|Xi−Hub(P)|≤β/2}≥p−/radicalbigg
log(1/δ)
2t−ε
to recover that the first term of the right-hand-side of Equation (25) is smaller than rt(δ). Then, using
Theorem 2, we get that with probability larger than 1−5 exp/parenleftbigg
−P2(t)128(p−5ε)2
49(1+2ε√
2)2/parenrightbigg
−9δ≥1−14δ,
|Hubβ(P)−SeqHubt|≤rt(δ) +
1
p−/radicalig
log(1/δ)
2t−ε−1
rP2(t)(δ).
A.4 Proof of Theorem 3: Regret Upper bound of HuberUCB
IfAt=ithen at least one of the following four inequalities is true:
Hub/logicalandtext
1,T1(t−1)+B1(T1(t−1),t)≤µ1 (26)
or
Hub/logicalandtext
i,Ti(t−1)≥µi+Bi(Ti(t−1),t) (27)
27Published in Transactions on Machine Learning Research (01/2024)
or
∆i<2Bi(Ti(t−1),t) (28)
or
T1(t−1)<slim(t) =98 log(t)
128 (p−5ε)2/parenleftbigg
1 + 2√
2/parenleftbigg
ε∨9
14√
2/parenrightbigg/parenrightbigg2
(29)
Indeed, ifTi(t−1)< slim(t), thenBi(Ti(t−1),t) =∞and Inequality (28) is true. On the other hand, if
Ti(t−1)≥slim(t), then we have Bi(Ti(t−1),t)is finite and all four inequalities are false, then,
Hub/logicalandtext
1,T1(t−1)+B1(T1(t−1),t)>µ1
=µi+ ∆i
≥µi+ 2Bi(Ti(t−1),n)
≥µi+ 2Bi(Ti(t−1),t)
≥Hub/logicalandtext
i,Ti(t−1)+Bi(Ti(t−1),t)
which implies that At̸=i.
Step 1. We have that P((26) is true )≤5/t.
Proof:
Then, we have that,
P/parenleftig
Hub/logicalandtext
1,T1(t−1)+B1(T1(t−1),t)≤µ1/parenrightig
≤t/summationdisplay
s=1P/parenleftig
Hub/logicalandtext
1,s+B1(s,t)≤µ1/parenrightig
=t/summationdisplay
s=⌈slim(t)⌉P/parenleftig
Hub/logicalandtext
1,s−µ1≤−B1(s,t)/parenrightig
Then, use Theorem 2, we get
P/parenleftig
Hub/logicalandtext
1,T1(t−1)+B1(T1(t−1),t)≤µ1/parenrightig
≤t/summationdisplay
s=⌈slim(t)⌉5e−log(t2)
≤t/summationdisplay
s=⌈slim(t)⌉5
t2≤5
t.
Step 2. Similarly, for arm i, we have
P/parenleftig
Hub/logicalandtext
i,Ti(t−1)≥µi+Bi(Ti(t−1),t)/parenrightig
≤5
t
Proof: We have,
P/parenleftig
Hub/logicalandtext
i,Ti(t−1)≥µi+Bi(Ti(t−1),t)/parenrightig
≤t/summationdisplay
s=⌈slim(t)⌉P/parenleftig
Hub/logicalandtext
i,s−µi≥Bi(s,t)/parenrightig
≤t/summationdisplay
s=⌈slim(t)⌉5e−log(t2)≤5
t.
28Published in Transactions on Machine Learning Research (01/2024)
Step 3. Letv∈N. If one of the two following conditions are true, then for all tsuch thatTi(t−1)≥v, we
have ∆i≥2Bi(Ti(t−1),t)(i.e. Equation (28) is false).
Condition 1: if /tildewide∆i,ε>12σ2
i
βi/parenleftig√
2 + 2βi
σiε/parenrightig2
andv≤log(n)96βi
9/tildewide∆i,ε.
Condition 2: if /tildewide∆i,ε≤12σ2
i
βi/parenleftig√
2 + 2βi
σiε/parenrightig2
andv≤50
9/tildewide∆2
i,ε/parenleftbig
σi√
2 + 2βiε/parenrightbig2log(n).
Proof: We search for the smallest value v≥slim(t)such that ∆iverifies
∆i≥2Bi(v,t) = 2σi/radicalig
2 log(t2)
v+βlog(t2)
3v+ 2εβi/radicalig
log(t2)
v+ 2βiε
/parenleftbigg
p−/radicalig
log(t2)
2v−ε/parenrightbigg + 2bi.
First, we simplify the expression, having that v≥slim(t), we have
log(t2)
2v≤128(p−5ε)2
98(1 + 9/7)2≤(p−ε)2
4,
hence we simplify to
∆i≥4
(p−ε)/parenleftigg
σi/radicalbigg
2 log(t2)
v+βilog(t2)
3v+ 2βiε/radicalbigg
log(t2)
v+ 2βiε/parenrightigg
+ 2bi
let us denote /tildewide∆i,ε= (∆i−2bi)(p−ε)−8βiε, we are searching for vsuch that
βilog(t2)
3v+/radicalbigg
log(t2)
v/parenleftig
σi√
2 + 2βiε/parenrightig
−/tildewide∆i,ε
4≤0
This is a second order polynomial in/radicalbig
log(t2)/v.
If/tildewide∆i,ε>0, then the smallest v>0is
/radicalbigg
log(t2)
v=3
2βi
−/parenleftig
σi√
2 + 2εβi/parenrightig
+/radicaligg
/parenleftig
σi√
2 + 2βiε/parenrightig2
+/tildewide∆i,εβi
3
.
First setting: if/tildewide∆i,ε>12σ2
i
βi/parenleftig√
2 + 2βi
σiε/parenrightig2
,
In that case, we have
/radicalbigg
log(t2)
v≥3
2βi
−/parenleftig
σi√
2 + 2βiε/parenrightig
+/radicaligg
βi/tildewide∆i,ε
3
≥3
2βi/radicaligg
βi/tildewide∆i,ε
12=/radicaligg
9/tildewide∆i,ε
48βi
Hence,v≤log(t)96βi
9/tildewide∆i,ε.
Second setting: if/tildewide∆i,ε≤12σ2
i
βi/parenleftig√
2 + 2βi
σiε/parenrightig2
, then we use Lemma 9, using that
/tildewide∆i,εβi
3/parenleftbig
σi√
2 + 2βiε/parenrightbig2≤4
and the fact that√1+4−1
4≥3
10, we get,
/radicalbigg
log(t2)
v≥3/tildewide∆i,ε
5/parenleftbig
σi√
2 + 2βiε/parenrightbig
29Published in Transactions on Machine Learning Research (01/2024)
Hence,
v≤50
9/tildewide∆2
i,ε/parenleftig
σi√
2 + 2βiε/parenrightig2
log(t).
Step 4. Using All the previous steps, we prove the theorem. Proof: We have
E[Ti(t)] =E/bracketleftiggt/summationdisplay
t=11{At=i}/bracketrightigg
≤⌊max(v,slim(t))⌋+E
t/summationdisplay
t=⌊max(v,slim(t))⌋+11{At=iand (28) is false}

≤⌊max(v,slim(t))⌋+E
t/summationdisplay
t=⌊max(v,slim(t))⌋+11{(26) or (27) or (29) is true }

=⌊max(v,slim(t))⌋+t/summationdisplay
t=⌊min(v,slim(t))⌋+1P((26) or (27) is true )
≤⌊max(v,slim(t))⌋+ 2t/summationdisplay
t=⌊min(v,slim(t))⌋+15
t
using the harmonic series bound by log(t) + 1, we have
E[Ti(t)]≤max(v,slim(t)) + 10(log( t) + 1)
Then, we replace the value of v,
First setting: /tildewide∆i,ε>12σ2
i
βi/parenleftig√
2 + 2βi
σiε/parenrightig2
E[Ti(t)]≤log(t) max/parenleftigg
96βi
9/tildewide∆i,ε,4
(p−5ε)2/parenleftbigg
1 + 2√
2/parenleftbigg
ε∨9
14√
2/parenrightbigg/parenrightbigg2/parenrightigg
+ 10(log(t) + 1)
Second setting: if/tildewide∆i,ε≤12σ2
i
βi/parenleftig√
2 + 2βi
σiε/parenrightig2
, then
E[Ti(t)]≤log(n) max/parenleftigg
50
9/tildewide∆2
i,ε/parenleftig
σi√
2 + 2βiε/parenrightig2
,4
(p−5ε)2/parenleftbigg
1 + 2√
2/parenleftbigg
ε∨9
14√
2/parenrightbigg/parenrightbigg2/parenrightigg
+ 10(log(t) + 1).
This concludes the proof of Theorem 3.
30Published in Transactions on Machine Learning Research (01/2024)
B Proof of Technical Lemmas and Corollaries
B.1 Preliminary lemmas
B.1.1 Proof of Lemma 1: Regret Decomposition
From Equation (Corrupted regret), we have
Rn=k/summationdisplay
a=1n/summationdisplay
t=1E/bracketleftig
(max
aEPa[X′]−X′
t)1{At=a}/bracketrightig
Then, we condition on At
E/bracketleftig
(max
aEPa[X′]−X′
t)1{At=a}|At/bracketrightig
=1{At=a}E[max
aEPa[X′]−X′
t|At]
=1{At=a}(max
aEPa[X′]−µAt)
=1{At=a}(max
aEPa[X′]−µa) =1{At=a}∆a
and this stays true whatever the policy, because the policy at time tuse knowledge up to time t−1, hence
its decision does not depend on Xt. Hence, we have
Rn(π) =k/summationdisplay
a=1∆aEπ(·|Xn
1,An
1)[Ta(n)]
whereTa(n)is with respect to the randomness of π, which is to say that we compute E[Ti(n)]in the corrupted
setting and not in the uncorrupted one.
Rn=k/summationdisplay
a=1∆aEνε[Ta(n)].
B.1.2 Proof of Lemma 3: KL for Student’s Distribution
First, we compute the χ2divergence between the two laws faandf0. We have, for any a≥0
dχ2(fa,f0) =/integraldisplay(fa(x)−f0(x))2
f0(x)dx
=Γ/parenleftbigd+1
2/parenrightbig
Γ/parenleftbigd
2/parenrightbig√
dπ/integraldisplay
R
1
/parenleftig
1 +(x−a)2
d/parenrightigd+1
2−1
/parenleftbig
1 +x2
d/parenrightbig+1
2
2
/parenleftbigg
1 +x2
d/parenrightbiggd+1
2
dx
=Γ/parenleftbigd+1
2/parenrightbig
Γ/parenleftbigd
2/parenrightbig√
dπ/integraldisplay
R/parenleftbigg/parenleftig
1 +(x−a)2
d/parenrightigd+1
2−/parenleftig
1 +x2
d/parenrightigd+1
2/parenrightbigg2
/parenleftig
1 +(x−a)2
d/parenrightigd+1/parenleftbig
1 +x2
d/parenrightbigd+1
2dx
=Γ/parenleftbigd+1
2/parenrightbig
Γ/parenleftbigd
2/parenrightbig√
dπ
/integraldisplay
Rdx
/parenleftbig
1 +x2
d/parenrightbigd+1
2−2/integraldisplay
Rdx
/parenleftig
1 +(x−a)2
d/parenrightigd+1
2+/integraldisplay
R/parenleftig
1 +x2
d/parenrightigd+1
2
/parenleftig
1 +(x−a)2
d/parenrightigd+1dx
.
The first two terms are respectively equal to 1and−2using the fact that the student distribution integrate
to1. Then, we do the change of variable y=x−ain the last integral to get
dχ2(fa,f0) =Γ/parenleftbigd+1
2/parenrightbig
Γ/parenleftbigd
2/parenrightbig√
dπ/integraldisplay
R/parenleftig
1 +(y+a)2
d/parenrightigd+1
2
/parenleftig
1 +y2
d/parenrightigd+1dy−1.
this is a polynomial of degree din the variable a. We have the following Lemma proven in Section B.3.4.
31Published in Transactions on Machine Learning Research (01/2024)
Lemma 10 Fora≥0andd≥0, we have the following algebraic inequality.
/integraldisplay
R/parenleftig
1 +(y+a)2
d/parenrightigd+1
2
/parenleftig
1 +y2
d/parenrightigd+1dy≤a2
2√
d(d+ 1)2/parenleftbigg
2 +a√
d/parenrightbiggd−1
+/integraldisplay
R(1 +y2/d)d+1
2
/parenleftig
1 +y2
d/parenrightigd+1dy.
Using this lemma, and because we recognize up to a constant the integral of the student distribution on R
in the right-hand side, we have
dχ2(fa,f0) =Γ/parenleftbigd+1
2/parenrightbig
Γ/parenleftbigd
2/parenrightbig√
dπ
a2
2√
d(d+ 1)2/parenleftbigg
2 +a√
d/parenrightbiggd−1
+/integraldisplay
R(1 +y2/d)d+1
2
/parenleftig
1 +y2
d/parenrightigd+1dy
−1
≤Γ/parenleftbigd+1
2/parenrightbig
Γ/parenleftbigd
2/parenrightbig√
dπa2
2√
d(d+ 1)2/parenleftbigg
2 +a√
d/parenrightbiggd−1
then, use that for any d≥1,Γ(d+1
2)≤Γ(d
2)/radicalbig
d/2from Wendel (1948), hence
dχ2(fa,f0)≤a2(d+ 1)2
2√
2dπ/parenleftbigg
2 +a√
d/parenrightbiggd−1
≤a2(d+ 1)2
5√
d/parenleftbigg
2 +a√
d/parenrightbiggd−1
,
using 2√
2π≥5·Then, we use the link between KL divergence and χ2divergence to get the result.
DKL(fa,f0)≤log(1 + dχ2(fa,f0))
≤log/parenleftigg
1 +a2(d+ 1)2
5√
d/parenleftbigg
2 +a√
d/parenrightbiggd−1/parenrightigg
(30)
Then, we have,
log/parenleftigg
1 +a2(d+ 1)2
5√
d/parenleftbigg
2 +a√
d/parenrightbiggd−1/parenrightigg
≤

log/parenleftig
1 + 3d−1(d+1)2
5√
da2/parenrightig
ifa<1
log/parenleftbigg
1 +(d+1)2
5√
dad+1/parenleftig
(d+1)2
√
d+1√
d/parenrightigd−1/parenrightbigg
ifa≥1
hence, using that 1≤3d−1(d+1)2
dad+1
log/parenleftigg
1 +a2(d+ 1)2
d/parenleftbigg
2 +a√
d/parenrightbiggd−1/parenrightigg
≤

3d−1(d+1)2
5√
da2ifa<1
(d+ 1) log (a) + log/parenleftig
3d(d+1)2
5√
d/parenrightig
ifa≥1.
Inject this in Equation (30) to get the result.
B.1.3 Proof of Lemma 4: KL for Corrupted Bernoulli Distribution
Letα∈(0,1/2)and denote δxthe Dirac distribution in x. Define
P0= (1−α)δ0+αδ1,
P1=αδ0+ (1−α)δ1,
Q0= (1−ε)(1−α)δ0+ (1−(1−ε)(1−α))δ1,
Q1= (1−(1−ε)(1−α))δ0+ (1−ε)(1−α)δ1.
One can check that Q0= (1−ε)P0+εδ1andQ1= (1−ε)P1+εδ0and henceQ0andQ1are in theε-corrupted
neighborhood of respectively P0andP1.
32Published in Transactions on Machine Learning Research (01/2024)
We have
DKL(Q0,Q1) =/summationdisplay
k∈{0,c}PQ0(X=k) log/parenleftbiggPQ0(X=k)
PQ1(X=k)/parenrightbigg
= (1−ε)(1−α) log/parenleftbigg(1−ε)(1−α)
1−(1−ε)(1−α)/parenrightbigg
+ (1−(1−ε)(1−α)) log/parenleftbigg1−(1−ε)(1−α)
(1−ε)(1−α)/parenrightbigg
= ((1−ε)(1−α)−(1−(1−ε)(1−α))) log/parenleftbigg(1−ε)(1−α)
1−(1−ε)(1−α)/parenrightbigg
= (1−2ε−2α+ 2εα) log/parenleftbigg
1 +1−2ε−2α+ 2εα
ε+α−εα/parenrightbigg
Then, note that ∆ =EP1[X]−EP0[X] = (1−2α)andσ2= VarP0(X) = VarP1(X) =α(1−α). Hence, with
α=1
2(1−∆).
DKL(Q0,Q1) = (1−2ε−(1−∆) (1−ε)) log/parenleftbigg
1 +1−2ε−(1−∆) (1−ε)
ε+1
2(1−∆) (1−ε)/parenrightbigg
(31)
= (∆(1−ε)−ε) log/parenleftbigg
1 +∆(1−ε)−ε
1
2(1 +ε)−1
2∆(1−ε)/parenrightbigg
(32)
Uniform bound : ifε>0, we have
DKL(Q0,Q1)≤(1−2ε) log/parenleftbigg
1 +1−2ε
ε/parenrightbigg
.
High distinguishibility regime : in the setting 2σ>∆, we have the bound
DKL(Q0,Q1)≤/parenleftbigg∆
2σ(1−ε)−ε/parenrightbigg
log/parenleftigg
1 + 2∆
2σ(1−ε)−ε
1−/parenleftbig∆
2σ(1−ε)−ε/parenrightbig/parenrightigg
=/parenleftbigg∆(1−ε)−2σε
2σ/parenrightbigg
log/parenleftbigg
1 + 2∆(1−ε)−2σε
2σ−(∆(1−ε)−2σε)/parenrightbigg
Low distinguishibility regime : if∆≤2σε√1−2ε. Then there exists ε′≤εsuch that ∆ = 2σε′
√1−2ε′
and then, from Equation (31), there exists Q′
0,Q′
1which areε′-corrupted versions of P0andP1such that
KL(Q′
0,Q′
1) = 0
33Published in Transactions on Machine Learning Research (01/2024)
B.2 Lemmas for Regret upper bound
B.2.1 Proof of Corollary 1: Simplified Upper Bound of HuberUCB
Replacingβiby4σi, we have
•If/tildewide∆i,ε>6σi/parenleftbig
1 + 4√
2ε/parenrightbig2, then
E[Ti(n)]≤log(n) max/parenleftigg
128σi
3/tildewide∆i,ε,4
(p−5ε)2/parenleftbigg
1 + 2√
2/parenleftbigg
ε∨9
14√
2/parenrightbigg/parenrightbigg2/parenrightigg
+ 10(log(n) + 1)
•If/tildewide∆i,ε>6σi/parenleftbig
1 + 4√
2ε/parenrightbig2, then
E[Ti(n)]≤log(n) max/parenleftigg
50σ2
i
9/tildewide∆2
i,ε/parenleftig√
2 + 8ε/parenrightig2
,4
(p−5ε)2/parenleftbigg
1 + 2√
2/parenleftbigg
ε∨9
14√
2/parenrightbigg/parenrightbigg2/parenrightigg
+ 10(log(n) + 1).
Then, we use that
/parenleftbigg
1 + 2√
2/parenleftbigg
ε∨9
14√
2/parenrightbigg/parenrightbigg2
≤2/parenleftigg
1 +/parenleftbigg
2√
2/parenleftbigg
ε∨9
14√
2/parenrightbigg/parenrightbigg2/parenrightigg
= 2 + 8/parenleftbigg
ε2∨81
392/parenrightbigg
≤8ε2+ 2 +648
392≤8ε2+ 4
and thatp−5ε≥1/4, to get
•If/tildewide∆i,ε>6σi/parenleftbig
1 + 4√
2ε/parenrightbig2, then
E[Ti(n)]≤log(n) max/parenleftigg
128σi
3/tildewide∆i,ε,512ε2+ 256/parenrightigg
+ 10(log(n) + 1)
=128
3log(n) max/parenleftigg
σi
/tildewide∆i,ε,12ε2+ 6/parenrightigg
+ 10(log(n) + 1)
≤43 log(n) max/parenleftigg
σi
/tildewide∆i,ε,12ε2+ 6/parenrightigg
+ 10(log(n) + 1)
•If/tildewide∆i,ε>6σi/parenleftbig
1 + 4√
2ε/parenrightbig2, then
E[Ti(n)]≤log(n) max/parenleftigg
50σ2
i
9/tildewide∆2
i,ε/parenleftig√
2 + 8ε/parenrightig2
,512ε2+ 256/parenrightigg
+ 10(log(n) + 1)
≤log(n) max/parenleftigg
100σ2
i
9/tildewide∆2
i,ε/parenleftbig
2 + 64ε2/parenrightbig
,512ε2+ 256/parenrightigg
+ 10(log(n) + 1)
≤23 log(n) max/parenleftigg
σ2
i
/tildewide∆2
i,ε/parenleftbig
1 + 32ε2/parenrightbig
,24ε2+ 12/parenrightigg
+ 10(log(n) + 1)
B.2.2 Proof of Lemma 6: Regret Upper bound for SeqHuberUCB
In this section we virtually copy the proof of the regret for HuberUCB done in Section A.4 with modified
constants and using the crude bound P2(s)≥s/2whenever necessary.
34Published in Transactions on Machine Learning Research (01/2024)
IfAt=ithen at least one of the following four inequalities is true:
SeqHub/logicalandtext
1,T1(t−1)+B1(T1(t−1),t)≤µ1 (33)
or
SeqHub/logicalandtext
i,Ti(t−1)≥µi+Bi(Ti(t−1),t) (34)
or
∆i<2Bi(Ti(t−1),t) (35)
or
P2(T1(t−1))<slim(t) =98 log(t)
128 (p−5ε)2/parenleftbigg
1 + 2√
2/parenleftbigg
ε∨9
14√
2/parenrightbigg/parenrightbigg2
(36)
Indeed, ifP2(Ti(t−1))<slim(t), thenBi(Ti(t−1),t) =∞and Inequality (35) is true. On the other hand,
ifP2(Ti(t−1))≥slim(t), then we have Bi(Ti(t−1),t)is finite and all four inequalities are false, then,
SeqHub/logicalandtext
1,T1(t−1)+B1(T1(t−1),t)>µ1
=µi+ ∆i
≥µi+ 2Bi(Ti(t−1),n)
≥µi+ 2Bi(Ti(t−1),t)
≥SeqHub/logicalandtext
i,Ti(t−1)+Bi(Ti(t−1),t)
which implies that At̸=i.
Step 1. We have that P((33) is true )≤14/t.
Proof:
Then, we have that,
P/parenleftig
SeqHub/logicalandtext
1,T1(t−1)+B1(T1(t−1),t)≤µ1/parenrightig
≤t/summationdisplay
s=1P/parenleftig
SeqHub/logicalandtext
1,s+B1(s,t)≤µ1/parenrightig
=t/summationdisplay
s=⌈slim(t)⌉P/parenleftig
SeqHub/logicalandtext
1,s−µ1≤−B1(s,t)/parenrightig
Then, use Theorem 4, we get
P/parenleftig
SeqHub/logicalandtext
1,T1(t−1)+B1(T1(t−1),t)≤µ1/parenrightig
≤t/summationdisplay
s=⌈slim(t)⌉14e−log(t2)
≤t/summationdisplay
s=⌈slim(t)⌉14
t2≤14
t.
Step 2. Similarly, for arm i, we have
P/parenleftig
SeqHub/logicalandtext
i,Ti(t−1)≥µi+Bi(Ti(t−1),t)/parenrightig
≤14
t
Proof: We have,
P/parenleftig
SeqHub/logicalandtext
i,Ti(t−1)≥µi+Bi(Ti(t−1),t)/parenrightig
≤t/summationdisplay
s=⌈slim(t)⌉P/parenleftig
SeqHub/logicalandtext
i,s−µi≥Bi(s,t)/parenrightig
≤t/summationdisplay
s=⌈slim(t)⌉14e−log(t2)≤14
t.
35Published in Transactions on Machine Learning Research (01/2024)
Step 3. Letv∈N. If one of the two following conditions are true, then for all tsuch thatP2(Ti(t−1))≥v,
we have ∆i≥2Bi(Ti(t−1),t)(i.e. Equation (35) is false).
Condition 1: if /tildewide∆i,ε>12σ2
i
βi/parenleftig√
2 + 2βi
σiε/parenrightig2
andv≤log(t)96βi
9/tildewide∆i,ε.
Condition 2: if /tildewide∆i,ε≤12σ2
i
βi/parenleftig√
2 + 2βi
σiε/parenrightig2
andv≤50
9/tildewide∆2
i,ε/parenleftbig
σi√
2 + 2βiε/parenrightbig2log(t).
Proof: We search for the smallest value v≥slim(t)such that ∆iverifies
∆i≥2Bi(v,t) = 2rv(1/t2) + 2
1
p−/radicalig
log(t2)
2v−ε−1
rP2(v)(1/t2) + 2bi.
First, we simplify the expression, having that v≥slim(t), we have
log(t2)
2v≤128(p−5ε)2
98(1 + 9/7)2≤(p−ε)2
4,
hencerv(1/t2)≤2
(p−ε)/parenleftbigg
σi/radicalig
2 log(t2)
v/parenrightbigg
and we simplify the condition to
∆i≥4
(p−ε)/parenleftigg
σi/radicalbigg
2 log(t2)
v+βilog(t2)
3v+ 2βiε/radicalbigg
log(t2)
v+ 2βiε/parenrightigg
+4
p−ε/parenleftigg
σi/radicaligg
2 log(t2)
P2(v)+βilog(t2)
3P2(v)+ 2βiε/radicaligg
log(t2)
P2(v)+ 2βiε/parenrightigg
+ 2bi
≥12
(p−ε)/parenleftigg
σi/radicalbigg
2 log(t2)
v+βilog(t2)
3v+ 2βiε/radicalbigg
log(t2)
v+ 2βiε/parenrightigg
+ 2bi
where we used that P2(v)≥v/2.
Let us denote /tildewide∆i,ε= (∆i−2bi)(p−ε)−24βiε, we are searching for vsuch that
βilog(t2)
3v+/radicalbigg
log(t2)
v/parenleftig
σi√
2 + 2βiε/parenrightig
−/tildewide∆i,ε
12≤0
This is a second order polynomial in/radicalbig
log(t2)/v.
If/tildewide∆i,ε>0, then the smallest v>0is
/radicalbigg
log(t2)
v=3
2βi
−/parenleftig
σi√
2 + 2εβi/parenrightig
+/radicaligg
/parenleftig
σi√
2 + 2βiε/parenrightig2
+/tildewide∆i,εβi
9
.
First setting: if/tildewide∆i,ε>36σ2
i
βi/parenleftig√
2 + 2βi
σiε/parenrightig2
,
In that case, we have
/radicalbigg
log(t2)
v≥3
2βi
−/parenleftig
σi√
2 + 2βiε/parenrightig
+/radicaligg
βi/tildewide∆i,ε
9
≥3
2βi/radicaligg
βi/tildewide∆i,ε
36=/radicaligg
/tildewide∆i,ε
16βi
Hence,v≤log(t)32βi
/tildewide∆i,ε.
36Published in Transactions on Machine Learning Research (01/2024)
Second setting: if/tildewide∆i,ε≤36σ2
i
βi/parenleftig√
2 + 2βi
σiε/parenrightig2
, then we use Lemma 9, using that
/tildewide∆i,εβi
9/parenleftbig
σi√
2 + 2βiε/parenrightbig2≤4
and the fact that√1+4−1
4≥3
10, we get,
/radicalbigg
log(t2)
v≥/tildewide∆i,ε
20/parenleftbig
σi√
2 + 2βiε/parenrightbig
Hence,
v≤40
/tildewide∆2
i,ε/parenleftig
σi√
2 + 2βiε/parenrightig2
log(t).
Step 4. Using All the previous steps, we prove the theorem.
Proof: We have
E[Ti(t)] =E/bracketleftiggt/summationdisplay
t=11{At=i}/bracketrightigg
≤⌊max(v,2slim(t))⌋+E
t/summationdisplay
t=⌊max(v,2slim(t))⌋+11{At=iand (35) is false}

≤⌊max(v,2slim(t))⌋+E
t/summationdisplay
t=⌊max(v,2slim(t))⌋+11{(33) or (34) or (36) is true }

=⌊max(v,2slim(t))⌋+t/summationdisplay
t=⌊min(v,2slim(t))⌋+1P((33) or (34) is true )
≤⌊max(v,2slim(t))⌋+ 2t/summationdisplay
t=⌊min(v,2slim(t))⌋+114
t
using the harmonic series bound by log(t) + 1, we have
E[Ti(t)]≤max(v,2slim(t)) + 28(log( t) + 1)
Then, we replace the value of v,
First setting: /tildewide∆i,ε>36σ2
i
βi/parenleftig√
2 + 2βi
σiε/parenrightig2
E[Ti(t)]≤log(t) max/parenleftigg
32βi
/tildewide∆i,ε,8
(p−5ε)2/parenleftbigg
1 + 2√
2/parenleftbigg
ε∨9
14√
2/parenrightbigg/parenrightbigg2/parenrightigg
+ 28(log(t) + 1)
Second setting: if/tildewide∆i,ε≤36σ2
i
βi/parenleftig√
2 + 2βi
σiε/parenrightig2
, then
E[Ti(t)]≤log(n) max/parenleftigg
40
/tildewide∆2
i,ε/parenleftig
σi√
2 + 2βiε/parenrightig2
,8
(p−5ε)2/parenleftbigg
1 + 2√
2/parenleftbigg
ε∨9
14√
2/parenrightbigg/parenrightbigg2/parenrightigg
+ 28(log(t) + 1).
Finish the proof of the Theorem using the given values for the constants βi,ε,p.
37Published in Transactions on Machine Learning Research (01/2024)
B.3 Lemmas for concentration of robust estimators
B.3.1 Proof of Lemma 7: Controlling Variance of Influence of Huber’s Estimator
Letρβbe Huber’s loss function, with ψβ=ρ′
β. We have that for any x>0,ψβ(x)2≤2ρβ(x). Hence,
Var(ψβ(Y−Hubβ(P))) =E[ψβ(Y−Hubβ(P))2]≤2E[ρβ(Y−Hubβ(P))].
Then, use that by definition of Hubβ(P),Hubβ(P)is a minimizer of θ∝⇕⊣√∫⊔≀→E[ρβ(Y−θ)], hence,
Var(ψβ(Y−Hubβ(P)))≤2E[ρβ(Y−E[Y])].
and finally, use that ρβ(x)≤x2/2to conclude.
B.3.2 Proof of Lemma 8 : Concentrating Huber’s Estimator by Concentrating the Influence
For alln∈N∗,λ>0, let
fn(λ) =sign(∆n)
nn/summationdisplay
i=1ψβ(Xi−Hubβ(P)−λsign(∆n)),
where ∆n= Hubβ(P)−Hubβ(Xn
1).
Step 1. For anyλ>0,P(|∆n|≥λ)≤P(fn(λ)≥0).
Proof: For ally∈R, letJn(y) =1
n/summationtextn
i=1ρβ(Xi−y)we have,
J′′
n(y) =1
nn/summationdisplay
i=1ψ′
β(Xi−y).
In particular, having fn(λ) =−sign(∆n)J′(Hubβ(P) +λsign(∆n))if we take the derivative of fnwith
respect toλ, we have the following equation
∂
∂λfn(λ) =−sign(∆n)2J′′
n(Hubβ(P) +λsign(∆n))
≤−1
nn/summationdisplay
i=1ψ′
β(Xi−Hubβ(P)−λsign(∆n)). (37)
Then, because ψ′
βis non-negative, the function λ∝⇕⊣√∫⊔≀→fn(λ,)is non-increasing. Hence, for all n∈N∗and
λ>0,
|∆n|≥λ⇒fn(|∆n|) = 0≤fn(λ),
Hence,
P(|∆n|≥λ)≤P(fn(λ)≥0). (38)
Step 2. For allλ>0,
fn(λ)≤fn(0)−λinf
t∈[0,λ]|f′
n(t)|.
Proof: We apply Taylor’s inequality to the function fn. Asfnis non-increasing (because its derivative
is non-positive, see Equation (37)), we get
fn(λ)≤fn(0)−λinf
t∈[0,λ]|f′
n(t)|.
38Published in Transactions on Machine Learning Research (01/2024)
Step 3. Letmn=E/bracketleftig
inft∈[0,λ]1
n/summationtextn
i=1ψ′
β(X′
i−Hubβ(P)−t)/bracketrightig
. With probability larger than 1−2e−2nη2,
inf
t∈[0,λ]|f′
n(t))|≥mn−2η−ε,
Proof: Write that Xi= (1−Wi)Yi+WiZiwhereW1,...,Wnare i.i.d Bernoulli random variable with
meanε,Y1,...,Ynare i.i.d∼PandZ1,...,Znare i.i.d with law H.
From equation (37),
|f′
n(t))|≥1
nn/summationdisplay
i=1ψ′
β(Xi−Hubβ(P)−tsign(∆))
≥1
nn/summationdisplay
i=11{Wi= 0}ψ′
β(Yi−Hubβ(P)−tsign(∆)) (39)
+1
nn/summationdisplay
i=11{Wi= 1}ψ′
β(Zi−Hubβ(P)−tsign(∆)) (40)
≥1
nn/summationdisplay
i=1ψ′
β(Yi−Hubβ(P)−tsign(∆)) (41)
+1
nn/summationdisplay
i=11{Wi= 1}/parenleftbig
ψ′
β(Zi−Hubβ(P)−tsign(∆))−ψ′
β(Wi−Hubβ(P)−tsign(∆))/parenrightbig
(42)
Hence, because ψ′
β∈[0,1], we have
|f′
n(t))|≥1
nn/summationdisplay
i=1ψ′
β(Yi−Hubβ(P)−tsign(∆))−1
nn/summationdisplay
i=11{Wi= 1}) (43)
The right-hand side depends on the infimum of the mean of ni.i.d random variables in [0,1]. Hence, the
function
Z(Xn
1)∝⇕⊣√∫⊔≀→sup
t∈[0,λ]n/summationdisplay
i=1ψ′
β(X′
i−Hubβ(P)−t)
satisfies, by sub-linearity of the supremum operator and triangular inequality, the bounded difference prop-
erty, with differences bounded by 1. Hence, by Hoeffding’s inequality, we get with probability larger than
1−e−2nη2,
inf
t∈[0,λ]|f′
n(t))|≥E/bracketleftigg
inf
t∈[0,λ]1
nn/summationdisplay
i=1ψ′
β(X′
i−Hubβ(P)−t)/bracketrightigg
−η−1
nn/summationdisplay
i=11{Wi= 1})
and using Hoeffding’s inequality to control1
n/summationtextn
i=11{Wi= 1}, we have with probability larger than 1−
2e−2η2/n,
inf
t∈[0,λ]|f′
n(t))|≥E/bracketleftigg
inf
t∈[0,λ]1
nn/summationdisplay
i=1ψ′
β(X′
i−Hubβ(P)−t)/bracketrightigg
−2η−ε
Step 4. Forλ∈(0,β/2),
P(|∆n|≥λ)≤P/parenleftigg/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle1
nn/summationdisplay
i=1ψβ(Xi−Hubβ(P))/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle≥λ(mn−η−ε)/parenrightigg
+ 2e−2nη2.
39Published in Transactions on Machine Learning Research (01/2024)
Proof: For anyλ>0, we have
P(|∆n|≥λ)≤P(fn(λ)≥0) ( from Step 1 )
≤1−P/parenleftbigg
fn(0)−λinf
t∈[0,λ]|f′
n(t)|≤0/parenrightbigg
(from Step 2 )
≤1−P(fn(0)≤λ(mn−2η−ε)) + 2e−2nη2(from Step 3 )
=P/parenleftigg/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle1
nn/summationdisplay
i=1ψβ(Xi−Hubβ(P))/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle≥λ(mn−η−ε)/parenrightigg
+ 2e−2nη2. (44)
Step 5. We prove that mn≥p,and hence
P(|∆n|≥λ)≤P/parenleftigg/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle1
nn/summationdisplay
i=1ψβ(Xi−Hub(P))/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle≥λ(p−η−ε)/parenrightigg
+ 2e−2nη2
Proof: For allλ≤β/2,
E/bracketleftigg
inf
t∈[0,λ]1
nn/summationdisplay
i=1ψ′
β(X′
i−Hubβ(P)−t)/bracketrightigg
=E/bracketleftigg
inf
t∈[0,λ]1
nn/summationdisplay
i=11{|X′
i−Hubβ(P)−t|≤β}/bracketrightigg
≥E/bracketleftigg
1
nn/summationdisplay
i=11{|X′
i−Hubβ(P)|≤β−λ}/bracketrightigg
≥E/bracketleftigg
1
nn/summationdisplay
i=11{|X′
i−Hubβ(P)|≤β/2}/bracketrightigg
=p
Then, we plug the bound on mnfound in the previous step in equation (44), we get for any η > 0and
λ∈(0,β/2],
P(|∆n|≥λ)≤P/parenleftigg/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle1
nn/summationdisplay
i=1ψβ(Xi−Hubβ(P))/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle≥λ(p−η−ε)/parenrightigg
+ 2e−2nη2
B.3.3 Proof of Lemma 9: Algebra tool for bounding polinomial roots
The solutions of the second order polynomial indicate that xmust verify
x≥−b+√
b2+ 4ac
2a≥b
2a/parenleftigg
−1 +/radicalbigg
1 +4ac
b2/parenrightigg
.
Then, use that the function x∝⇕⊣√∫⊔≀→√x+ 1is concave and hence the graph of x∝⇕⊣√∫⊔≀→√x+ 1is above its chords
and we have for any x∈[0,d],√1 +x≥1 +x√d+1−1
d. Hence,
x≥b
2a/parenleftbigg4ac(√
d+ 1−1)
db2/parenrightbigg
=2c(√
d+ 1−1)
db.
40Published in Transactions on Machine Learning Research (01/2024)
B.3.4 Proof of Lemma 10: Algebra on Student’s distribution
We have,
/integraldisplay
R/parenleftig
1 +(y+a)2
d/parenrightigd+1
2
/parenleftig
1 +y2
d/parenrightigd+1dy=/integraldisplay
Rd+1
2/summationdisplay
l=0/parenleftbiggd+1
2
l/parenrightbigg(y+a)2l
dl/parenleftig
1 +y2
d/parenrightigd+1dy
=/integraldisplay
Rd+1
2/summationdisplay
l=02l/summationdisplay
j=0/parenleftbiggd+1
2
l/parenrightbigg/parenleftbigg2l
j/parenrightbiggyja2l−j
dl/parenleftig
1 +y2
d/parenrightigd+1dy
=d+1
2/summationdisplay
l=02l/summationdisplay
j=0/parenleftbiggd+1
2
l/parenrightbigg/parenleftbigg2l
j/parenrightbigg/integraldisplay
Ryja2l−j
dl/parenleftig
1 +y2
d/parenrightigd+1dy
Remark that the integral is 0ifjis odd. Hence,
/integraldisplay
R/parenleftig
1 +(y+a)2
d/parenrightigd+1
2
/parenleftig
1 +y2
d/parenrightigd+1dy=d+1
2/summationdisplay
l=0l/summationdisplay
j=1/parenleftbiggd+1
2
l/parenrightbigg/parenleftbigg2l
2j/parenrightbigga2l−2j
dl/integraldisplay
Ry2j
/parenleftig
1 +y2
d/parenrightigd+1dy
Then, we compute the integrals. By change of variable u=y/d, we have
/integraldisplay
Ry2j
/parenleftig
1 +y2
d/parenrightigd+1dy=dj+1/2/integraldisplay
Ru2j
(1 +u2)d+1du≤2dj+1/2
and forl=j,
d+1
2/summationdisplay
l=0/parenleftbiggd+1
2
l/parenrightbigg1
dl/integraldisplay
Ry2l
/parenleftig
1 +y2
d/parenrightigd+1dy=/integraldisplay
R(1 +y2/d)d+1
2
/parenleftig
1 +y2
d/parenrightigd+1dy
Hence,
/integraldisplay
R/parenleftig
1 +(y+a)2
d/parenrightigd+1
2
/parenleftig
1 +y2
d/parenrightigd+1dy≤2d+1
2/summationdisplay
l=1l−1/summationdisplay
j=0/parenleftbiggd+1
2
l/parenrightbigg/parenleftbigg2l
2j/parenrightbigga2l−2j
dldj+1/2+/integraldisplay
R(1 +y2/d)d+1
2
/parenleftig
1 +y2
d/parenrightigd+1dy
= 2d+1
2/summationdisplay
l=1a2ll−1/summationdisplay
j=0/parenleftbiggd+1
2
l/parenrightbigg/parenleftbigg2l
2j/parenrightbigga−2j
dldj+1/2+/integraldisplay
R(1 +y2/d)d+1
2
/parenleftig
1 +y2
d/parenrightigd+1dy
≤2d+1
2/summationdisplay
l=1a2ll−1/summationdisplay
j=0/parenleftbiggd+1
2
l/parenrightbigg/parenleftbigg2l
2j/parenrightbigga−2j
dldj+1/2+/integraldisplay
R(1 +y2/d)d+1
2
/parenleftig
1 +y2
d/parenrightigd+1dy (45)
(46)
And,
d+1
2/summationdisplay
l=1a2ll−1/summationdisplay
j=0/parenleftbiggd+1
2
l/parenrightbigg/parenleftbigg2l
2j/parenrightbigga−2j
dldj+1/2=√
dd+1
2/summationdisplay
l=1l−1/summationdisplay
j=0/parenleftbiggd+1
2
l/parenrightbigg/parenleftbigg2l
2j/parenrightbigg
a2(l−j)dj−l
≤√
dd+1
2/summationdisplay
l=1l−1/summationdisplay
j=0/parenleftbiggd+1
2
l/parenrightbigg/parenleftbigg2(l−1)
2j/parenrightbigg
l2/parenleftbigga2
d/parenrightbiggl−j
.
41Published in Transactions on Machine Learning Research (01/2024)
Using that/parenleftbig2l
2j/parenrightbig
=/parenleftbig2(l−1)
2j/parenrightbig2l(2l−1)
(2l−2j)(2l−2j−1)≤/parenleftbig2(l−1)
2j/parenrightbig
l2.
Then, completing the binomial sum so that
l−1/summationdisplay
j=0/parenleftbigg2(l−1)
2j/parenrightbigg/parenleftbigga2
d/parenrightbigg−j
≤2(l−1)/summationdisplay
j=0/parenleftbigg2(l−1)
2j/parenrightbigg/parenleftbigga2
d/parenrightbigg−j
=/parenleftigg
1 +√
d
a/parenrightigg2(l−1)
,
we have,
d+1
2/summationdisplay
l=1a2ll−1/summationdisplay
j=0/parenleftbiggd+1
2
l/parenrightbigg/parenleftbigg2l
2j/parenrightbigga−2j
dldj+1/2≤1
2(d+ 1)√
dd+1
2/summationdisplay
l=1/parenleftbiggd+1
2
l/parenrightbigg/parenleftbigga2
d/parenrightbiggl
l/parenleftigg
1 +√
d
a/parenrightigg2(l−1)
=a2
2d(d+ 1)√
dd+1
2/summationdisplay
l=1/parenleftbiggd+1
2
l/parenrightbigg
l/parenleftbigga√
d+ 1/parenrightbigg2(l−1)
=a2
2d(d+ 1)√
dd−1
2/summationdisplay
l=0/parenleftbiggd−1
2
l/parenrightbigg(d+ 1)(l+ 1)
2(l+ 1)/parenleftbigga√
d+ 1/parenrightbigg2l
≤a2
4√
d(d+ 1)2/parenleftbigg
2 +a√
d/parenrightbiggd−1
Then, inject this in Equation (45) to get
/integraldisplay
R/parenleftig
1 +(y+a)2
d/parenrightigd+1
2
/parenleftig
1 +y2
d/parenrightigd+1dy≤a2
2√
d(d+ 1)2/parenleftbigg
2 +a√
d/parenrightbiggd−1
+/integraldisplay
R(1 +y2/d)d+1
2
/parenleftig
1 +y2
d/parenrightigd+1dy.
42Published in Transactions on Machine Learning Research (01/2024)
C Additional experimental results
C.1 Sensitivity to βandε
In this section, we illustrate the impact of the choice of βandεon the estimation.
Choice of β(Figure 5(b)): The choice of βis a trade-off between the bias (distance |Hubβ(P)−E[X]|
which decreases as βgo to infinity) and robustness (when βgoes to 0,Hubβ(P)goes to the median). To
illustrate this trade-off we use the Weibull distribution for which can be very asymmetric. We use a 3-
armed bandit problem with shape parameters (2,2,0.75)and scale parameters (0.5,0.7,0.8)which implies
that the means are approximately (0.44,0.62,0.95). These distributions are very asymmetric, hence the bias
|Hubβ(P)−E[X]|is high and in fact even though arm 3 has the optimal mean, arm 2 will have the optimal
median, the medians are given by (0.41,0.58,0.49). In this experiment we don’t use any corruption as we
don’t want to complicate the interpretation. As expected by the theory, we get that βishould not be too
small or too large but it should be around 4σi.
Choice of ε(Figure 5(a)): To illustrate the dependency in ε, we also use the Weibull distribution to
show the dependency in εwith the same parameters as in the previous Weibull example, except that we
chooseβi= 5σiwhich is around the optimum found in the previous experiment and we corrupt with 2%
of outliers (this is the true εwhile we will make the εused in the definition of the algorithm vary). The
outliers are constructed as in Section 7. The effect of the parameter εis difficult to assess because εhas an
impact on the length of force exploration that we impose at the beginning of our algorithm (the slim).
Figure 4: Cumulative regret plots for different values of the parameters εandβon a Weibull dataset.
0 2000 4000 6000 8000 10000
n050100150200Regretparam. 
0
0.01
0.02
0.03
0.05
(a) Dependency in ε
0 2000 4000 6000 8000 10000
n020406080100120Regret
4i
5i
10i
20i
 (b) Dependency in β
C.2 Corrupted bandits with adversarial algorithms
Toillustratetheperformancesofclassicalalgorithmsoncorruptedbanditsproblems, weredotheexperiments
from Section 7 with algorithms from the adversarial literature (Exp3 and FTRL with log-barrier). We also
include Thompson sampling in the case of Bernoulli inlier distributions. The results are rendered in Figure 5.
These results show that adversarial algorithms like EXP3 and FTRL, and also Thompson Sampling are very
inefficient when the corruption is important as in the case of the Pareto experiments with ε= 0.03and
ε= 0.05.
43Published in Transactions on Machine Learning Research (01/2024)
0100200300RegretBernoulli=0
 =0.03
 =0.05
02505007501000RegretStudent
0 5000 10000 15000
n0100020003000RegretPareto
0 5000 10000 15000
n
EXP3 FTRL SeqHuberUCB TS0 5000 10000 15000
n
Figure 5: Cumulative regret plot of the algorithms on a corrupted Bernoulli (above), Student’s (middle)
and Pareto (below) reward distributions with various corruption levels ε. Lower corrupted regret indicates
better performance for an algorithm.
44