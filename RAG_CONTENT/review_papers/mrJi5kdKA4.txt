Published in Transactions on Machine Learning Research (04/2024)
DSI2I: Dense Style for Unpaired Exemplar-based Image-to-
Image Translation
Baran Ozaydin baran.ozaydin@epfl.ch
School of Computer and Communication Sciences, EPFL, Switzerland
Tong Zhang tong.zhang@epfl.ch
School of Computer and Communication Sciences, EPFL, Switzerland
Sabine Süsstrunk sabine.susstrunk@epfl.ch
School of Computer and Communication Sciences, EPFL, Switzerland
Sabine Süsstrunk sabine.susstrunk@epfl.ch
School of Computer and Communication Sciences, EPFL, Switzerland
Reviewed on OpenReview: https: // openreview. net/ forum? id= mrJi5kdKA4
Abstract
Unpaired exemplar-based image-to-image (UEI2I) translation aims to translate a source
image to a target image domain with the style of a target image exemplar, without ground-
truth input-translation pairs. Existing UEI2I methods represent style using one vector per
image or rely on semantic supervision to define one style vector per object. Here, in contrast,
we propose to represent style as a dense feature map, allowing for a finer-grained transfer
to the source image without requiring any external semantic information. We then rely on
perceptual and adversarial losses to disentangle our dense style and content representations.
To stylize the source content with the exemplar style, we extract unsupervised cross-domain
semantic correspondences and warp the exemplar style to the source content. We demon-
strate the effectiveness of our method on four datasets using standard metrics together with
a localized style metric we propose, which measures style similarity in a class-wise man-
ner. Our results show that the translations produced by our approach are more diverse,
preserve the source content better, and are closer to the exemplars when compared to the
state-of-the-art methods. Project page: https://github.com/IVRL/dsi2i
1 Introduction
Unpaired image-to-image (UI2I) translation aims to translate a source image to a target image domain
by training a deep network using images from the source and target domains without ground-truth input-
translation pairs. In the exemplar-based scenario (UEI2I), an additional target image exemplar is provided
as input so as to further guide the style translation. Ultimately, the resulting translation should 1) preserve
the content/semantics of the source image; 2) convincingly seem to belong to the target domain; and 3)
adopt the specific style of the target exemplar image.
Some existing UEI2I strategies Huang et al. (2018); Lee et al. (2018) encode the style of the exemplar using a
global, image-level feature vector. While this has proven to be effective for relatively simple scenes, it leads to
undesirable artifacts for complex, multi-object ones, as illustrated in Fig. 1, where appearance information of
the dominating semantic regions, such as sky, unnaturally bleeds into other semantic areas, such as the road,
trees and buildings. Other UEI2I methods Bhattacharjee et al. (2020); Jeong et al. (2021); Kim et al. (2022);
Shen et al. (2019) address this by computing instance-wise or class-wise style representations. However, they
require knowledge of the scene semantics, e.g., segmentation masks or bounding boxes during training, which
limits their applicability.
1Published in Transactions on Machine Learning Research (04/2024)
Source Baseline DSI2I Exemplar
Figure 1: Global style vs dense style representations. The baseline method (MUNIT) Huang et al.
(2018) represents the exemplar style with a single feature vector per image. As such, some appearance
information from the exemplar bleeds into semantically-incorrect regions, giving, for example, an unnatural
bluish taint to the road and the buildings in the second row, first image. By modeling style densely, our
approach better respects the semantics when applying the style from the exemplar to the source content.
Our method also has finer-grained control over style. The color of the road and center line in the third row
reflect the exemplar appearance more accurately.
By contrast, we propose to model style densely. That is, we represent the style of an image with a feature
tensor that has the same spatial resolution as the content one. The difficulty of having spatial information
in style is that style information can more easily pollute the content one, and vice versa. To prevent this
and encourage the disentanglement of style and content, we utilize perceptual and adversarial losses, which
encourages the model to preserve the source content and semantics.
A dense style representation alone is not beneficial for UEI2I as the spatial arrangement of each dense style
component is only applicable for its own image. Hence, we propose a cross-domain semantic correspondence
module to spatially arrange/warp the dense style of the target image to the source content. To that end, we
utilize the CLIP Radford et al. (2021) vision backbone as feature extractor and establish correspondences
between the features of the source and target images using Optimal Transport Cuturi (2013); Liu et al.
(2020).
As a consequence, and as shown in Fig. 1, our approach transfers the local style of the exemplar to the source
content in a more natural manner than the global-style techniques. Yet, in contrast to Bhattacharjee et al.
(2020); Jeong et al. (2021); Kim et al. (2022); Shen et al. (2019), we do not require semantic supervision
during training, thanks to our dense modeling of style. To quantitatively evaluate the benefits of our
approach, we introduce a metric that better reflects the stylistic similarity between the translations and the
exemplars than the image-level metrics used in the literature such as FID Heusel et al. (2017), IS Salimans
et al. (2016), and CIS Huang et al. (2018).
Our contributions can be summarized as follows:
•We propose a dense style representation for UEI2I. Our method retains the source content in the
translation while providing finer-grain stylistic control.
•We show that adversarial and perceptual losses encourage the disentanglement of our dense style
and content representations.
•Wedevelopacross-domainsemanticcorrespondencemoduletowarptheexemplarstyletothesource
content.
•We propose a localized style metric to measure the stylistic accuracy of the translation.
Our experiments show both qualitatively and quantitatively the benefits of our method over global, image-
level style representations. We will make our code publicly available upon acceptance.
2Published in Transactions on Machine Learning Research (04/2024)
Method UnpairedLabel
FreeMulti
-modalExemplar
guidedLocal
style
MUNIT Huang et al. (2018) ✓ ✓ ✓ ✓ ✗
DRIT Lee et al. (2018) ✓ ✓ ✓ ✓ ✗
CUT Park et al. (2020) ✓ ✓ ✗ ✗ ✗
FSeSim Zheng et al. (2021) ✓ ✓ ✓ ✗ ✗
INIT Shen et al. (2019) ✓ ✗ ✓ ✓ ✓
DUNIT Bhattacharjee et al. (2020) ✓ ✗ ✓ ✓ ✓
MGUIT Jeong et al. (2021) ✓ ✗ ✓ ✓ ✓
CoCosNet Zhang et al. (2020) ✗ ✗ ✓ ✓ ✓
MCLNet Zhan et al. (2022b) ✗ ✗ ✓ ✓ ✓
MATEBIT Jiang et al. (2023) ✗ ✗ ✓ ✓ ✓
DSI2I ✓ ✓ ✓ ✓ ✓
Table 1:Comparison of I2I methods. Unpaired methods do not require ground-truth translation pairs.
Label free methods do not require object or segmentation annotations. Multimodal methods can produce
multiple translations for one content. Exemplar guided methods can stylize the translations based on an
exemplar image. The methods that represent style object-wise or densely have local style control.
2 Related Work
Our method primarily relates to three lines of research: Image-to-image (I2I) translation, Style Transfer, and
Semantic Correspondence. Our main source of inspiration is I2I research as it deals with content preservation
and domain fidelity. However, we borrow concepts from Style Transfer when it comes to adopting exemplar
style and evaluating stylistic accuracy. Furthermore, our approach to swapping styles across semantically
relevant parts of different images is related to semantic correspondences.
2.1 Image-to-image Translation
We focus the discussion of I2I methods on the unpaired scenario, as our method does not utilize paired data.
CycleGAN Zhu et al. (2017) was the first work to address this by utilizing cycle consistency. Recent works Hu
et al. (2022); Jung et al. (2022); Park et al. (2020); Zheng et al. (2021) lift the cycle consistency requirement
and perform one-sided translation using contrastive losses and/or self-similarity between the source and the
translation. Many I2I methods, however, are unimodal, in that they produce a single translation per input
image, thus not reflecting the diversity of the target domain, especially in the presence of high within-domain
variance. Although some works Jung et al. (2022); Zheng et al. (2021) extend this to multimodal outputs,
they cannot adopt the style of a specific target exemplar, which is what we address.
Some effort has nonetheless been made to develop exemplar-guided I2I methods. For example, Huang et al.
(2018); Lee et al. (2018) decompose the images into content and style components, and generate exemplar-
basedtranslationsbymergingtheexemplarstylewiththecontentofthesourceimage. However, thesemodels
define a single style representation for the whole image, which does not reflect the complexity of multi-object
scenes. By contrast, Bhattacharjee et al. (2020); Jeong et al. (2021); Kim et al. (2022); Mo et al. (2018); Shen
et al. (2019) reason about object instances for I2I translation. Their goal is thus similar to ours, but their
style representations focus on foreground objects only, and they require object-level (pseudo) annotations
during training. Moreover, these methods do not report how stylistically close their translations are to the
exemplars. Here, we achieve dense style transfer for more categories without requiring annotations and show
that our method generates translations closer to the exemplar style while having comparable domain fidelity
with that of the state-of-the-art methods.
2.2 Style Transfer
Style transfer aims to bring the appearance of a content image closer to a target image. The seminal work
of Gatys et al. Gatys et al. (2016) achieves so by matching the Gram matrices of the two images via image-
3Published in Transactions on Machine Learning Research (04/2024)
based optimization. Li et al. (2017b) provides an analytical solution to Gram matrix alignment, enabling
arbitrary style transfer without image based optimization. Huang & Belongie (2017) only matches the
diagonal of the Gram matrices by adjusting the channel means and standard deviations. Li et al. (2017a)
shows that matching the Gram matrices minimizes the Maximum Mean Discrepancy between the two feature
distributions. Inspired by this distribution interpretation, Kolkin et al. (2019) proposes to minimize a relaxed
Earth Movers Distance between the two distributions, showing the effectiveness of Optimal Transport in style
transfer. Zhang et al. (2019) defines multiple styles per image via GrabCut and exchanges styles between
the local regions in two images. Kolkin et al. (2019); Zhang et al. (2019) are particularly relevant to our
work as they account for the spatial aspect of style. Chiu & Gurari (2022); Li et al. (2018); Yoo et al. (2019)
aim to achieve photorealistic stylization using a pre-trained VGG based autoencoder. Kim et al. (2020); Liu
et al. (2021); Yang et al. (2022) model texture- and geometry-based style separately and learn to warp the
texture-based style to the geometry of another image. However, the geometric warping module they rely on
makes their methods only applicable to images depicting single objects. Our dense style representation and
our evaluation metric are inspired by this research on style transfer. Unlike these works, our image-to-image
translation method operates on complex scenes, deals with domain transfer and does not require image based
optimization.
2.3 Semantic Correspondence
Semantic correspondence methods aim to find semantically related regions across two different images. This
involves the challenging task of matching object parts and fine-grained keypoints. Early approaches Barnes
et al. (2009); Liu et al. (2010) used hand-crafted features. These features, however, are not invariant to
changes in illumination, appearance, and other low-level factors that do not affect semantics. Hence, they
have limited ability to generalize across different scenes. Aberman et al. (2018); Liu et al. (2020); Min
et al. (2019) use ImageNet Simonyan & Zisserman (2014) pre-trained features to address this issue and find
correspondences between images containing similar objects. However, these methods do not generalize to
finding accurate correspondences across images from different modalities/domains.
Semantic correspondences have been explored in the context of image to image translation as well. In
particular, Zhan et al. (2021; 2022b); Zhang et al. (2020); Zhou et al. (2021); Zhan et al. (2022a) use cross-
domain correspondences to guide paired exemplar-based I2I translation. These methods are applicable to
a single dataset where the two paired domains consist of segmentation labels and corresponding images.
Specifically, they aim to translate segmentation labels to real images. In this case, both the I2I and semantic
correspondence tasks benefit from the paired data, i.e., semantic supervision. We also use cross-domain
correspondences, but unlike these works, our method is 1) unpaired and unsupervised, i.e., the ground-truth
translation is unknown; 2) unsupervised in terms of semantics, i.e., we do not use segmentation labels during
training; 3) applicable to translation between two datasets from different domains.
3 Method
Let us now introduce our UEI2I approach using dense style representations. To this end, we first define the
main architectural components of our model. It largely follows the architecture of Huang et al. (2018) and
is depicted in Fig 2. Given two image domains X,Y⊂R3×H′W′, our model consists of two style encoders
Es
X,Es
Y:R3×H′W′→RS×HW, two content encoders Ec
X,Ec
Y:R3×H′W′→RC×HW, two generators GX,
GY:RC×HW×RS×HW→R3×H′W′, and two patch discriminators DX,DY:R3×H′W′→RS×H′′W′′.
The content and style representations are then defined as follows. The content of image xis computed as
Cx:=Ec
X(x), and its dense style as Sdense
x :=Es
X(x). Note that the latter departs from the definition of
style in Huang et al. (2018); here, instead of a global style vector, we use a dense style map with spatial
dimensions, which will let us transfer style in a finer-grained manner. Nevertheless, we also compute a global
style for image xasSglobal
x :=Avg(Sdense
x ), whereAvgdenotes spatial averaging and repeating a vector
across spatial dimensions. Furthermore, we define a mixed style Smix
x:= 0.5Sglobal
x + 0.5Sdense
x. As will be
shown later, this mixed style will allow us to preserve the content without sacrificing stylistic control.
4Published in Transactions on Machine Learning Research (04/2024)
Figure 2:Overview of method. We represent style as a feature map with spatial dimensions and constrain
itviaadversarialandperceptuallossesfordisentanglement. Ourmethoddoesnotrequireanylabelsorpaired
images during training. In test time, we warp the style of the exemplar for the source content using semantic
correspondence. At test time, we utilize the CLIP Radford et al. (2021) vision backbone to build semantic
correspondences. See Section 3 for definitions and explanations.
In the remainder of this section, we first introduce our approach to learning meaningful dense style repre-
sentations during training, shown in the top portion of Fig. 2. We then discuss how dense style is injected
architecturally, and finally how to exchange the dense styles of the source and exemplar images at inference
time, illustrated in the bottom portion of Fig. 2.
3.1 Learning Dense Style
Wedefinestyleaslowlevelattributesthatdonotaffectthesemanticsoftheimage. Theselowlevelattributes
can include lighting, color, appearance, and texture. We also believe that a change in style should not lead
to an unrealistic image and should not modify the semantics of the scene. In this work, we argue that, based
on this definition, style should be 1) represented densely to reflect finer grained stylistic attributes (stylistic
accuracy); 2) constrained by an adversarial loss to encourage fidelity to the target domain (domain fidelity);
3) constrained by a perceptual loss to preserve semantics (content preservation).
To learn a dense style representation that accurately reflects the stylistic attributes of the exemplar, we
utilize theL1reconstruction loss with Sdense
yto enable the flow of fine-grained dense information into the
style representation. This is expressed as
Lrecon =L1(GY(Cy,Sdense
y ),y). (1)
Such an image reconstruction loss encourages the content and dense style representation to contain all the
information in the input image. However, on its own, it does not prevent style from modeling content
and leading to unrealistic or semantic changes when edited. To encourage a rich content representation that
5Published in Transactions on Machine Learning Research (04/2024)
preserves semantics, we use adversarial and perceptual losses with a random style vector r∼N(0,1)∈RS×1
Ladv_random =LGAN(GY(Cx,r),DY), (2)
Lper_random =L1(V(x),V(GY(Cx,r))), (3)
These losses prevent our model from relying too much on dense style for reconstruction and translation by
leading to a richer content representation.
Having a rich content does not prevent dense style from polluting it. To prevent style from modeling content,
we constrain the dense style using the adversarial and perceptual losses
Ladv_global =LGAN(GY(Cx,Sglobal
y ),DY), (4)
Lper_global =L1(V(x),V(GY(Cx,Sglobal
y ))), (5)
whereLGANdenotes a standard adversarial loss, and Vrepresents the VGG16 backbone up to but excluding
the Global Average Pooling layer. The adversarial loss above encourages the fidelity of the translations to
the target domain Goodfellow et al. (2014); Zhu et al. (2017) whereas the perceptual losses help preserve
the semantics Huang et al. (2018); Johnson et al. (2016); Zhu et al. (2017). While the global losses in
Eqs. 4, 5 constrain dense style via the spatial averaging operation, there is no loss that involves Sdense.
Involving Sdensein the adversarial and perceptual losses tends to make the model learn to ignore the style
representation, referred to as style collapse. Also, note that all the constraints in Eqs. 2, 3, 4, 5 use a spatially
constant style representation. Hence, to involve a spatially varying style during the training and to avoid
style collapse, we introduce two losses computed on the mixed style Smix
y, given by
Ladv_mix=LGAN(GY(Cy,Smix
y),DY), (6)
Lper_mix=L1(V(y),V(GY(Cy,Smix
y))), (7)
3.2 Injecting Dense Style
Letusnowdescribehowweinjectadensestylemap, Sdense, inourframeworktoproduceanimage. Accurate
stylization requires the removal of the existing style as an initial step Li et al. (2017b;a). Thus, for our dense
style to be effective, we incorporate a dense normalization that first removes the style of each region. To this
end, inspired by Li et al. (2019); Park et al. (2019); Zhu et al. (2020), we utilize a Positional Normalization
Layer Li et al. (2019) followed by dense modulation. These operations are performed on the generator
activations that produce the images.
Formally, let P∈RC′×HWdenote the generator activations, with C′the number of channels. We compute
the position-wise means and standard deviations of P,µ,σ∈RHW. We then replace the existing style by
our dense one via the Dense Normalization (DNorm) function
FDNorm (P,α,β ) =P−µ
σβ+α, (8)
where the arithmetic operations are performed in an element-wise manner and by replicating µandσ C′
times to match the channel dimension of P. The tensors α,β∈RC′×HWare obtained by applying 1×1
convolutions to the dense style Sdense.
Up to now, we have discussed how to inject dense style in an image and how to learn a meaningful dense
style representation in the training stage. However, one problem remains unaddressed in the test stage: The
dense style extracted from an image is only applicable to that same image because its spatial arrangement
corresponds to that image. In this section, we therefore propose an approach to swapping dense style maps
across two images from different domains.
Our approach is motivated by the intuition that style should be exchanged between semantically similar
regions in both images. To achieve this, we leverage an auxiliary pre-trained network that generalizes
well across various image modalities Radford et al. (2021). Specifically, we extract middle layer features
Fx,Fy∈RF×HWby passing the source and exemplar images through the CLIP-RN50 backbone Radford
6Published in Transactions on Machine Learning Research (04/2024)
et al. (2021). We then compute the cosine similarity between these features, clipping the negative similarity
values to zero. We denote this matrix as Zyx∈RHW×HWand use it to solve an optimal transport problem
as described in Liu et al. (2020); Zhang et al. (2020). We construct our cost matrix Cas
C=1−Zyx,with (9)
Zyx=max(cos(Fy,Fx),0). (10)
We then use Sinkhorn’s algorithm Cuturi (2013) to compute a doubly stochastic optimal transportation
matrix Ayx∈RHW×HW, which corresponds to solving
Ayx= arg min
A⟨A,C⟩F−λh(A) (11)
s.tA1HW=py,AT1HW=px (12)
whereh(A)denotes the entropy of Aandλis the entropy regularization parameter. px,py∈RHW×1
constrain the row and column sums of Ayx, which are chosen as uniform distributions (see the supplementary
material for other choices). Optimal Transport returns a transportation plan Ayxto warp Sdense
yas
Sy→x=Sdense
yAyx, (13)
so that Sy→xis semantically aligned with xinstead of with y. This plan transports style across semantically
similar regions with the constraint that each region receives an equal mass. With this operation, each spatial
element Sy→x[h,w]can be seen as a weighted sum of spatial elements of Sdense
y [h′,w′]with the weights
being proportional to the semantic similarity between Fh,w
xandFh′w′
y. Hence, we can trade the style across
semantically similar regions.
Our semantic correspondence module can also be thought of as a cross attention mechanism across two
images with the queries being Fx, the keys Fyand the values Sdense
y. Note also that global style transfer,
as done in MUNIT Huang et al. (2018), is actually a special case of this formalism where Ayxis a constant
uniform matrix.
3.3 Discussion on Losses and Components
Discussiononthemodelcomponents. Thesemanticcorrespondencematrices AyxbuiltfromCLIPRad-
ford et al. (2021) features are 1) expensive in terms of computation and memory, and 2) noisy as each
point corresponds to the others with some non-negative weight. For example, a self-correspondence ma-
trixAxx(HWxHW) computed with CLIP Radford et al. (2021) would have large diagonal entries, positive
but smaller off-diagonal entries for related semantic pixel pairs, and ideally zero off-diagonal entries for
semantically unrelated pixel pairs. Instead of computing and storing these noisy and costly matrices with
CLIP Radford et al. (2021) during training, we provide the losses with SmixandSglb.
Our intuition for SmixandSglbis that these two style components replace noisy correspondence matrices of
CLIP Radford et al. (2021) during training. Sglbis used to imitate cross-correspondences Ayxand can be
seen as the output of a uniform, constant HWxHW correspondence matrix of 1/HW s (each content pixel
corresponding to all the exemplar pixels equally) as shown in 3 parts a) and d); Sdensecan be seen as the
output of an identity self-correspondence matrix (each pixel corresponding only to itself) as shown in 3 parts
b); and Smixis the output of a noisy self-correspondence matrix, imitating Axx, with large diagonal entries
and uniform non-diagonal entries (each pixel corresponds mainly to itself but also to all the others) as shown
in 3 part c).
Additionally, randomly sampled style codes simulate a zero correspondence matrix, as shown in 3 part e),
and enable our model to generalize to the cases where no style information (other than the random style
vector) is available. This intuition is linked to the previous works on VAE Kingma & Welling (2013) and
utilized for image translation in Liu et al. (2017); Huang et al. (2018).
Finally, these style components and analytical correspondence matrices enable our model to generalize to the
HWxHWcross-correspondencematricesofCLIPRadfordetal.(2021), withoutneedingtouseCLIPRadford
et al. (2021) during training.
7Published in Transactions on Machine Learning Research (04/2024)
a)Axxsimulated by Sglbb)Axxsimulated by Sdensec)Axxsimulated by Smix
used for Smixused for Smixand Eq. 1 used for Eqs. 6, 7
d)Axysimulated by Sglbe)Axysimulated by Srandf)Axycreated by CLIP
used for Eqs. 4, 5 used for Eqs. 2, 3 used during test time
Figure 3: Style components and correspondence matrices. Example for the simulated and created
correspondence matrices Axx,Axy∈[0,1]3×3. Top row includes the self-correspondence Axxbetween three
pixels from an image in the purple domain, whereas the bottom row displays cross-domain correspondence
Axybetween an image from the purple domain and another image from the yellow domain. Using a)-e)
during training enables our model to generalize to f) during test time.
Discussion on the adversarial and perceptual losses. Adversarial losses in our framework are mainly
intended to produce translations that have high domain fidelity, whereas the perceptual losses are intended
to preserve the content and semantics.
4 Experiments
4.1 Evaluation Metrics
In this UEI2I work, we have three goals and we evaluate these three goals with different metrics. To
evaluate stylistic accuracy , we propose a novel metric to assess classwise stylistic distance that takes semantic
information into account. To evaluate domain fidelity and how well the translations seem to belong to
the target domain, we report the standard FID Heusel et al. (2017) between the translations and the
targets. Lastly, to evaluate content preservation , we report segmentation accuracy with a segmentation
model, DRN Yu et al. (2017), trained on the target domain and tested on the translations.
4.2 Classwise Stylistic Distance
Our local style metric, Classwise Stylistic Distance (CSD), computes the stylistic distance between the
corresponding semantic classes in two images. We use VGG until its first pooling layer, denoted as ˆV, to
extract features of size RV×HWfrom the input image x, exemplar y, and translation x→y. Our metric
uses binary segmentation masks Mx∈RK×HWto compute the style similarity across corresponding classes.
Using the mask for class k,Mk
x∈R1×HW, we compute the Gram matrix Qk
xof the VGG features for class
8Published in Transactions on Machine Learning Research (04/2024) Baseline Ours Exemplar
Figure 4: Effect of the exemplar. Our method can change the appearance of each semantic region
differently, yet has realistic output. The colors of the road and car in the translations match the exemplar
road and car styles better than the baseline (MUNIT) Huang et al. (2018) does. Content image can be seen
in Figure 2
kin image xas
Qk
x=1/summationtext
lMk,l
x(ˆV(x)⊙Mk
x)(ˆV(x)⊙Mk
x)T. (14)
This operation is equivalent to treating each class as a separate image and computing their Gram matrices.
We then compute the distance between the Gram matrices of corresponding classes in two images, i.e.,
L(x,y,k) =∥Qk
x−Qk
y∥2
F. (15)
Note that L(x,y,k)denotes the Maximum Mean Discrepancy (MMD) between the features of the masked
regions with a degree 2 polynomial kernel Li et al. (2017a). Since this distance is computed based on VGG
features from an early layer, it implies a stylistic distance between the two images Gatys et al. (2016).
However, L(x,y,k)is not very informative as its scale is arbitrary and depends on the stylistic distance
between the input image pair x,y. Hence, we propose a metric that takes x,y, and x→yat the same time
for better interpretability. We express Classwise Stylistic Distance (CSD) as
H(x,y,x→y,k) =L(x→y,y,k)
L(x,y,k)1{˜Mkx>0} 1{˜Mky>0}, (16)
where 1{}is the indicator function and ˜Mk:=/summationtext
lMk,l.
Unlike L,His more interpretable because its value would be equal to one if the translation outputs the
content image. In an ideal translation scenario, we would expect the feature distributions of the translation
and exemplar to be close to each other Kolkin et al. (2019). Hence, we expect small values for more successful
translations.
Note that Zhang et al. (2020) also proposes a metric to assess classwise stylistic similarity. Instead of the L2
distancebetweentheclasswiseGrammatrices,itcomputesthecosinedistancebetweentheaveragefeaturesof
corresponding regions in the exemplars and the translations. However, exemplar guided translation involves
9Published in Transactions on Machine Learning Research (04/2024)
three images; the source, the exemplar, and the translation. We believe that evaluating UEI2I should take
all three images into account because stylistic distance between the source and exemplar affects the stylistic
distance between the translation and exemplar, i.e. they are positively correlated. Our metric normalizes
the stylistic distance between the exemplar and translation by the stylistic distance between the source and
exemplar. By doing so, we obtain an interpretable value that shows which portion of the stylistic gap is
closed for each translation, regardless of the initial style gap.
4.3 Implementation Details
We evaluate our method on real-to-synthetic and synthetic-to-real translations using the GTA Richter et al.
(2016), Cityscapes Cordts et al. (2016), and KITTI Geiger et al. (2012) datasets. We use the code published
by the baseline works Huang et al. (2018); Jeong et al. (2021); Lee et al. (2018); Park et al. (2020); Zheng
et al. (2021). Images are resized to have a short side of 256. We borrow the hyperparameters from Huang
et al. (2018) but we scale the adversarial losses by half since our method receives gradients from three
adversarial losses for one source image. We do not change the hyperparameters for the perceptual losses.
The entropy regularization term in Sinkhorn’s algorithm in Eq. 12 is set to 0.05. During training, we crop
the center 224x224 pixels of the images. During test time, we report single scale evaluation with the same
resolution for all the metrics. We use a pre-trained DRNYu et al. (2017) to report the segmentation results.
We also evaluate our method on real-to-real translation using the sunny and night splits of the INIT Shen
et al. (2019) dataset. We use the same setup as previous works and the results of the baselines are taken
from the respective papers.
4.4 Results
GTA→CS car skyvege-
tationbuil-
dingside-
walkroad Avg
MUNIT Huang et al. (2018) 0.43 0.78 0.21 0.28 0.13 0.06 0.32
DRIT Lee et al. (2018) 0.41 1.21 0.27 0.27 0.12 0.08 0.39
CUT Park et al. (2020) 0.44 0.92 0.24 0.36 0.16 0.13 0.38
FSeSimZheng et al. (2021) 0.40 0.96 0.25 0.38 0.15 0.13 0.38
MGUIT Jeong et al. (2021) 0.45 1.42 0.29 0.39 0.18 0.19 0.49
DSI2I 0.29 0.22 0.16 0.26 0.08 0.03 0.17KITTI→GTA car skyvege-
tationbuil-
dingside-
walkroad Avg
MUNIT Huang et al. (2018) 0.46 0.17 0.59 0.39 0.64 0.53 0.46
DRIT Lee et al. (2018) 0.52 0.22 0.61 0.44 0.85 0.55 0.53
CUT Park et al. (2020) 0.53 0.21 0.63 0.47 0.87 0.76 0.57
FSeSimZheng et al. (2021) 0.50 0.25 0.76 0.49 0.88 0.83 0.61
MGUIT Jeong et al. (2021) 0.40 0.21 0.74 0.47 0.84 0.55 0.53
DSI2I 0.29 0.08 0.42 0.34 0.59 0.23 0.32
Table 2:Stylistic Accuracy. Classwise Stylistic Distance between translation-exemplar pairs. Our trans-
lations match the classwise style of the exemplars better (lower is better).
Stylistic Accuracy. Firstly, we evaluate the stylistic distance between the exemplars and the translations
usingourmetricCSD.WereportthismetricforthemostfrequentsixclassesofGTARichteretal.(2016)and
Cityscapes Cordts et al. (2016). The trend with other classes is similar and can be seen in our supplementary
material. As shown in Table 2, our method outperforms the baselines in the synthetic-to-real and real-to-
syntheticscenarios. Notethatinthesyntheticdomains, stylisticdiversityisoverallhigherbecausetheimages
are more saturated. The results for translations in the opposite directions can be seen in our supplementary
material. Our dense style and semantic correspondence modules bring style of corresponding classes closer
to each other.
GTA→CS KITTI →GTA
Method FID ↓Seg Acc↑FID↓Seg Acc↑
MUNIT Huang et al. (2018) 47.76 0.79 53.48 0.73
DRIT Lee et al. (2018) 42.93 0.70 52.12 0.62
CUT Park et al. (2020) 49.82 0.65 62.30 0.59
FSeSim Zheng et al. (2021) 48.77 0.71 63.04 0.60
MGUIT Jeong et al. (2021) 44.36 0.65 57.00 0.57
DSI2I 42.61 0.82 48.30 0.75
Table 3: Content preservation and domain fi-
delity.Our method generates translations with high
fidelity and preserves the content.sunny→night night→sunny
Method CIS ↑IS↑CIS↑IS↑
MUNIT Huang et al. (2018) 1.159 1.278 1.036 1.051
DRIT Lee et al. (2018) 1.058 1.224 1.024 1.099
INIT Shen et al. (2019) 1.060 1.118 1.045 1.080
DUNIT Bhattacharjee et al. (2020) 1.166 1.259 1.083 1.108
MGUIT Jeong et al. (2021) 1.176 1.271 1.115 1.130
DSI2I 1.204 1.283 1.138 1.149
Table 4:Diversity. The translations produced by
our method have higher diversity than those of the
baselines.
10Published in Transactions on Machine Learning Research (04/2024) Source Exemplar Ours MUNIT DRIT CUT LSeSim MGUIT
Figure 5: Qualitative comparison with other methods. CS→GTA translations. In the first column,
our method disentangles the road from the sky and preserves the dark color for the road. In the second
column, the appearance of the road and roadlines in our translation are closest to those in the exemplar. In
the last two columns, our model preserves the semantics better, especially for tree and building classes.
Domain Fidelity. We then evaluate the domain fidelity of the translations using FID Heusel et al. (2017) in
Table 3. Our method generates translations with high fidelity in the synthetic-to-real and real-to-synthetic
scenarios, which pose large domain gaps.
Content preservation. We also evaluate how well our model preserves the content via segmentation
accuracy in Table 3. Our method preserves content better than other I2I methods.
Diversity. Although our main goal is not diversity but stylistic accuracy, having a finer-grained dense
style representation brings about diversity as a by product. We evaluate the diversity and quality of our
11Published in Transactions on Machine Learning Research (04/2024)
translations using the IS Salimans et al. (2016) and CIS Huang et al. (2018) metrics in real-to-real translation
in Table 4. Our results are better than those reported in the baseline papers. Even though the baselines
Bhattacharjee et al. (2020); Jeong et al. (2021); Shen et al. (2019) use object detection labels during training
to guide style, we outperform them without using labels. Note that we do not use semantic correspondences,
i.e., CLIP Radford et al. (2021), during training either. Hence, the performance increase is not due to
dense semantic correspondences or the use of CLIP Radford et al. (2021) during training. Our dense style
representation leads to greater stylistic control and diversity.
Method Test time label FID ↓Styl. Dist.↓
CoCosNetv2 GT Label 46.32 0.34
CoCosNetv2 Pred Label 51.32 0.37
DSI2I No Label 45.12 0.32
Table 5:Quantitative Comparison with CoCos-
Netv2 Zhou et al. (2021). Our method (without
train-test time labels) outperform CoCosNetv2 (with
train-test time labels). When the ground truth labels
are replaced with the predicted labels (%95 accurate)
CoCosNetv2 performance drops drastically.Method FID ↓Styl. Dist.↓
CoCosNetv2 Zhou et al. (2021) 51.32 0.37
MCLNet Zhan et al. (2022b) 50.42 0.38
MATEBIT Jiang et al. (2023) 49.25 0.36
DSI2I 45.12 0.32
Table 6:Quantitative Comparison with Seman-
tic Image Synthesis Methods Our method out-
performs the image synthesis baselines that use pre-
dicted labels.
Comparison to exemplar guided semantic image synthesis. Several works use semantic correspon-
dence in I2I Zhan et al. (2021; 2022b); Zhang et al. (2020); Zhou et al. (2021); Zhan et al. (2022a); Jiang
et al. (2023) to synthesize an image based on a given exemplar. As mentioned in Table 1, our method differs
from this line of research in terms of training resources in three ways; 1) we do not require any semantic
labels during training (Label Free), 2) our image translation task is not guided by ground truth translations
during training (Unpaired), and additionally, 3) our method does not rely on highly similar exemplar-target
pairs within the same domain.
To demonstrate the effectiveness of the unsupervised aspect of our method, we provide comparisons with the
exemplar based image synthesis works. To that end, we train CoCosNetv2 Zhou et al. (2021) on the GTA
dataset using the GTA labels. We test them with GTA images as the exemplars by giving 1) ground-truth
labels of a CS image, 2) segmentation predictions of a CS image (%95 accurate) as inputs. Our method
outperforms CoCosnetv2 Zhou et al. (2021) that use labels both during training and test time. Our method
also outperforms more recent works Zhan et al. (2022b); Jiang et al. (2023) even though we do not use any
labels or pretrained segmentation models neither during training nor during test time as seen in Table 6.
User study. We conduct a user study on Amazon Mechanical Turk and ask the users which translation is
closer to the exemplar in terms of classwise style, color and appearance. We show the users one target image,
and translations (CS →GTA) from the six methods in Fig. 5. Out of 3003 votes, our method received the
most votes (1062), see Table 7. MUNIT Huang et al. (2018) is the second best model with 860 votes. Our
method brings the style of semantically relevant regions closer to each other and is preferred by humans.
4.5 Ablation Study
Our ablations in Table 8 show that the losses on SmixandSglbencourage our model to preserve content
and generate high-quality translations. The effects of adversarial and perceptual losses are shown in Table 9.
Additional analysis on the model components can be found in the Appendix in Tables 12 13 14.
Tables 8 and 9 include ablations on GTA -> CS for our losses and style components (w/o Sglbis equivalent
to w/oLadv_glb,Lperc_glb; w/o Smixis equivalent to w/o Ladv_mix,Lperc_mix). The adversarial losses are
mainly helpful for domain fidelity (Table 9, FID column). The perceptual losses are mainly beneficial for
content preservation (Table 9, Seg. Acc. column). SmixandSglbprovide analytical noisy correspondences
during training time and lead to better FID and Seg. Acc. in Table 8 during test time, when CLIP
correspondences are used with OT. Altogether, our ablations in Tables 9 8 12 13 14 show that the adversarial
12Published in Transactions on Machine Learning Research (04/2024)
Source Ours CoCosNetv2 Exemplar
Figure 6: Qualitative results from Table 5. CS→GTA. CoCosNetv2 fails when Source and Exemplar
images are from different domains and have uncommon classes. The human in the 2nd row, the car in the
1st and 3rd rows and the buildings in all rows are preserved better with our method. Our translations are
more realistic and better represent the source content.
Method DSI2I MUNIT DRIT CUT FSeSim MGUIT
Ratio 35% 28% 18% 5% 7% 5%
Table 7: User study on similarity of translations with
exemplars.
and perceptual losses on SglbandSmixare useful in terms domain fidelity (FID), content preservation (Seg.
Acc.), and stylistic accuracy (Styl. Dis.).
5 Limitations
The main advantage of our method compared to the baselines is the dense modeling of style. Hence, our
method loses its advantage for simple scenes with fewer objects where dense style is not necessary.
13Published in Transactions on Machine Learning Research (04/2024)
GTA→CS FID ↓Seg Acc↑
DSI2I 42.61 0.82
DSI2I w/o Sglb43.52 0.80
DSI2I w/o Smix45.64 0.78
DSI2I w/o Smix,Sglb50.63 0.72
Table 8: Ablation study on SmixandSglb. Our
method benefits from both.GTA→CS FID ↓Seg Acc↑
DSI2I 42.61 0.82
DSI2I w/o Ladv∗ 48.30 0.82
DSI2I w/o Lperc∗ 42.96 0.73
DSI2I w/o Ladv∗,Lperc∗50.63 0.72
Table 9: Ablation study on adversarial and perceptual
losses with SmixandSglb. Adversarial loss encourages
domain fidelity whereas perceptual loss helps preserve
the content.
6 Conclusion
We present a framework for UEI2I that densely represents style and show how such a dense style repre-
sentation can be learned and exchanged across images. This formalism allows local stylistic changes across
semantic regions, while not requiring any labels. We demonstrate the effectiveness of our dense style repre-
sentation in the synthetic-to-real, real-to-synthetic and real-to-real scenarios by showing that our translations
match the style of the exemplar better, are more diverse, better preserve the content, and have high fidelity.
Acknowledgements. This work was supported by the Swiss National Science Foundation via the Sinergia
grant CRSII5-180359. We also thank Ehsan Pajouheshgar for valuable discussions and contributions.
14Published in Transactions on Machine Learning Research (04/2024)
References
Kfir Aberman, Jing Liao, Mingyi Shi, Dani Lischinski, Baoquan Chen, and Daniel Cohen-Or. Neural best-
buddies: Sparse cross-domain correspondence. ACM Transactions on Graphics (TOG) , 37(4):1–14, 2018.
Connelly Barnes, Eli Shechtman, Adam Finkelstein, and Dan B Goldman. Patchmatch: A randomized
correspondence algorithm for structural image editing. ACM Trans. Graph. , 28(3):24, 2009.
Deblina Bhattacharjee, Seungryong Kim, Guillaume Vizier, and Mathieu Salzmann. Dunit: Detection-based
unsupervised image-to-image translation. In Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition , pp. 4787–4796, 2020.
Xinyuan Chen, Chang Xu, Xiaokang Yang, and Dacheng Tao. Attention-gan for object transfiguration in
wild images. In Proceedings of the European conference on computer vision (ECCV) , pp. 164–180, 2018.
Tai-Yin Chiu and Danna Gurari. Photowct2: Compact autoencoder for photorealistic style transfer resulting
from blockwise training and skip connections of high-frequency residuals. In Proceedings of the IEEE/CVF
Winter Conference on Applications of Computer Vision , pp. 2868–2877, 2022.
Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo Benenson,
Uwe Franke, Stefan Roth, and Bernt Schiele. The cityscapes dataset for semantic urban scene understand-
ing. InProc. of the IEEE Conference on Computer Vision and Pattern Recognition , 2016.
Marco Cuturi. Sinkhorn distances: Lightspeed computation of optimal transport. Advances in neural
information processing systems , 26, 2013.
Leon A Gatys, Alexander S Ecker, and Matthias Bethge. Image style transfer using convolutional neural
networks. In Proceedings of the IEEE conference on Computer Vision and Pattern Recognition , pp. 2414–
2423, 2016.
Andreas Geiger, Philip Lenz, and Raquel Urtasun. Are we ready for autonomous driving? the kitti vision
benchmark suite. In Conference on Computer Vision and Pattern Recognition , 2012.
Ian J Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron
Courville, and Yoshua Bengio. Generative adversarial networks. arXiv preprint arXiv:1406.2661 , 2014.
Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans
trained by a two time-scale update rule converge to a local nash equilibrium. Advances in neural infor-
mation processing systems , 30, 2017.
Judy Hoffman, Eric Tzeng, Taesung Park, Jun-Yan Zhu, Phillip Isola, Kate Saenko, Alexei Efros, and Trevor
Darrell. Cycada: Cycle-consistent adversarial domain adaptation. In International Conference on Machine
Learning , pp. 1989–1998. PMLR, 2018.
Xueqi Hu, Xinyue Zhou, Qiusheng Huang, Zhengyi Shi, Li Sun, and Qingli Li. Qs-attn: Query-selected
attention for contrastive learning in i2i translation. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition , pp. 18291–18300, 2022.
Xun Huang and Serge Belongie. Arbitrary style transfer in real-time with adaptive instance normalization.
InProceedings of the IEEE International Conference on Computer Vision , pp. 1501–1510, 2017.
Xun Huang, Ming-Yu Liu, Serge Belongie, and Jan Kautz. Multimodal unsupervised image-to-image trans-
lation. In Proceedings of the European Conference on Computer Vision , pp. 172–189, 2018.
Somi Jeong, Youngjung Kim, Eungbean Lee, and Kwanghoon Sohn. Memory-guided unsupervised image-
to-image translation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pp. 6558–6567, 2021.
Chang Jiang, Fei Gao, Biao Ma, Yuhao Lin, Nannan Wang, and Gang Xu. Masked and adaptive transformer
for exemplar based image translation. In Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition , pp. 22418–22427, 2023.
15Published in Transactions on Machine Learning Research (04/2024)
Justin Johnson, Alexandre Alahi, and Li Fei-Fei. Perceptual losses for real-time style transfer and super-
resolution. In Proceedings of the European Conference on Computer Vision , pp. 694–711. Springer, 2016.
Chanyong Jung, Gihyun Kwon, and Jong Chul Ye. Exploring patch-wise semantic relation for contrastive
learning in image-to-image translation tasks. In Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition , pp. 18260–18269, 2022.
Soohyun Kim, Jongbeom Baek, Jihye Park, Gyeongnyeon Kim, and Seungryong Kim. Instaformer: Instance-
aware image-to-image translation with transformer. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition , pp. 18321–18331, 2022.
Sunnie SY Kim, Nicholas Kolkin, Jason Salavon, and Gregory Shakhnarovich. Deformable style transfer. In
Proceedings of the European Conference on Computer Vision , pp. 246–261. Springer, 2020.
Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114 ,
2013.
Nicholas Kolkin, Jason Salavon, and Gregory Shakhnarovich. Style transfer by relaxed optimal transport and
self-similarity. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition ,
pp. 10051–10060, 2019.
Hsin-Ying Lee, Hung-Yu Tseng, Jia-Bin Huang, Maneesh Singh, and Ming-Hsuan Yang. Diverse image-
to-image translation via disentangled representations. In Proceedings of the European Conference on
Computer Vision , pp. 35–51, 2018.
Boyi Li, Felix Wu, Kilian Q Weinberger, and Serge Belongie. Positional normalization. In Advances in
Neural Information Processing Systems , pp. 1620–1632, 2019.
Yanghao Li, Naiyan Wang, Jiaying Liu, and Xiaodi Hou. Demystifying neural style transfer. arXiv preprint
arXiv:1701.01036 , 2017a.
Yijun Li, Chen Fang, Jimei Yang, Zhaowen Wang, Xin Lu, and Ming-Hsuan Yang. Universal style transfer
via feature transforms. Advances in Neural Information Processing Systems , 30, 2017b.
Yijun Li, Ming-Yu Liu, Xueting Li, Ming-Hsuan Yang, and Jan Kautz. A closed-form solution to photore-
alistic image stylization. In Proceedings of the European Conference on Computer Vision , pp. 453–468,
2018.
Ce Liu, Jenny Yuen, and Antonio Torralba. Sift flow: Dense correspondence across scenes and its applica-
tions.IEEE transactions on pattern analysis and machine intelligence , 33(5):978–994, 2010.
Ming-Yu Liu, Thomas Breuel, and Jan Kautz. Unsupervised image-to-image translation networks. Advances
in neural information processing systems , 30, 2017.
Xiao-Chang Liu, Yong-Liang Yang, and Peter Hall. Learning to warp for style transfer. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pp. 3702–3711, 2021.
Yanbin Liu, Linchao Zhu, Makoto Yamada, and Yi Yang. Semantic correspondence as an optimal transport
problem. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pp.
4463–4472, 2020.
Juhong Min, Jongmin Lee, Jean Ponce, and Minsu Cho. Hyperpixel flow: Semantic correspondence with
multi-layer neural features. In Proceedings of the IEEE/CVF International Conference on Computer
Vision, pp. 3395–3404, 2019.
Sangwoo Mo, Minsu Cho, and Jinwoo Shin. Instagan: Instance-aware image-to-image translation. arXiv
preprint arXiv:1812.10889 , 2018.
Taesung Park, Ming-Yu Liu, Ting-Chun Wang, and Jun-Yan Zhu. Semantic image synthesis with spatially-
adaptive normalization. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recog-
nition, 2019.
16Published in Transactions on Machine Learning Research (04/2024)
Taesung Park, Alexei A Efros, Richard Zhang, and Jun-Yan Zhu. Contrastive learning for unpaired image-to-
image translation. In Proceedings of the European Conference on Computer Vision , pp. 319–345. Springer,
2020.
Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish
Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from
natural language supervision. In International Conference on Machine Learning , pp. 8748–8763. PMLR,
2021.
Stephan R. Richter, Vibhav Vineet, Stefan Roth, and Vladlen Koltun. Playing for data: Ground truth from
computer games. In Bastian Leibe, Jiri Matas, Nicu Sebe, and Max Welling (eds.), Proceedings of the
European Conference on Computer Vision , volume 9906 of LNCS, pp. 102–118. Springer International
Publishing, 2016.
Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved
techniques for training gans. Advances in neural information processing systems , 29, 2016.
Zhiqiang Shen, Mingyang Huang, Jianping Shi, Xiangyang Xue, and Thomas S Huang. Towards instance-
level image-to-image translation. In Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition , pp. 3683–3692, 2019.
Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition.
arXiv preprint arXiv:1409.1556 , 2014.
Xinlong Wang, Rufeng Zhang, Chunhua Shen, Tao Kong, and Lei Li. Dense contrastive learning for self-
supervised visual pre-training. In Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition , pp. 3024–3033, 2021.
JinchaoYang, FeiGuo, ShuoChen, JunLi, andJianYang. Industrialstyletransferwithlarge-scalegeometric
warping and content preservation. In Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition , pp. 7834–7843, 2022.
Jaejun Yoo, Youngjung Uh, Sanghyuk Chun, Byeongkyu Kang, and Jung-Woo Ha. Photorealistic style
transfer via wavelet transforms. In Proceedings of the IEEE/CVF International Conference on Computer
Vision, pp. 9036–9045, 2019.
Fisher Yu, Vladlen Koltun, and Thomas Funkhouser. Dilated residual networks. In Proceedings of the IEEE
conference on Computer Vision and Pattern Recognition , pp. 472–480, 2017.
Fangneng Zhan, Yingchen Yu, Kaiwen Cui, Gongjie Zhang, Shijian Lu, Jianxiong Pan, Changgong Zhang,
Feiying Ma, Xuansong Xie, and Chunyan Miao. Unbalanced feature transport for exemplar-based image
translation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition ,
pp. 15028–15038, 2021.
Fangneng Zhan, Yingchen Yu, Rongliang Wu, Jiahui Zhang, Kaiwen Cui, Aoran Xiao, Shijian Lu, and
Chunyan Miao. Bi-level feature alignment for versatile image translation and manipulation. In Computer
Vision–ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23–27, 2022, Proceedings, Part
XVI, pp. 224–241. Springer, 2022a.
Fangneng Zhan, Yingchen Yu, Rongliang Wu, Jiahui Zhang, Shijian Lu, and Changgong Zhang. Marginal
contrastive correspondence for guided image generation. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition , pp. 10663–10672, 2022b.
Pan Zhang, Bo Zhang, Dong Chen, Lu Yuan, and Fang Wen. Cross-domain correspondence learning for
exemplar-based image translation. In Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition , pp. 5143–5153, 2020.
Yulun Zhang, Chen Fang, Yilin Wang, Zhaowen Wang, Zhe Lin, Yun Fu, and Jimei Yang. Multimodal style
transfer via graph cuts. In Proceedings of the IEEE/CVF International Conference on Computer Vision ,
pp. 5943–5951, 2019.
17Published in Transactions on Machine Learning Research (04/2024)
Chuanxia Zheng, Tat-Jen Cham, and Jianfei Cai. The spatially-correlative loss for various image translation
tasks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pp.
16407–16417, 2021.
Xingran Zhou, Bo Zhang, Ting Zhang, Pan Zhang, Jianmin Bao, Dong Chen, Zhongfei Zhang, and Fang
Wen. Cocosnet v2: Full-resolution correspondence learning for image translation. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition , pp. 11465–11475, 2021.
Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A Efros. Unpaired image-to-image translation using
cycle-consistent adversarial networks. In Proceedings of the IEEE International Conference on Computer
Vision, pp. 2223–2232, 2017.
Peihao Zhu, Rameen Abdal, Yipeng Qin, and Peter Wonka. Sean: Image synthesis with semantic region-
adaptive normalization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pp. 5104–5113, 2020.
A Results on Unlabeled Datasets
We evaluated our method on datasets which have ground truth segmentation labels. The reason behind
this is that, even though our method is applicable to datasets without labels, the metrics we care about
(Seg. Acc. and Styl. Dist.) rely on ground-truth segmentation labels. KITTI, GTA and Cityscapes
satisfy this label requirement for quantitative evaluation. Furthermore, some of the baselines, CoCosNetv2,
MCLNet, MATEBIT and MGUIT, strictly rely on semantic segmentation labels for training, which makes
them inapplicable to unlabeled datasets.
Our method, however, is applicable to scenes without semantic labels. Hence, as requested by the reviewers,
we provide results on the summer2winter and monet2photo datasets Zhu et al. (2017), in both directions.
Our qualitative results show that dense modeling of style enables more accurate transportation of style
between semantically relevant regions.
Also, we would like to mention that, to our knowledge, our method is the first GAN-based I2I method to
model style densely and exchange it accurately across semantic regions, in unlabeled datasets.
B Limitations
Our method is effective at preserving the content for a semantically distinct image pair from two semantically
related datasets. The second row of Figure 5 is a good example, where our method preserves the content and
styles of the pedestrian, bike and rider classes even though there are no such classes in the exemplar. Another
example is provided in monet2photo in Figure 6, where the yellow leaf stylizes the vegetation in the ground
(a semantically relevant but distinct class) but the other semantic classes are less affected and retain their
style. However, our method is not effective for translation between semantically distinct dataset pairs. For
example, in horse2zebra dataset, where the image translation requires semantic changes, our method often
fails to add the stripes to the horses animals. In Figure 8, we show a cherry-picked result in the first row
and another example that reflects the general performance of our method in the second row. The limitation
might be partly due to the perceptual loss with VGG, which is too conservative for the horze2zebra task.
Inourwork, theattributestobeswappedarematchedviaCLIP-basedcorrespondencewhereastheattributes
to be preserved are constrained via VGG-based perceptual loss. The choice of the VGG backbone reflects
what kind of content we aim to preserve whereas the choice of the CLIP backbones reflects among which
regions we aim to exchange the dense style. Hence, for the applications whose style/correspondence/content
definitions differ from ours, a possible solution could be to experiment with other backbones instead of CLIP
and VGG. An example for horse2zebra dataset would be to use a background focused perceptual loss instead
of VGG-based perceptual loss and an animal-part focused correspondence backbone instead of the CLIP
based correspondence. One reference for such a solution could be AttentionGAN Chen et al. (2018).
18Published in Transactions on Machine Learning Research (04/2024)
Figure 7: Applicability of our method. DSI2I is effective in challenging datasets that do not contain
any semantic labels. Our method is the first to model the style densely in these datasets. In the first, third,
fourth, fifth and sixth rows, sky in our translations reflect the exemplar style of sky more accurately. In the
second row, the yellow leaf in the exemplar stylizes the vegetation (grass) whereas sky is less affected by the
style of the yellow leaf.
19Published in Transactions on Machine Learning Research (04/2024)
Figure 8: Limitation on I2I tasks that require semantic changes. On the horze2zebra dataset that
requires semantic changes, our method fails to make the required changes. We present a cherry picked result
on the first row. The quality of the outputs of our method is reflected more accurately in the second row
where the stripes are not added properly.
C Technical Details
In this section, we describe the technical details of the I2I methods that we used in our comparisons. For
each method, we adopt the default hyperparameters of the method. All the models are trained for 800K
iterations with 224x224 images. We use linear learning rate decay after 400K iterations as suggested in these
works.
For training, we resize the input images to have the shorter side of size 256 without changing the aspect
ratio and then crop a random 224x224 region. At test time, we generate translations without cropping the
images. We use the evaluation code of FSeSim Zheng et al. (2021) for computing the FID. We resize the
images to have a shorter side of size 299 without changing the aspect ratio when computing the FID. We
borrow the code for IS/CIS from MUNIT Huang et al. (2018). We report the exponential of IS and CIS as
done in Huang et al. (2018). The images are resized to have the shorter side of size 299 followed by taking
a center crop of size 299x299 as done in Huang et al. (2018). In FID, IS and CIS computations, we sample
100 random source images and 19 target images for each source image. We generate 1900 exemplar based
translations as done in Huang et al. (2018).
We use two pre-trained DRN models Yu et al. (2017) for segmentation. We use the pre-trained models for
GTA and CS from Hoffman et al. (2018) and Yu et al. (2017), respectively. The former is a DRN-C 26 model
whereas the latter is a DRN-D 22.
D Results
We provide additional results in this section on CS →GTA. Our method outperforms the baselines in CS
→GTA.
20Published in Transactions on Machine Learning Research (04/2024)
CS→GTA car skyvege-
tationbuil-
dingside-
walkroad Avg
MUNIT Huang et al. (2018) 0.57 0.44 0.59 0.53 0.470.35 0.49
DRIT Lee et al. (2018) 0.66 0.63 0.53 0.56 0.51 0.39 0.55
CUT Park et al. (2020) 0.88 0.58 0.75 0.67 0.72 0.74 0.72
FSeSimZheng et al. (2021) 0.77 0.68 0.75 0.75 0.70 0.63 0.71
MGUIT Jeong et al. (2021) 0.76 0.69 0.69 0.65 0.68 0.57 0.67
DSI2I 0.35 0.17 0.44 0.41 0.500.29 0.36
Table 10: Classwise stylistic distance
CS→GTA
FID↓ Seg Acc↑
MUNIT Huang et al. (2018) 48.91 0.79
DRIT Lee et al. (2018) 48.18 0.72
CUT Park et al. (2020) 65.68 0.61
FSeSim Zheng et al. (2021) 64.81 0.74
MGUIT Jeong et al. (2021) 55.72 0.68
DSI2I 45.12 0.81
Table 11: Fidelity and diversity of the translations. Our method outperforms all others on all metrics.
E Semantic Correspondence
E.1 Marginal Distributions in Optimal Transport
As mentioned in line 497 in the main paper, we discuss a better choice for the marginal distributions for
Sinkhorn’s Algorithm Cuturi (2013). The most straightforward choice for transportation masses pxandpy
is the uniform distribution. However, doing so transports equal mass from every location in the images.
This is problematic for us because we can see in Fig. 1 that when translation pairs have unbalanced classes,
the largest semantic region can dominate the style representation and lead to undesired artifacts. In our
example in Fig. 1, the content image expects to receive style vectors for roads, buildings, and tree but the
exemplar image provides style for sky and road. This results in building and tree regions being stylized by
sky attributes.
To solve the unbalanced class problem, we first assume that segmentation labels MxMy∈{0,1}K×HWfor
Kclasses are available. We define Mkas the binary mask for the k-th class. The number of pixels in class k
is defined as ˜Mk:=/summationtext
lMk,lwherelindexes the spatial dimension. We, then, define ˆMyy,ˆMyx∈RK×HW
as
ˆMyy=MT
y˜MyandˆMyx=MT
y˜Mx (17)
where ˜M∈RK×1is the concatenation of ˜Mk. We propose dividing the transportation mass pyof each
semantic region in yby the area of that semantic region to normalize the style based on class distribution
ofy. We also multiply the mass of each semantic region in yby the area of the same semantic class in xto
match the expectations of x. We set pxto be the uniform distribution and compute
ˆ py=ˆMyx⊘ˆMyy (18)
where⊘denotes Hadamard (element-wise) division. However, we perform correspondence only during test
time and we cannot rely on labels. Hence, we do not know the area of any of the classes. To that end, we
propose estimating ˆMyyandˆMyxbased on features FxandFy. As such, we define Zxxas self-similarity of
xsimilarly to Zyxand estimate ˆMyyandˆMyxwithRyyandRyxrespectively.
Ryy=/summationdisplay
lZl
yyandRyx=/summationdisplay
lZl
yx (19)
21Published in Transactions on Machine Learning Research (04/2024)
Source image Exemplar image Cosine Similarity
Figure 9: Visualization of Cosine Similarity across domains. We choose a region centered at the
red point from the source image in the first column and display the cosine similarity between the chosen
source region with all the other target regions. Our correspondence module is able to relate the object parts
that are not labeled in semantic segmentation annotations which is demonstrated by the correspondence of
roadlines in the second row and wheel in the third row.
where Zl∈RHW×1andlindexes to the second dimension of Z. We then compute ˆ pyas
ˆ py=Ryx
Ryy(20)
which is linearly scaled to obtain a probability distribution ˆ py. Lastly, we compute Ayxas to warp Sdense
y
as
Sy→x=Reshape (Sdense
yAyx). (21)
Corr Acc GTA →CS CS→GTA
Ours 0.59 0.59
Ours w/o py 0.57 0.56
Table 12: Accuracy of semantic correspondence. Our unsupervised pyincreases the accuracy of correspon-
dence.
E.2 Effect of the Backbones
WeuseCLIPRadfordetal.(2021)tobuildsemanticcorrespondencesbetweenthetwoimages. CLIPRadford
et al. (2021) is trained with image-caption pairs from the internet, to match the global representation of the
image with the language representation of the corresponding caption. Hence, it has never received pixel-level
supervision or segmentation masks.
22Published in Transactions on Machine Learning Research (04/2024)
We also experiment with a pre-trained DenseCL Wang et al. (2021) model. DenseCL Wang et al. (2021) is
trained in a self supervised way to predict the intersection of two crops from two augmentations of the same
image. We measure the accuracy of correspondence by warping the segmentation labels of the exemplar
viaMyAyxand then dividing the correctly classified pixels by the total number of pixels. We observe that
semantic correspondence with DenseCL Wang et al. (2021) is less accurate, hence we stick to using CLIP.
We use pre-trained weights for the ResNet50 architecture. Specifically, we extract features from the end of
the ’layer1’ and ’layer3’ stages of ResNet50 architecture.
Corr Acc GTA →CS CS→GTA
Ours w/ CLIP Radford et al. (2021) 0.59 0.59
Ours w/ DenseCL Wang et al. (2021) 0.55 0.54
Table 13: Accuracy of semantic correspondence with different backbones. Our method uses CLIP Radford
et al. (2021) unless otherwise mentioned
E.3 Ablation on Semantic Correspondence
The contributions of OT are analyzed in the supplementary material in Table 12 and the last two rows of
Table 14. Table 12 shows that controlling the marginal distributions in OT leads to more accurate semantic
correspondences. The last two rows of Table 14 demonstrate that OT contributes to the performance of our
I2I method. OT encourages one-to-one matches and increases the accuracy of these matches (correspondence
accuracy in Table 12). As a result, OT leads to better transportation of dense style from the exemplar to
the target images (Stylistic distance in Table 14, last two rows), more realistic translations with less artifacts
(FID score in Table 14, last two rows), and better content preservation (Segmentation Accuracy in Table 14,
last two rows). In addition to Table 12, using softmax instead of OT leads to lower correspondence accuracy
in both directions (0.57 -> 0.55 and 0.56 -> 0.53, compared to the last row of Table 12), which supports the
theoretical advantage of OT experimentally.
F Qualitative Results
F.1 Ablation Study
We show the qualitative effect of SmixandSglbin Fig. 10. Without using SmixorSglbwith perceptual
and adversarial losses, the content component encodes less information about the image, which leads to
unrealistic translations with the exemplar style.
LrandLglbLmixOT Styl. Dis.↓FID↓Seg.↑
✗ ✗ ✗ ✓ 0.36 59.72 0.73
✗ ✓ ✓ ✓ 0.38 52.82 0.75
✓ ✓ ✓ ✓ 0.32 45.12 0.81
✓ ✓ ✓ ✗ 0.35 45.72 0.77
Table 14: Ablation studies for loss terms and OT.
F.2 User Study
We conduct a user study on Amazon Mechanical Turk. We use GTA Richter et al. (2016) images that as
the exemplars because they have greater variety in style and appearance. Cityscapes Cordts et al. (2016)
images are used as source images. We removed samples which do not contain road and sky. We tried to
include complex scenes with multiple objects in the exemplar so that it contains style for many classes. We
form 90 source-exemplar pairs. The users are shown one exemplar image and translations from six models,
ours and five other baselines Huang et al. (2018); Jeong et al. (2021); Lee et al. (2018); Park et al. (2020);
Zheng et al. (2021), as shown in Fig. 11. They are asked to choose the image that looks the most similar to
23Published in Transactions on Machine Learning Research (04/2024) Source Exemplar DSI2I w/oSglbw/oSmixw/oSglb,Smix
Figure 10: Effect of SmixandSglb.The adversarial and perceptual losses on SmixandSglbconstrain
the dense style representation and, thus, encourage the preservation of content, semantics, and details of
the source image. As mentioned in the main paper, the labels are used to swap style across classes in our
ablation study instead of the semantic correspondence module. (CS Cordts et al. (2016) to GTA Richter
et al. (2016))
the exemplar image. The question they received was: ’Which image is more similar to the target image (T)?
Similar images would have closer road and sky colors and would reflect the same time of the day.’ We did
not provide the users with the source image S because most users were choosing the translations that were
closest to the content. We randomized the order of the choices (six methods) in our user study. We also
filtered the responses with a mock question in which users are shown the translation of one content image
to the style of six exemplars using MUNIT Huang et al. (2018). Only one of the six exemplars is provided
in the question as the exemplar. We ask users the same question and accept the answers of those who pick
the translation that matches the exemplar displayed in the question. We received answers from 77 users and
each user answered 39 questions (+1 mock question). Out of 3003 answers, 1082 picked our method as the
best, followed by MUNIT Huang et al. (2018) with 860 votes.
F.3 Comparison with Other Methods
We provide qualitative examples for our model in the end of our supplementary material.
24Published in Transactions on Machine Learning Research (04/2024)
Figure 11: Screenshot from our user study The users are asked to pick the translation (from CS Cordts
et al. (2016) to GTA Richter et al. (2016)) that looks the most similar to the exemplar image T. We do not
provide the users with the source image S and we randomized the order of the choices in our user study.
Here, (1): DSI2I, (2): MUNIT Huang et al. (2018), (3): DRIT Lee et al. (2018), (5): CUT Park et al. (2020),
(5): FSeSim Zheng et al. (2021), (6): MGUIT Jeong et al. (2021)
25Published in Transactions on Machine Learning Research (04/2024) Source Exemplar Ours MUNIT DRIT CUT LSeSim MGUIT
Figure 12: Qualitative comparison with other methods. CS→GTA. The road and sky appearance in
all the columns are closer to the exemplar road and sky with our method. In the second column, our method
is more accurate in the appearance of cars. In the second and third columns, the roadlines are yellow in our
translations, which is closer to the exemplar appearance.
26Published in Transactions on Machine Learning Research (04/2024) Source Exemplar Ours MUNIT DRIT CUT LSeSim MGUIT
Figure 13: Qualitative comparison with other methods. CS→GTA. Our method brings sky and road
appearances closer to those of the exemplar in all cases. In the second column, our method preserves the
tree whereas the other methods remove it and display sky.
27Published in Transactions on Machine Learning Research (04/2024) Source Exemplar Ours MUNIT DRIT CUT LSeSim MGUIT
Figure14: Qualitativecomparisonwithothermethods. CS→GTA.Ourmethodchangestheroadlines
based on the exemplar. In the first and second columns, the appearance of the roadlines is adjusted based
on the exemplar whereas the other methods either leave them as white or change them to yellow for all the
exemplars. In columns three and four, we can see that our method preserves the tree better than the other
methods do.
28Published in Transactions on Machine Learning Research (04/2024) Source Exemplar Ours MUNIT DRIT CUT LSeSim MGUIT
Figure 15: Qualitative comparison with other methods. CS→GTA. Our method preserves the
building pixels in the first two columns. In the last two columns tree and sky are better preserved with our
method and reflect closer appearance to the exemplar.
29Published in Transactions on Machine Learning Research (04/2024) Source Exemplar Ours MUNIT DRIT CUT LSeSim MGUIT
Figure 16: Qualitative comparison with other methods. CS→GTA. Our method yields a high output
diversity, yet preserves the trees in the first, second, and third columns. The road has the closest appearance
to the exemplar with our method in the last column.
30Published in Transactions on Machine Learning Research (04/2024) Source Exemplar Ours MUNIT DRIT CUT LSeSim MGUIT
Figure17: Qualitativecomparisonwithothermethods. GTA→CS.Inthisfigure, andinthefollowing
ones, we show translations in the opposite direction, namely from GTA to CS. Even though stylistic diversity
islessintherealimagedomain, theadvantageofourmethodisstillvisible. Ourmethodisbetteratmatching
the road and sky colors. In the second column, our method does not introduce trees instead of sky, which is
common in translations of GTA images.
31Published in Transactions on Machine Learning Research (04/2024) Source Exemplar Ours MUNIT DRIT CUT LSeSim MGUIT
Figure 18: Qualitative comparison with other methods. GTA→CS. Aside from road and sky colors,
our method is better at preserving the sky regions whereas other methods introduce trees.
32Published in Transactions on Machine Learning Research (04/2024) Source Exemplar Ours MUNIT DRIT CUT LSeSim MGUIT
Figure 19: Qualitative comparison with other methods. GTA→CS. In all the columns, sky is flipped
to tree with other methods. Our method is better at preserving the semantics, yet has diverse outputs.
33Published in Transactions on Machine Learning Research (04/2024) Source Exemplar Ours MUNIT DRIT CUT LSeSim MGUIT
Figure 20: Qualitative comparison with other methods. GTA→CS. Our method has much less
artifacts in sky in the first three columns. In the last column, the road has closer appearance to the
exemplar with our method.
34Published in Transactions on Machine Learning Research (04/2024) Source Exemplar Ours MUNIT DRIT CUT LSeSim MGUIT
Figure 21: Qualitative comparison with other methods. KITTI→GTA. The road and sky appearance
in all the columns are closer to the exemplar road and sky with our method. In the second column, red
colors from the truck pollute the style of the other areas with MUNIT.
35