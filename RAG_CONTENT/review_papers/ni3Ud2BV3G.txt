On the Impacts of the Random Initialization in the
Neural Tangent Kernel Theory
Guhan Chen
Department of Statistics and Data Science
Tsinghua University
Beijing, China
chen-gh23@mails.tsinghua.edu.cnYicheng Li
Department of Statistics and Data Science
Tsinghua University
Beijing, China
liyc22@mails.tsinghua.edu.cn
Qian Lin∗
Department of Statistics and Data Science
Tsinghua University
Beijing, China
qianlin@tsinghua.edu.cn
Abstract
This paper aims to discuss the impact of random initialization of neural networks in
the neural tangent kernel (NTK) theory, which is ignored by most recent works in
the NTK theory. It is well known that as the network’s width tends to infinity, the
neural network with random initialization converges to a Gaussian process fGP,
which takes values in L2(X), where Xis the domain of the data. In contrast, to
adopt the traditional theory of kernel regression, most recent works introduced a
special mirrored architecture and a mirrored (random) initialization to ensure the
network’s output is identically zero at initialization. Therefore, it remains a question
whether the conventional setting and mirrored initialization would make wide neural
networks exhibit different generalization capabilities. In this paper, we first show
that the training dynamics of the gradient flow of neural networks with random
initialization converge uniformly to that of the corresponding NTK regression with
random initialization fGP. We then show that P(fGP∈[HNT]s) = 1 for any
s <3
d+1andP(fGP∈[HNT]s) = 0 for any s≥3
d+1, where [HNT]sis the real
interpolation space of the RKHS HNTassociated with the NTK. Consequently,
the generalization error of the wide neural network trained by gradient descent
isΩ(n−3
d+3), and it still suffers from the curse of dimensionality. On one hand,
the result highlights the benefits of mirror initialization. On the other hand, it
implies that NTK theory may not fully explain the superior performance of neural
networks.
1 Introduction
In recent years, the advancement of neural networks has revolutionized various domains, including
computer vision, generative modeling, and others. Notably, large language models like the renowned
GPT series [ 8,51] have shown exceptional proficiency in language-related tasks. Similarly, neural
networks have achieved significant successes in image classification, as evidenced by works such
as [27,34,37]. This proliferation of neural networks spans a wide range of fields. Despite these
∗Corresponding author
38th Conference on Neural Information Processing Systems (NeurIPS 2024).impressive achievements, a comprehensive theoretical understanding of why neural networks perform
so well remains elusive in the academic community.
Several studies have delved into the theoretical properties of neural networks. Initially, researchers
were keen on exploring the expressive capacity of networks, as demonstrated in seminal works like
[17,28]. These studies established the Universal Approximation Theorem, asserting that sufficiently
wide networks can approximate any continuous function. More recent research, such as [ 15,26,43]
extended this exploration to the effects of deeper and wider network architectures. However, a
significant challenge remains in these studies: they often do not fully explain the generalization power
of neural networks, which is crucial for evaluating the performance of a statistical model.
Recently, some researchers have examined the generalization properties of networks. Bauer and
Kohler [5], Schmidt-Hieber [46] showed the minimax optimality of networks with various activation
functions for specific subclasses of Hölder functions, within the nonparametric regression framework.
In contrasts to the static ERM approach, some studies made more attention to the dynamics of neural
networks, particularly those trained using gradient descent (GD) and stochastic gradient descent
(SGD)[2, 13, 20].
With similar insights, Jacot et al. [31] explicitly introduced the Neural Tangent Kernel (NTK) concept,
demonstrating that there exists a time-varying neural network kernel (NNK) which converges to a
fixed deterministic kernel and remains almost invariant during training as network width approaches
infinity. And thus NTK theory proposes that network training can be approximated by a kernel
regression problem [ 4,29,39,50]. As a general case, fully-connected networks directly trained
by GD, Lai et al. [35], Li et al. [41] showed the generalization ability of two-layer and multi-layer
networks, respectively.
This paper mainly follows [ 4,35,41], and explores the impact of initialization in the NTK theory.
Prior research [ 35,41] which verified the minimax optimality of network utilized the so-called
mirrored initialization setting. It refers to a combination of mirrored structure and mirrored initial
value of parameters, which results in a zero initial output function. However, the assumption divates
from the commonly used initialization strategy in real-world applications, whose initial output is
actually non-zero. To bridging the gap, in this study we explore the generalization ability of standard
non-zero initialized network, within the NTK theory framework. Our findings reveal that the vanilla
non-zero initialization will theoretically results in poor generalization ability of network, especially
when the data has relatively large dimension. If that is true, it suggests a divergence between
theoretical models and real-world applications, highlighting a potential limitation in the current
understanding of the NTK theory. Therefore, we arrive at a critical problem central to this study:
Does initialization significantly impact the generalization ability of networks within the kernel
regime?
1.1 Our contribution
•Network converges to a NTK predictor uniformly . We show that under standard initialization, the
network function converges to the NTK predictor uniformly over the entire training process and
over all possible input in the domain. The convergence is essential in the study of the generalization
ability of network in NTK theory. However, in previous work, the initial values of network has
long been overlooked. Under mirrored initialization which leads to zero initial output function,
Arora et al. [4], Lai et al. [35], Li et al. [41] demonstrated the point-wise convergence and the
uniform convergence of network, respectively. More recently, Xu and Zhu [54] studied the uniform
convergence of NTK under standard initialization, but did not study the convergence of the network
function. Why the initial output of the network is ignored is not that it is insignificant, but rather
because it is a stochastic function, making it challenging to analyze in convergence. Our findings
make it valid to approximate the network’s generalization ability based on the corresponding NTK
predictor’s performance.
•The generalization ability of standardly non-zero initialized fully-connected network . Our research
explores the impact of standard non-zero initialization in NTK theory. At this issue, Zhang et al.
[56] proposes the existence of implicit bias induced by non-zero initialization, when the neural
network is completely overfitted. We delve deeper into this argument, studies the exact formula of
the bias at any stage of training, within the framework of NTK theory. Additionally, we established
that the (optimally tuned) learning rate of network is n−3
d+3, even when the regression function is
2sufficiently smooth. This insightful discovery implies a notable limitation in the generalization ability
of networks with non-zero initialization, if NTK theory can precisely approximate the performance
of real network. Consequently, we need to reconsider the weakness of NTK in the study of network
theory. Also, the results show that mirrored initialization is superior to standard initialization in
practical applications.
1.2 Related works
Our research is conducted within the framework of NTK theory. This type of research, in general, can
be categorized into two main steps: the approximation of the network trained by GD through a kernel
regression problem, and the evaluation on the corresponding kernel regression predictor. Several
studies [ 2,4,19,31] which focused on the former step, illustrated the point-wise convergence of
NTK for multi-layer ReLU networks. Additionally, [ 39] demonstrated the point-wise convergence of
the kernel regression predictor to the network. Furthermore, Lai et al. [35], Li et al. [41], Xu and Zhu
[54] demonstrated the uniform convergence result with respect to all input and all time on two-layer
and multi-layer networks. As to the latter step, a few researchers have analyzed the spectral properties
of the NTK [ 6,7] as well as kernel regression [ 40,55]. Building upon these findings, Lai et al. [35]
and Li et al. [41] demonstrated that early-stopping GD induces minimax optimality of the network.
It is worth noting that the setting in these works assumes mirrored initialization, which may not be
well-aligned with real-world scenarios. When it comes to initialization, Zhang et al. [56] provided
insights into the impact of initialization under kernel interpolation, which is a special case of our
results at t=∞.
2 Preliminaries
2.1 Model and notations
Suppose that {(xi, yi)}n
i=1are i.i.d. drawn from an unknown distribution ρwhich is given by
y=f∗(x) +ϵ, (1)
where f∗(x)is the regression function andϵis a centered random noise. Suppose that the marginal
distribution µ(x)of the radom variable xis supported in a non-empty bounded subset XofRdwith
C∞smooth boundary. The generalization error of an estimator ˆfoff∗is given by excess risk
E(ˆf;f∗) =ˆf−f∗2
L2(X,µ). (2)
We introduce the following standard assumption on the noise(e.g., [ 21,42]). It is clear that sub-
Gaussian noise satisfying this assumption.
Assumption 1 (Noise) .The noise term ϵsatisfies the following condition for some positive constant
σ, L, and m≥2:
E(|ϵ|m|x)≤1
2m!σ2Lm−2, a.e. x ∈ X. (3)
Notations Given a set of samples pairs {(xi, yi)}n
i=1, we denote XandYto be vector
(x1,···, xn)Tand(y1,···, yn)T, respectively. In a similar manner, (f(x1),···, f(xn))Tand
(f(y1),···, f(yn))Tare represented as f(X)andf(Y), where f(·) :Rd7→Ris an arbitrary
given function. Regarding a kernel function k(·,·) :Rd×Rd7→R, we use k(x, X)to denote the
vector (k(x, x1), k(x, x2),···, k(x, xn))andk(X, X )to denote the matrix [k(xi, xj)]n×n. For real
number sequences such as {an}and{bn}, we write an=O(bn)(oran=o(bn)), if there exists
absolute positive constant Csuch that |an| ≤C|bn|holds for any sufficiently large n(or|an|/|bn|
approaches zero). We also denote an≍bnif there exists absolute positive constant cabdCsuch that
c|bn| ≤ |an| ≤C|bn|holds for any sufficiently large n.
2.2 Reproducing kernel Hilbert space
Suppose that kis kernel function defined on the domain Xsatisfying that ∥k∥∞≤κ2. LetHkbe the
reproducing Hilbert space associated with kwhich is the closure of linear span of {k(x,·), x∈ X}
3under the inner product induced by ⟨k(x,·), k(y,·)⟩=k(x, y). Given a distribution µ(x)onX, we
can introduce an integral operator Tk:L2(X, µ)→L2(X, µ):
Tkf(x) =Z
Xk(x, y)f(y) dµ(y). (4)
The celebrated Mercer’s decomposition [12] asserts that
Tkf=X
i∈Nλi⟨f, ei⟩L2ei, k(x, y) =X
i∈Nλiei(x)ei(y), (5)
where {ei}i∈Nand{λ1
2
iei}i∈Nare the orthonormal basis of L2(X, µ)andHkrespectively. It is well
known that Hkcan be canonically embedded into L2(X, µ).
If the eigenvalues λiofkare polynomially decaying at rate β( i.e, λi≍i−β), we can further
introduce a concept of the relative smoothness of a function f∈L2(X, µ). More precisely, let us
recall the concept of real interpolation space [48] ( Please see more detailed information in the
Appendix).
Real interpolation space The real interpolation space [Hk]sis given by
[Hk]s:=(X
i∈Naiλs
2
iei(x)X
i∈Na2
i<∞)
, (6)
with the inner product ⟨P
i∈Naiλs
2
iei(x),P
i∈Nbjλs
2
jej(x)⟩[Hk]s=P
i∈Naibifors≥0.
It is clear that [Hk]sis a separable Hilbert space and is isometric to the l2space. With the definition
above, we can see that [Hk]0=L2(X, µ)and[Hk]1=Hk. Also, for any s2≥s1≥0, we know
[Hk]s1⊆[Hk]s2with compact embedding. Let
α0= inf
s
s|[Hk]s⊆C0(X)	
which is often referred to the embedding index of an RKHS Hk[21] . It is well known that α0≥1
β
and the equality holds for a large class of usual RKHSs if the eigenvalue decay rate is β. We further
define the relative smoothness of a given function f:
Definition 2.1 (Relative smoothness) .Given a kernel konXwith respect to measure µ, the
smoothness of a function fis defined as
α(f, k) = sup(
α >0X
i∈Nλ−α
ic2
i<∞)
, (7)
where ci=⟨f, ei⟩L2(X,µ).
2.3 Kernel gradient flow
For a positive definite reproducing kernel k, the dynamic of kernel gradient flow (KGF) [22] is
d
dtfGF
t(x) =−1
nk(x, X) 
fGF
t(X)−Y
, (8)
where fGF
tis the KGF predictor. In kernel gradient flow, the performance of kernel predictor depends
on the relative smoothness of regression function. People often consider the case that α(f, k)≥1
[10,11] . When the smoothness satisfies α(f, k)<1, the regression function is said to be poorly
smooth and belongs to the so-called misspecified spectral algorithm problem. We collect the related
result in Zhang et al. [55] and apply it to our case, to derive the following proposition:
Proposition 2.2. Suppose the eigenvalue decay rate of kisβand the embedding index is1
βwith
respect to µ. Suppose the noise term ϵsatisfies Assumption 1. Let the dynamic (8)starts from
fGF
0= 0. Also, suppose the regression function satisfies f∗∈[Hk]sand∥f∗∥[Hk]s≤R, for some
s >0. Let γ≤sand0≤γ≤1. By choosing t≍nβ
sβ+1, for any fixed δ∈(0,1), when nis
sufficient large, with probability at least 1−δ, we have
fGF
t−f∗2
[Hk]γ≤
ln6
δ2
R2Cn−(s−γ)β
sβ+1,
where Cis a positive constant.
43 Network and Neural Tangent Kernel
3.1 Network settings
We consider the fully-connected network with Lhidden layers. As is commonly-used in deep learning,
we consider the ReLU activation [ 44] defined by σ(x):= max( x,0). Denote f(·;θ) :Rd→Ras
the network output function, where θrepresenting the column vector that all parameters flattened
into. We can write the recursive structure of network as following:
α(1)(x) =r
2
m1
W(0)x+b(0)
;
α(l)(x) =r
2
mlW(l−1)(x)σ(α(l−1)(x)), l= 2,3,···, L;
f(x;θ) =W(L)σ(α(L)(x)),(9)
The parameter matrix for the l-th layer is denoted as W(l). Their dimensions are of ml+1×ml, where
mlis the number of units in layer landml+1is that of layer l+ 1. Also, the bias term of the first
layer is denoted as b(0)∈Rm1×1. The setting of bias term is to make sure the positive definiteness of
NTK [ 41]. We further assume that the number of units in each layer is at the same order while the
width comes to infinity, as cm≤min(m1,···, mL+1)≤max( m1,···, mL+1)≤Cm where c, C
are some absolute positive constants.
Standard initialization At initialization, the parameters are randomly set as i.i.d. standard normal
variables:
W(l)
ij, b(0)
ki.i.d.∼ N (0,1), l= 0,1,···, L;k= 1,···, m1. (10)
Remark 3.1 (Mirrored initialization) .As to the mirrored initialization considered in [ 4,35,41],
part of the network f(1)(·;θ(1)
0)undergoes standard initialization, while the other complicated
corresponding part f(2)(·;θ(2)
0)holds the same structure as f(1)(·;θ(1)
0), with parameters initialized
to the same values as θ(2)
0=θ(1)
0. Lastly, the neural network output function is defined as f(·;θ0) =√
2
2
f(1)(·;θ(1)
0)−f(2)(·;θ(2)
0)
. This setup ensures that f(·;θ0)is constantly zero.
The network is trained under the mean square loss function. If we suppose {(xi, yi)}n
i=1be the
training data, then the loss function is specified as
L(θ) =1
2nnX
i=1(f(xi;θ)−yi)2. (11)
For notational simplicity, we denote by fNN
t(x) =f(x;θt). The training process for the network is
performed by gradient flow, where the parameters are updated through the differential equation:
d
dtθt=−∂θL(θ) =−1
n[∂θfNN
t(X)]T(fNN
t(X)−Y), (12)
where ∂θfNN
t(X)is a matrix with dimensions n×M, with Mbeing the length of the parameter
vector θ. This matrix represents the gradient of the network output fNN
t(X)with respect to the
parameters θat time t. Incorporating the chain rule, we can formulate the gradient flow equation for
the network function as follows:
d
dtfNN
t(x) =−1
n∂θfNN
t(x)[∂θfNN
t(X)]T(fNN
t(X)−Y). (13)
3.2 Network at initialization
In order to state the properties of wide network with standard initialization, we need to introduce the
concept of Gaussian process.
5Gaussian process Gaussian process is a stochastic process for which every finite collection of
random variables follows a multivariate Gaussian distribution. Let Xbe a Gaussian process with
index t∈T. If the mean and covariance are given by the mean function mand the positive definite
kernel ksuch that E[X(t)] =m(t)andCov[X(t)X(t′)] =k(t, t′), which holds for any t, t′∈T,
then we say X∼ GP (m, k).
In standard initialization (10), the parameters of the neural network are i.i.d. samples from a standard
normal distribution. If the network contains only one hidden-layer (that is, if L= 2), it is direct to
prove that fNN
0(x)converges to a centered Gaussian distribution by CLT, for any fixed point x∈ X.
As to the multi-layer network, prior research [ 25] also proved that such initialized network converges
to a Gaussian process, as following:
Lemma 3.2 (Limit distribution of initialization) .As the network width mtend to infinity, the
sequence of network stochastic process {fNN
0}∞
m=1converges weakly in C(X,R)to a centered
Gaussian process fGP. The covariance function is the so-called random feature kernel (RFK), which
is denoted by KRFK(x, x′)as defined in (42) in Appendix C.1.
3.3 The kernel regime
As the gradient descent of neural network involves high non-linearity and non-convexity, it is difficult
to study the training process. However, Jacot et al. [31] introduced the Neural Tangent Kernel
(NTK) theory which provides a connection between network training and a class of kernel regression
problems, when the network width comes to infinity. To demonstrate this, we first define a Neural
Network Kernel (NNK):
Km
t(x, x′) = [∂θft(x)]T[∂θft(x′)]. (14)
Using this notation, we reformulate (13) in a kernel regression format:
d
dtfNN
t(x) =−1
nKm
t(x, X)(fNN
t(X)−Y). (15)
NTK theory shows, if the network width mtends to infinity, then the random kernel Km
t(·,·)will
converge to a time-invariant kernel KNTK(·,·) :Rd×Rd→R, which is referred to as the NTK of
network. The phenomenon is the so-called NTK regime [ 2,31,39]. The fixed kernel KNTKonly
depends on the structure of the neural network and the way of initialization. To get more knowledge
of NTK, we present the explicit expression of NTK in Appendix C.1. In NTK theory, the dynamic of
network (15) can be approximated by a kernel gradient flow equation:
d
dtfNTK
t(x) =−1
nKNTK(x, X)(fNTK
t(X)−Y), (16)
which starts from Gaussian process fNTK
0 =fGP. In this way, if we aims to derive the generalization
property of sufficiently wide network, we can achieve by considering the corresponding kernel
gradient flow predictor. Such approximation is strictly ensured by uniform convergence of fNN
t
andfNTK
t over all x∈ X and all t≥0asm→ ∞ , since we use L2excess risk to evaluate the
generalization ability. Actually, we have the following theorem, whose proof is given in Appendix B.
Proposition 3.3 (Uniform convergence) .Given training sample pairs {(xi, yi)}n
i=1. For any δ∈
(0,1)andε >0, when network width mis large enough, we have
sup
x,x′∈Xsup
t≥0|fNN
t(x)−fNTK
t(x)| ≤ε
holds with probability at least 1−δ.
In this theorem, we show the uniform convergence of network under standard initialization. Previous
related studies [ 4,35,41,54] always utilized delicately designed mirrored initialization (as shown in
Remark 3.1) to avoid the analysis on the initial output function of network, since it will lead to the
challenging problem that fNN
tandfNTK
t are both random, unlike that fNTK
t is a fixed function in
the case of mirrored initialization. However, as shown in Section 3.2, the initial output function is
near a Gaussian process that can not be overlooked. To under the performance of neural networks
commonly used in real world, it is necessary to analyzing the network initialization. To the best of our
knowledge, we are the first to consider the initial output function of network in uniform convergence.
This comprehensive result allows us to study the generalization error of network more precisely.
64 Impact of Initialization
4.1 Impact of standard initialization on the generalization error
The standard kernel gradient flow is always considered to start from zero, as in Proposition 2.2.
Therefore, we need to do a transformation since the initial value of predictor fNTK
t is actually fGP
instead of zero. Firstly, we can yield a solution of (8) in matrix form:
fGF
t(x) =fGF
0(x) +k(x, X)(I−e−1
nk(X,X)) [k(X, X )]−1(fGF
0(X)−f∗(X)−ϵX),(17)
where ϵXis employed to represent the n×1column noise term vector Y−f∗(X). We denote by
fGF
tthe kernel gradient flow predictor under initial function f0and denote by efGF
tthe KGF predictor
under initialization efGF
0≡0. If we plug them into (17) and excess risk (2), respectively, we directly
have the following theorem:
Proposition 4.1 (Impact of initialization in kernel gradient flow) .Denote ef∗=f∗−f0as the biased
regression function. For the KGF predictor fGF
tandefGF
tdefined above, we have
E(fGF
t;f∗) =E(efGF
t;ef∗). (18)
The theorem establishes the equivalence of the generalization properties between the KGF predictor
with initial value f0, regression function f∗and the KGF predictor with initial value zero, regression
function f∗−f0. Back to the network case, combining uniform convergence result in Proposition
3.3, it suggests that, compared to mirrored initialization, the impact of standard initialization which
has non-zero initial output function is equivalent to introducing a same-valued implicit bias to the
regression function. This is a generalization of the main result in Zhang et al. [56], which only
focused on case at t=∞. To summarize, Proposition 4.1 provides a convenient approach to quantify
the impact of standard initialization in early-stopping neural networks.
4.2 Smoothness of Gaussian process
Building upon the analysis above, our focus now turns to illustrating the smoothness of the Gaussian
process fGP, as it is the limit distribution of fNN
0. Actually, we can derive the following theorem:
Theorem 4.2 (Smoothness of Gaussian Process) .Suppose that fGPis a Gaussian process with mean
function 0 and covariance function KRFK. The following statements hold:
P 
fGP∈[HNT]s
= 1, s <3
d+ 1;
P 
fGP∈[HNT]s
= 0, s ≥3
d+ 1.(19)
We furnish a comprehensive proof for Theorem 4.2 in the Appendix C.
Let us now turn our attention to the implications established by this theorem. Recall that Proposition
4.1 has shown that, in KGF, the existence of initialization function fGPis equivalent to adding
a same-valued bias term to the regression function f∗. Consequently, the poor smoothness of
initialization function causes the high smoothness assumption on the regression function meaningless.
Regardless of how smooth we assume the regression function to be (e.g., α(f∗, KNTK)≥2), the
value of (relavtive) smoothness α(f∗−fGP, KNTK)will always be at most3
d+1. Namely, the biased
regression function f∗−fGPis always poorly smooth. In this specific case, we could hardly expect
the KGF predictor to have fine performance.
4.3 Upper bound
Now we are ready to provide the upper bound of generalization error of network. With the help of
Proposition 2.2, Proposition 3.3, Proposition 4.1 and Theorem 4.2, we derive the following theorem:
Theorem 4.3 (Generalization error upper bound) .Assume that the regression function f∗∈[HNT]s
for some s >0, and∥f∗∥[HNT]s≤Rwhere Ris a positive constant. Assume the marginal probability
measure µwith density p(x)satisfies c≤p(x)≤Cfor some positive constant candC.
7•For the case of s≥3
d+1, for any δ∈(0,1)andε∈(0,3
d+3), by choosing certain t=t(n)→ ∞
(as shown in Appendix), when nis sufficiently large and mis sufficiently large, with
probability 1−δwe have
fNN
t−f∗2
L2≤1
δln6
δ2
(R+Cε)2Cn−3
d+3+ε, (20)
where Cεis a positive constant related to ε.
•For the case of 0< s <3
d+1, for any δ∈(0,1), by choosing t≍nd+1
s(d+1)+ d, when nis sufficiently
large and mis sufficiently large, with probability 1−δwe have
fNN
t−f∗2
L2≤1
δln6
δ2
(R+Cs)2Cn−s(d+1)
s(d+1)+ d, (21)
where Csis a positive constant related to s.
The proof is provided in Appendix D. This result shows the generalization error upper bound for
network with standard initialization and demonstrate its negative effect. Even if the goal function
f∗is quite smooth, the generalization error upper bound n−3
d+3remains to be a quite low rate,
particularly considering that the dimension dof data is usually large in real world. It suggests that the
network no longer generalizes well, even if we adopt the once useful early stopping strategy in Li
et al. [41].
4.4 Lower bound
From the analysis above, we can see the poor generalization ability of network under standard
initialization. Furthermore, in this section, we take spherical data as example and provide the lower
bound of generalization error. Namely, we presume the input vectors xare distributed on the sphere
Sdwith probability measure µ, which is a common assumption in NTK theory [ 6,31,36,53]. We
also slightly change the network structure. Compared to the network (9), we eliminate the bias term
of the initial layer, as shown in (40) in Appendix. In this case, the NTK of new network is denoted by
KNTK
0 , and the RKHS HNT
0(Sd)is abbreviated as HNT
0, whose detailed properties is also given in
Appendix C.1. Additionally, we make more assumption on the noise of data. We assume the noise
termϵin(1)to have a constant second moment, as E
|ϵ|2|x
=σ2forx∈Sd, a.e. . Under these
conditions, with the help of method in Li et al. [40], we derive the theorem:
Theorem 4.4 (Generalization error lower bound) .We assume that the regression function f∗∈
[HNT
0]sfor some s >3
d+1, and denote by ∥f∗∥[HNT
0]s≤Rwhere Ris a positive constant. Assume
thatµis the uniform measure. For any δ∈(0,1), when nis large enough and mis large enough, for
any choice of t=t(n)→ ∞ , with probability at least 1−δwe have
EhfNN
t−f∗2
L2|Xi
= Ω
n−3
d+3
. (22)
The proof is given in Appendix E. Through Theorem 4.4, we derive n−3
d+3as the generalization
lower bound of standardly random-initialized network in NTK theory, even if the regression function
is quite smooth. The rate n−3
d+3means model suffers notably from data that has large dimension:
Ifdis relatively large, then this rate of convergence can be extremely slow. This is a manifestation
of the curse of dimensionality. In fact, it contrasts with the fact that neural networks excel at high-
dimensional problems. This contradiction underscores the limitation of NTK theory for interpreting
network performance.
5 Experiments
Our numerical experiments are conducted in two aspects to fully understand the impact of standard
initialization. First, we show the performance of standard initialized network is indeed worse than
the mirrored initialized case, on the aspect of learning rate. The phenomenon is in line with our
theoretical analysis. Second, the smoothness of regression function of real data is significantly larger
than3
d+1, which suggest the bad effect of non-zero intial output function of standard initialization
will indeed destroy the performance of network if NTK theory holds. It demonstrates the drawback
of NTK theory through contradiction.
85.1 Artificial data
In the first experiment, we employ artificial data to show the negative effect of standard initialization
on the generalization error of network. The detailed settings are shown in Appendix F.
Learning rate of network under different initialization The experiments are conducted for both
d= 5andd= 10 , contrasting network performance subject to mirrored and standard initialization
strategies. We choose a relatively smooth goal function to emphasize the impact of initialization.
Specifically, we use m= 20n,epoch = 10n, and the gradient learning rate lr= 0.6. The networks
are made sufficiently wide to ensure the overparametrization assumption is met. Additionally, we
implement the early-stopping strategy as mentioned in Theorem 4.3, that is, selecting the minimum
loss across all epochs as the generalization error. Finally, we test the network’s generalization error
on different levels of sample size n, and plot the log value of the generalization error corresponding
tolog(n)as shown in Figure 1. As we expected, the points in Figure 1 fits a linear trend. Moreover,
the figure highlights the difference in learning rate under different initialization methods. This aligns
with our theoretical results.
Figure 1: Generalization error decay curve of network. The scatter points show the averaged log
error over 20trials. The dashed lines are computed through least-squares. The scale of nis not broad
because a larger nrequires a larger m, which would induce higher computational costs.
5.2 Real data
In this subsection, we focus on datasets from the real world and estimate the smoothness of function.
Although we could not know the goal function that the real data is generated from, there exists a way
to estimate its smoothness [16]. We show the technical details in Appendix G.
Table 1: Smoothness of goal function
Dataset
Name Dimension Smoothness
MNIST 28×28×1 0 .40
CIFAR-10 32×32×3 0 .09
Fashion-MNIST 28×28×1 0 .22
Smoothness of goal function in real datasets We employed the MNIST, CIFAR-10 and Fashion-
MNIST datasets[ 33,38,52]. In the experiments, we evaluate the smoothness of goal function of the
datasets, with respect to the one-hidden layer NTK. The results are presented in Table 1. With the
input dimension d= 784 ,3072,784, we can compute that the smoothness of initialization function
is equal to3
d+1≈0. However, the smoothness of goal function is far better than3
d+1, which implies
that standard initialization will indeed destroy the generalization performance, under NTK theory.
The contradiction between NTK theory and the real situation shows its limitation and once again
confirms our conclusion.
96 Discussion
To summarize, this research focuses on the impact of standard random initialization on generalization
property of fully-connected network in the NTK theory, which makes up the gap in this field. Many
previous work [ 35,41] verified the statistical optimality of neural network under delicately designed
mirrored initialization, whose initial output function of network is zero. However, through our
study, we pinpoint that if we consider the commonly-used standard initialization, the learning rate of
network is notably slow when the dimension of data is slightly large, which fails to explain network’s
favorable performance in overcoming the curse of dimensionality. A direct implication of our work
is the superiority of mirror initialization over standard initialization, which suggests a direction for
future improvements. On a deeper level, although NTK theory can describe many properties of
network, at least for the fully connected networks with Gaussian initialization discussed in this paper,
we can explore better theoretical frameworks to characterize their generalization ability in the future.
10Acknowledgments
Lin’s research was supported in part by the National Natural Science Foundation of China (Grant
92370122, Grant 11971257).
References
[1] Robert A Adams and John JF Fournier. Sobolev spaces . Elsevier, 2003.
[2]Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via
over-parameterization. In International Conference on Machine Learning , pages 242–252.
PMLR, 2019.
[3]N. Aronszajn. Theory of reproducing kernels. Transactions of the American Mathematical
Society , 68(3):337–404, 1950. URL http://dx.doi.org/10.2307/1990404 .
[4]Sanjeev Arora, Simon S Du, Wei Hu, Zhiyuan Li, Russ R Salakhutdinov, and Ruosong Wang.
On exact computation with an infinitely wide neural net. Advances in Neural Information
Processing Systems , 32, 2019.
[5]Benedikt Bauer and Michael Kohler. On deep learning as a remedy for the curse of dimension-
ality in nonparametric regression. 2019.
[6]Alberto Bietti and Francis Bach. Deep equals shallow for relu networks in kernel regimes. arXiv
preprint arXiv:2009.14397 , 2020.
[7]Alberto Bietti and Julien Mairal. On the inductive bias of neural tangent kernels. Advances in
Neural Information Processing Systems , 32, 2019.
[8]Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,
Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are
few-shot learners. Advances in neural information processing systems , 33:1877–1901, 2020.
[9]William Elwood Byerly. An elemenatary treatise on Fourier’s series, and spherical, cylindrical,
and ellipsoidal harmonics, with applications to problems in mathematical physics . Dover
Publicatiions, 1893.
[10] Andrea Caponnetto. Optimal rates for regularization operators in learning theory. 2006.
[11] Andrea Caponnetto and Ernesto De Vito. Optimal rates for the regularized least-squares
algorithm. Foundations of Computational Mathematics , 7:331–368, 2007.
[12] Claudio Carmeli, Ernesto De Vito, and Alessandro Toigo. Reproducing kernel hilbert spaces
and mercer theorem. arXiv preprint math/0504071 , 2005.
[13] Lenaic Chizat, Edouard Oyallon, and Francis Bach. On lazy training in differentiable program-
ming. Advances in neural information processing systems , 32, 2019.
[14] Youngmin Cho and Lawrence Saul. Kernel methods for deep learning. Advances in neural
information processing systems , 22, 2009.
[15] Nadav Cohen, Or Sharir, and Amnon Shashua. On the expressive power of deep learning: A
tensor analysis. In Conference on learning theory , pages 698–728. PMLR, 2016.
[16] Hugo Cui, Bruno Loureiro, Florent Krzakala, and Lenka Zdeborová. Generalization error rates
in kernel regression: The crossover from the noiseless to noisy regime. Advances in Neural
Information Processing Systems , 34:10131–10143, 2021.
[17] George Cybenko. Approximation by superpositions of a sigmoidal function. Mathematics of
control, signals and systems , 2(4):303–314, 1989.
[18] Eleonora Di Nezza, Giampiero Palatucci, and Enrico Valdinoci. Hitchhiker’s guide to the
fractional sobolev spaces. Bulletin des sciences mathématiques , 136(5):521–573, 2012.
[19] Simon Du, Jason Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai. Gradient descent finds global
minima of deep neural networks. In International conference on machine learning , pages
1675–1685. PMLR, 2019.
[20] Simon S Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh. Gradient descent provably optimizes
over-parameterized neural networks. In International Conference on Learning Representations ,
2018.
11[21] Simon Fischer and Ingo Steinwart. Sobolev norm learning rates for regularized least-squares
algorithms. The Journal of Machine Learning Research , 21(1):8464–8501, 2020.
[22] L Lo Gerfo, Lorenzo Rosasco, Francesca Odone, E De Vito, and Alessandro Verri. Spectral
algorithms for supervised learning. Neural Computation , 20(7):1873–1897, 2008.
[23] Nadine Große and Cornelia Schneider. Sobolev spaces on riemannian manifolds with bounded
geometry: general coordinates and traces. Mathematische Nachrichten , 286(16):1586–1613,
2013.
[24] Moritz Haas, David Holzmüller, Ulrike von Luxburg, and Ingo Steinwart. Mind the spikes:
Benign overfitting of kernels and neural networks in fixed dimension. arXiv preprint
arXiv:2305.14077 , 2023.
[25] Boris Hanin. Random neural networks in the infinite width limit as gaussian processes. arXiv
preprint arXiv:2107.01562 , 2021.
[26] Boris Hanin and Mark Sellke. Approximating continuous functions by relu nets of minimal
width. arXiv preprint arXiv:1710.11278 , 2017.
[27] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition ,
pages 770–778, 2016.
[28] Kurt Hornik, Maxwell Stinchcombe, and Halbert White. Multilayer feedforward networks are
universal approximators. Neural networks , 2(5):359–366, 1989.
[29] Tianyang Hu, Wenjia Wang, Cong Lin, and Guang Cheng. Regularization matters: A non-
parametric perspective on overparametrized neural network. In International Conference on
Artificial Intelligence and Statistics , pages 829–837. PMLR, 2021.
[30] Simon Hubbert, Emilio Porcu, Chris Oates, Mark Girolami, et al. Sobolev spaces, kernels and
discrepancies over hyperspheres. arXiv preprint arXiv:2211.09196 , 2022.
[31] Arthur Jacot, Franck Gabriel, and Clément Hongler. Neural tangent kernel: Convergence and
generalization in neural networks. Advances in neural information processing systems , 31,
2018.
[32] Vladimir Koltchinskii and Evarist Giné. Random matrix approximation of spectra of integral
operators. Bernoulli , pages 113–167, 2000.
[33] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images.
2009.
[34] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep
convolutional neural networks. Advances in neural information processing systems , 25, 2012.
[35] Jianfa Lai, Manyun Xu, Rui Chen, and Qian Lin. Generalization ability of wide neural networks
on r. arXiv preprint arXiv:2302.05933 , 2023.
[36] Jianfa Lai, Zixiong Yu, Songtao Tian, and Qian Lin. Generalization ability of wide residual
networks. arXiv preprint arXiv:2305.18506 , 2023.
[37] Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning
applied to document recognition. Proceedings of the IEEE , 86(11):2278–2324, 1998.
[38] Yann LeCun, Corinna Cortes, and CJ Burges. Mnist handwritten digit database. ATT Labs
[Online]. Available: http://yann.lecun.com/exdb/mnist , 2, 2010.
[39] Jaehoon Lee, Lechao Xiao, Samuel Schoenholz, Yasaman Bahri, Roman Novak, Jascha Sohl-
Dickstein, and Jeffrey Pennington. Wide neural networks of any depth evolve as linear models
under gradient descent. Advances in neural information processing systems , 32:8572–8583,
2019.
[40] Yicheng Li, Weiye Gan, Zuoqiang Shi, and Qian Lin. Generalization error curves for analytic
spectral algorithms under power-law decay. arXiv preprint arXiv:2401.01599 , 2024.
[41] Yicheng Li, Zixiong Yu, Guhan Chen, and Qian Lin. On the eigenvalue decay rates of a class
of neural-network related kernel functions defined on general domains. Journal of Machine
Learning Research , 25(82):1–47, 2024.
12[42] Junhong Lin and V olkan Cevher. Optimal convergence for distributed learning with stochastic
gradient methods and spectral algorithms. Journal of Machine Learning Research , 21(147):
1–63, 2020.
[43] Zhou Lu, Hongming Pu, Feicheng Wang, Zhiqiang Hu, and Liwei Wang. The expressive power
of neural networks: A view from the width. Advances in neural information processing systems ,
30, 2017.
[44] Vinod Nair and Geoffrey E Hinton. Rectified linear units improve restricted boltzmann machines.
InProceedings of the 27th international conference on machine learning (ICML-10) , pages
807–814, 2010.
[45] Yoshihiro Sawano et al. Theory of Besov spaces , volume 56. Springer, 2018.
[46] Johannes Schmidt-Hieber. Nonparametric regression using deep neural networks with relu
activation function. 2020.
[47] Alex Smola, Zoltán Ovári, and Robert C Williamson. Regularization with dot-product kernels.
Advances in neural information processing systems , 13, 2000.
[48] Ingo Steinwart and Clint Scovel. Mercer’s theorem on general domains: On the interaction
between measures, kernels, and rkhss. Constructive Approximation , 35:363–417, 2012.
[49] Robert S Strichartz. A guide to distribution theory and Fourier transforms . World Scientific
Publishing Company, 2003.
[50] Namjoon Suh, Hyunouk Ko, and Xiaoming Huo. A non-parametric regression viewpoint:
Generalization of overparametrized deep relu network under noisy observations. In International
Conference on Learning Representations , 2021.
[51] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information
processing systems , 30, 2017.
[52] Han Xiao, Kashif Rasul, and Roland V ollgraf. Fashion-mnist: a novel image dataset for
benchmarking machine learning algorithms. arXiv preprint arXiv:1708.07747 , 2017.
[53] Lechao Xiao, Hong Hu, Theodor Misiakiewicz, Yue Lu, and Jeffrey Pennington. Precise
learning curves and higher-order scalings for dot-product kernel regression. Advances in Neural
Information Processing Systems , 35:4558–4570, 2022.
[54] Jiaming Xu and Hanjing Zhu. Overparametrized multi-layer neural networks: Uniform con-
centration of neural tangent kernel and convergence of stochastic gradient descent. Journal of
Machine Learning Research , 25(94):1–83, 2024.
[55] Haobo Zhang, Yicheng Li, and Qian Lin. On the optimality of misspecified spectral algorithms.
arXiv preprint arXiv:2303.14942 , 2023.
[56] Yaoyu Zhang, Zhi-Qin John Xu, Tao Luo, and Zheng Ma. A type of generalization error induced
by initialization in deep neural networks. In Mathematical and Scientific Machine Learning ,
pages 144–164. PMLR, 2020.
13A Further notations
In appendix, we will provide many technical proofs. Before that, let us provide more notations. For
two sets AandBwith a mapping function ϕ:A→B, the notation ϕ(A)is used to denote the image
set of Aunder ϕ. For two random variable sequences {un}and{vn}, we denote by un=oP(vn)
(orun= ΩP(vn)) if the ratio un/vnapproaches zero (or un≥cvnfor some positive constant c) in
probability as n→ ∞ with respect to probability measure P. For two real number sequence {an}
and{bn}, we denote by an= Ω(bn)if there exists positive constant candn0such that |an| ≥c|bn|
holds for any n≥n0. For two sequences of real numbers {an}and{bn}such that an= Ω(bn)(or
an=O(bn)), we also denote by an≳bn(oran≲bn) . Ifan≳bnandan≲bn, then we denote
byan≍bn.
B Proof of uniform convergence
In this section, we demonstrate the uniform convergence from fNN
ttofNTK
t .
B.1 Initialization
The following is a direct proposition based on Lemma H.2 and Lemma 3.2,
Proposition B.1. For the random network function sequence {fm
0}with probability measures on
(C(X,R),C), there exists {Xm}andXGPdefined on a new probability space (Ω′,F,P), on which
we have
P( lim
m→∞Xm−XGP
∞= 0) = 1 .
where XmandXGPhas the same distribution as fm
0andfGP, respectively.
Remark B.2. The separability of (C(X,R),C)can be derived by the density of polynomials.
Therefore, it satisfies the requirement of Lemma H.2. In the context of our study, our reliance is only
on the distribution of {fm
0}for each given value of m. Consequently, it is reasonable to reconstruct it
in the new probability space. For convenience, we directly denote Xm
0asfm
0(orfNN
0) and denote
andXGPasfGP, respectively. In other words, we are considering the network function in a new
probability space, even though this approach may result in a moderate abuse of notation.
B.2 Uniform convergence of network
Our aim is to give the uniform convergence between NTK regressor fNTK
t and network function
fNN
t. Note that the NTK regressor is trained by NTK, and the network function is trained by NNK,
which is denoted by Km
t. Here we first show the uniform convergence between NNK and NTK as m
comes to infinity.
Lemma B.3. For any δ∈(0,1), suppose mis large enough, then with probability at least 1−δ, we
have
sup
t≥0sup
x,x′∈X|Km
t(x, x′)−KNTK(x, x′)| ≤O(m−1
12p
logm).
Proof. The proof is similar to that in Li et al. [41], while the difference is the way of initialization.
So we only provide the sketch of proof. In Li et al. [41], the uniform convergence of NTK is proved
through a standard ϵ−netargument, which is divided into point-wise convergence and continuity of
both NTK and NNK. Namely, as the following decomposition:
|Km
t(x, x′)−KNTK(x, x′)| ≤|Km
t(x, x′)−Km
t(z, z′)|
+|Km
t(z, z′)−KNTK(z, z′)|+|KNTK(z, z′)−KNTK(x, x′)|.
(23)
where z, z′are the points in the ϵ−netwhich divides X.
Back to our case, in non-zero initialization, the structrue of NTK and NNK remain the same, as
well as the continuity property. Consequently, the effect of initialization reflects on the point-wise
convergence from Km
t(z, z′)toKNTK(z, z′), or more precisely, the NTK regime [ 2]. NTK regime
requires that the residual decays to near zero and thereby the parameters will not deviate too far
from their initial values in the training process, which holds under mirrored initialization. Standard
14initialization lets the residual at time 0befNN
0(X)−Y
2, instead of ∥Y∥2. Therefore, there is a
slight risk that the residual is too large to decay to near zero during training. However, since
fNN
0(X)
2≤O(n·m1
8), (24)
holds with high probability when mis large through Proposition B.1 and direct analysis on fGP, we
can verify that the residualfNN
t(X)−Y
2is still not large enough to break the stable lazy regime.
Namely, the control on parameter matrix that
sup
t≥0W(l)
t−W(l)
0
F=O(m1
4). (25)
still holds. In this way, we can finish the proof.
Then, we can derive the uniform convergence of network function.
Proof of Proposition 3.3. The proof is also similar to that of uniform convergence under mirrored
initialization. Therefore, we only exhibit the sketch of different part. Define event Aas
A=
∥fNN
0−fGP∥∞≤om(1)	
∩ {fGP(X)
2≤Cδ} (26)
where Cδis some constant related to δ, such that event Aholds with probability at least 1−δ
2when
mis large enough. Such a constant Cδis ascertainable, as fNN
0converges to fGPby Proposition B.1
andfGPis a Gaussian process with finite second moment. Define event Bas
B=
sup
t≥0sup
x,x′∈X|Km
t(x, x′)−KNTK(x, x′)| ≤om(1)
. (27)
We have event Bholds with probability at least 1−δ
2when mis large enough. Conditioned on event
AandB, we do kernel gradient flow by Km
tandKNTKonfNN
0andfGPrespectively. Let event C
be
C=
sup
t≥0∥fNN
t−fNTK
t∥∞≤om(1)
. (28)
Conditioned on event AandB, we can prove that event Cholds by Gronwall’s inequality, as the
same method in Lai et al. [35]. In this way, we can finish the proof.
After we get the uniform convergence of network function, we can obtain the proposition on the
convergence of excess risk:
Proposition B.4. Suppose f∗∈L2(X, µ). For any δ∈(0,1)andε >0, when mis large enough ,
with probability at least 1−δ, we have
sup
t>0fNN
t−f∗2
L2−fNTK
t−f∗2
L2≤ε (29)
Proof. Recall the dynamic equation of fNTK
t , we have
|fNTK
t(x)| ≤KNTK(x, X)T
2KNTK(X, X )−1
2fNTK
0(X)−Y
2. (30)
Since the kernel function KNTK(·,·)is bounded, there exists some positive constant C, such that
KNTK(x, X)T
2≤C√n. (31)
The initial function of kernel gradient flow fNTK
0 =fGPfollows a Gaussian process with mean 0
and covariance kernel function KRFK. By the boundness of KRFK, we can also bound fNTK
0 . That
is, for any δ∈(0,1), there exists a positive constant Mδsuch that with probability at least 1−δ/2,
fNTK
0(X)
2≤√nMδ. (32)
Denote λ0:=λmin 
KNTK(X, X )
. We have λ >0since KNTKis strictly positive definite [ 41].
Thus we have
|fNTK
t(x)| ≤C√nλ−1
0(√nMδ+∥Y∥2). (33)
15The excess risk
|E(fNN
t;f∗)− E(fNTK
t;f∗)|=Z
X|fNN
t−fNTK
t|2dµ+Z
X(fNTK
t−f∗)(fNN
t−fNTK
t) dµ
(34)
Since f∗∈L2(X, µ)where µis probability measure, we also have f∗∈L1(X, µ). Denote
Mf∗:=∥f∗∥L1and∆:= supx∈X,t≥0|fNN
t(x)−fNTK
t|. We have
|E(fNN
t;f∗)− E(fNTK
t;f∗)| ≤∆2·(1 +C√nλ−1
0(√nMδ+∥Y∥2) +Mf∗) (35)
By Proposition 3.3, when mis large enough, with probability at least 1−δwe have ∆2≤ε/(1 +
C√nλ−1
0(√nMδ+∥Y∥2) +Mf∗). Thus the proposition is proved.
C Proof of the Theorem 4.2
Before the proof, first we introduce some basic properties of NTK and RFK, as well as some technical
properties of Sobolev space. We say that two Hilbert space H1,H2are equivalent if they are equal as
sets and share equivalent norm. If H1andH2are equivalent, we denote by H1∼=H2.
C.1 Basic properties of NTK and RFK
Dot-product kernel A reproducing kernel function kis dot-product if its value only depends on
the dot-product of inputs. That is, there exists function κsuch that
k(x, x′) =κ(⟨x, x′⟩). (36)
A dot-product kernel on sphere can be decomposed with spherical harmonic polynomials as the
eigenfunction:
k(x, y) =∞X
n=0µnanX
l=1Yn,l(x)Yn,l(y). (37)
where spherical harmonic polynomials {Yn,l, l= 1,···, an}are also the orthonormal basis of
L2(Sd, σ), with σdenoting the uniform measure on Sd[47]. This is also its Mercer decomposition.
Now come back to our network case. We first define two dot-product kernels on Sd,
KNTK
0(x, y):=LX
r=0κ(r)
1(u)L−1Y
s=rκ0(κ(s)
1(u)), KRFK
0(x, y):=κ(L)
1(u), (38)
where u=⟨x, y⟩=xTyand
κ0(u) =1
π(π−arccos u), κ 1(u) =1
πp
1−u2+u
π(π−arccos u). (39)
The definition of κ(t)
1is given by the composition κ1◦κ1··· ◦κ1(a total of tcompositions). The
explicit expression indicates that KNTK
0 andKRFK
0 are dot-product kernels on Sd.
KNTK
0 andKRFK
0 is the homogeneous NTK and RFK of a homogeneous fully-connected network
fSdefined on Sd[6,14], whose structural difference from (9)is the removal of the bias term in the
first layer. Specifically, the network is structured as follows:
Homogeneous fully-connected network on sphere The network is constructed using the following
recursive formula:
α(1)(x) =r
2
m1W(0)x;
α(l)(x) =r
2
mlW(l−1)(x)σ(α(l−1)(x)), l= 2,3,···, L;
fS(x;θ) =W(L)σ(α(L)(x)),(40)
16where the function σis entrywise ReLU activation. The parameter matrix for the l-th layer is denoted
asW(l), whose dimensions are of ml+1×ml, where mlis the number of units in layer landml+1
is that of layer l+ 1forl∈ {0,1,···, L−1}. We also set m0to be equal to d+ 1andmL+1equal
to1. The network is also random initialized as (10).
We can easily build a connection between our NTK and RFK for network (9)and the homogeneous
kernels defined in (38). For x∈ X, letex= (x,1)which means add 1as the new last component
ofx. Define ϕ(x):=(x,1)
∥(x,1)∥being an isomorphism from open set Xto a subdomain of positive
hemisphere shell S=ϕ(X)⊂Sd
+. Then we have
f(x) =∥ex∥fS(ϕ(x)), (41)
where fis network (9) and fSis network (40). Actually, we can thus verify that
KNTK(x, y) =∥ex∥∥ey∥KNTK
0(ϕ(x), ϕ(y)), KRFK(x, y) =∥ex∥∥ey∥KRFK
0(ϕ(x), ϕ(y)).
(42)
We denote by HNT
0andHRF
0the RKHS on Sdwith respect to KNTK
0 andKRFK
0. Their eigenvalue
decay rates are well known:
Lemma C.1 (Bietti and Bach [6], Haas et al. [24]).ForKNTK
0 andKRFK
0 onSdwith uniform
measure σ, the decay rate of spherical harmonics coefficients satisfy
µn(KNTK
0)≍n−(d+1)and µn(KRFK
0)≍n−(d+3), (43)
while the eigenvalues satisfy
λi(KNTK
0,Sd, σ)≍i−d+1
d and λi(KRFK
0,Sd, σ)≍id+3
d. (44)
Additionally, we list some further result on the eigenvalue decay rate of NTK and RFK provided by
Li et al. [41], which will be used later:
Lemma C.2. Denote Ωas a non-empty subdomain of Sd. ForKNTK
0 andKRFK
0, we have eigenvalue
decay rate:
λi(KNTK
0,Ω, σ)≍i−d+1
d and λi(KRFK
0,Ω, σ)≍i−d+3
d
where σis the uniform measure on S.
Lemma C.3. ForKNTKandKRFK, we have eigenvalue decay rate:
λi(KNTK,X, µ)≍i−d+1
d and λi(KRFK,X, µ)≍i−d+3
d
where measure µon bounded domain X ⊂Rdhas density c≤p(x)≤Cwith respect to the
Lebesgue measure.
C.2 Basic concepts of Sobolev space
Sobolev Space of integer power LetXbe a open subset of Rd. Let m∈N,1≤p≤+∞.
Sobolev space Wm,p(X)is defined as a set of function such that
∥Dαf∥Lp<+∞, (45)
where αis a vector with length nandDαfis the weak α-th partial derivative of f. In other words,
the definition of Wm,pis:
Wm,p(X) ={f∈Lp(X)|Dαf∈Lp(X),∀|α| ≤m}, (46)
where 1≤p≤ ∞ . Conventionally, when the index pis equal to 2, we denote Wm,pbyHm, since
it is a Hilbert space. Further, if the index m >d
2, the Sobolev space Hmqualifies as a RKHS and
thus embraces the properties of RKHS. In our work, we mainly utilize its property of interpolation as
defined in (6). Consequently, we first introduce a generalized concept of real interpolation [ 1], as an
expansion to the definition in (6).
17Real interpolation For two Bananch spaces H1andH2, we use interpolation space to represent a
space that lies in between them in some specific way. We introduce the commonly-used K-method to
define real interpolation. Suppose 0< s < 1,q≥1. The space generated by their real interpolation
H= (H1,H2)s,qis defined by following:
K(t;x) = inf
x=x1+x2;x1∈H1,x2∈H2∥x1∥H1+t∥x2∥H2, (47)
and
∥x∥H=Z∞
0(t−sK(t;x))qdt
t1
q
. (48)
Based on the definition of real interpolation, we introduce some basic concepts about fractional power
Sobolev space.
Sobolev space of fractional power Suppose X ∈Rdis a bounded domain with smooth boundary
and denote Lebesgue measure by µ. We can define fractional power Sobolev space through real
interpolation (we refer to [45] Chapter 4.2.2 for more details):
Hs(X):= 
L2(X, µ), Hm(X
s
m,2(49)
The fractional power Sobolev space Hr(X)withr≥d
2is also a RKHS [ 1]. Specifically, Steinwart
and Scovel [48] reveals that for 0< s < 1,
[H]s∼= 
L2(X, µ),H
s,2(50)
for RKHS Hand the interpolation defined in (6). Therefore, the results above directly implies that
[Hr(X)]s=Hrs(X) (51)
holds for any r≥d
2ands >0.
Up to now, we have introduced the basic properties of Sobolev spaces on X, an open subset of Rd.
For Sobolev spaces defined on more intricate manifolds, such as hyperspheres, owing to the intricate
property of Sobolev spaces, numerous equivalent definitions emerges [1, 18].
We now delineate a kind of definition that will facilitate our subsequent proofs, since we will
consider RKHSs on Sd, likeHNT
0andHRF
0, which are the RKHSs associated with KNTK
0 and
KRFK
0, respectively. Such definition can form a linkage with the Sobolev spaces defined on Sd
and on domain X ⊂Rd, which is also utilized in Haas et al. [24]. Our exposition begins with the
characterization of a manifold.
Trivilization Define a trivialization of a Riemannian manifold (M, g)with bounded geometry of
dimension d, which consists three part. The first part is some locally finite open covering {Uα}α∈I.
The second part is the charts {κα}α∈Iwhich consists of smooth diffeomorphism κα:Vα⊂Rd→
Uα. The third part is a partion of unity hαsuch that supp( hα)⊂Uα,P
α∈Ihα= 1and0≤hα≤1.
In our case, we write a trivialization of Sd, which, is a manifold of dimension d. We write U1=
{xd+1< ϵ|x∈Sd}andU2={xd+1>ϵ
2|x∈Sd}for a small fixed ϵ >0. Let ϕ1:U1→Rd
andϕ2:U2→Rdbe stereographic projections with respect to x1= (0,0,···,1)andx2=
(0,0,···,−1), respectively. Namely, they are
ϕ1: (x1, x2,···, xd+1)7→1
1 +xd+1(x1, x2,···, xd) (52)
and
ϕ2: (x1, x2,···, xd+1)7→1
1−xd+1(x1, x2,···, xd). (53)
Finally, we can find C∞smooth functions h1andh2such that h1|Sd
+= 1. For the simple trivialization
above, we can directly verify that it meets the admissible trivialization condition (details see Große
and Schneider [23]). Thus we can apply Theorem 14 of [ 23] to define the norm of Sobolev space on
Sd:
∥f∥Hs(Sd)=(h1f)◦ϕ−1
12
Hs(Rd)+(h2f)◦ϕ−1
22
Hs(Rd)1
2, (54)
for distribution f∈ D′(Sd)[49]. It gives a kind of equivalent definition of Sobolev space on Sd.
18C.3 Relationship between dot-product kernel and Sobolev space
Previous work observed that, for dot-product kernels defined on sphere with polynomial eigenvalue
decay rate, their RKHSs are equivalent to Sobolev spaces:
Lemma C.4 (Hubbert et al. [30] Section 3) .For a dot-product kernel kdefined on Sdand its RKHS
Hk, if the coefficients of spherical harmonic polynomials satisfies µn≍ntfor some t≥d, then
there exists an equivalence between RKHS and Sobolev space:
Hk∼=Ht
2(Sd).
Recall that KNTK
0 andKRFK
0 are both dot-product kernels with polynomial eigenvalue decay rate
by Lemma C.1. Therefore, Lemma C.4 provides the equivalence between HNT
0,HRF
0and the
corresponding Sobolev spaces on Sd. We have the following proposition:
Proposition C.5. We have the following equivalence:
HNT
0∼=Hd+1
2(Sd)andHRF
0∼=Hd+3
2(Sd). (55)
C.4 Interpolation of HNT
0andHRF
0
In this subsection, we aims to provide the interpolation relationship between RKHSs associated
withKNTK
0 andKRFK
0, on a subdomain of Sd. We remind that if we consider the case on Sd, i.e.
HNT
0andHRF
0, the conclusion is direct since they are both dot-product kernels and share the same
orthogonal basis {Yn,l}as introduced in (37).
Suppose s≥d
2andΩbe a subdomain of SdwithC∞smooth boundary. With a little abuse of
notation, we define Hs(Ω)as the RKHS Hs(Sd)restricted to Ωin the way of Lemma H.3. For
an injection φ: Ω→Rd, we define Hs(φ(Ω))◦φ:={f◦φ|f∈Hs(φ(Ω))}with norm
∥f◦φ∥=∥f∥Hs(φ(Ω)). Recall that ϕ1is the stereographic projection defined in (52), now let us
show the equivalence of Hs(Sd
+)andHs(ϕ1(Sd
+))◦ϕ1.
Lemma C.6 (Equivalence as sets) .Suppose s≥d
2. At the aspect of sets, we have Hs(Sd
+) =
Hs(ϕ1(Sd
+))◦ϕ1.
Proof. For a f∈Hs(Sd
+), we have an extension ∥f′∥Hs(Sd)<∞suchf′|Sd
+=f. Thus
(h1f′)◦ϕ−1
1
Hs(Rd)≤f◦ϕ−1
1
Hs(Rd)<∞ (56)
which implies (h1f′)◦ϕ−1
1∈Hs(Rd). Then we have [(h1f′)◦ϕ−1
1]|ϕ1(Sd
+)∈Hs(ϕ1(Sd
+)). Since
f=f′|Sd
+andh1|Sd
+= 1, we have f◦ϕ−1
1∈Hs(ϕ1(Sd
+)).
In the converse direction, we assume f∈Hs(ϕ1(Sd
+)). Then we know there exists f′∈Hs(Rd)
such that f′|ϕ1(Sd
+)=f. Now we want to show f◦ϕ1∈Hs(Sd
+). Define a ψ∈C∞(Rd)such that
ψ(ϕ1(Sd
+))≡1andψ((ϕ1(U1/U2))c)≡0. According to (54), we have
∥f◦ϕ1∥Hs(Sd
+)≤ ∥(ψ·f′)◦ϕ1∥Hs(Sd)=(h1◦ϕ−1
1)·ψ·f′
Hs(Rd)<∞ (57)
Thus we finish the proof.
Lemma C.7 (Equivalence as space) .Suppose s≥d
2. At the aspect of spaces, we have Hs(Sd
+)∼=
Hs(ϕ1(Sd
+))◦ϕ1.
Proof. By Lemma C.6, we know Hs(Sd
+)∼=Hs(ϕ1(Sd
+))◦ϕ1as sets. Since Hs(Sd
+)and
Hs(ϕ1(Sd
+))◦ϕ1are both RKHSs, we can finish the proof by closed graph theorem.
For notational simplicity, denote by H1=Hs(Sd
+)andH2=Hs(ϕ1(Sd
+))◦ϕ1. Define the canonical
mapI:H1→ H 2asI:h7→h. Let{hn}n∈Nbe a sequence such that there exists h∈ H 1and
g∈ H 2where hn→hinH1andhn=Ihn→ginH2. It implies that h=g. Therefore, closed
graph theorem shows that the linear operator Iis bounded, which means that ∥h∥H1≤C∥h∥H2
holds for some positive constant Cand any h∈ H 1. We can also prove ∥h∥H2≤C′∥h∥H1for any
hin the same way. Consequently, the lemma is proved.
19Now we come back to our network case. Let S:=ϕ(X)⊂Sd
+whereXis the set from which data x
is sampled, and ϕis used in (42). Since the boundary of XisC∞smooth, we know that SisC∞
smooth. If we combine Lemma C.4, Lemma C.7 and Proposition H.4, then we can directly show the
following lemma:
Lemma C.8. Define X1=ϕ1(S). For KNTK
0 andKRFK
0 defined on S, we have the following
equivalence:
HRF
0(S)∼=Hd+3
2(X1)◦ϕ1,and
HNT
0(S)∼=Hd+1
2(X1)◦ϕ1.(58)
Now we can obtain the interpolation relationship between HRF
0(S)andHNT
0(S).
Lemma C.9. Suppose s≥0. We have
[HNT
0(S)]s∼=[HRF
0(S)]s(d+1)
d+3
Proof. Define X1=ϕ1(S). Let σbe the uniform measure on Sd. Recalling (51), we have the
interpolation on X1with lebesgue measure denoted by µ1:
[Hd+3
2(X1)]s(d+1)
d+3∼=Hs(d+1)
2(X1)∼=[Hd+1
2(X1)]s(59)
that is 
L2(X1, µ1), Hd+3
2(X1)
s(d+1)
d+3,2∼=
L2(X1, µ1), Hd+1
2(X1)
s,2(60)
Since f7→f◦ϕ1is an isometric isomorphism, we have

L2(X1, µ1)◦ϕ1, Hd+3
2(X1)◦ϕ1
s(d+1)
d+3,2∼=
L2(X1, µ1)◦ϕ1, Hd+1
2(X1)◦ϕ1
s,2(61)
Recall that Xis bounded and thus X1=ϕ1(ϕ(X))is bounded. Therefore, the Jacobian Jϕ−1
1
satisfies c≤ |Jϕ−1
1| ≤Cfor some constant candC. It is easy to verify that L2(X1, µ1)◦ϕ1=
L2(S, µ1◦ϕ1)∼=L2(S, σ). Finally, with Lemma C.8, Lemma H.5 and Lemma H.6, we have
[HRF
0(S)]s(d+1)
d+3∼=[HNT
0(S)]s(62)
with respect to the uniform measure σonS.
C.5 Smoothness of Gaussian process
Lemma C.9 provides the interpolation relationship between HNT
0(S)andHRF
0(S). By the kernel
transformation relationship of NTK and RFK from Rdand to Sdas described in (42), we can also
derive the interpolation relationship of HNTandHRF. It will help for us to derive the smoothness of
fGP.
Lemma C.10 (Interpolation of RKHSs) .Suppose s >0. We have
[HNT(X)]s∼=[HRF(X)]s(d+1)
d+3
with respect to measure µonXwhich has Lebesgue density c≤p(x)≤C.
Proof. Define a function ρ(x) =∥ex∥onX. Define measure νonXsuch that the Radon-Nikodym
derivative satisfiesdν
dµ=ρ2. We consider measure ν◦ϕonSas well as measure µonX, and then
define a map I: [HNT
0(S)]s→[HNT(X)]s:
I:f7→ρ·(f◦ϕ). (63)
Now we prove Iis an isometric isomorphism. We first show that for any eigen pair (f, λ)of
(KNTK
0, S, ν◦ϕ),(If, λ)is also an eigen pair of (KNTK,X, µ). Actually, for eigen pair (f, λ)we
have Z
SKNTK
0(x, y)f(y) d(ν◦ϕ) (y) =λf(z). (64)
20We perform a transformation of the integral domain,Z
XKNTK
0(ϕ(x), ϕ(y))f(ϕ(y))dν(y) =λf(ϕ(x))
=Z
XKNTK
0(ϕ(x), ϕ(y))f(ϕ(y))ρ2(y)dµ(y)(65)
Recalling the transformation between KNTK
0 andKNTKin (42), we haveZ
Xρ(x)KNTK
0(ϕ(x), ϕ(y))f(ϕ(y))ρ2(y)dµ(y) =λρ(x)f(ϕ(x))
=Z
XK(x, y)f(ϕ(y))ρ(y)dµ(y)(66)
These transformations are both reversible. Therefore, through the structure of real interpolation space
as described in (6), we can see Iis an isometric isomorphism . In the same way, there exist isometric
isomorphism I′: [HRF
0(S)]s(d+1)
d+3→[HRF(X)]s(d+1)
d+3:
I′:f7→ρ·(f◦ϕ). (67)
Combined the result in Lemma C.9, the Lemma is proved.
Now we are ready to give the smoothness of Gaussian process fGP. We remind the reader that
HNTandHRFare abbreviations used for denoting HNT(X)andHRF(X), respectively.
Proof of Theorem 4.2. Lett=s(d+1)
d+3to simplify the notation. By Lemma C.10, we have
[HNT]s∼=[HRF]t. (68)
Recalling the structure of interpolation space, we suppose [HRF]tcan be written as
[HRF]t=(X
i∈Nciλt
2
ieiX
i∈Nc2
i<∞)
. (69)
Recall that fGPrepresents a random function defined on (Ω,F,P), where each ω∈Ωcorresponds
to a path function fGP
ω:X → R. We can express this in the orthonormal basis as fGP
ω=P
i∈Nai(ω)λt
2
tei, where
ai(ω) =⟨fGP
ω, λt
2
iei⟩[HRF]t=λ−t
2
iZ
fGP
ωei(x)dµ(x).
Recall that as defined in Lemma 3.2, fGPhas the distribution GP(0, KRFK). From this, we can
acquire the joined distribution for ai. Firstly, let us compute the covariance:
Cov(ai, aj) =E[ai, aj]
=E
λ−t/2
iλ−t/2
jZ
XZ
XfGP(x)fGP(y)ei(x)ei(y) dµ(x)dµ(y)
=λ−t/2
iλ−t/2
jZ
XZ
XE
fGP(x)fGP(y)
ei(x)ei(y) dµ(x)dµ(y)
=λ−t/2
iλ−t/2
jZ
XZ
XKRFK(x, y)ei(x)ei(y) dµ(x)dµ(y)
=λ−(1−t)/2
i λ−(1−t)/2
j 1{i=j}.(70)
The exchange of integration is accomplished by Fubini’s theorem since KRFKis a bounded kernel
function, and both eiandejareL2integrable. Moreover, as fGPis a Gaussian process, we finally
getai∼N(0, λ1−t
i)fori∈N, and ai,ajare independent for any i̸=j. Consequently, we can
directly derive thatfGP2
[HNT]s=X
i∈Nλ1−t
iZ2
i, (71)
where {Zi}indicates a collection of independent and identically distributed standard Gaussian
random variables. Finally, as Lemma C.3 establishes the eigenvalue decay rate as
λi≍id+3
d, (72)
it is direct to prove the theorem.
21Part 1. When s <3
d+1, we haved+3
d·(1−t)>1and thus
EfGP2
[HNT]s≍X
i∈Ni−d+3
d·(1−t)<+∞. (73)
Consequently we have PfGP2
[HNT]s<∞
= 1.
Part 2. When s≥3
d+1, we ascertain thatd+3
d·(1−t)≤1and consequently
EfGP2
[HNT]s≍X
i∈Ni−d+3
d·(1−t)= +∞. (74)
Denote by Xn=Pn
i=1λ1−t
iZ2
i. We then obtain
EXn=nX
i=1λ1−t
i,VarXn=nX
i=12λ1−t
i. (75)
We can thus derive that
P(Xn≤EXn
2)≤P(|Xn−EXn| ≥EXn
2)≤4VarXn
[EXn]2=8Pn
i=1λ1−t
i. (76)
Given thatfGP2
[HNT]s≥Xnfor any n∈N+, we have
P(fGP2
[HNT]s=∞) = lim
M→∞P(fGP2
[HNT]s≥M)≥1−lim
n→∞P(Xn≤EXn
2) = 1 .(77)
This completes the proof.
D Proof of Theorem 4.3
With the findings from Theorem 4.2, Proposition 4.1, and Proposition B.4, the influence of non-zero
initialization could be interpreted in terms of a misspecified spectral algorithms problem. To apply
Proposition 2.2, it only remains to determine the embedding index of HNT. Now, let’s proceed to do
so.
D.1 Embedding index of HNT
Recall that the Proposition 2.2 requires the embedding index of HNTonXunder the probability
measure µ. Fortunately, the embedding index of the Sobolev space has been previously established
by Zhang et al. [55], which is helpful to simplify our proof.
Lemma D.1 (Zhang et al. [55] Section 4.2, Embedding index of Sobolev space) .Suppose r >d
2. For
a bounded open set X ⊂Rdand Lebesgue measure µ, the embedding index of Hr(X)equalsd
2r.
Since we have established the relationship between HNTand the Sobolev space, we can easily get
the embedding index through a similar way used in the proof of Lemma C.10.
Lemma D.2 (Embedding index of NTK) .Suppose that the density function p(x)of probability
measure µsatisfies the condition c≤p(x)≤C, where candCare positive constants. The
embedding index of HNT(X)with respect to µis concluded to bed
d+1.
We omit this proof as it can be carried out in the same manner as Lemma C.10. Here, we provide only
the structure. First, the embedding index of Hd+1
2(S)isd
d+1( Lemma D.1 and Lemma C.7). Second,
the embedding index of HNT
0(S)isd
d+1(Lemma C.4). Third, the embedding index of HNT(X)is
d
d+1since I:f7→ρ·(f◦ϕ)is isometric isomorphism both from HNT
0(S)toHNT(X)and from
L∞(S, ν′◦ϕ)toL∞(X, µ), where measure ν′is defined ondν′
dµ=ρ(an argument similar to that in
the proof of Lemma C.10).
22D.2 Proof of Theorem 4.3
Proof of Theorem 4.3. Recall that Proposition 4.1 elucidates the impact of non-zero initialization.
Namely, the generalization error of the kernel gradient flow with an initialization of f0and a regression
function f∗, is consequently equivalent to that of kernel gradient flow with initialization at 0and
a regression function of f∗−f0. On the other hand, Proposition B.4 demonstrated the uniform
convergence from the network function to the kernel gradient flow predictor as the network width m
tends to infinity. Lemma D.2 verify the embbeding index condition in Proposition 2.2. Thus, we only
need to verify the source condition that fGP−f∗fulfills and to incorporate it with Proposition 2.2 in
order to derive the generalization error of the kernel gradient flow.
Now we start the proof. Since the proofs for the cases s≥3
d+1and0< s <3
d+1are exactly the
same, we will only provide the proof for the former case here. Through Theorem 4.2, we know for
any0< r <3
d+1, it follows that EfGP2
[HNT]r=Pλ1−r
i<∞. Let Ct=EfGP
[HNT]r. By
the Markov inequality, for any δ′∈(0,1), we have with probability exceeding 1−δ′, that
fGP−f∗
[HNT]r≤R+Cr
δ′. (78)
Recall that the eigenvalue decay rate for KNTKisd+1
das mentioned in Lemma C.2. Therefore, we
have for any δ∈(0,1)and any ε∈(0,3
d+3), there exists r <3
d+1such thatrβ
rβ+1=3
d+3−ε(i.e.,
r=d2−6d−3d(d+3)ε
3(d+1)+ ε(d+1)(d+3)). Denote by ef∗=f∗−fGPandefNTK
t be the kernel gradient flow predictor
starts from initial value 0. Through Proposition 2.2, We thus have
efNTK
t−ef∗2
L2≤1
δ′ln6
δ2
(R+Cr)2C′n−3
d+3+ε, (79)
holds with probability at least 1−2δ′when t≍nβ
rβ+1. Through Proposition 4.1, also we have
fNTK
t−f∗2
L2≤1
δ′ln6
δ2
(R+Cr)2C′n−3
d+3+ε, (80)
holds with probability at least 1−2δ′. Through uniform convergence in Proposition B.4, we have
sup
t≥0fNN
t−f∗
L2−fNTK
t−f∗2
L2≤1
δ′ln6
δ2
(R+Cr)2C′n−3
d+3+ε, (81)
with probability at least 1−δ′when mis large enough. Therefore, with appropriate choice of δ′and
C′, we can finish the proof.
E Proof of Theorem 4.4
In this section, we establish the generalization error rate lower bound in our problem. We incorporate
a result delineated in [ 40], which systematically studies the learning rate of kernel regression. Prior
to this, we take some preparatory work.
We assume kis a dot-product kernel on Sdwith eigenvalue decay rate β, with respect to the uniform
measure. We notate the corresponding RKHS as Hk. Then, we can verify that Hksatisties to the
definition of regular RKHS , as detailed in [ 40]. Subsequently, the main theorem in [ 40] can be applied
under our proposed settings, since KNTK
0 is a dot-product kernel defined on Sd. It engenders the
following lemma.
Lemma E.1 (Generalization error lower bound) .Assume kis a dot-product kernel defined on Sd,
we have the interpolation space of its RKHS as [Hk]s=nP
i∈Naiλs
2
iei|P
i∈Na2
i<∞o
where
{ei}i∈Nis the orthonormal basis of L2(Sd, σ)andσdenotes the uniform measure. Decompose the
regression function f∗over the series of basis:
f∗=X
i∈Nfiei. (82)
23We assume that f∗∈[H]tholds for any t < s for a given s >0. Also, we we assume that
X
i:λ>λ i|fi|2= Ω ( λs). (83)
We also assume that the noise term satisties E[|ϵ|2|x] =σ2holds for x∈Sd,a.e. Then, we define the
main bias term in generalization error by
R2(t;f∗) =X
i∈Ne−2tλiλif2
i, (84)
and define the variance term by
N(t) =X
i∈N[λie−tλi]2. (85)
Fix the given input vectors of samples X. Consider the kernel gradient flow process detailed in (8)
and let it start from 0. For any choice of t=t(n)→ ∞ , we have
EhfGF
t−f∗2
L2|Xi
= ΩP
R2(t;f∗) +1
nN(t)
, (86)
With the lemma above, now we are ready to prove Theorem 4.4.
Proof of Theorem 4.4. By Proposition 4.1, we know the initial output function introduce an implicit
bias term to the regression function. And thus the original problem is same as to consider a standard
kernel gradient flow problem start from initial output zero with regression function ˜f∗=f∗−fGP.
Recall that µis the uniform measure. On sphere, the RKHSs of dot-product kernels KNTK
0 and
KRFK
0 are equivalent to corresponding Sobolev spaces through Lemma C.4. More precisely, suppose
that we have chosen an orthonormal basis {ei}i∈Nconsisting of spherical harmonic polynomials.
Then we have
[HNT
0]t=(X
i∈Naiωt/2
ieiX
i∈Na2
i<∞)
,
[HRF
0]t=(X
i∈Naiλt/2
ieiX
i∈Na2
i<∞) (87)
for any t≥0. Through Lemma C.2, we have the eigenvalue decay rate:
ωi≍i−d+1
d and λi≍i−d+3
d. (88)
We denote by β1=d+1
dandβ2=d+3
d. Similar to the proof of Theorem 4.2, we write the
Kosambi–Karhunen–Loève expansion of ˜f∗:
˜f∗=X
i∈Nefiei=X
i∈N(bi−ai)λ1/2
iei, (89)
where
aii.i.d.∼N(0,1),and f∗=X
i∈Nbiλ1/2
iei∈[HNT
0]s. (90)
Here aiis a sequence of independent standard Gaussian variables, and birepresents a sequence
derived from the decomposition of f∗. With such decomposition, we can verify that (83) holds with
probability 1−δ′for any δ′∈(0,1). Denote by g(λ) =P
i:λ>ω i|efi|2. Firstly, we have
E[g(λ)] =E"X
i:λ>ω i|efi|2#
≍E
X
i>⌊λ−d
d+1⌋|efi|2
≳λ3
d+1. (91)
We also have the variance
Var [g(λ)] = Var"X
i:λ>ω i|efi|2#
≍X
i:λ>ω iλi+X
i:λ>ω iλib2
i≲λ3
d+1. (92)
24Where the second term is controlled by the source condition assumption on f∗. Therefore, we have
P
|g(λ)−E[g(λ)]| ≥Eg(λ)
2
≤4Var[ g(λ)]
(Eg[(λ)])2=O(λ3
d+1). (93)
Define event A(λ) ={|g(λ)−E[g(λ])| ≤E[g(λ)]/2}. For any δ′∈(0,1), we choose a sequence
eλj, such that eλj=C′j−2(d+1)
3. Then we have
P
∪j∈NA
eλj
≥1−X
j∈N[C′]3
d+1j−2. (94)
We can choose appropriate C′>0such that ∪j∈NA(eλj)holds with probability at least 1−δ′, we
denote by event A. Conditioned on event A, for any eλj+1≤λ≤eλj, we have
g(λ)≥g(eλj+1)≳1
2(eλj+1)3
d+1 andeλj+1
eλj=j
j+ 12(d+1)
3
(95)
which shows that
g(λ)≳1
2(eλj+1)3
d+1≳1
2j
j+ 12
λ3
d+1. (96)
Therefore, we finish the proof of (83).
Then, we turns to the calculation of generalization error lower bound. First, we plug in the decompo-
sition and calculate the bias term R(t;˜f∗):
R2(t;˜f∗) =X
i∈Ne−2tλiλi(a2
i−2biai+b2
i). (97)
Recalling that the eigenvalue decay rate is denoted by β, it follows that
Eh
R2(t;˜f∗)i
≥X
i∈Ne−2tλiλi≍X
i∈Ne−2ti−β1i−β2≍t1
β1−β2
β1. (98)
Also, the variance of R2(t;˜f∗)follows that
Var(R2(t;˜f∗))≲X
i∈Ne−4tλiλ2
i+X
i∈Ne−4tλib2
iλ2
i, (99)
Here we introduce the denotations:
V0:=X
i∈Ne−4tλi2λ2
i,and V2:=X
i∈Ne−4tλib2
iλ2
i. (100)
We then have
V0=X
i∈Ne−4tλiλ2
i≍X
i∈Ne−4ti−β1i−2β2≍t1
β1−2β2
β1. (101)
As to V2, we first recall that the smoothness of f∗lead to the following inequality:
X
i∈Nb2
ii−1<∞, (102)
which implies thatX
i∈Nb4
ii−2<∞. (103)
Now we turn to the evaluation of V2:
V2=X
i∈Ne−4tλib2
iλ2
i≍X
i∈Ne−4ti−β1b2
ii−2β2=X
i∈Ne−4ti−β1b2
ii−1i−2β2+1
≤sX
i∈Ne−8ti−2β1i−4β2+2X
i∈Nb4
ii−2≲t1
2β1−2β2
β1+1
β1.(104)
25It is worth noting that we use Cauchy’s inequality to derive the upper bound above. With the control
ofV0andV2, we have
Var(R2(t;˜f∗))≍V0+V2≲t3
2β1−2β2
β1 (105)
Consequently, by Chebyshev’s inequality, we directly have
P
|R2(t;˜f∗)−Eh
R2(t;˜f∗)i
| ≥Eh
R2(˜f∗)i
/2
≤4Var(R2(t;˜f∗))

Eh
R2(t;˜f∗)i2=O
t−1
2β1
.(106)
Since t=t(n)→+∞, we have
R2(t;˜f∗) = Ω P
t1
β1−β2
β1
. (107)
In the same way, we also have the bound of variance term N(t).
N(t)≍1
nt1
β1. (108)
Finally, apply Lemma E.1 and Proposition 3.3. We derive that for any δ >0, as long as nis large
enough and mis large enough, for any choice of t=t(n)→ ∞ , with probability at least 1−δwe
have
E
∥fNN
t−f∗∥2
L2|X
= Ω
R2+1
nN
= Ω
t1
β1−β2
β1+1
nt1
β1
= Ω
n−3
d+3
. (109)
Thus the theorem is proved.
Remark E.2. In Proposition 3.3, we consider the situation that both input Xand output Yof samples
are fixed, while in the proof above we require that only Xis fixed. However, the conclusion of
Proposition 3.3 still holds when Yof samples is random. This is because the noise term has a finite
second moment. Also, the change of domain from XtoSdwill not affect the uniform convergence
result.
F Details in artificial data experiments
Fixing the dimension of data as d= 5,10. We draw samples for variable xfrom the standard
Gaussian distribution N(0, Id), which are consequently standardized to lie on the surface of the unit
hypersphere Sd. The dependent variable yis formulated as:
y=f(x) +ε, (110)
where f(x) =Pd
j=1xj2
,ε∼ N(0, σ2)andσ= 0.2. The function fexhibits notable smoothness,
since it can be linearly represented in terms of the first few spherical harmonic polynomials on Sd
[9] and the fact that KNTK
0 is a dot-product kernel. We consider fully-connected network with one
singular hidden layer, choosing m= 20∗nto ensure large enough width. Consistent to previous
sections, we choose ReLU function as the non-linear activation and train the network using Gradient
Descent for a sufficiently long time. We record the generalization error at each moment and define
the moment of minimum generalization error as the final generalization error. This is done to align
with the early stopping strategy mentioned in the Theorem 4.3.
G Details in real data experiments
In this subsection, we will provide the theoretical basis of the method which approximates the
smoothness of the goal function of real dataset. Let X ⊂ Rdbe a bounded domain. Given
a reproduce kernel k(·,·)onXand a probability measure µ. Denote the RKHS by Hk=nP
i∈Naiλ1
2
ieiP
i∈Na2
i<∞o
. We assume that there is a function f:X → Rand a proba-
bility density µonX. Suppose that the samples satisfies y=f(x), then fhas the decomposition:
f=X
i∈Nθiei. (111)
26The smoothness αf=α(f, k)depends on the coefficients ci: if we have θi≍i−dcandλi≍i−dλ,
then we derive the smoothness: αf=2dc−1
dλ.
We consider nsamples {(xi, yi)}n
i=1. The Gram matrix k(X, X )can be decomposed as
k(X, X ) =ϕΣϕT,and1
nϕϕT=In. (112)
In this regard, we can utilize the eigenvalue of the empirical kernel matrix to estimate the eigenvalue
of the kernel function, since previous work has shown the convergence of eigenvalue when nis large
enough [ 32]. Through the decomposition Y=ϕc(i.e.,c=1
nϕTY) and the approximation ci≈θi,
we can roughly estimate the eigenvalue decay rate when nis large enough with respect to i:
nX
k=ic2
k≍i−αfdλ. (113)
In our experiments, we let n= 3000 . Namely, an arbitrary selection of 3000 samples was made from
the each dataset. We did not use all samples in the datasets, because n= 3000 is already sufficient to
calculate the decay rate of eigenvalues. We consider the NTK of a one-hidden-layer fully connected
network as the kernel k. The results is shown in Figure 2, Figure 3 and Figure 4 for the three datasets,
respectively. In each figure, the scatter plot shows the log value of the summed squares of each ck
(fori≤k≤nas per equation (113) ) against log10ion the x-axis. Also, the dashed line represents
the corresponding least-square regression fitting using index ismaller than 2700 . Theoretically, the
slope of the dashed line will be −αfdλ.
Figure 2: Decay curve of the logarithm of sum of squared coefficients for NMIST.
Figure 3: Decay curve of the logarithm of sum of squared coefficients for Fashion-NMIST.
27Figure 4: Decay curve of the logarithm of sum of squared coefficients for CIFAR-10.
H Technical Lemmas
In this section, we introduce a series of technical lemmas. These will be helpful in our proof, and
many of the lemmas have been established by prior researchers.
Lemma H.1 (Change of measure [ 41] ).For a positive definite kernel kdefined on a compact set X,
it has the same eigenvalue decay rate under two measure νandσ:
λi(KNTK
0,X, ν)≍λi(KNTK
0,X, σ)
if the Radon derivative p=dν
dσexists and c≤p≤Cholds for some positive constant candC.
Lemma H.2 (Skorohod’s Representation Theorem) .Suppose that a sequence of probability distribu-
tion{Fn}converges weakly to FandFhas a separable support. Then there exist random variables
XnandX, defined on a new probability space (Ω′,F,P), such that the distribution of XnisFn, the
distribution of XisP, and Xn→Xholds almost surely.
Lemma H.3 (Restriction of RKHS [ 3]).Suppose Hkis a RKHS defined on Ewith the norm ∥·∥Hk,
thenk|Ωrestricted to a subset Ω⊂Eis the reproducing kernel of space {f′=f|Ω|, f∈ H k}with
norm defined by
∥f′∥= min
f|Ω=f′∥f∥Hk. (114)
The following proposition is a direct proposition of Lemma H.3:
Proposition H.4 (Equivalence of RKHS under resriction) .Assume RKHSs H1∼=H2are defined on
E. Write Ωas a subset of E. Then we have H1|Ω∼=H2|Ω.
The following two lemmas are common in real interpolation:
Lemma H.5 (Equivalence of interpolation spaces) .Suppose 0< s < 1. Denote L2=L2(X, µ)for
abbreviation. If we have RKHSs H1∼=H2, then (L2,H1)s,2∼=(L2,H2)s,2.
Proof. To prove the lemma, we only need to prove that the embedding (H1, L2)s,2,→(L2,H2)s,2
and(L2,H2)s,2,→(L2,H2)s,2are both bounded.
First we prove that(L2,H1)s,2,→(L2,H2)s,2≤C1where C1is an absolute positive constant.
For any x∈L2+H1, define the K-functional
K1(t;x) = inf
x0+x1=x;x0∈L2,x1∈H1(∥x0∥L2+t∥x1∥H1);
K2(t;x) = inf
x0+x1=x;x0∈L2,x1∈H2(∥x0∥L2+t∥x1∥H2).
SinceH1∼=H2, for any x∈ H 1, we have ∥x∥H2≤C∥x∥H1. Thus we have
K2(t;x)≤ inf
x0+x1=x;x0∈L2,x1∈H1(∥x0∥L2+Ct∥x1∥H1) =K1(Ct;x);
28Then we have
∥x∥(L2,H2)s,2=Z∞
0[t−sK2(t;x)]2dt
t
≤Z∞
0[t−sK1(Ct;x)]2dt
t
≤C2sZ∞
0[(Ct)sK1(Ct;x)]2d(Ct)
Ct
=C2s∥x∥(L2,H1)s,2(115)
LetC1=C2s, we have the canonical injection satisfies(L2,H1)s,2,→(L2,H2)s,2
op≤
C1. Also, since H1∼=H2, for any x∈ H 2, we have ∥x∥H1≤c∥x∥H2. We can prove(L2,H2)s,2,→(L2,H1)s,2
op≤C2in the same way. Then, we finish the proof.
Lemma H.6 (Equivalence of interpolation spaces) .Suppose 0< s < 1. Denote Hbe a RKHS
andµ, ν be measures on set X. If we have L2(X, µ)∼=L2(X, ν), then (L2(X, µ),H)s,2∼=
(L2(X, ν),H)s,2.
Proof. The proof in accomplished in the same way as Lemma H.5.
29NeurIPS Paper Checklist
1.Claims
Question: Do the main claims made in the abstract and introduction accurately reflect the
paper’s contributions and scope?
Answer: [Yes]
Justification: In the abstract and the introduction, we have summarized the background of
the NTK theory and the influence of initialization on neural networks under this background.
We have also discussed the results, which reflects the contributions of this paper.
Guidelines:
•The answer NA means that the abstract and introduction do not include the claims
made in the paper.
•The abstract and/or introduction should clearly state the claims made, including the
contributions made in the paper and important assumptions and limitations. A No or
NA answer to this question will not be perceived well by the reviewers.
•The claims made should match theoretical and experimental results, and reflect how
much the results can be expected to generalize to other settings.
•It is fine to include aspirational goals as motivation as long as it is clear that these goals
are not attained by the paper.
2.Limitations
Question: Does the paper discuss the limitations of the work performed by the authors?
Answer: [Yes]
Justification: Compared to the other sections, stronger assumptions were used in the lower
bound section (data distributed on the sphere). Although this is common in kernel regression,
we still reminded in the paper that this is a more strict assumption.
Guidelines:
•The answer NA means that the paper has no limitation while the answer No means that
the paper has limitations, but those are not discussed in the paper.
• The authors are encouraged to create a separate "Limitations" section in their paper.
•The paper should point out any strong assumptions and how robust the results are to
violations of these assumptions (e.g., independence assumptions, noiseless settings,
model well-specification, asymptotic approximations only holding locally). The authors
should reflect on how these assumptions might be violated in practice and what the
implications would be.
•The authors should reflect on the scope of the claims made, e.g., if the approach was
only tested on a few datasets or with a few runs. In general, empirical results often
depend on implicit assumptions, which should be articulated.
•The authors should reflect on the factors that influence the performance of the approach.
For example, a facial recognition algorithm may perform poorly when image resolution
is low or images are taken in low lighting. Or a speech-to-text system might not be
used reliably to provide closed captions for online lectures because it fails to handle
technical jargon.
•The authors should discuss the computational efficiency of the proposed algorithms
and how they scale with dataset size.
•If applicable, the authors should discuss possible limitations of their approach to
address problems of privacy and fairness.
•While the authors might fear that complete honesty about limitations might be used by
reviewers as grounds for rejection, a worse outcome might be that reviewers discover
limitations that aren’t acknowledged in the paper. The authors should use their best
judgment and recognize that individual actions in favor of transparency play an impor-
tant role in developing norms that preserve the integrity of the community. Reviewers
will be specifically instructed to not penalize honesty concerning limitations.
3.Theory Assumptions and Proofs
30Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?
Answer: [Yes]
Justification: We have provided assumptions in the main body of the paper, and have also
given comprehensive theoretical background information and proofs in the appendix.
Guidelines:
• The answer NA means that the paper does not include theoretical results.
•All the theorems, formulas, and proofs in the paper should be numbered and cross-
referenced.
•All assumptions should be clearly stated or referenced in the statement of any theorems.
•The proofs can either appear in the main paper or the supplemental material, but if
they appear in the supplemental material, the authors are encouraged to provide a short
proof sketch to provide intuition.
•Inversely, any informal proof provided in the core of the paper should be complemented
by formal proofs provided in appendix or supplemental material.
• Theorems and Lemmas that the proof relies upon should be properly referenced.
4.Experimental Result Reproducibility
Question: Does the paper fully disclose all the information needed to reproduce the main ex-
perimental results of the paper to the extent that it affects the main claims and/or conclusions
of the paper (regardless of whether the code and data are provided or not)?
Answer: [Yes]
Justification: In our experiments, our model is simple and uses public datasets. The
experimental results are easy to reproduce.
Guidelines:
• The answer NA means that the paper does not include experiments.
•If the paper includes experiments, a No answer to this question will not be perceived
well by the reviewers: Making the paper reproducible is important, regardless of
whether the code and data are provided or not.
•If the contribution is a dataset and/or model, the authors should describe the steps taken
to make their results reproducible or verifiable.
•Depending on the contribution, reproducibility can be accomplished in various ways.
For example, if the contribution is a novel architecture, describing the architecture fully
might suffice, or if the contribution is a specific model and empirical evaluation, it may
be necessary to either make it possible for others to replicate the model with the same
dataset, or provide access to the model. In general. releasing code and data is often
one good way to accomplish this, but reproducibility can also be provided via detailed
instructions for how to replicate the results, access to a hosted model (e.g., in the case
of a large language model), releasing of a model checkpoint, or other means that are
appropriate to the research performed.
•While NeurIPS does not require releasing code, the conference does require all submis-
sions to provide some reasonable avenue for reproducibility, which may depend on the
nature of the contribution. For example
(a)If the contribution is primarily a new algorithm, the paper should make it clear how
to reproduce that algorithm.
(b)If the contribution is primarily a new model architecture, the paper should describe
the architecture clearly and fully.
(c)If the contribution is a new model (e.g., a large language model), then there should
either be a way to access this model for reproducing the results or a way to reproduce
the model (e.g., with an open-source dataset or instructions for how to construct
the dataset).
(d)We recognize that reproducibility may be tricky in some cases, in which case
authors are welcome to describe the particular way they provide for reproducibility.
In the case of closed-source models, it may be that access to the model is limited in
some way (e.g., to registered users), but it should be possible for other researchers
to have some path to reproducing or verifying the results.
315.Open access to data and code
Question: Does the paper provide open access to the data and code, with sufficient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?
Answer: [Yes]
Justification: Yes, the datasets we utilized are publicly accessible for everyone. We have
comprehensively outlined the experimental parameters and model settings within the experi-
mental section of our paper. Notwithstanding the low complexity of our model, we are more
than willing to post our code on GitHub should there be a demand for it.
Guidelines:
• The answer NA means that paper does not include experiments requiring code.
•Please see the NeurIPS code and data submission guidelines ( https://nips.cc/
public/guides/CodeSubmissionPolicy ) for more details.
•While we encourage the release of code and data, we understand that this might not be
possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
including code, unless this is central to the contribution (e.g., for a new open-source
benchmark).
•The instructions should contain the exact command and environment needed to run to
reproduce the results. See the NeurIPS code and data submission guidelines ( https:
//nips.cc/public/guides/CodeSubmissionPolicy ) for more details.
•The authors should provide instructions on data access and preparation, including how
to access the raw data, preprocessed data, intermediate data, and generated data, etc.
•The authors should provide scripts to reproduce all experimental results for the new
proposed method and baselines. If only a subset of experiments are reproducible, they
should state which ones are omitted from the script and why.
•At submission time, to preserve anonymity, the authors should release anonymized
versions (if applicable).
•Providing as much information as possible in supplemental material (appended to the
paper) is recommended, but including URLs to data and code is permitted.
6.Experimental Setting/Details
Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?
Answer: [Yes]
Justification: The paper has indeed provided all necessary training and testing details for
understanding the results. We leveraged the several datasets which are publically accessible
to everyone. Furthermore, we have comprehensively outlined all the experimental parameters
and model settings.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The experimental setting should be presented in the core of the paper to a level of detail
that is necessary to appreciate the results and make sense of them.
•The full details can be provided either with the code, in appendix, or as supplemental
material.
7.Experiment Statistical Significance
Question: Does the paper report error bars suitably and correctly defined or other appropriate
information about the statistical significance of the experiments?
Answer: [Yes]
Justification: In the artificial data experiment, we can directly see the generalization error
decay rate in the figure. In the real data experiment, we do a least square regression to
calculate the smoothness of a function.
Guidelines:
32• The answer NA means that the paper does not include experiments.
•The authors should answer "Yes" if the results are accompanied by error bars, confi-
dence intervals, or statistical significance tests, at least for the experiments that support
the main claims of the paper.
•The factors of variability that the error bars are capturing should be clearly stated (for
example, train/test split, initialization, random drawing of some parameter, or overall
run with given experimental conditions).
•The method for calculating the error bars should be explained (closed form formula,
call to a library function, bootstrap, etc.)
• The assumptions made should be given (e.g., Normally distributed errors).
•It should be clear whether the error bar is the standard deviation or the standard error
of the mean.
•It is OK to report 1-sigma error bars, but one should state it. The authors should
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
of Normality of errors is not verified.
•For asymmetric distributions, the authors should be careful not to show in tables or
figures symmetric error bars that would yield results that are out of range (e.g. negative
error rates).
•If error bars are reported in tables or plots, The authors should explain in the text how
they were calculated and reference the corresponding figures or tables in the text.
8.Experiments Compute Resources
Question: For each experiment, does the paper provide sufficient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?
Answer: [Yes]
Justification: Yes, the computational resources needed are minimal, which means the
experiments can be easily conducted on nearly any GPU without any additional requirements.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The paper should indicate the type of compute workers CPU or GPU, internal cluster,
or cloud provider, including relevant memory and storage.
•The paper should provide the amount of compute required for each of the individual
experimental runs as well as estimate the total compute.
•The paper should disclose whether the full research project required more compute
than the experiments reported in the paper (e.g., preliminary or failed experiments that
didn’t make it into the paper).
9.Code Of Ethics
Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines ?
Answer: [Yes]
Justification: Yes, we conducted original theoretical research, which is in accordance with
the NeurIPS Code of Ethics.
Guidelines:
•The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
•If the authors answer No, they should explain the special circumstances that require a
deviation from the Code of Ethics.
•The authors should make sure to preserve anonymity (e.g., if there is a special consid-
eration due to laws or regulations in their jurisdiction).
10.Broader Impacts
Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?
Answer: [NA]
33Justification: Our work is mainly a theoratical work focusing on learning theory, so this
question is not applicable to our paper.
Guidelines:
• The answer NA means that there is no societal impact of the work performed.
•If the authors answer NA or No, they should explain why their work has no societal
impact or why the paper does not address societal impact.
•Examples of negative societal impacts include potential malicious or unintended uses
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
(e.g., deployment of technologies that could make decisions that unfairly impact specific
groups), privacy considerations, and security considerations.
•The conference expects that many papers will be foundational research and not tied
to particular applications, let alone deployments. However, if there is a direct path to
any negative applications, the authors should point it out. For example, it is legitimate
to point out that an improvement in the quality of generative models could be used to
generate deepfakes for disinformation. On the other hand, it is not needed to point out
that a generic algorithm for optimizing neural networks could enable people to train
models that generate Deepfakes faster.
•The authors should consider possible harms that could arise when the technology is
being used as intended and functioning correctly, harms that could arise when the
technology is being used as intended but gives incorrect results, and harms following
from (intentional or unintentional) misuse of the technology.
•If there are negative societal impacts, the authors could also discuss possible mitigation
strategies (e.g., gated release of models, providing defenses in addition to attacks,
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
feedback over time, improving the efficiency and accessibility of ML).
11.Safeguards
Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?
Answer: [NA]
Justification: Our work is mainly a theoratical work focusing on learning theory, so this
paper poses no such risks.
Guidelines:
• The answer NA means that the paper poses no such risks.
•Released models that have a high risk for misuse or dual-use should be released with
necessary safeguards to allow for controlled use of the model, for example by requiring
that users adhere to usage guidelines or restrictions to access the model or implementing
safety filters.
•Datasets that have been scraped from the Internet could pose safety risks. The authors
should describe how they avoided releasing unsafe images.
•We recognize that providing effective safeguards is challenging, and many papers do
not require this, but we encourage authors to take this into account and make a best
faith effort.
12.Licenses for existing assets
Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?
Answer: [Yes]
Justification: We have explicitly referenced the datasets in our paper, as MNIST, CIFAR-10
and Fashion-MNIST.
Guidelines:
• The answer NA means that the paper does not use existing assets.
• The authors should cite the original paper that produced the code package or dataset.
34•The authors should state which version of the asset is used and, if possible, include a
URL.
• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
•For scraped data from a particular source (e.g., website), the copyright and terms of
service of that source should be provided.
•If assets are released, the license, copyright information, and terms of use in the
package should be provided. For popular datasets, paperswithcode.com/datasets
has curated licenses for some datasets. Their licensing guide can help determine the
license of a dataset.
•For existing datasets that are re-packaged, both the original license and the license of
the derived asset (if it has changed) should be provided.
•If this information is not available online, the authors are encouraged to reach out to
the asset’s creators.
13.New Assets
Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?
Answer: [NA]
Justification: We did not utilize new assets in our work.
Guidelines:
• The answer NA means that the paper does not release new assets.
•Researchers should communicate the details of the dataset/code/model as part of their
submissions via structured templates. This includes details about training, license,
limitations, etc.
•The paper should discuss whether and how consent was obtained from people whose
asset is used.
•At submission time, remember to anonymize your assets (if applicable). You can either
create an anonymized URL or include an anonymized zip file.
14.Crowdsourcing and Research with Human Subjects
Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as
well as details about compensation (if any)?
Answer: [NA]
Justification: Our work does not involve crowdsourcing experiments or research related to
human subjects, therefore this question is not applicable.
Guidelines:
•The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
•Including this information in the supplemental material is fine, but if the main contribu-
tion of the paper involves human subjects, then as much detail as possible should be
included in the main paper.
•According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
or other labor should be paid at least the minimum wage in the country of the data
collector.
15.Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects
Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?
Answer: [NA]
Justification: Our work does not involve any study participants, so there is no need to discuss
potential risks they might face. Therefore, there is also no need for an Institutional Review
Board (IRB) approval. Hence, this question is not applicable to our paper.
35Guidelines:
•The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
•Depending on the country in which research is conducted, IRB approval (or equivalent)
may be required for any human subjects research. If you obtained IRB approval, you
should clearly state this in the paper.
•We recognize that the procedures for this may vary significantly between institutions
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
guidelines for their institution.
•For initial submissions, do not include any information that would break anonymity (if
applicable), such as the institution conducting the review.
36