Under review as submission to TMLR
Oﬄine Reinforcement Learning for Traﬃc Signal Control
Anonymous authors
Paper under double-blind review
Abstract
Traﬃc signal control is an important problem in urban mobility with a signiﬁcant potential
of economic and environmental impact. While there is a growing interest in Reinforcement
Learning (RL) for traﬃc signal control, the work so far has focussed on learning through
simulations which could lead to inaccuracies due to simplifying assumptions. Instead, real
experience data on traﬃc is available and could be exploited at minimal costs. Recent
progress in oﬄineorbatchRL has enabled just that. Model-based oﬄine RL methods, in
particular, have been shown to generalize from the experience data much better than others.
We build a model-based learning framework which infers a Markov Decision Process (MDP)
from a dataset collected using a cyclic traﬃc signal control policy that is both commonplace
and easy to gather. The MDP is built with pessimistic costs to manage out-of-distribution
scenarios using an adaptive shaping of rewards which is shown to provide better regularization
compared to the prior related work in addition to being PAC-optimal. Our model is evaluated
on a complex signalized roundabout showing that it is possible to build highly performant
traﬃc control policies in a data eﬃcient manner.
1 Introduction
Road traﬃc signal control has attracted substantial interest as an application of reinforcement learning
(RL) (Wei et al., 2019; Yau et al., 2017). However most published work in the area is unlikely to be applied
in practice as trial and error methods for interacting with the environment are not feasible in the real
world. Similarly trying to infer an RL policy using a simulator does not take advantage of the fact that real
experience data about traﬃc is available from transportation management operators.
A more appropriate and realistic set up is to use oﬄine RL training to learn from static experience data (Lange
et al., 2012). A typical data set will consist of a set of tuples of the form {si,ai,ri,si+1},i.e., when the
system was in state si, actionaiwas taken, which resulted in a reward riand the system then transitioned
into a new state si+1. From the experience data the objective is to learn a policy, i.e., a mapping from state
to action which maximizes the long term expected cumulative reward. In a traﬃc signal control setting, the
state captures the distribution of traﬃc on the road network, the action space consists of diﬀerent phases
(red, green, amber) on signalized intersections and the reward is a metric of traﬃc eﬃciency.
Compared to online RL approaches (Sutton & Barto, 2018), oﬄine (or batch) RL shifts the focus of learning
from data exploration to data-driven policy building. The oﬄine policy building is challenging due to
deviation in the state-action visitation by the policy being learned and the policy that logged the static
dataset (Fujimoto et al., 2019b; Levine et al., 2020). A number of diﬀerent solution frameworks are proposed
for oﬄine RL that are either model-free (Fujimoto & Gu, 2021; Kumar et al., 2020; Wu et al., 2019; Kostrikov
et al., 2021b;a) or derive a Markov Decision Process (MDP) model (Shrestha et al., 2020; Yu et al., 2020;
Kidambi et al., 2020; Yu et al., 2021) from the data set. We focus on model-based RL approaches which have
been shown to oﬀer better regularization in presence of uncertain data. Such approaches are characterized by
a mechanism to penalize under-explored or under-represented transitions, a notion referred to as pessimism
under uncertainty (Fujimoto et al., 2019b). While the RL algorithms, by nature, are highly sensitive to hard-
to-tune hyper-parameters (Ng, 2022), incorporation of the uncertainty penalties adds further supplementary
hyper-parameters. These are shown to aﬀect the performance of oﬄine learning signiﬁcantly (Lu et al.,
1Under review as submission to TMLR
Figure 1: A-DAC, our model-based oﬄine RL approach for traﬃc control achieves the best performance
out-of-the-box compared to three model-free (DQN (Mnih et al., 2016), BCQ (Fujimoto et al., 2019b), and
TD3+BC (Fujimoto & Gu, 2021)) , one model-based RL (MBRL (Yu et al., 2020)), and our predecessor
(DAC (Shrestha et al., 2020)) RL, predecessor of A-DAC. The dataset is collected by a behavioral policy
that cycles through each traﬃc signal change action. DQN is greedy and prone to exploration errors.
BCQ, TD3+BC and MBRL fail to generalize despite their built-in pessimism. DAC is sensitive to its
hyperparameters and needs multiple online evaluations. An Online RL baseline is included for comparison
which is matched or bettered by our approach in a fraction of time.
2021). Since interaction to the environment is not permitted in the oﬄine setting, adjustments to the
hyper-parameters are not trivial.
We build on the DAC framework (Shrestha et al., 2020), which derives a ﬁnite Markov Decision Process
(MDP) (Puterman, 2014) from a static dataset and solves it using an optimal planner. The MDP derivation
uses empirical averages to interpolate contributions from nearby transitions seen in the dataset with pessimistic
penalties applied to derivation of the rewards based on the coverage of the neighborhood. DAC, in a manner
similar to the model-based oﬄine RL algorithms discussed previously, is sensitive to its hyper-parameters
relating to the pessimistic costs. We incorporate an adaptive pessimistic reward shaping in DAC which
makes it both robust to dataset properties and signiﬁcantly faster to train by eliminating online interactions
required for tuning magnitude of conservatism. Our adaptive reward penalty formulation relies on local
Lipschitz smoothness (O’Searcoid, 2006) assumption on the Q-values exhibited by the true (but unknown)
MDP, a much weaker assumption compared to the DAC-MDP framework. We provide the same theoretical
guarantees on optimality while exhibiting a superior empirical performance.
Figure 1 illustrates our contribution, dubbed Adaptive(A)-DAC, evaluated on a real traﬃc signal control
setup. A-DAC ﬁnds a policy signiﬁcantly better than a common behavioral (or data collection) policy in a
small fraction of time compared to online learning and does so out-of-the-box. A key insight of our approach
is that data collected from cyclic policies that are oblivious to rate of traﬃc arrival and is often the norm in
many traﬃc signal scenarios, can be leveraged to infer superior policies which improve overall traﬃc eﬃciency.
Contributions:
•We formulate traﬃc signal control as an oﬄine RL problem. While RL has recently been proposed
for oﬄine optimization, to the best of our knowledge, it has not been used for the traﬃc signal
control before.
•We extend a recent model-based oﬄine RL framework, DAC, to our problem and improve it by
employing an adaptive reward penalty mechanism that enables the best trade-oﬀs in the performance
2Under review as submission to TMLR
Figure 2: A simple signalized intersection with
two traﬃc ﬂows: North-South ( NS) and West-
East (EW).Table 1: A small experience dataset collected
using Cyclictraﬃc signal control policy applied
on the intersection in Figure 2.
StatesActionaStates/primeRewardr(|NS|,|EW|)
(1,5) EW (3,3) 2
(3,3) NS (1,5) 2
(6,1) NS (2,3) 4
(2,3) EW (6,1) 2
(0,5) EW (2,3) 2
(2,3) NS (0,5) 2
Table 2: Derived rewards for the core states identiﬁed from the experience dataset in Table 1.
StatesAveragers ( C= 0)DAC @C= 1 DAC @C= 2 A-DAC
(|NS|,|EW|)˜R(s,NS)˜R(s,EW)˜R(s,NS)˜R(s,EW)˜R(s,NS)˜R(s,EW)˜R(s,NS)˜R(s,EW)
(2,3) 2.67 2 2.41 1.77 1.85 1.25 1.58 1.53
(6,1) 2.67 2 2.29 1.16 1.46 -0.7 1.17 0.32
(3,3) 2.67 2 2.45 1.66 1.98 0.89 1.82 1.31
(1,5) 2.67 2 2.14 1.85 0.96 1.52 0.55 1.70
(0,5) 2.67 2 2.03 1.82 0.63 1.43 0.14 1.65
and the policy building overheads. We provide Probably-Approximately-Correct (PAC) guarantees
for A-DAC under more relaxed and realistic assumptions on the Q-function.
•We propose a methodology for data collection at a traﬃc intersection using macro statistics provided
by traﬃc authorities. We evaluate our approach on a complex signalized roundabout where traﬃc is
coordinated using eleven phases.
Outline: The rest of the paper is structured as follows. In Section 2, we introduce a small idealized traﬃc
control problem as a working example. A primer on oﬄine RL methods and their applicability to traﬃc
control is provided in Section 3. In Section 4, our proposed approach, A-DAC, is introduced and elaborated
upon. Section 5 reasons about suitability of our approach to the traﬃc control problem. Section 6, then,
evaluates A-DAC and other baselines using a complex signalized roundabout. The most relevant related
work is presented in Section 7 and we conclude in Section 8 with a summary of the work.
2 Basic Traﬃc Signal Control
We start with a simple scenario of traﬃc signal control ﬁrst presented in Rizzo et al. (2019) where an optimal
model for traﬃc light phase duration is derived based on simplistic assumptions. When these assumptions are
relaxed in a realistic setting, more complex machinery is required for optimal control which we present later.
Consider a traﬃc signal at an intersection which controls traﬃc in only two directions: either from north to
south (NS) or from west to east ( EW) (See Figure 2). Suppose the traﬃc follows a Poisson process with
the rate of traﬃc arrival being λ1andλ2respectively for the two traﬃc ﬂows. The traﬃc starts arriving
from timet= 0and the total cycle time is T. The number of vehicles entering any incoming traﬃc arms is
uniformly distributed by deﬁnition of the Poisson process and the expected number at time tis given by λt.
The optimal setting for the duration of the green phase for NScan be derived by minimizing the average
delay faced by a vehicle. It evaluates to:λ1
λ1+λ2T. The equation states that the green phase duration should
be proportional to the arrival rate of the vehicles. It can be easily generalized to a case of nexclusive traﬃc
ﬂows where the optimal green phase duration for an edge iisλi/summationtextn
i=0λiT.
3Under review as submission to TMLR
We relax the assumption of the known arrival rate. Instead, we assume an experience dataset of vehicle
movement at the intersection exists where the signal is controlled by a cyclic policy. The Cyclicpolicy simply
alternates green phase between the NSﬂow and the EWﬂow at every time step, mimicking a commonly
observed scenario. The idea is to use the dataset to devise a smarter control policy. We study some of
the recent developments in oﬄine RL towards this problem scenario before introducing a real signalized
roundabout we target. The techniques we develop are general enough to apply directly to the more complex
test environment.
3 Oﬄine Reinforcement Learning
In Reinforcement Learning (RL) (Sutton & Barto, 2018), an agent interacts with an environment, assumed
to be a Markov Decision Process (MDP) (Puterman, 2014), in order to learn an optimal control (action
selection) policy. The MDP is given by a tuple (S,A,P,R,γ ), with a state space S, an action space A,
transition dynamics P(s,a,s/prime), a stochastic reward function R(s,a)and a discount factor γ∈[0,1). The
agent aims to learn a policy function π:S→Awhich maximizes the expected sum of discounted rewards.
Formally, the objective of RL is given by the following.
max
πE
at∼π(.|st)
st+1∼P(.|st,at)/bracketleftBig∞/summationdisplay
t=0γtR(st,at)/bracketrightBig
(1)
The policy πhas a Q-function Qπ(s,a)giving the expected inﬁnite-horizon discounted reward starting with
state-action pair (s,a). The optimal policy π∗maximizes the Q-function over all policies and state-action
pairs. The maximum Q-values are computed by repeated application of the Bellman backup (Bellman, 1966)
operatorBstated below.
B[Q](s,a) =R(s,a) +γE
s/prime∼P(.|s,a)/bracketleftbig
max
aQ(s/prime,a)/bracketrightbig
(2)
RL strives to discover a near-optimal policy by exploring actions in the environment. In an oﬄine or batch
setting (Levine et al., 2020; Ernst et al., 2005), the environment is replaced by a static dataset collected
apriori. The dataset Dis made up of tuples {(si,ai,ri,s/prime
i)}where each tuple takes action aifrom statesito
transition to state s/prime
iwhile giving the reward ri. The dataset is collected from multiple episodes/trajectories
of the form (s1,a1,r1,s2,...,sH)whereHis the trajectory length.
Example: Table 1 presents a sample experience data set collected by a policy cycling between the actions NS
and EW. The state is given by a vector of the number of vehicles arriving from each incoming lane. The
data set includes three trajectories, each contributing two tuples. Duplicates are removed to make the example
concise.
In the basic traﬃc control setup outlined in Section 2, an action corresponds to activating green phase for
one of the traﬃc ﬂows for a ﬁxed time duration, called an observation period . Therefore, ai∈{NS,EW}.
Observation state is given by a vector of the number of vehicles arriving from each incoming traﬃc arm,
making it 2-dimensional in our case. Reward is a non-negative integer denoting the number of vehicles that
cross the signal during the observation period.
3.1 Model-free Learning
The ﬁrst solution approach we consider is to adapt a popular oﬀ-policy Q-learning approach Deep Q Network
(DQN) (Mnih et al., 2015) to the oﬄine setting. The oﬄine setting often causes extrapolation errors in
Q-learning which result from mismatch between the dataset and the state-action visitation of the policy under
training. Fujimoto et al. (2019b) proposes a batch-constrained Q learning (BCQ) approach to minimize
distance between the selected actions and the dataset. BCQ trains a state-conditioned generative model of
the experience dataset. In discrete settings, the model Gωgives a good estimate of the behavioral policy πb
used to collect data. That means, we can constrain the selected actions to data using a threshold τ∈[0,1):
π(a|s) = arg max
a/prime|Gω(a/prime|s)/max ˆaGω(ˆa|s)>τQ(s,a/prime) (3)
4Under review as submission to TMLR
While the BCQ is eﬀective at pruning the under-explored transitions, its beneﬁts are limited when the
behavioral policy tends to a uniform distribution which holds true in our case: The behavior policy πbwe
aim to utilize is Cyclicfor whichπb(a|s) =πb(a) =1
|A|.
In another oﬄine RL approach, a minimalistic change to classic TD3 algorithm (Fujimoto et al., 2018) is
proposed in (Fujimoto & Gu, 2021) which regularizes the TD3 policy with a behavioral cloning (BC) (Osa
et al., 2018; Pomerleau, 1988) term:
π←arg max
πE
(s,a)∼D/bracketleftBig
λ Q/parenleftbig
s,π(s)/parenrightbig
−/parenleftbig
π(s)−a/parenrightbig2/bracketrightBig
(4)
Here,λis a normalizing scalar that is set to a value inverse of the average critic ( Q) function value. This
approach is termed TD3+BC and is regarded as the current state-of-the-art model-free oﬄine RL algorithm.
3.2 Derived MDP-based Learning
AcontrastiveapproachtoconstrainingtheRLtothedatasetistoderiveanMDPfromthedataandeithersolve
it optimally or use model-based policy optimization (MBPO) (Janner et al., 2019). This approach provides
better generalization since each transition gets more supervision compared to the oﬀ-policy approaches.
There exist multiple recent approaches built on this principle called Model-based (MB-)RL (Sutton, 1991).
MBRL primarily learns an approximate transition model and (optionally) a reward model by supervising
data followed by a phase of uncertainty quantiﬁcation to deal with out-of-distribution visitations (Yu et al.,
2020; Kidambi et al., 2020; Yu et al., 2021). We employ a simple instantiation, called DAC-MDP (Shrestha
et al., 2020), which is based on Averagers’ framework (Gordon, 1995). The idea is to learn transitions and
rewards as empirical averages over the nearest neighbors in the state-action space. It lends to a natural
approximation and enables an intuitive uncertainty quantiﬁer. Furthermore, DAC-MDP creates a ﬁnitely-
representated MDP by working on a set of corestates which can be reached from any non-core state using
a one-step transition and do not allow transition to a non-core state from within. The ﬁnite structure of
the MDP implies that an accurate solution is feasible (such as by using a value iteration solver) despite an
inﬁnite continuous state space. We contribute an adaptive reward penalty mechanism to the DAC framework
which works as an eﬀective uncertainty quantiﬁer. Before outlining our contributions, we formalize the DAC
framework.
Assumption 3.1. We assume a continuous state space Sand a ﬁnite action space A. We are given a nearest
neighbor function NN(s,a,k,α )that ﬁnds at most knearest neighbors to a state-action pair (s,a)with an
optional maximum distance threshold α.NNuses a metric function d, such as Euclidean (Toth et al., 2017),
that keeps the pairs with diﬀerent actions inﬁnitely distant while the distance between the pairs with the same
action is evaluated on the state metric space: For example, d(si,ai,sj,aj) =||si−sj||ifai=aj,∞otherwise .
We use shorthand dijfor distance between pairs (si,ai)and(sj,aj). Notation d/prime
ijindicates a normalized
version of distance dij. Given a smoothness parameter kand a distance threshold α, the derived MDP ˜Mis
deﬁned below.
Deﬁnition 3.2. MDP ˜M= (S,A,˜R,˜P,γ)shares the state space and the action space of the underlying
MDPM. For a state-action pair (s,a), letkNN =NN(s,a,k,α )be its nearest neighbors from D. The
reward and transition functions are then deﬁned as:
˜R(s,a) =1
k/summationdisplay
i∈kNNri,˜P(s,a,s/prime) =1
k/summationdisplay
i∈kNNI[s/prime=s/prime
i]
DAC modiﬁes the reward function to penalize transitions to an under-explored region with an additive penalty
parameterized by a cost parameter C≥0:
˜R(s,a) =1
k/summationdisplay
i∈kNN/parenleftbig
ri−C∗d(s,a,si,ai)/parenrightbig
(5)
5Under review as submission to TMLR
It should be noted that while the reward shaping in model-based online RL acts as a way to incorporate
additional incentive based on domain knowledge to an otherwise sparse reward function (Ng et al., 1999), the
oﬄine setting uses it as a means to incorporate pessimism to the MDP.
Example: Table 2 compares the rewards derived with diﬀerent cost penalties. k= 3throughout and the
distances are normalized to the maximum distance for ease of presentation. As an example, ˜RC=0((2,3),NS) =
1
3(r[1] +r[2] +r[5]) =1
3(4 + 2 + 2) = 2 .67, wherer[.]is indexes Table 1. State (2,3), for which the reward
rfrom dataset is equal for both actions, is assigned a higher reward for action NS due to inﬂuence from a
high-reward neighbor (6,1).
The DAC framework builds a ﬁnite MDP by focusing on a set of corestates that are extracted from the data
setDasSD={s/prime
i|(si,ai,ri,s/prime
i)}. As stated previously, states within SDdo not transition to non-core states.
The MDP built over the core states is solved using any standard tabular solver such as value iteration to
compute values ˜Vfor the core states. We can then compute ˜Qfor a non-core state using the following 1-step
lookup which is used to transition from the non-core state to one of the core states:
˜Q(s,a) =˜R(s,a) +γ/summationdisplay
s/prime∈˜P(s,a)˜P(s,a,s/prime).˜V(s/prime) (6)
Example: Consider a new state (1,4). Its Q-values on MDP with no cost penalties and γ= 0.99evaluate to
˜QC=0((1,4),NS) = 7.92and ˜QC=0((1,4),EW) = 7.25. The policy ˜πwould, therefore, choose action NS which
is suboptimal since the state indicates more traﬃc on the EW lane. Cost penalties help us in this instance:
The MDP derived with C= 2gives ˜QC=2((1,4),NS) = 32.6and ˜QC=2((1,4),EW) = 32.9. The setting
C= 2is not arbitrary: it is the lowest Cvalue for which the EW action gets chosen. We discuss this choice
further in the next section.
4 A-DAC MDP Derivation
We have just shown through example that building oﬄine solutions for traﬃc control is not trivial even in
the simplest of the scenarios. We build an adaptive approach for reward shaping that retains the optimality
guarantees while improving the eﬃciency of the DAC framework signiﬁcantly; the resulting framework is
called as A-DAC.
Deﬁnition 4.1. A-DAC automatically adjusts the penalties built into the reward function of the derived
MDP ˜Min the following manner:
GivenkNN =NN(s,a,k,α )andrmax= max
i∈kNNri
˜R(s,a) =1
k/summationdisplay
i∈kNNri−rmax∗d/prime(s,a,si,ai)
It should be noted that d/primeis a normalized version of dwhich brings the penalty term to the units of rewards.1
The intuition behind using the max reward in the neighborhood is to penalize the under-explored but highly
rewarding transitions more heavily. Let’s understand with an example.
Example: We saw earlier that for state (2,3)from Table 2, action NS brings a higher reward in DAC. This
is due to inﬂuence from a high-reward-getter neighbor (6,1). It can be noticed from the same table that
A-DAC’s adaptive rewards make both the actions equally rewarding. Moreover, Q-values for a non-core
state (1,4)evaluate to ˜QA-DAC ((1,4),‘NS’) = 31.4and ˜QA-DAC ((1,4),‘EW’ ) = 32.5selecting the action EW
automatically.
We present a canonical use case in Figure 3 to illustrate how the rewards are shaped in A-DAC. Of thek
neighbors considered, one neighbor is kept ﬂoating to simulate diﬀerent types of neighborhood. e.g. rmax= 1
1For practicality, we use max-normalization: d/prime=d/dmax, where dmaxis the diameter of the point space and is approximately
calculated by sampling a few points from data set Dand aggregating distance from these points to all points in D.
6Under review as submission to TMLR
Figure 3: A conﬁguration of k-nearest neighbors for state (s,a)wherek−1neighbors are at distance 1 each
with reward 1. The remaining neighbor ﬂoats at distance rmaxbringing in reward rmaxwherermax>1.
Figure 4: Comparison of rewards derived by DAC with diﬀerent settings of cost Cto those obtained by
A-DAC using the conﬁguration in Figure 3 controlled by variable rmax.A-DAC penalizes the conﬁgurations
with under-explored regions more while keeping the rewards high for homogeneous conﬁgurations.
gives the most homogeneous conﬁguration, while a high rmaxreplicates an under-represented region. Figure 4
shows how a global cost parameter Cwould shape the reward for (s,a). While a low Cgives high rewards
for an under-represented region, a high Cis detrimental to the homogeneous conﬁgurations. A-DAC can be
seen to oﬀer a good balance.
Optimality. The policy learned by solving the A-DAC ˜M, denoted ˜π, can potentially be arbitrarily
sub-optimal in true MDP M. We obtain a lower bound on the values obtained by policy V˜πin relation to
the valuesV∗provided by the optimal policy π∗inMunder a “smoothness” assumption.
Assumption 4.2. A-DAC assumes local Lipschitz continuity (O’Searcoid, 2006) for Q-function Q: For a
state-action pair (si,ai)and another pair (sj,aj)in its neighborhood, i.e.,d/prime
ij<αforα∈[0,1], there exists
a local constant LQ(i,α)≥0such that|Q(si,ai)−Q(sj,aj)|≤LQ(i,α)d/prime
ij.
The local continuity is a much weaker assumption compared to the global smoothness assumed in the DAC
framework. Further, it is found to be practical based on an analysis of our traﬃc control setup presented in
Section 5. In addition, we assume that the rewards are bounded to [0,Rmax]which holds for most practical
applications including ours. ˜π, then, provides the following PAC guarantee.
Theorem 4.3. Given a static dataset Dwith its sample complexity indicated by covering number (see
Deﬁnition A.2)NSA(α)and an A-DACMDP ˜Mbuilt onDwith parameters kandα, if˜Q2
max
/epsilon12sln/parenleftBig
2NSA(α)
δ/parenrightBig
≤
k≤2NSA(α)
δ, then
V˜π≥V∗−2/epsilon1s+¯dmaxRmax
1−γ,w.p. 1−δ
7Under review as submission to TMLR
Figure 5: Two experience trajectories from the traﬃc intersection in Figure 2 taken using Cyclicbehavior
policy under a ﬁxed traﬃc load assumption.
Figure 6: Policy learned by A-DAC using the dataset in Figure 5 improves the behavior policy by 33%.
The proof is presented in Appendix A.The ﬁrst component /epsilon1sdenotes the maximum sampling error caused
by using a ﬁnite number of neighbors; it helps us set a lower bound on k. The second component denotes the
estimation error due to neighbors being at non-zero distance: Quantity ¯dmaxgives the worst case average
distance which is upper bounded by αand can be lowered by augmenting the data set Dwith more diverse
data.
5 A-DAC’s Feasibility to Traﬃc Control
We use the simple two action single intersection model in Figure 2 to analyze A-DAC’s feasibility to traﬃc
control.
We assume a ﬁxed rate of arrival for each of the two incoming lanes where λEW= 3∗λNS. The starting state
is assumed to be (NS,EW ) = (1,3). A maximum capacity ( Rmax) of 4 is allowed in order to maintain a
steady ﬂow. On this setup, two experience trajectories are collected using Cycliccontrol policy as illustrated
in Figure 5. It can be easily seen that this policy moves 3Tvehicles inTtimesteps. Next, A-DAC is trained
on this toy dataset. Figure 6 shows a rollout with the same starting state. It can be noticed that a cumulative
reward of 4Tis achieved, giving 33% improvement over the behavioral policy (and matching the optimal
policy). It shows that A-DAC generalizes well. Inspect state (2,5) as an instance: The only action observed
from it in the dataset is NS, but our policy has learned to take action EWinstead.
Finally, using the same model, we can show that for the Q-function, local Lipschitz continuity is the right
condition to enforce. For example, suppose the arrival rate on each lane is λ1andλ2respectively. Assume,
we start the cycle from the λ1lane, then the optimal return is:
J(π∗) =λ2
1
λ1+λ2+γ∗λ2
2
λ1+λ2+γ2∗λ2
1
λ1+λ2+...
=1
1−γ2∗λ2
1
λ1+λ2+γ
1−γ2∗λ2
2
λ1+λ2
Now if we assume that the λ1>0andλ2>0andλ1+λ1≥δ, the above expression is upper bounded by a
convex function c∗(λ2
1+λ2
1)for somec>0which is locally Lipschitz (as a function of λ1andλ2).
8Under review as submission to TMLR
Figure 7: A signalized roundabout A-DAC optimizes. 68 loop detector devices installed in and around the
junction report the traﬃc they observe which forms the state.
6 Evaluation
This section addresses the following questions:
1. How well is the data collected from simple cyclic behavioral policy exploited by oﬄine learners? (§6.3)
2. Does a partially trained behavior policy oﬀer any added beneﬁts to oﬄine learning? (§6.4)
3. How do diﬀerent latent state representations used in A-DAC compare? (§6.5)
4. How does A-DAC’s eﬃciency and hyperparameter sensitivity compare to prior work? (§6.6)
Before delving into these questions, we describe a real complex traﬃc roundabout environment used in
evaluation (§6.1) and the details of our experimental setup (§6.2).
6.1 Environment for a Signalized Roundabout
We model a signalized roundabout shown in Figure 7. It is a complex intersection containing multiple lanes in
each traﬃc arm and 10 traﬃc signals controlling the area. We model the state of traﬃc as a 68-dimensional
vector, each dimension providing the number of vehicles seen at a designated location. An action corresponds
to a phase of traﬃc that covers a certain conﬁguration of the traﬃc signals. Details on engineering the states,
actions, and rewards for this intersection are provided in Appendix B. The appendix also provides details on
creating an experience batch from a micro-simulator bootstrapped with macro traﬃc statistics when access
to the real experience trajectories data is limited.
6.2 Experimental Setup
We use the signalized roundabout detailed in Section 6.1 for evaluation. Each episode lasts an hour with 360
time steps. O-D matrix data is available for each hour of a typical weekday which allows us to create a single
day batch. A random noise is added to the matrix when creating a larger batch. Two batches are used: (a) 1
day batch giving≈10k timesteps, and (b) 1 week batch giving≈70k timesteps.
Behavioral policies. We study two data collection policies:
1.Cyclic: Cycles through all actions. Represents a scenario commonly found across traﬃc intersections.
2.Partial-RL : An RL policy is partially trained via online interactions. A noisy version of the policy is then
used to control data. This policy has been shown to be suitable for oﬄine learning previously (Fujimoto
et al., 2019a).
Batch collection and evaluation is carried out using SUMO microsimulator (Lopez et al., 2018). A ﬁxed ﬁve
hour workload, that corresponds to ﬁve RL episodes, is used for evaluation throughout: it includes two hours
9Under review as submission to TMLR
1 day batch 1 week batch300400500Avg returns/ hourOﬄine RL algorithms on Cyclicbatch
Behavioral
Online RL
BCQ
DQN
TD3+BC
MOReL
MOPO
DAC
A-DAC (ours)
Figure 8: Comparison of oﬄine RL algorithms on Cyclicbatch. Error bars indicate the min-max interval
obtained after 5 runs with diﬀerent seeds under the best model settings. BCQ, TD3+BC, MOReL, and
MOPO fail to improve the behavioral policy. Both DAC and A-DAC improve the policy signiﬁcantly, even
matching the Online RL performance with the larger batch, While DAC needs a large evaluation overhead,
A-DAC works out-of-the-box.
1 day batch 1 week batch400450500Avg returns/ hourOﬄine RL algorithms on Partial-RL batch
Behavioral
Online RL
BCQ
DQN
TD3+BC
MOReL
MOPO
DAC
A-DAC (ours)
Figure 9: Comparison of oﬄine RL algorithms on Partial-RL batch. Error bars indicate the min-max interval
obtained after 5 runs with diﬀerent seeds under the best model settings. Failures of DQN, MOReL, and
MOPO exemplify the deadly triad issue in RL. Both DAC and A-DAC, on account of a more principled
approximation of the dynamics manage to improve the behavior policy, the magnitude being higher on the
larger batch. TD3+BC provides the best improvement where data from the partially trained policy aids with
early identiﬁcation of good actions on account of BC regularization.
of light traﬃc, one hour of medium traﬃc, and two hours of peak traﬃc. Each single hour episode measures
the cumulative discounted rewards. The average return across the ﬁve hours is reported as our performance
measure.
MDP derivation in DAC requires a nearest neighbor index: We use a memory-mapped fast approximate
nearest neighbor search index (ann). Distances are max-normalized by diameter of the state space before
deriving MDP. We use a sampling-based fast approximate algorithm to estimate the diameter. Derived MDP
is solved optimally with a value iteration algorithm (mdp) which can exploit sparseness in transition matrix
for eﬃciency gains.
Baselines. We compare A-DAC against recent approaches to oﬄine learning. Firstly, we use three model-
free oﬄine RL baselines, namely, DQN, BCQ, and TD3+BC. Next, we use MOReL and MOPO as two
representatives of the general MBPO (Janner et al., 2019) approach that covers both classical (naïve) MBRL
and Pessimistic MDP-based MBRL. We ﬁrst tune the parameters controlling pessimistic costs in each of
10Under review as submission to TMLR
the algorithms and report the results for the best setting.2Finally, we analyze the DAC-MDP framework
which, too, is tuned for cost parameter Cusing a grid search as suggested in Shrestha et al. (2020). Some of
the baselines we use here are originally developed for continuous action spaces; We describe the necessary
modiﬁcations for our discrete action setup in Appendix C. An online RL policy is also included in the
evaluation which uses an oﬀ-policy DQN fully trained for close to 100ktimesteps.
6.3 Cyclic Policy Batch
Figure 8 compares the algorithms on the Cyclicdataset. Approaches that employ pessimism, viz., BCQ and
both MBRL algorithms, fail to improve the behavioral policy as the Cyclicpolicy does not oﬀer much insight
that the deep function approximators (employed either to learn the policy or the Q-values) can easily exploit.
DAC-based policies provide considerable performance improvements due to the nearest neighbor-based
dynamics approximation specialized for ﬁnite spaces like ours. For DAC, we only report results from the
best hyper-parameter setting found after 6 online evaluations. As seen in Figure 1, the performance is highly
sensitive to these settings. A-DAC, on the other hand, does not need extra online evaluations and either
matches or equals the performance of the best DAC policy.
6.4 Partial RL Policy Batch
Figure 9 compares the oﬄine RL algorithms on the Partial-RL dataset. BCQ manages to exploit this batch
better with its constrained Q-value approximation. Surprisingly, the pessimistic MBRL approaches fail to
even match the behavioral policy performance. Along with DQN, these suﬀer from the issue of “the deadly
triad of deep RL” (Sutton & Barto, 2018) which exempliﬁes the inherent diﬃculties in planning with learned
deep-network models. The incorporation of behavioral cloning into TD3 policy updates in TD3+BC makes
the good actions stand out early during the training and, as an eﬀect, it works much better here than on
Cyclicdata set. DAC framework simpliﬁes the dynamics derivation process thus signiﬁcantly reducing the
reliance on the learned models. Between DAC and A-DAC,A-DAC, once again, matches or improves the
best DAC policy performance without any hyper-parameter tuning.
6.5 State Representations in A-DAC
Results for A-DAC (and for DAC) presented so far, did not discuss the latent state representations used by
the model. We analyze the following three state representations:
1. Loop Counts : 68 dimensional native representation, corresponding to counts from the loop detector
devices.)
2. BCQ : Deep representation learned by BCQ algorithm. Penultimate layer of a learned Q-network is used
as a surrogate high dimensional state.
3. DQN : Deep representation learned by DQN algorithm. The original state vector is mapped to a high
dimensional space in a manner similar to BCQ.
Table 3 shows that the native state representation can exploit the simplistic Cyclicbatch really well. For a
Partial-RL batch, however, a representation learned by a strong oﬄine exploration algorithm, such as BCQ,
oﬀers more beneﬁts provided a suﬃciently large sized batch is available. DQN uses a highly discriminative
model which leads to a largely unchanged state representation with increasing data size. Results point at the
ﬂexibility of DAC framework to use state representations learned from other batch learning approaches and
improve them further.
6.6 Eﬃciency and Hyperparameter Sensitivity
DAC results presented so far are obtained from the best policy from 6 policies trained with diﬀerent
hyperparameters based on a guideline given in Shrestha et al. (2020). It should be recalled that DAC requires
two hyperparameters: a smoothness parameter k, and a cost penalty C.A-DAC uses the parameter kas
2We use grid search to tune the parameters in both MOReL and MOPO that relate to behavior ranging from classical MBRL
to highly pessimistic MBRL. It has been observed that depending on the nature of the data set (stationary or moving) or its
size, the best settings diﬀer. We only report the results obtained on the best tuned version here.
11Under review as submission to TMLR
Table 3: Comparison of the state representations used in A-DAC. Error bars indicate the min-max interval
obtained after 5 runs with diﬀerent seeds. The Cyclicpolicy is improved signiﬁcantly using native state
representation. The Partial-RL batch is better optimized using the learned state representations.
A-DAC state representation
Batch type Batch size Loop Counts BCQ DQN
Cyclic1 day 495±5 473±10 459±12
1 week 487±7 507±5460±10
Partial-RL1 day 477±8 460±10 488±8
1 week 481±5 504±9 491±4
Table 4: Comparison of oﬄine RL algorithms based on the overhead required to build and evaluate policy
given a Cyclicexperience batch. DAC and A-DAC use the state representation from BCQ. An Online RL
algorithm is also included for completeness.
Algorithm Time (minutes) Number of timesteps
Online RL 660 100k
BCQ 99 24k
DQN 101 25k
TD3+BC 100 20k
MBRL 65 18k
DAC 180 28k
A-DAC 113 24k
well as a distance threshold αfor smoothness while the cost penalty is not required. We compare sensitivity
of the algorithms to each parameter in Appendix D. The main takeaways are presented next.
DAC has been shown to be robust to the smoothness parameter k; It holds for A-DAC’s smoothness
parameters too. We set a high value of α, such as 0.8, and use a low value of k, 5 by default, which enables
eﬃcient computation while achieving robust results. DAC is highly sensitive to parameter C. A signiﬁcant
time overhead is required for online evaluations in order to tune this parameter. A-DAC oﬀers robustness
guarantees that are experimentally veriﬁed.
We compare the total time overhead for our baselines in Table 4 using conservative settings for evaluation
stopping criteria or for the amount of policy optimization. In addition to the policy evaluation, we have to
use signiﬁcant computation overhead for building the MDP and solving it optimally. (See Appendix D for
details.) With DAC, this overhead multiplies by the number of MDPs it derives in order to explore the best
policy. While computational advances in future can potentially reduce this overhead, it is unlikely to be a
non-trivial component of the overall optimization time.
7 Related work
7.1 Traﬃc Signal Control
Classical transportation engineering techniques for Traﬃc Signal Control (TSC) can be categorized as optimal
control (D’ans & Gazis, 1976) or distributed network-of-queues control (Varaiya, 2013). The widely adopted
adaptive traﬃc control systems such as Scats (Sims & Dobinson, 1980) largely depend on pre-deﬁned rules or
expert knowledge on traﬃc intersection management. There has been a lot of research work in recent years
employing RL techniques to support dynamic traﬃc signal control (Rizzo et al., 2019; Wei et al., 2018; Nishi
et al., 2018). These approaches develop diﬀerent models for state and reward functions in order to optimize
various objectives such as throughput, waiting time, or carbon emissions (Yau et al., 2017; Wei et al., 2019).
Recent deep learning RL approaches such as deep policy gradient or graph convolutions have found success
on more complex problems such as coordinated multi-agent control (Chen et al., 2020).
12Under review as submission to TMLR
The related domain of Autonomous Vehicle (AV) control has recently seen a widespread use of RL for
automation (Toghi et al., 2021; Wu et al., 2021), with some approaches even deploying the solutions at a
small scale (Stern et al., 2018; Lichtlé et al., 2022). Shi et al. (2021) is the only work, to the best of our
knowledge, that uses oﬄine RL for AV control. Our work should be similarly treated as an important ﬁrst
step towards exploiting the big potential of oﬄine RL in TSC.
The RL-based approaches for both TSC and AV control prominently use traﬃc micro-simulators. Many of
the research projects use simulators built using multiple simplifying assumptions both on the road networks
and the traﬃc ﬂow models for eﬃciency reasons (Zhang et al., 2019; Fu et al., 2019). We make use of a
mature open-source micro-simulation framework of Sumo (Lopez et al., 2018) which allows us to simulate real
road network with traﬃc modeled on the previously collected statistics generated by loop detector devices
installed in the real environment.
7.2 Oﬄine Model-free RL
Oﬀ-policy model-free RL methods (Mnih et al., 2016) adapted to batch settings have been shown to fail in
practice due to extrapolation errors (Fujimoto et al., 2019b). Most of the oﬄine RL algorithms are built on
top of an existing oﬀ-policy deep RL algorithm, such as TD3 (Fujimoto et al., 2018) or SAC (Haarnoja et al.,
2018). In order to handle out-of-distribution errors, they apply various regularization measures such as actor
pre-training with imitation learning (Kumar et al., 2020), generative behavior cloning models (Wu et al.,
2019; Kostrikov et al., 2021a) KL-divergence (Kumar et al., 2019; Jaques et al., 2019), ensemble uncertainty
quantiﬁers (Agarwal et al., 2020), behavior support constraints (Kostrikov et al., 2021b; Fujimoto et al.,
2019b) or behavior cloning (Fujimoto & Gu, 2021). Each of these algorithmic modiﬁcations for conservatism
add supplementary hyper-parameters which may need to be tuned. In oﬄine settings, however, it is not
feasible to interact with the environment for tuning. Our approach has a built-in adaptive pessimism which
empowers it to work out-of-the-box.
7.3 Oﬄine Model-based RL
Compared to model-free, the model-based oﬄine approaches have proven to be more data-eﬃcient while also
beneﬁting from more supervision (Levine et al., 2020; Yu et al., 2020). Our work builds on this recent success
of MBRL to oﬄine batches. Uncertainty quantiﬁers are critical for generalization of the model (Argenson &
Dulac-Arnold, 2020). Reward penalties act as a strong regularizer (Yu et al., 2020; Kidambi et al., 2020)
and ﬁts naturally to the nearest neighbor approximation used in our MDP model (Shrestha et al., 2020).
The strong approximation of the dynamics and optimal planning on account of ﬁnite problem structure are
key to our approach avoiding the deadly triad of deep RL (Sutton & Barto, 2018). Further, our adaptive
mechanism for shaping the reward penalties enable a high quality out-of-the-box performance while the other
approaches are sensitive to hyper-parameters and require a careful parameter tuning Lu et al. (2021).
8 Conclusion
We have modeled traﬃc signal control as an oﬄine RL problem and learnt a policy from a static batch of
data without interacting with a real or simulated environment. The oﬄine RL is a more realistic set up as it
is practically infeasible to learn a policy by interacting with a real environment. Similarly in a simulator it is
not clear how to integrate real data that is often available through traﬃc signal operators.
We have introduced a model-based learning framework, A-DAC, that uses the principle of pessimism under
uncertainty to adaptively modify or shape the reward function to infer a Markov Decision Process (MDP).
Due to the adaptive nature of the reward function, A-DAC works out of the box while the nearest competitor
requires substantial hyperparameter tuning to achieve comparable performance. An evaluation is carried out
on a complex signalized roundabout showing a signiﬁcant potential to build high performance policies in
a data eﬃcient manner using simplistic cyclicbatch collection policies. In future, we would like to explore
other applications in the traﬃc domain which can beneﬁt from oﬄine learning.
13Under review as submission to TMLR
References
ANNOY: Approximate nearest neighbors in c++/python. https://github.com/spotify/annoy . Accessed:
2022-01-22.
Markov decision process (MDP) toolbox for python. https://github.com/hiive/hiivemdptoolbox . Ac-
cessed: 2022-01-22.
Rishabh Agarwal, Dale Schuurmans, and Mohammad Norouzi. An optimistic perspective on oﬄine reinforce-
ment learning. In International Conference on Machine Learning , pp. 104–114. PMLR, 2020.
Arthur Argenson and Gabriel Dulac-Arnold. Model-based oﬄine planning. arXiv preprint arXiv:2008.05556 ,
2020.
Richard Bellman. Dynamic programming. Science, 153(3731):34–37, 1966.
Chacha Chen, Hua Wei, Nan Xu, Guanjie Zheng, Ming Yang, Yuanhao Xiong, Kai Xu, and Zhenhui Li.
Toward a thousand lights: Decentralized deep reinforcement learning for large-scale traﬃc signal control.
InProceedings of the AAAI Conference on Artiﬁcial Intelligence , volume 34, pp. 3414–3421, 2020.
Petros Christodoulou. Soft actor-critic for discrete action settings, 2019. URL https://arxiv.org/abs/
1910.07207 .
GC D’ans and DC Gazis. Optimal control of oversaturated store-and-forward transportation networks.
Transportation Science , 10(1):1–19, 1976.
Damien Ernst, Pierre Geurts, and Louis Wehenkel. Tree-based batch mode reinforcement learning. Journal
of Machine Learning Research , 6, 2005.
Zishan Fu, Jia Yu, and Mohamed Sarwat. Building a large-scale microscopic road network traﬃc simulator
in apache spark. In 2019 20th IEEE International Conference on Mobile Data Management (MDM) , pp.
320–328, 2019. doi: 10.1109/MDM.2019.00-42.
Scott Fujimoto and Shixiang Shane Gu. A minimalist approach to oﬄine reinforcement learning. arXiv
preprint arXiv:2106.06860 , 2021.
Scott Fujimoto, Herke Hoof, and David Meger. Addressing function approximation error in actor-critic
methods. In International conference on machine learning , pp. 1587–1596. PMLR, 2018.
Scott Fujimoto, Edoardo Conti, Mohammad Ghavamzadeh, and Joelle Pineau. Benchmarking batch deep
reinforcement learning algorithms. arXiv arXiv:1910.01708 , 2019a.
Scott Fujimoto, David Meger, and Doina Precup. Oﬀ-policy deep reinforcement learning without exploration.
InInternational Conference on Machine Learning . PMLR, 2019b.
Geoﬀrey J Gordon. Stable function approximation in dynamic programming. In Machine Learning Proceedings
1995, pp. 261–268. Elsevier, 1995.
Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Oﬀ-policy maximum
entropy deep reinforcement learning with a stochastic actor. In International conference on machine
learning, pp. 1861–1870. PMLR, 2018.
Michael Janner, Justin Fu, Marvin Zhang, and Sergey Levine. When to trust your model: Model-based policy
optimization. Advances in Neural Information Processing Systems , 32, 2019.
Natasha Jaques, Asma Ghandeharioun, Judy Hanwen Shen, Craig Ferguson, Agata Lapedriza, Noah Jones,
Shixiang Gu, and Rosalind Picard. Way oﬀ-policy batch deep reinforcement learning of implicit human
preferences in dialog. arXiv preprint arXiv:1907.00456 , 2019.
Rahul Kidambi, Aravind Rajeswaran, Praneeth Netrapalli, and Thorsten Joachims. Morel: Model-based
oﬄine reinforcement learning. arXiv preprint arXiv:2005.05951 , 2020.
14Under review as submission to TMLR
Ilya Kostrikov, Rob Fergus, Jonathan Tompson, and Oﬁr Nachum. Oﬄine reinforcement learning with ﬁsher
divergence critic regularization. In International Conference on Machine Learning , pp. 5774–5783. PMLR,
2021a.
Ilya Kostrikov, Ashvin Nair, and Sergey Levine. Oﬄine reinforcement learning with implicit q-learning. arXiv
preprint arXiv:2110.06169 , 2021b.
Aviral Kumar, Justin Fu, Matthew Soh, George Tucker, and Sergey Levine. Stabilizing oﬀ-policy q-learning
via bootstrapping error reduction. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d /quotesingle.ts1Alché-Buc, E. Fox,
and R. Garnett (eds.), Advances in Neural Information Processing Systems , volume 32. Curran Associates,
Inc., 2019.
Aviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine. Conservative q-learning for oﬄine reinforce-
ment learning, 2020.
Sascha Lange, Thomas Gabel, and Martin Riedmiller. Batch reinforcement learning. In Reinforcement
learning, pp. 45–73. Springer, 2012.
Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu. Oﬄine reinforcement learning: Tutorial, review,
and perspectives on open problems. arXiv:2005.01643 , 2020.
Nathan Lichtlé, Eugene Vinitsky, Matthew Nice, Benjamin Seibold, Dan Work, and Alexandre M Bayen.
Deploying traﬃc smoothing cruise controllers learned from trajectory data. In 2022 International Conference
on Robotics and Automation (ICRA) , pp. 2884–2890. IEEE, 2022.
Pablo Alvarez Lopez, Michael Behrisch, Laura Bieker-Walz, Jakob Erdmann, Yun-Pang Flötteröd, Robert
Hilbrich, Leonhard Lücken, Johannes Rummel, Peter Wagner, and Evamarie Wießner. Microscopic traﬃc
simulation using sumo. In The 21st IEEE International Conference on Intelligent Transportation Systems .
IEEE, 2018. URL https://elib.dlr.de/124092/ .
Cong Lu, Philip J Ball, Jack Parker-Holder, Michael A Osborne, and Stephen J Roberts. Revisiting design
choices in model-based oﬄine reinforcement learning. arXiv preprint arXiv:2110.04135 , 2021.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex
Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control through
deep reinforcement learning. Nature, 518(7540):529–533, 2015.
Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim Harley,
David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement learning. In
International conference on machine learning , pp. 1928–1937. PMLR, 2016.
Andrew Ng. Reinforcement learning (RL) algorithms are quite ﬁnicky. https://read.deeplearning.ai/
the-batch/issue-156/ , 2022. Accessed: 2022-08-10.
Andrew Y Ng, Daishi Harada, and Stuart Russell. Policy invariance under reward transformations: Theory
and application to reward shaping. In ICML, volume 99, 1999.
Tomoki Nishi, Keisuke Otaki, Keiichiro Hayakawa, and Takayoshi Yoshimura. Traﬃc signal control based
on reinforcement learning with graph convolutional neural nets. In 2018 21st International conference on
intelligent transportation systems (ITSC) , pp. 877–883. IEEE, 2018.
David A Nix and Andreas S Weigend. Estimating the mean and variance of the target probability distribution.
InProceedings of 1994 ieee international conference on neural networks (ICNN’94) , volume 1, pp. 55–60.
IEEE, 1994.
Takayuki Osa, Joni Pajarinen, Gerhard Neumann, J Andrew Bagnell, Pieter Abbeel, Jan Peters, et al. An
algorithmic perspective on imitation learning. Foundations and Trends ®in Robotics , 7(1-2):1–179, 2018.
M. O’Searcoid. Metric Spaces . Springer Undergraduate Mathematics Series. Springer London, 2006. ISBN
9781846286278. URL https://books.google.com.qa/books?id=aP37I4QWFRcC .
15Under review as submission to TMLR
Jason Pazis and Ronald Parr. Pac optimal exploration in continuous space markov decision processes. In
AAAI, 2013. URL http://www.aaai.org/ocs/index.php/AAAI/AAAI13/paper/view/6453 .
Anders Peterson. The origin-destination matrix estimation problem: analysis and computations . PhD thesis,
Institutionen för teknik och naturvetenskap, 2007.
Dean A Pomerleau. Alvinn: An autonomous land vehicle in a neural network. Advances in neural information
processing systems , 1, 1988.
Martin L Puterman. Markov decision processes: discrete stochastic dynamic programming . John Wiley &
Sons, 2014.
Aravind Rajeswaran, Igor Mordatch, and Vikash Kumar. A game theoretic framework for model based
reinforcement learning. In International conference on machine learning , pp. 7953–7963. PMLR, 2020.
Stefano Giovanni Rizzo, Giovanna Vantini, and Sanjay Chawla. Time critic policy gradient methods for traﬃc
signal control in complex and congested scenarios. In Proceedings of the 25th ACM SIGKDD International
Conference on Knowledge Discovery & Data Mining , 2019.
Tianyu Shi, Dong Chen, Kaian Chen, and Zhaojian Li. Oﬄine reinforcement learning for autonomous driving
with safety and exploration enhancement. arXiv preprint arXiv:2110.07067 , 2021.
Aayam Shrestha, Stefan Lee, Prasad Tadepalli, and Alan Fern. Deepaveragers: Oﬄine reinforcement learning
by solving derived non-parametric mdps. arXiv preprint arXiv:2010.08891 , 2020.
A.G. Sims and K.W. Dobinson. The sydney coordinated adaptive traﬃc (scat) system philosophy and beneﬁts.
IEEE Transactions on Vehicular Technology , 29(2):130–137, 1980. doi: 10.1109/T-VT.1980.23833.
Raphael E. Stern, Shumo Cui, Maria Laura Delle Monache, Rahul Bhadani, Matt Bunting, Miles Churchill,
Nathaniel Hamilton, R’mani Haulcy, Hannah Pohlmann, Fangyu Wu, Benedetto Piccoli, Benjamin Seibold,
Jonathan Sprinkle, and Daniel B. Work. Dissipation of stop-and-go waves via control of autonomous
vehicles: Field experiments. Transportation Research Part C: Emerging Technologies , 89:205–221, 2018.
ISSN 0968-090X. doi: https://doi.org/10.1016/j.trc.2018.02.005. URL https://www.sciencedirect.com/
science/article/pii/S0968090X18301517 .
Richard S Sutton. Dyna, an integrated architecture for learning, planning, and reacting. ACM Sigart Bulletin ,
2(4):160–163, 1991.
Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction . MIT press, 2018.
Behrad Toghi, Rodolfo Valiente, Dorsa Sadigh, Ramtin Pedarsani, and Yaser P Fallah. Altruistic maneuver
planning for cooperative autonomous vehicles using multi-agent advantage actor-critic. arXiv preprint
arXiv:2107.05664 , 2021.
Csaba D Toth, Joseph O’Rourke, and Jacob E Goodman. Handbook of discrete and computational geometry .
CRC press, 2017.
Pravin Varaiya. Max pressure control of a network of signalized intersections. Transportation Research Part
C, 36(Complete):177–195, 2013. doi: 10.1016/j.trc.2013.08.014.
Hua Wei, Guanjie Zheng, Huaxiu Yao, and Zhenhui Li. Intellilight: A reinforcement learning approach for
intelligent traﬃc light control. In Proceedings of the 24th ACM SIGKDD International Conference on
Knowledge Discovery & Data Mining , pp. 2496–2505, 2018.
Hua Wei, Guanjie Zheng, Vikash Gayah, and Zhenhui Li. A survey on traﬃc signal control methods. arXiv
preprint arXiv:1904.08117 , 2019.
Cathy Wu, Abdul Rahman Kreidieh, Kanaad Parvate, Eugene Vinitsky, and Alexandre M Bayen. Flow: A
modular learning framework for mixed autonomy traﬃc. IEEE Transactions on Robotics , 2021.
16Under review as submission to TMLR
Yifan Wu, George Tucker, and Oﬁr Nachum. Behavior regularized oﬄine reinforcement learning. arXiv
preprint arXiv:1911.11361 , 2019.
Kok-Lim Alvin Yau, Junaid Qadir, Hooi Ling Khoo, Mee Hong Ling, and Peter Komisarczuk. A survey on
reinforcement learning models and algorithms for traﬃc signal control. ACM Computing Surveys (CSUR) ,
50(3):1–38, 2017.
Tianhe Yu, Garrett Thomas, Lantao Yu, Stefano Ermon, James Zou, Sergey Levine, Chelsea Finn, and
Tengyu Ma. Mopo: Model-based oﬄine policy optimization, 2020.
Tianhe Yu, Aviral Kumar, Rafael Rafailov, Aravind Rajeswaran, Sergey Levine, and Chelsea Finn. Combo:
Conservative oﬄine model-based policy optimization. Advances in neural information processing systems ,
34:28954–28967, 2021.
Huichu Zhang, Siyuan Feng, Chang Liu, Yaoyao Ding, Yichen Zhu, Zihan Zhou, Weinan Zhang, Yong Yu,
Haiming Jin, and Zhenhui Li. CityFlow: A multi-agent reinforcement learning environment for large scale
city traﬃc scenario. In The World Wide Web Conference . ACM, may 2019. doi: 10.1145/3308558.3314139.
URL https://doi.org/10.1145%2F3308558.3314139 .
17Under review as submission to TMLR
A Proof for Optimality Guarantee
In this section, we present the proof for Theorem 4.3 using the construction proposed in Pazis & Parr (2013)
for Probably Approximately Correct (PAC) exploration in continuous state MDPs, with some additional
modiﬁcation due to our pessimistic setting.
We assume that the rewards lie in [0,Rmax]. We further assume local Lipschitz continuity of the Q-function
as deﬁned in Assumption 4.2. The local Lipschitz continuity assumption allows us to use samples from nearby
state-actions as required by A-DAC model and approximate Q-function without much error.
Deﬁnition A.1. For a state-action pair (si,ai), the pessimistic Q-value function used in A-DAC MDP ˜Mis
deﬁned as:
˜Q(si,ai) =1
k/summationdisplay
j∈NN(si,ai,k,α)/parenleftbig
max{0,rj+γ˜V(s/prime
j)−rmaxd/prime
ij}/parenrightbig
(7)
whered/prime
ij:=d(si,ai,sj,aj)max-normalized by the diameter of the state-action sample space, αis the distance
threshold used in the nearest neighbor function, and rmax= maxj∈NN(si,ai,k,α)rj.
The number of samples required to get a good approximation depends on the covering number of the
state-action space NSA(α)deﬁned next.
Deﬁnition A.2. The covering number NSA(α)of a state-action space is the size of the largest minimal set
Cof state-action pairs such that for any (si,ai)reachable from the starting state, there exists (sj,aj)∈C
such thatd/prime
ij≤α.
Let˜πbe the optimal policy of A-DAC MDP ˜M. Our goal is to bound the value V˜π(s)of˜πin the true MDP
Min terms of the optimal value V∗(s)for any state s. The following lemma suggests that it is suﬃcient to
bound the Bellman error ˜Q(s,a)−B[˜Q](s,a)across all (s,a)pairs with respect to the true MDP.
Lemma A.3. [Theorem 3.12 from Pazis & Parr (2013)] Let /epsilon1−≥0and/epsilon1+≥0be constants such that
∀(s,a)∈(S,A),−/epsilon1−≤Q(s,a)−B[Q](s,a)≤/epsilon1+. Any greedy policy πover a Q-function Qthen satisﬁes:
∀s∈S,Vπ(s)≥V∗(s)−/epsilon1−+/epsilon1+
1−γ
In order to use this lemma, we want to bound the quantity Q(s,a)−B[Q](s,a)for a ﬁxed point solution
˜Qto the pessimistic Q-function in Deﬁnition A.13. For a locally Lipschitz continuous value function
(Assumption 4.2), the value of a state-action pair can be expressed in terms of any other state-action pair in
its neighborhood as Q(si,ai) =Q(sj,aj) +ξijLQ(i,α)d/prime
ij, whereξijis a ﬁxed but possibly unknown constant
in[−1,1]. For a sample (sj,aj,rj,s/prime
j), let
xij=rj+γV(s/prime
j) +ξijLQ(i,α)d/prime
ij
Then:
Es/prime
j[xij] =Es/prime
j[rj+γV(s/prime
j)] +ξijLQ(i,α)d/prime
ij
=Q(sj,aj) +ξijLQ(i,α)d/prime
ij
Consider a state-action pair (s0,a0)and itsk-nearest neighbors given by NN(s0,a0,k,α), we can estimate
theQ-value for the pair by averaging over the predicted values of its neighbors:
ˆQ(s0,a0) =1
k/summationdisplay
i∈NN(s0,a0,k,α)x0i (8)
Let us deﬁne a new Bellman operator ˆBcorresponding to the deﬁnition of ˆQabove. While Bdenotes the
exact Bellman operator, ˜Bdenotes the approximate Bellman operator for the pessimistic value function in
Deﬁnition A.1.
3It can be easily proven that ˜Qhas a unique ﬁxed point by showing that the Bellman operator ˜Bis a contraction in maximum
norm.
18Under review as submission to TMLR
The Bellman error can be decomposed into two parts: (a) the maximum sampling error /epsilon1scaused by using a
ﬁnite number of neighbors, and (b) the estimation error /epsilon1ddue to using neighbors at non-zero distance. The
following lemma from Pazis & Parr (2013) bounds the minimum number of neighbors krequired to guarantee
certain/epsilon1swith probability 1−δ:
Lemma A.4. [Lemma 3.13 from Pazis & Parr (2013)] If˜Q2
max
/epsilon12sln/parenleftBig
2NSA(α)
δ/parenrightBig
≤k≤2NSA(α)
δ,
∀(s,a),|ˆB[˜Q](s,a)−B[˜Q](s,a)|≤/epsilon1s,w.p. 1−δ
The proof (not included here) applies Hoeﬀding’s inequality to bound the diﬀerence in the true expectation
(given by operator B) and its estimation using mean over ksamples (given by operator ˆB).
The second piece of the Bellman error requires us to bound the term /epsilon1d=˜B[˜Q](s,a)−ˆB[˜Q](s,a).
Lemma A.5. For all known state-action pairs (s,a)
0≤˜B[˜Q](s,a)−ˆB[˜Q](s,a)≤¯dmaxRmax
Proof.We can simplify the estimation error /epsilon1dby using the deﬁnitions (7) and (8).
/epsilon1d=˜B[˜Q](si,ai)−ˆB[˜Q](si,ai)
=1
k/summationdisplay
j∈NN(si,ai,k,α)/parenleftbig
−rmax−ξijLQ(i,α)/parenrightbig
∗d/prime
ij
We can set ξij=−Rmax
LQ(i,α)which will bound the quantity inside the bracket from [0,Rmax],∀(s,a)since
0≤rmax≤Rmax. The worse case average distance is deﬁned as ¯dmax, therefore ensuring that /epsilon1d≤
¯dmaxRmax.
Finally, to prove the PAC bound in Theorem 4.3, we need to bound the quantity ˜Q(s,a)−B[˜Q](s,a). To
achieve that, we combine the above two lemmas and apply the operators on the ﬁxed point solution ˜Q,
giving us :
If˜Q2
max
/epsilon12sln/parenleftBig2NSA(α)
δ/parenrightBig
≤k≤2NSA(α)
δ,∀(s,a),−/epsilon1s≤˜Q(s,a)−B[˜Q](s,a)≤/epsilon1s+¯dmaxRmax
Putting these bounds in Lemma A.3 gives us the ﬁnal result.
B Modeling Environment for a Signalized Roundabout
We model a signalized roundabout (Figure 7) used previously to learn an online RL policy using policy
gradient Rizzo et al. (2019). It consists of three types of lanes or traﬃc arms: (a) approaching/ incoming
lanes, (b) outgoing lanes, and (c) circulatory lanes that enable traﬃc ﬂow redirection. Each traﬃc arm has
multiple lanes. It is important to consider each lane separately because the way the incoming traﬃc ‘weaves
in’ to the circulatory lanes impacts the wait time of vehicles. Movement in or out of the circulatory lanes is
controlled by traﬃc signals numbering 10 in total.
States. A state corresponds to the number of vehicles counted by the loop detector devices installed on every
lane. For each approaching or outgoing lanes, there are two devices per lane, one close to the roundabout
and another several meters farther. The detectors number 68 in total.
Actions. Actions correspond to traﬃc control phases activated for a ﬁxed time duration. A phase is provided
for each set of non-conﬂicting ﬂows. For example, traﬃc moving from north to south and from south to
north does not conﬂict and therefore constitutes a single phase. In all, we use a discrete set of 11 actions as
modeled in Rizzo et al. (2019).
19Under review as submission to TMLR
Rewards. Traﬃc signal control typically serves a dual objective: maximize throughput and avoid long traﬃc
queues. We optimize only the ﬁrst objective here and leave the the later for a future demonstration. For
throughput maximization, the rewards are modeled as the cumulative capacity: The cumulative capacity
C(t)at timetis the number of vehicles that left the roundabout from time 0tot. Reward at time step tis
then deﬁned as R(t) =C(t)−C(t−1).
O-D Driven Traﬃc Simulation: Access to real experience trajectories data is often very limited and/ or
allows only a speciﬁc behavioral policy (e.g. Cyclicin our case) oﬀering little room for experimentation. We
describe the process employed to augment the batch collection process in our environment.
A micro-simulator is set up using real network conﬁguration with traﬃc signals and loop detector devices
correctly placed. Traﬃc is generated using an Origin-Destination (O-D) matrix Peterson (2007) which is
prepared by urban planning authorities. The O-D matrix corresponds to macro statistics for a relatively
small, but signiﬁcantly larger than the traﬃc phase duration, period of the day. It enumerates the number of
vehicles that move between each pair of traﬃc zones positioned in the close vicinity of the roundabout. The
data is collected by routine monitoring of traﬃc by loop detector devices and is believed to be stable. The O-D
data is fed to Sumo micro-simulator which generates vehicle routes following the provided source, destination,
and frequency requirements. Such a real demand-driven simulation is used for batch data augmentation.
C Evaluation Baselines
A-DAC is evaluated against three RL approaches that are speciﬁcally created for discrete action spaces,
namely, (a) DQN (Mnih et al., 2015), (b) BCQ (Fujimoto et al., 2019b) which has both a continuous action
and a discrete action version, and (c) DAC-MDP (Shrestha et al., 2020). We use the author provided
implementations for each of these. In addition to these baselines, we also include some recent approaches built
speciﬁcally for continuous action spaces. We brieﬂy describe the modiﬁcations needed in these algorithms to
make them work for discrete action spaces.
C.1 Discrete TD3+BC
TD3 (Fujimoto et al., 2018), short for Twin Delayed DDPG, is a deep oﬀ-policy RL algorithm that can only
be used in continuous action environments. It provides three important features that make it a strong RL
baseline: (a) Clipped double-Q learning, (b) Delayed policy updates, and (c) Target policy smoothing. For
oﬄine algorithms, a behavior cloning regularization is applied (Fujimoto & Gu, 2021) to TD3 policy training
as shown in Eq. 4. We make the following modiﬁcations to the TD3+BC algorithm:
•The two Q-functions and the policy function take the form Qφi:S→R|A|,i∈{1,2}andπθ:S→
[0,1]|A|.
•Thelossfunctionforpolicynetwork πθisgivenbyL(θ,D) =E(s,a)∼Dπθ(s)T/parenleftbig
−λQφ1(s)/parenrightbig
+/parenleftbig
πθ(s)−a/parenrightbig2,
whereλis a normalizing scalar.
•Smoothing of action selection from policy network for target Qevaluation is disabled because it is
possible to calculate the exact action distribution in discrete setting.
•Each of the critic (Q) networks is trained with the loss function given by L(φi,D) =
E(s,a,s/prime,r)∈D[/parenleftbig
Qφi(s)−y(r,s/prime)/parenrightbig2], i∈ {1,2}where the target yis given by y(r,s/prime) =r+
γmini{πθ(s/prime)TQφi(s/prime)}
Please see Christodoulou (2019) for a similar exercise on deploying the equally popular SAC (Haarnoja et al.,
2018) algorithm on discrete action spaces.
C.2 Discrete MBPO
We evaluate two algorithms that ﬁrst build an approximate MDP dynamics from the oﬄine data set and
then do sample rollouts from the derived MDP to optimize a policy network. The ﬁrst, MOReL (Kidambi
20Under review as submission to TMLR
et al., 2020), derives an ensemble of learned dynamics models Nix & Weigend (1994) which allows tracking of
uncertainty in estimation of next state. This uncertainty quantiﬁcation is used in creating a pessimistic(P-)
MDP model where transitions to uncertain regions are restricted by a threshold parameter. While this
P-MDP can be solved using any planning algorithm, authors train a policy using model-based natural policy
gradient (Rajeswaran et al., 2020). Given this infrastructure, only minimal changes are required to run this
algorithm on discrete action setup. The dynamics models use a single dimension for action input. Further,
the policy network predict a probability distribution over all possible actions.
The second, MOPO (Yu et al., 2020), starts by building an ensemble of learned dynamics models similar
to MOReL. Further, it learns a reward model from the data set as well. It then derives a new uncertainty-
penalized MDP: the uncertainty quantiﬁcation given by the ensemble of models is used to penalize reward
estimates. This is followed by policy optimization using model rollouts. Similar to MOReL, we use a single
dimension action input for dynamics model and then use a DQN network to learn an optimal policy for the
derived MDP.
D Additional Evaluation
D.1 Hyperparameter Sensitivity of DAC
We analyze the sensitivity of each hyperparameter individually. A-DAC retains DAC’s robustness to
smoothness parameter k; Figure 10 provides the evidence. For a small value of k(ranging between 2−10),
we ﬁnd that parameter αhas a minimal role. For instance, Figure 11 studies the impact of αwhenkis set to
5. Except for very low values of α, the performance remains unaﬀected. It should be noted that having a
large distance threshold does not harm since our adaptive reward computation penalizes distant neighbors.
Based on this, we set αto 0.8 by default.
When it comes to parameter C, Figure 12 shows that DAC is highly sensitive to the parameter. The values
forCare varied between the minimum and the maximum rewards observed in the dataset. The robustness
oﬀered by A-DAC by adapting rewards based on local neighborhood ensures that A-DAC does not need to
spend expensive cycles on hyperparameter tuning.
D.2 Computational Overhead
A breakdown of computation time overhead is presented in Figure 13. All numbers are obtained from a
server running a 16-core 2nd generation Intel Xeon processor with 128GB RAM. Our implementation uses
fast approximate solutions to nearest neighbor searches and diameter computations. But the MDP build and
MDP solve operations suﬀer as they process a transition matrix growing quadratically with the number of
core states. Designing a distributed GPU based implementation for optimal planning is left as a future work.
21Under review as submission to TMLR
12345678910400450500Avg returns/ hourImpact of parameter konA-DAC
Figure 10: Analyzing the impact of smoothness parameter kinA-DAC. The setting k= 1makes the
MDP deterministic and incapable of exploiting the diﬀerent transitions observed in the experience data.
Performance is not much sensitive to values of k>1.
0.20.30.40.50.60.70.8400450500Avg returns/ hourImpact of parameter αonA-DAC
Figure 11: Analyzing the impact of smoothness parameter αinA-DAC. Low settings result in insuﬃcient
neighbors used in approximation that has an adverse eﬀect. High settings are more robust.
0123456789400450500A-DACAvg returns/ hourImpact of parameter Con DAC
Figure 12: Comparing the impact of cost penalty Cin DAC to the adaptive reward penalties in A-DAC. DAC
is highly sensitive to Cand requires a careful tuning. A-DAC manages to match or better the performance
of the best Csetting in DAC out-of-the-box.
22Under review as submission to TMLR
0 200 400 600 800 1,000 1,2001 day batch
(8k core states)1 week batch
(45k core states)
Time (seconds)Breakdown of computation time in A-DAC
ANN index build Diameter computation MDP build
MDP solve (VI) Action selection (ANN query)
Figure 13: Computation time breakdown between diﬀerent processes in A-DAC. The nearest neighbor
querying and the diameter computation use fast approximate algorithms and scale well. But MDP build and
solve stages suﬀer from quadratic time complexity.
23