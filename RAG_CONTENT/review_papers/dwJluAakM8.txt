Published in Transactions on Machine Learning Research (November/2024)
Optimized Tradeoffs for Private Prediction with Majority
Ensembling
Shuli Jiang shulij@andrew.cmu.edu
Robotics Institute, Carnegie Mellon University
Qiuyi (Richard) Zhang qiuyiz@google.com
Google DeepMind
Gauri Joshi gaurij@andrew.cmu.edu
Electrical and Computer Engineering, Carnegie Mellon University
Reviewed on OpenReview: https: // openreview. net/ forum? id= dwJluAakM8
Abstract
We study a classical problem in private prediction, the problem of computing an (mϵ,δ )-
differentially private majority of K(ϵ,∆)-differentially private algorithms for 1≤m≤K
and1>δ≥∆≥0. Standard methods such as subsampling or randomized response are
widely used, but do they provide optimal privacy-utility tradeoffs? To answer this, we
introduce the Data-dependent Randomized Response Majority (DaRRM) algorithm. It is
parameterized by a data-dependent noise function γ, and enables efficient utility optimization
over the class of all private algorithms, encompassing those standard methods. We show that
maximizing the utility of an (mϵ,δ )-private majority algorithm can be computed tractably
through an optimization problem for any m≤Kby a novel structural result that reduces
the infinitely many privacy constraints into a polynomial set. In some settings, we show
that DaRRM provably enjoys a privacy gain of a factor of 2 over common baselines, with
fixed utility. Lastly, we demonstrate the strong empirical effectiveness of our first-of-its-kind
privacy-constrained utility optimization for ensembling labels for private prediction from
private teachers in image classification. Notably, our DaRRM framework with an optimized
γexhibits substantial utility gains when compared against several baselines.
1 Introduction
Differential privacy (DP) is a widely applied framework for formally reasoning about privacy leakage when
releasing statistics on a sensitive database Erlingsson et al. (2014); Cormode et al. (2018). Differential privacy
protects data privacy by obfuscating algorithmic output, ensuring that query responses look similar on
adjacent datasets while preserving utility as much as possible Dwork et al. (2006).
Privacy in practice often requires aggregating or composing multiple private procedures that are distributed
for data or training efficiency. For example, it is common to aggregate multiple private algorithmic or model
outputs in methods such as boosting or calibration (Sagi & Rokach, 2018). In federated learning, model
training is distributed across multiple edge devices. Those devices need to send local information, such as
labels or gradients Konečn` y et al. (2016), to an aggregating server, which is often honest but curious about
the local training data. Hence, the output from each model at an edge device needs to be privatized locally
before being sent to the server. When translating from a local privacy guarantee to a centralized one, one
needs to reason about the composition of the local privacy leakage Naseri et al. (2020). Therefore, we formally
ask the following:
Problem 1.1 (Private Majority Ensembling (Illustrated in Figure 1)) .ConsiderK≥1 (ϵ,∆)-differentially
private mechanisms M1,...,MKforKodd. Given a dataset D, each mechanism outputs a binary answer —
that is,Mi:D→{ 0,1},∀i∈[K]. Given a privacy allowance 1≤m≤K,m∈Rand a failure probability
1Published in Transactions on Machine Learning Research (November/2024)
δ≥∆≥0,δ,∆∈[0,1), how can one maximize the utility of an (mϵ,δ )-differentially private mechanism A
to compute the majority function g(S1,S2,...,SK), whereSi∼Mi(D)?
Figure1: Anillustrationoftheproblemsetting. Theinputsarethedataset DandK(ϵ,∆)-differentiallyprivate
mechanisms M1,...,MK. One draws samples Si∼Mi(D)and computes an aggregated output g(S1,...,SK)
based on all observed samples. Our goal is to design a randomized algorithm Athat approximately computes
gand is (mϵ,δ )-differentially private for 1≤m≤Kandδ≥∆≥0. We focus on gbeing the majority
function .
The majority function gis often used in private prediction, where one studies the privacy cost of releasing
one prediction Dwork & Feldman (2018) and exploits the fact that releasing only the aggregated output
on sharded models is significantly more private than releasing each prediction. For example, this occurs in
semi-supervised knowledge transfer with private aggregated teacher ensembles (PATE) Papernot et al. (2017;
2018), in ensemble learning algorithms Jia & Qiu (2020); Xiang et al. (2018), machine unlearning Bourtoule
et al. (2021), private distributed learning algorithms such as Stochastic Sign-SGD Xiang & Su (2023), and in
ensemble feature selection Liu et al. (2018). Private prediction is also shown to be a competitive technique in
data-adaptive settings, where the underlying dataset is changing slowly over time, to quickly adjust to online
dataset updates Zhu et al. (2023). Furthermore, to address the large privacy loss of private prediction under
the many-query regime, there has been recent works in everlasting private prediction that extends privacy
guarantees with repeated, possibly infinite, queries without suffering a linear increase in privacy loss Naor
et al. (2023); Stemmer (2024).
These works, however, rely often on the standard sensitivity analysis of gto provide a private output and
thus generally provide limited utility guarantees. This is because the maximum sensitivity of gcan be too
pessimistic in practice, as observed in the problem of private hyperparameter optimization (Liu & Talwar,
2019). On the other hand, for private model ensembling, a naive way to bound privacy loss without restrictive
assumptions is to apply simple composition (Theorem 2.2) or general composition (Theorem 2.3, a tighter
version compared to advanced composition) to reason about the final privacy loss after aggregation. A
black-box application of the simple composition theorem to compute gwould incur a Kϵprivacy cost in the
pure differential privacy setting, that is, δ= 0, or if one is willing to tolerate some failure probability δ, general
composition would yield a O(√
Kϵ)privacy cost Kairouz et al. (2015). Thus, a natural baseline algorithm A
that is (mϵ,m ∆)-differentially private applies privacy amplification by subsampling and randomly chooses m
of theKmechanisms to aggregate and returns the majority of the subsampled mechanisms. This technique is
reminiscent of the subsampling procedure used for the maximization function g(Liu & Talwar, 2019) or some
general techniques for privacy amplification in the federated setting via shuffling (Erlingsson et al., 2019).
However, standard composition analysis and privacy amplication techniques can be suboptimal for computing
a private majority, in terms of both utility and privacy. Observe that if there is a clear majority among
the outputs of M1(D),...,MK(D), one can add less noise. This is because each mechanism Miis(ϵ,∆)-
differentially private already, and hence, is less likely to change its output on a neighboring dataset by
definition. This implies the majority outcome is unlikely to change based on single isolated changes in D.
Furthermore, composition theorems make two pessimistic assumptions: 1) the worst-case function gand
the datasetDare considered, and 2) all intermediate mechanism outputs M1(D),...,MK(D)are released,
2Published in Transactions on Machine Learning Research (November/2024)
rather than just the final aggregate. Based on these observations, is it possible then to improve the utility of
computing a private majority, under a fixed privacy loss?
1.1 Our Contributions
We give a (perhaps surprising) affirmative answer to the above question by using our novel data-dependent
randomized response framework ( DaRRM), which captures all private majority algorithms, we introduce a
tractable noise optimization procedure that maximizes the privacy-utility tradeoffs. Furthermore, we can
provably achieve a constant factor improvement in utility over simple subsampling by applying data-dependent
noise injection when Mi’s are i.i.d. and δ= 0. To our knowledge, this is the first of its work of its kind that
gives a tractable utility optimization over the possibly infinite set of privacy constraints.
Data-dependent Randomized Response Majority ( DaRRM).We generalize the classical Randomized
Response (RR) mechanism and the commonly used subsampling baseline for solving Problem 1.1 and propose
a general randomized response framework DaRRM(see Algorithm 1), which comes with a customizable noise
functionγ. We show that DaRRMactually captures all algorithms computing the majority whose outputs
are at least as good as a random guess (see Lemma 3.3), by choosing different γfunctions.
Designing γwith Provable Privacy Amplification. The choice of the γfunction in DaRRMallows
us to explicitly optimize noise while trading off privacy and utility. Using structural observations, we show
privacy amplification by a factor of 2 under mild conditions over applying simple composition in the pure
differential privacy setting when the mechanisms Mi’s are i.i.d. (see Theorem 4.1).
Finding the Best γthrough Dimension-Reduced Optimization. We further exploit the generality
ofDaRRMby applying a novel optimization-based approach that applies constrained optimization to find
a data-dependent γthat maximizes some measure of utility. One challenge is that there are infinitely
many privacy constraints, which are necessary for DaRRMwith the optimized γto satisfy the given privacy
loss. We show that we can reformulate the privacy constraints, which are infinite dimensional, to a finite
polynomial-sized constraint set, allowing us to efficiently constrain the optimization problem to find the best
γ, even for approximate differential privacy (see Lemma 5.1). Empirically, we show that with a small mand
ϵ, the optimized γ(seeγoptin Figure 2) achieves the best utility among all γfunctions, even compared to
the subsampling and the data-independent baseline. To our knowledge, this is the first utility maximization
algorithm that optimizes over all private algorithms by constrained optimization with dimension reduction.
Experiments. In downstream tasks, such as semi-supervised knowledge transfer for private image classi-
fication, we compare our DaRRMwith an optimized γto compute the private label majority from private
teachers against PATE Papernot et al. (2018), which computes the private label majority from non-private
teachers. We fix the privacy loss of the output of both algorithms to be the same and find that when the
number of teachers Kis small, DaRRMindeed has a higher utility than PATE, achieving 10%-15% and 30%
higher accuracy on datasets MNISTandFashion-MNIST , respectively.
2 Background
2.1 Related Work
Private Composition. Blackbox privacy composition analysis often leads to pessimistic utility guarantees.
In the blackbox composition setting, one can do no better than the O(Kϵ)privacy analysis for pure differential
privacy Dwork et al. (2014). For approximate differential privacy, previous work has found optimal constants
for advanced composition by reducing to the binary case of hypothesis testing with randomized response; and
optimal tradeoffs between ϵ,δfor black box composition are given in Kairouz et al. (2015), where there could
be a modest improvement 20%.
Thus, for specific applications, previous work has turned to white-box composition analysis for improved
utility. This includes, for example, moment accountant for private SGD Abadi et al. (2016) and the application
of contractive maps in stochastic convex optimization Feldman et al. (2018). For the specific case of model
ensembles, Papernot et al. (2018) shows a data-dependent privacy bound that vanishes as the probability of
3Published in Transactions on Machine Learning Research (November/2024)
disagreement goes to 0. Their method provides no utility analysis but they empirically observed less privacy
loss when there is greater ensemble agreement.
Whengis the maximization function, some previous work shows that an approximately maximum value can
be outputted with high probability while incurring O(ϵ)privacy loss, independently of K. Liu & Talwar
(2019) proposed a random stopping mechanism for m= 1that draws samples uniformly at random from
Mi(D)at each iteration. In any given iteration, the sampling halts with probability γand the final output is
computed based on the samples collected until that time. This leads to a final privacy cost of only 3ϵfor
the maximization function g, which can be improved to 2ϵ(Papernot & Steinke, 2022). In addition to the
aforementioned works, composing top-k and exponential mechanisms also enjoy slightly improved composition
analysis via a bounded-range analysis Durfee & Rogers (2019); Dong et al. (2020).
Bypassing the Global Sensitivity. To ensure differential privacy, it is usually assumed the query function
ghas bounded global sensitivity — that is, the output of gdoes not change much on anyadjacent input
datasets differing in one entry. The noise added to the output is then proportional to the global sensitivity of
g. If the sensitivity is large, the output utility will thus be terrible due to a large amount of noises added.
However, the worst case global sensitivity can be rare in practice, and this observation has inspired a line of
works on designing private algorithms with data-dependent sensitivity bound to reduce the amount of noises
added.
Instead of using the maximum global sensitivity of gon any dataset, the classical Propose-Test-Release
framework of Dwork Dwork & Lei (2009) uses a local sensitivity value for robust queries that is tested
privately and if the sensitivity value is too large, the mechanism is halted before the query release. The
halting mechanism incurs some failure probability but deals with the worst-case sensitivity situations, while
allowing for lower noise injection in most average-case cases.
One popular way to estimate average-case sensitivity is to use the Subsample-and-Aggregate framework by
introducing the notion of perturbation stability , also known as local sensitivity of a function gon a dataset
DThakurta & Smith (2013); Dwork et al. (2014), which represents the minimum number of entries in D
needs to be changed to change g(D). One related concept is smooth sensitivity , a measure of variability of g
in the neighborhood of each dataset instance. To apply the framework under smooth sensitivity , one needs to
privately estimate a function’s local sensitivity Lsand adapt noise injection to be order of O(Ls
ϵ), where
Lscan often be as small as O(e−n), wheren=|D|, the total dataset size Nissim et al. (2007). Generally,
the private computation of the smooth sensitivity of a blackbox function is nontrivial but is aided by the
Subsample and Aggregate approach for certain functions.
These techniques hinge on the observation that a function with higher stability on Drequires less noise to
ensure worst case privacy. Such techniques are also applied to answer multiple online functions/queries in
model-agnostic learning Bassily et al. (2018). However, we highlight two key differences in our setting with a
weaker stability assumption. First, in order to estimate the perturbation stability ofgonD, one needs to
downsample or split Dinto multiple blocks Thakurta & Smith (2013); Dwork et al. (2014); Bassily et al.
(2018), ˆD1,..., ˆDB, and estimate the perturbation stability based on the mode of g(ˆD1),...,g (ˆDB). This
essentially reduces the amount of change in the output of gdue to a single entry in D, with high probability
and replaces the hard-to-estimate perturbation stability ofgwith an easy-to-compute perturbation stability of
the mode. Such a notion of stability has also been successfully applied, along with the sparse vector technique,
for model-agnostic private learning to handle exponentially number of queries to a model Bassily et al. (2018).
Note that in these cases, since a private stochastic test is applied, one cannot achieve pure differential privacy
Dwork et al. (2014). In practice, e.g. federated learning, however, one does not have direct access to D, and
thus it is impractical to draw samples from or to split D. Second, to ensure good utility, one relies on a key
assumption, i.e. the subsampling stability ofg, which requires g(ˆD) =g(D)with high probability over the
draw of subsamples ˆD.
Although our intuition in designing DaRRMalso relies on the stability of the mode function g, previous usage
of stability to improve privacy-utility tradeoffs, e.g., propose-test-release Vadhan (2017); Dwork et al. (2014),
requires the testing of such stability, based on which one adds a larger (constant) noise γ. This can still lead
to adding redundant noise in our case.
4Published in Transactions on Machine Learning Research (November/2024)
Optimal Randomized Response. Holohan et al. (2017) and Kairouz et al. (2015) show that the classical
Randomized Response (RR) mechanism with a constant probability of faithfully revealing the true answer is
optimal in certain private estimation problems. Our proposed DaRRMframework and our problem setting is
a generalized version of the ones considered in both Holohan et al. (2017) and Kairouz et al. (2015), which
not only subsumes RR but also enables a data-dependent probability, or noise addition.
While RR with a constant probability can be shown optimal in problems such as private count queries or
private estimation of trait possession in a population, it is not optimal in other problems, such as private
majority ensembling, since unlike the former problems, changing one response of the underlying mechanisms
does not necessarily change the output of the majority. To explicitly compute the minimum amout of noise
required, one needs the output distributions of the underlying mechanisms but this is unknown. To resolve
this, our proposed DaRRM framework adds the amount of noise dependent on the set of observed outcomes
from the underlying private mechanisms, S, which is a random variable of the dataset and is hence a proxy.
This enables DaRRM to calibrate the amount of noise based on whether the majority output is likely to
change. The amount of noise is automatically reduced when the majority output is not likely to change.
Second, Holohan et al. (2017) and Kairouz et al. (2015) both consider a special case in our setting where all
Kprivate mechanisms are i.i.d., while our approach focuses on the more general setting where each private
mechanism can have a different output distribution.
Learning A Good Noise Distribution. There have been limited works that attempt to derive or learn a
good noise distribution that improves the utility. For deep neural networks inference, Mireshghallah et al.
(2020) attempts to learn the best noise distribution to maximizing utility subject to an entropy Lagrangian,
but no formal privacy guarantees were derived. For queries with bounded sensitivity, Geng & Viswanath
(2015) demonstrate that the optimal noise distribution is in fact a staircase distribution that approaches the
Laplacian distribution as ϵ→0.
Private Prediction. Instead of releasing a privately trained model as in private learning, private prediction
hides the models and only releases private outputs. Private prediction has been shown as a practical alternative
compared to private learning, as performing private prediction is much easier compared to private learning
on a wide range of tasks Dwork & Feldman (2018); Naor et al. (2023); van der Maaten & Hannun (2020).
Although a privately trained model can make infinitely many predictions at the inference time without
incurring additional privacy loss, since differential privacy is closed under post-processing, it has been shown
recently that it is indeed possible to make infinitely many private predictions Naor et al. (2023) with a finite
privacy loss for specific problems.
2.2 Preliminaries
We first introduce the definition of differential privacy, simple composition and general composition as follows.
The general composition Kairouz et al. (2015) gives a near optimal and closed-form bound on privacy loss
under adaptive composition, which improves upon advanced composition Dwork et al. (2014).
Definition 2.1 (Differential Privacy (DP) Dwork et al. (2014)) .A randomized mechanism M:D→Rwith
a domainDand rangeRsatisfies (ϵ,δ)-differential privacy for ϵ,δ≥0if for any two adjacent datasets
D,D′and for any subset of outputs S⊆Rit holds that Pr[M(D)∈S]≤eϵPr[M(D′)∈S] +δ.δ= 0is
often called pure differential privacy; while δ>0is often called approximate differential privacy.
Theorem 2.2 (Simple Composition Dwork et al. (2014)) .For anyϵ > 0andδ∈[0,1], the class of
(ϵ,δ)-differentially private mechanisms satisfy (kϵ,kδ )-differential privacy under k-fold adaptive composition.
Theorem 2.3 (General Composition (Theorem 3.4 of Kairouz et al. (2015))) .For anyϵ>0,δ∈[0,1]and
δ′∈(0,1], the class of (ϵ,δ)-differentially private mechanisms satisfies (ϵ′,1−(1−δ)k(1−δ′))-differential
privacy under k-fold adaptive composition for
ϵ′= min/braceleftig
kϵ,(eϵ−1)ϵk
eϵ+ 1+ϵ/radicaligg
2klog(e+√
kϵ2
δ′),(eϵ−1)ϵk
eϵ+ 1+ϵ/radicalbig
2klog(1/δ′)/bracerightig
We then formalize the error and utility metric in our problem as follows:
5Published in Transactions on Machine Learning Research (November/2024)
Definition 2.4 (Error Metric and Utility Metric) .For the problem setting in Definition 1.1, let the observed
(random) outcomes set be S={S1,..,Sk}, whereSi∼Mi(D). For a fixedD, we define the error of an
algorithmA, i.e.,E(A), in computing the majority function gas the Total Variation (TV) distance between
g(S)andA(D). Specifically,
E(A) =DTV(g(S)∥A(D)) =|Pr[A(D) = 1]−Pr[g(S) = 1]|
and the utility is defined as 1−E(A).
Notation. Throughout the paper, we use the same notations defined in Problem 1.1 and Definition 2.4.
Furthermore, let DandD′to denote a pair of adjacent datasets with one entry being different. Also, let
pi=Pr[Mi(D) = 1]andp′
i=Pr[Mi(D′) = 1],∀i∈[K]. We omit the subscript iwhen allpi’s orp′
i’s
are equal. I{·}denotes the indicator function and [K] ={1,2,...,K}. For the purpose of analysis, let
L(D) =/summationtextK
i=1Mi(D)∈{0,1,...,K}, i.e. the (random) sum of all observed outcomes on dataset D.Dis
omitted when the context is clear. Unless specified, we use the noise function γ:{0,1,...,K}→[0,1]as
input to our algorithms to calibrate the probabilistic noise injection. Unless specified, the privacy allowance
m∈R.
3 Private Majority Algorithms
The very first approach to consider when solving private majority ensembling (Problem 1.1), since the output
is binary, is the classical Randomized Response ( RR) mechanism Dwork et al. (2014), where one flips a biased
coin with a constant probability pconst∈[0,1]. If the coin lands on head with probability pconst, output the
true majority base on Ksamples; if not, then simply output a noisy random answer. However, to make the
output (mϵ,δ )-differential private, the success probability pconstcan be at most O(m
K)(orO(m√
K)) when
δ= 0(orδ>0) (see Appendix A.1), which is too small for any reasonable utility.
The key observation for improved utility is that the probability of success should not be a constant, but
should depend on the unpublished set of observed outcomes from the mechanisms S. If we see many 1’s
or 0’s inS, then there should be a clear majority even on adjacent datasets. On the other hand, if we see
about half 1’s and half 0’s, this means the majority is highly volatile to data changes, which implies we need
more noise to ensure privacy. In summary, if we can calibrate the success probability based on Sto smoothly
increase when there is a clear majority, we can improve the utility without affecting privacy.
Subsampling. One natural baseline is outputting the majority of mout ofKrandomly subsampled
mechanisms (without replacement), given a privacy allowance m∈[K]. Supposeδ≥m∆, the privacy loss of
the aggregated output can be reasoned through simple composition or general composition. Interestingly, we
show outputting the majority of mout ofKsubsampled mechanisms corresponds to RRwith anon-constant
probability pγ=γSub(L(D)), which is set by a polynomial function γSub:{0,...,K}→[0,1]based on the
sum of observed outcomes L(D)in Lemma 3.1 (see a full proof in Appendix A.2). Intuitively, subsampling may
be seen as implicitly adding noise by only outputting based on a randomly chosen subset of the mechanisms;
therefore this implicit noise is inherently data-dependent onL(D).
Lemma 3.1. Consider Problem 1.1, with the privacy allowance m∈[K]. Consider the data-dependent
algorithm that computes L(D)and then applies RRwith probability pγ. Ifpγ=γSub(l), wherel∈{0,1,...,K}
is the value ofL(D), i.e., the (random) sum of observed outcomes on dataset D, andγSub:{0,1,...,K}→
[0,1]is
γSub(l) =γSub(K−l) =

1−2/summationtextm
j=m+1
2(l
j)(K−l
m−j)
(K
m)ifmis odd
1−2/summationtextm
j=m
2+1(l
j)(K−l
m−j)
(K
m)−(lm
2)(K−l
m
2)
(K
m)ifmis even
then the majority of mout ofKsubsampled mechanisms without replacement and the output of our data-
dependent RRalgorithm have the same distribution.
One thing special about subsampling is that when m= 1, it indeed results in the optimal error, which we
show in Lemma 3.2 as follows. See a full proof in Appendix A.3. Note that when m= 1, subsampling outputs
6Published in Transactions on Machine Learning Research (November/2024)
a majority of 1 with probability exactly1
K/summationtextK
i=1pi. This lower bound only applies to the case when m= 1,
since when m> 1, the probability of subsampling outputting a majority of 1 is not necessary1
K/summationtextK
i=1pi.
Lemma 3.2 (Lower Bound on Error when m= 1).LetAbe an (ϵ,δ)-differentially private algorithm,
whereϵ∈(0,1
2)andδ∈[0,1
2), that computes the majority of K(ϵ,δ)-differentially private mechanisms
M1,...,MK, whereMi:D→{ 0,1}on datasetDandPr[Mi(D) = 1] =pi,∀i∈[K]. Then, the error
E(A)≥|Pr[g(S) = 1]−1
K/summationtextK
i=1pi|, whereg(S)is the probability of the true majority output being 1 as
defined in Definition 1.1.
Algorithm 1 DaRRM (·): Data-dependent Randomized Response Majority
1:Input:K(ϵ,∆)-DP mechanisms {Mi}K
i=1, noise function γ:{0,1}K+1→[0,1](in our specific setting
γ:{0,1,...,K}→[0,1]), datasetD, privacy allowance 1≤m≤K, failure probability δ≥∆≥0
2:Output: (mϵ,δ )-DP majority vote of {Mi}K
i=1
3:S={S1,..,SK}, whereSi∼Mi(D)
4:L=/summationtextK
i=1Si
5:Set probability pγ←γ(S)(in our setting pγ←γ(L))
6:Flip thepγ- biased coin
7:ifHead (with probability pγ)then
8:Output I{1
KL≥1
2}
9:else
10:Output 0/1with equal probability
11:end if
Data-dependent Randomized Response ( DaRRM).Does subsampling give optimal utility when m> 1?
Inspired by the connection between RRand subsampling, we propose Data-dependent Randomized Response
Majority ( DaRRM) in Algorithm 1, to study optimizing privacy-utility tradeoffs in private majority ensembling.
In particular, DaRRMhas anon-constant success probability pγthat is set by a parameterized noise function
γ, which in turn depends on the set of observed outcomes S={S1,...,SK}. In fact, we can show that
DaRRMis general: any reasonable algorithmA, name one whose output is at least as good as a random
guess, can be captured by the DaRRMframework in Lemma 3.3 (see a full proof in Appendix A.4). We
denote DaRRMinstantiated with a specific noise function γbyDaRRMγ.
Lemma 3.3 (Generality of DaRRM).LetAbe any randomized algorithm to compute the majority function
gonSsuch that for all S,Pr[A(S) =g(S)]≥1/2(i.e.Ais at least as good as a random guess). Then,
there exists a a general function γ:{0,1}K+1→[0,1]such that if one sets pγbyγ(S)inDaRRM, the output
distribution of DaRRMγis the same as the output distribution of A.
Designing the γFunction. With the DaRRMframework, we ask: how to design a good γfunction
that maximizes the utility? First, we introduce two characteristics of γthat do not affect the utility, while
simplifying the analysis and the empirical optimization:
(a)A function of the sum of observed samples : Since the observed samples set Sis a permutation-
invariant set, a sufficient statistic that captures the full state of SisL=/summationtextK
i=1Si, the sum of
observed outcomes. This allows us to reduce γ(S) =γ(L).Hence, in the rest of the paper, we focus
onγ:{0,1,...,K}→[0,1].
(b)Symmetric aroundK
2: Ifγis asymmetric, we can symmetrize by reflecting one region aboutK
2
and achieve better or equal expected utility, where the utility is summed over symmetric distributions
ofpi.
Note thatγSubsatisfies both characteristics. Now, recall L(D)andL(D′)are the sum of observed outcomes
on adjacent datasets DandD′. Also, recall pi=Pr[Mi(D) = 1]andp′
i=Pr[Mi(D′) = 1]are the output
probabilities of the mechanism MionD,D′. To design a good noise function γinDaRRM, we start by
deriving conditions for a γfunction such that DaRRMγis(mϵ,δ )-differentially private in Lemma 3.4 (see a
full proof in Appendix A.5).
7Published in Transactions on Machine Learning Research (November/2024)
Lemma 3.4 (γprivacy condition) .Consider using DaRRM(Algorithm 1) to solve Problem 1.1, let αl=
Pr[L(D) =l]andα′
l=Pr[L(D′) =l], whereDandD′are adjacent datasets and l∈{0,...,K}. For a noise
functionγ:{0,1,...,K}→[0,1]such thatγ(l) =γ(K−l),∀l,DaRRMγis(mϵ,δ )-differentially private if
and only if for all αl,α′
l, the following holds,
f(p1,...,pK,p′
1,...,p′
K;γ)≤emϵ−1 + 2δ (1)
wherefis called the privacy cost objective and
f(p1,...,pK,p′
1,...,p′
K;γ) :=K−1
2/summationdisplay
l=0(emϵα′
l−αl)·γ(l) +K/summationdisplay
l=K+1
2(αl−emϵα′
l)·γ(l)
4 Provable Privacy Amplification
We theoretically demonstrate that privacy is provably amplified under improved design of γin our DaRRM
framework. Specifically, we show when the mechanisms are i.i.d. and δ= 0, we gain privacy amplification by
a factor of 2 compared to the naïve subsampling baseline by carefully designing γ.
Theorem 4.1 (Provable Privacy Amplification by 2) .Consider using DaRRM(Algorithm 1) to solve
Problem 1.1, with i.i.d. mechanisms {Mi}K
i=1, i.e.,pi=p,p′
i=p′,∀i∈[K], the privacy allowance m∈[K]
andδ= ∆ = 0. Let the noise function γ:{0,1,...,K}→[0,1]be that:
ifm≥K+1
2,γ(l) = 1and ifm≤K−1
2,
γ(l) =/braceleftigg
1−2h(l)∀l≤K−1
2
2h(l)−1∀l≥K+1
2
whereh(l) =/summationtext2m−1
i=m(l
i)(K−l
2m−1−i)
(K
2m−1), then DaRRMγismϵ-differentially private.
Interpretation. First, when m≤K−1
2is small, the γ(l)in Theorem 4.1 corresponds to outputting the
majority based on subsampling 2m−1outcomes, from Lemma 3.1. However, the subsampling baseline,
whose privacy loss is reasoned through simple composition, would have indicated that one can only output
the majority based on moutcomes, therefore implying a 2x privacy gain. When m≥K+1
2, the above theorem
indicates that we can set a constant γ= 1, which implies we are optimally outputting the true majority with
no noise while still surprisingly ensuring mϵprivacy.
Intuition. This 2x privacy gain is intuitively possible because the majority is only dependent on half of the
mechanisms’ outputs, therefore the privacy leakage is also halved. To see this, we start by analyzing the privacy
cost objective in Eq. 31, where with a careful analysis of its gradient, we show that the maximum indeed
occurs (p∗,p′∗) = (0,0)whenγsatisfies certain conditions. Now, when (p∗,p′∗)→0, note that the probability
ratio of outputting 1with 2m−1outcomes is approximately emϵ, where dependence on mfollows because the
probability of outputting 1is dominated by the probability that exactly mmechanisms output 1. To rigorize
this, we derive sufficient conditions for γfunctions that satisfy max (p,p′)f(p,p′;γ) =f(0,0;γ)≤emϵ−1as
indicated by Lemma 3.4, to ensure DaRRMto bemϵ-differentially private and a more detailed overview and
the full proof can be found in Appendix B.
5 Optimizing the Noise Function γin DaRRM
Theoretically designing γand extending privacy amplification results to the δ>0case is difficult and it is
likely that our crafted γis far from optimal. On the other hand, one can optimize for such γ∗that maximizes
the utility but this involves solving a “Semi-infinite Programming” problem, due to the infinitely many
privacy constraints, which are the constraints in the optimization problem necessary to ensure DaRRMwith
the optimized γsatisfy a given privacy loss. Solving a “Semi-infinite Programming” problem in general is
non-tractable, but we show that in our specific setting this is in fact tractable, proposing a novel learning
8Published in Transactions on Machine Learning Research (November/2024)
approach based on DaRRMthat can optimize the noise function to maximize the utility. To the best of our
knowledge, such optimization, presented as follows, is the first of its kind:
min
γ∈[0,1]K+1Ep1,p2,...,pK∼T[E(DaRRMγ)] (2)
s.t. max
{(pi,p′
i)∈Fi}K
i=1f(p1,...,pK,p′
1,...,p′
K;γ)≤emϵ−1 + 2δ (3)
γ(l) =γ(K−l),∀l∈{0,1,...,K}
wherefis the privacy cost objective as defined in Lemma 3.4, Fiis the feasible region where (pi,p′
i)lies due
to each mechanism Mibeingϵ-differentially private. Observe that since γis symmetric aroundK
2, we only
need to optimizeK+1
2variables instead of K+ 1variables.Tis the distribution from which p1,...,pKare
drawn. We want to stress that no prior knowledge about the dataset or the amount of consensus among
the private mechanisms is required to use our optimization framework. When there is no prior knowledge
aboutp1,...,pK,Tis set to be the uniform distribution for maximizing the expected utility. Note the above
optimization problem also enables the flexibility of incorporating prior knowledge about the mechanisms by
choosing a prior distribution Tto further improve the utility.
Optimizing Over All Algorithms. We want to stress that by solving the above optimization problem,
we are indeed optimizing over allalgorithms for maximal utility, since we show in Lemma 3.3 DaRRMthat
captures all reasonable algorithms computing a private majority.
Linear Optimization Objective. Perhaps surprisingly, it turns out that optimizing for γ∗is a Linear
Programming (LP) problem! Indeed, after expanding the optimization objective in Eq. 2 by the utility
definition (see Definition 2.4), optimizing the above objective is essentially same as optimizing:
min
γ∈[0,1]K+1−1
2K/summationdisplay
l=K+1
2Ep1,p2,...,pK∼T[(αl−αK−l)]·γ(l)
whereαl=Pr[L(D) =l],∀l∈{0,1,...,K}and observeL(D)∼PoissonBinomial (p1,...,pK). The above
objective is linear in γ. See a full derivation in Appendix C.1.
Although taking the expectation over p1,...,pKinvolves integrating over Kvariables and this can be
computationally expensive, we discuss how to formulate a computationally efficient approximation of the
objective in Appendix C.2, which we later use in the experiments. Note that the objective only for maximizing
the utility and hence approximating the objective does not affect the privacy guarantee.
Reducing Infinitely Many Constraints to A Polynomial Set. The constraints in the optimization
problem (Eq. 3) is what makes sure the output of DaRRMγismϵ-differentially private. We thus call them
the privacy constraints . Note that the privacy constraints are linear in γ.
Though it appears we need to solve for infinitely many such privacy constraints since pi’s andp′
i’s are
continuous, we show that through a structural understanding of DaRRM, we can reduce the number of privacy
constraints from infinitely many to exponentially many, and further to a polynomial set. First, we observe
the privacy cost objective fis linear in each independent pair of (pi,p′
i)fixing all (pj,p′
j),∀j̸=i, and hence
finding the worst case probabilities in (pi,p′
i)given anyγ,(p∗
i,p′∗
i) =arg max(pi,p′
i)f(p1,...,pK,p′
1,...,p′
K;γ)
is a linear programming (LP) problem. Furthermore, since piandp′
iare the probability of outputting 1 from
thei-th(ϵ,∆)-differentially private mechanism Mion adjacent datasets, by definition, they are close and
lie in a feasible region Fi, which we show has 8 corners if δ>0(and only 4 corners if δ= 0). This implies
(p∗
i,p′∗
i)only happens at one of the corners of Fi, and hence the number of constraints reduces to K8(andK4
ifδ= 0). Second, observe that αlandα′
lin the privacy cost objective fare the pmf of two Poisson Binomial
distributions at l∈{0,...,K}. Notice that the Poisson Binomial is invariant under the permutation of its
parameters, i.e. PoissonBinomial (p1,...,pK)has the same distribution as PoissonBinomial (π(p1,...,pK)),
under some permutation π. Based on this observation, we show the number of constraints can be further
reduced to O(K7)ifδ >0(andO(K3)ifδ= 0). We formalize the two-step reduction of the number of
privacy constraints in Lemma 5.1 as follows. See a full proof in Appendix C.3.1
1Practical Limitation. Although the number of constraints is polynomial in Kand optimizing γinDaRRMis an LP,
O(K7)can still make the number of constraints intractably large when Kis large. In practice, we observe with the Gurobi
9Published in Transactions on Machine Learning Research (November/2024)
Lemma 5.1. Consider using DaRRM(Algorithm 1) to solve Problem 1.1 and let fbe the privacy cost
objective as defined in Lemma 3.4. Given an arbitrary noise function γ, let the worst case probabilities be
(p∗
1,...,p∗
K,p′∗
1,...,p′∗
K) = arg max{(pi,p′
i)}K
i=1f(p1,...,pK,p′
1,...,p′
K;γ).
(p∗
1,...,p∗
K,p′∗
1,...,p′∗
K) = arg max
{(pi,p′
i)}K
i=1f(p1,...,pK,p′
1,...,p′
K;γ)
Then, each pair (p∗
i,p′∗
i),∀i∈[K]satisfies
(p∗
i,p′∗
i)∈{(0,0),(1,1),(0,∆),(∆,0),(1−∆,1),
(1,1−∆),(eϵ+ ∆
eϵ+ 1,1−∆
eϵ+ 1),(1−∆
eϵ+ 1,eϵ+ ∆
eϵ+ 1)}
Furthermore, when δ > 0, there exists a finite vector set Pof sizeO(K7)such that if β=
max{(pi,p′
i)}K
i=1∈Pf(p1,...,pK,p′
1,...,p′
K;γ), thenf(p∗
1,...,p∗
K,p′∗
1,...,p′∗
K;γ)≤β. Whenδ= 0, the size of
Pcan be reduced to O(K3).
6 Experiments
We empirically solve2the above optimization problem (Eq. 2) using the Gurobi3solver and first present the
shape of the optimized γfunction, which we call γopt, and its utility in Section 6.1. Then, we demonstrate
the compelling effectiveness of DaRRMwith an optimized γfunction, i.e., DaRRMγopt, in ensembling labels
for private prediction from private teachers through the application of semi-supervised knowledge transfer for
private image classification in Section 6.2.
6.1 Optimized γin Simulations
00.51m = 1 m = 3
0 5.5 1100.51m = 5
0 5.5 11m = 7
Support l{0,1,...,K}
 values
Shape of  functions
0.00.10.2m = 1
0.00.1m = 3
0.000.050.10m = 5
0.000.05m = 7
 functions
Error(DaRRM)
Figure 2: Plots of the shape and E(DaRRMγ)of different γfunctions: the optimized γopt
, and the baselines γSub(corresponding to subsampling) and γconst(corresponding to RR). Here,
K= 11,m∈{1,3,5,7},ϵ= 0.1,∆ = 10−5andδ= 1−(1−∆)m≈m∆.
We compare the shape and the error E(DaRRMγ)of different γfunctions: an optimized γoptand the
subsampling γSubas in Lemma 3.14. We also compare against pconstin the classical baseline RR(see
optimizer, one can optimize γforK≤41on a laptop if δ>0. But ifδ= 0, since the number of privacy constraints is O(K3),
one can optimize for Kover 100.
2All code for the experiments can be found at https://anonymous.4open.science/r/OptimizedPrivateMajority-CF50
3https://www.gurobi.com/
4Note the subsampling mechanism from Section 4, which enjoys a privacy amplification by a factor of 2, only applies to pure
differential privacy settings (i.e., when ∆ =δ= 0). However, we focus on the more general approximate differential privacy
10Published in Transactions on Machine Learning Research (November/2024)
Section A.1) and E(RR). Here,pconstcan be viewed as a constant noise function γconst(l) =pconst,∀l∈
{0,1,...,K}; andE(RR)is the same asE(DaRRMγconst).
We present the results with K= 11,ϵ= 0.1,∆ = 10−5andm∈{1,3,5,7}. We assume there is no prior
knowledge about the mechanisms {Mi}K
i=1, and set the prior distribution from which pi’s are drawn,T, to be
the uniform distribution, in the optimization objective (Eq. 2) searching for γopt. To ensure a fair comparison
against the subsampling baseline, we set δto be the one by m-fold general composition (see Theorem 2.3),
which in this case, is δ= 1−(1−∆)m≈m∆. We plot each γfunctions over the support {0,1,...,K}and
the corresponding error of each algorithm in Figure 2.
Discussion. In summary, at m= 1, the optimized noise function γoptoverlaps with γsubwhich corresponds
to the subsampling baseline. This agrees with our lower bound on the error in Lemma 3.2, which implies that
atm= 1, subsampling indeed gives the optimal error. When m> 1, the optimized noise function γopthas
the highest probability of outputting the true majority over the support than the γfunctions corresponding
to the baselines. This implies DaRRMγopthas the lowest error (and hence, highest utility), which is verified on
the bottom set of plots. More results on comparing the DaRRMγoptoptimized under the uniform Tagainst
the baselines by general composition (Theorem 2.3) and in pure differential privacy settings (i.e., ∆ =δ= 0)
for largeKandmcan be found in Appendix D.1.1 and D.1.2. Furthermore, we include results optimizing γ
using a non-uniform Tprior in Appendix D.1.3.
6.2 Private Semi-Supervised Knowledge Transfer
Dataset MNIST
# QueriesGNMax
(Baseline)DaRRMγSub
(Baseline)DaRRMγopt
(Ours)
Q= 20 0.63 (0.09) 0.76 (0.09) 0.79 (0.09)
Q= 50 0.66 (0.06) 0.75 (0.06) 0.79 (0.05)
Q= 100 0.64 (0.04) 0.76 (0.04) 0.80 (0.04)Dataset Fashion-MNIST
# QueriesGNMax
(Baseline)DaRRMγSub
(Baseline)DaRRMγopt
(Ours)
Q= 20 0.65 (0.11) 0.90 (0.07) 0.96 (0.03)
Q= 50 0.59 (0.06) 0.94 (0.03) 0.96 (0.02)
Q= 100 0.64 (0.04) 0.93 (0.02) 0.96 (0.02)
Table 1: Accuracy of the predicted labels of Qquery samples on datasets MNIST(on the left) and
Fashion-MNIST (on the right). We report the mean and one std. in parentheses over 10 random draws of the
query samples from the test dataset. Note each prediction on the query sample is (ϵquery,δquery )
-differentially private. With the same per query privacy loss (and hence the same total privacy loss over Q
samples), DaRRMγoptachieves the highest accuracy compared to the other two baselines.
Semi-supervised Knowledge Transfer. We apply our DaRRMframework in the application of semi-
supervised knowledge transfer for private image classification. We follow a similar setup as in PATE Papernot
et al. (2017; 2018), where one trains Kteachers, each on a subset of a sensitive dataset, and at the inference
time, queries the teachers for the majority of their votes, i.e., the predicted labels, of a test sample. Each
time the teachers are queried, there is a privacy loss, and we focus on this private prediction subroutine in
this section. To limit the total privacy loss over all queries, the student model is also trained on a public
dataset without labels. The student model queries the labels of a small portion of the samples in this dataset
from the teachers and is then trained using semi-supervised learning algorithms on both the labeled and
unlabeled samples from the public dataset.
Baselines. We want the privacy loss per query of a test sample to the teachers to be (ϵquery,δquery ). This
can be achieved via two ways: 1) Train Knon-private teachers, add Gaussian noise to the number of predicted
labels from the teachers in each output class, and output the majority of the noisy votes. This is exactly the
GNMaxalgorithm from PATE Papernot et al. (2018). 2) Train K(ϵ,∆)-differentially private teachers and
output the majority of the teachers’ votes by adding a smaller amount of noise. This can be computed using
DaRRMwith an appropriate noise function γ. We compare the performance of GNMaxandDaRRMwith two
γfunctions:γopt(i.e., the optimized γ), andγSub(i.e., the subsampling baseline). The overall privacy loss
overQqueries to the teachers can be computed by general composition (Theorem 2.3).
settings (with ∆>0) in the experiments, and hence, the subsampling baseline we consider throughout this section is the
basic version without privacy amplification. To see how the subsampling mechanism from Section 4 with privacy amplification
compares against the other algorithms, please refer to Appendix D.1.2.
11Published in Transactions on Machine Learning Research (November/2024)
Experiment Setup. We use samples from two randomly chosen classes — class 5 and 8 — from the MNIST
and Fashion-MNIST datasets to form our training and testing datasets. Our MNISThas a total of 11272
training samples and 1866testing samples; our Fashion-MNIST has10000training samples and 2000testing
samples. We train K= 11teachers on equally divided subsets of the training datasets. Each teacher is a
CNN model. The non-private and private teachers are trained using SGDandDP-SGD Abadi et al. (2016),
respectively, for 5 epochs. DaRRMSetup:The Gaussian noise in DP-SGD has zero mean and std. σdpsgd = 12;
the gradient norm clipping threshold is C= 1. This results in each private teacher, trained on MNISTand
Fashion-MNIST , being (ϵ,∆) = (0.0892,10−4)and(0.0852,10−4)-differentially private, respectively, after
5 epochs. We set the privacy allowance m= 35and the privacy loss per query is then computed using
general composition under m-fold, which give the same privacy loss in the high privacy regime, resulting in
(ϵquery,δquery ) = (0.2676,0.0003)onMNISTand(0.2556,0.0003)onFashion-MNIST .GNMaxSetup:We now
compute the std. σof the Gaussian noise used by GNMaxto achieve a per-query privacy loss of (mϵ,m ∆), as
in the DaRRMsetup. We optimize σaccording to the Renyi differential privacy loss bound of Gaussian noise.
Although Papernot et al. (2018) gives a potentially tighter data-dependent privacy loss bound for majority
ensembling non-private teachers, we found when Kand the number of output classes are small as in our case,
even if all teachers agree on a single output class, the condition of the data-dependent bound is not satisfied.
Hence, we only use the privacy loss bound of Gaussian noise here to set σinGNMax. See Appendix D.2.1 for
more details, including the σvalues and other parameters. Finally, the per sample privacy loss and the total
privacy loss over Qqueries, which is computed by advanced composition, are reported in Table 9.
The testing dataset is treated as the public dataset on which one trains a student model. Papernot et al.
(2018) empirically shows querying Q= 1%Nsamples from a public dataset of size Nsuffices to train a
student model with a good performance. Therefore, we pick Q∈{20,50,100}. We repeat the selection of Q
samples 10 times and report the mean test accuracy with one std. in parentheses in Table 1. The Qqueries
serve as the labeled samples in training the student model. The higher the accuracy of the labels from the
queries, the better the final performance of the student model. We skip the actual training of the student
model using semi-supervised learning algorithms here.
Dataset # QueriesPrivacy loss
per query
(ϵquery,δquery )Total privacy loss
overQqueries
(ϵtotal,δtotal)
MNISTQ= 20
(0.2676,0.0003)(5.352,0.006)
Q= 50 (9.901,0.015)
Q= 100 (15.044,0.030)
Fashion
MNISTQ= 20
(0.2556,0.0003)(5.112,0.006)
Q= 50 (9.382,0.015)
Q= 100 (14.219,0.030)
Table 2: The privacy loss per query to the teachers and the total privacy loss over Qqueries. Note the total
privacy loss is computed by general composition (see Theorem 2.3), where we set δ′= 0.0001.
Discussion. Table 1 shows DaRRMγoptachieves the highest accuracy (i.e., utility) compared to the two
baselines on both datasets. First, comparing to DaRRMγSub, we verify that subsampling does not achieve a
tight privacy-utility tradeoff, and we can optimize the noise function γinDaRRMto maximize the utility
given a target privacy loss. Second, comparing to GNMax, the result shows there are regimes where ensembling
private teachers gives a higher utility than directly ensembling non-private teachers, assuming the outputs
in both settings have the same privacy loss. Intuitively, this is because ensembling private teachers adds
fine-grained noise during both training the teachers and aggregation of teachers’ votes, while ensembling
non-private teachers adds a coarser amount of noise only to the teachers’ outputs. This further motivates
5Here, we present results with privacy allowance m= 3because we think this is a more interesting case. m= 1is less
interesting, since one cannot get improvement compared to the subsampling baseline. mclose to aK
2≈5is also less interesting,
as this case seems too easy for our proposed method (the optimized γfunction is very close to 1, meaning very little noise
needs to be added in this case). Hence, we pick m= 3, which is a case when improvement is possible, and is also potentially
challenging for our optimization framework. This is also realistic as most applications would only want to tolerate a constant
privacy overhead. See more results with different privacy allowance m’s in this setting in Appendix D.2.2.
12Published in Transactions on Machine Learning Research (November/2024)
private prediction from private teachers and the practical usage of DaRRM, in addition to the need of
aggregating private teachers in federated learning settings with an honest-but-curious server.
7 Conclusion
In computing a private majority from Kprivate mechanisms, we propose the DaRRMframework, which
is provably general, with a customizable γfunction. We show a privacy amplification by a factor of 2
in the i.i.d. mechanisms and a pure differential privacy setting. For the general setting, we propose an
tractable optimization algorithm that maximizes utility while ensuring privacy guarantees. Furthermore, we
demonstrate the empirical effectiveness of DaRRMwith an optimized γ. We hope that this work inspires
more research on the intersection of privacy frameworks and optimization.
13Published in Transactions on Machine Learning Research (November/2024)
References
Martin Abadi, Andy Chu, Ian Goodfellow, H Brendan McMahan, Ilya Mironov, Kunal Talwar, and Li Zhang.
Deep learning with differential privacy. In Proceedings of the 2016 ACM SIGSAC conference on computer
and communications security , pp. 308–318, 2016.
Raef Bassily, Om Thakkar, and Abhradeep Thakurta. Model-agnostic private learning via stability. arXiv
preprint arXiv:1803.05101 , 2018.
Lucas Bourtoule, Varun Chandrasekaran, Christopher A Choquette-Choo, Hengrui Jia, Adelin Travers, Baiwu
Zhang, David Lie, and Nicolas Papernot. Machine unlearning. In 2021 IEEE Symposium on Security and
Privacy (SP) , pp. 141–159. IEEE, 2021.
Graham Cormode, Somesh Jha, Tejas Kulkarni, Ninghui Li, Divesh Srivastava, and Tianhao Wang. Privacy
at scale: Local differential privacy in practice. In Proceedings of the 2018 International Conference on
Management of Data , pp. 1655–1658, 2018.
Jinshuo Dong, David Durfee, and Ryan Rogers. Optimal differential privacy composition for exponential
mechanisms. In International Conference on Machine Learning , pp. 2597–2606. PMLR, 2020.
David Durfee and Ryan M Rogers. Practical differentially private top-k selection with pay-what-you-get
composition. Advances in Neural Information Processing Systems , 32, 2019.
Cynthia Dwork and Vitaly Feldman. Privacy-preserving prediction. In Conference On Learning Theory , pp.
1693–1702. PMLR, 2018.
Cynthia Dwork and Jing Lei. Differential privacy and robust statistics. In Proceedings of the forty-first
annual ACM symposium on Theory of computing , pp. 371–380, 2009.
Cynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam Smith. Calibrating noise to sensitivity in private
data analysis. In Theory of cryptography conference , pp. 265–284. Springer, 2006.
Cynthia Dwork, Aaron Roth, et al. The algorithmic foundations of differential privacy. Found. Trends Theor.
Comput. Sci. , 9(3-4):211–407, 2014.
ÚlfarErlingsson, VasylPihur, andAleksandra Korolova. Rappor: Randomizedaggregatable privacy-preserving
ordinal response. In Proceedings of the 2014 ACM SIGSAC Conference on Computer and Communications
Security, CCS ’14, pp. 1054–1067, New York, NY, USA, 2014. Association for Computing Machinery. ISBN
9781450329576. doi: 10.1145/2660267.2660348. URL https://doi.org/10.1145/2660267.2660348 .
Úlfar Erlingsson, Vitaly Feldman, Ilya Mironov, Ananth Raghunathan, Kunal Talwar, and Abhradeep
Thakurta. Amplification by shuffling: From local to central differential privacy via anonymity. In
Proceedings of the Thirtieth Annual ACM-SIAM Symposium on Discrete Algorithms , pp. 2468–2479. SIAM,
2019.
Vitaly Feldman, Ilya Mironov, Kunal Talwar, and Abhradeep Thakurta. Privacy amplification by iteration.
In2018 IEEE 59th Annual Symposium on Foundations of Computer Science (FOCS) , pp. 521–532. IEEE,
2018.
Quan Geng and Pramod Viswanath. The optimal noise-adding mechanism in differential privacy. IEEE
Transactions on Information Theory , 62(2):925–951, 2015.
Naoise Holohan, Douglas J. Leith, and Oliver Mason. Optimal differentially private mechanisms for randomised
response. IEEE Transactions on Information Forensics and Security , 12(11):2726–2735, November 2017.
ISSN 1556-6021. doi: 10.1109/tifs.2017.2718487. URL http://dx.doi.org/10.1109/TIFS.2017.2718487 .
Junjie Jia and Wanyong Qiu. Research on an ensemble classification algorithm based on differential privacy.
IEEE Access , 8:93499–93513, 2020. doi: 10.1109/ACCESS.2020.2995058.
Peter Kairouz, Sewoong Oh, and Pramod Viswanath. The composition theorem for differential privacy. In
International conference on machine learning , pp. 1376–1385. PMLR, 2015.
14Published in Transactions on Machine Learning Research (November/2024)
Jakub Konečn` y, H Brendan McMahan, Felix X Yu, Peter Richtárik, Ananda Theertha Suresh, and Dave Bacon.
Federated learning: Strategies for improving communication efficiency. arXiv preprint arXiv:1610.05492 ,
2016.
Jingcheng Liu and Kunal Talwar. Private selection from private candidates. In Proceedings of the 51st
Annual ACM SIGACT Symposium on Theory of Computing , STOC 2019, pp. 298–309, New York, NY,
USA, 2019. Association for Computing Machinery. ISBN 9781450367059. doi: 10.1145/3313276.3316377.
URL https://doi.org/10.1145/3313276.3316377 .
Zhongfeng Liu, Yun Li, and Wei Ji. Differential private ensemble feature selection. In 2018 International
Joint Conference on Neural Networks (IJCNN) , pp. 1–6, 2018. doi: 10.1109/IJCNN.2018.8489308.
Fatemehsadat Mireshghallah, Mohammadkazem Taram, Prakash Ramrakhyani, Ali Jalali, Dean Tullsen, and
Hadi Esmaeilzadeh. Shredder: Learning noise distributions to protect inference privacy. In Proceedings
of the Twenty-Fifth International Conference on Architectural Support for Programming Languages and
Operating Systems , pp. 3–18, 2020.
Moni Naor, Kobbi Nissim, Uri Stemmer, and Chao Yan. Private everlasting prediction. arXiv preprint
arXiv:2305.09579 , 2023.
Mohammad Naseri, Jamie Hayes, and Emiliano De Cristofaro. Local and central differential privacy for
robustness and privacy in federated learning. arXiv preprint arXiv:2009.03561 , 2020.
Kobbi Nissim, Sofya Raskhodnikova, and Adam Smith. Smooth sensitivity and sampling in private data
analysis. In Proceedings of the thirty-ninth annual ACM symposium on Theory of computing , pp. 75–84,
2007.
Nicolas Papernot and Thomas Steinke. Hyperparameter tuning with renyi differential privacy. In International
Conference on Learning Representations , 2022. URL https://openreview.net/forum?id=-70L8lpp9DF .
Nicolas Papernot, Martin Abadi, Ulfar Erlingsson, Ian Goodfellow, and Kunal Talwar. Semi-supervised
knowledge transfer for deep learning from private training data. In Proceedings of the International
Conference on Learning Representations , 2017. URL https://arxiv.org/abs/1610.05755 .
Nicolas Papernot, Shuang Song, Ilya Mironov, Ananth Raghunathan, Kunal Talwar, and Úlfar Erlingsson.
Scalable private learning with pate. arXiv preprint arXiv:1802.08908 , 2018.
Omer Sagi and Lior Rokach. Ensemble learning: A survey. Wiley Interdisciplinary Reviews: Data Mining
and Knowledge Discovery , 8(4):e1249, 2018.
Uri Stemmer. Private truly-everlasting robust-prediction. arXiv preprint arXiv:2401.04311 , 2024.
Abhradeep Guha Thakurta and Adam Smith. Differentially private feature selection via stability arguments,
and the robustness of the lasso. In Conference on Learning Theory , pp. 819–850. PMLR, 2013.
Salil Vadhan. The Complexity of Differential Privacy , pp. 347–450. Springer International Publishing, Cham,
2017. doi: 10.1007/978-3-319-57048-8_7. URL https://doi.org/10.1007/978-3-319-57048-8_7 .
Laurens van der Maaten and Awni Hannun. The trade-offs of private prediction, 2020.
Ming Xiang and Lili Su. $\beta$-stochastic sign SGD: A byzantine resilient and differentially private gradient
compressor for federated learning, 2023. URL https://openreview.net/forum?id=oVPqFCI1g7q .
Tao Xiang, Yang Li, Xiaoguo Li, Shigang Zhong, and Shui Yu. Collaborative ensemble learning under
differential privacy. Web Intelligence , 16:73–87, 03 2018. doi: 10.3233/WEB-180374.
Ying-Ying Zhang, Teng-Zhong Rong, and Man-Man Li. Expectation identity for the binomial distribution
and its application in the calculations of high-order binomial moments. Communications in Statistics
- Theory and Methods , 48(22):5467–5476, 2019. doi: 10.1080/03610926.2018.1435818. URL https:
//doi.org/10.1080/03610926.2018.1435818 .
Yuqing Zhu, Xuandong Zhao, Chuan Guo, and Yu-Xiang Wang. " private prediction strikes back!”private
kernelized nearest neighbors with individual renyi filter. arXiv preprint arXiv:2306.07381 , 2023.
15Published in Transactions on Machine Learning Research (November/2024)
A Details of Section 3
A.1 Randomized Response with Constant Probability pconst
Algorithm 2 Randomized Response Majority ( RR)
1:Input:K(ϵ,∆)-DP mechanisms {Mi}K
i=1, noise function γ:{0,...,K}→ [0,1], datasetD, privacy
allowance 1≤m≤K, failure probability δ≥∆≥0
2:Output: (mϵ,δ )-DP majority vote of {Mi}K
i=1
3:Compute a constant probability pconst∈[0,1]
4:Flip thepconst- biased coin
5:ifHead (with probability pconst)then
6:S={S1,..,Sk}, whereSi∼Mi(D)
7:L=/summationtextK
i=1Si
8:Output I{1
KL≥1
2}
9:else
10:Output 0/1with equal probability
11:end if
We show the magnitude of pconstinRR(Algorithm 2) to solve Problem 1.1, such that the output is (mϵ,δ )-DP,
in Lemma A.1.
Lemma A.1. Consider using RR(Algorithm 2) to solve Problem 1.1. Let the majority of K(ϵ,∆)-
differentially private mechanisms be (τϵ,λ)-differentially private, where τ∈[1,K]andλ∈[0,1)are computed
by simple composition (Theorem 2.2) or general composition (Theorem 2.3). If
pconst≤emϵ−1 + 2δ
2(eτϵ−emϵ+(1+emϵ)λ)
eτϵ+1+emϵ−1(4)
then RRis(mϵ,δ )-differentially private.
Proof of Lemma A.1. Letx∈{0,1}denote the output of RR. Letqx=Pr[L(D) =x]andq′
x=Pr[L(D′) =x],
whereL(D) =/summationtextK
i=1Mi(D),L(D′) =/summationtextK
i=1Mi(D′)andD,D′are adjacent datasets. Recall each mechanism
Miis(ϵ,∆)-differentially private, and the majority of the outputs of {Mi}K
i=1is(τϵ,λ)-differentially private.
When ∆ = 0, using simple composition, τ=Kandλ= 0. When ∆>0, using general composition τ≈√
K
andλ≈K∆. By definition of differential privacy (Definition 2.1), all of the following four constraints on
qx,q′
xapply:
qx≤eτϵq′
x+λ,and 1−q′
x≤eτϵ(1−qx) +λ
q′
x≤eτϵqx+λ,and 1−qx≤eτϵ(1−q′
x) +λ
To ensure RRis(mϵ,δ )-differentially private, pconstneeds to be such that for all possible qx,q′
x∈[0,1],
Pr[RR(D) =x]≤emϵPr[RR(D′) =x] +δ (5)
pconst·qx+1
2(1−pconst)≤emϵ(pconst·q′
x+1
2(1−pconst)) +δ (6)
(qx−emϵq′
x+1
2emϵ−1
2)·pconst≤1
2emϵ−1
2+δ (7)
Leth(qx,q′
x) :=qx−emϵq′
x+1
2emϵ−1
2. The above inequality of pconst(Eq. 7) needs to hold for worst case
output probabilities q∗
x,q′∗
xthat cause the maximum privacy loss. That is, pconstneeds to satisfy
pconst·maxqx,q′xh(qx,q′
x)≤1
2emϵ−1
2+δ (8)
16Published in Transactions on Machine Learning Research (November/2024)
To find the worst case output probabilities, we solve the following Linear Programming (LP) problem:
Objective: max
qx,q′xh(qx,q′
x) :=qx−emϵq′
x+1
2emϵ−1
2(9)
Subject to: 0≤qx≤1,0≤q′
x≤1 (10)
qx≤eτϵq′
x+λ,1−q′
x≤eτϵ(1−qx) +λ (11)
q′
x≤eτϵqx+λ,1−qx≤eτϵ(1−q′
x) +λ (12)
Figure 3: A visualization of the above LP problem.
The optimum of any LP problem is at the corners of the feasible region, which is bounded by the
optimization constraints. We plot the feasible region Fand the objective of the above LP prob-
lem in Figure 3. Here, (q∗
x,q′∗
x) = arg maxqx,q′
xh(qx,q′
x)∈ {(0,0),(1,1),(0,λ),(λ,0),(1−λ,1),(1,1−
λ),(1−λ
eτϵ+1,eτϵ+λ
eτϵ+1),(eτϵ+λ
eτϵ+1,1−λ
eτϵ+1)}. The optimum of the LP problem – that is, the worse case probabili-
tiesq∗
x,q′∗
x– is,
q∗
x=eτϵ+λ
eτϵ+ 1, q′∗
x=1−λ
eτϵ+ 1(13)
By Eq. 8,
pconst·/parenleftigeτϵ+λ
eτϵ+ 1−emϵ1−λ
eτϵ+ 1+1
2emϵ−1
2/parenrightig
≤1
2(emϵ−1) +δ (14)
pconst·/parenleftigeτϵ−emϵ+ (1 +emϵ)λ
eτϵ+ 1+1
2(emϵ−1)/parenrightig
≤1
2(emϵ−1) +δ (15)
pconst≤emϵ−1 + 2δ
2(eτϵ−emϵ+(1+emϵ)λ)
eτϵ+1+emϵ−1(16)
For smallm,ϵ,K, using the approximation ey≈1 +yand thatτϵ< 2,
pconst≈mϵ+ 2δ
2(τϵ−mϵ+(2+mϵ)λ)
τϵ+2+mϵ≈mϵ+ 2δ
τϵ+ (2 +mϵ)λ(17)
In the pure differential privacy setting, δ= 0,λ= 0,τ=K, and sopconst≈m
K; and in the approximate
differential privacy setting, λ≈0,δ≈0,τ≈√
K, and sopconst≈m√
K.
17Published in Transactions on Machine Learning Research (November/2024)
Algorithm 3 Subsampling Majority ( SubMaj)
1:Input:K(ϵ,∆)-DP mechanisms {Mi}K
i=1, noise function γ:{0,...,K}→ [0,1], datasetD, privacy
allowance 1≤m≤K, failure probability δ≥∆≥0
2:Output: (mϵ,δ )-DP majority vote of {Mi}K
i=1
3:S={S1,..,Sk}, whereSi∼Mi(D)
4:Jm←mindices chosen uniformly at random from [K]without replacement
5:/hatwideL=/summationtext
j∈JSj
6:Output I{1
m/hatwideL≥1
2}
A.2 Proof of Lemma 3.1
Lemma A.2 (Restatement of Lemma 3.1) .Consider Problem 1.1, with the privacy allowance m∈[K].
Consider the data-dependent algorithm that computes L(D)and then applies RRwith probability pγ. If
pγ=γSub(l), wherel∈{0,1,...,K}is the value ofL(D), i.e., the (random) sum of observed outcomes on
datasetD, andγSub:{0,1,...,K}→[0,1]is
γSub(l) =γSub(K−l)
=

1−2/summationtextm
j=m+1
2(l
j)(K−l
m−j)
(K
m)ifmis odd
1−2/summationtextm
j=m
2+1(l
j)(K−l
m−j)
(K
m)−(lm
2)(K−l
m
2)
(K
m)ifmis even
then the majority of mout ofKsubsampled mechanisms without replacement and the output of our data-
dependent RRalgorithm have the same distribution.
Proof of Lemma 3.1. LetL=/summationtextK
i=1Sibe the sum of observed outcomes from Kmechanisms. Following
Algorithm3,Jmdenotesthe mindiceschosenuniformlyatrandomfrom [K]withoutreplacement. Conditioned
onL, notice the output of SubMajfollows a hypergeometric distribution. The output probability of SubMajis
Pr[SubMaj (D) = 1] =K/summationdisplay
l=0Pr[SubMaj (D) = 1|L=l]·Pr[L=l] (18)
=K/summationdisplay
l=0Pr[/summationdisplay
j∈JmSj≥m
2|L=l]·Pr[L=l] (19)
=

/summationtextK
l=0(/summationtextm
j=m+1
2(l
j)(K−l
m−j)
(K
m))·Pr[L=l] ifmis odd
/summationtextK
l=0(/summationtextm
j=m
2+1(l
j)(K−l
m−j)
(K
m)+1
2(lm
2)(K−l
m
2)
(K
m))·Pr[L=l]ifmis even(20)
Consider an arbitrary noise function γSub:{0,1,...,K}→ [0,1]. Let RR-d (D)denote the output of the
data-dependent RR-don datasetD, where RR-dhas thenon-constant probability set by γSub. The output
probability of RRis,
Pr[RR-d (D) = 1] =K/summationdisplay
l=0Pr[RR-d (D) = 1|L=l]·Pr[L=l] (21)
=K/summationdisplay
l=0(γSub(l)·I{l≥K+ 1
2}+1
2(1−γSub(l)))·Pr[L=l] (22)
We want Pr[RR-d (D) = 1] = Pr[ Submaj (D) = 1].
18Published in Transactions on Machine Learning Research (November/2024)
Ifmis odd, for any l≤K−1
2, this is
1
2(1−γSub(l)) =m/summationdisplay
j=m+1
2/parenleftbigl
j/parenrightbig/parenleftbigK−l
m−j/parenrightbig
/parenleftbigK
m/parenrightbig
⇒γSub(l) = 1−2m/summationdisplay
j=m+1
2/parenleftbigl
j/parenrightbig/parenleftbigK−l
m−j/parenrightbig
/parenleftbigK
m/parenrightbig (23)
and for any l≥K+1
2, this is
1
2+1
2γSub(l) =m/summationdisplay
j=m+1
2/parenleftbigl
j/parenrightbig/parenleftbigK−l
m−j/parenrightbig
/parenleftbigK
m/parenrightbig
⇒γSub(l) = 2m/summationdisplay
j=m+1
2/parenleftbigl
j/parenrightbig/parenleftbigK−l
m−j/parenrightbig
/parenleftbigK
m/parenrightbig−1 (24)
Similarly, if mis even, for any l≤K−1
2, this is
1
2(1−γSub(l)) =m/summationdisplay
j=m
2+1/parenleftbigl
j/parenrightbig/parenleftbigK−l
m−j/parenrightbig
/parenleftbigK
m/parenrightbig+1
2/parenleftbigl
m
2/parenrightbig/parenleftbigK−l
m
2/parenrightbig
/parenleftbigK
m/parenrightbig
⇒γSub(l) = 1−2m/summationdisplay
j=m
2+1/parenleftbigl
j/parenrightbig/parenleftbigK−l
m−j/parenrightbig
/parenleftbigK
m/parenrightbig−/parenleftbigl
m
2/parenrightbig/parenleftbigK−l
m
2/parenrightbig
/parenleftbigK
m/parenrightbig (25)
and for any l≥K+1
2, this is
1
2+1
2γSub(l) =m/summationdisplay
j=m
2+1/parenleftbigl
j/parenrightbig/parenleftbigK−l
m−j/parenrightbig
/parenleftbigK
m/parenrightbig+1
2/parenleftbigl
m
2/parenrightbig/parenleftbigK−l
m
2/parenrightbig
/parenleftbigK
m/parenrightbig
⇒γSub(l) = 2m/summationdisplay
j=m
2+1/parenleftbigl
j/parenrightbig/parenleftbigK−l
m−j/parenrightbig
/parenleftbigK
m/parenrightbig+/parenleftbigl
m
2/parenrightbig/parenleftbigK−l
m
2/parenrightbig
/parenleftbigK
m/parenrightbig−1 (26)
Next, we show the above γSubis indeed symmetric aroundK
2. For anyl≤K−1
2, there isK−l≥K+1
2. Ifm
is odd,
γSub(K−l) = 2m/summationdisplay
j=m+1
2/parenleftbigK−l
j/parenrightbig/parenleftbigl
m−j/parenrightbig
/parenleftbigK
m/parenrightbig−1 = 2/parenleftig
1−m−1
2/summationdisplay
j=1/parenleftbigK−l
j/parenrightbig/parenleftbigl
m−j/parenrightbig
/parenleftbigK
m/parenrightbig/parenrightig
−1
= 1−2m−1
2/summationdisplay
j=1/parenleftbigK−l
j/parenrightbig/parenleftbigl
m−j/parenrightbig
/parenleftbigK
m/parenrightbig = 1−2m/summationdisplay
j=m+1
2/parenleftbigl
j/parenrightbig/parenleftbigK−l
m−j/parenrightbig
/parenleftbigK
m/parenrightbig
=γSub(l) (27)
Similarly, if mis even,
γSub(K−l) = 2m/summationdisplay
j=m
2+1/parenleftbigK−l
j/parenrightbig/parenleftbigl
m−j/parenrightbig
/parenleftbigK
m/parenrightbig +/parenleftbigl
m
2/parenrightbig/parenleftbigK−l
m
2/parenrightbig
/parenleftbigK
m/parenrightbig−1 = 2/parenleftig
1−m
2−1/summationdisplay
j=1/parenleftbigK−l
j/parenrightbig/parenleftbigl
m−j/parenrightbig
/parenleftbigK
m/parenrightbig−1
2/parenleftbigl
m
2/parenrightbig/parenleftbigK−l
m
2/parenrightbig
/parenleftbigK
m/parenrightbig/parenrightig
−1
= 1−2m
2−1/summationdisplay
j=1/parenleftbigK−l
j/parenrightbig/parenleftbigl
m−j/parenrightbig
/parenleftbigK
m/parenrightbig−/parenleftbigl
m
2/parenrightbig/parenleftbigK−l
m
2/parenrightbig
/parenleftbigK
m/parenrightbig = 1−2m/summationdisplay
j=m
2+1/parenleftbigl
j/parenrightbig/parenleftbigK−l
m−j/parenrightbig
/parenleftbigK
m/parenrightbig−/parenleftbigl
m
2/parenrightbig/parenleftbigK−l
m
2/parenrightbig
/parenleftbigK
m/parenrightbig
19Published in Transactions on Machine Learning Research (November/2024)
=γSub(l) (28)
Now, combining Eq. 23, Eq. 24 and Eq. 27, if mis odd, setting γSubas
γSub(l) =γSub(K−l) = 1−2m/summationdisplay
j=m+1
2/parenleftbigl
j/parenrightbig/parenleftbigK−l
m−j/parenrightbig
/parenleftbigK
m/parenrightbig (29)
makes RR-dhave the same output distribution as SubMaj.
Similarly, combining Eq. 25, Eq. 26 and Eq. 28, if mis even, setting γSubas
γSub(l) =γSub(K−l) = 1−2m/summationdisplay
j=m
2+1/parenleftbigl
j/parenrightbig/parenleftbigK−l
m−j/parenrightbig
/parenleftbigK
m/parenrightbig−/parenleftbigl
m
2/parenrightbig/parenleftbigK−l
m
2/parenrightbig
/parenleftbigK
m/parenrightbig (30)
makes RR-dhave the same output distribution as SubMaj.
A.3 Proof of Lemma 3.2
Lemma A.3 (Restatement of Lemma 3.2) .LetAbe an (ϵ,δ)-differentially private algorithm, where ϵ∈(0,1
2)
andδ∈[0,1
2), that computes the majority of K(ϵ,δ)-differentially private mechanisms M1,...,MK, where
Mi:D→{ 0,1}on datasetDandPr[Mi(D) = 1] =pi,∀i∈[K]. Then, the error E(A)≥|Pr[g(S) =
1]−1
K/summationtextK
i=1pi|, whereg(S)is the probability of the true majority output being 1 as defined in Definition 1.1.
Proof.Consider the setting where Mi’s are i.i.d., i.e., Pr[Mi(D) = 1] =p,∀i∈[K]for somep∈[0,1]on any
datasetD. Then, it suffices to show E(A)≥|Pr[g(S)] = 1−p|, because a lower bound in this special case
would indicate a lower bound for the more general case, where pi’s can be different.
Construct a dataset D0andKmechanisms{Mi}K
i=1such that Pr[Mi(D0) = 1] = Pr[Mi(D0) = 0] =1
2and
without loss of generality, we may assume Pr[A(D0) = 1]≤1
2.
Next, we construct a sequence of datasets D1,D2,...,DL, such thatDjandDj+1are neighboring datasets
tha t differ in one entry, for all j∈[L−1], and Pr[Mi(Dj) = 1] =1
2ejϵ+/summationtextj−1
l=0elϵδ,∀i∈[K],∀j∈[L].
ChooseL∈Nsuch that1
2eLϵ+/summationtextL−1
l=0eϵlδ=p, for some 1≥p>1
2.
Now, by definition of differential privacy,
Pr[A(D1) = 1]≤eϵPr[A(D0) = 1] +δ
Pr[A(D2) = 1]≤eϵPr[A(D1) = 1] +δ≤e2ϵPr[A(D0) = 1] +eϵδ+δ
...
Pr[A(DL) = 1]≤eLϵPr[A(D0) = 1] +L−1/summationdisplay
l=0eϵlδ≤eLϵ1
2+L−1/summationdisplay
l=0eϵlδ=p
Since the probability of true majority being 1 on dataset DLisPr[g(S) = 1]≥p>1
2, there is
E(A) =|Pr[g(S) = 1]−Pr[A(DL) = 1]|≥Pr[g(S) = 1]−p
20Published in Transactions on Machine Learning Research (November/2024)
A.4 Proof of Lemma 3.3
Lemma A.4 (Restatement of Lemma 3.3) .LetAbe any randomized algorithm to compute the majority
functiongonSsuch that for allS,Pr[A(S) =g(S)]≥1/2(i.e.Ais at least as good as a random guess).
Then, there exists a a general function γ:{0,1}K+1→[0,1]such that if one sets pγbyγ(S)inDaRRM, the
output distribution of DaRRMγis the same as the output distribution of A.
Proof of Lemma 3.3. For someDand conditioned on S, we see that by definition Pr[DaRRMγ(S) =g(S)] =
γ(S) + (1/2)(1−γ(S)). We want to set γsuch that Pr[DaRRMγ(S) =g(S)] =Pr[A(S) =g(S)]. Therefore,
we setγ(S) = 2 Pr[A(S) =g(S)]−1.
Lastly, we need to justify that γ∈[0,1]. Clearly,γ(S)≤2−1≤1since Pr[A(S) =g(S)]≤1. Note that the
non-negativity follows from assumption.
A.5 Proof of Lemma 3.4
Lemma A.5 (Restatement of Lemma 3.4) .Consider using DaRRM(Algorithm 1) to solve Problem 1.1,
letαl=Pr[L(D) =l]andα′
l=Pr[L(D′) =l], whereDandD′are adjacent datasets and l∈{0,...,K}.
For a noise function γ:{0,1,...,K}→[0,1]such thatγ(l) =γ(K−l),∀l,DaRRMγis(mϵ,δ )-differentially
private if and only if for all αl,α′
l, the following holds,
f(p1,...,pK,p′
1,...,p′
K;γ)≤emϵ−1 + 2δ (31)
wherefis called the privacy cost objective and
f(p1,...,pK,p′
1,...,p′
K;γ) :=K−1
2/summationdisplay
l=0(emϵα′
l−αl)·γ(l) +K/summationdisplay
l=K+1
2(αl−emϵα′
l)·γ(l)
Proof of Lemma 3.4. By the definition of differential privacy (Definition 2.1),
DaRRMγis(mϵ,δ )-differentially private
⇐⇒ Pr[DaRRMγ(D) = 1]≤emϵPr[DaRRMγ(D′) = 1] +δ,
and Pr[DaRRMγ(D) = 0]≤emϵPr[DaRRMγ(D′) = 0] +δ,∀adjacent datasets D,D′(32)
Let random variables L(D) =/summationtextK
i=1S(D)andL(D′) =/summationtextK
i=1S(D′)be the sum of observed outcomes
on adjacent datasets DandD′, based on which one sets pγinDaRRM. Letαl=Pr[L(D) =l]and
α′
l= Pr[L(D′) =l],∀l∈{0,1,...,K}.
Consider the output being 1.
Pr[DaRRMγ(D) = 1]≤emϵPr[DaRRMγ(D′) = 1] +δ (33)
⇐⇒K/summationdisplay
l=0Pr[DaRRMγ(D) = 1|L(D) =l]·Pr[L(D) =l] (34)
≤emϵ/parenleftigK/summationdisplay
l=0Pr[DaRRMγ(D′) = 1|L(D′) =l]·Pr[L(D′) =l]/parenrightig
+δ
⇐⇒K/summationdisplay
l=0/parenleftig
γ(l)·I{l≥K
2}+1
2(1−γ(l))/parenrightig
·Pr[L(D) =l] (35)
≤emϵ/parenleftigK/summationdisplay
l=0/parenleftig
γ(l)·I{l≥K
2}+1
2(1−γ(l))}/parenrightig
·Pr[L(D′) =l]/parenrightig
+δ
21Published in Transactions on Machine Learning Research (November/2024)
⇐⇒K/summationdisplay
l=K+1
2/parenleftig
γ(l) +1
2(1−γ(l))/parenrightig
·Pr[L(D) =l] +K−1
2/summationdisplay
l=01
2(1−γ(l))·Pr[L(D) =l] (36)
≤emϵ/parenleftigK/summationdisplay
l=K+1
2/parenleftig
γ(l) +1
2(1−γ(l))/parenrightig
·Pr[L(D) =l]/parenrightig
+emϵ/parenleftigK−1
2/summationdisplay
l=01
2(1−γ(l))·Pr[L(D′) =l]/parenrightig
+δ
⇐⇒K/summationdisplay
l=K+1
21
2γ(l)αl−K−1
2/summationdisplay
l=01
2γ(l)αl+1
2(37)
≤emϵK/summationdisplay
l=K+1
21
2γ(l)α′
l−emϵK−1
2/summationdisplay
l=01
2γ(l)α′
l+1
2emϵ+δ
⇐⇒K/summationdisplay
l=K+1
2(αl−emϵα′
l)γ(l)−K−1
2/summationdisplay
l=0(αl−emϵα′
l)γ(l)≤emϵ−1 + 2δ (38)
Similarly, consider the output being 0.
Pr[DaRRMγ(D) = 0]≤emϵPr[DaRRMγ(D′) = 0] +δ (39)
⇐⇒K/summationdisplay
l=0Pr[DaRRMγ(D) = 0|L(D) =l]·Pr[L(D) =l] (40)
≤emϵ/parenleftigK/summationdisplay
l=0Pr[DaRRMγ(D′) = 0|L(D′) =l]·Pr[L(D′) =l]/parenrightig
+δ
⇐⇒K/summationdisplay
l=0/parenleftig
γ(l)·I{l<K
2}+1
2(1−γ(l))/parenrightig
·Pr[L(D) =l] (41)
≤emϵ/parenleftigK/summationdisplay
l=0γ(l)·I{l<K
2}+1
2(1−γ(l))/parenrightig
·Pr[L(D′) =l] +δ
⇐⇒K−1
2/summationdisplay
l=0/parenleftig
γ(l) +1
2(1−γ(l))/parenrightig
·Pr[L(D) =l] +K/summationdisplay
l=K+1
21
2(1−γ(l))·Pr[L(D) =l] (42)
≤emϵ/parenleftigK−1
2/summationdisplay
l=0/parenleftig
γ(l) +1
2(1−γ(l))/parenrightig
·Pr[L(D′) =l] +K/summationdisplay
l=K+1
21
2(1−γ(l))·Pr[L(D′) =l]/parenrightig
+δ
⇐⇒K−1
2/summationdisplay
l=01
2γ(l)αl−K/summationdisplay
l=K+1
21
2γ(l)αl+1
2(43)
≤emϵK−1
2/summationdisplay
l=01
2γ(l)α′
l−emϵK/summationdisplay
l=K+1
21
2γ(l)α′
l+1
2emϵ+δ
⇐⇒K−1
2/summationdisplay
l=0(αl−emϵα′
l)γ(l)−K/summationdisplay
l=K+1
2(αl−emϵα′
l)γ(l)≤emϵ−1 + 2δ (44)
22Published in Transactions on Machine Learning Research (November/2024)
Therefore, plugging Eq. 38 and Eq. 44 into Eq. 32,
DaRRMγis(mϵ,δ )-differentially private
⇐⇒K/summationdisplay
l=K+1
2(αl−emϵα′
l)γ(l)−K−1
2/summationdisplay
l=0(αl−emϵα′
l)γ(l)≤emϵ−1 + 2δ (45)
andK−1
2/summationdisplay
l=0(αl−emϵα′
l)γ(l)−K/summationdisplay
l=K+1
2(αl−emϵα′
l)γ(l)≤emϵ−1 + 2δ (46)
whereαl= Pr[L(D) =l]andα′
l= Pr[L(D′) =l],∀l∈{0,1,...,K}andD,D′are any adjacent datasets.
Next, we show if γis symmetric aroundK
2, i.e.,γ(l) =γ(K−l), satisfying either one of Eq. 45 or Eq. 46
implies satisfying the other one. Following Eq. 45,
K/summationdisplay
l=K+1
2(αl−emϵα′
l)γ(l)−K−1
2/summationdisplay
l=0(αl−emϵα′
l)γ(l)≤emϵ−1 + 2δ (47)
⇐⇒K−1
2/summationdisplay
l=0(αK−l−emϵα′
K−l)·γ(K−l)−K/summationdisplay
l=K−1
2(αK−l−emϵα′
K−l)·γ(K−l)≤emϵ−1 + 2δ(48)
⇐⇒K−1
2/summationdisplay
l=0(αK−l−emϵα′
K−l)·γ(l)−K/summationdisplay
l=K−1
2(αK−l−emϵα′
K−l)·γ(l)≤emϵ−1 + 2δ (49)
Sinceγ(l) =γ(K−l)
For analysis purpose, we rewrite Eq. 46 as
K−1
2/summationdisplay
l=0(/tildewideαl−emϵ/tildewideα′
l)·γ(l)−K/summationdisplay
l=K−1
2(/tildewideαl−emϵ/tildewideα′
l)·γ(l)≤emϵ−1 + 2δ (50)
and proceed by showing Eq. 49 ⇐⇒Eq. 50.
Recallpi=Pr[Mi(D) = 1]andp′
i=Pr[Mi(D′) = 1]. ObserveL(D)∼PoissonBinomial ({pi}K
i=1)and
L(D′)∼PoissonBinomial ({p′
i}K
i=1). LetFl={A:|A|=l,A⊆ [K]}, for anyl∈{0,...,K}, denote the set
of all subsets of lintegers that can be selected from [K]. LetAc= [K]\AbeA’s complement set. Notice
FK−l={Ac:A∈Fl}.
Sinceαdenotes the pmf of the Poisson Binomial distribution at l, it follows that
αl= Pr[L(D) =l] =/summationdisplay
A∈FlΠi∈ApiΠj∈Ac(1−pj) (51)
Considerβi= 1−pi,∀i∈[K]and a new random variable Lβ∼PoissonBinomial ({βi}K
i=1), and let/tildewideαl=
Pr[Lβ= 1]. Observe that
/tildewideα′
l= Pr[Lβ=l] =/summationdisplay
A∈FlΠj∈AβiΠi∈Ac(1−βi) =/summationdisplay
A∈FlΠj∈A(1−pj)Πi∈Acpi
=/summationdisplay
Ac∈FK−lΠj∈A(1−pi)Πi∈Acpi=/summationdisplay
A∈FK−lΠi∈ApiΠj∈Ac(1−pi)
23Published in Transactions on Machine Learning Research (November/2024)
=αK−l (52)
Similarly, consider β′
i= 1−p′
i,∀i∈[K]and a new random variable L′β∼PoissonBinomial (β′
i}L
i=1), and let
/tildewideα′
l= Pr[L′β= 1]. Then,/tildewideα′
l=α′
K−l.
Since Eq. 49 holds for all possible αK−l,α′
K−l, Eq. 50 then holds for all /tildewideαl,/tildewideα′
lin theK-simplex, and so
Eq. 50 follows by relabeling αK−las/tildewideαlandα′
K−las/tildewideα′
l.
The above implies Eq. 45 ⇐⇒Eq. 46. Therefore,
DaRRMγis(mϵ,δ )-differentially private
⇐⇒K/summationdisplay
l=K+1
2(αl−emϵα′
l)γ(l)−K−1
2/summationdisplay
l=0(αl−emϵα′
l)γ(l)
/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
:=f(p1,...,pK,p′
1,...,p′
K;γ)≤emϵ−1 + 2δ (53)
24Published in Transactions on Machine Learning Research (November/2024)
B Details of Section 4: Provable Privacy Amplification
In this section, we consider Problem 1.1 in the pure differential privacy and i.i.d. mechanisms setting. That
is,δ= ∆ = 0 andp=pi=Pr[Mi(D) = 1],p′=p′
i=Pr[Mi(D′) = 1],∀i∈[K]. Our goal is to search for a
good noise function γsuch that: 1) DaRRMγismϵ-DP, and 2) DaRRMγachieves higher utility than that of
the baselines (see Section 3) under a fixed privacy loss. Our main finding of such a γfunction is presented in
Theorem 4.1, which states given a privacy allowance m∈[K], one can indeed output the majority of 2m−1
subsampled mechanisms, instead of just mas indicated by simple composition. Later, we formally verify in
Lemma B.11, Section B.3 that taking the majority of more mechanisms strictly increases the utility.
To start, by Lemma 3.4, for any noise function γ,γsatisfying goal 1) is equivalent to satisfying
f(p,p′;γ)≤eϵ−1 (54)
wheref(p,p′;γ) =/summationtextK−1
2
l=0(emϵα′
l−αl)·γ(l) +/summationtextK
l=K+1
2(αl−emϵα′
l)·γ(l)refers to the privacy cost objective
(see Lemma 3.4) in the i.i.d. mechanisms setting, and recall αl=Pr[L(D) =l]andα′
l=Pr[L(D′) =l],
∀l∈{0,1,...,K}. Notice in this setting, L(D)∼Binomial (p), andL(D′)∼Binomial (p′).
Monotonicity Assumption. For analysis, we restrict our search for a γfunction with good utility to the
class with a mild monotonicity assumption: γ(l)≥γ(l+ 1),∀l≤K−1
2andγ(l)≤γ(l+ 1),∀l≥K+1
2. This
matches our intuition that as L(D) =/summationtextK
i=1Si, i.e., the number of mechanisms outputting 1, approaches 0or
K, there is a clearer majority and so not much noise is needed to ensure privacy, which implies a larger value
ofγ.
Figure 4: The feasible region Fis plotted
as the blue area. The four boundaries are
implied by p,p′satisfyingϵ-differential pri-
vacy.Roadmap of Proof of Theorem 4.1. Sinceγneeds
to enable Eq. 54 to be satisfied for all p,p′∈[0,1], we
begin by showing characteristics of the worst case prob-
abilities , i.e., (p∗,p′∗) = arg max(p,p′)f(p,p′;γ), given any
γ:{0,1,...,K}→[0,1]that is symmetric aroundK
2and that
satisfies the above monotonicity assumption, in Lemma B.1, Sec-
tion B.1. We call (p∗,p′∗)the worst case probabilities, since they
incur the largest privacy loss. Later in Section B.2, we present
the main proof of Theorem 4.1, where we focus on searching
for a good γthat enables f(p∗,p′∗;γ)≤eϵ−1, based on the
characteristics of (p∗,p′∗)in Lemma B.1, to ensure DaRRMγis
mϵ-differentially private.
B.1 Characterizing the Worst Case Probabilities
First, note (p,p′)are close to each other and lie in a feasi-
ble regionF, due to each mechanism Mibeingϵ-differentially
private; and so does (p∗,p′∗). The feasible region, as illus-
trated in Figure 4, is bounded by (a) p′≤eϵp(b)p≤eϵp′
(c)1−p′≤eϵ(1−p), and (d) 1−p≤eϵ(1−p′), where the
four boundaries are derived from the definition of differential
privacy. Therefore, we only need to search for (p∗,p′∗) = arg max(p,p′)∈Ff(p,p′;γ).
Next, we show that given γsatisfying certain conditions, (p∗,p′∗)can only be on two of the four boundaries
ofFin Lemma B.1 — that is, either p∗=eϵp′, i.e., on the blue line in Figure 4, or 1−p′∗=eϵ(1−p∗), i.e.,
on the orange line in Figure 4.
Lemma B.1 (Characteristics of worst case probabilities) .For any noise function γ:{0,1,...,K}→[0,1]
that is 1) symmetric aroundK
2, 2) satisfies the monotonicity assumption, and 3) γ(K−1
2)>0andγ(K+1
2)>0,
the worst case probabilities given γ,(p∗,p′∗) =arg max(p,p′)∈Ff(p,p′;γ), must satisfy one of the following
two equalities:
p∗=eϵp′∗, ∀p∗∈[0,1
e−ϵ+ 1],p′∗∈[0,1
1 +eϵ]
25Published in Transactions on Machine Learning Research (November/2024)
or 1−p′∗=eϵ(1−p∗), ∀p∗∈[1
1 +e−ϵ,1],p′∗∈[1
1 +eϵ,1]
To show Lemma B.1, we first show in Lemma B.2 that the search of (p∗,p′∗)can be refined to one of the four
boundaries ofF, via a careful gradient analysis of f(p,p′;γ)inF, and then show in Lemma B.3 that the
search of (p∗,p′∗)can be further refined to two of the four boundaries, due to symmetry of p,p′. Lemma B.1
directly follows from the two.
Lemma B.2. For any noise function γ:{0,1,...,K}→[0,1]that is 1) symmetric aroundK
2, 2) satisfies
the monotonicity assumption, and 3) γ(K−1
2)>0andγ(K+1
2)>0, the worst case probabilities given γ,
(p∗,p′∗) = arg max(p,p′)∈Ff(p,p′;γ), must satisfy one of the following four equalities:
p′∗=eϵp∗, ∀p∗∈[0,1
1 +eϵ],p′∗∈[0,1
1 +e−ϵ]
p∗=eϵp′∗, ∀p∗∈[0,1
e−ϵ+ 1],p′∗∈[0,1
1 +eϵ]
1−p∗=eϵ(1−p′∗), ∀p∗∈[1
1 +eϵ,1],p′∗∈[1
1 +e−ϵ,1]
1−p′∗=eϵ(1−p∗), ∀p∗∈[1
1 +e−ϵ,1],p′∗∈[1
1 +eϵ,1]
Proof of Lemma B.2. Recall the privacy cost objective (as defined in Lemma 3.4) is now
f(p,p′;γ) =K−1
2/summationdisplay
l=0(emϵα′
l−αl)·γ(l) +K/summationdisplay
l=K+1
2(αl−emϵα′
l)·γ(l)
whereαl=Pr[L(D) =l]andα′
l=Pr[L(D′) =l],∀l∈{0,1,...,K}. SinceL(D)∼Binomial (p)and
L(D′)∼Binomial (p′)in the i.i.d. mechanisms setting, and using the pmf of the Binomial distribution, fcan
be written as
f(p,p′;γ) =K−1
2/summationdisplay
l=0(emϵ/parenleftbiggK
l/parenrightbigg
p′l(1−p′)K−l−/parenleftbiggK
l/parenrightbigg
pl(1−p)K−l)·γ(l) +K/summationdisplay
l=K+1
2(/parenleftbiggK
l/parenrightbigg
pl(1−p)K−l−emϵ/parenleftbiggK
l/parenrightbigg
p′l(1−p′)K−l)
The gradients w.r.t. pandp′are
∇pf(p,p′;γ) =K−1
2/summationdisplay
l=0−/parenleftbiggK
l/parenrightbigg
γ(l)·(lpl−1(1−p)K−l−pl(K−l)(1−p)K−l−1)
/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
:=A(55)
+K/summationdisplay
l=K+1
2/parenleftbiggK
l/parenrightbigg
γ(l)·(lpl−1(1−p)K−l−pl(K−l)(1−p)K−l−1)
/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
:=B
and
∇p′f(p,p′;γ) =K−1
2/summationdisplay
l=0emϵ/parenleftbiggK
l/parenrightbigg
γ(l)·(lp′l−1(1−p′)K−l−p′l(K−l)(1−p′)K−l−1) (56)
+K/summationdisplay
l=K+1
2−emϵ/parenleftbiggK
l/parenrightbigg
γ(l)·(lp′l−1(1−p′)K−l−p′l(K−l)(1−p′)K−l−1)
26Published in Transactions on Machine Learning Research (November/2024)
We show in the following ∀p∈(0,1),∇pf(p,p′;γ)>0and∇p′f(p,p′;γ)<0. This implies there is no local
maximum insideF, and so (p∗,p′∗) =arg maxp,p′f(p,p′;γ)must be on one of the four boundaries of F. Also,
ifp= 0, thenp′= 0, and (0,0)is a corner point at the intersection of two boundaries. Similarly, if p= 1,
thenp′= 1, and (1,1)is also a corner point. This concludes ∀p∈[0,1],(p∗,p′∗) =arg maxp,p′f(p,p′;γ)must
be on one of the four boundaries of F.
To show∇pf(p,p′;γ)>0forp∈(0,1), we write∇pf(p,p′;γ) =A+Bas in Eq. 55, and show that A>0
andB > 0.
To showA>0, first note
A:=K−1
2/summationdisplay
l=0γ(l)/parenleftbiggK
l/parenrightbigg
·(pl(K−l)(1−p)K−l−1−lpl−1(1−p)K−l)>0 (57)
⇐⇒K−1
2/summationdisplay
l=0γ(l)/parenleftbiggK
l/parenrightbigg
·pl(K−l)(1−p)K−l−1>K−1
2/summationdisplay
l=0γ(l)/parenleftbiggK
l/parenrightbigg
·lpl−1(1−p)K−l) (58)
⇐⇒K−1
2/summationdisplay
l=0γ(l)/parenleftbiggK−1
l/parenrightbiggK
K−l·pl(K−l)(1−p)K−l−1>K−1
2/summationdisplay
l=1γ(l)/parenleftbiggK−1
l−1/parenrightbiggK
l·lpl−1(1−p)K−l(59)
⇐⇒KK−1
2/summationdisplay
l=0γ(l)/parenleftbiggK−1
l/parenrightbigg
pl(1−p)K−l−1>KK−1
2/summationdisplay
l=1γ(l)/parenleftbiggK−1
l−1/parenrightbigg
pl−1(1−p)K−l(60)
⇐⇒K−1
2/summationdisplay
l=0γ(l)/parenleftbiggK−1
l/parenrightbigg
pl(1−p)K−l−1>K−1
2−1/summationdisplay
l=0γ(l+ 1)/parenleftbiggK−1
l/parenrightbigg
pl(1−p)K−l−1(61)
Since∀l≤K−1
2,γ(l)≥γ(l+ 1)andp∈(0,1), there is for l∈{0,...,K−1
2−1},
γ(l)/parenleftbiggK−1
l/parenrightbigg
pl(1−p)K−l−1≥γ(l+ 1)/parenleftbiggK−1
l/parenrightbigg
pl(1−p)K−l−1(62)
Furthermore, since γ(K−1
2)>0andp∈(0,1),
γ(K−1
2)/parenleftbiggK−1
K−1
2/parenrightbigg
pK−1
2(1−p)K−1
2>0 (63)
Eq. 62 and Eq. 63 combined implies
γ(K−1
2)/parenleftbiggK−1
K−1
2/parenrightbigg
pK−1
2(1−p)K−1
2+K−1
2−1/summationdisplay
l=0γ(l)/parenleftbiggK−1
l/parenrightbigg
pl(1−p)K−l−1>K−1
2−1/summationdisplay
l=0γ(l+ 1)/parenleftbiggK−1
l/parenrightbigg
pl(1−p)K−l−1
(64)
and hence, Eq. 61 holds. This further implies A>0.
Next, to show B > 0, note that
B:=K/summationdisplay
l=K+1
2/parenleftbiggK
l/parenrightbigg
γ(l)·(lpl−1(1−p)K−l−pl(K−l)(1−p)K−l−1)>0 (65)
⇐⇒K/summationdisplay
l=K+1
2/parenleftbiggK
l/parenrightbigg
γ(l)·lpl−1(1−p)K−l>K/summationdisplay
l=K+1
2/parenleftbiggK
l/parenrightbigg
pl(K−l)(1−p)K−l−1(66)
27Published in Transactions on Machine Learning Research (November/2024)
⇐⇒K/summationdisplay
l=K+1
2γ(l)/parenleftbiggK−1
l−1/parenrightbiggK
l·lpl−1(1−p)K−l(67)
>K−1/summationdisplay
l=K+1
2γ(l)/parenleftbiggK−1
l/parenrightbiggK
K−l·pl(K−l)(1−p)K−l−1
⇐⇒KK/summationdisplay
l=K+1
2γ(l)/parenleftbiggK−1
l−1/parenrightbigg
·pl−1(1−p)K−l(68)
>KK−1/summationdisplay
l=K+1
2γ(l)/parenleftbiggK−1
l/parenrightbigg
·pl(1−p)K−l−1
⇐⇒K/summationdisplay
l=K+1
2γ(l)/parenleftbiggK−1
l−1/parenrightbigg
·pl−1(1−p)K−l>K/summationdisplay
l=K+1
2+1γ(l−1)/parenleftbiggK−1
l−1/parenrightbigg
·pl−1(1−p)K−l(69)
Since∀l≥K+1
2,γ(l)≥γ(l−1)andp∈(0,1), there is for l∈{K+1
2+ 1,...,K},
γ(l)/parenleftbiggK−1
l−1/parenrightbigg
pl−1(1−p)K−l≥γ(l−1)/parenleftbiggK−1
l−1/parenrightbigg
pl−1(1−p)K−l(70)
Furthermore, since γ(K+1
2)>0andp∈(0,1),
γ(K+ 1
2)/parenleftbiggK−1
K−1
2/parenrightbigg
pK−1
2(1−p)K−1
2>0 (71)
Eq. 70 and Eq. 71 combined implies
γ(K+ 1
2)/parenleftbiggK−1
K−1
2/parenrightbigg
pK−1
2(1−p)K−1
2+K/summationdisplay
l=K+1
2+1γ(l)/parenleftbiggK−1
l−1/parenrightbigg
·pl−1(1−p)K−l>K/summationdisplay
l=K+1
2+1γ(l−1)/parenleftbiggK−1
l−1/parenrightbigg
·pl−1(1−p)K−l
(72)
and hence Eq. 69 holds. This further implies B > 0.
Following Eq.55, for p∈(0,1)andγsatisfying the three assumptions,
∇pf(p,p′;γ) =A+B > 0 (73)
Following similar techniques, one can show for p∈(0,1)andγsatisfying the three conditions,
∇p′f(p,p′;γ)<0 (74)
This implies there is no local minima or local maxima inside the feasible region F. Also recall (p,p′)∈
{(0,0),(1,1)}are two special cases where (p,p′)is at the intersection of two boundaries. Hence, we conclude
the worst case probability (p∗,p′∗) =arg maxp,p′∈Ff(p,p′;γ)is on one of the four boundaries of F— that
is,(p∗,p′∗)satisfy one of the following:
p′∗=eϵp∗, ∀p∈[0,1
1 +eϵ],p′∈[0,1
1 +e−ϵ]
p∗=eϵp′∗, ∀p∈[0,1
e−ϵ+ 1],p′∈[0,1
1 +eϵ]
1−p∗=eϵ(1−p′∗), ∀p∈[1
1 +eϵ,1],p′∈[1
1 +e−ϵ,1]
1−p′∗=eϵ(1−p∗), ∀p∈[1
1 +e−ϵ,1],p′∈[1
1 +eϵ,1]
28Published in Transactions on Machine Learning Research (November/2024)
Lemma B.3. For any noise function γ:{0,1,...,K}→[0,1]function that is 1) symmetric aroundK
2and
2) satisfies the monotonicity assumption, the privacy cost objective f(p,p′;γ)is maximized when p≥p′.
Proof of Lemma B.3. Following Eq. 33 and Eq. 38 in the proof of Lemma 3.4, and that δ= 0,
Pr[DaRRMγ(D) = 1]≤emϵPr[DaRRMγ(D′) = 1] (75)
⇐⇒K/summationdisplay
l=K+1
2(αl−emϵα′
l)γ(l)−K−1
2/summationdisplay
l=0(αl−emϵα′
l)γ(l)
/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
=f(p,p′;γ)≤emϵ−1 (76)
whereαl= Pr[L(D) =l]andα′
l= Pr[L(D′) =l],∀l∈{0,1,...,K}. This implies
f(p,p′;γ) =Pr[DaRRMγ(D) = 1]
Pr[DaRRMγ(D′) = 1]−1 (77)
Hence,f(p,p′;γ)is maximized when Pr[DaRRMγ(D) = 1]≥Pr[DaRRMγ(D′) = 1].
Pr[DaRRMγ(D) = 1] =K/summationdisplay
l=0Pr[DaRRMγ(D) = 1|L(D) = 1]·Pr[L(D) =l] (78)
=K/summationdisplay
l=0/parenleftig
γ(l)·I{l≥K
2}+1
2(1−γ(l))/parenrightig
·Pr[L(D) =l] (79)
=K−1
2/summationdisplay
l=01
2(1−γ(l))·αl+K/summationdisplay
l=K+1
2/parenleftig
γ(l) +1
2(1−γ(l))/parenrightig
·αl (80)
=1
2K/summationdisplay
l=K+1
2γ(l)/parenleftbiggK
l/parenrightbigg
pl(1−p)K−l−1
2K−1
2/summationdisplay
l=0γ(l)/parenleftbiggK
l/parenrightbigg
pl(1−p)K−l−1+1
2(81)
where the last line follows from the observation that in the i.i.d. mechanisms setting, L(D)∼Binomial (p)
andαlis hence the pmf of the Binomial distribution at l.
Similarly,
Pr[DaRRMγ(D′) = 1] =1
2K/summationdisplay
l=K+1
2γ(l)/parenleftbiggK
l/parenrightbigg
p′l(1−p′)K−l−1
2K−1
2/summationdisplay
l=0γ(l)/parenleftbiggK
l/parenrightbigg
p′l(1−p′)K−l−1+1
2(82)
Now define the objective
h(β) =1
2K/summationdisplay
l=K+1
2γ(l)/parenleftbiggK
l/parenrightbigg
βl(1−β)K−l−1
2K−1
2/summationdisplay
l=0γ(l)/parenleftbiggK
l/parenrightbigg
βl(1−β)K−l−1+1
2(83)
forβ∈[0,1]and it follows that Pr[DaRRMγ(D) = 1] =h(p)andPr[DaRRMγ(D′) = 1] =h(p′). We now
analyze the monotonicity of h(β)inβ.
For ease of presentation, define g(l) :=/braceleftigg
−1
2γ(l)∀l≤K
2
1
2γ(l)∀l≥K
2. Sinceγ(l)≥γ(l+ 1),∀l≤K
2andγ(l+ 1)≥
γ(l),∀l≥K
2, there isg(l+ 1)≥g(l),∀l∈{0,...,K}. And replacing γ(l)withg(l)in Eq. 83,
h(β) =K/summationdisplay
l=0g(l)/parenleftbiggK
l/parenrightbigg
βl(1−β)K−l(84)
29Published in Transactions on Machine Learning Research (November/2024)
∇βh(β) =K/summationdisplay
l=0g(l)/parenleftbiggK
l/parenrightbigg/parenleftig
lβl−1(1−β)K−l−(K−l)βl(1−β)K−l−1/parenrightig
(85)
=K/summationdisplay
l=1g(l)/parenleftbiggK−1
l−1/parenrightbiggK
llβl−1(1−β)K−l−K−1/summationdisplay
l=0/parenleftbiggK−1
l/parenrightbiggK
K−l(K−l)βl(1−β)K−l−1(86)
=KK/summationdisplay
l=1/parenleftbiggK−1
l−1/parenrightbigg
βl−1(1−β)K−l−KK−1/summationdisplay
l=0/parenleftbiggK−1
l/parenrightbigg
βl(1−β)K−l−1(87)
=KK−1/summationdisplay
l=0g(l+ 1)/parenleftbiggK−1
l/parenrightbigg
βl(1−β)K−l−1−KK−1/summationdisplay
l=0g(l)/parenleftbiggK−1
l/parenrightbigg
βl(1−β)K−l−1(88)
=KK−1/summationdisplay
l=0/parenleftig
g(l+ 1)−g(l)/parenrightig/parenleftbiggK−1
l/parenrightbigg
βl(1−β)K−l−1(89)
Sinceg(l+ 1)≥g(l)and/parenleftbigK−1
l/parenrightbig
βl(1−β)K−l−1≥0,∇βh(β)≥0. This implies h(β)is monotonically
non-decreasing in βand hence,
Pr[DaRRMγ(D) = 1]≥Pr[DaRRMγ(D′) = 1]⇐⇒p≥p′(90)
Therefore,f(p,p′;γ)is maximzied when p≥p′.
B.2 Proof of Privacy Amplification (Theorem 4.1)
Theorem B.4 (Restatement of Theorem 4.1) .Consider using DaRRM(Algorithm 1) to solve Problem 1.1,
with i.i.d. mechanisms {Mi}K
i=1, i.e.,pi=p,p′
i=p′,∀i∈[K], the privacy allowance m∈[K]andδ= ∆ = 0.
Let the noise function γ:{0,1,...,K}→[0,1]be that:
ifm≥K+1
2,
γ(l) = 1
and ifm≤K−1
2,
γ(l) =/braceleftigg
1−2h(l)∀l≤K−1
2
2h(l)−1∀l≥K+1
2
whereh(l) =/summationtext2m−1
i=m(l
i)(K−l
2m−1−i)
(K
2m−1), then DaRRMγismϵ-differentially private.
Roadmap. Theorem 4.1 consists of two parts: γunder a large privacy allowance m≥K+1
2andγunder
a small privacy allowance m≤K−1
2. We first show in Lemma B.5, Section B.2.1 that if m≥K+1
2, setting
γ= 1suffices to ensure DaRRMγto bemϵ-differentially private, and hence one can always output the true
majority of Kmechanisms. In contrast, simple composition indicates only when m=Kcan one output
the true majority of Kmechanisms. Next, we show in Lemma B.10, Section B.2.2 that if m≤K−1
2, one
can setγto beγDSub, which corresponds to outputting the majority of 2m−1subsampled mechanisms
(and hence the name “Double Subsampling”, or DSub). In contrast, simple compositon indicates one can
only output the majority of msubsampled mechanisms to make sure the output is mϵ-differentially private.
Theorem 4.1 follows directly from combining Lemma B.5 and Lemma B.10.
B.2.1 Privacy Amplification Under A Large Privacy Allowance m≥K+1
2
The proof of Lemma B.5 is straightforward. We show that given the constant γmax(l) = 1, ifm≥K+1
2, the
worst case probabilities are (p∗,p′∗) =arg max(p,p′)∈Ff(p,p′;γmax) = (0,0)and notice that f(0,0;γmax) =
emϵ−1, which satisfies the condition in Lemma 3.4. Hence, DaRRMγmaxismϵ-differentially private.
30Published in Transactions on Machine Learning Research (November/2024)
Lemma B.5 (Privacy amplification, m≥K+1
2).Consider using DaRRM(Algorithm 1) to solve Problem 1.1,
with i.i.d. mechanisms {Mi}K
i=1, i.e.,pi=p,p′
i=p′,∀i∈[K], the privacy allowance m≥K+1
2,m∈Zand
δ= ∆ = 0 . Let the noise function be the constant γmax(l) = 1,∀l∈{0,1,...,K}. Then, DaRRMγmaxis
mϵ-differentially private.
Proof of Lemma B.5. First, notice γmax(l) = 1,∀l∈{0,1,...,K}is: 1) symmetric aroundK
2, 2) satisfies the
monotonicity assumption, and 3) γmax(K−1
2)>0andγmax(K+1
2)>0. Therefore, by Lemma B.1, the worst
case probabilities given γmax, i.e., (p∗,p′∗) =arg max(p,p′)∈Ff(p,p′;γmax), are on one of the two boundaries
ofF, satisfying
p∗=eϵp′∗, ∀p∗∈[0,1
e−ϵ+ 1],p′∗∈[0,1
1 +eϵ]
or 1−p′∗=eϵ(1−p∗), ∀p∗∈[1
1 +e−ϵ,1],p′∗∈[1
1 +eϵ,1]
We now find the local maximums on the two possible boundaries, i.e.,
(p∗
local,p′∗
local) = arg max
(p,p′):p=eϵp′,p∈[0,1
e−ϵ+1]f(p,p′;γmax)
and
(p∗
local,p′∗
local) = arg max
(p,p′):1−p′=eϵ(1−p),p∈[1
1+e−ϵ,1]f(p,p′;γmax)
separately.
Part I: Local worst case probabilities on the boundary p=eϵp′.
Pluggingp=eϵp′into the privacy cost objective f(p,p′;γmax), one gets
f(p′;γmax) =K−1
2/summationdisplay
l=0(emϵ/parenleftbiggK
l/parenrightbigg
p′l(1−p′)K−l−/parenleftbiggK
l/parenrightbigg
(eϵp′)l(1−eϵp′)K−l) (91)
+K/summationdisplay
l=K+1
2(/parenleftbiggK
l/parenrightbigg
(eϵp′)l(1−eϵp′)K−l−emϵ/parenleftbiggK
l/parenrightbigg
p′l(1−p′)K−l)
The gradient w.r.t. p′is
∇p′f(p′;γmax) =K−1
2/summationdisplay
l=0/parenleftig
emϵ/parenleftbiggK
l/parenrightbigg
(lp′l−1(1−p′)K−l−p′l(K−l)(1−p′)K−l−1) (92)
−eϵ/parenleftbiggK
l/parenrightbigg
(l(eϵp′)l−1(1−eϵp′)K−l−eϵlp′l(K−l)(1−eϵp′)K−l−1)/parenrightig
+K/summationdisplay
l=K+1
2/parenleftig
eϵ/parenleftbiggK
l/parenrightbigg
(l(eϵp′)l−1(1−eϵp′)K−l−eϵlp′l(K−l)(1−eϵp′)K−l−1)
−emϵ/parenleftbiggK
l/parenrightbigg
(lp′l−1(1−p′)K−l−p′l(K−l)(1−p′)K−l−1)/parenrightig
=−KK−1
2/summationdisplay
l=0emϵ/parenleftbiggK−1
l/parenrightbigg
p′l(1−p′)K−l−1+KK−1/summationdisplay
l=K+1
2emϵ/parenleftbiggK−1
l/parenrightbigg
p′l(1−p′)K−l−1(93)
+KK−1
2/summationdisplay
l=0eϵ/parenleftbiggK−1
l/parenrightbigg
(ϵp′)ϵ(1−eϵp′)K−l−1−KK−1/summationdisplay
l=K+1
2eϵ/parenleftbiggK−1
l/parenrightbigg
(eϵp′)l(1−eϵp′)K−l−1
31Published in Transactions on Machine Learning Research (November/2024)
+KK−1
2−1/summationdisplay
l=0emϵ/parenleftbiggK−1
l/parenrightbigg
p′l(1−p′)K−l−1−KK−1/summationdisplay
l=K−1
2emϵ/parenleftbiggK−1
l/parenrightbigg
p′l(1−p′)K−l−1
−KK−1
2−1/summationdisplay
l=0eϵ/parenleftbiggK−1
l/parenrightbigg
(eϵp′)l(1−eϵp′)K−l−1+KK−1/summationdisplay
l=K−1
2eϵ/parenleftbiggK−1
l/parenrightbigg
(eϵp′)l(1−eϵp′)K−l−1
=−2Kemϵ/parenleftbiggK−1
K−1
2/parenrightbigg
p′K−1
2(1−p′)K−1
2
/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
:=A+2Keϵ/parenleftbiggK−1
K−1
2/parenrightbigg
(eϵp′)K−1
2(1−eϵp′)K−1
2
/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
:=B(94)
Notice that
A
B=emϵ/parenleftbigK−1
K−1
2/parenrightbig
p′K−1
2(1−p′)K−1
2
eϵ/parenleftbigK−1
K−1
2/parenrightbig
(eϵp′)K−1
2(1−eϵp′)K−1
2=emϵ
eK+1
2ϵ·(1−p′
1−eϵp′)K−1
2 (95)
Since1−p′
1−eϵp′≥1andm≥K+1
2,A
B≥1. This implies∇p′f(p′;γmax)≤0. Hence,f(p′;γmax)is monotonically
non-increasing on the boundary, for p′∈[0,1
1+eϵ].
Therefore, arg maxp′:p′∈[0,1
1+eϵ]f(p′;γmax) = 0. Sincep=eϵp′,p′= 0impliesp= 0.
Hence,
(p∗
local,p′∗
local) = arg max
(p,p′):p=eϵp′,p∈[0,1
e−ϵ+1]f(p,p′;γmax) = (0,0)
and
max
(p,p′):p=eϵp′,p∈[0,1
e−ϵ+1]f(p,p′;γmax) =f(0,0;γmax) =emϵ−1
Part II: Local worst case probabilities on the boundary 1−p′=eϵ(1−p).
For simplicity, let q= 1−pandq′= 1−p′. Note on this boundary p∈[1
1+e−ϵ,1]andp′∈[1
1+eϵ,1], and
hence,q∈[0,1
1+eϵ]andq′∈[0,1
1+e−ϵ].
Pluggingqandq′into the privacy cost objective f(p,p′;γmax), one gets a new objective in q,q′as
f(q,q′;γmax) =K−1
2/summationdisplay
l=0/parenleftig
emϵ/parenleftbiggK
l/parenrightbigg
(1−q′)lq′K−l−/parenleftbiggK
l/parenrightbigg
(1−q)lqK−l/parenrightig
·γmax(l) (96)
+K/summationdisplay
l=K+1
2/parenleftig/parenleftbiggK
l/parenrightbigg
(1−q)lqK−l−emϵ/parenleftbiggK
l/parenrightbigg
(1−q′)lq′K−l/parenrightig
·γmax(l)
=K−1
2/summationdisplay
l=0/parenleftig
emϵ/parenleftbiggK
l/parenrightbigg
(1−q′)lq′K−l−/parenleftbiggK
l/parenrightbigg
(1−q)lqK−l/parenrightig
(97)
+K/summationdisplay
l=K+1
2/parenleftig/parenleftbiggK
l/parenrightbigg
(1−q)lqK−l−emϵ/parenleftbiggK
l/parenrightbigg
(1−q′)lq′K−l/parenrightig
Since on this boundary, 1−p′=eϵ(1−p), writing this in q,q′, this becomes q′=eϵq. Pluggingq′=eϵqinto
f(q,q′;γmax), one gets
f(q;γmax) =K−1
2/summationdisplay
l=0/parenleftig
emϵ/parenleftbiggK
l/parenrightbigg
(1−eϵq)l(eϵq)K−l−/parenleftbiggK
l/parenrightbigg
(1−q)lqK−l/parenrightig
(98)
32Published in Transactions on Machine Learning Research (November/2024)
+K/summationdisplay
l=K+1
2/parenleftig/parenleftbiggK
l/parenrightbigg
(1−q)lqK−l−emϵ/parenleftbiggK
l/parenrightbigg
(1−eϵq)l(eϵq)K−l/parenrightig
The gradient w.r.t. qis
∇qf(q) =K−1
2/summationdisplay
l=0/parenleftig
emϵ/parenleftbiggK
l/parenrightbigg/parenleftig
(−eϵ)l(1−eϵq)l−1(eϵq)K−l+eϵ(K−l)(1−eϵq)l(eϵq)K−l−1/parenrightig
(99)
−/parenleftbiggK
l/parenrightbigg/parenleftig
−l(1−q)l−1qK−l+ (K−l)(1−q)lqK−l−1/parenrightig/parenrightig
+K/summationdisplay
l=K+1
2/parenleftig/parenleftbiggK
l/parenrightbigg/parenleftig
−l(1−q)l−1qK−l+ (K−l)(1−q)lqK−l−1/parenrightig
−emϵ/parenleftbiggK
l/parenrightbigg/parenleftig
(−eϵ)l(1−eϵq)l−1(eϵq)K−l+eϵ(K−l)(1−eϵq)l(eϵq)K−l−1/parenrightig/parenrightig
=−K−1
2/summationdisplay
l=1e(m+1)ϵ/parenleftbiggK−1
l−1/parenrightbiggK
ll(1−eϵq)l−1(eϵq)K−l+K−1
2/summationdisplay
l=0e(m+1)ϵ/parenleftbiggK−1
l/parenrightbiggK
K−l(K−l)(1−eϵq)l(eϵq)K−l−1
(100)
+K−1
2/summationdisplay
l=1/parenleftbiggK−1
l−1/parenrightbiggK
ll(1−q)l−1qK−l−K−1
2/summationdisplay
l=0/parenleftbiggK−1
l/parenrightbiggK
K−l(K−l)(1−q)lqK−l−1
−K/summationdisplay
l=K+1
2/parenleftbiggK−1
l−1/parenrightbiggK
ll(1−q)l−1qK−l+K−1/summationdisplay
l=K+1
2/parenleftbiggK−1
l/parenrightbiggK
K−l(K−l)(1−q)lqK−l−1
+K/summationdisplay
l=K+1
2e(m+1)ϵ/parenleftbiggK−1
l−1/parenrightbiggK
ll(1−eϵq)l−1(eϵq)K−l−K−1/summationdisplay
l=K+1
2e(m+1)ϵ/parenleftbiggK−1
l/parenrightbiggK
K−l(K−l)(1−eϵq)l(eϵq)K−l−1
=−KK−1
2/summationdisplay
l=1e(m+1)ϵ/parenleftbiggK−1
l−1/parenrightbigg
(1−eϵq)l−1(eϵq)K−l+KK−1
2/summationdisplay
l=0e(m+1)ϵ/parenleftbiggK−1
l/parenrightbigg
(1−eϵq)l(eϵq)K−l−1
(101)
+KK−1
2/summationdisplay
l=1/parenleftbiggK−1
l−1/parenrightbigg
(1−q)l−1qK−l−KK−1
2/summationdisplay
l=0/parenleftbiggK−1
l/parenrightbigg
(1−q)lqK−l−1
−KK/summationdisplay
l=K+1
2/parenleftbiggK−1
l−1/parenrightbigg
(1−q)l−1qK−l+KK−1/summationdisplay
l=K+1
2/parenleftbiggK−1
l/parenrightbigg
(1−q)lqK−l−1
+KK/summationdisplay
l=K+1
2e(m+1)ϵ/parenleftbiggK−1
l−1/parenrightbigg
(1−eϵq)l−1(eϵq)K−l−KK−1/summationdisplay
l=K+1
2e(m+1)ϵ/parenleftbiggK−1
l/parenrightbigg
(1−eϵq)l(eϵq)K−l−1
= 2Ke(m+1)ϵ/parenleftbiggK−1
K−1
2/parenrightbigg
(1−eϵq)K−1
2(eϵq)K−1
2−2K/parenleftbiggK−1
K−1
2/parenrightbigg
(1−q)K−1
2qK−1
2 (102)
Recallq∈[0,1
1+eϵ]and so (1−eϵq)(eϵq)≥(1−q)q. Furthermore, since e(m+1)ϵ≥1, there is∇qf(q)≥0.
This implies f(q)is monotonically non-decreasing in q, and so the local maximum on this boundary is
(q∗
local,q′∗
local) = arg max
(q,q′):q′=eϵq,q∈[0,1
1+eϵ]f(q,q′;γmax) = (1
1 +eϵ,1
1 +e−ϵ) (103)
33Published in Transactions on Machine Learning Research (November/2024)
That is,
(p∗
local,p′∗
local) = arg max
(p,p′):1−p′=eϵ(1−p),p∈[1
1+e−ϵ,1]f(p,p′;γmax) = (1−q∗
local,1−q′∗
local) = (1
1 +e−ϵ,1
1 +eϵ)
(104)
Part III: The global worst case probabilities.
Notice that (1
1+e−ϵ,1
1+eϵ), the maximum on the second boundary 1−p′=eϵ(1−p),∀p∈[1
1+e−ϵ,1], is indeed
the minimum on the first boundary p=eϵp′,∀p∈[0,1
1+e−ϵ+1].
Therefore, the global maximum given γmaxis
(p∗,p′∗) = arg max
(p,p′)∈Ff(p,p′;γmax) = arg max
(p,p′):p=eϵp′,p∈[0,1
1+e−ϵ]f(p,p′;γmax) = (0,0) (105)
and recall that f(0,0;γmax) =emϵ−1.
Hence, ifm≥K+1
2, by Lemma 3.4 DaRRMγmaxismϵ-differentially private.
B.2.2 Privacy Amplification Under A Small Privacy Allowance m≤K−1
2
The proof of Lemma B.10 is slightly more involved. First, recall by Lemma 3.1, γSub, the noise function that
makes the output of DaRRMγSuband the subsampling baseline the same, is
γSub(l) =γSub(K=l)
=

1−2/summationtextm
j=m+1
2(l
j)(K−l
m−j)
(K
m)ifmis odd
1−2/summationtextm
j=m
2+1(l
j)(K−l
m−j)
(K
m)−(lm
2)(K−l
m
2)
(K
m)ifmis even
forl∈{0,1,...,K}, suppose the privacy allowance m∈Z.
If we define h(l) :=

/summationtextm
j=m+1
2(l
j)(K−l
m−j)
(K
m)ifmis odd
/summationtextm
j=m
2+1(l
j)(K−l
m−j)
(K
m)−(lm
2)(K−l
m
2)
(K
m)ifmis even, thenγSub(l)can be written as γSub(l) =
/braceleftigg
1−2h(l)ifl≤K−1
2
2h(l)−1ifl≥K+1
2.
This can be generalized to a broader class of γfunctions — which we call the “symmetric form family” — as
follows
Definition B.6. γ:{0,1,...,K}→[0,1]is a member of the “symmetric form family” if γfollows
γ(l) =/braceleftigg
1−2h(l)ifl≤K−1
2
2h(l)−1ifl≥K+1
2(106)
whereh:{0,1,...,K}→[0,1]and
h(l) +h(K−l) = 1, h(l+ 1)≥h(l),∀l∈{0,1,...,K},andγ(K−1
2)>0,γ(K+ 1
2)>0
It is easy to verify any γfunction that belongs to the “symmetric form family” satisfies: 1) symmetric around
K
2and 2) the monotonicity assumption. Hence, Lemma B.1 can be invoked to find the worst case probabilities
34Published in Transactions on Machine Learning Research (November/2024)
given such γ, i.e., (p∗,p′∗) =arg max(p,p′)∈Ff(p,p′;γ), which in turn gives us the guarantee of DaRRMγ
beingmϵ-differentially private.
Roadmap. In this section, we restrict our search of a good γthat maximizes the utility of DaRRMγ
to in the “symmetric form family”. To show the main privacy amplification result under a small min
Lemma B.10, Section B.2.4, we need a few building blocks, shown in Section B.2.3. We first show in
Lemma B.7, Section B.2.3 two clean sufficient conditions that if a “symmetric form family” γsatisfies, then
DaRRMγismϵ-differentially private, in terms of the expectation of the γfunction applied to Binomial random
variables. The Binomial random variables appear in the lemma, because recall the sum of the observed
outcomes on a dataset D,L(D), follows a Binomial distribution in the i.i.d. mechanisms setting. Next, we
show a recurrence relationship that connects the expectation of Binomial random variables to Hypergeometric
random variables in Lemma B.9. This is needed because observe that for γfunctions that makes DaRRMγ
have the same output as the majority of subsampled mechanisms, the hfunction is now a sum of pmfs of the
Hypergeometric random variable.
Finally, the proof of the main result under a small m(Lemma B.10) is presented in Section B.2.4, based on
Lemma B.7 and Lemma B.9. We show in Lemma B.10 that γDSub, i.e., theγfunction that enables the output
ofDaRRMγDSuband outputting the majority of 2m−1subsampled mechanisms to be the same, belongs
to the “symmetric form family” and satisfies the sufficient conditions as stated in Lemma B.7, implying
DaRRMγDSubbeingmϵ-differentially private.
B.2.3 Building Blocks
Lemma B.7 (Privacy conditions of the “symmetric form family” functions) .Let random variables X∼
Binomial (K−1,p′),Y∼Binomial (K−1,eϵp′),ˆX∼Binomial (K−1,1−eϵ(1−p))andˆY∼Binomial (K−1,p).
For a function γ:{0,1,...,K}→[0,1]that belongs to the “symmetric form family” (Definition B.6), if γ
also satisfies both conditions as follows:
emϵEX[h(X+ 1)−h(X)]≥eϵEY[h(Y+ 1)−h(Y)],∀p′∈[0,1
1 +eϵ] (107)
e(m+1)ϵEˆX[h(ˆX+ 1)−h(ˆX)]≥EˆY[h(ˆY+ 1)−h(ˆY)],∀p∈[1
1 +e−ϵ,1] (108)
then Algorithm DaRRMγismϵ-differentially private.
Proof of Lemma B.7. Sinceh(l+1)≥h(l)onl∈{0,...,K},γ(l)≥γ(l+1),∀l≤K
2andγ(l+1)≥γ(l),∀l≥
K
2. Furthermore, since h(l)+h(K−l) = 1,γ(K−1
2) = 1−2h(K−1
2) = 1−2(1−h(K+1
2)) = 2h(K+1
2)−1. Hence,
anyγthat belongs to the “symmetric form family” satisfies: 1) symmetric aroundK
2, 2) the monotonicity
assumption, and 3) γ(K−1
2) =γ(K+1
2)>0.
Therefore, by Lemma B.1, the worst case probabilities (p∗,p′∗) =arg max(p,p′)∈Ff(p,p′;γ)are on one of the
two boundaries of F, satisfying
p∗=eϵp′∗, ∀p∗∈[0,1
e−ϵ+ 1],p′∗∈[0,1
1 +eϵ] (109)
or 1−p′∗=eϵ(1−p∗), ∀p∗∈[1
1 +e−ϵ,1],p′∗∈[1
1 +eϵ,1] (110)
We now derive the sufficient conditions that if any γfrom the “symmetric form family” satisfy, then DaRRMγ
ismϵ-differentially private, from the two boundaries as in Eq. 109 and Eq. 110 separately.
Part I: Deriving a sufficient condition from Eq. 109 for “symmetric form family” γ.
Consider the boundary of F,p=eϵp′,∀p∈[0,1
1+e−ϵ],p′∈[0,1
1+eϵ].
35Published in Transactions on Machine Learning Research (November/2024)
Given anyγ, pluggingp=eϵp′into the privacy cost objective f(p,p′;γ), one gets
f(p′;γ) =K−1
2/summationdisplay
l=0(emϵ/parenleftbiggK
l/parenrightbigg
p′l(1−p′)K−l−/parenleftbiggK
l/parenrightbigg
(eϵp′)l(1−eϵp′)K−l)·γ(l) (111)
+K/summationdisplay
l=K+1
2(/parenleftbiggK
l/parenrightbigg
(eϵp′)l(1−eϵp′)K−l−emϵ/parenleftbiggK
l/parenrightbigg
p′l(1−p′)K−l)·γ(l)
The gradient w.r.t. p′is
∇p′f(p′);γ
K=emϵK−1
2−1/summationdisplay
l=0/parenleftbiggK−1
l/parenrightbigg
p′l(1−p′)K−l−1/parenleftig
γ(l+ 1)−γ(l)/parenrightig
−2emϵ/parenleftbiggK−1
K−1
2/parenrightbigg
p′K−1
2(1−p′)K−1
2γ(K−1
2)
(112)
+emϵK−1/summationdisplay
l=K+1
2/parenleftbiggK−1
l/parenrightbigg
p′l(1−p′)K−l−1/parenleftig
γ(l)−γ(l+ 1)/parenrightig
+eϵK−1
2−1/summationdisplay
l=0/parenleftbiggK−1
l/parenrightbigg
(eϵp′)l(1−eϵp′)K−l−1/parenleftig
γ(l)−γ(l+ 1)/parenrightig
+ 2eϵ/parenleftbiggK−1
K−1
2/parenrightbigg
(eϵp′)K−1
2(1−eϵp′)K−1
2γ(K−1
2)
+eϵK−1/summationdisplay
l=K+1
2/parenleftbiggK−1
l/parenrightbigg
(eϵp′)l(1−eϵp′)K−l−1/parenleftig
γ(l+ 1)−γ(l)/parenrightig
Considerl∈{0,1,...,K}in the above Eq. 112. For any function γthat belongs to the “symmetric form
family”,
1. Ifl≤K
2,γ(l)−γ(l+ 1) = (1−2h(l))−(1−2h(l+ 1)) = 2h(l+ 1)−2h(l)
2. Ifl≥K
2,γ(l+ 1)−γ(l) = (2h(l+ 1)−1)−(2h(l)−1) = 2h(l+ 1)−2h(l)
3. Sinceγ(K−1
2) =γ(K+1
2),
2γ(K−1
2) =/parenleftig
γ(K−1
2) +γ(K+ 1
2)/parenrightig
(113)
=/parenleftig
1−2h(K−1
2) + 2h(K+ 1
2)−1/parenrightig
(114)
= 2h(K+ 1
2)−2h(K−1
2) (115)
Hence, following Eq. 112, the gradient, ∇p′f(p′;γ), given a “symmetric form family” γcan be written as
∇p′f(p′;γ)
K=−emϵK−1/summationdisplay
l=0/parenleftbiggK−1
l/parenrightbigg
p′l(1−p′)K−l/parenleftig
2h(l+ 1)−2h(l)/parenrightig
(116)
+eϵK−1/summationdisplay
l=0/parenleftbiggK−1
l/parenrightbigg
(eϵp′)l(1−eϵp′)K−l−1/parenleftig
2h(l+ 1)−2h(l)/parenrightig
=−2emϵEX[h(X+ 1)−h(X)] + 2eϵEY[h(Y+ 1)−h(Y)] (117)
whereX∼Binomial (K−1,p′)andY∼Binomial (K−1,eϵp′). The above implies
∇p′f(p′;γ)≤0⇐⇒eϵEY[h(Y+ 1)−h(Y)]≤emϵEX[h(X+ 1)−h(X)] (118)
36Published in Transactions on Machine Learning Research (November/2024)
If∇p′f(p′;γ)≤0, then we know the local worst case probabilities on the boundary p=eϵp′,∀p∈[0,1
1+e−ϵ]
given anyγis(p∗
local,p′∗
local) = arg max(p,p′):p=eϵp′,p∈[0,1
1+e−ϵ]f(p,p′;γ) = (0,0). Furthermore, recall the
privacy cost objective given any γis
f(p,p′;γ)
=K−1
2/summationdisplay
l=0(emϵα′
l−αl)·γ(l) +K/summationdisplay
l=K+1
2(αl−emϵα′
l)·γ(l)
=K−1
2/summationdisplay
l=0/parenleftig
emϵ/parenleftbiggK
l/parenrightbigg
p′l(1−p′)K−l−/parenleftbiggK
l/parenrightbigg
pl(1−p)K−l/parenrightig
·γ(l) +K/summationdisplay
l=K+1
2/parenleftig/parenleftbiggK
l/parenrightbigg
pl(1−p)K−l−emϵ/parenleftbiggK
l/parenrightbigg
p′l(1−p′)K−l/parenrightig
·γ(l)
and so for any γ,
f(0,0;γ) = (emϵ−1)·γ(0)≤emϵ−1 (119)
Also, notice the local minimum on this boundary is
(pmin,p′
min) = arg min
(p,p′):p=eϵp′,p∈[0,1
1+e−ϵ]f(p,p′l;γ) = (1
1 +e−ϵ,1
1 +eϵ) (120)
Part II: Deriving a sufficient condition from Eq. 110 for “symmetric form family” γ.
Consider the boundary of F,1−p′=eϵ(1−p),∀p∈[1
1+e−ϵ,1],p′∈[1
1+eϵ,1]. For simplicity, let q= 1−p∈
[0,1
1+eϵ]andq′= 1−p′∈[0,1
1+e−ϵ]. Pluggingq′=eϵqinto the privacy cost objective, one gets, given any γ,
f(q;γ) =K−1
2/summationdisplay
l=0/parenleftig
emϵ/parenleftbiggK
l/parenrightbigg
(1−eϵq)l(eϵq)K−l−/parenleftbiggK
l/parenrightbigg
(1−q)lqK−l/parenrightig
·γ(l) (121)
+K/summationdisplay
l=K+1
2/parenleftig/parenleftbiggK
l/parenrightbigg
(1−q)lqK−l−emϵ/parenleftbiggK
l/parenrightbigg
(1−eϵq)l(eϵq)K−l/parenrightig
·γ(l)
The gradient w.r.t. qis
∇qf(q;γ)
K=K−1
2−1/summationdisplay
l=0e(m+1)ϵ/parenleftbiggK−1
l/parenrightbigg
(1−eϵq)l(eϵq)K−l−1·/parenleftig
γ(l)−γ(l+ 1)/parenrightig
(122)
+K−1/summationdisplay
l=K+1
2/parenleftbiggK−1
l/parenrightbigg
(1−eϵq)l(eϵq)K−l−1·/parenleftig
γ(l+ 1)−γ(l)/parenrightig
+ 2e(m+1)ϵ/parenleftbiggK−1
K−1
2/parenrightbigg
(1−eϵq)K−1
2(eϵq)K−1
2·γ(K−1
2)
+K−1
2−1/summationdisplay
l=0/parenleftbiggK−1
l/parenrightbigg
(1−q)lqK−l−1·/parenleftig
γ(l+ 1)−γ(l)/parenrightig
+K−1/summationdisplay
l=K+1
2(1−q)lqK−l−1·/parenleftig
γ(l)−γ(l+ 1)/parenrightig
−2/parenleftbiggK−1
K−1
2/parenrightbigg
(1−q)K−1
2qK−1
2·γ(K−1
2)
For any function γthat belongs to the “symmetric form family”, the gradient ∇qf(q;γ)can be written as
∇qf(q;γ)
K=e(m+1)ϵK−1/summationdisplay
l=0/parenleftbiggK−1
l/parenrightbigg
(1−eϵq)l(eϵq)K−l−1·/parenleftig
2h(l+ 1)−2h(l)/parenrightig
(123)
37Published in Transactions on Machine Learning Research (November/2024)
−K/summationdisplay
l=0/parenleftbiggK−1
l/parenrightbigg
(1−q)lqK−l−1·/parenleftig
2h(l+ 1)−2h(l)/parenrightig
= 2e(m+1)ϵEˆX[h(ˆX+ 1)−h(ˆX)]−2EˆY[h(ˆY+ 1)−h(ˆY)] (124)
where ˆX∼Binomial (K−1,1−eϵ(1−p))and ˆY∼Binomial (K−1,p). The above implies
∇qf(q;γ)≥0⇐⇒e(m+1)ϵEˆX[h(ˆX+ 1)−h(ˆX)]≥EˆY[h(ˆY+ 1)−h(ˆY)] (125)
If∇qf(q;γ)≥0, then since q∈[0,1
1+eϵ], we know that the local maximum given any γis(q∗
local,q′∗
local) =
arg max(q,q′):q′=eϵq,q∈[0,1
1+eϵ]f(q,q′;γ) = (1
1+eϵ,1
1+e−ϵ). That is,
(p∗
local,p′∗
local) = arg max
(p,p′):1−p′=eϵ(1−p),p∈[1
1+e−ϵ,1]f(p,p′;γ) = (1−q∗
local,1−q′∗
local) = (1
1 +e−ϵ,1
1 +eϵ)
Notice by Eq. 120, the above (1
1+e−ϵ,1
1+eϵ)is the local minimum on the first boundary p=eϵp′,∀p∈[0,1
1+e−ϵ].
Therefore, given an arbitrary γfunction, if it satisfies both of the following:
1. On the boundary p=eϵp′,∀p∈[0,1
1+e−ϵ],∇p′f(p′;γ)≤0
2. On the boundary 1−p′=eϵ(1−p),∀p∈[1
1+e−ϵ,1],∇q′f(q′;γ)≥0whereq′= 1−p′
then the global worst case probabilities given this γis(p∗,p′∗) =arg max(p,p′)∈Ff(p,p′;γ) = (0,0). Further-
more, since by Eq. 119, f(0,0;γ)≤emϵ−1for anyγ, this implies DaRRMγismϵ-differentially private by
Lemma 3.4.
Now, ifγbelongs to the “symmetric form family”, by Eq. 118 and Eq. 125, the sufficient conditions for γ
that enables DaRRMγto bemϵ-differentially private are hence
eϵEY[h(Y+ 1)−h(Y)]≤emϵEX[h(X+ 1)−h(X)],∀p′∈[0,1
1 +eϵ]
ande(m+1)ϵEˆX[h(ˆX+ 1)−h(ˆX)]≥EˆY[h(ˆY+ 1)−h(ˆY)],∀p∈[1
1 +e−ϵ,1]
whereX∼Binomial (K−1,p′),Y∼Binomial (K−1,eϵp′),ˆX∼Binomial (K−1,1−eϵ(1−p))and
ˆY∼Binomial (K−1,p).
Lemma B.8 (Binomial Expectation Recurrence Relationship (Theorem 2.1 of Zhang et al. (2019))) .Let
X(K−1)∼Binomial (K−1,p)andX(K)∼Binomial (K,p). Letg(x)be a function with −∞<E[g(X(K−1))]<
∞and−∞<g(−1)<∞, then
KpEX(K−1)[g(X(K−1))] =EX(K)[X(K)g(X(K)−1)] (126)
Lemma B.9. Giveni,m,K∈Z,K≥1,0≤i≤m≤K, letX(K)∼Binomial (K,p)for somep∈[0,1],
there is
1/parenleftbigK
m/parenrightbigEX(K)/bracketleftbigg/parenleftbiggX
i/parenrightbigg/parenleftbiggK−X
m−i/parenrightbigg/bracketrightbigg
=/parenleftbiggm
i/parenrightbigg
pi(1−p)m−i(127)
Proof of Lemma B.9. We show the above statement in Eq. 127 by induction on Kandm.
Base Case: K= 1.
1. Ifm= 0, theni= 0.1
(1
0)EX(1)[/parenleftbigX
0/parenrightbig/parenleftbig1−X
0/parenrightbig
] =EX(1)[1] = 1, and/parenleftbig0
0/parenrightbig
p0(1−p)0= 1.
38Published in Transactions on Machine Learning Research (November/2024)
2. Ifm= 1,
(a)i= 0,1
(1
1)EX(1)[/parenleftbigX
0/parenrightbig/parenleftbig1−X
1/parenrightbig
] =EX(1)[1−X] = 1−p, and/parenleftbig1
0/parenrightbig
p0(1−p)1= 1−p
(b)i= 1,1
(1
1)EX(1)[/parenleftbigX
1/parenrightbig/parenleftbig1−X
0/parenrightbig
] =EX(1)[X] =p, and/parenleftbig1
1/parenrightbig
p1(1−p)0=p.
Hence, Eq. 127 holds for the base case.
Induction Hypothesis: Suppose the statement holds for some K≥1and 0≤i≤m≤K. Consider
1≤i≤m≤K+ 1,
1/parenleftbigK+1
m/parenrightbigEX(K+1)/bracketleftbigg/parenleftbiggX
i/parenrightbigg/parenleftbiggK+ 1−X
m−i/parenrightbigg/bracketrightbigg
(128)
=1/parenleftbigK+1
m/parenrightbigEX(K+1)[X!
i!(X−i)!(K+ 1−X)!
(m−i)!(K+ 1−X−(m−i))!] (129)
=1/parenleftbigK+1
m/parenrightbig
i!(m−i)!EX(K+1)[X(X−1)!
((X−1)−(i−1))!(K−(X−1))!
(K−(X−1)−((m−1)−(i−1)))!](130)
=1/parenleftbigK+1
m/parenrightbig
i!(m−i)!EX(K)[X!
(X−(i−1))!(K−X)!
(K−X−((m−1)−(i−1)))!] (131)
(By Lemma B.8)
=(i−1)!(m−i)!/parenleftbigK+1
m/parenrightbig
i!(m−i)!EX(K)[/parenleftbiggX
i−1/parenrightbigg/parenleftbiggK−X
(m−1)−(i−1)/parenrightbigg
] (132)
=(i−1)!/parenleftbigK+1
m/parenrightbig
i!(K+ 1)p/parenleftbiggK
m−1/parenrightbigg/parenleftbiggm−1
i−1/parenrightbigg
pi−1(1−p)m−i(133)
(By Induction Hypothesis)
=m!(K+ 1−m)!
(K+ 1)!iK!
(m−1)!(K−m+ 1)!(m−1)!
(i−1)!(m−i)!(K+ 1)pi(1−p)m−i(134)
=m!
i!(m−i)!pi(1−p)m−i=/parenleftbiggm
i/parenrightbigg
pi(1−p)m−i(135)
Now we consider the edge cases when 0 =i≤m.
Ifi= 0andm= 0,
1/parenleftbigK+1
0/parenrightbigEX(K+1)[/parenleftbiggX
0/parenrightbigg/parenleftbiggK+ 1−X
0/parenrightbigg
] = 1·EX(K+1)[1] = 1 =/parenleftbigg0
0/parenrightbigg
p0(1−p)0(136)
Ifi= 0andm> 0,
1/parenleftbigK+1
m/parenrightbigEX(K+1)[/parenleftbiggK+ 1−X
m/parenrightbigg
] (137)
=1/parenleftbigK+1
m/parenrightbigK+1/summationdisplay
x=0/parenleftbiggK+ 1−x
m/parenrightbigg/parenleftbiggK+ 1
x/parenrightbigg
px(1−p)K+1−x(138)
=1/parenleftbigK+1
m/parenrightbigK+1/summationdisplay
x=0/parenleftbiggK+ 1−x
m/parenrightbigg/parenleftig/parenleftbiggK
x/parenrightbigg
+/parenleftbiggK
x−1/parenrightbigg
I{x≥1}/parenrightig
px(1−p)K+1−x(139)
=1/parenleftbigK+1
m/parenrightbigK/summationdisplay
x=0/parenleftbiggK+ 1−x
m/parenrightbigg/parenleftbiggK
x/parenrightbigg
px(1−p)K+1−x+1/parenleftbigK+1
m/parenrightbigK+1/summationdisplay
x=1/parenleftbiggK+ 1−x
m/parenrightbigg/parenleftbiggK
x−1/parenrightbigg
px(1−p)K+1−x(140)
(Since when x=K+ 1andm> 0,/parenleftbiggK+ 1−x
m/parenrightbigg
= 0)
39Published in Transactions on Machine Learning Research (November/2024)
=1/parenleftbigK+1
m/parenrightbig/parenleftigK/summationdisplay
x=0/parenleftbiggK−x
m/parenrightbigg/parenleftbiggK
x/parenrightbigg
px(1−p)K+1−x+K/summationdisplay
x=0/parenleftbiggK−x
m−1/parenrightbigg/parenleftbiggK
x/parenrightbigg
px(1−p)K+1−x/parenrightig
(141)
+1/parenleftbigK+1
m/parenrightbigK/summationdisplay
x=0/parenleftbiggK−x
m/parenrightbigg/parenleftbiggK
x/parenrightbigg
px+1(1−p)K−x
(Since/parenleftbiggK+ 1−x
m/parenrightbigg
=/parenleftbiggK−x
m/parenrightbigg
+/parenleftbiggK−x
m−1/parenrightbigg
)
=1/parenleftbigK+1
m/parenrightbig/parenleftig
(1−p)EX(K)[/parenleftbiggK−X
m/parenrightbigg
] + (1−p)EX(k)[/parenleftbiggK−X
m−1/parenrightbigg
]/parenrightig
+1/parenleftbigK+1
m/parenrightbigpEX(K)[/parenleftbiggK−X
m/parenrightbigg
] (142)
=1/parenleftbigK+1
m/parenrightbig/parenleftig
EX(K)[/parenleftbiggK−X
m/parenrightbigg
] + (1−p)EX(K)[/parenleftbiggK−X
m−1/parenrightbigg
]/parenrightig
(143)
=1/parenleftbigK+1
m/parenrightbig/parenleftig/parenleftbiggK
m/parenrightbigg
(1−p)m+ (1−p)/parenleftbiggK
m−1/parenrightbigg
(1−p)m−1/parenrightig
(144)
(By Induction Hypothesis) (145)
=1/parenleftbigK+1
m/parenrightbig/parenleftbiggK+ 1
m/parenrightbigg
(1−p)m(146)
= (1−p)m(147)
Hence, Eq. 127 holds for all K≥1and0≤i≤m≤K.
B.2.4 Main Result: Privacy Amplification Under a Small m
Lemma B.10 (Privacy amplification, m≤K−1
2).Consider using DaRRM(Algorithm 1) to solve Problem 1.1,
with i.i.d. mechanisms {Mi}K
i=1,pi=p,p′
i=p′,∀i∈[K], the privacy allowance 1≤m≤K−1
2,m∈Zand
δ= ∆ = 0. Let the noise function be that
γDSub(l) =/braceleftigg
1−2h(l)∀l∈{0,1,...,K−1
2}
2h(l)−1∀l∈{K+1
2,...,K}(148)
whereh:{0,1,...,K} → [0,1]andh(l) =/summationtext2m−1
i=m(l
i)(K−l
2m−1−i)
(K
2m−1),∀l∈ {0,1,...,K}, then Algorithm
DaRRMγDSubismϵ-differentially private.
Proof of Lemma B.10. First, note γDSubbelongs to the “symmetric form family”. We show γDSubsatisfies
the two sufficient conditions in Lemma B.7 and hence by Lemma B.7, DaRRMγDSubismϵ-differentially private.
Specifically, we consider h(l) =/summationtext2m−1
i=m(l
i)(K−l
2m−1−i)
(K
2m−1),∀l∈{0,1,...,K}and1≤m≤K.
Two show the first condition is satisfied, let X(K−1)∼Binomial (K−1,p)andY(K−1)∼Binomial (K−1,eϵp),
and consider p∈[0,1
1+eϵ].
EX(K−1)[h(X+ 1)] =1/parenleftbigK
2m−1/parenrightbig2m−1/summationdisplay
i=mEX(K−1)[/parenleftbiggX+ 1
i/parenrightbigg/parenleftbiggK−X−1
2m−1−i/parenrightbigg
] (149)
=1/parenleftbigK
2m−1/parenrightbig2m−1/summationdisplay
i=mEX(K−1)[/parenleftbiggX
i/parenrightbigg/parenleftbiggK−X−1
2m−1−i/parenrightbigg
+/parenleftbiggX
i−1/parenrightbigg/parenleftbiggK−X−1
2m−1−i/parenrightbigg
] (150)
(Since/parenleftbiggX+ 1
i/parenrightbigg
=/parenleftbiggX
i/parenrightbigg
+/parenleftbiggX
i−1/parenrightbigg
I{i≥1})
40Published in Transactions on Machine Learning Research (November/2024)
=1/parenleftbigK
2m−1/parenrightbig2m−1/summationdisplay
i=m/parenleftig
EX(K−1)[/parenleftbiggX
i/parenrightbigg/parenleftbiggK−1−X
2m−1−i/parenrightbigg
] +EX(K−1)[/parenleftbiggX
i−1/parenrightbigg/parenleftbiggK−1−X
(2m−2)−(i−1)/parenrightbigg
]/parenrightig
(151)
=1/parenleftbigK
2m−1/parenrightbig2m−1/summationdisplay
i=m/parenleftig/parenleftbiggK−1
2m−1/parenrightbigg/parenleftbigg2m−1
i/parenrightbigg
pi(1−p)2m−1−i+/parenleftbiggK−1
2m−2/parenrightbigg/parenleftbigg2m−2
i−1/parenrightbigg
pi−1(1−p)2m−1−i/parenrightig
(152)
(By Lemma B.9)
EX(K−1)[h(X)] =1/parenleftbigK
2m−1/parenrightbig2m−1/summationdisplay
i=mEX(K−1)[/parenleftbiggX
i/parenrightbigg/parenleftbiggK−X
2m−1−i/parenrightbigg
] (153)
(Since/parenleftbiggK−X
2m−1−i/parenrightbigg
=/parenleftbiggK−1−X
2m−1−i/parenrightbigg
+/parenleftbiggK−1−X
2m−2−i/parenrightbigg
)
=1/parenleftbigK
2m−1/parenrightbig2m−1/summationdisplay
i=m/parenleftig
EX(K−1)[/parenleftbiggX
i/parenrightbigg/parenleftbiggK−1−X
2m−1−i/parenrightbigg
] +EX(K−1)[/parenleftbiggX
i/parenrightbigg/parenleftbiggK−1−X
2m−2−i/parenrightbigg
]I{i≤2m−2}/parenrightig
(154)
=1/parenleftbigK
2m−1/parenrightbig2m−1/summationdisplay
i=m/parenleftig/parenleftbiggK−1
2m−1/parenrightbigg/parenleftbigg2m−1
i/parenrightbigg
pi(1−p)2m−1−i+/parenleftbiggK−1
2m−2/parenrightbigg/parenleftbigg2m−2
i/parenrightbigg
pi(1−p)2m−2−iI{i≤2m−2}/parenrightig
(155)
(By Lemma B.9)
Hence, following Eq. 155 and Eq. 152,
EX(K−1)[h(X+ 1)−h(X)] (156)
=1/parenleftbigK
2m−1/parenrightbig/parenleftig2m−1/summationdisplay
i=m/parenleftbiggK−1
2m−2/parenrightbigg/parenleftbigg2m−2
i−1/parenrightbigg
pi−1(1−p)2m−1−i−2m−2/summationdisplay
i=m/parenleftbiggK−1
2m−2/parenrightbigg/parenleftbigg2m−2
i/parenrightbigg
pi(1−p)2m−2−i/parenrightig
(157)
=1/parenleftbigK
2m−1/parenrightbig/parenleftig2m−2/summationdisplay
i=m−1/parenleftbiggK−1
2m−2/parenrightbigg/parenleftbigg2m−2
i/parenrightbigg
pi(1−p)2m−2−i−2m−2/summationdisplay
i=m/parenleftbiggK−1
2m−2/parenrightbigg/parenleftbigg2m−2
i/parenrightbigg
pi(1−p)2m−2−i/parenrightig
(158)
=2m−1
K/parenleftbigg2m−2
m−1/parenrightbigg
pm−1(1−p)m−1(159)
Similarly,
EY(K−1)[h(Y+ 1)−h(Y)] =2m−1
K/parenleftbigg2m−2
m−1/parenrightbigg
(eϵp)m−1(1−eϵp)m−1(160)
Sincep∈[0,1
1+eϵ], there isp(1−p)≥e−ϵeϵp(1−eϵp). Hence,
e(m−1)ϵEX(K−1)[h(X+ 1)−h(X)] =2m−1
K/parenleftbigg2m−2
m−1/parenrightbigg
e(m−1)ϵpm−1(1−p)m−1(161)
≥2m−1
K/parenleftbigg2m−2
m−1/parenrightbigg
e(m−1)ϵ(e−ϵeϵp(1−eϵp))m−1(162)
=2m−1
K/parenleftbigg2m−2
m−1/parenrightbigg
(eϵp)m−1(1−eϵp)m−1(163)
41Published in Transactions on Machine Learning Research (November/2024)
=EY(K−1)[h(Y+ 1)−h(Y)] (164)
implying
emϵEX(K−1)[h(X+ 1)−h(X)]≥eϵEY(K−1)[h(Y+ 1)−h(Y)] (165)
and the first condition is satisfied.
To show the second condition is satisfied, let ˆX(K−1)∼Binom (K−1,1−eϵ(1−p))andˆY(K−1)∼Binom (K−
1,p), and consider p∈[1
1+e−ϵ,1].
EˆX(K−1)[h(ˆX+ 1)] =1/parenleftbigK
2m−1/parenrightbig2m−1/summationdisplay
i=m/parenleftig
EˆX(K−1)[/parenleftbiggˆX
i/parenrightbigg/parenleftbiggK−1−ˆX
2m−1−i/parenrightbigg
] +EˆX(K−1)[/parenleftbiggˆX
i−1/parenrightbigg/parenleftbiggK−1−ˆX
(2m−2)−(i−1)/parenrightbigg
]/parenrightig
(166)
=1/parenleftbigK
2m−1/parenrightbig2m−1/summationdisplay
i=m/parenleftig/parenleftbiggK−1
2m−1/parenrightbigg/parenleftbigg2m−1
i/parenrightbigg
(1−eϵ(1−p))i(eϵ(1−p))2m−1−i(167)
+/parenleftbiggK−1
2m−2/parenrightbigg/parenleftbigg2m−2
i−1/parenrightbigg
(1−eϵ(1−p))i−1(eϵ(1−p))2m−1−i/parenrightig
By Lemma B.9
and
EˆX(K−1)[h(ˆX)] =1/parenleftbigK
2m−1/parenrightbig2m−1/summationdisplay
i=m/parenleftig
EˆX(K−1)[/parenleftbiggˆX
i/parenrightbigg/parenleftbiggK−1−ˆX
2m−1−i/parenrightbigg
] +EˆX(K−1)[/parenleftbiggˆX
i/parenrightbigg/parenleftbiggK−1−ˆX
2m−2−i/parenrightbigg
]I{i≤2m−2}/parenrightig
(168)
=1/parenleftbigK
2m−1/parenrightbig2m−1/summationdisplay
i=m/parenleftig/parenleftbiggK−1
2m−1/parenrightbigg/parenleftbigg2m−1
i/parenrightbigg
(1−eϵ(1−p))i(eϵ(1−p))2m−1−i(169)
+/parenleftbiggK−1
2m−2/parenrightbigg/parenleftbigg2m−2
i/parenrightbigg
(1−eϵ(1−p))i(eϵ(1−p))2m−2−iI{i≤2m−2}/parenrightig
By Lemma B.9
Hence, following Eq. 167 and Eq. 169,
EˆX(K−1)[h(ˆX+ 1)−h(ˆX)] (170)
=1/parenleftbigK
2m−1/parenrightbig/parenleftig2m−1/summationdisplay
i=m/parenleftbiggK−1
2m−2/parenrightbigg/parenleftbigg2m−2
i−1/parenrightbigg
(1−eϵ(1−p))i−1(eϵ(1−p))2m−1−i(171)
−2m−2/summationdisplay
i=m/parenleftbiggK−1
2m−2/parenrightbigg/parenleftbigg2m−2
i/parenrightbigg
(1−eϵ(1−p))i(eϵ(1−p))2m−2−i/parenrightig
=1/parenleftbigK
2m−1/parenrightbig/parenleftig2m−2/summationdisplay
i=m−1/parenleftbiggK−1
2m−2/parenrightbigg/parenleftbigg2m−2
i/parenrightbigg
(1−eϵ(1−p))i(eϵ(1−p))2m−2−i(172)
−2m−2/summationdisplay
i=m/parenleftbiggK−1
2m−2/parenrightbigg/parenleftbigg2m−2
i/parenrightbigg
(1−eϵ(1−p))i(eϵ(1−p))2m−2−i/parenrightig
=2m−1
K/parenleftbigg2m−2
m−1/parenrightbigg
(1−eϵ(1−p))m−1(eϵ(1−p))m−1(173)
Similarly,
EˆY(K−1)[h(ˆY+ 1)−h(ˆY)] =2m−1
K/parenleftbigg2m−2
m−1/parenrightbigg
pm−1(1−p)m−1(174)
42Published in Transactions on Machine Learning Research (November/2024)
Hence,
e(m+1)ϵEˆX(K−1)[h(ˆX+ 1)−h(ˆX)] =e(m+1)ϵ2m−1
K/parenleftbigg2m−2
m−1/parenrightbigg
(1−eϵ(1−p))m−1(eϵ(1−p))m−1(175)
≥2m−1
K/parenleftbigg2m−2
m−1/parenrightbigg
(1−eϵ(1−p))m−1e(m−1)ϵ(1−p)m−1(176)
=2m−1
K/parenleftbigg2m−2
m−1/parenrightbigg
(eϵ−e2ϵ(1−p))m−1(1−p)m−1(177)
Note that
eϵ−e2ϵ(1−p) =eϵ−e2ϵ+e2ϵp≥p (178)
⇐⇒ (eϵ+ 1)(eϵ−1)p≥eϵ(eϵ−1) (179)
⇐⇒p≥eϵ
eϵ+ 1=1
1 +e−ϵ(180)
and the condition needs to hold for p∈[1
1+e−ϵ,1].
Therefore, following Eq. 177,
e(m+1)ϵEˆX(K−1)[h(ˆX+ 1)−h(ˆX)]≥2m−1
K/parenleftbigg2m−2
m−1/parenrightbigg
pm−1(1−p)m−1(181)
=EˆY(K−1)[h(ˆY+ 1)−h(ˆY)] (182)
implying the second condition is satisfied.
Therefore, by Lemma B.7, DaRRMγDSubismϵ-differentially private.
B.3 Comparing the Utility of Subsampling Approaches
Intuitively, if we subsample 2m−1mechanisms, the utility is higher than that of the naïve subsampling
approach which outputs the majority based on only mmechanisms. To complete the story, we formally
compare the utility of outputting the majority of 2m−1subsampled mechanisms (Theorem 4.1) and outputting
the majority of msubsampled mechanisms (simple composition, Theorem 2.2) in the i.i.d. mechanisms and
pure differential privacy setting, fixing the output privacy loss to be mϵ.
Lemma B.11. Consider Problem 1.1 with i.i.d. mechanisms {Mi}K
i=1, i.e.,p=pi=Pr[Mi(D) = 1],p′=
p′
i=Pr[Mi(D′) = 1],∀i∈[K]. Letγ1:{0,1,...,K}→[0,1],γ2:{0,1,...,K}→[0,1]be two functions that
are both symmetric aroundK
2. If1≥γ1(l)≥γ2(l)≥0,∀l∈{0,...,K}, thenE(DaRRMγ1)≤E(DaRRMγ2).
Proof.RecallS={S1,...,SK}, whereSi∼Mi(D), is the set of observed outcomes from the mechanisms
{Mi}K
i=1. By Definition 2.4, for any γthat is symmetric aroundK
2, the error of DaRRMγis
E(DaRRMγ) =/vextendsingle/vextendsingle/vextendsinglePr[DaRRMγ(D) = 1]−Pr[g(S) = 1]/vextendsingle/vextendsingle/vextendsingle (183)
=/vextendsingle/vextendsingle/vextendsingleK/summationdisplay
l=K+1
2/parenleftig
γ(l) +1
2(1−γ(l))/parenrightig
·αl+K−1
2/summationdisplay
l=01
2(1−γ(l))·αl−K/summationdisplay
l=K+1
2αl/vextendsingle/vextendsingle/vextendsingle (184)
=/vextendsingle/vextendsingle/vextendsingleK/summationdisplay
l=K+1
2/parenleftig1
2γ(l)−1
2/parenrightig
·αl+K−1
2/summationdisplay
l=0/parenleftig1
2−1
2γ(l)/parenrightig
·αl/vextendsingle/vextendsingle/vextendsingle (185)
=/vextendsingle/vextendsingle/vextendsingle1
2K/summationdisplay
l=K+1
2(1−γ(l))·(αl−αK−l)/vextendsingle/vextendsingle/vextendsingle (186)
43Published in Transactions on Machine Learning Research (November/2024)
whereαl=/parenleftbigK
l/parenrightbig
pl(1−p)K−l,∀l∈{0,1,...,K}and recallp= Pr[Mi(D) = 1],∀i∈[K].
For anyl≥K+1
2,
1. Ifp= 0orp= 1,αl=αK−l.
2. Otherwise, for p∈(0,1),
(a) Ifp≥1
2,
αl
αK−l=pl(1−p)K−l
pK−l(1−p)l=p2l−K(1−p)K−2l= (p
1−p/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
≥1)2l−K/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
≥0≥1,⇒αl≥αK−l(187)
(b) Ifp<1
2,
αl
αK−l= (p
1−p/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
≤1)2l−K/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
≥0≤1,⇒αl≤αK−l (188)
Hence, ifp≥1
2, thenαl≥αK−l,∀l≥K+1
2. Sinceγ1(l)≥γ2(l),∀l∈{0,...,K},1−γ1(l)≤1−γ2(l), and so
E(DaRRMγ1) =K/summationdisplay
l=K+1
21
2(1−γ1(l))·(αl−αK−l)≤K/summationdisplay
l=K+1
21
2(1−γ2(l))·(αl−αK−l) =E(DaRRMγ2)(189)
Similarly, if p<1
2, thenαl≤αK−l,∀l≥K+1
2and
E(DaRRMγ1) =K/summationdisplay
l=K+1
21
2(1−γ1(l))·(αK−l−αl)≤K/summationdisplay
l=K+1
21
2(1−γ2(l))·(αK−l−αl) =E(DaRRMγ2))(190)
Therefore,
E(DaRRMγ1)≤E(DaRRMγ2) (191)
SinceγDSub(l)≥γSub(l),∀l∈{0,1,...,K}, by Lemma B.11, E(DaRRMγDSub)≤E(DaRRMγSub)— that is,
outputting 2m−1mechanisms has a higher utility than outputting mmechanisms.
44Published in Transactions on Machine Learning Research (November/2024)
C Details of Section 5: Optimizing the Noise Function γin DaRRM
C.1 Deriving the Optimization Objective
For anyγfunction that is symmetric aroundK
2, we can write the optimization objective as
Ep1,p2,...,pK∼T[E(DaRRMγ)] (192)
=Ep1,p2,...,pK∼T[|Pr[DaRRMγ(D) = 1]−Pr[g(S) = 1]|] (193)
=Ep1,p2,...,pK∼T
/vextendsingle/vextendsingle/vextendsingleK/summationdisplay
l=K+1
2/parenleftig
αl·(γ(l) +1
2(1−γ(l)))−αl/parenrightig
+K−1
2/summationdisplay
l=0αl·1
2(1−γ(l))/vextendsingle/vextendsingle/vextendsingle
 (194)
=Ep1,p2,...,pK∼T
/vextendsingle/vextendsingle/vextendsingleK−1
2/summationdisplay
l=0αl(1
2γ(l)−1
2) +K/summationdisplay
l=K+1
2αl(1
2−1
2γ(l))/vextendsingle/vextendsingle/vextendsingle
 (195)
The above follows by conditioning on L=l∈{0,1,...,K}, i.e. the sum of observed outcomes in S
=Ep1,p2,...,pK∼T
/vextendsingle/vextendsingle/vextendsingle1
2K/summationdisplay
l=K+1
2(αl−αK−l) (1−γ(l))/vextendsingle/vextendsingle/vextendsingle
 (196)
The above follows by symmetry of γ
Furthermore, notice the objective is symmetric around 0, and can be written as
Ep1,p2,...,pK∼T
1
2K/summationdisplay
l=K+1
2(αl−αK−l) (1−γ(l))
 (197)
=1
2Ep1,p2,...,pK∼T
K/summationdisplay
l=K+1
2/parenleftig
(αl−αK−l)−(αl−αK−l)γ(l)/parenrightig
 (198)
=1
2Ep1,p2,...,pK∼T
K/summationdisplay
l=K+1
2(αl−αK−l)

/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
:=A−1
2Ep1,p2,...,pK∼T
K/summationdisplay
l=K+1
2(αl−αK−l)γ(l)

/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
:=B(199)
Since expression Ain Eq. 199 does not involve γ, we only need to optimize expression Bin Eq. 199. That is,
−1
2Ep1,p2,...,pK∼T
K/summationdisplay
l=K+1
2(αl−αK−l)γ(l)
 (200)
=−1
2K/summationdisplay
l=K+1
2Ep1,p2,...,pK∼T[(αl−αK−l)]·γ(l) (201)
Eq. 201 is the optimization objective we use in the experiments. We see the optimization objective is linear
inγ.
Note in the general setting, L(D)∼PoissonBinomial (p1,p2,...,pK), where recallL(D)is the sum of observed
outcomes on dataset D, and hence, αl=Pr[L(D) =l]is the pmf of the Poisson Binomial distribution at
l∈{0,1,...,K}.
C.2 Practical Approximation of the Objective
Since the optimization objective in Eq. 200 requires taking an expectation over p1,...,pK, and this invovles
integrating over Kvariables, which can be slow in practice, we propose the following approximation to
45Published in Transactions on Machine Learning Research (November/2024)
efficiently compute the objective. We start with a simple idea to compute the objective, by sampling pi’s
from [0,1]and take an empirical average of the objective value over all subsampled sets of p1,...,pKas the
approximation of the expectation in Section C.2.1. However, we found this approach is less numerically stable.
We then propose the second approach to approximate the objective in Section C.2.2, which approximates the
integration over pi’s using the rectangular rule instead of directly approximating the objective value. We use
the second approximation approach in our experiments and empirically demonstrates its effectiveness. Note
approximating the optimization objective does not affect the privacy guarantee.
C.2.1 Approximation via Direct Sampling of pi’s
One straightforward way of efficiently computing an approximation to the optimization objective is as follows:
Algorithm 4 Straightforward Approximation of the Optimization Objective
1:Input: # mechanisms K∈N, # iterations T∈N, noise function γ:{0,1,...,K}→[0,1]
2:fort= 1,2,...,Tdo
3:Sample ˆp1,ˆp2,..., ˆpK∼T
4:/hatwideL←PoissonBinomail (ˆp1,..., ˆpK)
5: ˆαl←Pr[/hatwideL=l],∀l{0,...,K}
6:gt←−1
2/summationtextK
l=K+1
2(ˆαl−ˆαK−l)·γ(l)
7:end for
8:Return1
T/summationtextT
t=1gt
However, we found this approximation is not very numerically stable even for T= 10000 in the experiments
and so we propose to adopt the second approximation as follows.
C.2.2 Approximating the Integration Over pi’s
Consider the following surrogate objective:
−1
2K/summationdisplay
l=K+1
2/integraldisplay1
0.5/integraldisplay1
0.5···/integraldisplay1
0.5(αl−αK−l)dp1dp2...dpK·γ(l) (202)
whereweapproximatetheintegrationinsteadofdirectlyapproximatingtheobjectivevalue. Theapproximation
of the integration is based on the rectangular rule and that the Poisson Binomial distribution is invariant to
the order of its probability parameters.
First, we discretize the integration over pi’s: pickτ= 50points representing probabilities between [0.5,1)
with equal distance θ=0.5
τ. Denote this set of points as W. We pick only τ= 50samples to ensure the
distance between each sample, i.e., θ, is not too small; or this can cause numerical instability. For each
l∈{K+1
2,K+1
2+ 1,...,K}, we want to compute an approximated coefficient for γ(l)as follows:
/integraldisplay1
0.5/integraldisplay1
0.5···/integraldisplay1
0.5(αl−αK−l)dp1dp2...dpK≈/summationdisplay
p1∈W/summationdisplay
p2∈W···/summationdisplay
pK∈W(αl−αK−l) (203)
which approximates integration over a K-dimensional grid WK.
The idea is then to sample points from this K-dimensional grid WKand compute an empirical mean of the
integration based on the sample probabilities for p1,...,pKfromWKas the approximation of the integration
in the objective.
Let(s1,s2,...,sK)be randomly sampled probability values from WKand we want to compute (αl−αK−l)
for alllbased on (p1,...,pK) = (s1,...,sK). To apply the rectangular rule, since the grid of probabilities
isK-dimensional, the weight of (αl−αK−l)in the approximate integration is θK. Furthermore, observe
46Published in Transactions on Machine Learning Research (November/2024)
thatαlis the pmf at lfrom a Poison Binomial distribution in our case, and PoissonBinomial (p1,...,pK)dist.∼
PoissonBinomial (π(p1,...,pK)), whereπdenotes a permutation of p1,...,pKanddist.∼denotes “the same
distribution”. Hence, with a single probability sample (s1,...,sK), we can indeed compute αl−αK−lfor each
latK!points from the grid WK, since they all have the same value. Therefore, we should set the weight of
αl−αK−lin the approximate integration as w=θK·K!. Furthermore, since the order of (p1,...,pK)does
not affect the objective value, there is a total of ( τchooseKwith replacement) =/parenleftbigτ+K−1
K/parenrightbig
:=Pdifferent
points in the grid WK.
In summary, the integration based approximation of the objective proceeds as follows:
Algorithm 5 Integration Based Approximation of the Optimization Objective
1:Input: # mechanisms K∈N, # iterations T= 10000∈N, noise function γ:{0,1,...,K}→ [0,1],
τ= 50: # samples between [0.5,1)to form the setW
2:θ←0.5/τdistance between samples
3:w←θK·K!
4:P←/parenleftbigτ+K−1
K/parenrightbig
5:fort= 1,2,...,Tdo
6:Sample probabilities (s1,s2,...,sK)∼WK
7:/hatwideL∼PoissonBinomial (s1,s2,...,sK)
8: ˆαl←Pr[/hatwideL=l],∀l∈{0,1,...,K}
9:gt←−1
2/summationtextK
l=K+1
2w·(ˆαl−ˆαK−l)·γ(l)
10:end for
11:ReturnP
N/summationtextT
t=1gt
C.3 Reducing # Constraints from ∞to a Polynomial Set
Lemma C.1 (Restatement of Lemma 5.1) .Consider using DaRRM(Algorithm 1) to solve Problem 1.1 and
letfbe the privacy cost objective as defined in Lemma 3.4. Given an arbitrary noise function γ, let the worst
case probabilities be
(p∗
1,...,p∗
K,p′∗
1,...,p′∗
K) = arg max
{(pi,p′
i)}K
i=1f(p1,...,pK,p′
1,...,p′
K;γ)
Then, each pair (p∗
i,p′∗
i),∀i∈[K]satisfies
(p∗
i,p′∗
i)∈{(0,0),(1,1),(0,∆),(∆,0),(1−∆,1),
(1,1−∆),(eϵ+ ∆
eϵ+ 1,1−∆
eϵ+ 1),(1−∆
eϵ+ 1,eϵ+ ∆
eϵ+ 1)}
Furthermore, when δ > 0, there exists a finite vector set Pof sizeO(K7)such that if β=
max{(pi,p′
i)}K
i=1∈Pf(p1,...,pK,p′
1,...,p′
K;γ), thenf(p∗
1,...,p∗
K,p′∗
1,...,p′∗
K;γ)≤β. Whenδ= 0, the size of
Pcan be reduced to O(K3).
47Published in Transactions on Machine Learning Research (November/2024)
Figure 5: An illustration of the feasible region Fi.
Proof.Part I: Reducing # privacy constraints from ∞to exponentially many.
Consider (pi,p′
i)for an arbitrary i∈[K]and fixing (pj,p′
j),∀j̸=i. Given any noise function γ, recall the
privacy cost objective f(p1,...,pK,p′
1,...,p′
K;γ)(see Lemma 3.4), is
f(p1,...,pK,p′
1,...,p′
K;γ) =K−1
2/summationdisplay
l=0(emϵα′
l−αl)·γ(l) +K/summationdisplay
l=K+1
2(αl−emϵα′
l)·γ(l)
and the privacy constraints are of the form
f(p1,...,pK,p′
1,...,p′
K;γ)≤emϵ−1 + 2δ
where recall that αl=Pr[L(D) =l]is a function of {pi}K
i=1andα′
l=Pr[L(D′) =l]is a func-
tion of{p′
i}K
i=1,∀l∈{0,1,...,K}andL(D),L(D′)are the sum of observed outcomes on neighboring
datasetsDandD′. By Lemma 3.4, γneeds to make the above privacy constraint hold for all possible
{(pi,p′
i)}K
i=1to make DaRRMγ(mϵ,δ )-differentially private. This is equivalent to saying, γneeds to ensure
max{(pi,p′
i}K
i=1f(p1,...,pK,p′
1,...,p′
K;γ)≤emϵ−1 + 2δ.
Notice that the sum of observed outcomes follows a Poisson Binomial distribution, i.e., L(D)∼
PoissonBinomial (p1,...,pK)andL(D′)∼PoissonBinomial (p′
1,...,p′
K). Hence, by the pmf of the Pois-
son Binomial distribution6, the privacy cost objective fis linear in each piandp′
i, fixing all (pj,p′
j),∀j̸=i.
Since each mechanism Miis(ϵ,∆)-differentially private, by definition, (pi,p′
i)satisfies all of the following:
pi≤eϵp′
i+ ∆, p′
i≤eϵp+ ∆
1−pi≤eϵ(1−p′
i) + ∆,1−p′
i≤eϵ(1−pi) + ∆
That is, (pi,p′
i)lies in a feasible region Fi(see Figure 5). Note the constraints on (pi,p′
i), that
is, the boundaries of Fi, are linear in piandp′
i. And so the optimization problem (p∗
i,p′∗
i) =
arg max(pi,p′
i)f(p1,...,pK,p′
1,...,p′
K;γ), which finds the worst case probabilities in (pi,p′
i), is a Linear
Programming (LP) problem in (pi,p′
i)fori∈[K]. This implies (p∗
i,p′∗
i)has to be on one of the eight corners
ofFi— that is (p∗
i,p′∗
i)∈{(0,0),(1,1),(0,∆),(∆,0),(1−∆,1),(1,1−∆),(eϵ+∆
eϵ+1,1−∆
eϵ+1),(1−∆
eϵ+1,eϵ+∆
eϵ+1)}:=C.
Since all (pi,p′
i)and(pj,p′
j), fori̸=j, are independent, we can search for the worst case probabilities by
searching for (p∗
i,p′∗
i)∈C, instead of searching for (pi,p′
i)∈Fi,∀i∈[K]. Therefore, the infinitely many
privacy constraints are now reduced to only 8Kto optimize for the best γfunction that maximizes the utility
ofDaRRMγ, while ensuring the output is mϵ-differentially private.
Part II: Reducing # privacy constraints from exponentially many to a polynomial set.
To further reduce the number of privacy constraints in optimization, observe that the Poisson Binomial
distribution is invariant under the permutation of its parameters. That is, PoissonBinomial (p1,...,pK)dist.∼
6See, e.g. https://en.wikipedia.org/wiki/Poisson_binomial_distribution , for the pmf of Poisson Binomial distribution.
48Published in Transactions on Machine Learning Research (November/2024)
PoissonBinomial (π(p1,...,pK)), for some permutation πanddist.∼means “follows the same distribution”.
Similarly, PoissonBinomial (p′
1,...,p′
K)dist.∼PoissonBinomial (π(p′
1,...,p′
K)).
The above observation implies if we have one privacy constraint f(p1=v1,...,pK=vK,p′
1=v′
1,...,p′
K=
v′
K;γ)≤emϵ−1 + 2δ, for some{(vi,v′
i)}K
i=1∈CK, then any privacy constraint f(p1=s1,...,pK=sK,p′
1=
s′
1,...,p′
K=s′
K;γ)≤emϵ−1 + 2δ, where (s1,...,sK) =π1(v1,...,vK),(s′
1,...,s′
K) =π(v′
1,...,v′
K), for
permutations π1andπ2, is redundant.
Therefore, there is a vector set P, where each probability vector (p1,...,pK,p′
1,...,p′
K)inPis constructed by
setting (p1,p′
1),(p2,p′
2),..., (pK,p′
K) = (v1,v2,...,vK), wherevi∈C,∀i∈[K], such that vectors constructed
by(p1,p′
1),(p2,p′
2),..., (pK,p′
K) =π(v1,v2,...,vK)is not inP. Note|P|=(8 chooses K with replacement)
=/parenleftbigK+8−1
K/parenrightbig
=O(K7). If we can restrict our search for the worst case probabilities to this set P— that
is, solving for β:=max{(pi,p′
i)}K
i=1∈Pf(p1,...,pK,p′
1,...,p′
K;γ), thenf(p∗
1,...,p∗
K,p′∗
1,...,p′∗
K;γ)≤β. This
implies we only need O(K7)privacy constraints to optimize for the best noise function γinDaRRM, while
making sure DaRRMγismϵ-differentially private.
Note if ∆ = 0, i.e., the mechanism Mi’s are pure differentially private, the feasible region Fiin which (pi,p′
i)
lies has only 4 corners instead of 8. This implies (p∗
i,p′∗
i)∈C={(0,0),(1,1),(eϵ
eϵ+1,1
eϵ+1),(1
eϵ+1,eϵ
eϵ+1)}.
Hence, in this case, |P|=(4 chooseKwith replacement) =/parenleftbigK+4−1
K/parenrightbig
=O(K3), which implies we only need
O(K3)privacy constraints to optimize for the best noise function γinDaRRM.
49Published in Transactions on Machine Learning Research (November/2024)
D Full Experiment Results
D.1 Optimized γin Simulations
D.1.1 Comparison Using General Composition
The general composition (Theorem 2.3) indicates less total privacy loss than simple composition (Theorem 2.2)
when the number of folds, m, is large, or when the failure probability δis large. To enable meaningful
comparison against general composition, we consider a larger Kand a larger failure probability δ.
ConsiderK= 35,ϵ= 0.1,∆ = 10−5. By general composition, if one outputs the majority of Msubsampled
mechanisms for some M <K, the majority output is (ϵopt,δopt)-differentially private, where
ϵopt= min/braceleftig
Mϵ,(eϵ−1)ϵM
eϵ+ 1+ϵ/radicaligg
2Mlog(e+√
Mϵ2
δ′),(eϵ−1)ϵM
eϵ+ 1+ϵ/radicalbigg
2Mlog(1
δ′)/bracerightig
, δopt= 1−(1−δ)M(1−δ′)
for someδ′≥0. We set this as the privacy guarantee of all majority ensembling algorithms. That is, if we
want the majority output to be (mϵ,δ )-differentially private, we set
m=ϵopt
ϵ= min/braceleftig
M,(eϵ−1)M
eϵ+ 1+/radicaligg
2Mlog(e+√
Mϵ2
δ′),(eϵ−1)M
eϵ+ 1+/radicalbigg
2Mlog(1
δ′)/bracerightig
andδ= 1−(1−δ)M(1−δ′)accordingly. The parameters τandλto compute pconstinRR(see Section A.1)
are set to be
τ= min/braceleftig
K,(eϵ−1)K
eϵ+ 1+/radicaligg
2Klog(e+√
Kϵ2
δ′),(eϵ−1)K
eϵ+ 1+/radicalbigg
2Klog(1
δ′)/bracerightig
andλ= 1−(1−δ)K(1−δ′).
In the experiments, we consider M={10,13,15,20}andδ′= 0.1; andγoptis computed using a uniform prior
T.
All values of the parameters of the private ensembling algorithms we use in the experiment are listed in the
table:
# Subsampled mechanisms M10 13 15 20
Privacy allowance m6.4521 7.5742 8.2708 9.8823
Parameter of constant γτ14.0328 14.0328 14.0328 14.0328
Parameter of constant γλ0.1003 0.1003 0.1003 0.1003
Overall privacy loss mϵ0.6452 0.7574 0.8271 0.9882
Overall failure probability δ0.1001 0.1001 0.1001 0.1002
Table 3: All parameter values. Note that all the private ensembling algorithms we compare in the experiment
is required to be (mϵ,δ )-differentially private. Here, K= 35,ϵ= 0.1,∆ = 10−5andδ′= 0.1.
50Published in Transactions on Machine Learning Research (November/2024)
00.51M = 10 M = 13
0 17.5 3500.51M = 15
0 17.5 35M = 20
Support l{0,1,...,K}
 values
Shape of  functions
0.000.050.10M = 10
0.000.05M = 13
0.000.05M = 15
0.000.020.04M = 20
 functions
Error(DaRRM)
Figure 6: Plots of the shape and E(DaRRMγ)of different γfunctions: the optimized γSub, and the baselines
γSub(corresponding to subsampling) and γconst(corresponding to RR). Here,K= 35,M∈{10,13,15,20},
∆ = 10−5,ϵ= 0.1,δ′= 0.1.
D.1.2 Comparison in Pure Differential Privacy Settings
Consider the pure differential privacy setting, where ∆ =δ= 0. Note in this setting, it is known that simple
composition is tight.
To compute an optimized γoptinDaRRM, since we have shown the number of constraints is O(K3)if
∆ =δ= 0(see Lemma 5.1), we can set Kto be larger. Here, we present results for K∈{11,101}and
ϵ= 0.1.
Again, we compare the shape of different γand the corresponding E(DaRRMγ)under those γfunctions, fixing
the total privacy loss to be mϵ.γoptis computed using a uniform prior T.
Since the subsampling mechanism from Section 4 with privacy amplification applies to this setting, we
compare four different γnoise functions here:
1.γopt(Ours): optimized γfunction using our optimization framework
2.γSub(Baseline): the γfunction that corresponds to outputting the majority of moutKsubsampled
mechanisms
3.γDSub(Baseline): the γfunction that corresponds to outputting 2m−1subsampled mechanisms
from Theorem 4.1, aka., Double Subsampling (DSub)
4.γconst(Baseline): the constant γfunction that corresponds to the classical Randomized Response
(RR) algorithm
Setting 1. K= 11,m∈{1,3,5,7,9,11}.
51Published in Transactions on Machine Learning Research (November/2024)
00.51m = 1 m = 3
00.51m = 5 m = 7
0 5.5 1100.51m = 9
0 5.5 11m = 11
Support l{0,1,...,K}
 values
Shape of  functions
0.00.10.2m = 1
0.00.1m = 3
0.000.050.10m = 5
0.000.05m = 7
0.000.02m = 9
0.05
0.000.05m = 11
 functions
Error(DaRRM)
Figure 7: Plots of shape and E(DaRRMγ)of different γfunctions: the optimized γOpt, the baselines γSuband
γDSub(Theorem 4.1), and the constant γconst(corresponding to RR). Here,K= 11,m∈{1,3,5,7,9,11},
ϵ= 0.1andδ= ∆ = 0. Note when m∈{7,9}, the cyan line ( γDSub) and the red line ( γopt) overlap. When
m= 11, all lines overlap. Observe that when m≥K+1
2, that is,m∈{7,9,11}in this case, the above plots
suggest both γoptandγDSubachieve the minimum error at 0. This is consistent with our theory.
Setting 2. K= 101,m∈{10,20,30,40,60,80}.
00.51m = 10 m = 20
00.51m = 30 m = 40
0 50.5 10100.51m = 60
0 50.5 101m = 80
Support l{0,1,...,K}
 values
Shape of  functions
0.000.050.10m = 10
0.000.050.10m = 20
0.000.050.10m = 30
0.000.05m = 40
0.000.020.04m = 60
0.000.010.02m = 80
 functions
Error(DaRRM)
Figure 8: Plots of shape and E(DaRRMγ)of different γfunctions: the optimized γOpt, the baselines
γSubandγDSub(Theorem 4.1), and the constant γconst(corresponding to RR). Here,K= 101,m∈
{10,20,30,40,60,80},ϵ= 0.1andδ= ∆ = 0.
52Published in Transactions on Machine Learning Research (November/2024)
D.1.3 Comparison Using Different Prior Distributions
When optimizing γthat maximizes the utility in DaRRM, recall that the objective takes an expectation
overpi’s forpi∼T, whereTis some distribution and pi=Pr[Mi(D) = 1]. The previous experiments
assume we do not have access to any prior knowledge about pi’s and henceTis the uniform distribution, i.e.,
Uniform ([0,1]). However, when one has knowledge about the mechanisms, one can set a proper prior Tto
further maximize the utility of DaRRM.
In this section, let TUdenoteUniform ([0,1])and we present results considering a different prior distribution,
which we callTP, as follows. Suppose our prior belief is that each mechanism Mihas a clear tendency towards
voting 0 or 1, i.e., piis far from 0.5. Let TPbe Uniform( [0,0.3]∪[0.7,1]).
To optimize γunderTP, we change the approximate optimization objective in Eq. 202, which optimizes γ
underTU, to be the following,
−1
2K/summationdisplay
l=K+1
2/integraldisplay1
0.7/integraldisplay1
0.7···/integraldisplay1
0.7(αl−αK−l)dp1dp2...dpK·γ(l) (204)
Setting.K= 11,m∈{3,5},ϵ= 0.1,δ= ∆ = 0.
We compare the shape and E(DaRRMγ)of different γfunctions:
1.γopt−Udenote the γfunction optimized under pi∼TU
2.γopt−Pdenote the γfunction optimized under pi∼TP
3.γSub, corresponding to the subsampling baseline
4.γconst, corresponding to the RRbaseline
Note when we compute the error, we take the expectation w.r.t. the actual pidistributions, regardless of the
prior used to optimize γ. In the experiments, we consider three different actual pidistributions:"
1. “Actual: Uniform ([0,1])”:pi∼TU,∀i∈[K]
2. “Actual: pi= 0.5”:pi= 0.5,∀i∈[K]
This setting implies the mechanisms do not have a clear majority
3. “Actual: Uniform ([0,0.1])”:pi∼Uniform ([0,0.1]),∀i∈[K]
This setting implies the mechanisms have a clear majority (i.e., 0)
Since our prior TPis closer to Uniform ([0,0.1])(i.e., there is a clear majority), we would expect
E(DaRRMγopt−P)to be the lowest when pi∼Uniform [0,0.1], but to be higher than E(DaRRMγopt−U)when
pi∼Uniform ([0,1])orpi= 0.5. The results are presented in Figure 9.
D.2 Private Semi-Supervised Knowledge Transfer
D.2.1 More Details about the Baseline GNMax Papernot et al. (2018)
The GNMaxaggregation mechanism for majority ensembling of non-private teachers proceeds as follows
(Section 4.1 of Papernot et al. (2018)): on input x,
Mσ(x) = arg max
i{ni(x) +N(0,σ2)} (205)
whereni(x)is # teachers who vote for class i.
How to set σinGNMax?
53Published in Transactions on Machine Learning Research (November/2024)
0 5 10
Support l{0,1,...,K}
0.250.500.751.00 values
Shape of  functions
 functions
0.000.050.100.15ErrorActual: pi Uniform([0, 1])
 functions
0.0000.0250.0500.075ErrorActual: pi=0.5
 functions
0.00.10.20.3ErrorActual: pi Uniform([0, 0.1])
K=11, m=3, =0.1
0 5 10
Support l{0,1,...,K}
0.250.500.751.00 values
Shape of  functions
 functions
0.000.050.10ErrorActual: pi Uniform([0, 1])
 functions
0.000.020.040.06ErrorActual: pi=0.5
 functions
0.00.10.2ErrorActual: pi Uniform([0, 0.1])
K=11, m=5, =0.1
Figure 9: Comparison of the shape and E(DaRRMγ)of differentγfunctions: 1) γoptimized under prior TU, 2)
γoptimized under prior TP, 3)γSub(corresponding to the subsampling baseline) and 4) γconst(corresponding
to the RRbaseline). Here, K= 11,m∈{3,5},ϵ= 0.1. Observe that if the prior TPused in optimizing γis
closer to the actual distribution of pi’s, there is additional utility gain (i.e., decreased error); otherwise, we
slightly suffer a utility loss (i.e., increased error), compared to optimize γunder theTUprior. Furthermore,
regardless of the choice of the prior distribution Tin optimizing γ,DaRRMγwith an optimized γachieves a
lower error compared to the the baselines.
54Published in Transactions on Machine Learning Research (November/2024)
Section 4.1 of Papernot et al. (2018) states the GNMax mechanism is (λ,λ/σ2)-Renyi differentially private
(RDP), for all λ≥1. RDP bounds can be converted to DP bounds as follows:
Theorem D.1 (RDP to DP (Theorem 5 of Papernot et al. (2018))) .If a mechanism Mguarantees (λ,ϵ)-RDP,
thenMguarantees (ϵ+log 1/δ
λ−1,δ)-differential privacy for δ∈(0,1).
Therefore, GNMaxwith parameter σ2guarantees (λ
σ2+log 1/δ
λ−1,δ)-differential privacy, ∀λ≥1. Givenm,ϵ, ∆,
we want to choose λandσ2here so that the output of GNMaxis(mϵ,m ∆)-differentially private. Here,
δ=m∆.
We first obtain a valid range of λ. Sincemϵ≥0,λ
σ2+log 1/δ
λ−1≥0and soλ≥log 1/δ
mϵ+ 1 :=λmin. And
σ2=λ
mϵ−log 1/δ
λ−1. Since the smaller σ2is, the higher the utility, we perform a grid search over λ∈[λmin,500],
with discretized λvalues of equal distance 0.5, to find the minimum σ2
min. For the (mϵ,m ∆)values used in
the experiments, we observe σ2decreases first and then increases as λincreases, as shown in Figure 10. The
λandσminvalues in the RDP bound of Gaussian noise to compute the privacy loss of GNMax’s output we
use in the experiments are presented in Table 4.
100 200 300 400 500
4006008001000120014001600180020002
Dataset: MNIST
100 200 300 400 500
6008001000120014001600180020002
Dataset: Fashion-MNIST
Figure 10: Plots of λvs.σ2in the Gaussian RDP privacy bound. The goal is to choose a λvalue that
minimizesσ2. It is not hard to see the value of σ2decreases at first and then increases as λincreases.
Privacy Loss Per Query
(mϵ,m ∆) λσmin
MNIST (0.2676,0.0003) 34.3121.46
Fashion-MNIST (0.2556,0.0003) 35.7422.46
Table 4: Parameters of the RDP bound of Gaussian noise to compute the privacy loss of GNMax’s output.
A Note on the Data-dependent Privacy Loss Bound
Papernot et al. (2018) gives a potentially tighter data-dependent bound on the privacy loss using GNMax
to output the majority of non-private teacherss votes. We give a clean pseudo-code on computing the
data-dependent privacy loss bound in Algorithm 6, based on the lemmas and theorems in Papernot et al.
(2018). Given privacy parameters σ,λand the teacher votes per class {ni}C
i=1forCclasses, the data-dependent
bound can be empirically evaluated and compared against the Gaussian privacy loss bound. The smaller
one is the final privacy loss. We empirically find that the condition of the data-dependent bound (line 8
in Algorithm 6) is not satisfied when Kand the number of classes Care small, e.g., K= 11,C= 2as in
our case, even if all teachers agree on the same output. And so in the experiments, we can only apply the
Gaussian privacy loss bound (line 14).
D.2.2 Additional Results for Private Semi-Supervised Knowledge Transfer
m= 1.
55Published in Transactions on Machine Learning Research (November/2024)
Algorithm 6 Compute Tighter Privacy Loss
1:Input: Std. of Gaussian noise σ, Privacy parameter λ, # teachers K, # classes C, # votes per class
{ni}C
i=1
2:B←{}bound candidates
3:fori= 1,2,...,Kdo
4:q(i)←1
2/summationtext
i̸=i∗erfc(ni∗−ni
2σ)
5:µ(i)
2←σ·/radicalbig
log 1/q(i),µ(i)
1←µ(i)
2+ 1
6:ϵ(i)
1←µ(i)
1
σ2,ϵ(i)
2←µ(i)
2
σ2
7:q(i)
ub←exp((µ(i)
2−1)ϵ(i)
2)/(µ(i)
1
µ(i)
1−1·µ(i)
2
µ(i)
2−1)µ(i)
2
8:ifq(i)<1andµ(i)
1≥λandµ2>1andq(i)≤q(i)
ubthen
9:A(i)←(1−q(i))/(1−q(i)·exp(ϵ(i)
2)µ(i)
2−1
µ(i)
2)
10:B(i)←exp(ϵ(i)
1)/(q(i))1
µ(i)
1−1
11:DataDependentBound ←1
λ−1·/parenleftig
(1−q(i))·(A(i))λ−1+q(i)·(B(i))λ−1/parenrightig
12:B←B∪ DataDependentBound
13:else
14:GaussianBound←λ
σ2
15:B←B∪ GaussianBound
16:end if
17:end for
18:Return minB
Dataset # QueriesPrivacy loss
per query
(ϵquery,δquery )Total privacy loss
overQqueries
(ϵtotal,δtotal)
MNISTQ= 20
(0.0892,0.0001)(1.704,0.002)
Q= 50 (2.837,0.005)
Q= 100 (4.202,0.010)
Fashion
MNISTQ= 20
(0.0852,0.0001)(1.620,0.002)
Q= 50 (2.695,0.005)
Q= 100 (3.988,0.010)
Table 5: The privacy loss per query to the teachers and the total privacy loss over Qqueries. Note the total
privacy loss is computed by general composition, where we set δ′= 0.0001.
Dataset MNIST
# QueriesGNMax
(Baseline)DaRRMγSub
(Baseline)DaRRMγopt
(Ours)
Q= 20 0.54 (0.11) 0.68 (0.07) 0.74 (0.08)
Q= 50 0.51 (0.07) 0.67 (0.05) 0.66 (0.05)
Q= 100 0.57 (0.03) 0.71 (0.03) 0.69 (0.04)Dataset Fashion-MNIST
# QueriesGNMax
(Baseline)DaRRMγSub
(Baseline)DaRRMγopt
(Ours)
Q= 20 0.56 (0.10) 0.92 (0.05) 0.89 (0.06)
Q= 50 0.52 (0.05) 0.89 (0.04) 0.92 (0.03)
Q= 100 0.56 (0.04) 0.89 (0.04) 0.91 (0.04)
Table 6: Accuracy of the predicted labels of Qquery samples on datasets MNIST(on the left) and
Fashion-MNIST (on the right). We report the mean and one std. in parentheses over 10 random draws of the
query samples from the test dataset. Note each prediction on the query sample is (ϵtotal,δtotal)-differentially
private. Note in this case where m= 1, by Lemma 3.2, subsampling achieves the optimal error/utility. Hence,
there is not much difference in terms of accuracy between DaRRMγSubandDaRRMγoptas expected.
m= 5.
56Published in Transactions on Machine Learning Research (November/2024)
Dataset # QueriesPrivacy loss
per query
(ϵquery,δquery )Total privacy loss
overQqueries
(ϵtotal,δtotal)
MNISTQ= 20
(0.4460,0.0005)(8.920,0.010)
Q= 50 (18.428,0.025)
Q= 100 (28.926,0.049)
Fashion
MNISTQ= 20
(0.4260,0.0005)(8.520,0.010)
Q= 50 (17.398,0.025)
Q= 100 (27.223,0.049)
Table 7: The privacy loss per query to the teachers and the total privacy loss over Qqueries. Note the total
privacy loss is computed by general composition, where we set δ′= 0.0001.
Dataset MNIST
# QueriesGNMax
(Baseline)DaRRMγSub
(Baseline)DaRRMγopt
(Ours)
Q= 20 0.73 (0.11) 0.76 (0.09) 0.84 (0.07)
Q= 50 0.75 (0.07) 0.82 (0.04) 0.83 (0.04)
Q= 100 0.72 (0.04) 0.79 (0.05) 0.83 (0.03)Dataset Fashion-MNIST
# QueriesGNMax
(Baseline)DaRRMγSub
(Baseline)DaRRMγopt
(Ours)
Q= 20 0.72 (0.10) 0.96 (0.04) 0.97 (0.04)
Q= 50 0.72 (0.08) 0.96 (0.02) 0.97 (0.02)
Q= 100 0.72 (0.06) 0.97 (0.01) 0.97 (0.01)
Table 8: Accuracy of the predicted labels of Qquery samples on datasets MNIST(on the left) and
Fashion-MNIST (on the right). We report the mean and one std. in parentheses over 10 random draws of the
query samples from the test dataset. Note each prediction on the query sample is (ϵtotal,δtotal)-differentially
private. With the same per query privacy loss (and hence the same total privacy loss over Qsamples),
DaRRMγoptachieves the highest accuracy compared to the other two baselines.
m= 7.
Dataset # QueriesPrivacy loss
per query
(ϵquery,δquery )Total privacy loss
overQqueries
(ϵtotal,δtotal)
MNISTQ= 20
(0.6244,0.0007)(12.488,0.014)
Q= 50 (28.392,0.035)
Q= 100 (45.683,0.068)
Fashion
MNISTQ= 20
(0.5964,0.0007)(11.928,0.014)
Q= 50 (26.738,0.035)
Q= 100 (42.873,0.068)
Table 9: The privacy loss per query to the teachers and the total privacy loss over Qqueries. Note the total
privacy loss is computed by general composition, where we set δ′= 0.0001.
Dataset MNIST
# QueriesGNMax
(Baseline)DaRRMγSub
(Baseline)DaRRMγopt
(Ours)
Q= 20 0.79 (0.07) 0.80 (0.09) 0.85 (0.08)
Q= 50 0.80 (0.05) 0.82 (0.05) 0.85 (0.04)
Q= 100 0.80 (0.04) 0.80 (0.04) 0.83 (0.03)Dataset Fashion-MNIST
# QueriesGNMax
(Baseline)DaRRMγSub
(Baseline)DaRRMγopt
(Ours)
Q= 20 0.79 (0.07) 0.95 (0.04) 0.96 (0.04)
Q= 50 0.79 (0.05) 0.96 (0.03) 0.97 (0.03)
Q= 100 0.79 (0.03) 0.96 (0.02) 0.96 (0.02)
Table 10: Accuracy of the predicted labels of Qquery samples on datasets MNIST(on the left) and
Fashion-MNIST (on the right). We report the mean and one std. in parentheses over 10 random draws of the
query samples from the test dataset. Note each prediction on the query sample is (ϵtotal,δtotal)-differentially
private. With the same per query privacy loss (and hence the same total privacy loss over Qsamples),
DaRRMγoptachieves the highest accuracy compared to the other two baselines.
57