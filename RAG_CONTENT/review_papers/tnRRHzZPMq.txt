Published in Transactions on Machine Learning Research (12/2022)
Communication-Efficient Distributionally Robust Decentral-
ized Learning
Matteo Zecchin matteo.zecchin@eurecom.fr
Communication Systems Department
EURECOM, Sophia Antipolis, France
Marios Kountouris marios.kountouris@eurecom.fr
Communication Systems Department
EURECOM, Sophia Antipolis, France
David Gesbert david.gesbert@eurecom.fr
Communication Systems Department
EURECOM, Sophia Antipolis, France
Reviewed on OpenReview: https: // openreview. net/ forum? id= tnRRHzZPMq
Abstract
Decentralized learning algorithms empower interconnected devices to share data and computa-
tional resources to collaboratively train a machine learning model without the aid of a central
coordinator. In the case of heterogeneous data distributions at the network nodes, collabora-
tion can yield predictors with unsatisfactory performance for a subset of the devices. For this
reason, in this work, we consider the formulation of a distributionally robust decentralized
learning task and we propose a decentralized single loop gradient descent/ascent algorithm
(AD-GDA) to directly solve the underlying minimax optimization problem. We render our
algorithm communication-efficient by employing a compressed consensus scheme and we
provide convergence guarantees for smooth convex and non-convex loss functions. Finally, we
corroborate the theoretical findings with empirical results that highlight AD-GDA’s ability
to provide unbiased predictors and to greatly improve communication efficiency compared to
existing distributionally robust algorithms.
1 Introduction
Decentralized learning algorithms have gained an increasing level of attention, mainly due to their ability
to harness, in a fault-tolerant and privacy-preserving manner, the large computational power and data
availability at the network edge (Chen & Sayed, 2012; Yan et al., 2012). In this framework, a set of
interconnected nodes (smartphones, IoT devices, health centers, research labs, etc.) collaboratively train a
machine learning model alternating between local model updates, based on in situ data, and peer-to-peer type
of communication to exchange model-related information. Compared to federated learning in which a swarm
of devices communicates with a central parameter server at each communication round, fully decentralized
learning has the benefits of removing the single point of failure and of alleviating the communication bottleneck
inherent to the star topology.
The heterogeneity of distributedly generated data entails a major challenge, represented by the notions of
fairness (Dwork et al., 2012) and robustness (Quiñonero-Candela et al., 2009). In the distributed setup, the
customary global loss function is the weighted sum of the local empirical losses, with each term weighted by
the fraction of samples that the associated node stores. However, in the case of data heterogeneity across
participating parties, a model minimizing such definition of risk can lead to unsatisfactory and unfair1
1In the machine learning community, the notion of fairness has many facets. In this work, we will use the term “fair” in
accordance with the notion of good-intent fairness as introduced in (Mohri et al., 2019).
1Published in Transactions on Machine Learning Research (12/2022)
inference capabilities for certain subpopulations. Consider, for example, a consortium of hospitals spread
across the world sharing medical data to devise a new drug and in which a small fraction of hospitals have
medical records influenced by geographical confounders, such as local diet, meteorological conditions, etc.
In this setting, a model obtained by myopically minimizing the standard notion of risk defined over the
aggregated data can be severely biased towards some populations at the expense of others. This can lead to
a potentially dangerous or unfair medical treatment as shown in Figure 2.
(a) Network Topology
0 1000 2000 3000 4000 5000
Iteration0.40.50.60.70.8Accuracy AD-GDA Microscope 1
CHOCO-SGD Microscope 1
AD-GDA Average
CHOCO-SGD Average
AD-GDA 4 Microscope 2
CHOCO-SGD Microscope 2 (b) Worst-node accuracy
Figure 2: Comparison between standard and distributionally robust decentralized learning procedures. We
consider a network of 10 nodes that collaboratively train a mouse cell image classifier based on the Cells Out
Of Sample 7-Class (COOS7) data set (Lu et al., 2019). Two nodes store samples obtained using a different
microscope from the rest of the network devices (left figure). On the right, we report the validation accuracy
of the standard decentralized learning (CHOCO-SGD) and the proposed distributionally robust decentralized
learning (AD-GDA). We consider three different validation sets: one made of samples from one microscope,
another with samples from the other, and one made of a 50/50 mixture of the two. CHOCO-SGD (dashed
lines) yields a final model with a 24% accuracy gap between the two types of instruments. On the contrary,
AD-GDA (solid lines) reduces the accuracy gap to less than 2% and improves fairness among collaborating
parties. The average performance is not affected by the distributionally robust procedure.
To tackle this issue, distributionally robust learning aims at maximizing the worst-case performance over a set
of distributions, termed an uncertainty set, which possibly contains the testing distribution of interest. Typical
choices of the uncertainty sets are balls centered around the training distribution (Esfahani & Kuhn, 2018)
or, whenever the training samples come from a mixture of distributions, the set of potential subpopulations
resulting in such mixture (Duchi et al., 2019; Duchi & Namkoong, 2018). Robust distributed learning with
heterogeneous data in which different distributions exist at the various devices falls in the latter category, as
the natural ambiguity set is the one represented by the convex combination of the local distributions. In that
case, minimizing the worst-case risk is equivalent to trying to ensure a minimum level of performance for each
participating device. Specifically for the federated case, Mohri et al. (Mohri et al., 2019) introduced agnostic
federated learning (AFL) as a means to ensure fairness and proposed a gradient-based algorithm to solve the
distributionally robust optimization problem. In (Deng et al., 2021) a communication-efficient version of AFL,
which avoids frequent retransmission of the dual variables, was proposed. More recently, distributionally
robust learning has also been studied in the fully decentralized case. In this setting, the underlying minimax
optimization problem becomes challenging and distributionally robust learning has been limited to a special
formulation — Kullback-Leibler (KL) regularization — that allows simplifying the problem (Issaid et al.,
2022).
In virtue of the advantages of the fully decentralized setup and advocating the necessity for robust and fair
predictors, in this work we propose AD-GDA, a novel distributionally robust algorithm for the decentralized
setting. In contrast to previous works, our solution directlytackles the minimax optimization problem in
afully decentralized fashion. Our algorithm is general and encompasses previous solutions as particular
choices of the regularization function. Furthermore, we show that it is possible to perform distributionally
robust optimization in a communication-efficient manner by employing a compressed gossip scheme without
hampering the rate of convergence of the algorithm.
2Published in Transactions on Machine Learning Research (12/2022)
Table 1: Comparison between the proposed and existing distributionally robust algorithms.
Features Convergence rate
Topologies Compression Regularizer Convex Non-convex
AD-GDA (ours) Connected ✓ Strongly-concave O(T−1/2)O(T−1/2)
DRFA (Deng et al., 2021) Star ✗ Strongly-concave O(T−3/8)O(T−1/8)
DR-DSGD (Issaid et al., 2022) Connected ✗ Kullback-Leibler ✗ O(T−1/2)
Contributions: The main contributions of the work are the following:
•We propose AD-GDA, an optimization algorithm to perform distributionally robust learning in a fully
decentralized fashion. As detailed in Table 1, previous works have either been limited to the federated
case or particular choices of the regularizer, AD-GDA directlytackles the distributionally robust
minimax optimization problem in a fully decentralized fashion. Despite the additional complexity
stemming from solving the decentralized minimax optimization problem, our solution is computation
and communication efficient. AD-GDA alternates between local single-loop stochastic gradient
descent/ascent model updates and compressed consensus steps to cope with local connectivity in a
communication-efficient manner.
•We establish convergence guarantees for the proposed algorithm both in the case of smooth convex
and smooth non-convex local loss functions that match or outperform the existing ones (see Table
1). In the former case, the algorithm returns an ϵ-optimal solution after O(1/ϵ2)iterations. In the
latter, the output is guaranteed to be an ϵ-stationary solution after O(1/ϵ2)iterations whenever the
stochastic gradient variance is also bounded by ϵ, otherwise, we can obtain the same guarantee by
increasing the number of calls to the stochastic gradient oracle.
•We empirically demonstrate AD-GDA capability in finding robust predictors under different com-
pression schemes, network topologies, and models. First, we compare the proposed algorithm with
compressed decentralized stochastic gradient descent (CHOCO-SGD) and highlight the merits of
the distributionally robust procedure. We then consider the existing distributionally robust learning
algorithms; namely, Distributionally Robust Federated Averaging (DRFA) (Deng et al., 2021) and
Distributionally Robust Decentralized Stochastic Gradient Descent (DR-DSGD) (Issaid et al., 2022).
We show that AD-GDA attains the same worst-case distribution accuracy transmitting a fraction of
the bits and it is up to 4 ×and 10×times more communication efficient compared to DRFA and
DR-DSGD, respectively.
2 Related Work
Communication-efficientdecentralizedlearning. Initiatedinthe80sbytheworkofTsitsiklis(Tsitsiklis,
1984; Tsitsiklis et al., 1986), the study of decentralized optimization algorithms was spurred by their
adaptability to various network topologies, reliability to link failures, privacy-preserving capabilities, and
potentially superior convergence properties compared to the centralized counterpart (Chen & Sayed, 2012;
Yan et al., 2012; Olfati-Saber et al., 2007; Ling et al., 2012; Lian et al., 2017). This growing interest and the
advent of large-scale machine learning brought forth an abundance of optimization algorithms both in the
deterministic and stochastic settings (Nedic & Ozdaglar, 2009; Wei & Ozdaglar, 2012; Duchi et al., 2011;
Shamir & Srebro, 2014; Rabbat, 2015). With the intent of extending its applicability, a concurrent effort has
been made to devise techniques able to reduce the delay due to inter-node communication. Notable results in
this direction are the introduction of message compression techniques, such as sparsification and quantization
(Stich et al., 2018; Aji & Heafield, 2017; Alistarh et al., 2018; 2017; Bernstein et al., 2018; Koloskova et al.,
2019b), and event-triggered communication to allow multiple local updates between communication rounds
(Stich, 2018; Yu et al., 2019)
3Published in Transactions on Machine Learning Research (12/2022)
Distributional robust learning. Tracing back to the work of Scarf (Scarf, 1957), distributional robustness
copes with the frequent mismatch between training and testing distributions by posing the training process
as a game between a learner and an adversary, which has the ability to choose the testing distribution within
an uncertainty set. Restraining the decisional power of the adversary is crucial to obtain meaningful and
tractable problems and a large body of the literature deals with uncertainty sets, represented by balls centered
around the training distribution and whose radii are determined by f-divergences (Namkoong & Duchi,
2016; Hu & Hong, 2013) or Wasserstein distance (Wozabal, 2012; Jiang & Guan, 2016; Esfahani & Kuhn,
2018). As such, distributional robustness is deeply linked to stochastic game literature. In this context, Lin
et al. (2020b) provides last-iterate guarantees in case of coercive games. Refined results are later obtained
in (Loizou et al., 2021), for expected coercive games and by relaxing the bounded gradient assumption.
Furthermore, distributional robustness is deeply linked with the notion of fairness as particular choices of
uncertainty sets allow guaranteeing uniform performance across the latent subpopulations in the data (Duchi
& Namkoong, 2018; Duchi et al., 2019). In the case of federated learning, robust optimization ideas have been
explored to ensure uniform performance across all participating devices (Mohri et al., 2019). Distributionally
robust learning has also been studied in the fully decentralized scenario in the case of Kullback-Leibler (KL)
regularizers for which exists an exact solution for the inner maximization problem (Issaid et al., 2022).
Decentralized minimax optimization. Saddle point optimization algorithms are of great interest given
their wide range of applications in different fields of machine learning, including generative adversarial
networks (Goodfellow et al., 2014), robust adversarial training (Sinha et al., 2017; Madry et al., 2017), and
multi-agent reinforcement learning (Pinto et al., 2017; Li et al., 2019). Their convergence properties have also
been studied in the decentralized scenario for the convex-concave setting (Koppel et al., 2015; Mateos-Núnez
& Cortés, 2015). More recently, the assumptions on the convexity and concavity of the objective function
have been relaxed. In Tsaknakis et al. (2020) an algorithm for nonconvex strongly-concave objective functions
has been proposed; however, the double-loop nature of the solution requires solving the inner maximization
problem with an increasing level of accuracy rendering it potentially slow. On the other hand, our algorithm
is based on a single loop optimization scheme - with dual and primal variables being updated at each iteration
in parallel - and, consequently, has a lower computational complexity. For the nonconvex-nonconcave case,
Liu et al. (2019b) provides a proximal point algorithm while a simpler gradient-based algorithm is provided
in Liu et al. (2019a) to train generative adversarial networks in a decentralized fashion. None of these works
take into consideration communication efficiency in their algorithms.
3 Preliminaries
Distributed system We consider a network of mdevices in which each node iis endowed with a local
objective function fi:Rd→Rgiven by Ez∼Piℓ(θ,z), withPidenoting the local distribution at node iand
θ∈Rdbeing the model parameter to be optimized. Whenever Piis replaced by an empirical measure ˆPi,ni,
the local objective function coincides with the empirical risk computed over nisamples. Network nodes are
assumed to be interconnected by a communication topology specified by a connected graph G:=(V,E)in
whichV={1,...,m}indexes the devices and (i,j)∈Eif and only if nodes iandjcan communicate. For each
nodei∈V, we define its set of neighbors by N(i) :={j: (i,j)∈E}and since we assume self-communication
we have (i,i)∈N(i)for alliinV. At each communication round, the network nodes exchange messages
with their neighbors and average the received messages according to a mixing matrix W∈Rm×m.
Assumption 3.1. The mixing matrix W∈Rm×mis symmetric and doubly-stochastic; we denote its
spectral gap — the difference between the moduli of the two largest eigenvalues — by ρ∈(0,1]and define
β=∥I−W∥2∈[0,2].
Compressed communication Being the communication phase the major bottleneck of decentralized
training, we assume that nodes transmit only compressed messages instead of sharing uncompressed model
updates. To this end, we define a, possibly randomized, compression operator Q:Rd→Rdthat satisfies the
following assumption.
Assumption 3.2. For anyx∈Rnand for some δ∈(0,1],
EQ/bracketleftbig
∥Q(x)−x∥2/bracketrightbig
≤(1−δ)∥x∥2. (1)
4Published in Transactions on Machine Learning Research (12/2022)
The above definition is quite general as it entails both biased and unbiased compression operators. For
instance, random quantization (Alistarh et al., 2017) falls into the former class and satisfies (1) with δ=1
τ.
For a given vector x∈Rdand quantization levels 2b, it yields a compressed message
xb=sign(x)∥x∥
2bτ/floorleftbigg
2b|x|
∥x∥+ξ/floorrightbigg
(2)
withτ= 1 + min/braceleftig
d/22b,√
d/2b/bracerightig
andξ∼U[0,1]⊗d. A notable representative of the biased category is the
top-Ksparsification (Stich et al., 2018), which for a given vector x∈Rdreturns the Klargest magnitude
components and satisfies ( 1) withδ=K
d. Operators of that type have been previously considered in the
context of decentralized learning and the effect of compressed communication in decentralized stochastic
optimization has been previously investigated (Koloskova et al., 2019a;b; Stich et al., 2018). The resulting
communication cost savings have been showcased in the context of decentralized training of deep neural
networks (Koloskova et al., 2019a). However, to the best of our knowledge, there are no applications of
compressed communication to distributional robust training in the decentralized setup.
Algorithm 1: Agnostic Decentralized GDA with Compressed Communication (AD-GDA)
Input : Number of nodes m, number of iterations T, learning rates ηθandηλ, mixing matrix W, initial values
θ0∈Rdandλ0∈∆m−1.
Output:θo=1
T/summationtextT−1
t=0¯θt,λo=1
T/summationtextT−1
t=0¯λt
initializeθ0
i=θ0,λ0
i=λ0ands0
i= 0fori= 1,...,m
fortin0,...T−1do
// In parallel at each node i
θt+1
2
i←θt
i−ηθ∇θgi(θt
i,λt
i,ξt
i) // Descent Step
λt+1
2
i←P Λ/parenleftbig
λt
i+ηλ∇λgi(θt
i,λt
i,ξt
i)/parenrightbig
// Projected Ascent Step
θt+1
i←θt+1
2
i+γ/parenleftbig
st
i−ˆθt
i/parenrightbig
// Gossip
qt
i←Q/parenleftbig
θt+1
i−ˆθt
i/parenrightbig
// Compression
send (qt
i,λt+1
2
i)toj∈N(i)and receive (qt
j,λt+1
2
j)fromj∈N(i) // Msgs exchange
ˆθt+1
i←qt
i+ˆθt
i // Public variables update
st+1
i←st
i+/summationtextm
j=1wi,jqj
λt+1
i←/summationtextm
j=1wi,jλt+1
2
j // Dual variable averaging
end
Distributionally robust network loss In order to obtain a final predictor with satisfactory performance
for all local distributions {Pi}m
i=1, the common objective is to learn global model which is distributionally
robust with respect to the ambiguity set P:=/braceleftbig/summationtextm
i=1λiPi:λi∈∆m−1/bracerightbig
where ∆m−1where denotes the
m−1probability simplex. As shown in Mohri et al. (2019), a network objective function that effectively
works as proxy for this scope is given by
min
θ∈Rdmax
λ∈∆m−1
g(θ,λ) :=1
mm/summationdisplay
i=1(λifi(θ) +αr(λ))/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
:=gi(θ,λ)
 (3)
in whichr: ∆m−1→Ris a strongly-concave regularizer and α∈R+. For instance, in the empirical risk
minimization framework in which each node iis endowed with a training set Di∼P⊗ni
iand the overall number
of training points is n=/summationtext
ini, a common choice of r(λ)isχ2(λ) :=/summationtext
i(λi−ni/n)2
ni/nor the Kullback-Leibler
divergence DKL(λ) :=/summationtext
iλilog (λin/ni)(Issaid et al., 2022). Restricting (3) to the latter regularizer, the
inner maximization problem can be solved exactly (Issaid et al., 2022; Mohajerin Esfahani & Kuhn, 2018).
In what follows, we refer to θandλas the primal and dual variables, respectively, and make the following
fairly standard assumptions on the local functions giand the stochastic oracles available at the network
nodes.
5Published in Transactions on Machine Learning Research (12/2022)
Assumption 3.3. Each function gi(θ,λ)is differentiable in Rd×∆m−1,L-smooth and µ- concave inλ.
Assumption 3.4. Each nodeihas access to the stochastic gradient oracles ∇θgi(θ,λ,ξi)and∇λgi(θ,λ,ξi),
with randomness w.r.t. ξi, which satisfy the following assumptions:
•Unbiasedness
Eξi[∇θgi(θ,λ,ξi)] =∇θgi(θ,λ),Eξi[∇λgi(θ,λ,ξi)] =∇λgi(θ,λ). (4)
•Bounded variance
Eξi/bracketleftig
∥∇θgi(θ,λ,ξi)−∇θgi(θ,λ)∥2/bracketrightig
≤σ2
θ,Eξi/bracketleftig
∥∇λgi(θ,λ,ξi)−∇λgi(θ,λ)∥2/bracketrightig
≤σ2
λ.(5)
•Bounded magnitude
Eξi/bracketleftig
∥∇θgi(θ,λ,ξi)∥2/bracketrightig
≤G2
θ,Eξi/bracketleftig
∥∇λgi(θ,λ,ξi)∥2/bracketrightig
≤G2
λ. (6)
The above assumption implies that each network node can query stochastic gradients that are unbiased, have
finite variance, and have bounded second moment. The bounded magnitude assumption is rather strong and
limits the choice of regularization functions, but it is often made in distributed stochastic optimization (Stich
et al., 2018; Koloskova et al., 2019a; Deng et al., 2021).
4 Distributionally Robust Decentralized Learning Algorithm
Problem (3) entails solving a distributed minimax optimization problem in which, at every round, collaborating
nodes store a private value of the model parameters and the dual variable, which are potentially different from
node to node. We denote the estimate of the primal and dual variables of node iat timetbyθt
iandλt
iand
the network estimates at time tas¯θt=1
m/summationtextm
i=1θt
iand¯λt=1
m/summationtextm
i=1λt
i, respectively. The main challenge
resulting from the decentralized implementation of the stochastic gradient descent/ascent algorithm consists
in approaching a minimax solution or a stationary point (depending on the convexity assumption on the loss
function) while concurrently ensuring convergence to a common global solution. To this end, the proposed
procedure, given in Algorithm 1, alternates between a local update step and a consensus step. At each round,
every node iqueries the local stochastic gradient oracle and, in parallel, updates the model parameter θiby
a gradient descent step with learning rate ηθ>0and the dual variable λiby a projected gradient ascent
one with learning rate ηλ>0(Euclidean projection). Subsequently, a gossip strategy is used to share and
average information between neighbors. In order to alleviate the communication burden of transmitting the
vector of model parameters, which is typically high dimensional and contributes to the largest share of the
communication load, a compressed gossip step is employed. To implement the compressed communication,
we consider the memory-efficient version of CHOCO-GOSSIP (Koloskova et al., 2019b) in which each node
needs to store only two additional variables ˆθiandsi, each of the same size as θi. The first one is a public
version ofθi, while the second is used to track the evolution of the weighted average, according to matrix
W, of the public variables at the neighboring nodes. Instead of transmitting θi, each node first computes
an averaging step to update the value of the private value using the information about the public variables
encoded in ˆθiandsi. It then computes qi, a compressed representation of the difference between ˆθiand
θi, and shares it with the neighboring nodes to update the value of ˆθiandsiused in the averaging step in
the next round. As the number of participating nodes is usually much smaller than the size of the model
(m≪d), the dual variable λiis updated sending uncompressed messages and then averaged according to
matrixW. Note that AD-GDA implicitly assumes that collaborating parties are honest and for this reason,
it does not employ any countermeasure against malicious nodes providing false dual variable information in
order to steer the distributional robust network objective at their whim.
4.1 Comparison with existing algorithms
Before deriving the convergence guarantees for AD-GDA we compare the features of AD-GDA against other
distributionally robust algorithms, DRFA and DR-DSGD (see Table 1). Firstly, we notice that AD-GDA and
6Published in Transactions on Machine Learning Research (12/2022)
DR-DSGD are fully decentralized algorithms, and therefore the only requirement for deployment is that the
network is connected. In contrast, DRFA is a client-server algorithm and star topologies, which are known to
be less fault tolerant and characterized by a communication bottleneck. Furthermore, AD-GDA is the only
algorithm that attains communication efficiency by the means of message compression that, as we show in
Section 5.2.2, allows AD-GDA to attain the same worst-node accuracy at a much lower communication cost
compared to DRFA and DR-DSGD. It is also important to notice that DR-SGD is obtained by restricting
the dual regularizer to Kullback-Leibler divergences, this allows sidestepping the minimax problem. On the
other hand, AD-GDA directly tackles the distributionally robust problem (3) and therefore can be applied
to any strongly concave regularizer that satisfies Assumption 3.4. For example, the chi-squared regularizer,
which allows AD-GDA to direclty minimize the distributionally objective of Mohri et al. (2019).. In Section
(4.3) we show AD-GDA recovers the same convergence rate of DR-DSGD in this more general set-up.
4.2 Convex Loss Function
We provide now a convergence guarantee for the solution output by Algorithm 1 for the case the loss function
ℓ(·)is convex in the model parameter θ. The result is given in the form of a sub-optimality gap bound for
the function
Φ(θ) =g(θ,λ∗(θ)),λ∗(·) := arg max
λ∈∆m−1g(·,λ) (7)
and it can be promptly derived from a primal-dual gap type of bound provided in the Appendix. In the
bound we also refer to θ∗(·)∈arg maxθ∈Rdg(θ,·).
Theorem 4.1. Under Assumptions 3.3, 3.4, we have that for any θ∗∈arg minθΦ(θ)the solutionθoreturned
by Algorithm 1 with learning rates ηθ=ηλ=1√
Tand consensus step size γ=ρ2δ
16ρ+ρ2+4β2+2ρβ2−8ρδsatisfies
E[Φ(θo)−Φ(θ∗)]≤O/parenleftbiggDθ+Dλ+G2
θ+G2
λ√
T/parenrightbigg
+O/parenleftbiggLDλGθ
c√
T+LDθGλ
ρ√
T/parenrightbigg
+O/parenleftbiggLG2
λ
ρ2T+LG2
θ
c2T/parenrightbigg
(8)
whereDλ:= maxtE/vextenddouble/vextenddouble¯λt−λ∗(θo)/vextenddouble/vextenddouble,Dθ:= maxtE/vextenddouble/vextenddouble¯θt−θ∗(λo)/vextenddouble/vextenddoubleandc=ρ2δ
82.
Theboundestablishesa O(1/√
T)non-asymptoticoptimalitygapguaranteefortheoutputsolution. Compared
to decentralized stochastic gradient descent (SGD) in the convex scenario, we obtain the same rate (without
a dependency on the number of workers m) but with a dependency on the network topology and compression
also in the lower order terms. Moreover, whenever θandλare constrained in convex sets, the diameter of
the two can be used to explicitly bound DθandDλ.
4.3 Non-convex Loss Function
We now focus on the case where the relation between the model parameters θand the value of the loss
function is non-convex. In this setting, carefully tuning the relation between primal and dual learning rates is
key to establishing a convergent recursion. From the centralized two-time scale stochastic gradient descent
literature, it is known that the primal learning rate ηθhas to be 1/(16(κ+ 1))time smaller than the dual
learning rate ηλ(Lin et al., 2020a). The above relationship ensures that the objective function changes
slowly enough in the dual variable λ, and it allows to bound the distance between optimal values of λ∗(θt)
and the current estimate λt. However, in the decentralized setup, the estimates of θandλdiffer at every
node and therefore there exists multiple optimal values of the dual variable; namely, λ∗(θt
i)fori= 1,...,m.
Nonetheless, we find that it is sufficient to control the quantity δt
λ:=/vextenddouble/vextenddoubleλ∗(¯θt)−¯λt/vextenddouble/vextenddouble2; the distance between
λ∗(¯θt), the optimal dual variable for the averaged primal estimate, and ¯λt, the current averaged dual estimate.
The following lemma, whose proof can be found in Appendix A.3, allows us to characterize the behaviour of
δt
λ.
Lemma 4.2. Forηθ=ηλ
16(κ+1)2andηλ=1
2L√
T, the sequence of {δt
λ}T
t=1generated by Algorithm 1 satisfies
T/summationdisplay
t=1E/bracketleftbig
δt
λ/bracketrightbig
≤5δ0
λκ
ηλµ+T/summationdisplay
t=15κ/parenleftigg
4Dt−1
λ/radicalbigg
1
mE/bracketleftbig
Ξt−1
θ/bracketrightbig
+3E/bracketleftbig
Ξt−1
θ/bracketrightbig
m+7E/bracketleftbig
Ξt−1
λ/bracketrightbig
m/parenrightigg
7Published in Transactions on Machine Learning Research (12/2022)
+T/summationdisplay
t=15/parenleftbigg8κ2η2
θ
η2
λµ2E/bracketleftig/vextenddouble/vextenddouble∇Φ(¯θt−1)/vextenddouble/vextenddouble2/bracketrightig/parenrightbigg
+ 5T/parenleftbigg2ηλσ2
λ
mµ+4σ2
θ
162m(κ+ 1)2µ2/parenrightbigg
(9)
where ΞθandΞλare the primal and dual consensus errors, and Dt−1
λ=/vextenddouble/vextenddouble¯λt−1−λ/vextenddouble/vextenddouble.
Lemma 4.2 provides us with an inequality that controls the “speed” at which the optimization problem
changes from a network-level perspective. As such, the expression (9) contains consensus error terms that do
not appear in the centralized setup. We find that to establish a convergent recursion while at the same time
controlling the consensus error terms, the primal learning rate ηθhas to be 1/(16(κ+ 1)2)time smaller than
the dualηλ(therefore 1/(κ+ 1)smaller compared to the centralized case). Once this condition is met it is
possible to provide a bound on the stationarity of the randomized solution, picked uniformly over time, that
matches the one known for the centralized case.
Theorem 4.3. Under Assumptions 3.3, 3.4, the iterates of Algorithm 1 with learning rates ηθ=ηλ
16(κ+1)2
andηλ=1
2L√
Tand consensus step size γ=ρ2δ
16ρ+ρ2+4β2+2ρβ2−8ρδsatisfy
/summationtextT
t=1E/bracketleftig/vextenddouble/vextenddouble∇Φ(¯θt−1)/vextenddouble/vextenddouble2/bracketrightig
T≤O/parenleftbigg
L∆ΦT
√
T+L2κ2D0
λ
2√
T/parenrightbigg
+O/parenleftbiggDλLGθ
c√
T+σ2
θ+κσ2
λ
m√
T/parenrightbigg
+O/parenleftbiggG2
θ
c2T+κG2
λ
ρ2T/parenrightbigg
+σ2
θ
m
(10)
where ∆ΦT=E[Φ(¯θ0)]−E[Φ(¯θT)]andc=ρ2δ
82.
We note that the bound decreases at a rate O(1/√
T), except the last variance term which is non-vanishing.
Nonetheless, whenever the variance of the stochastic gradient oracle for the primal variable is small or the
number of participating devices is large, this term becomes negligible. Otherwise, at a cost of increased
gradient complexity, each device can query the oracle O(1/ϵ2)times every round, average the results and
make the stochastic gradient variance O(1/ϵ2). This procedure makes the bound vanish and leads to a
gradient complexity matching the one of Sharma et al. (2022) given for the federated learning scenario.
Table 2: Final worst-case distribution accuracy attained by AD-GDA and CHOCO-SGD under different
compression schemes and compression ratios.
Quantization Sparsification
16bit 8bit 4bit 50% 25% 10%
Logistic AD-GDA 59.19±2.05 57.43±1.44 55.75±2.09 57.05±0.68 54.02±1.14 51.51±2.88
Logistic CHOCO-SGD 30.69±0.96 30.06±0.83 29.46±0.05 30.28±0.60 28.56±0.54 26.39±0.67
F.C. AD-GDA 54.99±1.92 48.99±2.30 47.08±2.53 51.85±2.11 43.65±2.97 38.95±3.21
F.C. CHOCO-SGD 30.83±2.22 28.08±2.50 28.01±2.59 29.92±2.54 27.11±2.96 25.91±3.20
5 Experiments
In this section, we empirically evaluate AD-GDA capabilities in producing robust predictors. We first
compare AD-GDA with CHOCO-SGD and showcase the merits of the distributionally robust procedure across
different learning models, communication network topologies, and message compression schemes. We then
consider larger-scale experimental setups in which we study the effect of the regularization on the worst-case
distribution accuracy and compare AD-GDA against Distributionally Robust Federated Averaging (DRFA)
(Deng et al., 2021) and Distributionally Robust Decentralized Stochastic Gradient Descent (DR-DSGD)
(Issaid et al., 2022).
8Published in Transactions on Machine Learning Research (12/2022)
0 1000 2000 3000
Iteration10−1100Loss4 bit Quant.
8 bit Quant.
16 bit Quant.
32 bit Quant.
(a) Quantization
0 1000 2000 3000
Iteration10−1100Loss25% Spars.
50% Spars.
75% Spars.
100% Spars. (b) Sparsification
Figure 3: Convergence plots of AD-GDA for different quantization
levels and schemes.
0 1000 2000 3000
Iteration10−1100LossRING
2D-TORUS
MESHFigure 4: Convergence plot of AD-
GDA for different topologies.
5.1 Fashion-MNIST Classification
We perform our experiments using the Fashion-MNIST data set (Xiao et al., 2017)2, a popular data set
made of images of 10 different clothing items, which is commonly used to test distributionally robust learners
(Mohri et al., 2019; Deng et al., 2021). To introduce data heterogeneity, samples are partitioned across the
network devices using a class-wise split. Namely, using a workstation equipped with a GTX 1080 Ti, we
simulate a network of 10 nodes, each storing data points coming from one of the 10 classes. In this setting,
we train a logistic regression model and a two-layer fully connected neural network with 25 hidden units to
investigate both the convex and the non-convex cases. In both cases, we use the SGD optimizer and, to
ensure consensus at the end of the optimization process, we consider a geometrically decreasing learning rate
ηt
θ=r−tη0
θwith ratior= 0.995and initial value η0
θ= 1. The metrics that we track are the final worst-node
distribution accuracy and the average accuracy over the aggregated data samples on the network estimate ¯θt.
5.1.1 Effect of Compression
We assess the effect of compression with a fixed budget in terms of communication rounds by organizing nodes
in a ring topology and training the logistic model and the fully connected network for T= 2000iterations. As
a representative of the unbiased compression operators, we consider the b-bit random quantization scheme for
b={16,8,4}bit levels, while for the biased category we implement the top- Ksparsification scheme saving
K={50%,25%,10%}of the original message components. For each compression scheme and compression
level, we tune the consensus step size γby performing a grid search. We train the different models for
20 different random placements of the data shards across the devices using the distributionally robust
and standard learning paradigms. In Table 2 we report the average worst-case accuracy attained by the
final averaged model ¯θT. AD-GDA almost doubles the worst-case accuracy compared to the not-robust
baseline CHOCO-SGD (Koloskova et al., 2019b). This gain holds for both compression schemes and across
different compression levels. For increased compression ratios, the worst-case accuracy degrades; however,
for a comparable saving in communication bandwidth the unbiased quantization scheme results in superior
performance than the biased sparsification compression operator. For a fixed optimization horizon compression
degrades performance. Nonetheless, compression allows us to obtain the same accuracy level with fewer
transmitted bits. In Figure 3 we report the convergence plot of AD-GDA under the different compression
schemes and ratios. For this experiment we track the worst-node loss of a logistic model trained using a fixed
learning rate ηθ= 0.1. The convergence plots confirm the sub-linear rate of AD-GDA and highlight the effect
of the compression level on the slope of convergence.
5.1.2 Effect of Topology
We now turn to investigate the effect of node connectivity. Sparser communication topologies slow down
the consensus process and therefore hamper the convergence of the algorithm. In the previous batch of
2The Fashion-MNIST data set is released under the MIT License
9Published in Transactions on Machine Learning Research (12/2022)
Table 3: Worst-node accuracy attained by AD-GDA and CHOCO-SGD for different network topologies.
Top-10% Sparsification 4-bit Quantization
2D Torus Mesh 2D Torus Mesh
Log. AD-GDA 54.00±0.61 54.07±0.03 56.94±0.38 57.11±0.03
Log. CHOCO-SGD 26.82±0.41 29.00±0.02 30.82±0.24 30.97±0.03
F.C. AD-GDA 44.31±2.47 45.21±2.22 50.16±1.85 50.80±1.83
F.C. CHOCO-SGD 26.02±2.29 26.38±2.65 28.79±2.22 28.96±1.87
experiments, we considered a sparse ring topology, in which each node is connected to only two other nodes.
Here, we explore two other network configurations with a more favorable spectral gap. The communication
topology with each node connected to the other 4 nodes and the mesh case, in which all nodes communicate
with each other. For these configurations, we consider the 4-bit quantization and top-10% sparsification
compression schemes. In Table 3 we report the final worst-case performance for the different network
configurations. As expected, network configurations with larger node degrees lead to higher worst-case
accuracy owing to the faster convergence. In Figure 4 we provide the convergence plot of AD-GDA for
different communication topologies. In particular, we track the worst-node loss versus the number of iterations
for the logistic model optimized using a fixed learning rate ηθ= 0.1. The predicted sublinear rate of the
AD-GDA is confirmed and it is also possible to appreciate the influence of the spectral gap on the convergence
rates.
5.2 Larger Scale Experiments
We now study AD-GDA and compared it with existing distributionally robust algorithms. We consider three
larger-scale experimental setups:
•A larger scale version of the Fashion MNIST classification task of Section 5.1. In this section, we
assume a larger network comprising a total of 50 devices with nodes storing samples from only one
class.
•A CIFAR-10 image classification task based on 4-layer convolutional neural networks (CNN)
(Krizhevsky et al., 2009). We consider a network of 20 network nodes and we introduce data
heterogeneity by evenly partitioning the training set and changing the contrast of the images stored
on the devices. The pixel value contrast P∈[0,255]is modified using the following non-linear
transformation
fc(P) =clip[0,255][(128 +c(P−128))1.1], (11)
whereclip[0,255](·)rounds values to the discrete set [0,255]. Forc<1the contrast is reduced while
forc >1it is enhanced. We consider two network nodes storing images with reduced contrast
(c= 0.5), two storing images with increased contrast ( c= 1.5), and the rest of the nodes storing
images with the original contrast level ( c= 1). This setup can be used to model a camera network
(e.g. surveillance network) containing devices deployed under different lighting conditions.
•A network of 10 nodes collaborating to train a 4-layer CNN to classify microscopy images from the
biological data set COOS7 (Lu et al., 2019). Data heterogeneity is a consequence of the existence
of different instruments used to sample the training data. In particular, we consider two of the
collaborating nodes using a different microscope from the rest of the collaborating devices. This
setup illustrates the risk of training models based on biological data affected by local confounders —
in this case, the usage of different sensors.
5.2.1 Effect of Regularization
We first evaluate the effect of the regularization parameter αin the distributionally robust formulation (3)
and study how it affects AD-GDA final performance. According to the two-player game interpretation of
10Published in Transactions on Machine Learning Research (12/2022)
Table 4: Testing accuracy attained by AD-DGA for different regularization values α. For the Fashion-MNIST
data set we report the worst and best class accuracy, for the CIFAR-10 data set the accuracy for the different
contrast images and for the COOS7 data set the accuracy for the different microscopes. The last column in
all tables is the the accuracy attained on a test data set comprising samples from all local data distributions.
(a) Fashion-MNIST.
Worst Class Best Class Average
α= 10 52 .83±1.14 90.63±0.18 77.31±0.19
α= 1 59.17±1.06 89.77±0.60 76.77±0.04
α= 0.01 58.47±0.92 89.56±0.52 76.67±0.09(b) COOS7.
Microscope 1 Microscope 2 Average
α= 10 66 .93±0.23 86.54±0.06 76.73±0.13
α= 1 72 .27±0.31 81.30±0.18 76.77±0.22
α= 0.01 75.00±0.24 75.63±0.40 75.31±0.22
(c) CIFAR-10.
Low Contrast High Contrast Original Contrast Average
α= 10 34 .66±0.47 40.67±1.29 44.28±0.84 39 .87±3.96
α= 0.1 37.30±0.73 40.93±1.54 43.62±0.91 40.61±2.59
α= 0.01 39.06±0.80 41.13±1.14 42.96±0.91 41.05±1.59
Table 5: Worst-case distribution accuracy attained by AD-GDA, DR-DSGD and DRFA.
Fashion-MNIST CIFAR-10 COOS7
AD-GDA 58.47±0.92 39.06±0.80 75.00±0.24
DRFA 56.68±1.04 37 .20±1.16 69.16±0.39
DR-DSGD 50.77±0.42 38 .00±0.25 67.09±0.42
the minimax optimization problem (3), the regularizer r(λ)reduces the freedom that an adversary has in
choosing the weighting vector λto maximize the training loss at every iteration t. As a result, the smaller the
value ofα, the less constrained is the adversary and the larger will be the emphasis on the worst-performing
nodes. This intuition is confirmed by the following experiments in which we consider a regularizer of the form
χ2(λ) :=/summationtext
i(λi−ni/n)2
ni/nand run AD-GDA for α={10,1,0.01}. In Table 4 we report the average accuracy
attained in the three simulation setups. For α= 10, we observe a large test accuracy gap between the worst
and best nodes in the network: 37% for the Fashion-MNIST data set, 9% for the CIFAR-10 data set and 19%
for the COOS7 data set. This large accuracy mismatch showcases how large regularization parameter values,
and standard decentralized optimization schemes (obtained for α=∞), are unable to guarantee uniform
performance across participating parties. On the other hand, using smaller regularization parameters, the
gap between is effectively reduced: 31% for the Fashion-MNIST, less than 2% for CIFAR-10 and less than 1%
for COOS7. At the same time, the improved fairness brought by AD-GDA does not significantly hamper the
average performance of the final model as reported in the last column of all the tables.
In Table 5 we compare the AD-GDA worst-node accuracy against the one obtained by DRFA and DR-DSGD.
In particular, we run AD-GDA with a regularization parameter α= 0.01and without message compression.
For DR-DSGD we consider the same network setup and we fix the regularization parameter α= 6, which
we find to yield the best performance. Finally, for DRFA, we organize the network nodes according to a
star topology and we run the federated optimization using half-user participation and with a number of
local iterations equal to 10. Across all experiments, we find that AD-GDA yields the largest worst-node
accuracy. Compared to DR-DSGD, we attribute the superiority of AD-GDA to the capability of solving the
distributionally robust optimization using any strongly-concave regularizer, in this case, the chi-squared one.
On the other hand, the superiority of DRFA can be attributed to the fact that DRFA attains distributional
robustness by sampling more frequently nodes with unsatisfactory performance. However, whenever the
fraction of these users is small compared to the overall number of devices that join the federated round,
DRFA is unable to prioritize the nodes with the worst performance since they remain under-represent within
the group of sampled devices.
11Published in Transactions on Machine Learning Research (12/2022)
5.2.2 Communication Efficiency
0 200 400 600 800 1000
MB0.10.20.30.40.5Accuracy
AD-GDA 4 bit Quant.
DR-DSGD
CHOCO-SGD 4 bit Quant.
DRFA
(a) Fashion-MNIST
0 500 1000 1500 2000 2500
MB0.100.150.200.250.300.35Accuracy
AD-GDA 4 bit Quant.
DR-DSGD
CHOCO-SGD 4 bit Quant.
DRFA (b) CIFAR-10
0 2000 4000 6000 8000
MB0.30.40.50.60.7Accuracy
AD-GDA 4 bit Quant.
DR-DSGD
CHOCO-SGD 4 bit Quant.
DRFA (c) COOS7
Figure 5: Comparison between the proposed algorithm (AD-GDA), Distributionally Robust Federated
Averaging (DRFA), Distributionally Robust Decentralized Stochastic Gradient Descent (DR-SGD) and
standarddecentralizedlearning(CHOCO-SGD).Thealgorithmsarecomparedintermsoftheircommunication
efficiency and worst node accuracy.
In the following, we compare AD-GDA against CHOCO-SGD and other existing distributionally robust
learning schemes; in particular, Distributionally Robust Federated Averaging (DRFA) (Deng et al., 2021) and
Distributionally Robust Decentralized Stochastic Gradient Descent (DR-SGD) (Issaid et al., 2022).CHOCO-
SGD represents a standard decentralized learning procedure that employs compressed gossip to attain
communication efficiency. DRFA is a federated distributionally robust learning scheme with network devices
connected according to a star topology and the star center represented by a central aggregator. Communication
efficiency is obtained allowing network devices to perform multiple local updates of the primal variable between
subsequent synchronization rounds at the central aggregator. We run DRFA allowing devices to perform 10
local gradient steps before sending their local models for the distributionally robust averaging steps and we
consider half-user participation at each round. DR-DSGD is a decentralized distributionally robust learning
scheme based on KL regularizers that do not employ compressed communication. For the decentralized
learning schemes, we consider a 2D torus topology with every node connected to the other four nodes and
use Metropolis mixing matrices. For AD-GDA and CHOCO-SGD, which employ compressed communication,
we consider a 4-bit quantization scheme. For AD-GDA we consider a chi-squared regularizer with α= 0.01,
while for DR-DSGD we set the KL regularizer parameter to α= 6as in Issaid et al. (2022). All algorithms
are run for T= 5000iterations using an SGD optimizer and the same exponentially learning rate schedule
ηt
θ=r−tη0
θwith decay r= 0.998. In order to make the comparison fair, the initial learning rate η0is set
differently at every node to ensure that the effective learning rate is equal across different algorithms. In fact,
DR-DSGD and AD-GDA descent steps are affected by the dual variable. The batch sizes are the same across
all algorithms and are set to {50,50,32}for the F-MNIST, CIFAR10 and COOS7 experiments, respectively.
The algorithms are compared in terms of communication efficiency, namely the capability of producing a fair
predictor communicating the smallest number of bits. To this end, in Figure 5 we compare the worst-case
distribution accuracy against the number of bits transmitted by the busiest node in the network. The
simulation results are reported over the different simulation scenarios. AD-GDA and CHOCO-SGD employ
the same compression scheme and converge within the same communication budget. However, AD-GDA
can greatly increase the worst node accuracy compared to CHOCO-SGD. This showcases the merits of
distributionally robust learning compared to standard learning. Compared to the other distributionally robust
algorithms, AD-GDA attains the largest worst-case accuracy by transmitting only a fraction of bits. In
particular, for the Fashion-MNIST setup, AD-GDA is 4x and 8x more communication-efficient compared to
DRFA and DR-DSGD respectively. For the CIFAR-10 data set, AD-GDA reduces by 3x and 5x the number
of bits necessary to attain the same final worst-node accuracy of DRFA and DR-DSGD respectively. Finally,
in the COOS7 case, AD-GDA is 3x more efficient than DRFA and 10x more efficient than DR-DSGD.
12Published in Transactions on Machine Learning Research (12/2022)
6 Conclusion
We provided a provably convergent decentralized single-loop gradient descent/ascent algorithm to solve the
distributionally robust optimization problem over a network of collaborating nodes with heterogeneous local
data distributions. Differently from previously proposed solutions, which are either limited to the federated
scenario with a central coordinator or to specific regularizers, our algorithm directlytackles the underlying
minimax optimization problem in a decentralized andcommunication-efficient manner. Experiments showed
that the proposed solution produces distributionally robust predictors and it attains superior communication
efficiency compared to the previously proposed algorithms. A combination of the compressed communication
and multiple local updates, combined with acceleration techniques, represents the natural extension of the
algorithm to further improve its efficiency.
Acknowledgments
The work of M. Zecchin is funded by the Marie Curie action WINDMILL (Grant agreement No. 813999). The
work of M. Kountouris has received funding from the European Research Council (ERC) under the European
Union’s Horizon 2020 research and innovation programme (Grant agreement No. 101003431). The work of
David Gesbert was partially supported by the 3IA interdisciplinary project ANR-19-P3IA-0002 funded from
the French National Research Agency (ANR)”
References
Alham Fikri Aji and Kenneth Heafield. Sparse communication for distributed gradient descent. arXiv preprint
arXiv:1704.05021 , 2017.
Dan Alistarh, Demjan Grubic, Jerry Li, Ryota Tomioka, and Milan Vojnovic. Qsgd: Communication-efficient
sgd via gradient quantization and encoding. Advances in Neural Information Processing Systems , 30:
1709–1720, 2017.
Dan Alistarh, Torsten Hoefler, Mikael Johansson, Sarit Khirirat, Nikola Konstantinov, and Cédric Renggli.
The convergence of sparsified gradient methods. arXiv preprint arXiv:1809.10505 , 2018.
Jeremy Bernstein, Yu-Xiang Wang, Kamyar Azizzadenesheli, and Animashree Anandkumar. signsgd:
Compressed optimisation for non-convex problems. In International Conference on Machine Learning , pp.
560–569. PMLR, 2018.
Jianshu Chen and Ali H Sayed. Diffusion adaptation strategies for distributed optimization and learning over
networks. IEEE Transactions on Signal Processing , 60(8):4289–4305, 2012.
YuyangDeng, MohammadMahdiKamani, andMehrdadMahdavi. Distributionally robustfederatedaveraging.
arXiv preprint arXiv:2102.12660 , 2021.
John Duchi and Hongseok Namkoong. Learning models with uniform performance via distributionally robust
optimization. arXiv preprint arXiv:1810.08750 , 2018.
John C Duchi, Alekh Agarwal, and Martin J Wainwright. Dual averaging for distributed optimization:
Convergence analysis and network scaling. IEEE Transactions on Automatic control , 57(3):592–606, 2011.
John C Duchi, Tatsunori Hashimoto, and Hongseok Namkoong. Distributionally robust losses against mixture
covariate shifts. Under review , 2019.
Cynthia Dwork, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Richard Zemel. Fairness through
awareness. In Proceedings of the 3rd innovations in theoretical computer science conference , pp. 214–226,
2012.
Peyman Mohajerin Esfahani and Daniel Kuhn. Data-driven distributionally robust optimization using the
wasserstein metric: Performance guarantees and tractable reformulations. Mathematical Programming , 171
(1):115–166, 2018.
13Published in Transactions on Machine Learning Research (12/2022)
Ian J Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron
Courville, and Yoshua Bengio. Generative adversarial networks. arXiv preprint arXiv:1406.2661 , 2014.
Zhaolin Hu and L Jeff Hong. Kullback-leibler divergence constrained distributionally robust optimization.
Available at Optimization Online , 2013.
Chaouki Ben Issaid, Anis Elgabli, and Mehdi Bennis. DR-DSGD: A distributionally robust decentralized
learning algorithm over graphs. Transactions on Machine Learning Research , 2022. URL https://
openreview.net/forum?id=VcXNAr5Rur .
Ruiwei Jiang and Yongpei Guan. Data-driven chance constrained stochastic program. Mathematical
Programming , 158(1):291–327, 2016.
Anastasia Koloskova, Tao Lin, Sebastian U Stich, and Martin Jaggi. Decentralized deep learning with
arbitrary communication compression. arXiv preprint arXiv:1907.09356 , 2019a.
Anastasia Koloskova, Sebastian Stich, and Martin Jaggi. Decentralized stochastic optimization and gossip
algorithms with compressed communication. In International Conference on Machine Learning , pp.
3478–3487. PMLR, 2019b.
Alec Koppel, Felicia Y Jakubiec, and Alejandro Ribeiro. A saddle point algorithm for networked online
convex optimization. IEEE Transactions on Signal Processing , 63(19):5149–5164, 2015.
Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.
Shihui Li, Yi Wu, Xinyue Cui, Honghua Dong, Fei Fang, and Stuart Russell. Robust multi-agent reinforcement
learning via minimax deep deterministic policy gradient. In Proceedings of the AAAI Conference on Artificial
Intelligence , volume 33, pp. 4213–4220, 2019.
Xiangru Lian, Ce Zhang, Huan Zhang, Cho-Jui Hsieh, Wei Zhang, and Ji Liu. Can decentralized algorithms
outperform centralized algorithms? a case study for decentralized parallel stochastic gradient descent.
arXiv preprint arXiv:1705.09056 , 2017.
Tianyi Lin, Chi Jin, and Michael Jordan. On gradient descent ascent for nonconvex-concave minimax
problems. In International Conference on Machine Learning , pp. 6083–6093. PMLR, 2020a.
Tianyi Lin, Zhengyuan Zhou, Panayotis Mertikopoulos, and Michael Jordan. Finite-time last-iterate conver-
gence for multi-agent learning in games. In International Conference on Machine Learning , pp. 6161–6171.
PMLR, 2020b.
Qing Ling, Zaiwen Wen, and Wotao Yin. Decentralized jointly sparse optimization by reweighted ℓq
minimization. IEEE Transactions on Signal Processing , 61(5):1165–1170, 2012.
Mingrui Liu, Wei Zhang, Youssef Mroueh, Xiaodong Cui, Jerret Ross, Tianbao Yang, and Payel Das. A
decentralized parallel algorithm for training generative adversarial nets. arXiv preprint arXiv:1910.12999 ,
2019a.
Weijie Liu, Aryan Mokhtari, Asuman Ozdaglar, Sarath Pattathil, Zebang Shen, and Nenggan Zheng. A
decentralized proximal point-type method for saddle point problems. arXiv preprint arXiv:1910.14380 ,
2019b.
Nicolas Loizou, Hugo Berard, Gauthier Gidel, Ioannis Mitliagkas, and Simon Lacoste-Julien. Stochastic
gradient descent-ascent and consensus optimization for smooth games: Convergence analysis under expected
co-coercivity. Advances in Neural Information Processing Systems , 34:19095–19108, 2021.
Alex X Lu, Amy X Lu, Wiebke Schormann, Marzyeh Ghassemi, David W Andrews, and Alan M Moses. The
cells out of sample (coos) dataset and benchmarks for measuring out-of-sample generalization of image
classifiers. arXiv preprint arXiv:1906.07282 , 2019.
14Published in Transactions on Machine Learning Research (12/2022)
Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards
deep learning models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083 , 2017.
David Mateos-Núnez and Jorge Cortés. Distributed subgradient methods for saddle-point problems. In 2015
54th IEEE Conference on Decision and Control (CDC) , pp. 5462–5467. IEEE, 2015.
Peyman Mohajerin Esfahani and Daniel Kuhn. Data-driven distributionally robust optimization using the
wasserstein metric: Performance guarantees and tractable reformulations. Mathematical Programming , 171
(1):115–166, 2018.
Mehryar Mohri, Gary Sivek, and Ananda Theertha Suresh. Agnostic federated learning. In International
Conference on Machine Learning , pp. 4615–4625. PMLR, 2019.
Hongseok Namkoong and John C Duchi. Stochastic gradient methods for distributionally robust optimization
with f-divergences. In NIPS, volume 29, pp. 2208–2216, 2016.
Angelia Nedic and Asuman Ozdaglar. Distributed subgradient methods for multi-agent optimization. IEEE
Transactions on Automatic Control , 54(1):48–61, 2009.
Reza Olfati-Saber, J Alex Fax, and Richard M Murray. Consensus and cooperation in networked multi-agent
systems. Proceedings of the IEEE , 95(1):215–233, 2007.
Lerrel Pinto, James Davidson, Rahul Sukthankar, and Abhinav Gupta. Robust adversarial reinforcement
learning. In International Conference on Machine Learning , pp. 2817–2826. PMLR, 2017.
Joaquin Quiñonero-Candela, Masashi Sugiyama, Neil D Lawrence, and Anton Schwaighofer. Dataset shift in
machine learning . Mit Press, 2009.
Michael Rabbat. Multi-agent mirror descent for decentralized stochastic optimization. In 2015 IEEE 6th
International Workshop on Computational Advances in Multi-Sensor Adaptive Processing (CAMSAP) , pp.
517–520. IEEE, 2015.
Herbert E Scarf. A min-max solution of an inventory problem. Technical report, RAND CORP SANTA
MONICA CALIF, 1957.
Ohad Shamir and Nathan Srebro. Distributed stochastic optimization and learning. In 2014 52nd Annual
Allerton Conference on Communication, Control, and Computing (Allerton) , pp. 850–857. IEEE, 2014.
Pranay Sharma, Rohan Panda, Gauri Joshi, and Pramod K Varshney. Federated minimax optimization:
Improved convergence analyses and algorithms. arXiv preprint arXiv:2203.04850 , 2022.
Aman Sinha, Hongseok Namkoong, Riccardo Volpi, and John Duchi. Certifying some distributional robustness
with principled adversarial training. arXiv preprint arXiv:1710.10571 , 2017.
Sebastian U Stich. Local sgd converges fast and communicates little. arXiv preprint arXiv:1805.09767 , 2018.
Sebastian U Stich, Jean-Baptiste Cordonnier, and Martin Jaggi. Sparsified sgd with memory. arXiv preprint
arXiv:1809.07599 , 2018.
Ioannis Tsaknakis, Mingyi Hong, and Sijia Liu. Decentralized min-max optimization: Formulations, algorithms
and applications in network poisoning attack. In ICASSP 2020-2020 IEEE International Conference on
Acoustics, Speech and Signal Processing (ICASSP) , pp. 5755–5759. IEEE, 2020.
John Tsitsiklis, Dimitri Bertsekas, and Michael Athans. Distributed asynchronous deterministic and stochastic
gradient optimization algorithms. IEEE transactions on automatic control , 31(9):803–812, 1986.
John Nikolas Tsitsiklis. Problems in decentralized decision making and computation. Technical report,
Massachusetts Inst of Tech Cambridge Lab for Information and Decision Systems, 1984.
Ermin Wei and Asuman Ozdaglar. Distributed alternating direction method of multipliers. In 2012 IEEE
51st IEEE Conference on Decision and Control (CDC) , pp. 5445–5450. IEEE, 2012.
15Published in Transactions on Machine Learning Research (12/2022)
David Wozabal. A framework for optimization under ambiguity. Annals of Operations Research , 193(1):
21–47, 2012.
Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: a novel image dataset for benchmarking
machine learning algorithms. arXiv preprint arXiv:1708.07747 , 2017.
Feng Yan, Shreyas Sundaram, SVN Vishwanathan, and Yuan Qi. Distributed autonomous online learning:
Regrets and intrinsic privacy-preserving properties. IEEE Transactions on Knowledge and Data Engineering ,
25(11):2483–2493, 2012.
Hao Yu, Sen Yang, and Shenghuo Zhu. Parallel restarted sgd with faster convergence and less communication:
Demystifying why model averaging works for deep learning. In Proceedings of the AAAI Conference on
Artificial Intelligence , volume 33, pp. 5693–5700, 2019.
A Appendix
A.1 Useful inequalities
This section contains a collection of ancillary results that are useful for the subsequent proofs.
Proposition A.1. A differentiable and L-smooth function f(x)satisfies
∥∇f(x)−∇f(x′)∥≤L∥x−x′∥. (12)
Furthermore, if f(x)is convex
f(y)≤f(x) +⟨∇f(x),y−x⟩+L
2∥y−x∥2(13)
and ifx∗is a minimizer
1
2L∥∇f(x)∥2≤f(x)−f(x∗). (14)
Otherwise, if f(x)concave
f(y)≥f(x) +⟨∇f(x),y−x⟩−L
2∥y−x∥2(15)
and ifx∗is a maximizer
1
2L∥∇f(x)∥2≤f(x∗)−f(x). (16)
Proposition A.2. A differentiable and µ-strongly convex function f(x)satisfies
f(y)≥f(x) +⟨∇f(x),y−x⟩+µ
2∥y−x∥2(17)
and a differentiable and µ-strongly concave function g(x)satisfies
g(y)≤g(x) +⟨∇g(x),y−x⟩−µ
2∥y−x∥2. (18)
Proposition A.3. Given two vectors a,b∈Rd, forβ >0we have
2⟨a,b⟩≤β−1∥a∥2+β∥b∥2(19)
and
∥a+b∥≤(1 +β−1)∥a∥2+ (1 +β)∥b∥2(20)
16Published in Transactions on Machine Learning Research (12/2022)
Proposition A.4. Given two matrices A∈Rp×q,B∈Rq×r, we have
∥AB∥F≤∥A∥F∥B∥2(21)
where∥·∥Fdenotes the Frobenius norm.
Proposition A.5. Given a set of vectors {ai}n
i=1we have
/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoublen/summationdisplay
i=1ai/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
≤nn/summationdisplay
i=1∥ai∥2. (22)
Consensus inequalities
To streamline the notation we define ˜∇gi(θt
i,λt
i) =∇gi(θt
i,λt
i,ξt
i)and introduce the following matrices
Θt=/bracketleftbig
θt
1,...,θt
m/bracketrightbig
∈Rd×m,ˆΘt=/bracketleftig
ˆθt
1,..., ˆθt
m/bracketrightig
∈Rd×m,Λt=/bracketleftbig
λt
1,...,λt
m/bracketrightbig
∈Rm×m.(23)
˜∇θG(Θt,Λt) =/bracketleftbig˜∇θg1(θt
1,λt
1),..., ˜∇θgm(θt
m,λt
m)/bracketrightbig
∈Rd×m(24)
˜∇λG(Θt,Λt) =/bracketleftbig˜∇λg1(θt
1,λt
1),..., ˜∇λgm(θt
m,λt
m)/bracketrightbig
∈Rm×m(25)
and for a matrix Xwe define ¯X=X11T
m.
The local update rule of Algorithm 1 can be rewritten as
Θt+1
2= Θt−ηθ˜∇θG(Θt,Λt) (26)
Λt+1
2=PΛ/parenleftbig
Λt+ηλ˜∇λG(Θt,Λt)/parenrightbig
(27)
wherePΛis applied column-wise. The compressed gossip algorithm CHOCO-GOSSIP (Koloskova et al.,
2019b) used to share model parameters preserves averages and satisfies the following recursive inequality
withc=ρ2δ
82
E/bracketleftbigg/vextenddouble/vextenddoubleΘt+1−¯Θt+1/vextenddouble/vextenddouble2
F+/vextenddouble/vextenddouble/vextenddoubleΘt+1−ˆΘt+1/vextenddouble/vextenddouble/vextenddouble2
F/bracketrightbigg
≤(1−c)E/bracketleftbigg/vextenddouble/vextenddouble/vextenddoubleΘt+1
2−¯Θt+1
2/vextenddouble/vextenddouble/vextenddouble2
F+/vextenddouble/vextenddouble/vextenddoubleΘt+1
2−ˆΘt/vextenddouble/vextenddouble/vextenddouble2
F/bracketrightbigg
.(28)
The uncompressed gossip scheme used to communicate Λsatisfies
E/bracketleftig/vextenddouble/vextenddoubleΛt+1−¯Λt+1/vextenddouble/vextenddouble2
F/bracketrightig
≤(1−ρ)E/bracketleftbigg/vextenddouble/vextenddouble/vextenddoubleΛt+1
2−¯Λt+1
2/vextenddouble/vextenddouble/vextenddouble2
F/bracketrightbigg
. (29)
Lemma A.6. (Consensus inequality for compressed communication (Koloskova et al., 2019a)) For a fixed
ηθ>0andγ=ρ2δ
16ρ+ρ2+4β2+2ρβ2−8ρδthe iterates of Algorithm 1 satisfy
E/bracketleftbig
Ξt
θ/bracketrightbig
=E/bracketleftiggm/summationdisplay
i=1/vextenddouble/vextenddouble/vextenddoubleθt
i−¯θt/vextenddouble/vextenddouble/vextenddouble2/bracketrightigg
≤12η2
θmG2
θ
c2. (30)
Lemma A.7. (Consensus Inequality for uncompressed communication (Koloskova et al., 2019b)) For a fixed
ηλ>0the iterates of Algorithm 1 satisfy
E/bracketleftbig
Ξt
λ/bracketrightbig
=E/bracketleftiggm/summationdisplay
i=1/vextenddouble/vextenddouble/vextenddoubleλt
i−¯λt/vextenddouble/vextenddouble/vextenddouble2/bracketrightigg
≤4η2
λmG2
λ
ρ2(31)
Lemma A.6 and A.7 apply to the primal and dual iterates obtained of Algorithm 1 and they are obtained
from Lemma A.2 of (Koloskova et al., 2019b).
17Published in Transactions on Machine Learning Research (12/2022)
A.2 Proof of Theorem 4.1: Convex case
Define
Φ(·) = max
λ∈∆m−1g(·,λ); (32)
under assumptions 3.3, 3.4 and if the local objective functions {fi(θ)}m
i=1are convex, Theorem 4.1 guarantees
that the output solution (θo,λo)satisfies
E/bracketleftbigg
Φ(θo)−min
θ∈ΘΦ(θ)/bracketrightbigg
≤4
T/parenleftbiggLG2
λ
ρ2+ 3LG2
θ
c2/parenrightbigg
+1√
T/parenleftbigg√
12DλLGθ
c+ 2DθLGλ
ρ/parenrightbigg
+1√
T/parenleftbiggDθ+Dλ
2+G2
θ+G2
λ
2/parenrightbigg
. (33)
The proof starts from the following decomposition of the sub-optimality gap
E/bracketleftbigg
max
λg(θo,λ)−min
θmax
λg(θ,λ)/bracketrightbigg
≤E/bracketleftbigg
max
λg(θo,λ)−max
λmin
θg(θ,λ))/bracketrightbigg
(34)
≤E/bracketleftbigg
max
λg(θo,λ)−min
θg(θ,λo))/bracketrightbigg
(35)
≤E/bracketleftbigg
max
λ,θg(θo,λ)−g(θ,λo))/bracketrightbigg
(36)
≤E/bracketleftigg
max
λ,θ1
TT−1/summationdisplay
t=0g(¯θt,λ)−g(θ,¯λt))/bracketrightigg
(37)
≤E/bracketleftigg
max
λ1
TT−1/summationdisplay
t=0g(¯θt,λ)−g(¯θt,¯λt)/bracketrightigg
+E/bracketleftigg
max
θ1
TT−1/summationdisplay
t=0g(¯θt,¯λt)−g(θ,¯λt)/bracketrightigg
. (38)
Thanks to Lemmas (A.8) and (A.9) proved below, the two summands can be bounded to obtain
E/bracketleftbigg
Φ(θo)−min
θ∈ΘΦ(θ)/bracketrightbigg
≤Dθ
2ηθT+ηθ
2/parenleftbigg
G2
θ+√
48DλLGθ
c/parenrightbigg
+ 12η2
θLG2
θ
c2
+Dλ
2ηλT+ηλ
2/parenleftbigg
G2
λ+ 4DθLGλ
δ/parenrightbigg
+ 4η2
λLG2
λ
ρ2. (39)
Settingηλ=ηθ=1√
T, the final result is obtained. □
Lemma A.8. ForT >0and anyθ, the sequence{¯θt,¯λt}T
t=0generated by Algorithm 1 satisfies
E/bracketleftigg
1
TT−1/summationdisplay
t=0g(¯θt,¯λt)−g(θ,¯λt)/bracketrightigg
≤Dθ
2ηθT+ηθ
2G2
θ+ 12η2
θLG2
θ
c2+ 2ηλDθLGλ
ρ(40)
whereDθ= maxt=0...,TE/vextenddouble/vextenddouble¯θt−θ/vextenddouble/vextenddouble.
Proof:From the update rule of the primal variable and the assumptions 3.4 on the stochastic gradient we
have, that for any θ
Eξt/vextenddouble/vextenddouble¯θt+1−θ/vextenddouble/vextenddouble2=Eξt/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble¯θt−θ−ηθ
mm/summationdisplay
i=1˜∇θgi(θt
i,λt
i)/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
(41)
=/vextenddouble/vextenddouble¯θt−θ/vextenddouble/vextenddouble2−2ηθ
mm/summationdisplay
i=1⟨¯θt−θ;Eξt/bracketleftbig˜∇θgi(θt
i,λt
i)/bracketrightbig
⟩+Eξt/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoubleηθ
mm/summationdisplay
i=1˜∇θgi(θt
i,λt
i)/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
(42)
18Published in Transactions on Machine Learning Research (12/2022)
≤/vextenddouble/vextenddouble¯θt−θ/vextenddouble/vextenddouble2−2ηθ
mm/summationdisplay
i=1⟨¯θt−θ;∇θgi(θt
i,λt
i)⟩
/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
:=T2+η2
θG2
θ. (43)
Denoting with Dt
θ=/vextenddouble/vextenddouble¯θt−θ/vextenddouble/vextenddoublewe have that for T2the following holds
T2=−2ηθ
m/parenleftiggm/summationdisplay
i=1⟨¯θt−θ;∇θgi(θt
i,¯λt)⟩+m/summationdisplay
i=1⟨¯θt−θ;∇θgi(θt
i,λt
i)−∇θgi(θt
i,¯λt)⟩/parenrightigg
(44)
≤−2ηθ
mm/summationdisplay
i=1⟨¯θt−θ;∇θgi(θt
i,¯λt)⟩+ 2ηθLDt
θ/radicalbigg
Ξt
λ
m(45)
≤−2ηθ
mm/summationdisplay
i=1/parenleftbig
⟨¯θt−θt
i;∇θgi(θt
i,¯λt)⟩+⟨θt
i−θ;∇θgi(θt
i,¯λt)⟩/parenrightbig
+ 2ηθLDt
θ/radicalbigg
Ξt
λ
m(46)
(15)
≤−2ηθ
mm/summationdisplay
i=1/parenleftbigg
gi(¯θt,¯λt)−gi(θ,¯λt)−L
2/vextenddouble/vextenddouble¯θt−θt
i/vextenddouble/vextenddouble2/parenrightbigg
+ 2ηθLDt
θ/radicalbigg
Ξt
λ
m(47)
=−2ηθ
mm/summationdisplay
i=1/parenleftbig
gi(¯θt,¯λt)−gi(θ,¯λt)/parenrightbig
+2ηθL
mΞt
θ+ 2ηθLDt
θ/radicalbigg
Ξt
λ
m. (48)
Plugging it back in (43), rearranging the terms and taking the expectation over the previous iterate we get
E/bracketleftbig
g(¯θt,¯λt)−g(θ,¯λt)/bracketrightbig
=1
mE/bracketleftiggm/summationdisplay
i=1gi(¯θt,¯λt)−gi(θ,¯λt)/bracketrightigg
(49)
≤E∥¯θt−θ∥2−E∥¯θt+1−θ∥2
2ηθ+ηθ
2G2
θ+L
mE/bracketleftbig
Ξt
θ/bracketrightbig
+LE/bracketleftbig
Dt
θ/bracketrightbig/radicalbigg
E[Ξt
λ]
m.(50)
Telescoping from t= 0tot=T−1and plugging the consensus inequalities (30) and (31), we get
1
TE/bracketleftiggT−1/summationdisplay
t=0g(¯θt,¯λt)−g(θ,¯λt)/bracketrightigg
≤Dθ
2ηθT+ηθ
2G2
θ+ 12η2
θLG2
θ
c2+ 2ηλDθLGλ
ρ(51)
whereDθ= maxt=0...,TE[Dt
θ] = maxt=0...,TE/vextenddouble/vextenddouble¯θt−θ/vextenddouble/vextenddouble. □
Lemma A.9. ForT >0and anyλ, the sequence{¯θt,¯λt}T
t=0generated by Algorithm 1 satisfies
E/bracketleftigg
1
TT−1/summationdisplay
t=0g(¯θt,λ)−g(¯θt,¯λt)/bracketrightigg
≤Dλ
2ηλT+ηλ
2G2
λ+ 4η2
λLG2
λ
ρ2+√
12ηθDλLGθ
c(52)
whereDλ= maxt=0...,TE/vextenddouble/vextenddouble¯λt−λ/vextenddouble/vextenddouble.
ProofThe proof follows similarly as in Lemma (A.8)
Eξt/vextenddouble/vextenddouble¯λt+1−λ/vextenddouble/vextenddouble2=Eξt/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoubleλ−¯λt+ηλ
mm/summationdisplay
i=1˜∇λgi(θt
i,λt
i)/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
(53)
=/vextenddouble/vextenddouble¯λt−λ/vextenddouble/vextenddouble2−2ηλ
mm/summationdisplay
i=1⟨λ−¯λt;Eξt/bracketleftbig˜∇λgi(θt
i,λt
i)/bracketrightbig
⟩+Eξt/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoubleηλ
mm/summationdisplay
i=1˜∇λgi(θt
i,λt
i)/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
(54)
=E/vextenddouble/vextenddouble¯λt−λ/vextenddouble/vextenddouble2−2ηλ
mm/summationdisplay
i=1⟨λ−¯λt;∇λgi(θt
i,λt
i)⟩
/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
:=T3+η2
λG2
λ. (55)
19Published in Transactions on Machine Learning Research (12/2022)
Denoting with Dt
λ=/vextenddouble/vextenddouble¯λt−λ/vextenddouble/vextenddoublewe have that for T3the following holds
T3=−2ηλ
m/parenleftiggm/summationdisplay
i=1⟨λ−¯λt;∇λgi(¯θt,λt
i)⟩+m/summationdisplay
i=1⟨λ−¯λt;∇λgi(θt
i,λt
i)−∇λgi(¯θt,λt
i)⟩/parenrightigg
(56)
≤−2ηλ
mm/summationdisplay
i=1/parenleftbig
⟨λ−¯λt;∇λgi(¯θt,λt
i)⟩/parenrightbig
+ 2ηλLDt
λ/radicalbigg
Ξt
θ
m(57)
≤−2ηλ
mm/summationdisplay
i=1/parenleftbig
⟨λ−λt
i;∇λgi(¯θt,λt
i)⟩+⟨λt
i−¯λt;∇λgi(¯θt,λt
i)⟩/parenrightbig
+ 2ηλLDt
λ/radicalbigg
Ξt
θ
m(58)
(15)
≤−2ηλ
mm/summationdisplay
i=1/parenleftbigg
gi(¯θt,λ)−gi(¯θt,¯λt)−L
2/vextenddouble/vextenddouble¯λt−λt
i/vextenddouble/vextenddouble2/parenrightbigg
+ 2ηλLDt
λ/radicalbigg
Ξt
θ
m(59)
=−2ηλ
mm/summationdisplay
i=1/parenleftbig
gi(¯θt,λ)−gi(¯θt,¯λt)/parenrightbig
+2ηλL
mΞt
λ+ 2ηλLDt
λ/radicalbigg
Ξt
θ
m. (60)
Plugging it back in (55), rearranging the terms and taking the expectation over the previous iterate we get
E/bracketleftbig
g(¯θt,λ)−g(¯θt,¯λt)/bracketrightbig
=1
mE/bracketleftiggm/summationdisplay
i=1gi(¯θt,λ)−gi(¯θt,¯λt)/bracketrightigg
(61)
≤E∥¯λt−λ∥2−E∥¯λt+1−λ∥2
2ηλ+ηλ
2G2
λ+L
mE/bracketleftbig
Ξt
λ/bracketrightbig
+LE/bracketleftbig
Dt
λ/bracketrightbig/radicalbigg
E[Ξt
θ]
m.(62)
Telescoping from t= 0tot=T−1and plugging the consensus inequalities (30) and (31) we get
1
TE/bracketleftiggT−1/summationdisplay
t=0g(¯θt,λ)−g(¯θt,¯λt)/bracketrightigg
≤Dλ
2ηλT+ηλ
2G2
λ+ 4η2
λLG2
λ
ρ2+√
12ηθDλLGθ
c(63)
whereDλ= maxt=0...,TE[Dt
λ] = maxt=0...,TE/vextenddouble/vextenddouble¯λt−λ/vextenddouble/vextenddouble. □
A.3 Proof of Theorem 4.3: Non-convex case
In the case of non-convex functions {fi}m
i=1, Theorem 4.3 provides the following ϵ-stationarity guarantee on
the randomized solution of Algorithm 1 :
1
TT/summationdisplay
t=1E/bracketleftig/vextenddouble/vextenddouble∇Φ(¯θt−1)/vextenddouble/vextenddouble2/bracketrightig
≤2L√
T/parenleftbigg
256/parenleftbig
E[Φ(¯θ0)]−E[Φ(¯θT)]/parenrightbig
+45Lκ2D0
λ
2/parenrightbigg
+1√
T/parenleftbigg
5DλLGθ
c+σ2
θ
2m+45κσ2
λ
4m/parenrightbigg
+1
T/parenleftbiggG2
θ
4c2+ 171κG2
λ
ρ2/parenrightbigg
+σ2
θ
m.(64)
The proof is inspired from recent results in (Lin et al., 2020a). Specifically, Lemma A.10, stated and proved
below, provides a descent inequality of the type
E[Φ(¯θt)]≤E[Φ(¯θt−1)] +η2
θκLσ2
θ
m−/parenleftigηθ
2−2η2
θκL/parenrightig
E/bracketleftbig
∇∥Φ(¯θt−1)∥2/bracketrightbig
+L2/parenleftigηθ
2+ 2η2
θκL/parenrightig/parenleftbiggE[Ξt
θ]
m+2E[Ξt
λ]
m+ 2E[δt
λ]/parenrightbigg
. (65)
Settingηθ=ηλ
16(κ+1)2andηλ≤1
2Lexpression (65) can be simplified thanks to the following chain of
inequalities
7ηθ
16≤ηθ(1
2−2ηθκL)≤ηθ(1
2+ 2ηθκL)≤9ηθ
16. (66)
20Published in Transactions on Machine Learning Research (12/2022)
Telescoping the simplified expression from t= 1toTwe obtain
E[Φ(¯θT)]≤E[Φ(¯θ0)] +Tη2
θκLσ2
θ
m−7ηθ
16T/summationdisplay
t=1E/bracketleftbig
∇∥Φ(¯θt−1)∥2/bracketrightbig
+L29ηθ
16T/summationdisplay
t=1/parenleftbiggE[Ξt
θ]
m+2E[Ξt
λ]
m/parenrightbigg
+9ηθL2
8E/bracketleftiggT/summationdisplay
t=1δt
λ/bracketrightigg
(67)
whereδt
λ:=/vextenddouble/vextenddoubleλ∗(¯θt)−¯λt/vextenddouble/vextenddouble2represents the squared distance between the optimal value of the dual variable
for the current averaged network belief and the current averaged value of the dual variable.
Lemma A.11, reported below, provides a bound on/summationtextT
t=1δt
λthat plugged in (67) yields
E[Φ(¯θT)]≤E[Φ(¯θ0)] +ηθ45Lκ2δ0
λ
8ηλ+ηθ/parenleftbigg45κ4η2
θ
η2
λ−7
16/parenrightbiggT/summationdisplay
t=1E/bracketleftig
∇/vextenddouble/vextenddoubleΦ(¯θt−1)/vextenddouble/vextenddouble2/bracketrightig
+Tηθ/parenleftbiggηθκLσ2
θ
m+45κLηλσ2
λ
4m+45σ2
θ
2·162m/parenrightbigg
+L29ηθ
16T/summationdisplay
t=1/parenleftbiggE[Ξt
θ]
m+2E[Ξt]λ
m+30κE[Ξt−1
θ]
m+70κE[Ξt−1
λ]
m/parenrightbigg
+L29ηθ
16T/summationdisplay
t=1/parenleftigg
40κDt−1
λ/radicalbigg
1
mE[Ξt−1
θ]/parenrightigg
. (68)
Moreover, the relation between the two step-sizes established above ensures that
/parenleftbigg45κ4η2
θ
η2
λ−7
16/parenrightbigg
≤−1
4(69)
and therefore rearranging terms, dividing by4
Tηθand recalling that κ≥1
1
TT/summationdisplay
t=1E/bracketleftig
∇/vextenddouble/vextenddoubleΦ(¯θt−1)/vextenddouble/vextenddouble2/bracketrightig
≤4
ηθT/parenleftbig
E[Φ(¯θ0)]−E[Φ(¯θT)]/parenrightbig
+45Lκ2δ0
λ
2Tηλ
+ 4/parenleftbiggηθκLσ2
θ
m+45κLηλσ2
λ
4m+45σ2
θ
2·162m+/parenrightbigg
+9L2
4TT/summationdisplay
t=1/parenleftbigg31κE[Ξt−1
θ]
m+72E[κΞt−1
λ]
m+31κE[Ξt−1
θ]
m+72E[κΞt−1
λ]
m/parenrightbigg
.(70)
Exploiting consensus inequalities (30), (31) and the fact that κ≥1andηθ=ηλ
16(κ+1)2≤1/2Lwe can simplify
and obtain
1
TT/summationdisplay
t=1E/bracketleftig
∇/vextenddouble/vextenddoubleΦ(¯θt−1)/vextenddouble/vextenddouble2/bracketrightig
≤64(κ+ 1)2
ηλT/parenleftbig
E[Φ(¯θ0)]−E[Φ(¯θT)]/parenrightbig
+45Lκ2δ0
λ
2Tηλ
+ 2/parenleftbigg
ηλLσ2
θ
m+ηλ45κLσ2
λ
2m+45σ2
θ
162m/parenrightbigg
+L29
4TT/summationdisplay
t=1/parenleftbigg
40κDt−1
λ√
12ηθGθ
c+ 372η2
θκG2
θ
c2+ 288η2
λκG2
λ
ρ2/parenrightbigg
.(71)
Simplifying and defining Dλ= maxt=0,...,TDt
λ
1
TT/summationdisplay
t=1E/bracketleftig
∇/vextenddouble/vextenddoubleΦ(¯θt−1)/vextenddouble/vextenddouble2/bracketrightig
≤64(κ+ 1)2
ηλT/parenleftbig
E[Φ(¯θ0)]−E[Φ(¯θT)]/parenrightbig
+45Lκ2δ0
λ
2Tηλ
21Published in Transactions on Machine Learning Research (12/2022)
+ 2/parenleftbigg
ηλLσ2
θ
m+ηλ45κLσ2
λ
2m+45σ2
θ
162m/parenrightbigg
+L2/parenleftbigg
10DληλGθ
c+η2
λG2
θ
c2+ 684η2
λκG2
λ
ρ2/parenrightbigg
. (72)
Grouping
1
TT/summationdisplay
t=1E/bracketleftig
∇/vextenddouble/vextenddoubleΦ(¯θt−1)/vextenddouble/vextenddouble2/bracketrightig
≤1
ηλT/parenleftbigg
256/parenleftbig
E[Φ(¯θ0)]−E[Φ(¯θT)]/parenrightbig
+45Lκ2δ0
λ
2/parenrightbigg
+ηλ/parenleftbigg
10DλL2Gθ
c+Lσ2
θ
m+45κLσ2
λ
2m/parenrightbigg
+η2
λ/parenleftbiggL2G2
θ
c2+ 684L2κG2
λ
ρ2/parenrightbigg
+σ2
θ
m.(73)
Settingηλ=1
2L√
Twe get
1
TT/summationdisplay
t=1E/bracketleftig
∇/vextenddouble/vextenddoubleΦ(¯θt−1)/vextenddouble/vextenddouble2/bracketrightig
≤2L√
T/parenleftbigg
256/parenleftbig
E[Φ(¯θ0)]−E[Φ(¯θT)]/parenrightbig
+45Lκ2δ0
λ
2/parenrightbigg
+1√
T/parenleftbigg
5DλLGθ
c+σ2
θ
2m+45κσ2
λ
4m/parenrightbigg
+1
T/parenleftbiggG2
θ
4c2+ 171κG2
λ
ρ2/parenrightbigg
+σ2
θ
m.(74)
Lemma A.10. For eacht= 1,...,Tthe iterates generated by Algorithm 1 satisfies
E[Φ(¯θt)]≤E[Φ(¯θt−1)] +η2
θκLσ2
θ
m−/parenleftigηθ
2−2η2
θκL/parenrightig
E/bracketleftbig
∇∥Φ(¯θt−1)∥2/bracketrightbig
+L2/parenleftigηθ
2+ 2η2
θκL/parenrightig/parenleftbiggE[Ξt
θ]
m+2E[Ξt
λ]
m+ 2E[δt
λ]/parenrightbigg
. (75)
Proof:From the 2κL-smoothness of Φ(·)(Lemma 4.3 of Lin et al. (2020a)) and the update rule we have:
Eξt−1/bracketleftbig
Φ(¯θt)/bracketrightbig
≤Φ(¯θt−1) +Eξt−1/bracketleftbig
⟨∇θΦ(¯θt−1),¯θt−¯θt−1⟩/bracketrightbig
+κLEξt−1/vextenddouble/vextenddouble¯θt−¯θt−1/vextenddouble/vextenddouble2(76)
≤Φ(¯θt−1)−ηθ⟨∇θΦ(¯θt−1),1
mm/summationdisplay
i=1Eξt−1/bracketleftbig˜∇θgi(θt−1
i,λt−1
i)/bracketrightbig
⟩
+η2
θκL
m2Eξt−1/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoublem/summationdisplay
i=1˜∇θgi(θt−1
i,λt−1
i)/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
(77)
≤Φ(¯θt−1) +ηθ⟨∇Φ(¯θt−1),∇Φ(¯θt−1)−1
mm/summationdisplay
i=1∇θgi(θt−1
i,λt−1
i)⟩
/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
:=T4
−ηθ∇∥Φ(¯θt−1)∥2+η2
θκL
m2Eξt/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoublem/summationdisplay
i=1˜∇θgi(θt−1
i,λt−1
i)/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
/bracehtipupleft/bracehtipdownright/bracehtipdownleft /bracehtipupright
:=T5. (78)
We now turn bounding term T4
T4=ηθ⟨∇Φ(¯θt−1),∇Φ(¯θt−1)−1
mm/summationdisplay
i=1∇θgi(θt−1
i,λt−1
i)⟩ (79)
(19)
≤ηθ
2
/vextenddouble/vextenddouble∇Φ(¯θt−1)/vextenddouble/vextenddouble2+/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble∇Φ(¯θt−1)−1
mm/summationdisplay
i=1∇θgi(θt−1
i,λt−1
i)/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
 (80)
≤ηθ
2
/vextenddouble/vextenddouble∇Φ(¯θt−1)/vextenddouble/vextenddouble2+/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble1
mm/summationdisplay
i=1∇θgi(¯θt−1,λ∗(¯θt−1))−∇θgi(θt−1
i,λt−1
i)/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
 (81)
22Published in Transactions on Machine Learning Research (12/2022)
(12)
≤ηθ
2/parenleftigg
/vextenddouble/vextenddouble∇Φ(¯θt−1)/vextenddouble/vextenddouble2+L2
mm/summationdisplay
i=1/vextenddouble/vextenddouble¯θt−1−θt−1
i/vextenddouble/vextenddouble2+L2
mm/summationdisplay
i=1/vextenddouble/vextenddoubleλ∗(¯θt−1)−λt−1
i)/vextenddouble/vextenddouble2/parenrightigg
(82)
(20)
≤ηθ
2
/vextenddouble/vextenddouble∇Φ(¯θt−1)/vextenddouble/vextenddouble2+L2Ξt−1
θ
m+2L2Ξt−1
λ
m+2L2
mm/summationdisplay
i=1/vextenddouble/vextenddoubleλ∗(¯θt−1)−¯λt−1/vextenddouble/vextenddouble2
/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
=δt−1
λ
(83)
(84)
and from stochastic gradient assumptions 3.4 we can bound T5as follows
T5=Eξt−1/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoublem/summationdisplay
i=1˜∇θgi(θt−1
i,λt−1
i)/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
(85)
=Eξt−1
/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoublem/summationdisplay
i=1/parenleftbig˜∇θgi(θt−1
i,λt−1
i)−∇θgi(θt−1
i,λt−1
i)/parenrightbig/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
+/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoublem/summationdisplay
i=1∇θgi(θt−1
i,λt−1
i)/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
(86)
(20)
≤mσ2
θ+ 2/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoublem/summationdisplay
i=1∇θgi(θt−1
i,λt−1
i)−∇θgi(¯θt−1,λ∗(¯θt−1))/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
+ 2/vextenddouble/vextenddoublem∇Φ(¯θt−1)/vextenddouble/vextenddouble2(87)
(12)
≤mσ2
θ+ 2L2mm/summationdisplay
i=1/vextenddouble/vextenddouble¯θt−1−θt−1
i/vextenddouble/vextenddouble2+ 2L2mm/summationdisplay
i=1/vextenddouble/vextenddoubleλ∗(¯θt−1)−λt−1
i/vextenddouble/vextenddouble2+ 2m2/vextenddouble/vextenddouble∇Φ(¯θt−1)/vextenddouble/vextenddouble2(88)
≤mσ2
θ+ 2L2mΞt−1
θ+ 4L2mΞt−1
λ+ 4L2mm/summationdisplay
i=1/vextenddouble/vextenddoubleλ∗(¯θt−1)−¯λt−1/vextenddouble/vextenddouble2
/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
=δt−1
λ+2m2/vextenddouble/vextenddouble∇Φ(¯θt−1)/vextenddouble/vextenddouble2.(89)
Recombining, grouping, and taking the expectation over the previous iterates we get the desired result. □
Lemma A.11. The sequence of{δt
λ}T
t=1generated by Algorithm 1 satisfies
T/summationdisplay
t=1E/bracketleftbig
δt
λ/bracketrightbig
≤5δ0
λκ
ηλµ+T/summationdisplay
t=15κ/parenleftigg
4Dt−1
λ/radicalbigg
1
mE/bracketleftbig
Ξt−1
θ/bracketrightbig
+3E/bracketleftbig
Ξt−1
θ/bracketrightbig
m+7E/bracketleftbig
Ξt−1
λ/bracketrightbig
m/parenrightigg
+T/summationdisplay
t=15/parenleftbigg8κ2η2
θ
η2
λµ2E/bracketleftig/vextenddouble/vextenddouble∇Φ(¯θt−1)/vextenddouble/vextenddouble2/bracketrightig/parenrightbigg
+ 5T/parenleftbigg2ηλσ2
λ
mµ+4σ2
θ
162m(κ+ 1)2µ2/parenrightbigg
(90)
whereDt−1
λ=/vextenddouble/vextenddouble¯λt−1−λ/vextenddouble/vextenddouble.
Proof:From (20), for b>0, we have
Eξt−1[δt
λ]≤/parenleftbigg
1 +1
b/parenrightbigg
Eξt−1/vextenddouble/vextenddoubleλ∗(¯θt−1)−¯λt/vextenddouble/vextenddouble2
/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
:=T6+ (1 +b)Eξt−1/vextenddouble/vextenddoubleλ∗(¯θt)−λ∗(¯θt−1)/vextenddouble/vextenddouble2
/bracehtipupleft/bracehtipdownright/bracehtipdownleft /bracehtipupright
:=T7. (91)
BoundingT6similarly
T6=Eξt−1/vextenddouble/vextenddoubleλ∗(¯θt−1)−¯λt/vextenddouble/vextenddouble2(92)
=Eξt−1/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoubleλ∗(¯θt−1)−¯λt−1−ηλ
mm/summationdisplay
i=1/parenleftbig˜∇λgi(θt−1
i,λt−1
i)±∇λgi(θt−1
i,λt−1
i)/parenrightbig/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
(93)
≤/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoubleλ∗(¯θt−1)−¯λt−1−ηλ
mm/summationdisplay
i=1∇λgi(θt−1
i,λt−1
i)/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
+η2
λσ2
λ
m(94)
=/vextenddouble/vextenddoubleλ∗(¯θt−1)−¯λt−1/vextenddouble/vextenddouble2+/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoubleηλ
mm/summationdisplay
i=1∇λgi(θt−1
i,λt−1
i)/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
/bracehtipupleft/bracehtipdownright/bracehtipdownleft /bracehtipupright
T6,1
23Published in Transactions on Machine Learning Research (12/2022)
−2⟨λ∗(¯θt−1)−¯λt−1;ηλ
mm/summationdisplay
i=1∇λgi(θt−1
i,λt−1
i)⟩
/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
T6,2+η2
λσ2
λ
m. (95)
Estimating T6,1
T6,1=2η2
λ/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble1
mm/summationdisplay
i=1∇λgi(θt−1
i,λt−1
i)±∇λgi(¯θt−1,¯λt−1)−∇λgi(¯θt−1,λ∗(¯θt−1))/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
(96)
≤2η2
λ
mm/summationdisplay
i=1/vextenddouble/vextenddouble∇λgi(θt−1
i,λt−1
i)−∇λgi(¯θt−1,¯λt−1)/vextenddouble/vextenddouble2
+ 2η2
λ/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble1
mm/summationdisplay
i=1∇λgi(¯θt−1,¯λt−1)−∇λgi(¯θt−1,λ∗(¯θt−1))/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
(97)
(12,16)
≤2η2
λ
mm/summationdisplay
i=1L2/vextenddouble/vextenddoubleλt−1
i−¯λt−1/vextenddouble/vextenddouble2+L2/vextenddouble/vextenddoubleθt−1
i−¯θt−1/vextenddouble/vextenddouble2+4η2
λL
mm/summationdisplay
i=1/bracketleftbig
gi(¯θt−1,λ∗(¯θt−1))−gi(¯θt−1,¯λt−1)/bracketrightbig
(98)
=2η2
λL2
mΞt−1
λ+2η2
λL2
mΞt−1
θ+4η2
λL
mm/summationdisplay
i=1/bracketleftbig
gi(¯θt−1,λ∗(θt−1
i))−gi(¯θt−1,¯λt−1)/bracketrightbig
. (99)
Estimating T6,2
T6,2=−2ηλ
mm/summationdisplay
i=1⟨λ∗(¯θt−1)−¯λt−1;∇λgi(θt−1
i,λt−1
i)⟩ (100)
=−2ηλ
mm/summationdisplay
i=1⟨λ∗(¯θt−1)−¯λt−1;∇λgi(θt−1
i,λt−1
i)±∇λgi(¯θt−1,λt−1
i)⟩ (101)
=−2ηλ
mm/summationdisplay
i=1⟨λ∗(¯θt−1)−¯λt−1;∇λgi(¯θt−1,λt−1
i⟩
+ 2ηλ
mm/summationdisplay
i=1⟨¯λt−1−λ∗(¯θt−1);∇λgi(θt−1
i,λt−1
i)−∇λgi(¯θt−1,λt−1
i)⟩ (102)
≤−2ηλ
mm/summationdisplay
i=1⟨λ∗(¯θt−1)−¯λt−1;∇λgi(¯xt−1,λt−1
i⟩+ 2ηλLDt−1
λ/radicalbigg
1
mΞt−1
θ(103)
=−2ηλ
mm/summationdisplay
i=1⟨λ∗(¯θt−1)−λt−1
i;∇λgi(¯θt−1,λt−1
i)⟩+⟨λt−1
i−¯λt−1);∇λgi(¯θt−1,λt−1
i)⟩
+ 2ηλLDt−1
λ/radicalbigg
1
mΞt−1
θ(104)
(15,18)
≤2ηλ
mm/summationdisplay
i=1/parenleftbig
gi(¯θt−1,¯λt−1)−gi(θt−1
i,λ∗(¯θt−1))/parenrightbig
+ 2ηλLDt−1
λ/radicalbigg
1
mΞt−1
θ
−2ηλ
mm/summationdisplay
i=1/parenleftbiggµ
2/vextenddouble/vextenddoubleλ∗(¯θt−1)−λt−1
i/vextenddouble/vextenddouble2+L
2/vextenddouble/vextenddouble¯λt−1−λt−1
i/vextenddouble/vextenddouble2/parenrightbigg
(105)
(20)
≤2ηλ
mm/summationdisplay
i=1/parenleftbig
gi(¯θt−1,¯λt−1)−gi(¯θt−1,λ∗(¯θt−1))/parenrightbig
+ 2ηλLDt−1
λ/radicalbigg
1
mΞt−1
θ
−2ηλ
mm/summationdisplay
i=1/parenleftbiggµ
4/vextenddouble/vextenddoubleλ∗(¯θt−1)−¯λt−1/vextenddouble/vextenddouble2−L+µ
2/vextenddouble/vextenddouble¯λt−1−λt−1
i/vextenddouble/vextenddouble2/parenrightbigg
(106)
24Published in Transactions on Machine Learning Research (12/2022)
=−µηλ
2/vextenddouble/vextenddoubleλ∗(¯θt−1)−¯λt−1/vextenddouble/vextenddouble2−2ηλ
mm/summationdisplay
i=1gi(¯θt−1,λ∗(¯θt−1))−gi(¯θt−1,¯λt−1)
+2Lηλ
mΞt−1
λ+ 2ηλLDt−1
λ/radicalbigg
1
mΞt−1
θ(107)
where the last inequality follows from choosing ηλ≤1/(2L). Substituting the expressions we get
T6=/parenleftig
1−µηλ
2/parenrightig/vextenddouble/vextenddoubleλ∗(¯θt−1)−¯λt−1/vextenddouble/vextenddouble2+η2
λσ2
λ
m+Lηλ
mΞt−1
θ+3Lηλ
mΞt−1
λ+ 2ηλLDt−1
λ/radicalbigg
1
mΞt−1
θ.(108)
Beingλ∗(·)isκ-smooth (Lemma 4.3 (Lin et al., 2020a)) we can bound T7as follows
T7=Eξt−1/vextenddouble/vextenddoubleλ∗(¯θt)−λ∗(¯θt−1)/vextenddouble/vextenddouble2(109)
≤κ2Eξt−1/vextenddouble/vextenddouble¯θt−¯θt−1/vextenddouble/vextenddouble2(110)
=κ2η2
θ
m2Eξt−1/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoublem/summationdisplay
i=1˜∇θgi(θt−1
i,λt−1
i)/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
(111)
=κ2η2
θ
m2
mσ2
θ+/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoublem/summationdisplay
i=1∇θgi(θt−1
i,λt−1
i)±m∇Φ(¯θt−1)/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
 (112)
(20)
≤κ2η2
θ
m2/parenleftigg
mσ2
θ+ 2/vextenddouble/vextenddoublem∇Φ(¯θt−1)/vextenddouble/vextenddouble2+ 2L2mm/summationdisplay
i=1/vextenddouble/vextenddouble¯θt−1−θt−1
i/vextenddouble/vextenddouble2+ 2L2mm/summationdisplay
i=1/vextenddouble/vextenddoubleλ∗(¯θt−1)−λt−1
i/vextenddouble/vextenddouble2/parenrightigg
(113)
=κ2η2
θ
m2/parenleftigg
mσ2
θ+ 2m2/vextenddouble/vextenddouble∇Φ(¯θt−1)/vextenddouble/vextenddouble2+ 2L2mΞt−1
θ+ 2L2mm/summationdisplay
i=1/vextenddouble/vextenddoubleλ∗(¯θt−1)−¯λt−1+¯λt−1−λt−1
i/vextenddouble/vextenddouble2/parenrightigg
(114)
(20)
≤κη2
θ/parenleftbiggσ2
θ
m+ 2/vextenddouble/vextenddouble∇Φ(¯θt−1)/vextenddouble/vextenddouble2+2L2Ξt−1
θ
m+4L2Ξt−1
λ
m+ 4L2/vextenddouble/vextenddoubleλ∗(¯θt−1)−¯λt−1/vextenddouble/vextenddouble2/parenrightbigg
(115)
=κ2η2
θ/parenleftbiggσ2
θ
m+ 2/vextenddouble/vextenddouble∇Φ(¯θt−1)/vextenddouble/vextenddouble2+2L2Ξt−1
θ
m+4L2Ξt−1
λ
m+ 4L2δt−1
λ/parenrightbigg
. (116)
Recombining and grouping we get
δt
λ≤/parenleftbigg/parenleftbigg
1 +1
b/parenrightbigg/parenleftig
1−µηλ
2/parenrightig
+ 4(1 +b)κ2η2
θL2/parenrightbigg
δt−1
λ+ 2/parenleftbigg
1 +1
b/parenrightbigg
ηλLDt−1
λ/radicalbigg
1
mΞt−1
θ
+/parenleftbigg/parenleftbigg
1 +1
b/parenrightbigg
Lηλ+ 2(1 +b)κ2η2
θL2/parenrightbiggΞt−1
θ
m+/parenleftbigg
1 +1
b/parenrightbiggη2
λσ2
λ
m+ (1 +b)κ2η2
θσ2
θ
m
+/parenleftbigg/parenleftbigg
1 +1
b/parenrightbigg
3Lηλ+ 4(1 +b)κ2η2
θL2/parenrightbiggΞt−1
λ
m+ 2κ2η2
θ(1 +b)/vextenddouble/vextenddouble∇Φ(¯θt−1)/vextenddouble/vextenddouble2. (117)
Settingb= 2/parenleftig
2
ηλµ−1/parenrightig
>0we get the following inequalities
/parenleftbigg
1 +1
b/parenrightbigg/parenleftig
1−ηλµ
2/parenrightig
≤/parenleftig
1−ηλµ
4/parenrightig
(118)
(1 +b)≤4
ηλµ(119)
/parenleftbigg
1 +1
b/parenrightbigg
≤2 (120)
(121)
that allows to simplify (117) as follows
δt
λ≤/parenleftbigg
1−ηλµ
4+16κ2η2
θL2
ηλµ/parenrightbigg
δt−1
λ+ 4ηλLDt−1
λ/radicalbigg
1
mΞt−1
θ+/parenleftbigg
2Lηλ+8κ2η2
θL2
ηλµ/parenrightbiggΞt−1
θ
m
25Published in Transactions on Machine Learning Research (12/2022)
+ 2η2
λσ2
λ
m+4κ2η2
θσ2
θ
mηλµ+/parenleftbigg
6Lηλ+16κ2η2
θL2
ηλµ/parenrightbiggΞt−1
λ
m+8κ2η2
θ
ηλµ/vextenddouble/vextenddouble∇Φ(¯xt−1)/vextenddouble/vextenddouble2. (122)
Fixingηx=ηλ
16(κ+1)2we get that
ν= 1−ηλµ
4+16κ2η2
xL2
ηλµ≤/parenleftig
1−ηλµ
5/parenrightig
. (123)
Taking the expectation over the current iterate and applying recursively the inequality we obtain
Eξt−1[δt
λ]≤νtδ0
λ+t−1/summationdisplay
i=0νt−1−i/parenleftbigg8κ2η2
θ
ηλµEξt−1[/vextenddouble/vextenddouble∇Φ(¯θt−1)/vextenddouble/vextenddouble2] + 2η2
λσ2
λ
m+4κ2η2
θ
ηλµσ2
θ
m/parenrightbigg
+t−1/summationdisplay
i=0νt−1−i/parenleftigg
4ηλLDt−1
λ/radicalbigg
1
mEξt−1[Ξt−1
θ]/parenrightigg
+t−1/summationdisplay
i=0νt−1−i/parenleftigg
3LηλEξt−1[Ξt−1
θ]
m+7LηλEξt−1[Ξt−1
λ]
m/parenrightigg
. (124)
Summing from t= 1toTand from (123) we get
T/summationdisplay
t=1Eξt−1[δt
λ]≤5δ0
λ
ηλµ+T/summationdisplay
t=15
ηλµ/parenleftbigg8κ2η2
θ
ηλµEξt−1[/vextenddouble/vextenddouble∇Φ(¯θt−1)/vextenddouble/vextenddouble2/parenrightbigg
+5T
ηλµ/parenleftbigg
2η2
λσ2
λ
m+4κ2η2
θ
ηλµσ2
θ
m/parenrightbigg
+T/summationdisplay
t=15
ηλµ/parenleftigg
4ηλLDt−1
λ/radicalbigg
1
mEξt−1[Ξt−1
θ]/parenrightigg
+T/summationdisplay
t=15
ηλµ/parenleftigg
3LηλEξt−1[Ξt−1
θ]
m+7LηλEξt−1[Ξt−1
λ]
m/parenrightigg
. (125)
□
26