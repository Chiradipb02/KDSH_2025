Published in Transactions on Machine Learning Research (02/2023)
Transfer Entropy Bottleneck:
Learning Sequence to Sequence Information Transfer
Damjan Kalajdzievski1,2,∗damjank7354@gmail.com
Ximeng Mao1,3,∗ximeng.mao@mila.quebec
Pascal Fortier-Poisson4pascal@bios.health
Guillaume Lajoie1,5,8,†g.lajoie@umontreal.ca
Blake A. Richards1,2,6,7,8,†blake.richards@mila.quebec
1Mila - Quebec AI institute, Montreal, QC, Canada
2Department of Neurology and Neurosurgery, McGill University, Montréal, QC, Canada
3Department of Computer Science and Operations Research, Université de Montréal, Montréal, QC, Canada
4BIOS Health Ltd., Cambridge, UK
5Department of Mathmatics and Statistics, Université de Montréal, Montréal, QC, Canada
6School of Computer Science, McGill University, Montréal, QC, Canada
7Montreal Neurological Institute, McGill University, Montréal, QC, Canada
8CIFAR, Toronto, ON, Canada
∗Equal Contribution
†Equal Advising
Reviewed on OpenReview: https: // openreview. net/ forum? id= kJcwlP7BRs
Abstract
When presented with a data stream of two statistically dependent variables, predicting
the future of one of the variables (the target stream) can benefit from information about
both its history and the history of the other variable (the source stream). For example,
fluctuations in temperature at a weather station can be predicted using both temperatures
and barometric readings. However, a challenge when modelling such data is that it is easy
for a neural network to rely on the greatest joint correlations within the target stream, which
may ignore a crucial but small information transfer from the source to the target stream. As
well, there are often situations where the target stream may have previously been modelled
independently and it would be useful to use that model to inform a new joint model. Here,
we develop an information bottleneck approach for conditional learning on two dependent
streams of data. Our method, which we call Transfer Entropy Bottleneck (TEB), allows
one to learn a model that bottlenecks the directed information transferred from the source
variable to the target variable, while quantifying this information transfer within the model.
As such, TEB provides a useful new information bottleneck approach for modelling two
statistically dependent streams of data in order to make predictions about one of them.
1Published in Transactions on Machine Learning Research (02/2023)
Figure 1: (a): Probabilistic graph representing directed information transfer between stochastic processes.
Xisthesourcestream, Yisthetargetstream. (b): ArchitecturediagramoftheTEBmodelasimplemented.
1 Introduction
Scientists are often presented with two streams of statistically coupled data, a source stream, which provides
information, and a target stream, which they wish to predict the future of (See Figure 1a). Obviously, one
canusethehistoryofanindividualvariabletopredictitsfuturevalues, butatthesametime, itisdesirableto
use the statistical dependency in a joint model, rather than treating each variable as independent. However,
using a typical joint input stream model ignores the directed nature of the information transfer from the
source stream to the target stream. Put another way, one would ideally use a method that can differentiate
the directionality of information in order to better modulate and interpret the role of different input streams
onprediction. Tothisend, we proposeto leverage transferentropy (Schreiber,2000), ameasureofdirected
information flow between stochastic processes, to help with the modelling. Specifically, if we have a source
stream,Xt−ℓ,...,Xt, and a target stream, Yt−ℓ,...,Yt, the transfer entropy from XtoY(with horizon
0<ℓ≤∞) is:
TXt→Y=I(Yt; (Xi)t−ℓ≤i<t|(Yi)t−ℓ≤i<t)
whereI(·;·|·)is the conditional mutual information.
Estimating transfer entropy with a deep learning approach has been extensively explored (Zhang et al., 2019)
(see Appendix C for discussion contrasting with TEB). But, leveraging the transfer entropy for prediction
is often a challenging problem and one that has yet to be resolved for deep neural networks (Schreiber,
2000; Gençağa, 2018). One reason for this difficulty is that the transfer entropy can be small relative to
the overall entropy in the data streams, and as such, many prediction models will ignore it. For example,
in weather forecasting, periodicity of average monthly temperatures could dominate a prediction model of
future temperatures, which would prevent a simple predictive model from detecting important anomalies
indicated in other sources of information such as air pressure, wind, greenhouse gas emission and other
human activities. Indeed, we shall show that a neural network trained on a joint input stream can struggle
to learn or generalize when the statistical coupling between the two processes is strongly correlated, but with
a conditionally small and crucial information transfer from the source stream.
We use an information bottleneck (IB) approach (Tishby et al., 2000) to derive a method, which we term
Transfer Entropy Bottleneck (TEB) , that learns a compressed conditional information representation.
This allows the benefits of IB to be applied to the new domain of double stream conditional processing. As
in other information bottleneck architectures, TEB works by learning a latent representation of the data, Z,
such thatZcontains only the relevant information required to capture the relationship between the random
processesXandY(Tishby et al., 2000). But, in TEB, this is done using information that is conditional on
2Published in Transactions on Machine Learning Research (02/2023)
the history of Y, allowing one to capture both the statistical dependencies over time on Y, as well as the
transfer entropy. As well, we design TEB in a manner that allows one to use a pre-trained predictive model
for the target stream. That is, if one has an existing model that has been trained to predict Ytfrom the
history ofY(i.e. (Yi)t−ℓ≤i<t), TEB allows one to plug this existing model into a new model that includes X
in order to capture the transfer entropy and use it to improve the prediction of Yt. This provides a flexible
solution when one might have reasons to keep a previously trained model fixed. In practice, the reasons
can be the desirable performance and interpretability of a given model, retraining being computationally
expensive and time-consuming, and/or compatibility to other components in a larger system.
More precisely, the contributions of this paper are the following:
•We design a loss function that promotes representations obeying a Conditional Minimum Nec-
essary Information (CMNI) principle, which ensures that the latent representation of the data
contains only the relevant information transferred from the source to the target stream. We use this
loss to develop TEB, an information bottleneck algorithm that learns representations following the
CMNI, such that:
1. The learnt representation captures information embedded in the transfer entropy.
2. The target stream can have its latent representation pre-trained separately from the joint latent
representation, which can provide numerous computational and other practical benefits.
•We introduce three synthetic tasks on dual stream modeling problems. We provide experiments on
these tasks to show that TEB allows one to improve the predictions of the target stream, and that
it is applicable to various data modalities including images and time-series signals.
Altogether, TEB is a mathematically principled new IB method for learning to predict from two streams of
statistically dependent data with a wide array of potential applications.
1.1 Related work
The concept of an IB, as first defined in Tishby et al. (2000), was introduced as a way of quantifying the
“quality” of a compressed representation, Z, of a signal, X, for making inferences about some other signal, Y.
This work defined high quality latent representations as being those that minimize the following Lagrangian:
IB=I(X;Z)−βI(Y;Z). (IB)
In Tishby & Zaslavsky (2015), the IB concept is explored in deep neural networks as a possible approach
for studying the quality of representations in the hidden layers of a network, and as a possible explanation
for generalization. The analysis of an IB so as to understand the learned representations of neural networks
was further studied in Saxe et al. (2019).
Traditionally, the IB literature seeks to find a model with the most compressed representation, Z, that
preserves the relevant information about Y. This approach seeks out a representation where the following
equality holds:
I(X;Z) =I(Y;Z). (MNI)
We use the terminology of Fischer (2020), and say that such a representation captures the Minimum
Necessary Information (MNI) . In the case of IB variable Zminimizing the information about Xrelevant
toY, this is the same as Zbeing a minimal sufficient statistic of XforY. Note that it might not be possible
to exactly reach MNI (Wu et al., 2019; Fischer, 2020), however, it proves to be a valid training objective in
practice (Fischer, 2020).
The previously developed IB methods most relevant to ours, are the Variational Information Bottleneck
(VIB)(Alemi et al., 2017) and the Conditional Entropy Bottleneck (CEB) (Fischer, 2020).
3Published in Transactions on Machine Learning Research (02/2023)
Deep VIB (Alemi et al., 2017) is a method using a latent variable encoder-decoder model to derive a
variationalIBprocedure, withatrainingapproachanalogoustovariationalautoencoders(Kingma&Welling,
2014). They show this procedure can be used to learn an IB representation in neural networks. Put more
precisely, with the computation arranged in a feed-forward structure X→Z→Y, via an encoder q(z|x)
and a decoder d(y|z), VIB optimizes q,dwith a variational bound for the IB loss:
VIB =Ep(x)[DKL(q(z|x)∥p(z))]−βEp(y,z)[log(d(y|z))] (VIB)
wherep(z)is a pre-specified fixed prior. In this loss, the KL divergence term bounds the information that
Zcarries about X:
I(Z;X)≤Ep(x)[DKL(q(z|x)∥p(z))],
and so minimizing this term compresses the representation, while the other term:
Ep(y,z)[log(d(y|z))]≤I(Y;Z)
is maximized to increase the information that Zcarries about Y. Alemi et al. (2017) show that their
method has increased generalization and adversarial robustness compared to other forms of regularization
like dropout (Srivastava et al., 2014), and confidence penalty with label smoothing (Pereyra et al., 2017).
The CEB method (Fischer, 2020), proceeds similarly to VIB, but instead of using the KL divergence to
a prior as a bound for I(Z;X),it minimizes an upper bound of the difference I(Z;X)−I(Y;Z)using a
backwards encoder b. The loss minimized for q,d,bis then:
CEB =Ep(y,x)[DKL(q(z|x)∥b(z|y))]−γEp(y,z)[log(d(y|z))]. (CEB)
TEB shares many conceptual links with both VIB and CEB. However, unlike VIB and CEB, TEB is specif-
ically designed to work with situations where one has twosequences of data, one serving as a source stream
and one as a target stream. Thus, TEB can be thought of as a means to bring similar principles of the IB
and MNI in order to obtain a high quality representation that captures directed information transfer.
In Skatchkovsky et al. (2021), a variational information bottleneck method is derived for directed information
(Kramer, 1998), which is a measure similar to transfer entropy. In contrast to ours, their method uses a
non-conditional prior, and imposes stronger Markov assumptions. In implementation, the prior is a fixed
distribution, they limit their approach to simple spiking network encoders, and only process non-sequential
data. Thus, though related, TEB solves a very different problem. Note that throughout the paper we
use the term “directed information transfer” to denote transfer entropy, not to be confused with “directed
information” above which is a separate measure.
Another related concept to transfer entropy is Granger causality (Granger, 1969), and many previous works
have focused on the relationship between the two (Barnett et al., 2009; Hlavackova-Schindler, 2011). For
example, Barnett et al. (2009) showed that the two concepts are equivalent under Gaussian assumptions.
Originally developed in econometrics, Granger causality has gained attention in other domains, including
neuroscience (Seth et al., 2015) and industrial processes (Lindner et al., 2019). The calculation of Granger
causality is traditionally via linear vector autoregressive models (VAR), which is generally considered cheaper
than transfer entropy. To estimate Granger causality of X to Y in practice, VAR trains two linear regression
models to predict Y with or without X as input, and Granger causality can be calculated as a measure
on how much the latter reduces the error of the former. Throughout the years, many works extended this
traditional approach in terms of both causality analysis from the data and time-series forecasting, for the
linear model (Lozano et al., 2009), as well as non-linear models including kernel regression (Gregorová et al.,
2017), radial basis functions neural network (Wismüller et al., 2021), multilayer perceptrons (Talebi et al.,
2017; Tank et al., 2022) and recurrent neural networks (Tank et al., 2022). In general, these methods aim to
learn the causal relationships among different time-series and / or among different time-lagged variables in
a regression problem, and that is very different from the objective of TEB, and other IB methods described
above, which is to learn a compressed latent representation of the data.
4Published in Transactions on Machine Learning Research (02/2023)
2 Methods
In this section we fully specify TEB for learning representations which compress the transfer entropy via a
bottleneck. Note that conditional mutual information is a special case of transfer entropy, and so TEB is
also an IB method for creating a conditional information bottleneck. We encourage the interested reader to
read the Appendix A for a thorough exposition of the mathematical details and proofs.
Our situation is analogous to a latent variable encoder-decoder model, but with a conditional structure; We
wish to learn a latent encoding Ztof(Xi)t−ℓ≤i<tvia a map q, which is conditional on (Yi)t−ℓ≤i<t, from
which one is able to decode Ytconditionally on (Yi)t−ℓ≤i<twith the decoder d. Put another way, we want
to be able to predict Yat timetusing the values of YandXup to timet−1, but using our latent encoding
ofXconditioned on the history of Y. Namely, denoting Z=Zt, X = (Xi)t−ℓ≤i<t,Y= (Yi)t−ℓ≤i<t, and
Y′=Yt,, we haveZ,p,qsuch that:
p(y′,z,x,y ) =p(y′,x,y)p(z|x,y) =p(y′,x,y)q(z|x,y), (1)
as illustrated in Figure 2a. Note that q(z|x,y)is actually the true distribution p(z|x,y), since we artificially
introduced the variable Zas defined by q. Note that Equation 1 is equivalent to the assumption on the
conditional independence Y′⊥Z|X,Y.
Furthermore, we require the distribution to be learned by a feed-forward encoder-decoder structure, which
assumes the following decomposition of joint probability distribution (as illustrated in Figure 2b), where
d(y′|z,y)is a variational approximation to p(y′|z,y):
p(y′,z,x,y ) =p(x,y)p(y′|z,y)p(z|x,y)≈p(x,y)d(y′|z,y)q(z|x,y). (2)
The first part of Equation 2 is equivalent to the conditional independence Y′⊥X|Z,Y.
ZY
XY′
(a)Z YY′
X
(b)ZCY
XY′
(c)Z C YY′
X
(d)
Figure 2: Probabilistic graph representing the random variables and their relationships as assumed by
TEB.(a): The actual random variables in the dataset. Independencies include Y′⊥Z|X,Y.(b): The
random variables as computed by our feed-forward computation. Independencies include Y′⊥X|Z,Y.(c),
(d): The actual and feed-forward graphs, respectively, when utilising a pre-trained context encoding C.
Independencies for (c) include Y′⊥Z|X,C,Y′⊥C|Y, andY′⊥C|X,Y, and for (d) include Y′⊥X|Z,C
andY′⊥Y|X,C.
As is standard in IB methods, we want our optimization procedure to target a MNI principle of optimal
compression; i.e. we wish to learn our latent representation, Z, such that it captures exactly the necessary
conditional information (conditioned on Y) betweenXandY′. Specifically we would like to target the
following analogous MNI point, the CMNI point:
I(Y′;X|Y) =I(Y′;Z|Y) =I(Z;X|Y). (CMNI)
Due to the more complex probabilistic structure of our setting, we do not have I(Y′;Z|Y)≤I(Y′;X|Y)
andI(Y′;X|Y)≤I(Z;X|Y)directly from the data processing inequalities, as opposed to the situation in
5Published in Transactions on Machine Learning Research (02/2023)
the standard IB setting, where the joint probability follows a simpler Markov chain Z→X↔Y. We thus
prove that this is the case given our set-up (see Proposition 3 in Appendix A), which implies that we can
arrive at the CMNI point by maximizing I(Y′;Z|Y)and minimizing I(Z;X|Y).We remark here that, as in
reaching the MNI point for IB methods, it is potentially not possible in practice to reach the CMNI point
exactly on a given dataset. However, precisely reaching the CMNI point does not appear to be a necessity
for effective compression in practice as long as one approaches it.
However, estimating directly I(Y′;Z|Y)andI(Z;X|Y)can be challenging, so we derive variational bounds
to optimize the two conditional mutual information terms.
First, to maximize I(Y′;Z|Y)we decompose this information as
I(Y′;Z|Y) =−H(Y′|Z,Y) +H(Y′|Y)∝−H(Y′|Z,Y). (3)
Importantly, the term H(Y′|Y)is fully determined by the true data distribution, and thus, can be dropped
from the optimization procedure, which is why the proportionality holds.1
We can readily bound −H(Y′|Z,Y)from below with:
−H(Y′|Z,Y) =Ep(y,z,y′)[log(p(y′|z,y))]
=Ep(z,y)[DKL(p(y′|z,y)∥d(y′|z,y))] +Ep(y,z,y′)[log(d(y′|z,y))]
≥Ep(y,z,y′)[log(d(y′|z,y))].(4)
Thus, in order to maximize I(Y′;Z|Y)we can simply maximize Ep(y,z,y′)[log(d(y′|z,y))]. Moreover, since
maximizing this bound also minimizes the expected KL divergence, i.e.
Ep(y,z,y′)[log(p(y′|z,y))]−Ep(y,z,y′)[log(d(y′|z,y))]
, we will simultaneously also learn to approximate p(y′|z,y)withd(y′|z,y), and the bound will become tight.
To minimize I(Z;X|Y)by a tractable upper bound one needs to do more work, and use more distributional
approximations. In VIB, one bounds I(Z;X)above with I(Z;X)≤Ep(x)[DKL(q(z|x)∥p(z))],wherep(z)is
a pre-specified fixed prior. We shall see that I(Z;X|Y)can be bounded above similarly for TEB, however
our prior cannot be a hand-set fixed distribution, since it must be relevant to Y. Introducing the distribution
qy(z|y)as a learnable approximation to p(z|y), and recalling that q(z|x,y) =p(z|x,y)is the true distribution,
we get that:
I(Z;X|Y) =Ep(y,z,x )/bracketleftbigg
log/parenleftbiggq(z|x,y)
p(z|y)/parenrightbigg/bracketrightbigg
=Ep(y,z,x )/bracketleftbigg
log/parenleftbiggq(z|x,y)
qy(z|y)/parenrightbigg/bracketrightbigg
−Ep(y)[DKL(p(z|y)∥qy(z|y))]
≤Ep(y,z,x )/bracketleftbigg
log/parenleftbiggq(z|x,y)
qy(z|y)/parenrightbigg/bracketrightbigg
=Ep(x,y)[DKL(q(z|x,y)∥qy(z|y))].(5)
Trainingqy(z|y)by minimizing this bound, also gives qy(z|y)≈p(z|y), since it squeezes the KL divergence
of the two, and so separate learning of qyto approximate p(z|y)is not necessary.
Equation 4 with Equation 5 is used to define a Lagrangian loss for the TEB algorithm, which can be shown
to bring us to the CMNI point when minimized:
Theorem 1. Under the independence assumptions implied by Equation 1 and 2, the minimum of the fol-
lowing objective with respect to q,qy,dwill be at the CMNI point for Z,X,Y,Y′, for some hyperparameter
β >0:
TEB =Ep(x,y)[DKL(q(z|x,y)∥qy(z|y))]−βEp(y,z,y′)[log(d(y′|z,y))]. (TEB)
1However, this also means that the information I(Y′;Z|Y)may not be possible to explicitly calculate from our existing
procedure.
6Published in Transactions on Machine Learning Research (02/2023)
Proof.Directly from Proposition 3 in Appendix A.1, the CMNI point can be arrived at by maximizing
I(Y′;Z|Y)and minimizing I(Z;X|Y). The proof of Theorem 1 is immediate from the proposition, by
replacingI(Y′;Z|Y)with its lower bound in Equation 4 and I(Z;X|Y)with its upper bound in Equation
5. Please refer to Appendix A.1 and A.2 for the full derivations of Theorem 1.
Moreover, at convergence, the value of the upper bound is given by:
Ep(x,y)[DKL(q(z|x,y)∥qy(z|y))]≈I(Z;X|Y) (6)
and so it can be used as a metric for I(Z;X|Y). Thus in the case that the TEB loss optimum is at the
CMNI point, we also have the fact that this bound measures the transfer entropy I(Y′;X|Y).Note that
if the decoder does not converge to the true decoder, the bounds we use for optimization are still valid,
and all we stand to lose is the ability to get to the CMNI point exactly. No matter how bad the decoder
is, for whatever output space we are able to represent, we still do bottleneck I(Z;X|Y), and our bound of
I(Z;X|Y)becomes tight if converged.
The loss function of TEB shares structural similarity with those of VIB and CEB but differs in the choice of
prior distribution in the Kullback-Leibler (KL) divergence term. The VIB method uses a fixed distribution
as the prior (a Gaussian in the original implementation), while for CEB, the prior is learned from the image
of a target Y′, mapped back to the latent via a backwards encoder. In contrast, TEB learns the prior only
from the history of Y, making the role of the KL term to regularize the jointly inferred latent Z, so that it
relies on as little information from the Xstream as possible.
Now, suppose one has already learned an encoding qy(·|y)forYthat provides a latent representation of Y.
We can use this as a fixed prior in TEB. This could be any existing model for encoding Yto a variable C,
e.g. a pre-trained self-supervised model, which will not be optimised with the derived TEB optimisation
procedure (see Figure 2c and 2d to see how we integrate this latent in the TEB computation). One can
then use this encoding in place of Yin the TEB algorithm. This could come with many practical advan-
tages like computational efficiency, training efficiency, reuse of existing models, and rapid prototyping when
experimenting with multiple configurations or hyperparameter settings for the full TEB model.
To successfully bottleneck the transfer entropy with respect to conditioning on Yinstead ofC, it suffices to
require that the representation Cis relevant to Y′in thatI(C;Y′)is maximized (see Lemma 5 in Appendix
A), so that I(X;Y′|C)≈I(X;Y′|Y).With this in mind we get the following TEB loss for CMNI learning
from a pre-existing contextual encoding:
Theorem 2. Under the independence assumptions Y′⊥Z|X,C,Y′⊥C|Y,Y′⊥C|X,Y,Y′⊥X|Z,C
Y′⊥Y|X,C, andC∼qy(·|y)such thatI(C,Y′) =I(Y,Y′), the minimum of the following objective with
respect toq,dwill be at the CMNI point for Z,X,Y,Y′, for some hyperparameter β >0:
TEBc=Ep(x,y)[DKL(q(z|x,c)∥qy(z|y))]−βEp(y,z,y′)[log(d(y′|z,c))]. (TEBc)
Proof. Directly from Lemma 5 in Appendix A.3, with I(C;Y′) =I(Y;Y′), we can replace Yeverywhere
withCand still arrive at the CMNI point. Then the proof is immediate by following a similar procedure as
Theorem 1. Please refer to Appendix A.3 for the full derivations of Theorem 2.
3 Experiments
To implement TEB we used an architecture as shown in Figure 1b. It should be noted that TEB could
be applied to other architectures, but here we will describe the architecture used in our experiments. The
model uses two initial encoders for the two streams qyandqx, both of which feed into two separate LSTMs
(Hochreiter & Schmidhuber, 1997) to aggregate information across the sequence. The hidden state from the
Ypathway LSTM is then linearly projected to calculate the means and log variances for the dimensions
of the prior qy(z|y), which are treated as being independent Gaussians in each dimension. The mean and
log variance of this prior are then combined with the output from the Xpathway LSTM, via a multi-
layer perceptron (MLP) that perturbs the prior qy, to arrive at means and log variances parameterizing
7Published in Transactions on Machine Learning Research (02/2023)
Figure 3: (a): Sample sequence of rotating MNIST, which switches example digit at the fourth frame. (b):
ThisisasampletestinggenerationofaTEBmodelusingafixedpre-trainedcontextencoding, reconstructing
the next step of a sequence which always switches digits at the prediction step. Images are arranged from
left to right as: two frames of input video, next step, generated prediction. As an added demonstration
of flexibility of TEB in this figure, the pre-trained context encoding was trained to reconstruct the next
step of a rotating digit. The dataset the pre-trained context encoding was trained on is a separate one
without switching of digits, hence the context encoding model had never encountered on video where the
digit example switched. Note that the blur in the prediction is expected since the optimal prediction is the
least squares minimum distance to all examples of the corresponding class.
the independent Gaussians in each dimension of q(z|x,y). Then the reparameterization trick (Kingma &
Welling, 2014) is used to sample latent representation Zfromq(z|x,y).
When the expected decoder doutput is an image, the decoder arranges the sampled latent representation Z
into a spatial representation using a positional encoding, and this version of the latent is then transformed via
spacepreservingconvolutionstoarriveattheoutputimage, Y′. Whenthedecoderisusedtogenerateatime-
series signal, it is implemented as a neuralODE (Chen et al., 2018) with its dynamics parameterized by an
MLP. In both cases, we assume the decoder output is the mean of a Gaussian distribution with fixed variance,
independently in each pixel or at each time-step. See Appendix B for exact specifications of model architec-
ture, more detailed descriptions on the task datasets, as well as more sample generations in every experiment.
Code of our implementations can be found at https://github.com/ximmao/TransferEntropyBottleneck .
3.1 Generating Rotated MNIST
To experimentally test Theorems 1 & 2, we create a dataset of videos of rotating MNIST (Lecun et al.,
1998) digits. In a given video sequence, a digit starts rotated at some random angle that is a multiple ofπ
4,
and rotates counter-clockwiseπ
4at every frame, but with a chance that the example digit in the video may
change to another random digit at a given frame with some specified probability. We can interpret this data
as a target stream Ywhich is the image video, and source stream Xconsisting of class labels for the next
step in the video. See Figure 3a for an illustration of the rotating MNIST task.
To test the properties of TEB near the CMNI point, we train several models with different values of βfor10
epochs. On this test we set a next step digit switching probability of 0.5, which gives that the true expected
transfer entropy from XtoYis1.67689nats. See Appendix B.2 for the calculation of the true transfer
entropy for this task.
From Figure 4a, we can see that there is a “bifurcation point” for βaround 0.2, such that for any βsmaller
than that the output loglikelihood sharply drops off. We refer to such a point as β1, and the sharp drop-off
forβ <β 1is likely due to that essential information transfer from X being left out of the latent representation
Z of the model, as in I(Z;X|Y)<I(Y′;X|Y). On the other hand, as in other IB methods (Wu et al., 2019),
there is another value of β, which we denote β0as in Wu et al. (2019), such that for β <β 0the model is not
learning and Zbecomes a trivial representation, which in our case means that we compressed too much on
8Published in Transactions on Machine Learning Research (02/2023)
(a) TEB testing
(b) TEBcfrom pre-trained Cin testing
Figure 4: Plots of the information metric and reconstruction loglikelihood vs βincremented by tenths,
with extra hundredths increments in the chaotic regime in [.1,.2], for both TEB and TEBctesting. In
the information metric subfigure (the left subfigure in both (a) and (b)), the green dashed horizontal line
indicates the true transfer entropy for this dataset. From this we obtain β∗as the specific value of βachieving
the true transfer entropy, calculated as the interpolated value of βfor whichI(Z;X|Y) =I(Y′;X|Y)on
average from repeated trials. In the reconstruction loglikelihood subfigure (the right subfigure), the output
loglikelihood of the model trained with the obtained β∗is marked using the red dashed vertical line. Note
that in (a) the variance of runs highly increases for β∈(.1,.2), but the models in this interval that found a
solution with I(Z;X|Y)≈I(Y′;X|Y)are the same models that achieved higher reconstruction performance.
the information in the bottleneck, so that the information from X to recover Y’ is not available to the model
any more. From Figure 4a, we can put β0roughly in the range from 0.1 to 0.14, corresponding to nearly zero
I(Z;X|Y). Note that β0,β1are “bifurcation points” for the task performance both in training and testing
(see Appendix B.2 for training plots), as for values of β∈(β0,β1)we quickly begin to learn representations
that perform better until β≥β1and the model does not compress lower than the true transfer entropy
I(Y′;X|Y)≤I(Z;X|Y).In Figure 5, we show the image generation samples when varying β.
We choose to mark β∗in Figure 4 as the interpolated value of βfor whichI(Z;X|Y) =I(Y′;X|Y)on
average. At β∗the true expected transfer entropy is approximately identifiable by the value of our upper
bound forI(Z;X|Y)in Equation 6, and we find that β∗lies close to β1. We can see that the models learned
9Published in Transactions on Machine Learning Research (02/2023)
atβ1are very close to the CMNI point with I(Z;X|Y)≈I(Y′;X|Y)≈I(Y′;Z|Y).2Consistent with IB
principles, we can see that at values of βwhich result in the representation closest to the CMNI point, TEB
obtains approximately the best testing score. Moreover, on this dataset the TEB model testing performance
decreases monotonically with distance of I(Z;X|Y)fromI(Y′;X|Y), outside of a small neighbourhood of
β1.
(a) TEB
 (b) TEBc
Figure 5: Sample testing generation on the same input for varying βequal to
0.1,0.12,0.14,0.16,0.18,0.2,0.3,0.4,0.5, from top to bottom respectively. For β= 0.1both models
learn the trivial representation which does not incorporate class information from the Xstream, i.e. change
of digit class from 1to0.(a):The TEB model’s representation remains trivial until the sharp increase
in reconstruction loglikelihood at β= 0.18, as seen in Figure 4. (b):The TEBcmodel’s representation
appears to transmit class information for βas low as 0.12; This is consistent with the appearance (see
Figure 4) of the reconstruction loglikelihood dropping off more gradually than TEB for β≥0.13, with the
transmitted information very near (distance <0.2nats) to the true expected transfer entropy for β=.12
on some seeds.
For the TEBcmodels, we pre-trained the context embedding module, which we will call the “ Ymodule”,
to represent Cwith next step prediction of Y′using CEB with γ= 1on the same dataset. In this case for
maximum training efficiency and difficulty for TEBc, we also fix the decoder from the Ymodule and use it
for the reconstruction from Zin the full TEBcmodel. We find in Figure 4b that the bifurcation point β1for
TEBcis similar to that recovered for TEB trained end to end. However, we do find that unlike the situation
for TEB, here the CMNI point does not appear to be reachable since our bound for I(Y′;Z|Y)continues to
increase for β > β 1. On the other hand, it is difficult to find β0directly from Figure 4b, but as shown in
Figure 5b, TEBcfails to predict the change of digit class at β= 0.1, which puts its β0approximately at 0.1.
One reason the CMNI point is not reachable could be due to our representation of the output loglikelihood;
As is the case for any gradients proportional to ℓ2orℓ1, the output probability space assumes independence
of each pixel, and therefore does not accurately represent the true image probability space for the data. This
cause is evidenced by the fact that the generation quality does not perceptibly vary much between models
forβ >β 1even though the loglikelihood does vary significantly. Another cause could be that I(Y′;C)was
2One can see in Figure 4 that our reconstruction loglikelihood bound for I(Y′;Z|Y)(which differs from I(Y′;Z|Y)by a
constant) is approximately maximized at β1, and since I(Z;X|Y)≈I(Y′;X|Y)atβ1, we are close to the CMNI point.
10Published in Transactions on Machine Learning Research (02/2023)
Figure 6: Sample TEB testing sequence of the needle in a haystack task with 20distractors, with a color
switch at the prediction step. Images are arranged from left to right as: three frames of input source stream,
next step, generated prediction. For visual clarity, we show only the needle pixel channel group of the source
stream, with the first 10distractor pixels added and randomly placed on the perimeter (note that needle
pixel always at top left). The actual streams in the experiments had stacked RGB channel groups, where
each group has either a needle or distractor pixel at top left.
not maximized enough due to regularisation. Also, since the Ymodule does not have any information to
reconstruct a randomly swapped in digit, and is never able to represent this on its own, it may not have
learned a representation of angle which is disentangled from the identity of the digit. This would result in
requiring a higher βfor learning a TEBcmodel which adjusts the latent by sending extra information from
Xon sequences which switch, in addition to the class of the digit. To partially test these causes, we check
the loglikelihood performance of TEBcwith a higher β= 10, from a pre-trained Ymodule such that it is
trained with a higher γ= 100to prioritise maximizing I(Y′;C). This results in a significantly improved
testing reconstruction loglikelihood of 19.9317±0.3105(as compared to −16.0291±0.6041atβ= 0.2).
Another way to find a better Cis to letYbe pre-trained on rotating digits that never switch, because then
the training of Ycan focus on learning just the rotating of digits. See Figure 3b for an example testing
generation in this case with β= 1. This also demonstrates that TEB is flexible in modulating a pre-trained
model for a new task modality.
3.2 Detecting a needle in a haystack
Here we design a challenging task to test performance of TEB, and to test its added ability to generalize
when the statistical coupling between the XandYstreams is high, but with a conditionally small and crucial
directed information transfer from XtoY. The task is next step prediction for a colored bouncing balls
(Sutskever et al., 2008) video, where the color of the balls can change to one of seven colors at the moment
of prediction with probability 0.5. The twist is that there are some number of randomly colored noisy pixels
in the video, of which one pixel, which we shall call the “needle pixel”, noisily corresponds to the color of the
balls in the next step; To predict the next step color correctly, a model has to decide on the three previous
frames of video which pixel corresponds to the needle pixel. The sequences for both XandYare defined
as video sequences consisting of the images of the balls and the distractor pixels, but importantly, only X
contains the added needle pixel. To make the task of predicting color more difficult still, we introduce a
baseline intensity in all color channels (red, green, and blue), at each pixel of each ball. This means that
the output reconstruction component of the loss, which is common to all models (for deterministic models it
is the only component of the loss), depends only mildly on predicting the correct color. As such, XandY
are very strongly correlated, both with each other and over time. Moreover, the transfer entropy provided
by the needle pixel is crucial to this color prediction task, though this information is relatively very small.
This ensures that our task fits the description of a large correlation of the sequences, but a small and crucial
directed information transfer from XtoY. See Appendix B.3 for more detailed descriptions on the utilized
XandYvideo sequences with the distractor pixels.
In our experiments, we also trained three baselines for comparison. The first two are deterministic baselines.
Between them, the first is simply a deterministic version of the TEB model, with the same architecture and
latent size, but we dropped the sampling from q(z|x,y)and excluded the KL divergence term in the objective,
which is the first term in the TEB objective function. It uses directly the mean of q(z|x,y)as the input
for the decoder. The second which we refer to as the joint stream deterministic model or “Deterministic
joint” in Table 1 and 2, is a ResNet18 (He et al., 2016) +LSTM which takes all the XandYinformation
11Published in Transactions on Machine Learning Research (02/2023)
as a unified input, followed by the same deterministic decoder architecture as TEB. Note that particularly
in this task, every image of Yis included in X, thusXitself is a valid unified input, and we adopted it in
our implementations. The third baseline is CEB, with the same architecture as the Ymodule of TEB, with
added backward encoder. As with “Deterministic joint”, CEB takes a unified input as well. The reasons
behind including joint stream models, i.e. “Deterministic joint” and CEB, as comparisons are to showcase
the advantage when bottlenecking transfer entropy in this task.
To evaluate performance of the directed information transfer from XtoY, we want to determine whether
the predicted image at time thas the correct color for the balls. To quantify the color accuracy we use
an additional color classification network. This classification network is trained to classify color into one of
seven color classes, on images of the balls from the training set. Since this is an easy task, the classification
achieves 100%accuracy on the test set as well. As such, any errors in color classification reflect errors in the
color of the predicted frame, and so, the color classification serves as a metric of color prediction, i.e. the
transferred information.
Table 1 shows that on the color prediction task for 20distractors, the deterministic version of the TEB
architecture does not perform much better than what one achieves by simply predicting a constant color
as previous frames, which gives an accuracy of approximately 57%. In contrast, TEB model picks up a
transfer entropy signal via information bottleneck, which allows it to predict the color at a much better
accuracy. TEB outperforms the deterministic baseline consistently across different number of distractors,
with an increasing gap for the harder task. In comparison, both the joint stream deterministic and stochastic
models (CEB) do not even learn to generalize better than the constant color baseline on 5distractors. One
explanation is that they are likely dealing with a harder task than TEB and “Deterministic”, because they
cannot make use of the difference from X to Y in the latent space, which is essentially the information
denoting the needle pixel. Another explanation lies in the joint stream formulation. For example for CEB,
it has to compress the joint stream input as a whole, including the bouncing balls images, needle pixel and
all the distractors, so it might ignore the needle pixel which is relatively small.
Table 1: Testing color prediction accuracy on the needle in a haystack task (8 seeds), best performance is
shown in bold.
Model 5distractors 10distractors 15distractors 20distractors
TEB 94.64±2.21 % 88.39 ±3.63 % 81.99 ±3.04 % 68.37 ±3.92 %
Deterministic 93.87 ±0.94 % 88.33 ±2.57 % 73.27 ±7.50 % 59.97 ±7.26 %
Deterministic joint 38.35 ±0.75 % - - -
CEB 40.78 ±0.67 % - - -
Table 2: Testing reconstruction log-likelihood on the needle in a haystack task (8 seeds), best performance
is shown in bold.
Model 5distractors 10distractors 15distractors 20distractors
TEB -551 ±25 -568±26 -562±17 -566±17
Deterministic -515±26 -542±21 -561±25 -558±28
Deterministic joint -573 ±30 - - -
CEB -823 ±66 - - -
However, Table 2 shows that unlike the case of color prediction accuracy, the aggregate statistics of recon-
struction loglikelihood do not differentiate between any model except for CEB, which trades loglikelihood
for better identifying the transfer entropy than “Deterministic joint”, no matter the number of distractors.
This is due to the added whitening on each of the color channels at each ball pixel. As such, the recon-
struction can still be largely accurate even when the system is not getting the color correct. In other words,
the majority of the reconstruction quality is dependent on the localisation of the balls and not the color of
12Published in Transactions on Machine Learning Research (02/2023)
Figure 7: Sample TEB testing generation for 5distractors
the balls, which highlights the high difficulty disentangling the needle pixel signal, and obtaining a quality
representation which allows for the prediction of the true color.
Lastly, we show batch sample generations of TEB for 5distractors in Figure 7. We believe that it can even
be a challenging task for people, without knowing that the needle pixels are fixed always on the top left
corner, and the models would have to infer the needle pixels based on the history. See Appendix B.3 for
more sample generations in both training and testing and the common generation error made by TEB.
3.3 Extrapolating time-series
We also evaluate the performance of TEB on synthetic time-series data, where each time-series signal is
generated as the average of five sinusoidal waves of distinct frequencies f∈{0.2,0.4,0.6,0.8,1.0}Hz with
starting point randomly in [−1,1]. Each sinusoidal wave is generated by unrolling a 2-dimensional continuous
time dynamical system with purely imaginary eigenvalues, and recording the ycoordinates of the resulting
trajectories. Note that with initial conditions lying on the unit circle, the trajectories of the said dynamical
system are rotations on the unit circle. See Appendix B.4 for more detailed descriptions on the implemented
dynamical system. Note that this is done to facilitate the sudden change of frequency at any given time-
step. We refer to this task as multi-component sinusoids. As with other tasks discussed above, a switch
with probability 0.5is also implemented in this dataset, where the frequency of one random component is
resampled every time a switch occurs. The task is to extrapolate the future sequence of length 20for a
given time-series signal of length 100. HereYis the averaged signal and Xis the noisy frequencies for all
the sub-components for the next time-step. The difficulty of this task comes in largely from the averaging
nature ofY, as there might be multiple ways to decompose the same signal. Hence it requires the model
to capture the information embedded in the transfer entropy in order to understand the true decomposition
based on the frequencies.
For this task, we compare TEB model with three other baselines. The first is “Deterministic” defined above,
that is simply a deterministic version of the TEB model architecture. The other two are both joint stream
models, taking the concatenation of XandYas a unified input. Between them, the second is simply
a deterministic LSTM, which outputs the next sequence as a vector, whereas the third is similar to the
latentODE model introduced in Chen et al. (2018), with an LSTM encoder and a neuralODE decoder, but
here we only ask the decoder to predict the extrapolation points, instead of reconstructing the inputs as the
13Published in Transactions on Machine Learning Research (02/2023)
Figure 8: Sample TEB testing sequence of the multi-component sinusoids task when there is a switch at
prediction time-step 100, indicated by the yellow vertical line. Note that the sharp change of curve at the
prediction point is due to a larger span in the change of frequency of the fourth sub-component, from 0.8to
0.2.
Table 3: Testing reconstruction log-likelihood on the multi-component sinusoids task (3 seeds), best perfor-
mance is shown in bold.
Model on the entire set only when switching
TEB 4.3497±0.0761 3.6839 ±0.1973
Deterministic 4.2279 ±0.0499 3.4123 ±0.1264
LSTM 3.9941 ±0.0046 2.8577 ±0.0168
latentODE-VIB 3.7902 ±0.0020 2.5475 ±0.0146
original latentODE. As with the original version, it uses a fixed Gaussian as a prior, so it is functionally
close to VIB, and we refer to it as latentODE-VIB.
Table 3 shows the reconstruction log-likelihood both on the entire testing set and only when switching to
a new frequency. Note that since we allow switching to the same frequency, the second scenario comprises
around 40%of the entire testing set. Judging from the results, TEB outperforms all the baselines in both
cases. In addition, although all the algorithms suffer from a reduction in performance when evaluating only
when there is change of frequency, TEB is able to achieve the smallest reduction among the four. This
indicates that TEB does a better job capturing the transfer entropy in this task than all other baselines.
Furthermore, we show batch sample generations of TEB in Figure 9, where rows 3in the left and 2in the
right show a common extrapolating error made by TEB. See Appendix B.4 for a case of large deviation in
TEB’s testing generation, which is more rarely seen in our experiments.
4 Discussion and future work
InthispaperwedevelopedTEB,aninformationbottleneckapproachdesignedtocapturethetransferentropy
between a source and target stream of data. In doing so, TEB brings the benefits of IB approaches to dual
stream modeling problems where disentangling statistical dependencies can be very challenging. As well,
TEB allows one to use pre-existing encoding models for the target stream. We showed experimentally that
TEB allows one to arrive at high quality latent representations that obey the CMNI principle, and use these
to make more accurate predictions about the target stream even when the total information transferred is
small relative to the joint information.
A limitation of TEB, transpiring from Theorems 1 and 2, is that TEB enables an exact estimation of the
true transfer entropy only at convergence to the optimum of the loss for a particular β, which is the βand
optimum reaching the CMNI point. Recall that βis a weighting coefficient in TEB’s loss function, where
a largerβfocuses more on the generations of the outputs and a smaller βfocuses more on reducing the
KL divergence. In practice depending on the architecture, learning, and dataset, the optimum may not be
reachedandCMNIlearningmaynotbepossible. Additionally, ifCMNIlearningispossible, theidentification
of this particular βmay not be possible. However, as shown in the rotating MNIST experiment, we identify
14Published in Transactions on Machine Learning Research (02/2023)
Figure 9: Sample TEB testing time-series sequence generation when there is a switch at prediction time-step
100, indicated by the yellow vertical line. The samples with a frequency switch are marked with green frames.
a phenomenon similar to the one observed for the usual IB case (Wu et al., 2019), which can provide for the
approximate identification of this β. We expect this to occur in some form for most datasets when learning
can reach a CMNI point optimum. But, more importantly, estimating the transfer entropy is not the primary
purpose of TEB, so even when one cannot get a hold of an estimate on this quantity, TEB can still use the
transfer entropy to improve its predictions.
TEB has many potential applications. In principle, TEB could be used in any situation where one pos-
sesses multimodal sequential data with statistical dependencies. This could include videos, natural language
captioning, financial modeling, medical device time-series, and more. In all cases, an appropriate architec-
ture would have to be built, but TEB can be applied generally. In fact, TEB can potentially be applied
to traditionally deterministic architectures for dual data streams tasks by adding some stochasticity to the
representations like the technique of Achille & Soatto (2018). As well, TEB could be modified to work in
situations with more than two streams of data. Future work will examine the utility of TEB with real data
in these potential applications and new architectures, and potential theoretical guarantees of learnability as
done in Wu et al. (2019) for the IB loss. This current work provides the analytical foundations of TEB and
shows with synthetic data that it works well in principle.
Broader Impact Statement
This work provides a method for prediction from dual-stream data sources. As with other machine learning
approaches to prediction, there may be questionable uses that such a system could be applied to. To avoid
such problematic situations, use of TEB should be carefully considered by researchers in advance, and any
potential harms should be considered before applying TEB to a new dataset.
Acknowledgments
This work was done in partnership with BIOS Health Ltd. (https://www.bios.health/) and supported by
MEDTEQ+ (Impact Grant 13-F Neuromodulation), Healthy Brains, Healthy Lives (Neuro-Partnerships
Program: 3d-13), Mitacs (Accelerate Program: IT19551), NSERC (Discovery Grant: RGPIN-2020-05105;
Discovery Accelerator Supplement: RGPAS-2020-00031), and CIFAR (Canada AI Chairs to BAR and GL;
Learning in Machine and Brains Fellowship to BAR). GL further acknowledges the Canada Research Chair
in Neural Computations and Interfacing (CIHR). This research was also enabled in part by support provided
by (Calcul Québec) (https://www.calculquebec.ca/en/) and Compute Canada (www.computecanada.ca).
15Published in Transactions on Machine Learning Research (02/2023)
References
Alessandro Achille and Stefano Soatto. Information dropout: Learning optimal representations through
noisy computation. IEEE Transactions on Pattern Analysis and Machine Intelligence , 40(12):2897–2905,
2018. doi: 10.1109/TPAMI.2017.2784440.
Alex Alemi, Ian Fischer, Josh Dillon, and Kevin Murphy. Deep variational information bottleneck. In ICLR,
2017. URL https://arxiv.org/abs/1612.00410 .
Lionel Barnett, Adam B. Barrett, and Anil K. Seth. Granger causality and transfer entropy are equivalent
for gaussian variables. Phys. Rev. Lett. , 103:238701, Dec 2009. doi: 10.1103/PhysRevLett.103.238701.
URL https://link.aps.org/doi/10.1103/PhysRevLett.103.238701 .
Mohamed Ishmael Belghazi, Aristide Baratin, Sai Rajeshwar, Sherjil Ozair, Yoshua Bengio, Aaron Courville,
and Devon Hjelm. Mutual information neural estimation. In Proceedings of the 35th International Confer-
ence on Machine Learning , volume 80 of Proceedings of Machine Learning Research , pp. 531–540. PMLR,
10–15 Jul 2018. URL https://proceedings.mlr.press/v80/belghazi18a.html .
Ricky T. Q. Chen, Yulia Rubanova, Jesse Bettencourt, and David Duvenaud. Neural ordinary differential
equations. Advances in Neural Information Processing Systems , 2018.
Ivan De La Pava Panche, Andres M. Alvarez-Meza, and Alvaro Orozco-Gutierrez. A data-driven measure of
effective connectivity based on renyi’s α-entropy. Frontiers in Neuroscience , 13, 2019. doi: 10.3389/fnins.
2019.01277. URL https://www.frontiersin.org/articles/10.3389/fnins.2019.01277 .
Ian Fischer. The conditional entropy bottleneck. Entropy, 22(9), 2020. ISSN 1099-4300. doi: 10.3390/
e22090999. URL https://www.mdpi.com/1099-4300/22/9/999 .
Deniz Gençağa. Transfer entropy. Entropy, 20(4), 2018. ISSN 1099-4300. doi: 10.3390/e20040288. URL
https://www.mdpi.com/1099-4300/20/4/288 .
C. W. J. Granger. Investigating causal relations by econometric models and cross-spectral methods. Econo-
metrica, 37(3):424–438, 1969. ISSN 00129682, 14680262. URL http://www.jstor.org/stable/1912791 .
MagdaGregorová, AlexandrosKalousis, andStéphaneMarchand-Maillet. Forecastingandgrangermodelling
withnon-lineardynamicaldependencies. InMichelangeloCeci, JaakkoHollmén, LjupčoTodorovski, Celine
Vens, and Sašo Džeroski (eds.), Machine Learning and Knowledge Discovery in Databases , pp. 544–558,
Cham, 2017. Springer International Publishing.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing human-
level performance on imagenet classification. In 2015 IEEE International Conference on Computer Vision
(ICCV), pp. 1026–1034, 2015. doi: 10.1109/ICCV.2015.123.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition.
In2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , pp. 770–778, 2016. doi:
10.1109/CVPR.2016.90.
Katerina Hlavackova-Schindler. Equivalence of granger causality and transfer entropy: A generalization.
Applied Mathematical Sciences , 5:3637 –3648, January 2011. ISSN 1314-7552. URL http://eprints.cs.
univie.ac.at/4968/ .
Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural Comput. , 9(8):1735–1780, nov
1997. ISSN 0899-7667. doi: 10.1162/neco.1997.9.8.1735. URL https://doi.org/10.1162/neco.1997.
9.8.1735 .
Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015. URL
http://arxiv.org/abs/1412.6980 .
Diederik P. Kingma and Max Welling. Auto-encoding variational bayes. In ICLR, 2014. URL http:
//arxiv.org/abs/1312.6114 .
16Published in Transactions on Machine Learning Research (02/2023)
Gerhard Kramer. Directed information for channels with feedback . PhD thesis, ETH Zurich, Zurich, 1998.
Diss. Techn. Wiss. ETH Zürich, Nr. 12656, 1998. Ref.: J. L. Massey ; Korref.: A. J. Han Vinck.
Y. Lecun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recognition.
Proceedings of the IEEE , 86(11):2278–2324, 1998. doi: 10.1109/5.726791.
Brian Lindner, Lidia Auret, Margret Bauer, and J.W.D. Groenewald. Comparative analysis of granger
causality and transfer entropy to present a decision flow for the application of oscillation diagnosis. Journal
of Process Control , 79:72–84, 2019. ISSN 0959-1524. doi: https://doi.org/10.1016/j.jprocont.2019.04.005.
URL https://www.sciencedirect.com/science/article/pii/S095915241830516X .
Aurélie C. Lozano, Naoki Abe, Yan Liu, and Saharon Rosset. Grouped graphical Granger modeling for gene
expression regulatory networks discovery. Bioinformatics , 25(12):i110–i118, 05 2009. ISSN 1367-4803. doi:
10.1093/bioinformatics/btp199. URL https://doi.org/10.1093/bioinformatics/btp199 .
Vinod Nair and Geoffrey E. Hinton. Rectified linear units improve restricted boltzmann machines. In ICML,
pp. 807–814, 2010.
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen,
Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary
DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and
Soumith Chintala. Pytorch: An imperative style, high-performance deep learning library. In Advances in
Neural Information Processing Systems 32 , pp. 8024–8035. 2019.
Gabriel Pereyra, George Tucker, Jan Chorowski, Łukasz Kaiser, and Geoffrey Hinton. Regularizing neural
networks by penalizing confident output distributions, 2017. URL https://arxiv.org/abs/1701.06548 .
Andrew M Saxe, Yamini Bansal, Joel Dapello, Madhu Advani, Artemy Kolchinsky, Brendan D Tracey, and
David D Cox. On the information bottleneck theory of deep learning. Journal of Statistical Mechanics:
Theory and Experiment , 2019(12):124020, dec 2019. doi: 10.1088/1742-5468/ab3985. URL https://doi.
org/10.1088/1742-5468/ab3985 .
Thomas Schreiber. Measuring information transfer. Phys. Rev. Lett. , 85:461–464, Jul 2000. doi: 10.1103/
PhysRevLett.85.461. URL https://link.aps.org/doi/10.1103/PhysRevLett.85.461 .
Anil K. Seth, Adam B. Barrett, and Lionel Barnett. Granger causality analysis in neuroscience and neu-
roimaging. Journal of Neuroscience , 35(8):3293–3297, 2015. ISSN 0270-6474. doi: 10.1523/JNEUROSCI.
4399-14.2015. URL https://www.jneurosci.org/content/35/8/3293 .
Nicolas Skatchkovsky, Osvaldo Simeone, and Hyeryung Jang. Learning to time-decode in spiking neural
networks through the information bottleneck. In Advances in Neural Information Processing Systems ,
2021.
Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout:
A simple way to prevent neural networks from overfitting. Journal of Machine Learning Research , 15(56):
1929–1958, 2014. URL http://jmlr.org/papers/v15/srivastava14a.html .
Ilya Sutskever, Geoffrey E Hinton, and Graham W Taylor. The recurrent temporal restricted boltzmann
machine. In Advances in Neural Information Processing Systems , volume 21, 2008.
Nasibeh Talebi, Ali Motie Nasrabadi, and Iman Mohammad-Rezazadeh. Estimation of effective connectivity
using multi-layer perceptron artificial neural network. Cognitive Neurodynamics , 12:21 – 42, 2017.
A. Tank, I. Covert, N. Foti, A. Shojaie, and E. B. Fox. Neural granger causality. IEEE Transactions on
Pattern Analysis and Machine Intelligence , 44(08):4267–4279, aug 2022. ISSN 1939-3539. doi: 10.1109/
TPAMI.2021.3065601.
Naftali Tishby and Noga Zaslavsky. Deep learning and the information bottleneck principle. In 2015 IEEE
Information Theory Workshop (ITW) , pp. 1–5, 2015. doi: 10.1109/ITW.2015.7133169.
17Published in Transactions on Machine Learning Research (02/2023)
Naftali Tishby, Fernando C. Pereira, and William Bialek. The information bottleneck method, 2000. URL
https://arxiv.org/abs/physics/0004057 .
Mauro Ursino, Giulia Ricci, and Elisa Magosso. Transfer entropy as a measure of brain connectivity: A
critical analysis with the help of neural mass models. Frontiers in Computational Neuroscience , 14, 2020.
doi: 10.3389/fncom.2020.00045. URL https://www.frontiersin.org/articles/10.3389/fncom.2020.
00045.
Axel Wismüller, Adora M. DSouza, Ali Vosoughi, and Anas Zainul Abidin. Large-scale nonlinear granger
causality for inferring directed dependence from short multivariate time-series data. Scientific Reports ,
11, 2021.
Tailin Wu, Ian S. Fischer, Isaac L. Chuang, and Max Tegmark. Learnability for the information bottleneck.
InUAI, 2019. URL https://arxiv.org/abs/1907.07331 .
Jingjing Zhang, Osvaldo Simeone, Zoran Cvetković, Eugenio Abela, and Mark P Richardson. Itene: Intrinsic
transfer entropy neural estimator. ArXiv, abs/1912.07277, 2019.
18Published in Transactions on Machine Learning Research (02/2023)
A Mathematical formulation
This section is written in a standalone succinct format for maximum clarity on the mathematical details.
We presume the reader has some familiarity with information bottleneck methods.
A.1 Setup
First we state the definition of transfer entropy:
Definition 1. Given sequences of random variables (Xi),(Yi), thetransfer entropy from (Xi)to(Yi)at
t>0(with horizon 0<ℓ≤∞) is the conditional mutual information:
TXt→Y=I(Yt; (Xi)tℓ≤i<t|(Yi)tℓ≤i<t)
=H(Yt|(Yi)tℓ≤i<t)−H(Yt|(Yi)tℓ≤i<t,(Xi)tℓ≤i<t)
wheretℓ= max(t−ℓ,0).
Note that the conditional mutual information between two random variables conditioned on another, is just
a special case of transfer entropy, and so all the methods developed apply to that case.
Latent variable encoder-decoder models seek to learn a conditional density p(y|x)by learning an encoded
latent representation ZofX, with an encoder q(z|x)which factorises the true density as a Markov chain
withZindependent of YgivenX(i.e.Z←X↔Y):
p(y,z,x ) =p(y,x)p(z|x) =p(y,x)q(z|x), (7)
wherep(z|x) =q(z|x)since we introduce Z. As well, we use a decoder d(y|z)that approximates p(y|z).
The functional Markov structure that comes from feed-forward computing zfromx, andyfromz, implicitly
assumes that the distribution is also factorisable as:
p(y,z,x ) =p(x)p(y|z)p(z|x)≈d(y|z)q(z|x)p(x). (8)
Our situation is analogous to this, but in our setting we actually wish to learn a latent encoding Ztof(Xi)i<t
via a mapq, which is conditional on (Yi)i<t, from which one is able to decode Ytconditionally on (Yi)i<t
with the decoder d. Put another way, we want to be able to predict Yat timetusing the values of Yand
Xup to time t−1, but using our encoding of XandYviaZ. Namely, denoting:
Z=Zt, X= (Xi)tℓ≤i<t, Y= (Yi)tℓ≤i<t, Y′=Yt, (9)
Assuming the conditional independence Y′⊥Z|X,Y., we haveZ,d,qare such that:
p(y′,z,x,y ) =p(y′,x,y)p(z|x,y) =p(y′,x,y)q(z|x,y). (10)
as illustrated in Figure 10a.
Furthermore, we require the distribution to be learned by a feed-forward encoder-decoder structure, which
assumes the following decomposition of joint probability distribution (as illustrated in Figure 10b, equivalent
to the conditional independence Y′⊥X|Z,Y.), whered(y′|z,y)is a variational approximation to p(y′|z,y):
p(y′,z,x,y ) =p(x,y)p(y′|z,y)p(z|x,y)≈p(x,y)d(y′|z,y)q(z|x,y). (11)
We call the method we develop using this structure the Transfer Entropy Bottleneck (TEB) algorithm .
19Published in Transactions on Machine Learning Research (02/2023)
Remark 1.As an aside, we note that for the TEB method we will also want to consider the case where
there is an existing model for encoding Y,qY(c|y)which could be any existing model for encoding Ywith a
variableC(e.g. a pre-trained self-supervised model). Thus, Yis first encoded into its own latent C, which
will not be optimised with the derived optimisation procedure (see Figures 10c and 10d); So in reading, keep
in mind that in the text one can replace “ Y” with “C”, and every result still holds.
For the rest of the text assume tis fixed and we will use the notations defined in Equation 9 above.
ZY
XY′
(a)Z YY′
X
(b)ZCY
XY′
(c)Z C YY′
X
(d)
Figure 10: Probabilistic graph representing the random variables and their relationships as assumed by
TEB.(a): The actual random variables in the dataset. Independencies include Y′⊥Z|X,Y.(b): The
random variables as computed by our feed-forward computation. Independencies include Y′⊥X|Z,Y.(c),
(d): The actual and feed-forward graphs, respectively, when utilising a pre-trained context encoding C.
Independencies for (c) include Y′⊥Z|X,C,Y′⊥C|Y, andY′⊥C|X,Y, and for (d) include Y′⊥X|Z,C
andY′⊥Y|X,C.
As is standard in information bottleneck methods, we follow a minimum necessary information principle, in
which we wish to learn our latent representation Zsuch that it captures exactly the necessary conditional
information (conditioned on Y) betweenXandY′.
Specificallywewouldlikethefollowingtohold,which,inanalogytotheusualminimumnecessaryinformation
principle, we call the Conditional Minimum Necessary Information point or CMNI point:
I(Y′;X|Y) =I(Y′;Z|Y) =I(Z;X|Y) (CMNI)
Due to the probabilistic structure of our setting, we do not have I(Y′;Z|Y)≤I(Y′;X|Y)andI(Y′;X|Y)≤
I(Z;X|Y)directly from the data processing inequalities, as opposed to the standard IB setting (where
I(Y;Z)≤I(Y;X)andI(Y;X)≤I(Z;X)). Thus, we first need to prove that we can arrive at the CMNI
point by the following procedure:
1. maximizing I(Y′;Z|Y) (CMNI procedure)
2. minimizing I(Z;X|Y)
To prove this, first we remind the reader of the chain rule for conditional mutual information here since we
will use it throughout the text:
Recall.The chain rule for conditional mutual information is
I(A;B|C) =I(A;B,C)−I(A;B)
or
I(A;B,C) =I(A;B) +I(A;B|C).
We can then prove the following proposition:
20Published in Transactions on Machine Learning Research (02/2023)
Proposition 3. The following inequalities hold under the independencies implied by Equation 10 and 11:
I(Y′;Z|Y)≤I(Y′;X|Y)≤I(Z;X|Y).
Proof.Inequality 1 (I(Y′;Z|Y)≤I(Y′;X|Y)): By the chain rule
I(Y′;X,Y,Z ) =I(Y′;Z,Y) +I(Y′;X|Z,Y)
=I(Y′;X,Y ) +I(Y′;Z|X,Y )
however, Equation 10 implies Y′⊥Z|X,Y, and soI(Y′;Z|X,Y ) = 0which implies
I(Y′;Z,Y)≤I(Y′;X,Y ).
Since
I(Y′;Z|Y) =I(Y′;Z,Y)−I(Y′;Y)
I(Y′;X|Y) =I(Y′;X,Y )−I(Y′;Y),
we get that I(Y′;Z|Y)≤I(Y′;X|Y).
Inequality 2 (I(Y′;X|Y)≤I(Z;X|Y)): By the chain rule:
I(X;Y′,Z,Y ) =I(X;Y′,Y) +I(X;Z|Y′,Y)
=I(X;Z,Y) +I(X;Y′|Z,Y)
however, Equation 11 implies X⊥Y′|Z,Y, and soI(X;Y′|Z,Y) = 0which implies
I(X;Y′,Y)≤I(X;Z,Y).
Since
I(X,Z|Y) =I(X;Z,Y)−I(X;Y)
I(X,Y′|Y) =I(X;Y′,Y)−I(X;Y),
we get that I(Y′;X|Y)≤I(Z;X|Y).
Remark 2.Note that in the proof above, if we had used Equation 11 and X⊥Y′|Z,Yin the first part in
addition to 10, we would have I(Y′;X|Y)≤I(Y′;Z|Y), and therefore I(Y′;X|Y) =I(Y′;Z|Y). This shows
that our feed-forward generation of Y′cannot represent the true distribution unless it maximizes I(Y′;Z|Y).
A.2 A transfer entropy bottleneck objective for CMNI learning
We will now prove that we can use these structures to derive a tractable loss function for optimizing a
model in accordance with the CMNI procedure in order to reach the CMNI point. This method is the TEB
algorithm.
First, in-line with item 1 of the CMNI procedure, we want to maximize I(Y′;Z|Y). One can decompose the
information I(Y′;Z|Y)as
I(Y′;Z|Y) =−H(Y′|Z,Y) +H(Y′|Y)∝−H(Y′|Z,Y). (12)
Importantly, the term H(Y′|Y)is fully determined by the true data distribution, and thus, can be dropped
from the optimization procedure, which is why the proportionality holds. However, this will lead to the
problem that the information I(Y′;Z|Y)will not be tractable to calculate from our existing procedure. We
will address this in the later section A.3 with another form of the TEB algorithm.
We can readily bound −H(Y′|Z,Y)from below with:
21Published in Transactions on Machine Learning Research (02/2023)
−H(Y′|Z,Y) =Ep(y,z,y′)[log(p(y′|z,y))]
=Ep(z,y)[DKL(p(y′|z,y)∥d(y′|z,y))] +Ep(y,z,y′)[log(d(y′|z,y))]
≥Ep(y,z,y′)[log(d(y′|z,y))](13)
Thus, in order to maximize I(Y′;Z|Y)we can simply maximize Ep(y,z,y′)[log(d(y′|z,y))]. Moreover, since
maximizing this bound also minimizes the expected KL divergence
Ep(y,z,y′)[log(p(y′|z,y))]−Ep(y,z,y′)[log(d(y′|z,y))],
we will simultaneously also learn to approximate p(y′|z,y)withd(y′|z,y), and the bound will become tight.
Now, per the second item of the CMNI procedure, we want to minimize I(Z;X|Y). To obtain a tractable
upper bound for I(Z;X|Y)one needs to do more work, and use more distributional approximations. In
VIB, one bounds I(Z;X)above with
I(Z;X)≤Ep(x)[DKL(q(z|x)∥p(z))],
wherep(z)is a pre-specified fixed prior. We shall see that I(Z;X|Y)can be bounded above similarly,
however our “prior” cannot be a hand-set fixed distribution, since it must be relevant to Y. We introduce
an additional encoder qy(z|y)for learning a conditional prior on the latent Z|Ythat is informative for the
full TEB computation. This gives a clear bound of I(Z;X|Y), which becomes tighter over training and thus
can also serve as an estimate of I(Z;X|Y)at convergence (and therefore an estimate of I(Y′;X|Y)):
I(Z;X|Y) =Ep(y,z,x )/bracketleftbigg
log/parenleftbiggq(z|x,y)
p(z|y)/parenrightbigg/bracketrightbigg
=Ep(y,z,x )/bracketleftbigg
log/parenleftbiggq(z|x,y)
qy(z|y)/parenrightbigg/bracketrightbigg
−Ep(y)[DKL(p(z|y)∥qy(z|y))]
≤Ep(y,z,x )/bracketleftbigg
log/parenleftbiggq(z|x,y)
qy(z|y)/parenrightbigg/bracketrightbigg
=Ep(x,y)[DKL(q(z|x,y)∥qy(z|y))].(14)
Trainingqy(z|y)by minimizing this bound, also gives qy(z|y)≈p(z|y), since it squeezes the KL divergence
of the two.
Equation 13 with Equation 14 gives the Lagrangian loss to minimize for the transfer entropy bottleneck
algorithm:
Theorem 4. Under the independence assumptions implied by Equation 10 and 11, the minimum of the
following objective with respect to q,qy,dwill be at the CMNI point for Z,X,Y,Y′, for some hyperparameter
β >0:
TEB =Ep(x,y)[DKL(q(z|x,y)∥qy(z|y))]−βEp(y,z,y′)[log(d(y′|z,y))]. (TEB)
A.3 Transfer entropy bottleneck from a learned context encoding
Suppose one has already learned an encoding qy(·|y)forYthat provides a latent representation of Y. This
section will show one can use this encoding in place of Yin the TEB algorithm. This can come with many
practical advantages like computational efficiency, training efficiency, reuse of existing models, and rapid
prototyping when experimenting with multiple configurations or hyperparameter settings for the full TEB
model.
To avoid confusion with the latent space Zof the TEB model, we will refer to this latent encoding for Yas
C. We integrate the computation of Cinto the TEB model according to Figures 10c and 10d. As promised
by remark 1, here we invoke the fact that if Cis fixed when following the procedure (CMNI procedure), so
that Equation 12 holds when replacing “ Y” with “C”, then all of the results of the previous sections hold
when replacing “ Y” with “C”. Since we actually care about bottlenecking conditioned on Yrather than C,
and measuring the information I(Y′;X|Y)rather than I(Y′;X|C), we need the following lemma
22Published in Transactions on Machine Learning Research (02/2023)
Lemma 5. IfY′⊥Y|X,C,Y′⊥C|X,Y,Y′⊥C|Y, andqy(c|y)is such that I(C;Y′)is maximized, then
I(X;Y′|C) =I(X;Y′|Y).
In particular, one has I(C;Y′)≈I(Y;Y′)givesI(X;Y′|C)≈I(X;Y′|Y).
Proof.SinceY′⊥C|Y, the data processing inequality gives I(C;Y′)≤I(Y;Y′),and so maximizing I(C;Y′)
takes us to I(C;Y′) =I(Y;Y′).Now, expanding I(Y′;X,Y,C )two different ways gives
I(Y′;X,Y,C ) =I(Y′;X,Y ) +I(Y′;C|X,Y )
=I(Y′;X|Y) +I(Y′;Y) +I(Y′;C|X,Y ),
and
I(Y′;X,Y,C ) =I(Y′;X,C ) +I(Y′;Y|X,C )
=I(Y′;X|C) +I(Y′;C) +I(Y′;Y|X,C ).
Using the facts that I(C;Y′) =I(Y;Y′),I(Y′;Y|X,C ) = 0byY′⊥Y|X,C, andI(Y′;C|X,Y ) = 0by
Y′⊥C|X,Y, we get
I(Y′;X|Y) +I(Y′;Y) =I(Y′;X|Y) +I(Y′;Y) +I(Y′;C|X,Y )
=I(Y′;X|C) +I(Y′;C) +I(Y′;Y|X,C ) =I(Y′;X|C) +I(Y′;Y),
and therefore I(Y′;X|Y) =I(Y′;X|C).
The condition that I(C;Y′)≈I(Y;Y′)viaqy(c|y), means that we (approximately) do not lose information
aboutY′usingCinY’s place. As such, qy(z|y)is a perfectly reasonable prior for the latent Z|Yof the TEB
model (which implies that Zis in the same vector space as C). Thus, we design the following loss such that
we assume p(z|y) :=qy(z|y)by definition. Noting that we required Cand thusqyto be fixed, this allows us
to use a new version of the loss function (TEB) with C,qyas:
Theorem 6. Under the independence assumptions Y′⊥Z|X,C,Y′⊥C|Y,Y′⊥C|X,Y,Y′⊥X|Z,C
Y′⊥Y|X,C, andC∼qy(·|y)such thatI(C,Y′) =I(Y,Y′), the minimum of the following objective with
respect toq,dwill be at the CMNI point for Z,X,Y,Y′, for some hyperparameter β >0:
TEBc=Ep(x,y)[DKL(q(z|x,c)∥qy(z|y))]−βEp(y,z,y′)[log(d(y′|z,c))]. (TEBc)
Let us also suppose now that we have a decoder dy(y′|c). This will allow us to estimate I(Y′;Z|Y)separately,
at convergence of the decoders d,dy, even ifβorγare such that we do not compress Zenough to converge
to the CMNI point, which can be seen from the following proposition:
Proposition 7. If the distributions d(y′|z,c),dy(y′|c)are such that d(y′|z,c)≈p(y′|z,c)anddy(y′|c)≈
p(y′|c), then
I(Y′;Z|Y)≈Ep(y′,z,c)[log (d(y′|z,c))−log (dy(y′|c))].
Proof.The proof is immediate from the assumptions, and the fact that
I(Y′;Z|Y)≈I(Y′;Z|C) =Ep(y′,c,z)[log (p(y′|z,c))−log (p(y′|c))],
The above approximation is readily interpretable, since it is the expected difference between the log-
likelihoods of Y′, as estimated by danddy. In practice, the above estimates are good when one knows
the form of the output distribution and thus is able to correctly parameterize it with our approximate
distributions. For example, when the outputs are categorical as in a classification task.
23Published in Transactions on Machine Learning Research (02/2023)
Remark 3.Finally, although we are assuming the representation Cis not trained by the TEB loss and is
fixed, this assumption is only needed for the proportionality in Equation 12. If we let qybe optimized by the
TEB loss, but at the same time continue to also optimize qysuch thatI(Y′;C)stays maximized during the
optimization process, then H(Y′|C) =−I(Y′;C) +H(Y′)remains fixed, and so Equation 12 remains valid.
We find in practice that if we let qybe optimized by the TEB loss, but continue to train qyto maximize
I(Y′;C)inthesamewaythatitwaspretrained, I(Y′;C)doesindeedstaymaximizedandtheTEBprocedure
works as well.
24Published in Transactions on Machine Learning Research (02/2023)
B Additional implementation and experimental details
B.1 Additional model implementation details
All of the implementations were done using PyTorch (Paszke et al., 2019). The parameters of the models
were initialized using the default He initialization using normal distribution (He et al., 2015). We used Adam
optimizer (Kingma & Ba, 2015) with the default parameters; an initial learning rate of 10−4with no weight
decay. All the experiments were conducted on a single GPU with 48G memory. For all models the latent
dimension of 128 was used in all non convolutional hidden layers, including the hidden dimension size of the
LSTMs.
The implementation of the TEB model uses two initial encoders for the two streams qyandqx, both of
which feed into two separate LSTMs to aggregate information across the sequence. The encoder depends on
the input modality; on images the initial encoders are ResNet18, on a discrete modality like class integers,
we used an embedding vector lookup table (i.e. a torch.nn.Embedding ), and on time-series it is just the
identity mapping. The hidden state from the Ypathway LSTM is then linearly projected into a 128×3
dimensional vector. The first two 128dimensional chunks correspond to the means µyand log variances
log(σ2
y)for the 128dimensions of the prior qy(z|y), which are treated as being independent Gaussians in each
dimension. The last 128dimensional chunk is fed, along with the output of the initial Xstream encoding
qx, into an MLP with one hidden layer with a ReLU activation function (Nair & Hinton, 2010)3. The
outputs of this MLP are means µxand log variances log(σ2
x). These lead to the parametrization of the 128
independent Gaussian dimensions of q(z|x,y)as having means µ=µy+µx, and log variances log(σ2
x), and
the latent representation Zis then sampled from this distribution. We note here that we initialise the last
linear layer of the MLP computing µxandlog(σ2
x)so thatµxis the zero vector, and log(σ2
x)islog(10−7)in
every coordinate.
For rotating MNIST and needle in the haystack tasks, the decoder dis a positional encoding followed by a
convolutional network, which results in an image with dimensions (c,h,w ), wherec= 1for rotating MNIST
andc= 3for the needle in a haystack task. To create the positional encoding, we first arrange a spatial
grid of shape (4,h,w )where each channel is the distance to each image border for a given coordinate,
normalized into [0,1]. This grid is passed through a learnable linear layer which maps every coordinate into
128dimensional space, and results in a positional encoding of shape (128,h,w ). The sampled Zis copied
and repeated in each spatial dimension into a tensor with same shape as the positional encoding, and is
then added to the positional encoding. The output image is then obtained by passing this through a space
preserving convolutional component of the decoder, where all convolutions have stride 1and are padded
(with reflection padding) to preserve spatial dimensions. For example for rotating MNIST the decoder has
the following architecture:
(1): Conv2d(features=128, kernel_size=(7, 7)), BatchNorm, ReLU
(2): Conv2d(features=64, kernel_size=(5, 5)), BatchNorm, ReLU
(3): Conv2d(features=32, kernel_size=(5, 5)), BatchNorm, ReLU
(4): Conv2d(features=16, kernel_size=(5, 5)), BatchNorm, ReLU
(5): Conv2d(features=8, kernel_size=(3, 3))
(6): Conv2d(features=4, kernel_size=(3, 3)), BatchNorm, ReLU
(7): Conv2d(features=c, kernel_size=(3, 3))
, whereas for the needle in the haystack task, the decoder has all the layers above except for 5and6.
3For the datasets in our experiments, we found that using an MLP as such was not necessary at all for performance, and we
could just use qxto outputµx,log(σ2
x)directly. However, the MLP may be necessary when using a dataset where the processing
of theXstream depends on the particular Yexample. Thus we kept the MLP in the architecture as such for full generality.
25Published in Transactions on Machine Learning Research (02/2023)
For multi-component sinusoids task, the decoder dis neuralODE, implemented using the torchdiffeq
package. The dynamics of the ODE is parameterized by a MLP with one hidden layer with 50hidden
units and a ReLU activation function, and another MLP with the same hidden units is utilized to map
each latent state to the scalar output at each time-step. The decoder uses default dopri5ODE solver for
forward propagation, representing Runge-Kutta of order 5 of Dormand-Prince-Shampine. The ODE solver
is tasked to solve an initial value problem for a sequence of time-steps from 0to1second with an interval
of0.05second. To calculate the gradient, since the memory cost is not our concern, we chose to directly
backpropagate through the operations of the solver instead of using the adjoint method, for a reduce in the
running time. Lastly, the output sequence of the decoder starts with the last point of the input, as we found
it to be beneficial for the overall performance.
To optimize the TEB loss on these architectures we interpret our output as probabilistic, so that we can
calculate the output loglikelihood term in the TEB loss. In our interpretation we assume each pixel in
each channel is independently distributed with means equal to the output image, and when a variance
parameter is required for the distribution we fix the variance to 0.1. This interpretation of the output
can give reconstruction loss gradients proportional to common usual loss functions like mean squared error,
ℓ1, or various others, with the particular loss depending on the chosen output distribution. For rotating
MNIST and multi-component sinusoids we choose a Gaussian output distribution, giving reconstruction
gradients proportional to mean squared error. For the needle in a haystack task we could have used the
same interpretation, however training times were significantly longer than for rotating MNIST, so instead we
interpreted each pixel as a binary intensity, as we found this speed up training time without compromising
performance. This gives reconstruction loss proportional to a binary cross entropy with logits across pixels.
Also for the needle in a haystack experiments, to test the accuracy of the models in reconstructing the
correct color out of the 7color classes, we trained a color classifier on the training set. The color classifier
is a convolutional network with the following architecture:
(1): Conv2d(features=16, kernel_size=(5, 5), stride=1, padding=2), ReLU
(2): MaxPool2d(kernel_size=(2, 2), stride=2)
(3): Conv2d(features=32, kernel_size=(3, 3), stride=1, padding=1), ReLU
(4): MaxPool2d(kernel_size=(2, 2), stride=2)
(5): Conv2d(features=64, kernel_size=(3, 3), stride=1, padding=1), ReLU
(6): MaxPool2d(kernel_size=(2, 2), stride=2)
(7): Linear(output_size=7)
B.2 Rotating MNIST details and samples
The rotating MNIST task was generated from a base dataset of videos of 500examples of rotating MNIST
digits (and an additional 500validation and 500testing base videos). In a given base video sequence, a digit
starting upright rotates counter-clockwiseπ
4at every frame, for 6frames. The dataset was generated from
this base dataset by sampling 300possible random digit changes in the last frame of every possible triple
of consecutive frames in a base video, with a given switching probability of 0.5. Note that when a digit is
switched, the new digit has the same angle as the digit it is replacing. Also, although a new switched digit
is from a different example digit, the class of the switched digit may be the same as the previous one.
NotethatwecancalculatethegroundtruthtransferentropyforrotatingMNISTtaskasfollows. Firstly, note
that digits’ class labels are the only mutual information captured in I(Y′;X|Y), without loss of generality,
26Published in Transactions on Machine Learning Research (02/2023)
we useY′,XandYto denote their classes respectively. Note that the task is such that we have
p(y′|x,y) =p(x|y,y′) =/braceleftigg
1,ifx=y′
0,otherwise,
and we have equal probability for each digit class
p(y) =1
size(Y)=1
10.
We can decompose transfer entropy for rotating MNIST as follows:
I(Y′;X|Y) =/summationdisplay
x,y,y′p(y′,x,y)/bracketleftbigg
log/parenleftbiggp(y′|x,y)
p(y′|y)/parenrightbigg/bracketrightbigg
=/summationdisplay
x,y,y′p(y)p(y′|y)p(x|y′,y)/bracketleftbigg
log/parenleftbiggp(y′|x,y)
p(y′|y)/parenrightbigg/bracketrightbigg
=−/summationdisplay
y′,y:y′=xp(y)p(y′|y) [log (p(y′|y))]
=−/summationdisplay
y′,yp(y)p(y′|y) [log (p(y′|y))]
=−size(Y)
size(Y)/summationdisplay
y′p(y′|y=k) [log (p(y′|y=k))]
=−/summationdisplay
y′p(y′|y=k) [log (p(y′|y=k))](15)
wherekis any of the digit classes. Following the equation, the ground truth transfer entropy is 1.67689nats,
with
p(y′=k′|y=k) =/braceleftigg
0.55,ifk′=k
0.05,otherwise.
We arrange each input sample for the Ymoduleqyas a sequence of 2input frames of a rotating digit
preceding a possible switch. The input for the Xmoduleqxis a length 2sequence of the class of the digit
in the next frame. The target Y′is the next frame of the video where a possible digit switch may have
occurred.
For these experiments we train the TEB models on 3seeds, for every βfrom 0.1to1incremented by tenths,
with extra hundredths increments in the chaotic regime in [.1,.2]. End to end TEB models were trained for
10epochs whereas TEBcmodels were trained for 5epochs. The TEBcmodels used a Ymodule trained for
20epochs to represent Cwith next step prediction of Y′using CEB with γ= 1. In this case for maximum
training efficiency and difficulty for TEBc, we also fixed the decoder from the Ymodule and use it for the
reconstruction from Zin the full TEBcmodel.
27Published in Transactions on Machine Learning Research (02/2023)
(a) TEB training
(b) TEB testing
Figure 11: Plots of the information metric and reconstruction loglikelihood vs βincremented by tenths,
with extra hundredths increments in the chaotic regime in [.1,.2], for TEB testing. The green dashed
horizontal line in the left subfigure indicates the true transfer entropy for this dataset, from which we obtain
β∗as the specific value of βachieving the true transfer entropy, calculated as the interpolated value of
βfor whichI(Z;X|Y) =I(Y′;X|Y)on average from repeated trials. In the right subfigure, the output
loglikelihood of the model trained with the obtain β∗is marked using the red dashed vertical line. Note that
the variance of runs highly increases for β∈(.1,.2), but the models in this interval that found a solution
withI(Z;X|Y)≈I(Y′;X|Y)are the same models that achieved higher reconstruction performance.
28Published in Transactions on Machine Learning Research (02/2023)
(a) TEBcin training
(b) TEBcin testing
Figure 12: Same plots as Figure 11, but for TEBc
29Published in Transactions on Machine Learning Research (02/2023)
Figure 13: Sample TEB testing generation with β= 0.2.
30Published in Transactions on Machine Learning Research (02/2023)
Figure 14: Sample TEBctesting generation with β= 0.2. Quality is somewhat worse in comparison to TEB.
For discussion relevant to this, we refer the reader to the end of section 3.1, and also Figure 15.
31Published in Transactions on Machine Learning Research (02/2023)
Figure 15: Sample TEBctesting generation with β= 10, which helps speed up learning, and with a pre-
trainedYmodule which maximizes I(Y′;C)more than in Figure 14 and 12; Ymodule is trained with the
CEB method with γ= 100, as opposed to γ= 1for Figure 14. Note the cleaner generation in comparison
to Figure 14, especially when the Ymodule does the majority of the “work” in the case where digits do not
switch.
32Published in Transactions on Machine Learning Research (02/2023)
Figure 16: Sample TEBctesting generation for β= 1, trained on digits which always switch, and where the
Ymodule is pre-trained on rotating digits that never switch. This demonstrates the flexibility of TEB in
modulating a pre-trained model with a new modality. This TEB model was trained for 30epochs as opposed
to5for all other TEBcexperiments (or 10for all other TEB experiments), which improves reconstruction
quality and consistency.
B.3 Needle in a haystack details and samples
The needle in a haystack task base dataset consists of 142videos of two colored bouncing balls consisting of
6frames, in each of 7distinct color classes (and an additional 75validation, and 75testing, base videos in
each color class). The chosen color classes are all possible RGB triples with values in the alphabet {85,255},
except (85,85,85). The balls all have a minimum intensity of 85in all red, green, blue color channels so
as to bound the distance between images of different colors with the same ball configuration, thus adding
difficulty.
The dataset was generated from this base dataset by sampling 5possible random ball color changes in the
last frame of every possible 4-tuple of consecutive frames in a base video, where a new ball color class in the
last frame was sampled with probability 0.5. In addition to this, Ddistractor pixels (we generated datasets
for all ofD∈{5,10,15,20}distractors), and one true or “needle” pixel, was generated and incorporated for
each such 4frame video section according to the described procedure as follows.
First the color of each distractor pixel v∈{0,255}3is randomly (uniformly) chosen each frame from the
available 7color classes. Second, the needle pixel’s color in each frame is set to the color class of the balls
33Published in Transactions on Machine Learning Research (02/2023)
in the next frame. Note that both distractor and needle pixels have no minimum baseline brightening (so
intensity 85is replaced with 0). Third, for each pixel including the needle pixel, u1,u2,u3i.i.d∼U(0,102)
noise is sampled, and that pixel’s color (vi)i≤3is replaced with (|vi−ui|)i≤3.
To incorporate these D+ 1extra pixels, each frame of the 3initial bouncing ball frames is repeated D+ 1
times, the repetitions are concatenated in the channel dimension, and the D+ 1individual colored pixels
are added to the top left of the frame; One pixel is added in each of the D+ 1repeated RGB channel groups
in a random order.
Figure 17: Illustration of repetitions and stackings of the RGB channel groups for input XandYin the
needle in the haystack task.
We arrange each input sample for the Xmoduleqxas such a sequence of 3input frames. The input for the
Ymoduleqyis this same sequence, but with the RGB channel group containing the needle pixel removed.
The target Y′is the next frame of the video of the bouncing balls, where a possible color class switch may
have occurred.
We trained all models on 8seeds and, in each seed, tested the model with the best color prediction accuracy
of the color classifier on the validation set. The TEB model was trained for 450epochs on each dataset. We
found the deterministic models stabilised early (Though as we see, the deterministic models are overfitting
to the distractor pixels for larger numbers of distractors); Validation and training metrics stabilised well
within 300epochs for 5distractors (usually within 200epochs), and well within 400for all other number
of distractors (usually well within 300epochs), and so we stopped training for deterministic models at 300
epochs for 5distractors, and 400epochs for all other distractor numbers.
For the TEB model, we conducted a hyperparameter search for values of βbetween 1and1012, and found
that since the task difficulty varied greatly with the number of distractors, different values for βcould be
best for the different datasets. Also, training difficulty meant that the best validating values of βwere quite
high relative to the rotating MNIST experiment. For example, for 20distractors we found that values of β
between 103and105achieve the best validation performance. To learn a more bottlenecked representation
we also gradually decreased βduring training from the selected hyperparameter.
To generate the results of Table 1 and 2 with the number of distractors D∈{5,15,20}, we used an initial
β= 105, decreasing to 104at epoch 300, and further decreasing to 103at epoch 400. ForD= 10we used
an initialβ= 103, decreased to 102at epoch 350, and decreased further to 10at epoch 400.
For the CEB model on 5distractors task, We also conducted a hyperparameter search for γbetween 0.5and
100and found that γ= 10achieved the best validating performance.
34Published in Transactions on Machine Learning Research (02/2023)
Figure 18: Sample TEB training generation for 20distractors
35Published in Transactions on Machine Learning Research (02/2023)
Figure 19: Sample TEB testing generation for 20distractors
36Published in Transactions on Machine Learning Research (02/2023)
Figure 20: Sample TEB testing generation for 20distractors, showing prediction errors. At times the color
prediction is missed, or balls generated are uneven/irregular.
Lastly, to demonstrate the performance of TEBcon this challenging dataset, we trained CEB with γ= 1
on a separate dataset of 5distractors with the bouncing balls colors never change for 1000epochs, and
used it as the pre-trained Y module to be incorporated into the TEBcformulation. This is in essence a
37Published in Transactions on Machine Learning Research (02/2023)
simpler task than the normal needle in the haystack task, and the objective of the pre-trained Y module is
to predict the localisation of the balls in the target frame while keeping the same color as in the histories.
We trained both TEBcwithβ= 10and “deterministic” with the same pre-trained Y module fixed, on the
5distractors dataset with 0.5probability of switching the color, but both models suffered severe overfitting
quickly. Therefore we increased the size of the dataset by 20times, and trained both models on it instead
for8seeds. We found the testing color accuracy of TEBcis 93.48±3.93 %, while that of the deterministic
baseline is 86.13 ±4.67 %. Therefore, introducing an information bottleneck is even more advantageous
with a fixed pre-trained Y module, compared to those without as reported in Table 1. One potential reason
for the deterministic baseline’s poorer performance is that the pre-trained Y module might just not be good
enough for it. Alternative reason could be that it is still easier to overfit in this scenario, since here the
deterministic model has all the information in the X stream just to find the color changes, without any form
of regularizations. This demonstrates that TEB has the capability of incorporating a pre-trained model in
this challenging task. However, training TEBcis considered as a more difficult task than TEB.
B.4 Multi-component sinusoids details and samples
The multi-component sinusoids task dataset consists of 30k,5k, and 5k time-series signal of 6sec-
onds, for training, validation, and testing, respectively. The sampling rate is 20Hz, corresponding to
in total 120points per signal. Each time-series signal has 5sinusoidal waves with distinct frequencies
f∈{0.2,0.4,0.6,0.8,1.0}Hz and starting point randomly sampled in [−1,1]as sub-components, and the
order of the 5components is also random. At the end of the 5th seconds, with 0.5probability, the frequency
of a randomly chosen component will be randomly resampled into one of the 5frequencies. The objective
of the task is to capture the change in frequency and accurately predict the last 20points of extrapolations
given the past.
Each sinusoidal wave of the time-series signal is generated by unrolling a 2-dimensional continuous time
linear autonomous dynamical system with purely imaginary eigenvalues.
dy
dt=/bracketleftbigg
0 2πf
−2πf 0/bracketrightbigg
y (16)
With initial condition y0lying on the unit circle, the trajectory of y(t)is clockwise rotation on the unit circle,
and we used the the second dimension of the trajectory’s Cartesian coordinates as the generated sinusoidal
wave for frequency f. When there is a switch in frequency, the future trajectory will be unrolled according
to the updated dynamical system. The dataset was generated by unrolling the dynamical system using
dopri5ODE solver of package torchdiffeq . The frequencies of all sinusoidal waves were recorded during
the dataset generation, and to add difficulty, we added U(−0.15,0.15)noise to the recorded frequencies
post-generation.
We arranged each input sample for the Ymoduleqyas a length 100averaged signals of the 5sinusoidal
waves preceding a possible switch. The input for the Xmoduleqxas a length 100sequence of the frequencies
of all waves in the next time-step. The target Y′is the length 21averaged signal, including the length 20
sequence after a possible frequency switch may have occurred, as well the last time-step of Y.
Wetrainedallmodelson 3seedsand, ineachseed, testedthemodelwiththebestreconstructionloglikelihood
on the validation set. All models were trained for 2000epochs. We found that most of the runs reached best
validation performance within 500epochs, after which, the validation performance of TEB and deterministic
model stabilised well, while that of latent ODE and deterministic LSTM degraded due to more severe
overfitting.
For the TEB and latentODE-VIB, we conducted hyperparameter search for values of βbetween 10and1000
and between 10and100, respectively, and we found that 1000for TEB and 50for latentODE-VIB achieved
the best validating performance. In addition, similar as in the needle in the haystack task, with the initial
β= 1000, we adopted the schedule of gradually decreasing βby100every 100epochs starting epoch 500,
until it reached β= 200at epoch 1200. However, unlike the cases in the needle in the haystack task, the
schedule was less effective, and we only found that it yielded an improving validating performance (only by
a small margin) in one run. Therefore, the schedule might not be necessary for this task.
38Published in Transactions on Machine Learning Research (02/2023)
Figure21: Sampledatainmulti-componentsinusoidsdataset. Thefirstrowshowsthesignalusedfortraining
in this task, made from averaging all sub-components from the other rows. Shown in the subsequent rows,
the sub-components are sinusoidal waves with distinct frequencies, with 0.5probability to switch frequency
at time-step 100. This example demonstrates a switch in the second sub-component from 0.2Hz to 1Hz.
39Published in Transactions on Machine Learning Research (02/2023)
Figure 22: Sample TEB testing time-series sequence generation when there is switch at prediction time-step
100, in which the switches are shown in green frame, showing prediction error. Note the large prediction
deviation from the target in the first row. This is more rarely seen case when there is error in predictions,
more frequently, the error occurs similar to the 4th row, with accumulated error at later time-steps.
40Published in Transactions on Machine Learning Research (02/2023)
C Discussion on TE estimation vs TEB
Here we provide a discussion contrasting methods which estimate transfer entropy with the goals of our TEB
method.
The main goal of our paper is learning a transfer entropy bottlenecked representation for prediction, and
we provide an estimate of the information transfer of the model only when converged to CMNI point in
rotating MNIST task. On the other hand, there are existing methods which focus on the estimation of
conditional mutual information and transfer entropy (Ursino et al., 2020; De La Pava Panche et al., 2019;
Zhang et al., 2019). Generally, these methods are algorithms which estimate by: kernel density estimation
methods, targetingaDonskerVaradhanbound, orbyamutualinformationestimationapproachtoseparately
estimate each component I(A;B,C),I(A;C), ofI(A;B|C) =I(A;B,C)−I(A;C).
In principle the bounds for I(Z;X|Y)orI(Y′;Z|Y)we derive may be substituted in our algorithm by
any estimator for TE, though as the goal of this paper is to introduce a variational learning technique
for learning a transfer entropy bottlenecked representation, examining the combination of these methods is
beyond the scope of this paper. In practice the price paid for using a separate learned information estimator
tobottlenecktherepresentationaretheextraparametersneededfortheneuralestimationoftheinformation,
and a potentially higher variance or less tractable optimisation procedure. For example, Zhang et al. (2019)
showed that utilizing directly MINE (Belghazi et al., 2018) estimator for transfer entropy would introduce
more variance to the loss. In addition, ITENE algorithm proposed in (Zhang et al., 2019) maximizes and
minimizes different MINE estimators for each of the chain-rule-decomposed parts of the conditional mutual
information, and one of these parts will not coincide with the direction of optimization of the TEB, and
more generally, conditional IB procedure. For example for I(Z;X|Y) =I(Z;X,Y )−I(Z;Y), ITENE
estimatesI(Z;X|Y)via maximizing MINE estimator of I(Z;Y)and minimizing that of I(Z;X,Y ), but IB
would require the resulted I(Z;X|Y)estimator to be minimized as a whole. Therefore one would follow
a more complicated optimization procedure which alternates maximizing the statistics network parameters
with respect to the information estimate, and the minimizing the bottleneck parameters with respect to
the information estimate. This could increase the variance of learning, and may require more steps of
optimizationfortheITENEphaseearlyinlearning, toensuretheinformationestimateprovidesameaningful
learning signal for the bottleneck.
41