A Simple and Optimal Approach for
Universal Online Learning with Gradient Variations
Yu-Hu Yan, Peng Zhao, Zhi-Hua Zhou
National Key Laboratory for Novel Software Technology, Nanjing University, China
School of Artificial Intelligence, Nanjing University, China
{yanyh, zhaop, zhouzh }@lamda.nju.edu.cn
Abstract
We investigate the problem of universal online learning with gradient-variation
regret. Universal online learning aims to achieve regret guarantees without the
prior knowledge of the curvature of the online functions. Moreover, we study the
problem-dependent gradient-variation regret as it plays a crucial role in bridging
stochastic and adversarial optimization as well as game theory. In this work, we
design a universal approach with the optimal gradient-variation regret simultane-
ously for strongly convex, exp-concave, and convex functions, thus addressing an
open problem highlighted by Yan et al. [2023]. Our approach is simple since it is
algorithmically efficient-to-implement with a two-layer online ensemble structure
and only 1gradient query per round, and theoretically easy-to-analyze with a
novel and alternative analysis to the gradient-variation regret. Concretely, previous
works on gradient variations require controlling the algorithmic stability, which
is challenging and leads to sub-optimal regret and less efficient algorithm design.
Our analysis overcomes this issue by using a Bregman divergence negative term
from linearization and a useful smoothness property.
1 Introduction
Online convex optimization (OCO) models a sequential T-round game between an online learner
and the environments [Hazan, 2016, Orabona, 2019]. In each round t∈[T], the learner selects a
decision xtfrom a convex compact set X ⊆Rd. Simultaneously, the environments adversarially
choose a convex loss function ft:X 7→ R. Subsequently, the learner suffers a loss of ft(xt),
receives feedback on the function ft(·), and updates her decision to xt+1. In OCO, the learner aims
to optimize the game-theoretical performance measure known as regret [Cesa-Bianchi and Lugosi,
2006], which is formally defined as
REGT≜TX
t=1ft(xt)−min
x∈XTX
t=1ft(x). (1.1)
It represents the learner’s excess cumulative loss compared with the best fixed comparator in hindsight.
In OCO, function curvatures play an important role in the best attainable regret bounds. Traditional
studies examine three types of curvatures: convexity, exp-concavity, and strong convexity. Specifically,
for convex functions, online gradient descent (OGD) achieves O(√
T)regret [Zinkevich, 2003]; for
α-exp-concave functions, online Newton step assuming αis known obtains O(d
αlogT)regret [Hazan
et al., 2007]; and for λ-strongly convex functions, OGD with known λattains O(1
λlogT)[Hazan
et al., 2007]. These results are shown to be minimax optimal [Ordentlich and Cover, 1998, Abernethy
et al., 2008]. Recent studies further strengthen them by introducing two levels of adaptivity .
∗Correspondence: Peng Zhao <zhaop@lamda.nju.edu.cn >
38th Conference on Neural Information Processing Systems (NeurIPS 2024).Table 1: Comparison with existing results. The second column shows the regret bounds for strongly convex,
exp-concave, and convex functions, following the O(·)-notation. Note that we only list universal guarantees
related to the gradient variation VTor the time horizon T. Each gradient-variation bound can directly apply a
corresponding small-loss regret in analysis, which is formally stated in Theorem 2 and omitted here for clarity.
We treat the log log Tfactor as a constant and omit it. “# Gradient” is the number of gradient queries in each
round, where “1” represents exactly one gradient query. And “# Base” stands for the number of base learners.
WorksRegret Bounds Efficiency
Strongly Convex Exp-concave Convex # Gradient # Base
van Erven and Koolen [2016] dlogT d logT√
T 1 logT
Wang et al. [2019] logT d logT√
T 1 logT
Zhang et al. [2022a] logVT dlogVT√
T logT logT
Yan et al. [2023] logVT dlogVT√VTlogVT 1 (logT)2
Ours logVT dlogVT√VT 1 logT
High-level Adaptivity to Unknown Curvatures. Traditional studies require function curvature
information in advance to select suitable algorithms for provable bounds. However, the information
could be hard to access in real-world applications. To this end, a line of research aims to design a
single universal algorithm that does not require the curvature information while achieving the same
regret guarantees as if knowing it, achieving the adaptivity to unknown curvatures. The pioneering
work of MetaGrad [van Erven and Koolen, 2016] proposed carefully designed surrogate functions and
achieved O(d
αlogT)forα-exp-concave functions and O(√
T)for convex functions. Subsequently,
Wang et al. [2019] obtained the optimal O(1
λlogT)regret for λ-strongly convex functions, while
maintaining the optimal rates in the other cases. Another remarkable progress of Zhang et al. [2022a]
proposed a flexible framework with simplified analyses and further enhanced the minimax results
using smoothness. We provide a detailed introduction of this work in Section 2.2.
Low-level Adaptivity to Gradient Variation. Although the regret guarantees based on the time
horizon Tare optimal in the minimax sense, in this work we are interested in achieving the gradient-
variation regret [Chiang et al., 2012, Yang et al., 2014], which replaces the dependence of the time
horizon Tby the gradient variation quantity defined in the following:
VT≜TX
t=2sup
x∈X∥∇ft(x)− ∇ft−1(x)∥2. (1.2)
Under smoothness assumptions, the minimax regret can be improved to O(1
λlogVT),O(d
αlogVT),
andO(√VT)forλ-strongly convex, α-exp-concave, and convex functions, respectively. In this work,
continuing previous gradient-variation online learning results [Zhao et al., 2020, 2024, Zhang et al.,
2022b, Chen et al., 2023], we focus on the gradient-variation regret for the following reasons: (i)
gradient-variation bounds safeguard the minimax guarantees. Besides, as demonstrated by Zhao
et al. [2024], the gradient-variation regret is more fundamental than another well-known problem-
dependent quantity known as the small loss FT≜minx∈XP
t≤Tft(x)[Srebro et al., 2010, Orabona
et al., 2012] since gradient-variation regret can imply small-loss bounds directly in analysis; (ii)the
gradient variation plays a crucial role in bridging adversarial and stochastic optimization [Sachs
et al., 2022]; and (iii)the gradient-variation regret can be used to achieve fast rates in multi-player
games [Syrgkanis et al., 2015, Zhang et al., 2022b]. More detailed explanations of the importance of
achieving such adaptivity are provided at the end of this section.
Motivated by the aforementioned two levels of adaptivity, we focus on the problem of achieving
universal gradient-variation regret, i.e., designing a single universal approach with gradient-variation
regret across different curvature types without the prior knowledge of them. For this problem, Zhang
et al. [2022a] achieved partial results of O(1
λlogVT),O(d
αlogVT),O(√
T)forλ-strongly convex,
α-exp-concave, and convex functions, respectively. Subsequently, Yan et al. [2023] proposed a
carefully designed three-layer online ensemble approach to stabilize the algorithm and improved the
convex result to O(√VTlogVT), achieving the first universal gradient-variation guarantee. Although
optimal for strongly convex and exp-concave functions, their results still exhibit a gap with the
optimal O(√VT)regret in the convex case. Here “optimal” refers to matching the best known results
with curvature information since problem-dependent lower bound cannot be easily obtained. The
only lower bound we are aware of is Ω(√VT)for convex functions [Yang et al., 2014, Remark 5].
2To handle the uncertainty, online ensemble is commonly employed and proven effective in enhancing
the robustness [Zhou, 2012, Zhao, 2021], such as adaptive regret minimization [Hazan and Seshadhri,
2007, Daniely et al., 2015, Zhang et al., 2019], dynamic regret minimization [Zhang et al., 2018,
Zhao et al., 2020, 2024], and universal online learning [van Erven and Koolen, 2016, Zhang et al.,
2022a, Yan et al., 2023]. Concretely, an online ensemble algorithm contains multiple base learners
for exploring the environments and a meta learner for ensemble. In universal online learning, base
learners make guesses on the curvature information, and the meta learner tracks the best base learner
(i.e., with the most accurate guess) on the fly.
Due to the deployment of the online ensemble framework, computational efficiency has become a
point of concern, with two essential factors. The first is the number of base learners , because each
independently runs an online learning algorithm that involves time-consuming gradient computations
and projections. The second factor is the number of gradient queries , especially when the gradient
evaluation is costly, e.g., in nuclear norm optimization [Ji and Ye, 2009] and mini-batch optimiza-
tion [Li et al., 2014]. In universal online learning, an “efficient” algorithm is expected to adopt only
O(logT)base learners, which is inherent to the online ensemble design, and only 1 gradient query
per round, matching the gradient query complexity of standard OGD. In terms of this metric, Zhang
et al. [2022a] employed O(logT)base learners, but required O(logT)gradient queries per round.
Yan et al. [2023] used only 1gradient query per round but required O((log T)2)base learners (caused
by their three-layer algorithm design), resulting in reduced efficiency.
Results. Motivated by the above considerations of optimality and efficiency, in this work, we
propose a simple universal approach that achieves the optimal O(1
λlogVT),O(d
αlogVT), and
O(√VT)regret simultaneously for λ-strongly convex, α-exp-concave, and convex functions, and
isefficient with one gradient query per round and O(logT)base learners, resolving a major open
problem highlighted by Yan et al. [2023]. We summarize our theoretical results in Theorem 1, and
compare our results with existing ones in Table 1. Furthermore, we validate the effectiveness of our
approach by: (i)showing that our universal gradient-variation regret directly implies the optimal
universal small-loss regret in analysis without any algorithm modifications; and (ii)applying them to
the stochastically extended adversarial (SEA) model [Sachs et al., 2022], an intermediate framework
between stochastic and adversarial optimization. We achieve the same state-of-the-art guarantees as
Chen et al. [2024], but without curvature information. The details are provided in Section 4.
Techniques. Our technical contributions include two simple and novel analyses. First, the key to
gradient-variation regret is to analyze its empirical version, formally, ∥∇ft(xt)− ∇ft−1(xt−1)∥2
2.
The previous approach to addressing this term involves controlling the algorithmic stability ∥xt−
xt−1∥2
2, which is highly challenging in universal online learning, leading to sub-optimal results and
less efficient algorithm design [Yan et al., 2023]. In this work, we overcome this issue via a novel
analysis by a useful smoothness property and a Bregman divergence negative term from linearization,
where the latter is inspired by the recent advance in stochastic optimization [Joulani et al., 2020].
Second, we adopt the surrogate functions proposed by Yan et al. [2023] to reduce the gradient query
complexity, and provide a novel analysis for the empirical gradient variation based on the surrogates.
A technical comparison with previous works is provided in Section 4.
Organization. The rest of the paper is structured as follows. Section 2 introduces preliminaries and
a general framework for universal online learning. Section 3 presents our efficient approach with the
optimal universal gradient-variation regret. Section 4 presents the implication and application of our
results. Finally Section 5 concludes the paper. All the proofs can be found in the appendices.
2 Preliminaries
In this section, we introduce some preliminary knowledge, including notations, assumptions, defini-
tions, and a general framework of universal online learning.
2.1 Notations, Assumptions, and Definitions
For simplicity, we use ∥ · ∥ for∥ · ∥ 2by default and use a≲bora=O(b)if there exists a constant
C <∞such that a/b≤C. In the following, we introduce the assumptions used in this work.
Assumption 1 (Boundedness) .For any x,y∈ X ⊆ Rd, the domain diameter satisfies ∥x−y∥ ≤D,
and for t∈[T], the gradient norm of the online functions is bounded as ∥∇ft(x)∥ ≤G.
3Assumption 2 (Smoothness) .For each t∈[T], the online function ft(·)isL-smooth, i.e., ∥∇ft(x)−
∇ft(y)∥ ≤L∥x−y∥holds for any x,y∈Rd.
Both assumptions are common in the literature. Specifically, the boundedness assumption is widely
used in OCO [Hazan, 2016]. And the smoothness assumption is essential for first-order algorithms in
achieving the gradient-variation regret [Chiang et al., 2012]. Note that here Assumption 2 requires
smoothness on the whole Rdspace and can be relaxed to a slightly larger domain than X, formally,
X+≜{x+b|x∈ X,b∈G/L·B}, where B≜{x| ∥x∥ ≤1}is a unit ball. We defer the details of
this relaxed smoothness requirement and its derivation to Appendix A. In the following, we provide
formal definitions of strong convexity and exp-concavity.
Definition 1. For any x,y∈ X, a function f(·)isλ-strongly convex if f(x)−f(y)≤ ⟨∇ f(x),x−
y⟩−λ
2·∥x−y∥2;f(·)isα-exp-concave if f(x)−f(y)≤ ⟨∇ f(x),x−y⟩−α
2·⟨∇f(x),x−y⟩2.
Note that the formal definition of β-exp-concavity is that exp(−βf(·))is concave. Under As-
sumption 1, β-exp-concavity implies Definition 1 with α=1
2·min{1/(4GD), β}[Hazan, 2016,
Lemma 4.3]. Therefore, we adopt Definition 1 as an alternative definition of exp-concavity for clarity.
2.2 A General Framework for Universal Online Learning
In this part, we present a general framework of universal online learning [Zhang et al., 2022a, Yan
et al., 2023]. Formally, we study the problem where the learner lacks the prior knowledge of curvature
information, including (i)curvature type: convexity, exp-concavity, or strong convexity; and (ii)
curvature coefficient: exp-concavity αor strong convexity λ. Without loss of generality, we focus
on the case of α, λ∈[1/T,1]. Ifα, λ < 1/T, even the optimal minimax results — O(d
αlogT)for
exp-concave functions and O(1
λlogT)for strongly convex functions [Hazan et al., 2007] – become
linear in T, rendering the regret bounds vacuous. On the other hand, if α, λ > 1, we can simply treat
them as α, λ= 1, only making the regret worsen by an ignorable constant factor. This simplification
is also adopted by Zhang et al. [2022a], Yan et al. [2023].
To handle the uncertainty of curvatures, an online ensemble structure is usually employed, with
multiple base learners exploring the environments and a meta learner tracking the best base learner.
More specifically, to deal with the unknown curvature coefficients αandλ, we discretize them into
the following candidate pool [Zhang et al., 2022a]:
H≜{1/T,2/T,4/T, . . . , 2n−1/T}, (2.1)
whose size is n=⌈log2T⌉+ 1 = O(logT). It can be proved that the discretized candidate pool H
can approximate the continuous value of αorλwith negligible constant errors. By doing this, it is
natural to design three distinct groups of base learners:
(i)strongly convex base learners: nin total, each of which implements the algorithm for
strongly convex functions with a guess λi∈ H of the strong convexity coefficient λ;
(ii)exp-concave base learners: nin total, each of which implements the algorithm for exp-
concave functions with a guess αi∈ H of the exp-concavity coefficient α;
(iii) convex base learner: only one, it runs the algorithm for convex functions.
In total, there are N≜(2n+ 1) = O(logT)base learners with a two-layer structure, which is for
now necessary in this problem. The best base learner is the one with the right guess of the curvature
type and the closest guess of the curvature coefficient. Taking λ-strongly convex functions as an
example, the guessed coefficient of the best base learner (indexed by i⋆) satisfies λi⋆≤λ≤2λi⋆.
Denoting by xt,ithe decision generated by the i-th base learner at the t-th round, pt,ithe weight of
the meta learner on the i-th base learner, an online ensemble method outputs the final decision as
xt=P
i∈[N]pt,ixt,i. This forms a general framework for universal online learning and it remains
to select suitable algorithms and loss functions for the meta and base learners. We illuminate our
algorithm design in Section 3.1.
3 Our Approach
In this section, we present our approach for universal online learning with gradient-variation regret.
Section 3.1 presents the overall procedure of our proposed algorithm. Subsequently, we outline
4Algorithm 1 A Simple Approach for Universal Online Learning with Gradient Variations
Input: Curvature coefficient pool Hdefined in (2.1), base learner number N
1:Initialize :A— meta learner running O PTIMISTIC -ADAPT -ML-P ROD with losses {ℓt}T
t=1,
optimisms {mt}T
t=1, and learning rates {εt}T
t=1, where ℓt= (ℓt,1, . . . , ℓ t,N),
mt= (mt,1, . . . , m t,N),andεt= (εt,1, . . . , ε t,N)for each t∈[T];
{Bi}i∈[N]— base learners as specified in Section 2
2:fort= 1toTdo
3: Submit xt=P
i∈[N]pt,ixt,i, suffer ft(xt), and observe the gradient ∇ft(xt)
# Meta Update:
4:Aupdates to pt+1= (pt+1,1, . . . , p t+1,N)withℓt,i=⟨∇ft(xt),xt,i⟩,εt,igiven in (B.5) ,
andmt,i=⟨∇ft−1(xt−1),xt−xt,i⟩for the convex base learner and mt,i= 0otherwise
# Base Update:
5: Construct surrogates hsc
t,i(·),hexp
t,i(·),hc
t,i(·)defined in (3.1) using only ∇ft(xt)
6: Send the surrogate functions to Bifor update and obtain xt+1,ifor each i∈[N]
7:end for
our two key technical components: Section 3.2 presents a novel analysis to handle the empirical
gradient variation, and Section 3.3 introduces surrogate functions to improve efficiency and provides
a corresponding analysis for the empirical gradient variation defined on surrogates. We finally provide
the optimal universal gradient-variation regret guarantees in Section 3.4.
3.1 Overall Algorithm
In this part, we present our simple approach for universal online learning with gradient variations,
summarized in Algorithm 1. Basically, it follows a two-layer online ensemble structure. Base learners
are implemented using the preliminary configurations given in Section 2.2 and on carefully designed
surrogate functions. The meta learner runs OPTIMISTIC -ADAPT -ML-P ROD [Wei et al., 2016] on
linearized losses. We specify the algorithmic details below.
In Line 3, the learner makes a weighted combination of the base learners’ decisions {xt,i}i∈[N]using
the meta learner’s weights pt= (pt,1, . . . , p t,N), submits the final decision xt, suffers a loss ft(xt),
and receives a single ∇ft(xt)as the gradient feedback, using only1gradient query per round.
Meta Algorithm. In Line 4, the meta learner uses OPTIMISTIC -ADAPT -ML-P ROD [Wei et al.,
2016] to update the weights by the following rule:
pt+1,i∝εt,i·exp(εt,imt+1,i)·Wt,i, Wt,i= 
Wt−1,i·exp 
εt−1,irt,i−ε2
t−1,i(rt,i−mt,i)εt,i
εt−1,i.
Specifically, denoting by ℓt,i≜⟨∇ft(xt),xt,i⟩the loss of the i-th dimension, the meta algorithm
inputs: rt,i=⟨ℓt,pt⟩ −ℓt,i, the instantaneous regret; εt,i, a time-varying learning rate; and mt,i, an
estimation of the true loss of the t-th round (the choice of optimisms will be shown later). The meta
algorithm then outputs the weights pt+1= (pt+1,1, . . . , p t+1,N)of the next round.
With appropriate learning rates (B.5) ,OPTIMISTIC -ADAPT -ML-P ROD achieves an optimistic second-
order bound ofP
t≤Trt,i≤ O(p
logNP
t(rt,i−mt,i)2+ log N), where the logNfactor is
negligible since the base learner number Nequals O(logT)and we can treat O(log log T)as a
constant [Luo and Schapire, 2015]. The formal guarantee of OPTIMISTIC -ADAPT -ML-P ROD is
deferred to Lemma 2 in the appendix. In our problem, the instantaneous regret rt,i=⟨ℓt,pt⟩−ℓt,i=
⟨∇ft(xt),xt−xt,i⟩. Thus we choose mt,i=⟨∇ft−1(xt−1),xt−xt,i⟩for the convex base learner
andmt,i= 0otherwise (i.e., for exp-concave and strongly convex base learners).2By doing this,
we can upper-boundP
t⟨∇ft(xt),xt−xt,i⟩byO(pP
t⟨∇ft(xt)− ∇ft−1(xt−1),xt−xt,i⟩2)for
the convex base learner and by O(pP
t⟨∇ft(xt),xt−xt,i⟩2)otherwise. Later in Section 3.3, we
illuminate how such results could benefit the final regret guarantees.
2Although xtis unknown when using mt,i, we only need the scalar value of ⟨∇ft−1(xt−1),xt⟩, which is
bounded and can be efficiently solved via a one-dimensional fixed-point problem of ⟨∇ft−1(xt−1),xt(z)⟩=z.
xtis a function of zbecause xtrelies on pt,i,pt,irelies on mt,iandmt,irelies on z. Interested readers can
refer to Section 3.3 of Wei et al. [2016] for more details.
5Base Algorithm. In Line 5, we adopt carefully designed surrogate functions for different types
of base learners to reduce the gradient query complexity [Yan et al., 2023]. Specifically, strongly
convex, exp-concave, and the convex base learners run on the surrogate functions below respectively:
hsc
t,i(x)≜⟨∇ft(xt),x⟩+λi
4∥x−xt∥2, hexp
t,i(x)≜⟨∇ft(xt),x⟩+αi
4⟨∇ft(xt),x−xt⟩2,(3.1)
andhc
t,i(x)≜⟨∇ft(xt),x⟩, where λi, αiare selected from the candidate coefficient pool Hin(2.1) .
We emphasize that the surrogate functions require only 1gradient query ∇ft(xt)per round. Finally,
in Line 6, the i-thbase learner Biupdates the decision to xt+1,iusing optimistic online mirror descent
(OOMD) [Rakhlin and Sridharan, 2013], which is general and covers many algorithms of interest,
such as OGD and online Newton step. For each curvature type (convexity, exp-concavity, or strong
convexity), we adopt a correspondingly configured OOMD as the base learner.
As for previous works, Zhang et al. [2022a] adopted ADAPT -ML-P ROD [Gaillard et al., 2014] as
the meta learner, which does not incorporate optimisms and thus is impossible to achieve gradient-
variation regret for convex functions, and operates on the original loss function ft(·)for base learners,
which leads to a less efficient gradient query complexity of O(logT)per round. Yan et al. [2023]
used a two-layer meta algorithm MSMWC-M ASTER [Chen et al., 2021] as the meta learner, resulting
in a three-layer ensemble structure, which is also not efficient enough. Compared with approaches
above, our Algorithm 1 is simpler and more efficient as it requires O(logT)base learners and only 1
gradient query in each round. We emphasize that our contributions mainly lie in the technical aspects
showing that although simple, our approach can achieve the optimal universal gradient-variation
regret, which is accomplished via two novel analytical components.
3.2 Novel Analysis on Empirical Gradient Variations
In this part, we provide a novel analysis of the gradient-variation regret. For clarity, we illustrate
from the lowest level — as we only use one gradient ∇ft(xt)in the t-th round, to obtain the
gradient variation VTdefined in (1.2) , it is necessary to first attain its empirical version ¯VT≜P
t≤T∥∇ft(xt)− ∇ft−1(xt−1)∥2. Previous studies decompose this term into two parts:
∥∇ft(xt)− ∇ft−1(xt−1)∥2≲∥∇ft(xt)− ∇ft−1(xt)∥2+∥∇ft−1(xt)− ∇ft−1(xt−1)∥2
≤supx∈X∥∇ft(x)− ∇ft−1(x)∥2+L2∥xt−xt−1∥2,
using smoothness (i.e., Assumption 2). Aggregating the first term over Trounds leads to the de-
sired VTquantity and the remaining challenge is to control the algorithmic stability ∥xt−xt−1∥2.
Consequently, since each decision is a weighted combination of base learners’ decisions (i.e.,
xt=P
i≤Npt,ixt,i), the algorithmic stability is difficult to control. To this end, Yan et al. [2023]
decomposed the stability term in the following way:
∥xt−xt−1∥2≲NX
i=1pt,i∥xt,i−xt−1,i∥2+∥pt−pt−1∥2
1. (3.2)
Consequently, for the first term, the authors injected correction terms to the meta learner follow-
ing Zhao et al. [2024]. To cancel the second term, the meta algorithm must include a corresponding
negative stability term in its analysis, while achieving an optimistic second-order bound simultane-
ously. To the best of our knowledge, the only feasible algorithm satisfying both requirements is the
two-layer meta algorithm MSMWC-M ASTER [Chen et al., 2021], which leads to a three-layer online
ensemble structure and therefore affects the efficiency. Besides, it attains a second-order bound of the
formO(p
QT,ilogQT,i), where QT,i≜P
t(ℓt,i−mt,i)2, which causes the sub-optimality of the
regret guarantees with an additional logarithmic factor in the results of Yan et al. [2023].
In this work, we handle the empirical gradient variation alternatively via a novel and simple analysis
with two key parts: (i)a negative term arising from linearization; and (ii)a useful smoothness property.
First, we observe that the instantaneous regret can be transformed as:
ft(xt)−ft(x⋆) =⟨∇ft(xt),xt−x⋆⟩ − D ft(x⋆,xt), (3.3)
where x⋆∈arg minx∈XP
tft(x)andDf(x,y)≜f(x)−f(y)− ⟨∇ f(y),x−y⟩is the Bregman
divergence associated with function f(·). The last term is a negative term from linearization , which
6can be seen as the compensation by treating a convex function as a linear one. Previous studies on
the gradient-variation regret usually omit this term, while we show below that this negative term
helps to achieve a much simpler analysis of the empirical gradient variation. Second, we introduce
a useful property of smoothness , formally introduced below.
Proposition 1 (Theorem 2.1.5 of Nesterov [2018]) .f(·)isL-smooth over Rdif and only if
∥∇f(x)− ∇f(y)∥2≤2L· Df(y,x),for any x,y∈Rd. (3.4)
Compared with the commonly used ∥∇f(x)− ∇f(y)∥ ≤L∥x−y∥, Proposition 1 gives a tighter
bound for the squared gradient changes since ∥∇f(x)− ∇f(y)∥2≤2LDf(y,x)≤L2∥x−y∥2,
where the second step is due to Df(y,x)≤L
2∥x−y∥2, for any x,y∈Rd[Nesterov, 2018,
Theorem 2.1.5], intuitively making the analysis easier. Combining the Bregman divergence negative
term (3.3) and this useful property (3.4), we address the empirical gradient variation effectively as
¯VT≲TX
t=2 
∥∇ft(xt)− ∇ft(x⋆)∥2+∥∇ft(x⋆)− ∇ft−1(x⋆)∥2+∥∇ft−1(x⋆)− ∇ft−1(xt−1)∥2
(3.4)
≲LTX
t=2Dft(x⋆,xt) +VT+LTX
t=2Dft−1(x⋆,xt−1)≤2LTX
t=1Dft(x⋆,xt) +VT, (3.5)
where the first step introduces intermediate terms ∇ft(x⋆)and∇ft−1(x⋆), the second step uses
Proposition 1 , and the last step combines two summations into one by shifting the indexes of t. The
Bregman divergence negative term in (3.3) can cancel the positive term in (3.5) , leaving only the
gradient variation quantity VTas desired.
Here we only require ∥∇f(x)−∇f(y)∥2≤2LDf(y,x)onXrather than Rd, which can be satisfied
by requiring L-smoothness only on a slightly larger domain than X(a relaxation of Assumption 2).
Interested readers can refer to Appendix A for details. Finally we end this part with several remarks.
Remark 1. We emphasize that the Bregman divergence negative term comes from the linearization
of convex functions, and is thus algorithm-independent . Therefore, we can eliminate the need to
control the algorithmic stability, in contrast to previous works for gradient-variation regret [Chiang
et al., 2012, Yan et al., 2023]. To the best of our knowledge, this is the firstalternative analysis of the
gradient-variation regret since first proposed by Chiang et al. [2012]. ◁
Remark 2. Our idea of the negative term from linearization is inspired by the recent advance in
stochastic optimization [Joulani et al., 2020]. Note that they focus on a different problem of achieving
theO(1/T2)rate as Nesterov’s accelerated gradient [Nesterov, 2018], while we investigate the
gradient-variation regret in the universal online (adversarial) convex optimization setup. ◁
Remark 3. Our analysis does not strictly outperform those that directly handle the stability term as
in(3.2) , e.g., Yan et al. [2023], since the latter can be used for fast rates in the multi-player game
setup [Syrgkanis et al., 2015, Zhang et al., 2022b]. A more detailed discussion of the advantages and
disadvantages over previous approaches is provided in Section 4.3. ◁
3.3 Novel Analysis on Empirical Gradient Variations of Surrogates
Section 3.2 already suffices to achieve the optimal universal gradient-variation regret if multiple
gradient queries are permitted. In this part, we consider further improving the computational efficiency
by using only 1gradient query per round, achieving the same gradient query complexity as OGD.
As stated in Section 3.1, we implement base algorithms on carefully designed surrogate functions
following Yan et al. [2023]. In this part, we show that additional novel analysis is required to handle
the empirical gradient variation defined on surrogates . To see this, we first provide the entire regret
decomposition to give readers a clear roadmap. Specifically, taking λ-strong convexity as an example,
the regret can be decomposed as follows:
REGT≤TX
t=1⟨∇ft(xt),xt−x⋆⟩ −λ
4TX
t=1∥xt−x⋆∥2−1
2TX
t=1Dft(x⋆,xt)
≤"TX
t=1rt,i⋆−λi⋆
4TX
t=1∥xt−xt,i⋆∥2#
+"TX
t=1 
hsc
t,i⋆(xt,i⋆)−hsc
t,i⋆(x⋆)#
−1
2TX
t=1Dft(x⋆,xt),
7where the first step uses (3.3) and the fact that Dft(x⋆,xt)≥λ
2∥xt−x⋆∥2since ft(·)isλ-strongly
convex. The second step uses the definition of the best base learner (indexed by i⋆):λi⋆≤λ≤2λi⋆
and the definition of surrogate functions defined in (3.1) . The first term above ( meta regret ) assesses
how well the algorithm tracks the best base learner, and the second term ( base regret ) measures the
best base learner’s performance. The meta regret contains a linearized regret with a negative term
from curvatures. This negative term is useful for exp-concave and strongly convex functions if the
linearized regret enjoys a second-order bound:
TX
t=1rt,i⋆−λi⋆
4TX
t=1∥xt−xt,i⋆∥2≲vuutTX
t=1⟨∇ft(xt),xt−xt,i⋆⟩2−λi⋆
4TX
t=1∥xt−xt,i⋆∥2≲1
λ,
where the 1/λfactor can be absorbed by the final regret O(1
λlogVT). The base regret is defined on
the surrogates, which preserves the curvature properties, but using only 1gradient ∇ft(xt).
Below we explain the reason for handling the empirical gradient variation defined on surrogates.
Taking the i-thstrongly convex base learner as an example, it updates as xt,i= ΠX[bxt,i−ηtmt]and
bxt+1,i= ΠX[bxt,i−ηt∇hsc
t,i(xt,i)], an initialization of the OOMD algorithm, where ηtrepresents
the step size, ΠX[x] = arg miny∈X∥x−y∥denotes the Euclidean projection onto X, andbxt,iis an
intermediate variable. With appropriately chosen step sizes, the base learner achieves an optimistic
bound of O(logDT), where DT=P
t≤T∥∇hsc
t,i(xt,i)−mt∥2(e.g., please refer to Theorem 15
of Chiang et al. [2012]). Therefore, choosing the optimism as mt=∇hsc
t−1,i(xt−1,i)leads to an
empirical gradient-variation bound O(logDT)defined on surrogates ,3where
DT=TX
t=2∥∇hsc
t,i(xt,i)− ∇hsc
t−1,i(xt−1,i)∥2
=TX
t=2∇ft(xt)− ∇ft−1(xt−1) +λi
2(xt,i−xt)−λi
2(xt−1,i−xt−1)2
.
The empirical gradient variation defined on the original functions, i.e., ∥∇ft(xt)− ∇ft−1(xt−1)∥2,
can be handled via the analysis in Section 3.2. The main challenge is to deal with the rest terms
caused by the surrogate functions. Yan et al. [2023] overcame this issue by controlling (xt−xt−1)
and(xt,i−xt−1,i)separately. Again, as we have explained in Section 3.2, since the decision xtis a
weighted combination of base learners’ decisions (i.e., xt=P
i≤Npt,ixt,i), handling the algorithmic
stability term ∥xt−xt−1∥2directly using (3.2) would results in a less efficient three-layer online
ensemble structure and the sub-optimality of the regret guarantees, as Yan et al. [2023] did.
In this work, we propose a novel analysis — while controlling (xt,i−xt)−(xt−1,i−xt−1)in each
individual round is hard, it can be bounded when aggregated over the time horizon . Specifically, we
bound it by combining two summations into one:
TX
t=2∥(xt,i−xt)−(xt−1,i−xt−1)∥2≲TX
t=2∥xt,i−xt∥2+TX
t=2∥xt−1,i−xt−1∥2≤2TX
t=1∥xt,i−xt∥2.
The same idea is also used in the derivation of (3.5) . Consequently, this term can be canceled out
by the negative term from curvatures in the meta regret. For this cancellation to occur, appropriate
coefficients are chosen, which are provided in the detailed proofs (e.g., the ‘Regret Analysis’ part in
the proof of Theorem 1) and are omitted here for clarity.
This simple and novel analysis eliminates the need to control the overall algorithmic stability term of
∥xt−xt−1∥2required by previous works, and is essential for achieving the improved computational
efficiency and the optimal regret guarantees, as shown in the next part.
3.4 Optimal Universal Gradient-Variation Regret Guarantees
In this part, we present our main theoretical result — our simple and efficient Algorithm 1 (in
Section 3.1) which adopts two novel analyses (in Section 3.2 and Section 3.3) achieves the optimal
3For strongly convex functions, it is possible to choose mt=∇ft−1(xt−1)to avoid additional surrogate-
induced terms, that will be discussed below. We choose the gradient of the last round as the optimism since this
is the the only choice at present to achieve an optimistic regret for exp-concave functions [Chiang et al., 2012].
8gradient-variation regret without requiring the curvature information in advance for universal online
learning. The corresponding proof is provided in Appendix B.
Theorem 1. Under Assumptions 1 and 2 (or the relaxed Assumption 3), Algorithm 1 achieves
O(logVT),O(dlogVT), andO(√VT)for strongly convex, exp-concave, and convex functions.
Theorem 1 improves the O(√VTlogVT)bound of Yan et al. [2023] and is optimal by matching the
best known results when the curvature information is known. It performs well when the gradient varia-
tion is small, such as f1=f2=···=fT(where VT= 0). Note that for α-exp-concave or λ-strongly
convex functions, our guarantee is actually O(min{d
αlogVT,√VT})orO(min{1
λlogVT,√VT}),
thus ensuring O(√VT)even when α=O(1/T)orλ=O(1/T). This is because exp-concave and
strongly convex functions are also convex and thus our convex bound is still applicable.
4 Implication, Application, and Discussion
In this section, we validate the effectiveness of our results by the implication of small-loss regret
and the application in the SEA model. We also discuss the technical comparison with the previous
correction-based approach [Yan et al., 2023] at the end of this section.
4.1 Implication to Universal Small-Loss Regret
In this part, we illustrate that our universal gradient-variation regret in Theorem 1 implies the universal
small-loss regret measured by FT≜minx∈XP
t≤Tft(x)directly in analysis , i.e., without any
algorithmic modification, and thus safeguards the case of FT≤VT, such as minx∈Xft(x) = 0 for
anyt∈[T](where FT= 0). The corresponding proof is provided in Appendix C.1.
Theorem 2. Under Assumptions 1 and 2 (or the relaxed Assumption 3), if the online functions
are non-negative, Algorithm 1 achieves O(logFT),O(dlogFT), andO(√FT)for strongly convex,
exp-concave, and convex functions, respectively.
Theorem 2 achieves the same optimal small-loss bounds as Zhang et al. [2022a]. Combined with
Theorem 1, our approach achieves the best known problem-dependent regret guarantees in the
universal online learning problem. In the end, we emphasize again that our approach is efficient as it
requires O(logT)base learners and only 1gradient query in each round.
4.2 Application to Stochastically Extended Adversarial (SEA) Model
Stochastically extended adversarial (SEA) model [Sachs et al., 2022] interpolates between stochastic
and adversarial online convex optimization. Formally, it assumes that the online function ft(·)
is sampled stochastically from an adversarially chosen distribution Dt. Denoting by Ft(·)≜
Eft∼Dt[ft(·)]the expected function, two terms capture the essential characteristics of SEA model:
σ2
1:T≜TX
t=1max
x∈XEft∼Dt
∥∇ft(x)− ∇Ft(x)∥2
,Σ2
1:T≜E"TX
t=2sup
x∈X∥∇Ft(x)− ∇Ft−1(x)∥2#
,
where σ2
1:Tis the variance in sampling ft(·)fromDt(·)andΣ2
1:Tis the variation of {Ft(·)}t∈[T].
Sachs et al. [2022] initiated the study of the SEA model. For smooth expected functions {Ft(·)}T
t=1,
they achieved the optimal O(p
σ2
1:T+ Σ2
1:T)regret for convex expected functions, and O((σ2
max+
Σ2
max) logT)in the strongly convex case, where σ2
max≜max t∈[T]maxx∈XEft∼Dt[∥∇ft(x)−
∇Ft(x)∥2]andΣ2
max≜max t∈[T]supx∈X∥∇Ft(x)− ∇Ft−1(x)∥2. Subsequently, Chen et al.
[2024] improved the strongly convex regret to O((σ2
max+Σ2
max) log(( σ2
1:T+Σ2
1:T)/(σ2
max+Σ2
max)))
and obtained O(dlog(σ2
1:T+ Σ2
1:T))regret for exp-concave individual functions {ft(·)}T
t=1.
The gradient variation is essential in connecting the stochastic and adversarial optimization [Chen
et al., 2023, Lemma 4], which is also restated in (C.2) . Therefore, universal gradient-variation regret
can be applied to this problem, achieving the same best known bounds as Chen et al. [2024], with a
single algorithm. Table 2 compares our results with existing ones. The following theorem presents
our results formally, with the corresponding proof provided in Appendix C.2.
9Table 2: Comparisons of our results with existing ones. The second column presents the regret bounds, where
σ2
1:TandΣ2
1:Trepresent the stochastic and adversarial statistics of the SEA problem. The last column indicates
whether the results can be achieved by a single algorithm (i.e., suitable in the universal setup). We achieve the
same state-of-the-art guarantees as Chen et al. [2024] using one single algorithm.
WorksRegret Bounds Single
Algorithm?Strongly Convex Exp-concave Convex
Sachs et al. [2022] O((σ2
max+ Σ2
max) logT) N/A O(p
σ2
1:T+ Σ2
1:T) ✗
Chen et al. [2024] O
(σ2
max+ Σ2
max) log
σ2
1:T+Σ2
1:T
σ2max+Σ2max
O(dlog(σ2
1:T+ Σ2
1:T)) O(p
σ2
1:T+ Σ2
1:T) ✗
Sachs et al. [2023] O((σ2
max+ Σ2
max+D2L2) log2T) N/A O(√TlogT) ✓
Yan et al. [2023] O((σ2
max+ Σ2
max) log( σ2
1:T+ Σ2
1:T))O(dlog(σ2
1:T+ Σ2
1:T))O(p
(σ2
1:T+ Σ2
1:T) log( σ2
1:T+ Σ2
1:T)) ✓
Ours O
(σ2
max+ Σ2
max) log
σ2
1:T+Σ2
1:T
σ2max+Σ2max
O(dlog(σ2
1:T+ Σ2
1:T)) O(p
σ2
1:T+ Σ2
1:T) ✓
Theorem 3. Under Assumption 1 and smoothness of Ft(·)for any t∈[T]: ifFt(·)is convex,
Algorithm 1 achieves O(p
σ2
1:T+ Σ2
1:T); ifft(·)is exp-concave, it achieves O(dlog(σ2
1:T+ Σ2
1:T));
and if Ft(·)is strongly convex, it achieves O((σ2
max+ Σ2
max) log(( σ2
1:T+ Σ2
1:T)/(σ2
max+ Σ2
max))).
Theorem 3 requires exp-concavity of the individual function ft(·)rather than the expected function
Ft(·). This assumption is also used by Chen et al. [2023] and common in the studies of stochastic
exp-concave optimization [Mahdavi et al., 2015, Koren and Levy, 2015].
4.3 Discussion on Comparison with Correction-based Approach
In this part, we discuss the technical comparison with the previous correction-based approach [Yan
et al., 2023]. Compared with their approach, ours is simpler and achieves the optimal universal
problem-dependent regret (Theorem 1 and Theorem 2) and the best known guarantees in the SEA
model (Theorem 3). Although not providing guarantees as favorable as ours, Yan et al. [2023] can
control the overall algorithmic stability (i.e., ∥xt−xt−1∥2) using collaborative online ensemble [Zhao
et al., 2024], which is necessary in achieving fast rates in multi-player games [Syrgkanis et al., 2015].
For example, in a min-max game minx∈Xmaxy∈Yx⊤Ay, where Ais a game matrix, since A
is unknown, the Nash equilibrium is typically computed through repeated play, i.e., player- xand
player- yselect{xt}T
t=1and{yt}T
t=1sequentially to approach the Nash equilibrium. For player- x,
in the t-th round, it suffers a loss x⊤
tAytand receives the gradient Ayt. Similarly, player- ysuffers
−x⊤
tAytand receives −Axt. For player- x, if it updates via OOMD, its gradient-variation regret
contains ∥Ayt−Ayt−1∥2, which includes the stability of player- y. In this case, to achieve fast rates,
we indeed need to control the algorithm stability like ∥xt−xt−1∥2and∥yt−yt−1∥2. This can be
done by Yan et al. [2023] while this work cannot since we do not directly control the algorithmic
stability. Interested readers can refer to Appendix A.2 in Yan et al. [2023] for more details.
5 Conclusion
In this work, we investigate universal online learning with gradient-variation regret. We propose
a simple two-layer online ensemble approach that not only achieves the optimal O(1
λlogVT),
O(d
αlogVT), andO(√VT)regret simultaneously for λ-strongly convex, α-exp-concave, and convex
functions and is efficient with O(logT)base learners and only 1gradient query per round. This is
done via the negative Bregman divergence term from linearization and the useful smoothness property
of∥∇f(x)− ∇f(y)∥2≤2LDf(y,x). We further validate the effectiveness of our approach and
results by implying the optimal universal small-loss regret directly in analysis and achieving the best
known results in the stochastically extended adversarial model.
Two future directions are worth investigating. The first is to reduce the number of projections to
only1in each round [Mhammedi et al., 2019, Zhao et al., 2022, Yang et al., 2024], thereby further
improving the computational efficiency. The second direction involves extending our algorithm and
results to the unconstrained setup [Orabona and P ´al, 2016, Cutkosky and Orabona, 2018, Jacobsen
and Cutkosky, 2022], broadening its applicability across a wider range of scenarios.
10Acknowledgements
This research was supported by National Science and Technology Major Project (2022ZD0114802),
NSFC (62176117), and JiangsuSF (BK20220776). Peng Zhao was supported in part by the Xiaomi
Foundation.
References
J. Abernethy, P. L. Bartlett, A. Rakhlin, and A. Tewari. Optimal strategies and minimax lower bounds
for online convex games. In Proceedings of the 21st Annual Conference on Learning Theory
(COLT) , pages 414–424, 2008.
N. Cesa-Bianchi and G. Lugosi. Prediction, Learning, and Games . Cambridge University Press,
2006.
L. Chen, H. Luo, and C. Wei. Impossible tuning made possible: A new expert algorithm and its
applications. In Proceedings of the 34th Annual Conference on Learning Theory (COLT) , pages
1216–1259, 2021.
S. Chen, W.-W. Tu, P. Zhao, and L. Zhang. Optimistic online mirror descent for bridging stochastic
and adversarial online convex optimization. In Proceedings of the 40th International Conference
on Machine Learning (ICML) , pages 5002–5035, 2023.
S. Chen, Y .-J. Zhang, W.-W. Tu, P. Zhao, and L. Zhang. Optimistic online mirror descent for bridging
stochastic and adversarial online convex optimization. Journal of Machine Learning Research , 25
(178):1 – 62, 2024.
C. Chiang, T. Yang, C. Lee, M. Mahdavi, C. Lu, R. Jin, and S. Zhu. Online optimization with gradual
variations. In Proceedings of the 25th Annual Conference on Learning Theory (COLT) , pages
6.1–6.20, 2012.
A. Cutkosky and F. Orabona. Black-box reductions for parameter-free online learning in Banach
spaces. In Proceedings of the 31st Annual Conference on Learning Theory (COLT) , pages 1493–
1529, 2018.
A. Daniely, A. Gonen, and S. Shalev-Shwartz. Strongly adaptive online learning. In Proceedings of
the 32nd International Conference on Machine Learning (ICML) , pages 1405–1411, 2015.
P. Gaillard, G. Stoltz, and T. van Erven. A second-order bound with excess losses. In Proceedings of
the 27th Annual Conference on Learning Theory (COLT) , pages 176–196, 2014.
E. Hazan. Introduction to Online Convex Optimization. Foundations and Trends in Optimization , 2
(3-4):157–325, 2016.
E. Hazan and C. Seshadhri. Adaptive algorithms for online decision problems. Electronic Colloquium
on Computational Complexity (ECCC) , 14(088), 2007.
E. Hazan, A. Agarwal, and S. Kale. Logarithmic regret algorithms for online convex optimization.
Machine Learning , 69(2-3):169–192, 2007.
A. Jacobsen and A. Cutkosky. Parameter-free mirror descent. In Proceedings of the 35th Annual
Conference on Learning Theory (COLT) , pages 4160–4211, 2022.
S. Ji and J. Ye. An accelerated gradient method for trace norm minimization. In Proceedings of the
26th International Conference on Machine Learning (ICML) , pages 457–464, 2009.
P. Joulani, A. Raj, A. Gyorgy, and C. Szepesv ´ari. A simpler approach to accelerated optimization:
Iterative averaging meets optimism. In Proceedings of the 37th International Conference on
Machine Learning (ICML) , pages 4984–4993, 2020.
T. Koren and K. Y . Levy. Fast rates for exp-concave empirical risk minimization. In Advances in
Neural Information Processing Systems 28 (NIPS) , pages 1477–1485, 2015.
11M. Li, T. Zhang, Y . Chen, and A. J. Smola. Efficient mini-batch training for stochastic optimization.
InProceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery &
Data Mining (KDD) , pages 661–670, 2014.
H. Luo and R. E. Schapire. Achieving all with no parameters: AdaNormalHedge. In Proceedings of
the 28th Annual Conference on Learning Theory (COLT) , pages 1286–1304, 2015.
M. Mahdavi, L. Zhang, and R. Jin. Lower and upper bounds on the generalization of stochastic
exponentially concave optimization. In Proceedings of the 28th Annual Conference on Learning
Theory (COLT) , pages 1305–1320, 2015.
Z. Mhammedi, W. M. Koolen, and T. van Erven. Lipschitz adaptivity with multiple learning rates
in online learning. In Proceedings of the 32nd Annual Conference on Learning Theory (COLT) ,
pages 2490–2511, 2019.
Y . Nesterov. Lectures on Convex Optimization , volume 137. Springer, 2018.
F. Orabona. A modern introduction to online learning. ArXiv preprint , arXiv:1912.13213, 2019.
F. Orabona and D. P ´al. Coin betting and parameter-free online learning. In Advances in Neural
Information Processing Systems 29 (NIPS) , pages 577–585, 2016.
F. Orabona, N. Cesa-Bianchi, and C. Gentile. Beyond logarithmic bounds in online learning. In
Proceedings of the 15th International Conference on Artificial Intelligence and Statistics (AISTATS) ,
pages 823–831, 2012.
E. Ordentlich and T. M. Cover. The cost of achieving the best portfolio in hindsight. Mathematics of
Operations Research , 23(4):960–982, 1998.
A. Rakhlin and K. Sridharan. Online learning with predictable sequences. In Proceedings of the 26th
Annual Conference on Learning Theory (COLT) , pages 993–1019, 2013.
S. Sachs, H. Hadiji, T. van Erven, and C. Guzm ´an. Between stochastic and adversarial online
convex optimization: Improved regret bounds via smoothness. In Advances in Neural Information
Processing Systems 35 (NeurIPS) , pages 691–702, 2022.
S. Sachs, H. Hadiji, T. van Erven, and C. Guzm ´an. Accelerated rates between stochastic and
adversarial online convex optimization. ArXiv preprint , arXiv:2303.03272, 2023.
N. Srebro, K. Sridharan, and A. Tewari. Smoothness, low noise and fast rates. In Advances in Neural
Information Processing Systems 23 (NIPS) , pages 2199–2207, 2010.
V . Syrgkanis, A. Agarwal, H. Luo, and R. E. Schapire. Fast convergence of regularized learning in
games. In Advances in Neural Information Processing Systems 28 (NIPS) , pages 2989–2997, 2015.
T. van Erven and W. M. Koolen. MetaGrad: Multiple learning rates in online learning. In Advances
in Neural Information Processing Systems 29 (NIPS) , pages 3666–3674, 2016.
G. Wang, S. Lu, and L. Zhang. Adaptivity and optimality: A universal algorithm for online convex
optimization. In Proceedings of the 35th Conference on Uncertainty in Artificial Intelligence (UAI) ,
pages 659–668, 2019.
C. Wei, Y . Hong, and C. Lu. Tracking the best expert in non-stationary stochastic environments. In
Advances in Neural Information Processing Systems 29 (NIPS) , pages 3972–3980, 2016.
Y .-H. Yan, P. Zhao, and Z.-H. Zhou. Universal online learning with gradient variations: A multi-layer
online ensemble approach. In Advances in Neural Information Processing Systems 36 (NeurIPS) ,
pages 37682–37715, 2023.
T. Yang, M. Mahdavi, R. Jin, and S. Zhu. Regret bounded by gradual variation for online convex
optimization. Machine Learning , 95(2):183–223, 2014.
W. Yang, Y . Wang, P. Zhao, and L. Zhang. Universal online convex optimization with 1 projection
per round. ArXiv preprint , arXiv:2405.19705, 2024.
12L. Zhang, S. Lu, and Z.-H. Zhou. Adaptive online learning in dynamic environments. In Advances in
Neural Information Processing Systems 31 (NeurIPS) , pages 1330–1340, 2018.
L. Zhang, T.-Y . Liu, and Z.-H. Zhou. Adaptive regret of convex and smooth functions. In Proceedings
of the 36th International Conference on Machine Learning (ICML) , pages 7414–7423, 2019.
L. Zhang, G. Wang, J. Yi, and T. Yang. A simple yet universal strategy for online convex optimization.
InProceedings of the 39th International Conference on Machine Learning (ICML) , pages 26605–
26623, 2022a.
M. Zhang, P. Zhao, H. Luo, and Z.-H. Zhou. No-regret learning in time-varying zero-sum games.
InProceedings of the 39th International Conference on Machine Learning (ICML) , pages 26772–
26808, 2022b.
P. Zhao. Online Ensemble Theories and Methods for Robust Online Learning . PhD thesis, Nanjing
University, Nanjing, China, 2021. Advisor: Zhi-Hua Zhou.
P. Zhao, Y .-J. Zhang, L. Zhang, and Z.-H. Zhou. Dynamic regret of convex and smooth functions. In
Advances in Neural Information Processing Systems 33 (NeurIPS) , pages 12510–12520, 2020.
P. Zhao, Y .-F. Xie, L. Zhang, and Z.-H. Zhou. Efficient methods for non-stationary online learning.
InAdvances in Neural Information Processing Systems 35 (NeurIPS) , pages 11573–11585, 2022.
P. Zhao, Y .-J. Zhang, L. Zhang, and Z.-H. Zhou. Adaptivity and non-stationarity: Problem-dependent
dynamic regret for online convex optimization. Journal of Machine Learning Research , 25(98):1 –
52, 2024.
Z.-H. Zhou. Ensemble Methods: Foundations and Algorithms . Chapman & Hall/CRC Press, 2012.
M. Zinkevich. Online convex programming and generalized infinitesimal gradient ascent. In
Proceedings of the 20th International Conference on Machine Learning (ICML) , pages 928–936,
2003.
13A On Smoothness Assumption
In this section, we propose a relaxation of the smoothness requirement to a slightly larger domain
than the feasible domain X, in contrast to the whole Rdspace as in Assumption 2. The relaxed
smoothness assumption is detailed in the following.
Assumption 3 (Relaxed Smoothness) .Under the condition of ∥∇ft(x)∥ ≤Gfor any x∈ X and
t∈[T], all online functions are L-smooth: ∥∇ft(x)− ∇ft(y)∥ ≤L∥x−y∥for any t∈[T]and
x,y∈ X+, where X+≜{x+b|x∈ X,b∈G/L·B}andB≜{x| ∥x∥ ≤1}is a unit ball.
The domain of Assumption 3 is slightly larger than X— for any x+∈ X+, we can always find an
x∈ X such that ∥x+−x∥ ≤ G/L. In this work, one of the key technical contributions is to handle
the empirical gradient variation via a useful smoothness property (Proposition 1). We show in the
following that this condition can be satisfied by requiring only Assumption 3.
Lemma 1. Under Assumption 1, for any online function f(·)satisfying Assumption 3, it holds that
∥∇f(x)− ∇f(y)∥2≤2LDf(y,x)for any x,y∈ X.
Proof. To begin with, we present the self-bounding property [Srebro et al., 2010], which is useful in
proving our result — if a function f:Rd7→RisL-smooth and bounded from below, then for any
x∈Rd, it holds that
∥∇f(x)∥2≤2L
f(x)−inf
y∈Rdf(y)
. (A.1)
Next, we aim to prove that if we only need (A.1) on a bounded domain X, we require smoothness
only on a slightly larger domain than X. To see this, we delve into the proof of the self-bounding
property. Specifically, for any x,v∈Rd, it holds that
⟨−∇f(x),v⟩ −L
2∥v∥2≤f(x)−f(x+v)≤f(x)−inf
y∈Rdf(y),
where the first step requires smoothness on xandx+v. Consequently, by taking maximization over
v, it holds that
f(x)−inf
y∈Rdf(y)≥sup
v∈Rd⟨−∇f(x),v⟩ −L
2∥v∥2=1
2L∥∇f(x)∥2,
which leads to the self-bounding property (A.1) by taking v=−1
L∇f(x). The above proof is from
Theorem 4.23 of Orabona [2019]. This means that for the self-bounding property, we only require the
smoothness to hold for any x∈ X andx−1
L∇f(x). Under Assumption 1, this can be satisfied by
requiring smoothness on a slightly larger domain than X, namely, X+≜{x+b|x∈ X,b∈G/L·B}.
Now we are ready to prove the final result. To begin with, we define a surrogate function of
g(x)≜f(x)− ⟨∇ f(x0),x⟩for any x∈ X, where x0∈ X. Due to the above property we have just
proved, by requiring smoothness on X+, we have
∥∇g(x)∥2≤2L
g(x)−inf
y∈Rdg(y)
.
Denoting by y⋆∈arg miny∈Rdg(y), the above inequality equals to
∥∇f(x)− ∇f(x0)∥2≤2L(f(x)− ⟨∇ f(x0),x⟩ −f(y⋆) +⟨∇f(x0),y⋆⟩)
= 2L(f(x)−f(y⋆)− ⟨∇ f(x0),x−y⋆⟩),
due to the definition of g(·). The proof using the self-bounding property is from Theorem 2.1.5
of Nesterov [2018]. Finally, we note that g(·)is minimized at y⋆=x0, leading to ∥∇f(x)−
∇f(x0)∥2≤2LDf(x0,x)for any x,x0∈ X, which finishes the proof.
B Proof for Section 3
In this section, we provide the proof of Theorem 1, our main theoretical result for the optimal
universal gradient-variation regret.
14Proof. We first give different decompositions of the regret for different curvature types, then analyze
the meta and base regret, and finally combine them to achieve the regret bound. For simplicity, we
define gt≜∇ft(xt)and
¯VT≜TX
t=2∥∇ft(xt)− ∇ft−1(xt−1)∥2
for short. Using the analysis in Section 3.2, the empirical gradient variation can bounded as
¯VT≤3TX
t=2∥∇ft(xt)− ∇ft(x⋆)∥2+ 3TX
t=2∥∇ft(x⋆)− ∇ft−1(x⋆)∥2
+ 3TX
t=2∥∇ft−1(x⋆)− ∇ft−1(xt−1)∥2≤6LTX
t=2Dft(x⋆,xt) + 3VT+ 6LTX
t=2Dft−1(x⋆,xt−1)
≤3VT+ 12LTX
t=1Dft(x⋆,xt). (B.1)
Regret Decomposition. Denoting by x⋆∈arg minx∈XP
t∈[T]ft(x), for convex functions, we
decompose the regret as
REGT=TX
t=1⟨gt,xt−x⋆⟩ −TX
t=1Dft(x⋆,xt) (by (3.3))
=TX
t=1⟨gt,xt−xt,i⋆⟩
| {z }
META-REG+TX
t=1hc
t,i⋆(xt,i⋆)−TX
t=1hc
t,i⋆(x⋆)
| {z }
BASE-REG−TX
t=1Dft(x⋆,xt), (B.2)
where hc
t,i(x)≜⟨gt,x⟩. For α-exp-concave functions, we decompose the regret as
REGT=TX
t=1⟨gt,xt−x⋆⟩ −1
2TX
t=1Dft(x⋆,xt)−1
2TX
t=1Dft(x⋆,xt) (by (3.3))
≤TX
t=1⟨gt,xt−x⋆⟩ −α
4TX
t=1⟨gt,xt−x⋆⟩2−1
2TX
t=1Dft(x⋆,xt)
≤TX
t=1⟨gt,xt−xt,i⋆⟩ −αi⋆
4TX
t=1⟨gt,xt−xt,i⋆⟩2
| {z }
META-REG(byαi⋆≤α≤2αi⋆)
+TX
t=1hexp
t,i⋆(xt,i⋆)−TX
t=1hexp
t,i⋆(x⋆)
| {z }
BASE-REG−1
2TX
t=1Dft(x⋆,xt), (B.3)
where the second step is due to the definitions of exp-concavity and Bregman divergence and the
last step is due to the definition of the surrogate function hexp
t,i(x)≜⟨gt,x⟩+αi
4⟨∇ft(xt),x−xt⟩2,
where αi∈ H, defined in (2.1) . Forλ-strongly convex functions, following the similar decomposition
in the exp-concavity case, we obtain
REGT≤TX
t=1⟨gt,xt−xt,i⋆⟩ −λi⋆
4TX
t=1∥xt−xt,i⋆∥2
| {z }
META-REG(byλi⋆≤λ≤2λi⋆)
+TX
t=1hsc
t,i⋆(xt,i⋆)−TX
t=1hsc
t,i⋆(x⋆)
| {z }
BASE-REG−1
2TX
t=1Dft(x⋆,xt), (B.4)
due to the definition of the surrogate hsc
t,i(x)≜⟨gt,x⟩+λi
4∥x−xt∥2, where λi∈ H in (2.1).
15Meta Regret Analysis. We adopt OPTIMISTIC -ADAPT -ML-P ROD [Wei et al., 2016] as the meta
learner, and present its regret analysis below for self-containedness.
Lemma 2 (Theorem 3.4 of Wei et al. [2016]) .Denoting by ptthe weights of the algorithm, ℓtthe
loss vector, and mt,ithe optimism, by choosing the learning rate optimally as
εt,i= min(
1
8,s
lnNP
s∈[t](rs,i−ms,i)2)
, (B.5)
the regret of OPTIMISTIC -ADAPT -ML-P ROD with respect to any expert i∈[N]satisfies
TX
t=1⟨ℓt,pt−ei⟩ ≤C0vuut1 +TX
t=1(rt,i−mt,i)2+C1,
where rt,i=⟨ℓt,pt−ei⟩,eidenotes the i-thstandard basis vector, C0=√
lnN+ ln(1 +N
e(1 +
ln(T+ 1))) /√
lnN, and C1=1
4(lnN+ ln(1 +N
e(1 + ln( T+ 1)))) + 2√
lnN+ 16 ln N.
Here we adopt ℓt,i=⟨gt,xt,i⟩such that ⟨ℓt,pt−ei⟩=⟨gt,xt−xt,i⟩. Besides, since the number
of base learners N=O(logT)as explained in Section 2, the constants C0andC1are in the order
ofO(log log T)and can be treated as ignorable constants, following previous convention [Luo and
Schapire, 2015, Gaillard et al., 2014].
Forconvex functions, we choose the optimism as mt,i=⟨gt−1,xt−xt,i⟩for the index iindicating
the convex base learner. As explained in Section 3.1, although xtis unknown for now, we only
require the scalar value of ⟨gt−1,xt⟩. Denoting by z=⟨gt−1,xt⟩, it actually forms a fixed-point
problem of z=⟨gt−1,xt(z)⟩, where xtis a function of zsince xtdepends on pt,i,pt,irelies on
mt,i, and mt,idepends on z. Such a one-dimensional fixed-point problem can be solved with an
O(1/T)approximation error through O(logT)binary searches, and aggregating the approximate
error over the whole time horizon will only incur an additive constant to the final regret. As a result,
such an optimism setup is valid. Consequently, the meta regret in (B.2) can be bounded as
META-REG≤C0vuut1 +TX
t=1⟨gt−gt−1,xt−xt,i⋆⟩2+C1 (by Lemma 2)
≤C0p
1 +D2¯VT+C1≤C0vuut1 + 3 D2VT+ 12LD2TX
t=1Dft(x⋆,xt) +C1 (by (B.1))
≤ O(p
VT) +C0vuut12LD2TX
t=1Dft(x⋆,xt)≤ O(p
VT) +O(C2) +C0
2C2TX
t=1Dft(x⋆,xt),
where the second step adopts Assumption 1, the fourth step uses√
a+b≤√a+√
bfor any a, b≥0,
the last step uses AM-GM inequality:√
ab≤ax
2+b
2xfor any a, b, x > 0. Note that C2is used to
ensure the positive Bregman divergence term to be canceled and will be specified in the end.
Forexp-concave functions, we choose the optimism as mt,i= 0for indexes iindicating exp-concave
base learners. By Lemma 2, the meta regret in (B.3) can be bounded as
META-REG≤C0vuut1 +TX
t=1⟨gt,xt−xt,i⋆⟩2−αi⋆
4TX
t=1⟨gt,xt−xt,i⋆⟩2+C1
≤ O(C3) +C0
2C3−αi⋆
4TX
t=1⟨gt,xt−xt,i⋆⟩2, (B.6)
where the last step omits the ignorable additive C0orC1terms and is due to AM-GM inequality. C2
is a constant to be specified.
16Forstrongly convex functions, we choose the optimism mt,i= 0for indexes iindicating strongly
convex base learners. By Lemma 2, the meta regret in (B.4) can be bounded as
META-REG≤C0vuut1 +TX
t=1⟨gt,xt−xt,i⋆⟩2−λi⋆
4TX
t=1∥xt−xt,i⋆∥2+C1 (by Lemma 2)
≤C0vuut1 +D2TX
t=1∥xt−xt,i⋆∥2−λi⋆
4TX
t=1∥xt−xt,i⋆∥2+C1(by Assumption 1)
≤ O(C4) +C0D2
2C4−λi⋆
4TX
t=1∥xt−xt,i⋆∥2, (B.7)
where the last step omits the ignorable additive C0orC1terms and is due to AM-GM inequality. C4
is a constant to be specified.
Base Regret Analysis. Forconvex functions, we implement Optimistic OGD (OOGD) on the
convex base learner (indexed by i):
xt,i= ΠX[bxt,i−ηtmt],bxt+1,i= ΠX[bxt,i−ηt∇hc
t,i(xt,i)]. (B.8)
By choosing the step size as ηt= min {D/q
1 +Pt−1
s=2∥∇hc
s,i(xs,i)− ∇hc
s−1,i(xs−1,i)∥2,1}and
the optimism as mt=∇hc
t−1,i(xt−1,i), due to the standard analysis of OOGD for convex functions
(e.g., Lemma 10 of Yan et al. [2023]), the base regret can be bounded as
BASE-REG≤5Dvuut1 +TX
t=2∥∇hc
t,i⋆(xt,i⋆)− ∇hc
t−1,i⋆(xt−1,i⋆)∥2+O(1)
= 5Dp
1 +¯VT+O(1)≤ O(p
VT) + 5Dvuut12LTX
t=1Dft(x⋆,xt) (by (B.1))
≤ O(p
VT) +O(C5) +5D
2C5TX
t=1Dft(x⋆,xt),
where the second step is due to the property of the surrogate function: ∇hc
t,i(xt,i) =gt, and the last
step uses AM-GM inequality. C5is a constant to be specified.
Forexp-concave functions, we run OOMD on the exp-concave base learners (indexed by i):
xt,i= arg min
x∈X{⟨mt,x⟩+Dψt(x,bxt,i)},bxt+1,i= arg min
x∈X
⟨∇hexp
t,i(xt,i),x⟩+Dψt(x,bxt,i)	
.
Choosing ψt(x) =1
2x⊤Utxfor any x,Ut= (1 +αiG2
2)I+αi
2Pt−1
s=1∇hexp
s,i(xs,i)∇hexp
s,i(xs,i)⊤,
andmt=∇hexp
t−1,i(xt−1,i), due to the standard analysis of OOMD for exp-concave functions (e.g.,
Lemma 11 of Yan et al. [2023]), the base regret can be bounded as
BASE-REG≤16d
αi⋆ln 
1 +αi⋆
8dTX
t=2∇hexp
t,i⋆(xt,i⋆)− ∇hexp
t−1,i⋆(xt−1,i⋆)2!
+O(1).(B.9)
Next, we analyze the empirical gradient variation defined on the surrogate function hexp
t,i(·):
TX
t=2∇hexp
t,i⋆(xt,i⋆)− ∇hexp
t−1,i⋆(xt−1,i⋆)2
=TX
t=2gt+αi⋆
2gt⟨gt,xt−xt,i⋆⟩ −gt−1−αi⋆
2gt−1⟨gt−1,xt−1−xt−1,i⋆⟩2
≤3¯VT+ 3TX
t=2αi⋆
2gt⟨gt,xt−xt,i⋆⟩2
+ 3TX
t=2αi⋆
2gt−1⟨gt−1,xt−1−xt−1,i⋆⟩2
17≤3¯VT+ 6TX
t=1αi⋆
2gt⟨gt,xt−xt,i⋆⟩2
(B.10)
≤9VT+ 36LTX
t=1Dft(x⋆,xt) + 2α2
i⋆G2TX
t=1⟨gt,xt−xt,i⋆⟩2,(by (B.1) and Assumption 1)
where the first step is due to the property of the surrogate function: ∇hexp
t,i(xt,i) =gt+αi
2gt⟨gt,xt−
xt,i⟩, the second step is by the Cauchy-Schwarz inequality: (a+b+c)2≤3(a2+b2+c2)for any
a, b, c∈R. Plugging the surrogate’s empirical gradient variation back to the base regret, we obtain
BASE-REG≤16d
αi⋆ln 
1 +9αi⋆
8dVT+9αi⋆L
2dTX
t=1Dft(x⋆,xt) +α3
i⋆G2
4dTX
t=1⟨gt,xt−xt,i⋆⟩2!
≤ Od
αln(C6VT)
+16d
C6αi⋆ 
9αi⋆L
2dTX
t=1Dft(x⋆,xt) +α3
i⋆G2
4dTX
t=1⟨gt,xt−xt,i⋆⟩2!
≤ Od
αlnVT
+72L
C6TX
t=1Dft(x⋆,xt) +4G2
C6TX
t=1⟨gt,xt−xt,i⋆⟩2+O(lnC6).
The second step requires C6≥1by Lemma 5 and uses the property of the best base learner, i.e.,
αi⋆≤α≤2αi⋆. The last step is because of αi≤1.
Forstrongly convex functions, we run OOGD (B.8) on the strongly convex base learners (indexed by i).
Specifically, by choosing the step size as ηt= 2/(1+λit)and the optimism as mt=∇hsc
t−1,i(xt−1,i),
due to the analysis of OOGD for strongly convex functions (e.g., Lemma 12 of Yan et al. [2023]), the
base regret can be bounded as
BASE-REG≤16G2
λi⋆ln 
1 +λi⋆TX
t=2∇hsc
t,i⋆(xt,i⋆)− ∇hsc
t−1,i⋆(xt−1,i⋆)2!
+O(1).(B.11)
Next, we analyze the empirical gradient variation defined on the surrogate function hsc
t,i(·):
TX
t=2∇hsc
t,i⋆(xt,i⋆)− ∇hsc
t−1,i⋆(xt−1,i⋆)2
=TX
t=2gt+λi⋆
2(xt,i⋆−xt)−gt−1−λi⋆
2(xt−1,i⋆−xt−1)2
≤3¯VT+ 3TX
t=2λi⋆
2(xt,i⋆−xt)2
+ 3TX
t=2λi⋆
2(xt−1,i⋆−xt−1)2
(B.12)
≤9VT+ 36LTX
t=1Dft(x⋆,xt) + 2λ2
i⋆TX
t=1∥xt,i⋆−xt∥2, (by (B.1))
where the first step is due to the property of the surrogate: ∇hsc
t,i(xt,i) =gt+λi
2(xt,i−xt), and the
second step is due to the Cauchy-Schwarz inequality. Plugging the surrogate’s empirical gradient
variation back to the base regret, we obtain
BASE-REG≤16G2
λi⋆ln 
1 + 9 λi⋆VT+ 36Lλi⋆TX
t=1Dft(x⋆,xt) + 2λ3
i⋆TX
t=1∥xt,i⋆−xt∥2!
≤ O1
λln(C7VT)
+16G2
C7λi⋆ 
36Lλi⋆TX
t=1Dft(x⋆,xt) + 2λ3
i⋆TX
t=1∥xt,i⋆−xt∥2!
≤ O1
λlnVT
+576G2L
C7TX
t=1Dft(x⋆,xt) +32G2
C7TX
t=1∥xt,i⋆−xt∥2+O(lnC7),
where the second step requires C7≥1by Lemma 5 and uses the property of the best base learner,
i.e.,λi⋆≤λ≤2λi⋆. The last step is due to λi≤1.
18Regret Analysis. Forconvex functions, by combining the meta and base regret, it holds that
REGT≤ O(p
VT) +O(C2+C5) +C0
2C2+5D
2C5−1TX
t=1Dft(x⋆,xt)≤ O(p
VT),
by choosing C2=C0andC5= 5D. For exp-concave functions, by combining the meta and base
regret, it holds that
REGT≤ Od
αlnVT
+O(C3+ lnC6) +C0
2C3+4G2
C6−αi⋆
4TX
t=1⟨gt,xt−xt,i⋆⟩2
+72L
C6−1
2TX
t=1Dft(x⋆,xt)≤ Od
αlnVT
,
by choosing C6= max {1,144L,32G2
αi⋆}andC3=4C0
αi⋆. Note that such a parameter configuration
will only add an O(1/α)factor to the final regret bound, which can be absorbed. For strongly convex
functions, by combining the meta and base regret, it holds that
REGT≤ O1
λlnVT
+O(C4+ lnC7) +C0D2
2C4+32G2
C7−λi⋆
4TX
t=1∥xt−xt,i⋆∥2
+576G2L
C7−1
2TX
t=1Dft(x⋆,xt)≤ O1
λlnVT
,
by choosing C7= max {1,1152G2L,256G2
λi⋆}andC4=4C0D2
λi⋆. Note that such a parameter configu-
ration will only add an O(1/λ)factor to the final regret bound, which can be absorbed.
Note that the constants C2, C3, C4, C5, C6, C7only exist in analysis and thus can be chosen arbitrarily,
finishing the proof.
C Proofs for Section 4
In this section, we provide proofs for Section 4, including Theorem 2 and Theorem 3.
C.1 Proof of Theorem 2
Proof. To begin with, we give a different decomposition for the empirical gradient variation ¯VT:
¯VT≤2TX
t=2∥gt∥2+ 2TX
t=2∥gt−1∥2≤4TX
t=1∥gt∥2≤16LTX
t=1ft(xt), (C.1)
where the last step is by the self-bounding property (A.1) for non-negative functions. For simplicity,
we denote by ¯FT≜PT
t=1ft(xt).
The regret decomposition is the same as that in the proof of Theorem 1 (i.e., (B.2) ,(B.3) , and (B.4) ),
and thus omitted here. In the following, we analyze the meta and base regret, and combine them for
the final regret bounds.
Meta Regret Analysis. Our Algorithm 1 achieves the small-loss bounds without modifying the
algorithm. As a result, the meta algorithm and the corresponding step size and optimism configurations
are the same as that in the proof of Theorem 1.
Forconvex functions, by choosing the optimism as mt,i=⟨gt−1,xt−xt,i⟩for the index iindicating
the base learner for the convex case, the meta regret in (B.2) can be bounded as
META-REG≤C0vuut1 +TX
t=1⟨gt−gt−1,xt−xt,i⋆⟩2+C1 (by Lemma 2)
19≤C0p
1 +D2¯VT+C1≤C0p
1 + 16 D2L¯FT+C1. (by Assumption 1 and (C.1))
Forexp-concave functions, we choose the optimism as mt,i= 0 for indexes iindicating the exp-
concave base learners. The meta regret is bounded in the same way as (B.6).
Forstrongly convex functions, we choose the optimism as mt,i= 0 for indexes iindicating the
strongly convex base learners. The meta regret is bounded in the same way as (B.7).
Base Regret Analysis. Forconvex functions, using the same base algorithms as in the proof of
Theorem 1, the base regret can be bounded as
BASE-REG≤5Dp
1 +¯VT+O(1)≤5Dp
1 + 16 L¯FT+O(1).
Forexp-concave functions, using the same base algorithms as in the proof of Theorem 1, the base
regret can be bounded by (B.9) . Following (B.10) , the empirical gradient variation defined on the
surrogate function hexp
t,i(·)can be bounded as
TX
t=2∇hexp
t,i⋆(xt,i⋆)− ∇hexp
t−1,i⋆(xt−1,i⋆)2≤3¯VT+ 6TX
t=1αi⋆
2gt⟨gt,xt−xt,i⋆⟩2
≤48L¯FT+ 2α2
i⋆G2TX
t=1⟨gt,xt−xt,i⋆⟩2. (by Assumption 1 and (C.1))
Plugging the surrogate’s empirical gradient variation back to the base regret, we obtain
BASE-REG≤16d
αi⋆ln 
1 +6Lαi⋆
d¯FT+α3
i⋆G2
4dTX
t=1⟨gt,xt−xt,i⋆⟩2!
+O(1)
≤16d
αi⋆ln
C8
1 +6Lαi⋆
d¯FT
+16d
C8αi⋆ 
α3
i⋆G2
4dTX
t=1⟨gt,xt−xt,i⋆⟩2!
≤32d
αln
1 +6L
d¯FT
+4G2
C8TX
t=1⟨gt,xt−xt,i⋆⟩2+O(lnC8),
where the second step requires C8≥1by Lemma 5 and uses the property of the best base learner,
i.e.,αi⋆≤α≤2αi⋆. The last step is due to αi≤1.
Forstrongly convex functions, using the same base algorithms as in the proof of Theorem 1, the base
regret can be bounded by (B.11) . Following (B.12) , the empirical gradient variation defined on the
surrogate function hsc
t,i(·)can be bounded as
TX
t=2∇hsc
t,i⋆(xt,i⋆)− ∇hsc
t−1,i⋆(xt−1,i⋆)2≤3¯VT+ 6TX
t=1λi⋆
2(xt,i⋆−xt)2
≤48L¯FT+ 2λ2
i⋆TX
t=1∥xt,i⋆−xt∥2. (by (C.1))
Plugging the surrogate’s empirical gradient variation back to the base regret, we obtain
BASE-REG≤16G2
λi⋆ln 
1 + 48 Lλi⋆¯FT+ 2λ3
i⋆TX
t=1∥xt,i⋆−xt∥2!
+O(1)
≤16G2
λi⋆ln 
C9 
1 + 48 Lλi⋆¯FT
+16G2
C9λi⋆ 
2λ3
i⋆TX
t=1∥xt,i⋆−xt∥2!
≤32G2
λln(1 + 48 L¯FT) +32G2
C9TX
t=1∥xt,i⋆−xt∥2+O(lnC9),
where the second step requires C9≥1by Lemma 5 and uses the property of the best base learner,
i.e.,λi⋆≤λ≤2λi⋆. The last step is due to λi≤1.
20Regret Analysis. Forconvex functions, by combining the meta and base regret, it holds that
REGT≤C0p
1 + 16 D2L¯FT+ 5Dp
1 + 16 L¯FT+C1≤ O(p
FT),
where the last step is due to Lemma 9 of Zhao et al. [2024], restated below for self-containedness.
Lemma 3 (Lemma 9 of Zhao et al. [2024]) .For any x, y, a, b > 0satisfying x−y≤√ax+b, it
holds that x−y≤√ay+ab+a+b.
Forexp-concave functions, by combining the meta and base regret, it holds that
REGT≤C0
2C3+4G2
C8−αi⋆
4TX
t=1⟨gt,xt−xt,i⋆⟩2+32d
αln
1 +6L
d¯FT
+O(C3+ lnC8)
≤32d
αln
1 +6L
d¯FT
+O(1)≤ Od
αlnFT
, (by Lemma 6)
where the second step chooses C3=4C0
αi⋆andC8= max {1,32G2
αi⋆}. Note that such a parameter
configuration will only add an O(1/α)factor to the final regret bound, which can be absorbed.
Forstrongly convex functions, by combining the meta and base regret, it holds that
REGT≤C0D2
2C4+32G2
C9−λi⋆
4TX
t=1∥xt−xt,i⋆∥2+32G2
λln(1 + 48 L¯FT) +O(C4+ lnC9)
≤32G2
λln(1 + 48 L¯FT)≤ O1
λlnFT
, (by Lemma 6)
where the second step is by choosing C4=4C0D2
λi⋆andC9= max {1,256G2
λi⋆}. Note that such a
parameter configuration will only add an O(1/λ)factor to the final regret bound, which can be
absorbed. Also note that the constants C3, C4, C8, C9only exist in analysis and thus can be chosen
arbitrarily, finishing the proof.
C.2 Proof of Theorem 3
Proof. To begin with, we give a different analysis of the empirical gradient variation:
E[¯VT]≤5E"TX
t=2∥∇ft(xt)− ∇Ft(xt)∥2#
+ 5TX
t=2∥∇Ft(xt)− ∇Ft(x⋆)∥2
+5E"TX
t=2∥∇Ft(x⋆)− ∇Ft−1(x⋆)∥2#
+ 5TX
t=2∥∇Ft−1(x⋆)− ∇Ft−1(xt−1)∥2
+5E"TX
t=2∥∇Ft−1(xt−1)− ∇ft−1(xt−1)∥2#
≤10σ2
1:T+ 5Σ2
1:T+ 20LTX
t=1DFt(x⋆,xt),(C.2)
where the first step is due to Cauchy-Schwarz inequality and the last step is because of the definitions
ofσ2
1:TandΣ2
1:T(given in Section 4) and the analysis proposed in Section 3.2.
In the following, we first give regret decompositions for different curvature types, then we analyze
the meta and base regret, and combine them for the final regret guarantees.
Regret Decomposition. Denoting by x⋆∈arg minx∈XP
t∈[T]ft(x), for convex functions, we
decompose the regret as
E[REGT] =E"TX
t=1Ft(xt)−TX
t=1Ft(x⋆)#
=E"TX
t=1⟨∇Ft(xt),xt−x⋆⟩#
−TX
t=1DFt(x⋆,xt)
=E"TX
t=1⟨∇ft(xt),xt−x⋆⟩#
−TX
t=1DFt(x⋆,xt)
21=E"TX
t=1⟨gt,xt−xt,i⋆⟩#
| {z }
META-REG+E"TX
t=1hc
t,i⋆(xt,i⋆)−hc
t,i⋆(x⋆)#
| {z }
BASE-REG−TX
t=1DFt(x⋆,xt),
where the first and third step use Ft(x) =E[ft(x)], the second step uses the definition of Bregman
divergence, and the fourth step is due to hc
t,i(x)≜⟨gt,x⟩.
Forexp-concave functions, following the similar decomposition as in the proof of Theorem 1 in
Appendix B, we decompose the regret as
E[REGT] =E"TX
t=1⟨∇Ft(xt),xt−x⋆⟩#
−1
2TX
t=1DFt(x⋆,xt)−1
2TX
t=1DFt(x⋆,xt)
=E"TX
t=1⟨∇ft(xt),xt−x⋆⟩#
−1
2TX
t=1Dft(x⋆,xt)−1
2TX
t=1DFt(x⋆,xt)
≤E"TX
t=1⟨gt,xt−x⋆⟩#
−α
4TX
t=1⟨gt,xt−x⋆⟩2−1
2TX
t=1DFt(x⋆,xt)
≤TX
t=1⟨gt,xt−xt,i⋆⟩ −αi⋆
4TX
t=1⟨gt,xt−xt,i⋆⟩2
| {z }
META-REG
+E"TX
t=1hexp
t,i⋆(xt,i⋆)−hexp
t,i⋆(x⋆)#
| {z }
BASE-REG−1
2TX
t=1DFt(x⋆,xt),
where the second step uses the definition of the expected function Ft(·), the third step requires the
exp-concavity of ft(·), and the fourth step is due to hexp
t,i(x)≜⟨gt,x⟩+αi
4⟨∇ft(xt),x−xt⟩2,
where αi∈ H, defined in (2.1) . For strongly convex functions, following the similar decomposition
as in Appendix B, we decompose the regret as
E[REGT] =E"TX
t=1⟨∇Ft(xt),xt−x⋆⟩#
−1
2TX
t=1DFt(x⋆,xt)−1
2TX
t=1DFt(x⋆,xt)
≤E"TX
t=1⟨gt,xt−x⋆⟩#
−λ
4TX
t=1∥xt−x⋆∥2−1
2TX
t=1DFt(x⋆,xt)
≤TX
t=1⟨gt,xt−xt,i⋆⟩ −λi⋆
4TX
t=1∥xt−xt,i⋆∥2
| {z }
META-REG
+E"TX
t=1hsc
t,i⋆(xt,i⋆)−hsc
t,i⋆(x⋆)#
| {z }
BASE-REG−1
2TX
t=1DFt(x⋆,xt),
where the second step, different from the exp-concave case, only requires the strong convexity of
Ft(·), and the third step is due to hsc
t,i(x)≜⟨gt,x⟩+λi
4∥x−xt∥2, where λi∈ H, defined in (2.1).
Meta Regret Analysis. Our Algorithm 1 can be applied to the SEA model without any algorithm
modifications. As a result, we directly use the same parameter configurations as in the proof of
Theorem 1 (i.e., in Appendix B).
Forconvex functions, the meta regret can be bounded as
META-REG≤Eh
C0p
1 +D2¯VT+C1i
≤C0q
1 +D2E[¯VT] +C1
22≤C0vuut1 + 5 D2(2σ2
1:T+ Σ2
1:T) + 20 D2LTX
t=1DFt(x⋆,xt) +C1 (by (C.2))
≤ Oq
σ2
1:T+ Σ2
1:T
+O(C10) +C0
2C10TX
t=1DFt(x⋆,xt),
where the second step is by Jensen’s inequality and the last step is due to AM-GM inequality. C10is
a constant to be specified.
Forexp-concave andstrongly convex functions, the meta regret is bounded in the same way as (B.6)
and (B.7), and thus omitted here.
Base Regret Analysis. Forconvex functions, the base regret can be bounded as
BASE-REG≤5Dq
1 +E[¯VT]≤5Dvuut1 + 10 σ2
1:T+ 5Σ2
1:T+ 20LTX
t=1DFt(x⋆,xt)
≤ Oq
σ2
1:T+ Σ2
1:T
+O(C11) +5D
2C11TX
t=1DFt(x⋆,xt),
where the first step is by Jensen’s inequality, the second step is due to (C.2) , and the last step is
because of AM-GM inequality. C11is a constant to be specified.
Forexp-concave functions, the base regret is bounded by (B.9) . Following (C.2) , we control the
empirical gradient variation defined on surrogates as
E"TX
t=2∇hexp
t,i⋆(xt,i⋆)− ∇hexp
t−1,i⋆(xt−1,i⋆)2#
≤3E[¯VT] + 6TX
t=1αi⋆
2gt⟨gt,xt−xt,i⋆⟩2
≤15(2σ2
1:T+ Σ2
1:T) + 60 LTX
t=1DFt(x⋆,xt) + 2α2
i⋆G2TX
t=1⟨gt,xt−xt,i⋆⟩2.
Plugging the surrogate’s empirical gradient variation back to the base regret, we obtain
BASE-REG≤16d
αi⋆ln 
1 +15αi⋆
8d(2σ2
1:T+ Σ2
1:T) +15Lαi⋆
2dTX
t=1DFt(x⋆,xt)
+α3
i⋆G2
4dTX
t=1⟨gt,xt−xt,i⋆⟩2!
≤ Od
αln 
σ2
1:T+ Σ2
1:T
+O(lnC12)
+120L
C12TX
t=1DFt(x⋆,xt) +4G2
C12TX
t=1⟨gt,xt−xt,i⋆⟩2,
where the second step requires C12≥1by Lemma 5.
Forstrongly convex functions, we need to delve into the proof details of the base algorithm, i.e.,
OOGD (B.8) with step size ηt= 2/(1 +λit)and optimism mt=∇hsc
t−1,i(xt−1,i). For example,
from Lemma 12 of Yan et al. [2023], the base regret can be bounded as
BASE-REG≤4TX
t=21
λi⋆tEh∇hsc
t,i⋆(xt,i⋆)− ∇hsc
t−1,i⋆(xt−1,i⋆)2i
+O(1).
Subsequently, we analyze the empirical gradient variation defined on surrogates in each round, i.e.,
∥∇hsc
t,i⋆(xt,i⋆)− ∇hsc
t−1,i⋆(xt−1,i⋆)∥2. Denoting by σ2
t≜maxx∈XEft∼Dt[∥∇ft(x)− ∇Ft(x)∥2]
andΣ2
t≜E[supx∈X∥∇Ft(x)− ∇Ft−1(x)∥2]for simplicity,
Eh∇hsc
t,i⋆(xt,i⋆)− ∇hsc
t−1,i⋆(xt−1,i⋆)2i
23=E"gt+λi⋆
2(xt,i⋆−xt)−gt−1−λi⋆
2(xt−1,i⋆−xt−1)2#
≤3E
∥gt−gt−1∥2
+ 3λi⋆
2(xt,i⋆−xt)2
+ 3λi⋆
2(xt−1,i⋆−xt−1)2
≤15(σ2
t+σ2
t−1+ 2LDFt(x⋆,xt) + 2LDFt−1(x⋆,xt−1) + Σ2
t) (by (C.2))
+λ2
i⋆∥xt,i⋆−xt∥2+λ2
i⋆∥xt−1,i⋆−xt−1∥2,
where the first step is due to the property of the surrogate: ∇hsc
t,i(xt,i) =gt+λi
2(xt,i−xt), and the
second step is due to the Cauchy-Schwarz inequality. Plugging the above term back into the base
regret and omitting the ignorable O(1)term, we achieve
BASE-REG≤60
λi⋆TX
t=2σ2
t+σ2
t−1+ Σ2
t
t+ 120 LTX
t=2DFt(x⋆,xt) +DFt−1(x⋆,xt−1)
λi⋆t
+ 4TX
t=2λ2
i⋆∥xt,i⋆−xt∥2+λ2
i⋆∥xt−1,i⋆−xt−1∥2
λi⋆t,
To handle the above term ofPT
t=1at/tfor some variable sequence {at}T
t=1, we import a useful
lemma from Yan et al. [2023].
Lemma 4 (Lemma 9 of Yan et al. [2023]) .For a sequence of {at}T
t=1andb, where at, b > 0for any
t∈[T], denoting by amax≜max tatandA≜⌈bPT
t=1at⌉, we have
TX
t=1at
bt≤amax
b(1 + ln A) +1
b2.
Using Lemma 4, we control the base regret as
BASE-REG≤ O1
λ 
σ2
max+ Σ2
max
lnσ2
1:T+ Σ2
1:T
σ2max+ Σ2max
+480LGD
λi⋆ln 
1 + 2 λi⋆TX
t=1DFt(x⋆,xt)!
+8D2
λi⋆ln 
1 + 2 λ3
i⋆TX
t=1∥xt,i⋆−xt∥2!
≤ O1
λ 
σ2
max+ Σ2
max
lnσ2
1:T+ Σ2
1:T
σ2max+ Σ2max
+O(lnC13+ lnC14)
+960LGD
C13TX
t=1DFt(x⋆,xt) +16D2
C14TX
t=2∥xt,i⋆−xt∥2,
where the first term initializes Lemma 4 as at=σ2
t+σ2
t−1+Σ2
t(i.e.,amax=O(σ2
max+Σ2
max)) and
b= 1/(σ2
max+ Σ2
max), the second term initializes Lemma 4 as at=DFt(x⋆,xt) +DFt−1(x⋆,xt−1)
(i.e., amax= 4GD due to Assumption 1) and b=λi⋆, the third term initializes Lemma 4 as
at=λ2
i⋆∥xt,i⋆−xt∥2+λ2
i⋆∥xt−1,i⋆−xt−1∥2(i.e.,amax= 2D2due to λi≤1and Assumption 1)
andb=λi⋆. The O(1)term contains ignorable terms like O(1/λ). The second step requires
C13, C14≥1by Lemma 5.
Regret Analysis. Forconvex functions, by combining the meta and base regret, it holds that
REGT≤ Oq
σ2
1:T+ Σ2
1:T
+O(C10+C11) +C0
2C10+5D
2C11−1TX
t=1DFt(x⋆,xt)
≤ Oq
σ2
1:T+ Σ2
1:T
,
by choosing C10=C0andC11= 5D. For exp-concave functions, by combining the meta and base
regret, it holds that
REGT≤ Od
αln 
σ2
1:T+ Σ2
1:T
+O(C3+ lnC12) +120L
C12−1
2TX
t=1DFt(x⋆,xt)
24+C0
2C3+4G2
C12−αi⋆
4TX
t=1⟨gt,xt−xt,i⋆⟩2≤ Od
αln 
σ2
1:T+ Σ2
1:T
,
by choosing C12= max {1,240L,32G2
αi⋆}andC3=4C0
αi⋆. Note that such a parameter configuration
will only add an O(1/α)factor to the final regret bound, which can be absorbed. For strongly convex
functions, by combining the meta and base regret, it holds that
REGT≤ O1
λ 
σ2
max+ Σ2
max
lnσ2
1:T+ Σ2
1:T
σ2max+ Σ2max
+O(C4+ lnC13+ lnC14)
+C0D2
2C4+16D2
C14−λi⋆
4TX
t=1∥xt−xt,i⋆∥2+960LGD
C13−1
2TX
t=1DFt(x⋆,xt)
≤ O1
λ 
σ2
max+ Σ2
max
lnσ2
1:T+ Σ2
1:T
σ2max+ Σ2max
,
by choosing C13= max {1,1920LGD},C14= max {1,128D2
λi⋆}andC4=4C0D2
λi⋆. Note that such a
parameter configuration will only add an O(1/λ)factor to the final bound, which can be absorbed.
Note that the constants C3, C4, C10, C11, C12, C13, C14only exist in analysis and thus can be chosen
arbitrarily, finishing the proof.
D Technical Lemmas
Lemma 5. For any a >1, b > 0, it holds that ln(a+b)≤ln(Ca) +b
Cfor some C≥1.
Proof. The one-line proof is presented below:
ln(a+b)≤ln(Ca+b)≤ln(Ca) + ln
1 +b
Ca
≤ln(Ca) +b
C,
where the first step is due to C≥1, and the last step adopts ln(1 + x)≤xfor any x≥0.
Lemma 6 (Corollary 5 of Orabona et al. [2012]) .Ifa, b, c, d, x > 0satisfy x−d≤aln(bx+c),
then it holds that
x−d≤aln
2abln2ab
e+ 2bd+ 2c
.
25NeurIPS Paper Checklist
1.Claims
Question: Do the main claims made in the abstract and introduction accurately reflect the
paper’s contributions and scope?
Answer: [Yes]
Justification: We have claimed the paper’s contribution in both the abstract and the introduc-
tion part.
Guidelines:
•The answer NA means that the abstract and introduction do not include the claims
made in the paper.
•The abstract and/or introduction should clearly state the claims made, including the
contributions made in the paper and important assumptions and limitations. A No or
NA answer to this question will not be perceived well by the reviewers.
•The claims made should match theoretical and experimental results, and reflect how
much the results can be expected to generalize to other settings.
•It is fine to include aspirational goals as motivation as long as it is clear that these goals
are not attained by the paper.
2.Limitations
Question: Does the paper discuss the limitations of the work performed by the authors?
Answer: [Yes]
Justification: One limitation we could discover is that our approach is not applicable in
multi-player games since it does not control the algorithmic stability, which is essential and
necessary in achieving fast rates in games.
Guidelines:
•The answer NA means that the paper has no limitation while the answer No means that
the paper has limitations, but those are not discussed in the paper.
• The authors are encouraged to create a separate ”Limitations” section in their paper.
•The paper should point out any strong assumptions and how robust the results are to
violations of these assumptions (e.g., independence assumptions, noiseless settings,
model well-specification, asymptotic approximations only holding locally). The authors
should reflect on how these assumptions might be violated in practice and what the
implications would be.
•The authors should reflect on the scope of the claims made, e.g., if the approach was
only tested on a few datasets or with a few runs. In general, empirical results often
depend on implicit assumptions, which should be articulated.
•The authors should reflect on the factors that influence the performance of the approach.
For example, a facial recognition algorithm may perform poorly when image resolution
is low or images are taken in low lighting. Or a speech-to-text system might not be
used reliably to provide closed captions for online lectures because it fails to handle
technical jargon.
•The authors should discuss the computational efficiency of the proposed algorithms
and how they scale with dataset size.
•If applicable, the authors should discuss possible limitations of their approach to
address problems of privacy and fairness.
•While the authors might fear that complete honesty about limitations might be used by
reviewers as grounds for rejection, a worse outcome might be that reviewers discover
limitations that aren’t acknowledged in the paper. The authors should use their best
judgment and recognize that individual actions in favor of transparency play an impor-
tant role in developing norms that preserve the integrity of the community. Reviewers
will be specifically instructed to not penalize honesty concerning limitations.
3.Theory Assumptions and Proofs
Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?
26Answer: [Yes]
Justification: The assumptions are provided and justified in Section 2. We provide theoretical
guarantees in Section 3 and Section 4, and all the corresponding proofs can be found in
Appendix B and Appendix C.
Guidelines:
• The answer NA means that the paper does not include theoretical results.
•All the theorems, formulas, and proofs in the paper should be numbered and cross-
referenced.
•All assumptions should be clearly stated or referenced in the statement of any theorems.
•The proofs can either appear in the main paper or the supplemental material, but if
they appear in the supplemental material, the authors are encouraged to provide a short
proof sketch to provide intuition.
•Inversely, any informal proof provided in the core of the paper should be complemented
by formal proofs provided in appendix or supplemental material.
• Theorems and Lemmas that the proof relies upon should be properly referenced.
4.Experimental Result Reproducibility
Question: Does the paper fully disclose all the information needed to reproduce the main ex-
perimental results of the paper to the extent that it affects the main claims and/or conclusions
of the paper (regardless of whether the code and data are provided or not)?
Answer: [NA]
Justification: This paper does not include experiments.
Guidelines:
• The answer NA means that the paper does not include experiments.
•If the paper includes experiments, a No answer to this question will not be perceived
well by the reviewers: Making the paper reproducible is important, regardless of
whether the code and data are provided or not.
•If the contribution is a dataset and/or model, the authors should describe the steps taken
to make their results reproducible or verifiable.
•Depending on the contribution, reproducibility can be accomplished in various ways.
For example, if the contribution is a novel architecture, describing the architecture fully
might suffice, or if the contribution is a specific model and empirical evaluation, it may
be necessary to either make it possible for others to replicate the model with the same
dataset, or provide access to the model. In general. releasing code and data is often
one good way to accomplish this, but reproducibility can also be provided via detailed
instructions for how to replicate the results, access to a hosted model (e.g., in the case
of a large language model), releasing of a model checkpoint, or other means that are
appropriate to the research performed.
•While NeurIPS does not require releasing code, the conference does require all submis-
sions to provide some reasonable avenue for reproducibility, which may depend on the
nature of the contribution. For example
(a)If the contribution is primarily a new algorithm, the paper should make it clear how
to reproduce that algorithm.
(b)If the contribution is primarily a new model architecture, the paper should describe
the architecture clearly and fully.
(c)If the contribution is a new model (e.g., a large language model), then there should
either be a way to access this model for reproducing the results or a way to reproduce
the model (e.g., with an open-source dataset or instructions for how to construct
the dataset).
(d)We recognize that reproducibility may be tricky in some cases, in which case
authors are welcome to describe the particular way they provide for reproducibility.
In the case of closed-source models, it may be that access to the model is limited in
some way (e.g., to registered users), but it should be possible for other researchers
to have some path to reproducing or verifying the results.
5.Open access to data and code
27Question: Does the paper provide open access to the data and code, with sufficient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?
Answer: [NA]
Justification: This paper does not include experiments requiring code.
Guidelines:
• The answer NA means that paper does not include experiments requiring code.
•Please see the NeurIPS code and data submission guidelines ( https://nips.cc/
public/guides/CodeSubmissionPolicy ) for more details.
•While we encourage the release of code and data, we understand that this might not be
possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
including code, unless this is central to the contribution (e.g., for a new open-source
benchmark).
•The instructions should contain the exact command and environment needed to run to
reproduce the results. See the NeurIPS code and data submission guidelines ( https:
//nips.cc/public/guides/CodeSubmissionPolicy ) for more details.
•The authors should provide instructions on data access and preparation, including how
to access the raw data, preprocessed data, intermediate data, and generated data, etc.
•The authors should provide scripts to reproduce all experimental results for the new
proposed method and baselines. If only a subset of experiments are reproducible, they
should state which ones are omitted from the script and why.
•At submission time, to preserve anonymity, the authors should release anonymized
versions (if applicable).
•Providing as much information as possible in supplemental material (appended to the
paper) is recommended, but including URLs to data and code is permitted.
6.Experimental Setting/Details
Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?
Answer: [NA]
Justification: This paper does not include experiments.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The experimental setting should be presented in the core of the paper to a level of detail
that is necessary to appreciate the results and make sense of them.
•The full details can be provided either with the code, in appendix, or as supplemental
material.
7.Experiment Statistical Significance
Question: Does the paper report error bars suitably and correctly defined or other appropriate
information about the statistical significance of the experiments?
Answer: [NA]
Justification: This paper does not include experiments.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The authors should answer ”Yes” if the results are accompanied by error bars, confi-
dence intervals, or statistical significance tests, at least for the experiments that support
the main claims of the paper.
•The factors of variability that the error bars are capturing should be clearly stated (for
example, train/test split, initialization, random drawing of some parameter, or overall
run with given experimental conditions).
•The method for calculating the error bars should be explained (closed form formula,
call to a library function, bootstrap, etc.)
28• The assumptions made should be given (e.g., Normally distributed errors).
•It should be clear whether the error bar is the standard deviation or the standard error
of the mean.
•It is OK to report 1-sigma error bars, but one should state it. The authors should
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
of Normality of errors is not verified.
•For asymmetric distributions, the authors should be careful not to show in tables or
figures symmetric error bars that would yield results that are out of range (e.g. negative
error rates).
•If error bars are reported in tables or plots, The authors should explain in the text how
they were calculated and reference the corresponding figures or tables in the text.
8.Experiments Compute Resources
Question: For each experiment, does the paper provide sufficient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?
Answer: [NA]
Justification: This paper does not include experiments.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The paper should indicate the type of compute workers CPU or GPU, internal cluster,
or cloud provider, including relevant memory and storage.
•The paper should provide the amount of compute required for each of the individual
experimental runs as well as estimate the total compute.
•The paper should disclose whether the full research project required more compute
than the experiments reported in the paper (e.g., preliminary or failed experiments that
didn’t make it into the paper).
9.Code Of Ethics
Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines ?
Answer: [Yes]
Justification: The research conducted in the paper conforms with the NeurIPS Code of
Ethics.
Guidelines:
•The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
•If the authors answer No, they should explain the special circumstances that require a
deviation from the Code of Ethics.
•The authors should make sure to preserve anonymity (e.g., if there is a special consid-
eration due to laws or regulations in their jurisdiction).
10.Broader Impacts
Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?
Answer: [NA]
Justification: This paper is a purely theoretical work, and we do not find specific societal
impacts that should be highlighted here.
Guidelines:
• The answer NA means that there is no societal impact of the work performed.
•If the authors answer NA or No, they should explain why their work has no societal
impact or why the paper does not address societal impact.
•Examples of negative societal impacts include potential malicious or unintended uses
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
(e.g., deployment of technologies that could make decisions that unfairly impact specific
groups), privacy considerations, and security considerations.
29•The conference expects that many papers will be foundational research and not tied
to particular applications, let alone deployments. However, if there is a direct path to
any negative applications, the authors should point it out. For example, it is legitimate
to point out that an improvement in the quality of generative models could be used to
generate deepfakes for disinformation. On the other hand, it is not needed to point out
that a generic algorithm for optimizing neural networks could enable people to train
models that generate Deepfakes faster.
•The authors should consider possible harms that could arise when the technology is
being used as intended and functioning correctly, harms that could arise when the
technology is being used as intended but gives incorrect results, and harms following
from (intentional or unintentional) misuse of the technology.
•If there are negative societal impacts, the authors could also discuss possible mitigation
strategies (e.g., gated release of models, providing defenses in addition to attacks,
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
feedback over time, improving the efficiency and accessibility of ML).
11.Safeguards
Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?
Answer: [NA]
Justification: This paper does not include experiments (data or models), and thus poses no
such risks.
Guidelines:
• The answer NA means that the paper poses no such risks.
•Released models that have a high risk for misuse or dual-use should be released with
necessary safeguards to allow for controlled use of the model, for example by requiring
that users adhere to usage guidelines or restrictions to access the model or implementing
safety filters.
•Datasets that have been scraped from the Internet could pose safety risks. The authors
should describe how they avoided releasing unsafe images.
•We recognize that providing effective safeguards is challenging, and many papers do
not require this, but we encourage authors to take this into account and make a best
faith effort.
12.Licenses for existing assets
Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?
Answer: [NA]
Justification: This paper does not use existing assets.
Guidelines:
• The answer NA means that the paper does not use existing assets.
• The authors should cite the original paper that produced the code package or dataset.
•The authors should state which version of the asset is used and, if possible, include a
URL.
• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
•For scraped data from a particular source (e.g., website), the copyright and terms of
service of that source should be provided.
•If assets are released, the license, copyright information, and terms of use in the
package should be provided. For popular datasets, paperswithcode.com/datasets
has curated licenses for some datasets. Their licensing guide can help determine the
license of a dataset.
•For existing datasets that are re-packaged, both the original license and the license of
the derived asset (if it has changed) should be provided.
30•If this information is not available online, the authors are encouraged to reach out to
the asset’s creators.
13.New Assets
Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?
Answer: [NA]
Justification: This paper does not release new assets.
Guidelines:
• The answer NA means that the paper does not release new assets.
•Researchers should communicate the details of the dataset/code/model as part of their
submissions via structured templates. This includes details about training, license,
limitations, etc.
•The paper should discuss whether and how consent was obtained from people whose
asset is used.
•At submission time, remember to anonymize your assets (if applicable). You can either
create an anonymized URL or include an anonymized zip file.
14.Crowdsourcing and Research with Human Subjects
Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as
well as details about compensation (if any)?
Answer: [NA]
Justification: This paper does not involve crowdsourcing nor research with human subjects.
Guidelines:
•The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
•Including this information in the supplemental material is fine, but if the main contribu-
tion of the paper involves human subjects, then as much detail as possible should be
included in the main paper.
•According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
or other labor should be paid at least the minimum wage in the country of the data
collector.
15.Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects
Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?
Answer: [NA]
Justification: This paper does not involve crowdsourcing nor research with human subjects.
Guidelines:
•The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
•Depending on the country in which research is conducted, IRB approval (or equivalent)
may be required for any human subjects research. If you obtained IRB approval, you
should clearly state this in the paper.
•We recognize that the procedures for this may vary significantly between institutions
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
guidelines for their institution.
•For initial submissions, do not include any information that would break anonymity (if
applicable), such as the institution conducting the review.
31