An Empirical Analysis of Compute-Optimal Inference
for Problem-Solving with Language Models
Anonymous Author(s)
Affiliation
Address
email
Abstract
The optimal training configurations of large language models (LLMs) with respect 1
to model sizes and compute budgets have been extensively studied. But how to 2
optimally configure LLMs during inference has not been explored in sufficient 3
depth. We study compute-optimal inference : designing models and inference 4
strategies that optimally trade off additional inference-time compute for improved 5
performance. As a first step towards understanding and designing compute-optimal 6
inference methods, we assessed the effectiveness and computational efficiency 7
of multiple inference strategies such as Greedy Search, Majority V oting, Best-of- 8
N, Weighted V oting, and their variants on two different Tree Search algorithms, 9
involving different model sizes (e.g., 7B and 34B) and computational budgets. We 10
found that a smaller language model with a novel tree search algorithm typically 11
achieves a Pareto-optimal trade-off. These results highlight the potential benefits of 12
deploying smaller models equipped with more sophisticated decoding algorithms 13
in end-devices to enhance problem-solving accuracy. For instance, we show that 14
the Llemma-7B model can achieve competitive accuracy to a Llemma-34B model 15
on MATH500 while using 2×less FLOPs. Our findings could potentially apply to 16
any generation task with a well-defined measure of success. 17
1 Introduction 18
Scaling laws of neural networks [Hestness et al., 2017, Rosenfeld et al., 2019] have been established 19
across a range of domains, including language modeling [Kaplan et al., 2020, Hoffmann et al., 2022, 20
OpenAI, 2023], image modeling [Henighan et al., 2020, Yu et al., 2022, Peebles and Xie, 2023], 21
video modeling [Brooks et al., 2024], reward modeling [Gao et al., 2023], and board games [Jones, 22
2021]. These studies have demonstrated how model performance is influenced by both the size of the 23
model and the amount of training computation. However, there is limited knowledge on how varying 24
the compute during inference affects model performance after the model has been trained. 25
To improve the task performance of large language models (LLMs), inference techniques typically 26
involve additional computation in a performance maximization step at inference time [Nye et al., 27
2021, Wei et al., 2022, Wang et al., 2022b, Yao et al., 2023, Chen et al., 2024b]. This cost must be 28
taken into account for compute-optimal inference. For example, a Monte Carlo Tree Search (MCTS) 29
method [Jones, 2021] may improve task performance, but potentially cost much more than simply 30
sampling solutions multiple times. Generally speaking, we need a comprehensive understanding of 31
how various inference-time methods (e.g., Best-of-N, majority voting) trade off between performance 32
and cost. To improve our understanding, this paper presents a thorough empirical evaluation with 33
careful analysis over various configurations of representative LLMs and inference algorithms. 34
Specifically, we explore how to select an optimal model size (e.g., 7B or 34B) for the policy model 35
and an effective inference strategy (e.g., Greedy Search, Majority V oting, Best-of-N, Weighted V oting, 36
Submitted to 38th Conference on Neural Information Processing Systems (NeurIPS 2024). Do not distribute.4 16 64 256 1024
Inference FLOPs per question (×1012)505560657075T est error
Inference scaling on MATH
Sampling (7B)
Sampling (34B)
MCTS (7B)
MCTS (34B)
4 16 64 256 1024
Inference FLOPs per question (×1012)505560657075T est error
Inference scaling on MATH
REBASE (7B)
REBASE (34B)Figure 1: The inference computation scaling laws exhibited in error rate on the MATH500 test set
based on weighted majority voting, where the left figure shows sampling vs. MCTS, and the right
figure shows our proposed REBASE . Clearly, the error rate decreases steadily when the computation
increases, and REBASE exhibits a Pareto-optimal tradeoff during inference.
and their Tree Search variants) to maximize performance (i.e., accuracy) within a given compute 37
budget. We manipulate the inference computation (FLOPs) of a fixed model by generating additional 38
tokens through the policy model, sampling further candidate solutions, and ranking them with a 39
reward model. We analyze the performance of a family of math-specialized LLMs (i.e., Llemma-7B 40
and Llemma-34B [Azerbayev et al., 2023]) fine-tuned on the MetaMath dataset [Yu et al., 2023] and 41
measure the error rate on the GSM8K test set [Cobbe et al., 2021a] and MATH500 test set [Hendrycks 42
et al., 2021b, Lightman et al., 2023b]. 43
Our analysis shows that voting-based methods generally outperform the strategy which selects the 44
best solution (i.e., Best-of-N), and weighted voting has the most favorable results (Section 4.3, 45
Figure 5 & 6). However, neither method shows a desirable behavior at high levels of compute. For 46
instance, weighted voting saturates when sampling more than 128 solutions (Figure 1). We have also 47
found that the commonly used MCTS method does not perform well with weighted voting, as it often 48
yields many unfinished solutions, hence having less votes. To address this issue, we propose a novel 49
tree search algorithm, REward BAlanced SEarch ( REBASE ), which pairs well with weighted voting 50
and improves the Pareto-optimal trade-off between accuracy and inference compute. The key idea of 51
REBASE is to use a node-quality based reward to control the exploitation and pruning properties of 52
tree search, while ensuring enough candidate solutions for voting or selection. 53
In our experiments, REBASE consistently outperforms sampling and MCTS methods across all 54
settings, models, and tasks. Importantly, we find that REBASE with a smaller language model 55
typically achieves a Pareto-optimal trade-off. For instance, we show that the Llemma-7B model can 56
achieve competitive accuracy to a Llemma-34B model while using 2×less FLOPs when evaluating 57
on MATH500 (Figure 1) or GSM8K (Figure 4). These findings underscore the advantages of using 58
smaller models with advanced inference-time algorithms on end-devices to improve problem-solving. 59
2 Related Works 60
Mathematical Reasoning with LLMs. Large language models have made significant progress 61
in recent years, and have exhibited strong reasoning abilities [Brown et al., 2020, Hoffmann et al., 62
2022, Chowdhery et al., 2022, Lewkowycz et al., 2022]. Mathematical problem solving is a key task 63
for measuring LLM reasoning abilities [Cobbe et al., 2021a, Hendrycks et al., 2021b]. [Ling et al., 64
2017] first developed the method of producing step by step solutions that lead to the final answer. 65
Later, [Cobbe et al., 2021b] extended the work by finetuning the pre-trained language model on a 66
large dataset to solve math word problems, a verifier is trained for evaluating solutions and ranking 67
solutions. Nye et al. [2021] train models to use a scratchpad and improve their performance on 68
algorithmic tasks. Wei et al. [2022] demonstrate that the reasoning ability of a language model can 69
be elicited through the prompting. Subsequent research [Kojima et al., 2022, Lewkowycz et al., 2022, 70
Zhou et al., 2022] in reasoning tasks has also highlighted the efficacy of rationale augmentation. We 71
choose problem solving in mathematics as the task to study the compute-optimal strategy since it 72
allows us to accurately evaluate the problem solving ability of LLMs. 73
2Figure 2: Illustration of compute-optimal scaling laws in training and inference. The Chinchilla
scaling law shows how to choose a model size and number of training tokens under a training-
compute budget, while ours shows how to choose a model size and an inference strategy under a
inference-compute budget.
Inference Strategies of LLM Problem Solving. A variety of inference (also called decoding) 74
strategies have been developed to generate sequences with a trained model. Deterministic methods 75
such as greedy decoding and beam search [Teller, 2000, Graves, 2012] find highly probable sequences, 76
often yielding high quality results but without diversity. Sampling algorithms (e.g., temperature 77
sampling [Ackley et al., 1985]) can produce a diverse set of results which are then aggregated to 78
achieve higher accuracy (e.g., via majority voting [Wang et al., 2022a]). Recent methods combine 79
search algorithms with modern LLMs, including breadth-first or depth-first search [Yao et al., 2023], 80
Monte-Carlo Tree Search (MCTS) [Zhang et al., 2023, Zhou et al., 2023, Liu et al., 2024, Choi et al., 81
2023], and Self-evaluation Guided Beam Search [Xie et al., 2023]. All of these methods show that 82
using search at inference time can lead to performance gains in various tasks. However, the trade-off 83
for the improved performance is the use of compute to perform the search. Analyzing the trade-off 84
between compute budget and LLM inference performance remains understudied. In this paper, we 85
systematically analyze the trade-off between compute budget and problem-solving performance, and 86
propose a tree search method that is empirically Pareto-optimal. 87
Process Reward Models. Process reward models (PRMs) have emerged as a technique to improve 88
the reasoning and problem-solving capabilities of LLMs. These models assign rewards to the 89
intermediate steps of the LLM generated sequences. PRMs have been shown effective in selecting 90
reasoning traces with a low error rate, and for providing rewards in reinforcement learning-style 91
algorithms [Uesato et al., 2022, Polu and Sutskever, 2020, Gudibande et al., 2023]. Ma et al. [2023] 92
applies the PRM to give rewards on the intermediate steps and guide the multi-step reasoning process. 93
The PRM can be either trained on human labeled data [Lightman et al., 2023a] or model-labeled 94
synthetic data [Wang et al., 2023]. In our work, we use the PRM as the reward model for selecting 95
generated solutions, and for selecting which partial solutions to explore in tree search. 96
3An Empirical Analysis of Compute-Optimal Inference for Problem-Solving 97
We explore the following question: Given a fixed FLOPs budget, how should one select an optimal 98
model size for the policy model, and an effective inference strategy to maximize performance (i.e., 99
accuracy)? To address this, we represent the problem-solving error rate E(N, T)as a function of the 100
number of model parameters Nand the number of generated tokens T. The computational budget C 101
is a deterministic function FLOPs (N, T), based on NandT. Our goal is to minimize Eunder the 102
test-time compute constraint FLOPs (N, T) =C: 103
Nopt(C), Topt(C) = arg min
N,T s.t. FLOPs (N,T )=CE(N, T) (1)
where Nopt(C)andTopt(C)denote the optimal allocation of a computational budget C. 104
3Here, the inference computation (FLOPs) for a fixed model can be modulated by generating more 105
tokens with the policy model, e.g., by sampling additional candidate solutions and subsequently 106
ranking them using a reward model. We primarily consider sampling and tree-search approaches 107
with reranking or majority voting as the means to consume more tokens, including Greedy Search, 108
Majority V oting, Best-of-N, Weighted V oting, and their variants on tree search methods. 109
3.1 Inference Strategies 110
3.1.1 Sampling 111
Greedy Search. This strategy generates tokens one at a time by selecting the highest probability token 112
at each step, without considering future steps. It is computationally efficient but often suboptimal in 113
terms of diversity. 114
Best-of-n. This strategy, also known as rejection sampling, samples multiple solutions and chooses 115
the one with the highest score given by the reward model. 116
Majority Voting. In this strategy, multiple model outputs are generated, and the final answer to the 117
problem is determined by the most frequently occurring answer in all the outputs. 118
Weighted Majority Voting. This strategy is a variant of majority voting in which the votes are 119
weighted based on the score given by the reward model. 120
3.1.2 Monte Carlo Tree Search (MCTS) 121
Monte Carlo Tree Search (MCTS) has proven effective in domains such as board games where 122
strategic decision-making is required [Silver et al., 2016, 2017, Jones, 2021]. Recent work has shown 123
that adapting MCTS to the context of LLMs can enhance the text generation process [Zhang et al., 124
2023, Zhou et al., 2023, Liu et al., 2024, Choi et al., 2023, Chen et al., 2024a, Tian et al., 2024, 125
Chen et al., 2024a]. In this context, MCTS is often paired with a value model to score and guide the 126
exploration steps. For additional background, we provide a review of MCTS in Appendix B. 127
Recent work in MCTS or its variants (e.g., Tree of Thoughts [Yao et al., 2023]) mainly focus on 128
improving the performance (e.g., accuracy) on the studied tasks. However, generic comparisons of 129
MCTS with conventional methods like Best-of-n and Majority V oting in terms of computational 130
budget, measured in generated tokens or processing time, are either scarce or indicating inference- 131
time issues. For example, MCTS consumes substantially more resources, often requiring dozens of 132
times more generated tokens than simpler methods. Specifically, a significant portion of the paths 133
in the search tree are used to estimate and select nodes, and these paths do not necessarily become 134
a part of the final candidate solution, although MCTS ensures that the sampled solutions comprise 135
high-quality intermediate steps. In contrast, sampling methods generate multiple solutions in parallel 136
and independently, and all the generated sequences are included in the candidate solutions. However, 137
the intermediate steps in these sequences are not guaranteed to be of high quality, as there is no 138
mechanism for pruning poor steps or exploiting promising ones. 139
This highlights the need for developing a new tree search method that can achieve a comparable (or 140
better) performance as MCTS, and that is computationally less costly, just like weighted majority 141
voting and best-of-n. This need leads to the development of our new method named Reward Balanced 142
SEarch (REBASE), as introduced next. 143
3.1.3 Reward Balanced Search (REBASE) 144
The REBASE tree search method inherits the exploitation and pruning properties of tree search, 145
while using the reward model alone to estimate the nodes’ qualities without additional computation 146
for estimating values by sampling children. The efficiency is achieved by constraining the total 147
expansion width of the tree at a certain depth. REBASE balances the expansion width among the 148
nodes at the same depth based on the rewards given by the Process Reward Model (PRM). The details 149
are provided below: 150
Notations. We consider the fine-tuned LLM as a policy πθ. Given a question qand the first ksteps 151
of a solution x1,···, xk, the(k+ 1) -th step is produced by πθ(xk+1|q, x 1···xk). When generating 152
solutions using tree search, the root of the tree corresponds to the question q. The node corresponding 153
4Figure 3: Illustration of one iteration of REward BAlanced SEarch (REBASE).
toxk+1is the child of the node corresponding to xkif it is sampled from πθ(·|q, x 1···, xk). The 154
reward of a node n(xk)is determined by the PRM as R(n(xk)) =R(q, x 1,···, xk). 155
Initialization. Given the question q, balance temperature Tb, and sampling number of solutions N, 156
we sample N instances of the first step for the question, yielding all the nodes of depth 1 in the search 157
tree. We set the sampling budget of depth 0 B0=Nas initialization. 158
Reward modeling and update. In the i-th iteration, the PRM assigns the rewards to all the nodes 159
at depth i. After that, the algorithm examines whether the solutions up to depth iare complete. 160
Supposing there are Cicompleted solutions, we update the sampling budget using Bi←Bi−1−Ci. 161
IfBi= 0, the process ends, and we obtain Nsolutions. 162
Exploration balancing and expansion. For all of the nodes njwith reward R(nj)in the depth iof 163
the tree, we calculate the expansion width of the njas: 164
Wj=Round
Biexp (R(nj)/Tb)P
kexp (R(nk)/Tb)
. (2)
Then we sample Wjchildren for njfor all the nodes in depth i, and start the next iteration. 165
3.1.4 Theoretical Analysis 166
Before empirically studying the scaling effects of increasing the inference-time compute budget, 167
we present two theorems which will help us understand the experimental results later. These two 168
theorems give an upper bound on the performance of sampling when fixing the LLM generator. 169
We assume the vocabulary is limited and the sequence length is constrained, thus the number of 170
possible solutions and answers are finite. The proofs are provided in the Appendix A. 171
Theorem 1. Given a test dataset Dand a LLM π.|A|is the finite set of all possible answers given 172
by LLM, the ground truth function gmaps test data dto the true answer. Denote the accuracy of the 173
LLM on this dataset with majority over N samples as ACC MV(π,D, N). The accuracy of majority 174
voting on the LLM will eventually saturate, i.e. 175
lim
N→∞ACC MV(π,D, N) =P
d∈DI((g(d) = arg maxa∈Aπ(a|d))
|D|. (3)
52 4 8 16 32 64 128 256
Inference FLOPs per question (×1012)1520253035T est error
Infer. scaling on GSM8K (Weighted Majority)
Sampling (Llemma-7B)
Sampling (Llemma-34B)
REBASE (Llemma-7B)
REBASE (Llemma-34B)
2 4 8 16 32 64 128 256
Inference FLOPs per question (×1012)1520253035T est error
Infer. scaling on GSM8K (Best-of-N)
Sampling (Llemma-7B)
Sampling (Llemma-34B)
REBASE (Llemma-7B)
REBASE (Llemma-34B)Figure 4: The inference computation scaling comparisons across different model sizes . The
left/right panel shows the GSM8K problem-solving error rate on GSM8K based on Weighted
Mjority/Best-of-N.
where π(x|d)denotes the probability that the LLM answers xgiven input dand Iis the indicator 176
function. 177
Theorem 2. Assume the reward model assigns an expected reward of R(a)toa∈ A among the 178
different solutions generated by LLM that yields a. Given a test dataset Dand a LLM π.|A|is the 179
finite set of all possible answers given by LLM, the ground truth function gmaps test data dto the 180
true answer. Denote the accuracy of the LLM on this dataset with weighted majority over N samples 181
asACC WV(π,D, N, R ). The accuracy of weighted majority voting on the LLM will eventually 182
saturate, i.e. 183
lim
N→∞ACC WV(π,D, N, R ) =P
d∈DI((g(d) = arg maxa∈AR(a)π(a|d))
|D|. (4)
where π(x|d)denotes the probability that the LLM answers xgiven input dand Idenotes the 184
indicator function. 185
Theorem 2 shows that as long as the reward model assigns higher rewards than the policy for correct an- 186
swers versus other answers in expectation, the upper bound of Weighted Majority V oting will be higher 187
than Majority V oting since I((g(d) = arg maxa∈AR(a)π(a|d))>I((g(d) = arg maxa∈Aπ(a|d)). 188
We put the figures comparing BoN and Weighted Majority V oting in the main paper and leave the 189
Majority V oting figures in Appendix D since Majority V oting is dominated by Weighted Majority 190
V oting. 191
4 Experiments 192
4.1 Setup 193
Datasets. We conduct experiments on two mathematical problem-solving datasets to investigate 194
the scaling effects of computation and our REBASE method for both challenging and simpler 195
problems. Specifically, MATH [Hendrycks et al., 2021a] and GSM8K[Cobbe et al., 2021b] are 196
datasets containing high school mathematics competition-level problems and grade-school level 197
mathematical reasoning problems, respectively. Following [Lightman et al., 2023b, Wang et al., 2024, 198
Sun et al., 2024], we use the MATH500 subset as our test set. 199
Generators. We use Llemma-7B and Llemma-34B [Azerbayev et al., 2024] as our base models and 200
finetune them on the MetaMath dataset [Yu et al., 2024] using full parameter supervised fine-tuning 201
(Full-SFT), The detailed finetuning configuration is given in the Appendix. Additionally, we test the 202
Mistral-7B model to expand our findings across different models. 203
Reward Model. All of the experiments use the same Llemma-34B reward model, which we 204
finetuned on the synthetic process reward modeling dataset, Math-Shepherd [Wang et al., 2024]. We 205
added a reward head to make the model, enabling it to output a scalar reward at the end of each step. 206
64 16 64 256 1024
Infer. FLOPs per question (×1012)505560657075T est error
Llemma-7B on MATH
Sampling W.M.
Sampling BoN
REBASE W.M.
REBASE BoN
1632641282565121024
Infer. FLOPs per question (×1012)5055606570T est error
Llemma-34B on MATH
Sampling W.M.
Sampling BoN
REBASE W.M.
REBASE BoN
4 16 64 256 1024
Infer. FLOPs per question (×1012)5560657075T est error
Mistral-7B on MATH
Sampling W.M.
Sampling BoN
REBASE W.M.
REBASE BoNFigure 5: The inference computation scaling laws of different models for the problem-solving
error rate on MATH500 test set. The tested models are Llemma-7B (left), Llemma-34B (middle),
& Mistral-7B (right). In the legend, W.M. and BoN refer to Weighted Majority and Best-of-N,
respectively.
Inference Configuration. For the MATH dataset, we sample 1, 2, 4, 8, 16, 32, 64, 128, and 256 207
solutions for the 7B models, and 1 to 64 solutions for the 34B Llemma model. For the GSM8K dataset, 208
we sample 1 to 32 solutions, as it is relatively easier. We use sampling and REBASE to generate 209
these samples and select the answer through Best-of-N, Majority V oting, and Weighted V oting. 210
Each configuration is run multiple times to calculate the mean and variance, thereby mitigating the 211
randomness and ensuring the reliability of our conclusions. 212
4.2 Main Results of Compute-Optimal Inference 213
In order to compare the compute budgets of 7B and 34B models, we plot the figures with the number 214
of FLOPs used per question during inference. We compute the inference FLOPs based on the standard 215
formula from [Kaplan et al., 2020]. 216
Llemma-7B model achieves competitive accuracy to Llemma-34B model with lower compute 217
budget. Figures 1 and 4 show the curves of error rates versus total number of inference FLOPs per 218
question. Inference methods with different model sizes are plotted in the same diagram. We found 219
that Llemma-7B costs approximately 2x less total FLOPs than Llemma-34B under the same method 220
(Sampling, MCTS, REBASE) and task (MATH, GSM8K) while achieving competitive accuracy. 221
This result suggests that, with the same training dataset and model family, training and inference with 222
a smaller model could be more favorable in terms of compute budget if multiple sampling or search 223
methods are employed. 224
All inference configurations will saturate eventually. This is expected as Theorem 1 and Theorem 225
2 show. Also illustrated in Figures 5 and 6, the slope of the erro rate curves start large, then decreases 226
and the curves finally become nearly flat as the number of samples scales, showing the effect of 227
saturation. 228
Scaling law of compute-optimal inference. The findings in our experiments are consistent with 229
the Theorem 1 and 2, After a threshold the accruacy of sampling more solutions saturate, we should 230
scale the model size. We interpolate the smoothed test error rate curve in Figure 1 and Figure 4, 231
and fit power laws to estimate the optimal model size Nand number of generated tokens Tfor any 232
given amount of compute. We obtained a relationship Nopt∝CaandTopt∝Cb, where a= 1.0 233
andb= 0.0for both sampling-based weighted voting and our tree-search method REBASE . Our 234
fitted curves indicate that the optimal inference strategy is invariant to the amount of compute (e.g., 235
re-ranking with 32sampled solutions or REBASE tree search with a compute budget of 64for 236
MATH), and the optimal model size grows linearly with the increased compute budget. 237
4.3 Comparing REBASE to Other Baselines 238
REBASE is Pareto-optimal. While MCTS undeperforms Sampling (Fig. 1), from Fig. 1, 4, 5, 239
and 6, we found that REBASE consistently outperforms the Sampling method in all settings, when 240
72 4 8 16 32 64
Infer. FLOPs per question (×1012)1520253035T est error
Llemma-7B on GSM8K
Sampling W.M.
Sampling BoN
REBASE W.M.
REBASE BoN
16 32 64 128 256
Infer. FLOPs per question (×1012)12141618202224T est error
Llemma-34B on GSM8K
Sampling W.M.
Sampling BoN
REBASE W.M.
REBASE BoN
2 4 8 16 32 64
Infer. FLOPs per question (×1012)1012141618202224T est error
Mistral-7B on GSM8K
Sampling W.M.
Sampling BoN
REBASE W.M.
REBASE BoNFigure 6: The inference computation scaling laws of different models for the problem-solving
error rate on GSM8K test set. The tested models are Llemma-7B (left), Llemma-34B (middle),
& Mistral-7B (right). In the legend, W.M. and BoN refer to Weighted Majority and Best-of-N,
respectively.
Table 1: REBASE with lower compute budget has competitive accuracy against Sampling with
higher compute budget. We use weighted voting to aggreagte the candidate solutions in both Sampling
and REBASE.
# SAMPLES FLOP S MATH500
MISTRAL -7B
SAMPLING 256 8.7×101442.8
REBASE 32 1.36×101445.0
LLEMMA -7B
SAMPLING 256 10.0×101445.5
REBASE 32 1.48×101446.8
LLEMMA -34B
SAMPLING 64 12.1×101446.7
REBASE 32 7.08×101449.2
fixing the model and the evaluation task. Table 1 shows that REBASE can achieve competitive 241
accuracy with even a lower compute budget than the sampling method. This finding is novel, and 242
differs from previous tree search works which typically improve the performance at the cost of higher 243
computational expense compared to sampling [Chen et al., 2024a, Xie et al., 2023]. Table 2 shows 244
that given the same compute budget (sampling 32 solutions for the 7B model and 8 solutions for 34B 245
model), using REBASE yields higher accuray than sampling. 246
Weaker models gain more from Tree Search. From Fig. 2, we saw that compared with sampling, 247
Mistral-7B, Llemma-7B, Llemma-34B increase 5.3%,3.3%,2.6%in MATH and 0.7%,1.9%,0.9% 248
in GSM8K. The order of accuracy increase is inversely related to the model’s corresponding greedy 249
search on those datasets. This suggests that weaker models, as indicated by their lower greedy search 250
accuracy, benefit more from tree search methods like REBASE. 251
REBASE saturates later than sampling with higher accuray. From Figure 5 and Figure 6, we 252
observe that both sampling and REBASE saturate early in GSM8K and relatively late in MATH, 253
which we attribute to the difference of the difficulty level. This can be explained through the LLM 254
may assign high probability to the true answer in easy problems than those of harder problem, as 255
suggested by Theorem 1 and 2 with their proofs A. On MATH (Figure 5), we see that REBASE 256
finally saturates with a higher accuracy than sampling. We hypothesize the reason is that REBASE 257
samples the truth answer with higher probability than sampling. And as demonstrated by Theorem 1 258
and 2, the upper bound becomes higher. 259
8Table 2: Accuracy of diffrent inference configurations under a specific compute budget. MV , BoN
and WV denote Majority V oting, Best-of-N and Weighted V oting, respectively.
# SAMPLES MATH FLOP SGSM8K FLOP SMATH500 GSM8K
MISTRAL -7B
GREEDY 1 3.4×10122.3×101228.6 77.9
SAMPLING + MV 32 109 .2×101272.6×101236.1 85.7
SAMPLING + B ON 32 109 .2×101272.6×101240.3 89.4
SAMPLING + WV 32 109 .2×101272.6×101239.7 89.1
REBASE + MV 32 136 .2×101278.9×101244.1 88.8
REBASE + B ON 32 136 .2×101278.9×101245.4 89.4
REBASE + WV 32 136 .2×101278.9×101245.0 89.8
LLEMMA -7B
GREEDY 1 3.92×10122.3×101230.0 68.5
SAMPLING + MV 32 125 .4×101273.9×101241.0 80.0
SAMPLING + B ON 32 125 .4×101273.9×101241.7 85.6
SAMPLING + WV 32 125 .4×101273.9×101243.5 85.4
REBASE + MV 32 148 .0×101282.6×101246.1 86.1
REBASE + B ON 32 148 .0×101282.6×101244.1 86.9
REBASE + WV 32 148 .0×101282.6×101246.8 87.3
LLEMMA -34B
GREEDY 1 19.0×101211.2×101233.0 78.4
SAMPLING + MV 8 152 .3×101289.7×101239.9 84.3
SAMPLING + B ON 8 152 .3×101289.7×101240.4 86.7
SAMPLING + WV 8 152 .3×101289.7×101241.0 86.0
REBASE + MV 8 176 .8×101298.7×101243.9 86.1
REBASE + B ON 8 176 .8×101298.7×101243.6 86.9
REBASE + WV 8 176 .8×101298.7×101242.9 86.9
5 Conclusion & Limitations 260
In this work, we have conducted a comprehensive empirical analysis of compute-optimal inference 261
for problem-solving with language models. Our study has revealed several key findings. First, with 262
an optimal inference configuration, a small language model can achieve competitive accuracy to a 263
4×larger model while using approximately 2×less total FLOPs under the same inference method 264
(Sampling, MCTS, REBASE) and task (MATH, GSM8K), suggesting that training and inference 265
with smaller models could be more favorable in terms of compute budget when combined with 266
multiple sampling or search strategies. Second, our new REBASE tree-search method consistently 267
outperforms sampling (and MCTS) across all settings, models, and tasks, achieving competitive 268
accuracy with even lower compute budget compared to sampling. Our findings highlight the potential 269
of deploying smaller models equipped with more sophisticated inference strategies like REBASE to 270
enhance problem-solving accuracy while maintaining computational efficiency. 271
Limitations First, our experiments focused specifically on mathematical problem-solving tasks, 272
and the generalizability of our findings to other domains remains to be explored. Second, we only 273
investigated a limited range of model scales, primarily focusing on 7B and 34B models. Future 274
research could extend our analysis to a wider range of model sizes to gain a more comprehensive 275
understanding of the scaling laws for compute-optimal inference. Finally, our experiments mainly 276
utilized the MetaMath dataset for training the models. It would be valuable to explore the impact of 277
different training datasets on the performance and efficiency of compute-optimal inference strategies 278
for mathematical problem-solving. 279
9References 280
David H Ackley, Geoffrey E Hinton, and Terrence J Sejnowski. A learning algorithm for boltzmann 281
machines. Cognitive science , 9(1):147–169, 1985. 282
Zhangir Azerbayev, Hailey Schoelkopf, Keiran Paster, Marco Dos Santos, Stephen McAleer, Albert Q 283
Jiang, Jia Deng, Stella Biderman, and Sean Welleck. Llemma: An open language model for 284
mathematics. arXiv preprint arXiv:2310.10631 , 2023. 285
Zhangir Azerbayev, Hailey Schoelkopf, Keiran Paster, Marco Dos Santos, Stephen McAleer, Albert Q. 286
Jiang, Jia Deng, Stella Biderman, and Sean Welleck. Llemma: An open language model for 287
mathematics, 2024. 288
Tim Brooks, Bill Peebles, Connor Holmes, Will DePue, Yufei Guo, Li Jing, David Schnurr, 289
Joe Taylor, Troy Luhman, Eric Luhman, Clarence Ng, Ricky Wang, and Aditya Ramesh. 290
Video generation models as world simulators. 2024. URL https://openai.com/research/ 291
video-generation-models-as-world-simulators . 292
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D. Kaplan, Prafulla Dhariwal, 293
Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are 294
few-shot learners. Advances in Neural Information Processing Systems , 33:1877–1901, 2020. 295
Guoxin Chen, Minpeng Liao, Chengxi Li, and Kai Fan. Alphamath almost zero: process supervision 296
without process, 2024a. 297
Ziru Chen, Michael White, Raymond Mooney, Ali Payani, Yu Su, and Huan Sun. When is tree search 298
useful for llm planning? it depends on the discriminator. arXiv preprint arXiv:2402.10890 , 2024b. 299
Sehyun Choi, Tianqing Fang, Zhaowei Wang, and Yangqiu Song. Kcts: Knowledge-constrained tree 300
search decoding with token-level hallucination detection, 2023. 301
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam 302
Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. PaLM: 303
Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311 , 2022. 304
Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, 305
Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John 306
Schulman. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168 , 307
2021a. 308
Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, 309
Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve 310
math word problems. arXiv preprint arXiv:2110.14168 , 2021b. 311
Leo Gao, John Schulman, and Jacob Hilton. Scaling laws for reward model overoptimization. In 312
International Conference on Machine Learning , pages 10835–10866. PMLR, 2023. 313
Alex Graves. Sequence transduction with recurrent neural networks, 2012. 314
Arnav Gudibande, Eric Wallace, Charlie Snell, Xinyang Geng, Hao Liu, Pieter Abbeel, Sergey Levine, 315
and Dawn Song. The false promise of imitating proprietary llms. arXiv preprint arXiv:2305.15717 , 316
2023. 317
Dan Hendrycks, Steven Basart, Saurav Kadavath, Mantas Mazeika, Akul Arora, Ethan Guo, Collin 318
Burns, Samir Puranik, Horace He, Dawn Song, et al. Measuring coding challenge competence 319
with apps. arXiv preprint arXiv:2105.09938 , 2021a. 320
Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn 321
Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. In 322
Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track 323
(Round 2) , 2021b. 324
Tom Henighan, Jared Kaplan, Mor Katz, Mark Chen, Christopher Hesse, Jacob Jackson, Heewoo Jun, 325
Tom B. Brown, Prafulla Dhariwal, Scott Gray, et al. Scaling laws for autoregressive generative 326
modeling. arXiv preprint arXiv:2010.14701 , 2020. 327
10Joel Hestness, Sharan Narang, Newsha Ardalani, Gregory Diamos, Heewoo Jun, Hassan Kianinejad, 328
Md Patwary, Mostofa Ali, Yang Yang, and Yanqi Zhou. Deep learning scaling is predictable, 329
empirically. arXiv preprint arXiv:1712.00409 , 2017. 330
Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza 331
Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. 332
Training compute-optimal large language models. arXiv preprint arXiv:2203.15556 , 2022. 333
Andy L Jones. Scaling scaling laws with board games. arXiv preprint arXiv:2104.03113 , 2021. 334
Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, 335
Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. 336
arXiv preprint arXiv:2001.08361 , 2020. 337
Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large 338
language models are zero-shot reasoners. arXiv preprint arXiv:2205.11916 , 2022. 339
Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ra- 340
masesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, et al. Solving quantitative 341
reasoning problems with language models. arXiv preprint arXiv:2206.14858 , 2022. 342
Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan 343
Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Let’s verify step by step. arXiv preprint 344
arXiv:2305.20050 , 2023a. 345
Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, 346
John Schulman, Ilya Sutskever, and Karl Cobbe. Let’s verify step by step, 2023b. 347
Wang Ling, Dani Yogatama, Chris Dyer, and Phil Blunsom. Program induction by rationale gen- 348
eration: Learning to solve and explain algebraic word problems. In Proceedings of the 55th 349
Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pages 350
158–167, 2017. 351
Jiacheng Liu, Andrew Cohen, Ramakanth Pasunuru, Yejin Choi, Hannaneh Hajishirzi, and Asli 352
Celikyilmaz. Don’t throw away your value model! generating more preferable text with value- 353
guided monte-carlo tree search decoding, 2024. 354
Qianli Ma, Haotian Zhou, Tingkai Liu, Jianbo Yuan, Pengfei Liu, Yang You, and Hongxia Yang. 355
Let’s reward step by step: Step-level reward model as the navigators for reasoning, 2023. 356
Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David 357
Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, et al. Show your work: 358
Scratchpads for intermediate computation with language models. arXiv preprint arXiv:2112.00114 , 359
2021. 360
OpenAI. Gpt-4 technical report, 2023. 361
William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of 362
the IEEE/CVF International Conference on Computer Vision , pages 4195–4205, 2023. 363
Stanislas Polu and Ilya Sutskever. Generative language modeling for automated theorem proving. 364
arXiv preprint arXiv:2009.03393 , 2020. 365
Jonathan S Rosenfeld, Amir Rosenfeld, Yonatan Belinkov, and Nir Shavit. A constructive prediction 366
of the generalization error across scales. arXiv preprint arXiv:1909.12673 , 2019. 367
David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, 368
Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering 369
the game of Go with deep neural networks and tree search. Nature , 529(7587):484–489, 2016. 370
David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez, 371
Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, et al. Mastering the game of go without 372
human knowledge. nature , 550(7676):354–359, 2017. 373
11Zhiqing Sun, Longhui Yu, Yikang Shen, Weiyang Liu, Yiming Yang, Sean Welleck, and Chuang 374
Gan. Easy-to-hard generalization: Scalable alignment beyond human supervision. arXiv preprint 375
arXiv:2403.09472 , 2024. 376
Virginia Teller. Speech and language processing: an introduction to natural language processing, 377
computational linguistics, and speech recognition, 2000. 378
Ye Tian, Baolin Peng, Linfeng Song, Lifeng Jin, Dian Yu, Haitao Mi, and Dong Yu. Toward self- 379
improvement of llms via imagination, searching, and criticizing. arXiv preprint arXiv:2404.12253 , 380
2024. 381
Jonathan Uesato, Nate Kushman, Ramana Kumar, Francis Song, Noah Siegel, Lisa Wang, Antonia 382
Creswell, Geoffrey Irving, and Irina Higgins. Solving math word problems with process- and 383
outcome-based feedback. arXiv preprint arXiv:2211.14275 , 2022. 384
Peiyi Wang, Lei Li, Zhihong Shao, RX Xu, Damai Dai, Yifei Li, Deli Chen, Y Wu, and Zhifang 385
Sui. Math-shepherd: Verify and reinforce llms step-by-step without human annotations. CoRR, 386
abs/2312.08935 , 2023. 387
Peiyi Wang, Lei Li, Zhihong Shao, R. X. Xu, Damai Dai, Yifei Li, Deli Chen, Y . Wu, and Zhifang 388
Sui. Math-shepherd: Verify and reinforce llms step-by-step without human annotations, 2024. 389
Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdh- 390
ery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. 391
International Conference on Learning Representations (ICLR 2023) , 2022a. 392
Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, and 393
Hannaneh Hajishirzi. Self-instruct: Aligning language model with self generated instructions. 394
arXiv preprint arXiv:2212.10560 , 2022b. 395
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. 396
Chain-of-thought prompting elicits reasoning in large language models. NeurIPS , 2022. 397
Yuxi Xie, Kenji Kawaguchi, Yiran Zhao, Xu Zhao, Min-Yen Kan, Junxian He, and Qizhe Xie. 398
Self-evaluation guided beam search for reasoning, 2023. 399
Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L Griffiths, Yuan Cao, and Karthik 400
Narasimhan. Tree of thoughts: Deliberate problem solving with large language models. arXiv 401
preprint arXiv:2305.10601 , 2023. 402
Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan, 403
Alexander Ku, Yinfei Yang, Burcu Karagol Ayan, et al. Scaling autoregressive models for content- 404
rich text-to-image generation. arXiv preprint arXiv:2206.10789 , 2(3):5, 2022. 405
Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengying Liu, Yu Zhang, James T Kwok, Zhenguo 406
Li, Adrian Weller, and Weiyang Liu. Metamath: Bootstrap your own mathematical questions for 407
large language models. arXiv preprint arXiv:2309.12284 , 2023. 408
Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengying Liu, Yu Zhang, James T. Kwok, 409
Zhenguo Li, Adrian Weller, and Weiyang Liu. Metamath: Bootstrap your own mathematical 410
questions for large language models, 2024. 411
Shun Zhang, Zhenfang Chen, Yikang Shen, Mingyu Ding, Joshua B. Tenenbaum, and Chuang Gan. 412
Planning with large language models for code generation, 2023. 413
Andy Zhou, Kai Yan, Michal Shlapentokh-Rothman, Haohan Wang, and Yu-Xiong Wang. Language 414
agent tree search unifies reasoning acting and planning in language models, 2023. 415
Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han, Keiran Paster, Silviu Pitis, Harris Chan, 416
and Jimmy Ba. Large language models are human-level prompt engineers. arXiv preprint 417
arXiv:2211.01910 , 2022. 418
12A Proofs of Theorem 1 and 2 419
A.1 Proof of Theorem 1 420
Proof. Suppose the possible answers of the LLM are x1, x2, x3,···, x|A|, with π(x1|d)> 421
π(x2|d)>···> π(x|A||d). After sampling Nsolutions from the LLM, we denote the occurence of 422
xiasf(xi), the probability that x1is not the most frequent output is 423
P(f(x1)̸= arg max
xf(x)) (5)
With Union bound, we get 424
P(x1̸= arg max
xf(x)) (6)
≤|A|X
i=2P(f(x1)≤f(xi)) (7)
≤|A|P(f(x1)≤f(x2)) (8)
=|A|(1−P(f(x1)≥f(x2))) (9)
≤|A|
1−P
f(x1)≥π(x1|d) +π(x2|d)
2N
P
f(x2)≤π(x1|d) +π(x2|d)
2N
(10)
≤|A|
1−
1−e−δ2
1
2π(x1|d)N
1−e−δ2
2
2+δ2π(x2|d)N
(11)
≤|A|CNfor some C < 1. (12)
Where (11) is by Chernoff Bound, δ1=π(x1|d)−π(x2|d)
2π(x1|d)andδ2=π(x1|d)−π(x2|d)
2π(x2|d). AsN→ ∞ , we 425
have 426
f(x) =M(x|N) = 1 ifx= arg maxa∈Aπ(a|d)
M(x|N) = 0 otherwise .(13)
Where M(x|N)denotes the probability that majority voting over Nsampled solutions gives x. The 427
proof of original theorem is automatically completed by (13). 428
A.2 Proof of Theorem 2 429
Proof. The proof is similar to the Theorem 1, We rank x1, x2,···, x|A|withR(x1)f(x1)>···> 430
R(x|A|f(x|A|). Denotes w(xi)as the the total weights of answer xiafter sampling N solutions. As 431
N→ ∞ ,w(xi)→R(xi)f(xi). Same as proof in theorem 1, we have 432
P(x1̸= arg max
xf(x)) (14)
≤|A|P(w(x1)≤w(x2)) (15)
=|A|(1−P(w(x1)≥w(x2))) (16)
≤|A|
1−P
w(x1)≥v(x1) +v(x2)
2N
P
w(x2)≤v(x1) +v(x2)
2N
. (17)
Where v(x) =R(x)π(x|d), the remaining proof completely follows Theorem 1. 433
B MCTS Details 434
The MCTS process can be represented as the following steps: 435
Selection The process begins at the root node. Here, the algorithm recursively selects the child 436
node that offers the highest Upper Confidence Bound applied to Trees (UCT) value, continuing until 437
a node is reached that has not been expanded. The UCT is calculated using the formula 438
UCT (s) =Q(s) +Cs
ln (N(Parent (s)))
N(s). (18)
13Table 3: Fine-tuning Hyper-parameters: LR refers to the learning rate, BS refers to the batch size.
Llemma-7B and LLemma-34B are the generators we use in our experiments, RM is short for Reward
Model.
Model # Epoch Dataset BS LR Max Seq Length Dtype
Llemma-7B 1 MetaMath 128 8E-6 1024 FP32
Llemma-34B 1 MetaMath 128 8E-6 768 FP32
Llemma-34B RM 2 Math-Shepherd 128 1E-5 768 BF16
4 16 64 256 1024
Infer. FLOPs per question (×1012)505560657075T est error
Llemma-7B on MATH
Sampling M.V.
REBASE M.V.
1632641282565121024
Infer. FLOPs per question (×1012)5055606570T est error
Llemma-34B on MATH
Sampling M.V.
REBASE M.V.
4 16 64 256 1024
Infer. FLOPs per question (×1012)5560657075T est error
Mistral-7B on MATH
Sampling M.V.
REBASE M.V.
Figure 7: The inference computation scaling laws of different models for the problem-solving
error rate on MATH test set. The tested models are Llemma-7B (left), Llemma-34B (middle), &
Mistral-7B (right). In the legend, M.V . refer to Majority V oting.
Where Q(s)represents the quality score of node s,N(s)is the number of visits to node s, andCis a 439
constant determining the level of exploration. 440
Expansion and evaluation Upon reaching a non-terminal node s, the node is expanded by gener- 441
ating multiple child nodes. Each child node cis then evaluated using a value function V(c), which 442
predicts the potential quality of continuing the sequence from node c. 443
Backpropagation After evaluation, the algorithm updates the UCT values and the visit counts for 444
all nodes along the path from the selected node back to the root. For any node nin this path, the 445
updates are made as follows: 446
N(n)←N(n) + 1,
Q(n)←(N(n)−1)Q(n) +V(s)
N(n).
C Hyper-parameters 447
Finetuning We put all the hyperparameters of fine-tuned models in the table 3. We preprocess the 448
MetaMath Dataset to make the solutions in a stepwise format. 449
Inference For all the inference strategies, the temperature of the LLM is set to 1.0. Max tokens 450
for the output is 1024 and max tokens for one step is 256. For REBASE, we chose the balance 451
temperature (the softmax temperature in the REBASE algorithm) as Tb= 0.1. For MCTS, we set 452
Cin the UCT value as 1 and we expand 4,8,16children for the root, 2 children for other selected 453
nodes with total 32, 64, 128 expansions respectively. 454
D Supplementary Figures 455
In the main part of paper, there isn’t enough space for showing the scaling effects of Majority V oting, 456
we append the figures about Majority V oting and Majority V oting v.s. Weighted Majority V oting 457
142 4 8 16 32 64
Infer. FLOPs per question (×1012)1520253035T est error
Llemma-7B on GSM8K
Sampling M.V.
REBASE M.V.
16 32 64 128 256
Infer. FLOPs per question (×1012)12141618202224T est error
Llemma-34B on GSM8K
Sampling M.V.
REBASE M.V.
2 4 8 16 32 64
Infer. FLOPs per question (×1012)12141618202224T est error
Mistral-7B on GSM8K
Sampling M.V.
REBASE M.V.Figure 8: The inference computation scaling laws of different models for the problem-solving
error rate on GSM8K test set. The tested models are Llemma-7B (left), Llemma-34B (middle), &
Mistral-7B (right). In the legend, M.V . refer to Majority V oting.
4 16 64 256 1024
Infer. FLOPs per question (×1012)505560657075T est error
Llemma-7B on MATH
Sampling M.V.
Sampling W.M.
REBASE M.V.
REBASE W.M.
1632641282565121024
Infer. FLOPs per question (×1012)5055606570T est error
Llemma-34B on MATH
Sampling M.V.
Sampling W.M.
REBASE M.V.
REBASE W.M.
4 16 64 256 1024
Infer. FLOPs per question (×1012)5560657075T est error
Mistral-7B on MATH
Sampling M.V.
Sampling W.M.
REBASE M.V.
REBASE W.M.
Figure 9: The inference computation scaling laws of different models for the problem-solving
error rate on MATH test set. The tested models are Llemma-7B (left), Llemma-34B (middle), &
Mistral-7B (right). In the legend, M.V . and W.M. refer to Majority V oting and Weighted Majority,
respectively.
2 4 8 16 32 64
Infer. FLOPs per question (×1012)1520253035T est error
Llemma-7B on GSM8K
Sampling M.V.
Sampling W.M.
REBASE M.V.
REBASE W.M.
16 32 64 128 256
Infer. FLOPs per question (×1012)12141618202224T est error
Llemma-34B on GSM8K
Sampling M.V.
Sampling W.M.
REBASE M.V.
REBASE W.M.
2 4 8 16 32 64
Infer. FLOPs per question (×1012)1012141618202224T est error
Mistral-7B on GSM8K
Sampling M.V.
Sampling W.M.
REBASE M.V.
REBASE W.M.
Figure 10: The inference computation scaling laws of different models for the problem-solving
error rate on GSM8K test set. The tested models are Llemma-7B (left), Llemma-34B (middle), &
Mistral-7B (right). In the legend, M.V . and W.M. refer to Majority V oting and Weighted Majority,
respectively.
(Fig. 7, 8 ,9, 10) in this appendix. The experiments show that although the gap between Majority 458
V oting and Weighted Majority V oting on sampling is huge. This gap becomes much smaller if we 459
apply REBASE. This phenomenon can be caused by the selection ability of tree search like REBASE. 460
Once REBASE already samples solutions with high rewards, conducing weighted majority voting 461
gains less since the sampled solutions may all have relatively high and stable rewards compared with 462
those of sampling. 463
15NeurIPS Paper Checklist 464
1.Claims 465
Question: Do the main claims made in the abstract and introduction accurately reflect the 466
paper’s contributions and scope? 467
Answer: [Yes] 468
Justification: In Abstract and Introduction, we claim that we investigate the compute-optimal 469
inference: designing models and inference strategies that optimally trade off additional 470
inference-time compute for improved performance. 471
Guidelines: 472
•The answer NA means that the abstract and introduction do not include the claims 473
made in the paper. 474
•The abstract and/or introduction should clearly state the claims made, including the 475
contributions made in the paper and important assumptions and limitations. A No or 476
NA answer to this question will not be perceived well by the reviewers. 477
•The claims made should match theoretical and experimental results, and reflect how 478
much the results can be expected to generalize to other settings. 479
•It is fine to include aspirational goals as motivation as long as it is clear that these goals 480
are not attained by the paper. 481
2.Limitations 482
Question: Does the paper discuss the limitations of the work performed by the authors? 483
Answer: [Yes] 484
Justification: The discussion is in the last section of the main paper. 485
Guidelines: 486
•The answer NA means that the paper has no limitation while the answer No means that 487
the paper has limitations, but those are not discussed in the paper. 488
• The authors are encouraged to create a separate "Limitations" section in their paper. 489
•The paper should point out any strong assumptions and how robust the results are to 490
violations of these assumptions (e.g., independence assumptions, noiseless settings, 491
model well-specification, asymptotic approximations only holding locally). The authors 492
should reflect on how these assumptions might be violated in practice and what the 493
implications would be. 494
•The authors should reflect on the scope of the claims made, e.g., if the approach was 495
only tested on a few datasets or with a few runs. In general, empirical results often 496
depend on implicit assumptions, which should be articulated. 497
•The authors should reflect on the factors that influence the performance of the approach. 498
For example, a facial recognition algorithm may perform poorly when image resolution 499
is low or images are taken in low lighting. Or a speech-to-text system might not be 500
used reliably to provide closed captions for online lectures because it fails to handle 501
technical jargon. 502
•The authors should discuss the computational efficiency of the proposed algorithms 503
and how they scale with dataset size. 504
•If applicable, the authors should discuss possible limitations of their approach to 505
address problems of privacy and fairness. 506
•While the authors might fear that complete honesty about limitations might be used by 507
reviewers as grounds for rejection, a worse outcome might be that reviewers discover 508
limitations that aren’t acknowledged in the paper. The authors should use their best 509
judgment and recognize that individual actions in favor of transparency play an impor- 510
tant role in developing norms that preserve the integrity of the community. Reviewers 511
will be specifically instructed to not penalize honesty concerning limitations. 512
3.Theory Assumptions and Proofs 513
Question: For each theoretical result, does the paper provide the full set of assumptions and 514
a complete (and correct) proof? 515
16Answer: [Yes] 516
Justification: It’s in Appendix. 517
Guidelines: 518
• The answer NA means that the paper does not include theoretical results. 519
•All the theorems, formulas, and proofs in the paper should be numbered and cross- 520
referenced. 521
•All assumptions should be clearly stated or referenced in the statement of any theorems. 522
•The proofs can either appear in the main paper or the supplemental material, but if 523
they appear in the supplemental material, the authors are encouraged to provide a short 524
proof sketch to provide intuition. 525
•Inversely, any informal proof provided in the core of the paper should be complemented 526
by formal proofs provided in appendix or supplemental material. 527
• Theorems and Lemmas that the proof relies upon should be properly referenced. 528
4.Experimental Result Reproducibility 529
Question: Does the paper fully disclose all the information needed to reproduce the main ex- 530
perimental results of the paper to the extent that it affects the main claims and/or conclusions 531
of the paper (regardless of whether the code and data are provided or not)? 532
Answer: [Yes] 533
Justification: We introduce our method in Section 3 and the hyperparameters are introduced 534
in the Appendix. 535
Guidelines: 536
• The answer NA means that the paper does not include experiments. 537
•If the paper includes experiments, a No answer to this question will not be perceived 538
well by the reviewers: Making the paper reproducible is important, regardless of 539
whether the code and data are provided or not. 540
•If the contribution is a dataset and/or model, the authors should describe the steps taken 541
to make their results reproducible or verifiable. 542
•Depending on the contribution, reproducibility can be accomplished in various ways. 543
For example, if the contribution is a novel architecture, describing the architecture fully 544
might suffice, or if the contribution is a specific model and empirical evaluation, it may 545
be necessary to either make it possible for others to replicate the model with the same 546
dataset, or provide access to the model. In general. releasing code and data is often 547
one good way to accomplish this, but reproducibility can also be provided via detailed 548
instructions for how to replicate the results, access to a hosted model (e.g., in the case 549
of a large language model), releasing of a model checkpoint, or other means that are 550
appropriate to the research performed. 551
•While NeurIPS does not require releasing code, the conference does require all submis- 552
sions to provide some reasonable avenue for reproducibility, which may depend on the 553
nature of the contribution. For example 554
(a)If the contribution is primarily a new algorithm, the paper should make it clear how 555
to reproduce that algorithm. 556
(b)If the contribution is primarily a new model architecture, the paper should describe 557
the architecture clearly and fully. 558
(c)If the contribution is a new model (e.g., a large language model), then there should 559
either be a way to access this model for reproducing the results or a way to reproduce 560
the model (e.g., with an open-source dataset or instructions for how to construct 561
the dataset). 562
(d)We recognize that reproducibility may be tricky in some cases, in which case 563
authors are welcome to describe the particular way they provide for reproducibility. 564
In the case of closed-source models, it may be that access to the model is limited in 565
some way (e.g., to registered users), but it should be possible for other researchers 566
to have some path to reproducing or verifying the results. 567
5.Open access to data and code 568
17Question: Does the paper provide open access to the data and code, with sufficient instruc- 569
tions to faithfully reproduce the main experimental results, as described in supplemental 570
material? 571
Answer: [Yes] 572
Justification: We only used open-source models in this work. The code will be released. 573
Guidelines: 574
• The answer NA means that paper does not include experiments requiring code. 575
•Please see the NeurIPS code and data submission guidelines ( https://nips.cc/ 576
public/guides/CodeSubmissionPolicy ) for more details. 577
•While we encourage the release of code and data, we understand that this might not be 578
possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not 579
including code, unless this is central to the contribution (e.g., for a new open-source 580
benchmark). 581
•The instructions should contain the exact command and environment needed to run to 582
reproduce the results. See the NeurIPS code and data submission guidelines ( https: 583
//nips.cc/public/guides/CodeSubmissionPolicy ) for more details. 584
•The authors should provide instructions on data access and preparation, including how 585
to access the raw data, preprocessed data, intermediate data, and generated data, etc. 586
•The authors should provide scripts to reproduce all experimental results for the new 587
proposed method and baselines. If only a subset of experiments are reproducible, they 588
should state which ones are omitted from the script and why. 589
•At submission time, to preserve anonymity, the authors should release anonymized 590
versions (if applicable). 591
•Providing as much information as possible in supplemental material (appended to the 592
paper) is recommended, but including URLs to data and code is permitted. 593
6.Experimental Setting/Details 594
Question: Does the paper specify all the training and test details (e.g., data splits, hyper- 595
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the 596
results? 597
Answer: [Yes] 598
Justification: We used the standard training and test splits or MATH and GSM8K and report 599
the hyperparameters in the appendix. 600
Guidelines: 601
• The answer NA means that the paper does not include experiments. 602
•The experimental setting should be presented in the core of the paper to a level of detail 603
that is necessary to appreciate the results and make sense of them. 604
•The full details can be provided either with the code, in appendix, or as supplemental 605
material. 606
7.Experiment Statistical Significance 607
Question: Does the paper report error bars suitably and correctly defined or other appropriate 608
information about the statistical significance of the experiments? 609
Answer: [Yes] 610
Justification: The error bars are included in our figures. 611
Guidelines: 612
• The answer NA means that the paper does not include experiments. 613
•The authors should answer "Yes" if the results are accompanied by error bars, confi- 614
dence intervals, or statistical significance tests, at least for the experiments that support 615
the main claims of the paper. 616
•The factors of variability that the error bars are capturing should be clearly stated (for 617
example, train/test split, initialization, random drawing of some parameter, or overall 618
run with given experimental conditions). 619
18•The method for calculating the error bars should be explained (closed form formula, 620
call to a library function, bootstrap, etc.) 621
• The assumptions made should be given (e.g., Normally distributed errors). 622
•It should be clear whether the error bar is the standard deviation or the standard error 623
of the mean. 624
•It is OK to report 1-sigma error bars, but one should state it. The authors should 625
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis 626
of Normality of errors is not verified. 627
•For asymmetric distributions, the authors should be careful not to show in tables or 628
figures symmetric error bars that would yield results that are out of range (e.g. negative 629
error rates). 630
•If error bars are reported in tables or plots, The authors should explain in the text how 631
they were calculated and reference the corresponding figures or tables in the text. 632
8.Experiments Compute Resources 633
Question: For each experiment, does the paper provide sufficient information on the com- 634
puter resources (type of compute workers, memory, time of execution) needed to reproduce 635
the experiments? 636
Answer: [Yes] 637
Justification: All the experiments are conducted on 8×H100 GPUs. 638
Guidelines: 639
• The answer NA means that the paper does not include experiments. 640
•The paper should indicate the type of compute workers CPU or GPU, internal cluster, 641
or cloud provider, including relevant memory and storage. 642
•The paper should provide the amount of compute required for each of the individual 643
experimental runs as well as estimate the total compute. 644
•The paper should disclose whether the full research project required more compute 645
than the experiments reported in the paper (e.g., preliminary or failed experiments that 646
didn’t make it into the paper). 647
9.Code Of Ethics 648
Question: Does the research conducted in the paper conform, in every respect, with the 649
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines ? 650
Answer: [Yes] 651
Justification: Yes, we conform with the NeurIPS Code of Ethics. 652
Guidelines: 653
•The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. 654
•If the authors answer No, they should explain the special circumstances that require a 655
deviation from the Code of Ethics. 656
•The authors should make sure to preserve anonymity (e.g., if there is a special consid- 657
eration due to laws or regulations in their jurisdiction). 658
10.Broader Impacts 659
Question: Does the paper discuss both potential positive societal impacts and negative 660
societal impacts of the work performed? 661
Answer: [NA] 662
Justification: We do not find significant positive societal impacts and negative societal 663
impacts of our work. 664
Guidelines: 665
• The answer NA means that there is no societal impact of the work performed. 666
•If the authors answer NA or No, they should explain why their work has no societal 667
impact or why the paper does not address societal impact. 668
19•Examples of negative societal impacts include potential malicious or unintended uses 669
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations 670
(e.g., deployment of technologies that could make decisions that unfairly impact specific 671
groups), privacy considerations, and security considerations. 672
•The conference expects that many papers will be foundational research and not tied 673
to particular applications, let alone deployments. However, if there is a direct path to 674
any negative applications, the authors should point it out. For example, it is legitimate 675
to point out that an improvement in the quality of generative models could be used to 676
generate deepfakes for disinformation. On the other hand, it is not needed to point out 677
that a generic algorithm for optimizing neural networks could enable people to train 678
models that generate Deepfakes faster. 679
•The authors should consider possible harms that could arise when the technology is 680
being used as intended and functioning correctly, harms that could arise when the 681
technology is being used as intended but gives incorrect results, and harms following 682
from (intentional or unintentional) misuse of the technology. 683
•If there are negative societal impacts, the authors could also discuss possible mitigation 684
strategies (e.g., gated release of models, providing defenses in addition to attacks, 685
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from 686
feedback over time, improving the efficiency and accessibility of ML). 687
11.Safeguards 688
Question: Does the paper describe safeguards that have been put in place for responsible 689
release of data or models that have a high risk for misuse (e.g., pretrained language models, 690
image generators, or scraped datasets)? 691
Answer: [NA] 692
Justification: 693
Guidelines: 694
• The answer NA means that the paper poses no such risks. 695
•Released models that have a high risk for misuse or dual-use should be released with 696
necessary safeguards to allow for controlled use of the model, for example by requiring 697
that users adhere to usage guidelines or restrictions to access the model or implementing 698
safety filters. 699
•Datasets that have been scraped from the Internet could pose safety risks. The authors 700
should describe how they avoided releasing unsafe images. 701
•We recognize that providing effective safeguards is challenging, and many papers do 702
not require this, but we encourage authors to take this into account and make a best 703
faith effort. 704
12.Licenses for existing assets 705
Question: Are the creators or original owners of assets (e.g., code, data, models), used in 706
the paper, properly credited and are the license and terms of use explicitly mentioned and 707
properly respected? 708
Answer: [Yes] 709
Justification: We use the proper citations. 710
Guidelines: 711
• The answer NA means that the paper does not use existing assets. 712
• The authors should cite the original paper that produced the code package or dataset. 713
•The authors should state which version of the asset is used and, if possible, include a 714
URL. 715
• The name of the license (e.g., CC-BY 4.0) should be included for each asset. 716
•For scraped data from a particular source (e.g., website), the copyright and terms of 717
service of that source should be provided. 718
•If assets are released, the license, copyright information, and terms of use in the 719
package should be provided. For popular datasets, paperswithcode.com/datasets 720
has curated licenses for some datasets. Their licensing guide can help determine the 721
license of a dataset. 722
20•For existing datasets that are re-packaged, both the original license and the license of 723
the derived asset (if it has changed) should be provided. 724
•If this information is not available online, the authors are encouraged to reach out to 725
the asset’s creators. 726
13.New Assets 727
Question: Are new assets introduced in the paper well documented and is the documentation 728
provided alongside the assets? 729
Answer: [Yes] 730
Justification: We use the proper citations. 731
Guidelines: 732
• The answer NA means that the paper does not release new assets. 733
•Researchers should communicate the details of the dataset/code/model as part of their 734
submissions via structured templates. This includes details about training, license, 735
limitations, etc. 736
•The paper should discuss whether and how consent was obtained from people whose 737
asset is used. 738
•At submission time, remember to anonymize your assets (if applicable). You can either 739
create an anonymized URL or include an anonymized zip file. 740
14.Crowdsourcing and Research with Human Subjects 741
Question: For crowdsourcing experiments and research with human subjects, does the paper 742
include the full text of instructions given to participants and screenshots, if applicable, as 743
well as details about compensation (if any)? 744
Answer: [NA] 745
Justification: No crowdsourcing experiments are used. 746
Guidelines: 747
•The answer NA means that the paper does not involve crowdsourcing nor research with 748
human subjects. 749
•Including this information in the supplemental material is fine, but if the main contribu- 750
tion of the paper involves human subjects, then as much detail as possible should be 751
included in the main paper. 752
•According to the NeurIPS Code of Ethics, workers involved in data collection, curation, 753
or other labor should be paid at least the minimum wage in the country of the data 754
collector. 755
15.Institutional Review Board (IRB) Approvals or Equivalent for Research with Human 756
Subjects 757
Question: Does the paper describe potential risks incurred by study participants, whether 758
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) 759
approvals (or an equivalent approval/review based on the requirements of your country or 760
institution) were obtained? 761
Answer: [NA] 762
Justification: No human subjects are involved. 763
Guidelines: 764
•The answer NA means that the paper does not involve crowdsourcing nor research with 765
human subjects. 766
•Depending on the country in which research is conducted, IRB approval (or equivalent) 767
may be required for any human subjects research. If you obtained IRB approval, you 768
should clearly state this in the paper. 769
•We recognize that the procedures for this may vary significantly between institutions 770
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the 771
guidelines for their institution. 772
•For initial submissions, do not include any information that would break anonymity (if 773
applicable), such as the institution conducting the review. 774
21