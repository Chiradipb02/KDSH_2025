Published in Transactions on Machine Learning Research (10/2023)
Deep Operator Learning Lessens the Curse of Dimensionality
for PDEs
Ke Chen kechen@umd.edu
Department of Mathematics
University of Maryland, College Park
Chunmei Wang chunmei.wang@ufl.edu
Department of Mathematics
University of Florida
Haizhao Yang hzyang@umd.edu
Department of Mathematics
University of Maryland, College Park
Reviewed on OpenReview: https: // openreview. net/ forum? id= zmBFzuT2DN
Abstract
Deep neural networks (DNNs) have achieved remarkable success in numerous domains, and
their application to PDE-related problems has been rapidly advancing. This paper provides
an estimate for the generalization error of learning Lipschitz operators over Banach spaces
using DNNs with applications to various PDE solution operators. The goal is to specify
DNNwidth, depth, andthenumberoftrainingsamplesneededtoguaranteeacertaintesting
error. Under mild assumptions on data distributions or operator structures, our analysis
shows that deep operator learning can have a relaxed dependence on the discretization
resolution of PDEs and, hence, lessen the curse of dimensionality in many PDE-related
problems including elliptic equations, parabolic equations, and Burgers equations. Our
results are also applied to give insights about discretization-invariance in operator learning.
1 Introduction
Nonlinear operator learning aims to learn a mapping from a parametric function space to the solution space
of specific partial differential equation (PDE) problems. It has gained significant importance in various fields,
including order reduction Peherstorfer & Willcox (2016), parametric PDEs Lu et al. (2021b); Li et al. (2021),
inverse problems Khoo & Ying (2019), and imaging problems Deng et al. (2020); Qiao et al. (2021); Tian
et al. (2020). Deep neural networks (DNNs) have emerged as state-of-the-art models in numerous machine
learning tasks Graves et al. (2013); Miotto et al. (2018); Krizhevsky et al. (2017), attracting attention for
their applications to engineering problems where PDEs have long been the dominant model. Consequently,
deep operator learning has emerged as a powerful tool for nonlinear PDE operator learning Lanthaler et al.
(2022); Li et al. (2021); Nelsen & Stuart (2021); Khoo & Ying (2019). The typical approach involves
discretizing the computational domain and representing functions as vectors that tabulate function values on
the discretization mesh. A DNN is then employed to learn the map between finite-dimensional spaces. While
this method has been successful in various applications Lin et al. (2021); Cai et al. (2021), its computational
cost is high due to its dependence on the mesh. This implies that retraining of the DNN is necessary when
using a different domain discretization. To address this issue, Li et al. (2021); Lu et al. (2022); Ong et al.
(2022) have been proposed for problems with sparsity structures and discretization-invariance properties.
Another line of works for learning PDE operators are generative models, including Generative adversarial
models (GANs) and its variants Rahman et al. (2022); Botelho et al. (2020); Kadeethum et al. (2021) and
diffusion models Wang et al. (2023). These methods can deal with discontinuous features, whereas neural
1Published in Transactions on Machine Learning Research (10/2023)
network based methods are mainly applied to operators with continuous input and output. However, most
of generative models for PDE operator learning are limited to empirical study and theoretical foundations
are in lack.
Despite the empirical success of deep operator learning in numerous applications, its statistical learning
theory is still limited, particularly when dealing with infinite-dimensional ambient spaces. The learning
theory generally comprises three components: approximation theory, optimization theory, and generalization
theory. Approximation theory quantifies the expressibility of various DNNs as surrogates for a class of
operators. The universal approximation theory for certain classes of functions Cybenko (1989); Hornik
(1991) forms the basis of the approximation theory for DNNs. It has been extended to other function
classes, such as continuous functions Shen et al. (2019); Yarotsky (2021); Shen et al. (2021), certain smooth
functions Yarotsky (2018); Lu et al. (2021a); Suzuki (2018); Adcock et al. (2022), and functions with integral
representations Barron (1993); Ma et al. (2022). However, compared to the abundance of theoretical works
on approximation theory for high-dimensional functions, the approximation theory for operators, especially
between infinite-dimensional spaces, is quite limited. Seminal quantitative results have been presented
in Kovachki et al. (2021); Lanthaler et al. (2022).
In contrast to approximation theory, generalization theory aims to address the following question:
How many training samples are required to achieve a certain testing error?
This question has been addressed by numerous statistical learning theory works for function regression using
neural network structures Bauer & Kohler (2019); Chen et al. (2022); Farrell et al. (2021); Kohler & Krzyżak
(2005); Liu et al. (2021); Nakada & Imaizumi (2020); Schmidt-Hieber (2020). In a d-dimensional learning
problem, the typical error decay rate is on the order of n−O(1/d)as the number of samples nincreases.
The fact that the exponent is very small for large dimensionality dis known as the curse of dimensionality
(CoD) Stone (1982). Recent studies have demonstrated that DNNs can achieve faster decay rates when
dealing with target functions or function domains that possess low-dimensional structures Chen et al. (2019;
2022); Cloninger & Klock (2020); Nakada & Imaizumi (2020); Schmidt-Hieber (2019); Shen et al. (2019).
In such cases, the decay rate becomes independent of the domain discretization, thereby lessening the CoD
Bauer & Kohler (2019); Chkifa et al. (2015); Suzuki (2018). However, it is worth noting that most existing
works primarily focus on functions between finite-dimensional spaces. To the best of our knowledge, previous
results de Hoop et al. (2021); Lanthaler et al. (2022); Lu et al. (2021b); Liu et al. (2022) provide the only
generalization analysis for infinite-dimensional functions. Our work extends the findings of Liu et al. (2022)
by generalizing them to Banach spaces and conducting new analyses within the context of PDE problems.
The removal of the inner-product assumption is crucial in our research, enabling us to apply the estimates
to various PDE problems where previous results do not apply. This is mainly because the suitable space
for functions involved in most practical PDE examples are Banach spaces where the inner-product is not
well-defined. Examples include the conductivity media function in the parametric elliptic equation, the drift
force field in the transport equation, and the solution to the viscous Burgers equation that models continuum
fluid. See more details in Section 3.
1.1 Our contributions
The main objective of this study is to investigate the reasons behind the reduction of the CoD in PDE-
related problems achieved by deep operator learning. We observe that many PDE operators exhibit a
composition structure consisting of linear transformations and element-wise nonlinear transformations with
a small number of inputs. DNNs are particularly effective in learning such structures due to their ability
to evaluate networks point-wise. We provide an analysis of the approximation and generalization errors
and apply it to various PDE problems to determine the extent to which the CoD can be mitigated. Our
contributions can be summarized as follows:
◦Our work provides a theoretical explanation to why CoD is lessened in PDE operator learning. We
extend the generalization theory in Liu et al. (2022) from Hilbert spaces to Banach spaces, and apply
it to several PDE examples. Such extension holds great significance as it overcomes a limitation
in previous works, which primarily focused on Hilbert spaces and therefore lacked applicability
2Published in Transactions on Machine Learning Research (10/2023)
in machine learning for practical PDEs problems. Comparing to Liu et al. (2022), our estimate
circumvented the inner-product structure at the price of a non-decaying noise estimate. This is a
tradeoff of accuracy for generalization to Banach space. Our work tackles a broader range of PDE
operators that are defined on Banach spaces. In particular, five PDE examples are given in Section
3 whose solution spaces are not Hilbert spaces.
◦Unlike existing works such as Lanthaler et al. (2022), which only offer posterior analysis, we provide
an a priori estimate for PDE operator learning. Our estimate does not make any assumptions about
thetrainedneuralnetworkandexplicitlyquantifiestherequirednumberofdatasamplesandnetwork
sizes based on a given testing error criterion. Furthermore, we identify two key structures—low-
dimensional and low-complexity structures (described in assumptions 5 and 6, respectively)—that
are commonly present in PDE operators. We demonstrate that both structures exhibit a sample
complexity that depends on the essential dimension of the PDE itself, weakly depending on the
PDE discretization size. This finding provides insights into why deep operator learning effectively
mitigates the CoD.
◦Most operator learning theories consider fixed-size neural networks. However, it is important to
account for neural networks with discretization invariance properties, allowing training and evalua-
tion on PDE data of various resolutions. Our theory is flexible and can be applied to derive error
estimates for discretization invariant neural networks.
1.2 Organization
In Section 2, we introduce the neural network structures and outline the assumptions made on the PDE
operator. Furthermore, we present the main results for generic PDE operators, and PDE operators that
have low-dimensional structure or low-complexity structure. At the end of the section, we show that the
main results are also valid for discretization invariant neural networks. In Section 3, we show that the
assumptions are satisfied and provide explicit estimates for five different PDEs. Finally, in Section 4, we
discuss the limitations of our current work.
2 Problem setup and main results
Notations. In a general Banach space X, we represent its associated norm as ∥·∥X. Additionally, we
denoteEn
Xas the encoder mapping from the Banach space Xto a Euclidean space RdX, wheredXdenotes
the encoding dimension. Similarly, we denote the decoder for XasDn
X:RdX→X. The Ωnotation for
neural network parameters in the main results section 2.2 denotes a lower bound estimate, that is, x= Ω(y)
means there exists a constant C > 0such thatx≥Cy. TheOnotation denotes an upper bound estimate,
that is,x=O(y)means there exists a constant C > 0such thatx≤Cy.
2.1 Operator learning and loss functions
We consider a general nonlinear PDE operator Φ :X∋u∝⇕⊣√∫⊔≀→v∈Yover Banach spaces XandY. In this
context, the input variable utypically represents the initial condition, the boundary condition, or a source
of a specific PDE, while the output variable vcorresponds to the PDE solution or partial measurements
of the solution. Our objective is to train a DNN denoted as ϕ(u;θ)to approximate the target nonlinear
operator Φusing a given data set S={(ui,vi),vi= Φ(ui)+εi,i= 1,..., 2n}. The data setSis divided into
Sn
1={(ui,vi),vi= Φ(ui) +εi,i= 1,...,n}that is used to train the encoder and decoders, and a training
data setSn
2={(ui,vi),vi= Φ(ui) +εi,i=n+ 1,..., 2n}. BothSn
1andSn
2are generated independently
and identically distributed (i.i.d.) from a random measure γoverX, withεirepresenting random noise.
In practical implementations, DNNs operate on finite-dimensional spaces. Therefore, we utilize empirical
encoder-decoder pairs, namely En
X:X → RdXandDn
X:RdX→X, to discretize u∈X. Similarly, we
employ empirical encoder-decoder pairs, En
Y:Y→RdYandDn
Y:RdY→Y, forv∈Y. These encoder-
decoder pairs are trained using the available data set Sn
1or manually designed such that Dn
X◦En
X≈IdXand
Dn
Y◦En
Y≈IdY. A common example of empirical encoders and decoders is the discretization operator, which
3Published in Transactions on Machine Learning Research (10/2023)
Figure 1: The target nonlinear operator Φ :u∝⇕⊣√∫⊔≀→vis approximated by compositions of an encoder En
X,
a DNN function Γ, and a decoder Dn
Y. The finite dimensional operator Γis learned via the optimization
problem equation 1.
maps a function to a vector representing function values at discrete mesh points. Other examples include
finite element projections and spectral methods, which map functions to coefficients of corresponding basis
functions. Our goal is to approximate the encoded PDE operator using a finite-dimensional operator Γso
that Φ≈Dn
Y◦Γ◦En
X. Refer to Figure 1 for an illustration. This approximation is achieved by solving the
following optimization problem:
ΓNN∈argmin
Γ∈FNN1
nn/summationdisplay
i=1∥Γ◦En
X(ui)−En
Y(vi)∥2
2. (1)
Herethefunctionspace FNNrepresentsacollectionofrectifiedlinearunit(ReLU)feedforwardDNNsdenoted
asf(x), which are defined as follows:
f(x) =WLϕL−1◦ϕL−2◦···◦ϕ1(x) +bL, ϕi(x):=σ(Wix+bi),i= 1,...,L−1, (2)
whereσis the ReLU activation function σ(x) = max{x,0}, andWiandbirepresent weight matrices and
bias vectors, respectively. The ReLU function is evaluated pointwise on all entries of the input vector. In
practice, the functional space FNNis selected as a compact set comprising all ReLU feedforward DNNs.
This work investigates two distinct architectures within FNN. The first architecture within FNNis defined
as follows:
FNN(d,L,p,K,κ,M ) ={Γ = [f1,f2,...,fd]⊤:for eachk= 1,...,d,fk(x)is in the form of (2)
withLlayers, width bounded by p,∥fk∥∞≤M,∥Wl∥∞,∞≤κ,∥bl∥∞≤κ,L/summationdisplay
l=1∥Wl∥0+∥bl∥0≤K},(3)
where∥f∥∞= maxx|f(x)|,∥W∥∞,∞= maxi,j|Wi,j|,∥b∥∞= maxi|bi|for any function f, matrixW,
and vector bwith∥·∥ 0denoting the number of nonzero elements of its argument. The functions in this
architecture satisfy parameter bounds with limited cardinalities. The second architecture relaxes some of
the constraints compared to the first architecture; i.e.,
FNN(d,L,p,M ) ={Γ = [f1,f2,...,fd]⊤:for eachk= 1,...,d,fk(x)is in the form of (2)
withLlayers, width bounded by p,∥fk∥∞≤M}.(4)
When there is no ambiguity, we use the notation FNNand omit its associated parameters.
WeconsiderthefollowingassumptionsonthetargetPDEmap Φ, theencoders En
X,En
Y, thedecoders Dn
X,Dn
Y,
and the data set Sin our theoretical framework.
Assumption 1 (Compactly supported measure) .The probability measure γis supported on a compact set
ΩX⊂X. For anyu∈ΩX, there exists RX>0such that∥u∥X≤RX.Here,∥·∥Xdenotes the associated
norm of the space X.
Assumption 2 (Lipschitz operator) .There exists LΦ>0such that for any u1,u2∈ΩX,
∥Φ(u1)−Φ(u2)∥Y≤LΦ∥u1−u2∥X.
Here,∥·∥Ydenotes the associated norm of the space Y.
4Published in Transactions on Machine Learning Research (10/2023)
Remark 1.Assumption 1 and Assumption 2 imply that the images v= Φ(u)are bounded by RY:=LΦRX
for allu∈ΩX. The Lipschitz constant LΦwill be explicitly computed in Section 3 for different PDE
operators.
Assumption 3 (Lipschitz encoders and decoders) .The empirical encoders and decoders En
X,Dn
X,En
Y,Dn
Y
satisfy the following properties:
En
X(0X) =0,Dn
X(0) = 0X,En
Y(0Y) =0,Dn
Y(0) = 0Y,
where 0denotes the zero vector and 0X,0Ydenote the zero function in XandY, respectively. Moreover,
we assume all empirical encoders are Lipschitz operators such that
∥En
Pu1−En
Pu2∥2≤LEn
P∥u1−u2∥P,P=X,Y,
where∥·∥2denotes the Euclidean L2norm,∥·∥Pdenotes the associated norm of the Banach space P, and
LEn
Pis the Lipschitz constant of the encoder En
P. Similarly, we also assume that the decoders Dn
P,P=X,Y
are also Lipschitz with constants LDn
P.
Assumption 4 (Noise) .Fori= 1,..., 2n, the noiseεisatisfies
1.εiis independent of ui;
2.E[εi] = 0;
3. There exists σ>0such that∥εi∥Y≤σ.
Remark 2.The above assumptions on the noise and Lipschitz encoders imply that ∥En
Y(Φ(ui) +εi)−
En
Y(Φ(u))∥2≤LEn
Yσ.
2.2 Main Results
For a trained neural network ΓNNover the data set S, we denote its generalization error as
Egen(ΓNN):=ESEu∼γ/bracketleftbig
∥Dn
Y◦ΓNN◦En
X(u)−Φ(u)∥2
Y/bracketrightbig
.
Note that we omit its dependence on Sin the notation. We also define the following quantity,
Enoise,proj :=L2
ΦESEu/bracketleftbig
∥Πn
X,dX(u)−u∥2
X/bracketrightbig
+ESEw∼Φ#γ/bracketleftbig
∥Πn
Y,dY(w)−w∥2
Y/bracketrightbig
+σ2+n−1,
where Πn
X,dX:=Dn
X◦En
XandΠn
Y,dY:=Dn
Y◦En
Ydenote the encoder-decoder projections on XandY
respectively. Here the first term shows that the encoder/decoder projection error ESEu/bracketleftig
∥Πn
X,dX(u)−u∥2
X/bracketrightig
forXis amplified by the Lipschitz constant; the second term is the encoder/decoder projection error for Y;
the third term stands for the noise; and the last term is a small quantity n−1. It will be shown later that
this quantity appears frequently in our main results.
Theorem 1. Suppose Assumptions 1-4 hold. Let ΓNNbe the minimizer of the optimization problem equa-
tion 1, with the network architecture FNN(dY,L,p,K,κ,M )defined in equation 3 with parameters
L= Ω/parenleftbigg
ln(n
dY)/parenrightbigg
, p = Ω/parenleftbigg
d2−dX
2+dX
YndX
2+dX/parenrightbigg
,
K= Ω(pL), κ = Ω(M2), M≥/radicalbig
dYLEn
YRY,
where the notation Ωcontains constants that solely depend on LEn
Y,LDn
Y,LEn
X,LDn
X,RXanddX. Then there
holds
Egen(ΓNN)≲d6+dX
2+dX
Yn−2
2+dX(1 +L2−dX
Φ )/parenleftbigg
ln3n
dY+ ln2n/parenrightbigg
+Enoise,proj.
Here≲contains constants that solely depend on LEn
Y,LDn
Y,LEn
X,LDn
X,RXanddX.
5Published in Transactions on Machine Learning Research (10/2023)
Theorem 2. Suppose Assumptions 1-4 hold ture. Let ΓNNbe the minimizer of the optimization problem
equation 1 with the network architecture FNN(dY,L,p,M )defined in equation 4 with parameters,
M≥/radicalbig
dYLEn
YRY,andLp≥/ceilingleftbigg
d4−dX
4+2dX
YndX
4+2dX/ceilingrightbigg
. (5)
Then we have
Egen(ΓNN)≲L2
Φlog(LΦ)d8+dX
2+dX
Yn−2
2+dXlogn+Enoise,proj,(6)
where≲contains constants that depend on dX,LEn
Y,LEn
X,LDn
Y,LDn
XandRX.
Remark3.The aforementioned results demonstrate that by selecting an appropriate width and depth for the
DNN,thegeneralizationerrorcanbebrokendownintothreecomponents: thegeneralizationerroroflearning
the finite-dimensional operator Γ, the projection error of the encoders/decoders, and the noise. Comparing
to previous results Liu et al. (2022) under the Hilbert space setting, our estimates show that the noise term
in the generalization bound is non-decaying without the inner-product structure in the Banach space setting.
This is mainly caused by circumventing the inner-product structure via triangle inequalities in the proof. As
the number of samples nincreases, the generalization error decreases exponentially. Although the presence
ofdXin the exponent of the sample complexity ninitially appears pessimistic, we will demonstrate that it
can be eliminated when the input data ΩXof the target operator exhibits a low-dimensional data structure
or when the target operator itself has a low-complexity structure. These assumptions are often satisfied
for specific PDE operators with appropriate encoders. These results also imply that when dXis large, the
neural network width pdoes not need to increase as the output dimension dYincreases. The main difference
between Theorem 1 and Theorem 2 lies in the different neural network architectures FNN(dY,L,p,K,κ,M )
andFNN(dY,L,p,M ). As a consequence, Theorem 2 has a smaller asymptotic lower bound Ω(n1/2)of the
neural network width pin the large dXregime, whereas the asymptotic lower bound is Ω(n)in Theorem 1.
Estimates with special data and operator structures
ThegeneralizationerrorestimatespresentedinTheorems1-2areeffectivewhentheinputdimension dXisrel-
ativelysmall. However, inpracticalscenarios, itoftenrequiresnumerousbasestoreducetheencoder/decoder
projection error, resulting in a large value for dX. Consequently, the decay rate of the generalization error
as indicated in Theorems 1-2 becomes stagnant due to its exponential dependence on dX.
Nevertheless, it is often assumed that the high-dimensional data lie within the vicinity of a low-dimensional
manifold by the famous “manifold hypothesis”. Specifically, we assume that the encoded vectors ulie on a
d0-dimensional manifold with d0≪dX. Such a data distribution has been observed in many applications,
including PDE solution set, manifold learning, and image recognition. This assumption is formulated as
follows.
Assumption 5. Letd0<dX∈N. Suppose there exists an encoder EX:X→RdXsuch that{EX(u)|u∈
ΩX}lies in a smooth d0-dimensional Riemannian manifold Mthat is isometrically embedded in RdX. The
reachNiyogi et al. (2008) of Misτ >0.
Under Assumption 5, the input data set exhibits a low intrinsic dimensionality. However, this may not hold
for the output data set that is perturbed by noise. The reach of a manifold is the smallest osculating circle
radius on the manifold. A manifold with large reach avoids rapid change and may be easier to learn by neural
networks. In the following, we aim to demonstrate that the DNN naturally adjusts to the low-dimensional
characteristics of the data set. As a result, the estimation error of the network depends solely on the intrinsic
dimensiond0, rather than the larger ambient dimension dX. We present the following result to support this
claim.
Theorem 3. Suppose Assumptions 1-4, and Assumption 5 hold. Let ΓNNbe the minimizer of the optimiza-
tion problem equation 1 with the network architecture FNN(dY,L,p,M )defined in equation 4 with parameters
L= Ω( ˜Llog˜L),p= Ω(dXdY˜plog ˜p), M≥/radicalbig
dYLEn
YRY, (7)
6Published in Transactions on Machine Learning Research (10/2023)
where ˜L,˜p>0are integers such that ˜L˜p≥/ceilingleftbigg
d−3d0
4+2d0
Ynd0
4+2d0/ceilingrightbigg
. Then we have
Egen(ΓNN)≲L2
Φlog(LΦ)d8+d0
2+d0
Yn−2
2+d0log6n+Enoise,proj, (8)
where the constants in ≲andΩ(·)solely depend on d0,logdX,RX,LEn
X,LEn
Y,LDn
X,LDn
Y,τ, the surface area
ofM.
It is important to note that the estimate equation 8 depends at most polynomially on dXanddY. The rate
of decay with respect to the sample size is no longer influenced by the ambient input dimension dX. Thus,
our findings indicate that the CoD can be mitigated through the utilization of the "manifold hypothesis." To
effectivelycapturethelow-dimensionalmanifoldstructureofthedata, thewidthoftheDNNshouldbeonthe
order ofO(dX). Additionally, another characteristic often observed in PDE problems is the low complexity
of the target operator. This holds true when the target operator is composed of several alternating sequences
of a few linear and nonlinear transformations with only a small number of inputs. We quantify the notion
of low-complexity operators in the following context.
Assumption 6. Let0<d0≤dX. Assume there exists EX,DX,EY,DYsuch that for any u∈ΩX, we have
ΠY,dY◦Φ(u) =DY◦g◦EX(u),
whereg:RdX→RdYis defined as
g(a) =/bracketleftbigg1(V⊤
1a),···,gdY(V⊤
dYa)/bracketrightbig
,
where the matrix is Vk∈RdX×d0and the real valued function is gk:Rd0→Rfork= 1,...,dY. See an
illustration in (44).
In Assumption 6, when d0= 1andg1=···=gdY,g(a)is the composition of a pointwise nonlinear transform
and a linear transform on a. In particular, Assumption 6 holds for any linear maps.
Theorem 4. Suppose Assumptions 1-4, and Assumption 6 hold. Let ΓNNbe the minimizer of the optimiza-
tion problem (1) with the network architecture FNN(dY,L,p,M )defined in (4) with parameters
Lp= Ω/parenleftbigg
d4−d0
4+2d0
Ynd0
4+2d0/parenrightbigg
,M≥/radicalbig
dYLEn
YRY.
Then we have
Egen(ΓNN)≲L2
Φlog(LΦ)d8+d0
2+d0
Yn−2
2+d0logn+Enoise,proj, (9)
where the constants in ≲andΩ(·)solely depend on d0,RX,RY,LEn
X,LEn
Y,LDn
X,LDn
Y.
Remark 4.Under Assumption 6, our result indicates that the CoD can be mitigated to a cost O(n−2
2+d0)
because the main task of DNNs is to learn the nonlinear transforms g1,···,gdYthat are functions over Rd0.
In practice, a PDE operator might be the repeated composition of operators in Assumption 6. This motivates
a more general low-complexity assumption below.
Assumption 7. Let0< d1,...,dk≤dXand0< ℓ0,...,ℓk≤min{dX,dY}withℓ0=dXandℓk=dY.
Assume there exists EX,DX,EY,DYsuch that for any u∈ΩX, we have
ΠY,dY◦Φ(u) =DY◦Gk◦···◦G1◦EX(u),
whereGi:Rℓi−1→Rℓiis defined as
Gi(a) =/bracketleftbiggi
1((Vi
1)⊤a),···,gi
ℓi((Vi
ℓi)⊤a)/bracketrightbig
,
where the matrix is Vi
j∈Rdi×ℓi−1and the real valued function is gi
j:Rdi→Rforj= 1,...,ℓi,i= 1,...,k.
See an illustration in (45).
7Published in Transactions on Machine Learning Research (10/2023)
Theorem 5. Suppose Assumptions 1-4, and Assumption 7 hold. Let ΓNNbe the minimizer of the optimiza-
tion (1) with the network architecture FNN(dY,kL,p,M )defined in equation 4 with parameters
Lp= Ω/parenleftbigg
d4−dmax
4+2dmax
Yndmax
4+2dmax/parenrightbigg
,M≥/radicalbig
ℓmaxLEn
YRY,
wheredmax= max{di}k
i=1andℓmax= max{ℓi}k
i=1. Then we have
Egen(ΓNN)≲L2
Φlog(LΦ)ℓ8+dmax
2+dmaxmaxn−2
2+dmaxlogn+Enoise,proj,
where the constants in ≲andΩ(·)solely depend on k,dmax,ℓmax,RX,LEn
X,LEn
Y,LDn
X,LDn
Y.
Discretization invariant neural networks
In this subsection, we demonstrate that our main results also apply to neural networks with the discretization
invariant property. A neural network is considered discretization invariant if it can be trained and evaluated
on data that are discretized in various formats. For example, the input data ui,i= 1,...,nmay consist
of images with different resolutions, or ui= [ui(x1),...,ui(xsi)]representing the values of uisampled at
different locations. Neural networks inherently have fixed input and output sizes, making them incompatible
for direct training on a data set {(ui,vi),i= 1,...,n}where the data pairs (ui∈Rdi,vi∈Rdi)have
different resolutions di,i= 1,...,n. Modifications of the encoders are required to map inputs of varying
resolutions to a uniform Euclidean space. This can be achieved through linear interpolation or data-driven
methods such as nonlinear integral transforms Ong et al. (2022).
Our previous analysis assumes that the data (ui,vi)∈X×Y is mapped to discretized data (ui,vi)∈
RdX×RdYusing the encoders En
XandEn
Y. Now, let us consider the case where the new discretized data
(ui,vi)∈Rsi×Rsiare vectors tabulating function values as follows:
ui=/bracketleftbig
ui(xi
1)ui(xi
2)... ui(xi
si)/bracketrightbig
,vi=/bracketleftbig
vi(xi
1)vi(xi
2)... vi(xi
si)/bracketrightbig
. (10)
The sampling locations xi:= [xi
1,...,xi
si]are allowed to be different for each data pair (ui,vi). We
can now define the sampling operator on the location xiasPxi:u∝⇕⊣√∫⊔≀→u(xi), where u(xi):=/bracketleftbigu(xi
1)u(xi
2)... u (xi
si)/bracketrightbig
.For the sake of simplicity, we assume that the sampling locations are equally
spaced grid points, denoted as si= (ri+ 1)d, whereri+ 1represents the number of grid points in
each dimension. To achieve the discretization invariance, we consider the following interpolation opera-
torIxi:u(xi)∝⇕⊣√∫⊔≀→˜u, where ˜urepresents the multivariate Lagrangian polynomials (refer to Leaf & Kaper
(1974) for more details). Subsequently, we map the Lagrangian polynomials to their discretization on a
uniformly spaced grid mesh ˆx∈RdXusing the sampling operator Pˆx. HeredX= (r+ 1)dandris the
highest degree among the Lagrangian polynomials ˜u. We further assume that the grid points xiof all given
discretized data are subsets of ˆx. We can then construct a discretization-invariant encoder as follows:
Ei
X=Pˆx◦Ixi◦Pxi.
We can define the encoder Ei
Yin a similar manner. The aforementioned discussion can be summarized in
the following proposition:
Proposition 1. Suppose that the discretized data {(ui,vi),i= 1,...,n}defined in equation 10 are images
of a sampling operator Pxiapplied to smooth functions (ui,vi), and the sampling locations xiare equally
spaced grid points with grid size h. Let ˆx∈RdXrepresent equally spaced grid points that are denser than all
xi,i= 1,...,nwithdX= (r+1)d. Define the encoder Ei
X=Ei
Y=Pˆx◦Ixi◦Pxi, and decoder Di
X=Di
Y=Iˆx.
Then the encoding error can be bounded as the following:
Eu/bracketleftbig
∥Πi
X,dX(u)−u∥2
∞/bracketrightbig
≤Ch2r∥u∥2
Cr+1,Ev/bracketleftbig
∥Πi
Y,dY(v)−v∥2
∞/bracketrightbig
≤Ch2r∥v∥2
Cr+1, (11)
whereC > 0is an absolution constant, Πi
X,dX:=Di
X◦Ei
XandΠi
Y,dY:=Di
Y◦Ei
Y.
Proof.This result follows directly from the principles of Multivariate Lagrangian interpolation and Theorem
3.2 in Leaf & Kaper (1974).
8Published in Transactions on Machine Learning Research (10/2023)
Remark 5.To simplify the analysis, we focus on the L∞norm in equation 11. However, it is worth noting
thatLpestimates can be easily derived by utilizing Lpspace embedding techniques. Furthermore, Cr
estimates can be obtained through the proof of Theorem 3.2 in Leaf & Kaper (1974). By observing that the
discretization invariant encoder and decoder in Proposition 1 satisfy Assumption 2 and Assumption 4, we can
conclude that our main results are applicable to discretization invariant neural networks. In this section, we
have solely considered polynomial interpolation encoders, which require the input data to possess a sufficient
degree of smoothness and for all training data to be discretized on a finer mesh than the encoding space
RdX. The analysis of more sophisticated nonlinear encoders and discretization invariant neural networks is
a topic for future research.
In the subsequent sections, we will observe that numerous operators encountered in PDE problems can
be expressed as compositions of low-complexity operators, as stated in Assumption 6 or Assumption 7.
Consequently, deep operator learning provides means to alleviate the curse of dimensionality, as confirmed
by Theorem 4 or its more general form, as presented in Theorem 5.
3 Explicit complexity bounds for various PDE operator learning
In practical scenarios, enforcing the uniform bound constraint in architecture (3) is often inconvenient. As
a result, the preferred implementation choice is architecture (4). Therefore, in this section, we will solely
focus on architecture (4). In this section, we will provide five examples of PDEs where the input space
Xand output space Yare not Hilbert. For simplicity, we assume that the computational domain for all
PDEs is Ω = [−1,1]d. Additionally, we assume that the input space Xexhibits Hölder regularity. In other
words, all inputs possess a bounded Hölder norm ∥·∥Cs, wheres > 0. The Hölder norm is defined as
∥f∥Cs=∥f∥Ck+ maxβ=k|Dβf|C0,α, wheres=k+α,kis an integer, 0< α < 1and|·|C0,αrepresents
theα-Hölder semi-norm |f|C0,α= supx̸=y|f(x)−f(y)|
∥x−y∥α. It can be shown that the output space Yalso admits
Hölder regularity for all examples considered in this section. Similar results can be derived when both the
input space and output space have Sobolev regularity. Consequently, we can employ the standard spectral
method as the encoder/decoder for both the input and output spaces. Specifically, the encoder En
Xmaps
u∈Xto the space Pr
d, which represents the product of univariate polynomials with a degree less than r. As
a result, the input dimension is thus dX=dimPr
d=rd.We then assign Lp-norm (p>1) to both the input
spaceXand output space Y. The encoder/decoder projection error for both XandYcan be derived using
the following lemma from Schultz (1969).
Lemma 1 (Theorem 4.3 (ii) of Schultz (1969)) .Let an integer k≥0and0< α < 1. For any f∈
Cs([−1,1]d)withs=k+α, denote by ˜fits spectral approximation in Pr
d, there holds
∥f−˜f∥∞≤Cd∥f∥Csr−s.
We can then bound the projection error
∥Πn
X,dXu−u∥p
Lp([−1,1]d)=/integraldisplay
[−1,1]d|u−˜u|pdx≤Cp
d2d∥u∥p
Csr−ps≤Cp
d2d∥u∥p
Csd−ps
d
X.
Therefore,
∥Πn
X,dXu−u∥2
Lp([−1,1]d)≤C2
d22d/p∥u∥2
Csd−2s
d
X. (12)
Similarly, we can also derive that
∥Πn
Y,dY(w)−w∥2
Lp([−1,1]d)≤C2
d22d/p∥u∥2
Ctd−2t
d
YL2
Φ, (13)
given that the output w= Φ(u)is inCtfor somet>0.
In the following, we present several examples of PDEs that satisfy different assumptions, including the low-
dimensional Assumption 5, the low-complexity Assumption 6, and Assumption 7. In particular, the solution
operators of Poisson equation, parabolic equation, and transport equation are linear operators, implying that
Assumption 6 is satisfied with gi’s being the identity functions with d0= 1. The solution operator of Burgers
9Published in Transactions on Machine Learning Research (10/2023)
equation is the composition of multiple numerical integration, the pointwise evaluation of an exponential
functiong1
j(·) = exp(·), and the pointwise division g2
j(a,b) =a/b. It thus satisfies Assumption 7 with d1= 1
andd2= 2. In parametric equations, we consider the forward operator that maps a media function a(x)
to the solution u. In most applications of such forward maps, the media function a(x)represents natural
images, such as CT scans for breast cancer diagnosis. Therefore, it is often assumed that Assumption 5
holds.
3.1 Poisson equation
Consider the Poisson equation which seeks usuch that
∆u=f, (14)
wherex∈Rd, and|u(x)|→0as|x|→∞. The fundamental solution of equation 14 is given as
Ψ(x) =/braceleftbigg1
2πln|x|,ford= 2,
−1
wd|x|2−d,ford≥3,
wherewdis the surface area of a unit ball in Rd. Assume that the source f(x)is a smooth function compactly
supportedin Rd. Thereexistsauniquesolutiontoequation14givenby u(x) = Ψ∗f. Noticethatthesolution
mapf∝⇕⊣√∫⊔≀→uis a convolution with the fundamental solution, u(x) = Ψ∗f. To show the solution operator is
Lipschitz, we assume the sources f,g∈Ck(Rd)with compact support and apply Young’s inequality to get
∥u−v∥Ck(Rd)=∥Dk(u−v)∥L∞(Rd)=∥Ψ∗Dk(f−g)∥L∞(Rd)≤∥Ψ∥Lp(Rd)∥f−g∥Ck(Ω)|Ω|1/q,(15)
wherep,q≥1so that 1/p+ 1/q= 1. Here Ωis the support of fandg.
For the Poisson equation (14) on an unbounded domain, the computation is often implemented over a
truncated finite domain Ω. For simplicity, we assume the source condition fis randomly generated in the
spaceCk(Ω)from a random measure γ. Since the solution uis a convolution of source fwith a smooth
kernel, both fanduare inCk(Ω).
We then choose the encoder and decoder to be the spectral method. Applying equation 12, the encoder and
decoder error of the input space can be calculated as follows
Ef/bracketleftig
∥Πn
X,dX(f)−f∥2
Lp(Ω)/bracketrightig
≤Cd,pd−2k
d
XEf/bracketleftig
∥f∥2
Ck(Ω)/bracketrightig
.
Similarly, applying Lemma 1 and equation 15, the encoder and decoder error of the output space is
ESEf∼γ/bracketleftig
∥Πn
Y,dY(u)−u∥2
Lp(Ω)/bracketrightig
≤Cd,pd−2k
d
YEf/bracketleftig
∥Ψ∗f∥2
Ck(Ω)/bracketrightig
≤Cd,p,Ωd−2k
d
YEf/bracketleftig
∥f∥2
Ck(Ω)/bracketrightig
.
Notice that the solution u(y) =/integraltext
RdΨ(y−x)f(x)dxis a linear integral transform of f, and that all linear
maps are special cases of Assumption 6 with gbeing the identity map. In particular, Assumption 6 thus
holds true by setting the column vector Vkas the numerical integration weight of Ψ(x−yk), and setting gk’s
as the identity map with d0= 1fork= 1,···,dY. By applying Theorem 4, we obtain that
ESEf∥Dn
Y◦ΓNN◦En
X(f)−Φ(f)∥2
Lp(Ω)≲r3dn−2/3logn+ (σ2+n−1) +r−2kEf/bracketleftig
∥f∥2
Ck(Ω)/bracketrightig
,(16)
where the input dimension dX=dY=rdand≲contains constants that depend on dX,d,pand|Ω|.
Remark 6.The above result equation 16 suggests that the generalization error is small if we have a large
number of samples, a small noise, and a good regularity of the input samples. Importantly, the decay rate
with respect to the number of samples is independent from the encoding dimension dXordY.
3.2 Parabolic equation
We consider the following parabolic equation that seeks u(x,t)such that
/braceleftbigg
ut−∆u= 0inRd×(0,∞),
u=gonRd×{t= 0}.(17)
10Published in Transactions on Machine Learning Research (10/2023)
The fundamental solution to equation 17 is given by Λ(x,t) = (4πt)−d/2e−|x|2
4tforx∈Rd,t> 0. The solution
mapg(·)∝⇕⊣√∫⊔≀→u(T,·)can be expressed as a convolution with the fundamental solution u(·,T) = Λ(·,T)∗g,
whereTis the terminal time. Applying Young’s inequality, the Lipschitz constant is ∥Λ(·,T)∥p, where
1≤p≤∞. As an example, we can explicitly calculate this number in 3D as ∥Λ(·,T)∥p=p3
2p. For the
parabolic equation (17), we consider a truncated finite computation domain Ω×[0,T]and assume an initial
conditiong∈Ck(Ω). Due to the similar convolution structure of the solution map compared to the Poisson
equation, we can obtain a similar result by applying Theorem 4.
ESEg∥Dn
Y◦ΓNN◦En
X(g)−Φ(g)∥2
Lp(Ω)≲r3dn−2/3logn+(σ2+n−1) +r−2kEg/bracketleftig
∥g∥2
Ck(Ω)/bracketrightig
,(18)
where the encoding dimension dX=dY=rd, the symbol “ ≲” denotes that the expression on the left-hand
side is bounded by the expression on the right-hand side, where the constants involved depend on dX,d,p,
and|Ω|. The reduction of the CoD in the parabolic equation follows a similar approach as in the Poisson
equation.
3.3 Transport equation
We consider the following transport equation that seeks usuch that
/braceleftigg
ut+a(x)·∇u= 0 in(0,∞)×Rd,
u(0,x) =u0(x)inRd,(19)
wherea(x)is the drift force field and u0(x)is the initial data. For convenience, we assume that the drift force
field satisfies a∈C2(Rd)∩W1,∞(Rd). By employing the classical theory of ordinary differential equations
(ODE), we consider the initial value problemdx(t)
dt=a(x(t)), x(0) =x, which admits a unique solution for
anyx∈Rd,t→x(t) =φt(x)∈C1(R;Rd). Applying the Characteristic method, the solution of equation 19
is given byu(t,x) :=u0(φ−1
t(x)). If we further assume that u0is randomly sampled with bounded Hsnorm,
s>3d
2, then by Theorem 5 of Section 7.3 of Evans (2010), we have u∈C1([0,∞);Rd). More specifically, we
have
∥u(T,·)∥C1(Rd)≤∥u0∥Hs(Rd)Ca,T,Ω,
whereCa,T,Ω>0is a constant that depends on the media a, terminal time T, and the support Ωof the
initial data. Since the initial data has C1regularity, by equation 12 the encoder/decoder projection error of
the input space is controlled via
Eu0/bracketleftig
∥Πn
X,dX(u0)−u0∥2
Lp(Ω)/bracketrightig
≤Cd,p,Ωd−2
d
XEf/bracketleftig
∥u0∥2
C1(Ω)/bracketrightig
.
Similarly, for the projection error of the output space, we have
ESEu∼Φ#γ/bracketleftig
∥Πn
Y,dY(u)−u∥2
Lp(Ω)/bracketrightig
≤Cd,p,Ωd−2
d
YEu0/bracketleftig
∥u(T)∥2
C1(Ω)/bracketrightig
≤Cd,p,a,T, Ωd−2
d
YEu0/bracketleftig
∥u0∥2
Hs(Ω)/bracketrightig
.
We again use the spectral encoder/decoder so dX=dY=rd. Notice that solution u(T,x) =u0(φ−1
T(x))is a
translation of the initial data u0byφ−1
T, which is a linear transform. Let V∈RdX×dYbe the corresponding
permutation matrix that characterizes the translation by φ−1
T, thenV⊤
kis thek-th row ofV. Then by setting
gk’s as the identity map, Assumption 6 holds with d0= 1. Apply Theorem 4 to derive that
ESEu∥Dn
Y◦ΓNN◦En
X(u)−Φ(u)∥2
Lp(Ω)≲r3dn−2/3logn+ (σ2+n−1)+r−2Eg/bracketleftig
∥u0∥2
C1(Ω)+∥u0∥2
Hs(Ω)/bracketrightig
,
(20)
where ≲contains constants that depend on d,p,a,r,T andΩ. The CoD in transport equation is lessened
according to equation 20 in the same manner as in the Poisson and parabolic equations.
11Published in Transactions on Machine Learning Research (10/2023)
3.4 Burgers equation
We consider the 1D Burgers equation with periodic boundary conditions:


ut+uux=κuxx,inR×(0,∞),
u(x,0) =u0(x),
u(−π,t) =u(π,t),(21)
whereκ>0is the viscosity constant. and we consider the solution map u0(·)∝⇕⊣√∫⊔≀→u(T,·). This solution map
can be explicitly written using the Cole-Hopf transformation u=−2κvx
vwhere the function vis the solution
to the following diffusion equation
/braceleftigg
vt=κvxx
v(x,0) =v0(x) = exp/parenleftig
−1
2κ/integraltextx
−πu0(s)ds/parenrightig
.
The solution to the above diffusion equation is given by
v(x,T) =−2κ/integraltext
R∂xK(x,y,T )v0(y)dy/integraltext
RK(x,y,T )v0(y)dy, (22)
where the integration kernel Kis defined asK(x,y,t ) =1√
4πκtexp/parenleftig
−(x−y)2
4πt/parenrightig
. Although there will be no
shock formed in the solution of viscous Burger equation, the solution may form a large gradient in finite
time for certain initial data, which makes it extremely hard to be approximated by a NN. We assume that
the terminal time Tis small enough so a large gradient is not formed yet. In fact, it is shown in Heywood
& Xie (1997) (Theorem 1) that if T≤C∥u0∥−4
H1, then∥u(·,T)∥H1≤C∥u0∥H1. We then assume an initial
datau0is randomly sampled with a uniform bounded H1norm. By Sobolev embedding, we have
∥u0∥C0,1/2≤C∥u0∥H1,∥u(·,T)∥C0,1/2≤C∥u0∥H1,
By 12, we can control the encoder/decoder projection error for the initial data
Eu0/bracketleftig
∥Πn
X,dX(u0)−u0∥2
Lp(Ω)/bracketrightig
≤Cd,pd−1
XEu0/bracketleftbig
∥u0∥2
H1/bracketrightbig
.
Since the terminal solution u(·,T)has same regularity as the initial solution, by 13 we also have
Eu0/bracketleftig
∥Πn
X,dX(u(·,T))−u(·,T)∥2
Lp(Ω)/bracketrightig
≤Cd,pd−1
YEu0/bracketleftbig
∥u(·,T)∥2
H1/bracketrightbig
≤Cd,pd−1
YEu0/bracketleftbig
∥u0∥2
H1/bracketrightbig
.
Similarly, we can choose dX=dY=r. The solution map is a composition of three mappings u0∝⇕⊣√∫⊔≀→v0,v0∝⇕⊣√∫⊔≀→
v(·,T)andv(·,T)∝⇕⊣√∫⊔≀→u(·,T). More specifically, v0(x) = exp/parenleftig
−1
2κ/integraltextx
−πu0(s)ds/parenrightig
so we can set V1
k=RdX×1
as the numerical integration vector on [−π,xk]andg1
k(x) = exp(−x
2κ)for allk= 1,...,dY. For the second
mappingv0∝⇕⊣√∫⊔≀→v(·,T)(c.f. 22), we set V2
k∈RdX×2where the first row is the numerical integration with
kernel∂xKand the second row is the numerical integration with kernel K, and we let g2
k(x,y) =−2κx
yfor
allk= 1,···,dX. For the third mapping u=−2κvx
v, we can set V3
k∈RdX×2, where the first row is the
k-row of the numerical differentiation matrix, and the second row is the Dirac-delta vector at xk, and we let
g3
k(x,y) =−2κx
yforallk= 1,···,dY. Therefore, Assumption7holdswith dmax= 2andlmax=dX=dY=r.
Then apply Theorem 5 to derive that
ESEu∥Dn
Y◦ΓNN◦En
X(u0)−u(·,T)∥2
Lp(Ω)≲r5/2n−1/2logn+ (σ2+n−1) +r−1Eg/bracketleftig
∥u0∥2
Hs(Ω)/bracketrightig
,(23)
where≲contains constants that depend on p,randT. The CoD in Burgers equations is lessened according
to equation 23 as well as in all other PDE examples.
12Published in Transactions on Machine Learning Research (10/2023)
3.5 Parametric elliptic equation
We consider the 2D elliptic equation with heterogeneous media in this subsection.
/braceleftigg
−div(a(x)∇xu(x)) = 0,inΩ⊂R2,
u=f,on∂Ω.(24)
The media coefficient a(x)satisfies that α≤a(x)≤βfor allx∈Ω, whereαandβare positive constants.
We further assume that a(x)∈C1(Ω). We are interested NN approximation of the forward map Φ :a∝⇕⊣√∫⊔≀→u
with a fixed boundary condition f, which has wide applications in inverse problems. The forward map
is Lipschitz, see Appendix A.2. We apply Sobolev embedding and derive that u∈C0,1/2(Ω). Since the
parameterahasC1regularity, the encoder/decoder projection error of the input space is controlled
Ea/bracketleftig
∥Πn
X,dX(a)−a∥2
Lp(Ω)/bracketrightig
≤Cpd−1
XEf/bracketleftig
∥a∥2
C1(Ω)/bracketrightig
.
The solution has1
2Hölder regularity, so we have
ESEu∼Φ#γ/bracketleftig
∥Πn
Y,dY(u)−u∥2
Lp(Ω)/bracketrightig
=Ea/bracketleftbig
∥Πn
Y,dY(u)−u∥Lp(Ω)/bracketrightbig2≤Cpd−1
2
YEa/bracketleftig
∥u∥2
C0,1/2(Ω)/bracketrightig
≤Cp,α,β,fd−1
2
Y.
We use the spectral encoder/decoder and choose dX=dY=r2. We further assume that the media functions
a(x)are randomly sampled on a smooth d0-dimensional manifold. Applying Theorem 3, the generalization
error is thus bounded by
ESEu∥Dn
Y◦ΓNN◦En
X(u)−Φ(u)∥2
Lp(Ω)≲d8+d0
2+d0
Yn−2
2+d0log6n+ (σ2+n−1) +r−2Ea/bracketleftig
∥a∥2
C1(Ω)/bracketrightig
+r−1,
where ≲contains constants that depend on p,Ω,α,βandf. Hered0is a constant that characterized the
manifold dimension of the data set of media function a(x). For instance, the 2D Shepp-Logan phantom
Gach et al. (2008) contains multiple ellipsoids with different intensities thus the images in this data set lies
on a manifold with a small d0. The decay rate in terms of the number of samples nsolely depends on d0,
therefore the CoD of the parametric elliptic equations is mitigated.
4 Limitations and discussions
Our work focuses on exploring the efficacy of fully connected DNNs as surrogate models for solving general
PDE problems. We provide an explicit estimation of the training sample complexity for generalization
error. Notably, when the PDE solution lies in a low-dimensional manifold or the solution space exhibits low
complexity, our estimate demonstrates a logarithmic dependence on the problem resolution, thereby reducing
the CoD. Our findings offer a theoretical explanation for the improved performance of deep operator learning
in PDE applications.
However, our work relies on the assumption of Lipschitz continuity for the target PDE operator. Conse-
quently, our estimates may not be satisfactory if the Lipschitz constant is large. This limitation hampers
the application of our theory to operator learning in PDE inverse problems, which focus on the solution-
to-parameter map. Although the solution-to-parameter map is Lipschitz in many applications (e.g., electric
impedance tomography, optical tomography, and inverse scattering), certain scenarios may feature an expo-
nentially large Lipschitz constant, rendering our estimates less practical. Therefore, our results cannot fully
explain the empirical success of PDE operator learning in such cases.
While our primary focus is on neural network approximation, determining suitable encoders and decoders
with small encoding dimensions ( dXanddY) remains a challenging task that we did not emphasize in this
work. In Section 2.2, we analyze the naive interpolation as a discretization invariant encoder using a fully
connected neural network architecture. However, this analysis is limited to cases where the training data is
sampled on an equally spaced mesh and may not be applicable to more complex neural network architectures
or situations where the data is not uniformly sampled. Investigating the discretization invariant properties
of other neural networks, such as IAE-net Ong et al. (2022), FNO Li et al. (2021), and DeepONet, would be
an interesting avenue for future research.
13Published in Transactions on Machine Learning Research (10/2023)
Acknowledgements
K.C.andH.Y.werepartiallysupportedbytheUSNationalScienceFoundationunderawardsDMS-2244988,
DMS-2206333, and the Office of Naval Research Award N00014-23-1-2007. C. W. was partially supported
by the National Science Foundation under awards DMS-2136380 and DMS-2206332.
References
Ben Adcock, Simone Brugiapaglia, Nick Dexter, and Sebastian Moraga. Near-optimal learning of banach-
valued, high-dimensional functions via deep neural networks. arXiv preprint arXiv:2211.12633 , 2022.
Martin Anthony, Peter L Bartlett, Peter L Bartlett, et al. Neural network learning: Theoretical foundations ,
volume 9. cambridge university press Cambridge, 1999.
Andrew R Barron. Universal approximation bounds for superpositions of a sigmoidal function. IEEE
Transactions on Information theory , 39(3):930–945, 1993.
Peter L Bartlett, Nick Harvey, Christopher Liaw, and Abbas Mehrabian. Nearly-tight vc-dimension and
pseudodimension bounds for piecewise linear neural networks. The Journal of Machine Learning Research ,
20(1):2285–2301, 2019.
Benedikt Bauer and Michael Kohler. On deep learning as a remedy for the curse of dimensionality in
nonparametric regression. The Annals of Statistics , 47(4):2261–2285, 2019.
Sergio Botelho, Ameya Joshi, Biswajit Khara, Vinay Rao, Soumik Sarkar, Chinmay Hegde, Santi Adavani,
and Baskar Ganapathysubramanian. Deep generative models that solve pdes: Distributed computing for
training large data-free models. In 2020 IEEE/ACM Workshop on Machine Learning in High Performance
Computing Environments (MLHPC) and Workshop on Artificial Intelligence and Machine Learning for
Scientific Applications (AI4S) , pp. 50–63. IEEE, 2020.
Shengze Cai, Zhicheng Wang, Lu Lu, Tamer A Zaki, and George Em Karniadakis. Deepm&mnet: Inferring
the electroconvection multiphysics fields based on operator approximation by neural networks. Journal of
Computational Physics , 436:110296, 2021.
Minshuo Chen, Haoming Jiang, Wenjing Liao, and Tuo Zhao. Efficient approximation of deep relu networks
for functions on low dimensional manifolds. Advances in neural information processing systems , 32, 2019.
Minshuo Chen, Haoming Jiang, Wenjing Liao, and Tuo Zhao. Nonparametric regression on low-dimensional
manifolds using deep relu networks: Function approximation and statistical recovery. Information and
Inference: A Journal of the IMA , 11(4):1203–1253, 2022.
Abdellah Chkifa, Albert Cohen, and Christoph Schwab. Breaking the curse of dimensionality in sparse
polynomial approximation of parametric pdes. Journal de Mathématiques Pures et Appliquées , 103(2):
400–428, 2015.
Alexander Cloninger and Timo Klock. Relu nets adapt to intrinsic dimensionality beyond the target domain.
arXiv preprint arXiv:2008.02545 , 2020.
George Cybenko. Approximation by superpositions of a sigmoidal function. Mathematics of control, signals
and systems , 2(4):303–314, 1989.
Maarten V de Hoop, Nikola B Kovachki, Nicholas H Nelsen, and Andrew M Stuart. Convergence rates for
learning linear operators from noisy data. arXiv preprint arXiv:2108.12515 , 2021.
Mo Deng, Shuai Li, Alexandre Goy, Iksung Kang, and George Barbastathis. Learning to synthesize: robust
phase retrieval at low photon counts. Light: Science & Applications , 9(1):1–16, 2020.
Lawrence C Evans. Partial differential equations , volume 19. American Mathematical Soc., 2010.
14Published in Transactions on Machine Learning Research (10/2023)
Max H Farrell, Tengyuan Liang, and Sanjog Misra. Deep neural networks for estimation and inference.
Econometrica , 89(1):181–213, 2021.
H Michael Gach, Costin Tanase, and Fernando Boada. 2d & 3d shepp-logan phantom standards for mri. In
2008 19th International Conference on Systems Engineering , pp. 521–526. IEEE, 2008.
Alex Graves, Abdel-rahman Mohamed, and Geoffrey Hinton. Speech recognition with deep recurrent neural
networks. In 2013 IEEE international conference on acoustics, speech and signal processing , pp.6645–6649.
Ieee, 2013.
JohnGHeywoodandWenzhengXie. Smoothsolutionsofthevectorburgersequationinnonsmoothdomains.
Diferential and Integral Equations , 10(5):961–974, 1997.
Kurt Hornik. Approximation capabilities of multilayer feedforward networks. Neural networks , 4(2):251–257,
1991.
Teeratorn Kadeethum, Daniel O’Malley, Jan Niklas Fuhg, Youngsoo Choi, Jonghyun Lee, Hari S
Viswanathan, and Nikolaos Bouklas. A framework for data-driven solution and parameter estimation
of pdes using conditional generative adversarial networks. Nature Computational Science , 1(12):819–829,
2021.
Yuehaw Khoo and Lexing Ying. Switchnet: a neural network model for forward and inverse scattering
problems. SIAM Journal on Scientific Computing , 41(5):A3182–A3201, 2019.
Michael Kohler and Adam Krzyżak. Adaptive regression estimation with multilayer feedforward neural
networks. Nonparametric Statistics , 17(8):891–913, 2005.
Nikola Kovachki, Samuel Lanthaler, and Siddhartha Mishra. On universal approximation and error bounds
for fourier neural operators. Journal of Machine Learning Research , 22:Art–No, 2021.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional
neural networks. Communications of the ACM , 60(6):84–90, 2017.
Samuel Lanthaler, Siddhartha Mishra, and George E Karniadakis. Error estimates for deeponets: A deep
learningframeworkininfinitedimensions. Transactions of Mathematics and Its Applications , 6(1):tnac001,
2022.
Gary K Leaf and Hans G Kaper. l∞-error bounds for multivariate lagrange approximation. SIAM Journal
on Numerical Analysis , 11(2):363–381, 1974.
Zongyi Li, Nikola Borislavov Kovachki, Kamyar Azizzadenesheli, Burigede liu, Kaushik Bhattacharya, An-
drewStuart, andAnimaAnandkumar. Fourierneuraloperatorforparametricpartialdifferentialequations.
InInternational Conference on Learning Representations , 2021. URL https://openreview.net/forum?
id=c8P9NQVtmnO .
Chensen Lin, Zhen Li, Lu Lu, Shengze Cai, Martin Maxey, and George Em Karniadakis. Operator learning
for predicting multiscale bubble growth dynamics. The Journal of Chemical Physics , 154(10):104118, 2021.
Hao Liu, Minshuo Chen, Tuo Zhao, and Wenjing Liao. Besov function approximation and binary classifica-
tion on low-dimensional manifolds using convolutional residual networks. In International Conference on
Machine Learning , pp. 6770–6780. PMLR, 2021.
Hao Liu, Haizhao Yang, Minshuo Chen, Tuo Zhao, and Wenjing Liao. Deep nonparametric estimation of
operators between infinite dimensional spaces. arXiv preprint arXiv:2201.00217 , 2022.
Jianfeng Lu, Zuowei Shen, Haizhao Yang, and Shijun Zhang. Deep network approximation for smooth
functions. SIAM Journal on Mathematical Analysis , 53(5):5465–5506, 2021a.
Lu Lu, Pengzhan Jin, Guofei Pang, Zhongqiang Zhang, and George Em Karniadakis. Learning nonlinear
operators via deeponet based on the universal approximation theorem of operators. Nature Machine
Intelligence , 3(3):218–229, 2021b.
15Published in Transactions on Machine Learning Research (10/2023)
Lu Lu, Xuhui Meng, Shengze Cai, Zhiping Mao, Somdatta Goswami, Zhongqiang Zhang, and George Em
Karniadakis. A comprehensive and fair comparison of two neural operators (with practical extensions)
based on fair data. Computer Methods in Applied Mechanics and Engineering , 393:114778, 2022.
Chao Ma, Lei Wu, et al. The barron space and the flow-induced function spaces for neural network models.
Constructive Approximation , 55(1):369–406, 2022.
Riccardo Miotto, Fei Wang, Shuang Wang, Xiaoqian Jiang, and Joel T Dudley. Deep learning for healthcare:
review, opportunities and challenges. Briefings in bioinformatics , 19(6):1236–1246, 2018.
Ryumei Nakada and Masaaki Imaizumi. Adaptive approximation and generalization of deep neural network
with intrinsic dimensionality. J. Mach. Learn. Res. , 21(174):1–38, 2020.
Nicholas H Nelsen and Andrew M Stuart. The random feature model for input-output maps between banach
spaces.SIAM Journal on Scientific Computing , 43(5):A3212–A3243, 2021.
Partha Niyogi, Stephen Smale, and Shmuel Weinberger. Finding the homology of submanifolds with high
confidence from random samples. Discrete & Computational Geometry , 39(1):419–441, 2008.
Yong Zheng Ong, Zuowei Shen, and Haizhao Yang. Integral autoencoder network for discretization-invariant
learning. The Journal of Machine Learning Research , 23(1):12996–13040, 2022.
Benjamin Peherstorfer and Karen Willcox. Data-driven operator inference for nonintrusive projection-based
model reduction. Computer Methods in Applied Mechanics and Engineering , 306:196–215, 2016.
Chang Qiao, Di Li, Yuting Guo, Chong Liu, Tao Jiang, Qionghai Dai, and Dong Li. Evaluation and
development of deep neural networks for image super-resolution in optical microscopy. Nature Methods ,
18(2):194–202, 2021.
Md Ashiqur Rahman, Manuel A Florez, Anima Anandkumar, Zachary E Ross, and Kamyar Azizzadenesheli.
Generative adversarial neural operators. arXiv preprint arXiv:2205.03017 , 2022.
Johannes Schmidt-Hieber. Deep relu network approximation of functions on a manifold. arXiv preprint
arXiv:1908.00695 , 2019.
Johannes Schmidt-Hieber. Nonparametric regression using deep neural networks with relu activation func-
tion.The Annals of Statistics , 48(4):1875–1897, 2020.
Martin H Schultz. l∞-multivariate approximation theory. SIAM Journal on Numerical Analysis , 6(2):
161–183, 1969.
Zuowei Shen, Haizhao Yang, and Shijun Zhang. Deep network approximation characterized by number of
neurons. arXiv preprint arXiv:1906.05497 , 2019.
Zuowei Shen, Haizhao Yang, and Shijun Zhang. Deep network with approximation error being reciprocal of
width to power of square root of depth. Neural Computation , 33(4):1005–1036, 2021.
Charles J Stone. Optimal global rates of convergence for nonparametric regression. The annals of statistics ,
pp. 1040–1053, 1982.
Taiji Suzuki. Adaptivity of deep relu network for learning in besov and mixed smooth besov spaces: optimal
rate and curse of dimensionality. arXiv preprint arXiv:1810.08033 , 2018.
Chunwei Tian, Lunke Fei, Wenxian Zheng, Yong Xu, Wangmeng Zuo, and Chia-Wen Lin. Deep learning on
image denoising: An overview. Neural Networks , 131:251–275, 2020.
TingWang, PetrPlechac, andJaroslawKnap. Generativediffusionlearningforparametricpartialdifferential
equations. arXiv preprint arXiv:2305.14703 , 2023.
Dmitry Yarotsky. Error bounds for approximations with deep relu networks. Neural Networks , 94:103–114,
2017.
16Published in Transactions on Machine Learning Research (10/2023)
Dmitry Yarotsky. Optimal approximation of continuous functions by very deep relu networks. In Conference
on learning theory , pp. 639–649. PMLR, 2018.
DmitryYarotsky. Elementarysuperexpressiveactivations. In International Conference on Machine Learning ,
pp. 11932–11940. PMLR, 2021.
17Published in Transactions on Machine Learning Research (10/2023)
A Appendix
A.1 Proofs of the main theorems
Proof of Theorem 1. TheL2squared error can be decomposed as
ESEu∼γ/bracketleftbig
∥Dn
Y◦ΓNN◦En
X(u)−Φ(u)∥2
Y/bracketrightbig
≤2ESEu∼γ/bracketleftbig
∥Dn
Y◦ΓNN◦En
X(u)−Dn
Y◦En
Y◦Φ(u)∥2
Y/bracketrightbig
+ 2ESEu∼γ/bracketleftbig
∥Dn
Y◦En
Y◦Φ(u)−Φ(u)∥2
Y/bracketrightbig
,
where the first term I = 2ESEu∼γ/bracketleftbig
∥Dn
Y◦ΓNN◦En
X(u)−Dn
Y◦En
Y◦Φ(u)∥2
Y/bracketrightbig
is the network estimation error
in theYspace, and the second term II = 2 ESEu∼γ/bracketleftbig
∥Dn
Y◦En
Y◦Φ(u)−Φ(u)∥2
Y/bracketrightbig
is the empirical projection
error, which can be rewritten as
II = 2ESEw∼Φ#γ/bracketleftbig
∥Πn
Y,dY(w)−w∥2
Y/bracketrightbig
. (25)
Weaimtoderiveanupperboundofthefirstterm I. First, notethatthedecoder Dn
YisLipschitz(Assumption
3). We have
I = 2ESEu∼γ/bracketleftbig
∥Dn
Y◦ΓNN◦En
X(u)−Dn
Y◦En
Y◦Φ(u)∥2
Y/bracketrightbig
≤2L2
Dn
YESEu∼γ/bracketleftbig
∥ΓNN◦En
X(u)−En
Y◦Φ(u)∥2
2/bracketrightbig
.
Conditioned on the data set S1, we can obtain
ES2Eu∼γ/bracketleftbig
∥ΓNN◦En
X(u)−En
Y◦Φ(u)∥2
2/bracketrightbig
=2ES2/bracketleftigg
1
n2n/summationdisplay
i=n+1∥ΓNN◦En
X(ui)−En
Y◦Φ(ui)∥2
2/bracketrightigg
+ES2Eu∼γ/bracketleftbig
∥ΓNN◦En
X(u)−En
Y◦Φ(u)∥2
2/bracketrightbig
−2ES2/bracketleftigg
1
n2n/summationdisplay
i=n+1∥ΓNN◦En
X(ui)−En
Y◦Φ(ui)∥2
2/bracketrightigg
=T1+T2,(26)
where the first term T1= 2ES2/bracketleftig
1
n/summationtext2n
i=n+1∥ΓNN◦En
X(ui)−En
Y◦Φ(ui)∥2
2/bracketrightig
includes the DNN ap-
proximation error and the projection error in the Xspace, and the second term T2=
ES2Eu∼γ/bracketleftbig
∥ΓNN◦En
X(u)−En
Y◦Φ(u)∥2
2/bracketrightbig
−T1captures the variance.
To obtain an upper bound of T1, we apply triangle inequality to separate the noise from T1
T1≤2ES2/bracketleftigg
1
n2n/summationdisplay
i=n+1∥ΓNN◦En
X(ui)−En
Y(vi)∥2
2/bracketrightigg
+ 2ES2/bracketleftigg
1
n2n/summationdisplay
i=n+1∥En
Y◦Φ(ui)−En
Y(vi)∥2
2/bracketrightigg
.
Using the definition of ΓNN, we have
T1≤2ES2/bracketleftigg
inf
Γ∈NN1
n2n/summationdisplay
i=n+1∥Γ◦En
X(ui)−En
Y(vi)∥2
2/bracketrightigg
+ 2L2
En
YES21
n2n/summationdisplay
i=n+1∥εi∥2
Y.
Using Fatou’s lemma, we have
T1≤4ES2/bracketleftigg
inf
Γ∈FNN1
n2n/summationdisplay
i=n+1∥Γ◦En
X(ui)−En
Y◦Φ(ui)∥2
2/bracketrightigg
+ 6L2
En
YES21
n2n/summationdisplay
i=n+1∥εi∥2
Y
≤4 inf
Γ∈FNNES2/bracketleftigg
1
n2n/summationdisplay
i=n+1∥Γ◦En
X(ui)−En
Y◦Φ(ui)∥2
2/bracketrightigg
+ 6L2
En
YES21
n2n/summationdisplay
i=n+1∥εi∥2
Y
= 4 inf
Γ∈FNNEu/bracketleftbig
∥Γ◦En
X(u)−En
Y◦Φ(u)∥2
2/bracketrightbig
+ 6L2
En
YES21
n2n/summationdisplay
i=n+1∥εi∥2
Y.(27)
18Published in Transactions on Machine Learning Research (10/2023)
To bound the first term on the last line of Equation equation 27, we consider the discrete transform Γn
d:=
En
Y◦Φ◦Dn
X∈F NN. Note that it is a vector filed that maps RdXtoRdY, and by Assumption 1, 2, and
3 each component h(·)is a function supported on [−B,B]dXwith Lipschitz constant M:=LDn
XLΦLEn
Y,
whereB=RXLEn
X. This implies that each component hhas an infinity bound ∥h∥∞≤R:=BM =
LDn
XLΦLEn
YRXLEn
X.
We now apply the following lemma to the component functions of Γn
d.
Lemma 2. For any function f∈Wn,∞([−1,1]d), andϵ∈(0,1), we assume that∥f∥Wn,∞≤1. There exists
a function ˜f∈FNN(1,L,p,K,κ,M )such that
∥˜f−f∥∞<ϵ,
where the parameters of FNNare chosen as
L= Ω((n+d) lnϵ−1+n2lnd+d2), p = Ω(dd+nϵ−d
nn−d2d2/n),
K= Ω(n2−ddd+n+22d2
nϵ−d
nlnϵ), κ = Ω(M2), M = Ω(d+n).(28)
Here all constants hidden in Ω(·)do not dependent on any parameters.
Proof.This is a direct consequence of proof of Theorem 1 in Yarotsky (2017) for Fn,d.
Lethi:RdX→R,i= 1,...,dYbe the components of Γn
d, then apply Lemma 2 to the rescaled component
1
Rhi(B·)withn= 1. It can be derived that there exists ˜hi∈FNN(1,L,˜p,K,κ,M )such that
max
x∈[−1,1]dX|1
Rhi(Bx)−˜hi(x)|≤˜ε1,
with parameters chosen as in equation 28, with n= 1,d=dX, andϵ= ˜ε. Using a change of variable, we
obtain that
max
x∈[−B,B]dX|hi(x)−R˜hi(x
B)|≤R˜ε1.
Assembling the neural networks R˜hi(·
B)together, we obtain an neural network ˜Γn
d∈FNN(dY,L,p,K,κ,M )
withp=dY˜p, such that
∥˜Γn
d−Γn
d∥∞≤ε1, (29)
Here the parameters of FNN(dY,L,p,K,κ,M )are chosen as
L= Ω(dXlnε−1
1), p = Ω(dYε−dX
1L−dX
Φ2d2
X),
K= Ω(pL), κ = Ω(M2), M≥/radicalbig
dYLEn
XRY.(30)
Here the constants in Ωmay depend on LDn
X,LEn
X,LEn
YandRX. Then we can develop an estimate of T1as
follows.
inf
Γ∈FNNEu/bracketleftbig
∥Γ◦En
X(u)−En
Y◦Φ(u)∥2
2/bracketrightbig
≤Eu/bracketleftbig
∥˜Γn
d◦En
X(u)−En
Y◦Φ(u)∥2
2/bracketrightbig
≤2Eu/bracketleftbig
∥˜Γn
d◦En
X(u)−Γn
d◦En
X(u)∥2
2/bracketrightbig
+ 2Eu/bracketleftbig
∥Γn
d◦En
X(u)−En
Y◦Φ(u)∥2
2/bracketrightbig
≤2dYε2
1+ 2Eu/bracketleftbig
∥Γn
d◦En
X(u)−En
Y◦Φ(u)∥2
2/bracketrightbig
,(31)
where we used the definition of infinimum in the first inequality, the triangle inequality in the second
inequality, and the approximation equation 29 in the third inequality. Using the definition of Φ, we obtain
inf
Γ∈FNNEu/bracketleftbig
∥Γ◦En
X(u)−En
Y◦Φ(u)∥2
2/bracketrightbig
=2dYε2
1+ 2Eu/bracketleftbig
∥En
Y◦Φ◦Dn
X◦En
X(u)−En
Y◦Φ(u)∥2
2/bracketrightbig
≤2dYε2
1+ 2L2
En
YL2
ΦEu/bracketleftbig
∥Dn
X◦En
X(u)−u∥2
X/bracketrightbig
=2dYε2
1+ 2L2
En
YL2
ΦEu/bracketleftbig
∥Πn
X,dX(u)−u∥2
X/bracketrightbig
,(32)
19Published in Transactions on Machine Learning Research (10/2023)
where we used the Lipschitz continuity of ΦandEn
Yin the inequality above. Combining equation 32 and
equation 27, and apply Assumption 4, we have
T1≤8dYε2
1+ 8L2
En
YL2
ΦEu/bracketleftbig
∥Πn
X,dX(u)−u∥2
X/bracketrightbig
+ 6L2
En
Yσ2. (33)
To deal with the term T2, we shall use the covering number estimate of FNN(dY,L,p,K,κ,M ), which has
been done in Lemma 6 and Lemma 7 in Liu et al. (2022). A direct consequence of these two lemmas is
T2≤35dYL2
En
YR2
Y
nlogN/parenleftigg
δ
4dYLEn
Y,FNN,∥·∥∞/parenrightigg
+ 6δ
≲d2
YKL2
Φ
n/parenleftbig
lnδ−1+ lnL+ ln(pB) +Llnκ+Llnp/parenrightbig
+δ
≲d2
YKL2
Φ
n/parenleftbig
lnδ−1+ ln(B) +Llnκ+Llnp/parenrightbig
+δ,
where we used Lemma 6 and 7 from Liu et al. (2022) for the second inequality. The constant in ≲depends
onLEn
YandRX. Substituting parameters K,B,κfrom equation 30, the above estimate gives
T2≲L2
Φd2
Yn−1pL/parenleftbig
lnδ−1+LlnB+LlnR+Llnp/parenrightbig
+δ
≲L2
Φd2
Yn−1pL(lnδ−1+L2) +δ
≲L2
Φd2
Yn−1p/parenleftbig
L3+ (lnδ−1)2/parenrightbig
+δ,
where we used the fact lnδ−1≲Lwith the choice equation 34. The constant in ≲depends on
LEn
Y,LDn
Y,LEn
X,LDn
X,RXanddX. We further substitute the values of pandLfrom equation 30 into the
above estimate
T2≲L2−dX
Φd3
Yn−1ε−dX
1/parenleftbig
d3
X(lnε−1
1)3+ (lnδ−1)2/parenrightbig
+δ
Combining the T1estimate above and the T2estimate in equation 33 yields that
T1+T2≲dYε2
1+L2−dX
Φd3
Yn−1ε−dX
1/parenleftbig
(lnε−1
1)3+ (lnδ−1)2/parenrightbig
+L2
ΦEu/bracketleftbig
∥Πn
X,dX(u)−u∥2
X/bracketrightbig
+σ2+δ.
In order to balance the above error, we choose
δ=n−1, ε 1=d2
2+dX
Yn−1
2+dX. (34)
Therefore,
T1+T2≲d6+dX
2+dX
Yn−2
2+dX(1 +L2−dX
Φ )/parenleftbigg
(lnn
dY)3+ (lnn)2/parenrightbigg
+L2
ΦEu/bracketleftbig
∥Πn
X,dX(u)−u∥2
X/bracketrightbig
+σ2+n−1,(35)
where we combine the choice in equation 34 and equation 30 as
L= Ω(ln(n
dY)), p = Ω(d2−dX
2+dX
YndX
2+dX),
K= Ω(pL), κ = Ω(M2), M≥/radicalbig
dYLEn
XRY.
Here the notation Ωcontains constants that depends on LEn
Y,LDn
Y,LEn
X,LDn
X,RXanddX.
Combining equation 25 and equation 35, we have
ESEu∼γ/bracketleftbig
∥Dn
Y◦ΓNN◦En
X(u)−Φ(u)∥2
Y/bracketrightbig
≲d6+dX
2+dX
Yn−2
2+dX(1 +L2−dX
Φ ))/parenleftbigg
(lnn
dY)3+ (lnn)2/parenrightbigg
+L2
ΦEu/bracketleftbig
∥Πn
X,dX(u)−u∥2
X/bracketrightbig
+ESEw∼Φ#γ/bracketleftbig
∥Πn
Y,dY(w)−w∥2
Y/bracketrightbig
+σ2+n−1.
20Published in Transactions on Machine Learning Research (10/2023)
Proof of Theorem 2. Similarly to the proof of Theorem 1, we have
ESEu∥Dn
Y◦ΓNN◦En
X(u)−Φ(u)∥2
Y≤I + II,
and
I≤2L2
Dn
Y(T1+T2),
whereT1andT2are defined in equation 26. Following the same procedure in equation 27, we have
T1≤4 inf
Γ∈FNNEu/bracketleftbig
∥Γ◦En
X(u)−En
Y◦Φ(u)∥2
2/bracketrightbig
+ 6ES21
n2n/summationdisplay
i=n+1∥εi∥2
Y.
To obtain an approximation of the discretized target map Γn
d:=En
Y◦Φ◦Dn
X, we apply the following lemma
for each component function of Γn
d.
Lemma 3 (Theorem 1.1 in Shen et al. (2019)) .Givenf∈C([0,1]d), for anyL∈N+,p∈N+, there exists
a functionϕimplemented by a ReLU FNN with width 3d+3max{d⌊p1/d⌋,p+ 1}, and depth 12L+ 14 + 2d
such that
∥f−ϕ∥∞≤19√
dωf(p−2/dL−2/d),
whereωf(·)is the modulus of continuity.
Apply Lemma 3 to each component hiofΓn
d, we can find a neural network ˜hi∈FNN(1,L,˜p,M)such that
∥hi−˜hi∥∞≤CLΦε1,
whereL,˜p >0are integers such that Lp=⌈ε−dX/2
1⌉, and the constant Cdepends on dX. Assembling the
neural networks ˜hitogether, we can find a neural network ˜Γn
dinFNN(dY,L,p,M )withp=dY˜p, such that
∥˜Γn
d−Γn
d∥∞≤CLΦε1.
Similarly to the derivations in equation equation 31 and equation 32, we obtain that
T1≲L2
ΦdYε2
1+L2
ΦEu/bracketleftbig
∥Πn
X,dX(u)−u∥2
X/bracketrightbig
+σ2, (36)
where the notation ≲contains constants that depend on dXandLEn
Y. To deal with term T2, we apply the
following lemma concerning the covering number.
Lemma 4. [Lemma 10 in Liu et al. (2022)] Under the conditions of Theorem 2, we have
T2≤35dYR2
Y
nlogN/parenleftigg
δ
4dYLEn
YRY,FNN,2n/parenrightigg
+ 6δ.
Combining Lemma 4 with equation 36, we derive that
I≤CL2
ΦL2
Dn
YdYε2
1+ 16L2
Dn
YL2
En
YL2
ΦEu/bracketleftbig
∥Πn
X,dX(u)−u∥2
X/bracketrightbig
+ 12L2
Dn
Yσ2
+70L2
Dn
YdYR2
Y
nlogN/parenleftigg
δ
4dYLEn
YRY,FNN(dY,L,p,M ),2n/parenrightigg
+ 12L2
Dn
Yδ.(37)
By the definition of covering number (c.f. Definition 5 in Liu et al. (2022)), we first note that the covering
number ofFNN(dY,L,p,M )is bounded by that of FNN(1,L,p,M ):
N/parenleftigg
δ
4dYLEn
YRY,FNN(dY,L,p,M ),2n/parenrightigg
≤CedYN/parenleftigg
δ
4dYLEn
YRY,FNN(1,L,p,M ),2n/parenrightigg
.
Thus it suffices to find an estimate on the covering number of FNN(1,L,p,M ). A generic bound for classes
of functions is provided by the following lemma.
21Published in Transactions on Machine Learning Research (10/2023)
Lemma 5 (Theorem 12.2 of Anthony et al. (1999)) .LetFbe a class of functions from some domain Ωto
[−M,M ]. Denote the pseudo-dimension of FbyPdim(F). For anyδ>0, we have
N(δ,F,m )≤/parenleftbigg2eMm
δPdim(F)/parenrightbiggPdim(F)
(38)
form> Pdim(F).
The next lemma shows that for a DNN FNN(1,L,p,M ), its pseudo-dimension of can be bounded by the
network parameters.
Lemma 6 (Theorem 7 of Bartlett et al. (2019)) .For any network architecture FNNwithLlayers and U
parameters, there exists an universal constant Csuch that
Pdim(FNN)≤CLU log(U). (39)
For the network architecture FNN(1,L,p,M ), the number of parameters is bounded by U=Lp2. We apply
Lemma 5 and 6 to bound the covering number by its parameters:
logN/parenleftigg
δ
4dYLEn
YRY,FNN(dY,L,p,M ),2n/parenrightigg
≤C1dYp2L2log/parenleftbig
p2L/parenrightbig/parenleftigg
log/parenleftigg
R2
XdYLEn
YLΦ
L2p2log(Lp2)/parenrightigg
+ logδ−1+ logn/parenrightigg
,
(40)
when 2n>C 2p2L2log(p2L)for some universal constants C1andC2. Note that p,Lare integers such that
pL=/ceilingleftig
dYε−dX/2
1/ceilingrightig
, therefore we have
logN/parenleftigg
δ
4dYLEn
YRY,FNN(dY,L,p,M ),2n/parenrightigg
≲d3
Yε−dX
1log(dYε−1
1)/parenleftbig
logLΦ−log(dYε−1) + logδ−1+ logn/parenrightbig
,
(41)
where the notation ≲contains constants that depend on RX,dXandLEn
Y.
Substituting the above covering number estimate back to equation 37 gives
I≲L2
ΦdYε2
1+L2
ΦEu/bracketleftbig
∥Πn
X,dX(u)−u∥2
X/bracketrightbig
+σ2
+L2
Φn−1d4
Yε−dX
1log(dYε−1
1)/parenleftbig
logLΦ−log(dYε−1) + logδ−1+ logn/parenrightbig
+δ,
where the notation ≲contains constant that depends on LEn
Y,LDn
Y,LEn
X,LDn
X,RXanddX. Letting
ε1=d3
2+dX
Yn−1
2+dX,δ=n−1,
we have
I≲L2
Φd8+dX
2+dX
Yn−2
2+dX+L2
ΦEu/bracketleftbig
∥Πn
X,dX(u)−u∥2
X/bracketrightbig
+ (σ2+n−1) +L2
Φlog(LΦ)d8+dX
2+dX
Yn−2
2+dXlog (n)
≲L2
Φlog(LΦ)d8+dX
2+dX
Yn−2
2+dXlogn+ (σ2+n−1) +L2
ΦEu/bracketleftbig
∥Πn
X,dX(u)−u∥2
X/bracketrightbig
,(42)
where ≲contains constants that depend on LEn
Y,LDn
Y,LEn
X,LDn
X,RXanddX. Combining our estimate
equation 42 and equation 25, we have
ESEu∥Dn
Y◦ΓNN◦En
X(u)−Ψ(u)∥2
Y≲L2
Φlog(LΦ)d8+dX
2+dX
Yn−2
2+dXlogn+ (σ2+n−1)
+L2
ΦEu/bracketleftbig
∥Πn
X,dX(u)−u∥2
X/bracketrightbig
+ESEw∼Φ#γ/bracketleftbig
∥Πn
Y,dY(w)−w∥2
Y/bracketrightbig
.
22Published in Transactions on Machine Learning Research (10/2023)
Proof of Theorem 3. Under Assumption 5, the target finite dimensional map becomes Γn
d:=EY◦Φ◦DX:
M → RdY, which is a Lipschitz map defined on M ⊂ RdX. Similar to the proof of Theorem 2, the
generalization error is decomposed as the following
ESEu∥DY◦ΓNN◦EX(u)−Φ(u)∥2
Y≤T1+T2+ II, (43)
whereT1,T2andIIare defined in equation 26 and equation 25 respectively. Following the same procedure
in equation 27, we obtained that
T1≤4 inf
Γ∈FNNEu/bracketleftbig
∥Γ◦En
X(u)−En
Y◦Φ(u)∥2
2/bracketrightbig
+ 6ES21
n2n/summationdisplay
i=n+1∥εi∥2
Y.
We then replace Lemma 3 by the following modified version of lemma 17 from Liu et al. (2022) to obtain an
FNN approximation to Γn
d.
Lemma 7 (Lemma 17 in Liu et al. (2022)) .Suppose assumption 5 holds, and assume that ∥a∥∞≤Bfor
alla∈M. For any Lipschitz function fwith Lipschitz constant RonM, and any integers ˜L,˜p>0, there
exists ˜f∈FNN(1,L,p,M )such that
∥˜f−f∥∞≤CR˜L−2
d0˜p−2
d0,
where the constant Csolely depends on d0,B,τand the surface area of M. The parameters of FNN(1,L,p,M )
are chosen as the following
L= Ω( ˜Llog˜L),p= Ω(dX˜plog ˜p),M=R.
The constants in Ωdepend ond0,B,τand the surface area of M.
Apply the above lemma to each component of EY◦Φ◦DXand assemble all individual neural networks
together, we obtain a neural network ˜Γn
d∈F(dY,L,p,M )such that
∥˜Γn
d−Γn
d∥∞≲LΦε,
Here the parameters L= Ω( ˜Llog˜L),p= Ω(dXdY˜plog ˜p),M= Ω(LΦ)with ˜L˜p= Ω(ε). The notation ≲
andΩcontains constants that solely depend on d0,RX,LEX,τand surface area of M. The rest of the proof
follows the same procedure as in proof of Theorem 2.
Proof of Theorem 4. The proof is similar to that of Theorem 2 with a slight change of the neural network
construction, so we only provide a brief proof below.
While Assumption 6 holds, the target map Φ :X∝⇕⊣√∫⊔≀→Ycan be decomposed as the following
Rd0 R
X RdX Rd0 R RdYY.
Rd0 Rg1
En
X ViV1
VdYgiDn
Y
gdY(44)
Notice that each route contains a composition of a linear function Viand a nonlinear map gi:Rd0→R.
The nonlinear function gican be approximated by a neural network with a size that is independent from
dX, while the linear functions Vican be learned through a linear layer of neural network. Consequently, the
functionhi:=Vi◦gican be approximated by a neural network ˜hi∈FNN(1,L+ 1,˜p,M)such that
∥hi−˜hi∥∞≤CLΦε
23Published in Transactions on Machine Learning Research (10/2023)
whereL,˜p>0are integers with Lp=⌈ε−d0/2
1⌉, and the constant Cdepends on d0. Assembling the neural
networks ˜hitogether, we can find a neural network ˜Γn
dinFNN(dY,L+ 1,p,M )withp=dY˜p, such that
∥˜Γn
d−Γn
d∥∞≤CLΦε1.
The rest of the proof follows the same as in the proof of Theorem 2.
Proof of Theorem 5. The proof is very similar to that of Theorem 4. Under Assumption 7, the target map
Φhas the following structure:
Rd1 R Rd2 R
X RdX Rd1 R Rl1 Rd2 R Rl2··· RdYY.
Rd1 R Rd2 Rg1
1 g2
1
En
X V1
iV1
1
V1
l1g1
iV2
1
V2
i
V2
l2g2
iDn
Y
g1
l1g2
l2
(45)
where the abbreviation notation ···denotes blocks Gi,i= 3,...,Gk. The neural network construction
for each block Giis the same as in the proof of Theorem 4. Specifically, there exists a neural network
Hi∈FNN(li,L+ 1,li˜p,M)such that
∥Gi−Hi∥∞≤CLGiε1,for alli= 1,...,k.
Concatenate all neural networks Hitogether, we obtain the following approximation
∥Gk◦···◦G1−Hk◦···◦H1∥≤CLΦε1.
The rest of the proof follows the same as in the proof of Theorem 2.
A.2 Lipschitz constant of parameter to solution map for Parametric elliptic equation
The solution uto equation 24 is unique for any given boundary condition fso we can define the solution
map:
Sa:f∈H1∝⇕⊣√∫⊔≀→u∈H3/2.
To obtain an estimate of the Lipschitz constant of the parameter-to-solution map Φ, we compute the Frechét
derivativeDSa[δ]with respect to aand derive an upper bound of the Lipschitz constant. It can be shown
that the Frechét derivative is
DSa[δ] :f∝⇕⊣√∫⊔≀→vδ,
wherevδsatisfies the following equation
/braceleftigg
−div(a(x)∇xvδ(x)) =div(δ∇u),inΩ,
vδ= 0,on∂Ω.
The above claim can be proved by using standard linearization argument and adjoint equation methods.
Using classical elliptic regularity results, we derive that
∥vδ∥H3/2≤C∥div(δ∇u)∥H−1/2
≤C∥δ∥L∞∥u∥H3/2≤C∥δ∥L∞∥f∥H1,
whereCsolely depends on the ambient dimension d= 2andα,β. Therefore, the Lipschitz constant is
C∥f∥H1.
24