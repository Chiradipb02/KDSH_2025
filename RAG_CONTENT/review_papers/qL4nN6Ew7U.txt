Fantasy: Transformer Meets Transformer in
Text-to-Image Generation
Anonymous Author(s)
Affiliation
Address
email
Abstract
We present Fantasy, an efficient text-to-image generation model marrying the 1
decoder-only Large Language Models (LLMs) and transformer-based masked im- 2
age modeling (MIM). While diffusion models are currently in a leading position in 3
this task, we demonstrate that with appropriate training strategies and high-quality 4
data, MIM can also achieve comparable performance. By incorporating pre-trained 5
decoder-only LLMs as the text encoder, we observe a significant improvement in 6
text fidelity compared to the widely used CLIP text encoder, enhancing the text- 7
image alignment. Our training approach involves two stages: 1) large-scale concept 8
alignment pre-training, and 2) fine-tuning with high-quality instruction-image data. 9
Evaluations on FID, HPSv2 benchmarks, and human feedback demonstrate the 10
competitive performance of Fantasy against state-of-the-art diffusion and autore- 11
gressive models. 12
1 Introduction 13
DALL¬∑E 2
FantasyPixart-Œ±SDV1.5
W√úRSTCHENParaDiffusion
Figure 1: Comparison of data usage,
training time and image quality. Colors
from dark to light represent parameters
increasing in size, and circles from small
to large indicate improvements in image
quality.Recent advances in text-to-image (T2I) models [ 3,5,12] 14
have become focal points within the computer vision field. 15
Most advances in T2I models, focused on generating high- 16
quality images based on relatively short descriptions, strug- 17
gle with intricate long-text semantic alignment due to in- 18
herent structure constraints and data limitations. Text 19
encoders used for T2I fall into three categories: CLIP 20
[30], encoder-decoder LLMs, and decoder-only LLMs. 21
Models using encoder-decoder LLMs like T5-XXL [ 31] 22
have shown improved text-image alignment over CLIP 23
by exploiting enhanced text understanding, increasing to- 24
ken capacity, yet without delving into the semantic align- 25
ment for longer texts. ParaDiffusion [ 43] indicates that 26
directly aligning text embeddings with visual features with- 27
out prior image-text knowledge is not the most effective 28
approach. Previous works [ 38,45] have highlighted short- 29
comings in existing text-image datasets [ 37], including 30
image-text mismatches, a lack of informative content, and 31
a pronounced long-tail effect. These deficiencies notably 32
impair training efficiency for T2I models and restrict their 33
ability to learn complex semantic alignment. 34
Existing diffusion-based T2I models [ 33,5,9,26] have achieved unprecedented quality. However, 35
as detailed in Fig. 1, these advanced models come with significant computational demands. The 36
Submitted to 38th Conference on Neural Information Processing Systems (NeurIPS 2024). Do not distribute.A snowy Sweden lake in a
vibrant, cinematic style with
intense detail and raytracing.
Afurry cat. Studio photo portrait of Lain
Iwakura wearing floral garlands
over her traditional dress.
A tiny planet image of Rio de
Janeiro.
A 3d render of a cute, blue,
anthropomorphic dragon with
ice crystals growing off her,
sharp focus.Majestic ornate great hall,
grand library, baroque, torches,
stained glass windows,
moonlight rays, dreamy mood.
The solitary great tree
centered in the image.
cloudless sunny sky. little
islands in the flooded plain.
Breath taking beautiful,
aesthetically pleasing, gouache
ocean waves ripples, sea foam,
sunset, digital concept art.
Beautiful warm tavern seen
from the outside, middle age,
river crossed by a bridge next
to the tavern, crepuscular light.
Ted bundy in a pixar movie.
Figure 2: Samples produced by Fantasy ( 512√ó512). Each image, generated in 1.26 seconds (without
super-resolution models), is accompanied by a descriptive caption showcasing diverse styles and
comprehension.
considerable expenses of these models create significant barriers for researchers and entrepreneurs. 37
Meanwhile, economical text-to-image models [ 25,15,48] compromise on image quality, yielding 38
lower resolution and diminished aesthetic appeal. 39
Given these challenges, a pivotal question arises: Can we develop a resource-efficient ,high-quality 40
image generator for long instructions? In this paper, we present Fantasy, significantly reducing 41
training demands while maintaining the capability of instruction understanding and competitive 42
image generation quality, as shown in Fig. 2. To achieve this, we propose three core designs: 43
Efficient T2I netwrok. To leverage the powerful understanding ability of a decoder-only LLM, 44
we choose the lightweight Phi-2 [ 24] as our text encoder. We derive discrete image tokens from a 45
pre-trained VQGAN [ 27], and employ Transformer-based masked image modeling (MIM) as our T2I 46
architecture. We also utilize the pre-trained VQGAN decoder [27] for pixel space restoration. 47
Hierarchical Training strategy. We propose a thoughtfully two-stage training strategy to address the 48
high computational demands of current leading models while maintaining competitive performance: 49
(1) large-scale concept alignment pre-training, (2) high-quality instruction-image fine-tuning. To 50
facilitate a coarse image-text alignment, we initially train the T2I model from scratch using relatively 51
lower-quality data. We then fine-tune the pre-trained T2I model and LLM on text-image pair data 52
rich in information density with superior aesthetic quality. 53
High-quality data. To achieve rough alignment while pre-training, we select the large-scale dataset 54
LAION-2B [ 37] and employ the filtering strategy proposed by DataComp [ 14]. We collect long- 55
text prompts and corresponding high-quality synthesized images for instruction tuning, including 56
DiffusionDB [ 42] and JourneyDB [ 39]. We further filter and discard texts with special characters and 57
data containing violence or pornography, retaining only instructions exceeding 30 words. 58
Our main contributions are summarized as follows: 59
1.We present Fantasy, a novel framework that is the first to integrate a lightweight decoder-only 60
LLM and a Transformer-based MIM for text-to-image synthesis, allowing for long-form 61
text alignment. 62
2.We show that our two-stage training strategy with high-quality data enables MIM to achieve 63
comparable performance at a significantly reduced training cost. 64
3.We provide comprehensive validation of the model‚Äôs efficacy based on automated metrics 65
and human feedback for visual appeal and text faithfulness. 66
2Masked Image 
Generatorùìî ùìì√óùëµ
ùìñ
‚ÄúAn owl character with high detail and 
dramatic lighting digital art headshot.‚Äù LLMCosine
Mask
Cross 
Attn
Super Resolution Model
Phi-2 ProjectionMasked Image Generator VQùììStage 1: Large-scale concept alignment pre-training Stage 2: Instruction fine-tuning
VQ ùìî
Phi-2 ProjectionMasked Image Generator VQùìì VQ ùìîFigure 3: (Up) Overview of Fantasy featuring text encoder, VQGAN (encoder Eand decoder D),
masked image generator G, and super-resolution model. ( Down ) Our training pipeline involves two
stages. The redparts are trainable and the blue parts are frozen; the yellow part is optionally
utilized during inference.
2 Method 67
2.1 Problem Formulation 68
As depicted in Fig. 3, Fantasy consists of a pre-trained text encoder T, a transformer-based masked 69
image generator G, a sampler S, a frozen VQGAN, and a pre-trained super-resolution model. T 70
maps a text prompt tto a continuous embedding space. Gprocesses a text embedding eto generate 71
logits lfor the visual token sequence. Sdraws a sequence of visual tokens vfrom logits via iterative 72
decoding [ 4], which runs Nsteps of inference conditioned on the text embeddings eand visual tokens 73
decoded from previous steps. Finally, Dmaps the sequence of discrete tokens to pixel space Z. To 74
summarize, given a text prompt t, an image ÀÜxis synthesized as follows: 75
ÀÜx=D(S(G,T(t))), ln=G(vn,T(t)), v n=M(E(x)) (1)
where nis the synthesis step, and lnare logits, from which the next set of visual tokens vn+1are 76
sampled. Mdenotes the masking operator that applies masks to the token in vn. We refer to [ 4,3] 77
for details on the iterative decoding process. The Phi-2 [ 24] forTand VQGAN [ 8] for encoder Eand 78
decoder Dare used. Gis trained on a large text-image pairs Dusing masked visual token modeling 79
loss: 80
L=E(x,t)‚àºD[CE(lN,E(x))], (2)
where CEis a weighted cross-entropy calculated by summing only over the unmasked tokens. 81
2.2 Model Architecture 82
2.2.1 VQGAN as Image Processor 83
VQGAN [ 8] is capable of transforming each image into discrete tokens with higher-level semantic 84
information from a learned codebook, while ignoring low level noise. The autoregressive tokens 85
prediction of VQGAN shares the same form as text tokens generated by LLMs. Prior research [ 46] 86
has shown that unifying vision and language by the same token space could enhance the coherency 87
for vision-text alignment. Furthermore, compared with RGB pixels, the visual token representation 88
has proven to reduce disk storage and improve the capability of robustness and generalization. 89
To reduce the computational burden, we initially compress an RGB image v‚ààRH√óW√ó3into a 90
diminished representation with a resolution of h√ów√ó3, where h=H/f andw=W/f , with 91
fdenoting the downsampling factor. We then employ a pre-trained f16 VQGAN [ 27] encoder E 92
to quantizate images x‚ààR3√ó256√ó256into discrete tokens of spatial dimensions 16√ó16from a 93
pre-trained codebook Z={zk}K
k=1consisting of K= 8192 vectors, resulting in the quantized 94
representation z=E(x,Z). 95
32.2.2 LLM as Text Encoder 96
Recent studies [ 10,5,3] tend to use encoder-decoder LLMs [ 31] for text encoding over CLIP [ 30], 97
which is adept at handling tasks that involve complex mappings between input and output sequences. 98
Due to the tremendous success of ChatGPT, attention has been drawn to models that consist solely of a 99
decoder. Also, [ 43] presents an insight that efficiently fine-tuning a more powerful decoder-only LLM 100
can yield stronger performance in long-text alignment. Consequently, to capitalize on the enhanced 101
semantic comprehension and generalization potential of LLMs while simultaneously reducing the 102
training burden, we employ Phi-2 [24], a state-of-the-art, lightweight LLM, as the text encoder. 103
Given the text prompt t, Fantasy first passes it through Phi-2, extracting the text embedding from the 104
last hidden layer L. However, typically, decoder-only architectures are not adept at feature extraction 105
and mapping tasks. [ 23] proposes that the conceptual representations learned by LLM‚Äôs are roughly 106
linearly mappable to those learned by models trained on vision tasks. Therefore, the embedding 107
vectors are linearly projected to the hidden size of the image generator G: 108
c=P(TL(t)) (3)
where T(¬∑)denotes the decoder-only Phi-2 and Lis the index of the last hidden layer. Prepresents 109
the projection from text space to visual space, and cis the text feature suitable for the image generator. 110
2.2.3 MIM as Image Generator 111
MIM narrows the gap between its modeling and the extensively studied area of language modeling, 112
making it straightforward to leverage the findings of the LLMs research community. Therefore, we 113
adopt a masked transformer as the image generator backbone of Fantasy [46]. 114
During training, we leave the projected text embeddings cunmasked and the image tokens zare 115
masked at a variable masking rate based on a Cosine scheduling Mas [4,3]. Specifically, for 116
each training example, we sample a masking rate rfrom [0,1]from a truncated arccos distribution 117
with density function p(r) =2
œÄ(1‚àír2)‚àí1
2. While autoregressive methods learn fixed-order token 118
distributions P(zi|z<i), random masking with variable ratios enables learning P(zi|zÃ∏=i)for any 119
token subset, crucial for our parallel sampling scheme. The sampling of a new state sn+1at each 120
successive step is conditioned on the previous state and the specified text condition c: 121
P(s|c) =Z
P(sN|sN‚àí1, c)N‚àí1Y
n=1P(sn|sn‚àí1, c)ds1. . . ds N‚àí1 (4)
For each training example, the most confidently predicted tokens are revealed at each step n, main- 122
taining cos n
N¬∑œÄ
2
masked until reaching Ntotal steps. 123
For the base model, we use a variant of MaskGiT [ 4], a masked image generative Transformer to 124
predict randomly masked tokens by attending to tokens in all directions. Leveraging the multi-layered 125
structure of the Transformer, we have developed scalable image generators with varying layer counts, 126
ranging in size from 257M parameters to 611M parameters (for the image generator; the Phi-2 model 127
has an additional 2.7B parameters). We first employ a series of Cross Attention blocks to optimize 128
text-driven feature extraction, before passing through Olayers of the masked image generator. Each 129
layer oof the Transformer is again formed by Multi-Head Self-Attentuib(MSA), LayerNorm (LN), 130
Cross Attention (CA) and Multi-Layer Perceptron (MLP) blocks: 131
Yo=MSA (LN(Zo)), Z o+1=MLP (CA((LN(Yo), c))). (5)
At the output layer, to reduce the training burden, ConvMLP [ 18] is utilized to transform masked 132
image embeddings into logits sets, aligning with the VQGAN codebook dimensions. Eventually, the 133
reconstructed lower-resolution tokens are restored with the pre-trained 256√ó256resolution VQGAN 134
decoder to the pixel space, resulting in the generated image ÀÜx: 135
ÀÜx=D(ConvMLP (ZO),Z) (6)
2.3 Training Strategy 136
Fig. 3 illustrates Fantasy‚Äôs two-stage training approach. Following prior works[ 43,35,9], we employ 137
large-scale pre-training to achieve general text-image concept alignment, and simultaneous fine-tuning 138
of Phi-2 [24] and the masked image generator using high-quality instruction-image pairs. 139
4Pre-training Stage. To perform general text-image concept alignment, the VQGAN and LLM 140
weights are frozen, and only the image generator is pre-trained on deduplicated LAION-2B [ 37] 141
with images above a 4.5 aesthetic score. We exclusively preserve prompts in English, filter out 142
images above a 50% watermark probability or above a 45% NSFW probability, yielding a final set 143
of 9 million images. Since the computational cost of upsampling is much lower than training a 144
super-resolution model, Fantasy is started with training at a resolution of 256√ó256. Note that the 145
pre-training only needs approximate image-text alignment, substantially lowering the training costs. 146
Fine-tuning Stage. [43] has proven that LLMs trained solely on text data lack prior image-text 147
knowledge, and that merely aligning their text embeddings with visual features might not be optimal. 148
Therefore, in the second stage, we gather an internal dataset of 7 million high-quality instruction- 149
image pairs to fine-tune both the Phi-2 model and the image generator of Fantasy, which ensures 150
enhanced compatibility of text embeddings within the text-image pair space, facilitating the use of 151
decoder-only LLMs in text-to-image generation tasks and harnessing their inherent advantages. To 152
prevent catastrophic forgetting in LLMs and preserve their understanding abilities during training, we 153
select questions from BIG-bench [ 2] and monitor the common sense question-answering ability of 154
Phi-2 in real-time throughout the training process. We construct our training dataset for the fine-tuning 155
stage by incorporating JourneyDB [ 39] and an internal synthetic dataset to enhance the aesthetic 156
quality of generated images beyond realistic photographs. To facilitate instruction-image alignment 157
learning, we retain only data with descriptions exceeding 30 words, as these provide enough detailed 158
insights into the image objects, including attributes and spatial relations. 159
With this approach, Fantasy trains a 0.6B parameter T2I model in about 69 A100 GPU days, 160
significantly reducing computation compared to existing diffusion-based methods, while maintaining 161
comparable visual and numerical fidelity. Throughout this paper, we present a comprehensive 162
evaluation of Fantasy‚Äôs efficacy, showcasing the potential in training high-quality transformer-based 163
image synthesis models compared to diffusion-based models in future. 164
2.4 High-quality Data Collection 165
To ensure rough alignment in the pre-training phase, we utilize the large-scale dataset LAION-2B 166
[37] and apply the filtering strategy developed by DataComp [ 14]. Furthermore, we gather long- 167
text prompts and corresponding high-quality images to achieve finer-grained text-image alignment 168
through instruction tuning. CapsFusion [ 47] employs a fine-tuned LLaMA [ 40] for recaptioning 169
LAION-2B [ 37] and LAION-COCO [ 1]. However, this approach still results in suboptimal image 170
quality and occasional mismatches between images and text. SAM-LLA V A [ 5] utilizes LLaV A [ 20] 171
to recaption the SAM dataset [ 17], which leads to images with blurred faces, a consequence of the 172
dataset‚Äôs inherent face-blurring. Therefore, we shift focus to synthesize images, mainly including 173
DiffusionDB [ 42] and JourneyDB [ 39], produced by Stable Diffusion and MidJourney, respectively. 174
To augment the diversity of the images, we minimize the use of datasets from specific domains, such 175
as gaming and anime. Furthermore, we implement filtering to discard texts with special characters 176
and data containing violence or pornography, retaining only instructions exceeding 30 words. 177
3 Experiments 178
In this section, we outline detailed training, inference, and evaluation protocols, followed by compre- 179
hensive comparisons across three key metrics. 180
3.1 Implementation Details 181
Training Details. Different from the prior works [ 9,43,32,34], we used a lightweight but powerful 182
decoder-only large language model Phi-2 [ 24] as the text encoder. Diverging from prior approaches 183
that extract a standard and fixed short text tokens, we extend the extraction to 256 tokens to master 184
long-term instruction-image alignment, ensuring precise alignment for more fine-grained prompts. 185
For the entire training process, we train Fantasy on 4√óA100 80G GPUs and set the accumulation 186
step to 2. At different stages, we employ varying learning rate strategies with single-cycle cosine 187
annealing decay. Furthermore, the AdamW optimizer [ 22] is utilized with a weight decay of 0.01. 188
Fantasy trains a 0.6B parameter T2I model in about 84.5 A100 GPU days, significantly reducing 189
computation compared to existing diffusion-based methods as shown in Fig. 1. 190
5Table 1: Evaluation of diffusion (upper) and transformer (down) models on HPSv2. We underline the
highest value and color the first above Fantasy in blue .
Model Type Params Animation Concept-art Painting Photo DrawBench [36]
GLIDE [25] Diff 5.0B 23.34¬±0.198 23 .08¬±0.174 23 .27¬±0.178 24 .50¬±0.290 25 .05¬±0.84
VQ-Diffusion [15] Diff 0.37B 24.97¬±0.186 24 .70¬±0.149 25 .01¬±0.145 25 .71¬±0.222 25 .44¬±0.83
Latent Diffusion [34] Diff 1.45B 25.73¬±0.125 25 .15¬±0.140 25 .25¬±0.178 26 .97¬±0.183 26 .17¬±0.85
DALL¬∑E 2 [26] Diff 6.5B 27.34¬±0.175 26 .54¬±0.127 26 .68¬±0.156 27.24¬±0.198 27.16¬±0.64
Stable Diffusion v1.4 [33] Diff 0.8B 27.26¬±0.156 26 .61¬±0.082 26 .66¬±0.143 27 .27¬±0.226 27 .23¬±0.57
Stable Diffusion v2.0 [33] Diff 0.8B 27.48¬±0.174 26 .89¬±0.076 26.86¬±0.120 27.46¬±0.198 27 .31¬±0.68
DeepFloyd-XL [11] Diff 4.3B 27.64¬±0.108 26.83¬±0.137 26.86¬±0.131 27.75¬±0.171 27.64¬±0.72
LAFITE [48] Trans 0.075B 24.63¬±0.101 24 .38¬±0.087 24 .43¬±0.155 25 .81¬±0.213 25 .23¬±0.72
FuseDream [21] Trans - 25.26¬±0.125 25 .15¬±0.107 25 .13¬±0.183 25 .57¬±0.248 25 .72¬±0.71
DALL¬∑E mini [7] Trans 0.4B 26.10¬±0.132 25 .56¬±0.137 25 .56¬±0.112 26 .12¬±0.233 26 .34¬±0.76
VQGAN + CLIP [8] Trans 0.2B 26.44¬±0.152 26 .53¬±0.075 26 .47¬±0.111 26 .12¬±0.210 26 .38¬±0.43
CogView2 [12] Trans 6B 26.50¬±0.129 26 .59¬±0.119 26 .33¬±0.100 26 .44¬±0.271 26 .17¬±0.74
Fantasy (ours) Trans 0.6B 27.03¬±0.131 26.66 ¬±0.117 26.72 ¬±0.176 26.80 ¬±0.174 26.78 ¬±0.523
Table 2: Comparison with recent T2I models. ‚ÄòTrained‚Äô indicates the model develops a text encoder
from scratch, foregoing a pre-trained one.
Method Type Text Encoder #Params #Images FID-30K ( ‚Üì)
LDM [34] Diff Trained 1.4B 400M 12.64
GLIDE [25] Diff Trained 5.0B - 12.24
DALL¬∑E 2 [26] Diff CLIP 6.5B 650M 10.39
Stable Diffusion v1.5 [33] Diff CLIP 0.9B 2000M 9.62
SD XL [29] Diff CLIP 2.6B - >18
W√ºrstchen [28] Diff CLIP 0.99B 1420M 23.6
ParaDiffusion [43] Diff LLaMA V2 1.3B >300M 9.64
Pixart- Œ±[5] Diff T5 0.6B - 5.51
Cogview2 [12] Trans CogLM 6B 35M 24.0
Muse [3] Trans T5-XXL 3B 460M 7.88
Fantasy Trans Phi-2 0.6B 16M 23.4
Inference Details. We use N= 32 sampling steps in all of our evaluation experiments. Since 191
Fantasy is trained at a resolution of 256√ó256, we employ the pre-trained diffusion-based super- 192
resolution model StableSR [41] to upscale images to 512√ó512. 193
Evaluation Metrics. We comprehensively evaluate Fantasy via four primary metrics, i.e., alignment 194
on HPSv2 [44], FID [16] on MSCOCO dataset [19] and human evaluation on a collected dataset. 195
3.2 Performance Comparisons and Analysis 196
Results on HPSv2. We utilize HPSv2 [ 44] as our primary automated metric, a preference prediction 197
model which can be used to compare images generated with the same prompt across five categories: 198
anime, concept art, paintings, photography, and DrawBench [ 36]. We present the results of HPSv2 199
between Fantasy and other state-of-the-art generative models in Tab. 1. Fantasy exhibited outstanding 200
performance across all key aspects among previous Transformer-based methods like CogView2 201
[12], which is expected. The results also reveal its competitive performance compared to prior 202
diffusion-based methods, especially in concept-art and painting, demonstrating similar performance 203
to DALL ¬∑E 2 [ 26]. This remarkable performance is primarily attributed to the text-image alignment 204
learning in fine-tuning stage, where high-quality text-image pairs were leveraged to achieve superior 205
alignment capabilities. In comparison, DeepFloyd-XL and other diffusion-based models achieve 206
better scores, while utilizing larger models with significantly higher compute budget. 207
Results on FID. We employ FID [ 16] to evaluate our models on COCO-30K [ 19]. To allow for 208
a fair comparison, all images are downsampled to 256√ó256pixels. The comparison between our 209
method and other methods in FID, and their training time is summarized in Tab. 2. We observe 210
that the FID of Fantasy is substantially higher compared to other state-of-the-art models. Visual 211
inspections reveal that images generated by Fantasy are smoother than those from other leading T2I 212
models. This discrepancy is most noticeable in real-world images like COCO, on which we compute 213
the FID-metric. Although the state-of-the-art models [ 43,11,29] exhibit lower FID, it relies on 214
unaffordable resources. Furthermore, prior studies [ 29,5,11] have demonstrated that FID may not 215
6Visual Appeal Text-image Alignment
(a) User study on long prompts.
Visual Appeal Text-image Alignment
 (b) User study on short prompts.
Figure 4: User study on prompts with different length. VC. , CV2. , FT. , SD. , and PA. refer to
VQGAN+CLIP [8], CogView2 [12], our Fantasy, Stable Diffusion v2.0 [33], and Pixart- Œ±[5].
be an appropriate metric for image quality evaluation, as a lower score does not necessarily reflect 216
superior image generation, and it is more authoritative to use the evaluation of human users. 217
3.3 Results on Human Evaluation 218
Following prior works [ 5,43,28], we also conduct a study with human participants to supplement 219
our evaluation and provide a more intuitive assessment of Fantasy‚Äôs performance. Participants are 220
asked to select a preference of the images based on the visual appeal of the generated images and the 221
precision of alignments between the text prompts and the corresponding images. 222
As involving human evaluators can be time-consuming, we choose the top-performing open-source 223
diffusion-based models (e.g., SD XL [ 33], and Pixart- Œ±[5]) and transformer-based models (e.g., 224
VQGAN+CLIP [ 8] and CogView2 [ 12]) as our baseline, which are accessible through APIs and 225
capable of generating images. We randomly select a total of 600 prompts from existing prompt 226
sets (e.g., ParaPrompt [ 43], ViLG-300 [ 13], COCO Captions [ 6]). To comprehensively contrast the 227
capabilities of Fantasy and other models in interpreting text prompts of varying lengths, we allocate 228
one subset to consist of 300 prompts ranging from 10 to 30 characters and another subset comprising 229
300 prompts exceeding 30 characters. For each model, we use a consistent set to generate images, 230
which are then evaluated by 50 individuals. 231
Fig. 4a clearly demonstrates that images generated on relatively long text prompts (longer than 30 232
words) by Fantasy are distinctly favored among the four models in both two perspective, especially 233
for text-image alignment, aligning closely with the intended use case of Fantasy. As illustrated 234
in Fig. 4b, for text prompts shorter than 30 words, our model outperforms existing open-source 235
Transformer-based models in fidelity and alignment for shorter prompts. Our model slightly lags 236
behind diffusion-based models in visual appeal, limited by the 8,192 size of VQGAN‚Äôs codebook 237
and not targeting visual appeal. Simultaneously, Fantasy lacks a distinct advantage in text-image 238
alignment in the short subset. We hypothesize that this is due to two main reasons: diffusion- 239
based models‚Äô ability to handle shorter prompts, and vague prompts generating diverse images that 240
make preferences more subjective, thus biasing outcomes towards aesthetically superior images. In 241
summary, the human preference experiments confirm the observation made in the HPSv2 benchmarks. 242
3.4 Case Study 243
A close-up photo of a person. The
subject is a male. He was wearing a
wide-brimmed hat, a gray-white beard
on his face, a brown coat. His facial
expression looked pensive and serious,
with the clear blue sky in the
background.
Fantasy
 ParaDiffusion
A young man wearing a black leather
jacket and tie stood behind an old door,
his gaze firmly fixed on the camera.
The door had patterns of leaves and
flowers on it, revealing a yellow
background. His hair was casually
curled and he appeared to be deep in
thought or contemplating something.
 Fantasy
 ParaDiffusion
Figure 6: Visual Comparison with ParaDiffusion [ 43]:
Red markings and boxes highlight text misalign-
ments in images generated by ParaDiffusion.Fig. 5 vividly illustrates Fantasy‚Äôs supe- 244
rior visual appeal and text-image alignment 245
over leading open-source transformer-based 246
T2I models [ 12,8] and diffusion-based T2I 247
models [ 29,26]. Fantasy significantly sur- 248
passes existing transformer-based T2I mod- 249
els, matches the performance of SDXL [ 29], 250
and qualitatively outperforms Dall ¬∑E 2 [ 26]. 251
Despite being trained on images with a res- 252
olution of 256√ó256, Fantasy ensures gener- 253
ated low-resolution images contain sufficient 254
details, indirectly supporting long prompts. 255
Limited by computing resources, we haven‚Äôt 256
7VQGAN+CLIP Cogview2 Fantasy SD XL Pixart-ùõº
(a) (c) (g) (e) (d) (f) (b)
Figure 5: Visual comparison with existing T2I models. (a) A hamster resembling a horse. (b)A
frontal portrait of a anime girl with chin length pink hair wearing sunglasses and a white T-shirt
smiling. (c)A colorful illustration of a suburban neighborhood on an ancient post-apocalyptic planet
featuring creatures made by Jim Henson‚Äôs workshop. (d)A blue-haired girl with soft features stares
directly at the camera in an extreme close-up Instagram picture. (e)A building in a landscape by
Ivan Aivazovsky. (f)Aoshima‚Äôs masterpiece depicts a forest illuminated by morning light. (g)The
image is a highly detailed portrait of an oak in GTA V, created using Unreal Engine and featuring
fantasy artwork by various artists.
Table 3: Ablation study on two stages with the best bolded. ‚ÄòBase‚Äô indicates the model after the
pre-training stage.
Model Training Part Animation Concept-art Painting Photo DrawBench [36]
Base MIM 25.27¬±0.190 24 .20¬±0.166 24 .60¬±0.146 25 .32¬±0.208 25 .49¬±0.230
Fantasy MIM+Phi-2 27.03¬±0.131 26.66 ¬±0.117 26.72 ¬±0.176 26.80 ¬±0.174 26.78 ¬±0.521
trained on higher resolutions like 512√ó512but aim to enhance Fantasy by training at higher 257
resolutions in the future. 258
ParaDiffusion [ 43] pioneers the use of decoder-only large language models as text encoders in 259
text-to-image generation. As illustrated in Fig. 6, our observations suggest that Fantasy more closely 260
aligns details with prompts than ParaDiffusion [43]. 261
4 Ablation Study 262
This section analyzes the effects of LLMs fine-tuning, and model scale on Fantasy‚Äôs performance 263
through ablation studies. More ablation study refers to appendix. 264
4.1 Effect of Language Model Fine-tuning 265
To assess the effect of training strategies on the comprehension of complex instructions, we perform 266
a human preference evaluation, as detailed in Sec. 3.3, using a subset of 300 prompts longer than 267
30 characters. ‚ÄòBase‚Äô denotes general text-image alignment with filtered LAION-2B [ 1] in the 268
pre-training stage. Compared to the base model, our synergy fine-tuning with Phi-2 demonstrates a 269
notable improvement in all aspects in Tab. 3. 270
8Table 4: Ablation study on models at different scales with the
bestbolded . DB. represents DrawBench [36].
Layers Param Animation Concept-art Painting Photo DB.
6 257M 25.79¬±0.15 25.84 ¬±0.11 25.92 ¬±0.19 25.63 ¬±0.18 25.18 ¬±0.22
12 421M 26.34¬±0.17 26.29 ¬±0.06 26.45 ¬±0.17 26.19 ¬±0.17 25.68 ¬±0.14
22 611M 27.03¬±0.13 26.66 ¬±0.11 26.72 ¬±0.17 26.80 ¬±0.17 26.78 ¬±0.52Table 5: Training cost for Fantasy at
3 different scales. BS. denotes batch
size and LR. denotes learning rate.
Layers Pre-training Fine-tuning
Steps (K) BS. LR. Steps (K) BS. LR.
6 180 768 1e-4 180 192 1e-4
12 220 768 1e-4 250 192 1e-4
22 370 256 5e-4 280 128 3e-4271
4.2 Scale of Image Generator 272
L6 L12 L22
A wooden outhouse sitting in the grass near trees.
A small kitchen does have plenty of cabinets.
L6 L12 L22
Figure 7: Examples generated by mod-
els at different scales: 1stcolumn for 6
layers, 2ndcolumn for 12 layers and 3rd
column for 22 layers.The hierarchical structure of the Transformer allows us 273
to train image generators with varying numbers of Trans- 274
former layers. As shown in Tab. 4, we evaluate models 275
of different sizes on the HPSv2 benchmark. The insight 276
indicates that as trainable parameters increase from 257 277
million to 611 million, performance consistently improves. 278
Therefore, we set the number of Transformer layers to 22 279
with 611 million trainable parameters as the optimal set- 280
ting. Tab. 5 showcases the required resources for models 281
of three different scales. Fig. 7 offers visual comparisons 282
across models of varying scales, illustrating a clear trend: 283
models with fewer parameters underperform on the HPSv2 284
benchmark, frequently resulting in distorted images and 285
omitted details, yet they may still generate acceptable 286
outcomes. Significantly, the visual quality diverges as 287
model size increases, highlighting the potential for scaling 288
up masked image modeling to enhance instruction-image 289
alignment and elevate generation quality. 290
5 Limitations and Social Impact 291
Limitations. Despite Fantasy achieving competitive performance in text-image alignment and visual 292
appeal, it requires improvements in handling complex scenes. We propose two possible strategies to 293
overcome the challenge in future research: Firstly, augmenting the dataset with high-quality images 294
can enhance diversity and refine the model. Secondly, since the scale of the masked image generator 295
affects instruction-image alignment, training an upscale image generator based on higher resolution 296
left further explored. 297
Social Impact. Generative models for media bring both benefits and challenges. They foster creativity 298
and make technology more accessible, yet pose risks by facilitating the creation of manipulated 299
content, spreading misinformation, and exacerbating biases, particularly affecting women with deep 300
fakes. Concerns also include the potential exposure of sensitive training data collected without 301
consent. Despite generative models potentially offering better data representation, the impact of 302
combining adversarial training with likelihood-based objectives on data distortion remains a crucial 303
research area. Ethical considerations of these models are significant and require thorough exploration. 304
6 Conclusion 305
In this paper, we introduce Fantasy, a lightweight and efficient text-to-image model that combines 306
Large Language Models (LLMs) with a transformer-based masked image modeling (MIM), effec- 307
tively transferring semantic understanding capabilities from LLMs to the text-to-image generation. 308
With our proposed two-stage training strategy and high-quality dataset, Fantasy significantly re- 309
duces computational requirements while producing high-fidelity images. Extensive experiments 310
demonstrate that Fantasy achieves comparable performance to models trained with significantly more 311
computational resources, illustrating the viability of our approach and suggesting potential efficient 312
scalability to even larger masked image modeling for text-to-image generation. 313
9References 314
[1]K√∂pf Andreas, Vencu Richard, Coombes Theo, and Beaumont Romain. Laion coco: 600m synthetic 315
captions from laion2b-en.[eb/ol], 2022. 316
[2]BIG bench authors. Beyond the imitation game: Quantifying and extrapolating the capabilities of language 317
models. Transactions on Machine Learning Research , 2023. 318
[3]Huiwen Chang, Han Zhang, Jarred Barber, AJ Maschinot, Jose Lezama, Lu Jiang, Ming-Hsuan Yang, 319
Kevin Murphy, William T Freeman, Michael Rubinstein, et al. Muse: Text-to-image generation via masked 320
generative transformers. arXiv preprint arXiv:2301.00704 , 2023. 321
[4]Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William T Freeman. Maskgit: Masked generative image 322
transformer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , 323
pages 11315‚Äì11325, 2022. 324
[5]Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Yue Wu, Zhongdao Wang, James 325
Kwok, Ping Luo, Huchuan Lu, et al. Fast training of diffusion transformer for photorealistic text-to-image 326
synthesis. arXiv preprint arXiv:2310.00426 , 2023. 327
[6]Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Piotr Doll√°r, and 328
C Lawrence Zitnick. Microsoft coco captions: Data collection and evaluation server. arxiv 2015. arXiv 329
preprint arXiv:1504.00325 , 2015. 330
[7]Craiyon. Dall ¬∑e mini: Generate images from any text prompt. https://wandb.ai/dalle-mini/ 331
dalle-mini/reports/DALL-E-mini-Generate-images-from-any-text-prompt--VmlldzoyMDE4NDAy , 332
2023. Accessed: 2024-02-27. 333
[8]Katherine Crowson, Stella Biderman, Daniel Kornis, Dashiell Stander, Eric Hallahan, Louis Castricato, 334
and Edward Raff. Vqgan-clip: Open domain image generation and editing with natural language guidance. 335
InEuropean Conference on Computer Vision , pages 88‚Äì105. Springer, 2022. 336
[9]Xiaoliang Dai, Ji Hou, Chih-Yao Ma, Sam Tsai, Jialiang Wang, Rui Wang, Peizhao Zhang, Simon 337
Vandenhende, Xiaofang Wang, Abhimanyu Dubey, et al. Emu: Enhancing image generation models using 338
photogenic needles in a haystack. arXiv preprint arXiv:2309.15807 , 2023. 339
[10] Deepfloyd. Deepfloyd. https://www.deepfloyd.ai/ , 2023. 340
[11] DeepFloyd. IF-I-XL-v1.0: A model by deepfloyd on hugging face models. https://huggingface.co/ 341
DeepFloyd/IF-I-XL-v1.0 , 2023. Accessed: 2024-02-28. 342
[12] Ming Ding, Wendi Zheng, Wenyi Hong, and Jie Tang. Cogview2: Faster and better text-to-image generation 343
via hierarchical transformers. Advances in Neural Information Processing Systems , 35:16890‚Äì16902, 344
2022. 345
[13] Zhida Feng, Zhenyu Zhang, Xintong Yu, Yewei Fang, Lanxin Li, Xuyi Chen, Yuxiang Lu, Jiaxiang 346
Liu, Weichong Yin, Shikun Feng, et al. Ernie-vilg 2.0: Improving text-to-image diffusion model with 347
knowledge-enhanced mixture-of-denoising-experts. In Proceedings of the IEEE/CVF Conference on 348
Computer Vision and Pattern Recognition , pages 10135‚Äì10145, 2023. 349
[14] Samir Yitzhak Gadre, Gabriel Ilharco, Alex Fang, Jonathan Hayase, Georgios Smyrnis, Thao Nguyen, 350
Ryan Marten, Mitchell Wortsman, Dhruba Ghosh, Jieyu Zhang, et al. Datacomp: In search of the next 351
generation of multimodal datasets. Advances in Neural Information Processing Systems , 36, 2024. 352
[15] Shuyang Gu, Dong Chen, Jianmin Bao, Fang Wen, Bo Zhang, Dongdong Chen, Lu Yuan, and Baining Guo. 353
Vector quantized diffusion model for text-to-image synthesis. In Proceedings of the IEEE/CVF Conference 354
on Computer Vision and Pattern Recognition , pages 10696‚Äì10706, 2022. 355
[16] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans 356
trained by a two time-scale update rule converge to a local nash equilibrium. Advances in neural information 357
processing systems , 30, 2017. 358
[17] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete 359
Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al. Segment anything. arXiv preprint 360
arXiv:2304.02643 , 2023. 361
[18] Jiachen Li, Ali Hassani, Steven Walton, and Humphrey Shi. Convmlp: Hierarchical convolutional mlps for 362
vision. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 363
6306‚Äì6315, 2023. 364
[19] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll√°r, 365
and C Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer Vision‚ÄìECCV 2014: 366
13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13 , pages 367
740‚Äì755. Springer, 2014. 368
[20] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning, 2023. 369
[21] Xingchao Liu, Chengyue Gong, Lemeng Wu, Shujian Zhang, Hao Su, and Qiang Liu. Fusedream: 370
Training-free text-to-image generation with improved clip+ gan space optimization. arXiv preprint 371
arXiv:2112.01573 , 2021. 372
[22] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint 373
arXiv:1711.05101 , 2017. 374
10[23] Jack Merullo, Louis Castricato, Carsten Eickhoff, and Ellie Pavlick. Linearly mapping from image to text 375
space. arXiv preprint arXiv:2209.15162 , 2022. 376
[24] Microsoft. Phi-2. https://huggingface.co/microsoft/phi-2 , 2023. 377
[25] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya 378
Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with text-guided 379
diffusion models. arXiv preprint arXiv:2112.10741 , 2021. 380
[26] OpenAI. Dall-e 2. https://openai.com/dall-e-2 , 2022. 381
[27] Suraj Patil, William Berman, Robin Rombach, and Patrick von Platen. amused: An open muse reproduction. 382
arXiv preprint arXiv:2401.01808 , 2024. 383
[28] Pablo Pernias, Dominic Rampas, Mats Leon Richter, Christopher Pal, and Marc Aubreville. W√ºrstchen: 384
An efficient architecture for large-scale text-to-image diffusion models. In The Twelfth International 385
Conference on Learning Representations , 2023. 386
[29] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas M√ºller, Joe Penna, 387
and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. arXiv 388
preprint arXiv:2307.01952 , 2023. 389
[30] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish 390
Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from 391
natural language supervision. In International conference on machine learning , pages 8748‚Äì8763. PMLR, 392
2021. 393
[31] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, 394
Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. 395
The Journal of Machine Learning Research , 21(1):5485‚Äì5551, 2020. 396
[32] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional 397
image generation with clip latents. arXiv preprint arXiv:2204.06125 , 1(2):3, 2022. 398
[33] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj√∂rn Ommer. High-resolution 399
image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer 400
Vision and Pattern Recognition (CVPR) , pages 10684‚Äì10695, June 2022. 401
[34] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj√∂rn Ommer. High-resolution 402
image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer 403
vision and pattern recognition , pages 10684‚Äì10695, 2022. 404
[35] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed 405
Ghasemipour, Burcu Karagol Ayan, S Sara Mahdavi, Rapha Gontijo Lopes, et al. Photorealistic text-to- 406
image diffusion models with deep language understanding, 2022. URL https://arxiv. org/abs/2205.11487 , 407
4. 408
[36] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar 409
Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to- 410
image diffusion models with deep language understanding. Advances in Neural Information Processing 411
Systems , 35:36479‚Äì36494, 2022. 412
[37] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, 413
Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scale 414
dataset for training next generation image-text models. Advances in Neural Information Processing 415
Systems , 35:25278‚Äì25294, 2022. 416
[38] Eyal Segalis, Dani Valevski, Danny Lumen, Yossi Matias, and Yaniv Leviathan. A picture is worth a 417
thousand words: Principled recaptioning improves image generation. arXiv preprint arXiv:2310.16656 , 418
2023. 419
[39] Keqiang Sun, Junting Pan, Yuying Ge, Hao Li, Haodong Duan, Xiaoshi Wu, Renrui Zhang, Aojun Zhou, 420
Zipeng Qin, Yi Wang, et al. Journeydb: A benchmark for generative image understanding. Advances in 421
Neural Information Processing Systems , 36, 2024. 422
[40] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth√©e Lacroix, 423
Baptiste Rozi√®re, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard 424
Grave, and Guillaume Lample. Llama: Open and efficient foundation language models. arXiv preprint 425
arXiv:2302.13971 , 2023. 426
[41] Jianyi Wang, Zongsheng Yue, Shangchen Zhou, Kelvin CK Chan, and Chen Change Loy. Exploiting 427
diffusion prior for real-world image super-resolution. arXiv preprint arXiv:2305.07015 , 2023. 428
[42] Zijie J. Wang, Evan Montoya, David Munechika, Haoyang Yang, Benjamin Hoover, and Duen Horng Chau. 429
DiffusionDB: A large-scale prompt gallery dataset for text-to-image generative models. arXiv:2210.14896 430
[cs], 2022. 431
[43] Weijia Wu, Zhuang Li, Yefei He, Mike Zheng Shou, Chunhua Shen, Lele Cheng, Yan Li, Tingting Gao, Di 432
Zhang, and Zhongyuan Wang. Paragraph-to-image generation with information-enriched diffusion model. 433
arXiv preprint arXiv:2311.14284 , 2023. 434
[44] Xiaoshi Wu, Yiming Hao, Keqiang Sun, Yixiong Chen, Feng Zhu, Rui Zhao, and Hongsheng Li. Human 435
preference score v2: A solid benchmark for evaluating human preferences of text-to-image synthesis. arXiv 436
preprint arXiv:2306.09341 , 2023. 437
11[45] Ling Yang, Zhaochen Yu, Chenlin Meng, Minkai Xu, Stefano Ermon, and Bin Cui. Mastering text-to-image 438
diffusion: Recaptioning, planning, and generating with multimodal llms. arXiv preprint arXiv:2401.11708 , 439
2024. 440
[46] Lijun Yu, Jos√© Lezama, Nitesh B Gundavarapu, Luca Versari, Kihyuk Sohn, David Minnen, Yong Cheng, 441
Agrim Gupta, Xiuye Gu, Alexander G Hauptmann, et al. Language model beats diffusion‚Äìtokenizer is key 442
to visual generation. arXiv preprint arXiv:2310.05737 , 2023. 443
[47] Qiying Yu, Quan Sun, Xiaosong Zhang, Yufeng Cui, Fan Zhang, Xinlong Wang, and Jingjing Liu. 444
Capsfusion: Rethinking image-text data at scale. arXiv preprint arXiv:2310.20550 , 2023. 445
[48] Y Zhou, R Zhang, C Chen, C Li, C Tensmeyer, T Yu, J Gu, J Xu, and T Sun. Lafite: Towards language-free 446
training for text-to-image generation. arxiv 2021. arXiv preprint arXiv:2111.13792 . 447
12NeurIPS Paper Checklist 448
1.Claims 449
Question: Do the main claims made in the abstract and introduction accurately reflect the 450
paper‚Äôs contributions and scope? 451
Answer: [Yes] 452
Justification: The abstract and introduction clearly define the paper‚Äôs contributions, which 453
involve advancements in urban simulation accuracy and computational efficiency. These 454
claims are backed by robust experimental validation detailed in the subsequent sections. 455
Guidelines: 456
‚Ä¢The answer NA means that the abstract and introduction do not include the claims 457
made in the paper. 458
‚Ä¢The abstract and/or introduction should clearly state the claims made, including the 459
contributions made in the paper and important assumptions and limitations. A No or 460
NA answer to this question will not be perceived well by the reviewers. 461
‚Ä¢The claims made should match theoretical and experimental results, and reflect how 462
much the results can be expected to generalize to other settings. 463
‚Ä¢It is fine to include aspirational goals as motivation as long as it is clear that these goals 464
are not attained by the paper. 465
2.Limitations 466
Question: Does the paper discuss the limitations of the work performed by the authors? 467
Answer: [Yes] 468
Justification: We have included a comprehensive discussion on limitations, particularly 469
focusing on the scalability of our simulations in extremely large urban environments and 470
potential biases in the modeling processes. 471
Guidelines: 472
‚Ä¢The answer NA means that the paper has no limitation while the answer No means that 473
the paper has limitations, but those are not discussed in the paper. 474
‚Ä¢ The authors are encouraged to create a separate "Limitations" section in their paper. 475
‚Ä¢The paper should point out any strong assumptions and how robust the results are to 476
violations of these assumptions (e.g., independence assumptions, noiseless settings, 477
model well-specification, asymptotic approximations only holding locally). The authors 478
should reflect on how these assumptions might be violated in practice and what the 479
implications would be. 480
‚Ä¢The authors should reflect on the scope of the claims made, e.g., if the approach was 481
only tested on a few datasets or with a few runs. In general, empirical results often 482
depend on implicit assumptions, which should be articulated. 483
‚Ä¢The authors should reflect on the factors that influence the performance of the approach. 484
For example, a facial recognition algorithm may perform poorly when image resolution 485
is low or images are taken in low lighting. Or a speech-to-text system might not be 486
used reliably to provide closed captions for online lectures because it fails to handle 487
technical jargon. 488
‚Ä¢The authors should discuss the computational efficiency of the proposed algorithms 489
and how they scale with dataset size. 490
‚Ä¢If applicable, the authors should discuss possible limitations of their approach to 491
address problems of privacy and fairness. 492
‚Ä¢While the authors might fear that complete honesty about limitations might be used by 493
reviewers as grounds for rejection, a worse outcome might be that reviewers discover 494
limitations that aren‚Äôt acknowledged in the paper. The authors should use their best 495
judgment and recognize that individual actions in favor of transparency play an impor- 496
tant role in developing norms that preserve the integrity of the community. Reviewers 497
will be specifically instructed to not penalize honesty concerning limitations. 498
3.Theory Assumptions and Proofs 499
13Question: For each theoretical result, does the paper provide the full set of assumptions and 500
a complete (and correct) proof? 501
Answer: [Yes] 502
Justification: All theoretical results are accompanied by a clear statement of assumptions and 503
are supported by complete proofs provided in the supplementary materials. Each theorem 504
and lemma are properly referenced and numbered for clarity and ease of access. 505
Guidelines: 506
‚Ä¢ The answer NA means that the paper does not include theoretical results. 507
‚Ä¢All the theorems, formulas, and proofs in the paper should be numbered and cross- 508
referenced. 509
‚Ä¢All assumptions should be clearly stated or referenced in the statement of any theorems. 510
‚Ä¢The proofs can either appear in the main paper or the supplemental material, but if 511
they appear in the supplemental material, the authors are encouraged to provide a short 512
proof sketch to provide intuition. 513
‚Ä¢Inversely, any informal proof provided in the core of the paper should be complemented 514
by formal proofs provided in appendix or supplemental material. 515
‚Ä¢ Theorems and Lemmas that the proof relies upon should be properly referenced. 516
4.Experimental Result Reproducibility 517
Question: Does the paper fully disclose all the information needed to reproduce the main ex- 518
perimental results of the paper to the extent that it affects the main claims and/or conclusions 519
of the paper (regardless of whether the code and data are provided or not)? 520
Answer: [Yes] 521
Justification: The paper provides detailed descriptions of the experimental setup, including 522
data splits, hyperparameters, and the type of optimizer used. We also provide access to the 523
source code and datasets in the supplementary materials to ensure full reproducibility. 524
Guidelines: 525
‚Ä¢ The answer NA means that the paper does not include experiments. 526
‚Ä¢If the paper includes experiments, a No answer to this question will not be perceived 527
well by the reviewers: Making the paper reproducible is important, regardless of 528
whether the code and data are provided or not. 529
‚Ä¢If the contribution is a dataset and/or model, the authors should describe the steps taken 530
to make their results reproducible or verifiable. 531
‚Ä¢Depending on the contribution, reproducibility can be accomplished in various ways. 532
For example, if the contribution is a novel architecture, describing the architecture fully 533
might suffice, or if the contribution is a specific model and empirical evaluation, it may 534
be necessary to either make it possible for others to replicate the model with the same 535
dataset, or provide access to the model. In general. releasing code and data is often 536
one good way to accomplish this, but reproducibility can also be provided via detailed 537
instructions for how to replicate the results, access to a hosted model (e.g., in the case 538
of a large language model), releasing of a model checkpoint, or other means that are 539
appropriate to the research performed. 540
‚Ä¢While NeurIPS does not require releasing code, the conference does require all submis- 541
sions to provide some reasonable avenue for reproducibility, which may depend on the 542
nature of the contribution. For example 543
(a)If the contribution is primarily a new algorithm, the paper should make it clear how 544
to reproduce that algorithm. 545
(b)If the contribution is primarily a new model architecture, the paper should describe 546
the architecture clearly and fully. 547
(c)If the contribution is a new model (e.g., a large language model), then there should 548
either be a way to access this model for reproducing the results or a way to reproduce 549
the model (e.g., with an open-source dataset or instructions for how to construct 550
the dataset). 551
14(d)We recognize that reproducibility may be tricky in some cases, in which case 552
authors are welcome to describe the particular way they provide for reproducibility. 553
In the case of closed-source models, it may be that access to the model is limited in 554
some way (e.g., to registered users), but it should be possible for other researchers 555
to have some path to reproducing or verifying the results. 556
5.Open access to data and code 557
Question: Does the paper provide open access to the data and code, with sufficient instruc- 558
tions to faithfully reproduce the main experimental results, as described in supplemental 559
material? 560
Answer: [Yes] 561
Justification: The paper does not propose a benchmark and we will release the code if the 562
paper is accepted. The model depends on non-open-sourced dataset, and the copyright of the 563
checkpoint belongs to the company. Detailed instructions for training our model, including 564
command lines, are provided in the supplementary materials. 565
Guidelines: 566
‚Ä¢ The answer NA means that paper does not include experiments requiring code. 567
‚Ä¢Please see the NeurIPS code and data submission guidelines ( https://nips.cc/ 568
public/guides/CodeSubmissionPolicy ) for more details. 569
‚Ä¢While we encourage the release of code and data, we understand that this might not be 570
possible, so ‚ÄúNo‚Äù is an acceptable answer. Papers cannot be rejected simply for not 571
including code, unless this is central to the contribution (e.g., for a new open-source 572
benchmark). 573
‚Ä¢The instructions should contain the exact command and environment needed to run to 574
reproduce the results. See the NeurIPS code and data submission guidelines ( https: 575
//nips.cc/public/guides/CodeSubmissionPolicy ) for more details. 576
‚Ä¢The authors should provide instructions on data access and preparation, including how 577
to access the raw data, preprocessed data, intermediate data, and generated data, etc. 578
‚Ä¢The authors should provide scripts to reproduce all experimental results for the new 579
proposed method and baselines. If only a subset of experiments are reproducible, they 580
should state which ones are omitted from the script and why. 581
‚Ä¢At submission time, to preserve anonymity, the authors should release anonymized 582
versions (if applicable). 583
‚Ä¢Providing as much information as possible in supplemental material (appended to the 584
paper) is recommended, but including URLs to data and code is permitted. 585
6.Experimental Setting/Details 586
Question: Does the paper specify all the training and test details (e.g., data splits, hyper- 587
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the 588
results? 589
Answer: [Yes] 590
Justification: The experimental section of the paper provides comprehensive details about 591
the training and test setups, including the rationale behind choosing specific hyperparameters 592
and the types of optimizers used. 593
Guidelines: 594
‚Ä¢ The answer NA means that the paper does not include experiments. 595
‚Ä¢The experimental setting should be presented in the core of the paper to a level of detail 596
that is necessary to appreciate the results and make sense of them. 597
‚Ä¢The full details can be provided either with the code, in appendix, or as supplemental 598
material. 599
7.Experiment Statistical Significance 600
Question: Does the paper report error bars suitably and correctly defined or other appropriate 601
information about the statistical significance of the experiments? 602
Answer: [Yes] 603
15Justification: All experimental results are presented with error bars reflecting the standard 604
deviation across multiple runs. We provide a detailed explanation of how these were 605
calculated and the assumptions underlying our statistical tests. 606
Guidelines: 607
‚Ä¢ The answer NA means that the paper does not include experiments. 608
‚Ä¢The authors should answer "Yes" if the results are accompanied by error bars, confi- 609
dence intervals, or statistical significance tests, at least for the experiments that support 610
the main claims of the paper. 611
‚Ä¢The factors of variability that the error bars are capturing should be clearly stated (for 612
example, train/test split, initialization, random drawing of some parameter, or overall 613
run with given experimental conditions). 614
‚Ä¢The method for calculating the error bars should be explained (closed form formula, 615
call to a library function, bootstrap, etc.) 616
‚Ä¢ The assumptions made should be given (e.g., Normally distributed errors). 617
‚Ä¢It should be clear whether the error bar is the standard deviation or the standard error 618
of the mean. 619
‚Ä¢It is OK to report 1-sigma error bars, but one should state it. The authors should 620
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis 621
of Normality of errors is not verified. 622
‚Ä¢For asymmetric distributions, the authors should be careful not to show in tables or 623
figures symmetric error bars that would yield results that are out of range (e.g. negative 624
error rates). 625
‚Ä¢If error bars are reported in tables or plots, The authors should explain in the text how 626
they were calculated and reference the corresponding figures or tables in the text. 627
8.Experiments Compute Resources 628
Question: For each experiment, does the paper provide sufficient information on the com- 629
puter resources (type of compute workers, memory, time of execution) needed to reproduce 630
the experiments? 631
Answer: [Yes] 632
Justification: The paper details the computational resources required for each experiment, 633
including the types of GPUs used, the amount of memory, and the execution time. This 634
ensures that other researchers can allocate the appropriate resources to reproduce our results. 635
Guidelines: 636
‚Ä¢ The answer NA means that the paper does not include experiments. 637
‚Ä¢The paper should indicate the type of compute workers CPU or GPU, internal cluster, 638
or cloud provider, including relevant memory and storage. 639
‚Ä¢The paper should provide the amount of compute required for each of the individual 640
experimental runs as well as estimate the total compute. 641
‚Ä¢The paper should disclose whether the full research project required more compute 642
than the experiments reported in the paper (e.g., preliminary or failed experiments that 643
didn‚Äôt make it into the paper). 644
9.Code Of Ethics 645
Question: Does the research conducted in the paper conform, in every respect, with the 646
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines ? 647
Answer: [Yes] 648
Justification: Our research adheres strictly to the NeurIPS Code of Ethics. We have consid- 649
ered ethical implications, especially regarding the generation of images from text, and have 650
implemented measures to prevent misuse. 651
Guidelines: 652
‚Ä¢The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. 653
‚Ä¢If the authors answer No, they should explain the special circumstances that require a 654
deviation from the Code of Ethics. 655
16‚Ä¢The authors should make sure to preserve anonymity (e.g., if there is a special consid- 656
eration due to laws or regulations in their jurisdiction). 657
10.Broader Impacts 658
Question: Does the paper discuss both potential positive societal impacts and negative 659
societal impacts of the work performed? 660
Answer: [Yes] 661
Justification: The paper includes content about broader impacts that discusses both the 662
potential positive applications of our method in educational and creative industries, and 663
potential negative impacts, such as the misuse of generated images. We also suggest 664
mitigation strategies for potential negative uses. 665
Guidelines: 666
‚Ä¢ The answer NA means that there is no societal impact of the work performed. 667
‚Ä¢If the authors answer NA or No, they should explain why their work has no societal 668
impact or why the paper does not address societal impact. 669
‚Ä¢Examples of negative societal impacts include potential malicious or unintended uses 670
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations 671
(e.g., deployment of technologies that could make decisions that unfairly impact specific 672
groups), privacy considerations, and security considerations. 673
‚Ä¢The conference expects that many papers will be foundational research and not tied 674
to particular applications, let alone deployments. However, if there is a direct path to 675
any negative applications, the authors should point it out. For example, it is legitimate 676
to point out that an improvement in the quality of generative models could be used to 677
generate deepfakes for disinformation. On the other hand, it is not needed to point out 678
that a generic algorithm for optimizing neural networks could enable people to train 679
models that generate Deepfakes faster. 680
‚Ä¢The authors should consider possible harms that could arise when the technology is 681
being used as intended and functioning correctly, harms that could arise when the 682
technology is being used as intended but gives incorrect results, and harms following 683
from (intentional or unintentional) misuse of the technology. 684
‚Ä¢If there are negative societal impacts, the authors could also discuss possible mitigation 685
strategies (e.g., gated release of models, providing defenses in addition to attacks, 686
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from 687
feedback over time, improving the efficiency and accessibility of ML). 688
11.Safeguards 689
Question: Does the paper describe safeguards that have been put in place for responsible 690
release of data or models that have a high risk for misuse (e.g., pretrained language models, 691
image generators, or scraped datasets)? 692
Answer: [NA] 693
Justification: Our paper poses no such risks. If then, we will describe the safeguards 694
implemented in releasing our models, including usage guidelines and limitations to access, 695
ensuring responsible use and mitigating risks of misuse. 696
Guidelines: 697
‚Ä¢ The answer NA means that the paper poses no such risks. 698
‚Ä¢Released models that have a high risk for misuse or dual-use should be released with 699
necessary safeguards to allow for controlled use of the model, for example by requiring 700
that users adhere to usage guidelines or restrictions to access the model or implementing 701
safety filters. 702
‚Ä¢Datasets that have been scraped from the Internet could pose safety risks. The authors 703
should describe how they avoided releasing unsafe images. 704
‚Ä¢We recognize that providing effective safeguards is challenging, and many papers do 705
not require this, but we encourage authors to take this into account and make a best 706
faith effort. 707
12.Licenses for existing assets 708
17Question: Are the creators or original owners of assets (e.g., code, data, models), used in 709
the paper, properly credited and are the license and terms of use explicitly mentioned and 710
properly respected? 711
Answer: [Yes] 712
Justification: All third-party assets used in our research are properly credited, and we have 713
explicitly mentioned and complied with the licensing terms. URLs and version numbers of 714
datasets and code are clearly listed in the references. 715
Guidelines: 716
‚Ä¢ The answer NA means that the paper does not use existing assets. 717
‚Ä¢ The authors should cite the original paper that produced the code package or dataset. 718
‚Ä¢The authors should state which version of the asset is used and, if possible, include a 719
URL. 720
‚Ä¢ The name of the license (e.g., CC-BY 4.0) should be included for each asset. 721
‚Ä¢For scraped data from a particular source (e.g., website), the copyright and terms of 722
service of that source should be provided. 723
‚Ä¢If assets are released, the license, copyright information, and terms of use in the 724
package should be provided. For popular datasets, paperswithcode.com/datasets 725
has curated licenses for some datasets. Their licensing guide can help determine the 726
license of a dataset. 727
‚Ä¢For existing datasets that are re-packaged, both the original license and the license of 728
the derived asset (if it has changed) should be provided. 729
‚Ä¢If this information is not available online, the authors are encouraged to reach out to 730
the asset‚Äôs creators. 731
13.New Assets 732
Question: Are new assets introduced in the paper well documented and is the documentation 733
provided alongside the assets? 734
Answer: [Yes] 735
Justification: Any new datasets or models introduced in the paper are accompanied by 736
thorough documentation detailing their creation, intended use, limitations, and licensing 737
information. 738
Guidelines: 739
‚Ä¢ The answer NA means that the paper does not release new assets. 740
‚Ä¢Researchers should communicate the details of the dataset/code/model as part of their 741
submissions via structured templates. This includes details about training, license, 742
limitations, etc. 743
‚Ä¢The paper should discuss whether and how consent was obtained from people whose 744
asset is used. 745
‚Ä¢At submission time, remember to anonymize your assets (if applicable). You can either 746
create an anonymized URL or include an anonymized zip file. 747
14.Crowdsourcing and Research with Human Subjects 748
Question: For crowdsourcing experiments and research with human subjects, does the paper 749
include the full text of instructions given to participants and screenshots, if applicable, as 750
well as details about compensation (if any)? 751
Answer: [NA] 752
Justification: This paper does not involve crowdsourcing experiments or research with 753
human subjects. 754
Guidelines: 755
‚Ä¢The answer NA means that the paper does not involve crowdsourcing nor research with 756
human subjects. 757
‚Ä¢Including this information in the supplemental material is fine, but if the main contribu- 758
tion of the paper involves human subjects, then as much detail as possible should be 759
included in the main paper. 760
18‚Ä¢According to the NeurIPS Code of Ethics, workers involved in data collection, curation, 761
or other labor should be paid at least the minimum wage in the country of the data 762
collector. 763
15.Institutional Review Board (IRB) Approvals or Equivalent for Research with Human 764
Subjects 765
Question: Does the paper describe potential risks incurred by study participants, whether 766
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) 767
approvals (or an equivalent approval/review based on the requirements of your country or 768
institution) were obtained? 769
Answer: [NA] 770
Justification: Our research did not involve human subjects, thus no IRB approval was 771
necessary. 772
Guidelines: 773
‚Ä¢The answer NA means that the paper does not involve crowdsourcing nor research with 774
human subjects. 775
‚Ä¢Depending on the country in which research is conducted, IRB approval (or equivalent) 776
may be required for any human subjects research. If you obtained IRB approval, you 777
should clearly state this in the paper. 778
‚Ä¢We recognize that the procedures for this may vary significantly between institutions 779
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the 780
guidelines for their institution. 781
‚Ä¢For initial submissions, do not include any information that would break anonymity (if 782
applicable), such as the institution conducting the review. 783
19