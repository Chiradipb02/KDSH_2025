AlignedCut: Visual Concepts Discovery on
Brain-Guided Universal Feature Space
Anonymous Author(s)
Affiliation
Address
email
Abstract
We study the intriguing connection between visual data, deep networks, and the 1
brain. Our method creates a universal channel alignment by using brain voxel 2
fMRI response prediction as the training objective. We discover that deep net- 3
works, trained with different objectives, share common feature channels across 4
various models. These channels can be clustered into recurring sets, correspond- 5
ing to distinct brain regions, indicating the formation of visual concepts. Tracing 6
the clusters of channel responses onto the images, we see semantically meaning- 7
ful object segments emerge, even without any supervised decoder. Furthermore, 8
the universal feature alignment and the clustering of channels produce a picture 9
and quantification of how visual information is processed through the different 10
network layers, which produces precise comparisons between the networks. 11
1 Introduction 12
Introducing a novel approach, Yang et al. (2024) has successfully established a method of computing 13
a mapping between the brain and deep-nets, effectively linking two black boxes. The brain fMRI 14
prediction task allows for visualizing information flow from layer to layer, using the brain as an 15
analysis tool. 16
channel (768D)activation
visual brainLinear
transformLH RH
Figure 1: Transform the hidden channel activation
of deep-nets into visual brain voxels’ response.If a picture is worth a thousand words, the 17
main idea is that the brain’s thousands of voxels 18
can be thought of as alphabets for these words 19
that describe an image. Just as alphabets must 20
be combined to form words and phrases with 21
meanings, we need to find the grouping of brain 22
voxels and their network channel counterparts 23
to understand their meaning (Figure 1). 24
Our main discovery is that while the network 25
layer structure differs, channel feature corre- 26
spondence exists across networks with a shared 27
encoding of reoccurring visual concepts. This paper builds upon the idea of ‘Rosetta stone’ neurons 28
(Dravid et al., 2023), which find channels across networks that share similar image responses in bi- 29
nary segmentation. If channels are alphabets, ‘Rosetta stone’ provides an alphabet-level translation 30
between networks. 31
Individual channel-level analysis could miss feature correspondence across networks at finer and 32
coarser levels. On a finer level, because the channels are invariant up to a linear transformation, we 33
might miss a reconstituted feature constructed from a composition of existing channels. On a coarse 34
level, the channels can be combined and clustered to form a bigger ‘Rosetta’ concept. 35
Submitted to 38th Conference on Neural Information Processing Systems (NeurIPS 2024). Do not distribute.To address fine-level channel analysis, we use brain voxel response as a reference signal and linearly 36
transform channels for each network into a shared space sufficient for brain fMRI prediction. This 37
process produces a universal feature space that aligns channel features across the layers and models. 38
To find bigger visual concepts, one can start with Neuroscience knowledge of brain regions (ROIs) 39
with specific brain functionality, i.e., V1, V4, and EBA. While tracing the mapping of the ROIs to 40
channels can produce visual concepts (Figure 2), brain regions don’t function in isolation. 41
Image Patchupperleft
centerlowerright02565127684
2
024
Brain ROI's
T op Channels
V1
V4
EBA
V1
V4
EBA
all
Figure 2: From the 768D feature on CLIP layer-6, we extract different levels of segmentation by
restricting the use of a subset of channels. Left: Channel activation on example image patches. The
ordering of channels is sorted from the early brain to the late brain by their weights for brain voxels.
Right: Spectral clustering on each subset of channels filtered by each brain ROI (V1, V4, EBA),
image pixels colored by 3D spectral-tSNE of top 10 eigenvectors.
Instead of searching through all possible channel grouping combinations, our first insight is that 42
we can create a channel grouping hypothesis by examining channels from each pixel’s perspective. 43
Think of the pixels and channels forming a bipartite graph; each channel produces a per-pixel re- 44
sponse (image activation map), defining the graph edge between the pixels and channels. Taking the 45
perspective of pixels, one can collect graph edges incident on each pixel into a vector, which can be 46
thresholded to produce a hypothesis grouping over channels. 47
Our second insight is that if a channel grouping hypothesis repeats across images, layers, and mod- 48
els, it is highly unlikely to be accidental and, therefore, signals meaningful visual concepts. 49
We formulate this clustering problem as a graph partition task. The graph nodes are the product 50
space of pixels and layers. We apply spectral clustering to produce k-top eigenvectors. We take 51
advantage of two properties of spectral clustering: it makes 1) soft-cluster embedding space in the 52
form of eigenvectors and 2) hierarchical clustering by varying the number of eigenvectors. 53
We made the following discoveries. First, shared channel sets, reoccurring across layers and models, 54
predict response in distinct brain regions. By tracing the channel activation to the known brain ROI 55
properties, we observe that the channel cluster encodes visual concepts at various levels of visual 56
abstraction. 57
Second, meaningful object segments can emerge by tracing the channel cluster responses onto each 58
image. We observed that some channel clusters produce figure/ground separation while others pro- 59
duce fine-grained category classification. Our image segmentation requires no additional segmenta- 60
tion decoder and uses only a simple distance measure over the eigenvectors. 61
Finally, the universal feature alignment and the spectral clustering of channels produce a picture and 62
quantification of how visual information is processed through the different network layers. 63
While these discoveries are promising, there are two main technical hurdles to overcome to verify 64
them on a large scale. Our method rests upon a crucial assumption: the channels across the different 65
layers and models can be mapped into a shared space. While brain prediction over thousands of 66
voxels can provide strong guidance for this alignment, an additional constraint would be needed 67
when the shared space has a large dimension (suitable for expressiveness). We use clustering as 68
a constraint, ensuring alignment linear transformation preserves spectral clustering eigenvectors. 69
Furthermore, the graph size is enormous as it is a product space over pixels, layers, images, and 70
models; therefore, computing eigenvectors over their pairwise affinity matrix can be computationally 71
infeasible. We developed a Nystrom-like approximation to ensure efficient computation. 72
2In summary, our key contributions are: 73
1. We constructed a universal channel-aligned space using brain encoding as supervision and spec- 74
tral clustering eigenvector constraints to ensure minimal channel signal loss. Brain encoding asso- 75
ciates the aligned channel space to brain regions and gives them meanings. 76
2. Models trained with different objectives learned similar visual concepts: corresponding channel 77
patterns exist across different models. The resulting visual concepts can be validated by unsuper- 78
vised segmentation benchmarks on ImageNet-segmentation and PASCAL VOC. 79
3. Models show divergent computation paths over the visual concept space formed by the top-k 80
spectral eigenvectors. Different models differ in trajectories and pace of movement layer-to-layer. 81
2 Methods: AlignedCut 82
0
4
8
0
4
8
0
4
0
4
8
0
4
8
0
4
8
before channel align
0
4
8
0
4
8
0
4
8 8
CLIP DINO MAE
0
4
8
0
4
8
0
4
8
Layerafter channel align
0.6
0.4
0.2
0.00.20.40.6
CLIP DINO MAE Layer
Figure 3: Cosine similarity of channel activa-
tion on the same image inputs.Just as human languages might consist of distinct 83
alphabets, features across different models appear 84
superficially in embedding spaces as almost mutu- 85
ally orthogonal (Figure 3). However, the underly- 86
ing information that they represent can be similar. 87
To jointly analyze features across models and lay- 88
ers, we proposed the channel align transform that 89
linearly projects features to a universal space. 90
The learning signal for the channel align transform 91
is provided by brain response prediction . Learn- 92
ing from brain prediction offers two advantages. 93
First, brain response covers rich representations from all levels of semantics; the channel alignment 94
removes irrelevant information while preserving the necessary and sufficient visual image features. 95
Second, knowledge of brain regions provides an interpretable understanding of their corresponding 96
channels derived from the alignment. 97
Our visual concept discovery is formulated as a graph partitioning task using spectral clustering . 98
We term our approach for this channel align and graph partitioning as AlignedCut . Furthermore, a 99
major challenge in applying spectral clustering to large graphs is the complexity scaling issue. To 100
address this, we developed a Nystrom-like approximation to reduce the computational complexity. 101
2.1 Brain-Guided Universal Channel Align 102
Brain Dataset We used the Algonauts competition (Gifford et al., 2023) release of Nature Scenes 103
Dataset (NSD) (Allen et al., 2022). Briefly, NSD provides an fMRI brain scan when watching 104
COCO images. Each subject viewed 10,000 images over 40 hours of scanning. We used the first 105
subject’s publicly shared pre-processed and denoised (Prince et al., 2022) data. 106
Channel Align LetV={V1,V2,···,Vn|Vi∈RP×Di}be the set of image features, extracted 107
from each layer of pre-trained ViT models, where P= (H×W+1)is image patches and class token, 108
Diis the hidden dimension. In particular, we used the attention layer output for each Viwithout 109
adding residual connections from previous layers. Let V′be the channel-aligned features; the goal 110
of channel alignment is to learn a set of linear transform W={W1,W2,···,Wn|Wi∈RDi×D′}. 111
In the new D′dimensional space, channels are aligned. 112
V′=V ⊙ W ={V1W1,V2W2,···,VnWn|ViWi∈RP×D′} (1)
Brain Prediction To produce a learning signal for channel align W, features from V′are summed 113
(not concatenated ) to do brain prediction. Let Y∈R1×Nbe the brain prediction target, where Nis 114
the number of flattened 3D brain voxels, and 1indicates that each voxel’s response is a scalar value. 115
LetFθ:RP×D′⇒R1×Nbe the learned brain encoding model; without loss of generalizability, we 116
setFθas global average pooling then linear weight βθ∈RD′×Nand bias ϵθ∈R1×N: 117
"
Avg Pool
p∈P(1
nnX
i=1(ViWi))×βθ+ϵθ#
⇒Y (2)
3Channel in the Brain’s Space LetB={B1,B2,···,Bn|Bi∈RP×N}be the set of channel 118
activations in the brain’s space. By defining Bi:=ViWi×βθ, we have the brain response prediction 119
Y=Avg Poolp∈P(1
nPn
i=1Bi) +ϵθ(Eq. (2)). Intuitively, we linearly transformed the activation 120
to the brain’s space, such that the activation from all slots sum up to the brain response prediction. 121
2.2 Graph Spectral Clustering 122
Spectral Clustering We use spectral clustering for visual concepts discovery and image-channel 123
analysis; it provides 1) soft-cluster embedding space and 2) unsupervised hierarchical image seg- 124
mentation. Normalized Cut (Shi and Malik, 2000) partitions the graph into sub-graphs with minimal 125
cost of breaking edges. It embeds the graph into a lower dimensional eigenvector representation, 126
where each eigenvector is a hierarchical sub-graph assignment. 127
LetA∈RM×Mbe the symmetric affinity matrix, where Mdenotes the total number of image 128
patches. Given channel aligned features V′∈RM×D′, we define Aij:= exp(cos( V′
i,V′
j)−1) 129
such that Aij>0measures the similarity between data iandj. The spectral clustering embedding 130
X∈RM×Cis solved by the top Ceigenvectors of the following generalized eigenproblem: 131
(D−1/2AD−1/2)X=XΛ (3)
where Dis the diagonal degree matrix Dii=P
jAij,Λis diagonal eigenvalue matrix. 132
Nystrom-like Approximation Computing eigenvectors for A∈RM×Mis prohibitively expen- 133
sive for enormous Mwith a time complexity of O(M3). The original Nystrom approximation 134
method (Fowlkes et al., 2004) reduced the time complexity to O(m3+m2M)by solving eigenvec- 135
tors on sub-sampled graph A′∈Rm×m, where m≪M. In particular, the orthogonalization step 136
of eigenvectors introduced the time complexity of O(m2M). Because our Nystrom-like approxi- 137
mation trades the O(m2M)orthogonalization term with the K-nearest neighbor, our Nystrom-like 138
approximation reduced the time complexity to O(m3+mM). 139
Our Nystrom-like Approximation first solves the eigenvector X′∈Rm×Con a sub-sampled graph 140
A′∈Rm×musing Equation (3), then propagates the eigenvector from the sub-graph mnodes 141
to the full-graph Mnodes. Let ˜X∈RM×Cbe the approximation ˜X≈X. The eigenvector 142
approximation ˜Xiof full-graph node i≤Mis assigned by averaging the top K-nearest neighbors’ 143
eigenvector X′
kfrom the sub-graph nodes k≤m: 144
Ki=KNN (A∗i;m, K ) = arg max
k≤mKX
k=1Aki
˜Xi=1P
k∈K iAkiX
k∈K iAkiX′
k(4)
where KNN (A∗i;m, K )denotes KNN from full-graph node i≤Mto sub-graph nodes k≤m. 145
2.3 Affinity Eigen-constraints as Regularization for Channel Align 146
Table 1: Affinity eigen-constraints improved
brain score ( R2: variance explained).
ROI Brain Score R2(±0.001)
λeigen V1 V4 EBA all
1.0 0.170 0.181 0.295 0.196
0.1 0.167 0.179 0.294 0.193
0 0.155 0.166 0.296 0.188While brain prediction can provide strong supervi- 147
sion for the learned channel align operation, we ob- 148
served that the quality of unsupervised segmentation 149
dropped after the channel alignment. To address this 150
issue, a regularization term is added: 151
Leigen =∥XbXT
b−XaXT
a∥ (5)
where XbandXa∈R˜m×care affinity matrix 152
eigenvectors before and after channel alignment, re- 153
spectively; ˜m= 100 are randomly sampled nodes 154
in a mini-batch and c= 6 are the top eigenvectors. The eigen-constraint preserves spectral clus- 155
tering eigenvectors in dot-product space, invariant to random rotations in eigenvectors. We found 156
adding eigen-constraints improved both the performance of segmentation (Figure 5) and the brain 157
prediction score (Table 1). 158
43 Results 159
Our spectral clustering analysis aims to discover visual concepts that share the same pattern of 160
channel activation across different models and layers. However, implementing spectral clustering 161
analysis comes with two main challenges. First, the models sit in different feature spaces, so direct 162
clustering will not reveal their overlap and similarities. Second, when scaling up to a large graph, 163
spectral clustering is computationally expensive. 164
To address the first challenge, we developed our channel align transform to align features into a 165
universal space. We extracted features from all 12 layers of the CLIP (ViT-B, OpenAI) (Radford 166
et al., 2021), DINOv2 (ViT-B with registers) (Darcet et al., 2024), and MAE (ViT-B) (He et al., 167
2022) and then transformed features from each layer into the universal feature space. 168
To address the second challenge, we developed our Nystrom-like approximation to reduce the com- 169
putational complexity. We extracted features from 1000 ImageNet (Deng et al., 2009) images, with 170
each image consisting of 197 patches per layer. The entire product space of all images and fea- 171
tures totaled M= 7e+6 nodes, from which we applied our Nystrom-like approximation with sub- 172
sampled m= 5e+4 nodes and KNN K= 100 , computing the top 20 eigenvectors. 173
To visualize the affinity eigenvectors, the top 20 eigenvectors were reduced to a 3-dimensional space 174
by t-SNE, and a color value was assigned to each node by the RGB cube. We call this approach 175
AlignedCut color. 176
CLIPLayer0
 Layer1
 Layer2
 Layer3
 Layer4
 Layer5
 Layer6
 Layer7
 Layer8
 Layer9
 Layer10
 Layer11
DINO
 MAE
CLIPLayer0
 Layer1
 Layer2
 Layer3
 Layer4
 Layer5
 Layer6
 Layer7
 Layer8
 Layer9
 Layer10
 Layer11
DINO
 MAE
CLIPLayer0
 Layer1
 Layer2
 Layer3
 Layer4
 Layer5
 Layer6
 Layer7
 Layer8
 Layer9
 Layer10
 Layer11
DINO
 MAE
Figure 4: Spectral clustering in the universal channel aligned feature space. The image pixels are
colored by our approach AlignedCut, the pixel RGB value is assigned by the 3D spectral-tSNE of
the top 20 eigenvectors. The coloring is consistent across all images, layers, and models.
In Figure 4, we displayed the analysis, AlignedCut color, and made the following observations: 177
1. In CLIP layer-5, DINO layer-6, and MAE layer-8, there is class-agnostic figure-ground sepa- 178
ration, with foreground objects from different categories grouped into the same AlignedCut color. 179
2. In CLIP layer-9, there is a class-specific separation of foreground objects, with foreground 180
objects grouped into AlignedCut colors with associated semantic categories. 181
3. Before layer-3, CLIP and DINO produce the same AlignedCut color regardless of the image 182
input. From layer-4 onwards, the AlignedCut color smoothly changes over layers. 183
53.1 Figure-ground representation emerge before categories 184
In this section, we benchmark each layer in CLIP with unsupervised segmentation. The key findings 185
from this benchmarking are: 1)The figure-ground representation emerges at CLIP layer-4 and is 186
preserved in subsequent layers; 2)Categories emerge over layers, peaking at layer-9 and layer-10. 187
01234567891011 Layer0.20.40.6mIoU
ImageNet-segmentation
before channel align
after channel align
  (w/o eigen constraints)
01234567891011 Layer0.20.4mIoU
PASCAL VOC (val2012)
Figure 5: Unsupervised segmentation scores from spectral clustering on each CLIP layer. ImageNet-
segmentation dataset is used with binary figure-ground labels, and the mIoU score peaks plateau
from layer-4 to layer-10. In PASCAL VOC with 20 class labels, the mIoU score peaks at layer-9.
From which layers did the figure-ground and category representations emerge? We conducted 188
experiments that compared the unsupervised segmentation scores across layers, tracing how well 189
each representation is encoded at each layer. We used two datasets: a) ImageNet-segmentation 190
(Guillaumin et al., 2014) with binary figure-ground labels, and b) PASCAL VOC (Everingham et al., 191
2010) with 20 category labels. The results are presented in Figure 5. On the ImageNet-segmentation 192
benchmark, the score peaks at layer-4 (mIoU=0.6) and plateaus in subsequent layers, suggesting that 193
the figure-ground representation is encoded and preserved from layer-4 onwards. On the PASCAL 194
VOC benchmark, the score peaks at layer-9 and layer-10 (mIoU=0.5) even though it is low at layer-4 195
(mIoU=0.2), indicating that category information is encoded at layer-9 and layer-10. Overall, we 196
conclude that the figure-ground representation emerges before the category representation. 197
3.2 Visual concepts: class-agnostic figure-ground 198
In this section, we use brain activation heatmaps and image similarity heatmaps to describe figure- 199
ground visual concepts. The key findings from these heatmaps are: 1)The figure vs. ground pixels 200
activate different channels; 2)The figure-ground visual concept is class-agnostic; 3)The figure- 201
ground visual concept is consistent across models. 202
foreground background
ref. bg fg
Figure 6: The figure-ground visual concepts in CLIP layer-5. Left: Mean activation of foreground
or background pixels, linearly transformed to the brain’s space. Right: Cosine similarity from one
reference pixel marked. The figure-ground visual concepts are agnostic to image categories.
How can the channel activation patterns of the figure-ground visual concept be described? We 203
averaged the channel activations from foreground and background pixels, using the ground-truth 204
labels from the ImageNet-segmentation dataset. The averaged channel activations were transformed 205
into the brain’s space. In Figure 6, foreground pixels exhibit positive activation in early visual brain 206
ROIs (V1 to V4) and the face-selective ROI (FFA), while negatively activating place-selective ROIs 207
(OPA and PPA). Interestingly, background pixels activate the reverse pattern compared to foreground 208
pixels. Overall, the figure and ground pixels activate distinct brain ROIs. 209
Is the figure-ground visual concept class-agnostic? We manually selected onepixel and computed 210
the cosine similarity to all of the other image pixels. In Figure 6, the results demonstrate that one 211
pixel (on the human) could segment out foreground objects from all other classes (shark, dog, cat, 212
rabbit). The same result holds true for one background pixel. We conclude that the figure-ground 213
visual concept is class-agnostic. 214
6Figure 7: The same figure-ground visual concepts are found in CLIP, DINO and MAE. Left: Mean
activation of all foreground (top) and background (bottom) pixels; the three models exhibit similar
activation patterns. Right: AlignedCut, pixels colored by 3D spectral-tSNE of the top 20 eigenvec-
tors; the three models show similar grouping colors for foreground pixels.
Is the figure-ground visual concept consistent across models? We performed the channel analysis 215
for CLIP, DINO, and MAE. In Figure 7, the foreground or background pixels activates similar 216
brain ROIs across the three models. Additionally, spectral clustering grouped the representations of 217
foreground objects into similar colors for CLIP and DINO (light blue), the grouping for MAE is less 218
similar (dark blue). Overall, the figure-ground visual concept is consistent across models. 219
3.3 Visual concepts: categories 220
In this section we use AlignedCut to discover category visual concepts. The key findings from the 221
category visual concepts are: 1)Class-specific visual concepts activate diverse brain regions; 2) 222
Visual concepts with higher channel activation values are more consistent. 223
Figure 8: Category visual concepts in CLIP layer-9. Left: Mean activation of all pixels within an
Euclidean sphere centered at the visual concept in the 3D spectral-tSNE space; the concepts activate
different brain regions. Middle: The standard deviation negatively correlates with absolute mean
activations. Right: AlignedCut, pixels colored by 3D spectral-tSNE of the top 20 eigenvectors.
How does each class-specific concept activate the channels? To answer this question, we sam- 224
pled class-specific concepts from CLIP layer-9. First, we used farthest point sampling to identify 225
candidate centers in the 3D spectral-tSNE space. Then, each candidate center was grouped with its 226
neighboring pixels within an Euclidean sphere in the spectral-tSNE space. Finally, the channel acti- 227
vations of the grouped pixels were averaged to produce the mean channel activation for each visual 228
concept. In Figure 8, Concept 1 (duck, goose) negatively activates late brain regions; Concept 2 229
7(snake, turtle) positively activates early brain regions and also FFA; Concept 3 (dog) negatively ac- 230
tivates early brain regions. Overall, category-specific visual concepts activate diverse brain regions. 231
How do we quantify the consistency of each visual concept? Qualitatively, Concept 1 exhibits more 232
consistent coloring (Figure 8, pink) than Concept 3 (purple). To further quantify this observation, we 233
computed the mean and standard deviation of channel activations for each Euclidean sphere centered 234
on a concept. In Figure 8, there is a reverse U-shape relation between magnitude and standard 235
deviation. The reverse U-shape implies that larger absolute mean channel activation corresponds 236
to lower standard deviation. Overall, higher channel activation magnitudes suggest more consistent 237
visual concepts. 238
3.4 Transition of visual concepts over layers 239
In this section, instead of using 3D spectral-tSNE, we use 2D spectral-tSNE to trace the layer-to- 240
layer feature computation. The key findings of spectral-tSNE in 2D are: 1)The figure vs. ground 241
pixels are encoded in separate spaces in late layers; 2)The representations for foreground and back- 242
ground bifurcate at CLIP layer-4 and DINO layer-5. 243
Layer
personunsupervised
horse
car
grass
skyroad
Figure 9: Trajectory of feature progression in layers for six example pixels. Left: 2D spectral-tSNE
plot of the top 20 eigenvectors, jointly clustered across all models; the foreground and background
pixels bifurcate at CLIP layer-4 and DINO layer-5. Right: Pixels colored by unsupervised segmen-
tation.
How does the network encode figure and ground pixels in each layer? We performed spectral 244
clustering and 2D t-SNE on the top 20 eigenvectors to project all layers into a 2D spectral-tSNE 245
space. In Figure 9, we found that all foreground and background pixels are grouped together in 246
each early layer. Each early layer (dark dots) forms an isolated cluster separate from other layers, 247
while late layers (bright dots) are grouped in the center. In the late layers, there is a separation 248
where foreground pixels occupy the upper part of 2D spectral-tSNE space, while background pixels 249
occupy the middle part. Overall, foreground and background pixels are encoded in separate spaces 250
in late layers. 251
How does the network process each pixel from layer to layer? In the 2D spectral-tSNE plot, 252
we traced the trajectory for each pixel from layer-3 to the last layer. In Figure 9, we found that 253
the trajectories for foreground and background pixels bifurcate: foreground pixels (person, horse, 254
car) traverse to the upper side and remain within the upper side; background pixels (grass, road, 255
sky) jump between the middle right and left sides. The same bifurcation is consistently observed 256
for CLIP from layer-3 to layer-4 and DINO from layer-4 to layer-5. Furthermore, to quantify the 257
bifurcation for foreground and background pixels, we first sampled 5 visual concepts from CLIP 258
layer-3 and layer-4. Then, we measured the transition probability between visual concepts, defined 259
as the proportion of pixels that transited from an Euclidean circle around concept A to a circle 260
around concept B. In Figure 10, the transition probability of foreground pixels to the upper side 261
(A1 to B0) is higher than that of background pixels (0.44 vs. 0.16), while the transition probability 262
of background pixels to the right side (A4 to B4) is higher than that of foreground pixels (0.36 vs. 263
0.06). Overall, this suggests a bifurcation of figure and ground pixel representations at the middle 264
layers of both CLIP and DINO. 265
8B0 B1 B2 B3 B4
CLIP Layer 4 (in)A0 A1 A2 A3 A4CLIP Layer 3 (out)0.04 0.10 0.05 0.10 0.00
0.16 0.26 0.03 0.01 0.00
0.06 0.25 0.23 0.02 0.00
0.00 0.00 0.12 0.57 0.00
0.00 0.01 0.10 0.00 0.36Transition Probability (background pixels)
Transition Probability
B0 B1 B2 B3 B4
CLIP Layer 4 (in)A0 A1 A2 A3 A4CLIP Layer 3 (out)0.28 0.07 0.01 0.01 0.00
0.44 0.08 0.01 0.00 0.00
0.23 0.16 0.03 0.00 0.00
0.04 0.01 0.04 0.11 0.00
0.03 0.02 0.12 0.00 0.06Transition Probability (foreground pixels)Figure 10: Transition probability of visual concepts from CLIP layer-3 to layer-4. Left: Five visual
concepts sampled from CLIP layer-3 and layer-4. Right: Transition probability measured separately
for foreground and background pixels; a bifurcation occurs where foreground pixels have more
traffic to concept B0, while background pixels have more traffic to concepts B3 and B4.
4 Related Work 266
Mechanistic Interpretability is a field of study that intends to understand and explain the inner 267
working mechanisms of deep networks. One approach is to interpret individual neurons (Bau et al., 268
2017; Dravid et al., 2023) and circuit connections between neurons (Olah et al., 2020). Another ap- 269
proach is to interpret transformer attention heads (Gandelsman et al., 2024) and circuit connections 270
between attention heads (Wang et al., 2023a). Other approaches also looked into the role of patch 271
tokens (Sun et al., 2024). These approaches made the assumption that channels are aligned within 272
the same model; we compare across models by actively aligning the channels to a universal space. 273
Spectral Clustering is a graphical method to analyze data grouping in the eigenvector space. Spec- 274
tral methods have been widely used for unsupervised image segmentation (Shi and Malik, 2000; 275
von Luxburg, 2007; Wu et al., 2018; Wang et al., 2023b). One major challenge for applying spectral 276
clustering to large graphs is the complexity scaling issue. To solve the scaling issue, the Nystrom 277
approximation (Fowlkes et al., 2004) approaches solve eigenvectors on sub-sampled graphs and then 278
propagate to the full graph. Another approach is the gradient-based eigenvector solver (Zhang et al., 279
2023), which solves the eigenvectors in mini-batches. Our proposed Nystrom-like approximation 280
achieves a computational speedup over the original Nystrom approximation, albeit at the expense of 281
weakened orthogonality of the eigenvectors. 282
Brain Encoding Model is widely used by the computational neuroscience community (Kriegeskorte 283
and Douglas, 2018). They have been using deep nets to explain the brain’s function. One approach 284
is to use the gradient of the brain encoding model to find the most salient image features (Sarch 285
et al., 2023). Another approach generate text caption for brain activation (Luo et al., 2024). Other 286
approaches compare brain prediction performance for different models (Schrimpf et al., 2020). The 287
field focused on using deep nets as a tool to explain the brain’s function; we go in the opposite 288
direction by using the brain to explain deep nets. 289
5 Conclusion and Limitations 290
We present a novel approach to interpreting deep neural networks by leveraging brain data. Our 291
fundamental innovation is twofold: First, we use brain prediction as guidance to align channels from 292
different models into a universal feature space; Second, we developed a Nystrom-like approximation 293
to scale up the spectral clustering analysis. Our key discovery is that recurring visual concepts exist 294
across networks and layers; such concepts correspond to different levels of objects, ranging from 295
figure-ground to categories. Additionally, we quantified the information flow from layer to layer, 296
where we found a bifurcation of figure-ground visual concepts. 297
Limitations. While the learned channel align transformation projects all features onto a universal 298
feature space, the nature of learned transformation does not preserve all the information. There 299
is a small drop in unsupervised segmentation performance after channel alignment, which is not 300
fully addressed by our proposed eigen-constraint regularization. Secondly, as a trade-off for faster 301
computation, our Nystrom-like approximation does not produce strictly orthogonal eigenvectors. To 302
produce expressive eigenvectors, our approximation relies on using larger sub-sample sizes than the 303
original Nystrom method. 304
9References 305
Allen, E. J., St-Yves, G., Wu, Y ., Breedlove, J. L., Prince, J. S., Dowdle, L. T., Nau, M., Caron, 306
B., Pestilli, F., Charest, I., Hutchinson, J. B., Naselaris, T., and Kay, K. (2022). A massive 7T 307
fMRI dataset to bridge cognitive neuroscience and artificial intelligence. Nature Neuroscience , 308
25(1):116–126. Number: 1 Publisher: Nature Publishing Group. 3 309
Bau, D., Zhou, B., Khosla, A., Oliva, A., and Torralba, A. (2017). Network Dissection: Quantifying 310
Interpretability of Deep Visual Representations. In Computer Vision and Pattern Recognition . 9 311
Darcet, T., Oquab, M., Mairal, J., and Bojanowski, P. (2024). Vision Transformers Need Registers. 312
InThe Twelfth International Conference on Learning Representations . 5 313
Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L. (2009). ImageNet: A large- 314
scale hierarchical image database. In 2009 IEEE Conference on Computer Vision and Pattern 315
Recognition , pages 248–255. 5 316
Dravid, A., Gandelsman, Y ., Efros, A. A., and Shocher, A. (2023). Rosetta Neurons: Mining the 317
Common Units in a Model Zoo. In Proceedings of the IEEE/CVF International Conference on 318
Computer Vision (ICCV) , pages 1934–1943. 1, 9 319
Everingham, M., Van Gool, L., Williams, C. K. I., Winn, J., and Zisserman, A. (2010). The Pascal 320
Visual Object Classes (VOC) Challenge. International Journal of Computer Vision , 88(2):303– 321
338. 6 322
Fowlkes, C., Belongie, S., Chung, F., and Malik, J. (2004). Spectral grouping using the Nystrom 323
method. IEEE Transactions on Pattern Analysis and Machine Intelligence , 26(2):214–225. 4, 9 324
Gandelsman, Y ., Efros, A. A., and Steinhardt, J. (2024). Interpreting CLIP’s Image Representation 325
via Text-Based Decomposition. In The Twelfth International Conference on Learning Represen- 326
tations . 9 327
Gifford, A. T., Lahner, B., Saba-Sadiya, S., Vilas, M. G., Lascelles, A., Oliva, A., Kay, K., Roig, G., 328
and Cichy, R. M. (2023). The Algonauts Project 2023 Challenge: How the Human Brain Makes 329
Sense of Natural Scenes. arXiv:2301.03198 [cs, q-bio]. 3 330
Guillaumin, M., K ¨uttel, D., and Ferrari, V . (2014). ImageNet Auto-Annotation with Segmentation 331
Propagation. International Journal of Computer Vision , 110(3):328–348. 6 332
He, K., Chen, X., Xie, S., Li, Y ., Doll ´ar, P., and Girshick, R. (2022). Masked Autoencoders Are 333
Scalable Vision Learners. In Proceedings of the IEEE/CVF Conference on Computer Vision and 334
Pattern Recognition (CVPR) , pages 16000–16009. 5 335
Kriegeskorte, N. and Douglas, P. K. (2018). Cognitive computational neuroscience. Nature Neuro- 336
science , 21(9):1148–1160. Publisher: Nature Publishing Group. 9 337
Lin, T.-Y ., Goyal, P., Girshick, R., He, K., and Dollar, P. (2017). Focal Loss for Dense Object 338
Detection. In Proceedings of the IEEE International Conference on Computer Vision (ICCV) . 14 339
Luo, A., Henderson, M. M., Tarr, M. J., and Wehbe, L. (2024). BrainSCUBA: Fine-Grained Natural 340
Language Captions of Visual Cortex Selectivity. In The Twelfth International Conference on 341
Learning Representations . 9 342
Olah, C., Cammarata, N., Schubert, L., Goh, G., Petrov, M., and Carter, S. (2020). Zoom In: An 343
Introduction to Circuits. Distill , 5(3):e00024.001. 9 344
Prince, J. S., Charest, I., Kurzawski, J. W., Pyles, J. A., Tarr, M. J., and Kay, K. N. (2022). Im- 345
proving the accuracy of single-trial fMRI response estimates using GLMsingle. eLife , 11:e77599. 346
Publisher: eLife Sciences Publications, Ltd. 3 347
Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., 348
Mishkin, P., Clark, J., Krueger, G., and Sutskever, I. (2021). Learning Transferable Visual Models 349
From Natural Language Supervision. In Meila, M. and Zhang, T., editors, Proceedings of the 38th 350
International Conference on Machine Learning , volume 139 of Proceedings of Machine Learning 351
Research , pages 8748–8763. PMLR. 5 352
Sarch, G. H., Tarr, M. J., Fragkiadaki, K., and Wehbe, L. (2023). Brain Dissection: fMRI-trained 353
Networks Reveal Spatial Selectivity in the Processing of Natural Images. In Thirty-seventh Con- 354
ference on Neural Information Processing Systems . 9 355
10Schrimpf, M., Kubilius, J., Lee, M. J., Murty, N. A. R., Ajemian, R., and DiCarlo, J. J. (2020). Inte- 356
grative Benchmarking to Advance Neurally Mechanistic Models of Human Intelligence. Neuron . 357
9 358
Shi, J. and Malik, J. (2000). Normalized cuts and image segmentation. IEEE Transactions on 359
Pattern Analysis and Machine Intelligence , 22(8):888–905. 4, 9 360
Sun, M., Chen, X., Kolter, J. Z., and Liu, Z. (2024). Massive Activations in Large Language Models. 361
arXiv:2402.17762 [cs]. 9 362
von Luxburg, U. (2007). A tutorial on spectral clustering. Statistics and Computing , 17(4):395–416. 363
9 364
Wang, K. R., Variengien, A., Conmy, A., Shlegeris, B., and Steinhardt, J. (2023a). Interpretabil- 365
ity in the Wild: a Circuit for Indirect Object Identification in GPT-2 Small. In The Eleventh 366
International Conference on Learning Representations . 9 367
Wang, X., Girdhar, R., Yu, S. X., and Misra, I. (2023b). Cut and learn for unsupervised object 368
detection and instance segmentation. In Proceedings of the IEEE/CVF Conference on Computer 369
Vision and Pattern Recognition , pages 3124–3134. 9 370
Wu, Z., Xiong, Y ., Stella, X. Y ., and Lin, D. (2018). Unsupervised Feature Learning via Non- 371
Parametric Instance Discrimination. In Proceedings of the IEEE Conference on Computer Vision 372
and Pattern Recognition . 9 373
Yang, H., Gee, J., and Shi, J. (2024). Brain Decodes Deep Nets. arXiv:2312.01280 [cs]. 1 374
Zhang, X., Yunis, D., and Maire, M. (2023). Deciphering ’What’ and ’Where’ Visual Pathways 375
from Spectral Clustering of Layer-Distributed Neural Representations. arXiv:2312.06716 [cs]. 9 376
11A Appendix overview 377
1. Appendix B summarizes background of brain ROIs. 378
2. Appendix C is implementation details 379
2.1. Additional regularization terms 380
2.2. Brain encoding model training loss function 381
2.3. Unsupervised segmentation evaluation pipeline 382
2.4. Nystrom-like approximation for t-SNE 383
3. Appendix D lists more image examples from the 3D spectral-tSNE. 384
4. Appendix E lists figure-ground channel activation for every model and layer. 385
5. Appendix F lists more example category-specific visual concepts. 386
6. Appendix G lists more example pixels from the 2D spectral-tSNE information flow. 387
12B Brain Region Background Knowledge 388
Figure 11: Brain Region of Interests (ROIs) . V1v: ventral stream, V1d: dorsal stream.
Table 2: Known function and selectivity of brain region of interests (ROIs).
ROI name V1 V2 V3 V4 EBA FBA OFA FFA OPA PPA OWFA VWFA
Known Function/Selectivity primary visual mid-level body face navigation scene words
This section briefly summarizes the known functions of key brain regions of interest (ROIs). Fig- 389
ure 11 provides an overview of these brain ROIs. Table 2 lists the known functions and selectivities 390
for each ROI. 391
In brief, V1 to V3 are the primary visual stream, which is further divided into ventral (lower) and dor- 392
sal (upper) streams. V4 is a mid-level visual area. EBA (extrastriate body area) and FBA (fusiform 393
body area) are selectively responsive to bodies, while FFA (fusiform face area) and OFA (occipital 394
face area) show selectivity for faces. OWFA (occipital word form area) and VWFA (visual word 395
form area) are selective for written words. PPA (parahippocampal place area) exhibits selectivity for 396
scenes and places, and OPA (occipital place area) is involved in navigation and spatial reasoning. 397
Visual information processing in the brain follows a hierarchical, feedforward organization. Be- 398
ginning in the primary visual cortex (V1) and progressing through higher visual areas like V2, V3, 399
and V4, neurons exhibit increasingly large receptive fields and represent increasingly abstract visual 400
concepts. While neurons in V1 encode low-level features like edges and orientations within a small 401
portion of the visual field, neurons in V4 synthesize more complex patterns and object representa- 402
tions across a larger area of the visual input. 403
13C Implementation Details 404
C.1 Additional Regularization for Channel Align Transformation 405
Additional Regularization are added to the channel align transform to ensure good properties of the 406
aligned features: 1) zero-centered, 2) small covariance between channels, and 3) focal loss. 407
Zero-centered regularization. We did not apply z-score normalization to the extracted features; 408
instead, we added a regularization term to ensure the transformed features are zero-centered. Recall 409
that the channel-aligned transformed feature V′∈RM×D′, where Mis the number of data points 410
andD′is the hidden dimension. The zero-center loss is defined as: 411
Lzero=1
D′1
MX
i≤M,j≤D′v′
ij (6)
Covariance regularization. We used the covariance loss to minimize the off-diagonal elements in 412
the covariance matrix of the transformed feature C(V′), aiming to bring them close to 0. Recall 413
that channel align transformed feature V′∈RM×D′, where Mis number of data, D′is the hidden 414
dimension. The covariance loss is defined as: 415
Lcov=1
D′X
i̸=j[C(V′)]2
i,j,where C(V′) =1
M−1MX
i=1 
v′
i−¯v′ 
v′
i−¯v′T,¯v′=1
MMX
i=1v′
i.
(7)
Focal Loss. Lin et al. (2017) introduced focal loss, which dynamically assigns smaller weights to 416
the loss function for hard-to-classify classes. In our scenario, we apply spectral clustering on the 417
affinity matrix Aa∈RM×Mafter performing the channel alignment transform, where Mrepresents 418
the number of data points. Due to the characteristics of spectral clustering, disconnected edges 419
play a more critical role than connected edges. Adding an edge between disconnected clusters 420
significantly reshapes the eigenvectors, while adding edges to connected clusters has only a minor 421
impact. Therefore, we aim to assign larger weights to disconnected edges in the loss function: 422
Leigen =∥(XbXT
b−XaXT
a)∗exp(−Ab)∥ (8)
where Ab∈RM×Mis the affinity matrix before the channel alignment transform, element wise dot- 423
product to exp(−Ab)assigned larger wights for disconnected edges. Xb∈RM×C,Xa∈RM×C424
are eigenvectors before and after channel align transform, respectively. 425
C.2 Brain Encoding Model Training Loss 426
LetY∈R1×Nrepresent the brain prediction target, where Nis the number of flattened 3D brain 427
voxels, and the 1indicates that each voxel’s response is a scalar value. ˆYis the model’s predicted 428
brain response. The brain encoding model training loss is the L1 loss: 429
Lbrain =∥Y−ˆY∥ (9)
C.3 Total Training Loss 430
The total training loss is a combination of the following components: 1) brain encoding model loss, 431
2) eigen-constraint regularization, 3) zero-centered regularization, and 4) covariance regularization: 432
L=Lbrain +λeigenLeigen +λzeroLzero+λcovLcov (10)
where we set λeigen = 1,λzero= 0.01,λcov= 0.01. 433
14C.4 Oracle-based Unsupervised Segmentation Evaluation Pipeline 434
Our unsupervised segmentation pipeline aims to benchmark and compare the performance across 435
each single layer of the CLIP model. The evaluation pipeline is oracle-based: 436
1. Apply spectral clustering jointly across all images, taking the top 10 eigenvectors. 437
2. For each class of object (plus one background class), use ground-truth labels from the dataset 438
to mask out the pixels and their eigenvectors, and then use the mean of the eigenvectors to define a 439
center for each class. 440
3. Compute the cosine similarity of each pixel to all class centers. 441
4. For each pixel, if the maximum similarity to all classes is less than a threshold value, assign 442
this pixel to the background class. 443
5. Assign pixels (with a similarity greater than the threshold value) to the class with the maximum 444
similarity. 445
There’s one hyper-parameter, the threshold value that requires different optimal value for each layer 446
of CLIP. To ensure a fair comparison across all layers, the threshold value is grid-searched from 10 447
evenly spaced values between 0 and 1, the maximum mIoU score in the grid search is taken for each 448
layer. 449
C.5 Nystrom-like approximation for t-SNE 450
To visualize the eigenvectors, we applied t-SNE to the eigenvectors X∈RM×C, where the number 451
of data points Mspan the product space of models, layers, pixels, and images. Due to the enormous 452
size of M= 7e+6 nodes, t-SNE suffered from complexity scaling issues. We again applied our 453
Nystrom-like approximation to t-SNE, with sub-sampled m= 10e+4 nodes and KNN K= 1. 454
It’s worth noting that, since the non-linear distance adjustment in t-SNE, it’s crucial to use only one 455
nearest neighbor K= 1for t-SNE. 456
C.6 Computation Resource 457
All of our experiments are performed on one consumer-grade RTX 4090 GPU. The brain encoding 458
model training took 3 hours on 4GB of VRAM, spectral clustering eigen-decomposition on large 459
graph took 10 minutes on 10GB of VRAM and 60GB of CPU RAM. 460
C.7 Code Release 461
Our code will be publicly released upon publication. 462
153D Spectral-tSNE
D 3D spectral-tSNE 463
CLIPLayer0
 Layer1
 Layer2
 Layer3
 Layer4
 Layer5
 Layer6
 Layer7
 Layer8
 Layer9
 Layer10
 Layer11
DINO
 MAE
CLIPLayer0
 Layer1
 Layer2
 Layer3
 Layer4
 Layer5
 Layer6
 Layer7
 Layer8
 Layer9
 Layer10
 Layer11
DINO
 MAE
CLIPLayer0
 Layer1
 Layer2
 Layer3
 Layer4
 Layer5
 Layer6
 Layer7
 Layer8
 Layer9
 Layer10
 Layer11
DINO
 MAE
CLIPLayer0
 Layer1
 Layer2
 Layer3
 Layer4
 Layer5
 Layer6
 Layer7
 Layer8
 Layer9
 Layer10
 Layer11
DINO
 MAE
CLIPLayer0
 Layer1
 Layer2
 Layer3
 Layer4
 Layer5
 Layer6
 Layer7
 Layer8
 Layer9
 Layer10
 Layer11
DINO
 MAE
CLIPLayer0
 Layer1
 Layer2
 Layer3
 Layer4
 Layer5
 Layer6
 Layer7
 Layer8
 Layer9
 Layer10
 Layer11
DINO
 MAE
Figure 12: Spectral clustering in the universal channel aligned feature space. The image pixels are
colored by our approach AlignedCut, the pixel RGB value is assigned by the 3D spectral-tSNE of
the top 20 eigenvectors. The coloring is consistent across all images, layers, and models.
163D Spectral-tSNE
CLIPLayer0
 Layer1
 Layer2
 Layer3
 Layer4
 Layer5
 Layer6
 Layer7
 Layer8
 Layer9
 Layer10
 Layer11
DINO
 MAE
CLIPLayer0
 Layer1
 Layer2
 Layer3
 Layer4
 Layer5
 Layer6
 Layer7
 Layer8
 Layer9
 Layer10
 Layer11
DINO
 MAE
CLIPLayer0
 Layer1
 Layer2
 Layer3
 Layer4
 Layer5
 Layer6
 Layer7
 Layer8
 Layer9
 Layer10
 Layer11
DINO
 MAE
CLIPLayer0
 Layer1
 Layer2
 Layer3
 Layer4
 Layer5
 Layer6
 Layer7
 Layer8
 Layer9
 Layer10
 Layer11
DINO
 MAE
CLIPLayer0
 Layer1
 Layer2
 Layer3
 Layer4
 Layer5
 Layer6
 Layer7
 Layer8
 Layer9
 Layer10
 Layer11
DINO
 MAE
CLIPLayer0
 Layer1
 Layer2
 Layer3
 Layer4
 Layer5
 Layer6
 Layer7
 Layer8
 Layer9
 Layer10
 Layer11
DINO
 MAE
Figure 13: Spectral clustering in the universal channel aligned feature space. The image pixels are
colored by our approach AlignedCut, the pixel RGB value is assigned by the 3D spectral-tSNE of
the top 20 eigenvectors. The coloring is consistent across all images, layers, and models.
173D Spectral-tSNE
CLIPLayer0
 Layer1
 Layer2
 Layer3
 Layer4
 Layer5
 Layer6
 Layer7
 Layer8
 Layer9
 Layer10
 Layer11
DINO
 MAE
CLIPLayer0
 Layer1
 Layer2
 Layer3
 Layer4
 Layer5
 Layer6
 Layer7
 Layer8
 Layer9
 Layer10
 Layer11
DINO
 MAE
CLIPLayer0
 Layer1
 Layer2
 Layer3
 Layer4
 Layer5
 Layer6
 Layer7
 Layer8
 Layer9
 Layer10
 Layer11
DINO
 MAE
CLIPLayer0
 Layer1
 Layer2
 Layer3
 Layer4
 Layer5
 Layer6
 Layer7
 Layer8
 Layer9
 Layer10
 Layer11
DINO
 MAE
CLIPLayer0
 Layer1
 Layer2
 Layer3
 Layer4
 Layer5
 Layer6
 Layer7
 Layer8
 Layer9
 Layer10
 Layer11
DINO
 MAE
CLIPLayer0
 Layer1
 Layer2
 Layer3
 Layer4
 Layer5
 Layer6
 Layer7
 Layer8
 Layer9
 Layer10
 Layer11
DINO
 MAE
Figure 14: Spectral clustering in the universal channel aligned feature space. The image pixels are
colored by our approach AlignedCut, the pixel RGB value is assigned by the 3D spectral-tSNE of
the top 20 eigenvectors. The coloring is consistent across all images, layers, and models.
183D Spectral-tSNE
CLIPLayer0
 Layer1
 Layer2
 Layer3
 Layer4
 Layer5
 Layer6
 Layer7
 Layer8
 Layer9
 Layer10
 Layer11
DINO
 MAE
CLIPLayer0
 Layer1
 Layer2
 Layer3
 Layer4
 Layer5
 Layer6
 Layer7
 Layer8
 Layer9
 Layer10
 Layer11
DINO
 MAE
CLIPLayer0
 Layer1
 Layer2
 Layer3
 Layer4
 Layer5
 Layer6
 Layer7
 Layer8
 Layer9
 Layer10
 Layer11
DINO
 MAE
CLIPLayer0
 Layer1
 Layer2
 Layer3
 Layer4
 Layer5
 Layer6
 Layer7
 Layer8
 Layer9
 Layer10
 Layer11
DINO
 MAE
CLIPLayer0
 Layer1
 Layer2
 Layer3
 Layer4
 Layer5
 Layer6
 Layer7
 Layer8
 Layer9
 Layer10
 Layer11
DINO
 MAE
CLIPLayer0
 Layer1
 Layer2
 Layer3
 Layer4
 Layer5
 Layer6
 Layer7
 Layer8
 Layer9
 Layer10
 Layer11
DINO
 MAE
Figure 15: Spectral clustering in the universal channel aligned feature space. The image pixels are
colored by our approach AlignedCut, the pixel RGB value is assigned by the 3D spectral-tSNE of
the top 20 eigenvectors. The coloring is consistent across all images, layers, and models.
19Figure-ground Channel Activation
E Figure-ground Channel Activation from All Layers and Models 464
foregroundCLIP L0
 CLIP L1
 CLIP L2
 CLIP L3
 CLIP L4
 CLIP L5
 CLIP L6
 CLIP L7
 CLIP L8
 CLIP L9
 CLIP L10
 CLIP L11
foregroundDINO L0
 DINO L1
 DINO L2
 DINO L3
 DINO L4
 DINO L5
 DINO L6
 DINO L7
 DINO L8
 DINO L9
 DINO L10
 DINO L11
foregroundMAE L0
 MAE L1
 MAE L2
 MAE L3
 MAE L4
 MAE L5
 MAE L6
 MAE L7
 MAE L8
 MAE L9
 MAE L10
 MAE L11
backgroundCLIP L0
 CLIP L1
 CLIP L2
 CLIP L3
 CLIP L4
 CLIP L5
 CLIP L6
 CLIP L7
 CLIP L8
 CLIP L9
 CLIP L10
 CLIP L11
backgroundDINO L0
 DINO L1
 DINO L2
 DINO L3
 DINO L4
 DINO L5
 DINO L6
 DINO L7
 DINO L8
 DINO L9
 DINO L10
 DINO L11
backgroundMAE L0
 MAE L1
 MAE L2
 MAE L3
 MAE L4
 MAE L5
 MAE L6
 MAE L7
 MAE L8
 MAE L9
 MAE L10
 MAE L11
Figure 16: Mean activation of foreground or background pixels at each layer of CLIP, DINO and
MAE. Channel activations are linearly transformed to the brain’s space. Large absolute activation
value means more consistent visual concepts.
20Visual Concepts: Category-specific
F Visual Concepts: Categories 465
Figure 17: Category visual concepts in CLIP Layer 9. Left: Mean activation of all pixels within an
Euclidean sphere centered at the visual concept in the 3D spectral-tSNE space; the concepts activate
different brain regions. Middle: The standard deviation negatively correlates with absolute mean
activations. Right: Spectral clustering, colored by 3D spectral-tSNE of the top 20 eigenvectors.
212D Spectral-tSNE Space Information Flow
G Layer-to-Layer Feature Computation Flow in 2D spectral-tSNE space 466
Figure 18: Trajectory of feature progression in from layer to layer, in the 2D spectral-tSNE space.
Arrows displayed for 10 randomly sampled example pixels. Top Right: Pixels are colored by unsu-
pervised segmentation.
222D Spectral-tSNE Space Information Flow
Figure 19: Trajectory of feature progression in from layer to layer, in the 2D spectral-tSNE space.
Arrows displayed for 10 randomly sampled example pixels. Top Right: Pixels are colored by unsu-
pervised segmentation.
232D Spectral-tSNE Space Information Flow
Figure 20: Trajectory of feature progression in from layer to layer, in the 2D spectral-tSNE space.
Arrows displayed for 10 randomly sampled example pixels. Top Right: Pixels are colored by unsu-
pervised segmentation.
242D Spectral-tSNE Space Information Flow
Figure 21: Trajectory of feature progression in from layer to layer, in the 2D spectral-tSNE space.
Arrows displayed for 10 randomly sampled example pixels. Top Right: Pixels are colored by unsu-
pervised segmentation.
252D Spectral-tSNE Space Information Flow
Figure 22: Trajectory of feature progression in from layer to layer, in the 2D spectral-tSNE space.
Arrows displayed for 10 randomly sampled example pixels. Top Right: Pixels are colored by unsu-
pervised segmentation.
26NeurIPS Paper Checklist 467
1.Claims 468
Question: Do the main claims made in the abstract and introduction accurately reflect the 469
paper’s contributions and scope? 470
Answer: [Yes] 471
Justification: 472
Guidelines: 473
• The answer NA means that the abstract and introduction do not include the claims 474
made in the paper. 475
• The abstract and/or introduction should clearly state the claims made, including the 476
contributions made in the paper and important assumptions and limitations. A No or 477
NA answer to this question will not be perceived well by the reviewers. 478
• The claims made should match theoretical and experimental results, and reflect how 479
much the results can be expected to generalize to other settings. 480
• It is fine to include aspirational goals as motivation as long as it is clear that these 481
goals are not attained by the paper. 482
2.Limitations 483
Question: Does the paper discuss the limitations of the work performed by the authors? 484
Answer: [Yes] 485
Justification: 486
Guidelines: 487
• The answer NA means that the paper has no limitation while the answer No means 488
that the paper has limitations, but those are not discussed in the paper. 489
• The authors are encouraged to create a separate ”Limitations” section in their paper. 490
• The paper should point out any strong assumptions and how robust the results are to 491
violations of these assumptions (e.g., independence assumptions, noiseless settings, 492
model well-specification, asymptotic approximations only holding locally). The au- 493
thors should reflect on how these assumptions might be violated in practice and what 494
the implications would be. 495
• The authors should reflect on the scope of the claims made, e.g., if the approach was 496
only tested on a few datasets or with a few runs. In general, empirical results often 497
depend on implicit assumptions, which should be articulated. 498
• The authors should reflect on the factors that influence the performance of the ap- 499
proach. For example, a facial recognition algorithm may perform poorly when image 500
resolution is low or images are taken in low lighting. Or a speech-to-text system might 501
not be used reliably to provide closed captions for online lectures because it fails to 502
handle technical jargon. 503
• The authors should discuss the computational efficiency of the proposed algorithms 504
and how they scale with dataset size. 505
• If applicable, the authors should discuss possible limitations of their approach to ad- 506
dress problems of privacy and fairness. 507
• While the authors might fear that complete honesty about limitations might be used by 508
reviewers as grounds for rejection, a worse outcome might be that reviewers discover 509
limitations that aren’t acknowledged in the paper. The authors should use their best 510
judgment and recognize that individual actions in favor of transparency play an impor- 511
tant role in developing norms that preserve the integrity of the community. Reviewers 512
will be specifically instructed to not penalize honesty concerning limitations. 513
3.Theory Assumptions and Proofs 514
Question: For each theoretical result, does the paper provide the full set of assumptions and 515
a complete (and correct) proof? 516
Answer: [NA] 517
27Justification: 518
Guidelines: 519
• The answer NA means that the paper does not include theoretical results. 520
• All the theorems, formulas, and proofs in the paper should be numbered and cross- 521
referenced. 522
• All assumptions should be clearly stated or referenced in the statement of any theo- 523
rems. 524
• The proofs can either appear in the main paper or the supplemental material, but if 525
they appear in the supplemental material, the authors are encouraged to provide a 526
short proof sketch to provide intuition. 527
• Inversely, any informal proof provided in the core of the paper should be comple- 528
mented by formal proofs provided in appendix or supplemental material. 529
• Theorems and Lemmas that the proof relies upon should be properly referenced. 530
4.Experimental Result Reproducibility 531
Question: Does the paper fully disclose all the information needed to reproduce the main 532
experimental results of the paper to the extent that it affects the main claims and/or conclu- 533
sions of the paper (regardless of whether the code and data are provided or not)? 534
Answer: [Yes] 535
Justification: Experimental details are in the appendix 536
Guidelines: 537
• The answer NA means that the paper does not include experiments. 538
• If the paper includes experiments, a No answer to this question will not be perceived 539
well by the reviewers: Making the paper reproducible is important, regardless of 540
whether the code and data are provided or not. 541
• If the contribution is a dataset and/or model, the authors should describe the steps 542
taken to make their results reproducible or verifiable. 543
• Depending on the contribution, reproducibility can be accomplished in various ways. 544
For example, if the contribution is a novel architecture, describing the architecture 545
fully might suffice, or if the contribution is a specific model and empirical evaluation, 546
it may be necessary to either make it possible for others to replicate the model with 547
the same dataset, or provide access to the model. In general. releasing code and data 548
is often one good way to accomplish this, but reproducibility can also be provided via 549
detailed instructions for how to replicate the results, access to a hosted model (e.g., in 550
the case of a large language model), releasing of a model checkpoint, or other means 551
that are appropriate to the research performed. 552
• While NeurIPS does not require releasing code, the conference does require all sub- 553
missions to provide some reasonable avenue for reproducibility, which may depend 554
on the nature of the contribution. For example 555
(a) If the contribution is primarily a new algorithm, the paper should make it clear 556
how to reproduce that algorithm. 557
(b) If the contribution is primarily a new model architecture, the paper should describe 558
the architecture clearly and fully. 559
(c) If the contribution is a new model (e.g., a large language model), then there should 560
either be a way to access this model for reproducing the results or a way to re- 561
produce the model (e.g., with an open-source dataset or instructions for how to 562
construct the dataset). 563
(d) We recognize that reproducibility may be tricky in some cases, in which case au- 564
thors are welcome to describe the particular way they provide for reproducibility. 565
In the case of closed-source models, it may be that access to the model is limited in 566
some way (e.g., to registered users), but it should be possible for other researchers 567
to have some path to reproducing or verifying the results. 568
5.Open access to data and code 569
Question: Does the paper provide open access to the data and code, with sufficient instruc- 570
tions to faithfully reproduce the main experimental results, as described in supplemental 571
material? 572
28Answer: [No] 573
Justification: The data is provided as open-source from another study; our code is not yet 574
released, it will be released upon publication. 575
Guidelines: 576
• The answer NA means that paper does not include experiments requiring code. 577
• Please see the NeurIPS code and data submission guidelines ( https://nips.cc/ 578
public/guides/CodeSubmissionPolicy ) for more details. 579
• While we encourage the release of code and data, we understand that this might not 580
be possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not 581
including code, unless this is central to the contribution (e.g., for a new open-source 582
benchmark). 583
• The instructions should contain the exact command and environment needed to run to 584
reproduce the results. See the NeurIPS code and data submission guidelines ( https: 585
//nips.cc/public/guides/CodeSubmissionPolicy ) for more details. 586
• The authors should provide instructions on data access and preparation, including how 587
to access the raw data, preprocessed data, intermediate data, and generated data, etc. 588
• The authors should provide scripts to reproduce all experimental results for the new 589
proposed method and baselines. If only a subset of experiments are reproducible, they 590
should state which ones are omitted from the script and why. 591
• At submission time, to preserve anonymity, the authors should release anonymized 592
versions (if applicable). 593
• Providing as much information as possible in supplemental material (appended to the 594
paper) is recommended, but including URLs to data and code is permitted. 595
6.Experimental Setting/Details 596
Question: Does the paper specify all the training and test details (e.g., data splits, hyper- 597
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the 598
results? 599
Answer: [Yes] 600
Justification: Experimental details are in the appendix 601
Guidelines: 602
• The answer NA means that the paper does not include experiments. 603
• The experimental setting should be presented in the core of the paper to a level of 604
detail that is necessary to appreciate the results and make sense of them. 605
• The full details can be provided either with the code, in appendix, or as supplemental 606
material. 607
7.Experiment Statistical Significance 608
Question: Does the paper report error bars suitably and correctly defined or other appropri- 609
ate information about the statistical significance of the experiments? 610
Answer: [Yes] 611
Justification: We provided standard deviation in Table 1, measured over training with 3 612
random seed. 613
Guidelines: 614
• The answer NA means that the paper does not include experiments. 615
• The authors should answer ”Yes” if the results are accompanied by error bars, confi- 616
dence intervals, or statistical significance tests, at least for the experiments that support 617
the main claims of the paper. 618
• The factors of variability that the error bars are capturing should be clearly stated (for 619
example, train/test split, initialization, random drawing of some parameter, or overall 620
run with given experimental conditions). 621
• The method for calculating the error bars should be explained (closed form formula, 622
call to a library function, bootstrap, etc.) 623
• The assumptions made should be given (e.g., Normally distributed errors). 624
29• It should be clear whether the error bar is the standard deviation or the standard error 625
of the mean. 626
• It is OK to report 1-sigma error bars, but one should state it. The authors should prefer- 627
ably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of 628
Normality of errors is not verified. 629
• For asymmetric distributions, the authors should be careful not to show in tables or 630
figures symmetric error bars that would yield results that are out of range (e.g. negative 631
error rates). 632
• If error bars are reported in tables or plots, The authors should explain in the text how 633
they were calculated and reference the corresponding figures or tables in the text. 634
8.Experiments Compute Resources 635
Question: For each experiment, does the paper provide sufficient information on the com- 636
puter resources (type of compute workers, memory, time of execution) needed to reproduce 637
the experiments? 638
Answer: [Yes] 639
Justification: 640
Guidelines: 641
• The answer NA means that the paper does not include experiments. 642
• The paper should indicate the type of compute workers CPU or GPU, internal cluster, 643
or cloud provider, including relevant memory and storage. 644
• The paper should provide the amount of compute required for each of the individual 645
experimental runs as well as estimate the total compute. 646
• The paper should disclose whether the full research project required more compute 647
than the experiments reported in the paper (e.g., preliminary or failed experiments 648
that didn’t make it into the paper). 649
9.Code Of Ethics 650
Question: Does the research conducted in the paper conform, in every respect, with the 651
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines ? 652
Answer: [Yes] 653
Justification: 654
Guidelines: 655
• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. 656
• If the authors answer No, they should explain the special circumstances that require a 657
deviation from the Code of Ethics. 658
• The authors should make sure to preserve anonymity (e.g., if there is a special consid- 659
eration due to laws or regulations in their jurisdiction). 660
10.Broader Impacts 661
Question: Does the paper discuss both potential positive societal impacts and negative 662
societal impacts of the work performed? 663
Answer: [NA] 664
Justification: 665
Guidelines: 666
• The answer NA means that there is no societal impact of the work performed. 667
• If the authors answer NA or No, they should explain why their work has no societal 668
impact or why the paper does not address societal impact. 669
• Examples of negative societal impacts include potential malicious or unintended uses 670
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations 671
(e.g., deployment of technologies that could make decisions that unfairly impact spe- 672
cific groups), privacy considerations, and security considerations. 673
30• The conference expects that many papers will be foundational research and not tied 674
to particular applications, let alone deployments. However, if there is a direct path to 675
any negative applications, the authors should point it out. For example, it is legitimate 676
to point out that an improvement in the quality of generative models could be used to 677
generate deepfakes for disinformation. On the other hand, it is not needed to point out 678
that a generic algorithm for optimizing neural networks could enable people to train 679
models that generate Deepfakes faster. 680
• The authors should consider possible harms that could arise when the technology is 681
being used as intended and functioning correctly, harms that could arise when the 682
technology is being used as intended but gives incorrect results, and harms following 683
from (intentional or unintentional) misuse of the technology. 684
• If there are negative societal impacts, the authors could also discuss possible mitiga- 685
tion strategies (e.g., gated release of models, providing defenses in addition to attacks, 686
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from 687
feedback over time, improving the efficiency and accessibility of ML). 688
11.Safeguards 689
Question: Does the paper describe safeguards that have been put in place for responsible 690
release of data or models that have a high risk for misuse (e.g., pretrained language models, 691
image generators, or scraped datasets)? 692
Answer: [NA] 693
Justification: 694
Guidelines: 695
• The answer NA means that the paper poses no such risks. 696
• Released models that have a high risk for misuse or dual-use should be released with 697
necessary safeguards to allow for controlled use of the model, for example by re- 698
quiring that users adhere to usage guidelines or restrictions to access the model or 699
implementing safety filters. 700
• Datasets that have been scraped from the Internet could pose safety risks. The authors 701
should describe how they avoided releasing unsafe images. 702
• We recognize that providing effective safeguards is challenging, and many papers do 703
not require this, but we encourage authors to take this into account and make a best 704
faith effort. 705
12.Licenses for existing assets 706
Question: Are the creators or original owners of assets (e.g., code, data, models), used in 707
the paper, properly credited and are the license and terms of use explicitly mentioned and 708
properly respected? 709
Answer: [Yes] 710
Justification: 711
Guidelines: 712
• The answer NA means that the paper does not use existing assets. 713
• The authors should cite the original paper that produced the code package or dataset. 714
• The authors should state which version of the asset is used and, if possible, include a 715
URL. 716
• The name of the license (e.g., CC-BY 4.0) should be included for each asset. 717
• For scraped data from a particular source (e.g., website), the copyright and terms of 718
service of that source should be provided. 719
• If assets are released, the license, copyright information, and terms of use in the pack- 720
age should be provided. For popular datasets, paperswithcode.com/datasets has 721
curated licenses for some datasets. Their licensing guide can help determine the li- 722
cense of a dataset. 723
• For existing datasets that are re-packaged, both the original license and the license of 724
the derived asset (if it has changed) should be provided. 725
31• If this information is not available online, the authors are encouraged to reach out to 726
the asset’s creators. 727
13.New Assets 728
Question: Are new assets introduced in the paper well documented and is the documenta- 729
tion provided alongside the assets? 730
Answer: [NA] 731
Justification: 732
Guidelines: 733
• The answer NA means that the paper does not release new assets. 734
• Researchers should communicate the details of the dataset/code/model as part of their 735
submissions via structured templates. This includes details about training, license, 736
limitations, etc. 737
• The paper should discuss whether and how consent was obtained from people whose 738
asset is used. 739
• At submission time, remember to anonymize your assets (if applicable). You can 740
either create an anonymized URL or include an anonymized zip file. 741
14.Crowdsourcing and Research with Human Subjects 742
Question: For crowdsourcing experiments and research with human subjects, does the pa- 743
per include the full text of instructions given to participants and screenshots, if applicable, 744
as well as details about compensation (if any)? 745
Answer: [NA] 746
Justification: 747
Guidelines: 748
• The answer NA means that the paper does not involve crowdsourcing nor research 749
with human subjects. 750
• Including this information in the supplemental material is fine, but if the main contri- 751
bution of the paper involves human subjects, then as much detail as possible should 752
be included in the main paper. 753
• According to the NeurIPS Code of Ethics, workers involved in data collection, cura- 754
tion, or other labor should be paid at least the minimum wage in the country of the 755
data collector. 756
15.Institutional Review Board (IRB) Approvals or Equivalent for Research with Human 757
Subjects 758
Question: Does the paper describe potential risks incurred by study participants, whether 759
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) 760
approvals (or an equivalent approval/review based on the requirements of your country or 761
institution) were obtained? 762
Answer: [NA] 763
Justification: 764
Guidelines: 765
• The answer NA means that the paper does not involve crowdsourcing nor research 766
with human subjects. 767
• Depending on the country in which research is conducted, IRB approval (or equiva- 768
lent) may be required for any human subjects research. If you obtained IRB approval, 769
you should clearly state this in the paper. 770
• We recognize that the procedures for this may vary significantly between institutions 771
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the 772
guidelines for their institution. 773
• For initial submissions, do not include any information that would break anonymity 774
(if applicable), such as the institution conducting the review. 775
32