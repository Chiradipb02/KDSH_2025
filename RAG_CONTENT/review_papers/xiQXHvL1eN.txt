Published in Transactions on Machine Learning Research (10/2023)
Dynamic Regret Analysis of Safe Distributed Online
Optimization for Convex and Non-convex Problems
Ting-Jui Chang chang.tin@northeastern.edu
Department of Mechanical and Industrial Engineering
Northeastern University
Sapana Chaudhary sapanac@tamu.edu
Department of Electrical and Computer Engineering
Texas A&M University
Dileep Kalathil dileep.kalathil@tamu.edu
Department of Electrical and Computer Engineering
Texas A&M University
Shahin Shahrampour s.shahrampour@northeastern.edu
Department of Mechanical and Industrial Engineering
Northeastern University
Reviewed on OpenReview: https://openreview.net/forum?id=xiQXHvL1eN
Abstract
This paper addresses safe distributed online optimization over an unknown set of linear safety con-
straints. A network of agents aims at jointly minimizing a global ,time-varying function, which is
only partially observable to each individual agent. Therefore, agents must engage in local com-
munications to generate a safe sequence of actions competitive with the best minimizer sequence
in hindsight, and the gap between the performance of two sequences is quantified via dynamic re-
gret. We propose distributed safe online gradient descent (D-Safe-OGD) with an exploration phase,
where all agents estimate the constraint parameters collaboratively to build estimated feasible sets,
ensuring the action selection safety during the optimization phase. We prove that for convex func-
tions, D-Safe-OGD achieves a dynamic regret bound of O(T2/3√logT+T1/3C∗
T), whereC∗
T
denotes the path-length of the best minimizer sequence. We further prove a dynamic regret bound
ofO(T2/3√logT+T2/3C∗
T)for certain non-convex problems, which establishes the first dynamic
regret bound for a safe distributed algorithm in the non-convex setting.
1 Introduction
Online learning (or optimization) is a sequential decision-making problem modeling a repeated game between a learner
and an adversary (Hazan, 2016). At each round t,t∈[T]≜{1,...,T}, the learner chooses an action xtin a
convex setX⊆ Rdusing the information from previous observations and suffers the loss ft(xt), where the function
ft:X→ Ris chosen by the adversary. Due to the sequential nature of the problem, a commonly used performance
metric is regret , defined as the difference between the cumulative loss incurred by the learner and that of a benchmark
comparator sequence. When the comparator sequence is fixed, this metric is called static regret, defined as follows
Regs
T≜T/summationdisplay
t=1ft(xt)−min
x∈XT/summationdisplay
t=1ft(x). (1)
Static regret is well-studied in the online optimization literature. In particular, it is well-known that online gradient
descent (OGD) achieves an O(√
T)(respectively, O(logT)) static regret bound for convex (respectively, exp-concave
1Published in Transactions on Machine Learning Research (10/2023)
and strongly convex) problems (Zinkevich, 2003; Hazan et al., 2007), and these bounds are optimal in the sense of
matching the lower bound of regret in the respective problems (Hazan, 2016).
For non-convex problems, however, we expect that the standard regret notion used in the convex setting may not
be a tractable measure for gauging the algorithm performance. For example, in the context of online non-convex
optimization, Hazan et al. (2017) quantified the regret in terms of the norm of (projected) gradients, consistent with the
stationarity measure in offline optimization. More recently, Ghai et al. (2022) showed that under certain geometric and
smoothness conditions, OGD applied to non-convex functions is an approximation of online mirror descent (OMD)
applied to convex functions under a reparameterization. In view of this equivalence, OGD achieves an O(T2/3)static
regret that is defined in (1).
A more stringent benchmark for measuring the performance of online optimization algorithms is the dynamic regret
(Besbes et al., 2015; Jadbabaie et al., 2015), defined as
Regd
T≜T/summationdisplay
t=1ft(xt)−T/summationdisplay
t=1ft(x∗
t), (2)
where x∗
t≜argminx∈Xft(x). It is well-known that dynamic regret scales linearly with Tin the worst-case scenario,
when the function sequence fluctuates drastically over time. Therefore, various works have adopted a number of
variation measures to characterize the dynamic regret bound. We provide a detailed review of these measures in
Section 2 and describe the safe distributed online optimization problem (which is the focus of this work) in the next
section.
1.1 Safe Distributed Online Optimization
There are two distinctive components that make “safe distributed online optimization” more challenging than the
standard centralized online optimization:
(i)Distributed Setting: Distributed online optimization has been widely applied to robot formation control (Dixit
et al., 2019b), distributed target tracking (Shahrampour & Jadbabaie, 2017), and localization in sensor networks (Ak-
bari et al., 2015). In this setting, a network of magents aims at solving the online optimization problem collectively.
The main challenge is that the time-varying function sequence is only partially observable to each individual agent.
Each agentj∈[m]receives (gradient) information about the “local" function fj,t(·), but the objective is to control the
dynamic regret of each agent with respect to the global function ft(·) =/summationtextm
i=1fi,t(·), i.e.,
Regd
j,T≜T/summationdisplay
t=1ft(xj,t)−T/summationdisplay
t=1ft(x∗
t) =T/summationdisplay
t=1m/summationdisplay
i=1fi,t(xj,t)−T/summationdisplay
t=1ft(x∗
t). (3)
Therefore, despite availability of only local information, the action sequence of agent jis evaluated in the global
function and is compared to the global minimizer sequence. It is evident that agents must communicate with each
other (subject to a graph/network constraint) to approximate the global function, which is common to distributed
problems. The discussion on the network structure and communication protocols is provided in Section 3.3.
(ii)Safety Constraints: The literature on distributed online optimization has mostly focused on problems where the
constraint setXis known, and less attention has been given to problems with unknown feasible sets (see Section 2
for a comprehensive literature review). However, in many real-world applications, this set represents certain safety
constraints that are unknown in advance. Examples include voltage regulation constraints in power systems (Dobbe
et al., 2020), transmission bandwidth in communication networks due to human safety considerations (Luong et al.,
2019) and stabilizing action set in robotics applications (Åström & Murray, 2010). In these scenarios, one needs to
perform parameter estimation to learn the safety constraints while ensuring that the regret is as small as possible.
1.2 Contributions
In this work, we address the problem of distributed online optimization with unknown linear safety constraints. In
particular, the constraint set
Xs≜{x∈Rd:Ax≤b},
2Published in Transactions on Machine Learning Research (10/2023)
is linear, where Aisunknown and must be learned by agents to subsequently choose their actions from this set.
The superscript sinXsalludes to safety. Our specific objective is to study dynamic regret (2) for both convex and
non-convex problems when the set Xsis unknown to agents. Our contributions are three-fold:
1) We propose and analyze safe distributed online gradient descent (D-Safe-OGD) algorithm, which has two
phases (exploration and optimization). In the exploration phase, agents individually explore and jointly es-
timate the constraint parameters in a distributed fashion. Then, each agent constructs a feasible set with its
own estimate, which ensures the action selection safety with high probability (Lemma 2). Since the estimates
are only local, in the optimization phase, agents apply distributed OGD projected to different feasible sets,
which brings forward an additional technical challenge. We tackle this using the geometric property of linear
constraints (Fereydounian et al., 2020) as well as the sensitivity analysis of perturbed optimization problems
with second order regular constraints (Bonnans et al., 1998), which allows us to quantify the distance between
projections of a point to two different sets that are “close enough” to each other (Lemma 12).
2) We analyze D-Safe-OGD in the convex setting. Due to the challenge discussed in the previous item, we
cannot directly apply existing results on distributed online optimization with a common feasible set. The
agents must use the exploration time to estimate their own feasible sets, during which they incur linear regret.
Therefore, after striking a trade-off between the exploration and optimization phases, we prove (Theorem 3)
a dynamic regret bound of O(T2/3√logT+T1/3C∗
T), where
C∗
T≜T/summationdisplay
t=2/vextenddouble/vextenddoublex∗
t−x∗
t−1/vextenddouble/vextenddouble, (4)
is the path-length , defined with respect to the global minimizer sequence (Mokhtari et al., 2016; Jadbabaie
et al., 2015). If the problem is centralized (single agent) and the comparator sequence is fixed, i.e., x∗
t=x,
our result recovers the static regret bound of Chaudhary & Kalathil (2022).
3) We further analyze D-Safe-OGD in the non-convex setting. We draw upon the idea of the algorithmic equiv-
alence between OGD and OMD (Ghai et al., 2022) to establish that in certain problem geometries (Assump-
tions 6-9), D-Safe-OGD can be approximated by a distributed OMD algorithm applied to a reparameterized
“convex” problem. We prove that the dynamic regret is upper bounded by O(T2/3√logT+T2/3C∗
T)in
Theorem 5, which is the first dynamic regret bound for a safe distributed algorithm in the non-convex setting.
If the problem is centralized (single agent) and the comparator sequence is fixed, i.e., x∗
t=x, our result
recovers the static regret bound of Ghai et al. (2022) (disregarding log factors).
1.3 Summary of the Technical Analysis
We now elaborate on the technical contribution of this work:
1)Estimation Error
To learn the constraint parameters in a distributed way, in the early stage of D-Safe-OGD (i.e., Algorithm
1), agents apply random actions to collectively form a least squares (LS) problem. Then, a distributed opti-
mization algorithm is applied to jointly learn the constraint parameters. Though we apply EXTRA (Shi et al.,
2015) in this work, we note that this part can be replaced by other distributed optimization methods suitable
for smooth, strongly convex problems. Based on existing results on LS estimators (Theorem 7), matrix con-
centration inequalities for random matrices (Theorem 8), and the convergence rate of EXTRA (Theorem 10),
we quantify the estimation error bound for each agent in Lemma 2.
2)Gap between Projections on Two Estimated Feasible Sets
As mentioned earlier, since the estimated feasible sets are different across agents, in our technical analysis
we need to quantify the gap between projections on different estimated sets to derive the final regret bounds.
To tackle this, in Lemma 12 we show that if we consider the projection problem for one agent as a quadratic
programming with second-order cone constraints, the projection problem for any other agent can be cast as
a perturbed version of the first problem. Then, we build on the sensitivity analysis of optimization problems
(Theorem 13) to show that the difference between the projected points (optimal solutions of the original
3Published in Transactions on Machine Learning Research (10/2023)
and perturbed problems) is of the same order as the difference between agents estimates of the constraint
parameters.
3)Regret Bounds for Convex and Non-convex Cases
Building on the previous points, we extend the analysis of the distributed OGD in Yan et al. (2012) to the
safe setup (Lemma 14), from which we derive dynamic regret bounds for both convex and non-convex cases.
We note that for the non-convex (centralized) case, Ghai et al. (2022) showed that if certain geometric prop-
erties are satisfied, OGD on non-convex costs can be well-approximated as OMD on convex costs through
reparameterization (Lemma 15). We establish in Lemma 17 that this approximation also holds for the safe,
distributed setup with an extra error term due to the fact that the estimated feasible sets are different across
agents.
The proofs of our results are provided in the Appendix.
2 Related Literature
I) Centralized Online Optimization: For static regret, as defined in (1), it is well-known that the optimal re-
gret bound is O(√
T)(respectively, O(logT)) for convex (respectively, exp-concave and strongly convex) problems
(Hazan, 2016). For dynamic regret, various regularity measures have been considered. Let us first define the dy-
namic regret with respect to a general comparator sequence {ut}T
t=1and the corresponding regularity measure called
path-length as follows
Regd
T(u1,...,uT)≜T/summationdisplay
t=1ft(xt)−T/summationdisplay
t=1ft(ut),
CT(u1,...,uT)≜T/summationdisplay
t=2/vextenddouble/vextenddoubleut−ut−1/vextenddouble/vextenddouble.(5)
The regret definition above is widely known as universal dynamic regret. Zinkevich (2003) showed that for the convex
functions, OGD attains an upper bound of O(√
T(1 +CT))on the universal dynamic regret (as defined in (5)). This
universal dynamic regret bound was later improved to O(/radicalbig
T(1 +CT))using expert advice by Zhang et al. (2018a).
More recently, for the class of comparator sequences with path length/summationtextT
t=2/vextenddouble/vextenddoubleut−ut−1/vextenddouble/vextenddouble
1≤C, Baby & Wang
(2021) presented, for exp-concave losses, universal dynamic regret bounds of ˜O/parenleftbig
d3.5(max{T1/3C2/3,1})/parenrightbig
, whered
is the problem dimension and ˜O(·)hides the logarithmic factors when C≥1/Tand ofO(d1.5log(T))otherwise.
Later Baby & Wang (2022) presented ˜O(max{d1/3T1/3C2/3,d})universal dynamic regret bound for strongly convex
losses. For dynamic regret defined with respect to the minimizer sequence (as in (2)), Mokhtari et al. (2016) showed
a regret bound of O(C∗
T)for strongly convex and smooth functions with OGD. A related notion of higher-order path-
length defined as C∗
p,T≜/summationtextT
t=2/vextenddouble/vextenddoublex∗
t−x∗
t−1/vextenddouble/vextenddoublephas also been considered by several works. When the minimizer
sequence{x∗
t}T
t=1is uniformly bounded, O(C∗
p,T)impliesO(C∗
q,T)forq <p . Zhang et al. (2017a) proved that with
multiple gradient queries per round, the dynamic regret is improved to O(min{C∗
T,C∗
2,T}).
II) Distributed Online Optimization: Yan et al. (2012) studied distributed OGD for online optimization and proved
a static regret bound of O(√
T)(respectively, O(logT)) for convex (respectively, strongly-convex) functions. Dis-
tributed online optimization for time-varying network structures was then considered in (Mateos-Núnez & Cortés,
2014; Akbari et al., 2015; Hosseini et al., 2016). Shahrampour & Jadbabaie (2018) proposed a distributed OMD
algorithm with a dynamic regret bound of O(√
T(1 +C∗
T)). Dixit et al. (2019a) considered time-varying network
structures and showed that distributed proximal OGD achieves a dynamic regret of O(logT(1 +C∗
T))for strongly
convex functions. Zhang et al. (2019) developed a method based on gradient tracking and derived a regret bound in
terms ofC∗
Tand a gradient path-length. More recently, Eshraghi & Liang (2022) showed that dynamic regret for
strongly convex and smooth functions can be improved to O(1 +C∗
T)using both primal and dual information boosted
with multiple consensus steps. The non-convex case is also recently studied by Lu & Wang (2021), where the regret
is characterized by the first-order optimality condition. On the other hand, for the projection free setup, Zhang et al.
(2017b) presented a distributed online conditional gradient algorithm with a static regret bound of O(T3/4)and a
4Published in Transactions on Machine Learning Research (10/2023)
communication complexity of O(T). The communication complexity was further improved to O(√
T)in (Wan et al.,
2020).
Nevertheless, the works mentioned in (I)and(II)do not consider neither long-term nor safety constraints, which are
discussed next.
Table 1: Related works on centralized and distributed constrained online optimization for general functions with regret and con-
straint violation (CV) guarantees. Let g(x) = (g1(x),g2(x),...,g n(x))⊤be the vector formed by nconvex constraints. Let
c∈(0,1),α0>1and[a]+= max {0,a}. Problem type ‘C’ stands for centralized, ‘D’ stands for distributed (or decentralized),
‘CNX’ stands for convex cost functions, and ‘N-CNX’ stands for non-convex cost functions. Notes: †:The CV bound in (Yu &
Neely, 2020) can be further reduced to O(1)under a Slater’s condition assumption.
CV Type Problem Reference Regret Type Regret Bound CV Bound
/summationtextT
t=1gi(xt)∀i∈[n] C, CNX (Mahdavi et al., 2012) Static O(√
T) O(T3/4)/summationtextT
t=1maxi∈[n]gi(xt) C, CNX (Jenatton et al., 2016) Static O(Tmax{c,1−c}) O(T1−c/2)/summationtextT
t=1[gi(xt)]+∀i∈[n] C, CNX (Yuan & Lamperski, 2018) Static O(Tmax{c,1−c}) O(T1−c/2)/summationtextT
t=1∥[g(xt)]+∥ C, CNX (Yi et al., 2021) Static O(Tmax{c,1−c}) O(T(1−c)/2)/summationtextT
t=1∥[g(xt)]+∥ C, CNX (Yi et al., 2021) Dynamic O(/radicalbig
T(1 +CT)) O(√
T)/summationtextT
t=1gi(xt)∀i∈[n] C, CNX (Yu & Neely, 2020) Static O(√
T) O(T1/4)†/bracketleftig/summationtextT
t=1gi(xj,t)/bracketrightig
+, i= 1,∀j∈[m] D, CNX (Yuan et al., 2017) Static O(T1/2+c/2) O(T1−c/4)
/summationtextT
t=1/summationtextm
j=1/summationtextn
i=1/bracketleftbig
a⊤
ixj,t/bracketrightbig
+D, CNX (Yuan et al., 2020) Static O(√
T) O(T3/4)/summationtextT
t=1/summationtextm
j=1/summationtextn
i=1[gi(xj,t)]+D, CNX (Yuan et al., 2021) Static O(Tmax{c,1−c}) O(T1−c/2)
1
m/summationtextm
i=1/summationtextT
t=1/vextenddouble/vextenddouble/vextenddouble[gt(xi,t)]+/vextenddouble/vextenddouble/vextenddouble D, CNX (Yi et al., 2022) Dynamic O((α2
0T1−c+Tc(1 +CT))/α0)O(/radicalbig
(α0+ 1)T2−c)
/summationtextT
t=1∥[Axt−b]+∥ C, CNX (Chaudhary & Kalathil, 2022) Static O(/radicalbig
log(T)T2/3) 0/summationtextT
t=1/summationtextm
j=1∥[Axj,t−b]+∥ D, CNX This work Dynamic O/parenleftbig
T2/3√logT+T1/3C∗
T/parenrightbig
0/summationtextT
t=1/summationtextm
j=1∥[Axj,t−b]+∥ D, N-CNX This work Dynamic O/parenleftbig
T2/3√logT+T2/3C∗
T/parenrightbig
0
III) Constrained Online Optimization: Practical systems come with inherent system imposed constraints on the
decision variable. Some examples of such constraints are inventory/budget constraints in one-way trading problem
(Lin et al., 2019) and time coupling constraints in networked distributed energy systems (Fan et al., 2020). For a
known constraint set, projecting the decisions back to the constraint set is a natural way to incorporate constraints
in online convex optimization (OCO). Here, projected OGD for general cost functions achieves O(√
T)static regret.
However, for complex constraints, projection can induce computational burden. An early work by Hazan & Kale
(2012) solves the constrained optimization problem by replacing the quadratic convex program with simpler linear
program using Frank-Wolfe. For understanding the following references better, let X={x∈X⊆Rd:g(x)≤0}
whereg(x) = (g1(x),g2(x),...,gn(x))⊤is the vector formed by nconvex constraints, and Xis a closed convex set.
The work by Mahdavi et al. (2012) proposed to use a simpler closed form projection in place of true desired projec-
tion attaining O(√
T)static regret with/summationtextT
t=1gi(xt)≤O(T3/4)constraint violation ∀i∈[n]. Thus, their method
achieves optimal regret with lesser computation burden at the cost of incurring constraint violations. The follow up
work by Jenatton et al. (2016) proposed adaptive step size variant of Mahdavi et al. (2012) with O/parenleftbig
Tmax{c,1−c}/parenrightbig
static regret and O/parenleftbig
T1−c/2/parenrightbig
constraint violation for c∈(0,1). These bounds were further improved in Yu & Neely
(2020) with a static regret bound of O(√
T)and constraint violation bound of O(T1/4). Here, the constraint violation
is further reduced to O(1)whengi(x)satisfy Slater’s condition. The work by Yuan & Lamperski (2018) consid-
ered stricter ‘cumulative’ constraint violations of the form/summationtextT
t=1[gi(xt)]+∀i∈[n]and proposed algorithms with
O/parenleftbig
Tmax{c,1−c}/parenrightbig
static regret and O/parenleftbig
T1−c/2/parenrightbig
‘cumulative’ constraint violation for c∈(0,1). For strongly convex
functions, Yuan & Lamperski (2018) proved O(log(T))static regret and the constraint violation of respective form
isO(/radicalbig
log(T)T). More recently, the work by Yi et al. (2021) proposed an algorithm with O/parenleftbig
Tmax{c,1−c}/parenrightbig
regret
and/summationtextT
t=1/vextenddouble/vextenddouble[g(xt)]+/vextenddouble/vextenddouble≤O/parenleftbig
T(1−c)/2/parenrightbig
‘cumulative’ constraint violation. For strongly convex functions, Yi et al.
(2021) reduced both static regret and constraint violation bounds to O(log(T)). Further, Yi et al. (2021) presented a
bound ofO(/radicalbig
T(1 +CT))for dynamic regret with an O(√
T)‘cumulative’ constraint violation. The algorithms in
Mahdavi et al. (2012); Jenatton et al. (2016); Yu & Neely (2020); Yuan & Lamperski (2018); Yi et al. (2021) employ
some flavor of online primal-dual algorithms. A series of recent works (Sun et al., 2017; Chen et al., 2017; Neely &
Yu, 2017; Yu et al., 2017; Cao & Liu, 2018; Liu et al., 2022) have also dealt with time-varying constraints. Yu et al.
(2017) specifically work with ‘stochastic’ time varying constraints.
5Published in Transactions on Machine Learning Research (10/2023)
More recent works (Yuan et al., 2017; 2020; 2021; Yi et al., 2022) have looked at distributed OCO with long term
constraints. The work by Yuan et al. (2017) proposed a consensus based primal-dual sub-gradient algorithm with
O(T1/2+β0)regret andO(T1−β0/2)constraint violation for β0∈(0,0.5). Single constraint function was considered
in (Yuan et al., 2017), where constraint violation is of the form [/summationtextT
t=1gi(xj,t)]+, i= 1,∀j∈[m]. Yuan et al. (2020)
proposed algorithms for distributed online linear regression with O(√
T)regret andO(T3/4)constraint violation.
Here, constraint violation takes the form/summationtextT
t=1/summationtextm
j=1/summationtextn
i=1/bracketleftbig
a⊤
ixj,t/bracketrightbig
+, where ai,i∈[n]is a constraint vector. An-
other primal-dual algorithm was presented in Yuan et al. (2021) with O(Tmax{1−c,c})regret andO(T1−c/2)constraint
violation of the form/summationtextT
t=1/summationtextm
j=1/summationtextn
i=1[gi(xj,t)]+forc∈(0,1). In all of (Yuan et al., 2017; 2020; 2021) constraint
functions are known a priori. More recently, Yi et al. (2022) proposed algorithms for distributed OCO with time-
varying constraints, and for stricter ‘network’ constraint violation metric of the form1
m/summationtextm
i=1/summationtextT
t=1/vextenddouble/vextenddouble/vextenddouble[gt(xi,t)]+/vextenddouble/vextenddouble/vextenddouble.
The algorithm in (Yi et al., 2022) gives a dynamic regret of O((α2
0T1−c+Tc(1+CT))/α0)withO(/radicalbig
(α0+ 1)T2−c)
constraint violation for α0>1andc∈(0,1). Additionally, constrained distributed OCO with coupled inequality
constraints is considered in (Yi et al., 2020a;b); with bandit feedback on cost function is considered in (Li et al., 2020);
with partial constraint feedback is studied in (Sharma et al., 2021). For more references in this problem space, we
refer the readers to the survey in (Li et al., 2022).
IV) Safe Online Optimization: Safe online optimization is a fairly nascent field with only a few works studying
per-time safety in optimization problems. Amani et al. (2019); Khezeli & Bitar (2020) study the problem of safe linear
bandits giving O(log(T)√
T)regret with no constraint violation, albeit under an assumption that a lower bound on the
distance between the optimal action and safe set’s boundary is known. Without the knowledge of such a lower bound,
Amani et al. (2019) show O(log(T)T2/3)regret. Safe convex and non-convex optimization is studied in (Usmanova
et al., 2019; Fereydounian et al., 2020). Safety in the context of OCO is studied in Chaudhary & Kalathil (2022) with
a regret ofO(/radicalbig
log(T)T2/3).
Remark 1. Different from the works listed above, we study the problem of safe distributed online optimization with
unknown linear constraints. We consider both convex and non-convex cost functions.
3 Preliminaries
3.1 Notation
[m] The set{1,2,...,m}for any integer m/vextenddouble/vextenddouble·/vextenddouble/vextenddouble
FFrobenius norm of a matrix/vextenddouble/vextenddoubleX/vextenddouble/vextenddouble
V/radicalbig
trace(X⊤VX), for a matrix Xand a positive-definite matrix V
ΠX[·] The operator for the projection to set X
[A]ij The entry in the i-th row andj-th column of A
[A]i,: Thei-th row of A
[A]:,j Thej-th column of A
1 The vector of all ones
ei Thei-th basis vector
Jf(x) The Jacobian of a mapping f(·)atx
3.2 Strong Convexity and Bregman Divergence
Definition 1. A functionf:X→ Risµ-strongly convex ( µ>0) over the convex set Xif
f(x)≥f(y) +∇f(y)⊤(x−y) +µ
2/vextenddouble/vextenddoublex−y/vextenddouble/vextenddouble2,∀x,y∈X.
Definition 2. For a strongly convex function ϕ(·), the Bregman divergence w.r.t. ϕ(·)overXis defined as
Dϕ(x,y)≜ϕ(x)−ϕ(y)−∇ϕ(y)⊤(x−y),x,y∈X.
6Published in Transactions on Machine Learning Research (10/2023)
3.3 Network Structure
The underlying network topology is governed by a symmetric doubly stochastic matrix P, i.e., [P]ij≥0,∀i,j∈[m],
and each row (or column) is summed to one. If [P]ij>0, agentsiandjare considered neighbors, and agent iassigns
the weight [P]ijto agentjwhen they communicate with each other. We assume that the graph structure captured by
Pis connected, i.e., there is a (potentially multi-hop) path from any agent ito another agent j̸=i. Each agent is
considered as a neighbor of itself, i.e., [P]ii>0for anyi∈[m]. These constraints on the communication protocol
imply a geometric mixing bound for P(Liu, 2008), such that/summationtextm
j=1/vextendsingle/vextendsingle[Pk]ji−1/m/vextendsingle/vextendsingle≤√mβk,for anyi∈[m], where
βis the second largest singular value of P.
Remark 2. In all of the algorithms proposed in the paper, we will see Pas an input. This does not contradict
the decentralized nature of the algorithms, as agent ionly requires the knowledge of [P]ji>0for anyjin its
neighborhood. The knowledge of Pis not global, and each agent only has local information about it.
4 Safe Set Estimation
To keep the regret small, we first need to identify the linear safety constrains. It is impossible to learn the safety
constraints if the algorithm receives no information that can be used to estimate the unknown constraints (Chaudhary
& Kalathil, 2022). In our problem setup, we assume that the algorithm receives noisy observations of the form
ˆxi,t=Axi,t+wi,t∀i∈[m],
at every time step t, where the nature of noise wi,tis described below. Here, A∈Rn×d,b∈Rn, andnis the number
of constraints. Note that all agent updates are synchronous.
4.1 Assumptions
We make the following assumptions common to both the convex and the non-convex problem settings.
Assumption 1. The setXsis a closed polytope, hence, convex and compact. Also,/vextenddouble/vextenddoublex/vextenddouble/vextenddouble≤L,∀x∈ Xs, and
maxi∈[n]/vextenddouble/vextenddouble[A]i,:/vextenddouble/vextenddouble
2≤LA.
Assumption 2. The constraint noise sequence {wi,t,t∈[T]}is R-sub-Gaussian with respect to the filtration
{Fi,t,t∈[T]}, i.e.,∀t∈[T],∀i∈[m],E[wi,t|Fi,t−1] = 0 and we have for any σ∈R
E[exp(σx⊤wi,t)|Ft−1]≤exp(σ2R2/vextenddouble/vextenddoublex/vextenddouble/vextenddouble2/2).
Assumption 3. Every agent has knowledge of a safe baseline action xs∈Xssuch that Axs=bs<b. The agents
are aware of xsandbsand thus, the safety gap ∆s≜mini∈[n](bi−bs
i), wherebi(respectively, bs
i) denotes the i-th
element of b(respectively, bs).
The first assumption is typical to online optimization, and the second assumption on the noise is standard. The third
assumption stems from the requirement to be absolutely safe at every time step. The assumption warrants the need for
a safe starting point which is readily available in most practical problems of interest. Similar assumptions can be found
in previous literature on safe linear bandits (Amani et al., 2019; Khezeli & Bitar, 2020), safe convex and non-convex
optimization (Usmanova et al., 2019; Fereydounian et al., 2020), and safe online convex optimization (Chaudhary &
Kalathil, 2022).
4.2 Explore and Estimate
In this section, we present an algorithmic subroutine, Algorithm 1, for agents to obtain sufficiently good local estimates
ofXsbefore beginning to perform OGD. For the first T0time steps, each agent safely explores around the baseline
action xs. Each exploratory action is a γ-weighted combination of the baseline action and an i.i.d random vector ζi,t.
Here, for the agent i∈[m]at time stept∈[T0],γ∈[0,1), andζi,tis zero mean i.i.d random vector with ∥ζi,t∥≤L
andCov(ζi,t) =σ2
ζI.Performing exploration in this manner ensures per time step safety requirement as noted in
Lemma 1. The proof of lemma is immediate from the assumptions.
7Published in Transactions on Machine Learning Research (10/2023)
Lemma 1. (Lemma 1 in Chaudhary & Kalathil (2022)) Let Assumptions 1-3 hold. With γ=∆s
LLA,Axi,t≤bfor
eachxi,t= (1−γ)xs+γζi,t∀i∈[m],t∈[T0].
Once the data collection phase is finished, each agent i∈[m]constructs local function li(A)of the form
li(A)≜T0/summationdisplay
t=1/vextenddouble/vextenddoubleAxi,t−ˆxi,t/vextenddouble/vextenddouble2+λ
m/vextenddouble/vextenddoubleA/vextenddouble/vextenddouble2
F.
Then, for time steps t∈[T0+ 1,T0+T1], Alg. EXTRA (Shi et al., 2015) is used to solve the global Least Squares
(LS) estimation problem/summationtextm
i=1li(A)in a distributed fashion with a proper choice of α.
Lemma 2. Suppose Assumptions 1-2 hold. Let Algorithm 1 run with T0= Ω(L2
mγ2σ2
ζlog(d
δ))for data collection and
T1= Θ(logTρ), whereρis a positive constant. Denote the final output of the algorithm as /hatwideAifor agenti∈[m].
Then, with probability at least (1−2δ), we have∀k∈[n]and∀i,j∈[m]
/vextenddouble/vextenddouble[/hatwideAi]k,:−[A]k,:/vextenddouble/vextenddouble≤1
Tρ+R/radicalig
dlog/parenleftbig1+mT 0L2/λ
δ/n/parenrightbig
+√
λLA
/radicalig
1
2mγ2σ2
ζT0,
/vextenddouble/vextenddouble[/hatwideAi]k,:−[/hatwideAj]k,:/vextenddouble/vextenddouble≤2
Tρ,(6)
where [/hatwideAi]k,:and[A]k,:are thek-th rows of/hatwideAiandA, respectively.
It is worth noting that the safety gap ∆saffects the estimation error according to Lemma 2. As we mentioned earlier,
the exploratory action xi,t= (1−γ)xs+γζi,t, where the coefficient γ=∆s
LLA. Intuitively, if ∆sis larger, we can
put more weight on the exploration through ζi,t, which is beneficial to the estimation accuracy. We see from Equation
(6) that when γis larger, the estimation error bound is tighter.
Let us also discuss the computational complexity of Algorithm 1. The time cost of the data-collection phase is
O(mdT 0), assuming that it takes O(d)time to compute each action. For the estimation phase, to perform a sin-
gle update, each agent spends O(mnd)time for the calculation of the weighted average and O(T0nd)time for the
gradient computation. Accordingly, the total time cost of Algorithm 1 is O/parenleftbig
mT1(mnd +T0nd)/parenrightbig
.
Let us now define the estimated safe set for each agent i∈[m]. Let the parameter estimate for agent i∈[m]at the
end ofT0+T1time step be denoted by /hatwideAi. For each row k∈[n]of/hatwideAi, a ball centered at [/hatwideAi]k,:with a radius ofBr
can be defined as follows
Ci,k≜{a∈Rd:/vextenddouble/vextenddoublea−[/hatwideAi]k,:/vextenddouble/vextenddouble≤Br}, (7)
whereBr≜1
Tρ+R/radicalig
dlog/parenleftbig
1+mT0L2/λ
δ/n/parenrightbig
+√
λLA/radicalbig
1
2mγ2σ2
ζT0. The true parameter [A]k,:lies inside the setCi,kwith a high proba-
bility. Now, using (7) the safe estimated set for agent i∈[m]can be constructed as follows:
/hatwideXs
i≜{x∈Rd:˜a⊤
kx≤bk,∀˜ak∈Ci,k,∀k∈[n]}. (8)
The safe estimated set above will be used by each agent for the projection step in subsequent algorithms.
5 Dynamic Regret Analysis for the Convex Setting
During the first T0+T1time steps in Algorithm 1 agents do not expend any effort to minimize the regret. This is due to
the fact that without the knowledge of the feasible set, they cannot perform any projection. In this section, we propose
D-Safe-OGD, which allows agents to carry out a safe distributed online optimization, and we analyze D-Safe-OGD in
theconvex setting.
D-Safe-OGD is summarized in Algorithm 2, where in the exploration phase, all agents collaboratively estimate the
constraint parameters based on Algorithm 1, and then each agent constructs the feasible set based on its own estimate.
8Published in Transactions on Machine Learning Research (10/2023)
Algorithm 1 Distributed Constraint Parameter Estimation
1:Require: number of agents m, doubly stochastic matrix P∈Rm×m,˜P≜I+P
2, hyper-parameters α,γandλ,
data-collection duration T0, constraint-estimation duration T1, a strictly feasible point xs(safe baseline action).
2:Explore around baseline action:
3:fort= 1,2,...,T 0do
4: fori= 1,2,...,m do
5: Select action xi,t= (1−γ)xs+γζi,t
6: Receive noisy observation ˆxi,t=Axi,t+wi,t
7: end for
8:end for
9:Form local functions using collected data:
li(A)≜T0/summationdisplay
t=1/vextenddouble/vextenddoubleAxi,t−ˆxi,t/vextenddouble/vextenddouble2+λ
m/vextenddouble/vextenddoubleA/vextenddouble/vextenddouble2
F.
10:Use EXTRA (Shi et al., 2015) to solve global LS problem/summationtextm
i=1li(A)in a distributed fashion:
11:Randomly generate /hatwideAT0
i∈Rn×dfor alli∈[m].
12:∀i∈[m],/hatwideAT0+1
i =/summationtextm
j=1[P]ji/hatwideAT0
j−α∇li(/hatwideAT0
i),where the gradient is computed based on the following
expression:
∇li(A) =T0/summationdisplay
t=1/bracketleftbig
2Axi,tx⊤
i,t−2ˆxi,tx⊤
i,t/bracketrightbig
+2λ
mA.
13:fort=T0,...,T 0+T1−2do
14: fori= 1,2,...,m do
15:/hatwideAt+2
i=/summationtextm
j=12[˜P]ji/hatwideAt+1
j−/summationtextm
j=1[˜P]ji/hatwideAt
j−α[∇li(/hatwideAt+1
i)−∇li(/hatwideAt
i)].
16: end for
17:end for
In the optimization phase, the network applies distributed OGD, where all agents first perform gradient descent with
their local gradients, and then they communicate their iterates with neighbors based on the network topology imposed
byP. We note that the projection operator of each agent is defined w.r.t. the local estimated feasible set (line 8
of Algorithm 2), thereby making the feasible sets close enough but slightly different from each other. Therefore,
previous regret bounds for distributed online optimization over a common feasible set (e.g., (Yan et al., 2012; Hosseini
et al., 2016; Shahrampour & Jadbabaie, 2018; Eshraghi & Liang, 2022)) are not immediately applicable. We tackle
this challenge by exploiting the geometric property of linear constraints (Fereydounian et al., 2020) as well as the
sensitivity analysis of perturbed optimization problems with second order regular constraints (Bonnans et al., 1998),
and we present an upper bound on the dynamic regret in terms of the path-length regularity measure.
We adhere to the following standard assumption in the context of OCO:
Assumption 4. The cost functions fi,tare convex∀i∈[m]and∀t∈[T], and they have a bounded gradient, i.e.,/vextenddouble/vextenddouble∇fi,t(x)/vextenddouble/vextenddouble≤Gfor any x∈Xs.
Theorem 3. Suppose Assumptions 1-4 hold and T= Ω/parenleftig/parenleftbigL2
mγ2σ2
ζlog(d
δ)/parenrightbig3/2/parenrightig
. By running Algorithm 2 with γ≤
∆s
LLA,η= Θ(T−1/3),T0= Θ(T2/3)andT1= Θ(logT), we have with probability at least (1−2δ)
xi,t∈Xs,∀i∈[m],t∈[T],and
Regd
i,T=O/parenleftbigg
T2/3/radicalbig
log(T/δ) +β
(1−β)T2/3+T1/3C∗
T/parenrightbigg
,∀i∈[m].
Theorem 3 establishes a dynamic regret bound for D-Safe-OGD that is at least O(T2/3√logT), and for a large enough
path-length the bound becomes O(T1/3C∗
T). We can also see the impact of network topology through β, the second
9Published in Transactions on Machine Learning Research (10/2023)
Algorithm 2 Distributed Safe OGD with linear constraints
1:Require: number of agents m, doubly stochastic matrix P∈Rm×m, hyper-parameters α,γ,η,δ,λ , time horizon
T, a strictly feasible point xs.
2:SpecifyT0andT1based on given hyper-parameters and run Algorithm 1 to learn agents estimates {ˆAi}i∈[m]in a
distributed fashion.
3:For alli∈[m], construct the safe set /hatwideXs
ifrom the estimate /hatwideAi.
4:Distributed online gradient descent over different feasible sets:
5:LetTs≜(T0+T1+ 1) and initialize all agents at the same point xi,Ts=xTschosen randomly.
6:fort=Ts,...,T do
7: fori= 1,2,...,m do
8:
yi,t= Π/hatwideXs
i[xi,t−η∇fi,t(xi,t)].
9: end for
10: For alli∈[m],
11:
xi,t+1=m/summationdisplay
j=1[P]jiyj,t.
12:end for
largest singular value of P. When the network connectivity is stronger (i.e., βis smaller), the regret bound is tighter.
For the dependence on other parameters, we refer readers to the exact upper bound expression (Equation (38)).
Corollary 4. Suppose that the comparator sequence is fixed over time, i.e., x∗
t=x∗,∀t∈[T]. Then, the individual
regret bound is O(T2/3√logT), which recovers the static regret bound of the centralized case in (Chaudhary &
Kalathil, 2022) in terms of order.
Remark 3. Note that when Ais known, there is no estimation error, and the trade-off in terms of ηandT0no longer
exists. In other words, the agents do not incur the initial regret of T0+T1=O(T2/3), caused by estimation. Then, by
choosingη= Θ(1√
T), the resulting bound is O(√
T(1 +C∗
T), which recovers the result of Shahrampour & Jadbabaie
(2018) in terms of order.
Remark 4. In the proof of Theorem 3, the regret bound is shown to be O(T0+1
η+1
ηC∗
T+T√
logT0√T0+βηT
(1−β)).
If agents have the knowledge of C∗
T, by settingη= Θ(T−1/2/radicalbigC∗
T), the regret bound in Theorem 3 is improved to
O(T2/3√logT+/radicalbigTC∗
T), which enjoys better dependence on C∗
T. Though this choice of step size is non-adaptive,
we conjecture that using techniques such as expert advice (Zhang et al. (2018a)) or adaptive step sizes (Jadbabaie
et al. (2015)), one can improve the regret bound, which is an interesting future direction.
6 Dynamic Regret Analysis for the Non-convex Setting
In this section, we study the non-convex setting for safe distributed online optimization. Even for offline optimization
in the non-convex setting, the standard metric for the convergence analysis is often stationarity, i.e., characterizing the
decay rate of the gradient norm. In online optimization, we can also expect that standard regret notions, used in the
convex setting, may not be tractable for understanding the algorithm performance. However, in a recent work by Ghai
et al. (2022), the authors studied an algorithmic equivalence property between OGD and OMD for certain problem
geometries, in the sense that OGD applied to non-convex problems can be approximated by OMD applied to convex
functions under reparameterization, using which a sub-linear static regret bound is guaranteed.
More specifically, for a centralized problem, suppose that there is a bijective non-linear mapping q, such that ut=
q(xt), and consider the OGD and OMD updates
Centralized OGD:
xt+1=argminx∈X/braceleftig
∇ft(xt)⊤(x−xt) +1
2η/vextenddouble/vextenddoublex−xt/vextenddouble/vextenddouble2/bracerightig
, (9)
10Published in Transactions on Machine Learning Research (10/2023)
Centralized OMD:
ut+1=argminu∈X′/braceleftig
∇˜ft(ut)⊤(u−ut) +1
ηDϕ(u,ut)/bracerightig
, (10)
whereX′is the image ofXunder the mapping q. Ghai et al. (2022) quantified the deviation/vextenddouble/vextenddoubleut+1−q(xt+1)/vextenddouble/vextenddoubleas
O(η3/2)under the following technical assumptions (together with boundedness of gradient norms):
Assumption 5. There exists a bijective mapping q:X→X′such that [∇2ϕ(u)]−1=Jq(x)Jq(x)⊤where u=q(x).
Assumption 6. LetW > 1be a constant. Assume that q(·)isW-Lipschitz,ϕ(·)is1-strongly convex and smooth with
its first and third derivatives upper bounded by W. The first and second derivatives of q−1(·)are also bounded by W.
For all u∈X′,Dϕ(u,·)isW-Lipschitz overX′.
Examples that satisfy these assumptions are provided in Section 3.1 of Ghai et al. (2022). For example, if ϕis the
negative entropy (respectively, log barrier), we can use quadratic (respectively, exponential) reparameterization for
q. Amid & Warmuth (2020) showed that in the continuous-time setup when Assumption 5 holds, the mirror descent
regularization induced by ϕcan be transformed back to the Euclidean regularization by q−1, which implies the equiv-
alence between OMD for convex functions and OGD for non-convex functions. This is due to the fact that higher than
second order factors vanish in continuous time, and this assures that the mirror flow and the reparameterized gradient
flow coincide. Though in the discrete-time case, the exact equivalence does not hold, Ghai et al. (2022) showed that
OGD for non-convex functions can still be approximated as OMD for convex functions, and the corresponding static
regret bound is O(T2/3)under the assumption that ft(x) =˜ft(q(x)), where ˜ft(·)is convex. However, we need more
technical assumptions to handle the discrete-time setup as higher order terms are relevant and must be judiciously
analyzed. Ghai et al. (2022) characterized the sufficient condition to achieve Assumption 5, which entails an implicit
OMD reparameterization for a non-convex OGD. We state these (two assumptions) by tailoring them to our problem
setting:
Assumption 7./vextenddouble/vextenddouble∇˜fi,t(u)/vextenddouble/vextenddouble≤GFfor all u∈Xs′andsupu,z∈Xs′Dϕ(u,z)≤D′.
Assumption 8. Properties of the mapping q(·):
•There exists a mapping q(·)such thatfi,t(x) =˜fi,t(q(x)), where ˜fi,t(·)is convex.
•q(·)is aC3-diffeomorphism, and Jq(x)is diagonal.
•For anyX⊂Xswhich is compact and convex, X′≜q(X)is convex and compact.
We again refer the reader to Section 3.1 of Ghai et al. (2022) for examples related to Assumption 8.
In this work, we extend this equivalence to “distributed” variants of OGD and OMD under the additional complexity
that the constraint set is unknown, and it can only be approximated via Algorithm 1. Our focus is on analyzing the
effect of (i) the constraint estimation as well as (ii) the distributed setup in non-convex online learning, and we also
generalize the analysis of Ghai et al. (2022) to the dynamic regret. For the technical analysis of the non-convex setting,
we also use the following assumption.
Assumption 9. Letuand{yi}m
i=1be vectors in Rd. The Bregman divergence satisfies the separate convexity in the
following sense
Dϕ(u,m/summationdisplay
iαiyi)≤m/summationdisplay
iαiDϕ(u,yi),
whereα∈∆mis on them−dimensional simplex.
This assumption is satisfied by commonly used Bregman divergences, e.g., Euclidean distance and KL divergence.
We refer interested readers to (Bauschke & Borwein, 2001; Shahrampour & Jadbabaie, 2018) for more information.
In the following theorem, we prove that with high probability, the dynamic regret bound of D-Safe-OGD is
O(T2/3√logT+T2/3C∗
T).
Theorem 5. Suppose Assumptions 1-3 and 6-9 hold and T= Ω/parenleftig/parenleftbigL2
mγ2σ2
ζlog(d
δ)/parenrightbig3/2/parenrightig
. By running Algorithm 2 with
γ≤∆s
LLA,η= Θ(T−2/3),T0= Θ(T2/3)andT1= Θ(logT), we have with probability at least (1−2δ)
xi,t∈Xs,∀i∈[m],t∈[T],and
11Published in Transactions on Machine Learning Research (10/2023)
Regd
i,T=O(T2/3/radicalbig
log(T/δ) +T2/3C∗
T),∀i∈[m].
The complete proof is provided in the Appendix, and the dependence on other problem parameters can be found in
Equation (54). The idea is to show that distributed OMD and distributed OGD iterates are close enough to each other
if the reference points of both updates are identical, i.e., ui,t=q(xi,t)for alli∈[m]. Then, distributed OGD can be
viewed as a perturbed version of distributed OMD, and under the assumption of convexity of ˜fi,tthe regret bound can
be established. Also, as mentioned in the convex case (Remark 4), we conjecture that the dependence of regret bound
to the path-length can be improved to/radicalbigC∗
T.
We further have the following corollary that shows our result is a valid generalization of Ghai et al. (2022) to the
distributed ,dynamic setting.
Corollary 6. Suppose that the comparator sequence is static over time, i.e., x∗
t=x∗,∀t∈[T]. Then, the individual
regret bound becomes O(T2/3√logT), which recovers the static regret bound of Ghai et al. (2022) up to log factors.
It is worth noting that though in the convex case, the estimation of unknown constraints exacerbates the regret bound
(due toO(T2/3)time spent on exploration), for the non-convex case, the resulting bound still matches the static regret
of Ghai et al. (2022), where there is no estimation error. In other words, there is no trade-off in this case as the static
regret (without estimation error) is O(T2/3)(Ghai et al., 2022) (disregarding log factors).
7 Discussion on Regret Bounds in Terms of Other Regularity Measures
In this work, we focused on dynamic regret bounds characterized by C∗
T, the path length of the minimizer se-
quence. It is worth noting that existing works also presented regret bounds in terms of other regularity measures,
which capture the properties of the function sequence from different perspectives. Such measures include (1) the
function variation VT≜/summationtextT
t=2supx∈X|ft(x)−ft−1(x)|(Besbes et al., 2015), (2) the predictive path-length
C′
T(u1,...,uT)≜/summationtextT
t=2/vextenddouble/vextenddoubleut−Ψt(ut−1)/vextenddouble/vextenddouble(Hall & Willett, 2013), where Ψtis a given dynamics, and (3) the gradient
variationDT≜/summationtextT
t=1/vextenddouble/vextenddouble∇ft(xt)−mt/vextenddouble/vextenddouble2(Rakhlin & Sridharan, 2013), where mtis a predictable sequence computed
by the learner. Besbes et al. (2015) proposed a restarting OGD algorithm and showed that when the learner has ac-
cess to only noisy gradients, the expected dynamic regret is bounded by O(T2/3(VT+ 1)1/3)for convex functions
andO(logT/radicalbig
T(1 +VT))for strongly convex functions. The above mentioned regularity measures are not directly
comparable to each other. In this regard, Jadbabaie et al. (2015) provided a dynamic regret bound in terms of C∗
T,DT
andVTfor the adaptive optimistic OMD algorithm. Also, Chang & Shahrampour (2021) revisited OGD with multiple
gradient queries per iteration (in the unconstrained setup) and established the regret bound of O(min{VT,C∗
T,C∗
2,T})
for strongly convex and smooth functions. Dynamic regret has also been studied for functions with a parameterizable
structure (Ravier et al., 2019) as well as composite convex functions (Ajalloeian et al., 2020).
Besides the dynamic regret, a relevant regret measure called adaptive regret (Hazan & Seshadhri, 2009) for a contigu-
ous time interval Tsubis defined as follows
Rega
T(Tsub)≜ max
[i,i+Tsub−1]⊂[T]/parenleftiggi+Tsub−1/summationdisplay
t=ift(xt)−min
x∈Xi+Tsub−1/summationdisplay
t=ift(x)/parenrightigg
. (11)
Zhang et al. (2018b) analyzed the connection between adaptive regret and dynamic regret and provided adaptive
algorithms with provably small dynamic regret for convex, exponentially concave, and strongly convex functions.
In contrast to aforementioned works, where a projection operator is needed, Wan et al. (2021) proposed a projection
free online method replacing the projection step with multiple linear optimization steps. Without assuming smooth-
ness, they proved dynamic regret bounds of O(max{T2/3V1/3
T,√
T})andO(max{√TVTlogT,logT})for convex
and strongly convex functions, respectively. On the other hand, Wan et al. (2023) considered the case of smooth
convex functions and improved the dynamic regret bound from O/parenleftbig√
T(1 +VT+√DT)/parenrightbig
toO(/radicalbig
T(1 +VT)).
Conclusion
In this work, we considered safe distributed online optimization with an unknown set of linear constraints. The
goal of the network is to ensure that the action sequence selected by each agent, which only has partial information
12Published in Transactions on Machine Learning Research (10/2023)
about the global function, is competitive to the centralized minimizers in hindsight without violating the safety con-
straints. To address this problem, we proposed D-Safe-OGD, where starting from a safe region, it allows all agents
to perform exploration to estimate the unknown constraints in a distributed fashion. Then, distributed OGD is ap-
plied over the feasible sets formed by agents estimates. For convex functions, we proved a dynamic regret bound of
O(T2/3√logT+T1/3C∗
T), which recovers the static regret bound of Chaudhary & Kalathil (2022) for the central-
ized case (single agent). Then, we showed that for the non-convex setting, the dynamic regret is upper bounded by
O(T2/3√logT+T2/3C∗
T), which recovers the static regret bound of Ghai et al. (2022) for the centralized case (sin-
gle agent) up to log factors. Possible future directions include improving the regret using adaptive techniques and/or
deriving comprehensive regret bounds in terms of other variation measures, such as VT.
Acknowledgements
The authors gratefully acknowledge the support of National Science Foundation (NSF). T-J. Chang and S. Shahram-
pour were supported by NSF ECCS-2136206. The work of S. Chaudhary and D. Kalathil was supported in part by
grants NSF CAREER-EPCN-2045783 and NSF CNS-1955696.
References
Yasin Abbasi-Yadkori, Dávid Pál, and Csaba Szepesvári. Improved algorithms for linear stochastic bandits. Advances
in neural information processing systems , 24, 2011.
Amirhossein Ajalloeian, Andrea Simonetto, and Emiliano Dall’Anese. Inexact online proximal-gradient method for
time-varying convex optimization. In American Control Conference (ACC) , pp. 2850–2857, 2020.
Mohammad Akbari, Bahman Gharesifard, and Tamás Linder. Distributed online convex optimization on time-varying
directed graphs. IEEE Transactions on Control of Network Systems , 4(3):417–428, 2015.
Sanae Amani, Mahnoosh Alizadeh, and Christos Thrampoulidis. Linear stochastic bandits under safety constraints.
Advances in Neural Information Processing Systems , 32, 2019.
Ehsan Amid and Manfred KK Warmuth. Reparameterizing mirror descent as gradient descent. Advances in Neural
Information Processing Systems , 33:8430–8439, 2020.
Karl Johan Åström and Richard M Murray. Feedback systems . Princeton university press, 2010.
Dheeraj Baby and Yu-Xiang Wang. Optimal dynamic regret in exp-concave online learning. In Conference on Learn-
ing Theory , pp. 359–409. PMLR, 2021.
Dheeraj Baby and Yu-Xiang Wang. Optimal dynamic regret in proper online learning with strongly convex losses and
beyond. In International Conference on Artificial Intelligence and Statistics , pp. 1805–1845. PMLR, 2022.
Heinz H Bauschke and Jonathan M Borwein. Joint and separate convexity of the bregman distance. In Studies in
Computational Mathematics , volume 8, pp. 23–36. Elsevier, 2001.
Omar Besbes, Yonatan Gur, and Assaf Zeevi. Non-stationary stochastic optimization. Operations research , 63(5):
1227–1244, 2015.
J Frédéric Bonnans, Roberto Cominetti, and Alexander Shapiro. Sensitivity analysis of optimization problems under
second order regular constraints. Mathematics of Operations Research , 23(4):806–831, 1998.
Xuanyu Cao and KJ Ray Liu. Online convex optimization with time-varying constraints and bandit feedback. IEEE
Transactions on automatic control , 64(7):2665–2680, 2018.
Ting-Jui Chang and Shahin Shahrampour. On online optimization: Dynamic regret analysis of strongly convex and
smooth problems. Proceedings of the AAAI Conference on Artificial Intelligence , 35(8):6966–6973, 2021.
Sapana Chaudhary and Dileep Kalathil. Safe online convex optimization with unknown linear safety constraints.
Proceedings of the AAAI Conference on Artificial Intelligence (AAAI) , 36(6):6175–6182, 2022.
13Published in Transactions on Machine Learning Research (10/2023)
Tianyi Chen, Qing Ling, and Georgios B Giannakis. An online convex optimization approach to proactive network
resource allocation. IEEE Transactions on Signal Processing , 65(24):6350–6364, 2017.
Rishabh Dixit, Amrit Singh Bedi, Ketan Rajawat, and Alec Koppel. Distributed online learning over time-varying
graphs via proximal gradient descent. In 2019 IEEE 58th Conference on Decision and Control (CDC) , pp. 2745–
2751. IEEE, 2019a.
Rishabh Dixit, Amrit Singh Bedi, Ruchi Tripathi, and Ketan Rajawat. Online learning with inexact proximal online
gradient descent algorithms. IEEE Transactions on Signal Processing , 67(5):1338–1352, 2019b.
Roel Dobbe, Patricia Hidalgo-Gonzalez, Stavros Karagiannopoulos, Rodrigo Henriquez-Auba, Gabriela Hug, Dun-
can S Callaway, and Claire J Tomlin. Learning to control in power systems: Design and analysis guidelines for
concrete safety problems. Electric Power Systems Research , 189:106615, 2020.
Nima Eshraghi and Ben Liang. Improving dynamic regret in distributed online mirror descent using primal and dual
information. In Learning for Dynamics and Control Conference , pp. 637–649. PMLR, 2022.
Shuai Fan, Guangyu He, Xinyang Zhou, and Mingjian Cui. Online optimization for networked distributed energy
resources with time-coupling constraints. IEEE Transactions on Smart Grid , 12(1):251–267, 2020.
Mohammad Fereydounian, Zebang Shen, Aryan Mokhtari, Amin Karbasi, and Hamed Hassani. Safe learning under
uncertain objectives and constraints. arXiv preprint arXiv:2006.13326 , 2020.
Udaya Ghai, Zhou Lu, and Elad Hazan. Non-convex online learning via algorithmic equivalence. Advances in Neural
Information Processing Systems (NeurIPS) , 35:22161–22172, 2022.
Eric Hall and Rebecca Willett. Dynamical models and tracking regret in online convex programming. In International
Conference on Machine Learning , pp. 579–587. PMLR, 2013.
Elad Hazan. Introduction to online convex optimization. Foundations and Trends in Optimization , 2(3-4):157–325,
2016.
Elad Hazan and Satyen Kale. Projection-free online learning. In International Coference on International Conference
on Machine Learning (ICML) , pp. 1843–1850, 2012.
Elad Hazan and Comandur Seshadhri. Efficient learning algorithms for changing environments. In International
Conference on Machine Learning (ICML) , pp. 393–400, 2009.
Elad Hazan, Amit Agarwal, and Satyen Kale. Logarithmic regret algorithms for online convex optimization. Machine
Learning , 69(2-3):169–192, 2007.
Elad Hazan, Karan Singh, and Cyril Zhang. Efficient regret minimization in non-convex games. In International
Conference on Machine Learning (ICML) , pp. 1433–1441. PMLR, 2017.
Saghar Hosseini, Airlie Chapman, and Mehran Mesbahi. Online distributed convex optimization on dynamic networks.
IEEE Transactions on Automatic Control , 61(11):3545–3550, 2016.
Ali Jadbabaie, Alexander Rakhlin, Shahin Shahrampour, and Karthik Sridharan. Online optimization: Competing
with dynamic comparators. In Artificial Intelligence and Statistics , pp. 398–406, 2015.
Rodolphe Jenatton, Jim Huang, and Cédric Archambeau. Adaptive algorithms for online convex optimization with
long-term constraints. In International Conference on Machine Learning , pp. 402–411. PMLR, 2016.
Kia Khezeli and Eilyan Bitar. Safe linear stochastic bandits. Proceedings of the AAAI Conference on Artificial
Intelligence , 34(06):10202–10209, 2020.
Jueyou Li, Chuanye Gu, Zhiyou Wu, and Tingwen Huang. Online learning algorithm for distributed convex optimiza-
tion with time-varying coupled constraints and bandit feedback. IEEE transactions on cybernetics , 2020.
Xiuxian Li, Lihua Xie, and Na Li. A survey of decentralized online learning. arXiv preprint arXiv:2205.00473 , 2022.
14Published in Transactions on Machine Learning Research (10/2023)
Qiulin Lin, Hanling Yi, John Pang, Minghua Chen, Adam Wierman, Michael Honig, and Yuanzhang Xiao. Compet-
itive online optimization under inventory constraints. Proceedings of the ACM on Measurement and Analysis of
Computing Systems , 3(1):1–28, 2019.
Jun S Liu. Monte Carlo strategies in scientific computing . Springer Science & Business Media, 2008.
Qingsong Liu, Wenfei Wu, Longbo Huang, and Zhixuan Fang. Simultaneously achieving sublinear regret and con-
straint violations for online convex optimization with time-varying constraints. ACM SIGMETRICS Performance
Evaluation Review , 49(3):4–5, 2022.
Kaihong Lu and Long Wang. Online distributed optimization with nonconvex objective functions: Sublinearity of
first-order optimality condition-based regret. IEEE Transactions on Automatic Control , 67(6):3029–3035, 2021.
Nguyen Cong Luong, Dinh Thai Hoang, Shimin Gong, Dusit Niyato, Ping Wang, Ying-Chang Liang, and Dong In
Kim. Applications of deep reinforcement learning in communications and networking: A survey. IEEE Communi-
cations Surveys & Tutorials , 21(4):3133–3174, 2019.
Mehrdad Mahdavi, Rong Jin, and Tianbao Yang. Trading regret for efficiency: online convex optimization with long
term constraints. The Journal of Machine Learning Research , 13(1):2503–2528, 2012.
David Mateos-Núnez and Jorge Cortés. Distributed online convex optimization over jointly connected digraphs. IEEE
Transactions on Network Science and Engineering , 1(1):23–37, 2014.
Aryan Mokhtari, Shahin Shahrampour, Ali Jadbabaie, and Alejandro Ribeiro. Online optimization in dynamic envi-
ronments: Improved regret rates for strongly convex problems. In IEEE 55th Conference on Decision and Control
(CDC) , pp. 7195–7201, 2016.
Michael J Neely and Hao Yu. Online convex optimization with time-varying constraints. arXiv preprint
arXiv:1702.04783 , 2017.
Alexander Rakhlin and Karthik Sridharan. Online learning with predictable sequences. In Conference on Learning
Theory , pp. 993–1019. PMLR, 2013.
Robert J Ravier, A Robert Calderbank, and Vahid Tarokh. Prediction in online convex optimization for parametrizable
objective functions. In IEEE 58th Conference on Decision and Control (CDC) , pp. 2455–2460, 2019.
Shahin Shahrampour and Ali Jadbabaie. An online optimization approach for multi-agent tracking of dynamic param-
eters in the presence of adversarial noise. In American Control Conference (ACC) , pp. 3306–3311, 2017.
Shahin Shahrampour and Ali Jadbabaie. Distributed online optimization in dynamic environments using mirror de-
scent. IEEE Transactions on Automatic Control , 63(3):714–725, 2018.
Pranay Sharma, Prashant Khanduri, Lixin Shen, Donald J Bucci, and Pramod K Varshney. On distributed online
convex optimization with sublinear dynamic regret and fit. In 2021 55th Asilomar Conference on Signals, Systems,
and Computers , pp. 1013–1017. IEEE, 2021.
Wei Shi, Qing Ling, Gang Wu, and Wotao Yin. Extra: An exact first-order algorithm for decentralized consensus
optimization. SIAM Journal on Optimization , 25(2):944–966, 2015.
Wen Sun, Debadeepta Dey, and Ashish Kapoor. Safety-aware algorithms for adversarial contextual bandit. In Inter-
national Conference on Machine Learning , pp. 3280–3288. PMLR, 2017.
Joel A Tropp et al. An introduction to matrix concentration inequalities. Foundations and Trends® in Machine
Learning , 8(1-2):1–230, 2015.
Ilnura Usmanova, Andreas Krause, and Maryam Kamgarpour. Safe convex learning under uncertain constraints. In
The 22nd International Conference on Artificial Intelligence and Statistics , pp. 2106–2114. PMLR, 2019.
Yuanyu Wan, Wei-Wei Tu, and Lijun Zhang. Projection-free distributed online convex optimization with o(√
T)
communication complexity. In International Conference on Machine Learning , pp. 9818–9828. PMLR, 2020.
15Published in Transactions on Machine Learning Research (10/2023)
Yuanyu Wan, Bo Xue, and Lijun Zhang. Projection-free online learning in dynamic environments. In Proceedings of
the AAAI Conference on Artificial Intelligence , volume 35, pp. 10067–10075, 2021.
Yuanyu Wan, Lijun Zhang, and Mingli Song. Improved dynamic regret for online frank-wolfe. arXiv preprint
arXiv:2302.05620 , 2023.
Feng Yan, Shreyas Sundaram, SVN Vishwanathan, and Yuan Qi. Distributed autonomous online learning: Regrets
and intrinsic privacy-preserving properties. IEEE Transactions on Knowledge and Data Engineering , 25(11):2483–
2493, 2012.
Xinlei Yi, Xiuxian Li, Lihua Xie, and Karl H Johansson. Distributed online convex optimization with time-varying
coupled inequality constraints. IEEE Transactions on Signal Processing , 68:731–746, 2020a.
Xinlei Yi, Xiuxian Li, Tao Yang, Lihua Xie, Tianyou Chai, and Karl Henrik Johansson. Distributed bandit online
convex optimization with time-varying coupled inequality constraints. IEEE Transactions on Automatic Control ,
66(10):4620–4635, 2020b.
Xinlei Yi, Xiuxian Li, Tao Yang, Lihua Xie, Tianyou Chai, and Karl Johansson. Regret and cumulative constraint vio-
lation analysis for online convex optimization with long term constraints. In International Conference on Machine
Learning , pp. 11998–12008. PMLR, 2021.
Xinlei Yi, Xiuxian Li, Tao Yang, Lihua Xie, Tianyou Chai, and H Karl. Regret and cumulative constraint violation
analysis for distributed online constrained convex optimization. IEEE Transactions on Automatic Control , 2022.
Hao Yu and Michael J Neely. A Low Complexity Algorithm with O(√
T)Regret andO(1)Constraint Violations
for Online Convex Optimization with Long Term Constraints. Journal of Machine Learning Research , 21(1):1–24,
2020.
Hao Yu, Michael Neely, and Xiaohan Wei. Online convex optimization with stochastic constraints. Advances in
Neural Information Processing Systems , 30, 2017.
Deming Yuan, Daniel WC Ho, and Guo-Ping Jiang. An adaptive primal-dual subgradient algorithm for online dis-
tributed constrained optimization. IEEE transactions on cybernetics , 48(11):3045–3055, 2017.
Deming Yuan, Alexandre Proutiere, and Guodong Shi. Distributed online linear regressions. IEEE Transactions on
Information Theory , 67(1):616–639, 2020.
Deming Yuan, Alexandre Proutiere, and Guodong Shi. Distributed online optimization with long-term constraints.
IEEE Transactions on Automatic Control , 67(3):1089–1104, 2021.
Jianjun Yuan and Andrew Lamperski. Online convex optimization for cumulative constraints. Advances in Neural
Information Processing Systems , 31, 2018.
Lijun Zhang, Tianbao Yang, Jinfeng Yi, Rong Jin, and Zhi-Hua Zhou. Improved dynamic regret for non-degenerate
functions. In Advances in Neural Information Processing Systems , pp. 732–741, 2017a.
Lijun Zhang, Shiyin Lu, and Zhi-Hua Zhou. Adaptive online learning in dynamic environments. In Advances in neural
information processing systems , pp. 1323–1333, 2018a.
Lijun Zhang, Tianbao Yang, Zhi-Hua Zhou, et al. Dynamic regret of strongly adaptive methods. In International
conference on machine learning , pp. 5882–5891. PMLR, 2018b.
Wenpeng Zhang, Peilin Zhao, Wenwu Zhu, Steven CH Hoi, and Tong Zhang. Projection-free distributed online
learning in networks. In International Conference on Machine Learning , pp. 4054–4062. PMLR, 2017b.
Yan Zhang, Robert J Ravier, Michael M Zavlanos, and Vahid Tarokh. A distributed online convex optimization
algorithm with improved dynamic regret. In 2019 IEEE 58th Conference on Decision and Control (CDC) , pp.
2449–2454. IEEE, 2019.
Martin Zinkevich. Online convex programming and generalized infinitesimal gradient ascent. In Proceedings of the
20th International Conference on Machine Learning , pp. 928–936, 2003.
16Published in Transactions on Machine Learning Research (10/2023)
A Appendix
In this section, we provide the proofs of our theoretical results. In Section A.1, we state the results we use in our
analysis. Section A.2 includes the proof of estimation error bound in Lemma 2. In Sections A.3 and A.4, we provide
the proofs for Theorem 3 (convex case) and Theorem 5 (non-convex case), respectively.
A.1 Preliminaries:
Theorem 7. (Theorem 2 in Abbasi-Yadkori et al. (2011)). Let {Ft}∞
t=0be a filtration and {wt}∞
t=1be a real-valued
stochastic process. Here, wtisFt-measurable and wtis conditionally R-sub Gaussian for some R≥0. Let{xt}∞
t=1
be an Rd-valued stochastic process such that xtisFt−1-measurable. Let VT≜/summationtextT
t=1xtx⊤
t+λIwhereλ>0. Define
yt=a⊤xt+wt, then ˆaT=V−1
T/summationtextT
t=1ytxtis theℓ2-regularized least squares estimate of a. Assume/vextenddouble/vextenddoublea/vextenddouble/vextenddouble≤LA
and/vextenddouble/vextenddoublext/vextenddouble/vextenddouble≤L,∀t. Then, for any δ∈(0,1), with probability (1−δ), the true parameter alies in the following set:
/braceleftigg
a∈Rd:/vextenddouble/vextenddoublea−ˆaT/vextenddouble/vextenddouble
VT≤R/radicalbigg
dlog/parenleftbig1 +TL2/λ
δ/parenrightbig
+√
λLA/bracerightigg
,
for allT≥1.
Theorem 8. (Theorem 5.1.1 in Tropp et al. (2015)). Consider a finite sequence {Xt}of independent, random and
positive semi-definite matrices of dimension d. Assume that λmax(Xt)≤L,∀t. Define Y≜/summationtext
tXtand denote
λmin(E[Y])asµ. Then, we have
P(λmin(Y)≤ϵµ)≤dexp/parenleftbig
−(1−ϵ)2µ
2L/parenrightbig
,for anyϵ∈(0,1).
Now, let us define the shrunk version of the polytope as follows
Xs
in≜{x∈Rd: [A]k,:x+τin≤bk,∀k∈[n]},for someτin>0. (12)
Lemma 9 (Lemma 1 in Fereydounian et al. (2020)) .Consider a positive constant τinsuch thatXs
inis non-empty. Then,
for any x∈Xs,
∥ΠXs
in(x)−x∥≤√
dτin
C(A,b), (13)
whereC(A,b)is a positive constant that depends only on the matrix Aand the vector b.
Theorem 10. (Theorem 3.7 in Shi et al. (2015)) Let us consider the following notation for EXTRA algorithm
xi,k:The iterate of agent iat timekof the EXTRA algorithm ,
Xk=
x⊤
1,k
...
x⊤
m,k
,
x∗=argminx/braceleftbiggm/summationdisplay
i=1fi(x)/bracerightbigg
,
X∗=
x∗⊤
...
x∗⊤
,
f(X) =m/summationdisplay
i=1fi(xi).
A convex function h(·)is restricted strongly convex w.r.t. a point yif there exists µ>0such that
⟨∇h(x)−∇h(y),x−y⟩≥µ/vextenddouble/vextenddoublex−y/vextenddouble/vextenddouble2,∀x.
17Published in Transactions on Machine Learning Research (10/2023)
Suppose that the gradient of f(X)w.r.t.Xis Lipschitz continuous with a constant Lfandf(X) +1
4α/vextenddouble/vextenddoubleX−X∗/vextenddouble/vextenddouble˜P−P
is restricted strongly convex w.r.t. X∗with a constant µg. Then, with a proper step size α<2µgλmin(˜P)
L2
f, there exists
ς >0such that/vextenddouble/vextenddoubleXk−X∗/vextenddouble/vextenddouble2
˜Pconverges to 0at theR-linear rate of O((1 +ς)−k).
A.2 Safe Distributed Set Estimation
Proof of Lemma 2. LetVT0≜/summationtextm
i=1/summationtextT0
t=1xi,tx⊤
i,tandV=VT0+λI. Let/hatwideAbe the solution of
argminA/summationtextm
i=1li(A).Let[/hatwideA]k,:and[A]k,:be thek-th rows of/hatwideAandA, respectively. Based on Theorem 7, we
have with probability at least (1−δ),
/vextenddouble/vextenddouble[/hatwideA]k,:−[A]k,:/vextenddouble/vextenddouble
V≤R/radicaligg
dlog/parenleftbig1 +mT0L2/λ
δ/n/parenrightbig
+√
λLA,∀k∈[n]. (14)
Knowing that∀i∈[m],∀t∈[T0],xi,t= (1−γ)xs+γζi,t, we haveλmax(xi,tx⊤
i,t)≤L2andE[xi,tx⊤
i,t] =
(1−γ)2xsxs⊤+γ2σ2
ζI⪰γ2σ2
ζI. Therefore, we have
λmin(E[VT0]) =λmin(m/summationdisplay
i=1T0/summationdisplay
t=1E[xi,tx⊤
i,t])≥mT0γ2σ2
ζ. (15)
Based on (15) and Theorem 8, we have
P/parenleftig
λmin(VT0)≤ϵλmin(E[VT0])/parenrightig
≤dexp/parenleftbig
−(1−ϵ)2mT0γ2σ2
ζ
2L2/parenrightbig
. (16)
By settingϵ=1
2andT0≥8L2
mγ2σ2
ζlog(d
δ), from (16), we have
P/parenleftbig
λmin(V)≥1
2mT0γ2σ2
ζ/parenrightbig
≥P/parenleftbig
λmin(VT0)≥1
2mT0γ2σ2
ζ/parenrightbig
≥(1−δ). (17)
Combining equations (14) and (17), we have with probability at least (1−2δ),
/vextenddouble/vextenddouble[/hatwideA]k,:−[A]k,:/vextenddouble/vextenddouble≤R/radicalig
dlog/parenleftbig1+mT 0L2/λ
δ/n/parenrightbig
+√
λLA
/radicalig
1
2mγ2σ2
ζT0,∀k∈[n]. (18)
Let agenti’s local estimate of Aat timet∈[T0+ 1,T0+T1]returned by the EXTRA algorithm (Shi et al., 2015)
be denoted by /hatwideAt
i. Next, we upper bound the distance between /hatwideA=argminA/summationtextm
i=1li(A)and/hatwideAt
ibased on Theorem
10 as follows. Based on the definition of li(A), considering the vectorized version of A, the Hessian matrix has the
following expression
∇2li(A) =T0/summationdisplay
t=12
xi,tx⊤
i,t
xi,tx⊤
i,t
...
xi,tx⊤
i,t
+ 2λ
mI⪯2(T0L2+λ
m)I,
where the inequality is due to the boundedness of the baseline action and the noise vector. From above, we know/summationtextm
i=1li(Ai)is Lipschitz smooth with the constant 2(T0L2+λ
m)and strongly convex with the constant 2λ
m, so by
selecting a step size α<(λ/m)λmin(˜P)
(T0L2+λ
m)2as suggested by Theorem 10, there exists a τ∈(0,1)such that
/vextenddouble/vextenddouble[/hatwideAt
i]k,:−[/hatwideA]k,:/vextenddouble/vextenddouble≤ντ(t−T0),∀i∈[m],k∈[n],t∈[T0+ 1,...,T 0+T1] (19)
18Published in Transactions on Machine Learning Research (10/2023)
whereν > 0is a constant. Based on (18), (19) and our choice of T1(T1= (−logτ)−1log(νTρ)), fork∈[n],
t∈[T0+ 1,...,T 0+T1]andi,j∈[m], we have
/vextenddouble/vextenddouble[/hatwideAt
i]k,:−[A]k,:/vextenddouble/vextenddouble≤/vextenddouble/vextenddouble[/hatwideAt
i]k,:−[/hatwideA]k,:/vextenddouble/vextenddouble+/vextenddouble/vextenddouble[/hatwideA]k,:−[A]k,:/vextenddouble/vextenddouble≤1
Tρ+R/radicalig
dlog/parenleftbig1+mT 0L2/λ
δ/n/parenrightbig
+√
λLA
/radicalig
1
2mγ2σ2
ζT0,(20)
and/vextenddouble/vextenddouble[/hatwideAt
i]k,:−[/hatwideAt
j]k,:/vextenddouble/vextenddouble≤/vextenddouble/vextenddouble[/hatwideAt
i]k,:−[/hatwideA]k,:/vextenddouble/vextenddouble+/vextenddouble/vextenddouble[/hatwideA]k,:−[/hatwideAt
j]k,:/vextenddouble/vextenddouble≤2
Tρ. (21)
Lemma 11. Define
Br≜1
Tρ+R/radicalig
dlog/parenleftbig1+mT 0L2/λ
δ/n/parenrightbig
+√
λLA
/radicalig
1
2mγ2σ2
ζT0.
For each agent i, construct/hatwideXs
ibased on (8)withCi,kfollowing from (7). By running Algorithm 1 with user-specified
T0= Ω(L2
mγ2σ2
ζlog(d
δ))andT1= Θ(logTρ), there exists a mutual shrunk polytope (see the definition in (12)) subset
Xs
in(τin= 2BrL) for/hatwideXs
i,∀i∈[m]with probability at least (1−2δ).
Proof of Lemma 11. Consider a mutual shrunk polytope subset Xs
in(τin= 2BrL). Based on Lemma 2, with probability
at least 1−2δ, we have for any x∈Xs
in,
[/hatwideAi]k,:x+Br/vextenddouble/vextenddoublex/vextenddouble/vextenddouble= [A]k,:x+ ([/hatwideAi]k,:−[A]k,:)x+Br/vextenddouble/vextenddoublex/vextenddouble/vextenddouble
≤[A]k,:x+/vextenddouble/vextenddouble[/hatwideAi]k,:−[A]k,:/vextenddouble/vextenddouble/vextenddouble/vextenddoublex/vextenddouble/vextenddouble+Br/vextenddouble/vextenddoublex/vextenddouble/vextenddouble
≤[A]k,:x+ 2Br/vextenddouble/vextenddoublex/vextenddouble/vextenddouble≤[A]k,:x+ 2BrL≤bk,∀k∈[n]and∀i∈[m],(22)
which implies that Xs
in⊂/hatwideXs
i,∀i.
Lemma 12. For each agent i, construct/hatwideXs
ibased on (8)withCi,kfollowing from (7). By running Algorithm 1 with
user-specified T0= Ω(L2
mγ2σ2
ζlog(d
δ))andT1= Θ(logTρ), we have for any point x,
/vextenddouble/vextenddoubleΠ/hatwideXs
i(x)−Π/hatwideXs
j(x)/vextenddouble/vextenddouble≤O(1
Tρ),∀i,j∈[m]. (23)
Before we discuss the proof of Lemma 12, for the sake of completeness, we provide the formal statement of Theorem
3.1 in Bonnans et al. (1998), used in the derivation of Lemma 12.
We first define the notations used in (Bonnans et al., 1998). Note that the notations here are only locally de-
fined for the statement of Theorem 3.1 in Bonnans et al. (1998). The work of Bonnans et al. (1998) focuses on the
sensitivity analysis of parametric optimization problems of the form
(Pu) : min
x∈Xf(x,u)subject toG(x,u)∈K,
whereXis a finite dimensional space, Uis a Banach space, Kis a closed subset of Banach space YandfandG
are twice continuously differentiable mappings from X×U toRandY, respectively. The optimization problem is
considered to be unperturbed when u= 0.
Given u, the feasible set, optimal value and set of optimal solutions of (Pu)are denoted as follows
Φ(u)≜{x∈X:G(x,u)∈K},
v(u)≜inf{f(x,u) :x∈Φ(u)},
S(u)≜argmin{f(x,u) :x∈Φ(u)}.
19Published in Transactions on Machine Learning Research (10/2023)
A point x∈X is called anϵ-optimal solution of (Pu)ifx∈Φ(u)andf(x,u)≤v(u) +ϵ.
We also define the following notations to present the theorem statement.
Y∗Dual space ofY
dist(y,X) The minimum distance from point yto setX:inf{/vextenddouble/vextenddoubley−x/vextenddouble/vextenddouble:x∈X}
TK(y) The tangent cone to the set Kat the point y∈K:{h∈Y:dist(y+th,K) =o(t)}
NK(y) The normal cone to the set Kat the point y∈K:{y∗∈Y∗:⟨y∗,h⟩≤0,∀h∈TK(y)}
Df(x,u) Derivative of f
Dxf(x,u) Partial derivative of fw.r.t.x
Dxxf(x,u) Second order derivative of fw.r.t.x
Df(x′,u′)(x,u)The linear function based on the derivative at (x′,u′)
L(x,λ,u) The Lagrangian f(x,u) +⟨λ,G(x,u)⟩, λ∈Y∗
Λu(x){λ∈NK(G(x,u)) :DxL(x,λ,u) = 0}
{X1+X2}∪{x1+x2},x1∈X1,x2∈X2
int(X) The interior of the set X
To study the first order differentiabilitiy of the optimal value function v(u), for a given direction d∈U and the
optimal solution of the unperturbed problem x0∈S(0), Bonnans et al. (1998) consider the linearization of the family
of problems (Ptd)and its dual as follows
(PLd) : min
hf(x0,0)(h,d)subject toDG(x0,0)(h,d)∈TK(G(x0,0)),
(DLd) : max
λ∈Λ0(x0)DuL(x0,λ,0)d.
Theorem 13. (Theorem 3.1 in Bonnans et al. (1998)) Let ¯x(t)be anO(t2)-optimal trajectory of (Ptd)converging to
a point x0∈Φ(0) ast→0. Assumev(PLd)to be finite. Suppose that the following conditions hold:
1.x0satisfies the directional constraint qualification, which is implied if
0∈int{G(x0,0) +DxG(x0,0)X−K}.
2.v(td)≤v(0) +tv(PLd) +O(t2), t≥0(Equation 3.4 in (Bonnans et al., 1998)).
3. The strong second order sufficient condition (Equation 3.1 in (Bonnans et al., 1998)) holds, which is implied
if
sup
λ∈S(DLd)D2
xxL(x0,λ,0)(h,h)>0,∀h∈C(x0)\{0},
whereC(x0)denotes the critical cone.
Then ¯x(t)is Lipschitz stable at x0, i.e., fort≥0,/vextenddouble/vextenddouble¯x(t)−x0/vextenddouble/vextenddouble=O(t).
Proof. (Proof of Lemma 12) The key idea is to leverage Theorem 13, which quantifies the sensitivity of the optimal
solution of a “perturbed” optimization problem. More specifically, it is shown that the distance between the original
optimal solution and the optimal solution of the perturbed problem is upper-bounded by the magnitude of the
perturbation.
First, we show that ∀i∈[m], the projection problem Π/hatwideXs
i(x)can be formulated as a quadratic programming
with second-order cone constraints. The definition of /hatwideXs
ihas the following equivalent expression
/hatwideXs
i≜{x∈Rd:˜a⊤
kx≤bk,∀˜ak∈Ci,k,∀k∈[n]}={x∈Rd: max
˜ak∈Ci,k˜a⊤
kx≤bk,∀k∈[n]}
={x∈Rd: [/hatwideAi]k,:x+Br/vextenddouble/vextenddoublex/vextenddouble/vextenddouble≤bk,∀k∈[n]},(24)
20Published in Transactions on Machine Learning Research (10/2023)
where each second-order cone inequality: [/hatwideAi]k,:x+Br/vextenddouble/vextenddoublex/vextenddouble/vextenddouble≤bkcan be equivalently written as a linear matrix
inequality (LMI):
[/hatwideAi]k,:x+Br/vextenddouble/vextenddoublex/vextenddouble/vextenddouble≤bk⇔Gk(x,/hatwideAi)≜/bracketleftigg
(bk−[/hatwideAi]k,:x)Brx⊤
Brx (bk−[/hatwideAi]k,:x)I/bracketrightigg
⪰0. (25)
For simplicity, we define the following matrix
G(x,/hatwideAi)≜
G1(x,/hatwideAi)
G2(x,/hatwideAi)
...
Gn(x,/hatwideAi)
.
Considering the intersection of all LMIs, we have
/hatwideXs
i≜{x∈Rd:G(x,/hatwideAi)⪰0}. (26)
Based on (26), for a point x∈Rd, we have Π/hatwideXs
i(x) =x+ξi, whereξiis derived by solving the following optimization
problem
ξi=argminξξ⊤ξ,s.t.
G1(x+ξ,/hatwideAi)
G2(x+ξ,/hatwideAi)
...
Gn(x+ξ,/hatwideAi)
⪰0. (27)
Based on Lemma 2, we have/vextenddouble/vextenddouble[/hatwideAi]k,:−[/hatwideAj]k,:/vextenddouble/vextenddouble=O(1
Tρ),∀i,j∈[m]and∀k∈[n]. Therefore, [/hatwideAj]k,:can
be expressed as [/hatwideAi]k,:+ψk, where/vextenddouble/vextenddoubleψk/vextenddouble/vextenddouble=O(1
Tρ). With this expression, the projection Π/hatwideXs
j(x) =x+ξjcan
be formulated as a perturbed version of the optimization (27), where the perturbation is parameterized in terms of
ψ= [ψ1,...,ψn]as follows:
ξj=argminξξ⊤ξ,s.t.
G1(x+ξ,/hatwideAi+ψ)
G2(x+ξ,/hatwideAi+ψ)
...
Gn(x+ξ,/hatwideAi+ψ)
⪰0. (28)
To show that/vextenddouble/vextenddoubleΠ/hatwideXs
i(x)−Π/hatwideXs
j(x)/vextenddouble/vextenddouble=/vextenddouble/vextenddoubleξi−ξj/vextenddouble/vextenddouble=O(/vextenddouble/vextenddoubleψ/vextenddouble/vextenddouble) =O(1
Tρ), we apply Theorem 13, where three conditions
need to be satisfied: directional constraint qualification (DCQ), Equation 3.4 in Bonnans et al. (1998) and strong
second-order sufficient conditions (we refer readers to Bonnans et al. (1998) for detailed definitions).
•DCQ :
A sufficient condition for DCQ is constraint qualification (CQ) (see the definition in Bonnans et al. (1998)),
which is satisfied in our problem formulation if the first-order approximation of G(x+ξ,/hatwideAi+ψ)w.r.t. the
variableξcan be positive-definite. Noting that G(x+ξ,/hatwideAi+ψ)is an affine function of ξ, the first-order
approximation is exactly the original function. Now suppose that ∀i∈[m],/hatwideXs
ihas a strictly feasible point
(this is implied by the existence of the mutual shrunk polytope), which means there exists a ˆξsuch that
G(x+ˆξ,/hatwideAi+ψ)is positive-definite, and then CQ is satisfied.
•Equation 3.4 in Bonnans et al. (1998) :
In Bonnans et al. (1998), the authors provided the sufficient conditions for Equation 3.4: DCQ and second-
order regularity (Definition 2.2 in Bonnans et al. (1998)). DCQ, as mentioned previously, holds in our case,
and second-order regularity holds for semi-definite optimization, which is the case for our problem setup.
21Published in Transactions on Machine Learning Research (10/2023)
•Second-order sufficient conditions :
The strong second-order sufficient condition (Equation 3.1 in Bonnans et al. (1998)) has an alternative form
(Equation 3.3 in Bonnans et al. (1998)), which is satisfied in our problem setup since the Hessian of the
Lagrangian is 2I, which is positive-definite.
Since all the conditions above are met, the lemma is proved by applying Theorem 13.
A.3 Convex Part
Lemma 14. Let Algorithm 2 run with step size η >0and define xt≜1
m/summationtextm
i=1xi,tandyt≜1
m/summationtextm
i=1yi,t. Under
Assumptions 1 to 3 and the fact that gradients are bounded, i.e.,/vextenddouble/vextenddouble∇fi,t(x)/vextenddouble/vextenddouble≤Gfor any x∈Xs, we have that
∀i∈[m]
/vextenddouble/vextenddoublext−xi,t/vextenddouble/vextenddouble≤/parenleftbig
O(1
Tρ) + 2ηG/parenrightbig√mβ
1−β.
Proof. For the presentation simplicity, we define the following matrices
Xt≜[x1,t,...,xm,t],Yt≜[y1,t,...,ym,t],Gt≜[∇f1,t(x1,t),...,∇fm,t(xm,t)],andRt≜[r1,t,...,rm,t],
where ri,t≜yi,t−/parenleftbig
xi,t−η∇fi,t(xi,t)/parenrightbig
. Then, the update can be expressed as Xt=Yt−1P=/parenleftbig
Xt−1−ηGt−1+
Rt−1/parenrightbig
P.
Expanding the update recursively, we have
Xt=XTsP(t−Ts)−ηt−Ts/summationdisplay
l=1Gt−lPl+t−Ts/summationdisplay
l=1Rt−lPl. (29)
Since Pis doubly stochastic, we have Pk1=1for allk≥1. Based on the geometric mixing bound of Pand the
above equation we get
/vextenddouble/vextenddoublext−xi,t/vextenddouble/vextenddouble=/vextenddouble/vextenddoubleXt(1
m1−ei)/vextenddouble/vextenddouble
≤/vextenddouble/vextenddoublexTs−XTs[P(t−Ts)]:,i/vextenddouble/vextenddouble+ηt−Ts/summationdisplay
l=1/vextenddouble/vextenddoubleGt−l(1
m1−[Pl]:,i)/vextenddouble/vextenddouble+t−Ts/summationdisplay
l=1/vextenddouble/vextenddoubleRt−l(1
m1−[Pl]:,i)/vextenddouble/vextenddouble
≤t−Ts/summationdisplay
l=1(ηG)√mβl+t−Ts/summationdisplay
l=1/parenleftbig
O(1
Tρ) +ηG/parenrightbig√mβl
≤/parenleftbig
O(1
Tρ) + 2ηG/parenrightbig√mβ
1−β,
where/vextenddouble/vextenddoublexTs−XTs[Pt−Ts]:,i/vextenddouble/vextenddouble= 0by the identical initialization of all agents with the same action at Ts, and the other
inequality is based on Lemma 12 as follows
/vextenddouble/vextenddouble/vextenddoubleri,t/vextenddouble/vextenddouble/vextenddouble=/vextenddouble/vextenddouble/vextenddoubleyi,t−/parenleftbig
xi,t−η∇fi,t(xi,t)/parenrightbig/vextenddouble/vextenddouble/vextenddouble
≤/vextenddouble/vextenddouble/vextenddouble/summationdisplay
j[P]jiΠ/hatwideXs
i[yj,t−1]−/parenleftig/summationdisplay
j[P]jiyj,t−1−η∇fi,t(xi,t)/parenrightig/vextenddouble/vextenddouble/vextenddouble
≤O(T−ρ) +ηG.
22Published in Transactions on Machine Learning Research (10/2023)
Proof of Theorem 3. First, we decompose the individual regret of agent jinto three terms:
/summationdisplay
t/summationdisplay
ifi,t(xj,t)−/summationdisplay
tft(x∗
t) =Ts−1/summationdisplay
t=1/summationdisplay
ifi,t(xj,t)−fi,t(x∗
t)
/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
Term I+T/summationdisplay
t=Ts/summationdisplay
ifi,t(xj,t)−fi,t(˜x∗
t)
/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
Term II+T/summationdisplay
t=Tsft(˜x∗
t)−ft(x∗
t)
/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
Term III,
(30)
where ˜x∗
tis the projection of x∗
tonXs
in, which is a mutual subset of {/hatwideXs
i}i∈[m]withτin= 2BrLbased on Equation
(22) in Lemma 11. We now proceed to bound each term.
The upper bound of Term I :
Note that by choosing γ≤∆s
LLA, we have∀i∈[m]andt∈[1,...,T 0+T1]
[A]k,:xi,t= [A]k,:((1−γ)xs+γζi,t)≤(1−γ)bs
k+ ∆s≤(1−γ)bs
k+ (bk−bs
k)<bk, (31)
which implies the safeness of the action.
Based on the Lipschitz property of the function sequence, we have
Ts−1/summationdisplay
t=1/summationdisplay
ifi,t(xj,t)−fi,t(x∗
t)≤Ts−1/summationdisplay
t=1/summationdisplay
iG/vextenddouble/vextenddoublexj,t−x∗
t/vextenddouble/vextenddouble≤2GLm (T0+T1). (32)
The upper bound of Term II :
Based on the update rule, ∀i∈[m]andt∈[Ts,...,T ]we have
fi,t(xi,t)−fi,t(˜x∗
t)≤∇fi,t(xi,t)⊤(xi,t−˜x∗
t)
=1
η/bracketleftig1
2η2/vextenddouble/vextenddouble∇fi,t(xi,t)/vextenddouble/vextenddouble2+1
2/vextenddouble/vextenddoublexi,t−˜x∗
t/vextenddouble/vextenddouble2−1
2/vextenddouble/vextenddoublexi,t−˜x∗
t−η∇fi,t(xi,t)/vextenddouble/vextenddouble2/bracketrightig
≤1
η/bracketleftig1
2η2/vextenddouble/vextenddouble∇fi,t(xi,t)/vextenddouble/vextenddouble2+1
2/vextenddouble/vextenddoublexi,t−˜x∗
t/vextenddouble/vextenddouble2−1
2/vextenddouble/vextenddoubleyi,t−˜x∗
t/vextenddouble/vextenddouble2/bracketrightig
=1
η/bracketleftig1
2η2/vextenddouble/vextenddouble∇fi,t(xi,t)/vextenddouble/vextenddouble2+1
2/vextenddouble/vextenddouble/vextenddouble/summationdisplay
j[P]jiyj,t−1−˜x∗
t/vextenddouble/vextenddouble/vextenddouble2
−1
2/vextenddouble/vextenddoubleyi,t−˜x∗
t/vextenddouble/vextenddouble2/bracketrightig
≤1
η/bracketleftig1
2η2/vextenddouble/vextenddouble∇fi,t(xi,t)/vextenddouble/vextenddouble2+1
2/summationdisplay
j[P]ji/vextenddouble/vextenddoubleyj,t−1−˜x∗
t/vextenddouble/vextenddouble2−1
2/vextenddouble/vextenddoubleyi,t−˜x∗
t/vextenddouble/vextenddouble2/bracketrightig
,(33)
where the second inequality is due to the projection property that/vextenddouble/vextenddoubleyi,t−˜x∗
t/vextenddouble/vextenddouble≤/vextenddouble/vextenddoublexi,t−η∇fi,t(xi,t)−˜x∗
t/vextenddouble/vextenddouble, and the
third inequality is due to the convexity of the square function.
Based on Equation (33) and Lemma 14, we have
fi,t(xj,t)−fi,t(˜x∗
t) =fi,t(xj,t)−fi,t(xi,t) +fi,t(xi,t)−fi,t(˜x∗
t)
≤G/vextenddouble/vextenddoublexj,t−xi,t/vextenddouble/vextenddouble+fi,t(xi,t)−fi,t(˜x∗
t)
≤2G/parenleftbig
O(1
Tρ) + 2ηG/parenrightbig√mβ
1−β+1
2η/vextenddouble/vextenddouble∇fi,t(xi,t)/vextenddouble/vextenddouble2+1
2η/summationdisplay
j[P]ji/vextenddouble/vextenddoubleyj,t−1−˜x∗
t/vextenddouble/vextenddouble2−1
2η/vextenddouble/vextenddoubleyi,t−˜x∗
t/vextenddouble/vextenddouble2.(34)
23Published in Transactions on Machine Learning Research (10/2023)
Summing Equation (34) over i, we get
/summationdisplay
i(fi,t(xj,t)−fi,t(˜x∗
t))
≤2mG/parenleftbig
O(1
Tρ) + 2ηG/parenrightbig√mβ
1−β+η
2/summationdisplay
i/vextenddouble/vextenddouble∇fi,t(xi,t)/vextenddouble/vextenddouble2+1
2η/summationdisplay
j/vextenddouble/vextenddoubleyj,t−1−˜x∗
t/vextenddouble/vextenddouble2−1
2η/summationdisplay
i/vextenddouble/vextenddoubleyi,t−˜x∗
t/vextenddouble/vextenddouble2
=2mG/parenleftbig
O(1
Tρ) + 2ηG/parenrightbig√mβ
1−β+η
2/summationdisplay
i/vextenddouble/vextenddouble∇fi,t(xi,t)/vextenddouble/vextenddouble2+1
2η/summationdisplay
i/parenleftig/vextenddouble/vextenddoubleyi,t−1/vextenddouble/vextenddouble2−/vextenddouble/vextenddoubleyi,t/vextenddouble/vextenddouble2+ 2(yi,t−yi,t−1)⊤˜x∗
t/parenrightig
.
(35)
Summing Equation (35) over t∈[Ts,...,T ], we have
T/summationdisplay
t=Ts/summationdisplay
i(fi,t(xj,t)−fi,t(˜x∗
t))
≤η
2T/summationdisplay
t=Ts/summationdisplay
i/vextenddouble/vextenddouble∇fi,t(xi,t)/vextenddouble/vextenddouble2+1
2η/summationdisplay
i/vextenddouble/vextenddoubleyi,Ts−1/vextenddouble/vextenddouble2+1
η/parenleftbig/summationdisplay
iy⊤
i,T˜x∗
T−/summationdisplay
iy⊤
i,Ts−1˜x∗
Ts−1/parenrightbig
+1
ηT−1/summationdisplay
t=Ts−1/summationdisplay
i(˜x∗
t−˜x∗
t+1)⊤yi,t+ 2TmG/parenleftbig
O(1
Tρ) + 2ηG/parenrightbig√mβ
1−β.(36)
The upper bound of Term III :
Based on Lemma 9, we have for any x∗
t∈Xsand its projection to Xs
in, denoted by ˜x∗
t, that
T/summationdisplay
t=Ts/summationdisplay
i(fi,t(˜x∗
t)−fi,t(x∗
t))≤T/summationdisplay
t=Ts/summationdisplay
iG/vextenddouble/vextenddouble˜x∗
t−x∗
t/vextenddouble/vextenddouble≤mTG2√
dLBr
C(A,b). (37)
Substituting Equations (32), (36) and (37) into Equation (30), we get
/summationdisplay
t/summationdisplay
i(fi,t(xj,t)−fi,t(x∗
t))
≤2GLm (T0+T1) +ηmTG2
2+1
2η/summationdisplay
i/vextenddouble/vextenddoubleyi,Ts−1/vextenddouble/vextenddouble2+1
η/parenleftbig/summationdisplay
iy⊤
i,T˜x∗
T−/summationdisplay
iy⊤
i,Ts−1˜x∗
Ts−1/parenrightbig
+1
ηT−1/summationdisplay
t=Ts−1/summationdisplay
i(˜x∗
t−˜x∗
t+1)⊤yi,t+ 2TmG/parenleftbig
O(1
Tρ) + 2ηG/parenrightbig√mβ
1−β+mTG2√
dLBr
C(A,b),(38)
which isO(T0+T1+1
η+1
ηC∗
T+T√
logT0√T0+βηT
(1−β))and the final regret bound is derived by substituting the choices
ofηandT0into above.
A.4 Non-convex Part
Lemma 15 (Lemma 4 in Ghai et al. (2022)) .Suppose Assumptions 5, 6, 7 hold and ut=q(xt), then/vextenddouble/vextenddoubleq(xt+1)−
ut+1/vextenddouble/vextenddouble=O(W4G3/2
Fη3/2)based on the following update rule:
ut+1=argminu∈Xs′/braceleftbigg
∇˜ft(ut)⊤u+1
ηDϕ(u,ut)/bracerightbigg
,
xt+1=argminx∈Xs/braceleftbigg
∇ft(xt)⊤x+1
2η/vextenddouble/vextenddoublex−xt/vextenddouble/vextenddouble2/bracerightbigg
.
24Published in Transactions on Machine Learning Research (10/2023)
Theorem 16 (Theorem 7 in Ghai et al. (2022)) .Given a convex and compact domain X⊂Xs, and not necessarily
convex loss ft(·)satisfying Assumption 7. When Assumption 8 is met, there exists an OMD object with convex loss
˜ft(·), a convex domain and a strongly convex regularization ϕsatisfying Assumption 5.
Lemma 17. Suppose Assumptions 5-7 hold and ui,t=q(xi,t),∀i∈[m]; then
/vextenddouble/vextenddoubleq(xi,t+1)−u′
i,t+1/vextenddouble/vextenddouble=O(1
T2ρ+η3/2),
based on the following update rules:
zi,t=argminu∈/hatwideXs′
i/braceleftbigg
∇˜fi,t(ui,t)⊤u+1
ηDϕ(u,ui,t)/bracerightbigg
,
u′
i,t+1=/summationdisplay
j[P]jizj,t,
yi,t=argminx∈/hatwideXs
i/braceleftbigg
∇fi,t(xi,t)⊤x+1
2η/vextenddouble/vextenddoublex−xi,t/vextenddouble/vextenddouble2/bracerightbigg
,
xi,t+1=/summationdisplay
j[P]jiyj,t.(39)
Proof. We first upper bound/vextenddouble/vextenddoubleq(xi,t+1)−u′
i,t+1/vextenddouble/vextenddoubleas follows
/vextenddouble/vextenddoubleq(xi,t+1)−u′
i,t+1/vextenddouble/vextenddouble≤/vextenddouble/vextenddouble/summationdisplay
j[P]jizj,t−/summationdisplay
j[P]jiq(yj,t)/vextenddouble/vextenddouble+/vextenddouble/vextenddouble/summationdisplay
j[P]jiq(yj,t)−q(/summationdisplay
j[P]jiyj,t)/vextenddouble/vextenddouble.(40)
To bound the second term, we consider the Taylor expansion of q(y)w.r.t. a point ˆyin the convex hull of {yi,t}i:
/vextenddouble/vextenddouble/summationdisplay
j[P]jiq(yj,t)−q(/summationdisplay
j[P]jiyj,t)/vextenddouble/vextenddouble≤/vextenddouble/vextenddouble/summationdisplay
j[P]ji/parenleftig
q(ˆy) +Jq(ˆy)(yj,t−ˆy) +O/parenleftbig/vextenddouble/vextenddoubleyj,t−ˆy/vextenddouble/vextenddouble2/parenrightbig/parenrightig
−/parenleftig
q(ˆy) +Jq(ˆy)(/summationdisplay
j[P]jiyj,t−ˆy) +O/parenleftbig/vextenddouble/vextenddouble/summationdisplay
j[P]jiyj,t−ˆy/vextenddouble/vextenddouble2/parenrightbig/parenrightig/vextenddouble/vextenddouble
≤O/parenleftbig/summationdisplay
j[P]ji/vextenddouble/vextenddoubleyj,t−ˆy/vextenddouble/vextenddouble2/parenrightbig
+O/parenleftbig/vextenddouble/vextenddouble/summationdisplay
j[P]jiyj,t−ˆy/vextenddouble/vextenddouble2/parenrightbig
≤O(D2),
(41)
whereDdenotes the diameter of the convex hull of {yi,t}and is upper bounded as follows
D≜max
(i,j)/vextenddouble/vextenddoubleyi,t−yj,t/vextenddouble/vextenddouble
= max
(i,j)/vextenddouble/vextenddoubleΠ/hatwideXs
i/parenleftbig
xi,t−η∇fi,t(xi,t)/parenrightbig
−Π/hatwideXs
j/parenleftbig
xj,t−η∇fj,t(xj,t)/parenrightbig/vextenddouble/vextenddouble
= max
(i,j)/vextenddouble/vextenddoubleΠ/hatwideXs
i/parenleftbig
xi,t−η∇fi,t(xi,t)/parenrightbig
−Π/hatwideXs
j/parenleftbig
xi,t−η∇fi,t(xi,t)/parenrightbig
+ Π/hatwideXs
j/parenleftbig
xi,t−η∇fi,t(xi,t)/parenrightbig
−Π/hatwideXs
j/parenleftbig
xj,t−η∇fj,t(xj,t)/parenrightbig/vextenddouble/vextenddouble
≤max
(i,j)/vextenddouble/vextenddoubleΠ/hatwideXs
i/parenleftbig
xi,t−η∇fi,t(xi,t)/parenrightbig
−Π/hatwideXs
j/parenleftbig
xi,t−η∇fi,t(xi,t)/parenrightbig/vextenddouble/vextenddouble
+/vextenddouble/vextenddouble/parenleftbig
xi,t−η∇fi,t(xi,t)/parenrightbig
−/parenleftbig
xj,t−η∇fj,t(xj,t)/parenrightbig/vextenddouble/vextenddouble
≤O(1
Tρ) + 2/parenleftbigg/parenleftbig
O(1
Tρ) + 2ηG/parenrightbig√mβ
1−β/parenrightbigg
+ 2Gη=O(1
Tρ+η).(42)
25Published in Transactions on Machine Learning Research (10/2023)
The first inequality follows from the non-expansive property of projection, where/vextenddouble/vextenddoubleΠX(x)−ΠX(y)/vextenddouble/vextenddouble≤/vextenddouble/vextenddoublex−y/vextenddouble/vextenddouble
for any x,yand a closed convex set X, and the last inequality is based on Lemma 12, Lemma 14 and the Lipschitz
continuity of the function sequence.
Substituting Equations (41) and (42) into Equation (40) and based on Lemma 15, we have
/vextenddouble/vextenddoubleq(xi,t+1)−u′
i,t+1/vextenddouble/vextenddouble≤/vextenddouble/vextenddouble/summationdisplay
j[P]jizj,t−/summationdisplay
j[P]jiq(yj,t)/vextenddouble/vextenddouble+/vextenddouble/vextenddouble/summationdisplay
j[P]jiq(yj,t)−q(/summationdisplay
j[P]jiyj,t)/vextenddouble/vextenddouble
≤O(W4G3/2
Fη3/2) +O(1
T2ρ+η2) =O(1
T2ρ+η3/2),(43)
whenηis small enough.
Proof of Theorem 5. As for the proof of Theorem 3, we decompose the individual regret into three terms:
/summationdisplay
t/summationdisplay
ifi,t(xj,t)−/summationdisplay
tft(x∗
t) =Ts−1/summationdisplay
t=1/summationdisplay
ifi,t(xj,t)−fi,t(x∗
t)
/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
Term I+T/summationdisplay
t=Ts/summationdisplay
ifi,t(xj,t)−fi,t(˜x∗
t)
/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
Term II+T/summationdisplay
t=Tsft(˜x∗
t)−ft(x∗
t)
/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
Term III,
(44)
where ˜x∗
tis the projection of x∗
tonXs
in, which is a mutual subset of {/hatwideXs
i}i∈[m]withτin= 2BrLbased on Equation
(22).
The upper bound of Term I :
Similar to the proof of convex part, during the estimation phase, γis less than∆s
LLAto ensure the safeness of each
agent’s action, and based on the Lipschitz property we have
Ts−1/summationdisplay
t=1/summationdisplay
ifi,t(xj,t)−fi,t(x∗
t) =Ts−1/summationdisplay
t=1/summationdisplay
i˜fi,t/parenleftbig
q(xj,t)/parenrightbig
−˜fi,t/parenleftbig
q(x∗
t)/parenrightbig
≤Ts−1/summationdisplay
t=1/summationdisplay
iGFW/vextenddouble/vextenddoublexj,t−x∗
t/vextenddouble/vextenddouble≤2GFWLm (T0+T1). (45)
The upper bound of Term II :
Define/hatwideXs′
i≜{q(x)|x∈/hatwideXs
i}, (same forXs
inandXs). Then, for any q(˜x∗
t) =˜u∗
t∈Xs′
in, based on Equation (39), we
26Published in Transactions on Machine Learning Research (10/2023)
have
η(fi,t(xi,t)−fi,t(˜x∗
t)) =η/parenleftbig˜fi,t(ui,t)−˜fi,t(˜u∗
t)/parenrightbig
≤η∇˜fi,t(ui,t)⊤(ui,t−˜u∗
t)
=/parenleftbig
∇ϕ(ui,t)−∇ϕ(zi,t)−η∇˜fi,t(ui,t)/parenrightbig⊤(˜u∗
t−zi,t)
+ (∇ϕ(zi,t)−∇ϕ(ui,t))⊤(˜u∗
t−zi,t) +η∇˜fi,t(ui,t)⊤(ui,t−zi,t)
≤(∇ϕ(zi,t)−∇ϕ(ui,t))⊤(˜u∗
t−zi,t) +η∇˜fi,t(ui,t)⊤(ui,t−zi,t)
=Dϕ(˜u∗
t,ui,t)−Dϕ(˜u∗
t,zi,t)−Dϕ(zi,t,ui,t) +η∇˜fi,t(ui,t)⊤(ui,t−zi,t)
≤Dϕ(˜u∗
t,ui,t)−Dϕ(˜u∗
t,zi,t)−Dϕ(zi,t,ui,t) +1
2/vextenddouble/vextenddoubleui,t−zi,t/vextenddouble/vextenddouble2+η2
2/vextenddouble/vextenddouble∇˜fi,t(ui,t)/vextenddouble/vextenddouble2
≤Dϕ(˜u∗
t,ui,t)−Dϕ(˜u∗
t,zi,t) +η2
2/vextenddouble/vextenddouble∇˜fi,t(ui,t)/vextenddouble/vextenddouble2
=Dϕ(˜u∗
t,ui,t)−Dϕ(˜u∗
t,u′
i,t) +Dϕ(˜u∗
t,u′
i,t)−Dϕ(˜u∗
t,zi,t) +η2
2/vextenddouble/vextenddouble∇˜fi,t(ui,t)/vextenddouble/vextenddouble2
≤Dϕ(˜u∗
t,ui,t)−Dϕ(˜u∗
t,u′
i,t) +/summationdisplay
j[P]jiDϕ(˜u∗
t,zj,t−1)−Dϕ(˜u∗
t,zi,t) +η2
2/vextenddouble/vextenddouble∇˜fi,t(ui,t)/vextenddouble/vextenddouble2,
(46)
where the second inequality is based on the optimality of zi,t; the fourth inequality is due to the strong convexity of
ϕ(·)and the fifth inequality is based on Assumption 9.
Based on Theorem 16, Lemma 17, and the Lipschitz assumption on Dϕ, we have
/vextenddouble/vextenddoubleDϕ(˜u∗
t,ui,t)−Dϕ(˜u∗
t,u′
i,t)/vextenddouble/vextenddouble≤W/vextenddouble/vextenddoubleui,t−u′
i,t/vextenddouble/vextenddouble≤O/parenleftbig
W(1
T2ρ+η3/2)/parenrightbig
. (47)
And based on Lemma 14, we get
max
i,j∈[m]/vextenddouble/vextenddoubleui,t−uj,t/vextenddouble/vextenddouble= max
i,j∈[m]/vextenddouble/vextenddoubleq(xi,t)−q(xj,t)/vextenddouble/vextenddouble=O/parenleftbig
Wη/parenrightbig
.(48)
With Equations (46), (47) and (48), we derive
˜fi,t(uj,t)−˜fi,t(˜u∗
t) =˜fi,t(uj,t)−˜fi,t(ui,t) +˜fi,t(ui,t)−˜fi,t(˜u∗
t)
≤GF/vextenddouble/vextenddoubleui,t−uj,t/vextenddouble/vextenddouble+O/parenleftbig
W(1
ηT2ρ+η1/2)/parenrightbig
+1
η/summationdisplay
j[P]jiDϕ(˜u∗
t,zj,t−1)−1
ηDϕ(˜u∗
t,zi,t) +η
2/vextenddouble/vextenddouble∇˜fi,t(ui,t)/vextenddouble/vextenddouble2
≤O/parenleftbig
GFWη/parenrightbig
+O/parenleftbig
W(1
ηT2ρ+η1/2)/parenrightbig
+1
η/summationdisplay
j[P]jiDϕ(˜u∗
t,zj,t−1)−1
ηDϕ(˜u∗
t,zi,t) +η
2/vextenddouble/vextenddouble∇˜fi,t(ui,t)/vextenddouble/vextenddouble2.(49)
Based on the definition of Bregman divergence, we have the following relationship
Dϕ(˜u∗
t,zi,t−1)−Dϕ(˜u∗
t,zi,t)
= (∇ϕ(zi,t)−∇ϕ(zi,t−1))⊤(˜u∗
t−zi,t) +Dϕ(zi,t,zi,t−1)
= (∇ϕ(zi,t)−∇ϕ(zi,t−1))⊤˜u∗
t+/parenleftbig
ϕ(zi,t)−∇ϕ(zi,t)⊤zi,t/parenrightbig
−/parenleftbig
ϕ(zi,t−1)−∇ϕ(zi,t−1)⊤zi,t−1/parenrightbig
.(50)
27Published in Transactions on Machine Learning Research (10/2023)
Summing Equation (49) over i, based on Equation (50) we get
/summationdisplay
i˜fi,t(uj,t)−˜fi,t(˜u∗
t)
≤O/parenleftbig
mGFWη/parenrightbig
+O/parenleftbig
mW(1
ηT2ρ+η1/2)/parenrightbig
+/summationdisplay
iη
2/vextenddouble/vextenddouble∇˜fi,t(ui,t)/vextenddouble/vextenddouble2
+1
η/summationdisplay
i/bracketleftig
(∇ϕ(zi,t)−∇ϕ(zi,t−1))⊤˜u∗
t+/parenleftbig
ϕ(zi,t)−∇ϕ(zi,t)⊤zi,t/parenrightbig
−/parenleftbig
ϕ(zi,t−1)−∇ϕ(zi,t−1)⊤zi,t−1/parenrightbig/bracketrightig
.(51)
Then, by summing Equation (51) over [Ts,...,T ], we have
T/summationdisplay
t=Ts/summationdisplay
i˜fi,t(uj,t)−˜fi,t(˜u∗
t)
≤O/parenleftbig
mTGFWη/parenrightbig
+O/parenleftbig
mTW (1
ηT2ρ+η1/2)/parenrightbig
+T/summationdisplay
t=Ts/summationdisplay
iη
2/vextenddouble/vextenddouble∇˜fi,t(ui,t)/vextenddouble/vextenddouble2
+1
η/bracketleftiggT−1/summationdisplay
t=Ts−1/summationdisplay
i(˜u∗
t−˜u∗
t+1)⊤∇ϕ(zi,t) +/summationdisplay
i∇ϕ(zi,T)⊤˜u∗
T−/summationdisplay
i∇ϕ(zi,Ts−1)⊤˜u∗
Ts−1/bracketrightigg
+1
η/summationdisplay
i/bracketleftbig/parenleftbig
ϕ(zi,T)−∇ϕ(zi,T)⊤zi,T/parenrightbig
−/parenleftbig
ϕ(zi,Ts−1)−∇ϕ(zi,Ts−1)⊤zi,Ts−1/parenrightbig/bracketrightbig
.(52)
The upper bound of Term III :
Based on Lemma 9, we have for any x∗
t∈Xsand its projection to Xs
in:˜x∗
t
T/summationdisplay
t=Ts/summationdisplay
i/parenleftbig˜fi,t(q(˜x∗
t))−˜fi,t(q(x∗
t))/parenrightbig
≤T/summationdisplay
t=Ts/summationdisplay
iGFW/vextenddouble/vextenddouble˜x∗
t−x∗
t/vextenddouble/vextenddouble≤mTGFW2√
dLBr
C(A,b). (53)
Substituting Equations (45), (52) and (53) into Equation (44), the final regret bound is as
/summationdisplay
t=1/summationdisplay
i(fi,t(xj,t)−fi,t(x∗
t))
≤O/parenleftbig
mTGFWη/parenrightbig
+O/parenleftbig
mTW (1
ηT2ρ+η1/2)/parenrightbig
+T/summationdisplay
t=Ts/summationdisplay
iη
2/vextenddouble/vextenddouble∇˜fi,t(ui,t)/vextenddouble/vextenddouble2
+1
η/bracketleftiggT−1/summationdisplay
t=Ts−1/summationdisplay
i(˜u∗
t−˜u∗
t+1)⊤∇ϕ(zi,t) +/summationdisplay
i∇ϕ(zi,T)⊤˜u∗
T−/summationdisplay
i∇ϕ(zi,Ts−1)⊤˜u∗
Ts−1/bracketrightigg
+ 2GFWLm (T0+T1)
+1
η/summationdisplay
i/bracketleftbig/parenleftbig
ϕ(zi,T)−∇ϕ(zi,T)⊤zi,T/parenrightbig
−/parenleftbig
ϕ(zi,Ts−1)−∇ϕ(zi,Ts−1)⊤zi,Ts−1/parenrightbig/bracketrightbig
+mTGFW2√
dLBr
C(A,b)
=O(T0+T1+T√η+T√logT0√T0+1
η+1
ηT/summationdisplay
t=Ts/vextenddouble/vextenddouble˜u∗
t−˜u∗
t+1/vextenddouble/vextenddouble),
(54)
where the final regret bound is proved by applying the specified ηandT0. By choosing ρas a large enough number,
1
ηT2ρis dominated by η1/2.
28