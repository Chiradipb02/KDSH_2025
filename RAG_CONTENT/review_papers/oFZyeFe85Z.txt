Under review as submission to TMLR
Summary Statistic Privacy in Data Sharing
Anonymous authors
Paper under double-blind review
Abstract
Data sharing between different parties has become increasingly common across industry
and academia. An important class of privacy concerns that arises in data sharing scenar-
ios regards the underlying distribution of data. For example, the total traffic volume of
data from a networking company can reveal the scale of its business, which may be consid-
ered a trade secret. Unfortunately, existing privacy frameworks (e.g., differential privacy,
anonymization) do not adequately address such concerns. In this paper, we propose sum-
mary statistic privacy , a framework for analyzing and protecting these summary statistic
privacy concerns. We propose a class of quantization mechanisms that can be tailored to
various data distributions and statistical secrets, and analyze their privacy-distortion trade-
offs under our framework. We prove corresponding lower bounds on the privacy-utility
tradeoff, which match the tradeoffs of the quantization mechanism under certain regimes,
up to small constant factors. Finally, we demonstrate that the proposed quantization mech-
anisms achieve better privacy-distortion tradeoffs than alternative privacy mechanisms on
real-world datasets.
1 Introduction
Data sharing between organizations is an important driver for many use cases, including data-driven product
development (Lee & Whang, 2000), industry-wide coordination efforts (e.g., cybersecurity (Choucri et al.,
2016), law enforcement (Jacobs & Blitsa, 2008)), and the creation of benchmarks for evaluating scientific
progress (Deng et al., 2009; Reiss et al., 2011; Luo et al., 2021). For example, network traces shared from
customers to networking vendors enable vendors to debug and improve products (Yin et al., 2022; cai).
Medical data shared between hospitals (Esteban et al., 2017; Warren et al., 2019) enables them to develop
new machine-learning-based diagnosis algorithms collaboratively (Chaibub Neto et al., 2019). Data shared
by researchers allow their research to be reproducible by others (Deng et al., 2009; Lin et al., 2020). In
recent years, data sharing has grown into its own sub-industry (e.g., data marketplaces on platforms such
as Databricks and Snowflake). Shared data can take many forms, including processed or scrubbed raw
data (Reiss et al., 2012; Google, 2018; Commission, 2018; Warren et al., 2019), aggregate analytics, and/or
synthetic data (Liu & Wu, 2022).
However, summary statistics of the shared data may leak sensitive information (Suri & Evans, 2021; Suri
et al., 2023). For example, property inference attacks allow an attacker to infer properties about the individ-
uals in the training dataset of a released machine learning model (Ateniese et al., 2015; Ganju et al., 2018;
Zhang et al., 2021; Mahloujifar et al., 2022; Chaudhari et al., 2022). A video content provider that shares
video session data may wish to hide the total or mean traffic volume, which could be used to infer the com-
pany’s total revenue (Manousis et al., 2021). A cloud provider that shares cluster performance traces may
not want to reveal the proportions of different server types that the cloud provider owns, which are regarded
as business secrets (Lin et al., 2020). Note that this information (total/mean traffic volume, proportions of
data types) cannot be inferred from any single record, but is inherent to the overall data distribution (or the
aggregate dataset).
Unfortunately, existing privacy metrics and privacy-preserving data sharing algorithms do not adequately
address these summary statistic privacy concerns . They either focus on protecting the privacy of individual
records in a database (e.g., differential privacy (Dwork et al., 2006), anonymization (Reiss et al., 2012), sub-
1Under review as submission to TMLR
GenderSalary…User1User2…
DataholderOriginaldataDistribution:secret≜"()
SummaryStatisticPrivacyToolboxDatareleasemechanism…selectDatareleasemechanismGenderSalary…User1User2…Releaseddata
DatauserPreventattackersfromguessingthesecretReleaseddatahasgoodutility
AttackerGuessthesecretConventionalapproaches(e.g.,differentialprivacy)focuson protecting"rowlevel"properties,notdistributionalproperties
Figure 1: Problem overview. The data holder produces released data and wants to hide statistical secrets
of the original data. The data user requires that the utility of the released data be good. The attacker
(could be the data user) also observes the released data, and wants to guess the secretsof the original data.
Note that we focus on secrets about the underlying distribution (e.g., mean, quantile, standard deviation,
of a specific data column). As a comparison, many of existing frameworks (e.g., differential privacy (Dwork
et al., 2006), anonymization (Reiss et al., 2012), sub-sampling (Reiss et al., 2012)) protect information from
individual samples (rows) . Our end goal is to provide a summary statistic privacy toolbox for data holders to
use. The summary statistic privacy toolbox contains data release mechanisms for a set of pre-defined secrets
and data distributions. Data holders can choose the mechanism according to the secret that they want to
hide and the closest data distributions.
sampling (Reiss et al., 2012)), or are designed for algorithms that release low-dimension statistical queries of
the dataset instead of the entire dataset (Zhang et al., 2022; Makhdoumi et al., 2014; Issa et al., 2019). For
example, differential privacy (DP) (Dwork et al., 2006), a de facto privacy definition, evaluates how much
individual samples influence the final output of an algorithm. Assume that a video content provider has a
dataset of daily page views that they want to release, and they are concerned about the meanpage views
(as this implies the revenue). A typical DP algorithm (Wasserman & Zhou, 2010) would add noise (e.g.,
Laplace) to the individual page view counts. This process does not change the meanof the entire data on
expectation. Indeed, DP mechanisms have been shown not to protect summary statistics (Ateniese et al.,
2015) (in fact, they are designed to preserve them). See more discussion in §2.2.
Hence, a privacy framework is needed for defining, analyzing, and protecting summary statistic privacy
concerns in data sharing settings. Early work in this space has aimed to obfuscate only between two possible
datadistributions(Suri&Evans,2021;Surietal.,2023), orhasbeenimplicitlydesignedforthereleaseoflow-
dimensional query release (Zhang et al., 2022). In this paper, we aim to design a general summary statistic
privacy framework that can apply to general data release settings. At a high level, the proposed framework
works as follows (detailed formulation in §3). A data holder first chooses one or more secrets, which are
mathematically defined as functions of the data holder’s data distribution. For example, a video analytics
company might choose the mean daily observed traffic as a secret quantity. Then, the data holder obfuscates
their data according to some mechanism and releases the output (Fig. 1). Our framework quantifies the
privacyof this mechanism by analyzing the probability that a worst-case attacker can infer the data holder’s
true secret after observing the output. To capture the utility of released data, we define the distortion of a
mechanism as the worst-case distance (where the distance metric can be chosen by the data holder or data
user) between the original and released data distributions. Our goal is to design data release mechanisms
that control tradeoffs between privacy and distortion.
1.1 Contributions
Our contributions are as follows.
2Under review as submission to TMLR
•Formulation (§3): We formalize the notion of summary statistic privacy and propose privacy and
distortion metrics tailored to data sharing applications. Intuitively, we define privacy as a worst-case
adversary’s probability of guessing a secret function of the underlying data distribution. We define
distortion as the worst-case distributional distance1between the original data distribution and the
released, perturbed data distribution. Precise definitions are in §3.
•Mechanism design (§5): We propose a class of mechanisms that achieve summary statistic pri-
vacy called quantization mechanisms , which intuitively quantize a data distribution’s parameters2into
bins. We present a sawtooth technique for theoretically analyzing the quantization mechanism’s privacy
tradeoff under various types of secret functions and data distributions (§5.3). Intuitively, the sawtooth
technique exploits the geometry of the distribution parameter(s) to divide the parametric space into two
regions: one in which privacy risk is small and analytically tractable, and another in which privacy risk
can be high, but which occurs with low probability. The method is named after the boundary of the
tractable region, which has a sawtooth shape. We use the sawtooth technique to analyze the quantiza-
tion mechanism under various secret functions and data distributions (summary in Table 1). For most of
these case studies, we provide concrete upper bounds characterizing the exact privacy-distortion trade-
off under a family of priors over the true data distribution parameters. For the remaining case studies,
we provide a dynamic programming algorithm that efficiently numerically instantiates the quantization
mechanism.
•Lower bounds (§4): We derive general lower bounds on distortion given a privacy budget for any
mechanism. These bounds depend on both the secret function and the data distribution. We then
instantiate the lower bounds for each of our case studies to show that for the case studies we analyze
theoretically in Table 1, our proposed quantization mechanism achieves a privacy-distortion tradeoff
within a small constant factor of optimal (usually 3) in the regime where quantization bins are small
relative to the overall support set of the distribution parameters.
•Empirical evaluation (§7): We give empirical results showing how to use summary statistic privacy
to release a real dataset, and how to evaluate the corresponding summary statistic privacy metric. We
show that the proposed quantization mechanism achieves better privacy-distortion tradeoffs than other
natural privacy mechanisms.
Thispaperisonlyafirststepinthestudyofsummarystatisticprivacy. Ourformulationhasmanylimitations
and leaves many questions unanswered (§9). Still, we hope it will draw attention to what we believe to be
an important privacy concern and research question.
2 Motivation and Related Work
In this section, we discuss motivating scenarios where summary statistic privacy is a concern (§2.1), and
why existing privacy frameworks are not able to capture and protect summary statistic privacy (§2.2).
2.1 Motivating Scenarios
Whether sharing data models (e.g., classifiers (Ateniese et al., 2015; Ganju et al., 2018; Mahloujifar et al.,
2022; Chaudhari et al., 2022), generative models (Zhou et al., 2021)) or datasets (e.g., cluster traces (Wilkes,
2020; Cortez et al., 2017; Luo et al., 2021), video session data (Jiang et al., 2016; Manousis et al., 2021),
network flow datasets (Zeng, 2017)), data sharing can leak sensitive global properties of the data distribution .
Examples include:
S1. Business strategies can be leaked from data. As mentioned before, cluster trace datasets (Wilkes,
2020; Cortez et al., 2017; Luo et al., 2021) are very useful in the systems community. However, cluster
traces can reveal strategic enterprise choices, such as the fraction of server types in use (Lin et al., 2020).
Such information reflects the company’s business strategy and should be kept secret from competitors and
vendors. Note that simply removing the server type from the dataset is not a good option, as server type is an
1In this work, we consider Wasserstein-1 distance and total variation distance (§3), though our formulation can accommodate
other distance metrics.
2We assume data distributions are drawn from a parametric family; more details in §3.
3Under review as submission to TMLR
0200400
201030
0200400
2010300200400
2010300200400
201030Originaldatadistribution(Histogram)Releaseddatadistribution(Histogram)DifferentialprivacyAnonymizationSub-samplingDoesnotchangemeanofdataonexpectationSecretisthemeanofthedistribution
Figure 2: An illustrative example of why some of the privacy frameworks are not suitable for summary statis-
tic privacy. Assume that we want to protect the meanof the data. A typical differential privacy algorithm
(Wasserman & Zhou, 2010) would add zero-mean noise (e.g., Laplace noise) to the bins. Anonymization
(Wilkes, 2020) removes sensitive features (e.g., name of users) from data but leaves other features the same.
Sub-sampling (Reiss et al., 2012) down-sample the dataset. All of these mechanisms do not change the
expected mean of the data, and thus an attacker can still guess the mean with a small (expected) error. See
§2.2 for the discussion of other privacy mechanisms.
important feature for the downstream applications of the dataset (e.g., for predicting future CPU/memory
usage).
S2. Business scales can be leaked from data. For example, networking datasets that contain traffic
measurements or raw records are another common type of data (e.g., Meta flow trace dataset (Zeng, 2017),
Wikipedia Web Traffic Dataset (Google, 2018), video session data used in Manousis et al. (2021)). While
being useful, the total (or mean) traffic volume in these datasets (e.g., number of transferred bytes in a
network, number of page views of websites, viewership values of video delivery systems) can reveal the scale
of the business such as the number of users and the revenue of the company. Indeed, due to these concerns,
it is a common practice to hide the actual traffic volumes of sensitive proprietary datasets even in research
papers (e.g., removing the actual traffic values in Manousis et al. (2021)).
S3. System capabilities can also be revealed. For instance, the cluster trace datasets mentioned before
(Wilkes, 2020; Cortez et al., 2017; Luo et al., 2021) contain CPU and memory usage of servers. It is likely
that the maximum value of memory usage is close to the memory size of the system. Such system capabilities
could be used by adversaries to launch attacks (e.g., denial-of-service attacks). Due to these concerns, some
companies use customized techniques to obfuscate system capabilities before data release (e.g., normalizing
system usage (Wilkes, 2020)).
S4. Company sentiment or performance [Example 1 from Mahloujifar et al. (2022)] A company
releases a spam classifier trained on company emails. However, using property inference, an attacker is able
to infer the aggregate sentiment of those emails (positive/negative). If the fraction of negative emails is high,
it suggests that company morale is low, which is sensitive.
2.2 Existing Privacy Frameworks are Insufficient for Summary Statistic Privacy
Most existing privacy frameworks or mechanisms are not suitable for summary statistic privacy because
they either focus on protecting individual records in the data (e.g., differential privacy (Dwork et al., 2006),
anonymization (Wilkes, 2020), sub-sampling (Reiss et al., 2012)) (Fig. 1), or are designed for algorithms
4Under review as submission to TMLR
that release low-dimension statistical queries of the dataset instead of the entire dataset (e.g., attribute
privacy (Zhang et al., 2022), maximal leakage (Issa et al., 2019), privacy funnel (Makhdoumi et al., 2014)).
We divide the relevant work into three categories: approaches that are based on indistinguishability over
candidate distributions or inputs, industry heuristics, and information-theoretic approaches.
2.2.1 Indistinguishability Approaches
This class of approaches provides privacy by ensuring that pairs of input datasets or data distributions are
indistinguishable. These approaches are typically motivated by differential privacy (Dwork et al., 2006).
Differential privacy (DP) Dwork et al. (2006) is one of the most popular privacy notions. A random
mechanismMis(ϵ,δ)-differentially-private if for any neighboring datasets D0andD1(i.e.,D0andD1differ
one sample), and any set S⊆range (M), we have
P(M(D0)∈S)≤eϵ·P(M(D1)∈S) +δ .
In our data sharing scenarios, we could apply DP framework by treating Mas the data release mechanism
that reads the original dataset and outputs the released dataset. However, the privacy concerns of DP and
our suggested framework are completely different: we aim to hide functions of a distribution , while DP aims
to hide whether any given sample contributed to the shared data. For example, we say that we want to
release the data in Fig. 2 while protecting its mean. A typical differential privacy algorithm (Wasserman
& Zhou, 2010) would add zero-mean noise (e.g., Laplace noise) to the bins. This process does not change
the expected mean of the data, and therefore, the attack is still able to derive an unbiased estimator of the
mean from the released data. Indeed, we will show through experiments in §7 that this DP mechanism is
not effective in hiding statistical secrets.
There exist generalizations of DP for protecting more general random variables (besides individual samples)
(Chatzikokolakis et al., 2013). However, a strong DP guarantee such that any two datasets with different
secrets are indistinguishable from the released datasets implies that the released dataset has bad utility. For
example, suppose that the original distributions are Gaussian distributions N/parenleftbig
µ,σ2/parenrightbig
, and the secret is the
mean of the distribution µ. Two distributions with different secrets could have very different σ2. To make
any two distributions with different secrets (e.g., N(0,1)andN(1,100)) indistinguishable from the released
dataset, we must destroy information about the true σ. While relaxations like metric differential privacy
relaxation may help (Chatzikokolakis et al., 2013), this also introduces new challenges, e.g., how to choose
the metric function that maps dataset distance to a privacy parameter.
Attribute privacy (Zhang et al., 2022) considers a similar privacy concern as us: it tries to protect a func-
tion of a sensitive column in the dataset (named dataset attribute privacy ) or a sensitive parameter of the
underlying distribution from which the data is sampled (named distribution attribute privacy ). Attribute pri-
vacy addresses the previously-mentioned shortcomings of vanilla DP under the pufferfish privacy framework
(Kifer & Machanavajjhala, 2014). Roughly, an algorithm is said to satisfy dataset/distribution attribute
privacy if for any two different ranges of a secret function value (e.g., the fraction of the server type Ais in
[0.1,0.2)or[0.2,0.3)), the distributions of the algorithm output do not differ too much. Attribute privacy
constrains the set of candidate distributions a priori, which prevents the problem we discussed earlier, in
which vanilla DP requires the addition of unbounded noise (Zhang et al., 2021).
Although their privacy concerns are highly related to ours, attribute privacy focuses on algorithms that
outputa statistical query of the dataset instead of the entire dataset. We could apply their framework to
analyze full-dataset-sharing algorithms, but due to the high dimensionality of the dataset, attribute privacy
needs to add substantial noise, which harms utility (§7).
Distribution privacy (Kawamoto & Murakami, 2019) is a closely related notion, which releases a full data
distribution under DP-style indistinguishability guarantees. Roughly, for any two input distributions θ0and
θ1from a pre-defined set of candidate distributions, a distribution private mechanism outputs a distribution
M(θi)such that for any set Sin the output space, we have P[M(θi)∈S]≤eϵP[M(θ1−i)∈S] +δ.
This formulation is stronger than what we need; by obfuscating the whole distribution, we inherently pro-
tect the private information in question. However mechanisms that protect distribution privacy may add
5Under review as submission to TMLR
more noise than what is required only to protect select secret(s). A recent work by Chen and Ohrimenko
(Chen & Ohrimenko, 2022) proposes mechanisms for distribution privacy, and we observe exactly this trend
experimentally in §7; the noise added by the mechanisms in Chen & Ohrimenko (2022) is larger than what
we require with summary statistic privacy.
Distribution inference (Suri & Evans, 2021; Suri et al., 2023) is very closely related to our goals. Like our
setting, the data holder is trying to protect a secret function of its data (or data distribution). To this end,
it sets up a hypothesis test in which the adversary must choose whether the released model (or data) comes
from one of two fixed data distributions, which are derived from an underlying public data distribution.
These two distributions are assumed to be known both to the attacker and the defenders. In many practical
settings, it may be difficult to establish a reasonable pair of candidate distributions; moreover, this approach
is not directly aligned with the data holder’s goal, which is simply to hide some secret quantities — not to
render the full data distribution indistinguishable with another (the latter is closer to distribution privacy).
2.2.2 Industry Heuristics
Industry heuristics are algorithms that are commonly used in industrial data sharing settings. They may
not provide provable privacy guarantees, and indeed, many of these heuristics have been broken in practice.
Examples include anonymization , which removes certain attributes (e.g., name of the patients in medical
data, name of jobs in cluster dataset) (Reiss et al., 2012); anonymization is widely used in the release of
datasets (e.g., Wilkes (2020)). However, it does not change the distribution of attributes. Another example
issub-sampling , which works by sampling the original datasets at the level of individual records (Reiss
et al., 2012). The intuition is that by reducing the number of samples, less information is leaked. However,
sub-sampling does not change statistical properties of the distribution.
2.2.3 Information-Theoretic Approaches
The third category of defenses are information theoretic. These approaches have a similar goal to ours and
typically rely on (or relate to) the mutual information between problem variables.
Maximal leakage (Issa et al., 2019) is an information-theoretic framework for quantifying the leakage of
sensitive information. We denote Xas the random variable of the data to be shared (which may contain
sensitive information), and Yas the random variable of the information that is processed from Xand is
accessible to the attacker. Having observed Y, the attacker’s goal is to guess a secret function of Xdenoted
byU, and the guess is denoted by ˆU. Based on this setup, the Markov chain U−X−Y−ˆUholds. Maximal
leakageLfromXtoYis defined as
L(X→Y) = sup
U−X−Y−ˆUlogP/parenleftig
U=ˆU/parenrightig
maxuPU(u)(1)
where the supis taken over U(i.e., considering the worst-case secret) and ˆU(i.e., considering the strongest
attacker). Intuitively, Eq. (1) evaluates the ratio (in nats) of the probabilities of guessing the secret U
correctly with and without observing Y.
To apply maximal leakage in data sharing scenario, we may regard Xas the original dataset, Yas the
released dataset, and Uas the secret (e.g., the fraction of a specific server type). However, this formulation
is still unsuitable for the following reasons. (1) Maximal leakage only considers discrete UandˆUunder finite
alphabet. Note that it is a critical assumption for making sure that P/parenleftig
U=ˆU/parenrightig
in the definition (Eq. (1))
is nonzero. However, in our problem, secrets typically have continuous support (e.g., §2.1). (2) Maximal
leakage assumes that the secret to protect Uis unknown a priori and therefore considers the worst-case
leakage among all possible secrets. However, in our problem, data holders know what secret they want to
protect. Although we cannot directly use maximal leakage in our problem, its core idea can be useful for
extending our framework (see §9).
Privacy funnel (Makhdoumi et al., 2014) is another popular information-theoretic privacy framework.
As with maximal leakage, we denote Xas the random variable of the data that many contain sensitive
6Under review as submission to TMLR
information U, andYas the random variable of the information that is processed from Xand is accessible
by the attacker. The privacy funnel framework evaluates privacy leakage with the mutual information
I(U;Y), and the utility of Ywith mutual information I(X;Y). To find a good privacy-preserving data
processing strategy PY|X, the privacy funnel solves the optimization
min
PY|X:I(X;Y)≥RI(U;Y),
whereRis a desired threshold on the utility of Y.
To apply it in data sharing problems, we could regard Xas the original data, Yas the released data, and
Uas the secret data holder wants to protect (e.g., the fraction of a specific server type). However, mutual
information is not a good metric for either privacy or utility. On the privacy front, prior work has shown
thatI(U;Y)can be reduced while allowing the attacker to guess Scorrectly from Ywith higher probability
(see Example 1 in Issa et al. (2019)). On the utility front, higher mutual information I(X;Y)does not
mean that the released data Yis a useful representation of X. For example, Ycould be an arbitrary one-
to-one transformation of X. In that case, I(X;Y)is maximized, but the data structure could be completely
destroyed. In addition, privacy funnel (Makhdoumi et al., 2014) only considers XandYin discrete supports,
which is too restrictive for our setting.
3 Summary Statistic Privacy Formulation
Notation. We denote random variables with uppercase English letters or upright Greek letters (e.g., X,μ),
and their realizations with italicized lowercase letters (e.g., x,µ). For a random variable X, we denote its
probability density function (PDF), or, in the case of discrete random variables, its probability mass function
(PMF), asfX, and its distribution measure as ωX. If a random variable Xis drawn from a parametric family
(e.g., Gaussian with specified mean and covariance); the parameters will be denoted with a subscript of X,
i.e., the above notations become Xθ,fXθ,ωXθrespectively for parameters θ∈Rq, whereq≥1denotes
the dimension of the parameters. In addition, we denote fX|Yas the conditional PDF or PMF of Xgiven
another random variable Y. We use Z,Z>0,N,R,R>0,to denote the set of integers, positive integers, natural
numbers, real numbers, and positive real numbers respectively.
Original data. Consider a data holder who possesses a dataset of nsamplesX={x1,...,xn}, where for
eachi∈[n],xi∈Rpis drawn i.i.d. from an underlying distribution. We assume the distribution comes
from a parametric family, and the parameter vector θ∈Rqof the distribution fully specifies the distribution.
That is,xi∼ωXθ, where we further assume that θis itself a realization of random parameter vector Θ, and
ωΘis the probability measure for Θ. We will discuss how to relax the assumption on this prior distribution
ofθin §9. We assume that the data holder knows θ(and hence knows its full data distribution ωXθ); our
results and mechanisms generalize to the case when the data holder only possesses the dataset X(see §6).
For example, suppose the original data samples come from a Gaussian distribution. We have θ= (µ,σ), and
Xθ∼N (µ,σ).ωΘ(orfΘ) describes the prior distribution over (µ,σ). For example, if we know a priori that
the mean of the Gaussian is drawn from a uniform distribution between 0 and 1, and σis always 1, we could
havefΘ(µ,σ) =I(µ∈[0,1])·δ(σ), where I(·)is the indicator function, and δis the Dirac delta function.
In practice, the underlying distribution can be much more complicated than a Gaussian.
In general, the data can be multi-dimensional (i.e., p >1). We study one-dimensional data as a starting
point (§3.2).
Statistical secrets to protect. We assume the data holder wants to hide ℓ∈Z>0secretsfrom the
original data distribution. Since the true data distribution is fully-specificed by parameter vector θ, these
secrets can be expressed as a function g(θ) :Rq→Rℓ. In the Gaussian example Xθ∼N (µ,σ), suppose
the random variable Xθrepresents the traffic volume experienced by an enterprise in a day. The data holder
may wish to hide the mean traffic per day, in which case g(·)would be the mean of the distribution, i.e.,
g(µ,σ) =µ. In this example, we are hiding only one secret (the mean), so ℓ= 1.In general, the secret
can be any (vector-valued) function that can be deterministically computed from θ.As shown in Fig. 1, the
secret could be derived from one feature (e.g., the mean salary) or computed from multiple features (e.g., the
7Under review as submission to TMLR
mean salary of males). The secrets could also be multi-dimensional (e.g., mean of salary, and the fraction
of males). In this paper, we present general results for one-dimensional secrets (i.e., ℓ= 1) and defer a
discussion of higher-dimensional secrets to future work (see §9).
Data release mechanism. The data holder releases data by passing the private parameter θthrough a
data release mechanism Mg. That is, for a given θ, the data holder first draws internal randomness z∼ωZ,
and then releases another distribution parameter θ′=Mg(θ,z), whereMgis a deterministic function, and
ωZis a fixed distribution from which zis sampled. Note that we assume both the input and output of Mg
are distribution parameters. It is straightforward to generalize to the case when the input and/or output
are datasets of samples (see §6).
For example, in the Gaussian case discussed above, the data release mechanism can be Mg((µ,σ),z) =
(µ+z,σ)wherez∼N (0,1). I.e., this mechanism shifts the mean of the Gaussian by a random amount
drawn from a standard Gaussian distribution and keeps the variance.
Threat model. We assume that the attacker knows the parametric family from which our data is drawn,
but does not know the initial parameter θ. The attacker is also assumed to know the data release mechanism
Mgand output θ′but not the realization of the data holder’s internal randomness z. The attacker guesses
the initial secret g(θ)based on the released parameter θ′according to estimate ˆg(θ′).ˆgcan be either
random or deterministic, and we assume no computational bounds on the adversary. For instance, in the
runningGaussianexample, anattackermay choose ˆg(µ′,σ′) =µ′. When thedataholderreleasesa datasetof
samples instead of the parameter θ′, this formulation can be used to upper bound the attacker’s performance
on correctly guessing the secret, since the estimation error on released distribution parameter is induced due
to the finite samples in the released dataset.
3.1 Metrics
Privacy metric. The data holder wishes to prevent an attacker from guessing its secrets. We define our
privacy metric privacy Πϵ,ωΘas the attacker’s probability of guessing the secret(s) to within a tolerance ϵ,
taken worst-case over all attackers ˆg:
Πϵ,ωΘ≜sup
ˆgP(|ˆg(θ′)−g(θ)|≤ϵ). (2)
The probability is taken over the randomness of the original data distribution ( θ∼ωΘ), the data release
mechanism ( z∼ωZ), and the attacker strategy ( ˆg).
Distortion metric. The main goal of data sharing is to provide useful data; hence, we (and data
holders and users) want to understand how much the released data distorts the original data. We define the
distortion ∆of a mechanism as the worst-case distance between the original distribution and the released
distribution:
∆≜ sup
θ∈Supp (ωΘ),θ′,
z∈Supp (ωZ):Mg(θ,z)=θ′d/parenleftbig
ωXθ∥ωXθ′/parenrightbig
, (3)
wheredis a general distance metric defined over distributions. The choice of the distance metric depends
on the data type and potentially on the applications that stakeholders care about. For example, if the data
holders or users have concrete metrics that they want to preserve (e.g., the difference between the mean
salaries of males and females in Fig. 1), they could use this quantity as the distance metric. Otherwise,
one can use statistical distance metrics between distributions (e.g., total variation distance, Wasserstein
distance). In this paper, we adopt Wasserstein-1 distance for continuous distributions and total variation
(TV) distance for discrete distributions. These distances are often used for evaluating data quality (e.g.,
Yin et al. (2022); Lin et al. (2020)) and as the distance metric in neural network design (e.g., Arjovsky et al.
(2017); Lin et al. (2018)). Note that the definition in Eq. (3) can be extended to data release mechanisms
that take datasets as inputs and/or outputs.
8Under review as submission to TMLR
Objective. To summarize, the data holder’s objective is to choose a data release mechanism that minimizes
distortion metric ∆subject to a constraint on privacy Πϵ,ωΘ:
min
Mg∆
subject to Πϵ,ωΘ≤T.(4)
The alternative formulation, minMgΠϵ,ωΘsubject to ∆≤Tis analyzed in App. A.
The optimal data release mechanisms for Eq. (4) depends on the secrets, the distance metric din Eq. (3), and
the characteristics of the original data. We envision a summary statistic privacy toolbox (Fig. 1) that encodes
data release mechanisms for a list of predefined secrets, d, and data distributions. Data holders specify the
secret function they want to protect and the desired distance metric; the toolbox then selects the data
distribution parametric family that most closely reflects the holder’s raw data and uses the corresponding
data release mechanism to process the raw data for sharing.
3.2 Scope of This Work
3.2.1 Simplifying Assumptions
Although our formulation supports a wide range of distribution distance metrics, secret functions, and para-
metric families of data distributions, we make simplifying assumptions as a starting point on this problem.
Distortion metric. As discussed in §3.1, we use Wasserstein-1 and TV as the distance metrics for
continuous and discrete distributions respectively in the case studies (§6). We leave the discussion of other
metrics to §9.
The type and the number of secrets. Our formulation supports general statistical secrets, as long as
they are a (possibly vector-valued) function of the data distribution. In this paper, we start by assuming
that the secret is one-dimensional, and discuss several natural secret functions in §6.
The dimension and distribution of the data. Although our formulation includes multi-dimensional
data, in this paper, we consider one-dimensional distributions as a starting point.
3.2.2 Research Questions
We aim to answer two questions:
Q1 What are fundamental limits on the tradeoff between privacy and distortion?
Q2 Do there exist data release mechanisms that can match or approach these fundamental limits?
In general, these questions can have different answers for different choices of distance metric in Eq. (3),
different parametric families of data distributions, and different secret functions. In §4 and §5, we first
present general results that do not depend on data distribution or secret function. We then present case
studies for specific secrets and data distributions for building up our initial summary statistic privacy toolbox
in §6.
4 General Lower Bound on Privacy-Distortion Tradeoffs
Given a privacy budget T, we first present a lower bound on distortion that applies regardless of the prior
distribution of data ωΘandregardless of the secret g. As discussed in §3.2, we assume that the secret is
scalar (i.e., ℓ= 1), but the data distribution can have arbitrary dimension.
Theorem 1 (Lower bound of privacy-distortion tradeoff) .LetD(Xθ1,Xθ2)≜1
2d/parenleftbig
ωXθ1∥ωXθ2/parenrightbig
, whered(·∥·)
is defined in the line after Eq. (3). Further, let R(Xθ1,Xθ2)≜|g(θ1)−g(θ2)|and
γ≜ inf
θ1,θ2∈Supp (ωΘ)D(Xθ1,Xθ2)
R(Xθ1,Xθ2). (5)
9Under review as submission to TMLR
For anyT∈(0,1), when Πϵ,ωΘ≤T,
∆>/parenleftbigg
⌈1
T⌉−1/parenrightbigg
·2γϵ . (6)
The proof is shown as below. From Thm. 1 we see that the lower bound of distortion is inversely correlated
with the privacy budget and positively correlated with the guess tolerance ϵ. The dependent quantity γin
Eq. (5) can be thought of as a conversion factor that bounds the translation from probability of detection to
distributional distance. Note that we have not made γexact as its form depends on the type of the secret
and prior distribution of data. We will instantiate it in the cases studies in §6.
Proof.Our proof proceeds by constructing an ensemble of attackers, such that at least one of them will be
correct by construction. We do this by partitioning the space of possible secret values, and having each
attacker output the midpoint of one of the subsets of the partition. We then use the fact that each attacker
can be correct with probability at most T, combined with γ, which intuitively relates the distance between
distributions to the distance between their secrets, to derive the claim. Recall that θis the true private
parameter vector, θ′is the released parameter vector as a result of the data release mechanism.
T≥Πϵ,ωΘ
= sup
ˆgP(ˆg(θ′)∈[g(θ)−ϵ,g(θ) +ϵ])
= sup
ˆgE/parenleftbigg
P/parenleftbigg
ˆg(θ′)∈[g(θ)−ϵ,g(θ) +ϵ]/vextendsingle/vextendsingle/vextendsingle/vextendsingleθ′/parenrightbigg/parenrightbigg
=E/parenleftbigg
sup
ˆgP/parenleftbigg
ˆg(θ′)∈[g(θ)−ϵ,g(θ) +ϵ]/vextendsingle/vextendsingle/vextendsingle/vextendsingleθ′/parenrightbigg/parenrightbigg
, (7)
where Eq. (7) is due to the following facts: (1) LHS ≤RHS because
supˆgP/parenleftbigg
ˆg(θ′)∈[g(θ)−ϵ,g(θ) +ϵ]/vextendsingle/vextendsingle/vextendsingle/vextendsingleθ′/parenrightbigg
≥P/parenleftbigg
ˆg(θ′)∈[g(θ)−ϵ,g(θ) +ϵ]/vextendsingle/vextendsingle/vextendsingle/vextendsingleθ′/parenrightbigg
for anyθ′; (2) RHS≤
LHS because ˆgcan only depend on θ′. Therefore, we can map any arg supˆgin the RHS to the
LHS and obtain the same value, since the expectation is taken over θ′. Thus, there exists θ′s.t.
supˆgP/parenleftbigg
ˆg(θ′)∈[g(θ)−ϵ,g(θ) +ϵ]/vextendsingle/vextendsingle/vextendsingle/vextendsingleθ′/parenrightbigg
≤T. Let
Lθ′≜ inf
θ∈Supp (ωΘ),z:Mg(θ,z)=θ′g(θ),
Rθ′≜ sup
θ∈Supp (ωΘ),z:Mg(θ,z)=θ′g(θ).
We can define a sequence of attackers and a constant Nsuch that ˆgi(θ′) =Lθ′+ (i+ 0.5)·2ϵfori∈
{0,1,...,N−1}andLθ′+ 2Nϵ≥Rθ′>Lθ′+ 2(N−1)ϵ(Fig. 3). From the above, we have
T·N≥/summationdisplay
iP/parenleftbigg
ˆgi(θ′)∈[g(θ)−ϵ,g(θ) +ϵ]/vextendsingle/vextendsingle/vextendsingle/vextendsingleθ′/parenrightbigg
≥1,
Therefore, we have N≥⌈1
T⌉, and
Rθ′−Lθ′>/parenleftbigg
⌈1
T⌉−1/parenrightbigg
·2ϵ . (8)
10Under review as submission to TMLR
!!!"!!!!!+2%!!!+4%!!!+2)%+')+'"+'1."Possiblerangeof'(,)Rangeof'(,)that+')succeedsRangeof'(,)that+'"succeedsRangeof'(,)that+'1."succeeds
Figure 3: The construction of attackers for proof of Thm. 1. The 2ϵranges of ˆg0,...,ˆgN−1jointly cover the
entire range of possible secret [Lθ′,Rθ′]. The probability of guessing the secret correctly for any attacker is
≤T. Therefore, Rθ′−Lθ′>/parenleftbig
⌈1
T⌉−1/parenrightbig
·2ϵ(Eq. (8)).
Then we have
∆≥ sup
θ∈Supp (ωΘ),z∈Supp (ωZ):Mg(θ,z)=θ′d/parenleftbig
ωXθ∥ωXθ′/parenrightbig
≥ sup
θi∈Supp (ωΘ),zi:Mg(θi,zi)=θ′D(Xθ1,Xθ2) (9)
>/parenleftbigg
⌈1
T⌉−1/parenrightbigg
·2γϵ. (10)
where in Eq. (9), θifori∈{1,2}denotes two arbitrary parameter vectors in the support space, and Eq. (9)
comes from the triangle inequality, and Eq. (10) utilizes Rθ′−Lθ′>/parenleftbig
⌈1
T⌉−1/parenrightbig
·2ϵand the definition of
γ.
5 Data Release Mechanisms
We first present in §5.1 the quantization mechanism , a template for data release mechanisms used in the
case studies of §6. The quantization mechanism can be instantiated differently for different secret functions
and data distributions. We show in §5.2 techniques for instantiating the quantization mechanism, either
based on theoretical insights or numerically. Finally, we give some intuition in §5.3 about how to analyze the
quantization mechanism. These insights will be used in our case studies (§6) to show that we can sometimes
match the lower bounds from §4 up to small constant factors.
5.1 The Quantization Mechanism
At a high level, the quantization mechanisms follow two steps:
1.Offline Phase: Partition the space of parameters Supp (Θ)into carefully-chosen bins.
2.Online Phase: For an observed data distribution parameter θ, deterministically release the quan-
tized parameters, according to the partition from the Offline Phase.
More precisely, we first divide the set of possible distribution parameters Supp (Θ)into subsetsSisuch that
∪i∈ISi⊇Supp (Θ)andSi1∩Si2=∅fori1̸=i2, whereIis the (possibly uncountable) set of indices of the
subsets. For θ∈Supp (Θ),I(θ)is the index of the set that θbelongs to; in other words, we have I(θ) =i,
whereθ∈Si. The mechanism first looks up which set θbelongs to (i.e., I(θ)), thendeterministically releases
a parameter θ∗
I(θ)that corresponds to the set. Here, θ∗
ifori∈Idenotes another parameter. In short, our
data release mechanism has the form
Mg(θ,z) =θ∗
I(θ).
11Under review as submission to TMLR
Note that the policy is fully determined by Siandθ∗
i. In the remainder of the paper, we will show different
ways of instantiating quantization mechanism to approach the lower bound in §4.
Intuitively, quantization mechanisms will have a bounded distortion as long as d/parenleftbigg
ωXθ∥ωXθ∗
I(θ)/parenrightbigg
is bounded
for allθ∈Supp (Θ). At the same time, they obfuscate the secret as different data distributions within the
same set are mapped to the same released parameter. It turns out this simple deterministic mechanism is
sufficient to achieve the (order) optimal privacy-distortion trade-offs in many cases, as opposed to DP where
randomness is required to achieve DP guarantees (Dwork et al., 2006) (examples in the case studies §6).
5.2 Algorithms for Instantiating the Quantization Mechanism
To implement the quantization mechanism, we need to define the quantization bins Siand the released
parameter per bin θ∗
i. Depending on the data distribution, the secret function, and quantization mechanism
parameters, the mechanism can have very different privacy-distortion tradeoffs. We present two methods for
selecting quantization parameters: (1) an analytical approach, and (2) a numeric approach.
(1) Analytical approach. In some cases, outlined in the case studies of §6 and the appendices, we can
find analytical expressions for Siandθ∗
iwhile (near-)optimally trading off privacy for distortion. This is
usually possible when the lower bound depends on the problem parameters in a particular way.
For example, for the Gaussian distribution where θ= (µ,σ), when secret=standard deviation, we can work
out the lower bound from Thm. 1 (details in App. G). Note that the lower bound is tight if our mechanism
minimizes
D(Xµ1,σ1,Xµ2,σ2)
R(Xµ1,σ1,Xµ2,σ2)=/radicalbigg
1
2πe−1
2/parenleftbigµ1−µ2
σ1−σ2/parenrightbig2
−/parenleftbiggµ1−µ2
σ1−σ2/parenrightbigg/parenleftbigg1
2−Φ/parenleftbigg/parenleftbiggµ1−µ2
σ1−σ2/parenrightbigg/parenrightbigg/parenrightbigg
(11)
where where D(Xθ1,Xθ2)andR(Xθ1,Xθ2)are defined in Thm. 1, and Φdenotes the CDF of the standard
Gaussian distribution. That is, for any true parameters µ1andσ1, the mechanism should always choose to
releaseµ2andσ2such that Eq. (11) is as small as possible. The exact form of Eq. (11) is not important for
now; notice instead that the problem parameters (σi,µi)take the same form every time they appear in this
equation. We define t(θ1,θ2) =µ1−µ2
σ1−σ2to be that form.3Next, we find the t(θ1,θ2)that minimizes Eq. (11):
t0≜arg inf
t(θ1,θ2)D(Xθ1,Xθ2)
R(Xθ1,Xθ2)
For instance, in our Gaussian example, we can write t0as
t0= arg inf
t(θ1,θ2)/radicalbigg
1
2πe−1
2(t(θ1,θ2))2−(t(θ1,θ2))/parenleftbigg1
2−Φ (t(θ1,θ2))/parenrightbigg
,
which can be solved numerically. Finally, we can choose Siandθ∗
ito be sets for which t(θ,θ∗
i) =t0,∀θ∈Si.
Using this rule, we derive the mechanism:
Sµ,i=/braceleftig
(µ+t0·t,σ+ (i+ 0.5)·s+t)|t∈/bracketleftig
−s
2,s
2/parenrightig/bracerightig
,
θ∗
µ,i= (µ,σ+ (i+ 0.5)·s),
I={(µ,i)|i∈N,µ∈R},
wheresis a hyper-parameter of the mechanism that divides (σ−σ), andσ,σare upper and lower bounds
ofσ.
For our Gaussian example, the resulting sets Sµ,ifor the quantization mechanism are shown in Fig. 4; the
space of possible parameters is divided into infinitely many subsets Sµ,i, each consisting of a diagonal line
3Indeed, for many of the case studies in §6, t(θ)takes an analogous form; we will see the implications of this in the analysis
of the upper bound in §5.3.
12Under review as submission to TMLR
segment (parallel blue lines in Fig. 4). The space of possible σvalues is divided into segments of length s,
which correspond to the horizontal bands in Fig. 4. The fact that the intervals Sµ,iare diagonal lines arises
from choosing t(θ1,θ2) =µ1−µ2
σ1−σ2; each interval corresponds to a set of points that satisfy t(θ1,θ2) =t0, i.e.,
with slope 1/t0.
We will see how to use this construction to obtain upper bounds on privacy-distortion tradeoffs in §5.3.
(2) Numeric approach. In some cases, the above procedure may not be possible. To this end, we present
a dynamic programming algorithm to numerically compute the quantization mechanism parameters. This
algorithm achieves an optimal privacy-distortion tradeoff (Bellman, 1966) among the class of quantization
algorithms with finite precision and continuous intervals Si. We use this algorithm in some of the case studies
in §6. We present our dynamic programming algorithm for univariate data distributions.
We assume Supp (Θ) =/bracketleftbig
θ,θ/parenrightbig
, whereθ,θare lower and upper bounds of θ, respectively. We consider the
class of quantization mechanisms such that Si=/bracketleftig
θi,θi/parenrightig
, i.e., each subset of parameters are in a continuous
range. Furthermore, we explore mechanisms such that θi,θi,θ∗
i∈/braceleftbig
θ,θ+κ,θ+ 2κ,...,θ/bracerightbig
, whereκis a
hyper-parameter that encodes numeric precision (and therefore divides (θ−θ)). For example, if we want to
hide the mean of a Geometric random variable with θ= 0.1andθ= 0.9, we could consider three-decimal-
place precision, i.e., κ= 0.001andθi,θi,θ∗
i∈{0.100,0.101,0.102,..., 0.900}.
Since ∆(Eq. (3)) is defined as the worst-case distortion whereas Πϵ,ωΘ(Eq. (2)) is defined as a probability ,
which is related to the original data distribution, optimizing Πϵ,ωΘgiven bounded ∆(Eq. (12)) is easier to
solve than the final goal of optimizing ∆given bounded Πϵ,ωΘ(Eq. (4)).
min
MgΠϵ,ωΘsubject to ∆≤T.(12)
Observing that in Eq. (4) the optimal value of minMg∆is a monotonic decreasing function w.r.t. the
thresholdT, we can use a binary search algorithm (shown in App. B) to reduce problem Eq. (4) to problem
Eq. (12). It calls an algorithm that finds the optimal quantization mechanism with numerical precision over
continuous intervals under a distortion budget T(i.e., solving Eq. (12)). This problem can be solved by a
dynamic programming algorithm. Let pri(t∗)(t∗∈/braceleftbig
θ,θ+κ,θ+ 2κ,...,θ/bracerightbig
) be the minimal privacy Πϵ,ωΘ
we can get for Supp (Θ) ={Xθ:θ∈[θ,t∗)}such that ∆≤T. DenoteD(θ1,θ2)as the minimal distortion a
quantization mechanism can achieve under the quantization bin [θ1,θ2), we have
D(θ1,θ2) = inf
θ∈Rqsup
θ′′∈[θ1,θ2)d/parenleftbig
ωXθ′′∥ωXθ/parenrightbig
,
whered(·∥·)is defined in Eq. (3). We also denote D∗(θ1,θ2) = arg inf θ∈[θ1,θ2)supθ′′∈[θ1,θ2)d/parenleftbig
ωXθ′′∥ωXθ/parenrightbig
. If
the prior over parameters is fΘ, we have the Bellman equation
pri(t∗) = min
θ∈[θ,t∗−κ],D(θ,t∗)≤T/integraltextθ
θfΘ(t) dt
/integraltextt∗
θfΘ(t) dt·pri(θ) +/integraltextt∗
θfΘ(t) dt
/integraltextt∗
θfΘ(t) dt·P(θ,t∗)
with the initial state pri(θ) = 0, where
P(θ,t∗) =P(ˆg∗(θ′)∈[g(θ0)−ϵ,g(θ0) +ϵ]|θ0∈[θ,t∗],θ′)
= sup
t1,t2: supt′,t′′∈[t1,t2]|g(t′′)−g(t′)|=2ϵ/integraltextmin{t2,t∗}
max{t1,θ}fΘ(t) dt
/integraltextt∗
θfΘ(t) dt.
θ′is the released parameter when the private parameter θ0∈[θ,t∗]andˆg∗is the optimal attack strategy.
The full algorithm is listed in Alg. 1. The time complexity of this algorithm is O/parenleftig/parenleftbig
θ−θ/κ/parenrightbig2·CD·CP·CI/parenrightig
,
whereCDis the time complexity for computing DandD∗,CPis the time complexity for computing P, and
CIis the time complexity for computing the integrals in the Bellman equation. In our cases studies, Dand
D∗can be computed in CD=O/parenleftbig
θ−θ/κ/parenrightbig
, andPand the integrals can be computed in closed forms within
constant time, i.e., CP=CI=O(1).
13Under review as submission to TMLR
Algorithm 1: Dynamic-programming-based data release mechanism for single-parameter distributions.
Input:Parameter range:/bracketleftbig
θ,θ/parenrightbig
Prior over parameter: fΘ
Distortion budget: T
Step size:κ(which divides θ−θ)
1pri(θ)←0
2I(θ)←∅
3fort∗←θ+κ,θ+ 2κ,...,θdo
4pri(t∗)←∞
5min_t←NULL
6forθ←t∗−κ,...,θdo
7ifD(θ,t∗)>Tthen
8 break
9p←/integraltextθ
θfΘ(t)dt
/integraltextt∗
θfΘ(t)dt·pri(θ) +/integraltextt∗
θfΘ(t)dt/integraltextt∗
θfΘ(t)dt·P(θ,t∗)
10 ifp<pri (t∗)then
11 pri(t∗)←p
12 min_t←θ
13ifmin_tis not NULL then
14St∗←[min_t, t∗)
15θ′
t∗←D∗(min_t,t∗)
16I(t∗)←I(min_t)∪{t∗}
17ifpri(θ) =∞then
18ERROR: No answer
19returnpri(θ),/braceleftbig
Si:i∈I/parenleftbig
θ/parenrightbig/bracerightbig
,/braceleftbig
θ′
i:i∈I/parenleftbig
θ/parenrightbig/bracerightbig
14Under review as submission to TMLR
0110
10…222|4)|2|4)|2…Thespaceofpossibleparameters………
Figure 4: We separate the space of possible parameters into two regions (yellow and green) and bound the
attacker’s success rate on each region separately. The blue lines represent examples of Sµ,i.
When dynamic programming is not practical (e.g., in high-dimensional problems), we also provide a greedy
algorithm in App. B as a baseline and show the empirical comparison between these two algorithms in the
case studies (Apps. E, G and H).
5.3 Technique for Analyzing the Quantization Mechanism
We next provide an overview of techniques for analyzing the quantization mechanism, both for privacy
and for distortion. We use these techniques for the analysis in our case studies, where we will make the
expressions and claims more precise. For concreteness, we will recall the Gaussian example from §5.2, for
which we have already derived a mechanism.
The mechanism presented in §5.2 can geometrically be interpreted as follows. Over the square of possible
parameter values µandσ(Fig. 4), the mechanism selects intervals Sµ,ithat consist of short diagonal line
segments (e.g., blue line segments in Fig. 4). When the true distribution parameters fall in one of these
intervals, the mechanism releases the midpoint of the interval.
We find that many of our case studies naturally give rise to the same form of t(θ). As a result, all of the
case studies we analyze theoretically (with multiple parameters) have mechanisms that instantiate intervals
Sµ,ias diagonal lines, as shown in Fig. 4. The sawtooth technique, which we present next, can be used to
analyzetheprivacyofallsuchmechanisminstantiations. Moreprecisely,thefollowingpatternofquantization
mechanism admits diagonal line intervals, and can be analyzed with the sawtooth technique (§6 and Apps. E
and G):
Sµ,i=/braceleftig
(µ+t0·t,σ+ (i+ 0.5)·s+t)|t∈/bracketleftig
−s
2,s
2/parenrightig/bracerightig
,
θ∗
µ,i= (µ,σ+ (i+ 0.5)·s),
I={(µ,i)|i∈N,µ∈R},
wheresis a hyper-parameter of the mechanism that denotes quantization bin size and divides (σ−σ)and
t0is a constant that can be determined by the mechanism design strategy described in §5.2.
(1) Privacy analysis. For ease of illustration, we assume that the support of parameters is Supp (Θ) =/braceleftbig
(a,b)|a∈/bracketleftbig
µ,µ/parenrightbig
,b∈[σ,σ)/bracerightbig
,but the analysis can be generalized to any case.
In Fig. 4, we separate the space of possible data parameters into two regions represented by yellow and
green colors. The yellow regions Syellowconstitute right triangles with height sand width|t0|s. The green
15Under review as submission to TMLR
regionSgreenis the rest of the parameter space. The high-level idea of our proof is as follows. Note that for
any parameter θ∈Sgreen, there exists a quantization bin Sµ,is.t.θ∈Sµ,iandSµ,i⊂Sgreen. This occurs
because the mechanism intervals (blue lines in Fig. 4) all have the same slope and a length of at most sfor
σ. As such, each interval is either fully in the green region, or fully in the yellow region. Since we know
the length of each bin, we can upper bound the attack success rate if θ∈Sgreen. While the attacker can
be more successful in the yellow region, the probability of θ∈Syellowis small. Hence, we upper bound the
overall attacker’s success rate (i.e., Πϵ,ωΘ). More specifically, let the optimal attacker be ˆg∗. We have
Πϵ,ωΘ=P/parenleftbig
ˆg∗/parenleftbig
θ′/parenrightbig
∈[g(θ)−ϵ,g(θ) +ϵ]/parenrightbig
=/integraldisplay
θ∈Sgreenp(θ)P/parenleftbig
ˆg∗/parenleftbig
θ′/parenrightbig
∈[g(θ)−ϵ,g(θ) +ϵ]/parenrightbig
dθ
+/integraldisplay
θ∈Syellowp(θ)P/parenleftbig
ˆg∗/parenleftbig
θ′/parenrightbig
∈[g(θ)−ϵ,g(θ) +ϵ]/parenrightbig
dθ
<sup
θ∈SgreenP/parenleftbig
ˆg∗/parenleftbig
θ′/parenrightbig
∈[g(θ)−ϵ,g(θ) +ϵ]/parenrightbig
+/integraldisplay
θ∈Syellowp(θ)dθ
The first term can be bounded away from 1 due to the carefully chosen t0. The second term is bounded
away from 1 because the size of Syellowis relatively small. The formal justification is given in Prop. 2
and Apps. C.4.2, F.2 and G.4.
(2) Distortion analysis. For the distortion performance, it is straightforward to show that
∆ = supθ∈Supp (Θ)d/parenleftbigg
ωXθ∥ωXθ∗
I(θ)/parenrightbigg
, whereθ∗
I(θ)is the released parameter when the original parameter is θ.
This quantity can often be derived directly from the mechanism and parameter support.
6 Case Studies
In this section, we instantiate the general results on concrete distributions and secrets (mean §6.1, quantile
§6.2, and we defer standard deviation and discrete distribution fractions to Apps. G and H). See Table 1
for a summary of each setting we consider, and a pointer to any theoretical results. Our results in each
setting generally include a privacy lower bound, a concrete instantiation of the quantization mechanism, and
privacy-distortion analysis of the data release mechanisms. In §6.3, we will discuss how to extend the data
release mechanisms to the cases when data holders only have data samples and do not know the parameters
of the underlying distributions.
These data release mechanisms serve as the initial version of summary statistic privacy toolbox (Fig. 1).
Table 1: Summary of the case studies.
SecretDistributionContinuous Distribution
(order-optimal mechanism)Ordinal Distribution
(Alg. 1 and Alg. 3)Categorical Distribution
(order-optimal mechanism)Gaussian Uniform Exponential Geometric Binomial Poisson
Mean §6.1 App. E Not applicable
Quantile §6.2 and App. F Not applicable Not applicable
Standard Deviation App. G.1 App. G.2 Not applicable
Fraction Not applicable App. H.1 App. H.2
6.1 Secret = Mean
In this section, we discuss how to protect the mean of a distribution for general continuous distributions.
We start with a lower bound.
Corollary1 (Privacylowerbound, secret=meanofacontinuousdistribution) .Consider the secret function
g(θ) =/integraltext
xxfXθ(x)dx. For anyT∈(0,1), when Πϵ,ωΘ≤T, we have ∆>/parenleftbig
⌈1
T⌉−1/parenrightbig
·ϵ.
16Under review as submission to TMLR
𝑓!!",$"𝑓"!,$𝑠𝑠𝑢+𝑠𝑢$𝑢=𝑢+2𝑠𝑢+0.5𝑠𝑢+1.5𝑠
Figure 5: Illustration of the data release mechanism for continuous distributions when secret=mean.
The proof is in App. C.1. We next design a data release mechanism that achieves a tradeoff close to this
bound.
Data release mechanism. We first consider continuous distributions that can be parameterized with a
location parameter, where the prior distribution of the location parameter is uniform and independent of
other factors. We relax this assumption to Lipschitz-continuous priors in App. D.1. For now, we assume the
following:
Assumption 1.The distribution parameter vector θcan be written as (u,v), whereu∈R,v∈Rq−1,
and for any u̸=u′,fXu,v(x) =fXu′,v(x−u′+u). The prior over distribution parameters is fU,V(a,b) =
fU(a)·fV(b), wherefU(a) =1
u−uI(a∈[u,u)).
Examples include the Gaussian, Laplace, and uniform distributions, as well as shifted distributions (e.g.,
shifted exponential, shifted log-logistic). Using the strategy from §5.2, we derive the following quantization
mechanism.
Mechanism 1(For secret = mean of a continuous distribution) .The parameters of the data release mech-
anism are
Si,v={(t,v)|t∈[u+i·s, u+ (i+ 1)·s)}, (13)
θ∗
i,v= (u+ (i+ 0.5)·s,v), (14)
I={(i,v) :i∈{0,1,...,N−1},v∈Supp (ωV)}, (15)
wheresis a hyper-parameter of the mechanism that divides (u−u)andN=u−u
s∈N.
Fig. 5 shows an example when the original data distribution is Gaussian, i.e., Xθ∼N (u,v), andu∈/bracketleftbig
µ,µ/parenrightbig
.
Intuitively, our data release mechanism “quantizes” the range of possible mean values into segments of length
s. It then shifts the mean of private distribution fXu,vto the midpoint of its corresponding segment, and
releases the resulting distribution. This simple deterministic mechanism is able to achieve order-optimal
privacy-distortion tradeoff in some cases, as shown below.
Proposition 1. Under Asm. 1, Mech. 1 has Πϵ,ωΘ≤2ϵ
sand∆ =s
2<2∆opt, where ∆optis the minimal
distortion an optimal data release mechanism can achieve given the privacy Mech. 1 achieves.
17Under review as submission to TMLR
The proof is in App. C.2. The two takeaways from this proposition are that: (1) the data holder can
usesto control the trade-off between distortion and privacy, and (2) the mechanism is order-optimal with
multiplicative factor 2.
6.2 Secret = Quantiles
S3 in §2.1 explains how quantiles of continuous distributions can reveal sensitive information. In this section,
we show how to protect it for a typical continuous distribution: the (shifted) exponential distribution. We
analyze the Gaussian and uniform distributions in App. F. We choose these distributions as a starting point
of our analysis as many distributions in real-world data can be approximated by one of these distributions.
In our analysis, the parameters of (shifted) exponential distributions are denoted by:
•Exponential distribution: θ=λ, whereλis the scale parameter. In other words, fXλ(x) =1
λe−x/λ.
•Shifted exponential distribution generalizes the exponential distribution with an additional shift param-
eterh:θ= (λ,h). In other words, fXλ,h(x) =1
λe−(x−h)/λ.
As before, we first present a lower bound.
Corollary 2 (Privacy lower bound, secret = α-quantile of a continuous distribution) .Consider the secret
functiong(θ) =α-quantile of fXθ. For anyT∈(0,1), when Πϵ,ωΘ≤T, we have ∆>/parenleftbig
⌈1
T⌉−1/parenrightbig
·2γϵ, where
γis defined as follows:
•Exponential:
γ=−1
2 ln (1−α).
•Shifted exponential:
γ=

1
2/vextendsingle/vextendsingle/vextendsingle/vextendsingle1 +ln(1−α)+1
W−1/parenleftbig
−ln(1−α)+1
2(1−α)e/parenrightbig/vextendsingle/vextendsingle/vextendsingle/vextendsingleα∈[0,1−e−1)
1
2/vextendsingle/vextendsingle/vextendsingle/vextendsingle1 +ln(1−α)+1
W0/parenleftbig
−ln(1−α)+1
2(1−α)e/parenrightbig/vextendsingle/vextendsingle/vextendsingle/vextendsingleα∈[1−e−1,1),
whereW−1andW0are Lambert Wfunctions.
The proof is in App. C.3. Next, we provide data release mechanisms for each of the distributions that achieve
trade-offs close to these bounds.
Mechanism 2(For secret = quantile of a continuous distribution) .We design mechanisms for each of the
distributions. In both cases, s>0is the quantization bin size chosen by the operator to divide/parenleftbig
λ−λ/parenrightbig
, where
λandλare upper and lower bounds of λ.
•Exponential:
Si= [λ+i·s,λ+ (i+ 1)·s),
θ∗
i=λ+ (i+ 0.5)·s ,
I=N.
•Shifted exponential:
Si,h=/braceleftig
(λ+ (i+ 0.5)s+t,h−t0·t)|t∈/bracketleftig
−s
2,s
2/parenrightig/bracerightig
,
θ∗
i,h= (λ+ (i+ 0.5)s,h),
I={(i,h)|i∈N,h∈R},
where
t0=

−1−ln (1−α)−W−1/parenleftig
−ln(1−α)+1
2(1−α)e/parenrightig/parenleftbig
α∈[0,1−e−1)/parenrightbig
−1−ln (1−α)−W0/parenleftig
−ln(1−α)+1
2(1−α)e/parenrightig/parenleftbig
α∈[1−e−1,1)/parenrightbig.
18Under review as submission to TMLR
For the privacy-distortion trade-off analysis of Mech. 2, we assume that the parameters of the original data
are drawn from a uniform distribution with lower and upper bounds. Again, we relax this assumption to
Lipschitz priors in App. D.2. Precisely,
Assumption 2.The prior over distribution parameters is:
•Exponential: λfollows the uniform distribution over/bracketleftbig
λ,λ/parenrightbig
.
•Shifted exponential: (λ,h)follows the uniform distribution over/braceleftbig
(a,b)|a∈/bracketleftbig
λ,λ/parenrightbig
,b∈/bracketleftbig
h,h/parenrightbig/bracerightbig
.
We relax Asm. 2 and analyze the privacy-distortion trade-off of Mech. 2 in App. D.2.
Proposition 2. Under Asm. 2, Mech. 2 has the following Πϵ,ωΘand∆value/bound.
•Exponential:
Πϵ,ωΘ=2ϵ
−ln (1−α)s, ∆ =1
2s<2∆opt.
•Shifted exponential:
Πϵ,ωΘ<2ϵ
|ln (1−α) +t0|s+|t0|s
h−h,
∆ =s
2(t0−1) +se−t0</parenleftigg
2 +|t0|·|ln (1−α) +t0|s2
ϵ/parenleftbig
h−h/parenrightbig/parenrightigg
∆opt.
Under the high-precision regime wheres2
h−h→0ass,(h−h)→∞, whenα∈[0.01,0.25]∪[0.75,0.99],∆
satisfies
lim sup
s2
h−h→0∆<3∆opt.
∆optis the optimal achievable distortion given the privacy achieved by Mech. 2, and t0is a constant defined
in Mech. 2.
The proof is in App. C.4. Note that the quantization bin size scannot be too small, or the attacker can
always successfully guess the secret within a tolerance ϵ(i.e., Πϵ,ωΘ= 1). Therefore, for the “high-precision”
regime, we consider the asymptotic scaling as both sandh−hgrow.
Prop. 2 shows that the quantization mechanism is order-optimal with multiplicative factor 2for the ex-
ponential distribution. For shifted exponential distribution, order-optimality holds asymptotically in the
high-precision regime.
6.3 Extending Data Release Mechanisms for Dataset Input/Output
The data release mechanisms discussed in previous sections assume that data holders know the distribution
parameter of the original data. In practice, data holders often only have a dataset of samples from the data
distribution and do not know the parameters of the underlying distributions. As mentioned in §3, our data
release mechanisms can be easily adapted to handle dataset input/output.
The high-level idea is that the data holders can estimate the distribution parameters θfrom the data samples
and find the corresponding quantization bins Siaccording to the estimated parameters, and then modify
the original samples as if they are sampled according to the released parameter θ∗
i. For brevity, we only
present the concrete procedure for secret=mean on continuous distributions as an example. For a dataset of
X={x1,...,xn}, the procedure is:
1. Estimate the mean from the data samples: ˆµ=1
n/summationtext
i∈[n]xi.
2. According to Eq. (13), compute the index of the corresponding set i=⌊ˆµ−µ
s⌋.
19Under review as submission to TMLR
3. According to Eq. (14), change the mean of the data samples to µtarget =µ+ (i+ 0.5)·s. This can
be done by sample-wise operation x′
i=xi−ˆµ+µtarget.
4. The released dataset is Mg(X,z) ={x′
1,...,x′
n}.
Note that this mechanism applies to samples. Therefore, it can be applied either to the original data, or as
an add-on to existing data sharing tools (Esteban et al., 2017; Lin et al., 2020; Yin et al., 2022; Jordon et al.,
2018; Yoon et al., 2019). For example, it can be used to modify synthetically-generated samples after they
are generated, or to modify the training dataset for a generative model, or to directly modify the original
data for releasing.
7 Experiments
In the previous sections, we theoretically demonstrated the privacy-distortion tradeoffs of our data release
mechanisms in some special case studies. In this section, we focus on orthogonal questions through real-
world experiments: (1) how well our data release mechanisms perform when the assumptions do not hold in
practice, and (2) why existing privacy frameworks are not suitable for summary statistic privacy (which we
explained qualitatively in §2.2).
Datasets. We use three real-world datasets to simulate each of the motivating scenarios in §2.1.
1. Wikipedia Web Traffic Dataset (WWT) (Google, 2018) contains the daily page views of 145,063
Wikipedia web pages in 2015-2016. To preprocess it for our experiments, we remove the web pages
with empty page view record on any day (117,277 left), and compute the mean page views across all
dates for each web page. Our goal is to release the page views (i.e., a 117,277-dimensional vector)
while protecting the mean of the distribution (which reveals the business scales of the company
§2.1).
2. Google Cluster Trace Dataset (GCT) (Reiss et al., 2011) contains usage logs (e.g., CPU/memory) of
an internal Google cluster with 12.5k machines in 2011. We use “platform ID” field of the dataset,
which represents “microarchitecture and chipset version of the machine” (Reiss et al., 2011). Our
goal is to release another distribution of platform ID while protecting the fraction of one specific
platform ID (which reveals business strategy §2.1).
3. Measuring Broadband America Dataset (MBA) (Commission, 2018) contains network statistics (in-
cluding network traffic counters) collected by United States Federal Communications Commission
from homes across United States. We select the average network traffic (GB/measurement) from
AT&T clients as our data. Our goal is to release a copy of this data while hiding the 0.95-quantile
(which reveals the network capability §2.1).
Baselines. We compare our mechanisms discussed in §6 with three popular mechanisms proposed in
prior work (§2.2): differentially-private density estimation (Wasserman & Zhou, 2010) (shortened to DP),
attribute-private Gaussian mechanism (Zhang et al., 2022) (shortened to AP), and Wasserstein mechanism
for distribution privacy (Chen & Ohrimenko, 2022) (shortened to DistP). Note that these mechanisms are
not designed for our problem setting—in that sense, these experiments are not a fair comparison. Nontheless,
we include them simply to illustrate that prior techniques are not sufficient for our problem setting .
For a dataset of samples X={x1,...,xn}, DP works by: (1) Dividing the space into mbins:B1,...,Bm.4
(2) Computing the histogram Ci=/summationtextn
j=1I(xj∈Bi). (3) Adding noise to the histograms Di=
max/braceleftbig
0,Ci+Laplace/parenleftbig
0,ϵ2/parenrightbig/bracerightbig
, where Laplace/parenleftbig
0,ϵ2/parenrightbig
means a random noise from Laplace distribution with
mean 0 and variance ϵ2. (4) Normalizing the histogram pi=Di/summationtextm
j=1Dj. We can then draw yiaccording
to the histogram and release Y={y1,...,yn}with differential privacy guarantees. AP works by releasing
4In Google Cluster Trace Dataset, the bin is already pre-specified (i.e., the platform IDs), so this step is skipped.
20Under review as submission to TMLR
Y=/braceleftbig
xi+N/parenleftbig
0,ϵ2/parenrightbig/bracerightbign
i=1.5DistP works by releasing Y=/braceleftbig
xi+Laplace/parenleftbig
0,ϵ2/parenrightbig/bracerightbign
i=1.6Note that for each
of these mechanisms, normally their noise parameters would be set carefully to match the desired privacy
guarantees (e.g., differential privacy). In our case, since our privacy metric is different, it is unclear how to
set the noise parameters for a fair privacy comparison. For this reason, we evaluate different settings of the
noise parameters, and measure the empirical tradeoffs.
Metrics. Our privacy and distortion metrics depend on the prior distribution of the original data θ∼ωΘ
(though the mechanism does not). In practice (and also in these experiments), the data holder only has one
dataset. Therefore, we cannot empirically evaluate the proposed privacy and distortion metrics, and resort
to surrogate metrics to bound our true privacy and distortion.
Surrogate privacy metric. For an original dataset X={x1,...,xn}and the released dataset Y={y1,...,yn},
we define the surrogate privacy metric ˜Πϵas the error of an attacker who guesses the secret of the released
dataset as the true secret: ˜Πϵ,ωΘ≜−|g(X)−g(Y)|, whereg(D) =mean ofD, fraction of a specific platform
ID inD, and 0.95-quantile ofDin WWT, GCT, and MBA datasets respectively. Note that in the definition
of˜Πϵ,ωΘ, a minus sign is added so that a smaller value indicates stronger privacy, as in privacy metric Eq. (2).
This simple attacker strategy is in fact a good proxy for evaluating the privacy Πϵ,ωΘdue to the following
facts. (1) For our data release mechanisms for these secrets Mechs. 1, 2 and 5, when the prior distribution
is uniform, this strategy is actually optimal, so there is a direct mapping between ˜ΠϵandΠϵ,ωΘ. (2) For
AP applied on protecting mean of the data (i.e., Wikipedia Web Traffic Dataset experiments), this strategy
gives an unbiased estimator of the secret. (3) For DP and AP on other cases, this mechanism may not be
an unbiased estimator of the secret, but it gives an upper bound on the attacker’s error.
Surrogate distortion metric. We define our surrogate distortion metric as the distance between the two
datasets: ˜∆≜d(pX∥pY)wherepDdenotes the empirical distribution of a dataset D, anddis defined as
in our formulation §3 (i.e., Wassersstein-1 distance for continuous distributions in WWT and MBA, and
TV distance for discrete distributions in GCT). This metric evaluates how much the mechanism distorts the
dataset.
In fact, we can deduce a theoretical lower bound for the surrogate privacy and distortion metrics for secret
= mean/fractions (shown later in Fig. 6) using similar techniques as the proofs in the main paper (see
App. C.5).
AP
DP
DistP
Ours
Lower bound
0 500 1000
Surrogate metric for distortion−800−600−400−2000Surrogate metric for privacy
(a) Wikipedia Web Traffic Dataset.
(secret=mean)
AP
DP
DistP
Ours
Lower bound
0.00 0.01 0.02 0.03
Surrogate metric for distortion−0.03−0.02−0.010.00Surrogate metric for privacy(b) Google Cluster Trace Dataset.
(secret=categorical fraction)
0.0 0.2 0.4 0.6 0.8
Surrogate metric for distortion−2.0−1.5−1.0−0.50.0Surrogate metric for privacyAP
DP
DistP
Ours(c) Measuring Broadband America
Dataset.
(secret=quantile)
Figure 6: Privacy (lower is better) and distortion (lower is better) of AP, DP, DistP, and ours. Each
point represents one instance of data release mechanism with one hyper-parameter. “Lower bound” is the
theoretical lower bound of the achievable region. Our data release mechanisms achieve better privacy-
distortion tradeoff than AP, DP, and DistP.
5In Google Cluster Trace Dataset, the Gaussian noise N/parenleftbig
0,ϵ2/parenrightbig
are added to the counts of different platform IDs. We then
normalize the counts and sample released platform IDs from this categorical distribution.
6In Google Cluster Trace Dataset, the Laplace noise Laplace/parenleftbig
0,ϵ2/parenrightbig
are added to the counts of different platform IDs. We
then normalize the counts and sample released platform IDs from this categorical distribution.
21Under review as submission to TMLR
7.1 Results
We enumerate the hyper-parameters of each method (bin size and ϵfor DP,ϵfor AP and DistP, and s
for ours). For each method and each hyper-parameter, we compute their surrogate privacy and distortion
metrics. The results are shown in Fig. 6 (bottom left is best); each data point represents one realization of
mechanismMgunder a distinct hyperparameter setting. Two takeaways are below.
(1) Our data release mechanisms has good privacy-distortion trade-offs even when the assumptions do not
hold.We envision that data holders can choose the data release mechanisms in the toolbox (Fig. 1) that
matches their need. However, in practical scenarios, the data distributions supported in the toolbox may not
always match real data exactly. Our data release mechanisms for mean (i.e., Mech. 1 used in WWT) and
fractions (i.e., Mech. 5 used in GCT) support general continuous distributions and categorical distributions,
and therefore, there is no such a distribution gap. Indeed, even for these surrogate metrics, our Mech. 1 and
Mech. 5 are also optimal (see App. C.5). This is visualized in Figs. 6a and 6b where we can see that our data
releasemechanismsmatchthetheoreticallowerboundofthetrade-off. However, ourdatareleasemechanisms
for quantiles (i.e., Mech. 2 used in Fig. 6c) are order-optimal only when the distributions are within certain
classes (§6.2). Observing that network traffic in MBA follows a one-side fat-tailed distribution (not shown),
we apply the data release mechanism for exponential distribution (Mech. 2) for this dataset. Despite the
distribution mismatch, our data release mechanism still achieves a good privacy-distortion compared to DP,
AP, and DistP (Fig. 6c). More discussions are below.
(2) Our data release mechanisms achieve better privacy-distortion trade-off than DP, AP, and DistP. AP
and DistP directly add Gaussian/Laplace noise to each sample. This process does not change the mean of
the distribution on expectation. Therefore, Figure 6 shows that AP and DistP have a bad privacy-distortion
tradeoff. DP quantizes (bins) the samples before adding noise. Quantization has a better property in terms
of protecting the mean of the distribution, and therefore we see that DP has a better privacy-distortion
tradeoff than AP and DistP, but still worse than ours. Note that in Fig. 6c, a few of the DP instances have
better privacy-distortion trade-offs than ours. This is notan indication that DP is fundamentally better.
Instead, it is due to the randomness in DP (from the added Laplace noise), and some realizations of the
specific noise in this experiment happened to lead to a better trade-off. Another instance of the DP algorithm
could lead to a bad trade-off, and therefore, DP’s achievable trade-off points are widespread.
Insummary, theseresultsconfirmourintuitionin§2.2thatDP,AP,andDistParenotsuitable
for summary statistic privacy (which is expected—they are designed for a different objective).
As such, the quantization mechanism (under the summary statistic privacy framework) gives
better practical protections for summary statistic privacy. Additional results on downstream tasks
are in App. I.
8 Limitations
This work has several important limitations, some of which relate to the framework itself, others of which
are specific to the mechanisms and results we prove. We outline several of these limitations.
8.1 Limitations of the Framework
Prior knowledge of distribution. The current privacy metric Πϵ,ωΘdepends on the prior distribution
of the parameters ωΘ, which is typically unknown. The outcome is that if a mechanism is analyzed under a
mismatched prior, it may lead a data holder to over- or under-estimate their privacy parameter.
Composition guarantees. Another limitation of the current privacy metric Πϵ,ωΘis that it does not
provide composition guarantees; in other words, if one applies a summary statistic-private mechanism υ
times, we cannot easily bound the privacy parameter of the υ-fold composed mechanism. In contrast,
composition is an important and desirable property exhibited by differential privacy (Dwork et al., 2006).
The lack of composition can be problematic in situations where a data holder wants to release a dataset (or
correlated datasets) multiple times.
22Under review as submission to TMLR
8.2 Limitations of the Analysis and Mechanisms
Our analysis in this work considers the simplest set of cases, which are neither fully representative of how
real data users release data, nor the secrets they wish to hide.
Number of secrets. In this work, we studied a case where the data holder only wishes to hide a single
secret. In practice, data holders often want to hide multiple properties of their underlying data.
The dimension and the type of data distributions. Although our lower bounds in Section 4 apply to
general prior distributions, we analyze the quantization mechanism under a limited set of one-dimensional
distributions (Table 1) under which different parameters of the distribution are drawn independently of each
other. An interesting direction for future work is to define mechanisms that have good tradeoffs under prior
distributions with correlated parameters and priors.
9 Discussion and Future Work
We introduce summary statistic privacy for defining, analyzing, and protecting summary statistic privacy
concerns in data sharing applications. This framework can be used to analyze the leakage of statistical
information and the privacy-distortion trade-offs of data release mechanisms (§ 3 and 4). Our data release
mechanisms can be used to protect statistical information (§ 5 and 6). However, as discussed in §8, this
paper leaves many questions unanswered. Several of these pose interesting questions for future work.
Approximation error. We studied a number of data distributions and prior distributions in this work.
However, an interesting question is to bound the error in privacy and distortion metrics as a function of
approximation error when describing either the original data distribution or the prior.
Extensions. As described in §8, one limitation of the current privacy metric Πϵ,ωΘis that it depends on
the prior distribution of the parameters ωΘ, which is unknown in many applications. Motivated by maximal
leakage (Issa et al., 2019) (§2.2), one possibility is to consider a normalized privacy metric:
Π′
ϵ,ωΘ≜sup
ωΘlogΠϵ,ωΘ
supˆgP(ˆg(ωΘ)∈[g(θ)−ϵ,g(θ) +ϵ]),
where ˆg(ωΘ)is an attacker that knows the prior distribution but does not see the released data, and the
denominator is the probability that the strongest attacker guesses the secret within tolerance ϵ. Similar
to maximal leakage, we consider the worst-case leakage among all possible priors. This normalized Π′
ϵ,ωΘ
considers how much additional “information” that the released data provides to the attacker in the worst-
case (see also inferential privacy (Ghosh & Kleinberg, 2016)). This privacy definition is strong so that we
will not be able to achieve good privacy and reasonable distortion at the same time.
Proposition 3. Let∆≜1
2supθ1,θ2∈Supp (ωΘ)d/parenleftbig
ωXθ1∥ωXθ2/parenrightbig
. There exists no Mgsuch that Π′
ϵ,ωΘ<log 2
and∆<∆.
The proof is in App. C.6. It would be interesting to further study the feasibility of such a formulation, for
instance by changing the utility metric to an expected distortion, rather than a worst-case one.
References
The caida ucsd anonymized internet traces. https://www.caida.org/catalog/datasets/passive_
dataset. Accessed: 2022-01-30.
Martin Arjovsky, Soumith Chintala, and Léon Bottou. Wasserstein generative adversarial networks. In
International conference on machine learning , pp. 214–223. PMLR, 2017.
Giuseppe Ateniese, Luigi V Mancini, Angelo Spognardi, Antonio Villani, Domenico Vitali, and Giovanni
Felici. Hacking smart machines with smarter ones: How to extract meaningful data from machine learning
classifiers. International Journal of Security and Networks , 10(3):137–150, 2015.
23Under review as submission to TMLR
Richard Bellman. Dynamic programming. Science, 153(3731):34–37, 1966.
Elias Chaibub Neto, Abhishek Pratap, Thanneer M Perumal, Meghasyam Tummalacherla, Phil Snyder,
Brian M Bot, Andrew D Trister, Stephen H Friend, Lara Mangravite, and Larsson Omberg. Detecting the
impact of subject characteristics on machine learning-based diagnostic applications. NPJ digital medicine ,
2(1):1–6, 2019.
Konstantinos Chatzikokolakis, Miguel E Andrés, Nicolás Emilio Bordenabe, and Catuscia Palamidessi.
Broadening the scope of differential privacy using metrics. In Privacy Enhancing Technologies: 13th
International Symposium, PETS 2013, Bloomington, IN, USA, July 10-12, 2013. Proceedings 13 , pp.
82–102. Springer, 2013.
Harsh Chaudhari, John Abascal, Alina Oprea, Matthew Jagielski, Florian Tramèr, and Jonathan Ullman.
Snap: Efficient extraction of private properties with poisoning. In 2023 IEEE Symposium on Security and
Privacy (SP) , pp. 1935–1952. IEEE Computer Society, 2022.
Michelle Chen and Olga Ohrimenko. Protecting global properties of datasets with distribution privacy
mechanisms. arXiv preprint arXiv:2207.08367 , 2022.
Nazli Choucri, Stuart Madnick, and Priscilla Koepke. Institutions for cyber security: International responses
and data sharing initiatives. Cambridge, MA: Massachusetts Institute of Technology , 2016.
Federal Communications Commission. Raw data - measuring broadband america - seventh re-
port, 2018. https://www.fcc.gov/reports-research/reports/measuring-broadband-america/
raw-data-measuring-broadband-america-seventh .
Eli Cortez, Anand Bonde, Alexandre Muzio, Mark Russinovich, Marcus Fontoura, and Ricardo Bianchini.
Resource central: Understanding and predicting workloads for improved resource management in large
cloud platforms. In Proceedings of the 26th Symposium on Operating Systems Principles , pp. 153–167,
2017.
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical
image database. In 2009 IEEE conference on computer vision and pattern recognition , pp. 248–255. Ieee,
2009.
Cynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam Smith. Calibrating noise to sensitivity in private
data analysis. In Theory of Cryptography: Third Theory of Cryptography Conference, TCC 2006, New
York, NY, USA, March 4-7, 2006. Proceedings 3 , pp. 265–284. Springer, 2006.
Cristóbal Esteban, Stephanie L Hyland, and Gunnar Rätsch. Real-valued (medical) time series generation
with recurrent conditional gans. arXiv preprint arXiv:1706.02633 , 2017.
Karan Ganju, Qi Wang, Wei Yang, Carl A Gunter, and Nikita Borisov. Property inference attacks on fully
connected neural networks using permutation invariant representations. In Proceedings of the 2018 ACM
SIGSAC conference on computer and communications security , pp. 619–633, 2018.
Arpita Ghosh and Robert Kleinberg. Inferential privacy guarantees for differentially private mechanisms.
arXiv preprint arXiv:1603.01508 , 2016.
Google. Web traffic time series forecasting, 2018. https://www.kaggle.com/c/
web-traffic-time-series-forecasting .
Ibrahim Issa, Aaron B Wagner, and Sudeep Kamath. An operational approach to information leakage. IEEE
Transactions on Information Theory , 66(3):1625–1657, 2019.
James B Jacobs and Dimitra Blitsa. Sharing criminal records: The united states, the european union and
interpol compared. Loy. LA Int’l & Comp. L. Rev. , 30:125, 2008.
24Under review as submission to TMLR
Junchen Jiang, Vyas Sekar, Henry Milner, Davis Shepherd, Ion Stoica, and Hui Zhang. {CFA}: A practical
prediction system for video {QoE}optimization. In 13th USENIX Symposium on Networked Systems
Design and Implementation (NSDI 16) , pp. 137–150, 2016.
James Jordon, Jinsung Yoon, and Mihaela Van Der Schaar. Pate-gan: Generating synthetic data with
differential privacy guarantees. In International conference on learning representations , 2018.
Yusuke Kawamoto and Takao Murakami. Local obfuscation mechanisms for hiding probability distributions.
InComputer Security–ESORICS 2019: 24th European Symposium on Research in Computer Security,
Luxembourg, September 23–27, 2019, Proceedings, Part I 24 , pp. 128–148. Springer, 2019.
Daniel Kifer and Ashwin Machanavajjhala. Pufferfish: A framework for mathematical privacy definitions.
ACM Transactions on Database Systems (TODS) , 39(1):1–36, 2014.
Hau L Lee and Seungjin Whang. Information sharing in a supply chain. International journal of manufac-
turing technology and management , 1(1):79–93, 2000.
Zinan Lin, Ashish Khetan, Giulia Fanti, and Sewoong Oh. Pacgan: The power of two samples in generative
adversarial networks. Advances in neural information processing systems , 31, 2018.
Zinan Lin, Alankar Jain, Chen Wang, Giulia Fanti, and Vyas Sekar. Using gans for sharing networked
time series data: Challenges, initial promise, and open questions. In Proceedings of the ACM Internet
Measurement Conference , pp. 464–483, 2020.
Terrance Liu and Zhiwei Steven Wu. Private synthetic data with hierarchical structure. arXiv preprint
arXiv:2206.05942 , 2022.
Shutian Luo, Huanle Xu, Chengzhi Lu, Kejiang Ye, Guoyao Xu, Liping Zhang, Yu Ding, Jian He, and
Chengzhong Xu. Characterizing microservice dependency and performance: Alibaba trace analysis. In
Proceedings of the ACM Symposium on Cloud Computing , pp. 412–426, 2021.
Saeed Mahloujifar, Esha Ghosh, and Melissa Chase. Property inference from poisoning. In 2022 IEEE
Symposium on Security and Privacy (SP) , pp. 1120–1137. IEEE, 2022.
Ali Makhdoumi, Salman Salamatian, Nadia Fawaz, and Muriel Médard. From the information bottleneck to
the privacy funnel. In 2014 IEEE Information Theory Workshop (ITW 2014) , pp. 501–505. IEEE, 2014.
Antonis Manousis, Harshil Shah, Henry Milner, Yan Li, Hui Zhang, and Vyas Sekar. The shape of view:
an alert system for video viewership anomalies. In Proceedings of the 21st ACM Internet Measurement
Conference , pp. 245–260, 2021.
Charles Reiss, John Wilkes, and Joseph L Hellerstein. Google cluster-usage traces: format+ schema. Google
Inc., White Paper , pp. 1–14, 2011.
Charles Reiss, John Wilkes, and Joseph L Hellerstein. Obfuscatory obscanturism: making workload traces
of commercially-sensitive systems safe to release. In 2012 IEEE Network Operations and Management
Symposium , pp. 1279–1286. IEEE, 2012.
Anshuman Suri and David Evans. Formalizing and estimating distribution inference risks. arXiv preprint
arXiv:2109.06024 , 2021.
Anshuman Suri, Yifu Lu, Yanjin Chen, and David Evans. Dissecting distribution inference. In First IEEE
Conference on Secure and Trustworthy Machine Learning , 2023.
Leigh R Warren, Jonathan Clarke, Sonal Arora, and Ara Darzi. Improving data sharing between acute
hospitals in england: an overview of health record system distribution and retrospective observational
analysis of inter-hospital transitions of care. BMJ open , 9(12):e031637, 2019.
LarryWassermanandShuhengZhou. Astatisticalframeworkfordifferentialprivacy. Journal of the American
Statistical Association , 105(489):375–389, 2010.
25Under review as submission to TMLR
John Wilkes. Google cluster-usage traces v3. Technical report, Google Inc., Mountain View, CA, USA, April
2020. Posted at https://github.com/google/cluster-data/blob/master/ClusterData2019.md .
Yucheng Yin, Zinan Lin, Minhao Jin, Giulia Fanti, and Vyas Sekar. Practical gan-based synthetic ip header
trace generation using netshare. In Proceedings of the ACM SIGCOMM 2022 Conference , pp. 458–472,
2022.
Jinsung Yoon, Daniel Jarrett, and Mihaela Van der Schaar. Time-series generative adversarial networks.
Advances in neural information processing systems , 32, 2019.
James Hongyi Zeng. Data sharing on traffic pattern inside facebook’s data center net-
work - meta research, Jan 2017. URL https://research.facebook.com/blog/2017/01/
data-sharing-on-traffic-pattern-inside-facebooks-datacenter-network/ .
Wanrong Zhang, Shruti Tople, and Olga Ohrimenko. Leakage of dataset properties in multi-party machine
learning. In USENIX Security Symposium , pp. 2687–2704, 2021.
Wanrong Zhang, Olga Ohrimenko, and Rachel Cummings. Attribute privacy: Framework and mechanisms.
In2022 ACM Conference on Fairness, Accountability, and Transparency , pp. 757–766, 2022.
Junhao Zhou, Yufei Chen, Chao Shen, and Yang Zhang. Property inference attacks against gans. arXiv
preprint arXiv:2111.07608 , 2021.
26Under review as submission to TMLR
Appendix
A Analysis of the Alternative Formulation
In this section, we present the alternative formulation of minimizing privacy metric Πϵ,ωΘsubject to a
constraint on distortion ∆:
min
MgΠϵ,ωΘsubject to ∆≤T(16)
Theorem 2 (Lower bound of privacy-distortion tradeoff) .LetD(Xθ1,Xθ2)≜1
2d/parenleftbig
ωXθ1∥ωXθ2/parenrightbig
, whered(·∥·)
is defined in Eq. (3). Further, let R(Xθ1,Xθ2)≜|g(θ1)−g(θ2)|, and letγ≜infθ1,θ2∈Supp (ωΘ)D(Xθ1,Xθ2)
R(Xθ1,Xθ2).
For anyT >0, when ∆≤T, we have Πϵ,ωΘ≥⌈T
2γϵ⌉−1.
Proof.For anyθ′, we have
T≥∆
≥ sup
θ∈Supp (ωΘ),z∈Supp (ωZ):Mg(θ,z)=θ′d/parenleftbig
ωXθ∥ωXθ′/parenrightbig
≥ sup
θi∈Supp (ωΘ),zi:Mg(θi,zi)=θ′D(Xθ1,Xθ2) (17)
≥γ· sup
θi∈Supp (ωΘ),zi:Mg(θi,zi)=θ′R(Xθ1,Xθ2)
where Eq. (17) comes from triangle inequality.
Let
Lθ′≜ inf
θ∈Supp (ωΘ),z:Mg(θ,z)=θ′g(θ),
Rθ′≜ sup
θ∈Supp (ωΘ),z:Mg(θ,z)=θ′g(θ).
From the above result, we know that Rθ′−Lθ′≤T
γ. We can define a sequence of attackers such that
ˆgi(θ′) =Lθ′+ (i+ 0.5)·2ϵfori∈/braceleftig
0,1,...,⌈T
2γϵ⌉−1/bracerightig
(Fig. 7). We have
!!!"!!!!!+2%!!!+4%!!!+'2(%2%+')+'"+'*+,-."Possiblerangeof'(,)Rangeof'(,)that+')succeedsRangeof'(,)that+'"succeedsRangeof'(,)that+'!"#$."succeeds
Figure 7: The construction of attackers for proof of Thm. 2. The 2ϵranges of ˆg0,...,ˆg⌈T
2γϵ⌉−1jointly cover the
entire range of possible secret [Lθ′,Rθ′]. Therefore, there exists one attacker whose probability of guessing
the secret correctly within ϵis≥⌈T
2γϵ⌉−1(Eq. (18)).
27Under review as submission to TMLR
/summationdisplay
iP/parenleftbigg
ˆgi(θ′)∈[g(θ)−ϵ,g(θ) +ϵ]/vextendsingle/vextendsingle/vextendsingle/vextendsingleθ′/parenrightbigg
≥1,
and therefore,
max
iP/parenleftbigg
ˆgi(θ′)∈[g(θ)−ϵ,g(θ) +ϵ]/vextendsingle/vextendsingle/vextendsingle/vextendsingleθ′/parenrightbigg
≥⌈T
2γϵ⌉−1, (18)
which implies that
sup
ˆgP/parenleftbigg
ˆg(θ′)∈[g(θ)−ϵ,g(θ) +ϵ]/vextendsingle/vextendsingle/vextendsingle/vextendsingleθ′/parenrightbigg
≥⌈T
2γϵ⌉−1.
Therefore, we have
Πϵ,ωΘ= sup
ˆgP(ˆg(θ′)∈[g(θ)−ϵ,g(θ) +ϵ])
= sup
ˆgE/parenleftbigg
P/parenleftbigg
ˆg(θ′)∈[g(θ)−ϵ,g(θ) +ϵ]/vextendsingle/vextendsingle/vextendsingle/vextendsingleθ′/parenrightbigg/parenrightbigg
=E/parenleftbigg
sup
ˆgP/parenleftbigg
ˆg(θ′)∈[g(θ)−ϵ,g(θ) +ϵ]/vextendsingle/vextendsingle/vextendsingle/vextendsingleθ′/parenrightbigg/parenrightbigg
≥⌈T
2γϵ⌉−1.
B Binary Search and Greedy Algorithms for Designing Quantization Mechanism
We use the binary search algorithm in Alg. 2 to search for the distortion budget that matches the privacy
budget under the optimal data release mechanism.
Algorithm 2: Data release mechanism with privacy budget.
Input:Parameter range:/bracketleftbig
θ,θ/parenrightbig
Privacy budget: T
Distortion budget search range: [B,B]
Step size:s(which divides θ−θ)
Precision:η
1whileT−T≥ηdo
2pri,S,θ′←Algorithm-1/parenleftig/bracketleftbig
θ,θ/parenrightbig
,T+T
2,κ/parenrightig
3ifpri>Tthen
4B←T+T
2
5else
6B←T+T
2
7returnData release mechanism parameters: S,θ′
We provide the greedy algorithm in Alg. 3. In this algorithm, we greedily select the ranges of θfor eachSi
in order. The left end point of the first range is the parameter lower bound (Line 2). We then scan across
all possible right end point such that the distortion for this range will not exceed the budget T(Line 8), and
pick the one that gives the minimal attacker confidence (Line 10). After deciding the range of θ, we will set
of the released distribution for this range (Line 16), and then move on to the next range (Line 21). The time
complexity of this algorithm is O/parenleftig/parenleftbig
θ−θ/κ/parenrightbig2·CD·CP/parenrightig
, the same as the dynamic programming algorithm.
28Under review as submission to TMLR
Algorithm 3: Greedy-based data release mechanism for single-parameter distributions.
Input:Parameter range:/parenleftbig
θ,θ/bracketrightbig
Prior over parameter: fΘ
Distortion budget: T
Step size:κ(which divides θ−θ)
1I←∅
2L←θ
3privacy←0
4whileL<θdo
5min_p←∞
6min_R←NULL
7R←L
8whileR≤θandD(L,R)≤Tdo
9p←P (L,R)
10 ifp≤min_pthen
11 min_p←p
12 min_R←R
13R←R+κ
14ifmin_Ris not NULL then
15SL←{Xθ:θ∈(L, min_R]}
16θ′
L←D (L,min_R)
17I←I∪{L}
18privacy←/integraltextL
θfΘ(t)dt
/integraltextmin_R
θfΘ(t)dt·privacy +/integraltextmin_R
LfΘ(t)dt/integraltextmin_R
θfΘ(t)dt·min_p
19else
20 ERROR: No answer
21L←min_R
22returnprivacy,{Si:i∈I},{θ′
i:i∈I}
29Under review as submission to TMLR
C Proofs
C.1 Proof of Corollary 1
Proof.For anyXθ1,Xθ2, we have
D(Xθ1,Xθ2) =1
2dWasserstein-1/parenleftbig
ωXθ1∥ωXθ2/parenrightbig
≥1
2|g(θ1)−g(θ2)| (19)
=1
2R(Xθ1,Xθ2).
where Eq. (19) comes from Jensen inequality. Therefore, we have γ= infθ1,θ2∈Supp (ωΘ)D(Xθ1,Xθ2)
R(Xθ1,Xθ2)≥1
2. The
result then follows from Thm. 1.
C.2 Proof of Prop. 1
Proof.For any released parameter θ′= (u′,v′), there exists i∈{0,...,N−1}such thatu′=u+ (i+ 0.5)·s.
We have
sup
ˆgP/parenleftbig
ˆg(θ′)∈[g(θ)−ϵ,g(θ) +ϵ]/vextendsingle/vextendsingleθ′/parenrightbig
= sup
ˆg/integraldisplayu+(i+1)·s
u+i·sfU|U′(u|u′)·/integraldisplayu+ϵ
u−ϵfˆg(u′,v′)(h) dhdu
= sup
ˆg/integraldisplayu+(i+1)·s+ϵ
u+i·s−ϵfˆg(u′,v′)(h)·/integraldisplayˆg/parenleftbig
fXu′,v′/parenrightbig
+ϵ
ˆg/parenleftbig
fXu′,v′/parenrightbig
−ϵfU|U′(u|u′) dudh
≤sup
ˆg/integraldisplayu+(i+1)·s+ϵ
u+i·s−ϵ2ϵ
s·fˆg(u′,v′)(h) dh
≤2ϵ
s.
Therefore, we have
Πϵ,ωΘ= sup
ˆgP(ˆg(θ′)∈[g(θ)−ϵ,g(θ) +ϵ])
= sup
ˆgE/parenleftbigg
P/parenleftbigg
ˆg(θ′)∈[g(θ)−ϵ,g(θ) +ϵ]/vextendsingle/vextendsingle/vextendsingle/vextendsingleθ′/parenrightbigg/parenrightbigg
=E/parenleftbigg
sup
ˆgP/parenleftbigg
ˆg(θ′)∈[g(θ)−ϵ,g(θ) +ϵ]/vextendsingle/vextendsingle/vextendsingle/vextendsingleθ′/parenrightbigg/parenrightbigg
≤2ϵ
s.
Forthedistortion, wecaneasilygetthat ∆ =s
2. AccordingtoCorollary1, wehave ∆opt>/parenleftig
⌈1
Πϵ,ωΘ⌉−1/parenrightig
ϵ≥
ϵ. We can get that
∆ = ∆ opt+ ∆−∆opt
<∆opt+ ∆−/parenleftbigg
⌈1
Πϵ,ωΘ⌉−1/parenrightbigg
·ϵ
≤∆opt+ϵ+ ∆−ϵ
Πϵ,ωΘ
≤∆opt+ϵ
≤2∆opt.
30Under review as submission to TMLR
C.3 Proof of Corollary 2
C.3.1 Exponential Distribution
Proof.LetXλ1,Xλ2be two exponential random variables. We have
D(Xλ1,Xλ2)
R(Xλ1,Xλ2)=1
2(λ1−λ2)
−ln (1−α) (λ1−λ2)=−1
2 ln (1−α). (20)
Therefore we can get that
γ=−1
2 ln (1−α).
C.3.2 Shifted Exponential Distribution
Proof.LetXλ1,h1,Xλ2,h2be random variables from shifted exponential distributions. Let λ2≤λ1without
loss of generality. Let a=λ1
λ2andb= (h1/λ1−h2/λ2)λ2. We can get that fXλ1,h1(x) =afXλ2,h2(a(x+b)),
and
D(Xλ1,h1,Xλ2,h2) =1
2dWasserstein-1/parenleftbig
ωXλ1,h1∥ωXλ2,h2/parenrightbig
=1
2/integraldisplay+∞
h1/vextendsingle/vextendsingle/vextendsinglex−/parenleftigx
a−b/parenrightig/vextendsingle/vextendsingle/vextendsinglefXλ1,h1(x) dx
=λ2
2λ1/integraldisplay+∞
h1|(1/λ2−1/λ1)x+h1/λ1−h2/λ2|e−1
λ1(x−h1)dx
=/braceleftigg
1
2(h2−h1+λ2−λ1)−eh2−h1
λ2−λ1(λ2−λ1) (h1<h2)
1
2(h1−h2+λ1−λ2) ( h1≥h2), (21)
R(Xλ1,h1,Xλ2,h2) =|ln (1−α) (λ1−λ2) +h2−h1|.
Whenh1<h2, lett=h2−h1
λ1−λ2∈(0,+∞). We have
D(Xλ1,h1,Xλ2,h2)
R(Xλ1,h1,Xλ2,h2)
=h2−h1+λ2−λ1−2eh2−h1
λ2−λ1(λ2−λ1)
2|ln (1−α) (λ1−λ2) +h2−h1|
=t+ 2e−t−1
2|ln (1−α) +t|
≥

1
2/vextendsingle/vextendsingle/vextendsingle/vextendsingle1 +ln(1−α)+1
W−1/parenleftbig
−ln(1−α)+1
2(1−α)e/parenrightbig/vextendsingle/vextendsingle/vextendsingle/vextendsingleα∈[0,1−e−1)
1
2/vextendsingle/vextendsingle/vextendsingle/vextendsingle1 +ln(1−α)+1
W0/parenleftbig
−ln(1−α)+1
2(1−α)e/parenrightbig/vextendsingle/vextendsingle/vextendsingle/vextendsingleα∈[1−e−1,1),
whereW−1andW0are Lambert Wfunctions. “ =” achieves when
t=t0≜

−1−ln (1−α)−W−1/parenleftig
−ln(1−α)+1
2(1−α)e/parenrightig/parenleftbig
α∈[0,1−e−1)/parenrightbig
−1−ln (1−α)−W0/parenleftig
−ln(1−α)+1
2(1−α)e/parenrightig/parenleftbig
α∈[1−e−1,1)/parenrightbig.
31Under review as submission to TMLR
Whenh1≥h2, lett=h1−h2
λ1−λ2∈(0,+∞). We have
D(Xλ1,h1,Xλ2,h2)
R(Xλ1,h1,Xλ2,h2)=h1−h2+λ1−λ2
2|ln (1−α) (λ1−λ2) +h2−h1|
=t+ 1
2|ln (1−α)−t|
≥min/braceleftbigg1
2,−1
2 ln (1−α)/bracerightbigg
.
Therefore we can get that
γ=

1
2/vextendsingle/vextendsingle/vextendsingle/vextendsingle1 +ln(1−α)+1
W−1/parenleftbig
−ln(1−α)+1
2(1−α)e/parenrightbig/vextendsingle/vextendsingle/vextendsingle/vextendsingleα∈[0,1−e−1)
1
2/vextendsingle/vextendsingle/vextendsingle/vextendsingle1 +ln(1−α)+1
W0/parenleftbig
−ln(1−α)+1
2(1−α)e/parenrightbig/vextendsingle/vextendsingle/vextendsingle/vextendsingleα∈[1−e−1,1).
C.4 Proof of Prop. 2
C.4.1 Exponential Distribution
Proof.The proof of ∆andΠϵ,ωΘis the same as App. C.2, except that we use the D(·,·)andR(·,·)from
Eq. (20).
For∆opt, we have ∆opt>/parenleftig
⌈1
Πϵ,ωΘ⌉−1/parenrightig
·2γϵ≥2γϵ, whereγ=−1
2 ln(1−α). We can get that
∆ = ∆ opt+ ∆−∆opt
<∆opt+ ∆−/parenleftbigg
⌈1
Πϵ,ωΘ⌉−1/parenrightbigg
·2γϵ
≤∆opt+ 2γϵ+ ∆−2γϵ
Πϵ,ωΘ
= ∆ opt+ 2γϵ
≤2∆opt.
C.4.2 Shifted Exponential Distribution
Proof.We first focus on the proof for Πϵ,ωΘ.
In Fig. 8, we separate the space of possible data parameters into two regions represented by yellow and green
colors. The yellow regions Syellowconstitute right triangles with height sand width|t0|s. The green region
Sgreenis the rest of the parameter space. The high-level idea of our proof is as follows. Note that for any
parameterθ∈Sgreen, there exists aSi,hs.t.θ∈Si,handSµ,i⊂Sgreen. Therefore, we can bound the attack
success rate if θ∈Sgreen. At the same time, the probability of θ∈Syellowis bounded. Therefore, we can
bound the overall attacker’s success rate (i.e., Πϵ,ωΘ). More specifically, let the optimal attacker be ˆg∗. We
32Under review as submission to TMLR
7ℎℎ7
ℎ7…222|4)|2|4)|2…Thespaceofpossibleparameters
Figure 8: The construction for proof of Prop. 2 for shifted exponential distributions. We separate the space
of possible parameters into two regions (yellow and green) and bound the attacker’s success rate on each
region separately.
have
Πϵ,ωΘ=P(ˆg∗(θ′)∈[g(θ)−ϵ,g(θ) +ϵ])
=/integraldisplay
θ∈Sgreenp(θ)P(ˆg∗(θ′)∈[g(θ)−ϵ,g(θ) +ϵ])dθ
+/integraldisplay
θ∈Syellowp(θ)P(ˆg∗(θ′)∈[g(θ)−ϵ,g(θ) +ϵ])dθ
<2ϵ
|ln (1−α) +t0|s+|t0|s
h−h.
For the distortion, it is straightforward to get that ∆ =s
2(t0−1) +se−t0from Eq. (21), and ∆opt>/parenleftig
⌈1
Πϵ,ωΘ⌉−1/parenrightig
·2γϵ≥2γϵ, whereγis defined in Corollary 2. Denote ζ=2ϵ
|ln(1−α)+t0|s+|t0|s
h−h−Πϵ,ωΘ, we
33Under review as submission to TMLR
can get that/parenleftig
Πϵ,ωΘ+ζ−|t0|s
h−h/parenrightig
·∆ = 2γϵand
∆ = ∆ opt+ ∆−∆opt
<∆opt+ ∆−/parenleftbigg
⌈1
Πϵ,ωΘ⌉−1/parenrightbigg
·2γϵ
≤∆opt+ 2γϵ+ ∆−2γϵ
Πϵ,ωΘ
= ∆ opt+ 2γϵ+|t0|s
h−h−ζ
2ϵ
|ln(1−α)+t0|s+|t0|s
h−h−ζ·∆
<∆opt+ 2γϵ+|t0|s
h−h
2ϵ
|ln(1−α)+t0|s+|t0|s
h−h·∆.
Therefore,
∆</parenleftigg
1 +|t0|·|ln (1−α) +t0|s2
2ϵ/parenleftbig
h−h/parenrightbig/parenrightigg
(∆opt+ 2γϵ)
≤/parenleftigg
2 +|t0|·|ln (1−α) +t0|s2
ϵ/parenleftbig
h−h/parenrightbig/parenrightigg
∆opt.
t0is bounded when α∈[0,c1]∪/bracketleftbig
1−1
e,c2/bracketrightbig
, wherec1∈/bracketleftbig
0,1−1
e/parenrightbig
,c2∈/bracketleftbig
1−1
e,1/parenrightbig
. Therefore, when α∈
[0.01,0.25]∪[0.75,0.99], we can get that
lim sup
s2
h−h→0∆<lim sup
s2
h−h→0/parenleftigg
2 +|t0|·|ln (1−α) +t0|s2
ϵ/parenleftbig
h−h/parenrightbig/parenrightigg
∆opt<3∆opt.
C.5 Proofs for the Surrogate Metrics
C.5.1 Secret=Mean
For anypY, we have
˜∆ =dWasserstein-1 (pX∥pY)≥/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle1
nn/summationdisplay
i=1xi−1
nn/summationdisplay
i=1yi/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle=−˜Πϵ,ωΘ.
ForpYreleased from our mechanism (§6.3), we have ˜∆ =dWasserstein-1 (pX∥pY) =/vextendsingle/vextendsingle1
n/summationtextn
i=1xi−1
n/summationtextn
i=1yi/vextendsingle/vextendsingle=
−˜Πϵ,ωΘ.
C.5.2 Secret=Fraction
Assume that we want to protect the fraction of class j, andfraction (D,j)means the fraction of sample j
in the datasetD.
For anypY, we have
˜∆ =dTV(pX∥pY)≥|fraction (X,j)−fraction (Y,j)|=−˜Πϵ,ωΘ.
ForpYreleased from our mechanism (Mech. 5), we have ˜∆ = dTV(pX∥pY) =
|fraction (X,j)−fraction (Y,j)|=−˜Πϵ,ωΘ.
34Under review as submission to TMLR
C.6 Proof of Prop. 3
Proof.We prove by contradiction. For any two parameters θ1,θ2∈Supp (ωΘ), we can construct a prior
distribution P(θ=θ1) =P(θ=θ2) =1
2. Because Π′
ϵ,ωΘ<log 2, we have
sup
ˆgP(ˆg(θ′)∈[g(θ)−ϵ,g(θ) +ϵ])<1
under this prior distribution. Therefore, there exists θ′andz1,z2∈Supp (ωΘ)s.t.Mg(θ1,z1) =
Mg(θ2,z2) =θ′. According to triangle inequality, we have max/braceleftbig
d/parenleftbig
ωXθ1∥ωXθ′/parenrightbig
,d/parenleftbig
ωXθ2∥ωXθ′/parenrightbig/bracerightbig
≥
1
2d/parenleftbig
ωXθ1∥ωXθ2/parenrightbig
. Therefore, we have ∆≥∆, which gives a contradiction.
D Privacy-Distortion Performance of Data Release Mechanism with Relaxed
Assumption
D.1 Privacy-Distortion Performance of Mech. 1 with Relaxed Assumption
We relax Asm. 1 as follows.
Assumption 3.The distribution parameter vector θcan be written as (u,v), whereu∈R,v∈Rq−1,
and for any u̸=u′,fXu,v(x) =fXu′,v(x−u′+u). The prior over distribution parameters is fU,V(a,b) =
fU(a)·fV(b), where Supp (U) = [u,u), andfUisL-Lipschitz continuous and has lower bound c.
Based on Asm. 3, the Privacy-distortion performance of Mech. 1 is shown below.
Proposition 4. Under Asm. 3, Mech. 1 has ∆ =s
2andΠϵ,ωΘ≤2ϵ[c+L(s−x∗−ϵ)]
cs+L
2(s−x∗)2, wherex∗=s+c
L−ϵ−
/radicalig/parenleftbigc
L−ϵ/parenrightbig2+2cs
L.
Proof.We first provide the following lemma.
Lemma 1. For aL-Lipschitz continuous function f(x),x∈[x,x],infx∈[x,x]f(x)≥c≥0, it satisfies
sup
x′∈[x,x−δ]/integraltextx′+δ
x′f(x)dx
/integraltextx
xf(x)dx≤δ/bracketleftbig
c+L/parenleftbig
x−x∗−δ
2/parenrightbig/bracketrightbig
c(x−x) +L
2(x−x∗)2,
wherex∗=x+c
L−δ
2−/radicalig/parenleftbigc
L−δ
2/parenrightbig2+2c(x−x)
L.
For any released parameter θ′= (u′,v′), there exists i∈{0,...,N−1}such thatu′=u+ (i+ 0.5)·s. We
have
sup
ˆgP/parenleftbig
ˆg(θ′)∈[g(θ)−ϵ,g(θ) +ϵ]/vextendsingle/vextendsingleθ′/parenrightbig
= sup
ˆg/integraldisplayu+(i+1)·s
u+i·sfU|U′(u|u′)·/integraldisplayu+ϵ
u−ϵfˆg(u′,v′)(h) dhdu
= sup
ˆg/integraldisplayu+(i+1)·s+ϵ
u+i·s−ϵfˆg(u′,v′)(h)·/integraldisplayˆg/parenleftbig
fXu′,v′/parenrightbig
+ϵ
ˆg/parenleftbig
fXu′,v′/parenrightbig
−ϵfU|U′(u|u′) dudh.
For/integraltextˆg/parenleftbig
fXu′,v′/parenrightbig
+ϵ
ˆg/parenleftbig
fXu′,v′/parenrightbig
−ϵfU|U′(u|u′) du, denote
x1= max/parenleftig
0,ˆg/parenleftig
fXu′,v′/parenrightig
−ϵ−u−i·s/parenrightig
,
x2= min/parenleftig
ˆg/parenleftig
fXu′,v′/parenrightig
+ϵ−u−i·s,s/parenrightig
,
35Under review as submission to TMLR
we have
/integraldisplayˆg/parenleftbig
fXu′,v′/parenrightbig
+ϵ
ˆg/parenleftbig
fXu′,v′/parenrightbig
−ϵfU|U′(u|u′) du=/integraltextx2
x1fU(u+i·s+x) dx/integraltexts
0fU(u+i·s+x) dx.
fU(u+i·s+x)isL-Lipschitz and has lower bound c.x2−x1≤2ϵandx1,x2∈[0,s]. According to
Lemma 1, we have
/integraldisplayˆg/parenleftbig
fXu′,v′/parenrightbig
+ϵ
ˆg/parenleftbig
fXu′,v′/parenrightbig
−ϵfU|U′(u|u′) du=/integraltextx2
x1fU(u+i·s+x) dx/integraltexts
0fU(u+i·s+x) dx
≤2ϵ[c+L(s−x∗−ϵ)]
cs+L
2(s−x∗)2,
wherex∗=s+c
L−ϵ−/radicalig/parenleftbigc
L−ϵ/parenrightbig2+2cs
L.
Therefore, we can get that
sup
ˆgP/parenleftbig
ˆg(θ′)∈[g(θ)−ϵ,g(θ) +ϵ]/vextendsingle/vextendsingleθ′/parenrightbig
≤sup
ˆg/integraldisplayu+(i+1)·s+ϵ
u+i·s−ϵ2ϵ[c+L(s−x∗−ϵ)]
cs+L
2(s−x∗)2·fˆg(u′,v′)(h) dh
≤2ϵ[c+L(s−x∗−ϵ)]
cs+L
2(s−x∗)2.
Therefore, we have
Πϵ,ωΘ= sup
ˆgP(ˆg(θ′)∈[g(θ)−ϵ,g(θ) +ϵ])
= sup
ˆgE/parenleftbigg
P/parenleftbigg
ˆg(θ′)∈[g(θ)−ϵ,g(θ) +ϵ]/vextendsingle/vextendsingle/vextendsingle/vextendsingleθ′/parenrightbigg/parenrightbigg
=E/parenleftbigg
sup
ˆgP/parenleftbigg
ˆg(θ′)∈[g(θ)−ϵ,g(θ) +ϵ]/vextendsingle/vextendsingle/vextendsingle/vextendsingleθ′/parenrightbigg/parenrightbigg
≤2ϵ[c+L(s−x∗−ϵ)]
cs+L
2(s−x∗)2.
For the distortion, we can easily get that ∆ =s
2.
D.1.1 Proof of Lemma 1
Without loss of generality, we assume that f(x)≥f(x). Based on simple geometric analysis, we can get that
when/integraltextx′+δ
x′f(x)dx/integraltextx
xf(x)dxachieves supremum, as illustrated in Fig. 9, f(x) =c,x′=x−δ, andf(x) =x+L(x−x′′),
wherex′′∈[x,x′].
In this case, we can get that
/integraltextx
x−δf(x)dx
/integraltextx
xf(x)dx=δ/bracketleftbig
c+L/parenleftbig
x−x′′−δ
2/parenrightbig/bracketrightbig
c(x−x) +L
2(x−x′′)2≜h(x′′),
36Under review as submission to TMLR
𝑥𝑥𝑐+ℒ𝑥−𝑥′′𝑥′′𝑐𝑥𝑓(𝑥)𝑥"=𝑥−𝛿
Figure 9: Illustration of f(x)when/integraltextx′+δ
x′f(x)dx/integraltextx
xf(x)dxachieves supremum.
wherex′′∈[x,x′]. Whenx′′=x+c
L−δ
2−/radicalig/parenleftbigc
L−δ
2/parenrightbig2+2c(x−x)
L≜x∗,h(x′′)achieves supremum. Therefore,
we have
sup
x′∈[x,x−δ]/integraltextx′+δ
x′f(x)dx
/integraltextx
xf(x)dx≤sup
fsup
x′∈[x,x−δ]/integraltextx′+δ
x′f(x)dx
/integraltextx
xf(x)dx
=δ/bracketleftbig
c+L/parenleftbig
x−x∗−δ
2/parenrightbig/bracketrightbig
c(x−x) +L
2(x−x∗)2.
D.2 Privacy-Distortion Performance of Mech. 2 with Relaxed Assumption
We relax Asm. 2 as follows.
Assumption 4.The prior over distribution parameters as specified below.
•Exponential: Supp (λ) =/bracketleftbig
λ,λ/parenrightbig
, andfλisL-Lipschitz continuous and has lower bound c.
•Shifted exponential: Supp (λ,h) =/braceleftbig
(a,b)|a∈/bracketleftbig
λ,λ/parenrightbig
,b∈/bracketleftbig
h,h/parenrightbig/bracerightbig
,fλ,h(a,b) =fλ(a)·fh(b), andfλ(resp.
fh) isLλ-Lipschitz (resp. Lh-Lipschitz) and has lower boundkλ
µ−µwithkλ∈(0,1](resp.kh
σ−σwithkh∈
(0,1]).
Based on Asm. 4, the Privacy-distortion performance of Mech. 2 is shown below.
Proposition 5. Under Asm. 4, Mech. 2 has the following ∆andΠϵ,ωΘvalue/bound.
•Exponential:
∆ =1
2s,
Πϵ,ωΘ≤2ϵ
−ln(1−α)·/bracketleftig
c+L/parenleftig
s−x∗+ϵ
ln(1−α)/parenrightig/bracketrightig
cs+L
2(s−x∗)2,
wherex∗=s+c
L+ϵ
ln(1−α)−/radicalbigg/parenleftig
c
L+ϵ
ln(1−α)/parenrightig2
+2cs
L.
37Under review as submission to TMLR
•Shifted exponential:
∆ =s
2(t0−1) +se−t0,
Πϵ,ωΘ<2ϵ
|ln(1−α)+t0|·/bracketleftig
c+Lλ,h/parenleftig
s
2−t∗−ϵ
|ln(1−α)+t0|/parenrightig/bracketrightig
cs+Lλ,h
2/parenleftbigs
2−t∗/parenrightbig2+
M/parenleftbigg
h−h,kh
h−h,Lh,1/parenrightbigg
·M/parenleftbigg
λ−λ,kλ
λ−λ,Lλ,1/parenrightbigg
·/parenleftbig
λ−λ/parenrightbig
|t0|s,
wherec=khkλ
(h−h)·(λ−λ), functionMsatisfies
M(x,c,L,A) =/braceleftigg
A
x+Lx
2, ifc≤A
x−Lx
2
c+/radicalbig
2L(A−cx),ifc>A
x−Lx
2,
Lλ,h=LλM/parenleftig
h−h
|t0|,kh
h−h,|t0|Lh,1
|t0|/parenrightig
+|t0|LhM/parenleftig
λ−λ,kλ
λ−λ,Lλ,1/parenrightig
, and
t∗=s
2+c
Lλ,h−ϵ
|ln(1−α)+t0|−/radicalbigg/parenleftig
c
Lλ,h−ϵ
|ln(1−α)+t0|/parenrightig2
+2cs
Lλ,h.
Thet0parameter is defined in Mech. 2.
D.2.1 Proof of Prop. 5 for Exponential Distribution
It is straightforward to get the formula for ∆from Eq. (20). Here we focus on the proof for Πϵ,ωΘ.
Similar to the proof in App. D.1, according to Lemma 1, we have
Πϵ,ωΘ=E/parenleftbigg
sup
ˆgP/parenleftbigg
ˆg(θ′)∈[g(θ)−ϵ,g(θ) +ϵ]/vextendsingle/vextendsingle/vextendsingle/vextendsingleθ′/parenrightbigg/parenrightbigg
≤sup
i∈N,t′∈R/integraltextmin/braceleftbig
s,t′−2ϵ
ln(1−α)/bracerightbig
max{0,t′}fλ(λ+i·s+t) dt
/integraltexts
0fλ(λ+i·s+t) dt
≤2ϵ
−ln(1−α)·/bracketleftig
c+L/parenleftig
s−x∗+ϵ
ln(1−α)/parenrightig/bracketrightig
cs+L
2(s−x∗)2,
wherex∗=s+c
L+ϵ
ln(1−α)−/radicalbigg/parenleftig
c
L+ϵ
ln(1−α)/parenrightig2
+2cs
L.
D.2.2 Proof of Prop. 5 for Shifted Exponential Distribution
It is straightforward to get the formula for ∆from Eq. (21). Here we focus on the proof for Πϵ,ωΘ.
According to Eq. (13), we can bound the attack success rate Πϵ,ωΘas
Πϵ,ωΘ<sup
θ∈SgreenP(ˆg∗(θ′)∈[g(θ)−ϵ,g(θ) +ϵ]) +/integraldisplay
θ∈Syellowp(θ)dθ.
As for the first term supθ∈SgreenP(ˆg∗(θ′)∈[g(θ)−ϵ,g(θ) +ϵ]), we can get that
sup
θ∈SgreenP(ˆg∗(θ′)∈[g(θ)−ϵ,g(θ) +ϵ])
= sup
i∈N,h,t′∈R/integraltextmin/braceleftbig
s
2,t′+2ϵ
|ln(1−α)+t0|/bracerightbig
max{−s
2,t′}fλ,h(λ+ (i+ 0.5)·s+t,h−t0·t) dt
/integraltexts
2
−s
2fλ,h(λ+ (i+ 0.5)·s+t,h−t0·t) dt.
To analyze the above term, we provide the following lemma.
38Under review as submission to TMLR
Lemma 2. For aL-Lipschitz continuous function f(x),x∈[x,x], if/integraltextx
xf(x)dx=Aandinfx∈[x,x]f(x)≥c,
it satisfies
sup
x∈[x,x]f(x)≤/braceleftiggA
x−x+L(x−x)
2, ifc≤A
x−x−L(x−x)
2
c+/radicalbig
2L(A−c(x−x)),ifc>A
x−x−L(x−x)
2
≜M(x−x,c,L,A).
The proof is in App. D.2.3.
Sincefλ,h(λ+ (i+ 0.5)·s+t,h−t0·t) =fλ(λ+ (i+ 0.5)·s+t)·fh(h−t0·t), according to Lemma 2,
we can get that fλ,hisLλ,h-Lipschitz continuous, where
Lλ,h=Lλ·M/parenleftbigg
h−h
|t0|,kh
h−h,|t0|Lh,1
|t0|/parenrightbigg
+|t0|Lh·M/parenleftbigg
λ−λ,kλ
λ−λ,Lλ,1/parenrightbigg
.
We can also get that
inf
a∈[λ,λ),b∈[h,h)fλ,h(a,b)≥khkλ/parenleftbig
h−h/parenrightbig
·/parenleftbig
λ−λ/parenrightbig≜c.
Therefore, according to Lemma 1, we can get that
sup
θ∈SgreenP(ˆg∗(θ′)∈[g(θ)−ϵ,g(θ) +ϵ])
= sup
i∈N,h,t′∈R/integraltextmin/braceleftbig
s
2,t′+2ϵ
|ln(1−α)+t0|/bracerightbig
max{−s
2,t′}fλ,h(λ+ (i+ 0.5)·s+t,h−t0·t) dt
/integraltexts
2
−s
2fλ,h(λ+ (i+ 0.5)·s+t,h−t0·t) dt
≤2ϵ
|ln(1−α)+t0|·/bracketleftig
c+Lλ,h/parenleftig
s
2−t∗−ϵ
|ln(1−α)+t0|/parenrightig/bracketrightig
cs+Lλ,h
2/parenleftbigs
2−t∗/parenrightbig2,
wheret∗=s
2+c
Lλ,h−ϵ
|ln(1−α)+t0|−/radicalbigg/parenleftig
c
Lλ,h−ϵ
|ln(1−α)+t0|/parenrightig2
+2cs
Lλ,h,Lλ,h=Lλ·M/parenleftig
h−h
|t0|,kh
h−h,|t0|Lh,1
|t0|/parenrightig
+
|t0|Lh·M/parenleftig
λ−λ,kλ
λ−λ,Lλ,1/parenrightig
, andc=khkλ
(h−h)·(λ−λ).
As for/integraltext
θ∈Syellowp(θ)dθ, we have
/integraldisplay
θ∈Syellowp(θ)dθ
≤M/parenleftbigg
h−h,kh
h−h,Lh,1/parenrightbigg
·M/parenleftbigg
λ−λ,kλ
λ−λ,Lλ,1/parenrightbigg
·/integraldisplay
θ∈Syellowdθ
=M/parenleftbigg
h−h,kh
h−h,Lh,1/parenrightbigg
·M/parenleftbigg
λ−λ,kλ
λ−λ,Lλ,1/parenrightbigg
·/parenleftbig
λ−λ/parenrightbig
|t0|s.
Above all, we can get that
Πϵ,ωΘ<sup
θ∈SgreenP(ˆg∗(θ′)∈[g(θ)−ϵ,g(θ) +ϵ]) +/integraldisplay
θ∈Syellowp(θ)dθ.
≤2ϵ
|ln(1−α)+t0|·/bracketleftig
c+Lλ,h/parenleftig
s
2−t∗−ϵ
|ln(1−α)+t0|/parenrightig/bracketrightig
cs+Lλ,h
2/parenleftbigs
2−t∗/parenrightbig2+
M/parenleftbigg
h−h,kh
h−h,Lh,1/parenrightbigg
·M/parenleftbigg
λ−λ,kλ
λ−λ,Lλ,1/parenrightbigg
·/parenleftbig
λ−λ/parenrightbig
|t0|s,
whereM(·,·,·,·),c,Lλ,h,t∗are defined as above.
39Under review as submission to TMLR
D.2.3 Proof of Lemma 2
Without loss of generality, we assume that f(x)≥f(x). Based on simple geometric analysis, we can get
that there are two patterns when supx∈[x,x]f(x)achieves supremum, which are shown in Fig. 10.
𝑥𝑓(𝑥)
𝑥𝑥𝑐!𝑐!+ℒ𝑥−𝑥
(a) Pattern 1.
𝑥𝑥𝑐+ℒ𝑥−𝑥′𝑥′𝑐𝑥𝑓(𝑥) (b) Pattern 2.
Figure 10: Two patterns when supx∈[x,x]f(x)achieves supremum.
For pattern 1, f(x) =c1≥c,f(x) =c1+L(x−x), and/integraltextx
xf(x)dx=/parenleftbig
c1+L
2(x−x)/parenrightbig
·(x−x) =A.
Therefore, when c≤A
x−x−L(x−x)
2, we have
sup
fsup
x∈[x,x]f(x) =c1+L(x−x) =A
x−x+L(x−x)
2.
For pattern 2, f(x) =c,f(x) =c+L(x−x′), wherex′∈(x,x], and/integraltextx
xf(x)dx=c(x−x)+L
2(x−x′)2=A.
Therefore, when c>A
x−x−L(x−x)
2, we have
sup
fsup
x∈[x,x]f(x) =c+L(x−x′) =c+/radicalbig
2L(A−c(x−x)).
Above all, we can get that
sup
x∈[x,x]f(x)≤sup
fsup
x∈[x,x]f(x)
=/braceleftiggA
x−x+L(x−x)
2, ifc≤A
x−x−L(x−x)
2
c+/radicalbig
2L(A−c(x−x)),ifc>A
x−x−L(x−x)
2.
E Discrete Distribution with Secret = Mean
Here, we consider three typical examples of discrete distributions: geometric distributions, binomial distri-
butions, and Poisson distributions with parameter θ. More specifically, the original distribution is
P(Xθ=k) =

(1−θ)kθ (geometric distribution)/parenleftbign
k/parenrightbig
θk(1−θ)n−k(binomial distribution)
θke−θ
k!(Poisson distribution)
wherenstandards for the number of trials in binomial distribution. The support of the parameter is
Supp (Θ) =/braceleftbig
Xθ:θ∈/parenleftbig
θ,θ/bracketrightbig/bracerightbig
where/parenleftbig
θ,θ/bracketrightbig
⊆(0,1)for geometric distribution and binomial distribution, and/parenleftbig
θ,θ/bracketrightbig
⊆(0,∞)for Poisson distribution.
We first analyze the lower bound.
40Under review as submission to TMLR
Corollary 3 (Privacy lower bound, secret = mean of a discrete distribution) .Consider the secret function
g(θ) =/summationtext
xxfXθ(x). For anyT∈(0,1), when Πϵ,ωΘ≤T, we have ∆>/parenleftbig
⌈1
T⌉−1/parenrightbig
·2γϵ, where the value of
γdepends on the type of the distributions:
•Geometric:
γ= inf
θ<θ1<θ2≤θ(1−θ2)h(θ1,θ2)−(1−θ1)h(θ1,θ2)
2/parenleftig
1
θ2−1
θ1/parenrightig ,
whereh(θ1,θ2) =⌊log(θ2)−log(θ1)
log(1−θ1)−log(1−θ2)⌋+ 1.
•Binomial:
γ= inf
θ<θ1<θ2≤θ
I1−θ2(n−h(θ1,θ2),1+h(θ1,θ2))−I1−θ1(n−h(θ1,θ2),1+h(θ1,θ2))
2n(θ1−θ2),
whereh(θ1,θ2) =⌊k′⌋,k′=nln/parenleftig
1−θ2
1−θ1/parenrightig/slashig
ln/parenleftig
θ1(1−θ2)
θ2(1−θ1)/parenrightig
, andIrepresents the regularized incomplete beta
function.
•Poisson:
γ= inf
θ<θ1<θ2≤θQ(h(θ1,θ2),θ2)−Q(h(θ1,θ2),θ1)
2 (θ1−θ2),
whereh(θ1,θ2) =⌊θ1−θ2
ln(θ1)−ln(θ2)⌋+ 1andQis the regularized gamma function.
The proof is in App. E.1. The above lower bounds can be computed numerically.
Since these distributions only have one parameter, we can use Alg. 1 and Alg. 3 to derive a data release
mechanism. The performance of greedy-based and dynamic-programming-based data release mechanisms
for each distribution is shown in Fig. 11.
0.002 0.004 0.006 0.008 0.010
Privacy0.00.10.20.30.40.50.6DistortionTheoretical lower bound
Dynamic-programming
Greedy
(a) Distribution = Geometric
0.002 0.004 0.006 0.008 0.010
Privacy0.20.40.60.8DistortionTheoretical lower bound
Dynamic-programming
Greedy (b) Distribution = Binomial
0.002 0.004 0.006 0.008 0.010
Privacy0.10.20.30.40.50.60.70.8DistortionTheoretical lower bound
Dynamic-programming
Greedy (c) Distribution = Poisson
Figure 11: Privacy-distortion performance of Alg. 1 and Alg. 3 for geometric, binomial and Poisson distri-
bution when secret = mean.
As we can observe, the distortion that dynamic-programming-based data release mechanism achieves it is
always smaller than or equal to that of the greedy-based data release mechanism.
E.1 Proof of Corollary 3
E.1.1 Geometric Distribution
Proof.LetXθ1andXθ2be two Geometric random variables with parameters θ1andθ2respectively. We
assume that θ1>θ2without loss of generality. Let k′satisfy (1−θ1)k′
θ1= (1−θ2)k′
θ2andk0=⌊k′⌋+ 1.
41Under review as submission to TMLR
Then we can get that
D(Xθ1,Xθ2) =1
2dTV/parenleftbig
ωXθ1∥ωXθ2/parenrightbig
=1
2(1−θ2)k0−1
2(1−θ1)k0,
R(Xθ1,Xθ2) =1
θ2−1
θ1.
Therefore, we have
γ= inf
θ<θ1<θ2≤θ(1−θ2)k0−(1−θ1)k0
2/parenleftig
1
θ2−1
θ1/parenrightig.
The rest follows from Thm. 1.
E.1.2 Binomial Distribution
Proof.LetXθ1andXθ2be two binomial random variables with parameters θ1andθ2respectively with fixed
number of trials n. We assume that θ1>θ2without loss of generality. Let k′satisfy/parenleftbign
k′/parenrightbig
θk′
1(1−θ1)n−k′
=/parenleftbign
k′/parenrightbig
θk′
2(1−θ1)n−k′
andk0=⌊k′⌋. We can get that
D(Xθ1,Xθ2) =1
2dTV/parenleftbig
ωXθ1∥ωXθ2/parenrightbig
=1
2I1−θ2(n−k0,1 +k0)−1
2I1−θ1(n−k0,1 +k0),
R(Xθ1,Xθ2) =n(θ1−θ2),
whereIrepresents the regularized incomplete beta function.
Therefore, we have
γ= inf
θ<θ1<θ2≤θI1−θ2(n−k0,1 +k0)−I1−θ1(n−k0,1 +k0)
2n(θ1−θ2).
The rest follows from Thm. 1.
E.1.3 Poisson Distribution
Proof.LetXθ1andXθ2be two Poisson random variables with parameters θ1andθ2respectively. We assume
thatθ1>θ2without loss of generality. Let k′satisfyθk′
1e−θ1=θk′
2e−θ2andk0=⌊k′⌋+ 1. Then we can get
that
D(Xθ1,Xθ2) =1
2dTV/parenleftbig
ωXθ1∥ωXθ2/parenrightbig
=1
2Q(k0,θ2)−1
2Q(k0,θ1),
R(Xθ1,Xθ2) =θ1−θ2,
whereQis the regularized gamma function.
Therefore, we have
γ= inf
θ<θ1<θ2≤θQ(k0,θ2)−Q(k0,θ1)
2 (θ1−θ2).
The rest follows from Thm. 1.
42Under review as submission to TMLR
F More Distributions with Secret = Quantiles
In this section, we discuss how to protect the quantiles for typical examples of continuous distributions:
Gaussian distributions and uniform distributions. In our analysis, their parameters are denoted by:
•Gaussian distributions: θ= (µ,σ), whereµ,σare the mean and the standard deviation of the Gaussian
distribution.
•Uniform distributions: θ= (m,n), wherem,ndenote the lower and upper bound of the uniform
distribution. In other words, Xm,nis a random variable from uniform distribution U ([m,n]).
As before, we first present the lower bound.
Corollary 4 (Privacy lower bound, secret = α-quantile of a continuous distribution) .Consider the secret
functiong(θ) =α-quantile of fXθ. For anyT∈(0,1), when Πϵ,ωΘ≤T, we have ∆>/parenleftbig
⌈1
T⌉−1/parenrightbig
·2γϵ, where
the value of γdepends on the type of the distributions:
•Gaussian:
γ= min
t/radicalig
1
2πe−1
2t2−t/parenleftbig1
2−Φ (t)/parenrightbig
|t+Qα|,
where Φdenotes the CDF of the standard Gaussian distribution and Qα≜Φ−1(α).
•Uniform:
γ=

/radicalig
α2−α+1
2+α−1
2α≤0.5/radicalig
α2−α+1
2−α+1
2α>0.5.
The proof is in App. F.1. The bound for uniform is in closed form, while the bound for Gaussian can be
computed numerically.
Next, we provide data release mechanisms for each of the distributions. Here, we assume that the parameters
of the original data are drawn from a uniform distribution with lower and upper bounds. In more details,
we make the following assumptions.
Assumption 5.The prior over distribution parameters as specified below.
•Gaussian: (μ,σ)follows the uniform distribution over/braceleftig
(a,b)|a∈/bracketleftbig
µ,µ/parenrightbig
,b∈[σ,σ)/bracerightig
.
•Uniform: (M,N )follows the uniform distribution over/braceleftbig
(a,b)|a∈[m,m),b∈[m,m),a<b/bracerightbig
.
Mechanism 3(For secret = quantile of a continuous distribution) .We design mechanisms for each of the
distributions.
•Gaussian:
Sµ,i=/braceleftig
(µ+t0·t,σ+ (i+ 0.5)·s+t)|t∈/bracketleftig
−s
2,s
2/parenrightig/bracerightig
,
θ∗
µ,i= (µ,σ+ (i+ 0.5)·s),
I={(µ,i) :i∈N,µ∈R},
wheresis a hyper-parameter of the mechanism that divides (σ−σ)and
t0= arg min
t/radicalig
1
2πe−1
2t2−t/parenleftbig1
2−Φ (t)/parenrightbig
|t+Qα|.
.
•Uniform:
Sm,i=/braceleftig
(m−t0·t,m+ (i+ 0.5)·s+t)|t∈/parenleftig
−s
2(t0+1),s
2(t0+1)/bracketrightig/bracerightig
,
θ∗
m,i= (m,m + (i+ 0.5)·s),
I={(m,i)|i∈Z>0,m∈R},
43Under review as submission to TMLR
wheret0=1
1
l−1for
l=

α+/radicalig
α2−α+1
2α≤0.5
α−/radicalig
α2−α+1
2α>0.5.
ands>0is a hyper-parameter of the mechanism that divides (m−m).
These data release mechanisms achieve the following ∆andΠϵ,ωΘ.
Proposition 6. Under Asm. 5, Mech. 3 has the following ∆andΠϵ,ωΘvalue/bound.
•Gaussian:
Πϵ,ωΘ<2ϵ
|t0+Qα|s+|t0|s
µ−µ,
∆ =s
2/radicalbigg
2
πe−1
2t2
0−t0s
2(1−2Φ (t0))</parenleftigg
2 +|t0|·|t0+Qα|s2
/parenleftbig
µ−µ/parenrightbig
ϵ/parenrightigg
∆opt.
Under the “high-precision” regime wheres2
µ−µ→0ass,(µ−µ)→∞,∆satisfies
lim sup
s2
µ−µ→0∆<3∆opt.
•Uniform:
Πϵ,ωΘ<2ϵ(t0+ 1)
|(1−α)t0−α|s+2s·t0
(t0+ 1) (m−m)+s2
2 (m−m)2,
∆ =/parenleftbig
t2
0+ 1/parenrightbig
s
4(t0+ 1)2
</parenleftigg
2 +|(1−α)t0−α|s
ϵ(t0+ 1)·/parenleftigg
2s·t0
(t0+ 1) (m−m)+s2
2 (m−m)2/parenrightigg/parenrightigg
∆opt.
Under the “high-precision” regime wheres2
m−m→0ass,(m−m)→∞,∆satisfies
lim sup
s2
m−m→0∆<3∆opt.
Thet0parameter is defined in Mech. 3 for each distribution.
The proof is in App. F.2. For Gaussian distribution, we relax Asm. 5 and analyze the privacy-distortion
performance of Mech. 3 in App. F.3. For both distributions, we consider the “high-precision” regime. The
two takeaways are that: (1) data holder can use sto control the trade-off between distortion and privacy,
and (2) the mechanism is order-optimal with multiplicative factor 3.
F.1 Proof of Corollary 4
F.1.1 Gaussian Distribution
Proof.LetXµ1,σ2,Xµ2,σ2betwoGaussianrandomvariableswithmeans µ1,µ2andsigmas σ1,σ2respectively.
LetΦdenotes the CDF of the standard Gaussian distribution and let Φ−1(α)≜Qα.
Whenσ1=σ2, we have
D(Xµ1,σ1,Xµ2,σ2)
R(Xµ1,σ1,Xµ2,σ2)=1
2|µ1−µ2|
|µ1+σQα−(µ2+σQα)|=1
2.
44Under review as submission to TMLR
Whenσ1̸=σ2, we assume σ2> σ1without loss of generality. Let a=σ1
σ2andb=σ2
σ1µ1−µ2. Leta=σ1
σ2andb=σ2
σ1µ1−µ2. We can get that fXµ1,σ1(x) =afXµ2,σ2(a(x+b)), and
D(Xµ1,σ1,Xµ2,σ2) =1
2dWasserstein-1/parenleftbig
ωXµ1,σ1∥ωXµ2,σ2/parenrightbig
=1
2/integraldisplay+∞
−∞/vextendsingle/vextendsingle/vextendsinglex−/parenleftigx
a−b/parenrightig/vextendsingle/vextendsingle/vextendsinglefXµ1,σ1(x) dx
= (µ1−µ2)/parenleftbigg
Φ/parenleftbiggµ1−µ2
σ2−σ1/parenrightbigg
−1
2/parenrightbigg
+/radicalbigg
1
2π(σ2−σ1)e−1
2/parenleftbigµ1−µ2
σ2−σ1/parenrightbig2
, (22)
R(Xµ1,σ1,Xµ2,σ2) =|µ1+σ1Qα−(µ2+σ2Qα)|
=|(µ1−µ2) + (σ1−σ2)Qα|.
Letµ1−µ2
σ1−σ2≜t, we can get that
D(Xµ1,σ1,Xµ2,σ2)
R(Xµ1,σ1,Xµ2,σ2)=/radicalig
1
2πe−1
2t2−t/parenleftbig1
2−Φ (t)/parenrightbig
|t+Qα|≜h(t).
Since limt→∞=1
2, we have min/braceleftbig
minth(t),1
2/bracerightbig
= minth(t), and therefore we can get that
γ= min
th(t).
F.1.2 Uniform Distribution
Proof.LetXm1,n1,Xm2,n2be two uniform random variables. Let FXm1,n1,FXm2,n2be their CDFs, and let
m2≥m1without loss of generality. We can get that
D(Xm1,n1,Xm2,n2) =1
2dWasserstein-1/parenleftbig
ωXm1,n1∥ωXm2,n2/parenrightbig
=1
2/integraldisplay+∞
−∞|FXm1,n1(x)−FXm2,n2(x)|dx
=/braceleftiggm2−m1+n2−n1
4n2≥n1
(m2−m1)2+(n1−n2)2
4(m2−m1+(n1−n2))n2<n1, (23)
R(Xm1,n1,Xm2,n2) =|m2+α(n2−m2)−[m1+α(n1−m1)]|
=|(1−α) (m2−m1) +α(n2−n1)|.
Whenn2=n1, we have
D(Xm1,n1,Xm2,n2)
R(Xm1,n1,Xm2,n2)=m2−m1
4 (1−α) (m2−m1)=1
4 (1−α).
45Under review as submission to TMLR
Whenn2>n1, lett1=m2−m1
n2−n1∈[0,+∞), we have
D(Xm1,n1,Xm2,n2)
R(Xm1,n1,Xm2,n2)=1
4m2−m1+n2−n1
(1−α) (m2−m1) +α(n2−n1)
=1
4t1+ 1
(1−α)t1+α
=1
4 (1−α)/parenleftigg
1 +1−2α
1−α·1
t1+α
1−α/parenrightigg
≥/braceleftigg
1
4(1−α)α≤0.5
1
4αα>0.5.
Whenn2<n1, lett2=m2−m1
n1−n2∈(0,+∞), we have
D(Xm1,n1,Xm2,n2)
R(Xm1,n1,Xm2,n2)=1
4(m2−m1)2+ (n1−n2)2
(m2−m1+ (n1−n2))·
1
|(1−α) (m2−m1)−α(n1−n2)|
=1
4t2
2+ 1
(t2+ 1)|(1−α)t2−α|
≥

/radicalig
α2−α+1
2+α−1
2α≤0.5/radicalig
α2−α+1
2−α+1
2α>0.5.
“=” achieves when t2=1
1
l−1≜t0, where
l=

α+/radicalig
α2−α+1
2α≤0.5
α−/radicalig
α2−α+1
2α>0.5.
Therefore we can get that
γ=

/radicalig
α2−α+1
2+α−1
2α≤0.5/radicalig
α2−α+1
2−α+1
2α>0.5.
F.2 Proof of Prop. 6
F.2.1 Gaussian Distribution
Proof.We first focus on the proof for Πϵ,ωΘ.
In Fig. 12, we separate the space of possible data parameters into two regions represented by yellow and
green colors. The yellow regions Syellowconstitute right triangles with height sand width|t0|s. The green
regionSgreenis the rest of the parameter space. The high-level idea of our proof is as follows. Note that for
any parameter θ∈Sgreen, there exists aSµ,is.t.θ∈Sµ,iandSµ,i⊂Sgreen. Therefore, we can bound the
attack success rate if θ∈Sgreen. At the same time, the probability of θ∈Syellowis bounded. Therefore, we
can bound the overall attacker’s success rate (i.e., Πϵ,ωΘ). More specifically, let the optimal attacker be ˆg∗.
46Under review as submission to TMLR
0110
10…222|4)|2|4)|2…Thespaceofpossibleparameters
Figure 12: The construction for proof of Prop. 6 for Gaussian distributions. We separate the space of
possible parameters into two regions (yellow and green) and bound the attacker’s success rate on each region
separately.
We have
Πϵ,ωΘ=P(ˆg∗(θ′)∈[g(θ)−ϵ,g(θ) +ϵ])
=/integraldisplay
θ∈Sgreenp(θ)P(ˆg∗(θ′)∈[g(θ)−ϵ,g(θ) +ϵ])dθ
+/integraldisplay
θ∈Syellowp(θ)P(ˆg∗(θ′)∈[g(θ)−ϵ,g(θ) +ϵ])dθ
<2ϵ
|t0+Qα|s+|t0|s
µ−µ.
For the distortion, it is straightforward to get that ∆ =s
2/radicalig
2
πe−1
2t2
0−t0s
2(1−2Φ (t0))from Eq. (22), and
∆opt>/parenleftig
⌈1
Πϵ,ωΘ⌉−1/parenrightig
·2γϵ≥2γϵ, whereγisdefinedinCorollary4. Wecangetthat/parenleftig
Πϵ,ωΘ−|t0|s
µ−µ/parenrightig
·∆<2γϵ
47Under review as submission to TMLR
and
∆ = ∆ opt+ ∆−∆opt
<∆opt+ ∆−/parenleftbigg
⌈1
Πϵ,ωΘ⌉−1/parenrightbigg
·2γϵ
≤∆opt+ 2γϵ+ ∆−2γϵ
Πϵ,ωΘ
<∆opt+ 2γϵ+|t0|s
µ−µ
2ϵ
|t0+Qα|s+|t0|s
µ−µ·∆
=/parenleftigg
1 +|t0|·|t0+Qα|s2
2ϵ/parenleftbig
µ−µ/parenrightbig/parenrightigg
(∆opt+ 2γϵ)
≤/parenleftigg
2 +|t0|·|t0+Qα|s2
ϵ/parenleftbig
µ−µ/parenrightbig/parenrightigg
∆opt.
F.2.2 Uniform Distribution
Proof.We first focus on the proof for Πϵ,ωΘ.
5665
65
23%3%4"2…Thespaceofpossibleparameters22223%3%4"
Figure 13: The construction for proof of Prop. 6 for uniform distributions. We separate the space of
possible parameters into two regions (yellow and green) and bound the attacker’s success rate on each region
separately.
In Fig. 13, we separate the space of possible data parameters into two regions represented by yellow and
green colors. The yellow regions Syellowconstitute triangles with heightst0
t0+1and widths(except for the
48Under review as submission to TMLR
right-bottom triangle with height and width s). The green region Sgreenis the rest of the parameter space.
The high-level idea of our proof is as follows. Note that for any parameter θ∈Sgreen, there exists aSµ,i
s.t.θ∈Sµ,iandSµ,i⊂Sgreen. Therefore, we can bound the attack success rate if θ∈Sgreen. At the same
time, the probability of θ∈Syellowis bounded. Therefore, we can bound the overall attacker’s success rate
(i.e., Πϵ,ωΘ). More specifically, let the optimal attacker be ˆg∗. We have
Πϵ,ωΘ=P(ˆg∗(θ′)∈[g(θ)−ϵ,g(θ) +ϵ])
=/integraldisplay
θ∈Sgreenp(θ)P(ˆg∗(θ′)∈[g(θ)−ϵ,g(θ) +ϵ])dθ
+/integraldisplay
θ∈Syellowp(θ)P(ˆg∗(θ′)∈[g(θ)−ϵ,g(θ) +ϵ])dθ
<2ϵ(t0+ 1)
|(1−α)t0−α|s+2s·t0
(t0+ 1) (m−m)+s2
2 (m−m)2.
Thesecondterm2s·t0
(t0+1)(m−m)boundstheprobabilityoftheyellowregionexceptfortheright-bottomtriangle,
and the last terms2
2(m−m)2is the probability of the right-bottom triangle.
For the distortion, it is straightforward to get that ∆ =(t2
0+1)s
4(t0+1)2from Eq. (23), and ∆opt>/parenleftig
⌈1
Πϵ,ωΘ⌉−1/parenrightig
·
2γϵ≥2γϵ, whereγis defined in Corollary 4. We can get that/parenleftig
Πϵ,ωΘ−2s·t0
(t0+1)(m−m)−s2
2(m−m)2/parenrightig
·∆<2γϵ
and
∆ = ∆ opt+ ∆−∆opt
<∆opt+ ∆−/parenleftbigg
⌈1
Πϵ,ωΘ⌉−1/parenrightbigg
·2γϵ
≤∆opt+ 2γϵ+ ∆−2γϵ
Πϵ,ωΘ
<∆opt+ 2γϵ+2s·t0
(t0+1)(m−m)+s2
2(m−m)2
2ϵ(t0+1)
|(1−α)t0−α|s+2s·t0
(t0+1)(m−m)+s2
2(m−m)2·∆
=/parenleftbigg
1 +|(1−α)t0−α|s
2ϵ(t0+ 1)/parenleftbigg
2s·t0
(t0+ 1) (m−m)+s2
2 (m−m)2/parenrightbigg/parenrightbigg
(∆opt+ 2γϵ)
≤/parenleftbigg
2 +|(1−α)t0−α|s
ϵ(t0+ 1)·/parenleftbigg
2s·t0
(t0+ 1) (m−m)+s2
2 (m−m)2/parenrightbigg/parenrightbigg
∆opt.
Whens2
m−m→0ass,(m−m)→∞, wecangetthats3
(m−m)2→0. Therefore, inthiscase, lim sup s2
m−m→0∆<
3∆opt.
F.3 Privacy-Distortion Performance of Mech. 3 with Relaxed Assumption
For Gaussian distribution, we relax Asm. 5 as follows.
Assumption 6.The prior over Gaussian distribution parameters satisfies Supp (μ,σ) =/braceleftbig
(a,b)|a∈/bracketleftbig
µ,µ/parenrightbig
,b∈[σ,σ)/bracerightbig
,fμ,σ(a,b) =fμ(a)·fσ(b), andfμ(a)(resp.fσ(b)) isLµ-Lipschitz
(resp.Lσ-Lipschitz) and has lower boundkµ
µ−µwithkµ∈(0,1](resp.kσ
σ−σwithkσ∈(0,1]).
Based on Asm. 6, the Privacy-distortion performance of Mech. 3 is shown below.
49Under review as submission to TMLR
Proposition 7. Under Asm. 6, Mech. 3 has the following ∆andΠϵ,ωΘvalue/bound:
∆ =s
2/radicalbigg
2
πe−1
2t2
0−t0s
2(1−2Φ (t0)),
Πϵ,ωΘ<2ϵ
|t0+Qα|·/bracketleftig
c+Lµ,σ/parenleftig
s
2−t∗−ϵ
|t0+Qα|/parenrightig/bracketrightig
cs+Lµ,σ
2/parenleftbigs
2−t∗/parenrightbig2+
M/parenleftbigg
µ−µ,kµ
µ−µ,Lµ,1/parenrightbigg
·M/parenleftbigg
σ−σ,kσ
σ−σ,Lσ,1/parenrightbigg
·(σ−σ)|t0|s,
wherec=kµkσ
(µ−µ)·(σ−σ), functionMsatisfies
M(x,c,L,A) =/braceleftigg
A
x+Lx
2, ifc≤A
x−Lx
2
c+/radicalbig
2L(A−cx),ifc>A
x−Lx
2,
Lµ,σ=Lσ·M/parenleftigµ−µ
|t0|,kµ
µ−µ,|t0|Lµ,1
|t0|/parenrightig
+|t0|Lµ·M/parenleftig
σ−σ,kσ
σ−σ,Lσ,1/parenrightig
, andt∗=s
2+c
Lµ,σ−ϵ
|t0+Qα|−
/radicalbigg/parenleftig
c
Lµ,σ−ϵ
|t0+Qα|/parenrightig2
+2cs
Lµ,σ.
Proof.It is straightforward to get the formula for ∆from Eq. (22). Here we focus on the proof for Πϵ,ωΘ.
Similar to App. D.2.2, based on Lemma 1 and Lemma 2, we can get that
sup
θ∈SgreenP(ˆg∗(θ′)∈[g(θ)−ϵ,g(θ) +ϵ])
= sup
i∈N,µ,t′∈R/integraltextmin/braceleftbig
s
2,t′+2ϵ
|t0+Qα|/bracerightbig
max{−s
2,t′}fμ,σ(µ+t0·t,σ+ (i+ 0.5)·s+t) dt
/integraltexts
2
−s
2fμ,σ(µ+t0·t,σ+ (i+ 0.5)·s+t) dt
≤2ϵ
|t0+Qα|·/bracketleftig
c+Lµ,σ/parenleftig
s
2−t∗−ϵ
|t0+Qα|/parenrightig/bracketrightig
cs+Lµ,σ
2/parenleftbigs
2−t∗/parenrightbig2,
wheret∗=s
2+c
Lµ,σ−ϵ
|t0+Qα|−/radicalbigg/parenleftig
c
Lµ,σ−ϵ
|t0+Qα|/parenrightig2
+2cs
Lµ,σ,Lµ,σ=Lσ·M/parenleftigµ−µ
|t0|,kµ
µ−µ,|t0|Lµ,1
|t0|/parenrightig
+|t0|Lµ·
M/parenleftig
σ−σ,kσ
σ−σ,Lσ,1/parenrightig
, andc=kµkσ
(µ−µ)·(σ−σ).
As for/integraltext
θ∈Syellowp(θ)dθ, we have
/integraldisplay
θ∈Syellowp(θ)dθ
≤M/parenleftbigg
µ−µ,kµ
µ−µ,Lµ,1/parenrightbigg
·M/parenleftbigg
σ−σ,kσ
σ−σ,Lσ,1/parenrightbigg
·/integraldisplay
θ∈Syellowdθ
=M/parenleftbigg
µ−µ,kµ
µ−µ,Lµ,1/parenrightbigg
·M/parenleftbigg
σ−σ,kσ
σ−σ,Lσ,1/parenrightbigg
·(σ−σ)|t0|s.
50Under review as submission to TMLR
Above all, we can get that
Πϵ,ωΘ<sup
θ∈SgreenP(ˆg∗(θ′)∈[g(θ)−ϵ,g(θ) +ϵ]) +/integraldisplay
θ∈Syellowp(θ)dθ.
≤2ϵ
|t0+Qα|·/bracketleftig
c+Lµ,σ/parenleftig
s
2−t∗−ϵ
|t0+Qα|/parenrightig/bracketrightig
cs+Lµ,σ
2/parenleftbigs
2−t∗/parenrightbig2+
M/parenleftbigg
µ−µ,kµ
µ−µ,Lµ,1/parenrightbigg
·M/parenleftbigg
σ−σ,kσ
σ−σ,Lσ,1/parenrightbigg
·(σ−σ)|t0|s,
whereM(·,·,·,·),c,Lµ,σ,t∗are defined as above.
G Case Study with Secret = Standard Deviation
In this section, we discuss how to protect standard deviation for several continuous and discrete distributions.
G.1 Continuous Distributions
Weconsiderthesamedistributionsdiscussedin§6.2andApp.F:Gaussian,uniform,and(shifted)exponential
distributions.
Corollary 5 (Privacy lower bound, secret = standard deviation of a continuous distribution) .Consider
the secret function g(θ) =standard deviation of fXθ. For any T∈(0,1), when Πϵ,ωΘ≤T, we have
∆>/parenleftbig
⌈1
T⌉−1/parenrightbig
·2γϵ, where the value of γdepends on the type of the distributions:
•Gaussian:
γ= min
t/radicalbigg
1
2πe−1
2t2−t/parenleftbigg1
2−Φ (t)/parenrightbigg
,
where Φdenotes the CDF of the standard Gaussian distribution.
•Uniform:γ=√
3
4.
•Exponential: γ=1
2.
•Shifted exponential: γ=ln 2
2.
The proof is in App. G.3. The bounds for Gaussian can be computed numerically, while the bounds for all
other distributions are in closed form.
Next, wepresentthedatareleasemechanismforthesedistributionsandthesecretunderthesameassumption
as Asm. 2.
Mechanism 4(For secret = standard deviation of a continuous distribution) .We design mechanisms for
each of the distributions.
•Gaussian:
Sµ,i=/braceleftig
(µ+t0·t,σ+ (i+ 0.5)·s+t)|t∈/bracketleftig
−s
2,s
2/parenrightig/bracerightig
,
θ∗
µ,i= (µ,σ+ (i+ 0.5)·s),
I={(µ,i)|i∈N,µ∈R},
wheresis a hyper-parameter of the mechanism that divides (σ−σ)and
t0= arg min
t/radicalbigg
1
2πe−1
2t2−t/parenleftbigg1
2−Φ (t)/parenrightbigg
.
.
51Under review as submission to TMLR
•Uniform:
Sm,i=/braceleftbig
(m−t,m+ (i+ 0.5)·s+t)|t∈/parenleftbig
−s
4,s
4/bracketrightbig/bracerightbig,
θ∗
m,i= (m,m + (i+ 0.5)·s),
I={(m,i)|i∈Z>0,m∈R},
wheres>0is a hyper-parameter of the mechanism that divides (m−m).
•Exponential:
Si= [λ+i·s,λ+ (i+ 1)·s),
θ∗
i=λ+ (i+ 0.5)·s ,
I=N,
wheres>0is a hyper-parameter of the mechanism that divides/parenleftbig
λ−λ/parenrightbig
.
•Shifted exponential:
Si,h=/braceleftig
(λ+ (i+ 0.5)s+t,h−ln 2·t)|t∈/bracketleftig
−s
2,s
2/parenrightig/bracerightig
,
θ∗
i,h= (λ+ (i+ 0.5)s,h),
I={(i,h)|i∈N,h∈R},
wheres>0is a hyper-parameter of the mechanism that divides/parenleftbig
λ−λ/parenrightbig
.
These data release mechanisms achieve the following ∆andΠϵ,ωΘ.
Proposition 8. Under Asm. 2, Mech. 4 has the following ∆andΠϵ,ωΘvalue/bound.
•Gaussian:
Πϵ,ωΘ<2ϵ
s+|t0|s
µ−µ,
∆ =s
2/radicalbigg
2
πe−1
2t2
0−t0s
2(1−2Φ (t0))</parenleftigg
2 +|t0|s2
/parenleftbig
µ−µ/parenrightbig
ϵ/parenrightigg
∆opt,
wheret0is defined in Mech. 4. Under the “high-precision” regime wheres2
µ−µ→0ass,(µ−µ)→∞,∆
satisfies
lim sup
s2
µ−µ→0∆<3∆opt.
•Uniform:
Πϵ,ωΘ<4√
3ϵ
s+s
(m−m)+s2
2 (m−m)2,
∆ =s
8</parenleftigg
2 +s
2√
3ϵ·/parenleftigg
s
m−m+s2
2 (m−m)2/parenrightigg/parenrightigg
∆opt.
Under the “high-precision” regime wheres2
m−m→0ass,(m−m)→∞,∆satisfies
lim sup
s2
m−m→0∆<3∆opt.
•Exponential:
Πϵ,ωΘ=2ϵ
s,
∆ =1
2s<2∆opt.
52Under review as submission to TMLR
•Shifted exponential:
Πϵ,ωΘ<2ϵ
s+sln 2
h−h,
∆ =sln 2
2</parenleftigg
2 +s2ln 2
ϵ/parenleftbig
h−h/parenrightbig/parenrightigg
∆opt.
Under the “high-precision” regime wheres2
h−h→0ass,(h−h)→∞,∆satisfies
lim sup
s2
h−h→0∆<3∆opt.
The proof is in App. G.4. For Gaussian, exponential and shifted exponential distributions, we relax Asm. 2
and analyze the privacy-distortion performance of Mech. 4 in App. G.5. From these propositions, we have
similar takeaways as the alpha-quantile case ( §6.2): (1) data holder can use sto control the trade-off between
distortion and privacy, and (2) the mechanism is order-optimal under the “high-precision” regime.
G.2 Discrete Distributions
Here, we consider the same discrete distributions studied in App. E: Geometric distributions, binomial
distributions, and Poisson distributions. We first analyze the lower bound.
Corollary 6 (Privacy lower bound, secret = standard deviation of a discrete distribution) .Consider the
secret function g(θ) =standard deviation of fXθ. For any T∈(0,1), when Πϵ,ωΘ≤T, we have ∆>/parenleftbig
⌈1
T⌉−1/parenrightbig
·2γϵ, where the value of γdepends on the type of the distributions:
•Geometric:
γ= inf
θ<θ1<θ2≤θ(1−θ2)h(θ1,θ2)−(1−θ1)h(θ1,θ2)
2/parenleftig√1−θ2
θ2−√1−θ1
θ1/parenrightig,
whereh(θ1,θ2) =⌊log(θ2)−log(θ1)
log(1−θ1)−log(1−θ2)⌋+ 1.
•Binomial:
γ= inf
θ<θ1<θ2≤θ
I1−θ2(n−h(θ1,θ2),1+h(θ1,θ2))−I1−θ1(n−h(θ1,θ2),1+h(θ1,θ2))
2/vextendsingle/vextendsingle√
nθ2(1−θ2)−√
nθ1(1−θ1)/vextendsingle/vextendsingle,
whereh(θ1,θ2) =⌊k′⌋,k′=nln/parenleftig
1−θ2
1−θ1/parenrightig/slashig
ln/parenleftig
θ1(1−θ2)
θ2(1−θ1)/parenrightig
, andIrepresents the regularized incomplete beta
function.
•Poisson:
γ= inf
θ<θ1<θ2≤θQ(h(θ1,θ2),θ2)−Q(h(θ1,θ2),θ1)
2/parenleftbig√θ1−√θ2/parenrightbig ,
whereh(θ1,θ2) =⌊θ1−θ2
ln(θ1)−ln(θ2)⌋+ 1andQis the regularized gamma function.
The proof is in App. G.6. The above lower bounds can be computed numerically.
Since these distributions only have one parameter, we can use Alg. 1 and Alg. 3 to derive a data release
mechanism. The performance of greedy-based and dynamic-programming-based data release mechanisms
for each distribution is shown in Fig. 14.
53Under review as submission to TMLR
0.002 0.004 0.006 0.008 0.010
Privacy0.00.10.20.30.40.50.6DistortionTheoretical lower bound
Dynamic-programming
Greedy
(a) Distribution = Geometric
0.05 0.10 0.15 0.20
Privacy0.10.20.30.40.5DistortionTheoretical lower bound
Dynamic-programming
Greedy (b) Distribution = Binomial
0.05 0.10 0.15 0.20
Privacy0.00.20.40.60.8DistortionTheoretical lower bound
Dynamic-programming
Greedy (c) Distribution = Poisson
Figure 14: Privacy-distortion performance of Alg. 1 and Alg. 3 for binomial and Poisson distribution when
secret = standard deviation.
G.3 Proof of Corollary 5
G.3.1 Gaussian Distribution
Proof.LetXµ1,σ2,Xµ2,σ2betwoGaussianrandomvariableswithmeans µ1,µ2andsigmas σ1,σ2respectively,
whereσ1̸=σ2. Let Φdenotes the CDF of the standard Gaussian distribution. We can get that
D(Xµ1,σ1,Xµ2,σ2) = (µ1−µ2)/parenleftbigg
Φ/parenleftbiggµ1−µ2
σ2−σ1/parenrightbigg
−1
2/parenrightbigg
+/radicalbigg
1
2π(σ2−σ1)e−1
2/parenleftbigµ1−µ2
σ2−σ1/parenrightbig2
,
R(Xµ1,σ1,Xµ2,σ2) =|σ1−σ2|.
Letµ1−µ2
σ1−σ2≜t, we can get that
D(Xµ1,σ1,Xµ2,σ2)
R(Xµ1,σ1,Xµ2,σ2)=/radicalbigg
1
2πe−1
2t2−t/parenleftbigg1
2−Φ (t)/parenrightbigg
≜h(t).
Therefore we can get that
γ= min
th(t).
G.3.2 Uniform Distribution
Proof.LetXm1,n1,Xm2,n2be two uniform random variables. Let FXm1,n1,FXm2,n2be their CDFs, and let
m2≥m1without loss of generality. We can get that
D(Xm1,n1,Xm2,n2) =1
2dWasserstein-1/parenleftbig
ωXm1,n1∥ωXm2,n2/parenrightbig
=1
2/integraldisplay+∞
−∞|FXm1,n1(x)−FXm2,n2(x)|dx
=/braceleftiggm2−m1+n2−n1
4n2≥n1
(m2−m1)2+(n1−n2)2
4(m2−m1+(n1−n2))n2<n1,
R(Xm1,n1,Xm2,n2) =/vextendsingle/vextendsingle/vextendsingle/vextendsingle1√
12(n1−m1)−1√
12(n2−m2)/vextendsingle/vextendsingle/vextendsingle/vextendsingle
=1√
12|m2−m1−(n2−n1)|.
54Under review as submission to TMLR
Therefore, we can get that when n2≥n1, we have
D(Xm1,n1,Xm2,n2)
R(Xm1,n1,Xm2,n2)=√
3
2m2−m1+n2−n1
|m2−m1−(n2−n1)|
≥√
3
2.
Whenn2<n1, we have
D(Xm1,n1,Xm2,n2)
R(Xm1,n1,Xm2,n2)=√
3
2(m2−m1)2+ (n1−n2)2
(m2−m1+ (n1−n2))2
=√
3
2(m2−m1)2+ (n1−n2)2
(m2−m1)2+ (n1−n2)2+ 2 (m2−m1) (n1−n2)
≥√
3
2·(m2−m1)2+ (n1−n2)2
2/bracketleftig
(m2−m1)2+ (n1−n2)2/bracketrightig
=√
3
4.
Therefore we can get that
γ=√
3
4.
G.3.3 Exponential Distribution
Proof.LetXλ1,Xλ2be two exponential random variables. We have
D(Xλ1,Xλ2)
R(Xλ1,Xλ2)=1
λ1−1
λ2
2/parenleftig
1
λ1−1
λ2/parenrightig=1
2.
Therefore we can get that
γ=1
2.
G.3.4 Shifted Exponential Distribution
Proof.LetXλ1,h1,Xλ2,h2be random variables from shifted exponential distributions. Let λ2≤λ1without
loss of generality. Let a=λ1
λ2andb= (h1/λ1−h2/λ2)λ2. We can get that fXλ1,h1(x) =afXλ2,h2(a(x+b)),
and
D(Xλ1,h1,Xλ2,h2) =1
2dWasserstein-1/parenleftbig
ωXλ1,h1∥ωXλ2,h2/parenrightbig
=1
2/integraldisplay+∞
h1/vextendsingle/vextendsingle/vextendsinglex−/parenleftigx
a−b/parenrightig/vextendsingle/vextendsingle/vextendsinglefXλ1,h1(x) dx
=λ2
2λ1/integraldisplay+∞
h1|(1/λ2−1/λ1)x+h1/λ1−h2/λ2|e−1
λ1(x−h1)dx
=/braceleftigg
1
2(h2−h1+λ2−λ1)−eh2−h1
λ2−λ1(λ2−λ1) (h1<h2)
1
2(h1−h2+λ1−λ2) ( h1≥h2),
R(Xλ1,h1,Xλ2,h2) =λ1−λ2. (24)
55Under review as submission to TMLR
Whenλ1=λ2andh1̸=h2, we haveD(Xλ1,h1,Xλ2,h2)
R(Xλ1,h1,Xλ2,h2)=∞.
Whenλ1̸=λ2andh1<h2, lett=h2−h1
λ1−λ2∈(0,+∞). We have
D(Xλ1,h1,Xλ2,h2)
R(Xλ1,h1,Xλ2,h2)=h2−h1+λ2−λ1−2eh2−h1
λ2−λ1(λ2−λ1)
2 (λ1−λ2)
=t+ 2e−t−1
2
≥ln 2
2.
“=” achieves when t=t0= ln 2.
Whenλ1̸=λ2andh1≥h2, we have
D(Xλ1,h1,Xλ2,h2)
R(Xλ1,h1,Xλ2,h2)=h1−h2+λ1−λ2
2 (λ1−λ2)≥λ1−λ2
2 (λ1−λ2)=1
2.
Therefore we can get that
γ=ln 2
2.
G.4 Proof of Prop. 8
The proof outline is almost the same as the ones in App. C.4 and App. F.2. We omit the details and point
to the proof sections where we can adapt from.
G.4.1 Gaussian Distribution
The proof is the same as App. F.2.1, except that we use the D(·,·)andR(·,·)from App. G.3.1.
G.4.2 Uniform Distribution
The proof is the same as App. F.2.2, except that we use the D(·,·)andR(·,·)from App. G.3.2.
G.4.3 Exponential Distribution
The proof is the same as App. C.4.1, except that we use the D(·,·)andR(·,·)from App. G.3.3.
G.4.4 Shifted Exponential Distribution
The proof is the same as App. C.4.2, except that we use the D(·,·)andR(·,·)from App. G.3.4.
G.5 Privacy-Distortion Performance of Mech. 4 with Relaxed Assumption
Based on Asm. 6 and Asm. 4, the Privacy-distortion performance of Mech. 4 is shown below.
Proposition 9. Under Asm. 6 and Asm. 4, Mech. 4 has the following ∆andΠϵ,ωΘvalue/bound.
•Gaussian:
∆ =s
2/radicalbigg
2
πe−1
2t2
0−t0s
2(1−2Φ (t0)),
Πϵ,ωΘ<2ϵ·/bracketleftbig
c+Lµ,σ/parenleftbigs
2−t∗−ϵ/parenrightbig/bracketrightbig
cs+Lµ,σ
2/parenleftbigs
2−t∗/parenrightbig2+
M/parenleftbigg
µ−µ,kµ
µ−µ,Lµ,1/parenrightbigg
·M/parenleftbigg
σ−σ,kσ
σ−σ,Lσ,1/parenrightbigg
·(σ−σ)|t0|s,
56Under review as submission to TMLR
wheret0is defined in Mech. 4, c=kµkσ
(µ−µ)·(σ−σ), functionMsatisfies
M(x,c,L,A) =/braceleftigg
A
x+Lx
2, ifc≤A
x−Lx
2
c+/radicalbig
2L(A−cx),ifc>A
x−Lx
2,
Lµ,σ=LσM/parenleftig
µ−µ
|t0|,kµ
µ−µ,|t0|Lµ,1
|t0|/parenrightig
+|t0|LµM/parenleftig
σ−σ,kσ
σ−σ,Lσ,1/parenrightig
,andt∗=s
2+c
Lµ,σ−ϵ−/radicalbigg/parenleftig
c
Lµ,σ−ϵ/parenrightig2
+2cs
Lµ,σ.
•Exponential:
∆ =1
2s,
Πϵ,ωΘ≤2ϵ·[c+L(s−x∗+ϵ)]
cs+L
2(s−x∗)2,
wherex∗=s+c
L+ϵ−/radicalig/parenleftbigc
L+ϵ/parenrightbig2+2cs
L.
•Shifted exponential:
∆ =sln 2
2,
Πϵ,ωΘ<2ϵ·/bracketleftbig
c+Lλ,h/parenleftbigs
2−t∗−ϵ/parenrightbig/bracketrightbig
cs+Lλ,h
2/parenleftbigs
2−t∗/parenrightbig2+
ln 2·M/parenleftbigg
h−h,kh
h−h,Lh,1/parenrightbigg
·M/parenleftbigg
λ−λ,kλ
λ−λ,Lλ,1/parenrightbigg
·/parenleftbig
λ−λ/parenrightbig
s,
wherec=khkλ
(h−h)·(λ−λ), functionMsatisfies
M(x,c,L,A) =/braceleftigg
A
x+Lx
2, ifc≤A
x−Lx
2
c+/radicalbig
2L(A−cx),ifc>A
x−Lx
2,
Lλ,h =LλM/parenleftig
h−h
ln 2,kh
h−h,ln 2·Lh,1
ln 2/parenrightig
+ ln 2· LhM/parenleftig
λ−λ,kλ
λ−λ,Lλ,1/parenrightig
,andt∗=s
2+c
Lλ,h−ϵ−
/radicalbigg/parenleftig
c
Lλ,h−ϵ/parenrightig2
+2cs
Lλ,h.
The proofs are the same as App. F.3, App. D.2.1 and App. D.2.2, except that we use the D(·,·), andR(·,·)
from App. G.3.1, App. G.3.3, and App. G.3.4.
G.6 Proof of Corollary 6
G.6.1 Geometric Distribution
Proof.LetXθ1andXθ2be two Geometric random variables with parameters θ1andθ2respectively. We
assume that θ1>θ2without loss of generality. Let k′satisfy (1−θ1)k′
θ1= (1−θ2)k′
θ2andk0=⌊k′⌋+ 1.
Then we can get that
D(Xθ1,Xθ2) =1
2dTV/parenleftbig
ωXθ1∥ωXθ2/parenrightbig
=1
2(1−θ2)k0−1
2(1−θ1)k0,
R(Xθ1,Xθ2) =√1−θ2
θ2−√1−θ1
θ1.
57Under review as submission to TMLR
Therefore, we can get that
γ= inf
θ<θ1<θ2≤θ(1−θ2)k0−(1−θ1)k0
2/parenleftig√1−θ2
θ2−√1−θ1
θ1/parenrightig.
G.6.2 Binomial Distribution
Proof.LetXθ1andXθ2be two binomial random variables with parameters θ1andθ2respectively with fixed
number of trials n. We assume that θ1>θ2without loss of generality. Let k′satisfy/parenleftbign
k′/parenrightbig
θk′
1(1−θ1)n−k′
=/parenleftbign
k′/parenrightbig
θk′
2(1−θ1)n−k′
andk0=⌊k′⌋. We can get that
D(Xθ1,Xθ2) =1
2dTV/parenleftbig
ωXθ1∥ωXθ2/parenrightbig
=1
2I1−θ2(n−k0,1 +k0)−1
2I1−θ1(n−k0,1 +k0),
R(Xθ1,Xθ2) =/vextendsingle/vextendsingle/vextendsingle/radicalbig
nθ2(1−θ2)−/radicalbig
nθ1(1−θ1)/vextendsingle/vextendsingle/vextendsingle,
whereIrepresents the regularized incomplete beta function.
Therefore, we can get that
γ= inf
θ<θ1<θ2≤θI1−θ2(n−k0,1 +k0)−I1−θ1(n−k0,1 +k0)/vextendsingle/vextendsingle/vextendsingle/radicalbig
nθ2(1−θ2)−/radicalbig
nθ1(1−θ1)/vextendsingle/vextendsingle/vextendsingle.
G.6.3 Poisson Distribution
Proof.LetXθ1andXθ2be two Poisson random variables with parameters θ1andθ2respectively. We assume
thatθ1>θ2without loss of generality. Let k′satisfyθk′
1e−θ1=θk′
2e−θ2andk0=⌊k′⌋+ 1. Then we can get
that
D(Xθ1,Xθ2) =1
2dTV/parenleftbig
ωXθ1∥ωXθ2/parenrightbig
=1
2Q(k0,θ2)−1
2Q(k0,θ1),
R(Xθ1,Xθ2) =/radicalbig
θ1−/radicalbig
θ2,
whereQis the regularized gamma function.
Therefore, we can get that
γ= inf
θ<θ1<θ2≤θQ(k0,θ2)−Q(k0,θ1)
2/parenleftbig√θ1−√θ2/parenrightbig.
H Case Study with Secret = Fraction
As indicated in S1 in §2.1, the fraction of discrete distributions can reveal sensitive information. In this
section, we first present the results for ordinal distributions, where there is a specific formula for the fractions
at each bin (i.e., binomial, Poisson, geometric that we discussed in Apps. E and G.2). We then present the
results for categorical distributions, where there is no constraint on the fractions of the bins so long as they
are normalized.
58Under review as submission to TMLR
H.1 Ordinal Distribution
Here, we consider the same three discrete distributions studied in Apps. E and G.2: geometric distributions,
binomial distributions, and Poisson distributions. We first analyze the lower bound. We assume that the
secrete is the fraction of the j-th bin.
Corollary7 (Privacylowerbound, secret=fractionofanordinaldistribution) .Consider the secret function
g(θ) =fXθ(j). For anyT∈(0,1), when Πϵ,ωΘ≤T, we have ∆>/parenleftbig
⌈1
T⌉−1/parenrightbig
·2γϵ, where the value of γ
depends on the type of the distributions:
•Geometric:
γ= inf
θ<θ1<θ2≤θ(1−θ2)h(θ1,θ2)−(1−θ1)h(θ1,θ2)
2/vextendsingle/vextendsingle/vextendsingle(1−θ2)jθ2−(1−θ1)jθ1/vextendsingle/vextendsingle/vextendsingle,
whereh(θ1,θ2) =⌊log(θ2)−log(θ1)
log(1−θ1)−log(1−θ2)⌋+ 1.
•Binomial:
γ= inf
θ<θ1<θ2≤θ
I1−θ2(n−h(θ1,θ2),1+h(θ1,θ2))−I1−θ1(n−h(θ1,θ2),1+h(θ1,θ2))
2|(n
j)θj
2(1−θ2)n−j−(n
j)θj
1(1−θ1)n−j|,
whereh(θ1,θ2) =⌊k′⌋,k′=nln/parenleftig
1−θ2
1−θ1/parenrightig/slashig
ln/parenleftig
θ1(1−θ2)
θ2(1−θ1)/parenrightig
, andIrepresents the regularized incomplete beta
function.
•Poisson:
γ= inf
θ<θ1<θ2≤θQ(h(θ1,θ2),θ2)−Q(h(θ1,θ2),θ1)
2/vextendsingle/vextendsingle/vextendsingleθj
1e−θ1
j!−θj
2e−θ2
j!/vextendsingle/vextendsingle/vextendsingle,
whereh(θ1,θ2) =⌊θ1−θ2
ln(θ1)−ln(θ2)⌋+ 1andQis the regularized gamma function.
The proof is in App. H.3. The above lower bounds can be computed numerically.
Since these distributions only have one parameter, we can use Alg. 1 and Alg. 3 to derive a data release
mechanism. The performance of greedy-based and dynamic-programming-based data release mechanisms
for each distribution is shown in Fig. 15.
0.05 0.10 0.15 0.20
Privacy0.10.20.30.40.50.6DistortionTheoretical lower bound
Dynamic-programming
Greedy
(a) Distribution = Geometric
0.05 0.10 0.15 0.20
Privacy0.20.40.60.8DistortionTheoretical lower bound
Dynamic-programming
Greedy (b) Distribution = Binomial
0.05 0.10 0.15 0.20
Privacy0.10.20.30.40.50.60.70.8DistortionTheoretical lower bound
Dynamic-programming
Greedy (c) Distribution = Poisson
Figure 15: Privacy-distortion performance of Alg. 1 and Alg. 3 for geometric, binomial and Poisson distri-
bution when secret = fraction.
H.2 Categorical Distribution
In this section, we consider categorical distributions where the fraction of each bin can be changed freely (as
long as they are normalized). We assume that θ= (p1,p2,...,pC)s.t.pi∈[0,1]∀i∈[C]and/summationtext
ipi= 1.
59Under review as submission to TMLR
Note that this is completely different from the distributions discussed in App. H.1 where the parameter of
the distribution is one-dimensional.
We first analyze the lower bound. Without loss of generality, we assume that we want to protect the fraction
of thej-th bin, i.e. pj.
Corollary 8 (Privacy lower bound, secret = fraction of a general discrete distribution) .Consider the secret
functiong(θ) =p1. For anyT∈(0,1), when Πϵ,ωΘ≤T, we have ∆>/parenleftbig
⌈1
T⌉−1/parenrightbig
·ϵ.
The proof is in App. H.4. Next, we present the data release mechanism under the following assumption.
Assumption 7.The prior distribution of (p1,...,pC)is a uniform distribution over all the probability
simplex{(p1,...,pC)|pi∈[0,1)∀i∈[C]and/summationtext
ipi= 1}.
Mechanism 5(For secret = fraction of a categorical distribution) .The parameters of the mechanism are
as follows.
Sp1,...,pC=/braceleftbigg/parenleftbigg
p1−t
C−1,...,pj−1−t
C−1,pj+t,
pj+1−t
C−1,...,pC−t
C−1/parenrightbigg/vextendsingle/vextendsingle/vextendsingle/vextendsinglet∈/bracketleftig
−s
2,s
2/parenrightig/bracerightbigg
,
θ∗
p1,...,pC=/parenleftbigg
p1−T,...,pj−1−T,pj+ (C−1)T,
pj+1−T,...,pC+1−T/parenrightbigg
,
whereT= min{p1,...,pj−1,pj+1,...,pC,0}, and
I=/braceleftbigg
(p1,...,pC)/vextendsingle/vextendsingle/vextendsingle/vextendsingle∀i pi∈/parenleftbigg
−s
2 (C−1),1/bracketrightbigg
,/summationdisplay
ipi= 1,
pj= (k+ 0.5)s,wherek∈{0,1,...,C−1}/bracerightbigg
.
Heres>0is a hyper-parameter of the mechanism that divides 1.
This data release mechanism achieves the following privacy-distortion trade-off.
Proposition 10. Under Asm. 7, Mech. 5 has the following Πϵ,ωΘand∆value/bound.
Πϵ,ωΘ<2ϵ
s+ 1−/parenleftbigg
1−s
C−1/parenrightbiggC−1
,
∆ =s
2</parenleftig
2 +s
ϵ/parenrightig
∆opt.
Under the regime sup (s)→Aϵ, whereAis a constant larger than 2,∆satisfies
lim
sup(s)→Aϵ∆<(2 +A)∆opt.
∆optis the minimal distortion an optimal data release mechanism can achieve given the privacy Mech. 5
achieves.
The proof is in App. H.5. To ensure that Πϵ,ωΘ<1,sshould satisfy s >2ϵ. According to Prop. 10, the
mechanism is order-optimal with multiplicative factor 2 +Awhen sup (s)→Aϵ, whereA>2.
H.3 Proof of Corollary 7
H.3.1 Geometric Distribution
Proof.LetXθ1andXθ2be two Geometric random variables with parameters θ1andθ2respectively. We
assume that θ1>θ2without loss of generality. Let k′satisfy (1−θ1)k′
θ1= (1−θ2)k′
θ2andk0=⌊k′⌋+ 1.
60Under review as submission to TMLR
Then we can get that
D(Xθ1,Xθ2) =1
2dTV/parenleftbig
ωXθ1∥ωXθ2/parenrightbig
=1
2(1−θ2)k0−1
2(1−θ1)k0,
R(Xθ1,Xθ2) =/vextendsingle/vextendsingle/vextendsingle(1−θ2)jθ2−(1−θ1)jθ1/vextendsingle/vextendsingle/vextendsingle.
Therefore, we can get that
γ= inf
θ<θ1<θ2≤θ(1−θ2)k0−(1−θ1)k0
2/vextendsingle/vextendsingle/vextendsingle(1−θ2)jθ2−(1−θ1)jθ1/vextendsingle/vextendsingle/vextendsingle.
H.3.2 Binomial Distribution
Proof.LetXθ1andXθ2be two binomial random variables with parameters θ1andθ2respectively with fixed
number of trials n. We assume that θ1>θ2without loss of generality. Let k′satisfy/parenleftbign
k′/parenrightbig
θk′
1(1−θ1)n−k′
=/parenleftbign
k′/parenrightbig
θk′
2(1−θ2)n−k′
andk0=⌊k′⌋. We can get that
D(Xθ1,Xθ2) =1
2dTV/parenleftbig
ωXθ1∥ωXθ2/parenrightbig
=1
2I1−θ2(n−k0,1 +k0)−1
2I1−θ1(n−k0,1 +k0),
R(Xθ1,Xθ2) =n(θ1−θ2),
whereIrepresents the regularized incomplete beta function.
Therefore, we can get that
γ= inf
θ<θ1<θ2≤θI1−θ2(n−k0,1 +k0)−I1−θ1(n−k0,1 +k0)
2/vextendsingle/vextendsingle/vextendsingle/parenleftbign
j/parenrightbig
θj
2(1−θ2)n−j−/parenleftbign
j/parenrightbig
θj
1(1−θ1)n−j/vextendsingle/vextendsingle/vextendsingle.
H.3.3 Poisson Distribution
Proof.LetXθ1andXθ2be two Poisson random variables with parameters θ1andθ2respectively. We assume
thatθ1>θ2without loss of generality. Let k′satisfyθk′
1e−θ1=θk′
2e−θ2andk0=⌊k′⌋+ 1. Then we can get
that
D(Xθ1,Xθ2) =1
2dTV/parenleftbig
ωXθ1∥ωXθ2/parenrightbig
=1
2Q(k0,θ2)−1
2Q(k0,θ1),
R(Xθ1,Xθ2) =/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleθj
1e−θ1
j!−θj
2e−θ2
j!/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle,
whereQis the regularized gamma function.
Therefore, we can get that
γ= inf
θ<θ1<θ2≤θQ(k0,θ2)−Q(k0,θ1)
2/vextendsingle/vextendsingle/vextendsingleθj
1e−θ1
j!−θj
2e−θ2
j!/vextendsingle/vextendsingle/vextendsingle.
61Under review as submission to TMLR
H.4 Proof of Corollary 8
Proof.LetXp1
1,p1
2,...,p1
CandXp2
1,p2
2,...,p2
Cbe two categorical random variables. We have
D/parenleftig
Xp1
1,p1
2,...,p1
C,Xp2
1,p2
2,...,p2
C/parenrightig
=1
2dTV/parenleftig
ωXp1
1,p1
2,...,p1
C∥ωXp2
1,p2
2,...,p2
C/parenrightig
≥1
2/vextendsingle/vextendsinglep1
j−p2
j/vextendsingle/vextendsingle, (25)
R/parenleftig
Xp1
1,p1
2,...,p1
C,Xp2
1,p2
2,...,p2
C/parenrightig
=/vextendsingle/vextendsinglep1
j−p2
j/vextendsingle/vextendsingle.
Therefore, we can get that
γ≥1
2.
H.5 Proof of Prop. 10
Proof.We first focus on the proof for Πϵ,ωΘ.
We separate the space of possible data parameters into two regions: S1=/braceleftig
(p1,...,pC)|pi∈
/bracketleftig
s
2(C−1),1−s
2(C−1)/bracketrightig
∀i∈[C]and/summationtext
ipi= 1/bracerightig
andS2={(p1,...,pC)|pi∈[0,1)∀i∈[C]and/summationtext
ipi= 1}\S1.
The high-level idea of our proof is as follows. Note that for any parameter θ∈S1, there exists aSp1,...,pC
s.t.θ∈Sp1,...,pCandSp1,...,pC⊂S1. Therefore, we can bound the attack success rate if θ∈S1. At the same
time, the probability of θ∈S2is bounded. Therefore, we can bound the overall attacker’s success rate (i.e.,
Πϵ,ωΘ). More specifically, let the optimal attacker be ˆg∗. We have
Πϵ,ωΘ=P(ˆg∗(θ′)∈[g(θ)−ϵ,g(θ) +ϵ])
=/integraldisplay
θ∈S1p(θ)P(ˆg∗(θ′)∈[g(θ)−ϵ,g(θ) +ϵ])dθ
+/integraldisplay
θ∈S2p(θ)P(ˆg∗(θ′)∈[g(θ)−ϵ,g(θ) +ϵ])dθ
<2ϵ
s+/parenleftigg
1−/parenleftbigg
1−s
C−1/parenrightbiggC−1/parenrightigg
.
62Under review as submission to TMLR
50000 100000 150000 200000 250000
Salary051015202530Count
Figure 16: Histogram of salary dataset.
For the distortion, it is straightforward to get that ∆ =s
2from Eq. (25), and ∆opt>/parenleftig
⌈1
Πϵ,ωΘ⌉−1/parenrightig
·ϵ≥ϵ
from Corollary 2. We can get that/parenleftbigg
Πϵ,ωΘ−/parenleftbigg
1−/parenleftig
1−s
C−1/parenrightigC−1/parenrightbigg/parenrightbigg
·∆ =ϵand
∆ = ∆ opt+ ∆−∆opt
<∆opt+ ∆−/parenleftbigg
⌈1
Πϵ,ωΘ⌉−1/parenrightbigg
·ϵ
≤∆opt+ϵ+ ∆−ϵ
Πϵ,ωΘ
= ∆ opt+ϵ+/parenleftbigg
1−/parenleftig
1−s
C−1/parenrightigC−1/parenrightbigg
2ϵ
s+/parenleftbigg
1−/parenleftig
1−s
C−1/parenrightigC−1/parenrightbigg·∆
=/parenleftigg
1 +s
2ϵ/parenleftigg
1−/parenleftbigg
1−s
C−1/parenrightbiggC−1/parenrightigg/parenrightigg
(∆opt+ 2γϵ)
≤/parenleftigg
2 +s
ϵ/parenleftigg
1−/parenleftbigg
1−s
C−1/parenrightbiggC−1/parenrightigg/parenrightigg
∆opt
</parenleftig
2 +s
ϵ/parenrightig
∆opt.
I Additional Results
In this section, we provide additional results on how released data from our mechanisms can support down-
stream applications.
We consider the salaries from people with Master’s and PhD degrees in this Kaggle dataset https://www.
kaggle.com/datasets/rkiattisak/salaly-prediction-for-beginer . We plot its histogram in Fig. 16.
We can see that there are two peaks. They correspond to people with age<=40 and age>40 (see Fig. 17).
Assume the goal is to release this dataset and preserve the salary difference between people with age<=40
and age>40, while protecting the mean salaries. We can apply our mechanism for mean (§6.3) on this
dataset. The histogram of the released data is shown in Fig. 18. Data receivers can obtain the salary
63Under review as submission to TMLR
50000 100000 150000 200000 250000
Salary051015202530Count<=40
>40
Figure 17: Histogram of salary dataset for people with age <= 40 and > 40.
50000 100000 150000 200000 250000
Salary051015202530Count
Figure 18: Histogram of salary dataset for applying the mechanism in §6.3.
difference between people with age<=40 and age>40 accurately by computing the difference between the
two peaks, while the mean salaries are protected under our mechanism.
Here we use the salary difference between people with age<=40 and age>40 as an example. In general, any
downstream tasks that depend only on the “shape” of the distribution will not be affected by our mechanism,
since our mechanism shifts all samples by the same amount.
64