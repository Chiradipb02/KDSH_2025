Under review as submission to TMLR
Normality-Guided Distributional Reinforcement Learning for
Continuous Control
Anonymous authors
Paper under double-blind review
Abstract
Learning a predictive model of the mean return, or value function, plays a critical role in
many reinforcement learning algorithms. Distributional reinforcement learning (DRL) has
been shown to improve performance by modeling the value distribution , not just the mean.
We study the value distribution in several continuous control tasks and find that the learned
value distribution is empirically quite close to normal. We design a method that exploits
this property, employing variances predicted from a variance network, along with returns,
to analytically compute target quantile bars representing a normal for our distributional
value function. In addition, we propose a policy update strategy based on the correctness as
measured by structural characteristics of the value distribution not present in the standard
value function. The approach we outline is compatible with many DRL structures. We
use two representative on-policy algorithms, PPO and TRPO, as testbeds. Our method
yields statistically significant improvements in 10 out of 16 continuous task settings, while
utilizing a reduced number of weights and achieving faster training time compared to an
ensemble-based method for quantifying value distribution uncertainty.
1 Introduction
In reinforcement learning, an agent receives rewards by interacting with the environment and updates their
policy to maximize the cumulative reward or return. The return often has a high variance, which may make
training unstable. To reduce the variance, actor-critic algorithms (Thomas, 2014; Schulman et al., 2015b;
Mnih et al., 2016; Gu et al., 2016) use a learned value function, a model of the return that serves a baseline
to reduce variance and speed up training.
The standard approach to learning a value function estimates a single scalar value for each state. Several
recent studies learn a value distribution instead, attempting to capture the randomness in the interaction
between the agent and environment. One method for achieving this involves using multiple value functions
(referred to as an ensemble) so that these functions collectively represent the value distribution (Lee et al.,
2020). Analternativemethodrequiresthevaluefunctiontoproducemultipleoutputs, representedasquantile
bars (Bellemare et al., 2017a; Barth-Maron et al., 2018; Dabney et al., 2017; Singh et al., 2020; Yue et al.,
2020). These techniques, known as distributional reinforcement learning (DRL), have proven to enhance
stability and expedite the learning process. The target for generating quantile bars is commonly derived
from the distributional Bellman optimality operator, and the distributional value function is fitted to the
target with the quantile regression (Bellemare et al., 2017a; Dabney et al., 2017).
Our goal is to derive quantile bars based on the Markov chain central limit theorem (MC-CLT) rather
than the distributional Bellman optimality operator, and to introduce a novel uncertainty measure absent
in standard distributional value functions, applying it to policy updates. The MC-CLT indicates that in
continuous tasks, the variance of the return keeps decreasing as the timestep goes. Moreover, when sufficient
remaining timesteps exist, the return conforms to a normal distribution. We examine the value distribution
trained using quantile regression and find, in continuous tasks, the predictions of the distribution closely
resemble normal distributions, as described by the MC-CLT and depicted in Figure 1. This contrasts
with the distributional Bellman optimality operator, which causes the variance of the value distribution to
1Under review as submission to TMLR
Figure 1: Results of trained the distributional value function VD,πwith the Huber quantile regression. Each
line represents different timesteps: the initial step T= 0, the intermediate step T= 500, and the last
stepT= 1000. Within each cell, the quantile output of VD,πis represented by bars, the corresponding
cumulative distribution function (CDF) is depicted in orange, and the computed normal CDF based on the
bars is shown in green. The policies are updated with PPO and VD,πs are updated with the quantile Huber
loss. The number of quantile bars is 200, but only 45 are shown for visualization. Notably, for continuous
tasks, the distribution predicted by VD,πaligns remarkably well with the normal distribution, even at the
final step.
increase over timesteps (Figure 2), due to the multiplication of γ <1with the next state and action Q value
(Equation 4). Section 4.2 provides a detailed example of this variance increase.
In this paper, we assume that the value distribution of continuous tasks is governed by the MC-CLT rather
thanthedistributionalBellmanoptimalityoperator. Toappropriatelyguidethedistributionalvaluefunction,
we provide analytically computed quantile targets representing normal distributions, grounded in variances
sourced from a variance network (Nix & Weigend, 1994; Kendall & Gal, 2017) and returns. We assess
the accuracy of the current predictions made by the distributional value function. We aim to leverage
distributional information, prioritizing values that are accurately estimated during policy updates.
As we consider the reliable values for policy updates, we incorporate existing actor-critic algorithms (Lee
et al., 2020; Mai et al., 2022) that factor in uncertainty during policy updates as a comparison. These
algorithms typically minimize the influence of samples with high variances. However, some states inherently
possess elevated variances. For instance, the true variances are higher near the initial timestep because the
expected return from each timestep gets smaller as timesteps goes on (Appendix D). The question arises:
Should states having higher variances have less impact on policy updates? Although the predicted variance
iscorrectfor a state, its impact could be lessened simply because its variance is high. In contrast, we
determine the impact of policy updates by evaluating how closely our distributional value function predicts
the quantiles for normals. Given that our distributional value function is fitted to a normal quantile target,
a well-trained distributional value function should yield quantiles that closely align with the shape of the
normal distribution. This alignment indicates the function’s accuracy in reflecting its models’ underlying
distribution. In Appendix E, we provide additional discussion on tasks where the normality assumption
may not be applicable, as well as experiments in the opposite direction, where samples that are not closely
aligned are given higher weights for exploration.
In summary, our key contributions are:
2Under review as submission to TMLR
•We examine that the distributional value function approximates a normal for continuous tasks, and
this normal’s variance increases with each timestep. To address this, we guide the distributional
value function to produce quantiles representing a normal distribution, leveraging variances sourced
from the variance network (Nix & Weigend, 1994; Kendall & Gal, 2017) and sampled returns.
•Under the normality assumption, we measure the uncertainty (or correctness) of the value distri-
bution for each state, rather than relying the uncertainty derived from multiple value functions
(ensemble). We propose an uncertainty-based policy update strategy that assigns a high weight to
a correctly predicted state.
•We provide an empirical validation that uses PPO (Schulman et al., 2017) and TRPO (Schulman
et al., 2015a) on several continuous control tasks. We compare our methods to standard algorithms
(PPO and TRPO) as well as the ensemble-based approach. We find that our method exhibits better
performance than the ensemble-based method in 10/16 tested environments, while using half as
many weights and training twice as fast.
2 Related Work
We briefly review related work on distributional reinforcement learning (DRL) and supporting methods.
Bellemare et al. (2017a) studied the distribution of the value network for deep Q-learning and proposed
C51, a DRL algorithm that uses a categorical distribution to model the value distribution. C51 produced
state-of-the-art performance on the Arcade Learning Environment (Bellemare et al., 2013), but required the
parameters of the modeling distribution to be fixed in advance. Dabney et al. (2017) proposed QR-DQN,
which leverages quantile regression and allows for an adaptive distribution structure. In addition, they
resolved a key theoretical question with KL divergence ( DKL), which could lead to non-convergence, by
replacing it with Wasserstein distance metric and showing convergence of the distributional value network.
While these methods focus solely on the mean of the value distribution, our approach seeks to leverage the
distributional characteristics through the normality assumption as well.
Several extensions of DRL to continuous tasks have been proposed, including distributed distributional de-
terministic policy gradients (D4PG) (Barth-Maron et al., 2018), sample-based distributional policy gradient
(SDPG) (Singh et al., 2020) and implicit distributional actor-critic (IDAC) (Yue et al., 2020). Our method
is model agnostic and compatible with any existing DRL algorithm that learns the value distribution with
a single neural network.
Compared to supervised learning, reinforcement learning has an unstable learning process, and uncertainty
is often modeled to make RL learning more stable. We can decompose uncertainty into aleatoric and
epistemic (Hora, 1996; Kiureghian & Ditlevsen, 2009). Aleatoric comes from the stochasticity of data and is
often measured with the attenuation loss function (Nix & Weigend, 1994). Epistemic uncertainty is caused
by the limits of generalization of models and can be measured by dropout (Kendall & Gal, 2017) or ensemble
methods (Lakshminarayanan et al., 2016). The RL community models these two types of uncertainty to
update the model safely or to support sparse reward functions (Bellemare et al., 2016; Strehl & Littman,
2005; Burda et al., 2018). Clements et al. (2019) penalizes actions with high aleatoric uncertainty, and
epistemic uncertainty is used to guide exploration. To our knowledge, Moerland et al. (2017) is the only
previous work assuming a normal distribution for the return distribution, which they use to construct a
bootstrap estimate of combined uncertainty. SUNRISE (Lee et al., 2020) measures epistemic uncertainty
with an ensemble technique and uses it to reduce the gradient of a sample with high uncertainty. Mai
et al. (2022) leverage the two types of uncertainty with the combination of ensemble and attenuation loss.
We utilize quantile bars, which represent the return’s distribution, as a proxy for epistemic uncertainty.
This proxy is less resource-intensive in terms of training compared to ensemble methods, which significantly
increase training costs (see Appendix of Mai et al. (2022)).
3Under review as submission to TMLR
3 Preliminaries
3.1 Reinforcement Learning
We consider an infinite-horizon Markov decision process (MDP) (Puterman, 2014; Sutton & Barto, 2018)
definedbythetuple M= (S,A,P,R,γ,µ ). Theagentinteractswithanenvironmentandtakesanaction at∈
Aaccording to a policy πθ(at|st)parameterized by θfor each state st∈Sat timet. Then, the environment
changes the current state stto the next state st+1based on the transition probability P(st+1|st,at). The
reward function R:S×A→ Rprovides the reward for ( st,at).γdenotes the discount factor and µis the
initial state distribution for s0. The goal of a reinforcement learning (RL) algorithm is to find a policy πθ
that maximizes the expected cumulative reward, or return:
θ∗= argmax
θEs0∼µ
at∼πθ(·|st)
st+1∼P(·|st,at)/bracketleftigg∞/summationdisplay
t=0γtR(st,at)/bracketrightigg
. (1)
In many RL algorithms, a value function Vπ(st)is trained to estimate the expected return under the current
policy, E[/summationtext∞
t′=tγt′R(st′,at′)], and is updated with the target:
Vπ(st) =R(st,at) +γVπ(st+1)
wherest+1∼P(·|st,at)(2)
3.2 Distributional Reinforcement Learning
In distributional reinforcement learning (DRL), Bellemare et al. (2017b) introduced an action-state distri-
butional value function QD,π(s,a)which models the distribution of returns for an agent taking action aand
then following policy π. We model the distribution by having QD,πoutputNquantile bars{q0,q1,...,qN−1}.
Similar to the Bellman optimality operator Tπ(Watkins & Dayan, 1992) (Equation 3) for the state-action
value function Qπ,QD,πis updated by a distributional Bellman optimality operator (Equation 4):
TπQπ(st,at) =E[R(st,at)] +γEst+1∼P(·|st,at)[max
at+1Qπ(st+1,at+1)]
wherest+1∼P(·|st,at),at+1= argmaxat+1Qπ(st+1,at+1)(3)
TπQD,π(st,at) =R(st,at) +γQD,π(st+1,at+1)
wherest+1∼P(·|st,at),at+1= argmaxat+1E[QD,π(st+1,at+1)](4)
3.3 Markov Chain Central Limit Theorem
We introduce the Markov Chain Central Limit Theorem (MC-CLT) (Jones, 2004) (Theorem 1). We leverage
this theorem to argue that returns are normally distributed in some continuous environments.
Theorem 1 (Markov Chain Central Limit Theorem) .LetXbe a Harris ergodic Markov chain on Xwith
stationary distribution ρand letf:X→Ris a Borel function. If Xis uniformly ergodic and Eρ[f2(x)]<∞
then for any initial distribution, as n→∞,
√n(¯fn−Eρ[f])d− →N (0,σ2),
where ¯fn=1
nn/summationdisplay
i=1f(Xi)a.s−−→Eρ[f]
σ2= Varρ(f(X1)) + 2∞/summationdisplay
k=1Covρ(f(X1),f(X1+k)).(5)
4Under review as submission to TMLR
Figure 2: The standard deviation computed from each of the distributional value functions trained on
the distributional Bellman optimality operator (Equation 4) with exponential smoothing. The estimated
standard deviation increases as timestep increases.
To relate this to RL, we let fbe the reward function RandXbe the state-action space. If the Markov
chain induced by a policy is Harris ergodic, we expect that the theorem holds (as rewards being a Borel
function is a mild condition). In detail, a return G(st)is consists of the sum of the rewards/summationtextT
i=tR(st,at).
Thus, for sufficiently large T, the distribution of G(st)tends towards a normal distribution. This might
not consistently be the case, particularly when tclosely approaches TandR(st,·)deviates from a normal
for each possible action. Nevertheless, a distributional value function, trained using the Huber quantile
regression, outputs a distribution that closely represents a normal for the final timestep, even for terminal
states (Figure 1). It’s also worth noting that the presence of transient states can disrupt Harris ergodicity.
However, this issue is not prevalent in many continuous control tasks, such as robotics environments in
OpenAI Gym, since the optimal policy is stable and repetitive.
4 Approach
Inthispaper,weintroduceamethodthatleveragesthedistributionalpropertyofvaluedistribution,assuming
normality in continuous tasks. The objective of this approach is to determine the impact of individual
samples on policy updates. We argue that existing quantile distributional value functions yield estimates
where variance increases with timestep. To handle this issue, we guide our distributional value function VD,π
using quantile bars of a normal distribution derived from variance and return. Furthermore, we discuss how
to measure the uncertainty (or accuracy) of the current estimation of VD,πfor each state and incorporate
this measure into policy updates. We also further discuss scenarios where a task doesn’t satisfy the normality
condition in Appendix E.
4.1 State Distributional Value Function
We first similarly introduce a state distributional value function VD,π(s)that also outputs quantile bars
for states. The distributional value function VD,π(s)for the general actor-critic policy gradient is only
dependent on the state. To be specific, the target for VD,π(st)isR(st,at) +VD,π(st+1)similar to Equation
(4). The quantile bars of VD,π(st)is shifted by the amount of R(st,at). This let us use the actor-critic
algorithms such as Policy Optimization (PPO) (Schulman et al., 2017) and Trust Region Policy Optimization
(TRPO) (Schulman et al., 2015a).
4.2 Analysis of Distributional Value Function
Motivated by Theorem 1, we examine the learned distributional value functions VD,πacross different states
and at several timesteps at convergence (Figure 1). These distributions appear close to normal distributions.
Furthermore, our observation reveals that the learned VD,πandQD,πexhibit an increase in variances as
timesteps progress, contradicting Theorem 1. The distributional Bellman optimality operator (Bellemare
et al., 2017b) causes the variance of the distributional value function to increase with timestep as the discount
factor shrinks quantiles. To illustrate, suppose QD,π(st+1,at+1)={q0,q1,...,qN−1}at timestep t+ 1. Then,
the target for QD,π(st,at)under the distributional Bellman operator will be {R(st,at) +γq0,R(st,at) +
5Under review as submission to TMLR
Figure3: Illustrationofhowwemeasureuncertainty. When N= 4, thereare4quantilebars {q0,q1,q2,q3}as
an output of VD,π. Compute the mean qavgof the quantile bars, and find σisuch thatP(Z≤qi−qavg
σi) = 0.2∗
(i+ 1) =i+1
N+1fori∈{0,1,2,3}. We consider VD,πas approximately N(qavg,σ2
avg)whereσavg=1
4/summationtexti=3
i=0σi.
We find the exact quantile location {q′
0,q′
1,q′
2,q′
3}ofN(qavg,σ2
avg), then measure how much two types of
quantiles are different with mean squared error.
γq1,...,R (st,at) +γqN−1}, i.e.,{q0,q1,...,qN−1}is shifted by R(st,at)and shrunk by γ <1. Consequently
the range of the distribution for timestep tis narrower than the range for timestep t+ 1.
We test this empirically by training distributional value functions with PPO and QR-DQN using the dis-
tributional Bellman optimality operator (Equation 4) and graphing the estimated standard deviation from
quantile output for each state (Figure 2). Indeed, the variance estimates appear to increase with timestep
on average. We now propose a method that utilizes the variance network and returns to generate quantiles
for the distributional value function.
4.3 Variance of Normal Distribution
Variance network ( σ2
ψ), introduced by Nix & Weigend (1994) and named by Mai et al. (2022), are designed
to capture both the mean and Gaussian error (variance). In line with the normal assumption, the variance
network allows us to estimate the variance of a Gaussian distribution. We leverage a variance network to
estimate a variance for each state when computing quantile bars.
In the original variance network, the mean and variance are predicted jointly within a single network. We
separate these predictions: the variance network σ2
ψsolely predicts the variance, while the mean is predicted
byVD,π
θlike the original actor critic method.
Lσ2(ψ) =1
TT/summationdisplay
i=11
2σ2
ψ(si)||qavg(si)−G(si)||2+1
2lnσ2
ψ(si) (6)
whereqavg(s)is the mean of VD,π
θ(s)andG(s)is the sum of sampled rewards from the state s(return).
This separation of prediction eliminates the need for additional hyperparameter tuning that would have been
required for the simultaneous prediction of mean and variance, as observed in Mai et al. (2022). Although
the numerator of Equation 6 is separated from ψ, the update is performed in the direction of increasing σ2
ψ
when the error of the numerator is significant. Conversely, if the error is sufficiently small, σ2
ψis decreased.
σ2
ψis trained to predict variance in a manner consistent with the original variance network.
4.4 Target for Distributional Value Function
We provide the quantile bars of an approximated normal distribution to VD,π
θwith the variance from σ2
ψ
above and returns. To compute the target quantile bars, we need the mean of the approximated normal
distribution. The mean can be TD target or the sum of rewards G(s)fromsto end. Here, we use return G
for the sake of notation simplicity. To obtain a quantile target for VD,π, we first precompute the Zvalues
of the standard normal distribution. For example, when N= 4,Z={−0.841,−0.253,0.253,0.841}, i.e.,
P(X < =Z[i])≈0.2∗(i+ 1) =i+1
N+1fori∈{0,1,2,3}. Note that we use N= 8quantile bars for all
experiments in practice, as we empirically find that N≥8provides satisfactory performance. Then, we
compute the target quantile bars q′
tofVD,π
θ(st)such thatq′
t={Rt+σψZ[i]|i∈{0,1,...,N−1}}and
6Under review as submission to TMLR
fitVD,π
θtoq′
tby quantile Huber loss (Aravkin et al., 2014). The quantile Huber loss makes VD,π
θoutput
quantile bars that match the definition of quantiles. The Huber loss (Huber, 1964) is as follows:
Lκ(u) =/braceleftigg
1
2u2,if|u|<κ
κ(|u|−1
2κ),otherwise.(7)
The quantile Huber loss is an asymmetric variant of the Huber loss.
Lq(θ) =1
TNT/summationdisplay
t=0N−1/summationdisplay
i=0ρκ
τi(qi(st)−q′
t,i),
whereρκ
τi(u) =|τi−δ{u<0}|Lκ(u)
τi=i+ 1
N+ 1fori={0,1,...,N−1}(8)
4.5 Uncertainty Weight
SinceVD,πis a distributional value function, we can use the properties of the distribution to calculate the
uncertainty weight for policy updates. We measure the distance between the current quantile output of
VD,πand the normal distribution computed based on VD,π’s quantile output. We evaluate how close VD,π
is to the target normal distribution to judge whether VD,πgeneralizes well for the visited states, under the
hypothesis that a more accurate VD,πwill lead to better policy improvement. For example, when VD,π
does not align with normal quantile bars for a state, the discrepancy between VD,πand its target becomes
substantial. We discount the impact of such samples having high discrepancy (or high uncertainty) on policy
updates to focus more on reliable values that are accurately estimated from the distributional value function.
Figure 3 illustrates this when the number of quantile bars is N= 4for simplicity. In practice, we use N= 8
quantilebarsforallexperiments, asweempiricallyfindthat N≥8providessimilarsatisfactoryperformance.
If the prediction of VD,πis reliable, it should be symmetrical with respect to the mean of the quantile bars
from our normality assumption, and the location of the quantile bars should follow a normal distribution.
We first find the mean qavg(s)=1
N/summationtexti=N−1
i=0qi(s)of the quantile bars and calculate the standard deviations
{σ0,σ1,...,σN−1}for each quantile such that P(Z≤qi−qavg
σi) =i+1
N+1fori∈{0,1,...,N−1}. We consider
VD,πis trying to approximate N(qavg(s),σ2
avg)whereσavg=1
N/summationtexti=N−1
i=0σi.
To measure the uncertainty wfor policy updates, we find the locations of each quantile {q′
0(s),...,q′
N−1(s)}
based onN(qavg(s),σ2
avg). The better VD,πresembles the normal distribution, the smaller the difference
between these two types of quantile bars. Therefore, we use the mean sqaured error E=/summationtexti=N−1
i=0(qi(s)−
q′
i(s))2, and we compute the uncertainty weight w(s)as follows:
w(s) =σ(−E∗T) + 0.5 (9)
whereT >0is a temperature and σis the sigmoid function. Although Lee et al. (2020) suggest values for T,
it seems that finding the reasonable value for Tstill requires some tuning. We instead set a target uncertainty
weight and perform a parametric search by adjusting Tto achieve that target weight (Appendix B).
4.6 Policy Update with Uncertainty Weight
We leverage the uncertainty weights wto update the policy to mainly focus on values for which the current
VD,πpredicts correctly. Each reinforcement learning algorithm has a policy objective function. We scale
the objective function with the uncertainty weight to adjust the influence of the gradient for each sample.
For TRPO (Schulman et al., 2015a),
maximize
θEt/bracketleftbigg
w(st)πθ(at|st)
πθold(at|st)ˆAt/bracketrightbigg
subject to Et/bracketleftbigg
DKL(πθold(·|st)||πθ(·|st))/bracketrightbigg
≤δ,(10)
7Under review as submission to TMLR
Algorithm 1 MC-CLT with Uncertainty Weight
1:Input:policyπ, distributional value function VD,π
θ, variance network σ2
ψ, rollout buffer B
2:Initializeπ,VD,π, andσ2
ψ
3:fori= 1to epoch_num do
4: forj= 1to rollout_num do
5:at∼π(·|st)
6:st+1∼P(·|st,at)
7: ComputeN(qavg(s),σ2
avg)withVD,π(st) ={q0(st),q1(st),...,qN−1(st)}
8: Findq′(st)fromN(qavg(s),σ2
avg)
9: Compute the mean squared error E=/summationtextN−1
i=0(qi(st)−q′
i(st))2
10: Store (st,at,r(st,at),E,VD,π(st),σ2
ψ(st)) inB
11: ifst+1is terminal then
12: Reset env
13: end if
14: end for
15:Perform the parametric search to find the temperature T(Appendix B)
16:Compute target quantiles for VD,πwith the stored return and σ2
ψ
17:MinimizeLσ2(ψ)andLq(θ)
18:Optimizeπwith a policy objective scaled by w
19:end for
whereπθis a policy and ˆAtis an advantage estimator at timestep t. The more accurate the prediction of
VD,π, the closer the value of wis to 1; hence the gradient is more significant than other inaccurate samples.
Note that we normalize the advantages when we sample a batch for optimization in practice. Therefore, the
magnitude of the gradient is not changed, and we do not need to tune the learning rate as a consequence of
introducing the uncertainty weight.
We propose a normality-guided algorithm with the uncertainty weight to improve policy for PPO and TRPO
in Algorithm 1, but this method can be applied to various policy update algorithms as well.
5 Experiments
5.1 Experiment Setups
Our implementation is based on Spinning Up (Achiam, 2018), an open-source resource by OpenAI that pro-
vides implementations of several reinforcement learning algorithms and tools to help researchers get started
with deep RL. For our experiments, we chose two representative deep reinforcement learning algorithms, Pol-
icy Optimization (PPO) (Schulman et al., 2017) and Trust Region Policy Optimization (TRPO) (Schulman
et al., 2015a), to evaluate our method. The advantage is computed by GAE (Schulman et al., 2015b).
We use the default hyperparameters such as learning rate and batch size. All policies have a two-layer
tanhnetwork with 64 x 32 units, and all of the value function and the distributional value function has
a two-layer ReLUnetwork with 64 x 64 units or 128 x 128 units for all environments. All networks are
updated with Adam optimizer (Kingma & Ba, 2014). We evaluate our method on continuous OpenAI
gym Box2D (Brockman et al., 2016) and MuJoCo tasks (Todorov et al., 2012), as these environments
have continuous action spaces and dense reward functions to use the normal approximation of MC-CLT
(Theorem 1).
5.2 Detailed Ablation Analysis and Comparison
There exist three components that separate MC-CLT from the baseline. We perform an ablation study to
analyze the impact of each. The first component involves substituting the standard value function with
the distributional value VD,πtrained with the Huber quantile regression. We refer to this as PPO +VD,π
8Under review as submission to TMLR
Table 1: Ablation study of TRPO and PPO. Each entry represents an average return and a standard error,
derived from 30 different settings, with each setting running for 100 episodes. VD,π(QR) represents the dis-
tributional value function trained with the Huber quantile regression. This table illustrates an improvement
trend across the scores with the addition of each component from the baseline to MC-CLT. Also, MC-CLT
results generally better than the Ensemble method, while using fewer weights and reduced training time.
An asterisk (*) indicates a statistically significant improvement between MC-CLT and Ensemble method at
a significance level of 0.05
BipedalWalker Hopper HalfCheetah Ant
TRPO 115±3.2 2724±31 2749 ±37 2811 ±16
TRPO +VD,π(QR) 168±3.1 2893±28 2633 ±35 2962 ±21
MC-CLT TRPO w/o w 186±3.1 2835±27 2984 ±39 3107 ±18
Ensemble TRPO w/ w 180±2.7 2742±29 3212±44 3368 ±19
MC-CLT TRPO (ours) 195±2.6∗2927±27∗3172±43 3329 ±20
Swimmer Walker2d InvertedDoublePendulum LunarLander(C)
TRPO 121±0.2 2558±26 7969 ±109 198 ±3.1
TRPO +VD,π(QR) 117±0.2 2398±25 7878 ±112 227 ±2.6
MC-CLT TRPO w/o w 118±0.4 2616±30 7662 ±114 245 ±2.2
Ensemble TRPO w/ w 116±0.2 2834±24 8566±85 242±1.8
MC-CLT TRPO (ours) 118±0.2∗2904±23∗8404±93 258±1.8∗
BipedalWalker Hopper HalfCheetah Ant
PPO 179±2.9 2973±27 2902 ±42 1595 ±10
PPO +VD,π(QR) 208±2.7 2778±30 2574 ±33 1742 ±11
MC-CLT PPO w/o w 215±2.4 3184±23 2870 ±39 1743 ±13
Ensemble PPO w/ w 227±2.2 3208±22 3068 ±43 1863 ±12
MC-CLT PPO (ours) 236±2.3∗3230±22 3110 ±39 1900 ±13∗
Swimmer Walker2d InvertedDoublePendulum LunarLander(C)
PPO 122±0.1 2198±33 8814 ±72 229 ±2.1
PPO +VD,π(QR) 112±0.8 2037±34 8913 ±64 250 ±2.4
MC-CLT PPO w/o w 117±0.6 2610±35 9187 ±41 268 ±1.7
Ensemble PPO w/ w 119±0.2 2669±34 8933 ±64 256 ±1.8
MC-CLT PPO (ours) 122±0.1∗2716±33 9225 ±36∗278±1.2∗
(QR) and TRPO +VD,π(QR). The second is the addition of the normal target for VD,π, but without
incorporating the uncertainty weight, denoted as (MC-CLT w/ w). Lastly, our method MC-CLT is utilizing
the uncertainty weight in policy updates to prioritize more reliable samples (MC-CLT w/ w). Furthermore,
we compare our method with the ensemble-based approach with uncertainty weight, as their approach also
measures uncertainties and integrates them into policy updates in the same manner. Thus, we focuses on
evaluating which type of uncertainty more accurately reflects sample reliability: the one derived from the
network’s accuracy or the one based on standard deviation.
Table 1 represents the experimental results across 8 environments. The first section of the table provides
results from policies trained using TRPO, while the second section gives results from policies trained with
PPO. As we incrementally add each component from the baseline to MC-CLT, a consistent trend of per-
formance improvement is observed. Also, our method shows better results compared to the ensemble-based
approach in general. In 10 out of the 16 settings, we achieve statistically significant improvements between
MC-CLT and the Ensemble method, as indicated by an asterisk ( ∗) in the table, at a significance level of 0.05.
It’s worth noting that the ensemble-based approach with TRPO exhibits better mean performances in three
settings: HalfCheetah, Ant, and InvertedDoublePendulum. However, the differences are not statistically
significant.
9Under review as submission to TMLR
Table 2: Training cost in terms of time and the required number of weights based on the baseline.
Baseline MC-CLT Ensemble-based
Time 1 1.5−1.9 2.8−3.1
# weights 1 2.0−2.2 5
In addition, MC-CLT utilizes fewer weights and achieves faster training time, as shown in (Table 2). Table 2
presents a relative comparison with the other two methods based on the baseline. Specifically, when the
training time for PPO is set to 1, the MC-CLT method requires 1.5 to 1.9 times more training time compared
to the baseline. Similarly, the ensemble-based approach takes 2.8 to 3.1 times more time. Additionally, the
MC-CLT method requires approximately 2.0 to 2.2 times more weights, while the ensemble-based approach
uses 5 times as many parameters. We use 8 quantile bars for all experiments, as we empirically find that
using 8 or more quantile bars provides smiliar satisfactory performance.
While MC-CLT requires hyperparameters for the number of quantile bars and the variance network, it
necessitates fewer hyperparameters compared to previous methods (Lee et al., 2020; Mai et al., 2022). We
observe satisfactory performance when the number of quantiles is set to ≥8and the size of the variance
network equals to that of the distributional value function. In practice, we fix the number of quantiles at 8
for all experiments. Consequently, aside from the aspects controlling uncertainty, MC-CLT shares the same
hyperparameters as the baseline method. The detailed hyperparameter settings are discussed in Appendix A.
5.3 Seeking Accuracy vs. Seeking Exploration
MC-CLT and the ensemble-based methods seek accuracy by reducing weights for samples with high un-
certainty. This raises exploration concerns regarding states where the current distributional value function
fails to predict accurately. If such states are not given sufficient weight, the policy may never cover them
effectively. To address this concern, we conduct additional experiments where states with higher uncertainty
receive higher weights to encourage the agent to explore these states more. We compute the weight with
w(s) =E∗T+ 0.5, whereE=/summationtextN−1
i=0(qi(s)−q′
i(s))2, and run Algorithm 2 as in MC-CLT to find the
temperature T(whereleftandrightare adjusted in the opposite direction from accuracy seeking). Table 3
shows the comparisons between accuracy-seeking and exploration-seeking approaches. In all experiments,
the accuracy-seeking approach shows better performance. This is likely because the environments use highly
engineeredrewardfunctions, providingsufficientfeedbackwithoutrequiringadditionalexploration. Fortasks
where discovering novel states is important, such as in Atari games (Bellemare et al., 2013), the exploration-
seeking approach would have a positive effect on performance (Obando-Ceron et al., 2023; Schaul et al.,
2022).
6 Conclusion
We have presented a distributional reinforcement learning (DRL) method in that output quantiles approxi-
mate the value distribution as a normal distribution under the mild assumption for Markov Chain Central
Limit Theorem (Jones, 2004). Existing actor-critic algorithms that assess uncertainty typically employ mul-
tiple value functions (ensemble) to estimate variance. Given the presence of states with relatively high
variance, we propose an alternative approach to measure uncertainty. Rather than relying on the variance,
we evaluate how much the predicted quantiles are close to a normal.
Furthermore, when updating the distribution value function using the distributional Bellman optimality
operator, we observe that the variance estimates tend to increase with each timestep. The discount factor of
the operator yields a variance proportional to timestep. To address this, we guide the distributional value
function by utilizing analytically computed quantile bars derived from returns and the variance network.
Our method prioritizes accurate samples that are well-estimated by the current distributional value function,
thereby promoting improved policy performance. Overall, our method exhibits better performance compared
10Under review as submission to TMLR
to the baselines in our evaluations. Additionally, it leverages a reduced number of weights, leading to faster
training times than the ensemble-based method.
Although our proposed method has been primarily discussed in continuous tasks, where the assumption is
that the return distribution follows a normal distribution, it also potentially works in other scenarios, such
as tasks with discrete action spaces. Since we obtain the mean of the return through sampling, exactly as
in original actor-critic methods, it remains unbiased, and the uncertainty is calculated solely based on the
deviation from the target that our distributional value function is intended to fit. Additional insights and
details are provided in Appendix E.
Reproducibility Statement
We provide the hyperparameters used in our evaluations and all source code will be made publicly available.
Please refer to the submitted supplementary materials (appendix and code) for review.
References
Joshua Achiam. Spinning Up in Deep Reinforcement Learning, 2018. OpenAI.
AleksandrY.Aravkin, AnjuKambadur, AurélieC.Lozano, andRonnyLuss. Sparsequantilehuberregression
for efficient and robust estimation. ArXiv, abs/1402.4624, 2014.
Gabriel Barth-Maron, Matthew W. Hoffman, David Budden, Will Dabney, Dan Horgan, Dhruva TB, Alistair
Muldal, Nicolas Heess, and Timothy P. Lillicrap. Distributed distributional deterministic policy gradients.
CoRR, abs/1804.08617, 2018. URL http://arxiv.org/abs/1804.08617 .
M. G. Bellemare, Y. Naddaf, J. Veness, and M. Bowling. The arcade learning environment: An evaluation
platform for general agents. Journal of Artificial Intelligence Research , 47:253–279, jun 2013.
Marc G. Bellemare, Sriram Srinivasan, Georg Ostrovski, Tom Schaul, David Saxton, and Rémi Munos.
Unifying count-based exploration and intrinsic motivation. CoRR, abs/1606.01868, 2016. URL http:
//arxiv.org/abs/1606.01868 .
Marc G. Bellemare, Will Dabney, and Rémi Munos. A distributional perspective on reinforcement learning.
CoRR, abs/1707.06887, 2017a. URL http://arxiv.org/abs/1707.06887 .
Marc G. Bellemare, Ivo Danihelka, Will Dabney, Shakir Mohamed, Balaji Lakshminarayanan, Stephan
Hoyer, and Rémi Munos. The cramer distance as a solution to biased wasserstein gradients. CoRR,
abs/1705.10743, 2017b. URL http://arxiv.org/abs/1705.10743 .
Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech
Zaremba. Openai gym. CoRR, abs/1606.01540, 2016. URL http://arxiv.org/abs/1606.01540 .
Yuri Burda, Harrison Edwards, Amos J. Storkey, and Oleg Klimov. Exploration by random network distil-
lation.CoRR, abs/1810.12894, 2018. URL http://arxiv.org/abs/1810.12894 .
William R. Clements, Bastien Van Delft, Benoît-Marie Robaglia, Reda Bahi Slaoui, and Sébastien Toth.
Estimating risk and uncertainty in deep reinforcement learning, 2019. URL https://arxiv.org/abs/
1905.09638 .
Will Dabney, Mark Rowland, Marc G. Bellemare, and Rémi Munos. Distributional reinforcement learning
with quantile regression. CoRR, abs/1710.10044, 2017. URL http://arxiv.org/abs/1710.10044 .
Scott Fujimoto, Herke van Hoof, and David Meger. Addressing function approximation error in actor-critic
methods. CoRR, abs/1802.09477, 2018. URL http://arxiv.org/abs/1802.09477 .
ShixiangGu,TimothyLillicrap,ZoubinGhahramani,RichardETurner,andSergeyLevine. Q-prop: Sample-
efficient policy gradient with an off-policy critic. arXiv preprint arXiv:1611.02247 , 2016.
11Under review as submission to TMLR
Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy maximum
entropy deep reinforcement learning with a stochastic actor. CoRR, abs/1801.01290, 2018. URL http:
//arxiv.org/abs/1801.01290 .
Stephen C. Hora. Aleatory and epistemic uncertainty in probability elicitation with an example from haz-
ardous waste management. Reliability Engineering & System Safety , 54(2):217–223, 1996. ISSN 0951-8320.
doi: https://doi.org/10.1016/S0951-8320(96)00077-4. URL https://www.sciencedirect.com/science/
article/pii/S0951832096000774 . Treatment of Aleatory and Epistemic Uncertainty.
Peter J. Huber. Robust Estimation of a Location Parameter. The Annals of Mathematical Statistics , 35(1):
73 – 101, 1964. doi: 10.1214/aoms/1177703732. URL https://doi.org/10.1214/aoms/1177703732 .
Galin L. Jones. On the Markov chain central limit theorem. Probability Surveys , 1(none):299 – 320, 2004.
doi: 10.1214/154957804100000051. URL https://doi.org/10.1214/154957804100000051 .
Alex Kendall and Yarin Gal. What uncertainties do we need in bayesian deep learning for computer vision?
CoRR, abs/1703.04977, 2017. URL http://arxiv.org/abs/1703.04977 .
Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. CoRR, abs/1412.6980,
2014. URL http://dblp.uni-trier.de/db/journals/corr/corr1412.html#KingmaB14 .
Armen Der Kiureghian and Ove Ditlevsen. Aleatory or epistemic? does it matter? Structural
Safety, 31(2):105–112, 2009. ISSN 0167-4730. doi: https://doi.org/10.1016/j.strusafe.2008.06.020. URL
https://www.sciencedirect.com/science/article/pii/S0167473008000556 . Risk Acceptance and
Risk Communication.
Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. Simple and scalable predictive uncer-
tainty estimation using deep ensembles, 2016. URL https://arxiv.org/abs/1612.01474 .
Kimin Lee, Michael Laskin, Aravind Srinivas, and Pieter Abbeel. SUNRISE: A simple unified framework for
ensemble learning in deep reinforcement learning. CoRR, abs/2007.04938, 2020. URL https://arxiv.
org/abs/2007.04938 .
Vincent Mai, Kaustubh Mani, and Liam Paull. Sample efficient deep reinforcement learning via uncertainty
estimation. CoRR, abs/2201.01666, 2022. URL https://arxiv.org/abs/2201.01666 .
Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim Harley,
David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement learning. In Inter-
national conference on machine learning , pp. 1928–1937. PMLR, 2016.
Thomas M. Moerland, Joost Broekens, and Catholijn M. Jonker. Efficient exploration with double uncertain
value networks. CoRR, abs/1711.10789, 2017. URL http://arxiv.org/abs/1711.10789 .
D.A. Nix and A.S. Weigend. Estimating the mean and variance of the target probability distribution. In
Proceedings of 1994 IEEE International Conference on Neural Networks (ICNN’94) , volume 1, pp. 55–60
vol.1, 1994. doi: 10.1109/ICNN.1994.374138.
Johan Obando-Ceron, Marc G. Bellemare, and Pablo Samuel Castro. Small batch deep reinforcement learn-
ing, 2023. URL https://arxiv.org/abs/2310.03882 .
Martin L Puterman. Markov decision processes: discrete stochastic dynamic programming . John Wiley &
Sons, 2014.
Tom Schaul, André Barreto, John Quan, and Georg Ostrovski. The phenomenon of policy churn, 2022. URL
https://arxiv.org/abs/2206.00730 .
John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region policy
optimization. In Francis Bach and David Blei (eds.), Proceedings of the 32nd International Conference on
Machine Learning , volume 37 of Proceedings of Machine Learning Research , pp. 1889–1897, Lille, France,
07–09 Jul 2015a. PMLR. URL http://proceedings.mlr.press/v37/schulman15.html .
12Under review as submission to TMLR
John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel. High-dimensional
continuous control using generalized advantage estimation. arXiv preprint arXiv:1506.02438 , 2015b.
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimiza-
tion algorithms. CoRR, abs/1707.06347, 2017. URL http://arxiv.org/abs/1707.06347 .
Rahul Singh, Keuntaek Lee, and Yongxin Chen. Sample-based distributional policy gradient. CoRR,
abs/2001.02652, 2020. URL http://arxiv.org/abs/2001.02652 .
Alexander L. Strehl and Michael L. Littman. A theoretical analysis of model-based interval estimation. In
ICML, pp. 856–863, 2005. URL https://doi.org/10.1145/1102351.1102459 .
Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction . MIT press, 2018.
Philip Thomas. Bias in natural actor-critic algorithms. In International conference on machine learning , pp.
441–448. PMLR, 2014.
Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control. In
2012 IEEE/RSJ International Conference on Intelligent Robots and Systems , pp. 5026–5033, 2012. doi:
10.1109/IROS.2012.6386109.
Christopher JCH Watkins and Peter Dayan. Q-learning. Machine learning , 8(3):279–292, 1992.
Yuguang Yue, Zhendong Wang, and Mingyuan Zhou. Implicit distributional reinforcement learning. CoRR,
abs/2007.06159, 2020. URL https://arxiv.org/abs/2007.06159 .
A Hyperparameters
Deep reinforcement learning algorithms pose a significant challenge for evaluation due to the inherent insta-
bility and stochasticity during training. To ensure a fair comparison with other algorithms, we attempted to
maintain a consistent set of hyperparameters as much as possible. We follow Spinning Up (Achiam, 2018)
default settings for learning rate, optimizer, etc. All policies have a two-layer tanhnetwork with 64 x 32
units, and all of the value function and the distributional value function has a two-layer ReLUnetwork with
64 x 64 units or 128 x 128 units for all environments.
Although Lee et al. (2020) suggest the formula to compute the uncertainty weight w(s) =σ(−E∗T) +wmin,
it seems that finding the reasonable value for Tis not intuitive for various tasks. (Note that Eis computed
as the normality error of the distributional value function in our method and Eis replaced with the standard
deviation of the value functions in Lee et al. (2020)). We instead set a target uncertainty weight and perform
a parametric search by adjusting Tto achieve that target weight (Appendix B). This process substitutes T
with the target weight wtaras a hyperparameter. wtaris chosen from wtar∈{0.85,0.9}, andwminis chosen
fromwmin∈{0.4,0.5,0.6}.
MC-CLT has the number of quantile output nodes of the distributional value function as a hyperparameter.
We examine how performance is affected by the number of quantile bars, with values such as N= 4,8,12,
and20. We observe satisfactory performance when the number of quantile bars is set to N≥8, so we fix
the number of quantiles at 8 for all experiments. Also, the ensemble-based method introduces the ensemble
size as a hyperparameter. As Lee et al. (2020) and Mai et al. (2022) demonstrate that ensemble size = 5
shows the sufficient results, we set the ensemble size to 5for these models.
In order to ensure a fair comparison with the ensemble methods, we selected 3 – 5 best configurations, each
of which is trained with ten different seeds. The numbers reported in the tables correspond to the results
from the last 10 runs.
13Under review as submission to TMLR
Figure 4: Changes in standard deviation over timestep. We train ensemble value functions and monitor the
changes in standard deviations. The graphs indicate that the standard deviation consistently decreases as
the timestep progresses.
B Algorithms In Detail
Instead of manually tuning Tfor each task, we dynamically adjust Tsuch that the average uncertainty lies
within the range [wtar−ϵ,wtar+ϵ]. The following algorithm illustrates the parametric search employed to
find such a T. While MC-CLT utilizes the error set E, the ensemble-based methods leverage the standard
deviation set. The term 2(1−wmin)ensures that each uncertainty weight lies within the range (wmin,1.0).
Algorithm 2 Parametric search to find the temperature T
1:Input:target weight wtar, minimum weight wmin, error setE
2:Initializeleft= 0,right = 212
3:whileleft≤right do
4:T= (left+right )/2
5:W= 2(1−wmin)σ(−E∗T) +wmin
6:Compute the mean of uncertainty weight wavg= Avg(W)
7: ifwavgis in the range [wtar−ϵ,wtar+ϵ]then
8: UseTto compute the uncertainty weight
9: break
10: else ifwavg≥wtar+ϵthen
11:left=T
12: else
13:right =T
14: end if
15:end while
14Under review as submission to TMLR
Mean
Figure 5: The green line depicts a non-normal complex distribution. The blue bars represent normal quantile
bars, which are analytically computed using the mean of the complex distribution, with a variance obtained
from a variance network. The orange line illustrates the line version of the normal distribution derived from
the quantile bars.
C Variance Network
In practice, the variance network (Kendall & Gal, 2017) is trained to predict log variance lnσ2
ψinstead of
predicting variance directly. This avoids a division by zero or a negative value in the log term in Equation
(6). However, after predicting log variance, it is necessary to retrieve the actual variance value through the
exponential operation, i.e., σ2
ψ= exp(lnσ2
ψ).
However, in cases of environments having the large observation space, such as Ant-v2 or HumanoidStandup-
v2, we frequently observe overflow issues during the exp operation, particularly in the early training epochs.
Thus, we directly predict the variance by limiting the minimum value σ2
ψ= max(ϵ,σ2
ψ). We setϵ= 0.0001,
and this resolves the overflow issue for the large observation space environments. We also compared the
outcomes of the two methods on tasks with a smaller observation space. We couldn’t see any meaningful
difference between the two methods, so we conducted experiments for all environments by directly predicting
the variance.
D Standard Deviation Change
Certain states display relatively higher variances compared to others. For instance, in fragile control tasks
like Walker2d-v2, the states close to the terminal state typically exhibit higher variances. This is because
once the agent overcomes the terminal states, several more successful actions are likely to be generated,
increasing the sum of rewards (return). Additionally, the closer the state is to the initial timestep, the
higher the variance. This correlation is apparent when comparing the changes in return from the initial and
intermediate timesteps.
The graphs (Figure 4) have been obtained by applying exponential smoothing to the standard deviation of
the trained value functions (the ensemble-based method), Notably, the standard deviation is observed to
decrease over the timestep.
Motivated by these observations, we propose a novel approach. Rather than decreasing the uncertainty
weight of states with higher variance, we suggest a method for measuring the accuracy of the value function
prediction. We then use this measure to obtain the uncertainty weight.
E Normality Discussion
This section addresses scenarios where tasks deviate from the normality condition. Our method does not
compromise the policy update, even when the normality condition is not satisfied. Since we first compute
the mean of the target quantiles from sampled episodes and then analytically compute the quantile bars (the
variance is derived from the variance network), the mean represented by these quantile bars is unbiased.
In detail, suppose a complex green line distribution Dis the true return distribution of a state sunder a
policyπ, as shown in Figure 5. A standard value function Vπ, which predicts a single value (return) Vπ(s)
15Under review as submission to TMLR
for a state, estimates the mean of this complex distribution D, indicated by the dotted line, through sampled
episodes.
Rather than just outputting the single value, our distributional value function VD,πis trained to output blue
quantile bars for sunder the same policy π, representing a normal distribution (the orange line). Although
VD,π(s)represents the orange normal distribution, distinct from the green one, the mean of these quantile
bars is also located at the dotted line position because the sampling mechanism are both the same. In other
words,1
N/summationtextN−1
i=0qi(s) =Vπ(s), whereVD,π(s) ={q0(s),q1(s),...,qN−1(s)}. Given that this mean is used to
update the policy, this alignment confirms that our approach without uncertainty weights is unbiased.
Thequestionarises: What happens when we use the uncertainty weights to update the policy in tasks deviating
from the normality condition? In this case, we argue that our method becomes a special case of Exploration
by Random Network Distillation (RND) (Burda et al., 2018), in which a random neural network is provided,
and another neural network is fitted to this random network. While the model is fitted, an exploration bonus
is computed based on the differences between the two models. Differing from RND’s approach of providing
a random neural network, we provide a normal quantile target, expecting that VDwill also produce quantile
bars indicative of a normal distribution. We then measure how closely VD’s predictions mirror the shape of
a normal distribution.
Both approaches rely on the premise that the difference diminishes with sufficient learning. The distinction
between RND and our method is that RND utilizes uncertainty as a bonus added to the reward, seeking
novelty, whereas our approach is a risk-averse strategy by reducing the impact of gradients from high-
uncertainty samples. These different search strategies can be chosen depending on the problem. Given
that the reward function in control tasks, such as MuJoCo, is highly engineered, being risk-averse is more
effective than seeking novelty, as the feedback from the reward function is sufficient, unlike sparse reward
settings. We actually conduct experiments to check whether encouraging the agent to visit high-uncertainty
states is beneficial for our environments (Table 3), and find that seeking exploration does not outperform in
environments with highly engineered rewards.
F Limitation
Our current method is primarily effective in stochastic continuous tasks with dense rewards based on the
principle of MC-CLT. However, in environments where a value distribution is non-normal, alternative meth-
ods may be more effective for accurately capturing these non-normal value distributions. It’s important to
note that any limitations in our current approach don’t lead to significant failures, as detailed in Appendix E.
G Potential Research Directions
Theuseofourmethoddoesnotexcludethepossibilityofutilizingtheensembletechnique. Infact, bycombin-
ing our method with the ensemble, it becomes possible to leverage multiple distributional value functions.
However, simply averaging the uncertainties from each function has shown no performance improvement
compared to our method alone. Besides weighting policy updates, exploring alternative uses of uncertainties
from each distributional value function would be an intriguing avenue for future research. For instance, in
methods like Soft Actor-Critic (SAC) (Haarnoja et al., 2018) or Twin Delayed DDPG (TD3) (Fujimoto et al.,
2018), where the minimum value is selected among two Q functions, the Q function with lower uncertainty
could be chosen to utilize a more accurate value because our uncertainty weight represents the accuracy of
the network.
16Under review as submission to TMLR
Table 3: Seeking Accuracy vs. Seeking Exploration Table: We compute the uncertainty weight (our method)
based on Equation 9. However, this raises concerns regarding exploration. To address this, we conduct an
experiment using Equation (2) to compute the exploration weight, ensuring that states with high uncertainty
are visited more frequently. Each entry represents an average return and a standard error, derived from 30
different settings, with each setting running for 100 episodes.
BipedalWalker Hopper HalfCheetah Ant
Exploration TRPO 34±3.3 2328±34 2459 ±32 1663 ±16
MC-CLT TRPO (ours) 195±2.6 2927±27 3172 ±43 3329 ±20
Swimmer Walker2d InvertedDoublePendulum LunarLander(C)
Exploration TRPO 36±2.2 1086±33 7628 ±112 179 ±4.4
MC-CLT TRPO (ours) 118±0.2 2904±23 8404 ±93 258 ±1.8
BipedalWalker Hopper HalfCheetah Ant
Exploration PPO 70±3.6 2727±28 2756 ±37 1315 ±10
MC-CLT PPO (ours) 236±2.3 3230±22 3110 ±39 1900 ±13
Swimmer Walker2d InvertedDoublePendulum LunarLander(C)
Exploration PPO 44±0.8 916±24 9027 ±54 230 ±2.8
MC-CLT PPO (ours) 122±0.1 2716±33 9225 ±36 278 ±1.2
17