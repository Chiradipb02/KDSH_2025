Published in Transactions on Machine Learning Research (11/2024)
Neural Graph Reasoning: A Survey on Complex Logical
Query Answering
Hongyu Ren*1hyren@cs.stanford.edu
Mikhail Galkin*2mikhail.galkin@intel.com
Zhaocheng Zhu3zhaocheng.zhu@umontreal.ca
Jure Leskovec1jure@cs.stanford.edu
Michael Cochez4m.cochez@vu.nl
*Equal contribution
1Stanford University2Intel AI Lab3Mila - Québec AI Institute and Université de Montréal4Vrije Universiteit
Amsterdam and Elsevier discovery lab, Amsterdam, the Netherlands
Reviewed on OpenReview: https: // openreview. net/ forum? id= xG8un9ZbqT
Abstract
Complex logical query answering (CLQA) is a recently emerged task of graph machine learn-
ing that goes beyond simple one-hop link prediction and solves the far more complex task of
multi-hop logical reasoning over massive, potentially incomplete graphs. The task received
significant traction in the community; numerous works expanded the field along theoretical
and practical axes to tackle different types of complex queries and graph modalities with
efficient systems. In this paper, we provide a holistic survey of CLQA with a detailed tax-
onomy studying the field from multiple angles, including graph types (modality, reasoning
domain, background semantics), modeling aspects (encoder, processor, decoder), supported
queries (operators, patterns, projected variables), datasets, evaluation metrics, and appli-
cations. Finally, we point out promising directions, unsolved problems and applications of
CLQA for future research.
Contents
1 Introduction 2
2 Preliminaries 4
2.1 Types of Graphs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4
2.2 Basic Graph Query Answering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5
2.3 Approximate Graph Query Answering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6
3 A Taxonomy of Query Reasoning Methods 7
4 Graphs 9
4.1 Modality . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9
1Published in Transactions on Machine Learning Research (11/2024)
4.2 Reasoning Domain . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9
4.3 Background Semantics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11
5 Modeling 13
5.1 Encoder . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14
5.2 Processor . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17
5.3 Decoder . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23
5.4 Computation Complexity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25
6 Queries 26
6.1 Query Operators . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26
6.2 Query Patterns . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32
6.3 Projected Variables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34
7 Datasets and Metrics 36
7.1 Evaluation Setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36
7.2 Query Types . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36
7.3 Training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37
7.4 Inference . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38
7.5 Metrics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40
8 Applications 41
9 Summary and Future Opportunities 42
A Definitions and Theoretical Foundations 56
A.1 Types of Knowledge Graphs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56
A.2 Basic Approximate Graph Query Answering . . . . . . . . . . . . . . . . . . . . . . . . . . . . 57
A.3 Graph Query Answering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58
A.4 Triangular Norms and Conorms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61
A.5 Graph Representation Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 62
B CLQA vs KGQA 62
1 Introduction
Graph databases (graph DBs) are key architectures to capture, organize and navigate structured relational
information over real-world entities. Unlike traditional relational DBs storing information in tables with
a rigid schema, graph DBs store information in the form of heterogeneous graphs, where nodes represent
entities and edges represent relationships between entities. In graph DBs, a relation (i.e. heterogeneous
connection between entities) is a first-class citizen. With the graph structure and a more flexible schema,
graph DBs allow for an efficient and expressive way to handle higher-order relationships between distant
2Published in Transactions on Machine Learning Research (11/2024)
UvA UofT Stanford 
UdeM NYU Welling 
Deep 
Learning Turing 
Award 
Hinton 
Knuth 
LeCun Bengio 
?u ⋀win
ﬁeld university At what universities  do the Turing Award  winners  in the ﬁeld  of Deep Learning  work? 
SELECT ?uni  WHERE 
{
  TuringAward win     ?person  .
  DeepLearning field   ?person  .
  ?person  university  ?uni     . 
}
win
ﬁeld 
university collab 
given edge 
predicted 
edge SPARQL query (edge traversal) 
Neural query execution (+ link prediction) 
UofT UdeM NYU Answer set UofT 
UofT UdeM NYU Easy Hard b c
da
Figure 1: A complex logical query aand its execution over an incomplete graph b. Symbolic engines like
SPARQL cperform edge traversal and retrieve an incomplete set of easyanswers directly reachable in the
graph, i.e., { UofT}. Neural query execution drecovers missing ground truth edges (dashed) and returns
an additional set of hardanswers { UdeM,NYU} unattainable by symbolic methods.
entities, especially navigating through multi-hop hierarchies. While traditional DBs require expensive join
operations to retrieve information, graph DBs can directly traverse the graph and navigate through links
more efficiently. Due to its capabilities, graph databases serve as the backbone of many critical industrial
applications including question answering in virtual assistants (Flint, 2021; Ilyas et al., 2022), recommender
systems in marketplaces (Dong, 2018; Hamad et al., 2018), social networking in mobile applications (Bronson
et al., 2013), and fraud detection in financial industries (Tian et al., 2019; Pourhabibi et al., 2020).
One of the most important tasks of graph DBs is to perform complex query answering. The goal is to retrieve
the answers of a given input query from the graph database. Given the query, graph DBs first translate and
optimize the query into a more efficient graph traversal pattern with a query planner, and then execute the
pattern on the graph database to retrieve the answers from the graph storage using the query executor. The
storage compresses the graphs into symbolic indexes suitable for fast table lookups. Querying is thus fast
and efficient under the assumption of completeness, i.e., stored graphs have no missing edges.
However, most real-world graphs are notoriously incomplete, e.g., in Freebase, 93.8% of people have no
place of birth and 78.5% have no nationality (Mintz et al., 2009), and about 68% of people do not have
any profession (West et al., 2014), while in Wikidata, about 50% of artists have no date of birth (Zhang
et al., 2022a), and only 0.4% of known buildings have information about height (Ho et al., 2022). In light of
incompleteness, naïvely traversing the graph to find answers leads to a significant miss of relevant results, and
the issue further exacerbates with an increasing query complexity. This inherently hinders the application of
graph databases. Link prediction aims to predict missing information, but is a challenging task. Prior works
predict links by learning a latent representation of entities or links (Bordes et al., 2013; Yang et al., 2015;
Trouillon et al., 2016; Sun et al., 2019) or mining rules (Galárraga et al., 2013; Xiong et al., 2017; Lin et al.,
2018; Qu et al., 2021). While it is possible to use one-hop link predictors to materialize all predicted facts
(above certain confidence threshold) and run deterministic query answering pipelines, the computational
complexity of this operation is quadratic in the number of entities and is prohibitively expensive for any
real-world graph. Furthermore, these approaches rank possible candidates for completion, meaning that they
do not tell which of the completions could be traversed. Also reasoning can be used to complete specific
information, but there is always a trade-off between possibly incomplete results and decidability – with a
3Published in Transactions on Machine Learning Research (11/2024)
denser graph, some SPARQL entailment regimes (Hawke et al., 2013) do not guarantee that query execution
terminates in finite time.
On the other hand, recent advances in graph machine learning enabled expressive reasoning over large graphs
in a latent space without facing decidability bottlenecks. The seminal work of Hamilton et al. (2018) on
Graph Query Embedding (GQE) laid foundations of answering complex, database-like logical queries over
incomplete KGs where inferring missing links during query execution is achieved via parameterization of
entities, relations, and logical operators with learnable vector representations and neural networks. For
the incomplete knowledge graph in (Fig. 1), given a complex query “At what universities do the Turing
Award winners in the field of Deep Learning work?” , traditional symbolic graph DBs (SPARQL- or Cypher-
like) would return only one answer ( UofT), reachable by edge traversal. In contrast, neural query embedding
parameterizesthegraphandthequerywithlearnablevectorsintheembeddingspace. Neuralqueryexecution
is akin to traversing the graph and executing logical operators in the embedding space that infers missing links
and enriches the answer set with two more relevant answers, UdeMandNYU, unattainable by symbolic DBs.
Since then, the area has seen a surge of interest with numerous improvements of supported logical operators,
query types, graph modalities, and modeling approaches. In our view, those improvements have been rather
scattered, without an overall aim. There still lacks a unifying framework to organize the existing works and
guide future research. To this end, we present one of the first holistic studies about the field. Conceptually,
we devise the taxonomy of CLQA methods that includes various aspects of query answering and is supposed
to be a bridge between data management and ML communities. The taxonomy (Section 3) classifies existing
works along three main axes, i.e., (i)Graphs (Section 4 – logical formalisms behind the underlying graph
and its schema) (ii) Modeling (Section 5 – what are the neural approaches to answer queries) (iii) Queries
(Section 6 – what queries can be answered). We then discuss Datasets and Metrics (Section 7 – how we
measure the performance of query answering). Each of these dimensions is further divided into fine-grained
aspects. Finally, wediscussCLQAapplications(Section8)andsummarizeopenchallengesforfutureresearch
(Section 9).
Related Work. While there exist insightful surveys on general graph machine learning (Chami et al., 2022),
simple link prediction in KGs (Ali et al., 2021; Chen et al., 2023), and logic-based link prediction (Zhang
et al., 2022b; Delong et al., 2024), the complex query answering area remained uncovered so far. With our
work, we close this gap and provide a holistic view on the state of affairs in this emerging field. We also
elaborate on the similarities and differences between CLQA and KG-based Question Answering (KGQA) (a
different subfield of NLP) in Appendix B.
2 Preliminaries
Here, we discuss the foundational terms and preliminaries often seen in the database and graph learning
literature. The rest of the preliminaries is deferred to the Appendix A elaborating on the mapping to
structured languages like SPARQL, formalizing approximate graph query answering, logical operators, and
fuzzy logic. The full hierarchy of definitions is presented in Fig. 3.
2.1 Types of Graphs
Here we introduce different types of graphs relevant for this research area. Our definitions are adaptations
from those in (Hogan et al., 2021, ch. 2). We begin by defining a set with all elements used in our graph
Definition 2.1 ( Con)Conis an infinite set of constants.1
Definition 2.2 ((Standard / Triple based) Knowledge Graph) We define a knowledge graph (KG)
G= (E,R,S), whereE⊂Conrepresents a set of nodes (entities), R⊂ Conis a set of relations (edge types),
andS⊂(E×R×E )is a set of edges (triples)2. Each edge s∈Son a KGGdenotes a statement (or fact)
(es,r,eo)in a triple format or r(es,eo)as a formula in first-order logic (FOL) form, where es,eo∈Edenote
1In case the constants do not include representations for the real numbers, this set would be countably infinite.
2There might be an overlap between EandR, i.e., some constants might be used as an edge and as a node.
4Published in Transactions on Machine Learning Research (11/2024)
Hint on Cambridge E dinbur gh education education 
Hint on Cambridge E dinbur gh education education 
Bachelor PhD degree degree education_bachelor 
education_phd Hint on E dinbur gh 
PhD Bachelor Cambridge 
Triple-based KG Hyper-Relational KG 
(with relation-entity qualiﬁers) Hypergraph KG 
(with ad-hoc hyperedge types) Hint on Cambridge E dinbur gh education education 
Hint on Cambridge E dinbur gh education education 
Bachelor PhD degree degree education_bachelor 
education_phd Hint on E dinbur gh 
PhD Bachelor Cambridge 
Figure 2: An example of KG modalities. Triple-only graphs have directed binary edges between nodes.
Hyper-relational graphs allow key-value relation-entity attributes over edges. Hypergraphs consist of hyper-
edges composed of multiple nodes.
the subject and object entities, and r∈Rdenotes the relationship between the subject and the object. Often,
the graph is restricted such that the set of nodes and edges must be finite, we will make this assumption
unless indicated otherwise.
For example, one statement on the KG in Fig. 1 bis(Hinton, university, UofT) or
university(Hinton, UofT) . Note all relations are binary on KGs, i.e., each relation involves exactly two
entities. This is also the choice made for the resource description framework (RDF) (Brickley et al., 2014)
that defines a triple as a basic representation of a fact, yet RDF allows a broader definition of triples, where
the object can be literals including numbers and timestamps (detailed in the following paragraphs).
We elaborate on hyper-relational KGs and hypergraph KGs in Appendix A.1.
2.2 Basic Graph Query Answering
We base our definition of basic graph queries on the one from (Hogan et al., 2021, section 2.2.1 basic graph
patterns), but adapt it to our graph formalization. Other definitions can be found in Appendix A.2.
Definition 2.3 (Term and Var) Given a knowledge graph G= (E,R,S)(or equivalentG= (E,R,S,L)),
we define the set of variables Var={v1,v2,...}which take values from Con, but is strictly disjoint from it.
We call the union of the variables and the constants the terms: Term =Con∪Var
Basic Graph Query
(Definition 2.4)
•Conjunctive
•Tree/DAG/Cycle
•Multi-hopRegular Path Query
Definition A.7Graph Query (extend)
•Union / UCQ (A.6)
•Optional (A.6)
•Join (A.11)
•Projection (A.9)
Basic Graph Query
Answering
Definition 2.5Regular Path Query
Answering
Definition A.8Graph Query
Answering
Definition A.5
Basic Approximate
Graph Query
Answering
Definition A.4Approximate Graph Query Answering
Definition 2.6
Figure 3: A hierarchy of definitions from
Graph Query to Graph Query Answering and
more general Approximate Graph Query Answering .With these concepts in place a Basic Graph Query
is defined as follows:
Definition 2.4 (Basic Graph Query) A ba-
sic graph query is a 4-tuple Q= (E′,R′,S′,S′)
(equivalently a 5-tuple Q= (E′,R′,S′,S′,L′),
with literals), with E′⊂Terma set of node
terms,R′⊂Terma set of relation terms, and
S′,S′⊂E′×R′×E′, two sets of edges (or equiv-
alent to how graph edges are defined with literals).
The query looks like two (small) graphs; one formed
by the edges inS′, and another by the edges in S′.
The former set includes edges that must be matched
in the graph to obtain answers to the query, while the latter contains edges that must not be matched (i.e.,
atomic negation).
5Published in Transactions on Machine Learning Research (11/2024)
With VarQ= (E′∪R′)∩Var(all variables in the query), the answer to the query is defined as follows.
Definition 2.5 (Basic Graph Query Answering) Given a knowledge graph Gand a queryQas defined
above, an answer to the query is any mapping µ:VarQ→Consuch that replacing each variable viin the
edges inS′withµ(vi)results in an edge in S,andreplacing each variable vjin the edges inS′withµ(vj)
results in an edge that is notinS. The set of all answers to the query is the set of all such mappings.
For triple-based KGs, each edge (a,R,b )in the edge setS′(respectivelyS′) of a query can be seen as a
relation projection R(a,b)(resp.¬R(a,b)),i.e., as binary functions. Now, because the conjunction ( i.e., all)
of these relation projections (resp. the negation) have to be true, we also call these conjunctive queries with
atomic negation (CQ neg). The SPARQL query language defines basic graph patterns (BGPs). These closely
resemble our basic graph query for RDF graphs, but with S′=∅,i.e., they do not support atomic negation
but only conjunctive queries (CQ). We could analogously create CQ and CQ negclasses for the other KG
types.
In Fig. 1cwe provide an example of a Basic Graph Query expressed in the form of a SPARQL query. This
corresponds to our formalism:
Q= ({TuringAward ,?person,DeepLearning ,?uni}, (# Node TermsE′)
{win,field,university}, (# Relation Terms R′)
{(TuringAward ,win,?person ),(DeepLearning ,field,?person ),(?person,university,?uni)},(S′)
∅) (S′)
A possible answer to this query is the following partial mapping: µ1={(?person,Hinton ),(?uni,UofT )}
This is also the only answer, so the set of all answers is {µ1}
If we want to exclude people who have worked together with Welling, then we modify the query as follows:
Q= ({TuringAward ,?person,DeepLearning ,?uni},
{win,field,university},
{(TuringAward ,win,?person ),(DeepLearning ,field,?person ),(?person,university,?uni)},
{(?person,collab,Welling )} {(?person,collab,Welling )} {(?person,collab,Welling )})
The key difference here is that we add {( ?person,collab,Welling)} toS′, resulting in the empty answer set.
2.3 Approximate Graph Query Answering
Definition 2.6 (Approximate Graph Query Answering) Given a knowledge graph G, subgraph of a
complete, but not observable knowledge graph ˆG, a query formalism, anygraph queryQaccording to that
formalism, and the scoring domain R.
An approximate graph query answer to the query Qis a function fwhich maps every possible mapping
(µ:VarQ→Con) toR.
Note there that the variables are not always mapped to nodes which occur in the graph. It is well possible
that the query contains an aggregation function which results in a literal value.
In the neural logical query answering literature (starting from Hamilton et al. (2018); Ren et al. (2020)), we
can find the concepts of easyandhardanswers. This refers to whether the answers can be found by only
having access toGor not.
Definition 2.7 (Easy and Hard answers) Given a knowledge graph G, subgraph of a larger unobservable
graph ˆG, and a queryQ. Easy and hard answers are defined in terms of exact query answering (Defini-
tion A.5). The set of easyanswers is the intersection of the answers obtained from G, and those from ˆG.
The set of hardanswers is the set difference between the answers from ˆGand those fromG.
6Published in Transactions on Machine Learning Research (11/2024)
Note the asymmetry in the definitions. Easy answers are those that can be found in both Gand ˆG. Hard
answers are those that can be found only in ˆGbutnotinG. For example, the query in Fig. 1 has one easy
answer UofTas it is reachable by traversing the original graph Gand twohardanswers UdeM,NYUsince they
require predicting missing links (missing in Gbut true links in the non-observable full graph ˆG). For Basic
Graph Queries (Definition 2.4), all easy answers can also be found from ˆG. However, for some more complex
query types ( e.g., these which allow negation) there could be answers found in Gwhich are not found in ˆG.
We call these answers false positives in the context of answering over ˆG.
3 A Taxonomy of Query Reasoning Methods
In the following sections, we devise a taxonomy of query answering works. We categorize existing and
envisioned approaches along three main directions: (i) Graphs – what is the underlying structure against
which we answer queries; (ii) Modeling – how we answer queries and which inductive biases are employed;
(iii)Queries – what we answer, what are the query structures and what are the expected answers. The
taxonomy is presented in Fig. 4. In the following sections, we describe each direction in detail and illustrate
them with examples covering the currently existing CLQA literature (more than 50 papers).
7Published in Transactions on Machine Learning Research (11/2024)
CLQAGraphs
Section 4Modality
Section 4.1Triple KGs
Hyper-Relational KGs
Hypergraphs
Multi-modal KGs
Reasoning Domain
Section 4.2Discrete
Discrete + Time
Discrete + Continuous
Background
Semantics
Section 4.3Facts-only (ABOX)
+ Class Hierarchies
+ Complex Axioms (TBOX)
Modeling
Section 5Encoder
Section 5.1Shallow Embedding
Transductive Encoder
Inductive Encoder
Processor
Section 5.2Neural
Neuro-SymbolicGeometric
Probabilistic
Fuzzy LogicDecoder
Section 5.3Non-parametric
Parametric
Queries
Section 6Query Operators
Section 6.1Conjunctive (∃∧)
EPFO (∃∧∨ )
EPFO + Negation (∃∧∨¬ )
Regex and Property Paths (∃∧
∨+∗? !)
Filter
Aggregations
OPTIONAL, Solution Modifiers
Query Patterns
Section 6.2Path
Tree
Arbitrary DAGs
Cyclic patterns
Projected Variables
Section 6.3Zero Variables
One Variable
Multiple Variables
Figure 4: The Neural Query Engine Taxonomy consists of three main branches – Graphs, Modeling, and
Queries. We describe each branch in more detail including prominent examples in the relevant sections.
8Published in Transactions on Machine Learning Research (11/2024)
4 Graphs
TheGraphscategory covers the underlying graph structure ( Gin Definition A.4) against which complex
queries are sent and the answers are produced. Understanding the graph, its contents and modeling
paradigms is crucial for designing query answering models. To this end, we propose to analyze the un-
derlying graph from three aspects: Modality ,Reasoning Domain , andBackground Semantics .
4.1 Modality
We highlight four modalities common for KGs and graph databases: standard triple-based KGs ,hyper-
relational KGs , andhypergraph KGs . The difference among the three modalities is illustrated in Fig. 2.
Additionally, we outline multi-modal KGs that contain not just a graph of nodes and edges, but also text,
images, audio, video, and other data formats linked to the underlying graph explicitly or implicitly.
We categorize the literature along the Modality aspect in Table 1. To date, most query answering approaches
operatesolelyon triple-based graphs. Amongapproachessupporting hyper-relational graphs, weareonly
aware of StarQE (Alivanistos et al., 2022) that incorporates entity-relation qualifiers over labeled edges and
its extension NQE (Luo et al., 2023). We posit that the hyper-relational model might serve as a theoretical
foundationoftemporalqueryansweringapproachessincetemporalattributesareinfactcontinuouskey-value
edge attributes. To date, we are not aware of complex query answering models supporting hypergraphs or
multi-modal graphs. We foresee them as possible area of future research in the area.
Table 1: Complex Query Answering approaches categorized under Modality .
Triple-only Hyper-Relational Hypergraph Multi-modal
GQE(Hamilton et al., 2018) , GQE w hash (Wang et al., 2019) , CGA (Mai
et al., 2019) , TractOR (Friedman & Van den Broeck, 2020) , Query2Box (Ren
et al., 2020) , BetaE (Ren & Leskovec, 2020) , EmQL (Sun et al., 2020) , MPQE
(Daza & Cochez, 2020) , Shv(Gebhart et al., 2023) , Q2B Onto (Andresel et al.,
2021), RotatE-Box (Adlakha et al., 2021) , BiQE (Kotnis et al., 2021) , HyPE
(Choudhary et al., 2021b) , NewLook (Liu et al., 2021) , CQD (Arakelyan et al.,
2021), PERM (Choudhary et al., 2021a) , ConE (Zhang et al., 2021b) , LogicE
(Luusetal.,2021) ,MLPMix (Amayuelasetal.,2022) ,FuzzQE (Chenetal.,2022) ,
GNN-QE (Zhu et al., 2022) , GNNQ (Pflueger et al., 2022) , SMORE (Ren et al.,
2022), KGTrans (Liu et al., 2022) , LinE(Huang et al., 2022b) , Query2Particles
(Bai et al., 2022) , TAR(Tang et al., 2022) , TeMP (Hu et al., 2022) , FLEX (Lin
et al., 2022) , TFLEX (Lin et al., 2023) , NodePiece-QE (Galkin et al., 2022b) ,
ENeSy (Xu et al., 2022) , GammaE (Yang et al., 2022a) , NMP-QEM (Long
et al., 2022) , QTO (Bai et al., 2023c) , SignalE (Wang et al., 2022) , LMPNN
(Wang et al., 2023e) , Var2Vec (Wang et al., 2023a) , CQDA(Arakelyan et al.,
2023), Query2Geom (Sardina et al., 2023) , SQE(Bai et al., 2023b) , RoConE
(He et al., 2023) , FIT(Yin et al., 2023b) , LitCQD (Demir et al., 2023) , CylE
(Nguyen et al., 2023b) , LARK (Choudhary & Reddy, 2023) , WFRE (Wang et al.,
2023d), BiDAG (Xu et al., 2023a) , NRN(Bai et al., 2023a) , Query2Triple (Xu
et al., 2023b) , SCoNe (Nguyen et al., 2023a) , CQD Onto (Andresel et al., 2023) ,
UnRavL (Cucumides et al., 2024) , UltraQuery (Galkin et al., 2024)StarQE (Alivanistos et al., 2022) ,
NQE(Luo et al., 2023)None None
4.2 Reasoning Domain
Following Definition 2.5, a query Qincludes constants Con, variables Var, and returns answers as mappings
µ:VarQ→Con. ByReasoning Domain we understand the space of possible constants that query answering
models can reason about. We highlight three common domains ( Discrete,Discrete + Time ,Discrete +
Continuous ), illustrate them in Fig. 5, and categorize existing works in Table 2. Each subsequent domain
is a superset of the previous domains, e.g.,Discrete + Continuous includes the capabilities of Discrete and
Discrete + Time and expands the space to continuous inputs and outputs.
In theDiscrete domain, constants, variables, and answers can be entities Eand (or) relation types Rof the
KG, Con⊆E∪R ,Var⊆E∪R ,µ⊆E∪R . That is, queries may only contain entities (or relations) and
their answers are only entities (or relations). For example, a query ?x:education (Hinton,x)in Fig. 5 can
9Published in Transactions on Machine Learning Research (11/2024)
Hint on Cambridge E dinbur gh 
education education Hint on Cambridge E dinbur gh 
education education 
start: 1972 
end:   1975 
Discrete 
(entities + relations only) Discrete + Time 
(reasoning over timestamps) Discrete + Continuous 
(including literals, e.g., numbers) Hint on Cambridge E dinbur gh 
education education 
established: 1583 
students:       35,375 established: 1209 
students:       24,450 start: 1967 
end:   1970 
?x education 
?x education 
?x education 
students < 30,000 
?x: education(Hinton, x) ?x: education(year == 1973)(Hinton, x)      ?x: education(Hinton, x) ⋀ 
         x.students < 30,000 year == 1973 
Figure 5: Reasoning Domains. The Discrete domain only allows entities and relations as constants, variables,
and answers. The Discrete + Time domain extends the space to discrete timestamps and time-specific
operators. The Discrete + Continuous domain allows continuous inputs (literals) and outputs.
only return two entities {Edinburgh,Cambridge}as answers. Conceptually, the framework allows relation
typesr∈Rto be variables as well. For example, a SPARQL graph pattern {Hinton ?r Cambridge} – or
?r:r(Hinton,Cambridge )in the functional form – that returns all relation types between two nodes Hinton
andCambridge . However, to the best of our knowledge, all current approximate query answering literature
and datasets limit the queries such that they do never have variables in the relation position, i.e.,R′⊂Con.
(we discuss queries structure in more detail in Section 6). To date, most of the literature in the field belongs
to theDiscrete reasoning domain (Table 2).
Some nodes and edges might have timestamps from a set of discrete timestamps t∈TSindicating a validity
period of a certain statement. In a more general case, certain subgraphs might be timestamped. We define
theDiscrete + Time domain when queries include temporal data. In this domain, the set of constants is
extended with the set of timestamps, i.e., TS⊂ Con, and relation projections may be instantiated with a
certain timestamp Rt(a,b). With such a graph, one can extend set of queries and the set of possible answers
to include time information. An example of such a work is this is the Temporal Feature-Logic Embedding
Framework (TFLEX) by Lin et al. (2023) that defines additional operators before,after,between over
edges with discrete timestamps.
For instance (Fig. 5), given a timestamped graph and a query ?x:education year ==1973 (Hinton,x), the
answer set includes only Edinburgh as the timestamp 1973falls into the validity period of only one edge.
Finally, the most expressive domain is Discrete + Continuous that enables reasoning over continuous
inputs (such as numbers, texts, continuous timestamps) often available as node and edge attributes or
literals. Formally, for numerical data, the space of constants is extended with real numbers R⊂Con. Also
the query formalism is extended to allow reasoning over the real numbers. An example query in Fig. 5
?x:education (Hinton,x)∧x.students<30000includes a conjunctive term x.students<30000that
requires numerical reasoning over the students attribute of a variable xto produce the answer Cambridge .
In a similar fashion, extending the answer set to continuous outputs can be framed as a regression task.
To date, LitCQD (Demir et al., 2023) and NRN (Bai et al., 2023a) are the only approaches supporting
numerical literals (in R) as query constants and potential query answers. NRN, however, treats literals as
discrete entities in the graph. LitCQD employs TransEA (Wu & Wang, 2018) (n addition to the ComplEx-
based (Lacroix et al., 2018) link predictor) as a regressor to predict numerical values of entities’ attributes.
Incorporating literals into queries, LitCQD defines filtering operators less than ,greater than ,equal
implemented as exponential functions of predicted and target values. For example, the term x.students<
30000is represented as a conjuction students (x,C)∧lt(C,30000)of a relation projection and the less-than
10Published in Transactions on Machine Learning Research (11/2024)
filter. The ability to reason over continuous data is crucial for query answering given that most real-world
KGs heavily rely on literals.
Table 2: Complex Query Answering approaches categorized under Reasoning Domain .
Discrete Discrete + Time Discrete + Continuous
GQE(Hamilton et al., 2018) , GQE w hash (Wang et al., 2019) , CGA(Mai et al.,
2019), TractOR (Friedman & Van den Broeck, 2020) , Query2Box (Ren et al.,
2020), BetaE (Ren & Leskovec, 2020) , EmQL (Sun et al., 2020) , MPQE (Daza
& Cochez, 2020) , Shv(Gebhart et al., 2023) , Q2B Onto (Andresel et al., 2021) ,
RotatE-Box (Adlakha et al., 2021) , BiQE (Kotnis et al., 2021) , HyPE (Choud-
hary et al., 2021b) , NewLook (Liu et al., 2021) , CQD (Arakelyan et al., 2021) ,
PERM (Choudhary et al., 2021a) , ConE (Zhang et al., 2021b) , LogicE (Luus
et al., 2021) , MLPMix (Amayuelas et al., 2022) , FuzzQE (Chen et al., 2022) ,
GNN-QE (Zhu et al., 2022) , GNNQ (Pflueger et al., 2022) , SMORE (Ren et al.,
2022), KGTrans (Liu et al., 2022) , LinE(Huang et al., 2022b) , Query2Particles
(Bai et al., 2022) , TAR(Tang et al., 2022) , TeMP (Hu et al., 2022) , FLEX (Lin
et al., 2022) , NodePiece-QE (Galkin et al., 2022b) , ENeSy (Xu et al., 2022) ,
GammaE (Yang et al., 2022a) , NMP-QEM (Long et al., 2022) , StarQE (Ali-
vanistos et al., 2022) , QTO (Bai et al., 2023c) , SignalE (Wang et al., 2022) ,
LMPNN (Wang et al., 2023e) , NQE (Luo et al., 2023) , Var2Vec (Wang et al.,
2023a), CQDA(Arakelyan et al., 2023) , Query2Geom (Sardina et al., 2023) ,
SQE(Bai et al., 2023b) , RoConE (He et al., 2023) , FIT(Yin et al., 2023b) , CylE
(Nguyen et al., 2023b) , LARK (Choudhary & Reddy, 2023) , WFRE (Wang et al.,
2023d), BiDAG (Xu et al., 2023a) , Query2Triple (Xu et al., 2023b) , SCoNe
(Nguyen et al., 2023a) , CQD Onto (Andresel et al., 2023) , UnRavL (Cucumides
et al., 2024) , UltraQuery (Galkin et al., 2024)TFLEX (Lin et al., 2023) LitCQD (Demir et al., 2023) ,
NRN(Bai et al., 2023a)
4.3 Background Semantics
Hint on Cambridge E dinbur gh 
education education Hint on Cambridge E dinbur gh 
education education 
start: 1972 
end:   1975 
Facts-only (ABOX) + Class Hierarchy +  Complex Axioms (TBOX) Hint on Cambridge E dinbur gh 
education education 
established: 1583 
students:       35,375 established: 1209 
students:       24,450 start: 1967 
end:   1970      ?x: education(Hinton, x) ⋀ 
         x.students < 30,000 Hint on Cambridge E dinbur gh education education 
Hint on Cambridge E dinbur gh education education 
Bachelor PhD degree degree ABox TBox
type subClassOf 
type subClassOf Professor ⊑ ≥1 hasStudent ⊓ 
∀works.University 
Figure 6: Background Semantics. Facts-only are graphs that only have assertions ( ABox) and have no
higher-level schema. Class Hierarchy introduces node types (classes) and hierarchical relationships between
classes. Finally, Complex Axioms add even more complex logical rules ( TBox), for example, governed by
certain OWL profiles, e.g.,A professor is someone who has one or more students and works at university.
Relational databases often contain a schema, that is, a specification of how tables and columns are organized
that gives a high-level overview of the database content. In graphs databases, schemas exist as well and,
for instance, node types are common in the Labeled Property Graph (LPG) paradigm. RDF graphs employ
the standards based on Description Logics (Baader et al., 2003). As incorporating schema is crucial for
11Published in Transactions on Machine Learning Research (11/2024)
designing effective query answering ML models, we introduce Background Semantics (Fig. 6) as the notion
of additional schema information available on top of plain facts (statements).
Facts-only. In the simplest case, there is no background schema such that a KG consists of statements
(facts) only, that is, a KG follows Definition 2.2, G= (E,R,S). In terms of description logics, the graph only
hasassertions (ABox). Queries, depending on the Reasoning Domain (Section 4.2), involve only entities,
relations, and literals. The original GQE (Hamilton et al., 2018) focused on facts-only graphs and the
majority of subsequent query answering approaches (Table 3) operate exclusively on schema-less KGs.
Class Hierarchy. Classes of entities, or node types, are a natural addition to a facts-only graph as a basic
schema. Using Definition 2.2 with a set of types T, a graphGis defined asG= (E,R,S), whereE=¯E∪T
(with ¯E∩T =∅). In other words, the types can be used as normal entities. To indicate that an entity
has a type, a specially labeled edge can be used (e.g., rdf:type ), or a labeling function can be defined;
both options have equivalent expressive power. Because types are nodes themselves, it is possible to specify
hierarchical relationships between classes using RDF Schema (RDFS) (Brickley et al., 2014)3. In LPG, it is
more common to only have a labeling function, and not allow type hierarchies. Practically, edges involving
types might be present physically in a KG or be considered as an additional input to a particular model.
To date, we are aware of three query answering approaches (Table 3) that incorporate entity types. Con-
textual Graph Attention ( CGA, Mai et al. (2019)) only uses types for entity embedding initialization and
requires each entity to have only one type. A similar technique was used in the evaluation of mpqe, but
only to initialize the representation of variables (for which types were assumed given). The queries are not
conditioned by entity types and the answer set still includes entities only. That is, constants in queries and
those bound to variables are never types.
In Type-Aware Message Passing ( Temp, Hu et al. (2022)), type embeddings are used to enrich entity and
relation representations that are later sent to a downstream query answering model. Each node might have
several types. In the inductive scenario (we elaborate on inference scenarios in Section 5.1) with unseen
nodes at inference time, type and relation embeddings are learned invariants that transfer across training
and inference entities. Queries, and bound variables are still limited to non-types only (im (µ)⊆¯E).
TheTBoxandABoxNeural Reasoner ( TAR, Tang et al. (2022)) incorporates types and their hierarchy to
improve predictions over entities. They also introduce the task of predicting types of answer entities, that
is,µ(t)⊆(¯E∪T ), for one specific variable called t. Other variables cannot bind to types and constants in
the query cannot be types either. The class hierarchy in TARis used in three auxiliary losses besides the
original entity prediction, that is, concept retrieval – prediction of the answer set of types, subsumption –
predicting which type is a subclass of another type, and instantiation – predicting a type for each entity.
A natural next step for the Class Hierarchy family of approaches is to incorporate types in queries in the
form of constants and variables.
Complex Axioms. Finally, a schema might contain not just a class hierarchy but a set of more complex
axioms involving, for example, a hierarchy of relations, restrictions on relations, or composite classes. Such
a complex schema can now be treated as an ontologyOand we extend the definition of the graph to include
it:G= (E,R,S,O). In Fig. 6, the axiom Professor⊑≥1hasStudent⊓∀works.University describes that
A professor is someone who has one or more students and works at university. In terms of description logics,
a graph has an additional terminology component ( TBox). The expressiveness of the TBoxdirectly affects
the complexity of symbolic reasoning engines up to exponential ( ExpTime ) for most expressive fragments.
In graph representation learning, incorporating complex ontological axioms is non-trivial even for simple
link prediction models (Zhang et al., 2022b). In the query answering literature, the only attempt to include
complex axioms is taken by Andresel et al. (2021). In Q2B Onto (O2B), an extension of Query2Box (Q2B,
Ren et al. (2020)), the set of considered complex axioms belongs to the DL-LiteRfragment and supports
the hierarchy of classes ( subclasses ), the hierarchy of relations ( subproperties ), as well as rangeanddomain
of relations. The model architecture is not directly conditioned on the axioms and remains the original
3RDFS has more expressive means ( e.g., a hierarchy of relations) but we leave them to Complex Axioms
12Published in Transactions on Machine Learning Research (11/2024)
Query2Box. Instead, the axioms affect the graph structure, query sampling, and an auxiliary loss, that
is,query rewriting mechanisms are used to materialize more answers to original queries as if executed
against the complete graph ( deductive closure ) akin to data augmentation. During optimization, an auxiliary
regularization loss aims at including a specialized query box qinto the more general version of this query q′.
Still, even the expensive procedure of incorporating complex axioms in query sampling in O2B benefits
mostly the deductive capabilities of query answering, that is, inferring answers that are already implied by
the graphGand ontologyO, and does not improve the generalization capabilities when missing edges cannot
be inferred by ontological axioms. We elaborate on deductive ,generalization , and other setups in Section 7.
Another avenue for future work is a better understanding of theoretical expressiveness of Graph Neural Net-
work (GNN) encoders when applied to multi-relational KGs. Initial works on non-relational graphs (Barceló
et al., 2020) map the expressiveness to the FOC 2subset of FOL with two variables and counting quantifiers,
and to FOC B(Luo et al., 2022) for hypergraphs of maximum arity B. In relational graphs, Barceló et al.
(2022) quantified the expressiveness of relational GNNs in terms of the relational Weisfeiler-Leman (RWL)
test proving that RWL is more expressive than classical WL test (Weisfeiler & Leman, 1968) and that com-
mon relational GNN architectures like R-GCN (Schlichtkrull et al., 2018) and CompGCN (Vashishth et al.,
2020) are bounded by 1-RWL. Using RWL, Huang et al. (2023) derive that the family of GNNs conditioned
on the query node, such as Neural Bellman-Ford Networks (Zhu et al., 2021), are bounded by the asymmetric
local 2-RWL and expressive as rGFO3
cnt, restricted guarded first-order logic fragment with three variables
and counting. Concurrently, Gao et al. (2023) study KGs as double permutation equivariant structures
(to permuting nodes and edge types) and map their expressiveness to universally quantified entity-relation
(UQER) Horn clauses. However, it is still an open question if there exists GNNs architectures that can
capture OWL-like axioms and leverage them as an inductive bias in complex query answering.
Table 3: Complex Query Answering approaches categorized under Background Semantics .
Facts-only (ABOX) + Class Hierarchy + Complex Axioms (TBOX)
GQE(Hamiltonet al.,2018) , GQE w hash (Wang etal., 2019) , TractOR
(Friedman &VandenBroeck,2020) , Query2Box (Renetal., 2020) , BetaE
(Ren & Leskovec, 2020) , EmQL (Sun et al., 2020) , Shv(Gebhart et al.,
2023), RotatE-Box (Adlakhaetal.,2021) , MPQE (Daza&Cochez,2020) ,
BiQE(Kotnis et al., 2021) , HyPE (Choudhary et al., 2021b) , NewLook
(Liu et al., 2021) , CQD(Arakelyan et al., 2021) , PERM (Choudhary et al.,
2021a), ConE (Zhang et al., 2021b) , LogicE (Luus et al., 2021) , MLP-
Mix(Amayuelasetal.,2022) , FuzzQE (Chenetal.,2022) , GNN-QE (Zhu
et al., 2022) , GNNQ (Pflueger et al., 2022) , SMORE (Ren et al., 2022) ,
KGTrans (Liuetal.,2022) , LinE(Huangetal.,2022b) , Query2Particles
(Bai et al., 2022) , FLEX (Lin et al., 2022) , TFLEX (Lin et al., 2023) ,
NodePiece-QE (Galkin et al., 2022b) , ENeSy (Xu et al., 2022) , Gam-
maE(Yang et al., 2022a) , NMP-QEM (Long et al., 2022) , StarQE (Ali-
vanistos et al., 2022) , QTO(Bai et al., 2023c) , SignalE (Wang et al., 2022) ,
LMPNN (Wang et al., 2023e) , NQE (Luo et al., 2023) , Var2Vec (Wang
et al., 2023a) , CQDA(Arakelyan et al., 2023) , Query2Geom (Sardina
et al., 2023) , SQE(Bai et al., 2023b) , RoConE (He et al., 2023) , FIT
(Yin et al., 2023b) , LitCQD (Demir et al., 2023) , CylE (Nguyen et al.,
2023b), LARK (Choudhary & Reddy, 2023) , WFRE (Wang et al., 2023d) ,
BiDAG (Xu et al., 2023a) , NRN (Bai et al., 2023a) , Query2Triple (Xu
et al., 2023b) , SCoNe (Nguyen et al., 2023a) , UnRavL (Cucumides et al.,
2024), UltraQuery (Galkin et al., 2024)CGA (Mai et al.,
2019), TeMP (Hu
et al., 2022) , TAR
(Tang et al., 2022)Q2B Onto (Andresel et al., 2021) ,
CQD Onto (Andresel et al., 2023)
5 Modeling
In this section, we discuss the literature from the perspective of Modeling . Following the common methodol-
ogy(Battagliaetal.,2018), wesegmentthe Modeling methodsthroughthelensof Encoder-Processor-Decoder
modules (illustrated in Fig. 7). (1) The Encoder Enc()takes an input query q, target graphGwith its en-
tities and relations, and auxiliary inputs ( e.g., node, edge, graph features) to build their representations in
13Published in Transactions on Machine Learning Research (11/2024)
Hint on Cambridge E dinbur gh education education 
Hint on Cambridge E dinbur gh education education 
Bachelor PhD degree degree 
Query 
Shallow Embedding Transductive 
Embedding 
Discrete Outputs 
Continuous Outputs 
Query 
Graph 
Additional inputs 
Neural Query Executor 
Figure 7: Neural Query Execution through the Encoder-Processor -Decoder modules. Encoder function f
buildsrepresentationsofinputs(query, targetgraph, auxiliarydata)inthelatentspace. Processor Pexecutes
the query with its logical operators against the graph conditioned on other inputs. Decoder function gbuilds
requested outputs that might be discrete or continuous.
the latent space. (2) The ProcessorPleverages the chosen inductive biases to process representations of the
query with its logical operators in the latent or symbolic space. (3) The Decoder Dec()takes the processed
latents and builds desired outputs such as a distribution over discrete entities or regression predictions in
case of continuous tasks. Generally, encoder, processor, and decoder can be parameterized with a neural
networkθor be non-parametric. Finally, we analyze computational complexity of existing processors.
5.1 Encoder
We start the modeling section with encoders, i.e., how different methods encode and represent entities and
relations from the KG. There are three different categories, Shallow Embedding ,Transductive Encoder , and
Inductive Encoder each representing a different way of producing the neural representation of the enti-
ties/relations. Different encoding methods are suitable in different inference setups (details in Section 7.4),
and may further require different logical operator methods (details in Section 5.2). Fig. 8 illustrates the
three common encoding approaches.
Hint on Cambridge E dinbur gh education education 
Hint on Cambridge E dinbur gh education education 
Bachelor PhD degree degree 
lookup 
Query 
entity 
lookup relation 
Query 
lookup 
Query entity 
lookup relation 
Graph 
Classes 
Axioms 
Other 
inputs 
…
Shallow Embedding 
Transductive 
Encoder 
Inductive Encoder 
Graph 
Classes 
Axioms 
Other 
inputs 
…
relation 
lookup 
if relation 
set is ﬁxed reconstruct 
entity embeddings optionally 
a 
b 
c 
Figure 8: Categorization of Encoders .aShallow encoders perform entity and relation embedding lookup
and send them to the processor. bTransductive encoders additionally enrich the representations with
query, graph, classes, or other latents. cInductive encoders do not need learnable entity embeddings.
Shallow Embeddings. The first line of approaches encodes each entity/relation on the graph as a low-
dimensional vector, and thus we achieve an entity embedding matrix Eand a relation embedding matrix R.
The shape of the entity embedding matrix is |E|×d(|R|×d), wheredis the dimension of the embedding.
Shallow embedding methods assume independence of the representation of all the nodes on the graph.
14Published in Transactions on Machine Learning Research (11/2024)
Table 4: Complex Query Answering approaches categorized under Encoder.
Shallow Embedding Transductive Encoder Inductive Encoder
GQE(Hamilton et al., 2018) , GQE w hash (Wang et al., 2019) ,
CGA(Mai et al., 2019) , TractOR (Friedman & Van den Broeck, 2020) ,
Query2Box (Ren et al., 2020) , BetaE (Ren & Leskovec, 2020) , EmQL
(Sun et al., 2020) , Shv(Gebhart et al., 2023) , Q2B Onto (Andresel et al.,
2021), RotatE-Box (Adlakha et al., 2021) , HyPE (Choudhary et al.,
2021b),NewLook (Liuetal.,2021) ,CQD(Arakelyanetal.,2021) ,PERM
(Choudharyetal.,2021a) , ConE (Zhangetal.,2021b) , LogicE (Luusetal.,
2021), FuzzQE (Chen et al., 2022) , SMORE (Ren et al., 2022) , LinE
(Huang et al., 2022b) , Query2Particles (Bai et al., 2022) , TAR (Tang
et al., 2022) , FLEX (Lin et al., 2022) , TFLEX (Lin et al., 2023) , Gam-
maE(Yang et al., 2022a) , NMP-QEM (Long et al., 2022) , QTO (Bai
et al., 2023c) , SignalE (Wang et al., 2022) , Var2Vec (Wang et al., 2023a) ,
CQDA(Arakelyan et al., 2023) , Query2Geom (Sardina et al., 2023) , Ro-
ConE(He et al., 2023) , FIT(Yin et al., 2023b) , LitCQD (Demir et al.,
2023), CylE (Nguyen et al., 2023b) , WFRE (Wang et al., 2023d) , NRN
(Bai et al., 2023a) , SCoNe (Nguyen et al., 2023a) , CQD Onto (Andresel
et al., 2023)MPQE (Daza & Cochez,
2020), BiQE (Kotnis et al.,
2021), KGTrans (Liu et al.,
2022), StarQE (Alivanistos
et al., 2022) , MLPMix
(Amayuelas et al., 2022) ,
ENeSy (Xu et al., 2022) ,
LMPNN (Wang et al.,
2023e), NQE (Luo et al.,
2023), SQE (Bai et al.,
2023b), BiDAG (Xu et al.,
2023a), Query2Triple (Xu
et al., 2023b) ,NodePiece-QE (Galkin
et al., 2022b) , GNN-QE
(Zhu et al., 2022) , GNNQ
(Pflueger et al., 2022) , TeMP
(Hu et al., 2022) , UnRavL
(Cucumides et al., 2024) ,
UltraQuery (Galkin et al.,
2024)
This independence assumption gives the model much freedom, free parameters to learn. Such modeling
origins from the KG completion literature, where the idea is to learn the entity and relation embedding
matrices by optimizing a pre-defined distance/score function over all edges on the graph, e.g., a triplet fact
dist (es,r,eo). The majority of query answering literature follows the same paradigm with various different
embedding spaces and distance functions to learn the entity and relation embedding matrices. Multiple
embedding spaces have been proposed. For example, GQE (Hamilton et al., 2018) and Query2Box (Ren
et al., 2020) embed into Rd(point vector in the Euclidean space); FuzzQE (Chen et al., 2022) embeds into the
space of real numbers in range [0,1](fuzzy logic score); BetaE (Ren & Leskovec, 2020) uses Beta distribution,
a probabilistic embedding space; ConE Zhang et al. (2021b) on the other hand embeds entities as a point
on a unit circle. Each design choice motivates the inductive bias for executing logical operators (as we show
in Section 5.2). Some approaches employ shallow entity and relation embeddings already pre-trained on a
simple link prediction task and just apply on top of them a query answering decoder with non-parametric
logical operators. For example, CQD (Arakelyan et al., 2021), LMPNN (Wang et al., 2023e), Var2Vec (Wang
et al., 2023a), and CQDA(Arakelyan et al., 2023) take pre-trained embeddings in the complex space Cd
and apply non-parametric t-norms andt-conorms to model intersection and union, respectively. QTO (Bai
et al., 2023c) goes even further and fully materializes scores of all possible triples in one [0,1]|R|×|E|×|E|
matrix given pre-trained entity and relation embeddings at preprocessing stage.
Despite being the mainstream design choice, the downside of shallow methods is that (1) shallow embeddings
do not use any inductive bias and prior knowledge of the entity or its neighboring structure since the
parameters of all entities/relations are free parameters learned from scratch; (2) they are not applicable
in the inductive inference setting since these methods do not have a representation/embedding for those
unseen novel entities by design. One possible solution is to randomly initialize one embedding vector for a
novel entity and finetune the embedding vector by sampling queries involving the novel entity (detailed in
Section 7.3). However, such a solution requires gradient steps during inference, rendering it not ideal.
Transductive Encoder. Similar to shallow embedding methods, transductive encoder methods learn the
same entity embedding matrix E. Besides, they learn an additional encoder Encθ(q,E,R,...)(parameter-
ized withθ) on top of the query q, entity and relation embedding matrices (and, optionally, other available
inputs). The goal is to apply the encoder to the embeddings of entities in the query qin order to capture
dependencies between neighboring entities in the graph. Specifically, the additional encoder may take several
rows of the feature matrix as input and further apply transformations. For example, BiQE (Kotnis et al.,
2021) and kgTransformer (Liu et al., 2022) linearize a query graph Gqinto a sequence and apply a Trans-
former (Vaswani et al., 2017) encoder that attends to all other embeddings in the query and obtain the final
representation of the [MASK]token as the target query. MPQE (Daza & Cochez, 2020) and StarQE (Ali-
15Published in Transactions on Machine Learning Research (11/2024)
vanistos et al., 2022) run a message passing GNN architecture on top of the query graph Gqto enrich entity
and relation embeddings and extract the final node representation as the query embedding. These methods
share similar benefit and disadvantage of the shallow embeddings. Namely, there are many free parameters
in the method to train. Unlike the shallow embeddings, the additional encoder leverages relational inductive
bias between an entity and its neighboring entities or other entities in a query, allowing for a better learned
entity representation and generalization capacity. However, since at its core the method is still based on the
large look-up matrix of entity embeddings, it still exhibits the same downside that all such methods cannot
be directly applied to an inductive setting where we may observe new entities.
Inductive Encoder. In order to address the aforementioned challenges of shallow embeddings and trans-
ductive encoders, inductive encoder methods aim to avoid learning an embedding matrix Efor a fixed
number of entities. Instead, inductive representations are often calculated by leveraging certain invariances ,
that is, the features that remain the same when transferred onto different graphs with new entities at infer-
ence time. As we describe in Section 7.4, inductive encoders might employ different invariances albeit the
majority of inductive encoders rely on the assumption of the fixed set of relation types R. Formally, follow-
ing Definition 2.4, given a complex query Q= (E′,R′,S′,¯S′)composed of entity and relation terms E′,R′
(that, in turn, contain constants Conand variables Var), relation projections R(a,b)∈S(and, optionally, in
¯S′), a target graph G(and, optionally, other inputs), inductive encoders learn a conditional representation
function Encθ(e|E′,R′,G,...)for each entity e∈E. Galkin et al. (2022b) devise two families of inductive
representations, i.e., (1)inductive node representations and(2)inductive relational structure representations.
Inductive node representation approaches parameterize Encθas a function of a fixed-size invariant vocab-
ulary. For instance, NodePiece-QE (Galkin et al., 2022b) employs the invariant vocabulary of relation types
and parameterizes each entity through the set of incident relations. TeMP (Hu et al., 2022) employs the
invariant vocabulary of entity types and class hierarchy and injects their representations into entity represen-
tations. Inductive node representation approaches reconstruct embeddings of new entities and can be used
as a drop-in replacement of shallow lookup tables paired with any processor method, e.g., NodePiece-QE
used CQD as the processor while TeMP was probed with GQE, Query2Box, BetaE, and LogicE processors.
Inductive relational structure representation methods parameterize Encθas a function of the relative
relational structure that only requires learning of relation embeddings and uses relations as invariants. Such
methods often employ various labeling tricks (Zhang et al., 2021a) to label constants (anchor entities) of
the input queryQsuch that after the message passing procedure all other nodes would encode a graph
structure relative to starting nodes. In particular, GNN-QE (Zhu et al., 2022) labels anchor nodes with the
embedding vector of the queried relations, e.g., for a projection query (h,r,?)a nodehwill be initialized with
the embedding of relation r, whereas all other nodes are initialized with the zero vector. In this way, GNN-
QE learns only relation embeddings Rand GNN weights. GNNQ (Pflueger et al., 2022) represents a query
with its variables and relations as a hypergraph and learns a relational structure through applying graph
convolutions on hyperedges. Hyperedges are parameterized with multi-hot feature vectors of participating
relations, so the only learnable parameters are GNN weights.
Still, there exists a set of open problems for inductive models. As the majority of inductive methods rely on
learning relation embeddings, they cannot be easily used in setups where at inference time KGs are updated
with new, unseen relation types, that is, relations are not invariant. This fact might require exploration of
novel invariances and featurization strategies (Huang et al., 2022a; Gao et al., 2023; Chen et al., 2023). To
date, the only fully-inductive CLQA model capable of generalizing to new, unseen graphs with new sets of
entities and relations at inference time is UltraQuery (Galkin et al., 2024) that leverages the invariance of
relational structure . In particular, relational representations (for both seen and new unseen relation types)
are obtained through the graph of relational interactions and the conditional message passing GNN encoder
which is independent from relation identities and thus generalizable to any multi-relational graph. Inductive
models are more expensive to train in terms of both time and memory than shallow models and cannot yet
be easily extended to large-scale graphs. We conjecture that inductive encoders will be in the focus of the
future work in CLQA as generalization to unseen entities and graphs at inference time without re-training
is crucial for updatability. Furthermore, updatability might increase the role of continual learning (Thrun,
1995; Ring, 1998) and amplify the negative effects of catastrophic forgetting (McCloskey & Cohen, 1989)
16Published in Transactions on Machine Learning Research (11/2024)
that have to be addressed by the encoders. Larger inference graphs also present a major size generalization
issue (Yehudai et al., 2021; Buffelli et al., 2022; Zhou et al., 2022) when performance of GNNs trained on
small graphs decreases when running inference on much larger graphs. The phenomenon has been observed
by Galkin et al. (2022b) in the inductive complex query answering setup.
5.2 Processor
Having encoded the query and other available inputs, the ProcessorPexecutes the query in the latent (or
symbolic) space against the input graph. Recall that a query qis defined as q(E′,R′,S,¯S)whereE′andR′
terms include constants Conand variables Var, statements inSand ¯Sinclude relation projections R(a,b),
and logical operators opsover the variables. We define ProcessorPas a collection of modules that perform
relation projections R(a,b)given constants Conand logical operators ops⊆{∧,∨,¬,...}over variables
Var(we elaborate on the logical operators in Section 6.1). Depending on the chosen inductive biases and
parameterization strategies behind those modules, we categorize Processors intoNeuralandNeuro-Symbolic
(Table 5a). Furthermore, we break down the Neuro-Symbolic processors into Geometric ,Probabilistic , and
Fuzzy Logic (Table 5b). Note that in this section we omit pure entity encoder approaches like TeMP (Hu
et al., 2022), NodePiece-QE (Galkin et al., 2022b), and NRN (Bai et al., 2023a) (for numerical literals only)
that can be paired with any neural or neuro-symbolic processor. To describe processor models more formally,
we denote eas an entity vector, ras a relation vector, and qas the query embedding that is often a function
ofeandr. We useGqas the query graph.
Table 5: Categorization of Query Processors. Table 5a provides a general view on Neural, Symbolic, Neuro-
Symbolic methods as well as encoders that can be paired with any processor. Table 5b further breaks down
neuro-symbolic processors.
(a) Complex Query Answering approaches categorized under the Processor type.
Any Processor Neural Neuro-Symbolic
TeMP (Hu et al., 2022) ,
NodePiece-QE (Galkin
et al., 2022b) , NRN (Bai
et al., 2023a)GQE(Hamilton et al., 2018) , GQE w/ hashing (Wang et al.,
2019), CGA (Mai et al., 2019) , BiQE (Kotnis et al., 2021) ,
MPQE (Daza & Cochez, 2020) , StarQE (Alivanistos et al., 2022) ,
MLPMix (Amayuelas et al., 2022) , Query2Particles (Bai et al.,
2022), KGTrans (Liu et al., 2022) , RotatE-m, DistMult-m,
ComplEx-m (Ren et al., 2022) , GNNQ (Pflueger et al., 2022) ,
SignalE (Wang et al., 2022) , LMPNN (Wang et al., 2023e) , SQE
(Bai et al., 2023b) , LARK (Choudhary & Reddy, 2023) , BiDAG
(Xu et al., 2023a) , Query2Triple (Xu et al., 2023b)Table 5b
(b) Neuro-symbolic Processors
Geometric [Neuro-Symbolic] Probabilistic [Neuro-Symbolic] Fuzzy Logic [Neuro-Symbolic]
Query2Box (Ren et al., 2020) , Query2Onto
(Andresel et al., 2021) , RotatE-Box (Adlakha
et al., 2021) , NewLook (Liu et al., 2021) ,
Knowledge Sheaves (Gebhart et al., 2023) ,
HypE(Choudhary et al., 2021b) , ConE (Zhang
et al., 2021b) , Query2Geom (Sardina et al.,
2023), CylE (Nguyen et al., 2023b) , RoConE
(He et al., 2023) , SCoNe (Nguyen et al., 2023a)BetaE (Ren & Leskovec, 2020) ,
PERM (Choudhary et al.,
2021a), LinE (Huang et al.,
2022b), GammaE (Yang et al.,
2022a), NMP-QEM (Long et al.,
2022)EmQL (Sun et al., 2020) , TractOR (Friedman
& Van den Broeck, 2020) , CQD (Arakelyan et al.,
2021), LogicE (Luus et al., 2021) , FuzzQE (Chen
et al., 2022) , TAR(Tang et al., 2022) , FLEX (Lin
et al., 2022) , TFLEX (Lin et al., 2023) , UnRavL
(Cucumides et al., 2024) , ENeSy (Xu et al., 2022) ,
QTO(Bai et al., 2023c) , NQE (Luo et al., 2023) ,
Var2Vec (Wang et al., 2023a) , CQDA(Arakelyan
et al., 2023) , FIT (Yin et al., 2023b) , LitCQD
(Demir et al., 2023) , WFRE (Wang et al., 2023d) ,
CQD Onto (Andresel et al., 2023) , UnRavL (Cu-
cumides et al., 2024) , UltraQuery (Galkin et al.,
2024)
Neural Processors. Neural processors execute relation projections and logical operators directly in the
latent space Rdparameterizing them with neural networks. To date, most existing purely neural approaches
17Published in Transactions on Machine Learning Research (11/2024)
a 
b ⋀ win 
ﬁeld univ ersity i 
i 
(a) 
a 
b ⋀ win 
ﬁeld univ ersity i 
i 
(a) 
Hint on Cambridge E dinbur gh education education 
Hint on Cambridge E dinbur gh education education 
Bachelor PhD degree degree 
lookup 
Query 
entity 
lookup relation 
a
b⋀ win
ﬁeld university i 
i 
(a) 
a
b⋀ win
ﬁeld university i 
i a
win
ﬁeld uni
b
uniT r ansformer [MASK] 
[MASK] ⋀ [TAR] 
pool 
(b) linearization to graph 
[MASK] 
[MASK] GNN 
Figure 9: Neural Processors. (a) Relation projections Rθ(a,b)and logical operators (non-parametric or
parameterized with θ) are executed sequentially in the latent space; (b) a query is encoded to a graph or
linearized to a sequence and passed through the encoder (GNN or Transformer, respectively). A pooled
representation denotes the query embedding.
operateexclusivelyonthequerygraph Gqonlyexecutingoperatorswithinasinglequeryanddonotcondition
the execution process on the full underlying graph structure G. Since the query processing is performed in
the latent space with neural networks where Union ( ∨) and Negation (¬) are not well-defined, the majority of
neural processors implement only relation projection (R(a,b))and intersection (∧)operators. We aggregate
the characteristics of neural processors as to their embedding space, the way of executing relation projection,
logical operators, and the final decoding distance function in Table 6. We illustrate the difference between
two families of neural processors (sequential execution and joint query encoding) in Fig. 9.
The original GQE (Hamilton et al., 2018) is the first example of the neural processor. That is, queries q,
entities e, and relations rare vectors in Rd. Query embedding starts with embeddings of constants C(anchor
nodes e) and they get progressively refined through relation projection and intersection, i.e., it is common to
assume that query embedding at the initial step 0 is equivalent to embedding(s) of anchor node(s), q(0)=e.
Relation projection is executed in the latent space with the translation function q+r, and intersection
is modeled with the permutation-invariant DeepSet (Zaheer et al., 2017) neural network. Several follow-up
works improved GQE to work with hashed binary vectors {+1,−1}d(Wang et al., 2019) or replaced DeepSet
with self-attention and translation-based projection to a matrix-vector product (Mai et al., 2019). Recently,
Ren et al. (2022) proposed DistMult-m, ComplEx-m, and RotatE-m, extensions of simple link prediction
models for complex queries that, inspired by GQE, perform relation projection by the respective composition
function and model the intersection operator with DeepSet and, optionally, L2 norm.
The other line of works apply neural encoders to whole query graphs Gqwithout explicit execution of logical
operators. Depending on the query graph representation, such encoders are often GNNs, Transformers,
or MLPs. It is assumed that neural encoders can implicitly capture logical operators in the latent space
during optimization. For instance, MPQE (Daza & Cochez, 2020), StarQE (Alivanistos et al., 2022), and
LMPNN (Wang et al., 2023e) represent queries as relational graphs (optionally, hyper-relational graphs for
StarQE) where each edge is a relation projection and intersection is modeled as two incoming projections
to the same variable node. All constants Cand known relation types are initialized from the respective
embedding matrices. All variable nodes in all query graphs are initialized with the same learnable [VAR]
feature vector while all target nodes are initialized with the same [TAR]vector. Then, the query graph is
passed through a GNN encoder (R-GCN (Schlichtkrull et al., 2018) for MPQE, StarE (Galkin et al., 2020)
for StarQE, GIN (Xu et al., 2019) for LMPNN), and the final state of the [TAR]target node is considered
the final query embedding ready for decoding. A recent LMPNN extends query graph encoding with an
additional edge feature indicating whether a given projection R(a,b)has a negation or not and derives a
closed-form solution for the merged projection and negation operator for the ComplEx composition function.
A different approach is taken by GNNQ (Pflueger et al., 2022) that frames query answering as a subgraph
classification task. That is, an input query is not directly executed over a given graph G, but, instead,
the task is to classify whether a given precomputed subgraph G′⊂Gsatisfies a given conjunctive query.
18Published in Transactions on Machine Learning Research (11/2024)
For that, GNNQ first augments the graph with Datalog-derived triples and converts the subgraph to a
hypergraph where only hyperedges are parameterized with learnable vectors. On the one hand, this strategy
allows GNNQ to be inductive and not learn entity embeddings. On the other hand, GNNQ is limited to
conjunctive queries only and extensions to union and negation queries are not defined.
A more exotic approach by Gebhart et al. (2023) is based on the sheaf theory and algebraic topology (Hansen
& Ghrist, 2019). There, a graph is represented as a cellular sheaf and conjunctive queries are modeled as
chains of relations (0-cochains). A sheaf is induced over the query graph and relevant answers should be
consistent with the induced sheaf and entity embeddings. The optimization problem is a harmonic extension
of a 0-cochain using sheaf Laplacian andSchur complement of the sheaf Laplacian. Conceptually, this
approach merges execution of projection and intersection operators as functions over topological structures.
Considering Transformer encoders, BiQE (Kotnis et al., 2021), kgTransformer (Liu et al., 2022), and
SQE (Bai et al., 2023b) linearize a conjunctive query graph into a sequence of relational paths composed of
entity constantsCand relation tokens. The order of tokens in paths and intersections of paths are marked
with positional encodings. The target node (present in many paths) is marked with the [MASK]token (op-
tionally, kgTransformer also annotates existentially quantified variables with [MASK]). SQE does not model
variables explicitly but instead relies on auxiliary brackettokens that separate branches of the computation
graph. Passing the sequence through the Transformer encoder, the final query embedding is the aggregated
representation of the target node. BiQE only supports conjunctive queries while kgTransformer converts
queries with unions to the Disjunctive Normal Form (DNF) with post-processing of score distributions (we
elaborate on query rewritings and normal forms in Section 6.1). SQE explicitly includes all operator tokens
into the linearized sequence and thus supports negations.
Finally, MLPMix (Amayuelas et al., 2022) sequentially executes operations of the query where projection,
intersection, and negation operators are modeled as separate learnable MLPs. Union queries are converted
to DNF such that they can be answered with projection and intersection operators with the final post-
processing of scores as a union operator. Similarly, Query2Particles (Bai et al., 2022) represents each query
as a set of vectors in the embedding space and models projection, intersection, and negation operators as
attention over the set of particles followed by an MLP. Union is a concatenation of query particles.
Table 6: Neural Processors. Most methods implement only Relation Projection and Intersection operators.
The top part of models execute a query sequentially, the bottom part encode the whole query graph Gq.
Model Embedding Space Relation Projection Intersection Union Negation Distance
GQE q,e,r∈Rdq+r DeepSet ({qi}) - - ∥q−e∥
GQE+hashing q,e∈{± 1}d,r∈Rdsgn(q+r) sgn(DeepSet ({qi})) - - −cos(q,e)
CGA q,e,r∈RdWrq SelfAttn ({qi}) - - −cos(q,e)
RotatE-m q,e,r∈Cdq◦r DeepSet ({qi}) - - ∥q−r∥
DisMult-m q,e,r∈RdL2Norm (q◦r) L2Norm (DeepSet ({qi})) - - −⟨q,e⟩
ComplEx-m q,e,r∈CdL2Norm (q◦r) L2Norm (DeepSet ({qi})) - - −Re(⟨q,e⟩)
Query2Particles q,e∈Rd×K,r∈Rd f(q,r)MLP(Attn ([q1,q2])) [q1,...,qN]MLP(Attn (q))−maxk⟨qk,e⟩fis neural gates
SignalEq,e,r= [z,v]zq◦zr SelfAttn ({zqi}) DNF(1−zamp
q, β∥vq−ve∥2+
z∈Cd,v=IDFT (z)∈Rdzphase
q)∥zq−ze∥2
MLPMix q,e,r∈RdMLP mix(q,r) MLP mix(q1,q2) DNF MLP(q)∥q−e∥
MPQE q,e,r∈RdWrq RGCN (q,Gq) - - cos(q,e)
StarQE q,e,r∈Rdf(q,g(r,hquals)) StarE (q,Gq) - - dot(q,e)
GNNQ q,r∈RdHypergraph RGCN (G′) - - BCE
LMPNN q,e,r∈Cdρ(q,r,dir,neg) GIN(q,Gq) DNF ρ(q,r,dir,neg)cos(q,e)
BiQE q,e,r∈RdTransformer (q,linearizedGq) - - CE
kgTransformer q,e,r∈RdTransformer (q,linearizedGq) DNF - dot(q,e)
SQE q,e,r∈RdLSTM / Transformer (q,linearizedGqwith operator tokens ) dot(q,e)
Sheaves q,e∈Rd,Fr∈Rd×df(q,Gqas cochain ) - - ∥Frq−Fre∥
Neuro-SymbolicProcessors. Incontrasttopurelyneuralandsymbolicmodels, wedefine neuro-symbolic
processors as those who (1) explicitly design logic modules (or neural logical operators) that simulate the
real logic/set operations, or rely on various kinds of fuzzy logic formulations to provide a probabilistic view
of the query execution process, and (2) execute relation traversal in the latent space. The key difference
between neuro-symbolic processors and the previous two is that neuro-symbolic processors explicitly model
19Published in Transactions on Machine Learning Research (11/2024)
Hint on Cambridge E dinbur gh education education 
Hint on Cambridge E dinbur gh education education 
Bachelor PhD degr ee degr ee 
lookup 
Quer y 
entity 
lookup r elation 
a 
b ⋀win 
ﬁeld univ ersity i 
i 
pr ojection 
(bo x es) 
intersection 
(bo x es) union 
(cones) negation 
(cones) 
q1 q2 
Figure 10: Geometric Processors and their inductive biases.
the logical operations with strong inductive bias so that the processing / execution is better aligned with
the symbolic operation ( e.g., by imposing restrictions on the embedding space) and more interpretable. We
further segment these methods into the following categories.
Geometric Processors. Geometric processors design an entity/query embedding space with different
geometric intuitions and further customize neuro-symbolic operators that directly simulate their logical
counterparts with similar properties (as illustrated in Fig. 10). We aggregate the characteristics of geometric
models as to their embedding space and inductive biases for logical operators in Table 7.
Query2Box(Renetal.,2020)embedsqueries qashyper-rectangles(high-dimensionalboxes)intheEuclidean
space. To achieve that, entities eand relations rare embedded as points in the Euclidean space where each
relation has an additional learnable offset vector ro(entities’ offsets are zeros). The projection operator
is modeled as an element-wise summation q+rof centers and offsets of the query and relation, that is,
the initial box is obtained by projecting the original anchor node embedding e(with zero offset) with the
relation embedding rand relation offset ro. Accordingly, an attention-based neuro-intersection operator is
designed to simulate the set intersection of the query boxes in the Euclidean space. The operator is closed,
permutation invariant and aligns well with the intuition that the size of the intersected set is smaller than
that of all input sets. The union operator is achieved via DNF, that is, union is the final step of concatenating
resultsofoperandboxes. SeveralworksextendQuery2Box, i.e., Query2Onto(Andreseletal.,2021)attempts
to model complex ontological axioms by materializing entailed triples and enforcing hierarchical relationship
using inclusion of the box embeddings; RotatE-Box (Adlakha et al., 2021) designs an additional rotation-
based Kleene plus ( +) operator denoting relational paths (we elaborate on the Kleene plus operator in
Section 6.1); NewLook (Liu et al., 2021) adds symbolic lookup from the adjacency tensor4to the operators,
modifies projection with MLPs and models the difference operator as attention over centers and offsets
(note that the difference operator is a particular case of the 2innegation query, we elaborate on that in
Section 6.1); Query2Geom (Sardina et al., 2023) replaces an attention-based intersection operator with a
simple non-parametric closed-form geometric intersection of boxes.
HypE (Choudhary et al., 2021b) extends the idea of Query2Box and embeds a query as a hyperboloid (two
parallelpairsofarc-alignedhorocycles)inaPoincaréhyperballtobettercapturethehierarchicalinformation.
A similar attention-based neuro-intersection operator is designed for the hyperboloid embeddings with the
goal to shrink the limits with DeepSets.
ConE (Zhang et al., 2021b), on the other hand, embeds queries on the surface of a set of unit circles. Each
query is represented as a cone section and the benefit is that in most cases the intersection of cones is still
a cone, and the negation/complement of a cone is also a cone thanks to the angular space bounded by 2π.
Based on this intuition, they design geometric neuro-intersection and negation operators. RoConE (He et al.,
4Incorrect implementation led to the major test set leakage and incorrect reported results.
20Published in Transactions on Machine Learning Research (11/2024)
Table 7: Geometric Neuro-Symbolic Processors. ⊙denotes element-wise multiplication, ⊕cdenotes Möbius
addition,⊙cdenotes Möbius scalar product. DSdenotes DeepSets neural network. NewLook only partially
implements negation as the difference operator.
Model Embedding Space Relation Projection Intersection Union Negation Distance
Query2Boxq,r∈R2d,e∈Rdq+rqc=Attn ({qi
c})DNF - d out+αdinQuery2Onto qo=min({qi
o})⊙σ(DS({qi
o}))
Query2Geom q,r∈R2d,e∈Rdq+rqc=1
2((qi
c+qi
o) + (qj
c−qj
o))DNF - d out+αdinqo=qc−(qi
c−qi
o)
RotatE-Box q,r∈C2d,e∈Cd(qc◦rc,qo+ro) -DNF /- d out+αdinDS({qi})
NewLookq,r∈R2d,e∈RdMLP[MLP(qc+rc)∥ qc=Attn ({qi
c,xi})DNFAttn ({qi
c})dout+αdinx∈T|R|×|E|×|E|MLP(ro)∥xt] qo=min({qi
o})⊙σ(DS({qi
o})) Attn ({qi
o,xi})
HypE q,r∈R2d,e∈Rdq⊕crqc=Attn ({qi
c})DNF - d out+αdinqo=min({qi
o})⊙cσ(DS({qi
o}))
ConEq,r= (θax,θap)g(MLP(q+r))θax=SemanticAvg (q1,...,qn)
DNF/DMθax=θax±π
dout+αdine= (θax,0)
θax∈[−π,π)dggatesθaxandθapθap=CardMin (q1,...,qn) θap= 2π−θapRoConE θap∈[0,2π]dq◦r
CylESame as ConE +g(MLP(q+r)) q=Attn ({qi}) DNF Same as ConE d out+αdinθhe∈(−π,π)d
eq
projection
(PERM)00.2 0.4740.571 1
intersection
(BetaE)q1
q2
({q1,q2})
q1
q2
union
(PERM)0 0.5 1
negation
(BetaE)q
(q)
Figure 11: Probabilistic Processors and their inductive biases.
2023)replacestheprojectionoperatorofConEwiththeangularrotationinthecomplexplane. CylE(Nguyen
et al., 2023b) turns cones into cylinders by adding an additional height dimension.
To sum up, the geometric-based processors are often designed with a strong geometric prior such that
properties of the logical/set operations can be better simulated or satisfied.
Probabilistic Processors. Instead of a geometric embedding space, probabilistic processors aim to model
the query and the logic/set operations in a probabilistic space. Some examples of implementing logical
operators in a probabilistic space are illustrated in Fig. 11. The aggregated characteristics are presented in
Table 8.
BetaE (Ren & Leskovec, 2020) builds upon the Beta distribution and embeds entities/queries as high-
dimensional Beta distribution with learnable parameters. The benefit is that one can design a parameterized
neuro-intersection operator over two Beta embeddings where the output is still a Beta embedding with more
concentrated density function. A neuro-negation operator can be designed by simply taking the reciprocal
of the parameters in order to flip the density. PERM (Choudhary et al., 2021a) looks at the Gaussian
distribution space and embeds queries as a multivariate Gaussian distribution. Since the product of Gaussian
probability density functions (PDFs) is still a Gaussian PDF, the neuro-intersection operator accordingly
calculates the parameters of the Gaussian embedding of the intersected set. NMP-QEM (Long et al., 2022)
21Published in Transactions on Machine Learning Research (11/2024)
projection
(scoring function)intersection 
(product t-norm)union 
(product t-conorm)negation 
(fuzzy)awin
0.10.50.3 0.20.30.9
0.02 0.15 0.27 0.10.50.3 0.20.30.9
0.28 0.65 0.93 0.10.50.3
0.90.50.7
Figure 12: Fuzzy-Logic Processors and fuzzy logical operators.
develops this idea further and represents a query as a mixture of Gaussians where logical operators are
modeled with MLP or attention over distribution parameters. LinE (Huang et al., 2022b) transforms the
Beta distribution into a discrete sequence of values. A similar neuro-negation operator is introduced by
taking the reciprocal as BetaE while designing a new neuro-intersection/union operator by taking element-
wise min/max. GammaE (Yang et al., 2022a) replaces Beta distribution with Gamma distribution as entity
and query embedding space. Parameterizing logical operators with operations over mixtures of Gamma
distributions, union and negation become closed, do not need DNF or DM transformations, and can be
executed sequentially along the query computation graph. Overall, probabilistic processors are similar to
geometric processors since they are all inspired by certain properties of the probability and geometry used
for embeddings and customize neuro-logic operators.
Table 8: Probabilistic Neuro-Symbolic Processors.
Model Embedding Space Relation Projection Intersection Union Negation Distance
BetaE q,e∈R2dMLPr(q) q= [(/summationtextwiαi,/summationtextwiβi)]DNF/DM1
qKL(e;q)
PERM q,e,r=N(µ,Σ)N(µq+µr, Σ−1
q= Σ−1
q1+ Σ−1
q2 Attn ({qi}) -(µq−µe)TΣ−1
q
(Σ−1
q+ Σ−1
r)−1)µq= Σq(Σ−1
q2µ1+ Σ−1
q1µ2) ( µq−µe)
LinE q,e∈Rk×hMLPr(q) min{q1,q2} max{q1,q2}[1
pq
1,...,1
pq
k]h∥q−e∥2
2
GammaE q,e∈R2dMLPr(q) q= [(/summationtextwiαi,/summationtextwiβi)] Attn ({qi}) [1
α,β] KL(e;q)
NMP-QEMe∈Rd
MLPr(q) Attn ({qi}) DNF MLP(q)∥e−/summationtextK
i=1ωq
iµq
i∥q=/summationtextK
i=1ωiN(µi,Σi)
Fuzzy-Logic Processors. Unlike the methods above, fuzzy-logic processors directly model all logical
operations using existing fuzzy logic theory (Klement et al., 2013; van Krieken et al., 2022) where intersection
can be expressed via t-norms and union via corresponding t-conorms (Section A.4). In such a way, fuzzy-
logic processors avoid the need to manually design or learn neural logical operators as in the previous two
processors but rather directly use established fuzzy operators commonly expressed as differentiable, element-
wise algebraic operators over vectors (Fig. 12). While intersection, union, and negation are non-parametric,
the projection operator might still be parameterized with a neural network. The aggregated characteristics
of fuzzy-logic processors are presented in Table 9. Generally, fuzzy processors aim to combine execution in
embedding space (vectors) with entity space (symbols). The described methods are different in designing
such a combination.
One of the first fuzzy processors is EmQL (Sun et al., 2020) that imbues entity and relation embeddings
with a count-min sketch (Cormode & Muthukrishnan, 2005). There, projection, intersection, and union are
performed both in the embedding space and in the symbolic sketch space, e.g., intersection is modeled as
an element-wise multiplication and union is an element-wise summation of two sketches. CQD (Arakelyan
et al., 2021) scores each atomic formula in a query with a pretrained neural link predictor and uses t-norms
and t-conorms to compute the final score of a query directly in the embedding space . CQD does not train
any neural logical operators and only requires pretraining the entity and relation embeddings with one-hop
links such that the projection operator is equivalent to top-kresults of the chosen scoring function, e.g.,
ComplEx (Lacroix et al., 2018). The idea was then extended in several directions: Query Tree Optimization
(QTO) (Bai et al., 2023c) added a look-up from the materialized tensor of scores of all possible triples
22Published in Transactions on Machine Learning Research (11/2024)
M∈[0,1]|R|×|E|×|E|to the relation projection step; CQDA(Arakelyan et al., 2023) and Var2Vec (Wang
et al., 2023a) learn an additional linear transformation of the entity-relation concatenation W[q,r]; Fuzzy
Inference with Truth Value (FIT) (Yin et al., 2023b) added several query rewriting steps to tackle DAG and
cyclic queries, LitCQD (Demir et al., 2023) added a jointly trained regression model on entities’ numerical
attributes to include numerical literals into queries and answers. CQD Onto (Andresel et al., 2023) further
adds the support for ontological axioms.
LogicE (Luus et al., 2021) designs logic embeddings for each entity with a list of lower bound – upper
boundpairs in range [0,1], which can be interpreted as a uniform distribution between the lower and upper
bound. LogicE executes negation and conjunction with continuous t-norms over the lower and upper bounds.
FuzzQE(Chenetal.,2022), TAR(Tangetal.,2022), andWFRE(Wangetal.,2023d)embedquerytoahigh-
dimensional fuzzy space [0,1]dand similarly use Gödel t-norm and Łukasiewicz t-norm to model disjunction,
conjunction and negation. FuzzQE and WFRE model relation projection as a relation-specific MLP whereas
TAR uses a geometric translation (element-wise sum). FLEX (Lin et al., 2022) and TFLEX (Lin et al.,
2023) embed a query as a mixture of feature and logic embedding. For the logic part, both methods use the
real logical operations in vector logic (Mizraji, 2008). TFLEX adds a temporal module conditioning logical
operators on the time embedding.
GNN-QE (Zhu et al., 2022) models the likelihood of all entities for each relation projection step with a
graph neural network NBFNet (Zhu et al., 2021). It further adopts product logic to directly model the
set operations (intersection, union, and negation) over the fuzzy set obtained after a relation projection.
GNN-QE employs a node labeling technique where a starting node is initialized with the relation vector
(while other nodes are initialized with zeros). This allows GNN-QE to be inductive and not rely on trainable
entity embeddings. UnRavL (Cucumides et al., 2024) extends the framework to queries that might not have
a starting anchor node. ENeSy (Xu et al., 2022), on the other hand, maintains both vector and symbolic
representations for queries, entities, and relations (where symbolic relations are encoded into Mrsparse
adjacency matrices). Logical operators are executed first in the neural space, e.g., relation projection is
RotatE composition function (Sun et al., 2019), and then get intertwined with symbolic representations.
Logical operators in the symbolic space employ a generalized version of the product logic and corresponding
t-(co)norms.
In summary, fuzzy-logic processors directly rely on established fuzzy logic formalisms to perform all the
logical operations in the query and avoid manually designing and learning neural operators in (possibly)
unbounded embedding space. The fuzzy logic space is continuous but bounded within [0,1]– this is both
the advantage and weakness of such processors. The bounded space is beneficial for closed logical operators
as their output values still belong to the same bounded space. On the other hand, most of the known
t-norms (and corresponding t-conorms) still lead to vanishing gradients and only the Product logic norms
are stable (van Krieken et al., 2022; Badreddine et al., 2022). Another caveat is designing an effective and
differentiable interaction mechanism between the fuzzy space [0,1]dand unbounded embedding space Rd(or
Cd) where relation representations are often initialized from. That is, re-scaling and squashing of vector
values when processing a computation graph might lead to noisy gradients and unstable training which is
observed, for instance, by GNN-QE that has to turn off gradients from all but last projection step.
5.3 Decoder
The goal of decoding is to obtain the final set of answers or a ranking of all the entities. It is the final
step of the query answering task after processing. Here we categorize the methods into two buckets: non-
parametric andparametric . Parametric methods require a parameterized method to score an entity (or
predict a regression target from the processed latents) while non-parametric methods can directly measure
the similarity (or distance) between a pair of query and entity on the graph. Most of the methods belong to
the non-parametric category as shown in the Distance column of processor tables Table 6, Table 7, Table 8,
Table 9. For instance, geometric models (Ren et al., 2020; Andresel et al., 2021; Adlakha et al., 2021;
Choudhary et al., 2021b; Zhang et al., 2021b) pre-define a distance function between the representation of
the query and that of an entity. Commonly employed distance functions are L1 (Hamilton et al., 2018; Ren
et al., 2022; Gebhart et al., 2023; Amayuelas et al., 2022; Tang et al., 2022; Xu et al., 2022; Long et al.,
2022), L2 (Huang et al., 2022b; Chen et al., 2022), or their variations (Luus et al., 2021; Lin et al., 2022;
23Published in Transactions on Machine Learning Research (11/2024)
Table 9: Fuzzy Neuro-Symbolic Processors.
Model Embedding Space Relation Projection Intersection Union Negation Distance
EmQLe,r∈Rd,q∈R3d
MIPS (q,[r,eh,et])(q1+q2)/2 ( q1+q2)/2- dot(q,e)bcount-min sketch b1⊙b2 b1+b2
CQDq,e,r∈Cd minq2d(q1,r,q2)or Product: q1·q2 q1+q2−q1·q2- dist (q,e)LitCQD topk(d(q1,r,ek))Gödel: min (q1,q2) max(q1,q2)
CQDA q,e,r∈Cdθ=W[q1,r] Product: q1·q2 q1+q2−q1·q2 1−qdist (q,e)W∈R2×2dtopk[ρθ(d(q1,r,ek))]Gödel: min (q1,q2) max(q1,q2) (1 + cos( πq))/2
Var2Vecq,e,r∈Cdq2=W[q1,r] Product: q1·q2 q1+q2−q1·q21−q dist (q,e)W∈Rd×2dd(q1,r,q2) Gödel: min (q1,q2) max(q1,q2)
QTO q,e,r∈Cdd(q1,r,q2)q1·q2 q1+q2−q1·q2d(q1,r,q2)dist (q,e)FIT M∈[0,1]|R|×|E|×|E|rowq1(Mr) rowq1(1−Mr)
LogicEq,e= ([li,ui],σ(max(0,max(0, ([⊤(l(1)
i,...,l(n)
i),
DM([1−li,
∥q−e∥1 li,ui∈[0,1])d
i=1 [r,q]F1)F2)F3)⊤(u(1)
i,...,u(n)
i)]) 1 −ui])
r∈Rdi= 1...d i = 1...d
FuzzQE q,e∈[0,1]dσ(MLPr(q))Product: q1·q2 q1+q2−q1·q2 1−qdot(q,e)Gödel: min (q1,q2) max(q1,q2) -
WFRE q,e∈[0,1]dσ(MLPr(q)) min(q1,q2) max(q1,q2) 1−q WFR(q,e)
TAR q,e,r∈Rdq+r Attn (q1,q2) max( q1,q2) 1−q∥q−e∥
GNN-QE q∈R|E|,r∈Rdσ(GNN(q,G)) q1·q2 q1+q2−q1·q2 1−q BCE(q)
FLEXq= (θf,θl)g(MLP([θf+θf,r;θf=/summationtext
iaiθqi,fθf=/summationtext
iaiθqi,fθf=L·tanh(
θf∈[−L,L]dθl+θl,r])) θl=/summationtext
iθqi,l− MLP([θf;θl]))∥θe
f−θq
f∥1+
e= (θf,0)ggatesθf,θe. θl= Πiθqi,l/summationtext
1≤i<j≤nθqi,lθqj,l+θl= 1−θlθq
l
θl∈[0,1]d···+ (−1)n−1Πiθqi,l
TFLEXq,r= (qe
f,qe
l,qt
f,qt
l)Entity:g(MLP(/summationtext
iαqe
i,f,/circlemultiplytext
i({qe
i,l}),/summationtext
iαqe
i,f,/circleplustext
i({qe
i,l}), fe
not(qe
f),∥ee
f−qe
f∥1+
qe
f,qt
f,ee
f∈Rdq+r+t))/summationtext
iβqt
i,f,/circlemultiplytext
i({qt
i,l})/summationtext
iβqt
i,f,/circlemultiplytext
i({qt
i,l})⊖(qe
l),qt
f,qt
l qe
f∥1+qe
l
e= (ee
f,0,0,0) Time:g(MLP(/summationtext
iαqe
i,f,/circlemultiplytext
i({qe
i,l}),/summationtext
iαqe
i,f,/circleplustext
i({qe
i,l}), qe
f,qe
l,∥tt
f−qt
f∥1+
qe
l,qt
l∈[0,1]dq1+r+q2))/summationtext
iβqt
i,f,/circlemultiplytext
i({qt
i,l})/summationtext
iβqt
i,f,/circlemultiplytext
i({qt
i,l})ft
not(qt
f),⊖(qt
l) qt
l
ENeSyq,e,r∈CdNeural: q◦r
g(p1·p2) g(p1+p2−p1·p2)g(α
|E|−p)∥q−e∥ pq,pe∈{0,1}|E|Symb:g(pqMr)⊤
Mr∈{0,1}|E|×|E|g=x/sum(x)
NQE q,e,r∈[0,1]dσ(Trf(q,Gq)) q1·q2 q1+q2−q1·q2 1−q dot(q,e)
2023), cosine similarity (Wang et al., 2019; Mai et al., 2019; Daza & Cochez, 2020; Wang et al., 2023e), dot
product (Sun et al., 2020; Ren et al., 2022; Alivanistos et al., 2022; Liu et al., 2022; Bai et al., 2022; Luo et al.,
2023), or naturally model the likelihood of all the entities without the need of a distance function (Arakelyan
et al., 2021; Kotnis et al., 2021; Zhu et al., 2022; Pflueger et al., 2022; Bai et al., 2023c; Arakelyan et al., 2023;
Wang et al., 2023a). Probabilistic models often employ KL divergence (Ren & Leskovec, 2020; Yang et al.,
2022a) or Mahalanobis distance (Choudhary et al., 2021a). A rather exotic approach is Wasserstein-Fisher-
Rao metric (WFR) used in WFRE Wang et al. (2023d). WFR is an optimal transport (OT) metric between
distributions. Due to its computational complexity, the metric is approximated by the 1D convolution-based
Sinkhorn optimization routine.
One important direction (orthogonal to the distance function) that current methods largely ignore is how
to perform efficient answer entity retrieval over extremely large graphs with billions of entities. A scalable
and approximate nearest neighbor (ANN) search algorithm is necessary. Existing frameworks including
FAISS(Johnsonetal.,2019)orScaNN(Guoetal.,2020)providescalableimplementationsofANN.However,
ANN is limited to L1, L2 and cosine distance and mostly optimized for CPUs. It is still an open research
problem how to design efficient scalable ANN search algorithms for more complex distance functions such
as KL divergences so that we can retrieve with much better efficiency for different CLQA methods with
different distance functions (preferably, using GPUs).
We conjecture that parametric decoders are to gain more traction in numerical tasks on top of plain entity
retrieval for query answering. Such tasks might involve numerical and categorical features on node-, edge-,
and graph levels, e.g., training a regressor to predict numerical values for node attributes like age,length,
etc. Besides, a parametric decoder gives new opportunities to generalize to inductive settings where we may
have unseen entities during evaluation. SE-KGE (Mai et al., 2020) takes a step in this direction by predicting
geospatial coordinates of query targets. LitCQD (Demir et al., 2023) expands the set of query variables and
answers to continuous numerical values by joint training of a link prediction model between entities and a
regression model on top of entities’ attributes with numerical values.
24Published in Transactions on Machine Learning Research (11/2024)
5.4 Computation Complexity
Here we analyze the time complexity of different query reasoning models categorized by different operations,
including relation projection, intersection, union, negation and answer retrieval after obtaining the repre-
sentation of the query. We list the asymptotic complexity in Table 10 for methods that perform stepwise
encoding of the query graph, and Table 11 for methods (mostly GNN-based) that encode the whole query
graph simultaneously including projection and other logic operations.
Table 10: Time complexity of each operation on G= (E,R,S). Across all methods, we denote the embedding
dimension and hidden dimension of MLPs as d, number of layers of MLPs/GNNs as l, number of branches
in an intersection operation as i, number of branches in a union operation as u.
Model Projection Intersection Negation Union Answer Retrieval Definitions
GQE
O(d) O(ild2) - - O(|E|d) -GQE+hashing
RotatE-m
Distmult-m
ComplEx-m
CGA O(d2) O(id+d2) - - O(|E|d) -
Query2Particles O(Kd2) O(iKd2) O(Kd2) DNF O(|E|dK) K:#particles.
SignalE O(d) O(ild2) O(d) DNF O(|E|d) -
MLPMix O(ld2) O(ild2) O(ld2) DNF O(|E|d) -
Query2BoxO(d) O(ild2) - DNF O(|E|d) -Query2Onto
Query2Geom O(d) O(d) - DNF O(|E|d) -
RotatE-Box O(d) - - DNF / O(uld2) O(|E|d) -
NewLook O(|E|d+ld2)O(i(|E|+ld2)) O(ld2) DNF O(|E|d) -
HypE O(d) O(ild2) - DNF O(|E|d) -
ConE/BetaE O(ld2) O(ild2) O(d) DNF O(|E|d) -
PERM O(d2) O(id3) - DNF O(|E|d2) -
LinE O(ld2) O(d) O(d) O(d) O(|E|d) -
GammaE O(ld2) O(ild2) O(d) O(uld2) O(|E|d) -
NMP-QEM O(Kld2) O(iK2d) O(Kld2) DNF O(|E|dK) K:# centers
EmQL O(|E|d) O(d) - O(d) O(|E|d) -
CQD-CO O(|E|d) Opt - Opt O(|E|d) -
CQD-Beam O(|E|d) O(|E|kd) - O(|E|kd) O(|E|d) -
QTO O(|E|) O(|E|) O(|E|) O(|E|)T∗(v) :the
O(max k|T∗(vk) maximum truth value
>0||E|) for the subquery
rooted at node v.
LogicE O(ld2) O(d) O(d) O(d) O(|E|d) -
FuzzQE O(d2) O(d) O(d) O(d) O(|E|d) -
TAR O(d) O(d) O(d) O(d) O(|E|d) -
GNN-QE O(|E|d2+|S|d) O(|E|) O(|E|) O(|E|) O(|E|) -
FLEX O(ld2) O(ild2) O(ld2) O(uld2+ 2ud) O(|E|d) -
TFLEX O(ld2) O(ild2) O(ld2) O(uld2) O(|E|d) -
Table 11: Time complexity of answering a query for methods that directly encode the query graph. Besides
the operators defined in Table 10, we denote nqas average degree of the query graph Gq= (Eq,Rq,Sq).
Model Projection Intersection Negation Union Answer Retrieval
MPQE / GNNQ O(d2nql|Eq|) - - O(|E|d)
StarQEO(d2nql|Eq|+|Sq||qp|d)- - O(|E|d)
LMPNN O(d2nql|Eq|) DNFO(|E|d)
BiQEO((|Eq|d2+|Eq|2d)l) - - O(|E|d)
kgTransformer O((d2nq|Eq|+|Eq|n2
qd)l) -O(|E|d)
SQE O((|Eq|d2+|Eq|2d)l) O(|E|d)
25Published in Transactions on Machine Learning Research (11/2024)
6 Queries
The third direction to segment the methods is from the queries point of view. Under the queries category, we
have three subcategories: Query Operators ,Query Patterns , andProjected Variables . For query operators,
methods have different operator expressiveness, which means the set of query operators one model is able
to handle including existential quantification ( ∃), conjunction (∧), disjunction (∨), negation (¬), Kleene
plus (+), filter and various aggregation operators. For query patterns, we refer to the structure/pattern of
the (optimized) query plan, ranging from paths and trees to arbitrary directed acyclic graphs (DAGs) and
cyclic patterns. As to projected variables (by projected we refer to target variables that have to be bound
to particular graph elements like entity or relation), queries might have a different number (zero or more) of
target variables. We are interested in the complexity of such projections as binding of two and more variables
involves relational algebra (Codd, 1970) and might result in a Cartesian product of all retrieved answers.
6.1 Query Operators
Different query processors have different expressiveness in terms of operators a method can handle. Through-
out all the works, we compiled Table 12 that classifies all methods based on the supported operators.
Table 12: Query answering processors and supported query operators. Processors supporting unions ∨also
support projection and intersection ( ∧). Models supporting negation ( ¬) also support unions, projections,
and intersections.
Projection and Intersection ( ∧) Union ( ∨) Negation ( ¬) Kleene + Filter & Aggr
GQE(Hamilton et al., 2018)
GQE hashed (Wang et al., 2019)
CGA(Mai et al., 2019)
TractOR (Friedman & Van den Broeck, 2020)
MPQE (Daza & Cochez, 2020)
BiQE(Kotnis et al., 2021)
Sheaves (Gebhart et al., 2023)
StarQE (Alivanistos et al., 2022)
RotatE-m, DistMult-m,
ComplEx-m (Ren et al., 2022)
GNNQ (Pflueger et al., 2022)Query2Box (Ren et al., 2020)
EmQL (Sun et al., 2020)
Query2Onto (Andresel et al., 2021)
HypE(Choudhary et al., 2021b)
NewLook (Liu et al., 2021)
PERM (Choudhary et al., 2021a)
CQD(Arakelyan et al., 2021)
kgTransformer (Liu et al., 2022)
Query2Geom (Sardina et al., 2023)
LitCQD (Demir et al., 2023)
BiDAG (Xu et al., 2023a)
NRN(Bai et al., 2023a)
CQD Onto (Andresel et al., 2023)BetaE(Ren & Leskovec, 2020)
ConE(Zhang et al., 2021b)
LogicE (Luus et al., 2021)
MLPMix (Amayuelas et al., 2022)
Query2Particles (Bai et al., 2022)
LinE(Huang et al., 2022b)
GammaE (Yang et al., 2022a)
NMP-QEM (Long et al., 2022)
FuzzQE (Chen et al., 2022)
TAR(Tang et al., 2022)
GNN-QE (Zhu et al., 2022)
FLEX(Lin et al., 2022)
TFLEX (Lin et al., 2023)
ENeSy (Xu et al., 2022)
QTO(Bai et al., 2023c)
SignalE (Wang et al., 2022)
LMPNN (Wang et al., 2023e)
NQE(Luo et al., 2023)
Var2Vec (Wang et al., 2023a)
CQDA(Arakelyan et al., 2023)
SQE(Bai et al., 2023b)
RoConE (He et al., 2023)
CylE(Nguyen et al., 2023b)
FIT(Yin et al., 2023b)
WFRE (Wang et al., 2023d)
LARK (Choudhary & Reddy, 2023)
SCoNe (Nguyen et al., 2023a)
Query2Triple (Xu et al., 2023b)
UnRavL (Cucumides et al., 2024)
UltraQuery (Galkin et al., 2024)RotatE-Box
(Adlakha et al., 2021)LitCQD (numbers)
(Demir et al., 2023)
We start from simplest conjunctive queries that involve only existential quantification ( ∃) and conjunction
(∧) and gradually increase the complexity of supported operators.
Existential Quantification ( ∃).When∃appears in a query, this means that there exists at least one
existentially quantified variable. For example, given a query “At what universities do the Turing Award
winners work?” and its logical form q=U?.∃V:win(TuringAward ,V)∧university (V,U ?), hereVis
the existentially quantified variable. Query processors model existential quantification by using a relation
projection operator. Mapping to query languages like SPARQL, relation projection is equivalent to a triple
patternwith one variable, e.g.,{TuringAward win ?v} (Fig. 13). Generally, as introduced in Section 5.2,
query embedding methods embed a query in a bottom-up fashion, starting with the embedding of the anchors
26Published in Transactions on Machine Learning Research (11/2024)
(leaf nodes) and gradually traversing the query tree up to the root. In such a way, query embedding methods
(e.g., geometricorprobabilistic)explicitlyobtainanembeddingfortheexistentiallyquantifiedvariables. The
embeddings/representations of these variables are calculated by a relation projection function implemented
as shallow vector operations (Hamilton et al., 2018; Ren et al., 2020; Choudhary et al., 2021b; Arakelyan
et al., 2021; Ren et al., 2022) or deep neural nets (Ren & Leskovec, 2020; Zhang et al., 2021b; Bai et al.,
2022; Amayuelas et al., 2022). Another set of methods based on GNNs and Transformers directly assigns
a learnable initial embedding for the existentially quantified variables. These embeddings are then updated
through several message passing layers over the query plan (Daza & Cochez, 2020; Alivanistos et al., 2022;
Pflueger et al., 2022; Wang et al., 2023e) or attention layers over the serialized query plan (Kotnis et al.,
2021; Liu et al., 2022).
The major drawback of existing neural query processors is the assumption of at least one anchor en-
tity in the query from which the answering process starts and relation projections can be executed. It
remains an open challenge and an avenue for future work to support queries without anchor entities,
e.g.,q1=U?.∃v1,v2:win(v1,v2)∧university (v2,U?), and queries where relations are variables, e.g.,
q2=r?:r?(TuringAward ,Bengio ), that can be framed as the relation prediction task.
Universal Quantification ( ∀).A universally quantified variable ∀x.P(x)means that a logical for-
mulaP(x)holds for all possible x. Usually, the universal quantifier does not appear in facts-only ABox
graphs without types and complex axioms (see Section 4.3) as it would imply that some entity is con-
nected to all other entities. For example, ∀V.win(TuringAward ,V)without other constraints implies that
all entities are connected to TuringAward by thewinrelation (which does not occur in practice). How-
ever, universal quantifiers are more useful when paired with the class hierarchy (unary relations), e.g.,
∃?paper,∀r∈Researcher :Researcher (r)∧authored (r,?paper )means that the authored relation projection
would be applied only to entities of class Researcher .
Currently, existing CLQA approaches do not support universal quantifiers explicitly nor the datasets include
queries with the∀quantifier. Still, using the basic identity ∀x.P(x)≡¬(∃x.¬P(x))it is possible to model
the universal quantifier by any approach supporting existential quantification ∃and negation¬. By default,
we assume the closed-world assumption.We leave the implications of universal quantification pertaining to
the open-world assumption (OWA) out of the scope of this work.
Conjunction (∧).We elaborate on the differences of those query patterns in the following Section 6.2
and emphasize our focus on more complex intersection queries going beyond simpler path-like queries.
Query processors, as described in Section 5.2, employ different parameterizations of conjunctions as
permutation-invariant set functions. A family of neural processors (Hamilton et al., 2018; Wang et al.,
2019; Ren et al., 2022) often resort to the DeepSet architecture (Zaheer et al., 2017) that first projects each
set element independently and then pools representations together with a permutation-invariant function
(e.g., sum, mean) followed by an MLP. Alternatively, (Mai et al., 2019; Bai et al., 2022) self-attention, can
serve as a replacement of the DeepSet where set elements are weighted with the attention operation. The
other family of neural processors combine projection and intersection by processing the whole query graph
with GNNs (Daza & Cochez, 2020; Alivanistos et al., 2022; Pflueger et al., 2022; Wang et al., 2023e) or with
the Transformer over linearized query sequences (Kotnis et al., 2021; Liu et al., 2022). Geometric proces-
sors (Ren et al., 2020; Liu et al., 2021; Choudhary et al., 2021b; Zhang et al., 2021b; He et al., 2023; Nguyen
et al., 2023b) implement conjunction as the attention-based average of centroids and offsets of respective
geometric objects (boxes, hyperboloids, cones, or cylinders). Probabilistic processors (Ren & Leskovec, 2020;
Choudhary et al., 2021a; Huang et al., 2022b; Yang et al., 2022a) implement intersection as a weighted sum
of parametric distributions that represent queries and variables.
Fuzzy-logic processors (Arakelyan et al., 2021; Luus et al., 2021; Chen et al., 2022; Zhu et al., 2022; Wang
et al., 2023d; Yin et al., 2023b; Demir et al., 2023) commonly resort to t-norms, generalized versions of
conjunctions in the continuous [0,1]space and corresponding t-conorms for modeling unions (Section A.4).
Often, due to the absence of a principal study, the choice of the fuzzy logic is a hyperparameter. We posit
that such a study is an important avenue for future works in fuzzy processors. More exotic neuro-symbolic
methods for modeling conjunctions include element-wise product of count-min sketches (Sun et al., 2020) or
27Published in Transactions on Machine Learning Research (11/2024)
T ?v ?uwin uni⋀
{
  TuringAward win ?v.
}
Existential Quantiﬁcation 
(Relation Projection) 
T ?vwinSELECT ?uni WHERE 
{
  TuringAward win     ?v .
  DeepLearning field   ?person . 
  ?person university   ?uni    . 
}
SPARQL 
Basic 
Graph 
Pattern 
Computation 
Graph SPARQL Basic Graph Pattern 
Intersection 
(Join) Union 
(OR) Negation 
(NOT) {
  TuringAward win ?v.
  ?vuni ?u.
}{ DeepLearning field ?v.} 
UNION
{ Mathematics field ?v.}
D
M?v
?v⋁ﬁeld 
ﬁeld u 
u { 
 ?s ?p ?v. 
 FILTER NOT EXISTS 
 {DeepLearning field ?v.}
}
D ?vﬁeld n a
bwin
ﬁeld i 
n 
Kleene Plus ( +)
(Property Path) { 
 JohnDoe knows+ ?v.
}
J ?vknows knows 
…T
D?v
?v⋀win
ﬁeld i 
i Intersection 
(Join) {
  TuringAward win  ?v.
  DeepLearning field ?v.
}
TuringAward 
Figure 13: Query operators (relation projection and intersection), corresponding SPARQL basic graph pat-
terns (BGP), and their computation graphs. Relation Projection (left) corresponds to a triple pattern.
as a weighted sum in the feature logic (Lin et al., 2022; 2023). Finally, some processors (Tang et al., 2022;
Xu et al., 2022) perform conjunctions both in the embedding and symbolic space with neural and fuzzy
operators.
We note that certain neural query processors that embed queries directly (Hamilton et al., 2018; Wang et al.,
2019; Mai et al., 2019; Friedman & Van den Broeck, 2020; Ren et al., 2022) or via GNN/Transformer encoder
over a query graph (Daza & Cochez, 2020; Kotnis et al., 2021; Gebhart et al., 2023; Alivanistos et al., 2022;
Das et al., 2022; Pflueger et al., 2022) support only projections and intersections, that is, their extensions
to more complex logical operators are non-trivial and might require changing the underlying assumptions of
modeling entities, variables, and queries. In some cases, support of unions might be enabled when re-writing
a query to the disjunctive normal form (discussed below).
Disjunction (∨).Query processors implement the disjunction operator in several ways. However, model-
ing disjunction is notoriously hard since it requires modeling any powerset of entities on the graph in a vector
space. Before delving into details about different ways of modeling disjunction, we first refer the readers to
the Theorem 1 in Query2Box (Ren et al., 2020). The theorem proves that we need the VC dimension of the
function class of the distance function to be around the number of entities on the graph.
The theorem shows that in order to accurately model anyEPFO query with the existing framework, the
complexity of the distance function measured by the VC dimension needs to be as large as the number of
KG entities. This implies that if we use common distance functions based on hyper-plane, Euclidean sphere,
or axis-aligned rectangle,5their parameter dimensionality needs to be at least Θ(|E|)for real KGs. In other
words, the dimensionality of the logical query embeddings needs to be Θ(|E|), which is not low-dimensional;
thus not scalable to large KGs and not generalizable in the presence of unobserved KG edges.
The first idea proposed in Query2Box (Ren et al., 2020) is that given a model has defined a distance function
betweenaqueryrepresentationandtheentityrepresentation, thenaquerycanbetransformed(orre-written)
into its equivalent disjunctive normal form (DNF),i.e., a disjunction of conjunctive queries. For example,
we can safely convert a query (A∨B)∧(C∨D)to((A∧C)∨(A∧D)∨(B∧C)∨(B∧D)), whereA,B,C,D
are atomic formulas. In such a way, we only need to process disjunction ∨at the very last step. For models
that have defined a distance function between the query representation and entity representation d(q,e)
(such as geometric processors (Ren et al., 2020; Andresel et al., 2021; Adlakha et al., 2021; Choudhary
et al., 2021b; Zhang et al., 2021b; Sardina et al., 2023; He et al., 2023; Nguyen et al., 2023b) and some
neural processors (Liu et al., 2022; Amayuelas et al., 2022; Wang et al., 2023e), the idea of using DNF to
handle disjunction is to (1) embed each atomic formula / conjunctive query in the DNF into a vector qi, (2)
5For the detailed VC dimensions of these function classes, see Vapnik (2013). Crucially, their VC dimensions are all linear
with respect to the number of parameters d.
28Published in Transactions on Machine Learning Research (11/2024)
T ?v ?uwin uni⋀
{
  TuringAward win ?v.
}
T ?vwinSELECT ?uni WHERE 
{
  TuringAward win     ?v .
  DeepLearning field   ?person . 
  ?person university   ?uni    . 
}
SPARQL 
Basic 
Graph 
Pattern 
Computation 
Graph SPARQL Basic Graph Pattern 
{
  TuringAward win ?v.
  ?vuni ?u.
}{ DeepLearning field ?v.} 
UNION
{ Mathematics field ?v.}
D
M?v
?v⋁ﬁeld 
ﬁeld u 
u { 
 ?s ?p ?v. 
 FILTER NOT EXISTS 
 {DeepLearning field ?v.}
}
D ?vﬁeld n a
bwin
ﬁeld i 
n 
Kleene Plus ( +)
(Property Path) { 
 JohnDoe knows+ ?v.
}
J ?vknows knows 
…
T
D?v
?v⋀win
ﬁeld i 
i {
  TuringAward win  ?v.
  DeepLearning field ?v.
}TuringAward 
Existential Quantiﬁcation 
(Relation Projection) 
Intersection 
(Join) 
Intersection 
(Join) 
Union 
(OR) 
Negation 
(NOT) 
Figure 14: Query operators (union, negation, Kleene plus), corresponding SPARQL basic graph patterns
(BGP), and their computation graphs.
calculate the distance between the representation/embedding of each atomic formula / conjunctive query
and the entity d(qi,e), (3) take the minimum of the distances mini(d(qi,e)). The intuition is that since
disjunction models the union operation, as long as the node is close to one atomic formula / conjunctive
query, it should be close to the whole query. Potentially, many neural processors with the defined distance
function and originally supporting only intersection and projection can be extended to supporting unions
with DNF. One notable downside of this modeling is that it is exponentially expensive (to the number of
disjunctions) in the worst case when converting a query to its DNF.
Another category of mostly probabilistic processors (Choudhary et al., 2021a; Yang et al., 2022a) proposes
a neural disjunction operator implemented with the permutation-invariant attention over the input set with
theclosureassumption that the result of attention weighting union remains in the same probabilistic space
as its inputs. Such models design a more black-box framework to handle the disjunction operation under
the strong closure assumption that might not be true in all cases.
The third way of modeling disjunction is based on the De Morgan’s laws (Ren & Leskovec, 2020). According
totheDeMorgan’slaws(DM),thedisjunctionisequivalenttothenegationoftheconjunctionofthenegation
of the statements making up the disjunction, i.e.,A∨B=¬(¬A∧¬B). For methods that can handle the
negation operator (detailed in the following paragraph), they model disjunction by using three negation
operations and one conjunction operation. DM conversion was explicitly probed in probabilistic (Ren &
Leskovec, 2020), geometric (Zhang et al., 2021b), and fuzzy (Luus et al., 2021) processors.
Finally, most fuzzy-logic (Arakelyan et al., 2021; Chen et al., 2022; Tang et al., 2022; Zhu et al., 2022; Bai
et al., 2023c; Arakelyan et al., 2023; Wang et al., 2023a;d; Yin et al., 2023b; Demir et al., 2023) processors
employt-conorms , generalized versions of disjunctions in the continuous [0,1]space (Section A.4). More
exotic versions of neuro-symbolic disjunctions include element-wise summation of count-min sketches (Sun
et al., 2020), feature logic operations (Lin et al., 2022; 2023), as well as performing a union in both embedding
and symbolic spaces (Tang et al., 2022; Xu et al., 2022) with fuzzy operators.
Negation (¬).For negation operation, the goal is to model the complement set, i.e., the answersAq
to a query q=V?:¬r(v,V ?)are the exact complement of the answers Aq′to queryq′=V?:r(v,V ?):
Aq=V/Aq′. Correspondingly, negation in SPARQL can be implemented with FILTER NOT EXISTS or
MINUSclauses. For example (Fig. 14), a logical formula with negation ¬field(DeepLearning, V )is equivalent
to the SPARQL BGP {?s ?p ?v. FILTER NOT EXISTS {DeepLearning field ?v}} where {?s ?p ?v}
models the universe set (1) of all facts that gets filtered by the triple pattern.
Modeling the universe set (1) and its complement is the key problem when designing a negation operator
in neural query processors, e.g., an arbitrary real Ror complex Cspace is unbounded such that 1is not
29Published in Transactions on Machine Learning Research (11/2024)
defined. For that reason, many neural processors do not support the negation operator. Still, there exist
several approaches to handle negation.
The first line of works (Bai et al., 2022; Amayuelas et al., 2022; Long et al., 2022) designs a purely neural
MLP-based negation operator over the query representation avoiding the universe set altogether. Similarly,
a token of the negation operator can be included into the linearized query representation (Bai et al., 2023b)
to be encoded with Transformer or recurrent network. A step aside from purely neural operators is taken by
GNN-based processors (Wang et al., 2023e) that treat a negation edge as a new edge type during message
passing over the query computation graph.
The second line is customized to different embedding spaces and aims to simulate the calculation of the
universe and complement in the embedding space, e.g., using geometric cones (Zhang et al., 2021b; He et al.,
2023) or cylinders (Nguyen et al., 2023b), parameters are angles θsuch that the space (and, hence, 1) is
bounded to 2πand the complement is straight 2π−θ. Probabilistic methods (Ren & Leskovec, 2020; Huang
et al., 2022b; Yang et al., 2022a) naturally represent negation as an inverse of distribution parameters.
Thirdly, fuzzy logic processors explicitly model the universe set 1and the complement over the same real
valued logic space. For instance, LogicE (Luus et al., 2021), FuzzQE (Chen et al., 2022), WFRE (Wang
et al., 2023d) restrict the query embedding space to the range [0,1]dwhere each query q∈[0,1]dis a vector.
This way, the universe 1is represented with a vector of all ones (in the embedding space 1d) and negation is
simply 1−q. TAR (Tang et al., 2022), GNN-QE (Zhu et al., 2022), FIT (Yin et al., 2023b) operate over fuzzy
sets where each entity has a corresponding scalar q∈[0,1]in the bounded range. Therefore, the universe
1can still be a vector of all ones (in the entity space 1|E|) and negation is 1−q. ENeSy (Xu et al., 2022)
defines the universe as the uniform distribution over the entity space with each element weightingα
|E|(αis
a hyperparameter). CQDA(Arakelyan et al., 2023) employs a strict cosine fuzzy negation1
2(1 + cos(πq))
over scalar scores q. More exotic processors (Lin et al., 2022; 2023) employ feature logic for modeling
negation. We also note that the difference operator introduced in Liu et al. (2021) is in fact a common
intersection-negation ( 2in) query pattern used in all standard benchmarks (Section 7).
Kleene Plus (+) and Property Paths. Kleene Plus is an operator that applies compositionally and
recursively to any regular expression (RegEx) that denotes one or more occurrence of the specified pattern.
Regular expressions exhibit a direct connection to property paths in SPARQL. We defined a very basic regular
graph query in Definition A.7, here we generalize that further to property paths. To define property paths
more formally, given a set of relations Rand operators{+,∗,?,!,ˆ,/,|}, a property path pcan be obtained
from the recursive grammar p::=r|p+|p∗|p?|!p|ˆp|p1/p2|“p1|p2”Here,ris any element of R,+is
a Kleene Plus denoting one or more occurrences,∗is a Kleene Star denoting zero or more occurrences, ?
denoteszero or one occurences, !denotes negation of the relation or path, ˆptraverses an edge of type pin the
opposite direction, p1/p2is a sequence of relations (corresponds to relation projection ), andp1|p2denotes an
alternative path of p1orp2(corresponds to a unionoperation). For example (Fig. 14), an expression with
Kleene plus knows (JohnDoe,V)+can be represented as a SPARQL property path {JohnDoe knows+ ?v.} .
Property paths are non-trivial to model for neural query processors due to compositionalilty and recursive
nature. To the best of our knowledge, RotatE-Box (Adlakha et al., 2021) is the only geometric processor
that handles the Kleene Plus operator implementing the subset of operators {+,/,|}. RotatE-Box provides
two ways to handle Kleene Plus. The first method is to define a r+embedding for each relation r∈R,
note this is independent and separate from the regular relation embedding for r; another way is to use a
trainable matrix to transform the relation embedding rto the r+embedding. Note the two methods do
not support Kleene Plus over paths. RotatE-Box also implements relation projection (as a rotation in the
complex space) and union (with DeepSets or DNF) but does not support the intersection operator.
We hypothesize that better support of the property paths vocabulary might be one of main focuses in future
neuralqueryprocessors. ParticularlyforKleenePlus, someunresolvedissuesincludesupportingidempotence
((r+)+=r+) and infinite union of sets ( r+=r|(r/r)|(r/r/r )...).
Filter. Filter is an operation that can be inserted in a SPARQL query. It takes any expression of boolean
type as input and aims to filter the results based on the boolean value, i.e., only the results rendered True
30Published in Transactions on Machine Learning Research (11/2024)
T ?v ?uwin uni⋀
{
  TuringAward win ?v.
}
T ?vwinSELECT ?uni WHERE 
{
  TuringAward win     ?v .
  DeepLearning field   ?person . 
  ?person university   ?uni    . 
}
SPARQL 
Basic 
Graph 
Pattern 
Computation 
Graph SPARQL Basic Graph Pattern 
{
  TuringAward win ?v.
  ?vuni ?u.
}{ 
 StephenKing  wrote ?book.
 ?book numPages  ?pages 
 FILTER ?pages > 100. 
}SELECT COUNT(?book) as ?n 
{ 
 StephenKing  wrote ?book. 
}a
bwin
ﬁeld i 
n 
Optional 
(LEFT JOIN) { 
 King wrote ?book.
 OPTIONAL 
  {King award ?a.}
}
T
D?v
?v⋀win
ﬁeld i 
i {
  TuringAward win  ?v.
  DeepLearning field ?v.
}TuringAward 
Existential Quantiﬁcation 
(Relation Projection) 
Intersection 
(Join) 
Intersection 
(Join) 
FIL TER Aggregations 
(COUNT) 
{ DeepLearning field ?v.} 
UNION
{ Mathematics field ?v.}
D
M?v
?v⋁ﬁeld 
ﬁeld u 
u { 
 ?s ?p ?v. 
 FILTER NOT EXISTS 
 {DeepLearning field ?v.}
}
D ?vﬁeld n Kleene Plus ( +)
(Property Path) { 
 JohnDoe knows+ ?v.
}
J ?vknows knows 
…Union 
(OR) 
Negation 
(NOT) 
S ?bwrote 
?ppages 
>100? K ?bwrote 
?a award S ?bwrote ?nPred 
optional 
Figure 15: Query operators ( Filter,CountAggregation, Optional ), corresponding SPARQL basic graph
patterns (BGP), and their computation graphs.
under the expression will be returned. The boolean expression can thus be seen as a condition that the
answers to the query should follow. For the filter operator, we can do filter on values/literals/attributes,
e.g.,Filter (Vdate≥“2000−01−01”&&Vdate≤“2000−12−31”)means we would like to filter dates not
in the year 2000; Filter (LANG (Vbook) = “en”)means we would like to filter books not written in English,
Filter (?pages>100)means returning the books that have more than 100 pages (as illustrated in Fig. 15).
To the best of our knowledge, there does not exist a reasoning model that claims to handle all possible Filters,
which leaves room for future work on this direction. However, the first attempt towards handling filters is
taken by LitCQD (Demir et al., 2023) that allows greater than ,less than , andequalfiltering operators
over numerical values and treats them as conjunctive terms to the main logical query, e.g.,lt(?pages,100).
Such terms are processed by a jointly trained regression model.
We envision several possibilities to support filtering in neural query engines: (1) the simplest option used
by Thorne et al. (2021a;b) in natural language engines is to defer filtering to the postprocessing stage when
the set of candidate nodes is identified and their attributes can be extracted by a lookup. (2) Filtering often
implies reasoning over literal values and numerical node attributes, that is, processors supporting continuous
values (as described in Section 4.1) might be able to perform filtering in the latent space by attaching, for
instance, a parametric regressor decoder (Section 5.3) when predicting ?pages>100.
Aggregation. Aggregation is a set of operators in SPARQL queries including COUNT(return the number
of elements), MIN,MAX,SUM,AVG(return the minimum / maximum / sum / average value of all elements),
SAMPLE(return any sample from the set). For example (Fig. 15), given a triple pattern {StephenKing wrote
?book.}, the clause COUNT (?book) as ?n returns the total number of books written by StephenKing .
Most aggregation operators require reasoning over sets of numerical values/literals. Such symbolic operations
have long been considered a challenge for neural models (Hendrycks et al., 2021). How to design a better
representation for numerical values / literals requires remains an open question. Some neural query proces-
sors (Ren et al., 2020; Ren & Leskovec, 2020; Zhang et al., 2021b; Zhu et al., 2022), however, have the means
to estimate the cardinality of the answer set (including predicted hard answers) that directly corresponds
to the COUNTaggregation over the target projected variable (assumed to be an entity, not a literal). For
example, GNN-QE (Zhu et al., 2022) returns a fuzzy set, i.e., a scalar likelihood value for each entity, that,
after thresholding, has low mean absolute percentage error (MAPE) of the number of ground truth answers.
Answer cardinality estimation is thus obtained as a byproduct of the neural query processor without tailored
predictors. Alternatively, when models cannot predict the exact count, Spearman’s rank correlation is a
surrogate metric to evaluate the correlation between model predictions and the exact count. Spearman’s
rank correlation and MAPE of the number of ground truth answers are common metrics to evaluate the
performance of neural query processors and we elaborate on the metrics in Section 7.5. LitCQD (Demir
31Published in Transactions on Machine Learning Research (11/2024)
T ?v ?uwin uni⋀
{
  TuringAward win ?v.
}
T ?vwinSELECT ?uni WHERE 
{
  TuringAward win     ?v .
  DeepLearning field   ?person . 
  ?person university   ?uni    . 
}SPARQL Basic Graph Pattern 
{
  TuringAward win ?v.
  ?vuni ?u.
}a
bwin
ﬁeld i 
n 
T
D?v
?v⋀win
ﬁeld i 
i {
  TuringAward win  ?v.
  DeepLearning field ?v.
}TuringAward 
Existential Quantiﬁcation 
(Relation Projection) 
Intersection 
(Join) 
Intersection 
(Join) 
{ DeepLearning field ?v.} 
UNION
{ Mathematics field ?v.}
D
M?v
?v⋁ﬁeld 
ﬁeld u 
u { 
 ?s ?p ?v. 
 FILTER NOT EXISTS 
 {DeepLearning field ?v.}
}
D ?vﬁeld n Kleene Plus ( +)
(Property Path) { 
 JohnDoe knows+ ?v.
}
J ?vknows knows 
…Union 
(OR) 
Negation 
(NOT) 
T ?v ?uwin uniT
D?v
?v⋀win
ﬁeld i 
i ⋀
S
Path Queries Tree-like Queries DAG Queries Cyclic Queries S
Stanfor d student r oommate student 
S
i 
i S
S
i 
i 
S
Stanfor d student classmate 
student 
student r oommate 
classmate student r oommate 
classmate student r oommate uni 
born S
student 
r oommate classmate 
r oommate r oommate 
Figure 16: Query patterns: path, tree-like, DAG, and cyclic queries. A DAG query has two branches from
the intermediate variable, a cyclic query contains a 3-cycle. Existing neural query processors support path
and tree-like patterns.
et al., 2023) extends the answer set to numerical literals and by default implements the AVGaggregation of
predicted values when a query has multiple correct target entities. The target metrics are typical regression
metrics like mean absolute error (MAE) or mean squared error (MSE).
Optional and Solution Modifiers. SPARQL offers many features yet to be incorporated into neural
query engines to extend their expressiveness. Some of those common features include the OPTIONAL clause
thatisessentiallya LEFT JOIN operator. Forexample(Fig.15), givenatriplepattern {King wrote ?book.}
that returns books, the optional clause {King wrote ?book. OPTIONAL {King award ?a.}} enriches the
answer set with any existing awards received by King. Importantly, if there are no bindings to the optional
clause, the query still returns the values of ?book. In the query’s computation graph, the optional clause
correspondstotheoptionalbranchofarelationprojection. Aparticularchallengeforneuralqueryprocessors
operating on incomplete graphs is that the absence of the queried edge in the graph does not mean that
there are no bindings – instead, the edge might be missing and might be predicted during query processing.
Solution modifiers, e.g.,GROUP BY ,ORDER BY ,LIMIT, apply further postprocessing of projected (returned)
results and are particularly important when projecting several variables in the query. So far, all existing
neural query processors are tailored for only one return variable. We elaborate on this matter in Section 6.3.
A General Note on Incompleteness. Finally, we would like to stress out that neural query engines
performing all the described operators ( Projection ,Intersection ,Union,Negation ,Property Paths ,
Filters,Aggregations ,Optionals , andModifiers )assumetheunderlyinggraphisincompleteandqueries
might have some missing answers to be predicted, hence, all the operators should incorporate predicted hard
answersin addition to easy answers reachable by graph traversal as in symbolic graph databases. Evaluation
of query performance with those operators in light of incompleteness is still an open challenge (we elaborate
on that in Section 7.5), e.g., having an Optional clause, it might be unclear when there is no true answer
(even predicted ones are in fact false) or a model is not able to predict them.
6.2 Query Patterns
Here, we introduce several types of query patterns commonly used in practical tasks and sort them in the
increasing order of complexity. Starting with chain-like Pathqueries known in the literature for years,
we move to Tree-Structured queries (the main supported pattern in modern CLQA systems). Then, we
overview DAGandcyclicpatterns which currently are not supported by any neural query answering system
and represent a solid avenue for future work.
Path Queries. As introduced in Section A.3, previous literature starts with path queries ( akamulti-hop
queries), where the goal is simply to go beyond one-hop queries such as q=V?.r(v,V ?), wherer∈R,v∈V
andV?represents the answer variable. As shown in Fig. 22 and Fig. 16, there is no logical operator such as
branch intersection or union involved. Therefore, in order to answer such a query, we simply find or infer the
neighbors of the entity vwith relation r. Path queries are a natural extension of one-hop queries. Formally,
we denote a path query as follows. qpath=V?.∃V1,...,Vk−1:r1(v,V 1)∧r2(V1,V2)∧···∧rk(Vk−1,V?), where
32Published in Transactions on Machine Learning Research (11/2024)
T ?v ?uwin uni⋀
{
  TuringAward win ?v.
}
T ?vwinSELECT ?uni WHERE 
{
  TuringAward win     ?v .
  DeepLearning field   ?person . 
  ?person university   ?uni    . 
}SPARQL Basic Graph Pattern 
{
  TuringAward win ?v.
  ?vuni ?u.
}a
bwin
ﬁeld i 
n 
T
D?v
?v⋀win
ﬁeld i 
i {
  TuringAward win  ?v.
  DeepLearning field ?v.
}TuringAward 
Existential Quantiﬁcation 
(Relation Projection) 
Intersection 
(Join) 
Intersection 
(Join) 
{ DeepLearning field ?v.} 
UNION
{ Mathematics field ?v.}
D
M?v
?v⋁ﬁeld 
ﬁeld u 
u { 
 ?s ?p ?v. 
 FILTER NOT EXISTS 
 {DeepLearning field ?v.}
}
D ?vﬁeld n Kleene Plus ( +)
(Property Path) { 
 JohnDoe knows+ ?v.
}
J ?vknows knows 
…Union 
(OR) 
Negation 
(NOT) 
T ?v ?uwin uniT
D?v
?v⋀win
ﬁeld i 
i ⋀
A
Answers: Answers: Answers: S
r oommate student 
S
i 
i S
S
i 
i 
 SC
Stanfor d 
student classmate student 
student r oommate 
classmate student r oommate 
classmate student 
r oommate F
DEB
AE Aclassmate 
r oommate r oommate 
D
Figure 17: Answers to example tree-like, DAG, and cyclic query patterns given a toy graph. Note the
difference in the answer set to the tree-like and DAG queries – in the DAG query, a variable vmust have
two outgoing edges from the same node.
ri∈R,∀i∈[1,k],v∈V,Viare all existentially quantified variables. We denote a k-hop path query if it has
katomic formulas. The query plan of a k-hop path query is a chain of length kstarting from the anchor
entity. For example (Fig. 16), a 2-hop path query is V?.∃v:student (Stanford,v)∧roommate (v,V ?)where
Stanford is the starting anchor node, vis a tail variable of the first projection studentand at the same time
is the head variable of the second projection roommate thus forming a chain.
As shown in the definition, in order to handle path queries, it is necessary to develop a method to handle
existential quantification ∃and conjunction∧operators. Several query reasoning methods (Guu et al., 2015;
Das et al., 2017) aim to answer the path queries with sequence models using either chainable KG embeddings
(e.g., TransE) in Guu et al. (2015) or LSTM in Das et al. (2017). These methods initiated one of the first
efforts that use embeddings and neural methods to answer multi-hop path queries. We acknowledge the
efforts in this domain but emphasize their limitations in terms of query expressiveness and, therefore, focus
our attention in this work on more expressive query answering methods that operate on tree-like and more
complex patterns.
Tree-Structured Queries. Path queries only have one anchor entity and one answer variable. Such
queries have limited expressiveness and are far away from real-world query complexity seen in the logs (Maly-
shev et al., 2018) of real-world KGs like Wikidata. One direct extension to increase the expressiveness and
complexity is to support tree-structured (tree-like) queries. Tree-like queries may have multiple anchor enti-
ties, and different branches (from different anchors) will merge at the final single answer node, thus forming
a tree structured query plan. Such merge can be achieved by intersection, union, or negation operators. For
example, as shown in Fig. 1, the query plan of “At what universities do the Turing Award winners in the
field of Deep Learning work?” is not a path but a tree. Alternatively, the example in Fig. 16 depicts a query
q=V?,∃v1,v2:student (Stanford,v1)∧roommate (v1,V?)∧student (Stanford,v2)∧classmate (v2,V?)that
consists of two branches of 2-hop path queries joined by the intersection operator at the end.
Tree-like queries pose more challenges to the previous models that are only able to handle path (multi-hop)
queries since a sequence model no longer applies to tree-structured execution plans with logical operators. In
light of the challenges, neural and neuro-symbolic query processors (described in Section 5) are designed to
executemorecomplexquerypatterns. Theseprocessorsdesignneuralset/logicoperatorsanddoabottom-up
traversal of the tree up to the single root node.
Arbitrary DAGs. Based on tree-structured queries, one can further increase the complexity of the query
pattern to arbitrary directed acyclic graphs (DAGs). The key difference between the two types of queries
is that for DAG-structured queries, one variable node in the query plan (that represents a set of enti-
ties) may be split and routed to different reasoning paths, while the number of branches/reasoning paths
in the query plan always decreases from the anchor nodes to the answer node. We show one example
in Fig. 16 and in Fig. 17. Consider the tree-like query from the previous paragraph q1=V?,∃v1,v2:
student (Stanford,v1)∧roommate (v1,V?)∧student (Stanford,v2)∧classmate (v2,V?)and the DAG query
q2=V?,∃v:student (Stanford,v)∧roommate (v,V ?)∧classmate (v,V ?). The two queries search for V?who
areroommate andclassmate with Stanford students. However, the answer sets of the two queries are differ-
ent (illustrated in Fig. 17). That is, the answer to the DAG query Vq2={A}is the subset of the answers to
33Published in Transactions on Machine Learning Research (11/2024)
the tree-like query Vq1={A,E}because the answers to q2have to be both roommate andclassmate with the
sameStanford student in the intermediate variable v. On the other hand, the two branches of the tree-like
queryq1are independent such that intermediate variables v1andv2need not be the same entities, hence, the
query has more valid intermediate answers and more correct answers. To the best of our knowledge, there
still does not exist a neural query processor that can faithfully handle any DAG query. Although BiQE (Kot-
nis et al., 2021) claims to support DAG queries, the mined dataset consists of tree-like queries. FIT (Yin
et al., 2023b) processes DAG queries by rewriting and decomposing the query into fragments executable
by CQD-like inference mechanism. Nevertheless, we hypothesize that, potentially, processors with message
passing or Transformer architectures that consider the entire query graph structure Gqmay be capable of
handling DAG queries and leave this question for future work.
Cyclic Queries. Cyclic queries are more complex than DAG-structured queries. A cycle in a query
naturally entails no particular order to traverse the query plan. An example of the cyclic query is
illustrated in Fig. 16 and Fig. 17: q=V?,∃v1,v2,v3:student (Stanford,v1)∧roommate (v1,v2)∧
roommate (v2,v3)∧roommate (v3,v1)∧classmate (v1,V?). Inq, three variables form a triangle cycle
roommate (v1,v2)∧roommate (v2,v3)∧roommate (v3,V?). Given a graph in Fig. 17, the cycle starts and
ends at node C, hence the only correct answer is obtained after performing the classmate relation projection
from Cending in D,Vq={D}.
Reasoning methods and query processors that assume a particular traversal or node ordering on the query
plan, therefore, cannot faithfully answer cyclic queries. Yin et al. (2023b) attempt to answer cyclic queries
by cutting query edges according to pre-defined decomposition rules. UnRavL (Cucumides et al., 2024)
approximates cyclic queries by a set of tree-like queries with some theoretical guarantees.) It remains an
open question how to effectively model a query with cyclic structures. Moreover, cyclic structures often
appear when processing queries with regular expressions and property paths (Section 6.1). We posit that
supporting cycles might be a necessary condition to fully enable property paths in neural query engines.
6.3 Projected Variables
Byprojected variables we understand target query variables that have to be bound to particular graph
elements such as entity, relation, or literals. For example, a query in Fig. 1 q=V?.∃v:win(TuringAward ,v)∧
field(DeepLearning ,v)∧university (v,V ?)has one projected variable V?that can be bound to three answer
nodes in the graph, V?={UofT,UdeM,NYU}. In the SPARQL literature (Hawke et al., 2013), the SELECT
query specifies which existentially quantified variables to project as final answers. The pairs of projected
variables and answers form bindings as the result of the SELECTquery. Generally, queries might have zero,
one, or multiple projected variables, and we align our categorization with this notion. Examples of such
queries and their possible answers are provided in Fig. 18. Currently, most neural query processors focus on
the setting where queries have only one answer variable – the leaf node of the computation graph, as shown
in Fig. 18 (center).
Zero Projected Variables. Queries with zero projected variables (Boolean queries) do not return
any bindings but rather probe the graph on the presence of a certain subgraph or relational pattern
where the answer is Boolean TrueorFalse. In SPARQL, the equivalent of zero-variable queries is the
ASKclause. Zero-variable queries might have all entities and relations instantiated with constants, e.g.,
q=student (S,D)∧roommate (D,E)as in Fig. 18 (left) is equivalent to the SPARQL query ASK WHERE {S
student D. D roommate E.} . The query probes whether a graph contains a particular subgraph (path)
induced by the constants. Such a path exists, so the answer is q={True}.
Alternatively, zero-variable queries might have existentially quantified variables that are never projected (up
to the cases where all subjects, predicates, or objects are variables). For example, a query q1=∃v1,v2,v3:
student (v1,v2)∧roommate (v2,v3)probes whether there exist any nodes forming a relational path v1student−−−−−→
v2roommate−−−−−−→v3. In a general case, a query q2=∃p,s,o :p(s,o)asks if a graph contains at least one edge.
We note that in the main considered setting with incomplete graphs and missing edges zero-variable queries
are still non-trivial to answer. Particularly, a subfield of neural subgraph matching (Rex et al., 2020; Huang
34Published in Transactions on Machine Learning Research (11/2024)
⋀
SELECT ?uni WHERE 
{
  TuringAward win     ?v .
  DeepLearning field   ?person . 
  ?person university   ?uni    . 
}SPARQL Basic Graph Pattern 
a
bwin
ﬁeld i 
n 
T ?v ?uwin uniA
Answers: Answers: Answers: 
S
r oommate S
i 
i 
SC
Stanfor d 
student classmate student r oommate 
classmate student 
r oommate 
F
DEB
AEAclassmate 
r oommate r oommate 
D
SELECT ?v1 ?v2 ?v WHERE {
  Stanford student   ?v1 .
  ?v1   roommate   ?v  .
  Stanford  student   ?v2 .
  ?v2   classmate  ?v  .
}SELECT DISTINCT  ?v WHERE {
  Stanford student   ?v1 . 
  ?v1    roommate   ?v  .
  Stanford  student   ?v2 . 
  ?v2 classmate  ?v  .
}
?v1 ?v2 ?v 
FFA
CFA
DFE?v 
A
EProjected 
variables: 3 Projected 
variables: 1 ASK WHERE {
  Sstudent   D .
  D roommate   E .
}
Projected 
variables: 0 True 
Figure 18: Projected variables of the tree-like query from Fig. 17. Current neural query processors support
the single-variable DISTINCT mode (center) whereas queries might have zero return variables akin to a
subgraph matching Boolean ASKquery (left) or multiple projected variables (right) that imply returning
intermediate answers and form output tuples.
et al., 2022a) implies having incomplete graphs. We hypothesize such approaches might be found useful for
neural query processors to support answering zero-variable queries.
One Projected Variable. Queries with one projected variable return bindings for one (of possibly many)
existentially quantified variable. In SPARQL, the projected variable is specified in the SELECTclause,e.g.,
SELECT DISTINCT ?v in Fig. 18 (center). Although SPARQL allows projecting variables from any part of a
query, most neural query engines covered in Section 5 follow the task formulation of GQE (Hamilton et al.,
2018) and allow the projected target variable to be only the leaf node of the query computation graph.
This limitation is illustrated in Fig. 18 (center) where the target variable ?vis the leaf node of the query
graph and has two bindings v={A,E}.
It is worth noting that existing neural query processors are designed to return a unique set of answers to the
input query, i.e., it corresponds to the SELECT DISTINCT clause in SPARQL. In contrast, the default SELECT
returnsmultisets with possible duplicates. For example, the same query in Fig. 18 (center) without DISTINCT
would have bindings v={A,A,E}as there exist two matching graph patterns ending in A. Implementing
non-DISTINCT query answering remains an open challenge.
Mostneuralqueryprocessorshaveanotionofintermediatevariablesandmodeltheirdistributionintheentity
space. For instance, having a defined distance function, geometric processors (Ren et al., 2020; Choudhary
et al., 2021b) can find nearest entities as intermediate variables. Similarly, fuzzy-logic processors operating
on fuzzy sets (Tang et al., 2022; Zhu et al., 2022) already maintain a scalar distribution over all entities after
each execution step. Finally, GNN-based (Daza & Cochez, 2020; Alivanistos et al., 2022) and Transformer-
based (Liu et al., 2022) processors explicitly include intermediate variables as nodes in the query graph (or
tokens in the query sequence) and can therefore decode their representations to the entity space. The main
drawback of all those methods is the lack of filtering mechanisms for the sets of intermediate variables after
the leaf node has been identified. That is, in order to filter and project only those intermediate variables
that lead to the final answer, some notion of backward pass is required. The first step in this direction is
taken by QTO (Bai et al., 2023c) that runs the pruning backward pass after reaching the answer leaf node.
Multiple Projected Variables. The most general and complex case for queries is to have multiple
projected variables as illustrated in Fig. 18 (right). In SPARQL, all projected variables are specified in
theSELECTclause (with the possibility to project all variables in the query via SELECT * ). In the logical
form, a query has several target variables q=?v1,?v2,?v:student (Stanford,?v1)∧roommate (?v1,?v)∧
student (Stanford,?v2)∧classmate (?v2,?v)such that the output bindings are organized in tuples. For ex-
ample, one possible answer tuple is {?v1:F,?v2:F,?v:A}denotes particular nodes (variable bindings) that
satisfy the query pattern.
As shown in the previous paragraph about one-variable queries, some neural query processors have the means
to keep track of the intermediate variables. However, none of them have the means to construct answer tuples
with variables bindings and it remains an open challenge how to incorporate multiple projected variables
35Published in Transactions on Machine Learning Research (11/2024)
3p2p1p2i3in2inn3inninpnpinnpniippi2uuuupuu
Figure 19: Standard query patterns with names, where pis projection, iis intersection, uis union, nis
negation. In a pattern, blue node represents a non-variable entity, grey node represents a variable node, and
the green node represents the answer node. In a typical training protocol, models are trained on 10 patterns
(first and third rows) and evaluated on all patterns. In the hardest generalization case, models are only
trained on 1pqueries. Some datasets further modify the patterns with additional features like qualifiers or
temporal timestamps.
into such processors. Furthermore, some common caveats to be taken into account include (1) dealing with
unbound variables that often emerge, for example, in OPTIONAL queries covered in Section 6.1, where answer
tuples might contain an empty value ( ∅orNULL) for some variables; (2) the growing complexity issue where
the answer set might potentially be polynomially large depending on the number of projected variables.
7 Datasets and Metrics
7.1 Evaluation Setup
Multiple datasets have been proposed for evaluation of query reasoning models. Here we introduce the
common setup for CLQA task. Given a knowledge graph G= (E,R,S), the standard practice is to split
Ginto a training graph Gtrain, a validation graph Gvaland a test graph Gtest(simulating the unobserved
complete graph ˆGfrom Section 2). The standard experiment protocol is to train a query reasoning model
only on the training graph Gtrain, and evaluate the model on answering queries over the validation graph
Gvaland the test graph Gtest. Given a query q, denote the answers of this query on training, validation
and test graph as JqKtrain,JqKvaland JqKtest. During evaluation, queries may have missing answers, e.g.,
a validation query qmay have answers JqKvalthat are not in JqKtrain, a test query qmay have answers
JqKtestthat are not in JqKval. The overall goal of CLQA task is to find these missing answers. The details
of typical training queries, training protocol, inference and evaluation metrics are introduced in Section 7.2,
Section 7.3, Section 7.4, and Section 7.5, respectively.
7.2 Query Types
The standard set of graph queries used in many datasets includes 14 types:
1p/2p/3p/2i/3i/ip/pi/2u/up/2in/3in/inp/pni/pin wherepdenotes relation projection, iis intersec-
tion,uis union, nis negation, and a number denotes the number of hops for projection queries or number of
branches to be merged by a logical operator. Fig. 19 illustrates common query patterns. For example, 3pis
a chain-like query of three consecutive relation projections, 2iis an intersection of two relation projections,
3inis an intersection of three relation projections where one of the branches contains negation, upis a union
of two relation projections followed by another projection. The original GQE by Hamilton et al. (2018)
36Published in Transactions on Machine Learning Research (11/2024)
introduced 7 query patterns with projection and intersection 1p/2p/3p/2i/3i/ip/pi , Query2Box (Ren et al.,
2020) added union queries 2u/up, and BetaE (Ren & Leskovec, 2020) added five types with negation.
Subsequent works modified the standard set of query types in several ways, e.g., hyper-relational queries (Ali-
vanistos et al., 2022; Luo et al., 2023) with entity-relation qualifiers on relation projections, or temporal op-
erators on edges (Lin et al., 2023). New query patterns include queries with regular expressions of relations
(property paths) (Adlakha et al., 2021), more tree-like queries (Kotnis et al., 2021), and more combinations
of projections, intersections, and unions (Wang et al., 2021; Pflueger et al., 2022). We summarize existing
query answering datasets and their properties in Table 14 covering supported query operators, inference
setups, and additional features like temporal timestamps, class hierarchies, or complex ontological axioms.
Commonly, query datasets are sampled from different KGs to study model performance under different graph
distributions, for example, BetaE datasets include sets of queries from denser Freebase (Bollacker et al.,
2008) with average node degree of 18 and sparser WordNet (Miller, 1998) and NELL (Mitchell et al., 2015)
with average node degree of 2. Hyper-relational datasets WD50K (Alivanistos et al., 2022) and WD50K-
NFOL (Luo et al., 2023) were sampled from Wikidata (Vrandecic & Krötzsch, 2014) where qualifiers are
natural. TAR datasets with class hierarchy (Tang et al., 2022) were sampled from YAGO 4 (Pellissier Tanon
et al., 2020) and DBpedia (Lehmann et al., 2015) where class hierarchies are well-curated. Q2B Onto
datasets with ontological axioms (Andresel et al., 2021) were sampled from LUBM (Guo et al., 2005) and
NELL. Temporal TFLEX datasets (Lin et al., 2023) were sampled from ICEWS (Boschee et al., 2015) and
GDELT (Leetaru & Schrodt, 2013) that maintain event information. InductiveQE datasets (Galkin et al.,
2022b) were sampled from Freebase and Wikidata, while inductive GNNQ datasets (Pflueger et al., 2022)
were sampled from the WatDiv benchmark (Aluç et al., 2014) and Freebase.
7.3 Training
Query reasoning methods are trained on the given Gtrainwith different objectives/losses and dif-
ferent datasets. Following the standard protocol, methods are trained on 10 query patterns
1p/2p/3p/2i/3i/2in/3in/inp/pni/pin and evaluated on all 14 patterns including generalization to unseen
ip/pi/2u/up patterns. That is, the training protocol assumes that models trained on atomic logical opera-
tors would learn to compositionally generalize to patterns using several operators such as ipandpiqueries
that use both intersection and projection.
We summarize different training objectives in Table 13. Most methods that learn a representation of the
queries and entities on the graph optimize a contrastive loss, i.e., minimizing the distance between the
representation of a query qand its positive answers ewhile maximizing that between the representation of
a query and negative answers e′. Various objectives include: (1) max-margin loss (first column in Table 13)
with the goal that the distance of negative answers should be larger than that of positive answers at least
by the margin γ. Such loss is often of the form as the equation below.
ℓ= max(0,γ−dist (q,e) +dist (q,e′));
(2) LogSigmoid loss (second column in Table 13) with a similar goal that pushes the distance of negatives
up and vice versa. Often the loss also includes a margin term and the gradient will gradually decrease when
the margin is satisfied.
ℓ=−logσ(γ−dist (q,e))−/summationdisplay1
klogσ(dist (q,e′)−γ),
wherekis the number of negative answers. Other methods (third column in Table 13) that directly model
a logit vector over all the nodes on the graph may optimize a cross entropy loss instead of a contrastive loss.
Besides, methods such as the two variants of CQD (Arakelyan et al., 2021; 2023), QTO (Bai et al., 2023c),
FIT (Yin et al., 2023b), and LitCQD (Demir et al., 2023) only optimize the link prediction loss since they
do not learn a representation of the query.
Almost all the datasets including GQE (Hamilton et al., 2018), Q2B (Ren et al., 2020), BetaE (Ren &
Leskovec, 2020), RegEx (Adlakha et al., 2021), BiQE (Kotnis et al., 2021), Query2Onto (Andresel et al.,
2021), TAR (Tang et al., 2022), StarQE (Alivanistos et al., 2022), GNNQ (Pflueger et al., 2022), TeMP (Hu
37Published in Transactions on Machine Learning Research (11/2024)
Table 13: Complex Query Answering approaches categorized under Loss.
Max Margin LogSigmoid (Sun et al., 2019) Cross Entropy
GQE (Hamilton et al.,
2018), GQE w hash (Wang
et al., 2019) , CGA (Mai
et al., 2019) , MPQE (Daza
& Cochez, 2020) , HyPE
(Choudhary et al., 2021b) ,
Shv(Gebhart et al., 2023)Query2Box (Renetal.,2020) , BetaE (Ren&Leskovec,
2020), RotatE-Box (Adlakha et al., 2021) , ConE
(Zhang et al., 2021b) , NewLook (Liu et al., 2021) , Q2B
Onto(Andresel et al., 2021) , PERM (Choudhary et al.,
2021a), LogicE (Luus et al., 2021) , MLPMix (Amayue-
las et al., 2022) , FuzzQE (Chen et al., 2022) , FLEX
(Lin et al., 2022) , TFLEX (Lin et al., 2023) , SMORE
(Ren et al., 2022) , LinE (Huang et al., 2022b) , Gam-
maE(Yang et al., 2022a) , NMP-QEM (Long et al.,
2022), ENeSy (Xuetal.,2022) , RoMA (Xietal.,2022) ,
SignalE (Wang et al., 2022) , Query2Geom (Sardina
et al., 2023) , RoConE (He et al., 2023) , CylE(Nguyen
et al., 2023b) , WFRE (Wang et al., 2023d)BiQE (Kotnis et al., 2021) ,
NodePiece-QE (Galkin et al.,
2022b), GNN-QE (Zhu et al.,
2022), GNNQ (Pflueger et al.,
2022), KGTrans (Liu et al., 2022) ,
Query2Particles (Bai et al., 2022) ,
EmQL (Sun et al., 2020) , LMPNN
(Wang et al., 2023e) , StarQE (Ali-
vanistos et al., 2022) , NQE (Luo
et al., 2023) , CQDA(Arakelyan
et al., 2023) , SQE(Bai et al., 2023b)
et al., 2022), TFLEX (Lin et al., 2023), InductiveQE (Galkin et al., 2022b), SQE (Bai et al., 2023b) provide
a set of training queries of given structures sampled from the Gtrain. The benefit is that during training,
methods do not need to sample queries online. However, it often means that only a portion of information
is utilized from the Gtrainsince exponentially more multi-hop queries exist on Gtrainand the dataset can
never pre-generate all offline. SMORE (Ren et al., 2022) proposes a bidirectional online query sampler
such that methods can directly do online sampling efficiently without the need to pre-generate a training
set offline. Alternatively, methods that do not have parameterized ways to handle logical operations, e.g.,
CQD (Arakelyan et al., 2021), only require one-hop edges to train the overall system.
7.4 Inference
Hinton Cambridge Edinburgh 
education education Hinton Cambridge Edinburgh 
education education 
start: 1972 
end:   1975 
Transductive Inductive 
(superset) Inductive 
(disjoint) Hinton Cambridge Edinburgh 
education education 
established: 1583 
students:       35,375 established: 1209 
students:       24,450 start: 1967 
end:   1970      ?x: education(Hinton, x) ⋀ 
         x.students < 30,000 Hinton Cambridge Edinburgh education education 
Hinton Cambridge Edinburgh education education 
Bachelor PhDdegree degree 
Training and 
Inference Graph 
Training Inference 
Training 
Inference 
known link 
missing link 
Figure 20: Inference Scenarios. In the Transductive case, training and inference graphs are the same and
share the same nodes ( Einf=Etrain).Inductive cases can be split into superset (orsemi-inductive ) where
the inference graph extends the training one ( Etrain⊆E inf, missing links cover both seen and unseen nodes)
anddisjoint(orfully-inductive ) where the inference graph is disconnected ( Etrain∩E inf=∅, missing links
are among unseen nodes).
ByInference we understand testing scenarios on which a trained query answering model will be deployed
and evaluated. Following the literature, we distinguish Transductive andInductive inference (Fig. 20). In
the transductive case, inference is performed on the graph with the same set of nodes and relation types as
in training but with different edges. Any other scenario when either the number of nodes or relation types
of an inference graph is different from that of the training is deemed inductive. The inference scenario plays
a major role in designing query answering models, that is, transductive models can learn a shallow entity
38Published in Transactions on Machine Learning Research (11/2024)
Table 14: Existing logical query answering datasets classified along supported query operators ,inference
scenarios, and domain properties. +is partial support – some operators on literals can be treated as filters.
Query Operators Inference Domain
Source Dataset
Conjunctive
Union
Negation
Kleene Plus
Filter + Agg
Transductive
Inductive
Discrete
+ Timestamps
+ Continuous
Types
Rules
Qualifiers
Hamilton et al. (2018) GQE datasets ✓ ✓ ✓
Ren et al. (2020) Q2B datasets ✓ ✓ ✓ ✓
Ren & Leskovec (2020) BetaE datasets ✓ ✓ ✓ ✓ ✓
Adlakha et al. (2021) Regex queries ✓ ✓ ✓ ✓ ✓
Kotnis et al. (2021) DAG queries ✓ ✓ ✓
Wang et al. (2021) EFO-1 queries ✓ ✓ ✓ ✓ ✓
Andresel et al. (2021) LUBM/NELL (type) ✓ ✓ ✓ ✓ ✓
Tang et al. (2022) TAR datasets ✓ ✓ ✓ ✓ ✓
Ren et al. (2022) SMORE datasets ✓ ✓ ✓ ✓
Alivanistos et al. (2022) WD50K dataset ✓ ✓ ✓ ✓
Hu et al. (2022) TeMP dataset ✓ ✓ ✓ ✓ ✓
Pflueger et al. (2022) GNNQ dataset ✓ ✓ ✓
Lin et al. (2023) TFLEX dataset ✓ ✓ ✓ +✓ ✓ ✓
Galkin et al. (2022b) InductiveQE dataset ✓ ✓ ✓ ✓ ✓
Luo et al. (2023) WD50K-NFOL dataset ✓ ✓ ✓ ✓ ✓ ✓
Bai et al. (2023b) SQE dataset ✓ ✓ ✓ ✓ ✓
Huang et al. (2022b) WN18RR dataset ✓ ✓ ✓ ✓ ✓
Demir et al. (2023) FB15k237 w/ literals ✓ ✓ +✓ ✓ ✓
Yin et al. (2023b) FIT dataset (w/ cycles) ✓ ✓ ✓ ✓ ✓
Bai et al. (2023a) NRN datasets ✓ ✓ ✓ ✓ ✓
Cucumides et al. (2024) Test set w/ cycles ✓ ✓ ✓
Yin et al. (2023a) EFO kdataset ✓ ✓ ✓ ✓ ✓
Galkin et al. (2024) WikiTopics-CLQA dataset ✓ ✓ ✓ ✓ ✓
embedding matrix thanks to the fixed entity set whereas inductive models have to rely on other invariances
available in the underlying graph in order to generalize to unseen entity/relation types. We discuss many
transductive and inductive models in Section 5.2. Below, we categorize existing datasets from the Inference
perspective. The overview of existing CLQA datasets is presented in Table 14 through the lens of supported
query operators, inference scenario, graph domain, and other features like types or qualifiers.
TransductiveInference. Formally, givenatraininggraph Gtrain= (Etrain,Rtrain,Strain), thetransductive
inference graphGinf6contains the same set of entities and relation types, that is, Etrain=EinfandRtrain=
Rinf, while the edge set on Gtrainis a subset of that on the inference graph Ginf,i.e.,Strain⊂S inf. In this
setup, query answering is performed on the same nodes and edges seen during training. From the entity set
perspective, the prediction pattern is seen-to-seen – missing links are predicted between known entities.
Traditionally, KG link prediction focused more on the transductive task. In CLQA, therefore, the majority
of existing datasets (Table 14) follow the transductive scenario. Starting from simple triple-based graphs
with fixed query patterns in GQE datasets (Hamilton et al., 2018), Query2Box datasets (Ren et al., 2020),
and BetaE datasets (Ren & Leskovec, 2020) that became de-facto standard benchmarks for query answering
approaches, newer datasets include regex queries (Adlakha et al., 2021), wider set of query patterns (Kotnis
et al., 2021; Wang et al., 2021; Bai et al., 2023b), entity type information (Tang et al., 2022), ontological
axioms (Andresel et al., 2021), hyper-relational queries with qualifiers (Alivanistos et al., 2022; Luo et al.,
2023), temporal queries (Lin et al., 2023), very large graphs up to 100M nodes (Ren et al., 2022), hierarchical
6Below we useGinfto refer to the graphs we use during inference, it can be GvalorGtestwithout loss of generalization.
39Published in Transactions on Machine Learning Research (11/2024)
graphs (Huang et al., 2022b), queries with numerical literals (Demir et al., 2023; Bai et al., 2023a), or queries
with cycles and multiedges (Yin et al., 2023b;a; Cucumides et al., 2024).
Inductive Inference. Formally, given a training graph Gtrain= (Etrain,Rtrain,Strain), the inductive infer-
ence graphGinf= (Einf,Rinf,Sinf)is different from the training graph in either the entity set or the relation
set or both. The nature of this difference explains several subtypes of inductive inference. First, the set of
relations might or might not be shared at inference time, that is, Rinf⊆R trainor|Rinf\R train|>0. Most
of the literature on inductive link prediction (Teru et al., 2020; Zhu et al., 2021; Galkin et al., 2022a) in KGs
assumes the set of relations is shared whereas the setup where new relations appear at inference time is still
highly non-trivial (Huang et al., 2022a; Gao et al., 2023; Chen et al., 2023).
On the other hand, the inference graph might be either a superset of the training graph after adding new
nodes and edges, Etrain⊆E inf, or a disjoint graph with completely new entities as a disconnected component,
Einf∩E train =∅as illustrated in Fig. 20. From the node set perspective, the superset inductive inference
case might contain both unseen-to-seen andunseen-to-unseen missing links whereas in the disjoint inference
graph only unseen-to-unseen links are naturally appearing.
In CLQA, inductive reasoning is still an emerging area as it has a direct impact on the space of possible
variablesV, constantsC, and answers Athat might now include entities unseen at training time. Several
most recent works started to explore inductive query answering (Table 14). InductiveQE datasets (Galkin
et al., 2022b) focus on the inductive superset case where a training graph can be extended with up to 500%
new unseen nodes. Test queries start from unseen constants and answering therefore requires reasoning over
both seen and unseen nodes. Similarly, training queries can have many new correct answers when answered
against the extended inference graph. GNNQ datasets (Pflueger et al., 2022) focus on the disjoint inductive
inference case where constants, variables, and answers all belong to a new entity set. TeMP datasets (Hu
et al., 2022) focus on the disjoint inductive inference as well but offer to leverage an additional class hierarchy
as a learnable invariant . That is, the set of classes at training and inference time does not change. The
only suite of datasets for fully-inductive inference on both unseen entities and relations was introduced in
UltraQuery (Galkin et al., 2024).
Inductive inference is crucial to enable running models over updatable graphs without retraining. We con-
jecture that inductive datasets and models are likely to be the major contribution area in the future work.
7.5 Metrics
Several metrics have been proposed to evaluate the performance of query reasoning models that can be
broadly classified into generalization ,entailment , andquery representation quality metrics.
Generalization Metrics. Since the aim of query reasoning models is to perform reasoning over massive
incomplete graphs, most metrics are designed to evaluate models’ generalization capabilities in discovering
missinganswers, i.e.,JqKtest\JqKvalforagiventestquery q. Asoneofthefirstworksinthefield, GQE(Hamil-
ton et al., 2018) proposes ROC-AUC and average percentile rank (APR). The idea is that for a given test
queryq, GQE calculates a score for all its missing answers e∈JqKtest\JqKvaland the negatives e′/∈JqKtest.
The model’s performance is the ROC-AUC score and APR, where they rank a missing answer against at
most 1000 randomly sampled negatives of the same entity type. Besides GQE, GQE+hashing (Wang et al.,
2019), CGA (Mai et al., 2019) and TractOR (Friedman & Van den Broeck, 2020) use the same evaluation
metrics.
However, the above metrics do not reflect the real world setting where we often have orders of magnitude
more negatives than the missing answers. Instead of ROC-AUC or APR, Query2Box (Ren et al., 2020)
proposes ranking-based metrics, such as mean reciprocal rank (MRR) and hits@ k. Given a test query q, for
each missing answer e∈JqKtest\JqKval, we rank it against all the other negatives e′/∈JqKtest. Given the
rankingr, MRR is calculated as1
rand hits@kis1[r≤k]. This has been the most used metrics for the task.
Note that the final rankings are computed only for the hardanswers that require predicting at least one
missing link. Rankings for easyanswers reachable by edge traversal are usually discarded.
40Published in Transactions on Machine Learning Research (11/2024)
Representation Quality Metrics. Besides evaluating model’s capability of finding missing answers,
another aspect is to evaluate the quality of the learned query representation for all models. BetaE (Ren &
Leskovec, 2020) proposes to evaluate whether the learned query representation can model the cardinality of
a query’s answer set, and view this as a proxy of the quality of the query representation. For models with
a sense of “volume” ( e.g., differential entropy for Beta embeddings), the goal is to measure the Spearman’s
rank correlation coefficient and Pearson’s correlation coefficient between the “volume” of a query (calculated
from the query representation) and the cardinality of the answer set. BetaE also proposed to evaluate an
ability to model queries without answers using ROC-AUC.
Entailment Metrics. The other evaluation protocol is about whether a model is also able to discover
the existing answers, e.g.,JqKvalfor test queries, that does not require inferring missing links but focuses on
memorizing the graph structure ( easyanswers in the common terminology). This is referred to as faithfulness
(orentailment ) in EmQL (Sun et al., 2020). Natural for database querying tasks, it is expected that query
answering models first recover easyanswers already existing in the graph (reachable by edge traversal) and
then enrich the answer set with predicted hardanswers inferred with link prediction. A natural metric is
therefore an ability to rank easy answers higher than hard answers – this was studied by InductiveQE (Galkin
et al., 2022b) that proposed to use ROC-AUC as the main metric for this task.
Still, we would argue that existing metrics might not fully capture the nature of neural query answering and
new metrics might be needed. For example, some under-explored but potentially useful metrics include (1)
studying reasonable answers (in between easy and hard answers) that can be deduced by symbolic reasoners
using a higher-level graph schema (ontology) (Andresel et al., 2021). A caveat in computing reasonable
answers is a potentially infinite processing time of symbolic reasoners that have to be limited by time or
expressiveness in order to complete in a finite time. Hence, the set of reasonable answers might still be
incomplete; (2) evaluation in light of the open-world assumption (OWA) stating that unknown triples in
the graph might not necessarily be false (as postulated by the standard closed-world assumption used a lot
in link prediction). Practically, OWA means that even the test set might be incomplete and some high-
rank predictions deemed incorrect by the test set might in fact be correct in the (possibly unobservable)
completegraph. InitialexperimentsofYangetal.(2022b)withOWAevaluationoflinkpredictionexplainthe
saturation of ranking metrics ( e.g., MRR) on common datasets by the performance of neural link predictors
able to predict the answers from the complete graph missed in the test set. For example, saturated MRR of
0.4 on the test set might correspond to MRR of 0.9 on the true complete graph. Studying OWA evaluation
in the query answering task in both transductive and inductive setups is a solid avenue for future work.
8 Applications
The framework of complex query answering is applied in a variety of graph-conditioned machine learning
tasks. For example, SE-KGE (Mai et al., 2020) applies GQE to answer geospatial queries conditioned on nu-
merical{x,y}coordinates. The coordinates encoder fuses numerical representations with entity embeddings
such that the prediction task is still entity ranking.
In case-based reasoning, CBR-SUBG (Das et al., 2022) is a method for question answering over KGs based
on subgraph extraction and encoding. As a byproduct, CBR-SUBG is capable of answering conjunctive
queries with projections and intersections. However, due to a non-standard evaluation protocol and custom
synthetic dataset, its performance cannot be directly compared to CLQA models. Similarly, Wang et al.
(2023b) merge a Query2Box-like model with a pre-trained language model to improve question answering
performance. LEGO (Ren et al., 2021) also applies CLQA models for KG question answering. The idea is
to simultaneously parse a natural language question as a query step and execute the step in the latent space
with CLQA models.
LogiRec (Wu et al., 2022) frames product recommendation as a complex logical query such that source
products are root nodes, combinations of multiple products form intersections, and non-similar products to
be filtered out form negations. LogiRec employs BetaE as a query engine. Similarly, Syed et al. (2022) design
an explainable recommender system based on Query2Box. Given a logical query, they first use Query2Box
to generate a set of candidates and rerank them using neural collaborate filtering (He et al., 2017).
41Published in Transactions on Machine Learning Research (11/2024)
9 Summary and Future Opportunities
We proposed a deep and detailed review of Complex logical query answering (CLQA) methods based on the
new taxonomy in Section 3 categorizing the existing approaches along three main areas: Graphs,Modeling ,
andQuerieswith their respective sub-areas. Going forward, there is still much room to unlock the full power
of CLQA by addressing numerous open challenges. Adhering to the taxonomy, we summarize the challenges
in three main areas: Graphs, Modeling, and Queries.
Along the Graphbranch:
•Modality: Futuresystemsneedtohandlearichervarietyofgraphstructuresanddatatypes. Forex-
ample, beyond conventional knowledge graphs that contain only triples of subject–predicate–object,
we want to support hyper-relational graphs where edges can connect more than two nodes or have
additional qualifiers. We also aim to include hypergraphs, which represent complex relationships
with edges that join multiple entities simultaneously. In addition, we envision multimodal data
sources where graph data is combined with textual descriptions, images, or other media. For in-
stance, a system might incorporate a knowledge graph of people and places along with associated
images and text captions, integrating all these modalities seamlessly.
•Reasoning Domain: Another goal is to facilitate logical reasoning and neural query answering
over dynamic and continuous information within graphs. Many real-world knowledge graphs include
temporal facts (such as events happening at specific times) and literal values (such as numerical
measurements or textual strings). Because a large portion of a graph’s information is stored as these
literal values—like a numerical temperature reading or a textual label—a robust system should be
able to answer queries that involve these literals. For example, the system might reason over a time-
series of events to answer a query about when a particular relationship held, or interpret textual
data attached to nodes to respond to a query that references an entity’s description.
•Background Semantics: We want to incorporate complex background knowledge encoded by
formal semantics and axioms. This means supporting not just direct entity-to-entity relationships
but also higher-order relationships between classes of entities and their hierarchical structures. For
example, the system should enable neural reasoning that respects the rules of description logics or
subsets of the Web Ontology Language (OWL). In practice, this might allow the model to deduce
that if an entity belongs to a particular subclass (like “Cat” within “Mammal”), it should inherit
relationships and constraints from its superclass. A concrete example might be reasoning that if
a graph knows “Fluffy is a Cat” and that “All Cats are Mammals,” then the system can conclude
“Fluffy is a Mammal.” Supporting these complex axioms will allow more powerful and semantically
rich reasoning over graph data.
In theModeling branch:
•Encoder:
We seek inductive encoders that can interpret new relationships in a knowledge graph even if they
were unseen during training. For example, if a graph’s schema is updated with a new relation type
like “collaboratesWith,” the encoder should handle queries involving this relation without requiring
retraining. This capability supports two key goals: (1) Updatability: The neural database can
quickly adapt to changes in the graph’s schema or content. For instance, if new nodes and edges
appear in a KG about scientific publications, the system can incorporate them on the fly. (2)
Pretrain-Finetune Strategy: A model could be pre-trained on general graph structures and then
fine-tuned to answer queries on a new, custom graph with a unique set of entity and relation types.
Ultimately, we aspire to create a single CLQA model that can respond to queries about any unseen
KG, regardless of the vocabulary of entities and relations it uses.
•Processor: We want processor networks that can execute a wide range of complex query operators
similar to those in SPARQL or Cypher languages efficiently and effectively. For example, a system
42Published in Transactions on Machine Learning Research (11/2024)
should handle queries that require union, intersection, property paths, or filters over nodes and
edges. Improving the sample efficiency of these neural processors is crucial—meaning the model
should require less training data or time while preserving accuracy. For example, if training a model
to interpret SPARQL “FILTER” operator, we want the model to learn from relatively few examples
and still apply the filter operator correctly in new queries without extensive retraining.
•Decoder: Currently, neural query decoders typically return results as discrete graph nodes. We
want to extend this to continuous outputs for queries that involve, for example, numerical attributes
or categories not purely represented as nodes. For instance, if the query is “What is the average
temperature of city X in July?” the decoder should produce a numeric result (like 29.5 °C) rather
than just pointing to a node. This ability is essential for real-world datasets where many queries
involve combining discrete graph structures with numeric or categorical data.
•Complexity: Processor networks can face significant computational challenges, whether from the
high dimensionality of their embeddings (in purely neural models) or the large number of graph
nodes (in neuro-symbolic approaches). To handle massive KGs with billions of nodes and trillions of
edges, we need more efficient algorithms for implementing neural logic operators (like those for union
or negation) and for retrieving relevant information from the graph. For example, if a query asks
about relationships among millions of nodes, the system should use optimized retrieval methods,
ensuring that answering complex queries remains feasible at scale.
In theQueries branch:
•Operators: We aim to develop neural methods that can handle a wider variety of complex query
operators found in declarative graph query languages. For example, a system should support the
Kleene plus and star operators, which let queries match repeated patterns in a path (like “all
ancestors” or “paths of any length connecting two nodes”). It should also manage property paths,
where queries involve sequences of edges that fulfill certain property constraints (such as “entities
connected by a path of type ‘friendOf’ repeated one or more times”). In addition, the system should
handle filters that restrict results based on conditions, for example retrieving people from a graph
who are over 30 years old and live in a specific city.
•Patterns: We want query answering systems that can handle patterns beyond simple tree struc-
tures. For instance, a query might involve a directed acyclic graph (DAG) pattern such as “Find
topics that are prerequisites for both X and Y courses” or a cyclic graph pattern like “Identify people
who follow each other on a social network.” This complexity means the system must interpret and
respond to queries where relationships loop back or branch in multiple directions, replicating the
complexity of many real-world graph queries.
•ProjectedVariables: Insteadofonlyreturningasinglefinalanswerentity(likethenameofacity),
queriesmayrequirereturningintermediatevariablesorentiretuplesofvariables. Forexample, auser
might ask “Which actors and their respective directors collaborated on a film in 2020?” The answer
requires projecting multiple variables—both the actor and the director—for each case. Similarly, a
query might require returning a relationship as a variable, such as identifying not just the entities
but also the nature of their relationship. The formalism presented in this survey supports this, and
some papers present preliminary results, but none are evaluated on the quality of bindings to these
intermediate variables.
•Expressiveness: We want systems to answer queries that go beyond simple fragments like EPFO
(existential positive first-order) and EFO (existential first-order). This means supporting the union
of conjunctive queries with negations (UCQ and UCQneg) and moving toward the expressiveness
found in full database query languages. For instance, the system should handle a query like “Find all
persons who are not authors of any paper in 2021 but who co-authored a paper in 2020.” This query
includes negation (“not authors”) combined with conjunction (“and who co-authored”), illustrating
advanced expressiveness.
43Published in Transactions on Machine Learning Research (11/2024)
•Integrate KGQA: Finally, we envision work on integrating integrating Knowledge Graph Ques-
tion Answering and CLQA. KGQA systems interpret natural language questions and map them to
complex graph queries. Suppose someone asks, “Where do the people who won the most prestigious
prize in computer science work?”. The system’s capabilities will parse this question, interpret it as a
graph query with appropriate operators and patterns, and execute the query on a knowledge graph
that is assumed to be complete . Combining this with CLQA would lead to systems that can answer
natural language questions with incomplete knowledge graphs. We elaborate on the similarities and
differences between CLQA and KGQA in Appendix B.
InDatasets andEvaluation :
•We require larger, more varied benchmark datasets that reflect real-world complexity. For example,
a new benchmark might include not just classic triple-based knowledge graphs but also hyper-
relational graphs or graphs that combine textual and visual data. These benchmarks should present
queries that use more expressive semantics—such as temporal conditions (“Which actors starred in
movies after 2010?”) or numeric filters (“Cities with population over one million?”). They should
also incorporate a wider range of query operators (like union or intersection) and complex query
patterns (such as nested queries). For instance, a robust benchmark might ask the system to handle
a query like: “Find images of all institutions connected to entities that are both scientists and
authors” requiring the system to interpret graph modalities, apply multiple operators, and return
results from multiple data sources. Finally, we would need more insight into how difficult certain
evaluation datasets really are. Currently, we cannot say whether the queries in one dataset are
intrinsically harder to answer or whether it is just an artifact of how systems are designed that
scores are different. This would also lead to more insight into what query answering system is
expected to be better for what datasets and queries.
•Currentevaluationmethodsoftenfocusnarrowlyonwhetherasystempredictsthecorrect“hard”an-
swers. We need a more comprehensive evaluation framework that measures a system’s performance
across the entire query answering process. For example, this framework might include metrics that
assess the efficiency of the query plan, how well the system handles uncertainty in queries, how it
ranks potential answers by relevance, or how it deals with partial information. Concretely, beyond
just checking if the system finds the correct city for a query like “What is the capital of France?” we
might evaluate how it handles ambiguous queries (“What is the capital of a big European country
near Germany?”), or how well it explains its reasoning steps. This means developing new met-
rics that reflect accuracy in retrieving possible answers (“hard” answers), correctness in reasoning
about complex semantics, and clarity in explaining results, ensuring a more principled and nuanced
evaluation of query answering systems.
Overall CLQA has demonstrated promise across a wide range of domains. In this survey we provide a
detailed taxonomy to study CLQA. We envision CLQA to be among the core tasks and capabilities of the
next generation graph databases.
Acknowledgments
We thank Pavel Klinov (Stardog), Matthias Fey (Kumo.AI), Qian Li (Stanford) and Keshav Santhanam
(Stanford) for providing valuable feedback on the draft. In addition, Michael wants to thank several people
from the L&R group from the VU Amsterdam, including Daniel Daza, Dimitrios Alivanistos, Frank van
Harmelen, Ruud van Bakel, Yannick Brunink, as well as Teodor Aleksiev (BSc. VU), Max Zwager (MSc.
VU),andPatrickKoopmann(UniversityDresden)fordiscussionsonvariousaspectsofthework. Michaelwas
partially funded by the Graph-Massivizer project, part of the Horizon Europe programme of the European
Union (grant 101093202); his research was further made possible by a gift from Accenture, LLP.
44Published in Transactions on Machine Learning Research (11/2024)
References
Vaibhav Adlakha, Parth Shah, Srikanta J. Bedathur, and Mausam. Regex queries over incomplete knowledge
bases. In 3rd Conference on Automated Knowledge Base Construction, AKBC , 2021.
Mehdi Ali, Max Berrendorf, Charles Tapley Hoyt, Laurent Vermue, Mikhail Galkin, Sahand Sharifzadeh,
Asja Fischer, Volker Tresp, and Jens Lehmann. Bringing light into the dark: A large-scale evaluation of
knowledge graph embedding models under a unified framework. IEEE Transactions on Pattern Analysis
and Machine Intelligence , 2021. doi: 10.1109/TPAMI.2021.3124805.
Dimitrios Alivanistos, Max Berrendorf, Michael Cochez, and Mikhail Galkin. Query embedding on hyper-
relational knowledge graphs. In International Conference on Learning Representations , 2022.
Güneş Aluç, Olaf Hartig, M Tamer Özsu, and Khuzaima Daudjee. Diversified stress testing of RDF data
management systems. In 13th International Semantic Web Conference , pp. 197–212. Springer, 2014.
Alfonso Amayuelas, Shuai Zhang, Xi Susie Rao, and Ce Zhang. Neural methods for logical reasoning over
knowledge graphs. In International Conference on Learning Representations , 2022.
Medina Andresel, Csaba Domokos, Daria Stepanova, and Trung-Kien Tran. A neural-symbolic approach for
ontology-mediated query answering. arXiv preprint arXiv:2106.14052 , 2021.
Medina Andresel, Trung-Kien Tran, Csaba Domokos, Pasquale Minervini, and Daria Stepanova. Combining
inductive and deductive reasoning for query answering over incomplete knowledge graphs. In Proceedings
of the 32nd ACM International Conference on Information and Knowledge Management , 2023. URL
https://doi.org/10.1145/3583780.3614816 .
Erik Arakelyan, Daniel Daza, Pasquale Minervini, and Michael Cochez. Complex query answering with
neural link predictors. In International Conference on Learning Representations , 2021.
Erik Arakelyan, Pasquale Minervini, , Daniel Daza, and Michael Cochez Isabelle Augenstein. Adapting
neural link predictors for complex query answering. In Thirty-seventh Conference on Neural Information
Processing Systems , 2023. URL https://openreview.net/forum?id=1G7CBp8o7L .
Akari Asai, Kazuma Hashimoto, Hannaneh Hajishirzi, Richard Socher, and Caiming Xiong. Learning to
retrieve reasoning paths over wikipedia graph for question answering. In International Conference on
Learning Representations , 2020. URL https://openreview.net/forum?id=SJgVHkrYDH .
Franz Baader, Diego Calvanese, Deborah McGuinness, Peter Patel-Schneider, Daniele Nardi, et al. The
description logic handbook: Theory, implementation and applications . Cambridge university press, 2003.
Samy Badreddine, Artur d’Avila Garcez, Luciano Serafini, and Michael Spranger. Logic tensor networks.
Artificial Intelligence , 303:103649, 2022.
Jinheon Baek, Alham Fikri Aji, and Amir Saffari. Knowledge-augmented language model prompting for
zero-shot knowledge graph question answering. In Proceedings of the 1st Workshop on Natural Language
Reasoning and Structured Explanations (NLRSE) , pp. 78–106, Toronto, Canada, 2023. Association for
Computational Linguistics. doi: 10.18653/v1/2023.nlrse-1.7. URL https://aclanthology.org/2023.
nlrse-1.7 .
Jiaxin Bai, Zihao Wang, Hongming Zhang, and Yangqiu Song. Query2Particles: Knowledge graph reasoning
with particle embeddings. In Findings of the Association for Computational Linguistics: NAACL 2022 .
Association for Computational Linguistics, 2022.
Jiaxin Bai, Chen Luo, Zheng Li, Qingyu Yin, Bing Yin, and Yangqiu Song. Knowledge graph reasoning over
entities and numerical values. In KDD 2023 , 2023a.
Jiaxin Bai, Tianshi Zheng, and Yangqiu Song. Sequential query encoding for complex query answering on
knowledge graphs. Transactions on Machine Learning Research , 2023b. ISSN 2835-8856. URL https:
//openreview.net/forum?id=ERqGqZzSu5 .
45Published in Transactions on Machine Learning Research (11/2024)
Yushi Bai, Xin Lv, Juanzi Li, and Lei Hou. Answering complex logical queries on knowledge graphs via
query computation tree optimization. In Proceedings of the 40th International Conference on Machine
Learning , pp. 1472–1491, 2023c.
Pablo Barceló, Egor V. Kostylev, Mikael Monet, Jorge Pérez, Juan Reutter, and Juan Pablo Silva. The
logical expressiveness of graph neural networks. In International Conference on Learning Representations ,
2020. URL https://openreview.net/forum?id=r1lZ7AEKvB .
Pablo Barceló, Mikhail Galkin, Christopher Morris, and Miguel Romero Orth. Weisfeiler and leman go
relational. In The First Learning on Graphs Conference , 2022. URL https://openreview.net/forum?
id=wY_IYhh6pqj .
Peter W Battaglia, Jessica B Hamrick, Victor Bapst, Alvaro Sanchez-Gonzalez, Vinicius Zambaldi, Mateusz
Malinowski, Andrea Tacchetti, David Raposo, Adam Santoro, Ryan Faulkner, et al. Relational inductive
biases, deep learning, and graph networks. arXiv preprint arXiv:1806.01261 , 2018.
Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim Sturge, and Jamie Taylor. Freebase: a collaboratively
created graph database for structuring human knowledge. In Proceedings of the 2008 ACM SIGMOD
international conference on Management of data , pp. 1247–1250, 2008.
Antoine Bordes, Nicolas Usunier, Alberto Garcia-Duran, Jason Weston, and Oksana Yakhnenko. Translating
embeddings for modeling multi-relational data. Advances in neural information processing systems , 26,
2013.
Elizabeth Boschee, Jennifer Lautenschlager, Sean O’Brien, Steve Shellman, James Starz, and Michael Ward.
ICEWS Coded Event Data. Harvard Dataverse , 2015. URL https://doi.org/10.7910/DVN/28075 .
Dan Brickley, Ramanathan V Guha, and Brian McBride. RDF schema 1.1. W3C recommendation , 25:
2004–2014, 2014.
Nathan Bronson, Zach Amsden, George Cabrera, Prasad Chakka, Peter Dimov, Hui Ding, Jack Ferris,
Anthony Giardullo, Sachin Kulkarni, Harry Li, et al. {TAO}: Facebook’s distributed data store for the
social graph. In 2013{USENIX}Annual Technical Conference ( {USENIX}{ATC}13), pp. 49–60, 2013.
Michael M Bronstein, Joan Bruna, Taco Cohen, and Petar Veličković. Geometric deep learning: Grids,
groups, graphs, geodesics, and gauges. arXiv preprint arXiv:2104.13478 , 2021.
Davide Buffelli, Pietro Liò, and Fabio Vandin. SizeShiftReg: a regularization method for improving size-
generalization in graph neural networks. In Advances in Neural Information Processing Systems , 2022.
Nilesh Chakraborty, Denis Lukovnikov, Gaurav Maheshwari, Priyansh Trivedi, Jens Lehmann, and Asja
Fischer. Introduction to neural network-based question answering over knowledge graphs. WIREs Data
Mining and Knowledge Discovery , 11(3):e1389, 2021. doi: https://doi.org/10.1002/widm.1389. URL
https://wires.onlinelibrary.wiley.com/doi/abs/10.1002/widm.1389 .
Ines Chami, Sami Abu-El-Haija, Bryan Perozzi, Christopher Ré, and Kevin Murphy. Machine learning on
graphs: A model and comprehensive taxonomy. Journal of Machine Learning Research , 23(89):1–64, 2022.
URL http://jmlr.org/papers/v23/20-852.html .
Mingyang Chen, Wen Zhang, Yuxia Geng, Zezhong Xu, Jeff Z Pan, and Huajun Chen. Generalizing to
unseen elements: A survey on knowledge extrapolation for knowledge graphs. In Proceedings of the Thirty-
Second International Joint Conference on Artificial Intelligence , 2023. doi: 10.24963/ijcai.2023/737. URL
https://doi.org/10.24963/ijcai.2023/737 .
Xuelu Chen, Ziniu Hu, and Yizhou Sun. Fuzzy logic based logical query answering on knowledge graphs. In
Thirty-Sixth AAAI Conference on Artificial Intelligence, AAAI , pp. 3939–3948. AAAI Press, 2022.
Nurendra Choudhary and Chandan K. Reddy. Complex logical reasoning over knowledge graphs using large
language models. arXiv preprint arXiv:2305.01157 , 2023.
46Published in Transactions on Machine Learning Research (11/2024)
Nurendra Choudhary, Nikhil Rao, Sumeet Katariya, Karthik Subbian, and Chandan Reddy. Probabilistic
entity representation model for reasoning over knowledge graphs. In Advances in Neural Information
Processing Systems , 2021a.
Nurendra Choudhary, Nikhil Rao, Sumeet Katariya, Karthik Subbian, and Chandan K Reddy. Self-
supervised hyperboloid representations from logical queries over knowledge graphs. In Proceedings of
the Web Conference 2021 , pp. 1373–1384, 2021b.
Michael Cochez. Semantic agent programming language: use and formalization. Master’s thesis, University
of Jyväskylä, 2012.
Edgar F Codd. A relational model of data for large shared data banks. Communications of the ACM , 13
(6):377–387, 1970.
Graham Cormode and Shan Muthukrishnan. An improved data stream summary: the count-min sketch and
its applications. Journal of Algorithms , 55(1):58–75, 2005.
Tamara Cucumides, Daniel Daza, Pablo Barceló, Michael Cochez, Floris Geerts, Juan L Reutter, and Miguel
Romero. UnRavL: A neuro-symbolic framework for answering graph pattern queries in knowledge graphs.
The Third Learning on Graphs Conference , 2024. URL https://openreview.net/forum?id=183XrFqaHN .
Rajarshi Das, Arvind Neelakantan, David Belanger, and Andrew McCallum. Chains of reasoning over
entities, relations, and text using recurrent neural networks. In Proceedings of the 15th Conference of the
European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers , pp. 132–141,
Valencia, Spain, 2017. Association for Computational Linguistics. URL https://aclanthology.org/
E17-1013 .
Rajarshi Das, Ameya Godbole, Ankita Naik, Elliot Tower, Manzil Zaheer, Hannaneh Hajishirzi, Robin Jia,
and Andrew McCallum. Knowledge base question answering by case-based reasoning over subgraphs. In
International Conference on Machine Learning, ICML 2022 , Proceedings of Machine Learning Research.
PMLR, 2022.
Daniel Daza and Michael Cochez. Message passing query embedding. Proceedings of the ICML 2020 Work-
shop on Graph Representation Learning and Beyond , 2020.
Lauren Nicole Delong, Ramon Fernández Mir, and Jacques D. Fleuriot. Neurosymbolic AI for reasoning
over knowledge graphs: A survey. IEEE Transactions on Neural Networks and Learning Systems , 2024.
doi: 10.1109/TNNLS.2024.3420218.
Caglar Demir, Michel Wiebesiek, Renzhong Lu, Axel-Cyrille Ngonga Ngomo, and Stefan Heindorf. LitCQD:
Multi-hop reasoning in incomplete knowledge graphs with numeric literals. In ECML PKDD , 2023.
Xin Luna Dong. Challenges and innovations in building a product knowledge graph. In Proceedings of the
24th ACM SIGKDD International conference on knowledge discovery & data mining , pp. 2869–2869, 2018.
Emma Flint. Announcing alexa entities (beta): Create more intelligent and engaging skills with
easy access to alexa’s knowledge, 2021. URL https://developer.amazon.com/en-US/blogs/alexa/
alexa-skills-kit/2021/02/alexa-entities-beta .
Tal Friedman and Guy Van den Broeck. Symbolic querying of vector spaces: Probabilistic databases meets
relational embeddings. In Conference on Uncertainty in Artificial Intelligence , pp. 1268–1277. PMLR,
2020.
Luis Galárraga, Christina Teflioudi, Katja Hose, and Fabian M. Suchanek. AMIE: Association rule mining
under incomplete evidence in ontological knowledge bases. In Proceedings of the International World Wide
Web Conference (WWW) , 2013.
MikhailGalkin, PriyanshTrivedi, GauravMaheshwari, RicardoUsbeck, andJensLehmann. Messagepassing
for hyper-relational knowledge graphs. In Proceedings of the 2020 Conference on Empirical Methods in
Natural Language Processing, EMNLP 2020 , pp. 7346–7359. Association for Computational Linguistics,
2020.
47Published in Transactions on Machine Learning Research (11/2024)
Mikhail Galkin, Etienne Denis, Jiapeng Wu, and William L. Hamilton. NodePiece: Compositional and
parameter-efficient representations of large knowledge graphs. In International Conference on Learning
Representations , 2022a. URL https://openreview.net/forum?id=xMJWUKJnFSw .
MikhailGalkin, ZhaochengZhu, HongyuRen, andJianTang. Inductivelogicalqueryansweringinknowledge
graphs. In Advances in Neural Information Processing Systems , 2022b.
Mikhail Galkin, Jincheng Zhou, Bruno Ribeiro, Jian Tang, and Zhaocheng Zhu. A foundation model for zero-
shot logical query reasoning. In The Thirty-eighth Annual Conference on Neural Information Processing
Systems, 2024. URL https://openreview.net/forum?id=JRSyMBBJi6 .
Jianfei Gao, Yangze Zhou, and Bruno Ribeiro. Double permutation equivariance for knowledge graph com-
pletion.arXiv preprint arXiv:2302.01313 , 2023.
Thomas Gebhart, Jakob Hansen, and Paul Schrater. Knowledge sheaves: A sheaf-theoretic framework for
knowledgegraphembedding. In Proceedings of The 26th International Conference on Artificial Intelligence
and Statistics , pp. 9094–9116, 2023. URL https://proceedings.mlr.press/v206/gebhart23a.html .
Justin Gilmer, Samuel S. Schoenholz, Patrick F. Riley, Oriol Vinyals, and George E. Dahl. Neural message
passing for quantum chemistry. In ICML, pp. 1263–1272, 2017.
Aditya Grover and Jure Leskovec. node2vec: Scalable feature learning for networks. In Proceedings of the
22nd ACM SIGKDD international conference on Knowledge discovery and data mining , pp. 855–864, 2016.
Ruiqi Guo, Philip Sun, Erik Lindgren, Quan Geng, David Simcha, Felix Chern, and Sanjiv Kumar. Acceler-
ating large-scale inference with anisotropic vector quantization. In International Conference on Machine
Learning , 2020. URL https://arxiv.org/abs/1908.10396 .
Yuanbo Guo, Zhengxiang Pan, and Jeff Heflin. LUBM: A benchmark for OWL knowledge base systems.
Journal of Web Semantics , 3(2-3):158–182, 2005.
Kelvin Guu, John Miller, and Percy Liang. Traversing knowledge graphs in vector space. In Proceedings
of the 2015 Conference on Empirical Methods in Natural Language Processing , pp. 318–327. Association
for Computational Linguistics, 2015. doi: 10.18653/v1/D15-1038. URL https://aclanthology.org/
D15-1038 .
Ferras Hamad, Issac Liu, and Xian Xing Zhang. Food discovery with Uber Eats: Building a query under-
standing engine, 2018. URL https://www.uber.com/blog/uber-eats-query-understanding/ .
Will Hamilton, Payal Bajaj, Marinka Zitnik, Dan Jurafsky, and Jure Leskovec. Embedding logical queries
on knowledge graphs. In Advances in Neural Information Processing Systems , volume 31, 2018.
William L Hamilton. Graph Representation Learning . Morgan & Claypool Publishers, 2020.
Jakob Hansen and Robert Ghrist. Toward a spectral theory of cellular sheaves. Journal of Applied and
Computational Topology , 3:315–358, 2019.
Olaf Hartig, Pierre-Antoine Champin, Gregg Kellogg, and Andy Seaborne. RDF-star and SPARQL-star.
Draft Community Group Report , 2022.
Sandro Hawke, Ivan Herman, Bijan Parsia, Axel Polleres, and Andy Seaborne. SPARQL 1.1 entailment
regimes. W3C recommendation , 2013.
Xiangnan He, Lizi Liao, Hanwang Zhang, Liqiang Nie, Xia Hu, and Tat-Seng Chua. Neural collaborative
filtering. In Proceedings of the 26th international conference on world wide web , pp. 173–182, 2017.
Xiaoxin He, Yijun Tian, Yifei Sun, Nitesh V Chawla, Thomas Laurent, Yann LeCun, Xavier Bresson, and
Bryan Hooi. G-retriever: Retrieval-augmented generation for textual graph understanding and question
answering. In The Thirty-eighth Annual Conference on Neural Information Processing Systems , 2024.
URL https://openreview.net/forum?id=MPJ3oXtTZl .
48Published in Transactions on Machine Learning Research (11/2024)
Yunjie He, Mojtaba Nayyeri, Bo Xiong, Evgeny Kharlamov, and Steffen Staab. Modeling relational patterns
for logical query answering over knowledge graphs. arXiv preprint arXiv:2303.11858 , 2023.
Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and
Jacob Steinhardt. Measuring mathematical problem solving with the MATH dataset. NeurIPS , 2021.
Vinh Thinh Ho, Daria Stepanova, Dragan Milchevski, Jannik Strötgen, and Gerhard Weikum. Enhancing
knowledge bases with quantity facts. In WWW ’22: The ACM Web Conference 2022 , pp. 893–901. ACM,
2022.
AidanHogan,EvaBlomqvist,MichaelCochez,Claudiad’Amato,GerarddeMelo,ClaudioGutierrez,Sabrina
Kirrane, José Emilio Labra Gayo, Roberto Navigli, Sebastian Neumaier, et al. Knowledge graphs. ACM
Computing Surveys (CSUR) , 54(4):1–37, 2021.
Zhiwei Hu, Víctor Gutiérrez-Basulto, Zhiliang Xiang, Xiaoli Li, Ru Li, and Jeff Z Pan. Type-aware
embeddings for multi-hop reasoning over knowledge graphs. In Proceedings of the Thirty-First In-
ternational Joint Conference on Artificial Intelligence, IJCAI-22 , pp. 3078–3084, 2022. URL https:
//doi.org/10.24963/ijcai.2022/427 .
Qian Huang, Hongyu Ren, and Jure Leskovec. Few-shot relational reasoning via connection subgraph pre-
training. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (eds.), Advances in
Neural Information Processing Systems , 2022a. URL https://openreview.net/forum?id=LvW71lgly25 .
Xingyue Huang, Miguel Romero Orth, İsmail İlkan Ceylan, and Pablo Barceló. A theory of link prediction
via relational weisfeiler-leman. In Thirty-seventh Conference on Neural Information Processing Systems ,
2023.
Zijian Huang, Meng-Fen Chiang, and Wang-Chien Lee. LinE: Logical query reasoning over hierarchical
knowledge graphs. In Aidong Zhang and Huzefa Rangwala (eds.), KDD ’22: The 28th ACM SIGKDD
Conference on Knowledge Discovery and Data Mining , pp. 615–625. ACM, 2022b.
Ihab F Ilyas, Theodoros Rekatsinas, Vishnu Konda, Jeffrey Pound, Xiaoguang Qi, and Mohamed Soliman.
Saga: A platform for continuous construction and serving of knowledge at scale. In Proceedings of the
2022 International Conference on Management of Data , pp. 2259–2272, 2022.
Jeff Johnson, Matthijs Douze, and Hervé Jégou. Billion-scale similarity search with GPUs. IEEE Transac-
tions on Big Data , 7(3):535–547, 2019.
Daniel Keysers, Nathanael Schärli, Nathan Scales, Hylke Buisman, Daniel Furrer, Sergii Kashubin, Nikola
Momchev, Danila Sinopalnikov, Lukasz Stafiniak, Tibor Tihon, Dmitry Tsarkov, Xiao Wang, Marc van
Zee, and Olivier Bousquet. Measuring compositional generalization: A comprehensive method on realistic
data. In International Conference on Learning Representations , 2020. URL https://openreview.net/
forum?id=SygcCnNKwr .
Thomas N. Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. In
ICLR, 2017.
Erich Peter Klement, Radko Mesiar, and Endre Pap. Triangular norms , volume 8. Springer Science &
Business Media, 2013.
Bhushan Kotnis, Carolin Lawrence, and Mathias Niepert. Answering complex queries in knowledge graphs
with bidirectional sequence encoders. In Proceedings of the AAAI Conference on Artificial Intelligence ,
pp. 4968–4977. AAAI Press, 2021.
TimothéeLacroix, NicolasUsunier, andGuillaumeObozinski. Canonicaltensordecompositionforknowledge
base completion. In International Conference on Machine Learning , pp. 2863–2872. PMLR, 2018.
Kalev Leetaru and Philip A Schrodt. Gdelt: Global data on events, location, and tone, 1979–2012. In ISA
annual convention , volume 2, pp. 1–49. Citeseer, 2013.
49Published in Transactions on Machine Learning Research (11/2024)
Jens Lehmann, Robert Isele, Max Jakob, Anja Jentzsch, Dimitris Kontokostas, Pablo N Mendes, Sebastian
Hellmann, Mohamed Morsey, Patrick Van Kleef, Sören Auer, and Christian Bizer. Dbpedia–a large-scale,
multilingual knowledge base extracted from wikipedia. Semantic web , 6(2):167–195, 2015.
Xi Victoria Lin, Richard Socher, and Caiming Xiong. Multi-hop knowledge graph reasoning with reward
shaping. In Empirical Methods in Natural Language Processing (EMNLP) , 2018.
Xueyuan Lin, Gengxian Zhou, Tianyi Hu, Li Ningyuan, Mingzhi Sun, Haoran Luo, et al. FLEX: Feature-
logic embedding framework for CompleX knowledge graph reasoning. arXiv preprint arXiv:2205.11039 ,
2022.
Xueyuan Lin, Chengjin Xu, Fenglong Su, Gengxian Zhou, Tianyi Hu, Ningyuan Li, Mingzhi Sun, Haoran
Luo, et al. TFLEX: Temporal feature-logic embedding framework for complex reasoning over temporal
knowledge graph. In Thirty-seventh Conference on Neural Information Processing Systems , 2023. URL
https://openreview.net/forum?id=oaGdsgB18L .
Lihui Liu, Boxin Du, Heng Ji, ChengXiang Zhai, and Hanghang Tong. Neural-answering logical queries on
knowledge graphs. In Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data
Mining, pp. 1087–1097, 2021.
Xiao Liu, Shiyu Zhao, Kai Su, Yukuo Cen, Jiezhong Qiu, Mengdi Zhang, Wei Wu, Yuxiao Dong, and
Jie Tang. Mask and reason: Pre-training knowledge graph transformers for complex logical queries. In
Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery & Data Mining , 2022.
Xiao Long, Liansheng Zhuang, Aodi Li, Shafei Wang, and Houqiang Li. Neural-based mixture probabilistic
query embedding for answering FOL queries on knowledge graphs. In Proceedings of the 2022 Conference
on Empirical Methods in Natural Language Processing, EMNLP 2022 , pp. 3001–3013. Association for
Computational Linguistics, 2022.
Haoran Luo, Haihong E, Yuhao Yang, Gengxian Zhou, Yikai Guo, Tianyu Yao, Zichen Tang, Xueyuan
Lin, and Kaiyang Wan. NQE: N-ary query embedding for complex query answering over hyper-relational
knowledge graphs. In AAAI 2023 , 2023.
Zhezheng Luo, Jiayuan Mao, Joshua B. Tenenbaum, and Leslie Pack Kaelbling. On the expressiveness and
learning of relational neural networks on hypergraphs, 2022. URL https://openreview.net/forum?id=
HRF6T1SsyDn .
Francois Luus, Prithviraj Sen, Pavan Kapanipathi, Ryan Riegel, Ndivhuwo Makondo, Thabang Lebese, and
Alexander Gray. Logic embeddings for complex query answering. arXiv preprint arXiv:2103.00418 , 2021.
Gengchen Mai, Krzysztof Janowicz, Bo Yan, Rui Zhu, Ling Cai, and Ni Lao. Contextual graph attention
for answering logical queries over incomplete knowledge graphs. In Proceedings of the 10th International
Conference on Knowledge Capture , pp. 171–178, 2019.
Gengchen Mai, Krzysztof Janowicz, Ling Cai, Rui Zhu, Blake Regalia, Bo Yan, Meilin Shi, and Ni Lao. SE-
KGE: A location-aware knowledge graph embedding model for geographic question answering and spatial
semantic lifting. Transactions in GIS , 24(3):623–655, 2020.
Stanislav Malyshev, Markus Krötzsch, Larry González, Julius Gonsior, and Adrian Bielefeldt. Getting the
most out of wikidata: Semantic technology usage in Wikipedia’s knowledge graph. In Proceedings of the
17th International Semantic Web Conference (ISWC’18) , volume 11137 of LNCS, pp. 376–394. Springer,
2018.
Michael McCloskey and Neal J Cohen. Catastrophic interference in connectionist networks: The sequential
learning problem. In Psychology of learning and motivation , volume 24, pp. 109–165. Elsevier, 1989.
George A Miller. WordNet: An electronic lexical database . MIT press, 1998.
50Published in Transactions on Machine Learning Research (11/2024)
Mike Mintz, Steven Bills, Rion Snow, and Dan Jurafsky. Distant supervision for relation extraction without
labeled data. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th
International Joint Conference on Natural Language Processing of the AFNLP , pp. 1003–1011, 2009.
T. Mitchell, W. Cohen, E. Hruschka, P. Talukdar, J. Betteridge, A. Carlson, B. Dalvi, M. Gardner, B. Kisiel,
J. Krishnamurthy, N. Lao, K. Mazaitis, T. Mohamed, N. Nakashole, E. Platanios, A. Ritter, M. Samadi,
B. Settles, R. Wang, D. Wijaya, A. Gupta, X. Chen, A. Saparov, M. Greaves, and J. Welling. Never-ending
learning. In Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence (AAAI-15) , 2015.
Eduardo Mizraji. Vector logic: a natural algebraic representation of the fundamental logical gates. Journal
of Logic and Computation , 18(1):97–121, 2008.
Boris Motik, Peter F Patel-Schneider, Bijan Parsia, Conrad Bock, Achille Fokoue, Peter Haase, Rinke
Hoekstra, Ian Horrocks, Alan Ruttenberg, Uli Sattler, et al. OWL 2 web ontology language: Structural
specification and functional-style syntax. W3C recommendation , 27(65):159, 2009.
ChauNguyen, TimFrench, WeiLiu, andMichaelStewart. SConE:Simplifiedconeembeddingswithsymbolic
operators for complex logical queries. In Findings of the Association for Computational Linguistics: ACL
2023, pp. 11931–11946, 2023a. URL https://aclanthology.org/2023.findings-acl.755 .
Chau Duc Minh Nguyen, Tim French, Wei Liu, and Michael Stewart. CylE: Cylinder embeddings for multi-
hop reasoning over knowledge graphs. In Proceedings of the 17th Conference of the European Chapter of
the Association for Computational Linguistics , pp. 1736–1751. Association for Computational Linguistics,
2023b. URL https://aclanthology.org/2023.eacl-main.127 .
Thomas Pellissier Tanon, Gerhard Weikum, and Fabian Suchanek. YAGO 4: A reason-able knowledge base.
InEuropean Semantic Web Conference , pp. 583–596. Springer, 2020.
Bryan Perozzi, Rami Al-Rfou, and Steven Skiena. Deepwalk: Online learning of social representations. In
Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining ,
pp. 701–710, 2014.
Maximilian Pflueger, David Tena Cucala, and Egor Kostylev. GNNQ: A neuro-symbolic approach to query
answering over incomplete knowledge graphs. In International Semantic Web Conference , 2022.
Tahereh Pourhabibi, Kok-Leong Ong, Booi H Kam, and Yee Ling Boo. Fraud detection: A systematic
literature review of graph-based anomaly detection approaches. Decision Support Systems , 133:113303,
2020.
Meng Qu, Junkun Chen, Louis-Pascal Xhonneux, Yoshua Bengio, and Jian Tang. RNNLogic: Learning
logic rules for reasoning on knowledge graphs. In International Conference on Learning Representations
(ICLR), 2021.
Hongyu Ren and Jure Leskovec. Beta embeddings for multi-hop logical reasoning in knowledge graphs. In
Advances in Neural Information Processing Systems , volume 33, 2020.
Hongyu Ren, Weihua Hu, and Jure Leskovec. Query2box: Reasoning over knowledge graphs in vector space
using box embeddings. In International Conference on Learning Representations , 2020.
Hongyu Ren, Hanjun Dai, Bo Dai, Xinyun Chen, Michihiro Yasunaga, Haitian Sun, Dale Schuurmans, Jure
Leskovec, and Denny Zhou. Lego: Latent execution-guided reasoning for multi-hop question answering on
knowledge graphs. In International Conference on Machine Learning , pp. 8959–8970. PMLR, 2021.
Hongyu Ren, Hanjun Dai, Bo Dai, Xinyun Chen, Denny Zhou, Jure Leskovec, and Dale Schuurmans.
SMORE: Knowledge graph completion and multi-hop reasoning in massive knowledge graphs. Proceedings
of the 28th ACM SIGKDD Conference on Knowledge Discovery & Data Mining , 2022.
Rex, Ying, Zhaoyu Lou, Jiaxuan You, Chengtao Wen, Arquimedes Canedo, and Jure Leskovec. Neural
subgraph matching. arXiv preprint arXiv:2007.03092 , 2020.
51Published in Transactions on Machine Learning Research (11/2024)
Mark B Ring. CHILD: A first step towards continual learning. Learning to learn , pp. 261–292, 1998.
Jeffrey Sardina, Callie Sardina, John D Kelleher, and Declan O’Sullivan. Analysis of attention mechanisms
in box-embedding systems. In Artificial Intelligence and Cognitive Science: 30th Irish Conference, AICS
2022, pp. 68–80. Springer, 2023.
Apoorv Saxena, Aditay Tripathi, and Partha Talukdar. Improving multi-hop question answering over knowl-
edge graphs using knowledge base embeddings. In Proceedings of the 58th Annual Meeting of the Associa-
tion for Computational Linguistics , pp. 4498–4507, Online, 2020. Association for Computational Linguis-
tics. doi: 10.18653/v1/2020.acl-main.412. URL https://aclanthology.org/2020.acl-main.412 .
Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Hagenbuchner, and Gabriele Monfardini. The graph
neural network model. IEEE transactions on neural networks , 20(1):61–80, 2008.
Michael Schlichtkrull, Thomas N Kipf, Peter Bloem, Rianne Van Den Berg, Ivan Titov, and Max Welling.
Modeling relational data with graph convolutional networks. In European semantic web conference , pp.
593–607. Springer, 2018.
Haitian Sun, Andrew Arnold, Tania Bedrax Weiss, Fernando Pereira, and William W Cohen. Faithful
embeddings for knowledge base queries. In Advances in Neural Information Processing Systems , 2020.
Zhiqing Sun, Zhi-Hong Deng, Jian-Yun Nie, and Jian Tang. RotatE: Knowledge graph embedding by
relational rotation in complex space. In International Conference on Learning Representations , 2019.
URL https://openreview.net/forum?id=HkgEQnRqYQ .
Muzamil Hussain Syed, Tran Quoc Bao Huy, and Sun-Tae Chung. Context-aware explainable recommenda-
tion based on domain knowledge graph. Big Data and Cognitive Computing , 6(1):11, 2022.
Zhenwei Tang, Shichao Pei, Xi Peng, Fuzhen Zhuang, Xiangliang Zhang, and Robert Hoehndorf. TAR:
Neural logical reasoning across tbox and abox. arXiv preprint arXiv:2205.14591 , 2022.
Komal Teru, Etienne Denis, and Will Hamilton. Inductive relation prediction by subgraph reasoning. In
International Conference on Machine Learning , pp. 9448–9457. PMLR, 2020.
James Thorne, Majid Yazdani, Marzieh Saeidi, Fabrizio Silvestri, Sebastian Riedel, and Alon Halevy.
Database reasoning over text. In Proceedings of the 59th Annual Meeting of the Association for Computa-
tional Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume
1: Long Papers) , pp. 3091–3104. Association for Computational Linguistics, 2021a.
James Thorne, Majid Yazdani, Marzieh Saeidi, Fabrizio Silvestri, Sebastian Riedel, and Alon Halevy. From
natural language processing to neural databases. In Proceedings of the VLDB Endowment , volume 14, pp.
1033–1039. VLDB Endowment, 2021b.
Sebastian Thrun. A lifelong learning perspective for mobile robot control. In Intelligent robots and systems ,
pp. 201–214. Elsevier, 1995.
Yuanyuan Tian, Wen Sun, Sui Jun Tong, En Liang Xu, Mir Hamid Pirahesh, and Wei Zhao. Synergistic
graph and sql analytics inside IBM Db2. Proceedings of the VLDB Endowment , 12(12):1782–1785, 2019.
ThéoTrouillon, JohannesWelbl, SebastianRiedel, ÉricGaussier, andGuillaumeBouchard. Complexembed-
dings for simple link prediction. In International conference on machine learning , pp. 2071–2080. PMLR,
2016.
Emile van Krieken, Erman Acar, and Frank van Harmelen. Analyzing differentiable fuzzy logic operators.
Artificial Intelligence , 302:103602, 2022. ISSN 0004-3702. doi: https://doi.org/10.1016/j.artint.2021.
103602. URL https://www.sciencedirect.com/science/article/pii/S0004370221001533 .
V Vapnik. The nature of statistical learning theory. Springer science & business media, Berlin, Germany ,
2013.
52Published in Transactions on Machine Learning Research (11/2024)
Shikhar Vashishth, Soumya Sanyal, Vikram Nitin, and Partha Talukdar. Composition-based multi-relational
graph convolutional networks. In International Conference on Learning Representations , 2020. URL
https://openreview.net/forum?id=BylA_C4tPr .
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz Kaiser,
and Illia Polosukhin. Attention is all you need. In Advances in Neural Information Processing Systems ,
volume 30, 2017.
Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Liò, and Yoshua Bengio.
Graph attention networks. In ICLR, 2018.
Denny Vrandecic and Markus Krötzsch. Wikidata: a free collaborative knowledgebase. Commun. ACM , 57
(10):78–85, 2014.
Dingmin Wang, Yeyuan Chen, and Bernardo Cuenca Grau. Efficient embeddings of logical variables for
query answering over incomplete knowledge graphs. In Proceedings of the AAAI Conference on Artificial
Intelligence . AAAI Press, 2023a.
Kai Wang, Chunhong Zhang, Jibin Yu, and Qi Sun. Signal embeddings for complex logical reasoning in
knowledge graphs. In International Conference on Knowledge Science, Engineering and Management , pp.
255–267. Springer, 2022.
Meng Wang, Haomin Shen, Sen Wang, Lina Yao, Yinlin Jiang, Guilin Qi, and Yang Chen. Learning to hash
for efficient search over incomplete knowledge graphs. In 2019 IEEE International Conference on Data
Mining (ICDM) , 2019.
Siyuan Wang, Zhongyu Wei, Jiarong Xu, Taishan Li, and Zhihao Fan. Unifying structure reasoning and
language model pre-training for complex reasoning. IEEE/ACM Trans. Audio, Speech and Lang. Proc. ,
pp. 1586–1595, 2023b.
Yanan Wang, Michihiro Yasunaga, Hongyu Ren, Shinya Wada, and Jure Leskovec. Vqa-gnn: Reasoning
with multimodal knowledge via graph neural networks for visual question answering. In 2023 IEEE/CVF
International Conference on Computer Vision (ICCV) , pp. 21525–21535. IEEE Computer Society, 2023c.
URL https://doi.ieeecomputersociety.org/10.1109/ICCV51070.2023.01973 .
Zihao Wang, Hang Yin, and Yangqiu Song. Benchmarking the combinatorial generalizability of complex
query answering on knowledge graphs. In Thirty-fifth Conference on Neural Information Processing Sys-
tems Datasets and Benchmarks Track (Round 2) , 2021.
Zihao Wang, Weizhi Fei, Hang Yin, Yangqiu Song, Ginny Y. Wong, and Simon See. Wasserstein-Fisher-
Rao embedding: Logical query embeddings with local comparison and global transport. In Findings
of the Association for Computational Linguistics: ACL 2023 , pp. 13679–13696, 2023d. URL https:
//aclanthology.org/2023.findings-acl.864 .
Zihao Wang, Yangqiu Song, Ginny Y. Wong, and Simon See. Logical message passing networks with one-
hop inference on atomic formulas. In The Eleventh International Conference on Learning Representations,
ICLR, 2023e. URL https://openreview.net/forum?id=SoyOsp7i_l .
Boris Weisfeiler and Andrey Leman. The reduction of a graph to canonical form and the algebra which
appearstherein. Nauchno-Technicheskaya Informatsia , 2(9):12–16, 1968. EnglishtranslationbyG.Ryabov
is available at https://www.iti.zcu.cz/wl2018/pdf/wl_paper_translation.pdf .
Robert West, Evgeniy Gabrilovich, Kevin Murphy, Shaohua Sun, Rahul Gupta, and Dekang Lin. Knowledge
base completion via search-based question answering. In 23rd International World Wide Web Conference,
WWW ’14 , pp. 515–526. ACM, 2014.
Longfeng Wu, Yao Zhou, and Dawei Zhou. Towards high-order complementary recommendation via logical
reasoning network. arXiv preprint arXiv:2212.04966 , 2022.
53Published in Transactions on Machine Learning Research (11/2024)
Yanrong Wu and Zhichun Wang. Knowledge graph embedding with numeric attributes of entities. In Pro-
ceedings of the Third Workshop on Representation Learning for NLP , pp. 132–136. Association for Com-
putational Linguistics, 2018. doi: 10.18653/v1/W18-3017. URL https://aclanthology.org/W18-3017 .
Zhaohan Xi, Ren Pang, Changjiang Li, Tianyu Du, Shouling Ji, Fenglong Ma, and Ting Wang. Reasoning
over multi-view knowledge graphs. arXiv preprint arXiv:2209.13702 , 2022.
Wenhan Xiong, Thi-Lan-Giao Hoang, and William Yang Wang. DeepPath: A reinforcement learning method
for knowledge graph reasoning. In Empirical Methods in Natural Language Processing (EMNLP) , 2017.
Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural networks? In
International Conference on Learning Representations , 2019.
Yao Xu, Shizhu He, Li Cai, Kang Liu, and Jun Zhao. Prediction and calibration: Complex reasoning over
knowledge graph with bi-directional directed acyclic graph neural network. In Findings of the Association
for Computational Linguistics: ACL 2023 , pp. 7189–7198, 2023a. URL https://aclanthology.org/
2023.findings-acl.450 .
Yao Xu, Shizhu He, Cunguang Wang, Li Cai, Kang Liu, and Jun Zhao. Query2Triple: Unified query
encoding for answering diverse complex queries over knowledge graphs. In Findings of the Association for
Computational Linguistics: EMNLP 2023 , pp. 11369–11382, 2023b. URL https://aclanthology.org/
2023.findings-emnlp.761 .
Zezhong Xu, Wen Zhang, Peng Ye, Hui Chen, and Huajun Chen. Neural-symbolic entangled framework for
complex query answering. In Advances in Neural Information Processing Systems , 2022.
Bishan Yang, Wen-tau Yih, Xiaodong He, Jianfeng Gao, and Li Deng. Embedding entities and relations
for learning and inference in knowledge bases. In International Conference on Learning Representations ,
2015.
Dong Yang, Peijun Qing, Yang Li, Haonan Lu, and Xiaodong Lin. GammaE: Gamma embeddings for logical
queries on knowledge graphs. In Proceedings of the 2022 Conference on Empirical Methods in Natural
Language Processing, EMNLP 2022 , pp. 745–760. Association for Computational Linguistics, 2022a.
Haotong Yang, Zhouchen Lin, and Muhan Zhang. Rethinking knowledge graph evaluation under the
open-world assumption. In Advances in Neural Information Processing Systems , 2022b. URL https:
//openreview.net/forum?id=5xiLuNutzJG .
Michihiro Yasunaga, Hongyu Ren, Antoine Bosselut, Percy Liang, and Jure Leskovec. QA-GNN: Rea-
soning with language models and knowledge graphs for question answering. In Proceedings of the 2021
Conference of the North American Chapter of the Association for Computational Linguistics: Human
Language Technologies , pp. 535–546, Online, 2021. Association for Computational Linguistics. doi:
10.18653/v1/2021.naacl-main.45. URL https://aclanthology.org/2021.naacl-main.45 .
Gilad Yehudai, Ethan Fetaya, Eli A. Meirom, Gal Chechik, and Haggai Maron. From local structures to size
generalization in graph neural networks. In Proceedings of the 38th International Conference on Machine
Learning , 2021.
Hang Yin, Zihao Wang, Weizhi Fei, and Yangqiu Song. EFO k-CQA: Towards knowledge graph complex
query answering beyond set operation. arXiv preprint arXiv:2307.13701 , 2023a.
Hang Yin, Zihao Wang, and Yangqiu Song. On existential first order queries inference on knowledge graphs.
arXiv preprint arXiv:2304.07063 , 2023b.
Manzil Zaheer, Satwik Kottur, Siamak Ravanbakhsh, Barnabas Poczos, Russ R Salakhutdinov, and Alexan-
der J Smola. Deep sets. In Advances in Neural Information Processing Systems , volume 30, 2017.
Bohui Zhang, Filip Ilievski, and Pedro Szekely. Enriching wikidata with linked open data. arXiv preprint
arXiv:2207.00143 , 2022a.
54Published in Transactions on Machine Learning Research (11/2024)
Muhan Zhang, Pan Li, Yinglong Xia, Kai Wang, and Long Jin. Labeling trick: A theory of using graph
neural networks for multi-node representation learning. In Advances in Neural Information Processing
Systems, volume 34, pp. 9061–9073. Curran Associates, Inc., 2021a.
Wen Zhang, Jiaoyan Chen, Juan Li, Zezhong Xu, Jeff Z Pan, and Huajun Chen. Knowledge graph reasoning
with logics and embeddings: Survey and perspective. arXiv preprint arXiv:2202.07412 , 2022b.
Zhanqiu Zhang, Jie Wang, Jiajun Chen, Shuiwang Ji, and Feng Wu. Cone: Cone embeddings for multi-hop
reasoning over knowledge graphs. In Advances in Neural Information Processing Systems , 2021b.
Yangze Zhou, Gitta Kutyniok, and Bruno Ribeiro. OOD link prediction generalization capabilities of
message-passing GNNs in larger test graphs. In Advances in Neural Information Processing Systems ,
2022.
Zhaocheng Zhu, Zuobai Zhang, Louis-Pascal Xhonneux, and Jian Tang. Neural bellman-ford networks: A
general graph neural network framework for link prediction. Advances in Neural Information Processing
Systems, 34:29476–29490, 2021.
Zhaocheng Zhu, Mikhail Galkin, Zuobai Zhang, and Jian Tang. Neural-symbolic models for logical queries
on knowledge graphs. In International Conference on Machine Learning, ICML 2022 , Proceedings of
Machine Learning Research. PMLR, 2022.
55Published in Transactions on Machine Learning Research (11/2024)
A Definitions and Theoretical Foundations
A.1 Types of Knowledge Graphs
Hyper-relational KGs generalize triple KGs by allowing edges to have relation-entity qualifiers. We define
them as follows:
Definition A.1 (Hyper-relational Knowledge Graph - derived from Alivanistos et al. (2022))
WithEandRdefined as in Definition 2.2, let Q= 2(R×E ),7we define a hyper-relational knowledge graph
G= (E,R,S). In this case, the edge set S⊂ (E×R×E× Q)consists of qualified statements, where each
statements= (es,r,eo,qp)includesqpthat represent contextual information for the main relation r. This
setqp={q1,...}={(qr1,qe 1),(qr2,qe 2)...}⊂R×E is the set of qualifier pairs , where{qr1,qr 2,...}are
thequalifier relations and{qe1,qe 2,...}thequalifier entities .
We note that ifEandRare finite sets, then also Qis finite and, there are only a finite number of ( r,qp)
combinations possible. As a consequence, we find that with these conditions, and by defining a canonical
ordering over Q, we can represent the hyper-relational graph using first order logic by coining a new predicate
rqpfor each combination. The statement from the definition can then be written as rqp(es,eo).
For example, one statement on a hyper-relational KG in (Fig. 2) is (Hinton, education, Cambridge,
{(degree, Bachelor)}) . The qualifier pair {(degree, Bachelor)} provides additional context to the
main triple and helps to distinguish it from other education facts. If the conditions mentioned above are
met, we can write this statement as education{(degree :Bachelor )}(Hinton,Cambridge ).
Hyper-relational KGs can capture a more diverse set of facts by using the additional qualifiers for facts on
the graph.
Hinton Cambridge Edinburgh education education 
Welling Utrecht Edinburgh 
education 
Bachelor PhDdegree degree education_bachelor 
education_phd Hinton Edinburgh 
PhDBachelor Cambridge RDF star 
Hypergraph KG 
(with ad-hoc hyperedge types) Hinton Cambridge Edinburgh education education 
Hinton Cambridge Edinburgh education education 
Bachelor PhDdegree degree education_bachelor 
education_phd Hinton Edinburgh 
PhDBachelor Cambridge 
Welling Utrecht education 
Bachelor PhDdegree 
Welling Utrecht education education 
Bachelor PhDdegree degree Utrecht 
Gravity Flat 
Universe Flat 
Universe Gravity topic
topic topicHyper-Relational 
Figure 21: RDF-star and
Hyper-relational Knowl-
edge GraphsThis concept is closely related (albeit with different theoretical foundations) to
the RDF-star format introduced in Hartig et al. (2022). RDF-star explicitly
separates metadata (context) from data (statements). RDF-star allows us to
use a triple as the subject or object of another triple, which gives the same
expressive power than hyper-relational graphs. A core difference, affecting the
way data is modeled, is that the identity of a triple is solely defined by its con-
stituents. This means that if a triple is used in two places, it is the same triple,
which makes making complex statements about the triple more complicated
(Fig. 21). For example, assume we want to express that Max Welling did his
BSc thesis onflat universe inUtrecht and his PhD degree ongravity
at the same university. With a hyper-relational graph we can make two edges.
Each edge would state that Max Welling studied in Utrecht, but each would
have two qualifier pairs with the corresponding topic and institute information.
In RDF-star, we cannot model it with the same triple as subject of another
triples, because RDF-star has no way to ‘pair-up’ the topic and degree level,
and will instead think that the main triple has four pieces of information con-
nected to it. The hyper-relational graphs introduced above do not have such
issues. RDF-star further extends the values allowed in the object and qualifier
value position to include literals, which we will discuss below in Definition A.3.
Definition A.2 (Hypergraph Knowledge Graph) WithEandRdefined as in Definition 2.2, a hy-
pergraph KG is a graph G= (E,R,S)where statements S⊂ (R× 2E)are hyperedges. Such a hyperedge
s= (r,e) = (r,{e1,...,ek})has one relation type rand links the kentities together, where the order of these
entities is not significant. k, the size of eis called the arityof the hyperedge.
An example hyperedge in Fig. 2 is (education_degree ,{Hinton,Cambridge,Bachelor}). This is a 3-ary
statement (being comprised of three entities). In contrast to hyper-relational KGs, hyperedges consist of
7Given a set S, we use the notation 2Sto denote the powerset of S. Here, R×Edenotes the Cartesian product of the set
of relations and the set of entities. So, 2(R×E )denotes the set containing all possible sets of pairs of a relation and an entity.
56Published in Transactions on Machine Learning Research (11/2024)
only one relation type and cannot naturally incorporate more fine-grained relational compositions. Instead,
the type of the hyperedge is rather a merged set of statement relations and every composition leads to a new
relation type on hypergraph KGs. It is thus not as compositional as the hyper-relational KG. That is, when
describing, for instance, a majorrelation of the education(Hinton, Cambridge) fact, a hyper-relational
KG can simply use it as a qualifier and retain both relations whereas a hypergraph model has to come
up with a new hyperedge type education_major . With a growing number of relations, such a strategy of
creating new hyperedge relation types might lead to a combinatorial explosion. However, the hypergraph
model is more suitable for representing relationships of varying arity, with equal contributions of all entities
such as partnership(companyA, companyB, companyC) .
Each of the graph types introduced above can be extended to also support literals. This happens by
allowing nodes or qualifier values to contain a literal value. Literals contain numerical, categorical, discrete
timestamps, or text data that cannot be easily discretized in an exhaustive set of possible values, like entities.
Similarly to the Web Ontology Language (OWL, Motik et al. (2009)) that distinguishes object properties
that connect two entities from datatype properties that connect an entity and a literal, it is common to use
the term relationfor an edge between two entities, and attribute for an edge between an entity and a literal.
Definition A.3 (KG with Literals) WithEandRas for one of the graph types above, a correspond-
ing KG with literals GL= (E,R,SL,L)has an additional set of literals L ⊂ Conrepresenting numer-
ical, categorical, textual, images, sound waves, or other continuous values ( Lis disjoint from Eand
R). If we extend standard KGs to RDF graphs, literals can only be used in the objects position, that is
SL⊂(E×R× (E∪L )). In RDF-star graphs, literals can be objects or qualifier values, that is, Q= 2(R×(E∪L ))
andSL⊂(E×R× (E∪L )×Q).8For both of these, we could also define graph types with literals in other
positions of the triples, as necessary, or introduce more complex substructures in the elements of the triple
(see e.g., Cochez (2012)). In hypergraph KGs, literals can be introduced in the elements of a hyperedge,
SL⊂(R×2E∪L)
If the set of possible statements of the KG with literals has the same finiteness properties as the one without
literals, then the properties regarding expressing it using first order logic do not change. Some graphs, like
property graphs allow nodes to contain attributes, but this is equivalent with creating extra nodes with these
attribute values and adding relations to those. In theory, also the edge type could be a literal, but since this
can be modeled using a hyper-relational graph with literals, we exclude these from this work.
An example triple with a literal object (Fig. 5) is (Cambridge, established, 1209) . In (Hinton,
education, Cambridge) ,education is a relation, whereas in (Cambridge, established, 1209) ,
established is an attribute.
The knowledge graph definitions above are not exhaustive. It is possible to create graphs with other prop-
erties. Examples include graphs with undirected edges, without edge labels, with time characteristics, with
probabilistic or fuzzy relations, etc. Besides, it is also possible to have graphs or edges with combined char-
acteristics. One could, for instance, define a hyperedge with qualifying information. Because of this plethora
of options, we decided to limit ourselves to the options above. In the next section we introduce how to query
these graphs.
A.2 Basic Approximate Graph Query Answering
Until now, we have assumed that our KG is complete, i.e., it is possible to exactly answer the queries.
However, we are interested in the setting where we do not have the complete graph. The situation can be
described as follows: Given a knowledge graph G(subset of a complete, but not observable graph ˆG) and a
basic graph query Q. Basic Approximate Graph Query Answering is the task of answering Q, without having
access to ˆG. Depending on the setting, an approach to this could have access to G, or to a set of example
(query, answer) pairs, which can be used to produce the answers (see also Section 7.3). In the example from
8Both RDF and RDF-star also allow blank nodes used to indicate entities without a specified identity in the subject and
object position Brickley et al. (2014) . Besides, they also have support for named graphs (and in some cases for quadruples).
We do not support these explicitly in our formalism, but all of these can be modeled using hyper-relational graphs.
57Published in Transactions on Machine Learning Research (11/2024)
Fig. 1, several edges are drawn with dashes; these are true edges, but only there in the non-observable part
of the graph ˆG.
The goal is to find the answer set as if ˆGwas known. In this case, the complete answer set becomes
{µ1,{(?person,Bengio ),(?uni,UdeM )},{(?person,LeCun ),(?uni,NYU)}}, which includes the answer µ1of the
non-approximate version.
The query which includes the negation would have the following answers if our graph was complete:
{{(?person,Bengio ),(?uni,UdeM )},{(?person,LeCun ),(?uni,NYU)}}, which does not include µ1.
Approximate Query Answering Systems provide a score ∈Rforevery possible mapping9. Hence, the answer
provided by these systems is a function from mappings ( i.e., aµ) to their corresponding score in R.
Definition A.4 (Basic Approximate Graph Query Answering) Given a knowledge graph G(sub-
graph of a complete, but not observable knowledge graph ˆG), a basic graph query Q, and the scoring domain
R, abasicapproximategraphqueryanswer to the queryQis a function fwhich maps every possible mapping
(µ:VarQ→Con) toR.
Table 15: Ordered scored map-
pingsfortheexamplequery. The
two wrong mappings are in red.
?person ?uni score
Hinton UofT 40
Bengio UdeM 35
Welling UofT 34
UdeM Hinton 33
LeCun NYU 32
... ... ...The objective is to make it such that the correct answers according to the
graph ˆGget a better score (are ranked higher, have a higher probability,
have a higher truth value) than those which are not correct answers.
However, it is not guaranteed that answers which are in the observable
graphGwill get a high score.
For our example, a possible mapping is visualized in Table 15. Each row
in the table corresponds to one mapping µ. Ideally, all correct mappings
should be ranked on top, and all others below. However, in this example
we see several correct answers ranked high, but also two which are wrong.
A.3 Graph Query Answering
In the previous sections, we introduced the basic graph query and how it can be answered either exactly,
or in an approximate graph querying setting. However, there is variation in what types of queries methods
can answer; some only support a subset of our basic graph queries, while others support more complicated
queries. In this section we will again focus on exact (non-approximate) query answering and look at some
possible restrictions and then at extensions.
Definition A.5 (Graph Query Answering) Given a knowledge graph G, a query formalism, and a graph
queryQaccording to that formalism. An answer to the query is any mapping µ:VarQ→Consuch that the
requirements of the query formalism are met. The set of all answers is the set of all such mappings.
For our basic graph queries introduced in Definition 2.4, the query formalism sets requirements as to what
edgesmustandmustnotexistinthegraph(Definition2.5). Inthatcontextwealreadymentionedconjunctive
queries, which exist either with (CQ neg) or without (CQ) negation. If the conditions for writing our graphs
using first order logic (FOL) hold, we can equivalently write our basic graph queries in first order logic.
Each variable in the query becomes existentially quantified, and the formula becomes a conjunction of 1) the
terms formed from the triples in S′, and 2) the negation of the terms formed from the triples in S′. If there
are variables on the edges of the query graph, then we can rewrite the second order formula in first order
logic, by interpreting them as a disjunction over all possible predicates from the finite number of options.
Our example query from above becomes the following FOL sentence.
q=∃?person,?uni :win(TuringAward ,?person )∧field(DeepLearning ,?person )∧university (?person,?uni)
The answer to the query is the variable assignment function. For several of the more restrictive fragments,
it is useful to formulate the queries using this FOL syntax.
9This score can indicate the rank of the mapping, a likelihood or a truth value, or be a binary indicating value. In some
cases, only a partial mapping is provided. Anything not in this mapping has an implicit (zero or false) value.
58Published in Transactions on Machine Learning Research (11/2024)
Path Queries Tree Queries DAG Queries Cyclic 
 Queries 
SPARQL , 
SPARQL* , 
Cypher 
queries constant 
variable 
target 
Figure 22: A space of query patterns and their relative expressiveness. Multi-hop reasoning systems tackle
the simplest Pathqueries. Existing complex query answering systems support Treequeries whereas DAG
andCyclicqueries remain unanswerable by current neural models.
Restrictions The first restriction one can introduce are multi-hop queries , known in the NLP and KG
literature for a long time (Guu et al., 2015; Das et al., 2017; Asai et al., 2020) mostly in the context of
question answering. Formally, multi-hop queries (or pathqueries) are CQ which form a chain, where the tail
of one projection is a head of the following projection, i.e.,
qpath=Vk,∃V1,...,Vk−1:r1(v,V 1)∧r2(V1,V2)∧···∧rk(Vk−1,Vk)
wherev∈E,∀i∈[1,k] :ri∈R,Vi∈Varand allViare existentially quantified variables. In other words,
path queries do not contain branch intersections and can be solved iteratively by fetching the neighbors of
the nodes. One could also define multi-hop queries which allow negation.
Other ways of restricting CQ and CQ neg, resulting in more expressive queries than the multi-hop ones exist.
One can define families of logical queries shaped as a Tree, aDAG, and allowing cyclicparts. Illustrated
in Fig. 22, path (multi-hop) queries form the least expressive set of logical queries. Tree queries add more
reasoning branches connected by intersection operators, DAG queries drop the query tree requirement and
allow queries to be directed acyclic graphs, and, finally, cyclic queries further drop the acyclic requirement.
Note that these queries do not allow variables in the predicate position. Besides, all entities (in this context
referred to as anchors) must occur before all variables in the topological ordering.
We elaborate more on these query types in Section 6.2 and note that the majority of surveyed neural CLQA
methods in Section 5 are still limited to Tree-like queries, falling behind the expressiveness of many graph
database query languages. Bridging this gap is an important avenue for future work.
Extensions The first extension we introduce is the union.
Definition A.6 (Union of Sets of Mappings) Given two sets of mappings (like µfrom Definition 2.5),
we can create a new set of mappings by taking their union. This union operator is commutative and asso-
ciative, we can hence also talk about the union of three or more mappings. It is permissible that the domains
of the mappings in the input sets are not the same.
Wecandefineanewtypeofquerybyapplyingtheunionoperationontheoutcomesoftwoormoreunderlying
queries. If these underlying queries are basic graph queries, we will call this new type of queries Unions
of Conjunctive Queries with negation (UCQ neg)and if the basic queries did not include negation, as
Union of Conjunctive Queries (UCQ) . These classes are also familiar from FOL, and indeed correspond
to a disjunction of conjunctions.
59Published in Transactions on Machine Learning Research (11/2024)
As an example, the following query is in UCQ negbecause it is a disjunction of conjunctive terms which
consist of atoms aithat are relation projections or their negations:
q=v?.∃v1,...,vn:/parenleftig
r1(c,v1)/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
a1∧r2(v1,v2)/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
a2/parenrightig
∨/parenleftig
¬r3(v2,v3)/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
a3/parenrightig
∨···∨rk(vn,v?)/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
am
Moreover, there are FOL fragments that are equivalent to these fragments. Specifically, all queries in EPFO,
which are Existential Positive First Order sentences, have an equivalent query in UCQ, and all queries in
EFO, Existential First Order sentences, have an equivalent query in UCQ neg. The reason is that EPFO and
EFO sentences can be written in the Disjunctive Normal Form (DNF) as a union of conjunctive terms.
Some query languages, like SPARQL, allow an optional part to a query. In our formalism, we can define
theoptional part using the union operator. Assuming there are noptional parts in the query, create 2n
different queries, in which other combinations of optional parts are removed. The answer to the query is then
the union of the answers to all those queries. If the query language already allowed unions, then optionals
do not make it more expressive.
Beyond these extensions, one could extend further to all queries one could express with FOL , which
requires either universal quantification, or negation of conjunctions. These are, however, still not all possible
graph queries. An example of interesting queries, which are not in FOL, are conjunctive regular path
queries. These are akin to the path queries we discussed above, but without a specified length.
Definition A.7 (Regular Path Query) A regular path query is a 3-tuple (s,R,t ), wheres∈Termthe
starting term of the path, R∈Termthe relation term of the path, and t∈Termthe ending term of the path.
The query represents a path starting in s, traversing an arbitrary number of edges of type Rending int.
Because this kind of query is a conjunction of an arbitrary length, it cannot be represented in FOL. If one
wants to express paths with a fixed length, this would be a multi-hop path like the one described above. If
one wants to express a maximum length, then this could be done using a union of all allowed lengths. For
the two latter cases, the query can still be expressed in EPFO.
Definition A.8 (Regular Path Query Answering) Given a knowledge graph Gand a regular path query
Q= (s,R,t ). An answer to the query is any mapping µ:VarQ→Con, such that if we replace all variables
vinQwithµ(v), obtaining (ˆs,ˆR,ˆt), there exists a path in the graph that starts at node ˆs, then traverses one
or more edges of type ˆR, and ends in ˆt. The set of all answers to the query is the set of all such mappings.
There exist several variations on regular path queries and they can also be combined with the above query
types to form new ones. In Fig. 23 we illustrate how the fragments relate to other query classes. Most
methods fall strictly within the EFO fragment, i.e., they only support a subset with restrictions as we
discussed above. We will discuss further limitations of these methods in Section 6.
The two final aspects we want to highlight here are projections andjoins.
Definition A.9 (Projection of a Query Answer) Given a query answer µ, and a set of variables V∈
Var. The projection of the query answer on the variables Vis{(var,val )|(var,val )∈µ,var∈V}.
In other words, it is the answer but restricted to a specific set of variables. The query forms introduced
above can all be augmented with a projection to only obtain values for specific variables. This corresponds
to the SELECT ?var clause in SPARQL. Alternatively, it is possible to project all variables in Vwhich is
equivalent to the SELECT * clause. A query without any projected variable is a Boolean subgraph matching
problem equivalent to the ASKquery in SPARQL.
A join is used to combine the results of two separate queries into one. Specifically,
Definition A.10 (Join of Query Answers) Given two query answers µAandµB, and VarA, and VarB
the variables occurring in µAandµB, respectively. The join of these two answers only exists in case they
have the same value for all variables they have in common, i.e., ∀var∈VarA∩VarB:µA(var) =µb(var).
In that case, join(µA,µB) =µA∪µB.
60Published in Transactions on Machine Learning Research (11/2024)
Current 
Models 
All 
Graph 
Queries SPARQL , 
SPARQL* , 
Cypher FOL EFO / 
UCQnegEPFO / 
 UCQ Regular 
Path 
Queries 
Figure 23: Current query answering models cover Existential Positive First Order (EPFO) and Existential
First Order (EFO) logic fragments (marked in a red rectangle). EPFO and EFO are equivalent to unions
of conjunctions (UCQ), and those with atomic negation (UCQ neg), respectively. These, in turn, are a
subset of first order logic (FOL). FOL queries, in turn, are only a subset of queries answerable by graph
database languages. For example regular path queries cannot be expressed in FOL. Languages like SPARQL,
SPARQL*, or Cypher, encompass all the query types and more.
Given two sets of answers, their join is defined as follows.
Definition A.11 (Join of Query Answer Sets) Given two sets of query answers A and B, the join of
these two is a new set join(A,B) ={join(a,b)|a∈A,b∈B,andjoin(a,b)exists}.
This operation enables us to combine multiple underlying queries, potentially of multiple types into a single
one. For example, given the set of answers from our example basic graph query above:
A={{(?person,Hinton ),(?uni,UofT )},{(?person,Bengio ),(?uni,UdeM )},{(?person,LeCun ),(?uni,NYU)}}
and another set of answers
B={{(?person,Hinton ),(?born,1947 )},{(?person,Bengio ),(?born,1964 )},{(?person,Welling ),(?born,1968 )}}
The join of these becomes:
join(A,B) ={{(?person,Hinton ),(?uni,UofT ),(?born,1947 )},{(?person,Bengio ),(?uni,UdeM ),(?born,1964 )}}
We will discuss joins further in Section 6.1, where we will use these basic building blocks to define a broader
set of query operators, aiming to cover all operations that exist in SPARQL. This includes Kleene plus/star
(+/*) for building property paths, FILTER, OPTIONAL, and different aggregation functions.
A.4 Triangular Norms and Conorms
Answering logical queries implies execution of logical operators. Approximate query answering, in turn,
implies continuous vector inputs and output truth values that are not necessarily binary. Besides, the
methods often require that the logical operators are smooth and differentiable. Triangular norms (T-norms)
and triangular conorms (T-conorms) define functions that generalize logical conjunction and disjunction,
respectively, to the continuous space of truth values and implement fuzzy logical operations.
T-norm defines a continuous function ⊤: [0,1]×[0,1]→[0,1]with the following properties ⊤(x,y) =
⊤(y,x)(commutativity), ⊤(x,⊤(y,z)) =⊤(⊤(x,y),z)(associativity), and y≤z→ ⊤ (x,y)≤ ⊤(x,z)
(monotonicity). Also, the identity element for ⊤is 1,i.e.,⊤(x,1) =x. The goal of t-norms is to generalize
logical conjunction with a continuous function. The T-conorm can be seen as a duality of a t-norm that
similarly defines a function ⊥with the same domain and range ⊥: [0,1]×[0,1]→[0,1]. T-conorms
use the continuous function ⊥to generalize disjunction to fuzzy logic. The function ⊥satisfies the same
commutativity, associativity, and monotonicity properties as ⊤, but with 0 as the identity element, i.e.,
⊥(x,0) =x.
61Published in Transactions on Machine Learning Research (11/2024)
There exist many triangular norms, conorms, and fuzzy negations (Klement et al., 2013; van Krieken et al.,
2022) that stem from corresponding logical formalisms, e.g., (1)Gödel logic defines t-norm:⊤min(x,y) =
min(x,y), t-conorm:⊥max(x,y) = max(x,y); (2)Product logic with t-norm:⊤prod(x,y) =x·y, t-
conorm:⊥prod(x,y) =x+y−x·y; (3) in the Łukasiewicz logic t-norm:⊤Łuk(x,y) = max(x+y−1,0),
t-conorm:⊥Łuk(x,y) = min(x+y,1). Using fuzzy negation, N(x) = 1−x, one can verify that
⊥(x,y) =N(⊤(N(x),N(y)))(De Morgan’s laws) naturally obtaining a pair of (⊤,⊥).
A.5 Graph Representation Learning
Graph Representation Learning (GRL) is a subfield of machine learning aiming at learning low-dimensional
vector representations of graphs or their elements such as single nodes (Hamilton, 2020). For example,
hv∈Rddenotes ad-dimensional vector associated with a node v. Conceptually, we want nodes that share
certain structural or semantic features in the graph to have similar vector representations (where similarity
is often measured by a distance function).
Shallow Embeddings The first GRL approaches focused on learning shallow node embeddings, that is,
learning a unique vector per node directly used in the optimization task. For homogeneous (single-relation)
graphs, DeepWalk (Perozzi et al., 2014) and node2vec (Grover & Leskovec, 2016) trained node embeddings
on the task of predicting walks in the graph whereas in multi-relational graphs TransE (Bordes et al., 2013)
trained node and edge type embeddings in the autoencoder fashion by reconstructing the adjacency matrix.
Graph Neural Networks The idea of graph neural networks (GNNs) (Scarselli et al., 2008) implies
learninganetworkencoderwithsharedparametersontopofgiven(orlearnable)nodefeaturesbyperforming
neighborhoodaggregation. Thisframeworkcanbegeneralizedto message passing (Gilmeretal., 2017)where
at each layer ta nodevreceives messages from its neighbors (possibly adding edge and graph features),
aggregates the messages in a permutation-invariant way, and updates the representation:
h(t)
v=Update/parenleftig
h(t−1)
v,Aggregate u∈N(v)/parenleftbig
Message (h(t−1)
v,h(t−1)
u,euv))/parenrightbig/parenrightig
Here, huis a feature of the neighboring node u,euvis the edge feature, the Message function builds a
message from node uto nodevand can be parameterized with a neural network. As the set of neighbors
N(v)is unordered, Aggregate is often a permutation-invariant function like sumormean. The Update
function takes the previous node state and aggregated messages of the neighbors to produce the final state
of the node vat layertand can be parameterized with a neural network as well.
Classical GNN architectures like GCN (Kipf & Welling, 2017), GAT (Velickovic et al., 2018), and GIN (Xu
et al., 2019) were designed to work with homogeneous, single-relation graphs. Later, several works have
developed GNN architectures that work on heterogeneous graphs with multiple relations (Schlichtkrull et al.,
2018; Vashishth et al., 2020; Zhu et al., 2021). GNNs and message passing paved the way for Geometric Deep
Learning (Bronstein et al., 2021) that leverages symmetries and invariances in the input data as inductive
bias for building deep learning models.
B CLQA vs KGQA
KG-based question answering (KGQA) (Chakraborty et al., 2021) is the adjacent sub-field tackling natural
language questions and multi-hop reasoning over structured data. The input of a typical KGQA model is a
question posed in natural language, and, given a background graph, the output is a set of possible answer
entities. While CLQA and KGQA are similar in terms of tackling complex multi-hop questions (and perhaps
spanning over several modalities), KGQA approaches are rather different for the following reasons:
•Most often, the graphs in KGQA tasks are assumed to be complete and do not require predicting
any missing links at inference time. This assumption significantly simplifies the prediction task and
62Published in Transactions on Machine Learning Research (11/2024)
enables two main branches of KGQA models, i.e., (1) semantic parsing from a natural language
question to a SPARQL query (Keysers et al., 2020) where the main challenge is compositional
generalization to unseen combinations of relations; (2) retrieval-based LLM approaches (Yasunaga
et al., 2021; Baek et al., 2023; He et al., 2024) where the main challenge is to maximize the recall
ofkextracted candidate subgraphs and re-rank those to predict the answers. Some approaches
that support predicting missing links on the fly rely on pre-trained shallow KG embeddings (Saxena
et al., 2020) which, essentially, perform a 1prelation projection step. As retrieval-based techniques
are better amenable to other modalities, extending KGQA to support images and scene graphs is
also possible (Wang et al., 2023c).
•The majority of KGQA benchmarks include only projection (such as 2p) and intersection queries
(2i / 3i) thus missing out on unions and negations.
•Since the query is posed in natural language, KGQA pipelines often rely on entity linking modules
tailored to a specific graph (to map a string to an entity ID in the graph). Besides being an external
component with possible error propagation, it hampers generalization capabilities of the models,
e.g., a KGQA pipeline with a tailored entity linker cannot generalize to unseen graphs. In contrast,
the language-agnostic nature of CLQA allows to train graph reasoning models that can generalize
to complex queries over completely unseen KGs at inference time (Galkin et al., 2024).
We believe that the synergy between CLQA and KGQA is achievable by combining the best from the both
worlds,e.g., inductive and generalizable reasoning over complex queries while predicting missing links (from
CLQA), and deriving logical forms from the questions along with efficient retrieval mechanisms for faster
scoring of relation projections (from KGQA).
63