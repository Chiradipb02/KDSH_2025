Directional Smoothness and Gradient Methods:
Convergence and Adaptivity
Aaron Mishkin∗
Stanford University
amishkin@cs.stanford.eduAhmed Khaled∗
Princeton University
ahmed.khaled@princeton.edu
Yuanhao Wang
Princeton University
yuanhaoa@princeton.eduAaron Defazio
FAIR, Meta AI
adefazio@meta.comRobert M. Gower
CCM, Flatiron Institute
gowerrobert@gmail.com
Abstract
We develop new sub-optimality bounds for gradient descent (GD) that depend on
the conditioning of the objective along the path of optimization rather than on global,
worst-case constants. Key to our proofs is directional smoothness, a measure of
gradient variation that we use to develop upper-bounds on the objective. Minimizing
these upper-bounds requires solving implicit equations to obtain a sequence of
strongly adapted step-sizes; we show that these equations are straightforward to
solve for convex quadratics and lead to new guarantees for two classical step-sizes.
For general functions, we prove that the Polyak step-size and normalized GD
obtain fast, path-dependent rates despite using no knowledge of the directional
smoothness. Experiments on logistic regression show our convergence guarantees
are tighter than the classical theory based on L-smoothness.
1 Introduction
Gradient methods for differentiable functions are typically analyzed under the assumption that fis
L-smooth, meaning ∇fisL-Lipschitz continuous. This condition implies fis upper-bounded by a
quadratic and guarantees that gradient descent (GD) with step-size η <2/Ldecreases the optimality
gap at each iteration (Bertsekas, 1997). However, experience shows that GD can still decrease the
objective when fis not L-smooth, particularly for deep neural networks (Bengio, 2012; Z. Li et al.,
2020; J. Cohen et al., 2021). Even for functions verifying smoothness, convergence rates are often
pessimistic and fail to predict optimization speed in practice (Paquette et al., 2023).
One alternative to global smoothness is local Lipschitz continuity of the gradient (“local smoothness”).
Local smoothness assumes different Lipschitz constants hold for different neighbourhoods, which
avoids global assumptions and improves rates. However, such analyses typically rely on boundedness
of the iterates and then use local smoothness to obtain L-smoothness over a compact set (Malitsky
and Mishchenko, 2020). Boundedness is guaranteed in several ways: Junyu Zhang and Hong (2020)
break optimization into stages, Patel and Berahas (2022) use stopping-times, and Lu and S. Mei
(2023) employ a line-search. Unfortunately, these approaches modify the underlying optimization
algorithm, require local smoothness oracles (Park et al., 2021), or rely on highly complex arguments.
In contrast, we prove simple rates for GD without global smoothness by deriving bounds of the form,
f(xk+1)≤f(xk) +⟨∇f(xk), xk+1−xk⟩+M(xk+1, xk)
2∥xk+1−xk∥2
2, (1)
∗Equal contribution.
38th Conference on Neural Information Processing Systems (NeurIPS 2024).50 100 150 200
Iterations10−2100102Optimality Gap
ionosphere
10 20 30 40
Iterations10−1010−610−2
mammographic
1/M(xk+1,xk) Polyak Bound (L-Smooth) Bound ( 1/M(xk+1,xk)) Bound (Polyak)Figure 1: Comparison of actual (solid lines) and theoretical (dashed lines) convergence rates for GD
with (i) step-sizes strongly adapted to the directional smoothness ( ηk= 1/M(xk+1, xk)) and (ii) the
Polyak step-size. Both problems are logistic regressions on UCI repository datasets (Asuncion and
Newman, 2007). Our bounds using directional smoothness are tighter than those based on global L-
smoothness of fand adapt to the optimization path. For example, on mammographic our theoretical
rate for the Polyak step-size concentrates rapidly exactly when the optimizer shows fast convergence.
where the directional smoothness function M(xk+1, xk)depends only on properties of falong the
chord between xkandxk+1. Our sub-optimality bounds provide a path-dependent perspective on GD
and are tighter than conventional analyses when the step-size sequence is adapted to the directional
smoothness, meaning ηk<2/M(xk+1, xk). See Figure 1 for two real-data examples highlighting
our improvement over classical rates. We summarize all our contributions as follows.
Directional Smoothness. We introduce three constructive directional smoothness functions M(x, y).
The first, point-wise smoothness, depends only on the end-points x, y and is easily computed,
while the second, path-wise smoothness, yields a tighter bound, but depends on the chord C=
{αx+ (1−α)y:α∈[0,1]}. The last function, which we call the optimal point-wise smoothness,
is both easy-to-evaluate and provides the tightest possible quadratic upper bound.
Sub-optimality bounds. We leverage directional smoothness functions to prove new sub-optimality
bounds for GD on convex functions. Our bounds are localized to the GD trajectory, hold for any
step-size sequence, and are tighter than the classic analysis using L-smoothness. They are also more
general since we do not need to assume that fis globally L-smooth to show progress; all we require
is a sequence of step-sizes adapted to the directional smoothness function. Furthermore, our approach
extends naturally to acceleration, allowing us to prove optimal rates for (strongly)-convex functions.
Adaptive Step-Sizes in the Quadratic Case. In the general setting, computing step-sizes which are
adapted to the directional smoothness requires solving a challenging non-linear root-finding problem.
For quadratic problems, we show that the ideal step-size that satisfies ηk= 1/M(xk+1, xk)is the
Rayleigh quotient and is connected to the hedging algorithm (Altschuler and Parrilo, 2023).
Exponential Search. Moving beyond quadratics, we prove that the equation ηk= 1/M(xk+1, xk)
admits a solution under mild conditions, meaning ideal step-sizes can be computed using Newton’s
method. Since computing these step-sizes is typically impractical, we adapt exponential search
(Carmon and Hinder, 2022) to obtain similar path-dependent complexities up to a log-log penalty.
Polyak and Normalized GD. More importantly, we show that the Polyak step-size (Polyak, 1987) and
normalized GD achieve fast, path-dependent rates without knowledge of the directional smoothness.
Our analysis reveals that the Polyak step-size adapts to anydirectional smoothness to obtain the
tightest possible convergence rate. This property is not shared by constant step-size GD and may
explain the superiority of the Polyak step-size in many practical settings.
1.1 Additional Related Work
Directional smoothness is a relaxation of non-uniform smoothness (J. Mei et al., 2021), which
restricts the smoothness function Mto depend only on x, the origin point. J. Mei et al. (2021)
leverage non-uniform smoothness and a non-uniform Łojasiewicz inequality to break lower-bounds
for first-order optimization. Similarly, Berahas et al. (2023) show that a weak local smoothness oracle
can break lower bounds for gradient methods. A major advantage of our work over such oracle-based
approaches is that we construct explicit directional smoothness functions that are easy to evaluate.
2Similar to non-uniform smoothness, Grimmer (2019) and Orabona (2023) consider Hölder-type
growth conditions with constants that depend on a neighbourhood of x. Since directional smoothness
is stronger than and implies these Hölder error bounds, our Mfunctions can be leveraged to make
their results fully explicit (the Hölder bounds are non-constructive). Finally, while they also analyze
normalized GD, our rates are anytime and do not use online-to-batch reductions like Orabona (2023).
Directional smoothness is also related to (L0, L1)-smoothness (Jingzhao Zhang et al., 2020; B. Zhang
et al., 2020), which can be interpreted as a directional smoothness function with exponential depen-
dence on the distance between xandy. The extension of (L0, L1)-smoothness to (r, l)-smoothness
by H. Li et al. (2023) shows how to bound sequences of such directional smoothness functions, even
for accelerated methods. These approaches are complementary to ours and showcase a setting where
directional smoothness leads to concrete convergence rates.
Our work is most closely connected to that by Malitsky and Mishchenko (2020), who use a smoothed
version of M(x, y)to set the step-size. Vladarean et al. (2021) apply a similar smoothed step-size
scheme to primal-dual hybrid gradient methods, while Zhao and Huang (2024) relate directional
smoothness to Barzilai-Borwein updates (Barzilai and Borwein, 1988) and Vainsencher et al. (2015)
use local smoothness over neighbourhoods of the global minimizer to set the step-size for SVRG.
Finally, we note that adaptivity to directional smoothness is different from adaptivity to the sequence
of observed gradients obtained by methods such as Adagrad (Duchi et al., 2010; Streeter and
McMahan, 2010). Adagrad and its variants are most useful when the gradients are bounded, such as
in Lipschitz optimization, although they can also be used to obtain rates for smooth functions (Levy,
2017). We do not address adaptivity to gradients in this work.
2 Directional Smoothness
We say that a convex function fisL-smooth if for all x, y∈Rd,
f(y)≤f(x) +⟨∇f(x), y−x⟩+L
2∥y−x∥2
2. (2)
Minimizing this quadratic upper bound in ygives the classical GD update with step-size ηk= 1/L.
However, this viewpoint leads to rates which depend on the global, worst-case growth of f. This is
both counter-intuitive and undesirable because the iterates of GD, xk+1=xk−ηk∇f(xk),depend
only on local properties of f. Ideally, the analysis should also depend only on the local conditioning
along the path {x1, x2, . . .}. Towards this end, we generalize the smoothness upper-bound as follows.
Definition 2.1. We call M:Rd,d→R+adirectional smoothness function forfif for all x, y∈Rd,
f(y)≤f(x)+⟨∇f(x), y−x⟩+M(x, y)
2∥y−x∥2. (3)
If a function is L-smooth, then M(x, y) =Lis a trivial choice of directional smoothness function. In
the rest of this section, we construct different Mfunctions that provide tighter bounds on fwhile
still being possible to evaluate. The first is the point-wise directional smoothness ,
D(x, y) :=2∥∇f(y)− ∇f(x)∥2
∥y−x∥2. (4)
Point-wise smoothness is a directional estimate of Land satisfies D(x, y)≤2L. Indeed, Lcan be
equivalently defined as the supremum of D(x, y)/2over the domain of f(Beck, 2017). If fis convex
and differentiable, then D(x, y)is a directional smoothness function according to Definition 2.1.
Lemma 2.2. Iffis convex and differentiable, then the point-wise directional smoothness satisfies,
f(y)≤f(x) +⟨∇f(x), y−x⟩+D(x, y)
2∥y−x∥2
2. (5)
See Appendix A (we defer all proofs to the relevant appendices). In the worst-case, the point-wise
directional smoothness Dis weaker than the standard upper-bound M(x, y) =Lby a factor of two.
This is notan artifact of the analysis and is generally unavoidable, as the next proposition shows.
Proposition 2.3. There exists a convex, differentiable fandx, y∈Rdsuch that if t <2, then
f(y)> f(x) +⟨∇f(x), y−x⟩+t∥∇f(x)− ∇f(y)∥
2∥y−x∥2∥y−x∥2
2. (6)
3xk
xk+1x∗f(x)Actual ProgressL-SmoothMk-Smooth
Figure 2: Illustration of GD with ηk= 1/L. Even though this step-size exactly minimizes the
upper-bound from L-smoothness, Mkdirectional smoothness better predicts the progress of the
gradient step because Mk≪L. Our rates improve on L-smoothness because of this tighter bound.
While the point-wise smoothness is easy to compute, this additional factor of two can make Equa-
tion (5) looser than L-smoothness — on isotropic quadratics, for example. As an alternative, we
define the path-wise directional smoothness ,
A(x, y) := sup
t∈[0,1]⟨∇f(x+t(y−x))−∇f(x), y−x⟩
t∥y−x∥2, (7)
and show it verifies the quadratic upper-bound and satisfies Definition 2.1 even without convexity.
Lemma 2.4. For any differentiable function f, the path-wise smoothness (7)satisfies
f(y)≤f(x) +⟨∇f(x), y−x⟩+A(x, y)
2∥y−x∥2
2. (8)
Path smoothness is tighter than point-wise smoothness since A(x, y)≤D(x, y), but hard to compute
because it depends on the chord between xandy. That is, it depends on the properties of fon the line
{tx+ (1−t)y:t∈[0,1]}rather than solely on the points xandylike the point-wise smoothness.
Point-wise and path-wise smoothness are constructive, but they may not yield the tightest bounds
in all situations. The tightest directional smoothness function, which we call the optimal point-wise
smoothness , is the smallest number for which the quadratic upper bound holds,
H(x, y) =|f(y)−f(x)− ⟨∇ f(x), y−x⟩|
1
2∥y−x∥2(9)
By definition, His the tightest possible directional smoothness function; it lower bounds any constant
Cthat satisfies the quadratic bound (2). Thus, H(x, y)≤M(x, y)for any smoothness function M.
The directional smoothness functions introduced in this section represent different trade-offs between
computability and tightness. The optimal point-wise smoothness H(x, y)requires access to both
the function and gradient values, whereas the point-wise directional-smoothness D(x, y)requires
only access to the gradients and convexity. In contrast, the path-wise direction smoothness A(x, y)
satisfies Lemma 2.4 with or without convexity, but may be hard to evaluate.
3 Path-Dependent Sub-Optimality Bounds
Using directional smoothness, we obtain a descent lemma which depends only on local geometry,
f(xk+1)≤f(xk)−
ηk−η2
kM(xk, xk+1)
2
∥∇f(xk)∥2
2. (10)
See Lemma A.1. If ηk<2/M(xk, xk+1), then GD is guaranteed to decrease the function value and
we call ηkadapted toM(xk, xk+1). However, computing adapted step-sizes is not always straight-
forward. For instance, finding ηk= 1/M(xk, xk+1(ηk))requires solving a non-linear equation.
The rest of this section leverages directional smoothness to derive new guarantees for GD with arbi-
trary step-sizes. We emphasize that these results are sub-optimality bounds , rather than convergence
rates; a sequence of adapted step-sizes is required to convert our propositions into a convergence
theory. As a trade-off, our bounds reflect the locality of GD, rather than treating it as a global method.
4We start with the case when fhas lower curvature. Instead of using strong convexity or the PL-
condition (Karimi et al., 2016), we propose the directional strong convexity constant,
µ(x, y)= inf
t∈[0,1]⟨∇f(x+t(y−x))−∇f(x), y−x⟩
t∥y−x∥2
2. (11)
Iffis convex, then µ(x, y)≥0and it verifies the standard lower-bound from strong convexity,
f(y)≥f(x) +⟨∇f(x), y−x⟩+µ(x, y)
2∥y−x∥2
2. (12)
Moreover, we have µ(x, y)≥µwhen fisµ–strongly convex. We prove two bounds for convex
functions using directional strong convexity. For brevity, we denote Mi:=M(xi, xi+1),µi:=
µi(xi, x∗),δi=f(xi)−f(x∗), and ∆i=∥xi−x∗∥2
2, where x∗is a minimizer of f.
Proposition 3.1. Iffis convex and differentiable, then GD with step-size sequence {ηk}satisfies,
δk≤"Y
i∈G(1 +ηiλiµi)#
δ0+X
i∈B
Y
j>i,j∈G(1 +ηjλjµj)
ηiλi
2∥∇f(xi)∥2
2, (13)
where λi=ηiMi−2,G={i:ηi<2/Mi}, andB= [k]\G.
The analysis splits iterations into good steps G, where ηkis adapted to the directional smoothness,
and bad steps B, where the step-size is too large and GD may increase the optimality gap. When fis
L-smooth and µ-strongly convex, using the step-size sequence ηk= 1/Lgives
f(xk+1)−f(x∗)≤"kY
i=0
1−µi(2−Mi/L)
L#
(f(x0)−f(x∗)) (14)
where µi(2−Mi/L)≥µ. Thus, Equation (13) gives at least as tight a rate as standard assumptions
by localizing to the convergence path using anydirectional smoothness M. When Mi< L, the gap
in constants yields a strictly improved rate (see Figure 2). We also prove a more elegant bound.
Proposition 3.2. Iffis convex and differentiable, then GD with step-size sequence {ηk}satisfies,
∆k≤"kY
i=0|1−µiηi|
1 +µi+1ηi#
∆0+kX
i=0
Y
j>i|1−µjηj|
1 +µj+1ηj
 
Miη3
i−η2
i
1 +µi+1ηi∥∇f(xi)∥2
2. (15)
Unlike Proposition 3.1, this analysis shows linear progress at each iteration and does not divide k
into good steps and bad steps. In exchange, the second term in Equation (15) reflects how much
convergence is degraded when ηkis not adapted to the directional smoothness function M. We
conclude this section with a bound for when there is no lower curvature, meaning µi= 0.
Proposition 3.3. Letxk=Pk
i=0ηixi+1/Pk
i=0ηi. Iffis convex and differentiable, then GD satisfies,
f(xk)−f(x∗)≤∥x0−x∗∥2
2
2Pk
i=0ηi+Pk
i=0η2
i(ηiMi−1)∥∇f(xi)∥2
2
2Pk
i=0ηi. (16)
Eq.(16) is faster than standard analyses whenever Mi< L; it will be a key tool in the next sections.
3.1 Path-Dependent Acceleration
Now we show that directional smoothness can also be used to derive path-dependent sub-optimality
bounds for accelerated algorithms — that is, methods obtaining optimal rates for smooth, convex
optimization. In particular, we study Nesterov’s accelerated gradient descent (AGD) (Nesterov, 1983)
and prove that directional smoothness leads to tighter rates given adapted step-sizes. Throughout this
section we assume that fisµ-strongly convex with µ= 0when fis merely convex.
Although our analysis uses estimating sequences (Nesterov et al., 2018), we state AGD in the
following “momentum” formulation, where ykis the momentum and αkthe momentum parameter,
xk+1=yk−ηk∇f(yk)
α2
k+1= (1−αk+1)α2
kηk+1
ηk+ηk+1αk+1µ
yk+1=xk+1+αk(1−αk)
α2
k+αk+1(xk+1−xk).(17)
5Ifηk≤1/M(xk, xk+1), then Equation (10) combined with 1−ηkM(xk, xk+1)/2≤1/2implies,
f(xk+1)≤f(xk)−ηk
2∥∇f(yk)∥2
2. (18)
Our analysis leverages the fact that this descent condition for xk+1is the only connection between
the smoothness of fand the convergence rate of AGD. Since Equation (18) depends only on the
step-size ηk, we can replace Lwithin the analysis of AGD with a sequence of adapted step-sizes. The
following theorem controls the effect of these step-sizes to obtain path-dependent bounds.
Theorem 3.4. Suppose fis differentiable, µ–strongly convex and AGD is run with adapted step-sizes
ηk≤1/Mk. Ifµ >0andα0=√η0µ, then AGD obtains the following accelerated rate:
f(xk+1)−f(x∗)≤kY
i=0(1−√µηi)h
f(x0)−f(x∗) +µ
2∥x0−x∗∥2
2i
. (19)
Letηmin= min i∈[k]ηi. Ifµ≥0andα0∈(√µη0, c), where cis the maximum value of α0for which
γ0=α2
0−η0α0µ
η0(1−α0)satisfies γ0<3/ηmin+µ, then AGD obtains the following rate:
f(xk+1)−f(x∗)≤4
ηmin(γ0−µ)(k+ 1)2h
f(x0)−f(x∗) +γ0
2∥x0−x∗∥2
2i
. (20)
Ifηk= 1/Mk>1/L, then these rates are strictly faster than those obtained under L-smoothness
and Theorem 3.4 shows that AGD provably benefits from taking the largest possible steps given the
local geometry of f. However, obtaining accelerated rates when µ= 0requires prior knowledge of
the minimum step-size; while this is straightforward for L-smooth functions, it is not clear how to
extend such result to non-strongly convex acceleration with locally Lipschitz gradients. For example,
while H. Li et al. (2023) show that the (r, l)-smoothness (a valid directional smoothness function) is
bounded over the iterate trajectory, their rate does not adapt to the optimization path.
4 Adaptive Learning Rates
Converting our sub-optimality bounds into convergence rates requires adapted step-sizes satisfying
ηk<2/M(xk, xk+1). Given an adapted step-size, the directional descent lemma (Equation (10))
implies GD decreases fand we can obtain fast rates if the step-sizes are bounded below. However,
xk+1is itself a function of ηk, meaning adapted step-sizes are not straightforward to compute.
ForL-smooth f, the different directional smoothness functions Mintroduced in Section 2 satisfy
M(xk, xk+1)≤2L. This implies ηk<1
Lis trivially adapted. As such step-sizes don’t capture local
properties of f, we introduce the notion of strongly adapted step-sizes , which satisfy
ηk= 1/M(xk+1(ηk), xk). (21)
Equation (10) implies GD with a strongly adapted step-size makes guaranteed progress as,
f(xk+1)≤f(xk)−[2M(xk+1, xk)]−1∥∇f(xk)∥2
2. (22)
This progress is greater than that guaranteed by L-smoothness when M(xk, xk+1)< L and holds
even when fis not L-smooth. However, it is not clear a priori if (i) strongly adapted step-sizes exist
or if (ii) any iterative method achieves the progress in Eq. (21). Surprisingly, we provide a positive
answer to both questions. Strongly adapted ηkare computable and we also prove GD with the Polyak
step-size adapts to any choice of directional smoothness, including the optimal point-wise smoothness.
Before presenting this strong result, we consider the illustrative case of quadratic minimization.
4.1 Adaptivity in Quadratics
Now we show that step-sizes adapted to both the point-wise smoothness Mand the path-wise
smoothness Aexist when fis quadratic. Let f(x) =x⊤Bx/2−c⊤x,where Bis positive semi-
definite. Assuming {ηk}is strongly adapted to the directional smoothness, Equation (16) implies
f(xk)−f(x∗)≤∥x0−x∗∥2
2
2Pk
i=0ηi=∥x0−x∗∥2
2
2Pk
i=01
M(xi,xi+1)≤∥x0−x∗∥2
2
2(k+ 1)Pk
i=0M(xi, xi+1)
k+ 1,(23)
6101103
Iteration104105106Optimality Gap
101103
Iteration102107Point-Wise Smoothness
101103
Iteration103
2×103
3×103
4×103
Adapted Step-Sizes
1/L 1/Dk1/Ak2/LFigure 3: Performance of GD with different step-size rules for a synthetic quadratic problem. We
run GD for 20,000 steps on 20 random quadratic problems with L= 1000 and Hessian skew. Left-
to-right, the first plot shows the optimality gap f(xk)−f(x∗), the second shows the point-wise
directional smoothness D(xk, xk+1), and the third shows step-sizes used by the different methods.
where we used ηiMi= 1as well as Jensen’s inequality. This guarantee depends solely on the average
directional smoothness along the optimization trajectory {x0, x1, . . .}. When fis quadratic, we can
exactly compute these smoothness constants. In particular, the point-wise directional smoothness is,
D(xi, xi+1) = 2∥B∇f(xi)∥2/∥∇f(xi)∥2.
Notably, D(xi, xi+1)has no dependence on xi+1and the corresponding strongly adapted step-size is
given by ηi=∥∇f(xi)∥2/(2∥B∇f(xi)∥2)— see Lemma C.1. Remarkably, this expression recovers
the step-size proposed by Dai and Yang (2006), who show it approximates the Cauchy step-size
and converges to the “edge-of-stability” (J. Cohen et al., 2021) at 2/Lask→ ∞ . Combining this
simple expression with Equation (23) gives a fast, non-asymptotic convergence rate for GD and new
theoretical justification for their work.
We can also compute the path-wise directional smoothness in closed form. As Lemma C.2 shows,
A(xi, xi+1) =∇f(xi)⊤B∇f(xi)/∇f(x)⊤∇f(x),
andηi=∇f(xi)⊤∇f(xi)/[∇f(xi)⊤B∇f(xi)]is the well-known Cauchy step-size. Path-wise
directional smoothness thus provides another interpretation (and convergence guarantee) for the
Cauchy step-size, which is traditionally derived by minimizing f(x−η∇f(x))inη.
4.2 Adaptivity for Convex Functions
In the last subsection, we proved that strongly adapted step-sizes for the point-wise and path-wise
directional smoothness functions have closed-form expressions when fis quadratic. Moreover,
these step-sizes recover two classical schemes from the optimization literature, giving them new
justification and fast convergence rates. Now we consider the existence of strongly adapted step-sizes
for general convex functions. Our first result gives simple conditions for Equation (21) to have at
least one solution when Mis the point-wise directional smoothness.
Proposition 4.1. Iffis convex and continuously differentiable, then either (i) fis minimized along
the ray x(η) =x−η∇f(x)or (ii) there exists η >0satisfying η= 1/D(x, x−η∇f(x)).
The next proposition uses a similar argument with slightly stronger conditions to show existence of
strongly adapted step-sizes for the path-wise smoothness.
Proposition 4.2. Iffis convex and twice continuously differentiable, then either (i) fis minimized
along the ray x(η) =x−η∇f(x)or (ii) there exists η >0satisfying η= 1/A(x, x−η∇f(x)).
Propositions 4.1 and 4.2 do not assume the global smoothness; although neither proof is constructive,
it is possible to compute strongly adapted step-sizes for the point-wise directional smoothness using
root-finding methods. We show in Section 5 that if fis twice differentiable, then strongly adapted
step-sizes can be found via Newton’s method using only Hessian-vector products, ∇2f(x)∇f(x).
4.2.1 Exponential Search
Now we show that the exponential search algorithm developed by Carmon and Hinder (2022) can
be used to find step-sizes that adapt on average to the directional smoothness. Consider a fixed
7optimization horizon kand denote by xi(η)the sequence of iterates obtained by running GD from x0
using a fixed step-size η. Define the criterion function,
ψ(η) =Pk
i=0∥∇f(xi(η))∥2
2Pk
i=0M(xi(η), xi+1(η))∥∇f(xi(η))∥2
2, (24)
and suppose that we have a step-size ηthat satisfies ψ(η)/2≤η≤ψ(η). Using these bounds in
Proposition 3.3 yields the following convergence rate,
f(xk)−f∗≤h
kPk
i=0M(xi,xi+1)∥∇f(xi)∥2
2Pk
i=0∥∇f(xi)∥2
2i
∥x0−x∗∥2
2. (25)
While ηdoes not adapt to each directional smoothness M(xi, xi+1)along the path, it adapts to a
weighted average of the directional smoothness constants, where the weights are the observed squared
gradient norms. This is always smaller than the maximum directional smoothness along the trajectory
and can be much smaller than the global smoothness. Furthermore, we have reduced our problem to
finding η∈[ψ(η)/2, ψ(η)], which is similar to the problem Carmon and Hinder (2022) solve with
exponential search. We adopt their approach as Algorithm 1 and give a convergence guarantee.
Theorem 4.3. Assume fis convex and L-smooth. Then Algorithm 1 with η0>0requires at most
2K(log log(2 η0/L)∨1)iterations of GD and in the last run it outputs a step-size ηand point
xK=1
KPK−1
i=0xi(η)such that exactly one of the following holds:
Case 1: η=η0and f(xK)−f(x∗)≤∥x0−x∗∥2
2
2Kη0
Case 2: η̸=η0and f(xK)−f∗≤∥x0−x∗∥2
2
2K"Pk
i=0Mi∥∇f(x′
i)∥2
2Pk
i=0∥∇f(x′
i)∥2
2#
,
where Midef=M(x′
i, x′
i+1)andx′
iare the iterates generated by GD with step-size η′∈[η,2η].
Theorem 4.3 requires fto be L-smooth, but has only a log log dependence on the global smoothness
constant. Moreover, the rate scales with the weighted average of smoothness constants along a very
close trajectory {x′
1, x′
2, . . .}. In the next section, we give convergence bounds that depend on the
unweighted average of the directional smoothness constants along the actual optimization trajectory.
4.2.2 Polyak’s Step-Size Rule
Our theory so-far suggests using strongly adapted step-sizes, but neither root-finding nor exponential
search are practical methods for large-scale optimization. Thus, we now consider other step-size
selection rules which may leverage directional smoothness. In particular, the Polyak step-size sets,
ηk=γ(f(xk)−f(x∗))/∥∇f(xk)∥2
2, (26)
for some γ >0, which is optimal for smooth and non-smooth optimization (Hazan and Kakade, 2019)
given knowledge of f(w∗). Surprisingly, we show that GD with the Polyak step-size also achieves
the same guarantee as strongly adapted step-sizes without knowledge of the directional smoothness.
Theorem 4.4. Suppose that fis convex and differentiable and let Mbe any directional smoothness
function for f. Let∆0:=∥x0−x∗∥2
2. Then GD with the Polyak step-size and γ∈(1,2)satisfies
f(xk)−f(x∗)≤c(γ)∆0
2Pk−1
i=0M(xi, xi+1)−1, (27)
where c(γ) =γ/(2−γ)(γ−1)andxk=Pk−1
i=0
M(xi, xi+1)−1xi
/Pk−1
i=0M(xi, xi+1)−1
.
Theorem 4.4 measures sub-optimality at an average iterate obtained using the directional smoothness.
However, it also holds for the best iterate, ˆxk= arg mini∈[k]f(xi), meaning no knowledge of the
directional smoothness is required to obtain the guarantee. We prove an alternative guarantee for the
Polyak step-size in Theorem D.2, where the progress depends on the sum of step-sizes rather than
on the average directional smoothness. This shows that the step-size in Equation (26) can itself be
viewed as a measure of local smoothness, albeit without formal justification.
8Compared with the standard guarantee for the Polyak step-size under L-smoothness, f(xk)−f(x∗)≤
2L∆0/k(Hazan and Kakade, 2019), our analysis in Theorem 4.4 with the choice γ= 1.5yields
f(xk)−f(x∗)≤3∆0Pk−1
i=0M(xi, xi+1)−1≤3∆0
kPk−1
k=0M(xi, xi+1)
k,
where the second bound follows from Jensen’s inequality and shows that the convergence depends
on the average directional smoothness along the trajectory, rather than on L. IffisL-smooth, then
M(xk, xk+1)≤Limmediately recovers the classic rate for Polyak’s method up to a 3/2constant
factor. If fis not L-smooth, but M(xk, xk+1)is bounded, then Equation (27) generalizes the O(1/k)
rate proved concurrently by Takezawa et al. (2024), but for any choice of directional smoothness (of
which (L0, L1)-smoothness (Jingzhao Zhang et al., 2020) is but one).
Comparison with strongly adapted step-sizes. As we saw for quadratics, strongly adapted step-sizes
for any directional smoothness function allow us to obtain the following convergence rate,
f(xk)−f(x∗)≤∥x0−x∗∥2
2
2Pk−1
i=0M(xi, xi+1)−1.
This is matches the guarantee given by Equation (27) up to constant factors. As a result, we give a
positive answer to the question posed earlier in this section: GD with the Polyak step-size achieves
the same convergence for any smoothness function Mas GD with step-sizes strongly adapted to M.
Application to the optimal directional smoothness. Theorem 4.4 holds for every directional
smoothness function M. Therefore we can specialize Equation (27) with the optimal point-wise
directional smoothness H(as defined in Equation (4)) and γ= 1.5to get the guarantee,
min
i∈[k−1][f(xi)−f(x∗)]≤3∥x0−x∗∥2
2Pk−1
i=0H(xi, xi+1)−1. (28)
This rate requires computing the iterate with the minimum function value, but that is easy to track
during optimization. Unlike our previous results, Equation (28) requires no access to the optimal
point-wise smoothness, yet obtains a dependence on the tightest constant possible.
4.3 Normalized Gradient Descent
Now we change directions slightly and study normalized GD, whose convergence also depends on the
directional smoothness. Normalized GD uses step-sizes which are divided by the gradient magnitude,
xk+1=xk−ηk
∥∇f(xk)∥2∇f(xk). (29)
Our next theorem shows that normalized GD obtains a guarantee which depends solely on the average
of the point-wise directional smoothness Dk:=D(xk, xk+1)despite no explicit knowledge of Dk.
Theorem 4.5. Suppose that fis convex and differentiable. Let Dbe the point-wise directional
smoothness defined by Equation (4)and∆0:=∥x0−x∗∥2
2. Then normalized GD with a sequence of
non-increasing step-sizes ηksatisfies
f(ˆxk)−f(x∗)≤∆0+Pk−1
i=0η2
i
2k2f(x0)
η2
0−f(x∗)
η2
k−1
+∆0+Pk−1
i=0η2
i
2kk−1X
i=0M(xi, xi+1)
k,(30)
where ˆxk= arg min i∈[k−1]f(xi). Ifmax i∈[k−1]M(xi, xi+1)is bounded for all k(i.e.fisL-
smooth), then for ηi= 1/√
iwe have f(ˆxk)−f(x∗)∈ O(1/k)and for ηi= 1/√
iwe get the
anytime result f(ˆxk)−f(x∗)∈ O(log(k)/k).
Theorem 4.5 gives a rate for normalized GD which is valid for any convex fwithout any dependence
on global smoothness. However, does not adapt to any smoothness function like the Polyak step-size.
5 Experiments
We evaluate the practical improvement of our convergence rates over those using L-smoothness
on two logistic regression problems taken from the UCI repository (Asuncion and Newman, 2007).
90 100 200
Iterations10−310−210−1Optimality Gap
ionosphere
0 100 200
Iterations10−1410−1110−810−510−2
horse-colic
0 100 200
Iterations10−210−1
ozone
GD ( 1/L) GD ( 1/Dk) Polyak Norm. GD AdGDFigure 4: Comparison of GD with ηk= 1/L, step-sizes strongly adapted to the point-wise smoothness
(ηk= 1/D(xk, xk+1)), and the Polyak step-size against normalized GD (Norm. GD) and the AdGD
method on three logistic regression problems. AdGD uses a smoothed version of the point-wise
directional smoothness from the previous iteration to set ηk. We find that GD methods with adaptive
step-sizes consistently outperform GD with ηk= 1/Land even obtain a linear rate on horse-colic .
Figure 1 compares GD with strongly adapted step-sizes η= 1/Mk, where Mkis the point-wise
smoothness, against GD with the Polyak step-size. We also plot the exact convergence rates for each
method, Equation (16) and Equation (27), respectively, and compare against the classical guarantee
for both methods. Our convergence rates are an order of magnitude tighter on the ionosphere
dataset and display a remarkable ability to adapt to the path of optimization on mammographic .
Figure 3 compares the performance of GD with strongly adapted step-sizes and with the fixed step-
sizeηk= 1/Lfor a synthetic quadratic with Hessian skew (R. Pan et al., 2022). Results are averaged
over twenty random problems. We find that strongly adapted step-sizes lead to significantly faster
convergence. Since Ak, Dk≪L, the adapted step-sizes are larger than 2/L, especially at the start of
training; they eventually converge to 2/L, indicating these methods operate at the edge-of-stability
(J. Cohen et al., 2021; J. M. Cohen et al., 2022). This is consistent with Ahn et al. (2022) and Y . Pan
and Y . Li (2023), who show local smoothness is correlated with edge-of-stability behavior.
We conclude with a comparison of empirical convergence rates on three additional logistic regression
problems from the UCI repository. We compare GD with ηk= 1/L, GD with step-sizes strongly
adapted to the point-wise smoothness ( ηk= 1/Dk), GD with the Polyak step-size (Polyak), and
normalized GD (Norm. GD) against the AdGD method (Malitsky and Mishchenko, 2020). The Polyak
step-size performs best on every dataset but ozone , where GD with ηk= 1/Dksolves the problem to
high accuracy in just a few iterations. Thus, although Polyak step-sizes have the optimal dependence
on directional smoothness, computing strongly adapted step-sizes can still be advantageous.
6 Conclusion
We present new sub-optimality bounds for GD under novel measures of local gradient variation which
we call directional smoothness functions. Our results hold for any step-sizes, improve over standard
analyses when ηkis adapted to the choice of directional smoothness, and depend only on properties
offlocal to the optimization path. For convex quadratics, we show that computing step-sizes strongly
adapted to directional smoothness functions is straightforward and recovers two well-known step-size
schemes, including the Cauchy step-size. In the general case, we prove that an algorithm based on
exponential search gives a weighted-version of the path-dependent convergence rate with no need for
adapted step-sizes. We also show that GD with the Polyak step-size and normalized GD both obtain
fast rates with no dependence on the global smoothness parameter. Crucially, the Polyak step-size
adapts to any choice of directional smoothness, including the tightest possible parameter.
10Acknowledgements
Aaron Mishkin was supported by NSF Grant DGE-1656518, by NSERC Grant PGSD3-547242-2020,
and by an internship at the Center for Computational Mathematics, Flatiron Institute. We thank Si Yi
Meng for insightful discussions during the preparation of this work and Fabian Schaipp for use of the
step-back code. We also thank the anonymous reviewers for comments leading to improvements in
Proposition 3.2 and the addition of Theorem D.2.
References
Ahn, Kwangjun, Jingzhao Zhang, and Suvrit Sra (2022). “Understanding the unstable convergence of
gradient descent”. In: International Conference on Machine Learning, ICML 2022, 17-23 July 2022,
Baltimore, Maryland, USA . V ol. 162. Proceedings of Machine Learning Research, pp. 247–257.
Altschuler, Jason M. and Pablo A. Parrilo (2023). “Acceleration by Stepsize Hedging I: Multi-Step
Descent and the Silver Stepsize Schedule”. In: CoRR abs/2309.07879.
Asuncion, Arthur and David Newman (2007). UCI machine learning repository .
Barzilai, Jonathan and Jonathan M Borwein (1988). “Two-point step size gradient methods”. In: IMA
journal of numerical analysis 8.1, pp. 141–148.
Beck, Amir (2017). First-order methods in optimization . MOS-SIAM series on optimization. Philadel-
phia : Philadelphia: Society for Industrial and Applied Mathematics ; Mathematical Optimization
Society. ISBN : 978-161-197-4-9-9-7.
Bengio, Yoshua (2012). “Practical Recommendations for Gradient-Based Training of Deep Archi-
tectures”. In: Neural Networks: Tricks of the Trade - Second Edition . V ol. 7700. Lecture Notes in
Computer Science, pp. 437–478.
Berahas, Albert S., Lindon Roberts, and Fred Roosta (2023). “Non-Uniform Smoothness for Gradient
Descent”. In: arXiv preprint arXiv:2311.08615 abs/2311.08615.
Bertsekas, Dimitri P (1997). “Nonlinear programming”. In: Journal of the Operational Research
Society 48.3, pp. 334–334.
Bubeck, Sébastien et al. (2015). “Convex optimization: Algorithms and complexity”. In: Foundations
and Trends® in Machine Learning 8.3-4, pp. 231–357.
Carmon, Yair and Oliver Hinder (2022). “Making SGD Parameter-Free”. In: Conference on Learn-
ing Theory, 2-5 July 2022, London, UK . V ol. 178. Proceedings of Machine Learning Research,
pp. 2360–2389.
Cohen, Jeremy et al. (2021). “Gradient Descent on Neural Networks Typically Occurs at the Edge
of Stability”. In: 9th International Conference on Learning Representations, ICLR 2021, Virtual
Event, Austria, May 3-7, 2021 .
Cohen, Jeremy M. et al. (2022). “Adaptive Gradient Methods At the Edge of Stability”. In: arXiv
preprint arXiv:2207.14484 abs/2207.14484.
Dai, Y . H. and X. Q. Yang (2006). “A New Gradient Method with an Optimal Stepsize Property”. In:
Computational Optimization and Applications 33.1, pp. 73–88.
Duchi, John C., Elad Hazan, and Yoram Singer (2010). “Adaptive Subgradient Methods for Online
Learning and Stochastic Optimization”. In: COLT 2010 - The 23rd Conference on Learning Theory,
Haifa, Israel, June 27-29, 2010 , pp. 257–269.
Fernández-Delgado, Manuel et al. (2014). “Do we need hundreds of classifiers to solve real world
classification problems?” In: The journal of machine learning research 15.1, pp. 3133–3181.
Grimmer, Benjamin (2019). “Convergence Rates for Deterministic and Stochastic Subgradient
Methods without Lipschitz Continuity”. In: SIAM J. Optim. 29.2, pp. 1350–1365.
Hazan, Elad and Sham Kakade (2019). “Revisiting the Polyak step size”. In: arXiv preprint
arXiv:1905.00313 .
He, Kaiming et al. (2015). “Delving deep into rectifiers: Surpassing human-level performance on
imagenet classification”. In: Proceedings of the IEEE international conference on computer vision ,
pp. 1026–1034.
Hogan, William W (1973). “Point-to-set maps in mathematical programming”. In: SIAM review 15.3,
pp. 591–603.
Karimi, Hamed, Julie Nutini, and Mark Schmidt (2016). “Linear convergence of gradient and
proximal-gradient methods under the polyak-łojasiewicz condition”. In: Machine Learning and
Knowledge Discovery in Databases: European Conference, ECML PKDD 2016, Riva del Garda,
Italy, September 19-23, 2016, Proceedings, Part I 16 . Springer, pp. 795–811.
11Levy, Kfir Y . (2017). “Online to Offline Conversions, Universality and Adaptive Minibatch Sizes”. In:
Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information
Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA , pp. 1613–1622.
Li, Haochuan et al. (2023). “Convex and Non-convex Optimization Under Generalized Smoothness”.
In:Advances in Neural Information Processing Systems 36: Annual Conference on Neural In-
formation Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16,
2023 .
Li, Zhiyuan, Kaifeng Lyu, and Sanjeev Arora (2020). “Reconciling Modern Deep Learning with
Traditional Optimization Analyses: The Intrinsic Learning Rate”. In: Advances in Neural Informa-
tion Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020,
NeurIPS 2020, December 6-12, 2020, virtual .
Liu, Dong C and Jorge Nocedal (1989). “On the limited memory BFGS method for large scale
optimization”. In: Mathematical programming 45.1-3, pp. 503–528.
Lu, Zhaosong and Sanyou Mei (2023). “Accelerated first-order methods for convex optimization with
locally Lipschitz continuous gradient”. In: SIAM Journal on Optimization 33.3, pp. 2275–2310.
Malitsky, Yura and Konstantin Mishchenko (2020). “Adaptive Gradient Descent without Descent”.
In:Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18
July 2020, Virtual Event . V ol. 119. Proceedings of Machine Learning Research, pp. 6702–6712.
Mei, Jincheng et al. (2021). “Leveraging Non-uniformity in First-order Non-convex Optimization”.
In:Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24
July 2021, Virtual Event . V ol. 139. Proceedings of Machine Learning Research, pp. 7555–7564.
Mishkin, Aaron, Mert Pilanci, and Mark Schmidt (2024). “Faster Convergence of Stochastic Acceler-
ated Gradient Descent under Interpolation”. In: arXiv preprint arXiv:2404.02378 .
Nesterov, Yurii (1983). “A method for solving the convex programming problem with convergence
rate O (1/k2)”. In: Dokl akad nauk Sssr . V ol. 269, p. 543.
Nesterov, Yurii et al. (2018). Lectures on convex optimization . V ol. 137. Springer.
Orabona, Francesco (2023). “Normalized Gradients for All”. In: arXiv preprint arXiv:2308.05621
abs/2308.05621.
Pan, Rui, Haishan Ye, and Tong Zhang (2022). “Eigencurve: Optimal Learning Rate Schedule for
SGD on Quadratic Objectives with Skewed Hessian Spectrums”. In: ICLR .
Pan, Yan and Yuanzhi Li (2023). “Toward understanding why adam converges faster than sgd for
transformers”. In: arXiv preprint arXiv:2306.00204 .
Paquette, Courtney et al. (2023). “Halting time is predictable for large models: A universality property
and average-case analysis”. In: Foundations of Computational Mathematics 23.2, pp. 597–673.
Park, Jea-Hyun, Abner J Salgado, and Steven M Wise (2021). “Preconditioned accelerated gradient
descent methods for locally Lipschitz smooth objectives with applications to the solution of
nonlinear PDEs”. In: Journal of Scientific Computing 89.1, p. 17.
Paszke, Adam et al. (2019). “Pytorch: An imperative style, high-performance deep learning library”.
In:Advances in neural information processing systems 32.
Patel, Vivak and Albert S. Berahas (2022). “Gradient descent in the absence of global Lipschitz conti-
nuity of the gradients: Convergence, divergence and limitations of its continuous approximation”.
In:arXiv preprint arXiv:2210.02418 .
Polyak, Boris T (1987). “Introduction to optimization”. In.
Streeter, Matthew and H. Brendan McMahan (2010). “Less Regret Via Online Conditioning”. In:
arXiv preprint arXiv:1002.4862 .
Takezawa, Yuki et al. (2024). “Polyak Meets Parameter-free Clipped Gradient Descent”. In: CoRR
abs/2405.15010. DOI:10 . 48550 / ARXIV . 2405 . 15010 . arXiv: 2405 . 15010 .URL:https :
//doi.org/10.48550/arXiv.2405.15010 .
Vainsencher, Daniel, Han Liu, and Tong Zhang (2015). “Local Smoothness in Variance Reduced
Optimization”. In: Advances in Neural Information Processing Systems 28: Annual Conference on
Neural Information Processing Systems 2015, December 7-12, 2015, Montreal, Quebec, Canada ,
pp. 2179–2187.
Virtanen, Pauli et al. (2020). “SciPy 1.0: Fundamental Algorithms for Scientific Computing in
Python”. In: Nature Methods 17, pp. 261–272.
Vladarean, Maria-Luiza, Yura Malitsky, and V olkan Cevher (2021). “A first-order primal-dual method
with adaptivity to local smoothness”. In: Advances in Neural Information Processing Systems 34:
Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December
6-14, 2021, virtual , pp. 6171–6182.
12Zhang, Bohang et al. (2020). “Improved Analysis of Clipping Algorithms for Non-convex Optimiza-
tion”. In: Advances in Neural Information Processing Systems 33: Annual Conference on Neural
Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual .
Zhang, Jingzhao et al. (2020). “Why Gradient Clipping Accelerates Training: A Theoretical Justifica-
tion for Adaptivity”. In: 8th International Conference on Learning Representations, ICLR 2020,
Addis Ababa, Ethiopia, April 26-30, 2020 .
Zhang, Junyu and Mingyi Hong (2020). “First-order algorithms without Lipschitz gradient: A
sequential local optimization approach”. In: arXiv preprint arXiv:2010.03194 .
Zhao, Weijing and He Huang (2024). “Adaptive stepsize estimation based accelerated gradient
descent algorithm for fully complex-valued neural networks”. In: Expert Systems with Applications
236, p. 121166.
13A Proofs for Section 2
Lemma 2.2. Iffis convex and differentiable, then the point-wise directional smoothness satisfies,
f(y)≤f(x) +⟨∇f(x), y−x⟩+D(x, y)
2∥y−x∥2
2. (5)
Proof. By the convexity of fwe have
f(x) +⟨∇f(x), y−x⟩ ≤f(y).
Rearranging and then using Cauchy-Schwarz we get
f(x)≤f(y) +⟨∇f(x), x−y⟩
=f(y) +⟨∇f(y), x−y⟩+⟨∇f(x)− ∇f(y), x−y⟩
≤f(y) +⟨∇f(y), x−y⟩+∥∇f(x)− ∇f(y)∥∥x−y∥
=f(y) +⟨∇f(y), x−y⟩+D(x, y)
2∥x−y∥2.
Lemma 2.4. For any differentiable function f, the path-wise smoothness (7)satisfies
f(y)≤f(x) +⟨∇f(x), y−x⟩+A(x, y)
2∥y−x∥2
2. (8)
Proof. Starting from the fundamental theorem of calculus,
f(y)−f(x)− ⟨∇ f(x), y−x⟩=Z1
0⟨∇f(x+t(y−x))− ∇f(x), y−x⟩dt
≤Z1
0A(x, y)t∥x−y∥2
2dt
=A(x, y)
2∥y−x∥2
2.
which completes the proof.
Proposition 2.3. There exists a convex, differentiable fandx, y∈Rdsuch that if t <2, then
f(y)> f(x) +⟨∇f(x), y−x⟩+t∥∇f(x)− ∇f(y)∥
2∥y−x∥2∥y−x∥2
2. (6)
Proof. LetHfdenote the optimal pointwise directional smoothness associated with some convex
and differentiable function f:Rd→R(as defined in Equation (4)), and Dfdenote the pointwise
directional smoothness associated with f(as defined in Equation (4)). For any t, the statement of (6)
is equivalent to saying Hf> t∥∇f(x)−∇f(y)∥
∥x−y∥for all x, y∈Rdand convex, differentiable f. Observe
that Lemma 2.2 already shows that for all convex and differentiable functions f:Rd→R
Hf(x, y)≤Df(x, y) = 2∥∇f(x)− ∇f(y)∥
∥x−y∥
for all x, y∈Rd. In order to show that this is tight, we suppose by the way of contradiction that there
exists some 2> t≥0such that for all convex and differentiable functions f:Rd→R
Hf(x, y)≤t·∥∇f(x)− ∇f(y)∥
∥x−y∥(31)
for all x, y∈R. We shall show that no such texists by showing for each such tthere exists a function
ftsuch that Equation (31) does not hold.
Consider fϵ(x) =√
x2+ϵ2forϵ≤1. The function fis differentiable. Moreover
f′
ϵ(x) =x√
x2+ϵ2, f′′
ϵ(x) =ϵ2
(ϵ2+x2)3
2≥0.
14Therefore fis convex. Let g(x) =|x|. Fixx= 1andy= 0, we have
|g(x)−g(y)−sign(y)·(x−y)| ≤ |fϵ(x)−fϵ(y)−f′
ϵ(y)·(x−y)|+|g(x)−fϵ(x)|
+|g(y)−fϵ(y)|+|(f′
ϵ(y)−sign(y))·(x−y)|
=|fϵ(x)−fϵ(y)−f′
ϵ(y)·(x−y)|+1−p
1 +ϵ2
+0−√
ϵ2+|(0−0)·(1−0)|
≤ |fϵ(x)−fϵ(y)−f′
ϵ(y)·(x−y)|+ 2ϵ.
Now observe that
g(x)−g(y)−sign(y)·(x−y) =|1| − |0| −0·(1−0) = 1 .
Therefore
|fϵ(x)−fϵ(y)−f′
ϵ(y)·(x−y)| ≥1−2ϵ. (32)
By definition we have1
2∥x−y∥2=1
2, therefore
Hf(x, y) =|fϵ(x)−fϵ(y)−f′
ϵ(y)·(x−y)|
1
2∥x−y∥2≥2−4ϵ. (33)
But by our starting assumption we have that there exists some t < 2such that Hf(x, y)≤
t∥f′(x)−f′(y)∥
|x−y|for all differentiable and convex functions f. Applying this to f=fϵwe get
Hfϵ(1,0)≤t|f′
ϵ(1)−f′
ϵ(0)|
|1|=t·1√
1 +ϵ2≤t. (34)
Combining Equations (33) and (34) we have
2−4ϵ≤Hfϵ(1,0)≤t
Rearranging we get
2−t≤4ϵ
Choosing ϵ=2−t
8>0we get a contradiction. It follows that the minimal tsuch that H(x, y)≤
t|f′(x)−f′(y)|
|x−y|for all convex and differentiable fist= 2.
Lemma A.1. One step of gradient descent with step-size ηk>0makes progress as
f(xk+1)≤f(xk)−ηk
1−ηkM(xk, xk+1)
2
∥∇f(xk)∥2
2.
Proof. Starting from Equation (4), we have
f(xk+1)≤f(xk) +⟨∇f(xk), xk+1−xk⟩+M(xk, xk+1)
2∥xk+1−xk∥2
2
=f(xk)−ηk∥∇f(xk)∥2
2+η2
kM(xk, xk+1)
2∥∇f(xk)∥2
2
=f(xk)−ηk1−ηkM(xk, xk+1)
2
∥∇f(xk)∥2
2.
B Proofs for Section 3
Lemma B.1. Iffis convex, then for any x, y∈Rd,
f(y)≥f(x) +⟨∇f(x), y−x⟩+µ(x, y)
2∥y−x∥2
2. (35)
Iffisµstrongly convex, then µ(x, y)≥µ.
15Proof. The fundamental theorem of calculus implies
f(x)− ⟨∇ f(x), y−x⟩=Z1
0⟨∇f(x+t(y−x))− ∇f(x), y−x⟩dt
≥Z1
0µ(x, y)t∥x−y∥2
2dt
=µ(x, y)
2∥y−x∥2
2.
Note that we have implicitly used convexity to verify the inequality in the second line in the case where
µ(x, y) = 0 . Now assume that fisµstrongly convex. As a standard consequence of strong-convexity,
we obtain:
⟨∇f(x+t(y−x))− ∇f(x), y−x⟩
t∥x−y∥2
2=⟨∇f(x+t(y−x))− ∇f(x), x+t(y−x)−x⟩
t2∥x−y∥2
2
≥µ∥x−t(y−x)−x∥2
2
t2∥y−x∥2
2
=µ.
Proposition 3.1. Iffis convex and differentiable, then GD with step-size sequence {ηk}satisfies,
δk≤"Y
i∈G(1 +ηiλiµi)#
δ0+X
i∈B
Y
j>i,j∈G(1 +ηjλjµj)
ηiλi
2∥∇f(xi)∥2
2, (13)
where λi=ηiMi−2,G={i:ηi<2/Mi}, andB= [k]\G.
Proof. First note that λi<0fori∈ Gandλi≥0fori∈ B. We start from Equation (10),
f(xk+1)≤f(xk) +ηkηkM(xk, xk+1)
2−1
∥∇f(xk)∥2
2
=f(xk) + 1k∈G·ηkλk
2∥∇f(xk)∥2
2
+ 1k∈B·ηkλk
2∥∇f(xk)∥2
2
≤f(xk) + 1k∈G·[ηkλkµk(f(xk)−f(x∗))] + 1k∈B·ηkλk
2∥∇f(xk)∥2
2
,
where we used that directional strong convexity gives
∥∇f(xk)∥2
2≥2µk(f(xk)−f(x∗)).
Subtracting f(x∗)from both sides and then recursively applying the inequality gives the result.
Proposition 3.2. Iffis convex and differentiable, then GD with step-size sequence {ηk}satisfies,
∆k≤"kY
i=0|1−µiηi|
1 +µi+1ηi#
∆0+kX
i=0
Y
j>i|1−µjηj|
1 +µj+1ηj
 
Miη3
i−η2
i
1 +µi+1ηi∥∇f(xi)∥2
2. (15)
Proof. Let∆k=∥xk−x∗∥2
2and observe
∆k=∥xk−xk+1+xk+1−x∗∥2
2= ∆ k+1+∥xk−xk+1∥2
2+ 2⟨xk−xk+1, xk+1−x∗⟩.
Using this expansion in ∆k+1−∆k, we obtain
∆k+1−∆k=−∥xk−xk+1∥2
2−2⟨xk−xk+1, xk+1−x∗⟩
=−η2
k∥∇f(xk)∥2
2−2ηk⟨∇f(xk), xk+1−x∗⟩
=−η2
k∥∇f(xk)∥2
2−2ηk⟨∇f(xk), xk+1−xk⟩ −2ηk⟨∇f(xk), xk−x∗⟩.
16Now we control the inner-products with directional strong convexity and directional smoothness.
≤ −η2
k∥∇f(xk)∥2
2−2ηk⟨∇f(xk), xk+1−xk⟩+ 2ηkh
f(x∗)−f(xk)−µk
2∆ki
≤ −η2
k∥∇f(xk)∥2
2+ 2ηk
f(xk)−f(xk+1) +M(xk, xk+1)η2
k
2∥∇f(xk)∥2
2
+ 2ηkh
f(x∗)−f(xk)−µk
2∆ki
=η2
k(M(xk, xk+1)ηk−1)∥∇f(xk)∥2
2+ 2ηk[f(x∗)−f(xk+1)]−µkηk∆k
≤η2
k(M(xk, xk+1)ηk−1)∥∇f(xk)∥2
2−ηkµk+1∆k+1−µkηk∆k,
where the last inequality follows from µk+1strong convexity between xk+1andx∗. Re-arranging
this expression allows us to deduce a rate with error terms depending on the local smoothness,
=⇒(1 +µk+1ηk)∆k+1≤(1−µkηk) ∆k+η2
k(M(xk, xk+1)η−1)∥∇f(xk)∥2
2
≤ |1−µkηk|∆k+η2
k(M(xk, xk+1)η−1)∥∇f(xk)∥2
2
=⇒∆k+1≤|1−µkηk|
1 +µk+1ηk∆k+η2
k(M(xk, xk+1)η−1)
1 +µk+1ηk∥∇f(xk)∥2
2
≤"kY
i=0|1−µiηi|
1 +µi+1ηi#
∆0
+kX
i=0
kY
j=i+1|1−µjηj|
1 +µj+1ηj
η2
i(M(xi, xi+1)ηi−1)
1 +µi+1ηi∥∇f(xi)∥2
2.
Proposition 3.3. Letxk=Pk
i=0ηixi+1/Pk
i=0ηi. Iffis convex and differentiable, then GD satisfies,
f(xk)−f(x∗)≤∥x0−x∗∥2
2
2Pk
i=0ηi+Pk
i=0η2
i(ηiMi−1)∥∇f(xi)∥2
2
2Pk
i=0ηi. (16)
Proof. Let∆k=∥xk−x∗∥2
2and observe
∆k=∥xk−xk+1+xk+1−x∗∥2
2= ∆ k+1+∥xk−xk+1∥2
2+ 2⟨xk−xk+1, xk+1−x∗⟩.
Using this expansion in ∆k+1−∆k, we obtain
∆k+1−∆k=−∥xk−xk+1∥2
2−2⟨xk−xk+1, xk+1−x∗⟩
=−η2
k∥∇f(xk)∥2
2−2ηk⟨∇f(xk), xk+1−x∗⟩
=−η2
k∥∇f(xk)∥2
2−2ηk⟨∇f(xk), xk+1−xk⟩ −2ηk⟨∇f(xk), xk−x∗⟩.
Now we use convexity and directional smoothness to control the two inner-products as follows:
∆k+1−∆k≤ −η2
k∥∇f(xk)∥2
2−2ηk(f(xk)−f(x∗))−2ηk⟨∇f(xk), xk+1−xk⟩
≤ −η2
k∥∇f(xk)∥2
2−2ηk(f(xk)−f(x∗)) + 2 ηk(f(xk)−f(xk+1))
+η3
kM(xk, xk+1)∥∇f(xk)∥2
2
=η2
k(ηkM(xk, xk+1)−1)∥∇f(xk)∥2
2−2ηk(f(xk+1)−f(x∗)).
Re-arranging this equation and summing over iterations implies the following sub-optimality bound:
kX
i=0ηiPk
i=0ηi(f(xi+1)−f(x∗))≤∆0+Pk
i=0η2
i(ηiM(xi, xi+1)−1)∥∇f(xi)∥2
2
2Pk
i=0ηi.
Convexity of fand Jensen’s inequality now imply the final result,
=⇒f(xk)−f(x∗)≤∆0+Pk
i=0η2
i(ηiM(xi, xi+1)−1)∥∇f(xi)∥2
2
2Pk
i=0ηi.
17B.1 Path-Dependent Acceleration: Proofs
This section proves Theorem 3.4 using estimating sequences. Throughout this section, we assume
µ≥0is the global strong convexity parameter, where µ= 0covers the non-strongly convex case.
We start from the estimating sequences version of Equation (17), which is given as follows:
α2
k=ηk(1−αk)γk+ηkαkµ
γk+1= (1−αk)γk+αkµ
yk=1
γk+αkµ[αkγkvk+γk+1xk]
xk+1=yk−ηk∇f(yk)
vk+1=1
γk+1[(1−αk)γkvk+αkµyk−αk∇f(yk)].(36)
The algorithm is initialized with x0=v0and some γ0>0. Note that y0=x0=v0since ykis a
convex combination of xkandvk. First we prove that this scheme is equivalent to the one given in
Equation (17).
Lemma B.2. Equation (36) and Equation (17) lead to equivalent updates for the yk,xk, and αk
sequences. Moreover, given initialization γ0>0, the corresponding initialization for α0is,
α0=η0
2
(µ−γ0) +p
(γ0−µ)2+ 4γ0/η0
. (37)
Proof. The proof follows Nesterov et al. (2018, Theorem 2.2.3). Expanding the definition of vk+1,
we obtain
vk+1=1
γk+1(1−αk)
αk[(γk+αkµ)yk−γk+1xk] +αkµyk−αk∇f(yk)
=1
γk+1(1−αk)γk
αkyk+µyk
−(1−αk)
αkxk−αk
γk+1∇f(yk)
=xk−ηk
αk∇f(yk) +1
αk(yk−xk)
=xk+1
αk(xk+1−xk).
Plugging this back into the expression for yk+1,
yk+1=1
γk+1+αk+1µ[αk+1γk+1vk+1+γk+2xk+1]
=1
γk+1+αk+1µ
αk+1γk+1(xk+1
αk(xk+1−xk)) +γk+2xk+1
=αk+1γk1+αk(1−αk+1)γk+1+αkαk+1µ
αk(γk+1+αk+1µ)xk+1−αk+1γk+1(1−αk)
αk(γk+1+αk+1µ)xk
=xk+1+αk+1γk+1(1−αk)
αk(γk+1+αk+1µ)(xk+1−xk)
=xk+1+αk+1γk+1(1−αk)
αk 
γk+1+α2
k+1/ηk−(1−αk+1)γk+1(xk+1−xk)
=xk+1+αk(1−αk)
(αk+1+α2
k)(xk+1−xk).
Note that this update is consistent with Equation (17). Since γk=α2
k/ηk, we can write,
α2
k+1=ηk+1(1−αk+1)γk+ηk+1αk+1µ
=ηk+1
ηk(1−αk+1)α2
k+ηk+1αk+1µ,
18which is also consistent with Equation (17). Finally, the initialization for α0is determined by γ0in
Equation (36) as,
α2
0=η0(1−α0)γ0+η0α0µ.
The quadratic formula now implies,
α0=η0
2
(µ−γ0) +p
(γ0−µ)2+ 4γ0/η0
.
This completes the proof.
As mentioned, our proof uses the concept of estimating sequences.
Definition B.3. Two sequences λk,ϕkare estimating sequences for fifλl≥0for all k∈N,
limk→∞λk= 0, and,
ϕk(x)≤(1−λk)f(x) +λkϕ0(x), (38)
for all x∈Rd.
We use the same estimating sequences as developed by Nesterov et al. (2018). Let λ0= 1,ϕ0(x) =
f(x0) +γ0
2∥x−x0∥2
2, and define the updates,
λk+1= (1−αk)λk
ϕk+1(x) = (1 −αk)ϕk(x) +αk
f(yk) +⟨∇f(yk), x−yk⟩+µ
2∥x−yk∥2
2
,(39)
where µ≥0is the strong convexity parameter, with µ= 0when fis merely convex. It is straight-
forward to differentiate ϕk+1to see that vk+1of Equation (36) is the minimizer. Indeed, Nesterov
et al. (2018, Lemma 2.2.3) shows that this choice for the estimating sequences obeys the following
canonical form:
ϕk+1(x) = min
zϕk+1(z) +γk+1
2∥x−vk+1∥2
2, (40)
where γk+1andvk+1are given by Equation (36) and the minimum value is,
min
zϕk+1(z) = (1 −αk) min
zϕk(z) +αkf(yk)−α2
k
2γk+1∥∇f(yk)∥2
2
+αk(1−αk)γk
γk+1µ
2∥yk−vk∥2
2+⟨∇f(yk), vk−yk⟩
.(41)
Before we can prove our main theorem, we must show that these choices for λkandϕkyield a valid
estimating sequence. The following proofs build on (Nesterov et al., 2018) and (Mishkin et al., 2024).
Lemma B.4. Assume αk∈(0,1]for all k∈N. Ifµ >0andγ0=µ, then
λk=k−1Y
i=0(1−√ηkµ). (42)
Ifµ≥0andγ0∈(µ, µ+ 3/ηmin), then,
λk≤4
ηmin(γ0−µ)(k+ 1)2. (43)
Proof. Assume γ0=µ >0. Then γk=µfor all kand,
α2
k= (1−αk)ηkµ+αkηkµ
=ηkµ.
As a consequence,
λk=k−1Y
i=0(1−√ηkµ),
as claimed.
19Now assume γ0∈(µ,3L+µ). Modifying the proof by Nesterov et al. (2018, Lemma 2.2.4), we
compute as follows:
γk+1−µ= (1−αk)γk+ (αk−1)µ= (1−αk)(γk−µ)
Recursing on this equality implies
γk+1= (γ0−µ)kY
i=0(1−αk) =λk+1(γ0−µ).
Ifαk= 1orλk= 0, then using λk+1= (1−αk)λkimplies λk+1= 0and the result trivially holds.
Otherwise, recall α2
k/γk+1=ηkto obtain,
1−λk+1
λk=αk= (γk+1ηk)1/2
= (ηkµ+ηkλk+1(γ0−µ))1/2
=⇒1
λk+1−1
λk=1
λ1/2
k+1ηkµ
λk+1+ηk(γ0−µ)1/2
≥1
λ1/2
k+1ηminµ
λk+1+ηmin(γ0−µ)1/2
.
Finally, this implies
2
λ1/2
k+1 
1
λ1/2
k+1−1
λ1/2
k!
≥ 
1
λ1/2
k+1−1
λ1/2
k! 
1
λ1/2
k+1+1
λ1/2
k!
≥1
λ1/2
k+1ηminµ
λk+1+ηmin(γ0−µ)1/2
.
Moreover, this bound holds uniformly for all k∈N. We have now exactly reached Eq. 2.2.11 of
Nesterov et al. (2018, Lemma 2.2.4) with Lreplaced by 1/ηmin. Applying that Lemma with this
modification, we obtain
λk≤4
ηmin(γ0−µ)(k+ 1)2,
which completes the proof.
Lemma B.5. Iffis strongly convex with parameter µ≥0andηk≤1/µfor all k∈N, then λkand
ϕkare estimating sequences.
Proof. Using the quadratic formula, we find
αk=µ−γk±p
(µ−γk)2+ 4ˆγk/ηk
2/ηk.
Thus,
(µ−γk) + 
(µ−γk)2+ 4ˆγk/ηk1/2>0.
is sufficient for αk>0. This holds if µ≥γk. Otherwise, we require,
(µ−γk)2+ 4ˆγk/ηk>(µ−γk)2,
which holds if and only if ηk, γk>0. On the other hand, we also need αk≤1, which is satisfied
when,
4 + 4 ηk(γk−µ) +η2
k(µ−γk)2≤η2
k(µ−γk)2+ 4ηkγk⇐⇒ ηk≤1
µ,
as claimed.
Recall λ0= 1andλk+1= (1−αk)λk. Since αk∈(0,1],λk≥0holds by induction. It remains to
show that λktends to zero, which holds by Lemma B.4 since we have shown αk∈(0,1]for all k.
Now we establish the last piece,
ϕk(x)≤(1−λk)f(x) +λkϕ0(x).
But this follows immediately by Nesterov et al. (2018, Lemma 2.2.2).
20Now we can prove the last major lemma before our convergence result.
Lemma B.6. Suppose fis strongly convex with parameter µ≥0andηkis a sequence of adapted
step-sizes, meaning ηk≤1/M(xk, xk+1). Then for every k∈N,
min
zϕk(z)≥f(xk).
Proof. We use an inductive proof again. The inductive assumption is
min
zϕk(z)≥f(xk),
It is easy to see this holds at k= 0since,
ϕ0(x) =f(x0) +γ0
2∥x−v0∥2
2,
implies minzϕ0(z) =f(x0). Using Equation (41), we obtain
min
zϕk+1(z) = (1 −αk) min
zϕk(z) +αkf(yk)−α2
k
2γk+1∥∇f(yk)∥2
+αk(1−αk)γk
γk+1µ
2∥yk−vk∥2+⟨∇f(yk, zk), vk−yk⟩
≥(1−αk)f(xk) +αkf(yk)−α2
k
2γk+1∥∇f(yk)∥2
+αk(1−αk)γk
γk+1µ
2∥yk−vk∥2+⟨∇f(yk), vk−yk⟩
,
where the inequality holds by the inductive assumption. Using convexity of fand recallingα2
k
γk+1=ηk
from the definition of the update (Equation (36)),
min
zϕk+1(z)≥(1−αk) (f(yk) +⟨∇f(yk), xk−yk⟩) +αkf(yk)−ηk
2∥∇f(yk)∥2
+αk(1−αk)γk
γk+1µ
2∥yk−vk∥2+⟨∇f(yk), vk−yk⟩
=f(yk) + (1 −αk)⟨∇f(yk), xk−yk⟩ −ηk
2∥∇f(yk)∥2
+αk(1−αk)γk
γk+1µ
2∥yk−vk∥2+⟨∇f(yk), vk−yk⟩
Using the fact that the step-sizes are adapted and invoking the directional descent lemma (i.e.
Equation (18)) now implies
min
zϕk+1(z)≥f(xk+1) + (1 −αk)
⟨∇f(yk), xk−yk⟩
+αkγk
γk+1µ
2∥yk−vk∥2+⟨∇f(yk), vk−yk⟩
.
The remainder of the proof is largely unchanged from the analysis in Nesterov et al. (2018). The
definition of ykgives xk−yk=αkγk
γk+1(yk−vk), which we use to obtain
min
zϕk+1(z)≥f(xk+1) + (1 −αk)αkγk
γk+1⟨∇f(yk), yk−vk⟩
+αkγk
γk+1µ
2∥yk−vk∥2+⟨∇f(yk), vk−yk⟩
=f(xk+1) +µαk(1−αk)γk
2γk+1∥yk−vk∥2
≥f(xk+1),
sinceµαk(1−αk)γk
2γk+1≥0. We conclude the desired result by induction.
21The main accelerated result now follows almost immediately.
Theorem 3.4. Suppose fis differentiable, µ–strongly convex and AGD is run with adapted step-sizes
ηk≤1/Mk. Ifµ >0andα0=√η0µ, then AGD obtains the following accelerated rate:
f(xk+1)−f(x∗)≤kY
i=0(1−√µηi)h
f(x0)−f(x∗) +µ
2∥x0−x∗∥2
2i
. (19)
Letηmin= min i∈[k]ηi. Ifµ≥0andα0∈(√µη0, c), where cis the maximum value of α0for which
γ0=α2
0−η0α0µ
η0(1−α0)satisfies γ0<3/ηmin+µ, then AGD obtains the following rate:
f(xk+1)−f(x∗)≤4
ηmin(γ0−µ)(k+ 1)2h
f(x0)−f(x∗) +γ0
2∥x0−x∗∥2
2i
. (20)
Proof. We analyze the equivalent formulation given in Equation (36). See Lemma B.2 for a formal
proof that these two schemes produce the same xk,yk, and αkiterates. Note that our proof follows
Nesterov et al. (2018) and Mishkin et al. (2024) closely; while their results are very similar, we are
not aware of pre-existing works which adapt them to our specific setting.
First, observe that M(xk, xk+1)≥µfor all k∈N. Since the step-sizes ηkare assumed to satisfy
ηk≤1/M(xk, xk+1), we also have that ηk≤1/µfor every k.
Thus, Lemma B.4 and Lemma B.5 apply. Using the definition of an estimating sequence and
Lemma B.6, we obtain,
f(xk)≤min
zϕk(z)
≤min
z(1−λk)f(z) +λkϕ0(z)
≤(1−λk)f(x∗) +λkϕ0(x∗).
Re-arranging this equation and expanding the definition ϕ0(Equation (39)), we deduce the following:
f(xk)−f(x∗)≤λk(ϕ0(x∗)−f(x∗))
=λk
f(x0)−f(x∗) +γ0
2∥x0−x∗∥2
2
.
We see that the rate of convergence of AGD is entirely controlled by the convergence of the sequence
λk. Ifµ >0andγ0=µ, then Lemma B.4 implies
f(xk)−f(x∗)≤k−1Y
i=0(1−√µηi)h
f(x0)−f(x∗) +µ
2∥x0−x∗∥2
2i
.
By Lemma B.2, this initialization is equivalent to choosing α0=√η0µ, which is the setting claimed
in the theorem.
Alternatively, if µ≥0andγ0∈(µ, µ+ 3/ηmin), then,
f(xk)−f(x∗)≤4
ηmin(γ0−µ)k2h
f(x0)−f(x∗) +γ0
2∥x0−x∗∥2
2i
,
where the equality,
γ0=α2
0−η0α0µ
η0(1−α0),
holds by Lemma B.2. Since α0≤1forη0≤1/µ,η0is an increasing function of γ0. Thus, an
upper-bound conα0can be deduced from that on γ0using the quadratic formula:
c=−3η0
2ηmin+η0
29
(ηmin)2+ 43ηmin+µ
η01/2
=3η0
2ηmin"
1 + 4( ηmin)23ηmin+µ
9η01/2
−1#
.
22C Proofs for Section 4.1
Lemma C.1. LetBbe a positive semi-definite matrix and suppose that
f(x) =1
2x⊤Bx−c⊤x.
Letxi+1=xi−η∇f(xi). Then for any η >0, the pointwise directional smoothness between the
gradient descent iterates xi, xi+1is given by
1
2D(xi, xi+1) =∥B∇f(xi)∥2
∥∇f(xi)∥2.
Proof. We have by straightforward algebra,
1
2D(xi, xi+1) =∥∇f(xi+1)− ∇f(xi)∥2
∥xi+1−xi∥2
=∥[Bxi+1−c]−[Bxi−c]∥2
∥xi+1−xi∥2
=∥B[xi+1−xi]∥2
∥xi+1−xi∥2
=∥B[−η∇f(xi)]∥2
∥ −η∇f(xi)∥2
=∥B∇f(xi)∥2
∥∇f(xi)∥2.
Lemma C.2. LetBbe a positive semi-definite matrix and suppose that
f(x) =1
2x⊤Bx−c⊤x.
Letxi+1=xi−η∇f(xi). Then for any η >0, the path-wise directional smoothness between the
gradient descent iterates xi, xi+1is given by by
A(xi, xi+1) =∇f(xi)⊤B∇f(xi)
∇f(xi)⊤∇f(xi).
Proof. LetAt(x, y) =⟨∇f(x+t(y−x))−∇f(x),y−x⟩
t∥x−y∥2
2. We have
At(x, y) =⟨∇f(x+t(y−x))− ∇f(x), y−x⟩
t∥x−y∥2
2
=⟨(B(x+t(y−x)))−c−[Bx−c], y−x⟩
t∥x−y∥2
2
=⟨t·B(y−x), y−x⟩
t∥x−y∥2
2
=(y−x)⊤B(y−x)
∥x−y∥2
2.
The path-wise directional smoothness Ais therefore
A(x, y) = sup
t∈[0,1]At(x, y)
= sup
t∈[0,1](y−x)⊤B(y−x)
∥x−y∥2
2
=(y−x)⊤B(y−x)
∥x−y∥2
2.
23Plugging in y=x−η∇f(x) =x−η[Bx−c]in the above gives
A(x, x−η∇f(x)) =(−η[Bx−c])B(−η) [Bx−c]
∥η[Bx−c]∥2
2
=(Bx−c)⊤B(Bx−c)
∥Bx−c∥2
2
=(Bx−c)⊤B(Bx−c)
∥Bx−c∥2
2
=∇f(x)⊤B∇f(x)
∇f(x)⊤∇f(x).
D Proofs for Section 4.2
Proposition 4.1. Iffis convex and continuously differentiable, then either (i) fis minimized along
the ray x(η) =x−η∇f(x)or (ii) there exists η >0satisfying η= 1/D(x, x−η∇f(x)).
Proof. LetI={η:∇f(x−η∇f(x)) =∇f(x)}. For every η∈ I, it holds that
−⟨∇f(x−η∇f(x)),∇f(x)⟩=−∥∇f(x)∥2
2.
However, since fis convex, the directional derivative
−⟨∇f(x−η′∇f(x)),∇f(x)⟩,
is monotone non-decreasing in η′. We deduce that Imust be an interval of form [0,η]. Ifηis not
bounded, then fis linear along −∇f(x)and is minimized by taking η→ ∞ . Therefore, we may
assume ηis finite.
Letη >η. Then we have the following:
x−η∇f(x) =x−2∥x−η∇f(x)−x∥2
∥∇f(x−η∇f(x))− ∇f(x)∥2∇f(x)
⇐⇒ ∇ f(x) =2∥∇f(x)∥2
∥∇f(x−η∇f(x))− ∇f(x)∥2∇f(x),
from which we deduce
∥∇f(x−η∇f(x))− ∇f(x)∥2= 2∥∇f(x)∥2,
is sufficient for the implicit equation to hold. Squaring both sides and multiplying by 1/2, we obtain
the following alternative root-finding problem:
h(η) :=1
2∥∇f(x−η∇f(x))∥2
2− ⟨∇ f(x−η∇f(x)),∇f(x)⟩ −1
2∥∇f(x)∥2
2= 0. (44)
Because fisC1,his a continuous function and it suffices to show that there exists an interval in
which hcrosses 0. From the display above, we see
h(η) =−∥∇f(x)∥2
2<0.
Continuity now implies ∃η′>ηsuch that h(η′)<0. Now, suppose h(η)≤0for all η≥η′. Working
backwards, we see that this can only occur when
η≤2∥x−η∇f(x)−x∥2
∥∇f(x−η∇f(x))− ∇f(x)∥2=1
D(x(η), x−η∇f(x))
for all η≥η′. The directional descent lemma (Equation (10)) now implies
f(x−η∇f(x))≤f(x)−η
1−ηD(x, x−η∇f(x))
2
∥∇f(x)∥2
2≤f(x)−η
2∥∇f(x)∥2
2,
Taking limits on both sides as η→ ∞ implies f(x−η∇f(x))is minimized along the ray x(η) =
x−η∇f(x). Thus, we deduce that either there exists η′′> η′such that h(η′′)>0exists, or fis
minimized along the gradient direction as claimed.
24Proposition 4.2. Iffis convex and twice continuously differentiable, then either (i) fis minimized
along the ray x(η) =x−η∇f(x)or (ii) there exists η >0satisfying η= 1/A(x, x−η∇f(x)).
Proof. Let
J=
η:⟨∇f(x−η∇f(x)),∇f(x)⟩=∥∇f(x)∥2
2	
.
Since fis convex, the directional derivative
−⟨∇f(x−η′∇f(x)),∇f(x)⟩,
is monotone non-decreasing in η′. We deduce that Jmust be an interval of form [0,η]. Ifηis not
bounded, then convexity implies
lim
η→∞f(x−η∇f(x))≤lim
η→∞f(x)−η⟨∇f(x−η∇f(x)),∇f(x)⟩
=−∞,
meaning fis minimized along −∇f(x). Therefore, we may assume ηis finite.
We have
x−η∇f(x) =x−1
A(x, x−η∇f(x))∇f(x)
⇐⇒ η= inf
t∈[0,1]tη∥∇f(x)∥2
2
⟨∇f(x)− ∇f(x−tη∇f(x)),∇f(x)⟩.
Thus, for η >η, the equation we must solve reduces to
h(η) :=η−inf
t∈[0,1]tη∥∇f(x)∥2
2
⟨∇f(x)− ∇f(x−tη∇f(x)),∇f(x)⟩= 0.
Since fisC2,his continuous (see, e.g. Hogan (1973, Theorem 7)) and it suffices to show that there
exists an interval over which hcrosses 0.
Using Taylor’s theorem, we can re-write this expression as
h(η) =η−inf
t∈[0,1]∥∇f(x)∥2
2
⟨∇f(x),∇2f(x−α(tη)∇f(x))∇f(x)⟩,
where for some α(tη)∈[0, tη]. Examining the denominator, we find that,
Zt
0∇f(x)⊤∇2f(x−tη∇f(x))∇f(x)dt=⟨∇f(x−η∇f(x))− ∇f(x),∇f(x)⟩= 0,
which, since fis convex, implies
∇f(x)⊤∇2f(x−α∇f(x))∇f(x) = 0 ,
for every α∈[0,η]. By continuity of the Hessian, for every ϵ >0, there exists δ >0such that
η′∈[η,η+δ]guarantees,
∇f(x)⊤∇2f(x−η′∇f(x))∇f(x)< ϵ.
Substituting this into our expression for h,
h(η′) =η′−inf
t∈[0,1]∥∇f(x)∥2
2
⟨∇f(x),∇2f(x−α(tη′)∇f(x))∇f(x)⟩
<η+δ−∥∇f(x)∥2
2
ϵ
<0,
forϵ, δsufficiently small. Thus, there exists η′>ηfor which h(η′)<0.
Now let us show that h(η′′)>0for some η′′. For convenience, define
g(η) = inf
t∈[0,1]t∥∇f(x)∥2
2
⟨∇f(x)− ∇f(x−tη∇f(x)),∇f(x)⟩,
25Algorithm 1 Gradient Descent with Exponential Search
1:Procedure ExponentialSearch( x, η0)
2:fork= 1,2,3, . . .do
3: ηout←RootFindingBisection
x,2−2kη0, η0
.
4: ifηout<∞then
5: Return ηout
6: end if
7:end for
8:End Procedure
9:Procedure RootFindingBisection( x, ηlo, ηhi)
10:Define ϕ(η) =η−ψ(η)where ψ(η)is given in (24)\\One access to ϕrequires Tdescent
steps.
11:ifϕ(ηhi)≤0then
12: Return ηhi
13:end if
14:ifϕ(ηlo)>0then
15: Return ∞
16:end if
17:while ηhi>2ηlodo
18: ηmid=√ηloηhi
19: ifϕ(ηmid)>0then
20: ηhi=ηmid
21: else
22: ηlo=ηmid
23: end if
\\Invariant: ϕ(ηhi)>0, and ϕ(ηlo)≤0.
24:end while
25:Return ηlo
26:End Procedure
which is a continuous and monotone non-increasing function. Take η→ ∞ and let
lim
η→∞g(η) =c,
where the limit exists, but may be −∞. Indeed, it must hold that c <∞since,
lim
η→∞g(η)< g(η′)<∞.
Ifc <0, then taking η′′large enough that g(η′′)≤0suffices. Alternatively, if c≥0, then there
exists ˜ηsuch that g(η)≤c+ϵfor every η≥˜η. Choosing η′′>max{˜η, c}+ϵyields
h(η′′) =η′′−g(η′′)> c+ϵ−c−ϵ= 0.
This completes the proof.
Theorem 4.3. Assume fis convex and L-smooth. Then Algorithm 1 with η0>0requires at most
2K(log log(2 η0/L)∨1)iterations of GD and in the last run it outputs a step-size ηand point
xK=1
KPK−1
i=0xi(η)such that exactly one of the following holds:
Case 1: η=η0and f(xK)−f(x∗)≤∥x0−x∗∥2
2
2Kη0
Case 2: η̸=η0and f(xK)−f∗≤∥x0−x∗∥2
2
2K"Pk
i=0Mi∥∇f(x′
i)∥2
2Pk
i=0∥∇f(x′
i)∥2
2#
,
where Midef=M(x′
i, x′
i+1)andx′
iare the iterates generated by GD with step-size η′∈[η,2η].
26Proof of Theorem 4.3. This analysis follows (Carmon and Hinder, 2022). First, instantiate Equa-
tion (16) from Proposition 3.3 with ηi=ηfor all ito obtain
f(xk)−f∗≤∥x0−x∗∥2
2ηk+ηh
ηPk
i=0M(xi, xi+1)∥∇f(xi)∥2−Pk
i=0∥∇f(xi)∥2i
2k.(45)
Now, observe that if we get a “Lucky strike” and ϕ(ηhi) =ϕ(η0)≤0, then specializing Equation (45)
forη=η0we get
f(xk)−f(x∗)≤∥x0−x∗∥2
2
2η0k+η0
2k"
η0kX
i=0M(xi, xi+1)∥∇f(xi)∥2
2−kX
i=0∥∇f(xi)∥2
2#
=∥x0−x∗∥2
2
2η0k+η0Pk
i=0M(xi, xi+1)∥∇f(xi)∥2
2
2k·ϕ(η0)
≤∥x0−x∗∥2
2
2η0k.
This covers the first case of Theorem 4.3.
With the first case out of the way, we may assume that ϕ(ηhi)>0. This implies that ηhi>1
L, since
ifη≤1
Lwe have ϕ(η)≤0. Now observe that when ηlo= 22−kη0≤1
L, we have that ϕ(ηlo)≤0,
therefore it takes at most k=⌈log logη0
L−1⌉to find such an ηlo. From here on, we suppose that
ϕ(ηhi)>0andϕ(ηlo)≤0. Now observe that the algorithm’s main loop always maintains the
invariant ϕ(ηhi)>0andϕ(ηlo)≤0, and every iteration of the loop halves logηhi
ηlo, therefore we
make at most ⌈log log η0L⌉loop iterations. The output step-size ηlosatisfiesηhi
2≤ηlo≤ηhiand
ϕ(ηlo)≤0. Specializing Equation (45) for η=η0and using that ϕ(ηlo)≤0we get
f(xk)−f(x∗)≤∥x0−x∗∥2
2
2ηlok+ηloPk
i=0M(xi(ηlo), xi+1(ηlo))∥∇f(xi(ηlo))∥2
2
2k·ϕ(ηlo)
≤∥x0−x∗∥2
2
2ηlok. (46)
By the loop invariant ϕ(ηhi)>0we have
ϕ(ηhi)>0⇔ηhi>PK
i=0∥∇f(xi(ηhi))∥2
2PK
i=0∥∇f(xi(ηhi))∥2
2M(xi(ηhi), xi+1(ηhi))
By the loop termination condition we have ηlo≥ηhi
2, combining this with the last equation we get
ηlo≥ηhi
2≥1
2PK
i=0∥∇f(xi(ηhi))∥2
2PK
i=0∥∇f(xi(ηhi))∥2
2M(xi(ηhi), xi+1(ηhi)).
Plugging this into Equation (46) we obtain
f(xk)−f(x∗)≤∥x0−x∗∥2
2
k·PK
i=0∥∇f(xi(ηhi))∥2
2M(xi(ηhi), xi+1(ηhi))
PK
i=0∥∇f(xi(ηhi))∥2
2
It remains to notice that ηhi∈[ηlo,2ηlo].
Theorem 4.4. Suppose that fis convex and differentiable and let Mbe any directional smoothness
function for f. Let∆0:=∥x0−x∗∥2
2. Then GD with the Polyak step-size and γ∈(1,2)satisfies
f(xk)−f(x∗)≤c(γ)∆0
2Pk−1
i=0M(xi, xi+1)−1, (27)
where c(γ) =γ/(2−γ)(γ−1)andxk=Pk−1
i=0
M(xi, xi+1)−1xi
/Pk−1
i=0M(xi, xi+1)−1
.
For the proof of this theorem, we will need the following proposition:
27Proposition D.1. Letx∈Rd. Define ηx=γf(x)−f(x∗)
∥∇f(x)∥2for some γ∈(1,2)and let ˜x=x−
ηx∇f(x). Then,
f(x)−f(x∗)≥γ−1
γ22
M(x,˜x)∥∇f(x)∥2
2.
Proof. Observe
f(x)−f(x∗) =f(x)−f(˜x) +f(˜x)−f(x∗)
≥f(x)−f(˜x). (47)
By smoothness we have
f(˜x)≤f(x) +⟨∇f(x),˜x−x⟩+M(x,˜x)
2∥˜x−x∥2
=f(x)−ηx∥∇f(x)∥2
2+η2
xM(x,˜x)
2∥∇f(x)∥2
2.
Plugging back into Equation (47) we get
f(x)−f(x∗)≥ηx∥∇f(x)∥2
2−η2
xM(x,˜x)
2∥∇f(x)∥2
2.
Let us now use the definition of ηx=γf(x)−f(x∗)
∥∇f(x)∥2
2to get
f(x)−f(x∗)≥γ(f(x)−f(x∗))−γηxM(x,˜x)
2(f(x)−f(x∗)).
Assuming that f(x)̸=f(x∗)then we get by cancellation
1≥γ−γηxM(x,˜x)
2.
Using the definition of ηxagain
1−γ≥ −γ2M(x,˜x)
2f(x)−f(x∗)
∥∇f(x)∥2
2
Rearranging we get
f(x)−f(x∗)≥γ−1
γ22
M(x,˜x)∥∇f(x)∥2
2.
Iff(x) =f(x∗)then∥∇f(x)∥2
2= 0, both sides are identically zero and the statement holds
trivially.
Now we can prove our theorem on the convergence of GD with Polyak step-sizes:
Proof of Theorem 4.4. We start by considering the distance to the optimum and expanding the square
∥xk+1−x∗∥2
2=∥xk−x∗∥2
2+ 2⟨xk+1−xk, xk−x∗⟩+∥xk+1−xk∥2
2
=∥xk−x∗∥2
2−2ηk⟨∇f(xk), xk−x∗⟩+η2
k∥∇f(xk)∥2
2. (48)
Letδk=f(xk)−f(x∗). By convexity we have f(x∗)≥f(xk) +⟨∇f(xk), x∗−xk⟩. Therefore
we can upper bound Equation (48) as
∥xk+1−x∗∥2
2≤ ∥xk−x∗∥2
2−2ηkδk+η2
k∥∇f(xk)∥2
2
=∥xk−x∗∥2
2−2ηkδk+ηk 
γδk
∥∇f(xk)∥2
2!
∥∇f(xk)∥2
2
=∥xk−x∗∥2
2−(2−γ)ηkδk, (49)
28where in the second line we used the definition of ηk. By Proposition D.1 we have
δk≥γ−1
γ2
M(xk, xk+1)∥∇f(xk)∥2
2. (50)
Using this in Equation (49) gives
∥xk+1−x∗∥2
2≤ ∥xk−x∗∥2
2−(2−γ)ηkγ−1
γ22
M(xk, xk+1)∥∇f(xk)∥2
2
=∥xk−x∗∥2
2−(2−γ)γ−1
γ22
M(xk, xk+1) 
γδk
∥∇f(xk)∥2
2!
∥∇f(xk)∥2
2
=∥xk−x∗∥2
2−2(2−γ)(γ−1)
γM(xk, xk+1)δk.
Rearranging we get
2(2−γ)(γ−1)
γM(xk, xk+1)δk≤ ∥xk−x∗∥2
2− ∥xk+1−x∗∥2
2.
Summing up and telescoping we get
k−1X
i=02(2−γ)(γ−1)
γM(xi, xi+1)δi≤ ∥x0−x∗∥2
2.
Letxk=1Pk−1
i=0M(xi,xi+1)−1Pk−1
i=0M(xi, xi+1)−1xi, then by the convexity of fand Jensen’s
inequality we have
f(xk)−f(x∗)≤1
Pk−1
i=0M(xi, xi+1)−1k−1X
i=0M(xi, xi+1)−1δi
≤γ
2(2−γ)(γ−1)1
Pk−1
i=0M(xi, xi+1)−1∥x0−x∗∥2
2.
Theorem D.2. Iffis convex and differentiable, then GD with the Polyak step-size and γ <2satisfies,
f(xk)−f(x∗)≤1
(2−γ)Pk
i=0ηi∥x0−x∗∥2
2, (51)
where xk=Pk−1
i=0ηixi/Pk−1
i=0ηi
.
Proof. The proof begins in the same manner as that for Theorem 4.4,
∥xk+1−x∗∥2
2=∥xk−x∗∥2
2+ 2⟨xk+1−xk, xk−x∗⟩+∥xk+1−xk∥2
2
=∥xk−x∗∥2
2−2ηk⟨∇f(xk), xk−x∗⟩+η2
k∥∇f(xk)∥2
2
≤ ∥xk−x∗∥2
2−2ηkδk+η2
k∥∇f(xk)∥2
2
=∥xk−x∗∥2
2−2ηkδk+ηk 
γδk
∥∇f(xk)∥2
2!
∥∇f(xk)∥2
2
=∥xk−x∗∥2
2−(2−γ)ηkδk.
Re-arranging, summing from i= 0tok−1, and dividing byPk−1
i=0ηi,
=⇒k−1X
i=0ηiPk
i=0ηi(f(xi)−f(w∗))≤1
(2−γ)Pk
i=0ηi
∥x0−x∗∥2
2− ∥xk−x∗∥2
2
=⇒f(xk)−f(x∗)≤1
(2−γ)Pk
i=0ηi∥x0−x∗∥2
2,
which completes the proof.
29Lemma D.3. Normalized GD with step-sizes ηksatisfies
−ηk
∥∇f(xk)∥2⟨∇f(xk),∇f(xk+1)⟩ ≤η2
kM(xk, xk+1)−ηk∥∇f(xk)∥2. (52)
Proof. By convexity we have
f(xk+1)≤f(xk) +⟨xk+1−xk,∇f(xk+1)⟩
=f(xk)−ηk
∥∇f(xk)∥2⟨∇f(xk),∇f(xk+1)⟩ (53)
Now note that
−ηk
∥∇f(xk)∥2⟨∇f(xk),∇f(xk+1)⟩=ηk
∥∇f(xk)∥2⟨∇f(xk),∇f(xk)− ∇f(xk+1)⟩ (54)
−ηk∥∇f(xk)∥2
≤ηk∥∇f(xk)− ∇f(xk+1)∥ −ηk∥∇f(xk)∥2, (55)
where we used Cauchy-Schwarz. Recalling the definition of directional smoothness
M(xk, xk+1)def=∥∇f(xk)− ∇f(xk+1)∥
∥xk−xk+1∥=∥∇f(xk)− ∇f(xk+1)∥
ηk
in Equation (55) gives
−ηk
∥∇f(xk)∥2⟨∇f(xk),∇f(xk+1)⟩ ≤η2
kM(xk, xk+1)−ηk∥∇f(xk)∥2.
Theorem 4.5. Suppose that fis convex and differentiable. Let Dbe the point-wise directional
smoothness defined by Equation (4)and∆0:=∥x0−x∗∥2
2. Then normalized GD with a sequence of
non-increasing step-sizes ηksatisfies
f(ˆxk)−f(x∗)≤∆0+Pk−1
i=0η2
i
2k2f(x0)
η2
0−f(x∗)
η2
k−1
+∆0+Pk−1
i=0η2
i
2kk−1X
i=0M(xi, xi+1)
k,(30)
where ˆxk= arg min i∈[k−1]f(xi). Ifmax i∈[k−1]M(xi, xi+1)is bounded for all k(i.e.fisL-
smooth), then for ηi= 1/√
iwe have f(ˆxk)−f(x∗)∈ O(1/k)and for ηi= 1/√
iwe get the
anytime result f(ˆxk)−f(x∗)∈ O(log(k)/k).
Proof. Here we will first establish that for any non-increasing sequence of step-sizes ηk>0we have
that
min
i∈[k−1]f(xi)−f(x∗)≤1
2∆0+Pk−1
i=0η2
i
k 
f(x0)
kη2
0−f(x∗)
kη2
k−1+k−1X
i=0M(xi, xi+1)
k!
. (56)
The specialized results follow by assuming thatPk−1
i=0M(xi,xi+1)
kis bounded, which it is the case
ofL–Lipschitz gradients. In particular the mini∈[k−1]f(xi)−f(x∗)∈ O(1/T)result follows by
plugging in ηi= 1/√
kand using that
k−1X
i=0η2
i=k−1X
i=01
k= 1
f(x0)
kη2
0−f(x∗)
kη2
k−1=f(x0)−f(x∗).
Alternatively we get mini∈[k−1]f(xi)−f(x∗)∈ O(log(T)/T)by plugging in ηi= 1/√i+ 1and
using that
k−1X
i=0η2
i=k−1X
i=01
i+ 1≤log(k)
f(x0)
kη2
0−f(x∗)
kη2
k−1=f(x0)
k−f(x∗).
30With this in mind, let us prove Equation (56).
By convexity,
f(xk+1)≤f(xk)−ηk
∥∇f(xk)∥2∇f(xk)⊤∇f(xk+1)
≤f(xk)−ηk∥∇f(xk)∥2+η2
kM(xk, xk+1). (Using (52))
Re-arranging, dividing through by η2
k, and then summing over i= 0,···, k−1gives
k−1X
i=0∥∇f(xi)∥2
ηi≤f(x0)
η2
0+k−2X
i=1f(xi)1
η2
i−1
η2
i−1
−f(x∗)
η2
k−1+k−1X
i=0M(xi, xi+1)
≤f(x0)
η2
0−f(x∗)
η2
k−1+k−1X
i=0M(xi, xi+1), (57)
where we used that ηi−1≤ηi=⇒1
η2
i−1
η2
i−1≤0. Using Jensen’s inequality over the map a7→1/a,
which is convex for apositive, gives
k−1X
i=0ηi
∥∇f(xk)∥2≥k2
Pk−1
i=0∥∇f(xk)∥2/ηi(57)
≥k2
f(x0)
η2
0−f(x∗)
η2
k−1+Pk−1
i=0M(xi, xi+1). (58)
Meanwhile, recall our notation ∆i=∥xi−x∗∥2
2. Expanding the squares and using that f(x)is
convex, we have that
∆i+1= ∆ i−2ηi
∥∇f(xi)∥2∇f(xi)⊤(xi−x∗) +η2
i
≤∆i−2ηif(xi)−f(x∗)
∥∇f(xk)∥2+η2
i.
As before, we use δi:=f(xi)−f(x∗). Re-arranging, summing both sides of the above over
i= 0, . . . , k −1and using telescopic cancellation gives
k−1X
i=0ηiδi
∥∇f(xi)∥≤∆0+Pt−1
i=0η2
i
2.
Using the above along with (58) gives,
min
i∈[k−1]δi≤1
Pk−1
i=0ηi
∥∇f(xi)∥2k−1X
i=0ηiδi
∥∇f(xi)∥2
≤1
2∆0+Pk−1
i=0η2
iPk−1
i=0ηi
∥∇f(xi)∥
≤1
2∆0+Pk−1
i=0η2
i
k 
f(x0)
kη2
0−f(x∗)
kη2
k−1+k−1X
i=0M(xi, xi+1)
k!
E Experimental Details
In this section we provide additional details necessary to reproduce our experiments. We run our
logistic regression experiments using PyTorch (Paszke et al., 2019). For the UCI datasets, we use the
pre-processed version of the data provided by Fernández-Delgado et al. (2014), although we do not
use their evaluation procedure as it is known have test-set leakage. Instead, we randomly perform
an 80–20 train-test split and use the test set for validation. Unless otherwise stated, all methods are
initialized using the Kaiming initialization (He et al., 2015), which is standard in PyTorch.
31In order to compute the strongly adapted step-sizes, we run the SciPy (Virtanen et al., 2020) imple-
mentation of Newton method on Equation (44). In general, we find this procedure is surprisingly
robust, although it can be slow.
Figure 1 : We pick two datasets from the UCI repository to showcase different behaviors of the
upper-bounds. We compute a tight-upper bound on Las follows. Recall that for logistic regression
problems the Hessian is given by
∇2f(x) =A⊤Diag1
σ(−y·Ax) + 2 + σ(y·Ax)
A,
where Ais the data matrix and σ(z) =1
1+exp( z)is the sigmoid function. A short calculation shows
that the diagonal matrix
Diag1
σ(−y·Ax) + 2 + σ(y·Ax)
⪯1
4I,
which is tight when x= 0. As a result, L=λmax(A⊤A)/4. We compute this manually. We also
compute the optimal value for the logistic regression problem using the SciPy implementation of
BFGS (Liu and Nocedal, 1989). We use this value for f(x∗)to compute the Polyak step-size and
when plotting sub-optimality. It turns out that the upper-bound based on L-smoothness for both GD
with the Polyak step-size (Hazan and Kakade, 2019) and standard GD (Bubeck et al., 2015) is
f(xk)−f(x∗)≤2L∥x0−x∗∥2
2
k.
Figure 3 : We run these experiments using vanilla NumPy. As mentioned in the text, we generate a
quadratic optimization problem
min
x1
2x⊤Ax−b⊤x,
where the eigenvalues of Awere generated to follow power law distribution with parameter α= 3.
We scaled the eigenvalues to ensure L= 1000 . The dimension of the problem we create is d= 300 .
We repeat the experiment for 20random trials and plot the mean and standard deviations.
Figure 4 : We pick three different datasets from the UCI repository to showcase the possible con-
vergence behavior of the optimization methods. We compute Landf(w∗)as described above for
Figure 1. For normalized GD, we use the step-size schedule ηk=η0/√
kas suggested by our
theory. To pick η0, we run a grid search on the grid generated by np.logspace(-8, 1, 20) . We
implement AdGD from scratch and use a starting step-size of η0= 10−3. We use the same procedure
to compute the strongly adapted step-sizes as described above.
F Computational Details
The experiments in Figure 3 were run on a MacBook Pro (16 inch, 2019) with a 2.6 GHz 6-Core
Intel i7 CPU and 16GB of memory. All other experiments were run on a Slurm cluster with several
different node configurations. Our experiments on the cluster were run with nodes using (i) Nvidia
A100 GPUs (80GB or 40GB memory) or Nvidia H100-80GB GPUs with Icelake CPUs, or (ii) Nvidia
V100-32GB or V100-16GB GPUs with Skylake CPUs. All jobs were allocated a single GPU and
24GB of RAM.
32NeurIPS Paper Checklist
1.Claims
Question: Do the main claims made in the abstract and introduction accurately reflect the
paper’s contributions and scope?
Answer: [Yes]
Justification: All claims in the abstract and introduction are justified with rigorous proofs
and supported by experiments.
Guidelines:
•The answer NA means that the abstract and introduction do not include the claims
made in the paper.
•The abstract and/or introduction should clearly state the claims made, including the
contributions made in the paper and important assumptions and limitations. A No or
NA answer to this question will not be perceived well by the reviewers.
•The claims made should match theoretical and experimental results, and reflect how
much the results can be expected to generalize to other settings.
•It is fine to include aspirational goals as motivation as long as it is clear that these goals
are not attained by the paper.
2.Limitations
Question: Does the paper discuss the limitations of the work performed by the authors?
Answer: [Yes]
Justification: The limitations of our theoretical results, including necessary assumptions, are
addressed in the text.
Guidelines:
•The answer NA means that the paper has no limitation while the answer No means that
the paper has limitations, but those are not discussed in the paper.
• The authors are encouraged to create a separate "Limitations" section in their paper.
•The paper should point out any strong assumptions and how robust the results are to
violations of these assumptions (e.g., independence assumptions, noiseless settings,
model well-specification, asymptotic approximations only holding locally). The authors
should reflect on how these assumptions might be violated in practice and what the
implications would be.
•The authors should reflect on the scope of the claims made, e.g., if the approach was
only tested on a few datasets or with a few runs. In general, empirical results often
depend on implicit assumptions, which should be articulated.
•The authors should reflect on the factors that influence the performance of the approach.
For example, a facial recognition algorithm may perform poorly when image resolution
is low or images are taken in low lighting. Or a speech-to-text system might not be
used reliably to provide closed captions for online lectures because it fails to handle
technical jargon.
•The authors should discuss the computational efficiency of the proposed algorithms
and how they scale with dataset size.
•If applicable, the authors should discuss possible limitations of their approach to
address problems of privacy and fairness.
•While the authors might fear that complete honesty about limitations might be used by
reviewers as grounds for rejection, a worse outcome might be that reviewers discover
limitations that aren’t acknowledged in the paper. The authors should use their best
judgment and recognize that individual actions in favor of transparency play an impor-
tant role in developing norms that preserve the integrity of the community. Reviewers
will be specifically instructed to not penalize honesty concerning limitations.
3.Theory Assumptions and Proofs
Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?
33Answer: [Yes]
Justification: Our theoretical results are accompanied by their necessary assumptions and
rigorous proofs are provided in the appendix.
Guidelines:
• The answer NA means that the paper does not include theoretical results.
•All the theorems, formulas, and proofs in the paper should be numbered and cross-
referenced.
•All assumptions should be clearly stated or referenced in the statement of any theorems.
•The proofs can either appear in the main paper or the supplemental material, but if
they appear in the supplemental material, the authors are encouraged to provide a short
proof sketch to provide intuition.
•Inversely, any informal proof provided in the core of the paper should be complemented
by formal proofs provided in appendix or supplemental material.
• Theorems and Lemmas that the proof relies upon should be properly referenced.
4.Experimental Result Reproducibility
Question: Does the paper fully disclose all the information needed to reproduce the main ex-
perimental results of the paper to the extent that it affects the main claims and/or conclusions
of the paper (regardless of whether the code and data are provided or not)?
Answer: [Yes]
Justification: All experimental procedures and hyper-parameter settings are provided in
Appendix E.
Guidelines:
• The answer NA means that the paper does not include experiments.
•If the paper includes experiments, a No answer to this question will not be perceived
well by the reviewers: Making the paper reproducible is important, regardless of
whether the code and data are provided or not.
•If the contribution is a dataset and/or model, the authors should describe the steps taken
to make their results reproducible or verifiable.
•Depending on the contribution, reproducibility can be accomplished in various ways.
For example, if the contribution is a novel architecture, describing the architecture fully
might suffice, or if the contribution is a specific model and empirical evaluation, it may
be necessary to either make it possible for others to replicate the model with the same
dataset, or provide access to the model. In general. releasing code and data is often
one good way to accomplish this, but reproducibility can also be provided via detailed
instructions for how to replicate the results, access to a hosted model (e.g., in the case
of a large language model), releasing of a model checkpoint, or other means that are
appropriate to the research performed.
•While NeurIPS does not require releasing code, the conference does require all submis-
sions to provide some reasonable avenue for reproducibility, which may depend on the
nature of the contribution. For example
(a)If the contribution is primarily a new algorithm, the paper should make it clear how
to reproduce that algorithm.
(b)If the contribution is primarily a new model architecture, the paper should describe
the architecture clearly and fully.
(c)If the contribution is a new model (e.g., a large language model), then there should
either be a way to access this model for reproducing the results or a way to reproduce
the model (e.g., with an open-source dataset or instructions for how to construct
the dataset).
(d)We recognize that reproducibility may be tricky in some cases, in which case
authors are welcome to describe the particular way they provide for reproducibility.
In the case of closed-source models, it may be that access to the model is limited in
some way (e.g., to registered users), but it should be possible for other researchers
to have some path to reproducing or verifying the results.
5.Open access to data and code
34Question: Does the paper provide open access to the data and code, with sufficient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?
Answer: [No]
Justification: We will release the code to reproduce our experiments upon acceptance of the
paper. All non-synthetic data used in this paper is open source and freely accessible online.
Guidelines:
• The answer NA means that paper does not include experiments requiring code.
•Please see the NeurIPS code and data submission guidelines ( https://nips.cc/
public/guides/CodeSubmissionPolicy ) for more details.
•While we encourage the release of code and data, we understand that this might not be
possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
including code, unless this is central to the contribution (e.g., for a new open-source
benchmark).
•The instructions should contain the exact command and environment needed to run to
reproduce the results. See the NeurIPS code and data submission guidelines ( https:
//nips.cc/public/guides/CodeSubmissionPolicy ) for more details.
•The authors should provide instructions on data access and preparation, including how
to access the raw data, preprocessed data, intermediate data, and generated data, etc.
•The authors should provide scripts to reproduce all experimental results for the new
proposed method and baselines. If only a subset of experiments are reproducible, they
should state which ones are omitted from the script and why.
•At submission time, to preserve anonymity, the authors should release anonymized
versions (if applicable).
•Providing as much information as possible in supplemental material (appended to the
paper) is recommended, but including URLs to data and code is permitted.
6.Experimental Setting/Details
Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?
Answer: [Yes]
Justification: The details necessary to interpret our experimental results are provided in
Section 5, while additional details necessary to reproduce the experiments are given in
Appendix E.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The experimental setting should be presented in the core of the paper to a level of detail
that is necessary to appreciate the results and make sense of them.
•The full details can be provided either with the code, in appendix, or as supplemental
material.
7.Experiment Statistical Significance
Question: Does the paper report error bars suitably and correctly defined or other appropriate
information about the statistical significance of the experiments?
Answer: [NA]
Justification: Our experiments consider only deterministic optimization methods.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The authors should answer "Yes" if the results are accompanied by error bars, confi-
dence intervals, or statistical significance tests, at least for the experiments that support
the main claims of the paper.
35•The factors of variability that the error bars are capturing should be clearly stated (for
example, train/test split, initialization, random drawing of some parameter, or overall
run with given experimental conditions).
•The method for calculating the error bars should be explained (closed form formula,
call to a library function, bootstrap, etc.)
• The assumptions made should be given (e.g., Normally distributed errors).
•It should be clear whether the error bar is the standard deviation or the standard error
of the mean.
•It is OK to report 1-sigma error bars, but one should state it. The authors should
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
of Normality of errors is not verified.
•For asymmetric distributions, the authors should be careful not to show in tables or
figures symmetric error bars that would yield results that are out of range (e.g. negative
error rates).
•If error bars are reported in tables or plots, The authors should explain in the text how
they were calculated and reference the corresponding figures or tables in the text.
8.Experiments Compute Resources
Question: For each experiment, does the paper provide sufficient information on the computer
resources (type of compute workers, memory, time of execution) needed to reproduce the
experiments?
Answer: [Yes]
Justification: We detail the compute resources used in our paper in Appendix F.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The paper should indicate the type of compute workers CPU or GPU, internal cluster,
or cloud provider, including relevant memory and storage.
•The paper should provide the amount of compute required for each of the individual
experimental runs as well as estimate the total compute.
•The paper should disclose whether the full research project required more compute
than the experiments reported in the paper (e.g., preliminary or failed experiments that
didn’t make it into the paper).
9.Code Of Ethics
Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines ?
Answer: [Yes]
Justification: All authors are familiar with the code of ethics and conform to its principles.
Moreover, our research is primarily theoretical and is of minor ethical concern.
Guidelines:
•The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
•If the authors answer No, they should explain the special circumstances that require a
deviation from the Code of Ethics.
•The authors should make sure to preserve anonymity (e.g., if there is a special consid-
eration due to laws or regulations in their jurisdiction).
10.Broader Impacts
Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?
Answer: [NA]
Justification: Our research is primarily theoretical and has no societal impact beyond the
impact of general advances in the fields of machine learning and optimization.
Guidelines:
• The answer NA means that there is no societal impact of the work performed.
36•If the authors answer NA or No, they should explain why their work has no societal
impact or why the paper does not address societal impact.
•Examples of negative societal impacts include potential malicious or unintended uses
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
(e.g., deployment of technologies that could make decisions that unfairly impact specific
groups), privacy considerations, and security considerations.
•The conference expects that many papers will be foundational research and not tied
to particular applications, let alone deployments. However, if there is a direct path to
any negative applications, the authors should point it out. For example, it is legitimate
to point out that an improvement in the quality of generative models could be used to
generate deepfakes for disinformation. On the other hand, it is not needed to point out
that a generic algorithm for optimizing neural networks could enable people to train
models that generate Deepfakes faster.
•The authors should consider possible harms that could arise when the technology is
being used as intended and functioning correctly, harms that could arise when the
technology is being used as intended but gives incorrect results, and harms following
from (intentional or unintentional) misuse of the technology.
•If there are negative societal impacts, the authors could also discuss possible mitigation
strategies (e.g., gated release of models, providing defenses in addition to attacks,
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
feedback over time, improving the efficiency and accessibility of ML).
11.Safeguards
Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?
Answer: [NA]
Justification: No new data or models are released as part of this paper.
Guidelines:
• The answer NA means that the paper poses no such risks.
•Released models that have a high risk for misuse or dual-use should be released with
necessary safeguards to allow for controlled use of the model, for example by requiring
that users adhere to usage guidelines or restrictions to access the model or implementing
safety filters.
•Datasets that have been scraped from the Internet could pose safety risks. The authors
should describe how they avoided releasing unsafe images.
•We recognize that providing effective safeguards is challenging, and many papers do
not require this, but we encourage authors to take this into account and make a best
faith effort.
12.Licenses for existing assets
Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?
Answer: [Yes]
Justification: All libraries, models, and data sources are appropriately referenced.
Guidelines:
• The answer NA means that the paper does not use existing assets.
• The authors should cite the original paper that produced the code package or dataset.
•The authors should state which version of the asset is used and, if possible, include a
URL.
• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
•For scraped data from a particular source (e.g., website), the copyright and terms of
service of that source should be provided.
37•If assets are released, the license, copyright information, and terms of use in the
package should be provided. For popular datasets, paperswithcode.com/datasets
has curated licenses for some datasets. Their licensing guide can help determine the
license of a dataset.
•For existing datasets that are re-packaged, both the original license and the license of
the derived asset (if it has changed) should be provided.
•If this information is not available online, the authors are encouraged to reach out to
the asset’s creators.
13.New Assets
Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?
Answer: [NA]
Justification: No new assets are introduced.
Guidelines:
• The answer NA means that the paper does not release new assets.
•Researchers should communicate the details of the dataset/code/model as part of their
submissions via structured templates. This includes details about training, license,
limitations, etc.
•The paper should discuss whether and how consent was obtained from people whose
asset is used.
•At submission time, remember to anonymize your assets (if applicable). You can either
create an anonymized URL or include an anonymized zip file.
14.Crowdsourcing and Research with Human Subjects
Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as
well as details about compensation (if any)?
Answer: [NA]
Justification: We did not use any crowdsourcing for this work
Guidelines:
•The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
•Including this information in the supplemental material is fine, but if the main contribu-
tion of the paper involves human subjects, then as much detail as possible should be
included in the main paper.
•According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
or other labor should be paid at least the minimum wage in the country of the data
collector.
15.Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects
Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?
Answer: [NA]
Justification: IRB approval was not required for this paper.
Guidelines:
•The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
•Depending on the country in which research is conducted, IRB approval (or equivalent)
may be required for any human subjects research. If you obtained IRB approval, you
should clearly state this in the paper.
38•We recognize that the procedures for this may vary significantly between institutions
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
guidelines for their institution.
•For initial submissions, do not include any information that would break anonymity (if
applicable), such as the institution conducting the review.
39