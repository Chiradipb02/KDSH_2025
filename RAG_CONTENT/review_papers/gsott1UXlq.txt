Transparent Networks for Multivariate Time Series
Anonymous Author(s)
Affiliation
Address
email
Abstract
Transparent models, which are machine learning models that produce inherently 1
interpretable predictions, are receiving significant attention in high-stakes domains. 2
However, despite much real-world data being collected as time series, there is a lack 3
of studies on transparent time series models. To address this gap, we propose a novel 4
transparent neural network model for time series called Generalized Additive Time 5
Series Model (GATSM). GATSM consists of two parts: 1) independent feature 6
networks to learn feature representations, and 2) a transparent temporal module to 7
learn temporal patterns across different time steps using the feature representations. 8
This structure allows GATSM to effectively capture temporal patterns and handle 9
dynamic-length time series while preserving transparency. Empirical experiments 10
show that GATSM significantly outperforms existing generalized additive models 11
and achieves comparable performance to black-box time series models, such as 12
recurrent neural networks and Transformer. In addition, we demonstrate that 13
GATSM finds interesting patterns in time series. The source code is available at 14
https://anonymous.4open.science/r/GATSM-78F4/ . 15
1 Introduction 16
Artificial neural networks excel at learning complex representations and demonstrate remarkable 17
predictive performance across various fields. However, their complexity makes interpreting the 18
decision-making processes of neural network models challenging. Consequently, post-hoc explainable 19
artificial intelligence (XAI) methods, which explain the predictions of trained black-box models, 20
have been widely studied in recent years [ 1,2,3,4]. XAI methods are generally effective at 21
providing humans with understandable explanations of model predictions. However, they may 22
produce incorrect and unfaithful explanations of the underlying black-box model and cannot provide 23
actual contributions of input features to model predictions [ 5,6]. Therefore, their applicability to 24
high-stakes domains-such as healthcare and fraud detection, where faithfulness to the underlying 25
model and actual contributions of features are important-is limited. 26
Due to these limitations, transparent (i.e., inherently interpretable) models are attracting attention as 27
alternatives to XAI in high-stakes domains [7, 8, 9]. Modern transparent models typically adhere to 28
thegeneralized additive model (GAM) framework [ 10]. A GAM consists of independent functions, 29
each corresponding to an input feature, and makes predictions as a linear combination of these 30
functions (e.g., the sum of all functions). Therefore, each function reflects the contribution of its 31
respective feature. For this reason, interpreting GAMs is straightforward, making them widely used in 32
various fields, such as healthcare [ 11,12], survival analysis [ 13], and model bias discovery [ 7,14,15]. 33
However, despite much real-world data being collected as time series, research on GAMs for time 34
series remains scarce. Consequently, the applicability of GAMs in real-world scenarios is still limited. 35
To overcome this limitation, we propose a novel transparent model for multivariate time series 36
called Generalized Additive Time Series Model (GATSM). GATSM consists of independent feature 37
networks to learn feature representations and a transparent temporal module to learn temporal patterns. 38
Submitted to 38th Conference on Neural Information Processing Systems (NeurIPS 2024). Do not distribute.Since employing distinct networks across different time steps requires a massive amount of learnable 39
parameters, the feature networks in GATSM share the weights across all time steps, while the 40
temporal module independently learns temporal patterns. GATSM then generates final predictions by 41
integrating the feature representations with the temporal information from the temporal module. This 42
strategy allows GATSM to effectively capture temporal patterns and handle dynamic-length time 43
series while preserving transparency. Additionally, this approach facilitates the separate extraction of 44
time-independent feature contributions, the importance of individual time steps, and time-dependent 45
feature contributions through the feature functions, temporal module, and final prediction. To 46
demonstrate the effectiveness of GATSM, we conducted empirical experiments on various time series 47
datasets. The experimental results show that GATSM significantly outperforms existing GAMs 48
and achieves comparable performances to black-box time series models, such as recurrent neural 49
networks and Transformer [ 16]. In addition, we provide visualizations of GATSM’s predictions to 50
demonstrate that GATSM finds interesting patterns in time series. 51
2 Related Works 52
Various XAI studies have been conducted over the past decade [ 7,8,9,17,18]; however, they are 53
less relevant to the transparent model that is the subject of this study. Therefore, we refer readers to 54
[19,20] for more detailed information on recent XAI research. In this section, we review existing 55
transparent models closely related to our GATSM and discuss their limitations. 56
Table 1: Advantages of GATSM.
Time series input Temporal pattern Dynamic time series
existing GAMs
NATM ✓
GATSM (our) ✓ ✓ ✓
The simple linear model is designed to fit the conditional expectation g(E(y|x)) =PM
i=1xiwi, 57
where g(·)is a link function, Mindicates the number of input features, yis the target value for the 58
given input features x∈RM, andwi∈Ris the learnable weight for xi. This model captures only 59
linear relationships between the target yand the inputs x. To address this limitation, GAM [ 10] 60
extends the simple linear model to the generalized form as follows: 61
g(E(y|x)) =MX
i=1fi(xi), (1)
where each fi(·)is a function that models the effect of a single feature, referred as a feature function. 62
Typically, fi(·)becomes a non-linear function such as a decision tree or neural network to capture 63
non-linear relationships. 64
Originally, GAMs were fitted via the backfitting algorithm using smooth splines [ 10,21]. Later, Yin 65
Lou et al. [22] and Harsha Nori et al. [23] have proposed boosted decision tree-based GAMs, which 66
use boosted decision trees as feature functions. Spline- and tree-based GAMs have less flexibility 67
and scalability. Thus, extending them to transfer or multi-task learning is challenging. To overcome 68
this problem, various neural network-based GAMs have been proposed in recent years. Potts [24] 69
introduced generalized additive neural network, which employs 2-layer neural networks as feature 70
functions. Similarly, Rishabh Agarwal et al. [7]proposed neural additive model (NAM) that employs 71
multi-layer neural networks. To improve the scalability of NAM, Chun-Hao Chang et al. [8]and 72
Filip Radenovic et al. [9]proposed the neural oblivious tree-based GAM and the basis network-based 73
GAM, respectively. Xu et al. [25] introduced a sparse version of NAM using the group LASSO. One 74
disadvantage of GAMs is their limited predictive power, which stems from the fact that they only 75
learn first-order feature interactions-i.e., relationships between the target value and individual features. 76
To address this, various studies have been conducted to enhance the predictive powers of GAMs by 77
incorporating higher-order feature interactions, while still maintaining transparency. GA2M [26] 78
simply takes pairwise features as inputs to learn pairwise interactions. GAMI-Net [ 27], a neural 79
network-based GAM, consists of networks for main effects (i.e., first-order interactions) and pairwise 80
interactions. To enhance the interpretability of GAMI-Net, the sparsity and heredity constraints are 81
added, and trivial features are pruned in the training process. Sparse interaction additive network [ 28] 82
2𝑥!,!ℎ#𝑥!,!ℎ!𝑥!,!ℎ$𝑥!,!…𝑥!,#ℎ#𝑥!,#ℎ!𝑥!,#ℎ$𝑥!,#…𝑥!,%ℎ#𝑥!,%ℎ!𝑥!,%ℎ$𝑥!,%……𝑤!,!#$%𝑤!,&#$%𝑤!,'#$%𝑤!,!#$%𝑤!,&#$%𝑤!,'#$%𝑤!,!#$%𝑤!,&#$%𝑤!,'#$%𝑥#!,!𝑥#!,#𝑥#!,%
: shared basesTime-Sharing NBMT steps
Time Sharing NBMInput Time SeriesTransformed Time SeriesMasked MHAAttention MapOutput
GATSM𝒙%&𝒙%'LinearLinearSoftmaxMasked MHAPositional EncodingConcatLinearFigure 1: Architecture of GATSM.
is a 3-phase method for exploiting higher-order interactions. Initially, a black-box neural network is 83
trained; subsequently, the top- kimportant features are identified using explainable feature attribution 84
methods like LIME [ 1] and SHAP [ 2], and finally, NAM is trained with these extracted features. 85
Dubey et al. [29] introduced scalable polynomial additive model, an end-to-end model that learns 86
higher-order interactions via polynomials. Similarly, Kim et al. [15] proposed higher-order NAM that 87
utilizes the feature crossing technique to capture higher-order interactions. Despite their capabilities, 88
the aforementioned GAMs cannot process time series data, which limits their applicability in real- 89
world scenarios. Recently, neural additive time series Model (NATM) [ 30], a time-series adaptation 90
of NAM, has been proposed. However, NATM handles each time step independently with separate 91
feature networks. This approach cannot capture effective temporal patterns and only takes fixed-length 92
time series as input. Our GATSM not only captures temporal patterns but also handles dynamic-length 93
time series. Table 1 shows the advantages of our GATSM compared to existing GAMs. 94
3 Problem Statement 95
We tackle the problem of the existing GAMs on time series. Equation (1) outlines the GAM framework 96
for tabular data, which fails to capture the interactions between current and previous observations in 97
time series. A straightforward method to extend GAM to time series, adopted in NATM, is applying 98
distinct feature functions to each time step and summing them to produce predictions: 99
g(E(yt|X:t)) =tX
i=1MX
j=1fi,j(xi,j), (2)
where X∈RT×Mis a time series with Ttime steps and Mfeatures, and tis the current time step. 100
This method can handle time series data as input but fails to capture effective temporal patterns 101
because the function fi,j(·)still does not interact with previous time steps. To overcome this problem, 102
3we suggest a new form of GAM for time series defined as follows: 103
g(E(yt|X:t)) =tX
i=1MX
j=1fi,j(xi,j,X:t). (3)
Definition 3.1 GAMs for time series, which capture temporal patterns hold the form of Equation 3 . 104
In Equation (3), the function f(·,·)can capture interactions between current and previous time steps. 105
Therefore, GAMs adhering to Definition 3.1 are capable of capturing temporal patterns. However, 106
implementing such a model while maintaining transparency poses challenges. In the following 107
section, we will describe our approach to implementing a GAM that holds Definition 3.1. To the best 108
of our knowledge, no existing literature addresses Definition 3.1. 109
4 Our Method: Generalized Additive Time Series Model 110
4.1 Architecture 111
Figure 1 shows the overall architecture of GATSM. Our model has two modules: 1) feature networks, 112
called time-sharing neural basis model, for learning feature representations, and 2) masked multi-head 113
attention for learning temporal patterns. 114
Time-Sharing NBM: Assume a time series with Ttime steps and Mfeatures. Applying GAMs 115
to this time series necessitates T×Mfeature functions, which becomes problematic when dealing 116
with large TorMdue to increased model size. This limits the applicability of GAMs to real-world 117
datasets. To overcome this problem, we extend neural basis model (NBM) [9] to time series as: 118
˜xi,j=fj(xi,j) =BX
k=1hk(xi,j)wnbm
j,k. (4)
We refer to this extended version of NBM as time-sharing NBM. Time-sharing NBM has Bbasis 119
functions, with each basis hk(·)taking a feature xi,jas input. The feature-specific weight wnbm
j,k120
then projects the basis to the transformed feature ˜xi,j. As depicted in Equation 4, the basis functions 121
are shared across all features and time steps, drastically reducing the number of required feature 122
functions T×MtoB. We use B= 100 and implement hk(·)using multi-layer perceptron (MLP). 123
Masked MHA: GATSM employs multi-head attention (MHA) to learn temporal patterns. Although 124
the dot product attention [ 16] is popular, simple dot operation has low expressive power [ 31]. 125
Therefore, we adopt the 2-layer attention mechanism proposed by [ 31] to GATSM. We first transform 126
˜xi= [˜xi,1,˜xi,2,···,˜xi,M]∈RMproduced by Equation 4 as follows: 127
vi=˜x⊺
iZ+pei, (5)
where Z∈RM×Dis a learnable weight, pei= [pei,1, pei,2,···, pei,D]∈RDis the positional 128
encoding for i-th step, and Dindicates the hidden size. The positional encoding is defined as follows: 129
pei,j=sin i
100002j/D
ifjmod2 = 1 ,
cos i
100002j/D
otherwise .(6)
The positional encoding helps GATSM effectively capture temporal patterns. While learnable position 130
embedding also works in GATSM, we recommend positional encoding because position embedding 131
requires knowledge of the maximum number of time steps, which is often unknown in real-world 132
settings. After computing vi, we calculate the attention scores as follows: 133
ek,i,j=σ 
[vi|vj]⊺wattn
k
mi,j, (7)
ak,i,j=exp(ek,i,j)PT
t=1exp(ek,i,t), (8)
where kis attention head index, σ(·)is an activation function, wattn
k∈R2D, and mi,j∈Ris the 134
mask value used to block future information. The time mask is defined as follows: 135
mi,j=1 ifi≤j,
−∞ otherwise .(9)
4Inference: The prediction of GATSM is produced by combining the transformed features from 136
time-sharing NBM with the attention scores from masked MHA. 137
ˆyt=KX
k=1a⊺
k,t˜Xwout
k, (10)
where Kis the number of attention heads, ak,t= [ak,i,1, ak,i,2,···, ak,i,T]∈RTis the attention 138
map in Equation 8, ˜X= [˜x1,˜x2,···,˜xT]∈RT×Mis the transformed features in Equation 4, and 139
wout
k∈RMis the learnable output weight. 140
Interpretability: We can rewrite Equation 10 as the following scalar form: 141
KX
k=1a⊺
k,t˜Xwout
k=tX
u=1MX
m=1KX
k=1BX
b=1ak,t,uhb(xt,m)wnbm
m,bwout
k,m
=tX
u=1MX
m=1fu,m(xu,m,X:t)(11)
Equation 11 shows that GATSM satisfying Definition 3.1. We can derive three types of interpretations 142
from GATSM: 1) ak,t,u indicates the importance of time step uat time step t, 2)hb(xt,m)wnbm
m,bwout
k,m143
represents the time-independent contribution of feature m, and 3) ak,t,uhb(xt,m)wnbm
m,bwout
k,mrepre- 144
sents the time-dependent contribution of feature mat time step t. 145
5 Experiments 146
5.1 Experimental Setup 147
Datasets: We conducted our experiments using eight publicly available real-world time series 148
datasets. From the Monash repository [ 32], we sourced three datasets: Energy, Rainfall, and 149
AirQuality. Another three datasets, Heartbeat, LSST, and NATOPS, were downloaded from the 150
UCR repository [ 33]. The remaining two datasets, Mortality and Sepsis, were downloaded from 151
the PhysioNet [ 34]. We perform ordinal encoding for categorical features and standardize features 152
to have zero-mean and unit-variance. For forecasting tasks, target value y is also standardized to 153
zero-mean and unit-variance. If the dataset contains missing values, we impute categorical features 154
with their modes and numerical features with their means. The dataset is split into a 60%/20%/20% 155
ratio for training, validation, and testing, respectively. Table 2 shows the statistics of the experimental 156
datasets. Further details of the experimental datasets can be found in Appendix B. 157
Table 2: Dataset statistics.
Dataset Task Variable length # of time series Avg. length # of features # of classes
Energy 1-step FCST No 137 24 24 -
Rainfall 1-step FCST No 160,267 24 3 -
AirQuality 1-step FCST No 16,966 24 9 -
Heartbeat Binary No 409 405 61 2
Mortality Binary Yes 12,000 49.861 41 2
Sepsis Binary Yes 40,336 38.482 40 2
LSST Multi-class No 4,925 36 6 14
NATOPS Multi-class No 360 51 24 6
FCST: forecasting
Baselines: We compare our GATSM with 12 baselines, which can be categorized into four groups: 1) 158
Black-box tabular models include extreme gradient boosting (XGBoost) [ 35] and MLP. 2) Black-box 159
time series models include simple recurrent neural network (RNN), gated recurrent unit (GRU), long 160
short-term memory (LSTM), and Transformer [ 16]. 3) Transparent tabular models are simple linear 161
model (Linear), explainable boosting machine (EBM) [ 23], NAM [ 7], NodeGAM [ 8], and NBM [ 9]. 162
4) NATM [30] is a transparent time series model. 163
Implementation: We implement XGBoost and EBM models using the xgboost andinterpretml 164
libraries, respectively. For NodeGAM, we employ the official implementation provided by its authors 165
[8]. The remaining models are developed using PyTorch [ 36]. All models undergo hyperparameter 166
5Table 3: Predictive performance comparison of various models.
Model Type Model Energy Rainfall AirQuality Heartbeat Mortality Sepsis LSST NATOPS Avg. Rank
Black-box
Tabular ModelXGBoost0.094 0.002 0.532 0.679 0.707 0.816 0.424 0.200 8.500
(±0.137) ( ±0.002) ( ±0.019) ( ±0.094) ( ±0.015) ( ±0.007) ( ±0.012) ( ±0.049) (±4.000)
MLP0.459 0.011 0.423 0.654 0.842 0.786 0.417 0.211 7.375
(±0.101) ( ±0.004) ( ±0.031) ( ±0.082) ( ±0.014) ( ±0.007) ( ±0.008) ( ±0.065) (±2.134)
Black-box
Time Series ModelRNN0.320 0.068 0.644 0.661 0.581 0.782 0.422 0.592 7.750
(±0.122) ( ±0.020) ( ±0.032) ( ±0.078) ( ±0.040) ( ±0.009) ( ±0.029) ( ±0.110) (±2.712)
GRU0.435 0.089 0.701 0.694 0.818 0.785 0.629 0.931 4.375
(±0.107) ( ±0.034) ( ±0.018) ( ±0.052) ( ±0.014) ( ±0.010) ( ±0.013) ( ±0.045) (±2.669)
LSTM0.359 0.090 0.683 0.648 0.790 0.779 0.491 0.908 6.375
(±0.112) ( ±0.031) ( ±0.026) ( ±0.042) ( ±0.020) ( ±0.008) ( ±0.082) ( ±0.035) (±3.623)
Transformer0.263 0.098 0.711 0.690 0.844 0.789 0.679 0.967 4.000
(±0.263) ( ±0.035) ( ±0.027) ( ±0.040) ( ±0.019) ( ±0.010) ( ±0.019) ( ±0.029) (±3.703)
Transparent
Tabular ModelLinear0.482 0.004 0.241 0.637 0.838 0.723 0.311 0.206 10.125
(±0.112) ( ±0.001) ( ±0.019) ( ±0.070) ( ±0.017) ( ±0.011) ( ±0.010) ( ±0.045) (±3.871)
EBM-0.200 0.004 0.324 0.666 0.729 0.802 0.408 0.164 9.750
(±0.409) ( ±0.001) ( ±0.014) ( ±0.056) ( ±0.017) ( ±0.011) ( ±0.016) ( ±0.053) (±3.284)
NAM0.363 0.006 0.300 0.645 0.853 0.800 0.400 0.242 7.875
(±0.218) ( ±0.002) ( ±0.013) ( ±0.026) ( ±0.014) ( ±0.006) ( ±0.011) ( ±0.040) (±3.643)
NodeGAM0.398 0.006 0.380 0.681 0.854 0.802 0.400 0.247 6.375
(±0.195) ( ±0.002) ( ±0.032) ( ±0.046) ( ±0.013) ( ±0.007) ( ±0.028) ( ±0.012) (±3.623)
NBM0.330 0.007 0.301 0.716 0.852 0.799 0.388 0.189 7.875
(±0.251) ( ±0.003) ( ±0.012) ( ±0.039) ( ±0.014) ( ±0.006) ( ±0.014) ( ±0.029) (±3.603)
Transparent
Time Series ModelNATM0.304 0.038 0.548 0.724N/A N/A0.452 0.878 5.667
(±0.122) ( ±0.011) ( ±0.028) ( ±0.043) ( ±0.010) ( ±0.058) (±2.582)
GATSM (ours)0.493 0.073 0.583 0.843 0.853 0.797 0.570 0.956 3.125
(±0.173) ( ±0.027) ( ±0.026) ( ±0.025) ( ±0.015) ( ±0.007) ( ±0.024) ( ±0.027) (±1.808)
tuning via Optuna [ 37]. The pytorch-based models are optimized with the Adam with decoupled 167
weight decay (AdamW) [ 38] optimizer on an NVIDIA A100 GPU. Model training is halted if the 168
validation loss does not decrease over 20 epochs. We use mean squared error for the forecasting tasks, 169
and for classification tasks, we use cross-entropy loss. Further details of the model implementations 170
and hyper-parameters are provided in Appendix C. 171
5.2 Comparison with baselines 172
Table 3 shows the predictive performances of the experimental models. We report mean scores 173
and standard deviations over five different random seeds. For the forecasting datasets, we evaluate 174
R2scores. For the binary classification datasets, we assess the area under the receiver operating 175
characteristic curve (AUROC). For the multi-class classification datasets, we measure accuracy. We 176
highlight the best-performing model in bold andunderline the second-best model. Since the tabular 177
models cannot handle time series, they only take xtto produce yt. 178
On the Energy and Heartbeat datasets, which are small in size, our GATSM demonstrates the best 179
performance, indicating strong generalization ability. EBM, XGBoost, and Transformer struggle 180
with overfitting on the Energy dataset. For the Mortality and Sepsis datasets, there is no significant 181
performance difference between tabular and time series models, nor between black-box and trans- 182
parent models. This suggests that these two healthcare datasets lack significant temporal patterns 183
and feature interactions. It is likely that seasonal patterns are hard to detect in medical data, and 184
the patient’s current condition already encapsulates previous conditions, making historical data less 185
crucial. Since these datasets contain variable-length time series, the performance of NATM, which 186
can only handle fixed-length time series, is not available. On the Rainfall, AirQuality, LSST, and 187
NATOPS datasets, the time series models significantly outperform the tabular models, indicating 188
that these datasets contain important temporal patterns that tabular models cannot capture. Addition- 189
ally, the black-box models outperform the transparent models, suggesting that these datasets have 190
higher-order feature interactions that transparent models cannot capture. Nevertheless, GATSM is the 191
best model within the transparent model group and performs comparably to Transformer. Overall, 192
GATSM achieved the best average rank in the experiments, followed by the Transformer, indicating 193
GATSM’s superiority. Additional experiments on model throughput and an ablation study on the 194
basis functions are presented in Appendix D. 195
6Table 4: Ablation study on different feature functions.
Feature Function Energy Rainfall AirQuality Heartbeat Mortality Sepsis LSST NATOPS
Linear 0.283(±0.277) 0.071( ±0.024) 0.563( ±0.019) 0.766( ±0.024) 0.832( ±0.015) 0.735( ±0.012) 0.398( ±0.030) 0.972 (±0.020)
NAM 0.304(±0.229) 0.068( ±0.021) 0.564( ±0.019) 0.838( ±0.032) 0.851( ±0.013) 0.801 (±0.005) 0.553( ±0.023) 0.933( ±0.039)
NBM 0.493 (±0.173) 0.073 (±0.027) 0.583 (±0.026) 0.843 (±0.025) 0.853 (±0.015) 0.797( ±0.007) 0.570 (±0.024) 0.956( ±0.027)
Table 5: Ablation study on the temporal module.
Temporal Module Energy Rainfall AirQuality Heartbeat Mortality Sepsis LSST NATOPS
Base 0.452(±0.087) 0.007( ±0.002) 0.299( ±0.012) 0.661( ±0.043) 0.854 (±0.013) 0.798( ±0.008) 0.392( ±0.006) 0.192( ±0.027)
Base + PE 0.397(±0.054) 0.007( ±0.003) 0.299( ±0.012) 0.681( ±0.068) 0.852( ±0.013) 0.799 (±0.007) 0.385( ±0.027) 0.228( ±0.029)
Base + MHA 0.368(±0.230) 0.048( ±0.017) 0.555( ±0.020) 0.821( ±0.044) 0.847( ±0.020) 0.779( ±0.033) 0.595 (±0.013) 0.856( ±0.059)
Base + PE + MHA 0.493 (±0.173) 0.073 (±0.027) 0.583 (±0.026) 0.843 (±0.025) 0.853( ±0.015) 0.797( ±0.007) 0.570( ±0.024) 0.956 (±0.027)
5.3 Ablation study 196
Choice of feature function: We evaluate the performance of GATSM by changing the feature 197
functions using three models: Linear, NAM, and NBM. Table 4 presents the results of this experiment. 198
The simple linear function performs poorly because it lacks the capability to capture non-linear 199
relationships. In contrast, NAM, which can capture non-linearity, shows improved performance over 200
the linear function. However, NBM stands out by achieving the best performance in six out of eight 201
datasets. This indicates that the basis strategy of NBM is highly effective for time series data. 202
Design of temporal module: We evaluate the performance of GATSM by modifying the design of 203
the temporal module. The results are presented in Table 5. GATSM without the temporal module 204
(Base) fails to learn temporal patterns and shows poor performance in the experiment. GATSM with 205
only positional encoding (Base + PE) also shows similar performance to the Base, indicating that 206
positional encoding alone is insufficient for capturing effective temporal patterns. GATSM with only 207
multi-head attention (Base + MHA) outperforms the previous two methods, demonstrating that the 208
MHA mechanism is beneficial for capturing temporal patterns. Finally, our full GATSM (Base + PE + 209
MHA) significantly outperforms the other methods, suggesting that the combination of PE and MHA 210
creates a synergistic effect. Consistent with our previous findings in section 5.2, all four methods 211
show similar performances on the Mortality and Sepsis datasets, which lack significant temporal 212
patterns. 213
5.4 Interpretation 214
In this section, we visualize four interpretations of GATSM’s predictions on the AirQuality dataset. 215
In addition, interpretations for the Rainfall dataset can be found in Appendix E. 216
0 10 20
Time steps0.030.040.050.060.07Avg. attention score
Figure 2: Average attention
scores of time steps on the
AirQuality dataset.
0 200.000.050.10SO2
0 50.00.10.2NO2
0 50.00.20.4CO
0 100.000.050.10O3
2.5
 0.0 2.50.050
0.025
0.0000.025temperature
2.5
 0.0 2.50.05
0.000.05pressure
2.5
 0.00.0000.0020.004dew point
0 500.6
0.4
0.2
0.0rainfall
0 50.00.10.2windspeed
Feature valueFeature contribution
Figure 3: Global interpretations of features in the Air Quality dataset.
70 10 200.0200
0.0175
0.0150
0.0125
0.0100
0.0075
0.0050
0.0025
0.0000SO2
0 10 200.03
0.02
0.01
0.000.010.020.030.04NO2
0 10 200.08
0.07
0.06
0.05
0.04
0.03
0.02
0.01
0.00CO
0 10 200.02
0.000.020.040.060.08O3
0 10 200.015
0.010
0.005
0.0000.0050.0100.015temperature
0 10 200.04
0.02
0.000.020.04pressure
0 10 200.0015
0.0010
0.0005
0.00000.00050.00100.0015dew point
0 10 200.014
0.012
0.010
0.008
0.006
0.004
0.002
0.000rainfall
0 10 200.008
0.006
0.004
0.002
0.0000.002windspeed0.6
0.5
0.4
0.3
0.2
0.1
0.0
1.00
0.75
0.50
0.25
0.000.250.500.751.00
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0.0
1
01234
1.5
1.0
0.5
0.00.51.01.5
1.5
1.0
0.5
0.00.51.01.5
1.5
1.0
0.5
0.00.51.01.5
0.08
0.07
0.06
0.05
0.04
0.03
0.02
0.01
0.00
1.25
1.00
0.75
0.50
0.25
0.000.250.50
Time stepFeature contribution
Feature valueFigure 4: Local time-independent feature contributions.
0 10 200.20
0.15
0.10
0.05
0.00SO2
0 10 200.10
0.05
0.000.050.10NO2
0 10 200.7
0.6
0.5
0.4
0.3
0.2
0.1
0.0CO
0 10 200.1
0.00.10.20.30.40.5O3
0 10 200.10
0.05
0.000.050.10temperature
0 10 200.4
0.3
0.2
0.1
0.00.10.20.30.4pressure
0 10 200.015
0.010
0.005
0.0000.0050.0100.015dew point
0 10 200.12
0.10
0.08
0.06
0.04
0.02
0.00rainfall
0 10 200.06
0.04
0.02
0.000.02windspeed0.6
0.5
0.4
0.3
0.2
0.1
0.0
1.0
0.5
0.00.51.0
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0.0
1
012345
1.5
1.0
0.5
0.00.51.01.5
1.5
1.0
0.5
0.00.51.01.5
1.5
1.0
0.5
0.00.51.01.5
0.08
0.07
0.06
0.05
0.04
0.03
0.02
0.01
0.00
1.25
1.00
0.75
0.50
0.25
0.000.250.50
Time stepFeature contribution
Feature value
Figure 5: Local time-dependent feature contributions.
8Time-step importance: We plot the average attention scores at the last time step Tin Figure 2. 217
The process for extracting the average attention score of time step uat time step tis formalized as 218PK
k=1ak,t,u. This process is repeated over all data samples, and the results are averaged. Based 219
on Figure 2, it seems that GATSM pays more attention to the initial and last states than to the 220
intermediate states. This indicates that the current concentration of particulate matter depends on the 221
initial state. 222
Global feature contribution: Figure 3 illustrates the global behavior of features in the 223
AirQuality dataset, with red bars indicating the density of training samples. We extract 224PK
k=1hb(xt,m)wnbm
m,bwout
k,mfrom GATSM and repeat this process over the range of minimum to 225
maximum feature values to plot the line. We found that the behavior of SO2,O3, and windspeed is 226
inconsistent with prior human knowledge. Typically, high levels of SO2 andO3are associated with 227
poor air quality. However, GATSM learned that particulate matter concentration starts to decrease 228
when SO2 exceeds 10 and O3exceeds 5. This discrepancy may be due to sparse training samples in 229
these regions, leading to insufficient training, or there may be interactions with other features. Another 230
known fact is that high windspeed decreases particulate matter concentration. This is consistent when 231
windspeed is below 0.7 in our observation. However, particulate matter concentration drastically 232
increases when windspeed exceeds 0.7, likely due to the wind causing yellow dust. 233
Local time-independent feature contribution: To interpret the prediction of a data sample, we 234
plot the local time-independent feature contributions,PK
k=1hb(xt,m)wnbm
m,bwout
k,m, in Figure 4. The 235
main x-axis (blue) represents feature contribution, the sub x-axis (red) represents feature value, and 236
the y-axis represents time steps. We found that SO2,NO2 ,CO, and O3have positive correlations. 237
In contrast, temperature ,pressure ,dew point , and windspeed have negative correlations. These are 238
consistent with the global interpretations shown in Figure 3. Rainfall has the same values across all 239
time steps. 240
Local time-dependent feature contribution: We also visualize the local time-dependent feature con- 241
tributions,PK
k=1ak,t,uhb(xt,m)wnbm
m,bwout
k,m. Figure 5 illustrates the interpretation of the same data 242
sample as in Figure 4. The time-dependent interpretation differs slightly from the time-independent 243
interpretation. We found that there are time lags in SO2,NO2,CO, and O3, meaning previous feature 244
values affect current feature contributions. For example, in the case of SO2, low feature values around 245
time step 5 lead to low feature contributions around time step 13. 246
6 Future Works & Conclusion 247
Although GATSM achieved state-of-the-art performance within the transparent model category, 248
it has several limitations. This section discusses these limitations and suggests future work to 249
address them. GAMs have relatively slower computational times and larger model sizes compared to 250
black-box models because they require the same number of feature functions as input features. To 251
address this problem, methods such as the basis strategy can be proposed to reduce the number of 252
feature functions, or entirely new methods for transparent models can be developed. The attention 253
mechanism in GATSM may be a bottleneck. Fast attention mechanisms proposed in the literature 254
[39,40,41,42,43], or the recently proposed Mamba [ 44], can help overcome this limitation. Existing 255
time series models, including GATSM, only handle discrete time series and have limited length 256
generalization ability, resulting in significantly reduced performance when very long sequences, 257
unseen during training, are input. Extending GATSM to continuous models using NeuralODE [ 45] 258
or HiPPO [ 46] could address this issue. GATSM still cannot learn higher-order feature interactions 259
internally and shows low performance on complex datasets. Feature interaction methods proposed 260
for transparent models may help address this problem [29, 15]. 261
In this papre, we proposed a novel transparent model for time series named GATSM. GATSM 262
consists of time-sharing NBM and the temporal module to effectively learn feature representations 263
and temporal patterns while maintaining transparency. The experimental results demonstrated that 264
GATSM has superior generalization ability and is the only transparent model with performance 265
comparable to Transformer. We provided various visual interpretations of GATSM, demonstrated that 266
GATSM capture interesting patterns in time series data. We anticipate that GATSM will be widely 267
adopted in various fields and demonstrate strong performance. The broader impacts of GATSM 268
across various fields can be found in Appendix A. 269
9References 270
[1]Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. "Why Should I Trust You?": Explain- 271
ing the Predictions of Any Classifier. In ACM SIGKDD International Conference on Knowledge 272
Discovery and Data Mining , 2016. 273
[2]Scott M. Lundberg and Su-In Lee. A Unified Approach to Interpreting Model Predictions. In 274
Advances in Neural Information Processing Systems , 2017. 275
[3]Ramprasaath R. Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi 276
Parikh, and Dhruv Batra. Grad-CAM: Visual Explanations From Deep Networks via Gradient- 277
Based Localization. 2017. 278
[4]Ramaravind K. Mothilal, Amit Sharma, and Chenhao Tan. Explaining Machine Learning 279
Classifiers through Diverse Counterfactual Explanations. In Proceedings of the 2020 Conference 280
on Fairness, Accountability, and Transparency , 2020. 281
[5]Cynthia Rudin. Please Stop Explaining Black Box Models for High Stakes Decisions. In 282
Advances in Neural Information Processing Systems, Workshop on Critiquing and Correcting 283
Trends in Machine Learning , 2018. 284
[6]Cynthia Rudin. Stop explaining black box machine learning models for high stakes decisions 285
and use interpretable models instead. Nature Machine Intelligence , 1:206–215, May 2019. 286
[7]Rishabh Agarwal, Levi Melnick, Nicholas Frosst, Xuezhou Zhang, Ben Lengerich, Rich 287
Caruana, and Geoffrey E. Hinton. Neural Additive Models: Interpretable Machine Learning 288
with Neural Nets. In Advances in Neural Information Processing Systems , 2021. 289
[8]Chun-Hao Chang, Rich Caruana, and Anna Goldenberg. NODE-GAM: Neural Generalized 290
Additive Model for Interpretable Deep Learning. In International Conference on Learning 291
Representations , 2022. 292
[9]Filip Radenovic, Abhimanyu Dubey, and Dhruv Mahajan. Neural Basis Models for Inter- 293
pretability. In Advances in Neural Information Processing Systems , 2022. 294
[10] Trevor Hastie and Robert Tibshirani. Generalized Additive Models. Statistical Science , 1(3): 295
297–318, August 1986. 296
[11] Rich Caruana, Yin Lou, Johannes Gehrke, Paul Koch, Marc Sturm, and Noémie Elhadad. Intel- 297
ligible Models for HealthCare: Predicting Pneumonia Risk and Hospital 30-day Readmission. 298
InACM SIGKDD International Conference on Knowledge Discovery and Data Mining , 2015. 299
[12] Chun-Hao Chang, Sarah Tan, Ben Lengerich, Anna Goldenberg, and Rich Caruana. How 300
Interpretable and Trustworthy are GAMs? In ACM SIGKDD International Conference on 301
Knowledge Discovery and Data Mining , 2021. 302
[13] Lev V . Utkin, Egor D. Satyukov, and Andrei V . Konstantinov. SurvNAM: The machine learning 303
survival model explanation. Neural Networks , 147:81–102, March 2022. 304
[14] Sarah Tan, Rich Caruana, Giles Hooker, and Yin Lou. Distill-and-Compare: Auditing Black- 305
Box Models Using Transparent Model Distillation. In Proceedings of the 2018 AAAI/ACM 306
Conference on AI, Ethics, and Society , 2018. 307
[15] Minkyu Kim, Hyun-Soo Choi, and Jinho Kim. Higher-order Neural Additive Models: An Inter- 308
pretable Machine Learning Model with Feature Interactions. arXiv preprint arXiv:2209.15409 , 309
2022. 310
[16] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, 311
Łukasz Kaiser, and Illia Polosukhin. Attention Is All You Need. In Advances in Neural 312
Information Processing Systems , 2017. 313
[17] Sebastian Bach, Alexander Binder, Grégoire Montavon, Frederick Klauschen, Klaus-Robert 314
Müller, and Wojciech Samek. On Pixel-Wise Explanations for Non-Linear Classifier Decisions 315
by Layer-Wise Relevance Propagation. PLoS ONE , 10(7), July 2015. 316
[18] Avanti Shrikumar, Peyton Greenside, and Anshul Kundaje. Learning Important Features 317
Through Propagating Activation Differences. In International Conference on Machine Learning , 318
2017. 319
[19] Sajid Ali, Tamer Abuhmed, Shaker El-Sappagh, Khan Muhammad, Jose M. Alonso-Moral, 320
Roberto Confalonieri, Riccardo Guidotti, Javier Del Ser, Natalia Díaz-Rodríguez, and Francisco 321
10Herrera. Explainable Artificial Intelligence (XAI): What we know and what is left to attain 322
Trustworthy Artificial Intelligence. Information Fusion , 99:101805, November 2023. 323
[20] Vikas Hassija, Vinay Chamola, Atmesh Mahapatra, Abhinandan Singal, Divyansh Goel, Kaizhu 324
Huang, Simone Scardapane, Indro Spinelli, Mufti Mahmud, and Amir Hussain. Interpreting 325
Black-Box Models: A Review on Explainable Artificial Intelligence. Cognitive Computation , 326
16(1):45–74, January 2024. 327
[21] Grace Wahba. Spline Models for Observational Data . SIAM, September 1990. 328
[22] Yin Lou, Rich Caruana, and Johannes Gehrke. Intelligible Models for Classification and 329
Regression. In ACM SIGKDD International Conference on Knowledge Discovery and Data 330
Mining , 2012. 331
[23] Harsha Nori, Samuel Jenkins, Paul Koch, and Rich Caruana. InterpretML: A Unified Framework 332
for Machine Learning Interpretability. arXiv preprint arXiv:1909.09223 , 2019. 333
[24] William J. E. Potts. Generalized Additive Neural Networks. In ACM SIGKDD International 334
Conference on Knowledge Discovery and Data Mining , 1999. 335
[25] Shiyun Xu, Zhiqi Bu, Pratik Chaudhari, and Ian J. Barnett. Sparse Neural Additive Model: 336
Interpretable Deep Learning with Feature Selection via Group Sparsity. In Joint European 337
Conference on Machine Learning and Knowledge Discovery in Databases , 2023. 338
[26] Yin Lou, Rich Caruana, Johannes Gehrke, and Giles Hooker. Accurate intelligible models with 339
pairwise interactions. In ACM SIGKDD International Conference on Knowledge Discovery and 340
Data Mining , 2013. 341
[27] Zebin Yang, Aijun Zhang, and Agus Sudjianto. GAMI-Net: An Explainable Neural Network 342
based on Generalized Additive Models with Structured Interactions. Pattern Recognition , 120: 343
108192, December 2021. 344
[28] James Enouen and Yan Liu. Sparse Interaction Additive Networks via Feature Interaction 345
Detection and Sparse Selection. Advances in Neural Information Processing Systems , 35, 2022. 346
[29] Abhimanyu Dubey, Filip Radenovic, and Dhruv Mahajan. Scalable Interpretability via Polyno- 347
mials. Advances in Neural Information Processing Systems , 2022. 348
[30] Wonkeun Jo and Dongil Kim. Neural additive time-series models: Explainable deep learning 349
for multivariate time-series prediction. Expert Systems with Applications , 228:120307, October 350
2023. 351
[31] Petar Veli ˇckovi ´c, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Liò, and Yoshua 352
Bengio. Graph Attention Networks. In International Conference on Learning Representations , 353
2018. 354
[32] Chang Wei Tan, Christoph Bergmeir, Francois Petitjean, and Geoffrey I. Webb. Monash Univer- 355
sity, UEA, UCR Time Series Extrinsic Regression Archive. arXiv preprint arXiv:2006.10996 , 356
2020. 357
[33] Anthony Bagnall, Hoang Anh Dau, Jason Lines, Michael Flynn, James Large, Aaron Bostrom, 358
Paul Southam, and Eamonn Keogh. The UEA multivariate time series classification archive, 359
2018. arXiv preprint arXiv:1811.00075 , 2018. 360
[34] Ary L. Goldberger, Luis A. N. Amaral, Leon Glass, Jeffrey M. Hausdorff, Plamen Ch. Ivanov, 361
Roger G. Mark, Joseph E. Mietus, George B. Moody, Chung-Kang Peng, and H. Eugene Stanley. 362
PhysioBank, PhysioToolkit, and PhysioNet. Circulation , 101(23):e215–e220, June 2000. 363
[35] Tianqi Chen and Carlos Guestrin. XGBoost: A Scalable Tree Boosting System. In Proceedings 364
of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , 365
2016. 366
[36] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, 367
Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas 368
Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, 369
Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. PyTorch: An Imperative Style, 370
High-Performance Deep Learning Library. In Advances in Neural Information Processing 371
Systems , 2019. 372
[37] Takuya Akiba, Shotaro Sano, Toshihiko Yanase, Takeru Ohta, and Masanori Koyama. Optuna: 373
A Next-generation Hyperparameter Optimization Framework. In Proceedings of the 25th ACM 374
SIGKDD International Conference on Knowledge Discovery & Data Mining , 2019. 375
11[38] Ilya Loshchilov and Frank Hutter. Decoupled Weight Decay Regularization. In International 376
Conference on Learning Representations , 2019. 377
[39] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and François Fleuret. Transformers are 378
RNNs: Fast Autoregressive Transformers with Linear Attention. In International Conference 379
on Machine Learning , 2020. 380
[40] Lovish Madaan, Srinadh Bhojanapalli, Himanshu Jain, and Prateek Jain. Treeformer: Dense 381
Gradient Trees for Efficient Attention Computation. In International Conference on Learning 382
Representations , 2023. 383
[41] Sinong Wang, Belinda Z. Li, Madian Khabsa, Han Fang, and Hao Ma. Linformer: Self-Attention 384
with Linear Complexity. arXiv preprint arXiv:2006.04768 , 2020. 385
[42] Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, 386
Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, David Belanger, 387
Lucy Colwell, and Adrian Weller. Rethinking Attention with Performers. In International 388
Conference on Learning Representations , 2021. 389
[43] Nikita Kitaev, Łukasz Kaiser, and Anselm Levskaya. Reformer: The Efficient Transformer. In 390
International Conference on Learning Representations , 2020. 391
[44] Albert Gu and Tri Dao. Mamba: Linear-Time Sequence Modeling with Selective State Spaces, 392
2023. 393
[45] Ricky T. Q. Chen, Yulia Rubanova, Jesse Bettencourt, and David K Duvenaud. Neural Ordinary 394
Differential Equations. In Advances in Neural Information Processing Systems , 2018. 395
[46] Albert Gu, Tri Dao, Stefano Ermon, Atri Rudra, and Christopher Ré. HiPPO: Recurrent Memory 396
with Optimal Polynomial Projections. In Advances in Neural Information Processing Systems , 397
2020. 398
[47] Eric J. Pedersen, David L. Miller, Gavin L. Simpson, and Noam Ross. Hierarchical generalized 399
additive models in ecology: an introduction with mgcv. PeerJ , 7:e6876, May 2019. 400
[48] Trevor Hastie and Robert Tibshirani. Generalized additive models for medical research. Statisti- 401
cal Methods in Medical Research , 4(3):187–196, September 1995. 402
[49] Appliances Energy Dataset, 2020. URL https://doi.org/10.5281/zenodo.3902637 . 403
[50] Australia Rainfall Dataset, 2020. URL https://doi.org/10.5281/zenodo.3902654 . 404
[51] Beijing PM10 Dataset, 2020. URL https://doi.org/10.5281/zenodo.3902667 . 405
[52] Classification of Heart Sound Recordings: The PhysioNet/Computing in Cardiology Challenge 406
2016, 2016. URL https://physionet.org/content/challenge-2016/1.0.0/ . 407
[53] Predicting Mortality of ICU Patients: The PhysioNet/Computing in Cardiology Challenge 2012, 408
2012. URL https://physionet.org/content/challenge-2012/1.0.0/ . 409
[54] Early Prediction of Sepsis from Clinical Data: The PhysioNet/Computing in Cardiology Chal- 410
lenge 2019, 2019. URL https://physionet.org/content/challenge-2019/1.0.0/ . 411
[55] PLAsTiCC Astronomical Classification, 2018. URL https://www.kaggle.com/c/ 412
PLAsTiCC-2018 . 413
[56] AALTD’16 Time Series Classification Contest, 2016. URL https://aaltd16.irisa.fr/ 414
challenge/ . 415
[57] Ignacio Oguiza. tsai - a state-of-the-art deep learning library for time series and sequential data. 416
Github, 2023. URL https://github.com/timeseriesAI/tsai . 417
[58] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. ImageNet Classification with Deep 418
Convolutional Neural Networks. In Advances in Neural Information Processing Systems , 2012. 419
[59] Saining Xie, Ross Girshick, Piotr Dollar, Zhuowen Tu, and Kaiming He. Aggregated Residual 420
Transformations for Deep Neural Networks. In Proceedings of the IEEE Conference on 421
Computer Vision and Pattern Recognition , 2017. 422
[60] James Bergstra, Rémi Bardenet, Yoshua Bengio, and Balázs Kégl. Algorithms for Hyper- 423
Parameter Optimization. In Advances in Neural Information Processing Systems , 2011. 424
[61] Tri Dao, Daniel Y . Fu, Stefano Ermon, Atri Rudra, and Christopher Ré. FlashAttention: Fast 425
and Memory-Efficient Exact Attention with IO-Awareness. arXiv preprint arXiv:2205.14135 , 426
2022. 427
12NeurIPS Paper Checklist 428
1.Claims 429
Question: Do the main claims made in the abstract and introduction accurately reflect the 430
paper’s contributions and scope? 431
Answer: [Yes] 432
Justification: The main claims made in the abstract and introduction accurately reflect the 433
paper’s contributions and scope. 434
Guidelines: 435
•The answer NA means that the abstract and introduction do not include the claims 436
made in the paper. 437
•The abstract and/or introduction should clearly state the claims made, including the 438
contributions made in the paper and important assumptions and limitations. A No or 439
NA answer to this question will not be perceived well by the reviewers. 440
•The claims made should match theoretical and experimental results, and reflect how 441
much the results can be expected to generalize to other settings. 442
•It is fine to include aspirational goals as motivation as long as it is clear that these goals 443
are not attained by the paper. 444
2.Limitations 445
Question: Does the paper discuss the limitations of the work performed by the authors? 446
Answer: [Yes] 447
Justification: The limitations of our work are described in section 6. 448
Guidelines: 449
•The answer NA means that the paper has no limitation while the answer No means that 450
the paper has limitations, but those are not discussed in the paper. 451
• The authors are encouraged to create a separate "Limitations" section in their paper. 452
•The paper should point out any strong assumptions and how robust the results are to 453
violations of these assumptions (e.g., independence assumptions, noiseless settings, 454
model well-specification, asymptotic approximations only holding locally). The authors 455
should reflect on how these assumptions might be violated in practice and what the 456
implications would be. 457
•The authors should reflect on the scope of the claims made, e.g., if the approach was 458
only tested on a few datasets or with a few runs. In general, empirical results often 459
depend on implicit assumptions, which should be articulated. 460
•The authors should reflect on the factors that influence the performance of the approach. 461
For example, a facial recognition algorithm may perform poorly when image resolution 462
is low or images are taken in low lighting. Or a speech-to-text system might not be 463
used reliably to provide closed captions for online lectures because it fails to handle 464
technical jargon. 465
•The authors should discuss the computational efficiency of the proposed algorithms 466
and how they scale with dataset size. 467
•If applicable, the authors should discuss possible limitations of their approach to 468
address problems of privacy and fairness. 469
•While the authors might fear that complete honesty about limitations might be used by 470
reviewers as grounds for rejection, a worse outcome might be that reviewers discover 471
limitations that aren’t acknowledged in the paper. The authors should use their best 472
judgment and recognize that individual actions in favor of transparency play an impor- 473
tant role in developing norms that preserve the integrity of the community. Reviewers 474
will be specifically instructed to not penalize honesty concerning limitations. 475
3.Theory Assumptions and Proofs 476
Question: For each theoretical result, does the paper provide the full set of assumptions and 477
a complete (and correct) proof? 478
Answer: [NA] 479
13Justification: Our work does not include theoretical results. 480
Guidelines: 481
• The answer NA means that the paper does not include theoretical results. 482
•All the theorems, formulas, and proofs in the paper should be numbered and cross- 483
referenced. 484
•All assumptions should be clearly stated or referenced in the statement of any theorems. 485
•The proofs can either appear in the main paper or the supplemental material, but if 486
they appear in the supplemental material, the authors are encouraged to provide a short 487
proof sketch to provide intuition. 488
•Inversely, any informal proof provided in the core of the paper should be complemented 489
by formal proofs provided in appendix or supplemental material. 490
• Theorems and Lemmas that the proof relies upon should be properly referenced. 491
4.Experimental Result Reproducibility 492
Question: Does the paper fully disclose all the information needed to reproduce the main ex- 493
perimental results of the paper to the extent that it affects the main claims and/or conclusions 494
of the paper (regardless of whether the code and data are provided or not)? 495
Answer: [Yes] 496
Justification: We provided experimental setup and implementation details in section 5.1 and 497
Appendix C. 498
Guidelines: 499
• The answer NA means that the paper does not include experiments. 500
•If the paper includes experiments, a No answer to this question will not be perceived 501
well by the reviewers: Making the paper reproducible is important, regardless of 502
whether the code and data are provided or not. 503
•If the contribution is a dataset and/or model, the authors should describe the steps taken 504
to make their results reproducible or verifiable. 505
•Depending on the contribution, reproducibility can be accomplished in various ways. 506
For example, if the contribution is a novel architecture, describing the architecture fully 507
might suffice, or if the contribution is a specific model and empirical evaluation, it may 508
be necessary to either make it possible for others to replicate the model with the same 509
dataset, or provide access to the model. In general. releasing code and data is often 510
one good way to accomplish this, but reproducibility can also be provided via detailed 511
instructions for how to replicate the results, access to a hosted model (e.g., in the case 512
of a large language model), releasing of a model checkpoint, or other means that are 513
appropriate to the research performed. 514
•While NeurIPS does not require releasing code, the conference does require all submis- 515
sions to provide some reasonable avenue for reproducibility, which may depend on the 516
nature of the contribution. For example 517
(a)If the contribution is primarily a new algorithm, the paper should make it clear how 518
to reproduce that algorithm. 519
(b)If the contribution is primarily a new model architecture, the paper should describe 520
the architecture clearly and fully. 521
(c)If the contribution is a new model (e.g., a large language model), then there should 522
either be a way to access this model for reproducing the results or a way to reproduce 523
the model (e.g., with an open-source dataset or instructions for how to construct 524
the dataset). 525
(d)We recognize that reproducibility may be tricky in some cases, in which case 526
authors are welcome to describe the particular way they provide for reproducibility. 527
In the case of closed-source models, it may be that access to the model is limited in 528
some way (e.g., to registered users), but it should be possible for other researchers 529
to have some path to reproducing or verifying the results. 530
5.Open access to data and code 531
Question: Does the paper provide open access to the data and code, with sufficient instruc- 532
tions to faithfully reproduce the main experimental results, as described in supplemental 533
material? 534
14Answer: [Yes] 535
Justification: We used public datasets and opened our code. 536
Guidelines: 537
• The answer NA means that paper does not include experiments requiring code. 538
•Please see the NeurIPS code and data submission guidelines ( https://nips.cc/ 539
public/guides/CodeSubmissionPolicy ) for more details. 540
•While we encourage the release of code and data, we understand that this might not be 541
possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not 542
including code, unless this is central to the contribution (e.g., for a new open-source 543
benchmark). 544
•The instructions should contain the exact command and environment needed to run to 545
reproduce the results. See the NeurIPS code and data submission guidelines ( https: 546
//nips.cc/public/guides/CodeSubmissionPolicy ) for more details. 547
•The authors should provide instructions on data access and preparation, including how 548
to access the raw data, preprocessed data, intermediate data, and generated data, etc. 549
•The authors should provide scripts to reproduce all experimental results for the new 550
proposed method and baselines. If only a subset of experiments are reproducible, they 551
should state which ones are omitted from the script and why. 552
•At submission time, to preserve anonymity, the authors should release anonymized 553
versions (if applicable). 554
•Providing as much information as possible in supplemental material (appended to the 555
paper) is recommended, but including URLs to data and code is permitted. 556
6.Experimental Setting/Details 557
Question: Does the paper specify all the training and test details (e.g., data splits, hyper- 558
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the 559
results? 560
Answer: [Yes] 561
Justification: We described the experimental setting in section 5.1. 562
Guidelines: 563
• The answer NA means that the paper does not include experiments. 564
•The experimental setting should be presented in the core of the paper to a level of detail 565
that is necessary to appreciate the results and make sense of them. 566
•The full details can be provided either with the code, in appendix, or as supplemental 567
material. 568
7.Experiment Statistical Significance 569
Question: Does the paper report error bars suitably and correctly defined or other appropriate 570
information about the statistical significance of the experiments? 571
Answer: [Yes] 572
Justification: We provided standard deviations with experimental results. 573
Guidelines: 574
• The answer NA means that the paper does not include experiments. 575
•The authors should answer "Yes" if the results are accompanied by error bars, confi- 576
dence intervals, or statistical significance tests, at least for the experiments that support 577
the main claims of the paper. 578
•The factors of variability that the error bars are capturing should be clearly stated (for 579
example, train/test split, initialization, random drawing of some parameter, or overall 580
run with given experimental conditions). 581
•The method for calculating the error bars should be explained (closed form formula, 582
call to a library function, bootstrap, etc.) 583
• The assumptions made should be given (e.g., Normally distributed errors). 584
•It should be clear whether the error bar is the standard deviation or the standard error 585
of the mean. 586
15•It is OK to report 1-sigma error bars, but one should state it. The authors should 587
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis 588
of Normality of errors is not verified. 589
•For asymmetric distributions, the authors should be careful not to show in tables or 590
figures symmetric error bars that would yield results that are out of range (e.g. negative 591
error rates). 592
•If error bars are reported in tables or plots, The authors should explain in the text how 593
they were calculated and reference the corresponding figures or tables in the text. 594
8.Experiments Compute Resources 595
Question: For each experiment, does the paper provide sufficient information on the com- 596
puter resources (type of compute workers, memory, time of execution) needed to reproduce 597
the experiments? 598
Answer: [Yes] 599
Justification: We provided information on the computational resource used in the experi- 600
ments. 601
Guidelines: 602
• The answer NA means that the paper does not include experiments. 603
•The paper should indicate the type of compute workers CPU or GPU, internal cluster, 604
or cloud provider, including relevant memory and storage. 605
•The paper should provide the amount of compute required for each of the individual 606
experimental runs as well as estimate the total compute. 607
•The paper should disclose whether the full research project required more compute 608
than the experiments reported in the paper (e.g., preliminary or failed experiments that 609
didn’t make it into the paper). 610
9.Code Of Ethics 611
Question: Does the research conducted in the paper conform, in every respect, with the 612
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines ? 613
Answer: [Yes] 614
Justification: Our work conform with the NeurIPS Code of Ethics. 615
Guidelines: 616
•The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. 617
•If the authors answer No, they should explain the special circumstances that require a 618
deviation from the Code of Ethics. 619
•The authors should make sure to preserve anonymity (e.g., if there is a special consid- 620
eration due to laws or regulations in their jurisdiction). 621
10.Broader Impacts 622
Question: Does the paper discuss both potential positive societal impacts and negative 623
societal impacts of the work performed? 624
Answer: [Yes] 625
Justification: We discussed the potential impacts of GATSM in Appendix A. 626
Guidelines: 627
• The answer NA means that there is no societal impact of the work performed. 628
•If the authors answer NA or No, they should explain why their work has no societal 629
impact or why the paper does not address societal impact. 630
•Examples of negative societal impacts include potential malicious or unintended uses 631
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations 632
(e.g., deployment of technologies that could make decisions that unfairly impact specific 633
groups), privacy considerations, and security considerations. 634
16•The conference expects that many papers will be foundational research and not tied 635
to particular applications, let alone deployments. However, if there is a direct path to 636
any negative applications, the authors should point it out. For example, it is legitimate 637
to point out that an improvement in the quality of generative models could be used to 638
generate deepfakes for disinformation. On the other hand, it is not needed to point out 639
that a generic algorithm for optimizing neural networks could enable people to train 640
models that generate Deepfakes faster. 641
•The authors should consider possible harms that could arise when the technology is 642
being used as intended and functioning correctly, harms that could arise when the 643
technology is being used as intended but gives incorrect results, and harms following 644
from (intentional or unintentional) misuse of the technology. 645
•If there are negative societal impacts, the authors could also discuss possible mitigation 646
strategies (e.g., gated release of models, providing defenses in addition to attacks, 647
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from 648
feedback over time, improving the efficiency and accessibility of ML). 649
11.Safeguards 650
Question: Does the paper describe safeguards that have been put in place for responsible 651
release of data or models that have a high risk for misuse (e.g., pretrained language models, 652
image generators, or scraped datasets)? 653
Answer: [NA] 654
Justification: Our work poses no such risks. 655
Guidelines: 656
• The answer NA means that the paper poses no such risks. 657
•Released models that have a high risk for misuse or dual-use should be released with 658
necessary safeguards to allow for controlled use of the model, for example by requiring 659
that users adhere to usage guidelines or restrictions to access the model or implementing 660
safety filters. 661
•Datasets that have been scraped from the Internet could pose safety risks. The authors 662
should describe how they avoided releasing unsafe images. 663
•We recognize that providing effective safeguards is challenging, and many papers do 664
not require this, but we encourage authors to take this into account and make a best 665
faith effort. 666
12.Licenses for existing assets 667
Question: Are the creators or original owners of assets (e.g., code, data, models), used in 668
the paper, properly credited and are the license and terms of use explicitly mentioned and 669
properly respected? 670
Answer: [Yes] 671
Justification: We properly cited the used codes and data. 672
Guidelines: 673
• The answer NA means that the paper does not use existing assets. 674
• The authors should cite the original paper that produced the code package or dataset. 675
•The authors should state which version of the asset is used and, if possible, include a 676
URL. 677
• The name of the license (e.g., CC-BY 4.0) should be included for each asset. 678
•For scraped data from a particular source (e.g., website), the copyright and terms of 679
service of that source should be provided. 680
•If assets are released, the license, copyright information, and terms of use in the 681
package should be provided. For popular datasets, paperswithcode.com/datasets 682
has curated licenses for some datasets. Their licensing guide can help determine the 683
license of a dataset. 684
•For existing datasets that are re-packaged, both the original license and the license of 685
the derived asset (if it has changed) should be provided. 686
17•If this information is not available online, the authors are encouraged to reach out to 687
the asset’s creators. 688
13.New Assets 689
Question: Are new assets introduced in the paper well documented and is the documentation 690
provided alongside the assets? 691
Answer: [Yes] 692
Justification: We opened the source code of GATSM, and the document to run the code is 693
provided along with the code. 694
Guidelines: 695
• The answer NA means that the paper does not release new assets. 696
•Researchers should communicate the details of the dataset/code/model as part of their 697
submissions via structured templates. This includes details about training, license, 698
limitations, etc. 699
•The paper should discuss whether and how consent was obtained from people whose 700
asset is used. 701
•At submission time, remember to anonymize your assets (if applicable). You can either 702
create an anonymized URL or include an anonymized zip file. 703
14.Crowdsourcing and Research with Human Subjects 704
Question: For crowdsourcing experiments and research with human subjects, does the paper 705
include the full text of instructions given to participants and screenshots, if applicable, as 706
well as details about compensation (if any)? 707
Answer: [NA] 708
Justification: Our work does not involve crowdsourcing nor research with human subjects. 709
Guidelines: 710
•The answer NA means that the paper does not involve crowdsourcing nor research with 711
human subjects. 712
•Including this information in the supplemental material is fine, but if the main contribu- 713
tion of the paper involves human subjects, then as much detail as possible should be 714
included in the main paper. 715
•According to the NeurIPS Code of Ethics, workers involved in data collection, curation, 716
or other labor should be paid at least the minimum wage in the country of the data 717
collector. 718
15.Institutional Review Board (IRB) Approvals or Equivalent for Research with Human 719
Subjects 720
Question: Does the paper describe potential risks incurred by study participants, whether 721
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) 722
approvals (or an equivalent approval/review based on the requirements of your country or 723
institution) were obtained? 724
Answer: [NA] 725
Justification: Our work does not involve crowdsourcing nor research with human subjects. 726
Guidelines: 727
•The answer NA means that the paper does not involve crowdsourcing nor research with 728
human subjects. 729
•Depending on the country in which research is conducted, IRB approval (or equivalent) 730
may be required for any human subjects research. If you obtained IRB approval, you 731
should clearly state this in the paper. 732
•We recognize that the procedures for this may vary significantly between institutions 733
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the 734
guidelines for their institution. 735
•For initial submissions, do not include any information that would break anonymity (if 736
applicable), such as the institution conducting the review. 737
18A Broader impact 738
We discuss the expected impacts of GATSM across various fields. 739
•Time series adaptation: GATSM extends existing GAMs to time series, enabling tasks that 740
traditional GAMs could not perform in this context - e.g., better performance on time series and 741
finding temporal patterns. 742
•Improved decision-making system: GATSM can show users their exact decision-making process, 743
providing trust and confidence in its predictions to users. This enables decision-makers to make 744
more informed choices, crucial in high-stakes domains such as healthcare. 745
•Ethical AI: GATSM can examine that their outcomes are biased or discriminatory by displaying 746
the shape of feature functions. This is particularly important in ethically sensitive domains, such as 747
recidivism prediction. 748
•Scientific discovery: Transparent models have already been used in various research fields for 749
scientific discovery [ 47,48]. GATSM also can be applied to these domains to obtain novel scientific 750
insights. 751
Despite these advantages, it is important to remember that the interpretations of transparent models 752
do not necessarily reflect exact causal relationships. While transparent models provide clear and 753
faithful interpretations, they are still not capable of identifying causal relationships. Causal discovery 754
is a complex task that requires further research. 755
B Dataset details 756
We use eight publicly available datasets for our experiments. Three datasets - Energy, Rainfall, and 757
AirQuality - can be downloaded from the Monash repository [ 32]. Another three datasets - Heartbeat, 758
LSST, and NATOPS - are available from the UCR repository [ 33]. The remaining two datasets can 759
be downloaded from the PhysioNet [34]. Details of the datasets are provided below: 760
•Energy [49]: This dataset consists of 24 features related to temperature and humidity from sensors 761
and weather conditions. These features are measured every 10 minutes. The goal of this dataset is 762
to predict total energy usage. 763
•Rainfall [50]: This dataset consists of temperatures measured hourly. The goal of this dataset is to 764
predict total daily rainfall in Australia. 765
•AirQuality [51]: This dataset consists of features related to air pollutants and meteorological data. 766
The goal of this dataset is to predict the PM10 level in Beijing. 767
•Heartbeat [52]: This dataset consists of heart sounds collected from various locations on the body. 768
Each sound was truncated to five seconds, and a spectrogram of each instance was created with a 769
window size of 0.061 seconds with a 70% overlap. The goal of this dataset is to classify the sounds 770
as either normal or abnormal. 771
•Mortality [53] This dataset consists of records of adult patients admitted to the ICU. The input 772
features include the patient demographics, vital signs, and lab results. The goal of this dataset is to 773
predict the in-hospital death of patients. 774
•Sepsis [54]: This dataset consists of records of ICU patients. The input features include patient 775
demographics, vital signs, and lab results. The goal of this dataset is to predict sepsis six hours in 776
advance at every time step. 777
•LSST [55]: This challenge dataset aims to classify astronomical time series. These time series 778
consist of six different light curves, simulated based on the data expected from the Large Synoptic 779
Survey Telescope (LSST). 780
•NATOPS [56]: This dataset aims to classify the Naval Air Training and Operating Procedures 781
Standardization (NATOPS) motions used to control aircraft movements. It consists of 24 features 782
representing the x, y, and z coordinates for each of the eight sensor locations attached to the body. 783
We used get_UCR_data() andget_Monash_regression_data() functions in the tsai library 784
[57] to load the UCR and Monash datasets. 785
19Table 6: Optimal hyper-parameters for GATSM.
GATSM: [256, 256, 128] hidden dims, 100 basis functions
Dataset Batch Size NBM Batch Norm. NBM Dropout Attn. Embedding Size Attn. Heads Attn. Dropout Learning Rate Weight Decay
Energy 32 False 2.315e-1 110 8 6.924e-2 4.950e-3 1.679e-3
Rainfall 32,768 False 5.936e-3 44 7 1.215e-3 9.225e-3 2.204e-6
AirQuality 4,096 False 2.340e-2 81 8 1.169e-1 6.076e-3 5.047e-6
Heartbeat 64 True 1.749e-1 92 2 1.653e-1 8.061e-3 4.787e-6
Mortality 512 False 7.151e-2 125 8 7.324e-1 7.304e-3 2.181e-4
Sepsis 512 True 6.523e-2 90 6 8.992e-1 4.509e-3 2.259e-2
LSST 1,024 False 2.500e-2 59 7 2.063e-1 5.561e-2 5.957e-3
NATOPS 64 True 4.827e-3 49 8 7.920e-1 8.156e-3 2.748e-2
C Implementation details 786
We use 13 models, including GATSM, for our experiments. We implement XGBoost and EBM 787
using the xgboost [35] and interpretml [23] libraries, respectively. For NodeGAM, we employ 788
the official implementation provided by its authors [ 8]. The remaining models are developed using 789
PyTorch [ 36]. In addition, we implement the feature functions in NAM and NBM using grouped 790
convolutions [ 58,59] to enhance their efficiency. XGBoost and EBM are trained on two AMD EPYC 791
7513 CPUs, while the other models are trained on an NVIDIA A100 GPU with 80GB VRAM. All 792
models undergo hyperparameter tuning via Optuna [ 37] with the Tree-structured Parzen Estimator 793
(TPE) algorithm [ 60] in 100 trials. The hyperparameter search space and the optimal hyperparameters 794
for the models are provided below: 795
•XGBoost: We tune the n_estimators in the integer interval [1, 1000], max_depth in the integer 796
interval [0, 2000], learning rate in the continuous interval [1e-6, 1], subsample in the continuous 797
interval [0, 1], and colsample_bytree in the continuous interval [0, 1]. 798
•MLP, NAM, NBM and NATM: We tune the batchnorm in the descret set {False, True}, dropout 799
in the continuous interval [0, 0.9], learning_rate in the continuous interval [1e-3, 1e-2], and 800
weight_decay in the continuous interval [1e-6, 1e-1] on a log scale. 801
•RNN, GRU and LSTM: We tune the hidden_size in the integer interval [8, 128], dropout 802
in the continuous interval [0, 0.9], learning_rate in the continuous interval [1e-3, 1e-2], and 803
weight_decay in the continuous interval [1e-6, 1e-1] on a log scale. 804
•Transformer: We tune the n_layers in the integer interval [1, 4], emb_size in the integer 805
interval [8, 32], hidden_size in the integer interval [8, 128], n_heads in the integer interval [1, 806
8],dropout in the continuous interval [0, 0.9], learning_rate in the continuous interval [1e-3, 807
1e-2], and weight_decay in the continuous interval [1e-6, 1e-1] on a log scale. 808
•Linear: We tune the learning_rate in the continuous interval [1e-3, 1e-2], and weight_decay 809
in the continuous interval [1e-6, 1e-1] on a log scale. 810
•EBM: We tune max_bins in the integer interval [8, 512], min_samples_leaf andmax_leaves 811
in the integer interval [1, 50], inner_bags andouter_bags in the integer interval [1, 128], 812
learning_rate in the continuous interval [1e-6, 100] on a log scale, and max_rounds in the 813
integer interval [1000, 10000]. 814
•NodeGAM: We tune n_trees in the integer interval [1, 256], n_layers anddepth in the integer 815
intervals [1, 4], dropout in the continuous interval [0, 0.9], learning_rate in the continuous 816
interval [1e-3, 1e-2], and weight_decay in the continuous interval [1e-6, 1e-1] on a log scale. 817
•GATSM: We tune nbm_batchnorm in the descret set {False, True}, nbm_dropout in the con- 818
tinuous interval [0, 0.9], attn_emb_size in the integer interval [8, 128], attn_n_heads in the 819
integer interval [1, 8], attn_dropout in the continuous interval [0, 0.9], learning_rate in the 820
continuous interval [1e-3, 1e-2], and weight_decay in the continuous interval [1e-6, 1e-1] on a 821
log scale. The optimal hyper-parameters for GATSM across all experimental datasets are provided 822
in Table 6. 823
20D Additional experiments 824
D.1 Inference speed 825
The inference speed of machine learning models is a crucial metric for real-world systems. We 826
evaluate the throughput of various models. The results are presented in Table 7. Since the datasets 827
have fewer features than the number of basis functions in NBM, NAM achieves higher throughput 828
than NBM. Transparent tabular models typically exhibit fast speeds. However, their throughput 829
significantly decreases in datasets with many features, such as Heartbeat, Mortality, and Sepsis, 830
because they require the same number of feature functions as the number of input features. Trans- 831
former shows higher throughput than the transparent time series models because it does not require 832
feature functions, which are the main bottleneck of transparent models. Additionally, the PyTorch 833
implementation of Transformer uses the flash attention mechanism [ 61] to enhance its efficiency. 834
NATM has slightly higher throughput than GATSM, as it does not require the attention mechanism 835
and has fewer feature functions compared to the number of basis functions in GATSM. 836
Table 7: Inference throughput of different models.
Energy Rainfall AirQuality Heartbeat Mortality Sepsis LSST NATOPS
NAM 65.3K 1.8M 5.1M 139.1K 772.2K 23.9K 2.3M 147.9K
NBM 45.5K 1.1M 1.0M 55.9K 375.8K 6.5K 1.6M 85.6K
Transformer 30.9K 240.5K 174.2K 15.7K 161.9K 134.6K 214.4K 68.3K
NATM 5.3K 699.3K 241.3K 1.3K N/A N/A 28.6K 19.2K
GATSM 6.1K 350.6K 192.8K 1.2K 4.9K 3.8K 126.5K 12.5K
D.2 Number of basis functions 837
We evaluate GATSM by varying the number of basis functions in the time-sharing NBM. The results 838
for forecasting, binary classification, and multi-class classification datasets are presented in Figure 6. 839
For the Sepsis dataset, using 200 and 300 basis functions causes the out-of-memory error. For the 840
Energy and Heartbeat datasets, performance improves up to 100 basis functions but shows no further 841
benefit when the number of bases exceeds 100. In other datasets, performance changes are not 842
significant with different numbers of basis functions. In addition, there is a trade-off between the 843
number of basis functions and computational speed. Therefore, we recommend generally setting the 844
number of basis functions to 100. Note that the performance of GATSM with this hyper-parameter 845
depends on the dataset size and complexity. Hence, a larger number of basis functions may benefit 846
more complex datasets. 847
10 50 100 200 300
Number of bases0.10.20.30.40.50.6R2
Energy
Rainfall
AirQuality
(a) Forecasting
10 50 100 200 300
Number of bases0.790.800.810.820.830.840.850.86AUROC
Heartbeat
Mortality
Sepsis (b) Binary
10 50 100 200 300
Number of bases0.60.70.80.9Accuracy
LSST
NATOPS (c) Multi-class
Figure 6: Performances of GATSM on the different number of basis functions.
E Additional visualizations 848
In addition to the interpretations on the AirQuality dataset in section 5.4, we present another interesting 849
interpretations of GATSM on the Rainfall dataset. 850
Time-step importance: Figure 7 illustrates the average importance of all time steps at the final time 851
step. The importance exhibit a cyclical pattern of rising and falling at regular intervals, indicating 852
that GATSM effectively captures seasonal patterns in the Rainfall dataset. 853
21Global feature contribution: Figure 8 illustrates the global behavior of features in the Rainfall 854
dataset, with red bars indicating the density of training samples. Our findings indicate that low Max 855
Temperature and high Min Temperature contribute to an increase in rainfall. 856
Local time-independent feature contribution: Figure 9 shows the local time-independent feature 857
contributions. Consistent with the global interpretation, Avg. Temperature andMin Temperature have 858
positive correlations with rainfall, while Max Temperature has a negative correlation with rainfall. 859
Local time-dependent feature contribution: Figure 10 shows the local time-dependent feature 860
contributions. All features exhibit patterns similar to the local time-independent contributions. 861
However, we found that Avg. Temperature andMin Temperature have time lags between feature 862
values and contributions. 863
220 10 20
Time steps0.020.040.060.080.10Avg. attention scoreFigure 7: Average attention scores of time steps on the Rainfall dataset.
2.5
 0.0 2.50.1
0.0Avg. T emperature
2.5
 0.0 2.50.00.1Max T emperature
5
 00.000.05Min T emperature
Feature valueFeature contribution
Figure 8: Global interpretations of features in the Rainfall dataset.
0 10 200.035
0.030
0.025
0.020
0.015
0.010
0.005
0.000Avg. T emperature
0 10 200.10
0.05
0.000.050.10Max T emperature
0 10 200.030
0.025
0.020
0.015
0.010
0.005
0.000Min T emperature
1.75
1.50
1.25
1.00
0.75
0.50
0.25
0.00
1.5
1.0
0.5
0.00.51.01.5
1.75
1.50
1.25
1.00
0.75
0.50
0.25
0.00
Time stepFeature contribution
Feature value
Figure 9: Local time-independent contributions of features in the Rainfall dataset.
0 10 200.30
0.25
0.20
0.15
0.10
0.05
0.00Avg. T emperature
0 10 200.6
0.4
0.2
0.00.20.40.6Max T emperature
0 10 200.25
0.20
0.15
0.10
0.05
0.00Min T emperature
1.75
1.50
1.25
1.00
0.75
0.50
0.25
0.00
1.5
1.0
0.5
0.00.51.01.5
1.75
1.50
1.25
1.00
0.75
0.50
0.25
0.00
Time stepFeature contribution
Feature value
Figure 10: Local time-dependent contributions of features in the Rainfall dataset.
23