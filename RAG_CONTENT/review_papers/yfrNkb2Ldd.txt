Published in Transactions on Machine Learning Research (09/2024)
Noise Stability Optimization for Finding Flat Minima:
A Hessian-based Regularization Approach
Hongyang R. Zhang ho.zhang@northeastern.edu
Northeastern University, Boston
Dongyue Li li.dongyu@northeastern.edu
Northeastern University, Boston
Haotian Ju ju.h@northeastern.edu
Northeastern University, Boston
Reviewed on OpenReview: https://openreview.net/forum?id=yfrNkb2Ldd
Abstract
The training of over-parameterized neural networks has received much study in recent lit-
erature. An important consideration is the regularization of over-parameterized networks
due to their highly nonconvex and nonlinear geometry. In this paper, we study noise injec-
tion algorithms, which can regularize the Hessian of the loss, leading to regions with flat
loss surfaces. Specifically, by injecting isotropic Gaussian noise into the weight matrices of
a neural network, we can obtain an approximately unbiased estimate of the trace of the
Hessian. However, naively implementing the noise injection via adding noise to the weight
matrices before backpropagation presents limited empirical improvements. To address this
limitation, we design a two-point estimate of the Hessian penalty, which injects noise into
the weight matrices along both positive and negative directions of the random noise. In
particular, this two-point estimate eliminates the variance of the first-order Taylor’s expan-
sion term on the Hessian. We show a PAC-Bayes generalization bound that depends on the
trace of the Hessian (and the radius of the weight space), which can be measured from data.
We conduct a detailed experimental study to validate our approach and show that it can
effectively regularize the Hessian and improve generalization. First, our algorithm can out-
perform prior approaches on sharpness-reduced training, delivering up to a 2.4% test ac-
curacy increase for fine-tuning ResNets on six image classification datasets. Moreover, the
trace of the Hessian reduces by 15.8%, and the largest eigenvalue is reduced by 9.7% with
our approach. We also find that the regularization of the Hessian can be combined with
alternative regularization methods, such as weight decay and data augmentation, leading
to stronger regularization. Second, our approach remains highly effective for improving
generalization in pretraining multimodal CLIP models and chain-of-thought fine-tuning.
1 Introduction
The loss landscape and its geometry properties are a recurring theme in the study of neural networks
(Keskar et al., 2017; Hochreiter & Schmidhuber, 1997). Recently, the design of training methods such as
sharpness-aware minimization and stochastic weight averaging has led to empirical advances in a wide variety
of settings (Izmailov et al., 2018; Foret et al., 2021; Wortsman et al., 2022). The theoretical study of these
training methods has also been explored (Andriushchenko & Flammarion, 2022). For instance, recent work
shows that sharpness-aware minimization (Foret et al., 2021) has an implicit bias to flat surface regions by
penalizing the largest eigenvalue of the loss Hessian matrix (Wen et al., 2023; Bartlett et al., 2023). In this
paper, we study methods that can provide explicit regularization of the trace of the Hessian, and we will
1Published in Transactions on Machine Learning Research (09/2024)
𝑾𝒊	𝑼𝒊−𝑼𝒊∇𝑓(𝑊"+𝑈")∇𝑓(𝑊"−𝑈")𝑾𝒊#𝟏𝐍𝐒𝐎	
Figure 1: An illustration of one update step in our algorithm. At each iteration i, we sample a random
variableUifrom a zero-mean distribution P(e.g., an isotropic Gaussian with variance σ2), whereσis a
hyper-parameter that controls the strength of the noise injection (hence the regularization). We query the
gradient of f, atf(Wi+Ui), andf(Wi−Ui), and take their average. This results in a two-point noise
injection scheme, whose computation cost is the same as sharpness-aware minimization (Foret et al., 2021),
and twice the cost of running SGD. Notice that in practice, we can also implement an extension of this
algorithm, which samples multiple Us. For details, see Algorithm 1.
show provable generalization guarantees of our methods. More formally, given an input function f:Rd→R
that represents the empirical risk of a neural network and a d-dimensional distribution Pwith mean zero,
we consider minimizing the noise-perturbed function
F(W) :=E
U∼P[f(W+U)]. (1)
Minimizingthisperturbedfunctioncanimprovetheresilienceoftheneuralnetworktonoiseinjection, leading
to flatter loss surfaces and improved regularization (Nagarajan & Kolter, 2020). By analyzing the perturbed
loss of a fine-tuned model, one can identify a measure of the sharpness of loss surfaces based on the trace
of the Hessian (Ju et al., 2022; 2023). We remark that the minimization problem of the form (1) traces
back to earlier works on randomized smoothing (Duchi et al., 2012), which have provided a detailed study of
convergence rates for nonsmooth stochastic optimization. Our work differs from this line of literature in that
we focus on evaluating the regularization effect of penalizing the Hessian trace upon neural network training.
Although noise injection algorithms can be theoretically motivated as improving generalization (and sta-
bility), its practical implication is not evident (Hinton & Van Camp, 1993; An, 1996; Graves, 2011). To
motivate our study, we begin by running several empirical studies to compare the performance of (standard)
SGD and weight-perturbed SGD (WP-SGD), which first injects random noise into the weight matrices of a
neural network before computing its gradient in SGD. As mentioned above, this would provide a randomized
smoothing effect to the loss surface (Duchi et al., 2012). We will fine-tune (pretrained) ResNets on three
image classification tasks for this empirical study. To ensure the robustness of the analysis, we also vary the
distribution ofPand the variance of U. Our overall finding is that WP-SGD (or randomized smoothing)
does not offer clear benefits over SGD, which is also consistent with recent studies of weight noise injection
(Orvieto et al., 2023; Dauphin et al., 2024) (see Section 2.2, Table 2 for the complete results). However,
we hypothesize that these results may be due to the randomness of the noise injection (upon the Hessian
penalty term) rather than the ineffectiveness of regularizing the Hessian trace.
Our approach to mitigate the randomness of the noise injection on the Hessian penalty involves two parts.
First, we retrieve the gradient at W−Uto cancel out the first-order expansion term of W+U(recall that
Uis a random sample from P). Meanwhile, the second-order expansion term remains the same after this
cancellation. We term this modification a two-point noise injection scheme, which is reminiscent of two-point
gradient estimates in zeroth-order optimization (Duchi et al., 2015). The difference in our setting is that this
two-point averaging cancels out the first-order gradient term, thereby eliminating its variance on the Hessian
2Published in Transactions on Machine Learning Research (09/2024)
Table 1: Comparison between our approach (NSO) and SAM (Foret et al., 2021), based on inductive bias,
generalization guarantee, and convergence rate. In particular, the inductive bias of SAM is based on the
results of Wen et al. (2023). The list of notations used in the table is explained as follows. ∇2ℓrefers to
the Hessian matrix of the loss function ℓ.λ1[·]and Tr [·]refer to the largest eigenvalue and the trace of an
input matrix. αrefers to the trace norm, taken over the maximum of the entire hypothesis space and data
distribution (including unseen test data). ris the radius of the fine-tuning region measured in ℓ2distance.
nis the number of samples in the training dataset. Tis the total number of iterations run by our algorithm.
Approach Inductive Bias Generalization Guarantee Convergence Rate
Sharpness-Aware Minimization (SAM) λ1[∇2ℓ] - -
Noise Stability Optimization (NSO) Tr[∇2ℓ]/radicalig
αr2
n(Theorem 2.1) Θ/parenleftig
1√
T/parenrightig
(Section 4)
penalty. Second, we sample multiple perturbations U1,U2,...,Ukat each epoch and take their averaged
two-point (noise-injected) gradients. See Figure 1 for an illustration of one step.
A primary advantage of our approach compared to prior sharpness minimization algorithms is that our
approach can provide an approximately unbiased estimate of the Hessian trace. We empirically validate
this claim across three real-world settings (see Figure 2, Section 2.2 for an illustration). By utilizing this
property, we show a PAC-Bayes bound that depends on the trace of the Hessian and the radius of the weight
hypothesis space. We briefly describe this result, leaving a formal statement to Theorem 2.1. Let αbe an
upper bound on the trace of the Hessian measured within the hypothesis space and the data distribution (in
practice, one may take this as the union of training and testing data). Let rbe the radius of the hypothesis
space, measured in ℓ2distance. Suppose there are nempirical samples from an unknown distribution. We
show a generalization bound that scales as O/parenleftig/radicalig
αr2
n/parenrightig
. Our proof utilizes a linear PAC-Bayes bound (Catoni,
2007; McAllester, 2013; Alquier, 2021), and we optimize the variance of the prior and posterior distributions
to derive the result. A detailed proof sketch is presented in Section 2.3.
Next, we validate our approach with a detailed empirical study. First, we compare our approach with four
prior approaches for the setting of fine-tuning pretrained ResNets, including sharpness-aware minimization
(Foret et al., 2021), tested on six image classification datasets. We show that our algorithm can reduce the
trace and the largest eigenvalue of the loss Hessian matrix by 15.8% and 9.7%, respectively. Our approach
also improves test accuracy by 2.4%. Second, we show that by combining our approach with regularization
methods such as data augmentation and distance-based regularization (Gouk et al., 2022), we can further
regularize the Hessien, leading to 13.6% lower trace values and 16.3% lower test loss values (averaged over
six tasks). Third, we extend our approach to pretraining and chain-of-thought fine-tuning. The details can
be found in Section 3.2. Overall, our algorithm can consistently provide better regularization of Hessian and
improved test accuracy across these different settings and datasets. Some of these empirical results are not
completely explained by our theory, and we discuss the limitations in Section 7.
Finally, we analyze the convergence of our algorithm using techniques from the stochastic optimization
literature (Ghadimi & Lan, 2013; Lan, 2020; Carmon et al., 2020; Drori & Shamir, 2020; Zhang, 2023),
leading to matching upper and lower bounds. We also present a case study of Hessian regularization in over-
parametrized matrix sensing and show that it is equivalent to nuclear norm regularization for this setting.
Ourworkraisesseveralnewquestionsthatmaybeworthrevisiting: canacceleratedgradientdescentmethods
be applied to design flat-minima optimizers? Can recent advances in zeroth-order optimization be leveraged
to better regularize the training of transformer neural networks?
In summary, the contributions of this paper are three-fold. First, we present an algorithm that can explicitly
regularize the Hessian trace and show a PAC-Bayes generalization bound that could be measured from data.
Second, we conduct experiments on multiple settings to validate our approach by comparing downstream
performance and Hessian statistics with prior sharpness minimization algorithms and alternative regulariza-
tion methods. Third, we analyze the convergence of our algorithm using stochastic optimization techniques.
In Table 1, we highlight the key aspects of our approach compared to prior approaches.
3Published in Transactions on Machine Learning Research (09/2024)
Organization: The rest of this paper is organized as follows. In Section 2, we will present our approach.
We will start by presenting the motivating experiments. Then, we describe our algorithm and a PAC-Bayes
bound that depends on the Hessian. In Section 3, we present our experiments for validating the proposed
approach. Section 4 presents an analysis of the convergence rate. Section 5 provides a case study of the
regularization effect of the Hessian trace in the over-parameterized matrix sensing problem. In Section 6,
we discuss the related works. Finally, in Section 7, we state the conclusion and the limitations of this work.
We provide complete proof of our theoretical results in Appendix A-B. We provide additional experimental
details in Appendix C.
2 Our Approach
In this section, we present our approach. First, to set up the stage, we will study the straightforward
implementation of noise injection by directly adding noise to the weight matrices of the neural network before
computing thegradients in backpropagation. We term thisprocedure weight-perturbed SGD (or WP-SGDin
short), alsoknownasrandomizedsmoothing(Duchietal.,2012). Wewillcomparetheempiricalperformance
of these two approaches to evaluate the effect of noise injection. Then, we describe our algorithm and provide
empirical measurements of the trace of the Hessian, along with the actual perturbation gaps observed in
practice. Finally, we will show a PAC-Bayes generalization bound that depends on the trace of the Hessian,
which can be measured from data to compare different methods.
2.1 Motivating Experiments
We compare the results from running WP-SGD to standard SGD. We choose the setting of fine-tuning
pretrained foundation models, as overfitting is a common problem for this setting (Wortsman et al., 2022),
and strong regularization is needed (Li & Zhang, 2021; Ju et al., 2022). We will fine-tune a pretrained
ResNet-34 on several image classification datasets, including aircraft recognition (Aircraft) (Maji et al.,
2013), indoor scene recognition (Caltech-256) (Griffin et al., 2007), and medical image classification (retina
images for diabetic retinopathy classification) (Pachade et al., 2021). To implement WP-SGD, we sample
a random vector from Pand add it to the model weights at each iteration before computing the gradient.
We setPas the isotropic Gaussian and adjust its standard deviation between 0.008,0.01,and0.012via
cross-validation.
We report our results in Table 2, which indicate that the performance gap is less than 0.5%, ≈0.75standard
deviationsbasedonfiveindependentruns. Furthermore, varying Pdoesnotchangetheresults. Inparticular,
we test four types of P, including Gaussian, Laplace, uniform, and Binomial. We adjust standard deviations
between 0.008,0.01, and 0.012via cross-validation. We find that using Laplace and uniform distributions
achieves a performance comparable to that of Gaussian. However, using Binomial results in worse results.
These experiments suggest that the straightforward implementation of noise injection does not offer clear
benefits over SGD.
2.2 Description of Our Algorithm
In our approach, we make two modifications to WP-SGD. First, we add the perturbation from both the
positive and negative directions during the noise injection, as shown in line 5. Second, we average over
multiple noise injections to reduce the variance from noise injection, as described in line 7. As for the first
modification, recall that Pis a symmetric distribution. We use Taylor’s expansion on both f(W+U)and
f(W−U)as follows:
f(W+U) =f(W) +⟨U,∇f(W)⟩+1
2U⊤∇2f(W)U+O(∥Σ∥3
2
2), (2)
f(W−U) =f(W)−⟨U,∇f(W)⟩+1
2U⊤∇2f(W)U+O(∥Σ∥3
2
2). (3)
4Published in Transactions on Machine Learning Research (09/2024)
Table 2: Comparing the outcome of running WP-SGD to standard SGD across four different P, measured
over three image classification datasets. Recall that WP-SGD refers to normal weight perturbation (without
the paired perturbation). To be concise, we have included the results of running our approach (i.e., NSO).
All the results and their standard deviations are based on five independent runs.
Aircraft Indoor Retina Disease
P Train Acc. Test Acc. Train Acc. Test Acc. Train Acc. Test Acc.
SGD None 100.0% ±0.0 59.8%±0.7 100.0%±0.0 76.0%±0.4 100.0%±0.0 61.7%±0.8
WP-SGD Gaussian 98.4% ±0.2 60.4%±0.1 99.0%±0.3 76.3%±0.0 100.0%±0.0 62.3%±0.5
WP-SGD Laplace 98.3% ±0.1 60.3%±0.3 98.9%±0.1 76.4%±0.3 100.0%±0.0 62.0%±0.1
WP-SGD Uniform 98.6% ±0.3 60.3%±0.5 98.6%±0.3 76.6%±0.1 100.0%±0.0 62.3%±0.0
WP-SGD Binomial 19.6% ±0.1 11.3%±0.1 18.2%±0.9 10.7%±0.1 58.1%±0.1 57.1%±0.0
NSO Gaussian 95.8% ±0.4 62.3%±0.3 95.7%±0.2 77.4%±0.3 100.0%±0.0 66.6%±0.7
NSO Laplace 96.5% ±0.3 61.9%±0.3 96.1%±0.3 77.1%±0.1 100.0%±0.0 65.9%±0.1
NSO Uniform 96.4% ±0.4 61.9%±0.5 96.4%±0.2 76.8%±0.2 100.0%±0.0 65.7%±0.1
NSO Binomial 20.1% ±0.1 14.3%±0.3 22.8%±0.1 17.9%±0.2 59.2%±0.1 57.8%±0.1
We have that E[U] = 0, andE/bracketleftbig
UU⊤/bracketrightbig
= Σ. Thus, by taking the average of equations (2) and (3), we get
E
U∼P/bracketleftbigg1
2(f(W+U) +f(W−U))/bracketrightbigg
=F(W) =f(W) +1
2⟨Σ,∇2f(W)⟩+O/parenleftig
∥Σ∥3
2
2/parenrightig
. (4)
We can see that the two-point estimate eliminates the first-order gradient term, potentially reducing its vari-
ance in estimating the Hessian term. The second modification reduces the variance of the stochastic gradient,
using the fact that each perturbation is independent of the others. The entire procedure is summarized in
Algorithm 1. As a remark, two-point gradient estimators are commonly used in zeroth-order optimization
(Duchi et al., 2015). However, their use in designing flat minima optimizers has not been explored much.
Algorithm 1 Noise stability optimization (NSO) for regularizing the Hessian of neural networks
Input: Initialization W0∈Rd, a function f:Rd→R
Require : An estimator g:Rd→Rdthat for any W, returnsg(W)s.t.E[g(W)] =∇f(W)
Parameters: #perturbations k,#epochsT, step sizes η0,...,ηT−1
1:fori= 0,1,...,T−1do
2:/*Compute the two-point averaged stochastic gradient for each independent noise injection */
3:forj= 0,1,...,k−1do
4:U(j)
i←sampled independently from P
5:G(j)
i←g/parenleftbig
Wi+U(j)
i/parenrightbig
+g/parenleftbig
Wi−U(j)
i/parenrightbig
6:end for
7:Wi+1←Wi−ηi/parenleftig
1
2k/summationtextk
j=1G(j)
i/parenrightig
8:end for
Measurements of the Hessian trace and the perturbation gap: Next, we provide several examples
to measure the approximation quality of equation (4). Following the experimental setup of Section 2.1, we
will fine-tune a foundation model on a downstream task. After training, we will set Was the model weight
at the last epoch for all the measurements.
To measure equation (4), we then add UtoW, whereUis sampled from an isotropic Gaussian. We will
measuref(W+U)−f(W), averaged over 100independent samples of U, and we measure this and ∇2f(W)
by taking the average over the training dataset.
The results are shown in Figure 2. We can see that ∇2fprovides an accurate approximation to F(W)−f(W)
for various values of σ. In particular, the approximation error of equation (4) using the Hessian trace is less
5Published in Transactions on Machine Learning Research (09/2024)
than 3%. As a remark, the range of σ2differs across architectures because of the differing scales of their
weights. More details about the neural network architectures can be found in Appendix C.
0.020 0.025 0.030
σ123×10−2 MLP
Gap
Trace×σ2
2
0.0070 0.0075 0.0080
σ123×10−2 BERT
Gap
Trace×σ2
2
0.040 0.045 0.050
σ246×10−2 GNN
Gap
Trace×σ2
2
Figure 2: Illustration of the approximation quality of equation (4). We report all measurements based on the
network weight at the last epoch of fine-tuning. We can see that the perturbation gap (i.e., F(W)−f(W)
in equation (4)) andσ2
2Tr[∇2f(W)]are at the same order. Recall that σrefers to the standard deviation of
the Gaussian noise injected into the weight matrices. More specifically, σwill decide the strength of noise
injection or the strength of regularization on the Hessian trace.
2.3 Generalization Guarantee and Proof Sketch
Next, we present a PAC-Bayes generalization bound that depends on the trace of the Hessian. Our bound
can be related to the notion of trace norm, which has been used in earlier works for quantifying sample
complexity in the context of matrix recovery (Srebro & Shraibman, 2005).
Concretely, suppose we have a pretrained model in the fine-tuning setting. This can be viewed as our prior
belief of the target hypothesis in PAC-Bayes analysis. Once we have learned a model (though fine-tuning), we
can view this as the posterior in PAC-Bayes analysis. Let D⊆X×Y be an unknown data distribution, sup-
ported on the feature space Xand the label space Y. Givennrandom samples (x1,y1),(x2,y2),..., (xn,yn)
drawn fromD, the empirical loss (measured by loss function ℓ) applied to a model fW(withW∈Rp) is:
ˆL(W) =1
nn/summationdisplay
i=1ℓ(fW(xi),yi).
The population loss is L(W) =E(x,y)∼D[ℓ(fW(x),y)].It is sufficient to think that the empirical loss is less
than the population loss, and the goal is to bound the gap between ˆL(W)andL(W)from above (Shalev-
Shwartz & Ben-David, 2014).
LetWbe any learned hypothesis within the hypothesis space, denoted as H. Our generalization bound will
apply uniformly to Wwithin the hypothesis space. We state our result, including the required assumptions,
as follows.
Theorem 2.1. Assume that the loss function ℓis bounded between 0andCfor a fixed constant C > 0on the
data distribution D. Supposeℓ(fW(·),·)is twice-differentiable in Wand the Hessian matrix ∇2[ℓ(fW(·),·)]
is Lipschitz continuous within the hypothesis space. Suppose for any WinH, the trace norm of the Hessian
is less than α:
α:= max
W∈Hmax
(x,y)∼DTr/bracketleftbig
∇2ℓ(fW(x),y)/bracketrightbig
, (5)
and theℓ2-norm ofWis at mostrfor anyW∈H. Then, for any WinH, with probability at least 1−δ
for anyδ>0, the following must hold, for any ϵclose to zero:
L(W)≤(1 +ϵ)ˆL(W) + (1 +ϵ)/radicalbigg
Cαr2
n+O/parenleftig
n−3
4log(δ−1)/parenrightig
. (6)
6Published in Transactions on Machine Learning Research (09/2024)
Proof Sketch: We provide a high-level illustration of the proof of Theorem 2.1. Let Qdenote the posterior
distribution. Specifically, we consider Qas being centered at the learned hypothesis W(which could be
anywhere within the hypothesis space), given by a Gaussian distribution N(W,σ2Idp), where Idpdenotes
thepbypidentity matrix. Given a sample U∼N(0,σ2Idp), let the perturbed loss be given by
ℓQ(fW(x),y) =E
U[ℓ(fW+U(x),y)]. (7)
Then, let ˆLQ(W)be the averaged value of ℓQ(fW(·),·), taken over nempirical samples from the training
dataset. Likewise, let LQ(W)be the population average of ℓQ(fW(·),·), in expectation over an unseen data
sample from the underlying data distribution.
Having introduced the notations, we start with the linear PAC-Bayes bound (Catoni, 2007; McAllester, 2013;
Alquier, 2021) (see Theorem A.1 for reference), stated as follows, which holds with probability 1−δfor any
δ∈(0,1)andβ∈(0,1):
LQ(W)≤1
βˆLQ(W) +C(KL(Q||P ) + log(δ−1))
2β(1−β)n, (8)
wherePrefers to the priordistribution, Crefers to the upper bound on the loss value ℓ. For analyzing
fine-tuning, we view Pas centered at the pretrained model, with covariance matrix σ2Idp. By Taylor’s
expansion of ℓQ(see Lemma A.4 for the precise statement), we show that:
LQ(W) =L(W) +σ2
2E
(x,y)∼D/bracketleftbig
Tr/bracketleftbig
∇2ℓ(fW(x),y)/bracketrightbig/bracketrightbig
+O(σ3) (9)
ˆLQ(W) =ˆL(W) +σ2
2nn/summationdisplay
i=1Tr/bracketleftbig
∇2ℓ(fW(xi),yi)/bracketrightbig
+O(σ3). (10)
Since the Hessian operator is Lipschitz continuous by the assumption of Theorem 2.1, we can bound the gap
between the above two quantities with ϵ-covering arguments (see Lemma A.5 for the precise statement). By
plugging in these results back to the PAC-Bayes bound of equation (8), after some calculation, we can get:
L(W)≤1
βˆL(W) +σ2(1−β)α
2β+Cr2/2σ2
2β(1−β)n+O/parenleftbigg
σ3+σ2√p√n+log(δ−1)
n/parenrightbigg
. (11)
In particular, the above uses the fact that the ℓ2-norm ofWis less than rfor anyW∈H(the KL divergence
is discussed in Proposition A.2). By choosing σ2andβto minimize equation (11), we will obtain equation
(6). This summarizes the high-level proof idea. The complete proof can be found in Appendix A.1.
Remark 2.2. We highlight two key aspects of our results. The first is that our PAC-Bayes bound is non-
vacuous, meaning that it matches the scale of empirically observed gaps when measured in practice; this
is based on the trace measurements in Figures 2 and 3. The second is that this non-vacuous bound has
practical implications, meaning that we can utilize this bound to design optimization algorithms that improve
generalization.
These are non-trivial to achieve. To give some context, prior work has provided a PAC-Bayes margin
bound for multi-layer neural networks, which depends on the product of the spectral norm of the network
layers (Neyshabur et al., 2018). While this paper provides important insights regarding the generalization
of deep networks, the bound is vacuous when measured in practice. Arora et al. (2018) provide another
data-dependent PAC-Bayes bound based on compression techniques. Their work started with an experiment
in which they injected noise into the network layers and showed that deep nets can absorb the noise after
retraining. However, their bound remains orders of magnitude higher than the actual generalization errors
observed in practice.
In contrast, our bound matches the scale of empirically observed gaps. To achieve this, we start from the
line of work on data-dependent PAC-Bayes bounds. We build on the line of work on distance from the
initialization (Nagarajan & Kolter, 2020), which is ideal for understanding fine-tuning (Li & Zhang, 2021).
7Published in Transactions on Machine Learning Research (09/2024)
Our key breakthrough is to connect noise stability in PAC-Bayes bound with the loss Hessian matrix (cf.
equations (9)and(10)). Then, we can measure the Hessian of loss landscapes from data.
We additionally note that few existing works have considered using PAC-Bayes bounds to design algorithms.
The reason is that for new algorithm designs, we need to connect the PAC-Bayes bound with data in a non-
vacuous way. The work of Dziugaite & Roy (2017) has provided a computational framework to achieve non-
vacuous generalization bounds. Instead, our result provides an analytical expression that can be leveraged in
algorithm design. To operationalize the design, we utilize the explicit dependence of our result on the Hessian
to design the regularization scheme.
3 Experiments
We now turn to empirical validations of our algorithm. First, we apply our approach to fine-tune pretrained
ResNets on various image classification datasets. We find that NSO can more significantly regularize the
Hessian of the loss surface, resulting in reductions in the trace and the largest eigenvalue by 15.8% and
9.7%, respectively. After controlling computation costs, it can outperform four sharpness-reducing meth-
ods by up to 2.4%. In addition, we justify our algorithm design through detailed ablation analysis. We
also show that our approach is compatible with alternative regularization techniques, including distance-
based regularization and data augmentation, and combining these methods with our approach leads to more
significant regularization and test performance. Second, we show similar results for pretraining and chain-
of-thought fine-tuning. The experiment code for reproducing our empirical findings can be found online at:
https://github.com/VirtuosoResearch/Noise-stability-optimization .
3.1 Comparison with Sharpness Minimization Methods
We now compare Algorithm 1 with five sharpness-reducing training methods, including sharpness-aware
minimization(SAM)(Foretetal.,2021), unnormalizedSAM(USAM)(Agarwala&Dauphin,2023), adaptive
variants of SAM (ASAM), and random SAM (RSAM) (Liu et al., 2022). During the comparison, we control
for the same amount of computation (for Algorithm 1, we will set the number of sampled injections kas
1). Thus, all the methods under consideration will use twice the computation of SGD. For NSO, we sample
perturbation from an isotropic Gaussian distribution and adjust σbetween 0.008,0.01, and 0.012. For SAM,
we adjust the ℓ2norm of the perturbation between 0.01,0.02, and 0.05. For each method, we run it with
both momentum and weight decay. We ensure that all the training methods are carefully adjusted. See
Appendix C for the details.
3.1.1 Empirical Findings
In Table 3, we report the comparison between NSO, SGD, SAM, unnormalized SAM (USAM), and adaptive
SAM (ASAM). We find that our approach reduces the trace of Hessian by 15.8% on average. The largest
eigenvalue of the Hessian is also reduced by 9.7%. This finding is intriguing since SAM has been motivated
by a min-max problem. As for test accuracy, our approach can provide up to 2.4% lift, with an average
improvement of 1.2%. Additional comparisons are deferred to Table 6 in Appendix C.
Figure 3 illustrates the measurements between SGD, WP-SGD, and NSO. Curiously, we find that the trace of
the Hessian also decreases for SGD, possibly due to implicit norm control of SGD. While both WP-SGD and
NSO reduce the trace of the Hessian, our approach penalizes the Hessian more. Besides, the generalization
gap and the test loss are consistently lower during NSO training.
As a remark, the regularization effect of noise injection should be orthogonal to training methods such as
momentum, weight decay, learning rate scheduling, etc. To this end, we performed comparisons without
using either momentum or weight decay. Our approach can again reduce the trace of the Hessian by 17.7%
compared to the five sharpness-reducing methods on average, with up to 1.8% higher test accuracy.
8Published in Transactions on Machine Learning Research (09/2024)
Table 3: Comparison between our approach (NSO) with SGD, sharpness-aware minimization (SAM), un-
normalized SAM (USAM), and adaptive SAM (ASAM). We fine-tune the ResNet-34 network on six image
classification datasets and report the test accuracy and the trace of Hessian using the model in the last epoch
of training. The results are averaged over five random seeds.
CIFAR-10 CIFAR-100 Aircrafts Caltech-256 Indoor Retina
Basic
Statistics# Training 45,000 45,000 3,334 7,680 4,824 1,396
# Validation 5,000 5,000 3,333 5,120 536 248
# Test 10,000 10,000 3,333 5,120 1,340 250
# Classes 10 100 100 256 67 5
Trace
(↓)SGD 4128 ±83 13188±221 5471±65 3674±95 3629±61 28607±226
SAM 2429 ±87 9227±286 4499±70 3285±95 3159±75 15444±173
USAM 2352 ±61 7382±222 4298±94 3174±52 3072±51 12068±246
ASAM 2445 ±63 9960±313 4475±69 3339±78 3014±53 14155±136
NSO 1728 ±79 5244±89 3678±83 2958±77 2737±90 10970±146
Test Acc.
(↑)SGD 96.1% ±0.1 82.8%±0.1 60.5%±0.7 80.0%±0.1 76.7%±0.4 62.2%±0.8
SAM 97.0% ±0.2 84.0%±0.4 62.3%±0.3 77.0%±0.4 77.2%±0.3 65.0%±0.3
USAM 96.9% ±0.2 83.7%±0.2 61.9%±0.3 76.9%±0.2 76.7%±0.3 64.7%±0.1
ASAM 97.1% ±0.1 84.2%±0.3 62.4%±0.5 77.3%±0.2 77.2%±0.2 65.2%±0.3
NSO 97.6% ±0.4 84.9%±0.3 63.2%±0.3 78.1%±0.5 78.2%±0.3 67.0%±0.4
0 10 20 30
t0.50.60.70.8Test LossResNet 34
SGD
WP-SGD
NSO
0 10 20 30
t0.51.01.52.0Trace×104 ResNet 34
0 10 20 30
t0.00.20.40.6Generalization GapResNet 34
0 2 4 6
t0.81.01.21.4Test LossBERT Base
SGD
WP-SGD
NSO
0 2 4 6
t0.10.40.71.0Trace×104 BERT Base
0 2 4 6
t0.00.30.60.9Generalization GapBERT Base
Figure 3: Comparison between SGD, WP-SGD, and NSO for fine-tuning ResNet-34 and BERT-Base, re-
spectively, on an image and a text classification dataset. We evaluate the test loss, the trace of the Hessian,
and the generalization gap for the trained model at each epoch. For WP-SGD and NSO, we sample noise
from isotropic Gaussian with standard deviation σ= 0.01in both settings.
3.1.2 Ablation Analysis
Next, we conduct ablation studies of two modifications in our approach: the use of negative perturbations
and the sampling of multiple perturbations.
Comparing using negative cancellation or not after controlling computation costs: Recall that
our algorithm uses negative perturbations to zero out the first-order term in Taylor’s expansion of F(W).
We validate this by comparing the performance between using or not using the negative perturbation. We
control for the same amount of computation costs to ensure a fair comparison. In particular, we sample
two independent perturbations and take their averaged stochastic gradient. We find that using the nega-
9Published in Transactions on Machine Learning Research (09/2024)
tive perturbation achieves a 3.6% improvement in test accuracy (on average) over not using the negative
perturbation, i.e., randomized smoothing.
As discussed in Section 2.2, our intuition on why NSO can be expected to generalize better than randomized
smoothing is that it can better regularize the Hessian. In particular, even though, in theory, the expectation
off(W+U)and1
2(f(W+U) +f(W−U))overUare both equal to F(W). However, the two-point scheme
cancels out the gradient expansion term compared to randomized smoothing at every epoch. More precisely,
we believe that the improved regularization from our approach stems from its better estimate of the Hessian
penalty term. As illustrated in Figure 3, NSO consistently reduces the trace of the Hessian and achieves
lower generalization errors compared to randomized smoothing throughout model training. At the end of
the training, NSO yields 10.6% smaller trace of the Hessian on average than randomized smoothing.
Increasing the number of noise injections k:Recall that increasing the number of perturbations k
can reduce the variance of the estimated gradient. Thus, we consider increasing kin NSO and compare
that with a specific implementation of WP-SGD that uses the same amount of computation. Using k= 2
perturbations improves the test accuracy by 1.2% on average compared to k= 1.
Varying the learning rate and the number of epochs. We provide a detailed comparison between
NSO and WP-SGD when varying the learning rate and the number of epochs. The learning rate is varied be-
tween 0.0001,0.0002,0.0005,0.001,0.002,and 0.005. Thenumberofepochsisvariedbetween 10,20,30,40,50,
and60. We report the test loss from running an image classification task in Figure 4.
1e−42e−45e−41e−32e−35e−3
Learning rate234Test lossWP-SGD
NSO
10 20 30 40 50 60
Number of epochs1.21.41.61.8Test lossWP-SGD
NSO
Figure 4: Results of varying the learning rate and the number of epochs for running our approach and
WP-SGD. We report the test loss from the last epoch and average the results over five random seeds.
Remark 3.1 (Noise variance scheduling as kincreases) .A natural question is whether one can gradually
increase or decrease the regularization strength by σduring training, similar to learning rate scheduling. To
facilitate this discussion, we test two schedules for adjusting σ. The first schedule is to increase σto a
specified value at a linear rate. The second schedule exponentially increases σto reach a specified value. Our
preliminary experiments show that neither schedule offers significant performance improvements over using
a constant noise variance. One might also consider other scheduling schemes; we leave this to future work.
3.1.3 Detailed Comparison with Sharpness-Aware Minimization (SAM)
Varying the radius of SAM: WeprovideadetailedcomparisontoSAMbyvaryingtheperturbationradius
of SAM (denoted as ρ). To illustrate this comparison, we vary ρbetween 0.001,0.002,0.005,0.01,0.02,and
0.05. We report both the validation accuracy and the trace of the Hessian for SAM and unnormalized SAM
on an image classification dataset. We present the results in Table 4. We observe that using a smaller ρ(i.e.,
less than 0.01) results in worse results. Thus, we choose ρbetween 0.01,0.02,and0.05in our experiments.
Varying the batch size of SAM: Next, we measure the sensitivity of our approach concerning the batch
size. In particular, we vary the batch size between 8, 16, 32, and 64 for fine-tuning ResNet-34 on two image
classification datasets. The results are shown in the leftmost two panels of Figure 5. We use the same number
of epochs for each batch size configuration to ensure a fair comparison. On the indoor dataset, our approach
is less sensitive to different batch sizes than SAM. Across all the batch sizes and datasets, our approach
consistently provides a more robust regularization of the Hessian compared to SAM. The best results are
achieved when the batch size is 32. Thus, we use this particular setting in our experiments.
10Published in Transactions on Machine Learning Research (09/2024)
Table 4: Results of varying the perturbation radius of SAM (denoted as ρ) and unnormalized SAM. We
report both the test accuracy and the trace of the Hessian based on the model trained at the last epoch. We
report the averaged results and their standard deviations across five random seeds.
ρ 0.001 0 .002 0 .005 0 .01 0 .02 0 .05
Trace
(↓)SAM 4920 ±158 4347±166 4016±80 3918±94 3159±753028±78
Unnormalized SAM 4352 ±169 3990±70 3723±87 3427±57 3072±513048±22
Test Accuracy
(↑)SAM 73.6 ±0.2 74.4±0.4 74.8±0.6 75.2±0.3 76.6±0.5 73.8±0.7
Unnormalized SAM 74.1 ±0.1 74.1±0.7 74.7±0.5 74.6±0.3 76.3±0.3 73.1±0.6
8 16 32 64
Batch size0.81.01.21.4Test lossSAM
NSO
(a) Loss on Indoor dataset
8 16 32 64
Batch size1.01.41.82.2Test lossSAM
NSO (b) Loss on Aircraft dataset
0 10 20 30
t0.81.21.6Test Lossw/o dist.reg.
w/dist.reg. (c) Test loss, w/ dist. reg.
0 10 20 30
t0.81.21.6Test Lossw/o data aug.
w/data aug. (d) Test loss, w/ data aug.
8 16 32 64
Batch size34Trace×103
SAM
NSO
(e) Hessian, Indoor dataset
8 16 32 64
Batch size45Trace×103
SAM
NSO (f) Hessian, Aircraft dataset
0 10 20 30
t357Trace×103
w/o dist.reg.
w/dist.reg. (g) Hessian, w/ dist. reg.
0 10 20 30
t357Trace×103
w/o data aug.
w/data aug. (h) Hessian, w/ data aug.
Figure 5: Results of varying the batch size of our approach and SAM ran on two image classification datasets
(indoor scene recognition and Aircraft detection). We report the test loss and the trace of Hessian using the
model from the last epoch of training. The results are averaged over five random seeds. The regularization
provided by noise injection can be combined with distance-based regularization and data augmentation to
reduce the test loss and the Hessian trace.
3.1.4 Combining Algorithm 1 with Alternative Regularization Methods
In this section, we show that the regularization of the Hessian can serve as a complement to existing, alterna-
tive regularization methods. To validate this, we combine our training approach with data augmentation and
distance-based regularization (Gouk et al., 2022). In particular, the latter approach has been used to regular-
ize fine-tuning algorithms. We use a popular scheme for data augmentation that applies random horizontal
flipping and random cropping sequentially to each training image. As for distance-based regularization, we
penalize the ℓ2distance between the fine-tuned model and the pretrained initialization.
The results are shown in Figure 5 within the two rightmost panels. Combining our approach with each
regularization method further reduces the trace of the loss Hessian matrix by 13.6% (on average). This
further leads to 16.3% lower test loss of the fine-tuned network, suggesting that our approach can be used
on top of these preexisting regularization methods.
3.2 Applying Algorithm 1 to Pretraining and Fine-tuning
We apply our approach to pretraining randomly initialized models by replacing SGD to train contrastive
language-image (CLIP) models on a dataset of image-caption pairs. In particular, we use the Conceptual
11Published in Transactions on Machine Learning Research (09/2024)
Caption dataset, which contains 3.3 million image caption pairs. Each caption briefly describes the corre-
sponding image, with ten tokens on average. We use a 12-layer Vision Transformer as the image encoder
and a 12-layer GPT-2 transformer as the text encoder. We train the encoders jointly to maximize the cosine
similarity between the embedding of image caption pairs following the protocol of Radford et al. (2021).
Table 5 presents the results. For each algorithm, we evaluate the trace of the loss Hessian and recall scores
(of the top-10 scored images in retrieving images from texts) on the development set. The results show that
our approach can reduce the trace of the Hessian by 17% compared to both SAM and SGD. In addition,
our approach achieves 1.4% higher recall scores in image retrieval.
Lastly, we apply our algorithm to fine-tuning pretrained language models on chain-of-thought reasoning
datasets. The task is to generate the reasoning process, i.e., a chain of thoughts and the answer for a
given commonsense reasoning question. We fine-tune pretrained GPT-2 models on two question-answering
datasets: Commonsense QA and Strategy QA. Table 5 shows that our approach can yield 25% lower trace
values than SAM and SGD. In addition, we can obtain 5.3% higher test accuracy.
Table 5: Results for pretraining CLIP the Conceptual Caption and chain-of-thought fine-tuning on Com-
monsense/Strategy QA. We report the recall score of image retrieval/test accuracy and trace/ λ1using the
model at the last epoch. We report the averaged results and standard deviations over five random seeds.
Conceptual Caption Trace(↓)λ1(↓) Recall@10 (↑)
SGD 220 ±24 41±2.8 36.1%±0.3
SAM 144 ±20 30±1.1 36.9%±0.4
NSO 119 ±3422±1.2 37.5%±0.3
CommonsenseQA Trace(↓)λ1(↓)Test Accuracy ( ↑)
SGD 372 ±34 19±0.8 27.7%±1.8
SAM 288 ±15 15±0.3 32.7%±1.4
NSO 208 ±3113±0.6 39.2%±1.4
StrategyQA Trace(↓)λ1(↓)Test Accuracy ( ↑)
SGD 294 ±13 44±1.5 68.9%±1.0
SAM 249 ±33 42±2.6 71.1%±1.2
NSO 193 ±3133±1.8 75.2%±1.2
4 Convergence Rates
We now study the convergence of Algorithm 1. Recall that our algorithm minimizes f(W)plus a regulariza-
tion term on the Hessian trace. As is typical with regularization, the penalty is usually small relative to the
loss value. Thus, we aim to find a stationary point of F(W)instead off(W)because otherwise, we would
not have the desired regularization. We state the convergence to an approximate stationary point such that
∥∇F(W)∥is small, building on the following gradient oracle assumption (see, e.g., Ghadimi & Lan (2013);
Duchi et al. (2015)).
Assumption 4.1. Given a random seed z, letgz:Rd→Rdbe a continuous function that gives an unbiased
estimate of the gradient: Ez[gz(W)] =∇f(W), for anyW∈Rd. Additionally, the variance is bounded in
the sense that Ez/bracketleftig
∥gz(W)−∇f(W)∥2/bracketrightig
≤σ2.
To help understand the above assumption, suppose there is a dataset of size n. Then, in SGD, the stochastic
gradient would be an unbiased estimate of the gradient of the entire dataset. As for the variance of the
gradient estimator, we note that as long as the ℓ2norm of the gradient remains bounded, which will always
hold in practice, then the last equation of the above assumption will hold. We now state an upper bound
on the convergence rate of Algorithm 1.
12Published in Transactions on Machine Learning Research (09/2024)
Proposition 4.2. Suppose Assumption 4.1 holds. Let Pbe a distribution that is symmetric at zero and let
H(P) =E[∥U∥2]. LetCandDbe fixed, positive constants. Let W0∈Rddenote an arbitrary initialization.
SupposeF(W0)−minW∈RdF(W)≤D2,and∇fisC-Lipschitz continuous. There exists a fixed learning
rateη < C−1such that if we run Algorithm 1 with ηi=ηfor alliforTsteps, the algorithm returns Wt
(wheretis a random integer between 1,2,...,T), such that in expectation over the randomness of Wt:
E/bracketleftig
∥∇F(Wt)∥2/bracketrightig
≤/radicalbigg
2CD2(σ2+C2H(P))
kT+2CD2
T. (12)
As a remark, existing sharpness-reducing methods such as SAM seem to suffer from oscillation around the
local basin (Bartlett et al., 2023). Thus, the convergence behavior of SAM seems challenging to analyze for
nonconvex functions. By contrast, our algorithm is amenable to stochastic optimization techniques. Our
proof slightly extends the proof of Theorem 2.1, Ghadimi & Lan (2013), to tackle noise injection and other
variations. For details, see Appendix B.1.
Lower bounds: Next, weconstructanexampletomatchtherateofequation(12), essentiallyshowingthat
this is tight under the same set of assumptions. We use an example from the work of Drori & Shamir (2020).
The difference is that we have to deal with the perturbations added to the objective. For t= 0,1,...,d−1,
letet∈Rdbe the basis vector in dimension d, whoset-th coordinate is 1, while the remaining coordinates
are all zero. Let f:Rd→Rbe defined as
f(W) =1
2G⟨W,e 0⟩2+T−1/summationdisplay
i=0hi(⟨W,ei+1⟩), (13)
wherehiis a piece-wise quadratic function parameterized by αi, defined as follow:
hi(x) =

Cα2
i
4|x|≤αi,
−C/parenleftbig
|x|−αi/parenrightbig2
2+Cα2
i
4αi≤|x|≤3
2αi,
C/parenleftbig
|x|−2αi/parenrightbig2
23
2αi≤|x|≤2αi,
0 2 αi≤|x|.
One can verify that for each piece above, ∇hiisC-Lipschitz. As a result, provided that G≤C−1,∇fis
C-Lipschitz, based on the definition of fin equation (13).
The stochastic function Frequires setting the perturbation distribution P. We setPby truncating an
isotropic Gaussian N(0,σ2Idd)so that the i-th coordinate is at most 2−1αi−1, fori= 1,...,T. Additionally,
we set the initialization W0to satisfy⟨W0,ei⟩= 0for anyi≥1while⟨W0,e0⟩̸= 0. Finally, we choose the
gradient oracle to satisfy that the i-th step’s gradient noise ξi=⟨ξi,ei+1⟩ei+1, which means that ξiis along
the direction of the basis vector ei+1. In particular, this implies only coordinate i+ 1is updated in step i,
as long as⟨ξi,ei+1⟩≤2−1αi. With this construction, we state the lower bound below.
Theorem 4.3. Let the learning rates η0,...,ηT−1be at mostC−1. LetD> 0be a fixed value. When either/summationtextT−1
i=0ηi≲√
kT, orηi=η<C−1for any epoch i, then for the above construction, the following must hold
min
1≤t≤TE/bracketleftig
∥∇F(Wt)∥2/bracketrightig
≥D/radicalbigg
Cσ2
32k·T. (14)
We remark that the above construction requires T≤d. Notice that this is purely for technical reasons. We
briefly illustrate the key steps of the proof. At step i, the gradient noise ξiplus the perturbation noise is
less than 2−1αi+ 2−1αi=αiat coordinate i+ 1(by triangle inequality). Thus, h′
i(⟨Wt,ei+1⟩) = 0, which
holds for all prior update steps. This implies
∇f(Wi) =G−1⟨Wi,e0⟩.
13Published in Transactions on Machine Learning Research (09/2024)
Recall that F(W0)≤D2. This condition imposes how large the αi’s can be. In particular, we will set
αi= 2ηiσ/√
kin the proof. Then, based on the definition of f(W0),
hi(⟨W0,ei+1⟩) =Cα2
i
4,since⟨W0+U,ei+1⟩≤αi.
In Lemma B.3, we then argue that the learning rates in this case must satisfy/summationtextT−1
i=0ηi≤O(√
T).When
the learning rate is fixed and at least Ω(T−1/2), we construct a piece-wise quadratic function (similar to
equation (13)), now with a fixed α. This is described in Lemma B.4. In this case, the gradient noise grows
by1−C−1ηup toTsteps. We then carefully set αto lower bound the norm of the gradient. Combining
these two cases, we conclude the proof of Theorem 4.3. For details, see Appendix B.2. As is typical in
lower-bound constructions, our result holds for a specific instance (with a particular learning rate range).
The proof can also be extended to adaptive learning rate schedules. Notice that the above construction
holds for arbitrary learning rates defined as a function of previous iterates. Then, we set the width of each
functionht,αt, proportional to ηt>0, for anyηtthat may depend on previous iterates, as long as they
satisfy the constraint that/summationtextT−1
i=0ηi≤O(√
T).
Extension: We can show a similar lower bound for the momentum update rule. Recall this is defined as
Mi+1=µMi−ηiGi,andWi+1=Wi+Mi+1, (15)
fori= 0,1,...,T−1, whereGiis the specific gradient at step i. To handle this case, we will need a more
fine-grained control on the gradient, so we consider a quadratic function as f(W) =C
2∥W∥2.We leave the
result and its proof to Appendix B.3.
Remark 4.4. The novelty of our results lies in analyzing sharpness minimization using techniques from
stochastic optimization. This appears to be new, and we hope our work can inspire further studies, such as
designing accelerated sharpness minimization methods.
5 Regularization Effect of Hessian Trace in Over-Parameterized Matrix Sensing
Before proceeding, let us give an example of the regularization effect of penalizing the Hessian trace. We
consider the matrix sensing problem, whose generalization properties are particularly well-understood in the
nonconvex factorization setting (Li et al., 2018). Let there be an unknown, rank- rpositive semi-definite ma-
trixX⋆=U⋆U⋆⊤∈Rd×d. Theinputconsistsofalistof dbydGaussianmeasurementmatrix A1,A2,...,An.
The labels are given by yi=⟨Ai,X⋆⟩, for everyi= 1,2,...n. The empirical loss is
ˆL(W) =1
2nn/summationdisplay
i=1/parenleftbig
⟨Ai,WW⊤⟩−yi/parenrightbig2,whereW∈Rd×d. (16)
When the loss reaches near zero (which implies the gradient also reaches near zero), it is known that multiple
local minimum solutions exist (Li et al., 2018), and the Hessian becomes
1
nn/summationdisplay
i=1∥AiW∥2
F≈d∥W∥2
F=d/vextenddouble/vextenddoubleWW⊤/vextenddouble/vextenddouble
⋆.
By prior results (Recht et al., 2010), among all X=WW⊤such that ˆL(W) = 0,X⋆has the lowest nuclear
norm. Thus, the regularization placed on ˆL(W)is similar to nuclear norm regularization after interpolating
the training dataset. We formalize this discussion and state the result below.
Proposition 5.1. In the setting above, for any Wthat satisfies ˆL(W) = 0, the following must hold with
high probability:
Tr/bracketleftig
∇2[ˆL(U⋆)]/bracketrightig
≤Tr/bracketleftig
∇2[ˆL(W)]/bracketrightig
+O(n−1
2). (17)
14Published in Transactions on Machine Learning Research (09/2024)
A similar statement holds if the trace operator is replaced by λ1in equation (17). To see this, we look at
the quadratic form of the Hessian to find the maximum eigenvalue. Let ube ad2dimension vector with
length equal to one, ∥u∥= 1. One can derive that:
λ1[∇2ˆL(W)] = max
u∈Rd2:∥u∥=1u⊤∇2ˆL(W)u= max
u∈Rd2:∥u∥=11
nn/summationdisplay
i=1⟨AiW,u⟩2≥1
d2nn/summationdisplay
i=1∥AiW∥2
F.
The last step is by setting u=d−11d2, whose length is equal to one. The proof of Proposition 5.1 can be
found in Appendix A.2.
Simulation: We conduct a numerical simulation to compare algorithmic behaviors. We generate a low-
rank matrix U⋆∈Rd×rfrom the isotropic Gaussian. We set d= 100andr= 5. Then, we test three
algorithms: gradient descent (GD), weight-perturbed gradient descent (WP-GD), and Algorithm 1 (NSO).
In particular, we will implement the full gradient update rather than using the stochastic updates. We use an
initialization U0∈Rd×dwhere each matrix entry is sampled independently from standard Gaussian N(0,1).
Recall that WP-GD and NSO require setting σ. We choose σbetween 0.001,0.002,0.004,0.008,0.0016. NSO
additionally requires setting the number of sampled perturbations k. We setk= 1for faster computing. As
for the learning rate, we choose a fixed ηfor each run and vary its value between 0.001,0.0025,0.005, and
0.01. We find that setting ηas either 0.005or0.01would be too large, leading the loss values to explode.
Hence, we report the results for setting ηas0.0025or0.001.
Our findings are illustrated in Figure 6.
•We see that all three algorithms can reduce the training MSE to near zero, as shown in Figure 6a.
From the trends, we can see that the training loss has fully converged for all cases.
•GD suffers from overfitting to training data, while both WP-GD and NSO can generalize to the
validation samples. Moreover, NSO reduces this validation loss further in Figure 6b.
•Finally, we can see in Figure 6c that our algorithm can indeed produce a more accurate estimate of
the ground truth matrix X⋆, as measured by the Frobenius norm distance between WiW⊤
iandX⋆.
•The results for varying learning rates can be found in the bottom panel from Figure 6d to 6f. The
comparative results remain consistent.
6 Discussions and Related Work
Using noise injection during neural network training has appeared in very early studies of machine learning
research (Hinton & Van Camp, 1993; An, 1996). Graves (2011) test a variety of variational inference ap-
proaches with different prior and posterior distributions with recurrent neural networks. Cohen et al. (2019)
examine the use of randomized smoothing (with different smoothing distributions) against different ℓpad-
versaries for certified robustness. Camuto et al. (2020) propose a layer-wise regularization scheme motivated
by adaptation patterns of weights through deeper layers. Yang et al. (2020) show how to turn any classifier
that classifies well under Gaussian noise into a new classifier robust to adversarial perturbation under the
ℓ2norm. One of the implications of their work is that smoothing with Gaussian noise naturally confers
adversarial robustness in the ℓ2norm. Bisla et al. (2022) conduct an extensive empirical study to explore the
connection between sharpness and generalization for training neural networks. Orvieto et al. (2023) analyze
Taylor’s expansion of the stochastic objective after noise injection, examining the induced regularization in
various neural network training settings, and find that layer-wise perturbation can improve generalization.
There is also a line of work on Hessian and sharpness in the edge of stability regime during gradient descent
dynamics (Cohen et al., 2021). In particular, the edge of stability refers to scenarios where the learning
rate goes out of bounds beyond the Lipschitz continuity of a function, which is inversely proportional to
the largest eigenvalue of the Hessian matrix. Long & Bartlett (2024) identify the edge of stability regime
15Published in Transactions on Machine Learning Research (09/2024)
0 1000 2000 3000 4000 5000
Number of epochs0.000.010.020.030.040.050.06Training MSEGD
NSO ( k= 1)
WP -GD
(a) Training loss
0 1000 2000 3000 4000 5000
Number of epochs10−310−210−1100Validation MSEGD
NSO ( k= 1)
WP -GD (b) Validation loss
0 1000 2000 3000 4000 5000
Number of epochs10−310−210−1100Normalized DistanceGD
NSO ( k= 1)
WP GD (c) Normalized distance
0 5000 10000 15000 20000
Number of epochs10−310−210−1100101Training MSENSO ( k= 1)
WP -GD
(d) Training loss
0 5000 10000 15000 20000
Number of epochs10−310−210−1100101Validation MSENSO ( k= 1)
WP -GD (e) Validation loss
0 5000 10000 15000 20000
Number of epochs10−310−210−1100101Normalized DistanceNSO ( k= 1)
WP -GD (f) Normalized distance
Figure 6: Comparing the training loss, validation loss, and the normalized Frobenius norm distance, i.e.,
∥WiW⊤
i−X⋆∥2
F
∥X⋆∥2
F, between GD, our approach (NSO), and weight-perturbed (WP) GD (which computes the full
gradient as opposed to the stochastic gradient). For the top panel, the learning rate is fixed at 0.0025for all
the runs. For the bottom panel, the learning rate is set at 0.0001.σis set as 0.008for WP-GD and NSO.
Also, we trained sufficiently long until the loss curves fully converged.
for the SAM algorithm, highlighting the differences of these regimes between SAM and gradient descent.
Agarwala & Dauphin (2023) present a detailed study of the gradient dynamics of SAM, documenting various
respects of this algorithm. They first analyze the full-batch gradient descent with unnormalized SAM in a
quadratic regression model. This analysis suggests that at initialization, full-batch SAM presents limited
suppression of the largest eigenvalue of the Hessian matrix. They also show that as the batch size decreases,
the regularization of SAM becomes stronger. This work underscores the intricate dynamics of SAM due
to its connection to the min-max problem, which is computationally intractable (Daskalakis et al., 2021).
Dauphin et al. (2024) provide an in-depth comparison between SAM and weight noise by examining the
structure of the Hessian during training. Our results in Section 2.1, which show that weight noise remains
ineffective (for fine-tuning), are consistent with the findings of this work. Wu et al. (2020) study the structure
of the Hessian and conduct experiments on how the Hessian structure changes based on architecture and
the training method.
Randomized smoothing has been studied in stochastic optimization under various contexts, for instance, es-
timating gradients in zeroth-order optimization (Duchi et al., 2015), and for nonsmooth convex optimization
problems (Duchi et al., 2012). In particular, Duchi et al. (2012) analyze the convergence rates of stochastic
optimization algorithms and examine a convolution-based smoothing technique for nonsmooth stochastic op-
timization problems by drawing stochastic gradient samples from the smoothed problem with an appropriate
choice of smoothing density. They show that with the ability to issue several queries to the stochastic oracle,
the original problem can be solved with faster convergence rates than a simple stochastic oracle. Besides,
recent research has investigated the query complexity of finding stationary points of nonconvex functions
(Carmon et al., 2020; Arjevani et al., 2023). These results provide a fine-grained characterization of the
complexity of iterative methods under different orders of gradient oracles.
16Published in Transactions on Machine Learning Research (09/2024)
The findings from our work suggest several avenues that seem ripe for future work. Can recent advancements
in optimization be used to design better noise injection algorithms with faster convergence? Can we better
understand the effect of noise injection on the Hessian during training (e.g., in tensor regression where saddle
points are known to exist (Li et al., 2020))? In particular, our work highlights the need for more accurate
measurements to understand the learning mechanisms of complex models.
7 Conclusion and Limitations
This paper examines the regularization and generalization effects of noise-injection methods for training
neural networks. The study begins by noting that a straightforward implementation of injecting noise into
weight matrices (of a neural network) before computing the gradient in SGD does not perform well in
practice. Thus, an alternative, two-point noise injection scheme is proposed and is shown to be effective
through extensive experiments. In particular, this new algorithm can be used to regularize the Hessian
and improve generalization. The results are tested on fine-tuning, pretraining, and instruction tuning. As
a complement, a PAC-Bayes generalization bound is provided to support the rationale of this approach.
Finally, this paper presents a detailed convergence analysis of the proposed algorithm.
Limitations: In Theorem 2.1, we have shown that the generalization error of a training algorithm can be
bounded by the trace of the Hessian of the loss matrix, scaled by the distance of the hypothesis space. Notice
that this result applies to both Algorithm 1 (NSO) and the naive noise injection algorithm (WP-SGD). As
shown in Figure 3, this result can provide a descriptive measure to explain different algorithms. Since the
Hessian measurements can be used on both algorithms, they can only distinguish one from another after
taking the measurements from the data. Thus, our generalization theory should be interpreted with this
data-dependent lens in mind. We hope future work could work on addressing such limitations, along with
designing more principled optimization algorithms for training neural networks.
Acknowledgements
H. Z. would like to thank Huy Nguyen, Zhiyuan Li, and Guanghui Lan for the discussions and for pointing
out several references during various stages of this work. The authors would also like to thank the anonymous
reviewers and the action editor for their constructive feedback. We acknowledge financial support from NSF
award IIS-2412008.
References
Atish Agarwala and Yann Dauphin. Sam operates far from home: eigenvalue regularization as a dynamical
phenomenon. In International Conference on Machine Learning , pp. 152–168. PMLR, 2023. 8, 16
Pierre Alquier. User-friendly introduction to pac-bayes bounds. arXiv preprint arXiv:2110.11216 , 2021. 3, 7
Guozhong An. The effects of adding noise during backpropagation training on a generalization performance.
Neural computation , 8(3):643–674, 1996. 2, 15
Maksym Andriushchenko and Nicolas Flammarion. Towards understanding sharpness-aware minimization.
InICML, 2022. 1
Yossi Arjevani, Yair Carmon, John C Duchi, Dylan J Foster, Nathan Srebro, and Blake Woodworth. Lower
bounds for non-convex stochastic optimization. Mathematical Programming , 199(1):165–214, 2023. 16
Sanjeev Arora, Rong Ge, Behnam Neyshabur, and Yi Zhang. Stronger generalization bounds for deep nets
via a compression approach. In ICML, 2018. 7
Peter L Bartlett, Philip M Long, and Olivier Bousquet. The dynamics of sharpness-aware minimization:
Bouncing across ravines and drifting towards wide minima. Journal of Machine Learning Research , 24
(316):1–36, 2023. 1, 13
17Published in Transactions on Machine Learning Research (09/2024)
Devansh Bisla, Jing Wang, and Anna Choromanska. Low-pass filtering sgd for recovering flat optima in the
deep learning optimization landscape. In International Conference on Artificial Intelligence and Statistics ,
pp. 8299–8339. PMLR, 2022. 15
Alexander Camuto, Matthew Willetts, Umut Simsekli, Stephen J Roberts, and Chris C Holmes. Explicit
regularisation in gaussian noise injections. Advances in Neural Information Processing Systems , 33:16603–
16614, 2020. 15
Yair Carmon, John C Duchi, Oliver Hinder, and Aaron Sidford. Lower bounds for finding stationary points
i.Mathematical Programming , 184(1-2):71–120, 2020. 3, 16
Olivier Catoni. Pac-bayesian supervised classification: the thermodynamics of statistical learning. arXiv
preprint arXiv:0712.0248 , 2007. 3, 7
Jeremy Cohen, Elan Rosenfeld, and Zico Kolter. Certified adversarial robustness via randomized smoothing.
Ininternational conference on machine learning , pp. 1310–1320. PMLR, 2019. 15
Jeremy Cohen, Simran Kaur, Yuanzhi Li, J Zico Kolter, and Ameet Talwalkar. Gradient descent on neural
networks typically occurs at the edge of stability. ICLR, 2021. 15
Constantinos Daskalakis, Stratis Skoulakis, and Manolis Zampetakis. The complexity of constrained min-
max optimization. In Symposium on Theory of Computing , 2021. 16
Yann N Dauphin, Atish Agarwala, and Hossein Mobahi. Neglected hessian component explains mysteries in
sharpness regularization. arXiv preprint arXiv:2401.10809 , 2024. 2, 16
Yoel Drori and Ohad Shamir. The complexity of finding stationary points with stochastic gradient descent.
InICML, 2020. 3, 13
John C Duchi, Peter L Bartlett, and Martin J Wainwright. Randomized smoothing for stochastic optimiza-
tion.SIAM Journal on Optimization , 22(2):674–701, 2012. 2, 4, 16
John C Duchi, Michael I Jordan, Martin J Wainwright, and Andre Wibisono. Optimal rates for zero-order
convex optimization: The power of two function evaluations. IEEE Transactions on Information Theory ,
2015. 2, 5, 12, 16
Gintare Karolina Dziugaite and Daniel Roy. Computing nonvacuous generalization bounds for deep (stochas-
tic) neural networks with many more parameters than training data. UAI, 2017. 8
Pierre Foret, Ariel Kleiner, Hossein Mobahi, and Behnam Neyshabur. Sharpness-aware minimization for
efficiently improving generalization. ICLR, 2021. 1, 2, 3, 8
Saeed Ghadimi and Guanghui Lan. Stochastic first-and zeroth-order methods for nonconvex stochastic
programming. SIAM Journal on Optimization , 23(4):2341–2368, 2013. 3, 12, 13, 27
Henry Gouk, Timothy M Hospedales, and Massimiliano Pontil. Distance-based regularisation of deep net-
works for fine-tuning. In Ninth International Conference on Learning Representations 2021 , 2022. 3,
11
Alex Graves. Practical variational inference for neural networks. Advances in neural information processing
systems, 24, 2011. 2, 15
Gregory Griffin, Alex Holub, and Pietro Perona. Caltech-256 object category dataset. 2007. 4
Geoffrey E Hinton and Drew Van Camp. Keeping the neural networks simple by minimizing the description
length of the weights. In Proceedings of the sixth annual conference on Computational learning theory , pp.
5–13, 1993. 2, 15
Sepp Hochreiter and Jürgen Schmidhuber. Flat minima. Neural computation , 9(1):1–42, 1997. 1
18Published in Transactions on Machine Learning Research (09/2024)
PavelIzmailov, DmitriiPodoprikhin, TimurGaripov, DmitryVetrov, andAndrewGordonWilson. Averaging
weights leads to wider optima and better generalization. UAI, 2018. 1
Haotian Ju, Dongyue Li, and Hongyang R Zhang. Robust fine-tuning of deep neural networks with hessian-
based generalization guarantees. ICML, 2022. 2, 4
Haotian Ju, Dongyue Li, Aneesh Sharma, and Hongyang R Zhang. Generalization in graph neural networks:
Improved pac-bayesian bounds on graph diffusion. AISTATS , 2023. 2
Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak Peter Tang.
On large-batch training for deep learning: Generalization gap and sharp minima. ICLR, 2017. 1
Guanghui Lan. First-order and stochastic optimization methods for machine learning , volume 1. Springer,
2020. 3
Dongyue Li and Hongyang R Zhang. Improved regularization and robustness for fine-tuning in neural
networks. Advances in Neural Information Processing Systems , 34:27249–27262, 2021. 4, 7
Yuanzhi Li, Tengyu Ma, and Hongyang Zhang. Algorithmic regularization in over-parameterized matrix
sensing and neural networks with quadratic activations. In Conference On Learning Theory , pp. 2–47.
PMLR, 2018. 14
Yuanzhi Li, Tengyu Ma, and Hongyang R Zhang. Learning over-parametrized two-layer neural networks
beyond ntk. In Conference on learning theory , pp. 2613–2682. PMLR, 2020. 17
Yong Liu, Siqi Mai, Minhao Cheng, Xiangning Chen, Cho-Jui Hsieh, and Yang You. Random sharpness-
aware minimization. Advances in Neural Information Processing Systems , 2022. 8
Philip M Long and Peter L Bartlett. Sharpness-aware minimization and the edge of stability. Journal of
Machine Learning Research , 25(179):1–20, 2024. 15
Subhransu Maji, Esa Rahtu, Juho Kannala, Matthew Blaschko, and Andrea Vedaldi. Fine-grained visual
classification of aircraft. arXiv preprint arXiv:1306.5151 , 2013. 4
David McAllester. A pac-bayesian tutorial with a dropout bound. arXiv preprint arXiv:1307.2118 , 2013. 3,
7, 21
Vaishnavh Nagarajan and Zico Kolter. Deterministic pac-bayesian generalization bounds for deep networks
via generalizing noise-resilience. ICLR, 2020. 2, 7
Behnam Neyshabur, Srinadh Bhojanapalli, and Nathan Srebro. A pac-bayesian approach to spectrally-
normalized margin bounds for neural networks. In International Conference on Learning Representations ,
2018. 7
Antonio Orvieto, Anant Raj, Hans Kersting, and Francis Bach. Explicit regularization in overparametrized
models via noise injection. AISTATS , 2023. 2, 15
Samiksha Pachade, Prasanna Porwal, Dhanshree Thulkar, Manesh Kokare, Girish Deshmukh, Vivek Sa-
hasrabuddhe, Luca Giancardo, Gwenolé Quellec, and Fabrice Mériaudeau. Retinal fundus multi-disease
image dataset (rfmid): A dataset for multi-disease detection research. Data, 6(2):14, 2021. 4
Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish
Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning
transferable visual models from natural language supervision. In ICML, 2021. 12
Benjamin Recht, Maryam Fazel, and Pablo A Parrilo. Guaranteed minimum-rank solutions of linear matrix
equations via nuclear norm minimization. SIAM review , 52(3):471–501, 2010. 14, 25
Shai Shalev-Shwartz and Shai Ben-David. Understanding machine learning: From theory to algorithms .
Cambridge university press, 2014. 6
19Published in Transactions on Machine Learning Research (09/2024)
Nathan Srebro and Adi Shraibman. Rank, trace-norm and max-norm. In International conference on
computational learning theory , pp. 545–560. Springer, 2005. 6
Martin J Wainwright. High-dimensional statistics: A non-asymptotic viewpoint , volume 48. Cambridge
University Press, 2019. 24, 25
Kaiyue Wen, Tengyu Ma, and Zhiyuan Li. How does sharpness-aware minimization minimize sharpness?
ICLR, 2023. 1, 3
Mitchell Wortsman, Gabriel Ilharco, Samir Ya Gadre, Rebecca Roelofs, Raphael Gontijo-Lopes, Ari S Mor-
cos, HongseokNamkoong, AliFarhadi, YairCarmon, SimonKornblith, andLudwigSchmidt. Modelsoups:
averaging weights of multiple fine-tuned models improves accuracy without increasing inference time. In
International conference on machine learning , pp. 23965–23998. PMLR, 2022. 1, 4
YikaiWu, XingyuZhu, ChenweiWu, AnnieWang, andRongGe. Dissectinghessian: Understandingcommon
structure of hessian in neural networks. arXiv preprint arXiv:2010.04261 , 2020. 16
Greg Yang, Tony Duan, J Edward Hu, Hadi Salman, Ilya Razenshteyn, and Jerry Li. Randomized smoothing
of all shapes and sizes. In International Conference on Machine Learning , pp. 10693–10705. PMLR, 2020.
15
Tong Zhang. Mathematical analysis of machine learning algorithms . Cambridge University Press, 2023. 3
20Published in Transactions on Machine Learning Research (09/2024)
A Omitted Proofs from Section 2
We state a few standard notations. Given two matrices X,Yhaving the same dimension, let ⟨X,Y⟩=
Tr[X⊤Y]denote the matrix inner product of XandY. Let∥X∥2denote the spectral norm (largest singular
value) ofX, and let∥X∥Fdenote the Frobenius norm of X. We use the big-O notation f(x) =O(g(x))to
indicate that there exists a fixed constant Cindependent of xsuch thatf(x)≤C·g(x)for large enough x.
A.1 Proof of the PAC-Bayes Bound
We will use the following PAC-Bayes bound. For reference, see, e.g., Theorem 2, McAllester (2013).
Theorem A.1. Suppose the loss function ℓ(fW(x),y)lies in a bounded range [0,C]given anyx∈Xwith
labely. For anyβ∈(0,1)andδ∈(0,1), with probability at least 1−δ, the following holds:
LQ(W)≤1
βˆLQ(W) +C/parenleftbig
KL(Q||P ) + log1
δ/parenrightbig
2β(1−β)n. (18)
This result provides flexibility in setting β. Our results will set βto balance the perturbation error of Qand
the KL divergence between PandQ. We will need the KL divergence between the prior Pand the posterior
Qin the PAC-Bayesian analysis. This is stated in the following result.
Proposition A.2. SupposeP=N(X,Σ)andQ=N(Y,Σ)are both Gaussian distributions with mean
vectors given by X∈Rp,Y∈Rp, and population covariance matrix Σ∈Rp×p. The KL divergence between
PandQis equal to
KL(Q||P ) =1
2(X−Y)⊤Σ−1(X−Y).
Specifically, if Σ =σ2Idp, then the above simplifies to
KL(Q||P ) =∥X−Y∥2
2
2σ2.
We will use Taylor’s expansion on the perturbed loss. This is stated precisely as follows.
Claim A.3. LetfWbe twice-differentiable, parameterized by weight vector W∈Rp. LetU∈Rpbe another
vector with dimension p. For anyWandU, the following identity holds
ℓ(fW+U(x),y) =ℓ(fW(x),y) +U⊤∇ℓ(fW(x),y) +U⊤[∇2ℓ(fW(x),y)]U+R2(ℓ(fW(x),y)),
whereR2(ℓ(fW(x),y)))is a second-order error term in Taylor’s expansion.
Proof.The proof follows by the fact that ℓ◦fWis twice-differentiable. Let η∈Rpbe a vector with the same
dimension as WandUfrom the mean value theorem. There must exist an ηbetweenWandU+Wsuch
that the following equality holds:
R2(ℓ(fW(x),y)) =U⊤/parenleftig
∇2[ℓ(fη(x),y)]−∇2[ℓ(fW(x),y)]/parenrightig
U.
This completes the proof of the claim.
We provide Taylor’s expansion of ℓQ−ℓbased on the above.
Lemma A.4. In the setting of Theorem 2.1, suppose each parameter is perturbed by an independent noise
drawn from N(0,σ2). LetℓQ(fW(x),y)be the perturbed loss with noise perturbation injection vector on W.
There exist some fixed value C1that do not grow with nand1/δsuch that
/vextendsingle/vextendsingle/vextendsingle/vextendsingleℓQ(fW(x),y)−ℓ(fW(x),y)−1
2σ2Tr/bracketleftbig
∇2[ℓ(fW(x),y)]/bracketrightbig/vextendsingle/vextendsingle/vextendsingle/vextendsingle≤C1σ3.
21Published in Transactions on Machine Learning Research (09/2024)
Proof.We take the expectation over Ufor both sides of the equation in Claim A.3. The result becomes
E
U[ℓ(fW+U(x),y)] =E
U/bracketleftbig
ℓ(fW(x),y) +U⊤∇ℓ(fW(x),y) +U⊤∇2[ℓ(fW(x),y)]U+R2(ℓ(fW(x),y))/bracketrightbig
.
Then, we use the perturbation distribution QonEU[ℓ(fW+U(x),y)], and get
ℓQ(fW(x),y) =E
U[ℓ(fW(x),y)] +E
U/bracketleftbig
U⊤∇ℓ(fW(x),y)/bracketrightbig
+E
U/bracketleftbig
U⊤∇2[ℓ(fW(x),y)]U/bracketrightbig
+E
U[R2(ℓ(fW(x),y))].
Since E[U] = 0, the first-order term will be zero in expectation. The second-order term becomes equal to
E
U/bracketleftbig
U⊤[∇2ℓ(fW(x),y)]U/bracketrightbig
=σ2Tr/bracketleftbig
∇2[ℓ(fW(x),y)]/bracketrightbig
. (19)
The expectation of the error term R2(ℓ(fW(x),y))be
E
U[R2(ℓ(fW(x),y))] =E
U/bracketleftbig
U⊤/parenleftbig
∇2[ℓ(fη(x),y)]−∇2[ℓ(fW(x),y)]/parenrightbig
U/bracketrightbig
≤E
U/bracketleftig
∥U∥2
2·/vextenddouble/vextenddouble∇2[ℓ(fη(x),y)]−∇2[ℓ(fW(x),y)]/vextenddouble/vextenddouble
F/bracketrightig
≲E
U/bracketleftig
∥U∥2
2·C1∥U∥2/bracketrightig
≲C1σ3.
Thus, the proof is complete.
The last piece we will need is the uniform convergence of the Hessian operator. The result uses the fact that
the Hessian matrix is Lipschitz continuous.
Lemma A.5. In the setting of Theorem 2.1, there exist some fixed values C2,C3that do not grow with
nand1/δ, such that with probability at least 1−δfor anyδ >0, over the randomness of the ntraining
examples, we have
/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble1
nn/summationdisplay
i=1∇2[ℓ(fW(xi),yi)]−E
(x,y)∼D/bracketleftbig
∇2[ℓ(fW(x),y)]/bracketrightbig/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble
F≤C2/radicalbig
log(C3n/δ)√n. (20)
The proof will be deferred to Section A.1.2. With these results ready, we will provide proof of the Hessian-
based generalization bound.
A.1.1 Proof of Theorem 2.1
Proof of Theorem 2.1. First, we separate the gap of L(W)and1
βˆL(W)into three parts:
L(W)−1
βˆL(W) =L(W)−LQ(W) +LQ(W)−1
βˆLQ(W) +1
βˆLQ(W)−1
βˆL(W).
By Lemma A.4, we can bound the difference between L(W)andLQ(W)by the Hessian trace plus an error:
L(W)−1
βˆL(W)≤− E
(x,y)∼D/bracketleftbiggσ2
2Tr/bracketleftbig
∇2[ℓ(fW(x),y)]/bracketrightbig/bracketrightbigg
+C1σ3+/parenleftig
LQ(W)−1
βˆLQ(W)/parenrightig
+1
β/parenleftig1
nn/summationdisplay
i=1σ2
2Tr/bracketleftbig
∇2[ℓ(fW(xi),yi)]/bracketrightbig
+C1σ3/parenrightig
.
After re-arranging the terms, we can get the following:
L(W)−1
βˆL(W)≤− E
(x,y)∼D/bracketleftbiggσ2
2Tr/bracketleftbig
∇2[ℓ(fW(x),y)]/bracketrightbig/bracketrightbigg
+1
nβn/summationdisplay
i=1σ2
2Tr/bracketleftbig
∇2[ℓ(fW(xi),yi)]/bracketrightbig
/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
E1
+1 +β
βC1σ3+LQ(W)−1
βˆLQ(W)
/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
E2. (21)
22Published in Transactions on Machine Learning Research (09/2024)
We will examine E1by separating it into two parts:
E1=1
β/parenleftigg
1
nn/summationdisplay
i=1σ2
2Tr/bracketleftbig
∇2[ℓ(fˆW(xi),yi)]/bracketrightbig
−E
(x,y)∼D/bracketleftbiggσ2
2Tr/bracketleftbig
∇2[ℓ(fW(x),y)]/bracketrightbig/bracketrightbigg/parenrightigg
(22)
+1−β
βσ2
2E
(x,y)∼D/bracketleftbig
Tr/bracketleftbig
∇2ℓ(fW(x),y)/bracketrightbig/bracketrightbig
. (23)
We can use the uniform convergence result of Lemma A.5 to bound equation (22), leading to:
σ2
2β/parenleftigg
1
nn/summationdisplay
i=1Tr/bracketleftbig
∇2ℓ(fW(xi),yi)/bracketrightbig
−E
(x,y)∼D/bracketleftbig
Tr/bracketleftbig
∇2ℓ(fW(x),y))/bracketrightbig/bracketrightbig/parenrightigg
≤σ2
2β·√p·/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble1
nn/summationdisplay
i=1∇2[ℓ(fW(xi),yi)]−E
(x,y)∼D/bracketleftbig
∇2[ℓ(fW(x),y)]/bracketrightbig/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble
F(by Cauchy-Schwarz)
≤σ2√p·C2/radicalbig
log(C3n/δ)
2β√n. (24)
In particular, the second step also uses the fact that the Hessian matrix is a symmetric pbypmatrix. As
for equation (23), we recall that
α:= max
(x,y)∼DTr/bracketleftbig
∇2ℓ(fW(x),y)/bracketrightbig
.
Combined with equation (24), we have shown that
E1≤σ2√p·C2/radicalbig
log(C3n/δ)
2β√n+1−β
βσ2
2·α. (25)
As forE2, we will use the PAC-Bayes bound of Theorem A.1. In particular, we set the prior distribution P
as the distribution of Uand the posterior distribution Qas the distribution of W+U. Thus,
E2≤C/parenleftbig
KL(Q||P ) + log1
δ/parenrightbig
2β(1−β)n≤C/parenleftig
∥W∥2
2
2σ2+ log1
δ/parenrightig
2β(1−β)n≤C(r2
2σ2+ logδ−1)
2β(1−β)n. (26)
The last step is because ∥W∥2≤rby the assumption of the hypothesis space. Combining equations (21),
(25), (26), we claim that with probability at least 1−2δ, the following must be true:
L(W)−1
βˆL(W)≤σ2√p·C2/radicalbig
log(C3n/δ)
2β√n+1−β
βσ2
2α+1 +β
βC1σ3+C(r2
2σ2+ log1
δ)
2β(1−β)n.(27)
Thus, we will now choose σandβ∈(0,1)to minimize the term above. In particular, we will set σas
σ2=r
1−β/radicalbigg
C
αn. (28)
By plugging in σto equation (27) and re-arranging terms, the gap between L(W)andˆL(W)
βbecomes:
L(W)−1
βˆL(W)≤1
β/radicalbigg
Cαr2
n+C2/radicalbig
2plog(C3n/δ)
2β√nσ2+1 +β
βC1σ3+C
2β(1−β)nlog1
δ.
Letβbe a fixed value close to 1and independent of Nandδ−1, and letϵ= (1−β)/β. We get
L(W)≤(1 +ϵ)ˆL(W) + (1 +ϵ)/radicalbigg
Cαr2
n+ξ,where
ξ=C2/radicalbig
2plog(C3n/δ)
2β√nσ2+/parenleftig
1 +1
β/parenrightig
C1σ3+C
2β(1−β)nlog1
δ.
Notice that ξis of orderO(n−3
4+n−3
4+ log(δ−1)n−1)≤O(log(δ−1)n−3
4). Therefore, we have finished the
proof of equation (6).
23Published in Transactions on Machine Learning Research (09/2024)
Remark A.6. Whenfis strongly convex, the lowest eigenvalue of the Hessian is bounded from below. Once
the algorithm reaches the global minimizer, our result from Theorem 6 can be used to provide a generalization
bound based on the trace of the Hessian. Notice that the noise injection will add some bias to this minimizer,
leading to a sub-optimal empirical loss. To remedy this issue, one can place the regularization of Hessian as
a constraint, similar to how ℓ2-regularization can be implemented as a constraint.
A.1.2 Proof of Lemma A.5
In this section, we provide the proof of Lemma A.5, which shows the uniform convergence of the loss Hessian.
Proof of Lemma A.5. LetC,ϵ >0, and letS={W∈Rp:∥W∥2≤C}. There exists an ϵ-cover ofS
with respect to the ℓ2-norm at most max/parenleftig/parenleftbig3C
ϵ/parenrightbigp,1/parenrightig
elements; see, e.g., Example 5.8 (Wainwright, 2019).
LetT⊆Sdenote the set of this cover. Recall that the Hessian ∇2[ℓ(fW(x),y)]isC1-Lipschitz for all
(W+U)∈S,W∈S. Then we have
/vextenddouble/vextenddouble∇2[ℓ(fW+U(x),y)]−∇2[ℓ(fW(x),y)]/vextenddouble/vextenddouble
F≤C1∥U∥2.
For parameters δ,ϵ> 0, letNbe theϵ-cover ofSwith respect to the ℓ2-norm. Define the event
E=/braceleftig
∀W∈T,/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble1
nn/summationdisplay
i=1∇2[ℓ(fW(xi),yi)]−E
(x,y)∼D/bracketleftbig
∇2[ℓ(fW(x),y)]/bracketrightbig/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble
F≤δ/bracerightig
.
By the matrix Bernstein inequality, we have
Pr[E]≥1−4·|N|·p·exp/parenleftbigg
−nδ2
2α2/parenrightbigg
.
Next, for any W∈S, we can pick some W+U∈Tsuch that∥U∥2≤ϵ. We have
/vextenddouble/vextenddouble/vextenddouble/vextenddoubleE
(x,y)∼D/bracketleftbig
∇2[ℓ(fW+U(x),y)]/bracketrightbig
−E
(x,y)∼D/bracketleftbig
∇2[ℓ(fW(x),y)]/bracketrightbig/vextenddouble/vextenddouble/vextenddouble/vextenddouble
F≤C1∥U∥2≤C1ϵ
/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble1
nn/summationdisplay
j=1∇2[ℓ(fW+U(xj),yj)]−1
nn/summationdisplay
j=1∇2[ℓ(fW(xj),yj)]/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble
F≤C1∥U∥2≤C1ϵ.
Therefore, for any W∈S, we obtain:
/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble1
nn/summationdisplay
j=1∇2[ℓ(fW(xj),yj)]−E
(x,y)∼D/bracketleftbig
∇2[ℓ(fW(x),y)]/bracketrightbig/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble
F≤2C1ϵ+δ.
We will also set the value of δandϵ. First, set ϵ=δ/(2C1)so that conditional on E,
/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble1
nn/summationdisplay
j=1∇2[ℓ(fW(xj),yj)]−E
(x,y)∼D/bracketleftbig
∇2[ℓ(fW(x),y)]/bracketrightbig/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble
F≤2δ.
The eventEhappens with a probability of at least:
1−4|T|p·exp/parenleftbigg
−nδ2
2α2/parenrightbigg
= 1−4p·exp/parenleftbigg
log|T|−nδ2
2α2/parenrightbigg
.
We have log|T|≤plog(3B/ϵ) =plog(6CC1/δ). If we set
δ=/radicalbigg
4pα2log(3τCC 1n/α)
n
24Published in Transactions on Machine Learning Research (09/2024)
so that log(3τCC 1n/α)≥1(becausen≥eα
3C1andτ≥1), then we get
plog(6CC1/δ)−nδ2/(2α2) =plog/parenleftigg
6CC1√n/radicalbig
4pα2log(3τCC 1n/α)/parenrightigg
−2plog (3τCC 1n/α)
=plog/parenleftigg
3CC1√n
α/radicalbig
plog(3τCC 1n/α)/parenrightigg
−2plog (3τCC 1n/α)
≤plog (3τCC 1n/α)−2plog (3τCC 1n/α)(τ≥1,log(3τCC 1n/α)≥1)
=−plog (3τCC 1n/α)≤−plog(eτ). (3CC1n/α≥e)
Therefore, with a probability greater than
1−4|N|p·exp(−nδ2/(2α2))≥1−4p(eτ)−p,
the following estimate holds:
/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble1
nn/summationdisplay
j=1∇2[ℓ(fW(xj),yj)]−E
(x,y)∼D/bracketleftbig
∇2[ℓ(fW(x),y)]/bracketrightbig/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble
F≤/radicalbigg
16pα2log(3τCC 1n/α)
n.
Denoteδ′= 4p(eτ)−p,C2= 4α√p, andC3= 12pCC 1/(eα). With probability greater than 1−δ′, the final
result is:/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble1
nn/summationdisplay
i=1∇2[ℓ(fW(xi),yi)]−E
(x,y)∼D/bracketleftbig
∇2[ℓ(fW(x),y)]/bracketrightbig/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble
F≤C2/radicalbigg
log(C3n/δ′)
n.
This completes the proof of Lemma A.5.
A.2 Proof of Proposition 5.1
Proof of Proposition 5.1. We can calculate the gradient as
∇ˆL(W) =1
nn/summationdisplay
i=1(⟨Ai,WW⊤⟩−yi)AiW. (29)
For a particular entry Wj,kofW, for any 1≤j,k≤d, the derivative of the gradient with respect to Wj,kis
1
nn/summationdisplay
i=1/parenleftbigg
[AiW]j,kAiW+/parenleftig
⟨Ai,WW⊤⟩−yi/parenrightig∂(AiW)
∂Wj,k/parenrightbigg
. (30)
When ˆL(W)is zero, the second term of equation (30) above must be zero, because ⟨Ai,WW⊤⟩is equal to
yi, for anyi= 1,...,n.
We use the assumption that Aiis a random Gaussian matrix, in which every entry is drawn from a normal
distribution with mean zero and variance one. Notice that the expectation of ∥AiW∥2
Fsatisfies:
E/bracketleftig
∥AiW∥2
F/bracketrightig
=E/bracketleftbig
Tr/bracketleftbig
W⊤A⊤
iAiW/bracketrightbig/bracketrightbig
= Tr/bracketleftbig
W⊤(d·Idd×d)W⊤/bracketrightbig
=d·Tr/bracketleftbig
W⊤W/bracketrightbig
=d∥W∥2
F.
Thus, by concentration inequality for χ2random variables (e.g., Wainwright (2019, equation (2.19))), the
following holds for any 0<ϵ< 1,
Pr/bracketleftigg/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle1
nn/summationdisplay
i=1∥AiW∥2
F−d∥W∥2
F/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle≥ϵd∥W∥2
F/bracketrightigg
≤2 exp/parenleftig
−nϵ2
8/parenrightig
. (31)
This implies that ϵmust be smaller than O(n−1/2)with high probability. As a result, the average of ∥AiW∥2
F
must bed∥W∥2
Fplus some deviation error that scales with n−1/2times the expectation.
By Theorem 3.2, Recht et al. (2010), the minimum Frobenius norm ( ∥W∥2
F) solution that satisfies ˆL(W) = 0
(for Gaussian random matrices) is precisely U⋆. Thus, we conclude that equation (17) holds.
25Published in Transactions on Machine Learning Research (09/2024)
B Omitted Proofs from Section 4
B.1 Proof of Proposition 4.2
Recall that each iteration involves two sources of randomness stemming from gzand{U(j)
i}k
j=1, respectively.
Let us define
δi=1
2kk/summationdisplay
j=1/parenleftbig
∇f/parenleftbig
Wi+U(j)
i/parenrightbig
+∇f/parenleftbig
Wi−U(j)
i/parenrightbig/parenrightbig
−∇F(Wi),
ξi=1
2kk/summationdisplay
j=1/parenleftbig
G(j)
i−∇f/parenleftbig
Wi+U(j)
i/parenrightbig
−∇f/parenleftbig
Wi−U(j)
i/parenrightbig/parenrightbig
,
fori= 0,...,T−1. One can see that both δiandξihave mean zero. The former is by the symmetry of P.
The latter is because gzis unbiased under Assumption 4.1. The following result gives their variance.
Lemma B.1. In the setting of Proposition 4.2, for any i= 1,...,T, we have
E/bracketleftig
∥ξi∥2/bracketrightig
≤σ2
kandE/bracketleftig
∥δi∥2/bracketrightig
≤C2H(P)
k. (32)
The last step uses smoothness to show that ∥∇F(Wt)∥keeps reducing.
Proof.Let us bound the variance of δiandξifori= 0,1,...,T−1. First, we see that
E
U1
i,...,Uk
i/bracketleftig
∥δi∥2/bracketrightig
=E
U1
i,...,Uk
i
/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble1
2kk/summationdisplay
j=1/parenleftig
∇f(Wi+Uj
i) +∇f(Wi−Uj
i)−2∇F(Wi)/parenrightig/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2

=1
k2k/summationdisplay
j=1E
Uj
i/bracketleftigg/vextenddouble/vextenddouble/vextenddouble/vextenddouble1
2/parenleftig
∇f(Wi+Uj
i) +∇f(Wi−Uj
i)−2∇F(Wi)/parenrightig/vextenddouble/vextenddouble/vextenddouble/vextenddouble2/bracketrightigg
(33)
=1
kE
U1
i/bracketleftigg/vextenddouble/vextenddouble/vextenddouble/vextenddouble1
2/parenleftig
∇f(Wi+U1
i) +∇f(Wi−U1
i)/parenrightig
−∇F(Wi)/vextenddouble/vextenddouble/vextenddouble/vextenddouble2/bracketrightigg
(34)
where in the second line we use that Uj1
iandUj2
iare independent when j1̸=j2, in the last line we use fact
thatU1
i,...,Uk
iare identically distributed. In the second step, we use the fact that for two independent
random variables U,V, and any continuous functions h(U),g(V),h(U)andg(V)are still independent (recall
thatfis continuous since it is twice-differentiable). We include a short proof of this fact for completeness.
IfUandVare independent, we have Pr[U∈A,V∈B] = Pr[U∈A]·Pr[V∈B], for anyA,B∈Borel (R).
Thus, ifhandgare continuous functions, we obtain
Pr[h(U)∈A,g(V)∈B] = Pr[U∈h−1(A),V∈g−1(B)]
= Pr[U∈h−1(A)]·Pr[V∈g−1(B)] = Pr[h(U)∈A]·Pr[g(V)∈B].
Thus, we have shown that
E/bracketleftig
∥δi∥2/bracketrightig
=1
kE
U∼P/bracketleftigg/vextenddouble/vextenddouble/vextenddouble/vextenddouble1
2/parenleftig
∇f(Wi+U) +f(Wi−U)/parenrightig
−∇F(Wi)/vextenddouble/vextenddouble/vextenddouble/vextenddouble2/bracketrightigg
. (35)
Next, we deal with the variance of the two-point stochastic gradient. We will show that
E
U/bracketleftigg/vextenddouble/vextenddouble/vextenddouble/vextenddouble1
2/parenleftig
∇f(W+U) +∇f(W−U)/parenrightig
−∇F(W)/vextenddouble/vextenddouble/vextenddouble/vextenddouble2/bracketrightigg
≤C2H(P). (36)
26Published in Transactions on Machine Learning Research (09/2024)
We mainly use the Lipschitz continuity of the gradient of F. The left-hand side of equation (36) is equal to
E
U/bracketleftigg/vextenddouble/vextenddouble/vextenddouble/vextenddouble1
2/parenleftig
∇f(W+U)−∇F(W)/parenrightig
+1
2/parenleftig
∇f(W−U)−∇F(W)/parenrightig/vextenddouble/vextenddouble/vextenddouble/vextenddouble2/bracketrightigg
≤E
U/bracketleftbigg1
2∥∇f(W+U)−∇F(W)∥2+1
2∥∇f(W−U)−∇F(W)∥2/bracketrightbigg
(by Cauchy-Schwartz)
=1
2E
U/bracketleftig
∥∇f(W+U)−∇F(W)∥2/bracketrightig
(by symmetry of Psince it has mean zero)
=1
2E
U/bracketleftigg/vextenddouble/vextenddouble/vextenddouble/vextenddoubleE
U′∼P[∇f(W+U)−∇f(W+U′)]/vextenddouble/vextenddouble/vextenddouble/vextenddouble2/bracketrightigg
≤1
2E
U/bracketleftbigg
E
U′∼P/bracketleftig
∥∇f(W+U)−∇f(W+U′)∥2/bracketrightig/bracketrightbigg
≤1
2E
U,U′/bracketleftig
C2∥U−U′∥2/bracketrightig
=1
2C2E
U,U′/bracketleftig
∥U∥2+∥U′∥2/bracketrightig
=C2H(P) (by equation (38))
As for the variance of ξi, we note that U(1)
i,...,U(j)
iare all independent from each other. Therefore,
E/braceleftbig
U(j)
i,z(j)
i/bracerightbigk
j=1/bracketleftig
∥ξi∥2/bracketrightig
=1
4kE
U,z/bracketleftig
∥gz(W+U)−∇f(W+U) +gz(W−U)−f(W−U)∥2/bracketrightig
≤1
2kE
U,z/bracketleftig
∥gz(W+U)−∇f(W+U)∥2+∥gz(W−U)−∇f(W−U)∥2/bracketrightig
≤σ2
k.
The first step uses the fact that both gz(·)andf(·)are continuous functions The second step above uses
Cauchy-Schwartz inequality. The last step uses the variance bound of gz(·), Thus, the proof is finished.
In the next step, we use a result from Theorem 2.1, Ghadimi & Lan (2013). Our proof follows from their
work, but we deal with some extra technical details related to the noise injection.
Lemma B.2 (Slightly adapted from Theorem 2.1, Ghadimi & Lan (2013)) .In the setting of Proposition 4.2,
for anyη0,···,ηT−1less thanC−1and a random variable according to a distribution Pr[t=j] =ηj/summationtextT−1
i=0ηi,
for anyj= 0,...,T−1, the following holds:
E/bracketleftig
∥∇F(Wt)∥2/bracketrightig
≤2C/summationtextT−1
i=0ηiD2+C/summationtextT−1
i=0η2
i/parenleftbig
E/bracketleftig
∥δi∥2/bracketrightig
+E/bracketleftig
∥ξi∥2/bracketrightig/parenrightbig
/summationtextT−1
i=0ηi. (37)
Proof.First, let us show that ∇FisC-Lipschitz continuous. To see this, we apply the Lipschitz condition
of the gradient inside the expectation of F(W). For anyW1,W2∈Rd, by definition,
∥∇F(W1)−∇F(W2)∥=/vextenddouble/vextenddouble/vextenddouble/vextenddouble∇E
U∼P[f(W1+U)]−∇E
U∼P[f(W2+U)]/vextenddouble/vextenddouble/vextenddouble/vextenddouble
=/vextenddouble/vextenddouble/vextenddouble/vextenddoubleE
U∼P[∇f(W1+U)−∇f(W2+U)]/vextenddouble/vextenddouble/vextenddouble/vextenddouble
≤E
U∼P[∥∇f(W1+U)−∇f(W2+U)∥]≤C∥W1−W2∥.
Since∇F(W)isC-Lipschitz continuous, we have the following domination inequality:
|F(W2)−F(W1)−⟨∇F(W1),W2−W1⟩|≤C
2∥W2−W1∥2. (38)
27Published in Transactions on Machine Learning Research (09/2024)
Based on the above inequality, we have
F(Wi+1)≤F(Wi) +⟨∇F(Wi),Wi+1−Wi⟩+C
2η2
i/vextenddouble/vextenddouble/vextenddouble/vextenddouble1
2/parenleftig
∇f(Wi+Ui) +∇f(Wi−Ui)/parenrightig
+ξi/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
=F(Wi)−ηi⟨∇F(Wi),δi+ξi+∇F(Wi)⟩+Cη2
i
2∥δi+ξi+∇F(Wi)∥2
=F(Wi)−/parenleftig
ηi−Cη2
i
2/parenrightig
∥∇F(Wi)∥2−/parenleftig
ηi−Cη2
i/parenrightig
⟨∇F(Wi),δi+ξi⟩+Cη2
i
2∥δi+ξi∥2.
Summing up the above inequalities for i= 0,1,...,T−1, we obtain
T−1/summationdisplay
i=0F(Wi+1)≤T−1/summationdisplay
i=0F(Wi)−T−1/summationdisplay
i=0/parenleftig
ηi−Cη2
i
2/parenrightig
∥∇F(Wi)∥2
−T−1/summationdisplay
i=0/parenleftig
ηi−Cη2
i/parenrightig
⟨∇F(Wi),δi+ξi⟩+T−1/summationdisplay
i=0Cη2
i
2∥δi+ξi∥2,
which implies that
T−1/summationdisplay
i=0/parenleftig
ηi−Cη2
i
2/parenrightig
∥∇F(Wi)∥2≤F(W0)−F(WT)−T−1/summationdisplay
i=0/parenleftig
ηi−Cη2
i/parenrightig
⟨∇F(Wi),δi+ξi⟩+C
2T−1/summationdisplay
i=0η2
i∥δi+ξi∥2
≤D2−T−1/summationdisplay
i=0/parenleftig
ηi−Cη2
i/parenrightig
⟨∇F(Wi),δi+ξi⟩+C
2T−1/summationdisplay
i=0η2
i∥δi+ξi∥2. (39)
where in the last step, we use the fact that
F(W0)−F(WT)≤F(W0)−min
W∈RdF(W)≤D2.
For anyt= 0,1,...,T−1, notice that as long as 0<ηt≤1
C, thenηt≤2ηt−Cη2
t.Hence, we have
1
2T−1/summationdisplay
t=0ηt∥∇F(Wt)∥2≤T−1/summationdisplay
t=0/parenleftig
ηt−Cη2
t
2/parenrightig
∥∇F(Wt)∥2,
which implies that
1
2T−1/summationdisplay
i=0ηi∥∇F(Wi)∥2≤D2−T−1/summationdisplay
i=0/parenleftig
ηi−Cη2
i/parenrightig
⟨∇F(Wi),δi+ξi⟩+C
2T−1/summationdisplay
i=0η2
i∥δi+ξi∥2.(40)
Additionally, since Utis drawn from a distribution with mean zero. Hence, by symmetry, we get that
E
Ut[δt] =1
2E
Ut[∇f(Wt−Ut)−∇f(Wt+Ut)] = 0. (41)
Thus, ifwetaketheexpectationover U0,U1,...,UT−1,ξ0,ξ1,...,ξT−1, then E[⟨∇F(Wi),δi+ξi⟩] = 0.Recall
thattis a random variable whose probability mass is specified in Lemma B.2. We can write equation (40)
equivalently as (below, we take expectation over all the random variables along the update since Wtis a
function of the previous gradient updates, for each t= 0,1,...,T−1, recalling that Pr[t=i] =ηi/summationtextT−1
j=0ηj)
E
t;U0,...,U T−1,ξ0,ξ1,...,ξT−1/bracketleftig
∥∇F(Wt)∥2/bracketrightig
=/summationtextT−1
i=0ηiE/bracketleftig
∥∇F(Wi)∥2/bracketrightig
/summationtextT−1
i=0ηi
≤2D2+C/summationtextT−1
i=0η2
iE/bracketleftig
∥δi+ξi∥2/bracketrightig
/summationtextT−1
i=0ηi
=2D2+C/summationtextT−1
i=0η2
i/parenleftbig
E/bracketleftig
∥δi∥2/bracketrightig
+E/bracketleftig
∥ξi∥2/bracketrightig/parenrightbig
/summationtextT−1
i=0ηi.
where we use the fact that δiandξiare independent for any i. Hence, equation (37) is proved.
28Published in Transactions on Machine Learning Research (09/2024)
Based on the above result, we now finish the proof of Proposition 4.2.
Proof of Proposition 4.2. Let the step sizes be a fixed ηfor all epochs. Thus, equation (37) becomes
E/bracketleftig
∥∇F(Wt)∥2/bracketrightig
≤2
TηD2+Cη
TT−1/summationdisplay
i=0/parenleftig
E/bracketleftig
∥δi∥2/bracketrightig
+E/bracketleftig
∥ξi∥2/bracketrightig/parenrightig
. (42)
By Lemma B.1,
T−1/summationdisplay
i=0/parenleftig
E/bracketleftig
∥δi∥2/bracketrightig
+E/bracketleftig
∥ξi∥2/bracketrightig/parenrightig
≤T·σ2+C2H(P)
k. (43)
For simplicity, let us denote ∆ =σ2+C2H(P)
k. The proof is divided into two cases.
Case 1: ∆is large. More precisely, suppose that ∆≥2CD2/T. Then, minimizing over ηabove leads us
to the following upper bound on the right-hand side of equation (42):
/radicalbigg
2CD2∆
T, (44)
which is obtained by setting η=/radicalig
2D2
C∆T.One can verify that this step size is less than1
Csince ∆is at least
2CD2. Thus, we conclude that equation (42) must be less than
/radicalbigg
2CD2∆
T=/radicalbigg
2CD2(σ2+C2H(P)))
kT. (45)
Case 2: ∆is small. In this case, suppose ∆<2CD2/T. Then, the right-hand side of equation (42) must
be less than
2D2
Tη+2C2D2η
T≤2CD2
T. (46)
Thus, combining equations (45) and (46), we have completed the proof of equation (12).
B.2 Proof of Theorem 4.3
Recall our construction from Section 4 as follows. Let etbe the basis vector for the t-th dimension, for
t= 0,1,...,T−1. Definef(W)as
f(W) =1
2G⟨W,e 0⟩2+T−1/summationdisplay
i=0hi(⟨W,ei+1⟩),
wherehia quadratic function parameterized by αi, defined as follow:
hi(x) =

Cα2
i
4|x|≤αi
−C(|x|−αi)2
2+Cα2
i
4αi≤|x|≤3
2αi
C(|x|−2αi)2
23
2αi≤|x|≤2αi
0 2 αi≤|x|.
For technical reasons, we define a truncated perturbation distribution P. Given a sample Ufrom ad-
dimensional isotropic Gaussian N(0,Idd), we truncate the i-th coordinate of Uso that ˜Ui= min(Ui,ai), for
some fixed ai>0that we will specify below, for all i= 0,1,...,d−1. LetPdenote the distribution of ˜U.
The proof of Theorem 4.3 is divided into two cases. First, we examine the case when the averaged learning
rate isO(T−1/2).
29Published in Transactions on Machine Learning Research (09/2024)
Lemma B.3. In the setting of Theorem 4.3, suppose the learning rates satisfy that/summationtextT−1
i=0ηi≤/radicalig
D2kT
2σ2C,
consider the function f(W)constructed in equation (13), we have
min
1≤t≤TE/bracketleftig
∥∇F(Wt)∥2/bracketrightig
≥D/radicalbigg
Cσ2
32kT.
Proof.Westartbydefiningagradientoraclebychoosingthenoisevectors {ξt}T−1
t=0tobeindependentrandom
variables such that
ξt=⟨ξt,et+1⟩et+1and|⟨ξt,et+1⟩|≤σ√
k, (47)
whereet+1is a basis vector whose (t+ 1)-th entry is one and otherwise is zero. In other words, only the
(t+ 1)-th coordinate of ξtis nonzero. Otherwise, the rest of the vector remains zero. We use ¯ξtto denote
the averaged noise variable as
¯ξt=1
kk/summationdisplay
i=1ξ(i)
t,
whereξ(i)
tis defined following the condition specified in equation (47). Thus, we can also conclude that
|⟨¯ξt,et+1⟩|≤σ√
k.
We consider the objective function f(W) :Rd→Rdefined above (see also equation (13), Section 4), with
αi=2ηiσ√
k,fori= 0,1,...,T. (48)
We will analyze the dynamics of Algorithm 1 with the objective function f(W)and the starting point
W0=D√
G·e0, whereG= max/braceleftbig
C−1,2/summationtextT−1
i=0ηi/bracerightbig
. For the first iteration, we have
W1=W0−η0/parenleftig1
2k/summationdisplay
i=1/parenleftbig
∇f(W0+U(i)
0) +∇f(W0−U(i)
0)/parenrightbig
+¯ξ0/parenrightig
= (1−η0G−1)W0−η0¯ξ0,
whereUis a random draw from the truncated distribution Pwith⟨U,ei⟩= min{Pi,ai}forai=ηi−1σ√
k.
Next, from the construction of h1, we get
1
2/parenleftbig
∇f(W1+U) +∇f(W1−U)/parenrightbig
=G−1⟨W1,e0⟩e0+1
2/parenleftig
h′
0/parenleftbig
η0⟨¯ξ0,e1⟩+⟨U,e1⟩/parenrightbig
e1+h′
0/parenleftbig
η0⟨¯ξ0,e1⟩−⟨U,e1⟩/parenrightbig
e1/parenrightig
.
Here, using the fact that α0=2η0σ√
kfrom equation (48) above, and the truncation of U, which implies
|⟨U,e1⟩|≤η0σ√
k, and⟨¯ξ0,e1⟩≤σ√
k, we obtain
/vextendsingle/vextendsingleη0⟨¯ξ0,e1⟩+⟨U,e1⟩/vextendsingle/vextendsingle≤2η0σ√
k=α0,and similarly/vextendsingle/vextendsingleη0⟨¯ξ0,e1⟩−⟨U,e1⟩/vextendsingle/vextendsingle≤2η0σ√
k=α0,
which implies that
h′
0(η0⟨¯ξ0,e1⟩+⟨U,e1⟩) =h′
0(η0⟨¯ξ0,e1⟩−⟨U,e1⟩) = 0.
This is the first update. Then, in the next iteration,
W2=W1−η1/parenleftig
G−1⟨W1,e0⟩+¯ξ1/parenrightig
=−(1−η1G−1)(1−η0G−1)W0−η0¯ξ0−η1¯ξ1.
30Published in Transactions on Machine Learning Research (09/2024)
Similarly, we use the fact that αi=2ηiσ√
kand the fact that |⟨U,ei+1⟩|≤ηiσ√
k, which renders the gradient as
zero similar to the above reasoning. This holds for any i= 1,2,...,T−1.
At thet-th iteration, suppose we have that
Wt=W0t−1/productdisplay
i=0/parenleftig
1−ηiG−1/parenrightig
−t−1/summationdisplay
i=0ηi¯ξi.
Then by induction, at the (t+ 1)-th iteration, we must have
Wt+1=Wt−ηt/parenleftig
G−1⟨Wt,e0⟩+¯ξt/parenrightig
=W0t/productdisplay
i=0/parenleftig
1−ηiG−1/parenrightig
−t/summationdisplay
i=0ηi¯ξi. (49)
Next, from the definition of htabove, we have that
F(W0)−min
W∈RdF(W) =F(W0) (the minimum can be attained at zero)
=1
2G(D√
G)2+T−1/summationdisplay
i=0C
4/parenleftig2ηiσ√
k/parenrightig2
(since⟨W0+U,ei+1⟩≤αi)
The above must be at most D2, which implies that we should set the learning rates to satisfy (after some
calculation)
1
T/parenleftigT−1/summationdisplay
i=0ηi/parenrightig2
≤T−1/summationdisplay
i=0η2
i≤kD2
2Cσ2. (50)
We note that for all z∈[0,1],1−z
2≥exp(logz
2). Thus, applying this to the right-hand side of equation
(49), we obtain that for any t,
t/productdisplay
i=0/parenleftig
1−ηiG−1/parenrightig
≥1
2, (51)
where we recall that G= max{C−1,2/summationtextT−1
i=0ηi}. Our calculation so far shows that for all the hiexcepth0,
the algorithm has not moved at all from its initialization at W0under the above gradient noise. We thus
conclude that
min
1≤i≤T∥∇F(Wi)∥2= min
1≤i≤T/parenleftig
G−1⟨W0,e0⟩/parenrightig2
(by the construction of F(·))
≥1
4G−2(D√
G)2(by equations (49) and (51))
=D2
4min/braceleftig
C,1
2/summationtextT−1
i=0ηi/bracerightig
(recall the definition of Gabove)
≥D2
4min/braceleftig
C,√
2Cσ2
2D√
kT/bracerightig
≥D/radicalbigg
Cσ2
32kT. (by equation (50))
In the first step, we use the fact that ⟨¯ξi,e0⟩= 0, for all 0 = 1,2,...,T−1. Thus, we have proved that
equation (14) holds for Wifor anyi= 1,2,...,T. The proof of Lemma B.3 is finished.
Next, let us consider another case of the lower bound.
Lemma B.4. In the setting of Theorem 4.3, suppose the learning rates satisfy that/summationtextT−1
i=0ηi≥/radicalig
D2kT
2σ2C
andηi=ηfor some fixed η≤C−1. Then, consider the function from equation (13), we have that
min1≤t≤TE/bracketleftig
∥∇F(Wt)∥2/bracketrightig
≥D/radicalig
Cσ2
32kT.
31Published in Transactions on Machine Learning Research (09/2024)
Proof.We define the functions g, parametrized by a fixed, positive constants α=1−ρT
1−ρ·2cησ, as follows:
g(x) =

−C
2x2+C
4α2|x|≤α
2,
C
2(|x|−α)2α
2≤|x|≤α,
0 α≤|x|.
One can verify that ∇gisC-Lipschitz continuous, but gis not twice-differentiable. We also consider a
chain-like function:
f(W) =g(⟨W,e 0⟩) +d−1/summationdisplay
t=0C
2⟨W,et+1⟩2. (52)
From the definition of f, its gradient is C-Lipschitz continuous. Similar to equation (47), we define an
adversarial gradient oracle by choosing the noise vectors {ξt}T−1
t=0to be independent random variables such
that
ξt=⟨ξt,et+1⟩,E/bracketleftbig
⟨ξt,et+1⟩2/bracketrightbig
=σ2,and|⟨ξt,et+1⟩|≤cσ,
wherecis a fixed constant. We use ¯ξtto denote the averaged noise variable as
¯ξt=k/summationdisplay
i=1ξ(i)
t.
Suppose{ξ(i)
t}k
i=1are i.i.d. random variables for any t, we have
|⟨¯ξt,et+1⟩|≤cσandE/bracketleftig/vextenddouble/vextenddouble¯ξt/vextenddouble/vextenddouble2/bracketrightig
≤σ2
k. (53)
Next, we analyze the dynamics of Algorithm 1 with the objective function f(W)and the starting point
W0=/summationtextd
i=1/radicalig
D2
Cd·ei. In this case, by setting ηi=ηfor alli= 0,1,...,T−1. Recall that η<C−1. Denote
byρ=Cη, which is strictly less than one.
Sincehtis an even function, its derivative h′
tis odd. For the first iteration, we have
W1=W0−η/parenleftig1
2/parenleftbig
∇f(W0+U) +∇f(W0−U)/parenrightbig
+¯ξ0/parenrightig
= (1−Cη)W0−η¯ξ0.
whereUis a truncate distribution of P∼N(0,Idd)with⟨U,e0⟩= min{P0,a0}anda0=cησ.
Using the fact that α=1−ρT
1−ρ·2cησ,|⟨U,e0⟩|≤cησ, and⟨¯ξ0,e0⟩≤cσ, we have
g′(η⟨¯ξ0,e0⟩+⟨U,e0⟩) +g′(η⟨¯ξ0,e0⟩−⟨U,e0⟩) =−2Cη⟨¯ξ0,e0⟩.
Then, in the next iteration,
W2=W1−η/parenleftig
Cd/summationdisplay
i=1⟨W1,ei⟩−Cη¯ξ0+¯ξ1/parenrightig
= (1−Cη)2W0−(1−Cη)η¯ξ0−η¯ξ1.
Similarly, we use the fact that α=1−ρT
1−ρ·2cησand the fact that |⟨U,e0⟩|≤cησ, which renders the gradient
asg′(x) =−Cx, for anyi= 1,2,...,T−1.
At thet-th iteration, suppose that
Wt= (1−Cη)tW0−t−1/summationdisplay
i=0(1−Cη)t−1−iη¯ξi.
32Published in Transactions on Machine Learning Research (09/2024)
Then by induction, at the (t+ 1)-th iteration, we have
Wt+1=Wt−η/parenleftig
Cd/summationdisplay
i=1⟨Wt,ei⟩−Ct−1/summationdisplay
i=0(1−Cη)t−1−iη¯ξi+¯ξt/parenrightig
= (1−Cη)t+1W0−t/summationdisplay
i=0(1−Cη)t−1−iη¯ξi. (54)
Next, from the definition of Fabove, we have that
F(W0)−min
W∈RdF(W) =F(W0) =dC
2/parenleftig/radicalbigg
D2
Cd/parenrightig2
+C
4/parenleftig2(1−ρT)cησ
(1−ρ)/parenrightig2
,(since⟨W0+U,e0⟩≤α)
which must be at most D2. Thus, we must have (after some calculation)
c2≤D2(1−ρ)2
2σ2ρ2(1−ρT)2.
We conclude that
min
1≤i≤TE/bracketleftig
∥∇F(Wi)∥2/bracketrightig
= min
1≤i≤TE
d/summationdisplay
j=1C2⟨Wi,ej⟩2+C2⟨Wi,e0⟩2

= min
1≤i≤T/parenleftig
dC2(1−ρ)2t/parenleftig/radicalbigg
D2
Cd/parenrightig2
+σ2
k·ρ2t/summationdisplay
i=0(1−ρ)2(t−1−i)/parenrightig
≥min
1≤i≤T/parenleftig
CD2(1−ρ)2t+σ2
kρ
2−ρ/parenleftbig
1−(1−ρ)2t/parenrightbig/parenrightig
≥min/braceleftig
CD2,σ2
kρ
2−ρ/bracerightig
≥σ2
kC/radicalbigg
kD2
2Tσ2C1
2−C/radicalig
kD2
2Tσ2C≥D/radicalbigg
Cσ2
16k·T. (after some calculation)
Thus, we have proved this lemma.
Taking both Lemma B.3 and B.4 together, we thus conclude the proof of Theorem 4.3.
B.3 Proof of momentum lower bound
In this section, we prove the following result.
Theorem B.5. There exists a quadratic function fsuch that for the iterates W1,...,WTgenerated by
equation (15), we must have: min1≤t≤TE/bracketleftig
∥∇F(Wt)∥2/bracketrightig
≥O/parenleftbig
D/radicalig
Cσ2
k·T/parenrightbig
.
We will focus on a perturbation distribution Pequal to the isotropic Gaussian distribution for this result.
In this case, we know that F(W) =f(W) +d. For the quadratic function f(W) =C
2∥W∥2, its gradient is
C-Lipschitz continuous. We set the initialization W0∈Rdsuch that
F(W0)−min
W∈RdF(W) =D2.
This condition can be met when we set W0as a vector whose Euclidean norm is equal to
D/radicaltp/radicalvertex/radicalvertex/radicalbt2 max/braceleftig
C−1,2T−1/summationdisplay
i=0ηi/bracerightig
.
33Published in Transactions on Machine Learning Research (09/2024)
The case when µ= 0.We begin by considering the case when µ= 0. In this case, the update reduces to
SGD, and the iterate Wt+1evolves as follows:
Wt+1=/parenleftig
1−Cηt/parenrightig
Wt−ηt¯ξt, (55)
where we denote ¯ξtas the averaged noise k−1/summationtextk
j=1ξ(j)
t, and the noise perturbation U(j)
tcancelled out
between the plus and minus perturbations. The case when µ > 0builds on this simpler case, as we will
describe below.
The key observation is that the gradient noise sequence ¯ξ1,¯ξ2,..., ¯ξTforms a martingale sequence:
•For anyi= 1,2,...,T, conditioned on the previous random variables ξ(j)
i′for anyi′< iand any
j= 1,2,...,k, the expectation of ¯ξiis equal to zero.
•In addition, the variance of ¯ξiis equal tok−1σ2, since conditional on the previous random variables,
theξ(j)
is are all independent from each other.
The martingale property allows us to characterize the SGD path of ∥Wt∥2, as shown in the following result.
Lemma B.6. In the setting of Theorem B.5, for any step sizes η0,...,ηT−1less thanC−1, and anyt=
1,...,T, the expected gradient of Wt,E/bracketleftig
∥∇F(Wt)∥2/bracketrightig
, is equal to
2CD2t−1/productdisplay
j=0/parenleftbig
1−Cηj/parenrightbig2+Cσ2
kt−1/summationdisplay
i=0η2
it−1/productdisplay
j=i+1/parenleftbig
1−Cηj/parenrightbig2.
Proof.By iterating over equation (55), we can get
Wt=W0t−1/productdisplay
j=0/parenleftig
1−Cηj/parenrightig
−t−1/summationdisplay
i=0ηi¯ξit−1/productdisplay
j=i+1/parenleftig
1−Cηj/parenrightig
.
Meanwhile,
∇F(Wt) =CWt⇒∥∇F(Wt)∥2=C2∥Wt∥2.
Thus, by squaring the norm of Wtand taking the expectation, we can get
E/bracketleftig
∥∇F(Wt)∥2/bracketrightig
=C2∥W0∥2t−1/productdisplay
j=0/parenleftbig
1−Cηj/parenrightbig2+C2t−1/summationdisplay
i=0E/bracketleftig/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleηi¯ξit−1/productdisplay
j=i+1/parenleftbig
1−Cηj/parenrightbig/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle2/bracketrightig
. (56)
Above, we use martingale property a), which says the expectation of ¯ξiis equal to zero for all i. In addition,
based on property b), equation (56) is equal to
C2t−1/summationdisplay
i=0η2
i
t−1/productdisplay
j=i+1/parenleftig
1−Cηj/parenrightig2
E/bracketleftig/vextenddouble/vextenddouble¯ξi/vextenddouble/vextenddouble2/bracketrightig
=C2σ2
kt−1/summationdisplay
i=0η2
it−1/productdisplay
j=i+1/parenleftig
1−Cηj/parenrightig2
.
To see this, based on the martingale property of ¯ξagain, the cross terms between ¯ξiand¯ξjfor different i,j
are equal to zero in expectation:
E/bracketleftbig
⟨¯ξi,¯ξj⟩|¯ξj/bracketrightbig
= 0,for all 1≤j <i≤T.
Additionally, the second moment of ¯ξisatisfies:
E/bracketleftig/vextenddouble/vextenddouble¯ξi/vextenddouble/vextenddouble2/bracketrightig
=σ2
k,for anyi= 1,...,T.
34Published in Transactions on Machine Learning Research (09/2024)
Lastly, letW0be a vector such that
∥W0∥=D√
2C−1⇒F(W0)−min
W∈RdF(W)≤D2.
Setting∥W0∥=D√
2C−1in equation (56) leads to
E/bracketleftig
∥∇F(Wt)∥2/bracketrightig
= 2CD2t−1/productdisplay
j=0/parenleftig
1−Cηj/parenrightig2
+C2σ2
kt−1/summationdisplay
i=0η2
it−1/productdisplay
j=i+1/parenleftig
1−Cηj/parenrightig2
.
Thus, we conclude the proof of this result.
We now present the proof for the case when/summationtextT−1
i=0ηi≤O(√
T). For this result, we will use the following
quadratic function:
f(W) =1
2κ∥W∥2,whereκ= max{C−1,2T−1/summationdisplay
i=0ηi}, (57)
Lemma B.7. Considerfgiven in equation (57)above. For any step sizes η0,...,ηT−1less thanC−1, the
following holds for the stochastic objective F:
min
1≤t≤TE/bracketleftig
∥∇F(Wt)∥2/bracketrightig
≥D2
2 max{C−1,2/summationtextT−1
i=0ηi}.
Proof.The norm of the gradient of F(W)is equal to
∥∇F(W)∥=1
κ∥W∥. (58)
Following the update rule in NSO, similar to equation (55), Wtevolves as follows:
Wt+1=/parenleftigg
1−ηt
κ/parenrightigg
Wt−ηt¯ξt, (59)
where ¯ξthas variance equal to σ2/k, according to the proof of Lemma B.6. By iterating equation (59) from
the initialization, we can get a closed-form equation for W(1)
t, for anyt= 1,2,...,T:
Wt=W0t−1/productdisplay
j=0/parenleftigg
1−ηj
κ/parenrightigg
−t−1/summationdisplay
k=0ηkξkt−1/productdisplay
j=k+1/parenleftigg
1−ηj
κ/parenrightigg
. (60)
Following equation (58), we can show that ∥∇F(W)∥2=κ−2∥Wt∥2.Thus, in expectation,
E/bracketleftig
∥∇F(Wt)∥2/bracketrightig
=κ−2E/bracketleftig
∥Wt∥2/bracketrightig
=κ−2∥W0∥2t−1/productdisplay
j=0/parenleftig
1−κ−1ηj/parenrightig2
+κ−2t−1/summationdisplay
i=0E

ηi¯ξit−1/productdisplay
j=i+1/parenleftig
1−κ−1ηj/parenrightig
2

=κ−2∥W0∥2t−1/productdisplay
j=0/parenleftig
1−κ−1ηj/parenrightig2
+κ−2t−1/summationdisplay
i=0η2
it−1/productdisplay
j=i+1/parenleftig
1−κ−1ηj/parenrightig2
E/bracketleftig/vextenddouble/vextenddouble¯ξi/vextenddouble/vextenddouble2/bracketrightig
= 2D2κ−1t−1/productdisplay
j=0/parenleftig
1−κ−1ηj/parenrightig2
+σ2κ−2
kt−1/summationdisplay
i=0η2
it−1/productdisplay
j=i+1/parenleftig
1−κ−1ηj/parenrightig2
, (61)
35Published in Transactions on Machine Learning Research (09/2024)
where we use the definition of initialization W0and the variance of ¯ξiin the last step. In order to tackle
equation (61), we note that for all z∈[0,1],
1−z
2≥exp/parenleftig
log1
2·z/parenrightig
. (62)
Hence, applyingequation(62)totheright-handsideofequation(61), weobtainthatforany i= 0,1,...,t−1,
t−1/productdisplay
j=i/parenleftigg
1−ηj
max{C−1,2/summationtextT−1
j=iηi}/parenrightigg
≥exp/parenleftigg
log1
2·t−1/summationdisplay
j=iηj
max{(2C)−1,/summationtextT−1
i=0ηi}/parenrightigg
≥1
2.
Thus, equation (61) must be at least
E/bracketleftig
∥∇F(Wt)∥2/bracketrightig
≥2D2κ−1
4+σ2κ−2
kt−1/summationdisplay
i=0η2
i
4. (63)
The above result holds for any t= 1,2,...,T. Therefore, we conclude that
min
1≤t≤TE/bracketleftig
∥∇F(Wt)∥2/bracketrightig
≥D2
2 max{C−1,2/summationtextT−1
i=0ηi}.
Thus, the proof of Lemma B.7 is finished.
Next, we consider the other case, which is when the learning rates are fixed.
Lemma B.8. There exists convex quadratic functions fsuch that for any gradient oracle satisfying Assump-
tion 4.1 and any distribution Pwith mean zero, if ηi=η<C−1for anyi= 1,...,T, or if/summationtextT−1
i=0ηi≲√
T,
then the following must hold:
min
1≤t≤TE/bracketleftig
∥∇F(Wt)∥2/bracketrightig
≥D/radicalbigg
Cσ2
32k·T. (64)
Proof.By Lemma B.7, there exists a function such that the left-hand side of equation (64) is at least
D2
2 max{C−1,2/summationtextT−1
i=0ηi}≥CD2
2 max{1,2x−1√
T}=D2x
4√
T, (65)
which holds if/summationtextT−1
i=0ηi≤√
Tx−1for any fixed x>0.
On the other hand, if/summationtextT−1
i=0ηi≥x−1√
Tandηi=ηfor a fixedη, thenη>x−1/√
T. By setting ηi=ηfor
alliin Lemma B.6, the left-hand side of equation (64) is equal to
min
1≤t≤T/parenleftig
2CD2(1−Cη)2t+C2σ2
kt−1/summationdisplay
k=0η2(1−Cη)2(t−k−1)/parenrightig
.
Recall that η < C−1. Thus,ρ=Cηmust be less than one. With some calculations, we can simplify the
above to
min
1≤t≤T/parenleftbigg
2CD2(1−ρ)2t+σ2ρ2
k1−(1−ρ)2t
1−(1−ρ)2/parenrightbigg
= min
1≤t≤T/parenleftbiggσ2ρ
k(2−ρ)+ (1−ρ)2t/parenleftig
2CD2−σ2ρ
k(2−ρ)/parenrightig/parenrightbigg
. (66)
If2CD2<σ2ρ
k(2−ρ), the above is the smallest when t= 1. In this case, equation (66) is equal to
2CD2(1−ρ)2+σ2ρ2
k≥1
1
2CD2+k
σ2=O(1).
36Published in Transactions on Machine Learning Research (09/2024)
If2CD2≥σ2ρ
k(2−ρ), the above is the smallest when t=T. In this case, equation (66) is at least
σ2ρ
k(2−ρ)≥σ2ρ
2k≥σ2Cx−1
2k·1√
T. (67)
To conclude the proof, we set xso that the right-hand side of equations (65) and (67) match each other.
This leads to
x=/radicalbigg
2σ2C
kD2.
Thus, by combining the conclusions from both equations (65) and (67) with this value of x, we finally
conclude that if/summationtextT−1
i=0ηi≤√
Tx−1, or for alli= 0,...,T−1,ηi=η<C−1, then in both cases, there exists
a functionfsuch that equation (64) holds. This completes the proof of Lemma B.8.
The case when µ > 0.In this case, since the update of Wtalso depends on the momentum update, it
becomes significantly more involved. One can verify that the update from step tto stept+ 1is based on
Xu=/bracketleftbigg1−Cηtµ
Cηtµ/bracketrightbigg
. (68)
Our analysis examines the eigenvalues of the matrix XuX⊤
uand the first entry in the corresponding eigenvec-
tors. Particularly, we show that the two entries are bounded away from zero. Then, we apply the Hölder’s
inequality to reduce the case of µ>0to the case of µ= 0, Lemma B.8 in particular.
Proof.First, consider a quadratic function
f(W) =1
2C∥W∥2.
Clearly,f(W)isC-Lipschitz continuous. Further, F(W) =f(W) +d, forPbeing the isotropic Gaussian.
LetW0be a vector whose Euclidean norm equals D√
2C. Thus,
F(W0)−min
W∈RdF(W) =D2.
As for the dynamic of momentum SGD, recall that
Mt+1=µMt−ηtGtandWt+1=Wt+Mt+1.
We consider the case where ηt=ηfor all steps t. In this case, we can write the above update into a matrix
notation as follows:
/bracketleftbiggWt+1
Mt+1/bracketrightbigg
=/bracketleftbigg1−Cη µ
−Cη µ/bracketrightbigg/bracketleftbiggWt
Mt/bracketrightbigg
+Cη/bracketleftbigg¯ξt
¯ξt/bracketrightbigg
.
LetXµ= [1−Cη,µ ;−Cη,µ ]denote the 2by2matrix (that depends on µ) above. Similar to Lemma B.6,
we can apply the above iterative update to obtain the formula for Wt+1as:
/bracketleftbiggWt+1
Mt+1/bracketrightbigg
=Xt
u/bracketleftbiggW0
M0/bracketrightbigg
+t/summationdisplay
i=0CηXt−i
u/bracketleftbigg¯ξi
¯ξi/bracketrightbigg
. (69)
By multiplying both sides by the vector e1= [1,0]⊤, and then taking the Euclidean norm of the vector
(notice that this now only evolves that Wt+1vector on the left, and the Wtvector on the right), we now
obtain that, in expectation over the randomness of the ¯ξi’s, the following holds:
E/bracketleftig
∥Wt+1∥2/bracketrightig
= 2CD2(e⊤
1Xt
ue1)2+C2η2σ2
kt/summationdisplay
i=0/vextenddouble/vextenddoublee⊤
1Xi
ue/vextenddouble/vextenddouble2. (70)
37Published in Transactions on Machine Learning Research (09/2024)
Above, similar to Lemma B.6, we have set the length of W0appropriately so that its size is equal to D√
2C−1,
which has led to the CD2term above. Recall that M0is equal to zero in the beginning. To get the first
term above, we follow this calculation:
/vextenddouble/vextenddouble/vextenddouble/vextenddoublee⊤
1Xt
µ/bracketleftbiggW0
M0/bracketrightbigg/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
= Tr/bracketleftigg
e⊤
1Xt
µ/bracketleftbiggW0
M0/bracketrightbigg/bracketleftbiggW0
M0/bracketrightbigg⊤
Xt
µ⊤e1/bracketrightigg
= Tr/bracketleftbigg
e⊤
1Xt
µ/bracketleftbigg
CD20
0 0/bracketrightbigg
Xt
µ⊤e1/bracketrightbigg
= 2CD2(e⊤
1Xt
µe1)2.
We usee= [1,1]⊤to denote the vector of ones. Now, we focus on the 2by2matrixXu(recall this is
the coefficient matrix on the right side of equation (69)). Let its singular values be denoted as λ1andλ2.
In addition, to deal with equation (70), let α1andα2denote the first entry of Xu’s left singular vectors,
corresponding to aandb, respectively. Thus, we can write
(e⊤
1Xi
µe)2=α2
1λ2i
1+α2
2λ2i
2. (71)
Now, one can verify that λ2
1andλ2
2are the roots of the following quadratic equation over x:
x2−((1−Cη)2+ (Cη)2+ 2µ2)x+µ2= 0. (72)
This can be checked by first taking XutimesX⊤
u, then using the definition of the eigenvalues by calculating
the determinant of XuX⊤
u−xId = 0. Thus, we have that λ1andλ2are equal to:
λ1,λ2=(1−Cη)2+ (Cη)2+ 2µ2±/radicalbig
((1−Cη)2+ (Cη)2+ 2µ2)2−4µ2
2. (73)
Now,α2
1(andα2
2, respectively) satisfies that:
α2
1=−Cη(1−Cη) +µ2
(1−Cη)2+µ2−λ1+−Cη(1−Cη) +µ2. (74)
By enumerating the possible values of Cηbetween 0and1, one can verify that for a fixed value of µ,α2
1and
α2
2are both bounded below from zero. Therefore, we can claim that from equation (71),
α2
1λ2i
1+α2
2λ2i
2≳λ2i
1+λ2i
2. (75)
By the Hölder’s inequality,
(λ2i
1+λ2i
2)1
2i(1 + 1)1−1
2i≥λ1+λ2= (1−Cη)2+ (Cη)2+ 2µ2(76)
≥(1−Cη)2+ (Cη)2, (77)
which implies that
λ2i
1+λ2i
2≥((1−Cη)2+ (Cη)2)i
2(2i−1). (78)
Now, we consider two cases. If Cη < 1/2, then the above is greater than (1−Cη)2i, which holds for any
i= 0,1,...,T−1. By way of reduction, we can follow the proof of Lemma B.8 to complete this proof. If
Cη> 1/2, then the above is greater than (Cη)2i. Again by following the proof steps in Lemma B.8, we can
show that
T
min
t=1E/bracketleftig
∥Wt∥2/bracketrightig
≳D/radicalbigg
Cσ2
k·T.
This completes the proof of Theorem B.5.
38Published in Transactions on Machine Learning Research (09/2024)
C Experiment Details
We describe the setup for Figure 2, ran on (1) a two-layer Multi-Layer Perceptron (MLP) trained on the
MNIST digit classification dataset, (2) a twelve-layer BERT-Base model trained on the MRPC sentence
classification dataset from the GLUE benchmark and (3) a two-layer Graph Convolutional Network (GCN)
trained on the COLLAB node classification dataset. We set both MLP and GCN with a hidden dimension
of 128 for model architectures and initialize them randomly. We initialize the BERT model from pretrained
BERT-Base-Uncased. We train each model on the provided training set for the training process until the
training loss is close to zero. Specifically, we train the MLP, BERT, and GCN models for 30, 10, and 100
epochs. We use the model of the last epoch to measure the error in the approximation. We do this for 100
times and again measure the perturbed loss ℓQon the training set. We take the gap between ℓQandℓ. Our
measurements show that the error between the actual gap and the Hessian approximation is within 3%.
Table6reportsadditionalcomparisonsbetweenourapproachandseveralbaselines,includinglabelsmoothing
(LS), random SAM (RSAM), and Bayesian SAM (BSAM). We report the test accuracy and the trace of the
Hessian for the model weights at the last epoch of training on six image classification datasets. We observe
that NSO also further reduces the trace of the Hessian and improves the test accuracy over the baselines.
The largest eigenvalue reduces by 9.7%.
Table 6: Comparison between our approach (NSO), label smoothing (LS), random-SAM (RSAM), and
Bayesian SAM (BSAM). Also included is the largest eigenvalue of the Hessian.
CIFAR-10 CIFAR-100 Aircraft Caltech-256 Indoor Retina
Trace
(↓)LS 2690±85 10669±363 5699±72 3482±85 3650±82 17681±193
RSAM 2379±89 9762±422 4665±95 3224±97 3425±70 16950±257
BSAM 2768±54 9787±465 4750±55 3498±38 3162±73 16238±286
NSO 1728±79 5244±89 3678±83 2958±77 2737±90 10970±146
Test
Acc
(↑)LS 96.9%±0.1 83.8%±0.1 59.0%±0.2 76.6%±0.2 76.5%±0.3 64.2%±0.7
RSAM 96.8%±0.1 84.0%±0.1 60.9%±0.4 76.4%±0.1 76.8%±0.5 65.9%±0.3
BSAM 96.9%±0.1 83.9%±0.2 61.0%±0.3 76.8%±0.3 76.4%±0.3 65.4%±0.2
NSO 97.6% ±0.4 84.9%±0.3 63.2%±0.3 78.1%±0.5 78.2%±0.3 67.0%±0.4
CIFAR-10 CIFAR-100 Aircraft Caltech-256 Indoor Retina
λ1
(↓)SGD 1442±63 4639±95 1152±40 1064±44 1087±56 8276±91
LS 1311±81 3051±95 1144±88 893±79 764±75 4296±74
SAM 1326±72 2625±91 890±90 948±95 887±53 4033±52
USAM 1245±43 2299±98 592±32 782±38 755±58 3893±55
ASAM 1383±73 2638±86 615±95 795±72 697±36 3925±56
RSAM 1356±69 2901±121 895±74 779±68 988±65 4537±58
BSAM 1375±86 2788±177 972±79 843±97 939±73 4123±87
NSO 1070±74 2059±45 579±59 643±57 639±72 3681±66
Finally, we report the hyper-parameters for the experiments in Section 3. These include a learning rate of
0.0002, momentum of 0.99, weight decay of 0.0001, batch size of 32, and training epochs of 60. We reduce
the learning rate by 0.1every 20epochs. We choose these hyper-parameters based on a grid search on the
validation split. The range in which we conduct a grid search is as follows: Learning rate: 0.005, 0.002,
0.001, 0.0005, 0.0002, and 0.0001; Momentum: 0.9, 0.95, 0.99; Weight decay: 0.01, 0.001, 0.0001; Epochs:
20, 40, and 60; Batch size: 16, 32, and 64.
Each baseline method may have its own set of hyper-parameters, which are adjusted via a grid search. For
label smoothing, we choose the weight of the loss calculated from the incorrect labels between 0.1, 0.2, and
0.3; For SAM and BSAM, we choose the ℓ2norm of the perturbation between 0.01, 0.02, and 0.05; For
ASAM, we choose the ℓ2norm of the perturbation for the weights between 0.5, 1.0, and 2.0; For RSAM, we
choose theℓ2norm of the perturbation between 0.01, 0.02, and 0.05 and the standard deviation for sampling
perturbation between 0.008, 0.01, and 0.012.
39