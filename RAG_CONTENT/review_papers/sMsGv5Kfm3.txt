Published in Transactions on Machine Learning Research (01/2023)
Bayesian Causal Bandits with Backdoor Adjustment Prior
Jireh Huang jirehhuang@ucla.edu
Department of Statistics
University of California, Los Angeles
Qing Zhou zhou@stat.ucla.edu
Department of Statistics
University of California, Los Angeles
Reviewed on OpenReview: https: // openreview. net/ forum? id= sMsGv5Kfm3
Abstract
The causal bandit problem setting is a sequential decision-making framework where actions
of interest correspond to interventions on variables in a system assumed to be governed by
a causal model. The underlying causality may be exploited when investigating actions in
the interest of optimizing the yield of the reward variable. Most existing approaches assume
prior knowledge of the underlying causal graph, which is in practice restrictive and often
unrealistic. In this paper, we develop a novel Bayesian framework for tackling causal bandit
problems that does not rely on possession of the causal graph, but rather simultaneously
learns the causal graph while exploiting causal inferences to optimize the reward. Our meth-
ods efficiently utilize joint inferences from interventional and observational data in a unified
Bayesian model constructed with intervention calculus and causal graph learning. For the
implementationofourproposedmethodologyinthediscretedistributionalsetting, wederive
an approximation of the sampling variance of the backdoor adjustment estimator. In the
Gaussian setting, we characterize the interventional variance with intervention calculus and
propose a simple graphical criterion to share information between arms. We validate our
proposed methodology in an extensive empirical study, demonstrating compelling cumula-
tive regret performance against state-of-the-art standard algorithms as well as optimistic
implementations of their causal variants that assume strong prior knowledge of the causal
structure.
1 Introduction
The multi-armed bandit (MAB) problem is a well-known sequential allocation framework for experimental
investigations (Berry & Fristedt, 1985). Classically, the MAB problem formulation features an action set
Aconsisting of|A|=Kactions, also called arms, typically corresponding to interventions. Each arm
a∈Adefines a real-valued distribution for the reward signal, with expected reward µa. The objective of
an allocation policy is to sequentially pick arms in a manner that maintains a balance between exploration
and exploitation in the interest of identifying and obtaining the greatest reward. Maximally and effectively
utilizing all available information is imperative, especially when investigating interventions that are either
or both resource-demanding and time-consuming.
Lattimore et al. (2016) proposed the causal bandit (CB) problem setting wherein a non-trivial probabilistic
causal model is assumed to govern the distribution of the reward variable and its covariates (Pearl, 2000).
The addition of causal assumptions introduces avenues by which interventional distributions may be inferred
from observational distributions and information may be shared between arms. Most works addressing the
CB problem exploit strong assumptions as to prior knowledge of the underlying causal model to achieve
improvements over standard MAB algorithms. In this work, we develop a Bayesian CB framework that
1Published in Transactions on Machine Learning Research (01/2023)
does not require prior knowledge of the underlying causal structure, but instead efficiently utilizes previously
available observational data and acquired interventional data to inform exploitation and guide exploration.
Forillustrativepurposes, weborrowandadaptthefarmingexampledescribedinLattimoreetal.(2016)asan
illuminating motivating example of the problem setting of interest and the surrounding challenges. Suppose
a farmer wishes to optimize the yield of a certain crop, which she knows is only dependent on temperature,
a particular soil nutrient, and moisture level. While she understands that crop yield is somehow affected
by these factors, the underlying causality governing this system of four variables (including crop yield) is
unknown to her. The farmer’s resource limitations restrict her to intervening on at most one factor in each
crop season by adjusting the temperature, controlling the soil nutrient content, or regulating moisture level.
These experimental interventions are costly to perform, and each realization of the interventional data can
only be observed once a season. Hence, it is in the farmer’s best interest to leverage her historical logs
containing observational data accrued from previous seasons where no interventions were performed, but
rather the variables were passively observed as they naturally varied from season to season. In this paper,
we propose a framework by which the farmer may aggregate and synthesize the available evidence to optimize
resource allocation to attain the highest yield.
1.1 Related Work
In its original formulation by Lattimore et al. (2016), the CB problem presupposes knowledge of the un-
derlying causal graph. Accordingly, most proposed CB algorithms require knowledge of the causal graph
structure (Lattimore et al., 2016; Lee & Bareinboim, 2018; Maiti et al., 2021; Yabe et al., 2018), and some
additionally assume certain model parameters are given (Lu et al., 2020; Nair et al., 2021). Furthermore,
manyapproachesaredependentonsomerestrictiveformorclassofgraphs. Theseassumptionsarerestrictive
and often unrealistic in practice.
More recently, Lu et al. (2021) proposed a central node approach based on the work of Greenewald et al.
(2019) that does not assume prior knowledge of the causal graph, but rather asymptotic knowledge of the
observational distribution. Their approach is restrictive in terms of structural and distributional assump-
tions, and while it is generally reasonable to assume that observational data is much more accessible than
interventional data (Greenewald et al., 2019), the large-sample observational setting is not often realistic.
de Kroon et al. (2022) proposed an estimator using separating sets to share information between arms with-
out assuming prior knowledge of nor requiring discovery of the causal graph. Their methodology makes
no attempt to learn the causal graph, and makes use of observational data only to strengthen conditional
independence testing to identify separating sets.
Relevant to our work is intervention calculus, a set of inference rules proposed by Pearl (2000) that defines
avenues by which interventional probabilities may be estimated from observational data. In the CB setting,
Lattimore et al. (2016) and Nair et al. (2021) consider graph structures with no confounding such that the
interventional distributions are equivalent to conditional distributions, and Maiti et al. (2021) proposed a
consistentestimatorfortheexpectedrewardfordiscretevariablesusingbothinterventionalandobservational
data in the presence of confounding. Our work extends the Bayesian model averaging approach proposed
by Pensar et al. (2020) wherein the possible causal effect estimates are averaged across an observational
posterior distribution of graphs.
Bayesian approaches to the MAB problem has received significant attention in the past decade. Russo &
Van Roy (2014) translated existing regret bounds of algorithms based on optimism in the face of uncertainty
to Bayesian regret bounds for posterior sampling by establishing a deep connection between the two in
their study of the Bayesian regret. In their information-theoretic analysis of Thompson sampling, Lu &
Van Roy (2019) demonstrated improved regret performance by leveraging prior information regarding the
bandit environment, which was further tightened in the form of a prior-dependent bound by Kveton et al.
(2021). Numerous developments to learn the prior have been proposed in the bandit meta-learning space
(Basu et al., 2021; Kveton et al., 2021; Wan et al., 2021; Hong et al., 2022), with many works studying the
effects of prior misspecification (Bastani et al., 2022; Peleg et al., 2022; Simchowitz et al., 2021). While
these contributions operate in the multi-task setting, seeking to learn the prior by solving many similar tasks
typically corresponding to bandit instances, our work more resembles the approach of Kaufmann et al. (2012)
2Published in Transactions on Machine Learning Research (01/2023)
in applying Bayesian techniques to tackle a single bandit instance. In such a way, rather than an estimate
of the task prior, our proposed prior is best interpreted as encoding the prior information on the underlying
parameters of a given bandit instance based on available observational data generated within the task.
1.2 Our Contributions
We approach the CB problem from a Bayesian perspective, assuming simply that finite samples of observa-
tional data are available. Importantly, we do not assume the causal graph is known, nor are we restrictive
as to the class of graph structures. We design a novel Bayesian CB framework called Bayesian Backdoor
Bandit (BBB) that efficiently utilizes the entirety of evidence from an ensemble of observational and inter-
ventional data in a unified Bayesian model. Our proposed BBB methodology quantifies the uncertainty in
the expected reward estimates as contributed to by the reward signal and the causal model to identify po-
tentially profitable exploration, simultaneously learning the causal graph in addition to and for the purposes
of improving estimates to exploit. Through extensive numerical experiments, we validate our methodology
by demonstrating compelling empirical performance against both non-causal and causal algorithms. In par-
ticular, we show that our BBB approach is able to leverage modest samples of observational data, seamlessly
integrating causal effect estimation and structure modeling, to achieve substantially superior cumulative
regret performance compared to standard algorithms that make no use of observational information. We
similarly demonstrate competitive performance against a generously optimistic version of the causal central
node approach proposed by Lu et al. (2021) that assumes large-sample observational data. A preliminary
analysis shows that, under some assumptions on the posterior distributions in our Bayesian model, the de-
pendence of the cumulative regret of a BBB algorithm on the size of the action space can be greatly relaxed
given sufficient amount of observational data.
Additionally, in detailing the application of our methods to the discrete and Gaussian distributional settings,
we propose various developments that are of independent interest. In the discrete setting, we derive an ap-
proximation for the sampling variance of the backdoor adjustment probability estimate. In the Gaussian
setting, we characterize the interventional variance of a target variable using intervention calculus and corre-
spondingly propose an estimator, and we propose a simple graphical criterion for sharing causal information
between arms to perform intervention calculus with jointly observational and interventional data.
The remainder of the paper is arranged as follows. We first review relevant background and notation in
Section 2. Then, we develop the formulation of our proposed Bayesian backdoor adjustment prior and its
posterior update in Section 3, discussing the design of informative conditional priors given a graph and
Bayesian model averaging across graph structures. In Section 4, we develop our proposed algorithms by
applying established MAB algorithms under the BBB framework, and we discuss details regarding the
implementation of BBB in the discrete and Gaussian settings in Section 5. Finally, we provide extensive
empirical results in Section 6 and conclude with a discussion in Section 7. Appendices A through D contain
proofs, additional details and numerical results, and technical derivations.
2 Preliminaries
We consider the setting where the generative model governing a joint probability distribution Pof a set of
pvariables X={X1,...,Xp}is a causal Bayesian network (CBN). A CBN model is defined by B= (G,ΘG)
consisting of its structure G, which takes the form of a directed acyclic graph (DAG), and its parameters
ΘG. Its DAGG= (V,E), often referred to as the underlying causal graph, is composed of a set of nodes
V={1,...,p}in one-to-one correspondence with the variables, and a set of directed edges Eoriented such
that there are no directed cycles. As is standard in causal literature, we may refer to a node i∈Vand
its corresponding variable Xi∈Xinterchangeably. In our work, we assume that Xis a causally sufficient
system with no unobserved confounders.
The causal implications imposed by the underlying CBN of Xare expressed in the form of a structural
equation model (SEM), Xi=f(PaG
i,εi)for alli∈V, where PaG
i={Xj:j→i∈E}is the parents of
XiinGandεiis an exogenous noise term. Otherwise stated, each variable Xiis a function of its direct
causes inGand an independent noise variable, which defines its conditional distribution P(Xi|PaG
i,θG
i)
3Published in Transactions on Machine Learning Research (01/2023)
with local parameters θG
i∈ΘG. The joint distribution Pimposed by the CBN factorizes according to
structureG:P(X) =/producttextp
i=1P(Xi|PaG
i,θG
i). Realizations from P(X)without any experimental intervention
are referred to as observational data, whereas interventional data are realizations of the SEM when the value
of one or more variables are being controlled by intervention. In our work, we consider deterministic atomic
interventions denoted do(Xj=xj)(Pearl, 1995), where a single variable is forcibly controlled to a fixed value
xj∈Dom(Xj). This has the effect of mutilating the causal graph by deleting the direct effects of PaG
jon
j, correspondingly modifying the SEM to Xj=xjandXi=f(PaG
i,εi)for alli∈V\j. The interventional
distribution P(X\Xj|do(Xj=xj))is not in general equivalent to the observational conditional distribution
P(X\Xj|Xj=xj), motivating the calculation of the interventional distribution from the observational
distribution, such as (1) below, which we refer to as interventional calculus.
The action setAconsists of|A|=Karms that correspond to interventions on variables in X\Y, whereY=
Xpis the reward variable (Lattimore et al., 2016). In particular, let arm a∈Acorrespond to the intervention
do(X⟨a⟩=xa), fixingX⟨a⟩to some value xa∈Dom(X⟨a⟩), where⟨a⟩∈Vis the node corresponding to the
intervened variable. The expected reward of each arm a∈Ais given by µa:= EP[Y|do(X⟨a⟩=xa)]
where EP[·]is the expectation in Pdefined by CBNB, and there is some optimal arm a∗:= argmaxa∈Aµa
corresponding to the optimal reward µ∗:=µa∗. Given a horizon of time steps T, letat∈Abe the arm
pulled by an algorithm at time step t∈{1,...,T}. The objective of the algorithm is to pull arms over T
time steps with a balance between exploring different arms and exploiting the reward signal to minimize the
expected cumulative regret E[/summationtextT
t=1(µ∗−µat)].
We now describe a Bayesian approach to the general MAB problem, with some notation adapted from
Kaufmann et al. (2012). The parameters ΘA= (θa)a∈A, assumed to mutually independently define the
corresponding marginal reward distributions pθa(y):=P[Y=y|do(X⟨a⟩=xa)], jointly follow a modular
prior distribution Π0(ΘA) =/producttext
a∈Aπ0
a(θa). Typically, (π0
a)a∈Aare chosen to be all equal and uninformative.
Whenarmat∈Aispulledattimestep tandarealization yt←Y|do(X⟨at⟩=xat)isobserved, theposterior
Πtis computed by updating according to πt
at(θat)∝pθat(yt)πt−1
at(θat), whileπt
a=πt−1
afora̸=at. For each
arma∈A, the posterior πt
ainduces a posterior distribution for the expected reward µa, which is simply a
marginal or transformation of πt
asince, in general, µais a function of θa. These posteriors are utilized by
Bayesian MAB algorithms, which we discuss and apply under our proposed framework in Section 4.
In our problem formulation, we assume possession of n0samples of observational data D0prior to inves-
tigating arms. We denote by D(t)the interventional data acquired by pulling arm atat timet, and by
Da[t] =/uniontext
l≤t,al=aD(l)the accumulated interventional data from arm athrough time t. The combined ob-
servational and interventional data accrued through time tisD[t] =D0∪/uniontext
a∈ADa[t], which we refer to as
ensemble data.
3 Designing Informative Priors with Intervention Calculus
In this section, we detail the design of the cornerstone of BBB and what we refer to as the backdoor ad-
justment prior, an informative prior Π0that distinguishes BBB from standard non-causal bandit algorithms
by encoding inferences from observational data and seamlessly integrating interventional data to update to
the posterior Πt. We begin by introducing the construction of conditional priors given backdoor adjustment
sets before continuing to obtain the backdoor adjustment prior by Bayesian model averaging over parent
set probabilities. We conclude by discussing the formulation and considerations of the posterior distribution
over graph structures that gives rise to the parent set probabilities.
Conditional Priors . For each arm a∈A, we construct conditional priors π0
a|Z(θa)using the backdoor
adjustment given sets Z⊆X\X⟨a⟩as follows. If Zsatisfies the backdoor criterion relative to X⟨a⟩and
Y(Pearl, 2000, Definition 3.3.1), then the interventional distribution Y|do(X⟨a⟩=xa)may be expressed
in terms of the joint observational distribution of {X⟨a⟩,Y}∪Zvia the backdoor adjustment (Pearl, 2000,
Theorem 3.3.2):
P[Y=y|do(X⟨a⟩=xa)] =/summationdisplay
z∈Dom( Z)P(Y=y|X⟨a⟩=xa,Z=z)P(Z=z). (1)
4Published in Transactions on Machine Learning Research (01/2023)
Eq. (1) provides an avenue through which an estimator for µausing observational data may be derived,
which we denote ˆµa,bda(Z)and with which we design an informative prior π0
a|Zsuch that the induced prior
distribution of the expected reward µasatisfies
Eπ0
a|Z[µa] = ˆµa,bda(Z),Varπ0
a|Z[µa] =ˆSE2[ˆµa,bda(Z)]. (2)
Here, Eπ0
a|Z[·]andVarπ0
a|Z[·]are respectively the expectation and variance in π0
a|Z, and ˆSE2[ˆµa,bda(Z)]is
the estimated sampling variability of the expected reward estimate ˆµa,bda(Z)inP. The matching of the
prior variance with the sampling variance of the backdoor adjustment estimator endeavors to assign the
appropriate prior effective sample size. When arm at=ais pulled at time step t∈ {1,...,T}and a
realization of the reward yt←Y|do(X⟨a⟩=xa)is observed inD(t), the posterior πt
a|Zis computed by
updating according to πt
a|Z(θa)∝pθa(yt)πt−1
a|Z(θa).
Parent Set Averaging . Thus far we have taken for granted the possession of adjustment set Z, the validity
of which is dependent on the underlying causal structure Gwhich we assume to be unknown. If Y̸∈PaG
⟨a⟩,
thenZ=PaG
⟨a⟩satisfies the backdoor criterion relative to X⟨a⟩andY, and its uncertainty is quantified by
the posterior probability P(Pa⟨a⟩=Z|D[t])given the ensemble data at time t. Accordingly, the posterior
ofθais determined by averaging over all possible parent sets for X⟨a⟩:
πt
a(θa) =/summationdisplay
Z⊆X\X⟨a⟩πt
a|Z(θa)P(Pa⟨a⟩=Z|D[t]), (3)
which is the key posterior distribution to be updated at each time step tin the Bayesian CB problem. Note
that ifY∈PaG
⟨a⟩, thenP[Y=y|do(X⟨a⟩=xa)] =P(Y=y)holds straightforwardly for y∈Dom(Y).
Accordingly, if Y∈Z, we compute ˆµa,bda(Z)with the marginal distribution of Yfor the design of π0
a|Z.
Structure Posterior . The parent set distribution in (3) is obtained according to a posterior distribution
of DAG structures informed by jointly observational and interventional data D[t]:
P(Pai=Z|D[t]) =/summationdisplay
G′:PaG′
i=ZP(G′|D[t]). (4)
The structure posterior is given by P(G|D [t])∝P(D[t]|G)P(G), whereP(G)is the structure prior, and the
marginal likelihood P(D[t]|G) =/integraltext
P(D[t]|G,ΘG)P(ΘG|G)dΘGis obtained by integrating the likelihood
function over the support of a conjugate prior of the parameters as follows. Let m∈I:={1,...,M}index
theM=n0+tsamples of data in D[t], and letOi⊆Irepresent the data points for which Xiis not fixed
by intervention. We make standard assumptions for Bayesian network structure learning, namely that the
priors for the parameters of the conditional probability distributions satisfy global parameter independence,
with Π0
G(ΘA) =/producttext
a∈Aπ0
a|G(θa), as well as parameter modularity, with π0
a|G(θa) =π0
a|G′(θa) =π0
a|Z(θa)for
graphsGandG′where PaG
⟨a⟩=PaG′
⟨a⟩=Z(see Heckerman et al. (1995) and Friedman & Koller (2003)
for details). These allow us to express the marginal likelihood as P(D[t]|G) =/producttextp
i=1P(xi[Oi]|paG
i[Oi]),
wherexi[·]andpaG
i[·]represent indexed samples of XiandPaG
iinD[t], respectively. Assuming a conjugate
prior, each conditional likelihood P(xi[Oi]|paG
i[Oi])can be calculated in closed form by integrating over
the parameters:
P(xi[Oi]|paG
i[Oi]) =/integraldisplay/bracketleftigg/productdisplay
m∈OiP/parenleftbig
xi[m]|paG
i[m],θG
i/parenrightbig/bracketrightigg
P(θG
i)dθG
i (5)
whereθG
i=θXi|PaG
iis the parameters specifying the conditional distribution of Xigiven its parents (Eaton
& Murphy, 2007).
Assuming the distribution Pis faithful toG(that is, all and only the conditional independence relationships
inPare entailed byG), the posterior probability P(G|D [t])will concentrate around the Markov equivalence
class with increasing samples of observational data n0. The equivalence class consists of the identification
5Published in Transactions on Machine Learning Research (01/2023)
of all direct edge connections (that is, the skeleton of G) and some edge orientations called compelled edges,
but even with infinite observational data, in general, not all edge orientations are identifiable without inter-
ventional data. The effect on P(G|D [t])of pulling arm a∈Aand observing interventional data according
to the intervention do(X⟨a⟩=xa)is primarily though not limited to that of clarifying the orientation of the
edges incident to X⟨a⟩inG.
Considering that the DAG space grows super-exponentially with the number of variables (Robinson, 1977),
computation of the parent set probabilities P(Pai|D[t])is admittedly challenging, even when the maximum
number of parents is restricted. Due to the computational complexity, it is standard to assume a structure
prior satisfying modularity, that is P(G) = Πp
i=1P(PaG
i), so that the posterior distribution is proportional
to decomposable weights consisting of the product of local scores depending only on a node and its parents
(Friedman & Koller, 2003). This property of score decomposability is crucial for the efficient implementation
of Markov Chain Monte Carlo (MCMC) methods in which the probability distribution of features in Gmay
be estimated by sampling DAGs from a Markov chain with stationary distribution P(G|D [t])(Madigan
et al., 1995; Friedman & Koller, 2003; Kuipers & Moffa, 2017; Kuipers et al., 2022). Particularly useful for
our purposes is an algorithm developed by Pensar et al. (2020) to compute the exact parent set probabilities
for a graph in time O(3pp)that also takes advantage of score decomposability. In our empirical evaluation
of BBB, we apply BBB using both exact computation of parent set posteriors as well as approximation with
MCMC sampling.
4 Bayesian Backdoor Bandit Algorithms
In this section, we apply our proposed BBB framework to several state-of-the-art MAB algorithms, namely
upper confidence bound (UCB), Thompson sampling (TS), and Bayesian UCB (Bayes-UCB). Each method
is concerned with designing and computing some criterion Ua(t)to maintain a balance between exploration
and exploitation when selecting arms according to at∈argmaxa∈AUa(t). In what follows, we briefly intro-
duce these methods and discuss their application under the BBB framework. We then provide preliminary
theoretical analysis of the cumulative regret for BBB-UCB and BBB-TS.
4.1 Description of Algorithms
The general UCB family of algorithms operates under the principle of optimism in the face of uncertainty
(Lai & Robbins, 1985). Arms that have not been investigated as many times as others have more uncertain
rewardestimatesandthusoptimisticallyhavepotentialforgreaterreward, motivatingthedesignofapadding
functionFa(t)for computing the selection criterion Ua(t) = ˆµa(t) +Fa(t). Intuitively, the combination of
the expected reward estimate ˆµa(t)and the uncertainty Fa(t)maintains a balance between high confidence
exploitation and potentially profitable exploration. Perhaps the most well-known and typically the default
instantiation of UCB algorithms is UCB1 (Agrawal, 1995; Auer et al., 2002) which computes the following
criterion:
Ua(t) = ˆµa(t−1) +c/radicalbig
log(t−1)/na(t−1), (6)
wherena(t) =/summationtextt
l=11{al=a}denotes the number of times arm ahas been pulled in ttime steps. The
confidence tuning parameter c > 0, discussed in Sutton & Barto (2018), controls the desired degree of
exploration, where c=√
2in Auer et al. (2002). Hereafter, when discussing UCB, we refer to the policy
expressed by the criterion in (6).
In what we refer to as BBB-UCB, we estimate the expected reward with the posterior mean Eπt−1
a[µa]with
respect to (3), and we replace 1/na(t−1)with the posterior variance Varπt−1
a[µa]∼1/(n0+na(t−1)). In
particular, for each arm a∈A, we compute
Ua(t) = Eπt−1
a[µa] +c/radicalig
Varπt−1
a[µa] log(t), (7)
6Published in Transactions on Machine Learning Research (01/2023)
where, for outer expectation with respect to the parent set distribution Pt−1(Z):=P(Pai=Z|D[t−1])in
(4),
Eπt−1
a[µa] = E Pt−1/bracketleftig
Eπt−1
a|Z[µa]/bracketrightig
,Varπt−1
a[µa] = E Pt−1/bracketleftig
Varπt−1
a|Z[µa]/bracketrightig
+ Var Pt−1/bracketleftig
Eπt−1
a|Z[µa]/bracketrightig
.
The Bayesian procedures of Bayes-UCB and TS are especially amenable to straightforward application under
the BBB framework. These methods follow the Bayesian MAB formulation introduced in Section 2, typically
taking as input uninformative priors in Π0that are equivalent for each arm. At each time step t, TS samples
the expectations from the posterior Ua(t)←πt−1
a(µa), effectively selecting arm a∈Awith probability
equal to the posterior probability that µais the highest expectation (Thompson, 1933). Bayes-UCB instead
computes for each arm at time tan upper quantile of µabased on its posterior distribution induced by πt−1
a:
Ua(t) =Q/parenleftbigg
1−1
t(logT)c,πt−1
a/parenrightbigg
, (8)
whereQ(r,ρ)is the quantile function defining Pρ(X≤Q(r,ρ)) =rfor probability distribution ρand random
variableX∼ρ, andcis a constant for computing the quantile used in the theoretical analysis of Bayes-UCB,
withc= 0empirically preferred (Kaufmann et al., 2012). For the BBB variants of Bayes-UCB and TS, we
need simply to supply our designed backdoor adjustment prior Π0and make appropriate Bayesian updates
to obtain the posterior Πt.
We present our proposed BBB methodology applied to Bayes-UCB, TS, and UCB in Algorithm 1.
Algorithm 1 BBB-Alg (T,A,D0,c)
Require: HorizonT, action setA, observational data D0, confidence level c
1:Compute the observational parent set posteriors (4)
2:for alla∈AandZ⊆X\X⟨a⟩do
3:Computeπ0
a|Zaccording to (2)
4:end for
5:for allt= 1,...,T do
6: for alla∈Ado
7: Compute criterion Ua(t)according to Alg:
•Bayes-UCB :Ua(t) =Q(1−1/(t(logT)c),πt−1
a)as in (8)
•TS: SampleUa(t)←πt−1
a(µa)
•UCB:Ua(t) = Eπt−1
a[µa] +c/radicalig
Varπt−1
a[µa] log(t)as in (7)
8: end for
9:Pull armat∈argmaxa∈AUa(t)and observeD(t)
10: for all Z⊆X\X⟨a⟩wherea=atdo
11: Updateπt
a|Zaccording to πt
a|Z(θa)∝pθa(yt)πt−1
a|Z(θa)
12: end for
13:Compute or update the parent set posteriors (4)
14:end for
4.2 Preliminary Regret Analysis
In this section, we provide some preliminary analysis of the Bayesian cumulative regret for BBB-UCB and
BBB-TS. We begin by defining the Bayesian cumulative regret. Given reward parameters ΘA, the (expected)
cumulative regret of a policy is defined as
RT(ΘA) = E/bracketleftiggT/summationdisplay
t=1(µ∗−µat)/vextendsingle/vextendsingle/vextendsingle/vextendsingleΘA/bracketrightigg
,
7Published in Transactions on Machine Learning Research (01/2023)
where the parameters ΘAare fixed. Under our Bayesian setting, we specify a prior P(G)over the DAGG
and conditional priors π0
a|Zfor the parameters of the reward distribution under interventions a∈A, which
defines a prior Π0over ΘA. The Bayesian regret averages the regret RT(ΘA)over the prior distribution Π0:
RB
T= E Π0[RT(ΘA)] =T/summationdisplay
t=1E [µ∗−µat],
where the second expectation is with respect to the joint distribution over the parameters and the data
[G,ΘA,D[T]]. Note that µa=µa(ΘA)is a random variable under the Bayesian setting. If RB
T=O(g(T)),
thenRT(ΘA) =Op(g(T))with respect to the prior distribution of (G,ΘA).
Without loss of generality, assume µa∈[−C,C]for alla∈A. We first analyze the BBB-UCB algorithm
(Algorithm 1). By Eq. (2) in Russo & Van Roy (2014), for the UCB sequence {at},
RB
T≤T/summationdisplay
t=1E[Uat(t)−µat] + 2CT/summationdisplay
t=1P[µ∗>Ua∗(t)]. (9)
The first term E[Uat(t)−µat] = E [E{Uat(t)−µat|D[t−1]}]. SinceUa(t)in (7) is a deterministic function
ofD[t−1], we have E{Uat(t)|D[t−1]}=Uat(t)and consequently,
E[Uat(t)−µat] = E [Uat(t)−E(µat|D[t−1])]
=c/radicalbig
logtE/bracketleftig/radicalbig
Var(µat|D[t−1])/bracketrightig
≤c/radicalbig
logt/radicalbig
E [Var(µat|D[t−1])], (10)
where the last inequality follows from Jensen’s inequality. Note that Var(µa| D[t]) = Var πt(µa)and
E(µa|D[t]) = Eπt(µa)as in (7).
We make the following assumptions on the posterior distribution p(µa|D[t]). See Appendix A for a detailed
discussion on how to verify these assumptions.
Assumption 1. Letdabe the number of candidate parent sets for X⟨a⟩. For allt≥1anda∈A,
E [Var(µa|D[t])]≤c2
1
n0+na(t)+c2
2daexp(−δana(t)),
wherec1,c2andδaare positive constants.
The Markov equivalence class of the true DAG, represented by a CPDAG, can be accurately estimated with
a large amount of observational data ( n0is large). In such cases, da≤2mais usually quite small, where ma
is the number of undirected edges connected to X⟨a⟩in the CPDAG.
The second assumption is on the concentration of µ∗≡µa∗around its posterior mean E(µa∗|D[t]):
Assumption 2. For allt≥1,
P/braceleftigg
µ∗−E(µ∗|D[t])/radicalbig
Var(µ∗|D[t])> c/radicalbig
logt/vextendsingle/vextendsingle/vextendsingleD[t]/bracerightigg
≤c3t−b,
wherec3>0andb>1are constants.
Proposition 1. Under Assumptions 1 and 2, the Bayes regret of the BBB-UCB algorithm satisfies
RB
T≤/bracketleftigg
c4/parenleftig/radicalbig
KT+K2(n0−1)−/radicalbig
K2(n0−1)/parenrightig
+c5/summationdisplay
a∈A/radicalbig
da/bracketrightigg
/radicalbig
logT+c6, (11)
whereK=|A|andc4,c5,c6are positive constants.
8Published in Transactions on Machine Learning Research (01/2023)
Remark 1. By Proposition 1 of Russo & Van Roy (2014), the same upper bound (11)also applies to the
Bayes regret of the BBB-TS algorithm.
For anyn0≥1, by concavity of√x,
/radicalbig
K2(n0−1) +KT−/radicalbig
K2(n0−1)≤√
KT.
Therefore, when Tis large such that/summationtext
a√da=O(√
T), the regret RB
T=O(√KTlogT), which is identical
to the order of the regret of a standard MAB, e.g. Proposition 2 of Russo & Van Roy (2014). The benefit
of our backdoor adjustment prior is seen when n0is large relative to T. IfT/(n0−1)<M, whereMis a
constant, then
/radicalbig
K2(n0−1) +KT−/radicalbig
K2(n0−1) =/radicalbig
K2(n0−1)/bracketleftigg/braceleftbigg
1 +T
K(n0−1)/bracerightbigg1/2
−1/bracketrightigg
≤T
2√n0−1≤√
MT
2,
where the first inequality is due to (1 +x)1/2≤1 +x/2forx≥0. In this case, we obtain a regret
boundRB
T=O(√TlogT)independent of K. This confirms the advantage of using observational data
tosimultaneously estimate all rewards µa,a∈Athrough backdoor adjustment, which largely relaxes the
dependence of the regret on the number of actions.
5 Implementation Details
5.1 Nonparametric Discrete Setting
We now detail the application of our proposed construction of π0
a|Zto the setting where the conditional
probability distributions are multinomials, with each variable Xi∈Xprobabilistically attaining its states
depending on the attained state configuration of its parents PaG
i. The reward variable Y=Xpis a bi-
nary variable with Dom(Y) ={0,1}. IfY̸∈Z,µamay be estimated with observational data through
straightforward empirical estimation of (1):
ˆµa,bda(Z) =ˆP[Y= 1|do(X⟨a⟩=xa)] =1
n0/summationdisplay
zn0[1,xa,z]n0[z]
n0[xa,z], (12)
wheren0[1,xa,z]represents the number of the n0samples ofD0in whichY= 1,X=xa, and Z=z, with
corresponding definitions for n[xa,z]andn[z].
Analysis of the sampling distribution of (12) is admittedly challenging. To design an appropriately weighted
informativepriorasproposedin(2), werequiresomecharacterizationofthesamplingvariabilityof ˆµa,bda(Z).
Hence, we derive an approximation of the variance of (12), ˆSE2[ˆµa,bda(Z)]. We accomplish this by first re-
expressing the joint counts n0[·]as sums of elements of a multinomial random vector. The term within the
sum may then be expressed as a product and ratio of intersecting random quantities, which we approximate
through a first-order Taylor series expansion. The details of the derivation are delegated to Appendix D. It
is appropriate to acknowledge that Maiti et al. (2021) proposed a provably unbiased strategy for empirical
estimation of (1) through splitting the sample into independent partitions. However, this approach suffers
from severe loss of precision through what some may consider underutilization of the observed data. In
our experiments detailed in Appendix C.1, we find the empirical performance of (12) in our applications
to be acceptable. We additionally provide extensive empirical validation of our derived approximation,
demonstratingcoverageprobabilitiescomparabletoempiricalestimatesofthesamplingvariabilityformodest
sample sizes.
Since the reward variable under arm ais a Bernoulli random variable with probability parameter µa=
P[Y= 1|do(X⟨a⟩=xa)], we assume a conjugate prior π0
a|Z= Beta(α0,β0)forθa=µadesigned according
9Published in Transactions on Machine Learning Research (01/2023)
to (2), resulting in prior hyperparameters
α0= ˆµa,bda(Z)/parenleftigg
ˆµa,bda(Z)[1−ˆµa,bda(Z)]
ˆSE2[ˆµa,bda(Z)]−1/parenrightigg
, β 0=α0/parenleftbigg1−ˆµa,bda(Z)
ˆµa,bda(Z)/parenrightbigg
.
5.2 Gaussian Unit Deviation Setting
In this section, we consider the setting where the causal model may be expressed as a set of Gaussian
structural equations:
Xj=p/summationdisplay
i=1βijXi+εj, εj∼N(0,σ2
j), j = 1,...,p. (13)
There is no intercept term, which is analogous to having prior knowledge of the observational means, and
we consider interventions xa∈{− 1,1}, which may be interpreted as investigating unit deviations from the
observational means. In this setting, the causal effect of X⟨a⟩onYis given by
ψ⟨a⟩:= EP[Y|do(X⟨a⟩=x′+ 1)]−EP[Y|do(X⟨a⟩=x′)]
for anyx′∈R, derived via a special case of (1). Note that in our problem formulation, Y|do(X⟨a⟩= 1)
and−Y|do(X⟨a⟩=−1)are identically distributed, so all data generated from interventions on X⟨a⟩
may be combined to estimate ψ⟨a⟩. Sinceµa=xaψ⟨a⟩, we focus our efforts on estimating and modeling
ψ⟨a⟩. Accordingly, in constructing our priors using intervention calculus, we design priors π0
⟨a⟩|Zforθ⟨a⟩
correspondingtoestimating ψ⟨a⟩, andallowπ0
a|Ztobetheinducedpriorsfor θacorrespondingto µa=ψ⟨a⟩xa,
detailed as follows.
IfY̸∈Z, then a consistent estimator of ψ⟨a⟩, denoted ˆψ⟨a⟩,bda, may be obtained with observational data by
the least squares regression
Y=ψ⟨a⟩X⟨a⟩+γ⊤Z+e, e∼N(0,η2), (14)
where γ∈R|Z|is the coefficients of the parents Z(Maathuis et al., 2009; Pensar et al., 2020), and some
dependence on ais omitted for simplicity. Correspondingly, we express the desired interventional distribution
asY|do(X⟨a⟩=xa)∼N(ψ⟨a⟩xa,ω2). Claiming no prior knowledge of the interventional variance, we assume
a Normal-inverse-gamma ( N-Γ−1) conjugate prior π0
⟨a⟩|Zforθ⟨a⟩= (ψ⟨a⟩,ω2):
ψ⟨a⟩|ω2∼N/parenleftbig
m0,ω2ν−1
0/parenrightbig
, ω2∼Γ−1(u0,v0). (15)
Since in general, the residual variance η2in (14) is not equivalent to ω2, we propose the following to estimate
ω2from observational data.
Proposition 2. Suppose that Xfollows the causal structural equation model (SEM) in (13)with CBNB.
LetY,X∈X, and denote by ψthe causal effect of XonY. Then for any x∈Dom(X),
VarP[Y|do(X=x)] = VarP[Y−ψX].
Note that VarP[·]is the variance in Pdefined by CBNB, and the variance on the right side is with respect
to the observational distribution of X. Intuitively, subtracting by ψXnegates the noise variances σ2in (13)
propogated through and from XtoY. We include a detailed proof for Proposition 2 in Appendix A.
Thus, to estimate ω2fromD0, we propose the estimator ˆω2=/summationtext
i(˜yi−¯˜y)2/(n0−|Z|−2)where ˜yiare
realizations of ˜Y:=Y−ˆψ⟨a⟩,bda(Z)X⟨a⟩inD0, andn0−|Z|−2is the degrees of freedom resulting from
estimating ¯˜yin addition to|Z|+ 1coefficients in (14). Accordingly, we design the prior ω2∼Γ−1(u0,v0)to
have prior mean E[ω2] =v0/(u0−1) = ˆω2, resulting in hyperparameters u0= (n0−|Z|)/2andv0=/summationtext
i(˜yi−
¯˜y)2/2. After marginalizing out ω2,ψ⟨a⟩∼t2u0(m0,v0(u0ν0)−1), so we set Eπ0
⟨a⟩|Z[ψ⟨a⟩] =m0=ˆψ⟨a⟩,bda(Z)
and solve to obtain ν0=v0/(u0ˆSE2[ˆψ⟨a⟩,bda(Z)]).
10Published in Transactions on Machine Learning Research (01/2023)
To maximally utilize the ensemble data, we further generalize the estimation of ψ⟨a⟩via regression in (14)
to include eligible samples of intervention data. This is achieved through the following proposition, which
we prove in Appendix A. This result does not rely on any parametric assumptions for the underlying causal
model, assuming simply that Xfollows a general linear SEM with DAG G(Pearl, 2000).
Proposition 3. Suppose that Xfollows a linear SEM with CBN B= (G,ΘG), andX,Y∈X. Suppose that
W∈X\{X,Y}does not block any directed path from XtoYinG. Then for any w∈R,
∂
∂xEP[Y|do(X=x)] =∂
∂xEP[Y|do(X=x),do(W=w)].
Proposition 3 asserts a simple graphical criterion which, if satisfied, defines an avenue by which information
can be shared between arms. In our work, we check the graphical criterion for estimating the causal effect
ofX⟨a⟩onYwith interventional data generated from intervening on Xjas follows. Using another algorithm
proposed by Pensar et al. (2020) for computing exact ancestor posterior probabilities, we consider the
criterion satisfied at time step tif the event that Xjblocks a directed path from X⟨a⟩toYhas low posterior
probability:
P(X⟨a⟩⇝Xj⇝Y|D[t])≤min{P(X⟨a⟩⇝Xj|D[t]),P(Xj⇝Y|D[t])}≤τ (16)
whereXj⇝Ydenotes that Xjis an ancestor of Yand the threshold is set to τ= 0.1in our application. If
(16) holds at time step t, we combine the observational data and the data from interventions on Xjwhen
conducting the regression (14). While independent samples of observational and interventional data are
not guaranteed to have identically distributed errors in the regression (14), we provide extensive empirical
validation of our proposed regression with ensemble data in Appendix C.2, confirming indistinguishable
performance for the purposes of estimating ψ⟨a⟩and its sampling variability compared to that of purely
observational data.
6 Numerical Experiments
We conducted extensive numerical experiments to empirically validate our proposed methodology. For our
main experiments, we generated random CBN models in an effort to empirically demonstrate the merits
of our proposed methodology by evaluating BBB across a broad range of scenarios. To address the com-
putational challenges of BBB, we then evaluated an implementation using MCMC to estimate parent set
probabilities and applied it to a larger realistic reference network. Comprehensive experimental details suf-
ficient for reproducing our experiments, such as CBN preparation and algorithm parameters, are provided
in Appendix B. Additional supplemental experiments independently evaluating (12) and Proposition 3 may
be found in Appendix C. The complete code and instructions for reproducing our results have been made
available at the following link:
https://github.com/jirehhuang/bcb
6.1 Random Networks
For our main experiments, we generated CBN models with p= 10variables in the interest of representing a
diverse range of scenarios in our empirical comparisons. The structures were randomly generated, with the
reward variable designated to have |PaG
p|= 3parents, according to a process adapted from de Kroon et al.
(2022). The conditional probability distributions of each CBN were likewise generated randomly. Atomic
interventions as described in Section 5 were allowed on all variables excluding the reward variable, with the
discrete variables assumed to be binary, resulting in |A|= 2(p−1) = 18actions.
We evaluated our BBB methodology against algorithms designed to optimize cumulative regret, including
popular standard MAB algorithms Bayes-UCB, TS, and UCB that do not utilize causal assumptions (see
Section 4). Additionally, we compared against what can be interpreted as a highly optimistic version of
the central node approach by Lu et al. (2021), introduced in Section 1, by presupposing knowledge of the
direct causes of the reward variable. In particular, for Bayes-UCB∗, TS∗, and UCB∗, we executed the
11Published in Transactions on Machine Learning Research (01/2023)
respective algorithms over the reduced action set A′={a∈A :⟨a⟩∈PaG
p}. Accordingly, for the cases
where⟨a∗⟩̸∈PaG
p, we redefined the optimal intervention to a∗= argmaxa∈A′µawhen evaluating the regret
of TS∗and (Bayes-)UCB∗. To circumvent confounding arising from the differences in algorithm designs and
any relevant parameter tuning, we focus our comparisons within algorithm types, e.g. amongst TS, TS∗,
and BBB-TS.
Using the process described above, we generated 100CBN models for each distributional setting. For each
CBN, we executed the competing methods 10 times but our BBB methods only 5 times due to their greater
computational expense, with T= 5000time steps. The results presented are averaged across all simulations
for each time step, with the cumulative regret normalized by the optimal reward µ∗to ensure that each CBN
model contributes comparably. In preference to the competing methods, we tuned for their best-performing
parameters where relevant and applied them to our BBB implementations. We computed exact parent set
probabilities in BBB using an algorithm proposed by Pensar et al. (2020) in an effort to assess our BBB
methodology in the most precise implementation of its formulation. Additional experimental details may be
found in Appendix B, and additional supporting results are provided in Appendix C.
Note that while the simulations were executed across a cluster of heterogeneous nodes, it was not difficult
to observe that the computational requirements of BBB vastly exceeded that of the considered competing
methods. The average execution time of each iteration of our discrete and Gaussian implementations of BBB
on the random networks was respectively around 4.3 and 8.2 seconds, compared to less than 0.15 seconds for
thecompetingmethods. Thisincludestheexactcomputationofparentsetposteriors, computingorupdating
corresponding backdoor adjustment estimates, and in the Gaussian case, ancestor posterior probabilities.
While BBB requires significantly greater computational expense, the cost must be weighed against other
considerations such as the time and resources required for additional exploration of interventions.
6.1.1 Cumulative Regret Comparisons
The empirical cumulative regret results in Figure 1 demonstrate that in both the discrete and Gaussian
settings and for all algorithms, our BBB methodology is able to reliably outperform the non-causal variants
with finite samples of observational data. The improvement increases monotonically with increasing sample
sizes of observational data ( n0). While corresponding variants of Bayes-UCB and TS perform comparably,
UCB achieves substantially lower regret because the parameter cin (6) was tuned to maintain a balance
between exploration and exploitation that is most empirically preferred. In particular, UCB is able to
avoid excessive exploration by scaling its padding term with a relatively small constant, whereas Bayes-UCB
maintains a relatively high minimum exploration rate according to its formulation in (8), as does TS.
In comparison to the optimistic central node versions of the algorithms, BBB generally achieves lower
cumulative regret with n0≥800in the discrete setting and n0≥40in the Gaussian setting. Recall that,
in practical applications, the central node approach relies on the availability of large-sample observational
data as well as a sequence of interventions to recover the reward generating variables PaG
p. Based on our
simulation settings, this reduces the action set from |A|= 18arms to only|A′|= 2|PaG
p|= 6arms, and we
additionally artificially restrict a∗∈A′to evaluate the regret. Furthermore, the regret results reported for
these methods do not include the interventions required to identify PaG
p, thus representing a kind of best
case scenario for the central node approach. In contrast, our methodology derives substantial benefit from
modest amounts of observational data samples n0.
Indeed, wefindthatourBBBmethodsareabletoperformcompetitivelyagainstthecompetingmethodseven
when the latter are given n0time steps to explore arms before incurring regret. To compensate for the fact
that BBB utilizes n0samples of observational data prior to investigating arms, we present the results where
the competing algorithms TS(∗) and (Bayes-)UCB(∗) are given a head start of n0∈{100·2k:k= 0,1,..., 5}
time steps to explore arms before incurring regret. The results for the discrete setting are shown in Figure 2.
The Gaussian results are omitted because n0≤320is relatively small, so the head start does not offer
substantial benefit to the competing methods. In all cases, BBB still significantly outperforms the standard
algorithms TS and (Bayes-)UCB given the head start. Given sufficient samples of observational data, BBB
still performs comparably to if not better than the optimistic central node variants in terms of cumulative
12Published in Transactions on Machine Learning Research (01/2023)
Alg: Bayes−UCB Alg: TS Alg: UCB
0100200300Discrete Cumulative
Alg: Bayes−UCB Alg: TS Alg: UCB
0 1000 2000 3000 4000 5000 0 1000 2000 3000 4000 5000 0 1000 2000 3000 4000 50000100200300
Time StepGaussian Cumulative
Alg
Alg*BBB−Alg(k=0)
BBB−Alg(k=1)BBB−Alg(k=2)
BBB−Alg(k=3)BBB−Alg(k=4)
BBB−Alg(k=5)
Figure 1: Average cumulative regret against T= 5000time steps comparing Alg,Alg∗, and BBB-Algfor
Alg∈{Bayes-UCB,TS,UCB}. BBB methods were executed with n0= 100·2kin the discrete setting and
n0= 10·2kin the Gaussian setting.
regret, for which the head start is only an additional unwarranted advantage given that they already require
significantly more observational data as well as additional interventions.
6.1.2 Structure Identification
In addition to the cumulative regret performance, it is of interest to consider the structure identification
behavior of the BBB approach in our experiments. We measure the concentration of the posterior probability
across DAGsGwith respect to the underlying causal graph G∗using the edge support sum of absolute errors
(ESSAE), which is given at time tby
p/summationdisplay
i=1/summationdisplay
j̸=i/vextendsingle/vextendsingle/vextendsingleP(j∈PaG
i|D[t])−1/braceleftig
j∈PaG∗
i/bracerightig/vextendsingle/vextendsingle/vextendsingle.
This quantity may be understood as a probabilistic version of the structural hamming distance, a common
metric in Bayesian network structure learning literature. Lower ESSAE corresponds to greater concentration
of the posterior probability around the causal graph G∗. The results are provided in Figure 3.
In the discrete results for BBB-Bayes-UCB and BBB-TS, the initial ESSAE is unsurprisingly lower for the
larger sample sizes, but the trend quickly reverses as the time steps progress. This effect is also observed
occurring in the Gaussian results, but at an accelerated pace. This behavior is perhaps best understood in
complement to the cumulative regret results in Figure 1. If Pis faithful toG, then ifn0is large, the structure
priorP(G|D 0)is expected to concentrate around the Markov equivalence class, which entails identification
of the skeleton and in general, partial identification of the orientations. Additionally, the conditional priors
π0
a|Zwill be precise models, allowing BBB to quickly identify and select arm(s) a∈Awith small regret
13Published in Transactions on Machine Learning Research (01/2023)
100 200 400 800 1600 3200Bayes−UCB
100 200 400 800 1600 3200TS
100 200 400 800 1600 3200
Time StepUCB
Alg Alg* BBB−Alg
Figure 2: Discrete average cumulative regret for Alg∈{Bayes-UCB,TS,UCB}with a head start of n0∈
{100·2k:k= 0,1,..., 5}time steps for competing methods.
Alg: Bayes−UCB Alg: TS Alg: UCB
03691215Discrete ESSAE
Alg: Bayes−UCB Alg: TS Alg: UCB
0 1000 2000 3000 4000 5000 0 1000 2000 3000 4000 5000 0 1000 2000 3000 4000 50000246810
Time StepGaussian ESSAE
BBB−Alg(k=0)
BBB−Alg(k=1)BBB−Alg(k=2)
BBB−Alg(k=3)BBB−Alg(k=4)
BBB−Alg(k=5)
Figure 3: Discrete ( n0= 100·2k) and Gaussian ( n0= 10·2k) results of the average ESSAE of the full graph
structure for BBB-Alg,Alg∈{Bayes-UCB,TS,UCB}.
14Published in Transactions on Machine Learning Research (01/2023)
µ∗−µa, which has the effect of clarifying the orientation of edges incident to such X⟨a⟩. The policies take no
interest in determining the orientation of the remaining edges if the uncertainty does not indicate potential to
identify more profitable actions. In contrast, when n0is small, the greater uncertainty in both the structure
prior and the conditional priors encourage the exploration of many different arms, thus incurring greater
cumulative regret. In addition to clarifying the orientation of the incident edges, selecting arm(s) a∈A
contributes to identifying the direct edge connections excluding those from PaG
⟨a⟩toX⟨a⟩, as can be seen in
(5). Thus, the skeleton is recovered and more edge orientations are identified than in the case where n0is
large, achieving lower ESSAE at the cost of greater cumulative regret. Notably, while this reversal appears
to be absent in the discrete results for BBB-UCB, in actuality it has simply not yet been realized even after
T= 5000time steps due to the small exploration constant cin (7).
6.2 Scaling BBB With MCMC
Despite the efficiency of the algorithm proposed by Pensar et al. (2020), its scaling in pofO(3pp)means that
exact computation of the parent set posteriors is not always feasible. Pensar et al. (2020) noted that their
algorithm executed on 20 variables in about 25 minutes, which would translate to over 86 days for 5000 time
steps. While potentially justifiable depending on the context of the application such as when interventions
are particularly expensive or time-consuming, such intensive computational requirement for each time step
is likely to be limiting of the practical application of BBB on larger systems. As discussed in Section 3, the
parent set probabilities are derived from a posterior distribution of graph structures which may be estimated
by MCMC. In this section, we discuss the implementation of BBB using MCMC to estimate the structure
posterior. We provide preliminary results in the discrete setting assessing this approximation against the
exact computation of parent sets and evaluating its performance on a realistic reference network with p= 20
variables.
The posterior distribution of graph structures P(G|D [t])may be approximated by sampling DAGs Gtfrom
P(G|D [t])using MCMC and empirically estimating (4) from the sampled graphs:
P(Pai=Z|D[t])≈1
|Gt|/summationdisplay
G′∈Gt1/braceleftig
PaG′
i=Z/bracerightig
.
VariousMCMCsamplingschemesforDAGshavebeendeveloped, mostbasicofwhichisthestructureMCMC
sampler which accepts proposed single edge addition, deletion, or reversal steps according to a Metropolis-
Hastings probability (Madigan et al., 1995; Giudici & Castelo, 2003). With order MCMC, Friedman &
Koller (2003) reduced the search space to the topological orderings of the pnodes, significantly accelerating
convergence but retaining a degree of bias since a given DAG may belong to multiple orders. Kuipers &
Moffa (2017) addressed this issue of bias by considering the space of ordered partitions in partition MCMC,
though at the price of greater computational requirement (Suter et al., 2021).
Inoursimulations,wechosetouseorderMCMCdespitethepotentialbiasduetoitscomputationaladvantage
over partition MCMC while achieving satisfactory performance. For further improvements in efficiency, we
applied the hybrid approach proposed by Kuipers et al. (2022) in which DAGs are sampled from a loosely
restricted initial search space, with built-in provisions to expand the search space. In particular, before
performing any interventions, we obtained an initial search space with the PC algorithm (Spirtes & Glymour,
1991) and sampled DAGs from P(G|D 0). For subsequent iterations, we restricted the search space using the
structure posterior estimated from the immediately preceding iteration. Whenever conducting the restricted
sampling scheme, the search space was allowed to be extended as designed by Kuipers et al. (2022) to account
for false negatives in the restriction.
Wefirstcomparedthecumulativeregretamongstexactcomputationofparentsetposteriors(Exact), approx-
imation by MCMC without restricting the search space at any step (MCMC), and MCMC with restricting
the search space as described (Hybrid MCMC). Each of these were executed twice on each of the 100 ran-
domly generated discrete CBNs, and the results at various time steps are shown in Figure 4. Hybrid MCMC
performed equivalently with unrestricted MCMC, enjoying significant computational reductions without ex-
hibiting any inferiority in performance. However, while (hybrid) MCMC performed comparably with exact
computation in most cases, there were significantly more extreme values in the MCMC methods. These
15Published in Transactions on Machine Learning Research (01/2023)
0 2 8 12 1 9 26 35 1 9 26 34 0 0 3 7 1 7 20 31 1 6 20 31 0 2 4 4 6 17 20 22 6 17 20 23BBB−Bayes−UCB BBB−TS BBB−UCB
500 2000 3500 5000 500 2000 3500 5000 500 2000 3500 50000204060>60
Time StepCumulative Regret
Exact MCMC Hybrid MCMC
Figure 4: Cumulative regret of BBB-Alg for Alg∈{Bayes-UCB,TS,UCB}withn0= 3200on the discrete
random networks from Section 6.1 for time steps t∈{500,2000,3500,5000}comparing between computing
exact parent set posteriors and approximating with MCMC. Each boxplot represents 200 data points, with
the number of points above 60 labeled above.
may have resulted from the bias inherent to order MCMC, but given that general settings were applied and
convergence was not assessed for each iteration, another potential explanation would be insufficient itera-
tions for certain CBNs. In general, approximation of the structure posterior with MCMC appears to be an
acceptable scalable alternative to exact computation.
We then applied BBB with hybrid MCMC to CHILD, a moderately sized discrete reference network with
p= 20nodes for diagnosing congenital heart disease (Spiegelhalter, 1992). CHILD was constructed with
domain experts and made available in a Bayesian network repository (Spiegelhalter, 1992; Scutari, 2010).
The network was preprocessed to have binary variables, and the target variable was set to LowerBodyO2, a
leaf node with two parents. All other variables were intervenable, with |A|= 38arms, though only |A′|= 4
for the optimistic central node approach.
The cumulative regret results for 20 executions of various algorithms are presented in Figure 5. Even after
T= 5000time steps, most methods fail to exhibit meaningful convergence in the form of the flattening
of the cumulative regret curve. This is because all but two arms have expected reward µa>0.5, making
the optimal reward of approximately µ∗= 0.622difficult to distinguish without a great deal of exploration.
Unsurprisingly, the central node approach performs exceptionally well, having only four arms to investigate.
Withn0= 3200, BBB was able to adequately distinguish the arms with higher expected reward, achieving
cumulative regret competitive with the central node approach. The average execution time of each iteration
ofBBBwithMCMConCHILDwasaround9.4seconds, comparedtolessthan0.23secondsforthecompeting
methods.
7 Discussion
In this paper, we proposed the BBB framework for enhancing experimental investigations with observational
data. BBB consists of an aggregation of various strategies for estimating and modeling the parameters of
interest with jointly interventional and observational data in order to efficiently utilize all available data
to inform exploitation and exploration. Applied in our methodology but also of independent interest, we
derived a well-performing approximation for the variance of the discrete backdoor adjustment estimator, and
in the Gaussian setting, we characterized the interventional variance using the observational distribution and
proposed a simple graphical criterion for sharing information between arms. We supplied preliminary regret
analysis justifying our methodology, and empirically validated our proposed algorithms through extensive
16Published in Transactions on Machine Learning Research (01/2023)
Alg: Bayes−UCB Alg: TS Alg: UCB
0 1000 2000 3000 4000 5000 0 1000 2000 3000 4000 5000 0 1000 2000 3000 4000 50000200400600Average Cumulative
Alg Alg* BBB−Alg(k=5)
Alg: Bayes−UCB Alg: TS Alg: UCB
500 2000 3500 5000 500 2000 3500 5000 500 2000 3500 50000250500750
Time StepMedian Cumulative (95%)
Alg Alg* BBB−Alg(k=5)
Figure 5: Average cumulative regret and median cumulative regret with 95% percentile intervals against
T= 5000time steps comparing Alg,Alg∗, and BBB-AlgforAlg∈{Bayes-UCB,TS,UCB}, executed on
CHILD(p= 20). BBBmethodswereexecutedwith n0= 3200usinghybridMCMCtoestimatethestructure
posterior.
numerical experiments against standard MAB algorithms as well as a generously optimistic version of a
recently proposed CB approach.
Although our work notably does not depend on certain restrictive assumptions made by previous work,
namely knowledge of the causal graph or large-sample observational information, our proposed methodology
nonetheless requires a causally sufficient system which may not be available in practice. In the presence
of unobserved confounders, an obvious challenge is that the variables in the parent sets that BBB uses for
backdoor adjustment may not all be observed. Since sets that satisfy the backdoor criterion are not limited
to the parent set, one approach to this setting would be to otherwise identify valid adjustment sets. Perhaps
the most obvious extension of our methodology would be to model the underlying ancestral graph instead of
the DAG. From the ancestral graph, causal effects may be estimated from observational data by identifying
valid backdoor adjustment sets based on its structure.
The challenge of scaling BBB was addressed briefly by hybrid MCMC in Section 6, but the limits of the
computational feasibility of BBB have yet to be carefully investigated or precisely articulated. Preliminary
investigations suggest that BBB can scale to well over 100 variables, but extended empirical evaluation is
necessary. A careful technical study of the posterior distributions is left as future work to complete the
theoretical analysis of the cumulative regret. Finally, it would be interesting to consider how to share
information between arms in the discrete setting as in Proposition 3 with a similarly simple graphical
criterion.
17Published in Transactions on Machine Learning Research (01/2023)
Acknowledgements
This work was supported by US NSF grant DMS-1952929. This work used computational and storage
services associated with the Hoffman2 Shared Cluster provided by UCLA Institute for Digital Research and
Education’s Research Technology Group.
References
Rajeev Agrawal. Sample mean based index policies by O(logn) regret for the multi-armed bandit problem.
Advances in Applied Probability , 27(4):1054–1078, 1995. https://doi.org/10.2307/1427934 .
Peter Auer, Nicolo Cesa-Bianchi, and Paul Fischer. Finite-time Analysis of the Multiarmed Bandit Problem.
Machine learning , 47(2):235–256, 2002. https://doi.org/10.1023/A:1013689704352 .
Hamsa Bastani, David Simchi-Levi, and Ruihao Zhu. Meta Dynamic Pricing: Transfer Learning Across
Experiments. Management Science ,68(3):1865–1881,2022. https://doi.org/10.1287/mnsc.2021.4071 .
Soumya Basu, Branislav Kveton, Manzil Zaheer, and Csaba Szepesvári. No Regrets for Learning the
Prior in Bandits. Advances in Neural Information Processing Systems , 34:28029–28041, 2021. https:
//proceedings.neurips.cc/paper/2021/file/ec1f764517b7ffb52057af6df18142b7-Paper.pdf .
Donald A Berry and Bert Fristedt. Bandit problems: sequential allocation of experiments (Monographs
on statistics and applied probability). London: Chapman and Hall , 5(71-87):7–7, 1985. https://link.
springer.com/book/10.1007/978-94-015-3711-7 .
Arnoud de Kroon, Joris Mooij, and Danielle Belgrave. Causal bandits without prior knowledge using sepa-
rating sets. In First Conference on Causal Learning and Reasoning , 2022. https://proceedings.mlr.
press/v177/kroon22a.html .
Daniel Eaton and Kevin Murphy. Exact Bayesian structure learning from uncertain interventions. In
Marina Meila and Xiaotong Shen (eds.), Proceedings of the Eleventh International Conference on Artificial
Intelligence and Statistics , volume 2 of Proceedings of Machine Learning Research , pp. 107–114, San Juan,
Puerto Rico, 21–24 Mar 2007. PMLR. https://proceedings.mlr.press/v2/eaton07a.html .
Nir Friedman and Daphne Koller. Being Bayesian About Network Structure. A Bayesian Approach to
Structure Discovery in Bayesian Networks. Machine learning , 50(1):95–125, 2003. https://doi.org/10.
1023/A:1020249912095 .
Paolo Giudici and Robert Castelo. Improving Markov Chain Monte Carlo Model Search for Data Mining.
Machine learning , 50(1):127–158, 2003. https://doi.org/10.1023/A:1020202028934 .
Kristjan Greenewald, Dmitriy Katz, Karthikeyan Shanmugam, Sara Magliacane, Murat Kocaoglu, En-
ric Boix Adsera, and Guy Bresler. Sample Efficient Active Learning of Causal Trees. In H. Wallach,
H. Larochelle, A. Beygelzimer, F. d 'Alché-Buc, E. Fox, and R. Garnett (eds.), Advances in Neural Infor-
mation Processing Systems , volume 32. Curran Associates, Inc., 2019. https://proceedings.neurips.
cc/paper/2019/file/5ee5605917626676f6a285fa4c10f7b0-Paper.pdf .
Dominique MA Haughton. On the Choice of a Model to Fit Data from an Exponential Family. The Annals
of Statistics , pp. 342–355, 1988. https://www.jstor.org/stable/2241441 .
David Heckerman, Dan Geiger, and David M Chickering. Learning Bayesian networks: The combination
of knowledge and statistical data. Machine Learning , 20(3):197–243, 1995. https://doi.org/10.1007/
BF00994016 .
JoeyHong, BranislavKveton, ManzilZaheer, andMohammadGhavamzadeh. HierarchicalBayesianBandits.
InInternational Conference on Artificial Intelligence and Statistics , pp. 7724–7741. PMLR, 2022. https:
//proceedings.mlr.press/v151/hong22c.html .
Markus Kalisch and Peter Bühlmann. Estimating High-Dimensional Directed Acyclic Graphs with the PC-
Algorithm. Journal of Machine Learning Research , 8(22):613–636, 2007. http://jmlr.org/papers/v8/
kalisch07a.html .
18Published in Transactions on Machine Learning Research (01/2023)
Emilie Kaufmann, Olivier Cappe, and Aurelien Garivier. On Bayesian Upper Confidence Bounds for Bandit
Problems. In Neil D. Lawrence and Mark Girolami (eds.), Proceedings of the Fifteenth International Con-
ference on Artificial Intelligence and Statistics , volume 22 of Proceedings of Machine Learning Research ,
pp. 592–600, La Palma, Canary Islands, 21–23 Apr 2012. PMLR. https://proceedings.mlr.press/
v22/kaufmann12.html .
Jack Kuipers and Giusi Moffa. Partition MCMC for Inference on Acyclic Digraphs. Journal of the American
Statistical Association , 112(517):282–299, 2017. https://doi.org/10.1080/01621459.2015.1133426 .
Jack Kuipers, Polina Suter, and Giusi Moffa. Efficient Sampling and Structure Learning of Bayesian Net-
works.Journal of Computational and Graphical Statistics , 0(0):1–12, 2022. https://doi.org/10.1080/
10618600.2021.2020127 .
Branislav Kveton, Mikhail Konobeev, Manzil Zaheer, Chih-wei Hsu, Martin Mladenov, Craig Boutilier, and
Csaba Szepesvari. Meta-Thompson Sampling. In International Conference on Machine Learning , pp.
5884–5893. PMLR, 2021. https://proceedings.mlr.press/v139/kveton21a.html .
Tze Leung Lai and Herbert Robbins. Asymptotically efficient adaptive allocation rules. Advances in Applied
Mathematics , 6(1):4–22, 1985. ISSN 0196-8858. https://doi.org/10.1016/0196-8858(85)90002-8 .
Finnian Lattimore, Tor Lattimore, and Mark D Reid. Causal Bandits: Learning Good Interventions via
Causal Inference. In D. Lee, M. Sugiyama, U. Luxburg, I. Guyon, and R. Garnett (eds.), Advances in
Neural Information Processing Systems , volume29.CurranAssociates, Inc., 2016. https://proceedings.
neurips.cc/paper/2016/file/b4288d9c0ec0a1841b3b3728321e7088-Paper.pdf .
Sanghack Lee and Elias Bareinboim. Structural causal bandits: Where to intervene? In S. Bengio, H. Wal-
lach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett (eds.), Advances in Neural Informa-
tion Processing Systems , volume 31. Curran Associates, Inc., 2018. https://proceedings.neurips.cc/
paper/2018/file/c0a271bc0ecb776a094786474322cb82-Paper.pdf .
Xiuyuan Lu and Benjamin Van Roy. Information-Theoretic Confidence Bounds for Reinforcement Learn-
ing.Advances in Neural Information Processing Systems , 32, 2019. https://proceedings.neurips.cc/
paper/2019/file/411ae1bf081d1674ca6091f8c59a266f-Paper.pdf .
Yangyi Lu, Amirhossein Meisami, Ambuj Tewari, and William Yan. Regret Analysis of Bandit Problems
with Causal Background Knowledge. In Jonas Peters and David Sontag (eds.), Proceedings of the 36th
Conference on Uncertainty in Artificial Intelligence (UAI) , volume 124 of Proceedings of Machine Learning
Research , pp. 141–150. PMLR, 03–06 Aug 2020. https://proceedings.mlr.press/v124/lu20a.html .
Yangyi Lu, Amirhossein Meisami, and Ambuj Tewari. Causal Bandits with Unknown Graph Structure. In
M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan (eds.), Advances in Neural
Information Processing Systems , volume 34, pp. 24817–24828. Curran Associates, Inc., 2021. https:
//proceedings.neurips.cc/paper/2021/file/d010396ca8abf6ead8cacc2c2f2f26c7-Paper.pdf .
Marloes H. Maathuis, Markus Kalisch, and Peter Bühlmann. Estimating high-dimensional intervention
effects from observational data. The Annals of Statistics , 37(6A):3133 – 3164, 2009. https://doi.org/
10.1214/09-AOS685 .
David Madigan, Jeremy York, and Denis Allard. Bayesian Graphical Models for Discrete Data. International
Statistical Review / Revue Internationale de Statistique , 63(2):215–232, 1995. ISSN 03067734, 17515823.
http://www.jstor.org/stable/1403615 .
Aurghya Maiti, Vineet Nair, and Gaurav Sinha. Causal Bandits on General Graphs. arXiv preprint
arXiv:2107.02772 , 2021. https://arxiv.org/abs/2107.02772 .
Vineet Nair, Vishakha Patil, and Gaurav Sinha. Budgeted and Non-Budgeted Causal Bandits. In Arindam
Banerjee and Kenji Fukumizu (eds.), Proceedings of The 24th International Conference on Artificial Intel-
ligence and Statistics , volume 130 of Proceedings of Machine Learning Research , pp. 2017–2025. PMLR,
13–15 Apr 2021. https://proceedings.mlr.press/v130/nair21a.html .
19Published in Transactions on Machine Learning Research (01/2023)
Judea Pearl. Causal diagrams for empirical research. Biometrika , 82(4):669–688, 12 1995. ISSN 0006-3444.
https://doi.org/10.1093/biomet/82.4.669 .
Judea Pearl. Causality . Cambridge University Press, 2000.
Amit Peleg, Naama Pearl, and Ron Meir. Metalearning Linear Bandits by Prior Update. In International
Conference on Artificial Intelligence and Statistics , pp. 2885–2926. PMLR, 2022. https://proceedings.
mlr.press/v151/peleg22a.html .
Johan Pensar, Topi Talvitie, Antti Hyttinen, and Mikko Koivisto. A Bayesian Approach for Estimating
Causal Effects from Observational Data. Proceedings of the AAAI Conference on Artificial Intelligence ,
34(04):5395–5402, Apr. 2020. https://ojs.aaai.org/index.php/AAAI/article/view/5988 .
Robert W Robinson. Counting unlabeled acyclic digraphs. In Charles H. C. Little (ed.), Combinatorial
Mathematics V , pp. 28–43. Springer, Berlin, Heidelberg, 1977. ISBN 978-3-540-37020-8. https://doi.
org/10.1007/BFb0069178 .
Daniel Russo and Benjamin Van Roy. Learning to Optimize via Posterior Sampling. Mathematics of Oper-
ations Research , 39(4):1221–1243, 2014. https://doi.org/10.1287/moor.2014.0650 .
Gideon Schwarz. Estimating the Dimension of a Model. The Annals of Statistics , 6(2):461–464, 1978. ISSN
00905364. https://doi.org/10.1214/aos/1176344136 .
Marco Scutari. Learning Bayesian Networks with the bnlearn R Package. Journal of Statistical Software , 35
(3):1––22, 2010. https://doi.org/10.18637/jss.v035.i03 .
Max Simchowitz, Christopher Tosh, Akshay Krishnamurthy, Daniel J Hsu, Thodoris Lykouris, Miro
Dudik, and Robert E Schapire. Bayesian decision-making under misspecified priors with applications
to meta-learning. Advances in Neural Information Processing Systems , 34:26382–26394, 2021. https:
//proceedings.neurips.cc/paper/2021/file/ddcbe25988981920c872c1787382f04d-Paper.pdf .
David J Spiegelhalter. Learning in probabilistic expert systems. Bayesian statistics , 4:447–465, 1992.
Peter Spirtes and Clark Glymour. An Algorithm for Fast Recovery of Sparse Causal Graphs. Social Science
Computer Review , 9(1):62–72, 1991. https://doi.org/10.1177/089443939100900106 .
Polina Suter, Jack Kuipers, Giusi Moffa, and Niko Beerenwinkel. Bayesian structure learning and sampling
of Bayesian networks with the R package BiDAG. arXiv preprint arXiv:2105.00488 , 2021. https://
arxiv.org/abs/2105.00488 .
Richard S Sutton and Andrew G Barto. Reinforcement Learning: An Introduction . MIT Press, second
edition, 2018.
William R Thompson. On the likelihood that one unknown probability exceeds another in view of the
evidence of two samples. Biometrika , 25(3-4):285–294, 12 1933. ISSN 0006-3444. https://doi.org/10.
1093/biomet/25.3-4.285 .
Runzhe Wan, Lin Ge, and Rui Song. Metadata-based Multi-Task Bandits with Bayesian Hierarchical Mod-
els.Advances in Neural Information Processing Systems , 34:29655–29668, 2021. https://proceedings.
neurips.cc/paper/2021/file/f7cfdde9db36af8e0d9a6d123d5c385e-Paper.pdf .
Hao Wang. Constraint-Based Learning of Interventional Markov Equivalence Classes on High-Dimensional
Data. PhD thesis, UCLA, 2022. https://escholarship.org/uc/item/9j15j0fq .
Akihiro Yabe, Daisuke Hatano, Hanna Sumita, Shinji Ito, Naonori Kakimura, Takuro Fukunaga, and Ken-
ichi Kawarabayashi. Causal Bandits with Propagating Inference. In Jennifer Dy and Andreas Krause
(eds.),Proceedings of the 35th International Conference on Machine Learning , volume 80 of Proceedings
of Machine Learning Research , pp. 5512–5520. PMLR, 10–15 Jul 2018. https://proceedings.mlr.
press/v80/yabe18a.html .
20Published in Transactions on Machine Learning Research (01/2023)
A Proofs and Discussions
Proof of Proposition 1. LetTa={t≤T:at=a}be the time steps in which ais selected. Then by
inequality (10), Assumption 1 and the concavity of√x, we have
/summationdisplay
t∈TaE[Ua(t)−µa]≤c/radicalbig
logT/braceleftigg
c1/summationdisplay
t∈Ta[n0+na(t−1)]−1
2+c2/radicalbig
da/summationdisplay
t∈Taexp/bracketleftbigg
−δana(t−1)
2/bracketrightbigg/bracerightigg
.
Now the first summation on the right side
/summationdisplay
t∈Ta[n0+na(t−1)]−1
2=na(T)−1/summationdisplay
j=0(n0+j)−1
2
≤/integraldisplayn0+na(T)−1
n0−1x−1
2dx= 2(/radicalbig
(n0−1) +na(T)−√n0−1).
Similar derivation shows that the second summation
/summationdisplay
t∈Taexp/bracketleftbigg
−δana(t−1)
2/bracketrightbigg
≤ca,
wherecais a constant. Therefore,
/summationdisplay
t∈TaE[Ua(t)−µa]≤c/bracketleftig
2c1(/radicalbig
(n0−1) +na(T)−√n0−1) +c2ca/radicalbig
da/bracketrightig/radicalbig
logT.
By Cauchy-Schwartz inequality,
/summationdisplay
a∈A/radicalbig
(n0−1) +na(T)≤√
K/braceleftig
K(n0−1) +/summationdisplay
ana(T)/bracerightig1/2
=/radicalbig
KT+K2(n0−1).
Summing over actions, we arrive at
T/summationdisplay
t=1E[Ua(t)−µa] =/summationdisplay
a∈A/summationdisplay
t∈TaE[Ua(t)−µa]
≤/bracketleftigg
c4/parenleftig/radicalbig
KT+K2(n0−1)−/radicalbig
K2(n0−1)/parenrightig
+c5/summationdisplay
a∈A/radicalbig
da/bracketrightigg
/radicalbig
logT, (17)
wherec4= 2cc1andc5=cc2maxaca.
Next, we show that the second term in (9) is bounded. By the definition of Ua(t)in (7), Assumption 2
implies
P(µ∗>Ua∗(t)) = E [P(µa∗>Ua∗(t)|D[t−1])]≤c3(t−1)−b
fort≥2, and thus,
T/summationdisplay
t=1P(µa∗>Ua∗(t))≤1 +c3∞/summationdisplay
t=1t−b.
Therefore, the second term is bounded by a constant c6. Accordingly, (11) follows from (17).
Discussion of Assumption 1 and Assumption 2. We now demonstrate how to verify Assumption 1 and
Assumption 2 for Proposition 1. Recall that the posterior distribution p(µa|D[t])is defined via averaging
over possible parent set Za:=PaG
⟨a⟩⊆X\X⟨a⟩.
21Published in Transactions on Machine Learning Research (01/2023)
We start with decomposing Var(µa|D[t])by conditioning on Za,
Var(µa|D[t]) = E [Var( µa|Za,D[t])|D[t]] + Var [E(µa|Za,D[t])|D[t]]. (18)
Using the Gaussian setting as an example, under the conjugate prior (15), the conditional posterior p[µa|
Za,D[t]]is at-distribution with (n0+na(t)−|Za|)degrees of freedom and variance
Var(µa|Za,D[t]) =Vt
a(Za)
n0+na(t),
whereVt
a(Za) =Op(1)depends onD[t]. Now the first term on the right side of (18) is expressed as
E [Var(µa|Za,D[t])|D[t]] =/summationdisplay
ZaVt
a(Za)
n0+na(t)P(Za|D[t]). (19)
Taking further expectation to average over D[t], we get
E/braceleftbig
E [Var(µa|Za,D[t])|D[t]]/bracerightbig
=1
n0+na(t)E/bracketleftigg/summationdisplay
ZaVt
a(Za)P(Za|D[t])/bracketrightigg
≤c2
1
n0+na(t),(20)
wherec2
1is an upper bound for the expectation in the second step for all a∈A.
Letµt
a(Za):= E (µa|Za,D[t])andG∗be the true causal DAG. Then,
Var/bracketleftbig
µt
a(Za)|D[t]/bracketrightbig
≤/summationdisplay
Za̸=PaG∗
⟨a⟩/bracketleftig
µt
a(Za)−µt
a(PaG∗
⟨a⟩)/bracketrightig2
P(Za|D[t])
≤max
Za/bracketleftig
µt
a(Za)−µt
a(PaG∗
⟨a⟩)/bracketrightig2
P(Za̸=PaG∗
⟨a⟩|D[t])
≤4C2P(Za̸=PaG∗
⟨a⟩|D[t]),
where the last inequality is due to the assumption that µa∈[−C,C]for alla. Based on asymptotic approxi-
mation (Schwarz, 1978; Haughton, 1988), the posterior probability P(Za|D[t]) =Op(exp[−δ(Za)na(t)])for
anyZa̸=PaG∗
⟨a⟩whenna(t)is large, where δ(Za)>0is a constant depending on Za. Letdabe the number
of candidate parent sets for X⟨a⟩andδa= min{δ(Za) :Za̸=PaG∗
⟨a⟩}. Taking expectation, we arrive at
E/braceleftbig
Var/bracketleftbig
µt
a(Za)|D[t]/bracketrightbig/bracerightbig
≤c2
2daexp(−δana(t)), (21)
for some positive constant c2. Combining (20) and (21) leads to Assumption 1.
PutZ∗≡Za∗andna∗(t)≡n∗(t). To verify Assumption 2, we make use of concentration of the conditional
posterior distribution p(µ∗|Z∗,D[t])and concentration of E (µ∗|Z∗,D[t]). Define two events,
Et,1:=/braceleftigg
µ∗−E(µ∗|Z∗,D[t])/radicalbig
E [Var(µ∗|Z∗,D[t])|D[t]]>c
2/radicalbig
logt/bracerightigg
,
Et,2:=/braceleftigg
E(µ∗|Z∗,D[t])−E(µ∗|D[t])/radicalbig
Var [E(µ∗|Z∗,D[t])|D[t]]>c
2/radicalbig
logt/bracerightigg
.
By (18), Var(µ∗|D[t])≥E [Var(µ∗|Z∗,D[t])|D[t]]andVar(µ∗|D[t])≥Var [E(µ∗|Z∗,D[t])|D[t]]. Then,
we have
P/braceleftigg
µ∗−E(µ∗|D[t])/radicalbig
Var(µ∗|D[t])> c/radicalbig
logt/vextendsingle/vextendsingle/vextendsingleD[t]/bracerightigg
≤P(Et,1|D[t]) +P(Et,2|D[t]).
22Published in Transactions on Machine Learning Research (01/2023)
For the first probability, we further condition on Z∗:
P(Et,1|D[t]) =/summationdisplay
Z∗P(Et,1|Z∗,D[t])P(Z∗|D[t]).
According to (19), for a fixed D[t],Var(µ∗|Z∗,D[t]) =Op(E [Var(µ∗|Z∗,D[t])|D[t]]) =Op(1/[n0+n∗(t)]).
Then for some constant c(Z∗)>0,
P(Et,1|Z∗,D[t])≤P/braceleftigg
µ∗−E(µ∗|Z∗,D[t])/radicalbig
Var(µ∗|Z∗,D[t])>c(Z∗)/radicalbig
logt/vextendsingle/vextendsingle/vextendsingleZ∗,D[t]/bracerightigg
.
Upper bounds for the right side can be established based on concentration of many common posterior
distributions. For the Gaussian setting, µ∗|Z∗,D[t]follows atdistribution with (n0+n∗(t)−|Z∗|)degrees
of freedom. Existing concentration inequality for tdistribution with rdegrees of freedom, such as
P(|tr|≥x)≤2 exp(−x2/4) + exp(−r/16)
in Lemma 18 of Wang (2022), can be used to show that P(Et,1|Z∗,D[t]) =O(t−b)for someband every
candidate Z∗and therefore P(Et,1|D[t]) =O(t−b)for anyD[t].
Note that E(µ∗|Z∗,D[t]) =µt
a∗(Z∗)is a function of Z∗conditioning onD[t]and thus a discrete and bounded
random variable. The second probability
P(Et,2|D[t]) =P/braceleftigg
µt
a∗(Z∗)−E(µt
a∗(Z∗)|D[t])/radicalbig
Var [µt
a∗(Z∗)|D[t]]>c
2/radicalbig
logt/vextendsingle/vextendsingle/vextendsingleD[t]/bracerightigg
may be shown to be O(t−b)by existing concentration inequality of a discrete bounded random variable and
the concentration of [Z∗|D[t]]on the true parent set.
Proof of Proposition 2. LetZbe the parent set of X. Then by a special case of (1),
p(y|do(x)) =/integraldisplay
p(y|x,z)p(z)dz
=/integraldisplay
ϕ(y|ψx+γ⊤z,σ2)ϕ(z|0,ΣZ)dz
=ϕ(y|ψx,γ⊤ΣZγ+σ2),
whereϕ(·|µ,Σ)is the probability density function of N(µ,Σ)andΣZis the covariance matrix of Z. Thus,
Y|do(X=x)∼N(ψx,γ⊤ΣZγ+σ2).
Now representing [Y|X,Z]by a linear regression:
Y=ψX+γ⊤Z+ε,
whereε∼N(0,σ2)⊥Z∼N(0,ΣZ). Then we have
VarP(Y−ψX) = VarP(γ⊤Z+ε)
=γ⊤ΣZγ+σ2= VarP(Y|do(X=x)).
Proof of Proposition 3. The result follows straightforwardly from a simple graphical argument. Let ΞG
XY
denote the distinct directed paths from XtoYin the causal graph Ggiven the model (13), where ξ∈ΞG
XY
23Published in Transactions on Machine Learning Research (01/2023)
consists of all the directed edges i→j∈Eon the given path from XtoY. Then the causal effect of Xon
Ycan be expressed as the sum of propagated direct effects along all directed paths from XtoY:
ψXY:=∂
∂xEP[Y|do(X=x)] =/summationdisplay
ξ∈ΞG
XY/productdisplay
i→j∈ξβij.
We denote the variables under the intervention do(W=w),w∈Ras˜X, with resulting causal model
˜Xj=p/summationdisplay
i=1˜βij˜Xi+ ˜εj, j = 1...,p,
where
˜βij=/braceleftigg
0ifXj=W
βijotherwise,˜εj=/braceleftigg
wifXj=W
εjotherwise.
The corresponding causal graph for ˜Xis the mutilated graph ˜Gresulting from deleting all edges into W.
The causal effect of ˜Xon˜Yis then
ψ˜X˜Y:=∂
∂xEP[˜Y|do(˜X=x)] =∂
∂xEP[Y|do(X=x),do(W=w)] =/summationdisplay
ξ∈Ξ˜G
XY/productdisplay
i→j∈ξ˜βij.
SinceWdoes not block any directed path from XtoY, the mutilated graph ˜Gretains all the directed paths
fromXtoYinG, soΞ˜G
XY= ΞG
XY. By the same reasoning, ˜βij=βijfor alli→j∈ξwhereξ∈Ξ˜G
XY.
Therefore, for any w∈R,
∂
∂xEP[Y|do(X=x)] =∂
∂xEP[Y|do(X=x),do(W=w)].
B Experimental Details
In this section, we include details regarding the experiments discussed in Section 6.
For our random network simulations, we generated CBN models for p= 10variables with reward variable
Y=Xp. In order to investigate interesting structures with diverse non-trivial confounding relationships,
we randomly generated graph structures using the following process adapted from de Kroon et al. (2022).
Given a fixed topological sort of the variables X1≺···≺Xpwhere the reward variable is Y=Xp, we
sequentially considered nodes in reverse topological order: i=p−1,..., 1. We uniformly sampled the
maximum out-degree of Xi, denoteddi, from 1top−i. Then, for ditimes, we randomly selected Xj
from{Xj∈X:Xi≺Xj}, addingXi→Xjto the graph only if the edge was not already present and
|PaG
j|<3. We imposed an additional requirement that |PaG
p|= 3, randomly adding parents if necessary.
If the generated structure consisted of multiple disconnected components, we rejected the structure and
reattempted the process.
The conditional probability distributions of each CBN were likewise generated randomly. For discrete net-
works, the variables were all assumed to be binary, and the conditional probability tables were randomly
generated uniformly and normalized, and were accepted only if for every edge Xj→Xi, there is a suffi-
ciently large causal effect, with |P[Xi=xi|do(Xj=xj)]−P(Xi=xi)|≥0.05for somexi∈Dom(Xi)
andxj∈Dom(Xj). Additionally, we required the marginal probability of any single discrete level to be
at least 0.01, and that the reward signal of the optimal intervention a∗be sufficiently large with respect to
the observational mean: µ∗−E[Y]≥0.05. For Gaussian networks, according to the model expressed in
(13), we sampled coefficients uniformly from [−1,−0.5]∪[0.5,1]forXi∈PaG
jand standard deviations from
[√
0.5,1], and we normalized the system to have unit variance. Note that in the Gaussian setting, there are
24Published in Transactions on Machine Learning Research (01/2023)
effectively|A|= 9actions given that interventional data on the same variable may be combined as discussed
in Section 5.2, which we implement for the competing methods as well. We found that ⟨a∗⟩∈PaG
pheld
for 98% of the discrete models that we randomly generated, though only for 65% of the random Gaussian
models. As discussed in Section 6, we artificially enforced ⟨a∗⟩∈PaG
pwhen evaluating the regret of TS∗and
(Bayes-)UCB∗.
The randomly generated networks were limited to size p= 10in the interest of extending the scope of our
empirical investigation in other aspects, namely in representing a large number of random causal models and
executing enough repetitions and time steps to reasonably assess the expected performance. We emphasize
that this limitation is primarily due to the breadth of our simulation study, whereas in practical applications
there may not be the need for tens of thousands of executions.
The in-degree restriction of |PaG
j|≤3for allj∈Vwas largely due to the difficulty in reliably generating
random conditional probability distributions that have meaningful causal effects and reward signals, as
defined in the previous paragraph, for denser discrete networks. In general, it is not uncommon to assume
the underlying DAG structure is sparse (Kalisch & Bühlmann, 2007). Similarly, the choice of |PaG
p|= 3was
motivated by our interest in investigating non-trivial structures that have substantive connectivity between
the reward variable and the intervened variables. Note that without sufficiently meaningful connectivity and
causal effects, our BBB methodology is actually advantaged in that the interventional distributions generally
will not be substantively different than the observational distribution, thus nullifying the need for backdoor
adjustment and correspondingly causal structure learning.
For Bayes-UCB(∗), the best quantile constant in (8) was c= 0, in agreement with the empirical recommen-
dation by Kaufmann et al. (2012). The best exploration parameter for UCB in (6) was c= 1/(2√
2)for
UCB(∗) in the discrete setting. In the Gaussian setting, UCB and UCB∗preferredc= 1/2andc= 1/√
2,
respectively, the latter of which we applied for BBB. We used standard uninformative priors for TS(∗), with
α0=β0= 1for the Beta prior and m0= 0,ν0= 1, andu0=v0= 1for the N-Γ−1prior. For BBB,
we computed exact parent set probabilities (4) using the program1implementing the efficient algorithm
developed by and applied in Pensar et al. (2020), restricting the maximum size of parent sets to three and
using the Bayesian Dirichlet equivalent uniform and Bayesian Gaussian equivalent scores. For the Gaussian
setting, we checked the graphical criterion in Proposition 3 according to (16) with τ= 0.1.
While we focused in Section 3 on designing the marginal posteriors according to (3), a notable difference
betweenourproposedBayesianCBframeworkandtheBayesianMABapproachdescribedinSection2isthat
in our design, the posterior distribution is not modular, with the marginals (πt
a)a∈Amutually dependent on
the distribution of graph structures. However, because of software limitations and for simplicity, we sampled
the criterion Ua(t)for each arm independently in the implementation of BBB-TS in our random network
experiments (line 7 in Algorithm 1). Although preliminary results have shown the difference in empirical
performance to be negligible, a more precise implementation would first sample a DAG Gfrom the posterior
distribution P(G|D [t])and subsequently for each arm a∈A, sampleUa(t)fromπt
a|PaG
⟨a⟩, which we apply
using MCMC in our scaling experiments.
ForourinvestigationofscalingBBBwithMCMC,weusedthesamegeneratedrandomnetworksaspreviously
described for Figure 4. For the CHILD network, we coerced all variables to binary variables, with the
extraneous discrete states removed by sequentially merging states with least marginal probability. We
averaged the conditional probability distributions imposed by merged states weighted according to their
marginal probabilities.
Order MCMC was implemented by extending the BiDAG package (Suter et al., 2021) to accommodate
computing scores with ensemble data as described in Section 3. For each iteration of BBB, the structure
posterior was estimated by conducting 104iterations with a thinning interval of 10 and discarding the first
20% as burn-in steps. The resulting set of DAGs were used for Bayesian model averaging in BBB-(Bayes-
)UCB, and for BBB-TS one random DAG was selected. For hybrid MCMC, the search space was initially
gently restricted by executing the PC algorithm (Spirtes & Glymour, 1991) with a relatively large threshold
α= 0.1and only investigating conditioning sets of up to size one. For subsequent iterations, the search space
1Pensar et al. (2020) provided their code under the MIT License at https://github.com/jopensar/BIDA .
25Published in Transactions on Machine Learning Research (01/2023)
Alg: Bayes−UCB Alg: TS Alg: UCB
0200400600Discrete Cumulative
Alg: Bayes−UCB Alg: TS Alg: UCB
500 2000 3500 5000 500 2000 3500 5000 500 2000 3500 50000250500750
Time StepGaussian Cumulative
Alg Alg* BBB−Alg(k=0) BBB−Alg(k=3) BBB−Alg(k=5)
Figure 6: Median cumulative regret with 95% percentile error bars for time steps t∈{500,2000,3500,5000}
comparing Alg,Alg∗, and BBB-AlgforAlg∈{Bayes-UCB,TS,UCB}. BBB methods were executed with
n0= 100·2kin the discrete setting and n0= 10·2kin the Gaussian setting.
was restricted to edges that appear with at least 0.05 probability in the structure posterior estimated in the
preceding iteration. Note that the hybrid approach proposed by Kuipers et al. (2022) includes provisions for
extending the search space for greater robustness in the presence of false negatives, so with each iteration
the search space may be sequentially reduced or expanded as the structure posterior is increasingly informed
by interventional data.
C Additional Results
Due to the density of information communicated in figures such as Figure 1, along with the substantial
variability arising from the randomness in graph structures, conditional probability distributions, and data,
we chose not to include error bounds of the empirical variability. To visualize the variability in the empirical
results, we provide median cumulative regret with 95% percentile error bars in Figure 6.
Furthermore, in what follows we present the results from additional experiments designed to evaluate firstly
ourproposedapproximationofthesamplingvarianceofthediscretebackdooradjustmentestimator(12), and
26Published in Transactions on Machine Learning Research (01/2023)
secondly the application of Proposition 3 by way of Gaussian backdoor adjustment with jointly interventional
and observational data.
C.1 Discrete Backdoor Adjustment and Variance
In this section, we describe and present experiments evaluating the behavior of ˆµa,bda(Z)where Z=PaG
⟨a⟩
as in (12), as well as our proposed approximation of its variance, derived in detail in Appendix D. Four
variance estimation methods were investigated. In the naive approach, ˆµa,bda(Z)is treated as a conditional
proportion as is the case when |Z|= 0, and the variance is estimated as ˆµa,bda(Z)[1−ˆµa,bda(Z)]/n[xa]where
n[xa]is the number of samples of data where X⟨a⟩=xa. The sampling approach estimates the variance
from samples from the population distribution, and the bootstrap approach conducts resampling from each
sample distribution, each with 103repetitions.
The generation of discrete CBNs for the simulation scenarios was designed as follows. The graph structure
was generated simply by initializing a structure where there is a direct edge from the intervened node X⟨a⟩
to the reward variable YandX⟨a⟩has|Z|=mparents. For each parent Xj∈Z, an edgeXj→Ywas
randomly added with 0.5probability to create backdoor paths. Finally, conditional probability tables were
generated uniformly as described in Section 6.
For observational sample sizes n0∈{100·2k:k= 0,1,..., 5}and parent set sizes |Z|∈{ 0,1,2,3},103
scenarios were created by randomly generating CBNs as described above and the methods were assessed
under each scenario through the following process. First, 106datasets were generated, each with n0samples
of observational data, and for each dataset, ˆµa,bda(Z)was computed for some arbitrary xa∈Dom(X⟨a⟩).
Then, for each of the four methods, the variance was estimated corresponding to the first 103estimates of
ˆµa,bda(Z), andfromthosethe2standarddeviationintervalcoverageprobabilityofthetrue µawascomputed.
The estimator ˆµa,bda(Z)itself was found to be generally unbiased, with the average of the 106estimates
deviating from the true µaby less than 2%in over 99%of the 24,000 scenarios. The coverage probability
results are shown in Figure 7, where each boxplot visualizes the coverage probability of a method across
103scenarios randomly generated under the given simulation setting. The outliers and invalid values, which
typically corresponded to extreme scenarios, were removed. The naive approach is only correct when |Z|= 0
and performs poorly when otherwise. The general results may be summarized as Naive <Bootstrap≈
Proposed<Sampling, though our proposed estimator appears to outperform the bootstrap approach for
larger|Z|and perform comparably with the population sampling approach for larger n0while requiring
significantly less and nearly negligible computational expense compared to either.
C.2 Gaussian Backdoor Adjustment with Ensemble Data
In this section, we empirically validate our methodology of conducting the regression (14) with jointly
interventional and observational data to estimate ψ⟨a⟩, as discussed in Section 5.2. In particular, we compare
the coverage probability of ˆψ⟨a⟩,bda(Z)where Z=PaG
⟨a⟩estimated using purely observational data and
ensemble data. The ensemble data was generated by allowing each data sample to be generated by one
of the possible interventions {do(Xj=xj) :Xj∈Z,xj∈{− 1,1}}or by passive observation, with equal
probability given to each of the 2|Z|+ 1options.
For sample sizes n∈{10·2k:k= 0,1,..., 5}and parent set sizes |Z|∈{1,2,3,4},103scenarios were created
by randomly generating CBNs. The network structures were generated as described in Appendix C.1, and
the parameters as in Section 6. Each data generation method was evaluated for each scenario by generating
105datasets with nsamples and estimating ˆψ⟨a⟩,bda(Z)and ˆSE2[ˆψ⟨a⟩,bda(Z)]for each dataset by conducting
the regression (14). From those estimates, 95%confidence interval coverage probabilities were computed for
each scenario.
The average of the 105estimates of ˆψ⟨a⟩,bda(Z)deviated from the true ψ⟨a⟩by at most 0.9%across all
24,000 simulation scenarios for both data generation methods. The coverage probability results are shown
in Figure 8. Since the results did not vary across parent set sizes, each boxplot visualizes the coverage
probability of a method across the 4,000simulation scenarios at each sample size. It is easy to see equivalent
27Published in Transactions on Machine Learning Research (01/2023)
3200
03200
13200
23200
31600
01600
11600
21600
3800
0800
1800
2800
3400
0400
1400
2400
3200
0200
1200
2200
3100
0100
1100
2100
3
0.850.900.951.00
0.850.900.951.00
0.850.900.951.00
0.850.900.951.00
0.850.900.951.00
0.850.900.951.00
EstimatorCoverage per Scenario
Naive Bootstrap Proposed Sampling
Figure 7: Coverage probability per scenario using various estimators of Var[ˆµa,bda(Z)]acrossn0∈{100·2k:
k= 0,1,..., 5}samples of observational data and |Z|∈{0,1,2,3}adjustment set sizes.
28Published in Transactions on Machine Learning Research (01/2023)
0.948
0.950
0.952
10 20 40 80 160 320
Sample SizeCoverage per Scenario
Observational Ensemble
Figure 8: Coverage probability per simulation scenario across sample sizes for observational and ensemble
data generating methods.
performance of the estimator computed with ensemble data compared to observational data, with consistent
coverage across all sample sizes.
D Derivation of the Discrete Backdoor Adjustment Variance Approximation
In this section, we derive the approximation of the sampling variance of (12):
ˆµa,bda(Z) =1
n0/summationdisplay
zn0[1,xa,z]n0[z]
n0[xa,z].
For the entirety of this section, we assume that the expectations and variances are with respect to the discrete
probability distribution Pdefined by a fixed CBN B.
D.1 Introduction
For simplicity, we redefine some notation. The backdoor adjustment to estimate the interventional distribu-
tion ofY|do(X=x)with parent set Z=PaG
Xwithrparent configurations is given by:
P[Y=y|do(X=x)] =/summationdisplay
zP(Y=y|X=x,Z=z)P(Z=z).
Empirically, given nsamples of observational data, this quantity is estimated using counts:
ˆP[Y=y|do(X=x)] =/summationdisplay
zn[y,x,z]
n[x,z]n[z]
n=1
n/summationdisplay
zn[y,x,z]n[z]
n[x,z], (22)
wheren[y,x,z]represents the number of samples in which Y=y,X=x, and Z=z, with corresponding
definitions for n[x,z]andn[z]. The joint probability distribution of X,Y, and Zmay be lumped into a
multinomial random vector N= (N1,N1′,N1′′,...,Nr,Nr′,Nr′′)∈R3rwhere fori= 1,...,r,
Ni=n[y,x,zi], Ni′=n[¬y,x,zi], Ni′′=n[¬x,zi].
29Published in Transactions on Machine Learning Research (01/2023)
Note thatNi+Ni′+Ni′′=n[zi], so/summationtextr
i=1(Ni+Ni′+Ni′′) =n, soNmay be thought of as a repartitioning
of the joint probability distribution of X,Y, and Zinto3rdisjoint levels:
N= (N1,N1′,N1′′,...,Nr,Nr′,Nr′′)∼Multinom(n,p),
p= (p1,p1′,p1′′,...,pr,pr′,pr′′),where
pi= E/bracketleftbiggn[y,x,zi]
n/bracketrightbigg
, pi′= E/bracketleftbiggn[¬y,x,zi]
n/bracketrightbigg
, pi′′= E/bracketleftbiggn[¬x,zi]
n/bracketrightbigg
fori= 1,...,r.(23)
The advantage of such a representation is so that for each zi, the term within the summation may be
expressed as a function of three disjoint elements of a multinomial random vector:
1
nr/summationdisplay
i=1n[y,x,zi]n[zi]
n[x,zi]=1
nr/summationdisplay
i=1n[y,x,zi] (n[y,x,zi] +n[¬y,x,zi] +n[¬x,zi])
n[y,x,zi] +n[¬y,x,zi]
=1
nr/summationdisplay
i=1Ni(Ni+Ni′+Ni′′)
Ni+Ni′.(24)
Note that each term is not straightforward to compute. An obvious challenge is that the denominator of
each term in the summation in (24) can be zero, so there is no analytical solution for its mean, variance,
and covariance.
D.2 Taylor Series Expansion for Ratio Distribution
To circumvent this challenge, we approximate the ratio in (24) with the Taylor series approximation. We
begin by defining
Mi=Ni(Ni+Ni′+Ni′′)
n2,
Wi=Ni+Ni′
n,
Qi=f(Mi,Wi) =Mi
Wi.
This allows us to express the variance of (24) in terms of Qi:
Var/bracketleftig
ˆP[Y=y|do(X=x)]/bracketrightig
= Var/bracketleftiggr/summationdisplay
i=1Qi/bracketrightigg
=r/summationdisplay
iVar [Qi] + 2r/summationdisplay
i=1/summationdisplay
j>iCov [Qi,Qj].(25)
By Taylor series expansion around µi= (µMi,µWi) = (E[Mi],E[Wi]):
Qi=f(Mi,Wi)
=f(µi) + (Mi−µMi)∂f
∂Mi(µi) + (Wi−µWi)∂f
∂Wi(µi)
+1
2(Mi−µMi)2∂2f
∂M2
i(µi) +1
2(Wi−µWi)2∂2f
∂W2
i(µi)
+ (Mi−µMi)(Wi−µWi)∂2f
∂Mi∂Wi(µi)
+O/parenleftbig
∥(Mi,Wi)−µi∥3/parenrightbig
,(26)
30Published in Transactions on Machine Learning Research (01/2023)
where
∂f
∂Mi(Mi,Wi) =1
Wi,∂2f
∂M2
i(Mi,Wi) = 0,
∂f
∂Wi(Mi,Wi) =−Mi
W2
i,∂2f
∂W2
i(Mi,Wi) =2Mi
W3
i,
∂2f
∂Mi∂Wi(Mi,Wi) =∂2f
∂Wi∂Mi(Mi,Wi) =1
W2
i(27)
Given (26), we obtain an approximate expected value:
E[Qi]≈f(µi) +1
2∂2f
∂M2
i(µi)Var[Mi] +1
2∂2f
∂W2
i(µi)Var[Wi] +∂2f
∂Mi∂Wi(µi)Cov[Mi,Wi].(28)
For variance and covariance, we use a simpler approximation:
Qi=f(Mi,Wi)≈f(µi) + (Mi−µMi)∂f
∂Mi(µi) + (Wi−µWi)∂f
∂Wi(µi), (29)
resulting in
Var[Qi]≈∂f
∂Mi(µi)2Var[Mi] +∂f
∂Wi(µi)2Var[Wi]
+ 2∂f
∂Mi(µi)∂f
∂Wi(µi)Cov[Mi,Wi],(30)
and
E[QiQj]≈f(µi)f(µj)
+∂f
∂Mi(µi)∂f
∂Mj(µj)Cov[Mi,Mj] +∂f
∂Mi(µi)∂f
∂Wj(µj)Cov[Mi,Wj]
+∂f
∂Wi(µi)∂f
∂Mj(µj)Cov[Wi,Mj] +∂f
∂Wi(µi)∂f
∂Wj(µj)Cov[Wi,Wj],
so
Cov[Qi,Qj] = E[QiQj]−E[Qi]E[Qj]
=∂f
∂Mi(µi)∂f
∂Mj(µj)Cov[Mi,Mj] +∂f
∂Mi(µi)∂f
∂Wj(µj)Cov[Mi,Wj]
+∂f
∂Wi(µi)∂f
∂Mj(µj)Cov[Wi,Mj] +∂f
∂Wi(µi)∂f
∂Wj(µj)Cov[Wi,Wj].(31)
In what follows, we first derive important quantities from the multinomial distribution in Appendix D.3 and
apply them to compute the quantities in (25).
D.3 Multinomial Derivations
For this subsection, in an abuse of notation, let N= (N1,...,Nr)∼Multinom(n,p)andu,v,w,x∈
{1,...,r}are distinct values. It is well-known that E[Nu] =npu,Var[Nu] =npu(1−pu), and Cov(Nu,Nv) =
−npupv. Furthermore,
E[NuNv] = Cov[Nu,Nv] + E[Nu]E[Nv]
=n(n−1)pupv,(32)
31Published in Transactions on Machine Learning Research (01/2023)
and the first four moments from derivating the moment generating function are:
E[Nu] =npu,
E[N2
u] =n(n−1)p2
u+ E[Nu]
=npu[1 + ( n−1)pu],
E[N3
u] =n(n−1)[(n−2)p3
u+ 2p2
u] + E[ N2
u]
=npu[1 + ( n−1)pu(3 + ( n−2)pu)],
E[N4
u] =n(n−1)(n−2)/bracketleftbig
(n−3)p4
u+ 3p3
u/bracketrightbig
+ 2n(n−1)/bracketleftbig
(n−2)p3
u+ 2p2
u/bracketrightbig
+ E[N3
u]
=npu[1 + ( n−1)pu(7 + ( n−2)pu[6 + ( n−3)pu])].(33)
Define indicator random variable Uisuch thatUi= 1if the outcome for trial iisu∈{1,...,r}andUi= 0
otherwise. Similarly define Viforv̸=u,Wiforw̸=v̸=u, andXiforx̸=w̸=v̸=u. ThenNu,Nv,Nw,
andNxmay be expressed as
Nu=n/summationdisplay
i=1Ui, Nv=n/summationdisplay
i=1Vi, Nw=n/summationdisplay
i=1Wi, Nx=n/summationdisplay
i=1Xi.
We are interested in E[N2
uN2
v],E[N3
uNv],E[N2
uNvNw],E[NuNvNwNx],E[N2
uNv], and E[NuNvNw].
E[N2
uN2
v] = E/bracketleftigg/parenleftiggn/summationdisplay
i=1Ui/parenrightigg2/parenleftiggn/summationdisplay
i=1Vi/parenrightigg2/bracketrightigg
= E/bracketleftiggn/summationdisplay
i=1n/summationdisplay
j=1n/summationdisplay
k=1n/summationdisplay
l=1UiUjVkVl/bracketrightigg
by distributing
=n/summationdisplay
i=1n/summationdisplay
j=1n/summationdisplay
k=1n/summationdisplay
l=1E [UiUjVkVl] by linearity of expectation
=n/summationdisplay
i=1n/summationdisplay
j=1/summationdisplay
k ̸=i
k ̸=j/summationdisplay
l ̸=i
l ̸=jE [UiUjVkVl] since UiVi= 0for all i= 1, . . . , n
=n/summationdisplay
i=1n/summationdisplay
j=1/summationdisplay
k ̸=i
k ̸=j/summationdisplay
l ̸=i
l ̸=jE [UiUj] E [VkVl] by independence between trials
=/summationdisplay
i=j/summationdisplay
k=l
k ̸=iE [UiUj] E [VkVl] +/summationdisplay
i/summationdisplay
j ̸=i/summationdisplay
k ̸=i
k ̸=j/summationdisplay
l ̸=k
l ̸=i
l ̸=jE [UiUj] E [VkVl]
+/summationdisplay
i=j/summationdisplay
k ̸=i/summationdisplay
l ̸=k
l ̸=iE [UiUj] E [VkVl] +/summationdisplay
k=l/summationdisplay
i ̸=k/summationdisplay
j ̸=i
j ̸=kE [UiUj] E [VkVl]reexpressed
=/summationdisplay
i/summationdisplay
k=l
k ̸=iE[U2
i]E[V2
k] +/summationdisplay
i/summationdisplay
j ̸=i/summationdisplay
k ̸=i
k ̸=j/summationdisplay
l ̸=k
l ̸=i
l ̸=jE[Ui]E[Uj]E[Vk]E[Vl]reexpressed; independence; and
+/summationdisplay
i/summationdisplay
k ̸=i/summationdisplay
l ̸=k
l ̸=iE[U2
i]E[VkVl] +/summationdisplay
k/summationdisplay
i ̸=k/summationdisplay
j ̸=i
j ̸=kE[Ui]E[Uj]E[V2
k] since E[UiUj] = E[ Ui]E[Uj],i̸=j
=n(n−1)pupv+n(n−1)(n−2)(n−3)p2
up2
v since E[U2
i] = E[ Ui] =pu
+n(n−1)(n−2)pup2
v+n(n−1)(n−2)p2
upv
=n(n−1)pupv[1 + ( n−2)(pu+pv+ (n−3)pupv)] simplified .
Hence,
E[N2
uN2
v] =n(n−1)pupv[1 + (n−2)(pu+pv+ (n−3)pupv)]. (34)
32Published in Transactions on Machine Learning Research (01/2023)
Following the same derivation strategy,
E[N3
uNv] =n(n−1)pupv[1 + (n−2)pu(3 + (n−3)pu)], (35)
E[N2
uNvNw] =n(n−1)(n−2)pupvpw[1 + (n−3)pu], (36)
E[NuNvNwNx] =n(n−1)(n−2)(n−3)pupvpwpx, (37)
E[N2
uNv] =n(n−1)pupv[1 + (n−2)pu], (38)
E[NuNvNw] =n(n−1)(n−2)pupvpw. (39)
D.4 Numerator and Denominator of Ratio
We now turn to the task of deriving expressions for Var[Mi],Var[Wi], and Cov[Mi,Wi]in order to compute
(30), and additionally for Cov[Mi,Mj],Cov[Mi,Wj],Cov[Wi,Mj], and Cov[Wi,Wj]for (31). For this
subsection, return to the notation for Nexpressed in (23).
The distribution of Wi=n−1(Ni+Ni′)is most simple. By the lumping property of multinomial random
vectors,
E[Wi] =pi+pi′,
Var[Wi] =(pi+pi′)(1−pi−pi′)
n,
Cov[Wi,Wj] =−(pi+pi′)(pj+pj′)
n.(40)
The distribution of Mi=n−2Ni(Ni+Ni′+Ni′′)is more challenging. From (33) and (32), the expectation
is given by:
E[Mi] =n−2E[Ni(Ni+Ni′+Ni′′)]
=n−2/parenleftbig
E[N2
i] + E[NiNi′] + E[NiNi′′]/parenrightbig
=n−2(npi[1 + (n−1)pi] +n(n−1)pipi′+n(n−1)pipi′′)
=n−1pi[1 + (n−1)(pi+pi′+pi′′)].(41)
Next, the variance is given by:
Var[Mi] =n−4Var[Ni(Ni+Ni′+Ni′′)]
=n−4Var[N2
i+NiNi′+NiNi′′]
=n−4/parenleftbig
Var[N2
i] + Var[NiNi′] + Var[NiNi′′]
+ 2Cov[N2
i,NiNi′] + 2Cov[N2
i,NiNi′′] + 2Cov[NiNi′,NiNi′′]/parenrightbig
.
The terms in the expression above are given below. From the moments of the multinomial distribution (33):
Var[N2
i] = E[N4
i]−E[N2
i]2
=npi[1 + (n−1)pi(7 + (n−2)pi[6 + (n−3)pi])]−(npi[1 + (n−1)pi])2
=npi/bracketleftbig
1 + (n−1)pi(7 + (n−2)pi[6 + (n−3)pi])−npi(1 + (n−1)pi)2/bracketrightbig
.
From (34) and (32):
Var[NiNi′] = E[N2
iNi′2]−E[NiNi′]2
=n(n−1)pipi′[1 + (n−2)(pi+pi′+ (n−3)pipi′)]−[n(n−1)pipi′]2
=n(n−1)pipi′[1 + (n−2)(pi+pi′+ (n−3)pipi′)−n(n−1)pipi′],
Var[NiNi′′] =n(n−1)pipi′′[1 + (n−2)(pi+pi′′+ (n−3)pipi′′)−n(n−1)pipi′′].
33Published in Transactions on Machine Learning Research (01/2023)
From (35), (33), and (32):
Cov[N2
i,NiNi′] = E[N3
iNi′]−E[N2
i]E[NiNi′]
=n(n−1)pipi′[1 + (n−2)pi(3 + (n−3)pi)]
−npi[1 + (n−1)pi]n(n−1)pipi′
=n(n−1)pipi′/bracketleftbig
1 + (n−2)(3pi+ (n−3)p2
i)−npi(1 + (n−1)pi)/bracketrightbig
Cov[N2
i,NiNi′′] =n(n−1)pipi′′/bracketleftbig
1 + (n−2)(3pi+ (n−3)p2
i)−npi(1 + (n−1)pi)/bracketrightbig
.
From (36) and (32):
Cov[NiNi′,NiNi′′] = E[N2
iNi′Ni′′]−E[NiNi′]E[NiNi′′]
=n(n−1)(n−2)pipi′pi′′[1 + (n−3)pi]−n(n−1)pipi′n(n−1)pipi′′
=n(n−1)pipi′pi′′[(n−2)[1 + (n−3)pi]−n(n−1)pi].
Hence, Var[Mi]is derived:
Var[Mi] =n−4/parenleftbig
npi/bracketleftbig
1 + (n−1)pi(7 + (n−2)pi[6 + (n−3)pi])−npi(1 + (n−1)pi)2/bracketrightbig
+n(n−1)pipi′[1 + (n−2)(pi+pi′+ (n−3)pipi′)−n(n−1)pipi′]
+n(n−1)pipi′′[1 + (n−2)(pi+pi′′+ (n−3)pipi′′)−n(n−1)pipi′′]
+ 2n(n−1)pipi′/bracketleftbig
1 + (n−2)(3pi+ (n−3)p2
i)−npi(1 + (n−1)pi)/bracketrightbig
+ 2n(n−1)pipi′′/bracketleftbig
1 + (n−2)(3pi+ (n−3)p2
i)−npi(1 + (n−1)pi)/bracketrightbig
+n(n−1)pipi′pi′′[(n−2)[1 + (n−3)pi]−n(n−1)pi]/parenrightbig
.(42)
Next, consider Cov[Mi,Mj].
Cov[Mi,Mj] =n−4Cov/bracketleftbig
Ni(Ni+Ni′+Ni′′),Nj(Nj+Nj′+Nj′′)/bracketrightbig
=n−4Cov/bracketleftbig
N2
i+NiNi′+NiNi′′,N2
j+NjNj′+NjNj′′/bracketrightbig
=n−4/parenleftbig
Cov[N2
i,N2
j]
+ Cov[N2
i,NjNj′] + Cov[N2
i,NjNj′′] + Cov[NiNi′,N2
j] + Cov[NiNi′′,N2
j]
+ Cov[NiNi′,NjNj′] + Cov[NiNi′,NjNj′′]
+ Cov[NiNi′′,NjNj′] + Cov[NiNi′′,NjNj′′]/parenrightbig
.
The terms in the expression above are given below. From (34) and (33):
Cov[N2
i,N2
j] = E[N2
iN2
j]−E[N2
i]E[N2
j]
=n(n−1)pipj[1 + (n−2)(pi+pj+ (n−3)pipj)]
−npi[1 + (n−1)pi]npj[1 + (n−1)pj]
=npipj[(n−1)(1 + (n−2)(pi+pj+ (n−3)pipj))
−n(1 + (n−1)pi)(1 + (n−1)pj)].
From (36), (33), and (32):
Cov[N2
i,NjNj′] = E[N2
iNjNj′]−E[N2
i]E[NjNj′]
=n(n−1)(n−2)pipjpj′[1 + (n−3)pi]−npi(1 + (n−1)pi)n(n−1)pjpj′
=n(n−1)pipjpj′[(n−2)[1 + (n−3)pi]−n(1 + (n−1)pi)],
Cov[N2
i,NjNj′′] =n(n−1)pipjpj′′[(n−2)[1 + (n−3)pi]−n(1 + (n−1)pi)],
Cov[NiNi′,N2
j] =n(n−1)pjpipi′[(n−2)[1 + (n−3)pj]−n(1 + (n−1)pj)],
Cov[NiNi′′,N2
j] =n(n−1)pjpipi′′[(n−2)[1 + (n−3)pj]−n(1 + (n−1)pj)].
34Published in Transactions on Machine Learning Research (01/2023)
From (37) and (32):
Cov[NiNi′,NjNj′] = E[NiNi′NjNj′]−E[NiNi′]E[NjNj′]
=n(n−1)(n−2)(n−3)pipi′pjpj′−n(n−1)pipi′n(n−1)pjpj′
=n(n−1)pipi′pjpj′[(n−2)(n−3)−n(n−1)],
Cov[NiNi′,NjNj′′] =n(n−1)pipi′pjpj′′[(n−2)(n−3)−n(n−1)],
Cov[NiNi′′,NjNj′] =n(n−1)pipi′′pjpj′[(n−2)(n−3)−n(n−1)],
Cov[NiNi′′,NjNj′′] =n(n−1)pipi′′pjpj′′[(n−2)(n−3)−n(n−1)].
Hence, Cov[Mi,Mj]is derived:
Cov[Mi,Mj]
=n−4/parenleftbig
npipj/bracketleftbig
(n−1)(1 + (n−2)(pi+pj+ (n−3)pipj))
−n(1 + (n−1)pi)(1 + (n−1)pj)/bracketrightbig
+n(n−1)pipjpj′[(n−2)[1 + (n−3)pi]−n(1 + (n−1)pi)]
+n(n−1)pipjpj′′[(n−2)[1 + (n−3)pi]−n(1 + (n−1)pi)]
+n(n−1)pjpipi′[(n−2)[1 + (n−3)pj]−n(1 + (n−1)pj)]
+n(n−1)pjpipi′′[(n−2)[1 + (n−3)pj]−n(1 + (n−1)pj)]
+n(n−1)pipi′pjpj′[(n−2)(n−3)−n(n−1)]
+n(n−1)pipi′pjpj′′[(n−2)(n−3)−n(n−1)]
+n(n−1)pipi′′pjpj′[(n−2)(n−3)−n(n−1)]
+n(n−1)pipi′′pjpj′′[(n−2)(n−3)−n(n−1)]/parenrightbig
=n−3pipj/bracketleftbig
(n−1)/parenleftbig
1 + (n−2)(pi+pj+ (n−3)pipj)
+ (pj′+pj′′)[(n−2)[1 + (n−3)pi]−n(1 + (n−1)pi)]
+pjpi(pi′+pi′′)[(n−2)[1 + (n−3)pj]−n(1 + (n−1)pj)]
+ (pi′+pi′′)(pj′+pj′′)[(n−2)(n−3)−n(n−1)]/parenrightbig
−n(1 + (n−1)pi)(1 + (n−1)pj)/bracketrightbig(43)
Finally, we turn our attention to Cov[Mi,Wi],Cov[Mi,Wj], and Cov[Wi,Mj]. Beginning with Cov[Mi,Wi]:
Cov[Mi,Wi] =n−3Cov/bracketleftbig
Ni(Ni+Ni′+Ni′′),Ni+Ni′/bracketrightbig
=n−3Cov/bracketleftbig
N2
i+NiNi′+NiNi′′,Ni+Ni′/bracketrightbig
=n−3/parenleftbig
Cov[N2
i,Ni] + Cov[N2
i,Ni′]
+ Cov[NiNi′,Ni] + Cov[NiNi′,Ni′] + Cov[NiNi′′,Ni] + Cov[NiNi′′,Ni′]/parenrightbig
.
The terms in the expression above are given below. From (33):
Cov[N2
i,Ni] = E[N3
i]−E[N2
i]E[Ni]
=npi[1 + (n−1)pi(3 + (n−2)pi)]−npi[1 + (n−1)pi]npi
=npi[1 + (n−1)pi(3 + (n−2)pi)−npi(1 + (n−1)pi)]
=npi[1 +pi((n−1)[3−2pi]−n)].
From (38) and (33):
Cov[N2
i,Ni′] = E[N2
iNi′]−E[N2
i]E[Ni′]
=n(n−1)pipi′[1 + (n−2)pi]−npi(1 + (n−1)pi)npj
=npipi′[(n−1)(1 + (n−2)pi)−n(1 + (n−1)pi)].(44)
35Published in Transactions on Machine Learning Research (01/2023)
From (38) and (33):
Cov[NiNi′,Ni] = E[N2
iNi′]−E[NiNi′]E[Ni]
=n(n−1)pipi′[1 + (n−2)pi]−n(n−1)pipi′npi
=n(n−1)pipi′[1−2pi],
Cov[NiNi′,Ni′] =n(n−1)pi′pi[1−2pi′],
Cov[NiNi′′,Ni] =n(n−1)pipi′′[1−2pi].
From (39), (32), and (33):
Cov[NiNi′′,Ni′] = E[NiNi′′Ni′]−E[NiNi′′]E[Ni′]
=n(n−1)(n−2)pipi′′pi′−n(n−1)pipi′′npi′
=−2n(n−1)pipi′′pi′.(45)
Hence, Cov[Mi,Wi]is derived:
Cov[Mi,Wi] =n−3/parenleftbig
npi[1 +pi((n−1)[3−2pi]−n)]
+npipi′[(n−1)(1 + (n−2)pi)−n(1 + (n−1)pi)]
+n(n−1)pipi′[1−2pi]
+n(n−1)pi′pi[1−2pi′]
+n(n−1)pipi′′[1−2pi]
−2n(n−1)pipi′′pi′/parenrightbig
.
=n−2pi/parenleftbig
1 +pi((n−1)[3−2pi]−n)
+pi′[(n−1)(1 + (n−2)pi)−n(1 + (n−1)pi)]/parenrightbig
+n−2(n−1)(pipi′[2−2pi−2pi′] +pipi′′[1−2pi−2pi′]).(46)
Then, moving on to Cov[Mi,Wj]andCov[Wi,Mj]:
Cov[Mi,Wj] =n−3Cov/bracketleftbig
Ni(Ni+Ni′+Ni′′),Nj+Nj′/bracketrightbig
=n−3Cov/bracketleftbig
N2
i+NiNi′+NiNi′′,Nj+Nj′/bracketrightbig
=n−3/parenleftbig
Cov[N2
i,Nj] + Cov[N2
i,Nj′]
+ Cov[NiNi′,Nj] + Cov[NiNi′,Nj′] + Cov[NiNi′′,Nj] + Cov[NiNi′′,Nj′]/parenrightbig
.
The terms in the expression above are given below. From (44):
Cov[N2
i,Nj] =npipj[(n−1)(1 + (n−2)pi)−n(1 + (n−1)pi)],
Cov[N2
i,Nj′] =npipj′[(n−1)(1 + (n−2)pi)−n(1 + (n−1)pi)].
From (45):
Cov[NiNi′,Nj] =−2n(n−1)pipi′pj,
Cov[NiNi′,Nj′] =−2n(n−1)pipi′pj′,
Cov[NiNi′′,Nj] =−2n(n−1)pipi′′pj,
Cov[NiNi′′,Nj′] =−2n(n−1)pipi′′pj′.
Hence, Cov[Mi,Wj]andCov[Wi,Mj]are derived:
Cov[Mi,Wj] =n−3/bracketleftbig
npi(pj+pj′)[(n−1)(1 + (n−2)pi)−n(1 + (n−1)pi)]
−2n(n−1)pi(pi′pj+pi′pj′+pi′′pj+pi′′pj′)/bracketrightbig
=n−2pi(pj+pj′)/bracketleftbig
(n−1)(1 + (n−2)pi−2(pi′+pi′′)(pj+pj′))
−n(1 + (n−1)pi)/bracketrightbig
,
Cov[Wi,Mj] =n−2pj(pi+pi′)/bracketleftbig
(n−1)(1 + (n−2)pj−2(pj′+pj′′)(pi+pi′))
−n(1 + (n−1)pj)/bracketrightbig
.(47)
36Published in Transactions on Machine Learning Research (01/2023)
Thus, all quantities necessary to compute (25) are derived.
37