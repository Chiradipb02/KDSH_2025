Tight Rates for Bandit Control Beyond Quadratics
Y. Jennifer Sun
Princeton University
ys7849@princeton.eduZhou Lu
Princeton University
zhoul@princeton.edu
Abstract
Unlike classical control theory, such as Linear Quadratic Control (LQC), real-world
control problems are highly complex. These problems often involve adversarial
perturbations, bandit feedback models, and non-quadratic, adversarially chosen
cost functions. A fundamental yet unresolved question is whether optimal regret
can be achieved for these general control problems. The standard approach to
addressing this problem involves a reduction to bandit convex optimization with
memory. In the bandit setting, constructing a gradient estimator with low variance
is challenging due to the memory structure and non-quadratic loss functions.
In this paper, we provide an affirmative answer to this question. Our main con-
tribution is an algorithm that achieves an ˜O(√
T)optimal regret for bandit non-
stochastic control with strongly-convex and smooth cost functions in the presence of
adversarial perturbations, improving the previously known ˜O(T2/3)regret bound
from (Cassel and Koren, 2020). Our algorithm overcomes the memory issue by
reducing the problem to Bandit Convex Optimization (BCO) without memory and
addresses general strongly-convex costs using recent advancements in BCO from
(Suggala et al., 2024). Along the way, we develop an improved algorithm for BCO
with memory, which may be of independent interest.
1 Introduction
Optimal control lies at the heart of engineering and operations research, with applications ranging
from launching spacecraft to stabilizing economies. The theory of optimal control is a well-established
field with a rich history, dating back to 1868 when James Clerk Maxwell analyzed governors, and
flourishing in the mid-20th century with the development of dynamic programming by (Bellman,
1954) and the Kalman filter by (Kalman, 1960).
Classic optimal control theory studies the problem where a controller interacts with an environment,
according to a (partially observable) linear time-invariant (LTI) dynamical system:
xt+1=Axt+But+wt, yt=Cxt+et, (1)
where A, B, C are the dynamics governing the system, xt, ut, yt, wt, etrepresent the system state,
control input, observation, perturbation, and observation noise at time t, respectively. At each time t,
the controller observes ytand chooses a control ut, incurring a cost ct(yt, ut)based on the current
observation and control. The system then evolves according to Eq. (1) to reach the next state xt+1.
The theory of optimal control (e.g., LQC) typically relies on three key assumptions on the setting:
the perturbation wtis stochastic, the cost function ctis quadratic and known in advance, and the
function ctis observable to the controller. The linear-quadratic regulator (LQR) (Kalman et al., 1960)
provides a closed-form optimal solution under these conditions, representing a pinnacle of classical
control theory.
However, these assumptions are often too idealistic for practical scenarios. In real-world control
problems, the perturbations can be adversarial, the feedback model can be bandit, and the cost
38th Conference on Neural Information Processing Systems (NeurIPS 2024).function can be non-quadratic. This is evident in applications such as autonomous vehicle navigation,
advertisement placement, and traffic signal control. This discrepancy between theory and practice
raises a fundamental question for developing a more general control theory:
Can we devise algorithms with provable guarantees for LTI control problems with
adversarial perturbations, bandit feedback models, and non-quadratic cost?
Recent research in online non-stochastic control (see (Hazan and Singh, 2022) for a survey) aims
to address this broader goal by relaxing two of the standard assumptions: (1) the cost ctcan be
time-varying, non-quadratic convex functions unknown to the controller; (2) the perturbation wtand
the observation noise etcan be adversarially chosen.
The natural performance metric in this context is regret , defined as the excess cost incurred by the
controller compared to the best control policy in a benchmark policy class Π:
RegretΠ
T(controller ) =TX
t=1ct(yt, ut)−min
π∈ΠTX
t=1ct(yπ
t, uπ
t), (2)
where (yt, ut)is the observation-control pair reached by executing the controller at time t, and
(yπ
t, uπ
t)is the observation-control pair under the policy πat time t. An optimal ˜O(√
T)regret was
obtained by (Agarwal et al., 2019a) with the Gradient Perturbation Controller ( GPC) algorithm.
Several works in online control have made further progress towards the general question. (Cassel and
Koren, 2020; Sun et al., 2024) obtained optimal regret under bandit feedback for strongly convex and
smooth cost when the perturbation wtand observation noise etare semi-adversarial (i.e. they contain
an additive stochastic component that admits covariance matrices with least singular value bounded
from below). (Cassel and Koren, 2020) also showed a sub-optimal ˜O(T2/3)regret bound for fully
adversarial perturbations. More recently, the advancement of (Suggala et al., 2024) showed for the
first time that an optimal regret of ˜O(√
T)is achievable for strongly convex and smooth quadratic
costs in the presence of adversarial perturbations and observation noises.
Still, no previous work has simultaneously addressed all three challenges with an optimal regret
guarantee, which was left as an open problem by (Suggala et al., 2024). The main challenge lies in
how to construct a low-variance gradient estimator under bandit feedback, with the memory structure
and non-quadratic cost. Due to the Ω(T2/3)regret lower bound for general BCO with memory by
(Suggala et al., 2024), exploiting the special affine memory structure in control problems is crucial to
achieving optimal regret for general convex cost.
In this work, we provide the first affirmative answer to this general question by devising an algorithm
that handles all three challenges with an optimal ˜O(√
T)regret bound. Our approach involves
reducing the problem to no-memory BCO, which circumvents the high-dimensional estimator issue
for non-quadratic costs. We then leverage a special curvature structure of the loss function induced
by general strongly convex and smooth costs to obtain the optimal regret guarantee. Our result serves
as a preliminary step toward fully solving the general control problem.
1.1 Technical Overview
Several previous works have addressed the bandit non-stochastic control problem, but none achieved
optimal regret across all three generalities due to the following technical challenges:
1.The necessity of curvature : The first work tackling the bandit non-stochastic control
problem was (Cassel and Koren, 2020), which achieved an ˜O(T2/3)regret bound for
smooth convex cost. However, such sub-optimality is arguably inevitable due to the ˜Ω(T2/3)
regret lower bound for BCO-M with smooth convex1cost (Suggala et al., 2024): all existing
results on bandit non-stochastic control relies on reduction to BCO-M! Indeed, the algorithm
of (Cassel and Koren, 2020) is based on online gradient descent ( OGD) and ignores the
geometry of cost. This is the reason why we introduce the assumption on strong convexity.
2.Strong convexity is not enough : Does strong convexity alone become an easy remedy to
the control problem? Unfortunately, even if we assume the cost functions ctin the control
1In fact, the lower bound proved in (Suggala et al., 2024) even holds for quadratic loss functions.
2Regret Perturbation Feedback Loss type
(Agarwal et al., 2019a) ˜O(√
T) adversarial full strongly-convex
(Agarwal et al., 2019b) O(log7T) stochastic full convex
(Cassel and Koren, 2020) ˜O(T2/3) adversarial bandit convex smooth
(Cassel and Koren, 2020) ˜O(√
T) stochastic bandit strongly-convex smooth
(Sun et al., 2024) ˜O(√
T) semi-adv bandit str-conv smooth quadratic
(Suggala et al., 2024) ˜O(√
T) adversarial bandit str-conv smooth quadratic
This work ˜O(√
T) adversarial bandit strongly-convex smooth
Table 1: Comparison of results. This work is the first to address all three generalities with an ˜O(√
T)
optimal regret: (Agarwal et al., 2019a) addressed perturbation + loss, (Cassel and Koren, 2020)
addressed feedback + loss, (Suggala et al., 2024) addressed perturbation + feedback. (Cassel and
Koren, 2020) also obtained a sub-optimal ˜O(T2/3)regret for perturbation + feedback + loss.
problem are strongly-convex, the induced loss functions in the BCO-M problem are not
necessarily strongly-convex! It was observed by (Suggala et al., 2024) that the induced
loss functions satisfy a property called κ-convexity, allowing for low-variance estimation of
Hessian matrices, which is a key ingredient in Newton-based second-order update to exploit
the curvature.
3.Going beyond quadratic : However, finding a low-biased first-order estimation of gradients
remains a challenge for BCO-M, because the nice property on the unit sphere domain
Sd−1:={x∈Rd| ∥x∥2= 1}of the exploration term is not preserved under Cartesian
product: Sd−1×Sd−1̸=Sd×d−1. To handle this issue, (Suggala et al., 2024) relies on
the quadratic cost assumption which admits an unbiased estimator of the divergence of the
induced loss function. It’s unclear how to handle general κ-convex smooth cost in BCO-M.
Similar to its full-information counterpart, we reduce the bandit non-stochastic control problem to
bandit convex optimization with memory (BCO-M). To overcome these challenges, we further reduce
the BCO-M problem to a no-memory BCO problem following (Cassel and Koren, 2020), to avoid
the issue on the Cartesian product of Sd−1. This allows for the use of a Newton-based BCO-M
algorithm similar to (Suggala et al., 2024), which can handle general κ-convex smooth cost because
the exploration domain is now Sd−1. Besides this new approach, our method also includes novel
algorithmic components and analysis.
1.2 Related Work
Optimal control theory (Bellman, 1954) concerns finding a control for a dynamical system such that
some objective function is optimized. The basic form of optimal control is linear quadratic control
(Kalman, 1960), in which a quadratic function is minimized in a linear first-order dynamical system
with stochastic noise, whose solution is given by LQR.
Online non-stochastic control generalizes classic optimal control by considering adversarial pertur-
bation and cost, with the metric of regret to compete with the best fixed policy in some benchmark
class. The first work (Agarwal et al., 2019a) obtained an optimal regret bound for general convex
cost by reduction to online convex optimization with memory (Anava et al., 2015), under stability
assumptions on the system. For the line of works on the full information feedback setting, see (Hazan
and Singh, 2022) for a survey.
Online non-stochastic control with bandit feedback were first considered by (Cassel and Koren, 2020;
Gradu et al., 2020). Under a smoothness assumption on cost functions, the two works showed an
˜O(T2/3)and˜O(T3/4)regret bound respectively. When the cost functions are additionally strongly-
convex quadratics, (Sun et al., 2024) obtained an ˜O(√
T)regret, under semi-adversarial perturbations.
(Yan et al., 2024) showed sublinear regret for cost functions with heterogeneous curvatures under the
3same semi-adversarial perturbations. This restriction on perturbation was later removed by (Suggala
et al., 2024) which achieved the same regret for adversarial perturbations.
Recent advancements on BCO (Lattimore and György, 2023; Suggala et al., 2021; Lattimore, 2024)
considered Newton step updates to achieve improved regret bounds. In particular, (Suggala et al.,
2024) also showed an ˜Ω(T2/3)lower bound for bandit quadratic optimization with memory (BQO-M)
without strong convexity. When making use of the affine memory structure, (Suggala et al., 2024)
obtained an ˜O(√
T)regret bound for BQO-M.
2 Preliminary
2.1 Online non-stochastic control with bandit feedback
Consider the linear dynamical system in Eq. (1). At time t∈N, the learner receives observation yt
and outputs control ut, to which the learner receives a scalar instantaneous cost ct(yt, ut)depending
on both the current observation and the control, and no other information about ctis available.
The perturbation wtand observation noise etcan be adversarially chosen. The learner’s goal is to
minimize regret over a fixed time horizon T∈Ndefined in Eq. (2).
Throughout this paper, all cost functions we consider are assumed to be twice continuously differen-
tiable. Consistent with past literature (Sun et al., 2024; Suggala et al., 2024), we make the following
assumptions on the cost functions.
Assumption 1 (Cost curvature and regularity) .Letdy, du∈Ndenote the dimensions of observation
and control, respectively. The control cost function ct:Rdy×Rdu→R+satisfies that
1.(Curvature) ctisαc-strongly convex and βc-smooth over some compact set Y × U ⊂ Rdy×
Rdufor some αc, βc>0, i.e.,∇ct(x2)⊤(x1−x2) +βc
2∥x1−x2∥2
2≥ct(x1)−ct(x2)≥
∇ct(x2)⊤(x1−x2) +αc
2∥x1−x2∥2
2for any x1, x2∈ Y × U .
2.(Gradient bounds) There exists some Gc>0such that the gradients satisfy ∥∇ct(y, u)∥2≤
Gc∥(y, u)∥2for any y∈ Y, u∈ U.
We also make stability assumption on the LDS in Eq. (1) as well as norm bound on the perturbation
and noise. These assumptions are standard in literature (e.g. (Hazan and Singh, 2022)) and necessary
for deriving meaningful regret guarantees.
Assumption 2 (Strong stabilizability and bounded dynamics) .(A, B, C )that governs the LDS in
Eq.(1)satisfies that max{∥A∥op,∥B∥op,∥C∥op} ≤κsysfor some positive constant κsys. There exists
K∈Rdu×dys.t.A+BKC =HL−1Hfor some H≻0andmax{∥K∥2,∥H∥2,∥H−1∥2} ≤κ,
∥L∥2≤1−γfor some κ >0,0< γ≤1. Such Kis called a (κ, γ)-stabilizing linear controller for
the LDS.
Assumption 3 (Bounded perturbation and noise) .The perturbation and observation noise sequences
satisfy the norm bound max t∈[T]{max{∥wt∥2,∥et∥2}} ≤ Rw,efor some Rw,e>0.
We assume the adversary chooses the cost functions and noise sequences in an oblivious way, i.e.
they are chosen independently of the learner’s decisions.
Assumption 4 (Oblivious adversary) .The sequence of cost functions {ct}T
t=1and the noise sequences
{wt}T
t=1,{et}T
t=1are chosen by an oblivious adversary and does not depend on the control played
by the learner.
For partially observable systems, the standard comparator class in literature (Simchowitz et al., 2020)
is the class of Disturbance Response Controllers (DRC). Essentially, this class considers controllers
that choose linear combinations of past signals and observations. The signals are simply the would-be
observations had a stabilizing controller Kbeen used from the beginning of the time. Formally, the
class of DRC is given by the following definition:
Definition 1 (DRC) .(1) A disturbance response controller (DRC) is a policy πMparametrized by
m∈Nmatrices M=M[0:m−1]inRdu×dysuch that the control at time taccording to πMis
ut(πM) =Kyt+m−1X
j=0M[j]yt−j(K),
4where Kis a(κ, γ)-stabilizing linear controller, and yK
tis the would-be observation had the linear
policy Kbeen carried out from the beginning of the time.
(2) A DRC policy class M(m, RM), parameterized by m∈N, RM>0, is the set of all DRC
controller of length mand obeys the norm bound ∥M∥ℓ1,op:=Pm−1
j=0∥M[j]∥op≤RM.
A DRC policy always produces controls that ensure the system remains state bounded, the DRC class
is expressive enough to approximate the class of linear policies of past observations (see Theorem 1
in (Simchowitz et al., 2020)).
2.2 Bandit convex optimization with memory (BCO-M)
We start with the general protocol of bandit convex optimization with memory (BCO-M). In the
(improper) BCO-M problem, at each time t, the learner is asked to output a decision zt∈Rd, to
which a scalar loss ft(zt−m+1:t), depending on the learner’s most recent m∈Ndecisions is revealed.
Given a convex compact set K ⊂Rdas the domain of comparators, the regret is measured on the
best single point z∈ K over a time horizon of T∈N, formally given by
RegretK
T= max
z∈KE"TX
t=mft(zt−m+1:t)−ft(z,···, z)#
.
In non-stochastic control, the learner’s past decisions affect future states through an affine structure,
therefore we are interested in BCO-M problems with such structure. This leads to the following
structural assumption on the induced loss function with memory. We will show in Lemma 9 that the
bandit control problems with the standard regularity conditions satisfy Assumption 5.
Assumption 5 (Affine memory structure) .At time t∈N, the loss function with memory length m
takes the form of ft: (Rd)m→R+given by the following structure
ft(zt−m+1:t) =ℓt 
Bt+m−1X
i=0G[i]Yt−izt−i!
, (3)
with parameters Bt∈Rn,Yt−i∈Rp×d, and G=G[0:m−1]a sequence of mmatrices where
G[i]∈Rn×pfor some n, p∈N.2We denote Gt=Pm−1
i=0G[i]Yt−i∈Rn×d, and Ht=
G⊤
tGt∈Rd×d. There exists some RH>0such that max{1,∥Gt∥2,∥Yt∥2,∥Ht∥2} ≤ RH.
In addition, we assume that Gsatisfies positive convolution invertibility-modulus, i.e. κ(G) =
infP
n≥0∥un∥2
2=1P
n≥0∥Pn
i=0G[i]un−i∥2
2= Ω(1) . We assume that the learner receives Htevery
step after they incurred the loss.
We make two standard curvature and regularity assumptions on each of the instantaneous loss ft, that
ftis strongly-convex and smooth, with its subgradient norm bounded by some constants, following
the previous work of Suggala et al. (2024).
Assumption 6 (Curvature) .Consider a loss function ftsatisfying Assumption 5 and the set
Zt=(
Bt+m−1X
i=0G[i]Yt−izt−i:zt−m+1, . . . , z t∈ K+Bd)
⊂Rn,
where Bt, G, Y t−m+1:tare the parameters, and ℓt:Rn→R+is the function associated with ftin
Assumption 5. We assume that ℓtisαf-strongly-convex and βf-smooth, i.e. αfIn⪯ ∇2ℓt(z)⪯βfIn,
overZt, with 0< αf≤1≤βfhere3.
Assumption 7 (Regularity) .Ford∈N, denote Bd:={x∈Rd| ∥x∥2≤1}as the unit ball in Rd.
We assume that at time t, the with-memory loss function ft: (Rd)m→R+with memory parameter
m∈Nobeys the following gradient bounds over K+Bd:={x+y|x∈ K, y∈Bd}:
∥∇ft(z1, . . . , z m)∥2≤Gf,∀z1, . . . , z m∈ K+Bd.
Additionally, we assume that Khas Euclidean diameter D > 0.
2Here we could write G[i]Yt−iwith a single parameter, but the current form matches that of control.
3Anyαstrongly convex function is also min(1 , α)strongly convex and similar for β.
5We consider an oblivious adversary model, given by Assumption 8.
Assumption 8 (Oblivious adversary) .For a BCO-M instance, we assume that the adversary is
oblivious. In particular, let ztdenote the algorithm’s decision at time t, then the loss function ft
chosen by the adversary does not depend on z1:t.
In the definition of regret in BCO-M, we compare the cumulative loss of any algorithm with the best
fixed comparator z∈ K, evaluated on the induced unary form ft(z, . . . , z )of the loss ft. Formally,
we have the following definition
Definition 2 (Induced unary form) .∀t∈N, let¯ft:Rd→R+denote the induced unary form of the
lossft, given by ¯ft(z) =ft(z, . . . , z ). Then, for ftsatisfying Assumption 5, ¯ftadmits the structure
¯ft(z) =ℓt
Bt+Pm−1
i=0G[i]Yt−iz
.
We note that for ftsatisfying Assumption 6, the induced unary form in Definition 2 satisfies a
special curvature called κ-convexity introduced in (Suggala et al., 2024). To avoid confusion with the
notations with the bound on the dynamics (Assumption 2), we will call it κ0-convexity henceforth.
Definition 3 (κ0-convexity, (Suggala et al., 2024)) .A function f:Rd→Ris called κ0-convex
over a domain K ⊆Rdif and only if the following holds: fis convex and twice continuously
differentiable, and moreover ∃c, C > 0and a PSD matrix 0⪯H⪯Is.t. the Hessian of fat any
z∈ K satisfies
cH⪯ ∇2f(z)⪯CH,C
c≤κ0.
The benefit that the loss function exhibits an affine memory dependence is that its induced unary
form satisfies κ0convexity for some κ0>0, summarized the following observation.
Observation 4 (κ0-convexity and gradient bound of the unary loss.) .Consider ft: (Rd)m→R+
satisfying Assumption 5, Assumption 6, and Assumption 7. Then, the induced unary form ¯ft:Rd→
R+in Definition 2 satisfies the following two properties:
1.(κ0-convexity) ¯ftisκ0-convex with κ0=βf/αf,H=Ht:αfHt⪯ ∇2¯ft(z)⪯βfHt,
where Ht=G⊤
tGtas in Assumption 5, ∀z∈Rd.
2. (Gradient bound) ∥∇¯ft(z)∥2≤Gf√m,∀z∈ Zt.
Proof of Observation 4. By our assumption on the affine structure of ftin Assumption 5, ∀z∈Rd,
∇2¯ft(z) =G⊤
t∇2ℓt(Bt+Gtz)Gt.κ0-convexity follows from the curvature assumption in Assump-
tion 6. For any z1, z2∈Rd, we have that |¯ft(z1)−¯ft(z2)| ≤Gf∥(z1,···, z1)−(z2,···, z2)∥2=
Gf√m∥z1−z2∥2.
2.3 Notations
For a positive semidefinite square matrix H⪰0∈Rd×d, define the norm ∥ · ∥ HonRdso that
∥v∥2
H=v⊤Hv. We write ctto denote the cost function of the control problem, and ftto denote the
loss function for BCO with memory. For two sets A, B ,A+B={a+b:a∈A, b∈B}. For a
setS,|S|denotes the cardinality of S. We use Bd,Sd−1to denote the unit ball and unit sphere in
Rd, respectively. For nvectors v1, . . . , v n∈Rd, we slightly abuse notation and shorthand as v1:nto
denote the concatenated vector (v1, . . . , v n)∈Rnd.
3 Improved Bandit Convex Optimization with Memory
In this section, we introduce an improved algorithm for the bandit convex optimization with memory
problem. Our algorithm incorporates the occasional update idea from (Cassel and Koren, 2020) and
the Newton-based update for κ0-convex functions from (Suggala et al., 2024), achieving an optimal
˜O(√
T)regret bound, improving the previously best known ˜O(T2/3)result from (Cassel and Koren,
2020). Besides the advancement on BCO-M, this result is the key component of our main result in
control. First, we define the BCO-M instance.
6Definition 5 (BCO-M instance) .Given d∈N, a BCO-M instance is parametrized by O=
{K, m,{ft}t≥m,t∈N}, where K ⊂ Rdis a convex compact set; mis the memory length; ft:
Km→R+measures the instantaneous loss at time t. Given any bandit online learning with memory
algorithm AB, the regret of ABonOw.r.t. z∈ K is given by
RegretAB,z
T(O) =TX
t=mft(zt−m+1:t)−¯ft(z),
where zt=zAB
tis the decision according to ABat time t. We say a BCO-M instance Ois affine
and(α, β, G, D )-well-conditioned ifOsatisfies Assumption 5, Assumption 6 with αf=α, βf=β,
Assumption 7 with Gf=G, D f=D, and the adversary model satisfies Assumption 8.
3.1 BCO-M algorithm
We describe Algorithm 1 that runs bandit convex optimization for any BCO-M instance O=
{K, m,{ft}t≥m,t∈N}. On a high level, our algorithm employs the occasional update idea from
(Cassel and Koren, 2020). The decisions are updated at most once every msteps, ensured by the
condition btQm−1
i=1(1−bt−i) = 1 (line 8 in Algorithm 1), where btis the Bernoulli random variable
(partially) deciding whether the algorithm will make an update at the current time step. We write oas
the original ideal prediction, vas the random perturbation vector, and zis the actual prediction (as
the perturbed o) of the algorithm.
Algorithm 1 Improved Bandit Convex Optimization with Affine Memory
Input: convex compact set K ⊂Rd, step size η >0, memory parameter m, curvature parameter α,
time horizon T.
1:Initialize: o1=···=om∈ K,˜g0:m−1=0d,ˆA0:m−1=I,τ= 1.
2:Sample vt∼Sd−1i.i.d. uniformly at random for t= 1, . . . , m .
3:Setzt=ot+ˆA−1
2
t−1vt,t= 1, . . . , m .
4:Draw bt∼Ber 1
m
,t= 1, . . . , m .
5:fort=m, . . . , T do
6: Playzt, observe ft(zt−m+1:t), receive Ht=G⊤
tGt.
7: Draw bt∼Ber 1
m
.
8: ifbtQm−1
i=1(1−bt−i) = 1 then
9: Letsτ=t.
10: Update ˆAt=ˆAt−1+ηα
2Ht.
11: Create gradient estimate: ˜gt=d ft(zt−m+1:t)ˆA1
2
t−1vt∈Rd.
12: Update ot+1=QˆAsτ−1
Kh
ot−ηˆA−1
sτ−1˜gsτ−1i
.
13: Sample vt+1∼Sd−1uniformly at random, independent of previous steps.
14: Setzt+1=ot+1+ˆA−1
2
tvt+1.
15: τ←τ+ 1.
16: else
17: Setot+1=ot,vt+1=vt,zt+1=zt,ˆAt=ˆAt−1,˜gt= ˜gt−1.
18: end if
19:end for
Such occasional update essentially reduces the BCO-M problem to a new no-memory BCO problem,
whose equivalence will be shown later (see Appendix A.2). Consistent with the notation used in
(Cassel and Koren, 2020), we denote the following set
S:={t∈[T] :zt+1̸=zt} (4)
to be the set of time steps where the algorithm updates its decision. We readily have |S| ≤T
m.
Moreover, we have
ft(zt−m+1:t) =¯ft(zt−m+1) =¯ft(zt),∀t∈S, (5)
7since t∈Simplies bt−m+1=···=bt−1= 0. Thus, whenever Algorithm 1 updates, it effectively
updates with function value ¯ft(zt).
The occasional update alone only gives a sub-optimal ˜O(T2/3)regret as shown in (Cassel and
Koren, 2020). To achieve the optimal ˜O(√
T)regret, we use a Newton-based update to exploit the
κ0-convexity of general strongly-convex smooth functions, recently introduced by (Suggala et al.,
2024).
For this improvement we require the knowledge of a Hessian estimator Htas in Assumption
Assumption 5. This doesn’t hold in BCO in general, but we will show in the next section that
the control problem indeed satisfies this assumption: thanks to κ0-convexity, in the control problem
Htcan be constructed from the knowledge of system, instead of knowledge of loss which would
typically incur a huge variance term.
The regret guarantee of Algorithm 1 is given by the following theorem.
Theorem 6 (BCO-M regret guarantee) .Given an (α, β, G, D )-well-conditioned BCO-M instance
O={K, m,{ft}t≥m,t∈N}withm= poly(log T)andG, D =˜O(1)(Definition 5), let (K, η=
Θ(1/√
T), m, α, T )be the input to Algorithm 1. Algorithm 1 guarantees that
max
z∈KEh
RegretAlgorithm 1,z
T (O)i
≤˜Oβ
αGD√
T
,
where ˜O(·)hides all universal constants and logarithmic dependence in T.
Theorem 6 is the first algorithm to achieve the optimal ˜O(√
T)regret bound for the BCO-M problem
with general smooth convex loss. Compared with the ˜O(T2/3)bound from (Cassel and Koren, 2020),
our improvement exploits the new assumptions on the strong convexity of loss and the affine structure
of memory. We notice that the ˜O(√
T)regret bound of (Suggala et al., 2024) requires the additional
quadratic loss assumption and uses the special structure of quadratic functions to construct low-biased
gradient estimators for the unary form of losses. Algorithm 1 and its guarantee in Theorem 6 thus
improve upon both of these previous results. The proof of Theorem 6 is lengthy, therefore we leave it
to the appendix and include a sketch here.
3.2 Proof Sketch
The theorem is proven via reduction to a no-memory BCO algorithm with Newton-based updates.
Step 1: regret of the base algorithm. We devise a new BCO algorithm with two new ingredients
different from standard algorithms. First, our algorithm adopts Newton-based updates which require
estimating Hessians. In Assumption 5 we assume "free" access to a Hessian estimator Ht, which
holds in the control setting by utilizing the κ0-convexity property.
Second, we incorporate a new delay mechanism to decorrelate neighboring iterates such that otis
independent of recent perturbation vectors, which plays a crucial role in bounding the expectation
of moving cost. Our base BCO algorithm is then shown to have an ˜O(√
T)regret bound (see
Lemma 11).
Step 2: reduction to the base algorithm. We show that the regret of Algorithm 1 can be controlled
by the regret of the base algorithm plus a moving cost term. By the design of Algorithm 1, it updates
a univariate loss function at most once every msteps. If all the ztacross these msteps are the same,
Regret (Algorithm 1) will be exactly the same as m×Regret (base algorithm). When ztare different,
we suffer an additional moving cost EhPT
t=mft(zt−m+1:t)−¯ft(zt−m+1)i
, which can be bounded
by the assumption on the gradient of ftifztchanges slowly.
Step 3: bounding the moving cost. We partition the moving cost ft(zt−m+1:t)−¯ft(zt−m+1)into
three terms
ft(zt−m+1:t)−ft(ot−m+1:t)| {z }
(a)+ft(ot−m+1:t)−¯ft(ot−m+1)| {z }
(b)+¯ft(ot−m+1)−¯ft(zt−m+1)| {z }
(c).
8Here, (a), (c) can be seen as perturbation loss suffered by the algorithm during exploration. (b) is the
moving cost determined by the stability of the algorithm’s neighboring iterates.
For the three terms in the above equation, (c) is bounded by Jensen’s inequality using the convexity
of the unary form of loss. We bound (a) using the curvature assumptions and the affine memory
structure of ft, where we also make use of the delayed updates to decorrelate neighboring iterates for
technical reasons. (b) is bounded by the Lipschitzness of ftand the distance between neighboring
iterates. Theorem 6 is reached by putting the three steps together.
4 Optimal Regret for Bandit Non-stochastic Control
In this section, we show how to achieve optimal regret for the bandit non-stochastic control problem
with a partially observable LTI dynamical system as described in Eq. (1) for strongly-convex smooth
loss, by a reduction to the BCO-M algorithm (Algorithm 1) we devise in the previous section.
Previously, the best known regret bound for this problem was ˜O(T2/3)from (Cassel and Koren,
2020), which is also rooted in a reduction to BCO-M. We use a similar reduction, obtaining a
better bound thanks to the improved regret bound of BCO-M (Theorem 6). We first give the formal
definition of the control problem.
Definition 7 (Bandit non-stochastic control) .A bandit non-stochastic control problem of a partially
observable LDS is parametrized by a tuple L= (A, B, C, x 1,(wt)t∈N,(et)t∈N,(ct)t∈N,Π), where
A, B, C and(wt)t∈N,(et)t∈Nare the dynamics and perturbations in the LDS (Eq. (1)) with initial
statex1(we assume x1=0 without loss of generality henceforth); ctmeasures the instantaneous cost
at time t;Πis the comparator control policy class. Given any bandit non-stochastic control algorithm
A, the regret of ANConLover a time horizon T∈Nis given by
RegretANC
T(L,Π) =TX
t=1ct(yANC
t, uANC
t)−min
π∈ΠTX
t=1ct(yπ
t, uπ
t),
where (yANC
t, uANC
t),(yπ
t, uπ
t)are the observation, control pair at time tfollowing the trajec-
tory of ANCandπ, respectively. We say that a bandit non-stochastic control problem Lis
(α, β, G, κ sys, κ, γ, R w,e,Y,U)-well-conditioned ifLandΠsatisfy Assumption 1 with αc=
α, βc=β, G c=Gover some centered bounded convex domain Y × U ⊂ Rdy+du, Assump-
tion 2 with κsys, κ, γ , Assumption 3 with Rw,e, and Assumption 4.
The reduction from partially observable bandit non-stochastic control to BCO-M consists of two main
steps. The first step is to construct the would-be signal yt(K)from the observation yt, where yt(K)is
used for updating but not directly observed by the controller. yt(K)is computed by using the Markov
operator of the LDS (Simchowitz et al., 2020). The second step is a standard black-box reduction
from control to BCO-M, which uses the strong stability of the system. Reduction is formalized in the
following definition.
Definition 8 (Approximation) .A bandit non-stochastic control instance (Definition 7) Lwith some
convex comparator class Πover a time horizon T∈Nis said to be ε-approximated (ε >0) by a
BCO-M instance (Definition 5) Owith domain K= Π, if the existence of a BCO-M algorithm AB
implies the existence of a bandit non-stochastic control algorithm ANCsatisfying
Eh
RegretANC,K
T (L,Π)i
≤Eh
RegretAB,K
T(O)i
+εT.
The formal guarantee of the reduction is given below.
Lemma 9 (Control reduction) .Every instance of (αc, βc, Gc, κsys, κ, γ, R w,e,Y,U)-well-
conditioned bandit non-stochastic control problem Lover the ball Y × U ⊂ Rdy+duof radius
R4with comparator class Π =M(m, RM)(Definition 1) is 2GfDmT−1-approximated by a
(αf, βf, Gf, D)-well-conditioned BCO-M instance Owith
αf=αc, βf=βc, D =q
mmax{du, dy}RM, G f=4096√mGcR2
w,eR2
Md2.5
xκ3κ8
sys
γ5,
provided that m= Θ(log T/log(1/(1−γ))).
4See Appendix B, R=Rw,e
1 +√dxκsys
γ√
1 +κ2+RM
1 +√
dx(1+κ2)κ2
sys
γ
=O(1).
9Lemma 9 implies that every well-conditioned bandit non-stochastic control problem can be reduced
to a well-conditioned BCO-M instance, whose parameters are polynomial in those of the control
problem. As a result, any regret bound for BCO-M will directly transfer to the control problem (at the
expense of polynomial dependence on the system parameters). In particular, combining Theorem 6
and Lemma 9, we obtain an optimal ˜O(√
T)regret bound for the bandit non-stochastic control with
strongly-convex smooth cost.
Theorem 10 (Bandit non-stochastic control regret guarantee) .Given an
(α, β, G, κ sys, κ, γ, R w,e,Y,U)-well-conditioned bandit non-stochastic control instance Lover the
ballRBdy+du⊂Rdy+dufor the same Ras in Lemma 9, with G, κ sys, κ, γ, R w,e, dx, dy, du=˜O(1)5,
let(Π,K, η, m, T, G, K, α )be the input to Algorithm 2 with m= poly(log T). Algorithm 2
guarantees that
max
z∈KEh
RegretAlgorithm 2,z
T (O)i
≤˜Oβ
α√
T
,
where ˜O(·)hides all universal constants and logarithmic dependence in T.
Theorem 10 strictly improves Theorem 8 of (Cassel and Koren, 2020) and Theorem 5 of (Suggala
et al., 2024), in the sense that our result achieves the optimal regret with fewer assumptions: all
three results achieve the same ˜O
β
α√
T
regret bound, however (Cassel and Koren, 2020) requires
the additional assumption that perturbations are stochastic, while (Suggala et al., 2024) requires the
additional assumption that costs are quadratic.
Algorithm 2 Improved Bandit Non-stochastic Control
Input: Step size η >0, memory parameter m, DRC policy class M(m, RM), time horizon T,
system dynamics, (κ, γ)-strongly stabilizing linear policy K, strong convexity parameter α >0.
1:LetABbe an instance of Algorithm 1 with inputs (K=M(m, RM), η, m, α, T ).
2:Initialize: M[j]
1=···=M[j]
m= 0du×dy,∀j∈[m].˜g0=···= ˜gm−1= 0mdudy,ˆA0=
ˆAm−1=mImdudy×mdudy.
3:Initialize ABfort= 1,···, m−1(lines 1-4 in Algorithm 1).
4:Sample ξt∼Smdudy−1i.i.d. uniformly at random for t= 1, . . . , m .
5:Play control ut=Kyt, incur cost ct(yt, ut)fort∈[m].
6:fort=m, . . . , T do
7: Play control uMt
t=Kyt+Pm−1
j=0M[j]
tyt−j(K), incur cost ct(yt, ut).
8: Letft: (M(m, RM))m→Rbe the induced with-memory loss function via reduction in
Lemma 9 and Htbe the associated Hessian estimator in Assumption 5.
9: Update Mt+1← AB(Mt,{fs}t
s=m,{Hs}t
s=m).
10:end for
5 Conclusion
In this paper, we devise an algorithm with an ˜O(√
T)regret bound for the bandit non-stochastic
control problem with adversarial strongly-convex smooth cost functions. This is the first result with
optimal regret that simultaneously breaks the three assumptions of LQC (1) stochastic perturbation
(2) full-information feedback (3) quadratic cost. Our control algorithm is built upon an improved
algorithm for BCO-M, which may be of independent interest.
As a preliminary step to address the question of a general control theory for LTI, our result comes
with limitations and potential future research directions. Currently, the improvement over the ˜O(T2/3
regret by (Cassel and Koren, 2020) is made under the additional assumption of strong convexity. It’s
unclear whether this assumption is necessary for an ˜O(√
T)regret, we leave determining the minimal
assumption on the cost functions for optimal regret as an open question.
5The assumption that the system parameters are of order ˜O(1)is consistent with prior works (Hazan and
Singh, 2022).
10References
Agarwal, N., Bullins, B., Hazan, E., Kakade, S., and Singh, K. (2019a). Online control with
adversarial disturbances. In International Conference on Machine Learning , pages 111–119.
PMLR.
Agarwal, N., Hazan, E., and Singh, K. (2019b). Logarithmic regret for online control. Advances in
Neural Information Processing Systems , 32.
Anava, O., Hazan, E., and Mannor, S. (2015). Online learning for adversaries with memory: price of
past mistakes. Advances in Neural Information Processing Systems , 28.
Bellman, R. (1954). The theory of dynamic programming. Bulletin of the American Mathematical
Society , 60(6):503–515.
Cassel, A. and Koren, T. (2020). Bandit linear control. Advances in Neural Information Processing
Systems , 33:8872–8882.
Gradu, P., Hallman, J., and Hazan, E. (2020). Non-stochastic control with bandit feedback. Advances
in Neural Information Processing Systems , 33:10764–10774.
Hazan, E., Agarwal, A., and Kale, S. (2007). Logarithmic regret algorithms for online convex
optimization. Machine Learning , 69(2):169–192.
Hazan, E. and Singh, K. (2022). Introduction to online non-stochastic control. arXiv preprint
arXiv:2211.09619 .
Kalman, R. E. (1960). A new approach to linear filtering and prediction problems.
Kalman, R. E. et al. (1960). Contributions to the theory of optimal control. Bol. soc. mat. mexicana ,
5(2):102–119.
Lattimore, T. (2024). Bandit convex optimisation. arXiv preprint arXiv:2402.06535 .
Lattimore, T. and György, A. (2023). A second-order method for stochastic bandit convex optimisa-
tion. In The Thirty Sixth Annual Conference on Learning Theory , pages 2067–2094. PMLR.
Simchowitz, M. (2020). Making non-stochastic control (almost) as easy as stochastic. Advances in
Neural Information Processing Systems , 33:18318–18329.
Simchowitz, M., Singh, K., and Hazan, E. (2020). Improper learning for non-stochastic control. In
Conference on Learning Theory , pages 3320–3436. PMLR.
Suggala, A., Sun, Y . J., Netrapalli, P., and Hazan, E. (2024). Second order methods for bandit
optimization and control. arXiv preprint arXiv:2402.08929 .
Suggala, A. S., Ravikumar, P., and Netrapalli, P. (2021). Efficient bandit convex optimization: Beyond
linear losses. In Conference on Learning Theory , pages 4008–4067. PMLR.
Sun, Y . J., Newman, S., and Hazan, E. (2024). Optimal rates for bandit non-stochastic control.
Advances in Neural Information Processing Systems , 36.
Yan, Y .-H., Wang, J., and Zhao, P. (2024). Handling heterogeneous curvatures in bandit lqr control.
InForty-first International Conference on Machine Learning .
11Contents
1 Introduction 1
1.1 Technical Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2
1.2 Related Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3
2 Preliminary 4
2.1 Online non-stochastic control with bandit feedback . . . . . . . . . . . . . . . . . 4
2.2 Bandit convex optimization with memory (BCO-M) . . . . . . . . . . . . . . . . . 5
2.3 Notations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6
3 Improved Bandit Convex Optimization with Memory 6
3.1 BCO-M algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7
3.2 Proof Sketch . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8
4 Optimal Regret for Bandit Non-stochastic Control 9
5 Conclusion 10
A Proof of Theorem 6 13
A.1 Base No-Memory BCO Algorithm and Guarantees . . . . . . . . . . . . . . . . . 13
A.2 Reduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15
A.3 Bounding Moving Cost . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16
A.4 Proof of Theorem 6 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20
B Proof of Lemma 9 21
12A Proof of Theorem 6
The proof is reduction-based. It consists of three parts
1.We first consider an algorithm for the no-memory BCO problem, which uses Newton-based
updates and a novel delay mechanism. This algorithm will serve as the base algorithm.
2.We then show that the regret of our main algorithm 1 can be bounded by the regret of the
base algorithm plus a moving cost, via an improved analysis on κ0-convexity.
3. Finally, we bound the moving cost and then put every pieces together.
A.1 Base No-Memory BCO Algorithm and Guarantees
In this section, we prove Lemma 11, which establishes the regret guarantee for the base algorithm
used by Algorithm 1. The base no-memory algorithm is a variant of the Newton-based (improper)
BCO algorithm considered in Algorithm 1 of (Suggala et al., 2024). The main differences are:
1.Algorithm 1 in (Suggala et al., 2024) constructs Hessian estimators from the bandit feedback.
Algorithm 3 doesn’t require Hessian estimators, since the Htdetermining κ0-convexity is
given by the system instead of loss in bandit control problems, and thus it’s computable by
the learner given the system parameters.
2.Algorithm 3 introduces an additional delay mechanism (specified by the delay parameter
d0∈N) to decorrelate neighboring iterates, a necessary ingredient to later analysis.
Algorithm 3 Simple BCO-with-delay
Input: convex compact set K ⊂Rd, step size η >0, delay parameter d0∈N, curvature parameter
α >0, time horizon T∈N.
1:Initialize: o1∈ K,ˆA0=Id×d.ˆAt= 0d×d,∀t <0.˜gt= 0,∀t≤0.
2:Sample v1∼Sd−1uniformly at random. Set z1=o1+ˆA−1
2
0v1.
3:fort= 1, . . . , T do
4: Playzt, observe ¯ft(zt), receive Ht.
5: Update ˆAt=ˆAt−1+ηα
2Ht.
6: Create gradient estimate: ˜gt=d¯ft(zt)ˆA1
2
t−1vt∈Rd.
7: Update ot+1=QˆAt−d0+1
Kh
ot−ηˆA−1
t−d0+1˜gt−d0+1i
.
8: Sample vt+1∼Sd−1uniformly at random, independent of previous steps.
9: Setzt+1=ot+1+ˆA−1
2
tvt+1.
10:end for
Lemma 11 (Base BCO regret guarantee) .Suppose that the sequence of loss functions {¯ft}T
t=1and
the convex compact set Ksatisfy the conditions in Assumption 7, Assumption 8, and the properties in
Observation 4, and max{1,∥Ht∥2} ≤RH,∀t. With α=αfin the first condition in Observation 4,
(η, d0)satisfying d0≤2/(ηαR H), Algorithm 3 run with inputs (K, η, d 0, α, T )satisfies the following
regret guarantee: ∀o∈ K,
E"TX
t=1¯ft(zt)−¯ft(o)#
≤2βd
ηαlog(ηRHT+ 1) + 2 d0GD+D2d0RH
2η+ 3ηd0d2G2D2RHT.
In particular, by choosing η= Θ(1 /√
T)andd0=˜Θ(1) , the above regret is of order ˜O
β
α√
T
.
Proof of Lemma 11. First, note that by the condition on ηandd0, we have that ˆAt⪯2ˆAt−i,∀i≤d0.
To see this, it is equivalent to proving any PSD matrix Hwithλmax(H)≤1satisfies that H⪯I,
which follows from the fact that x⊤(I−H)x≥ ∥x∥2
2−λmax∥x∥2
2≥0,∀x.
13By the convexity and curvature assumption on ¯ftdescribed by the conditions in Observation 4, we
have
E"TX
t=1¯ft(zt)−¯ft(ot)#
≤TX
t=1E[∇¯ft(ot)⊤ˆA−1
2
t−1vt] +β
2TX
t=1E[v⊤
tˆA−1
2
t−1HtˆA−1
2
t−1vt],
where the first-order term equals 0since vtis drawn independently of ot,¯ft,ˆAt−1, andE[vt] = 0 .
We can further bound the second order term by
E"TX
t=1¯ft(zt)−¯ft(ot)#
≤β
2TX
t=1E[ˆA−1
t−1·Ht]≤2β
ηαfTX
t=1E[ˆA−1
t·(ˆAt−ˆAt−1)],
where the first step follows from E[v⊤Av|A] =A·E[vv⊤]and the cyclic property of trace, and the
last step follows from the definition of ˆAt, the stability condition that ˆAt⪯2ˆAt−1, and α=αf.
Using standard inequalities on log determinant in Newton step analysis (Hazan et al., 2007), we have
E"TX
t=1¯ft(zt)−¯ft(ot)#
≤2β
ηαflogdet(ˆAT)
det(ˆA0)≤2βd
ηαflogηαfRHT
2+ 1
.
Moreover, the curvature assumption on ¯ftalso implies
E"TX
t=1¯ft(ot)−¯ft(o)#
≤TX
t=1E[∇¯ft(ot)⊤(ot−o)]−αf
2TX
t=1E[∥ot−o∥2
Ht]
=TX
t=1E[˜g⊤
t(ot−o)]−αf
2TX
t=1E[∥ot−o∥2
Ht], (6)
where the second step follows from E[˜gt|ot] =∇¯ft(ot)by Stoke’s theorem. By the projection step
in Line 7 of Algorithm 3, we have
∥ot−o∥2
ˆAt−d0
≤ ∥ot−1−o−ηˆA−1
t−d0˜gt−d0∥2
ˆAt−d0
=∥ot−1−o∥2
ˆAt−d0−1+1
2∥ot−1−o∥2
ηαfHt−d0−2η˜g⊤
t−d0(ot−1−o) +η2∥˜gt−d0∥2
ˆA−1
t−d0
≤ ∥ot−1−o∥2
ˆAt−d0−1+∥ot−d0−o∥2
ηαfHt−d0+∥ot−d0−ot−1∥2
ηαfHt−d0−2η˜g⊤
t−d0(ot−d0−o)
+ 2η˜g⊤
t−d0(ot−d0−ot−1) +η2∥˜gt−d0∥2
ˆA−1
t−d0,
where the last inequality follows from that ∀H⪰0,∥x+y∥2
H≤2(∥x∥2
H+∥y∥2
H).
Rearranging, we have
˜g⊤
t−d0(ot−d0−o)−αf
2∥ot−d0−o∥2
Ht−d0
≤∥ot−1−o∥2
ˆAt−d0−1− ∥ot−o∥2
ˆAt−d0
2η+αf
2∥ot−d0−ot−1∥2
Ht−d0+ ˜g⊤
t−d0(ot−d0−ot−1) +η
2∥˜gt−d0∥2
ˆA−1
t−d0.
Note that ∀t,
∥˜gt∥2
ˆA−1
t≤(dGD )2v⊤
tˆA1
2
t−1ˆA−1
tˆA1
2
t−1vt≤(dGD )2,
∥ot−ot−1∥2≤ ∥ot−ot−1∥ˆAt−d0≤η∥˜gt−d0∥ˆA−1
t−d0≤ηdGD,
where the second inequality in the second bound follows from the update rule in Line 7 of Algorithm 3,
which gives ∥ot−ot−1∥2
ˆAt−d0≤ ∥ηˆA−1
t−d0˜gt−d0∥2
ˆAt−d0=η2∥˜gt−d0∥2
ˆA−1
t−d0. Thus, we can further
bound
αf
2∥ot−d0−ot−1∥2
Ht−d0≤αfRHd0
2d0X
i=0∥ot−d0+i−ot−d0+i+1∥2
2≤η2αfd0d2G2D2RH
2,
˜g⊤
t−d0(ot−d0−ot−1)≤ ∥˜gt−d0∥ˆA−1
t−d0∥ot−d0−ot−1∥ˆAt−d0≤2ηd0d2G2D2.
14Combining both and using that ˆAt⪯2ˆAt−i,∀i≤d0, we have that
T−d0X
t=d0˜g⊤
t(ot−o)−αf
2∥ot−o∥2
Ht
=TX
t=2d0˜g⊤
t−d0(ot−d0−o)−αf
2∥ot−d0−o∥2
Ht−d0
≤1
2η∥o2d0−1−o∥2
ˆAd0−1+ηTαfd0d2G2D2RH
2+ 2d0d2G2D2+d2G2D2
2
≤D2d0RH
2η+ 3ηd0d2G2D2RHT.
As a result,
E"TX
t=1¯ft(ot)−¯ft(o)#
≤2d0GD+D2d0RH
2η+ 3ηd0d2G2D2RHT.
Combining, we have
E"TX
t=1¯ft(zt)−¯ft(o)#
≤2βd
ηαlog(ηRHT+ 1) + 2 d0GD+D2d0RH
2η+ 3ηd0d2G2D2RHT.
A.2 Reduction
In this section, we prove Lemma 12, which states that the regret of BCO-M algorithm (Algorithm 1)
can be related to the regret guarantee of the base no-memory BCO algorithm (Algorithm 3).
We follow the proof idea of (Cassel and Koren, 2020). In the previous section we have proved an
˜O(√
T)regret bound for a “no-memory” base BCO algorithm. Our main Algorithm 1, however, has
memory dependence. When ztis changing slowly, Algorithm 1 is approximately a “no-memory”
algorithm, and we can do the following reduction: if all the ztacross these steps are the same,
Regret (Algorithm 1) will be exactly the same as m×Regret (base algorithm).
In this part, we show that when ztare different, we only suffer an additional moving cost
E"TX
t=mft(zt−m+1:t)−¯ft(zt−m+1)#
,
which we will bound in Appendix A.3.
Lemma 12 (Lemma 11, (Cassel and Koren, 2020)) .Let(K, η, m, T )be the input for Algorithm 1.
Suppose that the loss functions {ft}T
t=mand the convex compact set Ksatisfy Assumption 5, Assump-
tion 6, Assumption 7, and Assumption 8. Then, the regret of Algorithm 1 with respect to any o∈ K is
upper bounded by
E"TX
t=mft(zt−m+1:t)−¯ft(o)#
≤3m·RAlgorithm 3T
m
+E"TX
t=mft(zt−m+1:t)−¯ft(zt−m+1)#
,
where RAlgorithm 3 T
m
is the regret upper bound obtained in Lemma 11 with time horizonT
mand
d0= 2.
Proof of Lemma 12. First, note that by assumption, the conditions in Lemma 11 are satisfied for the
induced unary forms {¯ft}T
t=m. The argument follows by reducing Algorithm 1 to Algorithm 3.
Denote χt=btQm−1
i=1(1−bt−i)as the indicator of whether the algorithm gets updated during round
t. Recall the definition of S={t∈[T] :χt= 1}in Eq. (4). The algorithm updates during round tif
t∈S. For any t∈S, we have that χt−1=···=χt−m+1= 0. Therefore, we have zt−m+1=···=
15ztby design of Algorithm 1. Thus, we have that ft(zt−m+1:t) =¯ft(zt−m+1) =¯ft(zt). Therefore,
constrained to the time steps t∈S, Algorithm 1 is essentially running Algorithm 3 with a delay
parameter d0= 2(since in Line 12 of Algorithm 1, we update with the gradient information at time
sτ−1during round t). Note that since |S| ≤T
m(Algorithm 1 updates at most once every msteps),
we have that ∀o∈ K,
Eb1:T,{vt}t∈S"X
t∈S¯ft(zt)−X
t∈S¯ft(o)#
≤Eb1:T[RAlgorithm 3(|S|)]≤RAlgorithm 3T
m
.
It is left to relate the quantity on the left hand side of the above expression to the regret of Algorithm 1.
Since χtD=χm(the two random variables equal in distribution), ∀t, we have E[χt] =E[χm]. For
any fixed o∈ K, we have
Eb1:T,v1:T"X
t∈S¯ft(o)#
=Eb1:T,v1:T"TX
t=1¯ft(o)·χt#
=E[χm]·E"TX
t=m¯ft(o)#
. (7)
Fort∈N, denote Ftto be the σ-algebra generated by the randomness of Algorithm 1 through
sampling bs, vsup to time t, i.e.Ft=σ({bs, vs}t
s=1). Since zt=zt−m+1whenever χt= 1,
bt−m+1:tare drawn independently of zt−m+1andχtis independent of Ft−mby definition of χt, we
have
Eb1:T,v1:T"X
t∈S¯ft(zt)#
=Eb1:T,v1:T"TX
t=m¯ft(zt−m+1)·χt#
=Eb1:T,v1:T"TX
t=m¯ft(zt−m+1)·χt| Ft−m, vt−m+1#
=E[χm]·E"TX
t=m¯ft(zt−m+1)#
. (8)
Together, Eq. (7) and Eq. (8) give that
Eb1:T,{vt}t∈S"X
t∈S¯ft(zt)−X
t∈S¯ft(o)#
=E[χm]·E"TX
t=m¯ft(zt−m+1)−¯ft(o)#
≤E[χm]·RAlgorithm 3T
m
.
Therefore, we have that
E"TX
t=mft(zt−m+1:t)−¯ft(o)#
=E"TX
t=m¯ft(zt−m+1)−¯ft(o)#
+E"TX
t=mft(zt−m+1:t)−¯ft(zt−m+1)#
≤(E[χm])−1RAlgorithm 3T
m
+E"TX
t=mft(zt−m+1:t)−¯ft(zt−m+1)#
≤3m·RAlgorithm 3T
m
+E"TX
t=mft(zt−m+1:t)−¯ft(zt−m+1)#
,
where the last inequality follows from
E[χm] =E[bm]m−1Y
i=1E[1−bi] =1
m
1−1
mm−1
≥1
em>1
3m.
A.3 Bounding Moving Cost
Lemma 11 and Lemma 12 almost give the regret guarantee in Theorem 6. We are left with bounding
the moving cost term in Lemma 12
E"TX
t=mft(zt−m+1:t)−¯ft(zt−m+1)#
.
16(Cassel and Koren, 2020) bounded each of the summands by E[δt+νt], where δt=∥ot+1−ot∥2,
andνt=∥zt−ot∥2. Making use of the κ0-convexity induced by the affine memory structure of
non-stochastic control problems, we can establish a tighter bound that is necessary to obtain optimal
regret in Theorem 6.
Lemma 13 (Moving cost) .Let(K, η, m, T )be the input for Algorithm 1. Suppose that the loss
functions {ft}T
t=mand the convex compact set Ksatisfy Assumption 5, Assumption 6, Assumption 7,
and Assumption 8. Suppose m≤2/(ηαR H). Then, the iterates output by Algorithm 1 satisfy
E"TX
t=mft(zt−m+1:t)−¯ft(zt−m+1)#
≤12m4βRHd·max{2, ηαR H√
T}
ηακ(G)log(ηαR H+ 1) +10m4βR3
Hd√
T
κ(G)
+m2βdR3
H√
T+ηdGD2βT.
In particular, with η= Θ(√
T),m= poly(log T)andκ(G) = Ω(1) by Assumption 5, we have that
the above bound is of order ˜O(√
T).
Proof of Lemma 13. We can decompose the moving cost into three terms:
E"TX
t=mft(zt−m+1:t)−ft(ot−m+1:t)#
| {z }
(a)+E"TX
t=mft(ot−m+1:t)−¯ft(ot−m+1)#
| {z }
(b)+E"TX
t=m¯ft(ot−m+1)−¯ft(zt−m+1)#
| {z }
(c),
and we will bound each of the terms separately. In particular, (a),(c)can be seen as perturbation loss
suffered by the algorithm during exploration. (b)is the moving cost determined by the stability of the
algorithm’s neighboring iterates. We start with establishing bounds on (a),(c).
Perturbation loss. As before, we denote Ft=σ({bs, vs}t
s=1)to be the σ-algebra generated by the
randomness of Algorithm 1 through sampling bs, vsup to time t. First, (c)≤0by Jensen’s inequality
for conditional expectations. In particular, recall that χt=btΠm−1
i=1(1−bt−i)denotes whether the
decision is updated at time t. For every t∈N, denote as T(t) := max {s < t|χs= 1}the last time
that the algorithm updates its decision. It naturally holds that ot=oT(t)+1,vt=vT(t)+1by design
of Algorithm 1. Therefore, we have
E[¯ft(ot−m+1)−¯ft(zt−m+1)] =E[¯ft(oT(t−m+1)+1 )−¯ft(zT(t−m+1)+1 )]
=E[¯ft(oT(t−m+1)+1 )−¯ft(oT(t−m+1)+ˆA−1
2
t−mvT(t−m+1)+1 )].
By the sampling rule in Line 13 of Algorithm 1, if the algorithm updates its decision at time t
(χt= 1), then vt+1is independent drawn from previous steps. On the other hand, ot+1is measurable
w.r.t.Ft. Therefore, we have by Jensen’s inequality for conditional expectations that
E[¯ft(oT(t−m+1)+1 )−¯ft(oT(t−m+1)+1 +ˆA−1
2
t−mvT(t−m+1)+1 )]
=E[¯ft(oT(t−m+1)+1 )−E[¯ft(oT(t−m+1)+1 +ˆA−1
2
t−mvT(t−m+1)+1 )| FT(t−m+1)]]
≤E[¯ft(oT(t−m+1)+1 )−¯ft(E[oT(t−m+1)+1 +ˆA−1
2
t−mvT(t−m+1)+1 | FT(t−m+1)])]
=E[¯ft(oT(t−m+1))−¯ft(oT(t−m+1))]
= 0.
Summing up over t, we get that (c)≤0. We move on to bound (a). We start with a (conditional)
independence argument.
Independence argument. Fixt. Denote t1=T(t)to be the most recent time when the algorithm
makes an update. Additionally, denote t2=T(T(t))to be the second most recent time when the
algorithm makes an update. We already have ot=ot1+1. By the delayed update rule in Line 12
of Algorithm 1, we have that ot1+1is updated with ot1=ot2+1and gradient information ˜gt2and
thus is Ft2-measurable. On the other hand, consider the sequence of random vectors vt−m+1, . . . , v t.
We have that vs=vT(s)+1for every t−m+ 1≤s≤t. Since the algorithm updates at most once
every msteps, we have that T(s)≥t2for all t−m+ 1≤s≤t. Therefore, vt−m+1, . . . , v tis
independent of Ft2. This observation de-correlates otwithvt−m+1, . . . , v tconditioning on Ft2.
17For notation simplicity, for matrices A1, . . . , A n∈Rd×dand vectors v1, . . . , v n∈Rd, we slightly
abuse notation and denote as A1:nv1:n= (A1v1, . . . , A nvn)∈Rndto be the concatenated matrix-
vector product of the two sequences.
We apply Taylor’s theorem to the function ftand obtain that
(a) =TX
t=mE
∇ft(ot−m+1:t)⊤ˆA−1
2
t−m:t−1vt−m+1:t+1
2v⊤
t−m+1:tˆA−1
2
t−m:t−1∇2ft(qt−m+1:t)ˆA−1
2
t−m:t−1vt−m+1:t
,
where qt−m+1:tis some point that lies on the line segment connecting ot−m+1:tandzt−m+1:t. By
the conditional independence argument, we have that the first order term vanishes: let t2=T(T(t)),
Eh
∇ft(ot−m+1:t)⊤ˆA−1
2
t−m:t−1vt−m+1:ti
=Eh
Eh
∇ft(ot−m+1:t)⊤ˆA−1
2
t−m:t−1vt−m+1:t| Ft2ii
=Eh
∇ft(ot−m+1:t)⊤ˆA−1
2
t−m:t−1E[vt−m+1:t| Ft2]i
= 0.
The above sum thus reduces to the sum of second-order terms:
1
2TX
t=mEh
v⊤
t−m+1:tˆA−1
2
t−m:t−1∇2ft(qt−m+1:t)ˆA−1
2
t−m:t−1vt−m+1:ti
≤1
2TX
t=mEh
(ˆA−1
2
t−mvt−m+1, . . . , ˆA−1
2
t−mvt)⊤∇2ft(qt−m+1:t)(ˆA−1
2
t−mvt−m+1, . . . , ˆA−1
2
t−mvt)i
,
≤1
2TX
t=mE
 max
u∈Rdm:∥u∥2
2=mu⊤
ˆA−1
2
t−m 0 . . . 0
0 ˆA−1
2
t−m. . . 0
0 0 . . . ˆA−1
2
t−m
∇2ft(qt−m+1:t)
ˆA−1
2
t−m 0 . . . 0
0 ˆA−1
2
t−m. . . 0
0 0 . . . ˆA−1
2
t−m
u
,
≤m2
2TX
t=mE
∇2ft(qt−m+1:t)·
ˆA−1
t−m 0 . . . 0
0 ˆA−1
t−m. . . 0
0 0 . . . ˆA−1
t−m

,
=m2
2TX
t=mE"
ˆA−1
t−m·mX
i=1[∇2ft(qt−m+1:t)]ii#
,
where the first inequality follows from ˆAs⪯ˆAtfor all s < t ; the second inequality follows from
taking maximum over all concatenated unit vectors in Rd; the third inequality follows from the
inequality between trace and spectral norm.
From this point on, the bound follows almost identically to the proof of Proposition 21 in (Suggala
et al., 2024). We will reiterate the proof in a concise manner for completeness.
Note that by the assumption of affine memory structure (Assumption 5), we can write the following
expression for the Hessian matrix of ftevaluated at qt−m+1:tas the following:
∇2ft(qt−m+1:t) =
W⊤
t−m+1∇2ℓt(q)Wt−m+1. . . W⊤
t−m+1∇2ℓt(q)Wt
. . . . . . . . .
W⊤
t∇2ℓt(q)Wt−m+1 . . . W⊤
t∇2ℓt(q)Wt
,
where q=Bt+Pm−1
i=0G[i]Yt−iqt−i, and Wt−m+i=G[m−i]Yt−m+i,∀1≤i≤m. By the
curvature assumption on ℓt(Assumption 6), we can further bound the sum of diagonal blocks by
mX
i=1[∇2ft(qt−m+1:t)]ii⪯βmX
i=1W⊤
t−m+iWt−m+i
=βmX
i=1Y⊤
t−m+i(G[m−i])⊤G[m−i]Yt−m+i
⪯βRHmX
i=1Y⊤
t−m+iYt−m+i.
18Therefore, we can further bound
m2
2TX
t=mE"
ˆA−1
t−m·mX
i=1[∇2ft(qt−m+1:t)]ii#
≤m2βRH
2TX
t=mE"
ˆA−1
t−m· tX
s=t−m+1Y⊤
sYs!#
≤m2βRHTX
t=mE"
ˆA−1
t· tX
s=t−m+1Y⊤
sYs!#
.
Consider γ=⌊√
T⌋and endpoints kj=γ(j−1) +mforj= 1, . . . , J , and J=⌊T−m
γ⌋. Using
the fact that Trace (AC)≤Trace (BC)for any PSD matrices A, B, C withA⪯B, we can further
decompose and bound the sum in the above expression by
m2βRHTX
t=mˆA−1
t· tX
s=t−m+1Y⊤
sYs!
≤m2βRHJX
j=1ˆA−1
kj·
kj+1−1X
t=kjtX
s=t−m+1Y⊤
sYs
+m2βγdR3
H
≤m3βRHJX
j=1ˆA−1
kj·
kj+1−1X
t=kj−m+1Y⊤
tYt
+m2βγdR3
H
≤2m3βRH
κ(G)JX
j=1ˆA−1
kj·
5mR2
HI+kj+1−1X
t=kjHt
+m2βγdR3
H
=2m3βRH
κ(G)JX
j=1ˆA−1
kj·
kj+1−1X
t=kjHt
+10m4βR3
Hd√
T
κ(G)+m2βdR3
H√
T.
where the first inequality follows by applying the radius bounds on Ytfor the last T−Jγterms
and using the fact that ˆAs⪯ˆAtfor any s < t ; the last inequality follows by applying Proposition
4.8 in (Simchowitz, 2020). Therefore, it suffices to bound the sum in the first term in expectation.
First, recall that ∀t,χt=btΠm−1
i=1(1−bi)denotes whether Algorithm 1 updates during round t, and
S={m≤t≤T:χt= 1}denotes the set of all rounds where Algorithm 1 updates. To bound the
following, we make use of the facts that (1) Htis oblivious, and (2) χtis independent of ˆA−1
kfor any
k≤t−m. Then, we have that
E
JX
j=1X
t∈[kj,kj+1−1]∩SˆA−1
kj−m·Ht
=E
JX
j=1kj+1−1X
t=kjˆA−1
kj−m·Ht·χt

=JX
j=1kj+1−1X
t=kjE[ˆA−1
kj−m·Ht]·E[χt]
=E[χm]·E
JX
j=1kj+1−1X
t=kjˆA−1
kj−m·Ht

≥E[χm]·E
JX
j=1kj+1−1X
t=kjˆA−1
kj·Ht
.
19Using this and that ˆAt⪯2ˆAt−m, we have that
E
JX
j=1kj+1−1X
t=kjˆA−1
kj·Ht
≤2E[χm]−1E
JX
j=1X
t∈[kj,kj+1−1]∩SˆA−1
kj·Ht

=6m
ηα·E
JX
j=1ˆA−1
kj·(ˆAkj+1−1−ˆAkj−1)

≤6m·max{2, ηαR Hγ}
ηα·E
JX
j=1ˆA−1
kj+1−1·(ˆAkj+1−1−ˆAkj−1)

≤6m·max{2, ηαR Hγ}
ηα·E
JX
j=1log 
det(ˆAkj+1−1)
det(ˆAkj−1)!

≤6m·max{2, ηαR Hγ}
ηα·E[log det( ˆAT)]
≤6dm·max{2, ηαR Hγ}
ηαlog(ηαR H+ 1),
where the first inequality follows from ˆAkj+1−1⪯max{2, ηαR Hγ}ˆAkj, and rest follows from the
standard inequalities used in Newton-step analysis (Hazan et al., 2007).
Combining all the bounds, we have that
(a)≤12m4βRHd·max{2, ηαR H√
T}
ηακ(G)log(ηαR H+ 1) +10m4βR3
Hd√
T
κ(G)+m2βdR3
H√
T.
Movement cost. By design, the algorithm updates at most once in every miterations. Therefore,
∥(ot−m+1, . . . , o t)−(ot−m+1, . . . , o t−m+1)∥2≤s,
where sis the Euclidean distance between neighboring iterates in Algorithm 3. By analysis in
Lemma 11, we have s≤ηdGD . By Lipschitz assumption on ft, we have
(b)≤ηdGD2βT.
Combining, we have that the total moving cost is bounded by
E"TX
t=mft(zt−m+1:t)−¯ft(zt−m+1)#
≤12m4βRHd·max{2, ηαR H√
T}
ηακ(G)log(ηαR H+ 1) +10m4βR3
Hd√
T
κ(G)
+m2βdR3
H√
T+ηdGD2βT.
A.4 Proof of Theorem 6
Combining result from Lemma 11, Lemma 12, and Lemma 13, we have the regret of Algorithm 1
w.r.t. to any z∈ K is bounded by
E[RegretT(z)]≤3m2βd
ηαlog(ηRHT+ 1) + 2 d0(GD) +D2d0RH
2η+ 3ηd0d2(GD)2RHT
+12m4βRHd·max{2, ηαR H√
T}
ηακ(G)log(ηRHT+ 1) +10m4βR3
Hd√
T
κ(G)
+m2βdR3
H√
T+ηdGβD2T,
and by choosing η= Θ
1
α√
T
, we have the regret above is bounded by ˜O
β
αGD√
T
.
20B Proof of Lemma 9
In this section, we prove Lemma 9. We begin by defining the Markov operator (Simchowitz et al.,
2020).
Definition 14 (Markov operator) .Given a partially observable LDS instance parametrized by dynam-
icsA∈Rdx×dx, B∈Rdx×du, C∈Rdy×dxsatisfying Assumption 2 with (κ, γ)-strongly stabilizing
K, define the Markov operator to be a sequence of matrices G={G[i]}i≥0such that G0= [0; Idu]
and∀i >0,G[i]is given by
G[i]=
C
KC
(A+BKC )i−1B∈R(dy+du)×du.
The next observation (Observation 15) relates yt(K), the signals used by the DRC policies (Defini-
tion 1), to the observations and controls along the learner’s trajectory. This justifies the accessibility
of the signals.
Observation 15. Given a partially observable LDS instance with (κ, γ)-strongly stabilizing K,
letGbe the Markov operator in Definition 14. For t, m∈N, letM1, . . . , M t−1∈Rm×du×dy
be(t−1)DRC matrices (Definition 1). Let (yt, ut)be the observation-control pair reached by
playing M1, . . . , M t−1for time step t= 1, . . . , t −1. Letut(K)be the control at time tby executing
the linear policy K, i.e. yt(K) =Kut(K), and yt(K)be the would-be observation had Kbeen
executed from the beginning of the time. Then, (yt, ut)and(yt(K), ut(K))can be related by the
following equality:

yt
ut
=
yt(K)
Kyt(K)
+tX
i=1G[i]
m−1X
j=0M[j]
t−iyt−i−j(K)
. (9)
In particular, Eq. (9)implies that yt(K)can be computed by the learner through access to the Markov
operator Gand the observations along the learner’s own trajectory.
With Eq. (9), we are almost ready to reduce the control instance to the BCO-M problem. One
delicate detail is that the BCO-M problem is defined for vector-valued decisions, and the the space
M(m, RM)is a space of sequences of matrices. For clarity of presentation, we define the following
embedding operators.
Definition 16 (Embedding operators) .Form, d y, du∈N, denote d=mdydu. The embedding
operator e: (R(du×dy))m→Rdis the natural embedding of a DRC controller M=M[0:m−1]
(Definition 1) in Rd. In particular, ∀k∈[m−1], i∈[du], j∈[dy],
⟨ekdudy+(i−1)dy+j,e(M)⟩=M[k]
ij.
Let ey: (Rdy)m→Rdu×dbe given by ∀yt−m+1, . . . , y t∈Rdy,∀i∈[du], k∈[m],
[ey(yt−m+1:t)]i,j+1:j+dy=yt−k+1,ifj= (k−1)dudy+ (i−1)dy.
Proof of Lemma 9. LetBt= (yt(K), Ky t(K)) +Pt
i=mG[i]Yt−ie(Mt−i)∈Rdu+dy,∀t. Let
Yt=ey(yt−m+1:t(K)). Then, by Eq. (9), we have
ct(yt, ut) =ct 
Bt+m−1X
i=0G[i]Yt−ie(Mt−i)!
.
We can’t directly define our ftas this function because thePt
i=mG[i]Yt−ie(Mt−i)term in Bt
depends on historical steps, which will lead to an unbounded memory.
To this end, we define
ct 
(yt(K), Ky t(K)) +m−1X
i=0G[i]Yt−ie(Mt−i)!
=:ft(e(Mt−m+1), . . . , e(Mt)), (10)
21which is independent of Mt−m+1:t, and Ytis independent of M1:T. Note that by the loss function
with memory has an affine memory structure. Moreover, G[i]andYt−iare computable by the
learner given system parameters. By the choice of m= Θ(log T), the Lipschitzness of ct, and the
norm-decaying property of G[i]due to the stability of the system, we can bound the distance
|ct(yt, ut)−ft(e(Mt−m+1), . . . , e(Mt))|=O1
poly(T)
.
In particular, we can choose msuch that this term is O 1
T2
, then any regret bound on ftin the
BCO-M setting directly transfers to the same regret bound on ctin the control setting, at a negligible
o(1)cost which will be subsumed by the approximation error term.
We are left with two steps to conclude this lemma. First, we need to show that ftindeed satisfies the
conditions in Definition 5 and specify the constants. Next, because ftis not equal to ctwhile we only
have access to ct, the gradient estimator for ftconstructed from ctis biased. We need to bound this
error and show that it has negligible impact on the regret bound.
Step 1: We denote the unary form to be ¯ftas in Definition 2 . Let O=
{M(m, RM), m,{ft}t≥m,t∈N}be the associated BCO-M instance (Definition 5).
By construction, ftsatisfies Assumption 8 and Assumption 5 other than the assumption on positive
convolution invertibility-modulus if the truncated vector (yt(K), Ky t(K))+Pm−1
i=0G[i]Yt−ie(Mt−i)
always lives in the RBdy+duand the matrix parameters are bounded. For the positive convolution
invertibility-modulus, Lemma 3.1 in (Simchowitz, 2020) proves that the G[0], . . . , G[m−1]induced
by the Markov operator in Definition 14 satisfies the positive convolution invertibility-modulus, i.e.
κ(G) = Ω(1) . By assumption on ct, the curvature assumption in Assumption 6 is also satisfied. It is
left to check the diameter bounds in Assumption 5 and Assumption 7.
First, we establish a diameter bound on our learning comparator set K=M(m, RM).
Diameter bound on K=M(m, RM).The diameter of the set M(m, RM)is given by
max
M1,M2∈M(m,RM)∥e(M1)−e(M2)∥2≤√m max
M1,M2∈M(m,RM)max
0≤j≤m−1∥M[j]
1−M[j]
2∥F
≤q
mmax{du, dy} max
M1,M2∈M(m,RM)max
0≤j≤m−1∥M[j]
1−M[j]
2∥op
≤q
mmax{du, dy}RM.
Algorithm 2 essentially calls the improper BCO-M algorithm in Algorithm 1. With the diameter
bound on M(m, RM), we know that the matrices governing the controls picked by Algorithm 2
during each round lives in the set M(m, RM+m). For simplicity, we may assume that RM≥m
and thus the decisions made by Algorithm 2 live in M(m,2RM).
Now, we are ready to prove the gradient bound on ftand the bounds on the observation-control pairs
produced by Algorithm 2.
Gradient bound of ft.First, we bound the sum of operator norms for the Markov operator. By
Assumption 2 and the fact that ∀M∈Rn×n,∥M∥op≤√n∥M∥max≤√n∥M∥2, we have that
∞X
i=0∥G[i]∥op≤1 +∞X
i=1q
∥C(A+BKC )i−1B∥2op+∥KC(A+BKC )i−1B∥2op
≤1 +p
dx(1 +κ2)κ2
sys∞X
i=1∥HLi−1H−1∥2
≤1 +p
dx(1 +κ2)κ2
sys∞X
i=0(1−γ)i
= 1 +p
dx(1 +κ2)κ2
sys
γ.
22The signals yt(K)has the following norm bound:
max
t∈[T]∥yt(K)∥2= max
t∈[T]Ct−1X
i=0(A+BKC )iwt−i+et
2
≤Rw,e 
1 +p
dxκsys∞X
i=1∥HLi−1H−1∥2!
≤Rw,e
1 +√dxκsys
γ
.
By the unfolding of (yt, ut)in Eq. (9), the observation and control pair (yt, ut)has the following
norm bound: ∀t,
∥(yt, ut)∥2≤ ∥(yt(K), Ky t(K))∥2+tX
i=1G[i]
m−1X
j=0M[j]
t−iyt−i−j(K)

2
≤Rw,e
1 +√dxκsys
γ
p
1 +κ2+ 2RM∞X
i=0G[i]
op

≤Rw,e
1 +√dxκsys
γ p
1 +κ2+ 2RM 
1 +p
dx(1 +κ2)κ2
sys
γ!!
=:R.
The above bound holds similarly for the truncated (ˆyt,ˆut) := ( yt(K), Ky t(K)) +Pm−1
i=0G[i]Yt−ie(Mt−i)in Eq. (10), which shows that (ˆyt,ˆut)∈RBdy+du. This also completes
checking Assumption 5.
By assumption, ctobeys the Lipschitz bounds
∥∇ct(y, u)∥2≤Gc∥(y, u)∥2.
The gradient bound on ftis given by
∥∇ft(e(Mt−m+1), . . . , e(Mt))∥2
=((G[m−1]Yt−m+1)⊤∇ct(ˆyt,ˆut), . . . , (G[0]Yt)⊤∇ct(ˆyt,ˆut)
2
≤√m 
1 +p
dx(1 +κ2)κ2
sys
γ!
GcR2
w,e
1 +√dxκsys
γ2 p
1 +κ2+ 2RM 
1 +p
dx(1 +κ2)κ2
sys
γ!!2
≤4096√mGcR2
w,eR2
Md2.5
xκ3κ8
sys
γ5.
Step 2: Clearly the gradient estimator obtained for ftis biased. We denote this error as atwhich
has a norm bound ∥at∥2=O(1
T)with the same reasoning on bounding |ct−ft|. Now, we only need
to show that Algorithm 1 can tolerate such perturbation with no loss in regret.
It’s known that the OGD algorithm can tolerate per-round adaptive gradient perturbation with norm
bounded by O(1
T), with no cost in regret. We will prove a similar argument for Algorithm 3. Suppose
in line 6, the unbiased gradient estimator ˜gtis replaced by a biased estimator ˆgt= ˜gt+at, then we
want to extend the regret bound for Algorithm 3 in Lemma 11 in this setting with an additional term
depending on the sum of magnitude of at. The analysis in the proof of Lemma 11 remains unchanged
except when we bound the regret for the sequence {ot}T
t=1in Eq. (6), the inequality becomes
E"TX
t=1¯ft(ot)−¯ft(o)#
≤TX
t=1E[∇¯ft(ot)⊤(ot−o)]−αf
2TX
t=1E[∥ot−o∥2
Ht]
=TX
t=1E[ˆg⊤
t(ot−o)]−TX
t=1E[a⊤
t(ot−o)]−αf
2TX
t=1E[∥ot−o∥2
Ht].
23The rest of proof after Eq. (6) is indentical, except that we replace each appearance of ˜gtbyˆgt.
Therefore, the regret incurred by using ˆgtinstead of ˜gtas the gradient estimator is bounded by
the regret upper bound in Lemma 11 and the additional termPT
t=1E[a⊤
t(ot−o)]≤aDT , where
a= max t∈[T]∥at∥2. Since a=O(1
T), the additional regret is bounded by O(1).
Concluding the lemma: Denote
αf=αc, βf=βc, D =q
mmax{du, dy}RM,
Gf=4096√mGcR2
w,eR2
Md2.5
xκ3κ8
sys
γ5.
We have that by Assumption 4 that
Eh
RegretANC
T(L)i
= max
M∈M(m,RM)E"TX
t=1ct(yt(M1:t−1), ut(M1:t−1))−ct(yt(M), ut(M))#
≤GfDm+E"TX
t=mft(e(Mt−m+1), . . . , e(Mt))−¯ft(e(M))#
+E"TX
t=m¯ft(e(M))−ct(yt(M), ut(M))#
=E"TX
t=mct 
Bt+m−1X
i=0G[i]Yt−ie(M))!
−ct 
(yt(K), Ky t(K)) +tX
i=0G[i]Yt−ie(M)!#
+E"TX
t=mft(e(Mt−m+1), . . . , e(Mt))−¯ft(e(M))#
+GfDm,
where we have
ct 
Bt+m−1X
i=0G[i]Yt−ie(M))!
−ct 
(yt(K), Ky t(K)) +tX
i=0G[i]Yt−ie(M)!
≤GctX
i=mG[i]Yt−ie(M)
2≤GctX
i=m∥G[i]∥op∥Yt−i∥F∥M∥F.
Recall that
∥M∥2
F=m−1X
j=0∥M[j]∥2
F≤max{du, dy}m−1X
j=0∥M[j]∥2
op≤mmax{du, dy}R2
M,
max
t∈[T]∥Yt∥2=q
mdyd2u·max
t∈[T]∥yt(K)∥2≤q
mdyd2u·Rw,e
1 +√dxκsys
γ
,
∞X
i=m∥G[i]∥op≤p
dx(1 +κ2)κ2
sys∞X
i=m(1−γ)i≤p
dx(1 +κ2)κ2
sys(1−γ)m
γ.
We have for m= Θ (log T/log(1/(1−γ))),
GctX
i=m∥G[i]∥op∥Yt−i∥F∥M∥F≤4Gcmdyd2
udx(1 +κ2)κ4
sysRM(1−γ)m
γ2
≤Gcmdyd2
udxκ2κ4
sysRM
γ2T.
24Combining, we have
Eh
RegretANC
T(L)i
≤Eh
RegretAB
T(O)i
+Gcmdyd2
udxκ2κ4
sysRM
γ2+GfDm
≤Eh
RegretAB
T(O)i
+ 2GfDm,
which concludes the claim that O2GfDmT−1-approximates L.
25NeurIPS Paper Checklist
1.Claims
Question: Do the main claims made in the abstract and introduction accurately reflect the
paper’s contributions and scope?
Answer: [Yes]
Justification: we make sure all the claims are accurate.
Guidelines:
•The answer NA means that the abstract and introduction do not include the claims
made in the paper.
•The abstract and/or introduction should clearly state the claims made, including the
contributions made in the paper and important assumptions and limitations. A No or
NA answer to this question will not be perceived well by the reviewers.
•The claims made should match theoretical and experimental results, and reflect how
much the results can be expected to generalize to other settings.
•It is fine to include aspirational goals as motivation as long as it is clear that these goals
are not attained by the paper.
2.Limitations
Question: Does the paper discuss the limitations of the work performed by the authors?
Answer: [Yes]
Justification: we discuss the limitations and future directions for improvement in the Discus-
sion section.
Guidelines:
•The answer NA means that the paper has no limitation while the answer No means that
the paper has limitations, but those are not discussed in the paper.
• The authors are encouraged to create a separate "Limitations" section in their paper.
•The paper should point out any strong assumptions and how robust the results are to
violations of these assumptions (e.g., independence assumptions, noiseless settings,
model well-specification, asymptotic approximations only holding locally). The authors
should reflect on how these assumptions might be violated in practice and what the
implications would be.
•The authors should reflect on the scope of the claims made, e.g., if the approach was
only tested on a few datasets or with a few runs. In general, empirical results often
depend on implicit assumptions, which should be articulated.
•The authors should reflect on the factors that influence the performance of the approach.
For example, a facial recognition algorithm may perform poorly when image resolution
is low or images are taken in low lighting. Or a speech-to-text system might not be
used reliably to provide closed captions for online lectures because it fails to handle
technical jargon.
•The authors should discuss the computational efficiency of the proposed algorithms
and how they scale with dataset size.
•If applicable, the authors should discuss possible limitations of their approach to
address problems of privacy and fairness.
•While the authors might fear that complete honesty about limitations might be used by
reviewers as grounds for rejection, a worse outcome might be that reviewers discover
limitations that aren’t acknowledged in the paper. The authors should use their best
judgment and recognize that individual actions in favor of transparency play an impor-
tant role in developing norms that preserve the integrity of the community. Reviewers
will be specifically instructed to not penalize honesty concerning limitations.
3.Theory Assumptions and Proofs
Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?
Answer: [Yes]
26Justification: we provide the full set of assumptions for all theoretical results. We provide a
complete (and correct) proof to for all theoretical results, among which the simpler ones are
left to the appendix.
Guidelines:
• The answer NA means that the paper does not include theoretical results.
•All the theorems, formulas, and proofs in the paper should be numbered and cross-
referenced.
•All assumptions should be clearly stated or referenced in the statement of any theorems.
•The proofs can either appear in the main paper or the supplemental material, but if
they appear in the supplemental material, the authors are encouraged to provide a short
proof sketch to provide intuition.
•Inversely, any informal proof provided in the core of the paper should be complemented
by formal proofs provided in appendix or supplemental material.
• Theorems and Lemmas that the proof relies upon should be properly referenced.
4.Experimental Result Reproducibility
Question: Does the paper fully disclose all the information needed to reproduce the main ex-
perimental results of the paper to the extent that it affects the main claims and/or conclusions
of the paper (regardless of whether the code and data are provided or not)?
Answer: [NA]
Justification: the paper does not include experiments.
Guidelines:
• The answer NA means that the paper does not include experiments.
•If the paper includes experiments, a No answer to this question will not be perceived
well by the reviewers: Making the paper reproducible is important, regardless of
whether the code and data are provided or not.
•If the contribution is a dataset and/or model, the authors should describe the steps taken
to make their results reproducible or verifiable.
•Depending on the contribution, reproducibility can be accomplished in various ways.
For example, if the contribution is a novel architecture, describing the architecture fully
might suffice, or if the contribution is a specific model and empirical evaluation, it may
be necessary to either make it possible for others to replicate the model with the same
dataset, or provide access to the model. In general. releasing code and data is often
one good way to accomplish this, but reproducibility can also be provided via detailed
instructions for how to replicate the results, access to a hosted model (e.g., in the case
of a large language model), releasing of a model checkpoint, or other means that are
appropriate to the research performed.
•While NeurIPS does not require releasing code, the conference does require all submis-
sions to provide some reasonable avenue for reproducibility, which may depend on the
nature of the contribution. For example
(a)If the contribution is primarily a new algorithm, the paper should make it clear how
to reproduce that algorithm.
(b)If the contribution is primarily a new model architecture, the paper should describe
the architecture clearly and fully.
(c)If the contribution is a new model (e.g., a large language model), then there should
either be a way to access this model for reproducing the results or a way to reproduce
the model (e.g., with an open-source dataset or instructions for how to construct
the dataset).
(d)We recognize that reproducibility may be tricky in some cases, in which case
authors are welcome to describe the particular way they provide for reproducibility.
In the case of closed-source models, it may be that access to the model is limited in
some way (e.g., to registered users), but it should be possible for other researchers
to have some path to reproducing or verifying the results.
5.Open access to data and code
27Question: Does the paper provide open access to the data and code, with sufficient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?
Answer: [NA]
Justification: paper does not include experiments requiring code.
Guidelines:
• The answer NA means that paper does not include experiments requiring code.
•Please see the NeurIPS code and data submission guidelines ( https://nips.cc/
public/guides/CodeSubmissionPolicy ) for more details.
•While we encourage the release of code and data, we understand that this might not be
possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
including code, unless this is central to the contribution (e.g., for a new open-source
benchmark).
•The instructions should contain the exact command and environment needed to run to
reproduce the results. See the NeurIPS code and data submission guidelines ( https:
//nips.cc/public/guides/CodeSubmissionPolicy ) for more details.
•The authors should provide instructions on data access and preparation, including how
to access the raw data, preprocessed data, intermediate data, and generated data, etc.
•The authors should provide scripts to reproduce all experimental results for the new
proposed method and baselines. If only a subset of experiments are reproducible, they
should state which ones are omitted from the script and why.
•At submission time, to preserve anonymity, the authors should release anonymized
versions (if applicable).
•Providing as much information as possible in supplemental material (appended to the
paper) is recommended, but including URLs to data and code is permitted.
6.Experimental Setting/Details
Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?
Answer: [NA]
Justification: the paper does not include experiments.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The experimental setting should be presented in the core of the paper to a level of detail
that is necessary to appreciate the results and make sense of them.
•The full details can be provided either with the code, in appendix, or as supplemental
material.
7.Experiment Statistical Significance
Question: Does the paper report error bars suitably and correctly defined or other appropriate
information about the statistical significance of the experiments?
Answer: [NA]
Justification: the paper does not include experiments.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The authors should answer "Yes" if the results are accompanied by error bars, confi-
dence intervals, or statistical significance tests, at least for the experiments that support
the main claims of the paper.
•The factors of variability that the error bars are capturing should be clearly stated (for
example, train/test split, initialization, random drawing of some parameter, or overall
run with given experimental conditions).
•The method for calculating the error bars should be explained (closed form formula,
call to a library function, bootstrap, etc.)
28• The assumptions made should be given (e.g., Normally distributed errors).
•It should be clear whether the error bar is the standard deviation or the standard error
of the mean.
•It is OK to report 1-sigma error bars, but one should state it. The authors should
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
of Normality of errors is not verified.
•For asymmetric distributions, the authors should be careful not to show in tables or
figures symmetric error bars that would yield results that are out of range (e.g. negative
error rates).
•If error bars are reported in tables or plots, The authors should explain in the text how
they were calculated and reference the corresponding figures or tables in the text.
8.Experiments Compute Resources
Question: For each experiment, does the paper provide sufficient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?
Answer: [NA]
Justification: the paper does not include experiments.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The paper should indicate the type of compute workers CPU or GPU, internal cluster,
or cloud provider, including relevant memory and storage.
•The paper should provide the amount of compute required for each of the individual
experimental runs as well as estimate the total compute.
•The paper should disclose whether the full research project required more compute
than the experiments reported in the paper (e.g., preliminary or failed experiments that
didn’t make it into the paper).
9.Code Of Ethics
Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines ?
Answer: [Yes]
Justification: we carefully follow the NeurIPS Code of Ethics.
Guidelines:
•The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
•If the authors answer No, they should explain the special circumstances that require a
deviation from the Code of Ethics.
•The authors should make sure to preserve anonymity (e.g., if there is a special consid-
eration due to laws or regulations in their jurisdiction).
10.Broader Impacts
Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?
Answer: [NA]
Justification: there is no societal impact of the work performed.
Guidelines:
• The answer NA means that there is no societal impact of the work performed.
•If the authors answer NA or No, they should explain why their work has no societal
impact or why the paper does not address societal impact.
•Examples of negative societal impacts include potential malicious or unintended uses
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
(e.g., deployment of technologies that could make decisions that unfairly impact specific
groups), privacy considerations, and security considerations.
29•The conference expects that many papers will be foundational research and not tied
to particular applications, let alone deployments. However, if there is a direct path to
any negative applications, the authors should point it out. For example, it is legitimate
to point out that an improvement in the quality of generative models could be used to
generate deepfakes for disinformation. On the other hand, it is not needed to point out
that a generic algorithm for optimizing neural networks could enable people to train
models that generate Deepfakes faster.
•The authors should consider possible harms that could arise when the technology is
being used as intended and functioning correctly, harms that could arise when the
technology is being used as intended but gives incorrect results, and harms following
from (intentional or unintentional) misuse of the technology.
•If there are negative societal impacts, the authors could also discuss possible mitigation
strategies (e.g., gated release of models, providing defenses in addition to attacks,
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
feedback over time, improving the efficiency and accessibility of ML).
11.Safeguards
Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?
Answer: [NA]
Justification: the paper poses no such risks.
Guidelines:
• The answer NA means that the paper poses no such risks.
•Released models that have a high risk for misuse or dual-use should be released with
necessary safeguards to allow for controlled use of the model, for example by requiring
that users adhere to usage guidelines or restrictions to access the model or implementing
safety filters.
•Datasets that have been scraped from the Internet could pose safety risks. The authors
should describe how they avoided releasing unsafe images.
•We recognize that providing effective safeguards is challenging, and many papers do
not require this, but we encourage authors to take this into account and make a best
faith effort.
12.Licenses for existing assets
Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?
Answer: [NA]
Justification: the paper does not use existing assets.
Guidelines:
• The answer NA means that the paper does not use existing assets.
• The authors should cite the original paper that produced the code package or dataset.
•The authors should state which version of the asset is used and, if possible, include a
URL.
• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
•For scraped data from a particular source (e.g., website), the copyright and terms of
service of that source should be provided.
•If assets are released, the license, copyright information, and terms of use in the
package should be provided. For popular datasets, paperswithcode.com/datasets
has curated licenses for some datasets. Their licensing guide can help determine the
license of a dataset.
•For existing datasets that are re-packaged, both the original license and the license of
the derived asset (if it has changed) should be provided.
30•If this information is not available online, the authors are encouraged to reach out to
the asset’s creators.
13.New Assets
Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?
Answer: [NA]
Justification: the paper does not release new assets.
Guidelines:
• The answer NA means that the paper does not release new assets.
•Researchers should communicate the details of the dataset/code/model as part of their
submissions via structured templates. This includes details about training, license,
limitations, etc.
•The paper should discuss whether and how consent was obtained from people whose
asset is used.
•At submission time, remember to anonymize your assets (if applicable). You can either
create an anonymized URL or include an anonymized zip file.
14.Crowdsourcing and Research with Human Subjects
Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as
well as details about compensation (if any)?
Answer: [NA]
Justification: the paper does not involve crowdsourcing nor research with human subjects.
Guidelines:
•The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
•Including this information in the supplemental material is fine, but if the main contribu-
tion of the paper involves human subjects, then as much detail as possible should be
included in the main paper.
•According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
or other labor should be paid at least the minimum wage in the country of the data
collector.
15.Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects
Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?
Answer: [NA]
Justification: the paper does not involve crowdsourcing nor research with human subjects.
Guidelines:
•The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
•Depending on the country in which research is conducted, IRB approval (or equivalent)
may be required for any human subjects research. If you obtained IRB approval, you
should clearly state this in the paper.
•We recognize that the procedures for this may vary significantly between institutions
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
guidelines for their institution.
•For initial submissions, do not include any information that would break anonymity (if
applicable), such as the institution conducting the review.
31