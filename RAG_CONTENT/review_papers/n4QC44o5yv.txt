UnderreviewassubmissiontoTMLRContext-AwareEstimationofAttributionRobustnessInTextAnonymousauthorsPaperunderdouble-blindreviewAbstractExplanationsarecrucialpartsofdeepneuralnetwork(DNN)classiﬁers.Inhighstakesapplications,faithfulandrobustexplanationsareimportanttounderstandDNNclassiﬁersandgaintrust.However,recentworkhasshownthatstate-of-the-artattributionmethodsintextclassiﬁersaresusceptibletoimperceptibleadversarialperturbationsthatalterexpla-nationssigniﬁcantlywhilemaintainingthecorrectpredictionoutcome.Ifundetected,thiscancriticallymisleadtheusersofDNNs.Thus,itiscrucialtounderstandtheinﬂuenceofsuchadversarialperturbationsonthenetworks’explanations.Inthiswork,weestab-lishanoveldeﬁnitionofattributionrobustness(AR)intextclassiﬁcation.Crucially,itreﬂectsbothattributionchangeinducedbyadversarialinputalterationsandperceptibilityofsuchalterations.Moreover,weintroduceasetofmeasurestoeﬀectivelycaptureseveralaspectsofperceptibilityofperturbationsintext,suchassemanticdistancetotheoriginaltext,smoothnessandgrammaticalityoftheadversarialsamples.WethenproposeournovelContext-AwareExplanationAttack(CEA),astrongadversarythatprovidesatightestimationforattributionrobustnessintextclassiﬁcation.CEAusescontext-awaremaskedlanguagemodelstoextractwordsubstitutionsthatresultinﬂuentadversarialsamples.Finally,withexperimentsonseveralclassiﬁcationarchitectures,weshowthatCEAconsis-tentlyoutperformscurrentstate-of-the-artARestimators,yieldingperturbationsthatalterexplanationstoagreaterextentwhilebeinglessperceptible.1IntroductionAttributionmethodsaimtogiveinsightsintocausalrelationshipsbetweendeepneuralnetworks’(DNNs)inputsandtheiroutcomeprediction.Theyarefundamentaltounraveltheblack-boxnatureofDNNsandarewidelyusedbothintheimageandnaturallanguagedomain.CommonlyusedattributionslikeSaliencyMaps(Simonyanetal.,2013),IntegratedGradients(Sundararajanetal.,2017),DeepLIFT(Shrikumaretal.,2017)andSelf-Attention(Bahdanauetal.,2015)highlightinputfeaturesthataredeemedimportantfortheDNNsintheinferenceprocess.Thesemethodsareespeciallyattractiveanduseful,astheyprovideon-the-ﬂyexplanationswithoutrequiringanydomain-speciﬁcknowledgefromusersorextensivecomputationresources.However,ithasbeenshownrecentlythatmanyoftheseattributionslackrobustnesstowardsadversarialper-turbations(Ghorbanietal.,2019).Carefullycrafted,imperceptibleinputalterationschangetheexplanationssigniﬁcantlywithoutmodifyingtheoutputpredictionoftheDNNs.Thisviolatesthepredictionassumptionoffaithfulexplanations(Jacovi&Goldberg,2020),whichstatesthatsimilarinputsshouldhavesimilarexplanationsforidenticaloutputs.Figure1exempliﬁesthisfragilityofattributions.Inmanysafety-criticalnaturallanguageprocessingproblems,suchasEHRclassiﬁcation(Girardietal.,2018),robustnessisakeyfactorforDNNstobedeployedinreallife.Forinstance,amedicalprofessionalassessingEHRswouldneitherunderstandnortrustamodelthatyieldstwosigniﬁcantlydiﬀerentexplanationsforseeminglyidenticalinputtextsandpredictions.Hence,itisfundamentaltounderstandhowthenetworksandattributionsbehaveinthepresenceofinputperturbationsandhowperceptiblethosealterationsaretotheuser.Inthiswork,wefocusonunderstandingtheadversarialrobustnessofattributionmaps(AR)intextclassi-ﬁcationproblems.Speciﬁcally,weareinterestedininvestigatingandquantifyingtheextenttowhichsmallinputperturbationscanalterexplanationsinDNNsandhowperceptiblesuchalterationsare.Wedosoby1UnderreviewassubmissiontoTMLROriginalsampleCEAperturbedsample(ours)TEFperturbedsample(Ivankayetal.,2022)pressthedeletekey.hitthedeletekey.newspaperthedeletekey.F(s,“Negative”)=0.99F(s,“Negative”)=0.95F(s,“Negative”)=0.95r:30r:1.1SemS:0.98SemS:0.8PCC:-0.05PCC:0.6peekattheweek:benvs.thestreak|yetanotherriskygameforthatpatriotswinningstreak,nowat21.pittsburghhasn#39;tlostathome,androokiequarterbackbenroethlisbergerhasn#39;tlost,period.peekattheplayoﬀs:benvs.thesteelers|yetanotherriskygameforthatpatriotswinningstreak,nowat21.pittsburghhasn#34lostathome,androokiequarterbackbenroethlisbergerhasn#39;tlost,period>hoodwinkatthezou:suisvs.thewave|yetanotherriskygameforthatpatriotswinningstreak,nowat21.pittsburghhasn#39;tlostathome,androokiequarterbackbenroethlisbergerhasn#39;tlost,period.F(s,“Sports”)=0.99F(s,“Sports”)=0.95F(s,“Sports”)=1.0r:14.9r:3.4SemS:0.97SemS:0.9PCC:0.02PCC:0.22intelseenreadyingnewwi-ﬁchips|intelcorp.thisweekisexpectedtointroduceachipthataddssupportforarelativelyobscureversionofwi-ﬁ,analystssaidonmonday,inamovethatcouldhelpeasecongestiononwirelessnetworks.intelseenreadyingwirelesswi-ﬁchips|intelcorp.thisweekisexpectedtolaunchaspeciﬁcationthataddedsupportforarelativelyobscureversionofwi-ﬁ,analystssaidonmonday,inamovethatcouldhelpeasecongestiononwirelessnetworks.intelseenreadyingnouveauwi-ﬁchips|intelcorp.thisweekisexpectedtoinsertadiesthatsummingsupportforarelativelyobscureversionofwi-ﬁ,analystssaidonmonday,inamovethatcouldhelpeasecongestiononwirelessnetworks.F(s,“Sci/Tech”)=0.78F(s,“Sci/Tech”)=0.95F(s,“Sci/Tech”)=0.95r:20r:4SemS:0.98SemS:0.91PCC:0.27PCC:0.28Figure1:Threeexamplesoffragileattributionmapsintextsequenceclassiﬁers.Ineachrow,carefulalterationoftheoriginalsampleresultsinsigniﬁcantlydiﬀerentattributionmapswhilemaintainingthepredictionconﬁdenceFinthecorrectlypredictedclass.Redwordshavepositiveattributionvalues,i.e.contributetowardsthetrueclass,whilebluewordswithnegativeattributionsagainstit.OurnovelCEAattackyieldsperturbedsamplesthathavelowerPearsonCorrelationCoeﬃcient(PCC)valuesbetweenthewordshighlightedbytheattributionmethodintheoriginalandperturbedinputs,aswellashighersemanticsimilarityvalues(SemS)oftheoriginalandadversarialsentences,comparedtothebaselineTEFattack.Thisresultsinhigherestimatedrobustnessconstantsr(seeSection4),thuslowerrobustnessoftheclassiﬁersagainstattacks.focusingonmethodstoﬁndperturbationsthatmaximizethechangeinattributionwhilebeingasimper-ceptibleaspossible.Characterizingandquantifyingtherobustnessofattributionmethodsisanimportantsteptowardstrainingrobustclassiﬁersandattributionmethodsthatcanbedeployedinawidevarietyofcriticalreal-lifeusecases.Wesummarizeourcontributionsasfollows:•Wearetheﬁrsttointroduceadeﬁnitionofattributionrobustness(AR)intextclassiﬁcationthattakesboththeattributiondistanceandperceptibilityofperturbationsintoaccount.•Weproposeadiversesetofmetricstoeﬀectivelycaptureaspectslikesemanticdistancetooriginal,smoothnessandgrammaticalityofperturbedinputs.Thisiskeytounderstandtheperceptibilityofsmalladversarialinputperturbationsintext.2UnderreviewassubmissiontoTMLR•Weintroduceanovelandpowerfulattackalgorithm,Context-AwareExplanationAttack(CEA),whichisshowntoconsistentlyoutperformstate-of-the-artadversariesandthereforeallowsustomoreaccuratelyestimateattributionrobustnessintextclassiﬁers.•Wearetheﬁrsttoutilizemaskedlanguagemodels(MLMs)forcontext-awarecandidateextractioninattributionrobustnessestimation.Thisisimportantbecausedomain-speciﬁcMLMsarebecomingincreasinglyavailable,makingthemaprogressivelyattractivealternativetolesseﬀective,customsynonymembeddingsonwhichcurrentestimationmethodshavetorely.•Wesuccessfullyspeeduprobustnessestimationwiththeusageofdistilledlanguagemodelsandbatchmasking.2RelatedworkTherobustnessaspectoffaithfulexplanations(Jacovi&Goldberg,2020)hasrecentlybeenstudiedwithincreasinginterest.TheauthorsGhorbanietal.(2019)weretheﬁrsttoshowthatattributionmethodslikeIntegratedGradients(Sundararajanetal.,2017)andDeepLIFT(Shrikumaretal.,2017),amongstothers,lackrobustnesstolocal,imperceptibleperturbationsintheinputthatleadtosigniﬁcantlyalteredattributionmapswhilemaintainingthecorrectpredictionoftheimageclassiﬁer.TheworksofDombrowskietal.(2019),Chenetal.(2019),Moosavi-Dezfoolietal.(2019),Rigottietal.(2022)andIvankayetal.(2021)havefurtherstudiedthisphenomenonandestablishedtheoreticalframeworkstounderstandandmitigatethelackofattributionrobustnessintheimagedomain.However,explanationrobustnessinnaturallanguageprocessinghasnotbeenexploredasdeeply.TheauthorsJain&Wallace(2019)andWiegreﬀe&Pinter(2020)showthatsimilarinputscanleadtosimilarattentionvaluesbutdiﬀerentpredictions,andthatmodelscanberetrainedtoyielddiﬀerentattentionvaluesforidenticalinputsandoutputs.This,however,doesnotdirectlycontradictthepredictionassumptionoffaithfulness(Jacovi&Goldberg,2020)asdiscussedbyWiegreﬀe&Pinter(2020).Closertoourwork,theworksofIvankayetal.(2022)andSinhaetal.(2021)aretheﬁrsttoprovethatexplanationsintextclassiﬁersarealsosusceptibletoinputchangesinaverysmalllocalneighbourhoodoftheinput.Ivankayetal.(2022)introduceTextExplanationFooler(TEF)asabaselinetoalterattributionsandestimatelocalrobustnessofattributionsintext.However,theauthors’deﬁnitionofARdoesnottakesemanticdistancesbetweenoriginalandadversarialsamplesintoaccount.Moreover,itdrawstokensubstitutioncandidatesfromaseparatelytrainedcustomsynonymembedding.Thus,theirattackresultsinout-of-contextandnon-ﬂuentadversarialsamples,renderingsuchperturbationseasilydetectable.OurworkaimstoimprovetheimperceptibilityofinputalterationsandestimateARwithlessdetectableadversarialalterationsthatchangeattributionstoagreaterextent.3PreliminariesAtextdatasetSiscomprisedofNtextsamplessi,eachcontainingaseriesoftokenswifromavocabularyWandlabelslidrawnfromthelabelsetL.AtextclassiﬁerFisafunctionthatmapseachsamplesitoalabelyi∈L.ItconsistsofanembeddingfunctionEandaclassiﬁerfunctionf.TheembeddingfunctionE:S→Rd×p,E(s)=XmapsthetextsamplessitoacontinuousembeddingXi,whiletheclassiﬁerfunctionf:Rd×p→R|L|,f(X)=omapstheembeddingstotheoutputprobabilitiesforeachclass.AnattributionfunctionA(s,F,l)=aassignsarealnumbertoeachtokenwjinsamples.Thisrepresentsthetokensinﬂuencetowardstheclassiﬁcationoutcome.Apositivevaluerepresentsatokenthatisdeemedrelevanttowardsthelabell,anegativevalueagainstit.WeconsidertheattributionmethodsSaliency(S)(Simonyanetal.,2013),IntegratedGradients(IG)(Sundararajanetal.,2017)andSelf-Attention(A)(Bahdanauetal.,2015).Theperplexity(Brownetal.)ofatextsampleswithtokenswj,givenalanguagemodelL,measureshowwelltheprobabilitydistributiongivenbyLpredictsthesamples,asdeﬁnedinEquation(1):PP(s|L)=2−󰁓wj∈sp(wj|L,s)logp(wj|L,s)(1)3UnderreviewassubmissiontoTMLRwherePPdenotestheperplexityofthetextsamplesandp(wj|L,s)theprobabilityoftokenwjgivenLands.LowperplexityvaluesindicatethatthemodelLhascapturedthetruedistributionofthetextdatasetSwell.SentenceencodersareembeddingfunctionsEs:S→Rm,Es(s)=ethatassignacontinuousembeddingvectorofdimensionmtoeachtextsample(Reimers&Gurevych,2019).Theseembeddingsareusedtocapturehigher-levelrepresentationsofsentencesorshortparagraphsthatcanbeusedtotraindownstreamtaskseﬀectively.Astheyarejointlytrainedonadiversesetofmulti-taskproblems,theyarearguedtocapturethesemanticmeaningofthetextwellReimers&Gurevych(2019).4AttributionRobustnessInthissection,weintroduceournoveldeﬁnitionofattributionrobustness(AR)intextclassiﬁers.Wedescribeourattributionandtextdistancemeasures,whicharetakenfromcurrentwork.Furthermore,wedescribetheoptimizationproblemofestimatingAR,ourthreatmodelaswellasournewestimatoralgorithm.4.1AttributionRobustnessinTextMostrelatedworksdeﬁneARasthemaximalattributiondistancewithagivenlocalityconstraintinthesearchspace(Ivankayetal.,2022;Sinhaetal.,2021).Wearguethatthisispotentiallyproblematic,astheextentoftheinputperturbationisnottakenintoaccount.Twoadversarialsampleswithsimilarlyalteredattributionsmightinfactstronglydiﬀerintermsofhowwelltheymaintainsemanticsimilaritytotheoriginalsample(seee.g.3rdexampleinFigure1).Thissuggeststhatapropermeasureofattributionrobustnessshouldascribehigherrobustnesstomethodsthatareonlyvulnerabletolargerperturbationswhilebeingimpervioustoimperceptibleones.Thus,wegiveanoveldeﬁnitionforattributionrobustnessforagiventextsampleswithtrueandpredictedlabellasfunctionsofbothresultingattributiondistanceandinputperturbationsize,writteninEquation(2).r(s)=max˜s∈N(s)d󰀅A(˜s,F,l),A(s,F,l)󰀆ds(˜s,s)(2)withtheconstraintthatthepredictedclassesof˜sandsareequal,writteninEquation(3).argmaxi∈{1...|L|}Fi(˜s)=argmaxi∈{1...|L|}Fi(s)(3)Here,ddenotesthedistancebetweenattributionmapsA(˜s,F,l)andA(s,F,l),FthetextclassiﬁerwithoutputprobabilityFiforclassi,anddsthedistanceofinputtextsamples˜sands.N(s)indicatesaneighbourhoodofs:{N(s)=˜s|ds(˜s,s)<ε}forasmallε.Thisdeﬁnitionisinspiredbytherobustnessassumptionoffaithfulexplanations(Jacovi&Goldberg,2020).TheestimatedrobustnessofanattributionmethodAonamodelFthenbecomestheexpectedper-sampler(s)ondatasetS,seeEquation(4).r(A,F)=Es∈S󰀅r(s)󰀆(4)Wecallthisrtheestimatedattributionrobustness(AR)constant.TherobustnessofattributionmethodAonthemodelFisinverselyproportionaltor(A,F),ashighvaluesmeanlargeattributiondistancesandsmallinputperturbations,whichindicateslowrobustness.4.2DistancesinTextDataInordertocomputetheattributionrobustnessconstantrfromEquation(4),thedistancemeasuresinthenumeratoranddenominatorofEquation(2)needtobedeﬁned.InexplainableAI,itisoftenarguedthatonlytherelativerankbetweeninputfeaturesortokensisimportantwhenexplainingtheoutcomeofaclassiﬁer,orevenonlythetopfewfeatures.Usersfrequentlyfocusonthefeaturesdeemedmostimportanttoexplainadecisionanddisregardthelessimportantones(Ghorbanietal.,2019;Ivankayetal.,2021;Dombrowskietal.,2019).Therefore,itiscommonpractice(Sinhaetal.,2021;Ivankayetal.,4UnderreviewassubmissiontoTMLR2022)tousecorrelationcoeﬃcientsandtop-kintersectionsasdistancemeasuresbetweenattributions.Forthisreason,weutilizethePearsoncorrelationcoeﬃcient(PCC)(Pearson,1895)asattributiondistanced󰀅A(˜s,F,l),A(s,F,l)󰀆=1−1+PCC󰀅A(˜s,F,l),A(s,F,l)󰀆2ofEquation(2).ThedenominatorinEquation(2)containsthedistancebetweenoriginalandadversarialtextsamples.Intextualinputdomains,measuringdistancebetweeninputsintheadversarialsettingisnotasstraightforwardasintheimagedomain,whereℓp-norminduceddistancesarecommon.Stringdistancemetrics(Navarro,2001)canonlybeusedlimitedly,astwowordscanhavesimilarcharactersbutentirelydiﬀerentsemantics.Forthisreason,weproposethefollowingsetofmeasurestoeﬀectivelycapturesmoothness,semanticdistancetooriginal,andcorrectnessofgrammarofadversarialtextinputs.First,weutilizepretrainedsentenceencoderstomeasurethesemantictextualsimilaritybetweentheoriginalandadversarialtextsamples.Thiscanbecomputedbythecosinesimilaritybetweenthesentenceembeddingsofthetwotextsamples,givenasds(˜s,s)=1−scos[Es(˜s),Es(s)]+12(5)wheredsdenotesthesemanticdistancebetweensamples˜sands,scosthecosinesimilarity,andEs(˜s)andEs(s)thesentenceembeddingsofthetwoinputsamples.Thesemantictextualsimilarityprovidesameasurehowclosethetwoinputsareintheirsemanticmeaning.Tothisend,theUniversalSentenceEncoder(Ceretal.,2018)iswidely-usedinadversarialtextsetups(Sunetal.,2020;Ivankayetal.,2022).However,thisarchitectureisnotstate-of-the-artontheSTSBenchmarkdataset(Ceretal.,2017),abenchmarkusedtoevaluatesemantictextualsimilarity.Therefore,weutilizeasecondsentenceencoderarchitecturetrainedbytheauthorsWangetal.(2020),MiniLM.Thismodelachievesclosetostate-of-the-artperformanceonthebenchmarkwhilemaintainingalowcomputationalcost.Oursecondinputdistanceisderivedfromtheperplexityoforiginalandadversarialinputs˜sands.Wecapturetherelativeincreaseofperplexitywhenperturbingtheoriginalsentences,giventhepretrainedGPT-2languagemodel(Radfordetal.,2019)(Equation6).ds(˜s,s)=PP(˜s|L)−PP(s|L)PP(s|L)+ε(6)wheredsdenotesthedistancebetweeninputs˜sands,PPtheperplexityofthetextsamplegiventheGPT-2languagemodelLandεisasmallconstant.Intuitively,thismeasureindicateshownaturaltheresultingadversarialinputsare.Lastly,wecapturetheincreaseofgrammaticalerrorsintheinputsamplesusingtheLanguageToolAPI1.Asgrammaticalerrorsareeasilyperceivedbythehumanobserver,theysigniﬁcantlycontributetotheperceptibilityofadversarialperturbations(Ebrahimietal.,2018).4.3Context-AwareRobustnessEstimationGivenourARdeﬁnitioninEquation(2),inordertoestimatethetruerobustnessofanattributionmethodforagivenmodel,allpossibleinputsequences˜swithintheneighborhoodNofswouldhavetobechecked,whichisintractable.Therefore,werestrictthesearchspacetosequences˜sthatonlycontaintokensubstitutionsfromthepredeﬁnedvocabularysetW.Moreover,werestricttheratioofsubstitutedtokensintheoriginalsequencetoρmax,consideringonly|C|numberofpossiblesubstitutionsforeachtokenins.Thenumber|C|ischosentoyieldhighattributiondistancewhilekeepingthecomputationcostlow,detailedinSection5.Thisway,wereducethetotalperturbationsetfrom|W||s|to|C||s|·ρmaxsamples.Thesearewidelyusedsimpliﬁcationsoftheadversarialsearchintext(Lietal.,2020).Theadversarialsequencesadvthenbecomestheperturbedsequencethatmaximizesr(s)fromEquation(2)WeestimateARwithournovelContext-AwareExplanationAttack(CEA).CEAisablack-boxattack,onlyhavingaccesstothemodel’spredictionandtheaccompanyingattributions,nointermediaterepresen-tationsorgradients.CEAconsistsofthefollowingtwosteps.1https://languagetool.org5UnderreviewassubmissiontoTMLRAlgorithm1Context-AwareExplanationAttackInput:Inputsentenceswithlabell,classiﬁerF,attributionA,attributiondistanced,DistilBERT-MLML,numberofcandidatesN,maximumperturbationratioρmax,batchmaskingratioρbOutput:Adversarialsentencesadv1:sadv←s,dmax←0,n←02:forwi∈sdo3:Iwi=d󰀅A(swi→0,F,l),A(s,F,l)󰀆⊲ImportanceRanking4:sB←〈s1...b,sb+1...2b,...,s|s|−b+1...|s|〉withIwb−1≥Iwb∀j∈{2,...,|sB|}and∀b∈{1,..,|sj|}5:forsb∈sBdo6:Cb←L(sb→[MASK],sadv)⊲BatchMaskingandCandidateExtraction7:forwj∈sbdo8:ifwj∈SStopwordsthen⊲StopWordFilter9:continue10:forck∈Cjdo⊲IterateoverCandidates11:˜swj→ck←Replacewjinsadvwithck12:ifargmaxi∈{1:|L|}F(˜swj→ck)∕=lthen⊲PredictionFilter13:continue14:˜d=d󰀅A(˜swi→ck,F,l),A(s,F,l)󰀆15:if˜d>dmaxthen⊲CandidateSelection16:sadv←˜swi→ck17:dmax←˜d18:n←n+119:ifρ=n+1|s|>ρmaxthen⊲LimitofWordSubstitutions20:breakStep1:Wordimportanceranking.Theﬁrststepextractsapriorityrankingoftokensintheinputtextsamples.Foreachwordwiins,CEAcomputesIwi=d󰀅A(swi→0,F,l),A(s,F,l)󰀆,whereswi→0denotesthetokenwiinssettothezeroembeddingvectorandddenotestheattributiondistancemeasureinEquation(2),describedintheprevioussubsection.ThetokensinsarethensortedbydescendingvaluesofIwi.Thus,weestimatewordsthatarelikelytoresultinlargeattributiondistancesandprioritizethoseforsubstitutionstowardsbuildingexplanationattacks.Importancerankinghasbeenshowntobeeﬀectiveinprioritizingwordsthatyieldlargechangesintheoutcomes(Lietal.,2020;Ivankayetal.,2022).Step2:Candidateselectionandsubstitution.Thesecondstepsubstituteseachhighestrankedtokenins,computedinStep1,withatokenfromacandidatesetC,indescendingimportanceorder.Thecandidatesetforaspeciﬁcwordisextractedbyﬁrstsubstitutingthespeciﬁcwordwiththe"<MASK>"token,thenpropagatingthewholesentence(withthe"<MASK>"token)throughatransformer-basedmaskedlanguagemodel(MLM).TheMLMthenpredictswhattokensorwordsarethemostlikelytoﬁllinthemaskedwordbyassigningaprobabilitydistributionoverallpossibletokensinthevocabulary.CEAtakesthe|C|numberoftokenswithhighestprobabilitiesascandidatesettoreplacethespeciﬁcwordinthesentence.OutofthiscandidatesetC,theﬁnalsubstitutionisthenselectedbymaximizingtheattributiondistance.CEAperformsthiscandidatesubstitutionwiththeMLMforeachhighestrankedwordinthesentenceiterativelyinasequentialorder.Inordertokeepthecomputationalcostslow,weutilizetheDistilBERTpretrainedmaskedlanguagemodel(Sanhetal.,2019),aBERT-MLMwithsigniﬁcantlyfewerparametersandmorecomputationallyeﬃcient.Also,atmostn=⌊|s|·ρmax⌋wordsaresubstituted.Whilecandidateextractionwithmaskedlanguagemodelshasbeenintroducedbefore,wearetheﬁrsttoapplythisconcepttotheARestimationproblem.Inordertofurtherreducecomputationalcost,CEAusesbatchmasking.Thus,insteadofmaskingeachwordseparatelyinStep2,theﬁrstnb=|s|·ρbmostimportanttokensaremaskedatonceandthelanguagemodelisqueriedforcandidatesforallofthesemaskedtokens.Here,nbdenotesthenumber,ρbtheratiooftokensinstobemaskedatonce.Forinstance,duringARestimationofa100wordtextsample,given6UnderreviewassubmissiontoTMLRρmax=0.15andρb=0.05,theMLMisqueriedonly(100·0.15)/(100·0.05)=3timeswithbatchmaskinginsteadof100·0.15=15timeswithoutit.WecomparedtheruntimeofCEAusingnon-distilled(Devlinetal.,2019)anddistilled(Sanhetal.,2019)BERTMLMs,withandwithoutbatchmasking,andfoundconsiderableperformanceincreasewithbatchmaskinganddistillation.TheresultsarereportedinSection5.5ExperimentsInthissection,wepresentourARestimationexperiments.Speciﬁcally,wedescribetheevaluationsetupandresultswithournovelrobustnessdeﬁnition.WeshowthatCEAconsistentlyoutperformsourdirectstate-of-the-artcompetitor,TextExplanationFooler(TEF)intermsoftheattributionrobustnessconstantrdescribedinSection4.Thus,weconveythatCEAextractssmootheradversarialsamplesthatareabletoalterattributionsmoresigniﬁcantlythanTEF.Finally,wecomparetheruntimeofCEAtoTEFandshowthatCEAachievescomparableruntimes,whilestilloutperformingTEFinthepreviouslymentionedaspects.5.1SetupWeevaluatetherobustnessconstantrestimatedbyCEAontheAG’sNews(Zhangetal.,2015),MRMovieReviews(Zhangetal.,2015),IMDB(Maasetal.,2011),Yelp(Asghar,2016)andtheFakeNewsdatasetsLiﬀerth(2018).WetrainaCNN,anLSTM,anLSTMwithanattentionlayer(LSTMAtt),aﬁnetunedBERT(Devlinetal.,2019),RoBERTa(Liuetal.,2019)andXLNet(Yangetal.,2019)classiﬁerforeachdataset.Adescriptionofthesecanbefoundintheappendix.WeestimatetherobustnessoftheSaliency(S),IntegratedGradients(IG)andSelf-Attention(A)attributionmethods.TheCNNandLSTMarchitecturesareusedincombinationwithSandIG,theremainingLSTMAtt,BERT,RoBERTAandXLNetareusedwithallthreeattributions.Thus,weevaluate16combinationsofmodelsandattributionsforeachdataset.WevarytheρmaxparameterofCEAbetween0.01and0.4.Avalueofρmaxdoesnotnecessarilyleadtotheactualperturbedratiooftokensρtobeρ=ρmaxduetothepredictionconstraint.Wesetthebatchmaskingsizeρb=min(ρmax,0.15),astheMLMwastrainedbymasking∼15%ofthetokens(Sanhetal.,2019).Weset|C|=15,aslargervaluesdonotresultinbetterestimationintermsofr,butinsigniﬁcantlyhigherattackruntimes.ThismakesourexperimentscomparabletoTEF(Ivankayetal.,2022).OurattackandexperimentsareimplementedinPyTorch(Paszkeetal.,2019),utilizingtheHuggingFaceTransformerlibrary(Wolfetal.,2020),Captum(Kokhlikyanetal.,2020)andSpaCy(Honnibaletal.,2020).WeruneachexperimentonanNVIDIAA100GPUwiththreediﬀerentseedsandreporttheaverageresults.5.2ResultsWereportthefollowingmetricsasfunctionsofthetrueperturbedratioρ.TheaveragePCCvaluesoforiginalandadversarialattributionmapsindicatetheamountofchangeinexplanations.Lowervaluesmeanlargerattributionchanges,thuslessrobustattributionmethodsforthegivendatasetandclassiﬁcationmodel.Theinputdistancebetweentextsamplesiscapturedbythesemantictextualsimilarityvaluesoftheoriginalandadversarialsamples,measuredbythecosinesimilaritybetweentheUSE(Ceretal.,2018)andMiniLM(Wangetal.,2020)sentenceembeddings(SemSUSEandSemSMiniLM),aswellastherelativeperplexityincrease(∆PP).Theaverageincreaseinnumberofgrammaticalerrors(GE)afterperturbationisalsoreported.Atconstantattributionchange,highersemanticsimilaritiesandlowerperplexitiesindicatelowerattributionrobustness,assmaller,moreimperceptiblealterationsareenoughtochangetheoutcomeoftheattributions.Usingtheaforementionedvalues,wereporttheestimatedrobustnessconstantsrUSE,rMiniLMandrPP,accordingtoEquation(4).WecomparethesemetricsforournovelCEAalgorithmandthedirectcompetitorTEF(Ivankayetal.,2022).TheresultsarereportedinFigure2.ThecontinuouslinescontainthemetricsforourCEAattack,thedashedlinesforthebaselineTEF.TheﬁguresshowthatCEAperturbationsalterexplanationsmore(lowerPCCvalues)whileyieldingadversarialsamplessemanticallyequallyormoresimilartotheoriginalinputsthanTEF(higheraverageSemS,loweraverage∆PPandGEvalues).Moreover,the7UnderreviewassubmissiontoTMLRLSTMAtt-IntegratedGradients(IG)onFakeNewsPCCrSemSPpl.andGrammar0.16ρ−1−0.500.510.16ρ051015200.16ρ0.60.810.16ρ−20246XLNet-Self-Attention(A)onYelpPCCrSemSPpl.andGrammar0.16ρ−1−0.500.510.16ρ05100.16ρ0.60.810.16ρ−2024
PCCrUSErMiniLMrPPSemSUSESemSMiniLM∆PPGECEATEFFigure2:ARmetricsasfunctionsoftheratioofperturbedtokensρ.WeplotthemeanandstandarddeviationofthePearsoncorrelations(PCC)betweenoriginalandadversarialattributions,theestimatedARrobustnessconstants(r),thesemanticsimilarities(SemS),relativeperplexityincrease(∆PP)andincreaseofnumberofgrammaticalerrors(GE)inoriginalandadversarialtextinputs.WecomparethesevaluesforournovelContext-AwareExplanationAttack(CEA-continuouslines)andthebaselineTextExpla-nationFooler(TEF-dashedlines).WeobserveconsistentimprovementinrobustnessestimationwithCEAcomparedtoTEF,reﬂectedinhigherr-valuesinthesecondcolumn.ThisisattributedtobothlowerPCCvalues,highersemanticsimilaritiesofperturbedsentencestotheoriginalonesandloweradversarialperplexityofCEAperturbations.∆AUCUSEr∆AUCMiniLMr∆AUCPPr
AG’sNewsMRIMDBYelpFakeNews00.51
AG’sNewsMRIMDBYelpFakeNews00.51
AG’sNewsMRIMDBYelpFakeNews012Figure3:Relativeincrease∆ofAUCrwhenestimatingtherobustnessconstantsr(Equation4)withCEAcomparedtoTEF.Eachpointcorrespondstooneofthe16combinationsofmodelandattributionmethod,ontheindicateddataset.Ther-valuesareestimatedwiththePCCasattributionsimilarity,varyingtheinputdistancemeasuresdsasdescribedinSection4.2.Weobservearelativeincreaseof0.3−1.5foralmostallmodels,attributionmapsanddatasetsevaluatedon.ThisshowsthatCEAconsistentlyprovidesbetterperturbationsthatalterattributionsmorewhilebeingmoreﬂuentandsemanticallysimilartotheunperturbedinput.perplexityincreaseisconsistentlylowerforCEAperturbations,leadingtomoreﬂuentadversarialsamples.Thisiswell-capturedbyresultingrobustnessconstantsr,whicharehigherforCEAthanTEF,showingboththatourARdeﬁnitionofEquation(2)isasuitableindicatorforARintextclassiﬁers,andthatCEAestimatesthisrobustnessbetterthanthestate-of-the-artTEFattack.Therestoftheresultsisreportedintheappendix.8UnderreviewassubmissiontoTMLRCNN-IGonAG’sNewsRoBERTa-AonFakeNewsLSTM-SonIMDB
TEFCEA1CEA2CEA04sρmax=0.1ρmax=0.25TEFCEA1CEA2CEA0125sρmax=0.1ρmax=0.25TEFCEA1CEA2CEA035sρmax=0.1ρmax=0.25Figure4:Per-sampleruntime(s)ofourARestimatoralgorithmversions.CEA,withadistilledMLMandbatchmasking,achievescomparablyfastestimationtoTEF,whileCEAwithanon-distilledBERTMLM(CEA1)istheslowestestimator,witharelativeincreaseinruntimeofapprox.1.5-2.5comparedtoTEF.DistillationoftheMLM(CEA2)improvestheruntimebyaround25-35%comparedto(CEA1).5.2.1AreaUndertheCurvesToquantifytheoverallperformanceofCEAoverthewholeoperationintervalofρ,wecomputetheareaundertheestimatedrcurves(2ndcolumninFigure2).ThesearecalculatedastheintegralAUCr=󰁕ρr(A,F)dρ.HighAUCrvaluescorrespondtohighr-values,thuslowoverallattributionrobustness.WethencomparetheresultingAUCrestimatedwithourCEAalgorithmtothecompetitormethodTEF.Figure3showstherelativeincreaseofAUCwhenestimatingwithCEAratherthanTEF,foreachofthe16combinationsofmodelsandattributionmethodsforagivendataset.Forinstance,avalueof0.5indicatesanincreaseof50%inestimatedAUCr,i.e.ifTEFresultsinAUCr=1.0,CEAyieldsAUCr=1.5.WeplottheAUCrincreaseestimatedwiththesemantictextualsimilaritiesfromUSE(AUCUSEr),MiniLM(AUCMiniLMr)andwiththerelativeperplexityincrease(AUCPPr).TheattributiondistanceinthenumeratorofrissettothePCC,describedinSection4.WeobserveanincreaseinAUCrof0.3−0.5withUSEandMiniLM,and0.5−1.5withPPformostmodels,attributionmapsanddatasets.ThisfurthershowsthatCEAconsistentlyyieldshigherrobustnessconstantsrthanTEF,providingbetterperturbationsthatalterattributionsmorewhilebeinglessperceptible.5.2.2RuntimeAnalysisQueryingtransformer-basedMLMsiscomputationallyexpensive.SubstitutingthesynonymextractionfromTEFwithanMLM-basedcandidateextractionresultsinasigniﬁcantincreaseinestimationtime.Therefore,weusethemethodsdescribedinSection4tolowertheestimationtimeinCEA.Figure4containstheper-sampleattacktimeforTEF,CEAwiththenon-distilledBERTMLM(CEA1),CEAwithDistilBERTMLM(CEA2)andourCEAalgorithmwithDistilBERTMLMandbatchmasking,forρmax∈{0.1,0.25}.WeobservethatCEA1resultsinasigniﬁcantincreaseinmeanestimationtimebyafactorofaround2comparedtoTEFonbothasmaller,mediumandalargedatasets.UsingCEA2forestimatingARdecreasestheruntimebyalargemargincomparedtoCEA1.Finally,whenapplyingbothadistilledMLMandbatchmasking-CEA,theper-sampleattacktimeiscomparabletothebaselineTEF,whilemaintainingbetterARestimation.5.2.3AblationStudiesCEAdiﬀersfromourdirectcompetitorTEF(Ivankayetal.,2022)inStep2ofthealgorithms.InsteadofutilizingthesynonymembeddingsMrkšicetal.(2016)toextractsubstitutioncandidatesandpassingthosethroughapartofspeechﬁlter,CEAusesMLMstoextractthecandidates.Thus,ourablationsfocusaroundthisaspect.WecompareTEFsARperformancetotwoversionsofCEA,theoriginalasformulatedinAlgorithm1andonewherethecandidateextractionisstillperformedwithanMLMasinAlgorithm1,buttheselectionisrandom(i.e.Line14-16).Wedonotexperimentwithablatingthestopwordﬁlterofthepredictionﬁlter,asthoseareassumptionsofrobustnessandconstraintsoftheoptimizationproblem,notdirectlydesignchoicesofCEA.Figure5comparestheARmetricsofthesethreeestimatorsandreportsthemasfunctionsofρ.WeobservethatCEAoutperformsbothTEFandtheMLM-basedrandomsyno-nymselection,supportingthechoiceofMLM-basedcandidateextractionoverTEF’ssynonymembeddings.9UnderreviewassubmissiontoTMLRRoBERTa-IntegratedGradients(IG)onAG’sNewsPCCrSemS0.16ρ−1−0.500.510.16ρ0510150.16ρ0.60.70.80.91RoBERTa-IntegratedGradients(IG)onMRPCCrSemS0.16ρ−1−0.500.510.16ρ0510150.16ρ0.60.70.80.91
TEFMLM+RandomSelectionCEAFigure5:ARestimationperformanceofCEA,TEFandourablatedCEAwithrandomcandidateselection.WeobservethatCEAoutperformsTEFbothintermsofPCCaswellasr,indicatingthesuperiorper-formanceofMLM-basedcandidateselectionoverpretrained,counter-ﬁttedsynonymembeddings.However,randomlyselectingthesubstitutionsfromthecandidatesetyieldsworseperformancethanTEF.RandomlyselectingthesubstitutionfromthecandidatesetsigniﬁcantlyspeedsupARestimation,yieldshoweverinferiorresultstobothTEFandCEAintermsofbothPCCandr.6ConclusionInthiswork,weintroducedanoveldeﬁnitionofattributionrobustnessintextclassiﬁers.Crucially,ourdeﬁnitionincorporatesperturbationsize,whichcontributessigniﬁcantlytotheperceptibilityofattacks.Weintroducesemantictextualsimilaritymeasures,therelativeperplexityincreaseandthenumberofgram-maticalerrorsaswaystoeﬀectivelyquantifyperturbationsizeintext.Next,weintroducedContext-AwareExplanationAttack,anewstate-of-the-artattackmethodthatresultsinatighterestimatorforattributionrobustnessintextclassiﬁcationproblems.Itisablack-boxestimatorusingadistilledMLMwithbatchmaskingtoextractadversarialperturbationswithsmallcomputationaloverhead.Finally,weshowedthatournewalgorithmCEAoutperformscurrentattacksbyalteringDNNattributionsmorewithlessperceptibleperturbations.Oneimportantquestionarisesfromtherobustnessassumptionofinterpretations:aremorerobustexplana-tionsindeedmorefaithful?Currentworkhasalreadystartedtolookintothisresearchquestion.TheauthorsIvankayetal.(2023)examinetheinterplaybetweenrobustnessandplausibility.However,understandingtheimpactofrobustnessonthefaithfulnessofexplanationstillremainsanopenquestionthatweplantoexamineinfuturework.Tosumup,ourcontributionsallowforestimatingtherobustnessofattributionsmoreaccuratelyandareaﬁrststeptowardstrainingrobust,safelyapplicableDNNsincriticalareaslikemedicine,laworﬁnance.
10UnderreviewassubmissiontoTMLRReferencesN.Asghar.YELPDatasetChallenge:ReviewRatingPrediction.arXivpreprintarXiv:1605.05362,2016.D.Bahdanau,K.H.Cho,andY.Bengio.NeuralMachineTranslationbyJointlyLearningtoAlignandTranslate.InInternationalConferenceonLearningRepresentations,2015.P.E.Brown,V.J.DellaPietra,S.A.DellaPietra,andJ.C.Lai.AnEstimateofanUpperBoundfortheEntropyofEnglish.ComputationalLinguistics,18(1).D.Cer,M.Diab,E.Agirre,I.Lopez-Gazpio,andL.Specia.SemEval-2017Task1:SemanticTextualSimilarityMultilingualandCrosslingualFocusedEvaluation.InInternationalWorkshoponSemanticEvaluation(SemEval-2017),pp.1–14,2017.D.Cer,Y.Yang,S.-Y.Kong,N.Hua,N.Limtiaco,R.StJohn,N.Constant,M.Guajardo-Céspedes,S.Yuan,andC.Tar.UniversalSentenceEncoder.arXivpreprintarXiv:1803.11175,2018.J.Chen,X.Wu,V.Rastogi,Y.Liang,andS.Jha.RobustAttributionRegularization.InAdvancesinNeuralInformationProcessingSystems,pp.14300–14310,2019.J.Devlin,M.-W.Chang,L.Kenton,andL.K.Toutanova.BERT:Pre-TrainingofDeepBidirectionalTransformersforLanguageUnderstanding.InNAACL-HLT,pp.4171–4186,2019.A.-K.Dombrowski,MAlber,C.Anders,M.Ackermann,K.-R.Müller,andP.Kessel.ExplanationscanbeManipulatedandGeometryistoblame.InAdvancesinNeuralInformationProcessingSystems,pp.13589–13600,2019.J.Ebrahimi,A.Rao,D.Lowd,andD.Dou.HotFlip:White-BoxAdversarialExamplesforTextClassiﬁca-tion.InAnnualMeetingoftheAssociationforComputationalLinguistics(Volume2:ShortPapers),pp.31–36,2018.A.Ghorbani,A.Abid,andJ.Zou.InterpretationofNeuralNetworksisFragile.InAAAIConferenceonArtiﬁcialIntelligence,volume33,pp.3681–3688,2019.I.Girardi,P.Ji,A.-P.Nguyen,N.Hollenstein,A.Ivankay,L.Kuhn,C.Marchiori,andC.Zhang.PatientRiskAssessmentandWarningSymptomDetectionUsingDeepAttention-BasedNeuralNetworks.InInternationalWorkshoponHealthTextMiningandInformationAnalysis,pp.139–148,2018.M.Honnibal,I.Montani,S.VanLandeghem,andA.Boyd.spaCy:Industrial-strengthNaturalLanguageProcessinginPython,2020.URLhttps://doi.org/10.5281/zenodo.1212303.A.Ivankay,I.Girardi,C.Marchiori,andP.Frossard.FAR:AGeneralFrameworkforAttributionalRobust-ness.The32ndBritishMachineVisionConference,2021.A.Ivankay,I.Girardi,C.Marchiori,andP.Frossard.FoolingExplanationsinTextClassiﬁers.InInterna-tionalConferenceonLearningRepresentations,2022.AdamDanielIvankay,MattiaRigotti,andPascalFrossard.DARE:TowardsRobustTextExplanationsinBiomedicalandHealthcareApplications.InThe61stAnnualMeetingOfTheAssociationForComputa-tionalLinguistics,2023.A.JacoviandY.Goldberg.TowardsFaithfullyInterpretableNLPSystems:HowShouldWeDeﬁneandEvaluateFaithfulness?InAnnualMeetingoftheAssociationforComputationalLinguistics,pp.4198–4205,2020.S.JainandB.C.Wallace.AttentionisnotExplanation.InProceedingsofNAACL-HLT,pp.3543–3556,2019.N.Kokhlikyan,V.Miglani,M.Martin,E.Wang,B.Alsallakh,J.Reynolds,A.Melnikov,N.Kliushkina,C.Araya,andS.Yan.Captum:AUniﬁedandGenericModelInterpretabilityLibraryforPyTorch.arXivpreprintarXiv:2009.07896,2020.11UnderreviewassubmissiontoTMLRL.Li,R.Ma,Q.Guo,X.Xue,andX.Qiu.BERT-ATTACK:AdversarialAttackAgainstBERTUsingBERT.InConferenceonEmpiricalMethodsinNaturalLanguageProcessing(EMNLP),pp.6193–6202.AssociationforComputationalLinguistics,November2020.W.Liﬀerth.FakeNews,2018.URLhttps://kaggle.com/competitions/fake-news.Y.Liu,M.Ott,N.Goyal,J.Du,M.Joshi,DChen,O.Levy,M.Lewis,L.Zettlemoyer,andV.Stoyanov.RoBERTa:ARobustlyOptimizedBERTPretrainingApproach.arXivpreprintarXiv:1907.11692,2019.A.Maas,R.EDaly,P.TPham,D.Huang,A.Y.Ng,andC.Potts.LearningWordVectorsforSenti-mentAnalysis.InAnnualMeetingoftheAssociationforComputationalLinguistics:HumanLanguageTechnologies,pp.142–150,2011.S.-M.Moosavi-Dezfooli,A.Fawzi,J.Uesato,andP.Frossard.RobustnessviaCurvatureRegularization,andviceversa.InIEEE/CVFConferenceonComputerVisionandPatternRecognition,pp.9078–9086,2019.N.Mrkšic,D.OSéaghdha,B.Thomson,M.Gašic,L.Rojas-Barahona,P.-H.Su,D.Vandyke,T.-H.Wen,andS.Young.Counter-ﬁttingWordVectorstoLinguisticConstraints.InNAACL-HLT,pp.142–148,2016.G.Navarro.AGuidedTourtoApproximateStringMatching.ACMComputingSurveys(CSUR),33(1):31–88,2001.A.Paszke,S.Gross,F.Massa,A.Lerer,J.Bradbury,G.Chanan,T.Killeen,Z.Lin,N.Gimelshein,andL.Antiga.PyTorch:AnImperativeStyle,High-PerformanceDeepLearningLibrary.InInternationalConferenceonNeuralInformationProcessingSystems,pp.8026–8037,2019.K.Pearson.NotesonRegressionandInheritanceintheCaseofTwoParents.ProceedingsoftheRoyalSocietyofLondon,58(347-352):240–242,1895.A.Radford,J.Wu,R.Child,D.Luan,D.Amodei,andI.Sutskever.LanguageModelsareUnsupervisedMultitaskLearners.OpenAIblog,1(8):9,2019.N.ReimersandI.Gurevych.Sentence-BERT:SentenceEmbeddingsusingSiameseBERT-Networks.InConferenceonEmpiricalMethodsinNaturalLanguageProcessingandInternationalJointConferenceonNaturalLanguageProcessing(EMNLP-IJCNLP),pp.3982–3992,2019.M.Rigotti,C.Miksovic,I.Giurgiu,T.Gschwind,andP.Scotton.Attention-BasedInterpretabilitywithConceptTransformers.InInternationalConferenceonLearningRepresentations,2022.V.Sanh,L.Debut,J.Chaumond,andT.Wolf.DistilBERT,adistilledversionofBERT:smaller,faster,cheaperandlighter.arXivpreprintarXiv:1910.01108,2019.A.Shrikumar,P.Greenside,andA.Kundaje.LearningImportantFeaturesthroughPropagatingActivationDiﬀerences.InInternationalConferenceonMachineLearning,pp.3145–3153.PMLR,2017.K.Simonyan,A.Vedaldi,andA.Zisserman.DeepInsideConvolutionalNetworks:VisualisingImageClas-siﬁcationModelsandSaliencyMaps.arXivpreprintarXiv:1312.6034,2013.S.Sinha,H.Chen,A.Sekhon,Y.Ji,andY.Qi.PerturbingInputsforFragileInterpretationsinDeepNaturalLanguageProcessing.InBlackboxNLPWorkshoponAnalyzingandInterpretingNeuralNetworksforNLP,pp.420–434,2021.L.Sun,K.Hashimoto,W.Yin,A.Asai,J.Li,P.Yu,andC.Xiong.Adv-BERT:BERTisnotRobustonMisspellings!GeneratingNatureAdversarialSamplesonBERT.arXivpreprintarXiv:2003.04985,2020.M.Sundararajan,A.Taly,andQ.Yan.AxiomaticAttributionforDeepNetworks.InInternationalConfer-enceonMachineLearning,volume70,pp.3319–3328,2017.12UnderreviewassubmissiontoTMLRW.Wang,F.Wei,L.Dong,H.Bao,N.Yang,andM.Zhou.MiniLM:DeepSelf-AttentionDistillationforTask-AgnosticCompressionofPre-TrainedTransformers.AdvancesinNeuralInformationProcessingSystems,33:5776–5788,2020.S.WiegreﬀeandY.Pinter.AttentionisnotnotExplanation.InConferenceonEmpiricalMethodsinNat-uralLanguageProcessingandInternationalJointConferenceonNaturalLanguageProcessing,EMNLP-IJCNLP,pp.11–20.AssociationforComputationalLinguistics,2020.T.Wolf,L.Debut,V.Sanh,J.Chaumond,C.Delangue,A.Moi,P.Cistac,T.Rault,R.Louf,M.Funtowicz,J.Davison,S.Shleifer,P.vonPlaten,C.Ma,Y.Jernite,J.Plu,C.Xu,T.LeScao,S.Gugger,M.Drame,Q.Lhoest,andA.M.Rush.Transformers:State-of-the-ArtNaturalLanguageProcessing.InConferenceonEmpiricalMethodsinNaturalLanguageProcessing:SystemDemonstrations,pp.38–45,Online,Octo-ber2020.AssociationforComputationalLinguistics.URLhttps://www.aclweb.org/anthology/2020.emnlp-demos.6.Z.Yang,Z.Dai,Y.Yang,J.Carbonell,R.Salakhutdinov,andQ.V.Le.XLNet:GeneralizedAutoregressivePretrainingforLanguageUnderstanding.InInternationalConferenceonNeuralInformationProcessingSystems,pp.5753–5763,2019.X.Zhang,J.Zhao,andY.Lecun.Character-LevelConvolutionalNetworksforTextClassiﬁcation.AdvancesinNeuralInformationProcessingSystems,2015:649–657,2015.
13UnderreviewassubmissiontoTMLRAAppendixA.1StudyonRandomizedExplanationsRoBERTa-IGonAG’sNewsRoBERTa-IGonMR0.20.40.6ν01020300.20.40.6ν05101520
rMiniLMFigure6:Attributionrobustnessmetricrasfunctionoftheratioofrandomizedwordattributionsν.DuringARestimationwithCEA,wesetacertainratioofwordattributionstoarandomnumberin[-1,1].Avalueν=0.4correspondsto40%ofwordattributionsbeingrandom.OurARmetricrpositivelycorrelateswithν.ThissupportsourargumentthatthemetricisasuitablemeasureofAR,ashigherrvaluesindicatelessrobustattributions,whichisthecaseforhigherν-s,givenourassumptionthatbadqualityexplanationsarelessrobustthangoodqualityones.ThisexperimentalstudyshowshowourCEAalgorithmbehavesin,andourARmetriccorrelateswith,caseswherethemodelsfailtogivecorrectexplanations.Eventhoughassessingthetruequality(forinstancefaithfulnessorcompleteness)ofattributionmethodsusedisoutofscopeforthiswork,wewouldliketounderstandhowourmetriccorrelateswithpartiallyrandomizedexplanations.Weassumethatrobustnessofexplanationscorrelateswiththeirquality,andrandomattributionsdonotreﬂectthetruedecisionprocess,thusarebadqualityexplanations.Therefore,ametricthatrepresentsARwellwouldcorrelatewiththeamountofrandomnessintheexplanations.Higherrandomnesswouldindicatelowerrobustness.InFigure6,weexaminethebehaviourofrasafunctionofν,theratioofrandomizedwordattributionsineachsentence.Weobserveapositivecorrelationbetweenrandν,whichsupportsourhypothesisthatrisagoodmeasureforARandreﬂectsthecorrelationbetweenARandqualityofexplanationswell.A.2DatasetsWeestimatetherobustnessofourattributionmethodsandmodelsonﬁvepubliclyavailabledatasets.TheseareAG’sNews,MRmoviereview,IMDBmoviereview,YelpandFakeNews,allofwhichareinEnglish.AG’sNewsconsistsof127552newsarticlesamples,categorizedintotheclassesWorld,News,BusinessandSci/Tech.Weusetheconcatenationoftitleandtextofthesamplestofeedintoourtextclassiﬁers,strippinganysamplethatislongerthan64tokens.TheMRMovieReviewdatasetcontains10592shortsamplesofpositiveornegativemoviereviews.Weonlyusetheﬁrst32tokensineachsampleasinputtotheclassiﬁers.IMDBMovieReviewisadatasetconsistingof49952positiveandnegativemoviereviews,withamaximumtokenlengthof256.Yelpcategorizes700000reviewsofseveraltopicsinto5classes,eachrepresentingaratingfrom1to5.Westripthesamplestoamaximumlengthof256.FakeNewsisacollectionof20080newssamples,eachcategorizedintoreliableorunreliable.Theseareratherlongarticles,thusweuseamaximumsequencelengthof512forthisdataset.Weapplybasicpreprocessingtoallsamplesineachdataset,whichincludesconvertingthemtolowercase,removinganyspecialcharactersnotintheEnglishalphabetandemojis.Weuse60%ofthesamplesfortrainingtheclassiﬁermodels,20%forvalidationand20%fortestingandestimatingtherobustnessofattributionmethods.A.3ModelsAsdescribedinthemainpaper,wetrainsixclassiﬁcationarchitecturesforeachdataset,threeDNN-basedarchitectures,whichareaCNN,anLSTM,anLSTMwithanattentionlayer(LSTMAtt),aswellasthree14UnderreviewassubmissiontoTMLRDatasetCNNLSTMLSTMAttBERTRoBERTaXLNetAG’sNews89.7%90.8%91.4%94.2%94.0%93.8%MR73.0%76.4%78.0%82.2%87.7%86.3%IMDB82.0%87.2%87.3%89.4%93.3%93.7%Yelp49.0%54.8%60.0%62.6%67.6%-FakeNews98.9%99.6%99.6%99.8%100.0%100.0%Table1:Accuraciesofeachclassiﬁertrained.Ourmodelsachievecomparableresultstostate-of-the-artperformanceforeachdataset.AG’sNewsMRIMDBYelpFakeNewsCNNInputshape(64,300)(32,300)(256,300)(256,300)(512,300)Num.classes42252Filtersizes[3,5,7][3,5][3,5,7][3,5,7][3,5,7]Featuresizes[8,8,8][8,8][16,16,16][128,128,128][32,32,32]Poolingsizes[2,2,2][2,2][2,2,2][2,2,2][2,2,2]Lin.layerdim.88166432Num.params6774827946567458164282934091714LSTMInputshape(64,300)(32,300)(256,300)(256,300)(512,300)Num.classes42252Hiddendim.881625616Num.layers11221Poolingsizes22122Lin.layerdim.88163216Num.params109881045818162214669385986LSTMAttInputshape(64,300)(32,300)(256,300)(256,300)(512,300)Num.classes42252Hiddendim.881625616Num.layers41221Lin.layerdim.88163216Num.params250041999447666275290141826BERTInputshape(64,)(32,)(256,)(256,)(512,)Num.classes42252ModelIDbert-base-uncasedNum.params109485316109483778109483778109486085109483778RoBERTaInputshape(64,)(32,)(256,)(256,)(512,)Num.classes42252ModelIDroberta-baseNum.params124648708124647170124647170124649477124647170XLNetInputshape(64,)(32,)(256,)(256,)(512,)Num.classes42252ModelIDxlnet-base-casedNum.params117312004117310466117310466117312773117310466Table2:Modelspeciﬁcationstransformer-basedarchitectures,whichareaﬁnetunedBERT,RoBERTaandXLNet.TheCNN,LSTMandLSTMAttarchitecturesusethe6B-300-dimensionalGlovewordembeddings,whilethetransformer-basedarchitecturesusethepretrainedHuggingFaceembeddingsoftherespectivebase-uncasedversions.TheDNN-basedclassiﬁerseachcontainalinearlayerontopoftheirfeatureextractorsandusethebuilt-inSpaCyEnglishtokenizer,thetransformersdirectlymapthefeatureoutputstotheoutputlogitswithafully-connectedlayerandutilizetheHuggingFacepretrainedtokenizersforeacharchitecturerespectively.Table2containsthemodelspeciﬁcations.Wetraineachmodelwithastandardlearningrateof0.001,usingtheAdamoptimizerwiththecross-entropylossandearlystopping.WeutilizeNVIDIAA100GPUstospeeduptrainingandARestimation.TheresultingaccuraciesofthemodelscanbefoundinTable1.
15UnderreviewassubmissiontoTMLRA.4AdditionalExamplesOriginalsampleCEAperturbedsample(ours)TEFperturbedsample(Ivankayetal.,2022)withthenationsmediarainingheavycriticismdownuponhim,spaincoachluisaragoneschosetopinagallingnil-niluefaworldcupqualifyingresultwithlithuaniaonthelargeplayingwiththenationsmediarainingheavycriticismdownuponhim,spaincoachluisaragoneschosetopinagallingnil-niluefaworldjuniorclassiﬁcationresultwithlithuaniaonthelargeyellowwiththenationsmediarainingheavycriticismdownuponhim,spainbusesluisaragoneschosetopinagallingnil-niluefaworldgobletqualifyingresultwithlithuaniaonthelargereplayF(s,“Sports”)=1.00F(s,“Sports”)=1.00F(s,“Sports”)=1.00r:31.69r:5.37SemS:0.98SemS:0.95PCC:-0.22PCC:0.42whenstonehillhiredchriswoodsasitsfootballcoachafterlastseason,thehopewashecouldonceagainreviveadisappointingwhenRutgershiredchriswoodsasitsheadcoachafterlastseason,thehopewashecouldonceagainreviveadisappointingwhenvassarhiredchriswoodsasitsballooncoachafterlastseason,thehopewashecouldonceagainreviveadisappointingF(s,“Sports”)=1.00F(s,“Sports”)=1.00F(s,“Sports”)=1.00r:3.22r:2.96SemS:0.88SemS:0.85PCC:0.25PCC:0.12thespaceshuttlewillnotﬂybeforemay2005,accordingtonasaoﬃcials.thispushestheshuttle#39;sreturn-to-ﬂightschedulebackbytwomonths,andpostponesavitalservicingmissiontotheinternationalspacetheAtlantiscapsuleswillnotﬂybeforemay2005,accordingtonasaoﬃcials.thispushestheISS#39;sreturn-to-ﬂightschedulebackbytwomonths,andpostponesavitalservicingmissiontotheinternationalspacetheseparationshuttleswillnotﬂybeforemay2005,accordingtonasaoﬃcials.thispushestheferry#39;sreturn-to-ﬂightschedulebackbytwomonths,andpostponesavitalservicingmissiontotheinternationalspaceF(s,“Sci/Tech”)=1.00F(s,“Sci/Tech”)=1.00F(s,“Sci/Tech”)=1.00r:1.46r:3.62SemS:0.85SemS:0.88PCC:0.57PCC:0.15duelingciscosystemsinc.andjunipernetworksinc.arebothjockeyingforthespotlightonthehighendoftheroutingmarketwithannouncementsofnewdevelopmentsaroundtheirrespectivecrs-1andt-seriescoreTheciscosystemsinc.andjunipernetworksinc.arebothjockeyingforthespotlightonthehighendofthenetworkingmarketwithannouncementsofnewdevelopmentsaroundtheirrespectivecrs-1andt-seriescorejoustingbelkinsystemsinc.andjunipergridsinc.arebothjockeyingforthespotlightonthehighendoftheroutingmarketwithannouncementsofnewdevelopmentsaroundtheirrespectivecrs-1andt-seriescoreF(s,“Sci/Tech”)=0.98F(s,“Sci/Tech”)=0.99F(s,“Sci/Tech”)=0.99r:2.90r:4.58SemS:0.93SemS:0.90PCC:0.61PCC:0.04playboyenterprisesinc.(pla.n:quote,proﬁle,research),theadultentertainmentcompany,ontuesdayreportedathird-quarterproﬁt,reversingayear-earlierplayboyenterprisesinc.(pla.n:quote,proﬁle,research),thelargesttechcompany,ontuesdayreportedathird-quarterproﬁt,reversingayear-earlierplayboyenterprisesinc.(pla.n:quote,proﬁle,research),theadulthoodentertainmentcompany,onyesterdayreportedathird-quarterproﬁt,reversingayear-earlierF(s,“Business”)=1.00F(s,“Business”)=0.98F(s,“Business”)=1.00r:2.01r:21.20SemS:0.98SemS:0.99PCC:0.92PCC:0.6516UnderreviewassubmissiontoTMLROriginalsampleCEAperturbedsample(ours)TEFperturbedsample(Ivankayetal.,2022)jimmiejohnsonhasfoughtthroughmistakes,mechanicalfailuresandthedespairoflosingfriendsinaplanecrashtochargebackintonascar#39;sclosestchampionshipbattlejimmiejohnsonhasfoughtthroughmistakes,mechanicalfailuresandthedespairoflosingfriendsinaplanecrashtochargebackintoHalo#39;sclosestHalobattlejimmiejohnsonhasfoughtthroughmistakes,mechanicalfailuresandthedespairoflosingfriendsinaplanecrashtochargebackintodaytona#39;sclosestchampionbattleF(s,“Sports”)=1.00F(s,“Sports”)=0.56F(s,“Sports”)=1.00r:2.46r:27.69SemS:0.90SemS:0.98PCC:0.53PCC:0.11islamabad:pakistanandafghanistanhavereaﬃrmedtheyarepartnersinﬁghtingterrorism,afghanpresidenthamidkarzaideclaredattheendofatwo-dayislamabad:pakistanandafghanistanhavereaﬃrmedtheyarepartnersinﬁghtingterrorismafghaniahamidkarzaideclaredattheendofatwo-dayislamabad:pakistanandafghanistanhavereaﬃrmedtheyarealliesinﬁghtingterrorism,afghanchairmenhamidkarzaideclaredattheendofatwo-dayF(s,“World”)=1.00F(s,“World”)=1.00F(s,“World”)=1.00r:61.30r:11.94SemS:1.00SemS:0.97PCC:0.43PCC:0.31lustykoalasinsouthernaustraliaaregoingtobeputonthepilltostopthembreedingtooquicklyandputtingtoomuchstrainontheireucalyptus-forestlustykoalasinsouthernaustraliaaregoingtobeputonthespottostopthemdisappearingtooquicklyandputtingtoomuchstrainontheireucalyptus-forestlustykoalasinsouthernaustraliaaregoingtobeputonthetablettostopthemrearingtooquicklyandputtingtoomuchstrainontheireucalyptus-forestF(s,“Sci/Tech”)=1.00F(s,“Sci/Tech”)=0.89F(s,“Sci/Tech”)=1.00r:8.67r:9.29SemS:0.94SemS:0.94PCC:-0.02PCC:-0.10embarcaderotechnologiesonmondayisunveilingitsdbartisanworkbench8.0databaseadministrationtool,featuringenhancedbackupcapabilitiesformicrosoftsqlserverdatabasesandsupportforperformancemetricsintheoracle10gdatabasetechnologiesvendormondayisshowcasingitsdbartisanworkbench8.0databaseadministrationtool,featuringenhancedbackupcapabilitiesformicrosoftsqlserverdatabasesandsupportforperformancemetricsintheoracle10galamedatechsonmondayisbrandishingitsdbartisanworkbench8.0databaseadministrationtool,featuringenhancedbackupcapabilitiesformicrosoftsqlserverdatabasesandsupportforperformancemetricsintheoracle10gF(s,“Sci/Tech”)=0.99F(s,“Sci/Tech”)=0.99F(s,“Sci/Tech”)=0.99r:4.38r:2.08SemS:0.92SemS:0.88PCC:0.33PCC:0.49inamovelikelytohavemajorramiﬁcationsforthelibraryworld,googleannounceddecember14thatitwouldembarkonanambitiousprojecttodigitallyscanbooksfromthecollectionsofﬁvemajorresearchlibrariesandmakethemsearchableinamovelikelytohavemajorrepercussionsforthedigitalworld,Cambridgeannounceddecember14thatitwouldembarkonanambitiousprojecttodigitallyscandatafromthecollectionsofﬁvemajorresearchlibrariesandmakethemsearchableinamovelikelytohavemajorimplicationsforthelibraryworld,iphoneannounceddecember14thatitwouldembarkonanambitiousplanstodigitallyscanlivresfromthecollectionsofﬁvemajorresearchlibrariesandmakethemsearchableF(s,“Sci/Tech”)=0.94F(s,“Sci/Tech”)=1.00F(s,“Sci/Tech”)=1.00r:4.21r:3.71SemS:0.91SemS:0.83PCC:0.20PCC:-0.2517UnderreviewassubmissiontoTMLROriginalsampleCEAperturbedsample(ours)TEFperturbedsample(Ivankayetal.,2022)stitchisabadmannered,uglyanddestructivelittle****.nocutefactorhere.notthatimindugly;theproblemstitchisabadmannered,uglyandcutelittle****.nolimitingfactorhere.notthatimindugly;theproblemstitchisabadmannered,uglyanddetrimentallittle****.nolovelyfactorhere.notthatimindugly;theproblemF(s,“Negative”)=1.00F(s,“Negative”)=0.98F(s,“Negative”)=1.00r:18.75r:2.21SemS:0.99SemS:0.98PCC:0.54PCC:0.93miyazakihascreatedsuchavibrant,colorfulworld,it’salmostimpossiblenottobesweptawaybythesheerbeautyofhisimagesmiyazakihascreatedsuchavibrantlyimaginativeworld,it’salmostimpossiblenottobesweptawaybythesheerbeautyofhisimagesmiyazakihascreatedsuchabustling,picturesqueworld,it’salmostimpossiblenottobesweptawaybythesheerbeautyofhisimagesF(s,“Positive”)=1.00F(s,“Positive”)=1.00F(s,“Positive”)=1.00r:6.26r:3.67SemS:0.97SemS:0.98PCC:0.64PCC:0.82itisachallengingﬁlm,ifnotalwaysanarrativelycohesiveoneitisabeautifulﬁlm,ifnotalwaysanarrativelycohesiveoneitisaproblematicﬁlm,ifnotalwaysanarrativelycohesiveoneF(s,“Positive”)=1.00F(s,“Positive”)=1.00F(s,“Positive”)=0.98r:2.31r:2.89SemS:0.93SemS:0.91PCC:0.68PCC:0.50muchlikeitseasilydismissivetakeontheupscalelifestyle,thereisn’tmuchthereheremuchlikeitseasilyreadabletakeontheupscalelifestyle,thereisn’tmuchthereheremuchlikeitseasilysnidetakeontheupscalelifestyle,thereisn’tmuchtherehereF(s,“Negative”)=1.00F(s,“Negative”)=1.00F(s,“Negative”)=1.00r:4.53r:4.67SemS:0.93SemS:0.95PCC:0.40PCC:0.53it’sanicelydetailedworldofpawns,bishopsandkings,ofwagersindingybackroomsorpristineforestsit’sasurprisinglyrichworldofpawns,bishopsandkings,ofwagersindingybackroomsorpristineforestsit’sapolitelythoroughworldofpawns,bishopsandkings,ofwagersindingybackroomsorpristineforestsF(s,“Positive”)=1.00F(s,“Positive”)=1.00F(s,“Positive”)=0.98r:8.09r:20.51SemS:0.96SemS:0.97PCC:0.35PCC:-0.04anatonalestrogenoperathatdemonizesfeminismwhilegiftingthemostsympatheticmaleofthepiecewithanicevomitbathathisweddinganappallingestrogenoperathatdemonizesfeminismwhiledistractingthemostsympatheticmaleofthepiecewithanicevomitbathathisweddinganatonalhormoneteatrothatdemonizesfeminismwhilegiftingthemostsympatheticmaleofthepiecewithanicevomitbathathisweddingF(s,“Negative”)=1.00F(s,“Negative”)=1.00F(s,“Negative”)=1.00r:11.64r:7.93SemS:0.97SemS:0.96PCC:0.25PCC:0.35
18UnderreviewassubmissiontoTMLROriginalsampleCEAperturbedsample(ours)TEFperturbedsample(Ivankayetal.,2022)abrutalandfunnywork.nicoleholofcenter,theinsightfulwriter/directorresponsibleforthisilluminatingcomedydoesn’twraptheproceedingsupneatlyabrilliantandfunnywork.nicoleholofcenter,theinsightfulwriter/directorresponsibleforthisilluminatingcomedydoesn’twraptheproceedingsup...abarbaricandfunnywork.nicoleholofcenter,theinsightfulwriter/directorresponsibleforthisilluminatingcomedydoesn’twraptheproceedingsuppleasantlyF(s,“Positive”)=1.00F(s,“Positive”)=1.00F(s,“Positive”)=1.00r:14.80r:7.92SemS:0.98SemS:0.97PCC:0.52PCC:0.60leighisn’tbreakingnewground,butheknowshowadailygrindcankillloveleighisn’tbreakingnewground,butheknowshowadailyworkoutcankillloveleighisn’tbreakingnewground,butheknowshowadailysmoothingcankillloveF(s,“Positive”)=1.00F(s,“Positive”)=1.00F(s,“Positive”)=1.00r:9.09r:3.94SemS:0.96SemS:0.96PCC:0.27PCC:0.69newwaysofdescribingbadnessneedtobeinventedtodescribeexactlyhowbaditisnewwaysofdescribingcancerneedtobeinventedtodescribeexactlyhowbaditisnewwaysofdescribingperversityneedtobeinventedtodescribeexactlyhowbaditisF(s,“Negative”)=1.00F(s,“Negative”)=1.00F(s,“Negative”)=1.00r:1.13r:1.45SemS:0.82SemS:0.90PCC:0.60PCC:0.70tothedegreethativansxtc.works,it’sthankstohuston’srevelatoryperformancetothedegreethativansxtc.works,it’sthankstohuston’soutstandingperformancetothedegreethativansxtc.works,it’sthankstohuston’srevelatoryexecutionF(s,“Positive”)=1.00F(s,“Positive”)=1.00F(s,“Positive”)=1.00r:16.14r:4.44SemS:0.98SemS:0.96PCC:0.50PCC:0.60quiet,adultandjustaboutmorestatelythananycontemporarymoviethisyear...atruestudy,aﬁlmwithaquestioningheartandmature,funnyandjustaboutmorestatelythananycontemporarymoviethisyear...atruestudy,aﬁlmwithaquestioningheartandquiet,adulthoodandjustaboutmorestatelythananytopicalmoviethisyear...atruestudy,aﬁlmwithaquestioningheartandF(s,“Positive”)=1.00F(s,“Positive”)=1.00F(s,“Positive”)=1.00r:1.83r:1.37SemS:0.95SemS:0.97PCC:0.82PCC:0.92amarkedlyinactiveﬁlm,cityisconversationalborderingonconfessionalamarkedlyinactiveneighbourhood,cityisconversationalborderingonconfessionalamarkedlyidleﬁlm,cityisconversationalborderingonconfessionalF(s,“Negative”)=1.00F(s,“Negative”)=1.00F(s,“Negative”)=1.00r:3.92r:17.79SemS:0.91SemS:0.98PCC:0.32PCC:0.21anentertaining,ifsomewhatstandardized,actionmovieanexcellent,ifsomewhatstandardized,actionmovieanentertain,ifsomewhatstandardized,actionmovieF(s,“Positive”)=1.00F(s,“Positive”)=1.00F(s,“Positive”)=0.99r:3.65r:11.43SemS:0.94SemS:0.97PCC:0.59PCC:0.2219UnderreviewassubmissiontoTMLROriginalsampleCEAperturbedsample(ours)TEFperturbedsample(Ivankayetal.,2022)wonderofwonders–ateenmoviewithahumanisticmessageLandofwonders–ateenmoviewithahumanisticmessageastonishmentofwonders–ateenmoviewithahumanisticmessageF(s,“Positive”)=1.00F(s,“Positive”)=1.00F(s,“Positive”)=1.00r:9.92r:7.05SemS:0.97SemS:0.94PCC:0.32PCC:0.21startrekwaskindofterriﬁconce,butnowitisacopyofacopyofacopystartrekwaskindoflameonce,butnowitisahellofacopyofacopystartrekwaskindofsuperbonce,butnowitisacopiesofacopyofacopyF(s,“Negative”)=1.00F(s,“Negative”)=0.81F(s,“Negative”)=1.00r:20.64r:4.23SemS:0.96SemS:0.98PCC:-0.57PCC:0.82A.5AdditionalARResultsAsdescribedinthemainbodyofourpaper,weplotthePearsonCorrelationCoeﬃcientbetweenoriginalandadversarialattributionvaluesofthewords(1stcolumnfromleft),theestimatedrobustnessconstantsr(2ndcolumnfromleft)aswellasthesemanticsimilaritiesbetweenunperturbedandperturbedinputtexts,theperplexityincreaseandtheincreaseinnumberofgrammaticalerrors(3rdand4thcolumnfromleft)afterperturbation.Weconsiderahighestimatedrobustnessconstantrassuccessfulattack,thuslowPCCvaluesaccompaniedbyhighsemanticsimilarities,lowperplexityincreasevaluesandgrammaticalerrors.Basedonthegraphbelow,weconcludethatCEAconsistentlyyieldshigherestimatedrobustnessconstantsrthanthereferencemethodTEF,duetolowerPearsoncorrelationbetweenadversarialandoriginalattributionmaps,highersemanticsimilaritiesandsmallerperplexityincreasesafterapplyingtheadversarialperturbations.A.5.1AG’sNewsCNN-SaliencyMaps(S)onAG’sNews0.16ρ−1−0.500.510.16ρ02460.16ρ0.60.810.16ρ−2024CNN-IntegratedGradients(IG)onAG’sNews0.16ρ−1−0.500.510.16ρ02460.16ρ0.60.810.16ρ−2024
PCCrUSErMiniLMrPPSemSUSESemSMiniLM∆PPGECEATEF20UnderreviewassubmissiontoTMLRLSTM-SaliencyMaps(S)onAG’sNews0.16ρ−1−0.500.510.16ρ02460.16ρ0.60.810.16ρ−2024LSTM-IntegratedGradients(IG)onAG’sNews0.16ρ−1−0.500.510.16ρ024680.16ρ0.60.810.16ρ−2024LSTMAtt-SaliencyMaps(S)onAG’sNews0.16ρ−1−0.500.510.16ρ02460.16ρ0.60.810.16ρ−2024LSTMAtt-IntegratedGradients(IG)onAG’sNews0.16ρ−1−0.500.510.16ρ024680.16ρ0.60.810.16ρ−2024LSTMAtt-Self-Attention(A)onAG’sNews0.16ρ−1−0.500.510.16ρ024680.16ρ0.60.810.16ρ−2024BERT-SaliencyMaps(S)onAG’sNews0.16ρ−1−0.500.510.16ρ02460.16ρ0.60.810.16ρ−2024
PCCrUSErMiniLMrPPSemSUSESemSMiniLM∆PPGECEATEF21UnderreviewassubmissiontoTMLRBERT-IntegratedGradients(IG)onAG’sNews0.16ρ−1−0.500.510.16ρ02468100.16ρ0.60.810.16ρ−2024BERT-Self-Attention(A)onAG’sNews0.16ρ−1−0.500.510.16ρ024680.16ρ0.60.810.16ρ−2024RoBERTa-SaliencyMaps(S)onAG’sNews0.16ρ−1−0.500.510.16ρ0240.16ρ0.60.810.16ρ−2024RoBERTa-Self-Attention(A)onAG’sNews0.16ρ−1−0.500.510.16ρ02460.16ρ0.60.810.16ρ−2024XLNet-SaliencyMaps(S)onAG’sNews0.16ρ−1−0.500.510.16ρ02460.16ρ0.60.810.16ρ−2024
PCCrUSErMiniLMrPPSemSUSESemSMiniLM∆PPGECEATEF
22UnderreviewassubmissiontoTMLRXLNet-IntegratedGradients(IG)onAG’sNews0.16ρ−1−0.500.510.16ρ024680.16ρ0.60.810.16ρ−2024XLNet-Self-Attention(A)onAG’sNews0.16ρ−1−0.500.510.16ρ02460.16ρ0.60.810.16ρ−2024
PCCrUSErMiniLMrPPSemSUSESemSMiniLM∆PPGECEATEFA.5.2MRCNN-SaliencyMaps(S)onMR0.16ρ−1−0.500.510.16ρ024680.16ρ0.60.810.16ρ−2024CNN-IntegratedGradients(IG)onMR0.16ρ−1−0.500.510.16ρ05100.16ρ0.60.810.16ρ−2024LSTM-SaliencyMaps(S)onMR0.16ρ−1−0.500.510.16ρ024680.16ρ0.60.810.16ρ−2024
PCCrUSErMiniLMrPPSemSUSESemSMiniLM∆PPGECEATEF23UnderreviewassubmissiontoTMLRLSTM-IntegratedGradients(IG)onMR0.16ρ−1−0.500.510.16ρ05100.16ρ0.60.810.16ρ−2024LSTMAtt-SaliencyMaps(S)onMR0.16ρ−1−0.500.510.16ρ012340.16ρ0.60.810.16ρ−2024LSTMAtt-IntegratedGradients(IG)onMR0.16ρ−1−0.500.510.16ρ02460.16ρ0.60.810.16ρ−2024LSTMAtt-Self-Attention(A)onMR0.16ρ−1−0.500.510.16ρ0240.16ρ0.60.810.16ρ−2024BERT-SaliencyMaps(S)onMR0.16ρ−1−0.500.510.16ρ012340.16ρ0.60.810.16ρ−2024BERT-IntegratedGradients(IG)onMR0.16ρ−1−0.500.510.16ρ02460.16ρ0.60.810.16ρ−2024
PCCrUSErMiniLMrPPSemSUSESemSMiniLM∆PPGECEATEF24UnderreviewassubmissiontoTMLRBERT-Self-Attention(A)onMR0.16ρ−1−0.500.510.16ρ02460.16ρ0.60.810.16ρ−2024RoBERTa-SaliencyMaps(S)onMR0.16ρ−1−0.500.510.16ρ012340.16ρ0.60.810.16ρ−20246RoBERTa-Self-Attention(A)onMR0.16ρ−1−0.500.510.16ρ02460.16ρ0.60.810.16ρ−2024XLNet-SaliencyMaps(S)onMR0.16ρ−1−0.500.510.16ρ012340.16ρ0.60.810.16ρ−2024XLNet-IntegratedGradients(IG)onMR0.16ρ−1−0.500.510.16ρ02460.16ρ0.60.810.16ρ−2024
PCCrUSErMiniLMrPPSemSUSESemSMiniLM∆PPGECEATEF
25UnderreviewassubmissiontoTMLRXLNet-Self-Attention(A)onMR0.16ρ−1−0.500.510.16ρ012340.16ρ0.60.810.16ρ−2024
PCCrUSErMiniLMrPPSemSUSESemSMiniLM∆PPGECEATEFA.5.3IMDBCNN-SaliencyMaps(S)onIMDB0.16ρ−1−0.500.510.16ρ0240.16ρ0.60.810.16ρ05CNN-IntegratedGradients(IG)onIMDB0.16ρ−1−0.500.510.16ρ05100.16ρ0.60.810.16ρ−4−20246LSTM-SaliencyMaps(S)onIMDB0.16ρ−1−0.500.510.16ρ02460.16ρ0.60.810.16ρ05LSTM-IntegratedGradients(IG)onIMDB0.16ρ−1−0.500.510.16ρ05100.16ρ0.60.810.16ρ−202468
PCCrUSErMiniLMrPPSemSUSESemSMiniLM∆PPGECEATEF26UnderreviewassubmissiontoTMLRLSTMAtt-SaliencyMaps(S)onIMDB0.16ρ−1−0.500.510.16ρ02468100.16ρ0.60.810.16ρ05LSTMAtt-IntegratedGradients(IG)onIMDB0.16ρ−1−0.500.510.16ρ05100.16ρ0.60.810.16ρ−20246LSTMAtt-Self-Attention(A)onIMDB0.16ρ−1−0.500.510.16ρ024680.16ρ0.60.810.16ρ05BERT-SaliencyMaps(S)onIMDB0.16ρ−1−0.500.510.16ρ05100.16ρ0.60.810.16ρ−4−2024BERT-IntegratedGradients(IG)onIMDB0.16ρ−1−0.500.510.16ρ05100.16ρ0.60.810.16ρ−4−2024BERT-Self-Attention(A)onIMDB0.16ρ−1−0.500.510.16ρ05100.16ρ0.60.810.16ρ−4−2024
PCCrUSErMiniLMrPPSemSUSESemSMiniLM∆PPGECEATEF27UnderreviewassubmissiontoTMLRRoBERTa-SaliencyMaps(S)onIMDB0.16ρ−1−0.500.510.16ρ02468100.16ρ0.60.810.16ρ−20246RoBERTa-IntegratedGradients(IG)onIMDB0.16ρ−1−0.500.510.16ρ05100.16ρ0.60.810.16ρ−2024RoBERTa-Self-Attention(A)onIMDB0.16ρ−1−0.500.510.16ρ024680.16ρ0.60.810.16ρ−2024XLNet-SaliencyMaps(S)onIMDB0.16ρ−1−0.500.510.16ρ02468100.16ρ0.60.810.16ρ−2024XLNet-IntegratedGradients(IG)onIMDB0.16ρ−1−0.500.510.16ρ05100.16ρ0.60.810.16ρ−2024XLNet-Self-Attention(A)onIMDB0.16ρ−1−0.500.510.16ρ05100.16ρ0.60.810.16ρ−2024
PCCrUSErMiniLMrPPSemSUSESemSMiniLM∆PPGECEATEF28UnderreviewassubmissiontoTMLRA.5.4YelpCNN-SaliencyMaps(S)onYelp0.16ρ−1−0.500.510.16ρ02460.16ρ0.60.810.16ρ−505CNN-IntegratedGradients(IG)onYelp0.16ρ−1−0.500.510.16ρ05100.16ρ0.60.810.16ρ−4−20246LSTM-SaliencyMaps(S)onYelp0.16ρ−1−0.500.510.16ρ024680.16ρ0.60.810.16ρ05LSTM-IntegratedGradients(IG)onYelp0.16ρ−1−0.500.510.16ρ0510150.16ρ0.60.810.16ρ05LSTMAtt-SaliencyMaps(S)onYelp0.16ρ−1−0.500.510.16ρ02468100.16ρ0.60.810.16ρ05
PCCrUSErMiniLMrPPSemSUSESemSMiniLM∆PPGECEATEF
29UnderreviewassubmissiontoTMLRLSTMAtt-IntegratedGradients(IG)onYelp0.16ρ−1−0.500.510.16ρ05100.16ρ0.60.810.16ρ−4−20246LSTMAtt-Self-Attention(A)onYelp0.16ρ−1−0.500.510.16ρ02468100.16ρ0.60.810.16ρ05BERT-SaliencyMaps(S)onYelp0.16ρ−1−0.500.510.16ρ02468100.16ρ0.60.810.16ρ−4−2024BERT-IntegratedGradients(IG)onYelp0.16ρ−1−0.500.510.16ρ05100.16ρ0.60.810.16ρ−4−2024BERT-Self-Attention(A)onYelp0.16ρ−1−0.500.510.16ρ05100.16ρ0.60.810.16ρ−4−2024RoBERTa-SaliencyMaps(S)onYelp0.16ρ−1−0.500.510.16ρ02468100.16ρ0.60.810.16ρ−20246
PCCrUSErMiniLMrPPSemSUSESemSMiniLM∆PPGECEATEF30UnderreviewassubmissiontoTMLRRoBERTa-IntegratedGradients(IG)onYelp0.16ρ−1−0.500.510.16ρ05100.16ρ0.60.810.16ρ−2024RoBERTa-Self-Attention(A)onYelp0.16ρ−1−0.500.510.16ρ05100.16ρ0.60.810.16ρ−2024XLNet-SaliencyMaps(S)onYelp0.16ρ−1−0.500.510.16ρ024680.16ρ0.60.810.16ρ−2024XLNet-IntegratedGradients(IG)onYelp0.16ρ−1−0.500.510.16ρ05100.16ρ0.60.810.16ρ−2024
PCCrUSErMiniLMrPPSemSUSESemSMiniLM∆PPGECEATEFA.5.5FakeNewsCNN-SaliencyMaps(S)onFakeNews0.16ρ−1−0.500.510.16ρ02460.16ρ0.60.810.16ρ010
PCCrUSErMiniLMrPPSemSUSESemSMiniLM∆PPGECEATEF31UnderreviewassubmissiontoTMLRCNN-IntegratedGradients(IG)onFakeNews0.16ρ−1−0.500.510.16ρ0510150.16ρ0.60.810.16ρ010LSTM-SaliencyMaps(S)onFakeNews0.16ρ−1−0.500.510.16ρ0510150.16ρ0.60.810.16ρ051015LSTM-IntegratedGradients(IG)onFakeNews0.16ρ−1−0.500.510.16ρ051015200.16ρ0.60.810.16ρ051015LSTMAtt-SaliencyMaps(S)onFakeNews0.16ρ−1−0.500.510.16ρ0510150.16ρ0.60.810.16ρ051015LSTMAtt-Self-Attention(A)onFakeNews0.16ρ−1−0.500.510.16ρ0510150.16ρ0.60.810.16ρ051015BERT-SaliencyMaps(S)onFakeNews0.16ρ−1−0.500.510.16ρ05100.16ρ0.60.810.16ρ05
PCCrUSErMiniLMrPPSemSUSESemSMiniLM∆PPGECEATEF32UnderreviewassubmissiontoTMLRBERT-IntegratedGradients(IG)onFakeNews0.16ρ−1−0.500.510.16ρ0510150.16ρ0.60.810.16ρ05BERT-Self-Attention(A)onFakeNews0.16ρ−1−0.500.510.16ρ0510150.16ρ0.60.810.16ρ−505RoBERTa-SaliencyMaps(S)onFakeNews0.16ρ−1−0.500.510.16ρ0510150.16ρ0.60.810.16ρ−20246RoBERTa-IntegratedGradients(IG)onFakeNews0.16ρ−1−0.500.510.16ρ0510150.16ρ0.60.810.16ρ−202468RoBERTa-Self-Attention(A)onFakeNews0.16ρ−1−0.500.510.16ρ0510150.16ρ0.60.810.16ρ−202468XLNet-SaliencyMaps(S)onFakeNews0.16ρ−1−0.500.510.16ρ05100.16ρ0.60.810.16ρ−2024
PCCrUSErMiniLMrPPSemSUSESemSMiniLM∆PPGECEATEF33UnderreviewassubmissiontoTMLRXLNet-IntegratedGradients(IG)onFakeNews0.16ρ−1−0.500.510.16ρ0510150.16ρ0.60.810.16ρ−202468XLNet-Self-Attention(A)onFakeNews0.16ρ−1−0.500.510.16ρ0510150.16ρ0.60.810.16ρ0510
PCCrUSErMiniLMrPPSemSUSESemSMiniLM∆PPGECEATEF
34