Published in Transactions on Machine Learning Research (01/2025)
Cost-Efficient Online Decision Making:
A Combinatorial Multi-Armed Bandit Approach
Arman Rahbar∗armanr@chalmers.se
Chalmers University of Technology and University of Gothenburg
Niklas Åkerblom∗niklas.akerblom@volvocars.com
Volvo Car Corporation
Chalmers University of Technology and University of Gothenburg
Morteza Haghir Chehreghani morteza.chehreghani@chalmers.se
Chalmers University of Technology and University of Gothenburg
Reviewed on OpenReview: https: // openreview. net/ forum? id= vZGZIIgcG4
Abstract
Online decision making plays a crucial role in numerous real-world applications. In many
scenarios, the decision is made based on performing a sequence of tests on the incoming
data points. However, performing all tests can be expensive and is not always possible.
In this paper, we provide a novel formulation of the online decision making problem based
on combinatorial multi-armed bandits and take the (possibly stochastic) cost of performing
tests into account. Based on this formulation, we provide a new framework for cost-efficient
online decision making which can utilize posterior sampling or BayesUCB for exploration.
We provide a theoretical analysis of Thompson Sampling for cost-efficient online decision
making, and present various experimental results that demonstrate the applicability of our
framework to real-world problems.
1 Introduction
Online decision making (ODM) is concerned with interactions of an agent with an environment through a
series of sequential tests in order to acquire sufficient information about the environment and successively
learn how to make better decisions. Each such test and decision can be associated with either a reward or a
cost. Here, we focus on problem settings where tests may incur stochastic costs.
One such example is medical diagnosis. A patient arrives at a hospital with an unknown affliction. In order
to determine an appropriate treatment for the patient, a number of medical tests need to be performed.
Since each of these tests might take time (which can differ depending on the test result and the underlying
affliction), be expensive or cause discomfort to the patient, it is desirable to limit the set of tests to the
most informative ones. On the other hand, it is apparent that misdiagnosing the patient can have severe
consequences, so performing too few tests is also undesirable.
Another example is adaptive identification of driver preferences for in-car navigation systems. Consider the
selection of charging stations during long-distance trips with battery-electric vehicles (BEVs). Since the
duration of a single charging stop can be as long as 40 minutes, it may be desirable to select a charging
station with additional amenities in close proximity, such as restaurants, shops, or public restrooms. While
it is possible to ask questions through the vehicle user interface of drivers to learn their charging location
*These authors contributed equally to this work.
1Published in Transactions on Machine Learning Research (01/2025)
preferences, such questions should be kept few, short, and simple. We note that in both of these examples,
the problem instances (e.g., diagnosis of patients and identification of driver preferences) arrive incrementally
in anonlinemanner and should be processed accordingly.
We take a novel view of the online decision making problem described above, where we cast it as a combi-
natorial multi-armed bandit problem (CMAB) (Cesa-Bianchi & Lugosi, 2012), an extension of the classical
multi-armed bandit problem (MAB). The standard MAB problem is a common way of representing the
trade-off inherent to decision making problems between exploring an environment by performing successive
actions (playing arms, in bandit nomenclature) to gain more knowledge and exploiting that knowledge to
reach a long-term objective. In a CMAB problem, an agent interacts with the environment in each time step
by performing a set of actions (a super arm ), where the set may be subject to combinatorial constraints.
Each individual action ( base arm ) in that set may result in some observed feedback (so-called semi-bandit
feedback), and the agent also receives a reward associated with the complete super arm.
In this work, we view the combined choice of tests to perform and the selected decision as a super arm in a
CMAB problem where the underlying cost distribution parameters are initially unknown, which allows us to
leverage effective MAB exploration strategies like Thompson Sampling (TS) (Thompson, 1933) and Upper
Confidence Bound (UCB) (Auer et al., 2002) together with cost-efficient (active) information acquisition
methods, such as Equivalent Class Edge Cutting (EC2) (Golovin et al., 2010) and Information Gain (IG)
(Dasgupta, 2005). We summarize our contributions as follows.
1. We study the novel problem of outcome-dependent cost-efficient online decision making, where costs
may be stochastic and depend on both test and decision outcomes.
2. We propose a novel combinatorial semi-bandit framework for our online decision making problem.
This formulation involves an elegant definition of base arms and super arms, providing a new per-
spective on cost-efficient information acquisition while simultaneously exploring the concept of
interactive CMAB oracles.
3. We adapt a number of bandit methods to this framework, namely Thompson Sampling and Upper
Confidence Bound, combined with novel extensions of EC2and IG which can handle tests with
different and stochastic costs, which we call W-EC2and W-IG, respectively. Thereby, in addition
to Thompson Sampling, we develop W-EC2and W-IG oracles utilizing a Bayesian variant of UCB
(called BayesUCB by Kaufmann et al. 2012) that fits well to our framework and enables the UCB
method to effectively employ prior knowledge in the form of a prior distribution.
4. We demonstrate the effectiveness of our framework via several experimental studies performed on
data sets from a variety of important domains. We find that Thompson Sampling yields the best
performance in most of the experiments we perform.
5. We theoretically analyze the performance of Thompson Sampling within our framework for cost-
efficient online decision making and establish an upper bound on its Bayesian regret.
2 Related Work
Cost-efficient information acquisition. The primary goal of cost-efficient information acquisition (also
called Bayesian active learning in the literature) algorithms is to sequentially select from a number of avail-
able costly tests to make a decision (such as making a prediction about a label) with a minimal cost. To
minimize cost, these tests are performed until a sufficient level of confidence in the decision is reached.
Prominent algorithms for cost-efficient information acquisition are Information Gain (IG) (originally devel-
opedbyLindley1956), EC2(Golovinetal.,2010), andUncertaintySampling(US).BayesianActiveLearning
by Disagreement (BALD) (Houlsby et al., 2011) proposes an equivalent formulation of the underlying util-
ity function in IG which provides practical and computational advantages. EC2has been proved to enjoy
adaptive submodularity (Golovin et al., 2010), and thus yields near-optimal cost. Acquisition functions in
Bayesian optimization have also been used in active learning. Probability of Improvement (PI) (Kushner,
1964) and Expected Improvement (EI) (Jones et al., 1998) are two important acquisition functions used in
2Published in Transactions on Machine Learning Research (01/2025)
active learning. Both PI and EI use surrogate models (e.g., Gaussian Processes) to select the next sample
to query (see Di Fiore et al. (2024) for more details). The works of Rahbar et al. (2023); Chen et al. (2017)
consider active information acquisition in an online setting. The regret bounds in these works are based on
formulating the problem as a Partially Observable Markov Decision Process (POMDP) and are exponential
in the total number of possible tests. In this work, we consider a CMAB formulation of the problem and
derive a regret bound that is linearin the number of tests (and sublinear w.r.t. the horizon T). In addition,
unlike Rahbar et al. (2023); Chen et al. (2017), our framework supports variability in the cost of the tests
depending on the outcomes of the tests and the decisions.
Combinatorial semi-bandit algorithms. CMAB methods have been utilized to address online learning
problems in various settings but often without considering cost-efficient information acquisition. Durand &
Gagné (2014) adapt a combinatorial variant of Thompson Sampling for online feature selection where the
agent has a fixed budget for tests and the reward is a linear function of the test outcomes. Both assumptions
are used in later works studying the problem from a CMAB perspective (Bouneffouf et al., 2017; 2021), but
differ from the setting considered in this work (e.g., the budget for information acquisition is fixed with no
concept of achieving a decision, and the costs are independent of the outcomes of the tests and the decisions).
Wang & Chen (2018) analyze the regret of combinatorial Thompson sampling, but their proof technique is
not directly applicable to our problem since they do not allow changes in the set of available super arms.
Therefore, in this work, we adopt the approach originally developed by Russo & Van Roy (2014) for standard
and linear MAB. Finally, it is notable that the analysis of Wang & Chen (2018) yields an instance dependent
regret bound, whereas our analysis is focused on deriving Bayesian and instance-independent bounds.
Online decision making. Our work relates to the theoretical online decision making framework presented
by Cesa-Bianchi et al. (2021). Their framework aims to maximize total Return on Investment (ROI) in a
sequence of decision making tasks, where each task involves accepting or rejecting an innovation. The au-
thors provide an algorithm to learn ROI-maximizing decision making policies, with theoretical guarantees of
convergence to an optimal policy. In the framework, accept/reject decisions are made by drawing i.i.d. sam-
ples from a probability distribution modeling the value of an innovation. In contrast, our approach involves
making low-cost decisions by performing a sequence of different tests (with different costs). Furthermore,
our work is not limited to binary accept/reject decisions.
Algorithm 1 Online decision making protocol
1:{x1,x2,...,xn}: available tests
[m]: possible decisions
A={aij|i∈[n],j∈[m]}: set of base arms
µ(0)
ijandµ(1)
ij: costs of base arm aij(given test outcomes 0and1, respectively)
θij=Pr(xi= 1|y=j)
2:fort = 1, 2, ..., T do
3:Receive problem instance x(t).
4:for allselected tests x(t)
i(by an oracle) do
5:Observe outcome of test x(t)
i∼Bernoulli (θij).
6:end for
7:Make a decision y(t)=j∈[m]based on test outcomes.
8:Super arm S(t)={aij∈A|testx(t)
iis performed}∈I(t)whereI(t)⊆2Ais set of feasible super arms.
9:Receive reward R(S(t))where E[R(S(t))] =−/summationtext
ij∈S(t)µijandµij=µ(1)
ij×θij+µ(0)
ij×(1−θij).
10:end for
3 Problem Formulation
In this section, we propose our new formulation of the online decision making problem which incorporates
the cost of acquiring test outcomes. In each time step t, we receive the problem instance (e.g., data point)
x(t)withntests{x(t)
1,x(t)
2,...,x(t)
n}. Here, we assume that all test outcomes (unknown until after each test
3Published in Transactions on Machine Learning Research (01/2025)
is performed) are binary (i.e., x(t)
i∈{0,1}for alli), but our methods can easily be extended to other test
values (see Section 6.3). The goal is to make an accurate decision y(t)∈[m]for that problem instance with
a minimal cost of performing tests ( mrepresents the total number of options for the decision, or outcomes
if the correct decision is viewed as a random variable), where we denote the correct decision by y∗. As
an example, let’s consider the medical problem of diagnosing lung cancer. At each time step t, a patient
x(t)arrives, and our goal is to make a decision (prediction) y(t)∈{1(benign ),2(malignant ),3(infection )}
(m= 3). In this problem, there are several tests that can be performed to reach a decision. For instance,
available tests can include a CT1scan, a PET2scan, genetic testing and a biopsy ( n= 4). These tests can
have positive or negative outcomes (test results), which will be used in the decision making process.
Consistent with the common Naïve Bayes assumption, we let all tests x(t)
ibe mutually independent and
Bernoulli-distributed conditional on the decision y(t). The Naïve Bayes assumption is common for sequential
decision making, even in the offline setting, and is consistent with previous works including Chen et al.
(2017); Rahbar et al. (2023). This assumption is needed for computing the gains in our approximate oracles
defined in Section 4.1 (W-IG and W-EC2). We also let y∗be known given a full realization of x(t). This
implies that upon performing all available tests, we know the correct decision for the problem instance. This
is realistic and consistent with the settings of Golovin et al. (2010); Chen et al. (2017); Rahbar et al. (2023),
though they do not follow a CMAB formulation. As an example, consider a medical diagnosis problem. This
property means that if we perform all available medical tests, we can determine the correct diagnosis.
Regarding the costs, we assume that they directly depend on the outcomes of the tests and the decisions.
As motivation, again consider the medical diagnosis problem. Here, the costs (e.g., time and patient’s
discomfort) of medical tests can vary based on the underlying affliction or the outcome of the test itself.
For example, in the case of diagnosing cancer through biopsy procedures, the cost of the biopsy can change
based on the specific underlying condition and the outcome of the test. If the suspected cancerous tissue
can be accessed easily and shows a positive result for cancer cells, the biopsy procedure may cause lower
costs in terms of time and discomfort. Conversely, if the tissue is in a challenging area or requires multiple
biopsies, the procedure’s complexity may cause higher costs, both in terms of time and increased patient’s
discomfort.
Wemodeltheproblemdescribedabovebyanelegantdesignofastochasticcombinatorialmulti-armedbandit
(CMAB) problem (Cesa-Bianchi & Lugosi, 2012) with semi-bandit feedback. Briefly, CMAB corresponds to
a reward maximization problem with a set of base arms. In each time step, the agent selects a subset of
base arms. After playing this subset of base arms, called a super arm , the agent receives a reward from the
environment. The objective is to maximize the expected sum of rewards within a time horizon T, which is
typically formulated as a regret minimization task.
Perform tests
Tests 
Decisions
Base armsMake decision
Super arm
Figure 1: Illustration of base arms and super arms in our problem formulation. In this example, we have
four tests ( n= 4) and two possible decisions ( m= 2). Based on these tests and decisions, we have 4×2
possible base arms A={aij|i∈[4],j∈[2]}. If we perform tests with indices 2,4,1, and we make decision
y= 2(based on the test results), then the super arm will be S={a22,a42,a12}.
1Computed Tomography
2Positron Emission Tomography
4Published in Transactions on Machine Learning Research (01/2025)
In our problem, since the costs of tests depend on both test and decision outcomes, we need to consider a set
of base arms that reflect both. Therefore, we define the set Aofn×mbase armsaij(i∈[n]andj∈[m]).
In each time step t, the agent selects a super arm S(t)∈2Aand receives the reward R(S(t)) =R(t). A base
armaijinS(t)indicates that the test with index iis performed for making decision y(t)=j(see Figure
1). We defineI(t)⊆2Aas the set of feasiblesuper arms at time twhich yield a decision, i.e., each feasible
super arm consists of tests and a single decision (concretely, if aij,alk∈S∈I(t), thenj=k). The feedback
at timetof a base arm aij∈A(observable by the agent if aij∈S(t)) is the corresponding outcome of test
x(t)
i, which given a decision y(t)=jis Bernoulli-distributed (test outcomes are binary) with an unknown
(to the agent) parameter θij≜Pr(xi= 1|y=j), for alli,j. While tests may, in practice, be selected
sequentially, the super arm (and base arms) are only known in hindsight after the decision has been made.
This is intentional and allows us to analyze the problem as a CMAB.
We let the cost (assumed to be fixed and known) of each base arm aijbeµ(0)
ij∈[0,1]andµ(1)
ij∈[0,1](given
test outcomes 0and1, respectively). Then, we define the expected reward of the selected feasible super arm
S(t)as the negative sum of expected base arm costs, such that E[R(S(t))] =−/summationtext
ij∈S(t)µij, where
µij≜µ(1)
ij×θij+µ(0)
ij×(1−θij), (1)
with the vector of all µij’s denoted by µand the vector of all θij’s denoted by θ. We let µ∗be the true
underlying mean cost vector, with corresponding parameter vector θ∗. This is a novel problem; the previous
studies are special cases of this problem with µ(0)
ij=µ(1)
ij=ci, whereciis the cost of performing test iand
is independent of test outcome or the decision. We show an outline of our online decision making protocol
in Algorithm 1.
We define the suboptimality gap at time tof a feasible super arm Sas
∆(t)
µ∗(S)≜fµ∗(S∗,(t))−fµ∗(S), (2)
wherefγ(M)≜(−/summationtext
ij∈Mγij)denotes the expected reward of super arm Mgiven an arbitrary mean cost
vector γ, andS∗,(t)≜argmaxS∈I(t)fµ∗(S)is the optimal super arm found by the oracle applied on the
feasible setI(t). Then, the regret(which the agent should minimize) for a time horizon Tis defined as
Regretµ∗(T)≜/summationdisplay
t∈[T]∆(t)
µ∗(S(t)). (3)
4 CMAB Methods for ODM
In our framework, we utilize oracles(i.e., optimization algorithms) which, given a parameter vector θ, output
super arms with maximum expected reward (or approximately, in the case of approximate oracles), such
that
Oracle (θ)≜argmax
S∈I(t)E[R(S)|θ]. (4)
Theθ∗
ijvaluesofthetrueparametervector θ∗areinitiallyunknown,buttobeabletoemploypriorknowledge,
we consider a prior distribution over θ∗
ij’s. Formally, we assume that each θ∗
ijfollows a Beta distribution,
Beta(α(0)
ij,β(0)
ij). The reason is that a Beta distribution is defined on range [0,1], and parameter of a Bernoulli
distribution should be in this range (each θ∗
ijis parameter of a Bernoulli distribution). We also assume a
fixed and known distribution over decision outcomes, denoted by P(y)(i.e., the learning agent knows the
marginal distribution a priori). However, even if the oracle is assumed to always be able to find the correct
decision (i.e., it only needs to minimize the number of tests), the optimization problem is NP-hard, and thus
in practice, we use approximate oracles (introduced in Section 4.1).
Agreedymethod to address the problem defined in Section 3 is that in each time step t, we use an oracle
with the current (MAP) estimate of θ∗. However, this greedy method may converge to a sub-optimal super
arm due to lack of exploration. To balance exploration and exploitation, we adapt Thompson Sampling and
BayesUCB to our problem.
5Published in Transactions on Machine Learning Research (01/2025)
Thompson Sampling. Algorithm 2 summarizes our framework, with Thompson Sampling used as explo-
ration method. In each time step t, we sample the parameters from the current posterior distribution (line
2). We then (in lines 3-4) use the oracleto get a super arm based on the sampled parameters, and perform
the tests sequentially. Finally, we use Algorithm 3 to update the posterior distribution of the parameters.
Algorithm 3 uses the fact that Beta distribution is conjugate prior to the Bernoulli distribution.3
BayesUCB. To utilize BayesUCB for exploration instead of Thompson Sampling, we have to modify lines
2-3 of Algorithm 2 such that instead of sampling parameters from the posterior distribution, we retrieve
optimistic parameter estimates. Like Kaufmann et al. (2012) (done for classical bandits), we achieve this
by utilizing quantiles of the posterior distribution. However, since neither gain function (Eq. 7 or Eq.
11) of the approximate oracles introduced in this section is monotonically increasing w.r.t. the provided
parameter vector θ, a high value of a base arm parameter θijdoes not necessarily mean that test iis more
likely to be selected. Hence, with Qλ(η)denoting the quantile function (such that under the distribution
λ, Pr(X≤Qλ(η)) =η), we compute both an upper confidence bound θ(t)
ij=QBeta(α(t)
ij,β(t)
ij)(1−1/t)and a
lower confidence bound θ(t)
ij=QBeta(α(t)
ij,β(t)
ij)(1/t)for each base arm aij, which we pass to the oracles. Then,
while computing Eq. 7 and Eq. 11, whenever a high value for a term involving θijmakes the algorithm more
likely to select test i, we useθ(t)
ijas an optimistic estimate, and correspondingly, θ(t)
ijfor terms making iless
likely to be selected.
Algorithm 2 Cost-efficient online decision making
Require:P(y);µ; prior parameters (α(0)
ij,β(0)
ij)’s
1:fort = 1, 2, ..., T do
2:Sampleθ(t)
ijfrom Beta(α(t−1)
ij,β(t−1)
ij)for alli,j.
3:S(t)←Oracle (θ(t))(e.g., Algorithm 4).
4:Make decision based on S(t), and observe x(t)
ifor allisuch thataij∈S(t).
5:Observe correct decision y(t).
6:Call Algorithm 3 and obtain Beta(α(t)
ij,β(t)
ij)for alli,j.
7:end for
Algorithm 3 Posterior update
Require:S(t);x(t)
ifor allisuch thataij∈S(t);y(t)=k;α(t−1)
ij’s andβ(t−1)
ij’s
1:foreachx(t)
ido
2:ifx(t)
i= 1then
3:α(t)
ik←α(t−1)
ik+ 1
4:else
5:β(t)
ik←β(t−1)
ik+ 1
6:end if
7:end for
4.1 Cost-Efficient Approximate Oracles
Solving the optimization problem in Eq. 4 is intractable in general (see, e.g., Golovin et al. 2010; Chakar-
avarthy et al. 2007). Therefore, to implement line 3 of Algorithm 2, we propose two different approximate
oraclesusing algorithms that greedily select tests to optimize a surrogate objective function, Information
Gain(IG) (Zheng et al., 2012) and EC2(Golovin et al., 2010). We introduce extensions of IG and EC2
to handle tests with stochastic costs, which we call Weighted IG (W-IG) and Weighted EC2(W-EC2) re-
spectively. In both of these, all probabilities involved are computed using the fixed and known decision
3Since test outcomes are binary, then, given decision y=j, each testxifollows a Bernoulli distribution with parameter θij.
6Published in Transactions on Machine Learning Research (01/2025)
distribution P(y)and the conditional distribution over test outcomes P(x|y)(determined by a given pa-
rameter vector θ, see line 3 in Algorithm 2). In particular, the dependence on θis omitted in the following
section to simplify notation.
Before introducing W-IG and W-EC2, we need to define the notions of hypothesis anddecision region . We
call a full realization of test outcomes (i.e., of all possible tests) a hypothesis. Therefore, with nbinary tests,
we have 2ndifferent possible hypotheses. Let H≜{0,1}nbe the set of all possible hypotheses for ntests.
We can partition Hintomdisjoint sets. We call each of these disjoint sets a decision region . Each decision
region corresponds to a decision y∈[m]. So our objective will be finding the correct decision region, for
which the cost of performing tests is low. We want to emphasize that one important difference between
these oracles and standard CMAB oracles is that they are interactive . In other words, during a single time
step, they sequentially perform tests and observe outcomes to select subsequent tests and make a decision.
To our knowledge, our work is the first work which utilizes this approach for CMABs.
Weighted IG algorithm (W-IG). In the standard IG algorithm, based on a set of previously observed
test outcomes, we select the test that maximizes the reduction of entropy in the decision regions. Formally,
the gain of a test iin the IG algorithm is defined as:
∆IG(i|xP)≜H(y|xP)−Exi|xP[H(y|xP∪{i})], (5)
wherePis the set of previously performed tests, and xPis the vector of results of tests in P. The random
variable for decision regions is denoted y, and H(z)is the Shannon entropy of a random variable z. In the
W-IG algorithm, we sequentially perform tests that maximize ∆IG(i|xP)divided with the expected cost
of testigiven the previously performed tests. In other words, after performing and observing the outcomes
of the tests inP, the next test to perform will be
i∗= argmax
i∈([n]\P)∆W-IG (i|xP), (6)
where
∆W-IG (i|xP)≜∆IG(i|xP)
Cost(i), (7)
and
Cost(i)≜/summationdisplay
j∈[m]/summationdisplay
q∈{0,1}µ(q)
ij×Pr(xi=q,y=j|xP), (8)
given test outcome costs µ(0)
ijandµ(1)
ijfor alli,j. We call the set of hypotheses Hi={h|P(xi|h)>0}
consistent with the outcome of test i. We continue performing tests until all hypotheses in/intersectiontext
i∈PHibelong to
the same decision region, i.e., there is only one decision region remaining.
Weighted EC2algorithm (W-EC2).The standard EC2algorithm starts with a graph with Has the
set of nodes. We have an edge (h,h′)between two hypotheses handh′if and only if they do not belong
to the same decision region. The weight of an edge (h,h′)is set towhh′=P(h|xP)×P(h′|xP)where
P(·|xP)is the posterior distribution of a hypotheses after performing tests. Naturally, the weight of a set of
edgesEis defined as the sum of weights of edges in E, i.e.,w(E)≜/summationtext
(h,h′)∈Ewhh′. We say that performing
a testicutsan edge (h,h′)ifh /∈Hiorh′/∈Hi. We denote the set of edges cut by test iwithK(i).
We now define the objective function for the EC2algorithm as
fEC2(P)≜w/parenleftigg/uniondisplay
i∈PK(i)/parenrightigg
. (9)
Based on the objective function above, we define the EC2gain of a test ias
∆EC2(i|xP)≜Exi|xP[fEC2(P∪{i})−fEC2(P)], (10)
7Published in Transactions on Machine Learning Research (01/2025)
and, similar to W-IG, the W-EC2gain as
∆W-EC2(i|xP)≜∆EC2(i|xP)
Cost(i), (11)
with Cost (i)defined as in Eq. 8.
Also like with W-IG, in the W-EC2algorithm, we sequentially perform tests that maximize the gain
∆W-EC2(i|xP). We stop performing tests when all edges are cut, which means that we have only one
decision region left.
To be able to implement W-IG and W-EC2(calculate ∆W-IGand∆W-EC 2), similar to standard IG and EC2,
we assume that the outcomes of the tests are conditionally independent given the decision. We show the
test selection process in W-IG and W-EC2algorithms in Algorithm 4.
Algorithm 4 Cost-efficient approximate oracle
Require: Alg∈{W-IG,W-EC2};P(y);µ;θ
1:Enumerate hypotheses in H
2:P={}
3:whilemore than one decision region left do
4:Select next test (with ∆Alg(·)computed w.r.t. θ)i∗= argmax
i∈([n]\P)∆Alg(i|xP).
5:Perform test i∗and observe xi∗.
6:UpdateP(h|xP)based on results of tests in P.
7:end while
8:Select the decision associated to the remaining decision region.
5 Theoretical Analysis
In this section, we theoretically analyze the framework proposed in Section 4. Specifically, we provide an
upper bound on the Bayesian regret (defined below) of the online decision making procedure in Algorithm
2. The analysis technique that we use extends the approach by Russo & Van Roy (2014) to our setting
and yields an analysis which is significantly simpler than those by Chen et al. (2013) and Wang & Chen
(2018). Furthermore, in our problem, due to the interactive nature of the oracle, the set of available arms
changes in each time step. In general, a bandit problem with changing sets of available arms is called a
bandit with sleeping arms , a setting which is not considered in the analyses of combinatorial UCB (CUCB)
by Chen et al. (2013) and combinatorial Thompson sampling (CTS) by Wang & Chen (2018). Our analysis
holds for interactive oracles through the sleeping arms assumption. Moreover, the analyses of CUCB and
CTS yield instance-dependent regret bounds, whereas we focus on an instance-independent (and Bayesian)
bound, which also requires a different type of analysis (e.g., a different regret decomposition). Furthermore,
the analysis of CTS is tailored for one specific choice of uninformative prior, while ours, in principle, allows
for an arbitrary selection of the prior distribution (whether uninformative or informative).
In our problem setting, the set of feasible super arms I(t)is restricted such that y(t)=y∗(i.e., correct
decisions are guaranteed, and the objective is to minimize the cost of tests). This means that a super arm is
feasible if and only if the test outcomes determine the correct decision region.4We also consider access to
a perfect oraclewhich can solve the optimization problem Eq. 4. As mentioned earlier, the problem under
consideration is NP-hard. On the other hand, assuming access to an exact oracle is common for analyses of
regret bounds for CMABs, even in settings where the exact oracle needs to deal with an NP-hard problem.
For instance, the work of Hüyük & Tekin (2020) assumes access to exact oracles for NP-hard network
problems. Åkerblom et al. (2023) consider the intractable problem of stochastic bottleneck identification
in an online setting. In fact, via the elegant CMAB formulation of our problem (in contrast to, e.g., the
POMDP formulation of Rahbar et al. 2023 and Chen et al. 2017), we are able to obtain a regret bound that
is linear in the number of tests (rather than exponential).
4We relax this assumption in the experiments of Section 6.4.
8Published in Transactions on Machine Learning Research (01/2025)
The Bayesian regret is defined as the expected value of Regret (T)(see Eq. 3), with the expectation taken
over the prior distribution of µ∗and other random variables (e.g., randomness in the rewards and the policy
of the agent). Formally,
Bayesian Regret (T)≜E
/summationdisplay
t∈[T]∆(t)
µ∗(S(t))
. (12)
We then prove the following theorem which establishes an upper bound on the Bayesian regret of Thompson
Sampling applied to our framework.
Theorem 5.1. The Bayesian Regret of Algorithm 2 is O(mn√TlogT).
Proof.We begin our regret analysis by a decomposition of E[∆(t)
µ∗(S(t))]in the following lemma.
Lemma 5.2. For any upper confidence bound U(t):I(t)→Rand lower confidence bound L(t):I(t)→R,
E[∆(t)
µ∗(S(t))] =E[U(t)(S(t))−L(t)(S(t))] +E[L(t)(S(t))−fµ∗(S(t))] +E[fµ∗(S∗,(t))−U(t)(S∗,(t))].(13)
Proof.This lemma is based on Proposition 1 of Russo & Van Roy (2014). Given the history of played super
arms (and base arms) and their rewards, we know that in Thompson Sampling the optimal super arm S∗,(t)
and the played super arm S(t)follow the same distribution. Since U(t)(·)is a deterministic function, we
conclude that given the history, the expected values of U(t)(S(t))andU(t)(S∗,(t))are equal, and we have:
E[∆(t)
µ∗(S(t))] =E[U(t)(S(t))−fµ∗(S(t))] +E[fµ∗(S∗,(t))−U(t)(S∗,(t))],
and by adding and subtracting E[L(t)(S(t))]we prove Lemma 5.2.
Additionally, we define lower and upper confidence bounds as
L(t)(S)≜fˆµ(t−1)(S)−/summationdisplay
ij∈S/radicaligg
2 logT
N(t−1)(ij)(14)
U(t)(S)≜fˆµ(t−1)(S) +/summationdisplay
ij∈S/radicaligg
2 logT
N(t−1)(ij)(15)
where ˆµ(t)
ijis the average cost of base arm aijtill timet, andN(t)(ij)is the number of times base arm aij
has been played till time t. We continue the proof by bounding each term in Lemma 5.2. For the second
and third terms, we have the following lemma (where the proof is in Appendix B.1).
Lemma 5.3. E[L(t)(S(t))−fµ∗(S(t))]≤2mn
T,E[fµ∗(S∗,(t))−U(t)(S∗,(t))]≤2mn
T.
In order to bound the first term of the regret decomposition, we utilize the following lemma (where the
proof uses a similar technique as Lemma 8 by Åkerblom et al. (2023) for the combinatorial batched feedback
setting (with batch size 1). See Appendix B.3).
Lemma 5.4./summationtext
t∈[T]E[U(t)(S(t))−L(t)(S(t))]≤2mn√8TlogT.
Now, based on Lemma 5.2, Lemma 5.3, and Lemma 5.4 we have Bayesian Regret (T)≤4mn+
2mn√8TlogT=O(mn√TlogT).
We utilize the structure of the problem in our analysis and thereby, the bound derived in Theorem 5.1
is significantly tighter than the ones by Rahbar et al. (2023); Chen et al. (2017) which take a different
perspective than our CMAB formulation. Our bound is also consistent with Bayesian upper bounds for
combinatorial semi-bandit methods developed in other (standard) settings.
9Published in Transactions on Machine Learning Research (01/2025)
6 Experiments
In this section, we empirically validate our cost-efficient online decision making framework via various exper-
iments5. Unless otherwise specified, we consider Beta(2,2) as the prior distribution of all θ∗
ij’s. We employ
both of the cost-efficient approximate oracles introduced in Section 4.1 (W-IG and W-EC2) as the oracle
in Algorithm 2. We also employ the hypothesis enumeration procedure proposed by Chen et al. (2017) to
generate the most likely hypotheses for each decision region. In Section 6.3, we provide an extension of our
framework to real-valued non-binary test outcomes. Moreover, In Section 6.4, we demonstrate the applica-
bility of our framework in the setting where the decision regions for the set of hypotheses are not known.
We run each experiment with 5 different random seeds on a single-node CPU machine with 8 cores, 16GB
memory and macOS. We implement the algorithms in Python mainly using NumPy (Harris et al., 2020),
NetworkX (Hagberg et al., 2008) and SciPy (Virtanen et al., 2020).
Datasets. WeapplyourframeworktotheLEDdisplaydomaindatasetfromtheUCIrepository(Breiman
et al., 1988). Additionally, we use the ProPublica recidivism (Compas) data set (Larson et al., 2016) and the
Fair Isaac (Fico) credit risk data set (FICO et al., 2018), which both are preprocessed in the same way as
by Hu et al. (2019). The aforementioned data sets contain relatively large numbers of tests and few decision
outcomes, which correspond well to, e.g., the medical diagnosis example in Section 1. In applications like
the charging station selection example, it might instead be reasonable to expect a large number of decision
outcomes (individual charging stations or groups sharing certain characteristics) and relatively few tests
(questions). Hence, in order to thoroughly evaluate the latter type of setting, we create a synthetic data set
(called Navigation) with the desired characteristics. With n= 5tests andm= 20decision outcomes, we
sample parameters θ∗
ijfrom the prior distribution Beta(2,2) for each i∈[n]andj∈[m], which subsequently
use to generate the data set. Moreover, in Section 6.2, we present the application of our framework to a
real-world troubleshooting case study. Finally, to demonstrate the applicability of our methods to real-valued
test outcomes, we investigate the Breast Cancer Wisconsin data set (Street et al., 1993) (Section 6.3).
Algorithms. In our experiments, we investigate the cost of decision making using both W-IG and W-EC2
algorithms embedded in our framework (Algorithm 2). For each algorithm, we investigate both Thompson
Sampling and BayesUCB for exploration. In addition to Thompson Sampling (TS) and BayesUCB (BUCB),
we consider the following baselines for selecting tests in each time step:
Random Information Acquisition. Within our framework, we can use a random subset of tests decision
making. This baseline continues performing random tests until only one decision region is left (similar to
W-IG and W-EC2).
All.This baseline always performs all available tests, and does not consider the cost.
DPP.Determinantal Point Process (DPP) provides a way to select diverse sets. With this method, we use
exact sampling (Derezinski et al., 2019) (finite DPP with likelihood kernel) to subsample columns (tests).
We use the implementation of Gautier et al. (2019). To perform the sampling, in each time step t, we provide
all data points (all test results for all data points) until time t−1to the sampler. The number of selected
tests is related to the number of eigenvectors of the kernel matrix.
Table 1: Average cost ( ±standard deviation) of decision making in a time step (over the entire learning
period).
Data set \Algorithm W-EC2-TSW-EC2-BUCB W-IG-TS W-IG-BUCB Random All DPP
Navigation 1.746±0.030 1.746±0.030 1.859±0.020 1.867±0.023 1.975±0.008 2.305±0.000 2.300±0.002
LED 2.032±0.053 2.061±0.059 2.418±0.015 2.438±0.019 2.644±0.025 3.147±0.000 3.137±0.002
Fico 1.159±0.003 1.101±0.002 1.935±0.075 2.067±0.064 3.078±0.005 8.353±0.000 8.249±0.056
Compas 3.056±0.009 3.090±0.017 4.197±0.039 4.034±0.028 4.719±0.006 6.101±0.000 5.923±0.049
Breast cancer 1.437±0.019 1.554±0.032 2.091±0.048 2.213±0.136 6.376±0.08714.181±0.0009.286±0.077
Troubleshooting 6.207±0.013 7.615±0.034 12.010±0.07812.563±0.11723.236±0.07238.113±0.00024.538±0.309
5The source code for our experiments can be accessed here on GitHub.
10Published in Transactions on Machine Learning Research (01/2025)
Figure 2: Decision making costs of our framework applied to four different data sets.
6.1 Experimental Results
We first investigate the four data sets Navigation, LED, Fico, and Compas. As mentioned, to guarantee
accurate decision regions for all hypotheses, we implement the algorithm outlined by Chen et al. (2017)
for enumeration of hypotheses. In short, this algorithm produces the most probable hypotheses based on
a decision and the conditional probabilities of test outcomes associated with that decision. To assign the
correct decisions to hypotheses, we extract the parameter vector θfrom the entire data set and subsequently
utilize the enumeration procedure to generate the most likely hypotheses for each decision. However, the
parametersarenotknowntotheagentaftertheenumerationofhypotheses. Weassignfixedcosts µ(0)
ij∈[0,1]
andµ(1)
ij∈[0,1](randomly sampled from a uniform distribution) for each data set before performing our
experiments.
In Figure 2, we illustrate the cost6of performing tests (i.e., −E[R(S(t))]) during online learning for different
algorithms. The cost of each test is calculated based on Eq. 1, where the parameter θijis derived from
the data. The (maximum) numbers of enumerated hypotheses per decision region are 2, 5, 70 and 70 for
Navigation, LED, Fico and Compas datasets, respectively. Our results in Figure 2 show that our framework
yields the lowest information acquisition cost for all data sets. DPP has a low cost initially since it only
uses a few data points to perform sampling, but after a few time steps its cost becomes as high as ‘All’.
During these initial time steps DPP can make incorrect decisions. Additionally, we observe that the W-EC2
algorithm always has the lowest cost. This result is consistent with the theoretical results of Golovin et al.
(2010) for the cost of the original EC2algorithm in the offline setting. We also observe that the exploration
methods (TS and BayesUCB) exhibit very similar performances in terms of cost. During online learning, it is
observedthatthecostoftherandomalgorithmremainsrelativelyconstant, whereasthecostsassociatedwith
the W-IG and W-EC2algorithms decrease until they converge to a low value. This behavior is attributed
to the utilization of parameter estimates by the W-IG and W-EC2algorithms when selecting tests. As the
knowledge of the agent about these parameters improves over the course of learning, the associated costs
6The costs reported here directly correspond to the (instant) regret up to only a constant term. Therefore, the order of the
methods and their relative performances remain the same.
11Published in Transactions on Machine Learning Research (01/2025)
Figure 3: Decision making costs of our framework when applied to online troubleshooting.
decline. Furthermore, in Table 1, we report the average cost of decision making in a single time step for
different data sets. It is obvious that our framework incurs significantly lower costs for performing tests.
6.2 Application to Online Troubleshooting
In this section, we study the application of our decision making framework for online troubleshooting. The
data set for this real-world application is collected from contact center agents. The agents solve problems
from mobile devices. We use a subset of this data set with 15 possible decisions and 74 tests. Each test
corresponds to a symptom that a customer may or may not see on the device.
We experiment on approximately 1500 different scenarios (i.e., roughly 100 scenarios for each decision). A
scenario starts with a customer entering the system. Then the online learning agent aims at finding the
correct decision by asking a sequence of multiple questions of the customer (performing tests). The goal for
the agent is to ask the most informative questions in a cost-efficient manner. We enumerate 15 hypotheses
for each decision. Figure 3 shows the cost of making decisions for different time steps (i.e., scenarios) in
the same setting as Section 6.1. The results show that our framework enables the agent to find the correct
decisions for troubleshooting in mobile devices with a significantly lower cost compared to other methods.
Similar to previous observations, the W-EC2algorithm remains the most cost-effective method within our
framework.
Figure 4: Decision making costs of our framework extended to real-valued test outcomes in the Breast Cancer
Wisconsin data set.
12Published in Transactions on Machine Learning Research (01/2025)
Figure 5: Decision making cost (first row) and utility (second row) of our framework applied to the setting
that the decision regions are not known beforehand.
6.3 Extension to Real-Valued Test Outcomes
In this section, we extend our experimental results to real-valued (non-binary) test outcomes. For this
purpose, we adapt the discretization method proposed by Rahbar et al. (2023). Specifically, for each test, we
consider different thresholds for “binarization”. Then, in each time step, we can calculate the gain ( ∆W-IG
or∆W-EC2) for the tests using different binarization thresholds and choose the one with a maximal gain.
We employ this method on the Breast Cancer Wisconsin data set. In this data set, we predict the diagnosis
from 30 real-valued medical tests computed from an image of a fine needle aspirate. Figure 4 illustrates the
decision making cost incurred by our framework when applied to this data set. We enumerate 70 hypotheses
per decision region. Similar to the results depicted in Figure 2, our framework consistently achieves the
lowest cost, demonstrating its effectiveness in this setting. In particular, we observe that W-EC2-TS yields
the best results, and W-IG-TS, W-EC2-BUCB, and W-IG-BUCB are other suitable methods for this task.
6.4 Extension to Unknown Decision Regions
In this section, to demonstrate the generality of our framework, we examine the setting where the decision
regions for the enumerated hypotheses are not known. To assign hypotheses to decision regions, we employ
the hypothesis enumeration algorithm discussed in Section 6.1 to generate the most probable hypotheses
for each decision region based on the current parameter sample. Therefore, each hypothesis is assumed to
correspond to the decision it is assigned to. This situation may lead to incorrect decisions by the agent.
Specifically, in the context of online learning, our objectives are: i) to achieve highly accurate decisions, and
ii) to minimize the cost of performing tests. To quantify the accuracy of the decisions, we employ a utility
function. Specifically, we assign a utility of 2for a correct decision and −1for an incorrect one. Additionally,
we consider an “unknown” decision when the correct hypothesis is assigned to multiple decision regions. We
define the utility of such decisions as 0.
The first row in Figure 5 shows the cost of making decisions for different data sets (when the correct
decisions of the hypotheses are not known). We use the same numbers of enumerated hypotheses as Figure
2. Similar to the findings in Figure 2, we observe that our framework (with both W-IG and W-EC2) yields
a final decision with significantly lower cost than other algorithms. Again, we observe that EC2(with both
Thompson Sampling and BayesUCB) has the lowest cost of performing tests. The IG algorithm results in
the second lowest cost, and the random algorithm for picking tests is the third option.
13Published in Transactions on Machine Learning Research (01/2025)
As previously mentioned, when faced with unknown decision regions, our goal is to enhance the accuracy of
our decisions during online learning. The second row in Figure 5 illustrates the utility achieved by various
information acquisition algorithms. Notably, our framework demonstrates comparable performance to both
All and DPP in the early stages of learning. Therefore, our approach proves capable of making precise
decisions with a significantly reduced cost. We note that DPP yields very low costs in the early time steps,
but the corresponding utility is also very low.
7 Conclusion
We propose a novel framework for online decision making based on an elegant design of a combinatorial
multi-armed bandit problem, which incorporates the cost of performing tests, and where the costs may be
stochastic and depend on both test and decision outcomes. Within this framework, we develop various cost-
efficient online decision making methods such as W-EC2and W-IG. We also adapt Thompson Sampling and
BayesUCB, methods that are commonly used for exploration. In particular, we provide a theoretical upper
bound for the Bayesian regret of Thompson Sampling. We demonstrate the performance of the framework
on a number of data sets from different domains.
Acknowledgments
The work of Arman Rahbar and Morteza Haghir Chehreghani was partially supported by the Wallenberg
AI, Autonomous Systems and Software Program (WASP) funded by the Knut and Alice Wallenberg Foun-
dation. The work of Niklas Åkerblom was partially funded by the Strategic Vehicle Research and Innovation
Programme (FFI) of Sweden, through the project EENE (reference number: 2018-01937).
References
Niklas Åkerblom, Fazeleh Sadat Hoseini, and Morteza Haghir Chehreghani. Online learning of network
bottlenecks via minimax paths. Machine Learning , 112(1):131–150, 2023.
Peter Auer, Nicolo Cesa-Bianchi, and Paul Fischer. Finite-time analysis of the multiarmed bandit problem.
Machine learning , 47:235–256, 2002.
DjallelBouneffouf, IrinaRish, GuillermoA.Cecchi, andRaphaëlFéraud. Contextattentivebandits: Contex-
tual bandit with restricted context. In Proceedings of the 26th International Joint Conference on Artificial
Intelligence , IJCAI’17, pp. 1468–1475. AAAI Press, 2017. ISBN 9780999241103.
Djallel Bouneffouf, Raphael Feraud, Sohini Upadhyay, Irina Rish, and Yasaman Khazaeni. Toward optimal
solution for the context-attentive bandit problem. In Zhi-Hua Zhou (ed.), Proceedings of the Thirtieth
International Joint Conference on Artificial Intelligence, IJCAI-21 , pp. 3493–3500. International Joint
Conferences on Artificial Intelligence Organization, 8 2021. doi: 10.24963/ijcai.2021/481. URL https:
//doi.org/10.24963/ijcai.2021/481 . Main Track.
L. Breiman, J.H. Friedman, R.A. Olshen, and C.J. Stone. LED Display Domain. UCI Machine Learning
Repository, 1988.
Nicolo Cesa-Bianchi and Gábor Lugosi. Combinatorial bandits. Journal of Computer and System Sciences ,
78(5):1404–1422, 2012.
Nicolò Cesa-Bianchi, Tom Cesari, Yishay Mansour, and Vianney Perchet. Roi maximization in stochastic
online decision-making. Advances in Neural Information Processing Systems , 34:9152–9166, 2021.
Venkatesan T Chakaravarthy, Vinayaka Pandit, Sambuddha Roy, Pranjal Awasthi, and Mukesh Mohania.
Decision trees for entity identification: Approximation algorithms and hardness results. In Proceedings
of the twenty-sixth ACM SIGMOD-SIGACT-SIGART symposium on Principles of database systems , pp.
53–62, 2007.
14Published in Transactions on Machine Learning Research (01/2025)
Wei Chen, Yajun Wang, and Yang Yuan. Combinatorial multi-armed bandit: General framework and
applications. In International conference on machine learning , pp. 151–159. PMLR, 2013.
Yuxin Chen, Jean-Michel Renders, Morteza Haghir Chehreghani, and Andreas Krause. Efficient online
learning for optimizing value of information: Theory and application to interactive troubleshooting. In
Proceedings of the 33rd Conference on Uncertainty in Artificial Intelligence (UAI 2017) , 2017.
Sanjoy Dasgupta. Analysis of a greedy active learning strategy. Advances in neural information processing
systems, 17:337–344, 2005.
Michal Derezinski, Daniele Calandriello, and Michal Valko. Exact sampling of determinantal point processes
with sublinear time preprocessing. Advances in neural information processing systems , 32, 2019.
Francesco Di Fiore, Michela Nardelli, and Laura Mainini. Active learning and bayesian optimization: a
unified perspective to learn with a goal. Archives of Computational Methods in Engineering , pp. 1–29,
2024.
Audrey Durand and Christian Gagné. Thompson sampling for combinatorial bandits and its application to
online feature selection. In Workshops at the Twenty-Eighth AAAI Conference on Artificial Intelligence ,
2014.
FICO, Google, Imperial College London, MIT, University of Oxford, UC Irvine, and UC Berke-
ley. Explainable machine learning challenge, 2018. URL https://community.fico.com/s/
explainable-machine-learning-challenge .
GuillaumeGautier, GuillermoPolito, RémiBardenet, andMichalValko. DPPy: DPPSamplingwithPython.
Journal of Machine Learning Research - Machine Learning Open Source Software (JMLR-MLOSS) , 2019.
URL http://jmlr.org/papers/v20/19-179.html . Code at http://github.com/guilgautier/DPPy/ Doc-
umentation at http://dppy.readthedocs.io/.
Daniel Golovin, Andreas Krause, and Debajyoti Ray. Near-optimal bayesian active learning with noisy
observations. Advances in Neural Information Processing Systems , 23, 2010.
Aric Hagberg, Pieter J Swart, and Daniel A Schult. Exploring network structure, dynamics, and function
using networkx. Technical report, Los Alamos National Laboratory (LANL), Los Alamos, NM (United
States), 2008.
Charles R. Harris, K. Jarrod Millman, Stéfan J. van der Walt, Ralf Gommers, Pauli Virtanen, David
Cournapeau, Eric Wieser, Julian Taylor, Sebastian Berg, Nathaniel J. Smith, Robert Kern, Matti Pi-
cus, Stephan Hoyer, Marten H. van Kerkwijk, Matthew Brett, Allan Haldane, Jaime Fernández del
Río, Mark Wiebe, Pearu Peterson, Pierre Gérard-Marchant, Kevin Sheppard, Tyler Reddy, Warren
Weckesser, Hameer Abbasi, Christoph Gohlke, and Travis E. Oliphant. Array programming with
NumPy. Nature, 585(7825):357–362, September 2020. doi: 10.1038/s41586-020-2649-2. URL https:
//doi.org/10.1038/s41586-020-2649-2 .
Neil Houlsby, Ferenc Huszár, Zoubin Ghahramani, and Máté Lengyel. Bayesian active learning for classifi-
cation and preference learning. arXiv preprint arXiv:1112.5745 , 2011.
Xiyang Hu, Cynthia Rudin, and Margo Seltzer. Optimal sparse decision trees. Advances in Neural Informa-
tion Processing Systems , 32, 2019.
Alihan Hüyük and Cem Tekin. Thompson sampling for combinatorial network optimization in unknown
environments. IEEE/ACM Transactions on Networking , 28(6):2836–2849, 2020.
Donald R Jones, Matthias Schonlau, and William J Welch. Efficient global optimization of expensive black-
box functions. Journal of Global optimization , 13:455–492, 1998.
Emilie Kaufmann, Olivier Cappé, and Aurélien Garivier. On bayesian upper confidence bounds for bandit
problems. In Artificial intelligence and statistics , pp. 592–600. PMLR, 2012.
15Published in Transactions on Machine Learning Research (01/2025)
Harold J. Kushner. A new method of locating the maximum point of an arbitrary multipeak curve in the
presence of noise. Journal of Basic Engineering , 86:97–106, 1964. URL https://api.semanticscholar.
org/CorpusID:62599010 .
J. Larson, S. Mattu, L. Kirchner, and J. Angwin. How we analyzed the compas recidivism algorithm. SIAM
journal on computing , 2016.
Dennis V Lindley. On a measure of the information provided by an experiment. The Annals of Mathematical
Statistics , 27(4):986–1005, 1956.
Arman Rahbar, Ziyu Ye, Yuxin Chen, and Morteza Haghir Chehreghani. Efficient online decision tree
learning with active feature acquisition. International Joint Conference on Artificial Intelligence (IJCAI) ,
32, 2023.
DanielRussoandBenjaminVanRoy. Learningtooptimizeviaposteriorsampling. Mathematics of Operations
Research , 39(4):1221–1243, 2014.
Aleksandrs Slivkins et al. Introduction to multi-armed bandits. Foundations and Trends ®in Machine
Learning , 12(1-2):1–286, 2019.
W Nick Street, William H Wolberg, and Olvi L Mangasarian. Nuclear feature extraction for breast tumor
diagnosis. In Biomedical image processing and biomedical visualization , volume 1905, pp. 861–870. SPIE,
1993.
William R Thompson. On the likelihood that one unknown probability exceeds another in view of the
evidence of two samples. Biometrika , 25(3/4):285–294, 1933.
Pauli Virtanen, Ralf Gommers, Travis E. Oliphant, Matt Haberland, Tyler Reddy, David Cournapeau, Ev-
geni Burovski, Pearu Peterson, Warren Weckesser, Jonathan Bright, Stéfan J. van der Walt, Matthew
Brett, Joshua Wilson, K. Jarrod Millman, Nikolay Mayorov, Andrew R. J. Nelson, Eric Jones, Robert
Kern, Eric Larson, C J Carey, İlhan Polat, Yu Feng, Eric W. Moore, Jake VanderPlas, Denis Laxalde,
Josef Perktold, Robert Cimrman, Ian Henriksen, E. A. Quintero, Charles R. Harris, Anne M. Archibald,
Antônio H. Ribeiro, Fabian Pedregosa, Paul van Mulbregt, and SciPy 1.0 Contributors. SciPy 1.0:
Fundamental Algorithms for Scientific Computing in Python. Nature Methods , 17:261–272, 2020. doi:
10.1038/s41592-019-0686-2.
Siwei Wang and Wei Chen. Thompson sampling for combinatorial semi-bandits. In International Conference
on Machine Learning , pp. 5114–5122. PMLR, 2018.
Alice X Zheng, Irina Rish, and Alina Beygelzimer. Efficient test selection in active diagnosis via entropy
approximation. arXiv preprint arXiv:1207.1418 , 2012.
A Nomenclature
Table 2: Table of notation used in this paper.
Notation Description
t Time step
x A problem instance
x(t)A problem instance received at time t
i Test index
xi Test with index i
x(t)
i Test with index iperformed at time t
y A decision
y(t)A decision made at time t
n Number of available tests
16Published in Transactions on Machine Learning Research (01/2025)
Table 2: Table of notation used in this paper.
Notation Description
m Number of possible decisions
y∗Correct decision
T Time horizon
aij Base arm related to test with index iand decision with index j
A Set of base arms
S A super arm
S(t)A super arm selected at time t
R(t)Reward received at time t
R(S) Reward of super arm S
I(t)Set of feasible super arms at time t
θij Parameter of Bernoulli distribution for test iand decision j
µ(0)
ij Cost of base arm aijwhen test outcome is 0
µ(1)
ij Cost of base arm aijwhen test outcome is 1
µij Expected cost of base arm aij
µ Vector of all µij’s
θ Vector of all θij’s
µ∗True mean cost vector
θ∗True parameter vector
∆(t)
µ∗(S) Suboptimality gap at time tof super arm Swith mean cost vector is µ∗
fµ(S) Expected reward of super arm Sgiven a mean cost vector µ
S∗,(t)Optimal super arm at time t
Regretµ∗(T) Regret for a time horizon Twhen true mean cost vector is µ∗
α(t)
ij First shape parameter of Beta distribution related to θijat timet
β(t)
ij Second shape parameter of Beta distribution related to θijat timet
Qλ(η) Quantile function (if X has distribution λ, then Pr (X≤Qλ(η)) =η)
θ(t)
ij Upper confidence bound for θijused in BayesUCB
θ(t)
ij Lower confidence bound for θijused in BayesUCB
H Set of all possible hypotheses
h A hypothesis
P Set of performed tests
xP Vector of results of tests in P
H(z) Shannon entropy of a random variable z
∆Alg(i|xP)Gain of test iwith algorithm Alg when Pis the set of performed tests
i∗Index of next test to perform
Hi Set of hypotheses consistent with the outcome of test i
(h,h′) An edge between two hypotheses handh′
whh′ Weight of an edge (h,h′)
E A set of edges
w(E) Weight of a set of edges E
K(i) Set of edges cut by test i
fEC2(P) Objective function of EC2for a set of performed tests P
Bayesian Regret (T) Bayesian regret over a time horizon T
U(t)(S) Upper confidence bound for a super arm S
L(t)(S) Lower confidence bound for a super arm S
ˆµ(t)
ij Average cost of base arm aijtill timet
N(t)(ij) Number of times base arm aijhas been played till time t
¯x(τ)
ij Average of first τobservations of test iunder decision j
17Published in Transactions on Machine Learning Research (01/2025)
B Additional Proofs
B.1 Proof of Lemma 5.3
Proof.
E[L(t)(S(t))−fµ∗(S(t))]
=E/bracketleftigg
−/summationdisplay
ij∈S(t)ˆµ(t−1)
ij−/summationdisplay
ij∈S(t)/radicaligg
2 logT
N(t−1)(ij)+/summationdisplay
ij∈S(t)µ∗
ij/bracketrightigg
=/summationdisplay
ij∈S(t)E/bracketleftigg
−ˆµ(t−1)
ij−/radicaligg
2 logT
N(t−1)(ij)+µ∗
ij/bracketrightigg
≤/summationdisplay
ij∈S(t)E/bracketleftigg/vextendsingle/vextendsingle/vextendsingleµ∗
ij−ˆµ(t−1)
ij/vextendsingle/vextendsingle/vextendsingle−/radicaligg
2 logT
N(t−1)(ij)/bracketrightigg
([z]+= max{0,z})
≤/summationdisplay
ij∈AE/bracketleftigg/bracketleftigg
/vextendsingle/vextendsingleµ∗
ij−ˆµ(t−1)
ij/vextendsingle/vextendsingle−/radicaligg
2 logT
N(t−1)(ij)/bracketrightigg+/bracketrightigg
=/summationdisplay
ij∈A/parenleftigg
E/bracketleftigg
/vextendsingle/vextendsingleµ∗
ij−ˆµ(t−1)
ij/vextendsingle/vextendsingle−/radicaligg
2 logT
N(t−1)(ij)/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle“bad event”/bracketrightigg
×Pr{“bad event”}/parenrightigg
, (16)
where “bad event” is/vextendsingle/vextendsingleµ∗
ij−ˆµ(t−1)
ij/vextendsingle/vextendsingle>/radicalig
2 logT
N(t−1)(ij).
The last equality comes from the fact that/bracketleftig
|µ∗
ij−ˆµ(t−1)
ij|−/radicalig
2 logT
N(t−1)(ij)/bracketrightig+
= 0if/vextendsingle/vextendsingleµ∗
ij−ˆµ(t−1)
ij/vextendsingle/vextendsingle≤/radicalig
2 logT
N(t−1)(ij).
We continue with the following lemma.
Lemma B.1. Pr/braceleftigg
∃t∈[T]∃ij∈A,|µ∗
ij−ˆµ(t−1)
ij/vextendsingle/vextendsingle>/radicalig
2 logT
N(t−1)(ij)/bracerightigg
≤2
T.
Proof.To prove Lemma B.1, we use Eq. 1 which implies that |µ∗
ij−ˆµ(t−1)
ij/vextendsingle/vextendsingle=|µ(1)
ij−µ(0)
ij||θ∗
ij−ˆθ(t−1)
ij|
together with the fact that |µ(1)
ij−µ(0)
ij|≤1, as well as Hoeffding’s inequality. The full proof is given in
Appendix B.2.
Continuing from Eq. 16, using Lemma B.1, and noting that/vextendsingle/vextendsingleµ∗
ij−ˆµ(t−1)
ij/vextendsingle/vextendsingle≤1we have
E[L(t)(S(t))−fµ∗(S(t))]≤/summationdisplay
ij∈A2
T=2mn
T. (17)
We can similarly show that
E[fµ∗(S∗,(t))−U(t)(S∗,(t))]≤2mn
T. (18)
18Published in Transactions on Machine Learning Research (01/2025)
B.2 Proof of Lemma B.1
Proof.Let¯x(τ)
ijbe the average of first τobservations of xiunder the decision region y=j. We have:
Pr/braceleftigg
∃t∈[T]∃ij∈A,/vextendsingle/vextendsingleµ∗
ij−ˆµ(t−1)
ij/vextendsingle/vextendsingle>/radicaligg
2 logT
N(t−1)(ij)/bracerightigg
Eq.(1)=Pr/braceleftigg
∃t∈[T]∃ij∈A,/vextendsingle/vextendsingleµ(1)
ij−µ(0)
ij/vextendsingle/vextendsingle/vextendsingle/vextendsingleθ∗
ij−ˆθ(t−1)
ij/vextendsingle/vextendsingle>/radicaligg
2 logT
N(t−1)(ij)/bracerightigg
(since/vextendsingle/vextendsingleµ(1)
ij−µ(0)
ij/vextendsingle/vextendsingle≤1)
≤Pr/braceleftigg
∃t∈[T]∃ij∈A,/vextendsingle/vextendsingleθ∗
ij−ˆθ(t−1)
ij/vextendsingle/vextendsingle>/radicaligg
2 logT
N(t−1)(ij)/bracerightigg
(Union bound)
≤/summationdisplay
t∈[T]/summationdisplay
ij∈A(t−1)/summationdisplay
τ=1Pr/braceleftigg
/vextendsingle/vextendsingleθ∗
ij−¯x(τ)
ij/vextendsingle/vextendsingle>/radicalbigg
2 logT
τ/bracerightigg
(assumingT≥|A|)
≤T3×2T−4=2
T.
The last inequality is due to Hoeffding’s inequality (e.g., Theorem A.1 of Slivkins et al. (2019) with α= 2).
B.3 Proof of Lemma 5.4
Proof./summationdisplay
t∈[T]E[U(t)(S(t))−L(t)(S(t))]
=/summationdisplay
t∈[T]E/bracketleftigg
2/summationdisplay
ij∈S(t)/radicaligg
2 logT
N(t−1)(ij)/bracketrightigg
≤/radicalbig
8 logT/summationdisplay
t∈[T]E/bracketleftigg/summationdisplay
ij∈S(t)1/radicalbig
N(t−1)(ij)/bracketrightigg
=/radicalbig
8 logT/summationdisplay
ij∈AE/bracketleftigg/summationdisplay
t:ij∈S(t)1/radicalbig
N(t−1)(ij)/bracketrightigg
=/radicalbig
8 logT/summationdisplay
ij∈AE/bracketleftigg/summationdisplay
l∈[N(T)(ij)]1√
l/bracketrightigg
19Published in Transactions on Machine Learning Research (01/2025)
(Lemma 1 of Russo & Van Roy 2014)
≤2/radicalbig
8 logT/summationdisplay
ij∈AE/bracketleftigg/radicalig
N(T)(ij)/bracketrightigg
(Cauchy–Schwarz inequality)
≤2/radicalbig
8 logTE/bracketleftigg/radicaligg
mn/summationdisplay
ij∈AN(T)(ij)/bracketrightigg
≤2/radicalbig
8 logTE/bracketleftigg
/radicalbig
(mn)(mn)T/bracketrightigg
= 2mn/radicalbig
8TlogT.
20