DU-Shapley: A Shapley Value Proxy for
Efficient Dataset Valuation
Felipe Garrido-Lucero*
Inria, Fairplay joint team
Palaiseau, France
felipe.garrido-lucero@irit.fr
* Equal contributionBenjamin Heymann*
Criteo AI Lab
Paris, France
b.heymann@criteo.com
* Equal contributionMaxime Vono*
Criteo AI Lab
Paris, France
m.vono@criteo.com
* Equal contribution
Patrick Loiseau
Inria, Fairplay joint team
Palaiseau, France
patrick.loiseau@inria.frVianney Perchet
ENSAE, FairPlay joint team
Palaiseau, France
vianney@ensae.fr
Abstract
We consider the dataset valuation problem , that is, the problem of quantifying the
incremental gain, to some relevant pre-defined utility of a machine learning task,
of aggregating an individual dataset to others. The Shapley value is a natural tool
to perform dataset valuation due to its formal axiomatic justification, which can be
combined with Monte Carlo integration to overcome the computational tractability
challenges. Such generic approximation methods, however, remain expensive in
some cases. In this paper, we exploit the knowledge about the structure of the
dataset valuation problem to devise more efficient Shapley value estimators. We
propose a novel approximation, referred to as discrete uniform Shapley, which is
expressed as an expectation under a discrete uniform distribution with support of
reasonable size. We justify the relevancy of the proposed framework via asymptotic
and non-asymptotic theoretical guarantees and illustrate its benefits via an extensive
set of numerical experiments.
1 Introduction
One of the main challenges for training machine learning (ML) models with enough generalization
capabilities is to access a sufficiently large set of labeled training data. These data often exist but are
commonly spread across many parties, impairing their usage in a direct and simple way. Real world
examples range from the advertising industry, where different retailers hold sets of observations with
either similar or complementary features from consented data about browsing and shopping habits of
individual users; to the medical sector where hospitals may improve their diagnostics accuracy by
sharing their data. By collaborating with each other and pooling their individual datasets together,
these dataset owners could learn better ML models for their applications. Naturally, many questions
raise from such collaborations. Federated learning [ 8,9], for example, addresses the issues related to
the practical ways that dataset owners can share their data. We consider a complementary problem to
the one in federated learning: measuring the additional value each party would obtain by participating
in the joint ML effort. In order to compute or estimate compensating rewards allowing to incentivize
parties to share their data, a first stage that is commonly considered in the literature is to perform
so-called dataset valuation [1, 42, 44].
Motivated by natural properties expected for fair valuation, different solution concepts from coopera-
tive game theory [ 3] have been considered, the Shapley value [ 40] being arguably the most broadly
38th Conference on Neural Information Processing Systems (NeurIPS 2024).studied valuation scheme in ML due to its axiomatic justification. Agarwal et al. [ 1] designed a
data marketplace and used the Shapley value to allocate the data among buyers. Tay et al. [ 44]
considered a cooperative environment where agents can jointly train a generative model, from which
synthetic data are drawn and distributed to the parties according to their Shapley values. Sim et al.
[42] rewarded parties based on the Shapley value and information gain on model parameters. The
critical challenge when using the Shapley value is its well-known computational intractability. To
cope with it, [1, 44] considered Monte Carlo (MC) approximations, while [42] worked with a small
set of three players. This approximation methods, however, remain expensive whenever computing
the marginal contributions involve retraining. Moreover, they are generic and do not use the specific
structure of the dataset valuation problem at stake, leaving open the possibility to find more adapted
approximations for that problem.
The Shapley value was also used in the related problem of data valuation . Data valuation measures
the contribution of a single data point within a dataset in the training of a given prediction model.
Several solution concepts based on the Shapley value have been proposed for the data valuation
problem including Data Shapley [10,16],DShapley [11,24],Beta Shapley [23] orCS-Shapley
[38], together with different MC variants to cope with the computational intractability issue. For the
data valuation problem, the structure was exploited to give easier-to-compute solutions in certain
cases, in particular for the k-nearest neighbor problem [ 12,15,25,26,35,41,46]. Unlike data
valuation, however, dataset valuation aims at quantifying the marginal contribution of a whole dataset
to a given ML task with respect to (w.r.t.) the datasets brought by other dataset owners. Although
data and dataset valuation are related problems, they are different and the techniques developed for
data valuation cannot be used for the dataset valuation problem that we study (we further develop this
point in Section 2.3).
Contributions. We consider the dataset valuation problem. Following the ML literature, we
model it as a cooperative game whose value function relates to the considered ML task, and aim
at estimating the Shapley value to measure the dataset owners contribution. We propose a new
way to address the computational intractability issue of the Shapley value. Instead of relying on
generic MC approximation schemes, our approximation method leverages the structure of the dataset
valuation problem as well as a convergence result for a key random variable of the problem. Our
approximation behaves well in many cases, both theoretically and empirically. More specifically, our
main contributions can be summarized as follows:
1.We propose DU-Shapley (Definition 3), a novel Shapley value approximation that exponentially
reduces the number of utility function valuations required for the computation. This is the first
dataset valuation approach leveraging the specific structure of the utility function.
2.Based on three different use-cases, we establish asymptotic and non-asymptotic theoretical
guarantees for DU-Shapley , showing notably that it converges almost surely to the Shapley value
as the number of dataset owners grows.
3.We assess the benefits of the proposed methodology using extensive numerical experiments on
both Shapley value approximation and dataset valuation use-cases. We show, in particular, that
DU-Shapley outperforms all considered MC approximations of the Shapley value.
Additional Related Work. Cooperative game theory has been applied to solve multi-agents ML
problems beyond data and dataset valuation [ 6,18,29,47]. In particular, the Shapley value has been
used to solve several problems including variable selection [ 5], feature importance [ 7,27,28], or
model interpretation [ 4]. In these problems, similarly to the data and dataset valuation problems, the
computational intractability issue of the Shapley value is usually addressed via MC [2, 31, 32].
2 Problem Formulation and Main Concepts Involved
This section presents the dataset valuation problem we aim to solve, along with preliminaries
including the definition and classical approximations of the Shapley value. For n∈NandA, we
denote [n] :={1, .., n}andU(A)the uniform distribution with support on A.
2.1 Generic Model
We consider a collaborative ML setting involving a set IofI=|I| ∈ N∗dataset owners, also
referred to as players in the sequel, who are willing to cooperate in order to solve a common ML
problem. Each player i∈ Iis assumed to possess an individual dataset Di={(x(j)
i, y(j)
i)}j∈[ni]
2where x(j)
i∈ X ⊂ Rdstands for a feature vector, y(j)
i∈ Y is a label, ni=|Di|refers to the number
of data points in Di, and samples are drawn independently from a player-dependent distribution pi,
i.e.,(x(j)
i, y(j)
i)∼pi, for all j∈[ni]andi∈ I.
Our basic motivation is to quantify the incremental contribution that a given player i∈ Ibrings by
sharing her dataset Diwith other players towards solving some ML task. Hence, we are interested in
scenarios in which, even though the data distribution might differ across players, they face a similar
ML task, for instance the minimization of the expectation (with respect to pi) of some loss function
ℓ(ˆY , Y), where ˆYdenotes a prediction of Y. In such cases, players can usually learn from others’
datasets, in the sense that given some X, the optimal prediction ˆYthat minimizes E[ℓ(ˆY , Y)|X]is
the same for all player. This holds, e.g., if the conditional distributions (or, in many cases, simply the
conditional expectation) of y(j)given x(j)are the same but the marginal distributions of x(j)differ.
To model this problem with full generality, we assume that the players i∈ Icollaborate in solving
an ML task whose success is measured through some abstract metric uthat maps any dataset to a real
number (say, the prediction accuracy in a classification problem). With a slight abuse of notation,
for any coalition of players S ⊆ I , we define u(S) =u(DS), where DS:=∪i∈SDi. Hence,
u: 2I→Rcan be seen as a game-theoretical utility function that quantifies how well coalitions of
players can solve the considered ML task based on the union of their datasets.
The following subsections provide three theoretical use-cases that instantiate the generic model and
give specific utility functions uto illustrate the dataset valuation problem. Using different tools and
techniques, Section 3 provides theoretical guarantees in each of them. These theoretical results are
then complemented in Section 4 by numerical evidence of our proposed approach in more intricate
practical problems on real data.
2.1.1 Theoretical use-case 1: Non-parametric Regression
The first use case we shall investigate is quite generic and consists in non-parametric regression. We
assume the existence of a function f∗such that y(j)
i=f∗(x(j)
i)+η(j)
iwithη(j)
ii.i.d., and a quadratic
loss function. Without regularity assumption on f∗(·), learning can be arbitrarily slow; hence it is
usually assumed that this mapping is Lipschitz (or at least β-Hölder [13, 45]).
The standard estimation method of f∗we shall consider is called the regressogram orbinning
(also applied in [ 13] to study local differential privacy within regression) and consists in learning
optimal piece-wise constant functions. More precisely, given some parameter B∈N—chosen
exogeneously as a function of the function regularity β, the ambient dimension dand the total number
nof datapoints, typically B≃nd/(d+2β)—, the feature space Xis partitioned into Bcubic bins. The
excess risk of learning f∗can then be decomposed into
E
(ˆf(x)−f∗(x))2
=E
(ˆf(x)−¯f(x))2
+E
(¯f(x)−f∗(x))2
, (1)
where ˆfis the estimator of f∗,¯f(x) :=P
b∈[B]¯fb 1{x∈b}, and ¯fbis any value that f∗can take on
the bin b. The second term in (1)being agnostic to the players’ datasets, the problem of measuring the
contributions of the players to estimating f∗can be decomposed into measuring their contributions
to estimating each ¯fb. In particular, the utility u(S)of a coalition Scan be defined, and split into the
sum of Bsub-utilities ub(S)functions, as follows
u(S) :=−E
(ˆfS(x)−¯f(x))2
=X
b∈[B]−E
(ˆfS,b−¯fb)2
P(x∈b) =:X
b∈[B]ub(S)P(x∈b),
where ˆfSis the estimator of ¯fwhen using the datasets of all players in SandˆfS,bis the estimator
¯fbwhen using, for all players in S, the datasets of points in the bin b. Interestingly, after this
reduction, the problem is decomposed into Bindependent sub-problems—one per bin—, where
the utility is a sole function of the number of data points used to estimate ¯fb, i.e., we can write
ub(S) =wb(P
i∈Sni,b)for some function wb:N→R, where ni,bis the number of data points that
player ihas in the bin b. This last property motivates our second theoretical use-case.
2.1.2 Theoretical use-case 2: Homogeneous case
The second theoretical setting considers a general learning problem (not necessarily restricted to
regression) and supposes that all players have the same sampling distribution, i.e., it takes pi=p
3for all i∈ I. This homogeneity on the players allows to reduce the problem of measuring the
contribution of the players to just counting the number of data points contributed by each of them.
Formally, and similarly to the previous use-case, we suppose the existence of a function w:N→R
such that u(S) =w(P
i∈Sni).
2.1.3 Theoretical use-case 3: Heterogeneous Linear Regression - Local Differential Privacy
The third theoretical setting we consider is linear regression with random design and different variance
of the features and labels per player. Although the setting is more general, one of the motivations
behind it is standard linear regression with homogeneous data between players, but where players
can purposely add noise when sharing their dataset (in order to provide Local Differential Privacy, for
instance). Formally, for any i∈ I, we consider the following linear model that generates the dataset
Diof size ni:
y(j)
i=x(j)
iθ+η(j)
i,where η(j)
i∼N(0, ε2
i),andx(j)
i∼N(0d, σ2
iId),for any j∈[ni], (2)
withθ∈Rda ground-truth parameter, σipositive and known, and εithe differential privacy level
chosen by player i. Under the linear regression framework defined in (2), and following [ 8], the
utility function of a set S ⊆ I of players is defined by the negative expected mean square error over a
hold-out dataset, i.e.,
u(S) =−E 
x⊤ˆθS−x⊤θ2
, (3)
where the expectation is taken over the distribution ptestof a hold-out testing datum x∈Rd,
the sampling distributions N(0, σ2
iId)for all i∈ S, and the linear regression error distributions
N(0, ε2
i),∀i∈ S, j∈[ni], and ˆθSstands for the generalized least square estimator defined by
ˆθS= (X⊤
SΣ−1
SXS)−1X⊤
SΣ−1
SYS,where ΣS= diag(( ε2
i)i∈S)∈R|S|×|S|.The notations XSand
YSrefer to the concatenation of {Xi}i∈Sand{Yi}i∈S, respectively, and Xi∈Rni×dis defined by
Xi= ([x(1)
i]⊤, . . . , [x(ni)
i]⊤)⊤while Yi∈Rniis defined by Yi= (y(1)
i, . . . , y(ni)
i)⊤.
The following result provides a close-form expression for the utility function in this case:
Proposition 1. LetSbe a coalition of players and consider the value function as above. It follows,
u(S) =−Tr
E
xx⊤
q(S)−d−1,where q(S) :=$ P
i∈S(σi/εi)ni2
P
i∈S 
σi/εi2ni%
,with the convention q(∅) = 0 .
In particular, considering ptest= N(0 ,Id), we get u(S) =d
d+1−q(S).
Proposition 1 shows that, in this use-case, the utility function can be written as a function w(q(S))of
a scalar quantity q(S)that captures the datasets heterogeneity. Notice that in this use-case, if we add
the homogeneity assumption that σi/εi=σ/ε, for all i∈ I, then the term q(S)becomesP
i∈Sni
and, as a consequence, we get
u(S) =w(q(S)) =wX
i∈Sni
=d
d+ 1−P
i∈Sni.
Recall that, in the non-parametric regression use-case, it holds u(S) =P
b∈[B]P(x∈b)wb(qb(S))
where qb(S) =P
i∈Sni,b. Therefore, in our three uses-cases, the utility of a coalition can be
summarized as the function of some scalar quantity of interest. This observation will be useful to
state later our theoretical results.
2.2 Shapley Value
The Shapley value [ 40] is a classical solution concept in cooperative game theory to fairly allocate
the total gains generated by a coalition of players. Given a utility function u, the Shapley value of a
player iis defined as the average marginal contribution of her dataset Dito all possible subsets of
{Dj}j∈I\{ i}, built by aggregating the datasets of the other players. Formally, the Shapley value φi
of player iwrites
φi(u) =1
|Π(I)|X
π∈Π(I)[u(Pπ
i∪ {i})−u(Pπ
i)], (4)
4where Π(I)refers to the set of permutations over IandPπ
ito the set of predecessors of player i∈ I
in permutation π∈Π(I). The Shapley value of player iis equivalently expressed as
φi(u) =1
IX
S⊆I\{ i}I−1
|S|−1
[u(S ∪ { i})−u(S)]. (5)
The Shapley value has been commonly used in ML and cooperative game theory as it uniquely
satisfies the following set of desirable properties.
1.Efficiency.PI
i=1φi(u) =u(I), i.e, the sum of all Shapley values is equal to the value of I.
2.Symmetry. If, for any S ⊆ I \ { i1, i2},u(S ∪ { i1}) =u(S ∪ { i2}), then φi1(u) =φi2(u), i.e.,
whenever two players have the same marginal contributions, their Shapley values coincide.
3.Dummy. If, for any S ⊆ I \ { i},u(S ∪ { i}) =u(S), then φi(u) = 0 , i.e., whenever a player has
null marginal contributions, her Shapley value is zero.
4.Linearity. φi(u1+u2) =φi(u1) +φi(u2), i.e., the Shapley value of sums of games is the sum
of the Shapley values of the respective games.
MC approximation of the Shapley Value. Evaluating the Shapley value is unfortunately computa-
tionally expensive in general. As a consequence, many MC approximations have been considered by
sampling with replacement Tterms from the sum of either (4)or(5). Regarding (4), this boils down
to considering the estimator
ˆφi(u) =1
TXT
t=1[u(Pπt
i∪ {i})−u(Pπt
i)],where πt∼U(Π(I)). (6)
2.3 Data valuation vs Dataset valuation
A tentative, but naive, approach to solve the dataset valuation problem could be to run an auxiliary
data-valuation algorithm on all the data and to assign to each dataset the sum of the values of its
datapoints. We highlight the cons of this idea on a very simple, yet insightful example. Consider two
datapoints x1andx2, three datasets D1={x1},D2={x2},D3={x2, x2}, and the following toy
utility function u(D) = 1{x1, x2∈D}. In data valuation, any point x2shall have the same value, as
they are identical. In particular, a naive summation would value D3twice the value of D2. In dataset
valuation, and for this toy problem at hand, it is quite clear that both datasets should have the same
value. Moreover, the Shapley values are 1/6forD2andD3versus 2/3forD1.
The message here is twofold. Data valuation and dataset valuation are two fundamentally different
concepts and one cannot directly reduce the latter to the former. This is actually true, and this is the
second message, because the utility function uis highly non-linear (even for the regression task).
3 Discrete Uniform Shapley Value
This section introduces and studies our approximation scheme for the Shapley value. Section 3.1
shows an asymptotic property that gives the general intuition behind our approximation. The result
holds for the three use-cases of Sections 2.1.1 to 2.1.3. Section 3.2 presents a general approximation
methodology for dataset valuation and shows its almost surely convergence as the number of players
grows for our three uses-cases. Section 3.3 studies the rate of convergence, first for the homogeneous
setting (Section 2.1.2), and then leverages this result to obtain a similar one for the non-parametric
regression setting (Section 2.1.1). All proofs are postponed to the supplementary material.
3.1 Insights behind DU-Shapley
The Shapley value, by re-arranging the coalitions S ⊆ I \ { i}by their cardinality in the sum in (5),
can be equivalently expressed as
φi(u) =EK∼U({0,...,I−1})ES∼U 
2I\{i}
K[u(S ∪ { i})−u(S)], (7)
where 2I\{i}
K denotes the subsets of I \ {i}of cardinality K. In our three uses-cases, it follows that
φi(u) =φi(w) =EK∼U({0,...,I−1})ES∼U 
2I\{i}
K[w(q(S ∪ { i}))−w(q(S))], (8)
5where w:R+→Ris such that u(S) =w(q(S))for any S ⊆ I , and q(S)is the scalar quantity of
interest identified in Sections 2.1.1 to 2.1.3 for each use-case:
q(S) := P
i∈Sγini2
P
i∈Sγ2
ini
,where, for any i∈ I, γi=
1 for the second use-case ,
σi/εifor the third use-case ,(9)
and for the first use-case, qb(S)is analogously defined at every bin, with γb
i= 1for all players and
all bins. We remark that the definition of q(S)in the first and second use-cases is not restricted to
linear regression. Equation (8)explicitly reveals a key random variable, namely q(S). Interestingly,
Figure 1 suggests that q(S)converges in distribution to a uniform random variable as the number of
players increases (with i.i.d. datasets sizes). Theorem 2 proves this result formally for any (γi)i∈I.
Theorem 2. Let{ni, γi}i∈[I]be two sequences of positive numbers such that the following limits
lim
I→∞1
IX
i∈[I]niγi=µA,lim
I→∞1
IX
i∈[I](niγi−µA)2=σ2
A,
lim
I→∞1
IX
i∈[I]niγ2
i=µB,lim
I→∞1
IX
i∈[I](niγ2
i−µB)2=σ2
B,
all exist, for some constants µA, µB, σA, σB>0. Let K∼U({0, . . . , I }),SK∼U([2I
K]). Then,
almost surely,q(SK)
q(I)I→∞− − − → U([0,1]).
Figure 1: Distribution of q(S)/q(I)whenSis sampled as in (8)(i.e., first sample a size Kuniformly,
then sample a coalition Sof size Kuniformly). (left) I= 10 , (middle) I= 50 , (right) I= 500 . We
considered 104samples for each random variable, and the third use-case with ni∼U([100]) and
σi/εi∼U([10]) for each i∈ I.
3.2 Discrete Uniform Shapley value
The Shapley value re-arrangement in (7)exposes the main tool behind our approximation: it is
enough to approximate the distribution of the random variable DSthat takes values on the subsets
ofD−i:=∪j∈I\{ i}Dj(recall that u(S) =u(DS)). Theorem 2, taking the example of the second
use-case for intuition, indicates that these datasets have uniformly distributed numbers of points in the
limit. Generalizing this intuition, we propose to approximate DSby taking Isamples of increasing
size from the pool D−iby sampling data points uniformly. This leads to the following definition of
DU-Shapley for our generic model:
Definition 3. [DU-Shapley ] For any i∈ I, the discrete uniform Shapley value ( DU-Shapley ) of
thei-th player, denoted by ψi, is given by
ψi(u) :=1
IXI−1
k=0u(D(k)∪Di)−u(D(k)),
where D(k)is a set of data points uniformly sampled without replacement from D−iof size kµ−i,
withµ−i=1
(I−1)|D−i|.
Compared to the Shapley value defined in (5), which involves 2Iterms to compute, note that
DU-Shapley only involves Iterms and hence it presents an exponential reduction of the number of
utility function evaluations. Of course, these computational savings come at the cost of some bias.
The latter is precisely quantified in Section 3.3 for our first two use-cases.
By definition, DU-Shapley is a random variable which depends on the sampled data points. However,
whenever u(S) =w(q(S)), with q(S)some scalar quantify of interest, as in our use-cases, we
6can get rid of the stochastic nature of DU-Shapley by considering Ireal values from well chosen
intervals. In particular, in our uses-cases, DU-Shapley boils down to:
ψi(w) =1
IXI−1
k=0w(¯qk
i)−w(¯qk
−i), (10)
where
¯qk
i:=(γini+k
I−1P
j∈I\{ i}γjnj)2
γ2
ini+k
I−1P
j∈I\{ i}γ2
jnj
and¯qk
−i:=k
I−1·(P
j∈I\{ i}γjnj)2
P
j∈I\{ i}γ2
jnj
.
We remark the notation abuse as we should write ψ(w◦q). For simplicity, we omit the composition
and only write ψ(w). Equation (10) coincides exactly with Definition 3 in the first two use-cases,
i.e., when γj=γfor all j∈ I. Indeed, as the random datasets D(k)have a fixed size and the value
function only looks at the number of data points within the coalition, we obtain,
ψi(u) =1
II−1X
k=0u(D(k)∪Di)−u(D(k)) =1
II−1X
k=0w(|D(k)∪Di|)−w(|D(k)|)
=1
II−1X
k=0w(kµ−i+ni)−w(kµ−i) =ψi(w).
For the third use-case, Equation (10) is an approximation that comes from assuming that, for any
j∈ I \ { i},|Dj∩D(k)|=k·nj
I−1, which holds with high probability for large values of I, since
q(D(k)∪Di) = 
γini+P
j∈I\{ i}γj· |Dj∩D(k)|2
γ2
ini+P
j∈I\{ i}γ2
j· |Dj∩D(k)|
.
Theorem 2 implies the following result.
Corollary 4. Letφiandψibe, respectively, the Shapley value (5)and the DU-Shapley (10) of
player i. Then, in our three uses-cases, it holds, limI→∞|φi−ψi|= 0almost surely.
While our theoretical results are based on Equation (10) for the cases where u(S) =w(q(S)), we
will see through numerical experiments that Definition 3 gives good results in more general cases.
3.3 Non-Asymptotic Theoretical Guarantees
Corollary 4 states asymptotic guarantees for DU-Shapley . In this section, we show non-asymptotic
results that give the convergence rate for the first two uses-cases.1Recall that in non-parametric
estimation, the utility writes as u(S) =P
b∈[B]ub(S)P(x∈b), and therefore, by the linearity axiom
of the Shapley value, for any i∈ I, φi(u) =P
b∈[B]φi(ub)P(x∈b). As a consequence, in order to
estimate φi(u), it is enough to compute each φi(ub). In particular, the Shapley value approximation
error over the whole feature space becomes a simple aggregation of the Shapley value approximation
errors over the bins. We focus firstly on bounding the bias of our method in the homogeneous use-case
to then extend it to the non-parametric regression case.
As in the homogeneous use-case the utility function writes as u(S) =w(P
i∈Ini), we consider the
following regularity assumptions on w.
H1. The function w:R+→Ris increasing, twice continuously differentiable, and such that
limn→∞n2|w(2)(n)|<∞(where w(2)represents the second derivative).
Monotonicity is a natural assumption in our framework as, the more data, the more precise the ML
prediction is expected to be. The condition over the limit aims at controlling the growth behavior of
the utility function and it is automatically satisfied whenever wis bounded and w(2)is monotone, by
the mean value theorem. Theorem 5 bounds the bias of DU-Shapley for the homogeneous use-case.
Theorem 5. Under Assumption H1, there exists a constant κ >0, such that, for any i∈ I, it holds,
φi−ψi≤κ
(I−1)µ2
−i 
σ2
−i(1 + ln( I−1)) + ζ−i
,
1A similar result, albeit more technical, can be shown with the same arguments for the third use-case.
7where φiandψiare respectively the Shapley value and the DU-Shapley of player i,µ−i=
1
(I−1)|D−i|is the average dataset size of all players but i,σ2
−i=1
I−1P
j∈I\{ i}(nj−µ−i)2their
empirical variance, and ζ−imeasures the variability of the dataset sizes across players. Formally, it
is defined as ζ−i:=R2
−iτ2
−i/4nmax
−iwhere R−i:= max j∈I\{ i}|nj−µ−i|,nmax
−i:= max j∈I\{ i}nj,
andτ−i:=nmax
−i/minj∈I\{ i}nj.
The full proof of Theorem 5 is included in Appendix C.3 and it relies on controlling the absolute
value of E[w(µ−iK)−w(P
j∈Snj)], where K∼U({0, ..., I−1})andSis the random variable
in(7). Using a second order Taylor expansion, the problem is reduced to controlling the term related
to the second derivative of wby using the regularity assumptions in H1.
As advertised before, Theorem 5 can be directly generalized to the non-parametric use-case, since,
u(S) =X
b∈[B]wbX
i∈Sni,b
P(x∈b),forni,b=|{(x, y)∈Dj, x∈b}|.
Corollary 6. Under Assumption H1 for all functions wb, there exist constants κb>0, such that, for
anyi∈ I, it holds that
φi−ψi≤X
b∈[B]κbP(x∈b)
(I−1)µ2
−i,b 
σ2
−i,b(1 + ln( I−1)) + 2 ζ−i,b
, (11)
where φiandψiare respectively the Shapley value and the DU-Shapley of player i, and all terms
are equivalently defined to Theorem 5 at each bin b∈[B].
The upper bound in (11) depends on natural quantities related to the dataset valuation problem
described in Section 2.1 at each bin, such as the first two moments µ−i,bandσ−i,bof the datasets’
size distribution. More precisely, the error increases when there are some outlier players with a very
small or large dataset size. This behavior is expected since, in this particular setting, the random
variable inside of the Shapley value differs from a uniform random variable. As showcased in
Theorem 2, the error vanishes when the number of players Itends towards infinity.
4 Numerical Experiments
We illustrate the benefits of DU-Shapley by measuring numerically three properties: (1) how well
DU-Shapley approximates the Shapley value in real data, (2) how many (theoretical) iterations need
other methods to achieve the same accuracy level than DU-Shapley , and (3) how well DU-Shapley
performs in classical dataset valuation tasks with real data. Appendix A.1 complements the results by
a complexity comparison between our method and SVARM [22] and Appendix A.2 by experiments on
synthetic data. The experiments strongly suggest that DU-Shapley performs well in all tasks.
4.1 Approximating the Shapley Value in Real-World Data
We consider the real-world datasets in Mitchell et al. [32], whose details are provided in Table 3 in
the appendix. To tackle these problems we consider logistic regression models and gradient-boosted
decision trees (GBDT). For classification tasks, the utility function has been taken as the expected
accuracy of the trained logistic regression model over a hold-out testing set while for regression
tasks, the utility function corresponds to the averaged MSE over a hold-out testing set. In both cases
we took a hold-out testing set with 10% of the size of the training dataset. For each dataset, we
considered two worst-case scenarios for our method, namely I= 10 players and I= 20 players.
Starting from the datasets in Table 3, we heterogeneously allocate datasets to the players. We compare
ourselves with two approaches, referred to as MC-Shapley , for the standard MC approximation
defined in (6), and MC-anti-Shapley that considers, in addition, antithetic sampling [ 32]. We
compute the averaged MSE across all players between the true Shapley value and each estimator.
Since computing the marginal contributions in this experiment requires re-training, which is clearly not
feasible for a large number of epochs, we chose to restrict ourselves to 20 steps of stochastic gradient
descent for logistic regression and 20 boosting iterations for GBDTs. For MC-based approaches, we
considered Isamples to compare those approximations with the proposed methodology on a fair
basis, i.e., associated to the same computational budget.
8Table 1 depicts the results. We clearly see that, even in the worst-case scenario where the number of
players is small and far from the theoretical assumptions from Section 3.3, DU-Shapley competes
favorably with the MC-based methods.
Table 1: Worst-case comparison between DU-Shapley and competitors, for real-world datasets
considered in Table 3. We report the averaged MSE across all players w.r.t. the exact Shapley value.
Dataset adult breast-cancer bank cal-housing
Players 10 20 10 20 10 20 10 20
DU-Shapley 2.10−36.10−43.10−31.10−45.10−24.10−31.10−23.10−3
MC-Shapley 1.10−24.10−33.10−21.10−39.10−26.10−25.10−22.10−2
MC-anti-Shapley 8.10−32.10−31.10−28.10−48.10−24.10−23.10−21.10−2
Dataset make-regression year
Players 10 20 10 20
DU-Shapley 9.10−22.10−21.10−37.10−4
MC-Shapley 4.10−13.10−15.10−31.10−3
MC-anti-Shapley 4.10−12.10−15.10−31.10−3
4.2 Complexity of Computing the Shapley Values of all Players
We have looked at the number of iterations that DataShapley and the Improved Group Testing-Based
method [46] ( IGTB ) require to achieve DU-Shapley ’s accumulated bias, formally given by
DUbias( I) :=κ
I−1X
i∈I(9σ2
−i(1 + log( I−1)) + ζ−i)2
(µ−i)41/2
.
To do so, we have replaced ε= DUbias( I), respectively, in the formula in Section 4.1 in [ 16] and
Equation 5 in [ 46], with a value function motivated from our third use-case under the homogeneity
assumption σi/εi=σ/εfor all i∈ I. The results are illustrated in Figure 2. Remark DU-Shapley
requires I2iterations to compute all Shapley values. We observe that in all tested instances, both
methods require a higher number of iterations to achieve the same error than DU-Shapley.
Figure 2: Iterations required by DataShapley and the Improved Group Testing-Based method to
achieve DU-Shapley’s accumulated bias with function w(nS) = 1−10k(I)
10k(I)+nS, where nSis the
number of data points of the coalition S⊆ I, and k(I) :=⌊log(P
i∈Ini)⌋ −1is a normalization
factor. (top) δ= 0.01, (bottom) δ= 0.1, (left) nmax= 10 , (middle) nmax= 50 , (right) nmax= 100 .
4.3 Applying DU-Shapley to dataset valuation problems
We considered non-tabular datasets used in [ 17], namely bbc-embedding, IMDB-embedding, both
text datasets, and CIFAR10-embedding, an image dataset. Feature embedding have been generated
9using pretrained DistilBERT and ResNet50 models, respectively. In addition we have adapted three
baselines from data valuation to our setting: Leave-One-Out (LOO), DataShapley, and KNN-Shapley.
Appendix B.2 gives the implementations details. For these datasets associated to classification
problems, we used a multi-layer perceptron classifier as prediction model.
We have considered three dataset valuation problems, none of them needing the real Shapley values,
which allows us to increase the number of players w.r.t. the experiments in Section 4.1. We
investigated noisy label detection (NLD), dataset removal (DR), and dataset addition (DA) [ 17]. For
NLD, we used as a metric the F1-score (the larger the better). For DR, we used the testing accuracy
(the lesser the better). For DA, we used the testing accuracy (the lesser the better).
We considered splitting the dataset across I= 100 players. The results are summarized in Table 2.
We observe that DU-Shapley has competitive results compared to classical baselines despite of the
fact that none of the considered cases verifies the structural assumptions from Section 3.3. In addition,
we can see that DU-Shapley tends to have similar and even better results as Data Shapley (which
is a MC based method). This is in line with our theory as, for larger number of players, DU-Shapley
tends to better estimate the true Shapley value.
Table 2: Comparison between DU-Shapley and competitors for real-world datasets considered in
[17] in Noisy label detection, Dataset Removal and Dataset Addition.
Dataset CIFAR 10 BBC
ProblemNLD DR DA NLD DR DA
5% 15% 5% 15% 5% 15% 5% 15% 5% 15% 5% 15%
Random 0.11 0.19 0.61 0.60 0.25 0.41 0.11 0.19 0.90 0.88 0.68 0.81
LOO 0.13 0.18 0.62 0.60 0.15 0.32 0.11 0.17 0.90 0.88 0.61 0.77
DataShapley 0.13 0.25 0.61 0.59 0.12 0.18 0.12 0.20 0.89 0.87 0.08 0.12
KNN-Shapley 0.14 0.28 0.60 0.57 0.12 0.15 0.19 0.29 0.88 0.86 0.13 0.12
DU-Shapley 0.14 0.30 0.61 0.55 0.11 0.14 0.18 0.34 0.89 0.85 0.07 0.11
Dataset IMBD
ProblemNLD DR DA
5% 15% 5% 15% 5% 15%
Random 0.10 0.16 0.77 0.75 0.62 0.68
LOO 0.11 0.18 0.77 0.74 0.53 0.59
DataShapley 0.17 0.28 0.75 0.69 0.36 0.33
KNN-Shapley 0.18 0.29 0.76 0.68 0.41 0.37
DU-Shapley 0.18 0.32 0.76 0.66 0.33 0.34
5 Conclusion
We model the dataset valuation problem as a cooperative game and design a Shapley value ap-
proximation, named DU-Shapley , that exploits the underlying structure of the utility function and
exponentially reduces the number of functions valuations required for the computation. In three
different uses-cases, DU-Shapley is proved to almost surely converge to the real Shapley value as the
number of players grows. Moreover, we find the rate of convergence, which depends only on natural
parameters of dataset valuation. Numerical experiments showcase that DU-Shapley performs well in
approximating the Shapley value and performing dataset valuation tasks, even when the assumptions
needed for the theoretical guarantees do not hold, and it has a good complexity when computing the
Shapley values of all players.
Limitations of our method . Our non-asymptotic bound for the non-parametric regression setting in
Corollary 6 indicates that DU-Shapley works better when agents’ datasets are regular in the sense
that they have similar sizes. Hence, a limitation of our approximation is that it may work poorly in
settings where some players have large datasets compared to others, as the distribution of the random
variable within the Shapley value drives apart from being uniform. Moreover, our convergence
result in Theorem 2 (for all use-cases) assume the existence of limits, which roughly requires that
heterogeneity between players—in terms of both dataset size and variance—can be bounded. This
also indicates that convergence may be not be guaranteed if the heterogeneity is arbitrarily high.
10Acknowledgments
This research was supported in part by the French National Research Agency (ANR) in the framework
of the PEPR IA FOUNDRY project (ANR-23-PEIA-0003) and through the grant DOOM ANR-
23-CE23-0002. It was also funded by the European Union (ERC, Ocean, 101071601). Views and
opinions expressed are however those of the author(s) only and do not necessarily reflect those of the
European Union or the European Research Council Executive Agency. Neither the European Union
nor the granting authority can be held responsible for them.
References
[1]Anish Agarwal, Munther Dahleh, and Tuhin Sarkar. A Marketplace for Data: An Algorithmic
Solution. In Proceedings of the ACM Conference on Economics and Computation , page
701–726, 2019.
[2]Javier Castro, Daniel Gómez, and Juan Tejada. Polynomial calculation of the Shapley value
based on sampling. Computers & Operations Research , 36(5):1726–1730, 2009.
[3]Georgios Chalkiadakis, Edith Elkind, and Michael J. Wooldridge. Computational Aspects of
Cooperative Game Theory . Morgan & Claypool Publishers, 2011.
[4]Jianbo Chen, Le Song, Martin J. Wainwright, and Michael I. Jordan. L-Shapley and C-Shapley:
Efficient Model Interpretation for Structured Data. In International Conference on Learning
Representations , 2019.
[5]Shay Cohen, Eytan Ruppin, and Gideon Dror. Feature Selection Based on the Shapley Value.
InInternational Joint Conference on Artificial Intelligence , page 665–670, 2005.
[6]Mingshu Cong, Han Yu, Xi Weng, and Siu Ming Yiu. A game-theoretic framework for incentive
mechanism design in federated learning. Federated Learning: Privacy and Incentive , pages
205–222, 2020.
[7]Ian C. Covert, Scott Lundberg, and Su-In Lee. Understanding Global Feature Contributions
with Additive Importance Measures. In Advances in Neural Information Processing Systems ,
2020.
[8]Kate Donahue and Jon Kleinberg. Model-sharing games: Analyzing federated learning under
voluntary participation. Proceedings of the AAAI Conference on Artificial Intelligence , 35(6):
5303–5311, May 2021. URL https://ojs.aaai.org/index.php/AAAI/article/view/
16669 .
[9]Kate Donahue and Jon Kleinberg. Optimality and Stability in Federated Learning: A Game-
theoretic Approach. In Advances in Neural Information Processing Systems , volume 34, 2021.
[10] Amirata Ghorbani and James Zou. Data Shapley: Equitable Valuation of Data for Machine
Learning. In International Conference on Machine Learning , 2019.
[11] Amirata Ghorbani, Michael Kim, and James Zou. A Distributional Framework For Data
Valuation. In International Conference on Machine Learning , 2020.
[12] Amirata Ghorbani, James Zou, and Andre Esteva. Data shapley valuation for efficient batch
active learning. In 2022 56th Asilomar Conference on Signals, Systems, and Computers , pages
1456–1462. IEEE, 2022.
[13] László Györfi and Martin Kroll. On rate optimal private regression under local differential
privacy. arXiv preprint arXiv:2206.00114 , 2022.
[14] Wassily Hoeffding. Probability Inequalities for Sums of Bounded Random Variables. Journal
of the American Statistical Association , 58(301):13–30, 1963.
[15] Ruoxi Jia, David Dao, Boxin Wang, Frances Ann Hubis, Nezihe Merve Gurel, Bo Li, Ce Zhang,
Costas Spanos, and Dawn Song. Efficient Task-Specific Data Valuation for Nearest Neighbor
Algorithms. Proc. VLDB Endow. , 12(11):1610–1623, 2019.
11[16] Ruoxi Jia, David Dao, Boxin Wang, Frances Ann Hubis, Nick Hynes, Nezihe Merve Gürel,
Bo Li, Ce Zhang, Dawn Song, and Costas J. Spanos. Towards Efficient Data Valuation Based on
the Shapley Value. In International Conference on Artificial Intelligence and Statistics , 2019.
[17] Kevin Jiang, Weixin Liang, James Y Zou, and Yongchan Kwon. Opendataval: a unified
benchmark for data valuation. Advances in Neural Information Processing Systems , 36, 2023.
[18] Jiawen Kang, Zehui Xiong, Dusit Niyato, Shengli Xie, and Junshan Zhang. Incentive mechanism
for reliable federated learning: A joint optimization approach to combining reputation and
contract theory. IEEE Internet of Things Journal , 6(6):10700–10714, 2019.
[19] R. Kelley Pace and Ronald Barry. Sparse spatial autoregressions. Statistics & Probability
Letters , 33(3):291–297, 1997.
[20] Andre I Khuri, Thomas Mathew, and Daan G Nel. A test to determine closeness of multivariate
satterthwaite’s approximation. Journal of Multivariate Analysis , 51(1):201–209, 1994.
[21] Ron Kohavi. Scaling up the Accuracy of Naive-Bayes Classifiers: A Decision-Tree Hybrid. In
International Conference on Knowledge Discovery and Data Mining , 1996.
[22] Patrick Kolpaczki, Viktor Bengs, Maximilian Muschalik, and Eyke Hüllermeier. Approximating
the Shapley value without marginal contributions. In Proceedings of the AAAI Conference on
Artificial Intelligence , volume 38, pages 13246–13255, 2024.
[23] Yongchan Kwon and James Zou. Beta Shapley: a Unified and Noise-reduced Data Valuation
Framework for Machine Learning . In International Conference on Artificial Intelligence and
Statistics , pages 8780–8802, 2022.
[24] Yongchan Kwon, Manuel A. Rivas, and James Zou. Efficient Computation and Analysis
of Distributional Shapley Values . In International Conference on Artificial Intelligence and
Statistics , pages 793–801, 2021.
[25] Weixin Liang, James Zou, and Zhou Yu. Beyond user self-reported likert scale ratings: A
comparison model for automatic dialog evaluation. arXiv preprint arXiv:2005.10716 , 2020.
[26] Weixin Liang, Kai-Hui Liang, and Zhou Yu. Herald: an annotation efficient method to detect
user disengagement in social conversations. arXiv preprint arXiv:2106.00162 , 2021.
[27] Scott M. Lundberg and Su-In Lee. A Unified Approach to Interpreting Model Predictions. In
Advances in Neural Information Processing Systems , page 4768–4777, 2017.
[28] Scott M. Lundberg, Gabriel Erion, Hugh Chen, Alex DeGrave, Jordan M. Prutkin, Bala Nair,
Ronit Katz, Jonathan Himmelfarb, Nisha Bansal, and Su-In Lee. From local explanations to
global understanding with explainable AI for trees. Nature Machine Intelligence , 2(1):56–67,
2020.
[29] Lingjuan Lyu, Xinyi Xu, Qian Wang, and Han Yu. Collaborative fairness in federated learning.
Federated Learning: Privacy and Incentive , pages 189–204, 2020.
[30] Olvi L. Mangasarian, W. Nick Street, and William H. Wolberg. Breast Cancer Diagnosis and
Prognosis Via Linear Programming. Operations Research , 43(4):570–577, 1995.
[31] Irwin Mann and Lloyd S. Shapley. Values of Large Games, IV: Evaluating the Electoral College
by Montecarlo Techniques . RAND Corporation, Santa Monica, CA, 1960.
[32] Rory Mitchell, Joshua Cooper, Eibe Frank, and Geoffrey Holmes. Sampling Permutations for
Shapley Value Estimation. Journal of Machine Learning Research , 23(43):1–46, 2022.
[33] Sérgio Moro, Paulo Cortez, and Paulo Rita. A data-driven approach to predict the success of
bank telemarketing. Decision Support Systems , 62:22–31, 2014.
[34] Guillermo Owen. Multilinear Extensions of Games. Management Science , 18(5):64–79, 1972.
12[35] Konstantin D Pandl, Fabian Feiland, Scott Thiebes, and Ali Sunyaev. Trustworthy machine
learning for health care: scalable data valuation with the shapley value. In Proceedings of the
Conference on Health, Inference, and Learning , pages 47–57, 2021.
[36] Fabian Pedregosa, Gaël Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion,
Olivier Grisel, Mathieu Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, Jake Vander-
plas, Alexandre Passos, David Cournapeau, Matthieu Brucher, Matthieu Perrot, and Édouard
Duchesnay. Scikit-learn: Machine Learning in Python. Journal of Machine Learning Research ,
12(85):2825–2830, 2011.
[37] Gabriel Fernando Pivaro, Santosh Kumar, Gustavo Fraidenraich, and Claudio Ferreira Dias.
On the exact and approximate eigenvalue distribution for sum of wishart matrices. IEEE
Transactions on Vehicular Technology , 66(11):10537–10541, 2017.
[38] Stephanie Schoch, Haifeng Xu, and Yangfeng Ji. CS-shapley: Class-wise shapley values for
data valuation in classification. In Advances in Neural Information Processing Systems , 2022.
[39] Robert J Serfling. Probability inequalities for the sum in sampling without replacement. The
Annals of Statistics , pages 39–48, 1974.
[40] Lloyd S. Shapley. A Value for N-Person Games . RAND Corporation, Santa Monica, CA, 1952.
[41] Dongsub Shim, Zheda Mai, Jihwan Jeong, Scott Sanner, Hyunwoo Kim, and Jongseong Jang.
Online class-incremental continual learning with adversarial shapley value. In Proceedings of
the AAAI Conference on Artificial Intelligence , volume 35, pages 9630–9638, 2021.
[42] Rachael Hwee Ling Sim, Yehong Zhang, Mun Choon Chan, and Bryan Kian Hsiang Low.
Collaborative machine learning with incentive-aware model rewards. In Proceedings of the 37th
International Conference on Machine Learning , ICML’20. JMLR.org, 2020.
[43] WY Tan and RP Gupta. On approximating a linear combination of central wishart matrices with
positive coefficients. Communications in Statistics-Theory and Methods , 12(22):2589–2600,
1983.
[44] Sebastian Shenghong Tay, Xinyi Xu, Chuan-Sheng Foo, and Bryan Kian Hsiang Low. Incen-
tivizing collaboration in machine learning via synthetic data rewards. In AAAI Conference on
Artificial Intelligence , 2021.
[45] Alexandre B. Tsybakov. Introduction to Nonparametric Estimation . Springer Publishing
Company, Incorporated, 1st edition, 2008. ISBN 0387790519.
[46] Jiachen T Wang, Yuqing Zhu, Yu-Xiang Wang, Ruoxi Jia, and Prateek Mittal. Threshold
knn-shapley: A linear-time and privacy-friendly approach to data valuation. arXiv preprint
arXiv:2308.15709 , 2023.
[47] Han Yu, Zelei Liu, Yang Liu, Tianjian Chen, Mingshu Cong, Xi Weng, Dusit Niyato, and
Qiang Yang. A fairness-aware incentive scheme for federated learning. In Proceedings of the
AAAI/ACM Conference on AI, Ethics, and Society , pages 393–399, 2020.
A Complementary Numerical results
All experiments were executed on a laptop running macOS 13.3.1 and equipped with Apple M1 chip
with 16GB of RAM. The minimum amount of compute was roughly 5 minutes while the maximum
one roughly 10 hours.
A.1 DU-Shapley vs SV ARM
We have looked at the probability at which SV ARM (Theorem 4 [ 22]) can ensure, after I2iterations
(without considering the warm up as part of the budget), an error equal to DU-Shapley’s accumulated
bias. We have considered the same value function than in Section 4.2 with nmax∈ {2·103,3·
103,5·103,104}and 100 simulations of sets of players at each time. Figure 3 shows the results. We
observe how SV ARM cannot ensure, with high enough probability, an approximation error equal to
the one of DU-Shapley .
13Figure 3: Probability that SV ARM guarantees an error equal to DU-Shapley’s bias
A.2 Approximating the Shapley value in Synthetic Data
We consider a toy dataset valuation problem associated to our heterogeneous linear regression with
local differential privacy use-case (Section 2.1.3) and we measure the value of a coalition Swith the
utility function in close-form from Proposition 1. We consider d= 10 .
In order to benchmark the performances of DU-Shapley , we consider four competitive approaches,
relying on Monte Carlo (MC) approximation strategies [ 32]. The first one, referred to as MC-Shapley
is the standard MC approximation defined in (6). The second one, coined MC-anti-Shapley
is a variance-reduced version of MC-Shapley that considers antithetic sampling. The third one
coined Owen-Shapley stands for the multilinear extension of Owen [34] which represents the
Shapley value as two nested expectations (further explained in Appendix B.3). Finally, the fourth
approach, coined Orthogonal-Shapley , relies on efficient permutation sampling techniques on
the hypersphere to draw permutations in (4)in a dependent way. To assess the performance of the
aforementioned Shapley value estimators, we used the mean square error (MSE) averaged over all
players. DU-Shapley is computed exactly by using (10) while, for each MC-based estimator, we
performed 25 Shapley value estimations to compute the MSE, and did it 10 times to obtain confidence
intervals for the MSE.
Figure 4 compares DU-Shapley (the horizontal line which does not depend on the sampling budget as
we compute it exactly) and the MC-based methods, which are computed at several different budgets.
The x-axis corresponds to the sampling budget allowed to the MC-bases methods w.r.t. DU-Shapley ,
i.e.,10−1means a budget equal to 10% the one of DU-Shapley, 100means same budget (indicated
by the black vertical line), and 101means 10 times the DU-Shapley budget. Remark that, even when
the MC-methods use 10 times the budget of DU-Shapley , our method keeps approximating better
the Shapley value.
14Figure 4: Worst-case comparison between DU-Shapley and MC-based approximations with different
budgets on synthetic datasets. From left to right, I= 10 andI= 20 ,ni∼U([103]),∀i∈ I.
(top) Scenario with small heterogeneity, σi/εi∼U([0,10]),∀i∈ I, (bottom) scenario with high
heterogeneity, σi/εi∼U([0,100]),∀i∈ I.
B Further details about numerical implementations
B.1 Datasets considered in Section 4.1.
Table 3 summarizes the real-world datasets considered in Section 4.1.
Table 3: Datasets considered in Section 4.1.
Dataset Size d Task
adult [21] 48,842 107 classification
breast-cancer [30] 699 30 classification
bank [33] 45,211 16 classification
cal-housing [19] 20,640 8 regression
make-regression [36] 1,000 10 regression
year [36] 515,345 90 regression
B.2 OpenDataVal implementations
In this section we describe more in detail the implementations of DataShapley, Leave-One-Out
(LOO), and KNN-Shapley for our numerical results in Section 4.3.
DataShapley, applied to the dataset valuation problem, simply corresponds to the method coined MC
in Section 4.1. Therefore, we sample datasets and output the averaged marginal contribution.
Regarding LOO, notice that it corresponds to compute just one marginal contribution, usually
computed on the big coalition, i.e.,
LOO i:=u(I)−u(I \ {i}).
15As players’ marginal contributions to large datasets tend to be small, we have preferred to sample one
dataset Dfrom D−iand to output
LOO i:=u(D∪Di)−u(D).
Finally, regarding KNN-Shapley, we refer the reader to [ 15], Section E.3 of the appendix who explain
how to adapt the method to dataset valuation.
B.3 Owen’s Shapley value approximation
In Section A.2, we considered the Shapley value approximation referred to as Owen-Shapley as a
state-of-the-art competitor to DU-Shapley . We provide in the following additional details regarding
Owen-Shapley . For the other competitors, we directly refer the interested reader to Mitchell et al.
[32]. Owen [ 34] studied the multilinear extension of a cooperative game and an alternative way to
express the Shapley value. Formally, a cooperative game G= (I, u)consists on a set of Iplayers
I={1,2, ..., I}and a value function u: 2I→Rsuch that, for any S⊆ I,u(S)corresponds to the
value generated by the coalition S. The multilinear extension of G, denoted ¯G= (I,¯u), is obtained
when considering the value function ¯u: [0,1]I→Rgiven by,
¯u(x1, x2, ..., x I) =X
S⊆IY
i∈SxiY
j /∈S(1−xi)u(S).
Intuitively, ¯u(x1, x2, ..., x I)corresponds to the expected value of a coalition when each player i∈ I
joins the coalition with probability xi. Theorem 5 in [ 34] gives an alternative way to compute the
Shapley value φi(u)of player iin game G, namely,
φi(u) =Z1
0∂¯u
∂xi(τ, ..., τ )dτ=Z1
0X
S⊆I\{ i}τ|S|(1−τ)I−|S|−1[u(S∪ {i})−u(S)]dτ
=Z1
0E
u(Ei(τ)∪i)−u(Ei(τ))
dτ=Eτ∼U([0,1])
E
u(Ei(τ)∪i)−u(Ei(τ))
,
whereEi(τ)is a random subset of I \ {i}, such that, ∀j∈ I \ { i},jis included in Ei(τ)with proba-
bility τ. In words, the Shapley value of player icorresponds to her expected marginal contribution to
the random set Ei(τ), when τis uniformly distributed on [0,1]. This brings an alternative way to use
Monte Carlo to approximate the Shapley value φi(u), coined Owen-Shapley, as,
ˆφOwen
i(u) =1
TTX
t=1u(Ei(τt)∪i)−u(Et
i(τt)),
where for each t∈ {1, ..., T}, we draw τtindependently and uniformly in [0,1]and then, create a
random set Ei(τt)by adding each player j∈ I \ { i}to it with probability τt.
C Missing proofs
C.1 Proof of Proposition 1
Proposition 1 .LetS ⊆ I be a coalition of players and consider the value function uas in (3). It
follows,
u(S) =−Tr
E
xx⊤
q(S)−d−1,where q(S) := P
i∈Sσi
εini2
P
i∈S σi
εi2ni,with the convention q(∅) = 0 .
In particular, considering ptest= N(0 ,Id), we get,
u(S) =d
d+ 1−q(S).
16Proof. LetS ⊆ I be a coalition of players and XS, YSbe the concatenation of their datasets. The
linear model can be rewritten in matrix form as
YS=XSθ+ηS,
where ηSis the concatenation of η(j)
ifor all i∈ S andj∈[ni]. Take ˆθS=
(X⊤
SΣ−1
SXS)−1X⊤
SΣ−1
SYSwhere ΣS= diag(( ε2
i)i∈S), and let x∼ptestbe a hold-out testing
datum in Rd. It follows,
 
x⊤(θ−ˆθS)2=X
i∈Sηiε−2
iXiX
i∈SX⊤
iε−2
iXi−1
xx⊤X
i∈SX⊤
iε−2
iXi−1X
i∈Sηiε−2
iXi
= TrX
i∈Sηiε−2
iXiX
i∈SX⊤
iε−2
iXi−1
xx⊤X
i∈SX⊤
iε−2
iXi−1X
i∈Sηiε−2
iXi
= Tr
xx⊤X
i∈SX⊤
iε−2
iXi−1X
i∈Sηiε−2
iXiX
i∈Sηiε−2
iXiX
i∈SX⊤
iε−2
iXi−1
= Tr
xx⊤X
i∈SX⊤
iε−2
iXi−1X
i∈SX
j∈SX⊤
iε−2
iηiη⊤
jε−2
jXjX
i∈SX⊤
iε−2
iXi−1
We take expectation with respect to the different stochastic terms. Since η(k)
i∼N(0, ε2
i)for any
i∈ S, k∈[ni], it holds,
E(η(k)
i∼N(0,ε2
i))k∈[ni]
i∈S 
x⊤(θ−ˆθS)2
= Tr
xx⊤X
i∈SX⊤
iε−2
iXi−1X
i∈SX⊤
iε−2
iXiX
i∈SX⊤
iε−2
iXi−1
= Tr
xx⊤X
i∈SX⊤
iε−2
iXi−1
Since players distributions differ on their variances,P
i∈SX⊤
iε−2
iXicorresponds to a semi-
correlated Wishart random variable where each1
εiXi∼N(0,(σi
εi)2Id). In particular, the semi-
correlated Wishart random variable can be approximated by a central Wishart distribution [ 37,43],
whose precision depends on the homogeneity of the coefficients σi/εiover all i∈ I, as showed in
[20]. It follows,
E(Xi∼N(0,σ2
iId))i∈S"X
i∈SX⊤
iε−2
iXi−1#
≈Id
(q(S)−d−1),
where
q(S) := P
i∈Sσi
εini2
P
i∈S σi
εi2ni.
With all this in mind, it follows,
E(η(j)
i∼N(0,ε2
i))j∈[ni]
i∈S
(Xi∼N(0,σ2
iId))i∈S 
x⊤(θ−ˆθS)2
= Tr
xx⊤ Id
(q(S)−d−1)
=1
q(S)−d−1Tr
xx⊤
.
In particular, considering ptest= N(0 ,Id), we get,
u(S) =d
d+ 1−q(S).
17C.2 Proof of Theorem 2
Theorem 2. Let{ni, γi}i∈[I]be two sequences of positive numbers such that the following limits
lim
I→∞1
IX
i∈[I]niγi=µA,lim
I→∞1
IX
i∈[I](niγi−µA)2=σ2
A,
lim
I→∞1
IX
i∈[I]niγ2
i=µB,lim
I→∞1
IX
i∈[I](niγ2
i−µB)2=σ2
B,
all exist, for some constants µA, µB, σA, σB>0. LetK∼U({0, . . . , I }),SK∼U([2I
K]), and
define q(SK)as in (9)for the third use-case. Then, almost surely, q(SK)/q(I)I→∞− − − → U([0,1]).
Proof. Introduce, for any t, t0∈(0,1)and any s >0,
µA(I) =1
IX
i∈[I]niγi, µ B(I) =1
IX
i∈[I]niγ2
i,
YA(t, I) =X
i∈S⌊It⌋niγi, Y B(t, I) =X
i∈S⌊It⌋niγ2
i.
RA(I, t0, s) =P
sup
t>t0YA(t, I)
⌊It⌋−µA(I)> s
,
RB(I, t0, s) =P
sup
t>t0YB(t, I)
⌊It⌋−µB(I)> s
.
By construction, YA(t, I)andYB(t, I)are sums of sampling without replacement of ⌊It⌋elements.
Therefore, by Corollary 1.3 in [39], for sfixed, there exists IA
0, IB
0∈Nsuch that,
RA(I, t0, s)≤(1−t0)σ2
A
⌊It0⌋s2,∀I≥IA
0andRB(I, t0, s)≤(1−t0)σ2
B
⌊It0⌋s2,∀I≥IB
0.
In other words, for any s >0andIlarge enough, almost surely, it holds,YA(t, I)
⌊It⌋−µA(I)≤sandYB(t, I)
⌊It⌋−µB(I)≤s.
It follows,q(S⌊It⌋)
⌊It⌋−µA(I)2
µB(I)=1
⌊It⌋·YA(t, I)2
YB(t, I)−µA(I)2
µB(I)
=YA(t, I)2
⌊It⌋2−µA(I)2+µA(I)2⌊It⌋
YB(t, I)−1
µB(I)
+YA(t, I)2
⌊It⌋2−µA(I)21
µB(I)
≤s
s+µA(I) +1
µB(I)
,
which is arbitrarily small as µA(I), µB(I)→µA, µB<∞. Therefore, almost surely,
lim
I→∞q(S⌊It⌋)
IµA(I)2/µB(I)=t.
The proof concludes noticing that
q(I) =IµA(I)2
µB(I),
and that K=⌊IU⌋withU∼U([0,1]).
C.3 Proof of Theorem 5
To prove Theorem 5, we need two preliminary results: Lemma 3, which itself needs two supplemen-
tary results (Lemmas 1 and 2), and Lemma 4, which is directly proved.
18C.3.1 Technical lemmata
Lemma 1. Consider a set of Ivalues N={n1, . . . , n I}. LetX1, . . . , X kandY1, . . . , Y kdenote,
respectively, krandom samples with and without replacement from N. For any continuous and
convex function f, it follows,
E
fkX
i=1Yi
≤E
fkX
i=1Xi
Proof. The proof follows from [14].
Lemma 2. LetI∈N,N:={n1, . . . , n I} ∈RI
+,µ=1
IPI
i=1nibe their mean value and
σ2=1
IPI
i=1(ni−µ)2be their variance. For k∈ {0, . . . , n }, letSk∼U({Sk⊆[I] :|Sk|=k})
be a uniform random variable on the subsets of {1, . . . , I }of size k, and nSk=P
i∈Sknibe the
random variable defined by the sum of the elements of Sk. LetK∼U({0, . . . , I })and define
Y=nSK. Then,
E[Y−µK|K=k] = 0, (12)
E
(Y−µK)2|K=k
≤kσ2. (13)
Proof. We prove (12) directly.
E[Y|K=k] =X
Sk⊆[I]:|Sk|=knSk1 I
k=1 I
kX
Sk⊆[I]:|Sk|=kX
i∈Skni
=1 I
kX
i∈[I]X
Sk⊆[I]:|Sk|=k
i∈Skni
=1 I
kX
i∈[I]niI−1
k−1
=(I−k)!k!
I!·(I−1)!
(k−1)!(I−k)!X
i∈[I]ni=µk.
Thus, (12) follows as E[µK|K=k] =µk. To prove (13), let(Xi)k
i=1bekindependent samples
from the set N. From Lemma 1 it holds,
E
(Y−µK)2|K=k
≤E 
µK−KX
i=1Xi2|K=k
=EKX
i=1(µ−Xi)2
|K=k
.
Therefore,
E
(Y−µK)2|K=k
≤EKX
i=1KX
j=1(µ−Xi) (µ−Xj)
|K=k
=EKX
i=1KX
j=1 
µ2−µ(Xi+Xj) +XiXj
|K=k
=kX
i=1kX
j=1 
µ2−µ(E[Xi|K=k] +E[Xj|K=k]) +E[XiXj|K=k]
=kX
i=1kX
j=1 
µ2−µ(E[Xi] +E[Xj]) +E[XiXj]
=kX
i=1 
µ2−2µE[Xi] +E[X2
i]
+kX
i=1kX
j=1
j̸=i 
µ2−µ(E[Xi] +E[Xj]) +E[Xi]E[Xj]
19=kX
i=1E
(µ−Xi)2
+kX
i=1kX
j=1
j̸=i 
µ2−2µ2+µ2
=kX
i=1E
(µ−Xi)2
=kX
i=1Var(µ−Xi) =kσ2.
The steps come from rearranging the terms, using the independence of Xiwith respect to K, the
independence of Xi,Xjfori̸=j, and finally that E[Xi] =µand Var (Xi) =σ2.
Lemma 3. LetI∈N,N:={n1, . . . , n I} ∈RI
+, and define,
µ=1
IIX
i=1ni, σ2=1
IIX
i=1(ni−µ)2, nmax= max
i∈Ini,
R:= max
i∈[I]|ni−µ|, τ = max
i∈[I]ni/min
i∈[I]ni.
Consider SK,nSK,K, andYas in Lemma 2. Let w:R+→Rbe a function in C2, increasing, and
suppose there exists κ∈R+, such that,
w(2)(n)≤κ
n2,∀n >0,
where w(k)is the k-th derivative of w. Then, it holds,
E[w(µK)−w(Y)]≤κ
2µ2I
9σ2(1 + ln( I)) +2R2τ2
nmax
.
Proof. The proof considers a second-order Taylor extension of watµkto recover the expected value
ofE[w(µK)−w(Y)]. Noticing that the first derivative has a null expected value, the upper bound
stated on the Lemma comes from bounding the expected value of the second derivative.
The Taylor-Lagrange Theorem on watµk > 0provides,
w(y) =w(µk) +w(1)(µk)(µk−y) +w(2)(τ)(µk−y)2
2,
for some τbetween yandµk. Therefore, there exists a random variable T, almost surely between
µK+andY, such that,
E[w(Y)−w(µK+)] =E
w(1)(µK+)(µK+−Y) +1
2w(2)(T)(µK+−Y)2
,
whereK+corresponds to Kconditioned to be positive. To avoid overcharging the notation, we drop
the index from K+. We observe that,
E
w(1)(µK)(µK−Y)
=E
E
w(1)(µK)(µK−Y)|K=k
=E
w(1)(µk)E
(µK−Y)|K=k
= 0,
by Lemma 2, Equation (12). Therefore,
E[w(Y)−w(µK)]=1
2E
w(2)(T)(µK−Y)2
≤1
2Ew(2)(T)(µK−Y)2
≤1
2Eκ
T2(µK−Y)2
=κ
2E1
T2(µK−Y)2
.
SettingI:=
|µK−Y| ≤1
2(µK+Y)	
, the previous expected value can be expressed as,
E1
T2(µK−Y)2
=E1
T2(µK−Y)2·I
+E1
T2(µK−Y)2·Ic
.
20We deal with each term separately. Notice that, as Tis almost surely between YandµK,
|µK−Y| ≤1
2(µK+Y) =⇒T≥1
3µK.
Thus,
E(µK−Y)2
T2·I
≤E"
(µK−Y)2
(µK
3)2·I#
=9
µ2IX
k=11
I·E(µk−Y)2
k2·I|K=k
≤9
Iµ2IX
k=1E(µk−Y)2
k2|K=k
≤9
Iµ2IX
k=1kσ2
k2
≤9σ2
Iµ2·(1 + ln( I)).
Regarding the second term, denote n:= max i∈Iniandn:= min i∈Ini. AsKn≤min{µK,Y} ≤
T, we have,
E(µK−Y)2
T2·Ic
≤E(RK)2
T2·Ic
≤E(RK)2
(nK)2·Ic
=R2
In2IX
k=1E1
k2k2·Ic|K=k
=R2
In2IX
k=1P
|µk−Y|>1
2(µk+Y)|K=k
≤R2
In2IX
k=1P
|µk−Y|>µk
2|K=k
≤R2
In2IX
k=1exp
−µ2k
2n
=2R2τ2
Iµ2nIX
k=1µ2
2nexp
−µ2k
2n
≤2R2τ2
Iµ2nZ∞
0µ2
2nexp
−µ2k
2n
dk=2R2τ2
Iµ2n,
as the integral corresponds to the cumulative distribution function of an exponential random variable
of parameter λ=µ2/2n. The upper bound on the theorem’s statement is obtained when gathering
all together.
Lemma 4. Letw:R+→R+be a smooth and increasing function such that
lim
n→∞n2|w(2)(n)|<∞.
Then, there exists κ >0such that n2|w(2)(n)| ≤κ.
Proof. Notice that the assumptions imply, in particular, that |w(2)(n)|is bounded. We argue by
contradiction. Suppose that for any m >0, there exists nmsuch that
n2
m|w(2)(nm)|> m.
Suppose the sequence (nm)mconverges to a point n∗. Then,
lim
m→∞n2
m|w(2)(nm)|>lim
m→∞m=∞,
21which is a contradiction with |w(2)(n)|being bounded. Therefore, necessarily (nm)mhas to diverge.
However, this implies,
lim
n→∞n2|w(2)(n)|= lim
m→∞n2
m|w(2)(nm)|>lim
m→∞m=∞,
obtaining again a contradiction.
C.3.2 Proof of Theorem 5
We are ready to prove Theorem 5.
Theorem 5.Under Assumption H1, there exists a constant κ >0, such that, for any i∈ I, it holds,
φi−ψi≤κ
(I−1)µ2
−i 
σ2
−i(1 + ln( I−1)) + ζ−i
,
where φiandψiare respectively the Shapley value and the DU-Shapley of player i,µ−i=
1
I−1P
j∈I\{ i}njis the average dataset size of other players, σ2
−i=1
I−1P
j∈I\{ i}(nj−µ−i)2its
empirical variance, and ζ−imeasures the variability of the dataset sizes across players. Formally, it
is defined as
ζ−i:=R2
−iτ2
−i
4nmax
−i
where R−i:= max j∈I\{ i}|nj−µ−i|,nmax
−i:= max j∈I\{ i}nj, and τ−i:=nmax
−i
minj∈I\{ i}nj.
Proof. Under Assumption H1, Lemma 4 implies the existence of κ >0such that the value function
wsatisfies all assumptions from Lemma 3. Theorem 5 comes from (a) noticing that
φi=E[w(Y−i+ni)−w(Y−i)], ψ i=E[w(Kµ−i+ni)−w(Kµ−i)],
whereK∼U([I−1])andY−i=nS(i)
KwithS(i)
Ktaking values on the subsets of I \ {i}of sizeK,
(b) writing
|φi−ψi| ≤ |E[w(Y+ni)−w(Kµ−i+ni)]|+|E[w(Y)−w(Kµ−i)]|,
and (c) applying Lemma 3 to each of the expected values, as the function n→w(n+ni)also
satisfies H1.
22NeurIPS Paper Checklist
1.Claims
Question: Do the main claims made in the abstract and introduction accurately reflect the
paper’s contributions and scope?
Answer: [Yes]
Justification: Both abstract and introduction states exactly what we proved in our article.
Guidelines:
•The answer NA means that the abstract and introduction do not include the claims
made in the paper.
•The abstract and/or introduction should clearly state the claims made, including the
contributions made in the paper and important assumptions and limitations. A No or
NA answer to this question will not be perceived well by the reviewers.
•The claims made should match theoretical and experimental results, and reflect how
much the results can be expected to generalize to other settings.
•It is fine to include aspirational goals as motivation as long as it is clear that these goals
are not attained by the paper.
2.Limitations
Question: Does the paper discuss the limitations of the work performed by the authors?
Answer: [Yes]
Justification: A limitations section is included in Section 5. Moreover, all mathematical
assumptions are clearly stated on the main article and discussed.
Guidelines:
•The answer NA means that the paper has no limitation while the answer No means that
the paper has limitations, but those are not discussed in the paper.
• The authors are encouraged to create a separate "Limitations" section in their paper.
•The paper should point out any strong assumptions and how robust the results are to
violations of these assumptions (e.g., independence assumptions, noiseless settings,
model well-specification, asymptotic approximations only holding locally). The authors
should reflect on how these assumptions might be violated in practice and what the
implications would be.
•The authors should reflect on the scope of the claims made, e.g., if the approach was
only tested on a few datasets or with a few runs. In general, empirical results often
depend on implicit assumptions, which should be articulated.
•The authors should reflect on the factors that influence the performance of the approach.
For example, a facial recognition algorithm may perform poorly when image resolution
is low or images are taken in low lighting. Or a speech-to-text system might not be
used reliably to provide closed captions for online lectures because it fails to handle
technical jargon.
•The authors should discuss the computational efficiency of the proposed algorithms
and how they scale with dataset size.
•If applicable, the authors should discuss possible limitations of their approach to
address problems of privacy and fairness.
•While the authors might fear that complete honesty about limitations might be used by
reviewers as grounds for rejection, a worse outcome might be that reviewers discover
limitations that aren’t acknowledged in the paper. The authors should use their best
judgment and recognize that individual actions in favor of transparency play an impor-
tant role in developing norms that preserve the integrity of the community. Reviewers
will be specifically instructed to not penalize honesty concerning limitations.
3.Theory Assumptions and Proofs
Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?
Answer: [Yes]
23Justification: Assumptions are stated in each theoretical result and all proofs are included in
the supplementary material. We have carefully checked the accuracy of all mathematical
steps.
Guidelines:
• The answer NA means that the paper does not include theoretical results.
•All the theorems, formulas, and proofs in the paper should be numbered and cross-
referenced.
•All assumptions should be clearly stated or referenced in the statement of any theorems.
•The proofs can either appear in the main paper or the supplemental material, but if
they appear in the supplemental material, the authors are encouraged to provide a short
proof sketch to provide intuition.
•Inversely, any informal proof provided in the core of the paper should be complemented
by formal proofs provided in appendix or supplemental material.
• Theorems and Lemmas that the proof relies upon should be properly referenced.
4.Experimental Result Reproducibility
Question: Does the paper fully disclose all the information needed to reproduce the main ex-
perimental results of the paper to the extent that it affects the main claims and/or conclusions
of the paper (regardless of whether the code and data are provided or not)?
Answer: [Yes]
Justification: At the beginning of each numerical results section we explain the used
methodology and the sources of the used data. Besides, Appendix B complements these
descriptions.
Guidelines:
• The answer NA means that the paper does not include experiments.
•If the paper includes experiments, a No answer to this question will not be perceived
well by the reviewers: Making the paper reproducible is important, regardless of
whether the code and data are provided or not.
•If the contribution is a dataset and/or model, the authors should describe the steps taken
to make their results reproducible or verifiable.
•Depending on the contribution, reproducibility can be accomplished in various ways.
For example, if the contribution is a novel architecture, describing the architecture fully
might suffice, or if the contribution is a specific model and empirical evaluation, it may
be necessary to either make it possible for others to replicate the model with the same
dataset, or provide access to the model. In general. releasing code and data is often
one good way to accomplish this, but reproducibility can also be provided via detailed
instructions for how to replicate the results, access to a hosted model (e.g., in the case
of a large language model), releasing of a model checkpoint, or other means that are
appropriate to the research performed.
•While NeurIPS does not require releasing code, the conference does require all submis-
sions to provide some reasonable avenue for reproducibility, which may depend on the
nature of the contribution. For example
(a)If the contribution is primarily a new algorithm, the paper should make it clear how
to reproduce that algorithm.
(b)If the contribution is primarily a new model architecture, the paper should describe
the architecture clearly and fully.
(c)If the contribution is a new model (e.g., a large language model), then there should
either be a way to access this model for reproducing the results or a way to reproduce
the model (e.g., with an open-source dataset or instructions for how to construct
the dataset).
(d)We recognize that reproducibility may be tricky in some cases, in which case
authors are welcome to describe the particular way they provide for reproducibility.
In the case of closed-source models, it may be that access to the model is limited in
some way (e.g., to registered users), but it should be possible for other researchers
to have some path to reproducing or verifying the results.
245.Open access to data and code
Question: Does the paper provide open access to the data and code, with sufficient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?
Answer: [Yes]
Justification: The codes are included and the implementations are detailed in the appendix.
Guidelines:
• The answer NA means that paper does not include experiments requiring code.
•Please see the NeurIPS code and data submission guidelines ( https://nips.cc/
public/guides/CodeSubmissionPolicy ) for more details.
•While we encourage the release of code and data, we understand that this might not be
possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
including code, unless this is central to the contribution (e.g., for a new open-source
benchmark).
•The instructions should contain the exact command and environment needed to run to
reproduce the results. See the NeurIPS code and data submission guidelines ( https:
//nips.cc/public/guides/CodeSubmissionPolicy ) for more details.
•The authors should provide instructions on data access and preparation, including how
to access the raw data, preprocessed data, intermediate data, and generated data, etc.
•The authors should provide scripts to reproduce all experimental results for the new
proposed method and baselines. If only a subset of experiments are reproducible, they
should state which ones are omitted from the script and why.
•At submission time, to preserve anonymity, the authors should release anonymized
versions (if applicable).
•Providing as much information as possible in supplemental material (appended to the
paper) is recommended, but including URLs to data and code is permitted.
6.Experimental Setting/Details
Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?
Answer: [Yes]
Justification: All details are explained in each numerical results section.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The experimental setting should be presented in the core of the paper to a level of detail
that is necessary to appreciate the results and make sense of them.
•The full details can be provided either with the code, in appendix, or as supplemental
material.
7.Experiment Statistical Significance
Question: Does the paper report error bars suitably and correctly defined or other appropriate
information about the statistical significance of the experiments?
Answer: [Yes]
Justification: The experiments in Section 4.2 and Appendices A.1 and A.2 present error
bars.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The authors should answer "Yes" if the results are accompanied by error bars, confi-
dence intervals, or statistical significance tests, at least for the experiments that support
the main claims of the paper.
•The factors of variability that the error bars are capturing should be clearly stated (for
example, train/test split, initialization, random drawing of some parameter, or overall
run with given experimental conditions).
25•The method for calculating the error bars should be explained (closed form formula,
call to a library function, bootstrap, etc.)
• The assumptions made should be given (e.g., Normally distributed errors).
•It should be clear whether the error bar is the standard deviation or the standard error
of the mean.
•It is OK to report 1-sigma error bars, but one should state it. The authors should
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
of Normality of errors is not verified.
•For asymmetric distributions, the authors should be careful not to show in tables or
figures symmetric error bars that would yield results that are out of range (e.g. negative
error rates).
•If error bars are reported in tables or plots, The authors should explain in the text how
they were calculated and reference the corresponding figures or tables in the text.
8.Experiments Compute Resources
Question: For each experiment, does the paper provide sufficient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?
Answer: [Yes]
Justification: All details are included at the beginning of Appendix B.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The paper should indicate the type of compute workers CPU or GPU, internal cluster,
or cloud provider, including relevant memory and storage.
•The paper should provide the amount of compute required for each of the individual
experimental runs as well as estimate the total compute.
•The paper should disclose whether the full research project required more compute
than the experiments reported in the paper (e.g., preliminary or failed experiments that
didn’t make it into the paper).
9.Code Of Ethics
Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines ?
Answer: [Yes]
Justification: The research conducted conformes with all points in the NeurIPS Code of
Ethics.
Guidelines:
•The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
•If the authors answer No, they should explain the special circumstances that require a
deviation from the Code of Ethics.
•The authors should make sure to preserve anonymity (e.g., if there is a special consid-
eration due to laws or regulations in their jurisdiction).
10.Broader Impacts
Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?
Answer: [Yes]
Justification: We discuss potential applications in Section 1.
Guidelines:
• The answer NA means that there is no societal impact of the work performed.
•If the authors answer NA or No, they should explain why their work has no societal
impact or why the paper does not address societal impact.
26•Examples of negative societal impacts include potential malicious or unintended uses
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
(e.g., deployment of technologies that could make decisions that unfairly impact specific
groups), privacy considerations, and security considerations.
•The conference expects that many papers will be foundational research and not tied
to particular applications, let alone deployments. However, if there is a direct path to
any negative applications, the authors should point it out. For example, it is legitimate
to point out that an improvement in the quality of generative models could be used to
generate deepfakes for disinformation. On the other hand, it is not needed to point out
that a generic algorithm for optimizing neural networks could enable people to train
models that generate Deepfakes faster.
•The authors should consider possible harms that could arise when the technology is
being used as intended and functioning correctly, harms that could arise when the
technology is being used as intended but gives incorrect results, and harms following
from (intentional or unintentional) misuse of the technology.
•If there are negative societal impacts, the authors could also discuss possible mitigation
strategies (e.g., gated release of models, providing defenses in addition to attacks,
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
feedback over time, improving the efficiency and accessibility of ML).
11.Safeguards
Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?
Answer: [NA]
Justification: [NA]
Guidelines:
• The answer NA means that the paper poses no such risks.
•Released models that have a high risk for misuse or dual-use should be released with
necessary safeguards to allow for controlled use of the model, for example by requiring
that users adhere to usage guidelines or restrictions to access the model or implementing
safety filters.
•Datasets that have been scraped from the Internet could pose safety risks. The authors
should describe how they avoided releasing unsafe images.
•We recognize that providing effective safeguards is challenging, and many papers do
not require this, but we encourage authors to take this into account and make a best
faith effort.
12.Licenses for existing assets
Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?
Answer: [Yes]
Justification: All creators are properly credited by citing their works.
Guidelines:
• The answer NA means that the paper does not use existing assets.
• The authors should cite the original paper that produced the code package or dataset.
•The authors should state which version of the asset is used and, if possible, include a
URL.
• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
•For scraped data from a particular source (e.g., website), the copyright and terms of
service of that source should be provided.
•If assets are released, the license, copyright information, and terms of use in the
package should be provided. For popular datasets, paperswithcode.com/datasets
has curated licenses for some datasets. Their licensing guide can help determine the
license of a dataset.
27•For existing datasets that are re-packaged, both the original license and the license of
the derived asset (if it has changed) should be provided.
•If this information is not available online, the authors are encouraged to reach out to
the asset’s creators.
13.New Assets
Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?
Answer: [NA]
Justification: [NA]
Guidelines:
• The answer NA means that the paper does not release new assets.
•Researchers should communicate the details of the dataset/code/model as part of their
submissions via structured templates. This includes details about training, license,
limitations, etc.
•The paper should discuss whether and how consent was obtained from people whose
asset is used.
•At submission time, remember to anonymize your assets (if applicable). You can either
create an anonymized URL or include an anonymized zip file.
14.Crowdsourcing and Research with Human Subjects
Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as
well as details about compensation (if any)?
Answer: [NA]
Justification: [NA]
Guidelines:
•The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
•Including this information in the supplemental material is fine, but if the main contribu-
tion of the paper involves human subjects, then as much detail as possible should be
included in the main paper.
•According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
or other labor should be paid at least the minimum wage in the country of the data
collector.
15.Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects
Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?
Answer: [NA]
Justification: [NA]
Guidelines:
•The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
•Depending on the country in which research is conducted, IRB approval (or equivalent)
may be required for any human subjects research. If you obtained IRB approval, you
should clearly state this in the paper.
•We recognize that the procedures for this may vary significantly between institutions
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
guidelines for their institution.
•For initial submissions, do not include any information that would break anonymity (if
applicable), such as the institution conducting the review.
28