Published in Transactions on Machine Learning Research (12/2024)
Node-Specific Space Selection via Localized Geometric Hy-
perbolicity in Graph Neural Networks
See Hian Lee seehian001@e.ntu.edu.sg
Department of Electrical and Electronic Engineering
Nanyang Technological University
Ji Feng jifeng@ntu.edu.sg
Department of Electrical and Electronic Engineering
Nanyang Technological University
Tay Wee Peng wptay@ntu.edu.sg
Department of Electrical and Electronic Engineering
Nanyang Technological University
Reviewed on OpenReview: https: // openreview. net/ pdf? id= tHteJFeN1y
Abstract
Many graph neural networks have been developed to learn graph representations in either
Euclidean or hyperbolic space, with all nodes’ representations embedded in a single space.
However, a graph can have hyperbolic and Euclidean geometries at different regions of the
graph. Thus, it is sub-optimal to indifferently embed an entire graph into a single space.
In this paper, we explore and analyze two notions of local hyperbolicity, describing the
underlying local geometry: geometric (Gromov) and model-based, to determine the preferred
space of embedding for each node. The two hyperbolicities’ distributions are aligned using
the Wasserstein metric such that the calculated geometric hyperbolicity guides the choice
of the learned model hyperbolicity. As such our model Joint Space Graph Neural Network
(JSGNN) can leverage both Euclidean and hyperbolic spaces during learning by allowing
node-specific geometry space selection. We evaluate our model on both node classification
and link prediction tasks and observe promising performance compared to baseline models.
1 Introduction
Graph neural networks (GNNs) are neural networks that learn from graph-structured data. Many works
such as Graph Convolutional Network (GCN) (Kipf & Welling, 2016), Graph Attention Network (GAT)
(Veličković et al., 2018), GraphSAGE (Hamilton et al., 2017) and their variants operate on the Euclidean
space and have been applied in many areas such as recommender systems (Ying et al., 2018; Chen et al.,
2022a), chemistry (Gilmer et al., 2017) and financial systems (Sawhney et al., 2021). Despite their remarkable
accomplishments, their performances are still limited by the representation ability of Euclidean space. They
are unable to achieve the best performance in situations when the data exhibit non-Euclidean characteristics
such as scale-free, tree-like, or hierarchical structures (Yang et al., 2022).
As such, hyperbolic spaces have gained traction in research as they have been proven to better embed
tree-like, hierarchical structures compared to the Euclidean geometry (Bachmann et al., 2019; Cho et al.,
2019). Intuitively, encoding non-Euclidean structures such as trees in the Euclidean space would result in
more considerable distortion since the number of nodes in a tree increases exponentially with the depth of
the tree while the Euclidean space only grows polynomially (Zhu et al., 2020). In such cases, the hyperbolic
geometry serves as an alternative to learning those structures with comparably smaller distortion as the
hyperbolic space has the exponential growth property (Yang et al., 2022). As such, hyperbolic versions of
1Published in Transactions on Machine Learning Research (12/2024)
GNNs such as HGCN (Chami et al., 2019), HGNN (Liu et al., 2019), HGAT (Zhang et al., 2021a) and LGCN
(Zhang et al., 2021b) have been proposed.
Nevertheless, real-world graphs are often complex. They are neither solely made up of Euclidean nor non-
Euclidean structures alone but a mixture of geometrical structures. Consider a localized version of geometric
hyperbolicity, a concept from geometry group theory measuring how tree-like the underlying space is for each
node in the graph (refer to Section 3.1 for more details). We observe a mixture of local geometric hyperbolicity
values in most of the benchmark datasets we employ for our experiments as seen in Fig. 2. This implies
that the graphs contain a mixture of geometries and thus, it is not ideal to embed the graphs into a single
geometry space, regardless of Euclidean or hyperbolic as it inevitably leads to undesired structural inductive
biases and distortions (Yang et al., 2022).
Taking a graph containing both lattice-like and tree-like structures as an example, Fig. 1c and Fig. 1f
shows that 15 of the blue-colored nodes in the tree structure are calculated to have 2-hop local geometric
hyperbolicity value of zero, while 12 of the purple nodes have a value of one and the other 3 purple nodes
(at the center of the lattice) have a value of two (the smaller the hyperbolicity value, the more hyperbolic).
This localized metric can therefore serve as an indication during learning on which of the two spaces is more
suitable to embed the respective nodes.
(a)
 (b)
 (c)
(d)
 (e)
 (f)
Figure 1: Example graphs. (a) Lattice-like graph. (b) A tree. (c) A combined graph containing both lattice
and tree structure. (d-f) The histograms reflect the geometric hyperbolicity in the respective graphs.
Here we address this mixture of geometry in a graph and propose Joint Space Graph Neural Network (JSGNN)
that performs learning on a joint space consisting of both Euclidean and hyperbolic geometries. To achieve
this, we first update all the node features in both Euclidean and hyperbolic spaces independently, giving
rise to two sets of updated node features. Then, we employ exponential and logarithmic maps to bridge the
two spaces and an attention mechanism is used as a form of model hyperbolicity, taking into account the
underlying structure around each node and the corresponding node features. The learned model hyperbolicity
is guided by geometric hyperbolicity and is used to “softly decide” the most suitable embedding space for
each node and to reduce the two sets of updated features into only one set. Ideally, a node should be either
hyperbolic or Euclidean and not both simultaneously, thus, we also introduce an additional loss term to
achieve this non-uniform characteristic. Related works are discussed in Appendix E.
2Published in Transactions on Machine Learning Research (12/2024)
(a)
 (b)
 (c)
 (d)
(e)
 (f)
 (g)
Figure 2: Distributions of geometric hyperbolicity for all datasets, obtained by computing δGv,∞on each
nodes’ 2-hop subgraph. See Appendix C for more results.
2 Background
In this section, we give a brief overview of hyperbolic geometry that will be used in the paper. Readers are
referred to Lee (2018) for further details. Moreover, we review GAT and its hyperbolic version.
2.1 Hyperbolic geometry
A hyperbolic space is a non-Euclidean space with constant negative curvature. There are different but
equivalent models to describe the same hyperbolic geometry. In this paper, we work with the Poincaré ball
model, in which all points are inside a ball. The hyperbolic space with constant negative curvature cis
denoted by (Dn
c,gc
x). It consists of the n-dimensional hyperbolic manifold Dn
c={x∈Rn:c∥x∥<1}with the
Riemannian metric gc
x= (λc
x)2gE, whereλc
x= 2/(1−c∥x∥2)andgE=Inis the Euclidean metric.
At each x∈Dn
c, there is a tangent space TxDn
c, which can be viewed as the first-order approximation of the
hyperbolic manifold at x(Bachmann et al., 2019). The tangent space is then useful to perform Euclidean
operations that we are familiar with but are undefined in hyperbolic spaces. A hyperbolic space and the
tangent space at a point are connected through the exponential map expc
x:TxDn
c→Dn
cand logarithmic map
logc
x:Dn
c→T xDn
c, specifically defined as follows:
expc
x(v) =x⊕c/parenleftig
tanh/parenleftbigg√cλc
x∥v∥
2/parenrightbiggv√c∥v∥/parenrightig
, (1)
logc
x(y) =2√cλcxtanh−1(√c∥−x⊕cy∥)−x⊕cy
∥−x⊕cy∥, (2)
where x,y∈Dn
c,v∈TxDn
cand⊕cis the Möbius addition. For convenience, we write DforDn
cif no confusion
arises.
A salient feature of hyperbolic geometry is that it is “thinner” than Euclidean geometry. Visually, more points
can be squeezed in a hyperbolic subspace having the same shape as its Euclidean counterpart, due to the
different metrics in the two spaces. We discuss the graph version in Section 3.1 below.
2.2 Graph attention and message passing
Consider a graph G= (V,E), whereVis the set of vertices, Eis the set of edges, and each node in V
is associated with a node feature hv. Recall that GAT is a GNN that updates node representations using
3Published in Transactions on Machine Learning Research (12/2024)
message passing by updating edge weights concurrently. Specifically, for one layer of GAT (Veličković et al.,
2018), the node features are updated as follows:
h′
v=σ/parenleftig/summationdisplay
j∈N(v)αvjWhj/parenrightig
, (3)
αvj=exp(evj)/summationtext
k∈N(v)exp(evk), (4)
evj= LeakyReLU( a⊺[Whv∥Whj]), (5)
where∥denotes the concatenation operation, σdenotes an activation function, arepresents the learnable
attention vector, Wis the weight matrix for a linear transformation and αdenotes the normalized attention
scores.
This model has been proven to be successful in many graph-related machine learning tasks.
2.3 Hyperbolic attention model
To derive a hyperbolic version of GAT, we adopt the following strategy. We perform feature aggregation in
the tangent spaces of points in the hyperbolic space. Features are mapped between hyperbolic space and
tangent spaces using the pair of exponential and logarithmic functions: expc
xandlogc
x.
With this, we denote Euclidean features as hRand hyperbolic features as hD. Then one layer of message
propagation in the hyperbolic GAT is as follows (Zhu et al., 2020):
h′
v,D=σ/parenleftig/summationdisplay
j∈N(v)αvjlogc
o(W⊗chj,D⊕cb)/parenrightig
, (6)
evj= LeakyReLU/parenleftig
a⊺/bracketleftig
ˆhv∥ˆhj/bracketrightig
×dD(hv,D,hj,D)/parenrightig
, (7)
dD(hv,D,hj,D) =2√ctanh−1(√c∥−hv,D⊕chj,D∥), (8)
αvj= softmax j(evj), (9)
wheredDis the normalized hyperbolic distance, ˆhj=logc
o(W⊗chj,D), while⊗cand⊕crepresent the Möbius
matrix multiplication and addition, respectively.
3 Joint Space Learning
In this section, we propose our joint space learning model. The model relies on comparing two different
notions of hyperbolicity: geometric hyperbolicity and model hyperbolicity. We start by introducing the former,
which also serves as the motivation for the design of our GNN model.
3.1 Local geometry and geometric hyperbolicity
Gromov’sδ-hyperbolicity is a mathematical notion from geometry group theory to measure how tree-like a
metric space is in terms of metric or distance structure (Adcock et al., 2013; Chami et al., 2019). The precise
definition is given as follows.
Definition 1 (Gromov 4-point δ-hyperbolicity (Bridson & Haefliger, 1999) p.410) .For a metric space X
with metric d(·,·), it isδ-hyperbolic, where δ≥0if the four-point condition holds:
d(x,y) +d(z,t)≤
max{d(x,z) +d(y,t),d(z,y) +d(x,t)}+ 2δ,(10)
for anyx,y,z,t∈X.Xis hyperbolic if it is δ-hyperbolic for some δ≥0.
4Published in Transactions on Machine Learning Research (12/2024)
This condition of δ-hyperbolicity is equivalent to the Gromov thin triangle condition. For example, any tree
is (0-)hyperbolic, and Rn, wheren≥2is not hyperbolic. However, if Xis a compact metric space, then Xis
alwaysδ-hyperbolic for some δlarge enough such as δ=diameter (X). Therefore, it is insufficient to just
labelXas hyperbolic or not. We want to quantify hyperbolicity such that a space with smaller hyperbolicity
resembles more of a tree.
Inspired by the four-point condition, we define the ∞-version and the 1-version of hyperbolicity as follows.
Definition 2. For a compact metric space Xandx,y,z,t∈X, denote infδ≥0{(10)holds forx,y,z,t}by
τX(x,y,z,t ). Define
δX,∞= sup
x,y,z,t∈XτX(x,y,z,t ),
δX,1=Ex,y,z,t∼Unif(X4)[τX(x,y,z,t )],
where Unifrepresents the uniform distribution.
In order for these invariants to be useful for graphs, we require them to be almost identical for graphs with
similar structures. We shall see that this is indeed the case. Before stating the result, we need a few more
concepts.
LetGbe the space of weighted, undirected simple graphs. Though for most experiments, the given graphs are
unweighted. However, aggregation mechanisms such as attention essentially generate weights for the edges.
Therefore, for both theoretical and practical reasons, it makes sense to expand the graph domain to include
weighted graphs.
For eachG= (V,E)∈G, it has a canonical path metric dG, anddGmakesGinto a metric space including
non-vertex points on the edges. For ϵ >0, there is the subspace GϵofGconsisting of graphs whose edge
weights are greater than ϵ.
On the other hand, there is a metric on the space GandGϵ, called the Gromov-Hausdorff metric (Bridson &
Haefliger (1999) p.72). To define it, we first introduce the Hausdorff distance. Let XandYbe two subsets of
a metric space (M,d). Then the Hausdorff distance dH(X,Y )betweenXandYis
dH(X,Y ) = max{sup
x∈Xd(x,Y),sup
y∈Yd(X,y)},
whered(x,Y) =infy∈Yd(x,y),d(X,y) =infx∈Xd(x,y). The Hausdorff distance measures in the worst case,
how far away a point in Xis away from Yand vice versa.
In general, we want to also compare spaces that do not a priori belong to a common ambient space. For
this, ifX,Yare two compact metric spaces, then their Gromov-Hausdorff distance dGH(X,Y)is defined
as the infimum of all numbers dH(f(X),g(Y))for all metric spaces Mand all isometric embeddings
f:X→M,g :Y→M. Intuitively, the Gromov-Hausdorff distance measures how far XandYare from
being isometric. The following is proved in the Appendix.
Proposition 1. SupposeGand its subspaces have the Gromov-Hausdorff metric. Then δG,∞is Lipschitz
continuous w.r.t. G∈GandδG,1is continuous w.r.t. G∈Gϵfor anyϵ>0.
Consider a graph G. We fix either δG,∞orδG,1as a measure of hyperbolicity, and apply to each local
neighborhood of G. To be more precise, it is studied (Chen et al., 2020b; Rong et al., 2020) that many
popular GNN models have a shallow structure. It is customary to have a 2-layer network possibly due to
oversmoothing (Chen et al., 2020a; Chamberlain et al., 2021b; Zeng et al., 2021) and oversquashing (Topping
et al., 2022) phenomena. In such models, each node only aggregates information in a small neighborhood.
Therefore, if we fix a small kand letGvbe the subgraph of the k-hop neighborhood of v∈V, then it is more
appropriate to study the hyperbolicity δv, eitherδGv,∞orδGv,1, ofGv. For our experiments, the former is
utilized. We call δvthegeometric hyperbolicity at nodev. The collection ∆V={δv:v∈V}allows us to
obtain an empirical distribution µGof geometric hyperbolicity on the sample space R≥0.
For instance, we can build histograms to acquire the distributions as observed in Fig. 2. In Cora, we observe
that a substantial number of nodes have small (local) hyperbolicity, despite its high global hyperbolicity value
5Published in Transactions on Machine Learning Research (12/2024)
(a)
(b)
Figure 3: Comparison between JSGNN and GIL (Zhu et al., 2020) in leveraging Euclidean and hyperbolic
spaces. Both models utilizes GAT in Section 2.2 and HGAT in Section 2.3 for message passing in Euclidean
and hyperbolic space respectively. (a) Soft space selection mechanism of JSGNN where trainable selection
weightsβv,R,βv,Dare non-uniform, effectively selecting the better of the two spaces considered. (b) Feature
interaction mechanism of GIL where ζ,ζ′∈Rare trainable weights and dD,dRare the hyperbolic distance
(cf. (8)) and Euclidean distance respectively. The node embeddings of both spaces in GIL are adjusted
based on distance, potentially introducing more noise to the branches as there is minimal information in the
sub-optimal space to “enhance” the representation in the better space.
(Chami et al., 2019; Liu et al., 2022a). Meanwhile, Airport is argued to be globally hyperbolic, but a large
proportion of nodes has large local hyperbolicity. However, this is not a contradiction as we are considering
the local structures of the graph. We call µGthedistribution of geometric hyperbolicity . It depends only on G
andk.
3.2 Space selection and model hyperbolicity
In this section, we describe the backbone of our model and introduce the notion of model hyperbolicity. Our
model consists of two branches, one using Euclidean geometry and the other using hyperbolic geometry. We
primarily use GAT (cf. Section 2.2) for the Euclidean model and HGAT (cf. Section 2.3) for the hyperbolic
model. Other pairs of Euclidean or hyperbolic models (e.g., GCN and HGCN) can also be applied to the
corresponding branches. In Section 4.3, we show the experimental results on two variants of JSGNN.
After the respective message propagation, we would have two sets of updated node embeddings, the Euclidean
embedding ZRand the hyperbolic embedding ZD. The two sets of embeddings are combined into a single
embedding Z={zv,v∈V}through an attention mechanism that serves as a space selection procedure. The
attention mechanism is performed in a Euclidean space. Thus, the hyperbolic embeddings are first mapped
into the tangent space using the logarithmic map. Mathematically, the normalized attention score indicating
whether a node should be embedded in the hyperbolic space βv,Dor Euclidean space βv,Ris as follows:
wv,R=q⊺tanh(Mzv,R+b), (11)
wv,D=q⊺tanh(Mlogc
o(zv,D) +b), (12)
βv,R=exp(wv,R)
exp(wv,R) + exp(wv,D), (13)
6Published in Transactions on Machine Learning Research (12/2024)
βv,D=exp(wv,D)
exp(wv,R) + exp(wv,D), (14)
where qrefers to the learnable space selection attention vector, Mis a learnable weight matrix, bdenotes a
learnable bias and βv,D+βv,R= 1, for allv∈V. The weights βv,Dandβv,Rare conditioned to be non-uniform
as illustrated in Section 3.4. The two sets of space-specific node embeddings can then be combined via a
convex combination using the learned weights as follows:
zv=βv,Rzv,R+βv,Dlogc
o(zv,D),∀v∈V. (15)
This gives one layer of the model architecture of JSGNN, as illustrated in Fig. 3.
The parameter βv,R,v∈Vcontrols whether the combined output, consisting of both hyperbolic and Euclidean
components, should rely more on the hyperbolic components or not. We call βv,Rthemodel hyperbolicity at
the nodev. The notion of model hyperbolicity depends on node features as well as the explicit GNN model.
Similar to geometric hyperbolicity, the collection ΓG={βv,R:v∈V}gives rise to an empirical distribution
νGon[0,1]. We callνGthedistribution of model hyperbolicity .
To motivate the next subsection, from (15), we notice that the output depends smoothly on βv,R. If we
wish to have a similar output for nodes with similar neighborhood structures and features, we want their
selection weights to have similar values. On the other hand, we have seen (cf. Proposition 1) that geometric
hyperbolicities, which can be computed given G, are similar for nodes with similar neighborhoods. It suggests
that we may use geometric hyperbolicities to “guide” the choice of model hyperbolicities.
3.3 Model hyperbolicity vs.geometric hyperbolicity
We have introduced geometric and model hyperbolicities in the previous subsections. In this subsection, we
explore the interconnections between these two notions.
LetΘbe the parameters of a proposed GNN model. We assume that the model has the pipeline shown in
Fig. 4. Given node features {hv,v∈V}and model parameters Θ, the model generates (embedding) features
{zv,v∈V}and selection weights or model hyperbolicity {βv,R,v∈V}in the intermediate stage. For each
v∈V, there is a combination function ϕvsuch that the final output {ˆyv,v∈V}satisfies ˆyv=ϕv(zv,βv).
Figure 4: The model pipeline is shown in the (blue) dashed box, while the geometric hyperbolicity can be
computed independently of the model.
In principle, we want to compare {βv,R,v∈V}and{δv,v∈V}so that the geometric hyperbolicity guides the
choice of model hyperbolicity. However, comparing pairwise βvandδvfor eachv∈Vmay lead to overfitting.
An alternative is to compare their respective distributions νGandµG, or even coarser statistics (e.g., mean)
ofνGandµG(cf. Fig. 5). The latter may lead to underfitting. We perform an ablation study on the different
comparison methods in Section 4.5.
We advocate choosing the middle ground by comparing the distributions µGandνG. The former can be
computed readily as long as the ambient graph Gis given, while the latter is a part of the model that plays a
crucial role in feature aggregation at each node. Therefore, µGcan be pre-determined but not νG. We propose
to use the known µGto constrain νGand thus the model parameters Θ. A widely used comparison tool is the
Wasserstein metric.
7Published in Transactions on Machine Learning Research (12/2024)
Figure 5: Different ways of comparing geometric and model hyperbolicities.
Definition 3 (Wasserstein distance) .Givenp≥1, thep-Wasserstein distance metric Villani (2008) measures
the difference between two different probability distributions Gao et al. (2021). Let Π(νG,µG)be the set of all
joint distributions for random variables xandywherex∼νGandy∼µG. Then thep-Wasserstein distance
betweenµGandνGis as follows:
Wp(νG,µG) =/braceleftbigg
inf
γ∈Π(νG,µG)E(x,y)∼γ∥x−y∥p/bracerightbigg1/p
. (16)
To compute the Wasserstein distance exactly is costly given that the solution of an optimal transport
problem is required (Rowland et al., 2019; Chen et al., 2022b). However, for one-dimensional distributions,
thep-Wasserstein distance can be computed by ordering the samples from the two distributions and then
computing the average p-distance between the ordered samples (Kolouri et al., 2019; Rowland et al., 2019).
In ideal circumstances, considering the distributions do not lose much information. We first notice that for
bothβv,Randδv, a smaller value means more hyperbolic in an appropriate sense. Suppose βv,Ris increasing
w.r.t.δv, i.e.,δv≤δuimplies that βv,R≤βu,R. Then,W2(µG,νG) =/radicalig
1
|V|/summationtext
v∈V|βv,R−δv|2.
3.4 Non-uniformity of selection weights
A node is considered to be more suitable to be embedded in the hyperbolic space when βv,D>βv,R. Meanwhile
whenβv,D≤βv,R, the node is considered to be Euclidean. Nevertheless, to align with our motivation that each
node can be better embedded in one of the two spaces and the less suitable space would result in distortion
in representation, we require JSGNN to learn non-uniform attention weights, meaning that each pair of
attention weights (βv,D,βv,R)should significantly deviate from the uniform distribution. This is because soft
selection without a non-uniformity constraint may result in the assignment of nodes to be partially Euclidean
and partially hyperbolic with βv,R≈βv,D≈0.5. Hence, we include an additional component to the standard
loss function encouraging non-uniform learned weights as follows:
Lnu=−1
|V|/summationdisplay
v∈V/parenleftbig
β2
v,R+β2
v,D/parenrightbig
. (17)
Since−1≤−(β2
v,R+β2
v,D)≤−0.5andβv,R+βv,D= 1, minimizing the term would favor non-uniform
attention weights for each node.
In summary, we may combine hyperbolicity matching discussed in Section 3.3 and the non-uniformity loss to
form the loss function to optimize JSGNN.
Loverall =Ltask+ωnuLnu+ωwasW2(νG,µG), (18)
whereLtaskis the task-specific loss, while ωnuandωwasare balancing factors. For the node classification
task,Ltaskrefers to the cross-entropy loss over all labeled nodes while for link prediction, it refers to the
cross-entropy loss with negative sampling. This completes the description of the JSGNN model.
We speculate that the non-uniform component Lnushould push the model hyperbolicities towards the two
extremes 0and1. On the other hand, as we have seen in Section 3.3, to compute W2(νG,µG), we need to
order (δv)v∈V,(βv,R)v∈Vrespectively, and compute their pairwise differences. Therefore, W2(νG,µG)aligns
the shapes of νGandµG.
8Published in Transactions on Machine Learning Research (12/2024)
Table 1: Node classification result on Cora, Citeseer and Pubmed datasets. Performance score averaged over
ten runs. The best performance is boldfaced while the second-best performance is underlined.
MethodStandard split 60/20/20% split
Cora Citeseer Pubmed Cora Citeseer Pubmed
GCN 81.53 ±0.84 70.47±0.64 78.30±0.63 91.99±0.79 84.13±0.98 88.79±1.63
GAT 81.68 ±1.06 70.96±0.96 78.05±0.50 91.63±0.57 83.93±0.85 89.99±1.31
GraphSAGE 76.59 ±1.06 65.26±2.91 77.90±0.71 91.25±0.22 84.08±0.25 89.62±0.18
CurvGN 81.58 ±0.51 71.14±0.67 78.17±0.48 91.60±0.25 84.18±0.37 86.65±0.14
CGNN 82.15 ±0.6071.31±1.1678.34±0.83 91.96±0.27 84.29±0.34 86.86±0.16
HGNN 79.28 ±0.77 70.00±0.74 77.45±1.40 89.73±0.84 80.27±0.21 88.27±0.51
HGCN 78.68 ±0.77 67.25±1.45 76.72±0.92 91.57±0.28 83.68±0.52 86.83±0.31
HGAT 78.81 ±1.49 68.16±1.34 77.43±1.20 90.27±0.81 81.29±0.79 86.27±0.47
LGCN 78.93 ±0.79 68.59±0.64 78.08±0.65 92.55±0.57 85.03±0.2889.59±0.11
κ-GCN ( D16) 78.64 ±0.83 67.17±0.73 78.01±0.67 89.16±0.59 84.57±0.20 88.37±0.37
DeepHGCN 80.66 ±0.8972.11±0.6078.13±1.67 88.51±1.52 80.45±0.36 86.90±0.42
κ-GCN ( D16×R16) 78.71±1.02 66.96±1.13 77.67±0.74 88.85±0.88 84.04±0.81 85.59±0.53
GIL 79.97 ±1.93 67.54±1.23 76.62±0.81 91.90±0.84 82.39±0.90 87.39±0.21
JSGNN (GCN+HGCN) 81.79 ±0.80 70.55±1.09 78.38±0.7493.26±0.9284.95±0.31 89.68±0.60
JSGNN (GAT+HGAT) 82.94±0.5571.26±1.1378.57±0.9093.10±0.8685.10±0.64 90.53±0.32
4 Experiments
In this section, we evaluate JSGNN on node classification (NC) and link prediction (LP) tasks. Dataset
statistics, model settings, and model size and complexity are discussed in Appendix A and Appendix B.
4.1 Datasets
A total of seven benchmark datasets are employed for both NC and LP. Specifically, three citation datasets:
Cora, Citeseer, Pubmed; aflightnetwork: Airport; adiseasepropagationtree: Disease; anAmazonco-purchase
graph dataset: Photo; and a coauthor dataset: CS.
4.2 Baselines
For JSGNN, we consider two variants: GAT as the Euclidean model with HGAT as the hyperbolic model,
and GCN as the Euclidean model with HGCN as the hyperbolic model. We compare against the following
models: (a) Euclidean methods: GCN (Kipf & Welling, 2016), GraphSAGE (Hamilton et al., 2017) and
GAT (Veličković et al., 2018); (b) hyperbolic models: HGCN (Chami et al., 2019), HGNN (Liu et al.,
2019), HGAT (Zhang et al., 2021a), LGCN (Zhang et al., 2021b), DeepHGCN (Liu et al., 2024) and κ-GCN
(D16) (Bachmann et al., 2019), which is a constant (negative) curvature graph neural network based on
theκ-stereographic model; (c) CurvGN (Ye et al., 2020) and CGNN (Li et al., 2021), which utilizes graph
curvature information to filter messages differently based upon different local structures; (d) mixed models:
GIL (Zhu et al., 2020) and κ-GCN ( D16×R16), which similar to JSGNN, leverages multiple spaces or a mixed
curvature space. κ-GCN ( D16×R16) employs a two-component product space of negative and zero curvature.
4.3 Node classification
For the node classification task, each of the nodes in a dataset belongs to one of the Cclasses in the dataset.
With the final set of node representations, we aim to predict the labels of nodes that are in the testing set.
To test the performance of each model under both semi-supervised and fully-supervised settings, two data
splits are used in the node classification task for the Cora, Citeseer and Pubmed datasets. In the first split, we
followed the standard split for semi-supervised settings used in Kipf & Welling (2016); Veličković et al. (2018);
Monti et al. (2017); Chamberlain et al. (2021b); Zhu et al. (2020); Chamberlain et al. (2021a); Hamilton et al.
(2017); Liu et al. (2022b); Feng et al. (2020). The train set consists of 20 train examples per class while the
9Published in Transactions on Machine Learning Research (12/2024)
Table 2: Node classification result on CS, Photo, Airport and Disease datasets. OOMcorresponds to out-of-
memory.
Method CS Photo Airport Disease
GCN 96.53 ±0.10 94.05±0.27 79.62±1.28 83.32±1.37
GAT 96.36 ±0.38 94.45±0.92 83.07±1.52 86.05±1.08
GraphSAGE 96.45 ±0.91 96.13±1.61 81.73±0.98 83.47±1.77
CurvGN 96.91 ±0.09 94.21±0.22 87.25±0.88 84.35±3.20
CGNN 96.27 ±0.08 93.93±0.18 87.21±1.08 85.75±2.08
HGNN 96.72 ±0.19 94.74±0.66 84.37±1.19 87.40±1.66
HGCN 96.58 ±0.10 95.27±0.25 89.39±1.52 87.93±1.61
HGAT 96.65 ±0.15 96.62±0.28 89.31±1.09 90.04±1.50
LGCN OOM 96.71±0.2488.53±1.26 91.15±1.02
κ-GCN ( D16) 97.04 ±0.10 95.56±0.54 89.08±1.1592.39±0.73
DeepHGCN 95.80 ±0.88 94.37±1.43 90.24±1.88 90.38±1.76
κ-GCN ( D16×R16) 96.97±0.10 94.31±0.41 88.38±0.62 89.47±1.56
GIL 95.83 ±0.30 94.41±0.5790.78±1.7490.67±1.98
JSGNN (GCN+HGCN) 97.08 ±0.0495.69±0.22 90.59±1.7590.73±1.44
JSGNN (GAT+HGAT) 97.40±0.14 97.16±0.4490.33±1.61 90.88±1.54
validation set and test set consist of 500 samples and 1,000 samples, respectively.1Meanwhile, in the second
split, all labels are utilized and the percentages of training, validation, and test sets are set as 60/20/20%. For
the Photo and CS datasets, the labeled nodes are also split into three sets where 60% of the nodes made up
the training set, and the rest of the nodes were divided equally to form the validation and test sets. Airport
and Disease datasets were split in similar settings as Zhu et al. (2020).
In Table 1 and Table 2, the mean accuracy with standard deviation is reported for node classification, except
for the case of Airport and Disease datasets where the mean F1 score is reported. Our empirical results
demonstrate that JSGNN frequently outperforms the baselines, especially HGAT and GAT which are the
building blocks of JSGNN. Even though the performance of the variant JSGNN (GCN+HGCN) is often
slightly lower than JSGNN (GAT+HGAT), we have similarly observed it to consistently outperform its
building blocks GCN and HGCN. This is not necessarily observed for other mixed space models. Thus,
this shows the superiority of not only using both Euclidean and hyperbolic spaces but also our method of
incorporating the two spaces for graph learning as compared to GIL and κ-GCN ( D16×R16).
We also observe that Euclidean models such as GCN, GAT, GraphSAGE, CurvGN and CGNN perform better
than hyperbolic models in general on the Cora, Citeseer, and Pubmed datasets for both splits. Meanwhile,
hyperbolic models achieve better results on the CS, Photo, Airport, and Disease datasets. This means that
Euclidean features are more significant for representing Cora, Citeseer and Pubmed datasets while hyperbolic
features are more significant for the others. Nevertheless, JSGNN is able to perform relatively well across all
datasets. We note that JSGNN exceeds the performance of single-space baselines on all datasets except for
Disease. This can be explained by the fact that Disease consists of a perfect tree and thus, does not exhibit
different hyperbolicities in the graph. However, JSGNN still outperforms 3hyperbolic benchmarks and all
the other mixed models.
We also particularly note that the difference in results between single-space models using only the Euclidean
embedding space and hyperbolic models is not significant. This means that many of the node labels can be
potentially predicted even without the best representation from the right space. This might be the reason
why the gain in performance for the node classification task is not exceptional from embedding nodes in the
better space. Nevertheless, we still see improvements in predictions for cases where there is a mixture of local
hyperbolicities. Moreover, embedding nodes in a more suitable space can benefit other tasks that require
more accurate representations such as link prediction.
1Note that the top results on https://paperswithcode.com/sota/node-classification-on-cora used different data splits
(either semi-supervised settings with a larger number of training samples or fully-supervised settings such as the 60/20/20%
split) which give much higher accuracies
10Published in Transactions on Machine Learning Research (12/2024)
Table 3: Link prediction result averaged over ten runs.
Method Cora Citeseer Pubmed Airport Disease
GCN 88.22 ±1.01 90.60±1.10 87.63±3.25 91.79±1.48 61.60±3.76
GAT 85.47 ±2.28 85.31±1.89 85.30±1.46 93.70±0.65 61.23±2.75
GraphSAGE 88.94 ±0.81 91.61±1.00 88.42±1.14 91.63±0.81 68.31±2.94
CurvGN 94.40 ±2.13 95.38±2.33 94.55±0.89 93.90±0.37 95.47±0.81
CGNN 94.26 ±1.36 96.54±0.78 94.71±3.14 95.13±0.97 95.35±1.22
HGNN 91.48 ±0.38 93.63±0.14 92.95±0.35 96.31±0.30 82.98±0.98
HGCN 93.72 ±0.26 96.72±1.69 96.68±0.04 97.55±0.08 87.14±1.34
HGAT 94.06 ±0.11 95.60±0.20 95.78±0.05 97.86±0.08 86.61±1.67
LGCN 93.10 ±0.30 93.40±0.70 95.45±0.08 97.88±0.19 95.99±0.58
κ-GCN (D16) 92.43±0.63 94.38±0.51 94.89±0.07 96.78±0.19 93.58±0.31
κ-GCN ( D16×R16) 91.32±0.38 92.87±0.27 93.53±0.06 97.17±0.10 90.15±0.77
GIL 98.04 ±1.64 99.95±0.09 92.50±0.50 97.20±1.04100.00±0.00
JSGNN (GCN+HGCN) 98.83 ±0.9299.97±0.0997.67±0.0398.94±0.91100.00±0.00
JSGNN (GAT+HGAT) 99.43±0.21 99.98±0.0596.95±0.0399.26±1.23 99.97±0.08
Table 4: Ablation study of JSGNN (GAT+HGAT) for node classification task. Cora, Citeseer and Pubmed
on the standard split.
Method CS Photo Cora Citeseer Pubmed Airport Disease
JSGNN (GAT+HGAT) 97.40±0.14 97.16±0.44 82.94±0.55 71.26±1.13 78.57±0.90 90.33±1.61 90.88±1.54
w/o NU & W2 97.15±0.10 95.95±0.41 81.65±1.08 70.87±1.22 78.14±1.02 89.67±1.26 89.85±1.61
w/oW2 97.33±0.20 96.51±0.67 82.36±0.78 71.15±1.17 78.50±0.53 90.02±1.63 90.66±2.22
w/o NU 97.38 ±0.15 96.42±0.37 82.67±0.51 70.86±1.45 78.48±0.47 89.98±1.72 90.37±2.12
4.4 Link prediction
We employ the Fermi-Dirac decoder with a distance function to model the probability of an edge based on our
final output embedding, similar to Zhu et al. (2020); Sun et al. (2022); Chami et al. (2019). The probability
that an edge exists is given by P(evj∈E|Θ)= (e(d(xi,xj)−r)/t+ 1)−1wherer,t> 0are hyperparameters
anddis the distance function. The edges of the datasets are randomly split into 85/5/10% for training,
validation, and testing. The average ROC AUC for link prediction is recorded in Table 3. We observe that
JSGNN (GAT+HGAT) performs better than the baselines in most cases. For the link prediction task, we
notice that hyperbolic models consistently outperform Euclidean models by a significant margin. Moreover,
Euclidean methods such as CurvGN and CGNN benefit from using topological information during learning.
Empirical results also suggest that predicting the existence of edges seems to benefit from dual space models,
i.e., GIL and JSGNN, except for the case of κ-GCN ( D16×R16).
This finding is similar to that reported in Bachmann et al. (2019); Xiong et al. (2022) where despite slightly
different settings, the constant (negative) curvature κ-stereographic model frequently outperforms the κ-GCN
leveraging on the product of multiple constant curvature spaces . We hypothesize that the simple concatenation
to combine the embeddings of the two different component spaces in κ-GCN ( D16×R16) might be insufficient
and might have resulted in noise from the other space being passed to the negatively curved space which was
performing well standalone as seen in κ-GCN ( D16). As such, our aim to learn to select the better space for
each node is potentially capable of offering better representations with reduced distortions.
4.5 Ablation study
We conduct an ablation study on the node classification task by introducing three variants of JSGNN
(GAT+HGAT) to validate the effectiveness of the different components introduced:
•Without the non-uniformity constraint (w/o NU): This does not enforce the model to learn non-
uniform selection weights.
•Without the Wasserstein metric (w/o W2): The learning of model hyperbolicity is not guided by
geometric hyperbolicity.
11Published in Transactions on Machine Learning Research (12/2024)
•Without the non-uniformity loss and Wasserstein distance (w/o NU & W2): Only guided by the
cross entropy loss, i.e., ωnu= 0,ωwas= 0(cf. (18)).
Table 4 summarizes the results of our study, from which we observe that all variants of JSGNN (GAT+HGAT)
with some components discarded perform worse than the full model. Moreover, JSGNN (GAT+HGAT)
withoutW2always achieves better results than JSGNN (GAT+HGAT) without NU and W2, signifying
the importance of selecting the better of the two spaces instead of combining the features with relatively
uniform weights. Similarly, JSGNN (GAT+HGAT) without NU performs better than JSGNN (GAT+HGAT)
without NU and W2in most cases, suggesting that incorporating geometric hyperbolicity through distribution
alignment does help to improve the model. A comparison of the time taken for each variant is provided in
Appendix B.
To further analyze our model, we present a study regarding our method of incorporating the guidance of
geometric hyperbolicity through distribution alignment. The result is as seen in Table 5. We test and analyze
empirically different variants of our model based on the different comparisons shown in Fig. 5. Pairwise
match indicates minimizing the mean squared error between elements of ΓGand∆V(without sorting) while
mean match minimizes the squared loss between the means of ΓGand∆V. We observe that comparing the
distributions of νGandµGconsistently outperforms comparing their mean, demonstrating the insufficiency of
utilising coarse statistics for supervision. Secondly, pairwise matching gave better results than mean matching,
though still lower than distribution matching, suggesting the importance of fine-scale information yet, a need
to avoid potential overfitting.
Table 5: Node classification results of different comparison methods to incorporate geometric hyperbolicity to
guide model hyperbolicity.
Dataset Pairwise match Distribution Mean match
Cora 82.35 ±1.0682.94±0.5581.36±1.50
Citeseer 70.06 ±2.0571.26±1.1369.64±1.18
Pubmed 78.46 ±0.8678.57±0.9078.08±0.62
Aiport 90.13 ±1.5390.33±1.6189.31±2.22
Disease 90.66 ±1.9190.88±1.5487.53±6.24
Photo 96.17 ±0.2397.16±0.4495.96±0.59
CS 97.20 ±0.1697.40±0.1497.17±0.11
4.6 Analysis of hyperbolicities
We have speculated the effects of different components of our proposed model at the end of Section 3.4.
To verify that our model can learn model hyperbolicity that is non-uniform and similar in distribution as
geometric hyperbolicity, we analyze the learned model hyperbolicities (βv,R)v∈Vof JSGNN (GAT+HGAT)
and the model w/o NU & W2for the node classification task. Specifically, we extract the learned values from
the first two layers of JSGNN and its variant for ten separate runs. The learned values from the first two
layers were then averaged before determining W2(νG,Unif)andW2(νG,µG).
In Fig. 6, it can be inferred that JSGNN’s learned model hyperbolicity is always less uniform than that of the
model w/o NU & W2given JSGNN’s larger W2(νG,Unif)score, demonstrating a divergence from uniform
distribution. Meanwhile, for most cases, JSGNN’s W2(νG,µG)is smaller than that of the model w/o NU &
W2, suggesting that the shape between νGandµGof JSGNN is relatively more similar. At times, JSGNN’s
W2(νG,µG)is larger than the model w/o NU & W2, suggesting a tradeoff between NU and W2as we choose
the optimal combination for the model’s best performance.
5 Conclusion
In this paper, we have explored the learning of GNNs in a joint space setting given that different regions of
a graph can have different geometrical characteristics. In these situations, it would be beneficial to embed
different regions of the graph in different spaces that are better suited for their underlying structures, to
12Published in Transactions on Machine Learning Research (12/2024)
(a)
 (b)
Figure 6: Analysis of hyperbolicities on different datasets. (a) W2(νG,Unif). (b)W2(νG,µG).
reduce the distortions incurred while learning node representations. Our method JSGNN utilizes a soft
attention mechanism with non-uniformity constraint and distribution alignment between model and geometric
hyperbolicities to select the best space-specific feature for each node. This indirectly finds the space that is
best suited for each node. Experimental results of node classification and link prediction demonstrate the
effectiveness of JSGNN against various baselines. In future work, we aim to further improve our model with
an adaptive mechanism to determine the appropriate, node-level specific neighborhood to account for each
node’s hyperbolicity. Limitations are discussed in Appendix F.
6 Acknowledgements
The first author is supported by Shopee Singapore Private Limited under the Economic Development Board
Industrial Postgraduate Programme (EDB IPP). The programme is a collaboration between Shopee and
Nanyang Technological University, Singapore. This research is supported in part by the Singapore Ministry
of Education Academic Research Fund Tier 2 grant MOE-T2EP20220-0002. This work is carried out during
the first author’s participation in the EDB IPP.
References
Aaron B. Adcock, Blair D. Sullivan, and Michael W. Mahoney. Tree-like structure in large social and
information networks. In 2013 IEEE 13th International Conference on Data Mining , pp. 1–10, 2013. doi:
10.1109/ICDM.2013.77.
Gregor Bachmann, Gary Bécigneul, and Octavian-Eugen Ganea. Constant curvature graph convolutional
networks. In Proceedings of the 7th International Conference on Learning Representations , 2019.
M. Bridson and A. Haefliger. Metric Spaces of Non-Positive Curvature . Springer, 1999.
Benjamin Paul Chamberlain, James Rowbottom, Davide Eynard, Francesco Di Giovanni, Dong Xiaowen,
and Michael M Bronstein. Beltrami flow and neural diffusion on graphs. Proceedings of the Thirty-fifth
Conference on Neural Information Processing Systems (NeurIPS) , 2021a.
Benjamin Paul Chamberlain, James Rowbottom, Maria I. Gorinova, Stefan D Webb, Emanuele Rossi, and
Michael M. Bronstein. GRAND: Graph neural diffusion. In The Symbiosis of Deep Learning and Differential
Equations , 2021b.
Ines Chami, Zhitao Ying, Christopher Ré, and Jure Leskovec. Hyperbolic graph convolutional neural networks.
InAdvances in Neural Information Processing Systems , pp. 4869–4880, 2019.
Deli Chen, Yankai Lin, Wei Li, Peng Li, Jie Zhou, and Xu Sun. Measuring and relieving the over-smoothing
problem for graph neural networks from the topological view. Proceedings of the AAAI Conference on
Artificial Intelligence , 34(04):3438–3445, 2020a. doi: 10.1609/aaai.v34i04.5747.
13Published in Transactions on Machine Learning Research (12/2024)
Ming Chen, Zhewei Wei, Zengfeng Huang, Bolin Ding, and Yaliang Li. Simple and deep graph convolutional
networks. In Proceedings of the 37th International Conference on Machine Learning , pp. 1725–1735. PMLR,
2020b.
Yankai Chen, Menglin Yang, Yingxue Zhang, Mengchen Zhao, Ziqiao Meng, Jianye Hao, and Irwin King.
Modeling scale-free graphs with hyperbolic geometry for knowledge-aware recommendation. In Proceedings
of the 15th ACM International Conference on Web Search and Data Mining , WSDM ’22, pp. 94–102, 2022a.
ISBN 9781450391320.
Yidong Chen, Chen Li, and Zhonghua Lu. Computing wasserstein- pdistance between images with linear
cost. In2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , pp. 509–518,
2022b. doi: 10.1109/CVPR52688.2022.00060.
Hyunghoon Cho, Benjamin DeMeo, Jian Peng, and Bonnie Berger. Large-margin classification in hyperbolic
space. In Proceedings of the 22nd International Conference on Artificial Intelligence and Statistics , volume89
ofProceedings of Machine Learning Research , pp. 1832–1840. PMLR, 16–18 Apr 2019.
Wenzheng Feng, Jie Zhang, Yuxiao Dong, Yu Han, Huanbo Luan, Qian Xu, Qiang Yang, Evgeny Kharlamov,
and Jie Tang. Graph random neural networks for semi-supervised learning on graphs. In Proceedings of the
34th International Conference on Neural Information Processing Systems . Curran Associates Inc., 2020.
ISBN 9781713829546.
Ji Gao, Xiao Huang, and Jundong Li. Unsupervised graph alignment with wasserstein distance discriminator.
InProceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining , KDD ’21,
pp. 426–435, New York, NY, USA, 2021. Association for Computing Machinery. ISBN 9781450383325. doi:
10.1145/3447548.3467332.
Justin Gilmer, Samuel S. Schoenholz, Patrick F. Riley, Oriol Vinyals, and George E. Dahl. Neural message
passing for quantum chemistry. In Proceedings of the 34th International Conference on Machine Learning ,
ICML’17, pp. 1263–1272. JMLR.org, 2017.
W. L. Hamilton, R. Ying, and J. Leskovec. Inductive representation learning on large graphs. In Advances in
Neural Information Processing Systems , 2017.
Thomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional networks, 2016.
Soheil Kolouri, Kimia Nadjahi, Umut Simsekli, Roland Badeau, and Gustavo Rohde. Generalized sliced
wasserstein distances. In Advances in Neural Information Processing Systems , volume 32. Curran Associates,
Inc., 2019.
John M. Lee. Introduction to Riemannian Manifolds . Springer Cham, 2018.
Haifeng Li, Jun Cao, Jiawei Zhu, Yu Liu, Qing Zhu, and Guohua Wu. Curvature graph neural network.
Information Sciences , 2021. doi: 10.1016/j.ins.2021.12.077.
Jiahong Liu, Menglin Yang, Min Zhou, Shanshan Feng, and Philippe Fournier-Viger. Enhancing hyperbolic
graph embeddings via contrastive learning, 2022a. URL https://arxiv.org/abs/2201.08554 .
Jiaxu Liu, Xinping Yi, and Xiaowei Huang. Deephgcn: Toward deeper hyperbolic graph convolutional
networks. IEEE Transactions on Artificial Intelligence , pp. 1–14, 2024. doi: 10.1109/TAI.2024.3440223.
Qi Liu, Maximilian Nickel, and Douwe Kiela. Hyperbolic graph neural networks. In Proceedings of the 33rd
International Conference on Neural Information Processing Systems , pp. 8230–8241, 2019.
Songtao Liu, Rex Ying, Hanze Dong, Lanqing Li, Tingyang Xu, Yu Rong, Peilin Zhao, Junzhou Huang, and
Dinghao Wu. Local augmentation for graph neural networks. In International Conference on Machine
Learning , 2022b.
14Published in Transactions on Machine Learning Research (12/2024)
F. Monti, D. Boscaini, J. Masci, E. Rodola, J. Svoboda, and M. M. Bronstein. Geometric deep learning
on graphs and manifolds using mixture model cnns. In 2017 IEEE Conference on Computer Vision and
Pattern Recognition (CVPR) , pp. 5425–5434, Los Alamitos, CA, USA, jul 2017. IEEE Computer Society.
doi: 10.1109/CVPR.2017.576.
Yu Rong, Wenbing Huang, Tingyang Xu, and Junzhou Huang. Dropedge: Towards deep graph convolutional
networks on node classification. In International Conference on Learning Representations , 2020. URL
https://openreview.net/forum?id=Hkx1qkrKPr .
MarkRowland, JiriHron, YunhaoTang, KrzysztofChoromanski, TamasSarlos, andAdrianWeller. Orthogonal
estimation of wasserstein distances. In Proceedings of the 22nd International Conference on Artificial
Intelligence and Statistics , volume 89 of Proceedings of Machine Learning Research , pp. 186–195. PMLR,
16–18 Apr 2019.
Ramit Sawhney, Shivam Agarwal, Arnav Wadhwa, and Rajiv Shah. Exploring the scale-free nature of stock
markets: Hyperbolic graph learning for algorithmic trading. In Proceedings of the Web Conference , WWW
’21, pp. 11–22, New York, NY, USA, 2021. Association for Computing Machinery. ISBN 9781450383127.
doi: 10.1145/3442381.3450095.
Li Sun, Zhongbao Zhang, Junda Ye, Hao Peng, Jiawei Zhang, Sen Su, and Philip S. Yu. A self-supervised
mixed-curvature graph neural network. In Association for the Advancement of Artificial Intelligence
(AAAI), 2022.
Jake Topping, Francesco Di Giovanni, Benjamin Paul Chamberlain, Xiaowen Dong, and Michael M. Bronstein.
Understanding over-squashing and bottlenecks on graphs via curvature. In International Conference on
Learning Representations , 2022. URL https://openreview.net/forum?id=7UmjRGzp-A .
Petar Veličković, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Liò, and Yoshua Bengio.
Graph Attention Networks. International Conference on Learning Representations , 2018.
Cédric Villani. Optimal Transport: Old and New . Springer Science & Business Media, 2008.
Bo Xiong, Shichao Zhu, Nico Potyka, Shirui Pan, Chuan Zhou, and Steffen Staab. Pseudo-riemannian graph
convolutional networks. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (eds.),
Advances in Neural Information Processing Systems , 2022. URL https://openreview.net/forum?id=
KeIuNChob1H .
Menglin Yang, Min Zhou, Zhihao Li, Jiahong Liu, Lujia Pan, Hui Xiong, and Irwin King. Hyperbolic graph
neural networks: A review of methods and applications, 2022.
Ze Ye, Kin Sum Liu, Tengfei Ma, Jie Gao, and Chao Chen. Curvature graph network. In Proceedings of the
8th International Conference on Learning Representations (ICLR 2020) , April 2020.
Rex Ying, Ruining He, Kaifeng Chen, Pong Eksombatchai, William L. Hamilton, and Jure Leskovec. Graph
convolutional neural networks for web-scale recommender systems. In Proceedings of the 24th ACM
SIGKDD International Conference on Knowledge Discovery and Data Mining , KDD ’18, pp. 974–983, 2018.
ISBN 9781450355520.
Hanqing Zeng, Muhan Zhang, Yinglong Xia, Ajitesh Srivastava, Andrey Malevich, Rajgopal Kannan, Viktor
Prasanna, Long Jin, and Ren Chen. Decoupling the depth and scope of graph neural networks. In Advances
in Neural Information Processing Systems , 2021.
Yiding Zhang, Xiao Wang, Chuan Shi, Xunqiang Jiang, and Yanfang Fanny Ye. Hyperbolic graph attention
network. IEEE Transactions on Big Data , 2021a.
Yiding Zhang, Xiao Wang, Chuan Shi, Nian Liu, and Guojie Song. Lorentzian graph convolutional networks.
InProceedings of the Web Conference 2021 , WWW ’21, pp. 1249–1261, New York, NY, USA, 2021b.
Association for Computing Machinery. ISBN 9781450383127. doi: 10.1145/3442381.3449872.
Shichao Zhu, Shirui Pan, Chuan Zhou, Jia Wu, Yanan Cao, and Bin Wang. Graph geometry interaction
learning. In Advances in Neural Information Processing Systems , 2020.
15Published in Transactions on Machine Learning Research (12/2024)
A Dataset statistics and model settings
Dataset statistics are provided in Table 6.
Table 6: Dataset statistics.
Dataset Nodes Edges Classes Features
Cora 2708 5429 7 1433
Citeseer 3327 4732 6 3703
Pubmed 19717 44338 3 500
Aiport 3188 18631 4 4
Disease 1044 1043 2 1000
Photo 7650 119081 8 745
CS 18333 81894 15 6805
For all models, the hidden units are set to 16. We set the early stopping patience to 100 epochs with a
maximum limit of 2000 epochs. The hyperparameter settings for the baselines are the same as Zhu et al.
(2020) if given. The only difference is that the hyperparameter h-dropfor GIL in Zhu et al. (2020) (which
determines the dropout to the weight associated with the hyperbolic space embedding) is set to 0 for all
datasets as setting a large value essentially explicitly chooses one single space. Else, the hyperparameters are
chosen to yield the best performance. For JSGNN, we perform a grid search on the following search spaces:
Learning rate: [0.01, 0.005]; Dropout probability: [0.0, 0.1, 0.5, 0.6]; Number of layers: [1, 2, 3]; ωnuand
ωwas: [1.0, 0.5, 0.2, 0.1, 0.01, 0.005]; q(cf. (11)): [16, 32, 64]. The Wasserstein-2 distance is employed in all
variants of JSGNN.
The source code can be found at https://github.com/amblee0306/JSGNN_Mixed_Space_GNN . The imple-
mentation is based on the setup in https://github.com/CheriseZhu/GIL .
B Complexity, run-time and model size
We first incur an overhead to compute the geometric hyperbolicity δvfor each node. We follow the approach
as described below2rather than using the naive implementation with a time complexity of O(V4). Instead of
considering all possible 4-tuples in the graph, we heuristically sample K4-tuples from each nodes’ two-hop
subgraph. Consequently, the complexity is O(N)×[O(N2) +O(K×Esubgraph )]≈O(NKd2)whereddenotes
the average degree in the graph, Kdenotes the number of samples and Esubgraphrepresents the number of
edges in the two-hop subgraph of each node. Assuming the graph is sparse, dis small. This runtime can be
further optimized by parallelization, as each node’s calculation is independent. Also, notice that this step
is performed only once during pre-processing, and the step is model-agnostic. For a dataset, the computed
{δv}v∈Vcan be stored and re-used for different trainings and also for different base models. Moreover, if the
dataset is updated, we only need to adjust δvfor nodesvwhose neighborhood structures are changed during
the update, which can be done efficiently.
For training of the model, we usually use two base models Me(e.g., GCN) andMh(e.g., HGCN) for handling
Euclidean and hyperbolic structures respectively. The complexity of the message passing in our model is
O(Ce+Ch), whereCeandChare the message passing complexities of MeandMhrespectively.
Regarding the implementation of the extra loss terms, calculating the Wasserstein distance for one-dimensional
distributions requires a time complexity of O(NlogN)whereNis the number of nodes in the graph, primarily
dominated by the time needed to sort the distributions. Meanwhile, obtaining the non-uniformity loss has a
time complexity of O(N).
In Table 7, we present the time taken for 300 epochs across different variants of JSGNN that we examine in
our ablation study in Section 4.5. The results show that, despite the added time complexity introduced by
2https://github.com/HazyResearch/hgcn/blob/master/utils/hyperbolicity.py
16Published in Transactions on Machine Learning Research (12/2024)
Table 7: Time taken (in seconds) for 300 epochs for node classification task across different model variants.
Method Cora Citeseer Pubmed CS Photo
JSGNN (GAT+HGAT) 18.53 ±1.05 21.60±0.62 24.73±3.49 49.35±0.62 37.03±0.51
w/oW2 17.29±0.66 20.06±0.27 23.56±1.85 47.31±0.34 34.45±0.41
w/o NU 18.06 ±0.13 19.98±0.32 23.42±3.49 48.36±0.41 36.71±0.32
w/o NU & W2 17.68±0.25 19.67±0.34 23.09±1.97 46.97±0.60 34.16±0.23
Table 8: Time taken (in seconds) training each model. The ratiois between the run-time of JSGNN and that
of HGCN plus GCN.
Method Cora Citeseer Pubmed
GCN 1.08 1.07 1.10
HGCN 3.87 3.96 4.44
JSGNN 18.53 21.60 24.73
Ratio 3.74 4.29 4.45
additional loss terms, JSGNN incurs only a small increase in time compared to its simpler variants across the
evaluated datasets.
Additionally, in Table 8, we show the empirical run-time comparison. We see that the run-time ratio between
JSGNN and HGCN plus GCN remains stable for graphs of different sizes. Therefore, the scalability of the
well-studied models GCN, and HGCN implies that of JSGNN.
We show in Table 9 the model size comparison. We see that our model size is comparable to all the other
hybrid models and approximately double those of the single-type models.
C Distributions of hyperbolicity
We include Table 10 that tabulates the percentage of nodes whose geometric hyperbolicity δvis (approximately)
the prescribed value. Here, the hyperbolicity is taken in a neighborhood of each node to account for the local
nature of message passing. We see, for example, for the Cora dataset, there are more than 52.4%of nodes
whoseδvis at least 1. Hence, an Euclidean model is preferred. On the other hand, there are ≈23.0%of nodes
whose neighborhood is tree-like, and this is why a hybrid model can further improve the performance.
D Proof of Proposition 1
Proof.We first consider δG,∞. For two graphs G1= (V1,E1)andG2= (V2,E2), letf1:G1→M,f2:G2→
Mbe isometeric embeddings into a metric space (M,d)such thatdGH(G1,G2) =dH(f1(G1),f2(G2)). Denote
dGH(G1,G2)byη. Forx,y,z,tinG1, there are x′,y′,z′,t′∈G2such thatd(f1(x),f2(x′)),d(f1(y),f2(y′)),
Table 9: Number of trainable parameters.
Method Cora Citeseer Pubmed CS Photo
JSGNN (GAT+HGAT) 47128 119696 17720 219967 25928
κ-GCN 46112 118720 16128 218272 24128
GIL 46700 119286 19155 219036 25380
GAT 23452 59752 8843 109244 12144
GCN 23335 59638 8339 109151 12072
HGNN 23335 59638 8339 109151 12072
HGCN 23336 59638 8339 109151 12072
HGAT 23431 59734 8435 109199 12120
17Published in Transactions on Machine Learning Research (12/2024)
Table 10: Hyperbolicity distribution in each dataset computed based on each node’s two-hop subgraph. The
lower the hyperbolicity, the more hyperbolic the node’s neighborhood is.
Dataset 0 0.5 1 1.5 2 2.5
Disease 1.0000 0.0000 0.0000 0.0000 0.0000 0.0000
Cora 0.2296 0.2230 0.5240 0.0225 0.0007 0.0000
Citeseer 0.5648 0.1350 0.2958 0.0045 0.0000 0.0000
Pubmed 0.3064 0.2225 0.3880 0.0814 0.0017 0.0000
Airport 0.0471 0.2566 0.6963 0.0000 0.0000 0.0000
Photo 0.0251 0.0482 0.8825 0.0442 0.0000 0.0000
CS 0.0657 0.1284 0.5718 0.2332 0.0009 0.0000
d(f1(z),f2(z′)),d(f1(t),f2(t′))are all bounded by η. We now estimate:
dG1(x,y) +dG1(z,t) =d(f1(x),f1(y)) +d(f1(z),f1(t))
≤d(f2(x′),f2(y′)) +d(f2(z′),f2(t′)) + 4η
=dG2(x′,y′) +dG1(z′,t′) + 4η
≤max{dG2(x′,z′) +dG2(y′,t′),dG2(z′,y′) +dG2(x′,t′)}
+ 2δG2,∞+ 4η
≤max{d(f1(x),f1(z)) +d(f1(y),f1(t)),
d(f1(z),f1(y)) +d(f1(x),f1(t))}
+ 2δG2,∞+ 8η
= max{dG1(x,z) +dG1(y,t),dG1(z,y) +dG1(x,t)}
+ 2δG2,∞+ 8η.(19)
Therefore, δG1,∞≤δG2,∞+ 4η. By the same argument swapping the role of G1andG2, we haveδG2,∞≤
δG1,∞+ 4η. Therefore|δG1,∞−δG2,∞|≤4ηandδG,∞is Lipschitz continuous w.r.t. G.
The proof of the continuity of δG,1is more involved. Consider G1andG2inGϵ. Letf1,f2,(M,d),ηbe as
earlier and assume η≪ϵ, for example, η=αϵforαis smaller than all the numerical constants in the rest of
the proof.
We adopt the following convention: for any non-vertex point of a graph, its degree is 2. By subdividing the
edges ofG1andG2if necessary, we may assume that the length of each edge einE1orE2satisfiesϵ/2≤e<ϵ.
As a consequence, for (u,v)inE1(resp.E2),dG1(u,v)(resp.dG2(u,v)) is the same as the length of (u,v).
We define a map ϕ:G1→G2as follows. For v∈G1, there is a v′inG2such thatdGH(f1(v),f2(v′))≤η.
Then we set ϕ(v) =v′. The mapϕis injective on the vertex set V1. Indeed, for u̸=v∈V1,dG1(u,,v)≥ϵ/2
and hencedG2(ϕ(u),ϕ(v))≥ϵ/2−2η>0. The strategy is to modify ϕby a small perturbation such that the
resulting function ψ:G1→G2is a homeomorphism that is almost an isometry.
Forv∈V1, letNvbe the 5ηneighborhood of v. It is a star graph and its number of branches is the same
as the degree of v, sayk. Letv1,...,vkbe the endpoints of Nv. The convex hull (of shortest paths) Cvof
{ϕ(v1),...,ϕ (vk)}inG2is also a star graph. This is because Cvis contained in the 7ηneighborhood of ϕ(v)
and it contains at most 1vertex inV2.
We claim that Cvhas the same number of branches as Nv. First of all, Cvcannot have fewer branches. For
otherwise, there is a ϕ(vi)in the path connecting ϕ(v)andϕ(vj)for somej̸=i. Hence,
dG2(ϕ(vi),ϕ(vj))≤dG2(ϕ(vi),ϕ(v))≤7η
<10η−2η=dG1(vi,vj)−2η.
This is a contradiction with the property of ϕ. It cannot have more branches than kas it is the convex hull of
at mostkpoints.
18Published in Transactions on Machine Learning Research (12/2024)
We next consider different cases for k. Fork̸= 2, asCvis a star graph, it has a unique node v′with degree k
(inCv), anddG1(v′,ϕ(vj))>0,1≤j≤j. We claim that v′has degree exact kinG2. Suppose on the contrary,
its degree in G2is larger than k. Then there is a branch not contained in Cv. Letw′be a node on the new
branch such that 6η≤dG2(w′,ϕ(v))≤7η. Moreover, there is a node winNvsuch that 4η≤dG1(w,v)≤9η
andw′=ϕ(w). Moreover, wis on the branch containing vjfor somej, and hence dG1(w,vj)≤4η. Therefore,
dG1(w,vj)≤6η−2η
<dG2(v′,ϕ(vj)) +dG2(w′,v′)−2η
=dG2(ϕ(w),ϕ(vj))−2η,
which is a contradiction. In this case, we define ψ(v) =v′∈G2. Ifk= 2whenNvis a path, by a similar
argument, we have that Cvis a path. We set ψ(v) =ϕ(v). An illustration is given in Fig. 7.
v
Nv
(v)
 (v)
Cv
Figure 7: Illustration of ψ.
For eachv∈V1, we now enlarge the neighborhood and consider its ϵ/6-neighborhood N′
v. It does not contain
another vertex and hence is also a star graph. Moreover, if v̸=u∈V1, thenN′
v∩N′
u=∅for otherwise
dG1(u,v)≤ϵ/3, which is impossible. We may similarly consider the ϵ/6-neighborhoods C′
u,C′
vofψ(u)and
ψ(v). BothC′
uandC′
vdo not contain any vertex in V2with degree̸= 2.
AsN′
vandC′
vare star graphs with the same number of branches, there is an isometry (also denoted by)
ψ:N′
v→C′
vsuch thatdG2(ψ(w),ϕ(w))≤2η. By disjointedness of ϵ/6neighborhoods, we may combine all
the maps above together to obtain ψ:∪v∈V1N′
v→∪v∈V1C′
v.
For the rest of G1, consider any edge (u,v)∈E1. Without loss of generality, let u1andv1be the leaves
ofN′
uandN′
vcontained in (u,v). We claim that the shortest open path connecting ψ(u1)andψ(v1)is
disjoint from∪v∈V1C′
v. For otherwise, dG1(u1,v1)≥2ϵ/3, whiledG2(ϕ(u1),ϕ(u2))≤dG2(ψ(u1),ψ(u2))−4η≥
ϵ/2 + 2ϵ/6−4η. Therefore, 2ϵ/3−2η≥5ϵ/6−4η, which is impossible as η≪ϵ.
LetPu,vandQu,vbe the shortest paths connecting u1,v1andψ(u1),ψ(v1)respectively (illustrated in Fig. 8).
Then the length of Pu,vandQu,vdiffer at most by 4η. We may further extend ψ:Pu,v→Qu,vby a
linear scaling such that dG2(ψ(w),ϕ(w))≤3ηforw∈Pu,v. For different edges (u,v),(u′,v′), it is apparent
Qu,v∩Qu′,v′are disjoint, as the minimal distance between points on Pu,vandPu′,v′is at leastϵ/3. Therefore,
we obtain a continuous injection ψ:G1→G2, which maps homeomorphically onto its image.
We claim that ψis onto. If not, there is a vertex v′∈V2that is not in ψ(V1)but it has a neighboring vertex
u′=ψ(u). However, this implies that the degree of u′is strictly larger than that of u, which is impossible as
we have shown.
In summary, ψ:G1→G2is a homeomorphism such that |dG1(u,v)−dG2(u,v)|≤6ηfor anyu,v∈G1.
Moreover,ψis piecewise linear whose gradient ψ′is1in the interior of N′
v,v∈V1and satisfies
ϵ
6−6η
ϵ
6≤ψ′(w)≤ϵ
6+ 6η
ϵ
6, (20)
forwcontained in the interior of some Pu,v,(u,v)∈E1.
19Published in Transactions on Machine Learning Research (12/2024)
u
v
 (u)
 (v)
Pu;v
Qu;v
N0
u
N0
v
C0
u
C0
v
Figure 8: Illustration of Pu,vandQu,v.
We are ready to estimate |δG1,1−δG2,1|. Let|Gi|be the total edge weights of Gi,i= 1,2. For convenience,
we denote a typical tuple (u,v,w,t )∈G4
1as a vector v, and (ψ(u),ψ(v),ψ(w),ψ(t))byψ(v). The mapψ:
G4
1→G4
2,v∝⇕⊣√∫⊔≀→ψ(v)inherits the properties of its counterpart ψ, which is a piecewise linear homeomorphism.
In particular, its Jacobian J(v)is defined almost everywhere. Using Definition 2, we have:
|δG1,1−δG2,1|
=/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/integraldisplay
v∈G4
1|G1|−4τG1(v) dv
−/integraldisplay
v∈G4
1|G2|−4J(v)τG2(ψ(v)) dv|
≤sup
v∈G4
1|τG1(v)−|G1|4
|G2|4J(v)τG2/parenleftbig
ψ(v)/parenrightbig/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle.(21)
Similar to (19), we estimate
sup
v∈G4
1|τG1(v)−τG2/parenleftbig
ψ(v)/parenrightbig
|≤24η. (22)
Moreover, we have seen in the proof that ψcan only have distortion when restricted to Pu,vfor(u,v)∈E1.
As
2ϵ
3−6η
2ϵ
3≤|Pu,v|/|Qu,v|≤2ϵ
3+ 6η
2ϵ
3,
the same bounds holds for |G1|/|G2|. Both upper and lower bounds can be arbitrarily close to 1ifηis small
enough. Similarly, by (20), J(v)as a fourth power of ψ′can also be made arbitrarily close to 1. In conjunction
with (21) and (22), |δG1,1−δG2,1|can be arbitrarily small if ηis chosen to be small enough. This proves that
δG,1is continuous in G.
E Related works
In this appendix, models that utilize multiple spaces and advanced topological information such as curvature
are reviewed.
CurvGN (Ye et al., 2020) and Curvature Graph Neural Network (CGNN) (Li et al., 2021) learn to reweigh
messages propagated between nodes, in Euclidean space, using curvature information. Curvature measures
how easily information flows between two nodes. These works assume that the edges with low curvatures
indicate the class boundaries, thus low weights are assigned when the edges are of low curvature. In our work,
we use Gromov hyperbolicity instead of Ollivier’s Ricci curvature to choose the appropriate space and we do
not reweigh the edges.
To the best of our knowledge, the closest works to ours are Geometry Interaction Learning (GIL) (Zhu et al.,
2020) andκ-GCN (Bachmann et al., 2019). κ-GCN utilizes the (Cartesian) product space to model data
20Published in Transactions on Machine Learning Research (12/2024)
in different spaces and employs the κ-stereographic model in each of the spaces. The Cartesian product
enables a combinatorial construction of the mixed curvature space, thus the representations are first learned
independently in the respective spaces and then concatenated (Sun et al., 2022). In terms of implementation,
this is similar to our framework but instead of concatenation, we introduce a space selection mechanism
guided by hyperbolicity to fuse the representations.
On the other hand, GIL proposes a feature interaction scheme to leverage different spaces and a probability
assembling module to combine the classification probabilities for obtaining the final prediction. The feature
interaction scheme is where the node features in the respective two spaces are enhanced based on the
distance similarity of the two sets of spatial embeddings. The larger the distance between the different spatial
embeddings, the more significant the portion of features from the other space is summed to itself as seen in
Fig. 3. However, this may introduce noise to the respective spaces, which we explain further below.
Our approach differs from GIL (Zhu et al., 2020) in some key aspects. Firstly, we leverage the distribution of
geometric hyperbolicity to guide our model to learn to decide if each node better embedded in a Euclidean or
hyperbolic space instead of performing feature interaction learning. This is done by aligning the distribution of
the learned model hyperbolicity and geometric hyperbolicity using the Wasserstein distance. Our motivation
is that if a node can be best embedded in one of the two spaces, encoding it in another space other than
the optimal one would result in comparably larger distortions. Minimal information would be present in
the sub-optimal space to help “enhance” the representation in the better space. Hence, promoting feature
interaction could possibly introduce more noise to the corresponding spaces. The ideal situation is then to
learn normalized selection weights that are non-uniform for each node so that we select for each node a
single, comparably better space’s output embedding. To achieve this, we introduce an additional loss term
that promotes non-uniformity. Lastly, we do not require probability assembling since we only have one set of
output features at the end of the selection process.
F Limitations
The paper does not have a comprehensive numerical study on extremely large datasets. Therefore, we do not
have a definite answer to how JSGNN will perform on these graphs. However, the datasets being studied in
our paper are diverse enough in terms of geometric properties. We have theoretically analyzed the complexity,
which depends on the complexities of the hyperbolic and Euclidean base models. Therefore, its scalability
also depends on the based models. For example, GCN, GAT, HGCN, and HGAT all scale well with graph
size. Notice that message passing is a local operation, and hence model performance on graphs of medium
sizes should be illustrative enough.
In future works, it is worthwhile to test our model on extremely large graphs to verify whether the above
speculation holds.
G Broader impact
Since this research focuses on graph neural networks applied to publicly available datasets, there is no
immediate broader impact concerns. The datasets used are standard benchmarks in the field, and there is no
indication that the work introduces any significant ethical risks.
21