Published in Transactions on Machine Learning Research (08/2023)
Reinforcement Learning with Delayed, Composite, and Par-
tially Anonymous Reward
Washim Uddin Mondal wmondal@purdue.edu
School of IE and CE, Purdue University
Vaneet Aggarwal vaneet@purdue.edu
School of IE and ECE, Purdue University
Reviewed on OpenReview: https: // openreview. net/ forum? id= ubCoTAynPp
Abstract
We investigate an infinite-horizon average reward Markov Decision Process (MDP) with
delayed, composite, andpartiallyanonymousrewardfeedback. Thedelayandcompositeness
of rewards mean that rewards generated as a result of taking an action at a given state are
fragmented into different components, and they are sequentially realized at delayed time
instances. The partial anonymity attribute implies that a learner, for each state, only
observes the aggregate of past reward components generated as a result of different actions
taken at that state, but realized at the observation instance. We propose an algorithm
named DUCRL2 to obtain a near-optimal policy for this setting and show that it achieves a
regret bound of ˜O/parenleftig
DS√
AT+d(SA)3/parenrightig
whereSandAare the sizes of the state and action
spaces, respectively, Dis the diameter of the MDP, dis a parameter upper bounded by the
maximum reward delay, and Tdenotes the time horizon. This demonstrates the optimality
of the bound in the order of T, and an additive impact of the delay.
1 Introduction
Reinforcement learning (RL) enables an agent to learn a policyin an unknown environment by repeatedly
interacting with it. The environment has a state that changes as a result of the actions executed by the
agent. In this setup, a policy is a collection of rules that guides the agent to take action based on the
observed state of the environment. Several algorithms exist in the literature that learn a policy with near-
optimal regret (Jaksch et al., 2010; Azar et al., 2017; Jin et al., 2018; Fruit et al., 2018). However, all
of the above frameworks assume that the reward is instantly fed back to the learner after executing an
action. Unfortunately, this assumption of instantaneity may not hold in many practical scenarios. For
instance, in the online advertisement industry, a customer may purchase a product several days after seeing
the advertisement. In medical trials, the effect of a medicine may take several hours to manifest. In road
networks, it might take a few minutes to notice the impact of a traffic light change. In many of these
cases, the effect of an action is not entirely realized in a single instant, but rather fragmented into smaller
components that are sequentially materialized over a long interval. Reward feedback that satisfies this
property is referred to as delayed and composite reward. In several applications, the learner cannot directly
observe each delayed component of a composite reward but only an aggregate of the components realized at
the observation instance. For example, in the case of multiple advertisements, the advertiser can only see
the total number of purchases at a given time but is completely unaware of which advertisement resulted in
what fraction of the total purchase. Such feedback is referred to as anonymous reward.
Learning with delayed, composite, and anonymous rewards is gaining popularity in the RL community.
While most of the theoretical analysis has been directed towards the multi-arm bandit (MAB) framework
(Wang et al., 2021; Zhou et al., 2019; Vernade et al., 2020; Pike-Burke et al., 2018; Pedramfar and Aggarwal,
2023), recent studies have also analyzed Markov Decision Processes (MDPs) with delayed feedback (Howson
1Published in Transactions on Machine Learning Research (08/2023)
et al., 2023; Jin et al., 2022; Lancewicki et al., 2022). However, none of these studies have incorporated both
composite and anonymous rewards, which is the focus of this paper.
1.1 The Challenge and Our Contribution
Learning with delayed, composite, and anonymous rewards has gained popularity in the RL community.
While there has been extensive theoretical analysis of these types of rewards in the multi-arm bandit (MAB)
framework, extending these ideas to the full Markov decision process (MDP) setting poses significant chal-
lenges. For example, one of the main ideas used in the MAB framework can be stated as follows (Wang et al.,
2021). The learning algorithm proceeds in multiple epochs with the sizes of the epochs increasing exponen-
tially. At the beginning of each epoch, an action is chosen and it is applied at all instances within that epoch.
Due to multiple occurrences of the chosen action, and exponentially increasing epoch lengths, the learner
progressively obtains better estimates of the associated rewards despite the composite and anonymous na-
ture of the feedback. In an MDP setting, however, it is impossible to ensure that any state-action pair (s,a)
appears contiguously over a given stretch of time. The most that one can hope to ensure is that each state,
s, when it appears in an epoch (defined appropriately), is always paired with a unique action, a. In this
way, if the state in consideration is visited sufficiently frequently in that epoch, the learner would obtain an
accurate estimate of the reward associated with the pair (s,a). Unfortunately, in general, there is no way to
ensure a high enough visitation frequency for all states. Unlike the MAB setup, it is, therefore, unclear how
to develop regret optimal learning algorithms for MDPs with delayed, composite, and anonymous rewards.
Our article provides a partial resolution to this problem. In particular, we demonstrate that if the rewards
are delayed, composite, and partially anonymous , then an algorithm can be designed to achieve near-optimal
regret. In a fully anonymous setting, a learner observes the aggregate of the (delayed) reward components
generated as a result of all state-action pairs visited in the past. In contrast, a partially anonymous setup
allowsalearnertoobservethesumof(delayed)rewardcomponentsgeneratedasaresultofallpastvisitations
to any specified state. Our proposed algorithm, DUCRL2 , is built upon the UCRL2algorithm of (Jaksch
et al., 2010) and works in multiple epochs. Unlike the bandit setting, however, the epoch lengths are not
guaranteed to be exponentially increasing. Our primary innovation lies in demonstrating how an accurate
reward function estimate can be obtained using the partially anonymous feedback. DUCRL2 yields a regret
bound of ˜O(DS√
AT+d(SA)3)whereS,Adenote the sizes of the state and action spaces respectively, T
is the time horizon, Ddenotes the diameter of the MDP, and the parameter dis bounded by the maximum
delay in the reward generation process. The obtained result matches a well-known lower bound in T.
1.2 Relevant Literature
Below we describe in detail the relevant literature.
Regret Bounds in Non-delayed RL: The framework of regret minimization in RL with immediate
feedback is well-investigated in the literature. In particular, this topic has been explored in the settings of
both stochastic (Jaksch et al., 2010; Zanette et al., 2020; Agarwal and Aggarwal, 2023; Agarwal et al., 2022;
2023) and adversarial (Jin and Luo, 2020; Rosenberg and Mansour, 2019; Shani et al., 2020) MDPs. Our
setup can be considered to be the generalization of the stochastic MDPs with immediate feedback.
Delay in Bandit: Delayed feedback is a well-researched topic in the bandit literature, with numerous
studies conducted in both stochastic (Vernade et al., 2020; Pike-Burke et al., 2018; Zhou et al., 2019; Gael
et al., 2020; Lancewicki et al., 2021; Pedramfar and Aggarwal, 2023) and adversarial settings (Quanrud and
Khashabi, 2015; Cesa-Bianchi et al., 2016; Thune et al., 2019; Zimmert and Seldin, 2020; Bistritz et al.,
2019; Ito et al., 2020). However, as previously discussed, applying the insights of bandit learning to the
MDP setting with composite and anonymous rewards is challenging.
Delay in MDP: A number of recent papers have explored the incorporation of delayed feedback into the
MDP framework. For example, (Lancewicki et al., 2022; Jin et al., 2022) consider adversarial MDPs, while
(Howson et al., 2023) analyzes a stochastic setting. However, all of these articles focus on episodic MDPs
with non-composite and non-anonymous rewards, which is distinct from our work on infinite-horizon MDPs
with delayed, composite, and partially anonymous rewards. It is worth noting that while delayed reward is a
2Published in Transactions on Machine Learning Research (08/2023)
commonly studied topic in the literature, some works also consider delays in the state information (Agarwal
and Aggarwal, 2021; Bouteiller et al., 2021). Additionally, the impact of delay has also been explored in the
context of multi-agent learning to characterize coarse correlated equilibrium (Zhang et al., 2022).
2 Problem Setting
We consider an infinite-horizon average-reward Markov Decision Process (MDP) defined as, M≜{S,A,r,p}
whereS,Adenote the state, and action spaces respectively, r:S×A→ [0,1]is the reward function, and
p:S×A→ ∆(S)indicates the state transition function. The function, ∆(·)defines the probability simplex
over its argument set. The cardinality of the sets SandAare denoted as S,Arespectively. Both the reward
function,r, and the transition function, pare assumed to be unknown.
Alearner/agentinteractswithanenvironmentgovernedbytheMDPstatedaboveasfollows. Theinteraction
proceedsindiscretetimesteps, t∈{1,2,···}. Atthetime t, thestateoccupiedbytheenvironmentisdenoted
asst∈S. The learner observes the state, st, and chooses an action at∈Afollowing a predefined protocol, A.
As a result, a sequence of rewards rt(st,at)≜{rt,τ(st,at)}∞
τ=0is generated where rt,τ(st,at)is interpreted as
the non-negative component of the vector rt(st,at)that is realised at instant t+τ. The following assumption
is made regarding the reward generation process.
Assumption 1 It is assumed that ∀t∈{1,2,···},∀(s,a)∈S×A, the following holds.
(a)||rt(s,a)||1∼D(s,a)∈∆[0,1],
(b)E||rt(s,a)||1=r(s,a)∈[0,1],
(c){rt(s,a)}t≥1,(s,a)∈S×Aare mutually independent ,
where||·|| 1denotes the 1-norm.
Assumption 1(a) dictates that the reward sequences generated from (s,a)are such that the sum of their
components can be thought of as samples taken from a certain distribution, D(s,a), over [0,1]. Note that
the distribution, D(s,a), is independent of t. Assumption 1(b) explains that the expected value of the sum
of the reward components generated from (s,a)equalsr(s,a)∈[0,1]. Finally, Assumption 1(c) clarifies that
the reward sequences generated at different instances (either by the same or distinct state-action pairs) are
presumed to be independent of each other.
At the time instant t, the learner observes a reward vector xt∈RSwhoses-th element is expressed as
follows.
xt(s) =/summationdisplay
0<τ≤trτ,t−τ(s,aτ)1(sτ=s)
where 1(·)defines the indicator function. Note that the learner only has access to the lump-sum reward
xt(s), not its individual components contributed by past actions. This explains why the reward is termed
as partially anonymous. In a fully anonymous setting, the learner can only access ||xt||1, not its elements.
Although full anonymity might be desirable for many practical scenarios, from a theoretical standpoint, it
is notoriously difficult to analyze. We discuss this topic in detail in section 4.2. The expected accumulated
reward generated up to time Tcan be computed as,
R(s1,A,M,T ) =T/summationdisplay
t=1||rt(st,at)||1 (1)
We would like to emphasize that R(s1,A,M,T )is not the sum of the observed rewards up to time T. Rather,
it equates to the sum of all the reward components that are generated as a consequence of the actions taken
up to time T. We define the quantity expressed below
ρ(s1,A,M)≜lim
T→∞1
TE[R(s1,A,M,T )] (2)
3Published in Transactions on Machine Learning Research (08/2023)
as the average reward of the MDP Mfor a given protocol Aand an initial state s1∈S. Here the expectation
is obtained over all possible T-length trajectories generated from the initial state s1following the protocol A
and the randomness associated with the reward generation process for any given state-action pair. It is well
known that (Puterman, 2014) there exists a stationary deterministic policy π∗:S→Athat maximizes the
average reward∀s1∈SifD(M), the diameter of M(defined below) is finite. Also, in that case, ρ(s1,π∗,M)
becomes independent of s1and thus can be simply denoted as ρ∗(M). The diameter D(M)is defined as
follows.
D(M)≜max
s̸=s′min
π:S→AE[T(s′|s,π,M )]
whereT(s′|s,π,M )denotes the time needed for the MDP Mto reach the state s′from the state sfollowing
the stationary deterministic policy, π. Mathematically, Pr(T(s′|s,π,M ) =t)≜Pr(st=s|s1=s,sτ̸=s,1<
τ <t,sτ∼p(sτ−1,aτ−1),aτ∼π(sτ)). In simple words, given two arbitrary distinct states, one can always
find a stationary deterministic policy such that the MDP, M, on average, takes at most D(M)time steps
to transition from one state to the other.
We define the performance of a protocol, Aby the regret it accumulates over a horizon, Twhich is mathe-
matically expressed as,
Reg(s1,A,M,T ) =Tρ∗(M)−R(s1,A,M,T ) (3)
whereρ∗(M)is the maximum of the average reward given in (2), and the second term is defined in (1). We
would like to mention that in order to define regret, we use R(s1,A,M,T )rather than the expected sum of
theobserved rewards up to time T. The rationale behind this definition is that all the components of the
rewards that are generated as a consequence of the actions taken up to time Twould eventually be realized
if we allow the system to evolve for a long enough time. Our goal in this article is to come up with an
algorithm that achieves sublinear regret for the delayed, composite, and anonymous reward MDP described
above.
Before concluding, we would like to provide an example of an MDP with partially anonymous rewards. Let
us consider the Tround of interaction of a potential consumer with a website that advertises Scategories of
products. At round t, the state, st∈{1,···,S}observed by the website is the category of product searched
by the consumer. The s-th category where s∈{1,···,S}hasNsnumber of potential advertisements, and
the total number of advertisements is N=/summationtext
sNs. The website, in response to the observed state, st, shows
an ordered list of K <Nadvertisements (denoted by at), some of which may not directly correspond to the
searched category, st. This may cause the consumer, with some probability, to switch to a new state, st+1
in the next round. For example, if the consumer is searching for “Computers”, then showing advertisements
related to “Mouse” or “Keyboard” may incentivize the consumer to search for those categories of products.
Afterτrounds, 0<τ≤T, the consumer ends up spending rτ(s)amount of money for the s-th category of
product as a consequence of previous advertisements. Note that if the same state sappears in two different
roundst1<t2<τ, the website can potentially show two different ordered lists of advertisements, at1,at2to
the consumer. However, it is impossible to segregate the portions of the reward rτ(s)contributed by each
of those actions. Hence, the system described above can be modeled by an MDP with delayed, composite,
and partially anonymous rewards.
3 DUCRL2 Algorithm
In this section, we develop Delayed UCRL2 (DUCRL2) algorithm to achieve the overarching goal of our
paper. It is inspired by the UCRL2 algorithm suggested by (Jaksch et al., 2010) for MDPs with immediate
rewards (no delay). Before going into the details of the DUCRL2 algorithm, we would like to introduce the
following assumption.
Assumption 2 There exists a finite positive number dsuch that,∀t∈{1,2···},
/summationdisplay
τ1≥0max
(s,a)∈S×A
/summationdisplay
τ≥τ1rt,τ(s,a)
≤d (4)
4Published in Transactions on Machine Learning Research (08/2023)
Algorithm 1 DUCRL2 Algorithm
1:Input:δ∈(0,1),d,S,A
2:Initialization: Observe the initial state s1∈Sand sett←1,t0←0.
3:forepisodesk∈{1,2,···}do ▷Computing empirical estimates
4:tk←t
5: for(s,a)∈S×A do
6:νk(s,a)←0
7:Ej(s,a)←1/parenleftbig
(s,a)∈/braceleftbig
(sτ,aτ)/vextendsingle/vextendsingletj−1≤τ <tj/bracerightbig/parenrightbig
,∀j∈{1,···,k−1}
8:Ek(s,a)←/summationtext
0<j<kEj(s,a)
9:Nk(s,a)←/summationtext
0<τ<t k1(sτ=s,aτ=a)
10: ˆrk(s,a)←/summationtext
0<j<k/summationtext
tj≤τ<t j+1xτ(s)Ej(s,a)/max{1,Nk(s,a)}
11: fors′∈Sdo
12: ˆpk(s′|s,a) =/summationtext
0<τ<t k1(sτ=s,aτ=a,sτ+1=s′)/max{1,Nk(s,a)}
13: end for
14: end for
15:end for
16:LetMkbe the set of MDPs with state space S, action spaceA, transition probability ˜p, and reward
function ˜rsuch that
|˜r(s,a)−ˆrk(s,a)|≤/radicaligg
7 log(2SAtk/δ)
2 max{Nk(s,a),1}+dEk(s,a)
max{Nk(s,a),1}(5)
||˜p(·|s,a)−ˆpk(·|s,a)||1≤/radicaligg
14Slog(2Atk/δ)
max{1,Nk(s,a)}(6)
17:Using extended value function iteration (Algorithm 2), obtain a stationary deterministic policy ˜πkand
an MDP ˜Mk∈Mksuch that,
˜ρk≜min
s∈Sρ(s,˜πk,˜Mk)≥ max
M′∈M k,π,s′∈Sρ(s′,π,M′)−1√tk(7)
18:whileνk(st,˜πk(st))<max{1,Nk(st,˜πk(st))}do
19:Executeat= ˜πk(st)
20:Observe the reward rtand the next state st+1
21:νk(st,at)←νk(st,at) + 1
22:t←t+ 1
23:end while
with probability 1where{rt,τ(s,a)}∞
τ=0is the reward sequence generated by (s,a)at timet.
Notethatifthemaximumdelayis dmax, thenusingAssumption1(a), onecanshowthat d≤dmax. Therefore,
dcanbethoughtofasaproxyforthemaximumdelay. TobetterunderstandtheintuitionbehindAssumption
2, consider an interval {1,···,T1}. Clearly, the reward sequence generated at t1=T1−τ1,τ1∈{0,···,T1−
1}, isrt1(st1,at1). The portion of this reward that is realized after T1is expressed by the following quantity:/summationtext
τ≥τ1rt1,τ(st1,at1)which is upper bounded by max (s,a)/summationtext
τ≥τ1rt1,τ(s,a). Therefore, the total amount of
reward that is generated in {1,···,T1}but realized after T1is bounded by/summationtext
τ1≥0max (s,a)/summationtext
τ≥τ1rt1,τ(s,a).
As the distribution of the reward sequence rt1is the same for all t1(Assumption 1(a)), one can replace the
τ1dependent term t1with a generic quantity, t. Thus, Assumption 2 essentially states that the spillover of
the rewards generated within any finite interval {1,···,T1}can be bounded by the term d.
We would also like to emphasize that if dmaxis infinite, then dmay or may not be infinite depending on the
reward sequence. For example, consider a deterministic sequence whose components are rt,τ(s,a) = 2−1−τ,
(s,a)∈S×A,∀t∈{1,2,···},∀τ∈{0,1,···}. It is easy to show that one can choose d= 2thoughdmaxis
5Published in Transactions on Machine Learning Research (08/2023)
infinite. On the other hand, if rt,τ(s,a) =1(τ=τ′),∀(s,a),∀t,∀τwhereτ′is a random variable with the
distribution Pr(τ′=k) = (1−p)pk,∀k∈{0,1,···},0<p< 1, then one can show that (4) is violated with
probability at least pd. In other words, Assumption 2 is not satisfied for any finite d.
The DUCRL2 algorithm (Algorithm 1) proceeds in multiple epochs. At the beginning of epoch k, i.e., at
time instant t=tk, we compute two measures for all state-action pairs. The first measure is indicated as
Nk(s,a)which counts the number of times the pair (s,a)appears before the onset of the kth epoch. The
second measure is Ej(s,a)which is a binary random variable that indicates whether the pair (s,a)appears
at least once in the jth epoch, 0< j < k. Due to the very nature of our algorithm (elaborated later), in
a given epoch j, ifEj(s,a) = 1, thenEj(s,a′) = 0,∀a′∈A\{a}. Taking a sum over {Ej(s,a)}0<j<k, we
obtainEk(s,a)which counts the number of epochs where (s,a)appears at least once before tk, the start of
thekth episode.
Next, we obtain the reward estimate ˆrk(s,a)by computing the sum of the s-th element of the observed
reward over all epochs where (s,a)appears at least once and dividing it by Nk(s,a). Also, the transition
probability estimate ˆpk(s′|s,a)is calculated by taking a ratio of the number of times the transition (s,a,s′)
occurs before the kth episode and Nk(s,a). It is to be clarified that, due to the delayed composite nature
of the reward, the observed reward values that are used for computing ˆrk(s,a)may be contaminated by
the components generated by previous actions which could potentially be different from a. Consequently, it
might appear that the empirical estimates ˆrk(s,a)may not serve as a good proxy for r(s,a). However, the
analysis of our algorithm exhibits that by judiciously steering the exploration, it is still possible to obtain a
near-optimal regret.
Using the estimates ˆrk(s,a),ˆpk(·|s,a), we now define a confidence set Mkof MDPs that is characterized by
(5),(6). The confidence radius given in (5)is one of the main differences between our algorithm and the
UCRL2 algorithm given by (Jaksch et al., 2010). Applying extended value iteration (Appendix A), we then
derive a policy ˜πk, and an MDP ˜Mk∈Mkthat are near-optimal within the set Mkin the sense of (7). We
keep on executing the policy ˜πkuntil for at least one state-action pair (s,a), its total number of occurrences
within the current epoch, νk(s,a)becomes at least as large as Nk(s,a), the number of its occurrences before
the onset of the current epoch. When this criterion is achieved, a new epoch begins and the process described
above starts all over again. Observe that, as the executed policies are deterministic, no two distinct pairs
(s,a),(s,a′)can appear in the same epoch for a given state, s.
We would like to conclude with the remark that all the quantities used in our algorithm can be computed
in a recursive manner. Consequently, similar to UCRL2, the space complexity of our algorithm turns out to
beO(S2A)which is independent of T.
4 Regret Analysis
Below we state our main result.
Theorem 1 LetD≜D(M). With probability at least 1−δ,δ∈(0,1), for arbitrary initial state s, the
regret accumulated by the algorithm DUCRL2 overT >1steps can be bounded above as follows.
Reg(s,DUCRL2,M,T )≤34DS/radicaligg
ATlog/parenleftbiggT
δ/parenrightbigg
+ 2d(SA)3/bracketleftbigg
log2/parenleftbigg8T
SA/parenrightbigg/bracketrightbigg2
(8)
Substituting δ= 1/T, we can therefore bound the expected regret as,
E[Reg(s,DUCRL2,M,T )]≤68DS/radicalbig
ATlog (T) + 2d(SA)3/bracketleftbigg
log2/parenleftbigg8T
SA/parenrightbigg/bracketrightbigg2
(9)
Theorem 1 states that the regret accumulated by algorithm DUCRL2 is˜O(DS√
AT+d(SA)3)where ˜O(·)
hides the logarithmic factors. (Jaksch et al., 2010) showed that the lower bound on the regret is Ω(√
DSAT ).
As our setup is a generalization of the setup considered in (Jaksch et al., 2010), the same lower bound must
6Published in Transactions on Machine Learning Research (08/2023)
also apply to our model. Moreover, if we consider an MDP instance where the rewards arrive with a constant
delay,d, then in the first dsteps, due to lack of any feedback, each algorithm must obey the regret lower
bound Ω(d). Weconcludethattheregretlowerboundofoursetupis Ω(max{√
DSAT,d}) = Ω(√
DSAT +d).
Although it matches the orders of T, anddof our regret upper bound, there is still room for improvement
in the orders of D,SandA.
4.1 Proof Sketch of Theorem 1
In this section, we provide a brief sketch of the proof of Theorem 1.
Step 1: The first step is to rewrite the total regret as the sum of regrets accumulated over various epochs.
Particularly, we show that with probability at least 1−δ/12T5/4,δ>0the following bound holds.
Reg(s,DUCRL2,M,T )≤m/summationdisplay
k=1Regk
/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
≜Q1+/radicaligg
5T
8log/parenleftbigg8T
δ/parenrightbigg
/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
≜Q2
The term, Regkcan be defined as the regret accumulated over epoch k(a precise definition is given in the
appendix), and mis such that Tlies in the (m−1)th epoch. The additional term, Q2appears due to the
stochasticity of the observed reward instances. We now divide Q1into two parts as follows.
Q1=m/summationdisplay
k=1Regk1(M /∈Mk)
/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
≜Q11+m/summationdisplay
k=1Regk1(M∈Mk)
/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
≜Q12
Step 2: In order to bound Q11, it is important to have an estimate of Pr(M /∈Mk)which we obtain in
Lemma 1. We would like to elaborate that although the bound given in Lemma 1 is similar to that given in
(Jaksch et al., 2010), the proof techniques are different. In particular, here we account for the fact that the
reward estimates, {ˆrk(s,a)}(s,a)∈S×A, of thekth epoch, are potentially corrupted by delayed effects of past
actions. We resolve this problem by proving the following inequality (see (32)).
/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleˆrk(s,a)−1
Nk(s,a)/summationdisplay
0<τ<t k||rτ(s,a)||11(sτ=s,aτ=a)/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle≤dEk(s,a)
max{Nk(s,a),1}(10)
Here the second term in the LHS denotes an estimate of r(s,a)that the learner would have obtained had
there been no delay in the observation of the reward instances. In other words, inequality (10)estimates
the gap between an MDP with delayed observations, and a hypothetical MDP without any delayed effects.
Observe that the RHS of (10)also appears in the confidence radius (5). Therefore, the cost of incorporating
delay is a looser confidence in the reward estimates.
Step 3: Using Lemma 1, Q11is bounded by√
Twith high probability (Lemma 2).
Step 4: We now focus on bounding the other term, Q12. Lemma 3 shows that Regk≤J1
k+J2
k+J3
kwhere
the precise definition of the terms {Ji
k}i∈{1,2,3}are given in the appendix B.2. Furthermore, it also proves
that the following bound holds with high probability.
m/summationdisplay
k=1J1
k1(M∈Mk) =˜O
√
T+m/summationdisplay
k=1/summationdisplay
(s,a)∈S×Aνk(s,a)/radicalbig
max{Nk(s,a),1}

where ˜O(·)hides logarithmic factors and terms related to D,S,A. The other notations are identical to that
given in section 3.
7Published in Transactions on Machine Learning Research (08/2023)
Step 5: The second term, J2
kis bounded as follows.
J2
k≜/summationdisplay
(s,a)∈S×Aνk(s,a)(˜rk(s,a)−r(s,a))
≤/summationdisplay
(s,a)∈S×Aνk(s,a)|˜rk(s,a)−ˆrk(s,a)|+/summationdisplay
(s,a)∈S×Aνk(s,a)|ˆrk(s,a)−r(s,a)|
Notice that the first term can be bounded by invoking (5). The same inequality can also be used to bound the
second term provided that M∈Mkwhich, as we have stated before, is a high probability event (Lemma 1).
Using this logic, and some algebraic manipulations, we finally obtain the following high probability bound.
m/summationdisplay
k=1Jk
21(M∈Mk) =˜O
m/summationdisplay
k=1/summationdisplay
(s,a)∈S×Aνk(s,a)/radicalbig
max{Nk(s,a),1}+ 2dSAm2

Step 6: Finally, we obtain the following inequality related to the third term.
m/summationdisplay
k=1J3
k1(M∈Mk)≤2m/summationdisplay
k=1/summationdisplay
(s,a)∈S×Aνk(s,a)/radicalbig
max{Nk(s,a),1}
Step 7: Combining, we derive the bound stated below with high probability.
Q1=Q11+Q12≤√
T+3/summationdisplay
i=1m/summationdisplay
k=1Ji
k1 (M∈Mk)
=˜O
√
T+m/summationdisplay
k=1/summationdisplay
(s,a)∈S×Aνk(s,a)/radicalbig
max{Nk(s,a),1}+ 2dSAm2

Step 8: We conclude the proof using Lemma 4 which states that,
m≤SAlog2/parenleftbigg8T
SA/parenrightbigg
,andm/summationdisplay
k=1/summationdisplay
(s,a)∈S×Aνk(s,a)/radicalbig
max{Nk(s,a),1}≤(√
2 + 1)√
SAT
4.2 Limitation
It is important to understand why our approach works with partial anonymity but not with full anonymity.
Fix a state s∈Sand an epoch j. Recall from section 3that no two distinct pairs (s,a),(s,a′)can appear
in the same epoch. Utilizing this property, we can write down the following relation for a pair (s,a)that
appears in the jth epoch.
/summationdisplay
tj≤τ<t j+1xτ(s)
/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
≜R0=/summationdisplay
0<τ<t j/summationdisplay
τ1≥tj−τrτ,τ1(sτ,aτ)1(sτ=s)
/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
≜R1+/summationdisplay
tj≤τ<t j+1||rτ(s,a)||11(sτ=s)
/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
≜R2
−/summationdisplay
0<τ<t j+1/summationdisplay
τ1≥tj+1−τrτ,τ1(sτ,aτ)1(sτ=s)
/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
≜R3(11)
The term, R0denotes the sum of s-th components of all observed reward vectors within epoch j. Some
portion ofR0is due to actions taken before the onset of the jth epoch. This past contribution is denoted
by the term, R1. The rest of R0is entirely contributed by the actions taken within epoch j. The term, R2
8Published in Transactions on Machine Learning Research (08/2023)
denotes the sum of all the rewards generated as a result of actions taken within epoch j. However, due to
the delayed nature of the reward, some portion of R2will be realized after the jth epoch. This spillover part
is termed as R3. Using Assumption 2, we can write 0≤R1,R3≤dwith high probability which leads to the
following relation.
−d≤R0−R2≤d (12)
Eq.(12)isthefirststepinestablishing (10). Wewouldliketoemphasizethefactthattheterm, R2isentirely
contributed by (s,a)pairs appearing in epoch j(i.e., no contamination from actions other than a). In other
words, although our estimation, ˆr(s,a)is based on the contaminated observation R0, we demonstrate that it
is not far away from uncontaminated estimations. This key feature makes DUCRL2 successful despite having
partial anonymity. On the other hand, if rewards were fully anonymous, then (11)would have changed as
follows.
/summationdisplay
tj≤τ<t j+1/summationdisplay
s∈Sxτ(s)
/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
≜˜R0=/summationdisplay
0<τ<t j/summationdisplay
τ1≥tj−τrτ,τ1(sτ,aτ)
/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
≜˜R1+/summationdisplay
tj≤τ<t j+1||rτ(sτ,aτ)||1
/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
≜˜R2
−/summationdisplay
0<τ<t j+1/summationdisplay
τ1≥tj+1−τrτ,τ1(sτ,aτ)
/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
≜˜R3(13)
Note that in (13), the term, ˜R2is a mixer of contributions from various state-action pairs, unlike R2in(11).
This makes our algorithm ineffective in the presence of full anonymity.
Another limitation of our approach is that the delay parameter dis used as an input to Algorithm 1. One
can therefore ask how the regret bound changes if an incorrect estimate, ˆdofdis used in the algorithm. One
can easily prove that if ˆd>d, then the regret bound changes to O(DS√
AT+ˆd(SA)3). However, if ˆd<d,
then Lemma 1 no longer works, and consequently, the analysis does not yield any sub-linear regret.
5 Conclusion
In this work, we addressed the challenging problem of designing learning algorithms for infinite-horizon
Markov Decision Processes with delayed, composite, and partially anonymous rewards. We propose an
algorithm that achieves near-optimal performance and derive a regret bound that matches the existing lower
bound in the time horizon while demonstrating an additive impact of delay on the regret. Our work is the
first to consider partially anonymous rewards in the MDP setting with delayed feedback.
Possiblefutureworkincludesextendingtheanalysistothemoregeneralscenariooffullyanonymous, delayed,
and composite rewards, which has important applications in many domains. This extension poses theoretical
challenges, and we believe it is an exciting direction for future research. Overall, we hope our work provides a
useful contribution to the reinforcement learning community and inspires further research on this important
topic.
References
Mridul Agarwal and Vaneet Aggarwal. Blind decision making: Reinforcement learning with delayed obser-
vations.Pattern Recognition Letters , 150:176–182, 2021.
Mridul Agarwal and Vaneet Aggarwal. Reinforcement learning for joint optimization of multiple rewards.
J. Mach. Learn. Res. , 24:49:1–49:41, 2023. URL http://jmlr.org/papers/v24/19-980.html .
MridulAgarwal, QinboBai, andVaneetAggarwal. Regretguaranteesformodel-basedreinforcementlearning
with long-term average constraints. In Uncertainty in Artificial Intelligence , pages 22–31. PMLR, 2022.
9Published in Transactions on Machine Learning Research (08/2023)
Mridul Agarwal, Qinbo Bai, and Vaneet Aggarwal. Concave utility reinforcement learning with zero-
constraint violations. Transactions on Machine Learning Research , 2023.
Mohammad Gheshlaghi Azar, Ian Osband, and Rémi Munos. Minimax regret bounds for reinforcement
learning. In International Conference on Machine Learning , pages 263–272. PMLR, 2017.
Ilai Bistritz, Zhengyuan Zhou, Xi Chen, Nicholas Bambos, and Jose Blanchet. Online exp3 learning in
adversarial bandits with delayed feedback. Advances in neural information processing systems , 32, 2019.
Yann Bouteiller, Simon Ramstedt, Giovanni Beltrame, Christopher Pal, and Jonathan Binas. Reinforcement
learning with random delays. In International conference on learning representations , 2021.
Nicol‘o Cesa-Bianchi, Claudio Gentile, Yishay Mansour, and Alberto Minora. Delay and cooperation in
nonstochastic bandits. In Conference on Learning Theory , pages 605–622. PMLR, 2016.
Ronan Fruit, Matteo Pirotta, Alessandro Lazaric, and Ronald Ortner. Efficient bias-span-constrained
exploration-exploitation in reinforcement learning. In International Conference on Machine Learning ,
pages 1578–1586. PMLR, 2018.
Manegueu Anne Gael, Claire Vernade, Alexandra Carpentier, and Michal Valko. Stochastic bandits with
arm-dependent delays. In International Conference on Machine Learning , pages 3348–3356. PMLR, 2020.
Benjamin Howson, Ciara Pike-Burke, and Sarah Filippi. Optimism and delays in episodic reinforcement
learning. In International Conference on Artificial Intelligence and Statistics , pages 6061–6094. PMLR,
2023.
Shinji Ito, Daisuke Hatano, Hanna Sumita, Kei Takemura, Takuro Fukunaga, Naonori Kakimura, and Ken-
Ichi Kawarabayashi. Delay and cooperation in nonstochastic linear bandits. Advances in Neural Informa-
tion Processing Systems , 33:4872–4883, 2020.
Thomas Jaksch, Ronald Ortner, and Peter Auer. Near-optimal regret bounds for reinforcement learning.
Journal of Machine Learning Research , 11:1563–1600, 2010.
Chi Jin, Zeyuan Allen-Zhu, Sebastien Bubeck, and Michael I Jordan. Is Q-learning provably efficient?
Advances in neural information processing systems , 31, 2018.
Tiancheng Jin and Haipeng Luo. Simultaneously learning stochastic and adversarial episodic mdps with
known transition. Advances in neural information processing systems , 33:16557–16566, 2020.
Tiancheng Jin, Tal Lancewicki, Haipeng Luo, Yishay Mansour, and Aviv Rosenberg. Near-optimal regret for
adversarial mdp with delayed bandit feedback. In Advances in Neural Information Processing Systems ,
2022.
Tal Lancewicki, Shahar Segal, Tomer Koren, and Yishay Mansour. Stochastic multi-armed bandits with
unrestricted delay distributions. In International Conference on Machine Learning , pages 5969–5978.
PMLR, 2021.
Tal Lancewicki, Aviv Rosenberg, and Yishay Mansour. Learning adversarial markov decision processes with
delayed feedback. In Proceedings of the AAAI Conference on Artificial Intelligence , volume 36, pages
7281–7289, 2022.
Mohammad Pedramfar and Vaneet Aggarwal. Stochastic submodular bandits with delayed composite anony-
mous bandit feedback. arXiv preprint arXiv:2303.13604 , 2023.
Ciara Pike-Burke, Shipra Agrawal, Csaba Szepesvari, and Steffen Grunewalder. Bandits with delayed,
aggregated anonymous feedback. In International Conference on Machine Learning , pages 4105–4113.
PMLR, 2018.
Martin L Puterman. Markov decision processes: discrete stochastic dynamic programming . John Wiley &
Sons, 2014.
10Published in Transactions on Machine Learning Research (08/2023)
KentQuanrudandDanielKhashabi. Onlinelearningwithadversarialdelays. Advances in neural information
processing systems , 28, 2015.
Aviv Rosenberg and Yishay Mansour. Online convex optimization in adversarial markov decision processes.
InInternational Conference on Machine Learning , pages 5478–5486. PMLR, 2019.
Lior Shani, Yonathan Efroni, Aviv Rosenberg, and Shie Mannor. Optimistic policy optimization with bandit
feedback. In International Conference on Machine Learning , pages 8604–8613. PMLR, 2020.
Tobias Sommer Thune, Nicolò Cesa-Bianchi, and Yevgeny Seldin. Nonstochastic multiarmed bandits with
unrestricted delays. Advances in Neural Information Processing Systems , 32, 2019.
Claire Vernade, Alexandra Carpentier, Tor Lattimore, Giovanni Zappella, Beyza Ermis, and Michael Brueck-
ner. Linear bandits with stochastic delayed feedback. In International Conference on Machine Learning ,
pages 9712–9721. PMLR, 2020.
Siwei Wang, Haoyun Wang, and Longbo Huang. Adaptive algorithms for multi-armed bandit with composite
and anonymous feedback. In Proceedings of the AAAI Conference on Artificial Intelligence , volume 35,
pages 10210–10217, 2021.
Tsachy Weissman, Erik Ordentlich, Gadiel Seroussi, Sergio Verdu, and Marcelo J Weinberger. Inequalities
for the l1 deviation of the empirical distribution. Hewlett-Packard Labs, Tech. Rep , 2003.
Andrea Zanette, Alessandro Lazaric, Mykel Kochenderfer, and Emma Brunskill. Learning near optimal
policies with low inherent bellman error. In International Conference on Machine Learning , pages 10978–
10989. PMLR, 2020.
Yuyang Zhang, Runyu Zhang, Gen Li, Yuantao Gu, and Na Li. Multi-agent reinforcement learning with
reward delays. arXiv preprint arXiv:2212.01441 , 2022.
Zhengyuan Zhou, Renyuan Xu, and Jose Blanchet. Learning in generalized linear contextual bandits with
stochastic delays. Advances in Neural Information Processing Systems , 32, 2019.
Julian Zimmert and Yevgeny Seldin. An optimal algorithm for adversarial bandits with arbitrary delays. In
International Conference on Artificial Intelligence and Statistics , pages 3285–3294. PMLR, 2020.
11Published in Transactions on Machine Learning Research (08/2023)
A Extended Value Iteration
Algorithm 2 Extended Value Iteration
1:Input:{d(s,a),P(s,a),ˆr(s,a),P(s,a)}(s,a)∈S×A,ϵ>0
2:Initialization: u0≜{u0(s)}s∈S←0,i←0,error←2ϵ
3:while error<ϵdo
4: fors∈Sdo
5:ui+1(s) = max
a∈A/braceleftbigg
ˆr(s,a) +d(s,a) + max
p(·)∈P(s,a)/summationtext
s′∈Sp(s′|s,a)ui(s′)/bracerightbigg
6: end for
7: error←max
s∈S{ui+1(s)−ui(s)}−min
s∈S{ui+1(s)−ui(s)}
8:i←i+ 1
9:end while
Hered(s,a)can be though of as the confidence radius as depicted in (5)whereasP(s,a)is the set of
probability vectors that satisfy (6). Note that the stopping criteria for Algorithm 2 is the following.
max
s∈S{ui+1(s)−ui(s)}−min
s∈S{ui+1(s)−ui(s)}<ϵ (14)
In the context of Algorithm 1, we can take ϵ= 1/√tk. Theorem 7 of (Jaksch et al., 2010) guarantees that
the greedy policy deduced from the terminal utility vector uiof Algorithm 2 is ϵ-optimal in the sense of (7)
if the set of MDPs whose transition probability distribution p(·|s,a)lies in the confidence set P(s,a), and
the reward function, r(s,a)lies at most d(s,a)distance away from the estimate ˆr(s,a), comprises at least
one MDP with a finite diameter. As a consequence of this result, we can write the following corollary.
Corollary 1 Let,Mbe the collection of MDPs whose reward function r(·,·)and transition function, p(·|·,·)
satisfy the following for given {ˆr(s,a),d(s,a),P(s,a)}(s,a)∈S×A.
|r(s,a)−ˆr(s,a)|≤d(s,a),
p(·|s,a)∈P(s,a)
If the true MDP, Mlies inM, then Algorithm 2 always converges. Moreover, if uiindicates the terminal
utility vector for a given ϵ>0, and∀(s,a)∈S×A,
(˜π(s),˜p(·|s,a))≜ arg max
a∈A, p(·)∈P(s,a)/braceleftigg
ˆr(s,a) +d(s,a) +/summationdisplay
s′∈Sp(s′|s,a)ui(s′)/bracerightigg
, (15)
then the following inequality holds.
min
s∈Sρ(s,˜π,˜M)≥ max
M′∈M,π,s′∈Sρ(s′,π,M′)−ϵ
where ˜Mis an MDP with transition function, ˜p(·|··)defined by (15), and reward function ˜rthat obeys
˜r(s,a) = ˆr(s,a) +d(s,a),∀(s,a)∈S×A.
Corollary 1 is easily established by observing that the true MDP, Mis assumed to have a finite diameter. It
is also worthwhile to state that the complexity of updating the vector uiisO(S2A)as discussed in section
3.1.2 of (Jaksch et al., 2010).
B Proof of Theorem 1
LetTbe such that tm−1≤T <tm. Clearly,
T <tm=/summationdisplay
(s,a)∈S×ANm(s,a) =m/summationdisplay
k=1/summationdisplay
(s,a)∈S×Aνk(s,a)
12Published in Transactions on Machine Learning Research (08/2023)
Using the above relation, the regret given in (3)can be rewritten as,
Reg(s,DUCRL2,M,T ) =Tρ∗(M)−T/summationdisplay
t=1||rt(st,at)||1
</summationdisplay
(s,a)∈S×ANm(s,a) [ρ∗(M)−r(s,a)] +/summationdisplay
(s,a)∈S×ANm(s,a)r(s,a)−T/summationdisplay
t=1||rt(st,at)||1
=m/summationdisplay
k=1/summationdisplay
(s,a)∈S×Aνk(s,a) [ρ∗(M)−r(s,a)]
/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
≜Regk+/summationdisplay
(s,a)∈S×ANm(s,a)r(s,a)−T/summationdisplay
t=1||rt(st,at)||1
Theterm Regkcanbeinterpretedastheregretaccumulatedoverepoch k. Observethat, foragivenhistoryof
state-action evolution HT≜{(st,at)}T
t=1up to timeT, the collection of random variables {||rt(st,at)||1}T
t=1
are mutually independent. Moreover,
E/bracketleftiggT/summationdisplay
t=1||rt(st,at)||1/vextendsingle/vextendsingleHT/bracketrightigg
=T/summationdisplay
t=1r(st,at) =/summationdisplay
(s,a)∈S×Ar(s,a)T/summationdisplay
t=11(st=s,at=a)
=/summationdisplay
(s,a)∈S×ANm(s,a)r(s,a)
Using Hoeffding ’s inequality, we therefore obtain,
Pr

/summationdisplay
(s,a)∈S×ANm(s,a)r(s,a)−T/summationdisplay
t=1||rt(st,at)||1>/radicaligg
5T
8log/parenleftbigg8T
δ/parenrightbigg/vextendsingle/vextendsingle/vextendsingleHT

≤δ
12T5
4
This implies that, with probability at least 1−δ/12T5
4, the following holds.
Reg(s,DUCRL2,M,T )≤m/summationdisplay
k=1Regk+/radicaligg
5T
8log/parenleftbigg8T
δ/parenrightbigg
(16)
B.1 Regret bound on episodes where Mlie outside the confidence set
Recall thatMkis defined to be a collection of MDPs that obey the confidence bounds (5), and (6). In this
subsection, we calculate the regret contribution of the episodes where the true MDP Mdoes not satisfy
these bounds. The following lemma provides an upper bound estimate of the probability that the true MDP,
Mdoes not lie in the confidence set, Mk.
Lemma 1 Pr{M /∈Mk}≤δ
15t6
k
The proof of Lemma 1 is relegated to Appendix C. Although the final result in Lemma 1 is the same as
in (Jaksch et al., 2010), the proof techniques are quite different. In particular, we need to account for the
fact that reward estimates might be corrupted by contributions originating from various past actions. Using
Lemma 1, the following bound can be obtained.
Lemma 2 (Jaksch et al., 2010) With probability at least 1−δ/12T5
4,
m/summationdisplay
k=1Regk1(M /∈Mk)≤√
T (17)
We would like to mention here that although the definition of Mkused in our article is different from that
given in (Jaksch et al., 2010), the above result still holds. This is mainly because the only property of Mk
that is invoked to prove Lemma 2 is provided in Lemma 1 which is the same as in (Jaksch et al., 2010).
13Published in Transactions on Machine Learning Research (08/2023)
B.2 Regret bound on episodes where Mlie inside the confidence set
Letkbe the index of an episode where M∈Mkanduk={uk(s)}s∈Sbe the terminal utility vector obtained
via extended value iteration at the kth episode. Define, wk={wk(s)}s∈S,
wk(s)≜uk(s)−maxs∈Suk(s) + mins∈Suk(s)
2
Lemma 3 (Jaksch et al., 2010) If kis such that M∈Mk, then,
Regk≤/summationdisplay
s∈Sνk(s,˜πk(s))/bracketleftigg/summationdisplay
s′∈S˜pk(s′|s,˜πk(s))wk(s′)−wk(s)/bracketrightigg
/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
≜J1
k
+/summationdisplay
(s,a)∈S×Aνk(s,a)(˜rk(s,a)−r(s,a))
/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
≜J2
k+ 2/summationdisplay
(s,a)∈S×Aνk(s,a)√tk
/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
≜J3
k
where ˜rk,˜pk,˜πkare the reward, transition function, and policy of the MDP, ˜Mk. Moreover,
m/summationdisplay
k=1J1
k1(M∈Mk)≤D/radicaligg
5
2Tlog/parenleftbigg8T
δ/parenrightbigg
+DSA log2/parenleftbigg8T
SA/parenrightbigg
+D/radicaligg
14Slog/parenleftbigg2AT
δ/parenrightbiggm/summationdisplay
k=1/summationdisplay
(s,a)∈S×Aνk(s,a)/radicalbig
max{Nk(s,a),1}(18)
with probability at least 1−δ/12T5
4.
The only properties of Mkthat are used in the proof of Lemma 3 are (6), and (7)which are the same as
given in (Jaksch et al., 2010). Note that, the term J2
kdefined in Lemma 3 can be bounded as follows,
J2
k≤/summationdisplay
(s,a)∈S×Aνk(s,a)|˜rk(s,a)−ˆrk(s,a)|+/summationdisplay
(s,a)∈S×Aνk(s,a)|ˆrk(s,a)−r(s,a)|
(a)
≤/summationdisplay
(s,a)∈S×A2νk(s,a)/radicaligg
7 log/parenleftbig2SAT
δ/parenrightbig
2 max{Nk(s,a),1}+/summationdisplay
(s,a)∈S×A2dνk(s,a)
max{Nk(s,a),1}Ek(s,a)(19)
where (a)applies the facts that M,˜Mk∈Mk, andtk≤T. Note that our algorithm enforces νk(s,a)≤
max{Nk(s,a),1}. Therefore, Jk
2can be further bounded as,
Jk
2≤/summationdisplay
(s,a)∈S×Aνk(s,a)/radicalbig
max{Nk(s,a),1}/radicaligg
14 log/parenleftbigg2SAT
δ/parenrightbigg
+ 2d/summationdisplay
(s,a)∈S×AEk(s,a) (20)
Observe that, Ek(s,a)≤m,∀(s,a)∈S×A. Therefore,
m/summationdisplay
k=1Jk
21(M∈Mk)≤/radicaligg
14 log/parenleftbigg2SAT
δ/parenrightbiggm/summationdisplay
k=1/summationdisplay
(s,a)∈S×Aνk(s,a)/radicalbig
max{Nk(s,a),1}+ 2dSAm2(21)
Finally, injecting the inequality, max{Nk(s,a),1}≤tk, we obtain the following bound,
m/summationdisplay
k=1J3
k1(M∈Mk)≤2m/summationdisplay
k=1/summationdisplay
(s,a)∈S×Aνk(s,a)/radicalbig
max{Nk(s,a),1}(22)
We now use the following lemma to simplify the upper bounds.
14Published in Transactions on Machine Learning Research (08/2023)
Lemma 4 (Jaksch et al., 2010) The following inequalities hold true,
m≤SAlog2/parenleftbigg8T
SA/parenrightbigg
, (23)
m/summationdisplay
k=1/summationdisplay
(s,a)∈S×Aνk(s,a)/radicalbig
max{Nk(s,a),1}≤(√
2 + 1)√
SAT (24)
Using Lemma 4 and combining (18),(20),(22), we conclude that the following satisfies with probability at
least 1−δ/12T5
4.
m/summationdisplay
k=1Regk1(M∈Mk)≤D/radicaligg
5
2Tlog/parenleftbigg8T
δ/parenrightbigg
+DSA log2/parenleftbigg8T
SA/parenrightbigg
+2(√
2 + 1)/bracketleftigg
D/radicaligg
14Slog/parenleftbigg2AT
δ/parenrightbigg
+ 1/bracketrightigg
√
SAT + 2d(SA)3/bracketleftbigg
log2/parenleftbigg8T
SA/parenrightbigg/bracketrightbigg2(25)
B.3 Obtaining the Total Regret
Combining (16),(27), and (25), we can now establish that the following inequality is satisfied with at least
1−δ/12T5/4−δ/12T5/4−δ/12T5/4= 1−δ/4T5
4probability.
Reg(s,DUCRL2,M,T )≤34DS/radicaligg
ATlog/parenleftbiggT
δ/parenrightbigg
+ 2d(SA)3/bracketleftbigg
log2/parenleftbigg8T
SA/parenrightbigg/bracketrightbigg2
Taking a union bound on Tand noting that/summationtext∞
T=2δ/4T5
4<δ, we conclude the theorem.
C Proof of Lemma 1
The probability that the L1-deviation between the true and the empirical distributions of levents over n
independent sample exceeds ϵcan be bounded as (Weissman et al., 2003),
Pr{||ˆp(·)−p(·)||1>ϵ}≤(2l−2) exp/parenleftbigg
−nϵ2
2/parenrightbigg
(26)
Presume, without loss of generality that, Nk(s,a)≥1. In our case, inequality (26)can be utilised to obtain
the following bound ∀(s,a)∈S×A.
Pr/braceleftigg
||ˆpk(·|s,a)−p(·|s,a)||1>/radicaligg
14Slog (2Atk/δ)
Nk(s,a)/bracerightigg
(a)
≤/summationdisplay
0<n<t kPr/braceleftigg
||ˆpk(·|s,a)−p(·|s,a)||1>/radicalbigg
14Slog (2Atk/δ)
n/bracerightigg
≤/summationdisplay
0<n<t k2Sexp (−7Slog (2Atk/δ))≤/summationdisplay
0<n<t kδ
20SAt7
k=δ
20SAt6
k(27)
Inequality (a)is an application of the union bound. Recall that Ej(s,a)indicates whether the pair (s,a)
appears in the jth episode. Using this notation, we deduce that, if Ej(s,a) = 1for some (s,a)∈S×A, then,
/summationdisplay
tj≤τ<t j+1xτ(s) =/summationdisplay
tj≤τ<t j+1||rτ(s,a)||11(sτ=s) +/summationdisplay
0<τ<t j/summationdisplay
τ1≥tj−τrτ,τ1(sτ,aτ)1(sτ=s)
−/summationdisplay
0<τ<t j+1/summationdisplay
τ1≥tj+1−τrτ,τ1(sτ,aτ)1(sτ=s)(28)
15Published in Transactions on Machine Learning Research (08/2023)
Using Assumption 2, we can now write the following ∀j.
/summationdisplay
0<τ<t j/summationdisplay
τ1≥tj−τrτ,τ1(sτ,aτ)1(sτ=s)≤dwith probability 1 (29)
Combining (28),(29), we can now write the following with probability 1,
−d≤/summationdisplay
tj≤τ<t j+1xτ(s)−/summationdisplay
tj≤τ<t j+1||rτ(s,a)||11(sτ=s)≤d (30)
Taking a sum over all episodes j∈{1,···,k−1}whereEj(s,a) = 1, we get the following inequality that is
satisfied with probability 1.
/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/summationdisplay
0<j<k/summationdisplay
tj≤τ<t j+1xτ(s)Ej(s,a)−/summationdisplay
0<τ<t k||rτ(s,a)||11(sτ=s,aτ=a)/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle≤dEk(s,a) (31)
Using the definition of ˆrk(s,a), the above inequality can be rephrased as,
/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleˆrk(s,a)−1
Nk(s,a)/summationdisplay
0<τ<t k||rτ(s,a)||11(sτ=s,aτ=a)/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle≤dEk(s,a)
Nk(s,a)(32)
Using Assumption 1(b), we can show that,
E/bracketleftigg
1
Nk(s,a)/summationdisplay
0<τ<t k||rτ(s,a)||11(sτ=s,aτ=a)/bracketrightigg
=r(s,a) (33)
Therefore, the following sequence of inequalities can be derived.
Pr/braceleftigg
|ˆrk(s,a)−r(s,a)|>dEk(s,a)
Nk(s,a)+/radicaligg
7 log(2SAtk/δ)
2Nk(s,a)/bracerightigg
(a)
≤Pr/braceleftigg/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle1
Nk(s,a)/summationdisplay
0<τ<t k||rτ(s,a)||11(sτ=s,aτ=a)−r(s,a)/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle>/radicaligg
7 log(2SAtk/δ)
2Nk(s,a)/bracerightigg
(b)
≤/summationdisplay
0<n<t kPr/braceleftigg/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle1
nn/summationdisplay
l=1||rτl(s,a)||1−r(s,a)/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle>/radicalbigg
7 log(2SAtk/δ)
2n/bracerightigg
(c)
≤/summationdisplay
0<n<t k2δ
120SAt7
k=δ
60SAt6
k
Inequality (a)is a consequence of (32)while (b)follows from the union bound. Finally, (c)utilizes Hoeffding’s
inequality together with Assumption 1(a) and 1(c). Conjoining (27)and the above result, we establish the
lemma.
16