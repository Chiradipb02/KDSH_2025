Under review as submission to TMLR
U-Turn Diffusion
Anonymous authors
Paper under double-blind review
Abstract
We present a comprehensive examination of score-based diffusion models of AI for gener-
ating synthetic images. These models hinge upon a dynamic auxiliary time mechanism
driven by stochastic differential equations, wherein the score function is acquired from in-
put images. Our investigation unveils a criterion for evaluating efficiency of the score-based
diffusion models: the power of the generative process depends on the ability to de-construct
fast correlations during the reverse/de-noising phase. To improve the quality of the pro-
duced synthetic images, we introduce an approach coined "U-Turn Diffusion". The U-Turn
Diffusion technique starts with the standard forward diffusion process, albeit with a re-
duced duration compared to conventional settings. Subsequently, we execute the standard
reverse dynamics, initialized with the concluding configuration from the forward process.
This U-Turn Diffusion procedure, combining forward, U-turn, and reverse processes, creates
a synthetic image approximating an independent and identically distributed (i.i.d.) sample
from the probability distribution implicitly described via input samples. To analyze relevant
time scales we employ various analytical tools, including auto-correlation analysis, weighted
norm of the score-function analysis, and Kolmogorov-Smirnov Gaussianity test. The tools
guide us to establishing that analysis of the Kernel Inception Distance, a metric comparing
the quality of synthetic samples with real data samples, reveals the optimal U-turn time.
1 Introduction
The fundamental mechanics of Artificial Intelligence (AI) encompass a three-step process: acquiring data,
modeling it, and then predicting or inferring based on the constructed model. Culmination of this process
is the generation of synthetic data, which serves as a core component of the prediction step.
Synthetic data holds the potential to augment information in various ways. It achieves this by leveraging
model-derived conjectures to enrich the data’s complexity and structure. In particular, Score Based Diffusion
(SBD) models Song et al. (2021); Rombach et al. (2022); Ho et al. (2020) have emerged as a highly successful
paradigm in this context. The foundation of the SBD models success rests on the notion that their inherent
structure extracts a substantial amount of information from the data.
The essence of SBD models is deeply rooted in the concepts that the reality represented via data can emerge
from noise or chaos, suggesting a process akin to de-noising (reverse part of the SBD dynamics), and that
the introduction of diffusion can disrupt existing order within data (direct part of the SBD dynamics).
These fundamental principles underlie the audacious approach of building generative models upon these
very principles.
While the achievements of SBD models are impressive, they are not universally successful. Instances where
barriers are significant, referred to colloquially in physics jargon as "glassy" scenarios Charbonneau et al.
(2023), may necessitate the graceful addition of diffusion and bias/advection possibly nonlinear (in the state
space), compelling the extension of SBD model runtime for better performance.
Our overarching objective revolves around gaining insights into existing successful SBD models and further
enhancing their capabilities. We methodically approach this goal by breaking it down into steps. However, in
this manuscript our primary focus resides not in refining the diffusion component of the model. Instead, we
presume this component as given to us, as already developed and documented in prior works (e.g., Song et al.
1Under review as submission to TMLR
(2021); Rombach et al. (2022); Ho et al. (2020)). Then our attention centers on comprehending temporal
correlationswithinboththediffusionprocess(forwardpartofSBD)andthedenoising/reconstructionprocess
(reverse part of SBD).
A pricipal outcome of our analysis of temporal correlations is a fundamental realization concerning the
optimal termination point of the forward process, i.e. of the U-Turn point. This culminates in the proposal
of a novel algorithm termed "U-Turn Diffusion." This algorithm provides guidance on when to pivot from
the direct to the reverse process. Moreover, we naturally initialize the reverse process after the U-turn with
the last configuration of the forward process.
In summary, this manuscript presents a comprehensive exploration of the dynamics of SBD models, delving
into details of the temporal correlations that underpin their success. Our insights not only enhance the
understanding of these models but also lay the foundation for the development of novel techniques, such as
the U-Turn Diffusion algorithm, which promises to further elevate the capabilities of SBD-based generative
modeling.
The manuscript is structured as follows: In Section 2, we provide a technical introduction, laying the
foundation by outlining the construction of SBD models. Section 3 forms the first original contribution of
this work, encompassing an extensive correlation analysis. We delve into two-time auto-correlation functions
of the SBD, establishing relevant time scales. Additionally, we identify the emergence of similar time scales in
single-timetestsof(a)theaverage2-normofthescore-functionand(b)theKolmogorov-Smirnovcriterionfor
Gaussianity. This section reaches its climax with the proposal of the U-Turn diffusion algorithm, discussed
in Section 4. Our manuscript concludes by summarizing findings and outlining future directions for research
in Section 6.
2 Technical Introduction: Setting the Stage
Within this manuscript, we embrace the Score-Based Diffusion (SBD) framework, as expounded in Song
et al. (2021). The SBD harmoniously integrates the principles underlying the "Denoising Diffusion Proba-
bilistic Modeling" framework introduced in Sohl-Dickstein et al. (2015) and subsequently refined in Ho et al.
(2020), along with the "Score Matching with Langevin Dynamics" approach introduced by Song & Ermon
(2019). This seamless integration facilitates the reformulation of the problem using the language of stochastic
differential equations, paving the way to harness the Anderson’s Theorem Anderson (1982). As elucidated
in the following, this theorem assumes a principal role in constructing a conduit linking the forward and
reverse diffusion processes.
Let us follow Song et al. (2021) and introduce the forward-in-time Stochastic Ordinary Differential Equation
(SDE):
dxt=f(xt,t)dt+G(xt,t)dwt, (1)
and another reverse-in-time SDE:
dxt=/parenleftbig
f(xt,t)−∇.(G(xt,t)G(xt,t)T)−G(xt,t)G(xt,t)T(∇xtlog (pt(xt)))/parenrightbig
dt+G(xt,t)d¯wt,(2)
where the drift/advection f:Rnx×R→Rnxand diffusion G:Rnx×R→Rnx×Rnxare sufficiently
smooth (Lipschitz functions). Additionally, we assume the existence of a well-defined initial distribution p0(·)
represented by data (samples), and both forward and backward processes are subject to Ito-regularization.
The Wiener processes wtand ¯wtrepresent standard Wiener processes for forward and reverse in time,
respectively.
Anderson’s theorem establishes that the forward-in-time process and the reverse-in-time process have the
same marginal probability distribution, denoted by pt(·).
Remark. The proof of Anderson’s Theorem relies on the equivalence of the Fokker-Planck equations derived
for the direct (1) and inverse (2) dynamics:
2Under review as submission to TMLR
∂tpt(x)−∇ x(f(x,t)pt(x)) =1
2∇x/parenleftbig
G(x,t)G(x,t)T∇xpt(x)/parenrightbig
,
∂tpt(x)−∇ x(f(x,t)pt(x)) +∇x/parenleftbig
G(x,t)G(x,t)T(∇xlog (pt(x)))/parenrightbig
pt(x) =−1
2∇x/parenleftbig
G(x,t)G(x,t)T∇xpt(x)/parenrightbig
,
wherept(·)is the marginal probability distribution of xwhich is equivalent for the forward and inverse
processes (by construction).
The forward diffusion process transforms the initial distribution p0(·), represented by samples, into a final
distribution pT(·)at timeT. The terms f(x,t)andG(x,t)in the SDE are free to choose, but in the SBD
approach, they are selected in a data-independent manner such that pT(·)converges to π(·) =N(·,0,I)as
Tapproaches infinity. This convergence ensures that the generated samples align with a target distribution,
typically the standard normal distribution N(·,0,I).
Inference , which involves generating new samples from the distribution represented by the data, entails
initializing the reverse process (2) at t=T(large but finite) with a sample drawn from π(·), and then
running the process backward in time to reach the desired result at t= 0. This operation requires accessing
the so-called score function s(x,t) = (∇xlog(pt(x)), as indicated in Eq. (2). However, practically obtaining
the exact time-dependent score function is challenging. Therefore, we resort to approximating it with a
Neural Network (NN) parameterized by a vector of parameters θ:sθ(x,t)≈s(x,t).
The neural network-based approximation of the score function allows us to efficiently compute and utilize
gradients with respect to the input data xat different times t, which is essential for guiding the reverse
process during inference. By leveraging this neural network approximation, we can effectively sample from
the desired distribution and generate new images which are approximately i.i.d. from a target probability
distribution represented by input data. This approach enables us to achieve reliable and accurate inference
in complex high-dimensional spaces, where traditional methods may struggle to capture the underlying data
distribution effectively.
Training: The neural network sθ(xt,t)can be trained to approximate the score function ∇xtlogpt(xt)using
the weighted De-noising Score Matching (DSM) objective Song et al. (2021):
LDSM (θ,λ(·)) :=1
2Et∼U(0,T),
x0∼p0(x0),
xt∼∼pt(xt|x0)[λ(t)∥∇xtlogpt(xt|x0)−sθ(xt,t)∥2
2], (3)
This approach offers significant advantages over alternative methods, such as those described in Hyvärinen
(2005); Vincent (2011), due to the analytical evaluation of pt(xt|x0)as an explicit function of x0for various
simple drift and diffusion choices in the forward SDE. The objective function LDSMleverages the score
matching technique to ensure that the gradients estimated by the neural network closely align with the true
gradients of the log-likelihood. The weight function λ(t)allows us to assign varying importance to different
time points during training, offering further flexibility in optimizing the neural network’s performance.
In the two subsequent subsections, we will explore the freedom in selecting the drift/advection and diffusion
terms in the forward SDE, as well as the implications of choosing specific weight functions λ(t). This analysis
will provide valuable insights into the overall performance of the Score-Based Diffusion (SBD) framework
and its impact on improving the scheme.
2.1 Variance Preserving SDE
In this manuscript we focus on time inhomogeneous Ornstein Uhlenbeck Stochastic Differential Equations
(SDEs)knownasVariancePreserving(VP)StochasticDifferentialEquations(VP-SDEs)thatpossessclosed-
form solutions. The capability to uncover a closed-form solution is essential for learning efficiency, as it allows
for the evaluation of each sample just once, encompassing automatically all potential paths within the SDE
that adhere to the initial condition set by the sample. We achieve this by choosing specific drift and diffusion
3Under review as submission to TMLR
functions as follows:
f(xt,t) =−1
2β(t)xt, G (xt,t) =/radicalbig
β(t). (4)
This choice results in the following form of the VP-SDE (1) supplemented by the initial conditions:
dxt=−1
2β(t)xtdt+/radicalbig
β(t)dwt,x0∼pdata. (5)
Here,β(t)is a positive function, often referred to as the noise scheduler. We chose this specific form for
the drift and diffusion functions based on considerations of simplicity and historical context. Linearity in
Eq. (5) was a practical consideration that allows us to express xtanalytically in terms of x0. The affine drift
inxandx-independent diffusion, as opposed to more general linear forms in xfor both, were inherited from
their original discrete-time counterparts.
The original discrete version of the VP-SDE was given by:
xn=/radicalbig
1−bnxn−1+/radicalbig
bnzn−1,zn∼N(0,I), (6)
wheren= 1,···,N. By introducing β.=b/∆, t.= ∆n, T.= ∆N, and taking the limit: N→∞,∆→0,
Eq.(6) transforms into Eq.(5).
The solution to Eq. (5) is given by:
xt=/radicalbig
Φ(t,0)x0+/integraldisplayt
0/radicalbig
Φ(t,s)β(s)dws, (7)
where Φ(t,s) =e−/integraltextt
sβ(u)duandxtconditioned on x0is Gaussian:
p(xt|x0) =N/parenleftbigg
xt;/radicalbig
Φ(t,0)x0,(1−Φ(t,0))ˆI/parenrightbigg
. (8)
For completeness, we present a set of useful formulas derived from Eq. (8) that describe correlations within
the forward process:
E[xt] =/radicalbig
Φ(t,0)E[x0], (9)
E[x2
t] = Φ(t,0)E[x2
0] +/integraldisplayt
0Φ(t,s)β(s)ds= Φ(t,0)/parenleftbig
E[x2
0]−1/parenrightbig
+ 1, (10)
V[xt] =E[x2
t]−E[xt]2= 1−Φ(t,0), (11)
E[xtxs] =/radicalbig
Φ(ξ,γ)E[x2
γ], ξ = max(t,s), γ= min(t,s) (12)
C0(t).=E[x0xt]
E[x2
0]=/radicalbig
Φ(t,0), (13)
CT(t).=E[xTxt]
E[x2
T]=/radicalbig
Φ(T,t)E[x2
t]
E[x2
T], (14)
where E[···]andV[···]represent expectations and variances over the forward VP process (5). By shifting
and re-scaling the initial data to ensure that E[x0] = 0andE[x2
0] = 1, we find from Eqs. (9, 10, 11) that
E[xt] = 0, and xtindeed becomes Variance Preserving (VP), as the name suggests, since E[x2
t] = 1.
It is important to note that while the direct process (5) only depends on x0through the initial condition,
resulting in the explicit solution (7), the respective reverse process described by Eq.(2), with fixed f(·)and
G(·)according to Eq.(4), carries information about x0through the score function.
4Under review as submission to TMLR
2.2 Re-weighting in the Score Function Training
In the context of Eq. (3), choosing an appropriate weight function λ(t)is crucial. While there are various
options for λ(t)(as discussed in Karras et al. (2022); Kingma & Gao (2023) ), we adopt in this work the
approach introduced in Song & Ermon (2019). Specifically, we substitute:
λ(t)→1−Φ(t,0). (15)
This choice of λ(t)is well-motivated as it accounts for the scaling with the x0-independent part of the
conditional probability described by Eq. (8). To elaborate, we estimate the term involving λ(t)in the
objective function as follows:
λ(t)|∇xtlogpt(xt|x0)−sθ(xt;t)|2∼λ(t)|∇xtlogpt(xt|x0)|2∼λ(t)|xt−/radicalbig
Φ(t,0)x0|2
(1−Φ(t,0))2∼λ(t)
1−Φ(t,0),
This suggests that by choosing λ(t)according to Eq. (15), we equalize the contributions from different time
steps into the integration (expectation) over time in the objective function (3). This ensures a balanced
influence from all time steps, making the learning process more effective and efficient.
By incorporating the selected λ(t), our approach successfully captures the inherent characteristics of the
VP-SDE solution and leverages them for generative modeling.
3 Numerical Experiments in the Standard Setting
In this section, we present the results of our numerical experiments, which involve different direct and reverse
processes defined in Eq.(5) (or Eq.(7)) and Eq. (2), with the specific choices of Eq. (4). Additionally, we
explore various profiles for the function b(n), which are introduced in the following. These profiles allow
us to test the sensitivity and effectiveness of the methods under varying balance between advection and
diffusion. By systematically exploring these setups, we gain valuable insights into the generative capabilities
and limitations of the models based on the VP-SDE formulation.
The experimental findings presented below shed light on the interplay between the direct and reverse pro-
cesses, revealing how they collectively contribute to the overall generative performance, then suggesting how
to improve the process.
3.1 Profiles of b(discrete version of β)
Profile ofb Definition
Linear Ho et al. (2020) b(n) = (b2−b1)n
N+b1,n∈[0,N]
Cosine Nichol & Dhariwal (2021) b(n) = min/parenleftig
1−p(n)
p(0)−p(n),0.999/parenrightig
, p(n) = cos2/parenleftig
n/N+0.008
1+0.008.π
2/parenrightig
Sigmoid Xu et al. (2022) b(n) =b2−b1
1+exp(−12n/N+6)+b1,n∈[0,N]
Table 1: Profiles of b(discrete version of βassociated with the discretization of time tusing integer n, where
t=n∆) commonly employed in previous studies Ho et al. (2020); Nichol & Dhariwal (2021); Xu et al. (2022),
and also adopted for our investigation. For our discrete-time simulations (as discussed in the following), we
setb1= 0.0001andb2= 0.02, and consider integer values of nin the range 0,1,···,N= 1000. These
profiles have been selected to enable a comprehensive assessment of our approach and are representative of
the noise conditions commonly used in the literature.
Fig.(1) displays the mean and standard deviation (square root of variance) of the direct process samples xn,
which are/radicalbig
Φ(n∆,0)and/radicalbig
1−Φ(n∆,0)respectively, as described by Eq.(8). These results are presented
for three distinct Variance Dampening (VD) profiles of b(n)– linear, sigmoid, and cosine – as outlined in
Table 1.
5Under review as submission to TMLR
0 200 400 600 800 1000
n0.00.20.40.60.81.0√Φ( n Δ, 0)Linear
Sigmoid
cos
(a)
0 200 400 600 800 1000
n0.00.20.40.60.81.0√1 − Φ( n Δ, 0)
Linear
Sigmoid
cos (b)
Figure 1: Mean and standard deviation of the direct process samples xtfor three different Variance Damp-
ening (VD) protocols of b(n)– linear, sigmoid, and cosine. The x-axis represents time t, and they-axis
corresponds to the mean and standard deviation of the direct process samples xt.
The original design of b(n)was motivated by the desire to achieve a smooth transition forward in time from
the part of the dynamics, xn, that retains information about the initial condition x0(dominated by the drift
or ballistic dynamics) to a phase where this information is gradually forgotten (dominated by diffusion).
This trend is clearly evident in Figs.(1a,b), in line with the behavior described by Eqs.(9) and (11). Among
the three VD profiles, the linear one exhibits the most rapid decrease/increase in drift/diffusion over time,
while the cosprofile results in a more gradual and slower transition.
3.2 Samples of the forward and backward processes
Here we explore both the forward process (x(t)|t∈[0,T])and the reverse process (xr(t)|t∈[0,T]). For
consistency and clarity, we use the same notation for time in both processes, following a forward counting
scheme. In our simulations, we discretize time, taking values in the range 0,···,T= 1000to facilitate
numerical computations and analysis.
Figures (2) and (3) showcase temporal samples of both the forward and reverse processes, each corresponding
to the three different profiles of b. Notably, we observe that the linear and cosine profiles exhibit the most
rapid and gradual decay, respectively, among the three options.
3.3 Auto-Correlation Functions of the Reverse Process
In our analysis of the Score-Based-Diffusion (SBD) method, we conduct computations by averaging (comput-
ing expectations) over multiple samples of the stochastic processes. Our focus is on studying auto-correlation
functions, as they serve as direct indicators of how the processes retain or discard information over time.
The auto-correlation functions of the forward process are fully described in Eqs.(13) and (14). Therefore, nu-
merical experiments for the forward process serve primarily as a sanity check, since the analytical expressions
are available. However, for the reverse VP process, described by Eq.(2) with drift and diffusion functions
according to Eq.(4), no analytical expressions are available for the auto-correlation functions. Consequently,
we primarily investigate these auto-correlation functions numerically.
6Under review as submission to TMLR
(a) Linear profile
(b) Sigmoid profile
(c) Cosine profile
Figure 2: Temporal samples of the forward process xnshown for three different noise profiles described in
Table 1. Each figure represents a distinct noise profile, demonstrating the dynamic behavior of the process
over time.
(a) Linear profile
(b) Sigmoid profile
(c) Cosine profile
Figure 3: Temporal samples of the reverse process for three different noise profiles described in Table 1.
Each figure corresponds to a distinct noise profile, providing a representation of the dynamic behavior of the
reverse process over time.
Specifically, we study the auto-correlation functions of the reverse process between the current time τand
an earlier or later reference time t:
Ct;r(τ) =E[xr(t)xr(τ)]
E[(xr(t))2], (16)
7Under review as submission to TMLR
whereτcan be smaller or larger than t. These auto-correlation functions provide valuable insights into the
behavior of the reverse process and its ability to recall information from earlier time steps, contributing to
a comprehensive understanding of the generative capabilities of the SBD method.
0 200 400 600 800 1000
 Time Steps0.00.20.40.60.81.0Auto Correlationlinear
sigmoid
cosine
(a)C0(t=n∆), defined in Eq. (13), vs n
0 200 400 600 800 1000
 Time Steps0.00.20.40.60.81.0Auto Correlation
linear
sigmoid
cosine (b)CT(t=n∆), defined in Eq. (14), vs n
Figure 4: Auto correlation for forward process.
0 200 400 600 800 1000
 Time Steps0.00.20.40.60.81.0Auto Correlationlinear
sigmoid
cosine
(a)C0;r(t=n∆), defined by Eq. (16) evaluated at t= 0, vsn
0 200 400 600 800 1000
 Time Steps0.00.20.40.60.81.0Auto Correlationlinear
sigmoid
cosine(b)CT;r(t=n∆), defined by Eq. (16) evaluated at t=T, vsn
Figure 5: Auto correlation for reverse process.
The auto-correlation function analysis results for three different advection/diffusion (noise) profiles are pre-
sented in Fig.(4) and Fig.(5) for the forward and reverse processes, respectively. These findings yield several
important observations:
1. Theauto-correlationfunctionsdemonstratecleardifferencesamongthevariousprocesses, supporting
thenotionofusingauto-correlationasanindicatorof"correlation"decay, i.e., howquicklythesignals
losecorrelation(information)overtime. Amongthethreeforwardprocesses, the"linear"and"cosine"
profiles exhibit the fastest and slowest decay of correlations, respectively, which is consistent with
the temporal evolution of samples shown in Fig. (2).
2. Although correlations between t= 0and subsequent times are destroyed/reconstructed similarly in
both the forward and reverse processes, the correlations between t=Tand preceding moments of
8Under review as submission to TMLR
time are remarkably different. Specifically, the T-referenced auto-correlation function of the reverse
process,CT;r(t), decays much faster with decreasing tcompared to the auto-correlation function of
the forward process, CT(t). This observation indicates that while the forward process retains all
the original-sample-specific information in the initial conditions, the reverse process transforms this
information into the "advection" term, spreading the information over time.
3. Moreover, the decay of correlations in the reverse process counted from Tis the fastest for the
cosine profile. This finding implies that by engineering the forward process (independent of the
initial conditions), we can achieve a faster or slower decay of correlations in the reverse process.
4. Furthermore, the dramatic decay of correlations in the reverse process, as observed in CT;r(t),
indicates that the information contained in x(T)is rapidly forgotten. To quantify this behavior, we
study the 1/2-decay correlation time δ(t), defined by
Ct;r(t∓δ(t)) = 1/2, (17)
and its dependence on t, which is illustrated in Fig. (6).
In summary, these observations shed light on the distinctive behaviors of the forward and reverse processes,
providing valuable insights into their information retention capabilities and temporal characteristics. The
correlation decay analysis offers a deeper understanding of the generative dynamics underlying the SBD
method.
While analyzing the correlation function provides a direct means to assess information retention during the
forward and reverse processes, it lacks temporal locality. This absence of temporal locality poses challenges
when incorporating auto-correlation functions into the algorithm(s), in particular the U-turn diffusion algo-
rithm introduced in discussed in Section 4 below. Consequently, it motivates us to explore two alternative,
time-local features in the upcoming two subsections.
0 200 400 600 800 1000
n050100150200250300350δ(n/uni0394Δ//uni0394Linear
Sigmoid
Cosine
(a)
0 200 400 600 800 1000
n0100200300400500600(n)/
Linear
Sigmoid
Cosine (b)
Figure 6: Dependence of the 1/2-correlation time (discrete time version) δ(n∆)/∆of the reverse process, as
defined in Eq. (17), on the (discrete) time index, n, forτ <t, in (a), and for τ >t, in (b).
9Under review as submission to TMLR
0 200 400 600 800 1000
n = t//uni039410Δ210Δ1100S (t = /uni0394 n )linear
sigmoid
cosine
(a)S(t= ∆n), defined in Eq. (18) vs n=t/∆.
0 200 400 600 800 1000
n = t//uni039410Δ210Δ1100M (t = /uni0394 n )
linear
sigmoid
cosine (b)M(t= ∆n), defined in Eq. (19) vs n=t/∆.
Figure 7: Average of the Score Function 2-Norm for the experiments described in the main text.
3.4 Average of the Score Function 2-Norm
The analysis of the time-dependence of the average of the score function (a vector) 2-norm is presented in
Fig. (7a). This score function is denoted as:
S(t).=/radicaltp/radicalvertex/radicalvertex/radicalvertex/radicalbtE/bracketleftig
(∇xlogpt(x))2/bracketrightig
E/bracketleftig
(∇xlogp0(x))2/bracketrightig, (18)
In Fig. (7b), we present the average score function 2-norm, weighted with the/radicalbig
λ(t)-factor, and normalized
att=Taccording to:
M(t).=/radicaltp/radicalvertex/radicalvertex/radicalvertex/radicalbtλ(t)E/bracketleftig
(∇xlogpt(x))2/bracketrightig
λ(T)E/bracketleftig
(∇xlogpT(x))2/bracketrightig. (19)
These figures illustrate how the score function norms, weighted and not, evolve over time, offering valuable
insights into the generative modeling process and the relevance of the score function in capturing the under-
lying dynamics. The figures help to appreciate how the weighted score function provides a means to evaluate
the importance of different time steps in the generative learning process. Specifically, we observe in Fig. 7,
that the “mixed” regime establishes more rapidly: at n≈500for the linear profile, n≈600for sigmoid
profile, and n≈800for cosine profile. Notably, the score-function dynamics of the sigmoid profile also
exhibit a marked shift at an earlier time of n≈100. We also note that the “correlation” times observed in
the score-function profiles for different noise configurations approximately align with the “correlation” times
depicted in Fig. 6b, where we analyzed the variation of the 1/2-correlation decay (for the reversed process,
counted forward in time) in the auto-correlation function with respect to changes in the reference time.
3.5 Kolmogorov-Smirnov Gaussianity Test
We employ the Kolmogorov-Smirnov (KS) Gaussianity test to examine the null hypothesis: "Is a single-
variable marginal of the resulting multi-variate distribution of xr(t)at a given time tGaussian or not?"
To validate this hypothesis, we apply the KS test to each of the single-variable marginals pt(xk) =/integraltext
d(x\
xk)pt(x). The KS-ratio is then calculated as the number of single dimensions kfor which the Gaussianity
10Under review as submission to TMLR
0 200 400 600 800 1000
n=t/
0.00.20.40.60.81.0KS(t=n)
Linear forward
Linear reverse
Sigmoid forward
Sigmoid reverse
Cosine forward
Cosine reverse
Figure 8: Kolmogorov-Smirnov test for forward and reverse dynamics under different b-protocols.
of the corresponding xk(t)is not confirmed, divided by the total number of dimensions (cardinality of x(t)):
KS(t) =# ofpt(xk)failing the Gaussianity test
cardinality of x= 64×64×3.
The results of this analysis are displayed in Fig. 8.
Firstly, we note analyzing Fig. 8 that the “correlation” times from the KS profiles of the reverse process
(across different noise schedules) show a close alignment with those presented in Fig. 6 for the shifts in the
1/2-correlation decay of the auto-correlation function in relation to variations in reference time.
Secondly, we notice that there’s a noticeable misalignment in Fig. 8 between the forward and reverse KS
curves. Theoretically, when considering appropriate limits, these curves should coincide. Several factors
contribute to the score-function based estimation of the probability distribution constructed from the data,
resulting in misalignment. Firstly, the Neural Network is only an approximate fit to the score function it
approximates. Secondly, although the "reverse" Fokker-Planck equation is exact, ensuring the same marginal
probability distribution as derived in the forward process, this exactness holds only in continuous time,
while the actual implementation occurs in discrete time. Lastly, the number of input (data) samples, while
assumed to be large, is still finite. All three errors accumulate, leading to the observed mismatch between the
KS tests performed for the forward and reverse processes, as shown in Fig. (8). This discrepancy aligns with
earlier observations and discussions reported in De Bortoli et al. (2021); Block et al. (2022). Additionally, we
find that the mismatch in the KS-curves diminishes with improvements in the quality of the Neural Network,
reduction of the number of discretization steps, and/or an increase in the number of input samples. These
insights highlight the importance of these factors in refining the generative modeling process and reducing
discrepancies between forward and reverse processes.
We opted for the KS test over the KL test for several reasons. Firstly, KL divergence, a measure of dis-
similarity between distributions, is asymmetric and not truly a distance metric, whereas the KS test, which
assesses the maximum separation between a test distribution and a reference distribution, is symmetric.
Secondly, the KS test is non-parametric, meaning it doesn’t depend on the parameters of the benchmark dis-
tribution (in our case, Gaussian), which would be a concern with KL divergence. Thirdly, KL divergence is
more advantageous when comparing distributions with nontrivial tails, which isn’t the focus of our analysis.
Lastly, the KS test is computationally simpler.
3.6 Quality of Inference
Next we employ the Kernel Inception Distance (KID) Bińkowski et al. (2021) as our chosen metric to assess
the quality of inference. KID measures the dissimilarity between the distributions of real and generated
samples without assuming any specific parametric form for them. The KID is constructed by embedding
11Under review as submission to TMLR
0 200 400 600 800 1000
 Time Steps0.10.20.30.40.50.60.7KID
linear
sigmoid
cosine0 10 200.150.200.25
Figure 9: Kernel Inception Distance (KID) for the reverse process with different b-protocols displayed as a
function of n. The inset provides a closer view of the primary trend at the lowest nvalues, where the lowest
KID values (best results) are attained. (We remind that the synthetic sample is generated at the end of the
reverse process, i.e., at n= 0.)
real and generated images into a feature space using the Inception network Gretton et al. (2012). It then
becomes a squared Maximum Mean Discrepancy (MMD) between the inception features of the real and
generated images:
KID(Pr,Pg) =Exr,x′
r∼Pr,
xg,x′
g∼Pg/bracketleftbigg
k(xr,x′
r) +k(xg,x′
g)−2k(xr,xg)/bracketrightbigg
where PrandPgrepresent the distributions of real and generated samples, respectively and k(x,y) =
(x⊤y+ 1)3is a polynomial kernel. The KID quantifies the distance between the two distributions, with a
lower KID indicating that PrandPgare closer to each other. In our evaluation, we use a polynomial kernel
to calculate the KID.
Fig. 9 presents the results of our KID tests for different profiles of b. Notably, we observe that the cos-profile
yields a lower KID, indicating better quality of generated samples compared to other profiles.
Itisworthmentioningthatanotherpopularmeasureofsimilaritybetweentwodatasetsofimages, theFrechet
Inception Distance (FID) Heusel et al. (2018), is often used to evaluate the quality of generated samples.
However, FID is not applicable in our setting, as we intentionally work with a number of images smaller than
the dimensionality of the input vector. In our experiments, the input vector has a dimension of 2048(after
passing through the Inception-v3), but we use only 1000samples to estimate the covariance. Consequently,
the covariance matrix becomes singular, making FID unsuitable for our evaluation. Therefore, we rely on
KID as a robust alternative to assess the performance of our generative modeling approach.
3.7 Discussion
In this section, all the experiments presented and discussed were conducted under the "standard" setting of
the Score-Based Diffusion (SBD). As a reminder, the standard setting involves training the score function
on input data propagated with forward stochastic dynamics (1) from time 0toT. Subsequently, synthetic
data emerge through the propagation of an initially noisy (completely uninformative) image by the reverse
stochastic process (2), which depends on the score function.
Our analysis of the standard setting reveals that generating high-quality synthetic data, and potentially
enhancing their quality, does not necessitate initiating the inverse process at time T. This conclusion is
supported by our examination of the auto-correlation function in the reverse process, as shown in Fig.(5),
andthe1/2-correlationtime, asillustratedinFig.(6). Bothfiguresindicatethattheearlystagesofthereverse
process do not contribute significantly to generating a synthetic image. Specifically, Fig.(6) demonstrates
12Under review as submission to TMLR
(a) linear
(b) sigmoid
(c) cosine
Figure 10: Synthetic images generated at n= 0by making U-turn at different times from the [0,1000]range.
that correlations start forming not at n= 1000, but rather at n≈200for linearb-profiles and at n≈400
for sigmoid- and cosine- b-profiles. Visual inspection of samples from the reverse dynamics, as displayed
in Fig.(3), aligns with these findings. Similar observations and time scales are also evident in our reverse
process KID score test, depicted in Fig. (9).
Considering the role of the score function itself as a function of time, which is extracted from the evolving
data in the direct process, we inquire if it indicates when a synthetic image begins to form. Experimental
evidence from Fig. (7b), depicting the evolution of the properly normalized norm of the score function,
13Under review as submission to TMLR
100 200 300 400 500 600 700 800 900 1000
 T ime Steps0.040.060.080.100.120.140.160.18KIDlinear
cosine
sigmoid
Figure 11: KID score plotted against nfor synthetic images generated with the U-turn conducted at the
stepn. Notably, we observe a distinct increase in the rate of KID versus ngrowth at certain values of n,
which we identify as the optimal positions for conducting the U-turn.
Figure 12: KID score for synthetic images generated (at n= 0) by initiation of the reverse process at the time
nwith a random noise. We show KID as a function of the time nof the reverse process random initiation.
indicates that the score function ceases to change at n≈600for linear- and sigmoid- b-profiles, and at
n≈800for cosine-b-profiles.
Furthermore, the results of the KS test of Gaussianity in Fig. (8b) suggest that the reverse processes with
linear-, sigmoid-, and cosine- b-profiles become largely Gaussian (proxy for uninformative) at n≳600,
n≳700, andn≳900, respectively. These findings collectively demonstrate that initiating the reverse
process earlier than n≈600for linear- and sigmoid- b-profiles, and n≈800for cosine-b-profiles, does not
significantly impact the quality of synthetic data.
Based on the findings discussed above, we have made significant advancements in our proposal for generating
synthetic samples, which will be further elaborated in the following Section.
4 U-Turn Diffusion: Set Up and Numerical Experiments
We propose running a direct process, but with a shorter duration compared to the standard setting. Instead,
we reverse the approach earlier, initiating the process in the opposite direction. We initialize the reverse
14Under review as submission to TMLR
process using the last configuration from the forward process. This entire process, that is direct and reverse
combined, is termed U-Turn Diffusion , emphasizing our expectation that direct process followed by U-Turn
and the reverse process will ultimately produce a synthetic image. This synthetic image should, on one hand,
closely resemble samples from the probability distribution representing the input data. On the other hand,
it should be distinctly different from the original sample that initiated the preceding direct process when it
arrives att=n= 0.
Algorithm 1 U-Turn
Require:∇xlogp(x,t),x0∼pdata
1. PlotS(t)using Eq. 18
2. FindTwhereS(t)starts to flatten
3. Calculate xTusing Eq. 7
4. Initialize Eq. 2 at t=TwithxT
5. Run the dynamaics until t= 0
The U-turn algorithm, as outlined in Algorithm 1, operates through a structured step-by-step process. It
takes as input a dataset and a score function, which can be derived from either training or pre-trained
models. To determine the ideal moment to execute the U-turn, we initiate the process by plotting the norm
of the score function in the second step. This allows us to identify the point at which the score function
begins to stabilize. Once this stabilization point is determined, we proceed to initialize the reverse dynamic
at this moment using a noisy version of the actual images (as described in steps 3 and 4). Subsequently, we
run the dynamics until it reaches the time t= 0, as outlined in step 5.
Synthetic images generated at n= 0by making the U-turn at different times are shown in Fig. (10) for the
threeb-profiles. Consistently with the discussion above in Section 3.7 we observe, by examining the figures
visually, that a principally new image of a high quality is generated if the U-turn occurs at n≳600for the
linear-, sigmoid- b-protocols and at n≳850for the cosine- b-protocols.
The KID score, which compares synthetic images generated at n= 0with the original data, is analyzed as
a function of the U-turn time and presented in Fig. (11). The results displayed in the figures corroborate
the observations made in Fig. (10). Notably, Fig. (11) reveals a significant finding – when the U-turn time
surpasses an optimal threshold ( n≈600for linear and sigmoid b-profiles, and n≈850for cosineb-profiles),
the deterioration in the synthetic image quality accelerates considerably with increasing nas compared to
lower values. (Obviously, conducting a U-turn at a sufficiently small nyields synthetic images at n= 0that
closely resemble the original images, resulting in a minimal KID.) In light of these observations, we deduce
that these critical values of n, signifying a more rapid increase in KID with higher n, represent the optimal
choices for the U-turn.
Fig. (12) showcases the outcomes of experiments analogous to those described earlier (leading to the results
presented in Fig.(11)), however initiating the reverse process with a noise in this case. Evidently, in this
case the reverse process does not retain any memory of the forward process, thus leading to increase in
KID with decrease in n, wherenis the time step where we initialize the reverse process with the noise.
Notably, the dependence of the KID on nflattens asndecreases. The flattening occurs around n≈600for
linear and sigmoid b-profiles, and around n≈900for the cosine b-profile. This observation suggests that
for random initialization of the reverse process, starting the process at n= 1000is unnecessary. Instead, it
is advantageous to start the reverse process at a smaller n, chosen based on the b-profile. Remarkably, a
comparison between Fig.(11) and Fig.(12) underscores a notable advantage in initiating the reverse process
with the final configuration from the forward process preceding the U-turn. This approach yields a marked
reduction in KID, translating to an elevated quality of the synthetic image. For instance, examining Fig.(11),
we find that the KID value for the sigmoid- b-profile process, with the U-turn executed at n= 600(deemed
the optimal U-turn point as discussed earlier), is approximately 0.07. In contrast, Fig.(12) demonstrates
that random initiation of the reverse process at n= 600leads to a significantly higher KID of about 0.19.
Two remarks are in order:
15Under review as submission to TMLR
Remark. #1Upon the recommendations of reviewers, we expanded our experiments from the Butterfly
dataset (detailed in Appendix A) to encompass the CIFAR-10 dataset, with specifics outlined in Appendix
B. The key takeaway from the CIFAR-10 evaluation is that our empirical findings, both regarding the basic
and U-turn diffusion analyses, hold consistent with those observed in the Butterfly dataset.
Remark. #2After completing this work, we discovered a related method called "boomerang," which was
recently reported in Luzi et al. (2022). While there are some similarities between the boomerang and the U-
turn diffusion described in this manuscript, it is essential to emphasize that they are distinct from each other.
The boomerang method focuses on generating images that closely resemble the original samples, whereas the
U-turn diffusion aims to create distinctly different images that approximate i.i.d. samples from the entire
dataset. This fundamental difference also manifests in the applications suggested for the boomerang in Luzi
et al. (2022), such as constructing privacy preserving datasets, data-augmentation and super-resolution.
Given these distinctions, it would be intriguing to extend the analysis techniques we developed, including
the auto-correlation functions, the score-function norm, KS criteria, and the KID metric, to the boomerang
method and its interesting applications involving local sampling.
5 Related Papers
In this section, we follow the recommendation of an anonymous reviewer and provide a discussion of prior
studies, emphasizing their relevance to the topics covered in this manuscript.
5.1 Jain & Poole (2022): Optimizing Sample Quality
Jain et al. (2022) proposed a clever strategy for improving sample quality in diffusion models. They showed
that by focusing on estimating scores only in the early stages of the diffusion process ( t∈[0,0.4]) and
employing a reduced number of discretization steps in the reverse stochastic differential equation (SDE), they
could approach the desired quality achievable when considering all timesteps ( t∈[0,1]). To compensate for
the error introduced by this approach, they ran Markov Chain Monte Carlo (MCMC) at each step, effectively
pushing the samples towards the desired marginals. It’s worth noting that this approach didn’t significantly
increase computational overhead, as the reduction in the number of steps was balanced by the need to run
MCMC at each step.
5.2 Raya & Ambrogioni (2023): Early Stage Analysis
Raya et al. (2023) conducted a thorough analysis of diffusion models in the context of symmetry breaking.
Their study revealed that the early stages of the reverse dynamics contributed minimally to generating
new samples. Based on these observations, they proposed a "late start" strategy. While their idea aligns
conceptually with the U-turn approach discussed in this manuscript, it’s important to highlight that our
justification for the timing of the late start is rooted in analysis insights rather than being solely empirical,
as in their work.
5.3 Zheng et al. (2023); Franzese et al. (2023): Significance of Diffusion Time
The significance of diffusion time in basic score-based generative models was explored in Zheng et al. (2023)
and Smith et al. (2023). These studies advocate using shorter diffusion times to enhance both training and
sampling efficiency. Their approach involves employing auxiliary models to bridge the gap between the initial
and final distributions in the diffusion process. Additionally, they introduce pre-trained generative models,
such as Variational Autoencoders (VAEs) or Generative Adversarial Networks (GANs), to approximate
non-Gaussian distributions in the reverse process. Moreover, Smith et al. (2023) posed the question of
determining the optimal number of time steps as an optimization problem and argued that excessive diffusion
time can have detrimental effects. This conclusion aligns with some of our ideas behind the U-turn algorithm.
16Under review as submission to TMLR
5.4 Das et al. (2023): Noise Scheduling
Das et al. (2023) contributed to the topic of designing improved noise schedules. They initially studied the
shortest path in the parameter space between two Gaussian distributions and subsequently applied these
findings to fit the shortest path to data. Their claim is that this new schedule leads to better diffusion of
data in both space and time when compared to conventional diffusion models.
5.5 De Bortoli et al. (2021): Schrödinger Bridge Approach
DeBortolietal. (2021)introducedaSchrödingerBridge(SB)basedapproachtofacilitatethetransformation
between arbitrary probability measures within a finite time frame. They utilized the diffusion score matching
approach to address the SB problem. While their resulting scheme requires fewer time steps compared to
basic diffusion models, it necessitates multiple iterations between the two probability measures to converge.
5.6 Lipman et al. (2023): Generalized Markov Kernels
Lipman et al. (2023) proposed a generalization that removed the standard restriction on the Markov kernel,
allowing for greater flexibility in selecting the path between two distributions. They chose to train the model
using Optimal Transport conditional probabilities, resulting in more efficient paths compared to the popular
VP Diffusion process and requiring fewer time steps.
In conclusion for this section, the ideas discussed in the aforementioned papers – such as ignoring the early
stage of reverse dynamics, suggesting fewer steps, and other enhancements to computational efficiency – are
likely to contribute to improvements in the initial stages of our U-turn approach. However, it is essential
to emphasize that our contributions extend beyond the late start strategy; they also encompass the critical
utilization of the last sample from the direct process before initiating the U-turn. This unique combination
of techniques is critical to our overall conclusions regarding the advantages of the U-turn approach.
6 Conclusions and Path Forward
This paper delves into the analysis of popular Score-Based Diffusion Models (SBD), which are rooted in the
idea that observed data are outcomes of dynamic processes. These models involve two related stochastic
sub-processes: forward and reversed dynamics. The design of these models offers flexibility in choosing
the advection and diffusion components of the dynamics. Three distinct advection-to-diffusion b-protocols,
developed in prior publications on the subject, were adopted to explore this freedom. While the Fokker-
Planck equations for the two sub-processes are equivalent, actual samples diverge as one advances forward
and the other backward in time.
Our first accomplishment is extending analysis beyond single-time marginals to study time correlations
through auto-correlation functions. This allowed quantification of information retention, by distributing in
the score function, and than recovery of the information in the reverse process. The analysis unveiled diverse
regimes, time scales, and their dependency on the chosen protocols.
The study then connects the decay patterns in auto-correlation functions to single-time objects, average of
the weighted score-function 2-norm and the Kolmogorov-Smirnov metric. The temporal behaviors of these
single-time objects are linked to the two-time correlation patterns, providing insights for potential control
applications (see discussion below).
Informed by the temporal analysis of the SBD, a novel U-Turn Diffusion process, which is the climax of the
manuscript, wasdevised, suggesting an optimal time totransition from forward toreversesub-processes. The
paper employs the KID test to assess the quality of U-Turn diffusion. Remarkably, the results demonstrate
the existence of an optimal U-turn time for generating synthetic images which are of the best quality within
the scheme.
In summary, this work thus not only advances our understanding of the SBD models but also offers a new
U-Turn algorithm to enhance the quality of synthetically generated data.
17Under review as submission to TMLR
The avenues for further exploration stemming from this study are delineated along three principal lines, each
aiming to enhance further our understanding and application of the SBD models:
•Fine-Tuning Protocols Using Time-Local Indicators : Our immediate focus will be on optimizing
and controlling the b-protocols to be data-adaptive. Employing time-local indicators such as the
weighted average norm of the score-function and the KS test, we intend to align the b-protocols with
the specific data characteristics.
•Enhancing U-Turn enforced SBD with Data-Specific Dynamics: Building on the success of the U-
Turn enforced SBD approach, we aim to extend its utility by incorporating data-specific correlations
and sparsity features into the underlying advection/diffusion dynamics. For instance, when initial
data showcases spatial correlations, we plan to develop SBD techniques grounded in spatio-temporal
stochastic partial differential equations.
•Establishing Theoretical Connections to Non-Equilibrium Statistical Mechanics: We intend to work
on connecting the U-Turn enforced SBD approach to non-equilibrium statistical mechanics concepts,
particularly those like the fluctuation theorem (e.g., Jarzynski and Crook relations) and Schrödinger
bridge approaches. The exploration of this theoretical nexus, informed by existing literature and
approaches Jarzynski (1997); Crooks (1999); Léonard & ,Modal-X. Université Paris Ouest, Bât.
G, 200 av. de la République. 92001 Nanterre (2014); Chen et al. (2021); Sohl-Dickstein et al.
(2015); De Bortoli et al. (2021), holds potential for illuminating the underlying mechanisms driving
generative AI’s power.
References
BrianD.O.Anderson. Reverse-timediffusionequationmodels. Stochastic Processes and their Applications , 12
(3):313–326, May 1982. ISSN 03044149. doi: 10.1016/0304-4149(82)90051-5. URL https://linkinghub.
elsevier.com/retrieve/pii/0304414982900515 .
Mikołaj Bińkowski, Danica J. Sutherland, Michael Arbel, and Arthur Gretton. Demystifying MMD GANs,
January 2021. URL http://arxiv.org/abs/1801.01401 . arXiv:1801.01401 [cs, stat].
Adam Block, Youssef Mroueh, and Alexander Rakhlin. Generative Modeling with Denoising Auto-Encoders
and Langevin Sampling, October 2022. URL http://arxiv.org/abs/2002.00107 . arXiv:2002.00107 [cs,
math, stat].
Patrick Charbonneau, Enzo Marinari, Marc Mézard, Giorgio Parisi, Federico Ricci-Tersenghi, Gabriele Si-
curo, and Francesco Zamponi. Spin Glass Theory and Far Beyond . WORLD SCIENTIFIC, 2023. doi:
10.1142/13341. URL https://www.worldscientific.com/doi/abs/10.1142/13341 .
Yongxin Chen, Tryphon T. Georgiou, and Michele Pavon. Optimal Transport in Systems and Control.
Annual Review of Control, Robotics, and Autonomous Systems , 4(1):89–113, May 2021. ISSN 2573-5144,
2573-5144. doi: 10.1146/annurev-control-070220-100858. URL https://www.annualreviews.org/doi/
10.1146/annurev-control-070220-100858 .
Gavin E. Crooks. Entropy production fluctuation theorem and the nonequilibrium work relation for free
energy differences. Physical Review E , 60(3):2721–2726, September 1999. ISSN 1063-651X, 1095-3787.
doi: 10.1103/PhysRevE.60.2721. URL https://link.aps.org/doi/10.1103/PhysRevE.60.2721 .
Ayan Das, Stathi Fotiadis, Anil Batra, Farhang Nabiei, FengTing Liao, Sattar Vakili, Da shan Shiu, and
Alberto Bernacchia. Image generation with shortest path diffusion, 2023.
Valentin De Bortoli, James Thornton, Jeremy Heng, and Arnaud Doucet. Diffusion Schrödinger Bridge with
Applications to Score-Based Generative Modeling. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P. S.
Liang, and J. Wortman Vaughan (eds.), Advances in Neural Information Processing Systems , volume 34,
pp. 17695–17709. Curran Associates, Inc., 2021. URL https://proceedings.neurips.cc/paper_files/
paper/2021/file/940392f5f32a7ade1cc201767cf83e31-Paper.pdf .
18Under review as submission to TMLR
Nicki Skafte Detlefsen, Jiri Borovec, Justus Schock, Ananya Harsh Jha, Teddy Koker, Luca Di Liello, Daniel
Stancl, ChangshengQuan, MaximGrechkin, andWilliamFalcon. Torchmetrics-measuringreproducibility
in pytorch. Journal of Open Source Software , 7(70):4101, 2022. doi: 10.21105/joss.04101. URL https:
//doi.org/10.21105/joss.04101 .
Giulio Franzese, Simone Rossi, Lixuan Yang, Alessandro Finamore, Dario Rossi, Maurizio Filippone, and
Pietro Michiardi. How much is enough? a study on diffusion times in score-based generative models. En-
tropy, 25(4), 2023. ISSN 1099-4300. doi: 10.3390/e25040633. URL https://www.mdpi.com/1099-4300/
25/4/633 .
Arthur Gretton, Karsten M. Borgwardt, Malte J. Rasch, Bernhard Schölkopf, and Alexander Smola. A
Kernel Two-Sample Test. Journal of Machine Learning Research , 13(25):723–773, 2012. URL http:
//jmlr.org/papers/v13/gretton12a.html .
Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. GANs
Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium, January 2018. URL
http://arxiv.org/abs/1706.08500 . arXiv:1706.08500 [cs, stat].
Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising Diffusion Probabilistic Models, December 2020. URL
http://arxiv.org/abs/2006.11239 . arXiv:2006.11239 [cs, stat].
Aapo Hyvärinen. Estimation of Non-Normalized Statistical Models by Score Matching. Journal of Machine
Learning Research , 6(24):695–709, 2005. URL http://jmlr.org/papers/v6/hyvarinen05a.html .
Ajay Jain and Ben Poole. Journey to the BAOAB-limit: finding effective MCMC samplers for score-based
models. In NeurIPS 2022 Workshop on Score-Based Methods , 2022. URL https://openreview.net/
forum?id=j4-a3SNyaY .
C. Jarzynski. Nonequilibrium Equality for Free Energy Differences. Physical Review Letters , 78(14):2690–
2693, April 1997. ISSN 0031-9007, 1079-7114. doi: 10.1103/PhysRevLett.78.2690. URL https://link.
aps.org/doi/10.1103/PhysRevLett.78.2690 .
Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the Design Space of Diffusion-Based
Generative Models, October 2022. URL http://arxiv.org/abs/2206.00364 . arXiv:2206.00364 [cs, stat].
Diederik P. Kingma and Ruiqi Gao. Understanding diffusion objectives as the elbo with simple data aug-
mentation, 2023.
Yaron Lipman, Ricky T. Q. Chen, Heli Ben-Hamu, Maximilian Nickel, and Matthew Le. Flow matching for
generative modeling. In The Eleventh International Conference on Learning Representations , 2023. URL
https://openreview.net/forum?id=PqvMRDCJT9t .
Lorenzo Luzi, Ali Siahkoohi, Paul M. Mayer, Josue Casco-Rodriguez, and Richard Baraniuk. Boomerang:
Local sampling on image manifolds using diffusion models, October 2022. URL http://arxiv.org/abs/
2210.12100 . arXiv:2210.12100 [cs, stat].
Christian Léonard and ,Modal-X. Université Paris Ouest, Bât. G, 200 av. de la République. 92001 Nanterre.
A survey of the Schrödinger problem and some of its connections with optimal transport. Discrete &
Continuous Dynamical Systems - A , 34(4):1533–1574, 2014. ISSN 1553-5231. doi: 10.3934/dcds.2014.34.
1533. URL http://aimsciences.org//article/doi/10.3934/dcds.2014.34.1533 .
Alex Nichol and Prafulla Dhariwal. Improved Denoising Diffusion Probabilistic Models, February 2021. URL
http://arxiv.org/abs/2102.09672 . arXiv:2102.09672 [cs, stat].
Gabriel Raya and Luca Ambrogioni. Spontaneous symmetry breaking in generative diffusion models, June
2023. URL http://arxiv.org/abs/2305.19693 . arXiv:2305.19693 [cs].
Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution
image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition (CVPR) , pp. 10684–10695, June 2022.
19Under review as submission to TMLR
Jascha Sohl-Dickstein, Eric A. Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep Unsupervised
Learning using Nonequilibrium Thermodynamics, November 2015. URL http://arxiv.org/abs/1503.
03585. arXiv:1503.03585 [cond-mat, q-bio, stat].
Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data dis-
tribution. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d 'Alché-Buc, E. Fox, and
R. Garnett (eds.), Advances in Neural Information Processing Systems , volume 32. Curran As-
sociates, Inc., 2019. URL https://proceedings.neurips.cc/paper_files/paper/2019/file/
3001ef257407d5a371a96dcd947c7d93-Paper.pdf .
Yang Song, Jascha Sohl-Dickstein, Diederik P. Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole.
Score-Based Generative Modeling through Stochastic Differential Equations, February 2021. URL http:
//arxiv.org/abs/2011.13456 . arXiv:2011.13456 [cs, stat].
Pascal Vincent. A Connection Between Score Matching and Denoising Autoencoders. Neural Computation ,
23(7):1661–1674, July 2011. ISSN 0899-7667, 1530-888X. doi: 10.1162/NECO_a_00142. URL https:
//direct.mit.edu/neco/article/23/7/1661-1674/7677 .
Patrick von Platen, Suraj Patil, Anton Lozhkov, Pedro Cuenca, Nathan Lambert, Kashif Rasul, Mishig
Davaadorj, and Thomas Wolf. Diffusers: State-of-the-art diffusion models. https://github.com/
huggingface/diffusers , 2022.
Minkai Xu, Lantao Yu, Yang Song, Chence Shi, Stefano Ermon, and Jian Tang. GeoDiff: a Geometric
Diffusion Model for Molecular Conformation Generation, March 2022. URL http://arxiv.org/abs/
2203.02923 . arXiv:2203.02923 [cs, q-bio].
Huangjie Zheng, Pengcheng He, Weizhu Chen, and Mingyuan Zhou. Truncated diffusion probabilistic models
and diffusion-based adversarial auto-encoders. In The Eleventh International Conference on Learning
Representations , 2023. URL https://openreview.net/forum?id=HDxgaKk956l .
A Implementation Details
Inourexperimentation, weemployfundamentaldiffusionalgorithmssourcedfromtheHuggingFaceDiffusers
library von Platen et al. (2022). Our dataset consists of a collection of 1000 butterfly images, accessible via
the Hugging Face Hub1. Since the images vary in dimensions, we uniformly resize them to 64×64.
To model the score-functions within the diffusion models, we adhere to the established approach of utilizing
a U-net architecture. Detailed specifications of the U-net structure are available in the code (submitted
along with the manuscript). In our computations, we adopt a batch size of 64, conduct 100epochs, and
leverage the Adam optimizer with a learning rate of 8×10−5. For evaluating the KID, we make use of the
implementation provided by TorchMetrics Detlefsen et al. (2022).
B CIFAR-10
Incorporated in light of feedback from anonymous reviewers, this Appendix seeks to validate the findings
presented in the main manuscript using alternative and broader datasets, such as CIFAR-10. The goal is to
assess the diversity of synthetic images, both quantitatively and qualitatively, on a data set which is more
expressive (as buttterfly all look similar).
Subsequent sub-sections of the Appendix demonstrate that the newly introduce measure of diversity, as
well as the behavior of the norm of the score function of the KID (our primary single-time indicators), are
consistent with the findings reported in the main part of the paper.
Due to time constraints for addressing reviewer comments, we limit our discussion to training the diffusion
model on CIFAR-10 for 2000 epochs (a relatively modest duration by current standards) and only discuss
the linearb-protocol and the two indicators mentioned.
1https://huggingface.co/datasets/huggan/smithsonian_butterflies_subset
20Under review as submission to TMLR
B.1 Norm of the Score Function
0 200 400 600 800 1000
n=t/
102
101
100S(t=n)
linear
Figure 13: Norm of the score function for CIFAR-10.
Following Algorithm 1, the score function’s norm is derived from Eq. 18. Figure 13 reveals a saturation point
for the norm around t1= 760– therefore suggesting that this is the optimal time for making the U-turn.
B.2 Test of Diversity: Nearest Neighbor Test
(a) Generated Images
 (b) Nearest neighbors using cosine similarity 20.
Figure 14: U-Turn for T= 500
For a quantitative assessment, we applied the U-turn method for different Tvalues, examining the generated
image diversity by comparing them to their nearest neighbors using the cosine similarity formula:
Cx,¯x=⟨x,¯x⟩
∥x∥2∥¯x∥2. (20)
Here, xis an image from CIFAR-10, while ¯xis one generated by U-Turn.
21Under review as submission to TMLR
(a) generated
 (b) Nearest neighbors using cosine similarity 20.
Figure 15: U-Turn for T= 600
(a) Generated Images
 (b) Nearest neighbors using cosine similarity 20.
Figure 16: U-Turn for T= 700
Figures 14 through 19 illustrate our observations. Notably, applying U-turn before t1yields images with
limited diversity (Figures 14 through 16). However, post- t1applications, as in Figure 17, result in a more
varied set of generated images, as evident in Fig. 17 for T= 800. This increased diversity persists for
subsequent Tvalues. As seen in Figs. 18, 19 the diversity of images generated at T= 900andT= 100is
the same as T= 800, therefore reaffirming the U-turn method’s capability in image generation.
B.3 KID Test
Figure 20 showcases the KID scores across various time steps. These results echo our earlier observations
with the butterfly dataset, where adding extra diffusion steps after reaching the optimal point deteriorates
the KID. However, it’s noteworthy that the KID scores remain relatively high due to the limited training
epochs.
22Under review as submission to TMLR
(a) Generated Images
 (b) Nearest neighbors using cosine similarity 20.
Figure 17: U-Turn for T= 800
(a) Generated Images
 (b) Nearest neighbors using cosine similarity 20.
Figure 18: U-Turn for T= 900
23Under review as submission to TMLR
(a) Generated Images
 (b) Nearest neighbors using cosine similarity 20.
Figure 19: U-Turn for T= 1000
0 200 400 600 800 1,00023456
Time StepsKIDLinear
Figure 20: KID score for CIFAR-10 dataset. KID is computed for 50k generated images with the same
number of real images from the dataset.
24