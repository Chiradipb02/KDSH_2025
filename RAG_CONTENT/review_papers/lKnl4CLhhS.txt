Efficient and Private Marginal Reconstruction with
Local Non-Negativity
Brett Mullins1Miguel Fuentes1Yingtai Xiao2Daniel Kifer2
Cameron Musco1Daniel Sheldon1
1University of Massachusetts, Amherst2Penn State University
{bmullins,mmfuentes,cmusco,sheldon}@cs.umass.edu
{yxx5224,duk17}@psu.edu
Abstract
Differential privacy is the dominant standard for formal and quantifiable privacy
and has been used in major deployments that impact millions of people. Many
differentially private algorithms for query release and synthetic data contain steps
that reconstruct answers to queries from answers to other queries that have been
measured privately. Reconstruction is an important subproblem for such mecha-
nisms to economize the privacy budget, minimize error on reconstructed answers,
and allow for scalability to high-dimensional datasets. In this paper, we introduce a
principled and efficient postprocessing method ReM (Residuals-to-Marginals) for
reconstructing answers to marginal queries. Our method builds on recent work on
efficient mechanisms for marginal query release, based on making measurements
using a residual query basis that admits efficient pseudoinversion, which is an
important primitive used in reconstruction. An extension GReM-LNN (Gaussian
Residuals-to-Marginals with Local Non-negativity) reconstructs marginals under
Gaussian noise satisfying consistency and non-negativity, which often reduces error
on reconstructed answers. We demonstrate the utility of ReM and GReM-LNN by
applying them to improve existing private query answering mechanisms.
1 Introduction
Differential privacy is the dominant standard for formal and quantifiable privacy and has been used in
major deployments that impact millions of people such as the 2020 US Decennial Census [ 1]. One
of the most fundamental problems in differential privacy is answering a workload of linear queries.
Linear queries are used for basic descriptive statistics like counts and sums, and as building blocks
for more complex tasks. Marginal queries, which describe the frequency distribution of subsets
of discrete variables (e.g., income by age and education), are of particular interest as descriptive
statistics and for use in downstream tasks like regression analyses.
A key subproblem in linear query answering is reconstruction . Given a workload of linear queries,
most mechanisms select a different set of queries to measure to make the most efficient use of the
privacy budget, and then use the noisy answers to reconstruct answers to workload queries [ 2–11].
Effective reconstruction methods can combine information from all noisy measurements to provide
mutually consistent answers to workload queries.
Computational complexity is a key challenge for reconstruction methods. These methods answer
workload queries by—either explicitly or implicitly—reconstructing a data distribution that has size
exponential in the number of variables. To scale to high-dimensional data sets, existing approaches
must represent this distribution compactly through some form of parametric representation [ 8–12],
which introduces tradeoffs such as a restricted space of data distributions that can be represented [ 8–
38th Conference on Neural Information Processing Systems (NeurIPS 2024).11], non-convex optimization objectives to find the best representation [ 9–11], or complexity that
depends on the measured queries and is still exponential in the worst case [12].
We introduce ReM (residuals-to-marginals), a principled and scalable post-processing method to
reconstruct answers to a workload of marginal queries from noisy measurements of residuals .
Residuals are a class of linear queries that are related to marginals, which were recently introduced
in the privacy literature [ 6] but previously studied in statistics [ 13,14]. ReM uses a compact
representation of the data distribution to produce workload answers without exponential complexity
in the number of variables. ReM builds on the reconstruction approach of ResidualPlanner [ 6],
which utilizes Kronecker structure to efficiently perform pseudoinverse operations. ReM is a flexible
framework for performing reconstruction in a broad range of settings and it can be used with a variety
of existing query-answering mechanisms. ReM also extends to the common setting of reconstructing
answers to marginal queries from a set of noisy marginal measurements with isotropic Gaussian noise.
In this case, ReM performs the standard pseudoinverse reconstruction and is the first method to do so
efficiently. We also develop GReM-LNN (Gaussian ReM with local non-negativity), an extension
that reconstructs marginals satisfying non-negativity, which often reduces error on reconstructed
answers.
We demonstrate the utility of ReM and GReM-LNN by showing that they significantly reduce error
and enhance the scalability of existing private query answering mechanisms including ResidualPlan-
ner [6] and the multiplicative weights exponential mechanisms (MWEM) [ 15]. Our code is available
athttps://github.com/bcmullins/efficient-marginal-reconstruction .
2 Preliminaries
We consider a sensitive tabular dataset Dof records x(1), . . . , x(N). Each record x= (x1, . . . , x d)
consists of dcategorical attributes. The ith attribute xibelongs to the finite set Xiof size ni. The
data universe is X=Qd
i=1Xiand has size n=Q
ini. The data vector ordata distribution
p∈Rnis a vector indexed by Xthat counts the occurrences of each record in D; it has entries
p(x) =PN
i=1I[x(i)=x]. Since nis exponential in the data dimension d, it is computationally
intractable to work directly with data vectors in high dimensions.
2.1 Linear queries, marginals, and residuals
Linear queries are a rich class of statistics that include counts, sums, and averages and are used
as building blocks for more complex tasks. A linear query is the sum of a real-valued function
q:X →Rapplied to each record in the dataset. We adopt the equivalence that a query is a vector
q∈Rnwith answer q⊤p. Aquery matrix orworkload Wis a collection of mlinear queries arranged
row-wise in an m×nmatrix. The answer to workload Wfor data vector pis given by Wp.
Marginal queries are a common type of linear query for high-dimensional data. They count the
number of records that match certain values for a subset of the attributes – e.g., the number of
people in a dataset with education at least a college degree and income $50-$100K. Let γ⊆[d]be
a subset of attributes and xγ= (xi)i∈γbe the corresponding subvector of a record x. Further, let
Xγ=Q
i∈γXiandnγ=Q
i∈γni. The marginal µγ∈Rnγhas entries µγ(t) =PN
i=1I[x(i)
γ=t]
that count the number of occurrences in the dataset for each setting t∈ Xγof the attributes in γ.
LetMγ∈Rnγ×nbe the marginal workload so that µγ=Mγp. As shown in Fig. 1a, Mγcan be
written concisely as a Kronecker product over dimensions, with base matrices equal to the identity
Ik∈Rnk×nkfor attributes in γand the all ones vector 1⊤
k∈R1×nkfor attributes not in γ. Kronecker
product matrices can be understood as applying different linear operations along each dimension of a
multi-dimensional array. In this case Mγsums over dimensions of the array representation of pfor
attributes not in γ. We provide a brief summary of Kronecker products and their relevant properties
in Appendix A.
Residual queries are class of linear queries closely related to marginals. They were recently introduced
in the privacy literature [ 6] but previously studied in statistics as variable interactions [13,14]. For
τ⊆[d], theτ-residual is obtained from the marginal µτby applying a differencing operator along
each dimension. Let D(k)be the linear operator that computes successive differences for vectors of
length nk, i.e., (D(k)v)i=vi+1−vifori= 1, . . . , n k−1; an example is shown for nk= 3in Fig.
1b. Let Dτbe the matrix that applies this operation to all attributes in the τ-marginal as shown in
2Mγ=dO
k=1(
Ikk∈γ
1⊤
kk /∈γ
(a) MarginalsD(k)=
1−1 0
0 1 −1
(b) Differencing operator
forkth attribute.Dτ=dO
k=1(
D(k)k∈τ
1 k /∈τ
(c) Differencing operator
forτ-marginal.Rτ=dO
k=1(
D(k)k∈τ
1⊤
k k /∈τ
(d) Residuals
Figure 1: Kronecker structure of workloads.
Fig. 1c. The residual workload can be written as Rτ=DτMτ∈Rmτ×nwhere mτ=Q
i∈τ(ni−1),
which has the explicit Kroecker product form shown in Fig. 1d.1With these definitions, if µτ=Mτp
is the τ-marginal, the τ-residual is ατ=Dτµτ=Rτpand can be computed from either µτorp.
Residuals and marginals have an intricate structure. The γ-marginal is uniquely determined by the
τ-residuals for τ⊆γ, i.e., there is an invertible linear transformation between Mγand(Rτ)τ⊆γ(a
vertical block matrix). Intuitively, a γ-residual contains information notcontained in the τ-marginals
forτ⊂γ. Further, the row spaces of RτandRτ′are orthogonal for any τ̸=τ′, and the row
spaces of MγandRτare orthogonal when τ̸⊆γ[6,13,14]. Along with Kronecker structure, the
orthogonality of residuals is the key property we will leverage to perform efficient reconstruction.
A key advantage of residual workloads is that we can work with their pseudoinverses efficiently
in certain situations even though they have exponential size. Let Q+denote the Moore-Penrose
pseudoinverse of Q. The following proposition builds on the reconstruction method in [ 6] and will be
used to reconstruct answers to a marginal query Mγfrom measurements for a collection of residuals.
Proposition 1. LetRS= (Rτ)τ∈Sbe a combined workload of residual queries for all τin a
collection S ⊆2[d], where the individual matrices Rτare stacked vertically. The size of RSism×n
where m=P
τ∈Smτ. Then for any z= (zτ)τ∈S∈Rmand any γ, it holds that
MγR+
Sz=X
τ∈S,τ⊆γAγ,τzτ, where Aγ,τ:=dO
k=1

D+
(k)k∈τ 
1/nk
1kk∈γ\τ
1 k /∈γforτ⊆γ.
The matrix Aγ,τhas size nγ×mτand maps from the space of τ-residuals to the space of γ-marginals.
The running time to compute Aγ,τzτisO(|γ|nγ).
The proof of this result appears in Appendix C. The analysis of time complexity appears in Ap-
pendix E.
2.2 Differential Privacy
When releasing the results of any analysis performed on sensitive data, particular care needs to be
taken to avoid leaking private information contained in the dataset. Differential privacy is a mathe-
matical criterion that bounds the effect of any individual in the dataset on the output of a mechanism,
which is satisfied by adding noise to the computation. This allows for formal quantification of the
privacy risk associated with any release of information.
Definition 1. (Differential Privacy; [ 16]) LetM:X → Y be a randomized mechanism. For any
neighboring datasets D,D′that differ by adding or removing at most one record, denoted D ∼ D′,
and all measurable subsets S⊆ Y: ifPr(M(D)∈S)≤exp(ϵ)·Pr(M(D′)∈S) +δ, thenM
satisfies (ϵ, δ)-approximate differential privacy, denoted (ϵ, δ)-DP.
A fundamental property of differential privacy relevant to our work is the post-processing property,
which states that transformations of differentially private outputs that do not access the sensitive
dataset Dmaintain their privacy guarantees. Formally:
Proposition 2 (Post-processing; [ 17]).LetM1:X → Y satisfy (ϵ, δ)-DP and f:Y → Z be a
randomized algorithm. Then M:X → Z =f◦ M 1satisfies (ϵ, δ)-DP .
The reconstruction methods we propose in this paper are post-processing algorithms that take as input
a set of noisy linear query answers and, thus, inherit the privacy guarantees from those noisy answers.
Note that the present analysis is largely agnostic to the model of differential privacy used.
1Note that our matrix Dτis slightly different from the operator used in [ 6] but has the same row space [ 14].
3We discuss variants of differential privacy and privacy guarantees for query answering in Appendix B.
2.3 Private query answering
In private query answering, we are given a workload of linear queries W∈Rm×n. We seek to
approximate the answers Wpas accurately as possible while satisfying differential privacy. A general
recipe for private query answering is select-measure-reconstruct .Data-independent mechanisms
following this recipe such as the various matrix mechanisms [ 2–6] select and measure a set of queries
Qand reconstruct answers to W.Data-dependent mechanisms following this recipe such as MWEM
[15] and various synthetic data mechanisms [ 7,9,10,18,19] typically maintain a model ˆpof the
data distribution pthat is improved iteratively by repeating the steps of select-measure-reconstruct
and adaptively measuring queries that are poorly approximated by the current model ˆp. The key
idea is that it is often possible to obtain lower error by measuring a different set of queries Q
thanWand then using answers to Qto reconstruct answers for W. In this paper, we focus on
the reconstruction subproblem and propose methods applicable to both the data-independent and
data-dependent settings.
2.4 Query answer reconstruction
Reconstruction is a central subproblem to query answering. Suppose y=Qp+ξis the a set
of measurements. To reconstruct a data distribution, we seek ˆpsuch that Qˆp≈y. One method
is to set ˆp=Q+ywhere Q+is the Moore-Penrose pseudoinverse. This method is used in the
matrix mechanism [ 4] and HDMM [ 5] but is not tractable in high dimensions. One contribution
of our proposed method is to demonstrate that this pseudoinverse reconstruction is tractable when
the query matrix Qis a set of marginal measurements and ξis isotropic Gaussian noise. Other
reconstruction methods such as Private-PGM [ 12] and those used by the mechanisms PrivBayes [ 8],
GEM [ 9], RAP [ 10], and RAP++ [ 11] represent ˆpthrough a parametric representation. These (usually)
ensure tractability in high dimensions by using a compact representation, but introduce different
tradeoffs. The parametric assumption typically restricts the space of data distributions that can be
represented [ 8–11]. Optimizing over the parameteric representation is often non-convex, potentially
leading to suboptimal optimization [ 9–11]. Private-PGM solves a convex optimization problem and
is closest to the methods of this paper. However its complexity depends on the measured queries and
is still exponential in the worst case [12]; our methods will not have exponential complexity.
We note that all of these above reconstruction methods, and the methods presented in this work, only
depend on the dataset through the noisy query answers and, thus, satisfy the same degree of privacy
as the answers by the post-processing property of differential privacy (Proposition 2).
3 Efficient Marginal Reconstruction from Residuals
In this section, we discuss methods for reconstructing answers to a workload of marginal queries
given measurements of residuals. These methods utilize the structure of marginals and residuals
to make reconstruction tractable and minimize error. Let W ⊆ 2[d]andMW= (Mγ)γ∈Wbe the
combined workload of marginals for all of the attribute sets in W(e.g., all pairs or triples of attributes).
Similarly, let RS= (Rτ)τ∈Srepresent a set of residual queries for all τin a collection S. Our goal
is to estimate the marginal query answers MWpfrom noisy measurements z=RSp+ξ.
Algorithm 1 ResidualPlanner reconstruction
Input: Marginal workload W,S=W↓, measure-
ments zτ=Rτp+N(0,Στ)forτ∈ S
1:Reconstruct ˆµγ=P
τ⊆γAγ,τzτforγ∈ WResidualPlanner. ResidualPlanner [ 6] solves
this problem elegantly in the matrix mechanism
(i.e. data-independent) setting under Gaussian
noise. Let W↓={τ⊆γ:γ∈ W} denote
thedownward closure ofW. When S=W↓,
the residual queries for Suniquely determine
the marginals for W, i.e., there is an invertible linear transformation between MWandRS. This
yields the reconstruction approach in Alg. 1. We suppose the residual queries Rτare measured with
Gaussian noise to yield zτ. In Line 1, the marginals are reconstructed by applying the invertible
transformation from residuals to marginals. This reconstruction is equivalent to setting ˆµγ=Mγˆp
where ˆp=R+
Szandz= (zτ)τ∈Sby Proposition 1.
4The full ResidualPlanner algorithm additionally chooses each Στ=σ2
τDτD⊤
τsuch that the resulting
algorithm optimally answers the marginal workload indexed by Wto minimize error under a natural
class of convex loss functions for a given privacy budget [ 6]. That this can be done efficiently for a
broad class of error metrics for marginal workloads is significant given the computational challenges
that are often faced when attempting to optimally select measurements and reconstruct workload
answers in high dimensions.
A general approach to reconstruction. We propose a reconstruction algorithm that, like the one in
ResidualPlanner, is efficient and principled, but that applies in more general settings. Reconstruction
in ResidualPlanner uses the invertible transformation from residuals to marginals. This restricts to
the case where the measured queries exactly determine the workload queries in the absence of noise.
To address the full range of applications, it is important to address the cases where workload queries
are overdetermined, underdetermined, or both.
Algorithm 2 Residuals-to-Marginals (ReM)
Input: Marginal workload W, arbitrary S, measure-
ments zτ,i=Rτp+ξτ,iforτ∈ S,i= 1, . . . , k τ,
where ξτ,icomes from any noise distribution
1:Estimate ˆατ≈Rτpforτ∈ S by minimizing loss
function Lτ(ατ)
2:Reconstruct ˆµγ=P
τ∈S:τ⊆γAγ,τˆατforγ∈ WOur proposed algorithm, ReM, is shown
in Alg. 2. Compared to ResidualPlanner,
the main differences are: (1) the set Sof
measured residuals is arbitrary, (2) a resid-
ual query can be measured any number of
times with any noise distribution, (3) an
optimization problem is solved for each τ
to estimate the true residual query answer
ˆατ≈Rτp, (4) reconstruction uses the es-
timated residuals ˆατinstead of the noisy measurements zτ. The loss function Lτ(ατ)in Line 2
captures how well ατexplains the entire set of noisy measurements {zτ,i}i=1,...,k τ. For example,
a typical choice is Lτ(ατ) =−Pkτ
i=1logp(zτ,i|Rτp=ατ), the negative log-likelihood of the
measurements.
The following result shows that solving the optimization problems in Line 1 is equivalent to finding a
compact representation of a data distribution ˆpthat minimizes a global reconstruction loss and then
using ˆpto answer each marginal query.
Theorem 1. Suppose ˆατminimizes Lτ(ατ)overRmτfor each τ∈ Sand let ˆα= (ˆατ)τ∈S. Then
Alg. 2 outputs ˆµγ=Mγˆp, where ˆp=R+
Sˆαis a global minimizer of the combined loss functionP
τ∈SLτ(Rτp)overRn.
This result is proved (in Appendix D) by showing that Rτˆp= ˆατfor all τ, and thus ˆpoptimizes
each individual loss function Lτ, and so must be a global minimizer. Proposition 1 then shows that
ˆµγ=Mγˆp=MγR+
Sˆαhas the form given in Line 2 of the algorithm.
4 Applications of ReM under Gaussian Noise
In this section, we apply ReM to reconstruct answers to marginal queries in various settings: (1)
we reconstruct from residuals measured with Gaussian noise, (2) we reconstruct from marginals
measured with isotropic Gaussian noise, and (3) we reconstruct non-negative answers from residuals
measured with Gaussian noise.
4.1 Reconstruction under Gaussian noise
Algorithm 3 Gaussian ReM with Maximum Likelihood
Estimation (GReM-MLE)
Input: Marginal workload W, arbitrary S,
measurements zτ,i=Rτp+N(0,Στ,i)forτ∈ S,
i= 1, . . . , k τ
1:Estimate ˆατ= P
iΣ−1
τ,i−1P
iΣ−1
τ,izτ,iforτ∈ S
2:Reconstruct ˆµγ=P
τ∈S:τ⊆γAγ,τˆατforγ∈ WAn instance of ReM that allows for effi-
cient computation is when residuals are
measured with Gaussian noise i.e., zτ,i=
Rτp+ξτ,iwhere ξτ,i∼ N(0,Στ,i)and
the loss function Lτ(ατ)is the negative
log-likelihood of the measurements. In
this case, ˆα= (ˆατ)τ∈Sis the maximum
likelihood estimate of the residual an-
swers α= (ατ)τ∈S. We refer to this
setting as GReM-MLE (Gaussian ReM
with Maximum Likelihood Estimation), shown in Alg. 3.
5The loss function Lτ(ατ)is a sum of quadratic forms given by Lτ(ατ) =Pkτ
i=1(ατ−
zτ,i)⊤Σ−1
τ,i(ατ−zτ,i). In this setting, the optimization problems in Line 1 of Alg. 2 have the
closed-form solution ˆατ= P
iΣ−1
τ,i−1P
iΣ−1
τ,izτ,i, which is a form of inverse-variance weighting
and can be verified by setting the gradient of the loss function to zero.
GReM-MLE improves computational tractability by reducing Alg. 2 to operations on matrices.
Moreover, if the covariances among measurements of residual Rτdiffer only by a constant for τ∈ S,
i.e.,Στ,i=σ2
τ,iKτwhere στ,i∈R, then ˆατcan be computed as a weighted average given by
ˆατ= (P
iσ−2
τ,i)−1P
iσ−2
τ,izτ,i. All instances of GReM-MLE considered throughout the paper
satisfy this assumption of proportional covariances for each τ∈ S.
4.2 Reconstruction from marginals
A common practice in existing mechanisms is to measure marginal queries with isotropic Gaussian
noise [ 4,7,9,11,18,20]. In this special case, the measurements can be converted to an equivalent
set of residual measurements with independent Gaussian noise, allowing us to apply GReM-MLE.
The key observation is that a marginal query answer µγ=Mγpfor attribute set γcan be used to
derive residual answers ατ=Rτpfor each τ⊆γvia the following Lemma (proved in Appendix D):
Lemma 1. Forτ⊆γ, the residual Rτcan be recovered from the marginal Mγas
Rτ=A+
γ,τMγwhere A+
γ,τ=dO
k=1

D(k)k∈τ
1T
k k∈γ\τ
1 k /∈γ.
Whereas Aγ,τmaps answers from residual Rτto answers to marginal Mγ, the matrix A+
γ,τmaps
answers from marginal Mγto residual Rτ. Furthermore, µγcan be reconstructed from the set of
all residuals (ατ)τ⊆γ, so these residuals carry equivalent information to the marginal. Additionally,
when the marginal is observed with isotropic noise as yγ=Mγp+N(0, σ2
γI), the corresponding
noisy residuals A+
γ,τzτare independent. As a consequence, we can decompose a noisy marginal
measurement into a set of equivalent and independent noisy residual measurements.
Theorem 2. Letyγ∼ N (Mγp, σ2I)be a noisy marginal measurement with isotropic Gaus-
sian noise and let zτ=A+
γ,τyγfor each τ⊆γ. Then noisy residual zτhas distribution
N(Rτp, σ2DτD⊤
τQ
k∈γ\τnk)andzτis independent of zτ′forτ̸=τ′.
Furthermore, let Hγ= (A+
γ,τ)τ⊆γbe the matrix mapping from yγto(zτ)τ⊆γ. This matrix is
invertible, which implies that
logN(yγ|Mγp, σ2I) =X
τ⊆γlogN
zτRτp, σ2DτD⊤
τY
k∈γ\τnk
+ log|detHγ|. (1)
Given a collection of noisy marginal measurements, we can apply the above decomposition to obtain
a set of independent noisy residuals with proportional covariances. To reconstruct marginal answers,
we can apply GReM-MLE to the residuals. Alg. 4 shows this decomposition and reconstruction.
Equation (1)shows that the noisy residual measurements and noisy marginal measurements are
equivalent from the perspective of finding the best data vector pby maximum likelihood, because
the log-likelihood of the residual measurements differs from the log-likelihood of the marginal
measurement by a constant log|detHγ|that is independent of p, and measurements of marginals
are each independent. A maximum likelihood estimate of pfrom the marginal measurements yis
given by using the pseudoinverse of the measured workload to map noisy marginal measurements to
a data vector. The following result shows that the method in Alg. 4 is equivalent to answering queries
from this maximum likelihood estimate of the data vector given the marginal measurements when the
marginals are measured with the same noise scale.
6Algorithm 4 Efficient Marginal Pseudoinversion (EMP)
Input: Marginal workload W, measured marginals multiset Q, measurements y= (yγ)γ∈Qwhere
yγ=Mγp+N(0, σ2I)forγ∈ Q
Output: Marginal answers (MγM+
Qy)γ∈W
1:Initialize S=∅andkτ= 0for all τ ▷ Track measured residuals, lazy data structure for kτ
2:forγ∈ Q do
3: forτ⊆γdo
4: S=S ∪ { τ}, kτ←kτ+ 1
5: zτ,kτ=A+
γ,τyγ ▷Extract residual measurement from yγ
6: σ2
τ,kτ=σ2Q
k∈γ\τnk ▷Compute noise scale
7: Στ,kτ=σ2
τ,kτDτD⊤
τ ▷Proportional covariance
return GReM-MLE( W,S,z) where z= (zτ,i:τ∈ S, i= 1, . . . , k τ)
Theorem 3 (Efficient pseudoinversion of marginal query matrix) .LetMQ= (Mγ)γ∈Qbe the
query matrix for a multiset Qof marginals and let y= (yγ)γ∈Qbe corresponding noisy marginal
measurements with yγ=Mγp+N(0, σ2I). LetS={τ⊆γ:γ∈ Q} and for each τ∈ S letγτ,i
be the ith marginal in Qcontaining τ. Letzτ,i=A+
γτ,i,τyγτ,ibe the residual measurement obtained
fromγτ,iand let Στ,i=σ2
τ,iDτD⊤
τbe its covariance where σ2
τ,i=σ2Q
k∈γτ,i\τnk. Then, given
any workload of marginal queries W, for each γ∈ W , the marginal reconstruction ˆµγobtained
from Algorithm 3 on these residual measurements is equal to MγM+
Qy.
This result can be generalized to allow for differing noise scales between marginal measurements.
We prove this result and discuss the generalized form of Theorem 3 in Appendix D.
4.3 Reconstruction with local non-negativity
It is often possible to improve accuracy of a differentially private mechanism by forcing its outputs
to satisfy known constraints [ 4,10,21]. For our problem, true marginals are non-negative, so it is
desirable to enforce non-negativity in their private estimates. To enforce non-negativity, instead of
solving the separate problems in Line 1 of Alg. 2, we solve the following combined problem over the
full vector α= (ατ)τ∈W↓of residuals:
min
αX
τ∈SLτ(ατ)s.t.X
τ⊆γAγ,τατ≥0,∀γ∈ W. (2)
Reconstruction of marginals then proceeds as in Line 2 of Alg. 2. The constraints in Eq. (2) ensure
that the reconstructed marginals will be non-negative. We refer to this as local non-negativity , since
this problem solves for a data distribution ˆpthat is non-negative for marginals in Wrather than a data
distribution with non-negative entries.
A natural setting to apply local non-negativity to ReM is under Gaussian noise with covariance
Στ,i=σ2
τ,iDτD⊤
τandσ2
τ,i∈R. Recall that marginals measured with isotropic Gaussian noise
decompose into residuals with the above covariance structure. Our proposed application of local
non-negativity in the Gaussian noise setting GReM-LNN (Gaussian ReM with local non-negativity)
solves Eq. (2) for Lτ(ατ) =Pkτ
i=1(ατ−zτ,i)⊤K−1
τ,i(ατ−zτ,i)andKτ,i= 2|τ|DτD⊤
τ. In the
GReM-LNN setting, Eq. (2) is an convex program with linear constraints. Our implementation
solves this problem using a scalable dual ascent algorithm (described in Appendix F) but could be
solved in principle using standard optimizers, given sufficient resources [ 22]. With respect to the
loss function Lτ(ατ), adopting 2|τ|rather than Gaussian noise scale σ2
τ,iis a heuristic that weights
lower degree residual queries such as the total query and 1-way residuals more heavily than higher
degree queries such as 3-way residuals. In contrast, using the Gaussian noise scale σ2
τ,iobtained from
both ResidualPlanner and the marginal decomposition in Theorem 2 weights higher degree residual
7queries more than lower degree residuals. When enforcing local non-negativity, it is beneficial for
reducing reconstruction error to allocate more weight to residuals that affect more marginals through
reconstruction. The present choice of weights 2|τ|for GReM-LNN, however, remain a heuristic. We
discuss this point further in Section 6.
4.4 Computational Complexity
We summarize the complexity results in Table 1. Formal statements and proofs appear in Appendix E.
LetSbe the set of measured residuals. To understand the results, suppose that Rτis measured once,
given by zτ, for each τ∈ S. Recall from Proposition 1 that computing Aγ,τzτtakesO(|γ|nγ)time.
This operation maps from the space of τ-residuals to γ-marginals. To reconstruct an answer to the
marginal query Mγ, we apply the invertible transformation from residuals to marginals by summing
over contributions for each τ⊆γto yield ˆµγ=P
τ∈S:τ⊆γAγ,τzτ. In the worst case, this requires
computing Aγ,τzτfor2|γ|residuals. Then the running time of reconstructing an answer to marginal
MγisO(|γ|nγ2|γ|). IfWis a workload of marginals, then reconstructing answers to each γ∈ W is
O(P
γ∈W|γ|nγ2|γ|). The following result shows that the complexity of reconstructing an answer to
marginal Mγis almost linear with respect to domain size.
Theorem 4. Forε >0, reconstructing an answer to Mγiso(n1+ε
γ)asni→ ∞ for some i∈γ.
Method Running Time
GReM-MLE( W,S, z) O(P
γ∈W|γ|nγ2|γ|)
EMP(W,Q, y) O(P
γ∈W|γ|nγ2|γ|)
One Round of GReM-LNN( W,S, z)O(P
γ∈W|γ|nγ2|γ|)
Table 1: Summary of Complexity Results
GReM-MLE, given in Alg. 3, consists of two steps: estimating residual answers ˆατfrom residuals
answers zτ,ifori= 1, . . . , k τand each τ∈S, and reconstructing answers to marginal workload
W. Recall that we suppose that covariance is proportional among measurements of a given residual
Rτ, soˆατcan be computed in closed-form as a weighted average in O(nτ)time. Then GReM-
MLE takes O(P
γ∈W|γ|nγ2|γ|)time. The efficient marginal pseudoinversion, given in Alg. 4,
first decomposes marginals and then applies GReM-MLE. Computing A+
γ,τyγtakesO(|γ|nγ)time,
so the running time of decomposing the marginal measurements is O(P
γ∈Q|γ|nγ2|γ|)where
Qis the set of marginals measured with isotropic Gaussian noise. Then the efficient marginal
pseudoinversion is O(P
γ∈W|γ|nγ2|γ|). Additionally, one round of GReM-LNN, given in Alg. 6, is
O(P
γ∈W|γ|nγ2|γ|).
5 Experiments
In this section, we measure the utility of GReM-MLE and GReM-LNN by incorporating them as a
post-processing step into two mechanisms for privately answering marginals: (1) ResidualPlanner
[6], and (2) a data-dependent mechanism we call Scalable MWEM. Both mechanisms measure
queries with Gaussian noise and reconstruct answers to all three-way marginals for the given data
domain. For the ResidualPlanner experiment, we measure residuals for all subsets of three or fewer
attributes with Gaussian noise scales determined by ResidualPlanner. For the Scalable MWEM
experiment, we measure the total query and a subset of the 3-way marginals in the data domain with
isotropic Gaussian noise and reconstruct answers to all 3-ways marginals using the efficient marginal
pseudoinversion in Alg. 4. We fully describe Scalable MWEM in Appendix G.
We compare average ℓ1error with respect to the reconstructed marginals of the base mechanism
to post-processing with GReM-LNN and two heuristics that enforce non-negativity by truncating
negative values to zero (Trunc) and truncating to zero then rescaling (Trunc+Rescale). For the Scalable
MWEM experiment, we additionally compare to a well-studied reconstruction mechanism Private-
PGM [ 12]. We run these methods on four datasets of varying size and scale, Titanic [ 23], Adult [ 24],
8101
100101102
Titanic
 Adult
101
100101101
100101102
Salary
101
100101
Nist-T axiAverage Workload Error (1)
ResidualPlanner Trunc Trunc+Rescale GReM-LNNFigure 2: Average ℓ1workload error on all 3-way marginals across five trials and privacy budgets
ϵ∈ {0.1,0.31,1,3.16,10}andδ= 1×10−9for ResidualPlanner.
Salary [ 25], and Nist-Taxi [ 26], and various practical privacy regimes, ϵ∈ {0.1,0.31,1,3.16,10}
andδ= 1×10−9. For each setting, we run five trials and report the average error of each method as
well as minimum/maximum bands. Additional details are provided in Appendix H.
5.1 ResidualPlanner Results
Fig. 2 displays results for the ResidualPlanner experiment. Across all privacy budgets and datasets
considered, GReM-LNN significantly reduces workload error on the reconstructed marginals com-
pared to ResidualPlanner. Averaging over all settings and trials, GReM-LNN reduces ResidualPlanner
workload error by a factor of 44.0 ×. With respect to the heuristic methods, GReM-LNN reconstructs
marginals with lower error than Trunc across all privacy budgets and datasets. Except at the highest
privacy regime considered ( ϵ= 0.1) on Titanic and Salary, GReM-LNN yields lower error than
Trunc+Rescale. Averaging over all settings and trials, GReM-LNN has lower workload error by a
factor of 17.6 ×compared to Trunc and 3.2 ×compared to Trunc+Rescale. Note that GReM-MLE is
omitted from Fig. 2 since ResidualPlanner is the maximum likelihood reconstruction for its measure-
ments. Appendix I reports results for this experiment with respect to ℓ2workload error, which are
consistent with the present findings.
5.2 Scalable MWEM Results
Fig. 3 displays results for the Scalable MWEM experiment for 30 rounds of measurements. Observe
that Scalable MWEM runs for the settings considered, which would be infeasible for the original
MWEM mechanism due to large data domains. Of all methods considered, Private-PGM yields the
greatest reduction in workload error in settings where it ran; however, Private-PGM failed due to
exceeding memory resources (20 GB) at 30 rounds on Adult, Salary, and Nist-Taxi in all trials. In
Appendix I, we report the settings in which Private-PGM successfully ran across 10, 20, and 30
rounds of Scalable MWEM.
With respect to GReM-LNN, the findings from the prior experiment agree with the present results.
Across all privacy budgets and datasets considered, GReM-LNN significantly reduces workload
error on the reconstructed marginals compared to Scalable MWEM. Averaging over all settings
and trials, GReM-LNN reduces Scalable MWEM workload error by a factor of 12.3 ×. Averaging
over all settings and trials, GReM-LNN has lower workload error by a factor of 1.1 ×compared to
Trunc+Rescale. Note that we suppress results for Trunc due to space. Appendix I reports results for
this experiment with respect to ℓ2workload error, which are consistent with the present findings.
9100101102
Titanic
 Adult
101
100101100101102
Salary
101
100101
Nist-T axiAverage Workload Error (1)
Scalable MWEM Trunc+Rescale GReM-LNN Private-PGMFigure 3: Average ℓ1workload error on all 3-way marginals across five trials and privacy budgets
ϵ∈ {0.1,0.31,1,3.16,10}andδ= 1×10−9for Scalable MWEM with 30 rounds of measurements.
6 Discussion
We develop ReM, a method for reconstructing answers to marginal queries that scales to large data
domains. We also introduce a tractable method to incorporate local non-negativity that significantly
improves reconstruction quality. Finally, we show that ReM can be used to improve the existing
query answering mechanisms ResidualPlanner and a scalable version of MWEM.
Limitations. Many data-dependent query answering mechanisms also generate synthetic data. In
some cases, practitioners utilize these mechanisms primarily in order to use the synthetic data for
downstream tasks such as training a machine learning model [ 27,28]. For those users, the fact that
ReM does not generate synthetic data would be an important limitation. A broader limitation, which
is common to many methods in this field, is lack of support for continuous data. Marginal and residual
queries are only defined on discrete domains so continuous attributes need to be discretized.
Future Work and Broader Impacts. While developing effective algorithms for privacy-preserving
data analysis is generally beneficial, it is known that these methods can lead to unfair outcomes
[29]. One direction for future work is to further understand the fairness properties of the methods we
present and how to mitigate any undesirable outcomes. Another direction for future work is further
understanding the weighting scheme used in GReM-LNN to apply local non-negativity. Preliminary
experiments show that weighting lower-order residual queries more highly in the loss function yields
reconstructed answers with lower workload error as well as faster and more reliable convergence of
the optimization routine. In general, the relationship between residual weights in the loss function,
optimizer convergence, and reconstruction quality is not well understood.
Acknowledgments and Disclosure of Funding
This work was supported by the National Science Foundation under grants CNS-1931686 and
CNS-2317232 (Kifer); CCF-2046235 (Musco); and IIS-1749854 and DBI-2210979 (Sheldon).
References
[1]John M Abowd, Robert Ashmead, Ryan Cumings-Menon, Simson Garfinkel, Micah Heineck,
Christine Heiss, Robert Johns, Daniel Kifer, Philip Leclerc, Ashwin Machanavajjhala, et al. The
2020 census disclosure avoidance system topdown algorithm. Harvard Data Science Review , 2,
2022.
10[2]Chao Li, Michael Hay, Vibhor Rastogi, Gerome Miklau, and Andrew McGregor. Optimizing
linear counting queries under differential privacy. In Proceedings of the twenty-ninth ACM
SIGMOD-SIGACT-SIGART symposium on Principles of database systems , pages 123–134.
ACM, 2010. doi: 10.1145/1807085.1807104.
[3]Chao Li and Gerome Miklau. An adaptive mechanism for accurate query answering under
differential privacy. PVLDB , 5(6):514–525, 2012.
[4]Chao Li, Gerome Miklau, Michael Hay, Andrew McGregor, and Vibhor Rastogi. The matrix
mechanism: optimizing linear counting queries under differential privacy. The VLDB Journal ,
24(6):757–781, 2015. doi: 10.1007/s00778-015-0398-x.
[5]Ryan McKenna, Gerome Miklau, Michael Hay, and Ashwin Machanavajjhala. Optimizing error
of high-dimensional statistical queries under differential privacy. Proceedings of the VLDB
Endowment , 11(10):1206–1219, 2018. doi: 10.14778/3231751.3231769.
[6]Yingtai Xiao, Guanlin He, Danfeng Zhang, and Daniel Kifer. An optimal and scalable matrix
mechanism for noisy marginals under convex loss functions. Advances in Neural Information
Processing Systems , 36, 2024.
[7]Ryan McKenna, Brett Mullins, Daniel Sheldon, and Gerome Miklau. Aim: An adaptive and
iterative mechanism for differentially private synthetic data. Proc. VLDB Endow. , 15(11):
2599–2612, Jul 2022. ISSN 2150-8097. doi: 10.14778/3551793.3551817. URL https:
//doi.org/10.14778/3551793.3551817 .
[8]Jun Zhang, Graham Cormode, Cecilia M. Procopiuc, Divesh Srivastava, and Xiaokui Xiao.
Privbayes: Private data release via bayesian networks. ACM Transactions on Database Systems
(TODS) , 42(4):25:1–25:41, 2017. doi: 10.1145/3134428. URL https://doi.org/10.1145/
3134428 .
[9]Terrance Liu, Giuseppe Vietri, and Steven Wu. Iterative methods for private synthetic data:
Unifying framework and new methods. In A. Beygelzimer, Y . Dauphin, P. Liang, and J. Wortman
Vaughan, editors, Advances in Neural Information Processing Systems , 2021.
[10] Sergul Aydore, William Brown, Michael Kearns, Krishnaram Kenthapadi, Luca Melis, Aaron
Roth, and Ankit A Siva. Differentially private query release through adaptive projection. In
Marina Meila and Tong Zhang, editors, Proceedings of the 38th International Conference on
Machine Learning , volume 139 of Proceedings of Machine Learning Research , pages 457–
467. PMLR, 18–24 Jul 2021. URL https://proceedings.mlr.press/v139/aydore21a.
html .
[11] Giuseppe Vietri, Cedric Archambeau, Sergul Aydore, William Brown, Michael Kearns, Aaron
Roth, Ankit Siva, Shuai Tang, and Steven Z Wu. Private synthetic data for multitask learning
and marginal queries. Advances in Neural Information Processing Systems , 35:18282–18295,
2022.
[12] Ryan McKenna, Daniel Sheldon, and Gerome Miklau. Graphical-model based estimation and
inference for differential privacy. In International Conference on Machine Learning , pages
4435–4444, 2019. URL http://proceedings.mlr.press/v97/mckenna19a.html .
[13] JN Darroch and TP Speed. Additive and multiplicative models and interactions. The Annals of
Statistics , pages 724–738, 1983.
[14] Stephen E. Fienberg and Alessandro Rinaldo. Computing maximum likelihood estimates in
log-linear models. Technical report, Technical Report 835, Department of Statistics, Carnegie
Mellon University, 2006.
[15] Moritz Hardt, Katrina Ligett, and Frank McSherry. A simple and practical algorithm for
differentially private data release. In Peter L. Bartlett, Fernando C. N. Pereira, Christopher
J. C. Burges, Léon Bottou, and Kilian Q. Weinberger, editors, Advances in Neural Information
Processing Systems 25: 26th Annual Conference on Neural Information Processing Systems
2012. Proceedings of a meeting held December 3-6, 2012, Lake Tahoe, Nevada, United States ,
pages 2348–2356, 2012. URL https://proceedings.neurips.cc/paper/2012/hash/
208e43f0e45c4c78cafadb83d2888cb6-Abstract.html .
11[16] Cynthia Dwork, Frank McSherry Kobbi Nissim, and Adam Smith. Calibrating noise to sensitiv-
ity in private data analysis. In TCC , pages 265–284, 2006. doi: 10.29012/jpc.v7i3.405.
[17] Cynthia Dwork and Aaron Roth. The Algorithmic Foundations of Differential Privacy . Found.
and Trends in Theoretical Computer Science, 2014. doi: 10.1561/0400000042.
[18] Ryan McKenna, Gerome Miklau, and Daniel Sheldon. Winning the nist contest: A scalable and
general approach to differentially private synthetic data. Journal of Privacy and Confidentiality ,
11(3), 2021.
[19] Kuntai Cai, Xiaoyu Lei, Jianxin Wei, and Xiaokui Xiao. Data synthesis via differentially private
markov random fields. Proceedings of the VLDB Endowment , 14(11):2190–2202, 2021.
[20] Ryan McKenna, Gerome Miklau, Michael Hay, and Ashwin Machanavajjhala. Hdmm: Opti-
mizing error of high-dimensional statistical queries under differential privacy. arXiv preprint
arXiv:2106.12118 , 2021.
[21] Aleksandar Nikolov, Kunal Talwar, and Li Zhang. The geometry of differential privacy: the
sparse and approximate cases. In Proceedings of the forty-fifth annual ACM symposium on
Theory of computing , pages 351–360, 2013.
[22] Stephen P Boyd and Lieven Vandenberghe. Convex optimization . Cambridge university press,
2004.
[23] Thomas Cason Frank E. Harrell Jr. Encyclopedia titanica.
[24] Ron Kohavi et al. Scaling up the accuracy of naive-bayes classifiers: A decision-tree hybrid. In
Kdd, volume 96, pages 202–207, 1996.
[25] Michael Hay, Ashwin Machanavajjhala, Gerome Miklau, Yan Chen, and Dan Zhang. Principled
evaluation of differentially private algorithms using dpbench. In Proceedings of the 2016
International Conference on Management of Data , pages 139–154, 2016.
[26] Gregoire Lothe, Christine Task, Slavitt Isaac, Nicolas Grislain, Karan Bhagat, and Gary S.
Howarth. Sdnist: Benchmark data and evaluation tools for data synthesizers. 2021.
[27] Lucas Rosenblatt, Xiaoyan Liu, Samira Pouyanfar, Eduardo de Leon, Anuj Desai, and Joshua
Allen. Differentially private synthetic data: Applied evaluations and enhancements. arXiv
preprint arXiv:2011.05537 , 2020.
[28] Yuntao Du and Ninghui Li. Towards principled assessment of tabular data synthesis algorithms.
arXiv preprint arXiv:2402.06806 , 2024.
[29] David Pujol, Ryan McKenna, Satya Kuppam, Michael Hay, Ashwin Machanavajjhala, and
Gerome Miklau. Fair decision making using privacy-protected data. In Proceedings of the 2020
Conference on Fairness, Accountability, and Transparency , pages 189–199, 2020.
[30] Brigitte Plateau. On the stochastic structure of parallelism and synchronization models for dis-
tributed algorithms. In Proceedings of the 1985 ACM SIGMETRICS conference on Measurement
and modeling of computer systems , pages 147–154, 1985.
[31] Mark Bun and Thomas Steinke. Concentrated differential privacy: Simplifications, extensions,
and lower bounds. In Theory of Cryptography Conference , pages 635–658. Springer, 2016. doi:
10.1007/978-3-662-53641-4_24.
[32] Clément L. Canonne, Gautam Kamath, and Thomas Steinke. The discrete gaussian for differ-
ential privacy. In NeurIPS , 2020. URL https://proceedings.neurips.cc/paper/2020/
hash/b53b3a3d6ab90ce0268229151c9bde11-Abstract.html .
[33] Yingtai Xiao, Zeyu Ding, Yuxin Wang, Danfeng Zhang, and Daniel Kifer. Optimizing fitness-
for-use of differentially private linear queries. Proceedings of the VLDB Endowment , 14(10):
1730–1742, 2021.
[34] Frank McSherry and Kunal Talwar. Mechanism design via differential privacy. In FOCS , 2007.
doi: 10.1145/2090236.2090254.
12[35] Mark Cesar and Ryan Rogers. Bounding, concentrating, and truncating: Unifying privacy
loss composition for data analytics. In Proceedings of the 32nd International Conference on
Algorithmic Learning Theory , volume 132 of Proceedings of Machine Learning Research , pages
421–457, 2021. URL https://proceedings.mlr.press/v132/cesar21a.html .
[36] Jerzy K Baksalary and Oskar Maria Baksalary. Particular formulae for the moore–penrose
inverse of a columnwise partitioned matrix. Linear algebra and its applications , 421(1):16–23,
2007.
13A Kronecker Products
Kronecker products are a convenient way to represent highly structured matrices. Let Abe an
ma×namatrix A=
a1,1··· a1,na......
ama,1···ama,na
andBbe amb×nbmatrix. Then the Kronecker
product of AwithBis an mamb×nanbmatrix given by A⊗B=
a1,1B··· a1,naB
......
ama,1B···ama,naB
.
Kronecker products provide a compact representation of matrices by representing exponentially-many
entries of A⊗Bwith linearly-many entries in AandB. For the Kronecker product of a sequence of
matrices A1, . . . , A d, we use the notation
dO
i=1Ai=A1⊗ ··· ⊗ Ad
The Kronecker product is associative, so pairwise products can be taken in any order.
Kronecker products additionally possess useful algebraic properties. Let (·)+denote Moore-Penrose
pseudoinverse.
Proposition 3. (Kronecker Product Properties) Let A=Nd
i=1AiandB=Nd
j=1Bj. Then the
following properties hold:
1.A⊤=Nd
i=1A⊤
i.
2.A+=Nd
i=1A+
i.
3. IfAiandBiare compatible for multiplication for i= 1, . . . , d , then AB=Nd
i=1AiBi.
There are efficient algorithms for matrix-vector multiplication utilizing Kronecker structure such as
Alg. 5. Let A=Nℓ
i=1Aibe a Kronecker structured matrix where Aiis a matrix of size ai×biso
thatAhas size a×bwitha=Qℓ
i=1aiandb=Qℓ
i=1bi.
Algorithm 5 Kronecker Matrix-Vector Product [20, 30]
Input: Matrix A=Nℓ
i=1Ai, vector x
ai, bi=SHAPE (Ai)
r=Qℓ
i=1bi
x1=x
fori= 1, . . . , ℓ do
Z=RESHAPE (xi, bi,r/bi)
r=r·ai/bi
xi+1=RESHAPE (AiZ, r,1)
return xℓ+1
B Differential Privacy
Let us begin by introducing a useful variant of differential privacy: zero-concentrated differential
privacy (zCDP).
Definition 2. (Zero-Concentrated Differential Privacy; [ 31]) LetM:X → Y be a randomized
mechanism. For any neighboring datasets p, p′that differ by at most one record, denoted p∼p′,
and all measurable subsets S⊆ Y: ifDγ(M(p)||M(p′))≤ργfor all γ∈(1,∞)where Dγis the
γ-Renyi divergence between distributions M(p),M(p′), thenMsatisfies ρ-zCDP.
While (ϵ, δ)-DP is a more common notion, it is often more convenient to work with zCDP. There
exists a conversion from zCDP to (ϵ, δ)-DP.
14Proposition 4 (zCDP to DP Conversion; [ 32]).If mechanism Msatisfies ρ-zCDP , then Msatisfies
(ϵ, δ)-DP for any ϵ >0andδ= min α>1exp(( α−1)(αρ−ϵ))
α−1 
1−1
αα.
Next, we introduce two building block mechanisms. An important quantity in analyzing the privacy
of a mechanism is sensitivity. The ℓksensitivity of a function f:X → Ris given by ∆k(f) =
max p∼p′∥f(p)−f(p′)∥k. Iffis clear from the context, we write ∆k.
Proposition 5 (zCDP of Gaussian mechanism; [ 31]).LetWbe an m×nworkload. Given data
vector p, the Gaussian mechanism adds i.i.d. Gaussian noise to Wpwith scale parameter σi.e.,
M(p) = Wp+σ∆2(W)N(0,I), where Iis the m×midentity matrix. Then the Gaussian
Mechanism satisfies1
2σ2-zCDP .
Proposition 6 (zCDP of correlated Gaussian mechanism; [ 33]).LetWbe an m×nworkload. Given
data vector p, the correlated Gaussian mechanism adds Gaussian noise to Wp with covariance
matrix Σi.e.,M(p) =Mp+N(0,Σ). The correlated Gaussian mechanism satisfiesγ
2-zCDP where
γis the largest diagonal element of M⊤Σ−1M.
Proposition 7 (zCDP of exponential mechanism; [ 34,35]).Letϵ >0andScore r:X → Rbe
a quality score of candidate r∈ R for data vector p. Then the exponential mechanism outputs a
candidate r∈ R according to the following distribution: Pr(M(p) =r)∝exp ϵ
2∆1Score r(p)
.
The exponential mechanism satisfiesϵ2
8-zCDP .
Adaptive composition and post-processing are two important properties of differential privacy that
allow us to construct complex mechanisms from the above building blocks. Let us state these results
for zCDP.
Proposition 8 (zCDP Properties; [ 31,35]).zCDP satisfies these two properties of differential privacy:
1.(Adaptive Composition) Let M1:X → Y 1satisfy ρ1-zCDP and M2:X × Y 1→ Y 2satisfy
ρ2-zCDP . The mechanism p7→ M 2(p,M1(p))satisfies (ρ1+ρ2)-zCDP .
2.(Post-processing) Let M1:X → Y satisfy ρ-zCDP and f:Y → Z be a randomized algorithm.
ThenM:X → Z =f◦ M 1satisfies ρ-zCDP .
C Relationship between Marginals and Residuals
In this section, we prove Proposition 1, which provides a relationship between marginals and residuals.
Before proving this result, let us consider residual workloads as well as the subtraction matrix D(k).
Let us state some properties of residuals.
Proposition 9 (Residual Properties; [ 6,13,14]).LetΩbe the set of all tuples of attributes for a
given data universe X.
1.Rτis anmτ×nmatrix with full row rank.
2.Rτ, Rτ′are mutually orthogonal for τ̸=τ′i.e.RτR⊤
τ′=0.
3.Rτ, Mτ′are mutually orthogonal for τ̸⊆τ′i.e.RτM⊤
τ′=0.
4.(Rτ)τ∈ΩspansRn.
Lemma 2. Data vector p∈Rncan be decomposed uniquely as follows: p=P
τ∈ΩR⊤
τvτfor
vτ∈Rmτ.
Proof. Letpτ=R+
τRτpbe the projection of ponto the row-space of Rτ. By Proposition 9,
p=P
τ∈Ωpτ. Letvτ∈Rmτbe such that pτ=R⊤
τvτ. Since Rτis full row rank, vτis unique.
Now, let us consider D+
(k). Recall that D(k)is annk−1×nkmatrix given by
D(k)=
1−1 0 ··· 0
0 1 −1··· 0
............
0··· ··· 1−1
.
15The pseudoinverse of D(k)is known in closed-form:
D+
(k)=1
nk
nk−1nk−2··· 1
−1 nk−2··· 1
−1 −2··· 1
.........
−1 −2··· − (nk−1)

= (1/nk)(1ku⊤
k−nkCk),
where uk=
nk−1
nk−2
...
1
andCkis the nk×nk−1lower triangular matrix of ones.
Continuing the example from Fig. 1,
D(k)=
1−1 0
0 1 −1
D+
(k)=1
3"2 1
−1 1
−1−2#
.
Proposition 1. LetRS= (Rτ)τ∈Sbe a combined workload of residual queries for all τin a
collection S ⊆2[d], where the individual matrices Rτare stacked vertically. The size of RSism×n
where m=P
τ∈Smτ. Then for any z= (zτ)τ∈S∈Rmand any γ, it holds that
MγR+
Sz=X
τ∈S,τ⊆γAγ,τzτ, where Aγ,τ:=dO
k=1

D+
(k)k∈τ 
1/nk
1kk∈γ\τ
1 k /∈γforτ⊆γ.
The matrix Aγ,τhas size nγ×mτand maps from the space of τ-residuals to the space of γ-marginals.
The running time to compute Aγ,τzτisO(|γ|nγ).
Proof of Proposition 1. First note that R+
Sis the pseudoinverse of a block matrix. In general the
pseudoinverse of a vertical block matrix involves the pseudoinverse of each block multiplied by a
projection matrix [ 36]. In this case each block is a residual query, as discussed in Proposition 9, these
query matrices are mutually orthogonal so the pseudoinverse R+
Shas the form (R+
τ)T
τ∈S. Here, the
combined query matrix RSis constructed by stacking the blocks Rτvertically and the combined
pseudoinverse R+
Sstacks the blocks R+
τhorizontally. Given this block structure of R+
Swe can write
R+
Sz=X
τ∈SR+
τzτ=⇒ MγR+
Sz=X
τ∈SMγR+
τzτ. (3)
Another relevant property of residual queries given in Proposition 9 is that RτM⊤
τ′=0for
τ̸⊆τ′. When we drop these orthogonal queries from the summation, we get MγR+
Sz=P
τ∈S,τ⊆γMγR+
τzτ. When computing the product MγR+
τ, several properties of Kronecker products
given in Proposition 3 are relevant. The first is that (A⊗B)+=A+⊗B+. Applying this property
gives
R+
τ=dO
k=1(
D+
(k)k∈τ 
1⊤
k+k /∈τ. (4)
The next property is that when when AandBboth have compatible Kronecker structure, AB=N
iAiBi. Both MγandR+
τhave compatible Kronecker structure so we can write
MγR+
τ=dO
k=1

IkD+
(k)k∈τ
Ik 
1⊤
k+k∈γ\τ
1⊤
k 
1⊤
k+k /∈γ. (5)
16To evaluate this, notice that (1T
k)+= 1k(1T
k1k)−1= 1k/nkand1⊤
k(1k/nk) = 1 . Plugging this into
the equation above we get
Aγ,τ=MγR+
τ=dO
k=1

D+
(k)k∈τ
1k/nkk∈γ\τ
1 k /∈γ. (6)
Finally, this gives the full result that MγR+
Sz=P
τ∈S,τ⊆γAγ,τzτ.
We prove the time complexity result for Aγ,τzτin Appendix E.
D ReM Proofs
In this section, we prove results related to ReM from Sections 3 and 4.
Theorem 1. Suppose ˆατminimizes Lτ(ατ)overRmτfor each τ∈ Sand let ˆα= (ˆατ)τ∈S. Then
Alg. 2 outputs ˆµγ=Mγˆp, where ˆp=R+
Sˆαis a global minimizer of the combined loss functionP
τ∈SLτ(Rτp)overRn.
Proof. Suppose ˆατminimizes Lτfor all τand let ˆp=R+
Sˆα. Then for any p∈Rn
X
τ∈SLτ(Rτˆp) =X
τ∈SLτ(RτR+
Sˆα)(⋆)=X
τ∈SLτ(ˆατ)≤X
τ∈SLτ(Rτp). (7)
We will justify Equality (⋆)below. The inequality holds because ˆατminimizes Lτ. Thus, Equation (7)
shows that ˆpminimizes the combined lossP
τ∈SLτ(Rτp).
To justify Equality (⋆), first observe that RSR+
Sˆα= ˆαbecause RShas full row rank and thus
RSR+
S=I. We can see RShas full row rank by Proposition 9: each block of rows corresponding to
one residual has full row rank and these blocks are orthogonal. The equality RτR+
Sˆα= ˆατis obtained
by selecting the block of rows corresponding to residual τfrom the equality RSR+
Sˆα= ˆα.
Lemma 1. Forτ⊆γ, the residual Rτcan be recovered from the marginal Mγas
Rτ=A+
γ,τMγwhere A+
γ,τ=dO
k=1

D(k)k∈τ
1T
k k∈γ\τ
1 k /∈γ.
Proof of Lemma 1. Recall that we defined Aγ,τ=MγR+
τ. Then A+
γ,τ=RτM+
γ.Observe the
following:
A+
γ,τMγ=dO
k=1

D(k)Ikk∈τ
1⊤
kIk k∈γ\τ
1·1⊤
kk /∈γ
=dO
k=1D(k)k∈τ
1⊤
k k /∈τ
=Rτ
Theorem 2. Letyγ∼ N (Mγp, σ2I)be a noisy marginal measurement with isotropic Gaus-
sian noise and let zτ=A+
γ,τyγfor each τ⊆γ. Then noisy residual zτhas distribution
N(Rτp, σ2DτD⊤
τQ
k∈γ\τnk)andzτis independent of zτ′forτ̸=τ′.
17Furthermore, let Hγ= (A+
γ,τ)τ⊆γbe the matrix mapping from yγto(zτ)τ⊆γ. This matrix is
invertible, which implies that
logN(yγ|Mγp, σ2I) =X
τ⊆γlogN
zτRτp, σ2DτD⊤
τY
k∈γ\τnk
+ log|detHγ|. (1)
Proof of Theorem 2. Since yτ∼ N(Mγp, σ2I)andzτ=A+
γ,τyτ, standard properties of normal
distributions give that zτ∼ N(A+
γ,τMγp, σ2A+
γ,τ(A+
γ,τ)⊤). By Lemma 1, the mean is equal to Rτp,
as stated. For the covariance
A+
γ,τ(A+
γ,τ)⊤=

D(k)D⊤
(k)k∈τ
1⊤
k1k k∈γ\τ
1 k /∈γ
=Y
k∈γ\τnk·dO
k=1(
D(k)D⊤
(k)k∈τ
1 k /∈τ
=DτD⊤
τY
k∈γ\τnk
so the covariance is σ2DτD⊤
τQ
k∈γ\τnk, as stated.
Forτ̸=τ′the vectors zτandzτ′are jointly normal with covariance σ2A+
γ,τ(A+
γ,τ′)⊤. We will show
thatA+
γ,τ(A+
γ,τ′)⊤is a matrix of zeros, so the covariance matrix is identically zero and the vectors
are independent. By the Kronecker structure,
A+
γ,τ(A+
γ,τ′)⊤=dO
k=1

D(k)D⊤
(k)k∈τ∩τ′
1⊤
kD⊤
(k)k∈τ′\τ
D(k)1k k∈τ\τ′
1⊤
k1k k∈γ\(τ∪τ′)
1 k /∈γ
Observe that D(k)1k= 0is a vector of zeros because the rows of D(k)sum to zero, and similarly
1⊤
kD⊤
(k)is a row vector of zeros. Thus, any kin the symmetric difference (τ′\τ)∪(τ\τ′)will
contribute an all zeros matrix to the Kronecker product and cause A+
γ,τ(A+
γ,τ′)⊤to be an all zeros
matrix. But there must be at least one kin the symmetric difference because τ̸=τ′. This proves that
the covariance matrix is identically zero, as desired.
We will next show that the mapping Hγis invertible. Hγis a matrix with blocks A+
γ,τfor each τ⊆γ,
stacked vertically, and nγ=Q
k∈γnkcolumns. From the definition of the block A+
γ,τ, we can see it
hasmτ=Q
k∈τ(nk−1)rows and is of full row rank because D(k)is a full rank matrix with nk−1
rows and the other matrices in the Kronecker product have only one row. We showed above that
A+
γ,τ(A+
γ,τ′)⊤= 0forτ̸=τ′, which means that the blocks of Hγhave mutually orthogonal rows,
and combined with the fact that each block has full row rank this means that Hγhas rank equal to the
total number of rows. This number of rows isP
τ⊆γmτ=P
τ⊆γQ
k∈τ(nk−1) =Q
k∈γnk=nγ,
which equals the number of columns, and therefore Hγinvertible.2
Now, given what we have shown so far, we will write two different expressions for the log-probability
density function logpz(z)where z= (zτ)τ⊆γ. First, we have already derived the joint multivariate
distribution of z, which, due to independence, has log-density
logpz(z) =X
τ⊆γlogN
zτRτp, σ2DτD⊤
τY
k∈γ\τnk
.
2To see thatP
τ⊆γQ
k∈τ(nk−1) =Q
k∈γnk, observe that nγ=Q
k∈γnkcounts the number of ways to
map each k∈γto a value i∈ {1, . . . , n k}. Equivalently, we may consider selecting a subset τ⊆γ, assigning
eachk∈τto the value 1, and then assigning each k /∈τto one of the remaining values in {2, . . . , n k}. The
number of ways to do this isP
τ⊆γQ
k∈τ(nk−1) =Q
k∈γnk.
18Second, because z=Hγyγfor the multivariate normal random variable yγ, the change of variable
formula for probability densities gives that
logpz(z) = log N(yγ|Mγp, σ2I)−log|detHγ|.
Equating these two expressions gives Equation (1), which completes the proof.
Theorem 3 (Efficient pseudoinversion of marginal query matrix) .LetMQ= (Mγ)γ∈Qbe the
query matrix for a multiset Qof marginals and let y= (yγ)γ∈Qbe corresponding noisy marginal
measurements with yγ=Mγp+N(0, σ2I). LetS={τ⊆γ:γ∈ Q} and for each τ∈ S letγτ,i
be the ith marginal in Qcontaining τ. Letzτ,i=A+
γτ,i,τyγτ,ibe the residual measurement obtained
fromγτ,iand let Στ,i=σ2
τ,iDτD⊤
τbe its covariance where σ2
τ,i=σ2Q
k∈γτ,i\τnk. Then, given
any workload of marginal queries W, for each γ∈ W , the marginal reconstruction ˆµγobtained
from Algorithm 3 on these residual measurements is equal to MγM+
Qy.
Proof. By standard properties of the pseudoinverse, M+
Qyis the unique vector that minimizes
SE(p) =∥MQp−y∥2
2and is in the row span of MQ. We will show that the vector R+
Sˆαsatisfies
both properties, where ˆα= (ˆατ)τ∈Sis constructed in Algorithm 3, and thus R+
Sˆα=M+
Qy. Then,
by Proposition 1, the reconstructed marginal ˆµγin Algorithm 3 is equal to MγR+
Sˆαand hence also
equal to MγM+
Qy, as claimed.
We will first show R+
Sˆαminimizes SE(p). Observe that the SE(p)is equivalent to the negative
log-likelihood Ly(p)of the marginal measurements y:
SE(p) =∥MQp−y∥2
2
=X
γ∈Q∥Mγp−yγ∥2
2
=−2σ2X
γ∈QlogN(yγ|Mγp, σ2I) +const.
= 2σ2Ly(p) +const.
Therefore SE (p)andLy(p)have the same minimizers.
Then, by Theorem 2,
Ly(p) =X
γ∈Q−logN(yγ|Mγp, σ2I)
=X
γ∈QX
τ⊆γ−logN(A+
γ,τyτ|Rτp, σ2Y
k∈γ\τnk·DγD⊤
γ) +const.
=X
τ∈SkτX
i=1−logN(zτ,i|Rτp, σ2
τ,iDγD⊤
γ)
| {z }
Lz(p)+const.
where in the final line we rearranged terms using the notation of the theorem statement.
Therefore, SE (p),Ly(p)andLz(p)all have the same minimizers.
Furthermore, Lz(p)decomposes over residual measurements as Lz(p) =P
τ∈SLτ(Rτp)where
Lτ(ατ) =Pkτ
i=1−logN(zτ,i|ατ, σ2
τ,iDγD⊤
γ). Therefore, Theorem 1 allows us to minimize each
term separately. Algorithm 3 finds ˆατto minimize Lτ(ατ)for each τ∈ S using inverse variance
weighting. Then, by Theorem 1, the vector R+
Sˆαis a minimizer of Lz(p), and therefore also a
minimizer of SE (p).
It remains to show that R+
Sˆα∈row(MQ). This is true because R+
Sˆα∈col(R+
S) = row(RS)⊆
row(MQ).3The final inclusion is true by Lemma 1, since for each τ∈ S we have Rτ=A+
γ,τMγ
for some γ∈ Q.
3In fact row (RS) =row(MQ)but we only need the inclusion.
19Let us now discuss a generalization of Theorem 3 to the case where noise scales vary across marginal
measurements.
Theorem 5. LetMQ= (Mγ)γ∈Qbe the query matrix for a multiset Qof marginals and let
y= (yγ)γ∈Qbe corresponding noisy marginal measurements with yγ=Mγp+N(0, σ2I). Define
the scaled query matrix for QasVQ= (Vγ)γ∈Qwhere Vγ=1
σγMγand the scaled marginal
measurements as v= 
vγ
γ∈Qwhere vγ=1
σγyγ. LetS={τ⊆γ:γ∈ Q} and for each τ∈ S
letγτ,ibe the ith marginal in Qcontaining τ. Letzτ,i=A+
γτ,i,τyγτ,ibe the residual measurement
obtained from γτ,iand let Στ,i=σ2
τ,iDτD⊤
τbe its covariance where σ2
τ,i=σ2Q
k∈γτ,i\τnk.
Then, given any workload of marginal queries W, for each γ∈ W , the marginal reconstruction ˆµγ
obtained from Algorithm 3 on these residual measurements is equal to MγV+
Qv.
The result follows due to the following Lemma, which shows that V+
Qyis an MLE for pgiven the
noisy marginal measurements y.
Lemma 3. LetMQ= (Mγj)r
j=1be the query matrix for marginals Q= (γ1, . . . , γ r), which may
include duplicates, and let y= (yγj)r
j=1be corresponding noisy marginal measurements with
yγj=Mγjp+N(0, σ2
γjI). Define the scaled query matrix as VQ= (Vγj)r
j=1where Vγj=1
σγjMγj
and the scaled marginal measurements as v= 
vγjr
j=1where vγj=1
σγjyγj. Then V+
Qvis a MLE
ofpwith respect to noisy measurements y.
Proof. The log-likelihood of data vector punder noisy marginal measurement yγjcan be written as
Lyγj(p) =−1
2σ2γjyγj−Mγjp2
2+cγj
where cγjis a constant. Since the noisy marginal measurements are independent, the log-likelihood
of data vector punder noisy marginal measurements yis given by
Ly(p) =−1
2rX
j=11
σ2γjyγj−Mγjp2
2+c
where cis a constant. The vector ˆpis an MLE of punder noisy marginal measurements yif and only
ifˆpminimizes the loss function
Ly(p) =rX
j=11
σ2γjyγj−Mγjp2
2
=rX
j=11
σγj
yγj−1
σγj
Mγjp2
2
=rX
i=1vγj−Vγjp2
2
=∥v−VQp∥2
2.
Since V+
Qvminimizes Ly(p), it is an MLE of punder noisy marginal measurements y.
E Computational Complexity
In this section, we analyze the computational complexity of applications of ReM under Gaussian
noise. We state and prove the results discussed in Section 4.4. Let us first prove two useful lemmas
regarding the time complexity of Alg 5 for multiplying the Kronecker matrix A=Nℓ
i=1Aiby a
vector x. Recall that Aihas size ai×biandAhas size a×bwitha=Qℓ
i=1aiandb=Qℓ
i=1bi.
Lemma 4. At iteration i, Alg. 5 has the following time complexity:
(a) if Aiis an arbitrary matrix, iteration itakesO Qi
j=1ajQℓ
h=ibh
time.
20(b) if Ai=D+
(k), then iteration itakesO Qi−1
j=1ajQℓ
h=ibh
time, where bi=nk−1.
(c) if Ai=D(k), then iteration itakesO Qi
j=1ajQℓ
h=i+1bh
time, where ai=nk−1.
Proof. At iteration iof Alg. 5, Aiis multiplied by a matrix Zwith size bi× Qi−1
j=1ajQℓ
h=i+1bh
.
Then each row in Airequires bi Qi−1
j=1ajQℓ
h=i+1bh
= Qi−1
j=1ajQℓ
h=ibh
scalar multiplications.
Since Aihasairows, this yields Qi
j=1ajQℓ
h=ibh
multiplications over all rows. This proves (a).
Suppose Ai=D+
(k). Recall that D+
(k)= (1/nk)(1ku⊤
k−nkCk). We claim that computing D+
(k)v
for any vector vtakesO(nk)time. Ckvis a cumulative sum of the elements of vandu⊤
kvis
a dot product, both of which take O(nk)time to compute. The remaining steps cost 2(nk−1)
multiplications and nk−1sums. Thus each column of Zcan be multiplied by AiinO(nk)time.
Since Zhas Qi−1
j=1ajQℓ
h=i+1bh
columns, computing AiZtakesO 
bi Qi−1
j=1ajQℓ
k=i+1bk
=
O Qi−1
j=1ajQℓ
k=ibk
time, where bi=nk−1. This proves (b).
Suppose Ai=D(k). For vector v,D(k)vis the difference of consecutive elements of v, which takes
O(nk)time to compute. Thus each column of Zcan be multiplied by Aiwithnk−1operations.
Since Zhas Qi−1
j=1ajQℓ
h=i+1bh
columns, computing AiZtakesO 
ai Qi−1
j=1ajQℓ
k=i+1bk
=
O Qi
j=1ajQℓ
k=i+1bk
time, where ai=nk−1. This proves (c).
Lemma 5. The following hold for Alg. 5:
(a) If ai≥biand either Ai=D+
(k)orbi= 1fori= 1, . . . , ℓ , then Alg. 5 takes O 
a·ℓ)time.
(b) If ai≤biand either Ai=D(k)orai= 1fori= 1, . . . , ℓ , then Alg. 5 takes O 
b·ℓ)time.
Proof. Applying Lemma 4, if Ai=D+
(k)then iteration itakesO Qi−1
j=1ajQℓ
h=ibh
time, and,
ifbi= 1, then iteration itakesO Qi
j=1ajQℓ
h=i+1bh
time. We can bound these terms by
O Qℓ
j=1aj
=O(a). Summing over all ℓiterations of Alg. 5 yields O(Pℓ
i=1a) =O(a·ℓ). This
proves (a).
IfAi=D(k), then iteration iisO Qi
j=1ajQℓ
h=i+1bh
by Lemma 4 (c). Ifai= 1, then iteration
iisO Qi−1
j=1ajQℓ
h=ibh
by Lemma 4 (a). We can bound these terms by O Qℓ
h=1bh
=O(b).
Summing over all ℓiterations of Alg. 5 yields O(Pℓ
i=1b) =O(b·ℓ). This proves (b).
Theorem 6. LetWbe a workload of marginals. Then
(a) Reconstructing an answer to marginal Mγforγ∈ W takesO(|γ|nγ2|γ|)time.
(b)The time required for reconstructing an answer to marginal Mγforγ∈ W iso(n1+ϵ
γ)for
anyϵ >0asni→ ∞ for some i∈γ.
(c) GReM-MLE (W,S, z)takesO(P
γ∈W|γ|nγ2|γ|)time.
(d) EMP (W,Q, y)takesO(P
γ∈W|γ|nγ2|γ|)time.
(e) GReM-LNN (W,S, z)takesO(P
γ∈W|γ|nγ2|γ|)time per round.
Proof. Let us first consider the running time of Aγ,τzτfor some τ⊆γ. Recall that Aγ,τcan be
written as follows:
Aγ,τ:=O
k∈γ(
D+
(k)k∈τ 
1/nk
1kk∈γ\τ
Since Aγ,τsatisfies the conditions of Lemma 5 and has nγrows, computing Aγ,τzτtakesO(|γ|nγ)
time. Recall from Proposition 1 that reconstructing an answer to marginal Mγis given by
21P
τ∈S,τ⊆γAγ,τyτ. The number of terms in the summation is at most 2|γ|, so the total running
time of reconstructing an answer to MγisO(|γ|nγ2|γ|). This proves (a).
For(b), letϵ >0and consider the following quotient:
|γ|nγ2|γ|
|γ|n1+ϵγ=2|γ|
nϵγ=2|γ|
Q
i∈γnϵ
i.
Taking the limit as ni→ ∞ , the quotient tends to zero and we obtain the desired result.
With GReM-MLE, each residual query Rτ, τ∈ Scan have multiple measurements yτ,1, . . . , y τ,kτ
but with proportional covariances. For each τ∈ S, we combine the measurements using inverse
variance weighting to obtain ˆατ. We then reconstruct the marginals Mγforγ∈ W using the residual
answers ˆατforτ∈ S. By(a), the running time is O(P
γ∈W|γ|nγ2|γ|). This proves (c).
The efficient marginal pseudoinversion, given in Alg. 4, first decomposes marginals and then applies
GReM-MLE. Let Qbe the multiset of measured marginals and Wbe the workload of marginals to
answer. Let W↓denote the downward closure of W. We assume that Qis a consists of elements
ofW↓and each γ∈ W↓appears in Qat most btimes. For each γ∈ Q , we decompose the
marginal measurements into residual measurements by computing A+
γ,τyγfor each τ⊆γ. By
Lemma 5 (b), computing A+
γ,τyγtakesO(|γ|nγ)time. Then the running time of decomposing
the marginal measurements is O(P
γ∈Q|γ|nγ2|γ|). From (c), the running time of GReM-MLE is
O(P
γ∈W|γ|nγ2|γ|). Given that the running time of decomposition is at most a multiple of the
running time of GReM-MLE, O(P
γ∈W|γ|nγ2|γ|). This proves (d).
Let us turn to the running time of GReM-LNN. Let W↓be the downward closure of workload W.
The dual ascent algorithm for GReM-LNN (Alg. 6) consists of three steps each round requiring
matrix multiplications: computing ˆατforτ∈ S, computing ˆατ′for unmeasured τ′∈ W↓\ S, and
reconstructing answers to marginals Mγforγ∈ W .
First consider the case where τ∈S. Recall that in this case ˆατ= Pkτ
i=1K−1
τ,i−1 Pkτ
i=1K−1
τ,iyτ,i+P
γ⊇τAT
γ,τλγ
, where Kτ,i=σ2
τDτDT
τ. We can rewrite ˆατas follows:
ˆατ= X
γ⊇τσ−2
τ!−1X
γ⊇τσ−2
τyτ,i+ X
γ⊇τσ−2
τ!
DτDT
τX
γ⊇τAT
γ,τλγ.
The left summand requires no matrix multiplications and does not depend on λ. Then computing
AT
γ,τλγtakesO 
|γ|nγ)time. The right summand is obtained by computing AT
γ,τλγfor each γ⊇τ.
Then computing ˆατforτ∈ S takesO(P
γ⊇τ|γ|nγ)time.
Now consider the case where τ∈ W↓\S. Then ˆα=−(1/2)(AT
τ,τAτ,τ)−1P
γ⊇τAT
γ,τλγ. As with
the prior case, the desired term requires computing AT
γ,τλγfor each γ⊇τ. Then computing ˆατfor
τ∈ W↓\ SisO(P
γ⊇τ|γ|nγ).
Combing these results, computing ˆατforτ∈ W↓isO(P
τ∈W↓P
γ⊇τ|γ|nγ). Observe that for
eachγ∈ W , there are 2|γ|terms in the summation. By indexing the summation in terms of γ, we
obtain that computing ˆαisO(P
γ∈W|γ|nγ2|γ|). The remaining step of GReM-LNN is to reconstruct
answers to marginals Mγforγ∈ W . By (a), the running time is O(P
γ∈W|γ|nγ2|γ|). This
proves (e).
F GReM-LNN Implementation
Recall that GReM-LNN solves the following convex program:
min
αX
τ∈SX
i(ατ−zτ,i)⊤K−1
τ,i(ατ−zτ,i)s.t.X
τ⊆γAγ,τατ≥0,∀γ∈ W (8)
22Algorithm 6 GReM-LNN Dual Ascent
Input: Marginal workload W, residual workload S, residual measurements z, rounds T, step size s,
Lagrangian initialization λ, regularization weight η
1:Initialize λγ=λforγ∈ W
2:fort= 1, . . . , T do
3: Setατ= Pkτ
i=1Kτ,i−1(Pkτ
i=1K−1
τ,iyτ,i−P
γ⊇τA⊤
γτλγ)forτ∈ S
4: Setατ=−1/2η 
A⊤
ττAττ−1(P
γ⊇τA⊤
γτλγ)⊤forτ∈ W↓\ S
5: Calculate µγ(α) =P
τ⊆γAγτατforγ∈ W
6: Update λγ= min {λγ+sµγ(α),0}forγ∈ W
forKτ,i= 2|τ|DτDT
τ. Observe that the program in Eq. 8 only depends on unmeasured residuals in
W↓through the local non-negativity constraint. To make this problem more tractable and the solution
more stable, we introduce a regularization term to limit the contribution of unmeasured residuals to
reconstructed marginals:
min
αX
τ∈SX
i(ατ−zτ,i)⊤K−1
τ,i(ατ−zτ,i) +ηX
ν∈W↓\S∥Aνναν∥2
2
s.t.X
τ⊆γAγ,τατ≥0,∀γ∈ W(9)
Note that the introduction the regularization term in Eq. (9) is only relevant to the underdetermined
case, since, otherwise, W↓⊆ S. To solve the program in Eq. (9), we use an iterative dual ascent
algorithm described in pseudocode in Alg. 6.
Let us now show that Alg. 6 is correctly specified. Let us denote the objective as f(α) =P
τ∈SPkτ
i=1(ατ−zτ,i)⊤K−1
τ,i(ατ−zτ,i) +ηP
ν∈W↓\S∥Aνναν∥2
2and the constraint as µ(α) =
(µγ(α))γ∈W= (P
τ⊆γAγτατ)γ∈W≥0. Then the Lagrangian function is given by
L(α, λ) =f(α) +λ⊤µ(α)
=X
τ∈SkτX
i=1(ατ−zτ,i)⊤K−1
τ,i(ατ−zτ,i) +ηX
ν∈W↓\S∥Aνναν∥2
2+X
γ∈Wλ⊤
γX
τ⊆γAγτατ
where λ= (λγ)γ∈Wis the dual variable or Lagrangian multiplier and is constrained such that λ≤0.
The dual function is given by g(λ) = min αL(α, λ)and the dual problem is given by max λ≤0g(λ).
Under suitable regularity conditions, the optimal value of the dual problem is equivalent to the optimal
value of the primal problem. We can solve both by maximizing the dual function gto obtain λ∗and
then minimizing the Lagrangian L(α, λ∗)with respect to αto obtain α∗.
We can solve for each α∗
τin closed form for τ∈ W↓. Minimizing the Lagrangian L(α, λ∗)with
respect to αcorresponds to minimizing an unconstrained quadratic objective and can be solved
separately for each τ. To see this, let us fix λand solve for the critical point of L(α, λ). Ifτ∈ S,
then gradient of Lwith respect to ατis given by
∇ατL(α, λ) =kτX
i=1K−1
τ,i(ατ−zτ,i) +X
γ⊇τA⊤
γτλγ.
Setting this to zero and solving for α∗
τyields
α∗
τ=kτX
i=1K−1
τ,i−1kτX
i=1K−1
τ,izτ,i−X
γ⊇τA⊤
γτλγ
.
23Algorithm 7 Scalable MWEM
Input: Marginal workload W, privacy budget (ϵ, δ), initialization parameter α
1:Choose ρsuch that minα>1exp(( α−1)(αρ−ϵ))
α−1 
1−1
αα=δ
2:Setσ2
0, σ2=1
2αρ,T
(1−α)ρ
3:Initialize measurements y={M∅p+ξ0}withξ0∼ N(0, σ2
0I)and multiset Q={∅}
4:Initialize (ˆµγ)γ∈W=EMP(W,Q, y)
5:fort= 1, . . . , T do
6: Select γtwith the exponential mechanism using(1−α)ρ
2Tbudget according to
Score (p, γ, Y ) =∥Mγp−ˆµγ∥1∀γ∈ W
7: Measure yt=Mγtp+ξtwhere ξt∼ N(0, σ2I)and set Q=Q ∪ { γt}
8: Reconstruct (ˆµγ)γ∈W=EMP(W,Q, y)
return noisy answers (ˆµγ)γ∈W, noisy measurements y
Now, suppose τ∈ W↓\ S. Then gradient of Lwith respect to ατis given by
∇ατL(α, λ) = 2 ηα⊤
τA⊤
ττAττ+X
γ⊇τA⊤
γτλγ.
Setting this to zero and solving for α∗
τyields
α∗
τ=−1/2η
A⊤
ττAττ−1X
γ⊇τA⊤
γτλγ⊤
.
To update λ, we set λ∗= min {λ+tµ(α∗),0}where t >0is the step size. This can be seen as
projected gradient ascent on g(λ)since µ(α∗) =∇λL(α∗, λ) =∇λg(λ).
G Scalable MWEM with pseudoinverse reconstruction
The multiplicative weights exponential mechanism (MWEM) [ 15] is a canonical data-dependent
mechanism that maintains a model ˆpof the data distribution pthat is improved iteratively by adaptively
measuring marginal queries that are poorly approximated by the current model ˆp. MWEM has served
as the foundation for many related data-dependent mechanisms. A limitation of MWEM-style
algorithms is that representing ˆp, even implicitly, does not scale to high-dimensional data domains
without adopting parametric assumptions. In this section, we propose an MWEM-style algorithm
called Scalable MWEM (Alg. 7) that employs a standard reconstruction approach, the pseudoinverse
of the measured marginal queries, but scales to high-dimensional data domains.
In general, the pseudoinverse is infeasible as a reconstruction method for large data domains. Com-
puting the pseudoinverse Q+of an arbitrary query matrix Qscales exponentially in the number
of attributes and linearly in size of the data vector. Moreover, even storing the reconstructed data
vector ˆp=Q+yfrom noisy answers yin memory presents a limitation in practice. Scalable MWEM
overcomes this computational hurdle by measuring marginals with isotropic noise and utilizing the
efficient marginal pseudoinverse (Alg. 4).
Scalable MWEM initializes by using a predetermined fraction of the privacy budget to measure
the total query i.e. the 0-way marginal that counts the number of records in the dataset. Let Wbe
a workload of marginals e.g. all 3-way marginals. Then, for a fixed number of rounds, Scalable
MWEM privately selects a marginal γ∈ W that is poorly approximated by the pseudoinverse of the
current measurements using the exponential mechanism. The selected marginal is measured with
isotropic Gaussian noise and utilizes the efficient marginal pseudoinverse to reconstruct answers to
marginals in W. Being a full query answering mechanism rather than just a reconstruction method,
let us show that Scalable MWEM satisfies differential privacy.
Theorem 7. Scalable MWEM satisfies (ϵ, δ)-DP .
24Proof. We will refer to Algorithm 7 as M. Note that Mselects a parameter ρsuch that
δ= min α>1exp(( α−1)(αρ−ϵ))
α−1 
1−1
αα. By proposition 4, it suffices to show that Msatisfies
ρ-zCDP, then it also satisfies (ϵ, δ)-DP. In the initialization step, Mmeasures M∅pwith the Gaussian
mechanism using the noise scale σ2
o=1
2αρ. The query M∅pis the total query, so it has an ℓ2
sensitivity of 1 and therefore by proposition 5, this measurement satisfies1
2σ2o=2αρ
2=αρ-zCDP.
In each round, Mruns the exponential mechanism such that it satisfies(1−α)ρ
2T-zCDP. Also in each
round, Mruns the Gaussian mechanism to measure a marginal query with noise scale σ2=T
(1−α)ρ.
All marginal queries have an ℓ2sensitivity of 1 so again by proposition 5, this measurement sat-
isfies1
2σ2o=(1−α)ρ
2T-zCDP. By the adaptive composition result given in proposition 8, the overall
mechanism satisfies αρ+T((1−α)ρ
2T+(1−α)ρ
2T) =ρ-zCDP and also (ϵ, δ)-DP.
H Experiment Details
Datasets. In general, we follow the preprocessing steps described in [ 7]. All attributes in the datasets
are discrete. We identify the data domain by inferring the possible values for each attribute from the
observed values for each attribute.
Titanic [ 23] contains 9attributes, 1,304records, and has data vector size 8.9×107. Adult [ 24]
contains 14attributes, 48,842records, and has data vector size 9.8×1017. Salary [ 25] contains 9
attributes, 135,727records, and has data vector size 1.3×1013. Nist-Taxi [ 26] has 10attributes,
223,551records, and has data vector size 1.9×1013.
Compute Environment. All experiments were run on an internal compute cluster with two CPU
cores and 20GB of memory.
GReM-LNN Hyperparameters. For the ResidualPlanner experiments in Section 5.1, we set the
hyperparameters as follows: the maximum number of rounds T= 4000 , the Lagrangian initialization
parameter λ=−1, and the step size s= 0.1. For the Scalable MWEM experiments in Section 5.2,
we set the hyperparameters as follows: the maximum number of rounds T= 1000 , the Lagrangian
initialization parameter λ=−1, the step size s= 0.02, and regularization weight η= 40 . For
all experiments, if Alg. 6 fails, we divide the step size by√
10and rerun until convergence. We
additionally impose a time limit of 24H on a given run of Alg. 6.
I Additional Experiments
In this section, we detail additional experimental results. For the ResidualPlanner experiment, we
report ℓ2workload error for the reconstruction methods. For the Scalable MWEM experiment, we
report ℓ2workload error for the reconstruction methods as well as whether or not Private-PGM
successfully ran across various settings.
25I.1 Additional ResidualPlanner Experiments
103
102
101
100101
Titanic
 Adult
101
100101103
102
101
100101
Salary
101
100101
Nist-T axiAverage Workload Error (2)
ResidualPlanner Trunc Trunc+Rescale GReM-LNN
Figure 4: Average ℓ2workload error on all 3-way marginals across five trials and privacy budgets
ϵ∈ {0.1,0.31,1,3.16,10}andδ= 1×10−9for ResidualPlanner.
26I.2 Additional MWEM Experiments
101
100101
Titanic
 Adult
101
100101101
100101
Salary
101
100101
Nist-T axiAverage Workload Error (2)
Scalable MWEM Trunc+Rescale GReM-LNN Private-PGM
Figure 5: Average ℓ2workload error on all 3-way marginals across five trials and privacy budgets
ϵ∈ {0.1,0.31,1,3.16,10}andδ= 1×10−9for Scalable MWEM with 30 rounds of measurements.
Dataset Rounds Trials Total Trials Completed Trials >24H Trials Out-of-Memory
Titanic 10 25 25 0 0
20 25 25 0 0
30 25 25 0 0
Adult 10 25 0 25 0
20 25 14 8 3
30 25 0 0 25
Salary 10 25 11 14 0
20 25 0 0 25
30 25 0 0 25
Nist-Taxi 10 25 0 0 25
20 25 0 0 25
30 25 0 0 25
Table 2: Completion results of running Private-PGM by setting for the Scalable MWEM experiment.
Failure is broken down by exceeding the 24H time limit or exceeding the available memory (20GB).
27NeurIPS Paper Checklist
1.Claims
Question: Do the main claims made in the abstract and introduction accurately reflect the
paper’s contributions and scope?
Answer: [Yes]
Justification: The abstract claims that the ReM and ReM-LNN methods are efficient and
can be used to improve existing mechanisms. In the body of the paper we explain why the
methods are efficient and provide empirical results showing that the methods can be run on
large datasets and do in fact improve existing mechanisms.
Guidelines:
•The answer NA means that the abstract and introduction do not include the claims
made in the paper.
•The abstract and/or introduction should clearly state the claims made, including the
contributions made in the paper and important assumptions and limitations. A No or
NA answer to this question will not be perceived well by the reviewers.
•The claims made should match theoretical and experimental results, and reflect how
much the results can be expected to generalize to other settings.
•It is fine to include aspirational goals as motivation as long as it is clear that these goals
are not attained by the paper.
2.Limitations
Question: Does the paper discuss the limitations of the work performed by the authors?
Answer: [Yes]
Justification: We have a paragraph in the discussion section dedicated to discussing the
limitations of the work. We discuss how the limitations of the methods presented in this
paper compare to the limitations of the related methods that we compare against.
Guidelines:
•The answer NA means that the paper has no limitation while the answer No means that
the paper has limitations, but those are not discussed in the paper.
• The authors are encouraged to create a separate "Limitations" section in their paper.
•The paper should point out any strong assumptions and how robust the results are to
violations of these assumptions (e.g., independence assumptions, noiseless settings,
model well-specification, asymptotic approximations only holding locally). The authors
should reflect on how these assumptions might be violated in practice and what the
implications would be.
•The authors should reflect on the scope of the claims made, e.g., if the approach was
only tested on a few datasets or with a few runs. In general, empirical results often
depend on implicit assumptions, which should be articulated.
•The authors should reflect on the factors that influence the performance of the approach.
For example, a facial recognition algorithm may perform poorly when image resolution
is low or images are taken in low lighting. Or a speech-to-text system might not be
used reliably to provide closed captions for online lectures because it fails to handle
technical jargon.
•The authors should discuss the computational efficiency of the proposed algorithms
and how they scale with dataset size.
•If applicable, the authors should discuss possible limitations of their approach to
address problems of privacy and fairness.
•While the authors might fear that complete honesty about limitations might be used by
reviewers as grounds for rejection, a worse outcome might be that reviewers discover
limitations that aren’t acknowledged in the paper. The authors should use their best
judgment and recognize that individual actions in favor of transparency play an impor-
tant role in developing norms that preserve the integrity of the community. Reviewers
will be specifically instructed to not penalize honesty concerning limitations.
3.Theory Assumptions and Proofs
28Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?
Answer: [Yes]
Justification: Our paper includes several theorhetical results, each of which is given with a
full set of assumptions. The proofs are all complete and correct, they are provided in the
supplementary material.
Guidelines:
• The answer NA means that the paper does not include theoretical results.
•All the theorems, formulas, and proofs in the paper should be numbered and cross-
referenced.
•All assumptions should be clearly stated or referenced in the statement of any theorems.
•The proofs can either appear in the main paper or the supplemental material, but if
they appear in the supplemental material, the authors are encouraged to provide a short
proof sketch to provide intuition.
•Inversely, any informal proof provided in the core of the paper should be complemented
by formal proofs provided in appendix or supplemental material.
• Theorems and Lemmas that the proof relies upon should be properly referenced.
4.Experimental Result Reproducibility
Question: Does the paper fully disclose all the information needed to reproduce the main ex-
perimental results of the paper to the extent that it affects the main claims and/or conclusions
of the paper (regardless of whether the code and data are provided or not)?
Answer: [Yes]
Justification: We provide clear descriptions of the methods proposed in the paper and we
also provide code that can be used to reproduce the results in the paper.
Guidelines:
• The answer NA means that the paper does not include experiments.
•If the paper includes experiments, a No answer to this question will not be perceived
well by the reviewers: Making the paper reproducible is important, regardless of
whether the code and data are provided or not.
•If the contribution is a dataset and/or model, the authors should describe the steps taken
to make their results reproducible or verifiable.
•Depending on the contribution, reproducibility can be accomplished in various ways.
For example, if the contribution is a novel architecture, describing the architecture fully
might suffice, or if the contribution is a specific model and empirical evaluation, it may
be necessary to either make it possible for others to replicate the model with the same
dataset, or provide access to the model. In general. releasing code and data is often
one good way to accomplish this, but reproducibility can also be provided via detailed
instructions for how to replicate the results, access to a hosted model (e.g., in the case
of a large language model), releasing of a model checkpoint, or other means that are
appropriate to the research performed.
•While NeurIPS does not require releasing code, the conference does require all submis-
sions to provide some reasonable avenue for reproducibility, which may depend on the
nature of the contribution. For example
(a)If the contribution is primarily a new algorithm, the paper should make it clear how
to reproduce that algorithm.
(b)If the contribution is primarily a new model architecture, the paper should describe
the architecture clearly and fully.
(c)If the contribution is a new model (e.g., a large language model), then there should
either be a way to access this model for reproducing the results or a way to reproduce
the model (e.g., with an open-source dataset or instructions for how to construct
the dataset).
(d)We recognize that reproducibility may be tricky in some cases, in which case
authors are welcome to describe the particular way they provide for reproducibility.
In the case of closed-source models, it may be that access to the model is limited in
29some way (e.g., to registered users), but it should be possible for other researchers
to have some path to reproducing or verifying the results.
5.Open access to data and code
Question: Does the paper provide open access to the data and code, with sufficient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?
Answer: [Yes]
Justification: The anonymized code has been submitted as a .zip file along with the paper
submission. Upon acceptance, we will make the code publicly available on GitHub.
Guidelines:
• The answer NA means that paper does not include experiments requiring code.
•Please see the NeurIPS code and data submission guidelines ( https://nips.cc/
public/guides/CodeSubmissionPolicy ) for more details.
•While we encourage the release of code and data, we understand that this might not be
possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
including code, unless this is central to the contribution (e.g., for a new open-source
benchmark).
•The instructions should contain the exact command and environment needed to run to
reproduce the results. See the NeurIPS code and data submission guidelines ( https:
//nips.cc/public/guides/CodeSubmissionPolicy ) for more details.
•The authors should provide instructions on data access and preparation, including how
to access the raw data, preprocessed data, intermediate data, and generated data, etc.
•The authors should provide scripts to reproduce all experimental results for the new
proposed method and baselines. If only a subset of experiments are reproducible, they
should state which ones are omitted from the script and why.
•At submission time, to preserve anonymity, the authors should release anonymized
versions (if applicable).
•Providing as much information as possible in supplemental material (appended to the
paper) is recommended, but including URLs to data and code is permitted.
6.Experimental Setting/Details
Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?
Answer: [Yes]
Justification: In the experiments section we present the key details such as the data citations,
privacy parameters, and target workload. All other experimental details are provided in
Appendix H.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The experimental setting should be presented in the core of the paper to a level of detail
that is necessary to appreciate the results and make sense of them.
•The full details can be provided either with the code, in appendix, or as supplemental
material.
7.Experiment Statistical Significance
Question: Does the paper report error bars suitably and correctly defined or other appropriate
information about the statistical significance of the experiments?
Answer: [Yes]
Justification: In the experiments section, we explain that all experiments were run for 5 trials
and that we present average error along with minimum/maximum error bands.
Guidelines:
• The answer NA means that the paper does not include experiments.
30•The authors should answer "Yes" if the results are accompanied by error bars, confi-
dence intervals, or statistical significance tests, at least for the experiments that support
the main claims of the paper.
•The factors of variability that the error bars are capturing should be clearly stated (for
example, train/test split, initialization, random drawing of some parameter, or overall
run with given experimental conditions).
•The method for calculating the error bars should be explained (closed form formula,
call to a library function, bootstrap, etc.)
• The assumptions made should be given (e.g., Normally distributed errors).
•It should be clear whether the error bar is the standard deviation or the standard error
of the mean.
•It is OK to report 1-sigma error bars, but one should state it. The authors should
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
of Normality of errors is not verified.
•For asymmetric distributions, the authors should be careful not to show in tables or
figures symmetric error bars that would yield results that are out of range (e.g. negative
error rates).
•If error bars are reported in tables or plots, The authors should explain in the text how
they were calculated and reference the corresponding figures or tables in the text.
8.Experiments Compute Resources
Question: For each experiment, does the paper provide sufficient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?
Answer: [Yes]
Justification: We give the details for the compute environment in Appendix H.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The paper should indicate the type of compute workers CPU or GPU, internal cluster,
or cloud provider, including relevant memory and storage.
•The paper should provide the amount of compute required for each of the individual
experimental runs as well as estimate the total compute.
•The paper should disclose whether the full research project required more compute
than the experiments reported in the paper (e.g., preliminary or failed experiments that
didn’t make it into the paper).
9.Code Of Ethics
Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines ?
Answer: [Yes]
Justification: We have reviewed the ethics guidelines and can confirm that our research
practices conform to those standards.
Guidelines:
•The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
•If the authors answer No, they should explain the special circumstances that require a
deviation from the Code of Ethics.
•The authors should make sure to preserve anonymity (e.g., if there is a special consid-
eration due to laws or regulations in their jurisdiction).
10.Broader Impacts
Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?
Answer: [Yes]
Justification: In the discussion section, we include a paragraph titled "Future Work and
Broader Impacts" that discusses potential negative societal impacts of the work performed.
31Guidelines:
• The answer NA means that there is no societal impact of the work performed.
•If the authors answer NA or No, they should explain why their work has no societal
impact or why the paper does not address societal impact.
•Examples of negative societal impacts include potential malicious or unintended uses
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
(e.g., deployment of technologies that could make decisions that unfairly impact specific
groups), privacy considerations, and security considerations.
•The conference expects that many papers will be foundational research and not tied
to particular applications, let alone deployments. However, if there is a direct path to
any negative applications, the authors should point it out. For example, it is legitimate
to point out that an improvement in the quality of generative models could be used to
generate deepfakes for disinformation. On the other hand, it is not needed to point out
that a generic algorithm for optimizing neural networks could enable people to train
models that generate Deepfakes faster.
•The authors should consider possible harms that could arise when the technology is
being used as intended and functioning correctly, harms that could arise when the
technology is being used as intended but gives incorrect results, and harms following
from (intentional or unintentional) misuse of the technology.
•If there are negative societal impacts, the authors could also discuss possible mitigation
strategies (e.g., gated release of models, providing defenses in addition to attacks,
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
feedback over time, improving the efficiency and accessibility of ML).
11.Safeguards
Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?
Answer: [NA]
Justification: We do not release any data or models. We do not believe that algorithm for
private query answering have a high risk for misuse.
Guidelines:
• The answer NA means that the paper poses no such risks.
•Released models that have a high risk for misuse or dual-use should be released with
necessary safeguards to allow for controlled use of the model, for example by requiring
that users adhere to usage guidelines or restrictions to access the model or implementing
safety filters.
•Datasets that have been scraped from the Internet could pose safety risks. The authors
should describe how they avoided releasing unsafe images.
•We recognize that providing effective safeguards is challenging, and many papers do
not require this, but we encourage authors to take this into account and make a best
faith effort.
12.Licenses for existing assets
Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?
Answer: [Yes]
Justification: We utilize data that has been previously released for the purpose of academic
use, in all cases we cite the relevant papers that introduce the datasets.
Guidelines:
• The answer NA means that the paper does not use existing assets.
• The authors should cite the original paper that produced the code package or dataset.
•The authors should state which version of the asset is used and, if possible, include a
URL.
32• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
•For scraped data from a particular source (e.g., website), the copyright and terms of
service of that source should be provided.
•If assets are released, the license, copyright information, and terms of use in the
package should be provided. For popular datasets, paperswithcode.com/datasets
has curated licenses for some datasets. Their licensing guide can help determine the
license of a dataset.
•For existing datasets that are re-packaged, both the original license and the license of
the derived asset (if it has changed) should be provided.
•If this information is not available online, the authors are encouraged to reach out to
the asset’s creators.
13.New Assets
Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?
Answer: [Yes]
Justification: The anonymous code we provide at time of submission is well documented, it
does not currently include a license but at the time that the code is made public on GitHub it
will include a license.
Guidelines:
• The answer NA means that the paper does not release new assets.
•Researchers should communicate the details of the dataset/code/model as part of their
submissions via structured templates. This includes details about training, license,
limitations, etc.
•The paper should discuss whether and how consent was obtained from people whose
asset is used.
•At submission time, remember to anonymize your assets (if applicable). You can either
create an anonymized URL or include an anonymized zip file.
14.Crowdsourcing and Research with Human Subjects
Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as
well as details about compensation (if any)?
Answer: [NA]
Justification: We do not conduct any experiments with human subjects.
Guidelines:
•The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
•Including this information in the supplemental material is fine, but if the main contribu-
tion of the paper involves human subjects, then as much detail as possible should be
included in the main paper.
•According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
or other labor should be paid at least the minimum wage in the country of the data
collector.
15.Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects
Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?
Answer: [NA]
Justification: We do not conduct any experiments with human subjects or any experiments
that are otherwise subject to IRB review.
Guidelines:
33•The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
•Depending on the country in which research is conducted, IRB approval (or equivalent)
may be required for any human subjects research. If you obtained IRB approval, you
should clearly state this in the paper.
•We recognize that the procedures for this may vary significantly between institutions
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
guidelines for their institution.
•For initial submissions, do not include any information that would break anonymity (if
applicable), such as the institution conducting the review.
34