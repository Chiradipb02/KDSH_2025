Published in Transactions on Machine Learning Research (06/2023)
Improving Differentially Private SGD via Randomly Sparsi-
fied Gradients
Junyi Zhu junyi.zhu@esat.kuleuven.be
Center for Processing Speech and Images, Department of Electrical Engineering (ESAT)
KU Leuven, Belgium
Matthew B. Blaschko matthew.blaschko@esat.kuleuven.be
Center for Processing Speech and Images, Department of Electrical Engineering (ESAT)
KU Leuven, Belgium
Reviewed on OpenReview: https: // openreview. net/ forum? id= sY35BAiIf4
Abstract
Differentially private stochastic gradient descent (DP-SGD) has been widely adopted in
deep learning to provide rigorously defined privacy, which requires gradient clipping to
bound the maximum norm of individual gradients and additive isotropic Gaussian noise.
With analysis of the convergence rate of DP-SGD in a non-convex setting, we identify that
randomly sparsifying gradients before clipping and noisification adjusts a trade-off between
internal components of the convergence bound and leads to a smaller upper bound when the
noise is dominant. Additionally, our theoretical analysis and empirical evaluations show that
the trade-off is not trivial but possibly a unique property of DP-SGD, as either canceling
noisification or gradient clipping eliminates the trade-off in the bound. This observation
is indicative, as it implies DP-SGD has special inherent room for (even simply random)
gradient compression. To verify the observation an utilize it, we propose an efficient and
lightweight extension using random sparsification (RS) to strengthen DP-SGD. Experiments
with various DP-SGD frameworks show that RS can improve performance. Additionally,
the produced sparse gradients of RS exhibit advantages in reducing communication cost and
strengthening privacy against reconstruction attacks, which are also key problems in private
machine learning.
1 Introduction
Internet-scale data promises to accelerate the development of data-driven statistical approaches, but the
need for privacy constrains the amalgamation of such datasets. As a result, private data are in fact isolated,
constraining our ability to build models that learn from a large number of instances. On the other hand, the
information contained in locally stored data can also be exposed through releasing the model trained on a
local dataset (Fredrikson et al., 2015; Shokri et al., 2017), or even reconstructed when gradients generated
during training are shared (Zhu et al., 2019; Zhu & Blaschko, 2021; Zhu et al., 2023).
To address these issues, many applications of machine learning are expected to be privacy-preserving, while
differential privacy (DP) provides a rigorously defined and measurable privacy guarantee. As described in
Definition 1, DP defines privacy with respect to the difficulty of distinguishing the outputs of different data:
Definition1 ((ε,δ)-DP(Dwork&Roth,2014)) .Forapairofneighboringdatasets X,X′∈X,Xisobtained
fromX′by adding or removing an element. A randomized mechanism M:X →R is(ε,δ)-differentially
private, if for any subset of outputs S⊆Rit holds that:
Pr[M(X)∈S]≤eεPr[M(X′)∈S] +δ. (1)
1Published in Transactions on Machine Learning Research (06/2023)
(a) DP-SGD
 (b) DP-SGD with RS
 (c) noise norm ratio
 (d) gradient norm ratio
Figure 1: (a) Schematic of DP-SGD. We denote gas the gradient vector and omit the notation for input
data,g1andg2are two entries in gcorresponding to the coordinates w1andw2,¯gis the clipped gradient
vector. (b) Schematic of DP-SGD with RS, ¯g′
1and ¯g′
2are two possible results under RS. (c) Empirical
ratio of noise norm with RS ∥ξ′
σ∥and without RS∥ξσ∥, i.e.∥ξ′
σ∥/∥ξσ∥, we taked= 5.5×105which is the
dimension of DP-CNN. (d) Empirical ratio of the gradient norm of relaxed coordinates with RS ∥¯g′
D\S∥and
without RS∥¯gD\S∥, i.e.∥¯g′
D\S∥/∥¯gD\S∥, measured on DP-CNN with dataset CIFAR10. We remark that for
(a) and (b) clipping bound Cwithin range (max({|gi|}d
i=1),∥g∥)is representative, as in high-dimensional
space∥g∥≫max({|gi|}d
i=1), while the two strengths still exist for Cbeing smaller.
A common paradigm for applying DP in deep learning is differentially private stochastic gradient descent
(DP-SGD) proposed by Abadi et al. (2016b), which lets the randomized mechanism Moutput perturbed
gradients:
f(X) : =/summationdisplay
x∈Xg(x), (2)
M(X) : =f(X) +N(0,S2
fσ2Id), (3)
whereXnow stands for a batch of data and g(x)→Rdcomputes gradient given an individual example
in the batch. The isotropic Gaussian distributed noise N(0,S2
fσ2I)is calibrated to f’s sensitivity S2
f:=
maxX,X′∥f(X)−f(X′)∥, while the noise multiplier σcontrols the strength of the privacy guarantee. As Sf
is usually unknown, DP-SGD commonly clips the gradient of each individual example in Euclidean norm to
a preset bound C, such that ¯f(X) :=/summationtext
x∈Xg(x)·min(1,C/∥g(x)∥). ThenSfcan be safely replaced by C
and the randomized mechanism Mcan thus be expressed as:
M(X) := ¯f(X) +N(0,C2σ2Id). (4)
However, gradient clipping gives rise to a squeezed gradient distribution, while noisification further blurs the
gradient distribution, both cause adverse effects for optimization. Understanding the influence of these two
operations becomes crucial for the progress of DP-SGD.
1.1 Our contribution
In this work we present a special property of DP-SGD that is established jointly by gradient clipping and
noisification. In particular, we analyze DP-SGD in a non-convex and smooth setting and identify that
randomly sparsifying gradients before clipping and noisification can trigger a trade-off between internal
components of the convergence bound and possibly result in a smaller upper bound. This observation
suggests that in the context of DP-SGD randomly sparsified gradients could lead to faster convergence than
the full gradient. This also implies that DP-SGD has inherent room for (even simply random) gradient
compression. We further note that the same property is not present in other popular SGD schemes.
First, we describe the process of random sparsification (RS) and illustrate its impact on DP-SGD. RS
uniformly selects a random subset Sfrom the full coordinates set Dand zeros out their gradients, we define
2Published in Transactions on Machine Learning Research (06/2023)
sparsification rate r:=|S|/|D|as the percentage of coordinates we would like to sparsify. Figure 1a, 1b
schematically illustrate the vanilla DP-SGD and DP-SGD with RS using a 2D example. In this 2D example
RS can either have g1org2zeroed-out so rcan only be set to 0.5. In higher-dimensional cases it will be
possible to set rto other values within the range (0,1). Compared with vanilla DP-SGD, the drawback of
RS is indeed apparent as RS removes some gradient information. However, on the other side RS has two
strengths: (i) due to the isotropic Gaussian noise ξσ, the amount of noise in term of Euclidean norm scales
with the gradient dimension d=|D|. With RS the effective dis reduced as the sparsified coordinates do
not receive any gradient information, and noisification in those dimensions is no longer necessary. Figure 1c
demonstrates the empirical ratio of the noise norm with RS over without RS ∥ξ′
σ∥/∥ξσ∥. (ii) Due to gradient
clipping, for the coordinates D\Sthat are relaxed, i.e. corresponding gradients are not zeroed out, their
gradients have a higher magnitude than without RS (see |¯g′
1|>|¯g1|or|¯g′
2|>|¯g2|), implying the relaxed
coordinates are better optimized, Figure 1d demonstrates an empirical ratio of the gradient norm of relaxed
coordinates with RS over without RS ∥¯g′
D\S∥/∥¯gD\S∥. In Section 3, we will discuss the integrated impact
of the drawback and strengthens of DP-SGD with RS for non-convex and smooth problems and as we will
see RS gives rise to a trade-off between internal components of the convergence bound of DP-SGD. Another
interestingobservationwewillpresentisthatforotherpopularSGDschemes, i.e.SGDwithgradientclipping
(same as canceling noisification in DP-SGD), noisy SGD (same as canceling clipping in DP-SGD), and vanilla
SGD, the respective convergence bounds are only enlarged by applying RS. This observation highlights the
fact that the reason for RS improving DP-SGD is not trivially due to the reduced noise, and suggests that
the trade-off raised by RS in the context of DP-SGD is special, which manifests under two preconditions:
gradient clipping and noisification .
An alternative to random sparsification would be e.g. to sparsify the smallest entries of the gradient, w2
in Figure 1. It is worth emphasizing that the selection of significant coordinates is also privacy related. It
could be done by using a public dataset (Zhou et al., 2021; Kairouz et al., 2021) which we do not assume
exists, or by providing additional privacy budget for sparse vector techniques (Dwork & Roth, 2014) or DP
selection (Zhang et al., 2021), which have certain technical difficulties of defining the threshold and privacy
loss for the selection, and are thus only applied when the gradient is obviously sparse, e.g. the embedding
layer of a language model. Perhaps most importantly, the analysis of these methods assumes the original
sparsity of the gradients (e.g. glies inw1in Figure 1) or low-rank property (e.g. galways lies in a subspace).
In contrast, our analysis, and the strengths of RS as illustrated in Figure 1, do not rely on these assumptions,
which makes the contribution of this work complementary to previous works.
Paper organization: In Section 2, we discuss related work. In Section 3, we elaborate on the impact of
RS on popular SGD schemes and finally present the special trade-off of DP-SGD induced by RS. In Section 4,
we provide an efficient and lightweight RS algorithm based on our analysis. In Section 5, we discuss the
additional advantages of sparsified gradients. In Section 6 we empirically verify our analysis and show the
utility of our proposed algorithm. All proofs are deferred to the Appendix.
2 Related works
Chen et al. (2020) analyze the convergence rate of DP-SGD in a non-convex setting with characterization of
the adverse effect of the gradient clipping. Many works study adaptive clipping bounds (Andrew et al., 2021;
Pichapati et al., 2019). Other works study the adverse effect of noisification and prove that the performance
ofDP-SGDisdependentonthegradientdimension das, accordingtoEquation(4), theamountofnoisescales
withd. Bassily et al. (2014) show that in a convex setting, DP-SGD achieves excess risk of ˜O(√
d/nε ), where
nis the dataset size. Wang & Xu (2019) show that the empirical gradient norm decreases to ˜O(d1/4/√nε)
for DP-SGD with a smooth non-convex loss function.
A line of work builds on gradient space compression to improve the utility of DP-SGD. Abadi et al. (2016b)
propose DP linear probing to pre-train a network on an auxiliary dataset, then transfer the feature extractor
andonly re-trainthe linear classifieronthe private data. Similarly, Tramer &Boneh (2021)adopt ScatterNet
(Oyallon et al., 2019) to extract handcrafted features. Both works decrease dby excluding the majority of
parameters during DP learning. Inspired by the empirical observation that the optimization trajectory is
contained in a lower-dimensional subspace (Vogels et al., 2019; Gooneratne et al., 2020; Li et al., 2020),
3Published in Transactions on Machine Learning Research (06/2023)
several recent works (Zhou et al., 2021; Yu et al., 2021a; Kairouz et al., 2021; Yu et al., 2021b) project the
gradient into a subspace which is identified by auxiliary data or released historical gradients. Zhang et al.
(2021) target NLP tasks where gradients are extremely sparse, and propose a DP selection method.
In contrast to the strategy of reducing gradient space dimension, Papernot et al. (2021) propose a dedicated
DP-CNN with tempered activation function which is deemed as robust to the adverse effects of gradient
clipping and noisification in DP-SGD. Li et al. (2022) observe that DP-SGD works well on full fine-tuning of
large language models. Furthermore, Yu et al. (2022) propose incorporating parameter-efficient fine-tuning
(PEFT) techniques, such as low-rank adaptation (Hu et al., 2022) and adapter-based fine-tuning (Houlsby
et al., 2019). Bu et al. (2022) propose private bias-term only fine-tuning. Both approaches improve the
performance of DP fine-tuning and largely reduce the computational cost with large models, as well as the
communication overhead in distributed settings. However, PEFT methods require a pre-trained model as a
backbone and cannot be applied in scenarios where the network must be trained from scratch.
The studies of Damaskinos et al. (2021) and Mangold et al. (2022) on DP coordinate descent compute
the gradient of a random coordinate in backpropagation and in this aspect similar to RS. However, there is
notable difference between these existing works and our work. Damaskinos et al. (2021) study the generalized
linear model which includes a convex problem and investigate the dual formulation. Mangold et al. (2022)
also analyze convex optimization problem and rely on precise coordinate-wise regularity measures of the
objective function such that each coordinate can be noisified accordingly. Both works derive a modified
update step (beside sparsification). In this work, we study DP-SGD with RS in a non-convex setting. We do
not assume detailed characterization of the optimization landscape, e.g. coordinate-wise smoothness. And
RS is applied directly on the gradient computed with the loss function of the network’s prediction. Our
analysis is thereby more general.
3 Applying random sparsification to gradient descent methods
Notation & Terminology LetLbe the objective function L(w) :=Ex∈X[ℓ(w;x)]which isG-Lipschitz
smooth, and xdenotes a training example sampled from the dataset X. We define gt,i:=∇ℓ(wt;xi)the
gradient at step twith example xiand define the true gradient ∇wt:=Ex∈X[gt,i]. We assume the gradient
deviationgt,i−∇wtis sampled from a zero-mean random variable ξt. The averaged gradient of Bsamples at
the steptis denoted as gt:=1
B/summationtext
igt,i, and the averaged clipped gradient as ¯gt:=1
B/summationtext
igt,i·min(1,C/∥gt,i∥).
To conduct RS, we use a random mask m∈{0,1}dand uniformly draw rdindices and set these positions
in the mask to 0while others to 1so that∥m∥1= (1−r)d,ris the predefined sparsification rate. The
average of sparsified and clipped gradients can thus be expressed as ˆgt:=1
B/summationtext
ig′
t,i·min(1,C/∥g′
t,i∥), where
we defineg′
t,i:=m⊙gt.i, the Hadamard product is denoted using ⊙. For noisification, isotropic Gaussian
noise is denoted as ξσ, when RS is applied we do m⊙ξσ. As this work involves multiple ways of processing
gradients, we summarize their notation in Table 1 for convenience. We do not bold vectors or matrices in
the analysis since it is clear from the context. As RS modifies DP-SGD, we clarify that RS does not breach
privacy with the following theorem:
Theorem 3.1. For a Gaussian mechanism: M(x) :=/summationtext
x∈Xg(x)·min(1,C/∥g(x)∥) +N(0,C2σ2Id), which
satisfies (ε,δ)-DP, after applying RS with a mask m∈{0,1}d, the modified Gaussian mechanism M′(x) :=/summationtext
x∈Xm⊙g(x)·min(1,C/∥m⊙g(x)∥) +m⊙N(0,C2σ2Id)also satisfies (ε,δ)-DP.
Proof.Note thatM′is equivalent toM′′(x) :=/summationtext
x∈XgD\S(x)·min(1,C/∥gD\S(x)∥) +N(0,C2σ2I|D\S| )in
terms of privacy, then it is easy to observe that M′′(x)andM(x)provide the same level of DP. It should
be noted that the scenario of m=0is not considered in our analysis as it does not align with the objectives
of optimization.
3.1 Applying random sparsification to vanilla SGD and noisy SGD
We first note that for vanilla SGD (Bottou et al., 2018), applying RS is the same as randomly dropping gradi-
ent information, thus RS cannot provide any benefits. Next, we consider the impact of RS under noisification
and gradient clipping separately. If we cancel the gradient clipping in DP-SGD, the resulting optimization
4Published in Transactions on Machine Learning Research (06/2023)
Gradient
gt,i Gradient at step twith example xi, i.e.∇ℓ(wt,xi).
∇w True gradient at step t, i.e.Ex∈X[gt,i]
gt Mini-batch gradient at step t, i.e.1
B/summationtext
igt,i
¯gt Clipped mini-batch gradient at step t, i.e.1
B/summationtext
igt,i·min(1,C/∥gt,i∥)
g′
t,i Sparsified gradient due to random mask m, i.e.m⊙gt,i.
∇w′Sparsified true gradient due to random mask m, i.e.m⊙Ex∈X[gt,i]
ˆgt Sparsified and clipped mini-batch gradient at step t, i.e.1
B/summationtext
ig′
t,i·min(1,C/∥g′
t,i∥).
Gradient deivation
ξt Assumed random variable generating gradient deviations, i.e. gt,i−∇wt.
ξ′
t Assumed random variable generating sparsified gradient deviations, i.e. g′
t,i−∇w′
t.
pt True probability distribution of ξt.
˜pt A symmetric proxy of ptwhich approximates ptwhile satisfies ˜pt(ξt) = ˜pt(−ξt).
p′
t True probability distribution of ξ′
t.
˜p′
t A symmetric proxy of p′
twhich is projected from ptby random mask m.
Table 1: Notations for gradients.
method is noisy SGD which has been used in Bayesian learning, e.g. stochastic gradient Langevin dynamics
(Welling & Teh, 2011). When RS is applied (pseudo code is given in Appendix E), the amount of noise will
be reduced as illustrated in Figure 1, however such a benefit seems insufficient to compensate the loss of
gradient information, which we show in Theorem 3.2.
Theorem 3.2. Consider noisy SGD with RS on a G-smooth function Lwith isotropic Gaussian noise
ξσ∼N(0, σ2Id), learning rate γ=1
G√
T, batch size B and sparsification rate r, assume an upper bound of
individual gradient deviation ∥ξt∥2≤σ2
g, we have:
1
TT/summationdisplay
t=1∥∇wt∥2≤2G
(1−r)√
T∆L+1√
T(σ2
g
B+dσ2
B2), (5)
where we define ∆L:=E[L(w1)]−minwL(w). We note that the noise term dσ2/B2on the r.h.s. was reduced
by a factor of 1−rdue to the RS, but this factor is canceled out by the same factor from the l.h.s. (see
Appendix A). Additionally, Equation (5) can also be used to describe the case of vanilla SGD with RS by
removing the last term dσ2/B2at the r.h.s. Overall, we have the following observation:
Remark 1. Theorem 3.2 suggests that in applying RS to noisy SGD or vanilla SGD, the network may
converge more slowly, as ∥∇wt∥2has a larger upper bound for 0<r< 1.
3.2 Applying random sparsification to SGD with gradient clipping
If we cancel the noisification in DP-SGD, then the optimization method becomes SGD with gradient clipping,
which has been widely adopted for large network training to prevent exploding gradients or stabilize opti-
mization (Zhang et al., 2020; Pascanu et al., 2013). If RS is applied (pseudo code is given in Appendix E),
relaxed coordinators can be better optimized as illustrated in Figure 1, however such benefit also seems
insufficient to compensate the loss of gradient information.
First consider the convergence of gradient clipping without sparsification.
Lemma 3.1. Consider SGD on a G-smooth function Lwith gradient clipping of bound C, learning rate γ,
we have:
E[⟨∇wt,¯gt⟩]≤1
γE[Lt−Lt+1] +γGC2
2. (6)
As the distribution of the gradient is unknown, the expectation involving gradient clipping, i.e. E[⟨∇wt,¯gt⟩],
impedes the estimation of the convergence rate. Let ptbe the true distribution of the gradient deviation ξt.
5Published in Transactions on Machine Learning Research (06/2023)
Chen et al. (2020) firstly observe that ptappears to be symmetric and use this property to relate E[⟨∇wt,¯gt⟩]
to∥∇wt∥:
Lemma 3.2 (Chen et al. (2020)) .Assumept(ξt) =pt(−ξt),∀ξt∈Rd, gradient clipping with bound Chas
the following property:
E[⟨∇wt,¯gt⟩]≥Pξt∼pt(∥ξt∥<3C/4)h(∇w)∥∇wt∥, (7)
whereh(∇w) := min(∥∇wt∥,C/4).
We see the r.h.s. of Equation (7) is proportional to ∥∇w∥or∥∇w∥2as long asPξt∼pt(∥ξt∥<3C/4)is not
close to zero. Combining Lemma 3.1 with Lemma 3.2, it is possible to form an upper bound for finding
the critical point. However, the real gradient distribution cannot be exactly symmetric. Instead of using
ptin Equation (7), we can choose a proxy ˜pwhich is symmetric and use an error term btto represent the
difference as bt:=Eξt∼pt[⟨∇w,¯gt⟩]−Eξt∼˜pt[⟨∇w,¯gt⟩].
Theorem 3.3 (Chen et al. (2020)) .Consider SGD on a G-smooth function Lwith gradient clipping bound
Cand learning rate γ. Choose a symmetric distribution ˜p(·)satisfying ˜pt(ξt) = ˜pt(−ξt),∀ξt∈Rd, and
considerTiterations:
1
TT/summationdisplay
t=1Pξt∼˜pt(∥ξt∥<3C/4)h(∇wt)∥∇wt∥≤∆L
γT+γGC2
2−1
TT/summationdisplay
t=1bt. (8)
Chen et al. (2020) argues that there exists ˜ptwhich is a close approximation of ptsuch thatbtis small
whilePξt∼˜pt(∥ξt∥<3C/4)is bounded away from zero, as in practice pttends to be approximately sym-
metric. They also empirically demonstrate that ptbecomes approximately symmetric during training. So
Theorem 3.3 indicates the convergence rate of SGD with gradient clipping. To make sure this property
ofptis generally valid in our case, we have also verified that ptbecomes approximately symmetric in our
experimental environments in Section 6 (see Appendix F).
Now consider the convergence rate after applying RS, note that the sparsification happens before clipping,
so the conclusion cannot be straightforwardly derived from Theorem 3.3. Since m,ξtare independent:
E[⟨∇wt,ˆgt⟩] =Em/bracketleftbig
Eξ′
t∼˜p′
t[⟨∇w′
t,ˆgt⟩]/bracketrightbig
+Em[b′
t], (9)
where we define∇w′
t:=m⊙∇wt,ξ′
t:=m⊙ξtwith corresponding true distribution p′
tand proxy ˜p′
twhich
are projected from ptand˜pt,b′
t:=Eξ′
t∼p′
t[⟨∇w′,ˆgt⟩]−Eξ′
t∼˜p′
t[⟨∇w′,ˆgt⟩]. Since the projection of a symmetric
distribution to a subspace is symmetric, we have ˜p′
t(ξ′
t) = ˜p′
t(−ξ′
t), so following Lemma 3.2, we have for the
first term on the r.h.s. of Equation (9):
Em/bracketleftbig
Eξ′
t∼˜p′
t[⟨∇w′
t,ˆgt⟩]/bracketrightbig
≥Em[Pξ′
t∼˜p′
t(∥ξ′
t∥<3C/4)h(∇w′
t)∥∇w′
t∥]. (10)
Then to solve the expectation over the random mask min the Equation (10), we provide Lemma 3.3:
Lemma 3.3. Apply RS with sparsification rate r, choose ˜ptsuch thatPξt∼˜pt(∥ξt∥<3C/4)≥√1−r, then
∃κt∈(1−r,1)such that:
Em[Pξ′
t∼˜p′
t(∥ξ′
t∥<3C/4)h(∇w′
t)∥∇w′
t∥] =κtPξt∼˜pt(∥ξt∥<3C/4)h(∇wt)∥∇wt∥, (11)
κttakes the value 1whenr= 0. Based on Lemma 3.3, we provide Theorem 3.4 to characterize the
convergence of SGD with gradient clipping and RS.
Theorem 3.4. Consider SGD on a G-smooth function Lwith gradient clipping of bound C, learning
rateγ, and apply RS with sparsification rate r. Choose a symmetric distribution ˜p(·)satisfying ˜pt(ξt) =
˜pt(−ξt),∀ξt∈Rd, whilePξt∼˜pt(∥ξt∥<3C/4)≥√1−r, and consider Titerations, then∃κ∈(1−r,1)s.t.:
1
TT/summationdisplay
t=1Pξt∼˜pt(∥ξt∥<3C/4)h(∇wt)∥∇wt∥≤1
κ(∆L
γT+γGC2
2−1
TT/summationdisplay
t=1Em[b′
t]), (12)
Remark 2. Consider that bt, b′
ttend to be small and negligible, compared with Theorem 3.3, Theorem 3.4
suggests that RS could impede the convergence of SGD with gradient clipping as 1/κ> 1forr>0.
6Published in Transactions on Machine Learning Research (06/2023)
3.3 Applying random sparsification to DP-SGD
Although there is seemingly no advantage of using RS for noisy SGD or SGD with gradient clipping, we show
that in case of DP-SGD, i.e. when noisification and gradient clipping are both presented, RS can induce a
trade-off between internal components of the convergence bound and possibly achieve a smaller upper bound.
Lemma 3.4. Consider DP-SGD on a G-smooth function Lwith clipping bound C, isotropic Gaussian noise
ξσ∼N(0, σ2C2Id), learning rate γ, batch size B, sparsification rate r, we have:
E[⟨∇wt,ˆgt⟩]≤1
γE[Lt−Lt+1] +γGC2
2+ (1−r)γ∆σ, (13)
where we have defined ∆σ:=C2σ2dG
2B2. Now we provide Theorem 3.5 to characterize the convergence rate of
DP-SGD with RS.
Theorem 3.5. Consider DP-SGD on a G-smooth function Lwith clipping bound C, isotropic Gaussian
noiseξσ∼N(0, σ2C2Id), learning rate γ, batch size B, and apply RS with sparsification rate r. Choose a
symmetric distribution ˜p(·)satisfying ˜pt(ξt) = ˜pt(−ξt),∀ξt∈Rd, whilePξt∼˜pt(∥ξt∥<3C/4)≥√1−r, and
considerTiterations, then∃κ∈(1−r,1), such that:
1
TT/summationdisplay
t=1Pξt∼˜pt(∥ξt∥<3C/4)h(∇wt)∥∇wt∥≤1
κ(∆L
γT+γGC2
2−1
TT/summationdisplay
t=1Em[b′
t]) +1−r
κγ∆σ.(14)
It is worth noting that in Theorem 3.5 on the r.h.s. the noise term ∆σhas a factor 1−rwhich is induced
by sparsification, but unlike under noisy SGD (see Theorem 3.2), this factor is not fully canceled out as
κ>1−r. Also note that with no sparsification ( r= 0), we haveκ= 1, Theorem 3.5 has the following form:
1
TT/summationdisplay
t=1Pξt∼˜pt(∥ξt∥<3C/4)h(∇wt)∥∇wt∥≤∆L
γT+γ/parenleftbiggGC2
2+ ∆σ/parenrightbigg
−1
TT/summationdisplay
t=1bt, (15)
which recovers the convergence bound of vanilla DP-SGD (Chen et al., 2020), implying that our convergence
bound over RS is as tight as before. Comparing Equation (15) with Theorem 3.5, we observe:
Remark 3. RSintroducestwofactorsonther.h.s.: 1/κ,(1−r)/κ. Thenoiseterm ∆σisreducedby (1−r)/κ
while the remaining terms are enlarged by by 1/κ: a trade-off is established. The respective magnitude of
∆σand other terms determines whether it is a gain or loss to apply RS. It is worthwhile to note that ∆σ
scales with C2σ2and the gradient dimension d, and is inversely proportional to B2.
We also present the upper bound using privacy budget variables (ε,δ)in Appendix C with Corollary C.1.1.
4 Implementation of random sparsification for DP-SGD
Theorem 3.5 reveals that RS induces a trade-off between the internal components of the convergence bound
and has a chance to accelerate the convergence. However, the best sparsification rate is infeasible to be
computed a prioriand possibly varying during optimization. In this section, we present a practically efficient
and lightweight RS approach. The additional cost of running RS is negligible .
Algorithm 1 outlines our approach. RS is also compatible with SGD with gradient momentum and
Adam (Kingma & Ba, 2014). In case of non-zero momentum, coordinates that have been selected can
still be updated as long as their velocity has not decayed to zero. To make the algorithm efficient in practice,
we adopt gradual cooling (Line 3) and per-epoch randomization (Line 5), which we discuss in the sequel.
4.1 Gradual cooling
In practice we find that if we initiate the training with a constant large sparsification rate r, the network
converges slowly and performs poorly when the privacy budget has been fully consumed. According to The-
orem 3.5 and Remark 3, we see sparsification is beneficial once ∆σis significant compared with the other
7Published in Transactions on Machine Learning Research (06/2023)
Algorithm 1 DP-SGD with Random Sparsification
Input:Initial parameters w0; EpochsE; Sparsification rate: r∗; Clipping bound: C; Noise multiplier
σ; Momentum: µ; Learning rate γ.
1:fore= 0toE−1do
2:▷Gradual cooling ◁
3:r(e) =r∗·e
E−1;
4:▷Generate a random mask every epoch ◁
5:m∈{0,1}d, s.t.∥m∥1=d·(1−r(e));
6:fort= 0toT−1do
7:▷For eachxiin the Poisson-sampled batch B ◁
8:gt,i=∇ℓ(wt,xi);
9:▷Sparsify gradient ◁
10:g′
t,i=m⊙gt,i;
11:▷Clip each individual gradient ◁
12: ˆgt=1
|B|/summationtext
i∈Bg′
t,i·min(1,C/∥g′
t,i∥);
13:▷Add sparsified noise ◁
14: ˜gt= ˆgt+m⊙N(0,C2σ2
|B|2Id);
15:▷Update parameters ◁
16:vt+1=µ·vt+ ˜gt,wt+1=wt−γvt+1;
terms in the convergence bound. At early training stages the network converges fast: E[Lt−Lt+1]is large,
while during training the optimization reaches a plateau: E[Lt−Lt+1]decays. In contrast, ∆σis always
fixed and therefore its relative significance grows progressively. To fit this dynamics, we use a simple gradual
cooling strategy which linearly ramps up the sparsification rate from 0tor∗during training, i.e. r=r∗·e
E−1.
We provide a more detailed discussion in Appendix G.
4.2 Per-epoch randomization
RS alternates the optimization direction. As SGD takes several steps to reach the local minimum, frequently
alternating optimization direction might be harmful, which we have also observed in the experiments. Con-
sidering that tuning over the number of iterations for refreshing the random mask can be expensive, in this
work we set the refresh to be per-epoch. Another strength of per-epoch randomization is that for one epoch
there are certainly (1−r)dcoordinates updated, which is favorable in distributed learning as communication
overhead is a key issue, and data are not transmitted every iteration. For per-iteration randomization, the
cumulative number of updated parameters depends on the sparsification rate rand iterations between two
communication rounds, resulting in higher communication overheads than per-epoch randomization.
5 Advantages of sparsified gradients
Sparse representation of the gradient generally offers additional advantages. Our analysis in Section 3
demonstrates that DP-SGD provides an inherent potential for trivial random gradient compression. In the
sequel, we discuss the benefits of sparsified gradients in terms of private machine learning.
Reduced communication overhead of DP federated learning A line of work studies how to incor-
porate differential privacy in federated learning, which is a distributed learning scheme with the interest of
protecting the privacy of participants (McMahan et al., 2017; Yang et al., 2019; Shokri & Shmatikov, 2015;
Liu et al., 2020). While a major issue in federated learning is the communication bottleneck, sparse repre-
sentation produced by RS can be transferred in form of non-zero values and indices. The cost of indexing
is logarithmic in the number of parameters. Considering that log2109<32and for per-epoch randomiza-
tion multiple rounds of communication could share the same indices, the cost of indexing is negligible, and
communication overhead by RS is reduced to ˜O(1−r).
8Published in Transactions on Machine Learning Research (06/2023)
Strengthened privacy against gradient reconstruction attacks A gradient reconstruction attack is
one of the most hazardous privacy attacks, which assumes that the adversary has access to victim’s gradient
and intends to recover the victim’s training data through gradient matching (Zhu et al., 2019; Geiping et al.,
2020; Zhu & Blaschko, 2021; Yin et al., 2021; Zhu et al., 2023). This is a common scenario in distributed
learning schemes, as participants need to share their local update when computing the global model. Zhu &
Blaschko (2021) provide a closed-form solution of a gradient reconstruction attack: for a certain layer one
can form a constraint matrix Kover the input xgiveng, so thatKx=g, wherex,gare flattened vectors
(Zhu & Blaschko, 2021, Equation (15)). When Kis overdetermined, after receiving the gradient g, the input
xcan be reconstructed through least squares: x= (K⊤K)−1K⊤g. To prevent such an attack, DP-SGD can
be introduced so that gwill be perturbed by Gaussian noise ξσ, which leads to a squared reconstruction
error:
∥x−ˆx∥2=∥(K⊤K)−1K⊤ξσ∥2. (16)
If we add more noise, the expected squared error E[∥x−ˆx∥2]will be increased, but at the cost that the
network will lose performance. However, under a certain noise level, applying RS can further amplify the
reconstruction error (although the noise will also be sparsified).
To demonstrate this, we consider a single layer network with a one digit input x∈R, a constraint vector
k∈Rdand corresponding gradient vector g∈Rd:
Theorem 5.1. Conduct an attack using Equation (16)on the target data x, consider that the gradient
gis perturbed by isotropic Gaussian noise ξσ∼N (0,σ2Id)and further sparsified with sparsification rate
r∈(0,1). The expected squared error of reconstruction is:
E[∥x−ˆx∥2]≥σ2
(1−r)∥k∥2. (17)
We see the expected squared error has a lower bound which increases monotonically with the sparsification
rater. We also verify this phenomenon under a more general attacking scenario in Section 6.3.
6 Experiments
Setup: Our code is implemented in PyTorch (Paszke et al., 2019b). To compute the gradients of an
individual example in a mini-batch we use the BackPACK package (Dangel et al., 2020). Cumulative privacy
loss has been tracked with the Opacus package, which adopts Rényi differential privacy (Mironov, 2017; Balle
et al., 2020). A Poisson-sampled batch of data at each iteration implies privacy amplification (Balle et al.,
2018;Wangetal.,2019;Mironovetal.,2019). Intheimplementationofmanypreviousworks(Papernotetal.,
2021; Tramer & Boneh, 2021; Yu et al., 2021a), sampling is conducted by randomly shuffling and partitioning
the dataset into batches of fixed size, we follow this convention. We focus on DP image classification and
run all experiments on a cluster within the same container environment 5 times using the same group of 5
random seeds. Our code is available at https://github.com/JunyiZhu-AI/RandomSparsification .
6.1 Evidence of the trade-off and its uniqueness for DP-SGD
Theorem 3.5 implies that when the noise term ∆σis dominant in the convergence bound, the trade-off
induced by RS can achieve a smaller upper bound on the convergence rate. In the context of DP training,
the number of iterations is limited, so faster convergence means better accuracy. Since ∆σ∝σ2C2, we
conduct experiments on various combinations of σandCto verify our analysis. When conducting RS, we
always adopt gradual cooling as discussed in Section 4.1 to reach the final sparsification rate. Based on
Figure 2, except the last Figure 2i, where due to excessive noise the network does not converge after a few
training epochs, we see: (i) from left to right, as σgoes up, more RS is preferred; (ii) from top to bottom,
asCincreases, more can be gained from RS; (iii) the trade-off has the same tendency for the same value
ofσ2C2. These observations verify our analysis. We note that as C2σ2increases to 22, i.e. Figure 2f and
2h, RS can improve the performance of DP-SGD by more than 4%. Additionally, when C2σ2= 1, i.e.
Figure 2c, 2e, 2g, The performance is maintained under RS (or slightly increased). It should be noted that
9Published in Transactions on Machine Learning Research (06/2023)
(a)σ= 20,C= 2−2
(b)σ= 21,C= 2−2
(c)σ= 22,C= 2−2
(d)σ= 20,C= 2−1
(e)σ= 21,C= 2−1
(f)σ= 22,C= 2−1
(g)σ= 20,C= 20
(h)σ= 21,C= 20
(i)σ= 22,C= 20
Figure 2: Train accuracy vs. final sparsification rate r∗over various combinations of noise multiplier σand
clipping bound C, red dashed line marks the performance of no sparsification. We set batch size to 1000 and
train for 100 epochs, the network is DP-CNN. The result shows that the trade-off favors RS as ∆σ∝σ2C2
increasing, except the last Figure 2i, where due to excessive noise the network degrades after a few epochs.
(a)C= 2−2
(b)C= 2−1
(c)C= 20
(d)σ= 20
(e)σ= 21
(f)σ= 22
Figure 3: Train accuracy vs. final sparsification rate r∗over SGD with gradient clipping or noisy SGD (see
Algorithms 2 and 3 for pseudo code), other settings are the same as Figure 2. The red dashed line marks the
performance of no sparsification. Without noisification or gradient clipping RS only leads to utility drop,
which matches our theoretical analysis and indicates that the trade-off is a unique property of DP-SGD.
10Published in Transactions on Machine Learning Research (06/2023)
Dataset Approaches ε d Baseline RS (ours) Difference
CIFAR10H-CNN3.0187K69.7±0.19 70.0±0.11 +0.3
1.0 62.4±0.13 63.2±0.10 +0.8
DP-CNN3.0550K62.8±0.10 64.3±0.17 +1.5
1.0 52.5±0.25 55.1±0.11 +2.6
SVHNH-CNN3.0187K85.9±0.06 86.8±0.13 +0.9
1.0 80.7±0.15 82.2±0.19 +1.5
DP-CNN3.0550K83.4±0.11 84.5±0.14 +1.1
1.0 76.0±0.05 79.1±0.13 +3.1
FMNISTH-CNN/S3.033K88.9±0.09 89.2±0.07 +0.3
1.0 85.8±0.17 87.0±0.08 +1.2
DP-CNN/S3.026K86.6±0.09 87.4±0.10 +0.8
1.0 83.2±0.10 84.5±0.16 +1.3
Table 2: Test accuracy (% ±SEM) before and after adopting random sparsification. The difference of mean
accuracy is presented in the last column.
such a flat trend is also desirable as sparse gradients produced by RS are beneficial for private learning as
discussed in Section 5.
Theorems3.2and3.4suggestthatthetrade-offofDP-SGDisjointlyestablishedbynoisificationandclipping,
as with either absent, RS just reduces performance. To verify this, we conduct experiments on two ablation
variants of DP-SGD: noisy SGD and SGD with gradient clipping (pseudo code is given in Appendix E).
Figure 3 demonstrates that in the absence of either noisification or gradient clipping, RS always makes the
convergence slower, which indicates that the trade-off is a unique property of DP-SGD.
6.2 Improving performance of DP networks
In practice, the clipping bound Cand noise multiplier σare tuned as hyperparameters for the networks
to achieve the best performance under a certain privacy budget. To investigate whether RS has a chance
to achieve better performance in practical settings, we conduct experiments on the following baselines of
DP image classification: (i) DP-CNN (Papernot et al., 2021): a network for training from scratch; (ii) H-
CNN (Tramer & Boneh, 2021): a network for training solely on a private dataset, which uses handcrafted
features. Both are representative SOTA frameworks in high privacy regimes (De et al., 2022). We adopt
the best hyperparameters provided in previous works, then we do grid search for baselines and RS over the
clipping bound C∈{0.1,0.5,1}whereC= 0.1is given in previous works, and epochs E∈{E∗,1.2·E∗,1.5·
E∗}(σis adapted accordingly), where E∗is the best given in previous works for different frameworks. When
conducting RS we use gradual cooling and search for the final sparsification rate r∗∈{0.5,0.7,0.9}. We
also find an interesting scaling rule to efficiently find the optimal hyperparameters for DP frameworks which
is given in Appendix H. As a result, some baseline performances we give are higher than reported in previous
works. As in the previous section, we observe that RS is beneficial when ∆σis dominant, thus we conduct
experiments at high privacy regimes, i.e. ε∈{1,3}.
Table 2 shows that RS improves the performance of baselines and the gain is significant for smaller ε, where
more noise is added. We further note that DP-CNN generally gains more from RS, as it has comparatively
more parameters and ∆σscales with the gradient dimension d.
6.3 Advantages of sparse gradients
Beyond the improvement in performance, sparse gradients also offer additional advantages in the area of
private machine learning as discussed in Section 5. To investigate the sparsity we can achieve, we directly
plugged RS into various frameworks, including: DP-TL (Tramer & Boneh, 2021): a transfer learning frame-
work, and GEP (Yu et al., 2021a): a projected DP-SGD framework, as well as DP-CNN and H-CNN, using
the hyperparameters of no sparsification given in previous works (so the same iterations). Figure 4 shows
11Published in Transactions on Machine Learning Research (06/2023)
(a) DP-CNN, ε= 3
 (b) H-CNN, ε= 3
 (c) DP-TL, ε= 2
 (d) GEP,ε= 8
Figure 4: Test accuracy (% ±SEM) vs. final sparsification rate r∗when random sparsification is directly
plugged into different frameworks using the hyperparameters of no sparsification given in previous works.
Figure 5: MSE of reconstruction using IG (Geiping et al., 2020) vs. sparsification rate r. In the non-private
learning scheme, the MSE is 0.02±0.02represented by the red dashed line. The victim’s network is ResNet34
and added noise is sampled from N(0,10−3). The results are computed on 200 images from CIFAR10, four
example images are: ship, dog, car, horse.
that we can achieve up to 0.7 or 0.9 final sparsification rate, i.e. 0.35 or 0.45 average sparsity under gradual
cooling, while maintaining the same performance.
In Section 5 we have theoretically proven that with sparse representation, the communication overhead of DP
federated learning improves. We also provide Theorem 5.1 to show that the sparse gradients produced by RS
strengthen the privacy against gradient reconstruction attacks through an example of reconstruction using
the linear relation between data and gradient. To verify this advantage under a general setting, we conduct
experiments on ResNet34 (He et al., 2016) using the attack approach Inverting Gradients (IG) (Geiping
et al., 2020), see Figure 5.
7 Discussion and conclusion
During DP training, it is possible to save the perturbed gradients and accordingly find the coordinates
whose gradients were small and possibly continue being insignificant in subsequent iterations. One might
therefore expect better performance from a ranked sparsification algorithm, which for example ranks the
mean of the perturbed gradients of last epoch and sparsifies accordingly. However, we find that due to the
dominant noise, such ranking is in fact random, while on the other hand the resulting random sparsification
still improves the performance. Similarly, other methods that select unimportant coordinates more precisely,
e.g. DP selection or methods with public datasets, also cannot fully eliminate randomness. However, such
random sparsification may still be beneficial for DP-SGD, which was not realized previously. More discussion
are provided in Appendix I.
Scope and Limitations : In this work, we present a theoretical analysis of DP-SGD with random sparsifi-
cation, uncovering a special trade-off between internal components in the convergence bound not observed in
other popular SGD schemes. Our findings are supported by empirical evidence. By employing the proposed
efficient and lightweight RS approach, we successfully enhance the performance of baseline models across
various realistic settings. Additionally, the sparse gradients generated offer further advantages for addressing
key challenges in private learning. However, our analyses do not guarantee that RS always converges faster
12Published in Transactions on Machine Learning Research (06/2023)
than non-sparsified approaches. In certain scenarios, such as DP transfer learning, RS may be less efficient,
as discussed in the Appendix J. Developing a lower bound analysis of the convergence rate can also be inter-
esting in terms of the uniqueness of the trade-off, which we leave for future work. Despite these limitations,
we believe our insights on the intriguing interaction between RS and DP-SGD hold significant implications
for the broader research community and can inspire further investigation of DP-SGD.
It is important to note that DP only reduces, rather than eliminates, the statistical dependency between
its input and output. To ensure robust privacy protection, other techniques like cryptographic methods,
including homomorphic encryption and secure multi-party computation, should be considered based on
specific requirements and expectations for each individual application and policy context.
Acknowledgments
This research received funding from the Flemish Government (AI Research Program) and the Research
Foundation - Flanders (FWO) through project number G0G2921N.
References
Martín Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S Cor-
rado, Andy Davis, Jeffrey Dean, Matthieu Devin, et al. Tensorflow: Large-scale machine learning on
heterogeneous distributed systems. arXiv preprint arXiv:1603.04467 , 2016a.
MartinAbadi,AndyChu,IanGoodfellow,H.BrendanMcMahan,IlyaMironov,KunalTalwar,andLiZhang.
Deep learning with differential privacy. In Proceedings of the 2016 ACM SIGSAC Conference on Computer
and Communications Security , pp. 308–318, 2016b.
Galen Andrew, Om Thakkar, Brendan McMahan, and Swaroop Ramaswamy. Differentially private learning
with adaptive clipping. In Advances in Neural Information Processing Systems . Curran Associates, Inc.,
2021.
Borja Balle, Gilles Barthe, and Marco Gaboardi. Privacy amplification by subsampling: Tight analyses via
couplings and divergences. In Advances in Neural Information Processing Systems , 2018.
BorjaBalle, GillesBarthe, MarcoGaboardi, JustinHsu, andTetsuyaSato. Hypothesistestinginterpretations
and renyi differential privacy. In Proceedings of the Twenty Third International Conference on Artificial
Intelligence and Statistics , pp. 2496–2506. PMLR, 2020.
Raef Bassily, Adam Smith, and Abhradeep Thakurta. Private empirical risk minimization: Efficient al-
gorithms and tight error bounds. Proceedings - Annual IEEE Symposium on Foundations of Computer
Science, FOCS , pp. 464–473, 2014.
Jeremy Bernstein, Yu-Xiang Wang, Kamyar Azizzadenesheli, and Animashree Anandkumar. signSGD:
Compressed optimisation for non-convex problems. In Jennifer Dy and Andreas Krause (eds.), Proceedings
of the 35th International Conference on Machine Learning , volume 80 of Proceedings of Machine Learning
Research , pp. 560–569. PMLR, 10–15 Jul 2018.
Léon Bottou, Frank E Curtis, and Jorge Nocedal. Optimization methods for large-scale machine learning.
SIAM review , 60(2):223–311, 2018.
Zhiqi Bu, Yu-Xiang Wang, Sheng Zha, and George Karypis. Differentially private bias-term only fine-tuning
of foundation models. arXiv preprint arXiv:2210.00036 , 2022.
XiangyiChen, StevenZ.Wu, andMingyiHong. UnderstandinggradientclippinginprivateSGD:Ageometric
perspective. In Advances in Neural Information Processing Systems , 2020.
Georgios Damaskinos, Celestine Mendler-Dünner, Rachid Guerraoui, Nikolaos Papandreou, and Thomas
Parnell. Differentially private stochastic coordinate descent. Proceedings of the AAAI Conference on
Artificial Intelligence , 35(8):7176–7184, May 2021. doi: 10.1609/aaai.v35i8.16882.
13Published in Transactions on Machine Learning Research (06/2023)
Felix Dangel, Frederik Kunstner, and Philipp Hennig. Backpack: Packing more into backprop. In Interna-
tional Conference on Learning Representations , 2020.
Soham De, Leonard Berrada, Jamie Hayes, Samuel L Smith, and Borja Balle. Unlocking high-accuracy
differentially private image classification through scale. arXiv preprint arXiv:2204.13650 , 2022.
Cynthia Dwork and Aaron Roth. The algorithmic foundations of differential privacy. Foundations and
Trends ®in Theoretical Computer Science , 9:211–407, 2014.
Matt Fredrikson, Somesh Jha, and Thomas Ristenpart. Model inversion attacks that exploit confidence
informationandbasiccountermeasures. In Proceedings of the 22nd ACM SIGSAC Conference on Computer
and Communications Security , pp. 1322–1333, 2015.
Jonas Geiping, Hartmut Bauermeister, Hannah Dröge, and Michael Moeller. Inverting gradients - how easy
is it to break privacy in federated learning? In Advances in Neural Information Processing Systems , pp.
16937–16947, 2020.
Mary Gooneratne, Khe Chai Sim, Petr Zadrazil, Andreas Kabel, Françoise Beaufays, and Giovanni
Motta. Low-rank gradient approximation for memory-efficient on-device training of deep neural net-
work. In ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing
(ICASSP) , pp. 3017–3021, 2020.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In
2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , 2016.
Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Ges-
mundo, Mona Attariyan, and Sylvain Gelly. Parameter-efficient transfer learning for NLP. In Kamalika
Chaudhuri and Ruslan Salakhutdinov (eds.), Proceedings of the 36th International Conference on Machine
Learning , volume 97 of Proceedings of Machine Learning Research , pp. 2790–2799. PMLR, 09–15 Jun 2019.
Edward J Hu, yelong shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and
Weizhu Chen. LoRA: Low-rank adaptation of large language models. In International Conference on
Learning Representations , 2022.
Peter Kairouz, Monica Ribero Diaz, Keith Rush, and Abhradeep Thakurta. (nearly) dimension independent
private erm with adagrad rates
via publicly estimated subspaces. In Proceedings of Thirty Fourth Conference on Learning Theory , volume
134, pp. 2717–2746, 2021.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980 , 2014.
Xinyan Li, Qilong Gu, Yingxue Zhou, Tiancong Chen, and Arindam Banerjee. Hessian based analysis of sgd
for deep nets: Dynamics and generalization. In Proceedings of the 2020 SIAM International Conference
on Data Mining (SDM) , pp. 190–198, 2020.
Xuechen Li, Florian Tramer, Percy Liang, and Tatsunori Hashimoto. Large language models can be strong
differentially private learners. In International Conference on Learning Representations , 2022.
Ruixuan Liu, Yang Cao, Masatoshi Yoshikawa, and Hong Chen. Fedsel: Federated SGD under local differ-
ential privacy with top-k dimension selection. In Yunmook Nah, Bin Cui, Sang-Won Lee, Jeffrey Xu Yu,
Yang-Sae Moon, and Steven Euijong Whang (eds.), Database Systems for Advanced Applications , 2020.
Paul Mangold, Aurélien Bellet, Joseph Salmon, and Marc Tommasi. Differentially private coordinate de-
scent for composite empirical risk minimization. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba
Szepesvari, Gang Niu, and Sivan Sabato (eds.), Proceedings of the 39th International Conference on Ma-
chine Learning , volume 162 of Proceedings of Machine Learning Research , pp. 14948–14978. PMLR, 17–23
Jul 2022.
14Published in Transactions on Machine Learning Research (06/2023)
Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas.
Communication-Efficient Learning of Deep Networks from Decentralized Data. In Proceedings of the
20th International Conference on Artificial Intelligence and Statistics , pp. 1273–1282, 2017.
Ilya Mironov. Rényi differential privacy. 2017 IEEE 30th Computer Security Foundations Symposium (CSF) ,
2017.
Ilya Mironov, Kunal Talwar, and Li Zhang. Rényi differential privacy of the sampled gaussian mechanism.
arXiv preprint arXiv:1908.10530 , 2019.
Opacus. Opacus PyTorch library. Available from opacus.ai.
Edouard Oyallon, Sergey Zagoruyko, Gabriel Huang, Nikos Komodakis, Simon Lacoste-Julien, Matthew
Blaschko, and Eugene Belilovsky. Scattering networks for hybrid representation learning. IEEE Transac-
tions on Pattern Analysis and Machine Intelligence , 2019.
Nicolas Papernot, Abhradeep Thakurta, Shuang Song, Steve Chien, and Úlfar Erlingsson. Tempered sigmoid
activations for deep learning with differential privacy. Proceedings of the AAAI Conference on Artificial
Intelligence , pp. 9312–9321, 2021.
Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio. On the difficulty of training recurrent neural networks.
InProceedings of the 30th International Conference on Machine Learning , 2013.
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen,
Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary
DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and
Soumith Chintala. Pytorch: An imperative style, high-performance deep learning library. In Advances in
Neural Information Processing Systems , pp. 8024–8035. 2019a.
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen,
Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep
learning library. 32, 2019b.
Venkatadheeraj Pichapati, Ananda Theertha Suresh, Felix X Yu, Sashank J Reddi, and Sanjiv Kumar.
Adaclip: Adaptive clipping for private sgd. arXiv preprint arXiv:1908.07643 , 2019.
Reza Shokri and Vitaly Shmatikov. Privacy-preserving deep learning. In 2015 53rd Annual Allerton Con-
ference on Communication, Control, and Computing (Allerton) , pp. 909–910, 2015.
Reza Shokri, Marco Stronati, Congzheng Song, and Vitaly Shmatikov. Membership inference attacks against
machine learning models. In IEEE Symposium on Security and Privacy , 2017.
Ilya Sutskever, James Martens, George Dahl, and Geoffrey Hinton. On the importance of initialization and
momentum in deep learning. In Proceedings of the 30th International Conference on Machine Learning ,
pp. 1139–1147, 2013.
Florian Tramer and Dan Boneh. Differentially private learning needs better features (or much more data).
InInternational Conference on Learning Representations , 2021.
Thijs Vogels, Sai Praneeth Karimireddy, and Martin Jaggi. Powersgd: Practical low-rank gradient compres-
sion for distributed optimization. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d 'Alché-Buc, E. Fox,
and R. Garnett (eds.), Advances in Neural Information Processing Systems , volume 32. Curran Associates,
Inc., 2019.
Di Wang and Jinhui Xu. Differentially private empirical risk minimization with smooth non-convex loss
functions: A non-stationary view. Proceedings of the AAAI Conference on Artificial Intelligence , pp.
1182–1189, 2019.
15Published in Transactions on Machine Learning Research (06/2023)
Yu-Xiang Wang, Borja Balle, and Shiva Prasad Kasiviswanathan. Subsampled renyi differential privacy
and analytical moments accountant. In Proceedings of the Twenty-Second International Conference on
Artificial Intelligence and Statistics , 2019.
Max Welling and Yee Whye Teh. Bayesian learning via stochastic gradient langevin dynamics. In Proceedings
of the 30th International Conference on Machine Learning , 2011.
Qiang Yang, Yang Liu, Tianjian Chen, and Yongxin Tong. Federated machine learning: Concept and
applications. ACM Transactions on Intelligent Systems and Technology (TIST) , 2019.
Hongxu Yin, Arun Mallya, Arash Vahdat, Jose M. Alvarez, Jan Kautz, and Pavlo Molchanov. See through
gradients: Image batch recovery via gradinversion. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition (CVPR) , pp. 16337–16346, 2021.
Da Yu, Huishuai Zhang, Wei Chen, and Tie-Yan Liu. Do not let privacy overbill utility: Gradient embedding
perturbation for private learning. In International Conference on Learning Representations , 2021a.
Da Yu, Huishuai Zhang, Wei Chen, Jian Yin, and Tie-Yan Liu. Large scale private learning via low-rank
reparametrization. In International Conference on Machine Learning (ICML) , 2021b.
Da Yu, Saurabh Naik, Arturs Backurs, Sivakanth Gopi, Huseyin A Inan, Gautam Kamath, Janardhan
Kulkarni, Yin Tat Lee, Andre Manoel, Lukas Wutschitz, Sergey Yekhanin, and Huishuai Zhang. Differ-
entially private fine-tuning of language models. In International Conference on Learning Representations ,
2022.
Huanyu Zhang, Ilya Mironov, and Meisam Hejazinia. Wide network learning with differential privacy. arXiv
preprint arXiv:2103.01294 , 2021.
Jingzhao Zhang, Tianxing He, Suvrit Sra, and Ali Jadbabaie. Why gradient clipping accelerates training: A
theoretical justification for adaptivity. In International Conference on Learning Representations , 2020.
Yingxue Zhou, Steven Wu, and Arindam Banerjee. Bypassing the ambient dimension: Private SGD with
gradient subspace identification. In International Conference on Learning Representations , 2021.
Junyi Zhu and Matthew B. Blaschko. R-GAP: Recursive gradient attack on privacy. In International
Conference on Learning Representations , 2021.
Junyi Zhu, Ruicong Yao, and Matthew B. Blaschko. Surrogate model extension (SME): A fast and accurate
weight update attack on federated learning. In International Conference on Machine Learning , 2023.
Ligeng Zhu, Zhijian Liu, and Song Han. Deep leakage from gradients. In Advances in Neural Information
Processing Systems , 2019.
16Published in Transactions on Machine Learning Research (06/2023)
Appendix
In this appendix we provide full statements and proofs of our analyses (Appendix A-D). The pseudo code for
noisySGD,SGDwithgradientclippingandDP-SGDwithrankedsparsificationarepresentedinAppendixE.
Experimental results to verify the approximately symmetric property of the gradient deviation distribution
p(ξt)are demonstrated in Appendix F. More insights of gradual cooling are presented in Appendix G. Scaling
rule for efficient hyperparameter tuning is discussed in Appendix H. More details of DP-SGD with ranked
sparsifcation and connection between RS and selective sparsification or compression methods are given in
Appendix I. Limitations of applying RS to DP transfer learning are discussed in Appendix J. Comparison
between Poisson sampling and random shuffle is given in Appendix K.
Contents
A Proof of applying random sparsification to noisy SGD 18
A.1 Proof of Theorem 3.2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18
B Proof of applying random sparsification SGD with gradient clipping 19
B.1 Proof of Lemma 3.1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19
B.2 Proof of Equation (9) and Equation (10) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20
B.3 Proof of Lemma 3.3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20
C Proof of applying random sparsification to DP-SGD 22
C.1 Proof of Lemma 3.4 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22
C.2 Proof of Theorem 3.5 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22
C.3 Convergence bound with privacy budget variables . . . . . . . . . . . . . . . . . . . . . . . . . 22
D Proof of privacy against gradient reconstruction attacks 23
D.1 Proof of Theorem 5.1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23
E Additional algorithms 25
F Approximately symmetric distribution of the gradient deviation 27
G Further discussion of gradual cooling 28
H Scaling rule for hyperparameter tuning 29
I DP-SGD with ranked sparsification 31
J DP transfer learning with random sparsification 31
K Poisson sampling and random shuffle 32
17Published in Transactions on Machine Learning Research (06/2023)
A Proof of applying random sparsification to noisy SGD
Lemma A.1. u, v∈Rdare two arbitrary vectors, m∈{0,1}dis a random mask with sparsification rate r.
We have the following expectation, ∀k∈Z+,
E[⟨mk⊙u,v⟩] = (1−r)E[⟨u,v⟩]. (18)
Proof.
E[⟨mk⊙u,v⟩] =E[⟨m⊙u,v⟩] (19)
=/summationdisplay
iE[mi]E[uivi] (20)
= (1−r)E[⟨u,v⟩]. (21)
Corollary A.2. Following the notations of Lemma A.1, we have the expectation below:
E[⟨m⊙u,m⊙v⟩] = (1−r)E[⟨u,v⟩]. (22)
Proof.
E[⟨m⊙u,m⊙v⟩] =E[⟨m2⊙u,v⟩] (23)
18= (1−r)E[⟨u,v⟩]. (24)
A.1 Proof of Theorem 3.2
Proof.From theG-Lipschitz smoothness, and noting that Gaussian noise is added to the aggregated gradient
and will be divided by batch-size Bbefore updating the model, we have:
Lt+1≤Lt+⟨∇wt,wt+1−wt⟩+G
2∥wt+1−wt∥2(25)
=Lt−γ⟨∇wt,m⊙(∇wt+1
B(/summationdisplay
iξt,i+ξσ))⟩+Gγ2
2∥m⊙(∇wt+1
B(/summationdisplay
iξt,i+ξσ))∥2.(26)
Taking expectations on both sides and rearranging:
γE[⟨∇wt,m⊙(∇wt+1
B(/summationdisplay
iξt,i+ξσ))⟩]
≤E[Lt−Lt+1] +Gγ2
2E[∥m⊙(∇wt+1
B(/summationdisplay
iξt,i+ξσ))∥2],(27)
from which we can obtain:
(1−r)γ∥∇wt∥218
≤E[Lt−Lt+1] +(1−r)Gγ2
2(∥∇wt∥2+E[∥1
B/summationdisplay
iξt,i∥2] +dσ2
B2) (28)
≤E[Lt−Lt+1] +(1−r)Gγ2
2(∥∇wt∥2+σ2
g
B+dσ2
B2). (29)
We apply the same learning rate γ=1
G√
Tas in Bernstein et al. (2018), with which we can derive better
convergence under large noise. Combining ∥∇wt∥2, we have:
1−r
2G√
T∥∇wt∥2≤(1−r)(1
G√
T−1
2GT)∥∇wt∥2≤E[Lt−Lt+1] +1−r
2GT(σ2
g
B+dσ2
B2).(30)
18Published in Transactions on Machine Learning Research (06/2023)
Moving all the coefficients to the r.h.s.:
∥∇wt∥2≤2G√
T
1−rE[Lt−Lt+1] +1√
T(σ2
g
B+dσ2
B2). (31)
It is worth emphasizing that the factor 1−ron the r.h.s of Equation (30) is canceled out by the same factor
from the l.h.s. Consider the average over Tsteps:
1
T/summationdisplay
t∥∇wt∥2≤2G
(1−r)√
T/summationdisplay
tE[Lt−Lt+1] +1
T√
T/summationdisplay
t(σ2
g
B+dσ2
B2) (32)
=2G
(1−r)√
TE[L1−LT+1] +1√
T(σ2
g
B+dσ2
B2) (33)
≤2G
(1−r)√
TE[L1]−min
wL(w)] +1√
T(σ2
g
B+dσ2
B2). (34)
B Proof of applying random sparsification SGD with gradient clipping
Lemma B.1. u∈Rdis an arbitrary vector, m∈{0,1}dis a random mask with sparsification rate r. We
have the following inequality:
E[∥m⊙u∥]≥(1−r)∥u∥. (35)
Proof.
E[∥m⊙u∥] =1
∥u∥E[∥m⊙u∥∥u∥] (36)
≥1
∥u∥E[∥m⊙u∥2] (37)
22=1
∥u∥(1−r)E[⟨u,u⟩] (38)
= (1−r)∥u∥. (39)
Equality holds when r= 0.
B.1 Proof of Lemma 3.1
Proof.Following from the G-smoothness assumption, we have:
Lt+1≤Lt+⟨∇wt,wt+1−wt⟩+G
2∥wt+1−wt∥2(40)
=Lt−γ⟨∇wt,¯gt⟩+Gγ2
2∥¯gt∥2. (41)
Taking expectations on both sides and rearranging, we have:
E[⟨∇wt,¯gt⟩]≤1
γE[Lt−Lt+1] +Gγ
2E[∥¯gt∥2] (42)
≤1
γE[Lt−Lt+1] +γGC2
2, (43)
19Published in Transactions on Machine Learning Research (06/2023)
B.2 Proof of Equation (9) and Equation (10)
Similar to Lemma 3.1, consider gradient sparsification then clipping, we can obtain:
E[⟨∇wt,ˆgt⟩]≤1
γE[Lt−Lt+1] +γGC2
2. (44)
Now focusing on the l.h.s. we have:
E[⟨∇wt,ˆgt⟩] =⟨∇wt,E[ˆgt]⟩ (45)
=⟨∇wt,1
B/summationdisplay
iE[m⊙gt,i·min(1,C
∥m⊙gt,i∥)]⟩ (46)
=E[⟨∇wt,m⊙gt,i·min(1,C
∥m⊙gt,i∥)⟩] (47)
=E[⟨m⊙∇wt,m⊙gt,i·min(1,C
∥m⊙gt,i∥)⟩] (48)
=Em/bracketleftigg
Eξt[⟨∇w′
t,g′
t,i·min(1,C
∥g′
t,i∥)⟩]/bracketrightigg
(49)
=Em/bracketleftigg
Eξ′
t∼˜p′
t[⟨∇w′
t,g′
t,i·min(1,C
∥g′
t,i∥)⟩]/bracketrightigg
+Em[b′
t], (50)
where we define∇w′
t:=m⊙∇wt,ξ′
t:=m⊙ξtwith corresponding true distribution p′
tand proxy ˜p′
twhich
are projected from ptand ˜pt,b′
t:=Eξ′
t∼p′
t[⟨∇w′,ˆgt⟩]−Eξ′
t∼˜p′
t[⟨∇w′,ˆgt⟩]. Equation (49) holds because ξt
andmare independent. Since the projection of a symmetric distribution to a subspace is symmetric, i.e.
˜p′
t(ξ′
t) = ˜p′
t(−ξ′
t). We can adapt Theorem 3.3 for the first term:
Em/bracketleftigg
Eξ′
t∼˜p′
t[⟨∇w′
t,g′
t,i·min(1,C
∥g′
t,i∥)⟩]/bracketrightigg
≥Em[Pξ′
t∼˜p′
t(∥ξ′
t∥<3C/4)h(∇w′
t)∥∇w′
t∥]. (51)
B.3 Proof of Lemma 3.3
Proof.Consider the following two cases:
case 1:∥∇wt∥≤C/4, thenPm(∥∇w′
t∥≤C/4) = 1,
Em[Pξ′
t∼˜p′
t(∥ξ′
t∥<3C/4)h(∇w′
t)∥∇w′
t∥]≥Pξt∼˜pt(∥ξt∥<3C/4)Em[h(∇w′
t)∥∇w′
t∥] (52)
=Pξt∼˜pt(∥ξt∥<3C/4)Em[∥∇w′
t∥2] (53)
22=Pξt∼˜pt(∥ξt∥<3C/4)(1−r)∥∇wt∥2(54)
= (1−r)Pξt∼˜pt(∥ξt∥<3C/4)h(∇wt)∥∇wt∥.(55)
case 2:∥∇wt∥>C/ 4, consider two events A1:∥∇w′
t∥≤C/4;A2:∥∇w′
t∥>C/ 4then we have:
Em[Pξ′
t∼˜p′
t(∥ξ′
t∥<3C/4)h(∇w′
t)∥∇w′
t∥]≥Pξt∼˜pt(∥ξt∥<3C/4)Em[h(∇w′
t)∥∇w′
t∥] (56)
54
≥Pξt∼˜pt(∥ξt∥<3C/4)(P(A1)(1−r)∥∇wt∥2+(57)
P(A2)C/4·Em[∥∇w′
t∥])
35
≥Pξt∼˜pt(∥ξt∥<3C/4)(P(A1)(1−r)∥∇wt∥2+(58)
P(A2)C
4(1−r)∥∇wt∥)
≥Pξt∼˜pt(∥ξt∥<3C/4)(1−r)C
4∥∇wt∥ (59)
= (1−r)Pξt∼˜pt(∥ξt∥<3C/4)h(∇wt)∥∇wt∥, (60)
20Published in Transactions on Machine Learning Research (06/2023)
from which we conclude that:
Em[Pξ′
t∼˜p′
t(∥ξ′
t∥<3C/4)h(∇w′
t)∥∇w′
t∥]≥(1−r)Pξt∼˜pt(∥ξt∥<3C/4)h(∇wt)∥∇wt∥.(61)
The inequality above implies that ∃κt≥1−rsuch that:
Em[Pξ′
t∼˜p′
t(∥ξ′
t∥<3C/4)h(∇w′
t)∥∇w′
t∥] =κtPξt∼˜pt(∥ξt∥<3C/4)h(∇wt)∥∇wt∥. (62)
Wenotethat κt= 1ifr= 0andundernaturalassumption, e.g. Pξ′
t∼˜p′
t(∥ξ′
t∥<3C/4)>Pξt∼˜pt(∥ξt∥<3C/4),
we haveκt>1−rifr>0.
Now we form the lower bound for κtusing the condition Pξt∼˜pt(∥ξt∥<3C/4)≥√1−r. We emphasize that
as the network converges during training, the following property will be satisfied with the true probability
distribution pt:
Pξt∼pt(∥ξt∥<3C/4)≥√
1−r. (63)
So the condition of choosing ˜ptwill not lead to significant bias term bt. Based on the condition, we have:
Pξt∼˜pt(∥ξt∥<3C/4)h(∇w)∥∇w∥≥√
1−rh(∇w)∥∇w∥. (64)
Consider two cases:
case 1:∥∇w∥≥C/4, r.h.s. of Equation (64) has:
√
1−rh(∇w)∥∇w∥≥h(∇w)((1−r)∥∇w∥2)1/2(65)
22=h(∇w)(Em[∥∇w′∥2])1/2(66)
≥h(∇w)Em[∥∇w′∥] (67)
=Em[h(∇w)∥∇w′∥] (68)
≥Em[h(∇w′)∥∇w′∥], (69)
where Equation (67) is taken according to Jensens’ inequality.
case 2:∥∇w∥≤C/4, so we have∥∇w′∥≤C/4, then:
√
1−rh(∇w)∥∇w∥=√
1−r∥∇w∥2(70)
≥(1−r)∥∇w∥2(71)
22=Em[∥∇w′∥2] (72)
=Em[h(∇w′)∥∇w′∥]. (73)
Combing two cases, we can obtain the following property:
Pξt∼˜pt(∥ξt∥<3C/4)h(∇w)∥∇w∥≥√
1−rh(∇w)∥∇w∥ (74)
≥Em[h(∇w′)∥∇w′∥] (75)
≥Em[Pξ′
t∼˜p′
t(∥ξ′
t∥<3C/4)h(∇w′)∥∇w′∥]. (76)
It is worth noting that the above inequality can also be obtained under other conditions, the one we provided,
i.e.Pξt∼˜pt(∥ξt∥<3C/4)≥√1−r, is a sufficient but not necessary condition, which will be definitely
satisfied as the network converges, i.e. ∥∇w∥→0,∥ξt∥→0.
Combining Equation (76) with Equation (61), we obtain: ∃κt∈(1−r,1),
Em[Pξ′
t∼˜p′
t(∥ξ′
t∥<3C/4)h(∇w′
t)∥∇w′
t∥] =κtPξt∼˜pt(∥ξt∥<3C/4)h(∇wt)∥∇wt∥. (77)
κttakes 1 when r= 0.
21Published in Transactions on Machine Learning Research (06/2023)
C Proof of applying random sparsification to DP-SGD
C.1 Proof of Lemma 3.4
Proof.Following from the smoothness assumption, we have:
Lt+1≤Lt+⟨∇wt,wt+1−wt⟩+G
2∥wt+1−wt∥2(78)
=Lt−γ⟨∇wt,ˆgt+m⊙ξDP⟩+Gγ2
2∥ˆgt+m⊙ξDP∥2(79)
=Lt−γ⟨∇wt,ˆgt⟩−γ⟨∇wt,m⊙ξDP⟩+Gγ2
2∥ˆgt+m⊙ξDP∥2, (80)
taking expectations on both sides and rearranging, we have:
E[⟨∇wt,ˆgt⟩]≤1
γE[Lt−Lt+1]−E[⟨∇wt,m⊙ξDP⟩] +Gγ
2E[∥ˆgt+m⊙ξDP∥2] (81)
=1
γE[Lt−Lt+1]−0 +Gγ
2(E[∥ˆgt∥2] +E[∥m⊙ξDP∥2]−0) (82)
22=1
γE[Lt−Lt+1] +Gγ
2(E[∥ˆgt∥2] + (1−r)C2σ2d
B2) (83)
≤1
γE[Lt−Lt+1] +Gγ
2(C2+ (1−r)C2σ2d
B2). (84)
C.2 Proof of Theorem 3.5
Proof.Combine Equation (9), 10 and Lemma 3.3:
κtPξt∼˜pt(∥ξt∥<3C/4)h(∇wt)∥∇wt∥+Em[b′
t]≤E[⟨∇wt,ˆgt⟩]. (85)
Substitute Lemmma 3.4 and rearrange:
κtPξt∼˜pt(∥ξt∥<3C/4)h(∇wt)∥∇wt∥≤E[⟨∇wt,ˆgt⟩]−Em[bt], (86)
κtPξt∼˜pt(∥ξt∥<3C/4)h(∇wt)∥∇wt∥≤1
γE[Lt−Lt+1] +γGC2
2+ (1−r)γ∆σ−Em[b′
t],(87)
Pξt∼˜pt(∥ξt∥<3C/4)h(∇wt)∥∇wt∥≤1
κt(E[Lt−Lt+1]
γ+γGC2
2−Em[b′
t]) +1−r
κtγ∆σ.(88)
Sinceκ1:T∈(1−r,1), then∃κ∈(1−r,1), such that consider all T iterations, we have:
1
TT/summationdisplay
t=1Pξt∼˜pt(∥ξt∥<3C/4)h(∇wt)∥∇wt∥≤1
κ(∆L
γT+γGC2
2−1
TT/summationdisplay
t=1Em[b′
t]) +1−r
κγ∆σ,(89)
C.3 Convergence bound with privacy budget variables
Theorem C.1 (Abadi et al. (2016b)) .There exist constants uandvso that give sampling probability
q=B/Nand the number of steps T, for anyε<vq2T, DP-SGD is (ε,δ)-differentially private for any δ>0
if we choose:
σ≥uq/radicalbig
Tlog(1/δ)
ε. (90)
22Published in Transactions on Machine Learning Research (06/2023)
So to achieve (ε,δ)-differential privacy, we choose σ=uq√
Tlog(1/δ)
εand note that for DP-SGD, large batch
is preferred in practice, the following corollary proves that the upper bound of true gradient norm ∇wt
decreases w.r.t. iterations T:
Corollary C.1.1. Follow Theorem 3.5, consider the overall Titerations with learning rate γ=1
G√
T,
batch-sizeB=√
T, privacy budget (ε, δ), we have:
1
TT/summationdisplay
t=1Pξt∼˜pt(∥ξt∥<3C/4)h(∇wt)∥∇wt∥≤1
κ(∆LG√
T+C2
2√
T−1
TT/summationdisplay
t=1Em[b′
t]) +1−r
κ∆ε,(91)
where we define ∆ε:=dC2u2q2log(1/δ)
2√
Tε2.
Proof.
1
TT/summationdisplay
t=1Pξt∼˜pt(∥ξt∥<3C/4)h(∇wt)∥∇wt∥ (92)
≤1
κ(∆L
γT+γGC2
2−1
TT/summationdisplay
t=1Em[b′
t]) +1−r
κγ∆σ (93)
=1
κ(∆LG√
T+C2
2√
T−1
TT/summationdisplay
t=1Em[b′
t]) +1−r
κγ∆σ. (94)
Focusing on γ∆σ:
γ∆σ=γC2dG
2B2·u2q2Tlog(1/δ)
ε2(95)
=γC2dG
2·u2q2log(1/δ)
ε2(96)
=dC2u2q2log(1/δ)
2√
Tε2, (97)
Plugging Equation (97) into Equation (94):
1
TT/summationdisplay
t=1Pξt∼˜pt(∥ξt∥<3C/4)h(∇wt)∥∇wt∥≤1
κ(∆LG√
T+C2
2√
T−1
TT/summationdisplay
t=1Em[b′
t]) +1−r
κ∆ε.(98)
D Proof of privacy against gradient reconstruction attacks
D.1 Proof of Theorem 5.1
Proof.Analogous to Kx=g(Zhu & Blaschko, 2021, Equation (15)), we have the following system of linear
equations:
m⊙kx=m⊙g. (99)
Assume that m⊙k̸= 0(otherwise the reconstruction will fail, i.e. arbitrary reconstruction error), xcan be
estimated through least squares:
ˆx=/parenleftbig
(m⊙k)⊤(m⊙k)/parenrightbig−1(m⊙k)⊤(m⊙(g+ξσ)). (100)
23Published in Transactions on Machine Learning Research (06/2023)
The squared error of reconstruction can be expressed as:
∥x−ˆx∥2=∥x−/parenleftbig
(m⊙k)⊤(m⊙k)/parenrightbig−1(m⊙k)⊤(m⊙(g+ξσ))∥2(101)
=∥x−/parenleftbig
(m⊙k)⊤(m⊙k)/parenrightbig−1(m⊙k)⊤(g+ξσ)∥2(102)
=∥/parenleftbig
(m⊙k)⊤(m⊙k)/parenrightbig−1(m⊙k)⊤ξσ∥2. (103)
Take expectations on both sides:
E[∥x−ˆx∥2] =E[∥/parenleftbig
(m⊙k)⊤(m⊙k)/parenrightbig−1(m⊙k)⊤ξσ∥2(104)
=E[ξ⊤
σm⊙k
∥m⊙k∥2(m⊙k)⊤
∥m⊙k∥2ξ] (105)
=E[Tr{ξσξ⊤
σm⊙k
∥m⊙k∥2(m⊙k)⊤
∥m⊙k∥2}] (106)
= Tr{E[ξσξ⊤
σ]E[m⊙k
∥m⊙k∥2(m⊙k)⊤
∥m⊙k∥2]} (107)
= Tr{σ2IdE[m⊙k
∥m⊙k∥2(m⊙k)⊤
∥m⊙k∥2]} (108)
=σ2E[Tr{m⊙k
∥m⊙k∥2(m⊙k)⊤
∥m⊙k∥2}] (109)
=σ2E[1
∥m⊙k∥2]. (110)
We see that as more gradients are zeroed out, the expected squared error increases. In particular, according
to Jensen’s inequality and the convexity of 1/∥m⊙k∥2:
E[∥x−ˆx∥2]≥σ2
E[∥m⊙k∥2](111)
22=σ2
(1−r)∥k∥2. (112)
The lower bound of expected squared error increases monotonically with increasing sparsification rate r.
24Published in Transactions on Machine Learning Research (06/2023)
E Additional algorithms
We provide pseudo codes for Noisy SGD with RS (Algorithm 2), SGD with gradient clipping and RS
(Algorithm 3), and DP-SGD with ranked sparsification (Algorithm 4).
Algorithm 2 Noisy SGD with Random Sparsification
Input:Initial parameters w0; EpochsE; Batch size B; Sparsification rate: r∗; Momentum: µ; Learning
rateγ; Noise multiplier σ.
1:fore= 0toE−1do
2:▷Gradual cooling ◁
3:r(e) =r∗·e
E−1;
4:▷Generate a random mask every epoch ◁
5:m∈{0,1}d, s.t.∥m∥1=d·(1−r(e));
6:fort= 0toT−1do
7:▷Compute aggregated gradients ◁
8:gt=/summationtext
i∇ℓ(wt,xi);
9:▷Sparsify gradient ◁
10:g′
t=m⊙gt;
11:▷Add sparsified noise ◁
12: ˆgt=1
B(g′
t+m⊙N(0,σ2Id));
13:▷Update parameters ◁
14:vt+1=µ·vt+ ˆgt,wt+1=wt−γvt+1;
Algorithm 3 SGD with Gradient clipping and Random Sparsification
Input: Initial parameters w0; EpochsE; Batch size B; Sparsification rate: r∗; Clipping bound: C;
Momentum: µ; Learning rate γ.
1:fore= 0toE−1do
2:▷Gradual cooling ◁
3:r(e) =r∗·e
E−1;
4:▷Generate a random mask every epoch ◁
5:m∈{0,1}d, s.t.∥m∥1=d·(1−r(e));
6:fort= 0toT−1do
7:▷For eachxiin the minibatch of size B ◁
8:gt,i=∇ℓ(wt,xi);
9:▷Sparsify gradient ◁
10:g′
t,i=m⊙gt,i;
11:▷Clip each individual gradient ◁
12: ˆgt=1
B/summationtext
ig′
t,i·min(1,C/∥g′
t,i∥);
13:▷Update parameters ◁
14:vt+1=µ·vt+ ˆgt,wt+1=wt−γvt+1;
25Published in Transactions on Machine Learning Research (06/2023)
Algorithm 4 DP-SGD with Ranked Sparsification
Input:Initial parameters w0; EpochsE; Batch size B; Sparsification rate: r∗; Clipping bound: C;
Momentum: µ; Learning rate γ; Noise multiplier σ.
1:fore= 0toE−1do
2:ze={0}d;
3:▷Gradual Cooling ◁
4:r(e) =r∗·e
E−1;
5:▷Generate a ranked mask every epoch ◁
6:ifeis0then
7:m={1}d;
8:else
9: Sort the indices [1,...,d ]with respect to the magnitude of aggregated gradient of the last epoch
|ze−1|in ascending order then set the first d·r(e)positions in mask mto 0 and the rest to 1;
10:fort= 0toT−1do
11:▷For eachxiin the minibatch of size B ◁
12:gt,i=∇ℓ(wt,xi);
13:▷Sparsify gradient ◁
14:g′
t,i=m⊙gt,i;
15:▷Clip each individual gradient ◁
16: ˆgt,i=g′
t,i·min(1,C/∥g′
t,i∥);
17:▷Add sparsified noise ◁
18: ˜gt=1
B(/summationtext
iˆgt,i+m⊙N(0,C2σ2Id));
19:▷Update parameters ◁
20:vt+1=µ·vt+gt, wt+1=wt−γvt+1;
21:▷Save perturbed gradients ◁
22:ze=ze+ ˜gt+1
B(1−m)⊙N(0,C2σ2Id);
26Published in Transactions on Machine Learning Research (06/2023)
F Approximately symmetric distribution of the gradient deviation
Asthegradientdeviationdistribution p(ξt)ishigh-dimensional, demonstratingandverifyingitssymmetricity
is in general intractable. We therefore adopt random projection onto a 2D for the visualization, which is
also adopted by Chen et al. (2020). Our experiments reproduce their observation that the gradient deviation
distribution approximates symmetry during training with DP-SGD (see Figure 6 and 7). We verify that
this property is also valid for DP-SGD with RS (see Figure 8 and 9). We have repeated the experiments 10
times, all results are qualitatively the same as presented here.
(a) Epoch 0
 (b) Epoch 3
 (c) Epoch 6
 (d) Epoch 9
(e) Epoch 20
 (f) Epoch 40
 (g) Epoch 60
 (h) Epoch 79
Figure 6: Gradient deviation distribution during training, projected onto a 2D using a random matrix.
Network is DP-CNN, method is DP-SGD, dataset is CIFAR10.
(a) Epoch 0
 (b) Epoch 3
 (c) Epoch 6
 (d) Epoch 9
(e) Epoch 20
 (f) Epoch 40
 (g) Epoch 60
 (h) Epoch 79
Figure 7: Gradient deviation distribution during training, projected onto a 2D using a random matrix.
Network is DP-CNN, method is DP-SGD, dataset is CIFAR10.
27Published in Transactions on Machine Learning Research (06/2023)
(a) Epoch 0
 (b) Epoch 3
 (c) Epoch 6
 (d) Epoch 9
(e) Epoch 20
 (f) Epoch 40
 (g) Epoch 60
 (h) Epoch 79
Figure 8: Gradient deviation distribution during training, projected onto a 2D using a random matrix.
Network is DP-CNN, method is DP-SGD with RS, dataset is CIFAR10.
(a) Epoch 0
 (b) Epoch 3
 (c) Epoch 6
 (d) Epoch 9
(e) Epoch 20
 (f) Epoch 40
 (g) Epoch 60
 (h) Epoch 79
Figure 9: Gradient deviation distribution during training, projected onto a 2D using a random matrix.
Network is DP-CNN, method is DP-SGD with RS, dataset is CIFAR10.
G Further discussion of gradual cooling
For the convenience we restate Equation (14) here, and for the clarity we define ∆C:=∆L
γT+γGC2
2−
1
T/summationtextT
t=1Em[b′
t]to denote the terms that are enlarged by RS:
1
TT/summationdisplay
t=1Pξt∼˜pt(∥ξt∥<3C/4)h(∇wt)∥∇wt∥≤1
κ∆C+1−r
κγ∆σ. (113)
We see ∆σ=C2σ2dG/2B2can be computed given the constants, assume ∆Cis also given, to deduce the
optimal sparsification rate, we need to relate κtor. However, beyond giving a range κ∈(1−r,1), it is not
possible to represent κbyrwithout supposing an additional assumption over the shape of the distribution of
the gradient deviation, which is difficult to make. We therefore start from the empirical evidence in Figure 3.
28Published in Transactions on Machine Learning Research (06/2023)
Recall that the factor introduced by RS for noisy SGD is 1/(1−r)(see Theorem 3.2) and for SGD with
gradient clipping is 1/k(see Theorem 3.4). From Figures 3a, 3b, 3c, we see that as rincreases SGD
with gradient clipping also obviously converges slower, which implies that 1/kalso grows with r. From
Figure 3d, 3e, 3f, we find that the performance of noisy SGD tends to drop more dramatically in a high
sparsification regime, like a parabolic shape, which implies 1/kis comparatively more stable than 1/(1−r)
asrchanges.
Based on these observations from Figure 2, we suppose that κ=√1−r, which complies with the theoretical
analysis that κ= 1whenr= 0andκ∈(1−r,1). Using this relation we can express the convergence bound
as a function of sparsification rate r:
U(r) :=1√1−r∆C+√
1−r∆σ. (114)
To find the optimal r∗, we can simply compute the derivative of U:
U′(r) = 1/2(1−r)−3/2∆C−1/2(1−r)−1/2∆σ (115)
Consider the first order condition, we have:
r∗= 1−∆C/∆σ. (116)
So when ∆C>∆σ, i.e. the noise term is insignificant in the convergence bound, we have r∗= 0is opti-
mal. While when ∆σ>∆C, we haver∗= 1−∆C/∆σis optimal, and as ∆σbecomes dominant, higher
sparsification rate is preferred. We see this conclusion matches our empirical evidence in Figure 2.
Although in practice ∆Cis unknown and r∗therefore cannot be precisely estimated, the observation and
conclusion above reason and support the gradual cooling. At early training stages, the network converges
fast:E[Lt−Lt+1]is large, while at late training stages, the optimization reaches a plateau: E[Lt−Lt+1]
decays, in contrast ∆σis constant and becomes relatively large, the best sparsification rate r∗thus should
be increasing during training.
H Scaling rule for hyperparameter tuning
First order momentum is commonly adopted in the optimization with DP-SGD, because momentum can
alleviate oscillation and accelerate gradient descent (Sutskever et al., 2013), it is therefore believed to reduce
the number of iterations of training and therefore achieve less privacy loss. However, for privacy-preserving
training, momentum will also exaggerate the added Gaussian noise by incorporating current and all historical
noise.
Denote velocity update: vt+1=µ·vt+a·˜gt+1, wherev, µ, ˜gdenote perturbed velocity, momentum and
perturbed gradients, respectively, common implementation of SGD with momentum includes a= 1, e.g. for
Pytorch (Paszke et al., 2019a) or a=−γ, e.g. for Tensorflow (Abadi et al., 2016a). Using the expression of
one step noise in Equation (4) and denoting by ˆvtthe velocity after separating the noise, we have:
vt+1−ˆvt+1= (1 +µ+µ2+...+µt)·aN(0,C2σ2Id). (117)
After many iterations, the scalar approximates a geometric series, i.e.:
vt+1−ˆvt+1≈1
1−µ·aN(0,C2σ2Id), (118)
with a residual decaying exponentially. Pulling the clipping bound Cout and forming the noise as C(1−µ)·
aN(0,σ2Id), we present a scaling rule for adjusting Candµ, namely the local optimal values of C,µsatisfy
the sameC/(1−µ): the same amount of noise. As shown in Figure 10, the local optima of every column or
row is located on the diagonal, where C/(1−µ) =C∗/(1−µ∗)(C∗,µ∗are given in previous works).
The scaling rule suggests a workload saving pipeline for tuning, as it decomposes a 2D search into a line
search: i) Set either Corµfixed and search for the other (search in a row or column); ii) Adjust C,µjointly
according to the scaling rule to find the optimal combination (searching along the diagonal).
29Published in Transactions on Machine Learning Research (06/2023)
(a) DP-CNN, ε= 3
 (b) DP-CNN, ε= 7.53
(c) DP-Transfer Learning,
ε= 2
(d) Handcrafted CNN, ε= 3
Figure 10: Accuracy (%) under different combinations of clipping bound Cand momentum µ. The diagonals
represent combinations adjusted by the scaling rule based on values of Candµgiven in previous works. The
red dashed square marks the given hyperparameters, and the red solid square the optimal hyperparameters.
We note that the scaling rule assumes noise being directly added to velocity, for projected DP-SGD where
the noise has been projected, this rule may not be applied. Additionally, for some adaptive optimizers, e.g.
Adam (Kingma & Ba, 2014), computing of velocity is implemented as follows:
vt+1=µ·vt+ (1−µ)˜gt, (119)
v∗
t+1=vt+1/(1−µt), (120)
If we ignore the scaling in Equation (120) where the factor 1−µtincreases rapidly to 1, we have:
vt+1−ˆvt+1= (1 +µ+µ2+...+µt)·(1−µ)N(0,C2·σ2Id) (121)
≈1
1−µ·(1−µ)N(0,C2·σ2Id) (122)
=N(0,C2·σ2Id). (123)
Here we see Candµare decoupled in term of the noise amount, which suggests that they can be tuned
independently. So searching for the best combination can be decomposed into searching in a row followed
by its column or vice versa, as the best µis the same for different Cand vice versa, see Figure 11.
(a)ε= 3
 (b)ε= 7.53
Figure 11: Accuracy (%) under different combinations of clipping bound and first order momentum. The
network is DP-CNN and the optimizer is Adam (Kingma & Ba, 2014).
30Published in Transactions on Machine Learning Research (06/2023)
I DP-SGD with ranked sparsification
For ranked sparsification, we consider a straightforward implementation which ranks the (absolute) mean of
the perturbed gradients of last epoch and sparsifies accordingly. As DP is robust to post-processing, such
ranked sparsification does not consume the privacy budget.
Algorithm 4 describes the proposed ranked sparsification. Note that the noise added to the gradient mean
estimation zeis not sparsified (Line 22), otherwise the coordinates that get masked out at the first iteration
will receive gradient mean estimation as 0 and never get updated for all the remaining iterations, which
will degrade the network. Adding noise to sparsified coordinates can give these coordinates a chance to be
ranked in higher positions, while in turn the coordinates being updated but with low magnitude of their true
gradient may get masked out in the next iteration.
However, it turns out that ranked sparsification cannotoutperform random sparsification, more precisely
it performs the same as RS. We further find that it seems the ranking is fundamentally random as the
ranking is dominated by Gaussian noise. To demonstrate this, we run random sparsification and ranked
sparsification for 100 epochs, then statistically analyze the distribution of how many times a parameter is
masked out. The result shows the equivalency between these two strategies, which implies even averaging the
perturbed gradients over a full epoch cannot sufficiently mitigate the noise added in gradient perturbation,
see Figure 12.
Figure 12: Histogram of the number of parameters vs. the number of times a parameter is masked out
during 100 epochs training. For comparison we present non-privately ranked sparsification, i.e. ranking after
excluding noise. Random sparsification and ranked sparsification mostly overlap.
This indicates that in the context of DP-SGD selecting the unimportant coordinates is difficult and stochas-
ticity cannot be fully eliminated. Although there exist options to increase the precision of ranking, for
example using public dataset (Zhou et al., 2021; Yu et al., 2021a) or via sparse vector techniques (SVT)
and DP selection (Zhang et al., 2021; Dwork & Roth, 2014), there is probably a mismatch of the empirical
distributions of public dataset and private dataset, while SVT and DP selection definitely contains random-
ness. However, as we show with RS, under such randomness, sparsification could still be beneficial for the
optimization, which was not realized before.
J DP transfer learning with random sparsification
Differentially private (DP) transfer learning has recently gained popularity due to its effectiveness in various
downstream tasks (Tramer & Boneh, 2021; Li et al., 2022; Yu et al., 2022; Bu et al., 2022). DP linear probing
reuses the feature extractor, significantly reducing the gradient dimension (Abadi et al., 2016b; Tramer &
Boneh, 2021). Li et al. (2022) demonstrated that DP fine-tuning of pre-trained large language models can
achieve performance close to non-private fine-tuning. Meanwhile, Yu et al. (2022); Bu et al. (2022) showed
that parameter-efficient fine-tuning outperforms full fine-tuning.
31Published in Transactions on Machine Learning Research (06/2023)
However, when applying RS to DP transfer learning, we have not observed significant performance improve-
ment with high sparsification rates. The reason is that the configurations of DP transfer learning reduce the
efficiency of RS. Based on Remark 3, RS is beneficial when the clipping bound C, noise multiplier σ, and
gradient dimension dare large, while the batch size Bis small. Although DP transfer learning adopts large
models, linear probing or parameter-efficient methods constrain the gradient dimension dto be less than 1%
(Yu et al., 2022) or even 0.1%(Bu et al., 2022) of the original parameter space, rendering it small. Fur-
thermore, since pre-trained networks converge quickly on downstream tasks, DP transfer learning methods
typically employ hyperparameters such as large batch size B, small clipping bound C, and noise multiplier σ,
making random sparsification less efficient. In contrast, in a training-from-scratch scenario, networks require
more iterations to train. Large batch size Band small noise multiplier σresult in fewer iterations for a given
privacy budget while small clipping bound Climits the gradient magnitude, therefore they are not preferred
in the hyperparameter tuning. And random sparsification is more favorable for training from scratch.
RS may be beneficial for DP full fine-tuning where the gradient dimension dis sufficiently large. However,
as this work focuses on the unique interaction between DP-SGD and RS, we leave a more comprehensive
study of practical usage of RS for future research.
K Poisson sampling and random shuffle
In the analysis of DP-SGD Abadi et al. (2016b), Poisson sampling is used to induce the privacy amplification
(Balle et al., 2018; Wang et al., 2019; Mironov et al., 2019). In the implementation of our baselines and many
previous works (Papernot et al., 2021; Tramer & Boneh, 2021; Yu et al., 2021a), sampling is implemented by
random shuffling the dataset and partitioning with fixed batch size, i.e. uniform sampling without replace-
ment. It is reported that the model performance remains approximately the same under these two sampling
schemes (Tramer & Boneh, 2021). In this work, we follow the baselines to conduct the experiments. To
validate that the benefit of RS is consistent under this difference, we redo the experiments with DP-CNN on
CIFAR10. The results are given in Table 3. We note that there is no significant difference in performance
between "partition" and "Poisson" settings, and that the advantages of RS hold in both.
Sampling εBaseline RS (ours) Difference
Parition3.0 62.8±0.10 64.3±0.17 +1.5
1.0 52.5±0.25 55.1±0.11 +2.6
Poisson3.0 62.8±0.27 64.6±0.25 +1.8
1.0 52.8±0.24 55.2±0.25 +2.4
Table 3: Test accuracy (% ±SEM) before and after adopting random sparsification with different sampling
schemes. Partition indicates random shuffling the dataset and partitioning into batches of fixed size per
epoch.
32