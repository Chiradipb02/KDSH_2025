Published in Transactions on Machine Learning Research (01/2023)
Regularized Training of Intermediate Layers for Generative
Models for Inverse Problems
Sean Gunn gunn.s@northeastern.edu
Khoury College of Computer Sciences
Northeastern University
Jorio Cocola jcocola@seas.harvard.edu
Institute for Applied Computational Science
Harvard University
Paul Hand p.hand@northeastern.edu
Department of Mathematics and Khoury College of Computer Sciences
Northeastern University
Reviewed on OpenReview: https: // openreview. net/ forum? id= cKsKXR28cG
Abstract
Generative Adversarial Networks (GANs) have been shown to be powerful and flexible priors
when solving inverse problems. One challenge of using them is overcoming representation
error, the fundamental limitation of the network in representing any particular signal.
Recently, multiple proposed inversion algorithms reduce representation error by optimizing
over intermediate layer representations. These methods are typically applied to generative
models that were trained agnostic of the downstream inversion algorithm. In our work, we
introduce a principle that if a generative model is intended for inversion using an algorithm
based on the optimization of intermediate layers, it should be trained in a way that regularizes
those intermediate layers. We instantiate this principle for two notable recent inversion
algorithms: Intermediate Layer Optimization and the Multi-Code GAN prior. For both of
these inversion algorithms, we introduce a new regularized GAN training algorithm and
demonstrate that the learned generative model results in lower reconstruction errors across
a wide range of under-sampling ratios when solving compressed sensing, inpainting, and
super-resolution problems.
1 Introduction
The task in an inverse problem is to estimate an unknown signal given a (possibly noisy) set of measurements
of that signal. In practice, inverse problems are often ill-posed, which often require the incorporation of prior
information about the target signal to recover a reasonable estimate of it.
Deep generative models have demonstrated remarkable performance when used as priors for solving inverse
problems (Bora et al., 2017; Athar et al., 2018; Hand et al., 2018; Menon et al., 2020; Mardani et al., 2018;
Mosser et al., 2020; Pan et al., 2021). Typically, a generative modeling-based approach for inverse problems
has two phases: a training phase and an inversion/deployment phase. In the first phase, a generative
network is trained on a dataset of images. After training, the network parameters are typically fixed and an
optimization problem involving the known forward model is solved to estimate an unknown signal of interest.
An advantage of this class of methods is that the prior can be trained entirely in an unsupervised manner
and without previous knowledge of the specific inverse problem that needs to be solved downstream. This
allow the trained network to be used for a variety of inverse problems.
1Published in Transactions on Machine Learning Research (01/2023)
Some generative networks explicitly map a low-dimensional input to a high-dimensional signal space. The
outputs of the generative network, therefore, have a low intrinsic dimensionality. While this is useful for
regularizing the inverse problem, it can limit the expressivity of the network and lead to representation error ,
the error between a target signal and the closest signal in the range of the generative network.
Several recent methods have attempted to reduce representation error by introducing optimization algorithms
that enlarge the search space of the inversion algorithm. These include methods that optimize both over
the input of the network and over the activations of one or more intermediate layers. Examples of this class
of inversion methods include Intermediate Layer Optimization (ILO) (Daras et al., 2021), GAN Surgery
(Smedemark-Margulies et al., 2021), and the Multi-code GAN Prior (mGANprior) (Gu et al., 2020). These
algorithms were applied to state-of-the-art off-the-shelf GANs, such as PGGAN and StyleGAN (Karras
et al., 2018; 2020), which were trained agnostic to the specific optimization algorithm that would be used
for inversion. Consequently, the inversion algorithms optimize over a region of the space of intermediate
presentations that were not directly regularized during training. This suggests that it may be possible to
improve the reconstruction performance of these algorithms by explicitly regularizing the intermediate layers
during the training phase. In this paper, we confirm this hypothesis.
Figure 1: The workflow for using Regularized Training of Interme-
diate Layers (RTIL) to develop new training algorithms of GANs
that will be used for inversion. See the text for details.We consider the problem of training a
GAN that will be used for solving inverse
problems via an inversion algorithm that
optimizes over intermediate layers. We
putforwardthefollowingprincipletermed
Regularized Training of Intermediate Lay-
ers (RTIL) :
If a GAN’s intermediate
layers are optimized during inversion,
they should be regularized during training.
We use this principle to derive new train-
ingalgorithmsforGANsthatareintended
for solving inverse problems. For ILO and
mGANprior, we introduce a new training
algorithm for StyleGAN2 and PGGAN,
respectively. For these new trained GANs,
we achieve improved reconstruction qual-
ity relative to the corresponding GANs
trained without the principle. We note
that this paper is not about regularizing
GANs in order to stabilize training and
aiding the convergence of gradient descent
(as in Gulrajani et al. (2017a); Mescheder
et al. (2018)). Rather, those regularization methods can be used in conjunction with RTIL to improve upon
approximate learned inference for a specific algorithm used in solving inverse problems.
Our proposed principle may lead to the following workflow for solving inverse problems with GANs. First,
train a generative network with latent variable z0, sampled from a latent distribution pz0, and outputting
signalsxfrom the target distribution (Figure 1 Step 1). Second, explore various inversion algorithms,
including some that optimize over intermediate layers by introducing an additional optimization variable z1
(Figure 1 Step 2). Algorithms of this type include ILO and mGANprior. Third, if such an algorithm provides
competitive performance for inversion, then use RTIL to devise a new GAN training algorithm. This can be
achieved by introducing a new latent variable z1∼pz1wherepz1is an appropriate probability distribution
(Figure 1 Step 3). Finally, during deployment, use this trained generative network for inverse problems via
the selected inversion algorithm (Figure 1 Step 4).
This workflow demonstrates a way to use recently introduced inversion algorithms in order to inspire new
training algorithms for GANs. It provides additional ways of training GANs knowing that they will be used
2Published in Transactions on Machine Learning Research (01/2023)
for inversion, and it provides a way to enable some empirically successful inversion algorithms are operating in
a more principled manner by ensuring they are searching over a space of parameters that have been suitably
regularized.
The contributions of this paper are the following.
•We introduce Regularized Training of Intermediate Layers, a principle for training deep generative
networks that are intended to be used for inverse problems when solved by an algorithm that optimizes
over intermediate layer representations.
•In the case of Intermediate Layer Optimization, we use our principle to devise a novel GAN training
algorithm. With the resulting trained GAN, we demonstrate lower reconstruction errors (compared
to GAN training without the principle) for compressed sensing, inpainting, and super resolution over
a wide range of under sampling ratio.
•We show the versatility of the method by repeating the same contribution in the case of the Multi-Code
GAN Prior.
•We illustrate the benefits of compressed sensing with RTIL by theoretically showing that a model
trained without regularizing intermediate layers has a strictly larger reconstruction error than a
model where the intermediate layers were regularized during training. For simplicity, this result is
established in the case of a two layer linear neural network trained in a supervised setting.
1 .1 Related Works
In recent years, various approaches have been proposed for dealing with the representation error when solving
inverse problems with a learned generative prior. As described above, one class of approaches focuses on
enlarging the latent space dimension, either at training or at inversion time (Athar et al., 2018; Dhar et al.,
2018; Hussein et al., 2020). On the other hand, another notable class of approaches recently put forward is
based on flow-based invertible neural networks. These generative networks have invertible architectures and
a latent space with the same size of the image space, and thus have zero representation error (Ardizzone
et al., 2019; Asim et al., 2020; Ma & Le Dimet, 2009; Kelkar et al., 2021; Shamshad et al., 2019; Whang
et al., 2021a;b; Helminger et al., 2021). Both these type of approaches attempt to limit the representation
error during training or inversion.
Similarly to flow-based models, score networks and subsequent variants (Song & Ermon, 2019; 2020; Song
et al., 2020) have support on the entire signal space, allow for conditional sampling and likelihood estimation,
and have been object of recent interests for their use in inverse problems (Ramzi et al., 2020; Jalal et al.,
2021a;b).
The increased representation power of the above-mentioned generative networks, usually comes at a price of
increased computational cost, both during the training and the inversion phase. In contrast, our proposed
principle leads to training algorithms that have essentially the same computational cost as the standard ones,
also leaving the cost of the inversion algorithms the same.
2 Generative Models and Optimization Algorithms for Inverse Problems
2 .1 Background
We consider the problem of training a GAN for the use of a prior for solving inverse problems. We consider
a GANGwhich maps a latent space Rn0to an image space Rnd, wheren0≪nd. In this paper, we focus
on the linear imaging inverse problems of compressed sensing, inpainting, and super resolution, though our
proposed method also applies to nonlinear inverse problems such as phase retrieval, and inversion problems
about non-image signals. We consider the general linear inverse problem of recovering an image x∈Rnd
from a set of linear measurements y∈Rm, given byy=A(x), whereA:Rnd→Rmis a forward linear
measurement operator. We study only the noiseless case, but our method easily extends to the case with
measurement noise. In this paper, we study measurement operators Aof the following form:
3Published in Transactions on Machine Learning Research (01/2023)
•Compressed Sensing: A=A∈Rm×nd,Ais a random matrix that samples from a known distribution
withm<nd.
•Inpainting:A=M∈Rm×nd,Mis a masking matrix with binary entries.
•Super-Resolution: A=Sm↓∈Rm×ndwhereSm↓is the downsampling operator with downsampling
factorm↓.
An estimate of xcan be recovered by finding the image in the range of Gthat is most consistent with the
measurements yin the following sense, as introduced in (Bora et al., 2017). First, solve
ˆz0= arg min
z0∥y−A(G(z0))∥, (1)
then, the estimate of x0is given by G(z0).
As mentioned in the introduction, a difficulty of this optimization approach is that the estimated images are
constrained to live within the range of G, which is a n0-dimensional manifold in Rnd. Most images x0will
not live exactly in this range, and thus the method is limited by the representation error minz0∥x0−G(z0)∥.
In the next sections, we review two recent algorithms for mitigating representation error during inversion.
For ease of exposition, we will discuss the case where only one intermediate layer representation is optimized.
We writeG=g1◦g0whereg0:Rn0→Rn1andg1:Rn1→Rnd.
2 .2 Intermediate Layer Optimization (ILO)
Given a trained generative model G=g1◦g0, Intermediate Layer Optimization (ILO) extends the range of
the generative model by sequentially optimizing over each layer of the network, as demonstrated in Algorithm
1. The initial step begins exactly as in (Bora et al., 2017) by optimizing over the input vector z0∈Rn0. The
solution is obtained in line 3, by initializing a z0∼N(0,In0), then optimizing the loss with gradient descent.
After the solution ˆz0is obtained, the algorithm searches for a perturbation ˆz1ofg0(ˆz0)that further minimizes
the reconstruction error (line 4). The final approximation of the target image xis given by g1(ˆz1+g0(ˆz0)).
As the authors point out, there are multiple ways where ILO can be regularized, including by an L1 penalty
in the intermediate representation, or via early stopping. For the present paper, we use early stopping, as
this was the method used by the publicly available code from the authors. Throughout this paper, we use the
code provided by the authors when solving ILO.
2 .3 Multi-Code GAN (mGanPrior)
Algorithm 1 Intermediate Layer Optimization (ILO)
for Compressed Sensing (Daras et al., 2021).
1:Input:G=g1◦g0, measurement matrix A∈
Rm×nd, compressed measurements y.
2:Output: estimated image ˆx
3:ˆz0=arg min
z0∥y−Ag1/parenleftbig
g0(z0)/parenrightbig
∥Initialize atz0∼
N(0,In0)
4:ˆz1=arg min
z1∥y−Ag1(z1+g0(ˆz0))∥Initialize at
z1∼N(0,In1)
5:Return: ˆx=g1(ˆz1+g0(ˆz0))Multi-Code GAN Prior is an inversion method that
simultaneously optimizes over multiple latent codes
and composes their corresponding intermediate fea-
ture maps with adaptive channel importance. This
effectively extends the expressivity of the network by
giving it a higher dimensional input space and addi-
tional parameters to control the importance of each
channels in the intermediate layer representation.
Assume we are given a pre-trained generative model
G=g1◦g0, where the output of the first layer g0has
dimensionH1×W1×C1withC1being the number
of channels. Furthermore, chose Nlatent codes
{zk
0}N
i=1∈Rn0and channel importance {αk}N
i=1∈
RC1. Then the Multi-Code GAN Prior extended
architecture computes g1(/summationtextN
k=1g0(zk
0)⊙αk)where
{g0(zk
0)⊙αk}ijc={g0(zk
0)}ijc·{αk}cis channel-wise multiplication, i,jare spatial location, and cis the
channel index.
During the inversion phase, the Multi-Code GAN Prior method optimizes over both the latent vectors {zk
0}N
i=1
and the channel importance {αk}N
i=1(Algorithm 2).
4Published in Transactions on Machine Learning Research (01/2023)
(a) Comparison between ILO-RTIL (ours) and ILO
for compressed sensing for 3% of measurements.
(b) Comparison between mGANprior-RTIL (ours) and mGANprior
for compressed sensing for 5% of measurements.
Figure 2: Results on the left compare ILO-RTIL to ILO and then on the right mGANprior-RTIL to
mGANprior.
3 RTIL
Algorithm 2 Multi-Code GAN (mGANprior) Gu et al.
(2020).
1:Input: Trained network G=g1◦g0, latent codes
{zk
0}N
k=1∈Rn0,{αk}N
k=1∈Rn1, measurement matrix
A∈Rm×nd, compressed measurements y.
2:Output: estimated image ˆx
3:Initialize{zk
0}N
k=1∼pz,{αk}N
k=1∼pα
4:ˆz,ˆα= arg min
{zk
0}N
k=1,{αk}n
k=1∥y−Ag1(/summationtextn
k=1g0(zk
0)⊙αk)∥
5:Return: ˆx=g1(/summationtextN
k=1g0(ˆzk
0)⊙ˆαk)In this section, we present how the principle
of Regularized Training of Intermediate Layers
(RTIL) inspires training algorithms for GANs
that are intended for inversion by Intermediate
Layer Optimization (Section 3 .1) and mGAN-
prior (Section 3 .2).
We consider the case of a practitioner having
chosen, after some initial exploration, a base
generative network Gfor use as prior in solving
inverse problems. For simplicity, in this section
we will consider G=g1◦g0whereg0:Rn0→
Rn1andg1:Rn1→Rnd. The input latent
vectorsz0of the network Gare sampled from
pz0(e.g.pz0=N(0,In0)) andθis the set of
trained parameters.
We assume, furthermore, that the practitioner has selected an inversion algorithm that optimizes over the
latent variable z0and the intermediate layer between g0andg1ofG. Optimizing over the intermediate layer
corresponds to introducing a free variable z1betweeng0andg1(Figure 3).
The RTIL principle states that if one intends to solve inverse problems by means of intermediate layer
optimization algorithms, then intermediate layers optimized over during inversion should be regularized during
training.
This principle can be used to design a new training algorithm in the following manner. We identify the
additional free variable, z1, used for optimization over the intermediate layer, consider it as a latent variable
of the generative model, and provide it with a simple distribution pz1. For example, this could result in a
generative model /tildewideG:Rn0×Rn1→Rnd, such that/tildewideG(z0,z1) =g1(z1+g0(z0)), but could have alternative
functional forms. This generative model reduces to the base generative model G:Rn0→Rndifz1is suitably
chosen, for example if z1= 0.
5Published in Transactions on Machine Learning Research (01/2023)
The introduction of the latent variable z1explicitly increases the dimensionality of the latent space. In
practice, training latent variable models with high-dimensional latent spaces can be challenging and require
careful regularization (Athar et al., 2018). We address this difficulty by concurrently training the lower and
higher dimensional models Gθand/tildewideGθ, which share trainable weights θ(Figure 3).
We trainGθand/tildewideGθvia the following minimax formulation (Goodfellow et al., 2014)
min
θmax
ΘEx∼px/bracketleftbig
logDΘ(x)/bracketrightbig
+Ez0∼pz0z1∼pz1/bracketleftbig
λlog(1−DΘ(Gθ(z0)) + (1−λ) log(1−DΘ(/tildewideGθ(z0,z1))/bracketrightbig
,(2)
whereDΘ:Rnd→Ris the discriminative network and λ∈[0,1]is a hyperparameter. This method could be
extended to alternative GAN and non-GAN formulations (Arjovsky et al., 2017; Gulrajani et al., 2017b;
Bojanowski et al., 2018).
Once/tildewideGis trained, a practitioner could solve an inverse problem with forward operator Aby solving
ˆz0,ˆz1= arg min
z0,z1∥y0−A(/tildewideG(z0,z1))∥,
using the selected optimization algorithm, resulting in /tildewideG(ˆz0,ˆz1)as the estimate of the signal.
In this section, we considered the case where only one intermediate layer was optimized. The proposed
method can directly extend to the case where multiple layers are optimized.
Latent variables
Optimized variables
Parameter sharingInversion RTIL
Figure 3: Visual representation of the RTIL principle. An in-
version method that optimizes over intermediate layers informs
the training of a family of generative networks adapted to ILO
inversion method.We will next describe the details of the
application of RTIL in the case of Interme-
diate Layer Optimization and the Multi-
Code GAN Prior.
3 .1 RTIL for ILO
In the case of Intermediate Layer Opti-
mization (ILO), we now present how to
use RTIL to design a GAN training al-
gorithm. Consider the base generative
networkG(z0) =g1(g0(z0)), to be used
for inversion with Intermediate Layer Op-
timization. ILO (Algorithm 1) extends
the range of the network by optimizing
g1(z1+g0(z0))over latent variables z0
andz1. Consequently, we consider the
higher dimensional generative model
/tildewideG(z0,z1) =g1(z1+g0(z0)),z0∼N(0,In0),z1∼N(0,σ2In1),
whereσ2isahyperparameter. Thensimultaneouslythelowerdimensionalmodel G(z0)andhigherdimensional
model/tildewideG(z0,z1)are trained with the min max formulation above. Note z0∼pz0andz1∼pz1where Figure
3 depicts this process.
3 .2 RTIL for mGANprior
In the case of Multi-Code GAN Prior method, we now present how to use RTIL to design a GAN training
algorithm. Consider the base generative network Gto be used for inversion with the Multi-Code GAN
Prior method. The mGANprior Algorithm (Algorithm 2) extends the range of the network by optimizing
g1(/summationtextN
k=1g0(zk
0)⊙αk)over latent variables z0andz1. Consequently, training the higher dimensional model
6Published in Transactions on Machine Learning Research (01/2023)
yields
/tildewideG(z1
0,...,zN
0,α1,...,αN) =g1(N/summationdisplay
k=1g0(zk
0)⊙αk),zk
0∼N(0,In0), pα′∼DirN(1).
where each vector αkis taken to be αk={α′}k·1where 1is the vector of all ones and {α′}kis thek-th entry
ofα′∈RN. The vector α′is sampled from pα′∼DirN(1), whereDirN(1)is the flat Dirichlet distribution,
i.e. the uniform distribution over the (N−1)-dimensional simplex. Note this leads to each channel being
weighted equally during training. The training process is depicted in the appendix, Figure 13.
4 Experiments
Figure 4: Performance of ILO-RTIL and vanilla trained ILO for Compressed sensing, inpainting, and super
resolution for various under-sampling ratios. ILO-RTIL increases performances or ties for each under-sampling
ratio with respect to PSNR across each of the inverse problems compared to ILO. The vertical bars indicate
95% confidence intervals.
We observe that Regularized Training of Intermediate Layers is not tied to any specific architecture or
training procedure, and while, until this point, we have only presented it on two-layer networks for easiness
of exposition, in this section we demonstrate its successful application on two state-of-the-art architectures
on different imaging recovery problems. Specifically, we conduct extensive experiments comparing RTIL
versus vanilla training for compressed sensing, inpainting, and super-resolution, and for two different inverse
methods Multi-Code GAN Prior (Section 2 .3) and Intermediate Layer Optimization (ILO) (Section 2 .2) to
demonstrate the effectiveness of our method. The generative model architecture used for the mGANprior is
PGGAN (Karras et al., 2018) and for ILO is styleGAN-2 (Karras et al., 2020). All models were trained on
FFHQ data set (Karras et al., 2019) and tested on CelebA-HQ dataset (Karras et al., 2018), at an image size
of 256x256x3. The choice of architectures were based on the experimental section of the original ILO (Daras
et al., 2021) and mGANprior (Gu et al., 2020) papers. Refer to the appendix for the architecture details for
training networks using RTIL, as well as hyperparameters chosen for inversion methods.
For all experiments, compressed sensing use partial circulant measurement matrices with random signs Daras
et al. (2021), inpainting the pixels are missing at random, and downsample factor corresponds to how much
the height and width of the original image was reduced to. All experiments were trained with λ=1
2equation
2. The code is hosted on a GitHub repository https://github.com/g33sean/RTIL .
4 .1 Results ILO-RTIL
Results in this section correspond to Figure 4, where the results are averaged over 100 images randomly
sampled from CelebA-HQ. Note, the hyperparameter was set σ2= 1during training for all experiments.
7Published in Transactions on Machine Learning Research (01/2023)
Compressed Sensing - ILO-RTIL demonstrates an increase in reconstruction performance across each under-
sampling regime. The largest increase in performance with respect to PSNR occur at 25% (1.28 dB), 15%
(1.59 dB), and 10% (1.3 dB’s) measurements. For qualitative results, please refer to Figure 2(a). Inpainting -
ILO-RTIL demonstrates in an increase in reconstruction performance with 4 out of the 5 sampling regimes.
There is an increase in PSNR at 50% (1.88 dB), 25% (1.59 dB’s), 15% (.94 dB’s), and 5% (.29 db) of observed
pixels. At 1% of observed pixels, there is a decrease in performance in reconstruction of .5 dB’s, but the error
bars overlap each other, indicating there is no significant improvement. For qualitative results, please refer to
Figure 16. Super-Resolution - ILO-RTIL shows a slight increase in performance over the entire measurement
regime compared to ILO, the most significant occurs at1
4downsampling factor where on average the increase
in reconstruction is .77 dB’s and the 95% error bars are separated. All other sampling regime achieves
comparable performance between ILO-RTIL and ILO. For qualitative results, please refer to the appendix
Figure 18.
Figure 5: Performance of mGANprior-RTIL and vanilla trained mGANprior for compressed sensing, inpainting,
and super resolution for various under-sampling ratios. mGANprior-RTIL increases performances over vanilla
mGANprior with respect to PSNR over each under-sampling ratio, except for super-resolution problems at
low under-sampling ratio’s. The vertical bars indicate 95% confidence intervals.
4 .2 Results RTIL-Multi-Code
Results in this section correspond to Figure 15, where the results are average over 100 images randomly
sampled from CelebA-HQ and all experiments use N= 20latent codes. Compressed Sensing - mGANprior-
RTIL has a significant improvement in reconstruction performance over mGANprior. For each under-sampling
ratio, our method increases performance by at least 1.6 dB’s, which clearly separates the error bars. On
the other hand, at 1%of measurements where both methods achieve the similar performance, which is
seen by overlapping error bars. For qualitative results, refer to Figure 2(b). Inpainting - mGANprior-RTIL
demonstrates a significant improvement in reconstructing over mGANprior clearly being able the error bars.
Our method yields an increase in performance with respect to PSNR of 2.5 dB’s at 50%, 2.23 dB’s 30%, 1.99
dB’s at 20%, and 1.58 dB’s at 10% observed pixels. For 1% of observed pixels, these both methods yield
similar results. Lastly, qualitative results can be seen on Figure 17. Super-Resolution - mGANprior-RTIL show
substantial improvement with image reconstruction at downsampling factor of1
2and1
4, increase of 2.25 dB’s
and 1.51 dB’s respectively. However, at1
16down-sampling factor mGANprior outperforms mGANprior-RTIL
slightly, but within the error bars. For qualitative results, please refer to the appendix Figure 19.
4 .3 Ablation Study
This section provides two ablations studies, comparing vanilla training versus RTIL for each intermediate
layer trained using ILO-RTIL and mGANprior-RTIL. Each experiment was tested on 5 images sampled
randomly from CelebA-HQ.
8Published in Transactions on Machine Learning Research (01/2023)
4 .3.1 ILO-RTIL
Figure 6: Comparing compressed sensing and inpainting reconstructing
performance between ILO-RTIL and ILO for a various number of
intermediate layers.All results refer to Figure 6, recall
Figure 3 and Algorithm 1 for no-
tation,z0refers to optimizing over
the initial latent vector (Bora et al.,
2017),z1denotes sequentially opti-
mizing over the first two layers, and
so until representation error z4. Re-
fer to the appendix for number of
iterations per intermediate layer.
Compressed Sensing - Overall, opti-
mizing up to z4achieves the best
performance for ILO-RTIL and ILO,
in most under-sampling ratio’s it
outperforms and at worst ties com-
pared to other intermediate opti-
mization layers. Comparing ILO-
RTIL to ILO, there is an increase in
reconstruction performance on aver-
age across each under-sampling for
z4of 1.41 dB’s, z3.81 dB’s,z2.4
dB’s, andz1.86 dB’s. Inpaiting -
Refer to appendix for inpainting results in Figure 6.
Inpainting - Overall, ILO-RTIL sequentially optimizing up to z4performs the best overall, outperforming other
intermediate representations z3,z2,z1at 50% observed pixels then achieving the slightly better performance
at the under-sampling ratio’s. Comparing ILO-RTIL to ILO, there is increase in performance on average
between each experiment for z41.27 dB’s,z3.55 dB’s,z2.4 dB’s, andz11.27 dB’s. Moreover, ILO-RTIL
fromz4toz3on average there is increase of performance of .5 dB’s, where for vanilla trained there is a
decrease of .28 dB’s.
4 .3.2 mGANprior-RTIL
Figure 7: Comparing compressed sensing and inpainting reconstructing
performance between mGANprior-RTIL and mGANprior for various
number of latent codesCompressed Sensing - mGANpior-
RTIL outperformed vanilla mGAN-
pior in each optimization setting
usingN={1,10,20}, marginal
atN= 1, but more notice-
able atN={10,20}. Overall,
N= 20achieves the best perfor-
mance for both mGANprior and
mGANprior-RTIL training across
each under-sampling regime. As
well as mGANprior-RTIL increases
performance on average across all
the under-sampling experiments
withN= 10compared to N= 20
with mGANprior. Inpaiting -Refer
to appendix for inpainting results in
Figure 7.Overall, N= 20achieves
the best performance for both
vanilla and RTIL training across
each under-sampling regime. For
mGANprior has a substantial
9Published in Transactions on Machine Learning Research (01/2023)
increase in performance (1.83 dB’s) between 10 latent codes and 20 on average across each experiment. RTIL
training increases performance not as much, but a notable amount with an increase of .93 dBs. However,
mGANprior-RTIL with N= 10outperforms mGANprior on average across all the under-sampling ratios of
about.66dBs.
5 Theoretical Model for Compressed Sensing with RTIL
In this section, we examine a simple theoretical model for compressive sensing with RTIL. Our objective is to
shed some light on the reasons why RTIL may improve the performance of inversion algorithms that optimize
over intermediate layers.
We begin with an informal discussion. Recall that for a base generative architecture G=g1◦g0given by
the composition of two computational modules g0:Rn0→Rn1andg1:Rn1→Rnd, ILO uses the extended
generative network /tildewideG:Rn0×Rn1→Rnd, such that/tildewideG(z0,z1) =g1(z1+g0(z0)).
We would like to compare the performance of an extended model /tildewideGVantrained in a standard/vanilla way
and an extended model /tildewideGRTILtrained using RTIL the principle. In the vanilla case, during training, the
intermediate layer variable z1is ignored and the network may be trained via the standard minimax formulation
min
θmax
ΘEx∼px/bracketleftbig
logDΘ(x)/bracketrightbig
+Ez0∼pz0/bracketleftbig
log(1−DΘ(/tildewideGVan
θ(z0,0))/bracketrightbig
. (3)
Using the RTIL principle instead the network /tildewideGRTILmay be trained with (taking λ= 0in 2 for simplicity)
min
θmax
ΘEx∼px/bracketleftbig
logDΘ(x)/bracketrightbig
+Ez0∼pz0z1∼pz1/bracketleftbig
log(1−DΘ(/tildewideGRTIL
θ(z0,z1))/bracketrightbig
. (4)
For compressed measurements ywith the sensing matrix A, then ILO would perform the following steps (see
Algorithm 1)
ˆz0= arg min
z′
0∈Rn0∥y−A/tildewideG(z′
0,0)∥2
2,initializez′
0∼N(0,In0),
ˆz1= arg min
z′
1∈Rn1∥y−A/tildewideG(ˆz0,z′
1)∥2
2,initializez′
1∼N(0,In1).(5)
where/tildewideG=/tildewideGVan
θor/tildewideG=/tildewideGRTIL
θ.
Notice now, that when training ˜GVanusing 3 we compute ˜GVan(z0,0) =g1(g0(z0)). During training,
the computational block g1therefore only receives input vectors in the range of g0, and is not specified
in the directions off the range of g0. During inference, in the second step of 5, though, we compute
˜GVan(ˆz0,z′
1) =g1(g0(ˆz0) +z′
1). Hence, ILO optimizes the variable z′
1in directions for which the behavior of
g1may not be well specified, which could lead to large errors.
Contrary to vanilla training, when training /tildewideGRTIL
θwith RTIL we compute ˜GRTIL(z0,z1) =g1(g0(z0) +z1).
Hence, we expect the computational block g1to learn meaningful outputs for inputs g0(z0) +z1off the range
ofg0and ILO to perform better.
In the next sections, we formalize the above argument. To provide the most easily understandable context,
we consider the case of a generative model given by a two-layer linear neural network. Further, we consider
the case where the generative model is trained in a supervised manner, and we consider the regime of infinite
training data. The supervised setting allows us to avoid the technicalities of unsupervised adversarial training,
and working in the infinite data regime allows us to avoid statistical estimation errors.
5 .1 Problem Setup
We assume that the true signal distribution is given by x=G⋆(z0,z1) =W⋆
1(W⋆
0z0+z1), whereG⋆:
Rn0×Rn1→Rnd,z0∼N(0,In0)andz1∼N(0,In1)drawn independently, and where W⋆
1∈Rnd×n1and
W⋆
0∈Rn1×n0. We moreover assume that n0<n1<nd, andW⋆
0,W⋆
1are full rank. We furthermore assume
thatW⋆
0is known, but that W⋆
1is unknown.
10Published in Transactions on Machine Learning Research (01/2023)
Letx⋆=G⋆(z⋆
0,z⋆
1)be an unknown signal, where z⋆
0∼N (0,In0)andz⋆
1∼N (0,In1). We consider the
problem of recovering x⋆given compressed linear measurements y=Ax⋆∈Rmwheren1≤m < nd.
We assume that a practitioner has chosen a base generative model G:Rk→Rndgiven byG:z0∝⇕⊣√∫⊔≀→G(z0) =
W1(W⋆
0z0). Since this model has a nonzero representation error the practitioner will then consider the
extended generative network /tildewideG(z0,z1) :Rn0×Rn1→Rndgiven by/tildewideG:z0,z1∝⇕⊣√∫⊔≀→/tildewideG(z0,z1) =W1(W⋆
0z0+z1),
and an inversion method that estimates x⋆asx⋆≈/tildewideG(ˆz0,ˆz1)where (ˆz0,ˆz1)are found using ILO 5
We are then interested in analyzing the effects of different training methods for /tildewideGon the estimation of x⋆via
the inversion method 5.
5 .2 Training the Models
We will now compare the training of /tildewideGwith vanilla training and with RTIL. The model trained with vanilla
training will be denoted by /tildewideGVanand its weight by WVan
1. The model trained with RTIL will be denoted by
/tildewideGRTILand its weight by WRTIL
1.
Training each model consists of estimating WVan
1andWRTIL
1under a least squares loss. In the idealized
regime of infinite training data, vanilla training the generative network /tildewideGVancorresponds to ignoring the
intermediate variables z1, and in particular estimating W⋆
1by
WVan
1∈arg min
W1∈Rnd×n1Ez0,z1∥G⋆(z0,z1)−/tildewideGVan(z0,0)∥2
2. (6)
Since the objective is to solve compressed sensing with the intermediate layer optimization 5, training /tildewideGRTIL
with RTIL instead corresponds to
WRTIL
1∈arg min
W1∈Rnd×n1Ez0,z1∥G⋆(z0,z1)−/tildewideGRTIL(z0,z1)∥2
2. (7)
The next lemma characterizes the solutions of the optimization problems 6 and 7.
Lemma 5 .1. LetWVan
1as in 6 thenWVan
1W⋆
0=W⋆
1W⋆
0. Moreover, there exists a unique WRTIL
1solution
of 7, given by WRTIL
1 =W⋆
1.
While the above lemma shows that WVan
1will equalW⋆
1on the range of W⋆
0, its behavior on the space
orthogonal to the range of W⋆
0depends on how 6 is solved. To simplify the discussion below, we will consider
WVan
1to be the minimum (Frobenius) norm solution of 6.
5 .3 Compressed Sensing
We denote by (zVan
0,zVan
1)the solutions of the minimization problems 5 using /tildewideGVanin place of/tildewideG. The
estimate of x⋆using the vanilla trained model is then x⋆≈/tildewideGVan(zVan
0,zVan
1). Similarly, (zRTIL
0,zRTIL
1)
are the solutions of 5 using /tildewideGRTILin place of/tildewideG, so that the estimate of x⋆using the RTIL model is
x⋆≈/tildewideGVan(zRTIL
0,zRTIL
1).
The following lemma quantifies the reconstruction errors made by using the two generative models.
Lemma 5 .2. Assume that W⋆
0,W⋆
1are full rank. Let WVan
1be the minimum norm solution of 6 and
WRTIL
1be the solution of 7. Let A∈Rm×ndwith i.i.d.N(0,1)entries. Then with probability 1
Ez⋆
0,z⋆
1/bracketleftbig
∥G⋆(z⋆
0,z⋆
1)−/tildewideGVan(zVan
0,zVan
1)∥2/bracketrightbig
≥ max
h∈range (W⋆
0)⊥∥(Ind− PW⋆
1W⋆
0/parenrightbig
W⋆
1h∥2
2> 0(8)
and
Ez⋆
0,z⋆
1[∥G⋆(z⋆
0,z⋆
1)−/tildewideGRTIL(zRTIL
0,zRTIL
1)∥2] = 0. (9)
The above results illustrate why training generative models using RTIL can enable better compressed sensing
performance. In the simplified setting of a two-layer linear neural network trained in a supervised manner
11Published in Transactions on Machine Learning Research (01/2023)
with a known first layer and in the infinite data regime, we see that the generative model trained with vanilla
training incurs error in the second layer’s weights in the orthogonal complement of the range of the first layer.
This results in an increase in the reconstruction error when solving compressed sensing.
6 Conclusion
We have introduced a principle for training GANs that are intended to be used for solving inverse problems.
That principle states that if the inversion algorithm optimizes over intermediate layers of the network, then
during training the network should be regularized in those layers. We instantiate this principle for two
recent and successful optimization algorithms, Intermediate Layer Optimization (Daras et al., 2021) and the
Multi-Code Prior (Gu et al., 2020). For both of these algorithms, we devise a new GAN training algorithm.
Empirically, we show our trained GANs allow better reconstruction in compressed sensing, inpainting, and
super-resolution across multiple under-sampling regimes when compared to GANs trained in a vanilla manner.
We note that our methodology only applies in the case of inversion methods that optimize over intermediate
layers. However, there have been multiple competitive methods of this form published recently, each of
which we show can benefit from this approach. Tools like those proposed in this paper, provided sufficient
computational resources, may allow these methods to be even more competitive in real-world applications.
verification at larger image sizes and on generative models trained specifically for certain applications, such
as MRI can benefit from the increase in performance.
7 Acknowledgements and Disclosure of Funding
PH acknowledges support from NSF Awards DMS-2053448, DMS-1848087, and DMS-2022205.
12Published in Transactions on Machine Learning Research (01/2023)
References
Lynton Ardizzone, Jakob Kruse, Carsten Rother, and Ullrich Köthe. Analyzing inverse problems with
invertible neural networks. In International Conference on Learning Representations , 2019. URL https:
//openreview.net/forum?id=rJed6j0cKX .
Martin Arjovsky, Soumith Chintala, and Léon Bottou. Wasserstein generative adversarial networks. In
International conference on machine learning , pp. 214–223. PMLR, 2017.
Muhammad Asim, Max Daniels, Oscar Leong, Ali Ahmed, and Paul Hand. Invertible generative models
for inverse problems: mitigating representation error and dataset bias. In International Conference on
Machine Learning , pp. 399–409. PMLR, 2020.
ShahRukh Athar, Evgeny Burnaev, and Victor Lempitsky. Latent convolutional models. arXiv preprint
arXiv:1806.06284 , 2018.
Piotr Bojanowski, Armand Joulin, David Lopez-Pas, and Arthur Szlam. Optimizing the latent space of
generative networks. In International Conference on Machine Learning , pp. 600–609. PMLR, 2018.
Ashish Bora, Ajil Jalal, Eric Price, and Alexandros G Dimakis. Compressed sensing using generative models.
InInternational Conference on Machine Learning , pp. 537–546. PMLR, 2017.
Giannis Daras, Joseph Dean, Ajil Jalal, and Alex Dimakis. Intermediate layer optimization for inverse
problems using deep generative models. In Marina Meila and Tong Zhang (eds.), Proceedings of the 38th
International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event , volume 139
ofProceedings of Machine Learning Research , pp. 2421–2432. PMLR, 2021. URL http://proceedings.
mlr.press/v139/daras21a.html .
Manik Dhar, Aditya Grover, and Stefano Ermon. Modeling sparse deviations for compressed sensing using
generative models. In International Conference on Machine Learning , pp. 1214–1223. PMLR, 2018.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron
Courville, and Yoshua Bengio. Generative adversarial nets. In Z. Ghahramani, M. Welling, C. Cortes,
N. Lawrence, and K. Q. Weinberger (eds.), Advances in Neural Information Processing Systems , vol-
ume 27. Curran Associates, Inc., 2014. URL https://proceedings.neurips.cc/paper/2014/file/
5ca3e9b122f61f8f06494c97b1afccf3-Paper.pdf .
Jinjin Gu, Yujun Shen, and Bolei Zhou. Image processing using multi-code gan prior. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , June 2020.
Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron C Courville. Im-
proved training of wasserstein gans. In I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fer-
gus, S. Vishwanathan, and R. Garnett (eds.), Advances in Neural Information Processing Systems , vol-
ume 30. Curran Associates, Inc., 2017a. URL https://proceedings.neurips.cc/paper/2017/file/
892c3b1c6dccd52936e27cbd0ff683d6-Paper.pdf .
Ishaan Gulrajani, Faruk Ahmed, Martín Arjovsky, Vincent Dumoulin, and Aaron C Courville. Improved
training of wasserstein gans. In NIPS, 2017b.
Paul Hand, Oscar Leong, and Vlad Voroninski. Phase retrieval under a generative prior. In S. Bengio,
H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett (eds.), Advances in Neural
Information Processing Systems , volume 31. Curran Associates, Inc., 2018. URL https://proceedings.
neurips.cc/paper/2018/file/1bc2029a8851ad344a8d503930dfd7f7-Paper.pdf .
Leonhard Helminger, Michael Bernasconi, Abdelaziz Djelouah, Markus Gross, and Christopher Schroers.
Generic image restoration with flow based priors. In Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition , pp. 334–343, 2021.
Shady Abu Hussein, Tom Tirer, and Raja Giryes. Image-adaptive gan based reconstruction. In Proceedings
of the AAAI Conference on Artificial Intelligence , volume 34, pp. 3121–3129, 2020.
13Published in Transactions on Machine Learning Research (01/2023)
Ajil Jalal, Marius Arvinte, Giannis Daras, Eric Price, Alexandros G Dimakis, and Jonathan I Tamir. Robust
compressed sensing mri with deep generative priors. arXiv preprint arXiv:2108.01368 , 2021a.
Ajil Jalal, Sushrut Karmalkar, Alexandros G Dimakis, and Eric Price. Instance-optimal compressed sensing
via posterior sampling. arXiv preprint arXiv:2106.11438 , 2021b.
Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. Progressive growing of gans for improved
quality, stability, and variation. In International Conference on Learning Representations , 2018.
Tero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative adversarial
networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pp.
4401–4410, 2019.
Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Analyzing and
improving the image quality of stylegan. In Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition , pp. 8110–8119, 2020.
Varun A Kelkar, Sayantan Bhadra, and Mark A Anastasio. Compressible latent-space invertible networks
for generative model-constrained image reconstruction. IEEE Transactions on Computational Imaging , 7:
209–223, 2021.
Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Yoshua Bengio and
Yann LeCun (eds.), 3rd International Conference on Learning Representations, ICLR 2015, San Diego,
CA, USA, May 7-9, 2015, Conference Track Proceedings , 2015. URL http://arxiv.org/abs/1412.6980 .
Jianwei Ma and Francois-Xavier Le Dimet. Deblurring from highly incomplete measurements for remote
sensing. IEEE transactions on geoscience and remote sensing , 47(3):792–802, 2009.
Morteza Mardani, Enhao Gong, Joseph Y Cheng, Shreyas S Vasanawala, Greg Zaharchuk, Lei Xing, and
John M Pauly. Deep generative adversarial neural networks for compressive sensing mri. IEEE transactions
on medical imaging , 38(1):167–179, 2018.
Sachit Menon, Alexandru Damian, Shijia Hu, Nikhil Ravi, and Cynthia Rudin. Pulse: Self-supervised photo
upsampling via latent space exploration of generative models. In Proceedings of the ieee/cvf conference on
computer vision and pattern recognition , pp. 2437–2445, 2020.
Lars Mescheder, Andreas Geiger, and Sebastian Nowozin. Which training methods for GANs do actually
converge? In Jennifer Dy and Andreas Krause (eds.), Proceedings of the 35th International Conference on
Machine Learning , volume 80 of Proceedings of Machine Learning Research , pp. 3481–3490. PMLR, 10–15
Jul 2018. URL https://proceedings.mlr.press/v80/mescheder18a.html .
Lukas Mosser, Olivier Dubrule, and Martin J Blunt. Stochastic seismic waveform inversion using generative
adversarial networks as a geological prior. Mathematical Geosciences , 52(1):53–79, 2020.
Xingang Pan, Bo Dai, Ziwei Liu, Chen Change Loy, and Ping Luo. Do 2d {gan}s know 3d shape? unsupervised
3d shape reconstruction from 2d image {gan}s. In International Conference on Learning Representations ,
2021. URL https://openreview.net/forum?id=FGqiDsBUKL0 .
Zaccharie Ramzi, Benjamin Rémy, Francois Lanusse, Jean-Luc Starck, and Philippe Ciuciu. Denoising
score-matching for uncertainty quantification in inverse problems. arXiv preprint arXiv:2011.08698 , 2020.
Fahad Shamshad, Asif Hanif, and Ali Ahmed. Subsampled fourier ptychography via pretrained invertible
and untrained network priors. 2019.
Niklas Smedemark-Margulies, Jung Yeon Park, Max Daniels, Rose Yu, Jan-Willem van de Meent, and Paul
Hand. Generator surgery for compressed sensing. arXiv preprint arXiv:2102.11163 , 2021.
Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution. arXiv
preprint arXiv:1907.05600 , 2019.
14Published in Transactions on Machine Learning Research (01/2023)
Yang Song and Stefano Ermon. Improved techniques for training score-based generative models. arXiv
preprint arXiv:2006.09011 , 2020.
Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole.
Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456 ,
2020.
Jay Whang, Qi Lei, and Alex Dimakis. Solving inverse problems with a flow-based noise model. In
International Conference on Machine Learning , pp. 11146–11157. PMLR, 2021a.
Jay Whang, Erik Lindgren, and Alex Dimakis. Composing normalizing flows for inverse problems. In
International Conference on Machine Learning , pp. 11158–11169. PMLR, 2021b.
Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness
of deep features as a perceptual metric. In CVPR, 2018.
A Proof
Proof of Lemma 5 .1. Notice that for any W1∈Rnd×n1
Ez0,z1[∥W⋆
1(W⋆
0z0+z1)−W1W⋆
0z0∥2
2] =Ez0[∥(W⋆
1−W1)W⋆
0z0∥2
2] +Ez1[∥W⋆
1z1∥2
2]
=∥(W⋆
1−W1)W⋆
0∥2
F+∥W⋆
1∥2
F,
where the first equality used independence of z0andz1and the second used the fact that Ez∼N(0,In)∥Mz∥2
2=
∥M∥2
Ffor any matrix M∈Rm×n. The solutions of equation 6 are then solutions of
min
W1∈Rn1×n0∥L(W⋆
1)−L(W1)∥2
F (10)
whereL:Rnd×n1→Rn1×n0is the linear operator given by L:W1∝⇕⊣√∫⊔≀→W1W⋆
0. Sincen1>n0, this operator
is singular and there are infinite solutions of equation 10, all of which satisfy L(W⋆
1) =L(W1), i.e. the thesis.
Regarding 7, observe instead that for any W1∈Rnd×n1
Ez0,z1[∥W⋆
1(W⋆
0z0+z1)−W1(W⋆
0z0+z1)∥2
2] =∥(W⋆
1−W1)W⋆
0∥2
F+∥W⋆
1−W1∥2
F.
This showsW1=W⋆
1is the unique minimizer of 7.
Before analyzing the solution of compressed sensing with the trained generative models, we observe the
following fact on the minimum norm solution of 6.
Lemma A.1. LetWVan
1be the minimum Frobenius norm solution of equation 6. Then WVan
1 =
W⋆
1W⋆
0(W⋆
0)†where (W⋆
0)†is the pseudoinverse of W⋆
0.
Proof.Notice thatL(W⋆
1) =L(WVan
1), whereLis defined in the proof of Lemma equation 5 .1. We next
show thatWVan
1is orthogonal to the null space of L, which implies the thesis.
LetW1∈Rnd×n1be such thatL(W1) =W1W⋆
0= 0. Then we have
⟨WVan
1,W1⟩F=tr((WVan
1)TW1)
=tr(W1(W⋆
0(W⋆
0)†)T(W⋆
1)T)
=tr(W1W⋆
0(W⋆
0)†(W⋆
1)T)
= 0,
where third equality uses the fact that W⋆
0(W⋆
0)†is symmetric and the fourth equality uses the assumption
onW1.
15Published in Transactions on Machine Learning Research (01/2023)
Proof of Lemma 5 .2.
- Proof of equation 8. Notice that by the previous lemma WVan
1=W⋆
1W⋆
0(W⋆
0)†=W⋆
1PW⋆
0wherePW⋆
0is
the orthogonal projector onto the range of W⋆
0. Moreover, with probability 1, (AW⋆
1W⋆
0)is full rank. It
follows that (zVan
0,zVan
1)is given by
zVan
0= arg min
z′
0∈Rn0∥y−AW⋆
1W⋆
0z′
0∥2
2,
zVan
1= arg min
z′
1∈Rn1∥y−AW⋆
1(W⋆
0zVan
0+PW⋆
0z′
1)∥2
2.
In particularPW⋆
0zVan
1= 0and(In1−PW⋆
0)zVan
1is fixed at initialization. We have then /tildewideGVan(zVan
0,zVan
1) =
W⋆
1(W⋆
0z⋆
0+W⋆
0M1z⋆
1)whereM1= (AW⋆
1W⋆
0)†AW⋆
1.
The reconstruction error is then given by
Ez⋆
0,z⋆
1∥G⋆(z⋆
0,z⋆
1)−/tildewideGVan(zVan
0,zVan
1)∥2
2=∥W⋆
1−W⋆
1W⋆
0M1∥2
F
≥ min
M∈Rn0×n1∥W⋆
1−W⋆
1W⋆
0M∥2
F
=∥/parenleftbig
Ind−PW⋆
1W⋆
0/parenrightbig
W⋆
1∥2
F
wherethefirstequalityfollowsfromthe propertiesofthenormaldistribution. Regardingthen the minimization
problem minM∈Rn0×n1∥W⋆
1−W⋆
1W⋆
0M1∥2
F, notice that this is convex and the critical points satisfy
(W⋆
1W⋆
0)TW⋆
1= (W⋆
1W⋆
0)T(W⋆
1W⋆
0)M
Using the fact that W⋆
1W⋆
0is full rank, the unique solution is found to be
[(W⋆
1W⋆
0)T(W⋆
1W⋆
0)]−1(W⋆
1W⋆
0)TW⋆
1, which gives the last equality.
Note now thatPW⋆
1W⋆
0is the projector onto the range of W⋆
1W⋆
0andW⋆
1is full rank. Thus
∥/parenleftbig
Ind−PW⋆
1W⋆
0/parenrightbig
W⋆
1∥2
F≥∥/parenleftbig
Ind−PW⋆
1W⋆
0/parenrightbig
W⋆
1∥2
2= max
h∈range (W⋆
0)⊥∥/parenleftbig
Ind−PW⋆
1W⋆
0/parenrightbig
W⋆
1h∥2
2>0.
-Proof of equation 9 Notice again that with probability 1, AW⋆
1W⋆
0andAW⋆
1have full rank. Moreover
/tildewideGRTIL(zRTIL
0,zRTIL
1) =W⋆
1(W⋆
0zRTIL
0 +zRTIL
1)where
zRTIL
0 = arg min
z′
0∈Rn0∥y−AWRTIL
1W⋆
0z′
0∥2
2,
zRTIL
1 = arg min
z′
1∈Rn1∥y−AWRTIL
1 (W⋆
0zRTIL
0 +z′
1)∥2
2.
It is then easy to see that
zRTIL
0 =z⋆
0+ (AW⋆
1W⋆
0)†AW⋆
1z⋆
1
zRTIL
1 =z⋆
1−W⋆
0(AW⋆
1W⋆
0)†AW⋆
1z⋆
1
where (AW⋆
1W⋆
0)†denotes the pseudoinverse of (AW⋆
1W⋆
0). It then follows that W⋆
1(W⋆
0zRTIL
0 +zRTIL
1) =
W⋆
1(W⋆
0z⋆
0+z⋆
1) =x⋆, which implies the thesis.
16Published in Transactions on Machine Learning Research (01/2023)
A Appendix
Code provided in supplementary folder. Computational requirements for this paper are two NVIDIA 2080 Ti
GPU: training StyleGAN2 uses both GPU’s and one GPU for inversion, training PGGAN requires one GPU
and one for inversion.
A.1 RTIL-ILO Training Details
All experiments for ILO inversion method used StyleGAN2 architecture (Karras et al., 2020), both vanilla
and RTIL models were trained for 500,000 iterations using the same training parameters, i.e., learning rate,
batch size, and regularization updates. Below table 1 outlines the macroscopic view of the StlyeGAN2
architecture. Please refer StlyeGAN2 paper (Karras et al., 2020) for more architecture details, between each
convolutional layer there is a normalization operation called weight demodulation. During training for RTIL
the distribution was induced after each block in the network, which corresponds to cells 2-6 in table 1 up to
4-th convolutional block.
All experiments for ILO inversion method used StyleGAN2 architecture (Karras et al., 2020), both vanilla
and RTIL models were trained for 500,000 iterations using the same training parameters, i.e., learning rate,
batch size, and regularization updates. Please refer to the code for more details on the training process and
the StlyeGAN2 paper (Karras et al., 2020) for more details on the architecture.
Below Table 1 outlines the macroscopic view of the StlyeGAN2 architecture, between each convolutional
layer there is a normalization operation called weight demodulation. During training for RTIL the additional
latent variables were added after each block in the network, which correspond to cells 2-6 in Table 1 up to
4-th convolutional block.
Table 1: StyleGan2 for image size 256×256×3
Generator
Operation Activation Output Shape
Latent Vector None 512×1×1
8×MLP (Mapping Network) LRelu 512×14
Constant input None 512×4×4
Conv 3×3 LRelu 256×4×4
Upsample None 256×8×8
Conv 3×3 LRelu 256×8×8
Conv 3×3 LRelu 256×8×8
Upsample None 256×16×16
Conv 3×3 LRelu 256×16×16
Conv 3×3 LRelu 256×16×16
Upsample None 256×32×32
Conv 3×3 LRelu 256×32×32
Conv 3×3 LRelu 256×32×32
Upsample None 256×64×64
Conv 3×3 LRelu 256×64×64
Conv 3×3 LRelu 256×64×64
Upsample None 128×128×128
Conv 3×3 LRelu 128×128×128
Conv 3×3 LRelu 128×128×128
Upsample None 128×256×256
Conv 3×3 LRelu 64×256×256
Conv 3×3 LRelu 64×256×256
Conv 1×1 Linear 3×256×256
Trainable Parameters : 12,300,877
17Published in Transactions on Machine Learning Research (01/2023)
Figure 8: Samples from each of our trained generative models. Top row consist samples from vanilla trained
StyleGAN2, then rows G0
θ···G4
θare samples from family of generative models trained with RTIL.
18Published in Transactions on Machine Learning Research (01/2023)
A.2 RTIL-mGANprior Training Details
All experiments for mGANprior inversion method used PGGAN Architecture (Karras et al., 2018), both
vanilla and RTIL models were trained for 600,000 iterations and use the same training parameters, i.e.,
learning rate, batch size, and regularization updates. If interested, please to refer to the code for more details.
Below, table 2 outlines the details of the PGGAN architecture. The intermediate latent variable were added
after the 4-th block, which corresponds to the 4-th cell in Table 2.
Table 2: PGGAN for image size 256×256×3
Generator
Operation Activation Output Shape
Latent Vector None 512×1×1
Conv 4×4 LRelu 256×4×4
Conv 3×3 LRelu 256×4×4
Upsample None 256×8×8
Conv 3×3 LRelu 256×8×8
Conv 3×3 LRelu 256×8×8
Upsample None 256×16×16
Conv 3×3 LRelu 256×16×16
Conv 3×3 LRelu 256×16×16
Upsample None 256×32×32
Conv 3×3 LRelu 256×32×32
Conv 3×3 LRelu 256×32×32
Upsample None 256×64×64
Conv 3×3 LRelu 128×64×64
Conv 3×3 LRelu 128×64×64
Upsample None 128×128×128
Conv 3×3 LRelu 64×128×128
Conv 3×3 LRelu 64×128×128
Upsample None 64×256×256
Conv 3×3 LRelu 64×256×256
Conv 3×3 LRelu 64×256×256
Conv 1×1 Linear 3×256×256
Trainable Parameters : 7,445,443
19Published in Transactions on Machine Learning Research (01/2023)
Figure 10: Samples from each of our trained generative models. Top row consist samples from vanilla trained
PGGan, then rows G0
θ···G2
θare samples from family of generative models trained with RTIL.
A.3 Inversion Details for ILO
Hyper-parameters were tuned based on experiments in the appendix of the ILO paper (Daras et al., 2021)
and the official GitHub repository. Our implementation uses the code from the GitHub repository, please
refer to supplementary material to see. For more details about our hyper-parameter choice, learning rate and
number of iterations per layer were tuned for compressed sensing, then the same configuration were used for
inpainting and super-resolution. For experiments in Section 4 .1 Figure 4 the configuration for the number of
iterations per layer is {2000,1000,1000,1000,2000}. ILO-RTIL uses a learning rate that begins at .2at the
initial layer, then ramps up linearly and ramps down using a cosine scheduler, as proposed by (Karras et al.,
2020). As for ILO, each layer initialized with a learning rate of .1then optimized independently using the
same learning rate scheduler, which is proposed in official Github repository (Daras et al., 2021). We choose
the intermediate layer up to which optimize to, based on ablation study Section 4 .3.1 Figure 6. Below we
20Published in Transactions on Machine Learning Research (01/2023)
report the loss function used for each inverse problem in case of ILO-RTIL and RTIL proposed in Daras et al.
(2021):
•Compressed Sensing - Mean square error
•Inpainting - Equal weighted combination of mean square error and LPIPS (Zhang et al., 2018) for
sufficient number of measurements. Notice that with more than 50% missing pixels LPIPS did not
help reconstruction performance.
•Super-Resolution - Equal weighted combination of mean square error and LPIPS for ILO-RTIL, for
vanilla ILO LPIPS loss is weighted more with λ= 1.5. Except at downsampling ratio of1
16, only
MSE loss is utilized.
Figure 11: Performance of ILO-RTIL and vanilla trained ILO for Compressed sensing, inpainting, and super
resolution for various under-sampling ratios. ILO-RTIL increases performances or ties for each under-sampling
ratio with respect to SSIM across each of the inverse problems compared to ILO. The vertical bars indicate
95% confidence intervals.
Figure 12: Performance of ILO-RTIL and vanilla trained ILO for Compressed sensing, inpainting, and super
resolution for various under-sampling ratios. ILO-RTIL increases performances or ties for each under-sampling
ratio with respect to LPIPS across each of the inverse problems compared to ILO. The vertical bars indicate
95% confidence intervals.
21Published in Transactions on Machine Learning Research (01/2023)
A.4 Inversion Details for mGANprior
Hyper-parameters were tuned based on the Github repository (Gu et al., 2020) for Section 4 .2 Figure 15,
learning rate and the number of iterations were tuned for compressed sensing then reused for inpainting and
super-resolution. mGANprior-RTIL uses Adam (Kingma & Ba, 2015) optimizer initialized at learning rate of .1
and optimized for 2500 iterations. mGANprior uses SGD initialized at learning rate 1and optimized for 2500
iterations, which is based on the official GitHub repository (Gu et al., 2020). Empirically, the mGANprior (Gu
et al., 2020) improvement in reconstruction saturated after N= 20latent codes. Moreover, for selecting
which intermediate layer to optimize over for the vanilla model was determined by the ablation study in
Figure 22. Below we report the loss function used for each inverse problem in case of mGANprior-RTIL and
mGANprior
•Compressed Sensing - Mean square error loss for both methods.
•Inpainting - Mean square error plus l1LPIPS regularization proposed by (Gu et al., 2020). For
mGANprior-RTIL the regularization term was λ=.1and for mGANprior λ=.5.
•Super-Resolution - Mean square error plus l1LPIPS regularization proposed by (Gu et al., 2020) or
mGANprior-RTIL the regularization term was scaled λ=.1and for mGANprior λ=.5. Except at
downsampling ratio of1
16where the we only use MSE loss.
Figure 13: The left side portrays vanilla training for mGANprior, and the right side demonstrates RTIL.
For mGANprior-RTIL this example with N= 2latent codes, where the top model trains a vanilla model
analogous to model on the left, then parameter shares with the model below.
22Published in Transactions on Machine Learning Research (01/2023)
Figure 14: Performance of mGANprior-RTIL and vanilla trained mGANprior for compressed sensing,
inpainting, and super resolution for various under-sampling ratios. mGANprior-RTIL increases performances
over vanilla mGANprior with respect to SSIM over each under-sampling ratio, except for super-resolution
problems at low under-sampling ratio’s. The vertical bars indicate 95% intervals.
Figure 15: Performance of mGANprior-RTIL and vanilla trained mGANprior for compressed sensing,
inpainting, and super resolution for various under-sampling ratios. mGANprior-RTIL increases performances
over vanilla mGANprior with respect to SSIM over each under-sampling ratio, except for super-resolution
problems at low under-sampling ratio’s. The vertical bars indicate 95% intervals.
A.5 Inverse Problems Qualitative Results
23Published in Transactions on Machine Learning Research (01/2023)
Figure 16: Qualitative comparison between our method ILO-RTIL and ILO for inpainting at 5% of observed
pixels.
Figure 17: Qualitative comparison between our method mGANprior-RTIL and mGANprior for inpainting at
10% of observed pixels.
24Published in Transactions on Machine Learning Research (01/2023)
Figure 18: Comparison between ILO-RTIL to ILO for super-resolution LR 4x (Downsampling factor1
4)
Figure 19: Comparison between mGANprior-RTIL to mGANprior for super-Resolution LR 4x (Downsampling
factor1
4)
25Published in Transactions on Machine Learning Research (01/2023)
A.6 Ablation
A.6.1 ILO-Ablation
This section corresponds to Section 4 .3.1, Figure 6, the configuration for each optimization setting
go as:z0={2000},z1={2000,2000},z2={2000,1000,2000},z3={2000,1000,1000,2000},
z4={2000,1000,1000,1000,2000}iterations per layer.
Figure20demonstrateshowreconstructionperformancecanvarybasedonthenumberofiterationsperinterme-
diate layer while using ILO algorithm. The configuration for EXP 1-5 goes as following in order of number of it-
erations per intermeidate layer: EXP 1 ={100,100,100,100,100}, EXP 2 ={300,300,300,300,300}, EXP 3 =
{1000,1000,1000,1000,1000}, EXP 4 ={2000,1000,1000,1000,2000},EXP 5 ={2000,2000,2000,2000,2000}.
Figure 20: Compressed Sesning with Circulant Matrices reconstruction performance for various number of
iteration per intermediate layer with ILO.
26Published in Transactions on Machine Learning Research (01/2023)
Figure 21 demonstrates how reconstruction performance can be affected based on the size of ℓ1balls used
in ILO algorithm 1. The number of iterations for each of the experiments {2000,1000,1000,1000,2000}
corresponding to each intermediate layer, which is The same used in the paper. Size of the ℓ1
balls were determined based on the author’s recommendation in the appendix Daras et al. (2021).
EXP 1-4 goes as following in order of number intermeidate layer and the corresponding radius
of the size of search space for the ℓ1ball of radius of noise, latent code, and projection of the
previous solution: 1- {100,200,500,1000,2000},{100,200,300,800,1000},{100,200,300,800,1000},2-
{200,1000,100,2000,4000},{200,400,800,1600,3200},{200,400,800,1600,3200},3-
{1500,1500,2500,4000,5000},{300,500,1000,2000,3000},{300,1200,2000,3000,5000},4-
{500,1500,2500,4000,5000},{500,750,1500,2500,4000},{500,750,1500,2500,5000}.
Figure 21: Compressed Sesning with Circulant Matrices reconstruction performance for various ℓ1constraints.
27Published in Transactions on Machine Learning Research (01/2023)
A.6.2 mGANprior-Ablation
Figure 22: Effects on in-painting and super-resolution performance for various intermediate layer on validation
set of 5 images.
28