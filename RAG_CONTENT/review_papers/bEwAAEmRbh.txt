Published in Transactions on Machine Learning Research (11/2024)
Large-width asymptotics and training dynamics of α-Stable
ReLU neural networks
Stefano Favaro stefano.favaro@unito.it
Department of Economics and Statistics
University of Torino and Collegio Carlo Alberto
Sandra Fortini sandra.fortini@unibocconi.it
Department of Decision Sciences
Bocconi University
Stefano Peluchetti phd.st.p@gmail.com
Cogent Labs, Tokyo
Reviewed on OpenReview: https: // openreview. net/ forum? id= bEwAAEmRbh
Abstract
Large-width asymptotic properties of neural networks (NNs) with Gaussian distributed
weightshavebeenextensivelyinvestigatedintheliterature, withmajorresultscharacterizing
their large-width asymptotic behavior in terms of Gaussian processes and their large-width
training dynamics in terms of the neural tangent kernel (NTK). In this paper, we study
large-width asymptotics and training dynamics of α-Stable ReLU-NNs, namely NNs with
ReLU activation function and α-Stable distributed weights, with α∈(0,2). Forα∈(0,2],
α-Stable distributions form a broad class of heavy tails distributions, with the special case
α= 2corresponding to the Gaussian distribution. Firstly, we show that if the NN’s width
goes to inﬁnity, then a rescaled α-Stable ReLU-NN converges weakly (in distribution) to
anα-Stable process, which generalizes the Gaussian process. As a diﬀerence with respect
to the Gaussian setting, our result shows that the activation function aﬀects the scaling
of theα-Stable NN; more precisely, in order to achieve the inﬁnite-width α-Stable process,
the ReLU activation requires an additional logarithmic term in the scaling with respect
to sub-linear activations. Secondly, we characterize the large-width training dynamics of
α-Stable ReLU-NNs in terms an inﬁnite-width random kernel, which is referred to as the
α-Stable NTK, and we show that the gradient descent achieves zero training error at linear
rate, for a suﬃciently large width, with high probability. Diﬀerently from the NTK arising
in the Gaussian setting, the α-Stable NTK is a random kernel; more precisely, the random-
ness of the α-Stable ReLU-NN at initialization does not vanish in the large-width training
dynamics.
1 Introduction
There exists a vast literature on the interplay between Gaussian processes and the large-width asymptotic
behaviour of Gaussian neural networks (NNs), namely NNs with Gaussian distributed weights (Neal, 1996;
Der and Lee, 2006; Garriga-Alonso et al., 2018; Lee et al., 2018; Matthews et al., 2018; Novak et al., 2018;
Yang, 2019;a;b; Bracale et al., 2021; Eldan et al., 2021; Klukowski, 2022; Yang and Hu, 2021; Basteri and
Trevisan, 2022; Favaro et al., 2023; Hanin, 2023; Trevisan, 2023; Hanin, 2024). To deﬁne a Gaussian NN,
consider the following elements: i) for d,k≥1letXbe ad×kNN’s input, such that xj= (xj1,...,xjd)Tis
thej-th input (column vector); ii) let φbe an activation function; iii) for m≥1letW= (w(0)
1,...,w(0)
m,w)
be the NN’s weights, such that w(0)
i= (w(0)
i1,...,w(0)
id)andw= (w1,...,wm)with thew(0)
ij’s and thewi’s
1Published in Transactions on Machine Learning Research (11/2024)
being i.i.d. according to a Gaussian distribution with mean 0and variance σ2. A Gaussian φ-NN of width
mis
fm(W,X,φ ) = (fm(W,x 1,φ),...,fm(W,xk,φ)), (1)
where
fm(W,xj,φ) =m/summationdisplay
i=1wiφ(/angbracketleftw(0)
i,xj/angbracketright)j= 1,...,k.
Neal (1996) ﬁrst investigated the large-width behaviour of fm(W,X,φ ), which follows by a straightforward
application of the central limit theorem (CLT). In particular, it is well-known that if m→+∞, then
the rescaled Gaussian φ-NNm−1/2fm(W,X,φ )converges weakly (or in distribution) to a Gaussian process
with covariance function ΣX,φsuch that ΣX,φ[r,s] =σ2E[φ(/angbracketleftw(0)
i,xr/angbracketrightφ(/angbracketleftw(0)
i,xs/angbracketright]. Some extensions of this
inﬁnite-width limit are available for deep NNs (Matthews et al., 2018), more general NN’s architectures
(Yang, 2019a;b), and inﬁnite-dimensional inputs (Bracale et al., 2021; Eldan et al., 2021; Favaro et al.,
2023).
The large-width training dynamics of Gaussian NNs has been also extensively investigated in the literature,
with the training being performed through the gradient descent (Jacot et al., 2018; Arora et al., 2019; Du
et al., 2019; Lee et al., 2019). In particular, consider the Gaussian ReLU-NN fm(W,X ) =fm(W,X,ReLU ),
and set
˜fm(W,X ) :=1
m1/2fm(W,X ).
Let(X,Y )be the training set, where Y= (y1,...,yk)is the (training) output such that yjcorresponds
to thej-th inputxj. By considering a random initialization W(0)for the NN’s weights, and assuming a
squared-error loss, the gradient ﬂow of W(t)leads to the training dynamics of ˜fm(W(t),X), that is for any
t≥0
d˜fm(W(t),X)
dt=−(˜fm(W(t),X)−Y)ηmHm(W(t),X), (2)
whereηm>0is the learning rate, and Hm(W(t),X)is ak×krandom matrix whose (j,j/prime)entry is
/angbracketleft∂˜fm(W(t),xj)/∂W,∂ ˜fm(W(t),xj/prime)/∂W/angbracketright. By assuming ηm= 1, Jacot et al. (2018) ﬁrst characterized the
large-width training dynamics of ˜fm(W(t),X), showing that: i) if m→+∞thenHm(W(0),X)converges
in probability to a deterministic matrix H∗(X,X ); ii) the gradient descent achieves zero training error at
linear rate, i.e.
/bardblY−˜fm(W(t),X)/bardbl2
2≤exp(−λ0t)/bardblY−˜fm(W(0),X)/bardbl2
2
formsuﬃciently large, with high probability. The limiting matrix H∗(X,X )is refereed to as the neural
tangent kernel (NTK). See Yang (2019) and Yang and Littwin (2021) for extensions to deep NNs and general
architectures.
1.1 Our contributions
In this paper, we study large-width asymptotics and training dynamics of α-Stable ReLU-NNs, namely NNs
with a ReLU activation function and α-Stable distributed weights. For α∈(0,2],α-Stable distributions
form a broad class of heavy tails distributions, with the special case α= 2corresponding to the Gaussian
distribution; see Samoradnitsky and Taqqu (1994) and references therein for an overview on α-Stable dis-
tributions. According to the deﬁnition (1), we denote by fm(W,X,φ ;α)theα-Stableφ-NN, namely a NN
of the form (1) with the weighs Wdistributed according to the α-Stable distribution with α∈(0,2), thus
excluding the Gaussian case α= 2. In particular, fm(W,X ;α) =fm(W,X,ReLU ;α)denotes the α-Stable
ReLU-NN.
1.1.1 Related work
Neal (1996) considered α-Stable distributions to initialize NNs’ weights, showing that while all Gaussian
weights vanish in the inﬁnite-width limit, some α-Stable weights retain a non-negligible contribution. Such
a diﬀerent behaviour may be attribute to the diversity of the NN’s path properties as α∈(0,2]varies, which
makesα-Stable NNs more ﬂexible than Gaussian NNs; see Figure 1. Further works demonstrating practical
2Published in Transactions on Machine Learning Research (11/2024)
applications of α-Stable NNs, with respect to Gaussian NN’s, are Der and Lee (2006), Fortuin et al. (2019),
Fortuin (2022), Lee et al. (2022) and Li et al. (2022); the empirical analyses developed in Fortuin et al.
(2019) shows that wide α-Stable NNs trained with gradient descent lead to a higher classiﬁcation accuracy
than Gaussian NNs. Motivated by these works, Favaro et al. (2020; 2021) ﬁrst investigated the large with
asymptoticbehaviorof fm(W,X,φ ;α). Inparticular, assuming α∈(0,2)andasub-linearactivationfunction
φit is proved that if m→+∞, then the rescaled α-Stableφ-NNm−1/αfm(W,X,φ ;α)converges weakly
to anα-Stable process, that is a stochastic process with α-Stable ﬁnite-dimensional distributions. See also
(Jung, 2023).
Figure 1: Sample paths of α-Stable NNs, as a random function mapping an input in [0,1]2toR, with a
ReLU activation function and width m= 1024: i) top-left panel α= 2.0(Gaussian distribution); ii) top-right
panelα= 1.5; iii) bottom-left panel α= 1.0(Cauchy distribution); iv) bottom-right panel α= 0.5(Lévy
distribution).
1.1.2 Large-width asymptotics
We extend the main results of Favaro et al. (2020; 2021) to the ReLU activation function, which is arguably
one of the most popular activation function in the ﬁeld of NNs. In particular, we show that if m→+∞, then
the rescaled α-Stable ReLU-NN (mlogm)−1/αfm(W,X ;α)converges weakly to an α-Stable process. For
NNs with a single input, i.e. k= 1, the large-width limit follows by a direct application of the generalized
CLT for heavy tails distributions (Uchaikin and Zolotarev, 2011; Bordino et al., 2022), whereas for k >1
it requires to develop an alternative strategy that may be of independent interest in the context of multidi-
mensionalα-Stable distributions (Samoradnitsky and Taqqu, 1994, Chapter 1 and Chapter 2). Diﬀerently
from the Gaussian setting, the large-width asymptotic behaviour of α-Stable NNs shows how the choice of
the activation function φaﬀects the scaling of the NN. More precisely, in order to achieve the inﬁnite-width
α-Stable process, the use of the ReLU activation in place of a sub-linear activation results in a change of
the scaling m−1/αof the NN through the additional (logm)−1/αterm. See also Bordino et al. (2022) for
a detailed discussion of this peculiar phenomenon in the context of α-Stable ReLU-NN with a single input
(k= 1).
1.1.3 Large-width training dynamics
We investigate the large-width training dynamics of α-Stable ReLU-NNs, with the training being performed
bygradientdescentunderthesquared-errorloss. Inparticular, considerthe α-StableReLU-NN fm(W,X ;α),
3Published in Transactions on Machine Learning Research (11/2024)
and set
˜fm(W,X ;α) =1
(mlogm)1/αfm(W,X ;α). (3)
In analogy with (2), we deﬁne the training dynamics of ˜fm(W(t),X;α), with a learning rate ηmand ak×k
random matrix Hm(W(t),X;α)whose (j,j/prime)entry is/angbracketleft∂˜fm(W(t),xj;α)/∂W,∂ ˜fm(W(t),xj/prime;α)/∂W/angbracketright. By
assuming the learning rate ηm= (logm)2/α, we show that: i) if m→+∞then (logm)2/αHm(W(0),X;α)
converges weakly to an (α/2)-Stable (almost surely) positive deﬁnite random matrix ˜H∗(X,X ;α); ii) and
for everyδ > 0the gradient descent achieves zero training error at linear rate, for msuﬃciently large,
with probability 1−δ. The limiting random matrix ˜H∗(X,X ;α)is refereed to as the α-Stable NTK.
Diﬀerently from the NTK that arises from the Gaussian setting, the α-Stable NTK is a random kernel.
More precisely, the randomness of the α-Stable ReLU-NN at initialization does not vanish in the large-width
training dynamics.
1.2 Organization of the paper
The paper is organized as follows. In Section 2 we characterize its large-width asymptotic behaviour of
α-Stable ReLU-NNs in terms of the inﬁnite-width α-Stable process. In Section 3 we characterize the large-
widthtraining dynamicsof α-StableReLU-NNs interms ofthe α-StableNTK, andwe show thatthegradient
descent achieves zero training error at linear rate, for a suﬃciently large width, with high probability. Section
4 contains a discussion of our results with respect to some directions of future work. Proofs are deferred to
the appendix.
2 Large-width asymptotics of α-Stable ReLU-NNs
We study the large-width asymptotic behaviour of α-Stable ReLU-NNs. The section is organized as follows:
i) we recall the deﬁnition of multidimensional α-Stable distribution (Section 2.1); ii) we deﬁne the α-Stable
ReLU-NN and characterize its large-width asymptotic behaviour in terms of the inﬁnite-width α-Stable
process (Section 2.2); iii) we present some numerical illustrations of the large-width behaviour of α-Stable
ReLU-NNs (Section ??). The main result of this section is Theorem 2.1, whose proof is deferred to Appendix
A.1.
2.1 Multidimensional α-Stable distribution
Forα∈(0,2], a random variable S∈Ris distributed according to a symmetric and centered 1-dimensional
α-Stable distribution with scale σ > 0if its characteristic function is E(exp{izS}) = exp{−σα|z|α}, and
we writeS∼St(α,σ). If the stability parameter α= 2thenSis distributed as a Gaussian distribution
with mean 0and variance σ2. Let Sk−1be the unit sphere in Rk, withk≥1, and let Γbe a symmetric
ﬁnite measure on Sk−1. Forα∈(0,2], we say that a random variable S∈Rkis distributed according to
a symmetric and centered k-dimensional α-Stable distribution with spectral measure Γif its characteristic
function is
E(exp{i/angbracketleftz,S/angbracketright}) = exp/braceleftbigg
−/integraldisplay
Sk−1|/angbracketleftz,s/angbracketright|αΓ(ds)/bracerightbigg
,
and we write S∼Stk(α,Γ). Let 1rbe ther-dimensional (column) vector with 1in ther-th entry and
0elsewhere, for any r= 1,...,k. Then, the r-th element of S, that isS1ris distributed as an α-Stable
distribution with scale
σ=/parenleftbigg/integraldisplay
Sk−1|/angbracketleft1r,s/angbracketright|αΓ(ds)/parenrightbigg1/α
.
We deal mostly with k-dimensional α-Stable distributions with discrete spectral measure, that is Γ(·) =/summationtext
1≤i≤nγiδsi(·)withn∈N,γi∈Randsi∈Sk−1, fori= 1,...,n(Samoradnitsky and Taqqu, 1994,
Chapter 2). All the random variables are deﬁned on a common probability space, say (Ω,F,P), unless
otherwise stated.
We make use of the following characterization of the spectral measure of α-stable distributions (Samoradnit-
sky and Taqqu, 1994, Chapter 2): if S∼Stk(α,Γ), then for every Borel set BofSk−1such that Γ(∂B) = 0,
4Published in Transactions on Machine Learning Research (11/2024)
it holds
lim
r→∞rαP/parenleftbigg
/bardblS/bardbl>r,S
/bardblS/bardbl∈B/parenrightbigg
=CαΓ(B),
where
Cα=/braceleftBigg1−α
Γ(2−α) cos(πα/2)α/negationslash= 1
2
πα= 1.(4)
The proof of this result is reported in Appendix B Moreover, the distribution of a random vector ξbelongs
to the domain of attraction of the St k(α,Γ)distribution, with α∈(0,2)andΓsimmetric ﬁnite measure on
Sk−1, if and only if
lim
n→∞nP/parenleftbigg
||ξ||>n1/α,ξ
||ξ||∈A/parenrightbigg
=CαΓ(A) (5)
for every Borel set AofSsuch that Γ(∂A) = 0. We refer to Appendix B for more details on the derivation of
(5). See also Samoradnitsky and Taqqu (1994, Chapter 1 and Chapter 2) for further details on the constant
Cα.
2.2 The inﬁnite-width α-Stable process
To deﬁne a generic ReLU NN, let us consider the following elements: i) for d,k≥1letXbe thed×k
NN’s input, such that xj= (xj1,...,xjd)Tis thej-th input (column vector); ii) for m≥1letW=
(w(0)
1,...,w(0)
m,w)be the NN’s weights, such that w(0)
i= (w(0)
i1,...,w(0)
id)andw= (w1,...,wm). A ReLU-
NN of width mis
fm(W,X ;α) = (fm(W,x 1;α),...,fm(W,xk;α)), (6)
where
fm(W,xj;α) =m/summationdisplay
i=1wi/angbracketleftw(0)
i,xj/angbracketrightI(/angbracketleftw(0)
i,xj/angbracketright>0)j= 1,...,k,
withI(·)being the indicator function. We denote by W(0) = (w(0)
1(0),...,w(0)
m(0),w(0))the NN’s weights
at random initialization. If the NN’s weights w(0)
ij’s and thewi’s are initialized as i.i.d. α-Stable random
variables, with α∈(0,2)andσ>0, thenfm(W(0),X;α)deﬁnes anα-Stable ReLU-NN of width m.
Theorem 2.1. For anyα∈(0,2), letfm(W(0),X;α)be anα-Stable ReLU-NN of width m. Ifm→+∞
then1
(mlogm)1/αfm(W(0),X;α)w−→f(X),
wheref(X)∼Stk(α,ΓX), with
ΓX=Cα
4d/summationdisplay
i=1(/bardbl[xjiI(xji>0)]j/bardblα)D+
i(X) +/bardbl[xjiI(xji<0)]j/bardblα)D−
i(X)
such that
D+
i(X) =δ/parenleftbigg[xjiI(xji>0)]j
/bardbl[xjiI(xji>0)]j/bardbl/parenrightbigg
+δ/parenleftbigg
−[xjiI(xji>0)]j
/bardbl[xjiI(xji>0)]j/bardbl/parenrightbigg
and
D−
i(X) =δ/parenleftbigg[xjiI(xji<0)]j
/bardbl[xjiI(xji<0)]j/bardbl/parenrightbigg
+δ/parenleftbigg
−[xjiI(xji<0)]j
/bardbl[xjiI(xji<0)]j/bardbl/parenrightbigg
,
where, for any s∈Sk−1,δ(s)is the probability measure degenerate in s, andCαis in (4). The stochastic
processf(X) = (f(x1),...,f (xk)), as a process indexed by X, is anα-Stable process with spectral measure
ΓX.
Sketch of the proof of Theorem 2.1. Theα-stable ReLU-NN of width mis a sum of mindependent and
identically distributed random vectors. The proof relies on the analysis of the tail behavior of these sum-
mands, and it exploits a characterization of the multivariate α-Stable distribution as the limiting distribution
5Published in Transactions on Machine Learning Research (11/2024)
of sums of independent random vectors that exhibit speciﬁc tail properties. We refer to Appendix A.1 for
details.
For a broad class of bounded or sub-linear activation functions, Favaro et al. (2021) characterizes the large-
width distribution of deep α-Stable NNs. See also Bordino et al. (2022) and references therein. In particular,
let
fm(W,xj,φ;α) =m/summationdisplay
i=1wiφ/angbracketleftw(0)
i,xj/angbracketright
be theα-Stableφ-NN of width mfor the input xj, forj= 1,...,k, withφbeing a bounded activation
function. Let fm(X;α) = (fm(x1;α),...,fm(xk;α)). From Favaro et al. (2021, Theorem 1.2), if m→+∞
then
1
m1/αfm(W,X,φ ;α)w−→f(X), (7)
withf(X)being anα-Stable process with spectral measure ΓX,φ. Theorem 2.1 extends Favaro et al. (2021,
Theorem 1.2) to the ReLU activation function. Theorem 2.1 shows that the use of the ReLU activation
in place of a bounded activation results in a change of the scaling m−1/αin (7), through the inclusion of
the(logm)−1/αterm. This is a critical diﬀerence between the α-Stable setting and Gaussian setting, as
in the latter the choice of the activation function φdoes not aﬀect the scaling m−1/2required to achieve
the inﬁnite-width Gaussian process. For k= 1, we refer to Bordino et al. (2022) for a detailed analysis of
inﬁnitely wide limits of α-Stable NNs with general classes of sub-linear, linear and super-linear activation
functions.
Remark 2.1. The need of the additional log(m)can be clariﬁed by considering the α-Stable ReLU-NN
with a single input, i.e. k= 1, where the proof of Theorem 2.1 reduces to a straightforward application of
the generalized CLT for heavy tails distributions (Uchaikin and Zolotarev, 2011; Bordino et al., 2022). In
particular, we refer to Theorem 2.1. and Theorem 2.6 of Bordino et al. (2022), which show how the logterm
arises from the tail behaviour of the product of α-Stable random variable wiw(0)
i’s, which deﬁnes the NN; see
Cline (1986) and references therein. The logterm is expected to hold for any activation that has a linear
growth.
To demonstrate numerically Theorem 2.1, we sample random neural networks according to 3 for various
values of width mand stability index α. We evaluate these networks on a ﬁne uniform grid of points in
[0,1]2. Figure 2 displays the results, which show that the function samples remain well-behaved as mgrows
larger.
3 Large-width training dynamics of α-Stable ReLU-NNs
We study the large-width training dynamics of α-Stable ReLU-NNs. The section is organized as follows: we
deﬁne the training dynamics of the α-Stable ReLU-NN and characterize its large-width asymptotic behaviour
in terms of the α-Stable NTK (Section 3.1); ii) we show that the gradient descent achieves zero training
error at linear rate, for a suﬃciently large width, with high probability (Section 3.2). The main results of
this section are Theorem 3.1 and Theorem 3.2, whose proofs are deferred to Appendix A.2 and Appendix
A.4, respectively.
3.1 Theα-Stable NTK
Letfm(W,X ;α)be theα-Stable ReLU-NN deﬁned in (6), with α∈(0,2), and let (X,Y )be the training
set, whereY= (y1,...,yk)is the (training) output such that yjcorresponds to the j-th inputxj. Then, we
set
˜fm(W,X ;α) =1
(mlogm)1/αfm(W,X ;α),
such that ˜fm(W,xj;α) = (mlogm)−1/αfm(W,xj;α)is the (model) output of the j-th inputxj, forj=
1,...,k. Assuming the squared-error loss function /lscript(yj,˜fm(W,xj;α)) = 2−1/summationtext
1≤j≤k(˜fm(W,xj;α)−yj)2, by
6Published in Transactions on Machine Learning Research (11/2024)
Figure 2: Sample paths of α-Stable NNs, as a random function mapping an input in [0,1]2toR, with a
ReLU activation function; width values (left to right): m= 64,256,1024,4096;α= 1.5(top panel), α= 1.0
(bottom panel).
a direct application of the chain rule we obtain the training dynamics of ˜fm(W,X ;α). In particular, for any
t≥0
d˜fm(W(t),X;α)
dt=−(˜fm(W(t),X;α)−Y)ηmHm(W(t),X), (8)
where the kernel Hm(W(t),X)in the NN’s training dynamics is a k×krandom matrix whose (j,j/prime)entry
is
Hm(W(t),X)[j,j/prime] =/angbracketleftbigg∂˜fm(W(t),xj;α)
∂W,∂˜fm(W(t),xj/prime;α)
∂W/angbracketrightbigg
, (9)
andηmis the learning rate. The training dynamics for ˜fm(W,X ;α)is standard, and if follows training
dynamics presented in Section 1 for the Gaussian setting. See (Arora et al., 2019, Section 3) and references
therein for details.
For the training dynamics (8), we study the large-width behaviour of Hm(W(0),X)in (9). In particular, we
set
˜Hm(W(0),X) = (logm)2/αHm(W(0),X), (10)
and show that if m→+∞, then ˜Hm(W(0),X)converges weakly to a positive deﬁnite random matrix
˜H∗(X,X,α )distributedaccordingtoan (α/2)-Stabledistribution. Toprovethisresultitusefultodecompose
˜Hm(W(0),X)as
˜Hm(W(0),X) =˜H(1)
m(W(0),X) +˜H(2)
m(W(0),X), (11)
where
˜H(1)
m(W(0),X)[j,j/prime] =1
m2/αm/summationdisplay
i=1w2
i/angbracketleftxj,xj/prime/angbracketrightI(/angbracketleftw(0)
i,xj/angbracketright>0)I(/angbracketleftw(0)
i,xj/prime/angbracketright>0), (12)
and
˜H(2)
m(W(0),X)[j,j/prime] =1
m2/αm/summationdisplay
i=1/angbracketleftw(0)
i,xj/angbracketrightI(/angbracketleftw(0)
i,xj/angbracketright>0)/angbracketleftw(0)
i,xj/prime/angbracketrightI(/angbracketleftw(0)
i,xj/prime/angbracketright>0),(13)
7Published in Transactions on Machine Learning Research (11/2024)
respectively. The next theorem characterizes the large-width asymptotic behaviour of ˜Hm(W(0),X)in terms
of theα-Stable NTK.
Theorem 3.1. For anyα∈(0,2), let ˜Hm(W(0),X),˜H(1)
m(W(0),X)and ˜H(2)
m(W(0),X)be thek×krandom
matrices whose (j,j/prime)entries are deﬁned in (10), (12), and (13), respectively. Moreover, for every k≥1
andu∈{0,1}k, let
Bu={v∈Rd:/angbracketleftv,xj/angbracketright>0ifuj= 1,/angbracketleftv,xj/angbracketright≤0ifuj= 0,j= 1,...,k},
and for every i= 1,...,d, leteibe ad-dimensional vector such that eij= 1forj=iandeij= 0forj/negationslash=i.
Asm→+∞,
(˜H(1)
m(W(0),X),˜H(2)
m(W(0),X))w−→(˜H∗
1(α),˜H∗
2(α)),
where ˜H∗
1(α)and ˜H∗
2(α)arek×krandom matrices that are stochastically independent, positive semi-deﬁnite,
and distributed according to (α/2)-Stable distributions with spectral measures Γ∗
1andΓ∗
2, respectively, such
that
Γ∗
1=Cα/2/summationdisplay
u∈{0,1}kP(w(0)
i(0)∈Bu)δ/parenleftbigg[/angbracketleftxj,xj/prime/angbracketrightujuj/prime]j,j/prime
(/summationtext
j,j/prime/angbracketleftxj,xj/prime/angbracketright2ujuj/prime)1/2/parenrightbigg
/parenleftBig/summationtext
j,j/prime/angbracketleftxj,xj/prime/angbracketright2ujuj/prime/parenrightBig−α/4, (14)
and
Γ∗
2=Cα/2/summationdisplay
u∈{0,1}k/summationdisplay
{i:{ei,−ei}∩Bu/negationslash=∅}δ/parenleftbigg
[xjiujxj/primeiuj/prime]j,j/prime/summationtext
jx2
jiuj/parenrightbigg
/parenleftBig/summationtext
jx2
jiuj/parenrightBig−α/2, (15)
withCα/2in (4). Furthermore, as m→∞,
˜Hm(W(0),X)w−→ ˜H∗(X,X ;α),
where ˜H∗(X,X ;α)is ak×krandom matrix that is positive semi-deﬁnite and distributed according to an
(α/2)-Stable distribution with spectral measure Γ∗= Γ∗
1+ Γ∗
2.˜H∗(X,X ;α)is refereed to as the α-Stable
NTK.
Sketch of the proof of Theorem 3.1. We can see (˜H(1)
m(W(0),X),˜H(2)
m(W(0),X))as a random vector of
dimension 2k2, withk≥1, whose elements are sums of independent and identically distributed random
vectors. The proof relies on the analysis of the tail behavior of these summands, and it exploits a charac-
terization of the multivariate α-Stable distribution as limiting distribution of the sum of independent and
identically distributed random vectors that exhibit speciﬁc tail properties. We refer to Appendix A.2 for the
details.
It turns out that the (α/2)-Stable distributions of the limiting random matrices ˜H∗
1(α)and ˜H∗
2(α)are
absolutely continuous in suitable subspaces of the space of symmetric and positive semi-deﬁnite matrices;
see Lemma A.4 and Lemma A.5 for details on the distribution of the random matrix ˜H∗
1(α), and Lemma
A.6 and Lemma A.7 for details on the distribution of the random matrix ˜H∗
2(α). This is applied in the
next theorem to show that the minimum eigenvalues of ˜H(1)
m(W(0),X)and of ˜H(2)
m(W(0),X)are bounded
away from zero, uniformly in m, formsuﬃciently large, with arbitrarily high probability. Accordingly,
the minimum eigenvalue of ˜Hm(W(0),X) = ˜H(1)
m(W(0),X) +H(2)
m(W(0),X)is bounded away from zero,
uniformly in m, formsuﬃciently large, with arbitrarily high probability. We denote by λmin(·)the minimum
eigenvalue.
Proposition 3.1. For anyα∈(0,2), let ˜Hm(W,X ),˜H(1)
m(W,X )and ˜H(2)
m(W,X )be the random matrices
as in Theorem 3.1. For every δ >0there exist strictly positive numbers λ0,λ1andλ2such that, for m
suﬃciently large,
λmin(˜H(i)
m(W(0),X))>λii= 1,2,
and
λmin(˜Hm(W(0),X))>λ0.
with probability at least 1−δ.
8Published in Transactions on Machine Learning Research (11/2024)
See Appendix A.3 for the proof of Proposition 3.1. Theorem 3.1 and Proposition 3.1 provide an extension
of some of the main results of Jacot et al. (2018) to the setting of α-Stable ReLU NN, for α∈(0,2). See
also Du et al. (2019), Arora et al. (2019), Lee et al. (2019) and references therein. In particular, our results
show that
i) asm→+∞, the random matrix (logm)2/αHm(W(0),X)converges weakly to the α-Stable NTK
˜H∗(X,X ;α), such that ˜H∗(X,X ;α)is a(α/2)-Stable (almost surely) positive deﬁnite random ma-
trix;
ii) at random initialization for the α-Stable ReLU-NN, for every δ > 0the minimum eigenvalue of
the random matrix ˜Hm(W(0),X)remains bounded away from zero, for msuﬃciently large, with
probability 1−δ.
Diﬀerently from the NTK that arises from the Gaussian setting, the α-Stable NTK is a random kernel. That
is, the randomness of the α-Stable ReLU-NN at initialization does not vanish in the large-width training
dynamics. Such a randomness makes more challenging the study of the corresponding large-with training
dynamics.
3.2 Zero training error at linear rate
Under the training dynamics (8), we show that for every δ >0the gradient descent achieves zero training
error at linear rate, for msuﬃciently large, with probability 1−δ. In order to prove this result we combine
Proposition 3.1 with the next proposition, which shows that, if mis suﬃciently large, then with high
probability the minumum eigenvalue of the random matrix ˜Hm(W(t),X)remains bounded away from zero.
We denote by/bardbl·/bardblFand/bardbl·/bardbl 2the Frobenius and operator norms of symmetric and positive semi-deﬁnite
matrices, respectively.
Proposition 3.2. Letγ∈(0,1)andc>0. Fork≥1let the NN’s inputs x1,...,xkbe linearly independent
and such that/bardblxj/bardbl= 1. For anyα∈(0,2), let ˜Hm(W,X )and ˜H(2)
m(W,X )be the random matrices as in
Theorem 3.1. For every δ >0the following properties hold for every t≥0, with probability at least 1−δ,
formsuﬃciently large:
(i) for every j= 1,...,k,
(logm)2/α/vextenddouble/vextenddouble/vextenddouble/vextenddouble∂˜fm
∂w(W(t),xj;α)−∂˜fm
∂w(W(0),xj;α)/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
F<cm−2γ/α;
(ii) there exists λ0>0such that
/bardbl˜H(2)
m(W(t),X)−˜H(2)
m(W(0),X)/bardblF<λ0m−γ/α
and
λmin(˜Hm(W(t),X))>λ0
2.
Sketch of the proof of Proposition 3.2. The inequality displayed in (i) holds as long as W(t)stays within a
neighborhoodof W(0)withradiusontheorderof (logm)2/α,andviceversa. Thisimpliesthattheﬂuctuations
of∂˜f(W(t),X)/∂wduring the training of the α-Stable ReLU-NN vanish as m→∞. Consequently, the ﬁrst
inequality displayed in (ii) also holds throughout training if mis large enough. Together with Proposition
3.1, this ensures that the minimum eigenvalue of the random matrix ˜H(2)
m(W(t),X)remains bounded away
from zero during training. The same argument applies to the random matrix ˜Hm(W(t),X), which is the
sum of ˜H(2)
m(W(t),X)and of the non-negative deﬁnite matrix ˜H(1)
m(W(t),X). We refer to Appendix A.4 for
the details.
9Published in Transactions on Machine Learning Research (11/2024)
Now, we are in the position to show that the gradient descent achieves zero training error at linear rate, for
a suﬃciently large width, with high probability. From Proposition 3.2, for a ﬁxed δ >0, letmandλ0>0
be such that
λmin(˜Hm(W(s),X))>λ0
2.
for everys≤t, on a setN∈FwithP[N]>1−δ. Accordingly, for any random initialization W(0)(ω), with
ω∈N,
d
ds/bardblY−˜fm(W(s)(ω),X;α)/bardbl2
2≤−λ0/bardblY−˜fm(W(s)(ω),X;α)/bardbl2
2,
and hence
d
dsexp(λ0s)/bardblY−˜fm(W(s)(ω),X;α)/bardbl2
2≤0.
Therefore, by observing that exp(λ0s)/bardblY−˜fm(W(s)(ω),X;α)/bardbl2
2is a decreasing function of s>0, then we
write
/bardblY−˜fm(W(s)(ω),X;α)/bardbl2
2≤exp(−λ0s)/bardblY−˜fm(W(0)(ω),X;α)/bardbl2
2.
In the next theorem we summarize the main ﬁnding on the large-width training dynamics of α-Stable ReLU
NNs.
Theorem 3.2. Fork≥1let the NN’s inputs x1,...,xkbe linearly independent and such that /bardblxj/bardbl= 1.
For anyα∈(0,2), under the training dynamics (8), if the learning rate ηm= (logm)2/αthen for every
δ >0there exists λ0>0such that, for msuﬃciently large and any t>0, with probability at least 1−δit
holds true that
/bardblY−˜fm(W(t),X;α)/bardbl2
2≤exp(−λ0t)/bardblY−˜fm(W(0),X;α)/bardbl2
2.
4 Discussion
In this paper, we investigated large-width asymptotics and training dynamics of α-Stable ReLU-NNs, namely
NNs with a ReLU activation function and α-Stable distributed weights. With regards to the large-width
asymptotics, our result (Theorem 2.1) extends the main result of Favaro et al. (2020; 2021) to the ReLU
activationfunction, showingtheneedofanadditionallogarithmicterminthescalingoftheNNtoachievethe
inﬁnite-width α-Stable process. With regards to the large-width training dynamics, our results (Theorem 3.1
and Theorem 3.2) extends some of the main results of Jacot et al. (2018) to α-Stable ReLU-NNs, showing
that randomness of the α-Stable ReLU-NN at initialization does not vanish in the large-width training
dynamics.
Itremainsopentoestablishalarge-widthequivalencebetweentrainingan α-StableReLU-NNandperforming
a kernel regression with the α-Stable NTK. For Gaussian NN, Jacot et al. (2018) showed that during training
t > 0, ifmis suﬃciently large then the ﬂuctuations of the squared Frobenious norm /bardblHm(W(t),X)−
Hm(W(0),X)/bardbl2
Fare vanishing. This suggested to replace ηmHm(W(t),X)with the NTK H∗(X,X )in the
dynamics (2), and write
df∗(t,X)
dt=−(f∗(t,X)−Y)H∗(X,X ).
This is the dynamics of a kernel regression under gradient ﬂow, for which at t→+∞the prediction for
a generic test point x∈Rdis of the form f∗(x) =YH∗(X,X )−1H∗(X,x)T. In particular, the prediction
of the Gaussian NN ˜fm(W(t),x)att→+∞, formsuﬃciently large, is equivalent to the kernel regression
predictionf∗(x)(Arora et al., 2019). Within the α-Stable setting, it is not clear whether the ﬂuctuations
of˜Hm(W(t),X) =˜H(1)
m(W(t),X) +˜H(2)
m(W(t),X)during the training vanish, as m→∞. Proposition 3.2
shows that the ﬂuctuations of ˜H(2)
m(W(t),X)vanish, asm→∞. Such a result is based on the fact that for
everyδ>0it holds that
(logm)2/α/vextenddouble/vextenddouble/vextenddouble/vextenddouble∂˜fm
∂w(W,xj;α)−∂˜fm
∂w(W(0),xj;α)/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
F<cm−2γ/α,
10Published in Transactions on Machine Learning Research (11/2024)
for everyj= 1,...,k, and for every Wsuch that/bardblW−W(0)/bardblF≤(logm)2/α, with probability at least 1−δ,
ifmis suﬃciently large. In particular, we refer to Lemma A.8 for details. The same large-width property is
not true if the partial derivatives with respect to ware replaced by the partial derivatives with respect to
w(0). Accordingly, it is not clear whether the ﬂuctuations of ˜H(1)
m(W(t),X)during training also vanish, as
m→∞.
Another interesting avenue for future research would be to extend our results to the more general setting of
deep NNs, with D≥2being the depth. Let us consider the following setting: i) for d,k≥1letXbe the
d×kNN’s input, with xj= (xj1,...,xjd)Tbeing thej-th input (column vector); ii) for D,m≥1andn≥1
let: i) (W(1),...,W(D))be the NN’s weights such that W(1)= (w(1)
1,1,...,w(1)
m,d)andW(l)= (w(l)
1,1,...,w(1)
m,m)
for2≤l≤D, where the w(l)
i,j’s are i.i.d. as an α-Stable distribution with scale σ >0, e.g. we can assume
σ= 1. Then,
f(1)
i(X;α) =d/summationdisplay
j=1w(1)
i,jxj
and
f(l)
i,m(X;α) =m/summationdisplay
j=1w(l)
i,jf(l−1)
j(X,m )I(f(l−1)
j(X,m )>0)
withf(1)
i,m(X;α) =f(1)
i(X;α), is a deep α-Stable ReLU-NN of depth Dand width m. If the NN’s width
grows sequentially over the NN’s layers, i.e. m→+∞one layer at a time, it is easy to extend Theorem 2.1
tof(l)
i,m(X;α). Under the same assumption on the growth of m, we expect the analysis of the large-width
training dynamics to follow along lines similar to that of Theorem 3.1 and Theorem 3.2, though computations
may be more involved. A more challenging task would to extend our results to deep α-Stable ReLU-NNs
under the assumptions that the NN’s width grows jointly over the NN’s layers, i.e. m→+∞simultaneously
over the layers
Acknowledgments
The authors wish to thank the Action Editor, Professor Murat A. Erdogdu, and three anonymous Referees
for their helpful suggestions. Stefano Favaro was funded by the European Research Council under the
Horizon 2020 research and innovation programme, grant 817257. Stefano Favaro also gratefully acknowledge
support from the Italian Ministry of Education, University and Research, “Dipartimenti di Eccellenza" grant
2023-2027.
References
Arora, S., Du, S.S., Hu, W., Li, Z., Salakhutdinov, R.R. and Wang, R. (2019). On exact compu-
tation with an inﬁnitely wide neural net. In Advances in Neural Information Processing Systems .
Basteri, A. and Trevisan, D. (2022). Quantitative Gaussian approximation of randomly initialized deep
neural networks. Machine Learning 113, 6373–639.
Bordino, A., Favaro, S. and Fortini (2022). Inﬁnite-wide limits for Stable deep neural networks: sub-
linear, linear and super-linear activation functions. Transactions of Machine Learning Research .
Bracale, D., Favaro, S., Fortini and Peluchetti, S. (2021). Large-width functional asymptotics for
deep Gaussian neural networks. In International Conference on Learning Representations .
Cline, D.B.H. (1986). Convolution tails, product tails and domains of attraction. Probability Theory and
Related Fields 72, 525–557.
Der, R. and Lee, D. (2006). Beyond Gaussian processes: on the distributions of inﬁnite networks. In
Advances in Neural Information Processing Systems .
11Published in Transactions on Machine Learning Research (11/2024)
Du, S.S., Zhai, X., Poczos, B. and Singh, A. (2019). Gradient descent provably optimizes over-
parameterized neural networks. In International Conference on Learning Representations .
Eldan, R., Mikulincer, D. and Schramm, T. (2021).Non-asymptoticapproximationsofneuralnetworks
by Gaussian processes. In Conference on Learning Theory .
Favaro, S., Fortini, S. and Peluchetti, S. (2020). Stable behaviour of inﬁnitely wide deep neural
networks. In International Conference on Artiﬁcial Intelligence and Statistics .
Favaro, S., Fortini, S. and Peluchetti, S. (2020). Deep Stable neural networks: large-width asymp-
totics and convergence rates. Bernoulli 29, 2574–2597.
Favaro, S., Hanin, B., Marinucci, D., Nourdin, I. and Peccati, G. (2023). Quantitative central
limit theorems in deep neural networks. Preprint arXiv:2307.06092 .
Fortuin, V. (2022). Priors in Bayesian deep learning: a review. International Statistical Review 90, 563–
591.
Fortuin, V., Garriga-Alonso, A., Wenzel, F., Ratsch, G, Turner, R.E., van der Wilk, M.
and Aitchison, L. (2020). Bayesian neural network priors revisited. In Advances in Neural Information
Processing Systems .
Garriga-Alonso, A., Rasmussen, C.E. and Aitchison, L. (2018). Deep convolutional networks as
shallow Gaussian processes. In International Conference on Learning Representation .
Hanin, B. (2023). Random neural networks in the inﬁnite width limit as Gaussian processes. The Annals
of Applied Probability 33, 4798–4819.
Hanin, B. (2024). Random fully connected neural networks as perturbatively solvable hierarchies. Journal
of Machine Learning Research 25, 1–58.
Jacot, A., Gabriel, F, and Hongler, C. (2018). Neural tangent kernel: convergence and generalization
in neural networks. In Advances in Neural Information Processing Systems .
Jung, P., Lee, H., Lee, J. and Yang, H. (2023).α-Stable convergence of heavy-/light-tailed inﬁnitely
wide neural networks. Journal of Applied Probability 55, 1415–1441.
Klukowski, A. (2022). Rate of convergence of polynomial networks to Gaussian processes. In Conference
on Learning Theory .
Lee, H., Ayed, F., Jung, P., Lee, J., Yang, H., Caron, F. (2022). Deep neural networks with
dependent weights: Gaussian process mixture limit, heavy tails, sparsity and compressibility. Journal of
Machine Learning Research 24, 1–78.
Lee, J., Sohldickstein, J., Pennington, J., Novak, R., Schoenholz, S. and Bahri, Y. (2018).
Deep neural networks as Gaussian processes. In International Conference on Learning Representation .
Lee, J., Xiao, L., Schoenholz, S., Bahri, Y., Sohl-Dickstein, J. and Pennington, J. (2019).
Wide neural networks of any depth evolve as linear models under gradient descent. In Advances in Neural
Information Processing Systems .
Li, C., Dunlop, M. and Stadler, G. (2022).Bayesianneuralnetworkpriorsforedge-preservinginversion.
Inverse Problems and Imaging 16, 1229–1254.
Matthews, A.G., Rowland, M., Hron, J., Turner, R.E. and Ghahramani, Z. (2018). Gaussian
process behaviour in wide deep neural networks. In International Conference on Learning Representations .
Neal, R.M. (1996).Bayesian learning for neural networks . Springer.
12Published in Transactions on Machine Learning Research (11/2024)
Novak, R., Xiao, L., Bahri, Y., Lee, J., Yang, G., Hron, J., Abolafia, D., Pennington, J.
and Sohldickstein, J. (2018). Bayesian deep convolutional networks with many channels are Gaussian
processes. In International Conference on Learning Representation .
Samoradnitsky, G. and Taqqu, M.S (1994).Stable non-Gaussian random processes: stochastic models
with inﬁnite variance . Chapman and Hall/CRC.
Trevisan, D. (2023).WidedeepneuralnetworkswithGaussianweightsareveryclosetoGaussianprocesses.
Preprint arXiv:2312.11737 .
Uchaikin, V.V. and Zolotarev, V.M. (2011).Chance and stability: stable distributions and their appli-
cations. Walter de Gruyter.
Yang, G. (2019). Tensor programs II: neural tangent kernel for any architecture. Preprint arXiv:2006.14548 .
Yang, G. (2019). Scaling limits of wide neural networks with weight sharing: Gaussian process behavior,
gradient independence, and neural tangent kernel derivation. Preprint: arXiv:1902.04760 .
Yang, G. (2019). Tensor programs I: wide feedforward or recurrent neural networks of any architecture are
Gaussian processes. In Advances in Neural Information Processing Systems .
Yang, G. and Hu, E.J. (2021). Tensor programs IV: feature learning in inﬁnite-width neural networks. In
International Conference on Machine Learning .
Yang, G. and Littwin, E. (2021). Tensor programs IIb: architectural universality of neural tangent kernel
training dynamics. In International Conference on Machine Learning .
13Published in Transactions on Machine Learning Research (11/2024)
A
A.1 Proof of Theorem 2.1
To simplify the notation, we set in this section: w:=w(0),w(0):=w(0)(0), andW:=W(0). First, we will
prove that [/angbracketleftw(0)
i,xj/angbracketrightI(/angbracketleftw(0)
i,xj/angbracketright>0)]jbelongs to the domain of attraction of an α-stable law with spectral
measure
Γ1=CαEu∼Γ0/parenleftbigg
/bardbl[/angbracketleftu,xj/angbracketrightI(/angbracketleftu,xj/angbracketright>0)]j/bardblαδ/parenleftbigg[/angbracketleftu,xj/angbracketrightI(/angbracketleftu,xj/angbracketright>0)]j
/bardbl[/angbracketleftu,xj/angbracketrightI(/angbracketleftu,xj/angbracketright>0)]j/bardbl/parenrightbigg/parenrightbigg
,
where Γ0is the spectral measure of w(0)
i. For this, it is suﬃcient to show that
rαP/parenleftBigg
[/angbracketleftw(0)
i,xj/angbracketrightI(/angbracketleftw(0)
i,xj/angbracketright>0)]j
/bardbl[/angbracketleftw(0)
i,xj/angbracketrightI(/angbracketleftw(0)
i,xj/angbracketright>0)]j/bardbl∈B,/bardbl[/angbracketleftw(0)
i,xj/angbracketrightI(/angbracketleftw(0)
i,xj/angbracketright>0)]j/bardbl>r/parenrightBigg
→CαΓ1(B),
for every Borel set BofSk−1such that Γ1(∂B) = 0(see Appendix B). Let T:Sk−1/mapsto→[0,1]kandC:
Rk\{0}→Sk−1be deﬁned as T(u) = [/angbracketleftu,xj/angbracketrightI(/angbracketleftu,xj/angbracketright>0]jandC(v) =v//bardblv/bardbl, respectively. Fix a Borel
setBofSk−1such that Γ1(∂B) = 0. This condition implies that
Γ0/parenleftbig/braceleftbig
u∈Sk−1:/bardblT(u)/bardbl/negationslash= 0,T(u)∈C−1(∂B)/bracerightbig/parenrightbig
= Γ0/parenleftbigg/braceleftbigg
u∈Sk−1:/bardblT(u)/bardbl/negationslash= 0,T(u)
/bardblT(u)/bardbl∈∂B/bracerightbigg/parenrightbigg
= 0.
Hence
Γ0/parenleftbig
T−1/parenleftbig/braceleftbig
z∈[0,1]k:/bardblz/bardbl/negationslash= 0,z∈∂C−1(B)/bracerightbig/parenrightbig/parenrightbig
= Γ0/parenleftbig
T−1/parenleftbig/braceleftbig
z∈[0,1]k:/bardblz/bardbl/negationslash= 0,z∈C−1(∂B)/bracerightbig/parenrightbig/parenrightbig
= 0.
Now, letZ=T(w(0)
i//bardblw(0)
i/bardbl)I(/bardblw(0)
i/bardbl/negationslash= 0). We can write that
rαP/parenleftBigg
[/angbracketleftw(0)
i,xj/angbracketrightI(/angbracketleftw(0)
i,xj/angbracketright>0)]j
/bardbl[/angbracketleftw(0)
i,xj/angbracketrightI(/angbracketleftw(0)
i,xj/angbracketright>0)]j/bardbl∈B,/bardbl[/angbracketleftw(0)
i,xj/angbracketrightI(/angbracketleftw(0)
i,xj/angbracketright>0)]j/bardbl>r/parenrightBigg
=rαP/parenleftbigg
/bardblZ/bardbl/negationslash= 0,Z
/bardblZ/bardbl∈B,/bardblw(0)
i/bardbl/bardblZ/bardbl>r/parenrightbigg
=/integraldisplay
C−1(B)∩[0,1]krαP(/bardblw(0)
i/bardbl>r/bardblz/bardbl−1,Z∈dz)
=/integraldisplay
C−1(B)∩[0,1]k/bardblz/bardblα(r/bardblz/bardbl−1)αP(/bardblw(0)
i/bardbl>r/bardblz/bardbl−1,w(0)
i
/bardblw(0)
i/bardbl∈T−1(dz)).
Since Γ0/parenleftbig
T−1/parenleftbig/braceleftbig
z∈[0,1]k:z/negationslash= 0,z∈∂(C−1(B))/bracerightbig/parenrightbig/parenrightbig
= 0, then the points of discontinuity of the function
/bardblz/bardblαI(C−1(B))(z)have zero Γ0(T−1(·))-measure. It follows that
/integraldisplay
C−1(B)∩[0,1]k/bardblz/bardblα(r/bardblz/bardbl−1)αP(/bardblw(0)
i/bardbl>r/bardblz/bardbl−1,w(0)
i∈T−1(dz))
→Cα/integraldisplay
C−1(B)∩[0,1]k/bardblz/bardblαΓ0(T−1(dz))
=Cα/integraldisplay
Sk−1I(u∈B)/parenleftbiggT(u)
/bardblT(u)/bardbl/parenrightbigg
/bardblT(u)/bardblαΓ0(du)
=CαΓ1(B),
14Published in Transactions on Machine Learning Research (11/2024)
asr→∞, which completes the proof that [/angbracketleftw(0)
i,xj/angbracketrightI(/angbracketleftw(0)
i,xj/angbracketright>0)]jbelongs to the domain of attraction
of anα-stable law with spectral measure Γ1. Then, for every k-dimensional vector s,
1
m1/αm/summationdisplay
i=1k/summationdisplay
j=1sj/angbracketleftw(0)
i,xj/angbracketrightI(/angbracketleftw(0)
i,xj/angbracketright>0),
as a sequence of random variables in m, converges in distribution, as m→+∞, to a random variable with
α-stable distribution and characteristic function
exp/parenleftbigg
−|t|αEu∼Γ0/parenleftbig
|k/summationdisplay
j=1sj/angbracketleftu,xj/angbracketrightI(/angbracketleftu,xj/angbracketright>0)|α/parenrightbig/parenrightbigg
.
Thus, thedistributionof/summationtextk
j=1sj/angbracketleftw(0)
i,xj/angbracketrightI(/angbracketleftw(0)
i,xj/angbracketright>0)belongstothedomainofattractionofan α-stable
law. In particular, this implies that as m→+∞
rαP/parenleftbigg
|k/summationdisplay
j=1sj/angbracketleftw(0)
i,xj/angbracketrightI(/angbracketleftw(0)
i,xj/angbracketright>0)|>r/parenrightbigg
→CαEu∼Γ0/parenleftbigg
|k/summationdisplay
j=1sj/angbracketleftu,xj/angbracketrightI(/angbracketleftu,xj/angbracketright>0)|α/parenrightbigg
.
We now study the tail behaviour of/vextendsingle/vextendsinglewi/summationtextk
j=1sj/angbracketleftw(0)
i,xj/angbracketrightI(/angbracketleftw(0)
i,xj/angbracketright>0)/vextendsingle/vextendsingle. By Cline (1986, Section 5),
P/parenleftbigg
|wi| |k/summationdisplay
j=1sj/angbracketleftw(0)
i,xj/angbracketrightI(/angbracketleftw(0)
i,xj/angbracketright>0)|>et/parenrightbigg
=F∗G(t),
where
F(t) =P/parenleftbigg
|wi|>et/parenrightbigg
,G(t) =P/parenleftbigg
|k/summationdisplay
j=1sj/angbracketleftw(0)
i,xj/angbracketrightI(/angbracketleftw(0)
i,xj/angbracketright>0)|>et/parenrightbigg
.
We now prove that FandGsatisfy the assumptions of Cline (1986, Theorem 4) with β=γ= 0. The
distribution functions FandGhave exponential tails with rate α. Indeed, for all real u,
lim
t→∞F(t−u)
F(t)= lim
t→∞P(|wi|>et−u)
P(|wi|>et)=e−α(t−u)
e−αt=eαu.
Analogously for G. Moreover the functions b(t) =eαtF(t)andc(t) =eαtG(t)are regularly varying with
exponent zero: for all y>0,
lim
t→∞b(yt)
b(t)= lim
t→∞eαytP(|wi|>eyt)
eαtP(|wi|>et)= lim
t→∞eαyte−αyt
eαte−αt= 1 =y0.
The same property holds for c(t). By Cline (1986, Theorem 4 (v)), as t→∞,
P/parenleftbigg
|wi| |k/summationdisplay
j=1sj/angbracketleftw(0)
i,xj/angbracketrightI(/angbracketleftw(0)
i,xj/angbracketright>0)|>et/parenrightbigg
=F∗G(t)
∼C2
αEu∼Γ0/parenleftbig
|k/summationdisplay
j=1sj/angbracketleftu,xj/angbracketrightI(/angbracketleftu,xj/angbracketright>0)|α/parenrightbig
αte−αt,
15Published in Transactions on Machine Learning Research (11/2024)
ast→∞. Thus, for r→∞,
rαP/parenleftbigg
|wi| |k/summationdisplay
j=1sj/angbracketleftw(0)
i,xj/angbracketrightI(/angbracketleftw(0)
i,xj/angbracketright>0)|>r/parenrightbigg
∼C2
αEu∼Γ0/parenleftbig
|k/summationdisplay
j=1sj/angbracketleftu,xj/angbracketrightI(/angbracketleftu,xj/angbracketright>0)|α/parenrightbig
αlogr.
Let ˜L(r) =C2
αEu∼Γ0/parenleftbig
|/summationtextk
j=1sj/angbracketleftu,xj/angbracketrightI(/angbracketleftu,xj/angbracketright> 0)|α/parenrightbig
αlogr.Since the distribution of
wi/summationtextk
j=1sj/angbracketleftw(0)
i,xj/angbracketrightI(/angbracketleftw(0)
i,xj/angbracketright>0)is symmetric, then we can write that
1
amm/summationdisplay
i=1wik/summationdisplay
j=1sj/angbracketleftw(0)
i,xj/angbracketrightI(/angbracketleftw(0)
i,xj/angbracketright>0),
as a sequence of random variables in m, converges in distribution, as m→+∞, to a random variable with
symmetricα-stable law with scale 1provided (am)m≥1satisﬁes
m˜L(am)
aαm→Cα
asm→∞. The condition is satisﬁed if
am=
CαEu∼Γ0/parenleftbig
|k/summationdisplay
j=1sj/angbracketleftu,xj/angbracketrightI(/angbracketleftu,xj/angbracketright>0)|α/parenrightbig
mlogm
1/α
.
It follows that
1
(mlogm)1/αm/summationdisplay
i=1wik/summationdisplay
j=1sj/angbracketleftw(0)
i,xj/angbracketrightI(/angbracketleftw(0)
i,xj/angbracketright>0),
as a sequence of random variables in m, converges in distribution, as m→+∞, to a random variable with
symmetricα-stable distribution with scale of the form

CαEu∼Γ0/parenleftbig
|k/summationdisplay
j=1sj/angbracketleftu,xj/angbracketrightI(/angbracketleftu,xj/angbracketright>0)|α/parenrightbig
1/α
.
Since this holds for every vector s, then
1
(mlogm)1/αm/summationdisplay
i=1wi[/angbracketleftw(0)
i,xj/angbracketrightI(/angbracketleftw(0)
i,xj/angbracketright>0)]j,
as a sequence of random variables in m, converges in distribution, as m→+∞, to a random vector with
symmetricα-stable law with the spectral measure
ΓX=1
2CαEu∼Γ0/parenleftBigg
/bardbl[/angbracketleftu,xj/angbracketrightI(/angbracketleftu,xj/angbracketright>0)]j/bardblα
δ/parenleftbigg[/angbracketleftu,xj/angbracketrightI(/angbracketleftu,xj/angbracketright>0)]j
/bardbl[/angbracketleftu,xj/angbracketrightI(/angbracketleftu,xj/angbracketright>0)]j/bardbl/parenrightbigg
+δ/parenleftbigg
−[/angbracketleftu,xj/angbracketrightI(/angbracketleftu,xj/angbracketright>0)]j
/bardbl[/angbracketleftu,xj/angbracketrightI(/angbracketleftu,xj/angbracketright>0)]j/bardbl/parenrightbigg/parenrightBigg
.
Since Γ0=1
2/summationtextd
i=1(δ(ei) +δ(−ei)), whereeij= 1ifj=iand0otherwise, then
ΓX=Cα
4d/summationdisplay
i=1/parenleftbigg
/bardbl[xjiI(xji>0)]j/bardblα/parenleftbigg
δ/parenleftbig[xjiI(xji>0)]j
/bardbl[xjiI(xji>0)]j/bardbl/parenrightbig
+δ/parenleftbig
−[xjiI(xji>0)]j
/bardbl[xjiI(xji>0)]j/bardbl/parenrightbig/parenrightbigg
+/bardbl[xjiI(xji<0)]j/bardblα/parenleftbigg
δ/parenleftbig[xjiI(xji<0)]j
/bardbl[xjiI(xji<0)]j/bardbl/parenrightbig
+δ/parenleftbig
−[xjiI(xji<0)]j
/bardbl[xjiI(xji<0)]j/bardbl/parenrightbig/parenrightbigg/parenrightbigg
.
16Published in Transactions on Machine Learning Research (11/2024)
A.2 Proof of Theorem 3.1
To simplify the notation, we set in this section: w:=w(0),w(0):=w(0)(0),W:=W(0),˜H(1)
m:=
˜H(1)
m(W(0),X)and ˜H(2)
m:=˜H(2)
m(W(0),X), with ˜H(1)
m(W,X )and ˜H(2)
m(W,X )deﬁned in (12) and (13).
The proof of Theorem 3.1 is split into several steps.
Lemma A.1. Ifm→+∞then
˜H(1)
mw−→ ˜H∗
1(α),
where ˜H∗
1(α)is an (α/2)-Stable positive semi-deﬁnite random matrix with spectral measure
Γ∗
1=Cα/2/summationdisplay
u∈{0,1}kP(w(0)
i∈Bu)(/summationdisplay
j,j/prime/angbracketleftxj,xj/prime/angbracketright2ujuj/prime)α/4δ/parenleftBigg
[/angbracketleftxj,xj/prime/angbracketrightujuj/prime]j,j/prime
(/summationtext
j,j/prime/angbracketleftxj,xj/prime/angbracketright2ujuj/prime)1/2/parenrightBigg
,
where, for every u∈{0,1}k,Bu={v∈Rd:/angbracketleftv,xj/angbracketright>0ifuj= 1,/angbracketleftv,xj/angbracketright≤0ifuj= 0,j= 1,...,k}, and
Cα/2is the constant deﬁned in (4).
Proof.Since ˜H(1)
mis symmetric, is is suﬃcient to show that, for every k-dimensional vector s,
sT˜H(1)
msw→sT˜H∗
1(α)s.
We ﬁrst prove that the functions deﬁned, for t∈(−∞,+∞), byF(t) =P/parenleftbigg
w2
i>et/parenrightbigg
, and
G(t) =P/parenleftbiggk/summationdisplay
j,j/prime=1sjsj/prime/angbracketleftxj,xj/prime/angbracketrightI(/angbracketleftw(0)
i,xj/angbracketright>0)I(/angbracketleftw(0)
i,xj/prime/angbracketright>0)>et/parenrightbigg
=P/parenleftbigg
/bardblk/summationdisplay
j=1sjxjI(/angbracketleftw(0)
i,xj/angbracketright>0)/bardbl2>et/parenrightbigg
satisfy the assumptions of Cline (1986, Lemma 1). Indeed, Fhas exponentail tails with rate α/2, since by
the properties of the stable law,
lim
t→∞F(t−u)
F(t)= lim
t→∞P(|wi|>e(t−u)/2)
P(|wi|>et/2)=eαu/2.
Moreover, for any γ,
mG(γ) =/integraldisplay∞
0eγuG(du) =E/parenleftbig
/bardblk/summationdisplay
j=1sjxjI(/angbracketleftw(0)
i,xj/angbracketright>0)/bardblγ/parenrightbig
<∞.
By Cline (1986), Section 5 and Lemma 1, as t→∞,
P/parenleftbigg
w2
ik/summationdisplay
j,j/prime=1sjsj/prime/angbracketleftxj,xj/prime/angbracketrightI(/angbracketleftw(0)
i,xj/angbracketright>0)I(/angbracketleftw(0)
i,xj/prime/angbracketright>0)>et/parenrightbigg
=F∗G(t)∼mG(α/2)F(t)
∼Cα/2(et)−α/2E/parenleftbig/parenleftbigk/summationdisplay
j,j/prime=1sjsj/prime/angbracketleftxj,xj/prime/angbracketrightI(/angbracketleftw(0)
i,xj/angbracketright>0)I(/angbracketleftw(0)
i,xj/prime/angbracketright>0)/parenrightbigα/2/parenrightbig
.
By the properties of the stable law,
sT˜H(1)
ms=1
m2/αm/summationdisplay
i=1w2
ik/summationdisplay
j,j/prime=1sjsj/prime/angbracketleftxj,xj/prime/angbracketrightI(/angbracketleftw(0)
i,xj/angbracketright>0)I(/angbracketleftw(0)
i,xj/prime/angbracketright>0)
17Published in Transactions on Machine Learning Research (11/2024)
converges weakly, as m→∞, to a totally skewed to the right, α/2-stable random variable, with scale
parameter E/parenleftbig/vextendsingle/vextendsingle/summationtextk
j,j/prime=1sjsj/prime/angbracketleftxj,xj/prime/angbracketrightI(/angbracketleftw(0)
i,xj/angbracketright>0)I(/angbracketleftw(0)
i,xj/prime/angbracketright>0)/vextendsingle/vextendsingleα/2/parenrightbig2/α. Hence, for every t∈R, as
m→∞,
E/parenleftbigg
exp(itsT˜H(1)
ms)/parenrightbigg
→exp/parenleftbigg
−|t|α/2E/parenleftbig/vextendsingle/vextendsinglek/summationdisplay
j,j/prime=1sjsj/prime/angbracketleftxj,xj/prime/angbracketrightI(/angbracketleftw(0)
i,xj/angbracketright>0)I(/angbracketleftw(0)
i,xj/prime/angbracketright>0)/vextendsingle/vextendsingleα/2/parenrightbig/parenleftbig
1−isignutan(πα/4)/parenrightbigg/parenrightbigg
= exp/parenleftBigg
−/integraldisplay
Sk2−1/vextendsingle/vextendsingle/summationdisplay
j,j/primetsjsj/primevj,j/prime|α/2/parenleftbig
1−isign/parenleftbig
t/summationdisplay
j,j/primesjsj/primevj,j/prime/parenrightbig
tan(πα/4)Γ∗
1(dv)/parenrightBigg
,
where
Γ∗
1=Cα/2E/parenleftBigg
/bardbl[/angbracketleftxj,xj/prime/angbracketrightI(/angbracketleftw(0)
i,xj/angbracketright>0)I(/angbracketleftw(0)
i,xj/prime/angbracketright>0)]j,j/prime/bardblα/2
F
·δ/parenleftbigg[/angbracketleftxj,xj/prime/angbracketrightI(/angbracketleftw(0)
i,xj/angbracketright>0)I(/angbracketleftw(0)
i,xj/prime/angbracketright>0)]j,j/prime
/bardbl[/angbracketleftxj,xj/prime/angbracketrightI(/angbracketleftw(0)
i,xj/angbracketright>0)I(/angbracketleftw(0)
i,xj/prime/angbracketright>0)]j,j/prime/bardblF/parenrightbigg/parenrightBigg
.
It follows that, as m→+∞,
˜H(1)
mw−→ ˜H∗
1(α),
where ˜H∗
1(α)is an (α/2)-Stable random matrix with spectral measure Γ∗
1of the form
Γ∗
1 =Cα/2/summationdisplay
u∈{0,1}kP(w(0)
i∈Bu)(/summationdisplay
j,j/prime/angbracketleftxj,xj/prime/angbracketright2ujuj/prime)α/4δ/parenleftBigg
[/angbracketleftxj,xj/prime/angbracketrightujuj/prime]j,j/prime
(/summationtext
j,j/prime/angbracketleftxj,xj/prime/angbracketright2ujuj/prime)1/2/parenrightBigg
.
We will now prove that ˜H∗
1(α)is positive semi-deﬁnite. By deﬁnition, ˜H(1)
m(ω)is positive semi-deﬁnite for
everyωand everym. By Portmanteau Theorem, for every vector u∈Sk−1,
P/parenleftbig
uT˜H∗
1(α)u≥0/parenrightbig
≥lim sup
mP/parenleftBig
uT˜H(1)
mu≥0/parenrightBig
= 1.
LetAbe a countable dense subset of Sk−1. Then, with probability one, aT˜H∗
1(α)a≥0for everya∈A. By
continuity, this implies that the same property holds true with probability one for every u∈Sk−1, which
proves that ˜H∗
1(α)is almost surely positive semi-deﬁnite. By eventually modifying ˜H∗
1(α)on a null set, we
obtain a positive semi-deﬁnite random matrix.
Lemma A.2. Ifm→+∞then
˜H(2)
mw−→ ˜H∗
2(α),
where ˜H∗
2(α)is an (α/2)-Stable positive semi-deﬁnite random matrix with spectral measure
Γ∗
2=Cα/2/summationdisplay
u∈{0,1}k/summationdisplay
{i:{ei,−ei}∩Bu/negationslash=∅}(/summationdisplay
jx2
jiuj)α/2δ/parenleftBigg
[xjiujxj/primeiuj/prime]j,j/prime/summationtext
jx2
jiuj/parenrightBigg
,
whereBu={v∈Rd:/angbracketleftv,xj/angbracketright>0ifuj= 1,/angbracketleftv,xj/angbracketright≤0ifuj= 0,j= 1,...,k},eiis ad-dimensional vector
satisfyingeij= 1ifj=i, andeij= 0ifj/negationslash=i(i,j= 1,...,d ), andCα/2is the constant deﬁned in (4).
18Published in Transactions on Machine Learning Research (11/2024)
Proof.By the properties of the multivariate stable distribution (see Appendix B), it is suﬃcient to show
that
P
/bracketleftBig
/angbracketleftw(0)
1,xj/angbracketright/angbracketleftw(0)
1,xj/prime/angbracketrightI(/angbracketleftw(0)
1,xj/angbracketright>0)I(/angbracketleftw(0)
1,xj/prime/angbracketright>0)/bracketrightBig
j,j/prime
/bardbl/bracketleftBig
/angbracketleftw(0)
1,xj/angbracketright/angbracketleftw(0)
1,xj/prime/angbracketrightI(/angbracketleftw(0)
1,xj/angbracketright>0)I(/angbracketleftw(0)
1,xj/prime/angbracketright>0)/bracketrightBig
j,j/prime/bardblF∈·,
/bardbl/bracketleftBig
/angbracketleftw(0)
1,xj/angbracketright/angbracketleftw(0)
1,xj/prime/angbracketrightI(/angbracketleftw(0)
1,xj/angbracketright>0)I(/angbracketleftw(0)
1,xj/prime/angbracketright>0)/bracketrightBig
j,j/prime/bardblF>r/parenrightbigg
∼Cα/2r−α/2Γ∗
2(·),
asr→+∞. We can write that
P
/bracketleftBig
/angbracketleftw(0)
1,xj/angbracketright/angbracketleftw(0)
1,xj/prime/angbracketrightI(/angbracketleftw(0)
1,xj/angbracketright>0)I(/angbracketleftw(0)
1,xj/prime/angbracketright>0)/bracketrightBig
j,j/prime
/bardbl/bracketleftBig
/angbracketleftw(0)
1,xj/angbracketright/angbracketleftw(0)
1,xj/prime/angbracketrightI(/angbracketleftw(0)
1,xj/angbracketright>0)I(/angbracketleftw(0)
1,xj/prime/angbracketright>0)/bracketrightBig
j,j/prime/bardblF∈·,
/bardbl/bracketleftBig
/angbracketleftw(0)
1,xj/angbracketright/angbracketleftw(0)
1,xj/prime/angbracketrightI(/angbracketleftw(0)
1,xj/angbracketright>0)I(/angbracketleftw(0)
1,xj/prime/angbracketright>0)/bracketrightBig
j,j/prime/bardblF>r/parenrightbigg
=/summationdisplay
u∈{0,1}kP
/bracketleftBig
/angbracketleftw(0)
1,ujxj/angbracketright/angbracketleftw(0)
1,uj/primexj/prime/angbracketright/bracketrightBig
j,j/prime
/bardbl/bracketleftBig
/angbracketleftw(0)
1,ujxj/angbracketright/angbracketleftw(0)
1,uj/primexj/prime/angbracketright/bracketrightBig
j,j/prime/bardblF∈·,
/bardbl/bracketleftBig
/angbracketleftw(0)
1,ujxj/angbracketright/angbracketleftw(0)
1,uj/primexj/prime/angbracketright/bracketrightBig
j,j/prime/bardblF>r,w(0)
1∈Bu/parenrightbigg
.
For everyu∈{0,1}k, letXube thed×kmatrix, deﬁned as
Xu= [xjiuj]j=1,...,k,i =1,...,d.
Then we can write that
P
/bracketleftBig
/angbracketleftw(0)
1,ujxj/angbracketright/angbracketleftw(0)
1,uj/primexj/prime/angbracketright/bracketrightBig
j,j/prime
/bardbl/bracketleftBig
/angbracketleftw(0)
1,ujxj/angbracketright/angbracketleftw(0)
1,uj/primexj/prime/angbracketright/bracketrightBig
j,j/prime/bardblF∈·,
/bardbl/bracketleftBig
/angbracketleftw(0)
1,ujxj/angbracketright/angbracketleftw(0)
1,uj/primexj/prime/angbracketright/bracketrightBig
j,j/prime/bardblF>r,w(0)
1∈Bu/parenrightbigg
=P/parenleftBigg
XT
uw(0)
1(w(0)
1)TXu
(tr(XTu(w(0)
1)Tw(0)
1XuXTu(w(0)
1)Tw(0)
1Xu))1/2∈·,
tr(XT
u(w(0)
1)Tw(0)
1XuXT
u(w(0)
1)Tw(0)
1Xu)>r2,w(0)
1∈Bu/parenrightBig
=P/parenleftBigg
XT
u(w(0)
1)Tw(0)
1Xu
w(0)
1XuXTu(w(0)
1)T∈·,w(0)
1XuXT
u(w(0)
1)T>r,w(0)
1∈Bu/parenrightBigg
.
Noticethatthemaximumeigenvalueofthematrix XuXT
uissmallerthanorequalto k, sincethenormofeach
column ofXuis smaller than or equal to one. Then w(0)
1XuXT
u(w(0)
1)T>rimplies that/bardblw(0)
1/bardbl>(r/k)1/2.
We can therefore write that
P/parenleftBigg
XT
u(w(0)
1)Tw(0)
1Xu
w(0)
1XuXTu(w(0)
1)T∈·,w(0)
1XuXT
u(w(0)
1)T>r,w(0)
1∈Bu/parenrightBigg
=P/parenleftBigg
XT
u(w(0)
1)Tw(0)
1Xu
w(0)
1XuXTu(w(0)
1)T∈·,w(0)
1XuXT
u(w(0)
1)T>r,/bardblw(0)
1/bardbl>(r/k)1/2,w(0)
1∈Bu/parenrightBigg
.
19Published in Transactions on Machine Learning Research (11/2024)
SinceBuis a cone and the spectral measure of w(0)
1is given by/summationtext
i(δ(ei) +δ(−ei)), by the properties of the
multivariate stable distribution, we can write that
P/parenleftBigg
XT
u(w(0)
1)Tw(0)
1Xu
w(0)
1XuXTu(w(0)
1)T∈·,w(0)
1XuXT
u(w(0)
1)T>r,/bardblw(0)
1/bardbl>(r/k)1/2,w(0)
1∈Bu/parenrightBigg
∼Cα/2r−α/2/summationdisplay
{i:{e1,−ei}∩Bu/negationslash=∅}(k/summationdisplay
j=1x2
jiuj)α/2δ/parenleftBigg
[xjixj/primeiujuj/prime]j,j/prime/summationtext
jx2
jiuj/parenrightBigg
,
asr→+∞. The proof that ˜H∗
2(α)is positive semi-deﬁnite can be done by following the same line of
reasoning as in the proof of Lemma A.1.
Lemma A.3. Asm→+∞, the probability distribution of (˜H(1)
m,˜H(1)
m)converges weakly to the law of
independent stable random matrices, with spectral measures Γ∗
1andΓ∗
2as in (14) and (15), respectively.
Proof.Since ˜H(1)
mandH(2)
mconverge marginally to α/2-stable random matrices, by the properties of the
multivariate stable distributions it is suﬃcient to show that they converge to stochastically independent
random matrices. By Theorem B.1, we know that
nP/parenleftBigg
/bardbl[w2
i/angbracketleftxj,xj/prime/angbracketrightI(/angbracketleftw(0)
i,xj/angbracketright>0)I(/angbracketleftw(0)
i,xj/prime/angbracketright>0)]j,j/prime/bardblF>n2/α,
/bardbl[/angbracketleftxj,w(0)
i/angbracketright/angbracketleftxj/prime,w(0)
i/angbracketrightI(/angbracketleftw(0)
i,xj/angbracketright>0)I(/angbracketleftw(0)
i,xj/prime/angbracketright>0)]j,j/prime/bardblF>n2/α/parenrightBigg
and
nP/parenleftBigg
/bardbl[w2
i/angbracketleftxj,xj/prime/angbracketrightI(/angbracketleftw(0)
i,xj/angbracketright>0)I(/angbracketleftw(0)
i,xj/prime/angbracketright>0)]j,j/prime/bardblF>n2/α/parenrightBigg
converge to ﬁnite limits, as n→∞. Hence, again by Theorem B.1, it is suﬃcient to show that
lim
n→∞nP/parenleftBigg
/bardbl[w2
i/angbracketleftxj,xj/prime/angbracketrightI(/angbracketleftw(0)
i,xj/angbracketright>0)I(/angbracketleftw(0)
i,xj/prime/angbracketright>0)]j,j/prime/bardblF>n2/α,
/bardbl[/angbracketleftxj,w(0)
i/angbracketright/angbracketleftxj/prime,w(0)
i/angbracketrightI(/angbracketleftw(0)
i,xj/angbracketright>0)I(/angbracketleftw(0)
i,xj/prime/angbracketright>0)]j,j/prime/bardblF>n2/α/parenrightBigg
= 0,
which ensures that the Lévy measure of the limit inﬁnitely divisible distribution of (˜H(1)
m,˜H(2)
m)is the sum of
a measureν1concentrated on the space spanned by the ﬁrst k2coordinates and a measure ν2on the space
20Published in Transactions on Machine Learning Research (11/2024)
spanned by the last k2coordinates. We can write that
nP/parenleftBigg
/bardbl[w2
i/angbracketleftxj,xj/prime/angbracketrightI(/angbracketleftw(0)
i,xj/angbracketright>0)I(/angbracketleftw(0)
i,xj/prime/angbracketright>0)]j,j/prime/bardblF>n2/α,
/bardbl[/angbracketleftxj,w(0)
i/angbracketright/angbracketleftxj/prime,w(0)
i/angbracketrightI(/angbracketleftw(0)
i,xj/angbracketright>0)I(/angbracketleftw(0)
i,xj/prime/angbracketright>0)]j,j/prime/bardblF>n2/α/parenrightBigg
=n/summationdisplay
u∈{0,1}kP(w(0)
i∈Bu)
P/parenleftbigg
/bardbl[w2
i/angbracketleftxj,xj/prime/angbracketrightujuj/prime]j,j/prime/bardblF>n2/α,/bardbl[/angbracketleftxj,w(0)
i/angbracketright/angbracketleftxj/prime,w(0)
i/angbracketrightujuj/prime]j,j/prime/bardblF>n2/α|w(0)
i∈Bu/parenrightbigg
=n/summationdisplay
u∈{0,1}kP(w(0)
i∈Bu)P/parenleftbigg
/bardbl[/angbracketleftxj,w(0)
i/angbracketright/angbracketleftxj/prime,w(0)
i/angbracketrightujuj/prime]j,j/prime/bardblF>n2/α|w(0)
i∈Bu/parenrightbigg
P/parenleftbigg
/bardbl[w2
i/angbracketleftxj,xj/prime/angbracketrightujuj/prime]j,j/prime/bardblF>n2/α/parenrightbigg
=/summationdisplay
u∈{0,1}knP/parenleftbigg
/bardbl[/angbracketleftxj,w(0)
i/angbracketright/angbracketleftxj/prime,w(0)
i/angbracketrightujuj/prime]j,j/prime/bardblF>n2/α,w(0)
i∈Bu/parenrightbigg
P/parenleftbigg
/bardbl[w2
i/angbracketleftxj,xj/prime/angbracketrightujuj/prime]j,j/prime/bardblF>n2/α/parenrightbigg
→0,
asn→∞.
Now, we are in the position of proving Theorem 3.1. By Lemma A.1, Lemma A.1, Lemma A.3, and the
properties of stable distributions, ˜Hm(W(0),X)converges in distribution to a positive semi-deﬁnite random
matrix, with (α/2)-stable distribution, and spectral measure Γ∗
1+ Γ∗
2. This completes the proof of Theorem
3.1.
A.3 Proof of Proposition 3.1
To simplify the notation, we set in this section: w:=w(0),w(0):=w(0)(0),W:=W(0),˜H(1)
m:=
˜H(1)
m(W(0),X)and ˜H(2)
m:=˜H(2)
m(W(0),X), with ˜H(1)
m(W,X )and ˜H(2)
m(W,X )deﬁned in (12) and (13).
From (11), ˜Hm(W(0),X))is the sum of two positive semi-deﬁnite random matrices, ˜H(1)
mand ˜H(2)
m. The
following results show that for every δ>0, there exist λ1>0andλ2>0such that, for msuﬃciently large,
with probability at least 1−δ
λmin(˜H(i)
m)>λi.
with the large-width behaviour of ˜H(i)
mbeing characterized in Lemma A.1 and Lemma A.2, through an
(α/2)-Stable limiting random matrix ˜H∗
i(α)with spectral measure Γ∗
iof the form (14) and (15). To prove
that the minumum eigenvales of ˜H(1)
mand ˜H(2)
mare bounded away from zero, we ﬁrst need to inspect the
characteristics of the distributions of ˜H∗
1(α)and of ˜H∗
2(α). This is the content of Lemma A.4 and of Lemma
A.6. Then, the results concerning the minumum eigenvalues of ˜H(1)
mand ˜H(2)
mare given in Lemma A.5 and
Lemma A.7.
Lemma A.4. Under the assumptions of Theorem 3.2, the distribution of the random matrix ˜H∗
1(α)is
absolutely continuous in the subspace of the symmetric positive semi-deﬁnite matrices with zero entries in
the positions (j,j/prime)such that/angbracketleftxj,xj/prime/angbracketright= 0, withj,j/prime∈{1,...,k}, with the topology of Frobenius norm.
Proof.From Nolan (2010), it is suﬃcient to show that
inf
s∈Sk2−1
0/integraldisplay
|/angbracketlefts,u/angbracketright|α/2Γ∗
1(du)/negationslash= 0,
21Published in Transactions on Machine Learning Research (11/2024)
where Γ∗
1is the spectral measure (14), Sk2−1
0is the unit sphere in the space of the k×ksymmetric matrices
such thatsj,j/prime= 0if/angbracketleftxj,xj/prime/angbracketright= 0, with the Frobenius metric. Now, since
/integraldisplay
|/angbracketlefts,u/angbracketright|α/2Γ∗
1(du)
=Cα/2E
|/summationdisplay
j,j/primesj,j/prime/angbracketleftxj,xj/prime/angbracketrightI(/angbracketleftw(0)
i,xj/angbracketright>0)I(/angbracketleftw(0)
i,xj/prime/angbracketright>0)|α/2

is a continuous function of sthat takes value in a compact set, then the minimum is attained. Thus it is
suﬃcient to show that for every s∈Sk2−1
0,
E
|/summationdisplay
j,j/primesj,j/prime/angbracketleftxj,xj/prime/angbracketrightI/angbracketleftw(0)
i,xj/angbracketright>0)I(/angbracketleftw(0)
i,xj/prime/angbracketright>0)|α/2
/negationslash= 0.
For everyjand everyuj∈{0,1}, letAuj
jbe the event (/angbracketleftw(0)
i,xj/angbracketright>0)ifuj= 1and its complement if
uj= 0. Then
E
|/summationdisplay
j,j/primesj,j/prime/angbracketleftxj,xj/prime/angbracketrightI(/angbracketleftw(0)
i,xj/angbracketright>0)I(/angbracketleftw(0)
i,xj/prime/angbracketright>0)|α/2

=/summationdisplay
u1,...,u kP(Au1
1∩···∩Auk
k)|/summationdisplay
j,j/primeujuj/primesj,j/prime/angbracketleftxj,xj/prime/angbracketright|α/2.
Sincex1,...,xkare linearly independent, then for every u1,...,uk,P(Au1
1∩...,Auk
k)>0. To prove it,
assume, without loss of generality, that ui= 1for everyi. Sincex1,...,xkare linearly independent, then we
can complete the matrix X= [x1... xk]by addingk−dcolumns in such a way that the completed matrix
˜Xis non-singular. For every d-dimensional vector vsuch thatv1>0,...,vk>0there exists a vector usuch
thatu= (˜XT)−1v. Thus,
{u∈Rd:/angbracketleftu,x1/angbracketright>0,...,/angbracketleftu,xk/angbracketright>0}={(˜XT)−1v:v1>0,...,vk>0}
is an open non-empty set. Since w(0)
ihas independent and identically distributed components, with stable
distribution, then
P/parenleftBig
w(0)
i∈{(˜X)−1v:v1>0,...,vk>0}/parenrightBig
>0.
This concludes the proof that P(Au1
1∩...,Auk
k)>0for every (u1,...,uk)∈ {0,1}k}. It follows that/integraltext
|/angbracketlefts,u/angbracketright|α/2Γ∗
1(du)is zero if and only if, for every (u1,...,uk)∈{0,1}k, it holds
/summationdisplay
j,j/primeuj,uj/prime/angbracketleftxj,xj/prime/angbracketrightsj,j/prime= 0.
The only solution of the above system of equations in the space of symmetric matrices ssuch thatsj,j/prime= 0
if/angbracketleftxj,xj/prime/angbracketright= 0iss= 0, which is not consistent with /bardbls/bardblF= 1.
We observe that the space of the symmetric positive semi-deﬁnite matrices with zeros in the entries (j,j/prime)
such that/angbracketleftxj,xj/prime/angbracketright= 0contains all the matrices with non-zero diagonal element since /angbracketleftxj,xj/angbracketright= 1/negationslash= 0for
every index j.
Lemma A.5. Under the assumptions of Theorem 3.2, for every δ >0there exists λ1>0such that with
probability at least 1−δ
λmin(˜H∗
1(α))>λ1.
22Published in Transactions on Machine Learning Research (11/2024)
Proof.Since the distribution of ˜H∗
1(α)is absolutely continuous in the space of symmetric positive semi-
deﬁnite matrices with zero entries in the positions j,j/primesuch that/angbracketleftx,xj/prime/angbracketright= 0, and since this space contains
all the symmetric positive semi-deﬁnite matrices with non-zero diagonal entries, then we can write that
P(det( ˜H∗
1(α)) = 0) = 0 . Moreover, since ˜H∗
1(α)is positive semi-deﬁnite, then P(λmin(˜H∗
1(α))>0) = 1.
Thus, for every δ>0, the exists λ1>0such that P(λmin(˜H∗
1(α))>λ1)>1−δ.
Lemma A.6. Under the assumptions of Theorem 3.2, the distribution of the random matrix ˜H∗
2(α)is
absolutely continuous in the subspace of the symmetric positive semi-deﬁnite matrices, with the topology of
Frobenius norm.
Proof.From Nolan (2010), it is suﬃcient to show that
inf
s∈Sk2−1/integraldisplay
|/angbracketlefts,u/angbracketright|α/2Γ∗
2(du)/negationslash= 0,
where Γ∗
2is the spectral measure (15), Sk2−1is the unit sphere in the space of the k×ksymmetric positive
semi-deﬁnite matrices, with the Frobenius norm. For every u∈{0,1}k, letBu={v∈Rd:/angbracketleftv,xj/angbracketright>0ifuj=
1,/angbracketleftv,xj/angbracketright≤0ifuj= 0}. Moreover, for every i= 1,...,k, leteibe ad-dimensional random vector satisfying
eij= 1forj=iandeij= 0forj/negationslash=i. Finally, let Cα/2be the constant deﬁned in (4). Then
/integraldisplay
|/angbracketlefts,u/angbracketright|α/2Γ∗
2(du) =Cα/2|/summationdisplay
j,j/primesj,j/prime/summationdisplay
u∈{0,1}k/summationdisplay
{i:{ei,−ei}∩Bu/negationslash=∅}xjiujxj/primeiuj/prime|α/2.
Since/summationtext
j,j/primesj,j/prime/summationtext
u∈U/summationtext
Ezu,ixjiujxj/primeiuj/primeis continuous as a function of sandstakes values in a compact
set, then the minimum is attained. Thus it is suﬃcient to show that for every s∈Sk2−1,
/summationdisplay
u∈{0,1}k/summationdisplay
{i:{ei,−ei}∩Bu/negationslash=∅}/summationdisplay
j,j/primesj,j/primexjiujxj/primeiuj/prime/negationslash= 0.
Since/bardbls/bardblF= 1, thensis not the null matrix. Hence there exist c>0, a vectorawith/bardbla/bardbl= 1and a positive
semi-deﬁnite, symmetric matrix s/primesuch that
s=caaT+s/prime.
SinceBu∩Bu/prime=∅, whenu/negationslash=u/prime, then, for every i= 1,...,dandj= 1,...,k, there exists one and only one
u∈{0,1}ksuch thatuj= 1and{ei,−ei}∩Bu/negationslash=∅. Then we can write that
/summationdisplay
u∈{0,1}k/summationdisplay
{i:{ei,−ei}∩Bu/negationslash=∅}/summationdisplay
j,j/primesj,j/primexjiujxj/primeiuj/prime
≥c/summationdisplay
u∈{0,1}k/summationdisplay
{i:{ei,−ei}∩Bu/negationslash=∅}(/summationdisplay
jajxjiuj)2
=d/summationdisplay
i=1
(k/summationdisplay
j=1ajxji)2/summationdisplay
{u:{ei,−ei}∩Bu/negationslash=∅}uj

=d/summationdisplay
i=1(k/summationdisplay
j=1ajxji)2,
which is strictly positive, since the xjare linearly independent, and /bardbla/bardbl= 1. This concludes the proof.
Lemma A.7. Under the assumptions of Theorem 3.2, for every δ >0there exists λ2>0such that with
probability at least 1−δ
λmin(˜H∗
2(α))>λ2.
23Published in Transactions on Machine Learning Research (11/2024)
Proof.Since the distribution of ˜H∗
2(α)is absolutely continuous in the space of symmetric positive semi-
deﬁnite matrices then we can write that P(det( ˜H∗
2(α)) = 0) = 0 . Moreover, since ˜H∗
2(α)is positive semi-
deﬁnite, then P(λmin(˜H∗
2(α))>0) = 1. Thus, for every δ>0, the exists λ2>0such that P(λmin(˜H∗
2(α))>
λ2)>1−δ.
Now, we are in the position of proving Proposition 3.1. Let δ > 0be a ﬁxed number. By Lemmas A.5
and A.7, there exist λ1>0andλ2>0such that, for i= 1,2,P(λmin(˜H∗
i(α))> λi)≥1−δ/2. Since the
minimum eigenvalue map is continuous with respect to Frobenius norm then, by Portmanteau theorem, for
i= 1,2,
lim inf
mP(λmin(˜H(i)
m(W(0),X))>λi)≥P(λmin(˜H∗
i(α))>λi)≥1−δ/2.
Letλ0=λ1+λ2. Since the minimum eigenvalue of a sum of symmetric, positive semi-deﬁnite matrices
is greater than or equal to the sum of the eigenvalues of the two matrices (see Horn and Johnson (1985)
Theorem 4.3.1), then we can write that
lim inf
mP(λmin(˜Hm(W(0),X))>λ0)
≥lim inf
mP(λmin(˜H(1)
m(W(0),X)) +λmin(˜H(2)
m(W(0),X))>λ0)
≥lim inf
mP(∩i=1,2(λmin(˜H(i)
m(W(0),X))>λi))
≥1−lim sup
m/parenleftBigg2/summationdisplay
i=1P(λmin(˜H(i)
m(W(0),X))≤λi)/parenrightBigg
≥1−δ,
thus completing the proof of Proposition 3.1.
A.4 Proof of Proposition 3.2
Before proving Proposition 3.2, we give some preliminary results.
Lemma A.8. Letγ∈(0,1)andc>0be ﬁxed numbers. For every δ>0the following property holds true,
formsuﬃciently large, with probability at least 1−δ:
(logm)2/α/vextenddouble/vextenddouble/vextenddouble/vextenddouble∂˜fm
∂w(W,xj;α)−∂˜fm
∂w(W(0),xj;α)/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
F<cm−2γ/α,
for everyWsuch that||W−W(0)||F≤(logm)2/αand every NN’s input xj, withj= 1,...,k.
Proof.For a ﬁxed W(0), letWbe such that/bardblW−W(0)/bardblF≤(logm)2/α. Then it holds/bardblw(0)−w(0)(0)/bardbl2
F≤
/bardblW−W(0)/bardbl2
F≤(logm)4/α. Accordingly, we can write the following
(logm)2/α/vextenddouble/vextenddouble/vextenddouble/vextenddouble∂˜fm
∂w(W,xj;α)−∂˜fm
∂w(W(0),xj;α)/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
F
≤1
m2/αm/summationdisplay
i=1/parenleftBig
/angbracketleftw(0)
i,xj/angbracketrightI(/angbracketleftw(0)
i,xj/angbracketright>0)−/angbracketleftw(0)
i(0),xj/angbracketrightI(/angbracketleftw(0)
i(0),xj/angbracketright>0)/parenrightBig2
≤2
m2/αm/summationdisplay
i=1/parenleftBig
/angbracketleftw(0)
i,xj/angbracketright−/angbracketleftw(0)
i(0),xj/angbracketright/parenrightBig2
I(/angbracketleftw(0)
i,xj/angbracketright>0)
+2
m2/αm/summationdisplay
i=1/angbracketleftw(0)
i(0),xj/angbracketright2/parenleftBig
I(/angbracketleftw(0)
i,xj/angbracketright>0)−I(/angbracketleftw(0)
i(0),xj/angbracketright>0)/parenrightBig2
.
24Published in Transactions on Machine Learning Research (11/2024)
Wewillboundthetwotermsofthesumseparately. First, wedeﬁne ri=|/angbracketleftw(0)
i−w(0)
i(0),xj/angbracketright|fori= 1,...,m.
Then, we can write that
m/summationdisplay
i=1r2
i≤m/summationdisplay
i=1/bardblw(0)
i−w(0)
i(0)/bardbl2·/bardblxj/bardbl2≤/bardblw(0)−w(0)(0)/bardbl2
F≤(logm)4/α.
Sinceγ <1,
2
m2/αm/summationdisplay
i=1/parenleftBig
/angbracketleftw(0)
i,xj/angbracketright−/angbracketleftw(0)
i(0),xj/angbracketright/parenrightBig2
I(/angbracketleftw(0)
i,xj/angbracketright>0)
≤2m−2/α(logm)4/α<c
4m−2γ/α,
formsuﬃciently large. In order to bound the second term, we observe that the following set
{w(0)(0) :∃w(0)s.t.|/angbracketleftw(0)
i−w(0)
i(0),xj/angbracketright|=ri, I(/angbracketleftw(0),xj/angbracketright>0)/negationslash=I(/angbracketleftw(0)(0),xj/angbracketright>0)}
is included in the set {w(0)
i(0) :|/angbracketleftw(0)
i(0),xj/angbracketright|≤ri}.Therefore, we can write that
sup/summationtext
ir2
i≤logmsup
|w(0)
i−w(0)
i(0)|≤ri2
m2/αm/summationdisplay
i=1/angbracketleftw(0)
i(0),xj/angbracketright2/parenleftBig
I(/angbracketleftw(0)
i,xj/angbracketright>0)−I(/angbracketleftw(0)
i(0),xj/angbracketright>0)/parenrightBig2
≤ sup/summationtext
ir2
i≤logmsup
|w(0)
i−w(0)
i(0)|≤ri2
m2/αm/summationdisplay
i=1/angbracketleftw(0)
i(0),xj/angbracketright2I(/angbracketleftw(0)
i(0),xj/angbracketright<ri)
≤ sup/summationtext
ir2
i≤logmsup
|w(0)
i−w(0)
i(0)|≤ri2
m2/αm/summationdisplay
i=1r2
i
≤1
m2/α(logm)4/α<c
4m−2γ/α,
formsuﬃciently large.
Lemma A.9. For everyδ >0there exist λ > 0such that the following two properties hold true, for m
suﬃciently large, with a probability at least 1−δ:
i)
/bardbl˜H(2)
m(W,X )−˜H(2)
m(W(0),X)/bardblF<λm−γ/α;
ii)
λmin(˜Hm(W,X ))>λ
2;
for everyWsuch that/bardblW−W(0)/bardblF≤(logm)2/α.
Proof.By Lemma A.7, for every δ>0there exists λsuch that
λmin(˜H∗
2(α))>λ
25Published in Transactions on Machine Learning Research (11/2024)
with probability at least 1−δ/2. For every vector W, we can write that
|˜H(2)
m(W,X )[i,j]−˜H(2)
m(W(0),X)[i,j]|
= (logm)2/α/vextendsingle/vextendsingle/vextendsingle/vextendsingle/angbracketleftbigg∂˜fm
∂w(W,xi;α),∂˜fm
∂w(W,xj;α)/angbracketrightbigg
−/angbracketleftbigg∂˜fm
∂w(W(0),xi;α),∂˜fm
∂w(W(0),xj;α)/angbracketrightbigg/vextendsingle/vextendsingle/vextendsingle/vextendsingle
≤(logm)2/α/vextenddouble/vextenddouble/vextenddouble/vextenddouble∂˜fm
∂w(W,xi;α)/vextenddouble/vextenddouble/vextenddouble/vextenddouble
F/vextenddouble/vextenddouble/vextenddouble/vextenddouble∂˜fm
∂w(W,xj;α)−∂˜fm
∂w(W(0),xj;α)/vextenddouble/vextenddouble/vextenddouble/vextenddouble
F
+ (logm)2/α/vextenddouble/vextenddouble/vextenddouble/vextenddouble∂˜fm
∂w(W(0),xj;α)/vextenddouble/vextenddouble/vextenddouble/vextenddouble
F/vextenddouble/vextenddouble/vextenddouble/vextenddouble∂˜fm
∂w(W,xi;α)−∂˜fm
∂w(W(0),xi;α)/vextenddouble/vextenddouble/vextenddouble/vextenddouble
F
≤(logm)2/α/parenleftbigg/vextenddouble/vextenddouble/vextenddouble/vextenddouble∂˜fm
∂w(W(0),xi;α)/vextenddouble/vextenddouble/vextenddouble/vextenddouble
F+/vextenddouble/vextenddouble/vextenddouble/vextenddouble∂˜fm
∂w(W(0),xi;α)−∂˜fm
∂w(W,xi;α)/vextenddouble/vextenddouble/vextenddouble/vextenddouble
F/parenrightbigg
×/vextenddouble/vextenddouble/vextenddouble/vextenddouble∂˜fm
∂w(W,xj;α)−∂˜fm
∂w(W(0),xj;α)/vextenddouble/vextenddouble/vextenddouble/vextenddouble
F
+ (logm)2/α/vextenddouble/vextenddouble/vextenddouble/vextenddouble∂˜fm
∂w(W(0),xj;α)/vextenddouble/vextenddouble/vextenddouble/vextenddouble
F/vextenddouble/vextenddouble/vextenddouble/vextenddouble∂˜fm
∂w(W,xi;α)−∂˜fm
∂w(W(0),xi;α)/vextenddouble/vextenddouble/vextenddouble/vextenddouble
F.
For everyi= 1,...,k,
(logm)2/α/vextenddouble/vextenddouble/vextenddouble/vextenddouble∂˜fm
∂w(W(0),xi;α)/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
F=1
m2/αm/summationdisplay
i=1/angbracketleftw(0)
i(0),xi/angbracketright2I(|/angbracketleftw(0)
i(0),xi/angbracketright|>0)
≤1
m2/αm/summationdisplay
i=1/angbracketleftw(0)
i(0),xi/angbracketright2,
which converges in distribution, as m→∞. Thus there exist M > 0andm0such that for every m≥m0
and everyi= 1,...,k,
P/parenleftbigg
(logm)1/α/vextenddouble/vextenddouble/vextenddouble/vextenddouble∂˜fm
∂w(W(0),xi;α)/vextenddouble/vextenddouble/vextenddouble/vextenddouble
F>M/parenrightbigg
<δ
8k2.
By Lemma A.8, for msuﬃciently large, with probability at least 1−δ/(4k2)
(logm)1/α/parenleftbigg/vextenddouble/vextenddouble/vextenddouble/vextenddouble∂˜fm
∂w(W(0),xi;α)/vextenddouble/vextenddouble/vextenddouble/vextenddouble
F+/vextenddouble/vextenddouble/vextenddouble/vextenddouble∂˜fm
∂w(W(0),xi;α)−∂˜fm
∂w(W,xi;α)/vextenddouble/vextenddouble/vextenddouble/vextenddouble
F/parenrightbigg
<2M
whenever/bardblW−W(0)/bardblF<(logm)2/α. Lemma A.8 also implies that, for every γ∈(0,1), andi= 1,...,k,
with probability at least 1−δ/(8k2)
(logm)1/α/vextenddouble/vextenddouble/vextenddouble/vextenddouble∂˜fm
∂w(W,xi;α)−∂˜fm
∂w(W(0),xi;α)/vextenddouble/vextenddouble/vextenddouble/vextenddouble
F<λ
4Mk2m−γ/α
whenever/bardblW−W(0)/bardbl2
F<(logm)4/α, providedmis suﬃciently large,. Thus, with probability at least 1−δ,
ifmis suﬃciently large
max
i,j|˜H(2)
m(W,X )[i,j]−˜H(2)
m(W(0),X)[i,j]|<λ
k2m−γ/α,
whenever/bardblW−W(0)/bardblF<(logm)2/α. Thus
/bardbl˜H(2)
m(W,X )−˜H(2)
m(W(0),X)/bardbl2
≤/bardbl˜H(2)
m(W,X )−˜H(2)
m(W(0),X)/bardblF<λm−γ/α<λ
2,
whenever/bardblW−W(0)/bardblF<(logm)2/α, providedmis suﬃciently large. The last inequality and Lemma A.6
imply that, with probability at least 1−δ, ifmis suﬃciently large, then
/bardbl˜H(2)
m(W,X )/bardbl2>λ/ 2,
26Published in Transactions on Machine Learning Research (11/2024)
for everyWsuch that/bardblW−W(0)/bardblF<(logm)2/α. Since ˜Hm(W,X )is the sum of two positive semi-deﬁnite
matrices ˜H(1)
m(W,X )and ˜H(2)
m(W,X ), then
/bardbl˜Hm(W,X )/bardbl2≥/bardbl˜H(2)
m(W,X )/bardbl2>λ/ 2,
for everyWsuch that/bardblW−W(0)/bardblF<(logm)2/α, ifmis suﬃciently large.
Lemma A.10. For everyδ >0the following property holds true, for msuﬃciently large, with probabillity
at least 1−δ: there exists M > 0such that
(logm)1/α/vextenddouble/vextenddouble/vextenddouble/vextenddouble∂˜fm
∂w(0)(W,xj;α)−∂˜fm
∂w(0)(W(0),xj;α)/vextenddouble/vextenddouble/vextenddouble/vextenddouble
F<M,
for everyj= 1,...,k, and for every Wsuch that/bardblW−W(0)/bardblF≤(logm)2/α.
Proof.Let us deﬁne ri=|/angbracketleftw(0)
i−w(0)
i(0),xj/angbracketright|fori= 1,...,m. Now, since/bardblxj/bardbl= 1by assumption, for
j= 1,...,k, then we can write
/summationdisplay
ir2
i≤/bardblxj/bardbl2·/bardblw(0)
i−w(0)(0)/bardbl2
F≤/bardblW−W(0)/bardbl2
F≤(logm)4/α.
It holds
(logm)2/α/vextenddouble/vextenddouble/vextenddouble/vextenddouble∂˜fm
∂w(0)(W,xj;α)−∂˜fm
∂w(0)(W(0),xj;α)/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
F
≤1
m2/αm/summationdisplay
i=1/parenleftBig
wiI(/angbracketleftw(0)
i,xj/angbracketright>0)−wi(0)I(/angbracketleftw(0)
i(0),xj/angbracketright>0)/parenrightBig2
≤2
m2/αm/summationdisplay
i=1(wi−wi(0))2I(/angbracketleftw(0)
i,xj/angbracketright>0)
+2
m2/αm/summationdisplay
i=1wi(0)2|I(/angbracketleftw(0)
i,xj/angbracketright>0)−I(/angbracketleftw(0)
i(0),xj/angbracketright>0)|.
We will bound the two terms separately. First,
2
m2/αm/summationdisplay
i=1(wi−wi(0))2I(/angbracketleftw(0)
i,xj/angbracketright>0)
≤1
m2/αm/summationdisplay
i=1(wi−wi(0))2
≤2
m2(1−γ)/α/bardblw−w(0)/bardbl2
F
≤2
m2/α(logm)4/α<c
4m−2γ/α,
ifmis suﬃciently large. To bound the second term, we can write that
2
m2/αm/summationdisplay
i=1wi(0)2|I(/angbracketleftw(0)
i,xj/angbracketright>0)−I(/angbracketleftw(0)
i(0),xj/angbracketright>0)|
≤2
m2/αm/summationdisplay
i=1wi(0)2,
which converges in distribution to a stable random variable, as m→∞. Hence there exists M1such that,
with probability at least 1−δ/4,
2
m2/αm/summationdisplay
i=1(wi−wi(0))2I(/angbracketleftw(0)
i,xj/angbracketright>0)<M2
1
2k2
27Published in Transactions on Machine Learning Research (11/2024)
and
2
m2/αm/summationdisplay
i=1wi(0)2|I(/angbracketleftw(0)
i,xj/angbracketright>0)−I(/angbracketleftw(0)
i(0),xj/angbracketright>0)|<M2
1
2k2,
formsuﬃciently large, which entail
(logm)1/α/vextenddouble/vextenddouble/vextenddouble/vextenddouble∂˜fm
∂w(0)(W,xj;α)−∂˜fm
∂w(0)(W(0)(ω),xj;α)/vextenddouble/vextenddouble/vextenddouble/vextenddouble
F<M1
k.
On the other hand, there exist N3∈FandM2withP(N3)>1−δ/4such that, for every ω∈N3and for
msuﬃciently large,
/bardbl˜fm(W(0)(ω),X;α)−Y/bardblF<M 2,
and
max
1≤i≤k/vextenddouble/vextenddouble/vextenddouble/vextenddouble∂
∂W˜fm(W(0)(ω),xi;α)/vextenddouble/vextenddouble/vextenddouble/vextenddouble
F<M 2(logm)−1/α.
The above inequalities follow from the convergence in distribution of ˜fm(W(0),xi;α)and of
(logm)2/α/vextenddouble/vextenddouble/vextenddouble/vextenddouble∂
∂W˜fm(W(0),xi;α)/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
F=˜H(W(0),X;α)[i,i] (i= 1,...,k ),
asm→∞.
Lemma A.11. Letγ∈(0,1)andc>0be ﬁxed numbers. For every δ>0the following property holds true,
formsuﬃciently large, with probability at least 1−δ:
/bardblW(t)−W(0)/bardblF<(logm)2/α.
if
(logm)2/α/vextenddouble/vextenddouble/vextenddouble/vextenddouble∂˜fm
∂w(W(s),xj;α)−∂˜fm
∂w(W(0),xj;α)/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
F≤cm−2γ/α
for every NN’s input xj, withj= 1,...,k, and for every s≤t.
Proof.By Lemmas A.8 and A.9, there exists N1∈Fwith probability at least 1−δ/2such that, for every
ω∈N1,
(logm)2/α/vextenddouble/vextenddouble/vextenddouble/vextenddouble∂˜fm
∂w(W,xj;α)−∂˜fm
∂w(W(0)(ω),xj;α)/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
F<cm−2γ/α,
for arbitrarily ﬁxed c>andγ∈(0,1/2), and
λmin(˜Hm(W,X ))>λ
2,
for someλ>0, for everyWsuch that/bardblW−W(0)(ω)/bardblF≤(logm)2/αand everyj= 1,...,k, providedm
is suﬃciently large. Moreover, by Lemma A.10, there exist, for msuﬃciently large, M1>0andN2with
P(N2)>1−δ, such that
(logm)1/α/vextenddouble/vextenddouble/vextenddouble/vextenddouble∂˜fm
∂w(0)(W,xj;α)−∂˜fm
∂w(0)(W(0)(ω),xj;α)/vextenddouble/vextenddouble/vextenddouble/vextenddouble
F<M1
k,
for everyj= 1,...,k, and for every Wsuch that/bardblW−W(0)(ω)/bardblF≤(logm)2/α. We will prove, by
contradiction, that for every ω∈N1∩N2∩N3,/bardblW(t)−W(0)/bardblF<(logm)2/αfor everyt >0. In the
following we will write W(s)in the place of W(s)(ω)and always assume that ωbelongs toN1∩N2∩N3.
Suppose that there exists tsuch that/bardblW(t)−W(0)/bardblF≥(logm)2/α, and let
t0= argmint≥0{t:/bardblW(t)−W(0)/bardblF≥(logm)2/α}.
28Published in Transactions on Machine Learning Research (11/2024)
Since/bardblW(s)−W(0)/bardblF≤(logm)2/αfor everys≤t0, then, for every s≤t0,
λmin(˜Hm(W(s),X))>λ
2,
/vextenddouble/vextenddouble/vextenddouble/vextenddouble∂˜fm
∂w(W(s),xj;α)−∂˜fm
∂w(W(0),xj;α)/vextenddouble/vextenddouble/vextenddouble/vextenddouble
F<cm−γ/α(logm)−1/α,
/vextenddouble/vextenddouble/vextenddouble/vextenddouble∂˜fm
∂w(0)(W(s),xj;α)−∂˜fm
∂w(0)(W(0)(ω),xj;α)/vextenddouble/vextenddouble/vextenddouble/vextenddouble
F<M1
k(logm)−1/α(j= 1,...,k ),
/bardbl˜fm(W(0)(ω),X;α)−Y/bardblF<M 2,
max
1≤i≤k/vextenddouble/vextenddouble/vextenddouble/vextenddouble∂
∂W˜fm(W(0)(ω),xi;α)/vextenddouble/vextenddouble/vextenddouble/vextenddouble
F<M 2(logm)−1/α.
Let us now consider the gradient descent dynamic, with continuous learning rate η= (logm)2/α:
dW(s)
ds=−(logm)2/α∇W1
2k/summationdisplay
i=1/parenleftbig˜fm(W(s),xi;α)−yi/parenrightbig2
=−(logm)2/αk/summationdisplay
i=1/parenleftbig˜fm(W(s),xi)−yi/parenrightbig∂˜fm
∂W(W(s),xi;α).
This expression allows to write
/bardblW(t0)−W(0)/bardblF
≤/vextenddouble/vextenddouble/vextenddouble/vextenddouble/integraldisplayt0
0d
dsW(s)ds/vextenddouble/vextenddouble/vextenddouble/vextenddouble
F
≤(logm)2/α/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/integraldisplayt0
0k/summationdisplay
i=1(˜fm(W(s),xi;α)−yi)∂˜fm
∂W(W(s),xi;α)ds/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble
F
≤(logm)2/αmax
0≤s≤t0k/summationdisplay
i=1/vextenddouble/vextenddouble/vextenddouble/vextenddouble∂˜fm
∂W(W(s),xi;α)/vextenddouble/vextenddouble/vextenddouble/vextenddouble
F/integraldisplayt0
0/bardbl˜fm(W(s),X;α)−Y/bardblds.
To bound the term /bardbl˜fm(W(s),X;α)−Y/bardblwe will exploit the dynamics of the NN output
d˜fm(W(s),X;α)
ds=∂˜fm
∂W(W(s),X;α)dWT(s)
ds
=−(logm)2/α(˜fm(W(s),X;α)−Y)Hm(W(s),X)
=−(˜fm(W(s),X;α)−Y)˜Hm(W(s),X),
that gives
d
ds/bardbl˜fm(W(s),X;α)−Y/bardbl2
2=−2/parenleftbig˜fm(W(s),X;α)−Y/parenrightbig˜Hm(W(s),X)/parenleftbig˜fm(W(s),X;α)−Y/parenrightbigT.
Sinceλmin(˜Hm(W(s),X))>λ/ 2for everys≤t0, then
d
ds/bardbl˜fm(W(s),X;α)−Y/bardbl2
2≤−λ/bardbl˜fm(W(s),X;α)−Y/bardbl2
2,
which implies that
d
ds/parenleftbig
exp(λs)/bardbl˜fm(W(s),X;α)−Y/bardbl2
2/parenrightbig
≤0.
It follows that exp(λs)/bardbl˜fm(W(s),X;α)−Y/bardbl2
2is a decreasing function of s, and therefore
/bardbl˜fm(W(s),X;α)−Y/bardbl2≤exp(−λ/2)/bardbl˜fm(W(0),X;α)−Y/bardbl2,
29Published in Transactions on Machine Learning Research (11/2024)
for everys≤t0. Substituting in the integral, we can write that
/bardblW(t0)−W(0)/bardblF
≤(logm)2/αmax
0≤s≤t0k/summationdisplay
i=1/vextenddouble/vextenddouble/vextenddouble/vextenddouble∂˜fm
∂W(W(s),xi;α)/vextenddouble/vextenddouble/vextenddouble/vextenddouble
F/integraldisplayt0
0exp(−λs/2)ds·/bardbl˜fm(W(0),X;α)−Y/bardbl
≤2(logm)2/α
λmax
0≤s≤t0k/summationdisplay
i=1/parenleftbigg/vextenddouble/vextenddouble/vextenddouble/vextenddouble∂˜fm
∂W(W(0),xi;α)/vextenddouble/vextenddouble/vextenddouble/vextenddouble
F+/vextenddouble/vextenddouble/vextenddouble/vextenddouble∂˜fm
∂W(W(s),xi;α)−∂˜fm
∂W(W(0),xi;α)/vextenddouble/vextenddouble/vextenddouble/vextenddouble
F/parenrightbigg
×/bardbl˜fm(W(0),X;α)−Y/bardbl
≤2(logm)2/α
λmax
0≤s≤t0k/summationdisplay
i=1/parenleftbigg/vextenddouble/vextenddouble/vextenddouble/vextenddouble∂˜fm
∂W(W(0),xi;α)/vextenddouble/vextenddouble/vextenddouble/vextenddouble
F+/vextenddouble/vextenddouble/vextenddouble/vextenddouble∂˜fm
∂w(0)(W(s),xi;α)−∂˜fm
∂w(0)(W(0),xi;α)/vextenddouble/vextenddouble/vextenddouble/vextenddouble
F
+/vextenddouble/vextenddouble/vextenddouble/vextenddouble∂˜fm
∂w(W(s),xi;α)−∂˜fm
∂w(W(0),xi;α)/vextenddouble/vextenddouble/vextenddouble/vextenddouble
F/parenrightbigg
/bardbl˜fm(W(0),X;α)−Y/bardbl
≤2(logm)1/α
λ/parenleftBig
M2+M1+kcm−γ/α/parenrightBig
M2,
which, formlarge, contradicts /bardblW(t0)−W(0)/bardblF≥(logm)2/α.
Now, we are in the position of proving Proposition 3.2. Let m∈NandN∈Fbe such that P(N)>1−δ
and the properties mentioned in Lemma A.8, Lemma A.9, Lemma A.10 and Lemma A.11 hold true for every
ω∈N. Therefore, by means of Lemma A.8 and of Lemma A.9, it is suﬃcient to show that
/bardblW(t)−W(0)/bardbl2
F(ω)<(logm)2/α
for everyt>0andω∈N. By contradiction, suppose that there exists, for some ω∈N,t0(ω)ﬁnite with
t0(ω) := inf
t≥0{t:/bardblW(t)−W(0)/bardblF(ω)≥(logm)2/α}.
SinceW(t)(ω)is a continuous function of t, then/bardblW(t0(ω))−W(0)/bardbl2
F(ω) = (logm)2/α. Then, by Lemma
A.8,
(logm)2/α/vextenddouble/vextenddouble/vextenddouble/vextenddouble∂˜fm
∂W(W(s),xj;α)−∂˜fm
∂W(W(0),xj;α)/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
F(ω)<cm−2γ/α,
for everys≤t0and everyj. Therefore, by Lemma A.11 it holds true that /bardblW(t0(ω))−W(0)/bardblF(ω)<
(logm)2/α, which contradicts the deﬁnition of t0. This completes the proof of Proposition 3.2.
B
The distribution of a random vector ξis said to be inﬁnitely divisible if, for every n, there exist some i.i.d.
random vectors ξn1,...,ξnnsuch that/summationtext
kξnkd=ξ. Ak-dimensional random vector ξis inﬁnitely divisible if
and only if its characteristic function admits the representation eψ(u), where
ψ(u) =iuTb−1
2uTau+/integraldisplay/parenleftBig
eiuTx−1−iuTxI(||x||≤1)/parenrightBig
ν(dx) (16)
whereνis a measure on Rk\{0}satisfying/integraltext
(||x||2∧1)ν(dx)<∞,ais ak×kpositive semi-deﬁnite,
symmetric matrix and bis a vector. The measure νis called the Lévy measure of ξand(a,b,ν )are called
the characteristics of the inﬁnitely divisible distribution. We will write ξ∼i.d.(a,b,ν ). Other kinds of
truncation can be used for the term iuTx. This aﬀects only the vector of centering constants b. An i.i.d.
array of random vectors is a collection of random vectors {ξnj,j≤mn,n≥1}such that, for every n,
30Published in Transactions on Machine Learning Research (11/2024)
ξn1,...,ξnmnare i.i.d. The class of inﬁnitely divisible distributions coincides with the class of limits of sums
of i.i.d. arrays (Kallenberg, 2002, Theorem 13.12).
To state a general criterion of convergence, we ﬁrst introduce some notations. Let ξ∼i.d.(a,b,ν ). Deﬁne,
for eachh>0,
a(h)=a+/integraldisplay
||x||<hxxTν(dx),
b(h)=b−/integraldisplay
h<||x||≤1xν(dx),
where/integraltext
h<||x||≤1=−/integraltext
1<||x||≤hifh >1. Denote byv→vague convergence, that is convergence of measures
with respect to the topology induced by bounded, measurable functions with compact support. Moreover,
letRkbe the one-point compactiﬁcation of Rk. The following criterion for convergence holds (Kallenberg,
2002, Corollary 13.16).
Theorem B.1. Consider in Rkan i.i.d. array (ξnj)j=1,...,m n,n≥1and letξbei.d.(a,b,ν ). Leth>0be such
thatν(||x||=h) = 0. Then/summationtext
jξnjd→ξif and only if the following conditions hold:
(i)mnP(ξn1∈·)v→ν(·)onRk\{0}
(ii)mnE(ξn1ξT
n1I(||ξn1||<h))→a(h)
(iii)mnE(ξn1I(||ξn1||<h))→b(h)
Inside the class of inﬁnitely divisible distribution, we can distinguish the subclass of stable distributions. A
k-dimensional random vector ξhas stable distribution if, for every independent random vectors ξ1andξ2
withξ1d=ξ2d=ξand everya,b∈R, there exists c∈Randd∈Rksuch thataξ1+bξ2d=cξ+d. This is
equivalent to the condition: for every n≥1,
ξ1+···+ξnd=n1/αξ+dn (17)
whereα∈(0,2],ξ1,...,ξnare i.i.d. copies of ξanddnis a vector. The random vector ξis said to be strictly
stable if (17) holds with dn= 0. A stable vector ξis strictly stable if and only if all its components are
strictly stable. The coeﬃcient αis called the index of stability of ξand the law of ξis calledα-stable. A
stable vector ξis symmetric stable if P(ξ∈A) =P(−ξ∈A)for every Borel set A. A symmetric stable
vector is strictly stable. The class of stable distributions coincides with the class of limit laws of sequences
((/summationtextn
k=1Xk−bn)/an), where (Xn)are i.i.d. random variables.
A stable distribution is inﬁnitely divisible. Thus its characteristic function admits the Lévy representation
(16). Ifα= 2, then the Lévy measure is the null measure and, therefore, the stable distribution coincides
with the multivariate normal distribution with covariance matrix aand mean vector b. Ifα<2, thena= 0
(the zero matrix) and the α-stability implies that there exists a measure σon the unit sphere Sk−1such that
ν(dx) =r−(α+1)drσ(ds), wherer=||x||ands=x/||x||. Substituting in (16), we obtain
ψ(u) =iuTb+/integraldisplay
S/integraldisplay∞
0/parenleftBig
eiruTs−1−iruTsI(r≤1)/parenrightBig1
r1+αdrσ(ds)
Forα < 1, the centering iruTsI(r≤1)is not needed, since the function (of r) is integrable, and we can
write
ψ(u) =iuTb/prime+/integraldisplay
S/integraldisplay∞
0/parenleftBig
eiruTs−1/parenrightBig1
r1+αdrσ(ds),
for some vector b/prime. After evaluating the inner integrals as in Feller (1968, Example XVII.3), we obtain
ψ(u) =iuTb/prime−/integraldisplay
S|uTs|αΓ(1−α)/parenleftbig
cos(πα/2)−isign(uTs) sin(πα/2)/parenrightbig
σ(ds)
31Published in Transactions on Machine Learning Research (11/2024)
=iuTb/prime−/integraldisplay
S|uTs|α/parenleftbig
1−isign(uTs) tan(πα/2)/parenrightbig
Γ(1−α) cos(πα/2)σ(ds).
Forα>1, using the centering iruTs, we can write
ψ(u) =iuTb/prime/prime+/integraldisplay
S/integraldisplay∞
0/parenleftBig
eiruTs−1−iruTs/parenrightBig1
r1+αdrσ(ds),
for someb/prime/prime. After evaluating the inner integrals as in Feller (1968, Example XVII.3), we obtain
ψ(u) =iuTb/prime/prime+/integraldisplay
S|uTs|αΓ(2−α)
α−1/parenleftbig
cos(πα/2)−isign(uTs) sin(πα/2)/parenrightbig
σ(ds)
=iuTb/prime/prime−/integraldisplay
S|uTsα/parenleftbig
1−isign(uTs) tan(πα/2)/parenrightbigΓ(2−α)
1−αcos(πα/2)σ(ds).
Since, forα<1,Γ(2−α) = (1−α)Γ(1−α), we can encompass the above results in one equation, and write,
forα/negationslash= 1,
ψ(u) =iuTb/prime/prime/prime−/integraldisplay
S|uTs|α/parenleftbig
1−isign(uTs) tan(πα/2)/parenrightbigΓ(2−α)
1−αcos(πα/2)σ(ds),
for someb/prime/prime/prime. Finally, for α= 1, using the centering irsinruTs, we can write
ψ(u) =iuTb/prime/prime/prime/prime+/integraldisplay
S/integraldisplay∞
0/parenleftBig
eiruTs−1−irsinruTs/parenrightBig1
r2drσ(ds),
for someb/prime/prime/prime/prime. Evaluating the inner integral as in Feller (1968, Example XVII.3), we obtain
ψ(u) =iuTb/prime/prime/prime/prime−/integraldisplay
S|uTs|/parenleftBigπ
2+isign(uTs) log|uTs|/parenrightBig
σ(ds)
=iuTb/prime/prime/prime/prime−/integraldisplay
S|uTs|/parenleftbigg
1 +i2
πsign(uTs) log|uTs|/parenrightbiggπ
2σ(ds).
Considering the spectral representation eψ(u)of the multivariate stable characteristic function
ψ(u) =

−/integraltext
S|uTs|α/parenleftbig
1−isign(uTs) tan(πα/2)/parenrightbig
Γ(ds) +iuTµ(0)α/negationslash= 1
−/integraltext
S|uTs|/parenleftbig
1 +i2
πsign(uTs) log|uTs|/parenrightbig
Γ(ds) +iuTµ(0)α= 1,
we can establish the following relationship between the Lévy measure νand the spectral measure Γ:
ν(dx) =Cα1
rα+1Γ(ds),
wherer=||x||,s=x/||x||and
Cα=

1−α
Γ(2−α) cos(πα/2)α/negationslash= 1
2/π α = 1
A Stable random vector ξis strictly stable if and only if
/braceleftbigg
µ(0)= 0 α/negationslash= 1/integraltext
SsjΓ(ds) = 0for every j α= 1.
(see e.g. Samoradnitsky and Taqqu (1994, Theorem 2.4.1)). By Theorem B.1, the spectral measure Γof a
symmetric stable random vector ξsatisﬁes
lim
n→∞nP/parenleftbigg
||ξ||>n1/αx,ξ
||ξ||∈A/parenrightbigg
=Cαx−αΓ(A) (18)
for every Borel set AofSsuch that Γ(∂A) = 0. Moreover, the distribution of a random vector ξbelongs
to the domain of attraction of the St k(α,Γ)distribution, with α∈(0,2)andΓsimmetric ﬁnite measure on
Sk−1, if and only if (18) holds (see e.g. Davydov et al. (2008, Theorem 4.3)).
32Published in Transactions on Machine Learning Research (11/2024)
References
Cline, D.B.H. (1986). Convolution tails, product tails and domains of attraction. Probability Theory and
Related Fields 72, 525–557.
Davydov, Y., Molchanov, I. and Zuyev, S. (2008). Strictly stable distributions on convex cones.
Electronic Journal of Probability 13, 259–321.
Feller, W. (1968).An introduction to probability theory and its applications . Wiley.
Horn, R.A. and Johnson, C.R. (1985).Matrix Analysis . Cambridge University Press.
Kallenberg, O. (2002).Foundations of modern probability . Springer.
Nolan, J.P. (2010). Metrics for multivariate stable distributions. Banach Center Publications 90, 83–102.
33