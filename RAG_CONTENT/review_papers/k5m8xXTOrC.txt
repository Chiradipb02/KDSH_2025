Published in Transactions on Machine Learning Research (02/2023)
Solving a Special Type of Optimal Transport Problem by a
Modified Hungarian Algorithm
Yiling Xie yxie350@gatech.edu
School of Industrial and Systems Engineering
Georgia Institute of Technology
Yiling Luo yluo373@gatech.edu
School of Industrial and Systems Engineering
Georgia Institute of Technology
Xiaoming Huo huo@gatech.edu
School of Industrial and Systems Engineering
Georgia Institute of Technology
Reviewed on OpenReview: https: // openreview. net/ forum? id= k5m8xXTOrC
Abstract
Computing the empirical Wasserstein distance in the Wasserstein-distance-based indepen-
dence test is an optimal transport (OT) problem with a special structure. This observation
inspires us to study a special type of OT problem and propose a modified Hungarian algo-
rithmto solve it exactly. For the OT problem involving two marginals with mandnatoms
(m≥n), respectively, the computational complexity of the proposed algorithm is O(m2n).
Computing the empirical Wasserstein distance in the independence test requires solving this
special type of OT problem, where m=n2. The associated computational complexity of the
proposed algorithm is O(n5), while the order of applying the classic Hungarian algorithm
isO(n6). In addition to the aforementioned special type of OT problem, it is shown that
the modified Hungarian algorithm could be adopted to solve a wider range of OT problems.
Broader applications of the proposed algorithm are discussed—solving the one-to-many
assignment problem and the many-to-many assignment problem. We conduct numerical ex-
periments to validate our theoretical results. The experiment results demonstrate that the
proposed modified Hungarian algorithm compares favorably with the Hungarian algorithm,
the well-known Sinkhorn algorithm, and the network simplex algorithm.
1 Introduction
One appealing application of optimal transport (OT) and Wasserstein distance (Villani, 2009; Peyré &
Cuturi, 2019) is the independence test. The Wasserstein distance between two distributions µ1,µ2onZis
defined as:
W(µ1,µ2) := inf/braceleftbigg/integraldisplay
Z2d(z,z′)dγ(z,z′) :γis a distribution with marginals µ1andµ2/bracerightbigg
,
where (Z,d)is a metric space (1-Wasserstein distance is considered in this paper). The Wasserstein distance
is a metric on probability measures (Villani, 2009). To test the independence between the variables Y∼ν1
andZ∼ν2, people utilize the Wasserstein distance between the joint distribution of Y,Zand the product
distribution of Y,Z, i.e.,W(π,ν1⊗ν2), whereπdenotes the joint distribution of Y,Z, andν1⊗ν2denotes the
productdistributionof Y,Z. Whilethestatisticalpropertiesofthisapproachhavebeenintenselyinvestigated
(Nies et al., 2021; Mordant & Segers, 2022; Wiesel, 2022), no existing literature focuses on the computational
aspect. In this paper, we discuss the following:
1Published in Transactions on Machine Learning Research (02/2023)
How to compute the empirical Wasserstein distance in the independence test?
In practice, given ni.i.d. samples{(y1,z1),···,(yn,zn)}generated from (Y,Z), one can build the statistic—
W(/hatwideπ,/hatwideν1⊗/hatwideν2), where/hatwideπ,/hatwideνdenote the corresponding empirical distributions of πandν, respectively—to test
the independence. Computing W(/hatwideπ,/hatwideν1⊗/hatwideν2)is equivalent to solving the following optimization problem:
(more details are presented in Section 4.)
min
X◦∈Π◦n/summationdisplay
i,j,k=1d((yi,zj),(yk,zk))X◦
ij;k,Π◦=

X◦
ij;k≥0/vextendsingle/vextendsingle/vextendsingle/vextendsinglen/summationdisplay
k=1X◦
ij;k=1
n2,n/summationdisplay
i,j=1X◦
ij;k=1
n,∀i,j,k = 1,···,n.

,
(1)
where the metric dis usually chosen as d((yi,zj),(yk,zl)) =∥yi−yk∥p+∥zj−zl∥p, and∥·∥pdenotes the
lpnorm.
Problem (1) is an OT problem involving two marginals. One marginal is uniform with natoms (i.e., we
have/summationtextn
i,j=1X◦
ij;k= 1/n,∀k,1≤k≤n), and the other marginal is uniform with n2atoms (i.e., we have/summationtextn
k=1X◦
ij;k= 1/n2,∀i,j,1≤i,j≤n). Motivated by this structure, we study the following special OT
problem:
min
X′∈U′m/summationdisplay
i=1n/summationdisplay
j=1X′
ijCij,U′=

X′
ij≥0/vextendsingle/vextendsingle/vextendsingle/vextendsinglen/summationdisplay
j=1X′
ij=1
m,m/summationdisplay
i=1X′
ij=mj
m,∀i= 1,···,m;j= 1,···,n

.(2)
where 0< n≤m,mj’s are positive integers, and/summationtextn
j=1mj=mholds. One marginal of this OT problem
isn-dimensional where the probability of each component is prescribed as mj/m(i.e., we have/summationtextm
i=1X′
ij=
mj/m,∀j,1≤j≤n), andtheothermarginalisuniformwith matoms(i.e., wehave/summationtextn
j=1X′
ij= 1/m,∀i,1≤
i≤m). In essence, problem (1) is a special case of problem (2), where mj=n,m =n2,∀j= 1,···,n.
Throughout this paper, we consider real-valued entries in the cost matrix and later propose a strongly
polynomial-time algorithm to solve problem (2) precisely.
Per Birkhoff’s theorem (Birkhoff, 1946), the solution to problem (2) is a vertex (whose coordinates are zeros
and ones). Then, we could prove the following proposition, and the proof is relegated to the Appendix.
Proposition 1. The optimization problem (2) is equivalent to the optimization problem (3).
min
X∈Um/summationdisplay
i=1n/summationdisplay
j=11
mXijCij,U=

Xij={0,1}/vextendsingle/vextendsingle/vextendsingle/vextendsinglen/summationdisplay
j=1Xij= 1,m/summationdisplay
i=1Xij=mj,∀i= 1,···,m;j= 1,···,n

.(3)
One may recall the assignment problem, seeing the definition in Section 2, where the permutation matrix
is the solution matrix. X∈Uis similar but different from the permutation matrix: X∈Uis anm×n
matrix instead of a square matrix and has multiple entries of 1in each column instead of only one entry. In
this case, we are not able to directly apply algorithms for the assignment problem, such as the Hungarian
algorithm (Kuhn, 1955; Munkres, 1957). An approach to obtain the precise solution to problem (3) is first
to duplicate the columns of CandX, then apply the Hungarian algorithm. The computational complexity
of this approach is O(m3). In this paper, a modified Hungarian algorithm is proposed. The algorithm
specializes in solving the special type of OT problem (3), which is equivalent to problem (2), with a provable
lower order—O(m2n).
For the special type of OT problem (2), we require that one marginal of the OT problem should be uniform.
We could further relax the uniform requirement and consider the following more general OT problems:
min
X∗∈U∗m/summationdisplay
i=1n/summationdisplay
j=1X∗
ijCij,U∗=

X∗
ij≥0/vextendsingle/vextendsingle/vextendsingle/vextendsinglen/summationdisplay
j=1X∗
ij=ni
M,m/summationdisplay
i=1X∗
ij=mj
M,∀i= 1,···,m;j= 1,···,n

,(4)
where 0< n≤m,ni’s,mj’s are positive integers, and/summationtextn
j=1mj=/summationtextm
i=1ni=Mholds. The modified
Hungarian algorithm could be adapted to solve problem (4), and the associated computational complexity
isO(M2n).
2Published in Transactions on Machine Learning Research (02/2023)
Back to the Wasserstein-distance-based independence test problem (1), the resulting computational complex-
ity of applying the proposed algorithm is O(n5)while the order of applying the classic Hungarian algorithm
isO(n6). In this sense, the proposed algorithm is faster. In addition to the application in the Wasser-
stein independence test, broader applications of the modified Hungarian algorithm, including solving the
one-to-many assignment problem and the many-to-many assignment problem (Zhu et al., 2011; 2016), are
investigated. Two practical assignment problems involving the soccer game and agent-task assignment serve
as examples to illustrate how to apply the proposed algorithm.
1.1 Related work
1.1.1 Semi-assignment problem
The problem (3) is also called the semi-assignment problem in the literature (Barr et al., 1977; Kennington &
Wang, 1992). Kennington & Wang (1992) proposes a strongly polynomial-time to solve the semi-assignment
problem exactly. The proposed modified Hungarian algorithm and the algorithm proposed in Kennington
& Wang (1992) are fundamentally different. The algorithm in Kennington & Wang (1992) adjusts the
shortest path augmenting algorithm (Jonker & Volgenant, 1987), while our algorithm modifies the Hungarian
algorithm Kuhn (1955). As illustrated in Jonker & Volgenant (1987); Kennington & Wang (1992), the
Hungarian algorithm is a primal-dual method based on maximum flow, while the shortest path augmenting
algorithm considers the assignment problem as a minimum cost flow problem and is a dual method based on
the shortest path. More specifically, the modified Hungarian algorithm is based on a modified Kuhn-Munkres
theorem and iterates to identify a perfect pseudo-matching in the bipartite graph with some feasible dual
variables, while the algorithm in Kennington & Wang (1992) involves constructing the shortest augmenting
path in the auxiliary graph, and the flow is pushed along the path (Kennington & Wang, 1992). Regarding
computational complexity, both algorithms have the order of O(m2n). However, our algorithm is easier to
understandandimplementbecausetherearefourphases(columnreduction,reductiontransfer,rowreduction
augmentation, and shortest path augmentation) in the algorithm in Kennington & Wang (1992) while our
algorithm only involves two phases (updating the feasible labeling and improving the pseudo-matching).
1.1.2 Approximation algorithms
To solve the OT problem, we could apply the exact algorithms or the approximation algorithms. While
the proposed modified Hungarian algorithm is an exact OT solver, there are a bunch of approximation
algorithms. People usually consider the following OT problem:
min
X∈U(α,β)N1/summationdisplay
i=1N2/summationdisplay
j=1XijCij,U(α,β) :=

X∈RN1×N2
+/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleN1/summationdisplay
i=1Xij=αj,N2/summationdisplay
j=1Xij=βi

, (5)
where/summationtextN1
i=1βi= 1and/summationtextN2
j=1αj= 1. The approximation algorithms (Cuturi, 2013; Dvurechensky et al.,
2018; Lin et al., 2019a; Xie et al., 2022) in the literature are to obtain an ϵ-approximation /hatwideX∈U(α,β)to
(5) such that⟨/hatwideX,C⟩≤⟨X∗,C⟩+ϵ,whereX∗is the solution to (5).
The development of efficient exact algorithms is meaningful. Notably, precise solutions are needed in some
scenarios, and Dong et al. (2020) demonstrates the favorable numerical performance of the exact solutions
over the approximate solutions. Numerical experiments are conducted to compare the modified Hungarian
algorithm with the most widely-used approximation algorithm—the Sinkhorn algorithm, highlighting the
efficiency of our exact algorithm.
1.1.3 Exact algorithms
We review the exact algorithms to solve the OT problem and compare our algorithm with them as follows.
The special type of OT problem (2) is a minimum-cost flow problem and could be solved by the network
simplex algorithms. Note that Orlin (1997) proposes the first polynomial-time network simplex algorithm,
andTarjan(1997)furtherimprovestheresult. TheassociatedcomputationalcomplexityofapplyingTarjan’s
3Published in Transactions on Machine Learning Research (02/2023)
algorithmtoproblem(2)is O(m2nlog(m) min{log(mCmax),mn log(m)}), whereCmaxdenotesthemaximum
absolute value of the costs if all costs are integers and ∞otherwise (Orlin, 1997; Tarjan, 1997). More
specifically, if the costs are integral, the resulting computational complexity is O(m2nlog(m) log(mCmax)),
which is comparable to the proposed algorithm; if the costs are not integral, the resulting computational
complexity isO(m3n2log2(m)), which is worse than our algorithm.
The interior point algorithms can also be customized to solve the minimum-cost flow problems, e.g., Yeh
(1989); Resende & Veiga (1993). As discussed in Resende & Pardalos (1996), from the perspective of
computational complexity, Vavasis & Ye (1994) proposes a strongly polynomial-time interior point algorithm
for solving the minimum-cost flow problem. The adaption of this method to problem (2) has the order of
O(m6.5n6.5log(mn)), which is much worse than the proposed algorithm. As demonstrated in Resende &
Pardalos (1996), prior to Vavasis & Ye (1994), the fastest interior point method to solve the minimum-
cost flow problem comes from Vaidya (1989). The computational complexity of the adoption of Vaidya’s
algorithm to problem (2) is O(m2.5n0.5log(mCmax)). The complexity is worse than our algorithm, and the
algorithm requires that all costs are integers.
As shown in Peyré & Cuturi (2019), we could apply other combinatorial algorithms, including the auction
algorithm(Bertsekas&Eckstein,1988), andthedualascentalgorithm(Bertsimas&Tsitsiklis,1997), tosolve
the special type of OT problem (2). We compare the proposed modified Hungarian algorithm with them as
follows: Regarding the auction algorithm, the output is suboptimal (Peyré & Cuturi, 2019), while the output
ofouralgorithmisoptimal. Furthermore, Bertsimas&Tsitsiklis(1997);Bertsekas(1988)illustratethatfinite
termination of the auction algorithm with an optimal solution could come from the nature of integer-valued
cost coefficients. In terms of the dual ascent algorithm, Bertsimas & Tsitsiklis (1997) demonstrates that the
input of integral costs is one of the conditions for finite termination.
In conclusion, finite termination with an optimal solution or fast computation of the aforementioned algo-
rithms, including network simplex algorithm, interior point algorithm, auction algorithm, and dual ascent
algorithm, require integer-valued costs, while the proposed modified Hungarian algorithm could handle
real-valued costs. Although most of the problems in practice have rational costs, which can be scaled to
integer-valued costs, it is more convenient to use the proposed algorithm since it allows direct input of any
real-valued costs. Also, looking into OT problems with real-valued costs itself is of much theoretical interest.
Similar discussions and comparisons could be developed for the more general OT problem (4). Problem (4)
could be reformulated to a minimum-cost flow problem and can be solved by the exact solvers mentioned
above. Among the algorithms, the computational complexity of the network simplex algorithm can be
comparable to the proposed modified Hungarian algorithm— ˜O(M2n)—under the assumption that all costs
are integral.
1.1.4 Independence criteria
There are some other independence criteria based on OT or the Wasserstein distance. Shi et al. (2020);
Deb & Sen (2021) design the independence criterion by combining the distance covariance and OT. Liu
et al. (2022) applies the entropy-regularized OT. Wiesel (2022) utilizes the nested Wasserstein distance.
The criterion proposed in Mordant & Segers (2022) is based on the 2-Wasserstein distance under the quasi-
Gaussian assumption. This paper considers the criterion based on the general Wasserstein distance as shown
in the formulation (1).
1.2 Our contributions:
We propose a modified Hungarian algorithm to solve a special type of OT problem (2). The modification
enables us to deal with the scenario where two marginals have different sizes of atoms, and the atoms in
one of the marginals have multiple assignments. Further, the proposed modified Hungarian algorithm could
be extended to solve more general OT problems. Moreover, the applications of the proposed algorithm are
explored: adopting the modified Hungarian algorithm to solve the Wasserstein independence test problem
(1), the one-to-many assignment problem, and the many-to-many assignment problem. Finally, several
4Published in Transactions on Machine Learning Research (02/2023)
numerical experiments are carried out using Python to show the favorability of our algorithm over the
classic Hungarian algorithm, the Sinkhorn algorithm, and the network simplex algorithm.
1.3 Organization
The remainder of this paper is organized as follows. In Section 2, we introduce some basics of graph theory.
In Section 3, we propose the modified Hungarian algorithm and compute its computational complexity. In
Section 4, we apply the modified Hungarian algorithm to the Wasserstein-distance-based independence test
problem. In Section 5, we apply the modified Hungarian algorithm to the one-to-many assignment problem
and the many-to-many assignment problem. In Section 6, we carry out various numerical experiments
on both synthetic data and real data to validate our theoretical results and show the favorability of our
algorithm. We discuss some future work in Section 7.
2 Preliminaries
Some definitions related to combinatorial optimization and graph theory (Ahuja et al., 1988; Suri, 2006;
Burkard et al., 2012) are introduced. They will be needed in the rest of this paper.
Definition 1 (Assignment problem) .Given ank×kcost matrix with components cij≥0,i,j∈[k] =
{1,···,k}, the assignment problem is to solve minϕ/summationtextk
i=1ciϕ(i), whereϕis the permutation of set [k].
Definition 2 (Bipartite graph) .A graphG= (V,E)is called a bipartite graph if its nodes can be partitioned
into two subsets V1andV2so that for each edge (v1,v2)inE,v1∈V1andv2∈V2.
Definition 3 (Matching and perfect matching) .A matching in the bipartite graph G= (V,E)is a subset
M⊂Esuch that at most one edge in Mis incident upon v,∀v∈V.Mis called a perfect matching if every
node inGcoincides with exactly an edge of M.
Definition 4 (Weighted bipartite graph) .A weighted bipartite graph is a bipartite graph where each edge has
a weightw(·)≥0. The weight of a matching Mis the sum of the weights of edges in M, i.e.,/summationtext
e∈Mw(e).
Definition 5 (Labeling and feasible labeling) .For a weighted bipartite graph G= (V,E), whereV=V1∪V2,
a labeling is a function l:V→R. A feasible labeling is one labeling such that l(v1)+l(v2)≥w(v1,v2),∀v1∈
V1,v2∈V2.
Definition 6 (Equality graph and neighbor) .The equality graph w.r.t. the labeling lisG′= (V,El)
whereEl={(v1,v2) :l(v1) +l(v2) =w(v1,v2)}. The neighbor of v2∈V2andS⊂V2is defined as
Nl(v2) ={v1: (v1,v2)∈El}andNl(S) =∪v2∈SNl(v2), respectively.
Definition 7 (Alternating and augmenting path) .LetMbe a matching of the bipartite graph G= (V,E). A
path inG= (V,E)is a sequence of distinct nodes and edges i1,(i2,i2),···,(ir−1,ir),ir, satisfying (ik,ik+1)∈
Efor eachk= 1,···,r−1. A path in G= (V,E)is alternating if its edges alternate between MandE−M.
An alternating path is augmenting if both endpoints do not coincide with any edges in M.
3 Modified Hungarian algorithm
In this section, we propose a modified Hungarian algorithm to solve the special type of OT problem (2),
which is equivalent to problem (3).
3.1 Review of the Hungarian algorithm
We first review the Hungarian algorithm (Kuhn, 1955; Munkres, 1957). Recall that the assignment problem
is to solve minϕ/summationtextm
i=1ciϕ(i). If we negate the costs and add the maximum of the costs to each component,
solving the assignment problem is equivalent to finding a maximum weighted matching in the weighted
bipartite graph with weights w(i,j) = maxijcij−cij. The Kuhn-Munkres theorem (Munkres, 1957) shows
that finding a maximum weighted matching is equivalent to finding a perfect matching on the equality graph
associated with some feasible labeling in the bipartite graph. In this regard, the Hungarian algorithm solves
the assignment problem by identifying a perfect matching on some equality graph in the weighted bipartite
5Published in Transactions on Machine Learning Research (02/2023)
Figure 1: pseudo-matching (left) and perfect pseudo-matching (right), where n= 3,m= 9,m1= 2,m2=
3,m3= 4.
graph. One first generates an initial feasible labeling and an associated equality graph; then proceeds to
look for an augmenting path to augment the matching. (if an augmenting path does not exist, update the
feasible labeling.) If the matching is perfect on the equality graph concerning some feasible labeling, stop
and output the matching.
3.2 Pseudo-matching
In problem (3), X∈Uhas one entry of 1in each row, multiple entries of 1in each column, and 0′s elsewhere.
Since a permutation matrix corresponds to a (perfect) matching in the bipartite graph, we define ‘pseudo-
matching’ in the bipartite graph G= (V1∪V2,E)to describe X.V1hasmnodes representing the rows of
XwhileV2hasnnodes representing the columns of X. Notice that we usually have m > n. In this case,
each node in V1coincides with at most one edge, while multiple edges are allowed to connect with nodes in
V2. See the formal definition in Definition 8.
Definition 8 (pseudo-matching, perfect pseudo-matching) .In the bipartite graph G, where|V1|=m,|V2|=
n.PM⊂Eis a pseudo-matching if every node of V1coincides with at most one edge of PM, andjth
node ofV2coincides with at most mjedges ofPM, where/summationtextn
j=1mj=m. Furthermore, if every node of V1
coincides with exactly one edge of PMandjth node ofV2coincides with exactly mjedges ofPM,PMis
called a perfect pseudo-matching.
Figure 1 is an example of (perfect) pseudo-matching, where n= 3,m= 9,m1= 2,m2= 3,m3= 4. Under
this setting, each node in the left-hand side of the graph can coincide with at most one edge, while each
node in right-hand side can coincide with at most 2 edges, 3 edges, and 4 edges, respectively.
3.3 Our algorithm
Solving problem (3) is equivalent to looking for a maximum weighted pseudo-matching in the bipartite graph.
We develop a modified Kuhn-Munkres theorem based on the pseudo-matching. See Theorem 1. (The proof
can be found in the Appendix.) It demonstrates that we only need to find a perfect pseudo-matching on
some equality graph to solve problem (3).
Theorem 1 (Modified Kuhn-Munkres theorem) .Iflis a feasible labeling on the weighted bipartite graph
G= (V,E), andPM⊂Elis a perfect pseudo-matching on the corresponding equality graph G′= (V,El),
PMis a maximum weighted pseudo-matching.
Equipped with the modified Kuhn-Munkres theorem, we design a modified Hungarian algorithm (Algorithm
1). The definitions used in the algorithm are specified in Definition 9. The modified Hungarian algorithm
improves either the feasible labeling (adding edges to the associated equality graph) or the pseudo-matching
until the pseudo-matching is perfect on some equality graph w.r.t. some feasible labeling. The algorithm
improves the pseudo-matching by generating pseudo-augmenting paths and then exchanging the edge status
alongthepaths. Thisprocessiscalledthepseudo-augmentingprocess. Also, weforcethepseudo-augmenting
paths emanating from V2, which have a lower order of nodes.
Definition 9 (Free, matched, pseudo-matched, pseudo-alternating path, pseudo-augmenting path) .LetPM
be a pseudo-matching of G= (V,E).
6Published in Transactions on Machine Learning Research (02/2023)
BDACBDAC
Figure 2: pseudo-augmenting process, where n= 3,m= 9,m1= 2,m2= 3,m3= 4.
•If the node vis inV1, it is pseudo-matched if it is an endpoint of some edge in PM; if the node v
is thejth node inV2, it is pseudo-matched if it is an endpoint of mjedges inPM. Otherwise, the
node is free.
•If the node v∈V, we say it is matched if it is an endpoint of some edge in PM.
•A path is pseudo-alternating if its edge alternates between PMandE−PM. A pseudo-alternating
path is pseudo-augmenting if both its endpoints are free.
An example of the pseudo-augmenting process is given in Figure 2. The solid line means that the edge
belongs to the pseudo-matching. The dashed line means that the edge belongs to the equality graph but
does not belong to the pseudo-matching. Node B and node C are pseudo-matched. Edge A-B and edge
C-D are not in the pseudo-matching. In this sense, A-B-C-D is a pseudo-alternating path. Because node A
and node D are free, A-B-C-D is a pseudo-augmenting. The pseudo-augmenting process is to exchange the
status of the edges: delete B-C from the pseudo-matching and enter A-B, C-D into the pseudo-matching.
The pseudo-matching has been improved in this way.
3.4 Computational complexity
We now analyze the computational complexity of Algorithm 1. Similar to the Hungarian algorithm (Suri,
2006), we keep track of slackv1= minv2∈S{l(v1) +l(v2)−w(v1,v2)},∀v1̸∈T. The computational cost
increases when computing αlviaslacks, updating the values of slacks, and calculating the labeling.
The number of edges of the pseudo-matching increases by 1after one loop, so O(m)loops is needed to
form a perfect pseudo-matching. There are two subroutines in each loop: the first is to update the feasible
labeling (Step 2), and the second is to improve the pseudo-matching (Step 3). In the procedure of updating
the feasible labeling, since there are nnodes inV2, the improvement occurs O(n)times to build a pseudo-
alternating tree. In each time, computing αl, updating the slacks, and calculating the labeling cost O(m).
In the procedure of improving the pseudo-matching, when a new node has been added to S, it costsO(m)
to update slacks, andO(n)nodes could be added. On the other hand, when a node has been added to T, we
just remove the corresponding slackv1. We conclude that each loop costs O(mn), so the total computational
complexity of Algorithm 1 to solve problem (3) is O(m2n). We summarize the analysis above in Theorem 2.
Theorem 2. The computational complexity of applying the modified Hungarian algorithm to solve problem
(3) isO(m2n).
3.5 Solving more general OT problem
We could adapt the proposed modified Hungarian algorithm to solve the more general OT problem (4). We
first rewrite problem (4) as the formulation of the special type of OT problem (2) by duplicating the rows
of the cost matrix, seeing Proposition 2 (the proof is relegated to the Appendix).
Proposition 2. Problem (4) is equivalent to the following optimization problem:
min
X†∈U†M/summationdisplay
i=1n/summationdisplay
j=1X†
ijC†
ij,U†=

X†
ij≥0/vextendsingle/vextendsingle/vextendsingle/vextendsinglen/summationdisplay
j=1X†
ij=1
M,M/summationdisplay
i=1X†
ij=mj
M,∀i= 1,···,m;j= 1,···,n

.(6)
7Published in Transactions on Machine Learning Research (02/2023)
Algorithm 1: Modified Hungarian Algorithm
Generate an initial feasible labeling l:∀v2∈V2,l(v2) = 0;∀v1∈V1,l(v1) = maxv2∈V2{w(v1,v2)}and
initialize a pseudo-matching MinEl;
1ifMis a perfect pseudo-matching then
Stop
else
Pick up a free node vfree∈V2. SetS={vfree},T=∅;
forv1∈V1is matched to vfreedo
T=T∪v1
end
end
2ifNl(S)−T=∅then
update labeling such that forcing Nl(S)−T̸=∅:
αl= minv1̸∈T,v2∈S{l(v1) +l(v2)−w(v1,v2)}, l(v) =

l(v)−αlv∈S
l(v) +αl, v∈T
l(v),otherwise.
end
3ifNl(S)−T̸=∅then
pickv1∈Nl(S)−T;
ifv1is freethen
vfree→v1is a pseudo-augmenting path. Pseudo-augment the pseudo-matching M. Go to Step 1;
end
ifv1is pseudo-matched to zthen
extend the pseudo-matching tree: S=S∪{z},T=T∪{v1};
forv1∈V1is matched to zdo
T=T∪v1;
end
end
Go to Step 2.
end
2.0 2.5 3.0 3.5
ln(samplesize)468101214161820ln(#operation)
dependent case, p=1
2.0 2.5 3.0 3.5
ln(samplesize)468101214161820
dependent case, p=2
2.0 2.5 3.0 3.5
ln(samplesize)468101214161820
independent case, p=1
2.0 2.5 3.0 3.5
ln(samplesize)468101214161820
independent case, p=2
modified Hungarian average modified Hungarian best modified Hungarian worst Hungarian average Hungarian best Hungarian worst
Figure 3: Comparison with the Hungarian algorithm on synthetic data
whereC†is anM×nmatrix generated by duplicating the ith row ofC nitimes:
C†
tj=/braceleftigg
C1j1≤t≤n1
Cijn1+···+ni−1+ 1≤t≤n1+···+ni,2≤i≤m.
Problem (6) belongs to the special type of OT problem (2). We could apply the proposed modified Hungarian
algorithm to problem (6) and get the exact solution to problem (4). The resulting computational complexity
isO(M2n).
8Published in Transactions on Machine Learning Research (02/2023)
2.0 2.5 3.0 3.5
ln(samplesize)468101214161820ln(#operation)
dependent case, p=1
2.0 2.5 3.0 3.5
ln(samplesize)468101214161820
dependent case, p=2
2.0 2.5 3.0 3.5
ln(samplesize)468101214161820
independent case, p=1
2.0 2.5 3.0 3.5
ln(samplesize)468101214161820
independent case, p=2
modified Hungarian average modified Hungarian best modified Hungarian worst Hungarian average Hungarian best Hungarian worst
Figure 4: Comparison with the Hungarian algorithm on CIFAR10
2.0 2.5 3.0 3.5
ln(samplesize)468101214161820ln(#operation)
dependent case, p=1
2.0 2.5 3.0 3.5
ln(samplesize)468101214161820
dependent case, p=2
2.0 2.5 3.0 3.5
ln(samplesize)468101214161820
independent case, p=1
2.0 2.5 3.0 3.5
ln(samplesize)468101214161820
independent case, p=2
modified Hungarian average modified Hungarian best modified Hungarian worst Hungarian average Hungarian best Hungarian worst
Figure 5: Comparison with the Hungarian algorithm on Wisconsin breast cancer data
3.6 Compare with the Hungarian algorithm
We discuss the modification and the improvement of the proposed modified Hungarian algorithm
Our algorithm modifies the Hungarian algorithm to solve a wider class of problems. Hungarian algorithm
specializes in the assignment problem which corresponds to the matching in the bipartite graph. The
matching is embedded with the ‘one-to-one’ structure. However, as shown in the solution structure of
problem (3), the problems considered in this paper have a ‘one-to-many’ structure. To deal with the ‘one-
to-many’ structure, pseudo-matching is defined. The proposed algorithm is developed to identify the perfect
pseudo-matching while the Hungarian algorithm identifies the perfect traditional matching. In addition,
instead of building the augmenting paths in the Hungarian algorithm, we establish the pseudo-augmenting
path in the proposed algorithm as shown in Figure 2.
Our algorithm has a lower order of computational complexity. The complexity of converting the problems to
the assignment problem and solving them by the Hungarian algorithm only depends on the larger size of the
marginals—O(m3). To reduce the complexity, we force the pseudo-augmenting paths emanating from V2,
which corresponds to the marginal with the smaller size. In this way, the complexity of the direct application
of the proposed algorithm depends on the sizes of both marginals— O(m2n). Hence, the proposed modified
Hungarian algorithm will outperform, especially when m≫n.
4 Application to the independence test using the Wasserstein distance
In this section, we apply the modified Hungarian algorithm to the Wasserstein-distance-based independence
test, which originally motivates us to study the special type of OT problem (2).
Suppose that there are ni.i.d. samples{(y1,z1),···,(yn,zn)}, where (yi,zi)∼(Y,Z),Y∼ν1,Z∼ν2.
Recallπdenotes the joint distribution of Y,Z, and one could prove the following equivalence:
Y⊥Z⇐⇒ν1⊗ν2=π⇐⇒W(π,ν1⊗ν2) = 0,
which follows from the fact that the Wasserstein distance is a valid metric between probability measures.
Given the empirical data, we utilize the statistic W(/hatwideπ,/hatwideν1⊗/hatwideν2)to test the independence between YandZ,
9Published in Transactions on Machine Learning Research (02/2023)
2.0 2.5 3.0 3.5
ln(samplesize)468101214161820ln(#operation)
dependent case, p=1
2.0 2.5 3.0 3.5
ln(samplesize)468101214161820
dependent case, p=2
2.0 2.5 3.0 3.5
ln(samplesize)468101214161820
independent case, p=1
2.0 2.5 3.0 3.5
ln(samplesize)468101214161820
independent case, p=2
modified Hungarian average modified Hungarian best modified Hungarian worst Hungarian average Hungarian best Hungarian worst
Figure 6: Comparison with the Hungarian algorithm on DOT-benchmark with 512×512resolution
2.0 2.5 3.0 3.5
ln(samplesize)10
8
6
4
2
0ln(time)
dependent case, p=1
2.0 2.5 3.0 3.5
ln(samplesize)10
8
6
4
2
0
dependent case, p=2
2.0 2.5 3.0 3.5
ln(samplesize)10
8
6
4
2
0
independent case, p=1
2.0 2.5 3.0 3.5
ln(samplesize)10
8
6
4
2
0
independent case, p=2
modified Hungarian average modified Hungarian best modified Hungarian worst Hungarian average Hungarian best Hungarian worst
Figure 7: Comparison with the Hungarian algorithm on synthetic data
where/hatwideπ,/hatwideνdenote the empirical distributions and have the following expressions:
/hatwideπ=
1
n0
···
01
n
,/hatwideν1⊗/hatwideν2=
1
n2···1
n2
··· ··· ···
1
n2···1
n2
.
Plug in the Wasserstein distance formula, the resulting optimization problem is:
min
X∈Πn/summationdisplay
i,j,k,l =1d((yi,zj),(yk,zl))Xij;kl, (7)
where
Π =

Xij;kl≥0/vextendsingle/vextendsingle/vextendsingle/vextendsinglen/summationdisplay
k,l=1Xij;kl=1
n2,n/summationdisplay
i,j=1Xij;kl=/braceleftigg
1
nk=l
0k̸=l,∀i,j,k,l = 1,···,n.

.
It is worth noting that Xij;kl= 0,k̸=l. If we let X◦
ij;k:=/summationtextn
l=1Xij;kl, problem (7) can be simplified as
problem (1). Problem (1) belongs to the special type of OT problem, where mj=n,∀j,1≤j≤n,m =n2.
Adopting the Hungarian algorithm to problem (1) costs O(n6), while adopting the proposed Hungarian
algorithm directly costs O(n5).
5 Application to the one-to-many assignment problem and the many-to-many
assignment problem
In this section, we proceed to explain how to apply the modified Hungarian algorithm to solve the one-
to-many assignment problem (corresponding to problem (2)) and the many-to-many assignment problem
(corresponding to problem (4)). The applications are shown based on two practical examples.
Example 1 (one-to-many assignment problem): An assignment problem involving the soccer ball game
mentioned by Zhu et al. (2011) is considered here. Suppose a coach is tasked to choose players from a soccer
team with m1players (a1,···,am1). There are droles (r1,···,rd). It is assumed that m1> d. Suppose
each player’s performance evaluation of each role is known. The overall performance evaluation of the team
is the sum of each selected player’s performance evaluation of its assigned role. The optimal strategy is to
10Published in Transactions on Machine Learning Research (02/2023)
2.0 2.5 3.0 3.5
ln(samplesize)10
8
6
4
2
0ln(time)
dependent case, p=1
2.0 2.5 3.0 3.5
ln(samplesize)10
8
6
4
2
0
dependent case, p=2
2.0 2.5 3.0 3.5
ln(samplesize)10
8
6
4
2
0
independent case, p=1
2.0 2.5 3.0 3.5
ln(samplesize)10
8
6
4
2
0
independent case, p=2
modified Hungarian average modified Hungarian best modified Hungarian worst Hungarian average Hungarian best Hungarian worst
Figure 8: Comparison with the Hungarian algorithm on CIFAR10
2.0 2.5 3.0 3.5
ln(samplesize)10
8
6
4
2
0ln(time)
dependent case, p=1
2.0 2.5 3.0 3.5
ln(samplesize)10
8
6
4
2
0
dependent case, p=2
2.0 2.5 3.0 3.5
ln(samplesize)10
8
6
4
2
0
independent case, p=1
2.0 2.5 3.0 3.5
ln(samplesize)10
8
6
4
2
0
independent case, p=2
modified Hungarian average modified Hungarian best modified Hungarian worst Hungarian average Hungarian best Hungarian worst
Figure 9: Comparison with the Hungarian algorithm on Wisconsin breast cancer data
maximize the overall performance evaluation of the team. The coach should solve the following optimization
problem:
max
A∈Am1/summationdisplay
i=1d/summationdisplay
j=1AijPij,A=

Aij={0,1}/vextendsingle/vextendsingle/vextendsingle/vextendsingled/summationdisplay
j=1Aij≤1,m1/summationdisplay
i=1Aij=rj,∀i= 1,···,m1;j= 1,···,d

,
wherePij≥0denotes player ai’s performance evaluation of role j,Aij= 1means player aiis selected as
rolejwhileAij= 0means the player is not selected as role j.
Ifm1=/summationtextd
j=1rj, the optimization problem above belongs to the special type of OT problem (3), where
n=d,m =/summationtextd
j=1rj. The modified Hungarian algorithm could be applied to find the optimal strategy, and
the resulting computational complexity is O(dm2
1).
Ifm1>/summationtextd
j=1rj, we can’t apply the modified Hungarian algorithm directly. To make the problem tractable,
we create one more role, and each player’s performance evaluation of this role is 0. Players who are not
selected are ‘assigned’ to this role by default. In this scenario, our goal is to solve the following optimization
problem:
max
A†∈A†m1/summationdisplay
i=1d+1/summationdisplay
j=1A†
ijP†
ij,A†=

A†
ij={0,1}/vextendsingle/vextendsingle/vextendsingle/vextendsingled+1/summationdisplay
j=1A†
ij= 1,m1/summationdisplay
i=1A†
ij=/braceleftigg
rj 1≤j≤d
m1−/summationtext4
j=1rjj=d+ 1

,
where we append Pby adding one more column of zeros to get P†. It belongs to the special type of OT
problem, where n=d+ 1,m=m1. Then, we could apply the modified Hungarian algorithm to solve the
problem, and the resulting computational complexity is O((d+ 1)m2
1).
Note that the computational order of applying the algorithm developed by Zhu et al. (2011) is O(m3
1), which
is worse than the proposed modified Hungarian algorithm.
Example 2 (many-to-many assignment problem): The following example is an agent-task assignment
problem mentioned by Zhu et al. (2016). Assume there are m2tasks (t1,···,tm2) andn1agents (a1,···,an1)
in total. It is assumed that n1<m 2. Each task should be undertaken by many agents, and each agent can
perform many tasks. To be more specific, task timust be assigned to liagents, agent ajcan perform at most
11Published in Transactions on Machine Learning Research (02/2023)
2.0 2.5 3.0 3.5
ln(samplesize)10
8
6
4
2
02ln(time)
dependent case, p=1
2.0 2.5 3.0 3.5
ln(samplesize)10
8
6
4
2
02
dependent case, p=2
2.0 2.5 3.0 3.5
ln(samplesize)10
8
6
4
2
02
independent case, p=1
2.0 2.5 3.0 3.5
ln(samplesize)10
8
6
4
2
02
independent case, p=2
modified Hungarian average modified Hungarian best modified Hungarian worst Hungarian average Hungarian best Hungarian worst
Figure 10: Comparison with the Hungarian algorithm on DOT-benchmark with 512×512resolution
2.0 2.5 3.0 3.5
ln(samplesize)10
8
6
4
2
024ln(time)
dependent case, p=1
2.0 2.5 3.0 3.5
ln(samplesize)10
8
6
4
2
024
dependent case, p=2
2.0 2.5 3.0 3.5
ln(samplesize)10
8
6
4
2
024
independent case, p=1
2.0 2.5 3.0 3.5
ln(samplesize)10
8
6
4
2
024
independent case, p=2
modified Hungarian average modified Hungarian best modified Hungarian worst sinkhorn average sinkhorn best sinkhorn worst
Figure 11: Comparison with the Sinkhorn algorithm on synthetic data
sjtasks. Suppose the performance evaluation of each agent performing each task is known. The optimal
assignment plan is to maximize the overall performance. The resulting optimization problem is as follows:
max
A′∈A′m2/summationdisplay
i=1n1/summationdisplay
j=1A′
ijP′
ij,A′=

A′
ij={0,1}/vextendsingle/vextendsingle/vextendsingle/vextendsinglen1/summationdisplay
j=1A′
ij=li,m2/summationdisplay
i=1A′
ij≤sj,∀i= 1,···,m2;j= 1,···,n1

,
whereP′
ij≥0denotes agent aj’s performance evaluation on task ti,A′
ij= 1means that agent ajis assigned
to perform task tiwhileA′
ij= 0means that agent ajis not assigned to perform task ti.
If/summationtextm2
i=1li=/summationtextn1
j=1sj, the optimization problem follows the formulation of the problem (4), where n=
n1,M=/summationtextn1
j=1sj. We could apply the modified Hungarian algorithm to find the optimal assignment plan,
and the resulting computational complexity is O(n1(/summationtextn1
j=1sj)2).
If/summationtextm2
i=1li</summationtextn1
j=1sj, we create one more task which must be performed by (/summationtextn1
j=1sj−/summationtextm2
i=1li)agents, and
each agent’s performance of this new task equals 0. This reformulation promises that each agent performs
the maximum amount of tasks. Accordingly, we need to solve the following optimization problem:
max
A‡∈A‡m2+1/summationdisplay
i=1n1/summationdisplay
j=1A‡
ijP‡
ij,A‡=

A‡
ij={0,1}/vextendsingle/vextendsingle/vextendsingle/vextendsinglen1/summationdisplay
j=1A‡
ij=/braceleftigg
li 1≤i≤m2/summationtextn1
j=1sj−/summationtextm2
i=1lij=m2+ 1,m2+1/summationdisplay
i=1A‡
ij=sj,

,
where we append P′by adding one more row of zeros to get P‡. It follows the formulation of problem
(4), wheren=n1,M=/summationtextn1
j=1sj. We could adopt the method introduced in Section 3.5, and the resulting
computation complexity is O(n1(/summationtextn1
j=1sj)2).
NotethatthecomputationalorderofapplyingthealgorithmdevelopedbyZhuetal.(2016)is O((/summationtextn1
j=1sj)3),
which has a higher computational burden than our proposed method.
6 Numerical experiments
In this section, we carry out experiments on the Wasserstein independence test problem on a synthetic
dataset, CIFAR101(Krizhevsky et al., 2009), Wisconsin breast cancer dataset2(Dua & Graff, 2017) and
1https://www.cs.toronto.edu/~kriz/cifar.html
2https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic)
12Published in Transactions on Machine Learning Research (02/2023)
2.5 3.0 3.5 4.0
ln(samplesize)10
8
6
4
2
02468ln(time)
dependent case, p=1
2.5 3.0 3.5 4.0
ln(samplesize)10
8
6
4
2
02468
dependent case, p=2
2.5 3.0 3.5 4.0
ln(samplesize)10
8
6
4
2
02468
independent case, p=1
2.5 3.0 3.5 4.0
ln(samplesize)10
8
6
4
2
02468
independent case, p=2
modified Hungarian average modified Hungarian best modified Hungarian worst sinkhorn average sinkhorn best sinkhorn worst
Figure 12: Comparison with the Sinkhorn algorithm on CIFAR10
2.0 2.5 3.0 3.5
ln(samplesize)10
8
6
4
2
0ln(time)
dependent case, p=1
2.0 2.5 3.0 3.5
ln(samplesize)10
8
6
4
2
0
dependent case, p=2
2.0 2.5 3.0 3.5
ln(samplesize)10
8
6
4
2
0
independent case, p=1
2.0 2.5 3.0 3.5
ln(samplesize)10
8
6
4
2
0
independent case, p=2
modified Hungarian average modified Hungarian best modified Hungarian worst sinkhorn average sinkhorn best sinkhorn worst
Figure 13: Comparison with the Sinkhorn algorithm on Wisconsin breast cancer data
DOT-benmark (Schrieber et al., 2016). We compare the proposed modified Hungarian algorithm with
the Hungarian algorithm, the Sinkhorn algorithm, and the network simplex algorithm. The numerical
results show the favorability of applying the proposed algorithm over the Hungarian algorithm, the Sinkhorn
algorithm, and the network simplex algorithm.
6.1 Experiment setting
Our algorithm is adaptive to any metric. The foregoing experiments are based on the lpnorm-based metric:
d((xi,yj),(xk,yl)) =∥xi−xk∥p+∥yj−yl∥p. More specifically, we examine how the modified Hungarian
algorithm, the Hungarian algorithm and the Sinkhorn algorithm perform when p= 1andp= 2. We
create one dependent case and one independent case with different sample sizes for each dataset and run
the algorithms on each case 10 times. We plot the worst, best and average number of numerical operations
and/or running time for each case.
Synthetic data : Suppose that there are independent variables X∼N(5110,30I10), where110is a 10-
dimensional vector with all ones and I10is the identity matrix; and Y= (Y1,..,Y 25)T, whereYi’s are
independent and follow Unif (10,20). We calculate the empirical Wasserstein distance in (1) independent
case: between XandY; (2)dependent case : between XandZ(whereZ=X1+Y1,X1is the first 5
coordinates of X,Y1is the first 5 coordinates of Y).
Breast cancer data: There are 569 instances, and each instance possesses 30 features. Each instance is a
30-dimensional vector, and we rescale the components to [0,1]. There are two classes of instances: benign
and malignant. Let X∈R30be the distribution generated uniformly from the benign class, and Y∈R30be
the distribution generated uniformly from the malignant class. We calculate empirical Wasserstein distance
in (1)independent case : between X1andY2(whereX1is the first 5 coordinates of X,Y2the last 25
coordinates of Y); (2)dependent case : betweenXandZ(whereZ=X1∗Y1,X1is the first 5 coordinates
ofX,Y1is the first 5 coordinates of Y,∗means the coordinate-wise product).
CIFAR10 : Each image in CIFAR10 contains 32×32pixels, and each pixel is composed of 3 color channels.
Each image is essentially a 3072-dimensional vector. Then, we rescale the vector components to [0,1].
SupposeX∈R3072is the distribution generated uniformly from the images of classes: airplane, automobile,
bird, cat, and deer; Y∈R3072is the distribution generated uniformly from the images of other five classes.
We calculate the empirical Wasserstein distance in (1) independent case : betweenXandY1(whereY1is the
13Published in Transactions on Machine Learning Research (02/2023)
2.0 2.5 3.0 3.5
ln(samplesize)10.0
7.5
5.0
2.5
0.02.55.07.5ln(time)
dependent case, p=1
2.0 2.5 3.0 3.5
ln(samplesize)10.0
7.5
5.0
2.5
0.02.55.07.5
dependent case, p=2
2.0 2.5 3.0 3.5
ln(samplesize)10.0
7.5
5.0
2.5
0.02.55.07.5
independent case, p=1
2.0 2.5 3.0 3.5
ln(samplesize)10.0
7.5
5.0
2.5
0.02.55.07.5
independent case, p=2
modified Hungarian average modified Hungarian best modified Hungarian worst sinkhorn average sinkhorn best sinkhorn worst
Figure 14: Comparison with the Sinkhorn algorithm on DOT-benchmark with 512×512resolution
2.0 2.5 3.0 3.5
ln(samplesize)10
8
6
4
2
02ln(time)
dependent case, p=1
2.0 2.5 3.0 3.5
ln(samplesize)10
8
6
4
2
02
dependent case, p=2
2.0 2.5 3.0 3.5
ln(samplesize)10
8
6
4
2
02
independent case, p=1
2.0 2.5 3.0 3.5
ln(samplesize)10
8
6
4
2
02
independent case, p=2
modified Hungarian average modified Hungarian best modified Hungarian worst Network simplex average Network simplex best Network simplex worst
Figure 15: Comparison with the network simplex algorithm on synthetic data
first 1536 coordinates of Y); (2)dependent case : betweenXandZ(whereZ=X2/2 +Y1/2,X2is the last
1536 coordinates of X,Y1is the first 1536 coordinates of Y).
DOT-benchmark contains images with different resolutions from 10 classes. Each image is essentially a r×r-
dimensionalvector, where r= 32,64,128,256,512. Then, werescalethevectorcomponentsto [0,1]. Suppose
X∈Rr×ris the distribution generated uniformly from the images of classes: GRFrough, RFmoderate,
CauchyDensity, MicroscopyImages, Shapes; Y∈Rr×ris the distribution generated uniformly from the
imagesofotherfiveclasses. WecalculatetheempiricalWassersteindistancein(1) independent case : between
XandY; (2)dependent case : betweenXandZ(whereZ=X/2 +Y/2). To save space, we relegate the
numerical results when r= 32,64,128,256to the Appendix.
6.2 Comparison with the Hungarian algorithm
We compare the modified Hungarian algorithm with the classic Hungarian algorithm. The results in terms
of numerical operations are presented in Figure 3, 4, 5, 6. The figures illustrate that the proposed algorithm
gains a factor nin computational complexity when solving the proposed special type of OT problem. To be
more specific, notice that the slope of ln(number of numerical operations) over ln(sample size) indicates the
order of the associated algorithm, and the slope of our algorithm is around 5 while the slope of the Hungarian
algorithm is around 6. This observation implies that the order of applying our algorithm is O(n5)while the
order of applying the Hungarian algorithm is O(n6). Such observations are consistent with our theoretical
results.
The results in terms of running time are presented in Figure 7, 8, 9,10. One may observe that for almost all
instances, especially for larger sample size n, the modified Hungarian algorithm is faster than the Hungarian
algorithm. It indicates the practical improvement of the modified Hungarian algorithm over the classic
Hungarian algorithm.
6.3 Comparison with the Sinkhorn algorithm
We compare the modified Hungarian algorithm with the Sinkhorn algorithm. Among the state-of-the-art
approximation solvers for OT problems, the first-order approximation algorithms (Dvurechensky et al., 2018;
Lin et al., 2019b; Guo et al., 2020) are mainly employed to solve the balanced case ( m=n). The Sinkhorn
algorithm could deal with the unbalanced scenario ( m̸=n) and is widely used in all kinds of OT-related
14Published in Transactions on Machine Learning Research (02/2023)
2.0 2.5 3.0 3.5
ln(samplesize)10
8
6
4
2
02ln(time)
dependent case, p=1
2.0 2.5 3.0 3.5
ln(samplesize)10
8
6
4
2
02
dependent case, p=2
2.0 2.5 3.0 3.5
ln(samplesize)10
8
6
4
2
02
independent case, p=1
2.0 2.5 3.0 3.5
ln(samplesize)10
8
6
4
2
02
independent case, p=2
modified Hungarian average modified Hungarian best modified Hungarian worst Network simplex average Network simplex best Network simplex worst
Figure 16: Comparison with the network simplex algorithm on CIFAR10
2.0 2.5 3.0 3.5
ln(samplesize)10
8
6
4
2
02ln(time)
dependent case, p=1
2.0 2.5 3.0 3.5
ln(samplesize)10
8
6
4
2
02
dependent case, p=2
2.0 2.5 3.0 3.5
ln(samplesize)10
8
6
4
2
02
independent case, p=1
2.0 2.5 3.0 3.5
ln(samplesize)10
8
6
4
2
02
independent case, p=2
modified Hungarian average modified Hungarian best modified Hungarian worst Network simplex average Network simplex best Network simplex worst
Figure 17: Comparison with the network simplex algorithm on Wisconsin breast cancer data
models. Therefore, we choose the Sinkhorn algorithm as the baseline and then investigate the performance of
the Sinkhorn algorithm in the Wasserstein-distance-based independence test problem. When we implement
the Sinkhorn algorithm, we set the regularization parameter as 0.1and the accuracy as 0.0001.
The results in terms of running time are presented in Figure 11, 12, 13,14. We relegate the results in
terms of numerical operations to the Appendix. For most of the scenarios, the average running time of
the modified Hungarian algorithm is less than the Sinkhorn algorithm. Moreover, the performance of the
modified Hungarian algorithm has a lower variance than the Sinkhorn algorithm. The results demonstrate
that our proposed modified Hungarian algorithm should be chosen if one is interested in obtaining solutions
with a high accuracy.
6.4 Comparison with the network simplex algorithm
We compare the modified Hungarian algorithm with the network simplex algorithm. The results are pre-
sented in Figure 15, 16, 17, 18. We run the network simplex from networkX library in Python. Considering
the package requires integer-valued input, we round the costs to the nearest integers from the below. Ac-
cording to the experimental results, we could conclude that the modified Hungarian algorithm is superior to
the network simplex algorithm.
7 Discussion
A modified Hungarian algorithm is developed to efficiently solve a wide range of OT problems. Theoretical
analysis and numerical experiments demonstrate that the proposed algorithm compares favorably with the
Hungarian algorithm and the Sinkhorn algorithm. In addition to the computational aspects, broad applica-
tions are explored, including the Wasserstein-distance-based independence test, the one-to-many assignment
problem and the many-to-many assignment problem. The many-to-many assignment problem closely relates
to practical problems involving service assignment problems (Ng et al., 2008), sensor networks (Bhardwaj
& Chandrakasan, 2002), and access control (Ahn & Hu, 2007). Future work along this line is to apply the
proposed algorithm to problems involving engineering and control. Also, there is some possibility of applying
the proposed algorithm to some unsupervised learning problems. For example, the clustering problem could
be formulated as an OT problem (Genevay et al., 2019). Assume that there are nclusters and msamples
in total, and each cluster has mjsamples. If we want to identify the cluster assignment to minimize the
15Published in Transactions on Machine Learning Research (02/2023)
2.0 2.5 3.0 3.5
ln(samplesize)10
8
6
4
2
02ln(time)
dependent case, p=1
2.0 2.5 3.0 3.5
ln(samplesize)10
8
6
4
2
02
dependent case, p=2
2.0 2.5 3.0 3.5
ln(samplesize)10
8
6
4
2
02
independent case, p=1
2.0 2.5 3.0 3.5
ln(samplesize)10
8
6
4
2
02
independent case, p=2
modified Hungarian average modified Hungarian best modified Hungarian worst Network simplex average Network simplex best Network simplex worst
Figure 18: Comparison with the network simplex algorithm on DOT-benmark with 512×512resolution
‘distance’ between cluster ‘centers’ and the associated assigned samples, we are solving the special type of
OT problem in this paper. The future work along this line may be to find a scheme to determine ‘distance’
and ‘centers’ to promise desirable model performances.
Acknowledgement
The authors would like to thank the Action Editor and anonymous reviewers for their detailed and construc-
tive comments, which enhanced the quality and presentation of the manuscript.
This project is partially supported by the Transdisciplinary Research Institute for Advancing Data Science
(TRIAD), https://research.gatech.edu/data/triad , which is a part of the TRIPODS program at NSF
and locates at Georgia Tech, enabled by the NSF grant CCF-1740776. The authors are also partially
sponsored by NSF grants 2015363.
References
Gail-Joon Ahn and Hongxin Hu. Towards realizing a formal RBAC model in real systems. In Proceedings
of the 12th ACM symposium on Access control models and technologies , pp. 215–224, 2007.
Ravindra K Ahuja, Thomas L Magnanti, and James B Orlin. Network flows. 1988.
Richard S Barr, Fred Glover, and Darwin Klingman. A new alternating basis algorithm for semi-assignment
networks . Business Research Division, Graduate School of Business Administration ..., 1977.
Dimitri P Bertsekas. The auction algorithm: A distributed relaxation method for the assignment problem.
Annals of operations research , 14(1):105–123, 1988.
Dimitri P Bertsekas and Jonathan Eckstein. Dual coordinate step methods for linear network flow problems.
Mathematical Programming , 42(1):203–243, 1988.
Dimitris Bertsimas and John N Tsitsiklis. Introduction to linear optimization , volume 6. Athena Scientific
Belmont, MA, 1997.
Manish Bhardwaj and Anantha P Chandrakasan. Bounding the lifetime of sensor networks via optimal
role assignments. In Proceedings. Twenty-First Annual Joint Conference of the IEEE Computer and
Communications Societies , volume 3, pp. 1587–1596. IEEE, 2002.
Garrett Birkhoff. Three observations on linear algebra. Univ. Nac. Tacuman, Rev. Ser. A , 5:147–151, 1946.
Rainer Burkard, Mauro Dell’Amico, and Silvano Martello. Assignment problems: revised reprint . SIAM,
2012.
Marco Cuturi. Sinkhorn distances: Lightspeed computation of optimal transport. Advances in neural
information processing systems , 26, 2013.
Nabarun Deb and Bodhisattva Sen. Multivariate rank-based distribution-free nonparametric testing using
measure transportation. Journal of the American Statistical Association , pp. 1–16, 2021.
16Published in Transactions on Machine Learning Research (02/2023)
Yihe Dong, Yu Gao, Richard Peng, Ilya Razenshteyn, and Saurabh Sawlani. A study of performance of
optimal transport. arXiv preprint arXiv:2005.01182 , 2020.
Dheeru Dua and Casey Graff. UCI machine learning repository, 2017. URL http://archive.ics.uci.edu/
ml.
Pavel Dvurechensky, Alexander Gasnikov, and Alexey Kroshnin. Computational optimal transport: Com-
plexity by accelerated gradient descent is better than by Sinkhorn’s algorithm. In International conference
on machine learning , pp. 1367–1376. PMLR, 2018.
Aude Genevay, Gabriel Dulac-Arnold, and Jean-Philippe Vert. Differentiable deep clustering with cluster
size constraints. arXiv preprint arXiv:1910.09036 , 2019.
Wenshuo Guo, Nhat Ho, and Michael Jordan. Fast algorithms for computational optimal transport and
Wasserstein barycenter. In International Conference on Artificial Intelligence and Statistics , pp. 2088–
2097. PMLR, 2020.
Roy Jonker and Anton Volgenant. A shortest augmenting path algorithm for dense and sparse linear assign-
ment problems. Computing , 38(4):325–340, 1987.
J Kennington and Zhiming Wang. A shortest augmenting path algorithm for the semi-assignment problem.
Operations Research , 40(1):178–187, 1992.
Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.
Harold W Kuhn. The hungarian method for the assignment problem. Naval research logistics quarterly , 2
(1-2):83–97, 1955.
Tianyi Lin, Nhat Ho, and Michael Jordan. On efficient optimal transport: An analysis of greedy and
accelerated mirror descent algorithms. In International Conference on Machine Learning , pp. 3982–3991.
PMLR, 2019a.
Tianyi Lin, Nhat Ho, and Michael I Jordan. On the efficiency of Sinkhorn and Greenkhorn and their
acceleration for optimal transport. arXiv preprint arXiv:1906.01437 , 2019b.
Lang Liu, Soumik Pal, and Zaid Harchaoui. Entropy regularized optimal transport independence criterion.
InInternational Conference on Artificial Intelligence and Statistics , pp. 11247–11279. PMLR, 2022.
Gilles Mordant and Johan Segers. Measuring dependence between random vectors via optimal transport.
Journal of Multivariate Analysis , 189:104912, 2022.
James Munkres. Algorithms for the assignment and transportation problems. Journal of the society for
industrial and applied mathematics , 5(1):32–38, 1957.
Vincent TY Ng, Boris Chan, Louis LY Shun, and Ringo Tsang. Quality service assignments for role-based
web services. In 2008 IEEE International Conference on Systems, Man and Cybernetics , pp. 2219–2224.
IEEE, 2008.
Thomas Giacomo Nies, Thomas Staudt, and Axel Munk. Transport dependency: Optimal transport based
dependency measures. arXiv preprint arXiv:2105.02073 , 2021.
James B Orlin. A polynomial time primal network simplex algorithm for minimum cost flows. Mathematical
Programming , 78(2):109–129, 1997.
Gabriel Peyré and Marco Cuturi. Computational optimal transport. Foundations and Trends in Machine
Learning , 11(5-6):355–607, 2019.
Mauricio GC Resende and Panos M Pardalos. Interior point algorithms for network flow problems. Advances
in linear and integer programming , pp. 147–187, 1996.
17Published in Transactions on Machine Learning Research (02/2023)
Mauricio GC Resende and Geraldo Veiga. An implementation of the dual affine scaling algorithm for
minimum-cost flow on bipartite uncapacitated networks. SIAM Journal on Optimization , 3(3):516–537,
1993.
Jörn Schrieber, Dominic Schuhmacher, and Carsten Gottschlich. Dotmark–a benchmark for discrete optimal
transport. IEEE Access , 5:271–282, 2016.
Hongjian Shi, Mathias Drton, and Fang Han. Distribution-free consistent independence tests via center-
outward ranks and signs. Journal of the American Statistical Association , pp. 1–16, 2020.
Subhash Suri. Bipartite matching & the Hungarian method. Notes, Department of Computer Science,
University of California, Santa Barbara , 8, 2006.
Robert E Tarjan. Dynamic trees as search trees via euler tours, applied to the network simplex algorithm.
Mathematical Programming , 78(2):169–177, 1997.
Pravin M Vaidya. Speeding-up linear programming using fast matrix multiplication. In 30th annual sympo-
sium on foundations of computer science , pp. 332–337. IEEE Computer Society, 1989.
Stephen A Vavasis and Yinyu Ye. An accelerated interior point method whose running time depends only
on a. In Proceedings of the twenty-sixth annual ACM symposium on Theory of Computing , pp. 512–521,
1994.
Cédric Villani. Optimal transport: old and new , volume 338. Springer, 2009.
Johannes CW Wiesel. Measuring association with Wasserstein distances. Bernoulli , 28(4):2816–2832, 2022.
Yiling Xie, Yiling Luo, and Xiaoming Huo. An accelerated stochastic algorithm for solving the optimal
transport problem. arXiv preprint arXiv:2203.00813 , 2022.
Quey-Jen Yeh. A reduced dual affine scaling algorithm for solving assignment and transportation problems .
Columbia University, 1989.
Haibin Zhu, MengChu Zhou, and Rob Alkins. Group role assignment via a Kuhn–Munkres algorithm-based
solution. IEEE Transactions on Systems, Man, and Cybernetics-Part A: Systems and Humans , 42(3):
739–750, 2011.
Haibin Zhu, Dongning Liu, Siqin Zhang, Yu Zhu, Luyao Teng, and Shaohua Teng. Solving the many to many
assignment problem by improving the Kuhn–Munkres algorithm with backtracking. Theoretical Computer
Science, 618:30–41, 2016.
A Appendix
A.1 Proof of Proposition 1
Proof.We first consider the following two optimization problems (8), (9):
min
X1∈U1m/summationdisplay
i=1m/summationdisplay
j=1X1
ijC‡
ij,U1=

X1
ij≥0/vextendsingle/vextendsingle/vextendsingle/vextendsinglem/summationdisplay
j=1X1
ij=1
m,m/summationdisplay
i=1X1
ij=1
m,∀i,j= 1,···,m

.(8)
whereC‡is anm×mmatrix generated by duplicating the jth column of C mjtimes:
C‡
it=/braceleftigg
Ci11≤t≤m1,
Cijm1+···+mj−1+ 1≤t≤m1+···+mj,2≤j≤n.
min
X‡∈U‡m/summationdisplay
i=1m/summationdisplay
j=11
mX‡
ijC‡
ij,U‡=

X‡
ij={0,1}/vextendsingle/vextendsingle/vextendsingle/vextendsinglem/summationdisplay
j=1X‡
ij= 1,m/summationdisplay
i=1X‡
ij= 1,∀i,j= 1,···,m

,(9)
18Published in Transactions on Machine Learning Research (02/2023)
Then, we denote the objective functions of problems (2), (3), (8) and (9) by f′(X′),f(X),f1(X1), and
f‡(X‡), respectively.
Firstly, we prove (2) ⇐⇒(8).
On one hand, for any X1∈U1, if we let
X′
ij=m1/summationdisplay
t=1X1
it, j = 1,
X′
ij=m1+···+mj/summationdisplay
t=m1+···+mj−1+1X1
it,2≤j≤n,
then we have
X′
ij≥0,
n/summationdisplay
j=1X′
ij=m1/summationdisplay
t=1X1
it+m1+···+mj/summationdisplay
t=m1+···+mj−1+1X1
it=m/summationdisplay
t=1X1
it=1
m,
m/summationdisplay
i=1X′
ij=m/summationdisplay
i=1m1/summationdisplay
t=1X1
it=m1
m, j = 1,
m/summationdisplay
i=1X′
ij=m/summationdisplay
i=1m1+···+mj/summationdisplay
t=m1+···+mj−1+1X1
it=mj
m,2≤j≤n.
Thus,X′∈U′.
For the objective functions, we have the following:
f′(X′) =m/summationdisplay
i=1n/summationdisplay
j=1X′
ijCij=m/summationdisplay
i=1
m1/summationdisplay
t=1X1
itC1
it+m1+···+mj/summationdisplay
t=m1+···+mj−1+1X1
itC1
it
=m/summationdisplay
i=1m/summationdisplay
t=1X1
itC1
it=f1(X1).
On the other hand, for any X′∈U, if we let
X1
it=/braceleftigg
X′
i1/m11≤t≤m1
X′
ij/mjm1+···+mj−1+ 1≤t≤m1+···+mj,2≤j≤n,
then we have
X1
it≥0,
m/summationdisplay
t=1X1
it=n/summationdisplay
j=1X′
ij
mjmj=n/summationdisplay
j=1X′
ij=1
m,
n/summationdisplay
i=1X1
it=n/summationdisplay
i=1X′
ij
mj=1
mjn/summationdisplay
i=1X′
ij=1
m.
Thus,X1∈U1.
For the objective functions, we have the following:
f1(X1) =m/summationdisplay
i=1m/summationdisplay
t=1X1
itC1
it=m/summationdisplay
i=1n/summationdisplay
j=1X′
ij
mjCitmj=f′(X′).
Hence, (2)⇐⇒(8).
By Birkhoff’s theorem, we know (8) ⇐⇒(9). Therefore, we have (2) ⇐⇒(9).
19Published in Transactions on Machine Learning Research (02/2023)
Similarly, for any X‡∈U‡, if we let
Xij=m1/summationdisplay
t=1X‡
it, j = 1,
Xij=m1+···+mj/summationdisplay
t=m1+···+mj−1+1X‡
it,2≤j≤n,
then we have X∈Uandf‡(X‡) =f(X).
For anyX∈U, if we let
X‡
it=/braceleftigg
Xi1/m1,1≤t≤m1
Xij/mjm1+···+mj−1+ 1≤t≤m1+···+mj,2≤j≤n
then we have X‡∈U‡andf‡(X‡) =f(X).
Therefore, (3)⇐⇒(9).
In conclusion, we have (3) ⇐⇒(9)⇐⇒(2).
A.2 Proof of Theorem 1
Proof.Denote the edge e∈Ebye= (ev1,ev2). LetPM′be any perfect pseudo-matching in G(not
necessarily in the equality graph El). Andvi
1,i= 1,···,m;vj
2,j= 1,···,nare nodes from V1andV2,
respectively. Since vi
1∈V1is covered exactly once by PM′, andvj
2∈V2is covered exactly mjtimes by
PM′, we have
w(PM′) =/summationdisplay
e∈PM′w(e)≤/summationdisplay
e∈PM′(l(ev1) +l(ev2)) =m/summationdisplay
i=1l(vi
1) +n/summationdisplay
j=1mjl(vj
2),
where the first inequality comes from the definition of feasible labeling.
Thus,/summationtextm
i=1l(vi
1) +/summationtextn
j=1mjl(vj
2)is the upper bound of the weight of any perfect pseudo-matching. Then
letPMbe a perfect pseudo-matching in the equality graph El, we have
w(PM) =/summationdisplay
e∈PMw(e) =m/summationdisplay
i=1l(vi
1) +n/summationdisplay
j=1mjl(vj
2).
Hencew(PM′)≤w(PM), andPMis the maximum weighted pseudo-matching.
A.3 Proof of Proposition 2
Proof.We denote the objective functions of problems (4) and (6) by g∗(X∗), andg†(X†), respectively.
On one hand, for any X†∈U†, if we let
X∗
ij=n1/summationdisplay
t=1X†
tj, i= 1,
X∗
ij=n1+···+ni/summationdisplay
t=n1+···+ni−1+1X†
tj,2≤i≤m,
then we have
X∗
ij≥0,
20Published in Transactions on Machine Learning Research (02/2023)
m/summationdisplay
i=1X∗
ij=n1/summationdisplay
t=1X†
tj+n1+···+ni/summationdisplay
t=n1+···+ni−1+1X†
tj=M/summationdisplay
t=1X†
tj=mj
M,
n/summationdisplay
j=1X∗
ij=n/summationdisplay
j=1n1/summationdisplay
t=1X†
tj=n1
M, i= 1,
m/summationdisplay
i=1X∗
ij=m/summationdisplay
i=1n1+···+ni/summationdisplay
t=n1+···+ni−1+1X†
it=ni
m,2≤i≤m.
Thus,X∗∈U∗.
For the objective function, we have the following:
g∗(X∗) =M/summationdisplay
i=1n/summationdisplay
j=1X∗
ijCij=n/summationdisplay
j=1
n1/summationdisplay
t=1X†
tjCtj+n1+···+ni/summationdisplay
t=n1+···+ni−1+1X†
itCit
=n/summationdisplay
j=1M/summationdisplay
t=1X†
itC†
it=g†(X†).
On the other hand, for any X∗∈U∗, if we let
X†
tj=/braceleftigg
X∗
1j/n1,1≤t≤n1
X∗
ij/nin1+···+ni−1+ 1≤t≤n1+···+ni,2≤i≤m.
then we have
X†
ij≥0,
n/summationdisplay
j=1X†
ij=n/summationdisplay
j=1X∗
ij
ni=ni
M,
M/summationdisplay
i=1X†
ij=M/summationdisplay
i=1X∗
ij
nini=m/summationdisplay
i=1X∗
ij=mj
M.
Thus,X†∈U†.
For the objective function, we have the following:
g†(X†) =M/summationdisplay
i=1n/summationdisplay
j=1X†
ijC†
ij=n/summationdisplay
j=1m/summationdisplay
i=1X∗
ij
niCitni=g∗(X∗).
Hence, (4)⇐⇒(6).
A.4 Additional experiment results
2.0 2.5 3.0 3.5
ln(samplesize)468101214161820ln(#operation)
dependent case, p=1
2.0 2.5 3.0 3.5
ln(samplesize)468101214161820
dependent case, p=2
2.0 2.5 3.0 3.5
ln(samplesize)468101214161820
independent case, p=1
2.0 2.5 3.0 3.5
ln(samplesize)468101214161820
independent case, p=2
modified Hungarian average modified Hungarian best modified Hungarian worst Hungarian average Hungarian best Hungarian worst
Figure 19: Comparison with the Hungarian algorithm on DOT-benchmark with 32×32resolution w.r.t.
numerical operations
21Published in Transactions on Machine Learning Research (02/2023)
2.0 2.5 3.0 3.5
ln(samplesize)468101214161820ln(#operation)
dependent case, p=1
2.0 2.5 3.0 3.5
ln(samplesize)468101214161820
dependent case, p=2
2.0 2.5 3.0 3.5
ln(samplesize)468101214161820
independent case, p=1
2.0 2.5 3.0 3.5
ln(samplesize)468101214161820
independent case, p=2
modified Hungarian average modified Hungarian best modified Hungarian worst Hungarian average Hungarian best Hungarian worst
Figure 21: Comparison with the Hungarian algorithm on DOT-benchmark with 128×128resolution w.r.t.
numerical operations
2.0 2.5 3.0 3.5
ln(samplesize)468101214161820ln(#operation)
dependent case, p=1
2.0 2.5 3.0 3.5
ln(samplesize)468101214161820
dependent case, p=2
2.0 2.5 3.0 3.5
ln(samplesize)468101214161820
independent case, p=1
2.0 2.5 3.0 3.5
ln(samplesize)468101214161820
independent case, p=2
modified Hungarian average modified Hungarian best modified Hungarian worst Hungarian average Hungarian best Hungarian worst
Figure 22: Comparison with the Hungarian algorithm on DOT-benchmark with 256×256resolution w.r.t.
numerical operations
2.0 2.5 3.0 3.5
ln(samplesize)468101214161820ln(#operation)
dependent case, p=1
2.0 2.5 3.0 3.5
ln(samplesize)468101214161820
dependent case, p=2
2.0 2.5 3.0 3.5
ln(samplesize)468101214161820
independent case, p=1
2.0 2.5 3.0 3.5
ln(samplesize)468101214161820
independent case, p=2
modified Hungarian average modified Hungarian best modified Hungarian worst Hungarian average Hungarian best Hungarian worst
Figure 20: Comparison with the Hungarian algorithm on DOT-benchmark with 64×64resolution w.r.t.
numerical operations
22Published in Transactions on Machine Learning Research (02/2023)
2.5 3.0 3.5 4.0
ln(samplesize)10
8
6
4
2
0246ln(time)
dependent case, p=1
2.5 3.0 3.5 4.0
ln(samplesize)10
8
6
4
2
0246
dependent case, p=2
2.5 3.0 3.5 4.0
ln(samplesize)10
8
6
4
2
0246
independent case, p=1
2.5 3.0 3.5 4.0
ln(samplesize)10
8
6
4
2
0246
independent case, p=2
modified Hungarian average modified Hungarian best modified Hungarian worst Hungarian average Hungarian best Hungarian worst
Figure 23: Comparison with the Hungarian algorithm on DOT-benchmark with 32×32resolution w.r.t.
running time
2.0 2.5 3.0 3.5
ln(samplesize)10
8
6
4
2
02ln(time)
dependent case, p=1
2.0 2.5 3.0 3.5
ln(samplesize)10
8
6
4
2
02
dependent case, p=2
2.0 2.5 3.0 3.5
ln(samplesize)10
8
6
4
2
02
independent case, p=1
2.0 2.5 3.0 3.5
ln(samplesize)10
8
6
4
2
02
independent case, p=2
modified Hungarian average modified Hungarian best modified Hungarian worst Hungarian average Hungarian best Hungarian worst
Figure 24: Comparison with the Hungarian algorithm on DOT-benchmark with 64×64resolution w.r.t.
running time
2.0 2.5 3.0 3.5
ln(samplesize)10
8
6
4
2
02ln(time)
dependent case, p=1
2.0 2.5 3.0 3.5
ln(samplesize)10
8
6
4
2
02
dependent case, p=2
2.0 2.5 3.0 3.5
ln(samplesize)10
8
6
4
2
02
independent case, p=1
2.0 2.5 3.0 3.5
ln(samplesize)10
8
6
4
2
02
independent case, p=2
modified Hungarian average modified Hungarian best modified Hungarian worst Hungarian average Hungarian best Hungarian worst
Figure 25: Comparison with the Hungarian algorithm on DOT-benchmark with 128×128resolution w.r.t.
running time
2.0 2.5 3.0 3.5
ln(samplesize)10
8
6
4
2
02ln(time)
dependent case, p=1
2.0 2.5 3.0 3.5
ln(samplesize)10
8
6
4
2
02
dependent case, p=2
2.0 2.5 3.0 3.5
ln(samplesize)10
8
6
4
2
02
independent case, p=1
2.0 2.5 3.0 3.5
ln(samplesize)10
8
6
4
2
02
independent case, p=2
modified Hungarian average modified Hungarian best modified Hungarian worst Hungarian average Hungarian best Hungarian worst
Figure 26: Comparison with the Hungarian algorithm on DOT-benchmark with 256×256resolution w.r.t.
running time
23Published in Transactions on Machine Learning Research (02/2023)
2.0 2.5 3.0 3.5
ln(samplesize)5.07.510.012.515.017.520.022.525.0ln(#operation)
dependent case, p=1
2.0 2.5 3.0 3.5
ln(samplesize)5.07.510.012.515.017.520.022.525.0
dependent case, p=2
2.0 2.5 3.0 3.5
ln(samplesize)5.07.510.012.515.017.520.022.525.0
independent case, p=1
2.0 2.5 3.0 3.5
ln(samplesize)5.07.510.012.515.017.520.022.525.0
independent case, p=2
modified Hungarian average modified Hungarian best modified Hungarian worst sinkhorn average sinkhorn best sinkhorn worst
Figure 27: Comparison with the Sinkhorn algorithm on synthetic data w.r.t. numerical operations
2.0 2.5 3.0 3.5
ln(samplesize)510152025ln(#operation)
dependent case, p=1
2.0 2.5 3.0 3.5
ln(samplesize)510152025
dependent case, p=2
2.0 2.5 3.0 3.5
ln(samplesize)510152025
independent case, p=1
2.0 2.5 3.0 3.5
ln(samplesize)510152025
independent case, p=2
modified Hungarian average modified Hungarian best modified Hungarian worst sinkhorn average sinkhorn best sinkhorn worst
Figure 28: Comparison with the Sinkhorn algorithm on CIFAR10 w.r.t. numerical operations
2.0 2.5 3.0 3.5
ln(samplesize)468101214161820ln(#operation)
dependent case, p=1
2.0 2.5 3.0 3.5
ln(samplesize)468101214161820
dependent case, p=2
2.0 2.5 3.0 3.5
ln(samplesize)468101214161820
independent case, p=1
2.0 2.5 3.0 3.5
ln(samplesize)468101214161820
independent case, p=2
modified Hungarian average modified Hungarian best modified Hungarian worst sinkhorn average sinkhorn best sinkhorn worst
Figure 29: Comparison with the Sinkhorn algorithm on Wisconsin cancer data w.r.t. numerical operations
2.0 2.5 3.0 3.5
ln(samplesize)5.07.510.012.515.017.520.022.525.0ln(#operation)
dependent case, p=1
2.0 2.5 3.0 3.5
ln(samplesize)5.07.510.012.515.017.520.022.525.0
dependent case, p=2
2.0 2.5 3.0 3.5
ln(samplesize)5.07.510.012.515.017.520.022.525.0
independent case, p=1
2.0 2.5 3.0 3.5
ln(samplesize)5.07.510.012.515.017.520.022.525.0
independent case, p=2
modified Hungarian average modified Hungarian best modified Hungarian worst sinkhorn average sinkhorn best sinkhorn worst
Figure 30: Comparison with the Sinkhorn algorithm on DOT-benchmark with 32×32resolution w.r.t.
numerical operations
2.0 2.5 3.0 3.5
ln(samplesize)5.07.510.012.515.017.520.022.525.0ln(#operation)
dependent case, p=1
2.0 2.5 3.0 3.5
ln(samplesize)5.07.510.012.515.017.520.022.525.0
dependent case, p=2
2.0 2.5 3.0 3.5
ln(samplesize)5.07.510.012.515.017.520.022.525.0
independent case, p=1
2.0 2.5 3.0 3.5
ln(samplesize)5.07.510.012.515.017.520.022.525.0
independent case, p=2
modified Hungarian average modified Hungarian best modified Hungarian worst sinkhorn average sinkhorn best sinkhorn worst
Figure 31: Comparison with the Sinkhorn algorithm on DOT-benchmark with 64×64resolution w.r.t.
numerical operations
24Published in Transactions on Machine Learning Research (02/2023)
2.0 2.5 3.0 3.5
ln(samplesize)5.07.510.012.515.017.520.022.525.0ln(#operation)
dependent case, p=1
2.0 2.5 3.0 3.5
ln(samplesize)5.07.510.012.515.017.520.022.525.0
dependent case, p=2
2.0 2.5 3.0 3.5
ln(samplesize)5.07.510.012.515.017.520.022.525.0
independent case, p=1
2.0 2.5 3.0 3.5
ln(samplesize)5.07.510.012.515.017.520.022.525.0
independent case, p=2
modified Hungarian average modified Hungarian best modified Hungarian worst sinkhorn average sinkhorn best sinkhorn worst
Figure 32: Comparison with the Sinkhorn algorithm on DOT-benchmark with 128×128resolution w.r.t.
numerical operations
2.0 2.5 3.0 3.5
ln(samplesize)510152025ln(#operation)
dependent case, p=1
2.0 2.5 3.0 3.5
ln(samplesize)510152025
dependent case, p=2
2.0 2.5 3.0 3.5
ln(samplesize)510152025
independent case, p=1
2.0 2.5 3.0 3.5
ln(samplesize)510152025
independent case, p=2
modified Hungarian average modified Hungarian best modified Hungarian worst sinkhorn average sinkhorn best sinkhorn worst
Figure 33: Comparison with the Sinkhorn algorithm on DOT-benchmark with 256×256resolution w.r.t.
numerical operations
2.0 2.5 3.0 3.5
ln(samplesize)510152025ln(#operation)
dependent case, p=1
2.0 2.5 3.0 3.5
ln(samplesize)510152025
dependent case, p=2
2.0 2.5 3.0 3.5
ln(samplesize)510152025
independent case, p=1
2.0 2.5 3.0 3.5
ln(samplesize)510152025
independent case, p=2
modified Hungarian average modified Hungarian best modified Hungarian worst sinkhorn average sinkhorn best sinkhorn worst
Figure 34: Comparison with the Sinkhorn algorithm on DOT-benchmark with 512×512resolution w.r.t.
numerical operations
2.0 2.5 3.0 3.5
ln(samplesize)10
8
6
4
2
0246ln(time)
dependent case, p=1
2.0 2.5 3.0 3.5
ln(samplesize)10
8
6
4
2
0246
dependent case, p=2
2.0 2.5 3.0 3.5
ln(samplesize)10
8
6
4
2
0246
independent case, p=1
2.0 2.5 3.0 3.5
ln(samplesize)10
8
6
4
2
0246
independent case, p=2
modified Hungarian average modified Hungarian best modified Hungarian worst sinkhorn average sinkhorn best sinkhorn worst
Figure 35: Comparison with the Sinkhorn algorithm on DOT-benchmark with 32×32resolution w.r.t.
running time
25Published in Transactions on Machine Learning Research (02/2023)
2.0 2.5 3.0 3.5
ln(samplesize)10
8
6
4
2
0246ln(time)
dependent case, p=1
2.0 2.5 3.0 3.5
ln(samplesize)10
8
6
4
2
0246
dependent case, p=2
2.0 2.5 3.0 3.5
ln(samplesize)10
8
6
4
2
0246
independent case, p=1
2.0 2.5 3.0 3.5
ln(samplesize)10
8
6
4
2
0246
independent case, p=2
modified Hungarian average modified Hungarian best modified Hungarian worst sinkhorn average sinkhorn best sinkhorn worst
Figure 36: Comparison with the Sinkhorn algorithm on DOT-benchmark with 64×64resolution w.r.t.
running time
2.0 2.5 3.0 3.5
ln(samplesize)10
8
6
4
2
0246ln(time)
dependent case, p=1
2.0 2.5 3.0 3.5
ln(samplesize)10
8
6
4
2
0246
dependent case, p=2
2.0 2.5 3.0 3.5
ln(samplesize)10
8
6
4
2
0246
independent case, p=1
2.0 2.5 3.0 3.5
ln(samplesize)10
8
6
4
2
0246
independent case, p=2
modified Hungarian average modified Hungarian best modified Hungarian worst sinkhorn average sinkhorn best sinkhorn worst
Figure 37: Comparison with the Sinkhorn algorithm on DOT-benchmark with 128×128resolution w.r.t.
running time
2.0 2.5 3.0 3.5
ln(samplesize)10.0
7.5
5.0
2.5
0.02.55.07.5ln(time)
dependent case, p=1
2.0 2.5 3.0 3.5
ln(samplesize)10.0
7.5
5.0
2.5
0.02.55.07.5
dependent case, p=2
2.0 2.5 3.0 3.5
ln(samplesize)10.0
7.5
5.0
2.5
0.02.55.07.5
independent case, p=1
2.0 2.5 3.0 3.5
ln(samplesize)10.0
7.5
5.0
2.5
0.02.55.07.5
independent case, p=2
modified Hungarian average modified Hungarian best modified Hungarian worst sinkhorn average sinkhorn best sinkhorn worst
Figure 38: Comparison with the Sinkhorn algorithm on DOT-benchmark with 256×256resolution w.r.t.
running time
2.0 2.5 3.0 3.5
ln(samplesize)10
8
6
4
2
02ln(time)
dependent case, p=1
2.0 2.5 3.0 3.5
ln(samplesize)10
8
6
4
2
02
dependent case, p=2
2.0 2.5 3.0 3.5
ln(samplesize)10
8
6
4
2
02
independent case, p=1
2.0 2.5 3.0 3.5
ln(samplesize)10
8
6
4
2
02
independent case, p=2
modified Hungarian average modified Hungarian best modified Hungarian worst Network simplex average Network simplex best Network simplex worst
Figure 39: Comparison with the network simplex algorithm on DOT-benchmark with 32×32resolution
w.r.t. running time
26Published in Transactions on Machine Learning Research (02/2023)
2.0 2.5 3.0 3.5
ln(samplesize)10
8
6
4
2
02ln(time)
dependent case, p=1
2.0 2.5 3.0 3.5
ln(samplesize)10
8
6
4
2
02
dependent case, p=2
2.0 2.5 3.0 3.5
ln(samplesize)10
8
6
4
2
02
independent case, p=1
2.0 2.5 3.0 3.5
ln(samplesize)10
8
6
4
2
02
independent case, p=2
modified Hungarian average modified Hungarian best modified Hungarian worst Network simplex average Network simplex best Network simplex worst
Figure 40: Comparison with the network simplex algorithm on DOT-benchmark with 64×64resolution
w.r.t. running time
2.0 2.5 3.0 3.5
ln(samplesize)10
8
6
4
2
02ln(time)
dependent case, p=1
2.0 2.5 3.0 3.5
ln(samplesize)10
8
6
4
2
02
dependent case, p=2
2.0 2.5 3.0 3.5
ln(samplesize)10
8
6
4
2
02
independent case, p=1
2.0 2.5 3.0 3.5
ln(samplesize)10
8
6
4
2
02
independent case, p=2
modified Hungarian average modified Hungarian best modified Hungarian worst Network simplex average Network simplex best Network simplex worst
Figure 41: Comparison with the network simplex algorithm on DOT-benchmark with 128×128resolution
w.r.t. running time
2.0 2.5 3.0 3.5
ln(samplesize)10
8
6
4
2
02ln(time)
dependent case, p=1
2.0 2.5 3.0 3.5
ln(samplesize)10
8
6
4
2
02
dependent case, p=2
2.0 2.5 3.0 3.5
ln(samplesize)10
8
6
4
2
02
independent case, p=1
2.0 2.5 3.0 3.5
ln(samplesize)10
8
6
4
2
02
independent case, p=2
modified Hungarian average modified Hungarian best modified Hungarian worst Network simplex average Network simplex best Network simplex worst
Figure 42: Comparison with the network simplex algorithm on DOT-benchmark with 256×256resolution
w.r.t. running time
27