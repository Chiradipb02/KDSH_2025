Published in Transactions on Machine Learning Research (06/2022)
Multi-Agent Oﬀ-Policy TDC with Near-Optimal Sample and
Communication Complexities
Ziyi Chen u1276972@utha.edu
Department of Electrical and Computer Engineering
University of Utah
Yi Zhou yi.zhou@utah.edu
Department of Electrical and Computer Engineering
University of Utah
Rong-Rong Chen rchen@ece.utah.edu
Department of Electrical and Computer Engineering
University of Utah
Reviewed on OpenReview: https: // openreview. net/ forum? id= tnPjQpYk7D
Abstract
The ﬁnite-time convergence of oﬀ-policy temporal diﬀerence (TD) learning has been compre-
hensively studied recently. However, such a type of convergence has not been established for
oﬀ-policy TD learning in the multi-agent setting, which covers broader reinforcement learning
applications and is fundamentally more challenging. This work develops a decentralized TD
with correction (TDC) algorithm for multi-agent oﬀ-policy TD learning under Markovian
sampling. In particular, our algorithm avoids sharing the actions, policies and rewards of the
agents, and adopts mini-batch sampling to reduce the sampling variance and communication
frequency. Under Markovian sampling and linear function approximation, we proved that
the ﬁnite-time sample complexity of our algorithm for achieving an /epsilon1-accurate solution is in
the order ofO/parenleftbigMln/epsilon1−1
/epsilon1(1−σ2)2/parenrightbig
, whereMdenotes the total number of agents and σ2is a network
parameter. This matches the sample complexity of the centralized TDC. Moreover, our
algorithm achieves the optimal communication complexity O/parenleftbig√
Mln/epsilon1−1
1−σ2/parenrightbig
for synchronizing
the value function parameters, which is order-wise lower than the communication complexity
of the existing decentralized TD(0). Numerical simulations corroborate our theoretical
ﬁndings.
1 Introduction
Multi-agent reinforcement learning (MARL) is an emerging technique that has broad applications in control
Yanmaz et al. (2017); Chalaki & Malikopoulos (2020), wireless sensor networks Krishnamurthy et al. (2008);
Yuan et al. (2020), robotics Yan et al. (2013), etc. In MARL, agents cooperatively interact with an environment
and follow their own policies to collect local rewards. In particular, policy evaluation is a fundamental problem
in MARL that aims to learn a multi-agent value function associated with the policies of the agents. This
motivates the development of convergent and communication-eﬃcient multi-agent TD learning algorithms.
For single-agent on-policy evaluation (i.e., samples are collected by target policy), the conventional TD(0)
algorithm Sutton (1988); Sutton & Barto (2018) and Q-learning algorithm Dayan (1992) have been developed
with asymptotic convergence guarantee. Recently, their ﬁnite-time (i.e., non-asymptotic) convergence has
been established under Markovian sampling (i.e., the samples are obtained from Markovian decision process
and thus are not i.i.d.) and linear approximation Bhandari et al. (2018); Zou et al. (2019). However, these
algorithms may diverge in the oﬀ-policy setting Baird (1995), where samples are collected by a diﬀerent
1Published in Transactions on Machine Learning Research (06/2022)
behavior policy. To address this important issue, a family of gradient-based TD (GTD) algorithms were
developed for oﬀ-policy evaluation with asymptotic convergence guarantee Sutton et al. (2008; 2009); Maei
(2011). In particular, the TD with gradient correction (TDC) algorithm has been shown to have superior
performance and its ﬁnite-time convergence has been established recently under Markovian sampling Xu
et al. (2019); Gupta et al. (2019); Kaledin et al. (2020).
For multi-agent on-policy evaluation, various decentralized TD learning algorithms have been developed. For
example, the ﬁnite-time convergence of decentralized TD(0) was established with i.i.d samples Wai et al.
(2018); Doan et al. (2019) and Markovian samples Sun et al. (2020), respectively, under linear function
approximation, and an improved result is further obtained in Wang et al. (2020) by leveraging gradient
tracking. However, these algorithms do not apply to the oﬀ-policy setting. In the existing literature,
decentralized oﬀ-policy TD learning has been studied only in simpliﬁed settings, for example, agents obtain
independent MDP trajectories Macua et al. (2014); Stanković & Stanković (2016); Cassano et al. (2020) or
share their behavior and target policies with each other Cassano et al. (2020), and the data samples are
either i.i.d. or have a ﬁnite sample size. These MARL settings either are impractical or reveal the agents’
policies that may be sensitive. Therefore, we want to ask the following question:
•Q1: Can we develop a decentralized oﬀ-policy TD algorithm for MARL with interdependent agents and
avoids sharing local actions, policies and rewards of the agents?
In fact, developing such a desired decentralized oﬀ-policy TD learning algorithm requires overcoming two
major challenges. First, to perform decentralized oﬀ-policy TD learning, all the agents need to obtain a
global importance sampling ratio (see Section 3.2). In Cassano et al. (2020), the authors obtained this ratio
by sharing all local policies among the agents, which may lead to information leakage. Therefore, we aim
to develop a safer scheme to synchronize the global importance sampling ratio among the agents without
sharing any sensitive local information. Second, the existing decentralized TD-type algorithm achieves a
communication complexity (number of communication rounds) of O/parenleftbig/parenleftbig
/epsilon1−1+√
M√/epsilon1(1−σ2)/parenrightbig
ln/epsilon1−1/parenrightbig
for networks
withMagents and parameter σ2(See Assumption 5 in Section 4 for the deﬁnition of σ2) Sun et al. (2020).
This induces much communication overhead when the target accuracy /epsilon1is small. Hence, we want to ask the
following theoretical question:
•Q2: Can we develop a decentralized oﬀ-policy TD learning algorithm that achieves a near-optimal ﬁnite-time
sample complexity and a near-optimal communication complexity under Markovian sampling?
In this work, we provide aﬃrmative answers to these questions by developing a decentralized TDC algorithm
that avoids sharing the sensitive information of the agents and achieves the near-optimal sample complexity
as well as a signiﬁcantly reduced communication complexity. We summarize our contributions as follows.
1.1 Summary of Contribution
To perform multi-agent oﬀ-policy evaluation, we develop a decentralized TDC algorithm with linear function
approximation. In every iteration, agents perform two-timescale TDC updates locally and exchange model
parameters with their neighbors. In particular, our algorithm adopts the following designs to avoid sharing
agents’ local sensitive information and reduce communication load.
•We let the agents perform local averaging on their local importance sampling ratios to obtain approximated
global importance sampling ratios.
•All the agents use a mini-batch of samples to update their model parameters in each iteration. The
mini-batch sampling reduces the sampling variance and the communication frequency, leading to an
improved communication complexity over that of the existing decentralized TD(0).
•After the decentralized TDC iterations, our algorithm performs additional local averaging steps to achieve
a global consensus on the model parameters. This turns out to be critical for achieving the near-optimal
complexity bounds.
Theoretically, we analyze the ﬁnite-time convergence of this decentralized TDC algorithm with Markovian
samples and show that it attains a fast linear convergence. The overall sample complexity for achieving
2Published in Transactions on Machine Learning Research (06/2022)
an/epsilon1-accurate solution is in the order of O/parenleftbigMln/epsilon1−1
/epsilon1(1−σ2)2/parenrightbig
. When there is a single agent ( M= 1), this sample
complexity result matches that of centralized TDC Xu et al. (2019) and matches the theoretical lower bound
O(/epsilon1−1)Kaledin et al. (2020) up to a logarithm factor. In addition, the sample complexity is proportional
toM, which matches the theoretical lower bound of decentralized strongly convex optimization Scaman
et al. (2017). Moreover, the communication complexity of our algorithm for synchronizing value function
parameters is in the order of O/parenleftbig√
Mln/epsilon1−1
1−σ2/parenrightbig
, which is signiﬁcantly lower than the communication complexity
O/parenleftbig
/epsilon1−1+√
M√/epsilon1(1−σ2)ln/epsilon1−1/parenrightbig
of the decentralized TD(0) Sun et al. (2020) and matches the communication
complexity lower bound Scaman et al. (2017).
Technically, our analysis is a nontrivial generalization of the analysis of centralized oﬀ-policy TDC to the
decentralized case. In particular, our analysis establishes tight bounds of the consensus error caused by
synchronizing the global importance sampling ratio, especially under the Markovian sampling where the
data samples are correlated. Moreover, we strategically bound the estimation error of the global importance
sampling logarithm-ratio. Please refer to the proof sketch at the end of Section 4 for details.
1.2 Other Related Work
Centralized policy evaluation. TD(0) with linear function approximation Sutton (1988) is popular for
on-policy evaluation. The asymptotic and non-asymptotic convergence results of TD(0) have been established
in Sutton (1988); Dayan (1992); Jaakkola et al. (1993); Gordon (1995); Baird (1995); Tsitsiklis & Van Roy
(1997); Tadić (2001); Hu & Syed (2019) and Korda & La (2015); Liu et al. (2015); Bhandari et al. (2018);
Dalal et al. (2018b); Lakshminarayanan & Szepesvari (2018); Wang et al. (2019); Srikant & Ying (2019); Xu
et al. (2020b) respectively. Sutton et al. (2009) proposed TDC for oﬀ-policy evaluation. The ﬁnite-sample
convergence of TDC has been established in Dalal et al. (2018a; 2020) with i.i.d. samples and in Xu et al.
(2019); Gupta et al. (2019); Kaledin et al. (2020) with Markovian samples.
Decentralized policy evaluation. Mathkar & Borkar (2016) proposed the decentralized TD(0) algorithm.
The asymptotic and non-asymptotic convergence rate of decentralized TD have been obtained in Borkar
(2009) and Sun et al. (2020); Wang et al. (2020) respectively. Exisitng decentralized oﬀ-policy evaluation
studies considered simpliﬁed settings. Macua et al. (2014); Stanković & Stanković (2016) obtained asymptotic
result for decentralized oﬀ-policy evaluation where the agents obtained independent MDPs. Cassano et al.
(2020) obtained linear convergence rate also with independent MDPs by applying variance reduction and
extended to the case where the individual behavior policies and the joint target policy are shared among the
agents.
Decentralized policy control. Decentralized policy control is also an important MARL problem where
the goal is to learn the optimal policy for each agent. Many algorithms have been proposed for decentralized
policy control, including policy gradient Chen et al. (2021) and actor-critic Qu et al. (2020); Zhang et al.
(2021).
2 Policy Evaluation in Multi-Agent RL
In this section, we introduce multi-agent reinforcement learning (MARL) and deﬁne the policy evaluation
problem. Consider a fully decentralized multi-agent network that consists of Magents. The network
topology is speciﬁed by an undirected graph G= (M,E), whereM={1,2,···,M}denotes the set of agents
andEdenotes the set of communication links. In other words, each agent monly communicates with its
neighborhoodNm:={m/prime∈M : (m,m/prime)∈E}. In MARL, the agents interact with a dynamic environment
through a multi-agent Markov decision process (MMDP) speciﬁed as {S,{A(m)}M
m=1,P,{R(m)}M
m=1,γ}. To
elaborate,Sdenotes a global state space that is shared by all the agents, A(m)corresponds to the action
space of agent m,Pis the state transition kernel and R(m)denotes the reward function of agent m. All the
state and action spaces have ﬁnite cardinality. γ∈(0,1]is a discount factor.
At any time t, assume that all the agents are in the global state st∈S. Then, each agent mtakes a certain
actiona(m)
t∈A(m)following its own stationary policy π(m), i.e.,a(m)
t∼π(m)(·|st). After all the actions are
taken, the global state transfers to a new state st+1according to the transition kernel P, i.e.,st+1∼P(·|st,at)
3Published in Transactions on Machine Learning Research (06/2022)
whereat:={a(m)
t}M
m=1. At the same time, each agent mreceives a local reward R(m)
t:=R(m)(st,at,st+1)
from the environment for this action-state transition. Throughout the MMDP, each agent mhas access to
only the global state {st}tand its own actions {a(m)
t}tand rewards{R(m)
t}t. The goal of policy evaluation
in MARL is to evaluate the following value function associated with all the local policies π:={π(m)}M
m=1for
any global state s.
Vπ(s) =E/bracketleftBig+∞/summationdisplay
t=0γt/parenleftBig1
MM/summationdisplay
m=1R(m)
t/parenrightBig/vextendsingle/vextendsingle/vextendsingles0=s,π/bracketrightBig
. (1)
A popular algorithm for policy evaluation in MARL is the decentralized TD(0) Sun et al. (2020). Speciﬁcally,
consider a popular linear function approximation of the value function Vθ(s) :=θ/latticetopφ(s), whereθ∈Rd
contains the model parameters and φ(s)is a feature vector that corresponds to the state s. In decentralized
TD(0), each agent mcollects a single Markovian sample {st,a(m)
t,st+1,R(m)
t}at timet(a(m)
t∼π(m)(·|st),
st+1∼P(·|st,at),R(m)
t:=R(m)(st,at,st+1)) and updates its own model parameters θ(m)
twith learning rate
α>0as follows.
θ(m)
t+1=/summationdisplay
m/prime∈NmUm,m/primeθ(m/prime)
t+α/parenleftbig
Atθ(m)
t+b(m)
t/parenrightbig
, (2)
whereUcorresponds to a doubly stochastic communication matrix, each agent monly communicates with
its neighborhood Nm, andAt=φ(st)(γφ(st+1)−φ(st))/latticetop,b(m)
t=R(m)
tφ(st). The above update rule applies
the local TD error to update the parameters and synchronize the parameters among neighboring agents
through the network. It can be inferred that θ(m)
tobtained by this decentralized TD(0) algorithm is /epsilon1-close
to the optimal solution with both sample complexity (the number of required samples) and communication
complexity (the number of communication rounds) being O(/epsilon1−1+/radicalbig
M//epsilon1(1−σ2)−1) ln(/epsilon1−1).1
3 Two-Timescale Decentralized TDC for Oﬀ-Policy Evaluation
3.1 Centralized TDC
In this subsection, we review the centralized TD with gradient correction (TDC) algorithm Sutton et al.
(2009). In RL, the agent may not have enough samples that are collected following the target policy π.
Instead, it may have some data samples that are collected under a diﬀerent behavior policy πb. Therefore, in
thisoﬀ-policy setting, the agent would like to utilize the historical data to help evaluate the value function
Vπassociated with the target policy π.
In Sutton et al. (2009), a family of gradient-based TD (GTD) learning algorithms have been proposed
for oﬀ-policy evaluation. In particular, the TDC algorithm has been shown to have superior performance.
To explain, consider the linear approximation Vθ(s) =θ/latticetopφ(s)and suppose the state space includes states
s1,...,sn, we can deﬁne a total value function Vθ:= [Vθ(s1),...,Vθ(sn)]/latticetop. The goal of TDC is to minimize the
following mean square projected Bellman error (MSPBE).
MSPBE (θ) :=Eµb/bardblVθ−ΠTπVθ/bardbl2,
whereµbis the stationary distribution under the behavior policy πb,Tπis the Bellman operator and
Π(V) :=arg minVθ/bardblVθ−V/bardbl2is a projection operator of any state value function V:S→Ronto the space of
linear value functions {Vθ:Vθ(s) =θ/latticetopφ(s)}. Given the i-th sample (si,ai,si+1,Ri)obtained by the behavior
policy, we deﬁne the following terms
ρi:=π(ai|si)
πb(ai|si), bi:=ρiRiφ(si), Ai:=ρiφ(si)(γφ(si+1)−φ(si))/latticetop,
Bi:=−γρiφ(si+1)φ(si)/latticetop, Ci:=−φ(si)φ(si)/latticetop, (3)
1Sun et al. (2020) does not report sample complexity and communication complexity, so we calculated them based on their
ﬁnite-time error bound in proposition 2.
4Published in Transactions on Machine Learning Research (06/2022)
whereρiis referred to as the importance sampling ratio . Then, with learning rates α,β > 0and initialization
parameters θ0,w0, the-two timescale oﬀ-policy TDC algorithm takes the following recursive updates for
iterationst= 0,1,2,...
(TDC):/braceleftBigg
θt+1=θt+α(Atθt+bt+Btwt),
wt+1=wt+β(Atθt+bt+Ctwt).(4)
Xu et al. (2019); Xu & Liang (2020) study slight variations of the above TDC algorithm by using projection
and minibatch technique respectively, and obtain that both variants obtain an /epsilon1-approximation of the optimal
model parameter θ∗=−A−1b(A:=Eπb[Ai],b:=Eπb[bi]) with sample complexity O(/epsilon1−1ln/epsilon1−1).
3.2 Decentralized Mini-batch TDC
In this subsection, we propose a decentralized TDC algorithm for oﬀ-policy evaluation in MARL. In the
multi-agent setting, without loss of generality, we assume that each agent mhas a target policy π(m)and
its samples are collected by a diﬀerent behavior policy π(m)
b. In particular, if agent mis on-policy, then we
haveπ(m)
b=π(m). In this multi-agent oﬀ-policy setting, the agents aim to utilize the data collected by the
behavior policies πb={π(m)
b}M
m=1to help evaluate the value function Vπassociated with the target policies
π={π(m)}M
m=1.
However, directly generalizing the centralized TDC algorithm to the decentralized setting will encounter
several challenges. First, the centralized TDC in eq. (4) consumes one sample per-iteration and achieves
the sample complexity O(/epsilon1−1log/epsilon1−1)Xu et al. (2019). Therefore, the corresponding decentralized TDC
would perform one local communication per-iteration and is expected to have a communication complexity
in the order of O(/epsilon1−1log/epsilon1−1), which induces large communication overhead . Second, in the multi-agent
oﬀ-policy setting, every agent mhas a local importance sampling ratio ρ(m)
i:=π(m)(a(m)
i|si)/π(m)
b(a(m)
i|si).
However, to correctly perform oﬀ-policy updates, every agent needs to know all the other agents’ local
importance sampling ratios in order to obtain the global importance sampling ratio ρi:=/producttextM
m=1ρ(m)
i.
To address these challenges that are not seen in decentralized TD(0) and centralized TDC , we next propose
a decentralized TDC algorithm that takes mini-batch stochastic updates.
To elaborate, note that ρican be rewritten as
ρi= exp/parenleftBig
M·1
M/summationtextM
m=1lnρ(m)
i/parenrightBig
.
Therefore, all the agents just need to obtain the average1
M/summationtextM
m=1lnρ(m)
i, which can be computed via local
communication of the logarithm-ratios {lnρ(m)
i}M
m=1forLrounds. Speciﬁcally, every agent minitializes
/tildewideρ(m)
i,0= lnρ(m)
iand for iterations /lscript= 0,...,L−1do
/tildewideρ(m)
i,/lscript+1=/summationdisplay
m/prime∈NmUm,m/prime/tildewideρ(m/prime)
i,/lscript, (5)
(Output) :/hatwideρ(m)
i= exp(M·/tildewideρ(m)
i,L). (6)
In Corollary 2 (see the appendix), we prove that all of these local estimates {/hatwideρ(m)
i}M
m=1converge exponentially
fast to the desired quantity ρiasLincreases. Then, every agent mperforms the following two-timescale
TDC updates
θ(m)
t+1=/summationdisplay
m/prime∈NmUm,m/primeθ(m/prime)
t+α
N(t+1)N−1/summationdisplay
i=tN/parenleftbig
A(m)
iθ(m)
t+/hatwideb(m)
i+B(m)
iw(m)
t/parenrightbig
, (7)
w(m)
t+1=/summationdisplay
m/prime∈NmUm,m/primew(m/prime)
t+β
N(t+1)N−1/summationdisplay
i=tN/parenleftbig
A(m)
iθ(m)
t+/hatwideb(m)
i+Ciw(m)
t/parenrightbig
, (8)
5Published in Transactions on Machine Learning Research (06/2022)
whereA(m)
i,B(m)
i,/hatwideb(m)
iare deﬁned by replacing the global variables ρiandRiinvolved in Ai,Bi,b(m)
i(see
eq. (3)) with local variables /hatwideρ(m)
iandR(m)
irespectively. To summarize, every TDC iteration of Algorithm 1
consumesNMarkovian samples, and requires two vector communication rounds for synchronizing the
parameter vectors θ(m)
t,w(m)
t, andLscalar communication rounds for estimating the global importance
sampling ratio ({ρ(m)
i,/lscript:i=tN,..., (t+ 1)N−1,m∈M}are shared in the /lscript-th communication round). We
summarize these update rules in Algorithm 1. Moreover, after the decentralized TDC updates, the agents
perform additional T/primelocal averaging steps to reach a global consensus on the model parameters.
Algorithm 1 Decentralized mini-batch TDC.
Input:Batch sizeN, iterations T,T/prime, learning rates α,β.
Initialize: θ(m)
0,w(m)
0for all agents m∈M.
foriterationt= 0,1,...,T−1do
Each agent collects NMarkovian samples and computes their local importance sampling ratios ρ(m)
i.
forcommunication round /lscript= 0,1,...,L−1do
foragentm∈Min parallel do
Communicate /tildewideρ(m)
i,/lscriptvia eq. (5) for the Nsamplesi=tN,..., (t+ 1)N−1.
end
end
foragentm∈Min parallel do
Agentmestimates global importance sampling ratios /hatwideρ(m)
ivia eq. (6) for i=tN,..., (t+ 1)N−1,
and then performs the updates in eqs. (7) and (8).
end
end
foriterationt=T,T+ 1,...,T +T/prime−1do
foragentm∈Min parallel do
θ(m)
t+1=/summationtext
m/prime∈NmUm,m/primeθ(m/prime)
t.
end
end
Output:{θ(m)
T+T/prime}M
m=1.
4 Finite-Time Analysis of Decentralized TDC
Inthissection, weanalyzetheﬁnite-timeconvergenceofAlgorithm1. Denote µπbasthestationarydistribution
of the Markov chain {st}tinduced by the collection of agents’ behavioral policies πb. Throughout the analysis,
we deﬁne the following notations.
A:=Eπb[Ai], B:=Eπb[Bi], C:=Eπb[Ci],bi:=1
M/summationtextM
m=1b(m)
i,b: =Eπb/bracketleftbig
bi/bracketrightbig
,
θt:=1
M/summationtextM
m=1θ(m)
t, θ∗:=−A−1b, w∗
t:=−C−1(Aθt+b),
where Eπbdenotes the expectation when st∼µπb,a(m)
t∼π(m)
b(st)andst+1∼P(·|st,at),Ai,Bi,Ciare
deﬁned in eq. (3) with exact global importance sampling ratio ρi,θ∗is the optimal model parameter, and w∗
t
is the optimal auxiliary parameter corresponding to θt. It is well-known that the optimal model parameter
isθ∗=−A−1bXu et al. (2020b); Xu & Liang (2020). We make the following standard assumptions.
Assumption 1. There exist constants ν >0andδ∈(0,1)such that for all t≥0,
sup
s∈SdTV(Pπb(st|s0=s),µπb)≤νδt, (9)
wheredTVdenotes the total-variation distance.
Assumption 2. The matrices AandCare invertible.
Assumption 3. The feature vectors satisfy /bardblφ(s)/bardbl≤1,∀s.
6Published in Transactions on Machine Learning Research (06/2022)
Assumption 4. There exist Rmax,ρmax>0such that for all m∈M:maxs,a,s/primeR(m)(s,a,s/prime)<R maxand
maxs,a(m)ρ(m)(s,a(m))< ρ maxwhereρ(m)(s,a(m)) :=π(m)(a(m)|s)
π(m)
b(a(m)|s)denotes the global importance sampling
ratio .
Assumption 5. The communication matrix Uis doubly stochastic, (i.e., the entries of each row and those
of each column sum up to 1.) , and its second largest singular value satisﬁes σ2∈[0,1).
Assumption 1 has been widely adopted in the existing literature Bhandari et al. (2018); Xu et al. (2019);
Xu & Liang (2020); Shaocong et al. (2020; 2021). It holds for all homogeneous Markov chains with ﬁnite
state-space and all uniformly ergodic Markov chains. Assumptions 2 – 4 are widely adopted in the analysis
of TD learning algorithms Xu et al. (2019); Xu & Liang (2020). In particular, Assumption 2 implies that
λ1:=−λmax(A/latticetopC−1A)>0,λ2:=−λmax(C)>0whereλmax(C)denotes the largest eigenvalue of matrix C.
Assumption 3 can always hold by normalizing the feature vectors. Assumption 4 holds for any uniformly
lower bounded behavior policy, i.e., inf(s,a,m )π(m)
b(s,a)>0, which ensures that every state-action pair (s,a)
is visited inﬁnitely often. Assumption 5 is standard in decentralized optimization Singh et al. (2020); Saha
et al. (2020) and TD learning Sun et al. (2020); Wang et al. (2020). σ2is an important measure that reﬂects
communication topology. For example, densely connected network tends to have smaller σ2than sparse
network.
We obtain the following ﬁnite-time error bound as well as the sample complexity (the number of required
samples) and communication complexity (the number of communication rounds) for Algorithm 1 with
Markovian samples.
Theorem 1. Let Assumptions 1–5 hold. Run Algorithm 1 for Titerations with learning rates α≤
min{O(β),O(1−σ2√
M)},β≤O(1), batch size N≥max{O(1),O(β
α)}andL≥O(lnM+Mlnρmax
lnσ−1
2)(see eq.(35)-
(38) for the full expressions ). Then, we have the following convergence rate (see eq. (28) for its full
expression)
E/bracketleftbig/vextenddouble/vextenddoubleθT−θ∗/vextenddouble/vextenddouble2/bracketrightbig
≤/parenleftBig
1−αλ1
6/parenrightBigT/parenleftbig/vextenddouble/vextenddoubleθ0−θ∗/vextenddouble/vextenddouble2+/bardblw0−w∗
0/bardbl2/parenrightbig
+O/parenleftBigβ
Nα+βσL/4
22T
M/parenrightBig
. (10)
Furthermore, after T/primeiterations of local averaging, the local models of all agents m= 1,...,Mhas the following
consensus error (see eq. (34) for its full expression) :
E/bracketleftbig
/bardblθ(m)
T+T/prime−θT/bardbl2/bracketrightbig
≤σ2T/prime
2O/parenleftBig
1 +M4βα
(1−σ2)2+MβασL/4
22T
1−σ2/parenrightBig
. (11)
Consequently, by choosing α=O(1−σ2√
M),β=O(1),T=O/parenleftbig√
Mln/epsilon1−1
1−σ2/parenrightbig
,N=O/parenleftbig√
M
/epsilon1(1−σ2)/parenrightbig
,L=O/parenleftbig√
Mln/epsilon1−1
(1−σ2)2+
M
1−σ2/parenrightbig
,T/prime=O/parenleftbig1
1−σ2lnM
/epsilon1(1−σ2)/parenrightbig
(See the end of Appendix B for the full expressions of these hyperparameters)
, we obtain that E(/bardblθ(m)
T+T/prime−θ∗/bardbl2)≤/epsilon1for allm. The overall communication complexities for synchronizing
θ(m)
tand imporance sampling ratio ρare respectively T+T/prime=O/parenleftbig√
Mln/epsilon1−1
1−σ2/parenrightbig
andTL=O/parenleftbig/parenleftbig√
Mln/epsilon1−1
(1−σ2)3+
M
(1−σ2)2/parenrightbig
lnM
/epsilon1(1−σ2)/parenrightbig
. The total sample complexity is NT=O/parenleftbigMln/epsilon1−1
/epsilon1(1−σ2)2/parenrightbig
.
The above theorem shows that our decentralized TDC achieves the sample complexity O/parenleftbigMln/epsilon1−1
/epsilon1(1−σ2)2/parenrightbig
, which, in
the centralized setting ( M= 1,σ2= 0), matchesO(/epsilon1−1ln/epsilon1−1)of centralized TDC for Markovian samples
Xu et al. (2019); Xu & Liang (2020) and matches the theoretical lower bound O(/epsilon1−1)given in Kaledin et al.
(2020) up to a logarithm factor. In addition, the sample complexity is proportional to M, which matches the
theoretical lower bound of decentralized strongly convex optimization in Scaman et al. (2017). Importantly, the
communication complexity O/parenleftbig√
Mln/epsilon1−1
1−σ2/parenrightbig
for synchronizing θ(m)
tis substantially lower than the communication
complexityO/parenleftbig/parenleftbig
/epsilon1−1+√
M√/epsilon1(1−σ2)/parenrightbig
ln/epsilon1−1/parenrightbig
of decentralized TD(0) Sun et al. (2020)2Intuitively, this is because
our algorithm adopts mini-batch sampling that signiﬁcantly reduces the communication frequency, since
2Since on-policy evaluation does not involve importance sampling ratio ρ, we only compare the communication complexity
for synchronizing θ(m)
twhich is involved in both on-policy and oﬀ-policy evaluation.
7Published in Transactions on Machine Learning Research (06/2022)
communication occurs after collecting a mini-batch of samples to compute the mini-batch updates. Moreover,
the communication complexity has a logarithm dependence ln/epsilon1−1on the target accuracy, and this matches
the theoretical lower bound of decentralized strongly convex optimization in Scaman et al. (2017). Note
that both communication complexity and sample complexity decrease with smaller σ2>0. Hence, the
communication matrix Ucan be selected with minimum possible σ2, under network topology constraint in
practice. In addition, our stepsizes α=O/parenleftbig1−σ2√
M/parenrightbig
andβ=O(1)do not scale with /epsilon1. Although αcan be small
whenσ2≈1andMis large, it is much larger than α=min/bracketleftbig
O(/epsilon1),O/parenleftbig/radicalbig/epsilon1
M(1−σ2)/parenrightbig/bracketrightbig
in decentralized TD (0)
Sun et al. (2020) with small /epsilon1.3
Taking a deeper look, Theorem 1 shows that the average model θTconverges to a small neighborhood
of the optimal solution θ∗at a fast linear convergence rate (10) that matches the convergence rate of
centralized TDC Xu et al. (2019); Xu & Liang (2020). In particular, the convergence error is in the order of
O/parenleftbigβ
Nα+βσL/4
22T
M/parenrightbig
, which can be driven arbitrarily close to zero by choosing a suﬃciently large mini-batch size
Nand communication rounds L(withTﬁxed), and choosing constant-level learning rates α,β. Moreover,
theT/primesteps of extra local model averaging further help all the agents achieve a small consensus error at
a linear convergence rate (11). Eqs. (10) and (11) together ensure the fast convergence of all the local
model parameters. We want to point out that the T/primelocal averaging steps are critical for establishing fast
convergence of local model parameters. Speciﬁcally, without the T/primelocal averaging steps, the consensus error
E/bracketleftbig
/bardblθ(m)
T−θT/bardbl2/bracketrightbig
would be in the order of at least O(1), which is constant-level and hence cannot guarantee
the local model parameters converge arbitrarily close to the true solution.
Proof sketch of Theorem 1. The proof of the theorem is a nontrivial generalization of the analysis of
centralized oﬀ-policy TDC to the decentralized case. Below, we sketch the technical proof and elaborate on
the technical novelties.
•Step 1. We ﬁrst consider an ideal case where the agents can access the exact global importance sampling
ratioρtat iteration t. In this ideal case, every agent mcan replace the estimated global importance
sampling ratio /hatwideρ(m)
iinvolved in A(m)
i,B(m)
i,/hatwideb(m)
iin the update rules (7) and (8) by the exact value ρi
(soA(m)
i,B(m)
i,/hatwideb(m)
ibecomeAi,Bi,b(m)
irespectively) . Then, with the notations deﬁned in Table 1 in
Appendix A, the averaged update rules (14) and (15) become
/tildewideθt+1=θt+α/parenleftbig/tildewideAtθt+/tildewidebt+/tildewideBtwt/parenrightbig
, (12)
/tildewidewt+1=wt+β/parenleftbig/tildewideAtθt+/tildewidebt+/tildewideCtwt/parenrightbig
, (13)
This can be seen as one step of centralized TDC. Hence, we can bound its optimization error terms
E/bracketleftbig
/bardbl/tildewidewt+1−w∗
t/bardbl2/bracketrightbig
andE/bracketleftbig
/bardbl/tildewideθt+1−θ∗/bardbl2/bracketrightbig
.
•Step 2. We return to Algorithm 1 and bound its optimization error terms E/bracketleftbig
/bardblwt+1−w∗
t+1/bardbl2/bracketrightbig
(by bounding
E/bracketleftbig
/bardblwt+1−w∗
t/bardbl2/bracketrightbig
ﬁrst) and E/bracketleftbig
/bardblθt+1−θ∗/bardbl2/bracketrightbig
. This is done by bounding the gap between the centralized
updates (12) and (13) (with exact ρt) and the decentralized updates (14) and (15) (with inexact ρt). The
key is to establish Corollary 2, which strategically controls the gap between the inexact global importance
sampling ratio /hatwideρ(m)
tand the exact value ρt. Such a challenge in bounding the importance sampling ratio
gap is not seen in the analysis of decentralized TD(0) and centralized TDC.
To elaborate, note that the locally-averaged importance sampling ratios /tildewideρ(m)
t,/lscriptin eq. (5) exponentially
converges to the value lnρt. However, the initial gap |lnρ(m)
t−lnρt|can be numerically large since ρ(m)
t
may be a numerically small positive number. To avoid such divergence issue, our proof discusses two
complementary cases. Case 1: the quantity ρmin:=minm∈Mρ(m)
t∈[σL/2
2,ρmax]. In this case, the proof is
straightforward as the initial gap is bounded. Case 2: the quantity ρmin∈(0,σL/2
2]. In this case, we show
that the locally-averaged logarithm-ratio /tildewideρ(m)
t,Lis below a large negative number O/parenleftbiglnρmin
M/parenrightbig
/lessmuch0(See eq.
(65)). which implies that both the global importance sampling ratio ρtand its estimation /hatwideρ(m)
tare close
3α=min/bracketleftbig
O(/epsilon1),O/parenleftbig/radicalbig/epsilon1
M(1−σ2)/parenrightbig/bracketrightbig
is obtained by letting the convergence rate in proposition 2 from Sun et al. (2020) to be
smaller than /epsilon1.
8Published in Transactions on Machine Learning Research (06/2022)
to zero. In both cases, {/hatwideρ(m)
t}M
m=1converge exponentially fast to ρtasLincreases. This prove eq. (10).
To the best of our knowledge, this technique for bounding the estimation error of the global importance
sampling ratio has not been developed in the existing literature.
•Step 3. Finally, we prove the consensus error (11). Although the consensus error exponentially decays
during the T/primeextra local average steps in Algorithm 1, it is non-trivial to bound the initial consensus
error/bardbl∆ΘT/bardblFof theT/primelocal average iterations (see eq. (34)), which is caused by the Tdecentralized
TDC steps. To bound this error, note that each decentralized TDC step consists of both local averaging
and TDC update, which makes the consensus error /bardbl∆Θt/bardblFdiminishes geometrically fast with a noise
term/summationtextM
m=1/bardblhm/bardbl(see eq. (30)). Such a noise term is induced by the TDC update and hence its bound
depends on both the consensus error and the model estimation error in eq. (10). We need to apply these
correlated bounds iteratively for Titerations to bound the initial consensus error /bardbl∆ΘT/bardblF.
5 Experiments
5.1 Simulated Multi-Agent Networks
We simulate a multi-agent MDP with 10 decentralized agents. The shared state space contains 10 states and
each agent can take 2 actions. All behavior policies are uniform policies (i.e., each agent takes all actions with
equal probability), and the target policies are obtained by ﬁrst perturbing the corresponding behavior policies
with Gaussian noises sampled from N(0,0.05)and then performing a proper normalization. The entries of
the transition kernel and the reward functions are independently generated from the uniform distribution on
[0,1](with proper normalization for the transition kernel). We generate all state features with dimension 5
independently from the standard Gaussian distribution and normalize them to have unit norm. The discount
factor isγ= 0.95.
We consider two types of network topologies: a fully connected network with communication matrix Uhaving
diagonal entries 0.8and oﬀ-diagonal entries 1/45, and a ring network with communication matrix Uhaving
diagonal entries 0.8and entries 0.1 for adjacent agents. We implement and compare two algorithms in these
networks: the decentralized TD(0) with batch size N= 1(used in Sun et al. (2020)) and our decentralized
TDC with batch sizes N= 1,10,20,50,100. Note that the original decentralized TD(0) is simply eq. (7)
with setting w(m)
t≡0andρi≡1in the deﬁnition of A(m)
i,B(m)
iand/hatwideb(m)
i, which only works for on-policy
evaluation. To adapt decentralized TD(0) to oﬀ-policy evaluation, we simply use /hatwideρ(m)
icomputed by eqs. (5)
and (6) with L= 3.
Eﬀect of Batch size: We test these algorithms with varying batch size Nand compare their sample and
communication complexities. We set learning rate α= 0.2for the decentralized TD(0) and α= 0.2∗N,
β= 0.002∗Nfor our decentralized TDC with varying batch sizes N= 1,10,20,50,100. Both algorithms
useL= 3communication rounds for synchronizing /hatwideρ(m)
t. All algorithms are repeated 100 times using a ﬁxed
set of 100 MDP trajectories, each of which has 20k Markovian samples.
We ﬁrst implement these algorithms in the fully connected network. Figure 1 plots the relative convergence
error/bardblθt−θ∗/bardbl//bardblθ∗/bardblv.s. the number of samples ( tN) and the number of communication rounds ( t) . For
each curve, its upper and lower envelopes denote the 95% and 5% percentiles of the 100 convergence errors,
respectively. It can be seen that our decentralized TDC with diﬀerent batch sizes achieve comparable sample
complexity to that of the decentralized TD(0), demonstrating the sample-eﬃciency of our algorithms. On the
other hand, our decentralized TDC requires much less communication complexities than the decentralized
TD(0) with N≥10, and the required communication becomes lighter as batch size increases. All these
results match our theoretical analysis well.
We further implement these algorithms in the ring network. The comparison results are exactly the same
as those in Figure 1, since the update rule of θtdoes not rely on the network topology under exact global
importance sampling.
Eﬀect of Communication Rounds: We test our decentralized TDC using varying communication rounds
L= 1,3,5,7. We use a ﬁxed batch size N= 100and set learning rates α= 5,β= 0.05, and repeat each
9Published in Transactions on Machine Learning Research (06/2022)
Figure 1: Comparison between decentralized TDC with varying batch sizes and decentralized TD(0).
algorithm 100 times using the set of 100 MDP trajectories. We also implement the decentralized TDC with
exact global importance sampling ratios as a baseline. Figure 2 plots the relative convergence error v.s. the
number of communication rounds ( t) in the fully-connected network (Left) and ring network (Right). It can
be seen that in both networks, the asymptotic convergence error of the decentralized TDC with inexact ρ
decreases as the number of communication rounds Lfor synchronizing the global importance sampling ratio
increases. In particular, with L= 1, decentralized TDC diverges asymptotically due to inaccurate estimation
of the global importance sampling ratio. As Lincreases to more than 5, the convergence error is as small as
that under exact global importance sampling.
Figure 2: Eﬀect of communication rounds Lon asymptotic convergence error.
We further plot the maximum relative consensus error among all agents maxm/bardblθ(m)
t−θt/bardbl//bardblθ∗/bardblv.s. the
number of communication rounds ( t) in the fully-connected network (Left) and ring network (Right) in
Figure 3, where the tails in both ﬁgures correspond to the extra T/prime= 20local model averaging steps. In both
networks, one can see that the consensus error decreases as Lincreases, and the extra local model averaging
steps are necessary to achieve consensus. Moreover, it can be seen that the consensus errors achieved in the
fully connected network are slightly smaller than those achieved in the ring network, as denser connections
facilitate achieving the global consensus.
5.2 Two-Agent Cliﬀ Navigation Problem
In this subsection, we test our algorithms in solving a two-agent Cliﬀ Navigation problem Qiu et al. (2021) in
a grid-world environment. This problem is adapted from its single-agent version (see Example 6.6 of Sutton
10Published in Transactions on Machine Learning Research (06/2022)
Figure 3: Eﬀect of communication rounds Lon consensus error.
Figure 4: Two-agent cliﬀ navigation. (“S”, “X”, “D” denote starting point, cliﬀ and destination respectively.
The optimal path is shown in red.)
Figure 5: Results on two-agent cliﬀ navigation problem.
& Barto (2018)). As illustrated in Figure 4, two agents start from the starting point “S” on a 3×4grid and
aim to reach the destination “D”. Here, global state is deﬁned as the joint location of the two agents, and
there are in total (3×4)2= 144global states. In most states, an agent can choose to move up, down, left or
right by one step and receives −1reward. However, once an agent falls into the cliﬀ “X”, it will return to the
starting point “S” and receive −100reward. When an agent reaches “D”, it will always stay at “D”, and
11Published in Transactions on Machine Learning Research (06/2022)
receives 0reward if the other agent also reaches/stays at “D”, or receives −0.5reward otherwise. If an agent
is not at “X” or “D” and selects a direction that points outside the grid, then it stays in the previous location
and receives−1reward.
We apply the aforementioned algorithms with diﬀerent batchsizes Nto solve this problem. The hyperparam-
eters and the ways to generate behavior policy and target policy are the same as the previous simulation
experiment, except that the communication matrix Uhas diagonal entries 0.7 and oﬀ-diagonals 0.3. All
algorithms are repeated 100 times using a ﬁxed set of 100 MDP trajectories, each of which has 20k Markovian
samples. Figure 5 plots the relative convergence error /bardblθt−θ∗/bardbl//bardblθ∗/bardblv.s. the number of samples ( tN)
and the number of communication rounds ( t) . We can see that compared with the decentralized TD(0),
our decentralized TDC achieve comparable sample complexities with diﬀerent batch sizes and much lower
communication complexities with N≥10. Moreover, the required communication becomes lighter as batch
size increases. These properties are similar to those shown in the simulation and thus have generality.
Figure 6: The map for path ﬁnding problem. (“S1”, “S2”, “S3” denote the starting points of the 3 agents.
“D1”, “D2”, “D3” denote their destinations. “X” denotes an obstacle.)
Figure 7: Results on path ﬁnding problem.
12Published in Transactions on Machine Learning Research (06/2022)
5.3 Application to Path Finding Problem
In this subsection, we test our algorithms in solving a multi-agent path ﬁnding problem Ma et al. (2021),
which has broad real-world applications including aircraft-towing vehicles Morris et al. (2016), ware-house
and oﬃce robots Wurman et al. (2008); Veloso et al. (2015), and video games Silver (2005); Ma et al. (2017).
As illustrated in Figure 6, three agents start fron the points “S1”, “S2”, “S3” on a 10×10grid and aim to
reach the destination “D1”, “D2”, “D3”, respectively. In most cases, an agent can choose to move up, down,
left or right by one step or stay and receives -0.075 reward. However, when an agent reaches its destination,
it always stay and receives 3 reward if all the agents reach their destinations, or receives 0 reward otherwise.
If an agent has not reached its destination and collides with an obstacle “X” or another agent, it receives -0.5
reward.
Weapplytheaforementionedalgorithmswithdiﬀerentbatchsizes Ntosolvethisproblem. Theimplementation
details are mostly the same as those of the previous two-agent cliﬀ navigation problem, with the following
diﬀerences. (1) The behavior policy selects from all available actions uniformly at random, and the target
policy selects from available actions that avoid obstacles uniformly at random; (2) The communication
matrixUhas diagonal entries 0.6 and oﬀ-diagonals 0.2; (3) We use α= 0.08∗Nobtained by tuning while
β= 0.002∗Nis the same as aforementioned. (4) The feature vector of any state s(all the agents’ locations)
isφ(s) = [φ(1)(s),φ(2)(s),φ(3)(s)]whereφ(m)∈{0,1}5is deﬁned as follows: φ(m)
1(s) = 1,φ(m)
2(s) = 1or
φ(m)
3(s) = 1if and only if the m-th agent reaches its goal, is 1 step away from its goal, or is 1 horizontal step
and 1 vertical step away from its goal, respectively. φ(m)
4(s) = 1if and only if the m-th agent has not reached
its goal and there is at least one obstacle or other agents in its 8 surrounding grids. φ(m)
5(s) = 1if and only if
them-th agent has not reached its goal and collides with an obstacle “X” or another agent.
Figure 7 plots the value function at the target state where all the agents reach the goal (i.e., the sum of entries
(θt)1+ (θt)6+ (θt)11) v.s. the number of samples ( tN) and the number of communication rounds ( t). We
can see that all the algorithms converge to the true value 3/(1−γ) = 60. Compared with the decentralized
TD(0), our decentralized TDC achieve comparable sample complexities with diﬀerent batch sizes and much
lower communication complexities with N≥10. Moreover, the required communication becomes lighter as
batch size increases. These properties are similar to those shown in the simulation and thus have generality.
6 Conclusion
In this paper, we develop a sample-eﬃcient and communication-eﬃcient decentralized TDC algorithm for
multi-agent oﬀ-policy evaluation. Our algorithm synchronizes the local importance sampling ratios among
the agents and adopts mini-batch stochastic updates to save communication. In particular, it avoids sharing
agents’ sensitive local information. We prove that the proposed decentralized TDC algorithms achieve a
near-optimal sample complexity as well as an optimal communication complexity that improves over the
existing decentralized TD(0). In the future, we expect that our algorithm can serve as a fundamental
component in the design of advanced policy optimization algorithms for MARL.
Acknowledgments
The work of Z. Chen and Y. Zhou was supported in part by U.S. National Science Foundation under the
Grants CCF-2106216 and DMS-2134223.
13Published in Transactions on Machine Learning Research (06/2022)
References
Leemon Baird. Residual algorithms: Reinforcement learning with function approximation. In Proc. Interna-
tional Conference on Machine Learning (ICML) , pp. 30–37, 1995.
Jalaj Bhandari, Daniel Russo, and Raghav Singal. A ﬁnite time analysis of temporal diﬀerence learning
with linear function approximation. In Proc. Conference on Learning Theory (COLT) , volume 75, pp.
1691–1692, 2018.
Vivek S Borkar. Stochastic approximation: a dynamical systems viewpoint , volume 48. 2009.
Lucas Cassano, Kun Yuan, and Ali H Sayed. Multi-agent fully decentralized value function learning with
linear convergence rates. IEEE Transactions on Automatic Control , 2020.
Behdad Chalaki and Andreas A Malikopoulos. A hysteretic q-learning coordination framework for emerging
mobility systems in smart cities. ArXiv:2011.03137 , 2020.
Tianyi Chen, Kaiqing Zhang, Georgios B Giannakis, and Tamer Basar. Communication-eﬃcient policy
gradient methods for distributed reinforcement learning. IEEE Transactions on Control of Network Systems ,
2021.
Gal Dalal, Balazs Szorenyi, Gugan Thoppe, and Shie Mannor. Finite sample analysis of two-timescale
stochastic approximation with applications to reinforcement learning. In Proc. Conference on Learning
Theory (COLT) , 2018a.
Gal Dalal, Balázs Szörényi, Gugan Thoppe, and Shie Mannor. Finite sample analyses for td (0) with function
approximation. In Proc. Association for the Advancement of Artiﬁcial Intelligence (AAAI) , volume 32,
2018b.
Gal Dalal, Balazs Szorenyi, and Gugan Thoppe. A tale of two-timescale reinforcement learning with the
tightest ﬁnite-time bound. In Proc. Association for the Advancement of Artiﬁcial Intelligence (AAAI) ,
volume 34, pp. 3701–3708, 2020.
Peter Dayan. The convergence of td ( λ) for general λ.Machine learning , 8(3-4):341–362, 1992.
Thinh Doan, Siva Maguluri, and Justin Romberg. Finite-time analysis of distributed TD(0) with linear
function approximation on multi-agent reinforcement learning. In Proc. International Conference on
Machine Learning (ICML) , volume 97, pp. 1626–1635, 09–15 Jun 2019.
Geoﬀrey J Gordon. Stable function approximation in dynamic programming. In Machine Learning Proceedings
1995, pp. 261–268. 1995.
Harsh Gupta, R. Srikant, and Lei Ying. Finite-time performance bounds and adaptive learning rate selection
for two time-scale reinforcement learning. In Proc. Advances in Neural Information Processing Systems
(NeurIPS) , volume 32, pp. 4704–4713, 2019.
Bin Hu and Usman Ahmed Syed. Characterizing the exact behaviors of temporal diﬀerence learning algorithms
using markov jump linear system theory. In Proc. Advances in Neural Information Processing Systems
(NeurIPS) , pp. 8479–8490, 2019.
Tommi Jaakkola, Michael Jordan, and Satinder Singh. Convergence of stochastic iterative dynamic pro-
gramming algorithms. In Proc. Advances in Neural Information Processing Systems (NIPS) , volume 6, pp.
703–710, 1993.
Maxim Kaledin, Eric Moulines, Alexey Naumov, Vladislav Tadic, and Hoi-To Wai. Finite time analysis of
linear two-timescale stochastic approximation with markovian noise. In Proc. Conference on Learning
Theory (COLT) , pp. 2144–2203, 2020.
Nathaniel Korda and Prashanth La. On td (0) with function approximation: Concentration bounds and a
centered variant with exponential convergence. In Proc. International Conference on Machine Learning
(ICML), pp. 626–634, 2015.
14Published in Transactions on Machine Learning Research (06/2022)
Vikram Krishnamurthy, Michael Maskery, and George Yin. Decentralized adaptive ﬁltering algorithms for
sensor activation in an unattended ground sensor network. IEEE Transactions on Signal Processing , 56
(12):6086–6101, 2008.
Chandrashekar Lakshminarayanan and Csaba Szepesvari. Linear stochastic approximation: How far does
constant step-size and iterate averaging go? In Proc. International Conference on Artiﬁcial Intelligence
and Statistics (AISTATS) , pp. 1347–1355, 2018.
Bo Liu, Ji Liu, Mohammad Ghavamzadeh, Sridhar Mahadevan, and Marek Petrik. Finite-sample analysis of
proximal gradient td algorithms. In Proc. Conference on Uncertainty in Artiﬁcial Intelligence (UAI) , pp.
504–513, 2015.
Hang Ma, Jingxing Yang, Liron Cohen, TK Kumar, and Sven Koenig. Feasibility study: Moving non-
homogeneous teams in congested video game environments. In Proc. the AAAI Conference on Artiﬁcial
Intelligence and Interactive Digital Entertainment , volume 13, pp. 270–272, 2017.
Ziyuan Ma, Yudong Luo, and Hang Ma. Distributed heuristic multi-agent path ﬁnding with communication.
InProc. IEEE International Conference on Robotics and Automation (ICRA) , pp. 8699–8705. IEEE, 2021.
Sergio Valcarcel Macua, Jianshu Chen, Santiago Zazo, and Ali H Sayed. Distributed policy evaluation under
multiple behavior strategies. IEEE Transactions on Automatic Control , 60(5):1260–1274, 2014.
Hamid Reza Maei. Gradient temporal-diﬀerence learning algorithms . PhD thesis, University of Alberta, 2011.
Adwaitvedant Mathkar and Vivek S Borkar. Distributed reinforcement learning via gossip. IEEE Transactions
on Automatic Control , 62(3):1465–1470, 2016.
Robert Morris, Corina S Pasareanu, Kasper Luckow, Waqar Malik, Hang Ma, TK Satish Kumar, and Sven
Koenig. Planning, scheduling and monitoring for airport surface operations. In Workshops at the Thirtieth
AAAI Conference on Artiﬁcial Intelligence , 2016.
Wei Qiu, Xinrun Wang, Runsheng Yu, Rundong Wang, Xu He, Bo An, Svetlana Obraztsova, and Zinovi
Rabinovich. Rmix: Learning risk-sensitive policies for cooperative reinforcement learning agents. In Proc.
Advances in Neural Information Processing Systems (NeurIPS) , 2021.
Guannan Qu and Na Li. Harnessing smoothness to accelerate distributed optimization. IEEE Transactions
on Control of Network Systems , 5(3):1245–1260, 2017.
Guannan Qu, Adam Wierman, and Na Li. Scalable reinforcement learning of localized policies for multi-agent
networked systems. In Learning for Dynamics and Control (L4DC) , pp. 256–266, 2020.
Rajarshi Saha, Stefano Rini, Milind Rao, and Andrea Goldsmith. Decentralized optimization over noisy,
rate-constrained networks: How to agree by talking about how we disagree. ArXiv:2010.11292 , 2020.
Kevin Scaman, Francis Bach, Sébastien Bubeck, Yin Tat Lee, and Laurent Massoulié. Optimal algorithms
for smooth and strongly convex distributed optimization in networks. In Proc. International Conference on
Machine Learning (ICML) , volume 70, pp. 3027–3036, 2017.
Ma Shaocong, Zhou Yi, and Zou Shaofeng. Variance-reduced oﬀ-policy tdc learning: Non-asymptotic
convergence analysis. In Proc. Advances in Neural Information Processing Systems (NeurIPS) , 2020.
Ma Shaocong, Chen Ziyi, Zhou Yi, and Zou Shaofeng. Greedy-{gq} with variance reduction: Finite-time
analysis and improved complexity. In Proc. International Conference on Learning Representations (ICLR) ,
2021.
David Silver. Cooperative pathﬁnding. In Proc. the AAAI conference on artiﬁcial intelligence and interactive
digital entertainment , volume 1, pp. 117–122, 2005.
Navjot Singh, Deepesh Data, Jemin George, and Suhas Diggavi. Squarm-sgd: Communication-eﬃcient
momentum sgd for decentralized optimization. ArXiv:2005.07041 , 2020.
15Published in Transactions on Machine Learning Research (06/2022)
Rayadurgam Srikant and Lei Ying. Finite-time error bounds for linear stochastic approximation and td
learning. In Proc. Conference on Learning Theory (COLT) , pp. 2803–2830, 2019.
Miloš S Stanković and Srdjan S Stanković. Multi-agent temporal-diﬀerence learning with linear function
approximation: Weak convergence under time-varying network topologies. In Proc. American Control
Conference (ACC) , pp. 167–172, 2016.
Jun Sun, Gang Wang, Georgios B Giannakis, Qinmin Yang, and Zaiyue Yang. Finite-sample analysis of
decentralized temporal-diﬀerence learning with linear function approximation. In Proc. International
Conference on Artiﬁcial Intelligence and Statistics (AISTATS) , pp. 4485–4495, 2020.
Richard S Sutton. Learning to predict by the methods of temporal diﬀerences. Machine learning , 3(1):9–44,
1988.
Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction . 2018.
Richard S Sutton, Csaba Szepesvári, and Hamid Reza Maei. A convergent o(n) algorithm for oﬀ-policy
temporal-diﬀerence learning with linear function approximation. In Proc. Advances in Neural Information
Processing Systems (NIPS) , volume 21, pp. 1609–1616, 2008.
Richard S Sutton, Hamid Reza Maei, Doina Precup, Shalabh Bhatnagar, David Silver, Csaba Szepesvári,
and Eric Wiewiora. Fast gradient-descent methods for temporal-diﬀerence learning with linear function
approximation. In Proc. International Conference on Machine Learning (ICML) , pp. 993–1000, 2009.
Vladislav Tadić. On the convergence of temporal-diﬀerence learning with linear function approximation.
Machine learning , 42(3):241–267, 2001.
John N Tsitsiklis and Benjamin Van Roy. An analysis of temporal-diﬀerence learning with function approxi-
mation.IEEE transactions on automatic control , 42(5):674–690, 1997.
Manuela Veloso, Joydeep Biswas, Brian Coltin, and Stephanie Rosenthal. Cobots: Robust symbiotic au-
tonomous mobile service robots. In Twenty-Fourth International Joint Conference on Artiﬁcial Intelligence ,
2015.
Hoi-To Wai, Zhuoran Yang, Zhaoran Wang, and Mingyi Hong. Multi-agent reinforcement learning via
double averaging primal-dual optimization. In Proc. Advances in Neural Information Processing Systems
(NeurIPS) , pp. 9672–9683, 2018.
Gang Wang, Bingcong Li, and Georgios B Giannakis. A multistep lyapunov approach for ﬁnite-time analysis
of biased stochastic approximation. ArXiv:1909.04299 , 2019.
Gang Wang, Songtao Lu, Georgios Giannakis, Gerald Tesauro, and Jian Sun. Decentralized td tracking
with linear function approximation and its ﬁnite-time analysis. In Proc. Advances in Neural Information
Processing Systems (NeurIPS) , volume 33, 2020.
Peter R Wurman, Raﬀaello D’Andrea, and Mick Mountz. Coordinating hundreds of cooperative, autonomous
vehicles in warehouses. AI magazine , 29(1):9–9, 2008.
Tengyu Xu and Yingbin Liang. Sample complexity bounds for two timescale value-based reinforcement
learning algorithms. ArXiv:2011.05053 , 2020.
Tengyu Xu, Shaofeng Zou, and Yingbin Liang. Two time-scale oﬀ-policy td learning: Non-asymptotic analysis
over markovian samples. In Proc. Advances in Neural Information Processing Systems (NeurIPS) , pp.
10634–10644, 2019.
Tengyu Xu, Zhe Wang, and Yingbin Liang. Improving sample complexity bounds for (natural) actor-critic
algorithms. In Proc. Advances in Neural Information Processing Systems (NeurIPS) , volume 33, 2020a.
Tengyu Xu, Zhe Wang, Yi Zhou, and Yingbin Liang. Reanalysis of variance reduced temporal diﬀerence
learning. In Proc. International Conference on Learning Representations (ICLR) , 2020b.
16Published in Transactions on Machine Learning Research (06/2022)
Zhi Yan, Nicolas Jouandeau, and Arab Ali Cherif. A survey and analysis of multi-robot coordination.
International Journal of Advanced Robotic Systems , 10(12):399, 2013.
Evşen Yanmaz, Markus Quaritsch, Saeed Yahyanejad, Bernhard Rinner, Hermann Hellwagner, and Christian
Bettstetter. Communication and coordination for drone networks. In Proc. International Conference on
Ad Hoc Networks , pp. 79–91, 2017.
Mingqi Yuan, Qi Cao, Man-on Pun, and Yi Chen. Towards user scheduling for 6g: A fairness-oriented
scheduler using multi-agent reinforcement learning. ArXiv:2012.15081 , 2020.
Junyu Zhang, Amrit Singh Bedi, Mengdi Wang, and Alec Koppel. Marl with general utilities via decentralized
shadow reward actor-critic. ArXiv:2106.00543 , 2021.
Shaofeng Zou, Tengyu Xu, and Yingbin Liang. Finite-sample analysis for sarsa with linear function
approximation. In Proc. Advances in Neural Information Processing Systems , pp. 8665–8675, 2019.
A Notations and Filtration
A.1 Notations to rewrite update rules in Algorithm 1
We introduce the notations in Table 1 that will be used throughout our proof .
Table 1: List of notations
Notations Explanation
ρi,Ai,Bi,Ci(deﬁned in eq. (3)), These quantities of the i-th sample
b(m)
i=ρiR(m)
iφ(si) use exact global importance sampling ratio ρi.
A(m)
i:=/hatwideρ(m)
iφ(si)(γφ(si+1)−φ(si))/latticetopReplaceρiandRiinAi,Bi(see eq. (3))
B(m)
i:=/hatwideρ(m)
iφ(si+1)φ(si))/latticetopwith/hatwideρ(m)
iandR(m)
irespectively.
/hatwideb(m)
i:=/hatwideρ(m)
iR(m)
iφ(si) Replaceρiinb(m)
iwith/hatwideρ(m)
i(deﬁned by eqs. (5) and (6)).
θt=1
M/summationtextM
m=1θ(m)
tAgents’ average model parameter.
wtandbiare deﬁned similarly.
/tildewideAt=1
N/summationtext(t+1)N−1
i=tNAiMinibatch average of global quantity.
/tildewideBtand/tildewideCtare deﬁned similarly.
/tildewideA(m)
t=1
N/summationtext(t+1)N−1
i=tNA(m)
iAverage local quantity over the minibatch at the t-th iteration.
/tildewideB(m)
t,/tildewideb(m)
tand/tildewide/hatwideb(m)
tare deﬁned similarly.
/tildewidebt=1
N/summationtext(t+1)N−1
i=tNbi=1
M/summationtextM
m=1/tildewideb(m)
t Average over both agents and minibatch.
A:=Eπb[Ai], B:=Eπb[Bi]Expected quantities.C:=Eπb[Ci],b: =Eπb/bracketleftbig
bi/bracketrightbig
θ∗=−A−1b The optimal model parameter
w∗
t=−C−1(Aθt+b) The optimal auxiliary parameter corresponding to θt.
/bardblA/bardblF:=/parenleftbig/summationtext
i,jA2
i,j/parenrightbig1/2Frobenius norm.
Then, by averaging the update rules (7) and (8) over m, we obtain the following update rules of the model
averageθt,wt.
θt+1=θt+α
MM/summationdisplay
m=1/parenleftbig/tildewideA(m)
tθ(m)
t+/tildewide/hatwideb(m)
t+/tildewideB(m)
tw(m)
t/parenrightbig
, (14)
wt+1=wt+β
MM/summationdisplay
m=1/parenleftbig/tildewideA(m)
tθ(m)
t+/tildewide/hatwideb(m)
t+/tildewideCtw(m)
t/parenrightbig
. (15)
17Published in Transactions on Machine Learning Research (06/2022)
A.2 Filtration
Deﬁne the ﬁltration Ft=σ/parenleftbig
{st/prime,at/prime}tN−1
t/prime=1∪{stN}/parenrightbig
. Then,
/tildewideAt,/tildewideBt,/tildewideCt,/tildewideb(m)
t,/tildewidebt,/tildewideA(m)
t,/tildewideB(m)
t,/tildewide/hatwideb(m)
t∈Ft+1/Ft,θ(m)
t,θt,w(m)
t,wt,w∗
t∈Ft/Ft−1.
B Proof of Theorem 1
Step 1: Bounding optimization error for ideal case. We ﬁrst consider an ideal case where the agents
can access the exact global importance sampling ratio ρtat iteration t. In this ideal case, every agent mcan
replace the estimated global importance sampling ratio /hatwideρ(m)
iinvolved in A(m)
i,B(m)
i,/hatwideb(m)
iin the update rules
(7) and (8) by the exact value ρi. Then, with the notations deﬁned in Appendix A, the averaged update rules
(14) and (15) respectively become eqs. (12) and (13) as repeated below
/tildewideθt+1=θt+α/parenleftbig/tildewideAtθt+/tildewidebt+/tildewideBtwt/parenrightbig
,
/tildewidewt+1=wt+β/parenleftbig/tildewideAtθt+/tildewidebt+/tildewideCtwt/parenrightbig
.
The aim of Step 1 is to bound the following optimization errors of /tildewidewt+1,/tildewideθt+1obtained by the centralized
update rules (13) and (12) respectively.
E/bracketleftbig
/bardbl/tildewidewt+1−w∗
t/bardbl2/vextendsingle/vextendsingleFt/bracketrightbig
=/bardblwt−w∗
t/bardbl2+ 2β(wt−w∗
t)/latticetopE/bracketleftbig/tildewideAtθt+/tildewidebt+/tildewideCtwt/vextendsingle/vextendsingleFt/bracketrightbig
/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
(I)+β2E/bracketleftbig/vextenddouble/vextenddouble/tildewideAtθt+/tildewidebt+/tildewideCtwt/vextenddouble/vextenddouble2/vextendsingle/vextendsingleFt/bracketrightbig
/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
(II),(16)
E/bracketleftbig
/bardbl/tildewideθt+1−θ∗/bardbl2/vextendsingle/vextendsingleFt/bracketrightbig
=/bardblθt−θ∗/bardbl2+ 2α/parenleftbig
θt−θ∗/parenrightbig/latticetopE/bracketleftbig/tildewideAtθt+/tildewidebt+/tildewideBtwt/vextendsingle/vextendsingleFt/bracketrightbig
/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
(III)+α2E/bracketleftbig/vextenddouble/vextenddouble/tildewideAtθt+/tildewidebt+/tildewideBtwt/vextenddouble/vextenddouble2/vextendsingle/vextendsingleFt/bracketrightbig
/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
(IV).(17)
The above four terms (I)-(IV) are respectively bounded below.
(I) = (wt−w∗
t)/latticetopE/bracketleftbig/tildewideAtθt+/tildewidebt+/tildewideCtwt/vextendsingle/vextendsingleFt/bracketrightbig
(i)= 2(wt−w∗
t)/latticetopE/bracketleftbig/tildewideCt−C/vextendsingle/vextendsingleFt/bracketrightbig
(wt−w∗
t) + 2(wt−w∗
t)/latticetopC(wt−w∗
t) + 2(wt−w∗
t)/latticetop
E/bracketleftbig
(/tildewideAt−/tildewideCtC−1A)θt+/tildewidebt−/tildewideCtC−1b/vextendsingle/vextendsingleFt/bracketrightbig
(ii)
≤2/vextenddouble/vextenddoubleE/bracketleftbig/tildewideCt−C/vextendsingle/vextendsingleFt/bracketrightbig/vextenddouble/vextenddouble
F/vextenddouble/vextenddoublewt−w∗
t/vextenddouble/vextenddouble2−2λ2/vextenddouble/vextenddoublewt−w∗
t/vextenddouble/vextenddouble2+λ2/bardblwt−w∗
t/bardbl2+3
λ2E/bracketleftbig/vextenddouble/vextenddouble/tildewidebt−/tildewideCtC−1b/vextenddouble/vextenddouble2/vextendsingle/vextendsingleFt/bracketrightbig
+3
λ2E/bracketleftbig/vextenddouble/vextenddouble(/tildewideAt−/tildewideCtC−1A)/vextenddouble/vextenddouble2
F/vextendsingle/vextendsingleFt/bracketrightbig/vextenddouble/vextenddoubleθt−θ∗/vextenddouble/vextenddouble2+3
λ2E/bracketleftbig/vextenddouble/vextenddouble/tildewideAt−/tildewideCtC−1A/vextenddouble/vextenddouble2
F/vextendsingle/vextendsingleFt/bracketrightbig/vextenddouble/vextenddoubleθ∗/vextenddouble/vextenddouble2
(iii)
≤/parenleftBig4νρmax
N(1−δ)−λ2/parenrightBig/vextenddouble/vextenddoublewt−w∗
t/vextenddouble/vextenddouble2+96ρ2
max(ν+ 1)
Nλ2(1−δ)/parenleftBig
1 +1
λ2/parenrightBig2/parenleftbig/vextenddouble/vextenddoubleθt−θ∗/vextenddouble/vextenddouble2+/vextenddouble/vextenddoubleθ∗/vextenddouble/vextenddouble2+R2
max/parenrightbig
(18)
where (i) uses the notation that w∗
t=−C−1(Aθt+b), (ii) uses λ2=−λmax(C), the inequality that
/bardblAx/bardbl≤/bardblA/bardblF/bardblx/bardblfor any matrix Aand vector x, and the inequality that 2a/latticetop
1a2≤σ−1/bardbla1/bardbl2+σ/bardbla2/bardbl2for
anya1,a2∈Rdandσ>0, and applies Jensen’s inequality to convex functions /bardbl·/bardbland/bardbl·/bardbl2and uses eq.
(44), (iv) uses eqs. (47) and (48).
(II) =E/bracketleftbig/vextenddouble/vextenddouble/tildewideAtθt+/tildewidebt+/tildewideCtwt/vextenddouble/vextenddouble2/vextendsingle/vextendsingleFt/bracketrightbig
18Published in Transactions on Machine Learning Research (06/2022)
(i)=E/bracketleftbig/vextenddouble/vextenddouble(/tildewideAt−/tildewideCtC−1A)(θt−θ∗) + (/tildewideAt−/tildewideCtC−1A)θ∗+/tildewidebt+/tildewideCt(wt−w∗
t)−/tildewideCtC−1b/vextenddouble/vextenddouble2/vextendsingle/vextendsingleFt/bracketrightbig
(ii)
≤4E/bracketleftbig
/bardbl/tildewideAt−/tildewideCtC−1A/bardbl2
F/vextendsingle/vextendsingleFt/bracketrightbig
/bardblθt−θ∗/bardbl2+ 4/bardblwt−w∗
t/bardbl2
+ 4E/bracketleftbig
/bardbl/tildewideAt−/tildewideCtC−1A/bardbl2
F/vextendsingle/vextendsingleFt/bracketrightbig
/bardblθ∗/bardbl2+ 4E/bracketleftbig
/bardbl/tildewidebt−/tildewideCtC−1b/bardbl2/vextendsingle/vextendsingleFt/bracketrightbig
(iii)
≤128ρ2
max(ν+ 1)
N(1−δ)/parenleftBig
1 +1
λ2/parenrightBig2/parenleftbig
/bardblθt−θ∗/bardbl2+/bardblθ∗/bardbl2+R2
max/parenrightbig
+ 4/vextenddouble/vextenddoublewt−w∗
t/vextenddouble/vextenddouble2(19)
where (i) uses the notation that w∗
t=−C−1(Aθt+b), (ii) uses/bardbl/summationtext4
k=1ak/bardbl2≤4/summationtext4
k=1/bardblak/bardbl2for any
a1,a2,a3,a4∈Rdand eq. (41), and /bardblAx/bardbl≤/bardblA/bardblF/bardblx/bardblfor any matrix Aand vector x, (iii) uses eqs. (47)
and (48).
(III) =/parenleftbig
θt−θ∗/parenrightbig/latticetopE/bracketleftbig/tildewideAtθt+/tildewidebt+/tildewideBtwt/vextendsingle/vextendsingleFt/bracketrightbig
(i)= 2/parenleftbig
θt−θ∗/parenrightbig/latticetopE/bracketleftbig
(/tildewideAt−/tildewideBtC−1A−A/latticetopC−1A)(θt−θ∗) +A/latticetopC−1A(θt−θ∗)
+ (/tildewideAt−/tildewideBtC−1A−A/latticetopC−1A)θ∗+/tildewideBt(wt−w∗
t) +/tildewidebt−b−(/tildewideBt−B)C−1b/vextendsingle/vextendsingleFt/bracketrightbig
(ii)
≤2E/bracketleftbig/vextenddouble/vextenddouble/tildewideAt−/tildewideBtC−1A−A/latticetopC−1A/vextenddouble/vextenddouble/vextendsingle/vextendsingleFt/bracketrightbig/vextenddouble/vextenddoubleθt−θ∗/vextenddouble/vextenddouble2−2λ1/vextenddouble/vextenddoubleθt−θ∗/vextenddouble/vextenddouble2+λ1/bardblθt−θ∗/bardbl2+/vextenddouble/vextenddouble/tildewideBt(wt−w∗
t)/vextenddouble/vextenddouble2
+4
λ1E/bracketleftbig/vextenddouble/vextenddouble/tildewideAt−/tildewideBtC−1A−A/latticetopC−1A/vextenddouble/vextenddouble2/vextenddouble/vextenddoubleθ∗/vextenddouble/vextenddouble2+/vextenddouble/vextenddouble/tildewidebt−b/vextenddouble/vextenddouble2+/vextenddouble/vextenddouble(/tildewideBt−B)C−1b/vextenddouble/vextenddouble2/vextendsingle/vextendsingleFt/bracketrightbig
(iii)
≤/parenleftBig64ρ2
max(ν+ 1)
N(1−δ)/parenleftBig
1 +ρmax
λ2/parenrightBig2
−λ1/parenrightBig/vextenddouble/vextenddoubleθt−θ∗/vextenddouble/vextenddouble2+4ρ2
max
λ1/vextenddouble/vextenddoublewt−w∗
t/vextenddouble/vextenddouble2
+32ρ2
max(ν+ 1)
Nλ1(1−δ)/parenleftBig
4/parenleftBig
1 +ρmax
λ2/parenrightBig2/vextenddouble/vextenddoubleθ∗/vextenddouble/vextenddouble2+R2
max+ρmaxRmax
λ2/parenrightBig
(20)
where (i) uses the notations that w∗
t=−C−1(Aθt+b)and thatb=−Aθ∗, and the relation that C−B=A/latticetop,
(ii) uses the notation that λ1=−λmax(A/latticetopC−1A)and the inequality that 2a/latticetop
1a2≤σ−1/bardbla1/bardbl2+σ/bardbla2/bardbl2for
anya1,a2∈Rdandσ>0, and applies Jensen’s inequality to the convex functions /bardbl·/bardbland/bardbl·/bardbl2, (iii) uses
eqs. (40), (42), (43), (45), (46) and (49).
(IV) =E/bracketleftbig/vextenddouble/vextenddouble/tildewideAtθt+/tildewidebt+/tildewideBtwt/vextenddouble/vextenddouble2/vextendsingle/vextendsingleFt/bracketrightbig
(i)=E/bracketleftbig/vextenddouble/vextenddouble(/tildewideAt−/tildewideBtC−1A)(θt−θ∗) +/tildewidebt−b+/tildewideBt(wt−w∗
t)
+ (/tildewideAt−/tildewideBtC−1A−A/latticetopC−1A)θ∗−(/tildewideBt−B)C−1b/vextenddouble/vextenddouble2/vextendsingle/vextendsingleFt/bracketrightbig
(ii)
≤10E/bracketleftbig/vextenddouble/vextenddouble/tildewideAt/vextenddouble/vextenddouble2
F+/vextenddouble/vextenddouble/tildewideBtC−1A/vextenddouble/vextenddouble2
F/vextendsingle/vextendsingleFt/bracketrightbig/vextenddouble/vextenddoubleθt−θ∗/vextenddouble/vextenddouble2+ 5E/bracketleftbig/vextenddouble/vextenddouble/tildewideAt−/tildewideBtC−1A−A/latticetopC−1A/vextenddouble/vextenddouble2
F/vextendsingle/vextendsingleFt/bracketrightbig/vextenddouble/vextenddoubleθ∗/vextenddouble/vextenddouble2
+ 5E/bracketleftbig/vextenddouble/vextenddouble/tildewidebt−b/vextenddouble/vextenddouble2/vextendsingle/vextendsingleFt/bracketrightbig
+ 5E/bracketleftbig/vextenddouble/vextenddouble/tildewideBt/vextenddouble/vextenddouble2
F/vextendsingle/vextendsingleFt/bracketrightbig/vextenddouble/vextenddoublewt−w∗
t/vextenddouble/vextenddouble2+ 5E/bracketleftbig/vextenddouble/vextenddouble/tildewideBt−B/vextenddouble/vextenddouble2
F/vextendsingle/vextendsingleFt/bracketrightbig/vextenddouble/vextenddoubleC−1b/vextenddouble/vextenddouble2
(iii)
≤40ρ2
max/parenleftBig
1 +ρmax
λ2/parenrightBig2/vextenddouble/vextenddoubleθt−θ∗/vextenddouble/vextenddouble2+ 5ρ2
max/vextenddouble/vextenddoublewt−w∗
t/vextenddouble/vextenddouble2
+160ρ2
max(ν+ 1)
N(1−δ)/parenleftBig
1 +ρmax
λ2/parenrightBig2/parenleftbig/vextenddouble/vextenddoubleθ∗/vextenddouble/vextenddouble2+R2
max/parenrightbig
(21)
where (i) uses the notations that w∗
t=−C−1(Aθt+b)and thatb=−Aθ∗, and the relation that C−B=A/latticetop,
(ii) uses/bardblAx/bardbl ≤ /bardblA/bardblF/bardblx/bardblfor any matrix Aand vector xand/bardbl/summationtextK
k=1ak/bardbl2≤K/summationtextK
k=1/bardblak/bardbl2for any
ak∈Rdand eqs. (40), (45), (46) and (49), (iii) uses eqs. (39), (40), (42) and (43) and (1 +ρ2
max/λ2
2)≤
(1 +ρmax/λ2)2. Substituting the above terms (18)-(21) into eqs. (16) and (17) gives the following upper
bounds of E/bracketleftbig
/bardbl/tildewidewt+1−w∗
t/bardbl2/vextendsingle/vextendsingleFt/bracketrightbig
andE/bracketleftbig
/bardbl/tildewideθt+1−θ∗/bardbl2/vextendsingle/vextendsingleFt/bracketrightbig
.
E/bracketleftbig
/bardbl/tildewidewt+1−w∗
t/bardbl2/vextendsingle/vextendsingleFt/bracketrightbig
≤/bracketleftBig
1 + 2β/parenleftBig4νρmax
N(1−δ)−λ2/parenrightBig
+ 4β2/bracketrightBig/vextenddouble/vextenddoublewt−w∗
t/vextenddouble/vextenddouble2
19Published in Transactions on Machine Learning Research (06/2022)
+64ρ2
max(ν+ 1)
N(1−δ)/parenleftBig
1 +1
λ2/parenrightBig2/parenleftBig3β
λ2+ 2β2/parenrightBig/parenleftbig
/bardblθt−θ∗/bardbl2+/bardblθ∗/bardbl2+R2
max/parenrightbig
(i)
≤/parenleftBig
1−βλ2
2/parenrightBig/vextenddouble/vextenddoublewt−w∗
t/vextenddouble/vextenddouble2+320βρ2
max(ν+ 1)
Nλ2(1−δ)/parenleftBig
1 +1
λ2/parenrightBig2/parenleftbig
/bardblθt−θ∗/bardbl2+/bardblθ∗/bardbl2+R2
max/parenrightbig
,(22)
where (i) uses N≥8νρmax
λ2(1−δ)andβ≤min/parenleftbigλ2
8,1
λ2/parenrightbig
.
E/bracketleftbig
/bardbl/tildewideθt+1−θ∗/bardbl2/vextendsingle/vextendsingleFt/bracketrightbig
≤/parenleftBig
1 +128αρ2
max(ν+ 1)
N(1−δ)/parenleftBig
1 +ρmax
λ2/parenrightBig2
−2αλ1+ 40α2ρ2
max/parenleftBig
1 +ρmax
λ2/parenrightBig2/parenrightBig/vextenddouble/vextenddoubleθt−θ∗/vextenddouble/vextenddouble2
+αρ2
max/parenleftBig8
λ1+ 5α/parenrightBig/vextenddouble/vextenddoublewt−w∗
t/vextenddouble/vextenddouble2+64αρ2
max(ν+ 1)
Nλ1(1−δ)/parenleftBig
4/parenleftBig
1 +ρmax
λ2/parenrightBig2/vextenddouble/vextenddoubleθ∗/vextenddouble/vextenddouble2+R2
max+ρmaxRmax
λ2/parenrightBig
+160α2ρ2
max(ν+ 1)
N(1−δ)/parenleftBig
1 +ρmax
λ2/parenrightBig2/parenleftbig/vextenddouble/vextenddoubleθ∗/vextenddouble/vextenddouble2+R2
max/parenrightbig
(i)
≤/parenleftBig
1−αλ1
2/parenrightBig/vextenddouble/vextenddoubleθt−θ∗/vextenddouble/vextenddouble2+13αρ2
max
λ1/vextenddouble/vextenddoublewt−w∗
t/vextenddouble/vextenddouble2+224αρ2
max(ν+ 1)
Nλ1(1−δ)/parenleftBig
1 +ρmax
λ2/parenrightBig2/parenleftbig
4/vextenddouble/vextenddoubleθ∗/vextenddouble/vextenddouble2+ 2R2
max+ 1/parenrightbig
,
(23)
where (i) uses N≥128ρ2
max(ν+1)
λ1(1−δ)/parenleftbig
1+ρmax
λ2/parenrightbig2,α≤min/bracketleftbigλ1
40ρ2max/parenleftbig
1+ρmax
λ2/parenrightbig−2,1
λ1/bracketrightbig
andρmaxRmax
λ2≤/parenleftbig
1+ρmax
λ2/parenrightbig2+R2
max.
Step 2: Bounding optimization error of Algorithm 1. With the above upper bounds (22) and (23),
we derive the upper bounds of E/bracketleftbig
/bardblwt+1−w∗
t/bardbl2/vextendsingle/vextendsingleFt/bracketrightbig
,E/bracketleftbig
/bardblwt+1−w∗
t+1/bardbl2/vextendsingle/vextendsingleFt/bracketrightbig
andE/bracketleftbig
/bardblθt+1−θ∗/bardbl2/vextendsingle/vextendsingleFt/bracketrightbig
as follows.
E/bracketleftbig
/bardblwt+1−w∗
t/bardbl2/vextendsingle/vextendsingleFt/bracketrightbig
(i)
≤/parenleftBig
1 +1
6/(βλ2)−3/parenrightBig
E/parenleftbig
/bardbl/tildewidewt−w∗
t/bardbl2/vextendsingle/vextendsingleFt/parenrightbig
+/parenleftBig
1 +6
βλ2−3/parenrightBig
E/parenleftbig/vextenddouble/vextenddoublewt−/tildewidewt/vextenddouble/vextenddouble2/vextendsingle/vextendsingleFt/parenrightbig
(ii)
≤6−2βλ2
6−3βλ2/bracketleftBig/parenleftBig
1−βλ2
2/parenrightBig/vextenddouble/vextenddoublewt−w∗
t/vextenddouble/vextenddouble2+320βρ2
max(ν+ 1)
Nλ2(1−δ)/parenleftBig
1 +1
λ2/parenrightBig2/parenleftbig
/bardblθt−θ∗/bardbl2+/bardblθ∗/bardbl2+R2
max/parenrightbig/bracketrightBig
+6
βλ2E/bracketleftBig/vextenddouble/vextenddouble/vextenddoubleβ
MM/summationdisplay
m=1/bracketleftbig
(/tildewideA(m)
t−/tildewideAt)θ(m)
t+/tildewide/hatwideb(m)
t−/tildewideb(m)
t/bracketrightbig/vextenddouble/vextenddouble/vextenddouble2/vextendsingle/vextendsingle/vextendsingleFt/bracketrightBig
(iii)
≤/parenleftBig
1−βλ2
3/parenrightBig/vextenddouble/vextenddoublewt−w∗
t/vextenddouble/vextenddouble2+4
3320βρ2
max(ν+ 1)
Nλ2(1−δ)/parenleftBig
1 +1
λ2/parenrightBig2/parenleftbig
/bardblθt−θ∗/bardbl2+/bardblθ∗/bardbl2+R2
max/parenrightbig
+6β
λ2E/bracketleftBig1
MM/summationdisplay
m=1/parenleftBig/vextenddouble/vextenddouble(/tildewideA(m)
t−/tildewideAt)θ(m)
t+/tildewide/hatwideb(m)
t−/tildewideb(m)
t/vextenddouble/vextenddouble2/parenrightBig/vextendsingle/vextendsingle/vextendsingleFt/bracketrightBig
(iv)
≤/parenleftBig
1−βλ2
3/parenrightBig
/bardblwt−w∗
t/bardbl2+427βρ2
max(ν+ 1)
Nλ2(1−δ)/parenleftBig
1 +1
λ2/parenrightBig2/parenleftbig
/bardblθt−θ∗/bardbl2+/bardblθ∗/bardbl2+R2
max/parenrightbig
+12βσL/4
2
Mλ 2/parenleftBig
16 max
m∈M(/bardblθ(m)
0/bardbl+/bardblw(m)
0/bardbl+Rmax)2[1 +β(2ρmax+ 3)]2t+R2
max/parenrightBig
, (24)
where (i) uses the inequality that /bardbla1+a2/bardbl2≤(1 +σ)/bardbla1/bardbl2+ (1 +σ−1)/bardbla2/bardbl2for anya1,a2∈Rdand
σ >0, (ii) uses eqs. (15), (13) and (22), (iii) applies Jensen’s inequality to the convex function /bardbl·/bardbl2and
usesβ≤1
λ2, (iv) uses the condition that β≤1
λ2which implies that 1 +1
6/(βλ2)−3≤2, the inequality that
/bardbla1+a2/bardbl2≤2/bardbla1/bardbl2+ 2/bardbla2/bardbl2for anya1,a2∈Rd, and eqs. (52), (54) and (68).
E/bracketleftbig
/bardblwt+1−w∗
t+1/bardbl2/vextendsingle/vextendsingleFt/bracketrightbig
(i)
≤/parenleftBig
1 +1
2[3/(βλ2)−1]/parenrightBig
E/bracketleftbig
/bardblwt+1−w∗
t/bardbl2/vextendsingle/vextendsingleFt/bracketrightbig
+/bracketleftbig
1 + 2/parenleftbig
3/(βλ2)−1/parenrightbig/bracketrightbig
E/bracketleftbig
/bardblw∗
t+1−w∗
t/bardbl2/vextendsingle/vextendsingleFt/bracketrightbig
20Published in Transactions on Machine Learning Research (06/2022)
(ii)
≤6/(βλ2)−1
2[3/(βλ2)−1]/bracketleftBig/parenleftBig
1−βλ2
3/parenrightBig
/bardblwt−w∗
t/bardbl2+427βρ2
max(ν+ 1)
Nλ2(1−δ)/parenleftBig
1 +1
λ2/parenrightBig2/parenleftbig
/bardblθt−θ∗/bardbl2+/bardblθ∗/bardbl2+R2
max/parenrightbig
+12βσL/4
2
Mλ 2/parenleftBig
16 max
m∈M(/bardblθ(m)
0/bardbl+/bardblw(m)
0/bardbl+Rmax)2[1 +β(2ρmax+ 3)]2t+R2
max/parenrightBig/bracketrightBig
+6
βλ2E/bracketleftbig
/bardblC−1A(θt+1−θt)/bardbl2/vextendsingle/vextendsingleFt/bracketrightbig
(iii)
≤/parenleftBig
1−βλ2
6/parenrightBig
/bardblwt−w∗
t/bardbl2+534βρ2
max(ν+ 1)
Nλ2(1−δ)/parenleftBig
1 +1
λ2/parenrightBig2/parenleftbig
/bardblθt−θ∗/bardbl2+/bardblθ∗/bardbl2+R2
max/parenrightbig
+15βσL/4
2
Mλ 2/parenleftbig
17 max
m∈M(/bardblθ(m)
0/bardbl+/bardblw(m)
0/bardbl+Rmax)2[1 +β(2ρmax+ 3)]2t/parenrightbig
+24α2ρ2
max
βλ3
2E/bracketleftBig/vextenddouble/vextenddouble/vextenddouble1
MM/summationdisplay
m=1/parenleftbig/tildewideA(m)
tθ(m)
t+/tildewide/hatwideb(m)
t+/tildewideB(m)
tw(m)
t/parenrightbig/vextenddouble/vextenddouble/vextenddouble2/vextendsingle/vextendsingle/vextendsingleFt/bracketrightBig
/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
(V), (25)
where (i) uses the inequality that /bardbla1+a2/bardbl2≤(1 +σ)/bardbla1/bardbl2+ (1 +σ−1)/bardbla2/bardbl2for anya1,a2∈Rdandσ>0,
(ii) uses eq. (24) as well as the notation that w∗
t=−C−1(Aθt+b), (iii) uses β≤1
λ2(this implies that
6/(βλ2)−1
2[3/(βλ2)−1]≤5
4) and eqs. (14), (39) and (43). The above term (V) can be upper bounded as follows.
(V) =E/bracketleftBig/vextenddouble/vextenddouble/vextenddouble1
MM/summationdisplay
m=1/parenleftbig/tildewideA(m)
tθ(m)
t+/tildewide/hatwideb(m)
t+/tildewideB(m)
tw(m)
t/parenrightbig/vextenddouble/vextenddouble/vextenddouble2/vextendsingle/vextendsingle/vextendsingleFt/bracketrightBig
(i)
≤2E/bracketleftBig/vextenddouble/vextenddouble/vextenddouble1
MM/summationdisplay
m=1/parenleftbig
(/tildewideA(m)
t−/tildewideAt)θ(m)
t+ (/tildewide/hatwideb(m)
t−/tildewideb(m)
t) + (/tildewideB(m)
t−/tildewideBt)w(m)
t/parenrightbig/vextenddouble/vextenddouble/vextenddouble2/vextendsingle/vextendsingle/vextendsingleFt/bracketrightBig
+ 2E/bracketleftBig/vextenddouble/vextenddouble/vextenddouble1
MM/summationdisplay
m=1/parenleftbig/tildewideAtθ(m)
t+/tildewideb(m)
t+/tildewideBtw(m)
t/parenrightbig/vextenddouble/vextenddouble/vextenddouble2/vextendsingle/vextendsingle/vextendsingleFt/bracketrightBig
(ii)
≤2
MM/summationdisplay
m=1E/bracketleftbig/vextenddouble/vextenddouble/parenleftbig
(/tildewideA(m)
t−/tildewideAt)θ(m)
t+ (/tildewide/hatwideb(m)
t−/tildewideb(m)
t) + (/tildewideB(m)
t−/tildewideBt)w(m)
t/parenrightbig/vextenddouble/vextenddouble2/vextendsingle/vextendsingleFt/bracketrightbig
+ 2E/parenleftbig/vextenddouble/vextenddouble/tildewideAtθt+/tildewidebt+/tildewideBtwt/vextenddouble/vextenddouble2/vextendsingle/vextendsingleFt/parenrightbig
(iii)
≤6
MM/summationdisplay
m=1E/bracketleftbig/vextenddouble/vextenddouble/tildewideA(m)
t−/tildewideAt/vextenddouble/vextenddouble2/vextenddouble/vextenddoubleθ(m)
t/vextenddouble/vextenddouble2+/vextenddouble/vextenddouble/tildewide/hatwideb(m)
t−/tildewideb(m)
t/vextenddouble/vextenddouble2+/vextenddouble/vextenddouble/tildewideB(m)
t−/tildewideBt/vextenddouble/vextenddouble2/vextenddouble/vextenddoublew(m)
t/vextenddouble/vextenddouble2/vextendsingle/vextendsingleFt/bracketrightbig
+ 2/parenleftBig
40ρ2
max/parenleftBig
1 +ρmax
λ2/parenrightBig2/vextenddouble/vextenddoubleθt−θ∗/vextenddouble/vextenddouble2+ 5ρ2
max/vextenddouble/vextenddoublewt−w∗
t/vextenddouble/vextenddouble2+160ρ2
max(ν+ 1)
N(1−δ)/parenleftBig
1 +ρmax
λ2/parenrightBig2/parenleftbig/vextenddouble/vextenddoubleθ∗/vextenddouble/vextenddouble2+R2
max/parenrightbig/parenrightBig
(iv)
≤6σL/4
2
M/parenleftBig
20 max
m∈M(/bardblθ(m)
0/bardbl+/bardblw(m)
0/bardbl+Rmax)2[1 +β(2ρmax+ 3)]2t+R2
max/parenrightBig
+ 2/parenleftBig
40ρ2
max/parenleftBig
1 +ρmax
λ2/parenrightBig2/vextenddouble/vextenddoubleθt−θ∗/vextenddouble/vextenddouble2+ 5ρ2
max/vextenddouble/vextenddoublewt−w∗
t/vextenddouble/vextenddouble2+160ρ2
max(ν+ 1)
N(1−δ)/parenleftBig
1 +ρmax
λ2/parenrightBig2/parenleftbig/vextenddouble/vextenddoubleθ∗/vextenddouble/vextenddouble2+R2
max/parenrightbig/parenrightBig
where (i) uses the inequality that /bardbla1+a2/bardbl2≤2/bardbla1/bardbl2+ 2/bardbla2/bardbl2for anya1,a2∈Rd, (ii) applies Jensen’s
inequalitytotheconvexfunction /bardbl·/bardbl2, (iii)useseq. (21)andtheinequalitythat /bardbla1+a2+a3/bardbl2≤3/summationtext3
k=1/bardblak/bardbl2
for anya1,a2,a3∈Rd, (iv) uses eqs. (52), (53), (54) and (68). Substituting the above inequality into eq.
(25) yields that
E/bracketleftbig
/bardblwt+1−w∗
t+1/bardbl2/vextendsingle/vextendsingleFt/bracketrightbig
≤/parenleftBig
1−βλ2
6+120α2ρ4
max
βλ3
2/parenrightBig
/bardblwt−w∗
t/bardbl2+/bracketleftBig534βρ2
max(ν+ 1)
Nλ2(1−δ)/parenleftBig
1 +1
λ2/parenrightBig2
+1920α2ρ4
max
βλ3
2/parenleftBig
1 +ρmax
λ2/parenrightBig2/bracketrightBig
/bardblθt−θ∗/bardbl2
+3840ρ2
max(ν+ 1)
N(1−δ)/bracketleftBigβ
λ2/parenleftBig
1+1
λ2/parenrightBig2
+α2ρ2
max
βλ3
2/parenleftBig
1+ρmax
λ2/parenrightBig2/bracketrightBig/parenleftbig
/bardblθ∗/bardbl2+R2
max/parenrightbig
21Published in Transactions on Machine Learning Research (06/2022)
+255σL/4
2
Mλ 2/parenleftBig
β+12α2ρ2
max
βλ2
2/parenrightBig
max
m∈M(/bardblθ(m)
0/bardbl+/bardblw(m)
0/bardbl+Rmax)2[1 +β(2ρmax+ 3)]2t. (26)
E/bracketleftbig
/bardblθt+1−θ∗/bardbl2/vextendsingle/vextendsingleFt/bracketrightbig
(i)
≤/parenleftBig
1 +1
6/(αλ1)−3/parenrightBig
E/bracketleftbig/tildewideθt+1−θ∗/vextendsingle/vextendsingleFt/bracketrightbig
+/parenleftBig
1 +6
αλ1−3/parenrightBig
E/bracketleftbig
θt+1−/tildewideθt+1/vextendsingle/vextendsingleFt/bracketrightbig
(ii)
≤6−2αλ1
6−3αλ1/bracketleftBig/parenleftBig
1−αλ1
2/parenrightBig/vextenddouble/vextenddoubleθt−θ∗/vextenddouble/vextenddouble2+13αρ2
max
λ1/vextenddouble/vextenddoublewt−w∗
t/vextenddouble/vextenddouble2
+224αρ2
max(ν+ 1)
Nλ1(1−δ)/parenleftbig
4/vextenddouble/vextenddoubleθ∗/vextenddouble/vextenddouble2+ 2R2
max+ 1/parenrightbig/parenleftBig
1 +ρmax
λ2/parenrightBig2/bracketrightBig
+6
αλ1E/bracketleftBig/vextenddouble/vextenddouble/vextenddoubleα
MM/summationdisplay
m=1/bracketleftbig
(/tildewideA(m)
t−/tildewideAt)θ(m)
t+/tildewide/hatwideb(m)
t−/tildewideb(m)
t+ (/tildewideB(m)
t−/tildewideBt)w(m)
t/bracketrightbig/vextenddouble/vextenddouble/vextenddouble2/vextendsingle/vextendsingle/vextendsingleFt/bracketrightBig
(iii)
≤/parenleftBig
1−αλ1
3/parenrightBig/vextenddouble/vextenddoubleθt−θ∗/vextenddouble/vextenddouble2+18αρ2
max
λ1/vextenddouble/vextenddoublewt−w∗
t/vextenddouble/vextenddouble2+300αρ2
max(ν+ 1)
Nλ1(1−δ)/parenleftbig
4/vextenddouble/vextenddoubleθ∗/vextenddouble/vextenddouble2+ 2R2
max+ 1/parenrightbig/parenleftBig
1 +ρmax
λ2/parenrightBig2
+6α
Mλ 1M/summationdisplay
m=1E/bracketleftbig/vextenddouble/vextenddouble(/tildewideA(m)
t−/tildewideAt)θ(m)
t+/tildewide/hatwideb(m)
t−/tildewideb(m)
t+ (/tildewideB(m)
t−/tildewideBt)w(m)
t/vextenddouble/vextenddouble2/vextendsingle/vextendsingleFt/bracketrightbig
(iv)
≤/parenleftBig
1−αλ1
3/parenrightBig/vextenddouble/vextenddoubleθt−θ∗/vextenddouble/vextenddouble2+18αρ2
max
λ1/vextenddouble/vextenddoublewt−w∗
t/vextenddouble/vextenddouble2+300αρ2
max(ν+ 1)
Nλ1(1−δ)/parenleftbig
4/vextenddouble/vextenddoubleθ∗/vextenddouble/vextenddouble2+ 2R2
max+ 1/parenrightbig/parenleftBig
1 +ρmax
λ2/parenrightBig2
+18ασL/4
2
Mλ 1/bracketleftBig
20 max
m∈M(/bardblθ(m)
0/bardbl+/bardblw(m)
0/bardbl+Rmax)2[1 +β(2ρmax+ 3)]2t+R2
max/bracketrightBig
, (27)
where (i) uses the inequality that /bardbla1+a2/bardbl2≤(1 +σ)/bardbla1/bardbl2+ (1 +σ−1)/bardbla2/bardbl2for anya1,a2∈Rdandσ>0,
(ii) uses eqs. (14), (12) and (23), (iii) uses α≤1/λ1and applies Jensen’s inequality to the convex function
/bardbl·/bardbl2, and (iv) uses the inequality that /bardbla1+a2+a3/bardbl2≤3/summationtext3
k=1/bardblak/bardbl2for anya1,a2,a3∈Rdand then uses
eqs. (52)-(54).
Taking expectation on both sides of eqs. (26) and (27) and summing up the two inequalities yields that
E(/bardblθt+1−θ∗/bardbl2) +E(/bardblwt+1−w∗
t+1/bardbl2)
(i)
≤/parenleftBig
1−βλ2
6+120α2ρ4
max
βλ3
2+18αρ2
max
λ1/parenrightBig
E(/bardblwt−w∗
t/bardbl2)
+/bracketleftBig
1−αλ1
3+534βρ2
max(ν+ 1)
Nλ2(1−δ)/parenleftBig
1 +1
λ2/parenrightBig2
+1920α2ρ4
max
βλ3
2/parenleftBig
1 +ρmax
λ2/parenrightBig2/bracketrightBig
E(/bardblθt−θ∗/bardbl2)
+3840ρ2
max(ν+ 1)
N(1−δ)/bracketleftBigβ
λ2/parenleftBig
1+1
λ2/parenrightBig2
+α2ρ2
max
βλ3
2/parenleftBig
1+ρmax
λ2/parenrightBig2
+α
λ1/bracketrightBig/parenleftbig
/bardblθ∗/bardbl2+R2
max+ 1/parenrightbig
+255σL/4
2
Mλ 2/parenleftBig
β+12α2ρ2
max
βλ2
2+αλ2
λ1/parenrightBig
max
m∈M(/bardblθ(m)
0/bardbl+/bardblw(m)
0/bardbl+Rmax)2[1 +β(2ρmax+ 3)]2t
(i)
≤/parenleftBig
1−αλ1
6/parenrightBig/bracketleftbig
E(/bardblθt−θ∗/bardbl2) +E(/bardblwt−w∗
t/bardbl2)/bracketrightbig
+6000βρ2
max(ν+ 1)
Nλ2(1−δ)/parenleftBig
1+ρmax
λ2/parenrightBig2/parenleftbig
/bardblθ∗/bardbl2+R2
max+ 1/parenrightbig
+574βσL/4
2
Mλ 2max
m∈M(/bardblθ(m)
0/bardbl+/bardblw(m)
0/bardbl+Rmax)2[1 +β(2ρmax+ 3)]2t,
where (i) uses the conditions that N≥6408βρ2
max(ν+1)
αλ1λ2(1−δ)/parenleftbig
1 +1
λ2/parenrightbig2,
α≤min/parenleftbigβλ1λ3
2
23040ρ4max,βλ2
2
53ρ2max,βλ1λ2
432ρ2max,βλ2
2λ1,βλ1
2λ2,βλ2
4ρmax/parenrightbig
. Iterating the inequality above yields the following
convergence rate of the optimization error E/bracketleftbig
/bardblwt+1−w∗
t+1/bardbl2/vextendsingle/vextendsingleFt/bracketrightbig
andE/bracketleftbig
/bardblθt+1−θ∗/bardbl2/vextendsingle/vextendsingleFt/bracketrightbig
E(/vextenddouble/vextenddoubleθT−θ∗/vextenddouble/vextenddouble2+/vextenddouble/vextenddoublewT−w∗/vextenddouble/vextenddouble2)
22Published in Transactions on Machine Learning Research (06/2022)
≤/parenleftBig
1−αλ1
6/parenrightBigT/parenleftbig/vextenddouble/vextenddoubleθ0−θ∗/vextenddouble/vextenddouble2+/bardblw0−w∗
0/bardbl2/parenrightbig
+T−1/summationdisplay
t=0/parenleftBig
1−αλ1
6/parenrightBigT−1−t/bracketleftBig6000βρ2
max(ν+ 1)
Nλ2(1−δ)/parenleftBig
1+ρmax
λ2/parenrightBig2/parenleftbig
/bardblθ∗/bardbl2+R2
max+ 1/parenrightbig
+574βσL/4
2
Mλ 2max
m∈M(/bardblθ(m)
0/bardbl+/bardblw(m)
0/bardbl+Rmax)2[1 +β(2ρmax+ 3)]2t/bracketrightBig
(i)
≤/parenleftBig
1−αλ1
6/parenrightBigT/parenleftbig/vextenddouble/vextenddoubleθ0−θ∗/vextenddouble/vextenddouble2+/bardblw0−w∗
0/bardbl2/parenrightbig
+36000βρ2
max(ν+ 1)
αNλ 1λ2(1−δ)/parenleftBig
1+ρmax
λ2/parenrightBig2/parenleftbig
/bardblθ∗/bardbl2+R2
max+ 1/parenrightbig
+574βσL/4
22T
Mλ 2max
m∈M(/bardblθ(m)
0/bardbl+/bardblw(m)
0/bardbl+Rmax)2
=/parenleftBig
1−αλ1
6/parenrightBigT/parenleftbig/vextenddouble/vextenddoubleθ0−θ∗/vextenddouble/vextenddouble2+/bardblw0−w∗
0/bardbl2/parenrightbig
+O/parenleftBigβ
Nα+βσL/4
22T
M/parenrightBig
, (28)
where (i) uses the conditions that α≤1
λ1andβ≤2
5(2ρmax+3)which respectively imply that 1−αλ1
6≥5
6and
that 1 +β(2ρmax+ 3)≤√
2. This proves eq. (10).
Step 3: Proof of consensus error (11). Note that the local model averaging iterations can be rewritten
into the matrix-vector form as Θt+1=UΘtwhereT≤t≤T+T/primeandΘt/triangle=[θ(1)
t;θ(2)
t;...;θ(M)
t]/latticetop. Hence, it
can be derived from Lemma C.3 that
/bardbl∆ΘT+T/prime/bardblF=/bardbl∆UT/primeΘT/bardblF=/bardblUT/prime∆ΘT/bardblF≤σT/prime
2/bardbl∆ΘT/bardblF. (29)
Hence, we only need to obtain an upper bound of the initial consensus error E/bardbl∆ΘT/bardbl2. Subtracting eq. (14)
from the local update rule yields that for any 0≤t≤T−1,
θ(m)
t+1−θt+1=/summationdisplay
m/prime∈NmUm,m/prime(θ(m/prime)
t−θt) +M−1
Mα/parenleftbig/tildewideA(m)
tθ(m)
t+/tildewide/hatwideb(m)
t+/tildewideB(m)
tw(m)
t/parenrightbig
−α
MM/summationdisplay
m/prime=1,m/prime/negationslash=m/parenleftbig/tildewideA(m/prime)
tθ(m/prime)
t+/tildewideb(m/prime)
t+/tildewideB(m/prime)
tw(m/prime)
t/parenrightbig
.
This can be rewritten into the following matrix-vector form,
∆Θt+1=U∆Θt+ [h1;h2;...;hM]/latticetop,
wherehm/triangle=M−1
Mα/parenleftbig/tildewideA(m)
tθ(m)
t+/tildewide/hatwideb(m)
t+/tildewideB(m)
tw(m)
t/parenrightbig
−α
M/summationtextM
m/prime=1,m/prime/negationslash=m/parenleftbig/tildewideA(m/prime)
tθ(m/prime)
t+/tildewideb(m/prime)
t+/tildewideB(m/prime)
tw(m/prime)
t/parenrightbig
.
The item 2 of Lemma C.3 implies that for any 0≤t≤T−1,
/bardbl∆Θt+1/bardblF≤σ2/bardbl∆Θt/bardblF+/radicaltp/radicalvertex/radicalvertex/radicalbtM/summationdisplay
m=1/bardblhm/bardbl2≤σ2/bardbl∆Θt/bardblF+M/summationdisplay
m=1/bardblhm/bardbl. (30)
Then, using eqs. (55)-(57) yields that
M/summationdisplay
m=1/bardblhm/bardbl≤M−1
M(2α)(2ρmax+ 2)M/summationdisplay
m=1/parenleftbig
/bardblθ(m)
t/bardbl+Rmax+/bardblw(m)
t/bardbl/parenrightbig
(i)
≤4α(ρmax+ 1)M/summationdisplay
m=1/parenleftbig
/bardblθ(m)
t−θt/bardbl+/bardblw(m)
t−wt/bardbl+/bardblθt−θ∗/bardbl+/bardblwt−w∗
t/bardbl+/bardblθ∗/bardbl+/bardblC−1(Aθt+b)/bardbl/parenrightbig
(ii)
≤4α(ρmax+ 1)/parenleftBigM/summationdisplay
m=1/parenleftbig
/bardblθ(m)
t−θt/bardbl+/bardblw(m)
t−wt/bardbl/parenrightbig
23Published in Transactions on Machine Learning Research (06/2022)
+M/parenleftBig
1 +2ρmax
λ2/parenrightBig
/bardblθt−θ∗/bardbl+M/bardblwt−w∗
t/bardbl+Mρmax
λ2(2/bardblθ∗/bardbl+Rmax)/parenrightBig
,
where (i) uses the notations that w∗
t=−C−1(Aθt+b), (ii) uses eqs. (39), (42) and (43). Hence, we obtain
that
E(/bardbl∆Θt+1/bardbl2
F)(i)
≤/parenleftBig
1 +σ−2
2−1
2/parenrightBig
σ2
2E/parenleftbig
/bardbl∆Θt/bardbl2
F/parenrightbig
+/parenleftBig
1 +2
σ−2
2−1/parenrightBig
E/bracketleftBig/parenleftBigM/summationdisplay
m=1/bardblhm/bardbl/parenrightBig2/bracketrightBig
,
(ii)
≤1 +σ2
2
2E/parenleftbig
/bardbl∆Θt/bardbl2
F/parenrightbig
+48α2(1 +σ2
2)
1−σ2
2(ρmax+ 1)2E/bracketleftBig
2MM/summationdisplay
m=1/parenleftbig
/bardblθ(m)
t−θt/bardbl2+/bardblw(m)
t−wt/bardbl2/parenrightbig
+M2/parenleftBig
1 +2ρmax
λ2/parenrightBig2/parenleftbig
/bardblθt−θ∗/bardbl2+/bardblwt−w∗
t/bardbl2/parenrightbig
+4M2ρ2
max
λ2
2(/bardblθ∗/bardbl+Rmax)2/bracketrightBig
, (31)
where (i) uses eq. (30) and the fact that (u+v)2≤(1 +σ)u2+ (1 +σ−1)v2for anyu,v,σ≥0, (ii)
uses (/summationtextn
i=1qi)2≤n/summationtextn
i=1q2
ifor anyqi∈Randn∈N+. Similarly, we obtain from the update rule of
Wt= [w(1)
t;w(2)
t;...;w(M)
t]/latticetop∈RM×dand eq. (15) that
E(/bardbl∆Wt+1/bardbl2
F)≤1 +σ2
2
2E/parenleftbig
/bardbl∆Wt/bardbl2
F/parenrightbig
+48α2(1 +σ2
2)
1−σ2
2(ρmax+ 1)2E/bracketleftBig
2MM/summationdisplay
m=1/parenleftbig
/bardblθ(m)
t−θt/bardbl2+/bardblw(m)
t−wt/bardbl2/parenrightbig
+M2/parenleftBig
1 +2ρmax
λ2/parenrightBig2/parenleftbig
/bardblθt−θ∗/bardbl2+/bardblwt−w∗
t/bardbl2/parenrightbig
+4M2ρ2
max
λ2
2(/bardblθ∗/bardbl+Rmax)2/bracketrightBig
, (32)
Summing up eqs. (31) and (32) yields that
E(/bardbl∆Θt+1/bardbl2
F+/bardbl∆Wt+1/bardbl2
F)
(i)
≤1 +σ2
2
2E/parenleftbig
/bardbl∆Θt/bardbl2
F+/bardbl∆Wt/bardbl2
F/parenrightbig
+96α2(1 +σ2
2)
1−σ2
2(ρmax+ 1)2E/bracketleftBig
2M/parenleftbig
/bardbl∆Θt/bardbl2
F+/bardbl∆Wt/bardbl2
F/parenrightbig
+M2/parenleftBig
1 +2ρmax
λ2/parenrightBig2/parenleftBig/parenleftBig
1−αλ1
6/parenrightBigt/parenleftbig/vextenddouble/vextenddoubleθ0−θ∗/vextenddouble/vextenddouble2+/bardblw0−w∗
0/bardbl2/parenrightbig
+36000βρ2
max(ν+ 1)
αNλ 1λ2(1−δ)/parenleftBig
1+ρmax
λ2/parenrightBig2/parenleftbig
/bardblθ∗/bardbl2+R2
max+ 1/parenrightbig
+574βσL/4
22t
Mλ 2max
m∈M(/bardblθ(m)
0/bardbl+/bardblw(m)
0/bardbl+Rmax)2/parenrightBig
+4M2ρ2
max
λ2
2(/bardblθ∗/bardbl+Rmax)2/bracketrightBig
(ii)
≤2 +σ2
2
3E/parenleftbig
/bardbl∆Θt/bardbl2
F+/bardbl∆Wt/bardbl2
F/parenrightbig
+192M2α2
1−σ2
2(ρmax+1)2/parenleftBig
1+2ρmax
λ2/parenrightBig2/bracketleftBig/parenleftbig/vextenddouble/vextenddoubleθ0−θ∗/vextenddouble/vextenddouble2+/bardblw0−w∗
0/bardbl2/parenrightbig
+/parenleftBig282β
αλ2+8M2ρ2
max
λ2
2/parenrightBig/parenleftbig
/bardblθ∗/bardbl2+R2
max+ 1/parenrightbig
+574βσL/4
22t
Mλ 2max
m∈M(/bardblθ(m)
0/bardbl+/bardblw(m)
0/bardbl+Rmax)2/parenrightBig/bracketrightBig
,(33)
where (i) uses eq. (28), and (ii) uses (/bardblθ∗/bardbl+Rmax)2≤2/bardblθ∗/bardbl2+ 2R2
max,α≤1−σ2
50√
M(ρmax+1)andN≥
128ρ2
max(ν+1)
λ1(1−δ)/parenleftbig
1+ρmax
λ2/parenrightbig2.
Iterating eq. (33) yields the following convergence rate of the initial consensus error E/parenleftbig
/bardbl∆ΘT/bardbl2
F/parenrightbig
.
E/parenleftbig
/bardbl∆ΘT/bardbl2
F/parenrightbig
≤E/parenleftbig
/bardbl∆ΘT/bardbl2
F+/bardbl∆WT/bardbl2
F/parenrightbig
≤/parenleftBig2 +σ2
2
3/parenrightBigT/parenleftbig
/bardbl∆Θ0/bardbl2
F+/bardbl∆W0/bardbl2
F/parenrightbig
+192M2α2
1−σ2
2(ρmax+1)2/parenleftBig
1+2ρmax
λ2/parenrightBig2
+/bracketleftBig3
1−σ2/parenleftBig/vextenddouble/vextenddoubleθ0−θ∗/vextenddouble/vextenddouble2/bardblw0−w∗
0/bardbl2+/parenleftBig282β
αλ2+8M2ρ2
max
λ2
2/parenrightBig/parenleftbig
/bardblθ∗/bardbl2+R2
max+ 1/parenrightbig/parenrightBig
+574βσL/4
22T
Mλ 2max
m∈M(/bardblθ(m)
0/bardbl+/bardblw(m)
0/bardbl+Rmax)2/bracketrightBig
24Published in Transactions on Machine Learning Research (06/2022)
≤/bardbl∆Θ0/bardbl2
F+/bardbl∆W0/bardbl2
F+O/bracketleftBigM2α2
(1−σ2)2/parenleftBigβ
α+M2/parenrightBig
+Mβα2σL/4
22T
1−σ2/bracketrightBig
(i)
≤O/parenleftBig
1 +M4βα
(1−σ2)2+MβασL/4
22T
1−σ2/parenrightBig
, (34)
where (i) uses α≤β
2ρmax+2=O(β). Substituting the above inequality into eq. (29) proves eq. (11).
Hyperparameter choice and complexities. To summarize, the following conditions of the hyperparame-
ters are used in the proof of Theorem 1, including those required by Corollary 2 and lemma C.4.
α≤min/parenleftBigβ
2ρmax+ 2,λ1
40ρ2max/parenleftBig
1+ρmax
λ2/parenrightBig−2
,1
λ1,βλ2
4ρmax,βλ1λ3
2
23040ρ4max,βλ2
2
53ρ2max,βλ1λ2
432ρ2max,
1−σ2
50√
M(ρmax+ 1),βλ2
2λ1,βλ1
2λ2/parenrightBig
= min{O(β),O(M−1/2(1−σ2))} (35)
β≤min/parenleftBig1
λ2,2
5(2ρmax+ 3)/parenrightBig
=O(1) (36)
N≥max/parenleftBig8νρmax
λ2(1−δ),6408βρ2
max(ν+ 1)
αλ1λ2(1−δ)/parenleftBig
1+1
λ2/parenrightBig2
,128ρ2
max(ν+ 1)
λ1(1−δ)/parenleftBig
1+ρmax
λ2/parenrightBig2/parenrightBig
=max{O(1),O(β/α)}(37)
L≥12 lnM+ (8M+ 10) lnρmax
lnσ−1
2=O/parenleftBigM
1−σ2/parenrightBig
(38)
Under the above conditions, we choose the following hyperparameter values.
α=O(M−1/2(1−σ2)),β=O(1)
T=6
αλ1ln/epsilon1−1=O/parenleftBig√
Mln/epsilon1−1
1−σ2/parenrightBig
N=β
α/epsilon1=O/parenleftBig√
M
/epsilon1(1−σ2)/parenrightBig
L=4
lnσ−1
2/parenleftBig
ln/parenleftBigβ
M/epsilon1/parenrightBig
+Tln 2/parenrightBig
+12 lnM+ (8M+ 10) lnρmax
lnσ−1
2=O/parenleftBig√
Mln/epsilon1−1
(1−σ2)2+M
1−σ2/parenrightBig
≤O/parenleftBigMln/epsilon1−1
(1−σ2)2/parenrightBig
T/prime=1
lnσ−1
2ln/parenleftBig
/epsilon1−1O/parenleftBig
1 +M4βα
(1−σ2)2+MβασL/4
22T
1−σ2/parenrightBig/parenrightBig
=1
lnσ−1
2ln/parenleftBig
/epsilon1−1O/parenleftBig
1 +M3.5
1−σ2+αM2/epsilon1
1−σ2/parenrightBig/parenrightBig
=O/parenleftBig1
1−σ2ln/parenleftBigM
/epsilon1(1−σ2)/parenrightBig/parenrightBig
.
Substituting these hyperparameters into eqs. (10) and (11) implies E(/vextenddouble/vextenddoubleθT−θ∗/vextenddouble/vextenddouble2),E(/bardblθ(m)
T+T/prime−θT/bardbl2)≤O(/epsilon1),
soE(/bardblθ(m)
T+T/prime−θ∗/bardbl2)≤2E(/bardblθ(m)
T+T/prime−θT/bardbl2) + 2E(/bardblθT−θ∗/bardbl2)≤O(/epsilon1). Therefore, the overall communication
complexity for synchronizing θ(m)
tisT+T/prime=O/parenleftbig√
Mln/epsilon1−1
1−σ2/parenrightbig
, and the total sample complexity is NT=
O/parenleftbigMln/epsilon1−1
/epsilon1(1−σ2)2/parenrightbig
.
C Supporting Lemmas
In this section, we prove some supporting lemmas that are used throughout the analysis of Algorithm 1.
Lemma C.1. Regarding the terms deﬁned in Appendix A, their norms have the following upper bounds.
/bardblAi/bardblF,/bardblA(m)
i/bardblF,/bardbl/tildewideAt/bardblF,/bardblA/bardblF≤2ρmax, (39)
/bardblBi/bardblF,/bardblB(m)
i/bardblF,/bardbl/tildewideBt/bardblF,/bardblB/bardblF≤ρmax, (40)
/bardblCi/bardblF,/bardbl/tildewideCt/bardblF,/bardblC/bardblF≤1, (41)
25Published in Transactions on Machine Learning Research (06/2022)
/bardblb(m)
i/bardbl,/bardbl/tildewideb(m)
t/bardbl,/bardblbi/bardbl,/bardbl/tildewidebt/bardbl,/bardblb/bardbl≤ρmaxRmax, (42)
/bardblC−1/bardbl=λ−1
2. (43)
Proof.Consider any two vectors u,v∈Rd, we have that/bardbluv/latticetop/bardblF=/radicalbig
tr(vu/latticetopuv/latticetop)=/bardblu/bardbl/bardblv/bardbl. Therefore, by
Assumption 3, we obtain that
/bardblAi/bardblF≤ρi/bardblφ(si)/bardbl/bardblγφ(si+1)−φ(si)/bardbl≤ρmax/bracketleftbig
γ/bardblφ(si+1)/bardbl+/bardblφ(si)/bardbl/bracketrightbig
≤2ρmax,
/bardblBi/bardblF≤γρi/bardblφ(si+1)/bardbl/bardblφ(si)/bardbl≤ρmax,
/bardblCi/bardblF≤/bardblφ(si)/bardbl2≤1,
/bardblb(m)
i/bardbl≤ρiR(m)
i/bardblφ(si)/bardbl≤ρmaxRmax.
The proof for/bardblA(m)
i/bardblF,/bardbl/tildewideb(m)
t/bardbl, etc. is similar.
On the other hand, by Jensen’s inequality, we obtain that
/bardblA/bardblF=/bardblEπb[Ai]/bardblF≤Eπb/bardblAi/bardblF≤2ρmax,/bardbl/tildewideAt/bardblF≤1
N(t+1)N−1/summationdisplay
i=tN/bardblAi/bardblF≤2ρmax.
The proof for the other remaining matrices in eqs. (39)-(42) is similar by using the Jensen’s inequality.
Finally, we prove eq. (43). Note that −C=Eπb/parenleftbig
φ(si)φ(si)/parenrightbig
/follows0withλmin(−C) =−λmax(C) =λ2. Hence,
/bardblC−1/bardbl=λmax(−C−1) =λ−1
min(−C) =λ−1
2.
Lemma C.2. Suppose the MDP trajectory {si,ai}i≥0is generated following a behavioral policy πbwhere
at/triangle={a(m)
t}m. For any deterministic mappings Y:S×A 1×...×AM×S→ Rp×qsuch that/bardblY(s,a,s/prime)/bardblF≤
Cy,∀s,s/prime∈S,a(m)∈Amwherea={a(m)}m, we have
/vextenddouble/vextenddouble/vextenddoubleE/bracketleftBig1
N(t+1)N−1/summationdisplay
i=tNY(si,ai,si+1)/vextendsingle/vextendsingle/vextendsingleFt/bracketrightBig
−Y/vextenddouble/vextenddouble/vextenddouble≤2νCy
N(1−δ),
E/bracketleftBig/vextenddouble/vextenddouble/vextenddouble1
N(t+1)N−1/summationdisplay
i=tNY(si,ai,si+1)−Y/vextenddouble/vextenddouble2
F/vextendsingle/vextendsingle/vextendsingleFt/bracketrightBig
≤8C2
y(ν+ 1)
N(1−δ),
whereY=EY(si,ai,si+1).
Note:A simpliﬁed version of the above lemma has been proposed and proved in Xu et al. (2020a), where ai
andsi+1are omitted in the above inequality. We add aiandsi+1so that this lemma can be better applied
to the quantities Ai,Bi,Ciandb(m)
iwhich rely on sias well asaiandsi+1. The proof logic is very similar
to that of Xu et al. (2020a) and thus omitted here.
Corollary 1. Regarding the terms deﬁned in Appendix A, they have the following upper bounds.
E/bracketleftbig
/bardbl/tildewideCt−C/bardblF/vextendsingle/vextendsingleFt/bracketrightbig
≤2νρmax
N(1−δ)(44)
E/bracketleftbig/vextenddouble/vextenddouble/tildewideBt−B/vextenddouble/vextenddouble2
F/vextendsingle/vextendsingleFt/bracketrightbig
≤8ρ2
max(ν+ 1)
N(1−δ)(45)
E/bracketleftbig/vextenddouble/vextenddouble/tildewidebt−b/vextenddouble/vextenddouble2/vextendsingle/vextendsingleFt/bracketrightbig
≤8ρ2
maxR2
max(ν+ 1)
N(1−δ)(46)
E/bracketleftbig/vextenddouble/vextenddouble/tildewideAt−/tildewideCtC−1A/vextenddouble/vextenddouble2
F/vextendsingle/vextendsingleFt/bracketrightbig
≤32ρ2
max(ν+ 1)
N(1−δ)/parenleftBig
1 +1
λ2/parenrightBig2
(47)
E/bracketleftbig/vextenddouble/vextenddouble/tildewidebt−/tildewideCtC−1b/vextenddouble/vextenddouble2/vextendsingle/vextendsingleFt/bracketrightbig
≤8ρ2
maxR2
max(ν+ 1)
N(1−δ)/parenleftBig
1 +1
λ2/parenrightBig2
(48)
E/bracketleftbig/vextenddouble/vextenddouble/tildewideAt−/tildewideBtC−1A−A/latticetopC−1A/vextenddouble/vextenddouble2
F/vextendsingle/vextendsingleFt/bracketrightbig
≤32ρ2
max(ν+ 1)
N(1−δ)/parenleftBig
1 +ρmax
λ2/parenrightBig2
, (49)
26Published in Transactions on Machine Learning Research (06/2022)
Proof.LetY(s,a,s/prime) =−γρ(s,a)φ(s/prime)φ(s)/latticetopin Lemma C.2. Then it can be checked that Y(st,at,st+1) =
Bt,Cy=ρmax,1
N/summationtext(t+1)N−1
i=tNY(si,ai,si+1) =/tildewideBt,andY=EπbY(si,ai,si+1) =B.
Applying Lemma C.2 to these equations proves eq. (45). The eqs. (44) and (46) can be proved in a similar
way.
LetY(s,a,s/prime) =ρ(s,a)φ(s)[γφ(s/prime)−φ(s)]/latticetop+γρ(s,a)φ(s/prime)φ(s)/latticetopC−1A. Then, it can be checked that
Y(si,ai,si+1) =Ai−BiC−1A,1
N/summationtext(t+1)N−1
i=tNY(si,ai,si+1) =/tildewideAt−/tildewideBtC−1A. Moreover,
/bardblY(s,a,s/prime)/bardblF≤ρmax(γ+ 1) +γρmax/bardblC−1/bardbl/bardblA/bardbl≤2ρmax+ρmax(λ−1
2)(2ρmax) = 2ρmax(1 +ρmax/λ2) :=Cy,
Y=EπbY(si,ai,si+1) =A−BC−1A=A/latticetopC−1A.
Applying Lemma C.2 to these equations proves eq. (49). The equations (47) and (48) can be proved in a
similar way.
Lemma C.3. The doubly stochastic matrix Uand the diﬀerence matrix ∆ =I−1
M11/latticetophave the following
properties:
1.∆U=U∆ =U−1
M11/latticetop
2.For anyx∈RMandn∈N+,/bardblUn∆x/bardbl≤σn
2/bardbl∆x/bardbl(σ2is the second largest singular value of U).
Hence, for any H∈RM×M,/bardblUn∆H/bardblF≤σn
2/bardbl∆H/bardblF
Proof.The ﬁrst item can be proved by the following two equalities.
∆U=/parenleftBig
I−1
M11/latticetop/parenrightBig
U=U−1
M11/latticetopU=U−1
M11/latticetop
U∆ =U/parenleftBig
I−1
d11/latticetop/parenrightBig
=U−1
MU11/latticetop=U−1
M11/latticetop
The proof of the item 2 follows from the claim in page 3 of Qu & Li (2017) that
/bardblUx−1x/bardbl≤σ/bardblx−1x/bardbl, (50)
where we replace their W∈Rn×nand vector ω∈Rninto ourU∈RM×Mand vector H∈RMrespectively,
x=1
M1/latticetopxandσis the largest singular value of U−1
M11/latticetop. We ﬁrst prove that σis also the second largest
singular value of U, i.e.,σ=σ2.
Consider the singular value decomposition of the doubly stochastic matrix U=Q/latticetopD/tildewideQ, where matrices
Q,/tildewideQare unitary and matrix D=diag(1,σ2,σ3,...,σM)is diagonal with 1> σ 2≥σ3≥σM≥0. Note
that 1=U1=Q/latticetopD/tildewideQ1which implies that Q1=D/tildewideQ1. Similarly, 1=U/latticetop1=/tildewideQ/latticetopDQ1⇒/tildewideQ1=DQ1.
Combining the above two results, we conclude that
Q1=D/tildewideQ1=DDU 1=D2U1,
that is, (I−D2)Q1=0. SinceI−D2is a diagonal matrix where the ﬁrst diagonal entry is zero but
the rest diagonal entries are strictly positive, it must hold that all the entries of Q1are zero except for
its ﬁrst entry, i.e., Q1=αe1whereα∈Ris the ﬁrst entry of Q1ande1= (1,0,0,..., 0)is a basis
vector. Hence, we conclude that /tildewideQ1=DQ1=αDe 1=αe1. Taking the norm of both sides yields that
/bardbl/tildewideU1/bardbl=|α|/bardble1/bardbl, i.e.,|α|=/bardbl/tildewideQ1/bardbl=/bardbl1/bardbl=√
M. Therefore, Q−1
n11/latticetop=Q/latticetop/parenleftbig
D−1
M(Q1)(/tildewideQ1)/latticetop/parenrightbig/tildewideQwhere
D−1
M(Q1)(/tildewideQ1)/latticetop=D−α2
Me1e/latticetop
1=D−e1e/latticetop
1=diag(0,σ2,σ3,...,σM), which proves that σ2is the largest
singular value of U−1
n11/latticetop, i.e.,σ=σ2.
Then, substituting σ=σ2,Ux−1x= (U−1
M11/latticetop)x=U∆x(the last step follows from the item 1 of this
Lemma) and x−1x= (I−1
M11/latticetop)x= ∆xinto eq. (50) yields that
/bardblU∆x/bardbl≤σ2/bardbl∆x/bardbl. (51)
27Published in Transactions on Machine Learning Research (06/2022)
A simple induction based on the above equality proves that /bardblUn∆x/bardbl≤σn
2/bardbl∆x/bardblfor anyn∈N+. Therefore,
for any matrix H= [h1,...,hM]∈RM×M, we can prove that
/bardblWn∆H/bardblF=/radicaltp/radicalvertex/radicalvertex/radicalbtM/summationdisplay
m=1/bardblWn∆hm/bardbl2≤/radicaltp/radicalvertex/radicalvertex/radicalbtM/summationdisplay
m=1(σn
2/bardbl∆hm/bardbl)2=σn
2/bardbl∆H/bardblF.
Based on Lemma C.3, we obtain the following inexactness of importance sampling ratio estimation /hatwideρ(m)
t≈ρt.
Corollary 2. Under Assumption 4 and choosing L≥O(lnM+Mlnρmax
lnσ−1
2), the estimation error of the inexact
global importance sampling ratio /hatwideρ(m)
isatisﬁes/summationtextM
m=1/parenleftbig
/hatwideρ(m)
i−ρi/parenrightbig2≤σL/4
2. Therefore, the following inequalities
hold.
M/summationdisplay
m=1/vextenddouble/vextenddoubleA(m)
i−Ai/vextenddouble/vextenddouble2
F,M/summationdisplay
m=1/vextenddouble/vextenddouble/tildewideA(m)
t−/tildewideAt/vextenddouble/vextenddouble2
F≤4σL/4
2 (52)
M/summationdisplay
m=1/vextenddouble/vextenddoubleB(m)
i−Bi/vextenddouble/vextenddouble2
F,M/summationdisplay
m=1/vextenddouble/vextenddouble/tildewideB(m)
t−/tildewideBt/vextenddouble/vextenddouble2
F≤σL/4
2, (53)
M/summationdisplay
m=1/vextenddouble/vextenddouble/hatwideb(m)
i−b(m)
i/vextenddouble/vextenddouble2,M/summationdisplay
m=1/vextenddouble/vextenddouble/tildewide/hatwideb(m)
t−/tildewideb(m)
t/vextenddouble/vextenddouble2≤σL/4
2R2
max. (54)
As a result, the following upper bounds hold.
/bardblA(m)
i/bardblF,/vextenddouble/vextenddouble/vextenddouble/tildewideA(m)
t/vextenddouble/vextenddouble/vextenddouble
F≤2ρmax+ 2 (55)
/bardblB(m)
i/bardblF,/vextenddouble/vextenddouble/vextenddouble/tildewideB(m)
t/vextenddouble/vextenddouble/vextenddouble
F≤ρmax+ 1 (56)
/bardbl/hatwideb(m)
i/bardbl,/bardbl/tildewide/hatwideb(m)
t/bardbl≤Rmax(ρmax+ 1) (57)
Proof.Eq. (5) can be rewritten into the following matrix form.
/bracketleftbig
/tildewideρ(1)
i,L;...;/tildewideρ(M)
i,L/bracketrightbig
=UL/bracketleftbig
/tildewideρ(1)
i,0;...;/tildewideρ(M)
i,0/bracketrightbig
.
Hence, the item 1 of Lemma C.3 yields that
∆/bracketleftbig
/tildewideρ(1)
i,L;...;/tildewideρ(M)
i,L/bracketrightbig
=UL∆/bracketleftbig
/tildewideρ(1)
i,0;...;/tildewideρ(M)
i,0/bracketrightbig
.
Then the item 2 of Lemma C.3 yields that
/vextenddouble/vextenddouble∆/bracketleftbig
/tildewideρ(1)
i,L;...;/tildewideρ(M)
i,L/bracketrightbig/vextenddouble/vextenddouble2≤σ2L
2/vextenddouble/vextenddouble∆/bracketleftbig
/tildewideρ(1)
i,0;...;/tildewideρ(M)
i,0/bracketrightbig/vextenddouble/vextenddouble2. (58)
Denoteρmin:=minm∈Mρ(m)
i. Then Assumption 4 implies that /tildewideρ(m)
i,0=lnρ(m)
i∈[lnρmin,lnρmax]. Then it
can be proved by iterating eq. (5) that /tildewideρ(m)
i,L∈[lnρmin,lnρmax]. Hence,
1
Mlnρi=1
MM/summationdisplay
m=1lnρ(m)
i∈[lnρmin,lnρmax] (59)
Then eqs. (58) and (59) imply that
M/summationdisplay
m=1/parenleftBig
/tildewideρ(m)
i,L−1
Mlnρi/parenrightBig2
≤σ2L
2M/summationdisplay
m=1/parenleftBig
/tildewideρ(m)
i,0−1
Mlnρi/parenrightBig2
≤Mσ2L
2ln2(ρmax/ρmin). (60)
28Published in Transactions on Machine Learning Research (06/2022)
Hence,
/vextendsingle/vextendsingle/vextendsingle/tildewideρ(m)
i,L−1
Mlnρi/vextendsingle/vextendsingle/vextendsingle≤√
MσL
2ln/parenleftBigρmax
ρmin/parenrightBig(i)
≤1
2Mln/parenleftBigρmax
ρmin/parenrightBig
, (61)
where (i) uses the conditions that L≥12 lnM+(8M+10) lnρmax
ln(σ−1
2)andσ2∈[0,1).
Hence, eqs. (59) and (60) imply that
/tildewideρ(m)
i,L≤lnρmax+1
2Mln/parenleftbig
ρmax/ρmin/parenrightbig
. (62)
Therefore, we obtain that
M/summationdisplay
m=1/parenleftbig
/hatwideρ(m)
i−ρi/parenrightbig2(i)=M/summationdisplay
m=1/parenleftbig
eM/tildewideρ(m)
i,L−elnρi)2
(ii)
≤M/summationdisplay
m=1/bracketleftbig
max/parenleftbig
eM/tildewideρ(m)
i,L,elnρi/parenrightbig/bracketrightbig2/parenleftbig
M/tildewideρ(m)
i,L−lnρi/parenrightbig2
(iii)
≤M3σ2L
2ρM
max/radicalbig
ρmax/ρminln2(ρmax/ρmin)
(iv)
≤M3σ2L
2(ρM+2.5
max/ρ2.5
min), (63)
where (i) uses eq. (6), (ii) uses the Lagrange’s Mean Value Theorem, (iii) uses eqs. (59), (60) and (62), (iv)
uses the inequality that lnx<xforx=ρmax/ρmin≥1.
Since at least one of {/tildewideρ(m)
i,0}m∈Mequals lnρmin, we have
lnρi=M/summationdisplay
m=1/tildewideρ(m)
i,0≤lnρmin+ (M−1) lnρmax. (64)
Then, eqs. (61) and (64) imply that
/tildewideρ(m)
i,L≤1
2Mlnρmin+/parenleftBig
1−1
2M/parenrightBig
lnρmax (65)
Hence, we conclude that
M/summationdisplay
m=1/parenleftbig
/hatwideρ(m)
i−ρi/parenrightbig2(i)=M/summationdisplay
m=1/parenleftbig
eM/tildewideρ(m)
i,L−elnρi)2≤M/summationdisplay
m=1max/parenleftbig
e2M/tildewideρ(m)
i,L,e2 lnρi)(ii)
≤Mρminρ2M−1
max (66)
where (i) uses eq. (6), (ii) uses eqs. (64) and (65).
Whenρmin≥σL/2
2, eq. (63) implies that/summationtextM
m=1/parenleftbig
/hatwideρ(m)
i−ρi/parenrightbig2≤M3ρM+2.5
maxσ0.75L
2; Whenρmin< σL/2
2<1,
eq. (66) implies that/summationtextM
m=1/parenleftbig
/hatwideρ(m)
i−ρi/parenrightbig2≤Mρ2M−1
maxσL/2
2. Both imply/summationtextM
m=1/parenleftbig
/hatwideρ(m)
i−ρi/parenrightbig2≤σL/4
2since
L≥12 lnM+(8M+10) lnρmax
ln(σ−1
2).
Then, eq. (52) can be proved as follows.
M/summationdisplay
m=1/vextenddouble/vextenddoubleA(m)
i−Ai/vextenddouble/vextenddouble2
F≤/vextenddouble/vextenddoubleφ(si)[γφ(si+1)−φ(si)]/latticetop/vextenddouble/vextenddouble2
FM/summationdisplay
m=1(/hatwideρ(m)
i−ρi)2≤(1 +γ)2σL/4
2≤4σL/4
2(67)
The above inequality implies that/summationtextM
m=1/vextenddouble/vextenddouble/tildewideA(m)
t−/tildewideAt/vextenddouble/vextenddouble2
F=/summationtextM
m=1/vextenddouble/vextenddouble1
N/summationtext(t+1)N−1
i=tN (A(m)
i−Ai)/vextenddouble/vextenddouble2
F≤4σL/4
2,
where≤applies Jensen’s inequality to the convex function /bardbl·/bardbl2
F. Eqs. (53) and (54) can be proved similarly.
Eq. (52) implies that/vextenddouble/vextenddoubleA(m)
i−Ai/vextenddouble/vextenddouble
F,/vextenddouble/vextenddouble/tildewideA(m)
t−/tildewideAt/vextenddouble/vextenddouble
F≤2. Hence, eq. (55) can be proved using triangle
inequality and eq. (39). Eqs. (56) and (57) can be proved similarly.
29Published in Transactions on Machine Learning Research (06/2022)
The proof of Corollary 2 introduces a new technique, which includes discussion of two cases: ρmin:=
minm∈Mρ(m)
ilies in [σL/2,ρmax]and(0,σL/2]. This is necessary as the local average is applied to ln/hatwideρ(m)
i,
which may be a large negative number that cannot ensure a small consensus error for a ﬁxed number of local
average steps L.
Lemma C.4. Under the update rules of Algorithm 1 and choosing L≥12 lnM+(8M+10) lnρmax
ln(σ−1
2),α≤β
2ρmax+2,
the parameters have the following upper bound.
max
m∈M/bardblθ(m)
T/bardbl+ max
m∈M/bardblw(m)
T/bardbl≤2 max
m∈M(/bardblθ(m)
0/bardbl+/bardblw(m)
0/bardbl+Rmax)[1 +β(2ρmax+ 3)]T.(68)
Proof.SinceL≥12 lnM+(8M+10) lnρmax
ln(σ−1
2), eqs. (55)-(57) hold. Hence, these equations and the update rule
imply that/bardblθ(m)
t+1/bardbl≤/summationtext
m/prime∈NmUm,m/prime/bardblθ(m/prime)
t/bardbl+α(ρmax+ 1)(2/bardblθ(m)
t/bardbl+Rmax+/bardblw(m)
t/bardbl).Taking maximum with
respect tomyields that
max
m∈M/bardblθ(m)
t+1/bardbl≤max
m∈M/summationdisplay
m/prime∈NmUm,m/primemax
m/prime/prime∈M/bardblθ(m/prime/prime)
t/bardbl+α(ρmax+ 1)/parenleftbig
2 max
m∈M/bardblθ(m)
t/bardbl+Rmax+ max
m∈M/bardblw(m)
t/bardbl/parenrightbig
.
≤α(ρmax+ 1)/parenleftbig
2 max
m∈M/bardblθ(m)
t/bardbl+Rmax+ max
m∈M/bardblw(m)
t/bardbl/parenrightbig
+ max
m∈M/bardblw(m)
t/bardbl. (69)
Similarly, it can be obtained that
max
m∈M/bardblw(m)
t+1/bardbl≤2β(ρmax+ 1) max
m∈M/bardblθ(m)
t/bardbl+ (1 +β) max
m∈M/bardblw(m)
t/bardbl+βRmax(ρmax+ 1).(70)
Adding up eqs. (69) and (70) yields that
max
m∈M/bardblθ(m)
t+1/bardbl+ max
m∈M/bardblw(m)
t+1/bardbl
≤2(α+β)(ρmax+ 1) max
m∈M/bardblθ(m)
t/bardbl+ (αρmax+α+β+ 1) max
m∈M/bardblw(m)
t/bardbl+Rmax(α+β)(ρmax+ 1)
(i)
≤β(2ρmax+ 3) max
m∈M/bardblθ(m)
t/bardbl+ (1.5β+ 1) max
m∈M/bardblw(m)
t/bardbl+ 0.5βRmax(2ρmax+ 3)
≤[1 +β(2ρmax+ 3)]/parenleftbig
max
m∈M/bardblθ(m)
t/bardbl+ max
m∈M/bardblw(m)
t/bardbl/parenrightbig
+ 0.5βRmax(2ρmax+ 3),
where (i) uses the condition that α≤β
2ρmax+2and (ii) uses ρmax≥1. By iterating the inequality above and
using maxm∈M/bardblθ(m)
0/bardbl+ maxm∈M/bardblw(m)
0/bardbl≤2 maxm∈M(/bardblθ(m)
0/bardbl+/bardblw(m)
0/bardbl), we prove eq. (68).
30