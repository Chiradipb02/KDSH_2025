Enriching Disentanglement:
From Logical Definitions to Quantitative Metrics
Yivan Zhang
The University of Tokyo, RIKEN AIP
Tokyo, Japan
yivanzhang@ms.k.u-tokyo.ac.jpMasashi Sugiyama
RIKEN AIP, The University of Tokyo
Tokyo, Japan
sugi@k.u-tokyo.ac.jp
Abstract
Disentangling the explanatory factors in complex data is a promising approach
for generalizable and data-efficient representation learning. While a variety of
quantitative metrics for learning and evaluating disentangled representations have
been proposed, it remains unclear what properties these metrics truly quantify.
In this work, we establish algebraic relationships between logical definitions and
quantitative metrics to derive theoretically grounded disentanglement metrics.
Concretely, we introduce a compositional approach for converting a higher-order
predicate into a real-valued quantity by replacing (i) equality with a strict premetric,
(ii) the Heyting algebra of binary truth values with a quantale of continuous values,
and (iii) quantifiers with aggregators. The metrics induced by logical definitions
have strong theoretical guarantees, and some of them are easily differentiable and
can be used as learning objectives directly. Finally, we empirically demonstrate the
effectiveness of the proposed metrics by isolating different aspects of disentangled
representations.
1 Introduction
Insupervised learning , we usually use a real-valued cost function ℓ:Y×Y→R≥0to measure how
close an output f(x)of a function f:X→Yis to a target label y, i.e., ℓ(f(x), y), to quantify the
cost of inaccurate prediction. Then, we can use the total cost over a collection of input-output pairs
to measure the performance of this function. From a functional perspective, this construction induces
a quantitative metric L: [X, Y]×[X, Y]→R≥0between functions:1
L(f, g):=P
x∈Xℓ(f(x), g(x)), (1)
where g:X→Yis a “ ground-truth function ” that maps each input xto its target label y. This metric
can be used as both learning objective andevaluation metric for the learning model f:X→Y.
What does L(f, g)quantify? It quantifies the extent to which two functions fandgareequal :
(f=[X,Y]g):=∀x∈X. f(x) =Yg(x).2(2)
Considering the equality as a predicate, we can observe a parallel between
(binary-valued) equality =Y:Y×Y→ {⊤ ,⊥}and (real-valued) cost ℓ:Y×Y→R≥0,3
universal quantifier (“ for all ”)∀x∈Xand summationP
x∈X, and
function equality =[X,Y]: [X, Y]×[X, Y]→ {⊤ ,⊥}and total cost L: [X, Y]×[X, Y]→R≥0.
We would like to ask: Is it possible to measure and optimize other properties in the same way?
1[X, Y ]denotes the set of all functions from a set Xto a set Y.
2In this paper, the domains of equality predicates are explicitly subscripted.
3{⊤,⊥}denotes the set of binary truth values: true⊤andfalse⊥.
38th Conference on Neural Information Processing Systems (NeurIPS 2024).Inrepresentation learning [Bengio et al., 2013], measuring and optimizing the performance of a
learning model becomes a non-trivial task. The quality of a model cannot always be measured by
how close it is to a fixed ground truth. Instead, we often need to consider the properties of the model
architecture or learned representation itself, such as convexity [Amos et al., 2017], uniformity [Wang
and Isola, 2020], invariance [Kvinge et al., 2022], and equivariance [Lee et al., 2019, Brehmer et al.,
2023]. A proper comprehension of what constitutes good representations and how to assess their
quality is important for designing suitable models, learning objectives, and evaluation metrics.
Disentangled representation learning: definitions, metrics, and methods Disentanglement is
an important property in representation learning, which intuitively means that different explanatory
factors in data should be encoded separately [Bengio et al., 2013]. However, disentanglement has no
universally agreed-upon formal definition [Higgins et al., 2018, Suter et al., 2019, Shu et al., 2020,
Fumero et al., 2021], and it is typically viewed not as a single property but rather as a combination of
several requirements [Ridgeway and Mozer, 2018, Eastwood and Williams, 2018, Do and Tran, 2020,
Tokui and Sato, 2022]. While many metrics for measuring disentanglement have been proposed
[Carbonneau et al., 2022], it remains unclear what properties these metrics truly quantify and how
they can be optimized directly. Often, a new evaluation metric is introduced along with a new learning
method, but it is usually unproven that the method can optimize the new metric [Higgins et al., 2017,
Kim and Mnih, 2018, Chen et al., 2018, Li et al., 2020]. This lack of theoretical understanding makes
it difficult to design learning models that can effectively learn disentangled representations.
A logical and algebraic approach to defining and measuring disentangled representations
Recently, Zhang and Sugiyama [2023] proposed a general and abstract definition of disentanglement,
shedding light on the common structures underlying the algebraic, statistical, and topological
definitions of disentanglement. It was shown that the abstract concept of product [Mac Lane, 1978]
underlies an essential property of disentanglement called modularity [Ridgeway and Mozer, 2018],
and other properties of learning models, such as informativeness [Eastwood and Williams, 2018],
can also be defined abstractly using only the composition and identity of morphisms. Following this
algebraic approach, we aim to derive theoretically grounded quantitative metrics of disentanglement
from the logical definitions of the desired properties, extending the parallel between Eqs. (1) and (2).
Contributions In this paper, we focus on logically defined properties of disentangled representation
learning, such as modularity and informativeness (Section 2). We introduce a compositional approach
to converting a higher-order equational predicate into a real-valued quantity (Table 1), which serves
as a quantitative metric of the extent to which a function satisfies the predicate (Section 3). Our
analysis on the relationship between the logical definitions and the induced quantitative metrics
provides theoretical guarantee on the properties of the optimal functions (Theorem 1). Then, we
demonstrate the usefulness of this conversion method by deriving quantitative metrics for measuring
properties of disentangled representations, and we analyze these metrics in terms of computation,
optimization, and differentiability (Section 4). Lastly, we compare the derived metrics with several
existing ones in a fully controlled experiment and demonstrate that the proposed metrics are able to
isolate different aspects of disentangled representations (Section 5).
2 Logical definitions of disentangled representations
In this section, let us first take a closer look at the logical definitions of two properties of disentangled
representation learning — informativeness [Eastwood and Williams, 2018] and modularity [Ridgeway
and Mozer, 2018], which are arguably more important than other properties [Carbonneau et al., 2022].
We limit our discussion to sets and functions, but the generalization to other morphisms, such as
equivariant, stochastic, or continuous functions, is straightforward.
2.1 Informativeness: injectivity or retractability of a learning model
Being informative, expressive, faithful, or useful is a basic requirement for learned representations
[Bengio et al., 2013]. We want a representation learning model to preserve explanatory factors in
data that are informative to the downstream tasks. For functions, this criterion could be formulated as
follows: If two factors yandy′are different, then their representations m(y)andm(y′)extracted by
a function m:Y→Zshould be different too. This means that the function mshould be injective :
2Definition 1 (Injective function) .A function m:Y→Zisinjective if
pinjective (m:Y→Z):=∀y∈Y.∀y′∈Y.(m(y) =Zm(y′))→(y=Yy′). (3)
Alternatively, because injective functions are precisely functions with retractions (left inverses)
[Lawvere and Rosebrugh, 2003, Chapter 2], we can measure the retractability instead:
Definition 2 (Retractable function) .A function m:Y→Zhas a retraction h:Z→Yif
pretractable (m:Y→Z):=∃h:Z→Y. h◦m=[Y,Y]idY. (4)
Note that these properties are predicates pinjective , pretractable : [Y, Z]→ {⊤ ,⊥}on the set [Y, Z]of all
functions from YtoZ. Analogous to using the total cost in Eq. (1) to measure function equality in
Eq. (2), if we want to measure the injectivity in Eq. (3) or retractability in Eq. (4), we need to find
quantitative counterparts of the implication →,universal quantifier ∀, and existential quantifier
∃used in their logical definitions. Generally, it is desirable to extend the parallel between Eqs. (1)
and (2) to other predicates by finding quantitative operations corresponding to logical connectives and
quantifiers. This correspondence allows us to construct and analyze quantitative metrics for machine
learning models in a compositional manner [Boole, 1854].
2.2 Modularity: product structure preserved by a learning model
Modularity [Ridgeway and Mozer, 2018] is an essential property of disentangled representation
learning, which means that the explanatory factors in data, such as the color and shape of an object,
are separated into independent components in the learned representation [Bengio et al., 2013].
red
circleblue
circle
red
triangleblue
trianglered blue
circle
triangle(1,0) (0,0)
(1,1) (0,1)0 1
0
1
factors observations codes
Ycolor×Yshape X Zcolor×Zshapeg
m:=f◦g=mcolor×mshapef
Figure 1: Disentangled representation learningAs shown in Fig. 1, modularity can be
defined as follows. We assume that
data with multiple explanatory factors
(e.g., color and shape) is generated via
a function g:Y→Xfrom aproduct
Y:=Y1×Y2offactors . An encoder
f:X→Zis a function toaproduct
Z:=Z1×Z2ofcodes . Then, an
encoder is said to be modular if it can
reconstruct the product structure ,
such that the composition m:=f◦g:
Y→Zof the generator gand the
encoder fis aproduct function.4
Formally, being a product function is also a property that can be represented as a predicate:
Definition 3 (Product function) .LetY:=Y1×Y2andZ:=Z1×Z2be products of sets. A function
m:Y→Zis aproduct function if
pproduct (m:Y→Z):=∃m1,1:Y1→Z1.∃m2,2:Y2→Z2. m=[Y,Z]m1,1×m2,2.(5)
Example 1. Let us compare the following two functions from Y:={0,1}2toZ:=R2:
m:=

(0,0)7→(1,2)
(0,1)7→(3,4)
(1,0)7→(5,6)
(1,1)7→(7,8)(6) m′:=

(0,0)7→(a, c)
(0,1)7→(a, d)
(1,0)7→(b, c)
(1,1)7→(b, d)=07→a
17→b
|{z}
m1,1×07→c
17→d
|{z}
m2,2(7)
where a,b,c, anddare arbitrary real numbers. According to Definition 3, only m′=m1,1×m2,2is
a product function, whose first/second output depends only on the first/second input.
In Example 1, although mis not a product function, we want to address the following questions:
(Metric) Can we quantify the extent to which it resembles a product function?
(Approximation) Can we find a product function that is closest to it?
(Differentiability) Can we make it slightly closer to a product function?
Answers to these questions will be given in the following sections.
4For two functions f:A→Candg:B→D, their product f×g:A×B→C×Dapplies these two
functions “ in parallel ” by mapping a pair (a, b)inA×Bto a pair (f(a), g(b))inC×D.
33 Enrichment: from logic to metric
In Appendices A and B, we describe in detail the theory of converting a higher-order predicate into
a real-valued quantity. In this section, we only introduce the conversion procedure using concrete
examples and present the theoretical results. A summary of the conversion is given in Table 1.
Table 1: From logic to metric
Logic Metric
truth values {⊤,⊥}real values [0,∞]
equality = strict premetric d
conjunction ∧ addition +
disjunction ∨ minimum min
implication → subtraction∗´
universal quantifier ∀ aggregator∗∗▽
existential quantifier ∃ infimum inf
∗truncated subtraction: b´a:= max {b−a,0}
∗∗e.g., maximum, sum, mean, and mean squareFirst of all, let us clarify the terms predicate
and quantity. In the realm of classical logic,
apredicate p:A→ {⊤ ,⊥}on a set Ais a
function from the set Ato the set {⊤,⊥}of
binary truth values. For example, the predicates
pinjective ,pretractable , and pproduct in Definitions 1
to 3 are functions from the set [Y, Z]of functions
to the set {⊤,⊥}. They are logical definitions
of some properties of functions. On the other
hand, in this work, a quantity q:A→[0,∞]
on a set Ais defined as a function to the set
[0,∞]of extended non-negative real numbers.
The quantities associated with a predicate will
serve as quantitative metrics for the property
defined by the predicate.
3.1 From equality predicate to strict premetric
In this work, a predicate of central importance is the equality predicate =A:A×A→ {⊤ ,⊥}
[Mazur, 2008]. A quantity associated with the equality predicate should be a strict premetric:
Definition 4 (Strict premetric) .Astrict premetric on a set Ais a function dA:A×A→[0,∞]that
∀a∈A.∀a′∈A.(dA(a, a′) = 0) ↔(a=Aa′). (8)
3.2 From logical operation to quantitative operation
Next, let us have a look at the logical connectives and quantifiers used in the definitions of properties.
The product of sets and functions plays a significant role in this work. Two functions f, g:C→A×B
to a product are equal if and only if all their component functions are equal:
(f=[C,A×B]g):= (f1=[C,A]g1)∧(f2=[C,B]g2).5(9)
Note that the conjunction ∧:{⊤,⊥} × {⊤ ,⊥} → {⊤ ,⊥}, a logical connective, is used in Eq. (9).
To obtain a corresponding quantity, we replace it with the addition + : [0 ,∞]×[0,∞]→[0,∞]:
d[C,A×B](f, g):=d[C,A](f1, g1) +d[C,B](f2, g2). (10)
Theuniversal quantifier on a set Ais a specific (second-order) predicate ∀A:{⊤,⊥}A→ {⊤ ,⊥}
on the set {⊤,⊥}Aof predicates. We can replace it with the supremum sup : [0 ,∞]A→[0,∞]. We
can also choose a function from the (i) maximum , (ii) sum, (iii) mean , and (iv) mean square when the
setAis finite. More generally, we can replace it with a quantity ▽A: [0,∞]A→[0,∞]on the set
[0,∞]Aof quantities that satisfies some conditions, which we refer to as a (universal) aggregator .
Intuitively, a universal aggregator should output 0if and only if all inputs are 0. Therefore, the median,
mode, and range are non-examples. Different choices of aggregators yield metrics with different
characteristics in computation and optimization. For example, the function equality predicate
(f=[A,B]g):=∀a∈A. f(a) =Bg(a) (11)
converts to a quantity whose aggregator ▽is not limited to the sum (cf. Eqs. (1) and (2)):
d[A,B](f, g):=▽
a∈AdB(f(a), g(a)). (12)
Dually, we also need to consider the disjunction ∨and the existential quantifier ∃. We replace
them with the minimum and the infimum , respectively. Lastly, we replace the implication a→bwith
the (truncated) subtraction b´a:= max {b−a,0}. These operations are illustrated in Fig. 2.
5For a function f:C→A×Bto a product A×B, itscomponent functions f1:C→A:=p1◦f
andf2:C→A:=p2◦fare denoted by numeric subscripts, where p1:A×B→A:= (a, b)7→aand
p2:A×B→B:= (a, b)7→bareprojection functions .
4(a) predicate and quantity
 (b) conjunction
 (c) disjunction
 (d) implication
Figure 2: From predicates and logical operations to quantities and quantitative operations
3.3 From compound predicate to compound quantity
Following Table 1, we can convert any compound predicate defined using equational predicates
and logical operations into a corresponding compound quantity defined using strict premetrics and
quantitative operations. Our main result on their relationship is as follows:
Theorem 1. Letp:A→ {⊤ ,⊥}be a predicate on a set A, and let q:A→[0,∞]be a quantity
converted from paccording to Table 1. Then, for any a∈A,q(a) = 0 implies p(a) =⊤. Conversely,
for any a∈A,p(a) =⊤implies q(a) = 0 if and only if pdoes not contain the implication.
The implication is special because we must sacrifice logical equivalence for the sake of continuity,
which is necessary for gradient-based optimization. We will explore this through a concrete example
regarding injectivity in Section 4.3 and discuss it in detail in Appendices B and D.
3.4 (Sub)homomorphism from metric to logic
Finally, for readers interested in the theoretical background, we briefly introduce the following
algebraic concepts and a proof sketch underlying Table 1 and Theorem 1.
Definition 5 (Zero predicate) .The zero predicate ζ: [0,∞]→ {⊤ ,⊥}:=x7→(x= 0) is a
function that maps 0to true ⊤and any positive value to false ⊥.
Definition 6 ((Sub)homomorphism from a quantity to a predicate) .LetAbe a set. A quantity
q:A→[0,∞]on the set Aishomomorphic to a predicate p:A→ {⊤ ,⊥}via the zero predicate
ζ: [0,∞]→ {⊤ ,⊥}ifζ◦q=p, and is subhomomorphic topifζ◦q→p.6
Definition 7 ((Sub)homomorphism from a quantitative operation to a logical operation) .Letn∈Nbe
a natural number. An n-ary quantitative operation α: [0,∞]n→[0,∞]ishomomorphic to a logical
operation β:{⊤,⊥}n→ {⊤ ,⊥}via the zero predicate ζ: [0,∞]→ {⊤ ,⊥}ifζ◦α=β◦ζn, and
issubhomomorphic toβifζ◦α→β◦ζn.7
Definition 8 ((Sub)homomorphism from an aggregator to a quantifier) .LetAbe a set. An aggregator
αA: [0,∞]A→[0,∞]on the set Aishomomorphic to a quantifier βA:{⊤,⊥}A→ {⊤ ,⊥}via
the zero predicate ζ: [0,∞]→ {⊤ ,⊥}ifζ◦αA=βA◦ζA, and is subhomomorphic toβAif
ζ◦αA→βA◦ζA.8
Homomorphic quantities, quantitative operations, and aggregators can be illustrated as follows:
A A
[0,∞] {⊤,⊥}qidA
p
ζ[0,∞]n{⊤,⊥}n
[0,∞] {⊤,⊥}αζn
β
ζ[0,∞]A{⊤,⊥}A
[0,∞] {⊤,⊥}αAζA
βA
ζ(13)
6We use the infix notation, so ζ◦q=pmeans that ∀a∈A.(q(a) = 0) ↔p(a), andζ◦q→pmeans that
∀a∈A.(q(a) = 0) →p(a).
7ζn: [0,∞]n→ {⊤ ,⊥}n:= (q1, . . . , q n)7→(q1= 0, . . . , q n= 0) is the n-fold product of the zero
predicate ζ: [0,∞]→ {⊤ ,⊥}.
8ζA: [0,∞]A→ {⊤ ,⊥}A:=ζ◦(−)is the postcomposition with the zero predicate ζ: [0,∞]→ {⊤ ,⊥}
that maps a quantity q:A→[0,∞]to the predicate ζ◦q:A→ {⊤ ,⊥}.
5Based on these algebraic concepts, we can say that strict premetrics are homomorphic to equality
predicates, addition is homomorphic to conjunction (since the sum is zero if and only if both addends
are zero), minimum is homomorphic to disjunction, truncated subtraction is subhomomorphic to
implication, and universal aggregators are homomorphic to the universal quantifier.
Theorem 1 means that any compound quantity is (sub)homomorphic to the corresponding compound
predicate if each component (quantities, quantitative operations, aggregators) is (sub)homomorphic
to the corresponding component (predicates, logical operations, quantifiers). For implication, we use
the truncated subtraction, which is only subhomomorphic, since there is no continuous operation that
is homomorphic to implication (see also Appendix D).
More abstractly and concisely, we can say that we replace the Heyting algebra of truth values {⊤,⊥}
with a quantale of extended non-negative real numbers [0,∞], and we replace the quantifiers ∀and∃
with aggregators ▽andinf(see also Appendix B). In this way, we can derive quantitative metrics for
any logically defined properties of learning models compositionally .
4 Quantitative metrics of disentangled representations
In this section, we demonstrate how to apply the conversion method introduced above to derive
quantitative metrics for measuring the modularity (Definition 3) and informativeness (Definitions 1
and 2) of disentangled representations.
In Sections 4.1 and 4.2, we introduce modularity metrics based on two approaches and discuss their
differences in terms of computation and optimization. We point out that the main obstacle lies in
the optimization step, resulting from the existential quantifiers in the definition. Then, we show that
we can derive easily computable and differentiable metrics from a logically equivalent definition. In
Section 4.3, we introduce informativeness metrics and present a result of Theorem 1.
4.1 Modularity metrics via product approximation
We begin with modularity , which is an essential property of disentangled representation learning.
Recall that modularity can be defined using the product function (Definition 3). For easier reference,
we provide the following diagram, which shows the domains and codomains of the functions involved
in the upcoming discussion:
Y1
y1Y1×Y2
(y1, y2)Y2
y2
Z1
m1(y1, y2)
=m1,1(y1)Z1×Z2
m(y1, y2)Z2
m2(y1, y2)
=m2,2(y2)m1,1 mp1 p2
m1 m2m2,2
p1 p2(14)
From Definition 3, we can derive the following metric:
Definition 9 (Product approximation) .Letm:Y→Zbe a function from a product Y:=Y1×Y2
of sets to another product Z:=Z1×Z2of sets. The extent to which mresembles a product function
can be measured by a distance between mand its best product function approximation:
qproduct (m:Y→Z):= inf
m1,1∈[Y1,Z1]inf
m2,2∈[Y2,Z2]d[Y,Z](m, m 1,1×m2,2). (15)
The derivation of qproduct from pproduct follows the conversion described in Table 1: replacing the
equality =[Y,Z]: [Y, Z]×[Y, Z]→ {⊤ ,⊥}with a strict premetric d[Y,Z]: [Y, Z]×[Y, Z]→[0,∞]
and the existential quantifiers ∃with the infimum operators inf.
This modularity metric can be interpreted as a distance from a point m∈[Y, Z]to a subset
{m1,1×m2,2|m1,1∈[Y1, Z1], m2,2∈[Y2, Z2]} ⊂[Y, Z]of product functions (cf. the Hausdorff
distance [Lawvere, 1986, Tuzhilin, 2016]). Following from Theorem 1, qproduct (m) = 0 if and only if
pproduct (m) =⊤. This means that the minimizers of this metric are precisely product functions.
6However, we still face two obstacles: the product operation and the minimization problem. For the
product operation, we can employ Eqs. (10) and (12) to rewrite qproduct into a more computable form:
Proposition 2. The quantity qproduct (m:Y→Z)equals
▽
y1∈Y1▽
y2∈Y2dZ1(m1(y1, y2), m∗
1,1(y1)) +▽
y2∈Y2▽
y1∈Y1dZ2(m2(y1, y2), m∗
2,2(y2)), (16)
where the functions m∗
1,1:Y1→Z1andm∗
2,2:Y2→Z2are given by
m∗
1,1:Y1→Z1:=y17→arg inf
z1∈Z1▽
y2∈Y2dZ1(m1(y1, y2), z1), (17)
m∗
2,2:Y2→Z2:=y27→arg inf
z2∈Z2▽
y1∈Y1dZ2(m2(y1, y2), z2). (18)
A detailed derivation can be found in Appendix C. Note that we can obtain the optimal product
function approximation m∗
1,1×m∗
2,2explicitly via Eqs. (17) and (18). Intuitively, we need to find an
approximation of a (multi)set of codes with one factor fixed and other factors varying, and then we
use the aggregation of all the approximation errors as a modularity metric.
The second obstacle — the minimization problem — still needs to be addressed. Since the code spaces
Z1andZ2can be infinite sets, the minimization problem may not have a closed-form minimizer or
even an exact solver. Even if an exact solver exists, the solution may not be differentiable with respect
to the inputs. Let us examine some concrete examples of qproduct by choosing different aggregators ▽
in Eqs. (17) and (18). In the following three examples, we assume that the code spaces Z1andZ2are
Euclidean spaces equipped with the usual Euclidean distances.
Example 2. If the aggregator ▽is the supremum , the best approximation is the center of the smallest
bounding sphere [Megiddo, 1983], and the approximation error is the radius .
This metric has the advantage of being definable even when the factor spaces Y1andY2are infinite
sets, and it can be computed using either randomized [Welzl, 1991] or exact [Fischer et al., 2003]
algorithms. However, it is not easy to calculate its gradient. Thus, we cannot use it as a learning
objective and directly optimize it using gradient-based optimization.
Example 3. If the aggregator ▽is the mean , the best approximation is the (geometric) median
[Weiszfeld, 1937], and the approximation error is the mean absolute deviation around the median .
It is known that there is no exact algorithm for obtaining the geometric median [Cockayne and Melzak,
1969], but it can be effectively approximated using convex optimization [Cohen et al., 2016]. The
geometric median has found applications in robust estimation in the fields of statistics and machine
learning [Meer et al., 1991, Minsker, 2015, Pillutla et al., 2022, Guerraoui et al., 2023].
Example 4. If the aggregator ▽is the mean square , the best approximation is the mean , and the
approximation error is the variance . In this case, qproduct (m)can be simplified to
mean
y1∈Y1var
y2∈Y2m1(y1, y2) + mean
y2∈Y2var
y1∈Y1m2(y1, y2). (19)
The variance is easier to compute and differentiate than the radius of the smallest bounding sphere
and the mean absolute deviation around the median, but it is also more susceptible to outliers and
noise. Further work could explore the theoretical implications of these metrics, especially in cases
where only partial combinations of factors or noisy annotations are available.
Then, let us revisit our motivating example in Example 1:
Example 5. Let us consider the function m:{0,1}2→R2in Eq. (6). Its best product function
approximation is
m∗:{0,1}2→R2:=

(0,0)7→(2,4)
(0,1)7→(2,6)
(1,0)7→(6,4)
(1,1)7→(6,6)=07→2
17→6
|{z}
m∗
1,1×07→4
17→6
|{z}
m∗
2,2(20)
because m∗
1,1(0) = 2 is the center/median/mean of the set {m1(0,0) = 1 , m1(0,1) = 3 }, and so on.
The modularity metric is a distance between mandm∗.
74.2 Modularity metrics via constancy
Upon analyzing the metrics above, it becomes evident that what we need is not the best approximation
itself (e.g., the mean) but rather the approximation error (e.g., the variance) — a measure of the
constancy of a set of codes. Following this insight, our next objective is to formulate a modularity
metric that eliminates the need for an optimization step. Zhang and Sugiyama [2023] have proved
that a function is a product function if and only if the curried functions9of its component functions
are constant, as shown in the following example:
Example 6. Consider the functions m, m′:{0,1}2→R2in Eqs. (6) and (7) and the curried
functions of their second component functions m2, m′
2:{0,1}2→R:
m2=

(0,0)7→2
(0,1)7→4
(1,0)7→6
(1,1)7→8∼=

07→07→2
17→4
17→07→6
17→8(21) m′
2=

(0,0)7→c
(0,1)7→d
(1,0)7→c
(1,1)7→d∼=

07→07→c
17→d
17→07→c
17→d(22)
The curried function cm2:{0,1} → [{0,1},R]is not constant, whilecm′
2is constant with value
{07→c,17→d} ∈[{0,1},R](and so iscm′
1), indicating that m′is a product function.
Based on this fact, we propose an alternative approach for measuring modularity:
Definition 10 (Constancy of curried function) .Letm1andm2be the component functions of a
function m:Y→Zfrom a product Y:=Y1×Y2of sets to another product Z:=Z1×Z2of sets.
The extent to which mresembles a product function can be measured by the constancy of the curried
functions of m1andm2:
qconst-curry (m:Y→Z):=qconst(cm1) +qconst(cm2), (23)
where qconstis a quantity for constant functions.
To complete this construction, we adopt the following definition and metric of the constant function:
Definition 11 (Constant function) .A function f:A→Bisconstant if
pconst(f:A→B):=∀a∈A.∀a′∈A. f(a) =Bf(a′), (24)
which can be measured by
qconst(f:A→B):=▽
a∈A▽
a′∈AdB(f(a), f(a′)). (25)
This constancy metric qconstonly needs to compute pairwise distances between the outputs, requiring
|A|2times distance computation but no optimization. Incorporating qconstintoqconst-curry , we can get
the following metric:
Proposition 3. The quantity qconst-curry (m:Y→Z)equals
▽
y1∈Y1▽
y2∈Y2▽
y′
2∈Y2dZ1(m1(y1, y2), m1(y1, y′
2))
+▽
y2∈Y2▽
y1∈Y1▽
y′
1∈Y1dZ2(m2(y1, y2), m2(y′
1, y2)).(26)
Here are two examples of qconstandqconst-curry using different aggregators ▽in Eqs. (25) and (26).
Example 7. If the aggregator ▽is the maximum ,qconstis the diameter (the maximum pairwise
distance) of the outputs. In this case, qconst-curry (m)can be simplified to
max
y1∈Y1diam
y2∈Y2m1(y1, y2) + max
y2∈Y2diam
y1∈Y1m2(y1, y2). (27)
Example 8. If the aggregator ▽is the mean square ,qconstis the mean pairwise squared distance,
which equals the variance . In this case, qconst-curry (m)coincides with Eq. (19).
In summary, Eqs. (19) and (27) are easily computable and differentiable metrics, and their minimizers
are precisely product functions. They do not contain any hyperparameters or stochastic components
and thus can serve as both learning objectives and evaluation metrics.
9For a binary function f:A×B→C, itscurried function bf:A→[B, C]is a unary function such that
for all a∈Aandb∈B,f(a, b) =bf(a)(b)[Curry, 1980].
84.3 Informativeness metrics
If an encoder f:X→Zis constant, mapping everything to the same value, according to Definition 3,
it is perfectly modular. However, a constant encoder is also completely useless. In this subsection,
we shift our focus to the property of informativeness — a measurement of usefulness.
Informativeness is not a unique requirement for disentangled representations. Other representation
learning paradigms, such as contrastive learning [Jaiswal et al., 2020, Wang and Isola, 2020] and
metric learning [Musgrave et al., 2020], also emphasize the importance of mapping dissimilar data
to far-apart locations in the representation space. While one could integrate this requirement into
a single disentanglement score (e.g., [Higgins et al., 2017, Kim and Mnih, 2018]), we argue that it
is better to evaluate the usefulness of representations separately for a more fine-grained assessment
[Carbonneau et al., 2022].
One straightforward way to measure informativeness is to measure how much we can invert the
encoding process:
Definition 12 (Retraction approximation) .Letm:Y→Zbe a function. The extent to which m
isretractable can be measured by a distance between the composition of mand its best retraction
approximation and the identity function:
qretractable (m:Y→Z):= inf
h∈[Z,Y]d[Y,Y](h◦m,idY) = inf
h∈[Z,Y]▽
y∈YdY(h(m(y)), y). (28)
This metric qretractable is derived from Definition 2 following the conversion procedure in Table 1.
This informativeness metric also involves an optimization step similar to the modularity metric
qproduct , potentially introducing randomness or higher computation costs. Note that we may use a
parameterized subset of the set [Z, Y]of all functions from codes Zto factors Y, such as the set of
linear functions. Then, the problem becomes a regression/classification problem, and the metric is
the performance of the predictor. A number of existing works adopted this approach and used the
accuracy, the area under the ROC curve (AUC-ROC), or the mean squared error (MSE) to measure
the informativeness [Ridgeway and Mozer, 2018, Eastwood and Williams, 2018, Eastwood et al.,
2023]. However, such metrics necessitate additional hyperparameter tuning and are more likely to
exhibit varying behavior across different implementations [Carbonneau et al., 2022].
It raises the question of whether we can measure the informativeness of an encoder without
approximating its retraction. We propose to measure informativeness by directly measuring the
injectivity of the encoding process:
Definition 13 (Contraction) .Letm:Y→Zbe a function. The extent to which misinjective can
be measured by how much mcontracts pairs of inputs:
qinjective (m:Y→Z):=▽
y∈Y▽
y′∈YdY(y, y′)´dZ(m(y), m(y′)). (29)
This metric qinjective is derived from Definition 1 following the conversion procedure in Table 1.
According to Theorem 1, we know that qretractable (m) = 0 if and only if mis retractable. However,
qinjective (m) = 0 implies the injectivity of mbut not the other way around:
(pretractable (m) =⊤) (pinjective (m) =⊤)
(qretractable (m) = 0) (qinjective (m) = 0)logically
equivalent
×Theorem 1 Theorem 1 (30)
In other words, a minimizer of qinjective is required to be non-contractive , which is a stronger condition
than being injective. For example, let us consider the function m: [0,1]→R:=y7→0.01×y.
Although it is injective, its outputs are less distinguishable from each other in terms of the Euclidean
distance. Therefore, qinjective still assign a non-zero value to this function.
Although not all injective functions necessarily minimize qinjective , according to Theorem 1, we can
guarantee that minimizing qinjective will not lead to non-injective functions. Moreover, qinjective does
not require training regressors or classifiers to approximate the retraction. Consequently, it does not
need any time-consuming hyperparameter tuning or cross-validation like existing informativeness
metrics [Eastwood and Williams, 2018, Ridgeway and Mozer, 2018].
9Table 2: Supervised disentanglement metrics
Modularity Informativeness Existing metrics
Product approx. Constancy Retraction approx. Contraction Pair Info. Regressor
Rad. MAD Var. Diam. MPD ME MAE MSE Max Mean BetaaFactorbMIGcDis.dCom.dInfo.d
entanglement ✗0.44 0 .75 0 .96 0 .19 0 .82✓0.76 0.96 0.99 0 .44 0 .78 0 .89 0 .83 0 .18 0 .28 0 .28 1 .00
rotation ✗0.22 0 .51 0 .80 0 .05 0 .64✓1.00 1.00 1.00 1.00 1.00 0 .96 0 .34 0 .17 0 .40 0 .40 1 .00
duplicate ✗0.24 0 .43 0 .67 0 .06 0 .56✓1.00 1.00 1.00 1.00 1.00 1 .00 1 .00 1 .00 1 .00 0 .59 1 .00
complement ✗0.12 0 .28 0 .55 0 .01 0 .42✓1.00 1.00 1.00 1.00 1.00 1 .00 0 .00 1 .00 1 .00 0 .63 1 .00
misalignment ✗0.22 0 .44 0 .74 0 .05 0 .58✓1.00 1.00 1.00 1.00 1.00 1 .00 0 .00 1 .00 1 .00 1 .00 1 .00
redundancy ✓ 1.00 1.00 1.00 1.00 1.00✓1.00 1.00 1.00 1.00 1.00 1 .00 0 .33 1 .00 1 .00 0 .93 1 .00
contraction ✓ 1.00 1.00 1.00 1.00 1.00✓1.00 1.00 1.00 0 .18 0 .49 1 .00 1 .00 1 .00 1 .00 1 .00 1 .00
nonlinear ✓ 1.00 1.00 1.00 1.00 1.00✓0.79 0.93 0.99 0 .65 0 .95 1 .00 1 .00 0 .88 1 .00 1 .00 1 .00
constant ✓ 1.00 1.00 1.00 1.00 1.00✗0.42 0 .76 0 .90 0 .18 0 .48 0 .33 0 .33 0 .00 0 .00 0 .00 0 .00
random ✗0.22 0 .48 0 .78 0 .05 0 .61✗0.42 0 .76 0 .90 0 .22 0 .83 0 .34 0 .33 0 .00 0 .00 0 .00 0 .04
a[Higgins et al., 2017]b[Kim and Mnih, 2018]c[Chen et al., 2018]d[Eastwood and Williams, 2018]
5 Experiments
In this section, we empirically demonstrate the effectiveness of the proposed metrics. Following
Carbonneau et al. [2022], we did not learn representations on datasets but directly defined functions
m:Y→Zfrom factors to codes, which allows us to capture typical failure patterns.
We evaluated modularity metrics based on (i) the radius of the smallest bounding sphere, (ii) mean
absolute deviation (MAD) around the median, (iii) variance, (iv) diameter, and (v) mean pairwise
distance (MPD) introduced in Sections 4.1 and 4.2. We evaluated informativeness metrics based on
retraction approximation using the maximum error (ME), mean absolute error (MAE), and mean
squared error (MSE), and we calculated the contraction discussed in Section 4.3. To compare with
existing metrics [Higgins et al., 2017, Kim and Mnih, 2018, Chen et al., 2018, Eastwood and Williams,
2018], we transformed the results isomorphically using e−x: [0,∞]→[0,1], meaning that 1is the
perfect score. The results are shown in Table 2, and our observations are as follows.
If a representation is given a perfect score by a proposed metric, it must satisfy the property
that the metric quantifies , which confirms our theoretical result. In Table 2, the light cells show
that the proposed metrics can assign a perfect score when the function truly satisfies the properties,
which is indicated by ✓and✗. The dark cells are supposed to be perfect scores, but they fall short
due to the limited expressiveness of the linear models used for the approximation. Meanwhile, some
existing metrics that only provide a single score may entangle modularity and informativeness.
The metrics derived from equivalent definitions may differ in terms of computation cost and
differentiability. Concretely, the radius, MAD, ME, MAE, and MSE are not differentiable due to
the inner optimization problem, while the variance, diameter, MPD, and max/mean contraction are
differentiable. In terms of computation, they are much faster than metrics requiring hyperparameter
tuning, such as DCI [Eastwood and Williams, 2018]. Further comparisons are provided in Appendix E.
Different metrics may rank imperfect representations differently, even though they have exactly
the same minimizers. This difference can lead to differences in risk preferences, sensitivity to
outliers, and learning dynamics when these metrics are used as learning objectives. Illustrations and
further discussion can be found in Appendix D.
Further, the proposed metrics can be used in weakly supervised orfine-grained evaluation. See
Appendix E for detailed data configuration and further experimental results.
6 Conclusion
In this work, we developed a systematic and rigorous method for converting logical definitions of
properties of representation learning models into quantitative metrics (Table 1). We applied this
method to assess two important and distinct properties of disentangled representations: modularity
and informativeness. We derived two families of metrics for each property based on their logically
equivalent definitions. We theoretically analyzed the minimizers of these metrics (Theorem 1) and
compared their differences in terms of computation cost and differentiability. Future research could
compare metrics derived from different aggregators and design appropriate models to optimize these
metrics with minimal supervision.
10Acknowledgments
We thank Paolo Perrone for generously sharing insights on Markov categories enriched in divergence
spaces. We thank Ken Sakayori for reviewing an earlier version of Appendices A and B and providing
constructive suggestions. We thank Zhiyuan Zhan for insightful discussions on metrics, topology,
measures, and optimization of function properties, and for reviewing and proofreading some parts of
Appendix D. We thank Masahiro Negishi for valuable discussions on disentanglement metrics and
weakly supervised disentanglement. We thank Jingwen Fu for checking the algebraic concepts in
Section 3. We also thank Johannes Ackermann, Xin-Qiang Cai, and Tongtong Fang for their valuable
feedback on the manuscript.
YZ was supported by JSPS KAKENHI Grant Number 22KJ0880. MS was supported by JST CREST
Grant Number JPMJCR18A2 and a grant from Apple, Inc. Any views, opinions, findings, and
conclusions or recommendations expressed in this material are those of the authors and should not be
interpreted as reflecting the views, policies or position, either expressed or implied, of Apple Inc.
References
Jiˇrí Adámek, Horst Herrlich, and George Strecker. Abstract and Concrete Categories: The Joy of
Cats. John Wiley and Sons, 1990. URL http://www.tac.mta.ca/tac/reprints/ar
ticles/17/tr17abs.html . A.1
K Amer. Equationally complete classes of commutative monoids with monus. Algebra Universalis ,
18:129–131, 1984. URL https://doi.org/10.1007/BF01182254 . 18
Brandon Amos, Lei Xu, and J Zico Kolter. Input convex neural networks. In International Conference
on Machine Learning , 2017. URL http://proceedings.mlr.press/v70/amos17b.
html . 1, D.1
Steve Awodey. Category Theory . Oxford University Press, 2010. URL https://doi.org/10
.1093/acprof:oso/9780198568612.001.0001 . A.1
Giorgio Bacci, Radu Mardare, Prakash Panangaden, and Gordon Plotkin. Propositional logics for the
Lawvere quantale. Electronic Notes in Theoretical Informatics and Computer Science , 3, 2023.
URL https://doi.org/10.46298/entics.12292 .https://arxiv.org/abs/
2302.01224 . B.9, D.2
Giorgio Bacci, Radu Mardare, Prakash Panangaden, and Gordon Plotkin. Polynomial Lawvere logic.
arXiv preprint , 2024. URL https://arxiv.org/abs/2402.03543 . D.2
Samy Badreddine, Artur d’Avila Garcez, Luciano Serafini, and Michael Spranger. Logic tensor
networks. Artificial Intelligence , 303:103649, 2022. URL https://doi.org/10.1016/j.
artint.2021.103649 .https://arxiv.org/abs/2012.13635 . D.2
Nikita Balabin, Daria V oronkova, Ilya Trofimov, Evgeny Burnaev, and Serguei Barannikov.
Disentanglement learning via topology. In International Conference on Machine Learning , 2024.
D.1
Han Bao and Masashi Sugiyama. Calibrated surrogate maximization of linear-fractional utility in
binary classification. In International Conference on Artificial Intelligence and Statistics , pages
2337–2347, 2020. URL http://proceedings.mlr.press/v108/bao20a.html . D.2
Han Bao, Clay Scott, and Masashi Sugiyama. Calibrated surrogate losses for adversarially
robust classification. In Conference on Learning Theory , pages 408–451, 2020. URL http:
//proceedings.mlr.press/v125/bao20a.html . D.2
Michael Barr and Charles Wells. Category Theory for Computing Science , volume 1. Prentice Hall
New York, 1990. URL https://www.math.mcgill.ca/triples/Barr-Wells-ctc
s.pdf . A.1
Sugato Basu, Arindam Banerjee, and Raymond J Mooney. Semi-supervised clustering by seeding. In
International Conference on Machine Learning , 2002. URL https://dl.acm.org/doi/1
0.5555/645531.656012 . E.2
11Jens Behrmann, Will Grathwohl, Ricky TQ Chen, David Duvenaud, and Jörn-Henrik Jacobsen.
Invertible residual networks. In International Conference on Machine Learning , 2019. URL
https://proceedings.mlr.press/v97/behrmann19a.html . D.1
Itaï Ben Yaacov. On the expressive power of quantifiers in continuous logic. arXiv preprint , 2022.
URLhttps://arxiv.org/abs/2207.01863 . D.2
Itaï Ben Yaacov and Alexander Usvyatsov. Continuous first order logic and local stability.
Transactions of the American Mathematical Society , 362(10):5213–5259, 2010. URL https:
//doi.org/10.1090/S0002-9947-10-04837-3 .https://arxiv.org/abs/08
01.4303 . D.2
Itaï Ben Yaacov, Alexander Berenstein, C. Ward Henson, and Alexander Usvyatsov. Model Theory for
Metric Structures , page 315–427. London Mathematical Society Lecture Note Series. Cambridge
University Press, 2008. URL https://doi.org/10.1017/CBO9780511735219.011 .
D.2
Yoshua Bengio, Aaron Courville, and Pascal Vincent. Representation learning: A review and
new perspectives. IEEE Transactions on Pattern Analysis and Machine Intelligence , 35(8):
1798–1828, 2013. URL https://doi.org/10.1109/TPAMI.2013.50 .https:
//arxiv.org/abs/1206.5538 . 1, 1, 2.1, 2.2, D.1
Merrie Bergmann. An Introduction to Many-Valued and Fuzzy Logic: Semantics, Algebras, and
Derivation Systems . Cambridge University Press, 2008. URL https://doi.org/10.1017/
CBO9780511801129 . B.1, D.2
Mikhail Bilenko, Sugato Basu, and Raymond J Mooney. Integrating constraints and metric learning
in semi-supervised clustering. In International Conference on Machine Learning , 2004. URL
https://dl.acm.org/doi/10.1145/1015330.1015360 . E.2
George Boole. An Investigation of the Laws of Thought: On Which Are Founded the Mathematical
Theories of Logic and Probabilities . Cambridge University Press, 1854. URL https:
//doi.org/10.1017/CBO9780511693090 . 2.1, D.2
Tai-Danae Bradley, John Terilla, and Yiannis Vlassopoulos. An enriched category theory of language:
From syntax to semantics. La Matematica , 1(2):551–580, 2022. URL https://doi.org/10
.1007/s44007-022-00021-2 .https://arxiv.org/abs/2106.07890 . A.3
Johann Brehmer, Pim De Haan, Phillip Lippe, and Taco Cohen. Weakly supervised causal
representation learning. In Neural Information Processing Systems , 2022. URL https:
//openreview.net/forum?id=dz79MhQXWvg . D.2
Johann Brehmer, Pim De Haan, Sönke Behrends, and Taco Cohen. Geometric algebra transformer.
InNeural Information Processing Systems , 2023. URL https://openreview.net/forum
?id=M7r2CO4tJC . 1, D.1
Leo Breiman, Jerome Friedman, Richard A. Olshen, and Charles J. Stone. Classification and
Regression Trees . Routledge, 1984. URL https://doi.org/10.1201/9781315139470 .
D.1
Chris Burgess and Hyunjik Kim. 3D shapes dataset, 2018. https://github.com/deepmin
d/3d-shapes (Apache License 2.0). 6, 5, E.3, 7, 10
Matteo Capucci. On quantifiers for quantitative reasoning. arXiv preprint , 2024. URL https:
//arxiv.org/abs/2406.04936 . D.2
Marc-André Carbonneau, Julian Zaidi, Jonathan Boilard, and Ghyslain Gagnon. Measuring
disentanglement: A review of metrics. IEEE Transactions on Neural Networks and Learning
Systems , 2022. URL https://doi.org/10.1109/TNNLS.2022.3218982 .
https://arxiv.org/abs/2012.09276 . 1, 2, 4.3, 4.3, 5, D.1
Hugo Caselles-Dupré, Michael Garcia Ortiz, and David Filliat. Symmetry-based disentangled
representation learning requires interaction with environments. In Neural Information Processing
Systems , 2019. URL https://proceedings.neurips.cc/paper/2019/hash/36e
729ec173b94133d8fa552e4029f8b-Abstract.html . D.1
12Chen Chung Chang. Algebraic analysis of many valued logics. Transactions of the American
Mathematical society , 88(2):467–490, 1958. URL https://doi.org/10.2307/1993227 .
D.2
Chen Chung Chang and H Jerome Keisler. Continuous Model Theory , volume 58 of Annals of
Mathematics Studies . Princeton University Press, 1966. URL https://doi.org/10.1515/
9781400882052 . D.2
Ricky TQ Chen, Xuechen Li, Roger B Grosse, and David K Duvenaud. Isolating sources of
disentanglement in variational autoencoders. In Neural Information Processing Systems , 2018.
URL https://proceedings.neurips.cc/paper/2018/hash/1ee3dfcd8a064
5a25a35977997223d22-Abstract.html . 1, 2, 5, D.1, 5, E.3, 6, 7
Shuxiao Chen, Edgar Dobriban, and Jane H Lee. A group-theoretic framework for data augmentation.
The Journal of Machine Learning Research , 21(245):1–71, 2020. URL http://jmlr.org/p
apers/v21/20-163.html . D.1
Yiting Chen, Zhanpeng Zhou, and Junchi Yan. Going beyond neural network feature similarity:
The network feature complexity and its interpretation using category theory. In International
Conference on Learning Representations , 2024. URL https://openreview.net/forum
?id=4bSQ3lsfEV . A.1
Kenta Cho and Bart Jacobs. Disintegration and Bayesian inversion via string diagrams. Mathematical
Structures in Computer Science , 29(7):938–971, 2019. URL https://doi.org/10.1017/
S0960129518000488 .https://arxiv.org/abs/1709.00322 . A.1
Simon Cho. Categorical semantics of metric spaces and continuous logic. The Journal of Symbolic
Logic , 85(3):1044–1078, 2020. URL https://doi.org/10.1017/jsl.2020.44 . D.2
Ernest J Cockayne and Zdzislaw A Melzak. Euclidean constructibility in graph-minimization
problems. Mathematics Magazine , 42(4):206–208, 1969. URL https://doi.org/10.108
0/0025570X.1969.11975961 . 4.1
Michael B Cohen, Yin Tat Lee, Gary Miller, Jakub Pachocki, and Aaron Sidford. Geometric median
in nearly linear time. In Proceedings of the forty-eighth annual ACM symposium on Theory of
Computing , pages 9–21, 2016. URL https://doi.org/10.1145/2897518.2897647 .
4.1
Taco Cohen. Equivariant convolutional networks . PhD thesis, University of Amsterdam, 2021. URL
https://hdl.handle.net/11245.1/0f7014ae-ee94-430e-a5d8-37d03d8d1
0e6. D.1
Taco Cohen and Max Welling. Learning the irreducible representations of commutative Lie groups.
InInternational Conference on Machine Learning , 2014. URL https://proceedings.ml
r.press/v32/cohen14.html . D.1
Taco Cohen and Max Welling. Transformation properties of learned visual representations. In
International Conference on Learning Representations , 2015. URL http://arxiv.org/ab
s/1412.7659 . D.1
Taco Cohen and Max Welling. Group equivariant convolutional networks. In International Conference
on Machine Learning , 2016. URL http://proceedings.mlr.press/v48/cohenc16
.html . D.1
Geoffrey SH Cruttwell, Bruno Gavranovi ´c, Neil Ghani, Paul Wilson, and Fabio Zanasi. Categorical
foundations of gradient-based learning. In Programming Languages and Systems , pages 1–28.
Springer International Publishing, 2022. URL https://doi.org/10.1007/978-3-030
-99336-8_1 .https://arxiv.org/abs/2103.01931 . A.1
Haskell B. Curry. Some philosophical aspects of combinatory logic. In Studies in Logic and
the Foundations of Mathematics , volume 101, pages 85–101. Elsevier, 1980. URL https:
//doi.org/10.1016/S0049-237X(08)71254-0 . 9
13Francesco Dagnino and Fabio Pasquali. Logical foundations of quantitative equality. In Logic in
Computer Science , 2022. URL https://doi.org/10.1145/3531130.3533337 . D.2
Hennie Daniels and Marina Velikova. Monotone and partially monotone neural networks. IEEE
Transactions on Neural Networks , 21(6):906–917, 2010. URL https://doi.org/10.110
9/TNN.2010.2044803 . D.1
Artur S. d’Avila Garcez, Krysia B. Broda, and Dov M. Gabbay. Neural-Symbolic Learning Systems .
Springer, 2002. URL https://doi.org/10.1007/978-1-4471-0211-3 . D.2
Pim de Haan, Taco Cohen, and Max Welling. Natural graph networks. In Neural Information
Processing Systems , 2020. URL https://proceedings.neurips.cc/paper/2020/
hash/2517756c5a9be6ac007fe9bb7fb92611-Abstract.html . A.1, D.1
Andrea Dittadi, Frederik Träuble, Francesco Locatello, Manuel Wuthrich, Vaibhav Agrawal, Ole
Winther, Stefan Bauer, and Bernhard Schölkopf. On the transfer of disentangled representations
in realistic settings. In International Conference on Learning Representations , 2021. URL
https://openreview.net/forum?id=8VXvj1QNRl1 . D.1
Kien Do and Truyen Tran. Theory and evaluation metrics for learning disentangled representations.
InInternational Conference on Learning Representations , 2020. URL https://openreview
.net/forum?id=HJgK0h4Ywr . 1, D.1
Andrew Dudzik. Quantales and Hyperstructures: Monads, Mo’Problems . PhD thesis, University of
California, Berkeley, 2017. URL https://arxiv.org/abs/1707.09227 . B.9
Andrew Joseph Dudzik and Petar Veli ˇckovi ´c. Graph neural networks are dynamic programmers. In
Neural Information Processing Systems , 2022. URL https://openreview.net/forum?i
d=wu1Za9dY1GY . A.1
Cian Eastwood and Christopher KI Williams. A framework for the quantitative evaluation of
disentangled representations. In International Conference on Learning Representations , 2018.
URL https://openreview.net/forum?id=By-7dz-AZ . 1, 1, 2, 4.3, 4.3, 2, 5, D.1, 5,
6, 7, E.4, E.5
Cian Eastwood, Andrei Liviu Nicolicioiu, Julius V on Kügelgen, Armin Keki ´c, Frederik Träuble,
Andrea Dittadi, and Bernhard Schölkopf. DCI-ES: An extended disentanglement framework with
connections to identifiability. In International Conference on Learning Representations , 2023.
URLhttps://openreview.net/forum?id=462z-gLgSht . 4.3, D.1
Ronald Fagin, Ryan Riegel, and Alexander Gray. Foundations of reasoning with uncertainty via
real-valued logics. Proceedings of the National Academy of Sciences , 121(21), 2024. URL
https://doi.org/10.1073/pnas.2309905121 . D.2
Daniel Figueroa and Benno van den Berg. A topos for continuous logic. Theory and Applications
of Categories , 38(28), 2022. URL http://www.tac.mta.ca/tac/volumes/38/28/3
8-28abs.html . D.2
Kaspar Fischer, Bernd Gärtner, and Martin Kutz. Fast smallest-enclosing-ball computation in high
dimensions. In European Symposium on Algorithms , pages 630–641. Springer, 2003. URL
https://doi.org/10.1007/978-3-540-39658-1_57 . 4.1
Brendan Fong and David I Spivak. An Invitation to Applied Category Theory: Seven Sketches in
Compositionality . Cambridge University Press, 2019. URL https://doi.org/10.1017/
9781108668804 .https://arxiv.org/abs/1803.05316 . A.1, B.1, B.6, B.8
Tobias Fritz. A synthetic approach to Markov kernels, conditional independence and theorems on
sufficient statistics. Advances in Mathematics , 370:107239, 2020. URL https://doi.org/
10.1016/j.aim.2020.107239 .https://arxiv.org/abs/1908.07021 . A.1
Soichiro Fujii. Ordered semirings and subadditive morphisms. arXiv preprint , 2023. URL
https://arxiv.org/abs/2311.03862 . B.9
14Marco Fumero, Luca Cosmo, Simone Melzi, and Emanuele Rodolà. Learning disentangled
representations via product manifold projection. In International Conference on Machine Learning ,
2021. URL http://proceedings.mlr.press/v139/fumero21a.html . 1, D.1
Bruno Gavranovi ´c, Paul Lessard, Andrew Joseph Dudzik, Tamara von Glehn, João
Guilherme Madeira Araújo, and Petar Veli ˇckovi ´c. Position: Categorical deep learning is an
algebraic theory of all architectures. In International Conference on Machine Learning , 2024.
URLhttps://openreview.net/forum?id=EIcxV7T0Sy . A.1
Muhammad Waleed Gondal, Manuel Wuthrich, Djordje Miladinovic, Francesco Locatello, Martin
Breidt, Valentin V olchkov, Joel Akpo, Olivier Bachem, Bernhard Schölkopf, and Stefan Bauer. On
the transfer of inductive bias from simulation to the real world: a new disentanglement dataset. In
Neural Information Processing Systems , 2019. URL https://proceedings.neurips.cc
/paper/2019/hash/d97d404b6119214e4a7018391195240a-Abstract.html .
https://github.com/rr-learning/disentanglement_dataset (Creative
Commons Attribution 4.0 International License). 6, 5, E.3, 7, E.6, 11
Ian Goodfellow, Honglak Lee, Quoc Le, Andrew Saxe, and Andrew Ng. Measuring invariances
in deep networks. In Neural Information Processing Systems , 2009. URL https://procee
dings.neurips.cc/paper/2009/hash/428fca9bc1921c25c5121f9da7815cd
e-Abstract.html . D.1
Rachid Guerraoui, Nirupam Gupta, and Rafael Pinot. Byzantine machine learning: A primer. ACM
Computing Surveys , 2023. URL https://doi.org/10.1145/3616537 . 4.1
Petr Hájek. Metamathematics of Fuzzy Logic , volume 4 of Trends in Logic . Springer, 1998. URL
https://doi.org/10.1007/978-94-011-5300-3 . D.2
Charles R. Harris, K. Jarrod Millman, Stéfan J. van der Walt, Ralf Gommers, Pauli Virtanen, David
Cournapeau, Eric Wieser, Julian Taylor, Sebastian Berg, Nathaniel J. Smith, Robert Kern, Matti
Picus, Stephan Hoyer, Marten H. van Kerkwijk, Matthew Brett, Allan Haldane, Jaime Fernández
del Río, Mark Wiebe, Pearu Peterson, Pierre Gérard-Marchant, Kevin Sheppard, Tyler Reddy,
Warren Weckesser, Hameer Abbasi, Christoph Gohlke, and Travis E. Oliphant. Array programming
with NumPy. Nature , 585(7825):357–362, 2020. URL https://doi.org/10.1038/s415
86-020-2649-2 .https://numpy.org . D.6
Irina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick,
Shakir Mohamed, and Alexander Lerchner. beta-V AE: Learning basic visual concepts with a
constrained variational framework. In International Conference on Learning Representations ,
2017. URL https://openreview.net/forum?id=Sy2fzU9gl . 1, 4.3, 2, 5, D.1, D.2,
5, E.3, E.4, 6, 7, E.6
Irina Higgins, David Amos, David Pfau, Sebastien Racaniere, Loic Matthey, Danilo Rezende, and
Alexander Lerchner. Towards a definition of disentangled representations. arXiv preprint , 2018.
URLhttps://arxiv.org/abs/1812.02230 . 1, D.1
Aapo Hyvärinen and Erkki Oja. Independent component analysis: Algorithms and applications.
Neural networks , 13(4):411–430, 2000. URL https://doi.org/10.1016/S0893-608
0(00)00026-5 . D.1
Isao Ishikawa, Takeshi Teshima, Koichi Tojo, Kenta Oono, Masahiro Ikeda, and Masashi Sugiyama.
Universal approximation property of invertible neural networks. Journal of Machine Learning
Research , 24(287):1–68, 2023. URL https://www.jmlr.org/papers/v24/22-0384.
html . D.1
Ashish Jaiswal, Ashwin Ramesh Babu, Mohammad Zaki Zadeh, Debapriya Banerjee, and Fillia
Makedon. A survey on contrastive self-supervised learning. Technologies , 9(1):2, 2020. URL
https://doi.org/10.3390/technologies9010002 .https://arxiv.org/ab
s/2011.00362 . 4.3
Peter Johnstone. Sketches of an Elephant: A Topos Theory Compendium . Oxford University Press,
2002. A.2
15Max Kelly. Basic concepts of enriched category theory. In London Mathematical Society Lecture
Note Series , volume 64. Cambridge University Press, 1982. URL http://www.tac.mta.ca
/tac/reprints/articles/10/tr10.html . A.3
Maurice G. Kendall. A new measure of rank correlation. Biometrika , 30(1/2):81–93, 1938. URL
https://doi.org/10.2307/2332226 . E.4
Hamza Keurti, Hsiao-Ru Pan, Michel Besserve, Benjamin F Grewe, and Bernhard Schölkopf.
Homomorphism AutoEncoder – learning group structured representations from observed
transitions. In International Conference on Machine Learning , 2023. URL https://pr
oceedings.mlr.press/v202/keurti23a.html . D.1
Hyunjik Kim and Andriy Mnih. Disentangling by factorising. In International Conference on Machine
Learning , 2018. URL http://proceedings.mlr.press/v80/kim18b.html . 1, 4.3,
2, 5, D.1, D.2, 5, E.3, E.4, 6, 7
Diederik P. Kingma and Max Welling. Auto-encoding variational bayes. In International Conference
on Learning Representations , 2014. URL https://openreview.net/forum?id=33X9
fd2-9FyZd .http://arxiv.org/abs/1312.6114 . E.3
Jonas Köhler, Leon Klein, and Frank Noé. Equivariant flows: exact likelihood generative learning
for symmetric densities. In International Conference on Machine Learning , 2020. URL
https://proceedings.mlr.press/v119/kohler20a.html . D.1
Daphne Koller and Nir Friedman. Probabilistic Graphical Models: Principles and Techniques . MIT
press, 2009. URL https://mitpress.mit.edu/9780262013192/ . D.1
Solomon Kullback and Richard A. Leibler. On information and sufficiency. The Annals of
Mathematical Statistics , 22(1):79–86, 1951. URL https://doi.org/10.1214/aoms
/1177729694 .https://www.jstor.org/stable/2236703 . D.2
Henry Kvinge, Tegan Emerson, Grayson Jorgenson, Scott Vasquez, Timothy Doster, and Jesse Lew.
In what ways are deep neural networks invariant and how should we measure this? In Neural
Information Processing Systems , 2022. URL https://openreview.net/forum?id=SC
D0hn3kMHw . 1, D.1, D.2
F. William Lawvere. Adjointness in foundations. Dialectica , pages 281–296, 1969. URL
https://doi.org/10.1111/j.1746-8361.1969.tb01194.x .http:
//www.tac.mta.ca/tac/reprints/articles/16/tr16abs.html . B.10
F. William Lawvere. Metric spaces, generalized logic, and closed categories. Rendiconti del seminario
matématico e fisico di Milano , 43:135–166, 1973. URL https://doi.org/10.1007/BF02
924844 .http://www.tac.mta.ca/tac/reprints/articles/1/tr1abs.html .
A.3, A.3, B.1, B.9, D.2
F. William Lawvere. Taking categories seriously. Revista colombiana de matematicas , 20(3-4):147–
178, 1986. http://www.tac.mta.ca/tac/reprints/articles/8/tr8abs.html .
4.1
F William Lawvere and Robert Rosebrugh. Sets for Mathematics . Cambridge University Press, 2003.
URLhttps://doi.org/10.1017/CBO9780511755460 . 2.1, A.2
Juho Lee, Yoonho Lee, Jungtaek Kim, Adam Kosiorek, Seungjin Choi, and Yee Whye Teh.
Set transformer: A framework for attention-based permutation-invariant neural networks. In
International Conference on Machine Learning , 2019. URL http://proceedings.mlr.pr
ess/v97/lee19d . 1, D.1
Tom Leinster. An informal introduction to topos theory. arXiv preprint , 2010. URL https:
//arxiv.org/abs/1012.5647 . A.2
Tom Leinster. Basic Category Theory . Cambridge University Press, 2014. URL https://doi.
org/10.1017/CBO9781107360068 .https://arxiv.org/abs/1612.09375 . A.1
16Zhiyuan Li, Jaideep Vitthal Murkute, Prashnna Kumar Gyawali, and Linwei Wang. Progressive
learning and disentanglement of hierarchical representations. In International Conference on
Learning Representations , 2020. URL https://openreview.net/forum?id=SJxpsx
rYPS . 1, D.1
Francesco Locatello, Gabriele Abbati, Thomas Rainforth, Stefan Bauer, Bernhard Schölkopf, and
Olivier Bachem. On the fairness of disentangled representations. In Neural Information Processing
Systems , 2019a. URL https://proceedings.neurips.cc/paper/2019/hash/1b4
86d7a5189ebe8d8c46afc64b0d1b4-Abstract.html . D.1
Francesco Locatello, Stefan Bauer, Mario Lucic, Gunnar Raetsch, Sylvain Gelly, Bernhard Schölkopf,
and Olivier Bachem. Challenging common assumptions in the unsupervised learning of
disentangled representations. In International Conference on Machine Learning , 2019b. URL
https://proceedings.mlr.press/v97/locatello19a.html . D.1, 14, E.3
Francesco Locatello, Ben Poole, Gunnar Rätsch, Bernhard Schölkopf, Olivier Bachem, and Michael
Tschannen. Weakly-supervised disentanglement without compromises. In International Conference
on Machine Learning , 2020. URL http://proceedings.mlr.press/v119/locatel
lo20a.html . D.2
Jan Łukasiewicz. O logice trójwarto ´sciowej (On three-valued logic). In Selected Works , volume 11
ofStudies in Logic , pages 87–88. North-Holland Publishing Company, 1920. D.2
Jan Łukasiewicz and Alfred Tarski. Untersuchungen über den Aussagenkalkül (Investigations on the
propositional calculus). Comptes Rendus des Séances de la Société des Sciences et des Lettres de
Varsovie, Class III , 23, 1930. D.2
Saunders Mac Lane. Categories for the Working Mathematician . Springer, 1978. URL https:
//doi.org/10.1007/978-1-4757-4721-8 . 1, A.1, A.1
Saunders Mac Lane and Ieke Moerdijk. Sheaves in Geometry and Logic: A First Introduction to Topos
Theory . Springer, 1994. URL https://doi.org/10.1007/978-1-4612-0927-0 . A.2
Louis Mahon, Lei Shah, and Thomas Lukasiewicz. Correcting flaws in common disentanglement
metrics. arXiv preprint , 2023. URL https://arxiv.org/abs/2304.02335 . D.7
Gregorz Malinowski. Many-valued logic and its philosophy. In Handbook of the History of Logic ,
volume 8, pages 13–94. Elsevier, 2007. URL https://doi.org/10.1016/S1874-585
7(07)80004-5 . D.2
Robin Manhaeve, Sebastijan Dumancic, Angelika Kimmig, Thomas Demeester, and Luc De Raedt.
DeepProbLog: Neural probabilistic logic programming. In Neural Information Processing Systems ,
2018. URL https://proceedings.neurips.cc/paper/2018/hash/dc5d637ed
5e62c36ecb73b654b05ba2a-Abstract.html . D.2
Radu Mardare, Prakash Panangaden, and Gordon Plotkin. Quantitative algebraic reasoning. In Logic
in Computer Science , 2016. URL https://doi.org/10.1145/2933575.2934518 . D.2
Radu Mardare, Prakash Panangaden, and Gordon Plotkin. Fixed-points for quantitative equational
logics. In Logic in Computer Science , 2021. URL https://doi.org/10.1109/LICS52
264.2021.9470662 . D.2
Haggai Maron, Heli Ben-Hamu, Nadav Shamir, and Yaron Lipman. Invariant and equivariant
graph networks. In International Conference on Learning Representations , 2019. URL
https://openreview.net/forum?id=Syx72jC9tm . D.1
Loic Matthey, Irina Higgins, Demis Hassabis, and Alexander Lerchner. dSprites: Disentanglement
testing sprites dataset, 2017. https://github.com/deepmind/dsprites-dataset
(Apache License 2.0). 6, 5, E.3, 7, 9
Barry Mazur. When is one thing equal to some other thing? In Proof and Other Dilemmas:
Mathematics and Philosophy , page 221–242. Mathematical Association of America, 2008. URL
https://doi.org/10.5948/UPO9781614445050.015 .https://people.math.
harvard.edu/~mazur/preprints/when_is_one.pdf . 3.1
17Peter Meer, Doron Mintz, Azriel Rosenfeld, and Dong Yoon Kim. Robust regression methods
for computer vision: A review. International journal of computer vision , 6:59–70, 1991. URL
https://doi.org/10.1007/BF00127126 . 4.1
Nimrod Megiddo. The weighted Euclidean 1-center problem. Mathematics of Operations Research ,
8(4):498–504, 1983. URL https://doi.org/10.1287/moor.8.4.498 . 2
Stanislav Minsker. Geometric median and robust estimation in Banach spaces. Bernoulli , 21(4):
2308–2335, 2015. URL https://doi.org/10.3150/14-BEJ645 . 4.1
Takeru Miyato, Masanori Koyama, and Kenji Fukumizu. Unsupervised learning of equivariant
structure from sequences. In Neural Information Processing Systems , 2022. URL https:
//openreview.net/forum?id=7b7iGkuVqlZ . D.1
Milton L. Montero, Jeffrey Bowers, Rui Ponte Costa, Casimir JH Ludwig, and Gaurav Malhotra.
Lost in latent space: Examining failures of disentangled models at combinatorial generalisation. In
Neural Information Processing Systems , 2022. URL https://openreview.net/forum?i
d=7yUxTNWyQGf . D.7
Milton Llera Montero, Casimir JH Ludwig, Rui Ponte Costa, Gaurav Malhotra, and Jeffrey
Bowers. The role of disentanglement in generalisation. In International Conference on Learning
Representations , 2021. URL https://openreview.net/forum?id=qbH974jKUVy .
D.1, D.7
Christopher J. Mulvey. &. Supplemento ai Rendiconti del Circolo Matemàtico di Palermo. Serie II ,
12:99–104, 1986. URL https://zbmath.org/0633.46065 . B.9
Kevin Musgrave, Serge Belongie, and Ser-Nam Lim. A metric learning reality check. In European
Conference on Computer Vision , pages 681–699, 2020. URL https://doi.org/10.1007/
978-3-030-58595-2_41 . 4.3
Aviv Navon, Aviv Shamsian, Idan Achituve, Ethan Fetaya, Gal Chechik, and Haggai Maron.
Equivariant architectures for learning in deep weight spaces. In International Conference on
Machine Learning , 2023. URL https://proceedings.mlr.press/v202/navon23a.
html . D.1
Chenri Ni, Nontawat Charoenphakdee, Junya Honda, and Masashi Sugiyama. On the calibration of
multiclass classification with rejection. In Neural Information Processing Systems , 2019. URL
https://proceedings.neurips.cc/paper/2019/hash/571d3a9420bfd9219
f65b643d0003bf4-Abstract.html . D.2
Matthew Painter, Adam Prugel-Bennett, and Jonathon Hare. Linear disentangled representations
and unsupervised action estimation. In Neural Information Processing Systems , 2020. URL
https://proceedings.neurips.cc/paper/2020/hash/9a02387b02ce7de2d
ac4b925892f68fb-Abstract.html . D.1
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan,
Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas
Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy,
Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. PyTorch: An imperative style, high-
performance deep learning library. In Neural Information Processing Systems , 2019. URL
https://proceedings.neurips.cc/paper/2019/hash/bdbca288fee7f92f2
bfa9f7012727740-Abstract.html .https://pytorch.org . D.6, E.3
Edward Pearce-Crump. Categorification of group equivariant neural networks. arXiv preprint , 2023.
URLhttps://arxiv.org/abs/2304.14144 . A.1
F. Pedregosa, G. Varoquaux, A. Gramfort, V . Michel, B. Thirion, O. Grisel, M. Blondel,
P. Prettenhofer, R. Weiss, V . Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher,
M. Perrot, and E. Duchesnay. Scikit-learn: Machine learning in Python. Journal of Machine
Learning Research , 12:2825–2830, 2011. URL https://jmlr.org/papers/v12/pedr
egosa11a.html .https://scikit-learn.org . E.4, E.5
18Paolo Perrone. Markov categories and entropy. IEEE Transactions on Information Theory , 2023.
URL https://doi.org/10.1109/TIT.2023.3328825 .https://arxiv.org/
abs/2212.11719 . A.1, D.2
David Pfau, Irina Higgins, Alex Botev, and Sébastien Racanière. Disentangling by subspace diffusion.
InNeural Information Processing Systems , 2020. URL https://proceedings.neurips.
cc/paper/2020/hash/c9f029a6a1b20a8408f372351b321dd8-Abstract.ht
ml. D.1
Krishna Pillutla, Sham M. Kakade, and Zaid Harchaoui. Robust aggregation for federated
learning. IEEE Transactions on Signal Processing , 70:1142–1154, 2022. URL https:
//doi.org/10.1109/TSP.2022.3153135 . 4.1, 13
Jean-Eric Pin. Tropical semirings. In Idempotency , pages 50–69. Cambridge University Press, 1998.
URLhttps://doi.org/10.1017/CBO9780511662508.004 . B.7
Robin Quessard, Thomas Barrett, and William Clements. Learning disentangled representations
and group structure of dynamical environments. In Neural Information Processing Systems , 2020.
URL https://proceedings.neurips.cc/paper/2020/hash/e449b9317dad9
20c0dd5ad0a2a2d5e49-Abstract.html . D.1
Scott E Reed, Yi Zhang, Yuting Zhang, and Honglak Lee. Deep visual analogy-making. In Neural
Information Processing Systems , 2015. URL https://proceedings.neurips.cc/pap
er/2015/hash/e07413354875be01a996dc560274708e-Abstract.html . 6, 5,
E.3, 7, 8
Mark D. Reid and Robert C. Williamson. Composite binary losses. Journal of Machine Learning
Research , 11(83):2387–2422, 2010. URL http://jmlr.org/papers/v11/reid10a.ht
ml. D.2
Danilo Rezende and Shakir Mohamed. Variational inference with normalizing flows. In International
Conference on Machine Learning , 2015. URL https://proceedings.mlr.press/v37/
rezende15.html . D.1
Karl Ridgeway and Michael C Mozer. Learning deep disentangled embeddings with the f-statistic
loss. In Neural Information Processing Systems , 2018. URL https://proceedings.neur
ips.cc/paper/2018/hash/2b24d495052a8ce66358eb576b8912c8-Abstract.
html . 1, 1, 2, 2.2, 4.3, 4.3, D.1, D.2, E.2
Karsten Roth, Mark Ibrahim, Zeynep Akata, Pascal Vincent, and Diane Bouchacourt.
Disentanglement of correlated factors via hausdorff factorized support. In International Conference
on Learning Representations , 2023. URL https://openreview.net/forum?id=OKcJ
hpQiGiX . D.7
Bernhard Schölkopf and Julius von Kügelgen. From statistical to causal learning. In International
Congress of Mathematicians . EMS Press, 2022. URL https://doi.org/10.4171/icm2
022/173 .https://arxiv.org/abs/2204.00607 . D.1
Prithviraj Sen, Breno WSR de Carvalho, Ryan Riegel, and Alexander Gray. Neuro-symbolic
inductive logic programming with logical neural networks. In Proceedings of the AAAI Conference
on Artificial Intelligence , 2022. URL https://doi.org/10.1609/aaai.v36i8.20795 .
D.2
Dan Shiebler, Bruno Gavranovi ´c, and Paul Wilson. Category theory in machine learning. arXiv
preprint , 2021. URL https://arxiv.org/abs/2106.07032 . A.1
Daniel Shiebler. Compositionality and Functorial Invariants in Machine Learning . PhD thesis,
University of Oxford, 2023. URL http://doi.org/10.5287/bodleian:DE1aDx4Zw .
A.1
Rui Shu, Yining Chen, Abhishek Kumar, Stefano Ermon, and Ben Poole. Weakly supervised
disentanglement with guarantees. In International Conference on Learning Representations , 2020.
URLhttps://openreview.net/forum?id=HJgSwyBKvr . 1, D.2, E.2
19Joseph Sill. Monotonic networks. In Neural Information Processing Systems , 1997. URL
https://proceedings.neurips.cc/paper/1997/hash/83adc9225e4de
b67d7ce42d58fe5157c-Abstract.html . D.1
Ingo Steinwart. How to compare different loss functions and their risks. Constructive Approximation ,
26:225–287, 2007. URL https://doi.org/10.1007/s00365-006-0662-3 . D.2
Raphael Suter, Djordje Miladinovic, Bernhard Schölkopf, and Stefan Bauer. Robustly disentangled
causal mechanisms: Validating deep representations for interventional robustness. In International
Conference on Machine Learning , 2019. URL http://proceedings.mlr.press/v97/
suter19a.html . 1
Seiya Tokui and Issei Sato. Disentanglement analysis with partial information decomposition. In
International Conference on Learning Representations , 2022. URL https://openreview.n
et/forum?id=pETy-HVvGtt . 1, D.1
Loek Tonnaer, Luis A Pérez Rey, Vlado Menkovski, Mike Holenderski, and Jacobus W Portegies.
Quantifying and learning linear symmetry-based disentanglement. In International Conference
on Machine Learning , 2022. URL https://proceedings.mlr.press/v162/tonna
er22a.html . D.1
Frederik Träuble, Elliot Creager, Niki Kilbertus, Francesco Locatello, Andrea Dittadi, Anirudh
Goyal, Bernhard Schölkopf, and Stefan Bauer. On disentangled representations learned from
correlated data. In International Conference on Machine Learning , 2021. URL https:
//proceedings.mlr.press/v139/trauble21a.html . D.7, D.7
Todd Trimble. An elementary approach to elementary topos theory, 2019. URL https:
//ncatlab.org/toddtrimble/published/An+elementary+approach+to+
elementary+topos+theory . A.2
Alexey A. Tuzhilin. Who invented the Gromov-Hausdorff distance? arXiv preprint , 2016. URL
https://arxiv.org/abs/1612.00728 . 4.1
Elise van der Pol, Herke van Hoof, Frans A Oliehoek, and Max Welling. Multi-agent MDP
homomorphic networks. In International Conference on Learning Representations , 2022. URL
https://openreview.net/forum?id=H7HDG--DJF0 . D.1
Pauli Virtanen, Ralf Gommers, Travis E. Oliphant, Matt Haberland, Tyler Reddy, David Cournapeau,
Evgeni Burovski, Pearu Peterson, Warren Weckesser, Jonathan Bright, Stéfan J. van der Walt,
Matthew Brett, Joshua Wilson, K. Jarrod Millman, Nikolay Mayorov, Andrew R. J. Nelson, Eric
Jones, Robert Kern, Eric Larson, C J Carey, ˙Ilhan Polat, Yu Feng, Eric W. Moore, Jake VanderPlas,
Denis Laxalde, Josef Perktold, Robert Cimrman, Ian Henriksen, E. A. Quintero, Charles R. Harris,
Anne M. Archibald, Antônio H. Ribeiro, Fabian Pedregosa, Paul van Mulbregt, and SciPy 1.0
Contributors. SciPy 1.0: Fundamental algorithms for scientific computing in Python. Nature
methods , 17(3):261–272, 2020. URL https://doi.org/10.1038/s41592-019-068
6-2.https://scipy.org . E.4
Kiri Wagstaff, Claire Cardie, Seth Rogers, and Stefan Schrödl. Constrained k-means clustering
with background knowledge. In International Conference on Machine Learning , 2001. URL
https://dl.acm.org/doi/10.5555/645530.655669 . E.2
Tongzhou Wang and Phillip Isola. Understanding contrastive representation learning through
alignment and uniformity on the hypersphere. In International conference on machine learning ,
2020. URL https://proceedings.mlr.press/v119/wang20k.html . 1, 4.3
Endre Weiszfeld. Sur le point pour lequel la somme des distances de n points donnés est minimum (on
the point for which the sum of the distances to n given points is minimum). Tohoku Mathematical
Journal, First Series , 43:355–386, 1937. URL https://doi.org/10.1007/s10479-0
08-0352-z . 3
Emo Welzl. Smallest enclosing disks (balls and ellipsoids). In New Results and New Trends in
Computer Science , pages 359–370. Springer, 1991. URL https://doi.org/10.1007/BF
b0038202 . 4.1, 12
20Zhenlin Xu, Marc Niethammer, and Colin A Raffel. Compositional generalization in unsupervised
compositional representation learning: A study on disentanglement and emergent language. In
Neural Information Processing Systems , 2022. URL https://openreview.net/forum?i
d=ZEQ5Gf8DiD . D.1
Tao Yang, Xuanchi Ren, Yuwang Wang, Wenjun Zeng, and Nanning Zheng. Towards building a
group-based unsupervised representation disentanglement framework. In International Conference
on Learning Representations , 2022. URL https://openreview.net/forum?id=YgPq
Nctmyd . D.1
Yang Yuan. On the power of foundation models. In International Conference on Machine Learning ,
2023. URL https://proceedings.mlr.press/v202/yuan23b.html . A.1
Manzil Zaheer, Satwik Kottur, Siamak Ravanbakhsh, Barnabas Poczos, Russ R Salakhutdinov,
and Alexander J Smola. Deep sets. In Neural Information Processing Systems , 2017. URL
https://proceedings.neurips.cc/paper/2017/hash/f22e4747da1aa27e3
63d86d40ff442fe-Abstract.html . D.1
Sharon Zhang, Amit Moscovich, and Amit Singer. Product manifold learning. In International
Conference on Artificial Intelligence and Statistics , 2021. URL http://proceedings.mlr.
press/v130/zhang21j.html . D.1
Yivan Zhang and Masashi Sugiyama. A category-theoretical meta-analysis of definitions of
disentanglement. In International Conference on Machine Learning , 2023. URL https:
//proceedings.mlr.press/v202/zhang23ak.html . 1, 4.2, A.1, D.1
Sharon Zhou, Eric Zelikman, Fred Lu, Andrew Y Ng, Gunnar Carlsson, and Stefano Ermon.
Evaluating the disentanglement of deep generative models through manifold topology. In
International Conference on Learning Representations , 2020. URL https://openrevi
ew.net/forum?id=djwS0m4Ft_A . D.1
21Contents
1 Introduction 1
2 Logical definitions of disentangled representations 2
2.1 Informativeness: injectivity or retractability of a learning model . . . . . . . . . . 2
2.2 Modularity: product structure preserved by a learning model . . . . . . . . . . . . 3
3 Enrichment: from logic to metric 4
3.1 From equality predicate to strict premetric . . . . . . . . . . . . . . . . . . . . . . 4
3.2 From logical operation to quantitative operation . . . . . . . . . . . . . . . . . . . 4
3.3 From compound predicate to compound quantity . . . . . . . . . . . . . . . . . . 5
3.4 (Sub)homomorphism from metric to logic . . . . . . . . . . . . . . . . . . . . . . 5
4 Quantitative metrics of disentangled representations 6
4.1 Modularity metrics via product approximation . . . . . . . . . . . . . . . . . . . . 6
4.2 Modularity metrics via constancy . . . . . . . . . . . . . . . . . . . . . . . . . . . 8
4.3 Informativeness metrics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9
5 Experiments 10
6 Conclusion 10
Bibliography 11
A Preliminaries 24
A.1 Basic category theory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24
A.2 Elementary topos theory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25
A.3 Enriched category theory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25
B Theory 26
B.1 Subobject quantifier and quantizer . . . . . . . . . . . . . . . . . . . . . . . . . . 26
B.2 Equality and premetric . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28
B.3 Preorder . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28
B.4 Operation: algebra over the product endofunctor . . . . . . . . . . . . . . . . . . . 29
B.5 Negation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31
B.6 Conjunction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32
B.7 Disjunction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34
B.8 Implication . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35
B.9 Heyting algebra, quantale, and ordered semiring . . . . . . . . . . . . . . . . . . . 36
B.10 Quantifier: algebra over the exponentiation endofunctor . . . . . . . . . . . . . . . 37
B.11 Enrichment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38
B.12 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39
C Proofs 41
C.1 Product function . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41
C.2 Constant curried function . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41
D Discussions 42
D.1 Background . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42
D.2 Related work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43
D.3 Implication and equivalence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45
D.4 Constant function . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46
D.5 Rank of imperfect representations . . . . . . . . . . . . . . . . . . . . . . . . . . 47
D.6 Implementation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48
D.7 Limitations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48
D.8 Broader impact . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48
E Experiments 49
E.1 Synthetic data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49
E.2 Weakly supervised modularity metrics . . . . . . . . . . . . . . . . . . . . . . . . 50
E.3 Evaluation of existing models on image datasets . . . . . . . . . . . . . . . . . . . . 51
E.4 Kendall tau distance between metrics . . . . . . . . . . . . . . . . . . . . . . . . . . 51
E.5 Computation time of metrics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 52
E.6 Factor-wise modularity metrics . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53
22List of Figures
1 Disentangled representation learning . . . . . . . . . . . . . . . . . . . . . . . . . 3
2 From predicates and logical operations to quantities and quantitative operations . . 5
3 Negation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31
4 Conjunction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32
5 Disjunction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32
6 Implication . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32
7 Equivalence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32
8 Quantitative operations for implication . . . . . . . . . . . . . . . . . . . . . . . . 45
9 Quantitative operations for equivalence . . . . . . . . . . . . . . . . . . . . . . . 45
10 Constancy metrics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46
11 Radius and variance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47
12 Rank of imperfect representations . . . . . . . . . . . . . . . . . . . . . . . . . . 47
13 Illustration of synthetic data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49
14 Entanglement of a distribution . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49
List of Tables
1 From logic to metric . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4
2 Supervised disentanglement metrics . . . . . . . . . . . . . . . . . . . . . . . . . 10
3 Supervised modularity metrics . . . . . . . . . . . . . . . . . . . . . . . . . . . . 50
4 Weakly supervised modularity metrics . . . . . . . . . . . . . . . . . . . . . . . . 50
5 Supervised disentanglement metrics on image datasets . . . . . . . . . . . . . . . . 51
6 Average Kendall tau rank distances bewteen disentanglement metrics . . . . . . . . 52
7 Computation time (seconds) of supervised disentanglement metrics on image datasets 52
8 Factor-wise modularity metrics on 3D Cars [Reed et al., 2015] . . . . . . . . . . . 53
9 Factor-wise modularity metrics on dSprites [Matthey et al., 2017] . . . . . . . . . 54
10 Factor-wise modularity metrics on 3D Shapes [Burgess and Kim, 2018] . . . . . . 55
11 Factor-wise modularity metrics on MPI3D [Gondal et al., 2019] . . . . . . . . . . 56
23A Preliminaries
In this paper, we used abstract mathematical tools such as category theory and topos theory to develop
a theory of the relationship between logical definitions and quantitative metrics. However, this level
of abstraction may be unfamiliar or even intimidating to some readers, and sometimes unnecessary
for machine learning practitioners. Therefore, we have used only the most basic algebraic concepts,
such as homomorphism, in the main text. For readers interested in the mathematical background, we
provide a brief introduction to the basic categorical concepts in this section.
A.1 Basic category theory
Category theory is a branch of mathematics that studies mathematical structures in an abstract way,
which is suitable for identifying and organizing common patterns across various fields of mathematics
[Mac Lane, 1978, Adámek et al., 1990, Awodey, 2010]. It has found applications in many fields,
including computer science [Barr and Wells, 1990], probability theory [Cho and Jacobs, 2019, Fritz,
2020, Perrone, 2023], and machine learning [de Haan et al., 2020, Shiebler et al., 2021, Cruttwell
et al., 2022, Dudzik and Veli ˇckovi ´c, 2022, Shiebler, 2023, Yuan, 2023, Pearce-Crump, 2023, Chen
et al., 2024, Gavranovi ´c et al., 2024].
The most fundamental concept is that of a category :
Definition 14. Acategory C= (Obj ,Hom,◦,id)consists of
a collection Obj of objects,
asetHom( A, B)of morphisms between objects,10
a composition function ◦: Hom( B, C)×Hom( A, B)→Hom( A, C)for each triple of objects,
and
an identity morphism idA∈Hom( A, A)for each object,
subject to
associativity: (h◦g)◦f=h◦(g◦f)and
identity: idB◦f=f=f◦idA.
A crucial example is the category Setof sets and functions. However, what is of particular interest
is not the category itself but its relationships with other categories. Building on the concepts of the
functor andnatural transformation , whose definitions are omitted here, we can develop tools to better
understand the properties of a category.
Moreover, we can define objects in terms of their relations with other objects, employing what is
known as universal construction . For example, a terminal object 1in a category is an object such that
for any object A, there exists a unique morphism eA:A→1to it, which we call a terminal morphism .
InSet, any set {∗}with only one element is a terminal object. Based on the concept of the terminal
object, a global element of an object Bis defined to be a morphism b: 1→Bfrom a terminal object.
We write bA:A→Bas an abbreviation for the constant morphism b◦eA:AeA− − →1b− →Bwith
value b: 1→B. These concepts will be used to develop our theory in Appendix B. Other important
universal constructions include the product ,pullback , and exponential . The concept of the product is
of great importance in disentangled representation learning [Zhang and Sugiyama, 2023].
Regarding the pullback, we need to mention the following useful lemma:
Lemma 4 (Pullback lemma) .Suppose that in the following commutative diagram, the right square is
a pullback.
· · ·
· · ·(31)
Then, the left square is a pullback if and only if the outer rectangle is a pullback.
This lemma is usually left as an exercise in textbooks [e.g., Mac Lane, 1978, p. 72, Exercise 8,
Leinster, 2014, Exercise 5.1.35]. A proof can be found in Fong and Spivak [2019, Proposition 7.3].
We need to use this lemma to (de)compose pullbacks. As a side note, we use the asterisk f∗gto
denote the pullback of galong f.
10To be more precise, a category whose morphisms are sets is called a locally small category.
24A.2 Elementary topos theory
Topos theory studies categories that, in some sense, exhibit behavior akin to the category of sets
and functions [Lawvere and Rosebrugh, 2003]. Topos theory has found applications in geometry,
topology, and logic [Mac Lane and Moerdijk, 1994, Johnstone, 2002, Leinster, 2010, Trimble, 2019].
In this work, we only explore its relation to logic.
To formally define a topos , two essential concepts are those of the subobject and subobject classifier.
Asubobject of an object Cis simply a monomorphism b:B↣Cto the object C. The subobject
classifier is defined as follows:
Definition 15. In a finitely complete category, a subobject classifier is a universal subobject
⊤: 1↣Ωsuch that for every subobject b:B↣C, there exists a unique morphism χb:C→Ω
such that bis a pullback of ⊤along χb. The morphism χbis called the classifying morphism ofb.
B 1
C Ωb⌟
⊤
χb(32)
Alternatively, we can state that
Proposition 5. A subobject classifier is precisely a terminal object in the category of monomorphisms
and pullbacks.
Then, we can study the morphisms to the object Ω:
Definition 16. In a category with a subobject classifier ⊤: 1↣Ω, apredicate on an object Cis a
morphism p:C→Ω.
Based on Definition 15, we can state that subobjects of an object Care classified by predicates on C.
For example, in Set, subobjects are subsets, a function from a singleton {∗}to a two-element set is
a subobject classifier, which is usually denoted by ⊤:{∗} → {⊤ ,⊥}, a predicate on a set Cis a
function p:C→ {⊤ ,⊥}, and a subset precisely corresponds to its indicator function.
Among various equivalent definitions of a topos, a concise one is as follows:
Definition 17. Anelementary topos is a finitely complete and cartesian closed category with a
subobject classifier.
Despite its concise definition, a great number of logical structures can be derived from it, which will
be explored in Appendix B.
A.3 Enriched category theory
Enriched category theory generalizes the concept of the category by replacing the sets of morphisms
with objects in a suitable category [Kelly, 1982]. It has been used to better understand a wide range
of domains, from metric spaces [Lawvere, 1973] to language [Bradley et al., 2022].
Let us dive into the definition of an enriched category:
Definition 18. A category C= (Obj ,Hom,◦,id)enriched in a monoidal category (V,⊗, I)consists
of
a collection Obj of objects,
ahom-object Hom( A, B)∈ObjVbetween objects,
a composition morphism ◦: Hom( B, C)⊗Hom( A, B)→Hom( A, C)for each triple of objects,
and
an identity element idA:I→Hom( A, A)for each object,
subject to associativity and identity.
Comparing Definitions 14 and 18, we can say that a (locally small) category is a category enriched in
the category Setof sets and functions. Enrichment is a way to describe the additional structures of
morphisms and the properties that need to be respected by composition.
25An example is a preorder, which can be seen as a category enriched in the category of boolean values.
Another example is a Lawvere metric space [Lawvere, 1973], which is a set with a premetric that
satisfies the triangle inequality. Note that the transitivity of a preorder and the triangle inequality of a
Lawvere metric are described by their composition morphisms, respectively.
In this work, we use enrichment to describe the association of a set of morphisms with additional
operations, such as a strict premetric and aggregators.
B Theory
In this section, we detail the theory of converting logical definitions into their corresponding
quantitative metrics based on elementary topos theory and enriched category theory.
B.1 Subobject quantifier and quantizer
Since our main goal is to develop a multi-valued (possibly continuous and differentiable)
quantification of properties defined by a certain type of logic, we begin with a category Ethat
has sufficient structures to allow the desired logical operations and build the quantification upon these
structures.
Firstly, recall that a subobject classifier Ω, if exists, is the representing object of the subobject functor
SubE, such that a subobject b:B↣Ccorresponds to a unique classifying morphism χb:C→Ω,
and an external operation on the set SubE(C)of subobjects that is natural in the object Ccorresponds
to an internal operation .
For example, the intersection
∩C: SubE(C)×SubE(C)→SubE(C) (33)
corresponds a natural transformation
HomE(−,Ω)×HomE(−,Ω)⇒HomE(−,Ω), (34)
which, because the hom-functor HomEpreserves limits, is isomorphic to a natural transformation
between hom-functors
HomE(−,Ω×Ω)⇒HomE(−,Ω), (35)
which, by the Yoneda lemma, is isomorphic to an internal operation on the subobject classifier Ω:
∧: Ω×Ω→Ω. (36)
Note that the subobject classifier, the classifying morphisms, and those internal operations are
determined uniquely up to isomorphism. However, in order to obtain a multi-valued quantification,
the requirement for uniqueness might be too restrictive. Thus, we propose to study a weakened
concept instead:
Definition 19. In a finitely complete category, a subobject quantifier is a subobject o: 1↣Ψsuch
that for every subobject b:B↣C, there exists at least one morphism ϕb:C→Ψsuch that bis
a pullback of oalong ϕb. The morphism ϕbis called a quantifying morphism ofb. If the category
has a subobject classifier ⊤: 1↣Ω, the quantizer κ: Ψ→Ωof the subobject quantifier ois the
classifying morphism of o.
B 1 1
C Ψ Ωb⌟
o⌟
⊤
ϕb
χbκ(37)
More succinctly, we can state that (cf. Proposition 5)
Proposition 6. A subobject quantifier is a weakly terminal object in the category of monomorphisms
and pullbacks.
26Thus, there is a unique morphism χb:C→Ωclassifying a subobject b:B↣Cof an object C,
but there could be multiple morphisms ϕb:C→Ψquantifying this subobject.
It is provable that the domain of a terminal object ⊤in the category of monomorphisms and pullbacks
(Proposition 5) must be a terminal object 1in the category E, but not all weakly terminal objects
in the category of monomorphisms and pullbacks (Proposition 6) are monomorphisms out of a
terminal object. We choose Definition 19 because we want only one global element o: 1↣Ψto be
designated to the truth value ⊤: 1↣Ω.
Here, we give two examples to motivate this definition of subobject quantifier. One is related to
three-valued logic [Bergmann, 2008, Fong and Spivak, 2019, Exercise 2.34]:
Example 9. InSet, the function
yes:{∗} → { no,maybe ,yes}, (38)
which maps the element ∗in a singleton set {∗}(a terminal object in Set) to an element yes in
a three-element set {no,maybe ,yes}is a subobject quantifier. For any subset Bof a set C, a
quantifying morphism ϕb:C→Ψis a function that maps all elements in the subset Btoyes and
all other elements to either maybe orno.
The other is related to metric spaces [Lawvere, 1973] and will be our running example in the following
subsections:
Example 10. InSet, the function 0 :{∗} → [0,∞]selecting the number 0out of the set [0,∞]of
extended non-negative real numbers is a subobject quantifier. The quantizer is a function
κ: [0,∞]→ {⊤ ,⊥}:=n7→⊤n= 0,
⊥n >0,(39)
which maps 0to⊤and any non-zero number to ⊥.
Intuitively, with a subobject quantifier, there is only one way to be true, but there may be many ways
to be false. In Set, a quantizer κmaps multiple “ degrees of truth ” from a potentially large, even
infinite set Ψto a smaller set Ωof truth values, hence the name.
Next, we define the counterpart of the concept of predicate (Definition 16):
Definition 20. In a category with a subobject quantifier o: 1↣Ψ, aquantity on an object Cis a
morphism q:C→Ψ.
Since we weakened the requirement for uniqueness, there is no one-to-one correspondence between
subobjects and quantities. However, they are still related as follows:
Lemma 7. In a category with a subobject classifier ⊤: 1↣Ω, a subobject quantifier o: 1↣Ψ,
and a quantizer κ: Ψ→Ω, a quantity q:C→Ψon an object Cis a quantifying morphism of a
subobject q∗oof the object C, which is isomorphic to a subobject (κ◦q)∗⊤.
Proof. q∗oand(κ◦q)∗⊤are both subobjects of Cbecause pullbacks preserve subobjects. Their
isomorphism follows from the pullback lemma.
Lemma 8. In a category with a subobject classifier ⊤: 1↣Ω, a subobject quantifier o: 1↣Ψ,
and a quantizer κ: Ψ→Ω, a quantity q:C→Ψon an object Cis a quantifying morphism of a
subobject b:B↣Cif and only if κ◦q=χb, where χbis the classifying morphism of the subobject
b.
Proof. Necessity follows from the pullback lemma and the uniqueness of the classifying morphism;
sufficiency follows from Lemma 7.
The following relationship between a quantity and the quantizer is also useful:
Lemma 9. In a category with a subobject classifier ⊤: 1↣Ω, a subobject quantifier o: 1↣Ψ,
and a quantizer κ: Ψ→Ω, for any quantity q:C→Ψon an object C,q=oCif and only if
κ◦q=κ◦oC=⊤C, where oCis the constant morphism o◦eC:CeC− − →1o− →Ψwith value
o: 1→Ψ.
Proof. This is due to the universal property of pullback oof⊤along κ.
We can see that the hom-functor on the quantizer HomE(−, κ) : Hom E(−,Ψ)⇒HomE(−,Ω)is
a natural transformation which maps the quantities HomE(C,Ψ)on an object Cto the predicates
HomE(C,Ω)onC, which are precisely subobjects of C.
27B.2 Equality and premetric
Next, we take a closer look at a concrete and important predicate — equality — and its corresponding
quantities.
Definition 21. In a category with a subobject classifier ⊤: 1→Ω, the equality predicate
=C:C×C→Ωon an object Cis the classifying morphism of the diagonal morphism
∆C:C↣C×C:=⟨idC,idC⟩.
By Lemma 8, a quantity dC:C×C→Ψis a quantifying morphism of ∆Cif and only if
κ◦dC= = C, depicted in the following diagram:
C 1 1
C×C Ψ Ω∆C⌟
o⌟
⊤
dC
=Cκ(40)
InSet, we have the following definitions:
Definition 22. Apremetric on a set Cis a binary function dC:C×C→[0,∞]such that
∀c∈C. d C(c, c) = 0 . (41)
Or equivalently, dC◦∆C= 0C, depicted in the following diagram:
C {∗}
C×C [0,∞]∆C 0
dC(42)
Definition 23. Astrict premetric on a set Cis a premetric dC:C×C→[0,∞]such that
∀c1∈C.∀c2∈C.(dC(c1, c2) = 0) →(c1=Cc2). (43)
Or equivalently, ∆Cis a pullback of 0along dC:
C {∗}
C×C [0,∞]∆C⌟
0
dC(44)
In other words, strict premetrics are precisely quantifying morphisms of the diagonal morphism ∆C
in the category Setwith0 :{∗} → [0,∞]as a subobject quantifier.
Note that the symmetry
∀c1∈C.∀c2∈C. d C(c1, c2) =dC(c2, c1) (45)
and the triangle inequality
∀c1∈C.∀c2∈C.∀c3∈C. d C(c1, c2) +dC(c2, c3)≥dC(c1, c3), (46)
which make dCametric , are not required. The addition +and the order ≥on the set [0,∞]are not
needed to define a strict premetric. However, they are necessary for defining other operations and
properties, which will be discussed in the next subsection.
B.3 Preorder
There is a preorder ⊆Cof inclusion defined on the set SubE(C)of subobjects of an object C: for two
subobjects a:A↣Candb:B↣C,a⊆Cbif and only if there exists a morphism f:A→B
such that a=b◦f:
A B
Cf
a b(47)
This preorder on the subobjects SubE(C)induces a preorder on the predicates HomE(C,Ω)via the
isomorphism. We can generalize this construction and define a preorder on other hom-sets:
28Definition 24. In a category Ewith pullbacks, the inclusion preorder ⊆Con the set SubE(C)of
subobjects of an object Cand a subobject m:S↣Tinduce a preorder ⪯m
Con the hom-set
HomE(C, T)via pullback of m: for any two morphisms f1, f2:C→T,f1⪯m
Cf2if and only if
f∗
1m⊆Cf∗
2m.
f∗S S
C Tf∗m⌟m
f(48)
Based on this definition, we can explore the preorders on any hom-sets. From now, we assume that E
is a category with necessary structures that we need.
Then, the preorder of predicates is ⪯⊤
ConHomE(C,Ω), and the preorder of quantities is ⪯o
Con
HomE(C,Ψ). By Lemma 7, we know that for two quantities q1, q2:C→Ψ,q1⪯o
Cq2if and
only if (κ◦q1)⪯⊤
C(κ◦q2), which means that HomE(C, κ)is an order-preserving function from
(Hom E(C,Ψ),⪯o
C)to(Hom E(C,Ω),⪯⊤
C).
Next, we will explore the structures of HomE(C,Ω)andHomE(C,Ψ). To begin with, ⊤Cis a top
inHomE(C,Ω), and oCis a top in HomE(C,Ψ), because idCis a top in SubE(C).
C 1 1
C Ψ ΩidC⌟
o⌟
⊤
oC
⊤Cκ(49)
The inclusion preorder on SubE(C)also has a bottom — the initial morphism iC: 0↣Cfrom an
initial object 0in the category Eto the object C. Then, its classifying morphism ⊥C:C→Ωis
a bottom in HomE(C,Ω). It can be proven that ⊥Cis a constant morphism with value ⊥: 1→Ω,
which is the classifying morphism of the initial/terminal morphism 0↣1. Any quantifying morphism
ψC:C→ΨofiCis a bottom in HomE(C,Ψ), but it is not necessarily a constant morphism.
0 1 1
C Ψ ΩiC⌟
o⌟
⊤
ψC
⊥Cκ(50)
The preorder on the global elements plays a special role:
Example 11. InSet,⪯⊤
1, also denoted by ⊢, is a preorder on the set {⊤,⊥}with only one non-
identity relation ⊥ ⊢ ⊤ .
Example 12. InSet,⪯0
1is a preorder on the set [0,∞]where n⪯0
10for any number n, andm⪯0
1n
andn⪯0
1mfor any positive numbers mandn.
By definition, ({⊤,⊥},⊢)and([0,∞],⪯0
1)are equivalent. However, we can consider a suborder of
([0,∞],⪯0
1), e.g., the usual “ greater than or equal to ”≥total order, to further differentiate positive
numbers. Note that 0remains the top in this suborder ([0,∞],≥).
B.4 Operation: algebra over the product endofunctor
In an elementary topos E, the subobjects SubE(C)of an object Cnot only forms a preorder by
inclusion but also are equipped with certain set operations (e.g., intersection and disjoint union),
which are defined in terms of the universal properties of their corresponding order operations (e.g.,
meet and join). Further, these operations are reflected in the structures of the subobject classifier (e.g.,
conjunction and disjunction).
Here, we establish a link between the structures of the subobject classifier and those of a subobject
quantifier. Our primary result is as follows:
29Theorem 10. Consider a category with a subobject classifier ⊤: 1↣Ω, a subobject quantifier
o: 1↣Ψ, and a quantizer κ: Ψ→Ω.
Letnbe a natural number. Let β: Ωn→Ωbe an n-ary logical operation on Ω, and let α: Ψn→Ψ
be an n-ary quantitative operation on Ψ.
Fori∈ {1, . . . , n }, letpi:C→Ωbe a predicate on an object C, and let qi:C→Ψbe a quantity
on the object Csuch that pi=κ◦qi. Let p=⟨p1, . . . , p n⟩be the tupling of the predicates, and
letq=⟨q1, . . . , q n⟩be the tupling of the quantities. Let b:B↣C:= (β◦p)∗⊤be the subobject
classified by β◦p, and let a:A↣C:= (α◦q)∗obe the subobject quantified by α◦q.
A α∗1 1
B β∗1 1
C ΨnΨ
C ΩnΩa⌟
α∗o⌟
o
b⌟ ⌟
q
idCα
κn κ
p ββ∗⊤ ⊤(51)
Then, we have
(i)κn◦q=p
(ii)(β◦κn◦α∗o=⊤α∗1)→((β◦p)◦a=⊤A)
(iii)(β◦κn=κ◦α)→(β◦κn◦α∗o=⊤α∗1)
(iv)(β◦κn=κ◦α)→(κ◦(α◦q) =β◦p)
(v)(β◦κn=κ◦α)→((α◦q)◦b=oB)
For convenience, we call an n-ary operation α: Ψn→Ψ(as an algebra over the product endofunctor
(−)n)homomorphic toβ: Ωn→Ωvia a morphism κ: Ψ→Ωif
β◦κn=κ◦α (52)
andsubhomomorphic toβ: Ωn→Ωif it satisfies the condition
β◦κn◦α∗o=⊤α∗1. (53)
Proof. (i) follows from the property of tupling and product: κn◦q=⟨κ◦q1, . . . , κ ◦qn⟩=
⟨p1, . . . , p n⟩=p.
(ii) states that if αis subhomomorphic to β, then β◦pis the classifying morphism of the subobject
quantified by α◦q.
β◦p◦a (54)
=β◦κn◦q◦a (i) (55)
=β◦κn◦α∗o◦(α∗o)∗q (pullback) (56)
=⊤α∗1◦(α∗o)∗q (subhomomorphism) (57)
=⊤A (composition) (58)
(iii) means that αbeing homomorphic to βis a stronger condition than being merely subhomomorphic
toβ.
β◦κn◦α∗o (59)
=κ◦α◦α∗o (homomorphism) (60)
=κ◦o◦o∗α (pullback) (61)
=⊤α∗1 (composition) (62)
(iv) shows the relationship between the predicate β◦pand the quantity α◦qwhen αis homomorphic
toβ.
κ◦α◦q (63)
=β◦κn◦q (homomorphism) (64)
=β◦p (i) (65)
300 1 2 3 4 5
n012345„pnqrn“0s
1´n
1
n
e´nFigure 3: Negation
(v) means that if αis homomorphic to β, then α◦qis a quantifying morphism of b.
κ◦α◦q◦b (66)
=β◦p◦b (iv) (67)
=⊤B (pullback) (68)
α◦q◦b=oBfollows from Lemma 9.
In summary, if αis subhomomorphic to β, then ais included in b((ii)); if αis homomorphic to β, then
aandbare isomorphic ((iii) and (v)). We consider this weaker condition because subhomomorphic
but non-homomorphic operations may exhibit favorable properties in other aspects, such as continuity.
We will discuss several concrete examples in the following subsections.
B.5 Negation
First, let us take a closer look at a unary logical operation — negation ¬: Ω→Ω, which is defined
as the classifying morphism of ⊥: 1→Ω. Recall that ⊥is the classifying morphism of 0↣1.
Let us consider a unary quantitative operation ∼: Ψ→Ψ. If∼is homomorphic to ¬via the
quantizer κ, it means that ¬ ◦κ=κ◦ ∼, or the following diagram commutes:
Ψ Ψ
Ω Ω∼
κ κ
¬(69)
Let us consider the set [0,∞]inSet. A quantitative operation ∼: [0,∞]→[0,∞]homomorphic to
the negation ¬:{⊤,⊥} → {⊤ ,⊥}is a function
∼(n):= [n= 0]×n0=n0n= 0,
0n >0,(70)
which maps 0to a non-zero number n0and any non-zero number to 0.11However, this function is
discontinuous at 0. On the other hand, if we consider a quantitative operation ∼subhomomorphic
to the negation ¬, then the only requirement is that for all n,∼(n) = 0 implies n > 0, or, by
contraposition, ∼(0)>0. Continuous choices include the hinge function n7→1´n= max {1−n,0},
reciprocal function n7→1
n, and exponential decay function n7→e−n(see Fig. 3). Note that the latter
11[−] :{⊤,⊥} → [0,∞]:=(
⊥ 7→ 0,
⊤ 7→ 1.
31Figure 4: Conjunction
 Figure 5: Disjunction
Figure 6: Implication
 Figure 7: Equivalence
two are actually homomorphic to the constant false ⊥because their outputs are always non-zero. The
hinge function 1´q, as discussed later, can be seen as derived from the implication p→ ⊥ .
In this way, if we have a quantity qfor a predicate p, we can obtain a quantity ∼qfor the negation ¬p
of the predicate as well. If the quantitative operation ∼is subhomomorphic but not homomorphic
to the logical operation ¬, we can guarantee that for any x,∼q(x) = 0 implies ¬p(x) =⊤, but not
vice versa.
B.6 Conjunction
Next, let us move on to an important binary logical operation — conjunction ∧: Ω×Ω→Ω, which
is defined as the classifying morphism of ⟨⊤,⊤⟩: 1↣Ω×Ω. Similarly, we consider a binary
quantitative operation ⊗: Ψ×Ψ→Ψhomomorphic to the conjunction ∧via the quantizer κ:
Ψ×Ψ Ψ
Ω×Ω Ω⊗
κ×κ κ
∧(71)
32By abuse of notation, the conjunction ∧also denotes a binary operation on the set HomE(C,Ω)of
predicates, such that for any two predicates p1, p2∈HomE(C,Ω),
p1∧p2:=∧ ◦ ⟨p1, p2⟩. (72)
The same goes for the quantitative operation ⊗.
For the conjunction, we can prove a stronger result:
Theorem 11. Consider a category with a subobject classifier ⊤: 1↣Ω, a subobject quantifier
o: 1↣Ψ, and a quantizer κ: Ψ→Ω.
Let the conjunction ∧: Ω×Ω→Ωbe the classifying morphism of ⟨⊤,⊤⟩: 1↣Ω×Ω, and let
⊗: Ψ×Ψ→Ψbe a binary operation on Ψhomomorphic to the conjunction ∧via the quantizer κ.
Letp1, p2:C→Ωbe two predicates on an object C, and let q1, q2:C→Ψbe two quantities on
the object Csuch that p1=κ◦q1andp2=κ◦q2. Letp=⟨p1, p2⟩be the pairing of the predicates,
and let q=⟨q1, q2⟩be the pairing of the quantities. Let b:B↣C:= (p1∧p2)∗⊤be the subobject
classified by p1∧p2.
B 1 1
C Ψ×Ψ Ψ
C Ω×Ω Ωb⌟
⟨o,o⟩⌟
o
⟨q1,q2⟩
idC⊗
κ×κ κ
⟨p1,p2⟩ ∧(73)
Then,⊗is a quantifying morphism of ⟨o, o⟩, and bis a pullback of ⟨o, o⟩along⟨q1, q2⟩.
Proof. Ifq1⊗q2=oC, then (κ◦q1)∧(κ◦q2) =⊤C, which leads to ⟨κ◦q1, κ◦q2⟩=⟨⊤,⊤⟩◦eC=
⟨⊤C,⊤C⟩due to the universal property of pullback. Then, we have κ◦q1=κ◦q2=⊤Cdue
to the universal property of pairing, and consequently q1=q2=oCaccording to Lemma 9, i.e.,
⟨q1, q2⟩=⟨o, o⟩ ◦eC. Therefore, ⟨o, o⟩is a pullback of oalong⊗. According to Theorem 10, bis
a pullback of oalong q1⊗q2. Then, following the pullback lemma, bis a pullback of ⟨o, o⟩along
⟨q1, q2⟩.
In other words, the pullback square symbols in Eq. (73) are unambiguous — the top row, the left
column, the right column, the top-left square, and the top-right square are all pullbacks.
Note that for any quantities a, b, c :C→Ψ, we have
κ◦((a⊗b)⊗c) =κ◦(a⊗(b⊗c)), (74)
κ◦(a⊗b) =κ◦(b⊗a), (75)
κ◦(oC⊗a) =κ◦a, (76)
due to the associativity, commutativity, and unitality of the conjunction, but the quantitative operation
⊗is not required to satisfy these properties, i.e., it is possible that
(a⊗b)⊗c̸=a⊗(b⊗c), (77)
a⊗b̸=b⊗a, (78)
oC⊗a̸=a. (79)
However, in the following examples, we mainly consider quantitative operations ⊗such that these
properties are satisfied. In such cases, (Hom E(C,Ψ),⊗, oC)forms a commutative monoid, and
consequently HomE(C, κ)is a monoid homomorphism from it to (Hom E(C,Ω),∧,⊤C).
In Appendix B.3, we introduced preorder structures on the predicates HomE(C,Ω)and the quantities
HomE(C,Ψ). The conjunction ∧is a commutative monoidal structure compatible with the preorder
(Hom E(C,Ω),⪯⊤
C)because it is the meet operation. For the set HomE(C,Ψ)of quantities, we can
choose the quantitative operation ⊗to be the meet operation as well. Alternatively, we can only
require it to be compatible with the preorder, in the sense that the monoid product is order-preserving:
for any quantities q1, q′
1, q2, q′
2∈HomE(C,Ψ), ifq1⪯o
Cq′
1andq2⪯o
Cq′
2, then q1⊗q2⪯o
Cq′
1⊗q′
2.
In other words, we require (Hom E(C,Ψ),⪯o
C,⊗, oC)to be a symmetric monoidal preorder [Fong
and Spivak, 2019, Definition 2.2].
33Example 13. Let us consider two commutative monoidal structures on the preorder ([0,∞],≥). The
max operation max : [0 ,∞]×[0,∞]→[0,∞]is the meet, i.e., cartesian product, while the addition
+ : [0 ,∞]×[0,∞]→[0,∞]is a monoidal product. They are both semicartesian because the top 0
is the unit.
Note that ([0,∞],+,0),([0,1],×,1), and ([1,∞],×,1)are isomorphic to each other with the
following isomorphisms:
([0,∞],+,0)
([0,1],×,1) ([1,∞],×,1)exp(−x)exp(
x)
1
x−log(x)
1
xlog(
x)(80)
We can also induce a monoidal structure on a set if it is isomorphic to a monoid:
Lemma 12. Letf:A⇄B:gbe a pair of bijections between sets AandB. If(B,⊗B, IB)is a
monoid, then (A,⊗A:=g◦ ⊗B◦(f×f), IA:=g◦IB)is also a monoid.
Proof. Associativity:
(a⊗Ab)⊗Ac (81)
=g((f(a)⊗Bf(b))⊗Bf(c)) (82)
=g(f(a)⊗B(f(b)⊗Bf(c))) (83)
=a⊗A(b⊗Ac) (84)
Left unitality:
IA⊗Aa (85)
=g(IB)⊗Aa (86)
=g(f(g(IB))⊗Bf(a)) (87)
=g(IB⊗Bf(a)) (88)
=g(f(a)) (89)
=a (90)
Right unitality can be proven similarly.
In this way, we can obtain a richer choice of monoidal structures on the set [0,∞]beyond the addition
(see Fig. 4):
Example 14 (Semicartesian monoidal product) .
ex−1 : ([0 ,∞],⊗,0)⇄([0,∞],+,0) : log(1 + x) a⊗b:= log( ea+eb−1) (91)
x2: ([0,∞],⊗,0)⇄([0,∞],+,0) :√x a ⊗b:=q
a2+b2(92)
√x: ([0,∞],⊗,0)⇄([0,∞],+,0) :x2a⊗b:=a+b+ 2√
ab (93)
x+ 1 : ([0 ,∞],⊗,0)⇄([1,∞],×,1) :x−1 a⊗b:=a+b+ab (94)
In summary, the max operation on [0,∞]can be regarded as a continuous conjunction , whereas a
semicartesian monoidal product, such as the addition, can be viewed as a soft max .
B.7 Disjunction
Dually, the disjunction ∨: Ω×Ω→Ωreflects the join of the inclusion preorder of subobjects.
Similarly, we want to find a quantitative operation ⊕: Ψ×Ψ→Ψhomomorphic to the disjunction
∨via the quantizer κ:
Ψ×Ψ Ψ
Ω×Ω Ω⊕
κ×κ κ
∨(95)
Using the same technique as in Lemma 12, we can obtain several monoidal structures on the set
[0,∞]homomorphic to the disjunction (see Fig. 5):
34Example 15 (Semicocartesian monoidal product) .
1
x: ([0,∞],⊕,∞)⇄([0,∞],+,0) :1
xa⊕b:=ab
a+b(96)
1
x2: ([0,∞],⊕,∞)⇄([0,∞],+,0) :1√xa⊕b:=abp
a2+b2(97)
1−e−x: ([0,∞],⊕,∞)⇄([0,1],×,1) :−log(1−x)a⊕b:=−log(e−a+e−b−e−(a+b))(98)
tanh : ([0 ,∞],⊕,∞)⇄([0,1],×,1) : arctanh a⊕b:= arctanh(tanh( a) tanh( b)) (99)
However, we usually choose the quantitative operation ⊕to be the join operation min of the preorder
([0,∞],≥). In this way, ⊗distributes over ⊕, i.e., for any quantities a, b, c :C→Ψ, we have
a⊗(b⊕c) = (a⊗b)⊕(a⊗c). (100)
A typical example is the min-plus semiring ([0,∞],min,+)[Pin, 1998].
B.8 Implication
Finally, let us construct a quantitative counterpart of the implication →: Ω×Ω→Ω, which is right
adjoint to the conjunction ∧. For global elements a, b, c : 1→Ω, this means that
c∧a⊢bif and only if c⊢a→b. (101)
There are two ways to construct a quantitative operation ⊸: Ψ×Ψ→Ψcorresponding to the
implication →. One way is to find a quantitative operation ⊸homomorphic to the implication →
via the quantizer κ:
Ψ×Ψ Ψ
Ω×Ω Ω⊸
κ×κ κ
→(102)
Example 16. For the set [0,∞], a quantitative operation ⊸: [0,∞]×[0,∞]→[0,∞]homomorphic
to the implication →:{⊤,⊥} × {⊤ ,⊥} → {⊤ ,⊥}is a function
(a, b)7→[a= 0]×[b >0]×f(b) =f(b)a= 0andb >0,
0 otherwise ,(103)
where f: (0,∞]→(0,∞]is an arbitrary function to non-zero numbers. Note that this function is
discontinuous at the line a= 0andb >0(cf. Appendix B.5).
The other way is to find a quantitative operation ⊸right adjoint to an operation ⊗homomorphic to
the conjunction ∧, i.e., the internal hom of the monoidal closed preorder [Fong and Spivak, 2019,
Definition 2.79].
Example 17. For the meet-semilattice ([0,∞],≥,max,0), the function
(a, b)7→[a < b ]×b=0a≥b,
b a < b,(104)
is right adjoint to the max because
max{c, a} ≥bif and only if c≥0a≥b,
b a < b.(105)
While this function is subhomomorphic to the implication, it is still discontinuous at the line a=b.
Example 18. For the monoidal preorder ([0,∞],≥,+,0), the truncated subtraction ´(a.k.a. monus
[Amer, 1984])
b´a:= max {b−a,0}=0 a≥b,
b−a a < b,(106)
35is right adjoint to the addition because
c+a≥bif and only if c≥b´a. (107)
The truncated subtraction is continuous and subhomomorphic to the implication. Note that the hinge
function n7→max{1−n,0}= 1´nfor the negation can be interpreted as the quantitative operation
derived from ¬n=n→ ⊥ , where 1is homomorphic to the constant false ⊥.
Similarly to Lemma 12, if two symmetric monoidal preorders are isomorphic and one of them is
closed, we can induce that the other is also closed:
Lemma 13. Letf:A⇄B:gbe a pair of isomorphisms between symmetric monoidal
preorders (A,⪯A,⊗A, IA)and(B,⪯B,⊗B, IB). If(B,⪯B,⊗B, IB,⊸B)is closed, then (A,⪯A
,⊗A, IA,⊸A:=g◦⊸B◦(f×f))is also closed.
Proof. For all a, b, c∈A, we have
c⪯A(a⊸Ab) (108)
≡c⪯Ag(f(a)⊸Bf(b)) (109)
≡f(c)⪯Bf(a)⊸Bf(b) (110)
≡f(c)⊗Bf(a)⪯Bf(b) (111)
≡c⊗Aa⪯Ab (112)
This means that ⊸Ais right adjoint to ⊗A.
In this way, we can find the internal homs corresponding to the monoidal products discussed in
Appendix B.6 (see Fig. 6).
As a side note, we can use the quantitative operations discussed above to define quantitative operations
for other logical connectives. For example, the logical equivalence a↔bcan be represented as
(a→b)∧(b→a)(bi-implication), (¬a∨b)∧(¬b∨a)(conjunctive normal form (CNF)), or
(a∧b)∨(¬a∧ ¬b)(disjunctive normal form (DNF)), and its quantitative operations can be defined
accordingly. Some examples are shown in Fig. 7.
B.9 Heyting algebra, quantale, and ordered semiring
Now, having constructed the logical operations and their corresponding quantitative operations, we
can compare the structures of the subobject classifier with those of a subobject quantifier.
It is known that the global elements of the subobject classifier with the logical operations form a
Heyting algebra (Ω,⊢,⊤,⊥,∧,∨,→):
Definition 25. AHeyting algebra is a cartesian closed bounded lattice.
In categorical terms, ⊤is the terminal object, ⊥is the initial object, ∧is the product, ∨is the
coproduct, and →is the exponential in the preorder (Ω,⊢)(as a thin category).
We weakened the requirements to construct the algebraic structures of the subobject quantifier, which
usually forms what is called a quantale (Ψ,⪯,1,0,⊗,⊕,⊸)[Mulvey, 1986, Dudzik, 2017]:
Definition 26. A (unital) quantale is a monoidal closed suplattice.
This means that we can consider a preorder (Ψ,⪯)on the subobject quantifier as a thin category,
where 1is the terminal object, 0is the initial object, ⊗is a monoidal product and not necessarily the
product, ⊕is still the coproduct, and ⊸is the internal hom right adjoint to the monoidal product
⊗. An example is the Lawvere quantale ([0,∞],≥,0,∞,+,min,´)[Lawvere, 1973, Bacci et al.,
2023]. Then, the quantizer κ: Ψ→Ωis a homomorphism preserving some or all the structures.
Note that due to the adjoint functor theorem and the fact that left adjoints preserve colimits, the
product distributes over the coproduct in a Heyting algebra, and the monoidal product distributes
over the coproduct in a quantale. We can further relax the requirement for ⊕to be the coproduct and
instead consider a monoidal product (Appendix B.7). If we still require the distributivity for the two
monoidal structures ⊗and⊕, the algebraic structure is an ordered semiring [Fujii, 2023]. Further
investigation is left for future work.
36B.10 Quantifier: algebra over the exponentiation endofunctor
Up to this point, our focus has been on n-ary operations in propositional logic (Appendix B.4). Next,
we introduce the universal quantification ∀and existential quantification ∃used in predicate logic
and their quantitative counterparts.
Externally, the universal quantification and the existential quantification are right and left adjoint
to the pullback of projection, respectively [Lawvere, 1969]. Internally, the universal quantifier
∀D: ΩD→Ωand existential quantifier ∃D: ΩD→Ωare given by morphisms from the power
object ΩDof an object Dto the subobject classifier Ω.
Recall that the power object ΩDis also an exponential object into the subobject classifier Ω, so
the universal quantifier ∀Dand existential quantifier ∃Dcan also be viewed as algebras over the
exponentiation endofunctor (−)Dof exponentiation on the subobject classifier Ω. Then, we can
consider algebras on the subobject quantifier Ψhomomorphic to them, which serve as quantitative
counterparts of these quantifiers.
Our main result is as follows (cf. Theorem 10):
Theorem 14. Consider an elementary topos with a subobject classifier ⊤: 1↣Ω, a subobject
quantifier o: 1↣Ψ, and a quantizer κ: Ψ→Ω.
Letp:C×D→Ωbe a predicate on a product, and let q:C×D→Ψbe a quantity such that
p=κ◦q. Letbp:C→ΩDandbq:C→ΨDbe their exponential transposes.
LetβD: ΩD→Ωbe a predicate, and let αD: ΨD→Ψbe a quantity homomorphic to βDvia the
quantizer κ, i.e., βD◦κD=κ◦αD.
Letb:B↣C:= (βD◦bp)∗⊤be the subobject classified by βD◦bp, and let a:A↣C:= (αD◦bq)∗o
be the subobject quantified by αD◦bq.
A 1
B 1
C ΨDΨ
C ΩDΩa⌟
o
b⌟
bq
idCαD
κD κ
bp βD⊤(113)
Then, we have
(i)κD◦bq=bp
(ii)κ◦(αD◦bq) =βD◦bp
(iii)(βD◦bp)◦a=⊤A
(iv)(αD◦bq)◦b=oB
Proof. (i) follows from the property of exponential.
(ii) shows the relationship between the predicate βD◦bpand the quantity αD◦bqwhen αDis
homomorphic to βD.
κ◦αD◦bq (114)
=βD◦κD◦bq (homomorphism) (115)
=βD◦bp (i) (116)
(iii) means that βD◦bpis a classifying morphism of a.
βD◦bp◦a (117)
=κ◦αD◦bq◦a (ii) (118)
=κ◦oA (pullback) (119)
=⊤A (composition) (120)
37(iv) means that αD◦bqis a quantifying morphism of b.
κ◦αD◦bq◦b (121)
=βD◦bp◦b (ii) (122)
=⊤B (pullback) (123)
αD◦bq◦b=oBfollows from Lemma 9.
Definition 27 (Universal aggregator) .Auniversal aggregator ▽D: ΨD→Ψis a quantity that is
homomorphic to the universal quantifier ∀D: ΩD→Ω:
ΨDΨ
ΩDΩ▽D
κDκ
∀D(124)
Definition 28 (Existential aggregator) .Anexistential aggregator △D: ΨD→Ψis a quantity that is
homomorphic to the existential quantifier ∃D: ΩD→Ω:
ΨDΨ
ΩDΩ△D
κDκ
∃D(125)
Example 19. For the set [0,∞], the canonical choices of universal aggregator ▽Dand existential
aggregator △Daresupandinf. If the set Dis finite, we can also use sum andmean as the
universal aggregator. Non-examples of universal aggregator include median andmode , which are
not homomorphic to the universal quantifier.
Note that the universal quantifier and existential quantifier are commutative up to isomorphism:
∀A×B∼=∀B◦ ∀B
A∼=∀A◦ ∀A
B∼=∀B×A, (126)
∃A×B∼=∃B◦ ∃B
A∼=∃A◦ ∃A
B∼=∃B×A. (127)
However, we are free to choose different aggregators for different objects that are not necessarily
commutative. For example, the sum of max is usually not equal to the max of sum.
B.11 Enrichment
Lastly, we describe the conversion based on enrichment.
First, let us define the enriching category:
Definition 29. Let(Ψ,⪯,⊗,⊕,⊸)be an internal quantale object in Set. We define Ψ-Setto be a
category whose objects are tuples consisting of a set C, aΨ-valued strict premetric dConC, and
universal and existential aggregators on C:
(C, dC:C×C→Ψ,▽C,△C: ΨC→Ψ), (128)
and morphisms from (A, dA,▽A,△A)to(B, d B,▽B,△B)are functions f:A→B.
Definition 30. A bifunctor ⊠onΨ-Set is given by the Cartesian product of sets and functions,
together with the following products of strict premetrics and aggregators:
dA⊠dB:(A×B)×(A×B)∼=(A×A)×(B×B)dA×dB− − − − − → Ψ×Ψ⊗− →Ψ. (129)
▽A⊠▽B:ΨA×B∼=(ΨA)B▽B
A− − →ΨB▽B− − →Ψ, (130)
△A⊠△B:ΨA×B∼=(ΨA)B△B
A− − →ΨB△B− − →Ψ. (131)
Proposition 15. The product dA⊠dBof strict premetrics given in Definition 30 is again a strict
premetric.
38Proof. This is a result of Theorem 11. Consider the following diagram:
A×B A×B 1 1
(A×B)2A2×B2Ψ×Ψ ΨidA×B
∆A×B⌟
∆A×∆B⌟
⟨o,o⟩⌟
o
∼= dA×dB⊗(132)
∆A×∆Bis a pullback of oalong⊗◦(dA×dB)because ⟨o, o⟩is a pullback of oalong⊗according
to Theorem 11, ∆A×∆Bis a pullback of ⟨o, o⟩along dA×dB, and we can apply the pullback
lemma. ∆A×∆Bis isomorphic to ∆A×B, which means that ⊗◦(dA×dB)is a strict premetric.
Based on this definition, we can show that
(dA⊠dB)⊠dC∼=dA⊠(dB⊠dC), (133)
because ×and⊗are associative up to isomorphism.
Further, we have
(▽A⊠▽B)⊠ ▽C∼=▽A⊠(▽B⊠▽C), (134)
(△A⊠△B)⊠ △C∼=△A⊠(△B⊠△C), (135)
because composition is associative.
The singleton ({∗}, o{∗}×{∗} ,id{∗},id{∗})is the unit of the bifunctor ⊠. In this way, (Ψ-Set,⊠,{∗})
forms a monoidal category.
However, note that the bifunctor ⊠is not symmetric because ▽A⊠▽Band△A⊠△Bare not
necessarily symmetric, and dA⊠dB∼=dB⊠dAif and only if the monoidal product ⊗of the quantale
Ψis commutative.
Since Ψ-Setis a monoidal category, we can consider a strict monoidal functor FΨ:Set→Ψ-Set,
which equips products of sets with product strict premetrics and product aggregators:
dA×B:=dA⊠dB, (136)
▽A×B:=▽A⊠▽B, (137)
△A×B:=△A⊠△B. (138)
Such a monoidal functor induces a functor from a ( Set-enriched) category to a Ψ-Set-enriched
category, called the base change of enriching category. This means that we have a systematic way to
equip a set [A, B]of morphisms with a strict premetric
d[A,B]: [A, B]×[A, B]→Ψ, (139)
a universal aggregator
▽[A,B]: Ψ[A,B]→Ψ, (140)
and an existential aggregator
△[A,B]: Ψ[A,B]→Ψ, (141)
which is compatible with the product.
Then, Theorem 1 is a special case of this enrichment. The relationship between predicates and
quantities follows from Theorems 10 and 14.
B.12 Summary
Finally, to accommodate readers without a background in category theory, we present the instantiated
definitions and theoretical results free of categorical terminology. The non-categorical proofs are
omitted. We will be using the following functions:
thezero predicate ζ: [0,∞]→ {⊤ ,⊥}:=x7→(x= 0) ,
theproduct ζn: [0,∞]n→ {⊤ ,⊥}n: (q1, . . . , q n)7→(q1= 0, . . . , q n= 0) , and
thepostcomposition ζA: [0,∞]A→ {⊤ ,⊥}A:=q7→ζ◦qof the zero predicate.
39Definition 31 (Quantity) .A quantity q:A→[0,∞]ishomomorphic to a predicate p:A→ {⊤ ,⊥}
ifp=ζ◦q:
∀a∈A.(q(a) = 0) ↔p(a). (142)
A quantity qissubhomomorphic to a predicate pifζ◦q→p:
∀a∈A.(q(a) = 0) →p(a). (143)
Example 20. Astrict premetric dA:A×A→[0,∞]is a quantity on the product set A×A
homomorphic to the equality predicate =A:A×A→ {⊤ ,⊥}.
Definition 32 (Quantitative operation) .Letn∈Nbe a natural number. A quantitative operation
α: [0,∞]n→[0,∞]ishomomorphic to a logical operation β:{⊤,⊥}n→ {⊤ ,⊥}via the zero
predicate ζifζ◦α=β◦ζn:
∀(q1, . . . , q n)∈[0,∞]n.(α(q1, . . . , q n) = 0) ↔β(q1= 0, . . . , q n= 0). (144)
A quantitative operation αissubhomomorphic to a logical operation βvia the zero predicate if
ζ◦α→β◦ζn:
∀(q1, . . . , q n)∈[0,∞]n.(α(q1, . . . , q n) = 0) →β(q1= 0, . . . , q n= 0). (145)
The relationship between quantitative operations and logical operations is as follows (Theorem 10):
Proposition 16. Letn∈Nbe a natural number. For i∈ {1, . . . , n }, letpi:A→ {⊤ ,⊥}be a
predicate, and let qi:A→[0,∞]be a quantity. Let p:A→ {⊤ ,⊥}n:=⟨p1, . . . , p n⟩be the
tupling of the predicates, and let q:A→[0,∞]n:=⟨q1, . . . , q n⟩be the tupling of the quantities.
Letα: [0,∞]n→[0,∞]be a quantitative operation, and let β:{⊤,⊥}n→ {⊤ ,⊥}be a logical
operation. Assume that for all i∈ {1, . . . , n },qiis homomorphic to pi. Then,
ifαis homomorphic to β,α◦qhomomorphic to β◦p; and
ifαis subhomomorphic to β,α◦qsubhomomorphic to β◦p.
In fact, we can also show that if for all i∈ {1, . . . , n },qiis subhomomorphic to pi, and αis
subhomomorphic to β, then α◦qis subhomomorphic to β◦p.
Definition 33 (Aggregator) .An aggregator αA: [0,∞]A→[0,∞]ishomomorphic to a quantifier
βA:{⊤,⊥}A→ {⊤ ,⊥}via the zero predicate ζifζ◦αA=βA◦ζA:
∀q∈[0,∞]A.
(α
a∈Aq(a)) = 0
↔
β
a∈A(q(a) = 0)
. (146)
Example 21. Auniversal aggregator ▽A: [0,∞]A→[0,∞]is a function such that
∀q∈[0,∞]A.
(▽
a∈Aq(a)) = 0
↔(∀a∈A. q(a) = 0) . (147)
Example 22. Anexistential aggregator △A: [0,∞]A→[0,∞]is a function such that
∀q∈[0,∞]A.
(△
a∈Aq(a)) = 0
↔(∃a∈A. q(a) = 0) . (148)
The relationship between aggregators and quantifiers is as follows (Theorem 14):
Proposition 17. Letp:A×B→ {⊤ ,⊥}be a predicate, and let q:A×B→[0,∞]be a
quantity. Let bp:B→ {⊤ ,⊥}Aandbq:B→[0,∞]Abe the exponential transposes of pandq. Let
αA: [0,∞]A→[0,∞]be an aggregator, and let βA:{⊤,⊥}A→ {⊤ ,⊥}be a quantifier. Then, if
qis homomorphic to p, and αis homomorphic to β, then αA◦bqis homomorphic to βA◦bp.
We have a compositional way to assign a strict premetric and an aggregator to a product of sets:
Proposition 18. LetdA:A×A→[0,∞]anddB:B×B→[0,∞]be strict premetrics. Then,
dA×B: (A×B)×(A×B)→[0,∞]:= ((a, b),(a′, b′))7→d(a, a′) +d(b, b′) (149)
is a strict premetric on the product set A×B.
Proposition 19. Let▽A: [0,∞]A→[0,∞]and▽B: [0,∞]B→[0,∞]be universal aggregators.
Then,
▽
A×B: [0,∞]A×B→[0,∞]:=q7→▽
b∈B▽
a∈Aq(a, b) (150)
is a universal aggregator on the product set A×B.
40C Proofs
C.1 Proposition 2
Proof.
qproduct (m:Y→Z) (151)
= inf
m1,1∈[Y1,Z1]inf
m2,2∈[Y2,Z2]d[Y,Z](m, m 1,1×m2,2) (152)
= inf
m1,1∈[Y1,Z1]inf
m2,2∈[Y2,Z2](d[Y,Z1](m1, m1,1◦p1) +d[Y,Z2](m2, m2,2◦p2)) (153)
= inf
m1,1∈[Y1,Z1]d[Y,Z1](m1, m1,1◦p1) + inf
m2,2∈[Y2,Z2]d[Y,Z2](m2, m2,2◦p2) (154)
= inf
m1,1∈[Y1,Z1]▽
y∈YdZ1(m1(y), m1,1(y1)) + inf
m2,2∈[Y2,Z2]▽
y∈YdZ2(m2(y), m2,2(y2)) (155)
= inf
m1,1∈[Y1,Z1]▽
y1∈Y1▽
y2∈Y2dZ1(m1(y1, y2), m1,1(y1))
+ inf
m2,2∈[Y2,Z2]▽
y2∈Y2▽
y1∈Y1dZ2(m2(y1, y2), m2,2(y2)) (156)
=▽
y1∈Y1▽
y2∈Y2dZ1(m1(y1, y2), m∗
1,1(y1))
+▽
y2∈Y2▽
y1∈Y1dZ2(m2(y1, y2), m∗
2,2(y2)), (157)
where
m∗
1,1:= arg inf
m1,1∈[Y1,Z1]▽
y1∈Y1▽
y2∈Y2dZ1(m1(y1, y2), m1,1(y1)) (158)
=y17→arg inf
z1∈Z1▽
y2∈Y2dZ1(m1(y1, y2), z1), (159)
m∗
2,2:= arg inf
m2,2∈[Y2,Z2]▽
y2∈Y2▽
y1∈Y1dZ2(m2(y1, y2), m2,2(y2)) (160)
=y27→arg inf
z2∈Z2▽
y1∈Y1dZ2(m2(y1, y2), z2). (161)
C.2 Proposition 3
Proof.
qconst-curry (m:Y→Z) (162)
=qconst(cm1) +qconst(cm2) (163)
=▽
y2∈Y2▽
y′
2∈Y2d[Y1,Z1](cm1(y2),cm1(y′
2)) +▽
y1∈Y1▽
y′
1∈Y1d[Y2,Z2](cm2(y1),cm2(y′
1)) (164)
=▽
y2∈Y2▽
y′
2∈Y2▽
y1∈Y1dZ1(cm1(y2)(y1),cm1(y′
2)(y1))
+▽
y1∈Y1▽
y′
1∈Y1▽
y2∈Y2dZ2(cm2(y1)(y2),cm2(y′
1)(y2)) (165)
=▽
y2∈Y2▽
y′
2∈Y2▽
y1∈Y1dZ1(m1(y1, y2), m1(y1, y′
2))
+▽
y1∈Y1▽
y′
1∈Y1▽
y2∈Y2dZ2(m2(y1, y2), m2(y′
1, y2)) (166)
=▽
y1∈Y1▽
y2∈Y2▽
y′
2∈Y2dZ1(m1(y1, y2), m1(y1, y′
2))
+▽
y2∈Y2▽
y1∈Y1▽
y′
1∈Y1dZ2(m2(y1, y2), m2(y′
1, y2)). (167)
41D Discussions
D.1 Background
Defining and measuring the properties of learning models is a core topic in machine learning,
especially representation learning [Bengio et al., 2013]. A proper comprehension of what constitutes
good representations and how to assess their quality is important for developing suitable learning
objectives and evaluation metrics. To define these properties, many important concepts are given
byequational predicates , such as independence of random variables, extensively used in statistical
learning and causal learning [Hyvärinen and Oja, 2000, Koller and Friedman, 2009, Schölkopf and
von Kügelgen, 2022], and equivariance of learning models, reflecting the symmetries and structures
of the data [Cohen and Welling, 2016, Zaheer et al., 2017, Higgins et al., 2018, Maron et al., 2019,
de Haan et al., 2020, Cohen, 2021, van der Pol et al., 2022, Navon et al., 2023].
Considerable efforts have been put into designing model architectures that perfectly satisfy specific
properties, such as monotonicity [Sill, 1997, Daniels and Velikova, 2010], invertibility [Rezende and
Mohamed, 2015, Behrmann et al., 2019, Ishikawa et al., 2023], convexity [Amos et al., 2017], and
equivariance [Lee et al., 2019, Brehmer et al., 2023]. However, hard-coding multiple properties
into a model by design could be challenging [Köhler et al., 2020]. Hence, it is desirable to devise
quantitative metrics to directly measure these properties, even if the models do not have the properties
built-in [Goodfellow et al., 2009, Chen et al., 2020, Kvinge et al., 2022]. Ideally, these metrics should
be easily computable or even differentiable, allowing us to directly optimize the properties.
Disentangled representation learning [Bengio et al., 2013], our main focus of this paper, is such a
field where defining and measuring the desired properties are not straightforward tasks [Carbonneau
et al., 2022, Zhang and Sugiyama, 2023]. It has been suggested that disentangling the underlying
explanatory factors in complex data is a promising approach for reliable, interpretable, generalizable,
and data-efficient representation learning [Locatello et al., 2019a,b, Montero et al., 2021, Dittadi
et al., 2021, Xu et al., 2022]. However, in contrast to the wealth of results regarding invariant and
equivariant layers, the exploration of designing a “ disentangled layer ” has been relatively limited.
One reason is that disentanglement was not considered a singular property but rather a combination of
several requirements. The absence of a clear definition and appropriate metrics for disentanglement
has created a gap between the learning objectives and evaluation metrics. A new evaluation metric is
often introduced along with a new representation learning method [Carbonneau et al., 2022], but it is
usually unproven that the method can optimize the new metric, and the metric truly quantifies the
alleged property [Higgins et al., 2017, Kim and Mnih, 2018, Chen et al., 2018, Li et al., 2020].
To formally define disentanglement, a line of research utilized group theory and representation theory
[Cohen and Welling, 2014, 2015, Higgins et al., 2018], with a focus on the direct product of groups .
Thanks to the rich algebraic structure, it becomes possible to derive various model architectures and
learning objectives from the equational requirements of the product and equivariance [Caselles-Dupré
et al., 2019, Pfau et al., 2020, Quessard et al., 2020, Painter et al., 2020, Miyato et al., 2022, Yang
et al., 2022, Tonnaer et al., 2022, Keurti et al., 2023]. Another approach adopted a topological
perspective, using concepts such as the product manifold to define disentanglement [Zhou et al., 2020,
Fumero et al., 2021, Zhang et al., 2021, Balabin et al., 2024]. However, theoretically comparing
different approaches has been a challenging task.
To quantitatively measure disentanglement, Ridgeway and Mozer [2018] proposed three concepts
called modularity, compactness, and explicitness , which were defined verbally but not mathematically.
Eastwood and Williams [2018] proposed similar three criteria called disentanglement, completeness,
and informativeness and corresponding evaluation metrics. However, it was unclear what properties
these metrics truly quantify. Additionally, due to the necessity for additional training of classifiers
along with hyperparameter tuning and the involvement of non-differentiable regressors such as
the random forest [Breiman et al., 1984], it is impossible to directly optimize these metrics using
gradient-based optimization. Recently, Eastwood et al. [2023] extended this framework with two new
metrics called explicitness/ease-of-use and size based on the functional capacity. Do and Tran [2020]
introduced metrics for informativeness, separability, independence, and interpretability from an
information-theoretic perspective, while Tokui and Sato [2022] introduced a new metric in terms of
uniqueness, redundancy, and synergy based on partial information decomposition. These metrics have
been mainly used during the evaluation stage, after a model is trained with other learning objectives.
42D.2 Related work
Equivariance The work by Kvinge et al. [2022] might be the closest to our approach in spirit. They
directly converted equivariance , an equational predicate, to a quantitative metric and analyzed their
relationship (Proposition 3.2). In contrast, based on our proposed conversion method, we can use the
following definition and metric:
Definition 34 (Equivariant function) .LetA,B, andCbe sets. A function f:A→Bisequivariant
to actions (any binary functions) ·A:C×A→Aand·B:C×B→Bif
pequivariant (f:A→B):=∀c∈C. f◦(c·A−) =[A,B](c·B−)◦f (168)
=∀c∈C.∀a∈A. f(c·Aa) =Bc·Bf(a), (169)
which can be measured by
qequivariant (f:A→B):=▽
c∈C▽
a∈AdB(f(c·Aa), c·Bf(a)). (170)
Calibration More broadly, the study of the relationship between different metrics in statistical
learning is called calibration analysis [Steinwart, 2007, Reid and Williamson, 2010, Ni et al., 2019,
Bao and Sugiyama, 2020, Bao et al., 2020]. Our work can be seen as an extension of the concept of
the calibration to a wider range of properties defined by equational predicates.
Disentanglement metric In disentangled representation learning, metrics similar to Eq. (26) have
been proposed by Higgins et al. [2017], Kim and Mnih [2018]. Their metrics also fix one factor and
vary all others and calculate some constancy metrics (the mean pairwise distance in Higgins et al.
[2017] and the variance in Kim and Mnih [2018]). However, both studies took an indirect approach,
involving the training of a classifier to predict the fixed factor. Consequently, the resulting metrics are
not differentiable anymore and entangle modularity and informativeness. In this work, we argue that
it is better to measure these two properties separately.
Weakly supervised disentanglement Ridgeway and Mozer [2018] proposed and investigated the
similarity supervision and argued that such supervision is easy to obtain via crowdsourcing. Shu
et al. [2020] further studied this type of supervision based on distribution matching and referred it
as match pairing. Other weaker forms of supervision were also investigated, such as the number of
changed factors [Locatello et al., 2020] or paired data with unknown intervention [Brehmer et al.,
2022]. Given that our theory can establish connections between logical definitions and quantitative
metrics, it holds promise for deriving disentanglement metrics for various types of weak supervision
based on logical inference.
Multi-valued logic Aristotelian logic assumes that every proposition is either true or false, adhering
to the principle of bivalence . The law of Aristotelian logic can be algebraically represented on the
set{0,1}of binary truth values [Boole, 1854], known as the two-element Boolean algebra . The
exploration of non-Aristotelian logic involves investigating logical systems that relax or modify this
strict binary valuation, allowing for a broader range of truth values and accommodating various
forms of uncertainty, vagueness, or context-dependence in reasoning [Hájek, 1998, Malinowski, 2007,
Bergmann, 2008].
The mathematical study of multi-valued logic can date back to the seminal work by Łukasiewicz in
1920, who introduced a third truth value interpreted as “ possibility ” and symbolized by1
2. Łukasiewicz
[1920] examined several principles in this three-valued logic such as the principles of identity,
implication, syllogism, and contradiction, and discussed its theoretical and practical importance in
indeterministic philosophy and deductive sciences. Later, Łukasiewicz and Tarski [1930] proposed
propositional calculus , a theory of propositions with values from the real interval [0,1], which is
now also commonly known as Łukasiewicz logic . Łukasiewicz logic involves new continuous logical
connectives such as strong/weak conjunction and disjunction.
Furthermore, Chang [1958] studied the algebraic systems for many-valued logic , called MV-algebras .
Chang and Keisler [1966] then proposed continuous model theory , also referred to as compact-valued
logic [Ben Yaacov, 2022], where the truth values can be in arbitrary compact Hausdorff spaces and
a wide variety of quantifiers was studied. Later, Ben Yaacov et al. [2008] studied model theory for
metric structures and proposed (real-valued) continuous first-order logic [Ben Yaacov and Usvyatsov,
432010], where the space of truth values is a closed, bounded interval of real numbers with the order
topology (e.g., [0,1]), and suggested that we only need two canonical quantifiers supandinf. From a
categorical perspective, Cho [2020] developed categorical semantics of metric spaces and continuous
logic by introducing the notion of continuous subobject classifier , and Figueroa and van den Berg
[2022] studied a topos of continuous logic using the notion of hyperdoctrine .
On the other hand, from a categorical perspective, Lawvere [1973] showed that a generalized metric
space, also known as a Lawvere metric space , is a category enriched over what is now commonly
called the Lawvere quantale ([0,∞],≥,+,0), i.e., the set [0,∞]of extended non-negative real
numbers equipped with addition +as a (semicartesian) monoidal product and truncated subtraction
´as the internal hom. In other words, a Lawvere metric space is a set Aequipped with a function
d:A×A→[0,∞]such that for all a∈A, we have 0≥d(a, a)ord(a, a) = 0 (identity), which
makes dapremetric , and for all a, b, c∈A, we have d(b, c)+d(a, b)≥d(a, c)(composition), which
means that dsatisfies the triangle inequality .
Recently, Mardare et al. [2016] took an equational approach to quantitative algebraic reasoning ,
which was later also referred to as quantitative equational logic [Mardare et al., 2021], by introducing
approximate equality predicates =εindexed by rational numbers ε(i.e.,a=εbifaandbare at most
εapart), and suggested that this approach essentially involves working with enriched Lawvere theory .
Dagnino and Pasquali [2022] provided a logical ground to quantitative reasoning in the categorical
language of Lawvere’s doctrines by viewing distances as equality predicates in linear logic . Bacci
et al. [2023] further studied the natural deduction systems of propositional logics for the Lawvere
quantale and introduced what was later called affine Lawvere logic , including Łukasiewicz logic and
Ben Yaacov’s continuous propositional logic. Bacci et al. [2024] extended affine Lawvere logic to
polynomial Lawvere logic by allowing multiplication as an extra logical connective. These studies
are on propositional logic and do not involve predicates and quantifiers. Recently, Capucci [2024]
studied a spectrum of quantifiers in [0,∞]-valued quantitative predicate logic.
In the context of machine learning, these relatively recently developed logics have yet to prove their
practical importance. While these innovative approaches often hold theoretical promise, they need to
demonstrate tangible benefits in real-world applications. Key areas where these new logics might
eventually make an impact include neuro-symbolic reasoning and logic/probabilistic programming
[d’Avila Garcez et al., 2002, Manhaeve et al., 2018, Sen et al., 2022, Badreddine et al., 2022, Fagin
et al., 2024], which hold promise for integrating low-level perception with high-level reasoning,
improving model interpretability, enhancing training efficiency, and enabling more robust decision-
making processes. However, widespread adoption and validation through practical use cases are
necessary to establish their true value and effectiveness in the machine learning landscape.
Under this background, let us contextualize our proposed methodology for deriving [0,∞]-valued
quantitative metrics from logical definitions. We highlight three characteristics of our framework:
We allowed not only metrics, but strict premetrics , such as the relative entropy (Kullback–Leibler
divergence) [Kullback and Leibler, 1951, Perrone, 2023] widely used in machine learning, as the
real-valued counterparts for the equality predicates;
We focused on whether the metrics are zero or not and the (differentiable) optimization of the
derived metrics, because our main goal is to guarantee that the minimizers of the derived metrics
satisfy the predicate;
We included a wide range of real-valued quantifiers (or aggregators in our terms) beyond supand
inf, such as mean and mean square, as long as they are homomorphic to the two-valued quantifiers,
because the derived metrics may have nicer properties or even analytical solutions.
44(a) homomorphic
 (b) adjoint to max
 (c) less coverage
 (d) more coverage
(e) negation, disjunction
 (f) min, hinge
 (g) min, reciprocal
 (h) fraction
Figure 8: Quantitative operations for implication
(a) bi-implication
 (b) CNF
 (c) DNF
 (d) fraction
Figure 9: Quantitative operations for equivalence
D.3 Implication and equivalence
In Section 3, we only used the truncated subtraction ´as a quantitative operation for the implication
→(Table 1). In Theorem 1, we noted that the implication is special because if a predicate involves
the implication, then not all elements satisfying the predicate minimize the corresponding quantities
(Fig. 2). In Appendix B.8, we discussed other possible quantitative operations ⊸corresponding to
the implication →.
In Fig. 8, we showed eight alternative quantitative operations for the implication. In the first row, the
first one is homomorphic to the implication, which means that its minimizers are exactly those that
satisfy the predicate (Example 16); the second one is right adjoint to the max (Example 17); and the
other two are variants of the truncated subtraction (Example 18), which have more or less coverage.
In the second row, we used the logical equivalence between a→band¬a∨bto define quantitative
operations for the implication using quantitative operations for the negation and disjunction. For
example, we can use the hinge function 1´nandmin, which lead to min{1´a, b}, or the reciprocal
function1
nandab
a+b, which lead to1
ab
1
a+b=b
1+ab.
Similarly, we can use logically equivalent expressions of the logical equivalence a↔b, such as
(a→b)∧(b→a)(bi-implication), (¬a∨b)∧(¬b∨a)(conjunctive normal form (CNF)), and
(a∧b)∨(¬a∧ ¬b)(disjunctive normal form (DNF)), to derive quantitative operations for the
equivalence, shown in Fig. 9.
45center median
mean
(a) central point
diameter
 (b) pairwise distance
Figure 10: Two approaches for measuring the constancy of a set in R2: (a) finding a central point,
such as the center of the smallest bounding sphere, the geometric median, or the mean, and then
measuring the dispersion around this point; and (b) aggregating pairwise distances between points.
Note that a quantitative operation homomorphic to the implication cannot be continuous everywhere,
which is undesirable for gradient-based optimization. For example, the following quantity also
measures the injectivity of a function m:Y→Z:
▽
y∈Y▽
y′∈Y[dZ(m(y), m(y′)) = 0] ×[dY(y, y′)>0]×dY(y, y′) (171)
=▽
y∈Y▽
y′∈Y[m(y) =Zm(y′)]×[y̸=Yy′]×dY(y, y′). (172)
This quantity aggregates distances between pairs of different inputs mapped to the same outputs.
However, unlike qinjective introduced in Section 4.3, it is not differentiable with respect to the function
m:Y→Z. Thus, we cannot use it to improve the injectivity of a function by gradient descent.
D.4 Constant function
Note that there are two logically equivalent definitions of a constant function (to a non-empty set).
One is based on the equality between all pairs, as in Definition 11. The other is based on the constant
output value (see Fig. 10):
Definition 35 (Constant function with value) .A function f:A→Bis aconstant function with
value b∈Bif
pconst-v(f:A→B):=∃b∈B.∀a∈A.(f(a) =Bb), (173)
which can be measured by
qconst-v(f:A→B):= inf
b∈B▽
a∈AdB(f(a), b). (174)
This quantity qconst-v finds a central point in the codomain that best approximates all the outputs of a
function, which is similar to the approach we discussed in Section 4.1. In fact, we can prove that if
we use qconst-v inqconst-curry , we will end up with the same quantity qproduct .
46(a) small radius, large variance
 (b) large radius, small variance
Figure 11: Metrics may rank imperfect representations differently.
Rad.7.62 MAD 3.97
 Rad.2.82 Var.1.93
 Rad. 2.78 Diam. 5.56
 Rad.3.63 MPD 2.69
 MAD 7.12 Var.49.02
 MAD 0.73 Diam. 6.55
 MAD 33.54 MPD 49.81
 Var.0.51 Diam. 6.89
 Var.15.82 MPD 6.12
 Diam. 5.50 MPD 2.44
Rad. 9.66 MAD 1.81
 Rad. 4.68 Var.0.85
 Rad.2.28 Diam. 4.56
 Rad. 7.75 MPD 1.42
 MAD 8.51 Var.46.15
 MAD 1.73 Diam. 4.86
 MAD 39.66 MPD 47.86
 Var.1.86 Diam. 5.63
 Var.16.83 MPD 5.11
 Diam. 7.19 MPD 0.94
Figure 12: For a pair of constancy metrics (each column), we can find two sets of points in R2ranked
differently by these metrics, except for the radius and diameter, because for a subset A0in a set A, we
haveinfa0∈Asupa∈A0dA(a0, a)≤infa0∈A0supa∈A0dA(a0, a)≤supa0∈A0supa∈A0dA(a0, a).
D.5 Rank of imperfect representations
It is worth noting that Theorem 1 only guarantees that the minimizers of different quantitative metrics
derived from the same logical definition are the same, but imperfect representations, whose evaluation
results are non-zero, may be ranked differently by different metrics.
For example, Fig. 11 illustrates two constancy metrics, the radius of the smallest bounding sphere and
the variance, on two sets of points in R2, where one set has a small radius but a large variance, while
the other has a large radius but a small variance. More examples are presented in Fig. 12, and such
results can also be observed in Table 2. This difference can lead to differences in risk preferences,
sensitivity to outliers, and learning dynamics when these metrics are used as learning objectives.
Further investigation of the characteristics of these metrics for imperfect representations is left for
future work.
47D.6 Implementation
Thanks to advanced indexing (e.g., NumPy [Harris et al., 2020] and PyTorch [Paszke et al., 2019])
and analytical solutions to some optimization problems (e.g., Eq. (19)), some of the proposed metrics
can be easily implemented, even as Python one-liners.
For example, the following function implements a family of modularity metrics:
def q_product(y: np.ndarray, z: np.ndarray, aggregate, deviation):
return np.sum([aggregate([deviation(zi[yi == yv]) for yvinnp.unique(yi)]) for yi, zi in
zip(y, z)])
Here, yand zare NumPy arrays of shape (factor, index) ;aggregate can be max,mean, orsum;deviation
can be a function calculating the radius of the smallest bounding sphere,12mean absolute deviation
around the geometric median,13variance, diameter, or mean pairwise distance. Please note, however,
that the deviation function can be computationally expensive, depending on the dimension of the
codes.
D.7 Limitations
Lastly, we discuss several aspects that are not covered in this work and potential directions for future
research.
Function equality A collection of input-out pairs {(xi, yi)}n
i=1∈(X×Y)nmay not define a
function g:X→Yfor two reasons: First, the set of all inputs X0:={xi}n
i=0is unlikely to
enumerate all possible inputs (i.e., X0⊊X), especially when the cardinality of the domain Xis
infinite (e.g., R), so the data may only define a partial function g:X ⇀ Y or a function from a
smaller domain g0:X0→Y. Second, the inputs may not be distinct, e.g., when an input is given
multiple labels by different annotators, so the data may define a multi-valued function . The extension
from functions to relations or stochastic maps is an important future direction of our work.
Partial combinations A more general issue is learning and evaluating disentangled representations
given only a subset of all combinations of factors, which is common when dealing with a large
number of factors [Träuble et al., 2021, Montero et al., 2021, 2022, Roth et al., 2023]. It is crucial to
evaluate and justify whether a metric computed on partial combinations of factors is a reliable proxy
for the performance of the model on unseen combinations.
Unknown projections Another common scenario is when the extracted representation is not
properly aligned with the underlying factors. For example, a model may extract a three-dimensional
representation z∈R3for two factors y∈[0,1]2, and it can project to ((z1, z2), z3)or(z1,(z2, z3)).
How can we determine which is better, without enumerating all possible projections? Finding the
optimal assignment [Mahon et al., 2023] and correcting a pre-trained model post hoc [Träuble et al.,
2021] based on the proposed metrics are interesting future directions.
D.8 Broader impact
This paper focuses on the theoretical aspects of disentangled representation learning, and we do
not foresee any immediate negative societal consequences. However, we would acknowledge that
disentanglement is closely related to data-efficiency and fairness, potentially sparking discussions
on ethical considerations. Besides, the application of category theory may facilitate the transfer and
integration of knowledge across disciplines, fostering closer connections between various fields of
study, even beyond the machine learning community.
12https://github.com/marmakoide/miniball (MIT License) [Welzl, 1991]
13https://github.com/krishnap25/geom_median (GNU General Public License, Version 3
(GPLv3)) [Pillutla et al., 2022]
48(a) true factors Y
deviation
approximation
contraction
 (b) entangled codes Z
 (c) product approximation
of an encoder m:Y→Z
(d) linear approximation of
its retraction h:Z→Y
Figure 13: (a) a set of factors Yrepresented by the RGB color model; (b) a set of entangled codes
Zextracted by an encoder m:Y→Z; (c) a product function approximation; and (d) a linear
approximation of the retraction h:Z→Yof the encoder.
6
 4
 2
 0 2 4 6
(a) original distribution0246
0.0 0.2 0.4 0.6 0.8 1.0
(b) integral transform 
 uniform distribution0.00.20.40.60.81.0
3
 2
 1
 0 1 2 3
(c) inverse transform 
 standard normal distribution3
2
1
0123
3
 2
 1
 0 1 2 3
(d) orthogonal transformation 
 standard normal distribution3
2
1
0123
0.0 0.2 0.4 0.6 0.8 1.0
(e) integral transform 
 uniform distribution0.00.20.40.60.81.0
6
 4
 2
 0 2 4 6
(f) inverse transform 
 transformed distribution0246
Figure 14: Entangling a multivariate distribution via probability integral transform, inverse transform,
and orthogonal transformation of standard normal distribution [Locatello et al., 2019b, Theorem 1].
E Experiments
In this section, we provide the detailed data configuration used in Section 5 and further experimental
results.
E.1 Synthetic data
We used a simple synthetic setup to simulate entanglement of factors and common failures patterns.
Concretely, we used a Cartesian product Y:={0,0.1, . . . , 1}3of three sets as the underlying factors
(Fig. 13a). We used a random rotation matrix Rto entangle factors and componentwise exponential
as a non-linear transformation. We composited this procedure twice and used an affine transformation
to normalize the outputs (Fig. 13b). That is, we used the following function as the data generating
process:
g:Y→X:y7→a·exp(R·exp(R·y)) +b. (175)
Note that this function is injective but not a product or linear.
We used the following functions as the function m:Y→Z:
entanglement (y1, y2, y3)7→g(y1, y2, y3)
rotation (y1, y2, y3)7→R·(y1, y2, y3)
duplicate (y1, y2, y3)7→((y1, y2, y3),(y1, y2, y3), y3)
complement (y1, y2, y3)7→((y2, y3),(y1, y3),(y1, y2))
misalignment (y1, y2, y3)7→(y2, y3, y1)
redundancy (y1, y2, y3)7→((y1,−y1), y2, y3)
contraction (y1, y2, y3)7→0.01×(y1, y2, y3)
nonlinear (y1, y2, y3)7→(y2
1, y2
2, y2
3)
constant (y1, y2, y3)7→(0,0,0)
The rotation operation entangles factors but can be (linearly) inverted. The duplicate encoder has a
modular decoder (projections), but itself is not modular. The redundancy encoder is both modular
and informative, but not all codes can be decoded. The constant encoder is perfectly modular but not
informative.
49Table 3: Supervised modularity metrics
Product approx. Constancy
Rad. MAD Var. Diam. MPD
entanglement 0.44 0 .75 0 .96 0 .19 0 .82
rotation 0.22 0 .51 0 .80 0 .05 0 .64
duplicate 0.24 0 .43 0 .67 0 .06 0 .56
complement 0.12 0 .28 0 .55 0 .01 0 .42
misalignment 0.22 0 .44 0 .74 0 .05 0 .58
random 0.22 0 .48 0 .78 0 .05 0 .61
Table 4: Weakly supervised modularity metrics
Product approx. Constancy
Rad. MAD Var. Diam. MPD
entanglement 0.50 0 .77 0 .96 0 .26 0 .84
rotation 0.24 0 .54 0 .83 0 .06 0 .68
duplicate 0.28 0 .46 0 .71 0 .07 0 .60
complement 0.14 0 .32 0 .59 0 .02 0 .47
misalignment 0.22 0 .48 0 .77 0 .05 0 .62
random 0.24 0 .52 0 .81 0 .06 0 .64
E.2 Weakly supervised modularity metrics
We briefly comment on the possibility of employing weak supervision for measuring disentangled
representations.
For supervised disentanglement metrics we discussed in Section 4, the necessary data consists
of observation-factor pairs (x, y), representing a generator g:Y→X. In order to evaluate an
encoder f:X→Z, we compose it with a generator and study the properties of the composition
m:Y→Z:=f◦g.
It is worth noting that pproduct andpinjective are equational predicates, which means that they are
invariant to bijections . Similarly, qproduct andqinjective areinvariant to isometries . This implies that the
exact values of the factors are not important; we only need to know if two factors are equal or not.
Hence, we only need weak supervision of the form (x, x′, yi=Yiy′
i)so that we can construct some
equivalence classes of factors, and we can still calculate or approximate Eqs. (19) and (27). Note that
this type of supervision has been partially investigated by Ridgeway and Mozer [2018], Shu et al.
[2020].
For example, suppose we have an object A (e.g., red circle) and an object B (e.g., red triangle), and
all we know is that objects A and B have the same color. Based on such weak information, we can
still construct an equivalence class containing objects with the same color as object A. Then, we
can regularize an encoder f:X→Zby minimizing the variance of the color representations over
this equivalence class (Eq. (19)). In this way, the modularity of the encoder can be improved. The
challenge arises when there is noise or only partial combinations, which is an interesting future work
direction. In such cases, we may need to use semi-supervised clustering to group the data [Wagstaff
et al., 2001, Basu et al., 2002, Bilenko et al., 2004].
To validate this idea, we conducted experiments where we only used a random sample of pairs and
their similarities. Table 3 is an excerpt of Table 2, showing only the proposed modularity metrics,
and Table 4 shows these metrics calculated using only similarity supervision. We reported the mean
values of 10random samples of pairs, and the variances are negligible. Comparing Tables 3 and 4, we
can observe that weakly supervised metrics may overestimate imperfect representations, but they can
still maintain the ranks. This observation suggests the potential utility of employing weak supervision
for both learning and evaluating disentangled representations using the proposed metrics.
50Table 5: Supervised disentanglement metrics on image datasets
Modularity Informativeness Existing metrics
Product approx. Constancy Retraction approx. Contraction Pair Info. Regressor
Rad. MAD Var. Diam. MPD ME MAE MSE Max Mean BetaaFactorbMIGcDis.dCom.dInfo.d
3D Cars [Reed et al., 2015]
V AE 0.27 0 .76 0 .95 0 .07 0 .83 0 .44 0 .82 0 .94 0 .21 0 .75 0 .90 0 .22 0 .02 0 .07 0 .05 0 .54
β-V AE 0.26 0 .76 0 .95 0 .07 0 .82 0 .42 0 .82 0 .94 0 .20 0 .74 0 .90 0 .21 0 .01 0 .13 0 .10 0 .54
FactorV AE 0.24 0 .75 0 .95 0 .06 0 .82 0 .35 0 .82 0 .94 0 .21 0 .74 0 .89 0 .20 0 .03 0 .11 0 .08 0 .54
β-TCV AE 0.28 0 .77 0 .95 0 .08 0 .83 0 .43 0 .82 0 .94 0 .21 0 .74 0 .90 0 .21 0 .02 0 .14 0 .11 0 .59
dSprites [Matthey et al., 2017]
V AE 0.24 0 .64 0 .94 0 .06 0 .74 0 .37 0 .82 0 .94 0 .18 0 .68 0 .54 0 .26 0 .09 0 .16 0 .15 0 .40
β-V AE 0.14 0 .64 0 .93 0 .02 0 .73 0 .41 0 .83 0 .94 0 .20 0 .70 0 .58 0 .29 0 .13 0 .20 0 .24 0 .44
FactorV AE 0.18 0 .63 0 .93 0 .03 0 .73 0 .38 0 .82 0 .94 0 .17 0 .67 0 .48 0 .26 0 .13 0 .20 0 .23 0 .36
β-TCV AE 0.22 0 .64 0 .93 0 .05 0 .73 0 .42 0 .83 0 .94 0 .21 0 .70 0 .56 0 .27 0 .18 0 .29 0 .31 0 .59
3D Shapes [Burgess and Kim, 2018]
V AE 0.25 0 .76 0 .96 0 .06 0 .82 0 .39 0 .88 0 .96 0 .20 0 .78 0 .99 0 .94 0 .19 0 .35 0 .29 0 .76
β-V AE 0.20 0 .72 0 .95 0 .04 0 .79 0 .39 0 .84 0 .94 0 .19 0 .69 0 .86 0 .77 0 .26 0 .69 0 .62 0 .99
FactorV AE 0.24 0 .69 0 .96 0 .06 0 .78 0 .34 0 .82 0 .93 0 .16 0 .64 0 .83 0 .57 0 .15 0 .40 0 .40 0 .84
β-TCV AE 0.25 0 .69 0 .94 0 .06 0 .77 0 .32 0 .82 0 .93 0 .18 0 .63 0 .76 0 .50 0 .11 0 .68 0 .57 0 .98
MPI3D [Gondal et al., 2019]
V AE 0.04 0 .56 0 .91 0 .00 0 .66 0 .30 0 .76 0 .89 0 .12 0 .46 0 .48 0 .11 0 .12 0 .29 0 .33 0 .64
β-V AE 0.02 0 .84 0 .97 0 .00 0 .87 0 .28 0 .75 0 .89 0 .10 0 .41 0 .39 0 .11 0 .07 0 .15 0 .18 0 .47
FactorV AE 0.09 0 .60 0 .93 0 .01 0 .70 0 .27 0 .75 0 .89 0 .11 0 .43 0 .47 0 .09 0 .12 0 .26 0 .30 0 .60
β-TCV AE 0.07 0 .65 0 .95 0 .01 0 .74 0 .28 0 .76 0 .89 0 .12 0 .46 0 .45 0 .07 0 .12 0 .24 0 .31 0 .57
a[Higgins et al., 2017]b[Kim and Mnih, 2018]c[Chen et al., 2018]d[Eastwood and Williams, 2018]
E.3 Evaluation of existing models on image datasets
We also report the results of several widely used unsupervised disentangled representation learning
methods (V AE [Kingma and Welling, 2014], β-V AE [Higgins et al., 2017], FactorV AE [Kim and
Mnih, 2018], and β-TCV AE [Chen et al., 2018]) evaluated on four image datasets ( 3D Cars [Reed
et al., 2015], dSprites [Matthey et al., 2017], 3D Shapes [Burgess and Kim, 2018], and MPI3D
[Gondal et al., 2019]) in Table 5.
We used a public PyTorch implementation [Paszke et al., 2019] of these methods and used the
same encoder/decoder architecture with the default hyperparameters described in Locatello et al.
[2019b] for all methods for a fair comparison. We used linear projection to find the most informative
representations for each factor. The experiments were conducted on a NVIDIA Tesla V100 GPU.
Before analyzing these results, it is important to note that the evaluation of these learning models is
notmeant to be a proof of the correctness of the proposed metrics, since we cannot tell whether a bad
result is due to the insufficiency of a learning method, to the quality of the datasets, or to the problem
of the evaluation, if we have no theoretical guarantee for the metrics. We can trust the results of the
proposed metrics because the properties of their minimizers are guaranteed by Theorem 1.
From Table 5 we can observe that the considered learning methods do not exhibit significant difference
in terms of modularity and informativeness. This result supports the theoretical finding of Locatello
et al. [2019b] that unsupervised learning of disentangled representations by matching the distributions
of observations is fundamentally impossible (see also Fig. 14) as well as their empirical finding that
there is no evidence that learning disentangled representations in an unsupervised manner is reliable.
E.4 Kendall tau distance between metrics
To analyze the relationship between these metrics, we report the Kendall tau distance [Kendall, 1938,
Virtanen et al., 2020] averaged over experimental settings in Table 6. The Kendall tau distance
is a correlation measure for ordinal data valued in [−1,1]which counts the number of pairwise
disagreements between two ranking lists. Values close to 1indicate strong agreement, and values
close to −1indicate strong disagreement.
From Table 6 we can observe that even though different metrics derived from the same logical
definition may rank imperfect representations differently (see also Fig. 12), they still have positive
correlations with each other, indicating that they measure the same property. The metrics proposed by
Higgins et al. [2017] and Kim and Mnih [2018] have the highest correlations with each other (except
for themselves), and we hypothesize that this is because they are both based on the pairwise distance
51Table 6: Average Kendall tau rank distances bewteen disentanglement metrics
Modularity Informativeness Existing metrics
Product approx. Constancy Retraction approx. Contraction Pair Info. Regressor
Rad. MAD Var. Diam. MPD ME MAE MSE Max Mean BetaaFactorbMIGcDis.dCom.dInfo.d
Rad. 1.00 0.08 0.25 1.00 0.33−0.08 0 .08 0 .17 0 .08 0 .17−0.25 0 .17 0 .00 0 .00 0 .17−0.08
MAD 0.08 1.00 0.50 0.08 0.75 0 .33 0 .67 0 .58 0 .00 0 .42−0.50−0.58−0.25 0 .25 0 .08−0.00
Var. 0.25 0.50 1.00 0.25 0.75 0 .33 0 .33 0 .42−0.17 0 .25−0.00−0.25−0.08 0 .58 0 .42 0 .33
Diam. 1.00 0.08 0.25 1.00 0.33−0.08 0 .08 0 .17 0 .08 0 .17−0.25 0 .17 0 .00 0 .00 0 .17−0.08
MPD 0.33 0.75 0.75 0.33 1.00 0 .25 0 .42 0 .50−0.08 0 .33−0.25−0.33−0.17 0 .33 0 .17 0 .08
ME −0.08 0 .33 0 .33−0.08 0 .25 1.00 0.50 0.58 0.33 0.42−0.17−0.25−0.42 0 .08−0.08 0 .00
MAE 0.08 0 .67 0 .33 0 .08 0 .42 0.50 1.00 0.92 0.17 0.75−0.67−0.58−0.25 0 .08−0.08−0.17
MSE 0.17 0 .58 0 .42 0 .17 0 .50 0.58 0.92 1.00 0.25 0.83−0.58−0.50−0.33 0 .00−0.17−0.25
Max 0.08 0 .00−0.17 0 .08−0.08 0.33 0.17 0.25 1.00 0.42−0.33 0 .08−0.42−0.25−0.42−0.33
Mean 0.17 0 .42 0 .25 0 .17 0 .33 0.42 0.75 0.83 0.42 1.00−0.75−0.50−0.33−0.17−0.33−0.42
Beta −0.25−0.50−0.00−0.25−0.25−0.17−0.67−0.58−0.33−0.75 1.00 0.58 0 .25 0 .25 0 .25 0 .50
Factor 0.17−0.58−0.25 0 .17−0.33−0.25−0.58−0.50 0 .08−0.50 0.58 1.00−0.17−0.17−0.17 0 .08
MIG 0.00−0.25−0.08 0 .00−0.17−0.42−0.25−0.33−0.42−0.33 0 .25−0.17 1.00 0 .17 0 .33 0 .08
Dis. 0.00 0 .25 0 .58 0 .00 0 .33 0 .08 0 .08 0 .00−0.25−0.17 0 .25−0.17 0 .17 1.00 0.83 0.75
Com. 0.17 0 .08 0 .42 0 .17 0 .17−0.08−0.08−0.17−0.42−0.33 0 .25−0.17 0 .33 0.83 1.00 0.75
Info. −0.08−0.00 0 .33−0.08 0 .08 0 .00−0.17−0.25−0.33−0.42 0 .50 0 .08 0 .08 0.75 0.75 1.00
a[Higgins et al., 2017]b[Kim and Mnih, 2018]c[Chen et al., 2018]d[Eastwood and Williams, 2018]
Table 7: Computation time (seconds) of supervised disentanglement metrics on image datasets
Modularity Informativeness Existing metrics
Product approx. Constancy Retraction approx. Contraction Pair Info. Regressor
Rad. MAD Var. Diam. MPD ME MAE MSE Max Mean BetaaFactorbMIGcDCId
3D Cars [Reed et al., 2015] 0.35 2 .22 0 .01 0 .03 0 .03 0 .14 2 .11 0 .00 1 .09 1 .12 4 .12 3 .77 2 .46 896 .99
dSprites [Matthey et al., 2017] 0.47 4 .04 0 .00 0 .03 0 .03 0 .51 3 .84 0 .00 1 .36 1 .37 7 .00 6 .37 11 .51 353 .04
3D Shapes [Burgess and Kim, 2018] 0.60 6 .02 0 .00 0 .03 0 .03 0 .66 4 .77 0 .00 1 .67 1 .52 4 .85 4 .65 10 .16 169 .11
MPI3D [Gondal et al., 2019] 0.86 9 .30 0 .00 0 .09 0 .09 1 .61 5 .98 0 .00 1 .89 1 .94 9 .54 8 .60 21 .70 310 .38
a[Higgins et al., 2017]b[Kim and Mnih, 2018]c[Chen et al., 2018]d[Eastwood and Williams, 2018]
approach. The DCI disentanglement metric [Eastwood and Williams, 2018] weakly agrees with the
modularity metrics. However, the DCI informativeness metric [Eastwood and Williams, 2018] weakly
disagrees with the informativeness metrics. It is possible that this is because of the different regressors
(sklearn.ensemble.GradientBoostingClassifier ,sklearn.linear_model.LinearRegression , and sklearn
.linear_model.QuantileRegressor [Pedregosa et al., 2011]) used in predicting factors from codes,
showing that random seeds and hyperparameters of the metrics may matter more than the models
when additional predictors need to be trained to evaluate the learning methods. This result indicates
the advantage of qinjective overqretractable .
However, it is important to note the limitations of these experimental results. Since the representations
were learned from data and not fully controlled, it is possible that such results are due to the choices
of datasets, learning algorithms, hyperparameters, and optimization errors. A high rank correlation
coefficient between two metrics in this specific setting cannot guarantee that these metrics always
measure the same property, or that they rank imperfect representations similarly in other settings. To
gain a deeper understanding of these metrics, it is preferable to analyze their minimizers theoretically
(Theorem 1) or test them in a fully controlled environment (Section 5).
E.5 Computation time of metrics
Finally, we report the computation time of the considered metrics in Table 7 to support our claim
that the proposed metrics are much faster than those that require training additional predictors and
hyperparameter tuning. We can see that, in an extreme case, the calculation of the DCI metrics
[Eastwood and Williams, 2018] using GradientBoostingClassifier [Pedregosa et al., 2011] takes
around 15 minutes, while other metrics can be calculated within seconds. This computation time may
be acceptable if the metrics are only used in the evaluation phase, but it is not feasible to use them as
learning objectives even in derivative-free optimization.
52Table 8: Factor-wise modularity metrics on 3D Cars [Reed et al., 2015]
Product approx. Constancy
Rad. MAD Var. Diam. MPD
Elevation (4)
V AE 0.49 0 .87 0 .97 0 .24 0 .90
β-V AE 0.52 0 .86 0 .97 0 .27 0 .90
FactorV AE 0.50 0 .86 0 .97 0 .25 0 .90
β-TCV AE 0.50 0 .87 0 .97 0 .25 0 .90
Azimuth (24)
V AE 0.62 0 .91 0 .99 0 .39 0 .94
β-V AE 0.60 0 .91 0 .98 0 .37 0 .93
FactorV AE 0.57 0 .90 0 .98 0 .32 0 .93
β-TCV AE 0.65 0 .91 0 .99 0 .42 0 .94
Object (183)
V AE 0.87 0 .97 1 .00 0 .75 0 .98
β-V AE 0.84 0 .97 1 .00 0 .71 0 .98
FactorV AE 0.85 0 .97 1 .00 0 .72 0 .98
β-TCV AE 0.86 0 .97 1 .00 0 .75 0 .98
E.6 Factor-wise modularity metrics
An advantage of the proposed modularity metrics is that we can even evaluate each factor separately,
which is impossible for those metrics that entangle modularity and informativeness. The results were
reported in Tables 8 to 11. We found that some learning methods may outperform others on one factor
but underperform on others, and different modularity metrics may rank learning methods differently
(see also Appendix D.5).
For example, on MPI3D [Gondal et al., 2019] (Table 11), β-V AE [Higgins et al., 2017] has the
highest scores (radius, MAD, variance, diameter, and MPD) on the horizontal and vertical axis factors,
but the lowest scores (radius and diameter) on the object size, camera height, and background color
factors. However, as measured by the MAD and MPD, it still has the highest scores on these factors.
This means that β-V AE may generally encode the object size, camera height, and background color
well compared to other considered methods, but has a small number of outliers. We believe that
such fine-grained evaluation can guide the design of learning objectives, data collection, and further
refinement of trained representation learning models.
53Table 9: Factor-wise modularity metrics on dSprites [Matthey et al., 2017]
Product approx. Constancy
Rad. MAD Var. Diam. MPD
Shape (3)
V AE 0.74 0 .89 0 .98 0 .55 0 .92
β-V AE 0.70 0 .88 0 .98 0 .50 0 .92
FactorV AE 0.73 0 .89 0 .98 0 .53 0 .92
β-TCV AE 0.72 0 .88 0 .98 0 .53 0 .92
Scale (6)
V AE 0.71 0 .90 0 .98 0 .51 0 .93
β-V AE 0.55 0 .88 0 .98 0 .30 0 .92
FactorV AE 0.67 0 .90 0 .98 0 .45 0 .93
β-TCV AE 0.77 0 .90 0 .98 0 .59 0 .93
Orientation (40)
V AE 0.92 0 .97 1 .00 0 .84 0 .98
β-V AE 0.86 0 .98 1 .00 0 .74 0 .98
FactorV AE 0.95 0 .98 1 .00 0 .90 0 .99
β-TCV AE 0.88 0 .97 1 .00 0 .77 0 .98
Position X (32)
V AE 0.71 0 .90 0 .98 0 .51 0 .93
β-V AE 0.66 0 .92 0 .99 0 .43 0 .95
FactorV AE 0.62 0 .89 0 .98 0 .39 0 .92
β-TCV AE 0.67 0 .91 0 .99 0 .44 0 .94
Position Y (32)
V AE 0.68 0 .91 0 .99 0 .47 0 .94
β-V AE 0.66 0 .92 0 .99 0 .44 0 .94
FactorV AE 0.64 0 .90 0 .98 0 .41 0 .93
β-TCV AE 0.67 0 .90 0 .98 0 .45 0 .93
54Table 10: Factor-wise modularity metrics on 3D Shapes [Burgess and Kim, 2018]
Product approx. Constancy
Rad. MAD Var. Diam. MPD
Floor hue (10)
V AE 0.85 0 .97 1 .00 0 .72 0 .98
β-V AE 0.87 0 .97 1 .00 0 .75 0 .98
FactorV AE 0.75 0 .93 0 .99 0 .57 0 .95
β-TCV AE 0.99 1 .00 1 .00 0 .97 1 .00
Wall hue (10)
V AE 0.85 0 .97 1 .00 0 .73 0 .98
β-V AE 0.94 0 .99 1 .00 0 .87 0 .99
FactorV AE 0.78 0 .94 0 .99 0 .60 0 .96
β-TCV AE 0.82 0 .94 0 .99 0 .68 0 .96
Object hue (10)
V AE 0.77 0 .95 0 .99 0 .59 0 .96
β-V AE 0.75 0 .92 0 .99 0 .56 0 .95
FactorV AE 0.75 0 .93 0 .99 0 .56 0 .95
β-TCV AE 0.77 0 .92 0 .99 0 .59 0 .95
Scale (8)
V AE 0.80 0 .95 0 .99 0 .65 0 .96
β-V AE 0.56 0 .91 0 .98 0 .32 0 .93
FactorV AE 0.79 0 .93 0 .99 0 .63 0 .95
β-TCV AE 0.80 0 .95 1 .00 0 .63 0 .97
Shape (4)
V AE 0.59 0 .91 0 .98 0 .35 0 .93
β-V AE 0.62 0 .91 0 .98 0 .38 0 .93
FactorV AE 0.77 0 .93 0 .99 0 .60 0 .95
β-TCV AE 0.69 0 .92 0 .98 0 .47 0 .94
Orientation (15)
V AE 0.95 0 .99 1 .00 0 .91 0 .99
β-V AE 0.94 0 .99 1 .00 0 .89 0 .99
FactorV AE 0.89 0 .98 1 .00 0 .80 0 .99
β-TCV AE 0.72 0 .91 0 .98 0 .52 0 .94
55Table 11: Factor-wise modularity metrics on MPI3D [Gondal et al., 2019]
Product approx. Constancy
Rad. MAD Var. Diam. MPD
Object color (6)
V AE 0.52 0 .92 0 .99 0 .27 0 .94
β-V AE 0.51 0 .97 0 .99 0 .26 0 .97
FactorV AE 0.66 0 .94 0 .99 0 .43 0 .96
β-TCV AE 0.57 0 .92 0 .99 0 .33 0 .94
Object shape (6)
V AE 0.88 0 .97 1 .00 0 .77 0 .98
β-V AE 0.94 1 .00 1 .00 0 .89 1 .00
FactorV AE 0.93 0 .99 1 .00 0 .87 0 .99
β-TCV AE 0.95 1 .00 1 .00 0 .91 1 .00
Object size (2)
V AE 0.70 0 .93 0 .99 0 .49 0 .95
β-V AE 0.63 0 .98 1 .00 0 .40 0 .98
FactorV AE 0.68 0 .94 0 .99 0 .46 0 .95
β-TCV AE 0.75 0 .96 1 .00 0 .56 0 .97
Camera height (3)
V AE 0.48 0 .87 0 .97 0 .23 0 .90
β-V AE 0.34 0 .95 0 .99 0 .11 0 .96
FactorV AE 0.58 0 .85 0 .96 0 .34 0 .90
β-TCV AE 0.69 0 .91 0 .99 0 .47 0 .94
Background color (3)
V AE 0.60 0 .92 0 .99 0 .36 0 .94
β-V AE 0.34 0 .96 0 .99 0 .11 0 .97
FactorV AE 0.76 0 .94 0 .99 0 .58 0 .96
β-TCV AE 0.59 0 .93 0 .99 0 .34 0 .95
Horizontal axis (40)
V AE 0.72 0 .93 0 .99 0 .52 0 .95
β-V AE 0.74 0 .99 1 .00 0 .55 0 .99
FactorV AE 0.69 0 .92 0 .99 0 .47 0 .94
β-TCV AE 0.73 0 .94 0 .99 0 .54 0 .96
Vertical axis (40)
V AE 0.67 0 .91 0 .99 0 .45 0 .94
β-V AE 0.75 0 .99 1 .00 0 .56 0 .99
FactorV AE 0.73 0 .93 0 .99 0 .53 0 .95
β-TCV AE 0.60 0 .93 0 .99 0 .36 0 .95
56NeurIPS Paper Checklist
1.Claims
Question: Do the main claims made in the abstract and introduction accurately reflect the
paper’s contributions and scope?
Answer: [Yes]
Justification: “ A theoretical connection between logical definitions of disentanglement and
quantitative metrics ” was detailed in Appendices A and B. “ A systematic approach for
converting a first-order predicate into a real-valued quantity ” was introduced in Section 3.
“The metrics induced by logical definitions, ” which were introduced in Section 4, “ have
strong theoretical guarantees, ” which was supported by Theorem 1. “ The effectiveness of
the proposed metrics ” was empirically validated in Section 5 “ by isolating different aspects
of disentangled representations ” and further investigated in Appendix E.
Guidelines:
The answer NA means that the abstract and introduction do not include the claims made
in the paper.
The abstract and/or introduction should clearly state the claims made, including the
contributions made in the paper and important assumptions and limitations. A No or NA
answer to this question will not be perceived well by the reviewers.
The claims made should match theoretical and experimental results, and reflect how much
the results can be expected to generalize to other settings.
It is fine to include aspirational goals as motivation as long as it is clear that these goals
are not attained by the paper.
2.Limitations
Question: Does the paper discuss the limitations of the work performed by the authors?
Answer: [Yes]
Justification: We discussed the limitations in Appendix D.7. Another limitation is that
we only studied the properties of the minimizers of the metrics (Theorem 1). Research
on the behavior of these metrics on imperfect representations is limited (Appendix D.5).
We claimed that some proposed metrics can serve as differentiable learning objectives, but
empirical evaluation is out of the scope of this work.
Guidelines:
The answer NA means that the paper has no limitation while the answer No means that
the paper has limitations, but those are not discussed in the paper.
The authors are encouraged to create a separate "Limitations" section in their paper.
The paper should point out any strong assumptions and how robust the results are to
violations of these assumptions (e.g., independence assumptions, noiseless settings, model
well-specification, asymptotic approximations only holding locally). The authors should
reflect on how these assumptions might be violated in practice and what the implications
would be.
The authors should reflect on the scope of the claims made, e.g., if the approach was only
tested on a few datasets or with a few runs. In general, empirical results often depend on
implicit assumptions, which should be articulated.
The authors should reflect on the factors that influence the performance of the approach.
For example, a facial recognition algorithm may perform poorly when image resolution
is low or images are taken in low lighting. Or a speech-to-text system might not be used
reliably to provide closed captions for online lectures because it fails to handle technical
jargon.
The authors should discuss the computational efficiency of the proposed algorithms and
how they scale with dataset size.
If applicable, the authors should discuss possible limitations of their approach to address
problems of privacy and fairness.
While the authors might fear that complete honesty about limitations might be used by
reviewers as grounds for rejection, a worse outcome might be that reviewers discover
limitations that aren’t acknowledged in the paper. The authors should use their best
judgment and recognize that individual actions in favor of transparency play an important
role in developing norms that preserve the integrity of the community. Reviewers will be
specifically instructed to not penalize honesty concerning limitations.
3.Theory Assumptions and Proofs
57Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?
Answer: [Yes]
Justification: All definitions of the proposed terms, theoretical results, and detailed proofs
were provided in Appendices A to C. The main theoretical result was stated in Theorem 1
and instantiated in Section 4.
Guidelines:
The answer NA means that the paper does not include theoretical results.
All the theorems, formulas, and proofs in the paper should be numbered and cross-
referenced.
All assumptions should be clearly stated or referenced in the statement of any theorems.
The proofs can either appear in the main paper or the supplemental material, but if they
appear in the supplemental material, the authors are encouraged to provide a short proof
sketch to provide intuition.
Inversely, any informal proof provided in the core of the paper should be complemented
by formal proofs provided in appendix or supplemental material.
Theorems and Lemmas that the proof relies upon should be properly referenced.
4.Experimental Result Reproducibility
Question: Does the paper fully disclose all the information needed to reproduce the
main experimental results of the paper to the extent that it affects the main claims and/or
conclusions of the paper (regardless of whether the code and data are provided or not)?
Answer: [Yes]
Justification: The contribution is primarily the new theoretical framework. For experiments,
we provided code snippets in Appendix D.6. We provided the detailed data configuration
used in Section 5 in Appendix E.
Guidelines:
The answer NA means that the paper does not include experiments.
If the paper includes experiments, a No answer to this question will not be perceived well
by the reviewers: Making the paper reproducible is important, regardless of whether the
code and data are provided or not.
If the contribution is a dataset and/or model, the authors should describe the steps taken
to make their results reproducible or verifiable.
Depending on the contribution, reproducibility can be accomplished in various ways.
For example, if the contribution is a novel architecture, describing the architecture fully
might suffice, or if the contribution is a specific model and empirical evaluation, it may
be necessary to either make it possible for others to replicate the model with the same
dataset, or provide access to the model. In general. releasing code and data is often
one good way to accomplish this, but reproducibility can also be provided via detailed
instructions for how to replicate the results, access to a hosted model (e.g., in the case
of a large language model), releasing of a model checkpoint, or other means that are
appropriate to the research performed.
While NeurIPS does not require releasing code, the conference does require all
submissions to provide some reasonable avenue for reproducibility, which may depend
on the nature of the contribution. For example
(a)If the contribution is primarily a new algorithm, the paper should make it clear how
to reproduce that algorithm.
(b)If the contribution is primarily a new model architecture, the paper should describe
the architecture clearly and fully.
(c)If the contribution is a new model (e.g., a large language model), then there should
either be a way to access this model for reproducing the results or a way to reproduce
the model (e.g., with an open-source dataset or instructions for how to construct the
dataset).
(d)We recognize that reproducibility may be tricky in some cases, in which case authors
are welcome to describe the particular way they provide for reproducibility. In the
case of closed-source models, it may be that access to the model is limited in some
way (e.g., to registered users), but it should be possible for other researchers to have
some path to reproducing or verifying the results.
5.Open access to data and code
58Question: Does the paper provide open access to the data and code, with sufficient
instructions to faithfully reproduce the main experimental results, as described in
supplemental material?
Answer: [Yes]
Justification: We provided the code to reproduce the modularity metrics in Table 2.
Guidelines:
The answer NA means that paper does not include experiments requiring code.
Please see the NeurIPS code and data submission guidelines ( https://nips.cc/pu
blic/guides/CodeSubmissionPolicy ) for more details.
While we encourage the release of code and data, we understand that this might not be
possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
including code, unless this is central to the contribution (e.g., for a new open-source
benchmark).
The instructions should contain the exact command and environment needed to run
to reproduce the results. See the NeurIPS code and data submission guidelines
(https://nips.cc/public/guides/CodeSubmissionPolicy ) for more
details.
The authors should provide instructions on data access and preparation, including how to
access the raw data, preprocessed data, intermediate data, and generated data, etc.
The authors should provide scripts to reproduce all experimental results for the new
proposed method and baselines. If only a subset of experiments are reproducible, they
should state which ones are omitted from the script and why.
At submission time, to preserve anonymity, the authors should release anonymized
versions (if applicable).
Providing as much information as possible in supplemental material (appended to the
paper) is recommended, but including URLs to data and code is permitted.
6.Experimental Setting/Details
Question: Does the paper specify all the training and test details (e.g., data splits,
hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand
the results?
Answer: [Yes]
Justification: See Appendix E.
Guidelines:
The answer NA means that the paper does not include experiments.
The experimental setting should be presented in the core of the paper to a level of detail
that is necessary to appreciate the results and make sense of them.
The full details can be provided either with the code, in appendix, or as supplemental
material.
7.Experiment Statistical Significance
Question: Does the paper report error bars suitably and correctly defined or other appropriate
information about the statistical significance of the experiments?
Answer: [Yes]
Justification: The results of the proposed metrics reported in Table 2 are deterministic and
do not contain randomness. We reported the mean values of 10random trials in Table 4, and
the variances are negligible.
Guidelines:
The answer NA means that the paper does not include experiments.
The authors should answer "Yes" if the results are accompanied by error bars, confidence
intervals, or statistical significance tests, at least for the experiments that support the main
claims of the paper.
The factors of variability that the error bars are capturing should be clearly stated (for
example, train/test split, initialization, random drawing of some parameter, or overall run
with given experimental conditions).
The method for calculating the error bars should be explained (closed form formula, call
to a library function, bootstrap, etc.)
The assumptions made should be given (e.g., Normally distributed errors).
It should be clear whether the error bar is the standard deviation or the standard error of
the mean.
59It is OK to report 1-sigma error bars, but one should state it. The authors should
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
of Normality of errors is not verified.
For asymmetric distributions, the authors should be careful not to show in tables or figures
symmetric error bars that would yield results that are out of range (e.g. negative error
rates).
If error bars are reported in tables or plots, The authors should explain in the text how
they were calculated and reference the corresponding figures or tables in the text.
8.Experiments Compute Resources
Question: For each experiment, does the paper provide sufficient information on the
computer resources (type of compute workers, memory, time of execution) needed to
reproduce the experiments?
Answer: [Yes]
Justification: See Appendix E.
Guidelines:
The answer NA means that the paper does not include experiments.
The paper should indicate the type of compute workers CPU or GPU, internal cluster, or
cloud provider, including relevant memory and storage.
The paper should provide the amount of compute required for each of the individual
experimental runs as well as estimate the total compute.
The paper should disclose whether the full research project required more compute than
the experiments reported in the paper (e.g., preliminary or failed experiments that didn’t
make it into the paper).
9.Code Of Ethics
Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines ?
Answer: [Yes]
Justification: We reviewed the NeurIPS Code of Ethics.
Guidelines:
The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
If the authors answer No, they should explain the special circumstances that require a
deviation from the Code of Ethics.
The authors should make sure to preserve anonymity (e.g., if there is a special
consideration due to laws or regulations in their jurisdiction).
10.Broader Impacts
Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?
Answer: [Yes]
Justification: See Appendix D.8.
Guidelines:
The answer NA means that there is no societal impact of the work performed.
If the authors answer NA or No, they should explain why their work has no societal
impact or why the paper does not address societal impact.
Examples of negative societal impacts include potential malicious or unintended uses
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g.,
deployment of technologies that could make decisions that unfairly impact specific
groups), privacy considerations, and security considerations.
The conference expects that many papers will be foundational research and not tied to
particular applications, let alone deployments. However, if there is a direct path to any
negative applications, the authors should point it out. For example, it is legitimate to point
out that an improvement in the quality of generative models could be used to generate
deepfakes for disinformation. On the other hand, it is not needed to point out that a
generic algorithm for optimizing neural networks could enable people to train models
that generate Deepfakes faster.
The authors should consider possible harms that could arise when the technology is being
used as intended and functioning correctly, harms that could arise when the technology is
being used as intended but gives incorrect results, and harms following from (intentional
or unintentional) misuse of the technology.
60If there are negative societal impacts, the authors could also discuss possible mitigation
strategies (e.g., gated release of models, providing defenses in addition to attacks,
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
feedback over time, improving the efficiency and accessibility of ML).
11.Safeguards
Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?
Answer: [NA]
Justification: This paper mainly focused on the theory of disentangled representation
learning.
Guidelines:
The answer NA means that the paper poses no such risks.
Released models that have a high risk for misuse or dual-use should be released with
necessary safeguards to allow for controlled use of the model, for example by requiring
that users adhere to usage guidelines or restrictions to access the model or implementing
safety filters.
Datasets that have been scraped from the Internet could pose safety risks. The authors
should describe how they avoided releasing unsafe images.
We recognize that providing effective safeguards is challenging, and many papers do not
require this, but we encourage authors to take this into account and make a best faith
effort.
12.Licenses for existing assets
Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?
Answer: [Yes]
Justification: See Appendix E.
Guidelines:
The answer NA means that the paper does not use existing assets.
The authors should cite the original paper that produced the code package or dataset.
The authors should state which version of the asset is used and, if possible, include a
URL.
The name of the license (e.g., CC-BY 4.0) should be included for each asset.
For scraped data from a particular source (e.g., website), the copyright and terms of
service of that source should be provided.
If assets are released, the license, copyright information, and terms of use in the package
should be provided. For popular datasets, paperswithcode.com/datasets has
curated licenses for some datasets. Their licensing guide can help determine the license
of a dataset.
For existing datasets that are re-packaged, both the original license and the license of the
derived asset (if it has changed) should be provided.
If this information is not available online, the authors are encouraged to reach out to the
asset’s creators.
13.New Assets
Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?
Answer: [NA]
Justification: This paper does not release new assets.
Guidelines:
The answer NA means that the paper does not release new assets.
Researchers should communicate the details of the dataset/code/model as part of their
submissions via structured templates. This includes details about training, license,
limitations, etc.
The paper should discuss whether and how consent was obtained from people whose
asset is used.
At submission time, remember to anonymize your assets (if applicable). You can either
create an anonymized URL or include an anonymized zip file.
14.Crowdsourcing and Research with Human Subjects
61Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as
well as details about compensation (if any)?
Answer: [NA]
Justification: This paper does not involve crowdsourcing nor research with human subjects.
Guidelines:
The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
Including this information in the supplemental material is fine, but if the main contribution
of the paper involves human subjects, then as much detail as possible should be included
in the main paper.
According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
or other labor should be paid at least the minimum wage in the country of the data
collector.
15.Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects
Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?
Answer: [NA]
Justification: This paper does not involve crowdsourcing nor research with human subjects.
Guidelines:
The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
Depending on the country in which research is conducted, IRB approval (or equivalent)
may be required for any human subjects research. If you obtained IRB approval, you
should clearly state this in the paper.
We recognize that the procedures for this may vary significantly between institutions
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
guidelines for their institution.
For initial submissions, do not include any information that would break anonymity (if
applicable), such as the institution conducting the review.
62