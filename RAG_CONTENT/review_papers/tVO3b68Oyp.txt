SyllableLM: Learning Coarse Semantic Units for
Speech Language Models
Anonymous Author(s)
Affiliation
Address
email
Abstract
Self-Supervised Transformer Models are the backbone of much of the recent 1
progress in deep learning. However, these models require their inputs to be tok- 2
enized, and tokenization strategies for continuous data like audio and vision are 3
often based on simple heuristics such as fixed sized convolutions or discrete clus- 4
tering. For speech and audio models in particular, the high resolution of waveforms 5
(16,000 samples/second or more) presents a significant challenge, as several times 6
more tokens are used per word than in textual language modeling. In this work, 7
we introduce a controllable, fully-self-supervised technique to dynamically merge 8
speech representations across time to as low as 5 Hz at 60 bits per second while 9
still preserving semantic information. We do this by 1) extracting noisy bound- 10
aries through analyzing correlations between mask spans and model losses and 2) 11
iteratively improving these representations with a novel agglomeration technique. 12
Using these new feature representations, we successfully train SyllableLM, a Neu- 13
ral Codec Language Model (NCLM) competitive with current SoTA NCLMs on 14
a range of common benchmarks with a 30x reduction in pretraining compute, 5x 15
reduction in inference compute, and 2.5x reduction in bitrate. 16
1 Introduction 17
Self-Supervised Learning (SSL) seeks to learn powerful, abstract representations of data without 18
external labels. These representations can then be used in downstream tasks to achieve high perfor- 19
mance even when modest amounts of supervised fine-tuning data are available. In audio and speech 20
processing, a key motivation for this learning paradigm is the fact that young children learn to listen 21
and speak well before they can read or write. While current textual language models [ 52,59,9] can 22
compose highly realistic text, the research community has not yet developed similarly performant 23
models that learn solely from spoken language. An increasing focus has coalesced around Generative 24
Spoken Language Modeling (GSLM) [34], which sets out to achieve this goal. 25
The most successful of these approaches are autoregressive decoder transformer models [ 53] such as 26
AudioLM [ 8] and TWIST [ 26], which operate on tokens learned through quantizing the output of 27
SSL encoder models [ 28,14]. However, these self-supervised tokenizations are much denser than 28
their textual counterparts with the token rates typically between 25 and 50 tokens per second for 29
speech models, as opposed to the typical human speaking rate of 2-5 words per second. The long 30
context lengths that result from high temporal resolution tokenizations in speech models substantially 31
impair both pretraining and inference speed, and it is additionally unclear to what extent modeling 32
speech with a high granularity harms more abstract semantic understanding. 33
Very recently, there has been significant progress in extracting coarser speech unit representations 34
from raw audio. In particular, SD-HuBERT [ 12] distills HuBERT [ 28] using only audio with a DINO- 35
like distillation objective, and VG-HuBERT [ 45,46] uses a contrastive loss against cross-modal 36
Submitted to 38th Conference on Neural Information Processing Systems (NeurIPS 2024). Do not distribute.8192 -
4096 -
0Hz-Student
HuBERTTeacher
HuBERT
LossPred
A
B
CMean 
Pool
MSE 
LossA
A
B
B
B
C
C
SylBoost8192 -
4096 -
0Hz-2111
215
987
24
108
Syllabic UnitsIter N
Iter0
527
527KMeans + 
agglome . 
clusteringFigure 1: Left-Top: The loss prediction matrix C, where brighter is higher likelihood placed on the
teacher label. A time-aligned transcript is on the bottom, and predicted cluster unit boundaries span
vertically as dashed-lines. Left-Bottom: A Mel-Spectrogram of the input waveform with an example
masked timespan in gray. The losses on tokens at timesteps covered by the solid blue and dotted red
spans are mapped to their corresponding rows and columns in Cas described in Section 3.1. Right:
Visual of our agglomeration procedure. We train a student to match intermediate teacher features
pooled over regions generated by pseudo-syllable-boundaries. We use a min-cut algorithm to extract
boundaries, and then apply K-Means and Agglomerative clustering to obtain discrete units.
visual inputs. We continue and significantly improve upon this line of research, resulting in the 37
first syllable-like units suitable for high-quality GSLM. Specifically, we demonstrate breakthrough 38
improvements in textual reconstruction from low-bitrate units of SSL models, reducing the word- 39
error-rate (WER) from 37% using SD-HuBERT units to 7%, and more than halving realized bitrate 40
of previous SpeechLM units from 175Bps to as low as 60Bps. We additionally find that our units 41
correlate strongly with syllables both in boundary detection and in cluster quality. 42
Furthermore, we evaluate the effects of training SpeechLMs on these new units and obtain state-of- 43
the-art results across a wide-variety of metrics, competitive with or outperforming AudioLM (350M 44
parameters) and all TWIST model sizes (125M-13B parameters) with fewer parameters and fewer 45
GPU-Hours. We commit to making our code open-source and plan to release our tokenizer and 46
SpeechLM parameters. Our contributions are as follows: 47
1.We propose a novel training-free algorithm named LossPred that reveals noisy syllabic- 48
like segmentation of unannotated speech signals by analyzing the loss of a pretrained 49
self-supervised model (e.g. HuBERT) across different masking spans. 50
2.We propose a novel bootstrapping framework for speech unit quantization named SylBoost 51
that achieves SotA unsupervised syllabic segmentation, categorization, and low-bitrate 52
unit-to-audio resynthesis. 53
3.Using quantized SylBoost units as a basis for tokenization, we train SyllableLM, a generative 54
spoken language model that outperforms or matches AudioLM and TWIST on a range of 55
tasks while being 30x faster to train, 5x faster for inference, and having a 2.5x reduction in 56
unit bitrate. 57
2 Related Work 58
Self-Supervised Encoder Models There has been a great amount of work in learning high-level 59
representations from data by reconstructing corrupted inputs across speech [ 3,28,6], audio [ 24], text 60
[20,15], and vision [ 10,27]. To navigate the lack of simple discrete targets in speech, much work 61
has been placed in finding high-quality targets, such as iterative clustering [ 28] and by predicting the 62
feature representations of a teacher network based on a running average of student model weights 63
[5,6]. An alternate but similar line of work has been placed into learning low-bitrate units for the 64
task of resynthesis [ 19,58,56,33,60,21], which include losses focused on reconstruction and use 65
an information bottleneck to enforce compression. 66
Applications of Neural Codecs The discrete units generated by these self-supervised encoders are 67
versatile and fundamental to much of the recent progress in speech research such as Text-To-Speech 68
2[54,29,50,47], joint audio-text foundation models [ 57,13,38], unsupervised speech recognition 69
[4], discrete unit resynthesis [ 48,19,58], text-to-audio [ 32,1,17], and generative spoken language 70
modeling [ 8,26,34]. Each of these methods operates on audio units exclusively greater than or equal 71
to 25Hz, which has been a frequently cited area for future work to improve on [ 26]. Recent work 72
[22] has also explored training speech encoder models with coarser units as targets. 73
Extracting Semantic Units from Raw Data Also relevant to our work are several approaches, 74
particularly in vision and audio, that generate emergent semantic clusterings from self-supervised 75
transformer [ 53] models. In particular, the DINO approach in Caron et al. [10] observes object 76
representations in attention maps through student-teacher distillation. Similar techniques have been 77
also applied to audio to discover emergent syllable boundaries [ 12,46]. These behaviors can vary 78
heavily with small changes in pretraining strategy as explored in Darcet et al. [18]. Merging similar 79
features has also been shown to produce significant vision model speedups such as in Bolya et al. [7]. 80
Most similar to our work, Algayres et al. [2]extracted coarse continuous representations for GSLM, 81
however these results trail behind NCLM-based approaches. 82
3Learning Self-Supervised, Syllable-Like Representations from Raw Speech 83
In this section, we describe the bootstrapping process by which we extract low-bitrate speech units. 84
We first describe LossPred, our algorithm to analyze outputs of self-supervised speech model loss 85
functions to generate initial unit boundaries. Following this, we define SylBoost, an agglomeration 86
procedure to iteratively refine these boundaries with student-teacher distillation. We also propose a 87
new algorithm for the efficient extraction of boundaries from feature self-similarity matrices to fix 88
the bottleneck slowing down VG-HuBERT and SD-HuBERT extraction. 89
3.1 LossPred: Extracting Syllable-like Segmentation from Relations in HuBERT’s Loss 90
HuBERT has previously been shown to learn phone-like units with its K-means clusterings [ 28] which 91
have formed the basis of subsequent works on GSLM and unsupervised ASR [ 4,34,26]. However, 92
other work [ 42,43] has shown that the representations learned by these models also correlate with 93
higher level structure such as words, despite these structures not immediately appearing during 94
clustering. Our goal in this section is to propose a method that can be applied to a pre-trained 95
HuBERT model in order to automatically extract unit boundaries at the level of syllables or words, 96
rather than phones. Although we apply our method to HuBERT, we expect that it could also be 97
applied to other SSL speech models that utilize a similar loss function such as WavLM [ 11] or 98
wav2vec2.0 [ 3]. The crucial commonality between these models is that they all utilize a masked 99
language modeling (MLM) training objective, whereby input speech tokens are randomly masked 100
and the model is trained to predict the masked inputs conditioned on the unmasked inputs. 101
We ground our intuition with the following thought experiment: If the input tokens corresponding 102
to an entire word were replaced with mask tokens, we would expect the HuBERT model loss at 103
these timesteps to be relatively high, as HuBERT would have to jointly predict word identity and 104
the underlying acoustics to predict the missing span. On the other hand, if only the latter portion 105
of a word were masked out, infilling this masked region given the word prefix may be easier by 106
comparison. With this, if we iteratively shift a contiguous mask over a span of tokens and look at the 107
loss, we would suspect to see a strong decrease in the loss throughout the timesteps corresponding to 108
a masked semantic unit (word, syllable, or otherwise) as the beginning or end of the unit was partially 109
revealed to the model. In our experiments, we find that semantic units extracted by this method tend 110
to be syllable-like (both via inspection, and also confirmed experimentally in our segmentation and 111
clustering experiments) and so we focus on these units for the rest of the paper. 112
We consider the setting of having a pretrained HuBERT teacher model and a HuBERT student model 113
trained to predict the quantized contextualized representations generated by the teacher at layer L, as 114
described in Hsu et al. [28]. Formally, given an input waveform W, we extract the teacher labels used 115
to train the student by passing Wunmodified into the frozen HuBERT teacher and then quantizing the 116
contextualized representations of layer Lwith K-Means. We denote these teacher labels as Y{1...T}, 117
where Tis the number of tokens outputted by the CNN feature encoder stage of HuBERT. During 118
pretraining, the student model is given a corrupted version of Wwhere tokens after CNN extraction 119
at select times are replaced with a learned ‘mask’ embedding. We denote these tokens input to the 120
3student as XM
{1...T}where M={t1, . . . t m}is a contiguous span of masked timesteps. The student 121
is then trained to predict these teacher labels at masked timesteps using a cross-entropy loss, which 122
we denote as EXM
tfor the loss on Yt, t∈Mgiven XM: 123
EXM
t:=−logp(Yt|XM) (1)
We look at the losses of the student model at the end of pretraining, and define the loss prediction 124
matrix Cwith mask span size parameter sto capture the raw probabilities of the losses that would 125
result from all possible temporal locations of the mask span M: 126
Cr,c∈RT×T
+ =

p(Yt|XM)|M={r+ 1, r+ 2, . . . r +s}ifr < c, |r−c| ≤ ⌊s
2⌋,
p(Yt|XM)|M={r−1, r−2, . . . r−s}ifr > c, |r−c| ≤ ⌊s
2⌋,
0 otherwise.
We separately calculate the upper and lower triangles of C, relating to the observed waveform being 127
before the mask and after the mask respectively. In the upper triangle, each entry Cr,cat row r 128
column cis equal to p(Yt|XM)given that the mask span in XMstarts just after time r. Inversely, 129
in the lower triangle, Cr,cis equal to p(Yt|XM)given that the mask span in ends just before 130
timer. We use a span size s= 50 corresponding to 1 second as this duration is long enough to 131
mask the majority of spoken words, and calculate the upper triangle based on the first 25 tokens 132
of the mask span, and the lower triangle based on the last 25. We choose to use a span of tokens 133
instead of masking all information after a timestep to prevent global information such as speaker 134
information available to the model changing with respect to mask location. However, this limits us to 135
only generating a diagonal span of probabilties as seen in 1. To extract kregions with boundaries 136
B={b1= 1< b 2< . . . < b k=T+ 1}fromC, we adopt the min-cut algorithm discussed in Peng 137
et al. [46], treating Cas the input feature-similarity matrix: 138
B:= arg min
{b1=1<b2...<b k+1=T+1}kX
t=1bt+1−1P
i=btTP
j=1(Ci,j+Cj,i)−2bt+1−1P
i,j=btCi,j
bt+1−1P
i=btTP
j=1(Ci,j+Cj,i)−bt+1−1P
i,j=btCi,j(2)
By choosing kto be proportional to the length of the utterance, we can control the sample rate of our 139
boundaries. We explore modifying this parameter in-depth throughout our experiments. 140
LossPred is expensive to run due to having repeat forward passes for sliding windows. To make this 141
efficient, we extract multiple masked spans simultaneously with a gap between spans of three seconds. 142
This results in roughly 200 forward passes of the student model to calculate Con an arbitrarily-sized 143
audio. We also preprocess the audio using a unsupervised voice activity dection model [51]. 144
3.2 SylBoost: Bootstrapping Pesudo-Syllabic Units with Iterative Distillation 145
Given the initial boundaries predicted by LossPred, we follow the paradigm of noisy-student-teacher 146
learning [ 55] to iterate and extract better representations. Our goal is to “sharpen” the syllabic 147
organization in the feature space of an input student model that initially results from LossPred, as 148
seen on the right of Figure 1. We choose a pretrained HuBERT [ 28] or Data2Vec2 [ 6] to initialize our 149
student and teacher models, with the teacher model parameters held constant. 150
For a set of hypothesized speech segment boundaries B={b1= 1< b 2< . . . < b k+1=T+ 1}, 151
we group together all temporal tokens between two boundaries into disjoint groups Gi={t|bi≤ 152
t < b i+1}. For notation, we let Htmap from tto its corresponding group: t∈GHt. We apply our 153
loss to the features at layer L, which we select based on syllabic correlation as explored in detail in 154
Pasad et al. [43]. This results in student features X(L)
{1...T}∈Rdand teacher features Y(L)
{1...T}∈Rd155
where dis the feature dimension. 156
Then the loss, which is applied to each token of the student model, is the mean squared error between 157
the student features X(L)
tand the mean of the teacher features in the token’s corresponding group: 158
Z:=1
TTX
t=1
X(L)
t−1
|GHi|X
s∈GHiY(L)
s
2
(3)
4This results in a model with a mean-squared-error feature similarity matrix as depicted in the right 159
side of Figure 1. We then extract boundaries using a cut algorithm described later in Sec. 3.3, although 160
the cut algorithm from Peng et al. [46] also works. With this, we can generate new pseudolabels and 161
iterate the process again to extract better boundaries, which we perform twice. 162
3.3 Efficient Extraction of Unit Boundaries with SylBoost 163
To extract boundary indices from learned feature representations Peng et al. [46] proposed adapting 164
the mincut approach in Malioutov and Barzilay [35]. However, for speech this approach is slow in 165
practice and difficult to parallelize, bottlenecking our ability to extract boundaries in bulk across the 166
large corpora necessary for downstream language modeling. Inspired by the SylBoost objective, we 167
propose a more efficient approach for extraction: given k+ 1potential boundaries, we seek to choose 168
groups that minimize the sum of the distances from each unit to the mean of its assigned group: 169
B:= arg min
{b1=1<b2...<b k+1=T+1}kX
i=1bi+1−1X
j=bi
X(L)
j−1
bi+1−bibi+1−1X
l=biX(L)
l
2
(4)
We further restrict the setting by choosing a maximum group length of Gtokens, where we choose 170
G= 50 to correspond to one second of tokens, as syllables or words longer than this are fairly rare. 171
With this, we can then split our algorithm into 1) calculating a distance array D∈RT×G, where Dt,g 172
is the cost of the group of length gending at token tand then 2) solving the minimal interval cover 173
from this distance array with dynamic programming. An efficient implementation using PyTorch 174
[44] on CUDA [40] runs in O(k)data-aware sequential steps. 175
4 Syllable-LM: Speech Unit Language Modeling Over Syllable-Like Units 176
4.1 Language Model 177
GSLM [ 34] defines a pipeline for modeling raw audio as three stages: 1) Audio-to-unit Tokenization, 178
2) Running a decoder transformer model on these units, and 3) Decoding the tokens back into a 179
waveform. Like AudioLM and TWIST, we use an autoregressive transformer decoder language 180
model to approximate p(xt|xt−1, . . . , x 1)given an input token sequence x1, . . . , x T. We refer to 181
this model as SpeechLM. We train it on clusters which we extract by mean pooling features at layer 182
L, chosen as before, over their boundary groups, followed by K-Means and Agglomerative Clustering 183
to a desired number of discrete units. Like TWIST, we prepend a <BOS> token and make no other 184
special changes. Due to current prevalence of this architecture, we refer to [ 26] for additional details. 185
4.2 Resynthesis and the Vocoder 186
For resynthesis, we adopt the interleaved decoding strategy from Song et al. [50] to output the 187
mHuBERT units from TWIST [ 26], obtaining a waveform by cascading this output into their provided 188
mHuBERT-to-speech vocoder. This interleaving strategy demonstrates superior performance in high- 189
difficulty settings compared to other Neural Codec Lanaugae Models like V ALL-E [ 54], and so we 190
use it for all resynthesis experiments. Although the cascading procedure may produce additional 191
errors, we choose this approach for the following reasons: 192
1.Text-to-speech systems like V ALL-E traditionally start by converting text units into phones 193
using rule-based strategies to improve quality. This indicates that traditional unit-to-speech 194
resynthesis methods might be challenging for our low-bitrate units. 195
2.This pipeline allows for fast experimentation as we can precompute the mHuBERT 25hz 196
units once for all training runs. 197
3. Using the same V ocoder allows for fairer comparisons against TWIST. 198
To interleave our units, we sort on the start-timestep of every pseudo-syllable unit and mHuBERT-unit 199
in ascending order. To decrease the odds of mHuBERT units appearing before the pseudo-syllable 200
unit corresponding to the same ground truth syllable due to errant SylBoost boundaries, we subtract 201
0.08s (the length of two mHuBERT frames) from each pseudo-syllable start time before sorting. For 202
the rest of the pipeline, we follow [ 50] with our syllables as a drop-in replacement for phones. We note 203
5Table 1: Unsupervised Syllable Boundary Detection and Clustering Accuracy on LibriSpeech [ 41]
Test. For F1 scores, the superscript is tolerance threshold in ms. All other metrics use 50ms. Higher
is better.
Approach Backbone Training F150F120Pr. Re. R CPur SPur
Feat-Sim[43] HuBERT no 47.3 24.7 46.6 48.0 54.5 28.0 30.0
LossPred (Ours) HuBERT no 59.6 31.4 54.9 66.7 56.3 - -
SD-HuBERT[12] HuBERT yes 66.1 32.2 64.9 67.4 70.7 43.2 45.0
SylBoost (Ours) HuBERT yes 70.9 40.1 70.8 71.4 75.1 28.9 47.8
SylBoost (Ours) Data2Vec2 yes 73.2 44.6 72.1 74.4 76.9 33.6 54.9
that although our interleaved resynthesis model slows down generation compared to TWIST, most 204
model parameter scaling happens in the SpeechLM. For example, the TWIST paper still observes 205
scaling improvements at 13B parameters while current SOTA TTS models such as [ 29] operate well 206
with fewer than 1B parameters. 207
We then generate continuations for a sample by 1) Extracting syllable-unit and mHuBERT units from 208
the sample, 2) Sampling syllable-unit continuations from the SpeechLM, 3) Continuing mHuBERT 209
units with our interleaved model conditioned on sample mHuBERT units, sample syllable-units, and 210
continued syllable-units, and 4) Resynthesizing these into speech using the vocoder. 211
5 Experiments 212
5.1 Training Datasets 213
We train our tokenizer using LibriSpeech [ 41], which contains 960 hours of audio books. We noticed 214
that the agglomeration procedure described in 3.2 converges before all data is used, and so we 215
randomly subsample LibriSpeech to a 100 hour train set and train for five epochs and two iterations 216
for all experiments. We train our SpeechLMs using all of LibriLight [ 30], which provides roughly 217
55k hours of speech. As a note on fair comparison, although AudioLM uses exactly this split of 218
LibriLight, TWIST collects an additional 100k hours of data, totaling to 155k hours. 219
5.2 Model Details 220
We implement using the OPT [ 59] flavor of models and default to using 12 layers, an embedding 221
dimension of 768, and learned positional embeddings for both our SpeechLM and our Interleaved- 222
V ocoder-LM. This totals to 90M non-embedding parameters, the same as TWIST-125M. We also 223
experiment with a larger 24 layer 1024 dimension model totaling to 300M non-embedding parameters, 224
the same as AudioLM and TWIST-350M. For all pretraining experiments we randomly crop files to 225
25 seconds, use a batch size of 80000 tokens, and train for 200k steps, which amounts to the same 226
compute as in TWIST. To make our approach entirely textless, we do not use TWIST initialization. 227
Additional hyperparameters and hardware details are in Appendix A.2. 228
5.3 Tokenizer Experiments 229
By varying the number of boundaries input to our cut algorithm at each stage in the agglomeration 230
pipeline, we can arbitrarily control our rate of temporal tokenization. We evaluate three main unit- 231
rates at 8.33Hz, 6.25Hz, and 5.00Hz, the latter which matches the empirical rate of SD-HuBERT 232
units on LibriSpeech dev-clean. Combining unit-rates with changing the number of clusters generated 233
by K-Means and Agglomeration gives us fine-grained control of the model bitrate. We note that 234
although SD-HuBERT applies a cut algorithm, this is done after thresholding low-magnitude features 235
that emerge from pretraining. As a result, we find that we cannot control the frequency of SD- 236
HuBERT units by changing parameters of its mincut algorithm becuase additional cuts result in 237
close-to-identical representations that map to the same quantized clusters. 238
From prior work, we compare against the AudioLM tokenizer w2v-BERT [ 14], and the tokenizer 239
from TWIST which is an open-source HuBERT model pretrained for an additional iteration on a large 240
and diverse set of multilingual data, henceforth mHuBERT. Both of these tokenizers operate at 25Hz 241
6Table 2: Unit Resynthesis. WER/CER results on 4-10 second examples on LibriSpeech [ 41] test-clean.
Hz and Bitrate are measured post Run-Length-Encoding (RLE) on LibriSpeech dev-clean.
Model Changes Hz #Units BPS WER ↓CER↓
SD-HuBERT [12] 5.0 4096 60 37.3 22.7
SylBoost (HuBERT) +Our Clustering 5.0 4096 60 18.5 10.2
SylBoost (D2V2) +Use Data2Vec2 5.0 4096 60 12.8 6.4
SylBoost (D2V2) +Increase #Units 5.0 16384 70 9.1 4.3
SylBoost (D2V2) +Tune unit-rate, #Units 8.33 2048 91 8.0 3.7
SylBoost (D2V2) +Tune unit-rate, #Units 6.25 8192 81 7.0 3.2
mHuBERT (upper bound) [26] 19.5 500 175 6.3 2.5
Table 3: Main SyllableLM results. We evaluate on sWUGGY (In-V ocab, All, Out-of-V ocab), sBLIMP
from ZeroSpeech [39], and tStoryCloze from Hassid et al. [26]. Higher is better. *Estimated.
#Data GPU- sWUGGY Semantics
Model Params #Units Hz BPS Toks Hours All IV OOV sBL. tSC
Phone Topline 90M 70 12.5 76 2.5B 70 81.4 95.2 67.7 68.8 80.6
Syllable Topline 90M 28k 5.0 74 1B 70 79.5 93.1 65.9 69.3 76.6
AudioLM [8] 300M 1k 25 250 5B 2.9k* 71.5 83.7 59.3 64.7 -
TWIST [26] 300M 500 19.5 175 9B 295 70.6 80.3 61.0 56.2 69.9
TWIST 1.3B 500 19.5 175 9B 1.1k* 71.8 81.1 62.3 57.0 70.6
TWIST 7B 500 19.5 175 9B 5.9k* 72.7 83.6 61.8 59.0 74.1
TWIST 13B 500 19.5 175 9B 10k* 73.9 84.1 63.7 59.2 76.4
TWIST-CI 90M 500 19.5 175 3.9B 84 69.7 79.8 59.7 55.5 69.0
BPE [49] 90M 4k 9.8 118 2B 84 61.8 66.7 56.8 54.5 56.2
SyllableLM 90M 2k 8.3 91 1.6B 70 72.2 81.7 62.6 62.4 71.4
SyllableLM 90M 8k 6.25 81 1.2B 75 72.1 82.2 61.9 62.9 70.2
SyllableLM 90M 16k 5.0 70 1B 82 67.6 76.9 58.3 63.2 69.0
SyllableLM 300M 8k 6.25 81 1.2B 290 72.2 82.2 62.0 63.7 75.4
followed by Run Length Encoding, which deduplicates repeated units. We additionally reimplement 242
Byte Pair Encoding as done in Shen et al. [49] on the deduplicated mHuBERT units, resulting in the 243
lowest bitrate encoding of speech outside of our model. We grid search and find that the minimum 244
bitrate from BPE is obtained from 4k-16k units and choose 4k units for all experiments (Shen et al. 245
[49] originally operated on 50Hz units, meaning that the 117bps rate obtained here is also new). 246
Because we want to use a 50Hz base encoder to match SD-HuBERT and have fine-grained boundary 247
control during syllable segmentation, we cannot use the 25hz mHuBERT encoder from TWIST. 248
Unfortunately, this means that the quality of the base encoder may be a confounding factor in our 249
SpeechLM evaluation. We choose Data2Vec2-base [ 6] as a middleground for training SpeechLMs on 250
syllable-like units because we find its quality enables lower bitrates than HuBERT, but it is older and 251
trains on less-data than mHuBERT from TWIST, and it has 6x fewer parameters than w2v-BERT, 252
used by AudioLM. We suspect that applying newer encoders like w2v-BERT 2 from Communication 253
et al. [16] could enable even better performance, which we leave to future work. We initialize 254
Data2Vec2 SylBoost from the same HuBERT loss boundaries as discussed in 3.1. 255
5.4 Results: Evaluating Unit Quality 256
We evaluate the quality of our semantic units with two approaches 1) measuring correspondence 257
with syllables and 2) running speech resynthesis followed by ASR. To measure correspondence with 258
syllables, we use the development and test sets of LibriSpeech [ 41] and follow the approach from 259
Peng et al. [46], extracting timesteps for phones on using the Montreal Forced Aligner [ 36] and then 260
converting these phones into syllables with a rule-based method [ 25]. We evaluate the quality of 261
syllable boundary detection with a ground truth boundary marked as hit if a proposed boundary is 262
present within a tolerance threshold. We report F1, Precision, Recall, and R score. We ablate F1 263
scores with tolerance windows of 20ms and 50ms. Given boundaries, we also evaluate the purity of 264
7Table 4: Boundary detaction with
different initialization using Hu-
BERT on LS dev-clean
Model F1 Pr. Re.
Similarity 46.7 48 45
-Iter 1 51.1 50 52
-Iter 2 50.4 51 50
Loss-Corr 60.1 53 68
-Iter 1 67.1 67 68
-Iter 2 70.2 70 70Table 5: Controllability of unit
rate measured on LibriSpeech
dev-clean boundary detection.
D2V2, 50ms threshold. P:Phone,
S:Syllable, W:word
Hz F1-P F1-S F1-W
8.33 72.0 63.5 56.8
6.25 65.2 71.8 66.0
5.0 58.7 73.0 71.8
4.3 54.3 73.2 74.0Table 6: Holding number of
units and unit rate constant.
ZeroSpeech development set.
Hz #T sWU. sBL.
8.33 4k 72.9 61.8
6.25 4k 69.3 63.3
5.00 4k 65.7 62.8
8.33 2k 72.1 62.0
8.33 4k 72.9 61.8
8.33 8k 72.9 61.2
our clusters with 4096 units, with Syllable Purity measuring the probability that a syllable is mapped 265
to its most corresponding cluster unit, and Cluster Purity measuring the probability that a cluster is 266
mapped to its most corresponding syllable unit. 267
Even if units do not correspond with syllables, they can still be of great use to SpeechLMs if they 268
can resynthesize back into speech that matches the original text. Additionally, training a resynthesis 269
model provides a stronger description of the semantic information contained in units than purity 270
metrics, which are especially problematic because SD-HuBERT does not provide a unit at every 271
timestep while our methods do, possibly making cluster and syllable purity evaluation unreliable. 272
To evaluate resynthesized speech, we follow AudioLM and measure Word Error Rate (WER) and 273
Character Error Rate (CER) on the set of 4-10 second segments from LibriSpeech test-clean. For 274
ASR, we follow V ALL-E [54] and use the public HuBERT-base CTC ASR model provided by [28]. 275
Table 1 shows our syllabic correspondence results against the prior-state-of-the-art SD-HuBERT [ 12] 276
and the HuBERT-based feature similarity strategy from [ 46]. Applying our LossPred followed by 277
agglomeration strategy on either HuBERT or Data2Vec2 improves performance across-the-board 278
except for in cluster purity. Although it LossPred SD-HuBERT in performance, it pushes the boundary 279
for syllable recognition using HuBERT without additional training. We justify using LossPred as a 280
bootstrapping source instead of a HuBERT similarity metric [ 46,43] in Table 4, which we discuss 281
more in Appendix A.4. Improvement across iterations and with different loss initialization can be 282
found in Table 4. We explore the effects of changing the unit rate on boundary predictions in Table 5. 283
We compare against prior SpeechLMs and demonstrate the step-by-step changes used to improve 284
unit cluster re-synthesis quality as compared to SD-HuBERT in table 2. We observe over a 50% 285
decrease in WER and CER by applying our method using the SD-HuBERT base parameters. We 286
further decrease WER by a third by using Data2Vec2, and from there by modifying the unit sample 287
rate and number of clusters can reach as low as 2048 clusters and a WER of 7%. These results 288
demonstrate by far the lowest bitrate we are aware of for ‘reasonable-quality’ self-supervised-unit 289
resynthesis. Resynthesis for all models we train is done back into mHuBERT-25Hz units, bounding 290
potential quality at a WER of 6.3%. 291
5.5 Results: Generative Spoken Lanauage Modeling 292
The end-to-end GSLM pipeline is deep, and so it is essential to have metrics to independently 293
evaluate different stages. To evaluate our SpeechLM stage, we follow Lakhotia et al. [34] and use 294
the ZeroSpeech [ 39] sWUGGY and sBLIMP evaluation. The sWUGGY dataset tasks the model 295
with outputting a higher perplexity on similar but fake spoken words (e.g. brick vs blick). Similarly, 296
the sBLIMP dataset checks syntactic correctness (e.g. the dog sleeps vs the dogs sleeps). We also 297
evaluate the SpeechLM on the tSC set from Hassid et al. [26], which operates like the ZeroSpeech 298
metrics on a spoken version of the StoryCloze dataset [ 37] with the last sentence in negative samples 299
randomly chosen. For all metrics we follow prior work and output the mean perplexity per token. 300
The results for SpeechLM metrics are in 3. We reimplement a 90M parameter model using the 301
TWIST mHuBERT units without textually-pretrained initialization (Cold-Init in the TWIST paper) 302
on our data split for an all-else-held equal comparison on unit type. We also train on BPE units as 303
described in 5.3, the next-lowest bitrate units outside of our model. For textual toplines, we train 304
on corresponding LibriLight text transcripts from Kang et al. [31] and convert text to phones and 305
8syllables using the same methods as in Section 5.4. We find that training with each of our syllable 306
units improves perfromance across-the-board on sBLIMP and tSC versus comparably-sized models 307
and is competitive against larger models. In fact, with under 90 hours of training, SyllableLM 308
outperforms even the 13B parameter TWIST on sBLIMP. We also beat AudioLM on the full split 309
of sWUGGY with 30x less GPU compute and TWIST model sizes up to 1.3B parameters. On tSC, 310
we observe that SyllableLM large approaches performance of the textual topline, outperforming all 311
models except for TWIST 13B. Due to compute requirements, we are unable to scale further. 312
Table 7: Continuation Metrics. We measure
PPX@Oracle-VERT and VERT@Oracle-
PPX as implemented in Lakhotia et al. [34]
Model PPX@O-V VERT@O-P
TWIST 300M 205 ±24 24.0±1.0
TWIST 1.3B 175 ±14 22.6±1.2
8.33Hz 2k 90M 159 ±8 15.1±0.9
6.25Hz 8k 90M 139 ±12 20.1±0.7
5.00Hz 16k 90M 131 ±11 15.2±1.0
6.25Hz 8k 300M 116±7 15.8±0.9We notice a decrease in sWUGGY quality with our 313
5.0Hz units, which we suspect is in part caused by 314
the short length of the dataset audios making input to- 315
kenization excessively short. We further ablate these 316
differences in table 6. We also find that BPE, despite 317
having the lowest bitrate outside of our approach, 318
does not approach the quality gains created by our 319
syllable-like units. 320
To measure the quality of end-to-end continuations, 321
we use the VERT@O-PPX and PPX@O-VERT met- 322
rics proposed in Lakhotia et al. [34], which are shown 323
to be the automatic metrics correlating best with hu- 324
man meaningfulness judgements. VERT@O-PPX measures the diversity of output at the sampling 325
temperature where the perplexity of generated audio transcriptions matches that of the ground truth 326
text, and PPX@O-VERT performs the inverse. Like Lakhotia et al. [34], we generate 10-second 327
continuations from 1000 randomly sampled 3-second crops from LibriSpeech test-clean, and measure 328
results using their provided environment and parameters. We report these in Table 7 with two sigma 329
error bars, outperforming TWIST 300M and 1.3B. 330
6 Limitations 331
Though speech is a very general medium, there are a number of challenges in adapting our methods 332
to generate low-bitrate units angled towards other audio tasks or other domains such as vision. 333
Our LossPred technique assumes that the semantic units to learn are separable across time, one- 334
dimensional, and contiguous. In audio tasks or settings with multiple speakers, sounds or words 335
can occur simultaneously and can’t be separated across the time dimension. Images and video 336
are multi-dimensional, not allowing a trivial sliding window approach. Images and video can also 337
have partially occluded or overlapping objects, violating continuity. Furthermore, it is still unclear 338
whether our longer units will be better at scaling to larger datasets, such as the 4.5M hours used by 339
Communication et al. [16]. For example, our semantic units may be losing out on useful paralinguistic 340
features like tone whose impact is only salient on non-audiobooks or at scale. It is also important to 341
note that large textual language models can have harmful effects, such as enabling the generation of 342
misinformation in mass. Although generative spoken language models have not yet caught up to their 343
textual counterparts, it is still necessary to be aware of potential misuses that could arise in the future. 344
7 Conclusion 345
We introduce a new method to tokenize speech for use in GSLMs. We do this by proposing a method 346
to elicit syllabic organization in pretrained speech encoder models, bootstrapping a feature-space 347
agglomeration algorithm from a static analysis of correlations in off-the-shelf teacher and student 348
model losses across time. We demonstrate the success of our technique both in having strong 349
associations with syllables and as an extremlely low-bitrate codec for speech resynthesis. Using this 350
tokenization strategy, we successfully train SyllableLM, a SpeechLM that out-performs comparable 351
state-of-the-art approaches across a diverse range of metrics with a significant inference speedup. 352
We further ablate several design decisions such as quantitization strategy, loss initialization, and 353
the effects of controllability for downstream usecases. Compression is a crucial aspect of learning, 354
and we hope that these significant improvements in the unsupervised learning of low-bitrate speech 355
units can serve as a foundation for approaches towards understanding spoken language and general 356
representation learning. 357
9References 358
[1]A. Agostinelli, T. I. Denk, Z. Borsos, J. Engel, M. Verzetti, A. Caillon, Q. Huang, A. Jansen, A. Roberts, 359
M. Tagliasacchi, M. Sharifi, N. Zeghidour, and C. Frank. Musiclm: Generating music from text, 2023. 360
[2]R. J. Algayres, Y . Adi, T. A. Nguyen, J. Copet, G. Synnaeve, B. Sagot, and E. Dupoux. Generative spoken 361
language model based on continuous word-sized audio tokens, 2023. URL https://openreview.net/ 362
forum?id=a0e7x2EuFO . 363
[3]A. Baevski, H. Zhou, A. Mohamed, and M. Auli. wav2vec 2.0: a framework for self-supervised learning 364
of speech representations. In Proceedings of the 34th International Conference on Neural Information 365
Processing Systems , NIPS ’20, Red Hook, NY , USA, 2020. Curran Associates Inc. ISBN 9781713829546. 366
[4]A. Baevski, W.-N. Hsu, A. Conneau, and M. Auli. Unsupervised speech recognition. In A. Beygelzimer, 367
Y . Dauphin, P. Liang, and J. W. Vaughan, editors, Advances in Neural Information Processing Systems , 368
2021. URL https://openreview.net/forum?id=QmxFsofRvW9 . 369
[5]A. Baevski, W.-N. Hsu, Q. Xu, A. Babu, J. Gu, and M. Auli. data2vec: A general framework for 370
self-supervised learning in speech, vision and language, 2022. 371
[6]A. Baevski, A. Babu, W.-N. Hsu, and M. Auli. Efficient self-supervised learning with contextualized target 372
representations for vision, speech and language. In Proceedings of the 40th International Conference on 373
Machine Learning , ICML’23. JMLR.org, 2023. 374
[7]D. Bolya, C.-Y . Fu, X. Dai, P. Zhang, C. Feichtenhofer, and J. Hoffman. Token merging: Your vit 375
but faster. In The Eleventh International Conference on Learning Representations , 2023. URL https: 376
//openreview.net/forum?id=JroZRaRw7Eu . 377
[8]Z. Borsos, R. Marinier, D. Vincent, E. Kharitonov, O. Pietquin, M. Sharifi, D. Roblek, O. Teboul, 378
D. Grangier, M. Tagliasacchi, and N. Zeghidour. Audiolm: a language modeling approach to audio 379
generation, 2023. 380
[9]T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, 381
A. Askell, S. Agarwal, A. Herbert-V oss, G. Krueger, T. Henighan, R. Child, A. Ramesh, D. Ziegler, J. Wu, 382
C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin, S. Gray, B. Chess, J. Clark, C. Berner, S. McCandlish, 383
A. Radford, I. Sutskever, and D. Amodei. Language models are few-shot learners. In H. Larochelle, M. Ran- 384
zato, R. Hadsell, M. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems , 385
volume 33, pages 1877–1901. Curran Associates, Inc., 2020. URL https://proceedings.neurips. 386
cc/paper_files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf . 387
[10] M. Caron, H. Touvron, I. Misra, H. Jégou, J. Mairal, P. Bojanowski, and A. Joulin. Emerging properties in 388
self-supervised vision transformers. In Proceedings of the International Conference on Computer Vision 389
(ICCV) , 2021. 390
[11] S. Chen, C. Wang, Z. Chen, Y . Wu, S. Liu, Z. Chen, J. Li, N. Kanda, T. Yoshioka, X. Xiao, J. Wu, L. Zhou, 391
S. Ren, Y . Qian, Y . Qian, M. Zeng, and F. Wei. Wavlm: Large-scale self-supervised pre-training for full 392
stack speech processing. IEEE Journal of Selected Topics in Signal Processing , 16:1505–1518, 2021. URL 393
https://api.semanticscholar.org/CorpusID:239885872 . 394
[12] C. J. Cho, A. Mohamed, S.-W. Li, A. W. Black, and G. K. Anumanchipalli. Sd-hubert: Sentence-level 395
self-distillation induces syllabic organization in hubert. 2024. 396
[13] J.-C. Chou, C.-M. Chien, W.-N. Hsu, K. Livescu, A. Babu, A. Conneau, A. Baevski, and M. Auli. Toward 397
joint language modeling for speech units and text. In Conference on Empirical Methods in Natural 398
Language Processing , 2023. URL https://api.semanticscholar.org/CorpusID:264128173 . 399
[14] Y .-A. Chung, Y . Zhang, W. Han, C.-C. Chiu, J. Qin, R. Pang, and Y . Wu. w2v-bert: Combining contrastive 400
learning and masked language modeling for self-supervised speech pre-training. 2021 IEEE Automatic 401
Speech Recognition and Understanding Workshop (ASRU) , pages 244–250, 2021. URL https://api. 402
semanticscholar.org/CorpusID:237048255 . 403
[15] K. Clark, M.-T. Luong, Q. V . Le, and C. D. Manning. Electra: Pre-training text encoders as discriminators 404
rather than generators. In International Conference on Learning Representations , 2020. URL https: 405
//openreview.net/forum?id=r1xMH1BtvB . 406
10[16] S. Communication, L. Barrault, Y .-A. Chung, M. C. Meglioli, D. Dale, N. Dong, M. Duppenthaler, 407
P.-A. Duquenne, B. Ellis, H. Elsahar, J. Haaheim, J. Hoffman, M.-J. Hwang, H. Inaguma, C. Klaiber, 408
I. Kulikov, P. Li, D. Licht, J. Maillard, R. Mavlyutov, A. Rakotoarison, K. R. Sadagopan, A. Ramakrishnan, 409
T. Tran, G. Wenzek, Y . Yang, E. Ye, I. Evtimov, P. Fernandez, C. Gao, P. Hansanti, E. Kalbassi, A. Kallet, 410
A. Kozhevnikov, G. M. Gonzalez, R. S. Roman, C. Touret, C. Wong, C. Wood, B. Yu, P. Andrews, 411
C. Balioglu, P.-J. Chen, M. R. Costa-jussà, M. Elbayad, H. Gong, F. Guzmán, K. Heffernan, S. Jain, J. Kao, 412
A. Lee, X. Ma, A. Mourachko, B. Peloquin, J. Pino, S. Popuri, C. Ropers, S. Saleem, H. Schwenk, A. Sun, 413
P. Tomasello, C. Wang, J. Wang, S. Wang, and M. Williamson. Seamless: Multilingual expressive and 414
streaming speech translation, 2023. 415
[17] J. Copet, F. Kreuk, I. Gat, T. Remez, D. Kant, G. Synnaeve, Y . Adi, and A. D’efossez. Simple and 416
controllable music generation. ArXiv , abs/2306.05284, 2023. URL https://api.semanticscholar. 417
org/CorpusID:259108357 . 418
[18] T. Darcet, M. Oquab, J. Mairal, and P. Bojanowski. Vision transformers need registers. In The Twelfth 419
International Conference on Learning Representations , 2024. URL https://openreview.net/forum? 420
id=2dnO3LLiJ1 . 421
[19] A. Défossez, J. Copet, G. Synnaeve, and Y . Adi. High fidelity neural audio compression. Transactions 422
on Machine Learning Research , 2023. ISSN 2835-8856. URL https://openreview.net/forum?id= 423
ivCd8z8zR2 . Featured Certification, Reproducibility Certification. 424
[20] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. BERT: Pre-training of deep bidirectional transformers 425
for language understanding. In J. Burstein, C. Doran, and T. Solorio, editors, Proceedings of the 2019 426
Conference of the North American Chapter of the Association for Computational Linguistics: Human 427
Language Technologies, Volume 1 (Long and Short Papers) , pages 4171–4186, Minneapolis, Minnesota, 428
June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1423. URL https: 429
//aclanthology.org/N19-1423 . 430
[21] Z. Du, S. Zhang, K. Hu, and S. Zheng. Funcodec: A fundamental, reproducible and integrable open-source 431
toolkit for neural speech codec. ArXiv , abs/2309.07405, 2023. URL https://api.semanticscholar. 432
org/CorpusID:261823065 . 433
[22] A. Elkahky, W.-N. Hsu, P. Tomasello, T.-A. Nguyen, R. Algayres, Y . Adi, J. Copet, E. Dupoux, and 434
A. Mohamed. Do coarser units benefit cluster prediction-based speech pre-training? In ICASSP 2023 - 435
2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) , pages 1–5, 436
2023. doi: 10.1109/ICASSP49357.2023.10096788. 437
[23] T. S. Fuchs and Y . Hoshen. Unsupervised word segmentation using temporal gradient pseudo-labels. In 438
ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) , 439
pages 1–5, 2023. doi: 10.1109/ICASSP49357.2023.10095363. 440
[24] Y . Gong, C.-I. Lai, Y .-A. Chung, and J. R. Glass. Ssast: Self-supervised audio spectrogram transformer. 441
ArXiv , abs/2110.09784, 2021. URL https://api.semanticscholar.org/CorpusID:239024736 . 442
[25] K. Gorman. Syllabify. https://github.com/kylebgorman/syllabify/tree/master , 2014. 443
[26] M. Hassid, T. Remez, T. A. Nguyen, I. Gat, A. Conneau, F. Kreuk, J. Copet, A. Défossez, G. Synnaeve, 444
E. Dupoux, R. Schwartz, and Y . Adi. Textually pretrained speech language models. In Thirty-seventh 445
Conference on Neural Information Processing Systems , 2023. URL https://openreview.net/forum? 446
id=UlHueVjAKr . 447
[27] K. He, X. Chen, S. Xie, Y . Li, P. Dollár, and R. Girshick. Masked autoencoders are scalable vision learners. 448
arXiv:2111.06377 , 2021. 449
[28] W.-N. Hsu, B. Bolte, Y .-H. H. Tsai, K. Lakhotia, R. Salakhutdinov, and A. Mohamed. Hubert: Self- 450
supervised speech representation learning by masked prediction of hidden units. IEEE/ACM Trans. Audio, 451
Speech and Lang. Proc. , 29:3451–3460, oct 2021. ISSN 2329-9290. doi: 10.1109/TASLP.2021.3122291. 452
URL https://doi.org/10.1109/TASLP.2021.3122291 . 453
[29] Z. Ju, Y . Wang, K. Shen, X. Tan, D. Xin, D. Yang, Y . Liu, Y . Leng, K. Song, S. Tang, Z. Wu, T. Qin, 454
X.-Y . Li, W. Ye, S. Zhang, J. Bian, L. He, J. Li, and S. Zhao. Naturalspeech 3: Zero-shot speech 455
synthesis with factorized codec and diffusion models. ArXiv , abs/2403.03100, 2024. URL https: 456
//api.semanticscholar.org/CorpusID:268248388 . 457
11[30] J. Kahn, M. Rivière, W. Zheng, E. Kharitonov, Q. Xu, P. E. Mazaré, J. Karadayi, V . Liptchinsky, 458
R. Collobert, C. Fuegen, T. Likhomanenko, G. Synnaeve, A. Joulin, A. Mohamed, and E. Dupoux. 459
Libri-light: A benchmark for asr with limited or no supervision. In ICASSP 2020 - 2020 IEEE Inter- 460
national Conference on Acoustics, Speech and Signal Processing (ICASSP) , pages 7669–7673, 2020. 461
https://github.com/facebookresearch/libri-light . 462
[31] W. Kang, X. Yang, Z. Yao, F. Kuang, Y . Yang, L. Guo, L. Lin, and D. Povey. Libriheavy: a 50,000 hours 463
asr corpus with punctuation casing and context, 2023. 464
[32] F. Kreuk, G. Synnaeve, A. Polyak, U. Singer, A. D’efossez, J. Copet, D. Parikh, Y . Taigman, and 465
Y . Adi. Audiogen: Textually guided audio generation. ArXiv , abs/2209.15352, 2022. URL https: 466
//api.semanticscholar.org/CorpusID:252668761 . 467
[33] R. Kumar, P. Seetharaman, A. Luebs, I. Kumar, and K. Kumar. High-fidelity audio compression with im- 468
proved rvqgan. ArXiv , abs/2306.06546, 2023. URL https://api.semanticscholar.org/CorpusID: 469
259138883 . 470
[34] K. Lakhotia, E. Kharitonov, W.-N. Hsu, Y . Adi, A. Polyak, B. Bolte, T.-A. Nguyen, J. Copet, A. Baevski, 471
A. Mohamed, and E. Dupoux. On generative spoken language modeling from raw audio. Transactions 472
of the Association for Computational Linguistics , 9:1336–1354, 2021. doi: 10.1162/tacl_a_00430. URL 473
https://aclanthology.org/2021.tacl-1.79 . 474
[35] I. Malioutov and R. Barzilay. Minimum cut model for spoken lecture segmentation. In N. Calzolari, 475
C. Cardie, and P. Isabelle, editors, Proceedings of the 21st International Conference on Computational 476
Linguistics and 44th Annual Meeting of the Association for Computational Linguistics , pages 25–32, 477
Sydney, Australia, July 2006. Association for Computational Linguistics. doi: 10.3115/1220175.1220179. 478
URL https://aclanthology.org/P06-1004 . 479
[36] M. McAuliffe, M. Socolof, S. Mihuc, M. Wagner, and M. Sonderegger. Montreal Forced Aligner: 480
Trainable Text-Speech Alignment Using Kaldi. In Proc. Interspeech 2017 , pages 498–502, 2017. doi: 481
10.21437/Interspeech.2017-1386. 482
[37] N. Mostafazadeh, N. Chambers, X. He, D. Parikh, D. Batra, L. Vanderwende, P. Kohli, and J. Allen. A 483
corpus and cloze evaluation for deeper understanding of commonsense stories. In K. Knight, A. Nenkova, 484
and O. Rambow, editors, Proceedings of the 2016 Conference of the North American Chapter of the 485
Association for Computational Linguistics: Human Language Technologies , pages 839–849, San Diego, 486
California, June 2016. Association for Computational Linguistics. doi: 10.18653/v1/N16-1098. URL 487
https://aclanthology.org/N16-1098 . 488
[38] T. Nguyen, B. Muller, B. Yu, M. R. Costa-jussà, M. Elbayad, S. Popuri, P.-A. Duquenne, R. Algayres, 489
R. Mavlyutov, I. Gat, G. Synnaeve, J. Pino, B. Sagot, and E. Dupoux. Spirit-lm: Interleaved spoken and 490
written language model. ArXiv , abs/2402.05755, 2024. URL https://api.semanticscholar.org/ 491
CorpusID:267547793 . 492
[39] T. A. Nguyen, M. de Seyssel, P. Rozé, M. Rivière, E. Kharitonov, A. Baevski, E. Dunbar, and E. Dupoux. 493
The zero resource speech benchmark 2021: Metrics and baselines for unsupervised spoken language 494
modeling, 2020. 495
[40] NVIDIA, P. Vingelmann, and F. H. Fitzek. Cuda, release: 10.2.89, 2020. URL https://developer. 496
nvidia.com/cuda-toolkit . 497
[41] V . Panayotov, G. Chen, D. Povey, and S. Khudanpur. Librispeech: An asr corpus based on public domain 498
audio books. In 2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) , 499
pages 5206–5210, 2015. doi: 10.1109/ICASSP.2015.7178964. 500
[42] A. Pasad, B. Shi, and K. Livescu. Comparative layer-wise analysis of self-supervised speech models. pages 501
1–5, 06 2023. doi: 10.1109/ICASSP49357.2023.10096149. 502
[43] A. Pasad, C.-M. Chien, S. Settle, and K. Livescu. What Do Self-Supervised Speech Models Know About 503
Words? Transactions of the Association for Computational Linguistics , 12:372–391, 04 2024. ISSN 504
2307-387X. doi: 10.1162/tacl_a_00656. URL https://doi.org/10.1162/tacl_a_00656 . 505
[44] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein, 506
L. Antiga, A. Desmaison, A. Kopf, E. Yang, Z. DeVito, M. Raison, A. Tejani, S. Chilamkurthy, 507
B. Steiner, L. Fang, J. Bai, and S. Chintala. Pytorch: An imperative style, high-performance 508
deep learning library. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d 'Alché-Buc, E. Fox, and 509
R. Garnett, editors, Advances in Neural Information Processing Systems , volume 32. Curran Asso- 510
ciates, Inc., 2019. URL https://proceedings.neurips.cc/paper_files/paper/2019/file/ 511
bdbca288fee7f92f2bfa9f7012727740-Paper.pdf . 512
12[45] P. Peng and D. Harwath. Word discovery in visually grounded, self-supervised speech models. In 513
Interspeech , pages 2823–2827, 09 2022. doi: 10.21437/Interspeech.2022-10652. 514
[46] P. Peng, S.-W. Li, O. Räsänen, A. Mohamed, and D. Harwath. Syllable segmentation and cross-lingual 515
generalization in a visually grounded, self-supervised speech model. In Interspeech , 2023. 516
[47] P. Peng, P.-Y . B. Huang, D. Li, A. Mohamed, and D. F. Harwath. V oicecraft: Zero-shot speech editing and 517
text-to-speech in the wild. ArXiv , abs/2403.16973, 2024. URL https://api.semanticscholar.org/ 518
CorpusID:268681356 . 519
[48] A. Polyak, Y . Adi, J. Copet, E. Kharitonov, K. Lakhotia, W.-N. Hsu, A. Mohamed, and E. Dupoux. Speech 520
Resynthesis from Discrete Disentangled Self-Supervised Representations. In Proc. Interspeech 2021 , 521
2021. 522
[49] F. Shen, Y . Guo, C. Du, X. Chen, and K. Yu. Acoustic bpe for speech generation with discrete tokens. In 523
ICASSP 2024 - 2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) , 524
pages 11746–11750, 2024. doi: 10.1109/ICASSP48485.2024.10446063. 525
[50] Y . Song, Z. Chen, X. Wang, Z. Ma, and X. Chen. Ella-v: Stable neural codec language modeling with 526
alignment-guided sequence reordering, 2024. 527
[51] Z.-H. Tan, A. kr. Sarkar, and N. Dehak. rvad: An unsupervised segment-based robust voice activ- 528
ity detection method. Computer Speech Language , 59:1–21, 2020. ISSN 0885-2308. doi: https: 529
//doi.org/10.1016/j.csl.2019.06.005. URL https://www.sciencedirect.com/science/article/ 530
pii/S0885230819300920 . 531
[52] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozière, N. Goyal, E. Hambro, 532
F. Azhar, A. Rodriguez, A. Joulin, E. Grave, and G. Lample. Llama: Open and efficient foundation lan- 533
guage models. ArXiv , abs/2302.13971, 2023. URL https://api.semanticscholar.org/CorpusID: 534
257219404 . 535
[53] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. u. Kaiser, and I. Polosukhin. 536
Attention is all you need. In I. Guyon, U. V . Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, 537
and R. Garnett, editors, Advances in Neural Information Processing Systems , volume 30. Curran As- 538
sociates, Inc., 2017. URL https://proceedings.neurips.cc/paper_files/paper/2017/file/ 539
3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf . 540
[54] C. Wang, S. Chen, Y . Wu, Z. Zhang, L. Zhou, S. Liu, Z. Chen, Y . Liu, H. Wang, J. Li, L. He, S. Zhao, and 541
F. Wei. Neural codec language models are zero-shot text to speech synthesizers, 2023. 542
[55] Q. Xie, M.-T. Luong, E. Hovy, and Q. V . Le. Self-training with noisy student improves imagenet 543
classification. In 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , pages 544
10684–10695, 2020. doi: 10.1109/CVPR42600.2020.01070. 545
[56] D. Yang, S. Liu, R. Huang, J. Tian, C. Weng, and Y . Zou. Hifi-codec: Group-residual vector quantization 546
for high fidelity audio codec. ArXiv , abs/2305.02765, 2023. URL https://api.semanticscholar. 547
org/CorpusID:258479750 . 548
[57] D. Yang, J. Tian, X. Tan, R. Huang, S. Liu, X. Chang, J. Shi, S. Zhao, J. Bian, X. Wu, Z. Zhao, S. Watanabe, 549
and H. Meng. Uniaudio: An audio foundation model toward universal audio generation, 2023. 550
[58] N. Zeghidour, A. Luebs, A. Omran, J. Skoglund, and M. Tagliasacchi. Soundstream: An end-to-end neural 551
audio codec, 2021. 552
[59] S. Zhang, S. Roller, N. Goyal, M. Artetxe, M. Chen, S. Chen, C. Dewan, M. T. Diab, X. Li, X. V . 553
Lin, T. Mihaylov, M. Ott, S. Shleifer, K. Shuster, D. Simig, P. S. Koura, A. Sridhar, T. Wang, and 554
L. Zettlemoyer. Opt: Open pre-trained transformer language models. ArXiv , abs/2205.01068, 2022. URL 555
https://api.semanticscholar.org/CorpusID:248496292 . 556
[60] X. Zhang, D. Zhang, S. Li, Y . Zhou, and X. Qiu. Speechtokenizer: Unified speech tokenizer for speech 557
large language models. ArXiv , abs/2308.16692, 2023. URL https://api.semanticscholar.org/ 558
CorpusID:261394297 . 559
13A Appendix / supplemental material 560
A.1 Randomly Sampled Example Segmentations 561
We provide randomly sampled example segmentations from the LibriSpeech [ 41] dev-clean set. 562
All models are the second iteration of Data2Vec2, which we use for our SyllableLM experiments 563
in Section 5.5. Top: Feature Self-Similarity matrix, darker green is closer. Segmented cuts span 564
vertically in blue from the top, ground truth boundaries span vertically in red at the bottom. Bottom: 565
time-aligned Mel-Spectrogram. We call attention to the interesting behavior of global correspondences 566
appearing when words or syllables are repeated. Best viewed zoomed in. 567
8.33Hz
 6.25Hz
 5.0Hz
1415A.2 Hardware And Hyperparameters 568
We implement all experiments using NVIDIA A40 46GB GPUS with a Intel Xeon Gold 6226R CPU 569
@ 2.90GHz. Estimated speeds are made using these results as well as scaling from Zhang et al. [59]. 570
Hyperparameters for pretraining our models are below. We note that the Batch Size is in terms of 571
tokens, which means that higher unit rates will have fewer seconds of raw audio per batch to keep 572
GPU compute roughly equal per model. 573
Table 8: Speech pre-training hyper-parameters.
SyllableLM Base SyllableLM Large
Layers 12 24
Embed Dim 768 1024
MLP Dim 3072 4096
GPUs 2 4
Learning rate 2×10−42×10−4
Adam β1/β2 0.9 / 0.98 0.9 / 0.98
Weight decay 0.01 0.01
Learning rate schedule Linear Decay Linear Decay
Dropout 0.1 0.1
LayerDrop 0.0 0.0
Warmup updates 8,000 16,000
Batch size (tokens) 80,000 80,000
Updates 200,000 200,000
Position Embeddings Learned Learned
A.3 Speedup 574
Table 9: Inference speed results, measured in Real-Time-Factor, the processed seconds per second.
We use 32 Batches with 25 seconds of audio each, which matches the length of our training data. 1
GPU, 16 Cores. Standard error less than 1 sec/sec
Encoder Real-Time-Factor ↑
SD-HuBERT [12] 368
HuBERT+MinCut [46] 88
HuBERT+MinSum 3.3 488
SpeechLM, 100M, Cached Units
TWIST 7.8k
Ours 6.25Hz 8k 34.7k
A.4 Discussion: Other Bootstrapping Strategies 575
Of course, there already exist several strategies for unsupervised syllable and word segmentation such 576
as Fuchs and Hoshen [23] and Pasad et al. [43] that could be used to bootstrap our first pseudolabels. 577
We find however in our experiments that these approaches, which are calculated using the similarity or 578
dissimilarity of HuBERT embeddings across time, converge to a lower quality in bootstrapping than 579
our proposed method. We suspect that this may be caused by the fact that although the representations 580
of these models correlate with boundaries, there is no modeling in the pretraining loss pushing the 581
representations to linearly separate across semantic differences. Meanwhile, the loss is forced to 582
change across semantic boundaries due to the difficulty of language modeling, albeit noisily. 583
16A.5 Sample Continuations 584
Below are sample continuations generated with temperature sampling parameter chosen to best match 585
Oracle VERT diversity scores. We provide continuations of roughly 3 seconds of audio, sampled 586
randomly from LibriSpeech test-clean. This text is given as output by our HuBERT ASR Model from 587
[28], with transcription errors present and with no additional modifications. The source text is bolded, 588
and sometimes cuts off mid-word, which can behave differently per sample based on unit rate and 589
quantization artifacts. 590
Model Continuation
TWIST 300M HE FILLED FOR AND FOUND THE WIZARD’S BLACK
CLOTH THE SQUIRE WAS QUITE DEVOTED TO CAR-
RYING IT IN THE PARKS I COULD TELL YOU THAT HE
RATHER LOVED ALL THE WIZARDS IN THE MATTER
OF HONEST FIGUROUS TEMPERED MEN THEN IMMEDI-
ATELY AND BECAUSE HE DID
TWIST 1.3B HE FILLET FOR AND FOUND THE WIZARD’S BLACK
CLOTH THE SQUIRE WAS QUICK IN MOVEMENT A
WHEEL OF HIS JOURNEY MADE HIM THE IMPRESSION
OF HARE’S BLOODSHED THERE WAS NO HELP FOR IT
HE SURMISED AND HE DID SO
SyllableLM 2k 8.33Hz 90M HE FILLED FOR AND FOUND THE WIZARD’S BLACK
CLOTH THE SQUIRE WAS GLAD THAT HIS LAD HAD
NOT RETURNED TO KING LENO FOR IT IS PROBABLE
THAT HE WOULD HA VE BECOME SO UNWELL THAT HE
COULD NOT HA VE SPOKEN
SyllableLM 8k 6.25Hz 300M HE FILT FOR AND FOUND THE WIZARD’S BLACK
CLOTH THE SQUIRE WAS HALF WAY DOWN THE PATH
WITH THE WIZARD BUT HE STOPPED SHORT BEFORE
THE GATE CLUTCHED HIS FISTS SEIZED HIS GOWN AND
Model Continuation
TWIST 300M WHEN THEY WERE OUTSIDE UM SIMPLY LATCHED
THE DOOR AND STARTED UP THE STAIRS AND EVEN
WENT INTO THE LITTLE HOUSE AROSE IN THE SETTING
SUN SET FORTH THE PLACES WHERE PEOPLE LIVED
FROM MON HONEYSUCKLE HANNEY
TWIST 1.3B WHEN THEY WERE OUTSIDE UM SIMPLY LATCHED
THE DOOR AND STARTED UP THE TURNPIKE OL HAT
DON KILL ME THE SLING DE IN YORN ME WEA VING
OUT CHARLEYS SENSE EXAMINED WHAT HE MADE
EXAMINES
SyllableLM 2k 8.33Hz 90M WHEN THEY WERE OUTSIDE UM SIMPLY LATCHED
THE DOOR AND STARTED WALKING IN THEY WERE
TOO OLD TO CARE MUCH ABOUT GOING HOME THEIR
RELATIVES LEFT
SyllableLM 8k 6.25Hz 300M WHEN THEY WERE OUTSIDE UM SIMPLY LATCHED
THE DOOR AND STARTED SLOWLY DOWN THE CORRI-
DOR AND MISSUS BAKER WALKED BESIDE THEODORA
THEY WERE NEAR THE OUTER DOOR WHERE
17Model Continuation
TWIST 300M DO BE OR NOT TO BE THAT IS THE QUESTION
WHETHER TIS NO BODY SIBL LINE IN OTHER SHIRTS
OR CHOCOLATE NOS MICOTTON BUTTER WHAT WE
WERE DO WE SEE THESE HITS WE’VE GOT THE GHOST
HERE THEY’RE LOOKING
TWIST 1.3B DO BE OR NOT TO BE THAT IS THE QUESTION
WHETHER TIS NO GOOD EITHER THAN TO GO
THROUGH THE JUDGMENT OF GAUL AND YOUR DOC-
TRINES THE LORD YOUR GOD AND YOUR GOSPEL IN
RESPECT OF THE POWER OF THIS
SyllableLM 2k 8.33Hz 90M DO BE OR NOT TO BE THAT IS THE QUESTION
WHETHER TIS NO OTHER THAN ESO’S OWN DESTINY
YOU SEE IT IS A LA MISTER PRIOR THAT THIS IS THE
CASE
SyllableLM 8k 6.25Hz 300M DO BE OR NOT TO BE THAT IS THE QUESTION
WHETHER TIS NO REGRET OR NO PLEASURE THAT
MAY BE RUSHED INTO ACTION AT ONCE WITH THE
GREATEST EAGERNESS OF IMPULSE AND ELASTICITY
OF HEART
Model Continuation
TWIST 300M HE IS CALLED AS YOU KNOW THE APOSTLE OF THE
INDI AN KING WHO IS SO GLORIOUS AND ACTING WHY
IS THE OTHER PRINCE NOT BELIEVED BY HIM IN EVERY
FAITH THAT IS FREE WILL EXCEPT WHEN HE
TWIST 1.3B HE IS CALLED AS YOU KNOW THE APOSTLE OF
THE INDI ES SAW WHAT HAD PASSED THROUGH
HIM LATER IN ANOTHER BOOK AMONG THOSE WHO
HAD ENGRA VED IT THIS VOLUME MISTER PICKWICK
THOUGHT IT RIGHT NOT TO INSULT YOU
SyllableLM 2k 8.33Hz 90M HE IS CALLED AS YOU KNOW THE APOSTLE OF THE
INVID ISIBLE BEFORE THEY RECEIVED THE GRACE OF
GODAD CHRIST THEN HAD IN THE FAITH OF HIS SON
SyllableLM 8k 6.25Hz 300M HE IS CALLED AS YOU KNOW THE APOSTLE OF THE
INDI ES HE IS THE FORERUNNER OF TEACHING AND
FAR BEYOND IT HE IS THE EXACT SCIENTIST WHO
MEASURES THE MOVE
591
18NeurIPS Paper Checklist 592
1.Claims 593
Question: Do the main claims made in the abstract and introduction accurately reflect the 594
paper’s contributions and scope? 595
Answer: [Yes] 596
Justification: We make concrete claims in the abstract and introduction regarding the 597
performance of our low bitrate units and SpeechLM results. We address each of these 598
explicitly within the experiments. 599
Guidelines: 600
•The answer NA means that the abstract and introduction do not include the claims 601
made in the paper. 602
•The abstract and/or introduction should clearly state the claims made, including the 603
contributions made in the paper and important assumptions and limitations. A No or 604
NA answer to this question will not be perceived well by the reviewers. 605
•The claims made should match theoretical and experimental results, and reflect how 606
much the results can be expected to generalize to other settings. 607
•It is fine to include aspirational goals as motivation as long as it is clear that these goals 608
are not attained by the paper. 609
2.Limitations 610
Question: Does the paper discuss the limitations of the work performed by the authors? 611
Answer: [Yes] 612
Justification: We provide a Limitations Section to address the assumptions we make for our 613
LossPred algorithm and on the nascent nature of SpeechLMs and their relation to scaling. 614
We explicitly describe why we choose the models we do, and acknowledge where in the 615
field it is hard to draw all-else-held-equal experiments. 616
Guidelines: 617
•The answer NA means that the paper has no limitation while the answer No means that 618
the paper has limitations, but those are not discussed in the paper. 619
• The authors are encouraged to create a separate "Limitations" section in their paper. 620
•The paper should point out any strong assumptions and how robust the results are to 621
violations of these assumptions (e.g., independence assumptions, noiseless settings, 622
model well-specification, asymptotic approximations only holding locally). The authors 623
should reflect on how these assumptions might be violated in practice and what the 624
implications would be. 625
•The authors should reflect on the scope of the claims made, e.g., if the approach was 626
only tested on a few datasets or with a few runs. In general, empirical results often 627
depend on implicit assumptions, which should be articulated. 628
•The authors should reflect on the factors that influence the performance of the approach. 629
For example, a facial recognition algorithm may perform poorly when image resolution 630
is low or images are taken in low lighting. Or a speech-to-text system might not be 631
used reliably to provide closed captions for online lectures because it fails to handle 632
technical jargon. 633
•The authors should discuss the computational efficiency of the proposed algorithms 634
and how they scale with dataset size. 635
•If applicable, the authors should discuss possible limitations of their approach to 636
address problems of privacy and fairness. 637
•While the authors might fear that complete honesty about limitations might be used by 638
reviewers as grounds for rejection, a worse outcome might be that reviewers discover 639
limitations that aren’t acknowledged in the paper. The authors should use their best 640
judgment and recognize that individual actions in favor of transparency play an impor- 641
tant role in developing norms that preserve the integrity of the community. Reviewers 642
will be specifically instructed to not penalize honesty concerning limitations. 643
3.Theory Assumptions and Proofs 644
19Question: For each theoretical result, does the paper provide the full set of assumptions and 645
a complete (and correct) proof? 646
Answer: [NA] 647
Justification: Although we provide new algorithms, we justify these with experiments 648
instead of rigorous theory, as done in prior work like [26] 649
Guidelines: 650
• The answer NA means that the paper does not include theoretical results. 651
•All the theorems, formulas, and proofs in the paper should be numbered and cross- 652
referenced. 653
•All assumptions should be clearly stated or referenced in the statement of any theorems. 654
•The proofs can either appear in the main paper or the supplemental material, but if 655
they appear in the supplemental material, the authors are encouraged to provide a short 656
proof sketch to provide intuition. 657
•Inversely, any informal proof provided in the core of the paper should be complemented 658
by formal proofs provided in appendix or supplemental material. 659
• Theorems and Lemmas that the proof relies upon should be properly referenced. 660
4.Experimental Result Reproducibility 661
Question: Does the paper fully disclose all the information needed to reproduce the main ex- 662
perimental results of the paper to the extent that it affects the main claims and/or conclusions 663
of the paper (regardless of whether the code and data are provided or not)? 664
Answer: [Yes] 665
Justification: We commit throughout our work to being entirely reproducable and open 666
source. We explicitly cite all architectures used, their hyperparameters, all datasets used, and 667
all models used to calculate metrics. We plan on releasing all SyllablLM model checkpoints 668
and code, and all datasets used are fully open-source. 669
Guidelines: 670
• The answer NA means that the paper does not include experiments. 671
•If the paper includes experiments, a No answer to this question will not be perceived 672
well by the reviewers: Making the paper reproducible is important, regardless of 673
whether the code and data are provided or not. 674
•If the contribution is a dataset and/or model, the authors should describe the steps taken 675
to make their results reproducible or verifiable. 676
•Depending on the contribution, reproducibility can be accomplished in various ways. 677
For example, if the contribution is a novel architecture, describing the architecture fully 678
might suffice, or if the contribution is a specific model and empirical evaluation, it may 679
be necessary to either make it possible for others to replicate the model with the same 680
dataset, or provide access to the model. In general. releasing code and data is often 681
one good way to accomplish this, but reproducibility can also be provided via detailed 682
instructions for how to replicate the results, access to a hosted model (e.g., in the case 683
of a large language model), releasing of a model checkpoint, or other means that are 684
appropriate to the research performed. 685
•While NeurIPS does not require releasing code, the conference does require all submis- 686
sions to provide some reasonable avenue for reproducibility, which may depend on the 687
nature of the contribution. For example 688
(a)If the contribution is primarily a new algorithm, the paper should make it clear how 689
to reproduce that algorithm. 690
(b)If the contribution is primarily a new model architecture, the paper should describe 691
the architecture clearly and fully. 692
(c)If the contribution is a new model (e.g., a large language model), then there should 693
either be a way to access this model for reproducing the results or a way to reproduce 694
the model (e.g., with an open-source dataset or instructions for how to construct 695
the dataset). 696
20(d)We recognize that reproducibility may be tricky in some cases, in which case 697
authors are welcome to describe the particular way they provide for reproducibility. 698
In the case of closed-source models, it may be that access to the model is limited in 699
some way (e.g., to registered users), but it should be possible for other researchers 700
to have some path to reproducing or verifying the results. 701
5.Open access to data and code 702
Question: Does the paper provide open access to the data and code, with sufficient instruc- 703
tions to faithfully reproduce the main experimental results, as described in supplemental 704
material? 705
Answer: [Yes] 706
Justification: We plan on open sourcing all code for training and evaluation, including 707
model parameter checkpoints. All datasets used are open source, and instructions for 708
downloading them can be found as cited. For training transformers, we use FAIRSEQ 709
https://github.com/facebookresearch/fairseq . For Unit Resynthesis, we adapt 710
the code from https://github.com/jasonppy/syllable-discovery . For SpeechLM 711
Evaluation, we use methods described at ?]. As with most in-reserach code, absolute file 712
paths and local environment modifications prohibit the code from being both deanonymized 713
and in a runnable state at the current time as suggested by https://nips.cc/public/ 714
guides/CodeSubmissionPolicy . 715
Guidelines: 716
• The answer NA means that paper does not include experiments requiring code. 717
•Please see the NeurIPS code and data submission guidelines ( https://nips.cc/ 718
public/guides/CodeSubmissionPolicy ) for more details. 719
•While we encourage the release of code and data, we understand that this might not be 720
possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not 721
including code, unless this is central to the contribution (e.g., for a new open-source 722
benchmark). 723
•The instructions should contain the exact command and environment needed to run to 724
reproduce the results. See the NeurIPS code and data submission guidelines ( https: 725
//nips.cc/public/guides/CodeSubmissionPolicy ) for more details. 726
•The authors should provide instructions on data access and preparation, including how 727
to access the raw data, preprocessed data, intermediate data, and generated data, etc. 728
•The authors should provide scripts to reproduce all experimental results for the new 729
proposed method and baselines. If only a subset of experiments are reproducible, they 730
should state which ones are omitted from the script and why. 731
•At submission time, to preserve anonymity, the authors should release anonymized 732
versions (if applicable). 733
•Providing as much information as possible in supplemental material (appended to the 734
paper) is recommended, but including URLs to data and code is permitted. 735
6.Experimental Setting/Details 736
Question: Does the paper specify all the training and test details (e.g., data splits, hyper- 737
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the 738
results? 739
Answer: [Yes] 740
Justification: We specify all hyperparameters for model training and evaluation throughout 741
the paper and in the appendix. All data splits are explicit and referenced in their tables. 742
Guidelines: 743
• The answer NA means that the paper does not include experiments. 744
•The experimental setting should be presented in the core of the paper to a level of detail 745
that is necessary to appreciate the results and make sense of them. 746
•The full details can be provided either with the code, in appendix, or as supplemental 747
material. 748
7.Experiment Statistical Significance 749
21Question: Does the paper report error bars suitably and correctly defined or other appropriate 750
information about the statistical significance of the experiments? 751
Answer: [No] 752
Justification: All datasets experimented on contain at least several thousand examples, 753
meaning that experimental significance results would be redundant and insignificant relative 754
to their metrics. Because of this, we follow prior published work for each experiment and 755
do not report these significance metrics. We also do not have the GPU resources to attempt 756
significance errors across multiple training runs or random seeds, as even our smallest 757
models taking 80 hours to train. 758
Guidelines: 759
• The answer NA means that the paper does not include experiments. 760
•The authors should answer "Yes" if the results are accompanied by error bars, confi- 761
dence intervals, or statistical significance tests, at least for the experiments that support 762
the main claims of the paper. 763
•The factors of variability that the error bars are capturing should be clearly stated (for 764
example, train/test split, initialization, random drawing of some parameter, or overall 765
run with given experimental conditions). 766
•The method for calculating the error bars should be explained (closed form formula, 767
call to a library function, bootstrap, etc.) 768
• The assumptions made should be given (e.g., Normally distributed errors). 769
•It should be clear whether the error bar is the standard deviation or the standard error 770
of the mean. 771
•It is OK to report 1-sigma error bars, but one should state it. The authors should 772
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis 773
of Normality of errors is not verified. 774
•For asymmetric distributions, the authors should be careful not to show in tables or 775
figures symmetric error bars that would yield results that are out of range (e.g. negative 776
error rates). 777
•If error bars are reported in tables or plots, The authors should explain in the text how 778
they were calculated and reference the corresponding figures or tables in the text. 779
8.Experiments Compute Resources 780
Question: For each experiment, does the paper provide sufficient information on the com- 781
puter resources (type of compute workers, memory, time of execution) needed to reproduce 782
the experiments? 783
Answer: [Yes] 784
Justification: We mention our system setup in the Appendix in A.2. We include GPU hours 785
used for all main SpeechLM results, and make experiments significantly cheaper than prior 786
work like Borsos et al. [8], Hassid et al. [26]. 787
Guidelines: 788
• The answer NA means that the paper does not include experiments. 789
•The paper should indicate the type of compute workers CPU or GPU, internal cluster, 790
or cloud provider, including relevant memory and storage. 791
•The paper should provide the amount of compute required for each of the individual 792
experimental runs as well as estimate the total compute. 793
•The paper should disclose whether the full research project required more compute 794
than the experiments reported in the paper (e.g., preliminary or failed experiments that 795
didn’t make it into the paper). 796
9.Code Of Ethics 797
Question: Does the research conducted in the paper conform, in every respect, with the 798
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines ? 799
Answer: [Yes] 800
22Justification: All datasets used for training and experiments use open-source and licensed 801
data. We do not use Human Judges. We focus purely on generating semantic continuations, 802
making our approach entirely orthogonal to generating realistic data that can mimic speaker 803
voices such as Wang et al. [54] 804
Guidelines: 805
•The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. 806
•If the authors answer No, they should explain the special circumstances that require a 807
deviation from the Code of Ethics. 808
•The authors should make sure to preserve anonymity (e.g., if there is a special consid- 809
eration due to laws or regulations in their jurisdiction). 810
10.Broader Impacts 811
Question: Does the paper discuss both potential positive societal impacts and negative 812
societal impacts of the work performed? 813
Answer: [Yes] 814
Justification: Generative Spoken Language Models have not yet caught up to their textual 815
counterparts, however it is still important to note that with increased scaling and methods 816
research GSLM systems may eventually be able to reach parity with today’s systems in 817
terms of quality. Because of this, we address the ethical considerations of generative models 818
in?? 819
Guidelines: 820
• The answer NA means that there is no societal impact of the work performed. 821
•If the authors answer NA or No, they should explain why their work has no societal 822
impact or why the paper does not address societal impact. 823
•Examples of negative societal impacts include potential malicious or unintended uses 824
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations 825
(e.g., deployment of technologies that could make decisions that unfairly impact specific 826
groups), privacy considerations, and security considerations. 827
•The conference expects that many papers will be foundational research and not tied 828
to particular applications, let alone deployments. However, if there is a direct path to 829
any negative applications, the authors should point it out. For example, it is legitimate 830
to point out that an improvement in the quality of generative models could be used to 831
generate deepfakes for disinformation. On the other hand, it is not needed to point out 832
that a generic algorithm for optimizing neural networks could enable people to train 833
models that generate Deepfakes faster. 834
•The authors should consider possible harms that could arise when the technology is 835
being used as intended and functioning correctly, harms that could arise when the 836
technology is being used as intended but gives incorrect results, and harms following 837
from (intentional or unintentional) misuse of the technology. 838
•If there are negative societal impacts, the authors could also discuss possible mitigation 839
strategies (e.g., gated release of models, providing defenses in addition to attacks, 840
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from 841
feedback over time, improving the efficiency and accessibility of ML). 842
11.Safeguards 843
Question: Does the paper describe safeguards that have been put in place for responsible 844
release of data or models that have a high risk for misuse (e.g., pretrained language models, 845
image generators, or scraped datasets)? 846
Answer: [NA] 847
Justification: The current quality of GSLM systems trails far behind language models in 848
terms of their capabilities for misuse, and this work instead focuses toward the direction of 849
discovering better representation learning algorithms. All output audio comes from a single 850
speaker, and our approach is orthogonal to voice conversion methods. 851
Guidelines: 852
• The answer NA means that the paper poses no such risks. 853
23•Released models that have a high risk for misuse or dual-use should be released with 854
necessary safeguards to allow for controlled use of the model, for example by requiring 855
that users adhere to usage guidelines or restrictions to access the model or implementing 856
safety filters. 857
•Datasets that have been scraped from the Internet could pose safety risks. The authors 858
should describe how they avoided releasing unsafe images. 859
•We recognize that providing effective safeguards is challenging, and many papers do 860
not require this, but we encourage authors to take this into account and make a best 861
faith effort. 862
12.Licenses for existing assets 863
Question: Are the creators or original owners of assets (e.g., code, data, models), used in 864
the paper, properly credited and are the license and terms of use explicitly mentioned and 865
properly respected? 866
Answer: Yes 867
Justification: All works used have licenses that do not require citing and are available for 868
both research and commercial use. We properly cite every dataset used when mentioned in 869
our work. 870
Guidelines: 871
• The answer NA means that the paper does not use existing assets. 872
• The authors should cite the original paper that produced the code package or dataset. 873
•The authors should state which version of the asset is used and, if possible, include a 874
URL. 875
• The name of the license (e.g., CC-BY 4.0) should be included for each asset. 876
•For scraped data from a particular source (e.g., website), the copyright and terms of 877
service of that source should be provided. 878
•If assets are released, the license, copyright information, and terms of use in the 879
package should be provided. For popular datasets, paperswithcode.com/datasets 880
has curated licenses for some datasets. Their licensing guide can help determine the 881
license of a dataset. 882
•For existing datasets that are re-packaged, both the original license and the license of 883
the derived asset (if it has changed) should be provided. 884
•If this information is not available online, the authors are encouraged to reach out to 885
the asset’s creators. 886
13.New Assets 887
Question: Are new assets introduced in the paper well documented and is the documentation 888
provided alongside the assets? 889
Answer: [Yes] 890
Justification: All models created and trained are standard transformer models, which have 891
been robustly documented for ease-of-use. Our model parameters can be dropped in to 892
existing model pipelines such as that of Zhang et al. [59] on online distribution services 893
such as https://huggingface.co/ 894
Guidelines: 895
• The answer NA means that the paper does not release new assets. 896
•Researchers should communicate the details of the dataset/code/model as part of their 897
submissions via structured templates. This includes details about training, license, 898
limitations, etc. 899
•The paper should discuss whether and how consent was obtained from people whose 900
asset is used. 901
•At submission time, remember to anonymize your assets (if applicable). You can either 902
create an anonymized URL or include an anonymized zip file. 903
14.Crowdsourcing and Research with Human Subjects 904
24Question: For crowdsourcing experiments and research with human subjects, does the paper 905
include the full text of instructions given to participants and screenshots, if applicable, as 906
well as details about compensation (if any)? 907
Answer: [NA] 908
Justification: The paper does not involve crowdsourcing nor research with human subjects. 909
Guidelines: 910
•The answer NA means that the paper does not involve crowdsourcing nor research with 911
human subjects. 912
•Including this information in the supplemental material is fine, but if the main contribu- 913
tion of the paper involves human subjects, then as much detail as possible should be 914
included in the main paper. 915
•According to the NeurIPS Code of Ethics, workers involved in data collection, curation, 916
or other labor should be paid at least the minimum wage in the country of the data 917
collector. 918
15.Institutional Review Board (IRB) Approvals or Equivalent for Research with Human 919
Subjects 920
Question: Does the paper describe potential risks incurred by study participants, whether 921
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) 922
approvals (or an equivalent approval/review based on the requirements of your country or 923
institution) were obtained? 924
Answer: [NA] 925
Justification: The paper does not involve crowdsourcing nor research with human subjects. 926
Guidelines: 927
•The answer NA means that the paper does not involve crowdsourcing nor research with 928
human subjects. 929
•Depending on the country in which research is conducted, IRB approval (or equivalent) 930
may be required for any human subjects research. If you obtained IRB approval, you 931
should clearly state this in the paper. 932
•We recognize that the procedures for this may vary significantly between institutions 933
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the 934
guidelines for their institution. 935
•For initial submissions, do not include any information that would break anonymity (if 936
applicable), such as the institution conducting the review. 937
25