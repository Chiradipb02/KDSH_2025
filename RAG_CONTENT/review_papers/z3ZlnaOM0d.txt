Published in Transactions on Machine Learning Research (01/2024)
Hyperspherical Prototype Node Clustering
Jitao Lu dianlujitao@gmail.com
School of Computer Science, School of Artificial Intelligence, OPtics and ElectroNics (iOPEN),
Northwestern Polytechnical University
Danyang Wu danyangwu.cs@gmail.com
State Key Laboratory for Manufacturing Systems Engineering, School of Electronic and Information Engineering,
Xi’an Jiaotong University
Feiping Nie∗feipingnie@gmail.com
School of Artificial Intelligence, OPtics and ElectroNics (iOPEN), Northwestern Polytechnical University
Rong Wang wangrong07@tsinghua.org.cn
School of Artificial Intelligence, OPtics and ElectroNics (iOPEN), Northwestern Polytechnical University
Xuelong Li li@nwpu.edu.cn
School of Artificial Intelligence, OPtics and ElectroNics (iOPEN), Northwestern Polytechnical University
Reviewed on OpenReview: https: // openreview. net/ forum? id= z3ZlnaOM0d
Abstract
Thegeneralworkflowofdeepnodeclusteringistoencodethenodesintonodeembeddingsvia
graphneuralnetworksanduncoverclusteringdecisionsfromthem, soclusteringperformance
is heavily affected by the embeddings. However, existing works only consider preserving the
semantics of the graph but ignore the inter-cluster separability of the nodes, so there’s no
guarantee that the embeddings can present a clear clustering structure. To remedy this
deficiency, we propose Hyperspherical Prototype NodeClustering (HPNC), an end-to-end
clustering paradigm that explicitly enhances the inter-cluster separability of learned node
embeddings. Concretely, we constrain the embedding space to a unit-hypersphere, enabling
us to scatter the cluster prototypes over the space with maximized pairwise distances. Then,
we employ a graph autoencoder to map nodes onto the same hypersphere manifold. Con-
sequently, cluster affinities can be directly retrieved from cosine similarities between node
embeddings and prototypes. A clustering-oriented loss is imposed to sharpen the affinity
distribution so that the learned node embeddings are encouraged to have small intra-cluster
distances and large inter-cluster distances. Based on the proposed HPNC paradigm, we
devise two schemes (HPNC-IM and HPNC-DEC) with distinct clustering backbones. Em-
pirical results on popular benchmark datasets demonstrate the superiority of our method
compared to other state-of-the-art clustering methods, and visualization results illustrate
improved separability of the learned embeddings.
1 Introduction
Graph-structured data are ubiquitous in numerous domains, including social networks, recommendation
systems, physics, and biology. Community structure is one of the most important characteristics of a graph
and is useful in many downstream tasks. Finding communities from a graph can be formulated as a node
clustering problem. Node clustering aims to categorize the nodes of a graph into a set of disjoint groups
such that similar nodes are assigned to a common group. Spectral clustering (von Luxburg, 2007) has
been one of the most successful and well-known node clustering methods in the past two decades. Given a
∗Corresponding author.
1Published in Transactions on Machine Learning Research (01/2024)
weighted undirected graph, spectral clustering firstly uses Laplacian eigenmaps (Belkin & Niyogi, 2003) to
embed the nodes into a low-dimensional feature space, then employs K-means to uncover clustering results
from them. Being the first step of spectral clustering, the task of encoding nodes into feature vectors is
callednode embeddings (Cui et al., 2019), and Laplacian eigenmap is essentially the earliest node embedding
method. Hence, the output of general-purpose node embedding methods can also be passed to K-means or
other clustering methods for node clustering. However, real-world graph data are usually too complex for
“shallow” embedding methods to capture the underlying relationships, leading to suboptimal performance
of downstream tasks including clustering.
Recently, a surge in research on graph neural networks (GNNs) has led to state-of-the-art results on numer-
ous downstream tasks. Moreover, the encoder can be easily chained with downstream tasks to customize
the embeddings for them, which is non-trivial for previous methods. Methods employing a graph neural
network (GNN) to do node embedding and perform clustering after that are called deep clustering . For
example, graph autoencoder (GAE, Kipf & Welling 2016) uses stacked graph convolution network (GCN,
Kipf & Welling 2017) layers to encode the input graph and node features into a low-dimensional space, then
reconstructs the graph by inner products of the latent embeddings and minimizes the reconstruction error
to self-supervise the training. After converging, the embeddings are passed to K-means to obtain clustering
results. Later autoencoder-based approaches (Wang et al., 2017; Pan et al., 2020; Cui et al., 2020; Wang
et al., 2020; Hou et al., 2022; Zhang et al., 2022) improved the GNN towards better latent embeddings.
However, they mostly focus on having the learned embeddings better reconstruct the input graph and/or
node features. That is, optimizing their objectives does not explicitly lead to better clusterability, so the
resulting embeddings may not be suitable for clustering. To be specific, these methods overlooked the dis-
tribution of cluster prototypes1:different cluster prototypes should be far from each other for their affiliated
node embeddings to be distinguishable. A concurrent work (Liu et al., 2023b) noticed this issue and proposed
to push away different cluster centroids by penalizing the smallest centroid distance, but this is implemented
as a regularization term so the effect is not guaranteed in practice.
On the other hand, most existing works either adopt the same sequential pipeline as GAE or follow (Yang
et al., 2017) to iteratively optimize reconstruction loss and K-means loss. The fact is not limited to
autoencoder-based approaches but also contrastive-learning-based approaches (Park et al., 2022; Devvrit
et al., 2022; Liu et al., 2022a) that becoming popular in recent years. Exceptions are DAEGC (Wang et al.,
2019) and SDCN (Bo et al., 2020). They pretrain the GAE as usual in the first step, then use K-means to
obtain cluster centroids from the latent embeddings and follow (Xie et al., 2016) to form soft labels based
on sample–centroid distances. After that, the embeddings are refined by self-training, and final clustering
results are obtained from the soft labels after refinement. SDCN additionally normalized the latent em-
beddings by softmax function so soft labels can also be obtained there, but K-means is still unavoidable to
obtain the centroids. The most critical drawback of K-means in a deep learning pipeline is that K-means’
objective is not differentiable. As a result, it cannot be optimized by gradient descent thus prohibiting
chained training with other downstream tasks. Moreover, the network parameters and clustering centroids
have to be alternatively updated as the original K-means does, which leads to error propagation, prevents
parallelism, and is prone to get stuck in bad local optimums.
In order for the learned embeddings to present a clear clustering structure, their intra-cluster distance should
be as small as possible and inter-cluster distance should be as large as possible. Motivated by this criterion,
this work mainly focuses on enhancing the separability of different clusters in unsupervised settings. In fact,
this is similar to the objective of distance metric learning (Wang et al., 2014; Zhao et al., 2021b), which treats
samples from the same class as positive pairs and samples from different classes as negative pairs and trains
the neural network to generate similar embeddings for positive pairs and dissimilar embeddings for negative
pairs. For instance, Proxy-NCA loss (Movshovitz-Attias et al., 2017) assigns a proxy to each class. During
training, a data point is encouraged to be close to its corresponding class proxy, and far apart from other
class proxies. As long as the proxies are separable, the learned embeddings are also separable. However,
these class proxies are selected according to ground truth labels of the training set, so it’s impossible to
follow its selection strategy in unsupervised tasks and metric learning methods cannot be trivially adopted
in clustering. Nonetheless, our method is heavily inspired by Proxy-NCA.
1Also known as cluster centroids in some clustering methods.
2Published in Transactions on Machine Learning Research (01/2024)
Figure 1: The workflow of HPNC. The input nodes are passed to several GNN layers to generate node
embeddings. Then, the embeddings are passed to a GNN layer to reconstruct node features and an inner
product decoder to reconstruct edges. L2-normalization is applied to make them fit on a unit-hypersphere
manifold. Given uniformly distributed cluster prototypes on the same manifold, the soft labels Qcan be
obtained by the cosine similarities between node embeddings (gray circle) and cluster prototypes (colored
triangles). A clustering loss is imposed on Qto sharpen it so that confident predictions are emphasized
and unconfident predictions are suppressed. The clustering loss is jointly minimized with the reconstruction
errors towards discriminative node embeddings. Clustering results are directly obtained from Q.
In this paper, we simultaneously address the issues mentioned above and propose Hyperspherical Prototype
NodeClustering(HPNC),a fully differentiable andend-to-end clusteringparadigmwhichexplicitlyconsiders
the separability of different clusters. Unlike previous works that infer clustering centroids from preliminary
node embeddings, we use predefined centroids and maximize their margins to encourage the separability
of different clusters. To this goal, we constrain the embedding space to a unit-hypersphere where the
maximum distance between points is bounded. Then, we scatter the cluster prototypes on the hypersphere
manifold so they have equal and maximized pairwise distances. After that, we use a graph autoencoder
to map the input nodes onto the same manifold. Consequently, the soft labels can be obtained by their
cosine similarities between prototypes. A clustering-oriented objective is jointly optimized with GAE to
push nodes to corresponding prototypes. Remarkably, our HPNC is a general pipeline and can be used with
any unsupervised representation learning methods and any deep clustering objective for joint representation
learning and clustering. We demonstrate such flexibility by devising two clustering schemes based on HPNC.
The general pipeline of HPNC is illustrated in Figure 1. Our main contributions are:
•We propose a novel node clustering paradigm called HPNC for joint representation learning and
node clustering. HPNC explicitly maximizes the inter-cluster distances of the learned embeddings
to provide sufficient discriminative power for clustering. Moreover, HPNC is fully differentiable so
it can be easily integrated into a deep learning pipeline and jointly optimized with other modules.
•We develop a learning-based prototype rotation strategy to assist GNN to find matching prototypes.
It allows the predefined prototypes to rotate on the hypersphere manifold so HPNC becomes less
sensitive to their initial coordinates. Ablation study results verify that it’s crucial to the clustering
performance.
•Based on the proposed HPNC paradigm, we devise two schemes ( i.e., HPNC-IM and HPNC-DEC)
to demonstrate the flexibility of HPNC to work with different clustering backbones. HPNC can also
combine with other backbones to enjoy their advantages.
•Empirical results on widely adopted node clustering benchmark datasets consistently verify the effec-
tivenessofourproposalcomparedtootherstate-of-the-artmethods. Toexplorethecharacteristicsof
3Published in Transactions on Machine Learning Research (01/2024)
the learned embeddings, we further apply K-means clustering and t-SNE visualization on them, and
the results significantly reveal that our method indeed leads to more discriminative and separable
latent embeddings.
2 Related Work
In this section, we briefly review the techniques closely related to our work, including unsupervised graph
representation learning and deep clustering methods.
2.1 Unsupervised Graph Representation Learning
Unsupervised graph representation learning (UGRL) aims at projecting the nodes, edges, subgraphs, or
entire graph, to a low-dimensional vector Rm, without access to ground truth labels, so that they can be
effectively handled by downstream machine learning algorithms. Traditional UGRL methods utilize matrix
factorization (Belkin & Niyogi, 2003; Cao et al., 2015) and random walks (Grover & Leskovec, 2016) to
capture graph characteristics. Specifically, DeepWalk Perozzi et al. (2014) defines the similarity between two
nodes as the probability that they co-occur on a random walk, so it first estimates these probabilities from
fixed-length and unbiased random walks sampled over the graph, then employs the skip-gram model to learn
nodeembeddingsthatreconstructtheseprobabilities. node2vecGrover&Leskovec(2016)improvedthewalk
strategybyusingflexible, biasedrandomwalksthatcantradeoffbetweenlocalandglobalviewsofthegraph.
NetRA Yu et al. (2018b) proposed to learn the node embeddings with adversarially regularized autoendoers.
Unlikeskip-grambasedmodels, itemploysalongshort-termmemory(LSTM)networktomaponehotvertex
encodings into latent representations and train them to reconstruct the input. In addition to static graphs,
NetWalkYuetal.(2018a)proposedanovelreservoirsamplingstrategytoincrementallymaintainthelearned
embeddings to effectively handle dynamic graphs. However, these methods utilize the graph topology only.
For attributed graphs where each node is also associated with a feature vector, they are unable to exploit
such extra information thus leading to suboptimal performance on downstream tasks. With the advance
of deep learning on graphs, recent UGRL methods employ a GNN to perform the projection and design
various unsupervised objectives to train the GNN. Autoencoder-based methods are the most popular thanks
to their simplicity and effectiveness. GAE and VGAE (Kipf & Welling, 2016) take the dot products of latent
embeddings as predicted edges and let them reconstruct the input graph. ARGA and ARVGA (Pan et al.,
2020) introduced an adversarial regularizer on top of GAE and VGAE to encourage the latent embeddings
to also follow a prior distribution. MGAE (Wang et al., 2017) passes corrupted node features to the GNN
and lets the latent embeddings reconstruct uncorrupted node features. GALA (Park et al., 2019) designed a
decoder to reconstruct node features from latent embeddings. Another fruitful avenue of UGRL is based on
contrastive learning (Zhu et al., 2021), which learns meaningful embeddings by pulling predefined positive
pairs and pushing negative pairs. DGI (Velickovic et al., 2019) generates graph embeddings of the input
graph and its corrupted counterpart, then treats them as positive and negative samples of the uncorrupted
node embeddings, respectively. InfoGraph (Sun et al., 2020) extends DGI to handle a batch of graphs, node
embeddings from the same graph are positive pairs, and vice versa. MVGRL (Hassani & Ahmadi, 2020)
employs graph diffusion (Klicpera et al., 2019) to generate an augmented view of the graph, then regards
node embeddings from one view and graph embedding of another view as positive pairs. GRACE (Zhu
et al., 2020) and SimGRACE (Xia et al., 2022) generate two augmented views of the graph, then follow the
instance discrimination task (Wu et al., 2018) to treat a pair of embeddings of the same node in different
views as positive, and all other pairs as negative. GGD (Zheng et al., 2022) simplified DGI and improved
it to scale to large data. Compared to traditional UGRL methods, GNN-based methods usually incorporate
the node attributes when learning latent embeddings, hence performing better on downstream tasks.
2.2 Deep Clustering
Real-world data are usually high-dimensional and non-globular, so shallow clustering methods such as K-
means can not effectively handle them. Methods employing a deep neural network (DNN) to do non-linear
mapping and perform clustering after that are called deep clustering , whose assumption is that mapped
data distributions are more suitable for clustering than raw features. However, directly combining DNN
4Published in Transactions on Machine Learning Research (01/2024)
with K-means is not a viable approach, because the global optimum of K-means is achieved when DNN
maps all data samples to one single point, which is called cluster collapse . To avoid this issue, most works
employ an autoencoder to do the feature mapping, because the autoencoder’s objective requires the latent
embeddings to reconstruct the original inputs, which ensures that critical semantic information is preserved.
GraphEncoder (Tian et al., 2014) is the earliest deep clustering method, it sequentially uses an autoencoder
to perform feature mapping and K-means to cluster the latent embeddings. Beyond that, DCN (Yang
et al., 2017) summed up K-means and autoencoder objectives and iteratively optimize their parameters,
indirectly avoiding cluster collapse by minimizing autoencoder’s reconstruction error. DEC (Xie et al., 2016)
developed a self-supervised loss to iteratively refine autoencoder embeddings by confident predictions. Other
than autoencoders, there are also deep clustering methods based on spectral clustering (Shaham et al., 2018),
subspace clustering (Ji et al., 2017), mixture of experts (Tsai et al., 2021), etc. We suggest the readers refer
to (Zhou et al., 2022) for a comprehensive survey on deep clustering methods.
As described above, the basic assumption of deep clustering is that the learned representations are more
suitable for clustering, i.e., more distinguishable from different clusters compared to raw features. Nonethe-
less, we argue that the assumption hardly holds for previous deep clustering methods. First, the repre-
sentation learning modules aim at preserving the semantics (Kipf & Welling, 2016; Bo et al., 2020) or
consistency (Bachman et al., 2019; Ji et al., 2019) of the latent embeddings, or making the embeddings
discriminative at instance level (Zhao et al., 2021a; Tao et al., 2021; Devvrit et al., 2022). Second, clustering
centroids and soft labels are inferred from the learned embeddings and (optionally) employed to finetune
the embeddings. Unfortunately, none of the representation learning objectives contribute to cluster-wise dis-
crimination due to inaccessible ground truth labels, and hence the ambiguous clustering centroids and soft
labels, leading to suboptimal clustering performance. In a word, the chicken or the egg causality dilemma
prohibits conventional deep clustering from learning unambiguous clustering results.
3 Method
An attributed undirected graph dataset G=⟨V,E,X⟩consists of a node set V, an edge setE, and a tabular
data matrixX∈Rn×d. Each node is associated with a feature vector xi∈Rd. The edge setEcan also be
represented by an adjacency matrix A, where
Au,v=/braceleftigg
1,if⟨u,v⟩∈E,
0,otherwise.(1)
Node clustering algorithms aim to partition the nodes into cdisjoint clusters without ground truth labels so
that nodes from the same cluster are similar to each other, and nodes from different clusters are not.
Next, we give an overview of the proposed HPNC method and analysis how it differs from conventional deep
clustering diagrams. After that, we introduce its pipeline and details of each submodule.
3.1 Overview
Intuitively, cluster prototypes should be as far as possible from each other, so that the data samples dis-
tributed around them are more distinguishable from different clusters. Unlike previous works that infer
cluster prototypes from the learned node embeddings in a K-means-like fashion (Xie et al., 2016; Devvrit
et al., 2022), we directly maximize their distances in a data-independent approach. However, it’s impossible
to give the coordinates of the farthest pair of points in unconstrained Euclidean space, because the distances
are up to infinity. Hence, the embedding space needs to be constrained to make the distances bounded.
In this work, we simply restrict the embedding space to a unit hypersphere, which is widely adopted and
massively studied in various machine learning tasks (Davidson et al., 2018; Xu et al., 2019; Wang & Isola,
2020; Tan et al., 2022). On a hypersphere manifold, the sum of pairwise distances of cluster prototypes is
maximized when uniformly scattered, which implies the best separability among different clusters. Once
obtaining the prototypes, their coordinates are fixed and no longer updated during training, so that their
separability is always guaranteed and the cluster collapse issue is thoroughly avoided.
5Published in Transactions on Machine Learning Research (01/2024)
3.2 Hyperspherical Prototype Node Clustering
Next, we elaborate on the pipeline of HPNC. It merely consists of a representation learning module to map
the nodes onto the same hypersphere manifold as the cluster prototypes, and a clustering module to push
them close to corresponding prototypes. In this way, the intra-cluster distances are also naturally minimized,
leading to discriminate embeddings. Moreover, the clustering labels can be directly retrieved from indexes
of the nearest prototypes for each node, without reliance on external clustering algorithms.
3.2.1 Pretrain Prototypes
The prerequisite step of HPNC is to obtain the coordinates of the cluster prototypes. Specifically, we propose
to use uniformly scattered points on a unit-hypersphere as cluster prototypes. Uniformly placing cpoints
on am-dimensional unit-hypersphere Sm−1so that the minimum distance between any two points gets
maximized is known as the Tammes problem (Tammes, 1930) in geometry. For S1(i.e., a unit-circle on a
2D plane), this is as easy as splitting it into cequal slices, and there are optimal solutions for S2as well.
Unfortunately, no such solutions exist for m> 3given an arbitrary c(Musin & Tarasov, 2015). To this end,
we follow (Mettes et al., 2019) to adopt a gradient-based approach. Concretely, we minimize the maximum
pairwise cosine similarities by gradient descent:
LHP=1
cc/summationdisplay
i=1max
µjµ⊤
iµj
∥µi∥·∥µj∥, j∈1,...,c,j̸=i, (2)
whereµi∈Rmare unnormalized prototypes. For convenience, we use ˜µi=µi
∥µi∥to denoteL2-normalized
prototypes. Thus, ˜µiwill uniformly scatter on Sm−1after convergence. It’s worth noting that the optimiza-
tion is very fast because there are just c×mscalars to update. The pretraining is data-independent and thus
only needs to be done once for each pair of ⟨c,m⟩. It’s worthwhile to emphasize again that the prototypes
arefixedafter pretraining, they work as constants and are no longer updated in later steps of HPNC.
3.2.2 Representation Learning
The next step is to map the input graph into low-dimensional node embeddings. Without access to ground
truth labels in a clustering context, an unsupervised objective is demanded to produce meaningful embed-
dings. Motivated by recent advances in generative graph representation learning, we adopt a masked graph
autoencoder (GraphMAE) (Hou et al., 2022) as the representation learning backbone. We briefly introduce
its objective in this section.
Masked feature reconstruction. Unlike conventional GAEs that reconstruct edges, GraphMAE devel-
oped a masked feature reconstruction strategy to recover masked node features X∈Rn×d. To be specific,
a large random subset ( e.g., 50%) of nodes Vmask∈Vare sampled, and their node features are replaced
by a shared, trainable mask token . Then, the partially masked features and unaltered input graph are fed
into multiple GNN layers to generate latent embeddings Z∈Rn×m. A re-mask strategy is applied to Z
before feeding it to the decoder, which replaces the embeddings of Vmaskagain, but with zeros. Denoting
the reconstructed node features as ˆX∈Rn×d, GraphMAE introduced scaled cosine error to minimize the
reconstruction error as
Lfea=1
|Vmask|/summationdisplay
vi∈Vmask/parenleftbigg
1−x⊤
iˆxi
∥xi∥·∥ˆxi∥/parenrightbiggγ
,γ⩾1. (3)
During inference, the original node features without mask and re-mask strategies are used. As a result, it
potentially leads to a mismatch between training and interference because the mask token is not observed in
the later stage. To mitigate this issue, GraphMAE adopts a "random-substitution" method that randomly
substitutes node features of a small subset of Vmask(e.g., 15%) with node features sampled from X.
Edge reconstruction. GraphMAE focuses on classification and the authors argue that feature recon-
struction is more beneficial than edge reconstruction (Hou et al., 2022), so they propose to reconstruct node
features only. However, reconstructing the edges Acaptures more node pair-level information (Liu et al.,
6Published in Transactions on Machine Learning Research (01/2024)
2023a) and thus beneficial for link prediction and clustering (Pan et al., 2020). Hence, we also reconstruct
the edges in addition to node features (Gao & Huang, 2018; Salehi & Davulcu, 2020). To be specific, we
adopt the widely used inner product decoder (Kipf & Welling, 2016) to predict edges based on the latent
embeddings:
Au,v=σ/parenleftbiggz⊤
uzv
∥zu∥·∥zv∥/parenrightbigg
, (4)
whereσ(·)is the sigmoid function. Then, we measure their discrepancies between ground truth edges by
binary cross entropy as
Ledge=−1
|E|
/summationdisplay
⟨u,v⟩∈ElogAu,v+/summationdisplay
⟨¯u,¯v⟩∈¯Elog(1−A¯u,¯v)
, (5)
where ¯Eis the set of unconnected edges obtained by negative sampling and |E|=|¯E|. By applying just
LfeaandLedge, the GNN can generate semantically meaningful node embeddings, and clustering results can
be obtained by applying external clustering algorithms such as K-means. Next, we describe how to further
refine them for better clustering performance and obtain clustering labels without reliance on K-means.
3.2.3 Rotated Clustering Affinity
In order to prevent collapsed clusters and encourage large inter-cluster distance of the latent embeddings, we
aim to map nodes around scattered cluster prototypes while leaving these prototypes unchanged. However,
weempiricallyfindthatit’stoochallengingfortheGNNtomapnodestoappropriateprototypes. Tomitigate
this issue, we propose allowing the prototypes to rotateon the hypersphere. With a shared rotation matrix,
their pairwise similarities still remain unchanged so the large separation property is preserved, while their
coordinates can change to some extent. In other words, the GNN doesn’t need to learn to rotate the nodes
anymore so it will converge easier. To be specific, we first apply L2-normalization to latent embeddings and
denote them as ˜zi=zi
∥zi∥, then the cosine similarity of the i-th node and j-th rotated prototype is simply
their inner product ˜z⊤
iR˜µj, whereR∈Rm×m,R⊤R=Iis the rotation matrix. Finally, we apply the
softmax function to make them sum up to 1:
Qi,j=exp(˜z⊤
iR˜µj)/summationtextc
j′=1exp(˜z⊤
iR˜µj′),s.t.R⊤R=I. (6)
Hence,Q∈Rn×ccan be interpreted as (soft) clustering labels. The rotation matrix Ris learnable and
updated by gradient descent2. After obtaining preliminary clustering affinities, we devise two schemes to
refine them (hence the node embeddings) as we elaborate below.
3.3 Scheme 1: HPNC-IM
The clustering affinities Qshould be unambiguous and sharp for ˜Zto be discriminative, so that intra-
cluster distances are minimized and inter-cluster distances are maximized. To this end, we can employ the
information maximization (IM) loss (Gomes et al., 2010; Hu et al., 2017; Liang et al., 2020) to encourage
the cluster distributions individually unambiguous and globally uniform. Let XandYdenote the domains
of data samples and cluster assignments, IM minimizes the following objective:
LIM=−(H(Y)−H(Y|X)), (7)
2We initialize Ras an identity matrix and enforce the orthogonal constraint with a PyTorch built-in function
torch.nn.utils.parametrizations.orthogonal during training.
7Published in Transactions on Machine Learning Research (01/2024)
whereX∈XandY∈Ydenotes the random variables for data samples and cluster assignments, H(·)and
H(·|·)are the marginal and conditional entropy, which can be estimated as
Lbal=H(Y) =h(pθ(y)) =h(Exi∈X(pθ(y|xi))) =h/parenleftigg
1
nn/summationdisplay
i=1Qi,:/parenrightigg
= logc−DKL/parenleftbigg
Exi∈X(pθ(y|xi))∥1
c1/parenrightbigg
,
(8)
Lent=H(Y|X) =Exi∈X(h(pθ(y|xi))) =1
nn/summationdisplay
i=1h(Qi,:), (9)
wherepθ(y)andpθ(y|·)are the marginal and conditional probabilities over label values y∈{1,...,c}
modeled by a GNN with parameters θ,h(p) =−/summationtextc
j=1pjlogpjdenotes the entropy function, and 1is a
vector with all ones.
Intuitively, maximizing Eq. (8) encourages the average cluster probabilities Exi∈X(pθ(y|xi))to converge to
the uniform distribution, so data samples tend to be evenly assigned to different clusters. On the other
hand, minimizing Eq. (9) encourages individual cluster probabilities pθ(y|xi)to approximate the one-hot
distribution, so the cluster assignment becomes unambiguous, and the samples are pushed far from the
decision boundaries. In summary, minimizing Eq. (7) trains the GNN to learn embeddings that agree
with the cluster assumption (Chapelle & Zien, 2005), hence improving clustering performance. Combining
Eqs. (3), (5) and (7) together, the full objective of HPNC-IM is
LHPNC-IM =Lfea+αLedge−βLbal+γLent. (10)
Unlike previous works that require bootstrapping the autoencoder for hundreds of epochs with the recon-
struction loss only, we randomly initialize the network parameters and train HPNC-IM with the full objective
from scratch.
Finally, clustering results are directly obtained as the indices of the largest Qi,jwithout relying on external
clustering algorithms such as K-means. Nonetheless, we’ll show later in Section 4.2.2 that the clustering per-
formance ofQand K-means on ˜Zare very close, verifying that our objective actually leads to discriminative
node embeddings.
3.4 Scheme 2: HPNC-DEC
In addition to the IM loss, we demonstrate that our proposed HPNC paradigm can also integrate with a
simplified version of the DEC loss (Xie et al., 2016). As introduced in Section 1, the original version of DEC
is a three-stage process:
1. Employing an autoencoder to learn preliminary embeddings {z1,...,zn}from the input data.
2. Performing K-means on the embeddings to obtain clustering centroids {µ1,...,µc}, then calculating
theclusterassignmentdistribution QwiththeStudent’s t-distributionasthesimilaritymeasurement
between the embeddings and clustering centroids:
Qi,j=(1 +∥zi−µj∥2/σ)−σ+1
2
/summationtext
j′(1 +∥zi−µj′∥2/σ)−σ+1
2, (11)
whereσis the degree of freedom of the Student’s t-distribution.
3. Defining an auxiliary distribution Pby raisingQto the second power and normalizing it to sum up
to1:
Pi,j=Q2
i,j//summationtextn
i′=1Qi′,j/summationtextc
j′=1(Q2
i,j′//summationtextn
i′=1Qi′,j′). (12)
Then, their Kullback-Leibler (KL) divergence is minimized to finetune the encoder network:
LDEC=DKL(P∥Q) =1
nn/summationdisplay
i=1c/summationdisplay
j=1Pi,jlogPi,j
Qi,j. (13)
8Published in Transactions on Machine Learning Research (01/2024)
Pis sharper than Qbecause confident predictions (ones with large Qi,j) are emphasized and unconfident
predictions (ones with small Qi,j) are suppressed. Hence, minimizing the KL divergence will sharpen Q,
leading to discriminative and cluster-aware embeddings. DEC iteratively performs 2) and 3) until conver-
gence. However, the reliance on K-means and iterative optimization prohibits the potential use of DEC in
an end-to-end deep learning pipeline.
We propose a crucial simplification to integrate DEC with the HPNC paradigm, that is to replace the
Student’st-distribution-based cluster assignment distribution ( i.e., step 2) with Eq. (6). Benefiting from the
pipeline of HPNC, we no longer need K-means because the clustering centroids are given in advance. The
iterative optimization of centroids and encoder parameters is also discarded because the centroids are not
updated during training HPNC.
Intuitively, DEC shares a similar purpose with the conditional entropy minimization part of IM (9) to make
the target cluster assignment certain. However, it doesn’t consider balancing the scale of different clusters
so it may produce empty clusters, which is undesired when we would like to ensure that all clusters are
assigned data samples. Thus, we simply employ the marginal entropy maximization part of IM (8) to
prevent degenerate solutions with empty clusters. Finally, the full objective of HPNC-DEC is as follows:
LHPNC-DEC =Lfea+αLedge−βLbal+γLDEC. (14)
As HPNC-IM, HPNC-DEC is also trained from randomly initialized network parameters by end-to-end
gradient descent, without any form of pretraining. The loss functions of both schemes Eqs. (10) and (14)
are fully differentiable and thus can be easily integrated into a deep learning pipeline and jointly optimized
with other modules.
3.5 Complexity Analysis
When pretraining prototypes, the time complexity of calculating their pairwise similarities is O(c2m), and
there arecmparameters to be updated, so the complexity of pretraining is O(c2mtpre)wheretprerefers to
the number of pretraining epochs. We empirically find that setting tpreto 3000 is sufficient, so this step
usually finishes in half of a minite.
The complexity of representation learning depends on the specific backbone, which is usually linear with the
number of nodes and edges.
The complexity of calculating rotated clustering affinity (6) is O(ncm +m2c)by applying rotation to pro-
totypes first and obtain their inner products with samples later. There are m2trainable parameters in R,
and its orthogonal parametrization is also O(m2). Hence, this step needs O((n+m)cmt)in total, where t
is the number of training epochs.
The complexity of the clustering backbone also depends on the specific choice, which is O(nct)for both IM
and DEC that employed in this work.
In summary, the time complexity of HPNC without representation learning is O(c2mtpre+ (n+m)cmt).
The time complexity of K-means clustering is O(ncmt ), so HPNC is comparable to applying K-means after
representation learning.
4 Experiments
In this section, we conduct extensive experiments to evaluate the effectiveness of the proposed HPNC
paradigm. The experiments are designed with the aim to answer the following research questions:
•RQ1:What’s the clustering performance of the proposed HPNC paradigm along with the two
devised schemes?
•RQ2:How useful is the proposed rotated clustering affinity compared to conventional cosine simi-
larity?
9Published in Transactions on Machine Learning Research (01/2024)
Table 1: Averaged clustering performance. The best results are in bold, second-best results are underlined .
MethodCora CiteSeer PubMed ACM DBLP
ACC NMI ARI ACC NMI ARI ACC NMI ARI ACC NMI ARI ACC NMI ARI
GAE 53.3±0.240.7±0.330.5±0.241.3±0.418.3±0.319.1±0.363.1±0.424.9±0.321.7±0.284.5±1.455.4±1.959.5±3.161.2±1.230.8±0.922.0±1.4
VGAE 56.0±0.338.5±0.434.7±0.344.4±0.222.7±0.320.6±0.365.5±0.225.0±0.420.3±0.284.1±0.253.2±0.557.7±0.758.6±0.126.9±0.117.9±0.1
MGAE 63.4±0.545.6±0.343.6±0.463.5±0.439.7±0.442.5±0.559.3±0.528.2±0.224.8±0.487.6±0.062.5±0.067.1±0.075.6±0.143.3±0.247.6±0.2
ARGA 63.9±0.445.1±0.335.1±0.557.3±0.535.2±0.334.0±0.468.0±0.527.6±0.429.0±0.486.3±0.456.2±0.863.4±0.964.8±0.629.4±0.928.0±0.9
ARVGA 64.0±0.544.9±0.437.4±0.554.4±0.526.1±0.524.5±0.351.3±0.411.7±0.37.8±0.283.9±0.551.9±1.057.8±1.254.4±0.425.9±0.319.8±0.4
NetRA 30.4±0.79.5±2.64.7±0.725.7±3.31.9±1.01.5±1.040.0±0.00.0±0.00.0±0.036.7±0.20.6±0.10.5±0.133.3±0.41.5±0.20.8±0.2
NetWalk 27.2±3.14.5±0.5-0.2±0.224.9±0.23.5±0.21.9±0.135.8±2.30.18±0.30.2±0.337.9±0.41.6±0.40.9±0.034.0±0.52.0±0.31.8±0.0
AGC 68.9±0.553.7±0.348.6±0.366.9±0.541.1±0.441.9±0.569.8±0.431.6±0.331.8±0.479.9±0.049.6±0.051.2±0.064.4±0.134.6±0.028.2±0.1
DAEGC 70.2±0.452.6±0.349.7±0.467.2±0.539.7±0.541.1±0.466.8±0.526.6±0.227.7±0.386.9±2.856.2±4.259.4±3.962.1±0.532.5±0.521.0±0.5
AGE 72.8±0.558.1±0.656.3±0.470.0±0.344.6±0.445.4±0.569.9±0.530.1±0.431.4±0.690.6±0.268.7±0.574.3±0.475.1±0.645.5±0.347.6±0.8
SDCN 48.5±0.524.6±0.420.6±0.366.0±0.338.7±0.340.2±0.464.2±1.322.9±2.022.3±2.090.5±0.268.3±0.373.9±0.468.1±1.839.5±1.339.2±2.0
DCRN 63.4±0.546.2±0.436.3±0.970.9±0.245.9±0.447.6±0.369.9±0.132.2±0.131.4±0.191.9±0.271.6±0.677.6±0.579.7±0.349.0±0.453.6±0.5
GraphMAE 68.0±2.057.6±1.150.2±2.869.0±0.443.6±0.544.4±0.569.9±0.534.4±0.532.9±0.789.6±0.266.7±0.371.8±0.473.6±0.545.7±0.343.6±0.7
SUBLIME 71.2±0.153.6±0.450.6±0.468.1±0.543.2±0.343.4±0.763.8±1.327.4±1.424.5±2.388.9±0.366.0±0.870.1±0.754.8±2.930.2±2.618.1±2.4
NAFS 70.4±0.056.6±0.048.0±0.071.8±0.045.1±0.047.6±0.070.5±0.033.9±0.033.2±0.081.2±0.151.4±0.352.9±0.252.8±0.025.7±0.114.7±0.0
CONVERT 74.1±1.555.6±1.150.5±2.068.4±0.741.6±0.742.8±1.667.1±1.830.0±1.529.3±1.884.4±2.955.3±4.459.6±6.852.9±2.820.4±2.117.2±2.36
HPNC-IM 74.1±0.558.9±0.651.5±0.972.4±0.546.3±0.748.4±0.770.4±0.434.6±0.733.5±0.792.1±0.172.1±0.378.1±0.380.0±0.449.8±0.454.9±0.6
HPNC-DEC 74.4±0.559.4±0.951.9±1.273.0±0.546.9±0.749.7±0.770.6±0.933.1±1.533.3±1.492.3±0.172.4±0.378.4±0.280.1±0.549.9±0.555.0±0.7
•RQ3:Despite clustering assignments can be directly retrieved from the soft labels Q, what’s the
difference between the ones obtained by performing K-means on the embeddings ˆZ?
•RQ4:What’s the relationship between the centroids found by performing K-means on ˆZand the
rotated HPNC prototypes {R˜µj}c
j=1?
•RQ5:Is the separability of the learned embeddings really improved?
Baselines. We compare HPNC with 13 classical and state-of-the-art node clustering and node embedding
models, including GAE, VGAE (Kipf & Welling, 2016), MGAE (Wang et al., 2017), ARGA, ARVGA (Pan
et al., 2020), NetRA Yu et al. (2018b), NetWalk Yu et al. (2018a), AGC (Zhang et al., 2019), DAEGC (Wang
et al., 2019), AGE (Cui et al., 2020), SDCN (Bo et al., 2020), DCRN (Liu et al., 2022b), GraphMAE (Hou
et al., 2022), SUBLIME (Liu et al., 2022a), NAFS (Zhang et al., 2022), and CONVERT Yang et al. (2023).
NetRA and NetWalk are random-walk-based node embedding methods that utilize graph topology only, and
the rest competitors are designed to handle attributed graphs.
Evaluation protocol. For node embedding methods that cannot directly produce cluster assignments, we
perform K-means on the learned embeddings to obtain labels for comparison. K-means is reinitialized for
50 times to eliminate randomness, so that their clustering performance solely depends on the quality of the
learned embeddings. We run all competitors five times with distinct random seeds and report the averaged
results and standard deviations of the best epochs.
We present the selected datasets, evaluation metrics and implementation details in Appendices A.1 to A.3,
respectively.
4.1 Node Clustering Performance Comparison (RQ1)
Table 1 reports the clustering performance of our method and other baselines. As shown in the table,
both schemes of our proposed HPNC consistently achieved the best or second-best clustering performance
on all five datasets, revealing the effectiveness of HPNC. In contrast, some competitors achieved appealing
performance on certain datasets but failed on others. For instance, the ACC of AGE on Cora is only 1.6
10Published in Transactions on Machine Learning Research (01/2024)
lower than that of HPNC-DEC, but it failed to handle DBLP, and the ACC is 5.0lower, which is a severe
performance drop. DCRN’s ACC on ACM is only 0.2lower than that of HPNC-IM, but 10.7lower on Cora.
Generally speaking, the performance of HPNC is quite competitive compared with recent state-of-the-art
models. Apart from clustering performance, HPNC also enjoys the benefits from its end-to-end training
strategy: Other strong baselines including but not limited to AGE, GraphMAE, and NAFS all rely on K-
means or spectral clustering to obtain clustering labels, increasing hardware requirements on CPU/memory
and prohibiting end-to-end training with other downstream tasks. Notably, HPNC-DEC seems to perform
slightly better than HPNC-IM, but their differences are marginal (less than 0.5) in most cases owing to
similar objectives between IM (9) and DEC (13).
4.2 Ablation Study
In this section, we further perform ablation studies to confirm the effectiveness of individual design choices
of HPNC.
4.2.1 Prototype Rotation (RQ2)
When calculating the affinities between nodes and cluster prototypes, we propose to introduce a learnable
rotation matrix Rapplied to these prototypes to allow them to rotate on the hypersphere manifold. To
verify whether it’s useful to assist the GNN to converge, we replace Rin Eq. (6) with an identity matrix
such that the coordinates of prototypes are strictly equal to their initial states, i.e., ones randomly initialized
and refined by minimizing Eq. (2). The clustering results are reported in Table 2, where the first four rows
do not apply rotation and the last four are their counterparts with rotation applied.
We observe that the clustering performances with prototype rotation outperform their counterparts without
rotation. Specifically, HPNC-IM and HPNC-DEC have drastic 24.3and31.1NMI drops on the Cora dataset
after disabling prototype rotation. What’s worse, the NMI on PubMed dataset without rotation is only
9.3for HPNC-IM and 8.5for HPNC-DEC, which means the clustering algorithm is not even working. In
contrast, they give a plausible performance with rotation enabled, and the standard deviations are much
lower. The results strongly suggest that allowing the prototypes to rotate on the hypersphere manifold is
indeed helpful and essential. The reason is that we consider pairwise distances of prototypes only in Eq. (2)
while their coordinates are ignored. And as a result, unsuitable random initialization of prototypes may fail
to match up semantic centroids of node embeddings, leading to performance degradation. Our prototype
rotation is thus also regarded as a learning-based strategy to reduce sensitivity to initial states to stabilize
training.
4.2.2 K-means Labels vs Soft Labels (RQ3)
The clustering results of our proposed HPNC are obtained by directly applying arg maxto Eq. (6) so external
clustering algorithms are not needed at all. Nonetheless, the learned node embeddings ˆZcan be also used for
clustering or potentially other downstream tasks. Here, we feed them into K-means and report the clustering
results in Table 2 (marked as ˜Z+KM). We observe that:
•With prototype rotation applied, the clustering performance of K-means and soft labels are very
comparable, whichmeansHPNCcanindeedguidetheGNNtowardsdiscriminativenodeembeddings
so that traditional clustering algorithms can also work well.
•The clustering performance of both K-means and soft-labels degenerate without prototype rotation,
but K-means works better than soft-labels in most cases. The reason might be that the autoencoder
backbone preserved the clustering structure of the latent embeddings to some extent for K-means
to discover, but their geometric centroids deviate far from the predefined prototypes so it’s too
challenging for HPNC to match them up.
To further investigate the consistency between K-means predictions and soft labels (HPNC-DEC is employed
in this experiment), we calculate the proportion of them being the same and plot the results in Figure 2. We
11Published in Transactions on Machine Learning Research (01/2024)
Table 2: Clustering performance of HPNC with different ablation settings. ˜Z+KM denotes K-means results
of latent embeddings, Qdenotes results of Eq. (6).
Rotate
prototypeScheme Target Metric Cora CiteSeer PubMed ACM DBLP
✗HPNC-IM˜Z+KMACC 60.2 ±5.0 59.9±10.3 69.1±1.0 80.4±9.8 64.4±4.3
NMI 50.2 ±2.7 37.1±7.0 32.3±1.9 58.0±8.3 35.4±2.2
ARI 40.0 ±3.3 36.7±8.4 31.3±1.7 59.9±11.5 37.6±4.3
QACC 49.3 ±4.0 44.9±2.0 52.5±6.7 87.5±2.5 63.9±5.1
NMI 34.6 ±7.2 26.3±3.3 9.3±5.6 63.0±5.1 35.7±3.0
ARI 26.5 ±5.6 22.4±3.2 9.6±7.3 67.4±5.6 37.8±5.1
HPNC-DEC˜Z+KMACC 62.3 ±2.9 61.1±3.8 69.3±1.2 92.1±0.2 64.7±4.6
NMI 52.5 ±1.8 38.0±1.4 32.5±2.0 72.1±0.5 36.3±2.5
ARI 42.5 ±1.7 36.6±2.4 31.5±1.9 78.0±0.4 38.8±4.6
QACC 44.9 ±2.5 53.5±3.5 52.0±5.4 92.1±0.1 64.2±4.5
NMI 28.3 ±4.2 32.4±3.7 8.5±3.7 72.2±0.5 36.1±2.8
ARI 20.0 ±3.2 27.8±4.0 8.3±5.2 78.1±0.4 38.4±4.7
✓HPNC-IM˜Z+KMACC 71.4 ±2.9 70.1±1.5 69.7±0.5 92.1±0.1 80.0±0.4
NMI 57.5 ±1.6 45.4±0.9 33.5±0.6 72.0±0.2 49.8±0.3
ARI 49.1 ±1.4 47.1±1.3 32.3±0.8 77.9±0.2 55.0±0.6
QACC 74.1 ±0.5 72.4±0.5 70.4±0.4 92.1±0.1 80.0±0.4
NMI 58.9 ±0.6 46.3±0.7 34.6±0.7 72.1±0.3 49.8±0.4
ARI 51.5 ±0.9 48.4±0.7 33.5±0.7 78.1±0.3 54.9±0.6
HPNC-DEC˜Z+KMACC 70.6 ±0.8 70.6±2.1 69.8±0.5 92.2±0.1 80.2±0.5
NMI 57.4 ±0.5 46.2±0.9 33.8±0.2 72.3±0.3 50.0±0.4
ARI 48.8 ±1.3 48.3±1.5 32.6±0.8 78.3±0.2 55.1±0.7
QACC 74.4 ±0.5 73.0±0.5 70.6±0.9 92.3±0.1 80.1±0.5
NMI 59.4 ±0.9 46.9±0.7 33.1±1.5 72.4±0.3 49.9±0.5
ARI 51.9 ±1.2 49.7±0.7 33.3±1.4 78.4±0.2 55.0±0.7
Cora CiteSeer PubMed ACM DBLP0.00.20.40.60.81.0% same predictionHPNC-IM
w/ rot
w/o rot
Cora CiteSeer PubMed ACM DBLP% same predictionHPNC-DEC
w/ rot
w/o rot
Figure 2: The proportion of Eq. (6) and K-means making the same predictions.
observe that they are highly consistent on all datasets when prototype rotation is applied but deviate far
from each other without it. This again confirms our analysis before. To summarize, prototype rotation is a
very important component of our proposed HPNC paradigm. It not only helps HPNC to match semantic
and predefined prototypes but also leads to more discriminative latent embeddings for potential use as input
of other downstream algorithms.
12Published in Transactions on Machine Learning Research (01/2024)
0.26 1.14 1.13 1.22 1.18 1.13 1.22
1.04 0.03 1.21 1.14 1.19 1.21 1.06
0.94 1.21 0.02 1.14 1.15 1.13 1.23
1.03 1.12 1.13 0.04 1.10 1.12 1.12
1.28 1.23 1.20 1.14 0.02 1.23 1.20
1.21 1.17 1.12 1.15 1.20 0.03 1.13
1.22 1.10 1.18 1.17 1.15 1.15 0.05
0.20.40.60.81.01.2
(a) Cora
0.01 1.23 1.26 1.24 1.23 1.22
1.24 0.01 1.24 1.22 1.27 1.24
1.23 1.22 0.01 1.19 1.20 1.20
1.20 1.16 1.17 0.01 1.19 1.15
1.23 1.26 1.21 1.24 0.01 1.18
1.10 1.11 1.11 1.09 1.10 0.02
0.20.40.60.81.01.2 (b) CiteSeer
0.59 1.14 1.21
1.17 0.61 1.26
1.24 1.25 0.53
0.60.70.80.91.01.11.2 (c) PubMed
0.01 1.50 1.50
1.49 0.01 1.49
1.50 1.50 0.01
0.20.40.60.81.01.21.4 (d) ACM
0.08 1.33 1.33 1.31
1.25 0.09 1.26 1.18
1.35 1.27 0.07 1.41
1.32 1.30 1.34 0.11
0.20.40.60.81.01.21.4 (e) DBLP
Figure 3: Cosine distances between rotated prototypes of HPNC and K-means centroids.
(a) GAE
 (b) AGE
 (c) DCRN
 (d) GraphMAE
 (e) HPNC
Figure 4: t-SNE visualization of learned latent embeddings on CiteSeer dataset. Pentagrams in HPNC
denote cluster prototypes. HPNC embeddings are the most discriminative and separable among all the
competitors.
4.2.3 K-means Centroids vs Prototypes (RQ4)
To investigate the relationships between the clustering centroids inferred from the latent embeddings by
K-means and HPNC’s (HPNC-DEC is employed in this experiment) rotated prototypes {R˜µj}c
j=1, we
calculate their pairwise cosine distances and plot the confusion matrices in Figure 3. We observe that
K-means centroids and rotated prototypes corresponding to the same cluster distribute very close to each
other, which means they are consistent. On the other hand, the pairwise distances between centroids
of different clusters are nearly the same, which means different clusters are successfully scattered on the
hypersphere manifold. Since K-means centroids are inferred from the latent embeddings, we conclude that
HPNC indeed successfully mapped nodes from different clusters to their corresponding prototypes, and made
them uniformly distribute on the hypersphere manifold to have large margins.
4.3 Visualization (RQ5)
We employ t-SNE (Van der Maaten & Hinton, 2008) to visualize the learned latent embeddings of HPNC
(HPNC-DEC is employed in this experiment) on the CiteSeer dataset and compare it with different baseline
methods. The results are illustrated in Figure 4. In addition, the cluster prototypes of HPNC are marked in
pentagrams. It’s obvious that the embeddings learned by HPNC are the most discriminative and separable
among all the competitors. Also, data samples from the same cluster distribute evenly around corresponding
prototypes, which is a desired property of our motivation.
5 Conclusion
This paper presents a novel HPNC paradigm for end-to-end node clustering. In HPNC, uniformly scattered
points on a unit-hypersphere are regarded as cluster prototypes. Then, a graph autoencoder is employed to
encode the nodes onto the same manifold with the guidance to push them close to the prototypes. Unlike
previous works that overlook the separability of learned embeddings, HPNC explicitly maximizes their inter-
cluster distances to provide sufficient discriminative power. Moreover, HPNC does not rely on external
13Published in Transactions on Machine Learning Research (01/2024)
clustering algorithms to uncover clustering labels and its objective is fully differentiable, which enables its
use as a module in deep learning pipelines. We devise two different schemes based on HPNC to demonstrate
itsflexibilitytoworkwithdifferentclusteringbackbones. Extensiveexperimentsonreal-worldgraphdatasets
strongly confirm the effectiveness of HPNC.
Acknowledgments
This work was supported in part by the China Postdoctoral Science Foundation under Grant 2022M722532.
References
Philip Bachman, R. Devon Hjelm, and William Buchwalter. Learning representations by maximizing mutual
information across views. In NeurIPS , pp. 15509–15519, 2019.
MikhailBelkinandParthaNiyogi. Laplacianeigenmapsfordimensionalityreductionanddatarepresentation.
Neural Comput. , 15(6):1373–1396, 2003.
Deyu Bo, Xiao Wang, Chuan Shi, Meiqi Zhu, Emiao Lu, and Peng Cui. Structural deep clustering network.
InWWW, pp. 1400–1410, 2020.
Shaosheng Cao, Wei Lu, and Qiongkai Xu. Grarep: Learning graph representations with global structural
information. In CIKM, pp. 891–900, 2015.
Olivier Chapelle and Alexander Zien. Semi-supervised classification by low density separation. In AISTATS ,
2005.
Ganqu Cui, Jie Zhou, Cheng Yang, and Zhiyuan Liu. Adaptive graph encoder for attributed graph embed-
ding. In KDD, pp. 976–985, 2020.
Peng Cui, Xiao Wang, Jian Pei, and Wenwu Zhu. A survey on network embedding. IEEE Trans. Knowl.
Data Eng. , 31(5):833–852, 2019.
Tim R. Davidson, Luca Falorsi, Nicola De Cao, Thomas Kipf, and Jakub M. Tomczak. Hyperspherical
variational auto-encoders. In UAI, pp. 856–865, 2018.
Fnu Devvrit, Aditya Sinha, Inderjit S Dhillon, and Prateek Jain. S3GC: Scalable self-supervised graph
clustering. In NeurIPS , pp. 3248–3261, 2022.
Hongchang Gao and Heng Huang. Deep attributed network embedding. In IJCAI, pp. 3364–3370, 2018.
Ryan Gomes, Andreas Krause, and Pietro Perona. Discriminative clustering by regularized information
maximization. In NIPS, pp. 775–783, 2010.
Aditya Grover and Jure Leskovec. node2vec: Scalable feature learning for networks. In KDD, pp. 855–864,
2016.
Kaveh Hassani and Amir Hosein Khas Ahmadi. Contrastive multi-view representation learning on graphs.
InICML, volume 119, pp. 4116–4126, 2020.
Zhenyu Hou, Xiao Liu, Yukuo Cen, Yuxiao Dong, Hongxia Yang, Chunjie Wang, and Jie Tang. Graphmae:
Self-supervised masked graph autoencoders. In KDD, pp. 594–604, 2022.
Weihua Hu, Takeru Miyato, Seiya Tokui, Eiichi Matsumoto, and Masashi Sugiyama. Learning discrete
representations via information maximizing self-augmented training. In ICML, pp. 1558–1567, 2017.
Pan Ji, Tong Zhang, Hongdong Li, Mathieu Salzmann, and Ian D. Reid. Deep subspace clustering networks.
InNIPS, pp. 24–33, 2017.
Xu Ji, Andrea Vedaldi, and João F. Henriques. Invariant information clustering for unsupervised image
classification and segmentation. In ICCV, pp. 9864–9873, 2019.
14Published in Transactions on Machine Learning Research (01/2024)
Thomas N. Kipf and Max Welling. Variational graph auto-encoders. CoRR, abs/1611.07308, 2016.
Thomas N. Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. In
ICLR, 2017.
Johannes Klicpera, Stefan Weißenberger, and Stephan Günnemann. Diffusion improves graph learning. In
NeurIPS , pp. 13333–13345, 2019.
Jian Liang, Dapeng Hu, and Jiashi Feng. Do we really need to access the source data? source hypothesis
transfer for unsupervised domain adaptation. In ICML, pp. 6028–6039, 2020.
Yixin Liu, Yu Zheng, Daokun Zhang, Hongxu Chen, Hao Peng, and Shirui Pan. Towards unsupervised deep
graph structure learning. In WWW, pp. 1392–1403, 2022a.
Yixin Liu, Ming Jin, Shirui Pan, Chuan Zhou, Yu Zheng, Feng Xia, and Philip S. Yu. Graph self-supervised
learning: A survey. IEEE Trans. Knowl. Data Eng. , 35(6):5879–5900, 2023a.
Yue Liu, Wenxuan Tu, Sihang Zhou, Xinwang Liu, Linxuan Song, Xihong Yang, and En Zhu. Deep graph
clustering via dual correlation reduction. In AAAI, pp. 7603–7611, 2022b.
Yue Liu, Ke Liang, Jun Xia, Sihang Zhou, Xihong Yang, Xinwang Liu, and Stan Z. Li. Dink-net: Neural
clustering on large graphs. In ICML, 2023b.
Pascal Mettes, Elise van der Pol, and Cees Snoek. Hyperspherical prototype networks. In NeurIPS , pp.
1485–1495, 2019.
Yair Movshovitz-Attias, Alexander Toshev, Thomas K. Leung, Sergey Ioffe, and Saurabh Singh. No fuss
distance metric learning using proxies. In ICCV, pp. 360–368, 2017.
Oleg R Musin and Alexey S Tarasov. The tammes problem for n= 14. Exp. Math. , 24(4):460–468, 2015.
Shirui Pan, Ruiqi Hu, Sai-Fu Fung, Guodong Long, Jing Jiang, and Chengqi Zhang. Learning graph embed-
ding with adversarial training methods. IEEE Trans. Cybern. , 50(6):2475–2487, 2020.
Jiwoong Park, Minsik Lee, Hyung Jin Chang, Kyuewang Lee, and Jin Young Choi. Symmetric graph
convolutional autoencoder for unsupervised graph representation learning. In ICCV, pp. 6518–6527, 2019.
Namyong Park, Ryan A. Rossi, Eunyee Koh, Iftikhar Ahamath Burhanuddin, Sungchul Kim, Fan Du,
Nesreen K. Ahmed, and Christos Faloutsos. CGC: contrastive graph clustering forcommunity detection
and tracking. In WWW, pp. 1115–1126, 2022.
Bryan Perozzi, Rami Al-Rfou, and Steven Skiena. Deepwalk: online learning of social representations. In
KDD, pp. 701–710, 2014.
Amin Salehi and Hasan Davulcu. Graph attention auto-encoders. In ICTAI, pp. 989–996, 2020.
Uri Shaham, Kelly P. Stanton, Henry Li, Ronen Basri, Boaz Nadler, and Yuval Kluger. Spectralnet: Spectral
clustering using deep neural networks. In ICLR, 2018.
Fan-Yun Sun, Jordan Hoffmann, Vikas Verma, and Jian Tang. Infograph: Unsupervised and semi-supervised
graph-level representation learning via mutual information maximization. In ICLR, 2020.
Pieter Merkus Lambertus Tammes. On the origin of number and arrangement of the places of exit on the
surface of pollen-grains. Recueil Trav. Bot. Néerl. , 27(1):1–84, 1930.
Cheng Tan, Zhangyang Gao, Lirong Wu, Siyuan Li, and Stan Z. Li. Hyperspherical consistency regulariza-
tion. InCVPR, pp. 7234–7245, 2022.
Yaling Tao, Kentaro Takagi, and Kouta Nakata. Clustering-friendly representation learning via instance
discrimination and feature decorrelation. In ICLR, 2021.
15Published in Transactions on Machine Learning Research (01/2024)
Fei Tian, Bin Gao, Qing Cui, Enhong Chen, and Tie-Yan Liu. Learning deep representations for graph
clustering. In AAAI, pp. 1293–1299, 2014.
Tsung Wei Tsai, Chongxuan Li, and Jun Zhu. Mice: Mixture of contrastive experts for unsupervised image
clustering. In ICLR, 2021.
Laurens Van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. JMLR, 9(11), 2008.
Petar Velickovic, William Fedus, William L. Hamilton, Pietro Liò, Yoshua Bengio, and R. Devon Hjelm.
Deep graph infomax. In ICLR, 2019.
Ulrike von Luxburg. A tutorial on spectral clustering. Stat. Comput. , 17(4):395–416, 2007.
Chun Wang, Shirui Pan, Guodong Long, Xingquan Zhu, and Jing Jiang. MGAE: marginalized graph
autoencoder for graph clustering. In CIKM, pp. 889–898, 2017.
Chun Wang, Shirui Pan, Ruiqi Hu, Guodong Long, Jing Jiang, and Chengqi Zhang. Attributed graph
clustering: A deep attentional embedding approach. In IJCAI, pp. 3670–3676, 2019.
Chun Wang, Bo Han, Shirui Pan, Jing Jiang, Gang Niu, and Guodong Long. Cross-graph: Robust and
unsupervised embedding for attributed graphs with corrupted structure. In ICDM, pp. 571–580, 2020.
Jiang Wang, Yang Song, Thomas Leung, Chuck Rosenberg, Jingbin Wang, James Philbin, Bo Chen, and
Ying Wu. Learning fine-grained image similarity with deep ranking. In CVPR, pp. 1386–1393, 2014.
Tongzhou Wang and Phillip Isola. Understanding contrastive representation learning through alignment and
uniformity on the hypersphere. In ICML, pp. 9929–9939, 2020.
Zhirong Wu, Yuanjun Xiong, Stella X. Yu, and Dahua Lin. Unsupervised feature learning via non-parametric
instance discrimination. In CVPR, pp. 3733–3742, 2018.
Jun Xia, Lirong Wu, Jintao Chen, Bozhen Hu, and Stan Z. Li. Simgrace: A simple framework for graph
contrastive learning without data augmentation. In WWW, pp. 1070–1079, 2022.
Junyuan Xie, Ross B. Girshick, and Ali Farhadi. Unsupervised deep embedding for clustering analysis. In
ICML, pp. 478–487, 2016.
Ruijia Xu, Guanbin Li, Jihan Yang, and Liang Lin. Larger norm more transferable: An adaptive feature
norm approach for unsupervised domain adaptation. In ICCV, pp. 1426–1435, 2019.
Bo Yang, Xiao Fu, Nicholas D. Sidiropoulos, and Mingyi Hong. Towards k-means-friendly spaces: Simulta-
neous deep learning and clustering. In ICML, pp. 3861–3870, 2017.
Xihong Yang, Cheng Tan, Yue Liu, Ke Liang, Siwei Wang, Sihang Zhou, Jun Xia, Stan Z. Li, Xinwang Liu,
and En Zhu. CONVERT: contrastive graph clustering with reliable augmentation. In ACM Multimedia ,
pp. 319–327, 2023.
Zhilin Yang, William W. Cohen, and Ruslan Salakhutdinov. Revisiting semi-supervised learning with graph
embeddings. In ICML, pp. 40–48, 2016.
Jiaxuan You, Zhitao Ying, and Jure Leskovec. Design space for graph neural networks. In NeurIPS , 2020.
Wenchao Yu, Wei Cheng, Charu C. Aggarwal, Kai Zhang, Haifeng Chen, and Wei Wang. Netwalk: A flexible
deep embedding approach for anomaly detection in dynamic networks. In KDD, pp. 2672–2681, 2018a.
Wenchao Yu, Cheng Zheng, Wei Cheng, Charu C. Aggarwal, Dongjin Song, Bo Zong, Haifeng Chen, and
Wei Wang. Learning deep network representations with adversarially regularized autoencoders. In KDD,
pp. 2663–2671, 2018b.
Wentao Zhang, Zeang Sheng, Mingyu Yang, Yang Li, Yu Shen, Zhi Yang, and Bin Cui. NAFS: A simple yet
tough-to-beat baseline for graph representation learning. In ICML, pp. 26467–26483, 2022.
16Published in Transactions on Machine Learning Research (01/2024)
Xiaotong Zhang, Han Liu, Qimai Li, and Xiao-Ming Wu. Attributed graph clustering via adaptive graph
convolution. In IJCAI, pp. 4327–4333, 2019.
Han Zhao, Xu Yang, Zhenru Wang, Erkun Yang, and Cheng Deng. Graph debiased contrastive learning
with joint representation clustering. In IJCAI, pp. 3434–3440, 2021a.
Wenliang Zhao, Yongming Rao, Ziyi Wang, Jiwen Lu, and Jie Zhou. Towards interpretable deep metric
learning with structural matching. In ICCV, pp. 9867–9876, 2021b.
Yizhen Zheng, Shirui Pan, Vincent C. S. Lee, Yu Zheng, and Philip S. Yu. Rethinking and scaling up graph
contrastive learning: An extremely efficient approach with group discrimination. In NeurIPS , 2022.
Sheng Zhou, Hongjia Xu, Zhuonan Zheng, Jiawei Chen, Zhao Li, Jiajun Bu, Jia Wu, Xin Wang, Wenwu
Zhu, and Martin Ester. A comprehensive survey on deep clustering: Taxonomy, challenges, and future
directions. CoRR, abs/2206.07579, 2022.
Yanqiao Zhu, Yichen Xu, Feng Yu, Qiang Liu, Shu Wu, and Liang Wang. Deep graph contrastive represen-
tation learning. CoRR, abs/2006.04131, 2020.
Yanqiao Zhu, Yichen Xu, Qiang Liu, and Shu Wu. An empirical study of graph contrastive learning. In
NeurIPS Datasets and Benchmarks , 2021.
A Appendix: Experimental Setups
A.1 Datasets
We evaluate the clustering performance on five widely adopted attributed graph datasets: Cora, CiteSeer,
PubMed (Yang et al., 2016)3, ACM and DBLP (Bo et al., 2020)4.
•Cora is a citation graph of a number of machine-learning papers divided into seven research topics.
Each edge represents a citation.
•CiteSeer is a citation graph of scientific publications classified into one of six machine-learning areas.
Each edge represents a citation.
•PubMed is a citation graph from the PubMed database, where nodes are papers about three diabetes
types and edges are citations among them.
•ACM is a paper graph from the ACM Digital Library. The papers are categorized into three research
areas. There is an edge between two papers if they are written by the same author.
•DBLPisanauthorgraphfromtheDBLPcomputersciencebibliography. Edgesdenoteco-authorship
between two authors. The authors are categorized into four research areas.
Statistics of these datasets are summarized in Table 3.
A.2 Evaluation Metrics
We employ three widely adopted metrics to evaluate node clustering performance, including clustering ac-
curacy (ACC), normalized mutual information (NMI), and adjusted Rand index (ARI). The larger values
are, the better the clustering performance.
3https://github.com/kimiyoung/planetoid
4https://github.com/bdy9527/SDCN
17Published in Transactions on Machine Learning Research (01/2024)
Table 3: Statistics for benchmark datasets.
Dataset Nodes Edges Clusters Dimension
Cora 2708 5278 7 1433
CiteSeer 3327 4552 6 3703
PubMed 19717 44324 3 500
ACM 3025 13128 3 1870
DBLP 4057 3528 4 334
A.3 Implementation Details
We implement our proposed HPNC in PyTorch 2.0 and PyG 2.3.0 and use GraphGym (You et al., 2020)
for experiment management to ensure reproducibility. Throughout the experiments, the encoders of HPNC
are all composed of two GAT layers with 4 128d attention heads and dropout probability 0.1for attention
coefficients. We also apply dropout with probability 0.2between the GAT layers and use PReLU as the
activation function. The decoder is a single GAT layer without non-linear activation. The coefficients α,β
andγin Eqs. (10) and (14) are tuned by random search within the following ranges: α∈{0.0,0.01,0.02},
β,γ∈(0.0,0.1]. Detailed hyperparameter settings can be found in YAML configuration files from our code.
18