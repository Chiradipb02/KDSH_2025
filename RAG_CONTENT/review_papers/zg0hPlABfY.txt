Under review as submission to TMLR
Enhancing Parameter Efficiency and Generalization in Large
Models: A Regularized and Masked Low-Rank Adaptation
Approach
Anonymous authors
Paper under double-blind review
Abstract
Large pre-trained models, such as large language models (LLMs), present significant resource
challenges for fine-tuning due to their extensive parameter sizes, especially for applications
in mobile systems. To address this, Low-Rank Adaptation (LoRA) has been developed to
reduce resource consumption while maintaining satisfactory fine-tuning results. Despite
its effectiveness, the original LoRA method faces the challenge of suboptimal performance.
This paper investigates the intrinsic dimension of the matrix updates approximated by the
LoRA method and reveals the performance benefits of increasing this intrinsic dimension.
By employing regularization and a gradient masking method that encourages higher intrinsic
dimension, the proposed method, termed Regularized and Masked LoRA (RM-LoRA),
achieves superior generalization performance with the same or lower trainable parameter
budget compared to the original LoRA and its latest variants across various open-source
vision and language datasets.
1 Introduction
Largepre-trainedmodels, suchaslarge-scalelanguagemodels(LLMs), haveshowcasedremarkableperformance
across a variety of tasks in computer vision and natural language processing (Zhang et al., 2022; Brown
et al., 2020; Touvron et al., 2023; Ouyang et al., 2022). Nonetheless, fine-tuning these models for specific
downstream tasks often presents substantial resource challenges due to their extensive parameter sizes. In
this context, parameter-efficient fine-tuning (PEFT) methods have been extensively explored to alleviate
resource consumption while preserving or enhancing fine-tuned model performance (Zaken et al., 2022; Hu
et al., 2021; Li & Liang, 2021; Lester et al., 2021; Vu et al., 2022; Guo et al., 2021; Zhang et al., 2023b; Liu
et al., 2022a; Houlsby et al., 2019; Liu et al., 2022b; Sung et al., 2021; 2022; Mao et al., 2022; Lee et al., 2020).
Among these methods, Low-Rank Adaptation (LoRA), which involves freezing the pre-trained weights and
approximating updates in weight matrices using the multiplication of two low-rank matrices, has emerged
as a promising approach to balance computational efficiency and task performance during the fine-tuning
process (Hu et al., 2021).
Despite its effectiveness, LoRA fine-tuning encounters challenges in determining the optimal size of LoRA
matrices for a given model and task. On one hand, excessively small matrices, with limited number of
trainable parameters, inevitably harm training convergence and generalization performance. On the other
hand, large matrices introduce redundant trainable parameters, which could be reduced to enhance parameter
efficiency. Moreover, some studies have indicated that large LoRA matrices may exacerbate overfitting, as
redundant parameters primarily contribute to training accuracy rather than test accuracy (Qiang et al., 2024;
Karimi Mahabadi et al., 2021).
Several approaches have been proposed to determine or adaptively adjust the size of LoRA matrices, often
referred to as the LoRA rank R, for improved efficiency and generalization (Valipour et al., 2023; Benedek
& Wolf, 2024; Kopiczko et al., 2023; Ding et al., 2023; Zhang et al., 2023a). However, none of these
methods investigate the intrinsic dimension rof the approximated matrix update ∆W=BAgiven by
1Under review as submission to TMLR
the product of low-rank LoRA matrices AandB. This intrinsic dimension r, instead of the previously
studied LoRA rank R, has been proven to play a crucial role in LoRA fine-tuning. Specifically, it has been
theoretically demonstrated that for fully connected neural networks, the LoRA approximation error given
by an approximated update ∆W=BAwith intrinsic dimension ris related to the r-th singular value of
the discrepancy E=Wtarget−Wfrozenbetween the target weight matrix and the frozen pre-trained weight
matrix (Zeng & Lee, 2023). In this context, the previously studied LoRA rank Rdetermined by the size of
matrices AandBacts as just an upper bound for ∆W’s intrinsic dimension r.
In other words, encouraging the intrinsic dimension rof∆Wto approximate the given LoRA rank Rbenefits
the generalization of LoRA fine-tuning under a given trainable parameter budget. Inspired by this theoretical
conclusion, this paper first adopts a regularization technique to encourage LoRA matrices to span a higher
intrinsic rank in their parameter space. Additionally, to maintain a reasonable budget of trainable parameters,
a gradient masking method is introduced to randomly mask a subset of parameters in each epoch instead
of updating all parameters in LoRA matrices. Experiments on multiple datasets have proven this method
also helps promote the growth of the intrinsic rank rand thus yields lower approximation error and better
generalization performance.
The contributions of this paper can be summarized as follows:
1.This paper extends previous theoretical bounds for LoRA approximation error from simulated
datasets to real-world datasets, providing further insights into the trade-off between LoRA rank R
and generalization performance.
2.Based on the analysis of LoRA rank and generalization performance, this paper designs a strategy for
fine-tuning LoRA matrices that encourages the growth of intrinsic rank r=rank(∆W)within the
LoRA parameter space defined by R. This strategy effectively alleviates the problem of overfitting
the training data by encouraging the LoRA matrices to explore the parameter space.
3.The experimental results across multiple open-source datasets demonstrate that this Regularized and
Masked version of LoRA ( RM-LoRA ) method manages to strike a better efficiency-generalization
tradeoff compared to the original LoRA method and its state-of-the-art variations, with better
generalization performance achieved with the same or lower trainable parameters budget.
2 Related Works
In attempts to address the computational challenges posed by updating the enormous amount of weights
in large pre-trained models, LoRA achieves outstanding model generalization with a significantly reduced
budget of trainable parameters during fine-tuning (Hu et al., 2021). However, LoRA still faces the challenges
of sub-optimal performance. Previous research addressing LoRA’s main challenges is briefly discussed as
follows:
Underfitting . While LoRA has demonstrated remarkable parameter efficiency and generalization perfor-
mance, it may lead to insufficient fine-tuning of large-scale models with high embedding dimensions (Hayou
et al., 2024). In some cases, there is a contradictory phenomenon where a higher LoRA rank doesn’t necessarily
yield better training results than a lower LoRA rank. Much research has been devoted to further enhancing
both the performance and efficiency achieved by LoRA, including the adaptive choice of LoRA rank (Zhang
et al., 2023a; Ding et al., 2023; Valipour et al., 2023), adjustment of learning rate (Hayou et al., 2024),
random projection (Kopiczko et al., 2023), derivative-free optimization (Jin et al., 2024), and pre-trained
weights optimization (Zi et al., 2023). Nevertheless, none of these methods consider the role of LoRA updates’
intrinsic dimension in mitigating the performance gap under a given LoRA rank setting.
Overfitting. Fine-tuning large pre-trained models with a large number of parameters can easily lead to
overfitting (Karimi Mahabadi et al., 2021). In the AdaLoRA method, the LoRA matrices of less important
pre-trained weight matrices are assigned a lower rank to prevent overfitting (Zhang et al., 2023a). However,
according to the experiments by Qiang et al. (2024), LoRA and AdaLoRA still clearly overfit the training
data as fine-tuning progresses, with decreases in training losses but increases in test losses. To alleviate the
overfitting problem, Qiang et al. (2024) developed the BiLoRA method, which iteratively trains different
2Under review as submission to TMLR
subsets of trainable parameters using different subsets of training data. Furthermore, Hayou et al. (2024)
pointed out that the overfitting of LoRA and its variants is due to some directions of LoRA matrices not
being sufficiently updated, and thus the change in model weights approximated by LoRA is restricted by the
vector (sub)space generated by the LoRA matrices’ columns at initialization.
The previous analysis of LoRA’s existing limitations and solutions gives rise to the idea and method employed
in this work. Specifically, for better performance under a given LoRA rank setting, this paper proposes a fine-
tuning strategy that promotes the growth of the intrinsic dimension of LoRA updates through regularization
and gradient masking, bridging the gap between practical performance and theoretical optimal performance.
The superiority of the two proposed techniques aligns with some of the previous studies. For example,
the proposed regularizer encourages parameter space exploration. It helps reduce the performance gap as
indicated by the performance benefits achieved by SoRA, which also tries to maintain a larger optimization
space by keeping each epoch’s sparsified components unchanged for the next epoch of updating (Ding et al.,
2023). Additionally, the gradient masking method is shown to further improve generalization performance due
to more efficient updates in certain LoRA directions. Such improvements can also be observed in DyLoRA,
which only updates one row and column in LoRA matrices in each step (Valipour et al., 2023).
3 Preliminary
Transformer Models. A transformer-based pre-trained model typically involves Lstacked encoder/decoder
blocks, with a multi-head attention module followed by a fully connected feed-forward network (FFN) in each
block. Given an input sequence X∈Rn×d, the output of the multi-head attention module can be written as:
MultiHead( X) = Concat(head 1,..., headh)WO,
with headi= Attention( Qi,Ki,Vi),
andAttention( Qi,Ki,Vi) = softmax/parenleftbiggQiKT
i√dk/parenrightbigg
Vi, (1)
where Qi=XWQ
i,Ki=XWK
i, and Vi=XWV
iare matrices of queries, keys, and values of headi
respectively, with projection matrices WQ
i∈Rd×dk,WK
i∈Rd×dk,WV
i∈Rd×dv, and WO∈Rhdv×d.
We refer readers to the original paper for a more comprehensive introduction to attention calculations in
general (Vaswani et al., 2017).
Given the output of the multi-head attention module, the FFN further projects the d-dimensional output X′
for each position. A two-layer FFN with a ReLU activation operates as follows:
FFN( X′) = max(0,X′Wf1+b1)Wf2+b2, (2)
where Wf1∈Rd×dmandWf2∈Rdm×d. Moreover, a residual connection followed by layer normalization is
applied to each layer to generate the output of each transformer block given the input sequence Xin the
following way:
LayerNorm( X+ FFN(LayerNorm( X+ MultiHead( X)))). (3)
LoRA Fine-tuning. For a pre-trained matrix W0∈Rd1×d2, LoRA, as proposed by Hu et al. (2021),
approximates its update ∆Wby∆W=BA, where A∈RR×d2andB∈Rd1×Rwith rankR≪min(d1,d2).
During model fine-tuning, the weight matrix W0is frozen, with only the LoRA adapters AandBbeing
trainable. The modified LoRA forward pass is:
h=W0x+ ∆Wx=W0x+BAx. (4)
Typically, the low-rank matrice Ais initialized using a random Gaussian distribution, while Bis initialized to
zero, ensuring ∆W= 0at the start of fine-tuning. Current approaches to fine-tuning large pre-trained models
with LoRA apply a pair of matrices to all weight matrices involved in each transformer block’s multi-head
attention module and FFN (He et al., 2021; Zhang et al., 2023a).
3Under review as submission to TMLR
The Expressive Power of LoRA. Zeng & Lee (2023) investigates the LoRA approximation error under a
mild non-singularity assumption. To begin with, a L-layer width- Dfully connected ReLU neural network is
denoted as FNNL,D(·; (Wl)L
l=1,(bl)L
l=1), where Wl∈RD×Dare the weight matrices and bl∈RDare the
biases for each layer l∈[L]. In LoRA method, the primary objective is to adapt a pre-trained frozen FNN f0
to approximate a target FNN ¯f, which are represented as follows:
Target FNN ¯f:=FNN ¯L,D(·; (¯Wl)L
l=1,(¯bl)L
l=1), (5)
Frozen FNN f0:=FNNL,D(·; (Wl)L
l=1,(bl)L
l=1), (6)
where ¯Wl∈RD×Dand¯bl∈RDare the weight matrix and bias vector for the l-th layer of the target model
¯f, while Wl∈RD×Dandbl∈RDare those for the l-th layer of the pre-trained frozen model f0.
With a LoRA rank setting R∈[D], the frozen FNN f0is adapted into a new model f:
Adapted FNN f:=FNNL,D(·; (Wl+∆Wl)L
l=1,(ˆbl)L
l=1), (7)
where ∆Wl∈RD×Dis the weight update approximated by LoRA with rank(∆Wl)≤Rl, and ˆblis the
updated bias vector or l∈[L]. Given that a large pre-trained model tends to be overparameterized, it is
reasonable to assume that L≥¯L, which means the pre-trained model is much deeper than the target model
to be approximated. Therefore, Zeng & Lee (2023) further introduces an ordered partition P={P1,...,P¯L}
to partition the Llayers in the adapted model f, such that/uniontext¯L
i=1Pi= [L]. Each partition element Pi∈P
consists of consecutive integers l∈Pi, which indicate the index of layers in the adapted model that will be
used to approximate the i-th layer in the target model.
With the above definition, the following theoretical result provides an upper bound on the approximation
error for the adapted model.
Theorem 3.1 (Theorem 6 (Zeng & Lee, 2023)) If/summationtext
l∈PiRl≥rank(¯Wi−/producttext
l∈PiWl)for alli∈[ˆL],
there exists LoRA adapters (∆Wl)L
l=1withrank(∆Wl)≤Rland biases (ˆbl)L
l=1such that the adapted model
fcan exactly approximate the target model ˆf.
Furthermore, define the approximation error of the i-th layer as ei=σ/summationtext
l∈PiRl+1(¯Wi−/producttext
l∈PiWl), and the
magnitude of the weight parameters and the input as
β:= max
i∈[¯L]
/radicalig
∥Σ∥Fi/productdisplay
j=1/vextenddouble/vextenddouble¯Wj/vextenddouble/vextenddouble
F+i/summationdisplay
j=1i−1/productdisplay
k=j+1/vextenddouble/vextenddouble¯Wk/vextenddouble/vextenddouble
F/vextenddouble/vextenddouble¯bj/vextenddouble/vextenddouble
2
∨/radicalig
∥Σ∥F.
Then, there exists LoRA adapters (∆Wl)L
l=1withrank(∆Wl)≤Rland biases (ˆbl)L
l=1such that for any input
xwithExxT=Σ, the approximation error can be bounded as
E/vextenddouble/vextenddoublef(x)−¯f(x)/vextenddouble/vextenddouble
2≤β¯L/summationdisplay
i=1max
k∈[¯L]/parenleftbig/vextenddouble/vextenddouble¯Wk/vextenddouble/vextenddouble
F+ek/parenrightbig¯L−iei. (8)
In the above bound, βand/vextenddouble/vextenddouble¯Wk/vextenddouble/vextenddouble
Fcapture the magnitude of the weight parameters in the target model
and the input. The LoRA rank setting Rlfor all layers l∈[L]in the adapted model contributes to this
bound through the term eifor alli∈[¯L]. The following section focuses on the interconnection among the
constituting parts of the eiterm and explains how this theoretical conclusion can be utilized to enhance
LoRA adaptation on real-world datasets.
4 RM-LoRA Method
Based on the analysis of LoRA’s expressive power described in Section 3, we introduce Regularized and
Masked LoRA (RM-LoRA), a robust method aimed at enhancing the generalization performance of LoRA
within a given parameter budget. This section presents the details of RM-LoRA, including its underlying
principles and specific techniques.
4Under review as submission to TMLR
4.1 Influence of the Intrinsic Dimension of LoRA Adapter ∆W
Note that the partition Piof the pre-trained model for the i-th layer in the target model is an intrinsic
but unknown property during adaptation, and consequently, the number and index of layers l∈Piare also
unknown. Nevertheless, with a pre-trained model f0to be adapted and the target model ¯fdetermined by a
given downstream task, the partition can be considered deterministic, as can the discrepancy between the
pre-trained model and the target model Ei=¯Wi−/producttext
l∈PiWl. Consider the case where the LoRA rank
setting for each layer l∈Piis the same as R. Theeiterm in Equation 8 can be rewritten as:
ei,rank(∆Wl∈Pi)≤R=σ/summationtext
l∈PiR+1(Ei)for each layer i∈[¯L], (9)
whererank(∆Wl∈Pi)≤Rrepresents the LoRA adapter for each layer l∈Pisatisfies the rank constraint
rank(∆Wl)≤R.
Clearly, increasing rank(∆Wl)helps relax the constraint on LoRA rank Rto achieve a certain level
of approximation error. For example, consider two LoRA rank settings R1andR2withR1< R 2. If
rank(∆Wl)≤R1<R 2, thenei,rank(∆Wl∈Pi)≤R2degenerates to ei,rank(∆Wl∈Pi)≤R1fori∈[¯L], despite the
larger size of LoRA matrices under the LoRA setting of R2. In this case, LoRA rank R1andR2yield the
same LoRA approximation error E∥f(x)−¯f(x)∥2according to Equation 8.
4.2 Regularization on LoRA Weights
Let a pair of LoRA low-rank matrices be denoted as WAandWB, respectively. To enforce the growth in
rank of ∆W=WBWA, the following regularizer is first used to encourage WAandWBto be orthogonal:
Reg(WA,WB) =∥WA(WA)⊤−I∥2
F+∥(WB)⊤WB−I∥2
F. (10)
The orthogonality of WAandWBhelps increase the rank(WA)andrank(WB). According to the lower
bound for the rank of the matrix product, for matrices A∈RR×d2andB∈Rd1×R, the rank of their product
matrix C=BAsatisfiesrank(C)≥max(rank(A) +rank(B)−R,0). This lower bound ensures the growth of
the intrinsic rank of the LoRA adapter ∆W=WAWBas therank(WA)andrank(WB)increase with the
regularizer shown in Equation 10. Note that there exist other alternative regularizers that theoretically can
also encourage the growth of rank(∆W), but are infeasible in reality due to considerations of differentiability,
numerical stability, and computational costs1.
4.3 Gradient Masking for Partial Updates
The gradient masking algorithm in RM-LoRA is designed to perform partial updates in LoRA matrices. The
algorithm takes as input the total number of steps T, the LoRA rank R, and the number of directions ˆrto
update in each step. In each training step t, it samples a mini-batch of data ξtand computes the gradients
∇ξtWA
tand∇ξtWB
tfor each pair of LoRA weight matrices. The corresponding gradient masks are first
initialized to zero, before a set Rtofˆrdistinct directions is randomly selected. The RM-LoRA method then
sets the relevant entries in the gradient masks to one according to the selected directions. These masks are
applied to the gradients to restrict the update directions. Finally, the algorithm updates the weight matrices
WA
tandWB
tusing the masked gradients, thus achieving the partial update of LoRA weight matrices. The
complete process of gradient masking is summarized in Algorithm 1.
By strategically updating a subset of directions, gradient masking helps promote sparsity in gradient flow,
which aligns with the inherent low-rank structure of many real-world datasets. To further improve efficiency,
the gradient masking process can be optimized by avoiding the computation of gradients for the masked
positions altogether. By skipping these unnecessary calculations, the computational overhead involved in the
backpropagation phase can be significantly reduced, especially in large-scale models with high-dimensional
parameter spaces. Modern training frameworks, such as PyTorch, provide native support for mechanisms
1The nuclear (trace) norm involves expensive computation for singular value decomposition, especially with large matrices.
Regularization on the determinants suffers from numerical instability since the determinant calculation is highly sensitive to
small changes in the matrix elements. Constraints on eigenvalues or singular values of the matrix are not directly differentiable.
5Under review as submission to TMLR
Algorithm 1 Gradient Masking Algorithm
1:Input:Total steps T, LoRA rank R, number of updated directions ˆr.
2:fort= 0toT−1do
3:foreach pair of LoRA weight matrices (WA
t,WB
t)in the model do
4:Sample a mini-batch data ξtand compute the gradients (∇ξtWA
t,∇ξtWB
t);
5:Initialize gradient masks (MA
t,MB
t)←0with the same shape as (∇ξtWA
t,∇ξtWB
t);
6:Construct the set Rtby randomly selecting ˆrdistinct integers from {1,2,...,R}.
7:foreachiinRtdo
8: MA
t[i,j] = 1for allj= 1,2,...,R.
9:end for
10:foreachjinRtdo
11: MB
t[i,j] = 1for alli= 1,2,...,R.
12:end for
13:Apply gradient mask ∇ξtWA
t←∇ξtWA
t⊙MA
t,∇ξtWB
t←∇ξtWB
t⊙MB
t.
14:Perform optimization step WA
t=WA
t−η∇ξtWA
t,WB
t=WB
t−η∇ξtWB
t.
15:end for
16:end for
17:Output: Updated LoRA weight matrices (WA
T,WB
T)for each fine-tuned module.
like partial gradient computation, which can be leveraged to achieve a more efficient implementation of the
proposed gradient masking.
5 Experiments
With the regularization and gradient masking technique as described in Section 4, the proposed RM-LoRA
method is expected to alleviate the problem of overfitting and achieve enhanced generalization for fine-tuning
pre-trained models. In this section, RM-LoRA is comprehensively evaluated against state-of-the-art LoRA
variants on multiple datasets.
5.1 Experimental Setup
The details of the experiment for the evaluation of the proposed RM-LoRA method are outlined as follows:
Models and Datasets . This paper compares our proposed RM-LoRA with the original LoRA and its recent
variants across both computer vision and natural language tasks. For the vision task, we fine-tune a Vision
Transformer (ViT-B/16) model, which has 86M parameters. This model was pre-trained on ImageNet dataset
as described in the original paper (Dosovitskiy et al., 2020). Fine-tuning is performed on the CIFAR-100
dataset and Food-101 dataset to evaluate our approach in the computer vision domain. For language tasks,
we fine-tune the DeBERTa-v3-base model with 184M parameters (He et al., 2022), and a GPT-2 Small model
with 117M parameters. Evaluations are conducted on the General Language Understanding Evaluation
(GLUE) benchmark (Wang et al., 2019) for language understanding and the Stanford Question Answering
Dataset (SQuAD 1.1) (Rajpurkar et al., 2016) for question answering.
Baselines . The following baselines are implemented within the same HuggingFace’s Transformers framework
for fair comparison (Wolf et al., 2019):
•Full fine-tuning (FT) uses the pre-trained model as the initialization point and updates all parameters in
the model through gradient backpropagation, resulting in a very large trainable parameter budget that
may be impractical in resource-limited environments.
•LoRA(Hu et al., 2021) approximates the incremental updates in pre-trained model weights by using
the product of two trainable matrices with rank R, which significantly reduces the number of trainable
parameters needed to adapt pre-trained models while achieving competitive generalization performance
on downstream tasks.
6Under review as submission to TMLR
/uni00000013 /uni00000014/uni00000013 /uni00000015/uni00000013 /uni00000016/uni00000013 /uni00000017/uni00000013 /uni00000018/uni00000013 /uni00000019/uni00000013
/uni00000028/uni00000053/uni00000052/uni00000046/uni0000004b/uni00000013/uni00000011/uni0000001a/uni00000018/uni00000014/uni00000011/uni00000013/uni00000013/uni00000014/uni00000011/uni00000015/uni00000018/uni00000014/uni00000011/uni00000018/uni00000013/uni00000014/uni00000011/uni0000001a/uni00000018/uni00000015/uni00000011/uni00000013/uni00000013/uni00000015/uni00000011/uni00000015/uni00000018/uni00000015/uni00000011/uni00000018/uni00000013/uni00000037/uni00000048/uni00000056/uni00000057/uni00000003/uni0000002f/uni00000052/uni00000056/uni00000056
/uni00000037/uni00000048/uni00000056/uni00000057/uni00000003/uni0000002f/uni00000052/uni00000056/uni00000056/uni00000003/uni00000059/uni00000011/uni00000056/uni00000011/uni00000003/uni00000028/uni00000053/uni00000052/uni00000046/uni0000004b
/uni00000013 /uni00000014/uni00000013 /uni00000015/uni00000013 /uni00000016/uni00000013 /uni00000017/uni00000013 /uni00000018/uni00000013 /uni00000019/uni00000013
/uni00000028/uni00000053/uni00000052/uni00000046/uni0000004b/uni0000001a/uni00000017/uni0000001a/uni00000018/uni0000001a/uni00000019/uni0000001a/uni0000001a/uni0000001a/uni0000001b/uni0000001a/uni0000001c/uni0000001b/uni00000013/uni00000037/uni00000048/uni00000056/uni00000057/uni00000003/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c
/uni00000037/uni00000048/uni00000056/uni00000057/uni00000003/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c/uni00000003/uni00000059/uni00000011/uni00000056/uni00000011/uni00000003/uni00000028/uni00000053/uni00000052/uni00000046/uni0000004b
/uni00000013 /uni00000014/uni00000013 /uni00000015/uni00000013 /uni00000016/uni00000013 /uni00000017/uni00000013 /uni00000018/uni00000013 /uni00000019/uni00000013
/uni00000028/uni00000053/uni00000052/uni00000046/uni0000004b/uni0000001b/uni00000013/uni0000001b/uni00000015/uni0000001b/uni00000017/uni0000001b/uni00000019/uni0000001b/uni0000001b/uni0000001c/uni00000013/uni0000001c/uni00000015/uni0000001c/uni00000017/uni0000001c/uni00000019/uni00000037/uni00000055/uni00000044/uni0000004c/uni00000051/uni00000003/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c
/uni00000037/uni00000055/uni00000044/uni0000004c/uni00000051/uni00000003/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c/uni00000003/uni00000059/uni00000011/uni00000056/uni00000011/uni00000003/uni00000028/uni00000053/uni00000052/uni00000046/uni0000004b
/uni00000013 /uni00000014/uni00000013 /uni00000015/uni00000013 /uni00000016/uni00000013 /uni00000017/uni00000013 /uni00000018/uni00000013 /uni00000019/uni00000013
/uni00000028/uni00000053/uni00000052/uni00000046/uni0000004b/uni00000017/uni00000019/uni0000001b/uni00000014/uni00000013/uni00000014/uni00000015/uni00000014/uni00000017/uni00000014/uni00000019/uni00000014/uni0000001b/uni0000002a/uni00000048/uni00000051/uni00000048/uni00000055/uni00000044/uni0000004f/uni0000004c/uni0000005d/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000028/uni00000055/uni00000055/uni00000052/uni00000055
/uni0000002a/uni00000048/uni00000051/uni00000048/uni00000055/uni00000044/uni0000004f/uni0000004c/uni0000005d/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000028/uni00000055/uni00000055/uni00000052/uni00000055/uni00000003/uni00000059/uni00000011/uni00000056/uni00000011/uni00000003/uni00000028/uni00000053/uni00000052/uni00000046/uni0000004b/uni00000018/uni00000018 /uni00000018/uni00000019 /uni00000018/uni0000001a /uni00000018/uni0000001b /uni00000018/uni0000001c /uni00000019/uni00000013/uni00000013/uni00000011/uni00000019/uni00000018/uni00000013/uni00000011/uni0000001a/uni00000013/uni00000013/uni00000011/uni0000001a/uni00000018/uni00000013/uni00000011/uni0000001b/uni00000013/uni00000013/uni00000011/uni0000001b/uni00000018
/uni0000002a/uni00000030/uni00000010/uni0000002f/uni00000052/uni00000035/uni00000024/uni00000003/uni0000000b/uni00000035/uni00000003/uni00000020/uni00000003/uni00000016/uni00000015/uni0000000c
/uni00000035/uni00000030/uni00000010/uni0000002f/uni00000052/uni00000035/uni00000024/uni00000003/uni0000000b/uni00000035/uni00000003/uni00000020/uni00000003/uni00000016/uni00000015/uni0000000c
/uni00000029/uni00000058/uni0000004f/uni0000004f/uni00000003/uni00000049/uni0000004c/uni00000051/uni00000048/uni00000010/uni00000057/uni00000058/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000035/uni00000010/uni0000002f/uni00000052/uni00000035/uni00000024/uni00000003/uni0000000b/uni00000035/uni00000003/uni00000020/uni00000003/uni00000016/uni00000015/uni0000000c
/uni0000002f/uni00000052/uni00000035/uni00000024/uni00000003/uni0000000b/uni00000035/uni00000003/uni00000020/uni00000003/uni00000016/uni00000015/uni0000000c/uni0000002a/uni00000030/uni00000010/uni0000002f/uni00000052/uni00000035/uni00000024/uni00000003/uni0000000b/uni00000035/uni00000003/uni00000020/uni00000003/uni00000019/uni00000017/uni0000000c
/uni00000035/uni00000030/uni00000010/uni0000002f/uni00000052/uni00000035/uni00000024/uni00000003/uni0000000b/uni00000035/uni00000003/uni00000020/uni00000003/uni00000019/uni00000017/uni0000000c/uni00000035/uni00000010/uni0000002f/uni00000052/uni00000035/uni00000024/uni00000003/uni0000000b/uni00000035/uni00000003/uni00000020/uni00000003/uni00000019/uni00000017/uni0000000c
/uni0000002f/uni00000052/uni00000035/uni00000024/uni00000003/uni0000000b/uni00000035/uni00000003/uni00000020/uni00000003/uni00000019/uni00000017/uni0000000c/uni0000002a/uni00000030/uni00000010/uni0000002f/uni00000052/uni00000035/uni00000024/uni00000003/uni0000000b/uni00000035/uni00000003/uni00000020/uni00000003/uni0000001b/uni00000013/uni0000000c
/uni00000035/uni00000030/uni00000010/uni0000002f/uni00000052/uni00000035/uni00000024/uni00000003/uni0000000b/uni00000035/uni00000003/uni00000020/uni00000003/uni0000001b/uni00000013/uni0000000c/uni00000035/uni00000010/uni0000002f/uni00000052/uni00000035/uni00000024/uni00000003/uni0000000b/uni00000035/uni00000003/uni00000020/uni00000003/uni0000001b/uni00000013/uni0000000c
/uni0000002f/uni00000052/uni00000035/uni00000024/uni00000003/uni0000000b/uni00000035/uni00000003/uni00000020/uni00000003/uni0000001b/uni00000013/uni0000000c
Figure 1: Results with ViT model on CIFAR-100.
•AdaLoRA (Zhang et al., 2023a) uses the product of three small matrices in the form of singular value
decomposition to parameterize the updates in pre-trained model weights, and then prunes the singular
values of lower importance in the diagonal matrix to achieve a pre-set total parameter budget bacross all
adapter weight matrices. Compared to LoRA with a fixed parameter budget, AdaLoRA provides a more
flexible solution for task-specific fine-tuning, resulting in a better efficiency-generalization trade-off.
•SoRA(Ding et al., 2023) parameterizes the updates in pre-trained model weights similarly to AdaLoRA,
with an additional gate unit in between, and controls the sparsity of the gate by pruning components with
absolute values lower than a pre-set threshold λ. By retaining pruned components until the pruning step
in the final epoch, SoRA maintains a larger parameter space for exploration during fine-tuning compared
to AdaLoRA.
These aforementioned LoRA methods aim to strike a balance between parameter efficiency and generalization
performance in adapting pre-trained models. Sharing this goal, our proposed RM-LoRA addresses the
challenge by exploring and enhancing the role of the intrinsic dimensions of LoRA matrices. For a fair
comparison of all the LoRA variants, including our proposed RM-LoRA method, their performance is
evaluated under the same parameter budget during inference . All experiments are conducted across
three independent runs with different random seeds, as discussed in the following sections of this paper.
7Under review as submission to TMLR
/uni00000013 /uni00000014/uni00000013 /uni00000015/uni00000013 /uni00000016/uni00000013 /uni00000017/uni00000013 /uni00000018/uni00000013 /uni00000019/uni00000013
/uni00000028/uni00000053/uni00000052/uni00000046/uni0000004b/uni00000014/uni00000011/uni00000013/uni00000014/uni00000011/uni00000015/uni00000014/uni00000011/uni00000017/uni00000014/uni00000011/uni00000019/uni00000037/uni00000048/uni00000056/uni00000057/uni00000003/uni0000002f/uni00000052/uni00000056/uni00000056
/uni00000037/uni00000048/uni00000056/uni00000057/uni00000003/uni0000002f/uni00000052/uni00000056/uni00000056/uni00000003/uni00000059/uni00000011/uni00000056/uni00000011/uni00000003/uni00000028/uni00000053/uni00000052/uni00000046/uni0000004b
/uni00000013 /uni00000014/uni00000013 /uni00000015/uni00000013 /uni00000016/uni00000013 /uni00000017/uni00000013 /uni00000018/uni00000013 /uni00000019/uni00000013
/uni00000028/uni00000053/uni00000052/uni00000046/uni0000004b/uni00000019/uni00000019/uni00000019/uni0000001b/uni0000001a/uni00000013/uni0000001a/uni00000015/uni0000001a/uni00000017/uni0000001a/uni00000019/uni00000037/uni00000048/uni00000056/uni00000057/uni00000003/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c
/uni00000037/uni00000048/uni00000056/uni00000057/uni00000003/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c/uni00000003/uni00000059/uni00000011/uni00000056/uni00000011/uni00000003/uni00000028/uni00000053/uni00000052/uni00000046/uni0000004b
/uni00000013 /uni00000014/uni00000013 /uni00000015/uni00000013 /uni00000016/uni00000013 /uni00000017/uni00000013 /uni00000018/uni00000013 /uni00000019/uni00000013
/uni00000028/uni00000053/uni00000052/uni00000046/uni0000004b/uni00000019/uni00000018/uni00000011/uni00000013/uni00000019/uni0000001a/uni00000011/uni00000018/uni0000001a/uni00000013/uni00000011/uni00000013/uni0000001a/uni00000015/uni00000011/uni00000018/uni0000001a/uni00000018/uni00000011/uni00000013/uni0000001a/uni0000001a/uni00000011/uni00000018/uni0000001b/uni00000013/uni00000011/uni00000013/uni0000001b/uni00000015/uni00000011/uni00000018/uni00000037/uni00000055/uni00000044/uni0000004c/uni00000051/uni00000003/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c
/uni00000037/uni00000055/uni00000044/uni0000004c/uni00000051/uni00000003/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c/uni00000003/uni00000059/uni00000011/uni00000056/uni00000011/uni00000003/uni00000028/uni00000053/uni00000052/uni00000046/uni0000004b
/uni00000013 /uni00000014/uni00000013 /uni00000015/uni00000013 /uni00000016/uni00000013 /uni00000017/uni00000013 /uni00000018/uni00000013 /uni00000019/uni00000013
/uni00000028/uni00000053/uni00000052/uni00000046/uni0000004b/uni00000015
/uni00000013/uni00000015/uni00000017/uni00000019/uni0000001b/uni0000002a/uni00000048/uni00000051/uni00000048/uni00000055/uni00000044/uni0000004f/uni0000004c/uni0000005d/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000028/uni00000055/uni00000055/uni00000052/uni00000055
/uni0000002a/uni00000048/uni00000051/uni00000048/uni00000055/uni00000044/uni0000004f/uni0000004c/uni0000005d/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000028/uni00000055/uni00000055/uni00000052/uni00000055/uni00000003/uni00000059/uni00000011/uni00000056/uni00000011/uni00000003/uni00000028/uni00000053/uni00000052/uni00000046/uni0000004b/uni00000018/uni00000018 /uni00000018/uni00000019 /uni00000018/uni0000001a /uni00000018/uni0000001b /uni00000018/uni0000001c /uni00000019/uni00000013/uni00000013/uni00000011/uni0000001c/uni00000013/uni00000013/uni00000013/uni00000011/uni0000001c/uni00000015/uni00000018/uni00000013/uni00000011/uni0000001c/uni00000018/uni00000013/uni00000013/uni00000011/uni0000001c/uni0000001a/uni00000018/uni00000014/uni00000011/uni00000013/uni00000013/uni00000013/uni00000014/uni00000011/uni00000013/uni00000015/uni00000018
/uni0000002a/uni00000030/uni00000010/uni0000002f/uni00000052/uni00000035/uni00000024/uni00000003/uni0000000b/uni00000035/uni00000003/uni00000020/uni00000003/uni00000019/uni00000017/uni0000000c
/uni00000035/uni00000030/uni00000010/uni0000002f/uni00000052/uni00000035/uni00000024/uni00000003/uni0000000b/uni00000035/uni00000003/uni00000020/uni00000003/uni00000019/uni00000017/uni0000000c/uni00000035/uni00000010/uni0000002f/uni00000052/uni00000035/uni00000024/uni00000003/uni0000000b/uni00000035/uni00000003/uni00000020/uni00000003/uni00000019/uni00000017/uni0000000c
/uni0000002f/uni00000052/uni00000035/uni00000024/uni00000003/uni0000000b/uni00000035/uni00000003/uni00000020/uni00000003/uni00000019/uni00000017/uni0000000c/uni0000002a/uni00000030/uni00000010/uni0000002f/uni00000052/uni00000035/uni00000024/uni00000003/uni0000000b/uni00000035/uni00000003/uni00000020/uni00000003/uni0000001b/uni00000013/uni0000000c
/uni00000035/uni00000030/uni00000010/uni0000002f/uni00000052/uni00000035/uni00000024/uni00000003/uni0000000b/uni00000035/uni00000003/uni00000020/uni00000003/uni0000001b/uni00000013/uni0000000c/uni00000035/uni00000010/uni0000002f/uni00000052/uni00000035/uni00000024/uni00000003/uni0000000b/uni00000035/uni00000003/uni00000020/uni00000003/uni0000001b/uni00000013/uni0000000c /uni0000002f/uni00000052/uni00000035/uni00000024/uni00000003/uni0000000b/uni00000035/uni00000003/uni00000020/uni00000003/uni0000001b/uni00000013/uni0000000c /uni00000029/uni00000058/uni0000004f/uni0000004f/uni00000003/uni00000049/uni0000004c/uni00000051/uni00000048/uni00000010/uni00000057/uni00000058/uni00000051/uni0000004c/uni00000051/uni0000004a
Figure 2: Results with ViT model on Food-101.
Table 1: Results with DeBERTaV3 model on GLUE benchmark
Method # T / I - ParamsCoLA
MccSTS-B
Pearson / SpearmanQNLI
AccMNLI
AccWNLI
AccRTE
AccMRPC
Acc / F1QQP
Acc / F1SST-2
Acc
Full FT 184M / 184M 0.668 0.892 / 0.890 0.940 0.907 0.563 0.852 0.853 / 0.891 0.918 / 0.891 0.934
LoRAR=41.26M / 1.26M 0.680 0.912 / 0.912 0.939 0.901 0.718 0.856 0.892 / 0.922 0.912 / 0.883 0.934
AdaLoRA 1.92M / 1.26M 0.663 0.905 / 0.909 0.934 0.904 0.563 0.845 0.880 / 0.911 0.912 / 0.884 0.955
SoRA 1.92M / 1.38M 0.655 0.905 / 0.907 0.926 0.889 0.690 0.848 0.895 / 0.924 0.820 / 0.762 0.820
R-LoRAR=41.26M / 1.26M 0.685 0.914 / 0.914 0.941 0.903 0.718 0.863 0.897 / 0.925 0.917 / 0.889 0.943
RM-LoRAR=41.26M / 1.26M 0.689 0.915 / 0.915 0.942 0.906 0.732 0.866 0.892 / 0.922 0.913 / 0.884 0.943
LoRAR=81.92M / 1.92M 0.672 0.913 / 0.914 0.938 0.900 0.718 0.856 0.900 / 0.928 0.916 / 0.889 0.936
AdaLoRA 3.25M / 1.92M 0.664 0.912 / 0.913 0.942 0.906 0.578 0.841 0.897 / 0.924 0.912 / 0.885 0.948
SoRA 3.25M / 2.24M 0.665 0.907 / 0.910 0.930 0.895 0.690 0.841 0.897 / 0.927 0.819 / 0.761 0.826
R-LoRAR=81.92M / 1.92M 0.694 0.914 / 0.914 0.943 0.904 0.732 0.863 0.907 / 0.933 0.917 / 0.890 0.940
RM-LoRAR=81.92M / 1.92M 0.688 0.915 / 0.915 0.944 0.907 0.747 0.870 0.914 / 0.938 0.913 / 0.885 0.948
5.2 Image Classification
Figure 1 illustrates the results achieved by the ViT model on the CIFAR-100 dataset, serving as a preliminary
measure of the performance of LoRA and the enhancement techniques for LoRA method proposed in this
paper. To simulate the theoretical results based on the fully connected layer, only the last classification
layerof the ViT model is fine-tuned by LoRA low-rank matrices. The four sub-figures of Figure 1 display
8Under review as submission to TMLR
Table 2: Results with DeBERTaV3 model on SQuAD
Method # T / I - Rank # T / I - Params EM F1 Score
Full FT N/A N/A 184M 184M 87.83±0.01 93.62 ±0.01
LoRAR=8 8 8 1.33M 1.33M 87.47 ±0.01 93.33 ±0.01
SoRAλ2=5e−4 16 16 2.66M 1.28M 82.73 ±0.02 90.49 ±0.01
RM-LoRA R=8 8 8 1.33M 1.33M 87.87±0.03 93.67±0.01
LoRAR= 4 4 4 0.67M 0.67M 87.43 ±0.02 93.35 ±0.02
AdaLoRA 8 4 1.33M 0.67M 86.48 ±0.02 92.83 ±0.01
SoRAλ2=5e−4 8 8 1.33M 0.61M 79.64 ±0.02 88.01 ±0.02
RM-LoRA R=4 4 4 0.67M 0.67M 87.57±0.03 93.51±0.02
Table 3: Results with GPT-2 model on SQuAD
Method # T / I - Rank # T / I - Params EM F1 Score
Full FT N/A N/A 117M 117M 63.30 ±0.01 74.41 ±0.02
LoRAR=16 16 16 1.69M 1.69M 61.41 ±0.02 72.76 ±0.03
AdaLoRA 32 16 3.39M 1.69M 61.70 ±0.01 72.30 ±0.03
SoRAλ2=5e−4 32 32 3.39M 1.62M 60.80 ±0.02 71.80 ±0.04
RM-LoRA R=16 16 16 1.69M 1.69M 62.80±0.01 73.50±0.02
LoRAR=8 8 8 0.85M 0.85M 60.34 ±0.02 71.92 ±0.02
AdaLoRA 16 8 1.69M 0.85M 60.20 ±0.02 71.40 ±0.02
SoRAλ2=5e−4 16 16 1.69M 0.81M 59.80 ±0.02 70.90 ±0.03
RM-LoRA R=8 8 8 0.85M 0.85M 61.00±0.01 72.20±0.01
the test loss, test accuracy, train accuracy, and generalization error (measured by train accuracy minus test
accuracy) for each method respectively.
As observed in Figure 1, the regularization and gradient masking techniques proposed in this paper both
effectively mitigate overfitting and achieve higher accuracy on the test dataset. Specifically, Regularized
LoRA (R-LoRA ) and Gradient Masking LoRA ( GM-LoRA ) represent the application of each technique
individually, while RM-LoRA combines them together as described in Section 4. Furthermore, Table 5
presents the orthogonal loss ∥∆W(∆W)⊤−I∥2
Fof∆Wafter being fine-tuned by each method, which
describes its spatial distribution. The results in Table 5 demonstrate that the orthogonal penalty term for the
LoRA matrices WAandWBin Eq. 10 effectively promotes the orthogonality of their product. Meanwhile, as
the orthogonal loss of ∆Wdecreases, the accuracy of LoRA fine-tuning on the test data increases. Therefore,
by effectively promoting the reduction of ∆W’s orthogonal loss, the RM-LoRA method proposed in this
paper achieves the best generalization performance across all rank settings.
Figure 2 presents the results achieved by the ViT model on the Food-101 dataset, which exhibit similar trends
to those observed in Figure 1 and further demonstrate the generalizability of the proposed advancements.
5.3 Natural Language Understanding
The GLUE benchmark includes two single-sentence classification tasks (CoLA, SST-2), three similarity
and paraphrase tasks (MRPC, STS-B, QQP), and four natural language inference tasks (QNLI, WNLI,
MNLI, RTE). The proposed RM-LoRA method is compared against the baseline methods under multiple
LoRA rank settings to demonstrate its superiority. Table 1 shows the performance achieved by different
methods on GLUE tasks, as well as the number of trainable and inference parameters ( # T / I - Params
respectively). The standard deviations (STD) across three independent runs are limited within 0.01 but are
omitted in the table due to space constraints. The best result for each task is highlighted in bold. R-LoRA
with the proposed regularizer consistently achieves performance gains under the same or lower inference
9Under review as submission to TMLR
Table 4: Hyperparameters for GLUE tasks
DatasetsLearning RateBatch Size # EpochsSoRA Parameters
FT LoRA λ Sparsity
CoLA 0.00001 0.001 32 3 0.001 68.96%
SST-2 0.00001 0.001 32 3 0.005 53.90%
MRPC 0.00001 0.001 32 5 0.001 79.27%
STS-B 0.00001 0.001 32 3 0.001 80.62%
QQP 0.00001 0.001 32 3 0.005 53.90%
QNLI 0.00001 0.001 32 3 0.0001 58.06%
MNLI 0.00001 0.001 32 3 0.005 64.77%
WNLI 0.00001 0.001 32 5 0.01 62.53%
RTE 0.00001 0.001 32 3 0.005 64.23%
parameter budget compared to other methods in most cases. Furthermore, RM-LoRA with gradient masking
outperforms R-LoRA in a majority of task settings. The specific fine-tuning hyperparameters adopted by
each method on the GLUE benchmark are summarized in Table 4.
Table 5: Correlation between the orthogonality of ∆Wand generalization performance
Method Rank of ∆W Orthogonality Loss of ∆W Test Acc (mean ±std)
LoRAR=32 29 585.835 76.97 ±0.02
GM-LoRA R=32 29 277.919 77.72 ±0.02
R-LoRAR=32 30 122.079 77.99 ±0.03
RM-LoRA R=32 30 95.240 78.05±0.02
LoRAR=64 55 410.154 78.41 ±0.03
GM-LoRA R=64 52 175.010 79.60 ±0.05
R-LoRAR=64 59 79.876 79.66 ±0.06
RM-LoRA R=64 57 58.010 80.07±0.04
LoRAR=80 64 342.197 78.13 ±0.03
GM-LoRA R=80 59 155.010 79.68 ±0.02
R-LoRAR=80 72 68.970 79.63 ±0.05
RM-LoRA R=80 69 48.832 80.20±0.02
5.4 Question Answering
The performance of different methods using the DeBERTaV3 model and the GPT-2 model on the SQuAD
dataset is shown in Table 2 and Table 3. Similarly, the proposed RM-LoRA method outperforms other
baselines under the same or lower inference parameter budget across varying LoRA rank settings, with EM
denoting the average exact match score and F1 referring to the average F1 score. These results highlight that
the RM-LoRA method consistently improves LoRA’s fine-tuning performance across various benchmarks.
The capability of achieving better or comparable performance with a reduced parameter budget is especially
significant for practical deployment in mobile systems, where efficiency and resource utilization are crucial
factors.
6 Conclusion
In conclusion, the exploration of the intrinsic dimension in LoRA fine-tuning reveals critical insights into
optimizing parameter efficiency and enhancing model generalization. The theoretical foundation indicates
that the intrinsic dimension of the approximated matrix updates is more pivotal in achieving effective LoRA
10Under review as submission to TMLR
fine-tuning than the previously emphasized LoRA rank. By employing a regularization technique and a
gradient masking method to encourage parameter space exploration while controlling the trainable parameters
budget, this paper presents an advanced low-rank adaptation strategy that addresses the challenges of
sub-optimal performance associated with LoRA. The better generalization performance achieved by the
proposed RM-LoRA under the same or lower parameter budget compared to other methods represents
progress in the field of parameter-efficient fine-tuning for large pre-trained models.
References
Nadav Benedek and Lior Wolf. PRILoRA: Pruned and rank-increasing low-rank adaptation. arXiv preprint
arXiv:2401.11316 , 2024.
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners.
Advances in Neural Information Processing Systems , 33, 2020.
Ning Ding, Xingtai Lv, Qiaosen Wang, Yulin Chen, Bowen Zhou, Zhiyuan Liu, and Maosong Sun. Sparse
low-rank adaptation of pre-trained language models. In Conference on Empirical Methods in Natural
Language Processing , 2023.
AlexeyDosovitskiy, LucasBeyer, AlexanderKolesnikov, DirkWeissenborn, XiaohuaZhai, ThomasUnterthiner,
Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words:
Transformers for image recognition at scale. In International Conference on Learning Representations ,
2020.
Demi Guo, Alexander M Rush, and Yoon Kim. Parameter-efficient transfer learning with diff pruning. In
Annual Meeting of the Association for Computational Linguistics , pp. 4884–4896, 2021.
Soufiane Hayou, Nikhil Ghosh, and Bin Yu. LoRA+: Efficient low rank adaptation of large models. arXiv
preprint arXiv:2402.12354 , 2024.
Junxian He, Chunting Zhou, Xuezhe Ma, Taylor Berg-Kirkpatrick, and Graham Neubig. Towards a unified
view of parameter-efficient transfer learning. In International Conference on Learning Representations ,
2021.
Pengcheng He, Jianfeng Gao, and Weizhu Chen. DeBERTaV3: Improving DeBERTa using electra-style
pre-training with gradient-disentangled embedding sharing. In International Conference on Learning
Representations , 2022.
Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Ges-
mundo, Mona Attariyan, and Sylvain Gelly. Parameter-efficient transfer learning for NLP. In International
Conference on Machine Learning , 2019.
EdwardJHu, PhillipWallis, ZeyuanAllen-Zhu, YuanzhiLi, SheanWang, LuWang, WeizhuChen, etal. LoRA:
Low-rank adaptation of large language models. In International Conference on Learning Representations ,
2021.
Feihu Jin, Yin Liu, and Ying Tan. Derivative-free optimization for low-rank adaptation in large language
models.arXiv preprint arXiv:2403.01754 , 2024.
Rabeeh Karimi Mahabadi, James Henderson, and Sebastian Ruder. Compacter: Efficient low-rank hypercom-
plex adapter layers. Advances in Neural Information Processing Systems , 34, 2021.
Dawid Jan Kopiczko, Tijmen Blankevoort, and Yuki Markus Asano. VeRA: Vector-based random matrix
adaptation. arXiv preprint arXiv:2310.11454 , 2023.
Cheolhyoung Lee, Kyunghyun Cho, and Wanmo Kang. Mixout: Effective regularization to finetune large-scale
pretrained language models. In International Conference on Learning Representations , 2020.
11Under review as submission to TMLR
Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient prompt tuning.
InConference on Empirical Methods in Natural Language Processing , pp. 3045–3059, 2021.
Xiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing continuous prompts for generation. In International
Joint Conference on Natural Language Processing , pp. 4582–4597, 2021.
Haokun Liu, Derek Tam, Mohammed Muqeeth, Jay Mohta, Tenghao Huang, Mohit Bansal, and Colin A
Raffel. Few-shot parameter-efficient fine-tuning is better and cheaper than in-context learning. Advances
in Neural Information Processing Systems , 35, 2022a.
Xiao Liu, Kaixuan Ji, Yicheng Fu, Weng Tam, Zhengxiao Du, Zhilin Yang, and Jie Tang. P-tuning: Prompt
tuning can be comparable to fine-tuning across scales and tasks. In Annual Meeting of the Association for
Computational Linguistic , pp. 61–68, 2022b.
Yuning Mao, Lambert Mathias, Rui Hou, Amjad Almahairi, Hao Ma, Jiawei Han, Wen Tau Yih, and Madian
Khabsa. UNIPELT: A unified framework for parameter-efficient language model tuning. In Annual Meeting
of the Association for Computational Linguistics , pp. 6253–6264, 2022.
Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang,
Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with
human feedback. Advances in Neural Information Processing Systems , 35, 2022.
Rushi Qiang, Ruiyi Zhang, and Pengtao Xie. BiLoRA: A Bi-level optimization framework for overfitting-
resilient low-rank adaptation of large pre-trained models. arXiv preprint arXiv:2403.13037 , 2024.
Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. SQuAD: 100,000+ questions for
machine comprehension of text. In Conference on Empirical Methods in Natural Language Processing , pp.
2383–2392, 2016.
Yi-Lin Sung, Varun Nair, and Colin A Raffel. Training neural networks with fixed sparse masks. Advances in
Neural Information Processing Systems , 34, 2021.
Yi-Lin Sung, Jaemin Cho, and Mohit Bansal. Lst: Ladder side-tuning for parameter and memory efficient
transfer learning. Advances in Neural Information Processing Systems , 35, 2022.
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix,
Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation
language models. arXiv preprint arXiv:2302.13971 , 2023.
Mojtaba Valipour, Mehdi Rezagholizadeh, Ivan Kobyzev, and Ali Ghodsi. DyLoRA: Parameter-efficient
tuning of pre-trained models using dynamic search-free low-rank adaptation. In Conference of the European
Chapter of the Association for Computational Linguistics , 2023.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser,
and Illia Polosukhin. Attention is all you need. Advances in Neural Information Processing Systems , 30,
2017.
Tu Vu, Brian Lester, Noah Constant, Rami Al-Rfou, and Daniel Cer. SPoT: Better frozen model adaptation
through soft prompt transfer. In Annual Meeting of the Association for Computational Linguistics , pp.
5039–5059, 2022.
Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. GLUE:
A multi-task benchmark and analysis platform for natural language understanding. In International
Conference on Learning Representations , 2019.
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric
Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, et al. Huggingface’s transformers: State-of-the-art
natural language processing. arXiv preprint arXiv:1910.03771 , 2019.
12Under review as submission to TMLR
Elad Ben Zaken, Yoav Goldberg, and Shauli Ravfogel. Bitfit: Simple parameter-efficient fine-tuning for
transformer-based masked language-models. In Annual Meeting of the Association for Computational
Linguistics , pp. 1–9, 2022.
Yuchen Zeng and Kangwook Lee. The expressive power of low-rank adaptation. In International Conference
on Learning Representations , 2023.
Qingru Zhang, Minshuo Chen, Alexander Bukharin, Pengcheng He, Yu Cheng, Weizhu Chen, and Tuo Zhao.
Adaptive budget allocation for parameter-efficient fine-tuning. In International Conference on Learning
Representations , 2023a.
Renrui Zhang, Jiaming Han, Chris Liu, Peng Gao, Aojun Zhou, Xiangfei Hu, Shilin Yan, Pan Lu, Hongsheng
Li, and Yu Qiao. Llama-adapter: Efficient fine-tuning of language models with zero-init attention. arXiv
preprint arXiv:2303.16199 , 2023b.
Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan,
Mona Diab, Xian Li, Xi Victoria Lin, et al. OPT: Open pre-trained transformer language models. arXiv
preprint arXiv:2205.01068 , 2022.
Bojia Zi, Xianbiao Qi, Lingzhi Wang, Jianan Wang, Kam-Fai Wong, and Lei Zhang. Delta-LoRA: Fine-tuning
high-rank parameters with the delta of low-rank matrices. arXiv preprint arXiv:2309.02411 , 2023.
13