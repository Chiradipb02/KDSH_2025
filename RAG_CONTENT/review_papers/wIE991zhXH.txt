Bandits with Preference Feedback:
A Stackelberg Game Perspective
Barna Pásztor⋆,1,2Parnian Kassraie⋆,1Andreas Krause1,2
1ETH Zurich2ETH AI Center
{bpasztor, pkassraie, krausea}@ethz.ch
Abstract
Bandits with preference feedback present a powerful tool for optimizing unknown
target functions when only pairwise comparisons are allowed instead of direct
value queries. This model allows for incorporating human feedback into online
inference and optimization and has been employed in systems for fine-tuning large
language models. The problem is well understood in simplified settings with linear
target functions or over finite small domains that limit practical interest. Taking
the next step, we consider infinite domains and nonlinear (kernelized) rewards. In
this setting, selecting a pair of actions is quite challenging and requires balancing
exploration and exploitation at two levels: within the pair, and along the iterations
of the algorithm. We propose MAXMINLCB , which emulates this trade-off as a
zero-sum Stackelberg game, and chooses action pairs that are informative and yield
favorable rewards. MAXMINLCB consistently outperforms existing algorithms
and satisfies an anytime-valid rate-optimal regret guarantee. This is due to our
novel preference-based confidence sequences for kernelized logistic estimators.
1 Introduction
In standard bandit optimization, a learner repeatedly interacts with an unknown environment that gives
numerical feedback on the chosen actions according to a utility function f. However, in applications
such as fine-tuning large language models, drug testing, or search engine optimization, the quantitative
value of design choices or test outcomes are either not directly observable, or are known to be
inaccurate, or systematically biased, e.g., if they are provided by human feedback [Casper et al., 2023].
A solution is to optimize for the target based on comparative feedback provided for a pair of queries,
which is proven to be more robust to certain biases and uncertainties in the queries [Ji et al., 2023].
Bandits with preference feedback, or dueling bandits, address this problem and propose strategies
for choosing query/action pairs that yield a high utility over the horizon of interactions. At the core
of such algorithms is uncertainty quantification and inference for fin regions of interest, which
is closely tied to exploration and exploitation dilemma over a course of queries. Observing only
comparative feedback poses an additional challenge, as we now need to balance this trade-off jointly
over two actions. This challenge is further exacerbated when optimizing over vast or infinite action
domains. As a remedy, prior work often grounds one of the actions by choosing it either randomly
or greedily, and tries to balance exploration-exploitation for the second action as a reaction to the
first [Ailon et al., 2014, Zoghi et al., 2014a, Kirschner and Krause, 2021, Mehta et al., 2023b]. This
approach works well for simple utility functions over low-dimensional domains, however does not
scale to more complex problems.
Aiming to solve this problem, we focus on continuous domains in the Euclidean vector space and
complex utility functions that belong to the Reproducing Kernel Hilbert Space (RKHS) of a poten-
tially non-smooth kernel. We propose MAXMINLCB , a sample-efficient algorithm that at every step
chooses the actions jointly , by playing a zero-sum Stackelberg (a.k.a Leader-Follower) game. We
choose the Lower Confidence Bound (LCB) of fas the objective of this game which the Leader aims
⋆Equal contribution.
38th Conference on Neural Information Processing Systems (NeurIPS 2024).to maximize and the Follower to minimize. The equilibrium of this game yields an action pair in which
the first action is a favorable candidate to maximize fand the second action is the strongest competitor
against the first. Our choice of using the LCB as the objective leads to robustness against uncertainty
when selecting the first action. Moreover, it makes the second action an optimistic choice as a competi-
tor, from its own perspective. We observe empirically that this approach creates a natural exploration
scheme, and in turn, yields a more sample-efficient algorithm compared to standard baselines.
Our game-theoretic strategy leads to an efficient bandit solver, if the LCB is a valid and tight lower
bound on the utility function. To this end, we construct a confidence sequence for fgiven pairwise
preference feedback, by modeling the noisy comparative observations with a logistic-type likelihood
function. Our confidence sequence is anytime valid and holds uniformly over the domain, under the
assumption that fresides in an RKHS. We improve prior work by removing or relaxing assumptions
on the utility while maintaining the same rate of convergence. This result on preference-based
confidence sequences may be of independent interest, as it targets the loss function that is typically
used for Reinforcement Learning with Human Feedback.
Contributions Our main contributions are:
•We propose a novel game-theoretic acquisition function for pairwise action selection with
preference feedback.
•We construct preference-based confidence sequences for kernelized utility functions that are
tight and anytime valid.
•Together this creates MAXMINLCB , an algorithm for bandit optimization with preference
feedback over continuous domains. MAXMINLCB satisfies O(γT√
T)regret, where Tis
the horizon and γTis the information gain of the kernel.
•We benchmark MAXMINLCB over a set of standard optimization problems and consistently
outperform the common baselines from the literature.
2 Related Work
Learning with indirect feedback was first studied in the supervised preference learning setting [Aiolli
and Sperduti, 2004, Chu and Ghahramani, 2005]. Subsequently, online and sequential settings were
considered, motivated by applications in which the feedback is provided in an online manner, e.g.,
by a human [Yue et al., 2012, Yue and Joachims, 2009, Houlsby et al., 2011]. Bengs et al. [2021]
surveys this field comprehensively; here we include a brief background.
Referred to as dueling bandits, a rich body of work considers (finite) multi-armed domains and learns
a preference matrix specifying the relation among the arms. Such work often relies on efficient
sorting or tournament systems based on the frequency of wins for each action [Jamieson and Nowak,
2011, Zoghi et al., 2014b, Komiyama et al., 2015, Wu and Liu, 2016, Falahatgar et al., 2017]. Rather
than jointly selecting the arms, such strategies often simplify the problem by selecting one at random
[Zoghi et al., 2014a, Zimmert and Seldin, 2018], greedily [Chen and Frazier, 2017], or from the
set of previously selected arms [Ailon et al., 2014]. In contrast, we jointly optimize both actions
by choosing them as the equilibrium of a two-player zero-sum Stackelberg game, enabling a more
efficient exploration/exploitation trade-off.
The multi-armed dueling setting, which is reducible to multi-armed bandits [Ailon et al., 2014],
naturally fails to scale to infinite compact domains, since regularity among “similar” arms is not
exploited. To go beyond finite domains, utility-based dueling bandits consider an unknown latent
function that captures the underlying preference, instead of relying on a preference matrix. The pref-
erence feedback is then modeled as the difference in the utility of two chosen actions passed through
a link function. Early work is limited to convex domains and imposes strong regularity assumptions
[Yue and Joachims, 2009, Kumagai, 2017]. These assumptions are then relaxed to general compact
domains if the utility function is linear [Dudík et al., 2015, Saha, 2021, Saha and Krishnamurthy,
2022]. Constructing valid confidence sets from comparative feedback is a challenging task. However,
it is strongly related to uncertainty quantification with direct logistic feedback, which is extensively
analyzed by the literature on logistic and generalized linear bandits [Filippi et al., 2010, Faury et al.,
2020, Foster and Krishnamurthy, 2018, Beygelzimer et al., 2019, Faury et al., 2022, Lee et al., 2024].
Preference-based bandit optimization with linear utility functions is well understood and is even
extended to reinforcement learning with preference feedback on trajectories [Saha et al., 2023, Zhan
et al., 2023, Zhu et al., 2023, Ji et al., 2023, Munos et al., 2023]. However, such approaches have
limited practical interest, since they cannot capture real-world problems with complex nonlinear
2utility functions. Alternatively, Reproducing Kernel Hilbert Spaces (RKHS) provide a rich model
class for the utility, e.g., if the chosen kernel is universal. Many have proposed heuristic algorithms
for bandits and Bayesian optimization in kernelized settings, albeit without providing theoretical
guarantees Brochu et al. [2010], González et al. [2017], Sui et al. [2017], Tucker et al. [2020],
Mikkola et al. [2020], Takeno et al. [2023].
There have been attempts to prove convergence of kernelized algorithms for preference-based
bandits [Xu et al., 2020, Kirschner and Krause, 2021, Mehta et al., 2023b,a]. Such works employ
a regression likelihood model which requires them to assume that both the utility and the probability
of preference, as a function of actions, lie in an RKHS. In doing so, they use a regression model
for solving a problem that is inherently a classification. While the model is valid, it does not result in
a sample-efficient algorithm. In contrast, we use a kernelized logistic negative log-likelihood loss to
infer the utility function, and provide confidence sets for its minimizer. In a concurrent work, Xu et al.
[2024] also consider the kernelized logistic likelihood model and propose a variant of the MULTI SBM
algorithm [Ailon et al., 2014] which uses likelihood ratio confidence sets. The theoretical approach
and resulting algorithm bear significant differences, and the regret guarantee has a strictly worse
dependency on the time horizon T, by a factor of T1/4. This is discussed in more detail in Section 5.
3 Problem Setting
Consider an agent which repeatedly interacts with an environment: at step tthe agent selects
two actions xt,x′
t∈ X and only observes stochastic binary feedback yt∈ {0,1}indicating
ifxt≻x′
t, that is, if action xtispreferred over action x′
t. Formally, P(yt= 1|xt,x′
t) =
P(xt≻x′
t), and yt= 0 with probability 1−P(xt≻x′
t). Based on the preference history
Ht={(x1,x′
1, y1), . . .(xt,x′
t, yt)}, the agent aims to sequentially select favorable action pairs.
Over a horizon of Tsteps, the success of the agent is measured through the cumulative dueling regret
RD(T) =TX
t=1P(x⋆≻xt) +P(x⋆≻x′
t)−1
2, (1)
which is the average sub-optimality gap between the chosen pair and a globally preferred action x⋆.
To better understand this notion of regret, consider the scenario where actions xtandx′
tare both
optimal. Then the probabilities are equal to 0.5and the dueling regret will not grow further, since
the regret incurred at step tis zero. This formulation of RD(T)is commonly used in the literature
of dueling Bandits and RL with preference feedback [Urvoy et al., 2013, Saha et al., 2023, Zhu
et al., 2023] and is adapted from Yue et al. [2012]. Our goal is to design an algorithm that satisfies a
sublinear dueling regret, where RD(T)/T→0asT→ ∞ . This implies that given enough evidence,
the algorithm will converge to a globally preferred action. To this end, we take a utility-based
approach and consider an unknown utility function f:X →R, which reflects the preference via
P(xt≻x′
t):=s(f(xt)−f(x′
t)) (2)
where s:R→[0,1]is the sigmoid function1, i.e. s(a) = (1 + e−a)−1. Referred to as the
Bradley-Terry model [Bradley and Terry, 1952], this probabilistic model for binary feedback is
widely used in the literature for logistic and generalized bandits [Filippi et al., 2010, Faury et al.,
2020]. Under the utility-based model, x⋆= arg maxx∈Xf(x)and we can draw connections to a
classic bandit problem with direct feedback over the utility f. In particular, Saha [2021] shows that
the dueling regret of (1)isequivalent up to constant factors, to the average utility regret of the two
actions, that isPT
t=1f(x⋆)−[f(xt) +f(x′
t)]/2.
Throughout this paper, we make two key assumptions over the environment. We assume that the
domain X ⊂Rd0is compact, and that the utility function lies in Hk, a Reproducing Kernel Hilbert
Space corresponding to some kernel function k∈ X × X → Rwith a bounded RKHS norm
∥f∥k≤B. Without a loss of generality, we further suppose that the kernel function is normalized
andk(x,x)≤1everywhere in the domain. Our set of assumptions extends the prior literature on
logistic bandits and dueling bandits from linear rewards or finite action spaces, to continuous domains
with non-parametric rewards.
While our theoretical framework targets euclidean domains, our methodology may be used on general
domains of text or images, given vector embeddings obtained via unsupervised learning. Solving
a bandit problem on top of embeddings from a pretrained language model is common practice in
1May be generalized to differentiable monotonically increasing functions satisfying s(x) +s(−x) = 1 .
3further fine-tuning of such models [e.g., Nguyen et al., 2024, Mehta et al., 2023a], and is further
demonstrated in our Yelp experiment (c.f Section 6.3). Lastly, we highlight that our results may be
smoothly extended to contextual bandits with stochastic context, by simply modifying the signature
of the kernel function to k′(x,x′,z) :X × X × Z → R, where z∈ Z denotes the context. This
setting accommodates applications in active learning for fine-tuning of large language models, where
the context is the prompt and the two actions are two alternative responses.
4 Kernelized Confidence Sequences with Direct Logistic Feedback
As a warm-up, we consider a hypothetical scenario where x′
t=xnullfor all t≥1such that
f(xnull) = 0 . Therefore at every step, we suggest an action xtand receive a noisy binary feedback
yt, which is equal to one with probability s(f(xt)). This example reduces our problem to logistic
bandits which has been rigorously analyzed for linear rewards [Filippi et al., 2010, Faury et al.,
2020]. We extend prior work to the non-parametric setting by proposing a tractable loss function for
estimating the utility function, a.k.a. reward. We present novel confidence intervals that quantify the
uncertainty on the logistic predictions uniformly over the action domain. In doing so, we propose
confidence sequences for the kernelized logistic likelihood model that are of independent interest for
developing sample-efficient solvers for online and active classification.
The feedback ytis a Bernoulli random variable, and its likelihood depends on the utility function as
s(f(xt))yt[1−s(f(xt))]1−yt. Then given history Ht, we can estimate fbyft, the minimizer of the
regularized negative log-likelihood loss
LL
k(f;Ht):=tX
τ=1−yτlog [s(f(xτ))]−(1−yτ) log [1 −s(f(xτ))] +λ
2∥f∥2
k(3)
where λ >0is the regularization coefficient. The regularization term ensures that ∥ft∥kis finite and
bounded. For simplicity, we assume throughout the main text that ∥ft∥k≤B. However, we do not
need to rely on this assumption to give theoretical guarantees. In the appendix, we present a more rig-
orous analysis by projecting ftback into the RKHS ball of radius Bto ensure that the B-boundedness
condition is met, instead of assuming it. We do not perform this projection in our experiments.
Solving for ftmay seem intractable at first glance since the loss is defined over functions in the large
space of Hk. However, it is common knowledge that the solution has a parametric form and may
be calculated by using gradient descent. This is a direct application of the Representer Theorem
[Schölkopf et al., 2001] and is detailed in Proposition 1.
Proposition 1 (Logistic Representer Theorem) .The regularized negative log-likelihood loss of (3)
has a unique minimizer ft, which takes the form ft(·) =Pt
τ=1ατk(·,xτ)where (α1, . . . α t)=:
αt∈Rtis the minimizer of the strictly convex loss
LL
k(α;Ht) =tX
τ=1−yτlog
s(α⊤kt(xτ))
−(1−yτ) log
1−s(α⊤kt(xτ))
+λ
2∥α∥2
2
withkt(x) = (k(x1,x), . . . , k (xt,x))∈Rt.
Given ft, we may predict the expected feedback for a point xass(ft(x)). Centered around this
prediction, we construct confidence sets of the form [s(ft(x))±βt(δ)σt(x)], and show their uniform
anytime validity. The width of the sets are characterized by σt(x)defined as
σ2
t(x):=k(x,x)−k⊤
t(x)(Kt+λκIt)−1kt(x) (4)
where κ= supa≤B1/˙s(a), with ˙s(a) =s(a)(1−s(a))denoting the derivative of the sigmoid
function, and Kt∈Rt×tis the kernel matrix satisfying [Kt]i,j=k(xi,xj). Our first main result
shows that for a careful choice of βt(δ), these sets contain s(f(x))simultaneously for all x∈ X and
t≥1with probability greater than 1−δ.
Theorem 2 (Kernelized Logistic Confidence Sequences) .Assume f∈ H kand∥f∥k≤B. Let
0< δ < 1and set
βt(δ):= 4LB+ 2Lr
2κ
λ(γt+ log 1 /δ), (5)
where γt:= max x1,...,xt1
2log det( It+ (λκ)−1KT), and L:= supa≤B˙s(a). Then
P(∀t≥1,x∈ X:|s(ft(x))−s(f(x))| ≤βt(δ)σt(x))≥1−δ.
4Function-valued confidence sets around the kernelized ridge estimator are analyzed and used exten-
sively to design bandit algorithms with noisy feedback on the true reward values [Valko et al., 2013,
Srinivas et al., 2010, Chowdhury and Gopalan, 2017, Whitehouse et al., 2023]. However, under noisy
logistic feedback, this literature falls short as the proposed confidence sets are no longer valid for the
kernelized logistic estimator ft. One could still estimate fusing a kernelized ridge estimator and
benefit from this line of work. However, as empirically demonstrated in Figure 1a, this will not be a
sample-efficient approach.
Proof Sketch. When minimizing the kernelized logistic loss, we do not have a closed-form solution
forft, and can only formulate it using the fact that the gradient of the loss evaluated at ftis the
null operator, i.e., ∇L(ft;Ht) :H → H =0. The key idea of our proof is to construct confidence
intervals as H-valued ellipsoids in the gradient space and show that the gradient operator evaluated
atfbelongs to it with high probability (c.f. Lemma 8). We then translate this back into intervals
around point estimates s(ft(x))uniformly for all points x∈ X. The complete proof is deferred to
Appendix A, and builds on the results of Faury et al. [2020] and Whitehouse et al. [2023].
Logistic Bandits. Such confidence sets are an integral tool for action selection under uncertainty,
and bandit algorithms often rely on them to balance exploration against exploitation. To demonstrate
how Theorem 2 may be used for bandit optimization with direct logistic feedback, we consider
LGP-UCB , the kernelized Logistic GP-UCB algorithm. Presented in Algorithm 2, it extends the
optimistic algorithm of Faury et al. [2020] from the linear to the kernelized setting, by using the
confidence bound of Theorem 2 to calculate an optimistic estimate of the reward. We proceed to
show that LGP-UCB attains a sublinear logistic regret, which is commonly defined as
RL(T) =TX
i=1s(f(x⋆))−s(f(xt)).
To the best of our knowledge, the following corollary presents the first regret bound for logistic
bandits in the kernelized setting and may be of independent interest.
Corollary 3. Letδ∈(0,1]and choose the exploration coefficients βt(δ)as described in Theorem 2
for all t≥0. Then LGP-UCB satisfies the anytime cumulative regret guarantee of
P
∀T≥0 :RL(T)≤CLβT(δ)p
Tγt
≥1−δ.
where CL:=p
8/log(1 + ( λκ)−1).
5 Main Results: Bandits with Preference Feedback
We return to our main problem setting in which a pair of actions, xtandx′
t, are chosen and the
observed response is a noisy binary indicator of xtyielding a higher utility than x′
t. While this type
of feedback is more consistent in practice, it creates quite a challenging problem compared to the
logistic problem of Section 4. The search space for action pairs X × X is significantly larger than
X, and the observed preference feedback of s(f(xt)−f(x′
t))conveys only relative information
between two actions. We start by presenting a solution to estimate fand obtain valid confidence sets
under preference feedback. Using these confidence sets we then design the MAXMINLCB algorithm
which chooses action pairs that are not only favorable, i.e., yield high utility, but are also informative
for improving the utility confidence sets.
5.1 Preference-based Confidence Sets
We consider the probabilistic model of ytas stated in Section 3, and write the corresponding
regularized negative loglikelihood loss as
LD
k(f;Ht):=tX
τ=1−yτlog [s(f(xτ)−f(x′
τ))]
−(1−yτ) log [1 −s(f(xτ)−f(x′
τ))] +λ
2∥f∥2
k.(6)
This loss may be optimized over different function classes and is commonly used for linear dueling
bandits [e.g., Saha, 2021], and has been notably successful in reinforcement learning with human
feedback [Christiano et al., 2017]. We proceed to show that the preference-based loss LD
kis
equivalent to LL
kD, the standard logistic loss (3)invoked with a specific kernel function kD. This will
5allow us to cast the problem of inference with preference feedback as a kernelized logistic regression
problem. To this end, we define the dueling kernel as
kD 
(x1,x′
1),(x2,x′
2):=k(x1,x2) +k(x′
1,x′
2)−k(x1,x′
2)−k(x′
1,x2)
for all (x1,x′
1),(x2,x′
2)∈ X × X , and let HkDbe the RKHS corresponding to it. While the two
function spaces HkDandHkare defined over different input domains, we can show that they are
isomorphic, under simple regularity conditions.
Proposition 4. Letf:X →R. Consider a kernel kand the sequence of its eigenfunctions (ϕi)∞
i=1.
Assume the eigenfunctions are zero-mean, i.e.R
x∈Xϕi(x)dx= 0. Then f∈ Hk, if and only if there
exists h∈ HkDsuch that h(x,x′) =f(x)−f(x′). Moreover, ∥h∥kD=∥f∥k.
The proof is left to Appendix C.1. The assumption on eigenfunctions in Proposition 4 is primarily
made to simplify the equivalence class. In particular, the relative preference function hcan only
capture the utility fup to a bias, i.e., if a constant bias bis added to all values of f, the corresponding
hfunction will not change. The value of bmay not be recovered by drawing queries from h,
however, this will not cause issues in terms of identifying arg max offthrough querying values
ofh. Therefore, we set b= 0by assuming that eigenfunctions of kare zero-mean. This assumption
automatically holds for all kernels that are translation or rotation invariant over symmetric domains,
since their eigenfunctions are periodic L2(X)basis functions, e.g., Matérn kernels and sinusoids.
Proposition 4 allows us to re-write the preference-based loss function of (6) as a logistic-type loss
LL
kD(h;Ht) =tX
τ=1−yτlog [s(h(xτ,x′
τ))]−(1−yτ) log [1 −s(h(xτ,x′
τ))] +λ
2∥h∥2
kD,
that is equivalent to (3)up to the choice of kernel. We define the minimizer ht:= arg min LL
kD(h;Ht)
and use it to construct anytime valid confidence sets for the utility fgiven only preference feedback.
Corollary 5 (Kernelized Preference-based Confidence Sequences) .Assume f∈ H kand∥f∥k≤B.
Choose 0< δ < 1and set βD
t(δ)andσD
tas in equations (4)and(5), with kDused as the kernel
function. Then,
P 
∀t≥1,x,x′∈ X:|s(ht(x,x′))−s(f(x)−f(x′))| ≤βD
t(δ)σD
t(x,x′)
≥1−δ.
where ht= arg min LL
kD(h;Ht).
Corollary 5 gives valid confidence sets for kernelized utility functions under preference feedback
and immediately improves prior results on linear dueling bandits and kernelized dueling bandits
with regression-type loss, to kernelized setting with logistic-type likelihood. To demonstrate this, in
Appendix C.3 we present the kernelized extensions of MAXINP(Saha [2021], Algorithm 3), and
IDS (Kirschner and Krause [2021], Algorithm 4) and prove the corresponding regret guarantees
(cf. Theorems 15 and 16). Corollary 5 holds almost immediately by invoking Theorem 2 with the
dueling kernel kDand applying Proposition 4. A proof is provided in Appendix C.1 for completeness.
Comparison to Prior Work. A line of previous work assumes that both fand the probability
s(f(x))areB-bounded members of Hk. This allows them to directly estimate s(f(x))via ker-
nelized linear regression [Xu et al., 2020, Mehta et al., 2023b, Kirschner and Krause, 2021]. The
resulting confidence intervals are then around the least squares estimator, which does not align with
the logistic estimator ft. This model does not encode the fact that s(f(x))only takes values in
[0,1]and considers a sub-Gaussian distribution for yt, instead of the Bernoulli formulation when
calculating the likelihood. Therefore, the resulting algorithms require more samples to learn an
accurate reward estimate. In a concurrent work, Xu et al. [2024] also consider the loss function of
Equation (6) and present likelihood-ratio confidence sets. The width of the sets at time T, scales withp
TlogN(Hk; 1/T)where the second term is the metric entropy of the B-bounded RKHS at resolu-
tion1/T, that is, the log-covering number of this function class, using balls of radius 1/T. It is known
thatlogN(Hk; 1/T)≍γTas defined in Theorem 2. This may be easily verified using Wainwright
[2019, Example 5.12] and [Vakili et al., 2021, Definition 1]. Noting the definition of βD
t, we see
that likelihood ratio sets of Xu et al. [2024] are wider than Corollary 5. Consequently, the presented
regret guarantee in this work is looser by a factor of T1/4compared to our bound in Theorem 6.
6Algorithm 1 MAXMINLCB
Input (βD
t)t≥1.
fort≥1do
Play the most potent pair (xt,x′
t)according to the Stackelberg game
xt= arg max
x∈Mts(ht(x,x′(x)))−βD
tσD
t(x,x′(x))
s.t.x′(x) = arg min
x′∈Mts(ht(x,x′))−βD
tσD
t(x,x′)
andx′
t=x′(xt).
Observe ytand append history.
Update ht+1andσD
t+1and the set of plausible maximizers
Mt+1={x∈ X|∀ x′∈ X:s(ht+1(x,x′)) +βD
t+1σD
t+1(x,x′)≥0.5}.
end for
5.2 Action Selection Strategy
Letx′(x) = arg min x′∈MtLCB t(x,x′)denote a response function. We propose MAXMINLCB in
Algorithm 1 for the preference feedback bandit problem that selects xtandx′
tjointly as
xt= arg max
x∈MtLCB t(x,x′(x))(Leader)
x′
t=x′(xt) (Follower)(7)
where the lower-confidence bound LCB t(x,x′) = s(ht(x,x′))−βD
tσD
t(x,x′)presents a
pessimistic estimate of handMt={x∈ X|∀ x′∈ X:s(ht(x,x′)) +βD
tσD
t(x,x′)≥0.5}is the
set of potentially optimal actions. The second action is chosen as x′
t=x′(xt). Equation (7) forms
a zero-sum Stackelberg (Leader–Follower) game where the actions xtandx′
tare chosen sequentially
[Stackelberg, 1952]. First, the Leader selects xt, then the Follower selects x′
tdepending on the
choice of xt. Importantly, due to the sequential nature of action selection, xtis chosen by the Leader
such that the Follower’s action selection function, x′(·), is accounted for in the selection of xt.
Sequential optimization problems are known to be computationally NP-hard even for linear functions
[Jeroslow, 1985]. However, due to their importance in practical applications, there are algorithms
that can efficiently approximate a solution over large domains [Sinha et al., 2017, Ghadimi and Wang,
2018, Dagréou et al., 2022, Camacho-Vallejo et al., 2023].
MAXMINLCB builds on a simple insight: if the utility fis known, both the Leader and the
Follower will choose x⋆yielding an objective value 0.5for both players, and zero dueling regret.
Since MAXMINLCB has no access to f, it leverages the confidence sets of Corollary 5 and
uses a pessimistic approach by considering the LCB instead. There are two crucial properties
of the Follower specific to this game. First, the Follower can not do worse than the Leader with
respect to the LCB t. In any scenario, the Follower can match the Leader’s action which results
inLCB t(xt,x′
t) = 0 .5. Second, for sufficiently tight confidence sets, the Follower will not select
sub-optimal actions. In this case, the Leader’s best action must be optimal as it anticipates the
Follower’s response and Equation (7) recovers the optimal actions. Therefore, the objective value
of the game considered in Equation (7) is always less than, or equal to the objective of the game
with known utility function f, i.e., LCB t(xt,x′
t)≤0.5 =f(x⋆,x⋆)and the gap shrinks with
the confidence sets. Overall, the Stackelberg game in Equation (7) can be considered as a lower
approximation of the game played with known utility function f.
The primary challenge for MAXMINLCB is to sample action pairs that sufficiently shrink the
confidence sets for the optimal actions without accumulating too much regret. MAXMINLCB
balances this exploration-exploitation trade-off naturally with its game theoretic formulation. We view
the selection of xtto be exploitative by trying to maximize the unknown utility f(xt)and minimizing
regret. On the other hand, x′
tis chosen to be the most competitive opponent to xt, i.e., testing whether
the condition LCB t(xt,x′
t)≥0.5holds. Note that LCB tis pessimistic concerning xtmaking it
robust against the uncertainty in the confidence set estimation. At the same time, LCB tis an optimistic
estimate for x′
tencouraging exploration. In our main theoretical result, we prove that under the
assumptions of Corollary 5, MAXMINLCB achieves sublinear regret on the dueling bandit problem.
70 250 500 750 1000 1250 1500 1750 2000
Time Step: T0100200300400500Regret:RL(T)
Ind-UCB
Log-UCB1LGP-UCB (Ours )
GP-UCB(a) Logistic feedback
0 200 400 600 800 1000
Time Step: T0255075100125Regret:RD(T)
MaxMinLCB (Ours )
MaxInPDoubler
RUCBMultiSBM
IDS (b) Preference feedback
Figure 1: Regret of learning the Ackley function with logistic and preference feedback. (a)Same
UCB algorithms, each using a different confidence set. LGP-UCB performs best, showcasing the
power of Theorem 2. (b): Algorithms with different acquisition functions, all using our confidence
sets. M AXMINLCB is more sample-efficient.
Theorem 6. Suppose the utility function flies in Hkwith a norm bounded by B, and that kernel k
satisfies the assumption of Proposition 4. Let δ∈(0,1]and choose the exploration coefficient βD
t(δ)
as in Corollary 5. Then MAXMINLCB satisfies the anytime dueling regret of
P
∀T≥0 :RD(T)≤C3βD
T(δ)q
TγD
T=O(γD
T√
T)
≥1−δ
where γD
Tis the T-step information gain of kernel kDandC3= (8 + 2 κ)/p
log(1 + 4( λκ)−1).
The proof is left to Appendix C.2. The information gain γD
Tin Theorem 6 quantifies the structural
complexity of the RKHS corresponding to kDand its dependence on Tis fairly understood for
kernels commonly used in applications of bandit optimization. As an example, for a Matérn kernel of
smoothness νdefined over a d-dimensional domain, γT=˜O(Td/(2ν+d))[Remark 2, Vakili et al.,
2021] and the corresponding regret bound grows sublinearly with T.
Restricting the optimization domain to Mt⊂ X is common in the literature [Zoghi et al., 2014a, Saha,
2021] despite being challenging in applications with large or continuous domains. We conjecture that
MAXMINLCB would enjoy similar regret guarantees without restricting the selection domain to Mt
as done in Equation (7). This claim is supported by our experiments in Section 6.2 which are carried
out without such restriction on the optimization domain.
6 Experiments
Our experiments are on finding the maxima of test functions commonly used in (non-convex)
optimization literature [Jamil and Yang, 2013], given only preference feedback. These functions cover
challenging optimization landscapes including several local optima, plateaus, and valleys, allowing
us to test the versatility of MAXMINLCB . We use the Ackley function for illustration in the main text
and provide the regret plots for the remainder of the functions in Appendix E. For all experiments, we
set the horizon T= 2000 and evaluate all algorithms on a uniform mesh over the input domain of size
100. Additionally, we conducted experiments on the Yelp restaurant review dataset to demonstrate the
applicability of MAXMINLCB on real-world data and its scaling to larger domains. All experiments
are run across 20random seeds and reported values are averaged over the seeds, together with standard
error. The environments and algorithms are implemented2end-to-end in JAX [Bradbury et al., 2018].
6.1 Benchmarking Confidence Sets
Performance of MAXMINLCB relies on validity and tightness of the LCB. We evaluate the quality
of our kernelized confidence bounds, using the potentially simpler task of bandit optimization given
logistic feedback. To this end, we fix the acquisition function for the logistic bandit algorithms to
the Upper Confidence Bound (UCB) function, and benchmark different methods for calculating the
confidence bound. We refer to the algorithm instantiated with the confidence sets of Theorem 2 as
LGP-UCB (c.f. Algorithm 2). The IND-UCB approach assumes that actions are uncorrelated, and
2The code is made available at github.com/lasgroup/MaxMinLCB.
8Table 1: Benchmarking RD
Tfor a variety of test utility functions, T= 2000 . The top 3 rows show
results for smoother functions without steep gradients and local optima while the bottom 5 rows show
the results for more challenging problems.
f MAXMINLCB D OUBLER MULTI SBM M AXINP RUCB IDS
Branin 104±13 114 ±9 89±13 340 ±2101±14 163 ±22
Matyas 125±5 136 ±4 106±7 136 ±6106±6 128 ±5
Rosenbrock 27±4 44 ±12 25±5 109 ±2 58 ±7 58 ±13
Ackley 56±2 72 ±2 65 ±0.5 120 ±1 84 ±0.7 111 ±9
Eggholder 113±6 154 ±4 134 ±3 230 ±34 213 ±40 141 ±12
Hoelder 141±26 154±3 136±15 204 ±20 200 ±28132±15
Michalewicz 138±14 183 ±11 155±10 260 ±40 269 ±46 188 ±21
Yelp 175±22 263 ±28 199±25 409 ±15 214 ±22 255 ±22
maintains an independent confidence interval for each action as in Lattimore and Szepesvári [2020,
Algorithm 3]. This demonstrates how LGP-UCB utilizes the correlation between actions. We also
implement LOG-UCB1 [Faury et al., 2020] that assumes that fis a linear function, i.e., f(x) =θTx
to highlight the improvements gained by kernelization. Last, we compare LGP-UCB with GP-UCB
[Srinivas et al., 2010] that estimates probabilities s(f(·))via a kernelized ridge regression task. This
comparison highlights the benefits of using our kernelized logistic estimator (Proposition 1) over
regression-based approaches [Xu et al., 2020, Kirschner and Krause, 2021, Mehta et al., 2023b,a].
Figure 1a shows that the cumulative regret of LGP-UCB is the lowest among the baselines. GP-UCB
performs closest to LGP-UCB , however, it accumulates regret linearly during the initial steps. Note
thatGP-UCB andLGP-UCB differ in the estimation of the utility function ftwhile estimating
the width of the confidence bounds similarly. This result suggests that using the logistic-type loss
(3)to infer the utility function is advantageous. As expected, IND-UCB converges at a slower rate
than LGP-UCB andGP-UCB due to ignoring the correlation between arms while LOG-UCB1 ’s
regret grows linearly as the Ackley function is misspecified under the assumption of linearity. We
defer the results on the rest of the utility functions to Table 2 in Appendix E and the figures therein.
6.2 Benchmarking Acquisition Functions
In this section, we compare MAXMINLCB with other utility-based bandit algorithms. To isolate
the benefits of our acquisition function, we instantiate all algorithms with the same confidence sets,
and use our improved preferred-based bound of Corollary 5. Therefore, our implementation differs
from the corresponding references, while we refer to the algorithms by their original name. We
consider the following baselines. DOUBLER andMULTI SBM [Ailon et al., 2014] choose xtas a
reference action from the recent history of actions and pair it with x′
twhich maximizes the joint
UCB (cf. Algorithm 5 and 6). RUCB [Zoghi et al., 2014a] chooses x′
tsimilarly, however, it selects
the reference action uniformly at random from Mt(Algorithm 7). MAXINP[Saha, 2021] also
maintains the set of plausible maximizers Mt, and at each time step, it selects the pair of actions
that maximize σD
t(x,x′)(Algorithm 3). IDS [Kirschner and Krause, 2021] selects the reference
action greedily by maximizing ft, and pairs it with an informative action (Algorithm 4). Notably,
all algorithms, with the exception of MAXINP, choose one of the actions independently and use it as
a reference point when selecting the other one. Figure 3 illustrates the differences in action selection
between UCB, maximum information, and MAXMINLCB approaches. We note that POP-BO [Xu
et al., 2024] and MULTI SBM only differ in the estimation of the confidence set. Since we deploy
the same confidence set for all acquisition functions, the two algorithms are equivalent and we use
MULTI SBM in our results, however, comparisons hold for POP-BO as well.
Figure 1b benchmarks the algorithms using the Ackley utility function, where MAXMINLCB out-
performs the baselines. All algorithms suffer from close-to-linear regret during the initial stages of
learning, suggesting that there is an inevitable exploration phase. Notably, MAXMINLCB ,IDS, and
DOUBLER are the first to select actions with high utility, while RUCB andMAXINPexplore for longer.
Table 1 shows the dueling regret for all utility functions. MAXMINLCB consistently outperforms the
baselines across the analyzed functions and achieves a low standard error, supporting its efficiency in
balancing exploration and exploitation in the preference feedback setting. Only MULTI SBM shows
comparable performance to MAXMINLCB and even outperforms it on the Matyas function which is a
relatively smooth function posing a simple optimization problem. However, MAXMINLCB achieves
lower regret on the Ackley and Eggholder functions which obtain many local optima and steeper
90 200 400 600 800 1000
Time Step: T0255075100125150Regret:RD(T)
MaxMinLCB (Ours )
MaxInPDoubler
RUCBMultiSBM
IDSFigure 2: LGP-UCB is more sample-efficient when making restaurant recommendations based on
Yelp open dataset with preference feedback. All baselines use the confidence sets of Cor. 5.
gradients showing that MAXMINLCB is preferable for challenging optimization problems. Other
acquisition functions work well only in certain cases, e.g., IDS achieves the smallest regret for opti-
mizing Matyas, while RUCB excels on the Branin function. This indicates the challenges each utility
function offers and the performance of the action selection is task dependent. The consistent perfor-
mance of MAXMINLCB demonstrates its robustness against the underlying unknown utility function.
6.3 Real-world Experiment
To demonstrate the scalability and applicability of MAXMINLCB , we conduct an experiment on
the Yelp dataset of restaurant reviews, which consists of 275restaurants and 20users after the
pre-processing stage. For each user, we want to find the restaurants that best fit their preference, via
making sequential recommendations and receiving comparative feedback. We define the action space
Xby assigning to each restaurant their respective 32-dimensional embedding of their reviews, i.e.,
X ⊆R32. The dataset provides utility values for users in the form of ratings on the scale of 1to5,
however, not all users rated every restaurant.
We estimate missing ratings using collaborative filtering [Schafer et al., 2007]. Further details on
the data processing are deferred to Appendix D.1. Figure 2 shows that the results of this larger
problem align with previous conclusions. MAXMINLCB remains the best-performing algorithm
with MULTI SBM following second. The cumulative regret is also reported in Table 1. Note that
neither of the algorithms is tuned or modified for this experiment. These results are only intended
to demonstrate that 1) the computations easily scale, and 2) the kernelized approach is still applicable
in a text-based domain, by using high-quality vector embeddings.
7 Conclusion
We addressed the problem of bandit optimization with preference feedback over large domains and
complex targets. We proposed MAXMINLCB , which takes a game-theoretic approach to the problem
of action selection under comparative feedback, and naturally balances exploration and exploitation by
constructing a zero-sum Stackelberg game between the action pairs. MAXMINLCB achieves a sublin-
ear regret for kernelized utilities, and performs competitively across a range of experiments. Lastly, by
uncovering the equivalence of learning with logistic or comparative feedback, we propose kernelized
preference-based confidence sets, which may be employed in adjacent problems, such as reinforce-
ment learning with human feedback. The technical setup considered in this work serves as a foun-
dation for a number of applications in mechanism design, such as preference elicitation and welfare
optimization from multiple feedback sources for social choice theory, which we leave as future work.
Acknowledgments and Disclosure of Funding
We thank Scott Sussex for his thorough feedback. This research was supported by the European
Research Council (ERC) under the European Union’s Horizon 2020 research and Innovation Program
Grant agreement No. 815943. Barna Pásztor was supported by an ETH AI Center doctoral fellowship,
and Parnian Kassraie by a Google PhD Fellowship.
10References
Yasin Abbasi-Yadkori. Online learning for linearly parametrized control problems . PhD thesis,
University of Alberta, 2013.
Nir Ailon, Zohar Karnin, and Thorsten Joachims. Reducing dueling bandits to cardinal bandits. In
International Conference on Machine Learning , pages 856–864. PMLR, 2014.
Fabio Aiolli and Alessandro Sperduti. Learning preferences for multiclass problems. Advances in
neural information processing systems , 17, 2004.
Sheldon Axler. Measure, integration & real analysis . Springer Nature, 2020.
Viktor Bengs, Róbert Busa-Fekete, Adil El Mesaoudi-Paul, and Eyke Hüllermeier. Preference-based
online learning with dueling bandits: A survey. The Journal of Machine Learning Research , 2021.
Alina Beygelzimer, David Pal, Balazs Szorenyi, Devanathan Thiruvenkatachari, Chen-Yu Wei, and
Chicheng Zhang. Bandit multiclass linear classification: Efficient algorithms for the separable
case. In International Conference on Machine Learning , pages 624–633. PMLR, 2019.
James Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal Maclau-
rin, George Necula, Adam Paszke, Jake VanderPlas, Skye Wanderman-Milne, et al. JAX: compos-
able transformations of Python+ NumPy programs, 2018. URL http://github.com/google/
jax.
Ralph Allan Bradley and Milton E Terry. Rank analysis of incomplete block designs: I. the method
of paired comparisons. Biometrika , 1952.
Eric Brochu, Vlad M Cora, and Nando De Freitas. A tutorial on bayesian optimization of expensive
cost functions, with application to active user modeling and hierarchical reinforcement learning.
arXiv preprint , 2010.
José-Fernando Camacho-Vallejo, Carlos Corpus, and Juan G Villegas. Metaheuristics for bilevel
optimization: A comprehensive review. Computers & Operations Research , page 106410, 2023.
Stephen Casper, Xander Davies, Claudia Shi, Thomas Krendl Gilbert, Jérémy Scheurer, Javier
Rando, Rachel Freedman, Tomasz Korbak, David Lindner, Pedro Freire, et al. Open problems
and fundamental limitations of reinforcement learning from human feedback. Transactions on
Machine Learning Research , 2023.
Bangrui Chen and Peter I Frazier. Dueling bandits with weak regret. In International Conference on
Machine Learning , pages 731–739. PMLR, 2017.
Sayak Ray Chowdhury and Aditya Gopalan. On kernelized multi-armed bandits. In International
Conference on Machine Learning , pages 844–853. PMLR, 2017.
Paul F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep
reinforcement learning from human preferences. Advances in neural information processing
systems , 30, 2017.
Wei Chu and Zoubin Ghahramani. Preference learning with gaussian processes. In Proceedings of
the 22nd international conference on Machine learning , pages 137–144, 2005.
Mathieu Dagréou, Pierre Ablin, Samuel Vaiter, and Thomas Moreau. A framework for bilevel
optimization that enables stochastic and global variance reduction algorithms. Advances in Neural
Information Processing Systems , 35:26698–26710, 2022.
Miroslav Dudík, Katja Hofmann, Robert E Schapire, Aleksandrs Slivkins, and Masrour Zoghi.
Contextual dueling bandits. In Conference on Learning Theory , pages 563–587. PMLR, 2015.
Moein Falahatgar, Alon Orlitsky, Venkatadheeraj Pichapati, and Ananda Theertha Suresh. Maximum
selection and ranking under noisy comparisons. In International Conference on Machine Learning ,
pages 1088–1096. PMLR, 2017.
11Louis Faury, Marc Abeille, Clément Calauzènes, and Olivier Fercoq. Improved optimistic algorithms
for logistic bandits. In International Conference on Machine Learning , pages 3052–3060. PMLR,
2020.
Louis Faury, Marc Abeille, Kwang-Sung Jun, and Clément Calauzènes. Jointly efficient and optimal
algorithms for logistic bandits. In International Conference on Artificial Intelligence and Statistics ,
pages 546–580. PMLR, 2022.
Sarah Filippi, Olivier Cappe, Aurélien Garivier, and Csaba Szepesvári. Parametric bandits: The
generalized linear case. Advances in neural information processing systems , 2010.
Dylan J Foster and Akshay Krishnamurthy. Contextual bandits with surrogate losses: Margin bounds
and efficient algorithms. Advances in Neural Information Processing Systems , 31, 2018.
Saeed Ghadimi and Mengdi Wang. Approximation methods for bilevel programming. arXiv preprint
arXiv:1802.02246 , 2018.
Javier González, Zhenwen Dai, Andreas Damianou, and Neil D Lawrence. Preferential bayesian
optimization. In International Conference on Machine Learning , pages 1282–1291. PMLR, 2017.
Neil Houlsby, Ferenc Huszár, Zoubin Ghahramani, and Máté Lengyel. Bayesian active learning for
classification and preference learning. arXiv preprint arXiv:1112.5745 , 2011.
Kevin G Jamieson and Robert Nowak. Active ranking using pairwise comparisons. Advances in
neural information processing systems , 24, 2011.
Momin Jamil and Xin-She Yang. A literature survey of benchmark functions for global optimisation
problems. International Journal of Mathematical Modelling and Numerical Optimisation , 4(2):
150–194, 2013.
Robert G Jeroslow. The polynomial hierarchy and a simple model for competitive analysis. Mathe-
matical programming , 32(2):146–164, 1985.
Xiang Ji, Huazheng Wang, Minshuo Chen, Tuo Zhao, and Mengdi Wang. Provable benefits of policy
learning from human preferences in contextual bandit problems. arXiv preprint arXiv:2307.12975 ,
2023.
Johannes Kirschner and Andreas Krause. Bias-robust bayesian optimization via dueling bandits. In
International Conference on Machine Learning . PMLR, 2021.
Johannes Kirschner, Tor Lattimore, and Andreas Krause. Information directed sampling for linear
partial monitoring. In Conference on Learning Theory , pages 2328–2369. PMLR, 2020.
Junpei Komiyama, Junya Honda, and Hiroshi Nakagawa. Optimal regret analysis of thompson
sampling in stochastic multi-armed bandit problem with multiple plays. In International Conference
on Machine Learning , pages 1152–1161. PMLR, 2015.
Wataru Kumagai. Regret analysis for continuous dueling bandit. Advances in Neural Information
Processing Systems , 30, 2017.
Tor Lattimore and Csaba Szepesvári. Bandit algorithms . Cambridge University Press, 2020.
Peter D Lax. Functional analysis , volume 55. John Wiley & Sons, 2002.
Junghyun Lee, Se-Young Yun, and Kwang-Sung Jun. Improved regret bounds of (multinomial)
logistic bandits via regret-to-confidence-set conversion. In International Conference on Artificial
Intelligence and Statistics , pages 4474–4482. PMLR, 2024.
Viraj Mehta, Vikramjeet Das, Ojash Neopane, Yijia Dai, Ilija Bogunovic, Jeff Schneider, and Willie
Neiswanger. Sample efficient reinforcement learning from human feedback via active exploration.
arXiv preprint , 2023a.
Viraj Mehta, Ojash Neopane, Vikramjeet Das, Sen Lin, Jeff Schneider, and Willie Neiswanger.
Kernelized offline contextual dueling bandits. arXiv preprint , 2023b.
12Petrus Mikkola, Milica Todorovi ´c, Jari Järvi, Patrick Rinke, and Samuel Kaski. Projective preferential
bayesian optimization. In International Conference on Machine Learning . PMLR, 2020.
Rémi Munos, Michal Valko, Daniele Calandriello, Mohammad Gheshlaghi Azar, Mark Rowland,
Zhaohan Daniel Guo, Yunhao Tang, Matthieu Geist, Thomas Mesnard, Andrea Michi, et al. Nash
learning from human feedback. arXiv preprint , 2023.
Tung Nguyen, Qiuyi Zhang, Bangding Yang, Chansoo Lee, Jorg Bornschein, Yingjie Miao, Sagi
Perel, Yutian Chen, and Xingyou Song. Predicting from strings: Language model embeddings for
bayesian optimization. arXiv preprint , 2024.
Aadirupa Saha. Optimal algorithms for stochastic contextual preference bandits. Advances in Neural
Information Processing Systems , 34:30050–30062, 2021.
Aadirupa Saha and Akshay Krishnamurthy. Efficient and optimal algorithms for contextual dueling
bandits under realizability. In International Conference on Algorithmic Learning Theory . PMLR,
2022.
Aadirupa Saha, Aldo Pacchiano, and Jonathan Lee. Dueling rl: Reinforcement learning with trajectory
preferences. In Proceedings of The 26th International Conference on Artificial Intelligence and
Statistics . PMLR, 2023.
J Ben Schafer, Dan Frankowski, Jon Herlocker, and Shilad Sen. Collaborative filtering recommender
systems. In The adaptive web: methods and strategies of web personalization , pages 291–324.
Springer, 2007.
Bernhard Schölkopf, Ralf Herbrich, and Alex J Smola. A generalized representer theorem. In
International conference on computational learning theory , pages 416–426. Springer, 2001.
Ankur Sinha, Pekka Malo, and Kalyanmoy Deb. A review on bilevel optimization: From classical to
evolutionary approaches and applications. IEEE transactions on evolutionary computation , 22(2):
276–295, 2017.
Niranjan Srinivas, Andreas Krause, Sham Kakade, and Matthias Seeger. Gaussian process opti-
mization in the bandit setting: No regret and experimental design. In Proceedings of the 27th
International Conference on International Conference on Machine Learning , 2010.
Heinrich von Stackelberg. Theory of the market economy . Oxford University Press, 1952.
Yanan Sui, Vincent Zhuang, Joel W Burdick, and Yisong Yue. Multi-dueling bandits with dependent
arms. arXiv preprint arXiv:1705.00253 , 2017.
Shion Takeno, Masahiro Nomura, and Masayuki Karasuyama. Towards practical preferential bayesian
optimization with skew gaussian processes. In International Conference on Machine Learning ,
pages 33516–33533. PMLR, 2023.
Maegan Tucker, Ellen Novoseller, Claudia Kann, Yanan Sui, Yisong Yue, Joel W Burdick, and
Aaron D Ames. Preference-based learning for exoskeleton gait optimization. In 2020 IEEE
international conference on robotics and automation (ICRA) . IEEE, 2020.
Tanguy Urvoy, Fabrice Clerot, Raphael Féraud, and Sami Naamane. Generic exploration and k-armed
voting bandits. In International Conference on Machine Learning . PMLR, 2013.
Sattar Vakili, Kia Khezeli, and Victor Picheny. On information gain and regret bounds in gaussian
process bandits. In International Conference on Artificial Intelligence and Statistics , pages 82–90.
PMLR, 2021.
Michal Valko, Nathaniel Korda, Rémi Munos, Ilias Flaounas, and Nelo Cristianini. Finite-time
analysis of kernelised contextual bandits. arXiv preprint arXiv:1309.6869 , 2013.
Martin J Wainwright. High-dimensional statistics: A non-asymptotic viewpoint , volume 48. Cam-
bridge university press, 2019.
Justin Whitehouse, Zhiwei Steven Wu, and Aaditya Ramdas. Improved self-normalized concentration
in hilbert spaces: Sublinear regret for gp-ucb. arXiv preprint arXiv:2307.07539 , 2023.
13Huasen Wu and Xin Liu. Double thompson sampling for dueling bandits. Advances in neural
information processing systems , 29, 2016.
Wenjie Xu, Wenbin Wang, Yuning Jiang, Bratislav Svetozarevic, and Colin N Jones. Principled
preferential bayesian optimization. arXiv preprint arXiv:2402.05367 , 2024.
Yichong Xu, Aparna Joshi, Aarti Singh, and Artur Dubrawski. Zeroth order non-convex optimization
with dueling-choice bandits. In Conference on Uncertainty in Artificial Intelligence . PMLR, 2020.
Yisong Yue and Thorsten Joachims. Interactively optimizing information retrieval systems as a
dueling bandits problem. In Proceedings of the 26th Annual International Conference on Machine
Learning , 2009.
Yisong Yue, Josef Broder, Robert Kleinberg, and Thorsten Joachims. The k-armed dueling bandits
problem. Journal of Computer and System Sciences , 2012.
Wenhao Zhan, Masatoshi Uehara, Nathan Kallus, Jason D Lee, and Wen Sun. Provable offline
reinforcement learning with human feedback. arXiv preprint , 2023.
Banghua Zhu, Michael Jordan, and Jiantao Jiao. Principled reinforcement learning with human
feedback from pairwise or k-wise comparisons. In International Conference on Machine Learning ,
pages 43037–43067. PMLR, 2023.
Julian Zimmert and Yevgeny Seldin. Factored bandits. Advances in Neural Information Processing
Systems , 31, 2018.
Masrour Zoghi, Shimon Whiteson, Remi Munos, and Maarten Rijke. Relative upper confidence
bound for the k-armed dueling bandit problem. In International conference on machine learning .
PMLR, 2014a.
Masrour Zoghi, Shimon A Whiteson, Maarten De Rijke, and Remi Munos. Relative confidence
sampling for efficient on-line ranker evaluation. In Proceedings of the 7th ACM international
conference on Web search and data mining , 2014b.
14Contents of Appendix
A Proofs for Bandits with Logistic Feedback 15
B Helper Lemmas for Appendix A 18
C Proofs for Bandits with Preference Feedback 20
C.1 Equivalence of Preference-based and Logistic Losses . . . . . . . . . . . . . . . . 20
C.2 Proof of the Preference-based Regret Bound . . . . . . . . . . . . . . . . . . . . . 21
C.3 Extending Algorithms for Linear Dueling Bandits to Kernelized Setting . . . . . . 23
C.4 Helper Lemmas for Appendix C.3 . . . . . . . . . . . . . . . . . . . . . . . . . . 27
D Details of Experiments 28
D.1 Yelp Experiment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30
E Additional Experiments 30
A Proofs for Bandits with Logistic Feedback
We have written the equations in the main text in terms of kernel matrices and function evaluations,
for easier readability. For the purpose of the proof however, we mainly rely on entities in the Hilbert
space. Consider the operator ϕ:X → H which corresponds to kernel kand satisfies k(x,·) =ϕ(x).
Then by Mercer’s theorem, any f∈ Hkmay be written as f=θ⊤ϕ, where θ∈ℓ2(N)and has a
B-bounded ℓ2norm. For a sequence of points x1, . . . ,xt∈ X, we define the (infinite-dimensional)
feature map Φt= [ϕ(x1),···,ϕ(xt)]⊤, which gives rise to the kernel matrix Kt:Rt→Rtand
the covariance operator St:Hk→ H k, respectively defined as Kt= ΦtΦ⊤
tandSt= Φ⊤
tΦt. LetIt
denote the t-dimensional identity matrix, and IHbe the identity operator acting on the RKHS. Then
it is widely known that the covariance and kernel operators are connected via det(IH+ρ−2St) =
det(It+ρ−2Kt)for any t≥1andρ̸= 0. For operators on the Hilbert space, det(A)refer to a
Fredholm determinant [c.f. Lax, 2002]. Lastly, in the appendix, we refer to the true unknown utility
function as f⋆(x) =ϕ⊤(x)θ⋆. In the main text, the true utility is simply referred to as f.
To analyze our function-valued confidence sequences, we start by re-writing the logistic loss function
L(θ;Ht) =tX
τ=1−yτlogs 
θ⊤ϕ(xτ)
−tX
τ=1(1−yτ) log 
1−s 
θ⊤ϕ(xτ)
+λ
2∥θ∥2
2
which is strictly convex in θand has a unique minimizer θtwhich satisfies
∇L(θt;Ht) =tX
τ=1−yτϕ(xτ) +gt(θt) = 0
where gt(θ) :H → H is a linear operator defined as
gt(θ):=tX
τ=1ϕ(xτ)s(θ⊤ϕ(xτ)) +λθ.
In the main text, we assumed that minimizer of Lsatisfies the norm boundedness condition. Here,
we present a more rigorous analysis which does not assume so. Instead, we work with a projected
estimator defined via
θP
t= arg min
∥θ∥2≤B∥gt(θ)−gt(θt)∥V−1
t. (8)
where Vt=St+κλIHandθtis the minimizer of L(θ;Ht). Roughly put, θP
t∈ℓ2(N)is a norm
B-bounded alternative to θt, who also satisfies a small ∇L, and therefore, is expected to result in an ac-
curate decision boundary. We will present our proof in terms of θP
t. This also proves the results in the
main text, since θP
t=θtifθtitself happens to have a B-bounded norm, as assumed in the main text.
Our analysis relies on a concentration bound for H-valued martingale sequences stated in
Abbasi-Yadkori [2013] and later in Whitehouse et al. [2023]. Below, we have adapted the statement
to match our notation.
15Lemma 7 (Corollary 1 Whitehouse et al. [2023]) .Suppose the sequence (xt)t≥1is(Ft)t≥1-adapted,
whereFt:=σ(x1, . . . ,xt, ε1, . . . , ε t−1)andεtare i.i.d. zero-mean σ-subGaussian noise. Consider
the RKHS Hcorresponding to a kernel k(x,x′) =ϕ⊤(x)ϕ(x′). Then, for any ρ >0andδ∈(0,1),
we have that, with probability at least 1−δ, simultaneously for all t≥0,
X
τ≤tετϕ(xτ)
V−1
t≤σs
2 log1
δp
det(It+ρ−2Kt)
where Vt=St+ρ2IH.
The following lemma, which extends Faury et al. [2020, Lemma 8] to H-valued operators, expresses
the closeness of θtandθ⋆in the gradient space, with respect to the norm of the covariance matrix.
Lemma 8 (Gradient Space Confidence Bounds) .Set0< δ < 1. Under the assumptions of Theorem 2
P 
∀t≥0 :∥gt(θt)−gt(θ⋆)∥V−1
t≤1
2p
2 log 1 /δ+ 2γT+r
λ
κB!
≥1−δ
where Vt=St+κλIH.
Proof of Lemma 8. Recall that gt(θ) =P
τ≤ts(θ⊤ϕ(xτ))ϕ(xτ) +λθ. Then it is straighforward
to show that
∇L(θ;Ht) =X
τ≤tyτϕ(xτ)−gt(θ).
Sinceθtis a minimizer of Lt, it holds that gt(θt) =P
τ≤tyτϕ(xτ). This allows us to write,
∥gt(θt)−gt(θ⋆)∥V−1
t=X
τ≤t 
yτ−s(ϕ⊤(xτ)θ⋆)
ϕ(xτ)−λθ⋆
V−1
t
≤X
τ≤tετϕ(xτ)
V−1
t+λ∥θ⋆∥V−1
t(9)
where ετ=yτ−s(ϕ⊤(xτ)θ⋆)is a history dependent random variable in [0,1]due to the data
model. To bound the first term, we recognize that any random variable in [0,1]isσsub-Gaussian
withσ= 0.5and apply Lemma 7. We obtain that for all t≥0, with probability greater than 1−δ
X
τ≤tετϕ(xτ)
V−1
t≤1
2s
2 log1
δp
det(It+ (λκ)−1Kt)
≤1
2p
2 log 1 /δ+ 2γT
since γt(ρ) = supx1,...,xt1
2log det( It+ρ−2Kt)). To bound the second term in (9), note that St=
Φ⊤
tΦtis PSD and therefore Vt≥κλIH. Then
λ∥θ⋆∥V−1
t≤λ√
λκ∥θ⋆∥2≤r
λ
κB.
concluding the proof.
The following lemma shows the validity of our parameter-space confidence sets.
Lemma 9. Set0< δ < 1and consider the confidence sets
Θt(δ):=n
∥θ∥ ≤B,θ−θP
t
Vt≤2√
λκB +κp
2 log 1 /δ+ 2γTo
.
Then,
P(∀t≥0 :θ⋆∈Θt(δ))≥1−δ
16Proof of Lemma 9. We check if θ⋆∈Θt(δ)by bounding the following norm
θ⋆−θP
t
Vt≤κgt(θ⋆)−gt(θP
t)
V−1
t(Lem. 12)
≤κ
∥gt(θ⋆)−gt(θt)∥V−1
t+gt(θt)−gt(θP
t)
V−1
t
≤2κ∥gt(θ⋆)−gt(θt)∥V−1
t(Eq. 8)
≤κp
2 log 1 /δ+ 2γT+ 2√
λκB (Lem. 8)
Lastly, we prove a formal extension of Theorem 2.
Theorem 10 (Theorem 2 - Formal) .Set0< δ < 1and consider the confidence sets Et(δ)⊂ H k
Et(δ) =
f(·) =θ⊤ϕ(·) :θ∈Θt(δ)	
.
Then, simultanously for all x∈ X,f∈ Et(δ)andt≥0
|s(f(x))−s(f⋆(x))| ≤βt(δ)σt(x)
with probability greater than 1−δ, where
βt(δ):= 4LB+ 2Lrκ
λp
2 log 1 /δ+ 2γT
Proof of Theorem 10. For simplicity in notation let us define
˜βt(δ):= 2√
λκB +κp
2 log 1 /δ+ 2γt.
Suppose f=θ⊤ϕ(·)is inEt(δ). Then
s(ϕ⊤(x)θ⋆)−s(ϕ⊤(x)θ)=α(x;θ,θ⋆)ϕ⊤(x)(θ−θ⋆) Lem. 11
≤Lϕ⊤(x)(θ−θ⋆) sisL-Lipschitz
≤L∥ϕ(x)∥V−1
t∥θ−θ⋆∥Vt
≤L∥ϕ(x)∥V−1
tθ−θP
t
Vt+θP
t−θ⋆
Vt
≤L∥ϕ(x)∥V−1
t
˜βt(δ) +θP
t−θ⋆
Vt
θ∈Θt(δ)
w.h.p.
≤2L˜βt(δ)∥ϕ(x)∥V−1
tLem. 9
≤2L˜βt(δ)√
λκσt(x) Lem. 13
=σt(x)
4LB+ 2Lrκ
λp
2 log 1 /δ+ 2γT
where the third to last inequality holds with probability greater than 1−δ, but the rest of the
inequalities hold deterministically.
Given the confidence set of Theorem 2, we give extend the LGP-UCB algorithm of Faury et al. to
the kernelized setting (c.f. Algorithm 2) and prove that it satisfies sublinear regret.
Proof of Corollary 3. Recall that if xtis the maximizer of the UCB, then
s(ϕ⊤(x⋆)θP
t)−s(ϕ⊤(xt)θP
t)≤σt(xt)βt(δ)−σt(x⋆)βt(δ)
Then using Theorem 10, we obtain the following for the regret at step t,
rt=s(ϕ⊤(x⋆)θ⋆)−s(ϕ⊤(xt)θ⋆)
=s(ϕ⊤(x⋆)θ⋆)−s(ϕ⊤(x⋆)θP
t) +s(ϕ⊤(xt)θP
t)−s(ϕ⊤(xt)θ⋆)
+s(ϕ⊤(x⋆)θP
t)−s(ϕ⊤(xt)θP
t)
≤σt(x⋆)βt(δ) +σt(xt)βt(δ) +σt(xt)βt(δ)−σt(x⋆)βt(δ)
≤2βt(δ)σt(xt)
17Algorithm 2 LGP-UCB
Initialize Set(βt)t≥1according to Theorem 2.
fort≥1do
Choose an optimistic action via
xt= arg max
x∈Xs(ft−1(x)) +βt−1(δ)σt−1(x)
Observe ytand append history.
Calculate ftacc. to Proposition 1 and update σtacc. to Theorem 2.
end for
with probability greater than 1−δfor all t≥0. Which allows us to bound the cumulative regret as,
RT=TX
t=1rt≤vuutTTX
t=1r2
t
≤2βT(δ)vuutTTX
t=1σ2
t(xt) βt(δ)≤βT(δ)
≤C1βT(δ)p
Tγt Lem. 14
where C1:=p
8/log(1 + ( λκ)−1).
B Helper Lemmas for Appendix A
Lemma 11 (Mean-Value Theorem) .For any x∈ X andθ1,θ2∈ℓ2(N)it holds that
s(θ⊤
2ϕ(x))−s(θ⊤
1ϕ(x)) =α(x;θ1,θ2)(θ2−θ1)⊤ϕ(x)
where
α(x;θ1,θ2) =Z1
0˙s(νθ⊤
2ϕ(x) + (1 −ν)θ⊤
1ϕ(x))dν (10)
Proof of Lemma 11. For any differentiable function s:R→Rby the fundamental theorem of
calculus we have
s(z2)−s(z1) =Zz2
z1˙s(z)dz.
We change the variable of integration to ν= (z−z1)/(z2−z1), then z=νz2+ (1−ν)z1and
re-writing the integral in terms of νgives,
s(z2)−s(z1) = (z2−z1)Z1
0˙s(νz2+ (1−ν)z1)dν.
Letting z1=θ⊤
1ϕ(x)andz2=θ⊤
2ϕ(x)concludes the proof.
Lemma 12 (Gradients to Parameters Conversion) .For all t≥0and norm B-bounded θ1,θ2∈ℓ2(N)
∥θ1−θ2∥Vt≤κ∥gt(θ1)−gt(θ2)∥V−1
t.
Proof of Lemma 12. We prove the lemma through an auxiliary operator Gt(θ1,θ2)operating on Hk
Gt(θ1,θ2) =λIH+X
τ≤tα(xτ;θ1,θ2)ϕ(xτ)ϕ⊤(xτ)
where αis defined in (10).
Step 1. First we establish how we can go back and forth between the operator norms defined based
onGtandVt. Recall that κ= supz≤B1
˙s(z). Therefore, κ−1≤˙s(z)for all z < B , implying that
α(x;θ1,θ2)≥R1
0κ−1dν=κ−1. We can then conclude,
Gt(θ1,θ2)≥λIH+X
τ≤tκ−1ϕ(xτ)ϕ⊤(xτ) =κ−1Vt. (11)
18Step 2. Now by the definition of gt(θ),
gt(θ2)−gt(θ1) =λ(θ2−θ1) +X
τ≤tϕ(xτ)
s(θ⊤
2ϕ(xτ))−s(θ⊤
1ϕ(xτ))
=λ(θ2−θ1) +X
τ≤tϕ(xτ)
α(xτ;θ1,θ2)ϕ⊤(xτ)(θ2−θ1)
(Lem. 11)
=
λIH+X
τ≤tα(xτ;θ1,θ2)ϕ(xτ)ϕ⊤(xτ)
(θ2−θ1)
=Gt(θ1,θ2) (θ2−θ1)
Therefore,
∥gt(θ2)−gt(θ1)∥G−1
t(θ1,θ2)= [gt(θ2)−gt(θ1)]⊤(θ1−θ2)
= (θ2−θ1)⊤Gt(θ2−θ1)
=∥θ2−θ1∥Gt(θ1,θ2). (12)
Step 3. Putting together the previous two steps, we can bound the Vt-norm over the parameters to
theV−1
trole in the gradients,
∥θ1−θ2∥Vt(11)
≤√κ∥θ1−θ2∥Gt(θ1,θ2)
(12)
≤√κ∥gt(θ1)−gt(θ2)∥G−1
t(θ1,θ2)
(11)
≤κ∥gt(θ1)−gt(θ2)∥V−1
t
concluding the proof.
The following two lemmas are standard results in kernelized bandits [Srinivas et al., 2010, Chowdhury
and Gopalan, 2017, e.g.,]. We include it here for completeness.
Lemma 13. Letσtbe as defined in (4). Then√
λκ∥ϕ(x)∥V−1
t=σt(x), for any x∈ X.
Proof of Lemma 13. We start by stating some identities which will later be of use. First note that 
Φ⊤
tΦt+λκIH
Φ⊤
t= Φ⊤
t 
ΦtΦ⊤
t+λκIt
which gives
Φ⊤
t 
ΦtΦ⊤
t+λκIt−1= 
Φ⊤
tΦt+λκIH−1Φ⊤
t. (13)
Moreover, by definition of ktwe have
kt(x) = Φ tϕ(x) (14)
which allow us to write 
Φ⊤
tΦt+λκIH
ϕ(x) = Φ⊤
tkt(x) +λκϕ(x),
and obtain
ϕ(x) = 
Φ⊤
tΦt+λκIH−1Φ⊤
tkt(x) +λκ 
Φ⊤
tΦt+λκI−1ϕ(x)
(13)= Φ⊤
t 
ΦtΦ⊤
t+λκIt−1kt(x) +λκ 
Φ⊤
tΦt+λκI−1ϕ(x).
Given the above equation, we conclude the proof by the following chain of equations:
k(x,x) =ϕ⊤(x)ϕ(x)
=
Φ⊤
t 
ΦtΦ⊤
t+λκIt−1kt(x) +λκ 
Φ⊤
tΦt+λκIH−1ϕ(x)⊤
ϕ(x)
=k⊤
t(x) 
ΦtΦ⊤
t+λκIt−1Φtϕ(x) +λκϕ⊤(x) 
Φ⊤
tΦt+λκIH−1ϕ(x)
(14)=k⊤
t(x) (Kt+λκIt)−1kt(x) +λκϕ⊤(x)V−1
tϕ(x)
To obtain the third equation we have used the fact that for bounded operators on Hilbert spaces,
the inverse of the adjoint is equal to the adjoint of the inverse [e.g., Theorem 10.19, Axler, 2020].
Re-ordering the equation above we obtain σ2
t(x) =λκ∥ϕ(x)∥2
V−1
t, concluding the proof.
19Lemma 14 (Controlling posterior variance with information gain) .For all T≥1,
TX
t=1σ2
t(xt)≤2γT
log(1 + ( λκ)−1),TX
t=1(σD
t(xt))2≤8γD
T
log(1 + 4( λκ)−1).
Proof of Lemma 14. By Srinivas et al. [2010, Lemma 5.3],
γT= max
x1,...xT1
2TX
t=1log(1 + ( λκ)−1σ2
t−1(xt)).
Following the technique in Srinivas et al. [2010, Lemma 5.4], since σ2
t≤1, then (λκ)−1σ2
t∈
[0,(λκ)−1]. Now for any z∈[0,(λκ)−1],z≤Clog(1 + z)where C= 1/(λκlog(1 + ( λκ)−1)).
We then may write,
TX
t=1σ2
t(xt) =TX
t=1λκ(λκ)−1σ2
t(xt)
≤TX
t=1λκClog 
1 + (λκ)−1σ2
t(xt)
=TX
t=1log(1 + ( λκ)−1σ2
t(xt))
log (1 + ( λκ)−1)
Putting both together proves the first inequality of the lemma. As for the dueling case, we can easily
check that σD
t≤2, and a similar argument yields the second inequality.
C Proofs for Bandits with Preference Feedback
This section presents the proof of main results in Section 5, and our additional contributions in the
kernelized Preference-based setting.
C.1 Equivalence of Preference-based and Logistic Losses
We start by establishing the equivalence between the logistic loss (3) and dueling loss (6).
Proof of Proposition 4. By Mercer’s theorem, we know that the kernel function khas eigen-
value eigenfunction pairs (√λi,˜ϕi)fori≥1where ˜ϕiare orthonormal. Then k(x,x′) =P
i≥1ϕi(x)ϕi(x′)with ϕi(x) =√λi˜ϕi(x). Now applying the definition of kD, it holds that
kD(z,z′) =P
i≥1ψ⊤
i(z)ψi(z′)where ψi(z) =√λi(ϕi(x)−ϕi(x′)). It is straighforward to check
thatψiare the eigenfunctions of kD, however, they may not be orthonormal. We have,
⟨ψi, ψi⟩L2= 2λi(1−b2
i)
⟨ψi, ψj⟩L2=−2p
λiλjbibj
where bi=R˜ϕi(x)d(x). By the assumption of the proposition, we have bi= 0. However, this
assumption holds automatically for all kernels commonly used in applications, e.g. any translation
invariant kernel, over many domains, since ˜ϕifor such kernels are a sine basis.
Now since f∈ Hk, it may be decomposed f=P
i≥1βiϕiand∥f∥2
k=P
i≥1β2
i≤ ∞ . And set the
difference function to h(x,x′) =P
i≥1βiψi(z). We can then bound the RKHS norm of hw.r.t. the
20kernel kDas follows
∥h∥2
kD=X
i≥1⟨h, ψi⟩L2
⟨ψi, ψi⟩L22
=X
i≥1P
j≥1βj⟨ψj, ψi⟩L2
2λi(1−bi)2
=X
i≥1
βi−bi√λi(1−bi)X
j̸=iβjbjp
λj
2
bi=0=∥f∥2
k≤B2.
Now by Mercer’s theorem, h∈ H kDsince it is decomposable as a sum of kDeigenfunctions, and
attains a B-bounded kD-norm which we showed to be equal to ∥f∥k. The other direction of the
statement is proved the same way.
Proof of Corollary 5. Consider the utility function fand define h(x,x′):=f(x)−f(x′). Then by
Proposition 4, his in RKHS of kDwith a kD-norm bounded by B. We may estimate hby minimizing
LL
kD(·;Ht). Now invoking Theorem 2 with the dueling kernel we have,
P 
∀t≥1,x,x′∈ X:|s(ht(x,x′))−s(h(x,x′))| ≤βD
t(δ)σD
t(x,x′)
≥1−δ
concluding the proof by definition of h.
C.2 Proof of the Preference-based Regret Bound
Recall Corollary 5, which states
|s(f(x⋆)−f(xt))−s(ht(x⋆,xt))| ≤βD
t(δ)σD
t(x,x′)
with high probability simultaneously for all (x,x′)andt≥1. For simplicity in notation in the rest
of this section, we define ωt(x,x′):=βD
t(δ)σD
t(x,x′)and
LCB t(x,x′) =s(ht(x,x′))−ωt(x,x′),
UCB t(x,x′) =s(ht(x,x′)) +ωt(x,x′).
Note that ωt(x,x′) =ωt(x′,x)by the symmetry of the dueling kernel kD. Furthermore, recall the
notation h(x,x) =f(x)−f(x).
Proof of Theorem 6. Step 1: First, we connect the term of x′
tin the dueling regret defined in
Equation (1) to that of xt. Note that both s(f(x∗)−f(x′
t))ands(f(x∗)−f(xt))are greater than
0.5due to the optimality of x⋆and the sigmoid function sis concave on the interval [0.5,∞). Using
the definition of concavity, we get
s(f(x∗)−f(x′
t))≤s(f(x∗)−f(xt)) + ˙s(f(x∗)−f(xt))(f(xt)−f(x′
t))
=s(f(x∗)−f(xt)) +s(f(x∗)−f(xt))s(f(xt)−f(x∗))(f(xt)−f(x′
t))
≤
1 +h(xt,x′
t)
2
s(f(x∗)−f(xt)) (15)
where the second line comes from the derivative of the sigmoid function, ˙s(x) =s(x)(1−s(x)) =
s(x)s(−x), and in the last line we use s(f(xt)−f(x∗))≤0.5.
Using Equation (15), we can upper bound the dueling regret in Equation (1) as
2rD
t=s(f(x⋆)−f(xt)) +s(f(x⋆)−f(x′
t))−1
≤s(f(x⋆)−f(xt)) +
1 +h(xt,x′
t)
2
s(f(x⋆)−f(xt))−1
≤2s(f(x⋆)−f(xt))−1 +h(xt,x′
t)
2s(f(x⋆)−f(xt)) (16)
21Step 2 : Next, we show that the single-step regret is bounded by ωt(xt,x′
t).
First, consider the term s(f(x⋆)−f(xt))
s(f(x⋆)−f(xt))−0.5≤s(ht(x⋆,xt)) +ωt(xt,x⋆)−0.5 Corollary 5
≤0.5−s(ht(xt,x⋆)) +ωt(xt,x⋆) Sigmoid equality
≤2ωt(xt,x⋆) (17)
In the last inequality, we used that xt∈ M timplying that 0.5−s(ht(xt,x⋆))≤ωt(xt,x⋆).
Combining Equation (16) and Equation (17) implies that
2rD
t≤4ωt(xt,x⋆) +h(xt,x′
t)
2s(f(x⋆)−f(xt)) (18)
Now, we bound ωt(xt,x⋆)byωt(xt,x′
t). Ifxt=x⋆, then ωt(xt,x⋆) = 0 ≤ωt(xt,x′
t). If
x′
t=x⋆, then the two expressions are equivalent. Now, assume that xt̸=x∗andx′
t̸=x⋆and
consider ωt(xt,x⋆).
Case 1: Assume that UCB t(xt,x⋆)≤UCB t(xt,x′
t). Then,
2ωt(xt,x∗) = UCB t(xt,x⋆)−LCB t(xt,x⋆)
≤UCB t(xt,x′
t)−LCB t(xt,x⋆)
≤UCB t(xt,x′
t)−LCB t(xt,x′
t) = 2 ωt(xt,x′
t) (19)
where we used the assumption in the first inequality and the definition of x′
tin the second inequality.
Case 2: Assume that UCB t(xt,x⋆)≥UCB t(xt,x′
t). First note the following connection between
LCB tandUCB t.
LCB t(x,x′) =s(ht(x,x′))−ωt(x,x′)
= 1−s(ht(x′,x))−ωt(x′,x)
= 1−UCB t(x′,x) (20)
where we used the equality s(z) = 1 −s(−z)in the second equality. Using Equation (20), the
assumption of Case 2 can be rewritten as UCB t(xt,x⋆)≥UCB t(xt,x′
t), implying that
LCB t(x′
t,xt)≥LCB t(x⋆,xt). (21)
Similarly, LCB t(xt,x′
t)≤LCB t(xt,x⋆)implies
UCB t(x′
t,xt)≥UCB t(x⋆,xt) (22)
Combining Equation (21) and Equation (22), we get
2ωt(xt,x∗) = UCB t(x⋆,xt)−LCB t(x⋆,xt)
≤UCB t(x⋆,xt)−LCB t(x′
t,xt)
≤UCB t(x′
t,xt)−LCB t(x′
t,xt) = 2 ωt(xt,x′
t) (23)
Combining Equation (19) and Equation (23), we can rewrite the first term in Equation (18) to get
2rD
t≤4ωt(xt,x′
t) +h(xt,x′
t)
2s(f(x⋆)−f(xt)). (24)
It remains to bound the second term of Equation (24). By the Mean-Value Theorem, ∃z∈
[0, h(xt,x′
t)]such that
˙s(z)(h(xt,x′
t)−0) = s(h(xt,x′
t))−f(0)
Now since κ= supz≤B1/˙s(z)then,
h(xt,x′
t)≤κ(s(h(xt,x′
t))−0.5) (25)
Next, we consider the term s(h(xt,x′
t))−0.5in Equation (25). Note that xt,x′
t∈ M timplies that
UCB t(xt,x′
t)≥0.5
s(ht(xt,x′
t))≥0.5−ωt(xt,x′
t) (26)
22Additionally note that LCB t(xt,xt) = 0 .5for all xt, therefore, by the definition of x′
t,
LCB t(xt,x′
t)≤0.5. It implies that
LCB t(xt,x′
t)≤0.5
s(ht(xt,x′
t))≤0.5 +ωt(xt,x′
t). (27)
From Equation (26) and Equation (27), it follows that
|s(ht(xt,x′
t))−0.5| ≤ωt(xt,x′
t)
furthermore,
UCB t(xt,x′
t)−0.5 =s(ht(xt,x′
t))−0.5 +ωt(xt,x′
t)
≤ |s(ht(xt,x′
t))−0.5|+ωt(xt,x′
t)
≤2ωt(xt,x′
t) (28)
and similarly
0.5−LCB t(xt,x′
t)≤2ωt(xt,x′
t). (29)
From Equation (28) and Equation (29), it follows that
|s(f(xt)−f(x′
t))−0.5| ≤max{UCB t(xt,x′
t)−0.5,0.5−LCB t(xt,x′
t)}Corollary 5
≤2ωt(xt,x′
t). (30)
Combining Equation (30) with Equation (25), it follows
h(xt,x′
t)≤2κωt(xt,x′
t) (31)
Using Equation (31) in Equation (24) and the fact that s(f(x∗)−f(xt))≤1, we get
2rD
t≤(4 +κ)ωt(xt,x′
t).
Therefore, for the cumulative dueling regret it holds
RD(T) =TX
t=1rD
t≤vuutTTX
t=1(rD
t)2
≤(2 +κ/2)βD
T(δ)vuutTTX
t=1(σD
t)2(xt,√
λκ) βt(δ)≤βD
T(δ)
≤C3βD
T(δ)q
TγD
t Lem. 14
with probability greater than 1−δfor all T≥1.
C.3 Extending Algorithms for Linear Dueling Bandits to Kernelized Setting
Maximum Informative Pair Algorithm. Proposed in Saha [2021] for linear utilities, the MAXINP
algorithm similarly maintains a set of plausible maximizer arms, and picks the pair of actions that
have the largest joint uncertainty, and therefore are expected to be informative. Algorithm 3 present
the kernelized variant of this algorithm. Using Corollary 5, we can show that the kernelized MAXINP
also satisfies a ˜O(γT√
T)regret.
Theorem 15. Letδ∈(0,1]and choose the exploration coefficient βD
t(δ)as defined in Corollary 5.
Then MAXINPsatisfies the anytime dueling regret guarantee of
P
∀T≥0 :RD(T)≤C2βD
T(δ)q
TγD
T
≥1−δ
where γD
Tis the T-step information gain of kernel kDandC2= 4/p
log(1 + 4( λκ)−1).
23Algorithm 3 MAXINP- Kernelized Variant
Input (βD
t)t≥1.
fort≥1do
Play the most informative pair via
xt,x′
t= arg max
x,x′MtσD
t(x,x′)
Observe ytand append history.
Update ht+1andσD
t+1and the set of plausible maximizers
Mt+1={x∈ X|∀ x′∈ X:s(ht+1(x,x′)) +βD
t+1σD
t+1(x,x′)>1/2}.
end for
Proof of Theorem 15. When selecting (xt,x′
t)according to Algorithm 3, we choose the pair via
xt,x′
t= arg max
x,x′∈Mtωt(x,x′) (32)
where action space is restricted to Mtand therefore,
s(ht(x⋆,xt))≤1/2 +ωt(xt,x⋆)
s(ht(x⋆,x′
t))≤1/2 +ωt(x′
t,x⋆)(33)
where we have used the identity s(−z) = 1−s(z). Simultaneously for all t≥1, we can bound the
single-step dueling regret with probability greater than 1−δ
2rD
t=s(f(x⋆)−f(xt)) +s(f(x⋆)−f(x′
t))−1
≤s(ht(x⋆,xt)) +ωt(x⋆,xt) +s(ht(x⋆,x′
t)) +ωt(x⋆,x′
t)−1 (w.h.p.)
≤2 (ωt(x⋆,xt) +ωt(x⋆,x′
t)) Eq. (33)
≤4ωt(xt,x′
t)) Eq. (32)
where for the first inequality we have invoked Corollary 5. Then for the regret satisfies
RD(T) =TX
t=1rD
t≤vuutTTX
t=1(rD
t)2
≤2βD
T(δ)vuutTTX
t=1(σD
t)2(xt,√
λκ) βt(δ)≤βD
T(δ)
≤C2βD
T(δ)q
TγD
t Lem. 14
with probability greater than 1−δfor all T≥1.
Dueling Information Directed Sampling ( IDS) Algorithm. To choose actions at each iteration t,
MAXINPandMAXMINLCB require solving an optimization problem on X × X . The Dueling IDS
approach addresses this issue and presents an algorithm which requires solving an optimization prob-
lem on X ×[0,1]and is computationally more efficient when d0>1. This work considers kernelized
utilities, however, assumes the probability of preference itself is in an RKHS and solves a kernelized
ridge regression problem to estimate the probability s(h(x,x′). In the following, we present an
improved version of this algorithm, by considering the preference-based loss (6)for estimating the
utility function. We modify the algorithm and the theoretical analysis to accommodate this.
Consider the sub-optimality gap ∆(x):=h(x⋆,x)for an action x∈ X. We may estimate this gap
using the reward estimate maximizer ˆx⋆
t:= arg maxx∈Xft(x). Suppose we choose ˆx⋆
tas one of
the actions. Then utshows an optimistic estimate of the highest obtainable reward at this step:
ut:= max
x∈Xh(x,ˆx⋆
t) +˜βtσD
t(x,x⋆
t).
where ˜βtis the exploration coefficient. We bound ∆(x)by the estimated gap
ˆ∆t(x):=ut+ht(ˆx⋆
t,x) (34)
24Algorithm 4 Preference-based IDS - Kernelized Logistic Variant
Initialize Set(βt)t≥1according to Theorem 2.
fort≥1do
Find a greedy action via fixing any point xnull∈ X and maximizing
x(1)
t=ˆx⋆
t= arg max
x∈Xht(x, xnull).
Update utandˆ∆t(x)acc. to (34).
Find an informative action and the probability of selection via
x(2)
t, pt= arg min
x∈X
p∈[0,1]
(1−p)ut+pˆ∆t(x)2
plog
1 + (λκ)−1 
σD
t(x(1)
t,x)2.
Draw αt∼Bern( pt).
ifαt= 1then choose pair (xt,x′
t) = (x(1)
t,x(2)
t)elsechoose (xt,x′
t) = (x(1)
t,x(1)
t).
Observe ytand append history.
Update ht+1andσD
t+1.
end for
and show its uniform validity in Lemma 17. We can now propose the Kernelized Logistic IDS algo-
rithm with preference feedback in Algorithm 4, as a variant of the algorithm of Kirschner and Krause.
Theorem 16. Letδ∈(0,1]and for all t≥1, set the exploration coefficient as ˜βt=βD
t(δ)/L. Then
Algorithm 4 satisfies the anytime cumulative dueling regret guarantee of
P
∀T≥0 :RD(T) =O
βD
T(δ)p
T(γT+ log 1 /δ)
≥1−δ.
Proof of Theorem 16. Our approach closely follows the proof of Kirschner and Krause [2021, The-
orem 1]. Let P(·)show the set of continuous probability distributions over a domain. Define the
expected average gap for a policy µ∈ P(X × X )
ˆ∆t(µ):=1
2Ex,x′∼µˆ∆t(x) +ˆ∆t(x′)
and the expected information ratio as
Ξt(µ):=ˆ∆2
t(µ)
Ex,x′∼µlog
1 + (λκ)−1 
σD
t(x,x′)2.
Algorithm 4 draws actions via µt= (1−pt)δ(x(1)
t,x(1)
t)+ptδ(x(1)
t,x(2)
t), where δ(x,x′)denotes a
Direct delta. Then by Kirschner et al. [2020, Lemma 1],
1
2TX
t=1h(x⋆,xt) +h(x⋆,x′
t)≤vuutTX
t=1Ξt(µt) (γT+O(log 1 /δ)) +O(logT/δ)
which allows us to bound the regret with probability greater than 1−δas
RD(T)≤LvuutTX
t=1Ξt(µt) (γT+O(log 1 /δ)) +O(LlogT/δ) (35)
since s(·)with its domain restricted to [−2B,2B]isL-Lipschitz. It remains to bound Ξt(µt), the
expected information ratio for Algorithm 4. Now by definition of µt
2ˆ∆t(µt) = (2 −pt)ˆ∆t(x(1)
t) +pt∆t(x(2)
t)
= (2−pt)
ut+ht(x(1)
t,x(1)
t)
+pt∆t(x(2)
t)
= 2(1 −pt)ut+pt(ˆ∆t(x(2)
t) +ut),
25and similarly
Eµtlog
1 +σD
t(x,x′)2
λκ
= (1−pt) log
1 +σD
t(x(1)
t,x(1)
t)2
λκ
+ptlog
1 +σD
t(x(1)
t,x(2)
t)2
λκ
=ptlog
1 + (λκ)−1σD
t(x(1)
t,x(2)
t)2
(σD
t(x,x) = 0 )
allowing us to re-write the expected information ratio as
Ξt(µt) =
2(1−pt)ut+pt(ˆ∆t(x(2)
t) +ut)2
4ptlog
1 + (λκ)−1σD
t(x(1)
t,x(2)
t)2
≤
(1−pt)ut+ptˆ∆t(x(2)
t)2
ptlog
1 + (λκ)−1σD
t(x(1)
t,x(2)
t)2 (ut≤ˆ∆t(x))
= min
x,p
(1−p)ut+pˆ∆t(x)2
plog
1 + (λκ)−1σD
t(x(1)
t,x)2 Def. ( pt,x(2)
t)
≤min
xˆ∆2
t(x)
log
1 + (λκ)−1σD
t(x(1)
t,x)2. Setp= 1
Now consider the definition of utand let ztdenote the action for which utis achieved, i.e. zt=
arg max h(x,ˆx⋆
t) +¯βt(δ)σD
t(x,ˆx⋆
t). Then
ˆ∆t(zt) =h(ˆx⋆
t,zt) +¯βt(δ)σD
t(zt,ˆx⋆
t) +h(zt,ˆx⋆
t) =¯βt(δ)σD
t(x,ˆx⋆
t),
therefore using the above chain of equations we may write
Ξt(µt)≤min
xˆ∆2
t(x)
log
1 +σD
t(x(1)
t,x)2
≤ˆ∆2
t(zt)
log
1 + (λκ)−1σD
t(x(1)
t,zt)2
≤¯β2
t(δ)σD
t(ztˆx⋆
t)2
log
1 + (λκ)−1σD
t(x(1)
t,zt)2
≤4¯β2
t(δ)
log (1 + 4( λκ)−1)(36)
where last inequality holds due to the following argument. Recall that k(x,x)≤1, implying that
σD
t(x,x′)2≤4and therefore log(1 + σD
t(x,x′)2)≥log(1 + ( λκ)−1)σD
t(x,x′)2/4, similar to
Lemma 14. To conclude the proof, from (35) and (36) it holds that
RD(T)≤LvuutTX
t=1Ξt(µt) (γT+O(log 1 /δ)) +O(LlogT/δ)
≤LvuutTX
t=14¯β2
t(δ)
log (1 + 4( λκ)−1)(γT+O(log 1 /δ)) +O(LlogT/δ)
≤Ls
4T¯β2
T(δ)
log (1 + 4( λκ)−1)(γT+O(log 1 /δ)) +O(LlogT/δ)
=O
βD
T(δ)p
T(γT+ log 1 /δ)
with probability greater than 1−δ, simultaneously for all T≥1.
26C.4 Helper Lemmas for Appendix C.3
Lemma 17. Let0< δ < 1andf∈ Hk. Suppose supa≤B˙s(a) =Landsupa≤B1/˙s(a) =κ. Then
P(∀t≥0,x∈ X: ∆(x)≤2ˆ∆t(x))≥1−δ.
Proof of Lemma 17. Note that for any three inputs x1,x2,x3
h(x1,x3) =h(x1,x2) +h(x2,x3). (37)
Therefore, from the definition of the estimated gap get
ˆ∆t(x) = max
z∈Xh(z,ˆx⋆
t) +ht(ˆx⋆
t,x) +¯βt(δ)σD
t(z,ˆx⋆
t)
= max
z∈Xh(z,x) +¯βt(δ)σD
t(z,ˆx⋆
t)
≥h(x,x) +¯βt(δ)σD
t(x,x⋆
t)
=¯βt(δ)σD
t(x,x⋆
t). (38)
Then going back to the definition of the true gap we may write
∆(x) = max
z∈Xh(z,x)
= max
z∈Xh(z,ˆx⋆
t) +h(ˆx⋆
t,x) Eq. (37)
w.h.p.
≤max
z∈XhP
t(z,ˆx⋆
t) +ht(ˆx⋆
t,x) +¯βt(δ)
σD
t(z,ˆx⋆
t) +σD
t(ˆx⋆
t,x)
Lem. 18
=ut+hP
t(ˆx⋆
t,x) +¯βt(δ)σD
t(ˆx⋆
t,x) Def.ut
=ˆ∆t(x) +¯βt(δ)σD
t(x⋆
t,x) Def. ˆ∆t(x)
≤2ˆ∆t(x) Eq. (38)
with probability greater than 1−δ.
Lemma 18. Assume f∈ Hk. Suppose supa≤B1/˙s(a) =κ. Then for any 0< δ < 1
P
∀t≥1, x∈ X:h(x,x′)−hP
t(x,x′)≤¯βt(δ)σD
t(x,x′;√
λκ)
≥1−δ
where
¯βt(δ):= 2B+rκ
λq
2 log 1 /δ+ 2γt(√
λκ).
Proof of Lemma 18. This lemma is effectively a weaker parallel of Corollary 5. We have
h(x,x′)−hP
t(x,x′)=f(x,)−f(x′)−(fP
t(x,)−fP
t(x′))
=ψ⊤(x,x′)(θ⋆−θP
t)
≤ ∥ψ(x,x′)∥(VD
t)−1θ⋆−θP
t
VD
t
w.h.p.
≤√
λκ¯βt(δ)∥ψ(x,x′)∥(VD
t)−1 Lem. 9
≤¯βt(δ)σD
t(x,√
λκ) Lem. 13
where the third to last inequality holds with probability greater than 1−δ, but the rest of the
inequalities hold deterministically.
27D Details of Experiments
(1,2) (1,3) (2,1) (2,3) (3,1) (3,2)
Actions: (x,x/prime)0.00.51.0P[x/followsx/prime]MaxMinLCBOptimism Max Info
True Prob. Estimated Prob. Conﬁdence Set Regret
Figure 3: Confidence sets for an illustrative problem with 3arms at a single time step. Annotated
arrows highlight the action selection for three common approaches. MAXMINLCB selects the
action pair (1,2)with the least regret. Upper-bound maximization ( OPTIMISM ) and information
maximization (M AXINFO) choose sub-optimal arms.
Test Environments. We use a wide range of target functions common to the optimization literature
[Jamil and Yang, 2013], to evaluate the robustness of MAXMINLCB . The results are reported in
Table 1 and Table 2. Note that for the experiments we negate them all to get utilities. We use a
uniform grid of 100points over their specified domains and scale the utility values to [−3,3].
• Ackley: X= [−5,5]d, d= 2
f(x) =−20 exp
−0.2vuut1
ddX
i=1x2
i
−exp 
1
ddX
i=1cos(2 πxi)!
+ 20 + exp(1)
• Branin: X= [−5,10]×[0,15]
f(x) =
x2−5.1
4π2x2
1+5
πx1−62
+ 10
1−1
8π
cos(x1) + 10
• Eggholder: X= [−512,512]2
f(x) =−(x2+ 47) sinr
|x2+x1
2+ 47|
−x1sinp
|x1−(x2+ 47)|
• Hölder: X= [−10,10]2
f(x) =−|sin(x1) cos( x2) exp 
|1−p
x2
1+x2
2
π|!
|
• Matyas: X= [−10,10]2
f(x) = 0 .26(x2
1+x2
2)−0.48x1x2
• Michalewicz: X= [0, π]d, d= 2, m= 10
f(x) =−dX
i=1sin(xi) sin2mix2
i
π
• Rosenbrock: X= [−5,10]2
f(x) =d−1X
i=1
100(xi+1−x2
i)2+ (xi−1)2
28Algorithm 5 DOUBLER [Ailon et al., 2014]
Input (βD
t)t≥1.
LetLbe any action from X
fort≥1do
forj= 1, . . . , 2tdo
Select x′
tuniformly randomly from L
Select xt= arg maxx∈Mts(ht(x,x′
t)) +βD
tσD
t(x,x′
t)
Observe ytand append history.
Update ht+1andσD
t+1
end for
L ← the multi-set of actions played as x′
tin the last for-loop over index j
end for
Algorithm 6 MULTI SBM [Ailon et al., 2014]
Input (βD
t)t≥1.
fort≥1do
Setxt←x′
t−1
Select x′
t= arg maxx∈Mts(ht(x,xt)) +βD
tσD
t(x,xt)
Observe ytand append history.
Update ht+1andσD
t+1and the set of plausible maximizers
Mt+1={x∈ X|∀ x′∈ X:s(ht+1(x,x′)) +βD
t+1σD
t+1(x,x′)>1/2}.
end for
Algorithm 7 RUCB [Zoghi et al., 2014a]
Input (βD
t)t≥1.
fort≥1do
Select x′
tuniformly randomly from Mt
Select xt= arg maxx∈Mts(ht(x,x′
t)) +βD
tσD
t(x,x′
t)
Observe ytand append history.
Update ht+1andσD
t+1and the set of plausible maximizers
Mt+1={x∈ X|∀ x′∈ X:s(ht+1(x,x′)) +βD
t+1σD
t+1(x,x′)>1/2}.
end for
Acquisition Function Maximization. In our computations, to eliminate additional noise coming
from approximate solvers, we use an exhaustive search over the domain for the action selection of
LGP-UCB ,MAXMINLCB , and other presented algorithms. For the numerical experiments presented
in this paper, we do not consider this as a practical limitation. Due to our efficient implementation in
JAX, this optimization step can be carried out in parallel and seamlessly support accelerator devices
such as GPUs and TPUs.
Hyper-parameters for Logistic Bandits. We set δ= 0.1for all algorithms. For GP-UCB and
LGP-UCB , we set β= 1, and 0.25for the noise variance. We use the Radial Basis Function
(RBF) kernel and choose the variance and length scale parameters from [0.1,1.0]to optimize their
performance separately. For LGP-UCB , we tuned λ, theL2penalty coefficient in Proposition 1,
on the grid [0.0,0.1,1.0,5.0]andBon[1.0,2.0,3.0]. The hyper-parameter selections were done
for each algorithm separately to create a fair comparison.
Hyper-parameters for Preference-based Bandits. We tune the same parameters of LGP-UCB
for the preference feedback bandit problem on the following grid: λ∈[0,0.1,1],B∈[1,2,3], and
[0.1,1]for the kernel variance and length scale. The same hyper-parameters are tuned separately for
every baseline .
Pseudo-code for Baselines. Algorithm 5, Algorithm 6, and Algorithm 7 described the baselines used
for the benchmark of Section 6.2. MAXINPandIDS are defined in Algorithm 3 and Algorithm 4,
29respectively, in Appendix C.3 alongside with their theoretical analysis. We note that DOUBLER
includes an internal for-loop, therefore, we adjusted the time-horizon Tsuch that it observes the same
number of feedback ytas the other algorithms for a fair comparison.
Computational Resources and Costs. We ran our experiments on a shared cluster equipped with
various NVIDIA GPUs and AMD EPYC CPUs. Our default configuration for all experiments was
a single GPU with 24 GB of memory, 16 CPU cores, and 16 GB of RAM. Each experiment of
the11configurations reported in Section 6.2 ran for about 12hours and the experiment reported
in Section 6.1 ran for 5 hours. The total computational cost to reproduce our results is around 140
hours of the default configuration. Our total computational costs including the failed experiments are
estimated to be 2-3 times more.
D.1 Yelp Experiment
We filter the Yelp data3for restaurants in Philadelphia, USA with at least 500reviews and users
who reviewed at least 90restaurants. The final dataset includes 275restaurants, 20users, and a
total of 2563 reviews. We define the action space Xby assigning to each restaurant their respective
32-dimensional embedding of their reviews, i.e., X ⊆R32. For each restaurant, we concatenate
all reviews in the filtered dataset and we use the TEXT -EMBEDDING -3-LARGE OpenAI embedding
model4to retrieve the embeddings. The Yelp dataset provides utility values for users in the form
of ratings on the scale of 1to5, however, not all users rated every restaurant. We use collaborative
filtering to estimate the missing reviews [Schafer et al., 2007]. For each user, these values are used as
the utilities for restaurants during the simulation. Experiments are conducted separately for each user,
therefore, utility values are not aggregated but the action space is identical for each experiment.
Note that we do not assume any explicit functional form of the utility functions fthat we calibrate
to this data. Instead, the actions space Xand utility values are derived separately from the dataset.
Regardless, the results presented in Section 6.3 show that our kernelized method achieves good
performance on this task.
E Additional Experiments
In this section, we provide Table 2 that details our logistic bandit benchmark, complementing the
results in Section 6.1. Figure 4 and Figure 5 show the logistic and dueling regret of additional test
functions, complementing the results of Table 1.
Table 2: Benchmarking RL
Tfor a variety of test utility functions, T= 2000 .
f LGP-UCB GP-UCB I ND-UCB L OG-UCB1
Ackley 23.97±1.54 96 .35±1.27 479 .63±3.42 1810 .30±0.00
Branin 75.23±17.51 44.81±2.81 142 .37±1.33 1810 .30±0.00
Eggholder 167.11±31.26 152.34±4.28 559 .56±4.15 1041 .00±0.00
Hoelder 57.35±10.23 150 .41±9.64 426 .28±2.94 105 .64±4.88
Matyas 36.64±8.77 50 .21±2.07 137 .98±1.21 920 .48±0.57
Michalewicz 283.85±3.62 175.46±2.86 566 .36±3.75 1810 .30±0.00
Rosenbrock 8.92±0.33 26 .14±0.87 76 .13±0.84 897 .04±120.68
0 250 500 750 1000 1250 1500 1750 2000
Time Step: T050100150200Regret:RL(T)
Ind-UCB
Log-UCB1LGP-UCB (Ours )
GP-UCB
0 250 500 750 1000 1250 1500 1750 2000
Time Step: T0100200300Regret:RD(T)
MaxMinLCB (Ours )
MaxInPDoubler
RUCBMultiSBM
IDS
Figure 4: Regret with Branin utility function with logistic (left) and preference (right) feedback.
3Source: https://www.yelp.com/dataset
4Source: https://platform.openai.com/docs/guides/embeddings
300 250 500 750 1000 1250 1500 1750 2000
Time Step: T0100200300400500Regret:RL(T)
Ind-UCB
Log-UCB1LGP-UCB (Ours )
GP-UCB
0 250 500 750 1000 1250 1500 1750 2000
Time Step: T050100150200250Regret:RD(T)
MaxMinLCB (Ours )
MaxInPDoubler
RUCBMultiSBM
IDS
0 250 500 750 1000 1250 1500 1750 2000
Time Step: T0100200300400500Regret:RL(T)
Ind-UCB
Log-UCB1LGP-UCB (Ours )
GP-UCB
0 250 500 750 1000 1250 1500 1750 2000
Time Step: T050100150200Regret:RD(T)
MaxMinLCB (Ours )
MaxInPDoubler
RUCBMultiSBM
IDS
0 250 500 750 1000 1250 1500 1750 2000
Time Step: T050100150200Regret:RL(T)
Ind-UCB
Log-UCB1LGP-UCB (Ours )
GP-UCB
0 250 500 750 1000 1250 1500 1750 2000
Time Step: T0255075100125Regret:RD(T)
MaxMinLCB (Ours )
MaxInPDoubler
RUCBMultiSBM
IDS
0 250 500 750 1000 1250 1500 1750 2000
Time Step: T0100200300400500Regret:RL(T)
Ind-UCB
Log-UCB1LGP-UCB (Ours )
GP-UCB
0 250 500 750 1000 1250 1500 1750 2000
Time Step: T0100200300Regret:RD(T)
MaxMinLCB (Ours )
MaxInPDoubler
RUCBMultiSBM
IDS
0 250 500 750 1000 1250 1500 1750 2000
Time Step: T020406080100Regret:RL(T)
Ind-UCB
Log-UCB1LGP-UCB (Ours )
GP-UCB
0 250 500 750 1000 1250 1500 1750 2000
Time Step: T020406080100Regret:RD(T)
MaxMinLCB (Ours )
MaxInPDoubler
RUCBMultiSBM
IDSFigure 5: Top to bottom Regret for Eggholder, Hölder, Matyas Michalewicz, Rosenbrock functions,
with logistic (left) and preference (right) feedback.
31NeurIPS Paper Checklist
1.Claims
Question: Do the main claims made in the abstract and introduction accurately reflect the
paper’s contributions and scope?
Answer: [Yes]
Justification: We provide theoretical guarantees of the proposed algorithms in Section 4 and
Section 5. In Section 6, we provide the results for our numerical experiments supporting our
claims.
Guidelines:
•The answer NA means that the abstract and introduction do not include the claims
made in the paper.
•The abstract and/or introduction should clearly state the claims made, including the
contributions made in the paper and important assumptions and limitations. A No or
NA answer to this question will not be perceived well by the reviewers.
•The claims made should match theoretical and experimental results, and reflect how
much the results can be expected to generalize to other settings.
•It is fine to include aspirational goals as motivation as long as it is clear that these goals
are not attained by the paper.
2.Limitations
Question: Does the paper discuss the limitations of the work performed by the authors?
Answer: [Yes]
Justification: The paper is of theoretical nature. All theoretical assumptions are presented in
the text, and we discuss how limiting they are. This is mainly in the Problem Setting section,
or theorem statements.
Guidelines:
•The answer NA means that the paper has no limitation while the answer No means that
the paper has limitations, but those are not discussed in the paper.
• The authors are encouraged to create a separate "Limitations" section in their paper.
•The paper should point out any strong assumptions and how robust the results are to
violations of these assumptions (e.g., independence assumptions, noiseless settings,
model well-specification, asymptotic approximations only holding locally). The authors
should reflect on how these assumptions might be violated in practice and what the
implications would be.
•The authors should reflect on the scope of the claims made, e.g., if the approach was
only tested on a few datasets or with a few runs. In general, empirical results often
depend on implicit assumptions, which should be articulated.
•The authors should reflect on the factors that influence the performance of the approach.
For example, a facial recognition algorithm may perform poorly when image resolution
is low or images are taken in low lighting. Or a speech-to-text system might not be
used reliably to provide closed captions for online lectures because it fails to handle
technical jargon.
•The authors should discuss the computational efficiency of the proposed algorithms
and how they scale with dataset size.
•If applicable, the authors should discuss possible limitations of their approach to
address problems of privacy and fairness.
•While the authors might fear that complete honesty about limitations might be used by
reviewers as grounds for rejection, a worse outcome might be that reviewers discover
limitations that aren’t acknowledged in the paper. The authors should use their best
judgment and recognize that individual actions in favor of transparency play an impor-
tant role in developing norms that preserve the integrity of the community. Reviewers
will be specifically instructed to not penalize honesty concerning limitations.
3.Theory Assumptions and Proofs
32Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?
Answer: [Yes]
Justification: Proposition 1, Theorem 2, Corollary 3, Proposition 4, Corollary 5, Theorem 6
describe our theoretical proofs with detailed explanation of the assumptions.
Guidelines:
• The answer NA means that the paper does not include theoretical results.
•All the theorems, formulas, and proofs in the paper should be numbered and cross-
referenced.
•All assumptions should be clearly stated or referenced in the statement of any theorems.
•The proofs can either appear in the main paper or the supplemental material, but if
they appear in the supplemental material, the authors are encouraged to provide a short
proof sketch to provide intuition.
•Inversely, any informal proof provided in the core of the paper should be complemented
by formal proofs provided in appendix or supplemental material.
• Theorems and Lemmas that the proof relies upon should be properly referenced.
4.Experimental Result Reproducibility
Question: Does the paper fully disclose all the information needed to reproduce the main ex-
perimental results of the paper to the extent that it affects the main claims and/or conclusions
of the paper (regardless of whether the code and data are provided or not)?
Answer: [Yes]
Justification: We describe the experimental setting in Section 6 while providing further
details on the hyperparameter selection, pseudocode, and utility function definition in
Appendix E.
Guidelines:
• The answer NA means that the paper does not include experiments.
•If the paper includes experiments, a No answer to this question will not be perceived
well by the reviewers: Making the paper reproducible is important, regardless of
whether the code and data are provided or not.
•If the contribution is a dataset and/or model, the authors should describe the steps taken
to make their results reproducible or verifiable.
•Depending on the contribution, reproducibility can be accomplished in various ways.
For example, if the contribution is a novel architecture, describing the architecture fully
might suffice, or if the contribution is a specific model and empirical evaluation, it may
be necessary to either make it possible for others to replicate the model with the same
dataset, or provide access to the model. In general. releasing code and data is often
one good way to accomplish this, but reproducibility can also be provided via detailed
instructions for how to replicate the results, access to a hosted model (e.g., in the case
of a large language model), releasing of a model checkpoint, or other means that are
appropriate to the research performed.
•While NeurIPS does not require releasing code, the conference does require all submis-
sions to provide some reasonable avenue for reproducibility, which may depend on the
nature of the contribution. For example
(a)If the contribution is primarily a new algorithm, the paper should make it clear how
to reproduce that algorithm.
(b)If the contribution is primarily a new model architecture, the paper should describe
the architecture clearly and fully.
(c)If the contribution is a new model (e.g., a large language model), then there should
either be a way to access this model for reproducing the results or a way to reproduce
the model (e.g., with an open-source dataset or instructions for how to construct
the dataset).
(d)We recognize that reproducibility may be tricky in some cases, in which case
authors are welcome to describe the particular way they provide for reproducibility.
33In the case of closed-source models, it may be that access to the model is limited in
some way (e.g., to registered users), but it should be possible for other researchers
to have some path to reproducing or verifying the results.
5.Open access to data and code
Question: Does the paper provide open access to the data and code, with sufficient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?
Answer: [Yes]
Justification: We include the code used to carry out the experiments in the Supplementary
Material.
Guidelines:
• The answer NA means that paper does not include experiments requiring code.
•Please see the NeurIPS code and data submission guidelines ( https://nips.cc/
public/guides/CodeSubmissionPolicy ) for more details.
•While we encourage the release of code and data, we understand that this might not be
possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
including code, unless this is central to the contribution (e.g., for a new open-source
benchmark).
•The instructions should contain the exact command and environment needed to run to
reproduce the results. See the NeurIPS code and data submission guidelines ( https:
//nips.cc/public/guides/CodeSubmissionPolicy ) for more details.
•The authors should provide instructions on data access and preparation, including how
to access the raw data, preprocessed data, intermediate data, and generated data, etc.
•The authors should provide scripts to reproduce all experimental results for the new
proposed method and baselines. If only a subset of experiments are reproducible, they
should state which ones are omitted from the script and why.
•At submission time, to preserve anonymity, the authors should release anonymized
versions (if applicable).
•Providing as much information as possible in supplemental material (appended to the
paper) is recommended, but including URLs to data and code is permitted.
6.Experimental Setting/Details
Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?
Answer: [Yes]
Justification: We provide comprehensive explanations of the setting, hyperparameters,
algorithms, and functions used in Section 6 and in Appendix E.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The experimental setting should be presented in the core of the paper to a level of detail
that is necessary to appreciate the results and make sense of them.
•The full details can be provided either with the code, in appendix, or as supplemental
material.
7.Experiment Statistical Significance
Question: Does the paper report error bars suitably and correctly defined or other appropriate
information about the statistical significance of the experiments?
Answer: [Yes]
Justification: We report all results in our figures and tables as the average and standard error
over20random seeds.
Guidelines:
• The answer NA means that the paper does not include experiments.
34•The authors should answer "Yes" if the results are accompanied by error bars, confi-
dence intervals, or statistical significance tests, at least for the experiments that support
the main claims of the paper.
•The factors of variability that the error bars are capturing should be clearly stated (for
example, train/test split, initialization, random drawing of some parameter, or overall
run with given experimental conditions).
•The method for calculating the error bars should be explained (closed form formula,
call to a library function, bootstrap, etc.)
• The assumptions made should be given (e.g., Normally distributed errors).
•It should be clear whether the error bar is the standard deviation or the standard error
of the mean.
•It is OK to report 1-sigma error bars, but one should state it. The authors should
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
of Normality of errors is not verified.
•For asymmetric distributions, the authors should be careful not to show in tables or
figures symmetric error bars that would yield results that are out of range (e.g. negative
error rates).
•If error bars are reported in tables or plots, The authors should explain in the text how
they were calculated and reference the corresponding figures or tables in the text.
8.Experiments Compute Resources
Question: For each experiment, does the paper provide sufficient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?
Answer: [Yes]
Justification: We describe computation resources and time used for the experiments in
Appendix D.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The paper should indicate the type of compute workers CPU or GPU, internal cluster,
or cloud provider, including relevant memory and storage.
•The paper should provide the amount of compute required for each of the individual
experimental runs as well as estimate the total compute.
•The paper should disclose whether the full research project required more compute
than the experiments reported in the paper (e.g., preliminary or failed experiments that
didn’t make it into the paper).
9.Code Of Ethics
Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines ?
Answer: [Yes]
Justification: The authors have reviewed the NeurIPS Code of Ethics and conducted the
research following the guidelines.
Guidelines:
•The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
•If the authors answer No, they should explain the special circumstances that require a
deviation from the Code of Ethics.
•The authors should make sure to preserve anonymity (e.g., if there is a special consid-
eration due to laws or regulations in their jurisdiction).
10.Broader Impacts
Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?
Answer: [NA]
35Justification: The research presented in this paper is theoretical without any immediate
societal impact.
Guidelines:
• The answer NA means that there is no societal impact of the work performed.
•If the authors answer NA or No, they should explain why their work has no societal
impact or why the paper does not address societal impact.
•Examples of negative societal impacts include potential malicious or unintended uses
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
(e.g., deployment of technologies that could make decisions that unfairly impact specific
groups), privacy considerations, and security considerations.
•The conference expects that many papers will be foundational research and not tied
to particular applications, let alone deployments. However, if there is a direct path to
any negative applications, the authors should point it out. For example, it is legitimate
to point out that an improvement in the quality of generative models could be used to
generate deepfakes for disinformation. On the other hand, it is not needed to point out
that a generic algorithm for optimizing neural networks could enable people to train
models that generate Deepfakes faster.
•The authors should consider possible harms that could arise when the technology is
being used as intended and functioning correctly, harms that could arise when the
technology is being used as intended but gives incorrect results, and harms following
from (intentional or unintentional) misuse of the technology.
•If there are negative societal impacts, the authors could also discuss possible mitigation
strategies (e.g., gated release of models, providing defenses in addition to attacks,
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
feedback over time, improving the efficiency and accessibility of ML).
11.Safeguards
Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?
Answer: [NA]
Justification: To the best of the authors’ knowledge, there is no such risk involved.
Guidelines:
• The answer NA means that the paper poses no such risks.
•Released models that have a high risk for misuse or dual-use should be released with
necessary safeguards to allow for controlled use of the model, for example by requiring
that users adhere to usage guidelines or restrictions to access the model or implementing
safety filters.
•Datasets that have been scraped from the Internet could pose safety risks. The authors
should describe how they avoided releasing unsafe images.
•We recognize that providing effective safeguards is challenging, and many papers do
not require this, but we encourage authors to take this into account and make a best
faith effort.
12.Licenses for existing assets
Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?
Answer: [Yes]
Justification: Assets used to conduct the research presented in this paper are always cited
and properly credited.
Guidelines:
• The answer NA means that the paper does not use existing assets.
• The authors should cite the original paper that produced the code package or dataset.
36•The authors should state which version of the asset is used and, if possible, include a
URL.
• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
•For scraped data from a particular source (e.g., website), the copyright and terms of
service of that source should be provided.
•If assets are released, the license, copyright information, and terms of use in the
package should be provided. For popular datasets, paperswithcode.com/datasets
has curated licenses for some datasets. Their licensing guide can help determine the
license of a dataset.
•For existing datasets that are re-packaged, both the original license and the license of
the derived asset (if it has changed) should be provided.
•If this information is not available online, the authors are encouraged to reach out to
the asset’s creators.
13.New Assets
Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?
Answer: [NA]
Justification: The paper does not release new assets.
Guidelines:
• The answer NA means that the paper does not release new assets.
•Researchers should communicate the details of the dataset/code/model as part of their
submissions via structured templates. This includes details about training, license,
limitations, etc.
•The paper should discuss whether and how consent was obtained from people whose
asset is used.
•At submission time, remember to anonymize your assets (if applicable). You can either
create an anonymized URL or include an anonymized zip file.
14.Crowdsourcing and Research with Human Subjects
Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as
well as details about compensation (if any)?
Answer: [NA]
Justification: The paper does not involve crowdsourcing nor research with human subjects.
Guidelines:
•The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
•Including this information in the supplemental material is fine, but if the main contribu-
tion of the paper involves human subjects, then as much detail as possible should be
included in the main paper.
•According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
or other labor should be paid at least the minimum wage in the country of the data
collector.
15.Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects
Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?
Answer: [NA]
Justification: The paper does not involve crowdsourcing nor research with human subjects.
Guidelines:
37•The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
•Depending on the country in which research is conducted, IRB approval (or equivalent)
may be required for any human subjects research. If you obtained IRB approval, you
should clearly state this in the paper.
•We recognize that the procedures for this may vary significantly between institutions
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
guidelines for their institution.
•For initial submissions, do not include any information that would break anonymity (if
applicable), such as the institution conducting the review.
38