Published in Transactions on Machine Learning Research (05/2024)
Task-Relevant Feature Selection with Prediction Focused
Mixture Models
Abhishek Sharma abhisheksharma@g.harvard.edu
School of Engineering and Applied Sciences,
Harvard University
Catherine Zeng catherinezeng@college.harvard.edu
School of Engineering and Applied Sciences,
Harvard University
Sanjana Narayanan sanjana.books@gmail.com
School of Engineering and Applied Sciences,
Harvard University
Sonali Parbhoo s.parbhoo@imperial.ac.uk
Imperial College London
Roy Perlis rperlis@mgh.harvard.edu
Massachusetts General Hospital
Finale Doshi-Velez ﬁnale@seas.harvard.edu
School of Engineering and Applied Sciences,
Harvard University
Reviewed on OpenReview: https: // openreview. net/ forum? id= voHKJOdCNw
Abstract
Probabilistic models, such as mixture models, can encode latent structures that both explain
the data and aid speciﬁc downstream tasks. We focus on a constrained setting where we
want to learn a model with relatively few components (e.g. for interpretability). Simultane-
ously, we ensure that the components are useful for downstream predictions by introducing
prediction-focused modeling for mixtures, which automatically selects data features rele-
vant to a prediction task. Our approach identiﬁes task-relevant input features, outperforms
models that are not prediction-focused, and is easy to optimize; most importantly, we also
characterize whenprediction-focused modeling can be expected to work.
1 Introduction
The subﬁeld of task-focused generative modeling uses tasks to identify relevant structure embedded in
complex data. For example, suppose we want to discover HIV subtypes (clusters) from electronic health
records. Health records are complex and contain structures irrelevant to our goal (e.g. eﬀects of insurance
type). A model small enough for us to inspect for medical insights (that is, one where we underspecify the
number of clusters), risks ﬁnding irrelevant patterns, while a model that is too large may be too complex to
inspect or learn eﬃciently.
Task-focused approaches to this problem introduce a downstream prediction task to separate relevant struc-
ture from irrelevant. For example, if the clusters can predict whether a patient will develop a speciﬁc
mutation, then it is likely those clusters are relevant to identifying HIV subtypes. This motivation results
in our desiderata: our goal is to train a mixture model that is (a) small enough for human inspection, and
1Published in Transactions on Machine Learning Research (05/2024)
(b) predictive for some task. Because inspection is our ultimate goal, simply creating a predictive model is
not suﬃcient. Additionally, focusing only on relevant structures also facilitates sample-eﬃcient learning.
Unfortunately, task-focused generative modeling is not easy: identifying clusters relevant to our task is
the challenge. As we detail in Section 6, the naive approach of augmenting a mixture model with labels
(supervised generative modeling) fails because the signal from the input features overwhelms the signal from
the labels. More principled approaches involve challenging optimization (Hughes et al., 2018b). Works like
Ren et al. (2020) achieve more tractable optimization by identifying a graphical model whose maximum
likelihood can be used as the solution. However, they do not address important theoretical questions, like if
andwhentheir objective makes the desired trade-oﬀ between predictive and generative performance.
In this work, we introduce and analyze prediction-focused Gaussian mixture models (pf-GMM) and hidden
Markov models (pf-HMM). All of our models identify relevant structure by automatically curating input
features as relevant or irrelevant. This curation enables us to ﬁnd predictive clusters even when the model
is misspeciﬁed. Unlike the prediction-constrained approach by Hughes et al. (2018b), we have formulated
our problem as a graphical model with a single hyperparameter (the probability that an input feature is
relevant). Unlike Ren et al. (2020), we provide a precise theoretical analysis identifying data regimes in
which this approach will work.
Speciﬁcally, we make the following contributions:
1. We develop a probabilistic graphical modeling approach (pf-GMM, pf-HMM) that automatically
curates and clusters features relevant to a downstream task. We outperform standard model-based
clustering approaches like Gaussian Mixture Models (with and without supervision) at predicting
the label and can use existing inference algorithms e.g. Expectation Maximization (EM).
2. We analytically characterize representative scenarios where our auto-curation model achieves the
desired relevant structure, and support this analysis with empirical evidence using simulations and
real datasets. Unlike Ren et al. (2020), we characterize the true likelihood objective, and not its
lower bound—making our analysis applicable irrespective of approximation in inference. In doing
so, we show in which data regimes this relatively simple and robust approach will work.
3. We demonstrate that pf-GMM achieves predictive clusters in even in misspeciﬁed-cluster settings on
several synthetic and real-world datasets. We show that our approach not only yields computational
beneﬁts but is also easier to optimize than the prediction-constrained approach (Hughes et al.,
2018b). pf-GMMs thus provide the best of both worlds—they achieve a good predictive-generative
trade-oﬀ and are easy to tune and optimize.
Outline
Therestofthepaperisorganizedasfollows. InSection2, wereviewtheliteratureonsemi-supervisedlearning
and task-focused generative modeling. In Section 3, we describe the data setting and the assumptions we
make about the data. In Sections 4 and 5, we introduce our prediction-focused models and describe how
to perform inference on them. Then in Section 6, we analyze the properties of prediction-focused mixture
models and characterize the data regimes in which they will work. In Sections 7 and 8, we present our
experiments and results. Finally, in Section 9, we discuss the implications of our work.
2 Related Work
Semi-supervised Learning Semi-supervised Learning augments the generative modeling of inputs X
with a supervisory target Y. Methods that incorporate supervisory labels under the paradigm of “supervised
clustering” fail to provide a density of the observed data (Eick et al., 2004) and are not aligned with our
goals. For methods that train the joint likelihood logf(X,Y)(Nigam et al., 1998; Kingma et al., 2014;
Ghahramani & Jordan, 1994; Fraley & Raftery, 2002), the generative process is typically modeled with
latent variables ZasX←Z→Y. However, these models fail to recognize the inherent asymmetry between
the dimensionalities of XandYand tend to ignore Yin favor of allocating the model’s capacity for the
2Published in Transactions on Machine Learning Research (05/2024)
much more structured X. We address this issue by incorporating speciﬁc input-focused latent variables
(called ‘switches’) that treat the inputs and targets diﬀerently.
Task-focused Generative Modeling Most work on learning generative models for speciﬁc downstream
tasks (Lacoste–Julien et al., 2011; Cobb et al., 2018; Stoyanov et al., 2011; Futoma et al., 2020) train for the
discriminative loss only. Closer to our goal of managing the generative-discriminative trade-oﬀ, Hughes et al.
(2017) proposed prediction-constrained learning in the context of mixture and topic models, formalizing the
problem as maximizing the likelihood of Xconstrained by the prediction objective. However, this objective
is hard to optimize in practice, and unlike our work, does not correspond to the maximum likelihood of a
valid graphical model.
Downstream use cases of Generative Models Semi-supervised generative models that use a discrim-
inative task to learn more useful generative representations have found several applications in the literature.
The review by (Chen et al., 2021) highlights the importance of generative models for understanding the dif-
ferences in survival time of patients with melanoma with diﬀerent demographics for aiding in better planning
of disease management. Halpern et al. (2016) describe the importance of generative models for identifying
groups of clinical conditions that can subsequently used to predict disease subtypes and clinical tags. Works
such as (Prabhakaran & Vogt, 2019) and (Prabhakaran et al., 2012) also show how generative models can be
used to learn clusters of similar chemical compounds or proteins in HIV treatment which can subsequently
be used to predict response patterns for patients who receive those treatments. Furthermore, mixture models
have been used in several classes of downstream tasks. For example, Sharma et al. (2023) use the density
p(X)from a mixture model to plan an elevator scheduling decision-making task, Silva & Deutsch (2018)
use the gaussian mixture model to impute missing values in a dataset of lateritic Nickel deposits, and Attar
et al. (2014) use gaussian mixture models for anomaly detection. Besides these, the interpretation of clusters
for exploration and science-oriented tasks is a common use case of the mixture models (Kuyuk et al., 2012;
Zhuang et al., 1996). Our work supports these downstream eﬀorts by providing a way to build a mixture
model by focusing on the relevant features.
Switchvariables Inthecontextoftopicmodelsfordocuments, Chemuduguntaetal.(2007)andRenetal.
(2020) use switch variables to include or exclude words based on their predictive relevance. Both approaches
rely on independence assumptions between switches and topic-word distributions in the approximate pos-
terior. Furthermore, the authors only study a lower bound to the likelihood—making it unclear whether
their results are due to the choice of model or inference. In contrast, we develop a generative-discriminative
model for a more general context. More importantly, we show that the prediction-focused properties exist by
our model structure alone (independent of the choice of inference), and (unlike previous work) we describe
analytically when our approach will work. Finally, some works impose structural constraints on the target
variables and covariates of Conditional Random Fields (CRFs) (Wainwright & Jordan, 2008), such as (Jiao
et al., 2006; Chen et al., 2015; Zheng et al., 2015). Unlike these works, we focus on the setting where a model
does not have enough parameters to fully model the input dimensions.
Feature selection/attribution methods Feature selection (FS) methods pick out a small subset of
features from the original feature set, and run subsequent analysis on this small subset of features (Jović
et al., 2015). This feature selection can be unsupervised or supervised. Among unsupervised FS methods,
clustering FS methods remove the irrelevant features without considering the prediction task (Witten &
Tibshirani; Dash & Liu, 2000; Jović et al., 2015). However, their notion of relevance does not consider the
alignment of the clusters to a supervisory signal available in the form of labels. This can lead to a clustering
of features that are irrelevant to the labels because the irrelevant features can be correlated to each other.
In contrast, we utilize the provided labels to cluster the relevant features. This leads to clusters that are
predictive of the labels.
Supervised feature selection and attribution methods directly identify the features that are useful for a
predictiontask(e.g.,RandomForestclassiﬁcation,LASSOclassiﬁcation,correlationcoeﬃcients)byremoving
irrelevant features or by providing importance scores. These scores are also an interpretability tool since they
help the user understand which features were useful for the task. However, these methods do not directly
provide a generative model (i.e. clustering) of the relevant features of the data. In addition, these methods
3Published in Transactions on Machine Learning Research (05/2024)
do not prioritize ﬁnding all features that are relevant to the outcome; as a result, the structures they ﬁnd
may be incomplete and thus confusing for domain experts to interpret. In contrast, our prediction-focused
modeling provides a model-based clustering using the relevant features andthe relative importance scores
of these features in the joint task of clustering and prediction.
3 Data Setting
In this section, we formally describe the data distribution that we assume in the analysis-related sections of
ourpaper. Thisisdonetoemphasizethatthisdistributionismuchmorecomplexthanthemodeldistribution
we introduce in Section 4. That said, we emphasize that our prediction-focused model is speciﬁcally designed
to work well in ‘misspeciﬁed’ settings where the model has fewer components than the true data generating
process, regardless of the speciﬁc data generating process.
3.1 Preliminaries
We denote random variables by capital letters (e.g. X,Y), (column) vectors by bold-face letters (e.g. u),
and random vectors by bold-faced capital letters (e.g. X). We denote an identity matrix of size D×Das
ID. We use [u;v]to denote the concatenation of vectors uandvalong the last dimension. We use 0D,
and1Dto denote constant vectors of length Dwith values 0 and 1 respectively. In our discussions, we use
the terms ‘feature’ and ‘dimension’ interchangeably. The true data-generated densities and parameters are
f∗,Θ∗, whereas the estimated quantities are denoted by f,Θ. We useφ(·; Θ)to denote the multivariate
normal density with parameters Θ ={µΘ,ΣΘ}.
3.2 Assumptions
We assume that we are given the dataset D={(X(n),Y(n))}N
n=1, where the input X(n)∈RDand the target
Y(n)∈{0,1}whenX(n)andY(n)are not time-series. In the time-series setting, the input X(n)∈RT×D
and the target Y(n)∈{0,1}Tare sequences of length T.
We consider the setting where the data is a concatenation of multiple sources with multiple irrelevant dimen-
sions each to one source that contains relevant dimensions—and eachsource is distributed as an independent
Gaussian Mixture Model. The relevant mixture is called so because it also generates Y. Speciﬁcally, we
assume that the input X(n)is a concatenation of Msources:
X(n)= [X(n)
1;...;X(n)
M]∀n∈{1,...,N} (1)
whereX(n)
m∈RDm∀m∈{1,...,M}andM/summationdisplay
m=1Dm=D. (2)
We also assume that there are much fewer relevant dimensions than irrelevant dimensions. This assumption
is the main source of the challenge since we only care about the relevant dimensions X(n)
1and the target
Y(n)when explaining the input X(n)and predicting the target Y(n).
3.3 Data distribution
We describe the data-generating process for a single data point (X,Y)in the non-time-series setting (the
description for the time-series setting appears in Supplement Section 3.4). As stated above in the assump-
tions, the ground truth data density f∗is a product of the relevant source f∗
1and each of the irrelevant
sourcesf∗
m(form≥2). The data density is (see Figure 1 for a graphical model depicting this process):
f∗(X,Y; Θ∗) =f∗
1(X1,Y; Θ∗
1)M/productdisplay
m=2f∗
m(Xm; Θ∗
m) (3)
where Θ∗={Θ∗
1,..., Θ∗
M}are the parameters of the mixture components. Mixture mhasKmcomponents.
4Published in Transactions on Machine Learning Research (05/2024)
Z(n)
1X(n)
d
Y(n)D1
N
Relevant Mixture ( m= 1)Z(n)
2X(n)
d
D2
N...
Irrelevant Mixtures ( m∈[2,M])Z(n)
MX(n)
d
DM
N
Figure 1: Data generating process concatenes Mmixture models. The ﬁrst mixture model also generates
the target variable Y(and hence relevant), while the remaining M−1mixture models are irrelevant.
The ﬁrst density in the above equation f∗
1(X1,Y; Θ∗
1)is special because correlated with Y:
f∗
1(X1,Y; Θ∗
1) =K1/summationdisplay
k=1π∗
1,kf∗
1,k(X1;β∗
1,k)f∗
Y,k(Y;η∗
k) (4)
whereπ∗
1,kare the mixture weights, β∗
1,kare the component’s input data parameters, and η∗
kare the compo-
nent’s target parameters.
Every subsequent density f∗
m(Xm; Θ∗
m)(m≥2) is independent of Yin Equation 3:
f∗
m(Xm; Θ∗
m) =Km/summationdisplay
k=1π∗
m,kf∗
m,k(Xm;β∗
m,k)form≥2 (5)
whereπ∗
m,kandβ∗
m,kare similarly deﬁned as above.
For each mixture density f∗
m(·), the mixture weights sum to one:
1 =/summationdisplay
kπ∗
m,k∀m. (6)
Note that we consider the case of Gaussian emissions, i.e. f∗
m,k(·;β∗
m,k) =φ(·;β∗
m,k)of dimension Dm, though
our approach applies to any exponential-family distribution. We assume f∗
Y,k(·;η∗
k)to be the Bernoulli
probability mass function, but again our approach extends to any exponential-family distribution.
Example: HIVsubtyping. Asanapplicationofthisdata-generatingprocess, considertheHIVsubtyping
example from the introduction (Section 1). The goal of the HIV subtyping problem is to identify data-
driven HIV subtypes in the form of patient clusters sharing similar attributes. The data for this problem
is the patient’s electronic health records, which include several sources of information such as blood count
information, genomics, and hospital visit codes during the patient’s lifetime. Among these, the relevant
features (X1) would include measurements of immune response and of co-infections expected to be correlated
with the mutations ( Y). The irrelevant features correspond to groups of diagnosis and medicine codes that
are unrelated to the patient’s HIV condition ( Xm,m≥2). For instance, a patient’s depression diagnosis,
along with related medical codes, is irrelevant to the HIV subtyping problem. The relevant features and each
group of irrelevant features can be thought of as being distributed as a mixture with Kmcomponents since
it is reasonable to expect that each condition (e.g. depression) has multiple subtypes. While it is challenging
to reconstruct all of the structure in the data as a mixture with K1×/producttext
m≥2Kmcomponents, our primary
focus, the discovery of HIV subtypes, does not necessitate perfect modeling of these irrelevant features.
5Published in Transactions on Machine Learning Research (05/2024)
3.4 Data distribution for the time-series case
In this subsection, we describe the case where a single input-target data pair (X,Y)is a sequence of length
T, i.e.X= [X1;...;XM]∈RT×(D1+···+DM)andY∈{0,1}T. This case is analogous to the non-time-series
setting, exceptthatweassumethatthedataisgeneratedby MindependentHiddenMarkovModels(HMMs)
instead ofMindependent Gaussian Mixture Models (GMMs). The data density is:
f∗(X,Y; Θ∗) =f∗
1(X1,Y; Θ∗
1)M/productdisplay
m=2f∗
m(Xm; Θ∗
m) (7)
f∗
1(X1; Θ∗
1) =K1/summationdisplay
k1=1···K1/summationdisplay
kT=1π∗
1,k1T/productdisplay
t=2A∗
1,kt−1,ktT/productdisplay
t=1f∗
1,kt(Xt,1;β∗
1,kt)f∗
Y,kt(Yt;η∗
k) (8)
f∗
m(Xm; Θ∗
m) =Km/summationdisplay
k1=1···Km/summationdisplay
kT=1π∗
m,k 1T/productdisplay
t=2A∗
m,kt−1,ktT/productdisplay
t=1f∗
m,kt(Xt,m;β∗
m,kt) (9)
(10)
where Θ∗={Θ∗
1,..., Θ∗
M}are the parameters of the HMMs and HMM mhasKmlatent states. π∗
m,kare
the initial state probabilities, A∗
m,j,kare the transition probabilities:
π∗
m,k=f∗
m,k 1(Z1,m=k) (11)
A∗
m,j,k =f∗
m,kt(Zt,m=k|Zt−1,m=j)∀t∈[2,T] (12)
whereZt,mis the latent state of HMM mat timet. These probabilities sum to one:/summationtext
kπ∗
m,k= 1,/summationtext
kA∗
m,j,k = 1for allm,j. Finally, the emission distributions are parameterized the same way as above:
β∗
m,kare the parameters of the input-emission distribution f∗
m,k(·)of HMMm’s latent state k, andη∗
kare
the parameters of target-emission f∗
Y,k(·)of HMM 1’s latent state k.
3.5 Consequences of our assumptions
The data may have exponentially many components but we only care about a few. In the GMM
case,f∗
m(·)is the probability density function of a Km−component mixture distribution, and f∗(X,Y)is the
density of a mixture distribution with exponentially many components (/producttext
mKmin total). The HMM case
is the same except that the mixture components are replaced by HMM components. However, modeling all
the components needed to reconstruct all of the structure in the true data-generating distribution may be
unnecessary if we are interested in only the structure relevant for prediction.
The data-generating process allows for several forms of correlation. The key consequence of our
assumption about the data setting is the form of correlations we allow in relevant and irrelevant features:
(a) correlation induced by being distributed as a GMM or an HMM, (b) correlation within each mixture
component, and (c) independence of relevant and irrelevant mixtures. Together, these correlations allow the
relevant features to have complex structure, while keeping the irrelevant features separable from the relevant
ones.
3.6 Problem Statement
We want a mixture model (GMM or HMM) that uses a user-speciﬁed budget of K ( /lessmuch/producttext
mKm) components
to: (a) predict targets Yat test time, and (b) maximize the data likelihood f(X)for a given prediction
quality of Y. To address these desiderata, we introduce prediction-focused mixture models in the next
section. Then in Section 6, we show why a GMM (with or without supervision) does not achieve good
predictive performance in this setting, and how our prediction-focused model is preferable.
6Published in Transactions on Machine Learning Research (05/2024)
Z(n) θX(n) B,π
Y(n)ηSd
pD
N
(a) pf-GMMZ(n)
1...Z(n)
t... θ
AX(n)
tB,π
η Y(n)
tSd
pD
N
(b) pf-HMM
Figure2: Graphicalmodelsforourmodelspf-GMM (a)andpf-HMM (b). Here, Sddenotetheper-dimension
switch variables, Xdenotes the data, Ydenotes the target variables. For pf-GMM, Zdenotes the cluster
assignments, and for pf-HMM, Ztdenotes the hidden states at time t.
4 Our Models: Prediction-focused Mixtures
In this section, we introduce our prediction-focused mixture models which satisfy the desiderata of maximiz-
ing the data likelihood while still performing well on the prediction task. This model is speciﬁcally designed
to work in the underspeciﬁed setting, i.e., when the model is provided much fewer components than the data
requires.
Our key assumption is that the input data dimensions can be partitioned as being either ‘relevant’ or
‘irrelevant’. Only ‘relevant’ dimensions are predictive of the target, Y. This assumption matches the data-
generating process: we preserve the connection between the relevant dimensions and the target and ap-
proximate the irrelevant dimensions with a background distribution. We start by introducing prediction-
focused Gaussian Mixture Models (pf-GMM) and then extend them to the time series setting by introducing
prediction-focused Hidden Markov Models (pf-HMM) (see Figures 2a and 2b for their graphical models).
4.1 Prediction-focused Gaussian Mixture Model (pf-GMM)
Consider the dataset D={(X(n),Y(n))}N
n=1, whereNis the total number of data points, and each input
X(n)∈RDhasDdimensions. The data-generating process under our model pf-GMM assumes the following
way of explaining the dataset D.
First, wedrawa‘switch’variable Sdperdimensiontoindicatewhetherthedimensionisrelevantorirrelevant.
IfSd= 1, then the dimension is relevant, and if Sd= 0, then the dimension is irrelevant. We also draw a
cluster identity Z(n)per data point to indicate which mixture component generated the data point:
Sd∼Bern (p)∀d∈{1,...,D}, Z(n)∼Cat(θ)∀n∈{1,...,N},
wherepis the prior probability that a dimension is relevant, and θis the prior probability of each mixture
component.
Next, we draw the input data dimensions X(n)
Sfrom a mixture distribution with Kcomponents, where Sis
a binary vector which is 1 for relevant dimensions (i.e. S= [S1;...;SD]) andX(n)
Sis the subvector of X(n)
with only the relevant dimensions. We draw the remaining dimensions X(n)
1−Sfrom a background distribution
(1−Sis the binary vector which is 1 for irrelevant dimensions):
Y(n)|Z(n)=j∼Bern (ηj),X(n)
S|Z(n)=j∼N(µB
j,ΣB
j),
X(n)
1−S∼N(µπ,Σπ)∀n∈{1,...,N}. (13)
Note that we chose speciﬁc distributions for XandYfor the sake of clarity but we could have chosen any
exponential family distribution.
7Published in Transactions on Machine Learning Research (05/2024)
The joint distribution for pf-GMM, fpfGMM (X,Y,Z,S)is:
fpfGMM (X,Y,Z,S) =/parenleftBiggD/productdisplay
d=1f(Sd)/parenrightBiggN/productdisplay
n=1f(Z(n))f(X(n)
1−s)f(X(n)
s|Z(n))f(Y(n)|Z(n)) (14)
Switch variables select relevant dimensions by tuning its prior pThe switch variables Sdare the
key to our model. They allow us to model the ‘irrelevant’ dimensions X1−Susing a background distribution,
and to jointly model the ‘relevant’ dimensions and target ( XS,Y) using a mixture distribution. Furthermore,
our models depend on only one hyperparameter—the switch prior, p. As we will see in Section 6, the switch
priorpcan betunedto trade oﬀ generative quality with predictive performance. This trading-oﬀ ability
allows our model to achieve good downstream performance even when they are (inevitably) misspeciﬁed with
respect to the data-generating process.
4.2 Prediction-focused Hidden Markov Model (pf-HMM)
The generative process for our pf-HMM model is as follows. In addition to the switch variables Sd, we draw
the state sequence variable Z(n)
tfor each data point nand time step t:
Sd∼Bern (p)∀d∈{1,...,D}, Z(n)
1∼Cat(θ)∀n∈{1,...,N},
Z(n)
t|Z(n)
t−1=j∼Cat(Aj)∀n∈{1,...,N},t≥2,
wherepis the prior probability that a dimension is relevant, θis the probability distribution of the ﬁrst
state, andAjis the probability distribution of the next state given the current state j.
For the set of relevant dimensions, we draw the data X(n)
S,tfrom a hidden Markov model with KGaussian
components. For the set of irrelevant dimensions, we draw the data X(n)
(1−S),tfrom a background Gaussian
distribution.
Y(n)
t|Z(n)
t=j∼Bern (ηj),X(n)
S,t|Z(n)
t=j∼N(µB
j,ΣB
j),
X(n)
(1−S),t∼N(µπ,Σπ),∀n,t. (15)
The joint distribution for pf-HMM, fpfHMM (X,Y,Z,S)is:
fpfHMM (X,Y,Z,S) =/parenleftBiggD/productdisplay
d=1f(Sd)/parenrightBiggN/productdisplay
n=1/parenleftBigg
f(Z(n)
1)T/productdisplay
t=2f(Z(n)
t|Z(n)
t−1)/parenrightBigg/parenleftBiggT/productdisplay
t=1f(X(n)
(1−S),t)f(X(n)
S,t|Z(n)
t)f(Y(n)
t|Z(n)
t)/parenrightBigg
(16)
The density fpfHMM (X,Y,Z,S)is similar in form to the pf-GMM case (Equation 14). Both models use a
complex structure to explain the relevant dimensions and the target while using a background distribution
to explain the irrelevant dimensions. Furthermore, both models use a switch variable Sdto select relevant
dimensions. The main diﬀerence for pf-HMM is that a Markov chain of variables {Z(n)
t}T
t=1has replaced the
single variable Z(n)to generalize the mixture model to the time-series setting.
5 Inference
Since pf-GMM and pf-HMM are valid graphical models, we can use eﬃcient inference algorithms for these
models instead of resorting to generic optimization methods. The likelihood of pf-GMM is:
L(Θ) =/summationdisplay
Z,SfpfGMM (X,Y,Z,S; Θ) (17)
This objective is intractable to compute for high dimensions because it scales exponentially with the di-
mensionality of X. Therefore, in practice, we use Variational Inference (VI) to optimize a lower bound
8Published in Transactions on Machine Learning Research (05/2024)
(the ELBO) to the intractable log-likelihood. We choose the following form for the approximate posterior,
q(Z,S|Θ,ϕ):
q(Z,S|Θ,ϕ) =N/productdisplay
n=1q(Z(n)|X(n),Y(n),ϕ,Θ)D/productdisplay
d=1q(S(n)
d|ϕd) (18)
where Θrefers to the model parameters. Note that we have added an index nto the switch variable, i.e.
S(n)
dis a function of the datum nas well as the dimension d. This allows the posterior to factorize over the
data. To avoid learning a separate switch posterior for each n, wetietheir parameters across nto obtain
q(S(n)
d|ϕd). Therefore we still get a per-dimension relevance.
The distribution q(Z(n)|X(n),Y(n),ϕ,Θ)is simply the posterior distribution conditioned on the variational
parameters ϕand can be computed as:
q(Z=k|X,Y,ϕ )∝f(Z=k)·fk(X|ϕ)·fY,k(Y)
At test time, the distribution q(Z=k|X,Y,ϕ )is computed without the Pk(Y)term but the feature selection
parameterϕhelps select the right features in the term fk(X|ϕ).
This results in the following ELBO objective (full derivation in Supplement Section 1):
ELBO (q) =E
q(Z,S)[logf(X,Y|Z,S; Θ)]−KL(q(Z,S|Θ,ϕ)||f(Z,S; Θ)) (19)
The model is trained using the variational EM algorithm which alternates between learning the parameters
Θ(M-step) and inferring the approximate posterior q(Z,S|Θ,ϕ)(E-step). We also try Gibbs sampling for
the E-step since the variables ZandSare conditionally conjugate. We provide the conditional distributions
for Gibbs sampling in the Supplement Section B.3.
Similarly, we train pf-HMM by variational EM using the following factorization for q(Z,S|Θ,ϕ):
q(Z,S|Θ,ϕ) =N/productdisplay
n=1/parenleftBiggD/productdisplay
d=1Tn/productdisplay
t=1q(S(n)
t,d|ϕd)/parenrightBigg
f(Z(n)|X(n),Y(n),ϕ,Θ)
where subscript tindexes time within a sequence, and Z(n),X(n),Y(n)are variables for each sequence n. We
use the Forward-Backward algorithm (Bishop, 2006) to infer f(Z(n)|X(n),Y(n),ϕ,Θ)during the E-step (see
Supplement Section B for details).
6 Analysis of Correctness of Prediction-focused Mixtures
In this section, we show how the switch variable in the pf-GMM is able to trade oﬀ generative and dis-
criminative performance when faced with misspeciﬁcation in the number of components. We explore the
limitations of GMMs in such a setting, even with supervision (i.e. for sup-GMM). Importantly, we propose
analytical expressions to determine how pf-GMM identiﬁes relevant dimensions in various data scenarios,
and conclude the section with an empirical evaluation of the expressions.
6.1 What is enabling the generative-discriminative trade-oﬀ?
The generative-discriminative trade-oﬀ can either be achieved by the choice of model and objective, or by
the choice of inference algorithm. Ren et al. (2020) motivated using switch variables to distinguish relevant
and irrelevant dimensions by providing a lower bound to the marginal log-likelihood objective which exhibits
an explicit generative-discriminative trade-oﬀ:
logf(X,Y)≥pE
f(Z)[logf(X|Z)] + (1−p) logf(X;π) +E
f(S)[logf(Y|X,S)] (20)
wherepis the switch prior. This suggests that the modeling choice of using switch variables is what
enables the generative-discriminative trade-oﬀ. However, this bound is not optimized during training (it is
9Published in Transactions on Machine Learning Research (05/2024)
computationally expensive to compute), and is mainly used to justify the use of switch variables. In fact,
their trade-oﬀ is achieved by the speciﬁc inference algorithm they use (variational inference with a speciﬁc
posterior approximation). Furthermore, favorable properties of a lower bound do not guarantee favorable
properties of the model and the objective.
In this section, we demonstrate analytically that the truepf-GMM likelihood can balance generation and
prediction in certain data regimes through the switch prior p. This result implies that the pf-GMM model
itself is able to trade oﬀ generation and prediction, providing a theoretical justiﬁcation for the use of switch
variables. Furthermore, we precisely characterize the circumstances in which the user can adjust pto achieve
the desired trade-oﬀ. We note that Hughes et al. (2017) also achieve such a trade-oﬀ through their proposed
objective, but their objective is hard to optimize. We demonstrate that our objective is easier to optimize
than theirs in the experiments section.
Figure 3: (A) We create representative datasets by varying the number ( DR,DIr) and the signal-to-noise
ratio (SNR; ∆R,∆Ir) for relevant and irrelevant dimensions. We ﬁx the total dimensions ( D=DR+DIr)
and the SNR for relevant dimensions; the resulting datasets are evaluated by each model to get the ﬁgures
in panel C. (B) There are three local optima to the Objective 25 in a two-dimensional feature space: aligned
components with label Y(left), unaligned components (middle), and just a single component (right). (C)
The yellow region in each plot corresponds to the datasets for which the model learns aligned components,
and the blue region corresponds to the datasets for which the model learns unaligned components. From
left to right, there is an increase in the space of datasets for which aligned components can be selected (i.e.
GMM< sup-GMM< pf-GMM). (D) For any dataset, we see how pf-GMM can achieve aligned components
by tuningp. Settingpto a value in (p1-G/a,pu/a)(yellow region) learns aligned components, whereas setting
pto a value in (0,p1-G/a)∪(pu/a,1)(blue region) learns unaligned components or a single component.
6.2 Deﬁning Pertinent Data Regimes
Consider data formed by concatenating multiple independent mixture distributions, one of which is relevant;
the rest are irrelevant. The possible ways in which each mixture distribution (relevant or irrelevant) can vary
are (a) separation between the components, (b) variance of components, and (c) relative counts of relevant
and irrelevant dimensions in X. Finally, the number of independent irrelevant mixtures can also vary. We
ﬁrst consider the case where the relevant dimensions come from one mixture, and the irrelevant dimensions
come from another mixture (see Figure 3(A)). This case generalizes to the analysis for multiple irrelevant
mixtures since it highlights the trade-oﬀs the model must make between relevant and irrelevant mixtures.
10Published in Transactions on Machine Learning Research (05/2024)
Consider input data X:= [XR;XIr]∈RDR+DIrwithDRrelevant and DIrirrelevant dimensions, and a
binary output label Y. Both the relevant dimensions XRand irrelevant dimensions XIrare distributed as
equal mixtures of two Gaussians. The data-generating process is:
ZR∼Bern (0.5), Z Ir∼Bern (0.5), (21)
XR∼ZR·N(0DR,σ∗2
RIDR) + (1−ZR)·N(µ∗
R·1DR,σ∗2
RIDR), (22)
XIr∼ZIr·N(0DIr,σ∗2
IrIDIr) + (1−ZIr)·N(µ∗
Ir·1DIr,σ∗2
IrIDIr), (23)
Y=ZR (24)
whereZRare the cluster identities for the relevant mixture and ZIrare the cluster identities for the irrelevant
mixture. Note that the output label Yis perfectly correlated with the relevant mixture’s cluster identity
ZR. Finally, let ∆ :=µ∗/σ∗as the signal-to-noise ratio (SNR) for both relevant and irrelevant mixtures
(i.e. ∆R,∆Ir). Note: Our analysis carries over to full-covariance Gaussians; the expressions are simply more
involved. We discuss the isotropic case here to convey the core ideas and provide the analogous derivations
for the full-covariance case in Supplement Section C.
6.3 Formalizing the Question
What are the datasets (i.e. values of ∆R,∆Ir,DR,DIr) for which a misspeciﬁed model can still select the
relevant components?
We answer this question by characterizing the population log-likelihood objective under the model density
fmodel (X,Y; Θ)and data density f∗(X,Y):
l(Θmodel ) =E
f∗[logfmodel (X,Y; Θ)] (25)
for three models: GMM, sup-GMM, and our pf-GMM. The GMM learns a Gaussian mixture model on X
alone, whereas the sup-GMM is the supervised variant that also uses Y. Upon optimization, the likelihood
objective 25 can result in three locally optimal parameters. As illustrated in Figure 3(B), these parameters
correspond to:
1. components that model relevant data ( XR,Y) well ( Θaoralignedparameters).
2. components that model irrelevant data ( XIr) well ( Θuorunaligned parameters).
3. (for pf-GMM only:) a single component ( Θ1-G); this case arises if the pf-GMM considers all dimen-
sions as irrelevant.
Among these, the aligned components have the highest predictive accuracy because they are correlated with
Y.
6.4 Results for the Analysis
6.4.1 Likelihood Gaps for GMM, sup-GMM, and pf-GMM
The likelihood gaps for GMM and sup-GMM depend on the data features ( ∆R,∆Ir,DR,DIr):
∆(Θa
GMM,Θu
GMM ) =QR−QIr (26)
∆(Θa
supGMM,Θu
supGMM ) = log 2 + QR−QIr (27)
whereQRis a shorthand for the following term that only depends on DRand∆R(and similarly for QIr):
QR=1
2/bracketleftbiggDR(1−∆2
R)
DR∆2
R+ 1+ log(DR∆2
R+ 1)/bracketrightbigg
QIr=1
2/bracketleftbiggDIr(1−∆2
Ir)
DIr∆2
Ir+ 1+ log(DIr∆2
Ir+ 1)/bracketrightbigg
11Published in Transactions on Machine Learning Research (05/2024)
For pf-GMM, the gaps also depend on the switch prior p:
∆(Θa
pfGMM,Θu
pfGMM ) = (DR−DIr) logp
1−p+ log 2 +QR−QIr (28)
∆(Θa
pfGMM,Θ1-G
pfGMM ) =DRlogp
1−p+ log 2 +QR (29)
When can a misspeciﬁed model select relevant components? The models will select aligned compo-
nents (over unaligned) when the likelihood gap ∆(Θa
model,Θu
model )is positive. For our pf-GMM, we also need
the gap ∆(Θa
pfGMM,Θ1-G
pfGMM )to be positive, that is the likelihood associated with the aligned components
must be higher than that of collapsing the model to a single Gaussian.
6.4.2 Tuning pf-GMM to prefer relevant components
For pf-GMM, we can tune the switch prior pto prefer relevant components in Equations 28 and 29. We can
perform this selection of relevant components by identifying values of pfor which ∆(Θa
pfGMM,Θu
pfGMM )>0
and∆(Θa
pfGMM,Θ1-G
pfGMM )>0. Doing so gives us the following constraints on p:
p<pu/a where pu/a=σ/parenleftbigglog 2 +QR−QIr
|DR−DIr|/parenrightbigg
ifDR<D Ir(30)
p>pu/a where pu/a=σ/parenleftbigg
−log 2 +QR−QIr
|DR−DIr|/parenrightbigg
ifDR>D Ir(31)
p>p 1-G/a where p1-G/a=σ/parenleftbigg
−log 2 +QR
DR/parenrightbigg
(32)
whereσ(x) =1
1+exp(−x)is the sigmoid function. The feasible range of pexists when pu/a>p1-G/a, which is
when:
DIrlog 2−DRQIr+DIrQR>0ifDR<D Ir
6.5 Empirical Illustrations of Analytical Results
We empirically illustrate when these gaps can be positive by setting DR+DIr= 8and∆R= 5as an example
in Figure 3(C). For each model, we vary ∆IrandDIrto create datasets with diﬀerent SNR and a number of
irrelevant dimensions. The yellow region in each plot corresponds to the datasets (i.e. values of ( ∆Ir,DIr))
for which the model learns aligned components as per Equations 26-29. The blue region corresponds to the
datasets for which the model fails to learn aligned components. We see that the range of datasets for the
aligned parameters (i.e. the yellow area) is largest for pf-GMM, followed by sup-GMM, and smallest for a
GMM.
Furthermore, in Figure 3(D), we illustrate how our pf-GMM model selects the aligned components by tuning
p. By setting pto a value in (p1-G/a,pu/a), we learn aligned components (corresponding to the yellow region
in the ﬁgure), whereas by setting pto a value in (0,p1-G/a)∪(pu/a,1), we either learn unaligned components
or just a single component (corresponding to the blue region in the ﬁgure). This ﬁgure provides us with
guidance on performing hyperparameter tuning for p: to learn aligned components, we decrease pif we
currently learned unaligned components, and increase pif we learned a single component.
6.6 Conclusions from the Analysis
The pf-GMM allows us to identify relevant structure by changing p.To choose the aligned
parameters, thegapsinEquations26-29mustbepositive. ComparingEquation26with27showssupervision
is useful over just using the GMM—the gap for sup-GMM is log 2larger than the gap for GMM. However,
the pf-GMM does better: its gaps (Equations 28 and 29) also include a term involving p, allowing us to tune
pso that the gaps are positive.
12Published in Transactions on Machine Learning Research (05/2024)
When the relevant dimensions are outnumbered by irrelevant dimensions, learning the unaligned compo-
nents would be preferred by GMM (and sup-GMM after an extent) since doing so increases the generative
performance of the model. However, maximizing predictive performance would require learning the aligned
components. Setting pto a value in (p1-G/a,pu/a)allows us to select aligned components, and setting it
in(pu/a,1)allows us to select unaligned components (Equations 30 and 32). In this manner, we can
trade oﬀ between predictive and generative performance : the aligned parameters achieve predictive
performance while the unaligned parameters achieve generative performance—and plets us decide which
ones to choose.
pf-GMM identiﬁes predictive mixtures even with many irrelevant dimensions. Suppose we
ﬁx meansµ∗
R,µ∗
Irand variances σ∗2
R,σ∗2
Ir(i.e., ﬁx ∆Rand∆Ir). For the interesting case of fewer relevant
dimensions DR<D Ir, loweringpbelowpu/aforces the aligned parameters to be preferred over the unaligned
parameters. The other models cannot achieve this: the gap ∆(Θa
GMM,Θu
GMM )is negative for the GMM and
only increases by log 2for the sup-GMM (Equations 26, 27). However, there are limits to pf-GMM, too: if
we lowerpbelowp1-G/awhenDR< D Ir, pf-GMM, prefers the single Gaussian solution (as seen in Figure
3(D)).
pf-GMM is more robust to high irrelevant SNR. We vary the irrelevant SNR ∆Irby varying the
gap between irrelevant component ( µ∗
Ir), while ﬁxing the means of the relevant components ( µ∗
R) and all
variances (σ∗2
R,σ∗2
Ir). The case of σ∗
R>> σ∗
Iris analogous. For all models, the likelihood gap decreases on
increasing ∆Ir(see Equations 26-29). However, pf-GMM selects aligned components for a larger range of
∆Ir. For the interesting case of DR< D Ir, we can see this by setting p < pu/ain Equation 30—doing so
adds a positive bias to the gaps of GMM and sup-GMM, making it easier to select aligned components.
Nonetheless, if ∆Iris too high, decreasing pno longer works because pu/a<p1-G/a: we go directly from the
unaligned solution Θuto the single Gaussian solution Θ1-G, bypassing the predictive solution Θa.
This case highlights the properties and limitations of all three models : our pf-GMM has the widest range of
datasets for which it will still select the desired aligned parameters, but there will be a level of signal-to-noise
ratio at which all models will fail.
Cases ofDR>DIrandDR=DIr.DR>D Iris the (easy) regime where GMM (and therefore sup-GMM)
work as desired and introducing pf-GMM is not necessary. Finally, for DR=DIr, the eﬀect of pdisappears
irrespective of the parameters. This case is an unlikely technicality, which we rarely expect in real datasets.
Eﬀect of the number of irrelevant mixtures. When multiple irrelevant mixtures are present, we only
need to focus on the irrelevant mixture that yields model parameters (with model components aligned with
mixture components) with the highest population log-likelihood, l(Θ). Then the same analysis applies.
7 Experimental Details
In this section, we empirically demonstrate that the pf-GMM successfully identiﬁes relevant input signal,
regardless of whether the number of model components is misspeciﬁed or not. We show that the switch
parameters and their prior pcan recover the relevant dimensions and maintain predictive performance even
under misspeciﬁcation. Speciﬁcally, we address the following questions:
Q1:Can prediction-focused models perform well when we misspecify the number of components?
Q2:Can prediction-focused models perform well as the number of irrelevant dimensions increases?
Q3:Do prediction-focused models select relevant dimensions, even when irrelevant dimensions are not
fully independent of the signal?
Q4:Is the optimization stable for these models?
For both pf-GMM and pf-HMM, we constrain the models to have diagonal covariance for faster inference;
analogous results for the full-covariance case are in Supplement Section 3.2.
13Published in Transactions on Machine Learning Research (05/2024)
AC
DEFB
Figure 4: Plots for synthetic data (Section 7). (A) pf-GMM is an order of magnitude faster than pc-
GMM, with respect to the average runtimes (log scale) per run. (B, E) pf-GMM trades oﬀ discriminative
and generative objectives better—it takes a small hit in generative quality to achieve discriminative quality
competitivewithpf-GMM trainedonlyonthepredictiveterm. Thebeneﬁtsarelargestwhenmisspeciﬁcation
is larger (fatter markers). (C) Constrained pf-GMM achieves optimal predictive performance by identifying
the relevant dimensions (random forest baseline provides the upper bound). (D) Posterior probabilities
f(Sd= 1|X,Y)for the switch variables correctly identify the ﬁrst 20 dimensions as relevant.
Table 1: Comparison of discriminative and generative metrics for the synthetic (complex) dataset shows that
pf-GMM, outperforms other baselines.
Model logP(Y|X) logP(X) logP(XR)Y AUROC
pf-GMM -0.05 -105.65 7.38 0.99
pc-GMM -0.50 -130.41 -15.51 0.66
sup-GMM -0.66 -30.81 -15.48 0.58
2-Step -0.70 5.79 -26.23 0.52
Class-speciﬁc GMM -5.92 14.87 -20.66 0.73
7.1 Baselines
We compare the generative and predictive performance of pf-GMM to (1) a two-step approach that learns a
GMMandtrainsalogisticregressionclassiﬁerontheposteriors f(Z|X)(2-Step-GMM),(2)supervisedGMM
trained jointly on X,Y(sup-GMM), (3) prediction-constrained GMM (Hughes et al., 2017) (pc-GMM) and
(4)sparseK-Means(Witten&Tibshirani)followedbylogisticregressiononclustervalues(sparseclustering).
In Supplement Section F, we also discuss a class-speciﬁc GMM baseline that learns a diﬀerent GMM for each
class value and shows that it is qualitatively similar to a GMM baseline. We also provide a random forest
baseline as a prediction upper bound. We discuss the utility of these baselines in depth in the Supplement
Section 4 For pf-HMM, comparable approaches are sup-HMM and 2-Step-HMM.
7.2 Datasets
Synthetic We generate two synthetic datasets, where the ﬁrst dataset is simple and the second dataset is
complex. For the ﬁrst dataset, we generate the relevant and irrelevant dimensions of Xfrom independent
14Published in Transactions on Machine Learning Research (05/2024)
AB
Mean Y v alue per data hist ogram binC
Figure 5: Densities of relevant (top) and irrelevant (bottom) dimensions of inputs Xfor Synthetic (left and
center) and Swiss Bank Notes datasets. The Synthetic (complex) in the middle column is complex because
all the classes have multimodal densities. By this deﬁnition, both Synthetic (simple) and Swiss Bank Notes
are considered simple. We also see that the pf-GMM, can model the relevant dimensions well (red line
corresponds to the pf-GMM, model density) in all the datasets. In contrast, class-speciﬁc GMM (olive line)
prioritizes modeling irrelevant dimensions’ data density (bottom rows) and is only able to capture relevant
dimensions’ data density (top row) when the datasets are simple.
Figure 6: Target AUROC performance as we vary the component budget Kin each of the domains for
GMMs (Top Row) and HMMs (Bottom Row). pf-GMMs and pf-HMMs outperform methods that do not
trade oﬀ prediction and generation for diﬀerent choices of K. Synthetic data for the HMM models has 2
relevant dimensions (out of 20 total).
3−component mixtures of Gaussians. The target Yis binary, with the assignment only depending on the
cluster identities in the relevant GMM. The dataset is simple in the sense that the relevant dimension
densities for each class are suﬃciently unimodal: there is only one mode for the class Y= 1and most of
the mass is concentrated in one mode for class Y= 0(see the histograms in the top left panel of Figure
5 for a visual representation). In the second dataset, we generate the relevant and irrelevant dimensions
from independent 4−component mixtures, and again the target is binary and depends on the relevant
15Published in Transactions on Machine Learning Research (05/2024)
GMM’s cluster identities. By the same deﬁnition of complexity, the second dataset is complex because the
relevant dimension densities are multimodal for each class (see the top center panel of Figure 5). The total
dimensionality of each observation is ﬁxed at 100. To generate time-series data, we use independent 4−state
HMMs with 20 dimensions. The emission distribution f∗
k(X|Z=k)given component kisN(·; 6k·1,I)for
relevant and irrelevant dimensions. The target Yis binary and depends on the relevant HMM (details in
Supplement Section 4). We vary the number of model components ( K) and the ratio of relevant dimensions
for diﬀerent levels of misspeciﬁcation. For Q3, we also simulate irrelevant dimensions that are not completely
independent of Y; for this experiment, we vary the correlation between Yand the cluster identity.
Swiss Bank Notes The Swiss bank notes dataset (available in the mclust library in R (Scrucca et al.,
2016)) contains 6 features about Swiss 1000-franc banknotes: length of bill, widths of left, right, top, bottom
edges, and length of diagonal. The dataset contains 100counterfeit and 100genuine notes. We want the
clusters to be predictive of whether a banknote is genuine or counterfeit (label Y). However, not all features
are relevant to the task of predicting this label. Therefore, prediction-focused modeling is appropriate for
this situation. To further demonstrate the robustness of pf-GMM to noisy features, we augment the existing
features with 30 irrelevant features generated using a 2−component mixture of Gaussians with components
N(−3,I)andN(3,I)to make prediction harder, and ﬁnally normalize the dataset. For a method to do well,
it must identify clusters that are predictive of the genuineness of the notes even when the cluster count is
small (e.g. when only 2 clusters are allowed).
HIVTherapy for HIV involves administering cocktails of antiretroviral treatments to bring the viral load
below detection limits ( ≤40copies/ml). These antiretroviral treatments belong to ﬁve classes: Non-
nucleoside Reverse Transcriptase Inhibitors (nnRTIs), Nucleoside Reverse Transcriptase Inhibitors (nRTIs),
Protease Inhibitors (PIs), Fusion Inhibitors (FIs), and Integrase Inhibitors (IIs). Our task is to predict
whether a treatment will bring the viral load below detection limits in the next time-step, or whether it will
increase the viral load ( Y). Each input observation ( X) contains 267features about the patient, including
their CD4+counts, their genetic mutations, their treatments in terms of drug classes, and their lab results.
We study 53,236patients with HIV from the EuResist Integrated Database. Each person has a time series of
an average length of 16steps where a time step is approximately 4months between consecutive treatments.
Though it is common to have many genetic mutations, only a few of these may be relevant for inducing drug
resistance thus increasing the viral load. Therefore, this problem is perfect for prediction-focused modeling.
We compare both HMM and GMM models, ignoring the time dependencies for the latter case. Further
details about the experiments are provided in Supplement Section D.
PsychiatricMedicalRecords Weanalyzedadatasetofelectronichealthrecordsfromtwolargeacademic
medicalcentersandtheiraﬃliatedcommunityhospitalsandoutpatientclinics. Weselectedacohortof16,653
patients with at least one diagnosis of major depressive disorder ICD-10 diagnosis code during psychiatric
care between 2017and2022. We remapped the ICD-10 codes to 423 CCSR codes (i.e. features). We mapped
each feature with a value of 1if the diagnosis code was observed, and 0otherwise. An independent standard
normal observation noise with a standard deviation of 0.1was added. After excluding codes with high
sparsity (≥90%patients with the code) and codes that were highly correlated with another code, we were
left with 106 features for a patient. Our prediction task is to ascertain a substance-use disorder outcome for
the patient, and the goal of the clustering step is to discover subgroups relevant to the prediction task. This
task would beneﬁt from prediction-focused modeling because, without the prediction task, the clusters would
correspond to common comorbidities of MDD. However, we wish to discover clusters of substance-use-related
comorbidities.
7.3 Evaluation
We measure the predictive performance of the models by logP(Y|X)and area under the receiver operating
characteristic (AUROC) for the classiﬁcation of Yon heldout test data. We measure generative performance
bylogP(X). For synthetic datasets, we know which dimensions are relevant, so we can also measure the
generative performance of the models on only the relevant dimension, logP(XR). Since we do not have this
metric in practice, we use YAUROC for model selection. Using YAUROC works well in practice because we
16Published in Transactions on Machine Learning Research (05/2024)
Method AUROC (µ±σ)Time(s)
pf-GMM 0.75±0.01 293
GMM 0.60±0.00 6
pc-GMM 0.75±0.096828
sup-GMM 0.62±0.01 278
Class-speciﬁc
GMM0.74±0.01 13
Sparse Clustering 0.54±0.021218
RandomForest
(Oracle)0.85±0.00 115
pfGMM - Top 8 features
Other speciﬁed and unspeciﬁed mood disorders
Alcohol-related disorders
Tobacco-related disorders
Other speciﬁed and unspeciﬁed liver disease
Trauma- and stressor-related disorders
Superﬁcial injury; contusion, initial encounter
Depressive disorders
Anxiety and fear-related disorders
External cause codes: intent of injury, acciden-
tal/unintentional
0.00.51.01.52.02.53.03.5Alcohol-related disorders
1
 0 1 20.00.51.01.52.02.53.03.5Other specified and unspecified mood disorders
0.0 0.2 0.4 0.6 0.8 1.0
Data Density: Outcome PrevalenceData Density
pfGMM Density
Class-specific GMM Density
Code Prevalence024Cluster
Prevalence of Codes per Cluster
(Median + IQR)
0.00 0.25 0.50 0.75 1.00
Outcome Prevalence024Cluster
Outcome Prevalence per Cluster
T obacco-related disorders
Other specified and unspecifie...
Alcohol-related disorders
Overall Outcome Prevalence
Per-cluster Outcome Prevalence
Figure 7: Results for psychiatric medical records dataset. Top Left : pf-GMM learns clusters that are
predictive of the substance use disorder outcome, as seen by a high YAUROC value. Other clustering
methods either produce clusters unrelated to the Youtcome (2-Step-GMM, sup-GMM, Sparse Clustering)
or are very slow to run and have higher variation (pc-GMM). Bottom Left : The top features that pf-
GMM clusters on are indeed related to substance use disorder. Center: The class-speciﬁc GMM does not
model relevant dimension (alcohol and mood disorders) density despite having a high YAUROC, whereas
the pf-GMM, correctly captures the density. Right: The pf-GMM clusters show a signiﬁcant variation in
the outcome and the relevant feature values, providing evidence that the clusters are both predictive and
relevant.
.
can achieve good predictive performance with the clusters only if the relevant dimensions are well modeled
(the class-speciﬁc GMM can create false positives with this metric because it has more parameters, as we
discuss later).
Model selection is done using a heldout validation set. For Swiss Bank Notes Data, we use 3-fold Stratiﬁed
Cross-validation because the observations are too few to have a separate validation set.
8 Results and Discussion
Prediction-focused models balance the discriminative and generative objectives eﬀectively.
Figure 4 (A) compares pf-GMM to competing approaches on the discriminative-generative landscape. A
higher number of irrelevant dimensions or a lower number of components are exactly the settings in which
the trade-oﬀ of discriminative and generative objectives is critical and pf-GMM manages to achieve superior
predictive performance in these situations. In contrast, sup-GMM and 2-Step-GMM are only useful when
the number of relevant dimensions (the number of components respectively) is increased—as predicted by
our analysis in Section 6. The class-speciﬁc GMM ﬁts independent GMMs on smaller subsets of the data,
causing it to overﬁt its logf(Y|X)density to the training set. Finally, directly optimizing for logf(Y|X)
(orange) gives up a lot on generative performance.
Prediction-focused models identify parameters that perform signiﬁcantly better at the down-
stream task. Prediction-focused models identify components that align with the relevant dimensions
(Figure 5 top left). This helps them outperform their non-prediction-focused counterparts on predictive
17Published in Transactions on Machine Learning Research (05/2024)
performance on several real and synthetic datasets (Figures 4, 6, and 7). While sup-GMM also maximizes
the joint likelihood of inputs and the target, it treats the target as just another input dimension. In contrast,
prediction-focused learning identiﬁes the asymmetry in the problem, and it emphasizes dimensions that are
predictive of the target. The class-speciﬁc GMM can achieve good predictive performance, suggesting it also
identiﬁes the relevant dimensions. However, we see in Figure 5 that it aligns its components with the irrel-
evant dimensions (bottom row), and can not align with relevant dimensions when the datasets are complex
(top row, center).
Prediction-focusedmodelsseekoutthemostcorrelateddimensionsﬁrst. AsweshowinFigure4(C
and F), our constrained pf-GMM achieves optimal predictive performance (random forest baseline provides
the upper bound). Ignoring irrelevant dimensions can be easy if the irrelevant dimensions are independent
of the relevant dimensions (i.e. correlation to Yis zero). But the pf-GMM seeks out relevant dimensions
even when the irrelevant dimensions ‘distract’ the model by being weakly correlated to Y, as seen in Figure
4(F) The rest of the models do a poor job at this: their performance is proportional to how correlated the
irrelevant dimensions are to Y. A correlation of one is trivial because the notion of an ‘irrelevant’ dimension
disappears.
Prediction-focused models are faster to compute and they are easier to tune and optimize.
In Figures 4(D) and 7(top left), we see that pf-GMM is orders of magnitude faster than the pc-GMM—
the only other baseline that can trade oﬀ generative and discriminative objectives. Moreover, pc-GMM’s
performance is unstable and it requires more independent trials to run, as can be seen from the large error
bars corresponding to [5,95]-percentile intervals (Figure 4(C) and 6(A)). The switch parameter pis the only
hyperparameter that must be tuned. In contrast, while pc-GMM also has one hyperparameter λ—used
for trading oﬀ generative with predictive performance—the model is still considerably harder to optimize.
Training requires a grid search over both learning rates and λ, and it takes much longer to converge than
the EM-based methods. This makes it prohibitively expensive to apply to HMMs.
Prediction-focused model learns an accurate density of the relevant dimensions even for com-
plex data distributions. In Figures 4(B) and 5(top row), we see that pf-GMM, learns a better density
for the relevant dimensions even when the model is constrained (i.e., for small values of K), as measured
bylogP(XR). In contrast, the class-speciﬁc GMM learns a poor density of the data even when it does well
at predicting the target Y. This phenomenon is obscured on simple data that does not have a multimodal
data density in Xfor the values of output class Y. This is because the class-speciﬁc GMM can use its
components to model the irrelevant dimension density but still do well on the prediction task and model
relevant dimension density well if the data set is simple. However, for a complex dataset with multimodal
relevant dimension density per class, this is no longer possible and the class-conditional baseline fails. We
discuss this further in the Supplement Section F.
Prediction-focused models identify relevant dimensions in both misspeciﬁed and high-noise
settings. In Figure 6, we see that prediction-focused models maintain high AUROC—indicating that the
components discovered are predictive of the downstream task. In particular, we see that pf-GMM maintains
good performance even when Kis small. The model also correctly identiﬁes that the ﬁrst 20 dimensions
contribute to clustering. In contrast, the baselines do not oﬀer ways of inspecting the models, with the
exception of Sparse Clustering. Both small Kand feature attribution in the misspeciﬁed setting are critical
in our motivating example of disease subtyping.
Conclusions from HIV dataset In HIV data (Figure 6, left column), for patients whose treatment
is predicted to fail by pf-GMM and pf-HMM, we observe that several resistance-relevant mutations are
identiﬁed as ‘relevant’. For example, for patients taking nRTIs experiencing virologic failure, prediction-
focused models predominantly identify certain mutations such as K65R, I116V, and M184V as relevant.
These are consistent with Wensing et al. (2019). Patients who are more likely to achieve treatment success
tend to be younger female patients or those with mutations consistent with subtypes that are known to be
easier to treat e.g. subtype B. This is consistent with some bodies of work that have examined the success
of treatment varies across diﬀerent HIV subtypes (e.g. Gatell (2011))
18Published in Transactions on Machine Learning Research (05/2024)
Conclusions from Psychiatric Medical Records dataset From the relevant features table in Figure
7, we see that pf-GMM correctly identiﬁes features relevant to substance use disorder (SUD) outcome. The
model correctly identiﬁes that the presence of diagnosis codes related to alcohol and tobacco disorders means
that the patient has SUD. It also identiﬁes that a history of trauma, mood disorders, depression, and liver
disease can be indicative of SUD prevalence. These discovered features are consistent with the comorbidities
of SUD in the literature (NIDA, 2020). On comparing the pf-GMM clusters on code prevalence (top right
panel in Figure 7), we observe that codes for mood, tobacco, and alcohol-related disorders vary signiﬁcantly
between clusters. This variation explains the diﬀerence in outcome prevalence (bottom right panel). Cluster
0consists of patients with a high prevalence of all three codes and high outcome prevalence. Cluster 5is
another pure cluster, consisting of patients with mood, tobacco, and alcohol-related disorders. Cluster 3only
consists of patients with mood disorders but no tobacco or alcohol use, suggesting that these patients used
other substances since the outcome prevalence is still high. However, mood disorders are not a necessary
condition for high SUD: cluster 4consists of patients with SUD but without diagnosis of mood disorders. pf-
GMM allowed us to recover these clusters despite other signiﬁcant (but irrelevant) clusters in the population.
The other model-based clustering models did not recover a meaningful clustering along these features and
did not identify the features they focused on.
9 Conclusion
We developed prediction-focused generative modeling, a model-based feature selection method for mixture
models and hidden Markov models. More importantly, we developed an understanding about when this
type of approach to feature selection will work and when it will fail. Prediction-focused modeling selects the
‘right’ features to cluster even when the inputs are corrupted with several independent irrelevant features. It
achieves this trade-oﬀ by maximizing the likelihood of the data directly, instead of resorting to any property
of a modiﬁed objective as proposed by Hughes et al. (2018a) or of the approximate posterior as done by
Ren et al. (2020). Our main assumption is in the types of correlations we assume for the relevant and
irrelevant variables (i.e. mixture distribution and hidden Markov distribution). In our analysis, we show
that prediction-focused models envelop supervised mixture models in the data space they can model in the
constrained-components setting. Finally, we validate our ﬁndings on both synthetic and real-world datasets,
including a medical dataset with a large amount of noise.
Acknowledgements
This material is based upon work supported by the National Science Foundation under Grant No. IIS-
1750358, Grant No. IIS-2007076 and by NIH award R01MH123804. Any opinions, ﬁndings, and conclusions
or recommendations expressed in this material are those of the author(s) and do not necessarily reﬂect the
oﬃcial views of the National Science Foundation or the National Institutes of Health.
References
Ali El Attar, Rida Khatoun, and Marc Lemercier. A Gaussian mixture model for dynamic detection of
abnormal behavior in smartphone applications. In 2014 Global Information Infrastructure and Networking
Symposium (GIIS) , pp. 1–6, September 2014. doi: 10.1109/GIIS.2014.6934278.
Christopher M. Bishop. Pattern Recognition and Machine Learning . Springer, 2006.
Chaitanya Chemudugunta, Padhraic Smyth, and Mark Steyvers. Modeling general and speciﬁc aspects of
documents with a probabilistic topic model. In B. Schölkopf, J. Platt, and T. Hoﬀman (eds.), Advances
in Neural Information Processing Systems , volume 19. MIT Press, 2007. URL https://proceedings.
neurips.cc/paper/2006/file/ec47a5de1ebd60f559fee4afd739d59b-Paper.pdf .
Irene Y. Chen, Shalmali Joshi, Marzyeh Ghassemi, and Rajesh Ranganath. Probabilistic Machine Learn-
ing for Healthcare. Annual Review of Biomedical Data Science , 4(1):393–415, 2021. doi: 10.1146/
annurev-biodatasci-092820-033938.
19Published in Transactions on Machine Learning Research (05/2024)
Tianqi Chen, Sameer Singh, Ben Taskar, and Carlos Guestrin. Eﬃcient second-order gradient boosting for
conditional random ﬁelds. In Artiﬁcial Intelligence and Statistics , pp. 147–155. PMLR, 2015.
Adam D. Cobb, Stephen J. Roberts, and Yarin Gal. Loss-calibrated approximate inference in bayesian
neural networks. arXiv:1805.03901 [cs, stat] , May 2018. URL http://arxiv.org/abs/1805.03901 .
arXiv: 1805.03901.
Manoranjan Dash and Huan Liu. Feature selection for clustering. In Takao Terano, Huan Liu, and Arbee
L. P. Chen (eds.), Knowledge Discovery and Data Mining. Current Issues and New Applications , pp.
110–121, Berlin, Heidelberg, 2000. Springer Berlin Heidelberg. ISBN 978-3-540-45571-4.
C.F.Eick,N.Zeidat,andZ.Zhao. Supervisedclustering-algorithmsandbeneﬁts. In 16th IEEEInternational
Conference on Tools with Artiﬁcial Intelligence , pp. 774–776, 2004. doi: 10.1109/ICTAI.2004.111.
Chris Fraley and Adrian E Raftery. Model-based clustering, discriminant analysis, and density estimation.
Journal of the American Statistical Association , 97(458):611–631, 2002. doi: 10.1198/016214502760047131.
URL https://doi.org/10.1198/016214502760047131 .
Joseph Futoma, Michael C. Hughes, and Finale Doshi-Velez. Popcorn: Partially observed prediction con-
strained reinforcement learning. Proceedings of the Twenty Third International Conference on Artiﬁcial
Intelligence and Statistics , 2020. URL https://arxiv.org/pdf/2001.04032.pdf .
José M Gatell. Antiretroviral therapy for hiv: do subtypes matter? Clinical infectious diseases , 53(11):
1153–1155, 2011.
Zoubin Ghahramani and Michael I Jordan. Supervised learning from incomplete data via an em approach.
InAdvances in neural information processing systems , pp. 120–127, 1994.
Yoni Halpern, Steven Horng, and David Sontag. Clinical Tagging with Joint Probabilistic Models. In
Proceedings of the 1st Machine Learning for Healthcare Conference , pp. 209–225. PMLR, December 2016.
Michael Hughes, Leah Weiner, Gabriel Hope, Thomas H. McCoy Jr., Roy H. Perlis, Erik B. Sudderth,
and Finale Doshi-Velez. Prediction-constrained training for semi-supervised mixture and topic models.
arXiv:1707.07341 [cs, stat] , Jul 2017. URL http://arxiv.org/abs/1707.07341 . arXiv: 1707.07341.
Michael Hughes, Gabriel Hope, Leah Weiner, Thomas McCoy, Roy Perlis, Erik Sudderth, and Finale Doshi-
Velez. Semi-supervised prediction-constrained topic models. In Proceedings of the Twenty-First Interna-
tional Conference on Artiﬁcial Intelligence and Statistics , volume 84 of Proceedings of Machine Learning
Research , pp. 1067–1076. PMLR, 09–11 Apr 2018a.
Michael C Hughes, Gabriel Hope, Leah Weiner, Thomas H McCoy Jr, Roy H Perlis, Erik B Sudderth, and
Finale Doshi-Velez. Semi-supervised prediction-constrained topic models. In AISTATS , pp. 1067–1076,
2018b.
FengJiao,ShaojunWang,Chi-HoonLee,RussellGreiner,andDaleSchuurmans. Semi-supervisedconditional
random ﬁelds for improved sequence segmentation and labeling. 2006.
A. Jović, K. Brkić, and N. Bogunović. A review of feature selection methods with applications. In 2015 38th
International Convention on Information and Communication Technology, Electronics and Microelectron-
ics (MIPRO) , pp. 1200–1205, 2015. doi: 10.1109/MIPRO.2015.7160458.
Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization, 2017.
Diederik P Kingma, Danilo J Rezende, Shakir Mohamed, and Max Welling. Semi-supervised learning with
deep generative models. arXiv preprint arXiv:1406.5298 , 2014.
H. S. Kuyuk, E. Yildirim, E. Dogan, and G. Horasan. Application of k-means and Gaussian mixture model
forclassiﬁcationofseismicactivitiesinIstanbul. Nonlinear Processes in Geophysics , 19(4):411–419, August
2012. ISSN 1023-5809. doi: 10.5194/npg-19-411-2012.
20Published in Transactions on Machine Learning Research (05/2024)
SimonLacoste–Julien, FerencHuszár, andZoubinGhahramani. Approximateinferencefortheloss-calibrated
bayesian. In Proceedings of the Fourteenth International Conference on Artiﬁcial Intelligence and Statistics ,
pp. 416–424. JMLR Workshop and Conference Proceedings, Jun 2011. URL http://proceedings.mlr.
press/v15/lacoste_julien11a.html .
NIDA. Common comorbidities with substance use disorders research report, 2020.
Kamal Nigam, Andrew McCallum, Sebastian Thrun, and Tom M. Mitchell. Learning to classify text from
labeled and unlabeled documents. In Jack Mostow and Chuck Rich (eds.), AAAI/IAAI , pp. 792–799.
AAAI Press / The MIT Press, 1998. ISBN 0-262-51098-7.
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan,
Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf,
Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit
Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. PyTorch: An Imperative Style, High-
Performance Deep Learning Library. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d’Alché
Buc, E. Fox, and R. Garnett (eds.), Advances in Neural Information Processing Systems
32, pp. 8024–8035. Curran Associates, Inc., 2019. URL http://papers.neurips.cc/paper/
9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf .
F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer,
R.Weiss,V.Dubourg,J.Vanderplas,A.Passos,D.Cournapeau,M.Brucher,M.Perrot,andE.Duchesnay.
Scikit-learn: Machine learning in Python. Journal of Machine Learning Research , 12:2825–2830, 2011.
Sandhya Prabhakaran and Julia E. Vogt. Bayesian Clustering for HIV1 Protease Inhibitor Contact Maps.
In David Riaño, Szymon Wilk, and Annette ten Teije (eds.), Artiﬁcial Intelligence in Medicine , Lecture
Notes in Computer Science, pp. 281–285, Cham, 2019. Springer International Publishing. ISBN 978-3-
030-21642-9. doi: 10.1007/978-3-030-21642-9_35.
Sandhya Prabhakaran, Karin J. Metzner, Alexander Böhm, and Volker Roth. Recovering Networks from Dis-
tance Data. In Proceedings of the Asian Conference on Machine Learning , pp. 349–364. PMLR, November
2012.
Jason Ren, Russell Kunes, and Finale Doshi-Velez. Prediction focused topic models via feature selection. In
International Conference on Artiﬁcial Intelligence and Statistics , pp. 4420–4429. PMLR, 2020.
Luca Scrucca, Michael Fop, T. Brendan Murphy, and Adrian E. Raftery. mclust 5: clustering, classiﬁcation
and density estimation using Gaussian ﬁnite mixture models. The R Journal , 8(1):289–317, 2016. URL
https://doi.org/10.32614/RJ-2016-021 .
Abhishek Sharma, Jing Zhang, Daniel Nikovski, and Finale Doshi-Velez. Travel-time prediction using neural-
network-based mixture models. Procedia Computer Science , 220:1033–1038, January 2023. ISSN 1877-
0509. doi: 10.1016/j.procs.2023.03.144.
Diogo S. F. Silva and Clayton V. Deutsch. Multivariate data imputation using Gaussian mixture models.
Spatial Statistics , 27:74–90, October 2018. ISSN 2211-6753. doi: 10.1016/j.spasta.2016.11.002.
Veselin Stoyanov, Alexander Ropson, and Jason Eisner. Empirical risk minimization of graphical model
parameters given approximate inference, decoding, and model structure. In Proceedings of the Fourteenth
International Conference on Artiﬁcial Intelligence and Statistics , pp. 725–733. JMLR Workshop and Con-
ference Proceedings, Jun 2011. URL https://proceedings.mlr.press/v15/stoyanov11a.html .
tsurumeso. Github pysparcl, 2024. URL https://github.com/tsurumeso/pysparcl .
Martin J Wainwright and Michael Irwin Jordan. Graphical models, exponential families, and variational
inference . Now Publishers Inc, 2008.
AnnemarieMWensing, VincentCalvez, FrancescaCeccherini-Silberstein, CharlotteCharpentier, HuldrychF
Günthard, Roger Paredes, Robert W Shafer, and Douglas D Richman. 2019 update of the drug resistance
mutations in hiv-1. Topics in antiviral medicine , 27(3):111, 2019.
21Published in Transactions on Machine Learning Research (05/2024)
Daniela M. Witten and Robert Tibshirani. A framework for feature selection in clustering. 105(490):713–726.
ISSN 0162-1459. doi: 10.1198/jasa.2010.tm09415.
Shuai Zheng, Sadeep Jayasumana, Bernardino Romera-Paredes, Vibhav Vineet, Zhizhong Su, Dalong Du,
Chang Huang, and Philip HS Torr. Conditional random ﬁelds as recurrent neural networks. In Proceedings
of the IEEE international conference on computer vision , pp. 1529–1537, 2015.
Xinhua Zhuang, Yan Huang, K. Palaniappan, and Yunxin Zhao. Gaussian mixture density modeling, de-
composition, and applications. IEEE Transactions on Image Processing , 5(9):1293–1302, September 1996.
ISSN 1941-0042. doi: 10.1109/83.535841.
A Supplement Outline
In Section B, we provide the derivation of ELBO and coordinate ascent updates for our prediction-focused
models. In Section C, we provide the derivations for the analysis we presented in the main paper. The
section is divided into three parts: ﬁrst, we provide the derivations for the sup-GMM case, then for the
GMM case, and ﬁnally for the pf-GMM case. For each method, we ﬁrst derive general expressions, and then
expressions for the special case to build intuition for the theory. In Section D and E, we provide additional
experiments and further details about the experiments.
B Inference Details
In this section, we provide the derivation of ELBO and coordinate ascent updates for our prediction-focused
models. We provide the derivation for the pf-HMM case, where Nis the number of sequences and Tnsigniﬁes
the length of sequence n. The results for pf-GMM case follow by setting Tnto 1 for all n. The parameters
of the model are Θ ={θ,A,B,π,η}.
B.1 ELBO derivation
The ELBO for one sequence X,Ywith the posterior distribution q(Z,S|Θ,ϕ)is:
logf(X,Y)≥E
q(Z,S)/bracketleftbigglogf(Z,S,X,Y|Θ,p)
q(Z,S|Θ,ϕ)/bracketrightbigg
=E
q(Z,S)[logf(X,Y|Z,S,Θ)]−KL(q(Z,S|Θ,ϕ)||f(Z,S|A,θ,p ))
=E
q[logf(X|Z,S,B,π )] +E
q/bracketleftbig
logf(Y|Z,ηµ,ηV)/bracketrightbig
−KL(logq(S|ϕ)||logf(S|p))
−KL(logq(Z|X,Y,ϕ,Θ)||logf(Z|θ,A))
where the individual terms are (deﬁne q(Zt|X,Y,Θ,ϕ) =q(Zt)) :
E
q[logf(X|Z,S,B,π )] = E
q(S|ϕ)q(Z|X,Y,Θ,ϕ)/bracketleftBiggN/summationdisplay
n=1Tn/summationdisplay
t=1D/summationdisplay
d=1Sn
t,dlogPB(Xn
td|Ztn,B) +/parenleftbig
1−Sn
t,d/parenrightbig
logPπ(Xn
td|π)/bracketrightBigg
=N/summationdisplay
n=1Tn/summationdisplay
t=1D/summationdisplay
d=1ϕdE
q(Zn
t)[logPB(Xn
td|Zn
t,B)] + (1−ϕd) logPπ(Xn
td|π)
22Published in Transactions on Machine Learning Research (05/2024)
E
q[logf(Y|Z,S,B,π )] = E
q(Z|X,Y,Θ,ϕ)/bracketleftBiggN/summationdisplay
n=1Tn/summationdisplay
t=1logf(Yn
t|Zn
t,ηµ,ηV)/bracketrightBigg
=N/summationdisplay
n=1Tn/summationdisplay
t=1E
q(Zn
t)/bracketleftbig
logp(Yn
t|Zn
t,ηµ,ηV)/bracketrightbig
−KL(logq(Z|X,Y,ϕ,Θ)||logf(Z|θ,A)) =N/summationdisplay
n=1E
q(Zn
1)/bracketleftbigg
logf(Zn
1|θ)
q(Zn
1|X,Y)/bracketrightbigg
+N/summationdisplay
n=1Tn/summationdisplay
t=2E
q(Zn
t−1,t|X,Y)/bracketleftbigg
logf(Zn
t|Zn
t−1,A)
q(Zn
t|Zn
t−1,X,Y)/bracketrightbigg
−KL(logq(S|ϕ)||logf(S|p)) = E
q(S|ϕ)/bracketleftBiggN/summationdisplay
n=1Tn/summationdisplay
t=1D/summationdisplay
d=1logf(Sn
t,d|p)
q(Sn
t,d|ϕd)/bracketrightBigg
=N/summationdisplay
n=1Tn/summationdisplay
t=1D/summationdisplay
d=1ϕdlogp
ϕd+ (1−ϕd) log1−p
1−ϕd
B.2 Coordinate Ascent Updates
Rewriting the ELBO again:
L=/summationdisplay
t/parenleftBiggD/summationdisplay
d=1ϕdE
q(Zt|X,Y)[logPB(Xtd|Zt)] + (1−ϕd) logpπ(Xtd)/parenrightBigg
+ E
q(ZT|X,Y)/bracketleftBiggTn/summationdisplay
t=0logp(Yt|Zt,ηµ,ηV)/bracketrightBigg
−/summationdisplay
dE
q(Sd|ϕd)/bracketleftbigg
logq(Sd|ϕd)
f(Sd|pd)/bracketrightbigg
−E
q(Z0|X,Y)/bracketleftbigg
logq(Z0|X,Y)
f(Z0|θ)/bracketrightbigg
−/summationdisplay
tE
q(Zt−1,t|X,Y)/bracketleftbigg
logq(Zt|Zt−1,X,Y)
f(Zt|Zt−1,A)/bracketrightbigg
=E
q[logf(X|Z,S,B,π )] +E
q/bracketleftbig
logf(Y|Z,ηµ,ηV)/bracketrightbig
+E
q[logf(Z|θ,A)] +E
q[logf(S|p)]
−E
q[logq(S|ϕ)]−E
q[logq(Z|X,Y,ϕ,Θ)]
B.2.1ϕupdate
Relevant terms:
Lϕ=E
q[logf(X|Z,S,B,π )] +E
q[logf(S|p)]−E
q[logq(S|ϕ)]
=N/summationdisplay
n=1Tn/summationdisplay
t=1D/summationdisplay
d=1ϕdE
q(Zt)[logPB(Xn
td|Zt,B)] + (1−ϕd) logPπ(Xn
td|π) +ϕdlogp
ϕd+ (1−ϕd) log1−p
1−ϕd
The gradient of the loss terms:
∇ϕd/primeLϕ=N/summationdisplay
n=1Tn/summationdisplay
t=1E
q(Zt)[logPB(Xn
td/prime|Zt,B)]−logPπ(Xn
td/prime|π) + logp
1−p−logϕd/prime
1−ϕd/prime
Update:
ϕd←σ/parenleftBigg
logp
1−p+/summationtextN
n=1/summationtextTn
t=1Eq(Zn
t|X,Y,ϕ)[logPB(Xn
td|Zn
t,B]−logPπ(Xn
td|π)
/summationtextN
n=1Tn/parenrightBigg
23Published in Transactions on Machine Learning Research (05/2024)
B.2.2Bupdate
µB
Relevant terms:
LµB=N/summationdisplay
n=1Tn/summationdisplay
t=1D/summationdisplay
d=1ϕdE
q(Zn
t)[logPB(Xn
td|Zn
t,B)]
=N/summationdisplay
n=1Tn/summationdisplay
t=1D/summationdisplay
d=1ϕdE
q(Zn
t)/bracketleftBigg
−1
2σB2
Zn
t,d/parenleftBig
Xn
td−µB
Zn
t,d/parenrightBig2
+c/bracketrightBigg
The gradient of the loss terms:
∇µB
j,d/primeLµB=N/summationdisplay
n=1Tn/summationdisplay
t=1ϕd/prime∇µB
j,d/primeK/summationdisplay
k=1q(Zn
t=k)·−1
2σB2
k,d/prime/parenleftbig
Xn
td/prime−µB
k,d/parenrightbig2
=N/summationdisplay
n=1Tn/summationdisplay
t=1ϕd/primeq(Zn
t=j)·/parenleftBig
Xn
td/prime−µB
j,d/prime/parenrightBig
σB2
k,d/prime
Update:
µB
j,d/prime←/summationtextN
n=1/summationtextTn
t=1q(Zn
t=j)Xn
td/prime/summationtextN
n=1/summationtextTn
t=1q(Zn
t=j)
where theq(Zn
t=j)term is computed using the forward-backward algorithm.
σB2
Relevant terms:
LσB2=N/summationdisplay
n=1Tn/summationdisplay
t=1D/summationdisplay
d=1ϕdE
q(Zn
t)/bracketleftBigg
−1
2logσB2
Zn
t,d−1
2σB2
Zn
t,d/parenleftBig
Xn
td−µB
Zn
t,d/prime/parenrightBig2
+c/bracketrightBigg
Gradient of the loss terms:
∇σB2
j,d/primeLσB2=N/summationdisplay
n=1Tn/summationdisplay
t=1ϕd/primeq(Zn
t=j)
−1
2σB2
j,d/prime+1
2/parenleftBig
σB2
j,d/prime/parenrightBig2/parenleftbig
Xn
td/prime−µB
j,d/prime/parenrightbig2

Update (ifϕd/prime/negationslash= 0):
σB2
j,d/prime←/summationtextN
n=1/summationtextTn
t=1q(Zn
t=j)/parenleftBig
Xn
td/prime−µB
j,d/prime/parenrightBig2
/summationtextN
n=1/summationtextTn
t=1q(Zn
t=j)
where theq(Zn
t=j)term is computed using the forward-backward algorithm.
B.2.3πupdate
µπ
24Published in Transactions on Machine Learning Research (05/2024)
Relevant terms:
Lµπ=N/summationdisplay
n=1Tn/summationdisplay
t=1D/summationdisplay
d=1(1−ϕd) logPπ(Xtd|π)
=N/summationdisplay
n=1Tn/summationdisplay
t=1D/summationdisplay
d=1(1−ϕd)−1
2σπ2
d(Xn
td−µπ
d)2+c
The gradient of the loss terms:
∇µπ
d/primeLµπ=N/summationdisplay
n=1Tn/summationdisplay
t=1(1−ϕd/prime)·(Xn
td/prime−µπ
d/prime)
σπ2
d/prime
Update (ifϕd/prime/negationslash= 1):
µπ
d/prime←/summationtextN
n=1/summationtextTn
t=1Xn
td/prime/summationtextN
n=1/summationtextTn
t=11
=/summationtextN
n=1/summationtextTn
t=1Xn
td/prime/summationtextN
n=1Tn
σπ2
Relevant terms:
Lσπ2=N/summationdisplay
n=1Tn/summationdisplay
t=1D/summationdisplay
d=1(1−ϕd)/parenleftbigg
−1
2logσπ2
d−1
2σπ2
d(Xn
td−µπ
d/prime)2+c/parenrightbigg
The gradient of the loss terms:
∇σπ2
d/primeLσπ2=N/summationdisplay
n=1Tn/summationdisplay
t=1(1−ϕd/prime)/parenleftBigg
−1
2σπ2
d/prime+1
2 (σπ2
d/prime)2(Xn
td/prime−µπ
d/prime)2/parenrightBigg
Update:
σπ2
d/prime←/summationtextN
n=1/summationtextTn
t=1(Xn
td/prime−µπ
d/prime)2
/summationtextN
n=1/summationtextTn
t=11
=/summationtextN
n=1/summationtextTn
t=1(Xn
td/prime)2
/summationtextN
n=1Tn−(µπ)2
B.2.4θupdate
Relevant terms (including the constraint and the regularizer):
Lθ= logf(θ|α) +/braceleftBiggN/summationdisplay
n=1K/summationdisplay
k=1q(Zn
1=k)logf(Zn
1=k|θ)/bracerightBigg
−λθ/parenleftBiggK/summationdisplay
k=1θk−1/parenrightBigg
The gradient of the loss terms:
∇θk/primeLθ=−λθ+ (α−1)1
θk/prime+N/summationdisplay
n=1q(Zn
1=k/prime)1
θk/prime
∇λθLθ=−K/summationdisplay
k=1θk
25Published in Transactions on Machine Learning Research (05/2024)
Update:
θk/prime←α−1 +/summationtextN
n=1q(Zn
1=k/prime)
Kα−K+N
=α−1 +E/bracketleftbig
N1
k/prime/bracketrightbig
Kα−K+N
where the terms q(Zn
1=k/prime)are computed using the forward-backward algorithm.
B.2.5Aupdate
Relevant terms:
LA=

N/summationdisplay
n=1Tn/summationdisplay
t=2J/summationdisplay
j=1K/summationdisplay
k=1q(Zn
t−1=j,Zn
t=k)logf(Zn
t=k|Zn
t−1=j,A)

−K/summationdisplay
l=1λAl(K/summationdisplay
k=1Alk−1)
The gradient of the loss terms:
∇Aj/prime,k/primeLA=−λAj/prime+N/summationdisplay
n=1Tn/summationdisplay
t=2q(Zn
t−1=j/prime,Zn
t=k/prime)1
Aj/prime,k/prime
∇λAl/primeLA=−K/summationdisplay
k=1Al/primek
Update:
Aj/prime,k/prime←/summationtextN
n=1/summationtextTn
t=2q(Zn
t−1=j/prime,Zn
t=k/prime)
/summationtextN
n=1/summationtextTn
t=2/summationtextK
k=1q(Zn
t−1=j/prime,Zn
t=k)=/summationtextN
n=1/summationtextTn
t=2q(Zn
t−1=j/prime,Zn
t=k/prime)
/summationtextN
n=1/summationtextTn
t=2q(Zn
t−1=j/prime)
=E[Nj/prime,k/prime]/summationtextK
k/prime=1E[Nj/prime,k/prime]
where the terms q(Zn
t−1=j/prime,Zn
t=k/prime)are computed using the forward-backward algorithm.
B.3 Gibbs Sampling Conditional Distributions
While variational EM is quite stable when working with diagonal covariance matrices for Xinfπandfk,
we require Gibbs sampling if we are working with full covariance matrices:
f(Z(n)|Z(−n),X,Y,S)∝f(Zn)f(Y(n)|Z(n))fk(X(n)
S|Z(n))
f(Sd|S−d,X,Y,Z) =f(Sd)/producttextN
n=1fπ(X(n)
1−S|Sd,S−d)fk(X(n)
S|Z(n),Sd,S−d)
/summationtext
s∈{0,1}f(Sd=s)/producttextN
n=1fπ(X(n)
1−S|Sd=s,S−d)fk(X(n)
S|Z(n),Sd=s,S−d)
26Published in Transactions on Machine Learning Research (05/2024)
C Derivations for Section 6 (Analysis)
C.1 Preliminaries
We ﬁrst state some useful notation and results that we will use in the proofs.
1. We use φ(·;µ,Σ)to denote the density of a multivariate Gaussian distribution with mean µand
covariance Σ.
2. We usefModel (·)to denote the density of the data under a model Model.
3. We use an asterisk superscript to denote the true / data-distributed quantities (e.g. f∗(·)for data
pdf,θ∗,µ∗,Σ∗for true parameters etc.)
4. We use XR∈RDRandXIr∈RDIrto denote the relevant and irrelevant components of the data
X∈RDrespectively. Similarly, we use similar notation for µR,µIr,ΣR,ΣIr.
Data Density First, we write the density of the data ([XR;XIr],Y). In our setup, we assume that (XR,Y)
andXIrare distributed as independent, equal mixtures with KandLcomponents respectively:
f∗(X,Y) =/bracketleftBiggK/summationdisplay
k=11
Kf∗
R,k(XR)f∗
Y,k(Y)/bracketrightBigg/bracketleftBiggL/summationdisplay
l=11
Lf∗
Ir,l(XIr)/bracketrightBigg
=1
KLK/summationdisplay
k=1L/summationdisplay
l=1f∗
Y,k(Y)f∗
R,k(XR)f∗
Ir,l(XIr)
Note that any of the components can be indexed as mkl= (k,l)wherekandlare the indices of the ‘relevant’
and ‘irrelevant’ components respectively:
f∗(X,Y) =1
KLK/summationdisplay
k=1L/summationdisplay
l=1f∗
mkl(X,Y)
We will use this notation in the proofs.
We assume that each of the input data components XRandXIrare Multivariate Normal variables and the
labelYis a Bernoulli variable:
1.f∗
R,k(·) =φ(·;µ∗
R,k,Σ∗
R,k)
2.f∗
Ir,k(·) =φ(·;µ∗
Ir,k,Σ∗
Ir,k)
3.f∗
Y,k(·) =Bern (·;η∗
k)
Lemma C.1. To aid in the proofs, we state the following useful results about the expectation of a function
gunder the data density f∗:
E
f∗[g(X,Y)] =1
KLK/summationdisplay
k=1L/summationdisplay
l=1E
f∗
R,kf∗
Ir,lf∗
Y,k[g(X,Y)] (33)
E
f∗[g(X)] =1
KLK/summationdisplay
k=1L/summationdisplay
l=1E
f∗
R,kf∗
Ir,l[g(X)] (34)
E
f∗[g(XR)] =1
KK/summationdisplay
k=1E
f∗
R,k[g(XR)] (35)
E
f∗[g(XIr)] =1
LL/summationdisplay
l=1E
f∗
Ir,l[g(XIr)] (36)
E
f∗[g(Y)] =1
KK/summationdisplay
k=1E
f∗
Y,k∗[g(Y)] (37)
27Published in Transactions on Machine Learning Research (05/2024)
Lemma C.2. For multivariate Gaussian distributions Q(X) =φ(X;µQ,ΣQ)andf(X) =φ(X;µP,ΣP),
the density φ, the entropy H (Q), cross-entropy H (Q,P)and KL-divergence KL(Q||P)are:
φ(X;µQ,ΣQ) =1/radicalbig
(2π)k|ΣQ|exp/bracketleftbigg
−1
2(X−µQ)TΣ−1
Q(X−µQ)/bracketrightbigg
(38)
H(Q) =k
2ln(2π) +k
2+1
2ln|ΣQ| (39)
H(Q,P) =1
2/bracketleftbig
(µP−µQ)TΣ−1
P(µP−µQ) +kln(2π) + ln|ΣP|+Tr/parenleftbig
Σ−1
PΣQ/parenrightbig/bracketrightbig
(40)
KL(Q||P) =H(Q,P)−H(Q) (41)
28Published in Transactions on Machine Learning Research (05/2024)
Z(n)θX(n) µ,Σ
Y(n)η
N
(a) sup-HMMZ(n)θX(n)µ,Σ
Y(n)η
N
(b) GMM
Figure 8: Graphical models for the GMM and sup-HMM models.
C.2 sup-GMM
We have the same setup as in Section 6 with the exception that we don’t restrict the data and model
to isotropic Gaussians for now. We ﬁrst state our assumptions about the data ﬁrst and then derive the
population log-likelihoods for sup-GMM and pf-GMM. We will use the sup-GMM case to derive the GMM
case.
Assumption: Model and Density We assume that the density of the data under the sup-GMM model
is anequalmixture of Gaussians with Jcomponents (see Figure 8a for the graphical model)
Z∼Cat(1/J),
X|Z=j∼N(µj,Σj),
Y|Z=j∼Bern (ηj)
The model parameters are Θ ={µj,Σj,ηj}J
j=1. The density of the data under the model is:
f(X,Y) =J/summationdisplay
j=1f(Z=j)f(X,Y|Z=j) =J/summationdisplay
j=11
Jf(X,Y|Z=j)
Assumption: Well-separated components We assume that the components of the mixture distribu-
tions are well-separated. The main reason we require this is to simplify the analysis by allowing the posterior
cluster-identity distribution f(Z|X,Y)to be either 0 or 1 for all (X,Y). An important consequence of well-
separated components is that each data point is assigned to exactly one model component with probability
1. Therefore, each data component will fall under exactly one model component. We will use this fact in
the proof of the population log-likelihood below.
Re-indexing data components Due to our “well-separated components” assumption, each data com-
ponent will fall under exactly one model component. If we call the set of data components that fall under
model component jasMj, i.e.
Mj={mkl|data component mklfalls under model component j},
then we can partition the set of data components {mkl}into{Mj|j∈{1,...,J}}.
Lemma C.3. Assume that the data is generated with density f∗(X,Y)speciﬁed by Equation C.1 and the
model is a sup-GMM with Jcomponents. Then, the population log-likelihood for the parameters ΘsupGMM
29Published in Transactions on Machine Learning Research (05/2024)
is:
l(ΘsupGMM ) :=E
f∗[logf(X,Y)] = log1
J+lX(ΘsupGMM ) +lY(ΘsupGMM ) +E
f∗[H[Z|X,Y]](42)
where
lX(ΘsupGMM ) =1
KLJ/summationdisplay
j=1/summationdisplay
(k,l)∈MjE
f∗mkl[logfX,j(X; Θj)]
lY(ΘsupGMM ) =1
KLJ/summationdisplay
j=1/summationdisplay
(k,l)∈MjE
f∗
Y,k[logfj(Y; Θj)]
H[Z|X,Y] :=−E
f(Z|X,Y)[logf(Z|X,Y)]
Proof.We derive a general expression for the population log-likelihood with parameters ΘsupGMM. For
brevity, we omit ΘsupGMMin the density functions.
l(ΘsupGMM ) :=E
f∗[logf(X,Y)]
a=E
f∗/bracketleftbigg
E
f(Z|X,Y)[logf(X,Y)]/bracketrightbigg
b=E
f∗/bracketleftbigg
E
f(Z|X,Y)[logf(X,Y,Z )−logf(Z|X,Y)]/bracketrightbigg
c=E
f∗/bracketleftbigg
E
f(Z|X,Y)[logf(Z) + logf(X|Z) + logf(Y|Z)]−E
f(Z|X,Y)[logf(Z|X,Y)]/bracketrightbigg
d=E
f∗/bracketleftbigg
E
f(Z|X,Y)/bracketleftbigg
log1
J+ logf(X|Z) + logf(Y|Z)/bracketrightbigg
+H[Z|X,Y]/bracketrightbigg
= log1
J+ +E
f∗
J/summationdisplay
j=1qj(X,Y) logfX,j(X; Θj)

/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
lX(ΘsupGMM )+E
f∗
J/summationdisplay
j=1qj(X,Y) logPj(Y; Θj)

/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
lY(ΘsupGMM )+E
f∗[H[Z|X,Y]]
= log1
J+lX(ΘsupGMM ) +lY(ΘsupGMM ) +E
f∗[H[Z|X,Y]]
whereqj(X,Y) :=f(Z=j|X,Y)andEf(Z|X,Y)[logf(Z|X,Y)] :=−H[Z|X,Y]. In the above derivation,
step (a) follows from the fact that the expectation of a constant doesn’t change, step (b) follows from the
fact thatf(X,Y ) =f(X,Y,Z )/f(Z|X,Y ), step (c) follows from the factorization of the model and step (d)
follows from the fact that f(Z) = 1/J.
Expanding the lXterm:
lX(ΘsupGMM ) =E
f∗
J/summationdisplay
j=1qj(X,Y) logfX,j(X; Θj)

=1
KLK/summationdisplay
k=1L/summationdisplay
l=1E
f∗mkl
J/summationdisplay
j=1qj(X,Y) logfX,j(X; Θj)

=1
KLJ/summationdisplay
j=1/summationdisplay
mkl∈MjE
f∗mkl[logfX,j(X; Θj)]
30Published in Transactions on Machine Learning Research (05/2024)
Similarly, we can expand the lY(ΘsupGMM )term:
lY(ΘsupGMM ) =E
f∗
J/summationdisplay
j=1qj(X,Y) logfY,j(Y; Θj)

=1
KLJ/summationdisplay
j=1/summationdisplay
mkl∈MjE
f∗mkl[qj(X,Y) logfY,j(Y; Θj)]
=1
KLJ/summationdisplay
j=1/summationdisplay
mkl∈MjE
f∗mkl[logfY,j(Y; Θj)]
C.2.1 Special case: J= 2,K= 2,L= 2
We now consider the special case where the data is generated with hyperparameters K=L= 2and the
sup-GMM model has J= 2components. This means that the data density is a mixture of 4 components,
while the model has only 2 components. We further assume that true parameters for Ycomponents are
η∗
1= 0,η∗
2= 1. Using this case, we determine the conditions that favor relevant components over irrelevant
ones.
Theorem C.4. Assume that the data is generated with density f∗(X,Y)and parameters Θ∗
supGMM =/braceleftbig
µ∗
j,Σ∗
j,η∗
j/bracerightbigJ
j=1speciﬁed by Equation C.1, and has hyperparameters K=L= 2. The sup-GMM model
withJ= 2components, learns parameters ΘsupGMM =/braceleftBig
ˆµj,ˆΣj,ˆηj/bracerightBigJ
j=1. We deﬁne Θa
supGMMas the param-
eters the sup-GMM the model learns when its components are aligned with data’s relevant components, and
Θu
supGMMwhen unaligned. Then, the population log-likelihood for the parameters Θa
supGMMandΘu
supGMM
is:
l(Θa
supGMM ) = log 1/2 +−1
42/summationdisplay
k=12/summationdisplay
l=1H/parenleftbig
φ(µ∗
R,k,Σ∗
R,k)/parenrightbig
+H/parenleftbig
φ(µ∗
Ir,l,Σ∗
Ir,l)/parenrightbig
+KL/parenleftbig
φ(µ∗
Ir,l,Σ∗
Ir,l)||φ(¯µIr,¯ΣIr)/parenrightbig
l(Θu
supGMM ) = log 1/2 +−log 2−1
42/summationdisplay
k=12/summationdisplay
l=1/parenleftbig
H/parenleftbig
φ(µ∗
Ir,l,Σ∗
Ir,l)/parenrightbig
+H/parenleftbig
φ(µ∗
R,k,Σ∗
R,k)/parenrightbig
+KL/parenleftbig
φ(µ∗
R,k,Σ∗
R,k)||φ(¯µR,¯ΣR)/parenrightbig/parenrightbig
Furthermore, the diﬀerence in the population log-likelihoods between the aligned and unaligned cases is:
l(Θa
supGMM )−l(Θu
supGMM ) = log 2 +1
42/summationdisplay
k=12/summationdisplay
l=1/parenleftbig
KL/parenleftbig
φ(µ∗
R,k,Σ∗
R,k)||φ(¯µR,¯ΣR)/parenrightbig
−KL/parenleftbig
φ(µ∗
Ir,l,Σ∗
Ir,l)||φ(¯µIr,¯ΣIr)/parenrightbig/parenrightbig
Proof.
Aligned case ( Θa
supGMM)Aligned parameters Θa
supGMMcorrespond to the case when the model compo-
nents are aligned with the relevant components of the data. Using this information, we ﬁrst write out the
31Published in Transactions on Machine Learning Research (05/2024)
Description Values
j Model component
indexj= 1 j= 2
(k,l)∈Mj Data component index
assigned to model
component jk= 1,l= 1k= 1,l= 2k= 2,l= 1k= 2,l= 2
Aligned Parameters ,Θa
supGMM
fR,j=f∗
R,k Learned model density
forXRf∗
R,1f∗
R,1f∗
R,2f∗
R,2
fIr,j=φ(¯µIr,¯ΣIr)Learned model density
forXIrφ(¯µIr,¯ΣIr)φ(¯µIr,¯ΣIr)φ(¯µIr,¯ΣIr)φ(¯µIr,¯ΣIr)
fY,j=f∗
Y,k Learned model density
forYf∗
Y,1 f∗
Y,1 f∗
Y,2 f∗
Y,2
Unaligned Parameters ,Θu
supGMM
fR,j=φ(¯µR,¯ΣR)Learned model density
forXRφ(¯µR,¯ΣR)φ(¯µR,¯ΣR)φ(¯µR,¯ΣR)φ(¯µR,¯ΣR)
fIr,j=f∗
Ir,l Learned model density
forXIrf∗
Ir,1f∗
Ir,2f∗
Ir,1f∗
Ir,2
fY,j=Bern (0.5)Learned model density
forYBern (0.5)Bern (0.5)Bern (0.5)Bern (0.5)
Table 2: sup-GMM: Summary of the learned model densities for the aligned and unaligned parameter cases.
individual terms in Θa
supGMM =/braceleftBig
ˆµa
j,ˆΣa
j,ˆηa
j/bracerightBig2
j=1:
ˆµa
j=/bracketleftbiggµ∗
R,j
¯µIr/bracketrightbigg
forj∈{1,2}where ¯µIr=1
L2/summationdisplay
l=1µ∗
Ir,l (43)
ˆΣa
j=/bracketleftbiggΣ∗
R,j 0
0 ¯ΣIr/bracketrightbigg
forj∈{1,2}where ¯ΣIr=1
L/parenleftBigg2/summationdisplay
l=1µ∗/latticetop
Ir,lµ∗
Ir,l+ Σ∗
Ir,l/parenrightBigg
−¯µ/latticetop
Ir¯µIr(44)
ˆηa
j=η∗
j where η∗
j=/braceleftBigg
0ifj= 1
1ifj= 2(45)
32Published in Transactions on Machine Learning Research (05/2024)
We ﬁrst expand the lXterm:
lX(Θa
supGMM ) =1
KLJ/summationdisplay
j=1/summationdisplay
(k,l)∈MjE
f∗mkl/bracketleftBig
logfX,j(X; ˆµa
j,ˆΣa
j)/bracketrightBig
=1
KLJ/summationdisplay
j=1/summationdisplay
(k,l)∈MjE
f∗
R,kf∗
Ir,l/bracketleftBig
logfR,j(XR; ˆµa
R,j,ˆΣa
R,j) + logfIr,j(XIr; ˆµa
Ir,j,ˆΣa
Ir,j)/bracketrightBig
=1
KLJ/summationdisplay
j=1/summationdisplay
(k,l)∈MjE
f∗
R,k/bracketleftBig
logfR,j(XR; ˆµa
R,j,ˆΣa
R,j)/bracketrightBig
+E
f∗
Ir,l/bracketleftBig
logfIr,j(XIr; ˆµa
Ir,j,ˆΣa
Ir,j)/bracketrightBig
a=1
KLJ/summationdisplay
j=1/summationdisplay
(k,l)∈MjE
f∗
R,k/bracketleftbig
logf∗
R,k(XR)/bracketrightbig
+E
f∗
Ir,l/bracketleftbig
logφ(XIr; ¯µIr,¯ΣIr)/bracketrightbig
b=1
KLJ/summationdisplay
j=1/summationdisplay
(k,l)∈Mj−H(φ(XR;µ∗
R,k,Σ∗
R,k))−H(φ(XIr;µ∗
Ir,l,Σ∗
Ir,l),φ(XIr; ¯µIr,¯ΣIr))
c=1
KLJ/summationdisplay
j=1/summationdisplay
(k,l)∈Mj−H/parenleftbig
φ(µ∗
R,k,Σ∗
R,k)/parenrightbig
−H/parenleftbig
φ(µ∗
Ir,l,Σ∗
Ir,l),φ(¯µIr,¯ΣIr)/parenrightbig
±H/parenleftbig
φ(µ∗
Ir,l,Σ∗
Ir,l)/parenrightbig
d=1
KLJ/summationdisplay
j=1/summationdisplay
(k,l)∈Mj−H/parenleftbig
φ(µ∗
R,k,Σ∗
R,k)/parenrightbig
−H/parenleftbig
φ(µ∗
Ir,l,Σ∗
Ir,l)/parenrightbig
−KL/parenleftbig
φ(µ∗
Ir,l,Σ∗
Ir,l)||φ(¯µIr,¯ΣIr)/parenrightbig
e=1
42/summationdisplay
k=12/summationdisplay
l=1−H/parenleftbig
φ(µ∗
R,k,Σ∗
R,k)/parenrightbig
−H/parenleftbig
φ(µ∗
Ir,l,Σ∗
Ir,l)/parenrightbig
−KL/parenleftbig
φ(µ∗
Ir,l,Σ∗
Ir,l)||φ(¯µIr,¯ΣIr)/parenrightbig
where equation (a) uses the learned model densities expressions from Table 2, (b) uses the entropy and
cross-entropy deﬁnitions, (c) and (d) add/subtract the entropy term and use the fact that KL-divergence
is the diﬀerence between cross-entropy and entropy, and (e) uses the fact that the data is generated with
hyperparameters K=L= 2and the model has J= 2components, and {(k,l)∈Mj|j∈ {1,2}}=
{(1,1),(1,2),(2,1),(2,2)}.
Next, we expand the lY(Θa
supGMM )term:
lY(Θa
supGMM ) =1
KLJ/summationdisplay
j=1/summationdisplay
mkl∈MjE
f∗mkl/bracketleftbig
logfY,j(Y; ˆηa
j)/bracketrightbig
=1
KLJ/summationdisplay
j=1/summationdisplay
mkl∈MjE
f∗
Y,k/bracketleftbig
logf∗
Y,k(Y)/bracketrightbig
=1
KLJ/summationdisplay
j=1/summationdisplay
mkl∈Mj−H(f∗
Y,k(Y))
= 0
The rest of the terms in Equation 42 are:
log1
J= log1
2
E
f∗[H[Z|X,Y]] = 0
33Published in Transactions on Machine Learning Research (05/2024)
The likelihood for the aligned case is therefore:
l(Θa
supGMM ) = log1
2−1
42/summationdisplay
k=12/summationdisplay
l=1H/parenleftbig
φ(µ∗
R,k,Σ∗
R,k)/parenrightbig
+H/parenleftbig
φ(µ∗
Ir,l,Σ∗
Ir,l)/parenrightbig
+KL/parenleftbig
φ(µ∗
Ir,l,Σ∗
Ir,l)||φ(¯µIr,¯ΣIr)/parenrightbig
Unaligned case ( Θu
supGMM)Unaligned parameters Θu
supGMMcorrespond to the case when the model
components are unaligned with the relevant components of the data. Using this information, we ﬁrst write
out the individual terms in Θu
supGMM =/braceleftBig
ˆµu
j,ˆΣu
j,ˆηu
j/bracerightBig2
j=1:
ˆµu
j=/bracketleftbigg¯µR
µ∗
Ir,j/bracketrightbigg
forj∈{1,2}where ¯µR=1
K2/summationdisplay
k=1µ∗
R,k (46)
ˆΣu
j=/bracketleftbigg¯ΣR 0
0 Σ∗
Ir,j/bracketrightbigg
forj∈{1,2}where ¯ΣR=1
K/parenleftBigg2/summationdisplay
k=1µ∗/latticetop
R,kµ∗
R,k+ Σ∗
R,k/parenrightBigg
−¯µ/latticetop
R¯µR(47)
ˆηu
j=1
2(48)
We can similarly expand the lXterm as we did for the aligned case:
lX(Θu
supGMM ) =1
KL2/summationdisplay
j=1/summationdisplay
(k,l)∈MjE
f∗mkl/bracketleftBig
logfX,j(X; ˆµu
j,ˆΣu
j)/bracketrightBig
=1
KL2/summationdisplay
j=1/summationdisplay
(k,l)∈Mj−H/parenleftbig
φ(µ∗
Ir,l,Σ∗
Ir,l)/parenrightbig
−H/parenleftbig
φ(µ∗
R,k,Σ∗
R,k)/parenrightbig
−KL/parenleftbig
φ(µ∗
R,k,Σ∗
R,k)||φ(¯µR,¯ΣR)/parenrightbig
=1
42/summationdisplay
k=12/summationdisplay
l=1−H/parenleftbig
φ(µ∗
Ir,l,Σ∗
Ir,l)/parenrightbig
−H/parenleftbig
φ(µ∗
R,k,Σ∗
R,k)/parenrightbig
−KL/parenleftbig
φ(µ∗
R,k,Σ∗
R,k)||φ(¯µR,¯ΣR)/parenrightbig
We can also expand the lY(Θu
supGMM )term as we did for the aligned case:
lY(Θu
supGMM ) =1
KLJ/summationdisplay
j=1/summationdisplay
mkl∈Mj−H(Bern (0.5))
=−log 2
TherestofthetermsinEquation42arethesameasforthealignedcase: log1
J= log1
2andEf∗[H[Z|X,Y]] =
0.
The likelihood for the unaligned case is therefore (we separate the log 2term to highlight that it is the
lY(Θu
supGMM )term):
l(Θu
supGMM ) = log1
2−log 2−1
42/summationdisplay
k=12/summationdisplay
l=1/parenleftbig
H/parenleftbig
φ(µ∗
Ir,l,Σ∗
Ir,l)/parenrightbig
+H/parenleftbig
φ(µ∗
R,k,Σ∗
R,k)/parenrightbig
+KL/parenleftbig
φ(µ∗
R,k,Σ∗
R,k)||φ(¯µR,¯ΣR)/parenrightbig/parenrightbig
This gives us the diﬀerence in the population log-likelihoods between the aligned and unaligned cases:
l(Θa
supGMM )−l(Θu
supGMM ) = log 2 +1
42/summationdisplay
k=12/summationdisplay
l=1/parenleftbig
KL/parenleftbig
φ(µ∗
R,k,Σ∗
R,k)||φ(¯µR,¯ΣR)/parenrightbig
−KL/parenleftbig
φ(µ∗
Ir,l,Σ∗
Ir,l)||φ(¯µIr,¯ΣIr)/parenrightbig/parenrightbig
34Published in Transactions on Machine Learning Research (05/2024)
C.3 GMM - General Case
The general case for GMM is similar to sup-GMM except that Yis not modeled as part of the mixture
distribution. Therefore, we state the model density and the theorem for GMM case by dropping the log 2
term since it is present for both aligned and unaligned parameters.
Assumption: Model and Density We assume that the density of the data under the GMM model is
anequalmixture of Gaussians with Jcomponents but Yis generated independently (see Figure 8b for the
graphical model):
Z∼Cat(1/J),
X|Z=j∼N(µj,Σj),
Y∼Bern (η)
The density of the data under the model is:
f(X,Y) =fYJ/summationdisplay
j=1f(Z=j)f(X|Z=j) =fYJ/summationdisplay
j=11
Jf(X|Z=j)
Theorem C.5. Assume that the data is generated with density f∗(X,Y)and parameters Θ∗
GMM =/braceleftbig
µ∗
j,Σ∗
j,η∗
j/bracerightbigJ
j=1speciﬁed by Equation C.1 and the model is a GMM with learnt parameters ΘGMM =
/braceleftBig
ˆµj,ˆΣj,ˆηj/bracerightBigJ
j=1. Further assume that the data is generated with hyperparameters K=L= 2and the
model hasJ= 2components. Deﬁne Θa
GMMas the parameters that GMM model learns when its compo-
nents are aligned with the relevant components of the data, and Θu
GMMas the parameters that GMM model
learns when its components are unaligned with the relevant components of the data. Then, the population
log-likelihood for the parameters Θa
GMMandΘu
GMMis:
l(Θa
GMM ) =−1
42/summationdisplay
k=12/summationdisplay
l=1H/parenleftbig
φ(µ∗
R,k,Σ∗
R,k)/parenrightbig
+H/parenleftbig
φ(µ∗
Ir,l,Σ∗
Ir,l)/parenrightbig
+KL/parenleftbig
φ(µ∗
Ir,l,Σ∗
Ir,l)||φ(¯µIr,¯ΣIr)/parenrightbig
l(Θu
GMM ) =−log 2−1
42/summationdisplay
k=12/summationdisplay
l=1/parenleftbig
H/parenleftbig
φ(µ∗
Ir,l,Σ∗
Ir,l)/parenrightbig
+H/parenleftbig
φ(µ∗
R,k,Σ∗
R,k)/parenrightbig
+KL/parenleftbig
φ(µ∗
R,k,Σ∗
R,k)||φ(¯µR,¯ΣR)/parenrightbig/parenrightbig
Furthermore, the diﬀerence in the population log-likelihoods between the aligned and unaligned cases is:
l(Θa
GMM )−l(Θu
GMM ) = log 2 +1
42/summationdisplay
k=12/summationdisplay
l=1/parenleftbig
KL/parenleftbig
φ(µ∗
R,k,Σ∗
R,k)||φ(¯µR,¯ΣR)/parenrightbig
−KL/parenleftbig
φ(µ∗
Ir,l,Σ∗
Ir,l)||φ(¯µIr,¯ΣIr)/parenrightbig/parenrightbig
35Published in Transactions on Machine Learning Research (05/2024)
C.4 pf-GMM - General case
Assumption: Model and Density
Sd∼Bern (p)∀d, Z∼Cat(1/J),
Y|Z=j∼Bern (ηj),XS|Z=j∼N(µj,Σj),
X1−S∼N(µπ,Σπ)∀n.
Assumption: Well-separated components and point-mass switch posterior
1. We again assume that the components of the mixture distributions are ‘well-separated’.
2. We also assume that the posterior for the switches is a point-mass at one of the values of S, i.e.
f(S=s|X,Y; ΘpfGMM ) = 1for somes∈{0,1}D. For the parameters we consider, the following are
the values of s:
(a)f(S=s|X,Y; Θa
pfGMM ) = 1fors= [1DR;0DIr]
(b)f(S=s|X,Y; Θa
pfGMM ) = 1fors= [0DR;1DIr].
(c)f(S=s|X,Y; Θ1-G
pfGMM ) = 1fors= [0DR;0DIr].
The density of the data under the model is (see Figure 2a for the graphical model):
f(X,Y) =/summationdisplay
s∈{0,1}DJ/summationdisplay
j=1/bracketleftBiggD/productdisplay
d=1fSd(sd)/bracketrightBigg
fZ(j)fX1−s(X1−s|S=s)fXs(Xs|Z=j,S=s)fY(Y|Z=j)
=1
J/summationdisplay
s∈{0,1}D/bracketleftBiggD/productdisplay
d=1fSd(sd)/bracketrightBigg
fX1−s(X1−s|S=s)J/summationdisplay
j=1fXs(Xs|Z=j,S=s)fY(Y|Z=j)
Lemma C.6. Assume that the data is generated with density f∗(X,Y)speciﬁed by Equation C.1 and the
model is a pf-GMM with Jcomponents. Then, the population log-likelihood for the parameters ΘpfGMMis:
l(ΘpfGMM ) :=E
f∗[logf(X,Y)] = log1
J+lS(ΘpfGMM ) +lX(ΘpfGMM ) +lY(ΘpfGMM ) +E
f∗[H[S,Z|X,Y]]
(49)
where
lS(ΘpfGMM ) =E
f∗/bracketleftBiggD/summationdisplay
d=1logfSd(sd)/bracketrightBigg
lX(ΘpfGMM ) =1
KLK/summationdisplay
k=1L/summationdisplay
l=1E
f∗mkl/bracketleftbig
logfX1−s(X1−s; ΘpfGMM )/bracketrightbig
+1
KLJ/summationdisplay
j=1/summationdisplay
(k,l)∈MjE
f∗mkl[logfXs,j(Xs; ΘpfGMM )]
lY(ΘpfGMM ) =1
KLJ/summationdisplay
j=1/summationdisplay
(k,l)∈MjE
f∗
Y,k[logfY,j(Y; ΘpfGMM )]
H[S,Z|X,Y] =−E
f∗/bracketleftbigg
E
f(S,Z|X,Y)[logf(S,Z|X,Y)]/bracketrightbigg
ψs(X,Y) :=f(S=s|X,Y),which is 1 for some s∈{0,1}Dand 0 otherwise.
36Published in Transactions on Machine Learning Research (05/2024)
Proof.The derivation is similar to the sup-GMM case:
l(ΘpfGMM ) =E
f∗[logf(X,Y)]
=E
f∗/bracketleftbigg
E
f(S,Z|X,Y)[logf(X,Y,S,Z)−logf(S,Z|X,Y)]/bracketrightbigg
=E
f∗/bracketleftBigg
E
f(S,Z|X,Y)/bracketleftBigg
log/bracketleftBiggD/productdisplay
d=1fSd(Sd)/bracketrightBigg
fZ(Z)fX1−S(X1−S; ΘpfGMM )fXS,j(XS; ΘpfGMM )fY,j(Y; ΘpfGMM )/bracketrightBigg/bracketrightBigg
−E
f∗/bracketleftbigg
E
f(S,Z|X,Y)[logf(S,Z|X,Y)]/bracketrightbigg
/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
H[S,Z|X,Y]
Speciﬁcally looking at the ﬁrst term:
a= log E
f∗/bracketleftbigg
E
f(Z|X,Y)[fZ(Z)]/bracketrightbigg
+E
f∗/bracketleftBigg
E
f(S|X,Y)/bracketleftBiggD/summationdisplay
d=1logfSd(Sd)/bracketrightBigg/bracketrightBigg
+E
f∗/bracketleftbigg
E
f(S,Z|X,Y)[logfY,j(Y; ΘpfGMM )]/bracketrightbigg
+E
f∗/bracketleftbigg
E
f(S|X,Y)f(Z|S,X,Y)/bracketleftbig
logfXS,j(XS; ΘpfGMM ) + logfX1−S(X1−S; ΘpfGMM )/bracketrightbig/bracketrightbigg
b= log1
J+E
f∗
/summationdisplay
s∈{0,1}Dψs(X,Y)D/summationdisplay
d=1logfSd(sd)

+E
f∗
/summationdisplay
s∈{0,1}DJ/summationdisplay
j=1ψs(X,Y)f(Z=j|S=s,X,Y) logfY,j(Y; ΘpfGMM )

+E
f∗
/summationdisplay
s∈{0,1}DJ/summationdisplay
j=1ψs(X,Y)f(Z=j|S=s,X,Y)/parenleftbig
logfXS,j(XS; ΘpfGMM ) + logfX1−s(X1−s; ΘpfGMM )/parenrightbig

c= log1
J+E
f∗/bracketleftBiggD/summationdisplay
d=1logfSd(sd)/bracketrightBigg
+1
KLK/summationdisplay
k=1L/summationdisplay
l=1E
f∗mkl
J/summationdisplay
j=1f(Z=j|S=s,X,Y) logfY,j(Y; ΘpfGMM )

+1
KLK/summationdisplay
k=1L/summationdisplay
l=1E
f∗mkl
logfX1−s(X1−s; ΘpfGMM ) +J/summationdisplay
j=1f(Z=j|S=s,X,Y) logfXs,j(Xs; ΘpfGMM )

d= log1
J+E
f∗/bracketleftBiggD/summationdisplay
d=1logfSd(sd)/bracketrightBigg
/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
lS(ΘpfGMM )+1
KLJ/summationdisplay
j=1/summationdisplay
(k,l)∈MjE
f∗mkl[logfY,j(Y; ΘpfGMM )]
/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
lY(ΘpfGMM )
+1
KLK/summationdisplay
k=1L/summationdisplay
l=1E
f∗mkl/bracketleftbig
logfX1−s(X1−s; ΘpfGMM )/bracketrightbig
+1
KLJ/summationdisplay
j=1/summationdisplay
(k,l)∈MjE
f∗mkl[logfXs,j(Xs; ΘpfGMM )]
/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
lX(ΘpfGMM )
where equation (a) expands the logterms, (b) expands the expectation over ZandS, (c) uses the assumption
thatψs(X,Y) = 1for somes∈{0,1}D, and (d) swaps the order of summations over jover (k,l)in a similar
way to the sup-GMM case.
37Published in Transactions on Machine Learning Research (05/2024)
Description Values
j Model component
indexj= 1 j= 2
(k,l)∈Mj Data component index
assigned to model
component jk= 1,l= 1k= 1,l= 2k= 2,l= 1k= 2,l= 2
Aligned Parameters ,Θa
pfGMM
S= [1DR;0DIr]Switches
fXS,j=f∗
R,k Learned model density
forXS=XRf∗
R,1f∗
R,1f∗
R,2f∗
R,2
fX1−S=φ(¯µIr,¯ΣIr)Learned model density
forX1−S=XIrφ(¯µIr,¯ΣIr)φ(¯µIr,¯ΣIr)φ(¯µIr,¯ΣIr)φ(¯µIr,¯ΣIr)
fY,j=f∗
Y,k Learned model density
forYf∗
Y,1 f∗
Y,1 f∗
Y,2 f∗
Y,2
Unaligned Parameters ,Θu
pfGMM
S= [0DR;1DIr]Switches
fXS,j=f∗
Ir,l Learned model density
forXS=XIrf∗
Ir,1f∗
Ir,2f∗
Ir,1f∗
Ir,2
fX1−S=φ(¯µR,¯ΣR)Learned model density
forX1−S=XRφ(¯µR,¯ΣR)φ(¯µR,¯ΣR)φ(¯µR,¯ΣR)φ(¯µR,¯ΣR)
fY,j=Bern (0.5)Learned model density
forYBern (0.5)Bern (0.5)Bern (0.5)Bern (0.5)
Single-Gaussian Parameters ,Θ1-G
pfGMM
S= [0DR;0DIr]Switches
fXS= 1 Learned model density
forXS={}1 1 1 1
fX1−S=φ(¯µ,¯Σ)Learned model density
forX1−S=Xφ(¯µ,¯Σ)φ(¯µ,¯Σ)φ(¯µ,¯Σ)φ(¯µ,¯Σ)
fY,j=Bern (0.5)Learned model density
forYBern (0.5)Bern (0.5)Bern (0.5)Bern (0.5)
Table 3: pf-GMM: Summary of the learned model densities for the aligned and unaligned cases.
Theorem C.7. Assume that the data is generated with density f∗(X,Y)and parameters Θ∗
pfGMM =/braceleftbig
µ∗
j,Σ∗
j,η∗
j/bracerightbigJ
j=1speciﬁed by Equation C.1 and the model is a pf-GMM with learnt parameters ΘpfGMM =
/braceleftBig
ˆµj,ˆΣj,ˆηj/bracerightBigJ
j=1,ˆµπ,ˆΣπ. Further assume that the data is generated with hyperparameters K=L= 2and
the model has J= 2components. Deﬁne Θa
pfGMMas the parameters that pf-GMM model learns when its
components are aligned with the relevant components of the data, and Θu
pfGMMas the parameters that pf-
GMM model learns when its components are unaligned with the relevant components of the data. Then, the
population log-likelihood for the parameters Θa
pfGMMandΘu
pfGMMis:
38Published in Transactions on Machine Learning Research (05/2024)
l(Θa
pfGMM ) = log1
2+DRlogp+DIrlog(1−p)
−1
42/summationdisplay
k=12/summationdisplay
l=1H/parenleftbig
φ(µ∗
R,k,Σ∗
R,k)/parenrightbig
+H/parenleftbig
φ(µ∗
Ir,l,Σ∗
Ir,l)/parenrightbig
+KL/parenleftbig
φ(µ∗
Ir,l,Σ∗
Ir,l)||φ(¯µIr,¯ΣIr)/parenrightbig
l(Θu
pfGMM ) = log1
2+DIrlogp+DRlog(1−p)−log 2
+1
42/summationdisplay
k=12/summationdisplay
l=1−H/parenleftbig
φ(µ∗
Ir,l,Σ∗
Ir,l)/parenrightbig
−H/parenleftbig
φ(µ∗
R,k,Σ∗
R,k)/parenrightbig
−KL/parenleftbig
φ(µ∗
R,k,Σ∗
R,k)||φ(¯µR,¯ΣR)/parenrightbig
l(Θ1-G
pfGMM ) = log1
2+Dlog(1−p)−log 2
+1
42/summationdisplay
k=12/summationdisplay
l=1−H/parenleftbig
φ(µ∗
R,k,Σ∗
R,k)/parenrightbig
−H/parenleftbig
φ(µ∗
Ir,l,Σ∗
Ir,l)/parenrightbig
+1
42/summationdisplay
k=12/summationdisplay
l=1−KL/parenleftbig
φ(µ∗
R,k,Σ∗
R,k)||φ(¯µR,¯ΣR)/parenrightbig
−KL/parenleftbig
φ(µ∗
Ir,l,Σ∗
Ir,l)||φ(¯µIr,¯ΣIr)/parenrightbig
Furthermore, the diﬀerence in the population log-likelihoods between the aligned and other cases is:
l(Θa
pfGMM )−l(Θu
pfGMM ) = (DR−DIr) logp
1−p+ log 2
+1
42/summationdisplay
k=12/summationdisplay
l=1KL/parenleftbig
φ(µ∗
R,k,Σ∗
R,k)||φ(¯µR,¯ΣR)/parenrightbig
−KL/parenleftbig
φ(µ∗
Ir,l,Σ∗
Ir,l)||φ(¯µIr,¯ΣIr)/parenrightbig
l(Θa
pfGMM )−l(Θ1-G
pfGMM ) =DRlogp
1−p+ log 2 +1
42/summationdisplay
k=12/summationdisplay
l=1KL/parenleftbig
φ(µ∗
R,k,Σ∗
R,k)||φ(¯µR,¯ΣR)/parenrightbig
Proof.Like in the sup-GMM case, we ﬁrst write out the individual terms for Θa
pfGMM,Θu
pfGMM,Θ1-G
pfGMM:
Alignedcase( Θa
pfGMM)Alignedparameters Θa
pfGMMcorrespondtothecasewhenthemodelcomponents
arealignedwith the relevant components of the data. We ﬁrst write out the individual terms in Θa
pfGMM:
ˆµa
j=/bracketleftbiggµ∗
R,j
¯µIr/bracketrightbigg
forj∈{1,2}where ¯µIr=1
L2/summationdisplay
l=1µ∗
Ir,l (50)
ˆΣa
j=/bracketleftbiggΣ∗
R,j 0
0 ¯ΣIr/bracketrightbigg
forj∈{1,2}where ¯ΣIr=1
L/parenleftBigg2/summationdisplay
l=1µ∗/latticetop
Ir,lµ∗
Ir,l+ Σ∗
Ir,l/parenrightBigg
−¯µ/latticetop
Ir¯µIr(51)
ˆηa
j=η∗
j where η∗
j=/braceleftBigg
0ifj= 1
1ifj= 2(52)
ˆµπa=/bracketleftbigg¯µR,j
¯µIr/bracketrightbigg
where ¯µR=1
K2/summationdisplay
k=1µ∗
R,k (53)
ˆΣπa=/bracketleftbigg¯ΣR0
0¯ΣIr/bracketrightbigg
where ¯ΣR=1
K/parenleftBigg2/summationdisplay
k=1µ∗/latticetop
R,kµ∗
R,k+ Σ∗
R,k/parenrightBigg
−¯µ/latticetop
R¯µR(54)
Note that the mixture parameters for pf-GMM are the same as sup-GMM and GMM parameters, and the
only additional parameters are the last two elements (corresponding to parameters ˆµπa,ˆΣπa).
39Published in Transactions on Machine Learning Research (05/2024)
Computing the lSterm (using ψs(X,Y) = 1fors= [1DR,0DIr]and0otherwise):
lS(Θa
pfGMM ) =E
f∗/bracketleftBigg/summationdisplay
d∈DlogfSd(sd)/bracketrightBigg
=/summationdisplay
d∈DlogfSd(sd)
=DRlogp+DIrlog(1−p)
Computing the lXterm:
lX(Θa
pfGMM ) =1
KLK/summationdisplay
k=1L/summationdisplay
l=1E
f∗mkl/bracketleftbig
logfX1−s(X1−s; ΘpfGMM )/bracketrightbig
+1
KLJ/summationdisplay
j=1/summationdisplay
(k,l)∈MjE
f∗mkl[logfXs,j(Xs; Θj)]
a=1
KL2/summationdisplay
k=12/summationdisplay
l=1E
f∗
Ir,l/bracketleftbig
logφ(XIr; ¯µIr,¯ΣIr)/bracketrightbig
+1
KL2/summationdisplay
j=1/summationdisplay
(k,l)∈MjE
f∗
R,k/bracketleftbig
logf∗
R,k(XR,k)/bracketrightbig
b=1
KL2/summationdisplay
k=12/summationdisplay
l=1/bracketleftbig
−H/parenleftbig
φ(µ∗
Ir,l,Σ∗
Ir,l)/parenrightbig
−KL/parenleftbig
φ(µ∗
Ir,l,Σ∗
Ir,l)||φ(¯µIr,¯ΣIr)/parenrightbig/bracketrightbig
+1
KL2/summationdisplay
j=1/summationdisplay
(k,l)∈Mj−H/parenleftbig
φ(µ∗
R,k,Σ∗
R,k)/parenrightbig
=1
42/summationdisplay
k=12/summationdisplay
l=1−H/parenleftbig
φ(µ∗
R,k,Σ∗
R,k)/parenrightbig
−H/parenleftbig
φ(µ∗
Ir,l,Σ∗
Ir,l)/parenrightbig
−KL/parenleftbig
φ(µ∗
Ir,l,Σ∗
Ir,l)||φ(¯µIr,¯ΣIr)/parenrightbig
which is the same expression as in sup-GMM case. Here, step (a) follows from the fact that X1−s=XIr
andXs=XRfor the aligned case, and the learned model densities for these cases are given in Table 3. Step
(b) follows from the deﬁnitions of entropy and KL-divergence.
ThelY,log1
J,Ef∗[H[Z|X,Y]]terms are same as in sup-GMM case:
lY(Θa
pfGMM ) = 0
log1
J= log1
2
E
f∗[H[Z|X,Y]] = 0
The likelihood for the aligned case is therefore:
l(Θa
pfGMM ) = log1
2+DRlogp+DIrlog(1−p)
−1
42/summationdisplay
k=12/summationdisplay
l=1H/parenleftbig
φ(µ∗
R,k,Σ∗
R,k)/parenrightbig
+H/parenleftbig
φ(µ∗
Ir,l,Σ∗
Ir,l)/parenrightbig
+KL/parenleftbig
φ(µ∗
Ir,l,Σ∗
Ir,l)||φ(¯µIr,¯ΣIr)/parenrightbig
40Published in Transactions on Machine Learning Research (05/2024)
Unaligned case ( Θu
pfGMM)
ˆµu
j=/bracketleftbigg¯µR
µ∗
Ir,j/bracketrightbigg
forj∈{1,2}where ¯µR=1
K2/summationdisplay
k=1µ∗
R,k (55)
ˆΣu
j=/bracketleftbigg¯ΣR 0
0 Σ∗
Ir,j/bracketrightbigg
forj∈{1,2}where ¯ΣR=1
K/parenleftBigg2/summationdisplay
k=1µ∗/latticetop
R,kµ∗
R,k+ Σ∗
R,k/parenrightBigg
−¯µ/latticetop
R¯µR(56)
ˆηu
j=1
2(57)
ˆµπu=/bracketleftbigg¯µR,j
¯µIr/bracketrightbigg
where ¯µIr=1
L2/summationdisplay
l=1µ∗
Ir,l (58)
ˆΣπu=/bracketleftbigg¯ΣR0
0¯ΣIr/bracketrightbigg
where ¯ΣIr=1
K/parenleftBigg2/summationdisplay
k=1µ∗/latticetop
Ir,kµ∗
Ir,k+ Σ∗
Ir,k/parenrightBigg
−¯µ/latticetop
Ir¯µIr(59)
Computing the lSterm (using ψs(X,Y) = 1fors= [0DR,1DIr]and0otherwise):
lS(Θu
pfGMM ) =E
f∗/bracketleftBigg/summationdisplay
d∈DlogfSd(sd)/bracketrightBigg
=DIrlogp+DRlog(1−p)
The rest of the terms are same as in the unaligned sup-GMM case:
lX(Θu
pfGMM ) =1
42/summationdisplay
k=12/summationdisplay
l=1−H/parenleftbig
φ(µ∗
Ir,l,Σ∗
Ir,l)/parenrightbig
−H/parenleftbig
φ(µ∗
R,k,Σ∗
R,k)/parenrightbig
−KL/parenleftbig
φ(µ∗
R,k,Σ∗
R,k)||φ(¯µR,¯ΣR)/parenrightbig
lY(Θu
pfGMM ) =−log 2
log1
J= log1
2
E
f∗[H[Z|X,Y]] = 0
to get:
l(Θu
pfGMM ) = log1
2+DIrlogp+DRlog(1−p)−log 2
+1
42/summationdisplay
k=12/summationdisplay
l=1−H/parenleftbig
φ(µ∗
Ir,l,Σ∗
Ir,l)/parenrightbig
−H/parenleftbig
φ(µ∗
R,k,Σ∗
R,k)/parenrightbig
−KL/parenleftbig
φ(µ∗
R,k,Σ∗
R,k)||φ(¯µR,¯ΣR)/parenrightbig
and the diﬀerence in the likelihoods between the aligned and unaligned cases is:
l(Θa
pfGMM )−l(Θu
pfGMM ) = (DR−DIr) logp
1−p+ log 2
+1
42/summationdisplay
k=12/summationdisplay
l=1KL/parenleftbig
φ(µ∗
R,k,Σ∗
R,k)||φ(¯µR,¯ΣR)/parenrightbig
−KL/parenleftbig
φ(µ∗
Ir,l,Σ∗
Ir,l)||φ(¯µIr,¯ΣIr)/parenrightbig
Single-Gaussian case ( Θ1-G
pfGMM)The single-Gaussian case is a special case for the pf-GMM model,
where the model has only one component (i.e. it considers all the data to be generated from a single
41Published in Transactions on Machine Learning Research (05/2024)
Gaussian distribution). The parameters for this case are:
ˆµ1-G
j= ˆµπ1-G=/bracketleftbigg¯µR,j
¯µIr/bracketrightbigg
forj∈{1,2}where ¯µR=1
K2/summationdisplay
k=1µ∗
R,k (60)
and ¯µIr=1
L2/summationdisplay
l=1µ∗
Ir,l (61)
ˆΣ1-G
j=ˆΣπ1-G=/bracketleftbigg¯ΣR0
0¯ΣIr/bracketrightbigg
forj∈{1,2}where ¯ΣR=1
K/parenleftBigg2/summationdisplay
k=1µ∗/latticetop
R,kµ∗
R,k+ Σ∗
R,k/parenrightBigg
−¯µ/latticetop
R¯µR(62)
and ¯ΣIr=1
K/parenleftBigg2/summationdisplay
k=1µ∗/latticetop
Ir,kµ∗
Ir,k+ Σ∗
Ir,k/parenrightBigg
−¯µ/latticetop
Ir¯µIr(63)
Computing the lSterm (using ψs(X,Y) = 1fors= [0DR,0DIr]and0otherwise):
lS(Θ1-G
pfGMM ) =E
f∗/bracketleftBigg/summationdisplay
d∈DlogfSd(sd)/bracketrightBigg
=Dlog(1−p)
Next we compute the lXterm:
lX(Θ1-G
pfGMM ) =1
KLK/summationdisplay
k=1L/summationdisplay
l=1E
f∗mkl/bracketleftbig
logfX1−s(X1−s; ΘpfGMM )/bracketrightbig
=1
42/summationdisplay
k=12/summationdisplay
l=1E
f∗
R,k/bracketleftbig
logφ(XR; ¯µR,¯ΣR)/bracketrightbig
+E
f∗
Ir,l/bracketleftbig
logφ(XIr; ¯µIr,¯ΣIr)/bracketrightbig
=1
42/summationdisplay
k=12/summationdisplay
l=1−H/parenleftbig
φ(µ∗
R,k,Σ∗
R,k)/parenrightbig
−KL/parenleftbig
φ(µ∗
R,k,Σ∗
R,k)||φ(¯µR,¯ΣR)/parenrightbig
+1
42/summationdisplay
k=12/summationdisplay
l=1−H/parenleftbig
φ(µ∗
Ir,l,Σ∗
Ir,l)/parenrightbig
−KL/parenleftbig
φ(µ∗
Ir,l,Σ∗
Ir,l)||φ(¯µIr,¯ΣIr)/parenrightbig
ThelY,log1
J,Ef∗[H[Z|X,Y]]terms are same as in the unaligned pf-GMM case: lY(Θ1-G
pfGMM ) =
−log 2,log1
J= log1
2,Ef∗[H[Z|X,Y]] = 0. Therefore, the likelihood for the single-Gaussian case is:
l(Θ1-G
pfGMM ) = log1
2+Dlog(1−p)−log 2
+1
42/summationdisplay
k=12/summationdisplay
l=1−H/parenleftbig
φ(µ∗
R,k,Σ∗
R,k)/parenrightbig
−H/parenleftbig
φ(µ∗
Ir,l,Σ∗
Ir,l)/parenrightbig
+1
42/summationdisplay
k=12/summationdisplay
l=1−KL/parenleftbig
φ(µ∗
R,k,Σ∗
R,k)||φ(¯µR,¯ΣR)/parenrightbig
−KL/parenleftbig
φ(µ∗
Ir,l,Σ∗
Ir,l)||φ(¯µIr,¯ΣIr)/parenrightbig
and the diﬀerence in the likelihoods between the aligned and single-Gaussian cases is:
l(Θa
pfGMM )−l(Θ1-G
pfGMM ) =DRlogp
1−p+ log 2 +1
42/summationdisplay
k=12/summationdisplay
l=1KL/parenleftbig
φ(µ∗
R,k,Σ∗
R,k)||φ(¯µR,¯ΣR)/parenrightbig
42Published in Transactions on Machine Learning Research (05/2024)
Spherical Gaussian case
To develop an intuition for the diﬀerence in likelihoods we derived above, we analyze the case where data
and model components are isotropic Gaussians. First, we deﬁne the data:
µ∗
R,1=−µ∗
R1R, µ∗
R,2=µ∗
R1R,Σ∗
R,1= Σ∗
R,2=σ∗2
RIDR
µ∗
Ir,1=−µ∗
Ir1Ir, µ∗
Ir,2=µ∗
Ir1Ir,Σ∗
Ir,1= Σ∗
Ir,2=σ∗2
IrIDIr
=⇒¯µR=0R,¯ΣR=µ∗2
R1R,R+σ∗2
RIDR
and¯µIr=0Ir,¯ΣIr=µ∗2
Ir1Ir,Ir+σ∗2
IrIDIr (64)
where1Iris aDIr-dimensional vector of all ones and 1Ir,Iris aDIr×DIrmatrix of all ones (similarly for
1R,1R,R). We also deﬁne the signal-to-noise ratio (SNR) for the relevant and irrelevant components as:
∆R:=µ∗
R
σ∗
R,∆Ir:=µ∗
Ir
σ∗
Ir
Using the above expressions, we can obtain the model parameters Θa,Θu, and Θ1-Gusing the parameter
deﬁnitions for each of the models.
For each of the models, we can compute the diﬀerence in log-likelihoods as follows (we use the notation
∆(Θa,Θu) :=l(Θa
Model )−l(Θu
Model )for each of the models):
∆(Θa
GMM,Θu
GMM ) =QR−QIr
∆(Θa
supGMM,Θu
supGMM ) = log 2 + QR−QIr
∆(Θa
pfGMM,Θu
pfGMM ) = (DR−DIr) logp
1−p+ log 2 +QR−QIr
∆(Θa
pfGMM,Θ1-G
pfGMM ) =DRlogp
1−p+ log 2 +QR
whereQRinvolves terms in ∆(Θa
GMM,Θu
GMM )that only depend on DRand∆R(and similarly for QIr):
QR=1
2/bracketleftbiggDR(1−∆2
R)
DR∆2
R+ 1+ log(DR∆2
R+ 1)/bracketrightbigg
QIr=1
2/bracketleftbiggDIr(1−∆2
Ir)
DIr∆2
Ir+ 1+ log(DIr∆2
Ir+ 1)/bracketrightbigg
When are relevant components preferred by each model? For GMM, the relevant components are
preferred when Θa
GMMhas a higher likelihood than Θu
GMM(and similarly for sup-GMM). For pf-GMM, the
relevant components are preferred when Θa
pfGMMhas a higher likelihood than Θu
pfGMMandΘ1-G
pfGMM.
Tuning pf-GMM to prefer relevant components For pf-GMM, we can tune the parameter pto prefer
relevant components. We can do this by setting ∆(Θa
pfGMM,Θu
pfGMM )>0and∆(Θa
pfGMM,Θ1-G
pfGMM )>0.
This gives us the following constraints on p:
p<pu/a =

σ/parenleftBig
log 2+QR−QIr
|DR−DIr|/parenrightBig
ifDR<D Ir
σ/parenleftBig
−log 2+QR−QIr
|DR−DIr|/parenrightBig
ifDR>D Ir
p>p 1-G/a =σ/parenleftbigg
−log 2 +QR
DR/parenrightbigg
43Published in Transactions on Machine Learning Research (05/2024)
whereσ(x) =1
1+exp(−x)is the sigmoid function. The feasible range of pexists when pu/a>p1-G/a, which is
when:
DIrlog 2−DRQIr+DIrQR>0ifDR<D Ir
DIrlog 2−DRQIr+DIrQR<0ifDR>D Ir
D Additional Experiments
D.1 pf-HMM Simulated Data Experiments
2 3 4
K0.60.8Y AUROC
HMM Performance on Synthetic Data (Drel=2)
2 3 4
K0.60.8Y AUROC
HMM Performance on Synthetic Data (Drel=3)
2 3 4
K0.60.8Y AUROC
HMM Performance on Synthetic Data (Drel=4)
2 3 4
K0.60.8Y AUROC
HMM Performance on Synthetic Data (Drel=5)
2 3 4
K0.60.8Y AUROC
HMM Performance on Synthetic Data (Drel=8)
2 3 4
K0.60.8Y AUROC
HMM Performance on Synthetic Data (Drel=12)
2 3 4
K0.70.80.9Y AUROC
HMM Performance on Synthetic Data (Drel=20)
pf-HMM (This Method)
sup-HMM
2-Step
Figure 9: pf-HMM performance with respect to target AUROC on simulated data when we vary the compo-
nent budget for the mixture models. Each panel corresponds to a dataset with a speciﬁed number of relevant
dimensions, while keeping the total number of dimensions ﬁxed at 20.
44Published in Transactions on Machine Learning Research (05/2024)
D.2 pf-GMM Simulated Data Experiments (Full Covariance Parameterization)
1 2 3 4
K0.50.60.70.80.91.0Y AUROC
Predictive Performance as K varies (Drel=1)
1 2 3 4
K0.50.60.70.80.91.0Y AUROC
Predictive Performance as K varies (Drel=2)
1 2 3 4
K0.50.60.70.80.91.0Y AUROC
Predictive Performance as K varies (Drel=3)
1 2 3 4
K0.50.60.70.80.91.0Y AUROC
Predictive Performance as K varies (Drel=4)
1 2 3 4
K0.50.60.70.80.91.0Y AUROC
Predictive Performance as K varies (Drel=5)
pf-GMM (This Method)
pc-GMM
sup-GMM
2-Step
Figure 10: pf-GMM performance (target AUROC) as we vary the component budget (K) for the mixture
models on simulated data. We now model the full covariance matrices for all components. Each panel
corresponds to a dataset with the number of relevant dimensions ﬁxed, while keeping the total number
of dimensions ﬁxed at 8. We see that 2-Step-GMM model under-performs when the Kis small, while
prediction-focus (in pf-GMM and pc-GMM) helps when the relevant dimensions are fewer. We also see
considerable optimization variation in the models, owing to a larger number of parameters needed to learn
a full-covariance matrix
45Published in Transactions on Machine Learning Research (05/2024)
logP(Y|X) logP(X) logP(XR)
DRModel
20 pf-GMM -0.05 -105.65 7.38
pf-GMM (Pred. Term Only) -0.18 -150.18 -20.93
pc-GMM -0.50 -130.41 -15.51
sup-GMM -0.66 -30.81 -15.48
2-Step -0.70 5.79 -26.23
Class Cond. GMM -5.92 14.87 -20.66
30 pf-GMM -0.07 -85.90 12.74
pf-GMM (Pred. Term Only) -0.19 -143.28 -33.05
pc-GMM -0.51 -126.11 -25.99
sup-GMM -0.66 -45.79 -23.08
2-Step -0.69 -40.33 -22.51
Class Cond. GMM -11.01 -20.63 12.13
40 pf-GMM -0.04 -66.07 18.19
pf-GMM (Pred. Term Only) -0.09 -131.40 -40.66
pc-GMM -0.49 -116.71 -30.75
sup-GMM -0.61 -66.95 -18.36
2-Step -0.69 -55.06 -26.88
Class Cond. GMM -3.90 -23.39 16.91
50 pf-GMM -0.09 -46.73 23.15
pf-GMM (Pred. Term Only) -0.25 -135.90 -56.48
pc-GMM -0.51 -112.77 -40.68
sup-GMM -0.63 -69.19 -22.89
2-Step -0.68 -66.60 -30.58
Class Cond. GMM -7.56 -14.29 20.76
80 pf-GMM -0.06 10.66 37.69
pf-GMM (Pred. Term Only) -0.18 -125.77 -95.61
pc-GMM -0.49 -84.83 -55.85
sup-GMM -0.41 -34.03 -3.01
2-Step -0.07 9.21 37.69
Class Cond. GMM -7.65 19.98 37.27logP(Y|X) logP(X) logP(XR)
K Model
4 pf-GMM -0.05 -105.65 7.38
pf-GMM (Pred. Term Only) -0.18 -150.18 -20.93
pc-GMM -0.50 -130.41 -15.51
sup-GMM -0.66 -30.81 -15.48
2-Step -0.70 5.79 -26.23
Class Cond. GMM -5.92 14.87 -20.66
6 pf-GMM -0.05 -92.20 7.49
pf-GMM (Pred. Term Only) -0.10 -151.88 -20.02
pc-GMM -0.51 -129.12 -15.48
sup-GMM -0.63 1.76 -9.23
2-Step -0.69 16.63 -13.77
Class Cond. GMM -2.25 28.47 6.96
8 pf-GMM -0.05 -57.23 8.37
pf-GMM (Pred. Term Only) -0.09 -150.80 -18.66
pc-GMM -0.50 -130.46 -15.55
sup-GMM -0.56 8.80 0.89
2-Step -0.68 23.28 -10.75
Class Cond. GMM -2.07 44.05 8.16
Figure 11: Discriminative and generative performance of all the methods for diﬀerent dataset complexities
(varyingDR) and diﬀerent model complexities (varying K) on simulated data (complex).
E Experimental and Baseline Details
E.1 Discussion: Baselines
Our central purpose for choosing baselines was to span the space of outcomes with generative and discrimi-
native models, and then position our model in that space.
1. 2-Step: This baseline uses a fully generative model for X, in the sense that it learns parameters
without using any information about Y. This learned model is then used for predicting Y.
2. sup-GMM: This baseline uses a fully generative model in a semi-supervised setting, i.e. it uses
information about both X and Y when ﬁtting its parameters.
3. pf-GMM:Thismodelexplicitlytradesoﬀgenerativeanddiscriminativequalityasdescribedinpaper.
4. pc-GMM: This baseline also trades oﬀ generative and discriminative quality in a diﬀerent way.
5. pf-GMM (Pred. Term Only): This baseline has the same model as pf-GMM but the objective is
purely discriminative, i.e. logf(Y|X). This objective is hard to optimize, though. Therefore, we
use this method to demonstrate the predictive-generative trade-oﬀ but not for the rest of the tasks.
6. Random Forest: This is a purely discriminative baseline. We expect it to be similar in performance
to “pf-GMM (Pred. Term Only)”. Random Forests is not a method of interest, though. We want a
generative model that is able to cluster the data, and RF clearly cannot achieve it.
7. Sparse Clustering: This baseline is another 2-step method commonly used in the clustering literature
to ignore certain features in the data. It has no generative model.
46Published in Transactions on Machine Learning Research (05/2024)
E.2 Training Details for Baselines
1. 2-Step: We used the scikit-learn (Pedregosa et al., 2011) implementations of GMM for the gener-
ative model and logistic regression for the discriminative model. The GMM is trained using the
Expectation-Maximization algorithm.
2. Random Forest: We used the scikit-learn (Pedregosa et al., 2011) implementation of Random Forest.
3. pc-GMM: We used the Adam optimizer (Kingma & Ba, 2017) to optimize the pc-GMM objective.
We used the PyTorch (Paszke et al., 2019) library for automatic diﬀerentiation. We search for the
best value of λand learning rate parameters using a grid search.
4. sup-GMM: We used the same implementation as pf-GMM since the special case of p= 1is equivalent
to sup-GMM.
5. Sparse Clustering: We used the implementation from the pysparcl (tsurumeso, 2024) library. We
used 25 permutations to search for the best value of the wparameter as proposed in (Witten &
Tibshirani).
6. pf-GMM (Pred. Term Only): We used the same implementation as pc-GMM with a very high value
ofλto ignore the generative term.
E.3 Simulated datasets for the GMM experiments
E.3.1 Synthetic (simple)
We follow the following generative process to generate a collection of datasets:
p= [p0,···,pKD−1]
[X1,y]∼KD−1/summationdisplay
k=0θR
k[N(K·6,I),Bern (pk)]
X2∼KD−1/summationdisplay
k=0θIr
kN(K·6,I)
X= [X1;X2];X1∈RDR;X2∈RDIr
whereBern (a)is a Bernoulli distribution with parameter a, and6is a constant vector. This process ensures
that the target yis correlated with the ﬁrst DRdimensions of the input Xbut not the last DIrdimensions.
The gap between successive components is ﬁxed to 6 so that the clusters are well separated and so that the
relevant and irrelevant dimensions have equivalent signal-to-noise ratio a-priori.
In our experiments, we set:
KD= 3
DR∈{20,30,40,50,80}
DR+DIr= 100
θR=normalize (0.5 + [0,...,K−1])
θIr=normalize (1 + [0,...,K−1])
wherenormalize (x) =x//bardblx/bardbl1makes sure the vectors are valid probability distributions.
This gives us a knob to tweak: DR(number of relevant dimensions). It is common for the input to only
have a few relevant dimensions. Therefore, we vary DRin { 10, 20, 30, 40, 50, 80, 100 }.
47Published in Transactions on Machine Learning Research (05/2024)
E.3.2 Synthetic (complex)
The generative process for the complex dataset is simple, except for the following diﬀerences:
KD= 4
p= [0.05,0.95,0.05,0.95]
DR∈{20,30,40,50,80}
E.4 Simulated datasets for the HMM experiments
[X1,y]∼K−1/summationdisplay
k=0θR
k[N(K·6,I),Bern (pk)]
X2∼K−1/summationdisplay
k=0θIr
kN(K·6K,I)
X= [X1;X2];X1∈RDR;X2∈RDIr
In our experiments, we set:
K= 4
p= [0.05,0.95,0.05,0.95]
DR∈{2,3,4,5,8,12,20}
DR+DIr= 20
θR=θIr= [.1,.2,.3,.4]
AR←normalize row(.1 +
−z1−
...
−zK−
+IK)
AIr←normalize row(.01 +
−z/prime
1−
...
−z/prime
K−
+IK)
zi,z/prime
i∼Cat(1/K),zi,z/prime
i∈RK
wherenormalize row(M)appliesnormalize to each row of matrix M. Also, the numbers .1 and .01 are
arbitrary choices to diﬀerentiate ARandAIr, and the simulation would work with diﬀerent values too.
E.5 HIV Experiments
Therapy for HIV involves administering cocktails of antiretrovirals from ﬁve classes namely, Non-nucleoside
Reverse Transcriptase Inhibitors (nnRTIs), Nucleoside Reverse Transcriptase Inhibitors (nRTIs), Protease
Inhibitors (PIs), Fusion Inhibitors (FIs), and Integrase Inhibitors (IIs) to bring the viral load below detection
limits (≤40copies/ml). We study 53 236 patients with HIV from the EuResist Integrated Database. Each
person has a time-series of an average length of 16 steps where a time step is approximately 4 months between
consecutive treatments. Our task is to predict whether a treatment will bring the viral load below detection
limits in the next time-step. Each input contains 267 features including CD4+counts, genetic mutations,
treatments in terms of drug classes, and lab results. Though it is common to have many genetic mutations,
only a few of these may be relevant for inducing drug resistance thus increasing the viral load.
48Published in Transactions on Machine Learning Research (05/2024)
Table 4: Comparison of the baselines with pf-GMM.
Class-speciﬁc-GMM GMM pf-GMM
Dataset 1 (Simple)YAUC 1.0 0.6 1.0
logP(X) -10.0 -16.9 -104.6
logP(XR) -19.6 -27.0 10.2
Dataset 2 (Complex)YAUC 0.7 0.5 1.0
logP(X) 10.4 5.6 -106.3
logP(XR) -21.4 -26.8 10.1
F Discussion: Naive Baseline of Fitting Class-Speciﬁc GMMs Fails
A simple alternative to the prediction-focused approach might simply try to learn a separate GMM for each
class label. In this case, the prediction Yfor an input Xcan be predicted based on the probability of the
input under each class-speciﬁc GMM. First, we note that this approach which splits up the data by class
does not allow us to identify structures that have commonalities between the classes. Second, this approach
will often produce poor predictions, as we demonstrate below. In this section, we demonstrate using two
datasets that a baseline that uses a separate GMM for each Ylabel is insuﬃcient in identifying and modeling
the relevant dimensions in the dataset.
F.1 Setup
‘Simple’ Dataset with a unimodal class-speciﬁc density along relevant dimensions. This dataset
contains 100 dimensions, where the ﬁrst 20 dimensions are relevant and generated using a 3-cluster GMM,
and the rest of the dimensions are irrelevant and generated using a separate 3-cluster GMM. Therefore, the
data density along relevant dimensions has three modes, where the middle mode corresponds to the Y= 1
class and the other two modes correspond to the Y= 0class (see the ﬁrst row of Figure 12 for the histogram
of a relevant dimension of this dataset). The data density along irrelevant dimensions also has three modes
but they do not diﬀer in the proportion of Ylabels (see the second row of Figure 12 for the histogram of
an irrelevant dimension). We will show below that this demonstrates the case where the class-speciﬁc GMM
can give good predictions without learning a good density for the relevant dimensions (i.e. for the wrong
reasons).
‘Complex’ Dataset with multimodal class-speciﬁc densities along relevant dimensions. This
dataset contains 100 dimensions, where the ﬁrst 20 dimensions are relevant and generated using a 4-cluster
GMM, and the rest of the dimensions are irrelevant and generated using a separate 4-cluster GMM. In
contrast to the previous dataset, the data density along relevant dimensions has four modes, where the ﬁrst
and the third modes correspond to Y= 0class and the second and the fourth modes correspond to Y= 1
class (see the third row of Figure 12 for the histogram of a relevant dimension of this dataset). The data
density along irrelevant dimensions also has four modes but again they do not diﬀer in the proportion of Y
labels (see the fourth row of Figure 12 for the histogram of an irrelevant dimension). We will show below that
the class-speciﬁc GMM will fail to give good predictions andlearn a poor density for the relevant dimensions
in this case.
Evaluation We evaluate the models on YAUC, logP(X), and logP(XR).YAUC measures how well the
model clusters can discriminate the label and logP(X)measures how accurate the model density is for all
the dimensions of X. Finally, logP(XR)measures how accurately the models estimate the density of only
the relevant dimensions (known to us because we generate the data).
49Published in Transactions on Machine Learning Research (05/2024)
Class-specific GMMDataset 1
Dataset 2GMMpfGMM
Figure 12: Comparison of the class-speciﬁc-GMM baseline (left column) on both datasets reveals that it is
similar to a GMM (middle column) in ignoring relevant dimensions. In contrast, a pf-GMM (right column)
can ignore irrelevant dimensions even when they outnumber the relevant dimensions.
F.2 Results
Class-speciﬁc GMM cannot ﬁlter out irrelevant dimensions In the left column of Figure 12, we see
that the class-speciﬁc GMM baseline clusters on the irrelevant dimensions, and fails to learn a good density
for the relevant dimensions, logP(XR), for both datasets. The densities learned by it are similar to densities
learned by the simple GMM baseline (middle column of Figure 12). In contrast, the pf-GMM model learns a
good density for the relevant dimensions, gives good predictions, and ignores the irrelevant dimensions (see
right column of Figure 12 and Table 4).
Class-speciﬁc GMM can be predictive of Ywithout clustering the relevant dimensions well
In Table 4, we see that the class-speciﬁc GMM can give good predictions on the dataset where the relevant
dimensions have a unimodal class-speciﬁc density (i.e. the ﬁrst dataset in the Setup). This is because it
can model the P(XR|Y= 1)density well and can tell when the data comes from it (as seen in the top row,
left column of Figure 12). However, this good predictive performance is not indicative of a good density
model for the relevant dimensions, as discussed in the previous paragraph. Furthermore, when the relevant
dimensions have all multimodal class-speciﬁc densities (i.e. in the second dataset), the class-speciﬁc GMM
fails to give good predictions andlearns a poor density for the relevant dimensions.
Class-speciﬁc GMM is qualitatively similar to a simple GMM The behavior of the class-speciﬁc
GMM baseline is qualitatively similar to that of a simple GMM: when faced with many irrelevant dimensions,
both these methods just cluster on the irrelevant dimensions (see Figure 12, middle column). Both these
methods achieve a much better overall density logP(X)than the pf-GMM model because they learn a good
densityfortheirrelevantdimensionsandtheirrelevantdimensionsdominatetheoveralldensity. Furthermore,
boththesemethodslearnapoordensityfortherelevantdimensions, logP(XR). Theclass-speciﬁcGMMstill
outperforms the simple GMM quantitatively (Table 4) since it uses twice as many parameters. In contrast,
50Published in Transactions on Machine Learning Research (05/2024)
the pf-GMM model is inherently diﬀerent in its behavior, as it learns to ignore irrelevant dimensions, learns a
good density for the relevant dimensions by clustering on them, and uses these clusters to make predictions.
51