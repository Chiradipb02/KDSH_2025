Kermut: Composite kernel regression for protein
variant effects
Peter Mørch Groth∗ ∗†
University of Copenhagen
NovonesisMads Herbert Kerrn∗†
University of Copenhagen
Lars Olsen
NovonesisJesper Salomon
NovonesisWouter Boomsma†
University of Copenhagen
Abstract
Reliable prediction of protein variant effects is crucial for both protein optimization
and for advancing biological understanding. For practical use in protein engi-
neering, it is important that we can also provide reliable uncertainty estimates for
our predictions, and while prediction accuracy has seen much progress in recent
years, uncertainty metrics are rarely reported. We here provide a Gaussian process
regression model, Kermut, with a novel composite kernel for modeling mutation
similarity, which obtains state-of-the-art performance for supervised protein variant
effect prediction while also offering estimates of uncertainty through its poste-
rior. An analysis of the quality of the uncertainty estimates demonstrates that our
model provides meaningful levels of overall calibration, but that instance-specific
uncertainty calibration remains more challenging.
1 Introduction
Accurately predicting protein variant effects is crucial for both advancing biological understanding
and for engineering and optimizing proteins towards specific traits. Recently, much progress has been
made in the field as a result of advances in machine learning-driven modeling [ 1–3], data availability
[4, 5], and relevant benchmarks [6, 7].
While prediction accuracy has received considerable attention, the ability to quantify the uncertainties
of predictions has been less intensely explored. This is of immediate practical consequence. One of
the main purposes of protein variant effect prediction is as an aid for protein engineering and design,
to propose promising candidates for subsequent experimental characterization. For this purpose, it is
essential that we can quantify, on an instance-to-instance basis, how trustworthy our predictions are.
Specifically, in a Bayesian optimization setting, most choices of acquisition function actively rely on
predicted uncertainties to guide the optimization, and well-calibrated uncertainties have been shown
to correlate with optimization performance [8].
Our goal with this paper is to start a discussion on the quality of the estimated uncertainties of
supervised protein variant effect prediction. Gaussian Processes (GP) are a standard choice for
uncertainty quantification due to the closed form expression of the posterior. We therefore first ask the
question whether state-of-the-art performance can be obtained within the GP framework. We propose
a composite kernel that comfortably achieves this goal, and subsequently investigate the quality of
the uncertainty estimates from such a model. Our results show that while standard approaches like
reliability diagrams give the impression of good levels of calibration, the quantification of per-instance
uncertainties is more challenging. We make our model available as a baseline and encourage the
∗equal contribution
†corresponding authors: petergroth@di.ku.dk ,make@di.ku.dk ,wb@di.ku.dk
38th Conference on Neural Information Processing Systems (NeurIPS 2024).community to place greater emphasis on uncertainty quantification in this important domain. Our
contributions can be summarized as follow:
•We introduce Kermut , a Gaussian process with a novel composite kernel for modeling
mutation similarity, leveraging signals from pretrained sequence and structure models;
•We evaluate this model on the comprehensive ProteinGym substitution benchmark and
show that it is able to reach state-of-the-art performance in supervised protein variant effect
prediction, outperforming recently proposed deep learning methods in this domain;
•We provide a thorough calibration analysis and show that while Kermut provides well-
calibrated uncertainties overall, the calibratedness of instance-specific uncertainties remains
challenging;
•We demonstrate that our model can be trained and evaluated orders of magnitude faster and
with better out-of-the-box calibration than competing methods.
2 Related work
2.1 Protein property prediction
Predicting protein function and properties using machine-learning based approaches continues to be
an innovative and important area of research.
Recently, unsupervised approaches have gained significant momentum where models trained in a
self-supervised fashion have shown impressive results for zero-shot estimates of protein fitness and
variant effects relative to a reference protein [3, 9–11].
Supervised learning is a crucial method of utilizing experimental data to predict protein fitness. This
is particularly valuable when the trait of interest correlates poorly with the evolutionary signals that
unsupervised models capture during training or if multiple traits are considered. Supervised protein
fitness prediction using machine learning has been explored in detail in [ 12], where a comprehensive
overview can be found. A common strategy is to employ transfer learning via embeddings extracted
from self-supervised models [ 13,14], an approach which increasingly relies on large pretrained
language models such as ProtTrans [ 15], ESM-2 [ 16], and SaProt [ 17]. In [ 18], the authors propose to
augment a one-hot encoding of the aligned amino acid sequence by concatenating it with a zero-shot
score for improved predictions. This was further expanded upon with ProteinNPT [ 19], where
sequences embedded with the MSA Transformer [ 20] and zero-shot scores were fed to a transformer
architecture for state-of-the-art supervised variant effect prediction with generative capabilities.
Considerable progress has been made in defining meaningful and comprehensive benchmarks to
reliably measure and compare model performance in both unsupervised and supervised protein fitness
prediction settings. The FLIP benchmark [ 6] introduced three supervised predictions tasks ranging
from local to global fitness prediction, where each task in turn was divided into clearly defined
splits. The supervised benchmarks often view fitness prediction through a particular lens. Where
FLIP targeted problems of interest to protein engineering; TAPE [ 21] evaluated transfer learning
abilities; PEER [ 22] focused on sequence understanding; ATOM3D [ 23] considered a structure-based
approach; FLOP [ 24] targeted wild type proteins; and ProteinGym focused exclusively on variant
effect prediction [ 11]. The ProteinGym benchmark was recently expanded to encompass more than
200 standardized datasets in both zero-shot and supervised settings, including substitutions, insertions,
deletions, and curated clinical datasets [11, 7].
2.2 Kernel methods for protein sequences
Kernel methods have seen much use for protein modeling and protein property prediction. Sequence-
based string kernels operating directly on the protein amino acid sequences are one such example,
where, e.g., matching k-mers at different ks quantify covariance. This has been used with support
vector machines to predict protein homology [ 25,26]. Another example is sub-sequence string
kernels, which in [ 27] is used in a Gaussian process for a Bayesian optimization procedure. In [ 28],
string kernels were combined with predicted physicochemical properties to improve accuracy in the
prediction of MHC-binding peptides and in protein fold classification. In [ 29], a kernel leveraging
the tertiary structure for a protein family represented as a residue-residue contact map was used to
2D
DDD
A
AAA
EEKK
KKD
DDD
A
AAA
EKK
KK
KD
DDD
A
AAA
EEK
KKE
D
DDD
A
AAA
EEKD
KKD
DDD
A
AAA
EEKA
KKFigure 1: Overview of Kermut’s structure kernel. Using an inverse folding model, structure-
conditioned amino acid distributions are computed for all sites in the reference protein. The structure
kernel yields high covariances between two variants if the local environments are similar, if the
mutation probabilities are similar, and if the mutates sites are physically close. Constructed examples
of expected covariances between variant x1andx2,3,4are shown.
predict various protein properties such as enzymatic activity and binding affinity. In [ 30], Gaussian
process regression (GPR) was used to successfully identify promising enzyme sequences which were
subsequently synthesized showing increased activity. In [ 31], the authors provide a comprehensive
study of kernels on biological sequences which includes a thorough review of the literature as well as
both theoretical, simulated, and in-silico results.
Most similar to our work is mGPfusion [ 32], in which a weighted decomposition kernel was defined
which operated on the local tertiary protein structure in conjunction with a number of substitution
matrices. Simulated stability data for all possible single mutations were obtained via Rosetta [ 33],
which was then fused with experimental data for accurate ∆∆G predictions of single- and multi-
mutant variants via GPR, thus incorporating both sequence, structure, and a biophysical simulator.
In contrast to our approach, the mGPfusion-model does not leverage pretrained models, but instead
relies on substitution matrices for its evolutionary signal. A more recent example of kernel-based
methods yielding highly competitive results is xGPR [ 34], in which Gaussian processes with custom
kernels show high performance when trained on protein language model embeddings, similarly to
the sequence kernel in our work (see Section 3.3). Where xGPR introduces a set of novel random
feature-approximated kernels with linear-scaling, Kermut instead uses the squared exponential kernel
for sequence modeling while additionally modeling local structural environments. The models in
xGPR were shown to provide both high accuracy and well-calibrated uncertainty estimation on the
FLIP and TAPE benchmarks.
2.3 Uncertainty quantification and calibration
Uncertainty quantification (UQ) for protein property prediction continues to be a promising area
of research with immediate practical consequences. In [ 35], residual networks were used to model
both epistemic and aleatoric uncertainty for peptide selection. In [ 36], GPR on MLP-residuals from
biLSTM embeddings was used to successfully guide in-silico experimental design of kinase binders
and fluorescent proteins. The authors of [ 37] augmented a Bayesian neural network by placing
biophysical priors over the mean function by directly using Rosetta energy scores, whereby the
model would revert to the biophysical prior when the epistemic uncertainty was large. This was
used to predict fluorescence, binding, and solubility for drug-like molecules. In [ 38], state-of-the-art
performance on protein-protein interactions was achieved by using a spectral-normalized neural
Gaussian process [ 39] with an uncertainty-aware transformer-based architecture working on ESM-2
embeddings.
In [40], a framework for evaluating the epistemic uncertainty of deep learning models using confidence
interval-based metrics was introduced, while [ 41] conducted a thorough analysis of uncertainty
quantification methods for molecular property prediction. Here, the importance of supplementing
confidence-based calibration with error-based calibration as introduced in [ 42] was highlighted,
whereby the predicted uncertainties are connected directly to the expected error for a more nuanced
calibration analysis. We evaluate our model using confidence-based calibration as well as error-based
3calibration following the guidelines in [ 41]. In [ 43], the authors conducted a systematic comparison
of UQ methods on molecular property regression tasks, while [ 44] investigated calibratedness of
regression models for material property prediction. In [ 45], the above approaches were expanded
to protein property prediction tasks where the FLIP [ 6] benchmark was examined, while [ 46]
benchmarked a number of UQ methods for molecular representation models. In [ 47], the authors
developed an active learning approach for partial charge prediction of metal-organic frameworks
via Monte Carlo dropout [ 48] while achieving decent calibration. In [ 49], a systematic analysis of
protein regression models was conducted where well-calibrated uncertainties were observed for a
range of input representations.
2.4 Local structural environments
Much work has been done to solve the inverse-folding problem, where the most probable amino acid
sequence to fold into a given protein backbone structure is predicted [ 50–56]. Inverse-folding models
are trained on large datasets of protein structures and model the physicochemical and evolutionary
constraints of sites in a protein conditioned on their structural contexts. These will form the basis of
the structural featurization in our work. Local structural environments have previously been used for
protein modeling. In [ 57], a 3D CNN was used to predict amino acid preferences giving rise to novel
substitution matrices. In [ 58] and [ 59], surface-level fingerprinting given structural environments was
used to model protein-protein interaction sites and for de novo design of protein interactions. In [ 60],
chemical microenvironments were used to identify potentially beneficial mutations, while [ 61] used a
similar approach, where they investigated the volume of the local environments and observed that
the first contact shell delivered the primary signal thus emphasizing the importance of locality. In
[62], a composition Hellinger distance metric based on the chemical composition of local residue
environments was developed and used for a range of structure-related experiments. Recently, local
structural environments were used to model mutational preferences for protein engineering tasks [ 63],
however not in a Gaussian process framework as we propose here.
3 Methods
3.1 Preliminaries
We want to predict the outcome of an assay measured on a protein, represented by its amino acid
sequence xof length L. We will assume that we have a dataset of Nsuch sequences available, and
that these are of equal length and structure, such that we can meaningfully refer to the effect at
specific positions (sites) in the protein. In protein engineering, we typically consider modifications
relative to an initial wild type sequence, xWT. We will assume that the 3D structure for the initial
sequence, s, is available (either experimentally determined or provided by a structure predictor like
AlphaFold [ 64]). Lastly, for variant xwith mutations at sites M⊆ {1, ..., L}, letxmdenote the
variant which has the same mutation as xat site mform∈Mand otherwise is equal to xWT.
3.2 Gaussian processes
To predict protein variant effects, we rely on Gaussian process regression, which we shall now briefly
introduce. For a comprehensive overview, see [65], which this section is based on.
LetXandYbe two random variables on the measurable spaces XandR, respectively, and let
X=x1, ...,xNandy=y1, ..., y Nbe realizations of these random variables. We assume that
yi=g(xi) +ϵ, where grepresents some unknown function and ϵ∼ N(0, σ2
ϵ)accounts for random
noise. Our objective is to model the distributions capturing our belief about g.
Gaussian processes are stochastic processes providing a powerful framework for modeling distribu-
tions over functions. The Gaussian process framework allows us to not only make predictions but also
to quantify the uncertainty associated with each prediction. A Gaussian process is entirely specified
by its mean andcovariance functions, m(x)andk(x,x′). We assume that the covariance matrix,
K, of the outputs {y1, ..., y N}can be parameterized by a function of their inputs {x1, ...,xN}. The
parameterization is defined by the kernel, k:X × X → Ryielding Ksuch that Kij=k(xi,xj).
Forkto be a valid kernel, Kneeds to be symmetric and positive semidefinite.
4Letfrepresent our approximation of g,f(x)≈g(x). Given a training set D= (X,y)and a
number of test points X∗, the function f∗predicts the values of y∗atX∗. Using rules of normal
distributions, we derive the posterior distribution p(f∗|X∗,D), providing both a prediction of y∗at
X∗and a confidence measure, often expressed as ±2σ, where σis the posterior standard deviation at
a test point x∗.
The kernel function often contains hyperparameters, η. These can be optimized by maximizing the
marginal likelihood, p(y|X, η), which is known as type II maximum likelihood.
3.3 Kermut
It has been shown that local structural dependencies are useful for determining mutation preferences
[57,60,61,63]. We therefore hypothesize that constructing a composite kernel with components
incorporating information about the local structural environments of residues will be able to model
protein variant effects. To this end, we define a structure kernel, kstruct, which models mutation
similarity given the local environments of mutated sites. A schematic of how the structure kernel
models covariances can be seen in Figure 1. In the following we shall define k1
struct, a structure kernel
that operating on single-mutant variants. Subsequently, we shall extend it to multi-mutant variants
resulting in the structure kernel, kstruct.
We hypothesize that for a given site in a protein, the distribution over amino acids given by a structure-
conditioned inverse folding model will reflect the effect of a mutation at that site. We consider such
an amino acid distribution a representation of the local environment for that site as it reflects the
various physicochemical and evolutionary constraints that the site is subject to. We thus presume that
two sites with similar local environments will behave similarly if mutated. For instance, mutations at
buried sites in the hydrophobic core of the protein will generally correlate more with each other than
with surface-level mutations.
For single mutant variants we quantify site similarity using the Hellinger kernel kH(x,x′) =
exp (−γ1dH(fIF(x), fIF(x′))), with γ1>0[66], where dHis the Hellinger distance (see Appendix
B.1). The function fIF:X1→[0,1]20takes a single-mutant sequence, x, as input and returns a
probability distribution over the 20 naturally occurring amino acids at the mutated site in xgiven by
the inverse folding model. The Hellinger kernel will assign maximum covariance when two sites are
identical. This however means that kHis incapable of distinguishing between different mutations at
the same site since dH(x,x′) = 1 , when xandx′are mutated at the same site.
To increase flexibility and to allow intra-site comparisons, we introduce a kernel operating on the
specific mutation likelihoods. We hypothesize that two variants with mutations on sites that are
close in terms of the Hellinger distance will correlate further if the log-probabilities of the specific
amino acids on the mutated sites are similar (i.e., the probability of the amino acid that we mutate
tois similar at the two sites). We incorporate this by defining kp(x,x′) =kexp(fIF1(x), fIF1(x′)) =
exp(−γ2||fIF1(x)−fIF1(x′)||), where fIF1:X1→[0,1]takes a single-mutant sequence, x, as input
and returns the log-probability (given by an inverse folding model) of the observed mutation, and
where kexpis the exponential kernel.
Finally, we hypothesize that the effect of two mutations correlate further if the sites are close in
physical space. Hence, we multiply the kernel with an exponential kernel on the Euclidean distance
between sites: kd(x,x′) = exp ( −γ3de(si, sj)). Thereby, the closer two sites are physically, the
more similar – and thus comparable – their local environments will be.
Taking the product of these kernel components, we get the following kernel for single-mutant variants,
which assigns high covariance when two single mutant variants have mutations that have similar
environments, are physically close, and have similar mutation likelihoods:
k1
struct(x,x′) =λkH(x,x′)kp(x,x′)kd(x,x′), (1)
where the kernel has been scaled by a non-negative scalar, λ >0.
In [63], the authors showed that a simple linear model operating on one-hot encoded mutations is
sufficient to accurately predict mutation effects given sufficient data. Thus, we generalize the kernel
to multiple mutations by summing over all pairs of sites differing at xandx′:
kstruct(x,x′) =X
i∈MX
j∈M′k1
struct(xi,x′j) (2)
5Table 1: Performance on the ProteinGym benchmark. Best results are bold. Kermut reaches superior
performance across splits with significant gains in the challenging modulo and contiguous settings.
OHE and NPT model types correspond to one-hot encodings and non-parametric transformers.
Model Model name Spearman (↑) MSE (↓)
type Contig. Mod. Rand. Avg. Contig. Mod. Rand. Avg.
OHE None 0.064 0.027 0.579 0.224 1.158 1.125 0.898 1.061
ESM-1v 0.367 0.368 0.514 0.417 0.977 0.949 0.764 0.897
DeepSequence 0.400 0.400 0.521 0.440 0.967 0.940 0.767 0.891
MSAT 0.410 0.412 0.536 0.453 0.963 0.934 0.749 0.882
TranceptEVE 0.441 0.440 0.550 0.477 0.953 0.914 0.743 0.870
Embed. ESM-1v 0.481 0.506 0.639 0.542 0.937 0.861 0.563 0.787
MSAT 0.525 0.538 0.642 0.568 0.836 0.795 0.573 0.735
Tranception 0.490 0.526 0.696 0.571 0.972 0.833 0.503 0.769
NPT ProteinNPT 0.547 0.564 0.730 0.613 0.820 0.771 0.459 0.683
GP Kermut 0.610 0.633 0.744 0.662 0.699 0.652 0.414 0.589
The structure kernel models mutations linearly and cannot capture epistatic effects. We propose to
add epistatic signals through a sequence kernel. Drawing on the rich literature for modeling protein
sequences with Gaussian processes on embeddings [ 34,45,49,67], we use a squared exponential
kernel which operates on sequence embeddings from a pretrained model. We use the 650M parameter
ESM-2 protein language model [ 16], and perform mean-pooling across the length dimension, yielding
z=f1(x), where f1produces mean-pooled embeddings, z∈R1280, of sequence x. We thus model
the covariance between these representations as
kseq(x,x′) =kSE(f1(x), f1(x′)) =kSE(z,z′) = exp
−||z−z′||2
2
2σ2
. (3)
We choose to add and weigh the structure and sequence kernels resulting in our final kernel formu-
lation, whereby the model can leverage either structure or sequence similarities, depending on the
presence and strength of each signal as determined through hyperparameter optimization:
k(x,x′) =πkstruct(x,x′) + (1 −π)kseq(x,x′). (4)
Additional details on both sequence and structure kernels, including a proof of the validity of the
structure kernel, implementation details, computational complexity details, and an automatic model
selection procedure can be found in Appendices B and C.
3.3.1 Zero-shot mean function
Kermut can be used with a constant mean function, m(x) =α, where αis a hyperparameter
optimized through the marginal likelihood. However, we posit that additional performance can be
gained by using an altered mean function which operates on zero-shot fitness estimates, which are
often available at relatively low cost: m(x) =αf0(x) +β, where f0is a zero-shot method evaluated
on input sequence x. This is similar to the approach employed in [ 37], where Rosetta scores are used
as a biophysical prior. We use ESM-2 [ 16], which yields the log-likelihood ratio between the variant
and wild type residue as in [10]. For details, see Appendix B.
3.4 Architecture considerations
While Kermut is based on relatively simple principles, its components are non-trivial in that they
exploit learned features from pretrained models: ESM-2 provides protein sequence level embeddings
and zero-shot scores, while ProteinMPNN provides a featurization of the local structural environments.
We stress that these pretrained components can be readily replaced by other pretrained models. Models
that generate (1) protein sequence embeddings, (2) structure-conditioned amino acid distributions,
and (3) zero-shot scores are plentiful and such models will progress further in future years. In our
work, we have not sought to find the optimal combination of these. Our work should instead be seen
6Table 2: Ablation results. Key components of the kernel are removed and the model is trained and
evaluated on 174/217 assays from the ProteinGym benchmark. The ablation column shows the
alteration to the GP formulation. The metrics are subtracted from Kermut to show the change in
performance. Negative ∆Spearman values indicate a drop in performance.
Description Ablation ∆Spearman
Contig. Mod. Rand. Avg.
No structure kernel kstruct= 0 −0.082 −0.078 −0.033 −0.065
No sequence kernel kseq= 0 −0.040 −0.046 −0.048 −0.045
No inter-residue dist. kd= 1 −0.049 −0.051 −0.004 −0.035
No mut. prob./site comp. kp=kH= 1 −0.035 −0.037 −0.006 −0.026
Const. mean m(x) =α−0.036 −0.032 −0.008 −0.026
No mut. prob. kp= 1 −0.022 −0.019 −0.005 −0.016
No site comp. kH= 1 −0.006 −0.009 0 .000 −0.006
as a framework demonstrating how such components can be meaningfully combined in a GP setting
to obtain state-of-the-art results.
4 Results
We evaluate Kermut on the 217 substitution DMS assays from the ProteinGym benchmark [ 7].
The overall benchmark results are an aggregate of three different cross-validation schemes: In the
“random” scheme, variants are assigned to one of five folds randomly. In the “modulo” scheme,
every fifth position along the protein backbone are assigned to the same fold, and in the “contiguous”
scheme, the protein is split into five equal-sized segments along its length, each constituting a fold.
For all three schemes, models are trained on four combined partitions and tested on the fifth for a
total of five runs per assay, per scheme. The results are processed using the functionality provided in
the ProteinGym repository. The average and per-scheme aggregated results can be seen in Table 1.
Our model reaches both higher Spearman correlations and lower mean squared errors than competing
methods and thereby achieves state-of-the-art performance in all three schemes, with the largest gains
in the challenging modulo and contiguous settings. In Table E.2, the performance per functional
category is shown, where we observe a significant performance increase in binding- and stability-
related prediction tasks, likely explained by inclusion of the structure kernel. In addition to its high
accuracy, Kermut is significantly faster compared to deep learning methods. We provide wall-clock
times for running a 5-fold CV loop for a single split-scheme for four select datasets for both Kermut
and ProteinNPT in Table C.1. Generating results for all three split schemes for the four datasets thus
takes Kermut approximately 10 minutes while ProteinNPT takes upwards of 400 hours.
The non-parametric bootstrap standard error for each model relative to Kermut can be seen in
Tables E.1 and E.3. In Appendix M, we provide additional results using alternate zero-shot functions.
The average and per-split performance for individual assays can be seen in Figures O.1 to O.4 while
additional details on computation time can be seen in Appendix C. Lastly, visualizations of the
distributions of optimized hyperparameters can be seen in Appendix N.
4.1 Ablation study
To examine the impact of Kermut’s components, we conduct an ablation study, where we ablate its
two main kernels from Equation (4) – the structure and sequence kernels – as well as the structure
kernel’s subcomponents. We similarly investigate the importance of the zero-shot mean function.
The ablation study is carried out on all split schemes on a subset of 174 datasets. The difference
between the ablation results and the Kermut results can be seen in Table 2, where larger values
indicate large component importance. For the absolute values, see Appendix F, where we additionally
include alternative kernel formulations for both structure and sequence kernels as well as for kernel
composition.
As indicated by the largest drop in performance, the single most important component is the structure
kernel. While removing the sequence kernel leads to comparable decreases for all three schemes,
70.00.20.40.60.8Predictive variance 2
Domain
1M1M (rand.)
1M1M (mod.)
1M1M (cont.)
1M/2M1M (rand.)
1M/2M2M (rand.)
1M2M (extrap.)
Figure 2: Distribution of predictive variances for datasets with double mutants, grouped by domain.
The three first elements correspond to the three split-schemes from ProteinGym. The third and fourth
correspond to training on both single and double mutants, and testing on each, respectively. For the
last column, we train on single and test on double mutants, corresponding to an extrapolation setting.
removing the structure kernel primarily leads to drops in the challenging contiguous and modulo
schemes. This shows that the structure kernel is crucial for characterizing unseen sites in the wild
type protein. While removing the site comparison and mutation probability kernels leads to small and
medium drops in performance, we observe that removing both leads to an even larger performance
drop, indicating a synergy between the two. In Table E.2, the ablation results per functional category
are shown, where observe that the inclusion of the structure kernel is crucial for the increased
performance in structure-related prediction tasks such as binding and stability.
4.2 Uncertainty quantification per mutation domain
By inspecting the posterior predictive variance, we can analyze model uncertainty. To this end, we
define several mutation domains of interest [ 49]. We designate the three split schemes from the
ProteinGym benchmark as three such domains. These are examples of interpolation, where we
both train and test on single mutants (1M →1M). While the main benchmark only considers single
mutations, some assays include additional variants with multiple mutations. We consider a number
of these and define two additional interpolation domains where we train on both single and double
mutations (1M/2M) and test on singles and doubles, respectively. As a challenging sixth domain, we
train on single mutations only and test on doubles (1M →2M), constituting an extrapolation domain.
For details on the multi-mutant splits, see Appendix G.
Figure 2 shows the distributions of mean predictive variances in the six domains. In the three single
mutant domains, we observe that the uncertainties increase from scheme to scheme, reflecting the
difficulties of the tasks and analogously the expected performance scores (Table 1). When training on
both single and double mutants (1M/2M), we observe a lower uncertainty on double mutants than
single mutants. For many of the multi-mutant datasets, the mutants are not uniformly sampled but
often include a fixed single mutation. A possible explanation is thus that it might be more challenging
to decouple the signal from a double mutation into its constituent single mutation signals. In the
extrapolation setting, we observe large predictive uncertainties, as expected. One explanation of
the discrepancy between the variance distributions in the multi-mutant domains might lie in the
difference in target distributions between training and test sets. Figure I.1 in the appendix shows
the overall target distribution of assays for the 51 considered multi-mutant datasets. The single and
double mutants generally belong to different modalities, where the double mutants often lead to a
loss of fitness. This shows the difficulty of predicting on domains not encountered during training.
For reference, we include the results for the multi-mutant domains in Table G.1 in the appendix.
4.3 Uncertainty calibration analysis
To clarify the relationship between model uncertainty and expected performance, we proceed with a
calibration analysis. First, we perform a confidence interval-based calibration analysis [ 41], resulting
in calibration curves which in the classification setting are known as reliability diagrams [ 68]. The
results for each dataset are obtained via five-fold cross validation, corresponding to five separately
trained models for each split scheme. We select four diverse datasets as examples (Table 3), reflecting
both high and low predictive performance. The mean calibration curves can be seen in Figure 3a. For
method details and results across all datasets, see Appendix J. The mean expected calibration error
(ECE) is shown in the bottom of each plot, where a value of zero indicates perfect calibration. Overall,
8Table 3: Details and results for four diverse ProteinGym datasets used for calibration analysis. The
results show the Spearman correlation for each CV-scheme and the average correlation.
Uniprot ID Spearman (↑) Details
Contig. Mod. Rand. Avg. N L Assay Source
BLAT_ECOLX 0.804 0.826 0.909 0.846 4996 286 Organismal fitness [69]
PA_I34A1 0.226 0.457 0.539 0.407 1820 716 Organismal fitness [70]
TCRG1_MOUSE 0.849 0.849 0.928 0.875 621 37 Stability [4]
OPSD_HUMAN 0.739 0.734 0.727 0.734 165 348 Expression [71]
the uncertainties appear to be well-calibrated both qualitatively from the curves and quantitatively
from the ECEs. Even the smallest dataset (fourth row, N= 165 ) achieves decent calibration, albeit
with larger variances between folds.
While the confidence interval-based calibration curves show that we can trust the uncertainty estimates
overall, they do not indicate whether we can trust individual predictions. We therefore supplement the
above analysis with an error-based calibration analysis [ 42], where a well-calibrated model will have
low uncertainty when the error is low. The calibration curves can be seen in Figure 3b. We compute
the per CV-fold expected normalized calibration error (ENCE) and the coefficient of variation (cv),
which quantifies the variance of the predicted uncertainties. Ideally, the ENCE should be zero while
the coefficient of variation should be relatively large (indicating spread-out uncertainties).
While the confidence interval-based analysis suggested that the uncertainty estimates are well-
calibrated with some under-confidence, the same is not as visibly clear for the error-based calibration
plots, suggesting that the expected correlation between model uncertainty and prediction error is
not a given. We do however see an overall trend of increasing error with increasing uncertainty in
three of the four datasets, where the curves lie close to the diagonal (as indicated by the dashed line).
The second row shows poorer calibration – particularly in the modulo and contiguous schemes. The
curves however remain difficult to interpret, in part due to the errorbars on both axes. To alleviate this,
we compute similar metrics for all 217 datasets across the three splits, which indicate that, though
varying, most calibration curves are well-behaved (see Appendix J.3).
We supplement the above calibration curves with Figures K.1 to K.4 in the Appendix, which show
the true values plotted against the predictions. These highlight the importance of well-calibrated
uncertainties and underline their role in interpreting model predictions and their trustworthiness.
4.3.1 Comparison with ProteinNPT
Monte Carlo (MC) dropout [ 48] is a popular uncertainty quantification technique for deep learning
models. Calibration curves for ProteinNPT with MC dropout for the same four datasets across the
three split schemes can be seen in Appendix L.2 while figures showing the true values plotted against
the predictions with uncertainties are shown in Appendix L.3. These indicate that employing MC
dropout on a deep learning model like ProteinNPT seems to provide lower levels of calibration,
providing overconfident uncertainties across assays and splits. Due to the generally low uncertainties
and the resulting difference in scales, the calibration curves are often far from the diagonal. The trends
in the calibration curves however show that the model errors often correlate with the uncertainties,
suggesting that the model can be recalibrated to achieve decent calibration.
Other techniques for uncertain quantification in deep learning models certainly exist, and we by
no means rule out that other techniques can outperform our method (see Discussion below). We
note, however, that many uncertainty quantification methods will be associated with considerable
computational overhead compared to the built-in capabilities of a Gaussian process.
5 Discussion
We have shown that a carefully constructed Gaussian process is able to reach state-of-the-art perfor-
mance for supervised protein variant effect prediction while providing reasonably well-calibrated
90.00.20.40.60.81.0Confidence
ECE: 0.04 (±0.02)Random
ECE: 0.01 (±0.01)Modulo
ECE: 0.03 (±0.03)Contiguous
0.00.20.40.60.81.0Confidence
ECE: 0.11 (±0.04)
 ECE: 0.09 (±0.02)
 ECE: 0.08 (±0.07)
0.00.20.40.60.81.0Confidence
ECE: 0.05 (±0.04)
 ECE: 0.05 (±0.07)
 ECE: 0.07 (±0.03)
0.0 0.2 0.4 0.6 0.8 1.0
Percentile0.00.20.40.60.81.0Confidence
ECE: 0.05 (±0.05)
0.0 0.2 0.4 0.6 0.8 1.0
Percentile
ECE: 0.05 (±0.02)
0.0 0.2 0.4 0.6 0.8 1.0
Percentile
ECE: 0.04 (±0.04)(a) Confidence interval-based calibration curves.
0.4 0.50.300.350.400.45RMSE
ENCE: 0.13 (±0.04)
cv: 0.11 (±0.01)Random
0.5 0.6 0.70.40.50.60.7
ENCE: 0.09 (±0.09)
cv: 0.11 (±0.03)Modulo
0.5 0.6 0.70.40.50.60.70.8
ENCE: 0.15 (±0.10)
cv: 0.11 (±0.03)Contiguous
0.8 0.9 1.00.00.51.01.5RMSE
ENCE: 0.32 (±0.19)
cv: 0.05 (±0.05)
0.90 0.95 1.000.00.51.01.5
ENCE: 0.26 (±0.16)
cv: 0.02 (±0.01)
0.95 1.00 1.050.00.51.01.5
ENCE: 0.30 (±0.15)
cv: 0.02 (±0.02)
0.2 0.30.00.20.40.6RMSE
ENCE: 0.22 (±0.12)
cv: 0.09 (±0.03)
0.4 0.60.00.20.40.6
ENCE: 0.25 (±0.14)
cv: 0.10 (±0.07)
0.4 0.60.00.51.0
ENCE: 0.33 (±0.29)
cv: 0.11 (±0.05)
0.4 0.6
RMV0.00.51.0RMSE
ENCE: 0.31 (±0.32)
cv: 0.12 (±0.04)
0.5 0.6 0.7
RMV0.250.500.751.00
ENCE: 0.26 (±0.09)
cv: 0.06 (±0.05)
0.5 0.6 0.7
RMV0.00.51.0
ENCE: 0.29 (±0.21)
cv: 0.06 (±0.02) (b) Error-based calibration curves.
Figure 3: Calibration curves for Kermut using different methods. Mean ECE/ENCE values ( ±2σ)
are shown. Dashed line ( x=y) corresponds to ideal calibration. The row order corresponds to the
ordering in Table 3. (a) exhibits good calibration as indicated by curves close to the diagonal and
ECE values close to zero, albeit with under-confident uncertainties in the second row. In (b), Kermut
is also relatively well-calibrated, as indicated by the increasing curves, albeit with large variances
along both axes. The low coefficients of variation ( cv) indicate similar predictive variances in each
setting. Overall, Kermut achieves good calibration in most cases as a result of the designed kernel.
uncertainty estimates. For a majority of datasets, this is achieved orders of magnitude faster than
competing methods.
While the predictive performance on the substitution benchmark is an improvement over previous
methods, our proposed model has its limitations. Due to the site-comparison mechanism, our model
is unable to handle insertions and deletions as it only operates on a fixed structure. Additionally,
as the number of mutations increases, the assumption of a fixed structure might worsen, depending
on the introduced mutations, which can affect reliability as the local environments might change.
An additional limitation is the GP’s O(N3)scaling with dataset size. While not a major obstacle
in the single mutant setting, dataset sizes can quickly grow when handling multiple mutants. The
last decades have however produced a substantial literature on algorithms for scaling GPs to larger
datasets [ 72–74], which could alleviate the issue, and we therefore believe this to be a technical
rather than fundamental limitation. An additional limitation might present itself it in the multi-mutant
setting, where the lack of explicit modeling of epistasis can potentially hinder extrapolation to
higher-order mutants, prompting further investigation.
Well-calibrated uncertainties are crucial for protein engineering; both when relying on a Bayesian
optimization routine to guide experimental design using uncertainty-dependent acquisition functions
and similarly to weigh the risk versus reward for experimentally synthesizing suggested variants. We
therefore encourage the community to place a greater emphasis on uncertainty quantification and
calibration for protein prediction models as this will have measurable impacts in real-life applications
like protein engineering – perhaps more so than increased prediction accuracy. We hope that Kermut
can serve as a fruitful step in this direction.
10Acknowledgments and Disclosure of Funding
This work was funded in part by Innovation Fund Denmark (1044-00158A), the Novo Nordisk
Synergy grant (NNF200C0063709), VILLUM FONDEN (40578), the Pioneer Centre for AI (DNRF
grant number P1), and the Novo Nordisk Foundation through the MLSS Center (Basic Machine
Learning Research in Life Science, NNF20OC0062606).
References
[1]Adam J. Riesselman, John B. Ingraham, and Debora S. Marks. Deep generative models of
genetic variation capture the effects of mutations. Nature Methods , 15(10):816–822, October
2018.
[2]Alexander Rives, Joshua Meier, Tom Sercu, Siddharth Goyal, Zeming Lin, Jason Liu, Demi Guo,
Myle Ott, C Lawrence Zitnick, Jerry Ma, et al. Biological structure and function emerge from
scaling unsupervised learning to 250 million protein sequences. Proceedings of the National
Academy of Sciences , 118(15):e2016239118, 2021.
[3]Jun Cheng, Guido Novati, Joshua Pan, Clare Bycroft, Akvil ˙e Žemgulyt ˙e, Taylor Applebaum,
Alexander Pritzel, Lai Hong Wong, Michal Zielinski, Tobias Sargeant, Rosalia G. Schnei-
der, Andrew W. Senior, John Jumper, Demis Hassabis, Pushmeet Kohli, and Žiga Avsec.
Accurate proteome-wide missense variant effect prediction with AlphaMissense. Science ,
381(6664):eadg7492, September 2023.
[4]Kotaro Tsuboyama, Justas Dauparas, Jonathan Chen, Elodie Laine, Yasser Mohseni Behbahani,
Jonathan J. Weinstein, Niall M. Mangan, Sergey Ovchinnikov, and Gabriel J. Rocklin. Mega-
scale experimental analysis of protein folding stability in biology and design. Nature , pages
1–11, July 2023.
[5]Mihaly Varadi, Damian Bertoni, Paulyna Magana, Urmila Paramval, Ivanna Pidruchna,
Malarvizhi Radhakrishnan, Maxim Tsenkov, Sreenath Nair, Milot Mirdita, Jingi Yeo, Oleg
Kovalevskiy, Kathryn Tunyasuvunakool, Agata Laydon, Augustin Žídek, Hamish Tomlinson,
Dhavanthi Hariharan, Josh Abrahamson, Tim Green, John Jumper, Ewan Birney, Martin Steineg-
ger, Demis Hassabis, and Sameer Velankar. AlphaFold Protein Structure Database in 2024:
Providing Structure Coverage for over 214 Million Protein Sequences. Nucleic Acids Research ,
52(D1):D368–D375, November 2023.
[6]Christian Dallago, Jody Mou, Kadina E Johnston, Bruce Wittmann, Nick Bhattacharya, Samuel
Goldman, Ali Madani, and Kevin K. Yang. FLIP: Benchmark tasks in fitness landscape inference
for proteins. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and
Benchmarks Track (Round 2) , 2021.
[7]Pascal Notin, Aaron W. Kollasch, Daniel Ritter, Lood Van Niekerk, Steffan Paul, Han Spinner,
Nathan J. Rollins, Ada Shaw, Rose Orenbuch, Ruben Weitzman, Jonathan Frazer, Mafalda Dias,
Dinko Franceschi, Yarin Gal, and Debora Susan Marks. ProteinGym: Large-Scale Benchmarks
for Protein Fitness Prediction and Design. In Thirty-Seventh Conference on Neural Information
Processing Systems Datasets and Benchmarks Track , November 2023.
[8]Jonathan Foldager, Mikkel Jordahn, Lars Kai Hansen, and Michael Riis Andersen. On the
Role of Model Uncertainties in Bayesian Optimization. In Proceedings of the Thirty-Ninth
Conference on Uncertainty in Artificial Intelligence , UAI ’23. JMLR.org, 2023.
[9]Jonathan Frazer, Pascal Notin, Mafalda Dias, Aidan Gomez, Joseph K. Min, Kelly Brock,
Yarin Gal, and Debora S. Marks. Disease variant prediction with deep generative models of
evolutionary data. Nature , 599(7883):91–95, November 2021.
[10] Joshua Meier, Roshan Rao, Robert Verkuil, Jason Liu, Tom Sercu, and Alex Rives. Language
models enable zero-shot prediction of the effects of mutations on protein function. Advances in
neural information processing systems , 34:29287–29303, 2021.
11[11] Pascal Notin, Mafalda Dias, Jonathan Frazer, Javier Marchena-Hurtado, Aidan N. Gomez,
Debora Marks, and Yarin Gal. Tranception: protein fitness prediction with autoregressive
transformers and inference-time retrieval. In International Conference on Machine Learning ,
pages 16990–17017. PMLR, 2022.
[12] Kevin K. Yang, Zachary Wu, and Frances H. Arnold. Machine-learning-guided directed
evolution for protein engineering. Nature Methods , 16(8):687–694, August 2019.
[13] Ehsaneddin Asgari and Mohammad R. K. Mofrad. Continuous Distributed Representation of
Biological Sequences for Deep Proteomics and Genomics. PloS one , 10(11):e0141287, 2015.
[14] Kevin K. Yang, Zachary Wu, Claire N. Bedbrook, and Frances H. Arnold. Learned protein
embeddings for machine learning. Bioinformatics (Oxford, England) , 34(15):2642–2648,
August 2018.
[15] Ahmed Elnaggar, Michael Heinzinger, Christian Dallago, Ghalia Rehawi, Yu Wang, Llion Jones,
Tom Gibbs, Tamas Feher, Christoph Angerer, Martin Steinegger, et al. ProtTrans: Toward
Understanding the Language of Life Through Self-Supervised Learning. IEEE transactions on
pattern analysis and machine intelligence , 44(10):7112–7127, 2021.
[16] Zeming Lin, Halil Akin, Roshan Rao, Brian Hie, Zhongkai Zhu, Wenting Lu, Nikita Smetanin,
Robert Verkuil, Ori Kabeli, Yaniv Shmueli, et al. Evolutionary-scale prediction of atomic-level
protein structure with a language model. Science , 379(6637):1123–1130, 2023.
[17] Jin Su, Chenchen Han, Yuyang Zhou, Junjie Shan, Xibin Zhou, and Fajie Yuan. SaProt: Protein
Language Modeling with Structure-aware V ocabulary. In The Twelfth International Conference
on Learning Representations , 2024.
[18] Chloe Hsu, Hunter Nisonoff, Clara Fannjiang, and Jennifer Listgarten. Learning protein fitness
models from evolutionary and assay-labeled data. Nature Biotechnology , pages 1–9, January
2022.
[19] Pascal Notin, Ruben Weitzman, Debora Marks, and Yarin Gal. ProteinNPT: Improving Protein
Property Prediction and Design with Non-Parametric Transformers. In Advances in Neural
Information Processing Systems , volume 36, pages 33529–33563. Curran Associates, Inc.,
2023.
[20] Roshan M. Rao, Jason Liu, Robert Verkuil, Joshua Meier, John Canny, Pieter Abbeel, Tom
Sercu, and Alexander Rives. MSA Transformer. In Proceedings of the 38th International
Conference on Machine Learning , pages 8844–8856. PMLR, July 2021.
[21] Roshan M. Rao, Nicholas Bhattacharya, Neil Thomas, Yan Duan, Peter Chen, John Canny,
Pieter Abbeel, and Yun Song. Evaluating protein transfer learning with TAPE. Advances in
neural information processing systems , 32, 2019.
[22] Minghao Xu, Zuobai Zhang, Jiarui Lu, Zhaocheng Zhu, Yangtian Zhang, Ma Chang, Runcheng
Liu, and Jian Tang. PEER: A Comprehensive and Multi-Task Benchmark for Protein Sequence
Understanding. Advances in Neural Information Processing Systems , 35:35156–35173, 2022.
[23] Raphael John Lamarre Townshend, Martin Vögele, Patricia Adriana Suriana, Alexander Derry,
Alexander Powers, Yianni Laloudakis, Sidhika Balachandar, Bowen Jing, Brandon M. Anderson,
Stephan Eismann, Risi Kondor, Russ Altman, and Ron O. Dror. ATOM3D: Tasks On Molecules
in Three Dimensions. In Thirty-fifth Conference on Neural Information Processing Systems
Datasets and Benchmarks Track (Round 1) , 2021.
[24] Peter Mørch Groth, Richard Michael, Jesper Salomon, Pengfei Tian, and Wouter Boomsma.
FLOP: Tasks for Fitness Landscapes Of Protein wildtypes, June 2023.
[25] Christina Leslie, Eleazar Eskin, and William Stafford Noble. The spectrum kernel: A string
kernel for SVM protein classification. In Biocomputing 2002 , pages 564–575. WORLD
SCIENTIFIC, December 2001.
12[26] Christina S. Leslie, Eleazar Eskin, Adiel Cohen, Jason Weston, and William Stafford Noble.
Mismatch String Kernels for Discriminative Protein Classification. Bioinformatics , 20(4):467–
476, March 2004.
[27] Henry Moss, David Leslie, Daniel Beck, Javier Gonzalez, and Paul Rayson. BOSS: Bayesian
Optimization over String Spaces. Advances in neural information processing systems , 33:15476–
15486, 2020.
[28] Nora C. Toussaint, Christian Widmer, Oliver Kohlbacher, and Gunnar Rätsch. Exploiting
physico-chemical properties in string kernels. BMC bioinformatics , 11:1–9, 2010.
[29] Philip A. Romero, Andreas Krause, and Frances H. Arnold. Navigating the protein fitness land-
scape with Gaussian processes. Proceedings of the National Academy of Sciences , 110(3):E193–
E201, January 2013.
[30] Jonathan C. Greenhalgh, Sarah A. Fahlberg, Brian F. Pfleger, and Philip A. Romero. Machine
learning-guided acyl-ACP reductase engineering for improved in vivo fatty alcohol production.
Nature communications , 12(1):5825, 2021.
[31] Alan Nawzad Amin, Eli Nathan Weinstein, and Debora Susan Marks. Biological sequence
kernels with guaranteed flexibility. arXiv preprint arXiv:2304.03775 , 2023.
[32] Emmi Jokinen, Markus Heinonen, and Harri Lähdesmäki. mGPfusion: Predicting protein
stability changes with Gaussian process kernel learning and data fusion. Bioinformatics ,
34(13):i274–i283, July 2018.
[33] Andrew Leaver-Fay, Michael Tyka, Steven M. Lewis, Oliver F. Lange, James Thompson,
Ron Jacak, Kristian W. Kaufman, P. Douglas Renfrew, Colin A. Smith, Will Sheffler, et al.
ROSETTA3: an object-oriented software suite for the simulation and design of macromolecules.
InMethods in enzymology , volume 487, pages 545–574. Elsevier, 2011.
[34] Jonathan Parkinson and Wei Wang. Linear-scaling kernels for protein sequences and small
molecules outperform deep learning while providing uncertainty quantitation and improved
interpretability. Journal of Chemical Information and Modeling , 63(15):4589–4601, 2023.
[35] Haoyang Zeng and David K Gifford. Quantification of Uncertainty in Peptide-MHC Binding
Prediction Improves High-Affinity Peptide Selection for Therapeutic Design. Cell systems ,
9(2):159–166, 2019.
[36] Brian Hie, Bryan D. Bryson, and Bonnie Berger. Leveraging Uncertainty in Machine Learning
Accelerates Biological Discovery and Design. Cell Systems , 11(5):461–477.e9, November
2020.
[37] Hunter Nisonoff, Yixin Wang, and Jennifer Listgarten. Coherent Blending of Biophysics-Based
Knowledge with Bayesian Neural Networks for Robust Protein Property Prediction. ACS
Synthetic Biology , 12(11):3242–3251, 2023.
[38] Young Su Ko, Jonathan Parkinson, Cong Liu, and Wei Wang. TUnA: an uncertainty-aware
transformer model for sequence-based protein–protein interaction prediction. Briefings in
Bioinformatics , 25(5):bbae359, 2024.
[39] Jeremiah Liu, Zi Lin, Shreyas Padhy, Dustin Tran, Tania Bedrax Weiss, and Balaji Lakshmi-
narayanan. Simple and Principled Uncertainty Estimation with Deterministic Deep Learning
via Distance Awareness. Advances in neural information processing systems , 33:7498–7512,
2020.
[40] Fredrik K. Gustafsson, Martin Danelljan, and Thomas B. Schon. Evaluating Scalable Bayesian
Deep Learning Methods for Robust Computer Vision. In Proceedings of the IEEE/CVF
conference on computer vision and pattern recognition workshops , pages 318–319, 2020.
[41] Gabriele Scalia, Colin A. Grambow, Barbara Pernici, Yi-Pei Li, and William H. Green. Evalu-
ating Scalable Uncertainty Estimation Methods for Deep Learning-Based Molecular Property
Prediction. ACS, 2020.
13[42] Dan Levi, Liran Gispan, Niv Giladi, and Ethan Fetaya. Evaluating and Calibrating Uncertainty
Prediction in Regression Tasks. Sensors , 22(15):5540, 2022.
[43] Lior Hirschfeld, Kyle Swanson, Kevin Yang, Regina Barzilay, and Connor W Coley. Uncertainty
Quantification Using Neural Networks for Molecular Property Prediction. Journal of Chemical
Information and Modeling , 60(8):3770–3780, 2020.
[44] Kevin Tran, Willie Neiswanger, Junwoong Yoon, Qingyang Zhang, Eric Xing, and Zachary W
Ulissi. Methods for comparing uncertainty quantifications for material property predictions.
Machine Learning: Science and Technology , 1(2):025006, 2020.
[45] Kevin P. Greenman, Ava P. Amini, and Kevin K. Yang. Benchmarking uncertainty quantification
for protein engineering. bioRxiv , pages 2023–04, 2023.
[46] Yinghao Li, Lingkai Kong, Yuanqi Du, Yue Yu, Yuchen Zhuang, Wenhao Mu, and Chao Zhang.
MUBen: Benchmarking the uncertainty of molecular representation models. Transactions on
Machine Learning Research , 2024.
[47] Stephan Thaler, Felix Mayr, Siby Thomas, Alessio Gagliardi, and Julija Zavadlav. Active
learning graph neural networks for partial charge prediction of metal-organic frameworks via
dropout Monte Carlo. npj Computational Materials , 10(1):86, 2024.
[48] Yarin Gal and Zoubin Ghahramani. Dropout as a Bayesian Approximation: Representing
Model Uncertainty in Deep Learning. In Proceedings of The 33rd International Conference on
Machine Learning , volume 48 of Proceedings of Machine Learning Research , pages 1050–1059,
New York, New York, USA, 20–22 Jun 2016. PMLR.
[49] Richard Michael, Jacob Kæstel-Hansen, Peter Mørch Groth, Simon Bartels, Jesper Salomon,
Pengfei Tian, Nikos S. Hatzakis, and Wouter Boomsma. A Systematic Analysis of Regression
Models for Protein Engineering. PLOS Computational Biology , 20(5):e1012061, May 2024.
[50] John Ingraham, Vikas Garg, Regina Barzilay, and Tommi Jaakkola. Generative models for
graph-based protein design. Advances in neural information processing systems , 32, 2019.
[51] Chloe Hsu, Robert Verkuil, Jason Liu, Zeming Lin, Brian Hie, Tom Sercu, Adam Lerer,
and Alexander Rives. Learning Inverse Folding from Millions of Predicted Structures. In
Proceedings of the 39th International Conference on Machine Learning , pages 8946–8970.
PMLR, June 2022.
[52] Justas Dauparas, Ivan Anishchenko, Nathaniel Bennett, Hua Bai, Robert J. Ragotte, Lukas F.
Milles, Basile I. M. Wicky, Alexis Courbet, Rob J. de Haas, Neville Bethel, et al. Robust deep
learning–based protein sequence design using ProteinMPNN. Science , 378(6615):49–56, 2022.
[53] Zhangyang Gao, Cheng Tan, and Stan Z. Li. PiFold: Toward effective and efficient protein
inverse folding. In The Eleventh International Conference on Learning Representations , 2023.
[54] Zhangyang Gao, Cheng Tan, Xingran Chen, Yijie Zhang, Jun Xia, Siyuan Li, and Stan Z. Li.
KW-Design: Pushing the Limit of Protein Design via Knowledge Refinement. In The Twelfth
International Conference on Learning Representations , 2024.
[55] Xinyi Zhou, Guangyong Chen, Junjie Ye, Ercheng Wang, Jun Zhang, Cong Mao, Zhanwei Li,
Jianye Hao, Xingxu Huang, Jin Tang, and Pheng Ann Heng. ProRefiner: an entropy-based
refining strategy for inverse protein folding with global graph attention. Nature Communications ,
14(1):7434, 2023.
[56] Milong Ren, Chungong Yu, Dongbo Bu, and Haicang Zhang. Accurate and robust protein
sequence design with CarbonDesign. Nature Machine Intelligence , 6(5):536–547, 2024.
[57] Wen Torng and Russ B. Altman. 3D deep convolutional neural networks for amino acid
environment similarity analysis. BMC bioinformatics , 18:1–23, 2017.
[58] Pablo Gainza, Freyr Sverrisson, Frederico Monti, Emanuele Rodola, Davide Boscaini,
Michael M Bronstein, and Bruno E Correia. Deciphering interaction fingerprints from protein
molecular surfaces using geometric deep learning. Nature Methods , 17(2):184–192, 2020.
14[59] Pablo Gainza, Sarah Wehrle, Alexandra Van Hall-Beauvais, Anthony Marchand, Andreas
Scheck, Zander Harteveld, Stephen Buckley, Dongchun Ni, Shuguang Tan, Freyr Sverris-
son, Casper Goverde, Priscilla Turelli, Charlène Raclot, Alexandra Teslenko, Martin Pacesa,
Stéphane Rosset, Sandrine Georgeon, Jane Marsden, Aaron Petruzzella, Kefang Liu, Zepeng
Xu, Yan Chai, Pu Han, George F. Gao, Elisa Oricchio, Beat Fierz, Didier Trono, Henning
Stahlberg, Michael Bronstein, and Bruno E. Correia. De novo design of protein interactions
with learned surface fingerprints. Nature , 617(7959):176–184, May 2023.
[60] Raghav Shroff, Austin W. Cole, Daniel J. Diaz, Barrett R. Morrow, Isaac Donnell, Ankur
Annapareddy, Jimmy Gollihar, Andrew D. Ellington, and Ross Thyer. Discovery of Novel
Gain-of-Function Mutations Guided by Structure-Based Deep Learning. ACS synthetic biology ,
9(11):2927–2935, 2020.
[61] Anastasiya V . Kulikova, Daniel J. Diaz, James M. Loy, Andrew D. Ellington, and Claus O.
Wilke. Learning the local landscape of protein structures with convolutional neural networks.
Journal of Biological Physics , 47(4):435–454, 2021.
[62] Zsolt Fazekas, Dóra K. Menyhárd, and András Perczel. LoCoHD: a metric for comparing local
environments of proteins. Nature Communications , 15(1):4029, 2024.
[63] David Ding, Ada Y . Shaw, Sam Sinai, Nathan Rollins, Noam Prywes, David F. Savage,
Michael T. Laub, and Debora S. Marks. Protein design using structure-based residue preferences.
Nature Communications , 15(1):1639, February 2024.
[64] John Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ron-
neberger, Kathryn Tunyasuvunakool, Russ Bates, Augustin Žídek, Anna Potapenko, Alex
Bridgland, Clemens Meyer, Simon A. A. Kohl, Andrew J. Ballard, Andrew Cowie, Bernardino
Romera-Paredes, Stanislav Nikolov, Rishub Jain, Jonas Adler, Trevor Back, Stig Petersen,
David Reiman, Ellen Clancy, Michal Zielinski, Martin Steinegger, Michalina Pacholska, Tamas
Berghammer, Sebastian Bodenstein, David Silver, Oriol Vinyals, Andrew W. Senior, Koray
Kavukcuoglu, Pushmeet Kohli, and Demis Hassabis. Highly accurate protein structure predic-
tion with AlphaFold. Nature , 596(7873):583–589, August 2021.
[65] Carl Edward Rasmussen and Christopher K. I. Williams. Gaussian Processes for Machine
Learning . Adaptive Computation and Machine Learning. MIT Press, Cambridge, Mass, 2006.
[66] Richard Michael, Simon Bartels, Miguel González-Duque, Yevgen Zainchkovskyy, Jes Frellsen,
Søren Hauberg, and Wouter Boomsma. A Continuous Relaxation for Discrete Bayesian
Optimization, 2024.
[67] Brian Hie, Bryan D. Bryson, and Bonnie Berger. Leveraging Uncertainty in Machine Learning
Accelerates Biological Discovery and Design. Cell systems , 11(5):461–477, 2020.
[68] Morris H. DeGroot and Stephen E. Fienberg. The Comparison and Evaluation of Forecasters.
Journal of the Royal Statistical Society. Series D (The Statistician) , 32(1/2):12–22, 1983.
[69] Michael A. Stiffler, Doeke R. Hekstra, and Rama Ranganathan. Evolvability as a Function of
Purifying Selection in TEM-1 β-Lactamase. Cell, 160(5):882–892, February 2015.
[70] Nicholas C. Wu, C. Anders Olson, Yushen Du, Shuai Le, Kevin Tran, Roland Remenyi, Danyang
Gong, Laith Q. Al-Mawsawi, Hangfei Qi, Ting-Ting Wu, and Ren Sun. Functional Constraint
Profiling of a Viral Protein Reveals Discordance of Evolutionary Conservation and Functionality.
PLOS Genetics , 11(7):e1005310, July 2015.
[71] Aliete Wan, Emily Place, Eric A. Pierce, and Jason Comander. Characterizing Variants of
Unknown Significance in Rhodopsin: A Functional Genomics Approach. Human Mutation ,
40(8):1127–1144, August 2019.
[72] Edward Snelson and Zoubin Ghahramani. Sparse Gaussian Processes using Pseudo-inputs.
Advances in neural information processing systems , 18, 2005.
[73] Ke Wang, Geoff Pleiss, Jacob Gardner, Stephen Tyree, Kilian Q Weinberger, and Andrew Gor-
don Wilson. Exact Gaussian Processes on a Million Data Points. Advances in neural information
processing systems , 32, 2019.
15[74] Giacomo Meanti, Luigi Carratino, Lorenzo Rosasco, and Alessandro Rudi. Kernel Methods
Through the Roof: Handling Billions of Points Efficiently. In Advances in Neural Information
Processing Systems , volume 33, pages 14410–14422. Curran Associates, Inc., 2020.
[75] Imre Csiszár and Paul C. Shields. Information theory and statistics: A tutorial. Foundations
and Trends® in Communications and Information Theory , 1(4):417–528, 2004.
[76] Nicki Skafte Detlefsen, Søren Hauberg, and Wouter Boomsma. Learning meaningful represen-
tations of protein sequences. Nature Communications , 13(1):1914, April 2022.
[77] Thomas Gärtner, Peter A. Flach, Adam Kowalczyk, and Alex J. Smola. Multi-instance kernels.
InProceedings of the Nineteenth International Conference on Machine Learning , ICML ’02,
page 179–186, San Francisco, CA, USA, 2002. Morgan Kaufmann Publishers Inc.
[78] David Duvenaud. Automatic Model Construction with Gaussian Processes . PhD thesis,
Computational and Biological Learning Laboratory, University of Cambridge, 2014.
[79] Jacob Gardner, Geoff Pleiss, Kilian Q. Weinberger, David Bindel, and Andrew G. Wilson. GPy-
Torch: Blackbox Matrix-Matrix Gaussian Process Inference with GPU Acceleration. Advances
in neural information processing systems , 31, 2018.
[80] Nicholas G. Polson and James G. Scott. On the Half-Cauchy Prior for a Global Scale Parameter.
Bayesian Analysis , 7(4):887–902, 2012.
[81] Ilya Loshchilov and Frank Hutter. Decoupled Weight Decay Regularization, 2019.
16Appendix
A License and code availability
The codebase is publicly available at https://github.com/petergroth/kermut under the open
source MIT License.
B GP details
B.1 Structure kernel
The structure kernel is comprised of three components, each increasing model flexibility. The site-
comparison kernel, kH, compares site-specific, structure-conditioned amino acid distributions. Given
two such discrete probability distributions, p:=fIF(x)andq:=fIF(x′), their distance is quantified
via the Hellinger distance [75]
dH(p, q) =1√
2vuut20X
i=1(√pi−√qi)2,
which is used in the Hellinger kernel [66]
kH(x,x′) = exp ( −γ1dH(p, q)) = exp
−γ11√
2vuut20X
i=1(√pi−√qi)2
.
For the mutation probability kernel, kp, we use the log-probabilities rather than the amino acid
identities to reflect that amino acids with similar probability on sites with similar distributions should
have similar biochemical effects on the protein. We do not include the log-probabilities of the wild
type amino acids, as the inverse folding model by definition is trained to assign high probabilities to
the wild type sequence. The exact probability of a wild type amino acid depends on how many other
amino acids that are likely to be at the given site. For example, we would expect a probability close
to one for a functionally critical amino acid at a particular site. Conversely, for a less critical surface-
level residue requiring, e.g., a polar uncharged amino acid, we would expect similar probabilities for
the four amino acids of this type. Thus, variations of log-probabilities of the wild type amino acids
should be reflected in the distribution on the sites captured by the Hellinger kernel.
For the per-residue amino acid distributions in kHandkp, we use ProteinMPNN [ 52]. ProteinMPNN
relies on a random decoding order and thus benefits from multiple samples. We decode the wild type
amino acid sequence a total of ten times while conditioning on the full structure and the sequence
that has been decoded thus far. We then compute a per-residue average distribution. We use the
v_48_020 weights.
For the distance kernel, kd, we calculate the Euclidean distance between αcarbon atoms, where
the unit is in Ångstrøm. All wild type structures used in kdand by ProteinMPNN are predicted via
AlphaFold2 [64] and are provided by ProteinGym [7].
The sum over all pairs of mutations in equation 2 is motivated by [ 63] who showed that a linear
model was sufficient to effectively model the mutation effects. Note that Cov(Y1+Y2, Y3+Y4) =
Cov(Y1, Y3) +Cov(Y1, Y4) +Cov(Y2, Y3) +Cov(Y2, Y4). Hence, if the Yi’s are the effects of single
mutations, we can find the covariance between two double mutant variants by calculating the pairwise
sum and similarly for other number of mutations.
B.2 Sequence kernel
For the sequence kernel, we use embeddings extracted from the ESM-2 protein language model [ 16].
We use the esm2_t36_650M_UR50D model with 650M parameters. The embeddings are mean-pooled
across the sequence dimension such that each variant is represented by a 1280 dimensional vector.
While it has been shown that other aggregation method can lead to large increases in performance
[76], we consider alternate methods such as training a bottleneck model out of the scope of this paper.
17B.3 Parametrization note
An alternative formulation of the kernel, where we omit λfrom Equation (1) and πfrom Equation (4),
and instead provide the structure and sequence kernels with separate coefficients have shown to
provide close to identical results when using a smoothed box prior (constrained between 0 and 1).
While this approach is somewhat more elegant, this does not justify the re-computation of all results.
B.4 Zero-shot mean function
For the zero-shot mean function, we download and use the pre-computed zero-shot scores from
ProteinGym at https://github.com/OATML-Markslab/ProteinGym . The zero-shot value is
calculated as the log-likelihood ratio between the variant and the wild type at the mutated residue as
described in [10]:
f0(x) =X
i∈Mlogp(xi)−logp(xWT
i)
The values can be computed straightforwardly using the ESM suite (using the masked-marginals
strategy). For multi-mutants, the sum of the ratios is taken.
B.5 Kernel proof
We will in the following argue why Kermut’s structure kernel is a valid kernel. Recall that a function
k:X × X → Ris a kernel if and only if the matrix K, where Kij=k(xi, xj), is symmetric
positive semi-definite [ 65]. From literature we know a number of kernels and certain ways these
can be combined to create new kernels. We will argue that Kermut is a kernel by showing how it is
composed of known kernels, combined using valid methods.
Note that, if we have a mapping f:X → Z and a kernel kZ, onZ, then kXdefined by kX(x, x′) :=
kZ(f(x), f(x′))is a kernel on X.
LetXbe the space of sequences parameterized with respect to a reference sequence. Let f1:X → Z
be a transformation of the sequences defined as in Section 3.3. kseqis the squared exponential kernel
on the transformed variants. Hence, kseqis is a kernel on X.
LetX1⊂ X denote the subspace of single mutant variants and f2:X1→R3be a function mapping
single mutant variants into the 3D coordinates of the α-carbon of the particular mutation. kdis the
exponential kernel on this transformed space. Thus kdis a kernel on X1.
LetfIF:X1→ G ⊆ [0,1]20be defined as in Section 3.3, where Gis the space of probability distribu-
tions over the 20 amino acids. fIF(x)is the probability distribution over the mutated site of xgiven
by an inverse folding model. kHis the Hellinger kernel on the single mutation variants transformed
byfIF, hence, a valid kernel on X1. Likewise kpis the exponential kernel of a transformation,
fIF1:X1→[0,1]as defined in Section 3.3, of the sequences, hence also a kernel.
Scaling, multiplying, and adding kernels result in new kernels, making k1
structa valid kernel for single
mutations [65]. We need to show that kstructand thereby kis valid for any number of mutations.
Letf4:X → B be a function taking a variant xwith Mmutations and mapping it to a set
b={xm}m∈Mof all the single mutations which constitutes x. Define the set kernel [77]
kset(b, b′) :=X
xm∈b,x′m∈b′λk1
struct(xm,x′m)
kstructis the set kernel on the transformed input and thus also a kernel. We have thereby shown that
Kermut is a kernel for variants with any number of mutations.
B.6 Automatic Model Selection
Kermut has been developed from the ground up as described Section 3.3, which led to separate
structure and sequence kernels which are added together. Alternatively, an automatic model selection
scheme can be employed for data-driven kernel composition as proposed in Chapter 3 in [ 78]. For
demonstrative purposes, we conduct such a model selection procedure. We define four base kernels
18(kH,kp,kd,kseq) as well as sum and product operations. We choose a subset of 17 ProteinGym
assays and fit a GP (with a zero-shot mean function) and each of the four kernels, where the best
performing kernel across splits is kept. For the second round, the remaining three kernels are either
added or multiplied to the existing kernel. This process is continued until either all four kernels are
used or until the test performance no longer increases. The results can be seen in Table B.1, where
the final kernel is the product of the four base kernels. The results on the 174 ablation datasets using
this kernel (denoted as "Kermut (product)") can be seen with the main Kermut GP in Table F.2. The
ECE/ENCE values for Kermut (product) are 0.060/0.192 vs. 0.051/0.170 for the main formulation.
Due to the potential for interpretability of the sum-formulation as well as slightly better calibration,
the main formulation of the Kermut GP remains as in Equation (4).
Table B.1: Automatic Model Selection with four base kernels and sum/product operations. The
resulting kernel is the product of all base kernels.
Kernel Round 1 Round 2 Round 3 Round 4
kseq 0.555
kd 0.589
kH 0.584
kp 0.521
kd×kseq 0.653
kd+kseq 0.642
kd×kH 0.623
kd+kH 0.622
kd×kp 0.633
kd+kp 0.629
kd×kseq×kH 0.666
kd×kseq+kH 0.657
kd×kseq×kp 0.678
kd×kseq+kp 0.666
kd×kseq×kp×kH 0.684
kd×kseq×kp+kH 0.676
C Implementation details
C.1 General details
We build our kernel using the GPyTorch framework [ 79]. We assume a homoschedastic Gaussian
noise model, on which we place a HalfCauchy prior [ 80] with scale 0.1. We fit the hyperparameters
by maximizing the exact marginal likelihood with gradient descent using the AdamW optimizer [ 81]
with learning rate 0.1 for a 150 steps, which proved to be sufficient for convergence for a number of
sampled datasets.
C.2 System details
All experiments are performed on a Linux-based cluster running Ubuntu 20.04.4 LTS, with a AMD
EPYC 7642 48-Core Processor with 192 threads and 1TB RAM. NVIDIA A40s were used for GPU
acceleration both for fitting the Gaussian processes and for generating the protein embeddings.
C.3 Compute time
There are multiple factors to consider when evaluating the training time of Kermut. A limitation of
using a Gaussian process framework is the cubic scaling with dataset size. For this reason, we ran the
ablation study on only 174 of the 217 datasets.
Training Kermut on these datasets for a single split-scheme using the aforementioned hardware (single
GPU) takes approximately 1 hour and 30 minutes. Getting ablation results for all three schemes thus
takes between 4-5 hours. This however assumes that
191. the embeddings for the sequence kernel have been precomputed,
2. the probability distribution for all sites in the wild type protein have been precomputed,
3. and that the zero-shot scores have been precomputed.
We do not see any of these as major limitations as the same applies to ProteinNPT and similar models.
Scaling the experiments to the full ProteinGym benchmark is however costly. We were able to
train/evaluate Kermut on 215/217 datasets using an NVIDIA A40 GPU with 48GB VRAM. The re-
maining two, POLG_CXB3N_Mattenberger_2021 andPOLG_DEN26_Suphatrakul_2023 , datasets
were too large to fit into GPU memory without resorting to reduced precision. For these, we trained
and evaluated the model using CPU only which takes considerable time.
C.4 Handling long sequences
The structures used for obtaining the site-wise probability distributions are predicted by Al-
phaFold2 [ 64] and are provided in the ProteinGym repository [ 7]. The provided structures for
A0A140D2T1_ZIKV andPOLG_HCVJF do not contain the full structures however, but only a localized
area where the mutations occur due to the long sequence lengths. Since our model only operates on
sites with mutations, this is not an issue for neither the inverse-folding probability distributions nor
the inter-residue distances.
ForBRCA2_HUMAN , three PDB files are provided due to the long wild type sequence length. We use
ProteinMPNN to obtain the distributions at all sites in each PDB file and stitch them together in a
preprocessing step. Calculating the inter-residue distances is however non-trivial and would require a
careful alignment of the three structures. Instead, we drop the distance term, kd, in the kernel for the
BRCA2_HUMAN_Erwood_2022_HEK293T dataset (equivalently setting it to one: kd(x,x′) = 1 ).
The sequence kernel operates on ESM-2 embeddings. The ESM-2 model has a maximum sequence
length of 1022 amino acids. Protein sequences that are longer than this limit are truncated.
C.4.1 Example wall-clock time
The wall-clock times for generating the results used for the calibration curves in Figures 3a and 3b
and Appendix L.2 for one split scheme can be seen in Table C.1 for Kermut and ProteinNPT. The
experiments were carried out using identical hardware. The test system is however a shared compute
cluster with sharded GPUs, so variance is expected between runs. Generating the full results for
the figures thus takes Kermut approximately 10 minutes while ProteinNPT takes 400 hours. This
shows the significant reduction in computational burden that Kermut allows for. Both ProteinNPT
and Kermut assumes that sequence embeddings are available a priori.
Table C.1: Approximate wall clock times for training and evaluating Kermut and ProteinNPT for a
single split scheme, i.e., by using 5-fold cross validation. While the runtime of Kermut scales with
dataset size, ProteinNPT appears to scale more strongly with sequence length due to the tri-axial
attention.
Dataset Kermut runtime PNPT runtime N L
BLAT_ECOLX 111s ≈32h 4996 286
PA_I34A1 45s ≈52h 1820 716
TCRG1_MOUSE 19s ≈22h 621 37
OPSD_HUMAN 14s ≈40h 165 348
C.5 Computational complexity
Evaluating the kernel for two variants xandx′, involves computation of the two components kseq
andkstruct:
For each variant kseqrequires a forward pass through ESM-2, which is based on the transformer
architecture and has quadratic scaling with respect to the sequence length. Given the ESM-2
embeddings, the computational complexity is constant in sequence length and number of mutations.
20kstruct requires a single forward pass through ProteinMPNN for the wild type protein. Given the
output of ProteinPMNN, the computational complexity for evaluation of the kernel is m1×m2
for two proteins with m1andm2number of mutations. The computational complexity of k1
structis
constant in sequence length and number of mutations.
D Data
All data and evaluation software is accessed via the ProteinGym [ 7] repository at https://github.
com/OATML-Markslab/ProteinGym which is under the MIT License.
E Detailed results
The ProteinGym suite provides an aggregation procedure, whereby the predictive performance across
both cross-validation schemes and functional categories can be gauged. We provide these results
in Tables E.1 to E.4. We mirror the label normalization from [ 19] and [ 7], where, for each fold of
cross-validation, train and test labels are normalized given the mean and standard deviation of the
training labels. The Spearman correlation coefficient and MSE are then computed in the normalized
space across folds, leading to a single Spearman coefficient and MSE per assay per split (i.e., not as
an average of metrics per CV-fold).
Table E.1: Aggregated Spearman results on the ProteinGym substitution benchmark. Performance
is shown per cross-validation scheme. Kermut reaches superior performance across the board. The
fifth data column shows the non-parametric bootstrap standard error of the difference between the
Spearman performance for each model and Kermut, computed over 10,000 bootstrap samples from
the set of proteins in the ProteinGym substitution benchmark.
Model name Spearman per scheme (↑) Std. err.
Cont. Mod. Rand. Avg.
Kermut 0.610 0.633 0.744 0.662 0.000
ProteinNPT 0.547 0.564 0.730 0.613 0.009
Tranception Emb. 0.490 0.526 0.696 0.571 0.008
MSAT Emb. 0.525 0.538 0.642 0.568 0.013
ESM-1v Emb. 0.481 0.506 0.639 0.542 0.011
TranceptEVE + OHE 0.441 0.440 0.550 0.477 0.012
Tranception + OHE 0.419 0.419 0.535 0.458 0.012
MSAT + OHE 0.410 0.412 0.536 0.453 0.014
DeepSequence + OHE 0.400 0.400 0.521 0.440 0.016
ESM-1v + OHE 0.367 0.368 0.514 0.417 0.014
OHE 0.064 0.027 0.579 0.224 0.014
Table E.2: Aggregated Spearman results on the ProteinGym substitution benchmark. Performance is
shown per functional category. Kermut reaches superior performance across the board.
Model name Spearman per function (↑)
Activity Binding Expression Fitness Stability
Kermut 0.606 0.630 0.672 0.581 0.824
ProteinNPT 0.577 0.536 0.637 0.545 0.772
Tranception Emb. 0.520 0.529 0.613 0.519 0.674
MSAT Emb. 0.547 0.470 0.584 0.493 0.749
ESM-1v Emb. 0.487 0.450 0.587 0.468 0.717
TranceptEVE + OHE 0.502 0.444 0.476 0.470 0.493
Tranception + OHE 0.475 0.416 0.476 0.448 0.473
MSAT + OHE 0.480 0.393 0.463 0.437 0.491
DeepSequence + OHE 0.467 0.418 0.424 0.422 0.471
ESM-1v + OHE 0.421 0.363 0.452 0.383 0.463
OHE 0.213 0.212 0.226 0.194 0.273
21Table E.3: Aggregated MSE results on the ProteinGym substitution benchmark. Performance is
shown per cross-validation scheme. Kermut reaches superior performance across the board. The fifth
data column shows the non-parametric bootstrap standard error of the difference between the MSE
performance for each model and Kermut, computed over 10,000 bootstrap samples from the set of
proteins in the ProteinGym substitution benchmark.
Model name MSE per scheme (↓) Std. err.
Cont. Mod. Rand. Avg.
Kermut 0.699 0.652 0.414 0.589 0.000
ProteinNPT 0.820 0.771 0.459 0.683 0.017
MSAT Emb. 0.836 0.795 0.573 0.735 0.021
Tranception Emb. 0.972 0.833 0.503 0.769 0.023
ESM-1v Emb. 0.937 0.861 0.563 0.787 0.030
TranceptEVE + OHE 0.953 0.914 0.743 0.870 0.019
MSAT + OHE 0.963 0.934 0.749 0.882 0.020
DeepSequence + OHE 0.967 0.940 0.767 0.891 0.017
Tranception + OHE 0.985 0.934 0.766 0.895 0.022
ESM-1v + OHE 0.977 0.949 0.764 0.897 0.013
OHE 1.158 1.125 0.898 1.061 0.017
Table E.4: Aggregated results on the ProteinGym substitution benchmark. Performance is shown per
functional category. Kermut reaches superior performance across the board.
Model name MSE per function (↓)
Activity Binding Expression Fitness Stability
Kermut 0.630 0.843 0.523 0.657 0.289
ProteinNPT 0.703 1.016 0.578 0.752 0.368
MSAT Emb. 0.728 1.092 0.660 0.789 0.405
Tranception Emb. 0.814 1.080 0.639 0.788 0.525
ESM-1v Emb. 0.799 1.231 0.655 0.792 0.456
TranceptEVE + OHE 0.793 1.199 0.780 0.825 0.756
MSAT + OHE 0.810 1.221 0.788 0.836 0.756
DeepSequence + OHE 0.830 1.140 0.832 0.860 0.793
Tranception + OHE 0.831 1.246 0.787 0.845 0.765
ESM-1v + OHE 0.843 1.192 0.795 0.870 0.783
OHE 1.022 1.306 0.986 1.040 0.949
E.1 Results on new splits
The modulo and contiguous splits in ProteinGym were updated in April 2024. For completeness, we
here provide results using Kermut on the updated splits for all 217 DMS assays. These can be seen in
Table E.5, where we observe a slight decrease in performance for the contiguous and modulo splits,
compared to the reference results in Table 1.
Table E.5: Performance on the ProteinGym benchmark using the corrected splits.
Model Model name Spearman (↑) MSE (↓)
type Contig. Mod. Rand. Avg. Contig. Mod. Rand. Avg.
GP Kermut 0.591 0.631 0.744 0.655 0.739 0.680 0.141 0.611
22F Ablation results
In Section 4.1, an ablation study was carried out by removing components of Kermut. In Table 2, the
performance difference in Spearman correlation was shown. In Table F.1 we see the performance
difference in MSE. The aggregated absolute Spearman and MSE values are shown in Tables F.2
and F.3, while we show the performance per functional category in Tables F.4 and F.5. These results
suggest that an even wider combinatorial examination of Kermut’s kernel composition might lead to
slightly increased performance, e.g., by multiplying a Matérn 5/2 sequence kernel with the structure
kernel. We must however note that the standard errors in Tables F.2 and F.3 suggest that while the
product and Matérn configurations lead to better results on the ablation datasets, the differences are
not significant.
In addition to the shown methods, we considered sequence-only kernels as a baselines. One example
is the inverse-multiquadratic Hamming (IMQ-H) kernel from [ 31]. This kernel, however, relies on
the Hamming distance between one-hot encoded sequences for its covariances. For ProteinGym’s
single-mutant benchmark, this is not sufficient as the Hamming distance between all variants is 2,
resulting in constant predictions for all folds and subsequent Spearman correlations narrowly centered
on 0.
Table F.1: Ablation results. Key components of the kernel are removed and the model is trained
and evaluated on 174/217 assays from the ProteinGym benchmark. The ablation column shows the
alteration to the GP formulation. The metrics are subtracted from Kermut to show the change in
performance. Positive ∆MSE values indicate drop in performance.
Description Ablation ∆MSE
Contig. Mod. Rand. Avg.
No structure kernel kstruct = 0 0.096 0 .087 0 .046 0 .076
No sequence kernel kseq= 0 0.060 0 .059 0 .077 0 .065
No inter-residue dist. kd= 1 0.058 0 .060 0 .011 0 .043
No mut. prob./site comp. kp=kH= 1 0.046 0 .040 0 .024 0 .036
Const. mean m(x) =α 0.042 0 .036 0 .015 0 .030
No mut. prob. kp= 1 0.032 0 .021 0 .022 0 .024
No site comp. kH= 1 0.005 0 .009 0 .008 0 .006
Table F.2: Ablation results. Key components of the kernel are removed or altered and the model
is trained and evaluated on 174/217 assays from the ProteinGym benchmark. The ablation column
shows the alteration to the kernel formulation.
Description Ablation Spearman (↑) Std. err.
Contig. Mod. Rand. Avg.
No structure kernel kstruct = 0 0.523 0.550 0.710 0.594 0.009
No sequence kernel kseq= 0 0.565 0.582 0.695 0.614 0.006
No inter-residue dist. kd= 1 0.556 0.577 0.739 0.624 0.004
No mut. prob./site comp. kp=kH= 1 0.570 0.591 0.737 0.633 0.006
Const. mean m(x) =α 0.569 0.596 0.735 0.633 0.007
No mut. prob. kp= 1 0.583 0.609 0.738 0.643 0.004
No site comp. kH= 1 0.599 0.619 0.743 0.653 0.002
Kermut (SE in kH) kH=kSE 0.598 0.621 0.745 0.655 0.002
Kermut (JSD in kH) kH=kJSD 0.604 0.626 0.745 0.658 0.001
Kermut (Matérn in kseq) kseq=kMatérn5/2 0.608 0.629 0.746 0.661 0.002
Kermut (product) k=kstruct×kseq 0.606 0.632 0.750 0.662 0.003
Kermut 0.605 0.628 0.743 0.659 0.000
23Table F.3: Ablation results. Key components of the kernel are removed or altered and the model
is trained and evaluated on 174/217 assays from the ProteinGym benchmark. The ablation column
shows the alteration to the kernel formulation.
Description Ablation MSE (↓) Std. err.
Contig. Mod. Rand. Avg.
No structure kernel kstruct = 0 0.825 0.769 0.460 0.684 0.012
No sequence kernel kseq= 0 0.791 0.744 0.492 0.676 0.008
No inter-residue dist. kd= 1 0.789 0.743 0.426 0.652 0.006
No mut. prob./site comp. kp=kH= 1 0.775 0.722 0.436 0.644 0.007
Const. mean m(x) =α 0.770 0.718 0.429 0.639 0.005
No mut. prob. kp= 1 0.761 0.704 0.436 0.634 0.006
No site comp. kH= 1 0.735 0.691 0.421 0.616 0.002
Kermut (SE in kH) kH=kSE 0.738 0.690 0.416 0.614 0.003
Kermut (JSD in kH) kH=kJSD 0.731 0.684 0.415 0.610 0.002
Kermut (Matérn in kseq) kseq=kMatérn5/2 0.724 0.678 0.414 0.605 0.002
Kermut (product) k=kstruct×kseq 0.726 0.678 0.411 0.605 0.004
Kermut 0.730 0.683 0.420 0.611 0.000
Table F.4: Ablation results. Key components of the kernel are removed or altered and the model
is trained and evaluated on 174/217 assays from the ProteinGym benchmark. The ablation column
shows the alteration to the kernel formulation. Performance is shown per functional category.
Model name Ablation Spearman per function (↑)
Activity Binding Expression Fitness Stability
Const. mean m(x) =α 0.579 0.580 0.651 0.536 0.821
No site comp. kH= 1 0.590 0.619 0.664 0.574 0.820
No mut. prob. kp= 1 0.590 0.611 0.651 0.564 0.801
No mut. prob./site comp. kp=kH= 1 0.569 0.601 0.648 0.557 0.790
No inter-residue dist. kd= 1 0.562 0.560 0.634 0.546 0.818
No sequence kernel kseq= 0 0.578 0.616 0.611 0.547 0.716
No structure kernel kstruct = 0 0.531 0.529 0.614 0.519 0.778
Kermut (Matérn in kseq) kseq=kMatérn5/2 0.604 0.625 0.667 0.581 0.828
Kermut (SE in kH) kH=kSE 0.593 0.623 0.664 0.576 0.818
Kermut (JSD in kH) kH=kJSD 0.599 0.628 0.664 0.578 0.823
Kermut (product) k=kstruct×kseq 0.599 0.632 0.674 0.578 0.829
Kermut 0.602 0.625 0.665 0.578 0.824
Table F.5: Ablation results. Key components of the kernel are removed or altered and the model
is trained and evaluated on 174/217 assays from the ProteinGym benchmark. The ablation column
shows the alteration to the kernel formulation. Performance is shown per functional category.
Model name Ablation MSE per function (↓)
Activity Binding Expression Fitness Stability
Const. mean m(x) =α 0.672 0.932 0.575 0.710 0.304
No site comp. kH= 1 0.651 0.906 0.557 0.669 0.295
No mut. prob. kp= 1 0.651 0.923 0.584 0.682 0.327
No mut. prob./site comp. kp=kH= 1 0.672 0.933 0.586 0.688 0.343
No inter-residue dist. kd= 1 0.696 0.965 0.599 0.702 0.301
No sequence kernel kseq= 0 0.673 0.926 0.622 0.718 0.440
No structure kernel kstruct = 0 0.712 0.993 0.626 0.734 0.358
Kermut (Matérn in kseq) kseq=kMatérn5/2 0.636 0.891 0.555 0.662 0.283
Kermut (SE in kH) kH=kSE 0.648 0.899 0.558 0.669 0.299
Kermut (JSD in kH) kH=kJSD 0.642 0.893 0.558 0.667 0.291
Kermut (product) k=kstruct×kseq 0.643 0.884 0.548 0.667 0.283
Kermut 0.640 0.903 0.558 0.665 0.289
24G Results for multi-mutants in ProteinGym
69 of the datasets from the ProteinGym benchmark include multi-mutants. In addition to the
random, modulo, and contiguous split, these also have a fold_rand_multiples split. We here
show the results for Kermut in this setting. Of the 69 datasets, we select 52 which (due to the
cubic scaling of fitting GPs) include fewer than 7500 variant sequences. We additionally ignore
theGCN4_YEAST_Staller_2018 dataset which has a very large number of mutations. This leads
to a total of 51 datasets. All results where the training domain is "1M/2M →" are from models
trained using the above split. The "1M →" domain results correspond to training the model once on
single mutants and evaluating it on double mutants. In addition to Kermut, we include as a baseline
results from a GP using the sequence kernel operating on mean-pooled ESM-2 embeddings. This is
equivalent to setting the structure kernel to 0, kstruct= 0as in Table 2. For results on the multi-mutant
GB1 landscape from FLIP [6], see Appendix H.
Table G.1: Results in multi-mutant setting. Each row corresponds to a different setting of training
and evaluation domain. Third row corresponds to the fold_rand_multiples split-scheme from
ProteinGym. Experiments are carried out on 51 datasets, corresponding to all datasets with multiple
mutants with less than 7500 variants in total with the exception of GCN4_YEAST_Staller_2018 ,
which has been removed due to its high mutation count.∗: Constant mean.
Spearman (↑) MSE (↓)
Domain Kermut Kermut∗kseq Kermut Kermut∗kseq
1M/2M →1M 0.910 0.908 0.879 0.139 0.143 1.154
1M/2M →2M 0.895 0.895 0.873 0.103 0.103 1.044
1M/2M →1M/2M 0.938 0.937 0.913 0.116 0.118 1.092
1M→2M 0.650 0.660 0.648 0.805 0.580 0.506
H Results on GB1 landscape from the FLIP benchmark
To further investigate Kermut’s performance in a multi-mutant setting, we apply it to the GB1 fitness
landscape from FLIP [ 6]. The results can be seen in Table H.1. Kermut’s base configuration severely
underperforms in the 1-vs-rest split, while reaching similar correlation coefficients in the 2-vs-rest
and 3-vs-rest splits. A reason for the initial low score might be the that the accuracy of zero-shot
methods at different mutation orders tend to decrease with higher mutation count. Despite this, we
still see low performance compared to other models when removing the zero-shot mean function.
The GB1 landscape is comprised of 149,361 mutations at exactly four highly epistatic positions
in the GB1 binding domain of Protein G. This is a challenging task for Kermut, whose structural
kernel directly compares sites. With only four sites to compare, Kermut fails to accurately model the
fitness landscape. Additionally, as described in the main text, the only epistatic modeling in Kermut
is via the mean-pooled protein language model embeddings in the sequence kernel, which proves
insufficient to capture the interplay of these highly epistatic sites. Further experimentation is required
to thoroughly gauge Kermut’s performance across diverse multi-mutant assays where the number of
mutated residues is variable and epistatis plays a central role.
25Table H.1: Performance on FLIP’s GB1 landscape.∗: Reference results from FLIP. Best and second
best scores per split has been highlighted.
Model 1-vs-rest 2-vs-rest 3-vs-rest low-vs-high
ESM-1b (per AA)∗0.28 0.55 0.79 0.59
ESM-1b (mean)∗0.32 0.36 0.54 0.13
ESM-1b (mut mean)∗-0.08 0.19 0.49 0.45
ESM-1v (per AA)∗0.28 0.28 0.82 0.51
ESM-1v (mean)∗0.32 0.32 0.77 0.10
ESM-1v (mut mean)∗0.19 0.19 0.80 0.49
ESM-untrained (per AA)∗0.06 0.06 0.48 0.23
ESM-untrained (mean)∗0.05 0.05 0.46 0.10
ESM-untrained (mut mean)∗0.21 0.21 0.57 0.13
Ridge∗0.28 0.59 0.76 0.34
CNN∗0.17 0.32 0.83 0.51
Levenshtein∗0.17 0.16 -0.04 -0.10
BLOSUM62∗0.15 0.14 0.01 -0.13
Kermut -0.14 0.52 0.77 0.35
Kermut (constant mean) 0.37 0.55 0.77 0.36
Baseline GP 0.40 0.57 0.73 0.42
I Histogram over assays for multi-mutant datasets
A histogram over normalized assay values for 51/69 dataset with multi-mutants (total fewer than
7500 sequences) can be seen in Figure I.1. The histograms are colored according to the number of
mutations per variant. The assay distribution belong to different modalities depending on the number
of mutations present, where double mutations often lead to a loss of fitness.
4
 2
 0 2 4 6
DMS score0.000.010.020.030.040.05ProbabilityNumber of mutations
1
2
Figure I.1: Histogram over normalized assay values for 51/69 datasets with multi-mutants. All
datasets with more than 7500 variants are ignored. The histograms are colored according to the
number of mutations per variant. The assay distribution belong to different modalities depending on
the number of mutations present, where double mutations commonly lead to a loss of fitness.
J Uncertainty calibration
J.1 Confidence interval-based calibration
Given a collection of mean predictions and uncertainties, we wish to gauge how well-calibrated the
uncertainties are. The posterior predictive mean and variance for each data point is interpreted as a
Gaussian distribution and symmetric intervals of varying confidence are placed on each prediction
[41]. In a well-calibrated model, approximately x% of predictions should lie within a x% confidence
26interval, e.g., 50% of observations should fall in the 50% confidence interval. The confidence
intervals are discretized into Kbins and the fraction of predictions falling within in bin is calculated.
The calibration curve then plots the confidence intervals vs. the fractions, whereby a diagonal
line corresponds to perfect calibration. Given the fractions and confidence intervals, the expected
calibration error (ECE) is calculated as
ECE =1
KKX
i=1|acc(i)−i|,
where Kis the number of bins, iindicates the equally spaced confidence intervals, and acc(i)is the
fraction of predictions falling within the ith confidence interval.
J.2 Error-based calibration
An alternative method of gauging calibratedness is error-based calibration where the prediction
error is tied directly to predictions [ 41,42]. The predictions are sorted according to their predictive
uncertainty and placed into Kbins. For each bin, the root mean square error (RMSE) and root mean
variance (RMV) is computed. In error-based calibration, a well-calibrated model as equal RMSE
and RMV , i.e., a diagonal line. The xandyvalues in the resulting calibration plot are however
not normalized from 0 to 1 as in confidence interval-based calibration. The expected normalized
calibration error (ENCE) can be computed as
ENCE =1
KKX
i=1|RMV (i)−RMSE (i)|
RMV (i).
Additionally, we compute the coefficient of variation (cv) as
cv=qPN
n=1(σn−µσ)2
N−1
µσ,
where µσ=1
NPN
n=1σn, and where nindexes the Ndata points [42].
J.3 Uncertainty calibration across all datasets
To quantitatively describe the calibratedness of Kermut across all datasets, we compute the above
calibration metrics for all split schemes and folds. We do this for Kermut and a baseline GP using the
sequence kernel on ESM-2 embeddings (equivalent to the sequence kernel from Equation (3) with a
constant mean). These values can be seen in Figure J.1 for each of the three main split-schemes, the
1M/2M →1M/2M split (“Multiples”), and the 1M →2M split (“Extrapolation”). For the three main
splits, we see that the calibratedness measured by the ENCE correlates with performance, where the
lowest values are seen in the random setting. Generally, we see that the inclusion of the structure
kernel improves not only performance (see Table 2) but also calibration, as the ECE and ENCE values
are consistently better for Kermut, with the exception of ENCE in the extrapolation domain. We
however see that the sequence kernel (squared exponential) consistently provide predictive variances
that themselves vary more, which is generally preferable.
Overall, we can conclude that Kermut appears to be well-calibrated both qualitatively and quantita-
tively. While the ECE values are generally small and similar, the ENCE values suggest a more nuanced
calibration landscape, where we can expect low errors when our model predicts low uncertainties,
particularly in the random scheme.
For each dataset, split, and fold, we perform a linear regression to the error-based calibration curves
and summarize the slope and intercept. As perfect calibration corresponds to a diagonal line, we want
the distribution over the slopes to be centred on one and the distribution over intercepts to be centred
on zero. Boxplots for this analysis can be seen in Appendix J.3. As indicated in (a), most of Kermut’s
calibration curves are well-behaved, while the baseline GP in (b) is generally poorly calibrated.
27Random Modulo
ContiguousMultiples
Extrapolation0.000.050.10ECE
Random Modulo
ContiguousMultiples
Extrapolation0.00.10.20.30.4ENCE
Random Modulo
ContiguousMultiples
Extrapolation0.00.10.20.3cv
Kermut Baseline GPFigure J.1: Calibration metrics per domain for Kermut and the sequence kernel on ESM-2 embed-
dings. Random, modulo, and contiguous domains are from the ProteinGym substitution benchmark.
Multiples corresponds to training and testing on both single and double mutants. Extrapolation
corresponds to training on singles and predicting on doubles. 51 datasets with multi-mutants was
used for the figure for all domains for comparability. The performance results for the multi-mutant
setting can be found in Table G.1. Errorbars correspond to standard error.
2
1
012Intercept
Random Modulo Contiguous2
024Slope
(a) Kermut
2
1
012Intercept
Random Modulo Contiguous2
024Slope (b) Baseline GP ( k=kseq,m=α)
Figure J.2: Boxplot of intercepts and slopes of error-based calibration curves for Kermut and a
baseline GP with the sequence kernel on ESM-2 embeddings. Perfect calibration has an intercept of
zero and a slope of one (indicated by dashed lines). The baseline GP has poor calibration compared
to Kermut. Horizontal lines indicate 0.9, 0.75, 0.5, 0.25, 0.1 quantiles.
28K Predicted vs. true values for calibration datasets
K.1 BLAT_ECOLX_Stiffler_2015
4
2
02Prediction (fold 1)
Random
 Modulo
 Contiguous
4
2
02Prediction (fold 2)
4
2
02Prediction (fold 3)
4
2
02Prediction (fold 4)
2.5
 0.0
Target4
2
02Prediction (fold 5)
2.5
 0.0
Target
2.5
 0.0
Target
Figure K.1: Predicted means ( ±2σ) vs. true values. Columns correspond to CV-schemes. Rows
correspond to test folds. Perfect prediction corresponds to dashed diagonal line ( x=y).
29K.2 PA_I34A1_Wu_2015
05Prediction (fold 1)
Random
 Modulo
 Contiguous
05Prediction (fold 2)
05Prediction (fold 3)
05Prediction (fold 4)
0.0 2.5 5.0
Target05Prediction (fold 5)
0.0 2.5 5.0
Target
0.0 2.5 5.0
Target
Figure K.2: Predicted means ( ±2σ) vs. true values. Columns correspond to CV-schemes. Rows
correspond to test folds. Perfect prediction corresponds to dashed diagonal line ( x=y).
30K.3 TCRG1_MOUSE_Tsuboyama_2023
4
2
02Prediction (fold 1)
Random
 Modulo
 Contiguous
4
2
02Prediction (fold 2)
4
2
02Prediction (fold 3)
4
2
02Prediction (fold 4)
2.5
 0.0
Target4
2
02Prediction (fold 5)
2.5
 0.0
Target
2.5
 0.0
Target
Figure K.3: Predicted means ( ±2σ) vs. true values. Columns correspond to CV-schemes. Rows
correspond to test folds. Perfect prediction corresponds to dashed diagonal line ( x=y).
31K.4 OPSD_HUMAN_Wan_2019
2.5
0.02.55.0Prediction (fold 1)
Random
 Modulo
 Contiguous
2.5
0.02.55.0Prediction (fold 2)
2.5
0.02.55.0Prediction (fold 3)
2.5
0.02.55.0Prediction (fold 4)
0 2 4
Target2.5
0.02.55.0Prediction (fold 5)
0 2 4
Target
0 2 4
Target
Figure K.4: Predicted means ( ±2σ) vs. true values. Columns correspond to CV-schemes. Rows
correspond to test folds. Perfect prediction corresponds to dashed diagonal line ( x=y).
32L Calibration curves for ProteinNPT
L.1 ProteinNPT details
We use ProteinNPT using the provided software in the paper with the default settings. We generate
the MSA Transformer embeddings manually using the provided software from ProteinNPT. During
evaluation, we predict using Monte Carlo dropout (with 25 samples, as described in the ProteinNPT
appendix). An uncertainty estimate per test sequence is obtained by taking the standard deviation
over the 25 samples as prescribed.
L.2 Calibration curves
0.00.20.40.60.81.0Confidence
ECE: 0.35 (±0.02)Random
ECE: 0.36 (±0.02)Modulo
ECE: 0.37 (±0.02)Contiguous
0.00.20.40.60.81.0Confidence
ECE: 0.40 (±0.01)
 ECE: 0.40 (±0.01)
 ECE: 0.40 (±0.01)
0.00.20.40.60.81.0Confidence
ECE: 0.39 (±0.02)
 ECE: 0.40 (±0.01)
 ECE: 0.40 (±0.01)
0.0 0.2 0.4 0.6 0.8 1.0
Percentile0.00.20.40.60.81.0Confidence
ECE: 0.39 (±0.02)
0.0 0.2 0.4 0.6 0.8 1.0
Percentile
ECE: 0.37 (±0.05)
0.0 0.2 0.4 0.6 0.8 1.0
Percentile
ECE: 0.38 (±0.03)
(a) Confidence interval-based calibration curves
0.025 0.0500.20.30.40.50.6RMSE
ENCE: 13.69 (±0.99)
cv: 0.10 (±0.02)Random
0.025 0.0500.40.60.8
ENCE: 24.79 (±6.24)
cv: 0.11 (±0.02)Modulo
0.025 0.0500.00.51.0
ENCE: 24.82 (±21.42)
cv: 0.10 (±0.01)Contiguous
0.015 0.0200.00.51.01.5RMSE
ENCE: 65.70 (±26.37)
cv: 0.03 (±0.00)
0.010 0.015 0.0200.00.51.01.52.0
ENCE: 66.27 (±19.62)
cv: 0.03 (±0.01)
0.015 0.020012
ENCE: 64.47 (±35.55)
cv: 0.03 (±0.00)
0.005 0.010 0.0150.00.20.40.6RMSE
ENCE: 30.72 (±8.16)
cv: 0.04 (±0.02)
0.00 0.020.00.51.0
ENCE: 54.58 (±33.48)
cv: 0.07 (±0.07)
0.00 0.050.00.51.0
ENCE: 60.71 (±26.90)
cv: 0.09 (±0.19)
0.00 0.02
RMV0.00.51.0RMSE
ENCE: 48.51 (±36.85)
cv: 0.06 (±0.03)
0.00 0.05
RMV0.00.51.0
ENCE: 48.54 (±20.82)
cv: 0.08 (±0.11)
0.000 0.025 0.050
RMV0.00.51.0
ENCE: 49.41 (±16.16)
cv: 0.08 (±0.06) (b) Error-based calibration curves
Figure L.1: Calibration curves for ProteinNPT on the four dataset from Table 3. Standard deviation
over CV folds is shown as vertical bars. Perfect calibration corresponds to diagonal lines ( y=x) and
is shown as dashed lines in each plot. The predictive uncertainties for ProteinNPT are very small,
resulting in poor out-of-the-box calibration as seen on the x-axis in (b) and in Figures L.2 to L.5.
However, as indicated by the trend in both (a) and (b), the errors correlate with the magnitude of the
uncertainties. This suggests that a recalibration might be sufficient to achieve good calibration.
33L.3 Predicted vs. true values for calibration datasets
L.3.1 BLAT_ECOLX_Stiffler_2015
2
0Prediction (fold 1)
Random
 Modulo
 Contiguous
2
0Prediction (fold 2)
2
0Prediction (fold 3)
2
0Prediction (fold 4)
2
 0
Target2
0Prediction (fold 5)
2
 0
Target
2
 0
Target
Figure L.2: Predicted means by ProteinNPT ( ±2σ) vs. true values. Columns correspond to CV-
schemes. Rows correspond to test folds. Perfect prediction corresponds to dashed diagonal line
(x=y). While the predictions are good, the model is very overconfident.
34L.3.2 PA_I34A1_Wu_2015
0246Prediction (fold 1)
Random
 Modulo
 Contiguous
0246Prediction (fold 2)
0246Prediction (fold 3)
0246Prediction (fold 4)
0 2 4 6
Target0246Prediction (fold 5)
0 2 4 6
Target
0 2 4 6
Target
Figure L.3: Predicted means by ProteinNPT ( ±2σ) vs. true values. Columns correspond to CV-
schemes. Rows correspond to test folds. Perfect prediction corresponds to dashed diagonal line
(x=y). Despite the relatively poor predictions, the model remains overconfident.
35L.3.3 TCRG1_MOUSE_Tsuboyama_2023
2
02Prediction (fold 1)
Random
 Modulo
 Contiguous
2
02Prediction (fold 2)
2
02Prediction (fold 3)
2
02Prediction (fold 4)
2.5
 0.0
Target2
02Prediction (fold 5)
2.5
 0.0
Target
2.5
 0.0
Target
Figure L.4: Predicted means by ProteinNPT ( ±2σ) vs. true values. Columns correspond to CV-
schemes. Rows correspond to test folds. Perfect prediction corresponds to dashed diagonal line
(x=y). While the predictions are good, the model is very overconfident. Despite the relatively poor
predictions, the model remains overconfident.
36L.3.4 OPSD_HUMAN_Wan_2019
024Prediction (fold 1)
Random
 Modulo
 Contiguous
024Prediction (fold 2)
024Prediction (fold 3)
024Prediction (fold 4)
0 2 4
Target024Prediction (fold 5)
0 2 4
Target
0 2 4
Target
Figure L.5: Predicted means by ProteinNPT ( ±2σ) vs. true values. Columns correspond to CV-
schemes. Rows correspond to test folds. Perfect prediction corresponds to dashed diagonal line
(x=y).
37M Alternative zero-shot methods
Kermut uses a linear transformation of a variant’s zero-shot score as its mean function. In the main
results ESM-2 was used. We here provide additional results where different zero-shot methods are
used. The experiments are carried out as the ablation results in Section 4.1, i.e., on 174/217 datasets.
All zero-shot scores are pre-computed and are available via the ProteinGym suite.
Using a zero-shot mean function instead of a constant mean evidently leads to increased performance.
The magnitude of the improvement depends on the chosen zero-shot method, where the order roughly
corresponds to that of the zero-shot scores in ProteinGym. We do however see that opting for EVE
yields the largest performance increase.
Table M.1: Performance using alternate zero-shot methods. The experiments are carried out on
174/217 datasets.∗: ESM-2 in this table is equivalent to Kermut from the main results in Table 1.
Zero-shot predictor Spearman (↑) MSE (↓)
Contig. Mod. Rand. Avg. Contig. Mod. Rand. Avg.
EVE 0.608 0.627 0.750 0.662 0.731 0.682 0.412 0.608
ESM-2∗0.605 0.628 0.743 0.659 0.730 0.683 0.420 0.611
GEMME 0.605 0.622 0.744 0.657 0.728 0.682 0.416 0.609
VESPA 0.606 0.623 0.742 0.657 0.737 0.698 0.424 0.620
TranceptEVE L 0.600 0.619 0.744 0.654 0.741 0.693 0.420 0.618
ESM-IF 0.583 0.606 0.738 0.642 0.757 0.708 0.424 0.630
ProteinMPNN 0.575 0.599 0.734 0.636 0.769 0.718 0.429 0.639
Constant mean 0.569 0.596 0.735 0.633 0.770 0.718 0.429 0.639
38N Hyperparameter visualization
N.1 Hyperparameter distributions
The distributions of Kermut’s hyperparameters across a number of datasets from ProteinGym can be
seen divided by split scheme in Figure N.1. λis a scale parameter for the structure kernel, while πis
a balancing parameter which lets the model assign importance to the structure and sequence kernels,
respectively. γ1,γ2, andγ3are scale coefficients in the kernels’ exponents. Their inverses are shown
to facilitate easier comparison with the squared exponential kernel’s lengthscale, lSE.
1 2 3
0.00.10.2ProbabilityRandom
1 2 3
Modulo
1 2 3
Contiguous
0.0 0.2 0.4 0.6 0.8
0.00.10.2Probability
0.0 0.2 0.4 0.6 0.8
 0.0 0.2 0.4 0.6 0.8
0 25 50 75 100 125
1
0.000.250.500.75Probability
0 25 50 75 100 125
1
0 25 50 75 100 125
1
0 50 100 150 200 250
2
0.00.10.20.3Probability
0 50 100 150 200 250
2
0 50 100 150 200 250
2
0 20 40 60
3
0.00.10.20.3Probability
0 20 40 60
3
0 20 40 60
3
0.0 0.5 1.0 1.5 2.0 2.5
SE
0.00.20.4Probability
0.0 0.5 1.0 1.5 2.0 2.5
SE
0.0 0.5 1.0 1.5 2.0 2.5
SE
Figure N.1: Distributions of Kermut’s hyperparameter across ProteinGym assays and splits. The
inverses of γ1,γ2, and γ3are shown to facilitate easier comparison with the sequence kernel’s
lengthscale.
39N.2 Hyperparameters vs. dataset size
1 2 3
0200040006000
NRandom
1 2 3
Modulo
1 2 3
Contiguous
0.0 0.2 0.4 0.6 0.8
0200040006000
N
0.0 0.2 0.4 0.6 0.8
 0.0 0.2 0.4 0.6 0.8
0 25 50 75 100 125
1
0200040006000
N
0 25 50 75 100 125
1
0 25 50 75 100 125
1
0 50 100 150 200 250
2
0200040006000
N
0 50 100 150 200 250
2
0 50 100 150 200 250
2
0 20 40 60
3
0200040006000
N
0 20 40 60
3
0 20 40 60
3
0.0 0.5 1.0 1.5 2.0 2.5
RBF
0200040006000
N
0.0 0.5 1.0 1.5 2.0 2.5
RBF
0.0 0.5 1.0 1.5 2.0 2.5
RBF
Figure N.2: Distributions of Kermut’s hyperparameter across ProteinGym assays and splits visualized
against dataset sizes. The inverses of γ1,γ2, and γ3are shown to facilitate easier comparison with
the sequence kernel’s lengthscale.
40N.3 Hyperparameters vs. sequence length
1 2 3
0100020003000Seq. lengthRandom
1 2 3
Modulo
1 2 3
Contiguous
0.0 0.2 0.4 0.6 0.8
0100020003000Seq. length
0.0 0.2 0.4 0.6 0.8
 0.0 0.2 0.4 0.6 0.8
0 25 50 75 100 125
1
0100020003000Seq. length
0 25 50 75 100 125
1
0 25 50 75 100 125
1
0 50 100 150 200 250
2
0100020003000Seq. length
0 50 100 150 200 250
2
0 50 100 150 200 250
2
0 20 40 60
3
0100020003000Seq. length
0 20 40 60
3
0 20 40 60
3
0.0 0.5 1.0 1.5 2.0 2.5
RBF
0100020003000Seq. length
0.0 0.5 1.0 1.5 2.0 2.5
RBF
0.0 0.5 1.0 1.5 2.0 2.5
RBF
Figure N.3: Distributions of Kermut’s hyperparameter across ProteinGym assays and splits visualized
against sequence length. The inverses of γ1,γ2, andγ3are shown to facilitate easier comparison with
the sequence kernel’s lengthscale.
41N.4 πvs.λ
0.0 0.2 0.4 0.6 0.8
0.51.01.52.02.53.0
Hyperparameters  vs. 
Figure N.4: The structure kernel scale hyperparameter, λ, is shown against the kernel balancing
hyperparameter, π.
42O Detailed results per DMS
We show the performance of Kermut and ProteinNPT per DMS assay in Figures O.1 to O.4. The
figures show the average performance and the performance per split, respectively.
O.1 Detailed results per DMS (average)
0.00 0.25 0.50 0.75 1.00Q53Z42_HUMAN_McShan_2019_expressionCAPSD_AAV2S_Sinai_2021DLG4_HUMAN_Faure_2021POLG_DEN26_Suphatrakul_2023ANCSZ_Hobbs_2022PHOT_CHLRE_Chen_2023SRC_HUMAN_Ahler_2019YAP1_HUMAN_Araya_2012CASP3_HUMAN_Roychowdhury_2020KCNE1_HUMAN_Muhammad_2023_functionQ59976_STRSQ_Romero_2015C6KNH7_9INFA_Lee_2018D7PM05_CLYGR_Somermeyer_2022TRPC_SACS2_Chan_2017A0A2Z5U3Z0_9INFA_Doud_2016NUD15_HUMAN_Suiter_2020VKOR1_HUMAN_Chiasson_2020_abundanceS22A1_HUMAN_Yee_2023_activityTADBP_HUMAN_Bolognesi_2019OPSD_HUMAN_Wan_2019PKN1_HUMAN_Tsuboyama_2023_1URFRASK_HUMAN_Weng_2022_binding-DARPin_K55MTH3_HAEAE_RockahShmuel_2015FKBP3_HUMAN_Tsuboyama_2023_2KFVCAR11_HUMAN_Meitlis_2020_lofRASH_HUMAN_Bandaru_2017NPC1_HUMAN_Erwood_2022_RPE1SPIKE_SARS2_Starr_2020_bindingAMIE_PSEAE_Wrenbeck_2017P53_HUMAN_Kotler_2018BLAT_ECOLX_Jacquier_2013RL40A_YEAST_Roscoe_2013RAD_ANTMA_Tsuboyama_2023_2CJJRFAH_ECOLI_Tsuboyama_2023_2LCLKKA2_KLEPN_Melnikov_2014S22A1_HUMAN_Yee_2023_abundanceCP2C9_HUMAN_Amorosi_2021_abundancePABP_YEAST_Melamed_2013UBR5_HUMAN_Tsuboyama_2023_1I2TSOX30_HUMAN_Tsuboyama_2023_7JJKSPG1_STRSG_Wu_2016P84126_THETH_Chan_2017PSAE_SYNP2_Tsuboyama_2023_1PSEODP2_GEOSE_Tsuboyama_2023_1W4GPR40A_HUMAN_Tsuboyama_2023_1UZCPOLG_PESV_Tsuboyama_2023_2MXDPRKN_HUMAN_Clausen_2023SBI_STAAM_Tsuboyama_2023_2JVGNPC1_HUMAN_Erwood_2022_HEK293TBCHB_CHLTE_Tsuboyama_2023_2KRUSAV1_MOUSE_Tsuboyama_2023_2YSBISDH_STAAW_Tsuboyama_2023_2LHRSPTN1_CHICK_Tsuboyama_2023_1TUDDOCK1_MOUSE_Tsuboyama_2023_2M0YPITX2_HUMAN_Tsuboyama_2023_2L7MTHO1_YEAST_Tsuboyama_2023_2WQGYAIA_ECOLI_Tsuboyama_2023_2KVTSCIN_STAAR_Tsuboyama_2023_2QFFRCRO_LAMBD_Tsuboyama_2023_1ORCDN7A_SACS2_Tsuboyama_2023_1JICCUE1_YEAST_Tsuboyama_2023_2MYXYNZC_BACSU_Tsuboyama_2023_2JVDCP2C9_HUMAN_Amorosi_2021_activitySPIKE_SARS2_Starr_2020_expressionARGR_ECOLI_Tsuboyama_2023_1AOYOTC_HUMAN_Lo_2023P53_HUMAN_Giacomelli_2018_WT_NutlinCBPA2_HUMAN_Tsuboyama_2023_1O6XOTU7A_HUMAN_Tsuboyama_2023_2L2DCATR_CHLRE_Tsuboyama_2023_2AMISPA_STAAU_Tsuboyama_2023_1LP1SRBS1_HUMAN_Tsuboyama_2023_2O2WRS15_GEOSE_Tsuboyama_2023_1A32NKX31_HUMAN_Tsuboyama_2023_2L9RMBD11_ARATH_Tsuboyama_2023_6ACVSPG2_STRSG_Tsuboyama_2023_5UBSGRB2_HUMAN_Faure_2021CBX4_HUMAN_Tsuboyama_2023_2K28MYO3_YEAST_Tsuboyama_2023_2BTTBLAT_ECOLX_Stiffler_2015VILI_CHICK_Tsuboyama_2023_1YU5BBC1_YEAST_Tsuboyama_2023_1TG0HCP_LAMBD_Tsuboyama_2023_2L6QA4GRB6_PSEAI_Chen_2020HECD1_HUMAN_Tsuboyama_2023_3DKMBLAT_ECOLX_Firnberg_2014FECA_ECOLI_Tsuboyama_2023_2D1UMAFG_MOUSE_Tsuboyama_2023_1K1VDNJA1_HUMAN_Tsuboyama_2023_2LO1CSN4_MOUSE_Tsuboyama_2023_1UFMEPHB2_HUMAN_Tsuboyama_2023_1F0MVG08_BPP22_Tsuboyama_2023_2GP8RCD1_ARATH_Tsuboyama_2023_5OAOOBSCN_HUMAN_Tsuboyama_2023_1V1CUBE4B_HUMAN_Tsuboyama_2023_3L1XPIN1_HUMAN_Tsuboyama_2023_1I6CVRPI_BPT7_Tsuboyama_2023_2WNMRD23A_HUMAN_Tsuboyama_2023_1IFYSDA_BACSU_Tsuboyama_2023_1PV0TCRG1_MOUSE_Tsuboyama_2023_1E0LTNKS2_HUMAN_Tsuboyama_2023_5JRTAMFR_HUMAN_Tsuboyama_2023_4G3ORBP1_HUMAN_Tsuboyama_2023_2KWHSR43C_ARATH_Tsuboyama_2023_2N88SQSTM_MOUSE_Tsuboyama_2023_2RRURPC1_BP434_Tsuboyama_2023_1R69PPARG_HUMAN_Majithia_2016NUSA_ECOLI_Tsuboyama_2023_1WCLNUSG_MYCTU_Tsuboyama_2023_2MI6
ProteinNPT Kermut
(a) Top half
0.00 0.25 0.50 0.75 1.00HIS7_YEAST_Pokusaeva_2019A0A1I9GEU1_NEIME_Kennouche_2019SCN5A_HUMAN_Glazer_2019CALM1_HUMAN_Weile_2017ENV_HV1B9_DuenasDecamp_2016CAS9_STRP1_Spencer_2017_positiveAICDA_HUMAN_Gajula_2014_3cyclesENVZ_ECOLI_Ghose_2023ENV_HV1BR_Haddox_2016TPK1_HUMAN_Weile_2017F7YBW8_MESOW_Aakre_2015PA_I34A1_Wu_2015F7YBW7_MESOW_Ding_2023RAF1_HUMAN_Zinkus-Boltz_2019B2L11_HUMAN_Dutta_2010_binding-Mcl-1REV_HV1H2_Fernandes_2016VKOR1_HUMAN_Chiasson_2020_activityGCN4_YEAST_Staller_2018CBS_HUMAN_Sun_2020A0A140D2T1_ZIKV_Sourisseau_2019KCNJ2_MOUSE_Coyote-Maestas_2022_functionCCDB_ECOLI_Tripathi_2016CCR5_HUMAN_Gill_2023HSP82_YEAST_Cote-Hammarlof_2020_growth-H2O2GFP_AEQVI_Sarkisyan_2016A0A2Z5U3Z0_9INFA_Wu_2014HEM3_HUMAN_Loggerenberg_2023TAT_HV1BR_Fernandes_2016GDIA_HUMAN_Silverstein_2021BRCA2_HUMAN_Erwood_2022_HEK293TMK01_HUMAN_Brenan_2016UBE4B_MOUSE_Starita_2013Q6WV13_9MAXI_Somermeyer_2022MSH2_HUMAN_Jia_2020NCAP_I34A1_Doud_2015CD19_HUMAN_Klesmith_2019_FMC_singlesGLPA_HUMAN_Elazar_2016I6TAH8_I68A0_Doud_2015ACE2_HUMAN_Chan_2020TPOR_HUMAN_Bridgford_2020RPC1_LAMBD_Li_2019_high-expressionNRAM_I33A0_Jiang_2016OXDA_RHOTO_Vanella_2023_activityKCNJ2_MOUSE_Coyote-Maestas_2022_surfaceSYUA_HUMAN_Newberry_2020PAI1_HUMAN_Huttinger_2021HSP82_YEAST_Flynn_2019KCNH2_HUMAN_Kozek_2020A4D664_9INFA_Soh_2019SHOC2_HUMAN_Kwon_2022RDRP_I33A0_Li_2023Q8WTC7_9CNID_Somermeyer_2022CAR11_HUMAN_Meitlis_2020_gofA0A247D711_LISMN_Stadelmann_2021AACC1_PSEAI_Dandage_2018DYR_ECOLI_Nguyen_2023PTEN_HUMAN_Mighell_2018ADRB2_HUMAN_Jones_2020DYR_ECOLI_Thompson_2019POLG_CXB3N_Mattenberger_2021HXK4_HUMAN_Gersing_2023_abundancePOLG_HCVJF_Qi_2014SRC_HUMAN_Chakraborty_2023_binding-DAS_25uMLYAM1_HUMAN_Elazar_2016UBC9_HUMAN_Weile_2017SUMO1_HUMAN_Weile_2017TPMT_HUMAN_Matreyek_2018OXDA_RHOTO_Vanella_2023_expressionBRCA1_HUMAN_Findlay_2018SERC_HUMAN_Xie_2023IF1_ECOLI_Kelsic_2016TRPC_THEMA_Chan_2017SRC_HUMAN_Nguyen_2022RPC1_LAMBD_Li_2019_low-expressionBLAT_ECOLX_Deng_2012MTHR_HUMAN_Weile_2021ESTA_BACSU_Nutschel_2020DLG4_RAT_McLaughlin_2012HSP82_YEAST_Mishra_2016RL40A_YEAST_Roscoe_2014PTEN_HUMAN_Matreyek_2021RL40A_YEAST_Mavor_2016ERBB2_HUMAN_Elazar_2016MET_HUMAN_Estevam_2023SPG1_STRSG_Olson_2014Q2N0S5_9HIV1_Haddox_2018KCNE1_HUMAN_Muhammad_2023_expressionHXK4_HUMAN_Gersing_2022_activityA0A192B1T2_9HIV1_Haddox_2018ILF3_HUMAN_Tsuboyama_2023_2L33PPM1D_HUMAN_Miller_2022Q837P4_ENTFA_Meier_2023SC6A4_HUMAN_Young_2021P53_HUMAN_Giacomelli_2018_Null_EtoposideQ837P5_ENTFA_Meier_2023HMDH_HUMAN_Jiang_2019CASP7_HUMAN_Roychowdhury_2020MLAC_ECOLI_MacRae_2023GAL4_YEAST_Kitzman_2015RNC_ECOLI_Weeks_2023R1AB_SARS2_Flynn_2022A4_HUMAN_Seuma_2022RASK_HUMAN_Weng_2022_abundanceQ53Z42_HUMAN_McShan_2019_binding-TAPBPRRL20_AQUAE_Tsuboyama_2023_1GYZP53_HUMAN_Giacomelli_2018_Null_NutlinLGK_LIPST_Klesmith_2015CCDB_ECOLI_Adkar_2012
ProteinNPT Kermut (b) Bottom half
Figure O.1: Spearman’s correlation per DMS assay, averaged over the three split schemes. The figure
has been split in two to fit.
43O.2 Detailed results per DMS (random)
0.00 0.25 0.50 0.75 1.00D7PM05_CLYGR_Somermeyer_2022P53_HUMAN_Kotler_2018ANCSZ_Hobbs_2022S22A1_HUMAN_Yee_2023_activityKCNE1_HUMAN_Muhammad_2023_expressionCAR11_HUMAN_Meitlis_2020_lofYAP1_HUMAN_Araya_2012Q837P4_ENTFA_Meier_2023RL20_AQUAE_Tsuboyama_2023_1GYZKKA2_KLEPN_Melnikov_2014Q837P5_ENTFA_Meier_2023TRPC_SACS2_Chan_2017DLG4_HUMAN_Faure_2021CAPSD_AAV2S_Sinai_2021NPC1_HUMAN_Erwood_2022_HEK293TR1AB_SARS2_Flynn_2022RASK_HUMAN_Weng_2022_binding-DARPin_K55POLG_DEN26_Suphatrakul_2023B2L11_HUMAN_Dutta_2010_binding-Mcl-1VKOR1_HUMAN_Chiasson_2020_abundanceSRC_HUMAN_Ahler_2019C6KNH7_9INFA_Lee_2018CP2C9_HUMAN_Amorosi_2021_abundanceAMIE_PSEAE_Wrenbeck_2017NUD15_HUMAN_Suiter_2020A0A192B1T2_9HIV1_Haddox_2018A0A2Z5U3Z0_9INFA_Doud_2016S22A1_HUMAN_Yee_2023_abundanceRASH_HUMAN_Bandaru_2017SPIKE_SARS2_Starr_2020_bindingQ2N0S5_9HIV1_Haddox_2018FKBP3_HUMAN_Tsuboyama_2023_2KFVRL40A_YEAST_Roscoe_2013BLAT_ECOLX_Deng_2012PABP_YEAST_Melamed_2013P84126_THETH_Chan_2017RFAH_ECOLI_Tsuboyama_2023_2LCLPKN1_HUMAN_Tsuboyama_2023_1URFSPG1_STRSG_Olson_2014OTC_HUMAN_Lo_2023PRKN_HUMAN_Clausen_2023UBR5_HUMAN_Tsuboyama_2023_1I2TRAD_ANTMA_Tsuboyama_2023_2CJJPR40A_HUMAN_Tsuboyama_2023_1UZCNKX31_HUMAN_Tsuboyama_2023_2L9RISDH_STAAW_Tsuboyama_2023_2LHRP53_HUMAN_Giacomelli_2018_WT_NutlinARGR_ECOLI_Tsuboyama_2023_1AOYYNZC_BACSU_Tsuboyama_2023_2JVDYAIA_ECOLI_Tsuboyama_2023_2KVTCP2C9_HUMAN_Amorosi_2021_activitySBI_STAAM_Tsuboyama_2023_2JVGODP2_GEOSE_Tsuboyama_2023_1W4GRBP1_HUMAN_Tsuboyama_2023_2KWHDOCK1_MOUSE_Tsuboyama_2023_2M0YCATR_CHLRE_Tsuboyama_2023_2AMIA4GRB6_PSEAI_Chen_2020GRB2_HUMAN_Faure_2021SPA_STAAU_Tsuboyama_2023_1LP1VG08_BPP22_Tsuboyama_2023_2GP8CBPA2_HUMAN_Tsuboyama_2023_1O6XPSAE_SYNP2_Tsuboyama_2023_1PSESPIKE_SARS2_Starr_2020_expressionSOX30_HUMAN_Tsuboyama_2023_7JJKSPTN1_CHICK_Tsuboyama_2023_1TUDBLAT_ECOLX_Stiffler_2015BLAT_ECOLX_Firnberg_2014RCD1_ARATH_Tsuboyama_2023_5OAOOTU7A_HUMAN_Tsuboyama_2023_2L2DDN7A_SACS2_Tsuboyama_2023_1JICCBX4_HUMAN_Tsuboyama_2023_2K28THO1_YEAST_Tsuboyama_2023_2WQGHCP_LAMBD_Tsuboyama_2023_2L6QPOLG_PESV_Tsuboyama_2023_2MXDRS15_GEOSE_Tsuboyama_2023_1A32SCIN_STAAR_Tsuboyama_2023_2QFFDNJA1_HUMAN_Tsuboyama_2023_2LO1MYO3_YEAST_Tsuboyama_2023_2BTTSDA_BACSU_Tsuboyama_2023_1PV0SPG2_STRSG_Tsuboyama_2023_5UBSBCHB_CHLTE_Tsuboyama_2023_2KRUSAV1_MOUSE_Tsuboyama_2023_2YSBFECA_ECOLI_Tsuboyama_2023_2D1UHECD1_HUMAN_Tsuboyama_2023_3DKMUBE4B_HUMAN_Tsuboyama_2023_3L1XVILI_CHICK_Tsuboyama_2023_1YU5RCRO_LAMBD_Tsuboyama_2023_1ORCCSN4_MOUSE_Tsuboyama_2023_1UFMPITX2_HUMAN_Tsuboyama_2023_2L7MOBSCN_HUMAN_Tsuboyama_2023_1V1CTCRG1_MOUSE_Tsuboyama_2023_1E0LEPHB2_HUMAN_Tsuboyama_2023_1F0MSQSTM_MOUSE_Tsuboyama_2023_2RRUVRPI_BPT7_Tsuboyama_2023_2WNMAMFR_HUMAN_Tsuboyama_2023_4G3OBBC1_YEAST_Tsuboyama_2023_1TG0MAFG_MOUSE_Tsuboyama_2023_1K1VSRBS1_HUMAN_Tsuboyama_2023_2O2WRD23A_HUMAN_Tsuboyama_2023_1IFYSR43C_ARATH_Tsuboyama_2023_2N88RPC1_BP434_Tsuboyama_2023_1R69MBD11_ARATH_Tsuboyama_2023_6ACVCUE1_YEAST_Tsuboyama_2023_2MYXPIN1_HUMAN_Tsuboyama_2023_1I6CNUSA_ECOLI_Tsuboyama_2023_1WCLTNKS2_HUMAN_Tsuboyama_2023_5JRTPPARG_HUMAN_Majithia_2016NUSG_MYCTU_Tsuboyama_2023_2MI6CCDB_ECOLI_Adkar_2012
ProteinNPT Kermut
(a) Top half
0.00 0.25 0.50 0.75 1.00HIS7_YEAST_Pokusaeva_2019A0A1I9GEU1_NEIME_Kennouche_2019GCN4_YEAST_Staller_2018SCN5A_HUMAN_Glazer_2019CALM1_HUMAN_Weile_2017CAS9_STRP1_Spencer_2017_positiveAICDA_HUMAN_Gajula_2014_3cyclesENV_HV1BR_Haddox_2016TPK1_HUMAN_Weile_2017RAF1_HUMAN_Zinkus-Boltz_2019VKOR1_HUMAN_Chiasson_2020_activityENVZ_ECOLI_Ghose_2023CBS_HUMAN_Sun_2020CCDB_ECOLI_Tripathi_2016KCNJ2_MOUSE_Coyote-Maestas_2022_functionF7YBW8_MESOW_Aakre_2015ENV_HV1B9_DuenasDecamp_2016HEM3_HUMAN_Loggerenberg_2023GDIA_HUMAN_Silverstein_2021CCR5_HUMAN_Gill_2023PA_I34A1_Wu_2015BRCA2_HUMAN_Erwood_2022_HEK293TA0A2Z5U3Z0_9INFA_Wu_2014GFP_AEQVI_Sarkisyan_2016UBE4B_MOUSE_Starita_2013REV_HV1H2_Fernandes_2016TAT_HV1BR_Fernandes_2016RPC1_LAMBD_Li_2019_high-expressionA0A140D2T1_ZIKV_Sourisseau_2019OXDA_RHOTO_Vanella_2023_activityPAI1_HUMAN_Huttinger_2021MK01_HUMAN_Brenan_2016SYUA_HUMAN_Newberry_2020ADRB2_HUMAN_Jones_2020HSP82_YEAST_Cote-Hammarlof_2020_growth-H2O2TPMT_HUMAN_Matreyek_2018CD19_HUMAN_Klesmith_2019_FMC_singlesCAR11_HUMAN_Meitlis_2020_gofTPOR_HUMAN_Bridgford_2020PTEN_HUMAN_Mighell_2018SHOC2_HUMAN_Kwon_2022NCAP_I34A1_Doud_2015KCNJ2_MOUSE_Coyote-Maestas_2022_surfaceHSP82_YEAST_Flynn_2019RDRP_I33A0_Li_2023SERC_HUMAN_Xie_2023AACC1_PSEAI_Dandage_2018OXDA_RHOTO_Vanella_2023_expressionHXK4_HUMAN_Gersing_2023_abundanceMSH2_HUMAN_Jia_2020SUMO1_HUMAN_Weile_2017F7YBW7_MESOW_Ding_2023A0A247D711_LISMN_Stadelmann_2021Q8WTC7_9CNID_Somermeyer_2022BRCA1_HUMAN_Findlay_2018PPM1D_HUMAN_Miller_2022DYR_ECOLI_Nguyen_2023MET_HUMAN_Estevam_2023LYAM1_HUMAN_Elazar_2016MTHR_HUMAN_Weile_2021KCNH2_HUMAN_Kozek_2020HXK4_HUMAN_Gersing_2022_activityPOLG_HCVJF_Qi_2014CASP7_HUMAN_Roychowdhury_2020PTEN_HUMAN_Matreyek_2021RPC1_LAMBD_Li_2019_low-expressionHSP82_YEAST_Mishra_2016RNC_ECOLI_Weeks_2023Q6WV13_9MAXI_Somermeyer_2022ACE2_HUMAN_Chan_2020OPSD_HUMAN_Wan_2019ERBB2_HUMAN_Elazar_2016POLG_CXB3N_Mattenberger_2021NRAM_I33A0_Jiang_2016UBC9_HUMAN_Weile_2017HMDH_HUMAN_Jiang_2019SRC_HUMAN_Nguyen_2022SC6A4_HUMAN_Young_2021RL40A_YEAST_Mavor_2016GAL4_YEAST_Kitzman_2015SRC_HUMAN_Chakraborty_2023_binding-DAS_25uMP53_HUMAN_Giacomelli_2018_Null_NutlinCASP3_HUMAN_Roychowdhury_2020I6TAH8_I68A0_Doud_2015ILF3_HUMAN_Tsuboyama_2023_2L33A4_HUMAN_Seuma_2022DYR_ECOLI_Thompson_2019RL40A_YEAST_Roscoe_2014A4D664_9INFA_Soh_2019ESTA_BACSU_Nutschel_2020DLG4_RAT_McLaughlin_2012Q59976_STRSQ_Romero_2015MTH3_HAEAE_RockahShmuel_2015GLPA_HUMAN_Elazar_2016P53_HUMAN_Giacomelli_2018_Null_EtoposideRASK_HUMAN_Weng_2022_abundanceNPC1_HUMAN_Erwood_2022_RPE1IF1_ECOLI_Kelsic_2016BLAT_ECOLX_Jacquier_2013TADBP_HUMAN_Bolognesi_2019MLAC_ECOLI_MacRae_2023SPG1_STRSG_Wu_2016Q53Z42_HUMAN_McShan_2019_expressionLGK_LIPST_Klesmith_2015KCNE1_HUMAN_Muhammad_2023_functionTRPC_THEMA_Chan_2017Q53Z42_HUMAN_McShan_2019_binding-TAPBPRPHOT_CHLRE_Chen_2023
ProteinNPT Kermut (b) Bottom half
Figure O.2: Spearman’s correlation per DMS assay in the random split scheme. The figure has been
split in two to fit. The performance difference between Kermut and ProteinNPT for the random split
is smallest (a), i.e., for the assays where the performance is high.
44O.3 Detailed results per DMS (modulo)
0.00 0.25 0.50 0.75 1.00RNC_ECOLI_Weeks_2023PHOT_CHLRE_Chen_2023CASP7_HUMAN_Roychowdhury_2020TRPC_SACS2_Chan_2017LGK_LIPST_Klesmith_2015YAP1_HUMAN_Araya_2012PKN1_HUMAN_Tsuboyama_2023_1URFNUD15_HUMAN_Suiter_2020FKBP3_HUMAN_Tsuboyama_2023_2KFVKCNE1_HUMAN_Muhammad_2023_functionP53_HUMAN_Giacomelli_2018_Null_NutlinVKOR1_HUMAN_Chiasson_2020_abundanceD7PM05_CLYGR_Somermeyer_2022C6KNH7_9INFA_Lee_2018RASK_HUMAN_Weng_2022_binding-DARPin_K55P53_HUMAN_Giacomelli_2018_Null_EtoposideSPIKE_SARS2_Starr_2020_bindingS22A1_HUMAN_Yee_2023_activityRFAH_ECOLI_Tsuboyama_2023_2LCLRAD_ANTMA_Tsuboyama_2023_2CJJA0A2Z5U3Z0_9INFA_Doud_2016NPC1_HUMAN_Erwood_2022_RPE1CASP3_HUMAN_Roychowdhury_2020AMIE_PSEAE_Wrenbeck_2017RASH_HUMAN_Bandaru_2017Q59976_STRSQ_Romero_2015UBR5_HUMAN_Tsuboyama_2023_1I2TCAR11_HUMAN_Meitlis_2020_lofSOX30_HUMAN_Tsuboyama_2023_7JJKPOLG_PESV_Tsuboyama_2023_2MXDP84126_THETH_Chan_2017RL40A_YEAST_Roscoe_2013NRAM_I33A0_Jiang_2016SAV1_MOUSE_Tsuboyama_2023_2YSBPSAE_SYNP2_Tsuboyama_2023_1PSEOPSD_HUMAN_Wan_2019ODP2_GEOSE_Tsuboyama_2023_1W4GMTH3_HAEAE_RockahShmuel_2015S22A1_HUMAN_Yee_2023_abundanceSBI_STAAM_Tsuboyama_2023_2JVGKKA2_KLEPN_Melnikov_2014P53_HUMAN_Kotler_2018BLAT_ECOLX_Jacquier_2013PR40A_HUMAN_Tsuboyama_2023_1UZCSPTN1_CHICK_Tsuboyama_2023_1TUDPABP_YEAST_Melamed_2013PRKN_HUMAN_Clausen_2023DN7A_SACS2_Tsuboyama_2023_1JICDOCK1_MOUSE_Tsuboyama_2023_2M0YBCHB_CHLTE_Tsuboyama_2023_2KRUTADBP_HUMAN_Bolognesi_2019CP2C9_HUMAN_Amorosi_2021_abundanceYAIA_ECOLI_Tsuboyama_2023_2KVTPITX2_HUMAN_Tsuboyama_2023_2L7MTHO1_YEAST_Tsuboyama_2023_2WQGCUE1_YEAST_Tsuboyama_2023_2MYXSRBS1_HUMAN_Tsuboyama_2023_2O2WCBPA2_HUMAN_Tsuboyama_2023_1O6XNPC1_HUMAN_Erwood_2022_HEK293TSPG1_STRSG_Wu_2016SPIKE_SARS2_Starr_2020_expressionSCIN_STAAR_Tsuboyama_2023_2QFFRCRO_LAMBD_Tsuboyama_2023_1ORCYNZC_BACSU_Tsuboyama_2023_2JVDISDH_STAAW_Tsuboyama_2023_2LHRMBD11_ARATH_Tsuboyama_2023_6ACVARGR_ECOLI_Tsuboyama_2023_1AOYSPA_STAAU_Tsuboyama_2023_1LP1RS15_GEOSE_Tsuboyama_2023_1A32CATR_CHLRE_Tsuboyama_2023_2AMIOTC_HUMAN_Lo_2023OTU7A_HUMAN_Tsuboyama_2023_2L2DSPG2_STRSG_Tsuboyama_2023_5UBSCBX4_HUMAN_Tsuboyama_2023_2K28MYO3_YEAST_Tsuboyama_2023_2BTTMAFG_MOUSE_Tsuboyama_2023_1K1VVILI_CHICK_Tsuboyama_2023_1YU5CP2C9_HUMAN_Amorosi_2021_activityEPHB2_HUMAN_Tsuboyama_2023_1F0MNKX31_HUMAN_Tsuboyama_2023_2L9RBBC1_YEAST_Tsuboyama_2023_1TG0DNJA1_HUMAN_Tsuboyama_2023_2LO1FECA_ECOLI_Tsuboyama_2023_2D1UGRB2_HUMAN_Faure_2021HECD1_HUMAN_Tsuboyama_2023_3DKMBLAT_ECOLX_Stiffler_2015HCP_LAMBD_Tsuboyama_2023_2L6QPIN1_HUMAN_Tsuboyama_2023_1I6CRD23A_HUMAN_Tsuboyama_2023_1IFYA4GRB6_PSEAI_Chen_2020CSN4_MOUSE_Tsuboyama_2023_1UFMRCD1_ARATH_Tsuboyama_2023_5OAOBLAT_ECOLX_Firnberg_2014OBSCN_HUMAN_Tsuboyama_2023_1V1CVRPI_BPT7_Tsuboyama_2023_2WNMVG08_BPP22_Tsuboyama_2023_2GP8TNKS2_HUMAN_Tsuboyama_2023_5JRTTCRG1_MOUSE_Tsuboyama_2023_1E0LUBE4B_HUMAN_Tsuboyama_2023_3L1XSR43C_ARATH_Tsuboyama_2023_2N88SDA_BACSU_Tsuboyama_2023_1PV0P53_HUMAN_Giacomelli_2018_WT_NutlinAMFR_HUMAN_Tsuboyama_2023_4G3OSQSTM_MOUSE_Tsuboyama_2023_2RRURBP1_HUMAN_Tsuboyama_2023_2KWHNUSA_ECOLI_Tsuboyama_2023_1WCLRPC1_BP434_Tsuboyama_2023_1R69PPARG_HUMAN_Majithia_2016NUSG_MYCTU_Tsuboyama_2023_2MI6
ProteinNPT Kermut
(a) Top half
0.00 0.25 0.50 0.75 1.00HIS7_YEAST_Pokusaeva_2019A0A1I9GEU1_NEIME_Kennouche_2019ENV_HV1B9_DuenasDecamp_2016SCN5A_HUMAN_Glazer_2019CALM1_HUMAN_Weile_2017AICDA_HUMAN_Gajula_2014_3cyclesENVZ_ECOLI_Ghose_2023CAS9_STRP1_Spencer_2017_positiveB2L11_HUMAN_Dutta_2010_binding-Mcl-1F7YBW8_MESOW_Aakre_2015F7YBW7_MESOW_Ding_2023TPK1_HUMAN_Weile_2017ENV_HV1BR_Haddox_2016REV_HV1H2_Fernandes_2016A0A140D2T1_ZIKV_Sourisseau_2019RAF1_HUMAN_Zinkus-Boltz_2019TAT_HV1BR_Fernandes_2016CCDB_ECOLI_Tripathi_2016VKOR1_HUMAN_Chiasson_2020_activityCBS_HUMAN_Sun_2020HSP82_YEAST_Cote-Hammarlof_2020_growth-H2O2PA_I34A1_Wu_2015KCNJ2_MOUSE_Coyote-Maestas_2022_functionGLPA_HUMAN_Elazar_2016ACE2_HUMAN_Chan_2020CCR5_HUMAN_Gill_2023KCNH2_HUMAN_Kozek_2020Q6WV13_9MAXI_Somermeyer_2022I6TAH8_I68A0_Doud_2015BRCA2_HUMAN_Erwood_2022_HEK293TGFP_AEQVI_Sarkisyan_2016A0A2Z5U3Z0_9INFA_Wu_2014HEM3_HUMAN_Loggerenberg_2023CD19_HUMAN_Klesmith_2019_FMC_singlesUBE4B_MOUSE_Starita_2013NCAP_I34A1_Doud_2015KCNJ2_MOUSE_Coyote-Maestas_2022_surfaceMK01_HUMAN_Brenan_2016TPOR_HUMAN_Bridgford_2020HSP82_YEAST_Flynn_2019GDIA_HUMAN_Silverstein_2021OXDA_RHOTO_Vanella_2023_activityQ8WTC7_9CNID_Somermeyer_2022A0A247D711_LISMN_Stadelmann_2021A4D664_9INFA_Soh_2019SHOC2_HUMAN_Kwon_2022SRC_HUMAN_Chakraborty_2023_binding-DAS_25uMPAI1_HUMAN_Huttinger_2021DYR_ECOLI_Thompson_2019RDRP_I33A0_Li_2023RPC1_LAMBD_Li_2019_high-expressionDYR_ECOLI_Nguyen_2023SPG1_STRSG_Olson_2014MSH2_HUMAN_Jia_2020SYUA_HUMAN_Newberry_2020POLG_CXB3N_Mattenberger_2021IF1_ECOLI_Kelsic_2016CAR11_HUMAN_Meitlis_2020_gofAACC1_PSEAI_Dandage_2018HXK4_HUMAN_Gersing_2023_abundanceTRPC_THEMA_Chan_2017RPC1_LAMBD_Li_2019_low-expressionLYAM1_HUMAN_Elazar_2016KCNE1_HUMAN_Muhammad_2023_expressionADRB2_HUMAN_Jones_2020SUMO1_HUMAN_Weile_2017PTEN_HUMAN_Mighell_2018ESTA_BACSU_Nutschel_2020SRC_HUMAN_Nguyen_2022BLAT_ECOLX_Deng_2012POLG_HCVJF_Qi_2014UBC9_HUMAN_Weile_2017RL40A_YEAST_Roscoe_2014HSP82_YEAST_Mishra_2016Q837P5_ENTFA_Meier_2023SERC_HUMAN_Xie_2023RL40A_YEAST_Mavor_2016TPMT_HUMAN_Matreyek_2018RL20_AQUAE_Tsuboyama_2023_1GYZOXDA_RHOTO_Vanella_2023_expressionBRCA1_HUMAN_Findlay_2018CAPSD_AAV2S_Sinai_2021A0A192B1T2_9HIV1_Haddox_2018Q2N0S5_9HIV1_Haddox_2018MTHR_HUMAN_Weile_2021GCN4_YEAST_Staller_2018DLG4_RAT_McLaughlin_2012Q837P4_ENTFA_Meier_2023MLAC_ECOLI_MacRae_2023MET_HUMAN_Estevam_2023CCDB_ECOLI_Adkar_2012R1AB_SARS2_Flynn_2022Q53Z42_HUMAN_McShan_2019_binding-TAPBPRILF3_HUMAN_Tsuboyama_2023_2L33HXK4_HUMAN_Gersing_2022_activitySRC_HUMAN_Ahler_2019ERBB2_HUMAN_Elazar_2016RASK_HUMAN_Weng_2022_abundancePOLG_DEN26_Suphatrakul_2023A4_HUMAN_Seuma_2022Q53Z42_HUMAN_McShan_2019_expressionDLG4_HUMAN_Faure_2021HMDH_HUMAN_Jiang_2019PTEN_HUMAN_Matreyek_2021PPM1D_HUMAN_Miller_2022ANCSZ_Hobbs_2022SC6A4_HUMAN_Young_2021GAL4_YEAST_Kitzman_2015
ProteinNPT Kermut (b) Bottom half
Figure O.3: Spearman’s correlation per DMS assay in the modulo split scheme. The figure has been
split in two to fit. In the modulo split we see the clear improvement that the structure kernel offers,
where performance increase over ProteinNPT is significantly higher in for many datasets.
45O.4 Detailed results per DMS (contiguous)
0.00 0.25 0.50 0.75 1.00TADBP_HUMAN_Bolognesi_2019SRC_HUMAN_Ahler_2019PKN1_HUMAN_Tsuboyama_2023_1URFPHOT_CHLRE_Chen_2023CASP7_HUMAN_Roychowdhury_2020RL20_AQUAE_Tsuboyama_2023_1GYZA4_HUMAN_Seuma_2022Q53Z42_HUMAN_McShan_2019_expressionPPM1D_HUMAN_Miller_2022NUD15_HUMAN_Suiter_2020CAPSD_AAV2S_Sinai_2021ANCSZ_Hobbs_2022RNC_ECOLI_Weeks_2023VKOR1_HUMAN_Chiasson_2020_abundanceRASH_HUMAN_Bandaru_2017RASK_HUMAN_Weng_2022_binding-DARPin_K55Q59976_STRSQ_Romero_2015D7PM05_CLYGR_Somermeyer_2022FKBP3_HUMAN_Tsuboyama_2023_2KFVKCNE1_HUMAN_Muhammad_2023_functionTRPC_SACS2_Chan_2017CASP3_HUMAN_Roychowdhury_2020RAD_ANTMA_Tsuboyama_2023_2CJJS22A1_HUMAN_Yee_2023_activitySPIKE_SARS2_Starr_2020_bindingSOX30_HUMAN_Tsuboyama_2023_7JJKRL40A_YEAST_Roscoe_2013AMIE_PSEAE_Wrenbeck_2017CAR11_HUMAN_Meitlis_2020_lofBCHB_CHLTE_Tsuboyama_2023_2KRUMTH3_HAEAE_RockahShmuel_2015RFAH_ECOLI_Tsuboyama_2023_2LCLPSAE_SYNP2_Tsuboyama_2023_1PSECP2C9_HUMAN_Amorosi_2021_abundanceP53_HUMAN_Kotler_2018POLG_PESV_Tsuboyama_2023_2MXDPABP_YEAST_Melamed_2013S22A1_HUMAN_Yee_2023_abundanceUBR5_HUMAN_Tsuboyama_2023_1I2TODP2_GEOSE_Tsuboyama_2023_1W4GOPSD_HUMAN_Wan_2019BLAT_ECOLX_Jacquier_2013KKA2_KLEPN_Melnikov_2014SBI_STAAM_Tsuboyama_2023_2JVGP53_HUMAN_Giacomelli_2018_WT_NutlinPR40A_HUMAN_Tsuboyama_2023_1UZCRCRO_LAMBD_Tsuboyama_2023_1ORCPRKN_HUMAN_Clausen_2023SAV1_MOUSE_Tsuboyama_2023_2YSBPITX2_HUMAN_Tsuboyama_2023_2L7MSCIN_STAAR_Tsuboyama_2023_2QFFISDH_STAAW_Tsuboyama_2023_2LHRCP2C9_HUMAN_Amorosi_2021_activityCUE1_YEAST_Tsuboyama_2023_2MYXP84126_THETH_Chan_2017NPC1_HUMAN_Erwood_2022_RPE1THO1_YEAST_Tsuboyama_2023_2WQGDOCK1_MOUSE_Tsuboyama_2023_2M0YOTU7A_HUMAN_Tsuboyama_2023_2L2DSPTN1_CHICK_Tsuboyama_2023_1TUDSPG1_STRSG_Wu_2016SPIKE_SARS2_Starr_2020_expressionDN7A_SACS2_Tsuboyama_2023_1JICCATR_CHLRE_Tsuboyama_2023_2AMIRS15_GEOSE_Tsuboyama_2023_1A32YAIA_ECOLI_Tsuboyama_2023_2KVTSRBS1_HUMAN_Tsuboyama_2023_2O2WNPC1_HUMAN_Erwood_2022_HEK293TSPG2_STRSG_Tsuboyama_2023_5UBSYNZC_BACSU_Tsuboyama_2023_2JVDGRB2_HUMAN_Faure_2021MBD11_ARATH_Tsuboyama_2023_6ACVARGR_ECOLI_Tsuboyama_2023_1AOYSPA_STAAU_Tsuboyama_2023_1LP1OTC_HUMAN_Lo_2023CBX4_HUMAN_Tsuboyama_2023_2K28MYO3_YEAST_Tsuboyama_2023_2BTTBLAT_ECOLX_Stiffler_2015CBPA2_HUMAN_Tsuboyama_2023_1O6XBBC1_YEAST_Tsuboyama_2023_1TG0VILI_CHICK_Tsuboyama_2023_1YU5NKX31_HUMAN_Tsuboyama_2023_2L9RHCP_LAMBD_Tsuboyama_2023_2L6QHECD1_HUMAN_Tsuboyama_2023_3DKMUBE4B_HUMAN_Tsuboyama_2023_3L1XBLAT_ECOLX_Firnberg_2014MAFG_MOUSE_Tsuboyama_2023_1K1VFECA_ECOLI_Tsuboyama_2023_2D1UA4GRB6_PSEAI_Chen_2020CSN4_MOUSE_Tsuboyama_2023_1UFMPIN1_HUMAN_Tsuboyama_2023_1I6COBSCN_HUMAN_Tsuboyama_2023_1V1CDNJA1_HUMAN_Tsuboyama_2023_2LO1VRPI_BPT7_Tsuboyama_2023_2WNMEPHB2_HUMAN_Tsuboyama_2023_1F0MSDA_BACSU_Tsuboyama_2023_1PV0VG08_BPP22_Tsuboyama_2023_2GP8RCD1_ARATH_Tsuboyama_2023_5OAOAMFR_HUMAN_Tsuboyama_2023_4G3OTNKS2_HUMAN_Tsuboyama_2023_5JRTTCRG1_MOUSE_Tsuboyama_2023_1E0LRD23A_HUMAN_Tsuboyama_2023_1IFYSR43C_ARATH_Tsuboyama_2023_2N88PPARG_HUMAN_Majithia_2016RBP1_HUMAN_Tsuboyama_2023_2KWHRPC1_BP434_Tsuboyama_2023_1R69SQSTM_MOUSE_Tsuboyama_2023_2RRUNUSA_ECOLI_Tsuboyama_2023_1WCLNUSG_MYCTU_Tsuboyama_2023_2MI6
ProteinNPT Kermut
(a) Top half
0.00 0.25 0.50 0.75 1.00A0A1I9GEU1_NEIME_Kennouche_2019ENV_HV1B9_DuenasDecamp_2016HIS7_YEAST_Pokusaeva_2019CALM1_HUMAN_Weile_2017NRAM_I33A0_Jiang_2016SCN5A_HUMAN_Glazer_2019B2L11_HUMAN_Dutta_2010_binding-Mcl-1ENVZ_ECOLI_Ghose_2023PA_I34A1_Wu_2015CAS9_STRP1_Spencer_2017_positiveF7YBW7_MESOW_Ding_2023AICDA_HUMAN_Gajula_2014_3cyclesF7YBW8_MESOW_Aakre_2015TPK1_HUMAN_Weile_2017ENV_HV1BR_Haddox_2016REV_HV1H2_Fernandes_2016MSH2_HUMAN_Jia_2020A4D664_9INFA_Soh_2019A0A140D2T1_ZIKV_Sourisseau_2019I6TAH8_I68A0_Doud_2015GLPA_HUMAN_Elazar_2016HSP82_YEAST_Cote-Hammarlof_2020_growth-H2O2Q6WV13_9MAXI_Somermeyer_2022VKOR1_HUMAN_Chiasson_2020_activityGFP_AEQVI_Sarkisyan_2016ACE2_HUMAN_Chan_2020NCAP_I34A1_Doud_2015CBS_HUMAN_Sun_2020RAF1_HUMAN_Zinkus-Boltz_2019A0A2Z5U3Z0_9INFA_Wu_2014MK01_HUMAN_Brenan_2016KCNJ2_MOUSE_Coyote-Maestas_2022_functionCCDB_ECOLI_Adkar_2012CCR5_HUMAN_Gill_2023CD19_HUMAN_Klesmith_2019_FMC_singlesGDIA_HUMAN_Silverstein_2021TPOR_HUMAN_Bridgford_2020CCDB_ECOLI_Tripathi_2016KCNJ2_MOUSE_Coyote-Maestas_2022_surfaceDYR_ECOLI_Thompson_2019UBC9_HUMAN_Weile_2017POLG_CXB3N_Mattenberger_2021HEM3_HUMAN_Loggerenberg_2023BLAT_ECOLX_Deng_2012HSP82_YEAST_Flynn_2019SYUA_HUMAN_Newberry_2020BRCA2_HUMAN_Erwood_2022_HEK293TPOLG_HCVJF_Qi_2014RPC1_LAMBD_Li_2019_high-expressionRDRP_I33A0_Li_2023KCNH2_HUMAN_Kozek_2020TAT_HV1BR_Fernandes_2016Q2N0S5_9HIV1_Haddox_2018UBE4B_MOUSE_Starita_2013Q8WTC7_9CNID_Somermeyer_2022SHOC2_HUMAN_Kwon_2022TRPC_THEMA_Chan_2017DYR_ECOLI_Nguyen_2023PAI1_HUMAN_Huttinger_2021OXDA_RHOTO_Vanella_2023_activityPTEN_HUMAN_Mighell_2018SRC_HUMAN_Chakraborty_2023_binding-DAS_25uMLYAM1_HUMAN_Elazar_2016AACC1_PSEAI_Dandage_2018A0A192B1T2_9HIV1_Haddox_2018A0A247D711_LISMN_Stadelmann_2021CAR11_HUMAN_Meitlis_2020_gofIF1_ECOLI_Kelsic_2016SPG1_STRSG_Olson_2014HXK4_HUMAN_Gersing_2023_abundanceDLG4_RAT_McLaughlin_2012P53_HUMAN_Giacomelli_2018_Null_EtoposideQ837P4_ENTFA_Meier_2023ADRB2_HUMAN_Jones_2020SRC_HUMAN_Nguyen_2022BRCA1_HUMAN_Findlay_2018ERBB2_HUMAN_Elazar_2016PTEN_HUMAN_Matreyek_2021R1AB_SARS2_Flynn_2022ESTA_BACSU_Nutschel_2020GCN4_YEAST_Staller_2018SUMO1_HUMAN_Weile_2017RL40A_YEAST_Roscoe_2014OXDA_RHOTO_Vanella_2023_expressionILF3_HUMAN_Tsuboyama_2023_2L33KCNE1_HUMAN_Muhammad_2023_expressionSC6A4_HUMAN_Young_2021RPC1_LAMBD_Li_2019_low-expressionMTHR_HUMAN_Weile_2021SERC_HUMAN_Xie_2023RL40A_YEAST_Mavor_2016DLG4_HUMAN_Faure_2021LGK_LIPST_Klesmith_2015Q837P5_ENTFA_Meier_2023TPMT_HUMAN_Matreyek_2018MLAC_ECOLI_MacRae_2023POLG_DEN26_Suphatrakul_2023Q53Z42_HUMAN_McShan_2019_binding-TAPBPRHSP82_YEAST_Mishra_2016A0A2Z5U3Z0_9INFA_Doud_2016MET_HUMAN_Estevam_2023C6KNH7_9INFA_Lee_2018GAL4_YEAST_Kitzman_2015RASK_HUMAN_Weng_2022_abundanceHXK4_HUMAN_Gersing_2022_activityYAP1_HUMAN_Araya_2012P53_HUMAN_Giacomelli_2018_Null_NutlinHMDH_HUMAN_Jiang_2019
ProteinNPT Kermut (b) Bottom half
Figure O.4: Spearman’s correlation per DMS assay in the contiguous split scheme. The figure has
been split in two to fit.
P Ethics
We have introduced a general framework to predict variant effects given labeled data. The intent of
our work is to use the framework to model and subsequently optimize proteins. We acknowledge
that – in principle – any protein property can be modeled (depending on the available data), which
means that potentially harmful proteins can be engineered using our method. We encourage the
community to use our proposed method for beneficial purposes only, such as the engineering of
efficient enzymes or for the characterization of potentially pathogenic variants for the betterment of
biological interpretation and clinical treatment.
46NeurIPS Paper Checklist
1.Claims
Question: Do the main claims made in the abstract and introduction accurately reflect the
paper’s contributions and scope?
Answer: [Yes]
Justification: The contributions are listed in the introduction (Section 1) and match the
results (Section 4) as described in the discussion (Section 5).
Guidelines:
•The answer NA means that the abstract and introduction do not include the claims
made in the paper.
•The abstract and/or introduction should clearly state the claims made, including the
contributions made in the paper and important assumptions and limitations. A No or
NA answer to this question will not be perceived well by the reviewers.
•The claims made should match theoretical and experimental results, and reflect how
much the results can be expected to generalize to other settings.
•It is fine to include aspirational goals as motivation as long as it is clear that these goals
are not attained by the paper.
2.Limitations
Question: Does the paper discuss the limitations of the work performed by the authors?
Answer: [Yes]
Justification: Limitations are outlined in the discussion (Section 5).
Guidelines:
•The answer NA means that the paper has no limitation while the answer No means that
the paper has limitations, but those are not discussed in the paper.
• The authors are encouraged to create a separate "Limitations" section in their paper.
•The paper should point out any strong assumptions and how robust the results are to
violations of these assumptions (e.g., independence assumptions, noiseless settings,
model well-specification, asymptotic approximations only holding locally). The authors
should reflect on how these assumptions might be violated in practice and what the
implications would be.
•The authors should reflect on the scope of the claims made, e.g., if the approach was
only tested on a few datasets or with a few runs. In general, empirical results often
depend on implicit assumptions, which should be articulated.
•The authors should reflect on the factors that influence the performance of the approach.
For example, a facial recognition algorithm may perform poorly when image resolution
is low or images are taken in low lighting. Or a speech-to-text system might not be
used reliably to provide closed captions for online lectures because it fails to handle
technical jargon.
•The authors should discuss the computational efficiency of the proposed algorithms
and how they scale with dataset size.
•If applicable, the authors should discuss possible limitations of their approach to
address problems of privacy and fairness.
•While the authors might fear that complete honesty about limitations might be used by
reviewers as grounds for rejection, a worse outcome might be that reviewers discover
limitations that aren’t acknowledged in the paper. The authors should use their best
judgment and recognize that individual actions in favor of transparency play an impor-
tant role in developing norms that preserve the integrity of the community. Reviewers
will be specifically instructed to not penalize honesty concerning limitations.
3.Theory Assumptions and Proofs
Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?
Answer: [Yes]
47Justification: See Section 3.3 for theoretical results and Appendix B for further details and
proofs.
Guidelines:
• The answer NA means that the paper does not include theoretical results.
•All the theorems, formulas, and proofs in the paper should be numbered and cross-
referenced.
•All assumptions should be clearly stated or referenced in the statement of any theorems.
•The proofs can either appear in the main paper or the supplemental material, but if
they appear in the supplemental material, the authors are encouraged to provide a short
proof sketch to provide intuition.
•Inversely, any informal proof provided in the core of the paper should be complemented
by formal proofs provided in appendix or supplemental material.
• Theorems and Lemmas that the proof relies upon should be properly referenced.
4.Experimental Result Reproducibility
Question: Does the paper fully disclose all the information needed to reproduce the main ex-
perimental results of the paper to the extent that it affects the main claims and/or conclusions
of the paper (regardless of whether the code and data are provided or not)?
Answer: [Yes]
Justification: Implementation details (both software and data) can be found in Appendix C.
Guidelines:
• The answer NA means that the paper does not include experiments.
•If the paper includes experiments, a No answer to this question will not be perceived
well by the reviewers: Making the paper reproducible is important, regardless of
whether the code and data are provided or not.
•If the contribution is a dataset and/or model, the authors should describe the steps taken
to make their results reproducible or verifiable.
•Depending on the contribution, reproducibility can be accomplished in various ways.
For example, if the contribution is a novel architecture, describing the architecture fully
might suffice, or if the contribution is a specific model and empirical evaluation, it may
be necessary to either make it possible for others to replicate the model with the same
dataset, or provide access to the model. In general. releasing code and data is often
one good way to accomplish this, but reproducibility can also be provided via detailed
instructions for how to replicate the results, access to a hosted model (e.g., in the case
of a large language model), releasing of a model checkpoint, or other means that are
appropriate to the research performed.
•While NeurIPS does not require releasing code, the conference does require all submis-
sions to provide some reasonable avenue for reproducibility, which may depend on the
nature of the contribution. For example
(a)If the contribution is primarily a new algorithm, the paper should make it clear how
to reproduce that algorithm.
(b)If the contribution is primarily a new model architecture, the paper should describe
the architecture clearly and fully.
(c)If the contribution is a new model (e.g., a large language model), then there should
either be a way to access this model for reproducing the results or a way to reproduce
the model (e.g., with an open-source dataset or instructions for how to construct
the dataset).
(d)We recognize that reproducibility may be tricky in some cases, in which case
authors are welcome to describe the particular way they provide for reproducibility.
In the case of closed-source models, it may be that access to the model is limited in
some way (e.g., to registered users), but it should be possible for other researchers
to have some path to reproducing or verifying the results.
5.Open access to data and code
Question: Does the paper provide open access to the data and code, with sufficient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?
48Answer: [Yes]
Justification: The full codebase can be found in an attached zip archive, while a public
GitHub repository will be made available when anonymity is no longer an issue. All data is
extracted directly from the ProteinGym repository, according to their instructions.
Guidelines:
• The answer NA means that paper does not include experiments requiring code.
•Please see the NeurIPS code and data submission guidelines ( https://nips.cc/
public/guides/CodeSubmissionPolicy ) for more details.
•While we encourage the release of code and data, we understand that this might not be
possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
including code, unless this is central to the contribution (e.g., for a new open-source
benchmark).
•The instructions should contain the exact command and environment needed to run to
reproduce the results. See the NeurIPS code and data submission guidelines ( https:
//nips.cc/public/guides/CodeSubmissionPolicy ) for more details.
•The authors should provide instructions on data access and preparation, including how
to access the raw data, preprocessed data, intermediate data, and generated data, etc.
•The authors should provide scripts to reproduce all experimental results for the new
proposed method and baselines. If only a subset of experiments are reproducible, they
should state which ones are omitted from the script and why.
•At submission time, to preserve anonymity, the authors should release anonymized
versions (if applicable).
•Providing as much information as possible in supplemental material (appended to the
paper) is recommended, but including URLs to data and code is permitted.
6.Experimental Setting/Details
Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?
Answer: [Yes]
Justification: All experimental details are described in Appendix C.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The experimental setting should be presented in the core of the paper to a level of detail
that is necessary to appreciate the results and make sense of them.
•The full details can be provided either with the code, in appendix, or as supplemental
material.
7.Experiment Statistical Significance
Question: Does the paper report error bars suitably and correctly defined or other appropriate
information about the statistical significance of the experiments?
Answer: [Yes]
Justification: The figures containing errorbars explicitly state which type of errorbar is
shown. For the main results, the non-parametric bootstrap error is included in Table E.2.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The authors should answer "Yes" if the results are accompanied by error bars, confi-
dence intervals, or statistical significance tests, at least for the experiments that support
the main claims of the paper.
•The factors of variability that the error bars are capturing should be clearly stated (for
example, train/test split, initialization, random drawing of some parameter, or overall
run with given experimental conditions).
•The method for calculating the error bars should be explained (closed form formula,
call to a library function, bootstrap, etc.)
49• The assumptions made should be given (e.g., Normally distributed errors).
•It should be clear whether the error bar is the standard deviation or the standard error
of the mean.
•It is OK to report 1-sigma error bars, but one should state it. The authors should
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
of Normality of errors is not verified.
•For asymmetric distributions, the authors should be careful not to show in tables or
figures symmetric error bars that would yield results that are out of range (e.g. negative
error rates).
•If error bars are reported in tables or plots, The authors should explain in the text how
they were calculated and reference the corresponding figures or tables in the text.
8.Experiments Compute Resources
Question: For each experiment, does the paper provide sufficient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?
Answer: [Yes]
Justification: Both computer resources and examples of execution time are described in
Appendix C.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The paper should indicate the type of compute workers CPU or GPU, internal cluster,
or cloud provider, including relevant memory and storage.
•The paper should provide the amount of compute required for each of the individual
experimental runs as well as estimate the total compute.
•The paper should disclose whether the full research project required more compute
than the experiments reported in the paper (e.g., preliminary or failed experiments that
didn’t make it into the paper).
9.Code Of Ethics
Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines ?
Answer: [Yes]
Justification: Ethical considerations are discussed in Appendix P.
Guidelines:
•The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
•If the authors answer No, they should explain the special circumstances that require a
deviation from the Code of Ethics.
•The authors should make sure to preserve anonymity (e.g., if there is a special consid-
eration due to laws or regulations in their jurisdiction).
10.Broader Impacts
Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?
Answer: [Yes]
Justification: Broader impacts of our model are discussed in Appendix P.
Guidelines:
• The answer NA means that there is no societal impact of the work performed.
•If the authors answer NA or No, they should explain why their work has no societal
impact or why the paper does not address societal impact.
•Examples of negative societal impacts include potential malicious or unintended uses
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
(e.g., deployment of technologies that could make decisions that unfairly impact specific
groups), privacy considerations, and security considerations.
50•The conference expects that many papers will be foundational research and not tied
to particular applications, let alone deployments. However, if there is a direct path to
any negative applications, the authors should point it out. For example, it is legitimate
to point out that an improvement in the quality of generative models could be used to
generate deepfakes for disinformation. On the other hand, it is not needed to point out
that a generic algorithm for optimizing neural networks could enable people to train
models that generate Deepfakes faster.
•The authors should consider possible harms that could arise when the technology is
being used as intended and functioning correctly, harms that could arise when the
technology is being used as intended but gives incorrect results, and harms following
from (intentional or unintentional) misuse of the technology.
•If there are negative societal impacts, the authors could also discuss possible mitigation
strategies (e.g., gated release of models, providing defenses in addition to attacks,
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
feedback over time, improving the efficiency and accessibility of ML).
11.Safeguards
Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?
Answer: [No]
Justification: We have not described safeguards due to the low risk of misuse, while we
however describe potential misuse in Appendix P.
Guidelines:
• The answer NA means that the paper poses no such risks.
•Released models that have a high risk for misuse or dual-use should be released with
necessary safeguards to allow for controlled use of the model, for example by requiring
that users adhere to usage guidelines or restrictions to access the model or implementing
safety filters.
•Datasets that have been scraped from the Internet could pose safety risks. The authors
should describe how they avoided releasing unsafe images.
•We recognize that providing effective safeguards is challenging, and many papers do
not require this, but we encourage authors to take this into account and make a best
faith effort.
12.Licenses for existing assets
Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?
Answer: [Yes]
Justification: Credit is given to the benchmark data and software in Appendix D.
Guidelines:
• The answer NA means that the paper does not use existing assets.
• The authors should cite the original paper that produced the code package or dataset.
•The authors should state which version of the asset is used and, if possible, include a
URL.
• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
•For scraped data from a particular source (e.g., website), the copyright and terms of
service of that source should be provided.
•If assets are released, the license, copyright information, and terms of use in the
package should be provided. For popular datasets, paperswithcode.com/datasets
has curated licenses for some datasets. Their licensing guide can help determine the
license of a dataset.
•For existing datasets that are re-packaged, both the original license and the license of
the derived asset (if it has changed) should be provided.
51•If this information is not available online, the authors are encouraged to reach out to
the asset’s creators.
13.New Assets
Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?
Answer: [Yes]
Justification: The codebase falls under the open source MIT Licence (Appendix A). Docu-
mentation can be found throughout the codebase and specifically in the README file in the
anonymous zip archive.
Guidelines:
• The answer NA means that the paper does not release new assets.
•Researchers should communicate the details of the dataset/code/model as part of their
submissions via structured templates. This includes details about training, license,
limitations, etc.
•The paper should discuss whether and how consent was obtained from people whose
asset is used.
•At submission time, remember to anonymize your assets (if applicable). You can either
create an anonymized URL or include an anonymized zip file.
14.Crowdsourcing and Research with Human Subjects
Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as
well as details about compensation (if any)?
Answer: [NA]
Justification: No crowdsourcing or research with human subjects was done in this study.
Guidelines:
•The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
•Including this information in the supplemental material is fine, but if the main contribu-
tion of the paper involves human subjects, then as much detail as possible should be
included in the main paper.
•According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
or other labor should be paid at least the minimum wage in the country of the data
collector.
15.Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects
Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?
Answer: [NA]
Justification: The study did not include any participants.
Guidelines:
•The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
•Depending on the country in which research is conducted, IRB approval (or equivalent)
may be required for any human subjects research. If you obtained IRB approval, you
should clearly state this in the paper.
•We recognize that the procedures for this may vary significantly between institutions
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
guidelines for their institution.
•For initial submissions, do not include any information that would break anonymity (if
applicable), such as the institution conducting the review.
52