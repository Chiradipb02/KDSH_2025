Under review as submission to TMLR
Posterior Sampling for Reinforcement Learning on Graphs
Anonymous authors
Paper under double-blind review
Abstract
Many Markov Decision Processes (MDPs) exhibit structure in their state and action spaces
that is not exploited. We consider the case where the structure can be modelled using
a directed acyclic graph (DAG) composed of nodes and edges, allowing for a more basic
or "atomic" representation of the state and action spaces. In this case, each node has a
state, and the state transition dynamics are influenced by the states and actions at its
parent nodes. We propose an MDP framework, Directed Acyclic Markov Decision Process
(DAMDP), that formalises this problem and algorithms to perform planning and learning.
Crucially, DAMDPs retain many of the benefits of MDPs, as we can show that Dynamic
Programming can find the optimal policy in known DAMDPs. We also demonstrate how
to perform Reinforcement Learning in DAMDPs when the transition probabilities and the
reward function are unknown. To this end, we derive a posterior sampling-based algorithm
that is able to leverage the graph structure to boost learning efficiency. Moreover, we ob-
tain a theoretical bound on the Bayesian regret for this algorithm, which directly shows
the efficiency gain from considering the graph structure. We then conclude by empirically
demonstratingthatbyharnessingtheDAMDP,ouralgorithmoutperformstraditionalposte-
rior sampling for Reinforcement Learning in both a maximum flow problem and a real-world
wind farm optimisation task.
1 Introduction
Reinforcement Learning (RL) algorithms are typically used to solve Markov Decision Processes (MDPs),
a general model for sequential decision-making under uncertainty in which the state of the environment
only depends on what happened in the previous time step. There exist many online reinforcement learning
approaches for finding an optimal policy in an unknown MDP (e.g. Watkins & Dayan (1992); Sutton et al.
(1999); Auer et al. (2008); Azar et al. (2017); Strens (2000)). However, these methods tend to be quite
general and do not make any assumptions beyond the MDP structure. This generality means that while
these methods can be applied to most MDP problems, they will fail to exploit any additional structure
present in the problem and, as such, can be sub-optimal in specific cases.
Inthispaper, weconsideraspecialcaseofMarkovDecisionProcessesinwhichtheenvironment’sdependency
structure is encoded in a directed acyclic graph. In such a setting, the state space can be decomposed into
the state at the nodes of the graph, and the reward and transitions depend on the connectivity of each node.
In particular, the state at each node of the graph will depend on the state and action taken at all its parent
nodes.
This structure is present in a lot of real-world settings. For example, in wind farm optimisation, e.g. Abkar
et al. (2023), a grid of rotating wind turbines creates a stream of turbulence that impacts the yield of
downstream wind turbines. The turbulence at downstream wind turbines depends on the orientation of the
upstream wind turbine. Finding the optimal orientation of all wind turbines in the wind farm becomes an
interesting control problem. Since the turbines’ locations are known, the interaction pattern between all
turbines composing the farm is also known. This interaction pattern can be encoded as a directed acyclic
graph, where each node represents a wind turbine and each edge indicates a potential interaction. The
setting we consider also covers any problem formulated as a maximum flow problem (i.e. network routing
Mammeri (2019) or supply chain management Rolf et al. (2023)). Such problems are typically modelled as
1Under review as submission to TMLR
directed acyclic graphs where the state at each node depends on the action taken at the parent nodes, and
we observe the outcome of the actions at the parent nodes before taking actions at the child nodes.
This paper will show that leveraging this latent graphical structure can significantly improve performance.
These improved results are achieved by a careful analysis of the proposed algorithm that breaks down
the combinatorial structure of the state and action spaces and also reuses repeated patterns within the
graph. Note that while there are some similarities between the DAMDPs that we consider and the Factored
MDP (FMDP) framework Boutilier et al. (2000), there are significant differences that necessitate further
innovation. As elaborated in Section 7, the main difference is that FMDPs consider a time-homogeneous
setting, where the state factorisation remains the same for every time step. On the other hand, we consider
a time inhomogeneous setting where the factorisation structure might change over time; this allows us to
capture more complex and realistic graph structures.
Contribution: To demonstrate the efficiency gains from leveraging a latent graphical structure, we start by
formalizing the sub-class of MDPs we focus on and then show that when the dynamics are known, a dynamic
programming approach can be applied to find an optimal policy. When the rewards and transitions are
unknown, we develop a posterior sampling algorithm and show an upper bound on its Bayesian regret. This
upper bound demonstrates an improvement compared to methods that ignore the latent graphical structure,
such as Auer et al. (2008); Osband et al. (2013). We also provide empirical validation of our method’s
performance gain, first on a maximum flow problem and then on a wind farm optimization problem. To
summarize, this paper proposes, analyses and evaluates a novel posterior sampling algorithm specifically
designed to exploit the graphical structure present in many real-world problems.
2 Background
Before we introduce our decision-making framework, it is helpful to recall the foundations of Markov Decision
Processes and Directed Acyclic Graphs.
2.1 Fixed-Horizon Markov Decision Process
A fixed-horizon time-inhomogeneous MDP is a tuple M=⟨{St}H
t=1,{At}H
t=1,{Rt}H
t=1,{Pt}H
t=1,H,ρ⟩. We
consider finite time-dependent state spaces Stand action spaces Atfort∈{1,...,H}. The mean reward
function is defined by Rt:St×At→[0,1]for allt∈[H]. In state st, after performing action at, the
agent observes a reward Rt(st,at) +ηt, whereηtare independent identically distributed sub-Gaussian noise
(σ= 1). The probability of reaching a specific state st+1∈St+1when action at∈Atwas performed in
statest∈Stis determined by Pt(st+1|st,at). The agent interacts with the environment during episodes of
lengthH, and in each episode, the initial state s1∼ρis drawn from the initial state distribution. For the
followingHtime steps, t∈{1,···,H}, the agent observes a state st∈Stand decides to perform an action
at; the result of this action is immediately observed as a new state st+1∼Pt(·|st,at)and an immediate
rewardRt(st,at) +ηt.
Reinforcement learning algorithms aim to find policies µt:St→Atfor allt∈{1,···,H}that maximise
the cumulative reward over an episode. We measure this in terms of the value function:
VM
µ,t(st) =E/bracketleftbiggH/summationdisplay
h=tRh(sh,ah)/vextendsingle/vextendsingle/vextendsingle/vextendsingleah=µh(sh),sh+1∼Ph(·|sh,ah),st=s/bracketrightbigg
, (1)
where the expectation is taken over the starting state distribution s1∼ρand the stochastic transition
dynamicssh+1∼Ph(·|sh,ah). Often, we evaluate the quality of a policy µin terms of the value function of
the initial state s1,VM
µ,1(s1), where the superscript Mindicates that we compute the value in the MDP M.
The aim is to find an optimal policy µ∗within the set of Markov deterministic policies Π, which maximizes
the value function,
µ∗∈arg max
µ∈ΠVM
µ,1(s1). (2)
It is also helpful to define the Q-function as the expected reward the policy can obtain given that at time tin
statest∈Stthe actionat∈Atis executed, Qµ,t(st,at) =rt(st,at) +/summationtext
st+1∈St+1Pt(st+1|st,at)Vµ,t+1(st+1).
2Under review as submission to TMLR
Directed Acyclic MDP Full states All contexts
12
34
t=1 t=2 t=3s1=/bracketleftbig
1/bracketrightbig
s2=/bracketleftbig
2,3/bracketrightbig
s3=/bracketleftbig
4/bracketrightbigyReward:r(x,y),
x
xyTransition: C(x)
x
x
Figure 1: Illustration of the important components of DAMDPs. Inside the leftmost box is an illustration
of a simple DAMDP, MG. The underlying graph has four nodes, which are visited within three time steps,
t={1,2,3}. The box in the middle illustrates the full state representation of MG. At time step t, thefull
statestconsists of the concatenation of the observations at each node in layer t. The rightmost box shows
in the first column the two atomic reward contexts that arise in MG, which is composed of the atomic state
xand theatomic action yof the current node. The second column shows the two atomic transition contexts
that arise in MG, which consists of the atomic states andatomic actions observed at the current node, x’s,
parents.
2.2 Directed Acyclic Graphs
Definition 2.1 (Directed Acyclic Graph) .A directed acyclic graph (DAG) G= (V,E)is defined by a set
of verticesVand a set of edges E. Each edge has a direction associated with it. The graph is said to be
acyclic if there is no node for which there exists a path (with one or more edges) that leads to itself.
For any DAG, there is a corresponding topological ordering of the nodes (Bang-Jensen & Gutin, 2008,
Sec. 2.3.2). It assigns a layer to each node; the layer of a node is equal to the length of the longest path
from the root to the current node. If the DAG does not have a root we can trivially add an artificial node
that serves as a root. In this work, we consider this form of layered directed acyclic graphs , whose edges only
connect nodes from adjacent layers.
Definition 2.2 (Layered Directed Acyclic Graphs) .A Layered, Directed Acyclic Graph (LDAG) is a DAG
with a specific topology. It has a unique root node and a unique leaf node. All nodes are organised in layers,
and all edges go from one layer to the next. Edges between nodes of the same layer are not allowed, as well
as edges that connect nodes that are two or more layers away.
With such an ordering, it is clear that all edges go from a layer lto a layerl+ 1withl∈{1,···,H−1}.
Larger steps, backward steps, and transversal steps (within the same layer) are not allowed. Note that
the assumption of LDAG does not lose any generality. We show in Appendix E.1 that for any DAG, we
can construct an LDAG. Since the corresponding LDAG conserves the same connectivity patterns, such
transformation has a limited impact on the complexity measure considered in this article. We illustrate and
discuss this point further in Appendix E.
3 Directed Acyclic Markov Decision Process
We consider a particular sub-class of MDPs that can be rolled out on an LDAG. We define a Directed
Acyclic Markov Decision Process (DAMDP) as a tuple MG=⟨X,{Yj}Ur
j=1,{rj}Ur
j=1,{pi}Uτ
i=1,H,{ρv
A}n1
v=1,G⟩.
An illustration of a simple DAMDP is given in the leftmost plot of Figure 1. Critically, in addition to the
MDP components (described below), a DAMDP MGis structured by an LDAG, G. The horizon Hof the
DAMDP corresponds to the number of layers in G= (V,E)since we sequentially observe the graph’s layers
and all edges e∈Econnect a node from a layer lto a node in layer l+ 1, forl∈{1,···H−1}. At each time
3Under review as submission to TMLR
step, a layer of nodes of Gdescribes the current state of the DAMDP; that is, in layer t, thentnodes that
compose this layer describe the full state at time step t. For each layer t, we consider an arbitrary ordering
of its nodes{1,···,nt}, which remains fixed for the entirety of the training. The edges of Gencode the
dependence between different nodes.
In the DAMDP context, each node of Ghas anatomic state x, wherexbelongs to the atomic state space
X. Similarly, at each node, we can perform an atomic action y∈Yjwhere{Yj}Ur
j=1denote the Urdifferent
atomic action spaces1. When the agent performs an atomic action yv
t∈Yjat a node which is in atomic
statexv
t, it receives a reward rj(xv
t,yv
t)+ηt,v, whererj(xv
t,yv
t)is the mean atomic reward function associated
with thejthatomic action space, ηt,vis i.i.d. sub-gaussian noise, and t∈{1,···,H}denotes the current
layer. Here, and throughout, v∈{1,···,nt}denotes the position of the current node in its layer, and we
use the notation xv
tandyv
tto refer to the vthatomic state and action in the tthlayer. If two nodes have the
same number of parents and if their parents have the same atomic action spaces, then these two nodes obey
the same dynamics and are said to belong to the same transition equivalence class [τi], withi∈{1,···Uτ}
andUτthe number of equivalence classes. Formally, this implies that the dynamics of nodes that belong
to the same equivalence class [τi]are governed by the same atomic transition distribution pi(·|c), withc
the transition context, which consists of the atomic state and actions at all parent nodes. Concretely, let’s
consider the vthnode of layer tand assume that this node belongs to the equivalence class [τit,v], where
here we use the notation it,vto denote the index of the equivalence class that node vin layertbelongs to.
The atomic state at this node is drawn from the atomic transition function xv
t∼pit,v(·|cv
t). The transition
contextcv
t∈Cit,vconsists of the concatenation of all parent’s nodes atomic states andatomic actions .Cit,v
is the space of all possible transition contexts and is the same for all nodes in the same equivalence class. We
denote byVithe set containing the parent nodes of the node of interest, then the space of transition contexts
for theithequivalence class is, Ci=/circlemultiplytext
v∈ViX×Yjv, the Cartesian product of all possible configuration of
the parent nodes atomic states andatomic actions . Finally, the initial atomic state components are sampled
from initial atomic state distribution xv
1∼ρv
Afor all nodes of the initial layer of G, v∈{1,···,n1}.
3.1 Relationship between DAMDPs and MDPs
A DAMDP MG=⟨X,{Yj}Ur
j=1,{rj}Ur
j=1,{pi}Uτ
i=1,H,ρA,G⟩is a special case of a finite horizon MDP, where
the state and action spaces are time-dependent, but the atomic dynamics are stationary. While the number
of nodes and the connection patterns might vary from one layer to the next, the atomic reward function and
the atomic transition function behaviours remain unchanged. Below, we discuss the relationship between
MGand the corresponding MDP Mconstructed from the atomic components and the graph G.
Time step and horizon: The execution of a DAMDP episode is tightly linked to the graph G. Since the
graph is layered, directed and acyclic, each layer lcorresponds to a time step tin the corresponding MDP
M. Hence, the depth Hof the graph corresponds to the MDP’s horizon. After executing K episodes, the
total number of time steps is T=HK.
State space: In any time step t, thefull statest∈Stis composed of the atomic state at each node in the
graph’stthlayer. Formally, in layer twe observe st= [x1
t,···,xnt
t], wherexv
tis the observation collected in
thevthnode of layer t(see Fig. 1, middle box) and ntdenotes the number of nodes in layer t. For simplicity,
we assume that each node has the same atomic state space, X. Then the full state space at time step tis the
Cartesian product of the atomic state space of each node in layer t,St=/circlemultiplytextnt
v=1X. We will use the term full
stateto refer to the concatenation of the observed values at each node of a layer, i.e. st= [x1
t,···,xnt
t], which
are the states observed in the corresponding MDP M. In contrast, an atomic state is the value observed at
a specific node, i.e. xv
tfor somev∈{1,...,nt}.
Action space: We similarly define an atomic action space, which is the set of actions that can be taken
at a particular node. Note that all nodes do not necessarily have the same action space; {Yj}Ur
j=1represents
theUrdifferent atomic action spaces available. Let yv
t∈Yjt,vbe theatomic action of thevthnode of layer
t, andYjt,vbe theatomic action space associated with this specific node, with jt,v∈{1,···,Ur}indicating
which equivalence class node vbelongs to. The full action space at time step tcan be expressed as the
1In some application, the atomic action description can be tightly linked to the number of outgoing edges at each node,
hence it is natural to define several action spaces.
4Under review as submission to TMLR
Cartesian product of the atomic action space of each node composing layer t,At=/circlemultiplytextnt
v=1Yjt,v. Thefull
actionat time step t,at∈At, is the concatenation of the atomic actions selected at each node of layer t,
at= [y1
t,···,ynt
t]. In Figure 1 (rightmost plot), we show examples where the dimension of the atomic action
depends on the number of exiting edges.
Policies: Letµ={µt}H
t=1denote a collection of time-dependent policies that maps a full statestto a
full action :µt:St→Atfor allt∈{1,···,H}. In order to be able to identify the optimal full action at
each layer, the policy needs to jointly select all atomic actions at the same time. Since nodes in the same
layer can have common successors, all atomic actions in a given layer must be jointly selected; ignoring
these common dependencies and selecting atomic actions independently could lead to sub-optimal policies.
Hence, the policies we consider still operate on the full state andfull action spaces. Note that sometimes,
we are interested in the action selected at a specific node. Let at=µt(st)be thefull action , we denote
theatomic action corresponding to the vthnode of layer tasµt(xv
t|st), to make it explicit that the policy
requires knowledge about the full state in order to select atomic actions .
Transition function: We assume that the known latent graphical structure encodes conditional indepen-
dence between nodes, with the state at a node depending only on the atomic state and actions taken at the
parent nodes. We split nodes into equivalence classes [τi]based on their connectivity patterns, with all nodes
in the same equivalence class having the same number of parents with the same atomic action spaces. The
DAMDP is then governed by a set of atomic transition functions {pi}Uτ
i=1. In particular, the probability of
observing an atomic state xv
t∈Xat thevthnode of layer tispit,v(xv
t|cv
t), wherecv
t∈Cit,vdenotes the atomic
stateand theatomic actions at all parent nodes and it,vdenotes the equivalence class of the node. Figure 1
(rightmost plot) shows all the transition contexts that arise in a simple DAMDP. The atomic transition
function offers two immediate benefits. First, it reduces the number of dependent variables, focusing only on
the subset of variables that influences the next atomic state. Second, it is possible that a layer contains more
than a single node belonging to the equivalence class [τi]; in that case, within a single step, the algorithm
will observe several samples from the same atomic transition function, pi, offering more opportunities to
collect data and learn the true unknown dynamics. We sometimes want the transition context to reflect that
a specific full action a∈At−1was executed in layer t−1. To clarify this relation, we write the context of
nodev∈{1,···,nt}in layertascv
t,a. The context of any node in layer tcan then be constructed using the
atomic description of a,a= [y1
t−1,···,ynt−1
t−1]. Similarly, if actions are selected by a given policy µ, we make
this relation explicit in our notation with cv
t,µ. Then, the probability of observing a new full statest+1, given
that the full action athas been executed in full statestis given by Pt(st+1|st,at) =/producttextnt+1
v=1pit,v(xv
t+1|cv
t,a),
wherest= [x1
t,···,xnt
t],st+1= [x1
t+1,···,xnt+1
t+1]andcv
t,adenotes the context of node vin layertgiven that
full actionawas selected. Note that for the first transition, at t= 1, we define the context to be the empty
set, that is, cv
1=∅.
Reward function: We assume that the expected reward at a time step tin the full MDP is the sum of
the atomic expected reward observed at each node composing the tthlayer,Rt(st,at) =/summationtextnt
v=1rjt,v(xv
t,yv
t),
wherexv
tandyv
tare theatomic states andatomic actions at nodev,jt,vdenotes the action space available
at thevthnode of layer t, andrjt,v(x,y)is the mean of the atomic reward distribution associated with atomic
action setYjt,v.
3.2 Learning in a DAMDP
As depicted in Figure 1 (rightmost plot), Gcan be partitioned into layers, each representing a time step,
t∈{1,...,H}, of the DAMDP. At each time step within an episode, t, the agent observes the state of each
node within the tthlayer of the graph. Note that the number of nodes, nt, will change depending on the
time stept. Hence, the dimensionality of the full state and the observation also depends on the time step t.
After observing each node’s atomic state , the agent chooses the full action it wishes to perform (recall that
this corresponds to an atomic action being taken at each node). Again, the dimensionality of the full action
will depend on the current time step. Once the full action is taken, the agent observes the reward obtained
at each node in the current layer and the new atomic states in each node of the next layer. This process
repeats until the algorithm goes through all the Hlayers in the graph, which completes an episode.
5Under review as submission to TMLR
Value function: Similar to the value function of a policy in an MDP, the value of a policy µin a DAMDP
consists of the expected reward accumulated at each node; when ambiguous, we use the subscript MGto
indicate that this value function is computed under the DAMDP MG, namely∀t∈{1,···,H}and∀s∈St,
VMG
µ,t(s) =E/bracketleftbiggH/summationdisplay
h=tnh/summationdisplay
v=1rjh,v(xv
h,yv
h)/vextendsingle/vextendsingle/vextendsingle/vextendsingleyv
h=µh(xv
h|sh),sh+1∼Ph(·|sh,ah),st= [x1
t,···,xnt
t]/bracketrightbigg
.(3)
Among all the Markov and deterministic policies µ∈Πwe aim to find the optimal policy, µ∗, i.e. the policy
with the largest value function:
µ∗= arg max
µ∈ΠVMG
µ,1(s1). (4)
We consider a sequential learning setting where the learner interacts with the DAMPD over Kepisodes.
After each episode, the learner can use the information gathered to improve their policy in the next episode.
To measure the learner’s performance on this task, we define the regret, which compares, at each point in
time, the performance of the current policy against the optimal policy µ∗.
Definition 3.1 (Regret).A reinforcement learning algorithm Achooses for each episode k∈{1,···,K}
the policyµkthat interacts with the environment. The regret of algorithm AoverKepisodes is defined as,
Regret (K,A) =K/summationdisplay
k=1∆k, (5)
where ∆kdenotes the difference of value function in the true DAMDP, M∗
G, between the optimal policy µ∗
and the current policy µk:
∆k=/summationdisplay
s∈S1ρ(s)/parenleftig
VM∗
G
µ∗,1(s)−VM∗
G
µk,1(s)/parenrightig
. (6)
whereρ(s) =/producttextn1
v=1ρv
A(xv
1)withs= [x1
1,···,xn1
1].
Notethattheregretisastochasticquantitythatdependsontherandombehaviourof M∗
Gandthealgorithm’s
sampling procedure. In the remainder of this paper, we consider the Bayesian regret, where we also take an
expectation over all possible DAMDPs MGthat are drawn from a prior over MDPs f,
E[Regret (K,A)] =K/summationdisplay
k=1E[∆k].
Note that, in general, we use tkto denote the starting time of the kthepisode, so tk= (k−1)H+ 1, and we
denote with tk+itheithtime step of episode k.
4 Dynamic Programming on Graphs
We begin by considering how to learn an optimal policy in a known DAMDP. This will form an important
building block of the reinforcement learning algorithm for DAMDPs presented in Section 5.
The planning algorithm 1 computes an optimal policy via backwards recursion. Dynamic programming
algorithms rely on the self-consistency property of the Bellman operator. We show that there exists an
atomic Bellman operator for any time step t∈{1,···,H},TMG
µ,t, that performs a one-step rollout according
to the policy µ, using only the atomic description of the DAMDP MG,
(TMG
µ,tVMG
µ,t+1)(st):=nt/summationdisplay
v=1rjt,v(xv
t,µt(xv
t|st)) +/summationdisplay
st+1∈St+1nt+1/productdisplay
v=1pit,v(xv
t+1|cv
t+1,µ)VM
µ,t+1(st+1),(7)
for allt∈{1,···,H}and for all st∈St. Wherest= [x1
t,···,xnt
t]andcv
t,µdenotes the transition context
of thevthnode of layer twhen actions are selected by µ.
The following lemma guarantees the self-consistency of the atomic Bellman operator in equation 7.
6Under review as submission to TMLR
Algorithm 1 Planning on a DAMDP
Input:{ˆpi}Uτ
i=1,{ˆrj}Ur
j=1,G
uH(sH) = maxa∈AH/summationtextnH
i=1ˆr(si
H,ai)∀sH∈SH
fort=H−1,···,1do
forst∈Stdo
Evaluateut(st)according to:
ut(st) = max
at∈At/braceleftbiggnt/summationdisplay
v=1ˆrjt,v(xv
t,yv
t) +/summationdisplay
st+1∈St+1nt+1/productdisplay
v=1ˆpit,v(xv
t+1|cv
t+1,at)ut+1(st+1)/bracerightbigg
,
Set
π(s) = arg max
at∈At/braceleftbiggnt/summationdisplay
v=1ˆrjt,v(xv
t,yv
t) +/summationdisplay
st+1∈St+1nt+1/productdisplay
v=1ˆpit,v(xv
t+1|cv
t+1,at)ut+1(st+1)/bracerightbigg
wherest= [x1
t,···,xnt
t]andat= [y1
t,···,ynt
t].
end for
end for
Output: πandu1(s1)∀s1∈S1
Lemma 4.1 (Consistency of atomic Bellman operator) .For any DAMDP MG =
⟨X,{Yj}Ur
j=1,{rj}Ur
j=1,{pi}Uτ
i=1,H,ρA,G⟩and policyµ, the value function VMG
µ,tsatisfies:
VMG
µ,t(st) =TMG
µ,tVMG
µ,t+1(st), (8)
fort∈{1,···,H−1}andst∈St, withVMG
µ,H(sH) = maxa∈AHRH(sH,a)for allsH∈SH.
This property is directly inherited from the MDP assumption; for the sake of completeness, a proof for this
lemma is given in Appendix A.
Before developing a reinforcement learning algorithm for DAMDPs in Section 5, we need to verify that there
exists a planning algorithm that can compute the optimal policy for a given atomic transition distribution p
and atomic reward function r. We show in Theorem 4.2 that this can be done via dynamic programming as
outlined in Algorithm 1. The algorithm leverages the one-step Bellman equation for DAMDP in equation 7.
It starts by finding the optimal full action for eachfull state at the last layer, H. This is trivial as the last
full action will not impact future rewards. For each time step t∈{H−1,···,1}and each full statest∈St
it uses the one-step Bellman equation for DAMDP equation 7 to find the corresponding value ut(s)and the
optimalfull action a∗. The following theorem guarantees that Algorithm 1 retrieves the optimal policy µ∗
and its associated value function VMG
µ∗,1for any known DAMDP MG. The policy µ∗can be directly used
to find the optimal atomic actions , since the optimal full action ,a∗
t= [y1,∗
t,···,ynt,∗
t], directly encodes the
optimalatomic action ,yi,∗
tfor each node i∈{1,···,nt}of thetthlayer ofG.
Theorem 4.2. If Algorithm 1 receives as input {pi}Uτ
i=1and{rj}Ur
j=1, the atomic reward and transition
functions of a known DAMDP MG, then it returns ut(st)andπ, the optimal value function and policy, i.e.
u1(s1) =VM
µ∗,1(s1)∀s1∈S1andπ=µ∗. (9)
The full proof is in Appendix B and follows the one presented in Puterman (2014, ch. 4).
Remark4.3.Using the atomic description of the environment does not impact the computational complexity
of the planning algorithm, which remains O(H|A||¯S|2), where ¯Sdenotes the largest full state space.
7Under review as submission to TMLR
Algorithm 2 Posterior sampling on graph MDPs (PSGRL)
InputPriorfencodingG
forepisodesk= 1,2,···do
SampleMG,k∼f(·|Dtk)
ComputeµMG,kusing Algorithm 1
fortime stepst= 1,···,Hdo
Select and execute at=µk(st,t)
Observert= [r1
t,···,rnt
t]andst+1= [x1
t+1,···,xnt+1
t+1]
Append (st,at,rt,st+1)toDtk+1
end for
end for
5 Posterior Sampling RL on Graphs
We now present a posterior sampling algorithm which leverages the rich underlying structure in the DAMDP
setting. The fact that both the transition and the reward functions can be written in terms of their atomic
counterparts represents an opportunity to increase the algorithm’s efficiency. Indeed, the size of the com-
binatorial spaces StandAtcan present a challenge when a learner is tasked to learn the full transition
distribution Pand the full reward function Rdirectly. The inputs of the atomic transition functions and
the atomic reward functions lie in spaces much smaller than their non-atomic counterparts, presenting an
opportunity to speed up learning. The input space for the atomic transition function idepends on the size
of the context space, |Ci|, fori∈{1,···Uτ}, whereUτis the number of equivalence classes. Critically, |Ci|
includes states and actions for only a subset of the nodes at the previous layer. This space is smaller than
thefull state-action space, which is the Cartesian product of the atomic state-action pairs of all nodes in the
previous layer. Similarly, the atomic reward only depends on the atomic state x∈Xand the atomic action
y∈Yj, forj∈{1,···,Ur}, which is also smaller than the full state-action spaces.
We can exploit the structure to achieve further efficiency gains by observing that the input of the atomic
transition and reward function can be observed more than once per time step. If two nodes in the same
layer have the same number of incoming edges, then they are learning the same atomic transition function.
Similarly, if they have the same atomic action spaceYj, then they are learning the same atomic reward
function. This benefits the algorithm as we observe more samples corresponding to the atomic functions
than their non-atomic counterparts. However, in the analysis, we need to account for the fact that we can
observe a reward or a transition input more than once per time step, which deviates from the classic analysis
of RL frameworks that assume that a single sample is collected at each time step.
We present our posterior sampling algorithm for DAMDPs in Algorithm 2. The algorithm proceeds in
episodes. At the beginning of each episode k, the algorithm samples a DAMDP MG,k∼f(·|Dk), where
f(·|Dk)represents the current belief about the true DAMDP M∗
G, given the observations collected up to
episodek,Dk. The algorithm then computes the optimal policy µMG,konMG,kand runs the policy for an
episode. The trajectory (st,at,rt,st+1)H
t=1is added to our dataset Dk. We then use Dkto update our belief
about the distribution of the true DAMDP M∗
G. We assume that each transition function is a categorical
distribution governed by a set of parameters θpi∈RX, for alli∈{1,···,Uτ}. The reward function has a
Gaussian distribution with mean µrjand standard deviation σrj, forj∈{1,···,Ur}. The prior fmaintains
a Dirichlet distribution over the space of parameters θpiand a Normal-inverse gamma prior over the space
of parameters µrjandσrj. This induces a prior distribution over DAMDPs.
The regret suffered by PSGRL (Alg. 2) is bounded by the following theorem.
Theorem 5.1. Iffis the distribution of M∗
Gthen the regret suffered by PSGRL (Alg. 2) over Kepisodes
is upper bounded by,
E[Regret (K,PSGRL )] = ˜O/parenleftbiggUr/summationdisplay
j=1H/radicalig
XYjmj
rK+Uτ/summationdisplay
i=1H/radicalig
(X¯Y)dimiτXK/parenrightbigg
(10)
8Under review as submission to TMLR
whereX=|X|is the size of the atomic state space, ¯Y= maxj∈{1,···,Ur}Yjis the size of the largest atomic
action space. For nodes that belong to the ithequivalence class [τi],didenotes the number of parent nodes
andmi
τdenotes the number nodes in Gthat share the same equivalence class [τi]. Finally,mj
ris the number
of nodes in Gwith the same atomic action space Yj.
The proof of Theorem 5.1 is given in Appendix C. It follows the proof in Osband et al. (2013), with the key
difference being that the confidence sets are built around the estimates of the atomic transition and reward
functions. This approach has two immediate benefits. First, we might be able to collect more than a single
sample per time step. If we consider the case where we have knodes in the same layer, t, that belong to the
same equivalence class, [τi], then at time step t, we collect ksamples from pi. Second, the input space of the
atomic reward and transition function can be significantly smaller than the input space in the original MDP
(i.e. the transition and reward functions over the full state and action spaces). This leads to a significant
gain in performance since the atomic transition and reward function are easier to estimate than their full
state-action counterparts.
The bound presented in Theorem 5.1 is an improvement over the lower bound that can be obtained by
running any RL algorithm in the full MDP and ignoring the latent graphical structure:
Ω/parenleftiggH/summationdisplay
h=1/radicalbig
StAtT/parenrightigg
= Ω/parenleftiggH/summationdisplay
h=1/radicalbig
(X¯Y)ntT/parenrightigg
. (11)
Where¯Y= minj∈{1,···,Ur}Yjis the size of the smallest action space in the DAMDP. The comparison of
PSGRL upper bound equation 10 and the lower bound equation 11 suggests an improved efficiency for
PSGRL compared to RL algorithms that ignore the latent graphical structure. In particular, note that the
number of incoming edges for any node in a given layer tis always bounded by the number of nodes at the
previous layer di≤nt−1whereidenotes the equivalence class of any node in layer t. In conclusion, the
regret suffered by PSGRL, as described in equation 10, is smaller than the regret any RL algorithm that
ignores the graphical structure Gcould suffer as described in equation 11.
Ur/summationdisplay
j=1H/radicalig
XYjmj
rK+Uτ/summationdisplay
i=1H/radicalig
(X¯Y)dimiτXK≤H/summationdisplay
h=1/radicalbig
(X¯Y)ntT
6 Experiments
We now illustrate that the performance gains suggested by Theorem 5.1 manifest themselves empirically. We
consider two experimental settings: the maximum leaky flow of a graph and wind farm yield optimization.
6.1 Maximum leaky flow of a graph
The maximum leaky flow problem is defined as a directed acyclic graph G= (V,E), with a single root node,
the source node v1∈V, and a single leaf node, called the sink node vN∈V. Using the edges to transport
flow from one node to the next, the goal is to transport as much flow as possible from the source node
to the sink node. Because Gis directed, it is only possible to transport flow in the edge’s direction. The
environment is governed by two parameters: edges have a given capacity, c>0, which denotes the maximum
amount of flow that can be transported along that edge, and an unknown failure probability, p∈[0,1], which
dictates how likely the edge is to fail to transport the flow assigned to it.
The above problem is similar to the graph maximum flow problem (Ford & Fulkerson, 1956). However, the
leaky version of the problem is more challenging as the probability of failure is unknown to the learner. In
the leaky version of the problem, the learner is tasked to jointly learn the unknown dynamics and the paths
that maximise the expected flow.
We consider two algorithms for this problem. The proposed PSGRL, which has access to the graphical
structure and an extension of the PSRL algorithm (Osband et al., 2013) that works in time-inhomogeneous
settings, which does not know the graphical structure.
9Under review as submission to TMLR
Figure 2: The first row depicts the graph that governs the DAMDP. The second row shows the learning
curve for both algorithms considered. PSRL ignores the latent graphical structure, and PSGRL leverages
the graphical structure. The left-most plot shows the performance obtained on a simple chain graph. As
expected, the performance for both algorithms is similar. Looking at the remaining plots, where we consider
larger diamond-shaped graphs, it becomes clear that as we increase the complexity of the graph, the benefit
of PSGRL becomes evident. To monitor the evolution of the regret, we ran ten different seeds; the solid line
represents the mean regret while the shaded area covers ±one standard deviation.
PSGRL representation: At each time step t, we observe the set of atomic states{xv
t}nt
v=1, which represent
the amount of flow into node vof layert. For each node, the agent decides how to distribute the current
flow along its edges. For each node, the atomic action space consists of the possible allocation of flow along
its edges; the action space is discrete and represents a possible fraction of the total flow. The atomic reward
is then the amount of flow a specific node sends to the next layer of the graph.
PSRL representation: Thefull state representation at time step t,stis a vector representing the amount
of flow in each node in layer t. Thefull action representation is the flow assignment for all edges connecting
nodes from layer tto layert+ 1. The reward at time step tis the total amount of flow the agent managed
to move from layer tto layert+ 1.
Figure 2 compares the performances of PSGRL and PSRL on the leaky maximum flow problem. The first
row of Figure 2 depicts the DAMDP considered, while the second row shows the cumulated regret incurred
by PSGRL (in blue) and PSRL (in red). The uncertainty estimates around the curve are obtained by running
the experiment with ten seeds. We present in Appendix F (see Figure 6) another series of experiments that
highlights similar performance results on a different family of graphs.
The leftmost DAMDP is a simple chain graph. For this specific DAMDP instance, the atomicrepresentation
and thefull state representation are equivalent. This explains the similar performance of PSGRL and PSRL.
In contrast, considering more complex DAMDP instances, the benefit of leveraging the latent graphical
structure becomes more apparent. Breaking down the combinatorial nature of the full state space and full
actionspace using the DAG structure is beneficial. This is even more obvious on the rightmost plot, which
shows the most complex leaky maximum flow instance considered.
Remark 6.1.Even for a simple example like the leaky maximum flow problem, it is interesting to analyse
how the Bayesian regret bound for PSGRL equation 10 compares to the RL lower bound equation 11.
The diamond-shaped graph with 16nodes in the rightmost part of Figure 2 implies that the DAMDP is
parameterized by the following quantities. The number of distinct atomic action spaces is Ur= 2as some
nodes can distribute their flow in a single edge while others can distribute it in two different edges. We
10Under review as submission to TMLR
Figure 3: The two leftmost plots summarize the results obtained on a wind farm task with six wind turbines.
The first plot shows the performance of PSGRL and PSRL, while the second plot illustrates the farm layout
(each node represents a wind turbine, and each edge contains the wake effect). The two rightmost plots
show the same experiment for a larger farm with nine wind turbines.
observe that there are 6nodes with a single outgoing edge, hence m1
r= 6and9nodes with two outgoing
edges,m2
r= 9. The number of transition equivalence classes is Uτ= 2as some nodes have two incoming
edges while others have a single incoming edge. We observe that there are 6nodes with a single incoming
edge (i.e.m1
τ= 6) and 9with two incoming edges (i.e. m2
τ= 9). We also note that the maximum number
of nodes in a layer is Nmax= 4, and the largest number of edges between two adjacent layers is Mmax= 6.
In this specific context, the regret upper bound of PSGRL can be expressed as follows:
E[Regret (K,PSGRL )] = ˜O/parenleftbiggUr/summationdisplay
j=1H/radicalig
2XYjmj
rK+Uτ/summationdisplay
i=1H/radicalig
2(X¯Y)dimiτXK/parenrightbigg
(12)
=˜O/parenleftig
H/radicalbig
12XY1K+H/radicalbig
18XY2K+H/radicalbig
12X2¯YK+H/radicalbig
19X3¯Y2K/parenrightig
.(13)
In orange, we have the regret associated with learning the transition function, and in blue, the regret
associated with learning the reward function. Similarly, using the characteristics of this specific problem
instance for any algorithm that do not leverage the latent graphical structure, such as PSRL, we obtain the
following lower bound:
E[Regret (K,PSRL )] = Ω/parenleftiggH/summationdisplay
h=1/radicalbig
StAtT/parenrightigg
= Ω/parenleftiggH/summationdisplay
h=1/radicalbig
(X¯Y)ntT/parenrightigg
(14)
= 2/radicalbig
(X¯Y)1T+ 2/radicalbig
(X¯Y)2T+ 2/radicalbig
(X¯Y)3T+/radicalbig
(X¯Y)4T, (15)
where¯Ydenotes the smallest atomic action space.
In this simple example, the efficiency gain provided by PSGRL will be significant as long as√
X3¯Y2</radicalbig
(X¯Y)4which is in general true, unless the discrepancy between the largest and smallest action space
becomes too large, that is,¯Y≪¯Y. Additionally, this efficiency gain is reinforced as we increase the size of
the graph. Considering that the number of nodes increases while the diamond-like architecture on Figure 2
is preserved, PSGRL’s regret equation 12 scales linearly with the number of nodes, through mi
τandmj
r. On
the other hand, PSRL’s lower bound on the regret scales exponentially with the number of nodes through
nt, the number of nodes at each layer of the graph.
6.2 Wind farm yield optimisation
One real-world example with a known latent graphical structure is the case of wind farm optimization. To
maximise the yield of a wind farm given a specific atmospheric condition (wind direction and speed), it is
important to consider the interaction between wind turbines. Wind turbines generate a stream of turbulent
wind that might impact the yield of a downstream wind turbine. This phenomenon is known as the wake
11Under review as submission to TMLR
FMDP/DAMDPt= 1t= 2t= 3t= 4
DAMDPt= 1t= 2t= 3t= 4
DAMDPt= 1t= 2t= 3t= 4
Figure 4: This figure shows 3 examples of DAMDPs; the leftmost one has a constant connectivity pattern.
Each layer has the same number of nodes, and the connectivity pattern from one layer to the next remains
the same. MDPs that exhibit such a structure could be modelled by an FMDP or a DAMDP. On the
contrary, the middle and the rightmost graphs could only be modelled by DAMDPs. The example depicted
in the middle graph shows interactions that vary from one time step to the next, which cannot be modelled
by an FMDP. The rightmost graph shows a graph where the number of nodes varies from time step to time
step, which also cannot be modelled as an FMDP
effect. The wake of a wind turbine can be deflected if the upstream wind turbine slightly rotates. This
introduces an interesting control problem as for a single wind turbine to maximize its yield, it has to face
the wind, but as soon as the yield of the entire farm is concerned, the optimal set of angles might include
wind turbines not facing the wind in order to maximize the yield of other wind turbines. In what follows,
we constructed a simple environment that models some of the dynamics of a wind farm. To model the
wake effect, we use FLORIS, a wind farm simulation software (Annoni et al., 2018)2. To simplify the
setting, we discretize the atomic actionY={30◦,0◦,−30◦}which are the possible angles (with respect to
the wind direction) for each wind turbine. The atomic state encodes the wind speed observed at each wind
turbine. We also discretize the state and consider all increments of 0.1m/sfrom 6m/sto10m/s. Similarly
to the maximum flow experiment, the full state andfull action at time step tare obtained, respectively,
by concatenating the atomic state of each node in layer tand theatomic action of each node in layer t.
The performance of PSGRL on the resulting DAMDP and PSRL on the corresponding MDP is shown in
Figure 3, where each plot considers wind farms with a grid layout with a varying number of wind turbines.
We consider a grid-like layout where we explicitly encode the impacts between a wind turbine and its closest
upstream wind turbine. While the plots on the left of Figure 3 show a wind farm composed of six wind
turbines, the right side of the figure shows an experiment with nine wind turbines. Again, it can be seen
that the benefits of running PSGRL on the DAMDP are significant compared to the performance of PSRL
on the corresponding MDP because of the re-usability of the atomic reward and transition function.
7 Related work
The framework we consider in this paper, the DAMDP model, is related to the factored MDP (FMDP)
framework. While the first works on FMDP considered only a factorisation of the state space (Kearns &
Koller,1999;Boutilieretal.,2000;Guestrinetal.,2003), theFMDPframeworkwaslaterextendedtoconsider
factorisation over the state-action space (Osband & Van Roy, 2014; Chen et al., 2020) - which in spirit is
more similar to DAMDP. Indeed, FMDPs and DAMDPs share similar assumptions; they both consider the
factorisation of the state and action spaces, the additivity of the reward function and the factorisation of
the transition function. However, an important difference between the two frameworks is that FMDPs were
initially proposed in a time-homogeneous setting, while DAMDPs allow the structure to vary as dictated by
the graph and its connectivity.
In particular, DAMDP models a larger class of problems, as depicted in Figure 4; the encoding of the entire
episode in a graph allows the representation of richer dynamics. The leftmost plot of Figure 4 shows a
graph that exhibits the same connection patterns across all layers; such a problem could be modelled with
FMDPs or DAMDPs. The remaining examples in the middle and on the right side of Figure 4 represent
problems that cannot be directly modelled with FMDPs. The example in the middle of Figure 4 shows a
2The code for the FLORIS simulator is available at the following address: https://github.com/NREL/floris
12Under review as submission to TMLR
scenario where the connection dynamics change from one layer to the next, while the rightmost example
shows a graph with a varying number of nodes and edges in each layer. Since DAMDPs are rolled out on an
arbitrary directed acyclic graph, we can model each of the three graphs in Figure 4 with a DAMDP.
On the subset of DAMDP problems that can be modelled as FMDP, the regret obtained for the algorithm
proposed in Osband & Van Roy (2014) is equivalent to the regret of our algorithm, PSGRL.
In general DAMDPs, a naive extension of the regret proposed Osband & Van Roy (2014) to the time
inhomogeneous setting, is worse than the result obtained in Theorem 5.1. A naive extension of the FMDP
framework to the time inhomogeneous setting, where the factored dynamics are encoded in graph G, would
produce a regret bound driven by the number of nodes in the graph G. To be more specific, using our
notation, theregretpaidbytheposteriorsampling-basedalgorithmproposedinOsband&VanRoy(2014)to
estimate the transition distribution of an FMDP is/summationtext|V|
v=1O(H(X¯Y)dv+/radicalig
(X¯Y)dvXT), where|V|represents
the number of nodes in Ganddvis the number of incoming edges in node v. For the case of DAMDPs,
the bound we propose in Theorem 5.1 is tighter as in equation 6 the size of the transition context (X¯Y)di
is under the square root and, in general,/summationtext|V|
v=1(XY)dv≥/summationtextUτ
i=1/radicalig
(X¯Y)di, as the first sum is over all nodes
in the graph while the second is only over the Uτequivalence classes. Additionally, this difference could
be reinforced if we consider increasing the number of nodes |V|while preserving a relatively small number
of equivalence classes Uτ. For example, in the case of the diamond-shaped graph, the graph’s architecture
remains constant, and the only term that grows is mt, the largest number of nodes in a single layer that has
the same transition context.
In this work, we have assumed that the graphical structure is known. An interesting direction for future
work would consist of learning the latent graphical structure. Structure discovery has been studied in the
case of time-homogeneous FMDP Degris et al. (2006); Strehl et al. (2007); Mutti et al. (2023), but these
methods have not been extended to the time-inhomogeneous setting. Approaches like the one considered in
Mutti et al. (2023), where the proposed algorithm, C−PSRL, can learn the factored representation, could
be extended to the time-inhomogeneous setting. Such an extension is not trivial as it would require adding
another layer of inference in our proposed algorithm and treating the latent graph Gas a random variable.
8 Conclusion
In this paper, we have formalised a new subclass of MDPs that have an additional graphical latent struc-
ture, DAMDPs. We have then presented a posterior sampling-based algorithm, PSGRL, that exploits this
graphical structure to efficiently learn a policy. We finally showed that PSGRL outperforms PSRL both in
theory and in practice when this graphical structure is present.
Although the structure of DAMDPs may appear restrictive, we argue that it does appear in many practical
settings, including the maximum flow of a graph and wind farm yield optimisation setting discussed in
this paper. However, an immediate future step could be to extend our model. First, we could add edge
attributes; for example, if we revisit the wind farm optimisation example presented in Section 6.2. We could
imagine that the potential effect two wind turbines have on each other might depend on the relative distance
between the two; adding additional attributes to the edge would allow us to consider a richer set of transition
dynamics and to model real-world problems more realistically. Such an increase in complexity will impact
the upper bound on the regret but would offer more flexibility in the modelling. Second, to accommodate
a wider range of real-world problems, the proposed framework could be extended to allow for continuous
state and action. Quantifying the benefit of the graphical factorisation in such a context remains of great
interest. We leave these endeavours for future work as, for the time being, our graph-based approach renders
a potentially large subset of real-world MDPs amenable to more efficient training. This phenomenon is even
more evident when we consider that the graph computations are inherently parallelisable.
References
Mahdi Abkar, Navid Zehtabiyan-Rezaie, and Alexandros Iosifidis. Reinforcement learning for wind-farm
flow control: Current state and future actions. Theoretical and Applied Mechanics Letters , pp. 100475,
13Under review as submission to TMLR
2023.
J. Annoni, P. Fleming, A. Scholbrock, J. Roadman, S. Dana, C. Adcock, F. Porte-Agel, S. Raach, F. Haiz-
mann, and D. Schlipf. Analysis of control-oriented wake modelling tools using lidar field results. Wind
Energy Science , 3(2):819–831, 2018. doi: 10.5194/wes-3-819-2018. URL https://wes.copernicus.org/
articles/3/819/2018/ .
Peter Auer, Thomas Jaksch, and Ronald Ortner. Near-optimal regret bounds for reinforcement learning.
Advances in Neural Information Processing Systems , 21, 2008.
Mohammad Gheshlaghi Azar, Ian Osband, and Rémi Munos. Minimax regret bounds for reinforcement
learning. In International Conference on Machine Learning , pp. 263–272. PMLR, 2017.
Jørgen Bang-Jensen and Gregory Z Gutin. Digraphs: theory, algorithms and applications . Springer Science
& Business Media, 2008.
Craig Boutilier, Richard Dearden, and Moisés Goldszmidt. Stochastic dynamic programming with factored
representations. Artificial intelligence , 121(1-2):49–107, 2000.
Xiaoyu Chen, Jiachen Hu, Lihong Li, and Liwei Wang. Efficient reinforcement learning in factored mdps
with application to constrained rl. arXiv preprint arXiv:2008.13319 , 2020.
Thomas Degris, Olivier Sigaud, and Pierre-Henri Wuillemin. Learning the structure of factored markov
decision processes in reinforcement learning problems. In International Conference on Machine Learning ,
pp. 257–264, 2006.
Lester Randolph Ford and Delbert R Fulkerson. Maximal flow through a network. Canadian journal of
Mathematics , 8:399–404, 1956.
Carlos Guestrin, Daphne Koller, Ronald Parr, and Shobha Venkataraman. Efficient solution algorithms for
factored mdps. Journal of Artificial Intelligence Research , 19:399–468, 2003.
Michael Kearns and Daphne Koller. Efficient reinforcement learning in factored mdps. In IJCAI, volume 16,
pp. 740–747, 1999.
ZoubirMammeri. Reinforcementlearningbasedroutinginnetworks: Reviewandclassificationofapproaches.
IEEE Access , 7:55916–55950, 2019.
Mirco Mutti, Riccardo De Santi, Marcello Restelli, Alexander Marx, and Giorgia Ramponi. Exploiting causal
graph priors with posterior sampling for reinforcement learning. arXiv preprint arXiv:2310.07518 , 2023.
Ian Osband and Benjamin Van Roy. Near-optimal reinforcement learning in factored mdps. Advances in
Neural Information Processing Systems , 27, 2014.
Ian Osband, Daniel Russo, and Benjamin Van Roy. (more) efficient reinforcement learning via posterior
sampling. Advances in Neural Information Processing Systems , 26, 2013.
Martin L Puterman. Markov decision processes: discrete stochastic dynamic programming . John Wiley &
Sons, 2014.
Benjamin Rolf, Ilya Jackson, Marcel Müller, Sebastian Lang, Tobias Reggelin, and Dmitry Ivanov. A review
on reinforcement learning algorithms and applications in supply chain management. International Journal
of Production Research , 61(20):7151–7179, 2023.
Alexander L Strehl, Carlos Diuk, and Michael L Littman. Efficient structure learning in factored-state mdps.
InAAAI, volume 7, pp. 645–650, 2007.
Malcolm Strens. A bayesian framework for reinforcement learning. In International Conference on Machine
Learning , volume 2000, pp. 943–950, 2000.
14Under review as submission to TMLR
Richard S Sutton, David McAllester, Satinder Singh, and Yishay Mansour. Policy gradient methods for
reinforcement learning with function approximation. Advances in Neural Information Processing Systems ,
12, 1999.
Christopher JCH Watkins and Peter Dayan. Q-learning. Machine learning , 8:279–292, 1992.
Tsachy Weissman, Erik Ordentlich, Gadiel Seroussi, Sergio Verdu, and Marcelo J Weinberger. Inequalities
for the l1 deviation of the empirical distribution. Hewlett-Packard Labs, Tech. Rep , 2003.
15Under review as submission to TMLR
A Proof of the consistency of the atomic Bellman operator
Lemma A.1 (Consistency of the atomic Bellman operator) .For any DAMDP MG =
⟨X,{Yj}Ur
j=1,{rj}Ur
j=1,{pi}Uτ
i=1,H,{ρv
A}n1
v=1,G⟩and policyµ, the value function VMGµsatisfies:
VMG
µ,t(st) =TMG
µ,tVMG
µ,t+1(st), (16)
fort∈{1,···,H−1}andst∈St, withVMG
µ,H(sH) = maxa∈AHRH(sH,a)for allsH∈SH.
Proof.
TMG
µ,tVMG
µ,t+1(st) =nt/summationdisplay
v=1rjt,v(xv
t,µ(xv
t|st)) +/summationdisplay
st+1∈St+1nt+1/productdisplay
v=1pit,v(xv
t+1|cv
t+1,µ)VMG
µ,t+1(st+1)
=nt/summationdisplay
v=1rjt,v(xv
t,µ(xv
t|st)) +/summationdisplay
st+1∈St+1nt+1/productdisplay
v=1/parenleftigg
pit,v(xv
t+1|cv
t+1,µ)E/bracketleftiggH/summationdisplay
h=t+1nh/summationdisplay
v=1rjt+1,v(xv
t+1,µ(xv
t+1|st+1))/bracketrightigg/parenrightigg
=E/bracketleftiggH/summationdisplay
h=tnh/summationdisplay
v=1rjt,v(xv
t,µ(xv
t|st))/bracketrightigg
=VM
µ,t(st)
To obtain the first equality, we used the definition of the Bellman operator, and to obtain the second
equality, we used the definition of VMG
µ,t+1. Observing that we consider all possible next state st+1weighted
by its probability of appearing, we can directly include this computation into the expectation leading to the
third equality.
B Proof of convergence of Algorithm 1
Theorem B.1. If Algorithm 1 receives as input pandr, the atomic reward and transition functions of a
known DAMDP MG, then it returns u1(s1)andπ, the optimal value function and policy, i.e.
u1(s1) =VMG
µ∗,1(s1)andπ=µ∗. (17)
Proof.The proof follows the one presented in Puterman (2014, ch. 4) and consists of two parts.
Parti:Showthatthelearnedvaluefunction ut(st)≥VMG
π,t(st)forallπ∈Πandforall (t,st),∈{1,···,H}×
St, where Πis the class of all deterministic and Markov policies.
Part ii: Show that the algorithm’s output satsifies ut(st) =VMG
π∗,t(st)for alltand alls∈St.
Proof of i: Letπ∈Πbe any policy in the class of deterministic Markov policies. Since at the last time
stepHthe goal is only to maximise the immediate reward, the suggested result holds at time step H,
uH(sH)≥VMG
π,H(sH), for allsH∈SH. Let’s now assume that ut(st)≥VMG
π,tfort=h+ 1,···,H. We prove
by induction that it also holds for t=h,
16Under review as submission to TMLR
uh(sh) = max
a∈Ah/braceleftbiggnh/summationdisplay
v=1rjh,v(xv
h,yv
h) +/summationdisplay
sh+1∈Sh+1nh+1/productdisplay
v=1pih,v(xv
h+1|cv
h+1,a)uh+1(sh+1)/bracerightbigg
≥max
a∈Ah/braceleftbiggnh/summationdisplay
v=1rjh,v(xv
h,yv
h) +/summationdisplay
sh+1∈Sh+1nh+1/productdisplay
v=1pih,v(xv
h+1|cv
h+1,a)VMG
π,h+1(sh+1)/bracerightbigg
≥nh/summationdisplay
v=1rjh,v(xv
h,π(xv
h|sh)) +/summationdisplay
sh+1∈Sh+1nh+1/productdisplay
v=1pih,v(xv
h+1|cv
h+1,π)VMG
π,h+1(sh+1)
=nh/summationdisplay
v=1rjh,v(xv
h,π(xv
h|sh)) +Eπ/bracketleftbiggH/summationdisplay
t=h+1nt/summationdisplay
v=1rjt,v(xv
t,yv
t)/bracketrightbigg
(by def. of VMGin equation 3)
=VMG
π,h(s)
where the notation Eπis used to explicitly mention that actions are selected according to the policy π.
Proof of ii: The second part is also shown by induction. We start by observing that the last step focuses
only on maximising the immediate reward. So at time step Hit trivially holds that uH(s) =VMG
∗,H(s)for all
s∈SH. We then assume that ut(s) =VMG
∗,t(s)fort=h+ 1,···,Hand for alls∈St. Then,
uh(s) = max
a∈Ah/braceleftbiggnh/summationdisplay
v=1rjh,v(xv
h,yv
h) +/summationdisplay
s′∈Sh+1nh+1/productdisplay
v=1pih,v(xv
h+1|cv
h+1,a)uh+1(s′)/bracerightbigg
= max
a∈Ah/braceleftbiggnh/summationdisplay
v=1rjh,v(xv
h,yv
h) +/summationdisplay
s′∈Sh+1nh+1/productdisplay
v=1pih,v(xv
h+1|cv
h+1,a)VMG
∗,h+1(s′)/bracerightbigg
=nh/summationdisplay
v=1rjh,v(xv
h,µ∗(xv
h|sh)) +/summationdisplay
s′∈Sh+1nh+1/productdisplay
v=1pih,v(xv
h+1|cv
h+1,∗)VMG
∗,h+1(s′)
=VMG
∗,h(s)
Where the third line is obtained by the hypothesis that ut(s) =VMG
∗,t(s)fort≥h+1and from the definition
ofµ∗.
C Proof of Theorem 5.1
We follow the proof structure of Osband et al. (2013), where the analysis focuses on a modified regret term
that removes the dependency on µ∗and is equivalent to the original regret in expectation. For any episode
k, we can write the modified regret as follows:
˜∆k=/summationdisplay
s1∈S1ρ(s1)(VMG,k
µk,1(s1)−VM∗
G
µk,1(s1)). (18)
whereM∗
Gis the true DAMDP and MG,kis the DAMDP sampled at episode k.
The equivalence in expectation between the original regret ∆kand the modified regret ˜∆kis possible thanks
to the following lemma, which is first presented in Osband et al. (2013, Lemma 1) and restated below in the
DAMDP setting:
Lemma C.1. Iffis the distribution of M∗
GandMG,k∼fthen, for any σ(Dk)−measurable function g,
E[g(M∗
G)|Dk] =E[g(MG,k)|Dk]. (19)
17Under review as submission to TMLR
Whereσ(Dk)is theσ-algebra generated by all the data accumulated up to episode k,Dk.
This allows the following equivalence between the two different regret terms, E/bracketleftbigg/summationtextK
k=1∆k/bracketrightbigg
=E/bracketleftbigg/summationtextK
k=1˜∆k/bracketrightbigg
.
Indeed, since ∆k−˜∆k=/summationtext
s1∈S1ρ(s1)(VMk
µk,1(s1)−VM∗
µ∗,1(s1)), from Lemma C.1, we get that E[∆k−˜∆k|Dk] =
0. Finally, by the tower rule E[∆k−˜∆k] =E[E[∆k−˜∆k|Dk]] = 0.
To lighten the notation we now write Vk
µk,iforVMk
µk,i. Working with the modified regret ˜∆(presented in
equation 18) allows us to rewrite the modified regret in terms of Bellman error,
E[˜∆k|M∗
G,MG,k] =E/bracketleftbiggH/summationdisplay
i=1(TMG,k
µ,i−TM∗
G
µ,i)Vk
µ,i+1(stk+i)/vextendsingle/vextendsingle/vextendsingle/vextendsingleM∗
G,MG,k/bracketrightbigg
. (20)
To prove that equation 20 holds, we apply the Dynamic programming equation 7 inductively (note that we
denote bypi
∗(x|c)the atomic true transition distribution for nodes that belong to the equivalence class [τi]):
(Vk
µk,1−V∗
µk,1)(stk+1) = (Tk
µk,1Vk
µk,2−T∗
µk,1V∗
µk,2)(stk+1)
= (Tk
µk,1−T∗
µk,1)Vk
µk,2(stk+1) +/summationdisplay
s′∈S2n2/productdisplay
v=1pi2,v
∗(xv
2|cv
2)(Vk
µk,2−V∗
µk,2)(s′)
= (Tk
µk,1−T∗
µk,1)Vk
µk,2(stk+1) + (Vk
µk,2−V∗
µk,2)(stk+2) +dtk+1
=···
=H/summationdisplay
h=1(Tk
µk,h−T∗
µk,h)Vk
µk,h+1(stk+h) +τ/summationdisplay
i=1dtk+h
for
dtk+h=/summationdisplay
s′∈Sh+1nh+1/productdisplay
v=1pih,v
∗(xv
h+1|cv
i+1)(Vk
µk,h+1−V∗
µk,h+1)(s′)−(Vk
µk,h+1−V∗
µk,h+1)(stk+h+1).
Since E[(Vk
µk,h+1−V∗
µk,h+1)(stk+h+1)|M∗
G,MG,k] =/summationtext
s′∈Sh+1/producttextnh+1
v=1(pih,v
∗(xv
h+1|cv
h+1,µk)(Vk
µk,h+1−
V∗
µk,h+1)(s′)the termsdtk+hdisappear in expectation (i.e. E[dtk+h|M∗
G,MG,k] = 0for allkandh∈
{1,···,H}) and so we obtain equation 20.
Since the regret has been rewritten as the sum of one-step Bellman errors (see equation 20), the next step is
to show that as interactions with the environment are observed, the sampled DAMDPs, MG,k, concentrate
around the true DAMDP M∗
G. This is done in the following subsection.
C.1 Confidence sets
For the remainder of this section, the notation is a bit more involved. The below list introduces or recalls
the concepts and the notation used.
1.G= (V,E)is an LDAG that encodes the underlying structure in the execution of a DAMDP episode.
We denote the ithnode of layer tbyvi
t. We recall that each layer t= 1,···,Hhasntnodes. The
graph has a total of |V|=/summationtextH
t=1nt=nnodes.
2. If two nodes vi1
t1andvi2
t2, with (i1,t1)̸= (i2,t2)have the same number of parents and their parents
have the same atomic action space, they belong to the same transition equivalence class [τi]. We
denote bydithe number of parent (or incoming edges) of a node that belongs to equivalence class
[τi]. All nodes in Gbelong to one of the Uτequivalence classes, [τi]Uτ
i=1. The number of nodes in
Gthat belong to the same equivalence class is denoted by mi
τ, and/summationtextUτ
i=1mi
τ=n. For example, in
Fig. 1, there are two different equivalence classes, Uτ= 2, because there exist nodes with two parent
nodes or a single parent node.
18Under review as submission to TMLR
3. Each equivalence class [τi]withi∈ {1,···,Uτ}, has a corresponding state-action space Ci=/circlemultiplytextdi
k=1(X×Yk)that contains all the possible values the transition context (of an element of the
ithequivalence class [τi]) can take. Recall that the atomic action space might depend on the node,
so two nodes belong to the same equivalence class if they have the same number of parent nodes
with the same atomic action spaces. This space consists of the atomic states and the atomic action
values of each parent of the node. We can upper-bound the size of this context space by (X¯Y)di,
wherediis the number of parent nodes in the ithequivalence class, and ¯Yis the size of the largest
atomic action space.
4. Each equivalence class [τi]has a dedicated transition function pi(·|ci)for alli∈{1,···,Uτ}and all
possible transition context ci∈Ci.
5.Uris the number of distinct atomic action sets. For example, in the case of the leaky maximum
flow problem in Section 6.1 Ur= 2, the atomic action space is different if a node has a single out-
going edge or two out-going edges. In the wind farm optimisation problem Ur= 1, regardless of
the number of outgoing edges, the atomic action space of a node remains the same. The number
of nodes in Gthat have the atomic action set jis denoted by mj
r, for allj∈{1,···,Ur}. Then,/summationtextUj
j=1mj
r=n, wherenis the number of nodes in G.
6. For each distinct atomic action set {Y}Ur
j=1, we call the atomic reward context the associated atomic
state-action space Zj=X×Yj. We denote by zj∈Zjan element of the reward context j.
7. For each atomic action set {Yj}Ur
j=1we define a corresponding atomic reward function rj(zj).
8. DefineNτ,i
tk(c)to count the number of times a node of the equivalence class [τi]has observed the
transition context c∈Ciduring the first tktime steps, where tkindicates the time step at which
episode k starts (i.e. tk= (k−1)∗H+ 1). Sometimes, we are interested in monitoring the number
of visits more closely. Then, Nτ,i
tk,v(c)counts the number of time the transition context c∈Cigiven
that the agent started episode kand already observed the vfirst nodes of G, withv∈{1,···,n}.
Nodes are indexed by layers, but the indexing within a layer is arbitrary and fixed before the learning
starts. In Figure 1 leftmost plot, we show an example of an LDAG with a valid node indexing.
9. DefineNr,j
tk(z)to count the number of times a node has observed the reward context z∈Zjduring
the firsttktime steps for all j∈[Ur]and for all z∈Zj.Nτ,i
tk,v(z)counts the number of times the
reward context zwas observed until the vthnode of the execution of the kthepisode, where the node
indexing is the same as the one considered for Nτ,i
tk,v(c)(see entry 8 of this list for more details).
The posterior sampling algorithm proceeds by sampling an atomic reward function for each possible action
set{Yj}Ur
j=1and a transition function for each equivalence class [τi]for alli∈{1,···,Uτ}. The atomic
transition distributions and the atomic reward distributions sampled at the beginning of episode k(step
3. in Alg. 2) are denoted {pi
k}Uτ
i=1and{rj
k}Ur
j=1, respectively. The sampled DAMDP is then constructed by
combining the atomic transition and reward function as dictated by the graph G. To show that the sampled
DAMDPs, MG,k, concentrate around the true DAMDP M∗
G, we construct confidence intervals around the
empirical atomic transition and reward functions. We define the confidence set of feasible DAMDPs that we
may sample at episode kas:
Mk
G=/braceleftbig
M:∥ˆpi
k(·|c)−pi(·|c)∥1≤βi
k(c)∀c∈Ci,∀i∈[Uτ] &
∥ˆrj
k(z)−rj(z)∥≤γj
k(z)∀z∈Zj∀j∈[Ur]/bracerightbig
. (21)
Where ˆpi
kdenotes the empirical estimate at time step kof theithatomic transition function, ˆrj
kdenotes the
empirical estimate at time step kof thejthreward function, βi
k(c)is the threshold for the error on the ith
transition function, and γj
k(z)is the error threshold for the error on the jthatomic reward function. We
19Under review as submission to TMLR
propose the following definition for the error thresholds:
βi
k(c) =/radicaligg
2
Nτ,i
tk(c)/parenleftbigg
Xlog/parenleftbigg2
δ/parenrightbigg
+ 2dilog/parenleftbiggX¯Ymi
tk
δ/parenrightbigg/parenrightbigg
∀i∈{1,···,Uτ}and∀c∈Ci(22)
γj
k(z) =/radicaltp/radicalvertex/radicalvertex/radicalvertex/radicalbtlog/parenleftbigg
X¯Ymj
rk
δ/parenrightbigg
max(1,Nr,j
tk(z))∀j∈{1,···,Ur}and∀z∈Zj, (23)
whereNr,j
tk(z)andNτ,i
tk(c)are defined above (elements 8 and 9 in the list).
C.1.1 Analysis of the Confidence Sets
Lemma C.2. For anyk≥1, the true MDP M∗
Gbelongs to the confidence set at episode k,Mk
G, defined in
equation 21 with probability:
P(M∗
G/∈Mk
G)≤δ/k.
Proof.TheL1deviation between the true atomic transition distribution pi(·|c)and its empirical estimate
ˆpi
k(·|c)is bounded for any ϵby (Weissman et al., 2003):
P/parenleftbigg
∥ˆpi
k(·|c)−pi(·|c)∥1≥ϵ/parenrightbigg
≤(2X−2) exp/parenleftbig
−nϵ2
2/parenrightbig
∀i∈{1,···,Uτ}and∀c∈Ci,(24)
whereXis the number of distinct outcomes of p(·|c)andnis a fixed number of samples.
We now define
ϵi
τ=/radicaligg
2
nlog/parenleftbigg2X2Uτ(X¯Y)dimi
tk2
δ/parenrightbigg
whereϵi
τ≤/radicaligg
2
n/parenleftbigg
Xlog/parenleftbigg2
δ/parenrightbigg
+ 2dilog/parenleftbigg2UτX¯Ymi
tk
δ/parenrightbigg/parenrightbigg
.
This upper bound on ϵi
τgives the exact expression of βi
k(c)defined in equation 22.
We can, therefore, show that the confidence bound in equation 21 holds with high probability. In particular,
for the atomic transition probability of a given context c∈Ciand fixed number of visits nwe have:
P/parenleftigg
∥ˆpi
k(·|c)−pi(·|c)∥1≥βi
k(c)/parenrightigg
=P/parenleftigg
∥ˆpi
k(·|c)−pi(·|c)∥1≥/radicaligg
2
n/parenleftbigg
Xlog/parenleftbigg2
δ/parenrightbigg
+ 2dilog/parenleftbigg2UτX¯Ymi
tk
δ/parenrightbigg/parenrightbigg/parenrightigg
≤2Xexp/parenleftbigg
−n
2(ϵi
τ)2/parenrightbigg
= 2Xexp/parenleftbigg
−n
22
nlog/parenleftbigg2X2Uτ(X¯Y)dimi
tk2
δ/parenrightbigg/parenrightbigg
=δ
2Uτ(X¯Y)dimi
tk2.
Now, with a fixed number of samples n, the deviation between the true atomic expected reward and the
empirical atomic mean reward is bounded by Hoeffding’s inequality for any ϵj
r>0:
P/parenleftbigg
|ˆrj
k(z)−rj(z)|≥ϵj
r/parenrightbigg
≤2 exp(−2n(ϵj
r)2)∀j∈{1,···,Ur}and∀z∈Zj. (25)
We define
ϵj
r=/radicaligg
1
2nlog/parenleftbigg4UrX¯Ymj
rk2
δ/parenrightbigg
and note that ϵj
r≤/radicaligg
1
nlog/parenleftbigg4UrX¯Ymj
rk
δ/parenrightbigg
.
20Under review as submission to TMLR
Again, this upper bound on ϵj
rwill be used to determine the exact expression of γj
k(z).
Combining Hoeffdings inequality with our definition of ϵj
r, we get:
P/parenleftigg
|ˆrj
k(z)−rj(z)|≥γj
k(z)/parenrightigg
=P/parenleftigg
|ˆrj
k(z)−rj(z)|≥/radicaligg
1
nlog/parenleftbigg4UrX¯Ymj
rk
δ/parenrightbigg/parenrightigg
(26)
≤2 exp(−2n(ϵj
r)2) (27)
= 2 exp/parenleftbigg
−2
nn
2log/parenleftbigg4UrX¯Ymj
rk2
δ/parenrightbigg/parenrightbigg
(28)
=δ
2UrX¯Ymj
rk2(29)
To compute the probability of interest P(M∗
G/∈Mk
G), we need to compute the union bound by summing
over all possible numbers of visits. It is important to note that in the same layer, more than one node
might observe a reward for the same atomic state-action pairz= (x,y). It is also possible that in the
same layer, more than one node observes the same transition context c. As a consequence, the number of
observations can increase by more than a unit per time step. If we perform kepisodes, the number of visits
for a specific reward context jmight range between 0 and kmj
r, wheremj
ris the number of occurrences of
reward architecture jinG(see element 5 of the above list for a more complete definition). Similarly, there
aremi
τnodes that belong to the equivalence class [τi], so a given transition context cican be observed up
tokmi
τtimes (see element 2 of the above list for a more complete definition). Now, computing the union
bound by summing over all possible numbers of visits, we obtain the following:
P/parenleftigg
|ˆrj
k(z)−rj(z)|≥/radicaltp/radicalvertex/radicalvertex/radicalbtlog/parenleftbig4UrX¯Ymj
rk
δ/parenrightbig
max(1,Nr,j
tk(zj))/parenrightigg
≤mj
rk/summationdisplay
n=1δ
2UrX¯Ymj
rk2<δ
2UrX¯Yk(30)
P/parenleftigg
∥ˆpi
k(·|c)−pi(·|c)∥1≥/radicaltp/radicalvertex/radicalvertex/radicalbt2
max(1,Nτ,i
tk(ci))/parenleftbigg
Xlog/parenleftbigg2
δ/parenrightbigg
+ 2dilog/parenleftbigg2UτX¯Ymi
tk
δ/parenrightbigg/parenrightigg
≤mi
tk/summationdisplay
n=1δ
2Uτ(XY)dimi
tk2<δ
2Uτ(X¯Y)dik
This allows us to define βi
k(c)(introduced in equation 21) for all i∈{1,···,Uτ}and for allc∈Ci:
βi
k(c):=/radicaligg
2
max(1,Nτ,i
tk(c))/parenleftbigg
Xlog/parenleftbigg2
δ/parenrightbigg
+ 2dilog/parenleftbigg2UτX¯Ymi
tk
δ/parenrightbigg/parenrightbigg
(31)
andγj
k(z)for allj∈{1,···,Ur}and for allz∈Zi, as
γj
k(z):=/radicaltp/radicalvertex/radicalvertex/radicalbtlog/parenleftbig4UrX¯Ymj
rk
δ/parenrightbig
max(1,Nr,j
tk(z)). (32)
21Under review as submission to TMLR
Summing the probabilities over all possible reward contexts z∈Zj(the number of which is bounded by
X¯Y) and transition context c∈Ci(the number of which is bounded by (X¯Y)di) we get
P(M∗
G/∈Mk
G) =Ur/summationdisplay
j=1/summationdisplay
z∈ZjP(|ˆrj
k(z)−rj(z)|<γj
k(z)) +Uτ/summationdisplay
i=1/summationdisplay
c∈CiP(∥ˆpi
k(·|c)−pi(·|c)∥<βi
k(c))(33)
≤Ur/summationdisplay
j=1/summationdisplay
z∈Zjδ
2UrX¯Yk+Uτ/summationdisplay
i=1/summationdisplay
c∈Ciδ
2Uτ(X¯Y)dik≤δ
k(34)
as desired.
C.2 Upper Bounding the Regret
The regret can be upper bounded by the sum of the errors made at each node.
Lemma C.3. IfMG,kandM∗
Gbelong to the confidence set at time step k,Mk
G, then the one-step Bellman
error is upper bounded by the size of the confidence interval:
|(Tk
µk,h−T∗
µk,h)Vk
µk,h+1(sh+1)|≤Hmin{n/summationdisplay
v=1βiv
k(cv
k) +γjv
k(zv
k),1} (35)
Proof.Error in the atomic Bellman operator has the most impact if the value function is high; we can then
directly upper bound the value function with H.
|(Tk
µk,h−T∗
µk,h)Vk
µk,h+1(sh+1)|≤H|(Tk
µk,h−T∗
µk,h)| (36)
≤H/parenleftbigg/vextendsingle/vextendsingle/vextendsingle/vextendsinglenh/summationdisplay
v=1rjv
k(xv
k|cv
h,µ) +nh+1/productdisplay
v=1piv
k(xv
h+1|cv
h+1,µ)−nh/summationdisplay
v=1rjv
∗(xv
k|cv
h,µ) +nh+1/productdisplay
v=1piv
∗(xv
h+1|cv
h+1,µ)/vextendsingle/vextendsingle/vextendsingle/vextendsingle/parenrightbigg
(37)
≤H/parenleftbigg/vextendsingle/vextendsingle/vextendsingle/vextendsinglenh/summationdisplay
v=1rjv
k(xv
k|cv
h,µ)−nh/summationdisplay
v=1rjv
∗(xv
k|cv
h,µ)/vextendsingle/vextendsingle/vextendsingle/vextendsingle+/vextendsingle/vextendsingle/vextendsingle/vextendsinglenh+1/productdisplay
v=1piv
k(xv
h+1|cv
h+1,µ)−nh+1/productdisplay
v=1piv
∗(xv
h+1|cv
h+1,µ)/vextendsingle/vextendsingle/vextendsingle/vextendsingle/parenrightbigg
(38)
≤H/parenleftbigg/vextendsingle/vextendsingle/vextendsingle/vextendsinglenh/summationdisplay
v=1rjv
k(xv
k|cv
h,µ)−nh/summationdisplay
v=1rjv
∗(xv
k|cv
h,µ)/vextendsingle/vextendsingle/vextendsingle/vextendsingle+nh+1/summationdisplay
v=1/vextendsingle/vextendsingle/vextendsingle/vextendsinglepiv
k(xv
h+1|cv
h+1,µ)−piv
∗(xv
h+1|cv
h+1,µ)/vextendsingle/vextendsingle/vextendsingle/vextendsingle/parenrightbigg
(39)
≤Hmin/braceleftbiggnh/summationdisplay
v=1γjv
k(zv
h,µ) +βiv
k(cv
h+1,µ),1/bracerightbigg
(40)
Equation 37 is directly obtained using the definition of the atomic Bellman operator equation 7. Then
equation 38 is obtained by grouping together the terms depending on the atomic reward functions and the
one depending on the atomic transition functions. To obtain equation 39 we use the following inequality:
n/productdisplay
i=1ai−n/productdisplay
i=1bi≤/summationdisplay
i=1|ai−bi|,∀ai,bi≤1.
Finally, in equation 40, we upper-bound the error in the transition and reward function at each node with
the width of the corresponding confidence interval. Note that since the reward at each time step is bounded
in[0,1], the sum of all the nodes’ errors in a given layer is upper-bounded by 1.
With this upper bound on the Bellman error, we are now equipped to upper-bound the regret; we first
decompose it as follows, observing that ˜∆k≤Hfor anyk∈{1,···,K}. Hence,
K/summationdisplay
k=1˜∆k≤K/summationdisplay
k=1˜∆k 1{Mk
G,M∗
G∈Mk
G}+HK/summationdisplay
k=1/bracketleftbig
1{M∗
G/∈Mk
G}+ 1{Mk
G/∈Mk
G}/bracketrightbig
.
22Under review as submission to TMLR
By Lemma C.1, E[ 1{MG,k/∈Mk
G}|Dtk] =E[ 1{M∗
G/∈Mk
G}|Dtk]. Additionally, setting δ=1
Kin Lemma C.2
shows the true DAMDP M∗
Gdoes not belong to the confidence set at time step k,Mk
G, with probability
P(M∗
G/∈Mk
G)<1
K. Then,
E/bracketleftbiggK/summationdisplay
k=1˜∆k/bracketrightbigg
≤E/bracketleftbiggK/summationdisplay
k=1˜∆k 1{MG,k,M∗
G∈Mk
G}/bracketrightbigg
+ 2HK/summationdisplay
k=1P(M∗/∈Mk) (41)
≤E/bracketleftbiggK/summationdisplay
k=1E[˜∆k|M∗
G,MG,k] 1{MG,k,M∗
G∈Mk
G}/bracketrightbigg
+ 2H (42)
≤E/bracketleftbiggK/summationdisplay
k=1H/summationdisplay
h=1|(Tk
µk,h−T∗
µk,h)Vk
µk,h+1(stk+1)| 1{MG,k,M∗
G∈Mk
G}/bracketrightbigg
+ 2H (43)
≤HE/bracketleftbiggK/summationdisplay
k=1H/summationdisplay
h=1min/braceleftbiggnh/summationdisplay
v=1βiv
k(cv
k) +γjv
k(zv
k),1/bracerightbigg/bracketrightbigg
+ 2H, (44)
where Eq. 42 is obtained by applying the tower property and noting that Mk
Gis measurable if MG,kis
known. Equation 43 is the definition of the modified regret term (see equation 18). Finally, equation 44 is a
direct consequence of Lemma C.3. We also denote by nthe number of nodes in G; we useivto denote the
equivalence class of the node vandjvto denote the atomic action set available at node v. The transition
context observed during episode kat nodevis denoted by cv
k∈Civ, and the reward context observed at
nodevduring the kthepisode is denoted by zv
k∈Zjv.
The contribution to the regret incurred by errors in the reward function rj
kcan be upper-bounded by the
sum ofγj
kfor every episode and every node v∈Vj, whereVjis the set of nodes that exhibit the reward
architecture j. Then, for a single reward architecture jwe have:
K/summationdisplay
k=1/summationdisplay
v∈Vjγj
k(zv
k) =K/summationdisplay
k=1/summationdisplay
v∈Vjγj
k(zv
k) 1{Nr,j
tk(zv
k)≤mj
r}+K/summationdisplay
k=1/summationdisplay
v∈Vjγj
k(zv
k) 1{Nr,j
tk(zv
k)>mj
r}
≤K/summationdisplay
k=1/summationdisplay
v∈Vj1{Nr,j
tk(zv
k)≤mj
r}+K/summationdisplay
k=1/summationdisplay
v∈Vj1{Nr,j
tk(zv
k)>mj
r}/radicaltp/radicalvertex/radicalvertex/radicalbtlog/parenleftbig4UrX¯Ymj
rk
δ/parenrightbig
max(1,Nr,j
tk(zv
k)).
Wherezv
k∈Zjis used to denote the reward context observed in the vthnode ofGduring the kthepisode.
Consider a fixed zv
k=z. The case where Nr,j
tk(z)≤mj
rhappens less than 2mj
rtimes for each z∈Zjcan be
upper bounded as follows/summationtextK
k=1/summationtext
v∈Vj1{Nr,j
tk(z)≤mj
r}≤2mj
rX¯Y. Now, let’s suppose that Nr,j
tk(z)>mj
rfor
az∈Zj. In thekthepisode, for any node v∈Vj, we haveNr,j
tk,n(z) + 1≤Nr,j
tk(z) +mj
r≤2Nr,j
tk(z). Note
that the number of occurrences of a specific reward architecture in each layer depends on G, but we know
that it occurs mj
rtimes over an episode.
23Under review as submission to TMLR
We can then bound the following ratio:
K/summationdisplay
k=1/summationdisplay
v∈Vj/radicaltp/radicalvertex/radicalvertex/radicalbt1{Nr,j
tk(zk,v)>mj
r}
Nr,j
tk(zk,v)≤K/summationdisplay
k=1/summationdisplay
v∈Vj/radicaligg
2
Nr,j
tk,v(zk,v) + 1(45)
=√
2Kmr
j/summationdisplay
t=1(Nr,j
t(zt) + 1)−1/2(46)
≤√
2/summationdisplay
z∈ZjNr,j
Tj(z)/summationdisplay
b=1b−1/2(withTj=Kmj
r+ 1) (47)
≤√
2/summationdisplay
z∈Zj/integraldisplayNr,j
Tj(z)
0x−1/2dx (48)
≤/radicaligg
2X¯Y/summationdisplay
z∈ZjNr,j
Tj(z) =/radicalig
2X¯YKmj
r. (49)
Equation 46 rewrites the two sums as a single one, where now, each index tinNr,j
tuniquely encodes a pair
(tk,n). We obtain equation 47 by considering all possible reward contexts z∈Zjand the total number of
times this specific reward context was visited. In equation 48 we use the fact that the sum/summationtextN
i=n+1x−1/2is
upper bounded by/integraltextN
nx−1/2dx. Lastly, equation 49 is obtained by Cauchy-Schwartz inequality.
With the same approach, we can bound the regret incurred by the error in the ithtransition function,
K/summationdisplay
k=1/summationdisplay
v∈Viβi
k(cv
k)≤K/summationdisplay
k=1/summationdisplay
v∈Vi1{Nτ,i
tk(cv
k)≤miτ}+K/summationdisplay
k=1/summationdisplay
v∈Vi1{Nτ,i
tk(cv
k)>miτ}βi
k(cv
k),
whereVidenotes all the nodes of the equivalence class [τi]. For anyc∈Ci, the case where Nτ,i
tk(c)≤mi
τ
happens less than 2mi
τtimes. Given that |Ci|≤(X¯Y)diwe can upper-bound/summationtextK
k=1/summationtext
v∈Vi1{Nτ,i
tk(c)≤mi
τ}≤
2mi
τ(X¯Y)di. For a given transition context cand episode kwhenNτ,i
tk> mi
τ, we have that for any node
V∈Vi,Nτ,i
tk,v(c) + 1≤Nτ,i
tk(c) +mi
τ≤2Nτ,i
tk(c).
We can then bound the following ratio:
K/summationdisplay
k=1/summationdisplay
v∈Vi/radicaltp/radicalvertex/radicalvertex/radicalbt1{Nτ,i
tk(cv
k)>miτ}
Nτ,i
tk(ckv)≤K/summationdisplay
k=1/summationdisplay
v∈Vi/radicaligg
2
Nτ,i
tk,v(cv
k) + 1(50)
=√
2Kmτ
i/summationdisplay
t=1(Nτ,i
t(ct) + 1)−1/2(51)
≤√
2/summationdisplay
c∈CiNτ,i
Ti(c)/summationdisplay
b=1b−1/2(withTi=Kmi
τ+ 1) (52)
≤√
2/summationdisplay
c∈Ci/integraldisplayNτ,i
Ti(c)
0x−1/2dx (53)
≤/radicaligg
2(X¯Y)di/summationdisplay
c∈CiNτ,i
Ti(c) =/radicalig
2(X¯Y)diKmiτ. (54)
Where all steps here are obtained following the same reasoning as in equation 45 toequation 49.
24Under review as submission to TMLR
Because the rewards at every time step (or layer) are bounded in [0,1], the total regret consists of the
following:
min/braceleftbigg
HK/summationdisplay
k=1H/summationdisplay
h=1min/braceleftbiggnh/summationdisplay
v=1γiv
k(zv
k) +βiv
k(cv
k),1/bracerightbigg
,T/bracerightbigg
(55)
≤min/braceleftbigg
HUr/summationdisplay
j=12mj
rX¯Y+/radicalig
2X¯YKmj
rlog(4UrX¯Ymj
rK)
+HUτ/summationdisplay
i=12mi
τ(X¯Y)di+/radicalbigg
2(X¯Y)diKmiτ/parenleftbig
Xlog(2
δ+ 2dilog(2UτX¯Ymiτ))/parenrightbig
,KH/bracerightbigg
(56)
≤min/braceleftbigg
HUr/summationdisplay
j=12mj
rX¯Y+/radicalig
2X¯YKmj
rlog(4UrX¯Ymj
rK),KH/bracerightbigg
+ min/braceleftbigg
HUτ/summationdisplay
i=12mi
τ(X¯Y)di+/radicalbigg
2(X¯Y)diKmiτ/parenleftbig
Xlog(2
δ+ 2dilog(2UτX¯Ymiτ))/parenrightbig
,KH/bracerightbigg
(57)
To complete the proof, we note that min(a+b,c)≤√ac+bholds fora,b,c > 0. We then apply this
inequality twice in equation 57.
Equation 57≤/radicaltp/radicalvertex/radicalvertex/radicalbtKHUr/summationdisplay
j=12mj
rX¯YH+Ur/summationdisplay
j=1/radicalig
2X¯YKmj
rlog(4UrX¯Ymj
rK)H
+/radicaltp/radicalvertex/radicalvertex/radicalbtKHUτ/summationdisplay
i=12miτ(X¯Y)diH+Uτ/summationdisplay
i=1/radicalig
2(X¯Y)diHKmiτ(X+ 2dilog(2UτX¯YmiτK))
≤HUr/summationdisplay
j=1/radicalig
2X¯YKmj
rlog(4UrX¯Ymj
rK) +Uτ/summationdisplay
i=1H/radicalig
2(X¯Y)diKmiτ(X+ 2dilog(2UτX¯YmiτK))
This gives us the result obtained in Eq. 10.
D Regret of PSRL on the fullMDP
AnaturalbaselinetoourapproachistoconsiderthePosteriorSamplingforReinforcementLearning(Osband
et al., 2013) algorithm. This algorithm can solve any DAMDP by considering the MDP built from the
DAMDP’s atomic components. In that case, the algorithm will directly learn the full state transition
distribution Pand the reward function R.
To simplify the analysis, we upper bound the state space at each time step t,St⊆XNmax, with the largest
state space in the MDP, note that Nmaxdenotes the number of nodes in the largest layer. Similarly, we
upper bound the full action space at each time step t,A⊆ ¯YNmax.
If we apply the results of Osband et al. (2013) to the proposed MDP, we get a regret of
O/parenleftbigg
HXNmax/radicalig
¯YNmaxTlog((X¯Y)NmaxT)/parenrightbigg
(58)
E Additional properties of DAMDP
Remark E.1.Any DAMDP with an arbitrary DAG, G, has an equivalent DAMDP with a layered directed
acyclic graph, G′.
25Under review as submission to TMLR
G1:12
34 G′
1:12
34 x
G2:12
34 G′
2:12
34
x1x2
Figure 5: Illustration of how to transform a directed acyclic graph (on the left) into a layered directed acyclic
graph (on the right)
We now show how a LDAG, G′, can be constructed from a DAG, G. In particular, we focus on two possible
transformations that are illustrated in Figure 5, that is, when an edge spans more than one layer (first row
of Fig. 5) or when we have a transversal edge, i.e. an edge that connects two nodes that belong to the same
layer, (second row of Fig. 5).
Note that both illustrations Figure 5 suggest that to transform a DAG into LDAG, we need to add artificial
nodes (represented by a red square) and artificial edges (coloured in blue and orange). The artificial node
does not directly increase the complexity of the problem, as both its atomic state and atomic action will
be identical to the atomic state and atomic action of its unique parent node. The additional edges do not
increase the problem complexity as when we remove an "illegal" edge with a single orange edge and a single
blue edge. In particular, it leaves the number of equivalence classes Uτunchanged since all incoming illegal
edges were replaced by a single blue edge, and it leaves the number of action spaces unchanged as well as
all outgoing illegal edges are replaced by an orange edge. Since those transformations are always possible
and leave the DAMDP characteristics unchanged, they have no impact on the algorithm complexity measure
presented in Theorem 5.1.
While this argument is reasonable in problems of the type of "the leaky maximum flow" problem, this
will not necessarily hold for more complex problems such as the wind farm optimization problem. Indeed,
the LDAG assumption allowed us to assume all the layers were spaced uniformly in the field. Hence, the
transition context did not necessarily have to include information about the relative distance between two
wind turbines, for example. If we would like to consider such scenarios, then the transformations are not
trivial and will have repercussions on the algorithm’s regret.
F Additional experiments
In Figure 6, we present an additional set of leaky maximum flow experiments. We consider an alternative
graph structure presented in the first row of Figure 6. Observing the respective performance of PSRL and
PSGRL, we can see that as the graph complexity increases, the benefit of PSGRL increases as well. These
results are similar to the experiment presented in the main paper; we just revisit this experiment considering
a different network architecture. In the leftmost case, we see no benefit in using PSGRL as we have a single
node per layer. However, as we increase the central grid of nodes, creating multiple nodes that belong to the
same equivalence class, the benefit of PSGRL becomes more evident.
26Under review as submission to TMLR
Figure 6: The first row depicts the graph that governs the DAMDP. The second row shows the learning
curve for both algorithms considered. PSRL, which ignores the latent graphical structure and PSGRL,
which leverages the graphical structure. The left-most plot shows the performance obtained on a simple
chain graph. As expected, the performance for both algorithms is similar. Looking at the remaining plots, it
becomes clear that as we increase the complexity of the graph, the benefit of PSGRL becomes self-evident.
27