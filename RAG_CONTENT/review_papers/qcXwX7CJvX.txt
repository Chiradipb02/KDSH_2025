Under review as submission to TMLR
SLM: End-to-end Feature Selection
via Sparse Learnable Masks
Anonymous authors
Paper under double-blind review
Abstract
Feature selection has been widely used to alleviate compute requirements during training,
elucidate model interpretability, and improve model generalizability. We propose SLM –
Sparse Learnable Masks – a canonical approach for end-to-end feature selection that scales
well with respect to both the feature dimension and the number of samples. At the heart
of SLM lies a simple but eﬀective learnable sparse mask, which learns which features to
select, and gives rise to a novel objective that provably maximizes the mutual information
(MI) between the selected features and the labels, which can be derived from a quadratic
relaxation of mutual information from ﬁrst principles. In addition, we derive a scaling
mechanism that allows SLM to precisely control the number of features selected, through a
novel use of sparsemax. This allows for more eﬀective learning as demonstrated in ablation
studies. Empirically, SLM achieves state-of-the-art results against a variety of competitive
baselines on eight benchmark datasets, often by a signiﬁcant margin, especially on those
with real-world challenges such as class imbalance.
1 Introduction
In many machine learning scenarios, a signiﬁcant portion of the input features may be irrelevant to the
output, especially with modern data management tools allowing easy construction of large-scale datasets by
joining features from many diﬀerent data sources. “Feature selection", or ﬁltering the most relevant features
for the downstream task, is an everlasting problem, with many methods proposed to date and used (Guyon
& Elisseeﬀ, 2003; Li et al., 2017; Dash & Liu, 1997).
Feature selection can bring a multitude of beneﬁts. Smaller number of features can yield superior generalization
and hence better test accuracy, by minimizing reliance on spurious patterns that do not hold consistently
(Sagawa et al., 2020), and not wasting model capacity on less relevant features. In addition, reducing the
number of input features can decrease the computational complexity and cost for deployed models, as the
models need to learn from smaller dimensional input data, and hence require reduced infrastructure. Lastly,
feature selection facilitates interpretability, as it sheds light on which features are most relevant for the
downstream task.
Given the wide applicability of feature selection, how can one select the target number of features in an
eﬃcient, eﬀective way? §2 summarizes numerous approaches. For superior task accuracy, one desired property
is that the feature selection method should consider the predictive model itself, as the optimal set of features
would depend on the mapping between the input data and output labels. Such end-to-end learning methods
have been approached in diﬀerent ways, such as via sparse regularization and its extensions (Lemhadri et al.,
2019), concrete autoencoders (Abid et al., 2019), or learned stochastic gates (Yamada et al., 2020), among
others. These constitute diﬀerent ways to tackle the fundamental challenge, making the feature selection
operation diﬀerentiable.
In this work, we present SLM – Sparse Learnable Masks – a novel soft approximator for the feature selection
with end-to-end learning. SLM is designed to be scalable and easily-adaptable into a variety of models, and
yields superior task performance. At its heart, SLM learns a sparse mask to ﬁlter out non-selected features.
This mask gives rise to a novel mutual information (MI) objective, which provably maximizes the MI between
1Under review as submission to TMLR
the labels and the selected features, based on a novel quadratic relaxation of the MI (§4). Furthermore, SLM
proposes a scaling mechanism for sparsemax (Martins & Astudillo, 2016) to precisely control the number of
features selected, which when allowed to vary during training, enables more eﬀective learning as demonstrated
in ablation studies. SLM scales well with respect to both the feature dimension and the number of features.
Speciﬁcally, as detailed in §4.4, it scales O(n)with respect to the dataset size n, andO(FlogF)with respect
to the feature dimension F. SLM can be integrated into any deep learning architecture, given the optimization
is gradient-descent based for joint training. We demonstrate state-of-the-art task performance with SLM,
against a myriad of competitive baseline methods, on nine datasets from wide-ranging domains.
2 Related work
Feature selection methods : Numerous methods have been studied for feature selection, and broadly fall
under three categories (Guyon & Elisseeﬀ, 2003):
•Wrappers recompute the predictive model for each subset of features. As exhaustive search is NP-hard and
computationally intractable, eﬃcient search strategies such as forward selection or backward elimination
have been developed. For instance, HSIC-Lasso (Yamada et al., 2014) proposes a feature-wise kernelized
Lasso for capturing non-linear dependencies. Wrappers are diﬃcult to integrate with modern deep learning,
as the training complexity gets prohibitively large.
•Filtersselect subsets of variables as a pre-processing step, independent of the predictive model. (Gu et al.,
2012) developed the Fisher score, which selects features to maximize (minimize) the distances between
data points in diﬀerent (same) classes in the space spanned by the selected features. Principal feature
analysis (PFA) (Lu et al., 2007b) selects features based on principal component analysis. (Pan et al., 2020)
uses adversarial validation to select the features, based on how much their characteristics diﬀer between
training and test splits, as a way to improve robustness. There are also various methods based on MI
maximization (Ding & Peng, 2005), selecting features independent of the predictive model (unlike SLM).
CMIM (Fleuret, 2004) maximizes the conditional MI between selected features and the class labels to
account for feature inter-dependence. On the other hand, JMIM (Bennasar et al., 2015) maximizes the
jointMI between class labels and the selected features, while addressing overconﬁdence in features that
correlate with already-selected features, with greedy search that selects features one at a time. (Zadeh
et al., 2017) formulates feature selection as a diversity maximization problem using a MI-based metric
amongst features. The fundamental disadvantage of ﬁlter-based methods, of not being optimized with the
predictive models, results in them often yielding suboptimal performance.
•Embedded methods combine selection into training and are usually speciﬁc to given predictive models.
Lasso regularization (Tibshirani, 1996) employs feature selection by varying the strength of the L1
regularization. (Feng & Simon, 2017) extends this idea by proposing an input-sparse neural network, where
the input weights are penalized using the group Lasso penalty. (Lemhadri et al., 2019) selects only a subset
of the features using input-to-output residual connections, allowing features to have non-zero weights only if
their skip-layer connections are active. Concrete Autoencoder (Abid et al., 2019) proposes an unsupervised
feature selector based on using a concrete selector layer as the encoder and using a standard neural network
as the decoder. FsNet (Singh et al., 2020) uses a concrete random variable for discrete feature selection
in a selector layer and a supervised deep neural network regularized with the reconstruction loss. STG
(Yamada et al., 2020) learns stochastic gates with a probabilistic relaxation of the count of the number of
selected features, it selects features and learns task prediction end-to-end.
Masking in deep neural networks : Masking the input to control information propagation is a commonly-
used approach in deep learning. Attention-based architectures, such as Transformer (Vaswani et al., 2017)
and Perceiver (Jaegle et al., 2021), show strong results across many domains, with learnable key and query
representations, whose alignment yields the masks that control the contribution of corresponding value
representations. While these eﬀectively reweigh the input, they typically do not completely mask out (i.e.
yielding zero attention weight) any part of the input. Towards this end, various works have focused on
bringing sparsity into masking, such as based on thresholding (Zhao et al., 2019) or sparse normalization
(Correia et al., 2019). TabNet (Arik & Pﬁster, 2019) directly generates sparse attention masks and applies
them sequentially to input data, which can perform sample-dependent feature selection. (Correia et al., 2020)
achieves sparsity in latent distributions in neural networks, by using sparsemax and its structured analogs,
2Under review as submission to TMLR
allowing for eﬃcient latent variable marginalization. (Lei et al., 2016) and (Bastings et al., 2019) learn
Bernoulli variables, which are analogous to SLM’s feature mask but in a local setting, for extractive rationale
prediction in text. (Paranjape et al., 2020) extends these ideas by proposing to control sparsity by optimizing
the Kullback–Leibler (KL) divergence between the mask distribution and a prior distribution with controllable
sparsity levels. (Guerreiro & Martins, 2021) develops a ﬂexible mask-based rationale extraction mechanism
using a constrained structured prediction algorithm on factor graphs. All these perform sample-wise , not
global, input selection. In this work, our goal is to explore globalfeature selection. When training and
testing datasets perfectly align in distribution, local feature selection can give superior performance due to its
input-dependence. However, there is rarely such perfect alignment, and global selection provides robustness
beneﬁts when there is distribution shift between training and test datasets, in addition to allowing more
computational eﬃciency by globally removing features.
3 Methods
Algorithm 1 describes SLM’s end-to-end feature selection and task learning. The predictor fθcan be any
gradient-descent based model, such as an MLP, with a task-speciﬁc loss function lsuch as the cross entropy
for classiﬁcation or mean absolute error (MAE) for regression. The following sections present SLM’s key
components in detail.
Notation. Throughout this work, we let X∈Rn×ddenote the input data, Xsp∈Rn×dthe selected features,
andmsp∈Rdthe learned sparse feature selection mask. We use ⊙to denote element-wise multiplication
between each input sample and msp:Xsp=X⊙msp. We letFtdenote the number of selected features at
stept, andNthe total training steps. Furthermore, I(X,Y )denotes the mutual information between Xand
Y, andIq(X,Y )is its quadratic relaxation. fθdenotes the task predictor used on the selected features.
Overview of SLM-based feature selection. As outlined in Algorithm 1, SLM ﬁrst learns a non-sparse
mask m∈Rd, which is turned into a sparse vector by applying the sparsemax operator (Martins & Astudillo,
2016), described in §3.1. We present a novel application of sparsemax that provably achieves output sparsity
at desired level exactly, for which we propose dynamically computing a scaling constant for the mask m,
detailed in §3.2. SLM uses the resulting sparse mask to zero out non-selected features. The mask sparsity
gradually increases throughout training to facilitate model convergence (§3.3). Finally, a predictor model
fθon the selected features is trained using the dataset task loss and a novel mutual information (MI) loss,
which is derived from ﬁrst principles in §4. The following sections explain the important constitutents of
SLM in detail.
Algorithm 1 Training for SLM-based feature selection.
Input:Input data Xwith target labels Y
Input:Total training steps N
Initialize: Learnable mask argument m←all ones vector
fort= 1toNdo
Obtain the number of selected features Ftusing Eq 4 in §3.3 for step t.
Compute scaling parameter mfor mask argument to control exact mask sparsity, using Lemma 3.2 in
§3.2.
Generate sparse mask msp=sparsemax (m∗m).
Selectandweightinput features with mask: Xsp=X⊙msp. Non-selected features are zeroed out.
Input the selected features into the predictor fθ(Xsp)for the downstream task.
Compute dataset task loss l(Xsp,Y)andMI lossE(Xsp,Y)in Eq 9 from §4.3.
Use the combined loss to update the model parameters θandm.
3.1 Mask sparsity via projection onto probability simplex
SLM selects features by learning a mask msp∈Rd, and zeroing out the features in the input X∈Rn×d
whose corresponding mask entries are zero. We use sparsemax normalization (Martins & Astudillo, 2016) to
achieve sparsity in m. Sparsemax achieves sparsity in its output by returning the Euclidean projection of the
3Under review as submission to TMLR
input vector v∈Rdonto the probability simplex ∆d−1:={f∈Rd
≥0|/summationtext
kfk= 1}:
sparsemax (v):=argminp∈∆d−1/bardblp−v/bardbl2. (1)
We apply sparsemax to the mask argument m∈Rdto obtain sparse feature mask:
msp:=sparsemax (m). (2)
In particular msp∈Rd
≥0. Compared to approaches like softmax normalization employed with thresholding,
the probability simplex projection in sparsemax (v)scales the top values in vso they are more equidistributed
over [0,1]. This equidistribution leads to greater feature weight separation, encouraging the model to
discriminate amongst the features. Additional discussion on the properties of SLM sparsemax can be found
in §A.5.
3.2 Mask scaling to yield desired number of selected features
Following its formulation, sparsemax does not yield a predetermined number of non-zero elements, as the
sparsity depends on the location on the probability simplex ∆d−1thatvprojects onto. For a non-uniform
vector v∈Rd, we can adjust its projection onto ∆d−1by multiplying vby a positive scalar. In particular,
a suﬃciently large scalar increases the sparsity, while a suﬃciently small scalar decreases the sparsity. To
illustrate this, we provide a simple example in Fig 1.
Example 3.1 (Adjusting sparsemax (v)sparsity by scaling) .The probability simplex ∆1inR2is the line
connecting (0,1)and(1,0), with these two points as the simplex boundary. Let v= (x,y)be a point in
R2, and (z,w)its projection onto ∆1. We show that by varying multiplier m,sparsemax (mv)would have
a varying degree of sparsity. The projection (z,w) =sparsemax ((x,y))is the unique point that satisﬁes
(z,w) =argmin(z,w)(/bardbly−w/bardbl2+/bardblx−z/bardbl2),(z,w)element-wise positive, and z+w= 1. As we scale (x,y)
withm, sparsemax (m(x,y)) =argmin (z,w)(/bardblmy−w/bardbl2+/bardblmx−z/bardbl2). This projection distance expands to
d(z,w):=/bardblmy−w/bardbl2+/bardblmx−z/bardbl2
=m2y2−2myw +w2+m2x2−2mxz +z2
Hence,d(0,1)−d(0.5,0.5) =mx−my+ 0.5(where (0.5,0.5)is the midpoint of the simplex), which means
that for any (x,y)andmwithy>x, sparsemax( m(x,y)) is closer to (0,1)∈∆1wheneverm> 1/(2(y−x)),
and closer to (0.5,0.5)otherwise. Since projection is linear, this means varying the multiplier mvaries the
sparsity of sparsemax ((x,y)). Figure 1 illustrates a concrete instance of scaling in the 2D case.
This example conveys the intuition that larger multipliers lead to sparser outputs. More generally, one can
show:
Lemma 3.2. Given a non-uniform vector v∈Rd, to obtainFnonzero elements in sparsemax( v),vshould
be multiplied with the scalar
m=

/parenleftBig/summationtextF+1
i=1v(i)−(F+ 1)·v(F+1)/parenrightBig−1
if|sparsemax (v)>0|>F
/parenleftBig/summationtextF
i=1v(i)−F·v(F)/parenrightBig−1
if|sparsemax (v)>0|<F,(3)
wherev(1)≥v(2)...≥v(d)denote sorted elements of vin descending order.
The proof can be found in §A.3. Lemma 3.2 allows us to scale the mask to achieve the desired number of
non-zero features. Note that since sparsemax has a particular Fenchel-Young loss (Blondel et al., 2020),
scaling its argument by mis equivalent to scaling the regularizer by 1/min the Fenchel-Young formulation
(Blondel et al., 2020; Peters et al., 2019).
3.3 Tempering feature sparsity to facilitate convergence
Starting training on only a randomly selected subset of features likely leads to suboptimal learning in the
initial steps, and if feature selection converges before the predictor converges, the predictor would be trained
4Under review as submission to TMLR
Figure 1: Mask scaling for sparsemax: We show an illustrative example on how varying the multiplier varies
the sparsity. Scaling vfrom theblackto theredpoint moves its projection (green dotted line) onto ∆1closer
to the simplex boundary, increasing sparsemax (v)sparsity, as the xcoordinate of the projection becomes 0.
The orange dotted line indicates the path of scaling by a constant. Example 3.1 contains concrete calculations
demonstrating this phenomenon.
with suboptimal features. To alleviate these and improve training stability, we propose gradually decreasing
the number of features selected until reaching the target FN:
Ft=/braceleftBigg
F0−t/Ntmp(F0−FN)ift<Ntmp
FN ift≥Ntmp,(4)
whereFtis the number of selected features at step t,Ntmpis the tempering threshold. In our experiments, we
simply setNtmp=N/2as it’s observed to be a reasonable value across a wide range of datasets (as before N
denotes the total number of training steps). To further stabilize training, instead of continuously decreasing
the number of features, we decrease the number of features at ﬁve evenly spaced steps. This tempering allows
the model to learn from more than the ﬁnal target number of features during training – an advantage not
shared by baseline methods. Furthermore, learning from all features initially likely provides a more robust
initialization compared to starting learning with the target number of features, as the randomness in the
initial selection is seldom optimal.
4 Mutual information maximization
As an inductive bias to the model that accounts for sample labels during feature selection, we propose to
maximize the mutual information (MI) between the distribution of the selected features and the distribution
of the labels. Speciﬁcally, we condition the MI on the probability that a feature is selected, as given by the
mask m. This stands in contrast to prior MI-based feature selection works such as (Fleuret, 2004; Bennasar
et al., 2015), which yield binary decisions on whether to select a feature.
LetXdenotethe random variable representing the features, and Ythe random variable representing the
labels, with value spaces X∈XandY∈Y. We letXandYbe discrete, following a long line of research on
mutual information and entropy estimation that focuses on the case where the random variables live in the
discrete space (Paninski, 2003; Kraskov et al., 2004; Valiant & Valiant, 2011; Han et al., 2015; Jiao et al., 2015;
Wu & Yang, 2016), this is because 1) many variables in machine learning are indeed discrete, e.g. vocabulary
index in NLP, categorical variables such as nationality, gender, etc, and 2) MI estimation in the continuous
case can be reduced to the discrete case via binning and taking a limit (Paninski, 2003; Kraskov et al., 2004).
Feature selection methods based on maximizing either the conditional or the joint MI between selected
features and labels require the computation of an exponential number of probabilities, the optimization of
which is intractable (Fleuret, 2004). Therefore, we propose an end-to-end diﬀerentiable, quadratic relaxation
5Under review as submission to TMLR
for MI. When we model XandYas random variables, their MI I(X,Y )can be deﬁned and reformulated as:
I(X,Y ):=/summationdisplay
x∈X/summationdisplay
y∈YPX,Y(x,y) logPX,Y(x,y)
PX(x)PY(y)
=/parenleftbigg/summationdisplay
x∈X/summationdisplay
y∈YPX,Y(x,y) logPX,Y(x,y)
PX(x)/parenrightbigg
−/summationdisplay
y∈YPY(y) logPY(y), (5)
where the second step derives from marginalizing over X. Since the second term above does not depend on
featuresX, it can be ignored during optimization.
4.1 Quadratic relaxation
We propose a quadratic relaxation Iq(X,Y )of Eq 5 to simplify I(X,Y )and its optimization, while retaining
much of its properties:
Iq(X,Y ):=/parenleftBig/summationdisplay
x∈X/summationdisplay
y∈YPX,Y(x,y)2/PX(x)/parenrightBig
−/summationdisplay
y∈YPY(y)2. (6)
Here, terms of the form plogqare relaxed to pq. Note that both plogqandpqare convex with respect
topandq, and hence have the same correlation behavior with respect to pandq. From an optimiza-
tion perspective, Iq(X,Y)is a good approximation of I(X,Y)wherePX,Y(X,Y)/PX(x)andPY(y)in
Eq 6 lie in the neighborhood (1−δ,1+δ). In this neighborhood, using Taylor expansion: log(q)= log (q0) +
(q−q0)/q0−(q−q0)2/2q2
0+···Whenq0=1, this becomes log(q)≈(q−1)−(q−1)2/2=−3/2+2q−q2/2, hence,
plog(q)has the second order approximation −3p/2+2pq(or−3p/2+2p2whenp=q). Applying this to Eq 5,
pisPX,Y(x,y)in the ﬁrst term and PY(y)in the second. Since both PX,Y(x,y)andPY(y)are probabilities,
and hence must sum to 1 across the label space for any given sample, the linear term −3p/2does not aﬀect
gradient descent optimization. Normalization is a hard constraint enforced during training that supersedes
this linear term in the objective. Therefore, during optimization, PX,Y(x,y)log(PX,Y(x,y)/PX(x))and
PX,Y(x,y)2/PX(x), and thusIq(X,Y )andI(X,Y ), agree on their second order approximation. Note that
the proposed relaxation is a variant of the commonly-used quadratic approximation based on Taylor’s theorem
(Shafer, 1974; Hsieh et al., 2011).
4.2 Relating MI Iq(X,Y )to model error E(X,Y )
Next we connect Iq(X,Y )with the model’s predictions using Lagrange multipliers. Let R(x,y) :X×Y→ [0,1]
denote the model’s probability output for sample xand outcome y. Below, we model the discrete label case,
e.g. for classiﬁcation; the case where labels are continuous can be done by ﬁrst discretizing the continuous
label space (Fleuret, 2004), and then taking the limit as the discretization becomes inﬁnitesimal. §A.4
contains further details. First, we deﬁne the quadratic error term E(X,Y )in terms of R(x,y), and expand:
E(X,Y ):=/summationdisplay
x∈X,y∈YPX,Y(x,y)/parenleftbigg
(1−R(x,y))2+/summationdisplay
y/prime∈Y\yR(x,y/prime)2/parenrightbigg
=/summationdisplay
x∈X,y∈YPX,Y(x,y)/parenleftbigg
1−2R(x,y) +R(x,y)2+/summationdisplay
y/prime∈Y\yR(x,y/prime)2/parenrightbigg
=/summationdisplay
x∈X,y∈YPX,Y(x,y)−2/summationdisplay
x∈X,y∈YPX,Y(x,y)R(x,y)
+/summationdisplay
x∈X,y∈Y,y/prime∈YPX,Y(x,y)R(x,y/prime)2/triangleleftCombine last two terms and expand.
= 1−2/summationdisplay
x∈X,y∈YPX,Y(x,y)R(x,y) +/summationdisplay
x∈X,y/prime∈YPX(x)R(x,y/prime)2/triangleleftMarginalize. (7)
Theorem 4.1. LetXandYdenote the random variables representing the features and labels, respectively,
andYthe value space for Y, then minimizing the optimum error E(X,Y )in the model space {f:X→Y}is
equivalent to maximizing the quadratic relaxation of mutual information Iq(X,Y ). More speciﬁcally,
min
f:X→YE(X,Y ) = 1−/summationdisplay
y∈YPY(y)2−Iq(X,Y ).
6Under review as submission to TMLR
The proof utilizes Lagrange multipliers to solve for the optimal model predictions in terms of PX,Y(x,y)and
PX(x), this can then be used to express the optimum objective E(X,Y )as a function of Iq(X,Y ). The full
proof can be found in §A.4.
4.3 Application to feature selection
Now, we apply this ﬁnding concretely to feature selection, by selecting a given number of features that minimize
E(X,Y ). Given a dataset, let Idenote the index set of the dataset samples, Jthe index set of the features,
andLthe set of possible labels. Let S⊂Jdenote the index set of features selected, XS
ithe random variable
representing a selected subset of features for the ithsample, and Yithe random variable representing the label
for theithsample. Then, the joint probability can be written as PX,Y(x,y) =|{i∈I|XS
i=x,Yi=y}|/|I|.
Plugging this into the deﬁnition of E(X,Y )we obtain:
E(X,Y ):=/summationdisplay
x∈X,y∈YPX,Y(x,y)/parenleftBig
(1−R(x,y))2+/summationdisplay
y/negationslash=YiR(x,y/prime)2/parenrightBig
=/summationdisplay
x∈X,y∈Y|{i∈I|XS
i=x,Yi=y}|
|I|/parenleftBig
(1−R(XS
i,Yi))2+/summationdisplay
y/negationslash=YiR(XS
i,y)2/parenrightBig
=/summationdisplay
i∈I/parenleftBig
(1−R(XS
i,Yi))2+/summationdisplay
y/negationslash=YiR(XS
i,y)2/|I|/parenrightBig
(8)
During training, Eq. 8 is minimized under the following consistency constraint: for two samples i1andi2
that have the same values in the selected features, i.e. XS
i1=XS
i2, their model predictions must be the
same, i.e.R(XS
i1,Yi1) =R(XS
i2,Yi2). To encourage the model to satisfy this constraint, we turn it into a soft
consistency regularization term rcs, converting constrained optimization to unconstrained optimization with
regularization:
rcs:=/summationdisplay
{i1,i2}∈I2,i1<i2P(XS
i1=XS
i2)/parenleftbig
R(XS
i1,Yi1)−R(XS
i2,Yi2)/parenrightbig2,
whereP(XS
i1=XS
i2)is the probability that the samples Xi1andXi2take the same values in the selected
feature setS.
Let the learned mask consists of probabilities m={pj}j∈J, i.e.pjis the probability that feature jis selected,
thenP(XS
i1=XS
i2) =/producttext
X(j)
i1/negationslash=X(j)
i2(1−pj), i.e.P(XS
i1=XS
i2)is the product over probabilities that feature
jis not selected, if Xi1andXi2diﬀer at feature j. (The diﬀerence in a feature that is not selected does
not contribute to P(XS
i1=XS
i2)). In this probabilistic form, the consistency regularizer also encourages the
selection of features with diverse ranges, since it encourages high pjfor the features with many X(j)
i1/negationslash=X(j)
i2
pairs. Therefore, the regularized objective to maximize the MI I(X,Y )between the selected features and the
labels becomes:
E(X,Y ) =/summationdisplay
i∈I/parenleftBig
(1−R(XS
i,Yi))2+/summationdisplay
y/negationslash=YiR(XS
i,y)2/parenrightBig
/|I|+rcs, (9)
where
rcs=/summationdisplay
{i1,i2}∈I2,i1<i2/parenleftbigg/productdisplay
X(j)
i1/negationslash=X(j)
i2(1−pj)/parenleftbig
R(XS
i1,Yi1)−R(XS
i2,Yi2)/parenrightbig2/parenrightbigg
. (10)
In practice, rcscan be enforced batch-wise, and can be eﬃciently vectorized for the parallel computation of
allX(j)
i1/negationslash=X(j)
i2pairs per batch using tensor operations. Note that since R(XS
i,Yi)are just model predictions,
andpjare learned feature mask probabilities, each component in E(X,Y)is easily accessible. When the
labels are in the continuous space, the minimization objective with the consistency regularizer is derived the
exact the same way to yield:
E(X,Y ) =/summationdisplay
i∈I/parenleftbig
Yi−R(XS
i)/parenrightbig2/|I|+rcs.
Our analysis is done with random variables XandYto apply tools from probability theory. The data
samples Xand labels Ycan be thought of as samples drawn from the distributions to which XandYbelong,
where in the limit with inﬁnitely many samples XandYperfectly reﬂect these distributions.
7Under review as submission to TMLR
4.4 SLM Computational complexity
As above, let hbe the hidden dimension, ndenote the number of samples, bthe batch size, and Nthe
total number of train steps; let F0be the total number of features, and FNthe target number of features.
We ﬁrst discuss the complexity of individual components. The sparsemax operation is dominated by
sorting, and hence has complexity O(F0logF0)per sample, with an overall complexity of O(nF0logF0).
The consistency regularizer rcsin the MI-maximizing objective E(X,Y)has complexity O(nbFN), as the
calculation/producttext
X(j)
i1/negationslash=X(j)
i2(1−pj)/parenleftbig
R(XS
i1,Yi1)−R(XS
i2,Yi2)/parenrightbig2in Eq 10 occurs over the selected feature index
setj∈S, and is done between each sample and others in its batch. The non-regularizer component in
E(X,Y)has complexity nc, wherecis the constant for the number of discrete or binned labels. Assuming
an MLP classiﬁer with hhidden units, which has complexity O(nh2), the overall algorithm has complexity
O(nF0logF0+nbFN+nc+nh2), making SLM amenable to scaling to a large number of features. In addition,
SLMamortizes the cost of feature selection across batches throughout training, making it more scalable with
respect to the number of samples. This is in contrast to PFA (Lu et al., 2007a) or many other MI-based
methods such as CMIM (Fleuret, 2004) or JMIM (Bennasar et al., 2015), which place the memory and
compute burden of selection for the entire dataset in the same step.
5 Experiments
5.1 Datasets and Settings
We present the eﬃcacy of SLM in feature selection on wide range of datasets from numerous domains. For
all experiments, we ensure fair comparison by employing similar hyperparameter search space and budget –
to search for hyperparameters such as batch size and learning rate for each baseline method and dataset,
we conduct an extensive random search within the search grid, by randomly generating a value within a
conceivable range. We run a total of 300 trials for each method-dataset combination to ensure suﬃcient
coverage, and tune all hyperparameters based on the validation accuracy. This process is chosen as it closely
resembles model benchmarking and selection in real-world applications. The Appendix includes a myriad
of additional experiments: selected feature interpretability (§A.6), compute timings (§A.7) and synthetic
data experiments (§A.9) to demonstrate SLM’s scalability, using the Hilbert-Schmidt Independence Criterion
(HSIC) in lieu of the MI regularizer to demonstrate the eﬀectiveness of the learned sparse mask (§A.8), as
well as comparisons with further end-to-end baselines (§A.10).
We benchmark on a variety of real-world datasets across many domains, including computer vision, biological
data, ﬁnancial data, etc. Concretely, we benchmark on Mice, MNIST, Fashion-MNIST, Isolet, Coil-20,
Activity, Ames Housing, and IEEE-CIS Fraud datasets. We use a 70-10-20 train/validation/test split; and
when available, we use the exact same train/validation/test samples as (Lemhadri et al., 2019) for fair
comparison. We give further detailed descriptions in §A.1. Cross entropy is used as the optimization objective
for classiﬁcation tasks, and MAE is used as the optimization objective for regression.
We benchmark SLM against a variety of competitive methods. The mutual information ( MI) based feature
selection baseline uses entropy estimation from k-nearest neighbors distances as described in (Kraskov et al.,
2004; Ross, 2014) to estimate MI. Tree-based methods yield Gini importance scores, which can be used
for feature selection. For this we benchmark two commonly used methods: random forest ( RF) (Breiman,
2001), an ensemble of independent trees, and XGBoost (Chen & Guestrin, 2016), a scalable end-to-end
tree boosting system. We furthermore benchmark against methods as discussed in §2: LassoNet (Lemhadri
et al., 2019), which uses residual connections to allow the network to learn whether to use any given feature in
a particular layer; feature importance ranking based on the Fischer score (Gu et al., 2012); principal feature
analysis ( PFA) (Lu et al., 2007a), a PCA-based method; and HSIC-Lasso (Yamada et al., 2014), which
uses kernel learning to ﬁnd non-linear feature interactions. Lastly, we benchmark against linearregression,
where feature importance is determined by the learned feature coeﬃcients. When available, we use results
from (Lemhadri et al., 2019). For consistency and fairness, each baseline method uses the same input as SLM
to select features, which are then passed to an MLP to compute the task metric.
8Under review as submission to TMLR
5.2 Task performance with feature selection
Method Mice↑MNIST↑Fashion↑Isolet↑Coil-20↑Activity↑Ames↓Fraud↑
All-features 0.9900.928 0.833 0.953 0.996 0.956 0.283 0.782
SLM (ours) 0.981 0.953 0.835 0.919 0.996 0.947 0.274 0.911
Fisher 0.9440.813 0.671 0.793 0.986 0.769 0.286 0.743
HSIC-Lasso 0.9580.870 0.785 0.877 0.972 0.829 0.273 0.846
PFA 0.9390.873 0.793 0.863 0.975 0.779 0.356 0.852
XGBoost 0.9680.913 0.832 0.879 0.986 0.926 0.403 0.872
MI 0.9490.882 0.645 0.751 0.976 0.883 0.335 0.711
Linear 0.9820.452 0.787 0.760 0.983 0.914 0.318 0.871
Anova 0.9950.113 0.719 0.811 0.986 0.901 0.358 0.744
RF 0.9670.928 0.829 0.892 0.993 0.893 0.303 0.773
Lassonet 0.9580.873 0.800 0.885 0.991 0.849 0.342 0.842
Table 1: Test performance on real-world benchmarks with 50 selected features. SLM outperforms competitive
baselines. The metrics reported are AUC for Fraud, since there is a high class imbalance; hence AUC is
reported; the median MAE on standard-normalized labels is reported for Ames; and accuracy is reported on
all other datasets. The arrow next to each dataset indicates whether a higher or lower value is more optimal.
These test results are selected based on the best validation set performance during 300 hyperparameter grid
search trials.
There is a rich body of work that assign importance to individual features (Lundberg & Lee, 2017; Shrikumar
et al., 2017; Zintgraf et al., 2017; Chang et al., 2019). We consider feature importance to be measured by
contribution towards the task metric, as accurate predictor performance is typically the end goal, and the
importance of each individual feature is not always well-deﬁned due to feature interactions. Therefore, we
focus on benchmarking task predictive accuracy given the selected features as the metric.
First, we study selecting a ﬁxed number of features across a wide range of high dimensional datasets (most
with>400 features) and feature selection methods. We consistently choose 50 selected features, as this
represents a small fraction of the total features for most datasets, as often done in practice. This number is
kept consistent without tuning for any given method, to avoid favoring any given one. Table 1 shows that the
SLM consistently yields competitive performance, outperforming all methods in all cases except on Mice
and Ames, for which the performance is saturated due to small numbers of original features, making feature
selection less relevant. Most feature selection methods are not consistent in their performance. On the other
hand, SLM’s strong performance is consistent – across a wide range of datasets, SLM selects the features
accurately. Interestingly, we observe that there are cases where SLM even outperforms the baseline of using
all features, which can likely be attributed to superior generalization when the limited model capacity is
focused on the most salient features. Especially for datasets that are non i.i.d. in nature, feature selection
can be a strong inductive bias integrated in the architecture, that can yield superior generalization.
MethodTest AUC on Fraud Detection ↑
20 features 50 features 100 features
SLM (ours) 89.32 91.06 91.75
Anova 71.81 74.41 82.91
RF 72.16 77.29 78.72
Linear 84.32 87.11 87.46
MI 65.37 71.05 74.91
XGBoost 85.45 87.24 81.67
Table 2: Test AUC on the Fraud Detection dataset (Kaggle, 2022) at diﬀerent feature selection levels: 20, 50
or 100 features selected. The superiority of SLM persists across diﬀerent numbers of selected features.
9Under review as submission to TMLR
Next, we conduct further experiments on the Fraud Detection dataset (Kaggle, 2022), a large-scale dataset
with many heterogeneous features. It is highly non i.i.d. (Grover et al., 2022), thus making feature selection
important given that high capacity models can be prone to overﬁtting and poor generalization. Table 2
shows that SLM outperforms other methods consistently for diﬀerent number of selected features, and its
performance degradation with respect to reducing the number of features is much smaller. Indeed, the AUC
with 20 features out of 432, is >10% better than using all features, indicating improved generalization.
5.3 Ablation studies
Figure 2: (1) and (2) show ablation studies on the eﬀect of MI regularization and tempering the number of
features. Both ablation studies have the same number (50) of selected features on all datasets. (3) shows the
task accuracy as a function of the number of features selected on the activity dataset. The dark line shows
the average of ten random hyperparameter trials, shown with light hue, demonstrating that task performance
can be near-optimal even with a small subset of features.
We study the utility of SLM components, particularly the eﬀects of the MI regularizer and tempering the
number of features, which gradually decreases the number of selected features from the full feature set to the
target number. The eﬀects are measured by randomly selecting ten hyperparameter settings and a seed, and
recording the average performance with or without either MI regularizer or tempering (without tempering
refers to keeping the number of selected features constant throughout training.). Fig 2 shows that both MI
regularization and tempering positively aﬀect task performance. This is consistent with the theory developed
in §3: the MI regularizer encourages maximal mutual information sharing between the labels and the selected
features; and tempering allows the model to initialize learning based on all features, rather than a randomly
selected subset.
6 Discussion
Feature importance interpretability . SLM learns a sparse mask Mthat contains the feature selection
coeﬃcients. We show that this approach yields superior results with end-to-end learning by allowing a smooth
transition between selecting and un-selecting features. In addition, SLM can also be used for interpretation
of global feature importance during inference, yielding the importance ranking of selected features, similar to
other commonly-used methods like SHAP (Lundberg & Lee, 2017). This can be highly desired in high-stakes
applications such as healthcare or ﬁnance, where an importance score can be more useful than simply whether
a feature is selected or not.
Feature interdependence during selection . Compared to prior MI-based feature selectors (Ding &
Peng, 2005; Fleuret, 2004; Bennasar et al., 2015), SLM accounts for feature inter-dependence by learning
inter-dependent probabilities {pj}jfor the selected feature, where {pj}jjointly maximize the MI between
features and labels. Furthermore, SLM learns feature selection and the task objective in an end-to-end way,
which alleviates the selection of repetitive features that may individually be predictive, as gradient descent
10Under review as submission to TMLR
favors increasing the probability for a non-redundant and loss decreasing but less predictive feature over an
individually predictive but redundant feature.
Improved model generalization via feature selection . Feature selection can help improve generalization
beyond the training set, especially for high capacity models like deep neural networks, which can easily overﬁt
patterns from spurious features that do not hold across training and test data splits (Arjovsky et al., 2019).
For instance, Table 1 shows that on some datasets, especially with SLM, prediction on a subset of features
can outperform that on all features. Furthermore, Fig 2 shows that task performance can reach near-optimum
with even a small subset of all features. Therefore, feature selection is a potential alternative for alleviating
compute cost during training and inference, without sacriﬁcing on accuracy.
Relation to other MI estimations in deep learning. MI-based objectives have been used in other deep
learning methods, such as InfoNCE (Oord et al., 2018), InfoGAN (Chen et al., 2016), and Deep Graph
Infomax (Velickovic et al., 2019). To estimate MI, these typically train classiﬁers on samples drawn from
the joint distribution and the product of the marginals, whose exact distributions can be intractable. In
contrast, for feature selection, while the exact distributions of the features and the labels are known, the
computation of their mutual information and its maximization is computationally intractable. To address
this, SLM proposes a quadratic relaxation of MI optimization, applied to feature selection by converting
MI maximization to minimizing a loss function. SLM does not need to sample from the joint or marginal
distributions, a potentially computationally intensive process. Furthermore, prior works (Chen et al., 2016;
Velickovic et al., 2019) often require a contrastive term in estimation of MI with negative sampling, a process
that is not needed in SLM.
Future work . SLM can be integrated into unsupervised or semi-supervised learning, with modiﬁed objectives.
In addition, our results indicate more signiﬁcant outperformance for datasets with non i.i.d. characteristics
as feature selection can eﬀectively reduce the feature dimensionality and reduce the risk of overﬁtting to the
spurious correlations of irrelevant features. Lastly, feature selection for data with structure (e.g. temporal
or graph) is an interesting extension, which might be based on modifying SLM to apply masking to entire
time-series or graph data.
7 Conclusion
We introduce SLM, a sparse learnable mask based feature selection framework that maximizes the MI between
features and labels, while optimizing the training objective end-to-end. Learning the feature masks allows
a smooth, probabilistic selection of features as well as insights on feature importance. SLM demonstrates
competitive performance against SOTA baselines, and opens door to future applications in domains such as
graph or time series representation learning.
References
Abubakar Abid, Muhammed Fatih Balin, and James Zou. Concrete autoencoders for diﬀerentiable feature
selection and reconstruction. Proceedings of the 37th International Conference on Machine Learning , 2019.
Sercan O. Arik and Tomas Pﬁster. Tabnet: Attentive interpretable tabular learning, 2019.
Martin Arjovsky, Léon Bottou, Ishaan Gulrajani, and David Lopez-Paz. Invariant risk minimization, 2019.
URL https://arxiv.org/abs/1907.02893 .
Jasmijn Bastings, Wilker Aziz, and Ivan Titov. Interpretable neural predictions with diﬀerentiable binary
variables. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics ,
pp. 2963–2977, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/
P19-1284. URL https://aclanthology.org/P19-1284 .
Mohamed Bennasar, Yulia Hicks, and Rossitza Setchi. Feature selection using joint mutual information max-
imisation. Expert Systems with Applications , 42(22):8520–8532, 2015. URL https://www.sciencedirect.
com/science/article/pii/S0957417415004674 .
11Under review as submission to TMLR
Mathieu Blondel, André F. T. Martins, and Vlad Niculae. Learning with fenchel-young losses. In Journal of
Machine Learning Research , 2020.
Leo Breiman. Random forests. Machine learning , 45(1):5–32, 2001.
Chun-Hao Chang, Elliot Creager, Anna Goldenberg, and David Duvenaud. Explaining image classiﬁers by
counterfactual generation. In International Conference on Learning Representations , 2019.
Tianqi Chen and Carlos Guestrin. Xgboost: A scalable tree boosting system. In KDD, 2016. doi: 10.1145/
2939672.2939785.
XiChen, YanDuan, ReinHouthooft, JohnSchulman, IlyaSutskever, andPieterAbbeel. Infogan: Interpretable
representation learning by information maximizing generative adversarial nets. Advances in neural
information processing systems , 29, 2016.
De Cock. Ames, iowa: Alternative to the boston housing data as an end of semester regression project.
Journal of Statistics Education , 19, 2011.
Gonçalo M. Correia, Vlad Niculae, and André F. T. Martins. Adaptively sparse transformers. CoRR,
abs/1909.00015, 2019. URL http://arxiv.org/abs/1909.00015 .
Gonçalo M. Correia, Vlad Niculae, Wilker Aziz, and André F. T. Martins. Eﬃcient marginalization of discrete
and structured latent variables via sparsity. In Advances in Neural Information Processing Systems , 2020.
M. Dash and H. Liu. Feature selection for classiﬁcation. Intelligent Data Analysis , 1(1):131–156, 1997. ISSN
1088-467X. doi: https://doi.org/10.1016/S1088-467X(97)00008-5. URL https://www.sciencedirect.
com/science/article/pii/S1088467X97000085 .
Chris Ding and Hanchuan Peng. Minimum redundancy feature selection from microarray gene expression
data.Journal of bioinformatics and computational biology , 3(02):185–205, 2005.
Frederick J. Eggers and Fouad Moumen. American housing survey: Between-survey changes in the number
of bedrooms in a unit, 2013.
Jean Feng and Noah Simon. Sparse-input neural networks for high-dimensional nonparametric regression and
classiﬁcation, 2017.
François Fleuret. Fast binary feature selection with conditional mutual information. Journal of Machine
learning research , 5(9), 2004.
A Gretton, O Bousquet, A Smola, and B Schölkopf. Measuring statistical dependence with hilbert schmidt
norms.Proc. Int. Conf. Algorithmic Learning Theory , 2005.
Prince Grover, Zheng Li, Jianbo Liu, Jakub Zablocki, Hao Zhou, Julia Xu, and Anqi Cheng. Fdb: Fraud
dataset benchmark, 2022. URL https://arxiv.org/abs/2208.14417 .
Quanquan Gu, Zhenhui Li, and Jiawei Han. Generalized ﬁsher score for feature selection, 2012. URL
https://arxiv.org/abs/1202.3725 .
Nuno Miguel Guerreiro and André F. T. Martins. Spectra: Sparse structured text rationalization, 2021.
Isabelle Guyon and André Elisseeﬀ. An introduction to variable and feature selection. J. Mach. Learn. Res. ,
3:1157–1182, mar 2003.
Yanjun Han, Jiantao Jiao, , and Tsachy Weissman. Adaptive estimation of shannon entropy. Information
Theory, 1372–1376, 2015.
Cho-Jui Hsieh, Inderjit Dhillon, Pradeep Ravikumar, and Mátyás Sustik. Sparse inverse covariance matrix
estimation using quadratic approximation. Advances in neural information processing systems , 24, 2011.
Andrew Jaegle, Felix Gimeno, Andrew Brock, Andrew Zisserman, Oriol Vinyals, and Joao Carreira. Perceiver:
General perception with iterative attention, 2021.
12Under review as submission to TMLR
Jiantao Jiao, Kartik Venkat, Yanjun Han, and Tsachy Weissman. Adaptive estimation of shannon entropy.
Transactions on Information Theory , 61, 2015.
Kaggle. IEEE-CIS fraud detection, 2022. URL https://www.kaggle.com/c/ieee-fraud-detection .
A. Kraskov, H. Stogbauer, and P. Grassberger. Estimating mutual information. Phys. Rev. E , 69, 2004.
Tao Lei, Regina Barzilay, and Tommi Jaakkola. Rationalizing neural predictions, 2016.
Ismael Lemhadri, Feng Ruan, Louis Abraham, and Robert Tibshirani. Lassonet: A neural network with
feature sparsity. arXiv:1907.12207 , 2019.
Jundong Li, Kewei Cheng, Suhang Wang, Fred Morstatter, Robert P. Trevino, Jiliang Tang, and Huan
Liu. Feature selection: A data perspective. ACM Comput. Surv. , 50(6), dec 2017. ISSN 0360-0300. doi:
10.1145/3136625. URL https://doi.org/10.1145/3136625 .
Yijuan Lu, Ira Cohen, Xiang Sean Zhou, and Qi Tian. Feature selection using principal feature analysis. In
Proceedings of the 15th ACM international conference on Multimedia , pp. 301–304, 2007a.
Yijuan Lu, Ira Cohen, Xiang Sean Zhou, and Qi Tian. Feature selection using principal feature analysis. In
Proceedings of the 15th ACM International Conference on Multimedia , MM ’07, pp. 301–304, New York, NY,
USA, 2007b. Association for Computing Machinery. ISBN 9781595937025. doi: 10.1145/1291233.1291297.
URL https://doi.org/10.1145/1291233.1291297 .
Scott M Lundberg and Su-In Lee. A uniﬁed approach to interpreting model predictions. In I. Guyon,
U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Advances
in Neural Information Processing Systems 30 , pp. 4765–4774. Curran Associates, Inc., 2017. URL http:
//papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions.pdf .
Wan-Duo Kurt Ma, J.P. Lewis, and W. Bastiaan Kleijn. The hsic bottleneck: Deep learning without
back-propagation. AAAI, 2020.
Andre Martins and Ramon Astudillo. From softmax to sparsemax: A sparse model of attention and multi-label
classiﬁcation. In International conference on machine learning , pp. 1614–1623. PMLR, 2016.
Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive coding,
2018. URL https://arxiv.org/abs/1807.03748 .
Jing Pan, Vincent Pham, Mohan Dorairaj, Huigang Chen, and Jeong-Yoon Lee. Adversarial validation
approach to concept drift problem in automated machine learning systems. CoRR, abs/2004.03045, 2020.
URL https://arxiv.org/abs/2004.03045 .
Liam Paninski. Estimation of entropy and mutual information. Neural Computation , 15, 2003.
Bhargavi Paranjape, Mandar Joshi, John Thickstun, Hannaneh Hajishirzi, and Luke Zettlemoyer. An
information bottleneck approach for controlling conciseness in rationale extraction. In Proceedings of the
2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) , pp. 1938–1952, Online,
November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.153. URL
https://aclanthology.org/2020.emnlp-main.153 .
Ben Peters, Vlad Niculae, and André F. T. Martins. Sparse sequence-to-sequence models, 2019.
B. C. Ross. Mutual information between discrete and continuous data sets. PLoS ONE , 9, 2014.
Shiori Sagawa, Aditi Raghunathan, Pang Wei Koh, and Percy Liang. An investigation of why overparameteri-
zation exacerbates spurious correlations. In Hal Daumé III and Aarti Singh (eds.), Proceedings of the 37th
International Conference on Machine Learning , volume 119 of Proceedings of Machine Learning Research ,
pp. 8346–8356. PMLR, 13–18 Jul 2020. URL https://proceedings.mlr.press/v119/sagawa20a.html .
Robert E Shafer. On quadratic approximation. SIAM Journal on Numerical Analysis , 11(2):447–460, 1974.
13Under review as submission to TMLR
Avanti Shrikumar, Peyton Greenside, and Anshul Kundaje. Learning important features through propagating
activation diﬀerences. International Conference on Machine Learning , 2017.
Dinesh Singh, Héctor Climente-González, Mathis Petrovich, Eiryo Kawakami, and Makoto Yamada. Fsnet:
Feature selection network on high-dimensional biological data. 2020.
Robert Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society:
Series B (Methodological) , 58(1):267–288, 1996.
Gregory Valiant and Paul Valiant. Estimating the unseen: an n/log(n)-sample estimator for entropy and
support size, shown optimal via new CLTs. Proceedings of the forty-third annual ACM symposium on
Theory of computing , 685–694, 2011.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz Kaiser,
and Illia Polosukhin. Attention is all you need. In I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach,
R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Advances in Neural Information Processing Systems ,
volume 30. Curran Associates, Inc., 2017. URL https://proceedings.neurips.cc/paper/2017/file/
3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf .
Petar Velickovic, William Fedus, William L Hamilton, Pietro Liò, Yoshua Bengio, and R Devon Hjelm. Deep
graph infomax. ICLR, 2(3):4, 2019.
Yihong Wu and Pengkun Yang. Minimax rates of entropy estimation on large alphabets via best polynomial
approximation. IEEE Transactions on Information Theory , 62, 2016.
Makoto Yamada, Wittawat Jitkrittum, Leonid Sigal, Eric P. Xing, and Masashi Sugiyama. High-dimensional
feature selection by feature-wise kernelized lasso. Neural Computation , 26(1):185–207, jan 2014. doi:
10.1162/neco_a_00537.
Yutaro Yamada, Oﬁr Lindenbaum, Sahand Negahban, and Yuval Kluger. Feature selection using stochastic
gates. In Hal Daumé III and Aarti Singh (eds.), Proceedings of the 37th International Conference on
Machine Learning , volume 119 of Proceedings of Machine Learning Research , pp. 10648–10659. PMLR,
13–18 Jul 2020. URL https://proceedings.mlr.press/v119/yamada20a.html .
Sepehr Abbasi Zadeh, Mehrdad Ghadiri, Vahab Mirrokni, and Morteza Zadimoghaddam. Scalable feature
selection via distributed diversity maximization. In Thirty-First AAAI Conference on Artiﬁcial Intelligence ,
2017.
Guangxiang Zhao, Junyang Lin, Zhiyuan Zhang, Xuancheng Ren, Qi Su, and Xu Sun. Explicit sparse
transformer: Concentrated attention through explicit selection, 2019.
Luisa M. Zintgraf, Taco S. Cohen, Tameem Adel, and Max Welling. Visualizing deep neural network decisions:
Prediction diﬀerence analysis. International Conference on Learning Representations , 2017.
14Under review as submission to TMLR
A Appendix
A.1 Dataset Details
This section provides additional details on the experimental data. We ﬁrst consider the real-world benchmark
datasets in (Lemhadri et al., 2019). Miceconsists of protein expression levels measured in the cortex of
normal and trisomic mice who had been exposed to diﬀerent experimental conditions. Each feature is the
expression level of one protein. MNIST andFashion-MNIST consist of 28-by-28 grayscale images of
hand-written digits and clothing items, respectively. The images are converted to tabular data by treating
each pixel as a separate feature. Isoletconsists of preprocessed speech data of people speaking the names
of the letters in the English alphabet with each feature being one of the preprocessed quantities, including
spectral coeﬃcients and sonorant features. Coil-20 consists of centered gray-scale images of 20 objects taken
at certain pose intervals, hence the features are image pixels. Activity consists of sensor data collected from
a smartphone mounted on subjects while they performed several activities such as walking or standing. For
these datasets, we use the exact same data splits and preprocessing approaches with (Lemhadri et al., 2019)
for fair comparison, as well as the same model hyperparameter search space.1In addition, we consider the
Ameshousing dataset (Cock, 2011), with the goal of predicting residential housing prices based on each
home’s features; as well as the IEEE-CIS FraudDetection dataset (Kaggle, 2022), with the goal of identifying
fraudulent transactions from numerous transaction and identity dependent features. Table 3 summarizes the
characteristics of the datasets used in the experiments.
Dataset Number of samples Number of features Number of classes
Mice 1080 77 8
MNIST 10000 784 10
Fashion 10000 784 10
Isolet 7797 617 26
Coil-20 1440 400 20
Activity 5744 561 6
Ames 1460 81 N/A
IEEE Fraud 590540 681 2
Table 3: Attributes of datasets used in experiments.
A.2 Experimental details
As described, we use hyperparameter tuning based on the validation accuracy for all cases. We use the
Adam optimizer for training, with exponential decay. For benchmarks from (Lemhadri et al., 2019), for a fair
comparison, our hyperparameter search space is same as the original paper. For Fraud, which is larger and
more complex, we extend the search space as in Table 4.
Hyperparameter Search space
Batch size [512, 1024, 2048, 4096]
Learning rate [0.001, 0.003, 0.01]
Decay steps [1000, 10000]
Decay rate [0.7, 0.9, 0.95, 0.99]
Number of epochs [30, 100, 200]
Number of hidden units [50, 100, 200]
Number of layers [1, 2, 3, 4]
Table 4: Hyperparameter tuning search space for experiments on the Fraud dataset.
1We use a single layer multi-layer perceptron (MLP) as the predictor, where the number of units is chosen from
[M/3,2M/3,M,4M/3].
15Under review as submission to TMLR
For baselines such as LassoNet, we tune additional method-speciﬁc hyperparameters. For instance, for
LassoNet, in addition to the hyperparameters, we also tune the /lscript2penalization on the skip connection, the
hierarchy parameter, and the dropout rate. For XGBoost, we also tune the number of estimators and the
maximum tree depth.
A.3 Proof of Lemma 3.2
Lemma 3.2. Given a nonuniform vector v∈RK, to obtain Fnonzero elements in sparsemax( v),vshould
be multiplied with the scalar
m=

/parenleftBig/summationtextF+1
i=1v(i)−(F+ 1)∗v(F+1)/parenrightBig−1
if|sparsemax (v)>0|>F
/parenleftBig/summationtextF
i=1v(i)−F∗vF/parenrightBig−1
if|sparsemax (v)>0|<F,(11)
wherev(1)≥v(2)...≥v(K)denote sorted elements of vin descending order.
Proof.We ﬁrst show the case when |sparsemax (v)>0|>F, i.e. the sparsity needs to be increased (the case
where sparsity needs to be decreased works analogously). By (Martins & Astudillo, 2016), the projection of v
onto ∆K−1in Eq 1 takes the form sparsemax (v) = [v−τ(v)]+, where [x]+=max{0,x}, andτtakes the
formτ=/parenleftBig/summationtext
i≤k(v)v(i)/parenrightBig
−1
k(v)withk(v)deﬁned as the index
k(v):= max/braceleftBig
k∈{1,...,K}|1 +kv(k)>/summationdisplay
i≤kv(i)/bracerightBig
. (12)
Hence, increasing the sparsity such that sparsemax outputs only Fnonzero elements, i.e. decreasing the
indexk(v)toF, requires ﬁnding the smallest msuch that 1 + (F+ 1)mv(F+1)>/summationtext
i≤(F+1)mv(i)does not
hold, i.e.F+ 1must be the ﬁrst kto fail the condition 1 +kv(k)>/summationtext
i≤kv(i). Rewriting this condition in
terms ofFwe obtain:
1 + (F+ 1)mv(F+1)>/summationdisplay
i≤(F+1)mv(i)
implies 1>m/parenleftbigg/summationdisplay
i≤(F+1)v(i)−(F+ 1)v(F+1)/parenrightbigg
(13)
The smallest msuch that condition Eq. 13 does not hold is m=/parenleftBig/parenleftBig/summationtextF+1
i=1v(i)/parenrightBig
−(F+ 1)∗v(F+1)/parenrightBig−1
, which
given Eq 12 implies mvhasFnonzero elements. Analogously, to derive the multiplier for vto decrease
sparsemax (v)sparsity, we need to increase the index k(v)toF. This requires ﬁnding the largest msuch
that 1 +F(mvF)>/summationtext
i≤Fmv(i)holds, which implies: m=/parenleftBig/summationtextF
i=1v(i)−F∗v(F)/parenrightBig−1
.
A.4 Proof of Theorem 4.1
Theorem 4.1. LetXandYdenote the random variables representing the features and labels, respectively,
andYthe value space for Y, then minimizing the optimum error E(X,Y )in the model space {f:X→Y}
is equivalent to maximizing the quadratic relaxation of mutual information Iq(X,Y ). More speciﬁcally,
min
f:X→YE(X,Y ) = 1−/summationdisplay
y∈YPY(y)2−Iq(X,Y )
Proof.During training, the model seeks to produce the optimal predictions R(x,y)thatminimizeE(X,Y ),
while satisfying the constraint/summationtext
y∈YR(x,y) = 1. Hence we can apply Lagrange multipliers to solve for the
optimalR(x,y). Taking the derivatives of E(X,Y)and the constraint g(X,Y) =/summationtext
x∈X,y∈YR(x,y)−|X|
with respect to R(x,y):
E/prime(X,Y ) =/summationdisplay
x∈X,y∈Y−2PX,Y(x,y) + 2PX(x)R(x,y) (14)
g/prime(X,Y ) =/summationdisplay
x∈X,y∈Y1
16Under review as submission to TMLR
Marginalizing E/prime(X,Y )overYyields:
E/prime(X,Y ) =/summationdisplay
x∈X−2PX(x) + 2PX(x) = 0/triangleleftSince/summationdisplay
y∈YR(x,y) = 1
By Lagrange multiplier theory, for an optimum set of model predictions R∗(X,Y ), there exists some λsuch
thatE/prime(X,Y )|R(X,Y)=R∗(X,Y)=λg/prime(X,Y ). SinceE/prime(X,Y )|R(X,Y)=R∗(X,Y)= 0,λ= 0.
Therefore, by Eq 14, R∗(x,y) =PX,Y(x,y)/PX(x). Plugging this into Eq 7, we obtain an expression relating
the mutual information Iq(X,Y )and the optimum error E(X,Y ):
min
f:X→YE(X,Y ) = 1−2/summationdisplay
x∈X,y∈YPX,Y(x,y)R∗(x,y) +/summationdisplay
x∈X,y∈YPX(x)R∗(x,y)2
= 1−2/summationdisplay
x∈X,y∈YPX,Y(x,y)PX,Y(x,y)
PX(x)+/summationdisplay
x∈X,y∈YPX,Y(x,y)2
PX(x)
= 1−/summationdisplay
x∈X,y∈YPX,Y(x,y)2
PX(x)
= 1−/summationdisplay
y∈YPY(y)2−Iq(X,Y )/triangleleftBy Eq 6
SincePY(y)isﬁxedforagivendataset, minimizing E(X,Y )acrossthemodelspaceisequivalenttomaximizing
Iq(X,Y ).
Continuous label space . For the case with continuous labels (as occur for regression problems), the
quadratic relaxation analogue of Eq 6 becomes:
˜Iq(X,Y ):=/parenleftbigg/summationdisplay
x∈X/integraldisplay
y∈YPX,Y(x,y)2/PX(x)/parenrightbigg
−/integraldisplay
y∈YPY(y)2. (15)
LetYkbe a discretization of the continuous labeling space Ywithkintervals, i.e. we are turning the
continuous labeling function X→Yinto a piecewise constant one with ksteps. Then, discretizing followed
by taking the limit as k→∞(and all bin sizes tend to 0) yields, analogous to Eq 7:
˜E(X,Y ):= lim
k→∞/summationdisplay
x∈X,y∈YkPX,Y(x,y)/parenleftbigg
(1−R(x,y))2+/summationdisplay
y/prime∈Yk\yR(x,y/prime)2/parenrightbigg
= 1−2/summationdisplay
x∈X/integraldisplay
y∈YPX,Y(x,y)R(x,y) +/summationdisplay
x∈X/integraldisplay
y/prime∈YPX(x)R(x,y/prime)2
Therefore following the same logic as in the proof above:
min
f:X→Y˜E(X,Y ) = 1−/summationdisplay
x∈X/integraldisplay
y∈YPX,Y(x,y)2
PX(x)
Plugging Eq 15 into this equation thus gives:
min
f:X→Y˜E(X,Y ) = 1−/integraldisplay
y∈YPY(y)2−˜Iq(X,Y ),
as desired. Thus, minimizing the optimum objective ˜E(X,Y)across the model space {f:X→Y}is
equivalent to maximizing Iq(X,Y ).
A.5 Properties of sparsemax for SLM
This section further details the key aspects of utilizing sparsemax for SLM, in terms of how it compares with
softmax with thresholding for feature selection, as well as how SLM avoids the sparsemax support collapse
problem.
17Under review as submission to TMLR
A.5.1 Sparsemax vs softmax with thresholding
Softmax is the commonly used nonlinear normalization function. An alternative method for learning the
sparse maskMspwould be to apply softmax normalization, followed by a top-k operation, and an additional
normalization to render it a probability mask. This method is not only unwieldy with additional steps, but
because the softmax-top-k normalization normalizes with respect to the absolutevalue of v, whereas sparsemax
normalizes with respect to its relativevalues (by subtracting a v-dependent threshold), sparsemax (v)is more
equi-distributed over the interval [0,1]than softmax-top-k normalization (i.e. sparsemax (v)has lower entropy
than softmax-top-k normalization), making it more discriminatory for feature selection. Furthermore, one
gradient computation advantage of sparsemax (v)is that it allows a faster computation of the Jacobian-vector
product – which typically suﬃces for backpropagation – in O(S(v))time,S(v)being the support of v
(Martins & Astudillo, 2016), compared with linear time (with respect to the size of v) for softmax.
A.5.2 SLM avoids sparsemax support collapse
In practice, since the gradient of sparsemax is zero for elements outside its support (Martins & Astudillo,
2016), an element that initially falls outside its support would stay so throughout training. This would lead
to sparsemax support collapse, i.e. the size of the support dwindles during training.
SLM avoids the sparsemax support collapse problem, due to its sparsemax argument scaling (Lemma 3.2).
In vanilla sparsemax, as the support of sparsemax is determined by the distances between the top-ranking
elements, having non-zero gradients only for the elements in the support of sparsemax causes only those
elements to drift. As the inputs to sparsemax can vary without bound, this drift becomes larger over time,
i.e. the distance between the top elements, which are in sparsemax’s support, becomes larger, making the
sparsemax support smaller, and blocking new elements from entering the support. However, when the input
to sparsemax is scaled as in Lemma 3.2, this drift is controlled, and can shorten the distances (in addition
to lengthening them) between the top elements, hence the sparsemax support can acquire new elements,
avoiding the collapse.
A.5.3 Experimental analysis of the number of features in support
Experimentally, when training SLM feature selection with a single-layer MLP architecture, on a dataset of
1000 samples with 100 features each, using sparsemax withscaling to select 30 features consistently yields 30
features in the sparsemax support. In contrast, without scaling, the sparsemax support consistently dwindles
to well below 10 features within 15 epochs.
A.6 Feature Interpretability Results
While SLM optimizes feature selection for the task metric, the fact that the selected features are global readily
opens the door for feature importance interpretability applications, as the chosen features can give insights
about the task. To this end, we focus on the Ames housing dataset (Cock, 2011), as its features are easily
understandable. As mentioned in §A.1, the features in the Ames dataset consist of characteristics of houses,
and the prediction target is the house price. We use the model parameters found in the best validation trial
reported in Table 1, and select the top ten out of the 81 features. To obtain importance scores of the selected
features, westudytheselectionprobabilitieslearnedinthefeaturemask. Usingthis, thetenhighest-probability
features in terms of determining a house’s prices are, with learned feature probabilities: ‘OverallQual’ (0.211),
‘FullBath’ (0.182), ‘GarageCars’ (0.124), ‘BsmtFullBath’ (0.0795), ‘MSSubClass’ (0.0758), ‘GarageFinish’
(0.0739), ‘HalfBath’ (0.0718), ‘PoolArea’ (0.0562), ‘Fireplaces’ (0.0473), ‘HouseStyle’ (0.0403).
Some aspects of this selection conform to common sense – the overall quality of the property, the number
of bathrooms, and the size of the garage or pool are good predictors of housing value. Other aspects are
more surprising, for instance the feature ’BedroomAbvGr’ – the number of bedrooms above ground – is not
selected, even though one would expect the number of bedrooms to be an important selling factor. However,
on further thought, as the number of bedrooms is positively correlated with the number of bathrooms (Eggers
& Moumen, 2013), SLM is avoiding feature redundancy by only selecting one of the correlated features. The
18Under review as submission to TMLR
same reasoning applies for the features ’OverallQual’, the overall quality, which is selected, and ’OverallCond’,
the overall condition, which is not selected.
A.7 Computational Complexity Experiments
As stated in § 4.4, let F0be the total number of features, and nthe number of samples, SLM has O(nF0logF0)
dependence on F0. To test that this low complexity in theory translates to actual fast feature selection
in practice, we present the wall clock timing of SLM. We compare speciﬁcally against LassoNet, a strong
baseline that also selects features end-to-end. Table 5 shows the timing results for one epoch on the Mice
dataset, demonstrating that SLM’s low complexity in theory also translates to fast execution in practice.
Feature Selection Method Timing (s)
SLM 1.21±0.016
LassoNet 19.62±0.796
Table 5: Timing results for one epoch on the mice dataset between SLM and LassoNet, a strong baseline that
also selects features end-to-end. This comparison is down under the exact same settings for both methods:
hidden dimension of 64, batch size of 256, one MLP predictor layer, selecting 50 features, run on a single V100
GPU. The result statistics are collected over ﬁve diﬀerent runs. Only the training component is measured,
not including data splitting and processing.
Furthermore, we discuss the computation of the MI objective in Eq 9. In particular, the consistency term
rcsin Eq 10, which ensures that if two samples have the same values in their selected features, their model
predictions are the same as well. In theory, if we imagine giving rcsa weight coeﬃcient αin Eq. 9, with the
interpretation that in the limit where α→∞, this consistency is strictly enforced; and in the limit where
α→0, not at all. In practice, given that they have the same orders on terms such as model predictions
R(X,Y )by design,rcsand the remaining term in Eq 9 have the same order of magnitude. Furthermore, rcs
indeed is the most compute-intensive part in Eq 9, as rcsrequires pairwise comparisons within the batch (for
each pairXi1,Xi2it is computed over the feature indices jwhereX(j)
i1/negationslash=X(j)
i2). Experimentally, on the Ames
dataset with a batch size of 128, the rcscomputation takes up 1.51 ±0.012 ms, out of 1.94 ±0.017 ms for
the entire MI regularizer computation in Eq 9. This reveals an interesting accuracy-compute trade-oﬀ, where
users may want to skip the rcscomputation for an even faster, approximate MI regularizer computation.
A.8 HSIC objective to demonstrate the eﬀectiveness of the learned sparse mask
WhilethemutualinformationregularizerisanintegralpartoftheSLM,inthissectionweshowthe eﬀectiveness
and generalizability of the learned feature selection mask approach, by replacing the MI regularizer with
another measure of dependency between random variables: the Hilbert-Schmidt Independence Criterion
(HSIC) (Gretton et al., 2005). Analogous to how we apply the mutual information regularizer, we consider
the HSIC between the features random variable and the labels random variable distributions.
Concretely, for two random variables XandY, the HSIC is the Hilbert-Schmidt norm of the covariance
operator between these random variable distributions in the Reproducing Kernel Hilbert Space, deﬁned as:
HSIC (PXY,F,G):=||Cov(X,Y )||2
HS
=EXX/primeYY/prime[kX(X,X/prime)kY/prime(Y,Y/prime)] +EXX/prime[kX(X,X/prime)]EYY/prime[kY(Y,Y/prime)]
−2EXY[EX/prime[kX(X,X/prime)]EY/prime[kY(Y,Y/prime)]], (16)
wherekXandkYdenote kernel functions; FandGare the Hilbert spaces of functions on XandY;
andCov(X,Y)denotes the cross-covariance operator F →G(Gretton et al., 2005). We experimented
with diﬀerent kernel functions for Eq. 16, and chose the Gaussian kernel based on ﬁnal task performance:
k(x,x/prime):=exp(−||x−x/prime||2/(2σ2)), where x,x/primeare samples drawn from the distribution PX, withσbeing
the standard deviation.
Unlike mutual information, HSIC is not a probabilistic measure, and does not have an interpretation in
terms of information theoretic quantities (bits or nats) (Ma et al., 2020). On the other hand, HSIC does
19Under review as submission to TMLR
not require any probability density estimation, which can be a computational bottleneck in approximating
mutual information. In our experiments, we compute the HSIC objective batch-wise between the features
and the labels, conditioned on the learned feature selection mask. This conditioning is done by scaling the
standard-normalized input features by the learned feature selection mask, before the mask is sparsiﬁed via
sparsemax. Table 6 shows the results of our HSIC experiments. Overall, we observe the version of SLM with
HSIC to be worse than the original version, but it does improve over the other baselines (and most notably,
signiﬁcantly better than HSIC-Lasso, a commonly-used feature selection method that integrates the HSIC
objective as well).
Feature Selection Method Isolet Activity
SLM 0.919 0.947
SLM with HSIC (instead of MI) 0.918 0.931
LassoNet 0.885 0.849
Fisher 0.793 0.769
HSIC-Lasso 0.877 0.829
PFA 0.863 0.779
XGBoost 0.879 0.926
MI 0.751 0.883
Linear 0.760 0.914
Anova 0.811 0.901
Random Forest 0.892 0.893
Table 6: Feature selection performance, including replacing the MI regularizer in SLM with the HSIC objective.
The hyperparameter tuning based on the validation set is performed similarly to the main experiments in
§5.2. SLM learned feature mask combined with the HSIC objective perform very competitively compared to
the myriad of strong baselines, demonstrating the eﬀectiveness and generalizability of SLM’s learn feature
selection mask approach.
A.9 Synthetic Data Experiments
We demonstrate the performance of SLM on a synthetic dataset that is speciﬁcally constructed such that only
a small subset of features aﬀect the output value while the vast majority are not useful for the task. All input
features Xi,jare sampled from the uniform distribution U[−1,1]and the noise at the end /epsilon1i,jare sampled
from standard Gaussian random variable with zero mean and unit variance. The input-output relationship
are governed by the equations shown below:
T(1)
i,j=1
LL/summationdisplay
i=1exp(Xi,j), (17)
T(2)
i,j= exp(1
L2L/summationdisplay
i=L+1|sin(2πXi,j|), (18)
T(3)
i,j=1
L3L/summationdisplay
i=2L+1−log(1.1 +Xi,j)), (19)
T(4)
i,j=1
L4L/summationdisplay
i=3L+1Xi,j, (20)
T(5)
i,j= 1/(1 +1
L5L/summationdisplay
i=4L+1|tanh( Xi,j)|), (21)
20Under review as submission to TMLR
Yi,j=/braceleftBigg
1,if(T(1)
i,j+T(2)
i,j+T(3)
i,j+T(4)
i,j+T(5)
i,j−3 + 0.2/epsilon1i,j)>0,
0,otherwise. (22)
As can be seen, the function is highly nonlinear in dependence to the input features, and in total 5Lfeatures
are salient.
Hyperparameter Search space
Batch size [128, 256, 512]
Learning rate [0.001, 0.003, 0.01]
Decay steps [1000, 10000]
Decay rate [0.7, 0.9, 0.95, 0.99]
Number of epochs [30, 100, 200]
Number of hidden units [30, 50, 100]
Number of layers [1, 2, 3,]
Table 7: Hyperparameter tuning search space for experiments on the Synthetic dataset.
We construct the dataset with 3000 features among which only 100 or 300 are salient, i.e. L= 20orL= 60for
two diﬀerent training dataset size values, 35000 and 14000 training samples respectively. Train-validation-test
are split with 0.7-0.1-0.2 ratio, similar to all other experiments and hyperparameter tuning is done with the
search space presented in 7. We compare SLM with other feature selection methods, when they are used to
select the 300 features. Table 8 and 9 highlight the superior performance of SLM compared to the alternative
methods for challenging datasets with a very large number of features.
Feature selection method Test accuracy (%)
SLM 71.5
Anova 67.9
RF 62.6
Linear 67.0
MI 63.0
Table 8: Test accuracy (%) on the Synthetic dataset with 300 salient features ( L= 60) and 14000 training
samples.
Feature selection method Test accuracy (%)
SLM 73.9
Anova 69.0
RF 69.9
Linear 69.7
MI 61.2
Table 9: Test accuracy (%) on the Synthetic dataset with 100 salient features ( L= 20) and 35000 training
samples.
A.10 Further Comparison with End-to-end Baselines
One of SLM’s strengths is end-to-end feature selection along with task learning, which allows the model to
incorporate inductive biases from the task directly into feature selection. Therefore, we speciﬁcally focus on
comparing SLM with additional end-to-end feature selection methods, beyond the results in Table 1. As
discussed in §2, Concrete Autoencoder (Abid et al., 2019) proposes an unsupervised feature selector based on
using a concrete selector layer as the encoder and using a deep neural network as the decoder. FsNet (Singh
et al., 2020) uses a concrete random variable for discrete feature selection in a selector layer and a supervised
21Under review as submission to TMLR
deep neural network regularized with the reconstruction loss, with a focus on biological data, which are often
high-dimensional with limited sample size. STG (Yamada et al., 2020) develops a fully embedded supervised
method that learns stochastic gates with a probabilistic relaxation of the count of the number of selected
features. While all these works selects features and learns task prediction end-to-end, given that SLM is a
supervised model, with a general focus beyond the high-dimensionality and low-sample-size setting, STG
(Yamada et al., 2020) is the strongest, most related baseline to compare SLM with. Table 10 shows the
comparison between SLM and STG on the Isolet and Activity datasets with 50 selected features. There
are certain similarities between how SLM and STG control which feature to select: SLM learns a sparse
probability mask mfor the features, whereas STG learns learn the parameters of the approximate Bernoulli
distributions via gradient descent for each feature. While STG learns the parameters for each Bernoulli
variable independently, one advantage SLM has is accounting for interdependence amongst selected features,
through both the fact that the probabilities in mare interdependent, and through the MI regularizer (further
details discussed in §6).
Feature Selection Method Isolet↑ Activity↑
SLM 92.49±0.20 93.35±0.82
STG 84.50±1.9891.81±0.71
Table 10: Test accuracy (%) comparison between SLM with a closely related, end-to-end feature selection
baseline STG, which controls feature selection via learned stochastic gates, on the Isolet and Activity datasets
with 50 features selected. The two methods are compared under the exact same conditions to the largest
extent possible: using the same hidden dimension, number of epochs, batch size, learning rate, etc., all
randomly generated from within a feasible range. The non-shared hyperparameters are also generated from
random within a feasible range. The results are averaged over ten diﬀerent runs. SLM is able to account for
interdependence amongst selected features, through the learned mask mand the MI regularizer.
22