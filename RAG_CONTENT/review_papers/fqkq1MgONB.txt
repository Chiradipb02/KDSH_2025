Published in Transactions on Machine Learning Research (12/2024)
BM2: Coupled Schrödinger Bridge Matching
Stefano Peluchetti stepelu@sakana.ai
Sakana AI
Reviewed on OpenReview: https: // openreview. net/ forum? id= fqkq1MgONB
Abstract
A Schrödinger bridge establishes a dynamic transport map between two target distributions
via a reference process, simultaneously solving an associated entropic optimal transport
problem. We consider the setting where samples from the target distributions are available,
and the reference diffusion process admits tractable dynamics. We thus introduce Coupled
Bridge Matching (BM2), a simple non-iterative approach for learning Schrödinger bridges
with neural networks. A preliminary theoretical analysis of the convergence properties of
BM2is carried out, supported by numerical experiments that demonstrate the effectiveness
of our proposal.
1 Introduction
The Schrödinger bridge problem seeks a process, the Schrödinger bridge, with prescribed initial and terminal
distributions, such that the distribution of the Schrödinger bridge minimizes the Kullback-Leibler (KL)
divergence to the distribution of a reference process. Schrödinger bridges play a central role in measure
transport theory (Marzouk et al., 2016). Notably, it is known that the initial-terminal distribution of a
Schrödinger bridge provides a solution to a corresponding entropic optimal transport problem (Peyré & Cuturi,
2020). Schrödinger bridges thus provide an effective framework for finding an alignment between samples
from two target distributions. Furthermore, diffusion-based generative models (Ho et al., 2020; Song et al.,
2021) can be interpreted as solving trivial instances of the Schrödinger bridge problem (Peluchetti, 2023).
Consequently, Schrödinger bridges offer a more general approach to contemporary generative applications.
We consider the setting where samples are readily available from both target distributions, and where the
reference process is a diffusion process solution to a stochastic differential equation (SDE). We thus introduce
Coupled Bridge Matching (BM2), a novel methodology aimed at computing the Schrödinger bridge given
the reference SDE and samples from the two marginal distributions of interest. BM2builds upon Bridge
Matching (BM), introduced1by Peluchetti (2021). Our approach advances recent contributions by Peluchetti
(2023); Shi et al. (2023) by removing the need to solve a sequence of optimization problems. A neural network
is employed to jointly learn a forward drift function and a backward drift function corresponding to the
forward and backward dynamics of a Schrödinger bridge. BM2achieves several key desiderata:
(i)non-iterative: training is conducted through standard stochastic gradient descent within a single
optimization loop;
(ii)exact: the idealized version of BM2yields the target Schrödinger bridge without approximations; the
only sources of error involved in its practical implementation are the neural network approximation
error and the discretization error due to sampling the learned SDE;
(iii)efficient gradient: the gradient of the loss function with respect to neural network parameters depends
solely on few random variables sampled at the current optimization step;
1Peluchetti (2021) used the term “Diffusion Bridge Mixture-Matching Transport” (DBMT), but we follow Shi et al. (2023) in
using the sleeker nomenclature “Bridge Matching” for this transport.
1Published in Transactions on Machine Learning Research (12/2024)
(iv)simple loss: the loss function avoids derivative terms with respect to neural network inputs and does
not impose hard constraints (such as conservative vector field requirements) on the neural network
approximator.
These features collectively enhance the efficiency and applicability of BM2in solving Schrödinger bridge
problems. Training is robust, as it does not depend on hyperparameters that are typically challenging to set
without time-consuming pilot runs, such as the number of training steps per optimization iteration (i) or
the level of approximation (ii). Moreover, the memory requirements are modest due to (iii). Finally, the
implementation is straightforward (i, iv), as illustrated in Algorithms 1 and 2 and in the annotated PyTorch
code of Listing 1.
Content : This paper is structured as follows. In Section 2, we formally introduce the Schrödinger bridge
problem with associated reference process dynamics. Section 3 reviews Bridge Matching, while Section 4
introduces Coupled Bridge Matching, discussing its theoretical properties and implementation aspects.
Numerical experiments are presented in Section 5, followed by a discussion of related works in Section 6.
Section 7 concludes the paper. For clarity, a more general formulation of BM2is deferred to Appendix A, all
proofs to Appendix B, an additional numerical experiment to Appendix C, and code listings to Appendix D.
Notation and Assumptions : To enhance accessibility, we refrain from discussing the more technical aspects
related to the Schrödinger bridge problem in its path measure formulation. The excellent treaties of Léonard
(2014b;a) and Bortoli et al. (2021, Appendices D, H) already serve this goal. We denote distributions with
uppercase letters and their corresponding (Lebesgue) densities with lowercase letters. All stochastic processes
considered are d-dimensional, continuous, and defined on the unit time interval [0,1]. For a stochastic
processXwith distribution P(denotedX∼P), we use subscripts to specify marginal distributions, joint
distributions, and conditional distributions of P.Pt: marginal distribution of Xtat timet, with density
pt;P0,1: initial-terminal joint distribution of (X0,X1);P|0: distribution of Xgiven its initial value X0.
Superscripts indicate a distribution P’s dependency on another distribution Z, as inPZ, or a sequence
of distributions, as in P(i),i≥1. For ad-dimensional distribution Q0, we define the stochastic process
mixture distribution Q0P|0as:(Q0P|0)(X∈·):=/integraltext
P|0(X∈·|x0)Q0(dx0). From a generative perspective,
X∼Q0P|0is obtained by sampling X0∼Q0and thenX∼P|0(·|X0)conditionally on X0. The marginal-
conditional decomposition of Pover its initial value is thus P=P0P|0. Similarly, for a d×d-dimensional
joint distribution Q0,1, we define Q0,1P|0,1such thatX∼Q0,1P|0,1is obtained by sampling (X0,X1)∼Q0,1
and thenX∼P|0,1(·|X0,X1)conditionally on X0andX1. Time is always indexed on a common forward
timescale, on which all stochastic processes’ distributions are defined. The dynamics of a diffusion process
X∼Pcan be formulated in both forward and backward time directions, through corresponding forward and
backward SDEs. In backward SDEs, tdecreases from 1to0(dtis negative), which is denoted by t∈[1,0].
All Brownian motions are independent. Unless otherwise noted, each diffusion process is a Markov diffusion
process which is a (weak) solution to an associated SDE.
2 Problem Setting
2.1 Schrödinger Bridges and Entropic Optimal Transport
For two target d-dimensional distributions Ψ0andΨ1, and a reference stochastic process distribution R, the
dynamic Schrödinger bridge (SB) problem seeks to find
SΨ0,Ψ1,R:= arg min
P∈P(Ψ0,Ψ1)KL(P∥R), (1)
where KL(·∥·)is the KL divergence and P(Ψ0,Ψ1)is the class of distributions of stochastic processes
having initial distribution Ψ0and terminal distribution Ψ1. We narrow down (1) to the case where Ris the
distribution of a diffusion process. In this case, under suitable conditions (Léonard, 2014b), (1) admits a
unique solution which is also a diffusion process. From this point forward, Ψ0,Ψ1andRare considered fixed.
For brevity, we will thus denote the Schrödinger bridge SΨ0,Ψ1,Rsimply asS, and apply the same notation
convention to any distribution dependent on these variables.
2Published in Transactions on Machine Learning Research (12/2024)
The forward and backward dynamics of X∼Sare given by:
X0∼Ψ0, dXt=µs(Xt,t)dt+σdWt, t∈[0,1], (S)
X1∼Ψ1, dXt=−υs(Xt,t)dt+σdWt, t∈[1,0], (← −S)
for the SB-optimal drift functions µs,υs. These functions are related to the Schrödinger potentials (Léonard,
2014b) and are not analytically available aside from very specific choices of Ψ0,Ψ1andR.
We assume that R0,1admits density r0,1. OnceSis obtained, the solution to the staticSchrödinger bridge
problem is given by S0,1:
S0,1= arg min
C0,1∈C(Ψ0,Ψ1)KL(C0,1∥R0,1),
= arg min
C0,1∈C(Ψ0,Ψ1)E
C0,1[−logr1|0(X1|X0)]−H(C0,1).(2)
In (2),C(Ψ0,Ψ1)denotes the class of d×d-dimensional joint distributions with marginal distributions Ψ0and
Ψ1, commonly referred to as the class of couplings of Ψ0andΨ1, andH(C0,1):=EC0,1[−logc1,0(X1,X0)]is
the entropy of C0,1.
The entropic optimal transport (EOT) solution for the cost function k(x0,x1)and regularization level εis
given by:
E0,1:= arg min
C0,1∈C(Ψ0,Ψ1)E
C0,1[κ(X1,X0)]−εH(C0,1). (3)
Thus, for each choice of R0,1in (2),S0,1solves a corresponding problem (3). As in the following, when R
is associated to ( R),S0,1solves the EOT problem (3) for the Euclidean cost κ(x1,x0) = 1/2∥x0−x1∥2and
regularization level ε=σ2.
We refer to Peyré & Cuturi (2020); Léonard (2014b); Gushchin et al. (2023) for related background material
from complementary perspectives.
2.2 Reference Dynamics
We focus on the case where Ris the distribution of a scaled Brownian motion:
X0∼Ψ0, dXt=σdWt, t∈[0,1], (R)
withσ>0. Our approach is not limited to the choice of SDE ( R), BM2readily extends to the broader class
of reference SDEs examined in Peluchetti (2023). The main requirement for the applicability of BM2is the
analytical availability of (4, 5) for the chosen reference SDE. We address the case, commonly employed in
generative applications, of dXt=σ√βtdWtfor a schedule βtexplicitly in Appendix A, and refer the reader
to Peluchetti (2021; 2023) for the general setting. As our developments are orthogonal to the specific choice
of reference process, we focus on the simplest case for explanatory reasons.
We collect here various results concerning ( R) that will be utilized in the following:
Rt|0(·|x0) =N(x0,σ2t), (4)
Rt|0,1(·|x0,x1) =N(x0(1−t) +x1t,σ2t(1−t)), (5)
µ01(xt,t,x 1):=σ2∇xtlogr1|t(x1|xt) =x1−xt
1−t, (6)
υ01(xt,t,x 0):=σ2∇xtlogrt|0(xt|x0) =x0−xt
t, (7)
γ01(xt,t,x 0,x1):=σ2∇xtlogrt|0,1(xt|x0,x1) =x0(1−t) +x1t−xt
t(1−t). (8)
Conditioning X∼Ron the endpoints X0=x0,X1=x1results in the diffusion bridge distribution R|0,1,
with associated forward and backward SDEs:
X0=x0, dXt=µ01(Xt,t,x 1)dt+σdWt, t∈[0,1], (R|0,1)
3Published in Transactions on Machine Learning Research (12/2024)
X1=x1, dXt=−υ01(Xt,t,x 0)dt+σdWt, t∈[1,0]. (←−−R|0,1)
3 Bridge Matching (BM)
We succinctly review Bridge Matching, and refer to Peluchetti (2021; 2023); Shi et al. (2023) for more details.
BM takes as input a joint distribution Q0,1with marginal distributions Q0,Q1and a SDE, ( R). Firstly, a
stochastic process ΠQ0,1is constructed as a mixture of diffusion bridges ( R|0,1), such that the endpoints
(X0,X1)ofX∼ΠQ0,1are distributed according to Q0,1. This process, which is a mixture of diffusion
processes, is not itself a diffusion process in general (Jamison, 1974). However, we can obtain a marginal-
matching diffusion process with distribution MQ0,1for whichMQ0,1
t = ΠQ0,1
t,0≤t≤1. Consequently,
X∼MQ0,1is a diffusion process for which X0∼Q0andX1∼Q1, i.e. it defines a dynamic transport from
Q0toQ1.
Concretely, let ΠQ0,1:=Q0,1R|0,1. The BM transport based on Q0,1with distribution MQ0,1is realized by
X0∼Q0, dXt=µQ0,1
m(Xt,t)/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
EΠQ0,1[µ01(Xt,t,X 1)|Xt]dt+σdWt, t∈[0,1], (M)
X1∼Q1, dXt=−υQ0,1
m(Xt,t)/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
EΠQ0,1[υ01(Xt,t,X 0)|Xt]dt+σdWt, t∈[1,0], (← −M)
and satisfies MQ0,1
t= ΠQ0,1
t,0≤t≤1.
As conditional expectations are mean squared error minimizers, suitable training objectives for the drift
functionsµQ0,1mandυQ0,1mare derived from
µQ0,1
m= arg min
µE
ΠQ0,1/bracketleftig1
2/integraldisplay1
0∥µ01(Xt,t,X 1)−µ(Xt,t)∥2dt/bracketrightig
, (9)
υQ0,1
m= arg min
υE
ΠQ0,1/bracketleftig1
2/integraldisplay1
0∥υ01(Xt,t,X 0)−υ(Xt,t)∥2dt/bracketrightig
, (10)
by replacing each integral with an expectation over uniform time t∼U(0,1), and then approximating both
expectations with Monte Carlo estimators. While we will rely exclusively on (9, 10) in the experiments of
Section 5,µQ0,1mandυQ0,1mcan be inferred from paths X∼ΠQ0,1also by performing maximum likelihood
estimation or by employing a drift matching estimator (Liu et al., 2022; Peluchetti, 2023).
We conclude this section by reviewing prior BM results relevant for BM2. Define:
P:={d-dimensional, continuous, stochastic processes on [0,1]},
R:={P∈P|P=P0,1R|0,1= ΠP0,1for someP0,1},
M:={P∈P|Pis a (Markov) diffusion process },
S:={P∈P|Pis a Schrödinger bridge for some target marginal distributions }=R∩M,
where the equivalence is established by Jamison (1975) under appropriate assumptions. We additionally
define the following restrictions: P(Ψ0,·):={P∈ P |P0= Ψ 0},P(·,Ψ1):={P∈ P |P1= Ψ 1},
P(Ψ0,Ψ1):={P∈P|P0= Ψ 0andP1= Ψ 1}. Restrictions toR,M,SandCemploy the same notation.
ForQ∈P, it is instructive to view BM as a map between distributions defined by the composition of two
projections: QRp→ΠQ0,1Mp→MQ0,1. Here, the reciprocal projection Rp:P→RprojectsQonto the reciprocal
classR, while the Markovian projection Mp:R→M projects ΠQ0,1onto the class of diffusion processes, see
Shi et al. (2023). It follows that if P∈R, thenP=Rp(P), and ifP∈M, thenP=Mp(P). Consequently,
ifP∈S,P= (Mp◦Rp)(P)for the BM map (Mp◦Rp), and conversely if P= (Mp◦Rp)(P)thenP∈S.
3.1 Iterated Bridge Matching (I-BM) and Diffusion Iterative Proportional Fitting (DIPF)
In the dynamic setting, Peluchetti (2023); Shi et al. (2023) demonstrate that, under suitable conditions,
iterative application of the BM procedure to an initial coupling C0,1∈C(Ψ0,Ψ1)results in convergence
4Published in Transactions on Machine Learning Research (12/2024)
towardS. Specifically, defining I(0):=MC0,1andI(i):=MI(i−1)
0,1fori≥1, it holds that KL(I(i)∥S)→0
asi→∞. In practical applications, the independent initial coupling given by the product distribution
C0,1= Ψ 0⊗Ψ1is frequently employed.
In the static setting, the classical procedure employed in solving problems (2, 3) is known by several names: the
Sinkhorn algorithm (Peyré & Cuturi, 2020), the Iterated Proportional Fitting (IPF) procedure (Ruschendorf,
1995), or Fortet iterations (Fortet, 1940). The iterates are given by D(0)
0,1:= Ψ 0R1|0,D(1)
0,1:= Ψ 1K(0)
0|1,
D(2)
0,1:= Ψ 0D(1)
1|0, and so on. At each iteration, one of the target marginal distributions is replaced while the
remaining conditional distribution is kept fixed. Alternatively, one can start from Ψ1R0|1following the same
logic. Under suitable conditions (Ruschendorf, 1995), KL convergence KL(S0,1∥D(i)
0,1)→0is established.
The key insight of Bortoli et al. (2021); Vargas et al. (2021) is that it is possible to extend the IPF iterations
to the dynamic setting. In this case, the IPF iterations are implemented by learning the time reversal of a
diffusion process at each iteration. We refer to the resulting training algorithm, as proposed by Bortoli et al.
(2021), as Diffusion Iterative Proportional Fitting (DIPF). Bortoli et al. (2021) establishes the convergence
properties of the DIPF iterates, see their Propositions 4, 5 and Section 3.5.
4 Coupled BM (BM2)
As a starting point for the derivation of BM2, consider the system of equations


HK′
0,1= Ψ 0MK′
0,1
|0
KH′
0,1= Ψ 1MH′
0,1
|1, (11)
whose variables are diffusion distributions HK′
0,1,KH′
0,1andH′,K′. That is,HK′
0,1is obtained as the BM
transport based on K′
0,1conditioned to have initial distribution Ψ0, whileKH′
0,1is obtained as the BM
transport based on H′
0,1conditioned to have terminal distribution Ψ1. Equivalently, (11) is expressed as
X0∼Ψ0, dXt=µK′
0,1m(Xt,t)dt+σdWt, t∈[0,1], (HK′
0,1)
X1∼Ψ1, dXt=−υH′
0,1m(Xt,t)dt+σdWt, t∈[1,0]. (← −KH′
0,1)
All ofµm,υm,Mare defined in Section 3. System (11) defines an update step (H′,K′)(11)→(HK′
0,1,KH′
0,1).
We are interested in the fixed points of such updates, i.e. (H′,K′)such that (H′,K′)(11)→(H′,K′). It holds
thatH′=K′=Sis a fixed point to (11). As S∈S(Ψ0,Ψ1),S= ΠS0,1=MS0,1, see the review at the end
of Section 3. Consequently, Ψ0MS0,1
|0= Ψ 0S|0=SandΨ1MS0,1
|1= Ψ 1S|1=S. In this case, the SB-optimal
driftsµsandυsof (S,← −S) respectively replace µK′
0,1mandυH′
0,1min (HK′
0,1,← −KH′
0,1). Under the additional
assumption that H′=K′, or equivalently that ( HK′
0,1,← −KH′
0,1) are the time reversal of each other, this
fixed point is unique. Let G=H′=K′, we haveG= Ψ 0MG0,1
|0=G0MG0,1
|0=MG0,1
0MG0,1
|0=MG0,1and
G0= Ψ 0,G1= Ψ 1, thusG=S(Ψ0,Ψ1) =S. We have shown the following:
Lemma 1 (Fixed points of (11)) .Under suitable conditions (Léonard, 2014a), the updates (H′,K′)(11)→
(HK′
0,1,KH′
0,1), parametrized by diffusion process distributions, admit H′=K′=Sas fixed point. If H′=K′,
this fixed point is unique.
WhenµK′
0,1m=µsandυH′
0,1m=υs, (11) has reached an equilibrium. The updates (H′,K′)(11)→(HK′
0,1,KH′
0,1)
are realized through the computation of the drifts µK′
0,1mandυH′
0,1m, i.e. by minimizing the losses (9, 10),
whereQ0,1is respectively equal to K′
0,1andH′
0,1. Our proposal, BM2, follows from replacing the complete
minimization of (9, 10) with partial and stochastic minimization of (9, 10) through stochastic gradient descent.
More precisely, consider the forward and backward SDEs with distributions F(θ)andB(θ):
X0∼Ψ0, dXt=µf(Xt,t,θ)dt+σdWt, t∈[0,1], (F(θ))
5Published in Transactions on Machine Learning Research (12/2024)
X1∼Ψ1, dXt=−υb(Xt,t,θ)dt+σdWt, t∈[1,0]. (← −B(θ))
µf(Xt,t,θ)andυb(Xt,t,θ)are drift functions to be learned, which are implemented through a neural network
with parameters θ. Letθ′represent the values of θat a given step during training, and define the losses
Lf(θ;θ′):=E
ΠB0,1(θ′)/bracketleftig1
2/integraldisplay1
0∥µ01(Xt,t,X 1)−µf(Xt,t,θ)∥2dt/bracketrightig
,
Lb(θ;θ′):=E
ΠF0,1(θ′)/bracketleftig1
2/integraldisplay1
0∥υ01(Xt,t,X 1)−υb(Xt,t,θ)∥2dt/bracketrightig
,
L(θ;θ′):=Lf(θ;θ′) +Lb(θ;θ′).(12)
At each optimization step, BM2attempts to minimize L(θ;θ′)inθvia a step of stochastic gradient descent,
starting from θ=θ′and keeping θ′fixed, resulting in θ′′. The subsequent optimization step employs θ′←θ′′.
The complete training objective is presented in Algorithm 1, where sg()refers to the stop-gradient operator —
L(θ;θ′)is minimized in the first arguments only — and discretize() represents a generic SDE discretization
scheme. For completeness, we outline the standard SGD training loop in Algorithm 2, where sgdstep() refers
to an update step via a generic gradient descent optimizer.
It should be noted that merely performing coupled drift matching of FandB, whereinFlearns the drift
consistent with paths from Band vice versa, does not yield the Schrödinger bridge as a fixed point (Bortoli
et al., 2021). The introduction of the mixing process Πis crucial in ensuring this property. Moreover, L(θ;θ′)
must be minimized only with respect to its first argument: the application of the stop-gradient operator sg()
is not an efficiency consideration but a necessary component.
Algorithm 1 BM2— training loss computation
outputs: l(θ): sampled loss value
inputs:θ: current parameters
1:function loss(θ)
2: f0∼Ψ0 ▷Marginal sampling
3: f∆t,..., f1|f0∼sg(discretize( f0,∆t,µf(·,·,θ))) ▷Discretization of ( F(θ))
4: b1∼Ψ1 ▷Marginal sampling
5: b1−∆t,..., b0|b1∼sg(discretize( b1,∆t,υb(·,·,θ))) ▷Discretization of (← −B(θ))
6: t∼U(0,1) ▷Time sampling
7:πft∼Rt|0,1(·|f0,f1) ▷Bridge sampling (5)
8:πbt∼Rt|0,1(·|b0,b1) ▷Bridge sampling (5)
9: lf(θ)←1/2∥µ01(πbt,t,b1)−µf(πbt,t,θ)∥2▷BM based on B0,1(6, 9)
10: lb(θ)←1/2∥υ01(πft,t,f0)−υb(πft,t,θ)∥2▷BM based on F0,1(7, 10)
11: l(θ)←lf(θ) +lb(θ)
12: return l(θ)
Algorithm 2 BM2— training loop
outputs:θ∗: trained parameters
inputs:θ◦: initial parameters
1:function train(θ◦)
2:θ←θ◦
3:whilenot converged do
4: l(θ)←loss(θ) ▷Sample loss with Algorithm 1
5:θ←sgdstep (θ,∇θl(θ)) ▷Perform SGD step
6:returnθ
4.1 Implementation Aspects
The following aspects are not presented in Listing 1, but impacts the performance of BM2.
6Published in Transactions on Machine Learning Research (12/2024)
Path Caching : as in Bortoli et al. (2021); Shi et al. (2023), to enhance efficiency, we cache the initial and
terminal endpoints of the paths sampled in lines 3 and 5 of Algorithm 1, and periodically refresh the cache
during training. Notably, it is unnecessary to cache entire paths; only the endpoints are required for bridge
sampling, which is advantageous from a memory perspective. Bridge sampling offers the additional benefit of
increased sample diversity: for cached (fixed) endpoints, the samples corresponding to lines 7 and 8 differ at
each step.
Model: we utilize a single neural network to parametrize both µf(x,t,θ )andυb(x,t,θ ). As the training
process is not iterative, it is unnecessary to introduce multiple neural networks (or parameters), one for each
iteration.
Sampling EMA : as in Ho et al. (2020); Song et al. (2021), to improve the stability of training we apply
the Exponential Moving Averaging (EMA) to the parameters employed in path sampling in lines 3 and 5 of
Algorithm 1.
Loss Singularities : the losses of lines 9 and 10 of Algorithm 1 diverge for t→1and t→0respectively.
Singularities of these kind are common to scalable losses for generative diffusion models. In our numerical
experiments we simply restrict sampling of ttoU(ϵ,1−ϵ)for a small ϵ>0. More sophisticated alternatives
involve either employing the dynamics of Appendix A for an appropriate scheduling βt, or learning terminal-
value predictors in place of drift terms, recovering the latter through (6, 7).
Two-Stage Training : a significant challenge in early training is the simulation-inference mismatch. To
achieve reliable results, the drift functions µf(xt,t,θ)andυb(xt,t,θ)must be accurately learned in regions
where the corresponding SDEs ( F(θ)) and (← −B(θ)) will be simulated. However, the processes F(θ)andB(θ)
typically differ at initialization. Because the drift of ( F(θ)) is inferred from samples of (← −B(θ)) (and vice
versa), the approximation quality can be poor; see Peluchetti (2023, Sections 2.3 and 6.2) for a detailed
analysis of this issue within the DIPF framework. To address this challenge, the BM transport based on
the independent coupling Ψ0⊗Ψ1can be learned in both directions in a first stage. By construction, the
BM transport circumvents the simulation-inference mismatch. Moreover, at convergence, the processes F(θ)
andB(θ)are the same. Subsequently, in the second stage, the BM2transport can be learned employing the
first-stage solution as initialization.
Forward-Backward Consistency : the mutual time reversal relationship between ( F(θ)) and (← −B(θ)), i.e.,
the equivalence of F(θ)andB(θ), can be encouraged leveraging the diffusion time-reversal result of Anderson
(1982), yielding the additional consistency loss term:
Lf,b(θ;θ′):= E
1
2(ΠF0,1(θ′)+ΠB0,1(θ′))/bracketleftig1
2/integraldisplay1
0∥µf(Xt,t,θ) +υb(Xt,t,θ)−γ01(Xt,t,X 0,X1)∥2dt/bracketrightig
,(13)
whereθ′denotes an independent copy of θ(implemented via the stop-gradient operator), and γ01(xt,t,x 0,x1)
is defined in (8). Indeed, for a mixture process ΠC0,1=C0,1R|0,1, the following relationship holds:
σ2∇logπC0,1
t(xt) =E
ΠC0,1[γ01(Xt,t,X 0,X1)|Xt=xt].
Although (13) shares similarities with the consistency loss proposed by Shi et al. (2023, page 33), (13)
utilizes the dynamic mixture1
2(ΠF(θ′)0,1+ ΠB(θ′)0,1) = Π1
2(F(θ′)0,1+B(θ′)0,1). When a single neural network
is employed to parametrize both µf(xt,t,θ)andυb(xt,t,θ), estimating (13) in addition to (12) results in a
negligible computational overhead per SGD step.
4.2 Convergence Properties
At each training step, BM2performs a partial and stochastic minimization of the loss L(θ;θ′)from (12) with
respect toθ, where L(θ;θ′)is defined by an expectation over a distribution dependent on θ′, yieldingθ′′.
Subsequently, θ′is updated to match θ′′, and the process advances to the next training step. The alternation
between expectation and maximization steps bears resemblance to the classical Expectation-Maximization
(EM) algorithm (Dempster et al., 1977).
7Published in Transactions on Machine Learning Research (12/2024)
4.2.1 Complete Minimization
We start by establishing in Theorem 1 that the version of BM2where L(θ;θ′)is fully minimized at each
training step recovers the I-BM and DIPF iterations for two specific initialization choices of ( F(θ),← −B(θ)).
The prior convergence results of Bortoli et al. (2021); Shi et al. (2023); Peluchetti (2023) (see the review of
Section 3.1) toward Sthus apply.
To facilitate the presentation of the convergence results in this section, we introduce, with a slight abuse of
notation, the following functional versions of the losses (12):
Lf(µf;υ′
b):=E
ΠB′
0,1/bracketleftig1
2/integraldisplay1
0∥µ01(Xt,t,X 1)−µf(Xt,t)∥2dt/bracketrightig
,
Lb(υb;µ′
f):=E
ΠF′
0,1/bracketleftig1
2/integraldisplay1
0∥υ01(Xt,t,X 1)−υb(Xt,t)∥2dt/bracketrightig
,
L(µf,υb;µ′
f,υ′
b):=Lf(µf;υ′
b) +Lb(υb;µ′
f).(14)
In (14) we identify µf,υbwithF,B, andµ′
f,υ′
bwithF′,B′(the remaining quantities defining F,B,F′,B′
are fixed). We will use Lf(µf;υ′
b),Lb(υb;µ′
f)andLf(θ;θ′),Lb(θ;θ′)interchangeably. We are now ready to
state our first convergence result.
Theorem 1 (Complete BM2Iterations) .Consider the SDEs ( F(θ),← −B(θ)), with initial drifts µ(0)
f,υ(0)
band
corresponding distributions F(0),B(0). For each i≥1, let (µ(i)
f,υ(i)
b) =arg min(µ,υ)L(µ,υ;µ(i−1)
f,υ(i−1)
b),
resulting in the distribution iterates F(i),B(i). We distinguish two cases:
(i)µ(0)
f=υ(0)
b= 0: both the iterates F(0),B(1),F(2),...and the iterates B(0),F(1),B(2),...are equiva-
lent to the DIPF iterates, started respectively from the forward and from the backward time direction;
(ii)µ(0)
f=µC0,1m,υ(0)
b=υC0,1mfor someC∈C(Ψ0,Ψ1):F(i)=B(i)=I(i)for eachi≥0whereI(i)are
the I-BM iterates.
4.2.2 Partial Minimization
In the EM algorithm it suffices to perform partial maximization steps. A partial result for the setting where
L(θ;θ′)is partially minimized with respect to θat each step is stated in Theorem 2, which is based on
Lemma 1 and Lemma 2.
Lemma 2 (Loss Interpretation) .It holds that
KL(B0∥Ψ0) +Lf(µf;υb) =KL(ΠB0,1∥F) +C1(B) =KL(MB0,1∥F) +C2(B),
KL(F1∥Ψ1) +Lb(υb;µf) =KL(ΠF0,1∥B) +D1(F) =KL(MF0,1∥B) +D2(F),(15)
forC1(B),C2(B)independent of F,D1(F),D2(F)independent of B, with 0≤C1(B)≤C2(B)and0≤
D1(F)≤D2(F).
The losses Lf(µf;υb)andLb(υb;µf)are easily amenable to optimization in their first arguments, as seen in
Algorithm 1. Lemma 2 relates these losses to more interpretable KL divergences between distributions. By
(15), a decrease of Lf(µf;υb)due to a change in µfcorresponds to equivalent decreases of KL(MB0,1∥F)for
a fixedυb, orB. Thus, partial minimization of Lf(µf;υb)bringsFcloser toMB0,1, the BM transport based
onB0,1, and the result of a complete minimization step, by means of reverse KL minimization. Symmetric
considerations apply to Lb(υb;µf)as function of its first argument. Putting this result and Lemma 1 together
yields Theorem 2.
Theorem 2 (Partial BM2Iterations) .At each optimization step, decreases of Lf(θ;θ′)andLb(θ;θ′)inθ
correspond to equivalent decreases of KL(MB0,1(θ′)∥F(θ))andKL(MF0,1(θ′)∥B(θ)). If the losses Lf(θ;θ′)
andLb(θ;θ′)cannot be decreased in θ, i.e., at optimality, and if F(θ) =B(θ), thenF(θ) =B(θ) =S.
8Published in Transactions on Machine Learning Research (12/2024)
4.2.3 Infinitesimal Minimization
We conclude our theoretical investigation by relating our proposal to the work of Karimi et al. (2023),
which introduces a continuous variant of the IPF procedure. In IPF, the two target marginal distributions
are replaced sequentially, one at a time. Each step corresponds to solving a static Schrödinger half-bridge
problem (Léonard, 2014a), where in (2), C(Ψ0,Ψ1)is replaced by either C(Ψ0,·)orC(·,Ψ1). The approach
proposed by Karimi et al. (2023) retains either the even or odd steps of the IPF scheme while substituting
the alternate steps with partial minimizations of the corresponding half-bridge problems. In the limit of
infinitesimally small improvements, this yields a dynamical system for the evolution of the iterates over
continuous algorithmic time.
We demonstrate that a similar result can be obtained for a modified version of BM2, where forward KL
divergences are minimized instead of reverse KL divergences. The resulting dynamical system is a symmetrized
version of the one obtained by Karimi et al. (2023). Let F′,B′represent the current state in the optimization
process. We consider a partial minimization of KL(F∥MB′
0,1), instead of KL(MB′
0,1∥F), inFand a
partial minimization of KL(B∥MF′
0,1), instead of KL(MF′
0,1∥B), inB. As in Karimi et al. (2023), partial
minimization is formulated as
F(λ):= arg min
F∈M(Ψ0,·)λKL(F∥MB′
0,1) + (1−λ)KL(F∥F′),
B(λ):= arg min
B∈M(·,Ψ1)λKL(B∥MF′
0,1) + (1−λ)KL(B∥B′),(16)
whereλ∈[0,1]controls the extent of the minimization. We begin by establishing two stability results: the
updates (F′,B′)(16)→(F(λ),B(λ))preserve bothRandS.
Lemma 3 (R-stability of F(λ),B(λ)).IfF′,B′∈R, thenF(λ),B(λ)∈Rfor eachλ∈[0,1].
Lemma 4 (S-stability of F(λ),B(λ)).IfF′,B′∈S, thenF(λ),B(λ)∈Sfor eachλ∈[0,1].
Provided that the initial values F′,B′∈S, Lemma 4 establishes that the iterates defined by the updates
(F′,B′)(16)→(F(λ),B(λ))always remain in S. It is straightforward to ensure that F′,B′∈Sat initialization
by setting the corresponding drifts to zero: µ′
f,υ′
b= 0, which we will assume henceforth. As MB′
0,1=B′and
MF′
0,1=F′, (16) can be reformulated in simpler terms:
F(λ):= arg min
F∈M(Ψ0,·)λKL(F∥B′) + (1−λ)KL(F∥F′),
B(λ):= arg min
B∈M(·,Ψ1)λKL(B∥F′) + (1−λ)KL(B∥B′).(17)
By Lemma 3, it suffices to solve (17) in the static setting,
F(λ)
0,1:= arg min
F0,1∈C(Ψ0,·)λKL(F0,1∥B′
0,1) + (1−λ)KL(F0,1∥F′
0,1),
B(λ)
0,1:= arg min
B0,1∈C(·,Ψ1)λKL(B0,1∥F′
0,1) + (1−λ)KL(B0,1∥B′
0,1).(18)
The dynamic solutions are then recovered by F(λ)
|0,1=B(λ)
|0,1=R|0,1.
We assume that F′
0,1,B′
0,1,Ψ0,Ψ1admit densities. By calculus of variations, the solution to (18) is
given byf(λ)
0,1(x0,x1) =ψ0(x0)f(λ)
1|0(x1|x0), andb(λ)
0,1(x0,x1) =b(λ)
0|1(x0|x1)ψ1(x1), wheref(λ)
1|0(x1|x0)∝
b′
1|0(x1|x0)λf′
1|0(x1|x0)1−λandb(λ)
0|1(x0|x1)∝f′
0|1(x0|x1)λb′
0|1(x0|x1)1−λ. The IPF iterations are recovered
whenλ= 1. Instead, taking the limit λ→0and applying Bayes theorem, we obtain the evolution of
9Published in Transactions on Machine Learning Research (12/2024)
logf(l)
1|0(x1|x0)andlogb(l)
0|1(x0|x1)as a function of algorithmic time l∈[0,∞)through the dynamical system
dlogf(l)
1|0(x1|x0)
dl=−logf(l)
1|0(x1|x0)
b(l)
0|1(x0|x1)ψ1(x1)+KL(f(l)
1|0(x1|x0)∥b(l)
0|1(x0|x1)ψ1(x1)), l∈[0,∞),
dlogb(l)
0|1(x0|x1)
dl=−logb(l)
0|1(x0|x1)
f(l)
1|0(x1|x0)ψ0(x0)+KL(b(l)
0|1(x0|x1)∥f(l)
1|0(x1|x0)ψ0(x0)), l∈[0,∞).(19)
In (19), KL(·∥·)denotes the generalized KL divergence between unnormalized densities, as is the case
here for the second arguments, and the initial conditions f(0)
1|0(x1|x0)andb(0)
0|1(x0|x1)are determined by
(F(θ),← −B(θ)) with null drift terms. (19) can be contrasted with Karimi et al. (2023, Equation (13)). In
Appendix C.1 we report a simple numerical application of (19) to the Gaussian setting, which recovers S.
5 Numerical Experiments
To evaluate the performance of BM2on EOT problems, we utilize the benchmark developed by Gushchin
et al. (2023). For the reference process ( R), this benchmark provides pairs of target distributions Ψ0,Ψ1
with analytical EOT solution S0,1and analytical SB-optimal drift function µs. We focus on the mixtures
benchmark, which consist of a centered Gaussian distribution as S0= Ψ 0and a mixture of 5 Gaussian
distributions for S1|0.S1= Ψ 1is not a mixture of Gaussian distributions, but has 5 distinct modes.
The benchmark is constructed for dimensions d∈{2,16,64,128}and entropic regularization parameters
ε∈{0.1,1,10}.
For each fully trained method, characterized by a stochastic process distribution Pand forward drift function
µp, we assess performance using two evaluation metrics:
•KL(S∥P)where, by Girsanov theorem (Øksendal, 2013),
KL(S∥P) =E
S/bracketleftig1
2σ2/integraldisplay1
0∥µs(Xt,t)−µp(Xt,t)∥2dt/bracketrightig
; (20)
•cBW2
2-UVP (S0,1,P0,1), where
cBW2
2-UVP (S0,1,P0,1):=100
1
2VS[X1]/integraldisplay
BW2
2(S1|0(X1|X0),P1|0(X1|X0))S0(dX0),(21)
BW2
2(·,·)is the squared Bures-Wasserstein distance, i.e. the squared Wasserstein-2 distance between
(assumed) multivariate Gaussian distributions (Dowson & Landau, 1982), and VS[X1]is the variance
ofX1∼S1.
We focus on the divergence KL(S∥P), rather than KL(P∥S), as a low KL(S∥P)more accurately indicates
thatPapproximates Seffectively across the entire support of S. The data-processing inequality implies
thatKL(S0,1∥P0,1)≤KL(S∥P). ThecBW2
2-UVP (·,·)metric, introduced by Gushchin et al. (2023), is a
normalized and conditional extension of the standard BW2
2(·,·)distance. Results for evaluation metrics (20)
and (21) are summarized in Table 1 and Table 2, respectively.
In our benchmarking, we compare BM2against the I-BM and DIPF methods (Section 3.1). Each experiment is
repeated five times, including both model training and metric evaluation, to obtain uncertainty quantification.
We use 1,000Monte Carlo samples to estimate (20, 21). For simplicity, we employ the Euler–Maruyama
scheme (Kloeden & Platen, 1992) with 200discretization steps ( ∆t= 0.005) in all path sampling procedures.
Each method undergoes 50,000SGD training steps with a batch size of 1,000, settings similar to those used
by Gushchin et al. (2023), enabling qualitative comparison of our results with theirs. We use the AdamW
optimizer with a learning rate of 10−4and hyperparameters: β= (0.9,0.999),ϵ= 10−8,wd= 0.01, wherewd
denotes weight decay. Time is sampled as t∼U(ϵ,1−ϵ)forϵ= 0.0025.
10Published in Transactions on Machine Learning Research (12/2024)
ε=0.1 ε=1 ε=10
Methodd=2d=16d=64d=128d=2d=16d=64d=128d=2d=16d=64d=128
BM20.01
0.010.20
0.021.03
0.073.06
0.160.01
0.000.11
0.001.43
0.038.29
0.360.11
0.012.25
0.0413.13
0.1340.46
0.49
BM2
σ 0.43
0.093.76
0.4639.55
1.96127.2
1.40.04
0.010.43
0.035.36
0.3518.66
0.730.15
0.002.64
0.0513.78
0.2443.42
1.43
I-BM 0.03
0.010.20
0.021.24
0.045.70
0.420.01
0.000.16
0.011.94
0.047.79
0.070.16
0.004.09
0.0317.17
0.2149.17
0.55
DIPF 0.59
0.142.39
0.057.93
1.2334.77
0.820.23
0.061.21
0.1813.13
0.7936.51
1.050.81
0.0628.25
2.12113.8
7.2345.8
8.1
Table 1: Monte Carlo estimate of KL(S∥P)as function of εandd, standard deviation in gray.
For BM2, we employ a single feedforward neural network with 3layers of width 768and ReLU activation,
resulting in approximately 1million parameters. We initialize the neural network parameters such that the
resulting initial forward and backward drift functions evaluate to zero everywhere, a choice that has proven
effective in our experiments. As mentioned in Section 4.1, we implement path caching and an exponential
moving average for parameters used in path sampling. The cache contains 5,000initial-terminal values
from both ( F(θ)) and (← −B(θ)), refreshed every 200training steps. We omit reporting the results for both
the two-stage training procedure and the consistency loss described in Section 4.1, as neither approach
demonstrated consistent performance improvements across the considered benchmark.
For I-BM and DIPF, each outer loop iteration comprises 5,000SGD steps, totaling 10outer loop (algorithmic)
iterations. Following best practices (Bortoli et al., 2021; Shi et al., 2023), we alternate time directions over
iterations for both algorithms. Each method employs two separate neural networks for forward and backward
time directions, maintaining a total parameter count close to 1million, matching BM2’s model size. As with
BM2, we implement path caching (for DIPF, entire discretized paths are cached) and EMA for sampling,
with an EMA decay rate of 0.99.
We also consider BM2
σ, a variant of BM2that learns Schrödinger bridges for Ψ0,Ψ1across multiple σvalues.
This amortized version leverages BM2’s non-iterative nature. At each optimization step, σis sampled from
U(0.1,4)and utilized in discretizing SDEs ( F(θ),← −B(θ)) (lines 3 and 5 of Algorithm 1) and in bridge sampling
(lines 7 and 8 of Algorithm 1). The neural network implementing drift functions µf(x,t,θ )andυb(x,t,θ )
is modified to accept σas an additional input, resulting in conditional drift functions µf(x,t,θ,σ )and
υb(x,t,θ,σ ). Path caching is adjusted to store σvalues corresponding to cached paths.
In Table 2, we additionally include three baselines. EOT: sampling from the EOT solution, accounting for
the bias due to Monte Carlo estimation. SB(discr): sampling from the SB solution via the SB-optimal drift
µs, additionally accounting for Euler–Maruyama scheme discretization error. Ψ0⊗Ψ1: sampling from the
independent coupling. Results for additional discretization intervals are reported in Appendix C.2, and we
illustrate in Figure 1 the evolution of (21) during training for a representative benchmark setting.
1 2 3 4 5251020
Figure 1: Evolution of the metric (21) for d= 64,ε= 1over SGD steps for BM2, I-BM and DIPF.
11Published in Transactions on Machine Learning Research (12/2024)
ε=0.1 ε=1 ε=10
Method d=2d=16d=64d=128d=2d=16d=64d=128d=2d=16d=64d=128
EOT 0.02
0.000.05
0.000.34
0.000.91
0.000.09
0.000.17
0.000.43
0.001.14
0.000.12
0.000.18
0.000.23
0.000.38
0.00
SB(discr.) 0.04
0.000.07
0.000.35
0.000.92
0.000.10
0.000.17
0.000.45
0.001.18
0.000.12
0.001.15
0.005.38
0.0110.48
0.01
Ψ0⊗Ψ1 195.8
6.9186.3
2.4162.6
0.8145.1
2.1136.1
4.7127.6
1.4113.0
1.793.61
1.578.07
0.334.88
0.144.22
0.094.45
0.07
BM20.73
0.404.64
0.586.84
0.598.28
0.620.14
0.030.41
0.041.72
0.098.30
1.170.14
0.012.30
0.0441.14
1.99264.4
7.0
BM2
σ 8.38
3.4016.06
2.8144.15
0.8483.84
0.920.20
0.072.61
0.4125.89
1.6564.76
2.660.14
0.012.57
0.0358.76
0.76323.0
8.8
I-BM 1.07
0.504.25
0.667.19
0.2816.63
2.070.20
0.090.53
0.042.20
0.357.79
0.790.14
0.025.21
0.11135.8
1.3578.7
9.8
DIPF 7.82
2.5115.30
1.0020.12
1.5329.36
1.021.66
0.245.98
0.6513.11
2.4928.86
3.520.69
0.086.85
0.2172.63
0.70226 .1
1.1
Table 2: Monte Carlo estimate of cB W2
2-UVP (S0,1,P0,1)as function of εandd, standard deviation in gray.
We now discuss the results presented in Tables 1 and 2. BM2demonstrates superior overall performance
across dimensions and entropic regularization settings in both metrics. I-BM also shows good performance,
particularly in comparison to the DIPF procedure, which aligns with the findings of Shi et al. (2023).
As expected, the performance of all methods deteriorates as the number of dimensions increases. This is
because the metric(20) scales linearly with the number of dimensions, assuming a constant error rate in
estimating each component of the true drift µs. Similar considerations apply to the metric (21).
While BM2
σexhibits a performance gap compared to BM2, it yields reasonable results in low-dimensional
settings (d= 2,16). This gap may be due to increased pressure on model capacity or the need to normalize
loss levels across σvalues. All methods perform poorly in the high regularization setting ( ε= 10), especially
in high dimensions ( d= 64,128), which we include for completeness. It should be noted that, in such cases,
sampling from the independent coupling (a trivial solution) is preferable to sampling from the SB-optimal
SDE for the chosen discretization interval.
6 Related Works
Relevant works that, like BM2, address the dynamic Schrödinger bridge problem (1) include:
Schrödinger Bridge Flow : in a concurrent work, De Bortoli et al. (2024) propose a non-iterative
methodology called α-DSBM (Diffusion Schrödinger Bridge Matching). α-DSBM is optimally implemented
through a forward-backward SDE approach, in which case α-DSBM’s formulation and training objective align
with our proposal (12, F(θ),← −B(θ)), see De Bortoli et al. (2024, Equations (10) and (11)). The formulation
ofα-DSBM begins by establishing a probability flow in the space of path probability measures, whose
discretization yields α-IMF (Iterative Markovian Fitting). Under mild conditions, two theoretical results
are established: (i) the α-IMF iterates converge to the SB solution, and (ii) the non-parametric updates of
the functional loss (14), obtained through functional gradient descent with respect to the drift functions,
recover the α-IMF iterates. The practical implementation, α-DSBM, thus replaces non-parametric drift
functions with parametric neural network approximators, and employs standard stochastic gradient descent
on the parametric loss (12). This theoretical framework provides strong convergence guarantees for α-DSBM
and paves the way to further theoretical developments. Furthermore, De Bortoli et al. (2024) demonstrate
the method’s effectiveness through extensive numerical experiments on high-dimensional computer vision
problems, complementing our synthetic benchmark results.
I-BM and DIPF : closely related to BM2are the iterative, sample-based DIPF (Bortoli et al., 2021; Vargas
et al., 2021) and I-BM (Shi et al., 2023; Peluchetti, 2023) procedures, which do not satisfy desiderata (i).
Built on similar bridge matching principles, BM2can be viewed as a modification of I-BM that employs a
single optimization loop, resulting in a simpler algorithm that we have empirically shown to be competitive.
12Published in Transactions on Machine Learning Research (12/2024)
Forward-Backward SB SDE : Chen et al. (2022) propose two training algorithms addressing the dynamic
SB problem. Both approaches employ loss functions that require divergence computations (violating desiderata
(iv)) and the use of two distinct neural networks. The first method is iterative, resembling DIPF (violating
desiderata (i)), while the second method involves differentiating through entire discretized paths, resulting in
high memory consumption (violating desiderata (iii)).
The subsequent works concentrate on solving the staticSchrödinger bridge (2), or EOT (3), problem. Once
this is achieved, solutions to the dynamic problem are trivially obtained through the standard decomposition
S=S0,1R|0,1. Although these works differ in nature and objectives, we include them here due to their shared
characteristic with BM2: the non-iterative nature of the algorithm.
Light SB : in two notable works, Korotin et al. (2023) and Gushchin et al. (2024) propose non-iterative,
sample-based EOT solvers for the Euclidean cost function, i.e., for the specific choice of reference dynamics
(R). Korotin et al. (2023) introduces an approximation to (an adjusted version of) the Schrödinger potential
forΨ1via a mixture of Gaussian distributions, resulting in a mixture of Gaussian distributions approximation
toS1|0. Gushchin et al. (2024) builds upon this approximation and introduces an additional sample-based
training objective that takes as input any coupling C0,1∈C(Ψ0,Ψ1), whereas Korotin et al. (2023) requires
the independent coupling Ψ0⊗Ψ0. While also non-iterative, the proposals of Korotin et al. (2023); Gushchin
et al. (2024) differ from BM2in two key aspects: (a) they learn a solution in the static setting instead of the
dynamic one, and (b) they employ mixture of Gaussian distributions approximations, rather than neural
network approximators for the drift functions. Consequently, these methods may face challenges in scaling to
modern generative ML applications. Light SB, in both variants, demonstrates strong performance in the
benchmark presented in Section 5: the results of (Gushchin et al., 2024, Table 1) and (Korotin et al., 2023,
Table 2) are directly comparable with the results of Table 2. However, it is worth noting that this benchmark
is particularly well-suited for Light SB, as acknowledged by its authors, since each benchmark target S0,1is
constructed such that S1|0is itself a mixture of 5 Gaussian distributions.
7 Conclusions
In this work we introduced Coupled Bridge Matching (BM2), a novel approach for learning Schrödinger
bridges from samples. BM2builds on the principles of Bridge Matching while addressing key limitations of
existing iterative methods. Our approach offers several advantages, including a simple single-loop optimization
procedure, exactness in the idealized setting, modest memory requirements, and a straightforward loss function.
The numerical experiments demonstrate that BM2is competitive with and often outperforms existing iterative
diffusion-based methods like I-BM and DIPF across various dimensions and entropic regularization settings.
On the theoretical front, there is substantial room for improvement. Firstly, while bearing some resemblance to
the standard convergence result for the EM algorithm, Theorem 2 lacks a quantity analogous to the likelihood
being maximized in the EM algorithm. It remains unclear whether decreases in KL(MB0,1(θ′)∥F(θ))and
KL(MF0,1(θ′)∥B(θ))can be linked to decreases in KL(F(θ)∥S)andKL(B(θ)∥S). Secondly, the requirement
thatF(θ) =B(θ), equivalently that ( F(θ)) and (← −B(θ)) are time-reversals of each other, appears unnecessary.
Notably, all numerical simulations conducted do not explicitly enforce this condition, which emerges naturally
during the training process. Thirdly, it would be valuable to study problem (16) where reverse KL divergences
are partially minimized, aligning more closely with the BM2algorithm. In this scenario, Lemma 4 no
longer holds, and it may be necessary to impose a corresponding additional constraint to maintain tractable
analytical computations. The attractors of (19), and of a corresponding dynamical system arising from
reverse KL minimization, can be investigated to assess further convergence properties of BM2.
On the empirical front, the applications of BM2in contemporary generative machine learning tasks remain
unexplored. Given the promising results from previous studies employing Bridge Matching, such as those by
Liu et al. (2023) and Somnath et al. (2023), it is anticipated that BM2could be effectively applied to various
domains, including image generation, audio synthesis, and molecular design. Future work could investigate
the scalability and performance of BM2in these domains.
13Published in Transactions on Machine Learning Research (12/2024)
References
Brian D.O. Anderson. Reverse-time diffusion equation models. Stochastic Processes and their Applications ,
12(3):313–326, 1982. ISSN 03044149.
Valentin De Bortoli, James Thornton, Jeremy Heng, and Arnaud Doucet. Diffusion Schrödinger Bridge
with Applications to Score-Based Generative Modeling. In Thirty-Fifth Conference on Neural Information
Processing Systems , 2021.
Tianrong Chen, Guan-Horng Liu, and Evangelos Theodorou. Likelihood Training of Schrödinger Bridge
using Forward-Backward SDEs Theory. In International Conference on Learning Representations , 2022.
Valentin De Bortoli, Iryna Korshunova, Andriy Mnih, and Arnaud Doucet. Schrödinger Bridge Flow for
Unpaired Data Translation, September 2024.
A. P. Dempster, N. M. Laird, and D. B. Rubin. Maximum Likelihood from Incomplete Data Via the EM
Algorithm. Journal of the Royal Statistical Society: Series B (Methodological) , 39(1):1–22, 1977. ISSN
0035-9246.
D. C Dowson and B. V Landau. The Fréchet distance between multivariate normal distributions. Journal of
Multivariate Analysis , 12(3):450–455, 1982. ISSN 0047-259X.
Robert Fortet. Résolution d’un systeme d’équations de M. Schrödinger. J. Math. Pure Appl. IX , 1:83–105,
1940.
Nikita Gushchin, Alexander Kolesov, Petr Mokrov, Polina Karpikova, Andrei Spiridonov, Evgeny Burnaev,
and Alexander Korotin. Building the Bridge of Schrödinger: A Continuous Entropic Optimal Transport
Benchmark. In Thirty-Seventh Conference on Neural Information Processing Systems Datasets and
Benchmarks Track , 2023.
Nikita Gushchin, Sergei Kholkin, Evgeny Burnaev, and Alexander Korotin. Light and Optimal Schrödinger
Bridge Matching. In Forty-First International Conference on Machine Learning , 2024.
Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising Diffusion Probabilistic Models. arXiv, 2020.
Benton Jamison. Reciprocal processes. Zeitschrift für Wahrscheinlichkeitstheorie und Verwandte Gebiete , 30
(1):65–86, 1974. ISSN 1432-2064.
Benton Jamison. The Markov processes of Schrödinger. Zeitschrift für Wahrscheinlichkeitstheorie und
Verwandte Gebiete , 32(4):323–331, 1975. ISSN 1432-2064.
Mohammad Reza Karimi, Ya-Ping Hsieh, and Andreas Krause. Sinkhorn Flow: A Continuous-Time
Framework for Understanding and Generalizing the Sinkhorn Algorithm, 2023.
Peter E. Kloeden and Eckhard Platen. Numerical Solution of Stochastic Differential Equations . Springer
Berlin Heidelberg, 1992.
Alexander Korotin, Nikita Gushchin, and Evgeny Burnaev. Light Schrödinger Bridge. In The Twelfth
International Conference on Learning Representations , 2023.
Guan-Horng Liu, Arash Vahdat, De-An Huang, Evangelos Theodorou, Weili Nie, and Anima Anandkumar.
I2SB: Image-to-Image Schrödinger Bridge. In Proceedings of the 40th International Conference on Machine
Learning , pp. 22042–22062. PMLR, 2023.
Xingchao Liu, Lemeng Wu, Mao Ye, and Qiang Liu. Let us Build Bridges: Understanding and Extending
Diffusion Generative Models. In NeurIPS 2022 Workshop on Score-Based Methods , 2022.
Christian Léonard. Some Properties of Path Measures. In Catherine Donati-Martin, Antoine Lejay, and Alain
Rouault (eds.), Séminaire de Probabilités XLVI , Lecture Notes in Mathematics, pp. 207–230. Springer
International Publishing, 2014a.
14Published in Transactions on Machine Learning Research (12/2024)
Christian Léonard. A survey of the Schrödinger problem and some of its connections with optimal transport.
Discrete & Continuous Dynamical Systems , 34(4):1533, 2014b.
Anton Mallasto, Augusto Gerolin, and Hà Quang Minh. Entropy-regularized 2-Wasserstein distance between
Gaussian measures. Information Geometry , 5(1):289–323, 2022. ISSN 2511-249X.
Youssef Marzouk, Tarek Moselhy, Matthew Parno, and Alessio Spantini. Sampling via Measure Transport:
An Introduction. In Roger Ghanem, David Higdon, and Houman Owhadi (eds.), Handbook of Uncertainty
Quantification , pp. 1–41. Springer International Publishing, 2016.
Stefano Peluchetti. Non-Denoising Forward-Time Diffusions. 2021.
Stefano Peluchetti. Diffusion Bridge Mixture Transports, Schrödinger Bridge Problems and Generative
Modeling. Journal of Machine Learning Research , 24(374):1–51, 2023. ISSN 1533-7928.
Gabriel Peyré and Marco Cuturi. Computational Optimal Transport. 2020.
Ludger Ruschendorf. Convergence of the Iterative Proportional Fitting Procedure. The Annals of Statistics ,
23(4):1160–1174, 1995. ISSN 0090-5364.
Yuyang Shi, Valentin De Bortoli, Andrew Campbell, and Arnaud Doucet. Diffusion Schrödinger Bridge
Matching. In Thirty-Seventh Conference on Neural Information Processing Systems , 2023.
Vignesh Ram Somnath, Matteo Pariset, Ya-Ping Hsieh, Maria Rodriguez Martinez, Andreas Krause, and
Charlotte Bunne. Aligned diffusion Schrödinger bridges. In Robin J. Evans and Ilya Shpitser (eds.),
Proceedings of the Thirty-Ninth Conference on Uncertainty in Artificial Intelligence , volume 216 of
Proceedings of Machine Learning Research , pp. 1985–1995. PMLR, 31 Jul–04 Aug 2023.
Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole.
Score-Based Generative Modeling through Stochastic Differential Equations. In International Conference
on Learning Representations , 2021.
Francisco Vargas, Pierre Thodoroff, Austen Lamacraft, and Neil Lawrence. Solving Schrödinger Bridges via
Maximum Likelihood. Entropy, 23(9):1134, 2021.
B. K. Øksendal. Stochastic Differential Equations: An Introduction with Applications . Universitext. Springer,
6th ed., 6th corrected printing edition, 2013.
15Published in Transactions on Machine Learning Research (12/2024)
A Additional Dynamics
In this section we consider a simple extension to the dynamics of Section 2.2, and refer the reader to Peluchetti
(2021; 2023) for the more general case. Here, we consider the case where the reference distribution Ris given
by the solution to:
X0∼Ψ0, dXt=σ/radicalbig
βtdWt, t∈[0,1], (22)
withσ≥0,βt: [0,1]→R>0strictly positive and continuous. With bs:t:=/integraltextt
sβudu,0≤s≤t≤1,βtis
chosen such that b0:1= 1, to disentangle the contribution of βtfrom the contribution of σ. Indeed, under
these conditions, βtdefines a time-warping: if Xtis the solution to ( R), thenXb0:thas the same distribution
as the solution to (22). Consequently, the solutions to (2) and (3) are independent of βt.
When employing (22), the definitions in Section 2.2 are replaced as follows:
Rt|0(·|x0) =N(x0,σ2b0:t), (23)
Rt|0,1(·|x0,x1) =N(x0bt:1+x1b0:t,σ2b0:tbt:1), (24)
µ01(xt,t,x 1):=σ2βt∇xtlogr1|t(x1|xt) =βt
bt:1(x1−xt), (25)
υ01(xt,t,x 0):=σ2βt∇xtlogrt|0(xt|x0) =βt
b0:t(x0−xt), (26)
γ01(xt,t,x 0,x1):=σ2βt∇xtlogrt|01(xt|x0,x1) =βt
b0:tbt:1(x0bt:1+x1b0:t−xt). (27)
B Proofs
Theorem 1 (Complete BM2Iterations) .Consider the SDEs ( F(θ),← −B(θ)), with initial drifts µ(0)
f,υ(0)
band
corresponding distributions F(0),B(0). For each i≥1, let (µ(i)
f,υ(i)
b) =arg min(µ,υ)L(µ,υ;µ(i−1)
f,υ(i−1)
b),
resulting in the distribution iterates F(i),B(i). We distinguish two cases:
(i)µ(0)
f=υ(0)
b= 0: both the iterates F(0),B(1),F(2),...and the iterates B(0),F(1),B(2),...are equiva-
lent to the DIPF iterates, started respectively from the forward and from the backward time direction;
(ii)µ(0)
f=µC0,1m,υ(0)
b=υC0,1mfor someC∈C(Ψ0,Ψ1):F(i)=B(i)=I(i)for eachi≥0whereI(i)are
the I-BM iterates.
Proof.DefineQassociated with
X1∼Ψ1, dXt=σdWt, t∈[1,0], (← −Q)
which is not the time reversal of ( R), butR|0,1=Q|0,1.
Firstly, consider the case of initial null drifts: µ(0)
f=υ(0)
b= 0, corresponding to F(0)= Ψ 0R|0= Ψ 0R1|0R|0,1=
F(0)
0,1R|0,1∈SandB(0)= Ψ 1Q|1= Ψ 1Q0|1R|0,1=B(0)
0,1R|0,1∈S. AsB(0)= ΠB(0)
0,1=MB(0)
0,1, we have
F(1)= Ψ 0MB(0)
0,1
|0= Ψ 0B(0)
|0= Ψ 0B(0)
1|0R|0,1∈S. AsF(0)= ΠF(0)
0,1=MF(0)
0,1,B(1)= Ψ 1MF(0)
0,1
|1= Ψ 1F(0)
|1=
Ψ1F(0)
0|1R|0,1∈S. By induction, F(i)= Ψ 0B(i−1)
1|0R|0,1∈SandB(i)= Ψ 1F(i−1)
0|1R|0,1∈S,i≥1. We now
construct two forward-backward sequences. For the sequence F(0),B(1),F(2),..., we haveF(0)
0,1= Ψ 0R1|0,
B(1)
0,1= Ψ 1F(0)
0|1,F(2)
0,1= Ψ 0B(1)
1|0, ... which are the static IPF iterates: one marginal gets replaced at a time
keeping the conditional distribution fixed. In the same way, for B(0),F(1),B(2),..., we haveB(0)
0,1= Ψ 1Q0|1,
F(1)
0,1= Ψ 0B(0)
1|0,B(2)
0,1= Ψ 1F(1)
0|1, ... which are again the static IPF iterates (for the backward formulation of
the dynamic SB problem, i.e. via← −Q|0as reference measure instead of R|0, and switched marginal distributions).
16Published in Transactions on Machine Learning Research (12/2024)
As each pair F(i),B(i)is of the form F(i)=F(i)
0,1R|0,1,B(i)=B(i)
0,1R|0,1, we also recover the dynamic DIPF
iterates.
Secondly, consider µ(0)
fandυ(0)
bboth corresponding to the BM transport based on the given coupling:
I(0)=MC0,1,F(0)=B(0)=I(0). Then, looking separately at either of the sequences F(i),i≥1, andB(i),
i≥1, we obtain that F(i)=B(i)=I(i),i≥1.
Lemma 2 (Loss Interpretation) .It holds that
KL(B0∥Ψ0) +Lf(µf;υb) =KL(ΠB0,1∥F) +C1(B) =KL(MB0,1∥F) +C2(B),
KL(F1∥Ψ1) +Lb(υb;µf) =KL(ΠF0,1∥B) +D1(F) =KL(MF0,1∥B) +D2(F),(15)
forC1(B),C2(B)independent of F,D1(F),D2(F)independent of B, with 0≤C1(B)≤C2(B)and0≤
D1(F)≤D2(F).
Proof.We consider only Lf(µf;υb), the arguments for Lb(υb;µf)are symmetric. By Girsanov Theorem
(Øksendal, 2013) and by the marginal-conditional decomposition of Kullback-Leibler divergences we have
KL(MB0,1∥F) =KL(B0∥Ψ0) +E
ΠB0,1/bracketleftig1
2/integraldisplay1
0∥µf(Xt,t)−µB0,1
m(Xt,t)∥2dt/bracketrightig
,
KL(ΠB0,1∥F) =KL(B0∥Ψ0) +E
ΠB0,1/bracketleftig1
2/integraldisplay1
0∥µf(Xt,t)−µB0,1
π(Xt,t,X 0)∥2dt/bracketrightig
,
KL(ΠB0,1∥F) =KL(B0∥Ψ0) +E
ΠB0,1/bracketleftig1
2/integraldisplay1
0∥µf(Xt,t)−µ01(Xt,t,X 1)∥2dt/bracketrightig
=KL(B0∥Ψ0) +Lf(µf;υb),
whereµB0,1π(Xt,t,X 0):=EΠB0,1[µ01(Xt,t,X 1)|Xt,X0],µB0,1m(Xt,t):=EΠB0,1[µ01(Xt,t,X 1)|Xt], andFis
distribution of the non-Markov diffusion solution to the auxiliary SDE
X0∼Ψ0, dXt= [µf(Xt,t)−µ01(Xt,t,X 1) +µB0,1
π(Xt,t,X 0)]dt+σdWt, t∈[0,1].(F)
By the tower property of conditional expectations and by the conditional Jensen inequality it follows that
KL(ΠB0,1∥F)−KL(ΠB0,1∥F)
=E
ΠB0,1/bracketleftig1
2/integraldisplay1
0∥µf(Xt,t)−µ01(Xt,t,X 1)∥2−∥µf(Xt,t)−µB0,1
π(Xt,t,X 0)∥2dt/bracketrightig
=E
ΠB0,1/bracketleftig1
2/integraldisplay1
0∥µ01(Xt,t,X 1)∥2−∥µB0,1
π(Xt,t,X 0)∥2dt/bracketrightig
=C1(B)≥0.
By the Pythagorean property of the BM transport (Liu et al., 2022; Peluchetti, 2023)
KL(ΠB0,1∥F)−KL(MB0,1∥F) =KL(ΠB0,1∥MB0,1) =K(B)≥0.
TakingC2(B) =C1(B) +K(B)completes the proof.
Lemma 3 (R-stability of F(λ),B(λ)).IfF′,B′∈R, thenF(λ),B(λ)∈Rfor eachλ∈[0,1].
Proof.By the marginal-conditional decomposition of Kullback-Leibler divergences
KL(F∥B′) =KL(F0,1∥B′
0,1) +E
F0,1[KL(F|0,1∥B′
|0,1)],
KL(F∥F′) =KL(F0,1∥F′
0,1) +E
F0,1[KL(F|0,1∥F′
|0,1)],
17Published in Transactions on Machine Learning Research (12/2024)
andB′
|0,1=F′
|0,1=R|0,1, hence
F(λ):= arg min
F∈P(Ψ0,·)λKL(F0,1∥B′
0,1) + (1−λ)KL(F0,1∥F′
0,1) +E
F0,1[KL(F|0,1∥R|0,1)],
B(λ):= arg min
B∈P(·,Ψ1)λKL(B0,1∥F′
0,1) + (1−λ)KL(B0,1∥B′
0,1) +E
B0,1[KL(B|0,1∥R|0,1)],
and thusF(λ)
|0,1=B(λ)
|0,1=R|0,1, which completes the proof.
Lemma 4 (S-stability of F(λ),B(λ)).IfF′,B′∈S, thenF(λ),B(λ)∈Sfor eachλ∈[0,1].
Proof.In view of Lemma 3, we have to verify that F(λ)
0,1,B(λ)
0,1solve the EOT problems (3) for some marginal
distributions if F′
0,1,B′
0,1do. For simplicity, we assume that all of F(λ)
0,1,B(λ)
0,1,F′
0,1,B′
0,1admits positive
densities on Rd×d, and that Ψ0andΨ1admits positive densities on Rd. The steps of this proof carry over to
the more general measure-theoretic setting.
We know that f(λ)
0,1(x0,x1) =ψ0(x0)f(λ)
1|0(x1|x0)andb(λ)
0,1(x0,x1) =b(λ)
0|1(x0|x1)ψ1(x1), wheref(λ)
1|0(x1|x0)∝
b′
1|0(x1|x0)λf′
1|0(x1|x0)1−λandb(λ)
0|1(x0|x1)∝f′
0|1(x0|x1)λb′
0|1(x0|x1)1−λ(see Section 4.2). On the other hand
f′
0,1(x0,x1) = exp/braceleftig
ϕf′
0(x0) +ϕf′
1(x1)−κ(x0,x1)
ε/bracerightig
,
b′
0,1(x0,x1) = exp/braceleftig
ϕb′
0(x0) +ϕb′
1(x1)−κ(x0,x1)
ε/bracerightig
,
for the Schrödinger potentials2ϕf′
0(x0),ϕf′
1(x1)andϕb′
0(x0) +ϕb′
1(x1)(Léonard, 2014a). It follows by direct
computation that f(λ)
0,1(x0,x1)andb(λ)
0,1(x0,x1)satisfy:
f(λ)
0,1(x0,x1) = exp/braceleftig
ϕf,λ
0(x0) +ϕf,λ
1(x1)−κ(x0,x1)
ε/bracerightig
,
b(λ)
0,1(x0,x1) = exp/braceleftig
ϕb,λ
0(x0) +ϕf,λ
1(x1)−κ(x0,x1)
ε/bracerightig
,
for some other Schrödinger potentials ϕf,λ
0(x0),ϕf,λ
1(x1)andϕb,λ
0(x0),ϕf,λ
1(x1).
C Additional Results
C.1 Infinitesimal Minimization, Gaussian Case
Consider the one-dimensional case d= 1, with target Gaussian marginal distributions Ψ0=N(µ0,σ2
0)and
Ψ1=N(µ1,σ2
1), and a reference diffusion distribution Rassociated with ( R). In this setting, the solution
to the static Schrödinger bridge problem (2) is known analytically and is given by a bivariate Gaussian
distribution (Mallasto et al., 2022).
We hypothesize that conditional Gaussian densities for f(l)
1|0(x1|x0)andb(l)
0|1(x0|x1)solve (19). Specifically, we
proposeF(l)
1|0=N(Af
lx0+af
l,vf
l)andB(l)
0|1=N(Ab
lx1+ab
l,vb
l), whereAf
l,af
l,Ab
l,ab
l∈Randvf
l,vb
l∈R>0
are algorithmic-time dependent scalar parameters. By construction, F(l)
0=N(µ0,σ2
0)andB(l)
1=N(µ1,σ2
1)
for eachl≥0. Substituting these expressions for f(l)
1|0(x1|x0)andb(l)
0|1(x0|x1)into (19) yields a six-dimensional
ODE system in the parameters. The initial conditions are Ab
0=Af
0= 1,af
0=ab
0= 0,vf
0=vb
0=σ2,
corresponding to initial null drift terms for ( F(θ),← −B(θ)), as discussed in Section 4.2.
To numerically solve the ODE and determine the values of Af
l,af
l,vf
l,Ab
l,ab
l,vb
loverl∈[0,L]for someL>0,
we evaluate (19) for three different pairs of (x0,x1). This provides sufficient constraints to identify the
2We formulate the potential with respect to the Lebesgue measure on Rd.
18Published in Transactions on Machine Learning Research (12/2024)
0 5 10 15 20-2-1012
Figure 2: Algorithmic-time levolution of EF(l)[X1],VF(l)[X1],CF(l)[X0,X1], compared with ES[X1],VS[X1],
CS[X0,X1]as dashed gray lines.
parameters. Subsequently, we verify that the proposed functional forms for f(l)
1|0(x1|x0)andb(l)
0|1(x0|x1)indeed
solve (19).
We examine the scenario where µ0=−2,µ1= 2,σ0=σ1=σ= 1. Figure 2 illustrates the evolution of
EF(l)[X1],VF(l)[X1], and CF(l)[X0,X1]over algorithmic time l. These quantities represent the mean and
variance of X1and the covariance between X0andX1according to F(l), respectively. The corresponding
values ES[X1],VS[X1], andCS[X0,X1]for the static Schrödinger bridge solution S0,1from Mallasto et al.
(2022) are depicted as dashed gray lines, demonstrating convergence.
C.2 Results for Additional Discretization Intervals
InTable3wereporttheresultsformetric(21)obtainedbyconsideringtheBM2, I-BMandDIPFmethodologies
for different discretization intervals ∆t= 1/TwhereTis the number of time-steps. For all methods we
employ the same number of time-steps at training and inference (testing) time. We recall that in Section 5
200time-steps have been employed to produce the results of Tables 1 and 2, and that we rely exclusively on
the Euler–Maruyama discretization scheme.
ε=0.1 ε=1 ε=10
Method d=2d=16d=64d=128d=2d=16d=64d=128d=2d=16d=64d=128
SB(100) 0.08
0.000.10
0.000.37
0.000.94
0.000.10
0.000.19
0.000.49
0.001.25
0.000.12
0.002.96
0.0014.86
0.0228.90
0.03
SB(50) 0.23
0.000.22
0.000.44
0.001.00
0.000.15
0.000.24
0.000.61
0.001.49
0.000.13
0.007.24
0.0137.24
0.0472.34
0.06
BM2(100) 1.17
0.795.08
0.227.12
0.717.94
0.780.14
0.040.41
0.031.71
0.148.91
0.350.15
0.024.06
0.0355.10
1.13295.82
10.60
BM2(50) 0.80
0.524.59
0.416.46
0.878.47
0.730.21
0.090.48
0.031.85
0.109.57
0.270.16
0.018.17
0.0478.76
1.85202.22
41.38
DIPF(100) 7.28
4.1915.72
1.3116.44
0.8926.69
0.711.43
0.455.96
0.3211.98
0.8222.60
0.861.14
0.077.36
0.1685.42
2.64212.19
1.19
DIPF(50) 6.90
3.0112.37
1.4014.98
2.1626.91
2.711.44
0.335.09
0.4310.73
0.2920.20
0.932.18
0.0611.32
0.1875.80
1.02226.81
0.92
I-BM(100) 0.98
0.224.54
0.986.35
0.8413.60
1.230.18
0.040.51
0.032.09
0.237.30
1.060.15
0.025.95
0.10113.91
0.93494.32
3.99
I-BM(50) 1.58
0.614.21
0.206.35
1.0113.75
2.930.28
0.050.60
0.041.96
0.257.31
1.240.16
0.029.14
0.07114.40
0.89432.20
5.82
Table 3: Results analogous to Table 2 but for varying discretization intervals ∆t= 1/T, where the number of
time-stepsTis indicated in parentheses after each method name.
19Published in Transactions on Machine Learning Research (12/2024)
D Python Code
1# dimensions: B: batch; D: data; T: time_steps + 1
2# required: sample_0(batch_dim, device), sample_1(batch_dim, device), fwd_drift_fn(x, t), bwd_drift_fn(x, t)
3import torch as th
4
5# sampling from Rt|0,1(5): (B, D), (B, D), (B,), () -> (B, D)
6def sample_bridge (x_0, x_1, t, sigma):
7B, D = x_0.shape
8mean_t = ( 1- t[..., None]) * x_0 + t[..., None] * x_1 # (B, D)
9var_t = sigma** 2* t[..., None] * ( 1- t[..., None]) # (B, D)
10z_t = th.randn_like(x_0) # (B, D)
11x_t = mean_t + th.sqrt(var_t) * z_t # (B, D)
12 returnx_t
13
14# fwd BM target (6): (B, D), (B, D), (B,) -> (B, D)
15def fwd_target (x_t, x_1, t):
16 return(x_1 - x_t) / ( 1- t[..., None]) # (B, D)
17
18# fwd BM target (7): (B, D), (B, D), (B,) -> (B, D)
19def bwd_target (x_t, x_0, t):
20 return(x_0 - x_t) / t[..., None] # (B, D)
21
22# Euler–Maruyama discretization scheme: fn(x, t), (B, D), (T), () -> (B, D)
23def discretization (drift_fn, initial_value, times, sigma):
24B, D = initial_value.shape
25times = times[..., None].expand(- 1, B) # (T, B)
26x_prev_t = initial_value # (B, D)
27 forprev_t, t in zip(times[:- 1], times[ 1:]): # (B), (B)
28 dt = t - prev_t # (B)
29 drift_t = drift_fn(x_prev_t, prev_t) # (B, D)
30 drift_part_t = drift_t * dt[..., None] # (B, D)
31 eps_t = th.randn_like(x_prev_t) # (B, D)
32 diffusion_part_t = (sigma * th.sqrt(th.abs(dt)))[..., None] * eps_t # (B, D)
33 x_t = x_prev_t + drift_part_t + diffusion_part_t # (B, D)
34 x_prev_t = x_t # (B, D)
35 returnx_t
36
37# BM2loss computation: fn(b, d), fn(b, d), fn(x, t), fn(x, t), (), (), (), () -> ()
38def sample_loss (sample_0, sample_1, fwd_drift_fn, bwd_drift_fn, batch_dim, time_steps, sigma, device):
39# sample from the target marginals:
40f_0 = sample_0(batch_dim, device) # (B, D)
41b_1 = sample_1(batch_dim, device) # (B, D)
42# sample according to current ( F(θ)) and (← −B(θ)):
43fwd_times = th.linspace( 0.0,1.0, time_steps + 1, device=device) # [0, 1/time_steps, ..., 1]
44bwd_times = th.linspace( 1.0,0.0, time_steps + 1, device=device) # [1, ..., 1/time_steps, 0]
45f_1 = discretization(fwd_drift_fn, f_0, fwd_times, sigma).detach() # (B, D)
46b_0 = discretization(bwd_drift_fn, b_1, bwd_times, sigma).detach() # (B, D)
47# sample time and mixture processes based on F0,1(θ)andB0,1(θ):
48t = th.rand((batch_dim,), device=device) # (B)
49pi_f_t = sample_bridge(f_0, f_1, t, sigma) # (B, D)
50pi_b_t = sample_bridge(b_0, b_1, t, sigma) # (B, D)
51# define regression targets and model predictions:
52target_f_t = fwd_target(pi_b_t, b_1, t) # (B, D)
53target_b_t = bwd_target(pi_f_t, f_0, t) # (B, D)
54prediction_f_t = fwd_drift_fn(pi_b_t, t) # (B, D)
55prediction_b_t = bwd_drift_fn(pi_f_t, t) # (B, D)
56# compute loss:
57loss_f_t = th.sum((target_f_t - prediction_f_t)** 2, dim= 1) / 2# (B)
58loss_b_t = th.sum((target_b_t - prediction_b_t)** 2, dim= 1) / 2# (B)
59loss_t = th.mean(loss_f_t + loss_b_t) # ()
60 returnloss_t
Listing 1: Basic implementation of BM2loss computation (Algorithm 1) in PyTorch.
20