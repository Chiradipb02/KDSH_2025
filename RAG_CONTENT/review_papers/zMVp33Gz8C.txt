Under review as submission to TMLR
On Equivalences between Weight and Function-Space
Langevin Dynamics
Anonymous authors
Paper under double-blind review
Abstract
Approximate inference for overparameterized Bayesian models appears challenging, due to
the complex structure of the posterior. To address this issue, a recent line of work has
investigated the possibility of directly conducting approximate inference in the “function
space”, the space of prediction functions. This paper provides an alternative perspective to
thisproblem, byshowingthatformanymodels–includingasimpliﬁedneuralnetworkmodel
– Langevin dynamics in the overparameterized “weight space” induces equivalent function-
space trajectories to certain Langevin dynamics procedures in function space. Thus, the
former can already be viewed as a function-space inference algorithm, with its convergence
unaﬀected by overparameterization. We provide simulations on Bayesian neural network
models and discuss the implication of the results.
1 Introduction
Consider a common Bayesian predictive modeling setting: we are provided with i.i.d. observations D:=
{(xi,yi)}n
i=1wherexi∈X,yi∈RandXdenotes the input space; a likelihood model p({yi}|{xi},θ) =/producttextn
i=1p(yi|f(xi;θ))determined by a prediction function f(·;θ); and a prior πθ(dθ). We are interested in
the predictive distribution p(y∗|x∗,D) =/integraltext
πθ|D(dθ)p(y∗|x∗,θ), induced by the posterior πθ|D.
Modern machine learning models are often overparameterized, meaning that multiple parameters may deﬁne
the same likelihood. For example, in Bayesian neural network (BNN) models where θ∈Rddenote the
networkweights, we can obtain a combinatorial number of equivalent parameters by reordering the neurons,
after which f(·;θ), and thus the likelihood, remain unchanged. Consequently, the posterior measure exhibits
complex structures and becomes hard to approximate; for example, its Lebesgue density may contain a large
number of global maxima.
Starting from Sun et al. (2019); Wang et al. (2019); Ma et al. (2019), a recent literature investigates the
possibility of simplifying inference by approximating a function-space posterior . Concretely, let A:Rd→
F⊂RX,θ/mapsto→f(·;θ)denote a “parameterization map”. Then
p(y∗|x∗,D) =/integraldisplay
πθ|D(dθ)p(y∗|f(x∗;θ)) =/integraldisplay
(A#πθ|D)(df)p(y∗|f(x∗)) =/integraldisplay
πf|D(df)p(y∗|f(x∗)),
whereA#(·)refers to the pushforward measure (Villani, 2009, p. 11), and πf|Ddenotes the function-space
posterior deﬁned by the prior A#πθ=:πfand likelihood p(y|x,f) =p(y|f(x)). As shown above, πf|Dis
suﬃcient for prediction. Moreover, it often has simpler structures: for example, for ultrawide BNN models
with a Gaussian πθ,πfmay converge to a Gaussian process (GP) prior (Lee et al., 2018; Matthews et al.,
2018; Yang, 2019), in which case πf|Dwill also converge to a GP posterior. Thus, it is natural to expect
approximate inference to be easier in function space.
While the intuition has been appealing, existing works on function-space inference tend to be limited by the-
oretical issues: principled applications may require full-batch training (Sun et al., 2019), Gaussian likelihood
(Shi et al., 2019), or speciﬁcally constructed models (Ma et al., 2019; Ma & Hernández-Lobato, 2021). Many
approaches rely on approximations to the function-space prior, which can make the functional KL divergence
1Under review as submission to TMLR
unbounded(Burtetal.,2020). Additionally, thereisalackofunderstandingaboutoptimizationconvergence,
or the expressivity of the variational families used. In contrast, gradient-based MCMC methods, such as
Hamiltonian Monte Carlo (HMC) or Langevin dynamics (LD)-based algorithms, can be applied to a broad
range of models. Their convergence behaviors are well-understood (Roberts & Tweedie, 1996; Villani, 2009),
and intriguingly, their performance often appears to be satisfying on massively overparameterized models
(Zhang et al., 2019; Izmailov et al., 2021), even though they are implemented in weight space.
This paper bridges the two lines of approaches by showing that
•In various overparameterized models, including two simpliﬁed BNN models (Sec. 2.1 and Ex. 2.3), weight-
space Langevin dynamics (LD) is equivalent to a reﬂected / Riemannian LD procedure in function space,
deﬁned by the pushforward metric.
•For practical feed-forward network models, a possible consequence of the equivalence still appears to hold
in simulations (Sec. 3): weight-space LD produces predictive distributions that appears to approach the
functional posterior, at a rate that does not depend on the degree of overparameterization.
Theequivalencehasimportantimplications: itmeansthat principled function-space inference has always been
possible and in use . Thus, explicit consideration of function-space posteriors alonewill not be suﬃcient to
guarantee improvement over existing approaches, and more careful analyses are necessary to justify possible
improvement. We also discuss how further insights into the behavior of weight-space LD could be gained by
comparing the pushforward metric with the prior (Sec. 2.2).
It should be noted that in several scenarios, it has been established that overparameterization does not nec-
essarily hinder the convergence of LD. Moitra & Risteski (2020) proves that polynomial convergence can be
possible for a family of locallyoverparameterized models, despite the non-convexity introduced by the over-
parameterization.1Dimensionality-independent convergence has also been established for inﬁnite-width NNs
in the mean-ﬁeld regime (e.g., Mei et al., 2019), even though its implication for practical, ﬁnite-width models
is less clear. More broadly, at a high level our work is also related to past works that studied alternative
inference schemes for diﬀerent models that exhibit some redundancy in the parameterization (Papaspiliopou-
los et al., 2007; Yu & Meng, 2011). We are unaware of strict equivalence results as provided in this paper,
but we should also emphasize that it is not their technical sophistication that makes them interesting; it is
rathertheir implications for BNN inference, which appear underappreciated : the results justify the use of LD
as an eﬀective function space inference procedure, in settings that match or generalize previous work. For
example, Example 2.1 covers overparameterized linear models, and many popular approaches (e.g., Osband
et al., 2018; He et al., 2020) are only justiﬁed in this setting.
Our results contribute to the understanding of the real-world performance of BNN models, as they provide
a theoretical support for the hypothesis that inference may be good enough in many applications, and is not
necessarily the limiting factor in a predictive modeling workﬂow. In this aspect, our results complement a
long line of existing work which examined the inﬂuence of likelihood, prior and data augmentation in BNN
applications, with an emphasis on classiﬁcation tasks with clean labels; see Aitchison (2020); Wenzel et al.
(2020); Fortuin et al. (2021), to name a few.
2 Equivalence between Weight and Function-Space Langevin Dynamics
Suppose the prior measure πθis supported on an open subset of Rdand has Lebesgue density pθ. The weight-
space posterior πθ|Dcan be recovered as the stationary measure of the (weight-space) Langevin dynamics
dθt=∇θ(logp(Y|θt,X) + logpθ(θt))dt+√
2dBt, (WLD)
where we write X:={xi}n
i=1,Y:={yi}n
i=1for brevity.
1This result is still not fully unimpeded by overparameterization, as it quantiﬁes convergence to the weight-space posterior,
which necessarily requires traversal through all symmetric regions.
2Under review as submission to TMLR
The pushforward measure A#πθ=:πfprovides a prior in function space. Combining πfand the likelihood
leadstoaposterior, πf|D. Whenthefunctionspace F:= suppπfisaRiemannianmanifold2ofdimensionality
k≤d, it is intuitive that we could sample from πf|Dby simulating a Riemannian Langevin dynamics on
F(Girolami & Calderhead, 2011). In coordinate form:
d˜ft=V(˜ft)dt+/radicalBig
2G−1(˜ft)dBt, (FLD)
where ˜ft∈Rkis the coordinate of ft∈F,G−1(˜f) = (gij)i,j∈[k]is the inverse of the metric matrix and gij
are the local representations for the metric (Lee, 2018, p. 13), dBtis the standard Brownian motion, and
Vi(˜f) =k/summationdisplay
j=1gij∂j/parenleftBig
logp(Y|˜f,X) + logdπf
dµF(˜f)−log|G|
2/parenrightBig
+k/summationdisplay
j=1∂jgij.
In the above, µFdenotes the corresponding Riemannian measure (Do Carmo, 1992, p. 45), and p(Y|
˜f,X) :=p(Y|f(X))denotes the likelihood of the function fcorresponding to ˜f.
We are interested in possible equivalences between the induced function-space trajectory of (WLD), {Aθt},
and the trajectory of possibly generalized versions of (FLD), with metric deﬁned as the pushforward of the
Euclideanmetricby Aoritsgeneralization. Byequivalencewemeanthatforany k∈Nand{ti}k
i=1⊂[0,∞),
{Aθti}k
i=1and{fti}k
i=1equal in distribution. When it holds, an algorithm that simulates (WLD) for a time
period ofTand returnsAθTcan be equivalently viewed as a “function-space inference algorithm” , as it is
then equivalent (in distribution) to the simulation of (FLD) which does not have to be deﬁned w.r.t. an
overparameterized model.
We will ﬁrst illustrate the equivalence on linear models (Sec. 2.1) which, while technically simple, pro-
vides intuition and formally covers NN models in the “kernel regime” (Woodworth et al., 2020). We will
then discuss the role of the pushforward metric in (FLD) (Sec. 2.2), and analyze general models in which
overparameterization can be characterized by group actions (Sec. 2.3).
2.1 Overparameterized Linear Models
The following is the easiest example where the equivalence can be demonstrated:
Example 2.1 (equivalence in linear models; see Appendix B.1 for details) .Suppose the mapAis linear. For
expository simplicity, further assume that πθ=N(0,I), and that the input space X={x1,x2,..., xK}has
ﬁnite cardinality K, so that any function can be represented by a Kdimensional vector (f(x1),...,f (xK)),
andAcan be identiﬁed as a matrix A∈RK×d.
(i) IfAis a bijection (i.e., d=KandAis invertible), the above vector representation will provide a
coordinate forF. In this coordinate, the metric matrix Gis(AA/latticetop)−1(see e.g., Bai et al., 2022).
(FLD)with this metric reduces to
d˜ft= (AA/latticetop)∇˜f/parenleftbigg
logp(Y|˜ft,X)−1
2/bardblA−1˜ft/bardbl2
2/parenrightbigg
dt+√
2AA/latticetopdBt. (1)
By Itô’s lemma, the above SDE also describes the evolution of Aθt, forθtfollowing (WLD).
(ii) The equivalence continue to hold in the overparameterized case (e.g., when d>K): consider the de-
composition Rd= Ran(A/latticetop)⊕Ker(A). Then the evolution of θtin(WLD)“factorizes” along the decom-
position: the likelihood gradient is fully contained in Ran(A/latticetop)and thus only inﬂuences ProjRan(A/latticetop)θt,
whereas ProjKer(A)θthas no inﬂuence on Aθt. Therefore, we can describe the evolution of the former
independently, thereby reducing to the exactly parameterized case.
The second case above provides the ﬁrst intuition on why (WLD) is not necessarily inﬂuenced by overparam-
eterization. While technically simple, it is relevant as it covers random feature models, which only require
2See Appendix A.2 for a conceptual review of relevant notions in Riemannian geometry.
3Under review as submission to TMLR
replacing Xwith preprocessed features. Random feature models formally include inﬁnitely wide DNNs in
the “kernel regime” (Jacot et al., 2018), where the pushforward metric converges to a constant value. As
referenced before, many popular procedures for BNN inference are only justiﬁed in this regime.
2.2 The Pushforward Metric
The pushforward metric that instantiates our (FLD) is an important object in the study of DNNs, in which
it is named the “neural tangent kernel” (NTK, Jacot et al., 2018). It acts as a preconditioner in our
function-space dynamics, and makes a similar appearance in the analysis of gradient descent (GD) where its
preconditioning eﬀect is often believed to be desirable (Arora et al., 2019a;b; Lee et al., 2019).
As cited before, for BNN models with a Gaussian πθ, the function-space prior can converge to a Gaussian
process (the “NNGP”, Lee et al., 2018) as the network width goes to inﬁnity. The NTK is closely related
to the covariance kernel of the NNGP; they are equivalent if only the last layer of the DNN is learnable,
and for more general models may still share the same Mercer eigenfunctions (Arora et al., 2019a, App. H).
When the two kernels are close and the BNN model is correctly speciﬁed, it can be informally understood
that (FLD) may enjoy good convergence properties , by drawing parallels to the analyses of GD (Arora et al.,
2019a; Lee et al., 2019);3consequently, the approximate posterior will have a good predictive performance.
However, for very deep networks, the Mercer spectra of the two kernels can be very diﬀerent (Arora et al.,
2019a, Fig. 4), in which case we can expect (FLD) to have poor convergence.
The above discussions immediately apply to (WLD) when it is equivalent to (FLD) or its suitable variants.
More generally, however, it can still be helpful to check for signiﬁcant diﬀerences between the NNGP and
NTK kernels when using (WLD), as part of a prior predictive check (Box, 1980) process. This is especially
relevant for deeper models, because in certain initialization regimes, both kernels can have pathological
behavior as the network depth increases (Schoenholz et al., 2016; Hayou et al., 2019).
2.3 Overparameterization via Group Actions
It is often the case that overparameterization can be characterized by group actions; in other words, there
exists some group HonRds.t. any two parameters θ,θ/prime∈Rdinduce the same function Aθ=Aθ/primeif and
only if they belong to the same orbit. In such cases, we can identify Fas the quotient space Rd/Hand the
mapA:Rd→Fas the quotient map, and it is desirable to connect (WLD) to possibly generalized versions
of (FLD) onF. This subsection presents such results.
To introduce our results, we ﬁrst recall some basic notions in group theory. (Additional background knowl-
edge is presented in Appendix A.) Let Hbe a Lie group. The unit element of His denoted as e, and we use
ϕ1ϕ2∈Hto denote the group operation of ϕ1,ϕ2∈H. AnactionofHonRdis a map Γ :H×Rd→Rd,
s.t. for all ϕ1,ϕ2∈Handp∈Rd, we have Γ(e,p) =p,Γ(ϕ1,Γ(ϕ2,p)) = Γ(ϕ1ϕ2,p)wheree∈Hde-
notes the identity. We use ϕ·pto denote Γ(ϕ,p)for simplicity. For any ϕ∈H, introduce the map
Γϕ:Rd→Rd,p/mapsto→ϕ·p. Then the action is freeifΓϕhas no ﬁxed point for all ϕ/negationslash=e,properif the preimage
of any compact set of the map (ϕ,p)/mapsto→ϕ·pis also compact, and smoothifΓϕis smooth for each ϕ∈H.
Anorbitis deﬁned as H·p:={ϕ·p:ϕ∈H}wherep∈Rd.
Analysis of free group actions The quotient manifold theorem (Lee, 2012, Theorem 21.10) guarantees
that the quotient space Rd/His a smooth manifold if the action is smooth, proper and free. To deﬁne the
pushforward metric on F, we further assume that the action is isometric, i.e., Γϕis an isometry for every
ϕ∈H. Under this condition, a metric on Fcan be deﬁned as4
/angbracketleft(dA|p)(u),(dA|p)(v)/angbracketrightTApF:=/angbracketleftu,v/angbracketrightRd,∀p∈F, u,v∈Tp(H·p)⊥⊂Rd.
3For the kernel regime and a Gaussian likelihood, a precise analysis can be possible: the evolution of ft(X)factorizes along
the eigenvectors of the NTK Gram matrix. We forgo it for brevity.
4It is well-deﬁned since dA|pis an isomorphism between Tp(H·p)⊥andTApF, and the isometry assumption ensures that
the deﬁnition is independent of the choice of pin the orbit (Lee, 2018).
4Under review as submission to TMLR
The above equation used some standard notations in diﬀerential geometry (Lee, 2018, p. 16): dA|p:Rd→
TApFis thediﬀerential ofAatp,Tpdenotes the tangent space of a manifold, and Tp(H·p)⊥is the orthogonal
complement of the tangent space of the orbit H·p, which is a submanifold of Rd.
The following proposition establishes the equivalence under discrete group action.
Proposition 2.1 (proofinAppendixB.2) .SupposeHis a discrete group (Hall, 2013, p. 28) acting smoothly,
freely, properly on Rd, andAis such thatAθ=Aθ/primeif and only if θ/prime∈H·θ. If either (a) the (improper)
priorpθis constant and the group action is isometric; or (b) H={e}is trivial, then the equivalence between
(WLD)and(FLD)will hold.
Remark 2.1.For continuous groups that act freely, the situation is more complicated, and depends on how
theorbitsareembeddedintheambientspace Rd. Forexample, adrifttermdependingonthemeancurvature
vector of the orbit may be introduced when pushing a Brownian motion using the quotient map (JE, 1990),
andwhenthemeancurvaturevanishes,theequivalencewillcontinuetohold, asshowninourExample2.1(ii).
Analysis for non-free group actions is primarily complicated by the fact that the quotient space is no longer
a manifold in general (Satake, 1956). Still, as we show in Example 2.3, similar results can be established
under the action of symmetric groups.
We now provide a concrete, albeit artiﬁcial, example in which the equivalence implies fast convergence to
the function-space posterior. It also highlights that VI and MCMC methods can have diﬀerent behavior
on overparameterized models, and that for VI methods it may still be necessary to explicitly account for
overparameterization. While recent works have made similar observations (e.g., Sun et al., 2019), and
provided some examples (Wang et al., 2019; Kurle et al., 2022), our example may provide additional insight:
Example 2.2 (LD vs. particle-based VI on torus) .LetAθ:= ([θ1],..., [θd]), where [a] :=a−⌊a⌋∈[0,1).
Letπθ,πfhave constant densities, and the negative log likelihood be unimodal and locally strongly convex.
Then we haveF=Td, thed-dimensional torus, and by Proposition 2.1, (WLD)is equivalent to Riemannian
LD onF. AsTdis a compact manifold, (FLD)enjoys exponential convergence (Villani, 2009), and so does
the induced function-space measure of (WLD).
Particle-based VI methods approximate the weight-space posterior with an empirical distribution of particles
{θ(i)}M
i=1, and update the particles iteratively. Consider the W-SGLD method in Chen et al. (2018): its
update rule resembles (WLD), but with the diﬀusion term replaced by a deterministic “repulsive force” term,
˜vt(θ)dt, where
˜vt(θ) :=M/summationdisplay
j=1∇θ(j)kh(θ,θ(j))/summationtextM
k=1kh(θ(j),θ(k))+/summationtextM
j=1∇θ(j)kh(θ,θ(j))
/summationtextM
k=1kh(θ,θ(k)),
andkhis a radial kernel with bandwidth h. Formally, in the inﬁnite-particle, continuous time limit, as
h→0, both ˜vtdtand the diﬀusion term implements the Wasserstein gradient of an entropy functional
(Carrillo et al., 2019), and W-SGLD and LD are formally equivalent (Chen et al., 2018).
The asymptotic equivalence between (WLD)and W-SGLD breaks down in this example: whereas (WLD)
induces a function-space measure that quickly converges to πf|D, this is not necessarily true for W-SGLD.
Indeed, its induced function-space measure may well collapse to a point mass around the MAP, regardless of
the number of particles. To see this, let θ∗∈[0,1)dbe any MAP solution so that ∇θlogp(Y|X,θ∗)p(θ∗) = 0.
Then for any ﬁxed h=O(1), asM→ ∞, the conﬁguration {θ(i,M)= (1010Mi,0,..., 0) +θ∗}M
i=1will
constitute an approximate stationary point for the W-SGLD update. This is because the posterior gradient
term is always zero, but the repulsive force term vanishes due to the very large distances between particles in
weight space.
Past works have noted the pathologies of particle-based VI in high dimensions (Zhuo et al., 2018; Ba et al.,
2021), but this example is interesting as it does not require an increasing dimensionality. Rather, it is global
overparameterization that breaks the asymptotic convergence to LD.
Analysis of non-free group actions As we have shown in Example 2.2, Proposition 2.1 already demon-
strates some equivalence between (WLD) and (FLD) in the presence of global overparameterization. It can
also be combined with Example 2.1 (ii) to construct models exhibiting both local and global overparameter-
ization. Still, we present a more relevant example below, which is a BNN model exhibiting permutational
5Under review as submission to TMLR
symmetry. Note that the model will still be diﬀerent from practical models, in particular because it precludes
continuous symmetry. However, it allows for a non-constant NTK, which is an important feature of eﬀective
NN models for high-dimensional data (see e.g., Ghorbani et al., 2019; Wei et al., 2019).
Example 2.3 (simpliﬁed BNN model) .Consider the model f(x;θ) :=/summationtextd
i=1sin(θix),which is a two-layer
BNN with the second layer frozen at initialization.
Let the prior support suppπθbe contained in (0,+∞)d. Then by the linear independence of sine functions,
forAθ=Aθ/primeto hold,θ/primemust be a permutation of θ, and thus the symmetry in this model can be described
by the symmetric group Sdconsisting of all permutations on the set {1,...,d}. The action of Snon the
weight space Rdis non-free, and the function space is a manifold with boundary, namely a polyhedral cone
Cn:={θ∈Rd:θ1≤θ2≤···≤θd}.
Let{θt}be the trajectory of (WLD)andptdenote the distribution of θt. Appendix B.3 proves that the push-
forward distribution ˜pt:=A#ptfollows the Fokker-Planck equation with the Neumann boundary condition:
/braceleftBigg
∂t˜pt(θ) =−∇· (˜pt(θ)∇θ(logp(Y|θ,X) + logpθ(θ))) + ∆˜pt(θ), θ∈F◦
∂θ˜pt(θ)/∂v= 0, v ∈Nθ,θ∈∂F,(2)
where∂FandF◦are the boundary and the interior of F, respectively, and Nθis the set of inward normal
vectors ofFatθ. The evolution of ˜ptis closely related to the reﬂected Langevin dynamics inF(Sato
et al., 2022), which keeps its trajectory in Fby reﬂecting it at ∂F; when the posterior is strongly log-
concave inCn, the connection suggests that the function-space measure ˜ptmay enjoy a fast convergence.5In
contrast, convergence of (WLD) to the weight-space posterior may be much slower , as it will have to visit
an exponential number of equivalence classes.
We note that mixture models exhibit a similar permutational invariance, and their inferential and computa-
tional issues have been extensively studied (Celeux et al., 2000; Frühwirth-Schnatter, 2001; Jasra et al., 2005;
Frühwirth-Schnatter & Frèuhwirth-Schnatter, 2006). However, those works typically focus on the mixing in
the parameter (i.e., weight) space, which is diﬀerent from our work which only concerns the function space.
3 Numerical Study
While our theoretical results have covered two simpliﬁed BNN models, the models are still diﬀerent from
those employed in practice. In this section we present numerical experiments that evaluate the eﬃcacy of
(WLD)-derived algorithms on practical BNN models. While they cannot provide direct evidence on the
equivalence between (WLD) and (FLD), they are still validating a possible consequence of it, as we expect
(FLD) to have good convergence properties (when the NTK and the NNGP kernels are not too diﬀerent,
Sec. 2.2) and (WLD) will inherit such a property if the equivalence holds. We will experiment on two setups,
a toy 1D regression dataset (Sec. 3.1) and a collection of semi-synthetic datasets adapted from the UCI
regression datasets (Sec. 3.2).
We note that it is impossible to implement (WLD) exactly, as it is a continuous-time process; thus, we will
experimentwithMetropolis-adjustedLangevinalgorithm(MALA,Roberts&Stramer,2002)andunadjusted
Langevin algorithm (ULA, Grenander & Miller, 1994), which are two standard, widely-used algorithms
derived from LD.6More importantly, it is diﬃcult to directly validate the equivalence between (WLD) and
(FLD) empirically, as the latter involves the function-space prior which cannot be computed or approximated
eﬃciently; for this reason we have resorted to indirect experiments . The experiments also share a similar
goal to our theoretical analysis, which is to understand the behavior of (WLD)-derived algorithms on BNN
models. In this aspect they complement previous works, by investigating practical, ﬁnite-width NN models
and eliminating the inﬂuence of possible model misspeciﬁcation in the evaluation.
5For a bounded convex domain and a smooth boundary, we can prove that (2) describes the density evolution of the reﬂected
LD, and its convergence rate has also been established (Bubeck et al., 2018, Proposition 2.6).
6Brieﬂy, ULA simulates a discretization of (WLD) and MALA corrects for the bias arising from the discretization. For
simplicity, we may refer to them as simulating (WLD) in the following.
6Under review as submission to TMLR
3.1 Sample Quality on a Toy Dataset
We ﬁrst consider BNN inference on a toy 1D regression dataset, and check if the function-space measure
induced by simulating (WLD) (i.e., the distribution of Aθt) appears to converge at a similar rate, across
models with increasing degree of overparameterization.
1. we will visualize the pointwise credible intervals of Aθt, which are informative about one-dimensional
marginal distributions of the function-space measure;
2. when the training sample size nis small, we approximately evaluate the approximation quality of (n+ 1)-
dimensional marginal distributions of f(Xe) := (f(x1),...,f (xn),f(x∗))withf∼πf|D, by estimating
the kernelized Stein discrepancy (KSD, Liu et al., 2016; Chwialkowski et al., 2016) between the marginal
distribution q(f(Xe))(wheref=Aθtandθtfollows (WLD)), and the true marginal posterior p(f(Xe))
(wheref∼πf|D).
KSD is often used for measuring sample quality (Gorham & Mackey, 2017; Anastasiou et al., 2023). We
use the U-statistic estimator in Liu et al. (2016, Eq. 14), which only requires the speciﬁcation of a kernel in
Rn+1, samples from qand thescore function ofp(f(Xe)). Importantly, we can estimate the score since it
admits the following decomposition:
∇f(Xe)logp=∇f(Xe)/parenleftBig
logdπf(Xe)
dµLeb+ logp(Y|f(Xe))/parenrightBig
=∇f(Xe)/parenleftBig
logdπf(Xe)
dµLeb+ logp(Y|f(X))/parenrightBig
, (sinceX⊂Xe) (3)
whereπf(Xe)denotes the respective marginal distribution of πf, andµLebdenotes the Lebesgue measure.
We estimate the ﬁrst term by ﬁtting nonparametric score estimators (Zhou et al., 2020) on prior samples.
The second term can be evaluated in closed form.
2
 0 22
02I = 25
2
 0 2I = 175
2
 0 2I = 775
2
 0 2I = 3175
2
 0 2I = 102375
(a)L= 3,W= 20
2
 0 22
02I = 25
2
 0 2I = 175
2
 0 2I = 775
2
 0 2I = 3175
2
 0 2I = 102375
(b)L= 3,W= 500
Figure 1: 1D regression: visualization of the induced function-space measure of MALA after Iiterations.
We plot the pointwise 80%credible intervals. The results for L= 2are deferred to Fig. 4.
We use feed-forward networks with factorized Gaussian priors, and the standard initialization scaling:
f(x;θ) :=f(L)(f(L−1)(...f(0)(x))),where
f(l)(h(l−1)) :=σ(l)/parenleftBig
B(l)h(l−1)+b(l)/parenrightBig
,vec(B(l))∼N/parenleftBig
0,(dimh(l−1))−1I/parenrightBig
, b(l)∼N(0,0.2I),(4)
7Under review as submission to TMLR
102103104105
# iterations051015sqrt{KSD}
H=20
H=50
H=500
(a)L= 2
102103104105
# iterations051015sqrt{KSD}
H=20
H=50
H=200 (b)L= 3
Figure 2: 1D regression: estimated√
KSD between the LD predictive distribution q(f(Xe))and the approx-
imate function-space posterior p(f(Xe)). We simulate 1000 LD chains. For the approximate posterior, we
estimate the prior score term in (3) using 5×106samples.
and the activation functions σ(l)are SELU (Klambauer et al., 2017) for hidden layers ( l<L) and the identity
map for the output layer ( l=L). We vary the network depth L∈{2,3}, and the width of all hidden layers
W∈[20,500].
The training data is generated as follows: the inputs consist of ⌊2n/3⌋evenly spaced points on [−2.5,−0.5],
and the remaining points are evenly placed on [1,2]. The output is sampled from p(y|x) =N(xsin(1.5x) +
0.125x3,0.01).We usen= 7for visualization, and n= 3for KSD evaluation. The diﬀerence is due to
challenges in approximating our KSD: (3) involves score estimation, and in our case we further need the
estimate to generalize to out-of-distribution inputs (approximate posterior as opposed to prior samples);
both are extremely challenging tasks in high dimensions7. We simulate (WLD) with MALA, and evaluate
the induced function-space samples for varying number of iterations. The step size is set to 0.025/nW, so
that the function-space updates have a similar scale.
We visualize the posterior approximations in Fig. 1 and Fig. 4, and report the approximate KSD in Fig. 2.
As we can see, the convergence appears to happen at a similar rate, which is a possible consequence of the
equivalence results.
3.2 Average-Case Predictive Performance on Semi-Synthetic Data
The previous experiments cannot scale to larger datasets due to the aforementioned challenges in estimating
the KSD. To investigate the behavior of (WLD)-derived algorithms on more realistic datasets, we turn to less
direct experiments and check whether in the absence of model misspeciﬁcation (WLD)-derived algorithms
will lead to competitive predictive performance.
Our experiments use semi-synthetic datasets adapted from the UCI machine learning repository. Speciﬁcally,
we modify the UCI datasets by keeping the input data and replacing the output with samples from the model
likelihoodp(y|x,f0), wheref0=fBNN(·;θ0)is sampled from the BNN prior:
θ0∼πθ, y|x∼p(y=·|fBNN(x;θ0)). (5)
We will consider Gaussian (resp. Laplacian) likelihood and check whether an approximate posterior mean
(resp. median) estimator, constructed using the (WLD)-derived algorithms, has a competitive average-case
performance across randomly sampled θ0. This will happen if the weight-space algorithms provide a reason-
ably accurate approximation to the function-space posterior, since the exactposterior mean (resp. median)
estimator will minimize the similarly deﬁned average-case risk
ˆf/mapsto→Ef0∼πfEY∼p(·|f0(X))Ex∗∼px,y∗∼p(·|f0(x∗))/lscript(ˆf(x∗),y∗), (6)
7Withoutstrongdiﬀerentiabilityassumptions, theerrorofscoreestimationmaysuﬀerfromcurseofdimensionality(Tsybakov
& Zaiats, 2009).
8Under review as submission to TMLR
where/lscriptdenotes the loss function derived from the model likelihood, and ˆfdenotes any estimator that maps
the data (X,Y)to a prediction function (we dropped the dependency on the data for readability). Therefore,
competitive predictive performance of the approximate predictor will provide evidence on the quality of
posterior approximation. Note that by using a semi-synthetic data generating process, we can allow the
input features to have realistic distributions, while avoiding the possible inﬂuence of model misspeciﬁcation
which cannot be ruled out in past works that experiment on real-world data (Wenzel et al., 2020; Fortuin
et al., 2021).
We estimate the average-case risk (6) for MALA and ULA, using a Monte-Carlo estimate; the full procedure
is summarized as Algorithm 1 in appendix. We instantiate (6) with Gaussian and Laplacian likelihoods,
which correspond to the square loss and the absolute error loss, respectively. The feed-forward network
architecture fBNNfollows Sec. 3.1 by varying L∈ {2,3},W∈ {50,200}, and we use 80% samples for
training and 20% for testing. To understand the performance of the (WLD)-derived algorithms, we report
theBayes error , which is the minimum possible average-case risk attainable with inﬁnite samples ; we also
include a baseline that replaces the (WLD)-derived algorithm with an ensemble gradient descent (GD)
procedure for a maximum a posterior (MAP) estimate. For all methods, the step size is selected from
{η/2nW:η∈{1,0.5,0.1,0.05,0.01,0.005}}such that the average acceptance rate of the ﬁrst 200MALA
iterations is closest to 0.7, wherendenotes the size of training set.
We plot the Bayes error and the estimated average-case risk against the number of iterations in Fig. 3 and
Fig. 5-6 in appendix, and report the performance at the best iteration in Table 1-3. As we can see, across all
settings, MALA and ULA lead to a similar predictive performance to GD, and all of them attain errors close
to the Bayes error, especially for a dataset with a larger training set. As it is well known that GD methods
perform well on DNN models (Du et al., 2018; Allen-Zhu et al., 2019; Mei et al., 2019; Arora et al., 2019a),
these results provide further evidence on the eﬃcacy of the (WLD)-derived algorithms.
4 Conclusion
In this work we have investigated the function space behavior of weight-space Langevin-type algorithms
on overparameterization models. Across multiple settings that encompass simpliﬁed BNN models, we have
established the equivalence of the function-space pushforward of weight-space LD to its various function-
space counterparts. Within their scope, the equivalence results allow us to view weight-space LD as a
function-space inference procedure, and understand its behavior by examining the preconditioner in the
equivalent function-space dynamics. Numerical experiments provide additional evidence for the eﬃcacy of
Langevin-type algorithms on practical feed-forward network models.
References
Laurence Aitchison. A statistical theory of cold posteriors in deep neural networks. arXiv preprint
arXiv:2008.05912 , 2020.
Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via over-
parameterization. In International Conference on Machine Learning , pp. 242–252. PMLR, 2019.
Andreas Anastasiou, Alessandro Barp, François-Xavier Briol, Bruno Ebner, Robert E Gaunt, Fatemeh
Ghaderinezhad, Jackson Gorham, Arthur Gretton, Christophe Ley, Qiang Liu, et al. Stein’s method
meets computational statistics: a review of some recent developments. Statistical Science , 38(1):120–139,
2023.
SanjeevArora,SimonDu, WeiHu, ZhiyuanLi, andRuosongWang. Fine-grainedanalysisofoptimizationand
generalization for overparameterized two-layer neural networks. In International Conference on Machine
Learning , pp. 322–332. PMLR, 2019a.
Sanjeev Arora, Simon S Du, Wei Hu, Zhiyuan Li, Russ R Salakhutdinov, and Ruosong Wang. On exact
computation with an inﬁnitely wide neural net. Advances in neural information processing systems , 32,
2019b.
9Under review as submission to TMLR
1021031041051060.050.060.070.080.090.100.11mean squared errorboston (n=404)
mala
gd
ula
Bayes error
1021031041051060.060.080.100.12concrete (n=824)
mala
gd
ula
Bayes error
1021031041051060.050.060.070.08energy (n=614)
mala
gd
ula
Bayes error
1021031041051060.060.080.100.12kin8nm (n=6553)
mala
gd
ula
Bayes error
102103104105106
# iterations0.0500.0550.0600.065mean squared errornaval (n=9547)
mala
gd
ula
Bayes error
102103104105106
# iterations0.050.060.070.08power_plant (n=7654)
mala
gd
ula
Bayes error
102103104105106
# iterations0.060.080.100.12wine (n=1279)
mala
gd
ula
Bayes error
102103104105106
# iterations0.040.060.080.10yacht (n=246)
mala
gd
ula
Bayes error
(a) Gaussian likelihood / mean square error
1021031041051060.150.200.250.300.35mean absolute errorboston (n=404)
mala
gd
ula
Bayes error
1021031041051060.1750.2000.2250.2500.2750.300concrete (n=824)
mala
gd
ula
Bayes error
1021031041051060.160.180.200.220.240.26energy (n=614)
mala
gd
ula
Bayes error
1021031041051060.200.250.30kin8nm (n=6553)
mala
gd
ula
Bayes error
102103104105106
# iterations0.160.170.180.190.200.21mean absolute errornaval (n=9547)
mala
gd
ula
Bayes error
102103104105106
# iterations0.160.180.200.220.240.26power_plant (n=7654)
mala
gd
ula
Bayes error
102103104105106
# iterations0.150.200.250.300.35wine (n=1279)
mala
gd
ula
Bayes error
102103104105106
# iterations0.150.200.250.30yacht (n=246)
mala
gd
ula
Bayes error
(b) Laplace likelihood / mean absolute error
Figure 3: Semi-synthetic experiment: estimated average-case risk (6) under diﬀerent choices of likelihood,
forL= 2,W= 200. Shade indicates standard deviation across 8independent replications.
10Under review as submission to TMLR
Jimmy Ba, Murat A Erdogdu, Marzyeh Ghassemi, Shengyang Sun, Taiji Suzuki, Denny Wu, and Tianzong
Zhang. Understanding the variance collapse of SVGD in high dimensions. In International Conference on
Learning Representations , 2021.
Qinxun Bai, Steven Rosenberg, and Wei Xu. Understanding natural gradient in Sobolev spaces. arXiv
preprint arXiv:2202.06232 , 2022.
George EP Box. Sampling and bayes’ inference in scientiﬁc modelling and robustness. Journal of the Royal
Statistical Society: Series A (General) , 143(4):383–404, 1980.
Sébastien Bubeck, Ronen Eldan, and Joseph Lehec. Sampling from a log-concave distribution with projected
Langevin Monte Carlo. Discrete & Computational Geometry , 59:757–783, 2018.
David R Burt, Sebastian W Ober, Adrià Garriga-Alonso, and Mark van der Wilk. Understanding variational
inference in function-space. arXiv preprint arXiv:2011.09421 , 2020.
José Antonio Carrillo, Katy Craig, and Francesco S Patacchini. A blob method for diﬀusion. Calculus of
Variations and Partial Diﬀerential Equations , 58(2):1–53, 2019.
GillesCeleux, MerrileeHurn, andChristianPRobert. Computationalandinferentialdiﬃcultieswithmixture
posterior distributions. Journal of the American Statistical Association , 95(451):957–970, 2000.
Changyou Chen, Ruiyi Zhang, Wenlin Wang, Bai Li, and Liqun Chen. A uniﬁed particle-optimization
framework for scalable Bayesian sampling. arXiv preprint arXiv:1805.11659 , 2018.
Kacper Chwialkowski, Heiko Strathmann, and Arthur Gretton. A kernel test of goodness of ﬁt. In Interna-
tional conference on machine learning , pp. 2606–2615. PMLR, 2016.
Manfredo Perdigao Do Carmo. Riemannian geometry , volume 6. Springer, 1992.
Simon S Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh. Gradient descent provably optimizes over-
parameterized neural networks. arXiv preprint arXiv:1810.02054 , 2018.
David Steven Dummit and Richard M Foote. Abstract algebra , volume 3. Wiley Hoboken, 2004.
VincentFortuin, AdriàGarriga-Alonso, FlorianWenzel, GunnarRätsch, RichardTurner, MarkvanderWilk,
and Laurence Aitchison. Bayesian neural network priors revisited. arXiv preprint arXiv:2102.06571 , 2021.
Sylvia Frühwirth-Schnatter. Markov chain Monte Carlo estimation of classical and dynamic switching and
mixture models. Journal of the American Statistical Association , 96(453):194–209, 2001.
Sylvia Frühwirth-Schnatter and Sylvia Frèuhwirth-Schnatter. Finite mixture and Markov switching models ,
volume 425. Springer, 2006.
Behrooz Ghorbani, Song Mei, Theodor Misiakiewicz, and Andrea Montanari. Limitations of lazy training of
two-layers neural networks. arXiv preprint arXiv:1906.08899 , 2019.
Mark Girolami and Ben Calderhead. Riemann manifold Langevin and Hamiltonian Monte Carlo methods.
Journal of the Royal Statistical Society Series B: Statistical Methodology , 73(2):123–214, 2011.
Jackson Gorham and Lester Mackey. Measuring sample quality with kernels. In International Conference
on Machine Learning , pp. 1292–1301. PMLR, 2017.
Ulf Grenander and Michael I Miller. Representations of knowledge in complex systems. Journal of the Royal
Statistical Society: Series B (Methodological) , 56(4):549–581, 1994.
Brian C Hall. Lie groups, Lie algebras, and representations . Springer, 2013.
Souﬁane Hayou, Arnaud Doucet, and Judith Rousseau. Exact convergence rates of the neural tangent kernel
in the large depth limit. arXiv e-prints , pp. arXiv–1905, 2019.
11Under review as submission to TMLR
Bobby He, Balaji Lakshminarayanan, and Yee Whye Teh. Bayesian deep ensembles via the neural tangent
kernel.Advances in Neural Information Processing Systems , 33:1010–1022, 2020.
Pavel Izmailov, Sharad Vikram, Matthew D Hoﬀman, and Andrew Gordon Gordon Wilson. What are
Bayesian neural network posteriors really like? In International Conference on Machine Learning , pp.
4629–4640. PMLR, 2021.
Arthur Jacot, Franck Gabriel, and Clément Hongler. Neural tangent kernel: Convergence and generalization
in neural networks. Advances in Neural Information Processing Systems , 31, 2018.
Ajay Jasra, Chris C Holmes, and David A Stephens. Markov chain monte carlo methods and the label
switching problem in bayesian mixture modeling. 2005.
Pauwels JE. Riemannian submersions of Brownian motions. Stochastics: An International Journal of
Probability and Stochastic Processes , 29(4):425–436, 1990.
Günter Klambauer, Thomas Unterthiner, Andreas Mayr, and Sepp Hochreiter. Self-normalizing neural
networks. Advances in Neural Information Processing Systems , 30, 2017.
Richard Kurle, Ralf Herbrich, Tim Januschowski, Yuyang Bernie Wang, and Jan Gasthaus. On the detri-
mental eﬀect of invariances in the likelihood for variational inference. Advances in Neural Information
Processing Systems , 35:4531–4542, 2022.
JaehoonLee,JaschaSohl-dickstein,JeﬀreyPennington,RomanNovak,SamSchoenholz,andYasamanBahri.
Deep neural networks as Gaussian processes. In International Conference on Learning Representations ,
2018.
Jaehoon Lee, Lechao Xiao, Samuel Schoenholz, Yasaman Bahri, Roman Novak, Jascha Sohl-Dickstein, and
Jeﬀrey Pennington. Wide neural networks of any depth evolve as linear models under gradient descent.
Advances in Neural Information Processing Systems , 32, 2019.
John M. Lee. Introduction to Smooth Manifolds . Springer, 2012.
John M Lee. Introduction to Riemannian manifolds . Springer, 2018.
Qiang Liu, Jason Lee, and Michael Jordan. A kernelized stein discrepancy for goodness-of-ﬁt tests. In
International conference on machine learning , pp. 276–284. PMLR, 2016.
Chao Ma and José Miguel Hernández-Lobato. Functional variational inference based on stochastic process
generators. Advances in Neural Information Processing Systems , 34:21795–21807, 2021.
Chao Ma, Yingzhen Li, and José Miguel Hernández-Lobato. Variational implicit processes. In International
Conference on Machine Learning , pp. 4222–4233. PMLR, 2019.
Alexander G de G Matthews, Mark Rowland, Jiri Hron, Richard E Turner, and Zoubin Ghahramani. Gaus-
sian process behaviour in wide deep neural networks. arXiv preprint arXiv:1804.11271 , 2018.
Song Mei, Theodor Misiakiewicz, and Andrea Montanari. Mean-ﬁeld theory of two-layers neural networks:
dimension-free bounds and kernel limit. In Conference on Learning Theory , pp. 2388–2464. PMLR, 2019.
Ankur Moitra and Andrej Risteski. Fast convergence for Langevin diﬀusion with manifold structure, Septem-
ber 2020. arXiv:2002.05576 [cs, math, stat].
IanOsband, JohnAslanides, andAlbinCassirer. Randomizedpriorfunctionsfordeepreinforcementlearning.
Advances in Neural Information Processing Systems , 31, 2018.
OmirosPapaspiliopoulos, Gareth ORoberts, and MartinSköld. Ageneral framework forthe parametrization
of hierarchical models. Statistical Science , pp. 59–73, 2007.
Gareth O Roberts and Osnat Stramer. Langevin diﬀusions and Metropolis-Hastings algorithms. Methodology
and computing in applied probability , 4:337–357, 2002.
12Under review as submission to TMLR
Gareth O Roberts and Richard L Tweedie. Exponential convergence of Langevin distributions and their
discrete approximations. Bernoulli , pp. 341–363, 1996.
IchirôSatake. Onageneralizationofthenotionofmanifold. Proceedings of the National Academy of Sciences ,
42(6):359–363, 1956.
Kanji Sato, Akiko Takeda, Reiichiro Kawai, and Taiji Suzuki. Convergence error analysis of reﬂected
gradient Langevin dynamics for globally optimizing non-convex constrained problems. arXiv preprint
arXiv:2203.10215 , 2022.
Samuel S Schoenholz, Justin Gilmer, Surya Ganguli, and Jascha Sohl-Dickstein. Deep information propaga-
tion.arXiv preprint arXiv:1611.01232 , 2016.
Jiaxin Shi, Mohammad Emtiyaz Khan, and Jun Zhu. Scalable training of inference networks for Gaussian-
process models. In International Conference on Machine Learning , pp. 5758–5768. PMLR, 2019.
Shengyang Sun, Guodong Zhang, Jiaxin Shi, and Roger Grosse. Functional variational Bayesian neural
networks. arXiv preprint arXiv:1903.05779 , 2019.
A B Tsybakov and Vladimir Zaiats. Introduction to Nonparametric Estimation . Springer series in statistics.
Springer, New York, NY, December 2009.
Cédric Villani. Optimal transport: old and new , volume 338. Springer, 2009.
Ziyu Wang, Tongzheng Ren, Jun Zhu, and Bo Zhang. Function space particle optimization for Bayesian
neural networks. In International Conference on Learning Representations , 2019.
Colin Wei, Jason D Lee, Qiang Liu, and Tengyu Ma. Regularization matters: Generalization and optimiza-
tion of neural nets v.s. their induced kernel. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d’Alché Buc,
E. Fox, and R. Garnett (eds.), Advances in Neural Information Processing Systems , volume 32, 2019.
Florian Wenzel, Kevin Roth, Bastiaan S Veeling, Jakub Świkatkowski, Linh Tran, Stephan Mandt, Jasper
Snoek, Tim Salimans, Rodolphe Jenatton, and Sebastian Nowozin. How good is the Bayes posterior in
deep neural networks really? arXiv preprint arXiv:2002.02405 , 2020.
Blake Woodworth, Suriya Gunasekar, Jason D Lee, Edward Moroshko, Pedro Savarese, Itay Golan, Daniel
Soudry, and Nathan Srebro. Kernel and rich regimes in overparametrized models. In Conference on
Learning Theory , pp. 3635–3673. PMLR, 2020.
Greg Yang. Scaling limits of wide neural networks with weight sharing: Gaussian process behavior, gradient
independence, and neural tangent kernel derivation. arXiv preprint arXiv:1902.04760 , 2019.
YamingYuandXiao-LiMeng. Tocenterornottocenter: Thatisnotthequestion—anancillarity–suﬃciency
interweaving strategy (asis) for boosting mcmc eﬃciency. Journal of Computational and Graphical Statis-
tics, 20(3):531–570, 2011.
Ruqi Zhang, Chunyuan Li, Jianyi Zhang, Changyou Chen, and Andrew Gordon Wilson. Cyclical stochastic
gradient mcmc for Bayesian deep learning. arXiv preprint arXiv:1902.03932 , 2019.
Yuhao Zhou, Jiaxin Shi, and Jun Zhu. Nonparametric score estimators. In International Conference on
Machine Learning , pp. 11513–11522. PMLR, 2020.
Jingwei Zhuo, Chang Liu, Jiaxin Shi, Jun Zhu, Ning Chen, and Bo Zhang. Message passing Stein variational
gradient descent. In International Conference on Machine Learning , pp. 6018–6027. PMLR, 2018.
13Under review as submission to TMLR
A Background Knowledge
A.1 Groups
AgroupHis a set equipped with an operation Ψ :H×H→H. It satisﬁes the following properties:
•There is a unit element einHsuch that Ψ(e,ϕ) = Ψ(ϕ,e) =ϕfor everyϕ∈H.
•Every element ϕ∈Hhas an inverse ϕ−1∈Hsuch that Ψ(ϕ,ϕ−1) = Ψ(ϕ−1,ϕ) =e.
•The operation Ψis associative, i.e., for ϕ1,ϕ2,ϕ3∈H, it holds that Ψ(ϕ1,Ψ(ϕ2,ϕ3)) =
Ψ(Ψ(ϕ1,ϕ2),ϕ3).
For simplicity, we use ϕ1ϕ2to denote Ψ(ϕ1,ϕ2)for everyϕ1,ϕ2∈H.
A groupHis aLie group if it is a smooth manifold (Hall, 2013), and we say Hisdiscreteif for eachϕ∈H
there exists a neighborhood Uϕ/ownerϕcontaining only ϕ.
A.2 Riemannian Manifolds
Toaidunderstanding, weprovideaconceptualintroductionofsomerelevantnotionsinRiemanniangeometry
in this section. We refer interested readers to Do Carmo (1992); Lee (2018) for rigorous deﬁnitions.
Ak-dimensional manifold Mis a topological space locally resembling a k-dimensional Euclidean space.
Speciﬁcally, for any p∈M, there exist open neighborhoods M⊃U/ownerpandRk⊃V/owner0and a homeomor-
phismψ:V→U. The map ψis called a coordinate map near pifψ(0) =p, and the coordinate of a point
q∈Uisψ−1(q). If two coordinate maps ψ1,ψ2overlap, we require the transition ψ−1
2◦ψ1to be smooth.
The tangent space at p∈Mis ak-dimensional linear space “orthogonal to M”, denoted as TpM. A
Riemannian structure equips the tangent space TpMwith an inner product /angbracketleft·,·/angbracketrightpfor everyp∈M. Given
a coordinate map ψ:V→Uanda∈V, the diﬀerential dψ|aatais a linear bijection between Rkand
Tψ(a)M. Lete1,e2,...,ek∈Rkbe the standard basis of Rk, then{Ei(a) := (dψ|a)(ei)}i∈[k]is a basis of
Tψ(a)M. We callgij(a) :=/angbracketleftEi(a),Ej(a)/angbracketrightψ(a)thecoordinate representation of the Riemannian metric and
G(a) := (gij(a))i,j∈[k]∈Rk×kthecoordinate representation of the metric matrix . For simplicity, we omit
the dependence on aofgij(a)andG(a).
Also, under a coordinate ψ:V→U, thevolumeof a setR⊂Uis deﬁned as (Do Carmo, 1992, p. 45)
Vol(R) :=/integraldisplay
ψ−1(R)/radicalbig
|G|dµLeb,
where|G|denotes the determinant of G, andµLebis thek-dimensional Lebesgue measure. The measure
dµM:=/radicalbig
|G|dµLebis called the Riemannian measure or thevolume form , and is independent of the choice
of the coordinate ψ.
Example A.1. As an example, let M:=T2:={(cosα,sinα,cosβ,sinβ) :α,β∈[0,2π)}be the two-
dimensional torus considered in Example 2.2. For any p= (cosα,sinα,cosβ,sinβ)∈T2, we can ﬁnd a
coordinate map ψ(ζ,ξ) := (cos(α+ζ),sin(α+ζ),cos(β+ξ),sin(β+ξ))∈R4with (ζ,ξ)∈V:= (−1,1)×
(−1,1). The tangent space TpT2=p+{(−tsinα,tcosα,−ssinβ,scosβ) :t,s∈R}is the plane orthogonal
top. Fort1,t2,s1,s2∈Rand tangent vectors v1=p+ (−t1sinα,t1cosα,−s1sinβ,s1cosβ)andv2=
p+ (−t2sinα,t2cosα,−s2sinβ,s2cosβ), we can deﬁne an inner product /angbracketleftv1,v2/angbracketrightp:= (v1−p)/latticetop(v2−p) =
t1t2+s1s2. The diﬀerential of ψis(dψ|0)(t,s) = (−tsinα,tcosα,−ssinβ,scosβ), mapping the basis
e1= (1,0),e2= (0,1)inR2toE1=p+ (−sinα,cosα,0,0),E2=p+ (0,0,−sinβ,cosβ)∈TpT2. The
coordinate representation of the metric is such that gij(0) = 1ifi=jandgij(0) = 0otherwise, and the
metric matrix is G(0) =/parenleftbigg1 0
0 1/parenrightbigg
.
14Under review as submission to TMLR
B Proofs
B.1 Details in Example 2.1
Asmentionedinthisexample, whentheinputspace Xhasﬁnitecardinality K∈N, afunction f:X→Rcan
be identiﬁed as the vector (f(x1),f(x2),...,f (xK))∈RK. Conversely, for a vector v= (v1,v2,...,vK)∈RK
we can deﬁne a function fv:X→Rsuch thatfv(xj) =vjforj= 1,...,K. In the remaining part, we will
represent the function fv∈RXby the vector v∈RK.
Proof of Example 2.1. Since the mapAis linear, there exists a matrix A∈RK×dsuch thatAθ=Aθfor
everyθ∈Rd.
Now the claim (i) follows by Proposition 2.1 (b): clearly, in this case H={e}fulﬁlls the conditions in the
proposition.
We now turn to (ii). When Ais a surjection with d>K. Let Ker(A) := Ker(A) :={x∈Rd:Ax= 0}be
the kernel of A, and Ran(A/latticetop) := Ran(A/latticetop) :={A/latticetopy∈Rd:y∈RK}be the range of A/latticetop, then the space Rd
has the orthogonal decomposition Rd= Ker(A)⊕Ran(A/latticetop), and there exists an orthogonal matrix Q∈Rd×d
such thatQ(Ker(A)) ={0}k×Rd−kandQ(Ran(A/latticetop)) =Rk×{0}d−k, wherek:= dim Ran( A/latticetop)≤K <d.
Then,θ∈Rdhas the representation θ=Q/latticetop(θ/bardbl,θ⊥)forθ/bardbl∈Rkandθ⊥∈Rd−k. Under this representation,
Abecomes ˜A:=A◦Q/latticetopand ˜A|Rk×{0}d−kis a bijection. We can also deﬁne a reduced likelihood ˜p(Y|
θ/bardbl,X) :=p(Y|Q/latticetop(θ/bardbl,0),X), then
d/parenleftbigg
θ/bardbl
t
θ⊥
t/parenrightbigg
= d(Qθ) = (Q∇θlogp(Y|θt,X)−Qθt)dt+√
2QdBt(pθ=N(0,Id))
=/parenleftbigg/parenleftbigg
∇θ/bardbllog ˜p(Y|θ/bardbl
t,X)
0/parenrightbigg
−/parenleftbigg
θ/bardbl
t
θ⊥
t/parenrightbigg/parenrightbigg
dt+√
2d˜Bt (QdBtd=/radicalbig
QQ/latticetopdBt= d˜Bt)
where ˜Btdenotes another Brownian motion. Therefore, θ/bardbl
tandθ⊥
tfactorize to independent processes, and
the equivalence holds by applying the result of (i) to θ/bardbl
tand ˜A(and noting that ˜Aθ/bardbl
t=Aθt).
B.2 Proof of Proposition 2.1
Proof of Proposition 2.1. By deﬁnitions, for any f∈F, there exists some θ∈Rdand one of its neighborhood
Nsuch thatf=Aθ, and that for U=A(N),(U,A|N)forms a coordinate chart. On this chart, the
coordinate matrix of the pushforward metric tensor equals identity, by its deﬁnition. Thus, the coordinate
representation (FLD) reduces to
dθt=∇θ/parenleftbigg
logp(Y|θt,X) + logdπf
dµF/parenrightbigg
dt+√
2dBt,
and it diﬀers from (WLD) only on the prior term. When condition (a) in the proposition holds, the prior is
uniform so the gradient vanishes. When condition (b) holds, the group is trivial and the quotient map Ais
a bijection. Thus, it suﬃces to show that for all θ∈suppπθ, we have
dπf
dµF(Aθ) =dπθ
dµLeb(θ) =pθ(θ),
whereµLebdenotes the Lebesgue measure. By the change of measure formula, the above will be implied by
πf(i)=A#πθ, µF(ii)=A#µLeb.
(i) is the deﬁnition of πf. For (ii), let ζ:F → Rbe any measurable function with a compact support,
{(Ui=A(Ni),A|Ni) :i∈[h]}be a ﬁnite chart covering of suppζ, and{ρi}be a corresponding partition of
unity. Then
/integraldisplay
Fζ(f)µF(df) =h/summationdisplay
i=1/integraldisplay
Ni(ρiζ)(A(θ))/radicalbig
|G(θ)|µLeb(dθ) =/integraldisplay
A−1(suppζ)ζ(A(θ))µLeb(dθ).
15Under review as submission to TMLR
This establishes (ii), and thus completes the proof.
B.3 Details in Example 2.3
Recall the deﬁnition of the cone Cd:={x∈Rd:x1≤x2≤...≤xd}, and the group Sdthat consists of all
permutations of length d. An action of SdonRdcan be naturally deﬁned, under which we have Cd=Rd/Sd.
We introduce a few additional notations. For x∈Rd, thestabilizer subgroup is deﬁned as StabSdx:={ϕ∈
Sd:ϕ·x=x}, and the orbit is Sd·x:={ϕ·x:ϕ∈Sd}. A vector nx∈Rdis aninward normal vector of
Cdatxif/angbracketleftnx,y−x/angbracketright≥0holds for all y∈Cd. Denote by Nxthe set of all inward normal vector of Cdatx.
For anyf:Rd→R, deﬁne the function
˜f:Cd→R,˜f(x) :=1
|Sd|/summationdisplay
ϕ∈Sdf(ϕ·x). (7)
Whenfis the density function of a measure πonRd, the pushforward measure under the quotient map
Rd→Cdhas the density function ˜f. The following lemma shows that the directional derivative of ˜falong
the normal direction vanishes.
Lemma B.1. Letx∈Cdand assume fis diﬀerentiable at every y∈Sd·x. Then
Dv˜f(x) =1
|Sd|/summationdisplay
y:=ψ·x∈Sd·xDψ·Wx(v)f(y),whereWx(v) :=/summationdisplay
ϕ∈Stabxϕ·v,
whereDvdenotes the directional derivative along v. Moreover, Wx(v) =vforx∈C◦
nandv∈Rd, and
Wx(v) = 0forx∈∂Cdandv∈Nx.
We postpone the proof of the above lemma to the end of this section, and ﬁrst present the following lemma,
which implies the invariance of the Fokker-Planck equation under orthogonal transformations.
Lemma B.2. Letf,g:Rd→Rbe two functions and Q∈Rd×dbe an orthogonal matrix, then [∇(f◦
Q)]T∇(g◦Q) = [(∇f)T∇g]◦Qand∆(f◦Q) = ∆f◦Q, in which Qis also regarded as a linear map
Q:Rd→Rd.
Proof.Note that∇(f◦Q) =QT(∇f◦Q). LetQibe thei-th column of Q, then
[∇(f◦Q)]T∇(g◦Q) =d/summationdisplay
i=1(∇f◦Q)TQiQT
i(∇g◦Q) = (∇f◦Q)T(∇g◦Q).
A similar result also holds for the Laplacian:
∆(f◦Q) =d/summationdisplay
i=1∂i∂i(f◦Q) =d/summationdisplay
i,j=1∂i(∂jf◦Q)qji=d/summationdisplay
i,j,k=1(∂k∂jf◦Q)qjiqki.
AsQis orthogonal, we know/summationtextd
i=1qjiqki=δjk, which completes the proof.
As the pushforward measure A#phas density ˜p, the following proposition establishes the equivalence result
claimed in the text.
Proposition B.1. Letp:Rd→Rbe any function that is invariant under the action of Sd, andXtfollow
the Langevin dynamics on Rd,
dXt=∇logp(Xt)dt+√
2dBt.
Then, the pushforward density ˜ptofXtwill evolve as
/braceleftBigg
∂t˜pt=−∇· (˜pt∇logp) + ∆˜pt,inC◦
n,
∂˜pt
∂v(x) = 0, ∀v∈Nx,x∈∂Cd.
16Under review as submission to TMLR
Proof.Letptbe the density of the distribution of Xt, then it follows the Fokker-Planck equation
∂tpt=−∇· (pt∇logp) + ∆pt=−(∇pt)T∇logp−pt∆ logp+ ∆pt.
Forϕ∈Sd, we denote Pϕ∈Rd×dby the corresponding matrix such that ϕ·x=Pϕxfor everyx∈Rd.
Then,Pϕis an orthogonal matrix, and by Lemma B.2
∂t(pt◦Pϕ) = (∂tpt)◦Pϕ=−(∇pt·∇logp)◦Pϕ−(pt∆ logp)◦Pϕ+ (∆pt)◦Pϕ
=−[∇(pt◦Pϕ)]T∇logp−(pt◦Pϕ)∆ logp+ ∆(pt◦Pϕ),
where the ﬁrst equation is because Pϕis independent to t, and the last equation follows from Lemma B.2
andlogp◦Pϕ= logp.
Therefore, we obtain the equation for ˜pt:
∂t˜pt=1
|Sd|/summationdisplay
ϕ∈Sd∂(pt◦Pϕ) =1
|Sd|/summationdisplay
ϕ∈Sd(−∇· ((pt◦Pϕ)∇logp) + ∆(pt◦Pϕ))
=−∇· (˜pt∇logp) + ∆˜pt. (8)
Combining with Lemma B.1 yields the boundary condition
∂˜pt
∂v(x) = 0,∀v∈Nx,x∈∂Cd. (9)
Proof of Lemma B.1. Since the group action is linear (i.e., ϕ·(x+y) =ϕ·x+ϕ·yandϕ·(tx) =tϕ·x),
we have
Dv˜f(x) = lim
t→0+1
t/parenleftbig˜f(x+tv)−˜f(x)/parenrightbig
=1
|Sd|/summationdisplay
ϕ∈SdDϕ·vf(ϕ·x).
To simplify the above summation, we introduce the coset ϕStabx:={ϕψ:ψ∈Stabx}for eachϕ∈Sd,
and the set of cosets Sd/Stabx:={ϕStabx:ϕ∈Sd}. Clearly, any two cosets are either equal or disjoint,
and the group Sdis partitioned by Sd/Stabx. The orbit-stabilizer theorem (Dummit & Foote, 2004, p. 114)
states that the map ϕStabx/mapsto→ϕ·xis a bijection between cosets Sd/Stabxand the orbit Sd·x, and thus8
Dv˜f(x) =1
|Sd|/summationdisplay
ϕ∈SdDϕ·vf(ϕ·x)
=1
|Sd|/summationdisplay
ϕ∈C
C=ψStabx∈Sd/StabxDϕ·vf(ϕ·x) (partition )
=1
|Sd|/summationdisplay
ϕ∈ψStabx
y:=ψ·x∈Sd·xDϕ·vf(ϕ·x) ( ψStabx/mapsto→ψ·xbijective )
=1
|Sd|/summationdisplay
y:=ψ·x∈Sd·x/summationdisplay
ϕ/prime∈StabxDψ·(ϕ/prime·v)f(y) (ϕ/prime:=ψ−1ϕ)
=1
|Sd|/summationdisplay
y:=ψ·x∈Sd·xDψ·Wx(v)f(y). (linearity of D(·)f)
This proves the ﬁrst claim.
For any interior point x∈C◦
d, we have Stabx={e}and thusWx(v) =v. For any boundary point x∈∂Cd,
the stabilizer subgroup is non-trivial, and it remains to show that Wx(v) = 0for normal vectors.
8It can be veriﬁed that the proof is independent on the choice of ψ.
17Under review as submission to TMLR
Table 1: Semi-synthetic experiment: average-case test risk for the best stopping iteration, for L= 2,W=
200.
Likelihood Algorithm boston concrete energy kin8nm naval power plant wine yacht
GaussianMALA 0.067 0.058 0.056 0.055 0.051 0.050 0.063 0.055
GD 0.068 0.059 0.056 0.055 0.051 0.050 0.063 0.056
ULA 0.067 0.058 0.056 0.055 0.051 0.050 0.063 0.055
LaplacianMALA 0.188 0.174 0.167 0.170 0.160 0.160 0.186 0.172
GD 0.189 0.175 0.167 0.170 0.161 0.160 0.186 0.173
ULA 0.188 0.175 0.167 0.170 0.161 0.160 0.186 0.171
An element ϕ∈Sdcan be identiﬁed as a permutation matrix Pϕ∈Rd×ds.t. the group action is the matrix-
vector multiplication ϕ·v=Pϕv, and clearly, the stabilizer of x∈∂Cdalways has the form of a Cartesian
product,/producttextmx
j=1Scj, where{cj}is s.t./summationtextmx
j=1cj=d.9Therefore, we have
Wx(v) =/summationdisplay
ϕ∈Stabxϕ·v=
/summationdisplay
ϕ∈/producttextmx
j=1ScjPϕ
v.
Note thatPϕ= blkdiag(P1,P2,...,Pmx),10with eachPj∈Rcj×cjbeing a permutation matrix, and the
sum of all size cjpermutation matrices is (cj−1)!1cj×cj, where 1denotes the all-ones matrix. Thus, by
decomposing Wx(v)∈RdintoRc1×Rc2×···× Rcmxwe have
Wx(v) =/parenleftbiggA0
c1s1/summationdisplay
i=s0+1vi1c1,A0
c2s2/summationdisplay
i=s1+1vi1c2, ...,A0
cmxsmx/summationdisplay
i=smx−1+1vi1cmx/parenrightbigg
,
whereA0=/producttextmx
j=1cj!andsj=/summationtext
l≤jcl.
Lete(j)∈Rdbe such that e(j)
k= 1ifsj−1<k≤sj, ande(j)
k= 0otherwise. Then a suﬃcient condition for
Wx(v) = 0is that/angbracketleftv,e(j)/angbracketright= 0for allj∈[mx]. Letnx∈Nxbe an inward normal vector and ﬁx j∈[mx].
Sincex±αje(j)∈Cdforαj= min(xsj−xsj−1,xsj+1−xsj)>0, we conclude that /angbracketleftnx,±e(j)/angbracketright≥0and hence
/angbracketleftnx,e(j)/angbracketright= 0. Thus,Wx(nx) = 0.
C Additional Results and Full Algorithm for Section 3.2
9For example, for x∈C5withx1=x2<x 3=x4<x 5, the stabilizer is S2×S2×S1.
10blkdiag(P1,P2,...,P mx)is the block diagonal matrix
P10··· 0
0P2··· 0
............
0 0···Pmx
∈Rd×d.
18Under review as submission to TMLR
Table 2: Semi-synthetic experiment: average-case test risk for the best stopping iteration, for L= 2,W= 50.
Likelihood Algorithm boston concrete energy kin8nm naval power plant wine yacht
GaussianMALA 0.071 0.058 0.053 0.054 0.052 0.050 0.063 0.057
GD 0.071 0.058 0.053 0.054 0.051 0.050 0.063 0.057
ULA 0.070 0.058 0.053 0.054 0.051 0.050 0.063 0.057
LaplacianMALA 0.194 0.172 0.170 0.167 0.161 0.160 0.186 0.167
GD 0.194 0.173 0.170 0.168 0.161 0.160 0.187 0.168
ULA 0.192 0.172 0.170 0.166 0.161 0.160 0.186 0.169
Table 3: Semi-synthetic experiment: average-case test risk for the best stopping iteration, for L= 3,W= 50.
Likelihood Algorithm boston concrete energy kin8nm naval power plant wine yacht
GaussianMALA 0.069 0.059 0.055 0.056 0.051 0.052 0.068 0.053
GD 0.070 0.059 0.056 0.056 0.051 0.051 0.069 0.054
ULA 0.069 0.059 0.055 0.056 0.051 0.050 0.068 0.053
LaplacianMALA 0.193 0.176 0.173 0.173 0.161 0.161 0.189 0.176
GD 0.197 0.176 0.172 0.173 0.161 0.161 0.190 0.175
ULA 0.194 0.176 0.172 0.172 0.161 0.161 0.190 0.174
Algorithm 1 The algorithm for evaluating (6) with MALA
Require: A training set Xtrainand a test set Xtest; a BNNfBNN(·;θ)and a prior pθ; a likelihood p(·|·).
Ensure: An approximation of (6).
1:fori= 1,..., 8do
2:θ(i)
∗∼pθ ⊿the groundtruth
3:Y(i)
train∼p(·|fBNN(Xtrain;θ(i)
∗))
4:Y(i)
test∼p(·|fBNN(Xtest;θ(i)
∗))
5: forj= 1,···,50do
6:θ(j)
init∼pθ ⊿the initial state
7:θ(j)
MALA←MALA (θ(j)
init,Xtrain,Y(i)
train) ⊿the posterior sample
8: end for
9: ifthe likelihood is Gaussian then
10:/lscript(ˆy,y) := (ˆy−y)2⊿the loss function derived from the likelihood
11: ˆf(i)(x) :=1
50/summationtext50
k=1fBNN(x;θ(k)
MALA ) ⊿the predictive function
12: else ifthe likelihood is Laplacian then
13:/lscript(ˆy,y) :=|ˆy−y|
14: ˆf(i)(x) :=median of{y(k):y(k)∼p(·|fBNN(x;θ(k)
MALA )),k= 1,..., 50}
15: end if
16:L(i)←1
|Xtest|/summationtext
(x,y)∈(Xtest,Y(i)
test)/lscript(ˆf(i)(x),y)
17:end for
18:L←1
8/summationtext8
i=1L(i)⊿the approximated average-case risk (6)
19Under review as submission to TMLR
2
 0 22
02I = 25
2
 0 2I = 175
2
 0 2I = 775
2
 0 2I = 3175
2
 0 2I = 102375
(a)L= 2,W= 20
2
 0 22
02I = 25
2
 0 2I = 175
2
 0 2I = 775
2
 0 2I = 3175
2
 0 2I = 102375
(b)L= 2,W= 50
2
 0 22
02I = 25
2
 0 2I = 175
2
 0 2I = 775
2
 0 2I = 3175
2
 0 2I = 102375
(c)L= 2,W= 500
Figure 4: Additional visualizations in the setting of Fig. 1.
20Under review as submission to TMLR
1021031041051060.060.080.100.12mean squared errorboston (n=404)
mala
gd
ula
Bayes error
1021031041051060.060.080.100.12concrete (n=824)
mala
gd
ula
Bayes error
1021031041051060.050.060.070.080.09energy (n=614)
mala
gd
ula
Bayes error
1021031041051060.060.080.10kin8nm (n=6553)
mala
gd
ula
Bayes error
102103104105106
# iterations0.0500.0550.0600.0650.070mean squared errornaval (n=9547)
mala
gd
ula
Bayes error
102103104105106
# iterations0.0500.0550.0600.0650.0700.0750.080power_plant (n=7654)
mala
gd
ula
Bayes error
102103104105106
# iterations0.060.080.100.12wine (n=1279)
mala
gd
ula
Bayes error
102103104105106
# iterations0.040.060.080.10yacht (n=246)
mala
gd
ula
Bayes error
(a) Gaussian likelihood / mean square error
1021031041051060.150.200.250.300.350.40mean absolute errorboston (n=404)
mala
gd
ula
Bayes error
1021031041051060.1750.2000.2250.2500.2750.300concrete (n=824)
mala
gd
ula
Bayes error
1021031041051060.160.180.200.220.240.260.28energy (n=614)
mala
gd
ula
Bayes error
1021031041051060.200.250.30kin8nm (n=6553)
mala
gd
ula
Bayes error
102103104105106
# iterations0.160.180.200.220.240.26mean absolute errornaval (n=9547)
mala
gd
ula
Bayes error
102103104105106
# iterations0.160.180.200.220.24power_plant (n=7654)
mala
gd
ula
Bayes error
102103104105106
# iterations0.150.200.250.300.350.40wine (n=1279)
mala
gd
ula
Bayes error
102103104105106
# iterations0.1500.1750.2000.2250.2500.2750.300yacht (n=246)
mala
gd
ula
Bayes error
(b) Laplace likelihood / mean absolute error
Figure 5: Semi-synthetic experiment: estimated loss (6) under diﬀerent likelihoods, for L= 2,W= 50.
21Under review as submission to TMLR
1021031041051060.060.080.100.120.14mean squared errorboston (n=404)
mala
gd
ula
Bayes error
1021031041051060.060.080.10concrete (n=824)
mala
gd
ula
Bayes error
1021031041051060.060.080.100.12energy (n=614)
mala
gd
ula
Bayes error
1021031041051060.060.080.100.120.14kin8nm (n=6553)
mala
gd
ula
Bayes error
102103104105106
# iterations0.050.060.070.08mean squared errornaval (n=9547)
mala
gd
ula
Bayes error
102103104105106
# iterations0.050.060.070.080.09power_plant (n=7654)
mala
gd
ula
Bayes error
102103104105106
# iterations0.060.080.100.120.14wine (n=1279)
mala
gd
ula
Bayes error
102103104105106
# iterations0.040.060.080.100.12yacht (n=246)
mala
gd
ula
Bayes error
(a) Gaussian likelihood / mean square error
1021031041051060.150.200.250.300.350.400.45mean absolute errorboston (n=404)
mala
gd
ula
Bayes error
1021031041051060.1750.2000.2250.2500.2750.300concrete (n=824)
mala
gd
ula
Bayes error
1021031041051060.150.200.250.300.35energy (n=614)
mala
gd
ula
Bayes error
1021031041051060.150.200.250.30kin8nm (n=6553)
mala
gd
ula
Bayes error
102103104105106
# iterations0.1750.2000.2250.2500.2750.300mean absolute errornaval (n=9547)
mala
gd
ula
Bayes error
102103104105106
# iterations0.160.180.200.220.240.26power_plant (n=7654)
mala
gd
ula
Bayes error
102103104105106
# iterations0.150.200.250.300.35wine (n=1279)
mala
gd
ula
Bayes error
102103104105106
# iterations0.150.200.250.300.350.40yacht (n=246)
mala
gd
ula
Bayes error
(b) Laplace likelihood / mean absolute error
Figure 6: Semi-synthetic experiment: estimated loss (6) under diﬀerent likelihoods, for L= 3,W= 50.
22