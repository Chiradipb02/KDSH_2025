Under review as submission to TMLR
Learning with Kan Extensions
Anonymous authors
Paper under double-blind review
Abstract
A common problem in machine learning is “use this function defined over this small set to
generate predictions over that larger set.” Extrapolation, interpolation, statistical inference
and forecasting all reduce to this problem. The Kan extension is a powerful tool in category
theorythatgeneralizesthisnotion. InthisworkweexploreapplicationsoftheKanextension
to machine learning problems. We begin by deriving a simple classification algorithm as a
Kan extension and experimenting with this algorithm on real data. Next, we use the Kan
extension to derive a procedure for learning clustering algorithms from labels and explore
the performance of this procedure on real data.
Although the Kan extension is usually defined in terms of categories and functors, this paper
assumes no knowledge of category theory. We hope this will enable a wider audience to learn
more about this powerful mathematical tool.
1 Introduction
A popular slogan in category theoretic circles, popularized by Saunders Mac Lane, is: “all concepts are Kan
extensions” (Mac Lane, 1971). While Mac Lane was partially referring to the fundamental way in which
many elementary category theoretic structures can be formulated as Kan extensions, there are many applied
areas that have Kan extension structure lying beneath the surface as well.
Casting a problem as a Kan extension can unveil hidden structure and suggest new avenues for exploration.
In this paper we aim to demonstrate what Kan extensions, and applied category theory more broadly, can
offer to machine learning researchers. We hope that our work will inspire more researchers to explore this
direction.
As a machine learning researcher it may be easiest to think of the Kan extension as tool for extrapolation.
We can use the Kan extension to expand a function over a small set to a similar function over a larger
set. However, the Kan extension perspective on extrapolation is fundamentally different from traditional
machine learning perspectives. Intuitively, traditional perspectives focus on means and sums whereas the
Kan extension perspective focuses on minimums and maximums. That is, a traditional machine learning
algorithm may try to extrapolate from data in a way that minimizes the total observed error. In contrast, an
algorithm derived from the Kan extension may try to solve a problem like “minimize false positives subject
to no false negatives on some set”.
In this paper we explore the ramifications of this difference across supervised and unsupervised learning
applications. To do this, we cast basic machine learning problems in category theoretic language, apply
the Kan extension, translate the result back to machine learning language, and study the behavior of the
resulting algorithms.
First, we derive a simple classification algorithm as a Kan extension and demonstrate experimentally that
this algorithm can learn to classify images. Next, we use Kan extensions to derive a novel method for
learning a clustering algorithm from labeled data and demonstrate experimentally that this method can
learn to cluster images. All code is available on GitHub.
For interested readers we include two additional examples of how Kan extensions can be applied to machine
learning in the Appendix. In Section A.2 we explore the structure of meta-supervised learning and use Kan
1Under review as submission to TMLR
extensions to derive supervised learning algorithms from sets of labeled datasets and trained functions. In
Section A.3 we use Kan extensions to characterize the process of approximating a complex function with a
simpler minimum description length (MDL) function.
2 Preliminaries
The foundational structures that we use in this paper are preorders and monotonic maps.
Definition 2.1. A preorder (P,≤)is a tuple of a set of objects Pand a reflexive, transitive relation ≤on
P.
Definition 2.2. A monotonic map f: (P1,≤1)→(P2,≤2)from the preorder (P1,≤1)to the preorder
(P2,≤2)is an order-preserving function from P1toP2. That is, for any x,y∈P1wherex≤1ywe have
f(x)≤2f(y).
For example, consider the preorder (Rn,≤∥∥)where forv,u∈Rnwe havev≤∥∥uwhen∥v∥≤∥u∥. Consider
also the preorder (Rn,≤∀)wherev≤∀uwhen∀i=1···n|vi|≤|ui|. The identity function id: (Rn,≤∀)→
(Rn,≤∥∥)is a monotonic map, but the identity function id: (Rn,≤∥∥)→(Rn,≤∀)is not a monotonic map.
Definition 2.3. (P,≤)is a discrete preorder if p1≤p2inPimpliesp1=p2
Definition 2.4. (P1,≤1)is a subpreorder of the preorder (P2,≤2)ifP1⊆P2and the identity function
id: (P1,≤1)→(P2,≤2)is a monotonic map.
For example, (Rn,≤∀)is a subpreorder of (Rn,≤∥∥). As another example, consider the preorder (Un,≤∀)
whereUnis the set of unit-norm vectors in Rn.(Un,≤∀)is a subpreorder of (Rn,≤∀).
In order to keep notation simple we will use bold characters like Ato represent the preorder (Ob(A),≤A).
In addition, given two monotonic maps f1,f2:A→Bwe will write f1≤f2to indicate that for all a∈A
we havef1(a)≤f2(a).
2.1 Kan Extensions
Now suppose we have three preorders A,B,Cand two monotonic maps G:A→B,K:A→Cand we
would like to derive the “best” monotonic map F:B→C:
B
A CFG
K
There are two canonical ways that we can do this.
Definition 2.5. The left Kan extension of K:A→CalongG:A→Bis the minimal monotonic map
LanGK:B→Csuch thatK≤(LanGK◦G).
That is, for any other monotonic map m:B→Csuch thatK≤(m◦G)we haveLanGK≤m.
Definition 2.6. The right Kan extension of K:A→CalongG:A→Bis the maximal monotonic map
Ran GK:B→Csuch that (Ran GK◦G)≤K.
That is, for any other monotonic map m:B→Csuch that (m◦G)≤Kwe havem≤Ran GK.
IfG:A◁arrowhookleft→Bis the inclusion map then the Kan extensions of KalongGare interpolations or extrapolations
ofKfromAto all of B. For example, suppose we want to interpolate a monotonic function K:Z→Rto
a monotonic function F:R→Rsuch thatF◦G=KwhereG:Z◁arrowhookleft→Ris the inclusion map.
2Under review as submission to TMLR
R
Z RFG
K
We have that LanGK:R→Ris simplyK◦floorandRan GK:R→Ris simplyK◦ceil, wherefloor,ceil
are the rounding down and rounding up functions respectively.
In this paper we explore a few applications of Kan extensions to machine learning. In each of these applica-
tions we first define preorders A,B,Cand a monotonic function K:A→Csuch that Ais a subpreorder of
BandG:A◁arrowhookleft→Bis the inclusion map. Then, we take the left and right Kan extensions LanGK,Ran GK
ofKalongGand study their behavior.
3 Classification
We start with a simple application of Kan extensions to supervised learning. Suppose that Iis a preorder,
I′⊆Iis a subpreorder of I, and{false,true}is the two element preorder where false <true. Suppose
also thatK:I′→{false,true}is a mapping into {false,true}and we would like to learn a monotonic
functionF:I→{false,true}that approximates KonI′. That is,Kdefines a finite training set of points
S={(x,K(x))|x∈I′}from which we wish to learn a monotonic function F:I→{false,true}. Of course,
it may not be possible to find a monotonic function that agrees with Kon all the points in I′.
I
I′{false,true}FG
K
We can solve this problem with the left and right Kan extensions of Kalong the inclusion map G:I′◁arrowhookleft→I.
Proposition 3.1. The left and right Kan extensions of K:I′→{false,true}along the inclusion map
G:I′◁arrowhookleft→Iare respectively:
LanGK:I→{false,true}Ran GK:I→{false,true}
LanGK(x) =/braceleftigg
true∃x′∈I′,x′≤x,K(x′) =true
false else
Ran GK(x) =/braceleftigg
false∃x′∈I′,x≤x′,K(x′) =false
true else
(Proof in Appendix A.1.1)
In the extreme case that Ob(I′) =∅, forx∈Iwe have that:
LanGK(x) =/parenleftigg/braceleftigg
true∃x′∈I′,x′≤x,K(x′) =true
false else/parenrightigg
=false
Ran GK(x) =/parenleftigg/braceleftigg
false∃x′∈I′,x≤x′,K(x′) =false
true else/parenrightigg
=true
3Under review as submission to TMLR
Similarly, in the extreme case that Ob(I′) =Ob(I)we have by the monotonicity of Kthat forx∈Iboth of
the following hold if and only if K(x) =true.
∃x′∈I′,x′≤x,K(x′) =true̸∃x′∈I′,x≤x′,K(x′) =false
Therefore in this extreme case we have LanGK(x) =Ran GK(x) =K(x).
Now suppose that I′contains at least one x′such thatK(x′) =true and at least one x′such thatK(x′) =
false. In this case LanGKandRan GKsplitIinto three regions: a region where both map all points to
false, a region where both map all points to true, and a disagreement region. Note that Ran GKhas no false
positives on I′andLanGKhas no false negatives on I′.
For example, suppose I=R,I′={1,2,3,4}and we have:
K(1) =falseK(2) =falseK(3) =trueK(4) =true
Then we have that:
LanGK(x) =/parenleftigg/braceleftigg
true∃x′∈I′,x′≤x,K(x′) =true
false else/parenrightigg
=/parenleftigg/braceleftigg
truex≥3
false else/parenrightigg
Ran GK(x) =/parenleftigg/braceleftigg
false∃x′∈I′,x≤x′,K(x′) =false
true else/parenrightigg
=/parenleftigg/braceleftigg
truex>2
false else/parenrightigg
In this case the disagreement region for LanGK,Ran GKis(2,3)and for any x∈(2,3)we haveLanGK(x)<
Ran GK(x).
As another example, suppose I=R,I′={5,6,7,8}and we have:
K(5) =falseK(6) =trueK(7) =falseK(8) =true
Then we have that:
LanGK(x) =/parenleftigg/braceleftigg
true∃x′∈I′,x′≤x,K(x′) =true
false else/parenrightigg
=/parenleftigg/braceleftigg
truex≥6
false else/parenrightigg
Ran GK(x) =/parenleftigg/braceleftigg
false∃x′∈I′,x≤x′,K(x′) =false
true else/parenrightigg
=/parenleftigg/braceleftigg
truex>7
false else/parenrightigg
In this case the disagreement region for LanGK,Ran GKis[6,7]and for any x∈[6,7]we haveRan GK(x)<
LanGK(x).
While this approach is effective for learning very simple mappings, there are many choices of Kfor which
LanGK,Ran GKdo not approximate Kparticularly well on I′and therefore the disagreement region is large.
In such a situation we can use a similar strategy to the one leveraged by kernel methods (Hofmann et al.,
2008) and transform Ito minimize the size of the disagreement region.
That is, we choose a preorder I∗and transformation f:I→I∗such that the size of the disagreement region
forLanf◦GK◦f,Ran f◦GK◦fis minimized.
I I∗
I′{false,true}f
F G
K
4Under review as submission to TMLR
For example, if I∗=Rawe can choose fto minimize the following loss:
Definition 3.2. Suppose we have a set I′⊆Iand function K:I′→{false,true}such that:
∃x′,x′′∈I′,K(x′) =true,K(x′′) =false
Then the ordering loss lmaps a function f:I→Rato an approximation of the size of the disagreement
region forLanf◦GK◦f,Ran f◦GK◦f. Formally, we define the ordering loss lto be:
l: (I→Ra)→R
l(f) =/summationdisplay
i≤amax(0,max{f(x)[i]|x∈I′,K(x) =false}−
min{f(x)[i]|x∈I′,K(x) =true})
wheref(x)[i]is theith component of the vector f(x)[i]∈Ra.
We can show that minimizing the ordering loss lwill also minimize the size of the disagreement region:
Proposition 3.3. The ordering loss l(Definition 3.2) is nonnegative and is only equal to 0 when ∀x∈I′
we have:
K(x) = (Lanf◦GK◦f)(x) = (Ran f◦GK◦f)(x)
Proof.First note that lmust be nonnegative since each term can be expressed as max(0,_). Next, suppose
thatl(f) = 0. Then it must be that for any x0,x1∈I′such thatK(x0) =false,K(x1) =true we have that
f(x0)≤f(x1). As a result, for any x∈I′there can only exist some x′∈I′wheref(x)≤f(x′),K(x′) =false
whenK(x) =false. Similarly, there can only exist some x′∈I′wheref(x′)≤f(x),K(x′) =true when
K(x) =true. Therefore:
K(x) = (Lanf◦GK◦f)(x) = (Ran f◦GK◦f)(x)
It is relatively straightforward to minimize the ordering loss with an optimizer like subgradient descent (Boyd
et al., 2003). For example, we can implement the ordering loss in Tensorflow (Abadi et al., 2015) as follows:
1 import numpy as np
2 import tensorflow as tf
3
4 def get_ordering_loss(model, X, y):
5 # model: Tensorflow sequential model
6 # X: 2D numpy float array in which each row is a feature vector
7 # y: 1D numpy boolean array of labels
8 X_false = X[np.logical_not(np.array(y, dtype=bool))]
9 false_preds = tf.transpose(model(X_false))
10
11 X_true = X[np.array(y, dtype=bool)]
12 true_preds = tf.transpose(model(X_true))
13
14 return tf.reduce_sum(tf.math.maximum(0,
15 tf.math.reduce_max(false_preds), axis=1) -
16 tf.math.reduce_min(true_preds), axis=1)
5Under review as submission to TMLR
Model Dataset True Positive Rate True Negative Rate
Left Kan Classifier Training 1.000 (±0.000) 0.612 (±0.042)
Right Kan Classifier Training 0.705 (±0.035) 1.000 (±0.000)
Left Kan Classifier Testing 0.815 (±0.020) 0.593 (±0.044)
Right Kan Classifier Testing 0.691 (±0.044) 0.837 (±0.026)
Table 1: True positive rate and true negative rate of the left Kan classifier Lanf◦GK◦fand the right
Kan classifier Ran f◦GK◦fwherefis a linear map trained to minimize the ordering loss l(f)(Definition
3.2) on the Fashion-MNIST “T-shirt” vs “shirt” task (Xiao et al., 2017). We run a bootstrap experiment
by repeatedly selecting 9000training samples and 1000testing samples, running the training procedure, and
computing true positive rate and true negative rate metrics. Mean and two standard error confidence bounds
from 10such bootstrap iterations are shown.
In Table 1 we demonstrate that we can use this strategy to distinguish between the “T-shirt” (false) and
“shirt” (true) categories in the Fashion MNIST dataset (Xiao et al., 2017). Samples in this dataset have
784features (pixels), so we train a simple linear model f:R784→R10with Adam (Kingma & Ba, 2014)
to minimize the ordering loss l(f)over a training set that contains 90%of samples in the dataset. We then
evaluate the performance of the left Kan classifer Lanf◦GK◦fand the right Kan classifier Ran f◦GK◦f
over both this training set and a testing set that contains the remaining 10%of the dataset. We look at two
metrics over both sets: the true positive rate and the true negative rate. Recall that the true positive rate
of a classifier is the proportion of all true samples which the classifier correctly labels as true and the true
negative rate of a classifier is the proportion of all false samples which the classifier correctly labels as false.
As we would expect from the definition of Kan extensions, the left Kan classifier Lanf◦GK◦fhas no false
negatives and the right Kan classifier Ran f◦GK◦fhas no false positives on the training set. The metrics
on the testing set are in line with our expectations as well: the left Kan classifier has a higher true positive
rate and the right Kan classifier has a higher true negative rate.
4 Clustering with Supervision
Clustering algorithms allow us to group points in a dataset together based on some notion of similarity
between them. Formally, we can consider a clustering algorithm as mapping a finite metric space (X,dX)to
a partition of X.
In most applications of clustering the points in the metric space (X,dX)are grouped together based solely
on the distances between the points and the rules embedded within the clustering algorithm itself. This is an
unsupervised clustering strategy since no labels or supervision influence the algorithm output. For example,
agglomerative clustering algorithms like HDBSCAN (McInnes & Healy, 2017) and single linkage partition
points inXbased on graphs formed from the points (vertices) and distances (edges) in (X,dX).
However, there are some circumstances under which we have a few ground truth examples of pre-clustered
training datasets and want to learn an algorithm that can cluster new data as similarly as possible to these
ground truth examples. We can define the supervised clustering problem as follows. Given a collection of
tuples
S={(X1,dX1,PX1),(X2,dX2,PX2),···,(Xn,dn,PXn)}
where each (Xi,dXi)is a finite metric space and PXiis a partition of Xi, we would like to learn a general
functionfthatmapsafinitemetricspace (X,dX)toapartition PXofXsuchthatforeach (Xi,dXi,PXi)∈S
the difference between f(Xi,dXi)andPXiis small.
In order to frame this objective in terms of Kan extensions we will first construct our preorder of metric
spaces.
6Under review as submission to TMLR
Definition 4.1. A nonexpansive map from the metric space (X,dX)to the metric space (Y,dY)is a function
f:X→Ysuch that for x1,x2∈Xwe have:
dY(f(x1),f(x2))≤dX(x1,x2)
Definition 4.2. In the preorder Met idthe setOb(Met id)consists of all metric spaces (X,dX)and
(X,dX)≤Met id(Y,dY)whenX⊆Yand the inclusion map ι: (X,dX)◁arrowhookleft→(Y,dY)is nonexpansive.
We can represent a clustering of a set Xwith a partition PXof that set. We can now construct our preorder
of partitions.
Definition 4.3. Consider the tuples (X,PX),(Y,PY)where PXis a partition of XandPYis a partition of
Y. Then a consistent map f: (X,PX)→(Y,PY)is a function f:X→Ysuch that for any set SX∈PX
there exists some set SY∈PYsuch thatf(SX)⊆SY.
Definition 4.4. In the preorder Part idthe setOb(Part id)consists of all partitions (X,PX)and
(X,PX)≤Part id(Y,PY)whenX⊆Yand the inclusion map ι: (X,PX)◁arrowhookleft→(Y,PY)is consistent.
We need one more condition to corral our definition of a clustering map.
Definition 4.5. We say that a monotonic map ffrom a subpreorder of Met idto a subpreorder of Part id
is well-behaved if for all (X,dX)in the domain of fwe have that f(X,dX) = (X,PX)for some partition PX
ofX.
Intuitively a well-behaved monotonic map from Met idtoPart idacts as the identity on underlying sets.
Now given a subpreorder D⊆Met id, a discrete preorder T⊆D, and a well-behaved monotonic map
K:T→Part id, our goal is to find the best well-behaved monotonic map F:D→Part idsuch that
F◦G=KwhereG:T◁arrowhookleft→Dis the inclusion map.
D(⊆Met id)
T(⊆D) Part idFG
K
Intuitively, Ob(T)is the set of unlabelled training samples, Kdefines the labels on these training samples,
andOb(D)is the set of testing samples.
We would like to use the Kan extensions of KalongGto find this best clustering map. However, these Kan
extensions are not guaranteed to be well-behaved. For example, consider the case in which Tis the discrete
preorder that contains the single-element metric space as its only object and Dis the discrete preorder that
contains two objects: the single-element metric space and Requipped with the Euclidean distance metric1.
{({∗},d{∗}),(R,dR)}
{({∗},d{∗})} Part idFG
K
1This counterexample due to Sam Staton
7Under review as submission to TMLR
Since Dis a discrete preorder, the behavior of Kon({∗},d{∗})will not affect the behavior of the left and
right Kan extensions of KalongGon(R,dR). For example, the left Kan extension of KalongGwill map
(R,dR)to the empty set and is therefore not well-behaved.
In order to solve this problem with Kan extensions we need to add a bit more structure. Suppose Ob(D)is
the discrete preorder with the same objects as Dand define the following:
Definition 4.6. The monotonic map KL:Ob(D)→Part idis equal to KonTand maps each object
(X,dX)inOb(D)−Ob(T)to(X,{{x}|x∈X}).
Definition 4.7. The monotonic map KR:Ob(D)→Part idis equal to KonTand maps each object
(X,dX)inOb(D)−Ob(T)to(X,{X}).
Intuitively, KLandKRare extensions of Kto all of the objects in D. For any metric space (X,dX)∈Met id
notinOb(T)themonotonicmap KLmaps (X,dX)tothefinestpossiblepartitionof XandKRmaps (X,dX)
to the coarsest possible partition of X.
Suppose we go back to the previous example in which Tis the discrete preorder containing only the single-
element metric space and Dis the discrete preorder containing both the single-element metric space and
(R,dR). Since:
KL(R,dR) = (R,{{x} |x∈R})
the left Kan extension of KLalong the inclusion G:Ob(D)◁arrowhookleft→Dmust map (R,dR)to the≤Part id-smallest
(X,PX)such that:
(R,{{x} |x∈R})≤Part id(X,PX)
which is (X,PX) = (R,{{x} |x∈R}). Similarly, since:
KR(R,dR) = (R,{R})
the right Kan extension of KRalong the inclusion G:Ob(D)◁arrowhookleft→Dmust map (R,dR)to the≤Part id-largest
(X,PX)such that:
(X,PX)≤Part id(R,{R})
which is (X,PX) = (R,{R}). We can apply the same logic to the behavior of the Kan extensions on the
single-element metric space as well, so both Kan extensions are well-behaved monotonic maps.
We can now build on this perspective to construct optimal extensions of K.
Proposition 4.8. Consider the monotonic map LanGKL:D→Part idthat sends the metric space
(X,dX)∈Dto the partition of Xdefined by the transitive closure of the relation Rwhere forx1,x2∈Xwe
havex1Rx 2if and only if there exists some metric space (X′,dX′)∈Twhere (X′,dX′)≤D(X,dX)and
x1,x2are in the same cluster in K(X′,dX′).
The mapLanGKL:D→Partis a well-behaved monotonic map. (Proof in Appendix A.1.2)
Proposition 4.9. Consider the map Ran GKR:D→Part idthat sends the metric space (X,dX)∈Dto
the partition of Xdefined by the transitive closure of the relation Rwhere forx1,x2∈Xwe havex1Rx 2if
and only if there exists no metric space (X′,dX′)∈Twhere (X,dX)≤D(X′,dX′)andx1,x2are in different
clusters inK(X′,dX′).
The mapRan GKR:D→Partis a well-behaved monotonic map (Proof in Appendix A.1.3)
We can now construct LanGKL,Ran GKRas Kan extensions.
Proposition 4.10. Suppose there exists some well-behaved monotonic map F:D→Part idwhereF◦G=
K.
ThenLanGKL:D→Part id(Proposition 4.8) is the left Kan extension of KL:Ob(D)→Part idalong the
inclusion map G:Ob(D)◁arrowhookleft→D.
8Under review as submission to TMLR
D(⊆Met id)
Ob(D) Part idLanGKLG
KL
In addition Ran GKR:D→Part id(Proposition 4.9) is the right Kan extension of KR:Ob(D)→Part id
along the inclusion map G:Ob(D)◁arrowhookleft→D.
D(⊆Met id)
Ob(D) Part idRanGKRG
KR
(Proof in Appendix A.1.4)
We will call LanGKLthe left Kan supervised clustering map and Ran GKRthe right Kan supervised clus-
tering map.
WhenOb(T) =∅we have for any (X,dX)∈Dthat:
LanGKL(X,dX) =KL(X,dX) = (X,{{x} |x∈X})
Ran GKR(X,dX) =KR(X,dX) = (X,{{X}})
In general for any metric space (X,dX)∈D−Ob(T)the monotonic maps LanGKL,Ran GKRrespectively
map (X,dX)to the finest (most clusters) and coarsest (fewest clusters) partitions of Xsuch that for any
metric space (X′,dX′)∈Twe have:
K(X′,dX′) =LanGKL(X′,dX′) =Ran GKR(X′,dX′)
andLanGKL,Ran GKRare monotonic maps. For example, suppose we have a metric space (X,dX)where
X={x1,x2,x3}. We can form the subpreorders T⊆D⊆Met idwhere:
Ob(T) ={({x1,x2},dX),({x1,x3},dX),({x2,x3},dX)}
Ob(D) =Ob(T)∪({x1,x2,x3},dX)
Tis a discrete preorder and we define ≤Dsuch thatS1≤DS2whenS1≤Met idS2. Now define K:T→
Part idto be the following monotonic map:
K({x1,x2},dX) ={{x1,x2}}
K({x1,x3},dX) ={{x1},{x3}}
K({x2,x3},dX) ={{x2},{x3}}
In this case we have that:
KL({x1,x2,x3},dX) ={{x1},{x2},{x3}}KR({x1,x2,x3},dX) ={{x1,x2,x3}}
9Under review as submission to TMLR
Since the only points that need to be put together are x1,x2and there is no metric space (X,dX)inD
where ({x1,x2,x3},dX)<D(X,dX)inD, we have:
LanGKL({x1,x2,x3},dX) ={{x1,x2},{x3}}
Ran GKR({x1,x2,x3},dX) ={{x1,x2,x3}}
As another example, suppose DisMet idandTis the discrete subpreorder of Dwhose objects are all metric
spaces with no more than 2elements. Define the following well-behaved monotonic map:
K({x1,x2},d) =/braceleftigg
{{x1,x2}}d(x1,x2)≤δ
{{x1},{x2}}else
Now for some metric space (X,dX)∈Dwith|X|>2and points x1,x2∈Xwe have that LanGKL
mapsx1,x2to the same cluster if and only if there exists some chain of points x1,···,x2inXwhere for
each pair of adjacent points x′
1,x′
2in this chain there exists some metric space ({x′
1,x′
2},dX′)∈Dwhere
({x′
1,x′
2},dX′)≤D(X,dX)andx′
1,x′
2are in the same cluster in K({x′
1,x′
2},dX′). This is the case if and
only ifdX(x′
1,x′
2)≤δ. Therefore, LanGKLmapsx1,x2to the same cluster if and only if x1,x2are in the
same connected component of the δ-Vietoris Rips complex of (X,dX). That is,LanGKLperforms the single
linkage clustering algorithm.
In contrast, since |X|>2there is no (X′,dX′)inTwhere (X,dX)≤D(X′,dX′). Therefore:
Ran GKR(X,dX) = (X,{X})
We can use this strategy to learn a clustering algorithm from real-world data. Recall that the Fashion
MNIST dataset (Xiao et al., 2017) contains images of clothing and the categories that each image falls into.
Suppose that we have two subsets of this dataset: a training set Xtrin which images are grouped by category
and a testing set Xteof ungrouped images. We can use UMAP (McInnes et al., 2018) to construct metric
spaces (Xtr,dXtr)and(Xte,dXte)from these sets.
Now suppose we would like to group the images in Xteas similarly as possible to the grouping of the images
inXtr.
For any collection of nonexpansive maps between (Xtr,dXtr)and(Xte,dXte)we can define subpreorders
T⊆D⊆Met idand monotonic map K:T→Part idas follows:
1. Initialize Ttoanemptypreorderand Dtobethediscretepreorderwithasingleobject {(Xte,dXte)}.
2. For every nonexpansive map f: (Xtr,dXtr)→(Xte,dXte)in our collection and pair (x1,x2)∈Xtr
of samples in the same clothing category, add the object ({f(x1),f(x2)},dXte)toTandDwhere:
({f(x1),f(x2)},dXte)≤D(Xte,dXte)
and defineK({f(x1),f(x2)},dXte)to mapf(x1)andf(x2)to the same cluster.
3. For every nonexpansive map f: (Xte,dXte)→(Xtr,dXtr)in our collection define a metric space
(X′
te,dX′
te)whereXte=X′
teanddXte=dX′
te. Add the object (X′
te,dX′
te)toTandDwhere:
(Xte,dXte)≤D(X′
te,dX′
te)and defineK(X′
te,dX′
te)to be the partition of X′
tedefined by the preim-
ages of the function (h◦f)wherehmaps each element of Xtrto the category of clothing it belongs
to.
We can now use LanGKLandRan GKRto partition Xte.
In Figure 1 we compare the clusterings produced by LanGKL,Ran GKRto the ground truth clothing cate-
gories. As a baseline we compute the δ-single linkage clustering algorithm with δchosen via line search to
maximize the adjusted Rand score (Hubert & Arabie, 1985; Pedregosa et al., 2011) with the ground truth
labels.
As expected, we see that LanGKLproduces a finer clustering (more clusters) than does Ran GKRand that
the clusterings produced by LanGKLandRan GKRare better than the clustering produced by single linkage
in the sense of adjusted Rand score with ground truth.
10Under review as submission to TMLR
Figure 1: Cluster assignments of a 100point testing set Xtefrom the Fashion MNIST dataset (Xiao
et al., 2017) shown in UMAP space (McInnes et al., 2018). Each color corresponds to a unique cluster, and
points without clusters are shown as black squares. We show ground truth clothing categories, unsupervised
δ-single linkage cluster assignments ( δchosen via line search), and the LanGKL,Ran GKRsupervised cluster
assignments. The LanGKL,Ran GKRalgorithms are trained on a separate 1000point random sample Xtr
from the Fashion MNIST dataset.
11Under review as submission to TMLR
Frequency that Left Kan Clustering
Beats Best δ-Single LinkageFrequency that Right Kan Clustering
Beats Best δ-Single Linkage
0.860 (±0.068) 0.680 (±0.091)
Table 2: We compare the performance of the left and right Kan supervised clustering maps on the Fashion
MNIST dataset to the performance of δ-single linkage clustering with an optimal choice of δ. We select
100bootstrap training and testing samples of the Fashion MNIST dataset. We then train and evaluate each
method on each such sample. To perform this evaluation we use the scikit-learn implementation of the
Adjusted Rand Score (Pedregosa et al., 2011) to compare the algorithmically generated clusterings with the
ground truth categorization. We then compute the frequency with which the left and right Kan supervised
clustering maps perform better (have a higher Adjusted Rand Score with ground truth) than choosing the
optimal value of δfor single linkage. The win rates and two standard error confidence bounds from the 100
experiments are shown. We see that the left and right Kan clustering maps both perform consistently better
than single linkage.
5 Related Work
Some authors have begun to explore Kan extension structure in topological data analysis. For example,
Bubenik et al. (2017) describe how three mechanisms for interpolating between persistence modules can
be characterized as the left Kan extension, right Kan extension, and natural map from left to right Kan
extension. Similarly, McCleary & Patel (2021) use Kan extensions to characterize deformations of filtra-
tions. Furthermore, Botnan & Lesnick (2018) use Kan extensions to generalize stability results from block
decomposable persistence modules to zigzag persistence modules and Curry (2013) uses Kan extensions to
characterize persistent structures from the perspective of sheaf theory.
Other authors have explored the application of Kan extensions to databases. For example, in categorical
formulations of relational database theory (Spivak & Wisnesky, 2015; Schultz & Wisnesky, 2017; Schultz
et al., 2016), the left Kan extension can be used for data migration. Spivak & Wisnesky (2020) exploit the
characterization of data migrations as Kan extensions to apply the chase algorithm from relational database
theory to the general computation of the left Kan extension.
6 Future Work
In this paper we demonstrate that Kan extensions can be used to derive many different kinds of supervised
learning algorithms. However, these algorithms are inherently focused on extreme values (minimums and
maximums) rather than averages. Averages are required to build algorithms that are robust to noise, and a
potential future direction for this work is to extend these algorithms to incorporate averages. For example,
we may be able to combine multiple Kan classifiers together to generate a robust Kan classifier ensemble. It
may even be possible to apply a boosting approach in which we minimize the ordering loss, fit Kan classifiers,
and then repeat on the samples in the disagreement region.
References
Martín. Abadi et al. TensorFlow: Large-scale machine learning on heterogeneous systems. 2015. URL
https://www.tensorflow.org/ .
Magnus Botnan and Michael Lesnick. Algebraic stability of zigzag persistence modules. Algebraic & Geo-
metric Topology , 18(6), Oct 2018. ISSN 1472-2747. doi: 10.2140/agt.2018.18.3133.
Stephen Boyd, Lin Xiao, and Almir Mutapcic. Subgradient methods. Course notes at Stanford University ,
2003. URL https://web.stanford.edu/class/ee392o/subgrad_method.pdf .
Peter Bubenik, Vin de Silva, and Vidit Nanda. Higher interpolation and extension for persistence
modules. SIAM Journal on Applied Algebra and Geometry , 1(1), Jan 2017. ISSN 2470-6566. doi:
10.1137/16m1100472.
12Under review as submission to TMLR
Justin M. Curry. Sheaves, cosheaves and applications. 2013. doi: 10.1.1.363.2881.
Thomas Hofmann, Bernhard Schölkopf, and Alexander J Smola. Kernel methods in machine learning. The
annals of statistics , 36(3), 2008.
Lawrence Hubert and Phipps Arabie. Comparing partitions. Journal of classification , 2(1), 1985.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. 2014. URL https:
//arxiv.org/abs/1412.6980 .
Saunders Mac Lane. Categories for the Working Mathematician . New York, 1971.
Alexander McCleary and Amit Patel. Edit distance and persistence diagrams over lattices. 2021. URL
https://arxiv.org/abs/2010.07337 .
Leland McInnes and John Healy. Accelerated hierarchical density clustering. 2017. URL https://arxiv.
org/abs/1705.07321 .
Leland McInnes, John Healy, and James Melville. UMAP: Uniform manifold approximation and projection
for dimension reduction. 2018. URL https://arxiv.org/abs/1802.03426 .
Fabian Pedregosa et al. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research ,
12, 2011.
Jorma Rissanen. Modeling by shortest data description. Automatica , 14(5):465–471, 1978. ISSN 0005-1098.
doi: https://doi.org/10.1016/0005-1098(78)90005-5. URL https://www.sciencedirect.com/science/
article/pii/0005109878900055 .
Patrick Schultz and Ryan Wisnesky. Algebraic data integration. 2017. URL https://arxiv.org/abs/
1503.03571 .
Patrick Schultz, David I Spivak, and Ryan Wisnesky. Algebraic model management: A survey. In Interna-
tional Workshop on Algebraic Development Techniques . Springer, 2016.
David I. Spivak and Ryan Wisnesky. Relational foundations for functorial data migration. 2015. URL
https://arxiv.org/abs/1212.5303 .
David I Spivak and Ryan Wisnesky. Fast left-Kan extensions using the chase. Preprint. Available at www.
categoricaldata. net , 2020.
Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-MNIST: a novel image dataset for benchmarking
machine learning algorithms. 2017. URL https://arxiv.org/abs/1708.07747 .
A Appendix
A.1 Proofs
A.1.1 Proof of Proposition 3.1
Proof.We first need to show that LanGK,Ran GKare monotonic. For any x1≤x2∈Isuppose that
LanGK(x1) =true. Then∃x′∈I′,x′≤x1,K(x′) =true. By transitivity we have x′≤x2, so:
LanGK(x2) =/parenleftigg/braceleftigg
true∃x′∈I′,x′≤x2,K(x′) =true
false else/parenrightigg
=true
andLanGKis therefore monotonic.
13Under review as submission to TMLR
Next, for any x1≤x2∈Isuppose that Ran GK(x2) =false. Then∃x′∈I′,x2≤x′,K(x′) =false. By
transitivity we have x1≤x′, so:
Ran GK(x1) =/parenleftigg/braceleftigg
false∃x′∈I′,x1≤x′,K(x′) =false
true else/parenrightigg
=false
andRan GKis therefore monotonic.
Next we will show that LanGKis the left Kan extension of KalongG. If for some x′∈I′we have that
K(x′) =true then:
LanGK(x′) =/parenleftigg/braceleftigg
true∃x′′∈I′,x′′≤x′,K(x′′) =true
false else/parenrightigg
=true
so we can conclude that K≤(LanGK◦G). Now consider any other monotonic map ML:I→{false,true}
such that∀x′∈I′,K(x′)≤ML(x′). We must show that ∀x∈I,Lan GK(x)≤ML(x). For some x∈I
supposeML(x) =false. Then since MLis a monotonic map it must be that ∀x′∈I′,x′≤x,M L(x′) =false.
SinceK≤(ML◦G)it must be that∀x′∈I′,x′≤x,K(x′) =false. Therefore LanGK(x) =false.
Next we will show that Ran GKis the right Kan extension of KalongG. If for some x′∈I′we have that
K(x′) =false then:
Ran GK(x′) =/parenleftigg/braceleftigg
false∃x′′∈I′,x′≤x′′,K(x′′) =false
true else/parenrightigg
=false
so we can conclude that (Ran GK◦G)≤K. Now consider any other monotonic map MR:I→{false,true}
such that∀x′∈I′,MR(x′)≤K(x′). We must show that ∀x∈I,MR(x)≤Ran GK(x). For some x∈I
supposeMR(x) =true. Then since MRis a monotonic map it must be that ∀x′∈I′,x≤x′,MR(x′) =true.
Since (MR◦G)≤Kit must be that∀x′∈I′,x≤x′,K(x′) =true. Therefore Ran GK(x) =true.
A.1.2 Proof of Proposition 4.8
Proof.LanGKLtrivially acts as the identity on underlying sets so we simply need to show that when:
(X,dX)≤D(Y,dY)
then
LanGKL(X,dX)≤Part idLanGKL(Y,dY)
Suppose there exists some x,x∗∈Xin the same cluster in LanGKL(X,dX). Then by the definition of
LanGKLthere must exist some sequence
(X1,dX1),(X2,dX2),···,(Xn,dXn)∈T
wherex∈X1,x∗∈Xnand each:
(Xi,dXi)≤D(X,dX)
as well as some sequence
x1,x2,···,xn−1,such thatxi∈Xi,xi∈Xi+1
where the pair (x,x 1)is in the same cluster in K(X1,dX1), the pair (xn−1,x∗)is in the same cluster in
K(Xn,dXn), and for each 1<i<n the pair (xi−1,xi)is in the same cluster in K(Xi,dXi). Since it must
be that each:
(Xi,dXi)≤D(Y,dY)
as well then by the definition of LanGKLit must be that x,x∗are in the same cluster in LanGKL(Y,dY).
14Under review as submission to TMLR
A.1.3 Proof of Proposition 4.9
Proof.Ran GKRtrivially acts as the identity on underlying sets so we simply need to show that when:
(X,dX)≤D(Y,dY)
then:
Ran GKR(X,dX)≤Part idRan GKR(Y,dY)
Suppose the points x,x∗∈Xare in the same cluster in Ran GKR(X,dX). Then by the definition of Ran GKR
there cannot be any (X′,dX′)inTsuch that:
(X,dX)≤D(X′,dX′)
andx,x∗are in different clusters in Ran GKR(X′,dX′). By transitivity this implies that there cannot be any
(X′′,dX′′)inTsuch that:
(Y,dY)≤D(X′′,dX′′)
andx,x∗are in different clusters in Ran GKR(X′′,dX′′). By the definition of Ran GKRthe pointsx,x∗must
therefore be in the same cluster in Ran GKR(Y,dY).
A.1.4 Proof of Proposition 4.10
We use the following Proposition in the proof below:
PropositionA.1. Suppose there exists some well-behaved monotonic map F:D→Part idwhereF◦G=K.
Then for (X,dX)∈Twe have that:
F(X,dX) =K(X,dX) =LanGKL(X,dX) =Ran GKR(X,dX)
Proof.Since each of:
F:D→Part
Ran GKR:D→Part
LanGKL:D→Part
are well-behaved monotonic maps we simply need to prove that all three maps generate the same partition
ofXfor any input (X,dX)∈T.
Consider some (X,dX)∈Tand two points x,x∗∈X. Supposex,x∗are in different clusters in
K(X,dX) =F(X,dX)
. Then since Fis a well-behaved monotonic map it must be that for any sequence
(X1,dX1),(X2,dX2),···,(Xn,dXn)∈T
wherex∈X1,x∗∈Xnand each:
(Xi,dXi)≤D(X,dX)
and any sequence
x1,x2,···,xn−1,such thatxi∈Xi,xi∈Xi+1
one of the following must be true:
15Under review as submission to TMLR
•The pair (x,x 1)are in different clusters in F(X1,dX1)
•The pair (xn−1,x∗)are in different clusters in F(Xn,dXn)
•For some 1<i<nthe pair (xi−1,xi)are in different clusters in F(Xi,dXi)
This implies that in LanGKL(X,dX)the points x,x∗must be in different clusters. Similarly, since
(X,dX)≤D(X,dX), by Proposition 4.9 it must be that x,x∗are in different clusters in Ran GKR(X,dX).
Now suppose x,x∗are in the same cluster in:
K(X,dX) =F(X,dX)
Since (X,dX)≤D(X,dX), by Proposition 4.8 it must be that x,x∗are in the same cluster in
LanGKL(X,dX). Similarly, since Fis a well-behaved monotonic map there cannot exist any metric space
(X′,dX′)∈Twhere:
(X,dX)≤D(X′,dX′)
andx,x∗are in different clusters in:
K(X′,dX′) =F(X′,dX′)
Thereforex,x∗are in the same cluster in Ran GKR(X,dX).
Now we can prove Proposition 4.10:
Proof.To start, note that Proposition A.1 implies that for any (X,dX)∈Twe have:
LanGKL(X,dX) =K(X,dX) =Ran GKR(X,dX)
By the definition of KL,KRwe can therefore conclude that for any (X,dX)∈Dwe have:
KL(X,dX)≤Part idLanGKL(X,dX)
Ran GKR(X,dX)≤Part idKR(X,dX)
Next, consider any monotonic map ML:D→Part idsuch that for all (X,dX)∈Dwe have:
KL(X,dX)≤Part id(ML◦G)(X,dX)
We must show that for any (X,dX)∈Dwe have:
LanGKL(X,dX)≤Part idML(X,dX)
To start, note that for any x,x∗∈Xthat are in the same cluster in LanGKL(X,dX)by the definition of
LanGKLthere must exist some sequence:
(X1,dX1),(X2,dX2),···,(Xn,dXn)∈T
wherex∈X1,x∗∈Xnand each:
(Xi,dXi)≤D(X,dX)
as well as some sequence
x1,x2,···,xn−1,such thatxi∈Xi,xi∈Xi+1
16Under review as submission to TMLR
where the pair (x,x 1)is in the same cluster in KL(X1,dX1), the pair (xn−1,x∗)is in the same cluster in
KL(Xn,dXn), and for each 1< i < n the pair (xi−1,xi)is in the same cluster in KL(Xi,dXi). Now since
for each (Xi,dXi)in this sequence we have that:
KL(Xi,dXi)≤Part idML(Xi,dXi)
it must be that the pair (x,x 1)is in the same cluster in ML(X1,dX1), the pair (xn−1,x∗)is in the same
cluster inML(Xn,dXn), and for each 1<i<nthe pair (xi−1,xi)is in the same cluster in ML(Xi,dXi).
SinceMLis a monotonic map it must therefore be that the pair x,x∗is in the same cluster in ML(X,dX)
and therefore:
LanGKL(X,dX)≤Part idML(X,dX)
Next, consider any monotonic map MR:D→Part idsuch that for all (X,dX)inD:
(MR◦G)(X,dX)≤Part idKR(X,dX)
We must show that for any (X,dX)inDwe have:
MR(X,dX)≤Part idRan GKR(X,dX)
To start, note that for any x,x∗∈Xsuch thatx,x∗are not in the same cluster in Ran GKR(X,dX)by the
definition of Ran GKRthere must exist some:
(X′,dX′)∈D,(X,dX)≤D(X′,dX′)
wherex,x∗are not in the same cluster in KR(X′,dX′). Now since:
MR(X′,dX′)≤Part idKR(X′,dX′)
it must be that x,x∗are not in the same cluster in MR(X′,dX′). SinceMRis a monotonic map we have:
MR(X,dX)≤Part idMR(X′,dX′)
sox,x∗are also not in the same cluster in MR(X,dX)and therefore:
MR(X,dX)≤Part idRan GKR(X,dX)
A.2 Meta-Supervised Learning
SupposeIis a set and Ois a partial order. A supervised learning algorithm maps a labeled dataset (set of
pairs of points in I×O) to a function f:I→O. For example, both LanGKandRan GKfrom Section 3
are supervised learning algorithms.
In this section we use Kan extensions to derive supervised learning algorithms from pairs of datasets and
functions. Our construction combines elements of Section 3’s point-level algorithms and Section 4’s dataset-
level algorithms.
Suppose we have a finite partial order Sf⊆(I→O)of functions where for f,f′∈Sfwe havef≤f′when
∀x∈I,f(x)≤f′(x).
Proposition A.2. For any subset S∗
f⊆Sfthe upper antichain of S∗
fis the set:
{f|f∈S∗
f,̸∃f∗∈S∗
f,f <f∗}}
The upper antichain of S∗
fis an antichain in S∗
f, and for any function f∈S∗
fthere exists some function f∗
in the upper antichain of S∗
fsuch thatf≤f∗.
17Under review as submission to TMLR
Proof.Supposef1,f2are in the upper antichain of S∗
f⊆Sfandf1≤f2. Then since
̸∃f∗
1∈S∗
f,f1<f∗
1
it must be that f1=f2and we can conclude that the upper antichain is an antichain.
Next, for any function f∈S∗
fconsider the set{f∗∈S∗
f,f <f∗}. SinceSfis finite this set must have finite
size. If this set is empty then fis in the upper antichain of S∗
f. If this set has size nthen for any f∗in
this set the set{f∗∗∈S∗
f,f∗<f∗∗}must have size strictly smaller than n. We can therefore conclude by
induction that the upper antichain of S∗
fcontains at least one function f∗wheref≤f∗.
Intuitively the upper antichain of S∗
fis the collection of all functions f∈S∗
fthat are not strictly upper
bounded by any other function in S∗
f. The upper antichain of an empty set is of course itself an empty set.
Definition A.3. We can form the following preorders:
DC: The objects in DCare≤-antichains of functions Xf⊆Sf.DCis a preorder in which Xf≤X′
f
if forf∈Xfthere must exist some f′∈X′
fwheref≤f′.
DB: The objects in DBare labeled datasets, or sets of pairs U={(x,y)|x∈I,y∈O}.DBis a
preorder such that U≤U′when for all (x,y′)∈U′there exists (x,y)∈Uwherey≤y′.
DA: A subpreorder of DBsuch that if U≤U′∈DBthenU≤U′∈DA.
Proposition A.4. DBandDCare preorders.
Proof.
DC
We trivially have Xf≤XfinDC. To see that≤is transitive in DCsimply note that if Xf1≤Xf2and
Xf2≤Xf3then forf1∈Xf1there must exist f2∈Xf2,f1≤f2, which implies that there must exist
f3∈Xf3,f1≤f2≤f3.
DB
We trivially have U≤UinDB. To see that≤is transitive in DBsimply note that if U1≤U2andU2≤U3
inDBthen for (x,y 3)∈U3there must exist (x,y 2)∈U2,y2≤y3which implies that there must exist
(x,y 1)∈U1,y1≤y2≤y3.
Intuitively, DAis a collection of labeled training datasets and DBis a collection of labeled testing datasets.
We can define a monotonic map that maps each training dataset to all of the trained models that agree with
that dataset.
Proposition A.5. The mapK:DA→DCthat maps the object U∈DAto the upper antichain of the
following set:
SK(U) ={f|f∈Sf,∀(x,y)∈U,f(x)≤y}
is a monotonic map.
Proof.To start, note that Kmaps objects in DAto objects in DCsince the upper antichain of SK(U)must
be an antichain in Sfby Proposition A.2.
Next, we need to show that if U≤U′thenK(U)≤K(U′). For anyx,y′∈U′it must be that there exists
(x,y)∈Uwherey≤y′, so iff∈K(U)then by the definition of Kwe havef(x)≤y≤y′. Therefore
f∈SK(U′), so by Proposition A.2 K(U′)containsf′wheref≤f′. Therefore K(U)≤K(U′).
Now define G:DA◁arrowhookleft→DBto be the inclusion map. A monotonic map F:DB→DCsuch thatF◦G
commutes with Kwill then be a mapping from the testing datasets in DBto collections of trained models.
18Under review as submission to TMLR
DB
DA DCFG
K
We can take the left and right Kan extensions of Kalong the inclusion map G:DA◁arrowhookleft→DBto find the
optimal such mapping.
Proposition A.6. The mapLanGKthat maps the object U∈DBto the upper antichain of the following
set:
SL(U) =/uniondisplay
{U′|U′∈DA,U′≤U}K(U′)
is the left Kan extension of KalongG.
Next, the map Ran GKthat maps the object U∈DBto the upper antichain of the following set:
SR(U) ={f|f∈Sf,∀U′∈{U′|U′∈DA,U≤U′},∃f′∈K(U′), f≤f′}
is the right Kan extension of KalongG.
Proof.We first need to show that LanGKis a monotonic map DB→DC. Note that LanGKmaps objects
inDBto objects in DCsince the upper antichain of SL(U)must be an antichain in Sf.
Next, suppose U1≤U2and thatf∈LanGK(U1). Consider the set of all U′∈DAwhereU′≤U1. Since
U1≤U2this is a subset of the set of all U′∈DAwhereU′≤U2. SinceSL(U1)is defined to be a union of
the elements in the set we have that SL(U1)⊆SL(U2). Sincef∈LanGK(U1)implies that f∈SL(U1)this
implies that f∈SL(U2)as well. Proposition A.2 then implies that there must exist f′∈LanGK(U2)where
f≤f′and therefore LanGK(U1)≤LanGK(U2).
Next, we will show that LanGKis the left Kan extension of KalongG.
•Consider some U∈DAandf∈K(U). SinceU≤Uwe have by the definition of SLthat
f∈SL(U). Proposition A.2 then implies that ∃f′∈LanGK(U)such thatf≤f′. This implies that
K≤LanGK◦G.
•Now consider any monotonic map ML:DB→DCsuch thatK≤(ML◦G). We must show that
LanGK≤ML. For some U∈DBsupposef∈LanGK(U). By the definition of SLthere must
exist some U′∈DAwhereU′≤Usuch thatf∈K(U′). SinceK(U′)≤ML(U′)there must exist
somef′∈ML(U′)wheref≤f′. SinceMLis a monotonic map we have ML(U′)≤ML(U)which
implies that there must exist some f∗∈ML(U)wheref≤f′≤f∗. Therefore LanGK≤ML.
Next, we need to show that Ran GKis a monotonic map DB→DC. Note that Ran GKmaps objects in
DBto objects in DCsince the upper antichain of SR(U)must be an antichain in Sf.
Next, suppose U1≤U2and thatf∈Ran GK(U1). Consider the set of all U′∈DAwhereU2≤U′. Since
U1≤U2this is a subset of the set of all U′∈DAwhereU1≤U′. Therefore by the definition of SRwe
have thatSR(U1)⊆SR(U2). Sincef∈Ran GK(U1)implies that f∈SR(U1)this implies that f∈SR(U2)
as well. Proposition A.2 then implies that there must exist f′∈Ran GK(U2)wheref≤f′and therefore
Ran GK(U1)≤Ran GK(U2).
Next, we will show that Ran GKis the right Kan extension of KalongG.
19Under review as submission to TMLR
•ForU∈DAsinceU≤Uwhenf∈SR(U)we have by the definition of SRthat∃f′∈K(U)such
thatf≤f′. SinceRan GK(U)is a subset of SR(U)this implies that Ran GK◦G≤K.
•Now consider any monotonic map MR:DB→DCsuch that (MR◦G)≤K. We must show
thatMR≤Ran GK. For some U∈DBsupposef∈MR(U). SinceMRis a monotonic map
it must be that for all U′∈DAwhereU≤U′we have that MR(U)≤MR(U′)and therefore
∃f′
MR∈MR(U′),f≤f′
MR. Since (MR◦G)≤Kthis implies that for all U′∈DAwhereU≤U′
we have that∃f′
K∈K(U′),f≤f′
MR≤f′
K. By the definition of SRthis implies that f∈SR(U).
Proposition A.2 therefore implies that there exists f′
R∈Ran GK(U)such thatf≤f′
R, and therefore
MR(U)≤Ran GK(U).
Intuitivelythefunctionsin Ran GK(U)andLanGK(U)areaslargeaspossiblesubjecttoconstraintsimposed
by the selection of sets in Ob(DA). The functions in LanGK(U)are subject to a membership constraint and
grow smaller when we remove objects from Ob(DA). The functions in Ran GK(U)are subject to an upper
boundedness-constraint and grow larger when we remove objects from Ob(DA).
Consider the extreme case where Ob(DA) =∅. For anyU∈DBwe have that:
SL(U) =/uniondisplay
{U′|U′∈∅,···}K(U′) =∅
SR(U) ={f|f∈Sf,∀U′∈∅,···} =Sf
soLanGK(U)is empty and Ran GK(U)is the upper antichain of Sf.
Now consider the extreme case where Ob(DA) =Ob(DB). For anyU∈DBandf∈K(U)the monotonicity
ofKimplies that:
∀U′∈{U′|U′∈DA,U≤U′},∃f′∈K(U′), f≤f′
and therefore f∈SR(U). This implies K(U)≤Ran GK(U). Similarly, for any f∈LanGK(U)it must be
that:
∃U′∈DA, U′≤U, f∈K(U′)
which by the monotonicity of Kimplies that:
∃f∗∈K(U),f≤f∗
and therefore LanGK(U)≤K(U). Therefore in this extreme case we have:
Ran GK(U) =LanGK(U) =K(U)
Let’s now consider a more concrete example. Suppose I=R2
≥0,O={false,true}, andSfis the finite set of
linear classifiers l:R2
≥0→{false,true}that can be expressed as:
la,b(x1,x2) =/braceleftigg
truex2≤a∗x1+b
false else
wherea,bare integers in (−100,100). Intuitively:
•The classifiers in LanGK(U)are selected to be the classifiers that predict true as often as possible
among the set of all classifiers that have no false positives on some U′∈DAwhereU′≤U.
20Under review as submission to TMLR
•The classifiers in Ran GK(U)are constructed to predict true as often as possible subject to a con-
straint imposed by the selection of sets in DA. For every set U′∈DAwhereU≤U′it must be
that each classifier in Ran GK(U)is upper bounded at each point in Iby some classifier in Sfwith
no false positives on U′.
A concrete example will demonstrate this. Suppose that DAis:
{((2,2),true),((1,3),true),((4,4),false)}
{((2,2),true),((1,3),false),((4,4),true)}{((2,2),false),((1,3),false),((4,4),false)}≤≤
and that DBis:
{((2,2),true),((1,3),true),((4,4),false)}
{((2,2),true),((1,3),false),((4,4),true)}{((2,2),true),((1,3),false),((4,4),false)}
{((2,2),false),((1,3),false),((4,4),false)}≤≤
≤
We can see the following:
•l(1,1)∈K({((2,2),true),((1,3),false),((4,4),true)})since:
l(1,1)(1,3) =/parenleftigg/braceleftigg
true 3≤1∗1 + 1
false else/parenrightigg
=false
but we have that:
l(1,2)(1,3) =/parenleftigg/braceleftigg
true 3≤1∗1 + 2
false else/parenrightigg
=true
l(2,1)(1,3) =/parenleftigg/braceleftigg
true 3≤2∗1 + 1
false else/parenrightigg
=true
•l(0,2)∈K({((2,2),true),((1,3),false),((4,4),true)})since:
l(0,2)(1,3) =/parenleftigg/braceleftigg
true 3≤0∗1 + 2
false else/parenrightigg
=false
but we have that:
l(0,3)(1,3) =/parenleftigg/braceleftigg
true 3≤0∗1 + 3
false else/parenrightigg
=true
l(1,2)(1,3) =/parenleftigg/braceleftigg
true 3≤1∗1 + 2
false else/parenrightigg
=true
21Under review as submission to TMLR
•l(0,3)∈K({((2,2),true),((1,3),true),((4,4),false)})since:
l(0,3)(4,4) =/parenleftigg/braceleftigg
true 4≤0∗4 + 3
false else/parenrightigg
=false
but we have that:
l(1,3)(4,4) =/parenleftigg/braceleftigg
true 4≤1∗4 + 3
false else/parenrightigg
=true
l(0,4)(4,4) =/parenleftigg/braceleftigg
true 4≤0∗4 + 4
false else/parenrightigg
=true
•l(0,1)∈K({((2,2),false),((1,3),false),((4,4),false)})since:
l(0,1)(2,2) =/parenleftigg/braceleftigg
true 2≤0∗2 + 1
false else/parenrightigg
=false
l(0,1)(1,3) =/parenleftigg/braceleftigg
true 3≤0∗1 + 1
false else/parenrightigg
=false
l(0,1)(4,4) =/parenleftigg/braceleftigg
true 4≤0∗4 + 1
false else/parenrightigg
=false
but we have that:
l(1,1)(4,4) =/parenleftigg/braceleftigg
true 4≤1∗4 + 1
false else/parenrightigg
=true
l(0,2)(2,2) =/parenleftigg/braceleftigg
true 2≤0∗2 + 2
false else/parenrightigg
=true
By the definition of LanGKwe have that:
LanGK({((2,2),true),((1,3),false),((4,4),false)})
must contain l(0,1)since we have that:
l(0,1)∈K({((2,2),false),((1,3),false),((4,4),false)})
but:
l(0,2)̸∈K({((2,2),false),((1,3),false),((4,4),false)})
l(1,1)̸∈K({((2,2),false),((1,3),false),((4,4),false)})
Similarly, by the definition of Ran GKwe have that:
Ran GK({((2,2),true),((1,3),false),((4,4),false)})
must contain l(0,2)since we have that:
l(0,2)≤l(0,3)l(0,2)≤l(1,2)
but that there is no l(a,b)such thatl(0,2)<l(a,b)that is in both:
K({((2,2),true),((1,3),true),((4,4),false)})
and:
K({((2,2),true),((1,3),false),((4,4),true)})
since:
l(1,2)̸∈K({((2,2),true),((1,3),true),((4,4),false)})
l(0,3)̸∈K({((2,2),true),((1,3),false),((4,4),true)})
22Under review as submission to TMLR
Figure 2: The decision boundaries defined by l(1,1),l(0,3), andl(0,1).
A.3 Function Approximation
In many learning applications there may be multiple functions in a class that fit a particular set of data
similarly well. In such a situation Occam’s Razor suggests that we are best off choosing the simplest such
function. For example, we can choose the function with the smallest Kolmogorov complexity, also known as
the minimum description length (MDL) function (Rissanen, 1978). In this section we will explore how we
can use Kan extensions to find the MDL function that fits a dataset.
SupposeIis a set,Ois a partial order, and Sis a finite subset of I. We can define the following preorder:
Definition A.7. Define the preorder ≤Son(I→O)such thatf1≤Sf2if and only if∀x∈S,f 1(x)≤f2(x).
Iff1≤Sf2,f2̸≤Sf1then writef1<Sf2and iff1≤Sf2≤Sf1then writef1=Sf2.
Now suppose also that C≤cis some finite subset of the space of all functions (I→O)equipped with a total
order≤csuch thatf1≤cf2whenever the Kolmogorov complexity of f1is no larger than that of f2. Note
that functions with the same Kolmogorov complexity may be ordered arbitrarily in C≤c.
Proposition A.8. Given a set of functions Sf⊆C≤cwe can define a map that sends each function f∈Sf
to the function:
fc= min
≤c{f′|f′∈Sf,f′=Sf}
wherefcsatisfiesfc≤cf.
This map is guaranteed to exist and we can define the minimum Kolmogorov subset SfcofSfto be the image
of this map. Sfccontains exactly one function fcwheref=Sfc.
Proof.For any function f∈Sfthere must exist some fc= min≤c{f′|f′∈Sf,f′=Sf}since{f′|f′∈
Sf,f′=Sf}is a nonempty finite total ≤c-order. Therefore we can define a map that sends each f∈Sfto
fcand we can define Sfcto be the image of this map.
23Under review as submission to TMLR
Since this map will send all f∈Sfin the same =Sequivalence class to the same function in that =S
equivalence class, Sfccontains exactly one function fcwheref=Sfc. This function fcsatisfiesfc≤cf.
We can use these constructions to define the following preorders:
Definition A.9. Given the sets of functions S1
f⊆S2
f⊆C≤cdefineS1
fcto be the minimum Kolmogorov
subset ofS1
f. We can construct the preorders FA,FB,FCas follows.
•The set of objects in the discrete preorder FAisS1
fc.
•The set of objects in FBisS2
f.FBis a preorder under ≤S.
•FCis the subpreorder of FBunder≤Sin which objects are functions in S1
fc.
Intuitively a monotonic map FB→FCacts as a choice of a minimum Kolmogorov complexity function
inS1
fcfor each function in S2
f. For example, if S1
fcontains all linear functions and S2
fis the class of all
polynomials then we can view a monotonic map FB→FCas selecting a linear approximation for each
polynomial in S2
f.
Proposition A.10. For some function g∈S2
fdefine its minimal S-overapproximation to be the function
h∈S1
fcwhereg≤Shand∀h′∈S1
fcwhereg≤Sh′we haveh≤Sh′. If this function exists it is unique.
Proof.Supposeh1,h2are both minimal S-overapproximations of g. Thenh1≤Sh2andh2≤Sh1which by
the definition of S1
fcimplies that h1=h2.
Proposition A.11. For some function g∈S2
fdefine its maximal S-underapproximation to be the function
h∈S1
fcwhereh≤Sgand∀h′∈S1
fcwhereh′≤Sgwe haveh′≤Sh. If this function exists it is unique.
Proof.Supposeh1,h2are both maximal S-underapproximations of g. Thenh2≤Sh1andh1≤Sh2which
by the definition of S1
fcimplies that h1=h2.
Proposition A.12. Suppose that for some g∈S2
fthere exists some h∈S1
fcsuch thath=Sg. Thenhwill
be both the minimal S-overapproximation and the maximal S-underapproximation of g.
Proof.To start, note that hmust satisfy g≤Shand for any h′∈S1
fcwhereg≤Sh′we have:
h=Sg≤Sh′
sohis the minimal S-overapproximation of g.
Next, note that hmust satisfy h≤Sgand for any h′∈S1
fcwhereh′≤Sgwe have:
h′≤Sg=Sh
sohis also the maximal S-underapproximation of g.
We can now show the following:
Proposition A.13. Define both K:FA◁arrowhookleft→FCandG:FA◁arrowhookleft→FBto be inclusion maps. Then:
•Suppose that for any function g∈S2
fthere exists a minimal S-overapproximation hofg. Then the
left Kan extension of KalongGis the monotonic map LanGKthat mapsgtoh.
•Suppose that for any function g∈S2
fthere exists a maximal S-underapproximation hofg. Then
the right Kan extension of KalongGis the monotonic map Ran GKthat mapsgtoh.
24Under review as submission to TMLR
FB
FA FCFG
K
Proof.We first show that LanGKis monotonic when it exists. Since FB,FCare preorders we simply need
to show that when f1≤Sf2thenLanGK(f1)≤SLanGK(f2). Sincef2≤SLanGK(f2)by the definition of
the minimal S-overapproximation of f2we have that f1≤SLanGK(f2). ThenLanGK(f1)≤SLanGK(f2)
by the definition of the minimal S-overapproximation of f1.
We next show that Ran GKis monotonic when it exists. Since FB,FCare preorders we simply need to show
that when f1≤Sf2thenRan GK(f1)≤SRan GK(f2). SinceRan GK(f1)≤Sf1by the definition of the
maximalS-underapproximation of f1we have that Ran GK(f1)≤Sf2. ThenRan GK(f1)≤SRan GK(f2)
by the definition of the maximal S-underapproximation of f2.
Next, we will show that LanGKandRan GKare respectively the left and right Kan extensions when they
exist. First, by Proposition A.12 if f∈S1
fcthenfmust be both the minimal S-overapproximation and
maximalS-underapproximation of f. Therefore we have:
K(f) =LanGK(f) =Ran GK(f)
Next, consider any monotonic map ML:FB→FCsuch that∀f∈S1
fc,K(f)≤SML(f). Sincef=SK(f)
this implies f≤SML(f)so by the definition of the minimal S-overapproximation LanGK(f)≤SML(f).
Next, consider any monotonic map MR:FB→FCsuch that∀f∈S1
fc,MR(f)≤SK(f). SinceK(f) =Sf
this implies MR(f)≤Sfso by the definition of the maximal S-underapproximation MR(f)≤Ran GK(f).
Intuitively, the Kan extensions of the inclusion map K:FA→FCalong the inclusion map G:FA→FB
map a function g∈S2
fto its bestS1
f-approximations over the points in S.
For example, suppose I=O=R,gis a polynomial, S1
fis the set of lines defined by all pairs of points in S
andS2
f=S1
f∪g.LanGKandRan GKmay or may not exist depending on the choice of Sandg. In Figure
3 we give an example S,gin whichLanGKexists andRan GKdoes not (left) and an example S,gin which
Ran GKexists andLanGKdoes not (right).
As another example, suppose I=O=R,S1
fis a subset of all polynomials of degree |S|−1andS2
fis a
subset of all functions R→R. Since there always exists a unique n−1degree polynomial through nunique
points, for any Sthere exists some S1
fso that both LanGKandRan GKexist and map g∈S2
fto the unique
|S|−1degree polynomial that passes through the points {(x,g(x))|x∈S}.
25Under review as submission to TMLR
Figure 3: Left and right Kan extensions of K:FA◁arrowhookleft→FCalongG:FA◁arrowhookleft→FBfor two example sets Sand
polynomials gwhereS1
fis the class of lines and S2
f=S1
f∪g.
26