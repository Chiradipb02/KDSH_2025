Under review as submission to TMLR
Distributed SGD in overparameterized Linear Regression
Anonymous authors
Paper under double-blind review
Abstract
We consider distributed learning using constant stepsize SGD over several devices, each
sending a final model update to a central server. In a final step, the local estimates are
aggregated. We prove in the setting of overparameterized linear regression general upper
bounds with matching lower bounds and derive learning rates for specific data generating
distributions. We show that the excess risk is of order of the variance provided the number
of local nodes grows not too large with the global sample size.
We further compare distributed SGD with distributed ridge regression and provide an upper
bound of the excess SGD-risk in terms of the excess RR-risk for a certain range of the sample
size.
1 INTRODUCTION
Deep neural networks possess powerful generalization properties in various machine learning applications,
despite being overparameterized. It is generally believed that the optimization algorithm itself, e.g., stochas-
tic gradient descent (SGD), implicitly regularizes such overparameterized models. This regularizing effect
due to the choice of the optimization algorithm is often referred to as implicit regularization . A refined
understanding of this phenomenon was recently gained in the setting of linear regression (to be considered
as a reasonable approximation of neural network learning) for different variants of SGD. Constant stepsize
SGD (with last iterate or tail-averaging) is investigated in Jain et al. (2018), in Dieuleveut & Bach (2016)
in an RKHS frameowrk and also in Mücke et al. (2019) with additional mini-batching, see also Mücke &
Reiss (2020) for a more general analysis in Hilbert scales. In Zou et al. (2021b;a) it is shown that benign
overfitting also occurs for SGD. Multi-pass SGD is analyzed in Lin et al. (2016); Jain et al. (2016); Lin &
Rosasco (2017); Zou et al. (2022) while last iterate bounds can be found in Jain et al. (2019); Wu et al.
(2022); Varre et al. (2021).
Despite the attractive statistical properties of all these SGD variants, the complexity of computing regression
estimates prevents it from being routinely used in large-scale problems. More precisely, the time complexity
and space complexity of SGD and other regularization methods in a standard implementation scale as O(nα),
α∈[2,3]. Such scalings are prohibitive when the sample size nis large.
Distributed learning (DL) based on a divided-and-conquer approach is an effective way to analyze large scale
data that can not be handled by a single machine. In this paper we study a distributed learning strategy
in linear regression (including both underparameterized and overparameterized regimes) via (tail-) averaged
stochastic gradient descent with constant stepsize (DSGD). The approach is quite simple and communication
efficient: The training data is distributed across several computing nodes where on each a local SGD is run.
In a final step, these local estimates are aggregated (a.k.a. one-shot SGD ). Local SGD has become state of
the art in large scale distributed learning, showing a linear speed-up in the number of workers for convex
problems, see e.g. Mcdonald et al. (2009); Zinkevich et al. (2010); Dieuleveut & Patel (2019); Stich (2018);
Spiridonoff et al. (2021) and references therein.
The field of DL has gained increasing attention in statistical learning theory with the aim of deriving
conditions under which minimax optimal rates of convergence can be guaranteed, see e.g. Chen & Xie
(2014), Mackey et al. (2011), Xu et al. (2019), Fan et al. (2019), Shi et al. (2018), Battey et al. (2018), Fan
et al. (2021), Bao & Xiong (2021). Indeed, the learning properties of DL in regression settings over Hilbert
1Under review as submission to TMLR
spaces are widely well understood. The authors in Zhang et al. (2015) analyze distributed (kernel) ridge
regression and show optimal learning rates with appropriate regularization, provided the number of machines
increases sufficiently slowly with the sample size, though under restrictive assumptions on the eigenfunctions
of the kernel integral operator. This has been alleviated in Lin et al. (2017). However, in these works the
number of machines saturates if the target is very smooth, meaning that large parallelization seems not
possible in this regime.
Anextensionoftheseworkstomoregeneralspectralregularizationalgorithmsfornonparametricleastsquare
regression in (reproducing kernel) Hilbert spaces is given in Guo et al. (2017), Mücke & Blanchard (2018),
including gradient descent (Lin & Zhou, 2018) and stochastic gradient descent (Lin & Cevher, 2018). The
recent work Tong (2021) studies DL for functional linear regression.
We finally mention the work of Mücke et al. (2022), where distributed ordinary least squares (DOLS) in over-
parameterized linear regression is studied, i.e. one-shot OLS without any explicit or implicit regularization.
It is shown that the number of workers acts as a regularization parameter itself.
Contributions. We analyze the performance of DSGD with constant stepsize in overparameterized linear
regression and provide upper bounds with matching lower bounds for the excess risk under suitable noise
assumptions. Our results show that optimal rates of convergence can be achieved if the number of local nodes
grows sufficiently slowly with the sample size. The excess risk as a function of data splits remains constant
until a certain threshold is reached. This threshold depends on the structural assumptions imposed on the
problem, i.e. on the eigenvalue decay of the Hessian and the coefficients of the true regression parameter.
We additionally perform a comparison between DSGD and DRR, showing that the excess risk of DSGD is
upper bounded by the excess risk of DRR under an assumption on the sample complexity (SC) of DSGD,
depending on the same structural assumptions. We show that the SC of DSGD remains within constant
factors of the SC of DRR.
Our analysis extends known results in this direction from Zou et al. (2021b;a) for the single machine case to
the distributed learning setting and from DOLS in Mücke et al. (2022) to SGD with implicit regularization.
Organization. In Section 2 we define the mathematical framework needed to present our main results in
Section 3, where we provide a theoretical analysis of DSGD with a discussion of our results. In Section 4 we
compare DSGD with DRR while Section 5 is devoted to showing some numerical illustrations. The proofs a
deferred to the Appendix.
Notation. ByL(H1,H2)we denote the space of bounded linear operators between real Hilbert spaces H1,
H2. We writeL(H,H) =L(H). For A∈L(H)we denote by ATthe adjoint operator. By A†we denote
the pseudoinverse of Aand forw∈Hwe write||w||2
A:=||A1
2w||for an PSD operator A.
We let [n] ={1,...,n}for everyn∈N. For two positive sequences (an)n,(bn)nwe writean≲bnifan≤cbn
for somec>0andan≃bnif bothan≲bnandbn≲an.
2 SETUP
In this section we provide the mathematical framework for our analysis. More specifically, we introduce
distributed SGD and state the main assumptions on our model.
2.1 SGD and linear regression
We consider a linear regression model over a real separable Hilbert space Hin random design. More precisely,
we are given a random covariate vector x∈Hand a random output y∈Rfollowing the model
y=⟨w∗,x⟩+ϵ, (1)
2Under review as submission to TMLR
whereϵ∈Ris a noise variable. We will impose some assumptions on the noise model in Section 3. The true
regression parameter w∗∈Hminimizes the least squares test risk, i.e.
L(w∗) = min
w∈HL(w), L (w) :=1
2E[(y−⟨w,x⟩)2],
where the expectation is taken with respect to the joint distribution Pof the pair (x,y)∈H× R. More
specifically, we let w∗be the minimum norm element in the set of all minimizers of L.
To derive an estimator ˆw∈Hforw∗we are given an i.i.d. dataset
D:={(x1,y1),...,(xn,yn)}⊂H× R,
following the above model equation 1, i.e.,
Y=Xw∗+ε,
with i.i.d. noise ε= (ε1,...,εn)∈Rn. The corresponding random vector of outputs is denoted as Y=
(y1,...,yn)T∈Rnand we arrange the data xj∈Hinto adata matrix X∈L(H,Rn)by setting (Xv)j=
⟨xj,v⟩forv∈H,1≤j≤n. IfH=Rd, then Xis an×dmatrix (with row vectors xj). We are particular
interested in the overparameterized regime, i.e. where dim(H)>n.
In the classical setting of stochastic approximation with constant stepsize, the SGD iterates are computed
by the recursion
wt+1=wt−γ(⟨wt,xt⟩−yt)xt, t= 1,...,n,
with some initialization w1∈Hand whereγ >0is the stepsize. The tail average of the iterates is denoted
by
¯wn
2:n:=1
n−n/2n/summationdisplay
t=n/2+1wt, (2)
and where we denote by ¯wn:= ¯w0:nthe full (uniform) average.
VariousformsofSGD(withiterateaveraging, tailaveraging, multipasses)inthesettingofoverparameterized
linear regression has been analyzed recently in Zou et al. (2021b), Wu et al. (2022), Zou et al. (2022),
respectively. In particular, the phenomenon of benign overfitting is theoretically investigated in these works.
It could be shown that benign overfitting occurs in this setting, i.e. the SGD estimator fits training data
very well and still generalizes.
We are interested in this phenomenon for localized SGD, i.e. when our training data is distributed over
several computing devices.
2.2 Local SGD
In the distributed setting, our data are evenly divided into M∈Nlocal disjoint subsets
D=D1∪...∪DM
of size|Dm|=n
M, form= 1,...,M. To each local dataset we associate a local design matrix Xm∈L(H,Rn
M)
(build with local row vectors x(m)
j) with local output vector Ym∈Rn
Mand a local noise vector εm∈Rn
M.
The local SGD iterates are defined as
w(m)
t+1=w(m)
t−γ/parenleftig/angbracketleftig
w(m)
t,x(m)
t/angbracketrightig
−yt/parenrightig
x(m)
t,
fort= 1,...,n
Mandm= 1,...,M. The averaged local iterates ¯w(m)
n
Mare computed according to equation 2.
We are finally interested in the uniform average of the local SGD iterates, building a global estimator:
wM:=1
MM/summationdisplay
m=1¯w(m)
n
M.
3Under review as submission to TMLR
Distributed learning in overparameterized linear regression is studied in Mücke et al. (2022) for the ordinary
leastsquaresestimator(OLS),i.e. withoutanyimplicitorexplicitregularizationandwithlocalinterpolation.
It is shown that local overfitting is harmless and regularization is done by the number of data splits.
We aim at finding optimal bounds for the excess risk
E/bracketleftbig
L(wM)/bracketrightbig
−L(w∗),
of distributed SGD (DSGD) with potential local overparameterization and as function of the number of local
nodesMand under various model assumptions, to be given in the next section.
3 MAIN RESULTS
In this section we present our main results. To do so, we first impose some model assumptions.
Definition 3.1.
1. We define the second moment of x∼Pxto be the operator H:H→H, given by
H:=E[x⊗x] =E[⟨·,x⟩x].
2. The fourth moment operator M:L(H)→L(H)is defined by
M:=E[x⊗x⊗x⊗x],
withM(A)(w) =E[⟨x,Ax⟩⟨w,x⟩x], for allw∈H.
3. The covariance operator of the gradient noise at w∗is defined as Σ:H→H,
Σ:=E[(⟨w∗,x⟩−y)2x⊗x].
Assumption 3.2 (Second Moment Condition) .We assume that E[y2|x]<∞almost surely. Moreover, we
assume that the trace of His finite, i.e., Tr[H]<∞.
Assumption 3.3 (Fourth Moment Condition) .We assume there exists a positive constant τ >0such that
for any PSD operator A, we have
M(A)⪯τTr[HA]H.
Note that this assumption holds if H−1xis sub-Gaussian, being a standard assumption in least squares
regression, see e.g. Bartlett et al. (2020), Zou et al. (2021b), Tsigler & Bartlett (2020).
Assumption 3.4 (Noise Condition) .Assume that
σ2:=||H−1
2ΣH−1
2||<∞.
This assumption on the noise is standard in the literature about averaged SGD, see e.g. Zou et al. (2021b),
Dieuleveut & Bach (2016).
We introduce some further notation involving the second moment operator H: We denote the eigendecom-
position as
H=∞/summationdisplay
j=1λjvj⊗vj,
where theλ1≥λ2≥...are the eigenvalues of Hand thev′
jsare the corresponding eigenvectors. For k≥1,
we let
H0:k:=k/summationdisplay
j=1λjvj⊗vj,Hk:∞:=∞/summationdisplay
j=k+1λjvj⊗vj.
4Under review as submission to TMLR
Similarly,
I0:k=k/summationdisplay
j=1vj⊗vj,Ik:∞:=∞/summationdisplay
j=k+1vj⊗vj.
A short calculation shows that for all w∈Hwe have
||w||2
H†
0:k=k/summationdisplay
j=1⟨w,vj⟩2
λj,||w||2
Hk:∞=∞/summationdisplay
j=k+1λj⟨w,vj⟩2.
We finally set
Vk(n,M ) :=k
n+γ2n
M2∞/summationdisplay
j=k+1λ2
j. (3)
3.1 Upper Bound
We now present an upper bound for the averaged local SGD iterates. The proof relies on a bias-variance
decomposition and is given in Appendix A.1.
Theorem3.5 (DSGDUpperBound) .Suppose Assumptions 3.2, 3.3 and 3.4 are satisfied and let γ <1
τTr[H],
w1= 0. The excess risk for the averaged local SGD estimate satisfies
E/bracketleftbig
L(wM)/bracketrightbig
−L(w∗)≤2Bias(wM) + 2Var(wM),
where
Bias(wM)≤M2
γ2n2||w∗||2
H†
0:k∗+||w∗||2
Hk∗:∞+2τM2/parenleftbig
||w∗||2
I0:k∗+γn
M||w∗||2
Hk∗:∞/parenrightbig
γn(1−γτTr[H])·Vk∗(n,M )
and
Var(wM)≤σ2
1−γτTr[H]·Vk∗(n,M ),
withk∗= max{k:λk≥M
γn}.
The excess risk is upper bounded in terms of the bias and variance. Both terms crucially depend on the
effective dimension k∗= max{k:λk≥M
γn}, dividing the full Hilbert space Hinto two parts. On the part
associated to the first largest k∗eigenvalues, the bias may decay faster than on the remaining tail part that
is associated to the smaller eigenvalues, see Zou et al. (2021b) in the context of single machine SGD, Bartlett
et al. (2020); Tsigler & Bartlett (2020), in the context of single machine ridge regression and Mücke et al.
(2022) for distributed ordinary least squares.
Our Theorem 3.5 reveals that the excess risk converges to zero if
||w∗||2
Hk∗:∞→0,2M2
γ2n2||w∗||2
H†
0:k∗→0
andVk∗(n,M )→0asn→∞. This requires the eigenvalues of Hto decay sufficiently fast and to choose
the number of local nodes M=Mnto be a sequence of n. Note that we have to naturally assume Mn≲n.
In Subsection 3.3 we provide two specific examples of data distributions with specific choices for (Mn)n∈N
such that the above conditions are met, granting not only convergence but also providing explicit rates of
convergence.
5Under review as submission to TMLR
3.2 Lower Bound
Before we state the lower bounds for the excess risk of the DSGD estimator we need to impose some
assumptions.
Assumption 3.6 (Fourth Moment Lower Bound) .We assume there exists a positive constant θ >0such
that for any PSD operator A, we have
M(A)−HAH⪰θTr[HA]H.
Assumption 3.7 (Well-Specified Noise) .The second moment operator His strictly positive definite with
Tr[H]<∞. Moreover, the noise ϵin equation 1 is independent of xand satisfies
ϵ∼N(0,σ2
noise).
We now come to the main result whose proof can be found in Appendix A.2.
Theorem 3.8 (DSGD Lower Bound) .Suppose Assumptions 3.6 and 3.7 are satisfied. Assume w1= 0. The
excess risk of the DSGD estimator satisfies
E/bracketleftbig
L(wM)/bracketrightbig
−L(w∗)≥M(M−1)
100γ2n2/parenleftbigg
||w∗||2
H†
0:k∗+γ2n2
M2||w∗||2
Hk∗:∞/parenrightbigg
+σ2
noise
100·Vk∗(n,M ),
whereVk∗(n,M )is defined in equation 3.
The lower bound for the excess risk also decomposes into a bias part (first term) and a part associated to
the variance (second term). Comparing the bias with the upper bound for the bias from Theorem 3.5 shows
that both are of the same order. Comparing the variances reveals that they are of the same order if
2τM2/parenleftbig
||w∗||2
I0:k∗+γn
M||w∗||2
Hk∗:∞/parenrightbig
γn(1−γτTr[H])≲1.
In the next section, we will provide specific conditions and examples when this is satisfied.
3.3 Fast Rates of convergence for specific distributions
We now consider two particular cases of data distributions, namely the spiked covariance model (with local
overparameterization) and the case where the eigenvalues of the second moment operator Hdecaypolyno-
mially. These are standard assumptions for the model, see e.g. Tsigler & Bartlett (2020); Zou et al. (2021b);
Mücke et al. (2022). In both cases, we determine a range of the number of local nodes Mndepending on
the global sample size such that the bias is dominated by the variance. The final error is then of the order
of the variance, if the number of local nodes grows sufficiently slowly with the sample size. The optimal1
number exactly balances bias and variance.
Corollary 3.9 (Spiked Covariance Model) .Suppose all assumptions of Theorem 3.5 are satisfied. Assume
that||w∗||≤Rfor someR> 0andH∈Rd×d. Letd=/parenleftbign
M/parenrightbigqfor someq >1and ˜d=/parenleftbign
M/parenrightbigr<dfor some
0<r≤1. Suppose the spectrum of Hsatisfies
λj=/braceleftigg
1
˜d:j≤˜d
1
d−˜d:˜d+ 1≤j≤d.
If
Mn≤/radicalbigg
γ(1−2γτ)n
R2
1Optimal in the sense of the maximal possible number of local nodes that balances bias and variance.
6Under review as submission to TMLR
then for any nsufficiently large, we have
E/bracketleftbig
L(wMn)/bracketrightbig
−L(w∗)≤c1
γMn/parenleftbiggMn
n/parenrightbiggν
,
whereν= min{1−r,q−1}and for some c<∞, depending on τ,γ,σ.
Choosing the maximum number of local nodes Mn≃√ngives the fast rate of order
E/bracketleftbig
L(wMn)/bracketrightbig
−L(w∗)≲/parenleftbigg1
n/parenrightbiggν+1
2
.
for the excess risk.
Corollary 3.10 (Polynomial Decay) .Suppose all assumptions of Theorem 3.5 are satisfied with γ <
min/braceleftig
1,1
τTr[H]/bracerightig
. Assume that||w∗||≤Rfor someR > 0. Suppose the spectrum2ofHsatisfies for some
r>0
λj=j−(1+r).
If
Mn≤/parenleftigγ
R2/parenrightig1+r
2+r·(γn)1
2+r,
then for any nsufficiently large, we have
E/bracketleftbig
(wM)/bracketrightbig
−L(w∗)≤cγ
Mn/parenleftbiggMn
n/parenrightbiggr
1+r
,
for somec<∞, depending on τ,γ,σ.
Choosing the maximum number of local nodes Mn≃n1
2+rgives the fast rate of order
E/bracketleftbig
L(wMn)/bracketrightbig
−L(w∗)≲/parenleftbigg1
n/parenrightbiggr+1
r+2
.
for the excess risk.
3.4 Discussion
Comparison to single machine SGD. We compare the DSGD algorithm with the single machine SGD
algorithm, i.e. when M= 1. For this case, we recover the results from Zou et al. (2021b) under the same
assumptions. Our Corollaries 3.9, 3.10 show that the excess risk is dominated by the variance as long as M
grows sufficiently slowly with the sample size. But we can say even more: In the spiked covariance model,
ifMn≃nβforβ∈[0,1/2], we see that DSGD performs as good as single machine SGD, provided ν≤1.
Indeed, a direct comparison shows that
1
γMn/parenleftbiggMn
n/parenrightbiggν
≃1
γnβ/parenleftbiggnβ
n/parenrightbiggν
≃1
γ/parenleftbigg1
n/parenrightbiggν
,
for anyβ∈[0,1/2]andν≤1. Recall that all our bounds are of optimal order, hence the relative efficiency
remains of constant order until the critical threshold for Mnis reached.
However, if Mnis larger than the threshold, i.e. if β∈(1/2,1], then the bias term is dominating. In this
case, the excess risk is of order
2M2
n2||w∗||2
H†
0:k∗+||w∗||2
Hk∗:∞≃/parenleftbiggMn
n/parenrightbigg2−r
+/parenleftbiggMn
n/parenrightbiggq
≃/parenleftbiggnβ
n/parenrightbigg2−r
+/parenleftbiggnβ
n/parenrightbiggq
,
2Note that the choice λj=j−(1+r)ensures that Tr[H]<∞.
7Under review as submission to TMLR
being larger than the variance, see the proof of Corollary 3.9, Appendix A.3.
The same observations can be made for the setting in Corollary 3.10 when the eigenvalues are polynomially
decaying. If we let Mn≃nβwithβ∈[0,1/(2 +r)], then the variance dominates and for all r>0, the test
error satisfies
1
γMn/parenleftbiggMn
n/parenrightbiggr
r+1
≃1
γnβ/parenleftbiggnβ
n/parenrightbiggr
r+1
≃1
γ/parenleftbigg1
n/parenrightbiggr
r+1
.
We refer to Section5 and Section C for some numerical experiments.
Comparison to distributed learning in RKHSs. We emphasize that all our results above hold for
a constant stepsize 0< γ < min/braceleftig
1,1
τTr[H]/bracerightig
. In particular, γdoes not depend on the number Mof local
nodes. This result is line with the results for regularized distributed learning over reproducing kernel Hilbert
spaces, see Zhang et al. (2015); Lin et al. (2017); Mücke & Blanchard (2018) and references therein. In
this setting it is shown for a large class of spectral regularization methods3that the optimal regularization
parameterλthat leads to minimax optimal bounds, depends on the global sample size only and is of order
n−α,α∈(0,1]. In particular, this parameter is chosen as in the single machine machine setting and each
local subproblem is underregularized. This leads to a roughly constant bias (unchanged by averaging) in
the distributed setting, an increase in variance but averaging reduces the variance sufficiently to obtain
optimal excess risk bounds. The same phenomenon occurs in our DSGD setting. On each local node the
same stepsize γas for theM= 1case is applied.
Comparison to distributed ordinary least squares (DOLS). We also compare our results
with those recently obtained in Mücke et al. (2022) for DOLS in random design linear regression. The
general observation in this work is that in the presence of overparameterization, the number of local nodes
acts as a regularization parameter, balancing bias and variance. Recall that this is in contrast to what
we observe for DSGD due to the implicit regularization. The optimal number of splits MOLS
optdepends
on structural assumptions, i.e. eigenvalue decay and decay of the Fourier coefficients of w∗(a.k.a.source
condition ).
For the spiked covariance model, the optimal number MOLS
nof DOLS is of order
MOLS
n≃/parenleftbiggdn3/2
d·˜d/parenrightbigg2/5
≃n3−2r
5−2r,
see Corollary 3.14 in Mücke et al. (2022). Comparing with our maximum number for Mn≃n1/2from our
Corollary 3.9 we observe that MOLS
n≲MSGD
nif1
2≤r≤1, i.e., DSGD allows for more parallelization in
this regime.
For polynomially decaying eigenvalues λj∼j1+r,r>0, the optimal number of data splits in Corollary 3.9
(Mücke et al., 2022) scales as MOLS
n≃n1/3. Compared to our result from Corollary 3.10 we have
MSGD
n≃n1
2+r≲n1/3
for allr≥1. Thus, DOLS seems to allow more data splits under optimality guarantees for fast polynomial
decay, i.e. large r.
4 COMPARISON OF SAMPLE COMPLEXITY OF DSGD AND DRR
Inthissectionwecomparethedistributedtail-averagedSGDestimatorwiththedistributedRidgeRegression
(RR) estimator (see Zhang et al. (2015); Lin et al. (2017); Mücke & Blanchard (2018); Sheng & Dobriban
(2020) or Tsigler & Bartlett (2020) for RR in the single machine case). Recall that RR reduces to ordinary
least-squares (OLS) if the regularization parameter is set to zero. As a special case, we compare our results
to local OLS from Mücke et al. (2022) and analyze the benefit of implicit regularization of local SGD in the
presence of local overparameterization.
3This class contains, among others, gradient descent and accelerated methods like Heavy ball and Nesterov, ridge regression
or PCA.
8Under review as submission to TMLR
We recall that for any m∈[M],λ≥0, the local RR estimates are defined by
ˆwRR
m(λ) =XT
m(XmXT
m+λ)−1Ym.
The average is
wRR
n(λ) =1
MM/summationdisplay
m=1ˆwRR
m.
We aim at showing that the excess risk of DSGD is upper bounded by the excess risk of DRR under suitable
assumptions on the sample complexity. To this end, we first derive a lower bound for DRR to compare with.
The proof follows by combining Proposition B.3 and Proposition B.5 with Lemma B.2.
Assumption 4.1. The variable H−1xis sub-Gaussian and has independent components.
Similarly to the bounds for DSGD, our bounds for DRR depend on the effective dimension
k∗
RR:= min

k:λk+1≤M/parenleftig
λ+/summationtext
j>kλj/parenrightig
bn

,
forλ>0and someb>1.
Theorem 4.2 (Lower Bound Distributed RR) .Suppose Assumption 4.1 holds and that His strictly positive
definite with Tr[H]<∞. Assume that k∗
RR≤n
c′Mfor somec′>1. There exist constants b,c> 1such that
the excess risk of the averaged RR estimator satisfies
E/bracketleftbig
L(wRR
n(λ))/bracketrightbig
−L(w∗)≥||w∗||2
Hk∗
RR:∞+M2/parenleftig
λ+/summationtext
j>k∗
RRλj/parenrightig2
cn2·||w∗||2
H−1
0:k∗
RR
+σ2
c/parenleftigg
k∗
RR
n+n
M2·/summationtext
j>k∗
RRλ2
j
(λ+/summationtext
j>k∗
RRλj)2/parenrightigg
.
We do our risk comparison particularly for tail-averaged DSGD and derive a bias-improved upper bound.
The proof is given in Section B.2 and is an extension of Lemma 6.1 in Zou et al. (2021a) to DSGD.
Theorem 4.3 (Upper Bound Tail-averaged DSGD) .Suppose Assumption 3.7 is satisfied. Let wMndenote
the tail-averaged distributed estimator with ntraining samples and assume γ < 1/Tr[H]. For arbitrary
k1,k2∈[d]
E/bracketleftbig
L(wM)/bracketrightbig
−L(w∗) = Bias(wM) + Var(wM)
with
Bias(wM)≤cbM2
γ2n2·/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleexp/parenleftig
−n
MγH/parenrightig
w∗/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle2
H−1
0:k1+||w∗||2
Hk1:∞,
Var(wM)≤cv(1 +R2)·σ2
k2
n+nγ2
M2·/summationdisplay
j>k2λ2
j
,
for some universal constants cb,cv>0.
To derive the risk comparison we fix a sample size nRRandnSGDfor DRR and tail-averaged DSGD, resp.,
and derive conditions on the sample complexities such that individually, the bias and variance of DSGD is
upper bounded by the bias and variance of DRR, respectively. Combining then both of the above theorems
finally leads to the risk comparison result. A detailed computation is given in Section B.3.
9Under review as submission to TMLR
Theorem 4.4 (Comparison DSGD with DRR) .LetwMnSGDdenote the tail-averaged distributed estimator
withnSGDtraining samples. Let further wRR
nRR(λ)denote the distributed RR estimator with nRRtraining
samples and with regularization parameter λ≥0. Suppose all assumptions from Theorems 4.2 ,4.3 are
satisfied. There exist constants b,c> 1and0<Lλ,γ≤L′
λ,γsuch that for C∗:=c/parenleftig
1 +||w∗||2
σ2/parenrightig
,
C∗
λ:=λ+/summationdisplay
j>k∗
RRλj, γ < min/braceleftbigg1
Tr[H],1√cC∗C∗
λ/bracerightbigg
(4)
and
Lλ,γ·nRR≤nSGD≤L′
λ,γ·nRR
the excess risks of DSGD and DRR satisfy
E/bracketleftbig
L(wM)/bracketrightbig
−L(w∗)≤E/bracketleftbig
L(wRR
nRR(λ))/bracketrightbig
−L(w∗). (5)
The constants Lλ,γ,L′
λ,γare explicitly given by
Lλ,γ= max

C∗,/radicalig
c(1−γλk∗
RR)
γC∗
λ

, L′
λ,γ=1
C∗γ2(C∗
λ)2.
Note that in the above Theorem, assumption equation 4 on the stepsize ensures that 0<Lλ,γ≤L′
λ,γ.
Our result shows that DSGD performs better than DRR/ DOLS if the sample complexity (SC) of SGD
differs from the SC of RR/OLS by no more than a constant. This constant depends on the amount of
regularization λ, the stepsize γand the tail behavior of the eigenvalues of the Hessian. We refer to Section
B.3.2 for a more detailed discussion. Our bound slightly differs from Zou et al. (2021a) for the case M= 1in
two respects: We scale our SC such that the constant in equation 5 is equal to one while Zou et al. (2021a)
show that both risks are of the same order (with a constant larger than one). Second, we also show that the
SC of DSGD is upper bounded by a factor of the SC of DRR/DOLS while Zou et al. (2021a) only derive a
lower bound. However, we remark that nSGDin our Theorem is larger that nRRas the constant Lλ,γ≥1.
A look onto Figure 1 reveals that optimally tuned DSGD may perform better than optimally tuned DRR
even with the same or smaller sample size for certain problem instances. This suggests that our bound may
be refined.
5 NUMERICAL EXPERIMENTS
We illustrate our theoretical findings with experiments on simulated and real data. The reader may find
additional experiments in Section C.
Simulated Data. In a first experiment in Figure 1 (left) we analyze the test error of DSGD as a function
of the local nodes M. We generate n= 500i.i.d. training data with xj∼N(0,H)with mildly overparam-
eterization d= 700. The target w∗satisfies three different decay conditions w∗
j=j−α,α∈{0,1,10}. The
eigenvalues of Hfollow a polynomial decay λj=j−2. The local nodes satisfy Mn=nβ,β∈{0,0.1,...,0.9}.
According to Corollary 3.10 we see that a fast decay of w∗
j(i.e. a smaller norm ||w∗||) allows for more
parallelization until the test error blows up.
In a second experiment we compare the sample complexity of optimally tuned tail-averaged DSGD and DRR
fordifferentsources w∗, seeFigures1(right), 2. Here, thedataaregeneratedasabovewith d= 200,λj=j−2
andw∗
j=j−α,α∈{0,1,10}. The number of local nodes is fixed at Mn=n1/3for eachn∈{100,...,6000}.
For this problem instance, DSGD may perform even better than DRR for sparse targets ( α= 10), i.e.,
DSGD achieves the same accuracy as DRR with less samples in this regime. For less sparse targets α= 1,
the sample complexities of DSGD and DRR are comparable while for non-sparse targets ( α= 0), DRR
outperforms DSGD.
10Under review as submission to TMLR
Figure 1: Left:Test error for DSGD with λj=j−2for different sources w∗as a function of M.
Right:Comparison of optimally tuned tail-ave DSGD with DRR with λj=j−2,w∗
j=j−10,Mn=n1/3.
Figure 2: Comparison of optimally tuned tail-ave DSGD with DRR with λj=j−2for different sources w∗,
withλj=j−2andMn=n1/3.Left:w∗
j=j−1Right:w∗
j= 1.
Real Data. To analyze the performance of DSGD on real data, we considered the classification problem of
the Gisette data set4, containing pictures of the digits four and nine. We used the first 3000samples of the
original train data set for training and the second 3000samples for evaluation. The feature dimension of one
picture isd= 5000. Hyper-parameters had been fine-tuned on the validation data set to achieve the best
performance. The first experiment in Figure 3(left) again analyzes the test error of DSGD as a function of
the local nodes M. Because the feature dimension is quite large, the optimal stepsize is small ( γ∼10−10).
Theorem 3.8 therefore explains why in our example the bias-term and thus the test error grows rather quickly
with the number of local nodes. In Figure 3(right) we compare DRR with tail- and full-averaged DSGD.
We observe that DRR slightly outperforms DSG. According to Theorem 4.4, we need sparsity for w∗so that
DSGD can keep up with DRR. This might be not the case for the Gisette data set.
6 Summary
We analyzed the performance of distributed constant stepsize (tail-) averaged SGD for linear regression in
an overparameterized regime. We find that the relative efficiency as a function of the number of workers
remains largely unchanged until a certain threshold is reached. This threshold depends on the structural
assumptions imposed by the problem at hand (eigenvalue decay of the Hessian Hand the norm of the
targetw∗). This is in contrast to distributed OLS without any implicit or explicit regularization with local
overparameterization, where the number of workers itself acts as a regularization parameter, see Figure 4 in
Appendix C.
We also compared the sample complexity of DSGD and DRR and find that the sample complexity of DSGD
remains within constant factors of the sample complexity of DRR. For some problem instances, tail-averaged
4http://archive.ics.uci.edu/ml/datasets/Gisette
11Under review as submission to TMLR
Figure 3: Left:Test error for DSGD with n= 1000,2000,3000and different M.Right:Comparison of
DSGD with DRR for Mn=n1/4.
SGD may outperform DRR, i.e., achieves the same or better accuracy with less samples. Our bound is not
sharp and may be improved in future research.
References
Yajie Bao and Weijia Xiong. One-round communication efficient distributed m-estimation. In International
Conference on Artificial Intelligence and Statistics , pp. 46–54. PMLR, 2021. 1
PeterLBartlett, PhilipMLong, GáborLugosi, andAlexanderTsigler. Benignoverfittinginlinearregression.
Proceedings of the National Academy of Sciences , 2020. 3, 3.1, B.1.3
Heather Battey, Jianqing Fan, Han Liu, Junwei Lu, and Ziwei Zhu. Distributed testing and estimation under
sparse high dimensional models. Annals of statistics , 46(3):1352, 2018. 1
Xueying Chen and Min-ge Xie. A split-and-conquer approach for analysis of extraordinarily large data.
Statistica Sinica , pp. 1655–1684, 2014. 1
Aymeric Dieuleveut and Francis Bach. Nonparametric stochastic approximation with large step-sizes. Ann.
Statist., 44(4):1363–1399, 08 2016. doi: 10.1214/15-AOS1391. 1, 3
Aymeric Dieuleveut and Kumar Kshitij Patel. Communication trade-offs for local-sgd with large step size.
Advances in Neural Information Processing Systems , 32, 2019. 1
Jianqing Fan, Dong Wang, Kaizheng Wang, and Ziwei Zhu. Distributed estimation of principal eigenspaces.
Annals of statistics , 47(6):3009, 2019. 1
Jianqing Fan, Yongyi Guo, and Kaizheng Wang. Communication-efficient accurate statistical estimation.
Journal of the American Statistical Association , pp. 1–11, 2021. 1
Zheng-Chu Guo, Shao-Bo Lin, and Ding-Xuan Zhou. Learning theory of distributed spectral algorithms.
Inverse Problems , 33(7):074009, 2017. 1
P. Jain, S.M. Kakade, R. Kidambi, P. Netrapalli, and A. Sidford. Parallelizing stochastic gradient descent for
least squaresregression: mini-batching, averaging and model misspecification. arXiv:1610.03774v3 , 2016.
1, A.1.1
Prateek Jain, Sham M Kakade, Rahul Kidambi, Praneeth Netrapalli, Venkata Krishna Pillutla, and Aaron
Sidford. A markov chain theory approach to characterizing the minimax optimality of stochastic gradient
descent (for least squares). In 37th IARCS Annual Conference on Foundations of Software Technology
and Theoretical Computer Science , 2018. 1
Prateek Jain, Dheeraj Nagaraj, and Praneeth Netrapalli. Making the last iterate of sgd information theo-
retically optimal. In Conference on Learning Theory , pp. 1752–1755. PMLR, 2019. 1
Junhong Lin and Volkan Cevher. Optimal distributed learning with multi-pass stochastic gradient methods.
InInternational Conference on Machine Learning , pp. 3092–3101. PMLR, 2018. 1
12Under review as submission to TMLR
Junhong Lin and Lorenzo Rosasco. Optimal rates for multi-pass stochastic gradient methods. Journal of
Machine Learning Research 18 , 2017. 1
Junhong Lin, Raffaello Camoriano, and Lorenzo Rosasco. Generalization properties and implicit regulariza-
tion for multiple passes sgm. International Conference on Machine Learning , 2016. 1
Shao-Bo Lin and Ding-Xuan Zhou. Distributed kernel-based gradient descent algorithms. Constructive
Approximation , 47(2):249–276, 2018. 1
Shao-Bo Lin, Xin Guo, and Ding-Xuan Zhou. Distributed learning with regularized least squares. The
Journal of Machine Learning Research , 18(1):3202–3232, 2017. 1, 3.4, 4
Lester Mackey, Ameet Talwalkar, and Michael I Jordan. Divide-and-conquer matrix factorization. Advances
in neural information processing systems , 24, 2011. 1
Ryan Mcdonald, Mehryar Mohri, Nathan Silberman, Dan Walker, and Gideon Mann. Efficient large-scale
distributed training of conditional maximum entropy models. Advances in neural information processing
systems, 22, 2009. 1
Nicole Mücke and Gilles Blanchard. Parallelizing spectrally regularized kernel algorithms. The Journal of
Machine Learning Research , 19(1):1069–1097, 2018. 1, 3.4, 4
Nicole Mücke and Enrico Reiss. Stochastic gradient descent in hilbert scales: Smoothness, preconditioning
and earlier stopping. stat, 1050:18, 2020. 1
Nicole Mücke, Gergely Neu, and Lorenzo Rosasco. Beating sgd saturation with tail-averaging and mini-
batching. In Advances in Neural Information Processing Systems , pp. 12568–12577, 2019. 1
Nicole Mücke, Enrico Reiss, Jonas Rungenhagen, and Markus Klein. Data-splitting improves statistical
performance in overparameterized regimes. In International Conference on Artificial Intelligence and
Statistics , pp. 10322–10350. PMLR, 2022. 1, 1, 2.2, 3.1, 3.3, 3.4, 4
Yue Sheng and Edgar Dobriban. One-shot distributed ridge regression in high dimensions. In International
Conference on Machine Learning , pp. 8763–8772. PMLR, 2020. 4
Chengchun Shi, Wenbin Lu, and Rui Song. A massive data framework for m-estimators with cubic-rate.
Journal of the American Statistical Association , 113(524):1698–1709, 2018. 1
Artin Spiridonoff, Alex Olshevsky, and Yannis Paschalidis. Communication-efficient sgd: From local sgd to
one-shot averaging. Advances in Neural Information Processing Systems , 34:24313–24326, 2021. 1
SebastianUStich. Localsgdconvergesfastandcommunicateslittle. In International Conference on Learning
Representations , 2018. 1
Hongzhi Tong. Distributed least squares prediction for functional linear regression. Inverse Problems , 38(2):
025002, 2021. 1
Alexander Tsigler and Peter L Bartlett. Benign overfitting in ridge regression. arXiv preprint
arXiv:2009.14286 , 2020. 3, 3.1, 3.3, 4, B.1
Aditya Vardhan Varre, Loucas Pillaud-Vivien, and Nicolas Flammarion. Last iterate convergence of sgd
for least-squares in the interpolation regime. Advances in Neural Information Processing Systems , 34:
21581–21591, 2021. 1
Jingfeng Wu, Difan Zou, Vladimir Braverman, Quanquan Gu, and Sham Kakade. Last iterate risk bounds
of sgd with decaying stepsize for overparameterized linear regression. In International Conference on
Machine Learning , pp. 24280–24314. PMLR, 2022. 1, 2.1
Ganggang Xu, Zuofeng Shang, and Guang Cheng. Distributed generalized cross-validation for divide-and-
conquer kernel ridge regression and its asymptotic optimality. Journal of computational and graphical
statistics , 28(4):891–908, 2019. 1
13Under review as submission to TMLR
Yuchen Zhang, John Duchi, and Martin Wainwright. Divide and conquer kernel ridge regression: A dis-
tributed algorithm with minimax optimal rates. The Journal of Machine Learning Research , 16(1):3299–
3340, 2015. 1, 3.4, 4
Martin Zinkevich, Markus Weimer, Lihong Li, and Alex Smola. Parallelized stochastic gradient descent.
Advances in neural information processing systems , 23, 2010. 1
Difan Zou, Jingfeng Wu, Vladimir Braverman, Quanquan Gu, Dean P Foster, and Sham Kakade. The
benefits of implicit regularization from sgd in least squares problems. Advances in Neural Information
Processing Systems , 34:5456–5468, 2021a. 1, 1, 4, 4, 1, 2, B.1.2, B.1.3, B.2
Difan Zou, Jingfeng Wu, Vladimir Braverman, Quanquan Gu, and Sham Kakade. Benign overfitting of
constant-stepsize sgd for linear regression. In Conference on Learning Theory , pp. 4633–4635. PMLR,
2021b. 1, 1, 2.1, 3, 3, 3.1, 3.3, 3.4, A.1.1, A.1.1, A.1.1, A.1.2, A.2.1, A.2.1, A.2.2
Difan Zou, Jingfeng Wu, Vladimir Braverman, Quanquan Gu, and Sham M Kakade. Risk bounds of multi-
pass sgd for least squares in the interpolation regime. arXiv preprint arXiv:2203.03159 , 2022. 1, 2.1
14Under review as submission to TMLR
Notation. ByL(H1,H2)we denote the space of bounded linear operators between real Hilbert spaces H1,
H2with operator norm ||·||. We writeL(H,H) =L(H). For A∈L(H)we denote by ATthe adjoint
operator. For two PSD operators on Hwe write A⪯Bif⟨(A−B)v,v⟩≥0for allv∈H. We further let
⟨A,B⟩op= Tr[ATB].
A PROOFS SECTION 3 (BOUNDS FOR DSGD)
A.1 Proofs Upper Bound
A.1.1 Bias-Variance Decomposition
We will use an iterative bias-variance-decomposition which has been extensively studied before in the non
distributed case (see Jain et al. (2016), Zou et al. (2021b)). First we need a couple of definitions.
-)Centered local iterates: Setη(m)
t:=w(m)
t−w∗and
¯η(m)
n:=M
nn/M/summationdisplay
t=1η(m)
t,¯¯ηM:=1
MM/summationdisplay
m=1¯η(m)
n.
-)Local bias: Form= 1,...,Mwe setb(m)
1=w1−w∗,
b(m)
t:= (I−γx(m)
t⊗x(m)
t)b(m)
t−1, t= 2,...,n
M
¯b(m)
n:=M
nn/M/summationdisplay
t=1b(m)
t,bM:=1
MM/summationdisplay
m=1¯b(m)
n.
-)Local variance: Form= 1,...,Mwe setv(m)
1= 0and
v(m)
t:= (I−γx(m)
t⊗x(m)
t)v(m)
t−1+γϵ(m)
tx(m)
t, t= 2,...,n
M,
¯v(m)
n:=M
nn/M/summationdisplay
t=1v(m)
t,vM:=1
MM/summationdisplay
m=1¯v(m)
n,
where we let ϵ(m)
t:=y(m)
t−/angbracketleftig
x(m)
t,w∗/angbracketrightig
.
Note that for any m= 1,...,Mandt≥1one has
E[b(m)
t+1] =E[E[b(m)
t+1|b(m)
t]] =E[E[(I−γx(m)
t+1⊗x(m)
t+1)b(m)
t|b(m)
t]] = (I−γH)E[b(m)
t]. (6)
Moreover, from B.4 in Zou et al. (2021b), we find
E[v(m)
t+1] = (I−γH)E[v(m)
t] = (I−γH)tE[v(m)
1] = 0. (7)
It is easy to see that η(m)
t=b(m)
t+v(m)
tand therefore
¯¯ηM=bM+vM. (8)
15Under review as submission to TMLR
Lemma A.1. Define
Bias(wM) :=1
2/angbracketleftig
H,E/bracketleftig
bM⊗bM/bracketrightig/angbracketrightig
op,Var(wM) :=1
2/angbracketleftbig
H,E/bracketleftbig
vM⊗vM/bracketrightbig/angbracketrightbig
op.
a)We have the following decomposition for the excess risk,
E/bracketleftbig
L(wM)/bracketrightbig
−L(w∗)≤/parenleftbigg/radicalig
Bias(wM) +/radicalig
Var(wM)/parenrightbigg2
.
b)Suppose the model noise ϵ(m)
tis well-specified, i.e., ϵ(m)
t:=y(m)
t−/angbracketleftig
x(m)
t,w∗/angbracketrightig
andx(m)
tare indepen-
dent and E[ϵ(m)
t] = 0, then we have the following equality for the excess risk,
E/bracketleftbig
L(wM)/bracketrightbig
−L(w∗) = Bias(wM) + Var(wM).
Proof of Lemma A.1. The proof strategy is similar to the non distributed case (see Zou et al. (2021b),
Lemma B2 and Lemma C1). For completeness we included it here.
a)By definition of the excess risk we have
L(wM)−L(w∗) =1
2/integraldisplay
H⟨wM−w∗,x⟩2Px(dx)
=1
2⟨H(wM−w∗),wM−w∗⟩
=1
2∥H1
2(wM−w∗)∥2
=1
2/vextenddouble/vextenddouble/vextenddoubleb+v/vextenddouble/vextenddouble/vextenddouble2
H,
where we used (8) for the last equality. Using Cauchy-Schwarz inequality we obtain
E[L(wM)−L(w∗)]≤/parenleftigg/radicalbigg
1
2E/vextenddouble/vextenddouble/vextenddoubleb/vextenddouble/vextenddouble/vextenddouble2
H+/radicalbigg
1
2E/vextenddouble/vextenddoublev/vextenddouble/vextenddouble2
H/parenrightigg2
=/parenleftigg/radicalbigg
1
2/angbracketleftig
H,E/bracketleftig
bM⊗bM/bracketrightig/angbracketrightig
op+/radicalbigg
1
2/angbracketleftbig
H,E/bracketleftbig
vM⊗vM/bracketrightbig/angbracketrightbig
op/parenrightigg2
b)SetP(m)
t=I−γx(m)
t⊗x(m)
t. Note that we have
b(m)
t=t/productdisplay
k=1P(m)
kb(m)
0, v(m)
t=γt/summationdisplay
i=1t/productdisplay
j=i+1ϵ(m)
iP(m)
jx(m)
i.
By assumption, we therefore have for all s,t≤n/Mandm,m′≤M,
E/bracketleftig
b(m)
s⊗v(m′)
t/bracketrightig
=γE
s/productdisplay
k=1P(m)
kb(m)
0⊗t/summationdisplay
i=1t/productdisplay
j=i+1ϵ(m′)
iP(m′)
jx(m′)
i

=γt/summationdisplay
i=1E
s/productdisplay
k=1P(m)
kb(m)
0⊗t/productdisplay
j=i+1P(m′)
jx(m′)
i
E[ϵ(m′)
i] = 0.
This implies
E/bracketleftig
bM⊗vM/bracketrightig
= 0. (9)
16Under review as submission to TMLR
From (8) we therefore have
E/bracketleftbig¯¯ηM⊗¯¯ηM/bracketrightbig
=E/bracketleftig
bM⊗bM/bracketrightig
+E/bracketleftbig
vM⊗vM/bracketrightbig
(10)
Finally, by definition of the excess risk we have
E[L(wM)−L(w∗)] =1
2E/bracketleftbigg/integraldisplay
H⟨wM−w∗,x⟩2Px(dx)/bracketrightbigg
=1
2E/bracketleftbig
⟨H(wM−w∗),wM−w∗⟩/bracketrightbig
=1
2/angbracketleftbig
H,E/bracketleftbig¯¯ηM⊗¯¯ηM/bracketrightbig/angbracketrightbig
op(11)
= Bias(wM) + Var(wM), (12)
where we used (10) for the last equality.
A.1.2 Upper Bound
For the non distributed case Zou et al. (2021b) (see Lemma B.11 and Lemma B.6 ) already established upper
bounds. More precisely we have for the local bias and variance term:
Proposition A.2. Setk∗= max/braceleftig
k:λk≥M
nγ/bracerightig
. If the step size satisfies γ <1/(τtr(H)), we have for every
m∈[M]:
a) Under Assumption 3.2 and 3.3, it holds that
Bias/parenleftig
¯w(m)
n
M/parenrightig
:=1
2/angbracketleftig
H,E/bracketleftig
b(m)
t⊗b(m)
t/bracketrightig/angbracketrightig
op
≤M2
γ2n2·∥w0−w∗∥2
H−1
0:k∗+∥w0−w∗∥H2
k∗:∞
+2τM2/parenleftig
∥w0−w∗∥2
I0:k∗+n
Mγ∥w0−w∗∥2
Hk∗:∞/parenrightig
γn(1−γτtr(H))·/parenleftigg
k∗
n+n
M2γ2/summationdisplay
i>k∗λ2
i/parenrightigg
.
b) Under Assumptions 3.2 - 3.4, it holds that
Var/parenleftig
¯w(m)
n
M/parenrightig
:=1
2/angbracketleftig
H,E/bracketleftig
¯v(m)
n⊗¯v(m)
n/bracketrightig/angbracketrightig
op≤σ2
1−γτtr(H)/parenleftigg
k∗M
n+γ2n
M·/summationdisplay
i>k∗λ2
i/parenrightigg
.
Lemma A.3. Setk∗= max/braceleftig
k:λk≥M
nγ/bracerightig
. If the step size satisfies γ < 1/(τtr(H)), we have for every
m∈[M]:
a) Under Assumption 3.2 and 3.3, it holds that
Bias(wM)≤M2
γ2n2·∥w0−w∗∥2
H−1
0:k∗+∥w0−w∗∥H2
k∗:∞
+2τM2/parenleftig
∥w0−w∗∥2
I0:k∗+n
Mγ∥w0−w∗∥2
Hk∗:∞/parenrightig
γn(1−γτtr(H))·/parenleftigg
k∗
n+n
M2γ2/summationdisplay
i>k∗λ2
i/parenrightigg
.
17Under review as submission to TMLR
b) Under Assumptions 3.2 - 3.4 , it holds that
Var(wM)≤σ2
1−γτtr(H)/parenleftigg
k∗
n+γ2n
M2·/summationdisplay
i>k∗λ2
i/parenrightigg
.
Proof of Lemma A.3 . a)For the Bias-term we simply use
Bias(wM) =1
2E/vextenddouble/vextenddouble/vextenddoubleb/vextenddouble/vextenddouble/vextenddouble2
H=1
2E/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble1
MM/summationdisplay
m=1¯bM(m)
n/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
H≤1
MM/summationdisplay
m=11
2E/vextenddouble/vextenddouble/vextenddouble¯b(m)
n/vextenddouble/vextenddouble/vextenddouble2
H=1
MM/summationdisplay
m=1Bias/parenleftig
¯w(m)
n
M/parenrightig
.(13)
Taking the bound of the local Bias-term Bias/parenleftig
¯w(m)
n
M/parenrightig
from A.2, proves the claim.
b)First we split the expectation operator as follows
E/bracketleftbig
vM⊗vM/bracketrightbig
=1
M2M/summationdisplay
m,m′=1E/bracketleftig
¯v(m)
n⊗¯v(m′)
n/bracketrightig
=1
M2M/summationdisplay
m=1E/bracketleftig
¯v(m)
n⊗¯v(m)
n/bracketrightig
+1
M2/summationdisplay
m̸=m′E/bracketleftig
¯v(m)
n⊗¯v(m′)
n/bracketrightig
=:I1+I2. (14)
Now we prove that the second operator I2is equal zero. First rewrite I2as
I2=1
M2/summationdisplay
m̸=m′M2
n2n
M−1/summationdisplay
s,t=0E[v(m)
t⊗v(m′)
s].
Therefore it is enough to to prove E[v(m)
t⊗v(m′)
s] = 0for anym̸=m′. Since we assume our data sets to
be independent we have E[v(m)
t⊗v(m′)
s] =E[⟨.,v(m)
t⟩]E[v(m′)
s] = 0, where the last equality follows from (7).
This proves I2= 0. To sum up we have from (14) for the variance term,
Var(wM) =1
2/angbracketleftbig
H,E/bracketleftbig
vM⊗vM/bracketrightbig/angbracketrightbig
op
=1
M2M/summationdisplay
m=11
2/angbracketleftig
H,E/bracketleftig
¯v(m)
n⊗¯v(m)
n/bracketrightig/angbracketrightig
op
=1
M2M/summationdisplay
m=1Var/parenleftig
¯w(m)
n
M/parenrightig
. (15)
Using the bound of the local variance term from A.2 completes the proof.
Proof of Theorem 3.5. Using lemma A.1 a)we have
E/bracketleftbig
L(wM)/bracketrightbig
−L(w∗)≤2Bias(wM) + 2Var(wM).
The claim now follows from lemma A.3.
18Under review as submission to TMLR
A.2 Proofs Lower Bound
A.2.1 Lower Bound Bias
Proposition A.4 (Lower Bound Bias) .Suppose Assumptions 3.2 and 3.6 are satisfied and let γ <1
||H||.
Recall the definition of Bias(wM)in Lemma A.1. The bias of the distributed SGD estimator satisfies the
lower bound
Bias(wM)≥M(M−1)
100γ2n2/parenleftbigg
||w1−w∗||2
H†
0:k∗+γ2n2
M2||w1−w∗||2
Hk∗:∞/parenrightbigg
.
Proof of Proposition A.4. From the definition of the bias in Lemma A.1, we have
Bias(wM) =1
2/angbracketleftig
H,E/bracketleftig
bM⊗bM/bracketrightig/angbracketrightig
op
=1
2M2M/summationdisplay
m1=1M/summationdisplay
m2=1/angbracketleftig
H,E/bracketleftig
¯b(m1)
n⊗¯b(m2)
n/bracketrightig/angbracketrightig
op
=1
2M2M/summationdisplay
m=1/angbracketleftig
H,E/bracketleftig
¯b(m)
n⊗¯b(m)
n/bracketrightig/angbracketrightig
op+1
2M2M/summationdisplay
m1̸=m2/angbracketleftig
H,E/bracketleftig
¯b(m1)
n⊗¯b(m2)
n/bracketrightig/angbracketrightig
op.(16)
We show that the first term in the above decomposition can be lower bounded by zero. Indeed, from (C.2)
and (C.4) in Zou et al. (2021b) we have for all m= 1,...,Mthe local lower bound
/angbracketleftig
H,E/bracketleftig
¯b(m)
n⊗¯b(m)
n/bracketrightig/angbracketrightig
op≥M2
n2n
M/summationdisplay
t=1n
M/summationdisplay
k=t/angbracketleftig
(I−γH)k−tH,E/bracketleftig
b(m)
t⊗b(m)
t/bracketrightig/angbracketrightig
op
≥M2
γn2/angbracketleftig
I−(I−γH)n
2M,S(m)
n
2M/angbracketrightig
op,
where we set
S(m)
n
2M:=n
2M/summationdisplay
t=1E/bracketleftig
b(m)
t⊗b(m)
t/bracketrightig
.
Setting B1=b(m)
1⊗b(m)
1= (w1−w∗)⊗(w1−w∗)and applying Lemma C.4 from Zou et al. (2021b) gives
then for all m= 1,...,M
S(m)
n
2M⪰θ
4Tr/bracketleftbig/parenleftbig
I−(I−γH)n
2M/parenrightbig
B1/bracketrightbig
·/parenleftbig/parenleftbig
I−(I−γH)n
2M/parenrightbig/parenrightbig
/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
PSD+n
M/summationdisplay
t=1(I−γH)t·B1·(I−γH)t
/bracehtipupleft/bracehtipdownright/bracehtipdownleft /bracehtipupright
PSD
⪰0.
Hence,
1
2M2M/summationdisplay
m=1/angbracketleftig
H,E/bracketleftig
¯b(m)
n⊗¯b(m)
n/bracketrightig/angbracketrightig
op≥1
2M2·M2
γn2M/summationdisplay
m=1/angbracketleftig
I−(I−γH)n
2M,S(m)
n
2M/angbracketrightig
op
≥0. (17)
19Under review as submission to TMLR
We now bound the second term in equation 16. Note that by independence of the local nodes and with
equation 6 we may write for any fixed m1̸=m2
E/bracketleftig
¯b(m1)
n⊗¯b(m2)
n/bracketrightig
=M2
n2n
M/summationdisplay
t=1n
M/summationdisplay
k=1E/bracketleftig
b(m1)
t/bracketrightig
⊗E/bracketleftig
b(m2)
k/bracketrightig
=M2
n2n
M/summationdisplay
t=1n
M/summationdisplay
k=1(I−γH)t·B1·(I−γH)k.
Hence,
1
2M2M/summationdisplay
m1̸=m2/angbracketleftig
H,E/bracketleftig
¯b(m1)
n⊗¯b(m2)
n/bracketrightig/angbracketrightig
op=1
2M2M2
n2M/summationdisplay
m1̸=m2n
M/summationdisplay
t=1n
M/summationdisplay
k=1/angbracketleftbig
H,(I−γH)t·B1·(I−γH)k/angbracketrightbig
op
=M(M−1)
2γn2/angbracketleftiggn
M/summationdisplay
k=1(I−γH)k/parenleftbig
I−(I−γH)n
M+1/parenrightbig
,B1/angbracketrightigg
op
=M(M−1)
2γ2n2/angbracketleftig/parenleftbig
I−(I−γH)n
M+1/parenrightbig2H−1,B1/angbracketrightig
op.
Following now the lines of the proof of Lemma C.5 in Zou et al. (2021b) (adapted to our local setting) gives
1
2M2M/summationdisplay
m1̸=m2/angbracketleftig
H,E/bracketleftig
¯b(m1)
n⊗¯b(m2)
n/bracketrightig/angbracketrightig
op≥M(M−1)
100γ2n2/parenleftbigg
||w1−w∗||2
H†
0:k∗+γ2n2
M2||w1−w∗||2
Hk∗:∞/parenrightbigg
.
Combining now the last bound with equation 17 and equation 16 finally gives
Bias(wM)≥M(M−1)
100γ2n2/parenleftbigg
||w1−w∗||2
H†
0:k∗+γ2n2
M2||w1−w∗||2
Hk∗:∞/parenrightbigg
.
A.2.2 Lower Bound Variance
Proposition A.5 (Lower Bound Variance) .Suppose Assumptions 3.2 and 3.6 are satisfied and letn
M≥500,
γ <1
||H||. Recall the definition of Var(wM)in Lemma A.1. The variance of the distributed SGD estimator
satisfies the lower bound
Var(wM)≥σ2
noise
100·
k∗
n+γ2n
M2/summationdisplay
j>k∗λ2
j
.
Proof of Proposition A.5. From the definition of the variance in Lemma A.1, we have
Var(wM) =1
2/angbracketleftbig
H,E/bracketleftbig
vM⊗vM/bracketrightbig/angbracketrightbig
op
=1
2M2M/summationdisplay
m1=1M/summationdisplay
m2=1/angbracketleftig
H,E/bracketleftig
¯v(m1)
n⊗¯v(m2)
n/bracketrightig/angbracketrightig
op
=1
2M2M/summationdisplay
m=1/angbracketleftig
H,E/bracketleftig
¯v(m)
n⊗¯v(m)
n/bracketrightig/angbracketrightig
op+1
2M2M/summationdisplay
m1̸=m2/angbracketleftig
H,E/bracketleftig
¯v(m1)
n⊗¯v(m2)
n/bracketrightig/angbracketrightig
op.(18)
20Under review as submission to TMLR
We first lower bound the first term. By Eq. (C.3) and Lemma C.3 in Zou et al. (2021b) (adapted to our
local setting) we obtain
1
2M2M/summationdisplay
m=1/angbracketleftig
H,E/bracketleftig
¯v(m)
n⊗¯v(m)
n/bracketrightig/angbracketrightig
op≥1
2M2M2
n2M/summationdisplay
m=1n
M−1/summationdisplay
t=0n
M−1/summationdisplay
k=t/angbracketleftig
(I−γH)k−tH,E/bracketleftig
v(m)
t⊗v(m)
t/bracketrightig/angbracketrightig
op
≥σ2
noise
100M2M/summationdisplay
m=1
M
nk∗+γ2n
M/summationdisplay
j>k∗λ2
j

=σ2
noise
100Vk∗(n,M ),
where
Vk∗(n,M ) :=
k∗
n+γ2n
M2/summationdisplay
j>k∗λ2
j
.
To derive the final bound we argue that the second term in equation 18 is zero. Indeed, by independence of
the local nodes we may write for any m1̸=m2with equation 7
E/bracketleftig
v(m1)
t⊗v(m2)
k/bracketrightig
=E/bracketleftig
v(m1)
t/bracketrightig
⊗E/bracketleftig
v(m2)
k/bracketrightig
= (I−γH)t(v(m1)
0⊗v(m2)
0)(I−γH)k
= 0,
sincev(m)
0= 0for allm= 1,...,M. Hence,
1
2M2M/summationdisplay
m1̸=m2/angbracketleftig
H,E/bracketleftig
¯v(m1)
n⊗¯v(m2)
n/bracketrightig/angbracketrightig
op= 0.
this finishes the proof.
A.3 Proofs Rates of Convergence
Proof of Corollary 3.9. Let the sequence Mn≤/radicalig
γ(1−2γτ)n
R2. By definition of k∗we know that k∗=˜d=/parenleftig
n
Mn/parenrightigr
andhenceλk∗=/parenleftbigMn
n/parenrightbigr. WefirstboundthebiasfromTheorem3.5. Since ||w∗||2≤Rbyassumption,
we find
||w∗||2
H†
0:k∗≤||w∗||2
2
λk∗≤R2/parenleftbiggn
Mn/parenrightbiggr
. (19)
Similarly, sincen
Mn→∞asn→∞, there exists n0∈Nsuch that
||w∗||2
Hk∗:∞≤R2
/parenleftig
n
Mn/parenrightigq
−/parenleftig
n
Mn/parenrightigr≤cn0R2/parenleftbiggMn
n/parenrightbiggq
, (20)
for anyn≥n0and somecn0<∞. Using that Tr[H] = 2and||w∗||2
I0:k∗≤R2, we find for all n≥n0, for
somen0∈N, that
2τM2/parenleftbig
||w∗|2
I0:k+γn
M||w∗||2
Hk:∞/parenrightbig
γn(1−γτTr[H])≤4 max{1,cn0}τR2
1−2γτM2
n
γn.
Note that we also use that Mn≤nand hence/parenleftbigMn
n/parenrightbigq−1≤1, sinceq>1. Since
Mn≤/radicalbigg
γ(1−2γτ)n
R2
21Under review as submission to TMLR
we have
τR2
1−2γτM2
n
γn≤1
and hence
2τM2
n/parenleftbig
||w∗|2
I0:k+γn
M||w∗||2
Hk:∞/parenrightbig
γn(1−γτTr[H])≤4 max{1,cn0}. (21)
We further observe that by the definition of the spectrum of H
/summationdisplay
j>k∗λ2
l=d/summationdisplay
j=˜d1
d−˜d=1/parenleftig
n
Mn/parenrightigq
−/parenleftig
n
Mn/parenrightigr≤cn0/parenleftbiggMn
n/parenrightbiggq
,
for anynsufficiently large, by using the argumentation as above. Hence,
Vk∗(n,Mn) :=k∗
n+γ2n
M2n∞/summationdisplay
j=k∗+1λ2
j
≤max{1,cn0}1
Mn·/parenleftigg/parenleftbiggMn
n/parenrightbigg1−r
+γ2/parenleftbiggMn
n/parenrightbiggq−1/parenrightigg
. (22)
Combining (19), (20), (21) and (22), we find for the bias term
Bias(Mn)≤R2
γ2(n/Mn)2/parenleftbiggn
Mn/parenrightbiggr
+cn0R2/parenleftbiggMn
n/parenrightbiggq
+ 4 max{1,cn0}Vk∗(n,Mn)
≤max{1,cn0}R2/parenleftigg
1
γ2/parenleftbiggMn
n/parenrightbigg2−r
+/parenleftbiggMn
n/parenrightbiggq/parenrightigg
+ (23)
4 max{1,cn0}21
Mn·/parenleftigg/parenleftbiggMn
n/parenrightbigg1−r
+γ2/parenleftbiggMn
n/parenrightbiggq−1/parenrightigg
. (24)
We now turn to the bound of the variance term. From equation 22 we have
Var(Mn)≤max{1,cn0}/parenleftbiggσ2
1−γτTr[H]/parenrightbigg
·1
Mn·/parenleftigg/parenleftbiggMn
n/parenrightbigg1−r
+γ2/parenleftbiggMn
n/parenrightbiggq−1/parenrightigg
.
Combining the bounds for bias and variance leads to the total error bound
E/bracketleftbig
L(wMn)/bracketrightbig
−L(w∗)≤
2˜cn0·cγ,τ,σ·/parenleftigg
R2/parenleftigg
1
γ2/parenleftbiggMn
n/parenrightbigg2−r
+/parenleftbiggMn
n/parenrightbiggq/parenrightigg
+·1
Mn·/parenleftigg/parenleftbiggMn
n/parenrightbigg1−r
+γ2/parenleftbiggMn
n/parenrightbiggq−1/parenrightigg/parenrightigg
,
with
cγ,τ,σ := 1 +σ2
1−γτTr[H],˜cn0= 4 max{1,cn0}2,
holding for any nsufficiently large. We proceed by further simplifying the right hand side of the above
inequality. Since τ≥1and1−γτTr[H]<1, the assumption on Mnimplies that
M2
n≤nγ
R2,
further implying that
R2
γ/parenleftbiggMn
n/parenrightbigg2−r
≤1
Mn/parenleftbiggMn
n/parenrightbigg1−r
22Under review as submission to TMLR
and
R2/parenleftbiggMn
n/parenrightbiggq
≤γ
Mn/parenleftbiggMn
n/parenrightbiggq−1
.
As a result, applying Theorem 3.5, the excess risk can be bounded by
E/bracketleftbig
L(wMn)/bracketrightbig
−L(w∗)≤4˜cn0·cγ,τ,σ·1
Mn/parenleftigg/parenleftbigg1
γ+ 1/parenrightbigg/parenleftbiggMn
n/parenrightbigg1−r
+ (γ+γ2)/parenleftbiggMn
n/parenrightbiggq−1/parenrightigg
≤4˜cn0·cγ,τ,σ·1
γMn/parenleftigg/parenleftbiggMn
n/parenrightbigg1−r
+/parenleftbiggMn
n/parenrightbiggq−1/parenrightigg
.
In the last step we use that γ <1
2τ<1
2.
Proof of Corollary 3.10. Assume the sequence (Mn)nsatisfiesMn/n→0asn→∞. We use Theorem 3.5
to bound the excess risk and find estimates for bias and variance. By the definition of k∗we have
k∗= max/braceleftigg
k∈N:k≤/parenleftbiggγn
Mn/parenrightbigg1
1+r/bracerightigg
=/floorleftigg/parenleftbiggγn
Mn/parenrightbigg1
1+r/floorrightigg
.
Hence, there exists n0∈Nsuch that for all n≥n0
cn0/parenleftbiggγn
Mn/parenrightbigg1
1+r
≤k∗≤Cn0/parenleftbiggγn
Mn/parenrightbigg1
1+r
,
for some constants 0<cn0≤Cn0. Therefore,
λk∗= (k∗)−(1+r)≤/parenleftbigg1
cn0/parenrightbigg1+r
·Mn
γn
and1
λk∗= (k∗)1+r≤C1+r
n0·n
γMn.
We therefore get for the first two terms of the bias
M2
n
γ2n2·||w∗||2
H†
0:k∗≤R2M2
n
γ2n2λk∗(25)
≤C1+r
n0R2
γ·Mn
n. (26)
and
||w∗||2
Hk∗:∞≤R2λk∗≤R2/parenleftbigg1
cn0/parenrightbigg1+r
·Mn
γn. (27)
We now bound the last term of the bias. To this end, we apply a well known bound for sums over decreasing
functions, i.e.,/summationdisplay
j≥kf(j)≤/integraldisplay∞
kf(x)dx.
This gives
/summationdisplay
j>k∗λ2
j≤/integraldisplay∞
k∗x−2(r+1)dx≤1
2r+ 1(k∗)−(2r+1)≤1
2r+ 1c−(2r+1)
n0/parenleftbiggMn
γn/parenrightbigg1+r
1+r
.
23Under review as submission to TMLR
Thus,
Vk∗(n,Mn) =k∗
n+γ2n
M2∞/summationdisplay
j=k∗+1λ2
j
≤1
nCn0/parenleftbiggγn
Mn/parenrightbigg1
1+r
+γ2n
M2nc−(2r+1)
n0
2r+ 1/parenleftbiggMn
γn/parenrightbigg1+r
1+r
≤c′
r,n0/parenleftigg
1
n/parenleftbiggγn
Mn/parenrightbigg1
1+r
+γ
Mn/parenleftbiggMn
γn/parenrightbiggr
1+r/parenrightigg
≤2c′
r,n0·γ
Mn/parenleftbiggMn
γn/parenrightbiggr
1+r
, (28)
with
c′
r,n0= max/braceleftigg
Cn0,c−(2r+1)
n0
2r+ 1/bracerightigg
.
Moreover,
2τM2
n/parenleftig
||w∗||2
I0:k∗+γn
Mn||w∗||2
Hk∗:∞/parenrightig
γn(1−γτTr[H])≤2τM2
n
γn(1−γτTr[H])/parenleftbigg
R2+R2γn
Mnλk∗/parenrightbigg
≤c′′
n02τM2
n
γn(1−γτTr[H])R2/parenleftbigg
1 +γn
MnMn
γn/parenrightbigg
≤2c′′
n02τ
(1−γτTr[H])·R2M2
n
γn,
with
c′′
n0= max/braceleftigg
1,/parenleftbigg1
cn0/parenrightbigg1+r/bracerightigg
.
Hence, combining this with equation 28 and choosing
Mn≤√γn
R(29)
leads to
2τM2
n/parenleftig
||w∗||2
I0:k∗+γn
Mn||w∗||2
Hk∗:∞/parenrightig
γn(1−γτTr[H])·Vk∗(n,Mn)
≤2c′′
n02τ
(1−γτTr[H])·R2M2
n
γn·2c′
r,n0·γ
Mn/parenleftbiggMn
γn/parenrightbiggr
1+r
≤cr,n0τ
(1−γτTr[H])·R2M2
n
γn·γ
Mn/parenleftbiggMn
γn/parenrightbiggr
1+r
≤cr,n0τ
(1−γτTr[H])·γ
Mn/parenleftbiggMn
γn/parenrightbiggr
1+r
, (30)
wherecr,n0= 8c′′
n0·c′
r,n0. Combining (26), (27) and (30), we find for all n≥n0
Bias(Mn)≤˜cr,n0·R2
γ·Mn
n+cr,n0τ
(1−γτTr[H])·γ
Mn/parenleftbiggMn
γn/parenrightbiggr
1+r
, (31)
where we set
˜cr,n0= 2 max/braceleftigg
C1+r
n0,/parenleftbigg1
cn0/parenrightbigg1+r/bracerightigg
.
24Under review as submission to TMLR
We now turn to bounding the variance. Using equation 28 once more, the variance can be bounded by
Var(Mn)≤σ2
1−γτTr[H]·2c′
r,n0·γ
Mn/parenleftbiggMn
γn/parenrightbiggr
1+r
.
Combining the bias bound equation 31 with the variance bound, we obtain for the excess risk
E/bracketleftbig
L(wMn)/bracketrightbig
−L(w∗)≤cr,n0,τ,σ/parenleftigg
R2
γ·Mn
n+γ
Mn/parenleftbiggMn
γn/parenrightbiggr
1+r/parenrightigg
.
cr,n0,τ,σ:= max/braceleftbigg
˜cr,n0,2cr,n0·c′
r,n0·max{τ,σ2}
1−γτTr[H]/bracerightbigg
.
Note that the choice
Mn≤/parenleftigγ
R2/parenrightig1+r
2+r·(γn)1
2+r (32)
leads to a dominating variance part, i.e.
R2
γ·Mn
n≤γ
Mn/parenleftbiggMn
γn/parenrightbiggr
1+r
and
E/bracketleftbig
L(wMn)/bracketrightbig
−L(w∗)≤2cr,n0,τ,σγ
Mn/parenleftbiggMn
γn/parenrightbiggr
1+r
.
Note that the choice equation 32 is compatible with the choice equation 29, i.e.,
Mn≤/parenleftigγ
R2/parenrightig1+r
2+r·(γn)1
2+r≤√γn
R,
following from the fact that r>0, provided that nis sufficiently large.
B PROOFS SECTION 4 ( COMPARISON OF SAMPLE COMPLEXITY OF DSGD
AND DRR)
B.1 Lower Bound for distributed ridge regression
In this section we derive a lower bound for the distributed RR estimator. We adopt the following notation
and assumptions from Tsigler & Bartlett (2020).
•H−1/2x, wherex∈Rdis sub-Gaussian with independent components
•X= (√λ1z1,...,√λdzd)withzjbeing sub-Gaussian with independent components
•A:=XXT+λIn,Am:=XmXT
m+λIn
•A−j=/summationtext
i̸jλizizT
i+λIn
Crucial for our analysis is the following quantity, called the local effective dimension for the RR problem:
k∗
RR:= min

k:λk+1≤M/parenleftig
λ+/summationtext
j>kλj/parenrightig
bn

. (33)
25Under review as submission to TMLR
B.1.1 Bias-Variance Decomposition DRR
Definition B.1 (Bias and Variance of Distributed RR) .Let
Πm(λ) :=/parenleftbig
XT
mXm+λ/parenrightbig−1XT
mXm−Id,
/hatwidestBias(wRR
n(λ)) :=/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleH1/2/parenleftigg
1
MM/summationdisplay
m=1Πm(λ)w∗/parenrightigg/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle2
,
/hatwidestVar(wRR
n(λ)) :=/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleH1/2/parenleftigg
1
MM/summationdisplay
m=1(XT
mXm+λ)−1XT
mϵm/parenrightigg/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle2
.
We call
Bias(wRR
n(λ)) =E/bracketleftig
/hatwidestBias(wRR
n(λ))/bracketrightig
the (expected) bias of the distributed RR estimator and
Var(wRR
n(λ)) =E/bracketleftig
/hatwidestVar(wRR
n(λ))/bracketrightig
the (expected) variance.
We immediately obtain:
Lemma B.2. The excess risk satisfies
E/bracketleftig
||H1/2(wRR
n(λ)−w∗)||2/bracketrightig
= Bias(wRR
n(λ)) + Var(wRR
n(λ)).
Proof of Lemma B.2. We split the excess risk as
||H1/2(wRR
n(λ)−w∗)||2=/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleH1/2/parenleftigg
1
MM/summationdisplay
m=1ˆwRR
m(λ)−w∗/parenrightigg/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle2
=/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleH1/2/parenleftigg
1
MM/summationdisplay
m=1(XT
mXm+λ)−1XT
mYm−w∗/parenrightigg/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle2
=/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleH1/2/parenleftigg
1
MM/summationdisplay
m=1(XT
mXm+λ)−1XT
m(Xmw∗+ϵm)−w∗/parenrightigg/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle2
=/hatwidestBias(wRR
n(λ)) +/hatwidestVar(wRR
n(λ))
+2
M2M/summationdisplay
m=1M/summationdisplay
m′=1/angbracketleftbig
HΠm(λ)w∗,(XT
mXm+λ)−1XT
mϵm/angbracketrightbig
.
We argue that the expectation with respect to the noise (i.e. conditioned on X) of the last term is equal to
zero. Indeed, by linearity and since ϵmis centered (conditioned on Xm) for allm∈[M], we find
Eϵm/bracketleftbig/angbracketleftbig
HΠm(λ)w∗,(XT
mXm+λ)−1XT
mϵm/angbracketrightbig/bracketrightbig
=/angbracketleftbig
HΠm(λ)w∗,(XT
mXm+λ)−1XT
mEϵm[ϵm]/angbracketrightbig
= 0.
Hence,
E/bracketleftig
||H1/2(wRR
n(λ)−w∗)||2/bracketrightig
=E/bracketleftig
/hatwidestBias(wRR
n(λ))/bracketrightig
+E/bracketleftig
/hatwidestVar(wRR
n(λ))/bracketrightig
.
26Under review as submission to TMLR
B.1.2 Lower Bound of Bias for DRR
PropositionB.3 (LowerBoundofBiasforlocalRR) .Assume His strictly positive definite with Tr[H]<∞.
There exist absolute constants b>1,c>1such that
Bias(wRR
n(λ))≥M−1
cM·
M2/parenleftig
λ+/summationtext
j>k∗
RRλj/parenrightig2
n2·||w∗||2
H−1
0:k∗
RR+||w∗||2
Hk∗
RR:∞
,
wherek∗
RRis defined in equation 33.
For proving this Proposition we need the following Lemma.
Lemma B.4. Let˜X∈Rn×dbe an independent copy of X∈Rn×dand set ˜A=˜X˜XT+λ. Define further
the operator
B:= (Id−XTA−1X)H(Id−˜XT˜A−1˜X).
1. For any i̸=j, we have
EX,˜X[Bij] = 0.
2. The diagonal elements satisfy for any k
EX,˜X[Bii]≥1
c·λi/parenleftig
1 +λi
λk+1·n
ρk/parenrightig2,
for some absolute constant c>1and where we define
ρk=λ+/summationtext
j>kλj
λk+1.
Proof of Lemma B.4. Recall that H=diag{λ1,...,λd}jand
X= (/radicalbig
λ1z1,...,/radicalbig
λdzd),˜X= (/radicalbig
λ1˜z1,...,/radicalbig
λd˜zd).
1. Leti̸=j. We expand
Bij=⟨ei,Hej⟩/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
=0−/radicalbig
λi/angbracketleftbig
ej,HXTA−1zi/angbracketrightbig
−/radicalbig
λj/angbracketleftbig
ei,H˜XT˜A−1˜zj/angbracketrightbig
+√zizj/angbracketleftbig
zi,A−1XH˜XT˜A−1˜zj/angbracketrightbig
=−λj/radicalbig
λiλj/angbracketleftbig
zj,A−1zi/angbracketrightbig
−λi/radicalbig
λiλj/angbracketleftbig
˜zi,˜A−1˜zj/angbracketrightbig
+/radicalbig
λiλj/angbracketleftbig
zi,A−1XH˜XT˜A−1˜zj/angbracketrightbig
.
We define the map F(zj) :=/angbracketleftbig
zj,A−1zi/angbracketrightbig
. Following the lines of the proof of Lemma C.7 in Zou
et al. (2021a) shows that Ezj[F(zj)] = 0. Using similar arguments, the same is true for the second
and last term, showing the result.
2. We expand
Bii=/angbracketleftig
H(ei−/radicalbig
λiXTA−1zi),ei−/radicalbig
λi˜XT˜A−1˜zi/angbracketrightig
=⟨Hei,ei⟩/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
=λi+λi/angbracketleftbig
HXTA−1zi,˜XT˜A−1˜zi/angbracketrightbig
−/radicalbig
λi/angbracketleftbig
Hei,˜XT˜A−1˜zi/angbracketrightbig
−/radicalbig
λi/angbracketleftbig
Hei,XTA−1zi/angbracketrightbig
=λi/bracketleftbig
1−λi/parenleftbig/angbracketleftbig
zi,A−1zi/angbracketrightbig
+/angbracketleftbig
˜zi,˜A−1˜zi/angbracketrightbig/parenrightbig/bracketrightbig
+λi/angbracketleftbig
HXTA−1zi,˜XT˜A−1˜zi/angbracketrightbig
.
Setting
ai:=/angbracketleftbig
zi,A−1zi/angbracketrightbig
,˜ai:=/angbracketleftbig
˜zi,˜A−1˜zi/angbracketrightbig
27Under review as submission to TMLR
we further find that
λi/angbracketleftbig
HXTA−1zi,˜XT˜A−1˜zi/angbracketrightbig
=λid/summationdisplay
j=1λj(XTA−1zi)j·(˜XT˜A−1˜zi)j
=λid/summationdisplay
j=1λ2
j/angbracketleftbig
zj,A−1zi/angbracketrightbig
·/angbracketleftbig
˜zj,˜A−1zi/angbracketrightbig
=λ3
i·ai·˜ai+λi/summationdisplay
j̸=iλ2
j/angbracketleftbig
zj,A−1zi/angbracketrightbig
·/angbracketleftbig
˜zj,˜A−1zi/angbracketrightbig
.
By independence, the last term is non-negative in expectation, i.e.
EX,˜X
λi/summationdisplay
j̸=iλ2
j/angbracketleftbig
zj,A−1zi/angbracketrightbig
·/angbracketleftbig
˜zj,˜A−1zi/angbracketrightbig

=λi/summationdisplay
j̸=iλ2
jEX/bracketleftbig/angbracketleftbig
zj,A−1zi/angbracketrightbig/bracketrightbig
·E˜X/bracketleftbig/angbracketleftbig
˜zj,˜A−1zi/angbracketrightbig/bracketrightbig
=λi/summationdisplay
j̸=iλ2
j·EX/bracketleftbig/angbracketleftbig
zj,A−1zi/angbracketrightbig/bracketrightbig2
≥0.
Hence, for deriving a lower bound in expectation it is sufficient to lower bound the expression
λi·[1−λi(ai+ ˜ai)] +λ3
i·ai·˜ai=λi·(1−λiai)·(1−λi˜ai).
Using independence once more we find
EX,˜X[Bii]≥λi·EX,˜X[(1−λiai)·(1−λi˜ai)].
We proceed as in the proof of Lemma C.7 in Zou et al. (2021a). Recall that
(1−λiai) =1
1 +λi/angbracketleftbig
zi,A−1
−izi/angbracketrightbig
and for allk/angbracketleftbig
zi,A−1
−izi/angbracketrightbig
≤c·n
λk+1ρk,
for somec>0, with high probability. Concluding as in Zou et al. (2021a) and using independence
finishes the proof.
Proof of Proposition B.3. Settingwm(λ) =H1/2Πm(λ)w∗(see Definition B.1), we decompose the bias as
Bias(wRR
n(λ)) =E/bracketleftig
/hatwidestBias(wRR
n(λ))/bracketrightig
=E
/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle1
MM/summationdisplay
m=1wm(λ)/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle2

=1
M2E/bracketleftigg
Tr/bracketleftigg/parenleftiggM/summationdisplay
m=1wm(λ)/parenrightigg
⊗/parenleftiggM/summationdisplay
m′=1wm′(λ)/parenrightigg/bracketrightigg/bracketrightigg
=1
M2M/summationdisplay
m=1E[Tr[wm(λ)⊗wm(λ)]] +1
M2/summationdisplay
m̸=m′E[Tr[wm(λ)⊗wm′(λ)]].(34)
28Under review as submission to TMLR
We aim to find a lower for the above expression. Since
M/summationdisplay
m=1E[Tr[wm(λ)⊗wm(λ)]]⪰0
we proceed to lower bound the second term in equation 34 . Setting
Bm,m′:= Πm(λ)◦H◦Πm′(λ)
form,m′∈[M]we may write
Bias(wRR
n(λ))≥1
M2/summationdisplay
m̸=m′E[Tr[wm(λ)⊗wm′(λ)]]
=1
M2/summationdisplay
m̸=m′E[⟨H◦Πm(λ)w∗,Πm′(λ)⟩]
=1
M2/summationdisplay
m̸=m′E[⟨Bm,m′w∗,w∗⟩]
=1
M2/summationdisplay
m̸=m′
/summationdisplay
iE[(Bm,m′)ii](w∗
i)2+ 2/summationdisplay
i>jE[(Bm,m′)ij]w∗
i·w∗
j
. (35)
We now apply Lemma B.4 and follow the lines of the proof of Theorem C.8 in Zou et al. (2021a) to obtain
for everyk
Bias(wRR
n(λ))≥1
M2/summationdisplay
m̸=m′/summationdisplay
iE[(Bm,m′)ii](w∗
i)2
≥1
cM2/summationdisplay
m̸=m′/summationdisplay
iλi·(w∗
i)2
/parenleftig
1 +λi
λk+1·n
Mρk/parenrightig2
=M−1
cM/summationdisplay
iλi·(w∗
i)2
/parenleftig
1 +λi
λk+1·n
Mρk/parenrightig2
≥M−1
cM·
M2/parenleftig
λ+/summationtext
j>k∗
RRλj/parenrightig2
n2·||w∗||2
H−1
0:k∗
RR+||w∗||2
Hk∗
RR:∞
, (36)
for somec>1.
B.1.3 Lower Bound of Variance for DRR
Proposition B.5 (Lower Bound of Bias for local RR) .Supposek∗
RR<n
c′M, for some universal constant
c′>1. There exist constants b,c> 1such that
Var(wRR
n(λ))≥σ2
c
k∗
RR
n+n
M2·(λ+/summationtext
j>k∗
RRλj)·/summationdisplay
j>k∗
RRλ2
j
,
wherek∗
RRis defined in equation 33.
29Under review as submission to TMLR
Proof of Proposition B.5. By definition of the variance, we may write
Var(wRR
n(λ)) =E/bracketleftig
/hatwidestVar(wRR
n(λ))/bracketrightig
=E
/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleH1/2/parenleftigg
1
MM/summationdisplay
m=1(XT
mXm+λ)−1XT
mϵm/parenrightigg/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle2

=1
M2M/summationdisplay
m,m′=1EX/bracketleftig
Tr/bracketleftig
H1/2(XT
mXm+λ)−1XT
mEϵm[ϵm⊗ϵm′]Xm(XT
m′Xm′+λ)−1H1/2/bracketrightig/bracketrightig
=σ2
M2M/summationdisplay
m=1EX/bracketleftig
Tr/bracketleftig
H1/2(XT
mXm+λ)−1XT
mXm(XT
mXm+λ)−1H1/2/bracketrightig/bracketrightig
=1
M2M/summationdisplay
m=1Var( ˆwRR
m(λ)),
where the local variance is given by
Var( ˆwRR
m(λ)) =σ2EX/bracketleftig
Tr/bracketleftig
H1/2(XT
mXm+λ)−1XT
mXm(XT
mXm+λ)−1H1/2/bracketrightig/bracketrightig
.
To lower bound the variance we utilize Theorem C.5 from Zou et al. (2021a) (see also Bartlett et al. (2020))
and obtain
Var(wRR
n(λ))≥σ2
cM2M/summationdisplay
m=1
M·k∗
RR
n+n
M·(λ+/summationtext
j>k∗
RRλj)·/summationdisplay
j>k∗
RRλ2
j

=σ2
c
k∗
RR
n+n
M2·(λ+/summationtext
j>k∗
RRλj)·/summationdisplay
j>k∗
RRλ2
j
,
providedk∗
RR<n
c′M, for some universal constants c,c′>1.
B.1.4 Proof of Theorem 4.2
The proof follows by combining Proposition B.3 and Proposition B.5 with Lemma B.2.
B.2 Upper Bound Excess Risk Tail-Averaged DSGD
Theorem B.6 (Upper Bound Tail-averaged DSGD) .Suppose Assumption 3.7 is satisfied. Let wMndenote
the tail-averaged distributed estimator with ntraining samples and assume γ < 1/Tr[H]. For arbitrary
k1,k2∈[d]
E/bracketleftbig
L(wM)/bracketrightbig
−L(w∗) = Bias(wM) + Var(wM)
with
Bias(wM)≤cbM2
γ2n2·/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleexp/parenleftig
−n
MγH/parenrightig
w∗/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle2
H−1
0:k1+||w∗||2
Hk1:∞,
Var(wM)≤cv(1 +R2)·σ2
k2
n+nγ2
M2·/summationdisplay
j>k2λ2
j
,
for some universal constants cb,cv>0.
30Under review as submission to TMLR
Proof of Theorem B.6. Utilizing equation 13 and Lemma 6.1 in Zou et al. (2021a), we have
Bias(wM) =1
MM/summationdisplay
m=1Bias/parenleftig
¯w(m)
n
M/parenrightig
≤1
MM/summationdisplay
m=1cbM2
γ2n2·/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleexp/parenleftig
−n
MγH/parenrightig/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle2
H−1
0:k1+||w∗||2
Hk1:∞
=cbM2
γ2n2·/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleexp/parenleftig
−n
MγH/parenrightig
w∗/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle2
H−1
0:k1+||w∗||2
Hk1:∞,
for some universal constant cb>0.
For the variance, we utilize equation 15 and Lemma 6.1 in Zou et al. (2021a) once more to obtain
Var(wM)≤1
M2M/summationdisplay
m=1Var/parenleftig
¯w(m)
n
M/parenrightig
≤cv(1 +R2)·σ2
M2M/summationdisplay
m=1
k2M
n+nγ2
M·/summationdisplay
j>k2λ2
j

=cv(1 +R2)·σ2
k2
n+nγ2
M2·/summationdisplay
j>k2λ2
j
,
for some universal constant cv>0.
B.3 Comparing DSGD with DRR
B.3.1 Proof of Theorem 4.4
To prove Theorem 4.4 we derive conditions on nRRandnSGDsuch that the upper bound for the excess risk
ofwMfor DSGD from Theorem 4.3 can be upper bounded by the lower bound of wRR
n(λ)for DRR from
Theorem 4.2, i.e. such that
cbM2
γ2n2
SGD·/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleexp/parenleftig
−nSGD
MγH/parenrightig
w∗/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle2
H−1
0:k∗
RR+||w∗||2
Hk∗
RR:∞
≤M2/parenleftig
λ+/summationtext
j>k∗
RRλj/parenrightig2
cn2
RR·||w∗||2
H−1
0:k∗
RR+||w∗||2
Hk∗
RR:∞(37)
and
cv/parenleftbigg
1 +||w∗||2
σ2/parenrightbigg
·σ2
k∗
RR
nSGD+nSGDγ2
M2·/summationdisplay
j>k∗
RRλ2
j
≤σ2
c/parenleftigg
k∗
RR
nRR+nRR
M2·/summationtext
j>k∗
RRλ2
j
(λ+/summationtext
j>k∗
RRλj)2/parenrightigg
.(38)
We start with equation 38. For
cv/parenleftbigg
1 +||w∗||2
σ2/parenrightbigg
·σ2k∗
RR
nSGD≤σ2
ck∗
RR
nRR
to hold we need that
C∗nRR≤nSGD, C∗:=cv·c·/parenleftbigg
1 +||w∗||2
σ2/parenrightbigg
. (39)
31Under review as submission to TMLR
To
cv/parenleftbigg
1 +||w∗||2
σ2/parenrightbigg
·σ2nSGDγ2
M2·/summationdisplay
j>k∗
RRλ2
j≤σ2
cnRR
M2·/summationtext
j>k∗
RRλ2
j
(λ+/summationtext
j>k∗
RRλj)2
to hold we need
nSGD≤nRR
C∗·(C∗
λ)2γ2, C∗
λ:=λ+/summationdisplay
j>k∗
RRλj. (40)
Finally, from equation 37 we need
cbM2
γ2n2
SGD·/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleexp/parenleftig
−nSGD
MγH/parenrightig
w∗/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle2
H−1
0:k∗
RR≤M2(C∗
λ)2
cn2
RR·||w∗||2
H−1
0:k∗
RR. (41)
To ensure this, note that
/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleexp/parenleftig
−nSGD
MγH/parenrightig
w∗/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle2
H−1
0:k∗
RR≤e−nSGD
Mγλk∗
RR·||w∗||2
H−1
0:k∗
RR≤(1−γλk∗
RR)·||w∗||2
H−1
0:k∗
RR.
Hence, equation 41 is implied if
cb
γ2n2
SGD(1−γλk∗
RR)≤(C∗
λ)2
cn2
RR,
being equivalent to/radicalig
ccb(1−γλk∗
RR)
γC∗
λnRR≤nSGD. (42)
Combining conditions equation 39, equation 40 and equation 42 we need
max

C∗,/radicalig
ccb(1−γλk∗
RR)
γC∗
λ

·nRR≤nSGD≤1
C∗·(C∗
λ)2γ2·nRR.
A short calculation shows that the condition
γ <min/braceleftbigg1
Tr[H],1√cC∗C∗
λ/bracerightbigg
implies that
max

C∗,/radicalig
ccb(1−γλk∗
RR)
γC∗
λ

≤1
C∗·(C∗
λ)2γ2.
This finishes the proof.
B.3.2 Discussion
We give a more detail discussion about the sample complexities (SCs) of DSGD and DRR. In particular, we
derive conditions under which the SCs are of the same order to ensure that
E/bracketleftig
L(wMnSGD)/bracketrightig
−L(w∗)≤E/bracketleftbig
L(wRR
nRR(λnRR))/bracketrightbig
−L(w∗).
Recall that in order to achieve this bound we would need
LλnRR,γ·nRR≤nSGD≤L′
λnRR,γ·nRR,
for a suitable choice of the regularization parameter λnRRand number of machines MnSGD.
32Under review as submission to TMLR
To ensure that nRR≲nSGDwe need to require that
1≲LλnRR,γ= max

C∗,/radicalig
c(1−γλk∗
RR)
γC∗
λnRR

,
withC∗
λ:=λ+/summationtext
j>k∗
RRλj. Recall that
γ <min/braceleftigg
1
Tr[H],1√cC∗C∗
λnRR/bracerightigg
,
and that 1−γλk∗
RR<1. A short calculation shows that
1≲/radicalig
c(1−γλk∗
RR)
γC∗
λnRR
if
γ
λnRR+/summationdisplay
j>k∗
RRλj
≲1.
Furthermore, to ensure that nSGD≲nRRwe have to require that
L′
λnRR,γ=1
C∗γ2(C∗
λnRR)2≲1.
This is satisfied if
1≲γ·C∗
λnRR=γ
λnRR+/summationdisplay
j>k∗
RRλj
.
We summarize our finding the following
Corollary B.7. Suppose all assumptions of Theorem 4.4 are satisfied. If
γ
λnRR+/summationdisplay
j>k∗
RRλj
≃1 (43)
holds, then the sample complexities of DSGD and DRR are of the same order, i.e.
nSGD≃nRR
and
E/bracketleftig
L(wMnSGD)/bracketrightig
−L(w∗)≤E/bracketleftbig
L(wRR
nRR(λnRR))/bracketrightbig
−L(w∗).
Example B.8 (Spiked Covariance Model) .We show that condition equation 43 is satisfied in the spiked
covariance model from Corollary 3.9 under a suitable choice for λnRRandMnSGD. Here, we assume that
with
MnSGD=MnRR≃n3−2r
5−2r
RR,
for1/2≤r≤1, see our discussion in Section 3.4 (comparison with DOLS). A short calculation shows that
k∗
RR≃˜d≃/parenleftbiggnRR
MnRR/parenrightbiggr
≃n2r
5−2r
RR.
33Under review as submission to TMLR
Moreover, for λnRR≃n−ζ
RR,ζ≥0andγ=const., we have
γ
λnRR+/summationdisplay
j>k∗
RRλj
≃γ/parenleftig
n−ζ
RR+ 1/parenrightig
≃1.
Hence, for a wide range of regularization, the condition equation 43 is met and the SCs of DSGD and DRR
in the spiked covariance model are of the same order.
C FURTHER NUMERICAL EXPERIMENTS
In this Section we collect further experimental results conducted on simulated data from Section 5.
Figure 4: Test error for distributed ridgeless regression with λj=j−2for different sources w∗as a function
ofM=nα,α∈{0,0.1,...,0.9}. The number of local nodes acts as a regularization parameter. We generate
n= 500i.i.d. training data with xj∼N(0,H)with mildly overparameterization d= 700.
We compare the sample complexity of optimally tuned full-averaged DSGD, tail-averaged DSGD and last-
iterate DSGD with optimally tuned DRR for different sources w∗, see Figures 5, 6 and 6. Here, the data are
generated as in Section 5 with d= 200,λj=j−2andw∗
j=j−α,α∈{0,1,10}. The number of local nodes
is fixed atMn=n1/3for eachn∈{100,...,6000}.
34Under review as submission to TMLR
Figure 5: Left:λj=j−10,w∗
j= 1Middle:λj=j−10,w∗
j=j−1Right:λj=j−10,w∗
j=j−10
35Under review as submission to TMLR
Figure 6: Left:λj=j−2,w∗
j= 1Middle:λj=j−2,w∗
j=j−1Right:λj=j−2,w∗
j=j−10
36Under review as submission to TMLR
Figure 7: Left:λj=j−1,w∗
j= 1Middle:λj=j−1,w∗
j=j−1Right:λj=j−1,w∗
j=j−10
37