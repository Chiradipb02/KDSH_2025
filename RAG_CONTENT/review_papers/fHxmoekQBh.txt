MaVEn: AnEffectiveMulti-granularityHybridVisual
Encoding Framework for Multimodal Large Language
Model
Chaoya Jiang1†, Hongrui Jia1†, Haiyang Xu2∗, Wei Ye1∗, Mengfan Dong1,
Ming Yan2, Ji Zhang2, Fei Huang2, Shikun Zhang1
1National Engineering Research Center for Software Engineering, Peking University
2Alibaba Group
{jiangchaoya, wye, zhangsk}@pku.edu.cn,
{shuofeng.xhy, fei.huang}@alibaba-inc.com
Abstract
This paper presents MaVEn, an innovative Multi-granularity Visual Encoding
framework designed to enhance the capabilities of Multimodal Large Language
Models(MLLMs)inmulti-imagereasoning. CurrentMLLMsprimarilyfocuson
single-imagevisualunderstanding,limitingtheirabilitytointerpretandintegrate
informationacrossmultipleimages. MaVEnaddressesthislimitationbycombining
discretevisualsymbolsequences,whichabstractcoarse-grainedsemanticconcepts,
withtraditionalcontinuousrepresentationsequencesthatmodelfine-grainedfea-
tures. This dual approach bridges the semantic gap between visual and textual
data, thereby improving the model’s ability to process and interpret information
from multiple images effectively. Additionally, we design a dynamic reduction
mechanism by for long-sequence continuous features to enhance multi-image
processing efficiency. Experimental results demonstrate that MaVEn significantly
enhances MLLMs’ understanding in complex multi-image scenarios, while also
improving performance in single-image contexts.
1 Introduction
Current multimodal large models (MLLMs) [ 37] concentrate on understanding single images
[24,43,36,22], which significantly restricts their ability to interpret and integrate information
across multiple images. As shown in Figure 1, typical scenarios [ 17] involving multiple images
include Knowledge Based VQA, Visual Relation Inference, Multi-image Reasoning and so on. These
scenarios present a wide array of practical applications.
Presentstrategiespredominantlyadoptadata-centricapproach,wheremethodssuchasthoseproposed
in[1,15,41,2]aimtostrengthenthemulti-imagecapabilitiesofMultimodalLargeLanguageModels
(MLLMs) by introducing interleaved image-text data during the pre-training and fine-tuning phases.
Althoughsomeefficacyhasbeenachieved,trainingsolelybasedoninterleaveddatastillfallsshort
in many multi-image scenarios. This is primarily because current MLLMs remain fundamentally
designed for single-image scenarios. This raises the question of whether the visual feature encoding
and bridging methods of MLLMs, originally designed for single-image input scenarios, are suitable
for multi-image inputs.
Current MLLMs encode visual inputs using either discrete symbol encoding [ 11,13,35,4] or
continuoussequenceencoding[ 7,27,34]. Forthecontinuoussequencefeatureencodingcategory,the
∗corresponding authors.
†These authors contributed equally to this work.
38th Conference on Neural Information Processing Systems (NeurIPS 2024).Image 1: Image 2: 
Whatarethedifferenceelementsinthefirstandsecondimages?Multi Image ReasoningImage 1: Image 2: Knowledge Based VQA
The person in the first picture is Yao Ming. Which of the two people in the second picture is Yao Ming?Image 1: Image 2: Visual Relation Inference
Where is the man in the first image positioned in the second image?
1.Aplateoffood,includingasandwichandonionrings,isonatablenexttoaglass.2.Aplateoffood,includingasandwichandonionrings,isonatablenexttoaglass.Inthefirstimage,thereisacola,whileinthesecondimage,therearefriedonions.
MaVEnLLaVA1.5YaoMingistheplayerinthesecondpicture.Heiswearingabasketballuniform.ThepersoninleftofthesecondpictureisYaoMing,heiswearingaredjersey.
MaVEnLLaVA1.5Thepersoninthefirstimageisinthecenterofthesecondimage.Theoldermaninthefirstimageintheleftsideofthesecondimage.
MaVEnLLaVA1.5
Figure1: We comparedtheperformance ofthe classicsingle-imagetask trainedMLLM LLaVA1.5
[22] and our model in three multi-image scenarios including Multi Image Reasoning, Knowledge
Based VQA and Visual Relation Inference. LLaVA1.5 exhibits significant limitations in multi-image
scenarios.
following issues are present: (1) Excessively Long Visual Feature Sequences: Most current MLLMs
utilize linear layers to bridge the visual sequence outputs from Vision Transformers (ViTs) [ 7]. Given
the lengthy encoding sequences of images and the finite context input length of current MLLMs, the
extended feature sequence inputs in multi-image contexts result in complex computational overhead
and adversely affect model performance. (2) Imprecise Visual Information Encoding: Some MLLMs
employ fixed-length latent queries to encode visual features through architectures like Q-Former
[18]. While this approach somewhat reduces the length of visual sequences, recent studies [ 20]
suggestthatitstilldoesnotencodevisualinformationfromimageswithsufficientaccuracy. There
remains a misalignment with textual representations, leading to the model’s confusion regarding
visual information.
Moreover, recent works [ 11,13,4] have started to explore encoding images as discrete symbol
sequences. Comparedtocomplexcontinuousvisualrepresentations,discretevisualrepresentations
offer simpler and clearer high-level semantic abstractions, closely aligning with the discrete nature of
textualrepresentations. Consequently,discretevisualencodinglookslikemoreconducivetocomplex
multi-image reasoning. However, given that discrete visual representations tend to be coarser in
granularity, relying solely on them may overlook fine-grained details within images [40].
In this study, we introduce MaVEn: an effective and efficient Multi-granularity Hybrid Visual
Encodingframework. MaVEnutilizesdiscretevisualsymbolsequencestoabstractcoarse-grained
semantic concepts, aiding in multi-image understanding, while traditional continuous representation
sequences model fine-grained features to support detailed understanding. Accordingly, we investigate
the synergy of multi-granularity visual features within the novel framework, design a dynamic
reduction mechanism for long-sequence continuous features to enhance multi-image processing
efficiency,andproposeamulti-stagemodeltrainingmethodologyaimedatimprovingmulti-image
comprehension. Experimental results demonstrate that the proposed method effectively enhances
theunderstandingcapabilitiesofMLLMsincomplexmulti-imagescenarios,whilealsoimproving
performance in single-image contexts. In summary, the contributions of this study include:
•Weintroduceaframeworkthatcombinesdiscreteandcontinuousvisualrepresentationsto
enhance multi-image reasoning in MLLMs. This framework improves the model’s ability to
process and interpret information from multiple images effectively.
•We design a dynamic reduction mechanism for long-sequence continuous visual features to
increase the efficiency of multi-image processing in MLLMs.
•Ourapproachdemonstratesremarkableperformanceacrossvariousmulti-imagescenarios
and also shows advantages in standard single-image benchmarks.
22 Related Work
2.1 Multimodal Large Language Models
Existing Multimodal Large Language Models (MLLMs) [ 37] typically consist of a visual encoder, a
visual interface, and a large language model (LLM). The visual encoder converts visual data into
continuous sequence features. The visual interface then maps these features into the LLM’s semantic
space, allowing the LLM to process visual information. Current research focuses on developing
effective visual interfaces. There are two main types: Latent-Query based Models: Used in MLLMs
likeBLIP-2[ 18]andMiniGPT-4[ 43],thisapproachusesafixednumberoflearnablelatentvectorsas
query vectors in an attention mechanism. These vectors interact with visual sequence representations
tosummarizeandintegratevisualinformation,effectivelyreducingsequencelengthbutpotentially
losing some visual details. Linear Mapping Models: Used in MLLMs like LLaVA [ 24], this method
directlymapsvisualfeaturesequencesintotheLLM’stextembeddingspaceviaalinearlayer. This
approach retains complete visual information but results in longer output sequences.
2.2 Visual Semantic Encoding Representations in MLLMs
EfficientvisualsemanticencodinghasbecomeakeyresearchareaforMLLMs. Researchershave
developed various methods to represent visual information, including: Continuous Visual Encoders:
ExamplesincludeVisualTransformer(ViT)[ 7]andSwin-Transformer[ 27]. ViTsegmentsimagesinto
patches and processes them sequentially, while Swin-Transformer uses a sliding window mechanism
to capture local structures more efficiently. These methods excel in capturing image details but
face challenges in aligning with textual encoding [ 20]. Discrete Visual Encoders: These methods
encode images into discrete sequences similar to text tokens, aligning visual and textual information
more closely. Examples include VQ-VAE [ 35], VQ-GAN [ 13], and SEED [ 11]. VQ-VAE uses
self-supervised learning to create a visual vocabulary from image patches. VQ-GAN combines
VQ-VAE with generative adversarial networks to capture semantic information and generate high-
quality images. SEED, the latest approach, encodes images into discrete visual sequences with
one-dimensionalcausaldependencies,aimingtoextracthigh-levelsemanticsforvisualunderstanding
and generation tasks.
3 Method
As illustrated in Figure 2, we proposes an MLLM architecture that leverages multi-granularity
visual features for enhanced multi-image understanding. Visual images are encoded as both discrete
symbolsequencesandcontinuoushigh-dimensionalvectorsequences. Thediscretevisualsymbol
sequencescaptureessentialcoarse-grainedvisualconceptsfromtheimages,whilethecontinuous
vector sequences encapsulate fine-grained details. Furthermore, to minimize redundant and irrelevant
visual representations in the continuous visual sequences and thereby reduce the input context length
in multi-image scenarios, we also introduces a dynamic reduction strategy for visual features, guided
by textual semantics.
3.1 Multi-Granularity Hybrid Encoding
As shown in Figure 2 (a), assume the input to the MLLM is {S, T}, where S={I1, I2, . . . , I K}
representsacollectionof Kimages,and Tdenotesthecorrespondingtextualcontent. Foreachimage
Ik,k∈ {1,2, . . . , K }, we employ both the discrete visual encoder SEED [ 11] and the continuous
visual encoder ViT [7] for encoding.
Visual Continuous Encoding : we utilize the Vision Transformer (ViT) model, which is widely
adopted by most modern Multimodal Large Language Models (MLLMs). For an RGB image Ikwith
dimensions W×H×3, the image is partitioned into patches of size p×p, resulting inW
p×H
p
patches. ThesepatchesarethenencodedbytheViTvisualencoderintoacontinuousvisualsequence:
Vk
c= [⃗ vk
1,⃗ vk
2, . . . ,⃗ vk
nc]. Here, nc=W
p×H
p, and ⃗ vk
i∈Rzrepresents a continuous vector of z
dimensions. Subsequently, we utilize the text-semantics-aware patch reduction module (details of
whichwillbeelaboratedinSubsection3.2)toselectpatchfeaturesrelevanttotheinputtextualcontent
T,therebyreducingthesequencelengthof Vk
c,whilepreservingessentialfine-grainedinformation.
The reduced feature sequence is denoted as Vk
c= [⃗ vk
p1,⃗ vk
p2, . . . ,⃗ vk
pmc], mc≪nc. Finally, we utilize
325
Visual Continuous Encoding48December. Montreal, Canada, a red house in the snow...Large Language Model
Unified V ocabulary 
Vision TransformerDynamic Patch Selection Linear Projector
Image Continuous SequenceImage Discrete SequencesText Discrete SequencesImage  Discrete TokenizerText Tokenizer5617N+2N+5N+4N+8
Visual Discrete EncodingImage1
Aligned to Unified V ocabularyEmbeddling Layer
v!",v"",…,v#!"v!!,v"!,…,v#!!̇𝑣!!,̇𝑣"!,…,̇𝑣#"!;̇𝑣!",̇𝑣"",…,̇𝑣#"";𝑡!,𝑡",…,𝑡##v$$",v$%",…,v$&!"v$$!,v$%!,…,v$&!!Image 2Text
Key Visual Tokens
RedundantVisual Tokens0.20.10.30.80.90.70.40.60.8Pacth Selector Continuous Visual Token SequenceImportance Prediction of EachPatchElement
EOS
EOSLLM
Concat
Discrete Visual Token SequenceViTA herd of horses is running
Grounded SAM Pseudo Patch-Level Annotation
Training
Image Patch Sequence(b) Continuous Visual Feature Reduction Mechanism(a) Multi-Granularity Hybrid Encoding Figure 2: Subfigure (a) illustrates the structural schematic of our proposed Multi-Granularity Hybrid
Encoding, while subfigure (b) demonstrates the mechanism for the reduction of continuous visual
tokens under the guidance of discrete visual information.
an Multi-Layer Perceptron, akin to that used in LLaVA 1.5, as a bridging projector to project Vcinto
the semantic space of the LLM embedding layer.
Visual Discrete Encoding : image Ikis tokenized by the image discrete tokenizer Dv(for details on
thedesign andtraining ofthediscrete tokenizer,please referto theoriginalwork[ 11])into avisual
discretesymbolsequence Vd= [dk
1, dk
2, . . . , dk
nd],where dk
i∈[1,2, . . . ,Nv]. Here, Nvdenotesthe
size of the visual discrete encoding vocabulary.
Unified Multimodal Vocabulary: Given that text modalities naturally possess a discrete vocabulary,
mergingthevisualdiscretevocabularywiththetextualdiscretevocabularyformsaunifiedmultimodal
vocabulary. Theadvantageofthisapproachliesinitsabilitytoachieveaunifiedrepresentationof
bothvisualandtextualmodalities,effectivelyaddressingthesemanticgapbetweenthem. Assume
that the vocabulary size of the LLM is N, and the vocabulary size of the visual discrete tokenizer is
Nv. The expanded multi-modal unified vocabulary size thus becomes Nu=N+Nv. Concurrently,
we align each element in Vdwith the index of the unified vocabulary to obtain the final discrete
encoding: Vd= [ˆvk
1,ˆvk
2, . . . , ˆvk
nd], where ˆvk
i=dk
i+N. Finally, the weight matrix of the LLM’s
embeddinglayer, W,isalsoexpandedfrom N×ztoNu×z. Consequently,theweightmatrixof
the LLM’s embedding layer, W, is expanded from N×ztoNu×z. This adjustment enables the
embeddinglayeroftheLLMtoconcurrentlyencodefeaturesfrombothvisualandtextualdiscrete
tokens. The embedded representation of Vdis denoted as [ ˙vk
1,˙vk
2, . . . , ˙vk
nd]which is output by the
expandedembeddinglayer. Finally,wesequentiallyinsertthecontinuousvisualtokensbeforethe
discretevisualtokenembeddingsoutputtedbytheembeddinglayer. Thefinalvisualrepresentation
inputted into the LLM is: [⃗ vk
p1,⃗ vk
p2, . . . ,⃗ vk
pmc,˙vk
1,˙vk
2, . . . , ˙vk
nd].
3.2 Continuous Visual Tokens Reduction Mechanism
Weaimforthediscretevisualtokenstoabstracthigh-level,coarse-grainedsemanticsfromtheimages,
whilethecontinuousvisualtokenscomplementthiswithlow-level,fine-graineddetails. However,
we found that the continuous visual tokens output by the Vision Transformer (ViT) encompass a
considerable amount of redundancy, with many tokens possessing repetitive or superfluous semantics.
Consequently, as shown in Figure 2 (b), we propose a continuous visual token reduction mechanism
guidedbythecoarse-grainedsemanticsofdiscretevisualtokens,aimedatachievingsemanticsynergy
between coarse-grained and fine-grained representations.
Firstly, after obtaining the sequence of discrete visual tokens Vd= [ˆvk
1,ˆvk
2, . . . , ˆvk
nd], we append
an <EOS> token to it. This sequence Vd= [ˆvk
1,ˆvk
2, . . . , ˆvk
nd, teos]is then passed through the
LLM to obtain the final layer’s output hidden state of the <EOS> token denoted as heos, which
representstheglobalinformationofthediscretevisualtokens: heos=LLM([ˆvk
1,ˆvk
2, . . . , ˆvk
nd, teos]).
wethenconcatenatetheEOStokenwitheachimagepatchtokenas ˙⃗ vk
i=concat (⃗ vk
i, heos),where
⃗ vk
i∈Rz, heos∈Rz,˙⃗ vk
i∈R2z, i∈ {1,2, . . . , n c}. Then the concatenated patch features ˙⃗ vk
iare
4EOS5693EOSGenerated Image discrete Tokens Cross-modal autoregressive generationText discrete TokensN+9N+3N+9N+3Image discrete Tokens Generated Text discrete Tokens
❄Large Language Model
❄LLM Embedding Layer
🔥Patch Selector
❄VisualProjector
❄Large Language Model
🔥LLM Embedding Layer
❄Patch Selector
❄VisualProjector
GroundingSAM
BinaryCrossEntropy
❄Large Language Model
❄LLM Embedding Layer
❄Patch Selector
🔥VisualProjector
🔥Large Language Model
🔥LLM Embedding Layer
❄Patch Selector
🔥VisualProjectorStage1Stage2Stage3Stage4
5693
Patch Relevant Prediction5693EOSText autoregressive generationImage ContinuousTokens Generated Text discrete Tokens5693EOSText autoregressive generationImage ContinuousTokens Image discrete Tokens Generated Text discrete TokensFigure 3: The diagram illustrates the training schematic for MaVEn. We divide the training of
MaVEn into four stages, where the snowflake icon indicates that the model parameters are frozen
during training, and the flame icon indicates that the model parameters are updated during training.
fed to the patch selector. The patch selector is an Multilayer Perceptron (MLP) denoted as Fthat
contains three linear layers and is used to predict the relevant score between patches and the discrete
visual tokens. The output of the last linear layer has only one dimension and will be fed to a Sigmoid
activation function to predict therelevant score aiwith the discretevisual tokensas ai=F(⃗ vk
i, heos)
Accordingtothepredictionofpatchselector F,thetop-mkeyimagepatchtokensarekeptandthe
unselected patch tokens which generally have lower relevant scores will be discarded. Finally, we
reconstruct the reduced visual sequence as vk=
⃗ vk
p1,···,⃗ vk
pm
, where m=nc×α, and αis a
hyper-parameterandnamedKeepingRatiowhichisusedtocontroltheproportionofselectedpatches
to total patches.
ConstructionofPatch-levelpseudo-labelannotation: To trainthepatchselector, weconstructed
patch-levelpseudo-labelannotationsbasedonGroundingSAM[ 32](arecentstate-of-the-artopen-
textvocabularysemanticsegmentationmodel). Weobservedthatthehigh-dimensionalsemantics
encapsulated within the discrete visual tokens are largely consistent with the semantics of the image
captions. Inspired by this observation, as shown in Figure 2 (b), we opted to use image captions as a
proxy for the high-dimensional semantic abstraction of the image. We employed Grounding SAM to
performtext-guidedsemanticsegmentationoftheimages. Afterobtainingthesemanticsegmentation
pixel masks, we computed the overlap between each patch and the mask labels. If there is an overlap,
the corresponding patch label is set to 1; otherwise, the label is set to 0.
3.3 Training Paradigm of MaVEn
ThetrainingprocessofMaVEnisdividedintofourstages. Inthefirststage,weutilizedimage-text
datasets like COCO [ 21] and Visual Genome (VG) [ 14] to annotate 1 million semantic segmentation
masks with textual annotations, based on Grounding SAM [ 32]. These masks were subsequently
convertedintopatch-levelpseudo-labels. Utilizingthisdataset,wetrainedthePatchSelectorwhile
keeping other model parameters frozen.
In the second stage, we exclusively trained the embedding layer of the LLM (Large Language Model)
to adapt to our expanded vocabulary for the LLM. Consequently, we utilized the LLaVA 558k
single-image pretraining dataset [ 24] and the MMC4 interleaved image-text dataset [ 45] for training.
At this stage, we employed only the visual discrete encoding, eschewing the visual continuous
encoding,withtheaimofadaptingtheLLMembeddinglayertotheexpandedunifiedvocabulary. We
trained using a cross-modal autoregressive generation task; given an input that might contain images,
we obtained tokenized discrete sequences through the text and image tokenizers. This enabled us to
generate discrete image token sequences from text discrete token sequences and vice versa.
Inthethirdstage,ourobjectiveistooptimizethevisualprojectorsothatthecontinuousvisualtokens,
after being processed by the visual projector, align with the semantic space distribution of the unified
multimodalvocabularyembeddings. Therefore, duringthis phase, wetrain solelythevisualprojector.
5MethodMulti Modal
DialogueVisual Story
Telling ListVisual Relation
InferenceMulti Modal
ClozeKnowledge
Grounded QAText Rich
Images QAMulti Image
Reasoning
BLIP-2 [19] 11.96 20.10 3.67 18.25 39.73 30.53 39.53
mPLUG-Owl [36] 12.67 19.33 5.40 16.25 33.27 32.47 42.50
InstructBLIP [6] 33.58 24.41 11.49 21.20 47.40 44.40 48.55
LLaMA-Adapter-v2 [10] 14.22 17.57 13.51 18.00 44.80 32.00 44.03
LLaVA [25] 7.79 10.70 8.27 15.85 36.20 28.33 41.53
MiniGPT-4 [44] 13.70 17.07 7.95 16.60 30.27 26.40 43.50
LLaVA-1.5 [23] 27.17 14.32 11.62 31.65 46.4 38.87 44.58
Otter [15] 15.37 15.57 11.39 16.00 41.67 27.73 43.85
OpenFliamingo [2] 16.88 24.22 13.85 21.65 32.00 30.60 41.63
VPG-C [17] 37.50 25.20 25.90 22.15 48.60 44.93 50.28
MaVEn 34.63 21.53 30.24 33.35 51.53 47.33 54.38
Table 1: Average results of zero-shot evaluation on each task of DEMON Benchmark [17].
Method Vision Encoder Language Model Avg. All Avg. Img Avg. Video
BLIP-2 [19] ViT-g (1.3B) Vicuna (7B) 46.4 49.7 36.7
mPLUG-Owl [36] ViT-L (0.3B) LLaMA (7B) 34 37.9 23
InstructBLIP [6] ViT-g (1.3B) Vicuna (7B) 53.4 58.8 38.1
LLaMA-Adapter-v2 [10] ViT-L (0.3B) LLaMA (7B) 32.7 35.2 25.8
Otter [15] ViT-L (0.3B) LLaMA (7B) 33.9 35.2 30.4
LLaVA [25] ViT-L (0.3B) Vicuna (7B) 33.5 37.0 23.8
MiniGPT-4 [44] ViT-g (1.3B) Vicuna (7B) 42.8 47.4 29.9
LLaVA-1.5 [23] ViT-L Vicuna (7B) 58.6 66.1 37.3
MaVEn ViT-L + SEED (1.3B) Vicuna (7B) 60.89 65.85 42.11
Table 2: Average results of zero-shot evaluation on each task category of SEED Benchmark [17].
We train using the LLaVA 558K image-text caption dataset, where images are encoded solely as
sequences of continuous visual tokens: Vk
c= [⃗ vk
p1,⃗ vk
p2, . . . ,⃗ vk
pmc]without employing visual discrete
coding. The model is required to generate captions for the images based on the visual input.
In the fourth stage, we introduce instruction fine-tuning data with the aim of enhancing the MLLM’s
capability to follow human instructions. During this phase, the MLLM undergoes comprehensive
fine-tuningwiththeLLaVA665kinstructionfine-tuningdatasets,unfreezingallmodelparameters
except for those of the visual encoder and patch selector for training and optimization.
4 Experiments
4.1 Experiment Setting
Dataset: We initially generated 1 million pseudo-labels for patch-level text semantic relevance
by utilizing the COCO[ 21], Visual Genome (VG)[ 14], and RefCOCO datasets [ 38], following the
methodologydelineatedinSubsection3.2andleveragingGroundSAM.Thesepseudo-labelswere
subsequently employed to train the patch selector within MaVEn. During the second phase of
modeltraining,theembeddinglayerofMaVEnwasrefinedusingtheMMC4-coredataset[ 45]and
the LLaVA 558K single-image pre-training dataset [ 22]. In the third phase, the visual projector
componentofMaVEnwasfurthertrainedusingtheLLaVA558Ksingle-imagedataset. Finally,in
the final phase, we fine-tuned the model with the LLaVA 665K instruction fine-tuning dataset.
Training Settings: MaVEn utilizes the ViT-L model [ 31] with a patch size of 14×14and is
pre-trainedataresolutionof 336×336,resultinginacontinuoustokenlengthof567fortheencoded
image. Forimagediscretetokenization,SEED[ 11]isemployedtotokenizetheimageinto32discrete
tokens. Forthecontinuousvisualtokens,duringpatchreduction,wesettheKeepingRatioto 0.25,
meaningthatonly25%ofthecontinuoustokensareretained. Consequently,thelengthofthefinal
continuousvisualtokensequencedecreasesfrom576to144,whilethelengthofthediscretetoken
sequence is 32. Ultimately, the entire visual hybrid encoding sequence has a length of 176. The
large language model Vicuna [ 42], with 7 billion parameters, is used to handle multi-modal features.
TheAdamWoptimizer[ 28]is usedfor optimization. During theinstructiontuning stage,the entire
model is trained for 1 epoch with a learning rate of 2e-5 and a batch size of 256. All experiments was
performed using 8 NVIDIA A100 GPUs, each with 80GB of memory.
6General VQA General VQA (Zero-shot) Zero-shot Multi-modal Benchmarks
Method #Params VQAv2 GQA VizWizQATextVQASciQAMME MMBench MM-Vet
BLIP-2 [19] 8.2B 65.0 41.0 19.6 42.5 61.0 1293.84 - 22.4
InstructBLIP [6] 8.2B - 49.2 34.5 50.1†60.5 1212.82 36.0 26.2
Unified-IO XL[29] 2.9B 77.9 - 57.4‡- - - - -
PaLM-E-12B [8] 12B 76.2 - - - - - - -
Shikra [5] 7.2B 77.4 - - - - - 58.8 -
Qwen-VL-Chat [3] 9.6B 78.2 57.5 38.9 61.5‡68.21487.58 60.6 -
LLaVA [23] 7.2B 71.3 41.3 36.7 50.2†61.5 502.82 36.2 28.1
MiniGPT-4 [23] 7.2B 65.2 30.8 30.2 52.3†58.4 581.67 23.0 22.1
LLaVA1.5 [23] 7.2B 78.5 62.0 50.0 58.2†66.8 1510.70 64.3 30.5
MaVEn 7.2B79.162.550.5 59.8†67.31530.10 65.2 30.4
Table 3: Performance comparison on visual question answering and zero-shot multi-modal
benchmarks. For VQA, accuracy is reported. Note that specialists are fine-tuned on each individual
dataset. †denotes OCR inputs are utilized. ‡indicates the model has trained on the dataset.
4.2 Main Results
TovalidatetheeffectivenessofMaVEninmulti-imagescenarios,weevaluatedMaVEn’sperformance
multi-imagevisualunderstandingandreasoning. Withinthemulti-imagevisualunderstandingcontext,
weassessedthemodelusingDemonBench[ 17]andSEED-Bench[ 16]. DemonBenchcomprisesseven
scenarios involvingmulti-image reasoning andunderstanding, includingtasks such asMulti-Modal
Dialogue, Visual Relation Inference, Knowledge Grounded QA, and Multi-Image Reasoning. SEED-
Bench, on the other hand, encompasses questions related to video comprehension. Additionally, we
also tested the performance of MaVEn in single-image scenarios.
4.2.1 Effectiveness of MaVEn on Multi-image Visual Comprehension
Results on DemonBench : As shown in the Table 1, we evaluated our model on DemonBench,
comparingitwithseveralmulti-imagedata-trainedMLLMmodelssuchasOpenflamingo,Otter,VPG-
C,aswellassingle-imagescenarioMLLMmodels. Ourmodelattainedthehighestscoresintasks
such as Visual Relation Inference, Multi-Modal Cloze, Text-Rich Images QA, Knowledge-Grounded
QA, and Multi-Image Reasoning, underscoring MaVEn ’s significant superiority in multi-image
understandingandreasoningtasks. ItalsoachievedcomparableperformanceinVisualStorytelling
and Multi-Modal Dialogue.
ResultsonSEED-Bench : Furthermore,weassessedourmodelonSEED-Bench[ 16],particularly
focusing on video understanding tasks like action prediction, action Recognition and procedure
understanding. As shown in Table 2 The experiments revealed that our approach significantly
outperformed existing models like LLaVA 1.5, Otter and so on. For instance, MaVEn exhibited
a 12-point improvement over Otter [ 15] in video understanding (30.4 -> 42.11). These findings
underscore MaVEn ’s effectiveness in multi-image understanding scenarios.
4.2.2 Effectiveness of MaVEn on Single-image Visual Comprehension
We intend to explore the influence of MaVEn on the model’s abilities of single image visual
comprehensionandgeneration. Toachievethisobjective,wecarriedoutassessmentsoncommon
benchmarks, such as Visual Question Answering (VQA) [ 12,30,33] and recently designed MLLM-
focused Multi-modal Benchmarks including MME [9], MMBench [26], MM-Vet [39].
ResultsonBenchmarkTasks AssummarizedinTable3. WecomparedperformanceofMaVEn
to other SOTA MLLMs such as BLIP2[ 19], InstructBLIP [ 6], Shikra [ 5], and Qwen-VL-Chat [ 3].
Our experimental results show that our approach can also successfully enhances the performance
across a range of single image understanding task. Notably, MaVEn outperforms LLaVA-1.5 [ 25] in
terms of consistency and accuracy across all VQA datasets.
MLLM-oriented Multi-modal Benchmarks. We also evaluate MaVEn on fourrecentlypopular
single-image multi-modal benchmarks in a zero-shot manner. The results of our evaluation are listed
in Table 3. We discovered that after implementing MaVEn, all three models exhibited improvements
7Discrete Continuous SEED-Bench DEMONBench VQA MMBench
! # 43.2 24.19 56.07 34.5
# ! 58.6 30.66 78.5 64.3
! ! 60.89 39.51 79.1 65.21
Table 4: Ablation evaluation on multi-modal benchmarks We evaluated the performance of
variousablationtargetsonbothmulti-image(SEED-Bench,DEMONBench)andsingle-image(VQA,
MMBench) benchmarks. MMBench [26].
0.2 0.4 0.6 0.8 1.0
Keeping ratio100200300400500600Value
891181762943784926080.25 Length
0.2 0.4 0.6 0.8 1.0
Keeping ratio5455565758596061Value
54.3258.8960.8961.0461.23 61.3 61.340.25 SEED Bench
0.2 0.4 0.6 0.8 1.0
Keeping ratio323334353637383940Value
32.337.2339.039.5339.8340.0240.140.25 Demo BenchResults of Different Benchmarks with Varying Keeping Ratios
Figure 4: Evaluation Results of MaVEn on different benchmarks with varying Keeping Ratios.
across multiple benchmarks. Notably, for LLaVA and MiniGPT-4, the enhancement was particularly
evident on the MME [ 9] benchmark. For instance, after implementing MaVEn, LLaVA’s MME score
improvedfrom581.67to653.94. TheseresultshighlightMaVEnroleinadvancingthestate-of-the-art
in both single-image and multi-image visual comprehension tasks.
4.3 Ablation Study
4.3.1 Effectiveness of Multi-Granularity Hybrid Encoding
To ascertain the efficacy of multi-granularity hybrid encoding, we conducted training using solely
visual discrete encoding and visual continuous encoding, respectively. We then evaluated and
comparedtheoutcomesonbothmulti-imageandsingle-imageevaluationbenchmarks. Theresults
are detailed in the Table 4 below. We observed that, compared with utilizing only visual continuous
encodingoremployingahybridofvisualdiscreteandcontinuousencoding,themodelsolelyonvisual
discrete encoding exhibits subpar performance in both multi-image and single-image contexts. This
underperformanceislikelyduetothenatureofdiscretevisualfeatureencoding,which,whilecapturing
the high-dimensional information of the image, forfeits a significant amount of low-dimensional,
fine-grained details, resulting in a lossy encoding process. As a result, it fares poorly in tasks like
image reasoning and understanding, which demand meticulously detailed information. Moreover,
the model with only continuous encoding also do not deliver optimal performance, particularly in
multi-image tasks. This further indicates that models based solely on visual continuous encoding are
unsuitable for multi-image scenarios.
4.3.2 Efficient of Continuous Visual Token Reduction Mechanism
Toverifytheeffectivenessofpatchreduction,wecomparedthelengthofvisualtokensatdifferent
Keeping Ratios and analyzed the performance across various benchmarks. We experimented with the
KeepingRatiosfrom0.1to1.0. TheexperimentalresultsareshownintheFigure4. Weobservedthat
when the Keeping Ratio was 0.1, the number of visual tokens decreased to 89, a significant reduction
from the initial number of patches. However, the model’s performance across multiple benchmarks
also significantly declined. Therefore, despite the reduction in the number of visual tokens, the
performance loss was too substantial, making it an unsuitable final choice. When the Keeping Ratio
was0.25,thenumberofvisualtokensremainedrelativelylow,butthemodel’sperformancewasmore
stable. Inthis case, themodelexhibitedbalancedperformanceacross various benchmarks, effectively
reducing the number of visual tokens while maintaining a high-performance level. Therefore, we
ultimately chose a Keeping Ratio of 0.25.
8Original ImageSelectedPatchesDiscrete Index DistribuJon
Relevant scoreFigure 5: This figure visualizes the distribution of discrete tokens in an image containing index 4568
discretetokens,along withtherelevantscorecomputedbased onthePatchSelectorandthe patches
chosen according to the relevant score that are most semantically related to the discrete visual tokens.
Image1Image2Text
Image1
Image2ContinuoustokensDiscrete tokenContinuoustokensDiscrete tokensImage1Image2TextAverage Attention Weights with Multi-granularity Hybrid Visual Encoding 
Average Attention Weights with Only Continuous Visual Tokens
Figure 6: Visualization of the attention maps with and without the visual discrete tokens . We
demonstrate the attentionmaps for the 31-stlayers, wherethe range of visualtokens is indicated by
orange and the range of text tokens is indicated by blue.
4.4 Qualitative Analysis
SemanticGranularityofdiscreteandcontinuousvisualtokens : Toinvestigatethesemanticsof
discrete tokens,we randomlyselected anindexfrom thevisual discretedictionary andsearched the
CC/SUB dataset for images containing this index in their encoding. As illustrated in Figure 5, we
randomly chose three images with index = 4568. We discovered that all three images featured the
depictionofasnowman,suggestingthatindex=4568canrepresenthigh-levelsemanticssuchasa
snowmanorwhitesnow. Weprovidedthedistributionofdiscretetokensforthesethreeimagesand
observed that the position of index = 4568 in the discrete sequences was also notably consistent.
Semanticsynergybetweenvisualdiscreteandcontinuousrepresentations : Furthermore,asshown
inFigure5, wealsovisualizetherelevantscorebetweenthepatchesandthesemanticsofdiscrete
visualtokenspredictedbythepatchselector,alongwiththepatchtokensselectedbasedonaKeeping
ratio of 0.25. We found that the patch selector tends to choose patches related to the semantics of
discretevisualtokens,therebysupplementingthemissinglow-levelfine-grainedinformationofthe
discrete tokens. The aforementioned findings further validate that multi-granularity hybrid encoding
facilitates mutual assistance and synergybetween discrete and continuousrepresentations, thereby
achieving efficient multi-granularity semantic encoding.
9The impact of discrete visual tokens on multi-image reasoning : To further validate the role of
discrete visual tokens in the inference process of multi-image instructions, we visualize the attention
weights of the last layer of the LLM. As illustrated in Figure 6, we have MaVEn compare the
commonalities between two images. For inputs using only continuous visual tokens, we observe that
the model’s attention during inference is primarily focused on text tokens, disregarding visual tokens.
Thismaystillbeduetothelowersemanticgranularityofcontinuousvisualtokens,makingitdifficult
toestablishdirectsemanticassociations. However,withinputsencodedusingmulti-granularityvisual
hybrid encoding, we notice that the model establishes attention associations with discrete visual
tokenswhenansweringquestions. Thisindicatesthat, inmulti-imagescenarios,discretevisualtokens
guide the LLM to focus on visual information during decoding.
5 Conclusion
In conclusion, we introduces MaVEn, a novel Multi-granularity Hybrid Visual Encoding framework
designed to enhance multi-image reasoning in MLLMs. By combining discrete visual symbols
for semantic abstraction with continuous sequences for detailed features, MaVEn improves both
understanding and processing efficiency. Our dynamic reduction mechanism and multi-stage training
strategy further enhance performance. Experimental results confirm that MaVEn significantly boosts
MLLM capabilities in both multi-image and single-image contexts.
6 Acknowledgement
This work is supported by the National Natural Science Foundation of China (623B2007) and
CCF-Zhipu Large Model Innovation Fund(NO.CCF-Zhipu202415).
References
[1]Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, et al. Flamingo: a visual language model for
few-shot learning. In NeurIPS, volume 35, 2022.
[2]AnasAwadalla,IrenaGao,JoshGardner,JackHessel,YusufHanafy,WanrongZhu,Kalyani
Marathe, Yonatan Bitton, Samir Gadre, Shiori Sagawa, et al. Openflamingo: An open-
source framework for training large autoregressive vision-language models. arXivpreprint
arXiv:2308.01390, 2023.
[3]Jinze Bai, Shuai Bai, Shusheng Yang, Shĳie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang
Zhou, and Jingren Zhou. Qwen-vl: A frontier large vision-language model with versatile
abilities. ArXiv, abs/2308.12966, 2023.
[4]Hangbo Bao, Li Dong, Songhao Piao, and Furu Wei. Beit: Bert pre-training of image
transformers. arXivpreprintarXiv:2106.08254, 2021.
[5]KeChen,ZhaoZhang,WeiliZeng,RichongZhang,FengZhu,andRuiZhao. Shikra: Unleashing
multimodal llm’s referential dialogue magic. ArXiv, abs/2306.15195, 2023.
[6]Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang,
BoyangAlbert Li,PascaleFung, andStevenC. H.Hoi. Instructblip: Towardsgeneral-purpose
vision-language models with instruction tuning. ArXiv, abs/2305.06500, 2023.
[7]Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, et al. An image is worth 16x16 words:
Transformers for image recognition at scale. In ICLR, 2021.
[8]Danny Driess, F. Xia, Mehdi S. M. Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter,
Ayzaan Wahid, Jonathan Tompson, Quan Ho Vuong, Tianhe Yu, Wenlong Huang, Yevgen
Chebotar, Pierre Sermanet, Daniel Duckworth, Sergey Levine, Vincent Vanhoucke, Karol
Hausman, Marc Toussaint, Klaus Greff, Andy Zeng, Igor Mordatch, and Peter R. Florence.
Palm-e: An embodied multimodal language model. In International Conference onMachine
Learning, 2023.
[9]ChaoyouFu,PeixianChen,YunhangShen,YuleiQin,MengdanZhang,XuLin,ZhenyuQiu,
WeiLin,JinruiYang,XiawuZheng,etal. Mme: Acomprehensiveevaluationbenchmarkfor
multimodal large language models. arXivpreprintarXiv:2306.13394, 2023.
10[10]Peng Gao, Jiaming Han, Renrui Zhang, Ziyi Lin, Shĳie Geng, Aojun Zhou, W. Zhang,
Pan Lu, Conghui He, Xiangyu Yue, Hongsheng Li, and Yu Jiao Qiao. Llama-adapter v2:
Parameter-efficient visual instruction model. ArXiv, abs/2304.15010, 2023.
[11]Yuying Ge, Yixiao Ge, Ziyun Zeng, Xintao Wang, and Ying Shan. Planting a seed of vision in
large language model. arXivpreprintarXiv:2307.08041, 2023.
[12]Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the V
inVQAmatter: ElevatingtheroleofimageunderstandinginVisualQuestionAnswering. In
Conference onComputer VisionandPatternRecognition (CVPR), 2017.
[13]Shuyang Gu, Dong Chen, Jianmin Bao, Fang Wen, Bo Zhang, Dongdong Chen, Lu Yuan, and
BainingGuo. Vectorquantizeddiffusionmodelfor text-to-imagesynthesis. In Proceedings of
theIEEE/CVF Conference onComputer VisionandPatternRecognition , pages 10696–10706,
2022.
[14]Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie
Chen, Yannis Kalantidis, Li-Jia Li, David A Shamma, et al. Visual genome: Connecting
language and vision using crowdsourced dense image annotations. International journalof
computer vision, 123:32–73, 2017.
[15]Bo Li, Yuanhan Zhang, Liangyu Chen, et al. Otter: A multi-modal model with in-context
instruction tuning. arXivpreprintarXiv:2305.03726, 2023.
[16]Bohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yixiao Ge, and Ying Shan. Seed-
bench: Benchmarking multimodal llms with generative comprehension. arXivpreprint
arXiv:2307.16125, 2023.
[17]Juncheng Li, Kaihang Pan, Zhiqi Ge, Minghe Gao, Wei Ji, Wenqiao Zhang, Tat-Seng Chua,
Siliang Tang, Hanwang Zhang, and Yueting Zhuang. Fine-tuning multimodal llms to follow
zero-shot demonstrative instructions. In TheTwelfthInternational Conference onLearning
Representations, 2023.
[18]JunnanLi,DongxuLi,SilvioSavarese,etal. Blip-2: Bootstrappinglanguage-imagepre-training
with frozen image encoders and large language models. In ICML, 2023.
[19]Junnan Li, Dongxu Li, Silvio Savarese, and Steven C. H. Hoi. Blip-2: Bootstrapping
language-image pre-training with frozen image encoders and large language models. ArXiv,
abs/2301.12597, 2023.
[20]Victor Weixin Liang, Yuhui Zhang, Yongchan Kwon, Serena Yeung, and JamesY Zou. Mind
the gap: Understanding the modality gap in multi-modal contrastive representation learning.
Advances inNeuralInformation Processing Systems, 35:17612–17625, 2022.
[21]Tsung-YiLin, MichaelMaire, SergeBelongie, JamesHays, PietroPerona, DevaRamanan, Piotr
Dollár, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer
Vision–ECCV 2014:13thEuropean Conference, Zurich,Switzerland, September 6-12,2014,
Proceedings, PartV13, pages 740–755. Springer, 2014.
[22]Haotian Liu, Chunyuan Li, Yuheng Li, et al. Improved baselines with visual instruction tuning.
arXivpreprintarXiv:2310.03744, 2023.
[23]Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual
instruction tuning. ArXiv, abs/2310.03744, 2023.
[24] Haotian Liu, Chunyuan Li, Qingyang Wu, et al. Visual instruction tuning. In NeurIPS, 2023.
[25]Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. ArXiv,
abs/2304.08485, 2023.
[26]Yuan Liu, Haodong Duan, Yuanhan Zhang, et al. Mmbench: Is your multi-modal model an
all-around player? arXivpreprintarXiv:2307.06281, 2023.
11[27]ZeLiu,YutongLin,YueCao,HanHu,YixuanWei,ZhengZhang,StephenLin,andBaining
Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings
oftheIEEE/CVF international conference oncomputer vision, pages 10012–10022, 2021.
[28] Ilya Loshchilov and Frank Hutter. Fixing weight decay regularization in adam. 2018.
[29]Jiasen Lu, Christopher Clark, Rowan Zellers, Roozbeh Mottaghi, and Aniruddha Kembhavi.
Unified-io: Aunifiedmodelforvision,language,andmulti-modaltasks. ArXiv,abs/2206.08916,
2022.
[30]Anand Mishra, Shashank Shekhar, Ajeet Kumar Singh, and Anirban Chakraborty. Ocr-vqa:
Visual question answering by reading text in images. In 2019international conference on
document analysisandrecognition (ICDAR), pages 947–952. IEEE, 2019.
[31]Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,
Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual
models from natural language supervision. In International conference onmachinelearning,
pages 8748–8763. PMLR, 2021.
[32]Tianhe Ren, Shilong Liu, Ailing Zeng, Jing Lin, Kunchang Li, He Cao, Jiayu Chen, Xinyu
Huang, Yukang Chen, Feng Yan, et al. Grounded sam: Assembling open-world models for
diverse visual tasks. arXivpreprintarXiv:2401.14159, 2024.
[33]Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv Batra, Devi
Parikh, and Marcus Rohrbach. Towards vqa models that can read. In Proceedings ofthe
IEEE/CVF conference oncomputer visionandpatternrecognition, pages 8317–8326, 2019.
[34] Quan Sun, Yuxin Fang, Ledell Wu, Xinlong Wang, and Yue Cao. Eva-clip: Improved training
techniques for clip at scale. arXivpreprintarXiv:2303.15389, 2023.
[35]AaronVanDenOord,OriolVinyals,etal. Neuraldiscreterepresentationlearning. Advances in
neuralinformation processing systems, 30, 2017.
[36]QinghaoYe,HaiyangXu,GuohaiXu,JiaboYe,MingYan,YiyangZhou,JunyangWang,Anwen
Hu, Pengcheng Shi, Yaya Shi, et al. mplug-owl: Modularization empowers large language
models with multimodality. arXivpreprintarXiv:2304.14178, 2023.
[37]ShukangYin,ChaoyouFu,SiruiZhao,etal. Asurveyonmultimodallargelanguagemodels.
arXivpreprintarXiv:2306.13549, 2023.
[38]Licheng Yu, Patrick Poirson, Shan Yang, Alexander C Berg, and Tamara L Berg. Modeling
contextinreferringexpressions. In Computer Vision–ECCV 2016:14thEuropean Conference,
Amsterdam, TheNetherlands, October11-14,2016,Proceedings, PartII14, pages 69–85.
Springer, 2016.
[39]WeihaoYu,ZhengyuanYang,LinjieLi,JianfengWang,KevinLin,ZichengLiu,XinchaoWang,
and Lĳuan Wang. Mm-vet: Evaluating large multimodal models for integrated capabilities.
arXivpreprintarXiv:2308.02490, 2023.
[40]Zheng Zhang, Yong Xu, Jian Yang, Xuelong Li, and David Dian Zhang. A survey of sparse
representation: Algorithms and applications. IEEEAccess, 3:490–530, 2015.
[41]HaozheZhao,ZefanCai,ShuzhengSi,XiaojianMa,KaikaiAn,LiangChen,ZixuanLiu,Sheng
Wang, Wenjuan Han, and Baobao Chang. Mmicl: Empowering vision-language model with
multi-modal in-context learning. arXivpreprintarXiv:2309.07915, 2023.
[42]LianminZheng,Wei-LinChiang,YingSheng,SiyuanZhuang,ZhanghaoWu,YonghaoZhuang,
Zi Lin, Zhuohan Li, Dacheng Li, Eric. P Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica.
Judging llm-as-a-judge with mt-bench and chatbot arena, 2023.
[43]DeyaoZhu,JunChen,XiaoqianShen,etal.Minigpt-4: Enhancingvision-languageunderstanding
with advanced large language models. arXivpreprintarXiv:2304.10592, 2023.
12[44]DeyaoZhu,JunChen,XiaoqianShen,XiangLi,andMohamedElhoseiny.Minigpt-4: Enhancing
vision-language understanding with advanced large language models. ArXiv, abs/2304.10592,
2023.
[45]Wanrong Zhu, Jack Hessel, Anas Awadalla, Samir Yitzhak Gadre, Jesse Dodge, Alex Fang,
Youngjae Yu, Ludwig Schmidt, William Yang Wang, and Yejin Choi. Multimodal c4: An open,
billion-scalecorpusofimagesinterleavedwithtext. Advances inNeuralInformation Processing
Systems, 36, 2024.
13NeurIPS Paper Checklist
1.Claims
Question: Dothemainclaimsmadeintheabstractandintroductionaccuratelyreflectthe
paper’s contributions and scope?
Answer: [Yes]
Justification: NA
Guidelines:
•TheanswerNAmeansthattheabstractandintroductiondonotincludetheclaimsmade
in the paper.
•The abstract and/or introduction should clearly state the claims made, including the
contributionsmadeinthepaperandimportantassumptionsandlimitations. ANoor
NA answer to this question will not be perceived well by the reviewers.
•The claims made should match theoretical and experimental results, and reflect how
much the results can be expected to generalize to other settings.
•Itisfinetoincludeaspirationalgoalsasmotivationaslongasitisclearthatthesegoals
are not attained by the paper.
2.Limitations
Question: Does the paper discuss the limitations of the work performed by the authors?
Answer: [Yes]
Justification: NA
Guidelines:
•The answer NA means that the paper has no limitation while the answer No means that
the paper has limitations, but those are not discussed in the paper.
•The authors are encouraged to create a separate "Limitations" section in their paper.
•Thepapershouldpointoutanystrongassumptionsandhowrobusttheresultsareto
violations of these assumptions (e.g., independence assumptions, noiseless settings,
modelwell-specification,asymptoticapproximationsonlyholdinglocally). Theauthors
should reflect on how these assumptions might be violated in practice and what the
implications would be.
•Theauthorsshouldreflectonthescopeoftheclaimsmade,e.g.,iftheapproachwas
only tested on a few datasets or with a few runs. In general, empirical results often
depend on implicit assumptions, which should be articulated.
•Theauthorsshouldreflectonthefactorsthatinfluencetheperformanceoftheapproach.
Forexample,afacialrecognitionalgorithmmayperformpoorlywhenimageresolution
is low or images are taken in low lighting. Or a speech-to-text system might not be
used reliably to provide closed captions for online lectures because it fails to handle
technical jargon.
•The authors should discuss the computational efficiency of the proposed algorithms
and how they scale with dataset size.
•Ifapplicable,theauthorsshoulddiscusspossiblelimitationsoftheirapproachtoaddress
problems of privacy and fairness.
•While the authors might fear that complete honesty about limitations might be used by
reviewersasgroundsforrejection,aworseoutcomemightbethatreviewersdiscover
limitations that aren’t acknowledged in the paper. The authors should use their best
judgmentandrecognizethatindividualactionsinfavoroftransparencyplayanimportant
role in developing norms that preserve the integrity of the community. Reviewers will
be specifically instructed to not penalize honesty concerning limitations.
3.Theory Assumptions and Proofs
Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?
Answer: [Yes]
14Justification: NA
Guidelines:
•The answer NA means that the paper does not include theoretical results.
•All the theorems, formulas, and proofs in the paper should be numbered and cross-
referenced.
•Allassumptionsshouldbeclearlystatedorreferenced inthestatementofanytheorems.
•The proofs can either appear in the main paper or the supplemental material, but if
they appear in the supplemental material, the authors are encouraged to provide a short
proof sketch to provide intuition.
•Inversely,anyinformalproofprovidedinthecoreofthepapershouldbecomplemented
by formal proofs provided in appendix or supplemental material.
•Theorems and Lemmas that the proof relies upon should be properly referenced.
4.Experimental Result Reproducibility
Question: Doesthepaperfullydisclosealltheinformationneededtoreproducethemain
experimentalresultsofthepapertotheextentthatitaffectsthemainclaimsand/orconclusions
of the paper (regardless of whether the code and data are provided or not)?
Answer: [Yes]
Justification: NA
Guidelines:
•The answer NA means that the paper does not include experiments.
•Ifthepaperincludesexperiments,aNoanswertothisquestionwillnotbeperceivedwell
bythereviewers: Makingthepaperreproducibleisimportant,regardlessofwhether
the code and data are provided or not.
•Ifthecontributionisadatasetand/ormodel,theauthorsshoulddescribethestepstaken
to make their results reproducible or verifiable.
•Dependingonthecontribution,reproducibilitycanbeaccomplishedinvariousways.
Forexample, ifthecontributionisanovelarchitecture, describingthearchitecturefully
might suffice, or if the contribution is a specific model and empirical evaluation, it may
be necessary to either make it possible for others to replicate the model with the same
dataset, or provide access to the model. In general. releasing code and data is often
one good way to accomplish this, but reproducibility can also be provided via detailed
instructionsfor howtoreplicatethe results,accessto ahostedmodel (e.g.,inthe case
ofalargelanguagemodel),releasingofamodelcheckpoint,orothermeansthatare
appropriate to the research performed.
•While NeurIPS does not require releasing code, the conference does require all
submissions to provide some reasonable avenue for reproducibility, which may depend
on the nature of the contribution. For example
(a)Ifthecontributionisprimarilyanewalgorithm,thepapershouldmakeitclearhow
to reproduce that algorithm.
(b)If the contribution is primarily a new model architecture, the paper should describe
the architecture clearly and fully.
(c)If the contribution is a new model (e.g., a large language model), then there should
eitherbeawaytoaccessthismodelforreproducingtheresultsorawaytoreproduce
themodel(e.g.,withanopen-sourcedatasetorinstructionsforhowtoconstructthe
dataset).
(d)Werecognizethatreproducibilitymaybetrickyinsomecases,inwhichcaseauthors
are welcome to describe the particular way they provide for reproducibility. In the
case of closed-source models, it may be that access to the model is limited in some
way(e.g.,toregisteredusers),butitshouldbepossibleforotherresearcherstohave
some path to reproducing or verifying the results.
5.Open access to data and code
Question: Doesthepaperprovideopenaccesstothedataandcode,withsufficientinstructions
tofaithfullyreproducethemainexperimentalresults,asdescribedinsupplementalmaterial?
15Answer: [No]
Justification: NA
Guidelines:
•The answer NA means that paper does not include experiments requiring code.
•Please see the NeurIPS code and data submission guidelines ( https://nips.cc/
public/guides/CodeSubmissionPolicy ) for more details.
•While we encourage the release of code and data, we understand that this might not be
possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
includingcode,unlessthisiscentraltothecontribution(e.g.,foranewopen-source
benchmark).
•The instructions should contain the exact command and environment needed to run
to reproduce the results. See the NeurIPS code and data submission guidelines
(https://nips.cc/public/guides/CodeSubmissionPolicy ) for more details.
•The authors should provide instructions on data access and preparation, including how
to access the raw data, preprocessed data, intermediate data, and generated data, etc.
•The authors should provide scripts to reproduce all experimental results for the new
proposed method and baselines. If only a subset of experiments are reproducible, they
should state which ones are omitted from the script and why.
•At submission time, to preserve anonymity, the authors should release anonymized
versions (if applicable).
•Providing asmuch informationas possiblein supplementalmaterial(appended tothe
paper) is recommended, but including URLs to data and code is permitted.
6.Experimental Setting/Details
Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?
Answer: [Yes]
Justification: NA
Guidelines:
•The answer NA means that the paper does not include experiments.
•Theexperimentalsettingshouldbepresentedinthecoreofthepapertoalevelofdetail
that is necessary to appreciate the results and make sense of them.
•The fulldetails canbe provided eitherwith thecode, in appendix,or as supplemental
material.
7.Experiment Statistical Significance
Question: Doesthepaperreporterrorbarssuitablyandcorrectlydefinedorotherappropriate
information about the statistical significance of the experiments?
Answer: [Yes]
Justification: NA
Guidelines:
•The answer NA means that the paper does not include experiments.
•Theauthorsshouldanswer"Yes"iftheresultsareaccompaniedbyerrorbars,confidence
intervals,orstatisticalsignificancetests,atleastfortheexperimentsthatsupportthe
main claims of the paper.
•The factors of variability that the error bars are capturing should be clearly stated (for
example, train/testsplit,initialization,randomdrawingofsomeparameter,oroverall
run with given experimental conditions).
•The method for calculating the error bars should be explained (closed form formula,
call to a library function, bootstrap, etc.)
•The assumptions made should be given (e.g., Normally distributed errors).
•Itshouldbeclearwhethertheerrorbaristhestandarddeviationorthestandarderrorof
the mean.
16•It is OK to report 1-sigma error bars, but one should state it. The authors should
preferablyreporta2-sigmaerrorbarthanstatethattheyhavea96%CI,ifthehypothesis
of Normality of errors is not verified.
•For asymmetric distributions, the authors should be careful not to show in tables or
figuressymmetricerrorbarsthatwouldyieldresultsthatareoutofrange(e.g. negative
error rates).
•If error bars are reported in tables or plots, The authors should explain in the text how
they were calculated and reference the corresponding figures or tables in the text.
8.Experiments Compute Resources
Question: Foreachexperiment,doesthepaperprovidesufficientinformationonthecomputer
resources(typeofcomputeworkers,memory,timeofexecution)neededtoreproducethe
experiments?
Answer: [Yes]
Justification: NA
Guidelines:
•The answer NA means that the paper does not include experiments.
•Thepapershouldindicatethetypeofcomputeworkers CPUorGPU,internalcluster,
or cloud provider, including relevant memory and storage.
•Thepapershouldprovidetheamountofcomputerequiredforeachoftheindividual
experimental runs as well as estimate the total compute.
•The paper should disclose whether the full research project required more compute
than the experiments reported in the paper (e.g., preliminary or failed experiments that
didn’t make it into the paper).
9.Code Of Ethics
Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines ?
Answer: [Yes]
Justification: NA
Guidelines:
•The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
•Ifthe authorsanswer No, theyshould explainthe specialcircumstances thatrequire a
deviation from the Code of Ethics.
•The authors should make sure to preserve anonymity (e.g., if there is a special
consideration due to laws or regulations in their jurisdiction).
10.Broader Impacts
Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?
Answer: [Yes]
Justification: NA
Guidelines:
•The answer NA means that there is no societal impact of the work performed.
•If the authors answer NA or No, they should explain why their work has no societal
impact or why the paper does not address societal impact.
•Examplesofnegativesocietalimpactsincludepotentialmaliciousorunintendeduses
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
(e.g.,deploymentoftechnologiesthatcouldmakedecisionsthatunfairlyimpactspecific
groups), privacy considerations, and security considerations.
•The conference expects that many papers will be foundational research and not tied
toparticularapplications,letalonedeployments. However,ifthereisadirectpathto
any negative applications, the authors should point it out. For example, it is legitimate
topointoutthatanimprovementinthequalityofgenerativemodelscouldbeusedto
17generate deepfakes for disinformation. On the other hand, it is not needed to point out
thatagenericalgorithmforoptimizingneuralnetworkscouldenablepeopletotrain
models that generate Deepfakes faster.
•The authors should consider possible harms that could arise when the technology is
being used as intended and functioning correctly, harms that could arise when the
technologyisbeingusedasintendedbutgivesincorrectresults,andharmsfollowing
from (intentional or unintentional) misuse of the technology.
•Iftherearenegativesocietalimpacts,theauthorscouldalsodiscusspossiblemitigation
strategies (e.g., gated release of models, providing defenses in addition to attacks,
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
feedback over time, improving the efficiency and accessibility of ML).
11.Safeguards
Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?
Answer: [NA]
Justification: NA
Guidelines:
•The answer NA means that the paper poses no such risks.
•Releasedmodelsthathaveahighriskformisuseordual-useshouldbereleasedwith
necessary safeguards to allow for controlled use of the model, for example by requiring
thatusersadheretousageguidelinesorrestrictionstoaccessthemodelorimplementing
safety filters.
•Datasets that have been scraped from the Internet could pose safety risks. The authors
should describe how they avoided releasing unsafe images.
•Werecognizethatprovidingeffectivesafeguardsischallenging,andmanypapersdo
not require this, but we encourage authors to take this into account and make a best
faith effort.
12.Licenses for existing assets
Question: Arethecreatorsororiginalownersofassets(e.g.,code,data,models),usedin
thepaper,properlycreditedandarethelicenseandtermsofuseexplicitlymentionedand
properly respected?
Answer: [Yes]
Justification: NA
Guidelines:
•The answer NA means that the paper does not use existing assets.
•The authors should cite the original paper that produced the code package or dataset.
•Theauthorsshouldstatewhichversionoftheassetisusedand,ifpossible,includea
URL.
•The name of the license (e.g., CC-BY 4.0) should be included for each asset.
•For scraped data from a particular source (e.g., website), the copyright and terms of
service of that source should be provided.
•If assets are released, the license, copyright information, and terms of use in the
package should be provided. For popular datasets, paperswithcode.com/datasets
hascuratedlicensesforsomedatasets. Theirlicensingguidecanhelpdeterminethe
license of a dataset.
•Forexistingdatasetsthatarere-packaged,boththeoriginallicenseandthelicenseof
the derived asset (if it has changed) should be provided.
•Ifthisinformationisnotavailableonline,theauthorsareencouragedtoreachouttothe
asset’s creators.
13.New Assets
Question: Arenewassetsintroducedinthepaperwelldocumentedandisthedocumentation
provided alongside the assets?
18Answer: [NA]
Justification: NA
Guidelines:
•The answer NA means that the paper does not release new assets.
•Researchers should communicate the details of the dataset/code/model as part of their
submissions via structured templates. This includes details about training, license,
limitations, etc.
•Thepapershoulddiscusswhetherandhowconsentwasobtainedfrompeoplewhose
asset is used.
•At submission time, remember to anonymize your assets (if applicable). You can either
create an anonymized URL or include an anonymized zip file.
14.Crowdsourcing and Research with Human Subjects
Question: For crowdsourcing experiments and research with human subjects, does the paper
includethefulltextofinstructionsgiventoparticipantsandscreenshots,ifapplicable,as
well as details about compensation (if any)?
Answer: [NA]
Justification: NA
Guidelines:
•The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
•Including this information in the supplemental material is fine, but if the main
contribution of the paper involves human subjects, then as much detail as possible
should be included in the main paper.
•According to the NeurIPS Code of Ethics, workersinvolvedin data collection, curation,
or other labor should be paid at least the minimum wage in the country of the data
collector.
15.InstitutionalReviewBoard(IRB)ApprovalsorEquivalentforResearchwithHuman
Subjects
Question: Doesthepaperdescribepotentialrisksincurredbystudyparticipants,whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals(oranequivalentapproval/reviewbasedontherequirementsofyourcountryor
institution) were obtained?
Answer: [NA]
Justification: NA
Guidelines:
•The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
•Dependingonthecountryinwhichresearchisconducted,IRBapproval(orequivalent)
mayberequiredforanyhumansubjectsresearch. IfyouobtainedIRBapproval,you
should clearly state this in the paper.
•Werecognizethattheproceduresforthismayvarysignificantlybetweeninstitutions
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
guidelines for their institution.
•For initial submissions, do not include any information that would break anonymity (if
applicable), such as the institution conducting the review.
19