NRGBoost: Energy-Based Generative Boosted Trees
Anonymous Author(s)
Affiliation
Address
email
Abstract
Despite the rise to dominance of deep learning in unstructured data domains, tree- 1
based methods such as Random Forests (RF) and Gradient Boosted Decision Trees 2
(GBDT) are still the workhorses for handling discriminative tasks on tabular data. 3
We explore generative extensions of these popular algorithms with a focus on 4
explicitly modeling the data density (up to a normalization constant), thus enabling 5
other applications besides sampling. As our main contribution we propose an 6
effective energy-based generative boosting algorithm that is analogous to the second 7
order boosting algorithm implemented in popular packages like XGBoost. We 8
show that, despite producing a generative model capable of handling inference tasks 9
over any input variable, our proposed algorithm can achieve similar discriminative 10
performance to GBDT algorithms on a number of real world tabular datasets and 11
outperform competing approaches for sampling. 12
1 Introduction 13
Generative models have achieved tremendous success in computer vision and natural language 14
processing, where the ability to generate synthetic data guided by user prompts opens up many 15
exciting possibilities. While generating synthetic table records does not necessarily enjoy the same 16
wide appeal, this problem has still received considerable attention as a potential avenue for bypassing 17
privacy concerns when sharing data. Estimating the data density, p(x), is another typical application 18
of generative models which enables a host of different use cases that can be particularly interesting 19
for tabular data. Unlike discriminative models which are trained to perform inference over a single 20
target variable, density models can be used more flexibly for inference over different variables or for 21
out of distribution detection. They can also handle inference with missing data in a principled way by 22
marginalizing over unobserved variables. 23
The development of generative models for tabular data has mirrored its progression in computer 24
vision with many of its Deep Learning (DL) approaches being adapted to the tabular domain [Jordon 25
et al., 2018, Xu et al., 2019, Engelmann and Lessmann, 2020, Fan et al., 2020, Zhao et al., 2021, 26
Kotelnikov et al., 2022]. Unfortunately, these methods are only useful for sampling as they either 27
don’t model the density explicitly or can’t evaluate it due to untractable marginalization over high 28
dimensional latent variable spaces. Furthermore, despite growing in popularity, DL has still failed to 29
displace tree-based ensemble methods as the tool of choice for handling tabular discriminative tasks 30
with gradient boosting still being found to outperform neural-network-based methods in many real 31
world datasets [Grinsztajn et al., 2022, Borisov et al., 2022a]. 32
While there have been recent efforts to extend the success of tree-based models to generative modeling 33
[Correia et al., 2020, Wen and Hang, 2022, Nock and Guillame-Bert, 2022, Watson et al., 2023, 34
Nock and Guillame-Bert, 2023, Jolicoeur-Martineau et al., 2023], we find that direct extensions of 35
Random Forests (RF) and Gradient Boosted Decision Tree (GBDT) are still missing. It is this gap 36
that we try to address, seeking to keep the general algorithmic structure of these popular algorithms 37
Submitted to 38th Conference on Neural Information Processing Systems (NeurIPS 2024). Do not distribute.Training Data
 NRGBoost
 TVAE
 TabDDPMFigure 1: Downsampled MNIST samples generated by NRGBoost and two tabular DL methods.
but replacing the optimization of their discriminative objective with a generative counterpart. Our 38
main contributions in this regard are: 39
•Proposing NRGBoost, a novel energy-based generative boosting model that, analogously to 40
the boosting algorithms implemented in popular GBDT packages, is trained to maximize a 41
local second order approximation to the likelihood at each boosting round. 42
•Proposing an approximate sampling algorithm to speed up the training of any tree-based 43
multiplicative generative boosting model. 44
•Exploring the use of bagged ensembles of Density Estimation Trees (DET) [Ram and Gray, 45
2011] with feature subsampling as the generative counterpart to RF. 46
The longstanding popularity of GBDT models in machine learning practice can, in part, be attributed 47
to the strength of its empirical results and the efficiency of its existing implementations. We therefore 48
focus on an experimental evaluation in real world datasets spanning a range of use cases, number 49
of samples and features. We find that, on smaller datasets, our implementation of NRGBoost can 50
be trained in a few minutes on a mid-range consumer CPU and achieve similar discriminative 51
performance to a standard GBDT model while also being able to generate samples that are generally 52
harder to distinguish from real data than state of the art neural-network-based models. 53
2 Energy Based Models 54
An Energy-Based Model (EBM) parametrizes the logarithm of a probability density function directly 55
(up to an unspecified normalizing constant): 56
qf(x) =exp (f(x))
Z[f]. (1)
Here f(x) :X → Ris a real function over the input domain.1We will avoid introducing any 57
parametrization, instead treating the function f∈ F(X)lying in an appropriate function space over 58
the input space as our model parameter directly. Z[f] =P
x∈Xexp (f(x)), known as the partition 59
function, is then a functional of fgiving us the necessary normalizing constant. 60
This is the most flexible way one could represent a probability density function making essentially 61
no compromises on its structure. The downside to this is that for most interesting choices of F, 62
computing or estimating this normalizing constant is untractable which makes training these models 63
difficult. Their unnormalized nature however does not prevent EBMs from being useful in a number 64
of applications besides sampling. Performing inference over a small enough subset of variables 65
requires only normalizing over the set of their possible values and for anomaly or out of distribution 66
detection, knowledge of the normalizing constant is not necessary. 67
One common way to train an energy-based model to approximate a data generating distribution, p(x), 68
is to minimize the Kullback-Leibler divergence between pandqf, or equivalently, maximize the 69
expected log likelihood functional: 70
L[f] =Ex∼plogqf(x) =Ex∼pf(x)−logZ[f] (2)
1We will assume that Xis finite and discrete to simplify the notation and exposition but everything is
applicable to bounded continuous input spaces, replacing the sums with integrals as appropriate.
2This optimization is typically carried out by gradient descent over the parameters of f, but due to 71
the untractability of the partition function, one must rely on Markov Chain Monte Carlo (MCMC) 72
sampling to estimate the gradients [Song and Kingma, 2021]. 73
3 NRGBoost 74
Expanding the increase in log-likelihood in equation 2 due to a variation δfaround an energy function 75
fup to second order we have 76
L[f+δf]−L[f]≈Ex∼pδf(x)−Ex∼qfδf(x)−1
2Varx∼qfδf(x) =:∆Lf[δf]. (3)
Theδfthat maximizes this quadratic approximation should thus have a large positive difference 77
between the expected value under the data and under qfwhile having low variance under qf. We 78
note that just like the original log-likelihood, this Taylor expansion is invariant to adding an overall 79
constant to δf. This means that, in maximizing equation 3 we can consider only functions that have 80
zero expectation under qfin which case we can simplify ∆Lf[δf]as 81
∆Lf[δf] =Ex∼pδf(x)−1
2Ex∼qfδf2(x). (4)
We thus formulate our boosting algorithm as modelling the data density with an additive energy 82
function. At each boosting iteration we improve upon the current energy function ftby finding an 83
optimal step δf∗
tthat maximizes ∆Lft[δf] 84
δf∗
t= arg max
δf∈Ht∆Lft[δf], (5)
where Htis an appropriate space of functions (satisfying Ex∼qftδf(x) = 0 if equation 4 is used). 85
The solution to this problem can be interpreted as a Newton step in the space of energy functions. 86
Because for an energy-based model, the Fisher Information matrix with respect to the energy function 87
and the hessian of the expected log-likelihood are the same, we can also interpret the solution to 88
equation 5 as a natural gradient step (see the Appendix A). This approach is essentially analogous 89
to the second order step implemented in modern discriminative gradient boosting libraries such as 90
XGBoost [Chen and Guestrin, 2016] and LightGBM [Ke et al., 2017] and which can be traced back 91
to Friedman et al. [2000]. 92
In updating the current iterate, ft+1=ft+αt·δf∗
t, we scale δf∗
tby an additional scalar step-size 93
αt. This can be interpreted as a globalization strategy to account for the fact that the quadratic 94
approximation in equation 3 is not necessarily valid over large steps in function space. A common 95
strategy in nonlinear optimization would be to select αtvia a line search based on the original 96
log-likelihood. Common practice in discriminative boosting however is to interpret this step size 97
as a regularization parameter and to select a fixed value in ]0,1]with (more) smaller steps typically 98
outperforming fewer larger ones when it comes to generalization. We choose to adopt a hybrid 99
strategy, first selecting an optimal step size by line search and then shrinking it by a fixed factor. We 100
find that this typically accelerates convergence allowing the algorithm to take comparatively larger 101
steps that increase the likelihood in the initial phase of boosting. For a starting point, f0, we can 102
choose the logarithm of any probability distribution over Xas long as it is easy to evaluate. Sensible 103
choices are a uniform distribution (i.e., f≡0), the product of marginals for the training set, or any 104
mixture distribution between these two. 105
3.1 Weak Learners 106
As a weak learner we will consider functions defined by trees over the input space. I.e., letting 107SJ
j=1Xj=Xbe the partitioning of the input space induced by the leaves of a binary tree whose 108
internal nodes represent a split along one dimension into two disjoint partitions, we take as Hthe set 109
of functions such as 110
δf(x) =JX
j=1wj1Xj(x), (6)
where 1Xdenotes the indicator function of a subset Xandwjare values associated with each 111
leafj∈[1..J]. In a standard decision tree these values would typically encode an estimate of 112
3p(y|x∈Xj), with ybeing a special target variable that is never considered for splitting. In our 113
generative approach they encode unconditional densities (or more precisely energies) over each leaf’s 114
support and every variable can be used for splitting. Note that our functions δfare thus parametrized 115
by the values wjas well the structure of the tree and the variables and values for the split at each 116
node which ultimately determine the Xj. We omit these dependencies for brevity. 117
Replacing the definition in equation 6 in our objective (equation 4) we get the following optimization 118
problem to find the optimal decision tree: 119
max
w1,...,w J,X1,...,X JJX
j=1
wjP(Xj)−1
2w2
jQf(Xj)
s.t.JX
j=1wjQf(Xj) = 0 ,(7)
where P(Xj)andQf(Xj)denote the probability of the event x∈Xjunder the respective distribution 120
and the constraint ensures that δfhas zero expectation under qf. With respect to the leaf weights this 121
is a quadratic program whose optimal solution and objective values are respectively given by 122
w∗
j=P(Xj)
Qf(Xj)−1, ∆L∗
f(X1, . . . , X J) =1
2
JX
j=1P2(Xj)
Qf(Xj)−1
. (8)
Because carrying out the maximization of this optimal value over the tree structure that determines 123
theXjis hard, we approximate its solution by greedily growing a tree that maximizes it when 124
considering how to split each node individually. A parent leaf with support XPis thus split into 2 125
child leaves, with disjoint support, XL∪XR=XP, so as to maximize over all possible partitionings 126
along a single dimension, P(XP), the following objective: 127
max
XL,XR∈P(XP)P2(XL)
Qf(XL)+P2(XR)
Qf(XR)−P2(XP)
Qf(XP). (9)
Note that when using parametric weak learners, computing a second order step would typically 128
involve solving a linear system with a full Hessian. As we can see, this is not the case when the 129
weak learners are decision trees where the optimal value to assign to a leaf jdoes not depend on 130
any information from other leaves and, likewise, the optimal objective value is a sum of terms, each 131
depending only on information from a single leaf. This would have not been the case had we tried to 132
optimize the likelihood functional in Equation 2 directly instead of its quadratic approximation. 133
3.2 Sampling 134
To compute the leaf values in equation 8 and the splitting criterion in equation 9 we would have to 135
know P(X)and be able to compute Qf(X)which is infeasible due to the untractable normalization 136
constant. We therefore estimate these quantities, with recourse to empirical data for P(X), and to 137
samples approximately drawn from the model with MCMC. Because even if the input space is not 138
partially discrete, fis still discontinuous and constant almost everywhere we can’t use gradient based 139
samplers and therefore rely on Gibbs sampling instead. This only requires evaluating each ftalong 140
one dimension at a time, while keeping all others fixed which can be computed efficiently for a tree 141
by traversing it only once. However, since at boosting iteration tour energy function is a sum of t 142
trees, this computation scales linearly with the iteration number. This makes the overall time spent 143
sampling quadratic in the number of iterations and thus precludes us from training models with a 144
large number of trees. 145
In order to reduce the burden associated with this sampling, which can dominate the runtime of 146
training the model, we propose a new sampling approach that leverages the cumulative nature of 147
boosting. The intuition behind this approach is that the set of samples used in the previous boosting 148
round are (approximately) drawn from a distribution that is already close to the new model distribution. 149
It could therefore be helpful to keep some of those samples, especially those that conform the best to 150
the new model. Rejection sampling allows us to do just that. The boosting update in terms of the 151
densities takes the following multiplicative form: 152
qt(x) =ktqt−1(x) exp ( αtδft(x)). (10)
4Here, kis an unknown multiplicative constant and since δftis given by a tree, we can easily bound 153
the exponential factor by finding the leaf with the largest value. We can therefore use the previous 154
model, qt−1(x), as a proposal distribution for which we already have a set of samples and keep each 155
sample, x, with an acceptance probability of: 156
paccept (x) = exph
αt
δft(x)−max
xδft(x)i
. (11)
We note that knowledge of the constant ktis not necessary to compute this acceptance probability. 157
After removing samples from the pool, we can use Gibbs sampling to draw a new set of samples in 158
order to keep a fixed total number of samples per round of boosting. Note also that q0is typically a 159
simple model for which we can both directly evaluate the desired quantities (i.e., Q0(X)for a given 160
partition X) and cheaply draw exact samples from. As such, no sampling is required for the first 161
iteration of boosting and for the second we can draw exact samples from q1with rejection sampling 162
using q0as a proposal distribution. 163
This approach works better when either the range of ftis small or when the step sizes αtare small as 164
this leads to larger acceptance probabilities. Note that in practice it can be helpful to independently 165
refresh a fixed fraction samples, prefresh , at each round of boosting in order to encourage more 166
diverse samples between rounds. This can be accomplished by keeping each sample with a probability 167
paccept (x)(1−prefresh )instead. 168
3.3 Regularization 169
The simplest way to regularize a boosting model is to stop training when overfitting is detected by 170
monitoring a suitable performance metric on a validation set. For NRGBoost this could be the increase 171
in log-likelihood at each boosting round. However, estimating this quantity would require drawing 172
additional validation samples from the model (see Appendix A). An alternative viable validation 173
strategy which needs no additional samples is to simply monitor a discriminative performance metric 174
(over one or more variables). This essentially amounts to monitoring the quality of qf(xi|x−i)instead 175
of the full qf(x). 176
Besides early stopping, the decision trees themselves can be regularized by limiting the depth or total 177
number of leaves of each tree. Additionally we can rely on other strategies such as disregarding splits 178
that would result in a leaf with too little training data, P(X), model data, Qf(X), volume V(X)or 179
too high of a ratio between training and model data P(X)/Qf(X). We found the latter to be the most 180
effective of these, not only yielding better generalization performance than other approaches, but also 181
having the added benefit of allowing us to lower bound the acceptance probability of our rejection 182
sampling scheme. 183
4 Density Estimation Trees and Density Estimation Forests 184
Density Estimation Trees (DET) were proposed by Ram and Gray [2011] as an alternative to 185
histograms and kernel density estimation but have received little attention as generative models 186
for sampling or other applications. They model the density function as a constant value over the 187
support of each leaf in a binary tree, q=PJ
j=1ˆP(Xj)
V(Xj)1Xj, with ˆP(X)being an empirical estimate 188
of probability of the event x∈XandV(X)denoting the volume of X. Note that it is possible 189
to draw an exact sample from this type of model by randomly selecting a leaf, j∈[1..J], given 190
probabilities ˆP(Xj), and then drawing a sample from a uniform distribution over Xj. 191
To fit a DET, Ram and Gray [2011] propose optimizing the Integrated Squared Error (ISE) between the 192
data and model distributions which, following a similar approach to Section 3.1, leads the following 193
optimization problem when considering how to split a leaf node: 194
max
XL,XR∈P(XP)D(P(XL), V(XL)) +D(P(XR), V(XR))−D(P(XP), V(XP)). (12)
For the ISE, Dshould be taken as the function DISE(P, V) =P2/Vwhich leads to a similar splitting 195
criterion to Equation 12 but replacing the previous model’s distribution with the volume measure V 196
which can be interpreted as the uniform distribution on X(up to a multiplicative constant). 197
5Maximum Likelihood Often generative models are trained to maximize the likelihood of the 198
observed data. This was left for future work in Ram and Gray [2011] but, as we show in Appendix 199
B, can be accomplished by replacing the Din Equation 12 with DKL(P, V) =Plog ( P/V).This 200
choice of minimization criterion can be seen as analogous to the choice between Gini impurity and 201
Shannon entropy in the computation of the information gain in decision trees. 202
Bagging and Feature Subsampling Following the common approach in decision trees, Ram and 203
Gray [2011] suggest the use of pruning for regularization of DET models. Practice has however 204
evolved to prefer bagging as a form of regularization rather than relying on single decision trees. We 205
employ same principle to DETs by fitting many trees on bootstrap samples of the data. We also adopt 206
the common practice from Random Forests of randomly sampling a subset of features to consider 207
when splitting any leaf node in order to encourage independence between the different trees in the 208
ensemble. The ensemble model, which we call Density Estimation Forests (DEF) in the sequence, 209
is thus an additive mixture of DETs with uniform weights, therefore still allowing for normalized 210
density computation and exact sampling. 211
5 Related Work 212
Generative Boosting Most prior work on generative boosting focuses on unstructured data and 213
the use of parametric weak learners and is split between two approaches: (i) Additive methods that 214
model the density function as an additive mixture of weak learners such as Rosset and Segal [2002], 215
Tolstikhin et al. [2017]. (ii) Those that take a multiplicative approach modeling the density function as 216
an unnormalized product of weak learners. The latter is equivalent to the energy based approach that 217
writes the energy function (log density) as an additive sum of weak learners. Welling et al. [2002] in 218
particular also approach boosting from the point of view of functional optimization of the likelihood 219
or the logistic loss of an energy-based model. However, they rely on a first order local approximation 220
of the objective since they focus on parametric weak learners such as restricted boltzman machines 221
for which a second order step would be impractical. 222
Greedy Multiplicative Boosting Another more direct multiplicative boosting framework was first 223
proposed by Tu [2007]. At each boosting round a discriminative classifier is trained to distinguish 224
between empirical data and data generated by the current model by estimating the likelihood ratio 225
p(x)/qt(x). This estimated ratio is used as a direct multiplicative factor to update the current model 226
qt(after being raised to an appropriate step size). In ideal conditions this greedy procedure would 227
converge in a single iteration if a step size of 1 would be used. While Tu [2007] does not prescribe a 228
particular choice of classifier to use, Grover and Ermon [2017] proposes a similar concept where the 229
ratio is estimated based on an adversarial bound for an f-divergence and Cranko and Nock [2019] 230
provides additional analysis on this method. In Appendix C we dive deeper into the differences 231
between NRGBoost and this approach when it is adapted to use trees as weak learners. We note, how- 232
ever, that the main difference is that NRGBoost attempts to update the current density proportionally 233
to an exponential of the ratio, exp (αt·p(x)/qt(x)), instead of the ratio directly. 234
Tree-Based Density Modelling Other authors have proposed tree-based density models similar to 235
DET [Nock and Guillame-Bert, 2022] or additive mixtures of tree-based models [Correia et al., 2020, 236
Wen and Hang, 2022, Watson et al., 2023] but perhaps surprisingly, the natural idea of creating an 237
ensemble of DET models through bagging has not been explored before as far as we are aware. Two 238
distinguishing features of some of these alternative approaches are: (i) Unlike DETs, the partitioning 239
of each tree is not driven directly by a density estimation goal. Correia et al. [2020] leverages 240
a standard discriminative Random Forest, therefore giving special treatment to a particular input 241
variable whose conditional estimation drives the choice of partitions and Wen and Hang [2022] 242
proposes using a mid-point random tree partitioning. (ii) Besides modelling the density function as 243
uniform at the leaf of each tree, other authors propose leveraging more complex models [Correia 244
et al., 2020, Watson et al., 2023] which can allow for the use of trees that are more representative 245
with a smaller number of leaves. (iii) Nock and Guillame-Bert [2022] and Watson et al. [2023] both 246
propose generative adversarial frameworks where the generator and discriminator are both a tree or 247
an ensemble of trees respectively. Note that, unlike with boosting, in these approaches the new model 248
doesn’t add to the previous one but replaces it instead. 249
6Table 1: Single variable inference results. The reported values are the averages over 5 cross-validation
folds. The corresponding sample standard deviations are reported in Appendix G.
R2↑ AUC↑ Accuracy ↑
AB CH PR AD MBNE MNIST CT
XGBoost 0.552 0.849 0.678 0.927 0.987 0.976 0.972
RFDE 0.071 0.340 0.059 0.862 0.668 0.302 0.681
DEF (ISE) 0.467 0.737 0.566 0.854 0.653 0.206 0.790
DEF (KL) 0.482 0.801 0.639 0.892 0.939 0.487 0.852
NRGBoost 0.547 0.850 0.676 0.920 0.974 0.966 0.949
Other Recent Tree-Based approaches Nock and Guillame-Bert [2023] proposes a different 250
ensemble approach where each tree does not have their own leaf values that get added or multiplied 251
to produce the final density, but instead serve to collectively define the partitioning of the input space. 252
To train such models the authors propose a boosting framework where, rather than adding a new tree 253
to the ensemble at every iteration, the model is initialized with a fixed number of tree root nodes and 254
each iteration adds a split to an existing leaf node. Finally Jolicoeur-Martineau et al. [2023] propose 255
a diffusion model where a tree-based model (e.g., GBDT) is used to regress the score function. Being 256
a diffusion model, however, means that computing densities is untractable. 257
6 Experiments 258
For our experiments we use 5 tabular datasets from the UCI Machine Learning Repository [Dheeru 259
and Karra Taniskidou, 2017]: Abalone (AB), Physicochemical Properties of Protein Tertiary Structure 260
(PR), Adult (AD), MiniBooNE (MBNE) and Covertype (CT) as well as the California Housing (CH) 261
available through the Scikit-Learn package [Pedregosa et al., 2011]. We also include a downsampled 262
version of MNIST (by 2x along each dimension) which allows us to visually assess the quality of 263
individual samples, something that is generally not possible with structured tabular data, and provides 264
an example of the performance that can be achieved in an unstructured dataset with many features 265
that are correlated among themselves. More details about these datasets are given in Appendix E. 266
We split our experiments into two sections, the first to evaluate the quality of density models directly 267
on a single variable inference task and the second to investigate the performance of our proposed 268
models when used for sampling. 269
6.1 Single Variable Inference 270
In this section we test the ability of a generative model, trained to learn the density over all input 271
variables, q(x), to infer the value of a single one. I.e., we wish to test how good is its estimate of 272
q(xi|x−i). For this purpose we pick xi=yas the original target of the dataset, noting that the 273
models that we train do not treat this variable in any special way, except for the selection of the best 274
model in validation. As such, we would expect that the model’s performance in inference over this 275
particular variable is indicative of its strength on any other single variable inference task and also 276
indicative of the quality of the full q(x)from which the conditional probability estimate is derived. 277
We use XGBoost [Chen and Guestrin, 2016] as a baseline for what should be achievable by a very 278
strong discriminative model. Note that this model is trained to maximize the discriminative likelihood, 279
Ex∼plogq(xi|x−i), directly, not wasting model capacity in learning other aspects of the full data 280
distribution. As another generative baseline we use our own implementation of RFDE [Wen and 281
Hang, 2022] which allows us to gauge the impact of the guided partitioning used in the DEF models 282
over a random partitioning of the input space. 283
We use random search to tune the hyperparameters of the XGBoost model and a grid search to tune the 284
most important hyperparameters of the generative density models. We employ 5-fold cross-validation, 285
repeating the hyperparameter tuning on each fold for all datasets except for the largest one (CT) for 286
which we report results on a single fold. For the full details of the experimental protocol please refer 287
to Appendix F. 288
7Table 2: ML Efficiency results. The reported values are the averages over 5 different datasets
generated by the same model. The best methods for each dataset are in bold and methods whose
difference is <2σaway from zero are underlined . The performance of XGBoost trained on the real
data is also reported for reference.
R2↑ AUC↑ Accuracy ↑
AB CH PR AD MBNE MNIST CT
XGBoost 0.554 0.838 0.682 0.927 0.987 0.976 0.972
TV AE 0.483 0.758 0.365 0.898 0.975 0.688 0.724
TabDDPM 0.539 0.807 0.596 0.910 0.984 0.579 0.818
DEF (KL) 0.450 0.762 0.498 0.892 0.943 0.230 0.753
NRGBoost 0.528 0.801 0.573 0.914 0.977 0.959 0.895
We find that NRGBoost performs better than the additive ensemble models (see Table 1) despite 289
producing more compact ensembles. It often achieves comparable performance to XGBoost on the 290
smaller datasets and with a small gap on the three larger ones. We note also that for the regression 291
datasets the generative models provide an estimate of the full conditional distribution over the target 292
variable rather than a point estimate like XGBoost. While there are other variants of discriminative 293
boosting that also provide an estimate of the aleatoric uncertainty [Duan et al., 2020], they rely on a 294
parametric assumption about p(y|x)that needs to hold for any x. 295
6.2 Sampling 296
In this section, we compare the sampling performance of our proposed methods to neural-network- 297
based methods TV AE [Xu et al., 2019] and TabDDPM [Kotelnikov et al., 2022] on two metrics. 298
Machine Learning Efficiency The Machine Learning (ML) efficiency has been a popular way 299
to measure the quality of generative models for sampling [Xu et al., 2019, Kotelnikov et al., 2022, 300
Borisov et al., 2022b]. It relies on using samples from the model to train a discriminative model which 301
is then evaluated on the real data. Note that this is similar to the single variable inference performance 302
from Section 6.1. In fact, if the density model’s support covers that of the full data, one would expect 303
the discriminative model to recover the generator’s q(y|x), and therefore its performance, in the limit 304
where infinite generated data is used to train it. 305
We use an XGBoost model (with the hyperparameters tuned in real data) as the discriminative model 306
and train it using a similar number of training and validation samples as in the original data. For 307
the density models, we generate samples from the best model found in the previous section and 308
for non-density models we select their hyperparameters by evaluating the ML Efficiency in the 309
real validation set. Note that this leaves the sampling models at a potential advantage since the 310
hyperparameter selection is based on the metric that is being evaluated rather than the direct inference 311
performance of the previous section. 312
Discriminator Measure Similar to Borisov et al. [2022b] we test the capacity of a discriminative 313
model to distinguish between real and generated data. We use the original validation set as the real 314
part of the training data in order to avoid benefiting generative methods that overfit their original 315
training set. A new validation set is carved out of the original test set (20%) and used to tune the 316
hyperparameters of an XGBoost model which we use as our choice of discriminator, evaluating its 317
AUC on the remainder of the real test data. 318
We repeat all experiments 5 times, with 5 different generated datatsets from each model. Results are 319
reported in Tables 2 and 3 showing that (i) NRGBoost outperforms all other methods by substantial 320
margins in the discriminator measure except for the PR and the MBNE datasets. (ii) On the ML 321
Efficiency metric, TabDDPM outperforms NRGBoost by small margins on the small datasets which 322
could in part be explained by the denser hyperparameter tuning favouring models that perform 323
particularly well at inferring the target variable at the expense of the others. Nevertheless, NRGBoost 324
still significantly outperforms all other models on MNIST and CT. Its samples also look visually 325
similar to the real data in both the MNIST and California datasets (see Figures 1 and 2). 326
8Table 3: Discriminator measure results. All results are the AUC of an XGBoost model trained to
distinguish real from generated data an therefore lower means better. The reported values are the
averages over 5 different datasets generated by the same model.
AB CH PR AD MBNE MNIST CT
TV AE 0.971 0.834 0.940 0.898 1.000 1.000 0.999
TabDDPM 0.818 0.667 0.628 0.604 0.789 1.000 0.915
DEF (KL) 0.823 0.751 0.877 0.956 1.000 1.000 0.999
NRGBoost 0.625 0.574 0.631 0.559 0.993 0.943 0.724
LongitudeLatitudeTraining Data
LongitudeNRGBoost
LongitudeDEF (KL)
LongitudeTabDDPM
LongitudeTVAE
Figure 2: Joint histogram for the latitude and longitude for the California Housing dataset.
7 Discussion 327
While the additive tree models like DEF require no sampling to train and are easy to sample from, we 328
find that in practice they require very deep trees to model the data well which, in turn, also requires 329
using a large number of trees in the ensemble to regularize. In our experiments we found that their 330
performance was often capped by the maximum number of leaves we allowed them to grow to ( 214). 331
In contrast, we find that NRGBoost is able to model the data better while using shallower trees 332
and in fewer number. Its main downside is that it can only be sampled from approximately using 333
more expensive MCMC and also requires sampling during the training process. While our fast 334
Gibbs sampling implementation coupled with our proposed sampling approach were able to mitigate 335
the slow training, making these models much more usable in practice they are still cumbersome to 336
use for sampling due to autocorrelation between samples from the same Markov Chain. We argue 337
however that unlike in image or text generation where fast sampling is necessary for an interactive 338
user experience, this can be less of a concern for the task of generating synthetic datasets where the 339
one time cost of sampling is not as important as faithfully capturing the data generating distribution. 340
We also find that tuning the hyperparameters of tree-based models is easier and less crucial than DL 341
models for which many trials fail to produce a reasonable model. In particular we found NRGBoost 342
to be rather robust, with different hyperparameters leading to small differences in performance. 343
Finally, we note that like any other machine learning models, generative models are susceptible to 344
overfitting and are thus liable to leak information about their training data when generating synthetic 345
samples. In this respect, we believe that NRGBoost offers better tools to monitor and control 346
overfitting than other alternatives (see Section 3.3) but, still, due consideration for this risk must be 347
taken into account when sharing synthetic data. 348
8 Conclusion 349
In this work, we extend the two most popular tree-based discriminative methods for use in generative 350
modeling. We find that our boosting approach, in particular, offers generally good discriminative 351
performance and better overall sampling performance than alternatives. We hope that these results 352
encourage further research into generative boosting approaches for tabular data, in particular exploring 353
other applications besides sampling that are enabled by density models. 354
9References 355
Vadim Borisov, Tobias Leemann, Kathrin Seßler, Johannes Haug, Martin Pawelczyk, and Gjergji Kasneci. 356
Deep Neural Networks and Tabular Data: A Survey. IEEE Transactions on Neural Networks and Learning 357
Systems , pages 1–21, 2022a. ISSN 2162-237X, 2162-2388. doi: 10.1109/TNNLS.2022.3229161. URL 358
http://arxiv.org/abs/2110.01889 . arXiv:2110.01889 [cs]. 359
Vadim Borisov, Kathrin Seßler, Tobias Leemann, Martin Pawelczyk, and Gjergji Kasneci. Language Mod- 360
els are Realistic Tabular Data Generators, October 2022b. URL http://arxiv.org/abs/2210.06280 . 361
arXiv:2210.06280 [cs]. 362
Tianqi Chen and Carlos Guestrin. XGBoost: A Scalable Tree Boosting System. In Proceedings of the 22nd ACM 363
SIGKDD International Conference on Knowledge Discovery and Data Mining , pages 785–794, August 2016. 364
doi: 10.1145/2939672.2939785. URL http://arxiv.org/abs/1603.02754 . arXiv:1603.02754 [cs]. 365
Alvaro Correia, Robert Peharz, and Cassio P de Campos. Joints in random forests. In H. Larochelle, M. Ranzato, 366
R. Hadsell, M.F. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems , volume 33, 367
pages 11404–11415. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper_ 368
files/paper/2020/file/8396b14c5dff55d13eea57487bf8ed26-Paper.pdf . 369
Zac Cranko and Richard Nock. Boosted Density Estimation Remastered. In Proceedings of the 36th International 370
Conference on Machine Learning , pages 1416–1425. PMLR, May 2019. URL https://proceedings.mlr. 371
press/v97/cranko19b.html . ISSN: 2640-3498. 372
Li Deng. The mnist database of handwritten digit images for machine learning research. IEEE Signal Processing 373
Magazine , 29(6):141–142, 2012. 374
Dua Dheeru and Efi Karra Taniskidou. UCI machine learning repository, 2017. URL http://archive.ics. 375
uci.edu/ml . 376
Tony Duan, Anand Avati, Daisy Yi Ding, Khanh K. Thai, Sanjay Basu, Andrew Y . Ng, and Alejandro Schuler. 377
NGBoost: Natural Gradient Boosting for Probabilistic Prediction, June 2020. URL http://arxiv.org/ 378
abs/1910.03225 . arXiv:1910.03225 [cs, stat]. 379
Justin Engelmann and Stefan Lessmann. Conditional Wasserstein GAN-based Oversampling of Tabular Data for 380
Imbalanced Learning, August 2020. URL http://arxiv.org/abs/2008.09202 . arXiv:2008.09202 [cs]. 381
Ju Fan, Junyou Chen, Tongyu Liu, Yuwei Shen, Guoliang Li, and Xiaoyong Du. Relational data synthesis using 382
generative adversarial networks: a design space exploration. Proceedings of the VLDB Endowment , 13(12): 383
1962–1975, August 2020. ISSN 2150-8097. doi: 10.14778/3407790.3407802. URL https://dl.acm.org/ 384
doi/10.14778/3407790.3407802 . 385
Jerome Friedman, Trevor Hastie, and Robert Tibshirani. Additive logistic regression: a statisti- 386
cal view of boosting (With discussion and a rejoinder by the authors). The Annals of Statis- 387
tics, 28(2):337–407, April 2000. ISSN 0090-5364, 2168-8966. doi: 10.1214/aos/1016218223. 388
URL https://projecteuclid.org/journals/annals-of-statistics/volume-28/issue-2/ 389
Additive-logistic-regression--a-statistical-view-of-boosting-With/10.1214/aos/ 390
1016218223.full . Publisher: Institute of Mathematical Statistics. 391
Léo Grinsztajn, Edouard Oyallon, and Gaël Varoquaux. Why do tree-based models still outperform deep learning 392
on tabular data?, July 2022. URL http://arxiv.org/abs/2207.08815 . arXiv:2207.08815 [cs, stat]. 393
Aditya Grover and Stefano Ermon. Boosted Generative Models, December 2017. URL http://arxiv.org/ 394
abs/1702.08484 . arXiv:1702.08484 [cs, stat]. 395
Charles R. Harris, K. Jarrod Millman, Stéfan J. van der Walt, Ralf Gommers, Pauli Virtanen, David Cournapeau, 396
Eric Wieser, Julian Taylor, Sebastian Berg, Nathaniel J. Smith, Robert Kern, Matti Picus, Stephan Hoyer, 397
Marten H. van Kerkwijk, Matthew Brett, Allan Haldane, Jaime Fernández del Río, Mark Wiebe, Pearu Peter- 398
son, Pierre Gérard-Marchant, Kevin Sheppard, Tyler Reddy, Warren Weckesser, Hameer Abbasi, Christoph 399
Gohlke, and Travis E. Oliphant. Array programming with NumPy. Nature , 585(7825):357–362, September 400
2020. doi: 10.1038/s41586-020-2649-2. URL https://doi.org/10.1038/s41586-020-2649-2 . 401
Alexia Jolicoeur-Martineau, Kilian Fatras, and Tal Kachman. Generating and imputing tabular data via diffusion 402
and flow-based gradient-boosted trees, 2023. 403
James Jordon, Jinsung Yoon, and Mihaela van der Schaar. PATE-GAN: Generating Synthetic Data with Differ- 404
ential Privacy Guarantees. December 2018. URL https://openreview.net/forum?id=S1zk9iRqF7 . 405
10Guolin Ke, Qi Meng, Thomas Finley, Taifeng Wang, Wei Chen, Weidong Ma, Qiwei Ye, and Tie-Yan Liu. 406
LightGBM: A Highly Efficient Gradient Boosting Decision Tree. In Advances in Neural Information 407
Processing Systems , volume 30. Curran Associates, Inc., 2017. URL https://proceedings.neurips. 408
cc/paper/2017/hash/6449f44a102fde848669bdd9eb6b76fa-Abstract.html . 409
Akim Kotelnikov, Dmitry Baranchuk, Ivan Rubachev, and Artem Babenko. TabDDPM: Modelling Tabular Data 410
with Diffusion Models, September 2022. URL http://arxiv.org/abs/2209.15421 . arXiv:2209.15421 411
[cs]. 412
Richard Nock and Mathieu Guillame-Bert. Generative trees: Adversarial and copycat. In Kamalika Chaudhuri, 413
Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato, editors, Proceedings of the 39th 414
International Conference on Machine Learning , volume 162 of Proceedings of Machine Learning Research , 415
pages 16906–16951. PMLR, 17–23 Jul 2022. URL https://proceedings.mlr.press/v162/nock22a. 416
html . 417
Richard Nock and Mathieu Guillame-Bert. Generative forests, 2023. 418
Melissa E. O’Neill. Pcg: A family of simple fast space-efficient statistically good algorithms for random number 419
generation. Technical Report HMC-CS-2014-0905, Harvey Mudd College, Claremont, CA, September 2014. 420
F. Pedregosa, G. Varoquaux, A. Gramfort, V . Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, 421
V . Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. Scikit-learn: 422
Machine learning in Python. Journal of Machine Learning Research , 12:2825–2830, 2011. 423
Parikshit Ram and Alexander G. Gray. Density estimation trees. In Proceedings of the 17th ACM SIGKDD 424
international conference on Knowledge discovery and data mining , pages 627–635, San Diego California 425
USA, August 2011. ACM. ISBN 978-1-4503-0813-7. doi: 10.1145/2020408.2020507. URL https: 426
//dl.acm.org/doi/10.1145/2020408.2020507 . 427
Saharon Rosset and Eran Segal. Boosting Density Estimation. In Advances in Neural Information Processing 428
Systems , volume 15. MIT Press, 2002. URL https://papers.nips.cc/paper_files/paper/2002/ 429
hash/3de568f8597b94bda53149c7d7f5958c-Abstract.html . 430
Yang Song and Diederik P. Kingma. How to Train Your Energy-Based Models. arXiv:2101.03288 [cs, stat] , 431
January 2021. URL http://arxiv.org/abs/2101.03288 . arXiv: 2101.03288. 432
Ilya Tolstikhin, Sylvain Gelly, Olivier Bousquet, Carl-Johann Simon-Gabriel, and Bernhard Schölkopf. AdaGAN: 433
Boosting Generative Models, May 2017. URL http://arxiv.org/abs/1701.02386 . arXiv:1701.02386 434
[cs, stat]. 435
Zhuowen Tu. Learning Generative Models via Discriminative Approaches. In 2007 IEEE Conference on 436
Computer Vision and Pattern Recognition , pages 1–8, June 2007. doi: 10.1109/CVPR.2007.383035. ISSN: 437
1063-6919. 438
David S. Watson, Kristin Blesch, Jan Kapar, and Marvin N. Wright. Adversarial random forests for density 439
estimation and generative modeling. In Francisco Ruiz, Jennifer Dy, and Jan-Willem van de Meent, editors, 440
Proceedings of The 26th International Conference on Artificial Intelligence and Statistics , volume 206 of 441
Proceedings of Machine Learning Research , pages 5357–5375. PMLR, 25–27 Apr 2023. URL https: 442
//proceedings.mlr.press/v206/watson23a.html . 443
Max Welling, Richard Zemel, and Geoffrey E Hinton. Self Supervised Boosting. In Advances in Neural 444
Information Processing Systems , volume 15. MIT Press, 2002. URL https://papers.nips.cc/paper_ 445
files/paper/2002/hash/cd0cbcc668fe4bc58e0af3cc7e0a653d-Abstract.html . 446
Hongwei Wen and Hanyuan Hang. Random forest density estimation. In Kamalika Chaudhuri, Stefanie Jegelka, 447
Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato, editors, Proceedings of the 39th International 448
Conference on Machine Learning , volume 162 of Proceedings of Machine Learning Research , pages 23701– 449
23722. PMLR, 17–23 Jul 2022. URL https://proceedings.mlr.press/v162/wen22c.html . 450
Lei Xu, Maria Skoularidou, Alfredo Cuesta-Infante, and Kalyan Veeramachaneni. Modeling Tabu- 451
lar data using Conditional GAN. In Advances in Neural Information Processing Systems , vol- 452
ume 32. Curran Associates, Inc., 2019. URL https://proceedings.neurips.cc/paper/2019/hash/ 453
254ed7d2de3b23ab10936522dd547b78-Abstract.html . 454
Zilong Zhao, Aditya Kunar, Hiek Van der Scheer, Robert Birke, and Lydia Y . Chen. CTAB-GAN: Effective 455
Table Data Synthesizing, May 2021. URL http://arxiv.org/abs/2102.08369 . arXiv:2102.08369 [cs]. 456
11A Additional Derivations 457
The expected log-likelihood for an energy-based model (EBM), 458
qf(x) =exp (f(x))
Z[f], (13)
is given by 459
L[f] =Ex∼plogqf(x) =Ex∼pf(x)−logZ[f]. (14)
Thefirst variation ofLcan be computed as 460
δL[f;g]:=dL[f+ϵg]
dϵ
ϵ=0=Ex∼pg(x)−δlogZ[f;g] =Ex∼pg(x)−Ex∼qfg(x).(15)
This is a linear functional of its second argument, g, and can be regarded as a directional derivative 461
ofLatfalong a variation g. The last equality comes from the following computation of the first 462
variation of the log-partition function: 463
δlogZ[f;g] =δZ[f;g]
Z[f](16)
=1
Z[f]X
xexp′(f(x))g(x) (17)
=X
xexp (f(x))
Z[f]g(x) (18)
=Ex∼qfg(x). (19)
Analogous to a Hessian, we can differentiate Equation 15 again along a second independent variation 464
hoffyielding a symmetric bilinear functional which we will write as δ2L[f;g, h]. Note that the 465
first term in equation 2 is linear in fand thus has no curvature, so we only have to consider the log 466
partition function itself: 467
δ2L[f;g, h]:=∂2L[f+ϵg+εh]
∂ϵ∂ε
(ϵ,ε)=0(20)
=−δ2logZ[f;g, h] =−δ{δlogZ[f;g]}[f;h] (21)
=−δ(
1
Z[f]X
xexp (f(x))g(x))
[f;h] (22)
=δZ[f;h]
Z2[f]X
xexp (f(x))g(x)−1
Z[f]X
xexp′(f(x))g(x)h(x) (23)
=δZ[f;h]
Z[f]·Ex∼qfg(x)−1
Z[f]X
xexp (f(x))g(x)h(x) (24)
=Ex∼qfh(x)·Ex∼qfg(x)−Ex∼qfh(x)g(x) (25)
=−Covx∼qf(g(x), h(x)). (26)
Note that this functional is negative semi-definite for all f, i.e.δ2L[f;h, h]≤0, meaning that the 468
log-likelihood is a concave functional of f. 469
Using these results, we can now compute the Taylor expansion of the increment in log-likelihood L 470
from a change f→f+δfup to second order in δf: 471
∆Lf[δf] =δL[f;δf] +1
2δ2L[f;δf, δf ] (27)
=Ex∼pδf(x)−Ex∼qfδf(x)−1
2Varx∼qfδf(x). (28)
12As an aside, defining the functional derivative,δJ[f]
δf(x), of a functional Jimplicitly by: 472
X
xδJ[f]
δf(x)g(x) =δJ[f;g], (29)
we can formally define, by analogy with the parametric case, the Fisher Information "Matrix" (FIM) 473
atfas the following bilinear functional of two independent variations gandh: 474
F[f;g, h]:=−X
y,z
Ex∼qfδ2logqf(x)
δf(y)δf(z)
g(y)h(z) (30)
=X
y,zδ2logZ[f]
δf(y)δf(z)g(y)h(z) (31)
=δ2logZ[f;g, h]. (32)
The only difference to the second-order variation of 2 computed in equation 20 would be that the 475
expectation is taken under the model distribution, qf, instead of the data distribution p. However, 476
because the only term in logqf(x)that is non-linear in fis the log-partition functional, which is not 477
a function of x, this expectation plays no role in the computation and we get the result that the FIM is 478
the same as the negative Hessian of the log-likelihood for these models. 479
A.1 Application to Piecewise Constant Functions 480
Considering a weak learner such as 481
δf(x) =JX
j=1wj1Xj(x), (33)
where the subsets Xjare disjoint and cover the entire input space, X, we have that 482
Ex∼qδf(x) =X
x∈Xq(x)JX
j=1wj1Xj(x) (34)
=JX
j=1wjX
x∈Xjq(x) =JX
j=1wjQ(Xj). (35)
Similarly, making use of the fact that 1Xi(x)1Xj(x) =δij1Xi(x), we can compute 483
Ex∼qδf2(x) =X
x∈Xq(x)
JX
j=1wj1Xj(x)
2
=JX
j=1w2
jQ(Xj). (36)
In fact, we can extend this to any ordinary function of δf: 484
Ex∼qg(δf(x)) =X
x∈Xq(x)JX
j=11Xj(x)g(δf(x)) (37)
=JX
j=1X
x∈Xjq(x)g(wj) (38)
=JX
j=1g(wj)Q(Xj), (39)
where we made use of the fact that the 1Xjconstitute a partition of unity: 485
1 =JX
j=11Xj(x). (40)
13Finally, we can compute the increase in likelihood from a step f→f+α·δfas 486
L[f+α·δf]−L[f] =Ex∼p[α·δf(x)]−logZ[f+α·δf] + log Z[f] (41)
=αEx∼pδf(x)−logEx∼qfexp(αδf(x)) (42)
=αJX
j=1wjP(Xj)−logJX
j=1Qf(Xj) exp ( αwj), (43)
where in equation 42 we made use of the equality: 487
logZ[f+α·δf]−logZ[f] = logP
xexp(f(x) +αδf(x))
Z[f]= logX
xqf(x) exp( αδf(x)),
(44)
and of the result in equation 39 in the final step. 488
This result can be used to conduct a line search over the step size using training data and to estimate 489
an increase in likelihood at each round of boosting for the purpose of early stopping, using validation 490
data. 491
B Training Density Estimation Trees 492
Density Estimation Trees (DET) [Ram and Gray, 2011] model the density function as a piecewise 493
constant function, 494
q(x) =JX
j=1vj1Xj(x), (45)
where Xjare given by a partitioning of the input space Xinduced by a binary tree and the vjare the 495
density values associated with each leaf that, for the time being, we will only require to be such that 496
q(x)sums to one. 497
Ram and Gray [2011] proposes fitting DET models to directly minimize a generative objective, the 498
Integrated Squared Error (ISE) between the data generating distribution, p(x)and the model: 499
min
q∈QX
x∈X(p(x)−q(x))2. (46)
Noting that qis a function as in Equation 45 and thatSJ
j=1Xj=X, we can rewrite this as 500
min
v1,...,v J,X1,...,X JX
x∈Xp2(x) +JX
j=1X
x∈Xj 
v2
j−2vjp(x)
s.t.JX
j=1X
x∈Xjvj= 1.(47)
Since the first term in the objective does not depend on the model this optimization problem can be 501
further simplified as 502
min
v1,...,v J,X1,...,X JJX
j=1 
v2
jV(Xj)−2vjP(Xj)
s.t.JX
j=1vjV(Xj) = 1 ,(48)
where V(X)denotes the volume of a subset X. Solving this quadratic program for the vjwe obtain 503
the following optimal leaf values and objective: 504
v∗
j=P(Xj)
V(Xj), ISE∗(X1, . . . , X J) =−JX
j=1P2(Xj)
Vf(Xj). (49)
One can therefore grow a tree by greedily choosing to split a parent leaf with support XPinto two 505
leaves with supports XLandXRso as to maximize the following criterion: 506
max
XL,XR∈P(XP)P2(XL)
V(XL)+P2(XR)
V(XR)−P2(XP)
V(XP). (50)
14B.1 Maximum Likelihood 507
To maximize the likelihood, 508
max
qEx∼plogq(x), (51)
rather than the ISE one can use the same approach. Here the optimization problem to solve is: 509
max
v1,...,v J,X1,...,X JJX
j=1P(Xj) logvj
s.t.JX
j=1vjV(Xj) = 1 .(52)
This is, again, easy to solve for vjsince it is separable over jafter removing the constraint using 510
Lagrange multipliers. The optimal leaf values and objective are in this case: 511
v∗
j=P(Xj)
V(Xj), L∗(X1, . . . , X J) =JX
j=1P(Xj) logP(Xj)
Vf(Xj). (53)
The only change is therefore to the splitting criterion which should become: 512
max
XL,XR∈P(XP)P(XL) logP(XL)
V(XL)+P(XR) logP(XR)
V(XR)−P(XP) logP(XP)
V(XP). (54)
C Greedy Tree Based Multiplicative Boosting 513
In multiplicative generative boosting an unnormalized current density model, ˜qt−1(x), is updated at 514
each boosting round by multiplication with a new factor δqαt
t(x): 515
˜qt(x) = ˜qt−1(x)·δqαt
t(x). (55)
For our proposed NRGBoost, this factor is chosen in order to maximize a local quadratic approx- 516
imation of the log likelihood around qt−1as a functional of the log density (see Section 3). The 517
motivation behind the greedy approach of Tu [2007] or Grover and Ermon [2017] is to instead make 518
the update factor δqt(x)proportional to the likelihood ratio rt(x):=p(x)/qt−1(x)directly, which 519
under ideal conditions would mean that the method converges immediately when choosing a step size 520
αt= 1. In more realistic setting, however, this method has been shown to converge under conditions 521
on the performance of the individual δqtas discriminators between real and generated data [Tu, 2007, 522
Grover and Ermon, 2017, Cranko and Nock, 2019]. 523
While in principle this desired rt(x)could be derived from any binary classifier that is trained to 524
predict a probability of a datapoint being generated (e.g., by training it to minimize a strictly proper 525
loss) and Tu [2007] does not prescribe any particular choice, Grover and Ermon [2017] propose 526
relying on the following variational bound of an f-divergence to derive an estimator for this ratio: 527
Df(P∥Qt−1)≥sup
u∈Ut
Ex∼pu(x)−Ex∼qt−1f∗(u(x))
. (56)
Heref∗denotes the convex conjugate of f. This bound is tight, with the optimum being achieved for 528
u∗
t(x) =f′(p(x)/qt−1(x)), ifUtis capable of representing this function. (f′)−1(u∗
t(x))can thus be 529
interpreted as an approximation of rt(x). 530
Adapting this method to use trees as weak learners can be accomplished by considering Utin Equation 531
56 to be defined by tree functions u=1/JPJ
j=1wj1Xjwith leaf values wjand leaf supports Xj. 532
At each boosting iteration a new tree, u∗
tcan thus be grown to greedily optimize the lower bound in 533
the r.h.s. of Equation 56 and setting δqt(x) = (f′)−1(u∗
t(x))which is thus also a tree with the same 534
leaf supports and leaf values given by vj:= (f′)−1(wj). This leads to the seaprable optimization 535
problem: 536
max
w1,...,w J,X1,...,X JJX
j[P(Xj)wj−Q(Xj)f∗(wj)]. (57)
15Table 4: Comparison of splitting criterion and leaf weights for the different versions of boosting.
Splitting Criterion Leaf Values (Density)
DiscBGM (KL) Plog ( P/Q) P/Q
DiscBGM ( χ2) P2/Q P/Q
NRGBoost P2/Q exp ( P/Q−1)
Note that we drop the iteration indices from this point onward for brevity. Maximizing over wjwith 537
theXjfixed we have that w∗
j=f′(P(Xj)/Q(Xj))which yields the optimal value 538
J∗(X1, . . . , X j) =X
j
P(Xj)f′P(Xj)
Q(Xj)
−Q(Xj)(f∗◦f′)P(Xj)
Q(Xj)
(58)
that in turn determines the splitting criterion as a function of the choice of f. Finally, the optimal 539
density values for the leaves are given by 540
v∗
j= (f′)−1(w∗
j) =P(Xj)
Q(Xj). (59)
It is interesting to note two particular choices of f-divergences. For the KL divergence, f(t) =tlogt 541
andf′(t) = 1 + log t= (f∗)−1(t). This leads to 542
JKL(X1, . . . , X j) =X
jP(Xj) logP(Xj)
Q(Xj)(60)
as the splitting criterion. The Pearson χ2divergence, with f(t) = (t−1)2, leads to the same splitting 543
criterion as NRGBoost. Note however that for NRGBoost the leaf values for the multiplicative update 544
of the density are given by exp ( P(Xj)/Q(Xj)−1)instead of the ratio directly. Table 4 summarizes 545
these results. 546
Another interesting observation is that a DET model can be interpreted as a single round of greedy 547
multiplicative boosting starting from a uniform initial model. The choice of the ISE as the criterion to 548
optimize the DET corresponds to the choice of Pearson’s χ2divergence and likelihood to the choice 549
of KL divergence. 550
D Implementation Details 551
Discretization In our practical implementation of tree based methods we first discretize the input 552
space by binning continuous numerical variables by quantiles. Furthermore we also bin discrete 553
numerical variables in order to keep their cardinalities smaller than 256. This can also be interpreted 554
as establishing a priori a set of discrete values to consider when splitting on each numerical variable 555
and is done for computational efficiency, being inspired by LightGBM [Ke et al., 2017]. 556
Categorical Splitting For splitting on a categorical variable we once again take inspiration from 557
LightGBM. Rather than relying on one-vs-all splits we found it better to first order the possible 558
categorical values at a leaf according to a pre-defined sorting function and then choose the optimal 559
many-vs-many split as if the variable was numerical. The function used to sort the values is the leaf 560
value function. E.g., for splitting on a categorical variable xiwe order each possible categorical value 561
kbyˆP(xi=k,X−i)/ˆQ(xi=k,X−i)in the case of NRGBoost where X−idenotes the leaf support over the 562
remaining variables. 563
Tree Growth Strategy We always grow trees in best first order. I.e., we always split the current 564
leaf node that yields the maximum gain in the chosen objective value. 565
Line Search As mentioned in Section 3, we perform a line search to find the optimal step size after 566
each round of boosting in order to maximize the likelihood gain in Equation 43. Because evaluating 567
multiple possible step sizes, αt, is inexpensive, we simply do a grid search over 101 different step 568
sizes in the range [10−3,10]with their logarithm uniformly distributed. 569
16Table 5: Dataset Information. We respect the original test sets of each dataset when provided,
otherwise we set aside 20% of the original dataset as a test set. 20% of the remaining data is set aside
as a validation set used for hyperparameter tuning.
Abbr Name Train + Val Test Num Cat Target Cardinality
AB Abalone 3342 835 7 1 Num 29
CH California Housing 16512 4128 8 0 Num Continuous
PR Protein 36584 9146 9 0 Num Continuous
AD Adult 32560 16280∗6 8 Cat 2
MBNE MiniBooNE 104051 26013 50 0 Cat 2
MNIST MNIST (downsampled) 60000 10000∗196 0 Cat 10
CT Covertype 464810 116202 10 2 Cat 7
∗Original test set was respected.
Random Forest Density Estimation (RFDE) We implement the RFDE method [Wen and Hang, 570
2022] after quantile discretization of the dataset and therefore split at the midpoint of the discretized 571
dimension instead of the original one. When a leaf support has odd cardinality over the splitting 572
dimension a random choice is made over the two possible splitting values. Finally, the original paper 573
does not mention how to split over categorical domains. We therefore choose to randomly split the 574
possible categorical values for a leaf evenly as we found that this yielded slightly better results than a 575
random one vs all split. 576
Code Our implementation of the proposed tree-based methods is mostly Python code using the 577
NumPy library [Harris et al., 2020]. We implement the tree evaluation and Gibbs sampling in C, 578
making use of the PCG library [O’Neill, 2014] for random number generation. 579
E Datasets 580
We use 5 datasets from the UCI Machine Learning Repository [Dheeru and Karra Taniskidou, 2017]: 581
Abalone, Physicochemical Properties of Protein Tertiary Structure (referred to as Protein in the 582
sequence), Adult, MiniBooNE and Covertype. We also use the California Housing dataset which was 583
downloaded through the Scikit-Learn package Pedregosa et al. [2011] and a downsampled version of 584
the MNIST dataset Deng [2012]. Table 5 summarizes the main details of these datasets as well as the 585
approximate number of samples used for train/validation/test for each cross-validation fold. 586
F Experimental Setup 587
F.1 XGBoost Hyperparameter Tuning 588
To tune the hyperparameters of XGBoost we use 100 trials of random search with the search space 589
defined in Table 6. 590
Table 6: XGBoost hyperparameter tuning search space. δ(0)denotes a point mass distribution at 0.
Parameter Distribution or Value
learning_rate LogUniform 
10−3,1.0
max_leaves Uniform ({16,32,64,128,256,512,1024})
min_child_weight LogUniform 
10−1,103
reg_lambda 0.5·δ(0) + 0 .5·LogUniform 
10−3,10
reg_alpha 0.5·δ(0) + 0 .5·LogUniform 
10−3,10
max_leaves 0 (we already limit the number of leaves)
grow_policy lossguide
tree_method hist
17Each model was trained for 1000 boosting rounds on regression and binary classification tasks. For 591
multi-class classification tasks a maximum number of 200 rounds of boosting was used due to the 592
larger size of the datasets and because a separate tree is built at every round for each class. The 593
best model was selected based on the validation set, together with the boosting round where the best 594
performance was attained. The test metrics reported correspond to the performance of the selected 595
model at that boosting round on the test set. 596
F.2 TV AE Hyperparameter Tuning 597
To tune the hyperparameters of TV AE we use 50 trials of random search with the search spaces 598
defined in Table 7. 599
The TV AE implementations used are from the latest version of the SDV package ( https://github. 600
com/sdv-dev/SDV ) available at the time. 601
Table 7: TV AE hyperparameter tuning search space. We set both compress_dims and
decompress_dims to have the number of layers specified by num_layers , with hidden_dim
hidden units in each layer. We use larger batch sizes and smaller number of epochs for the larger
datasets (MBNE, MNIST, CO).
Parameter Datasets Distribution or Value
epochs AB, CH, PR, AD Uniform ([100..500])
MBNE, MNIST, CO Uniform ([50..200])
batch_size AB, CH, PR, AD Uniform ({100,200, . . . , 500})
MBNE, MNIST, CO Uniform ({500,1000, . . . , 2500})
embedding_dim all Uniform ({32,64,128,256,512})
hidden_dim all Uniform ({32,64,128,256,512})
num_layers all Uniform ({1,2,3})
compress_dims all (hidden_dim,) * num_layers
decompress_dims all (hidden_dim,) * num_layers
F.3 TabDDPM Hyperparameter Tuning 602
To tune the hyperparameters of TabDDPM we use 50 trials of random search with the same search 603
space that the original authors use in their paper [Kotelnikov et al., 2022]. 604
We use the official implementation ( https://github.com/yandex-research/tab-ddpm ) 605
adapted to use our datasets and validation setup. 606
F.4 Random Forest Density Estimation 607
For RFDE models we train a total of 1000 trees. The only hyperparameter that we tune is the 608
maximum number of leaves per tree for which we test the values [26,27, . . . , 214]. For the Adult 609
dataset, due to limitations of our tree evaluation implementation we only values test up to 213. 610
F.5 Density Estimation Forests Hyperparameter Tuning 611
We train ensembles with 1000 DET models. Only three hyperparameters are tuned, using three nested 612
loops. Every loop runs over the possible values of a single parameter in a pre-defined order with early 613
stopping triggering if a value fails to improve the validation metric over the previous one. The tuned 614
parameters along with their possible values are reported in Table 8 615
F.6 NRGBoost 616
We train NRGBoost models for a maximum of 200 rounds of boosting. The starting point of each 617
NRGBoost model was selected as a mixture model between a uniform distribution (10%) and the 618
18Table 8: DEF models grid search space. Rows are in order of outermost loop to innermost loop.
Note that for the Adult dataset, due to limitations of the implementation a maximum number of 8192
leaves is used instead of 16384.
Parameter Description
max_leaves The maximum number of leaves per tree [16384 ,4096,1024,256]
feature_frac The fraction of features to consider when splitting a node
as a function of the total number of features d[d−1/2, d−1/4,1]
min_data_in_leaf The minimum number of data points that need to be left
in each leaf for a split to be considered[0,1,3,10,30]
product of training marginals (90%) on the discretized input space. We observed that this mixture 619
coefficient does not have much impact on the results however. 620
We only tune two parameters for NRGBoost Models: 621
•The maximum number of leaves for which we try the values [64,256,1024,4096] in order, 622
stopping if performance fails to improve from one value to the next. For the CT dataset we 623
also include 16384 in the values to test. 624
•The constant factor by which the optimal step size determined by the line search is shrunk 625
at each round of boosting. This is essentially the "learning rate" parameter. To tune it we 626
perform a Golden-section search for the log of its value using a total of 6 evaluations. The 627
range we use is [0.01,0.5]. 628
This means that at maximum we train only 24 NRGBoost models (30 for CT). 629
All other relevant parameters are fixed and their values, along with a short description, is given in 630
Table 9. 631
Table 9: NRGBoost fixed parameters.
Parameter Description
num_rounds Total number of rounds of boosting 200
splitter How the next leaf to split is determined best
line_search Whether to use a line search in determining the step size True
max_ratio_leaf Maximum ratio between training data and model data in each leaf 2
num_samples Total number of samples in the sample pool 80000
320000 (CT)
p_refresh Indepdendent probability that a sample from the pool is replaced 0.1
burn_in Number of samples to discard from the beginning of each chain 100
num_chains Number of independent chains used for sampling 16
64 (CT)
F.7 Evaluation Setup 632
Single variable inference For the single variable inference evaluation, the best models are selected 633
by their discriminative performance on a validation set. The entire setup is repeated five times with 634
different cross-validation folds and with different seeds for all sources of randomness except on the 635
CT dataset due to its large size. For the Adult and MNIST datasets the test set is fixed but training 636
and validation splits are still rotated. 637
Sampling For the sampling evaluation we use a single train/validation/test split of the real data 638
(corresponding to the first fold in the previous setup) for training the generative models. The density 639
models used are those previously selected based on their single variable inference performance 640
on the validation set. For the sampling models (TV AE and TabDDPM) we directly evaluate their 641
19ML Efficiency using the validation data by training an XGBoost model on generated data. The 642
hyperparameters used for this XGBoost model are those selected on the real data in the previous 643
experiment. We only use a generated validation set in order to select the best stopping point for 644
XGBoost. 645
ML Efficiency For each selected model we sample a train and validation sets with the same number 646
of samples as those used on the original data. For NRGBoost we generate these samples by running 647
64 chains in parallel with 100 steps of burn in and downsampling their outputs by 30 (for the smaller 648
datasets) or 10 (for MBNE, MNIST and CT). The setup is repeated 5 times with 5 different datasets 649
generated for each method. 650
Discriminator Measure We create the training, validation and test sets to train an XGBoost model 651
to discriminate between real and generated data using the following process: 652
•The original validation set is used as the real part of the training set in order to avoid 653
benefitting generative methods that overfit their training set. 654
•The original test set is split 20%/80%. The 20% portion is used as the real part of the 655
validation set and the 80% portion as the real part of the test set. 656
•To form the generated part of the training, validation and test sets for the smaller datasets 657
we sample data according to the original number of samples in the train, validation and 658
test splits on the real data. Note that this makes the ratio of real to synthetic data 1:4 in the 659
training set. This is deliberate because for these smaller datasets the original validation has 660
few samples and adding extra synthetic data helps the discriminator. 661
•For the larger datasets we generate the same number of synthetic samples as there are real 662
samples on each split, therefore making every ratio 1:1 because the discriminator is typically 663
already too powerful and doesn’t need extra data. 664
Because, in contrast to the previous metric, having a lower number of effective samples helps rather 665
than hurts we take extra precautions to not generate correlated data with NRGBoost. We draw each 666
sample by running its own independent chain for 100 steps starting from an independent sample from 667
the initial model which is a rather slow process. The setup is repeated 5 times with 5 different sets of 668
generated samples from each method. 669
F.8 Computational Resources 670
The experiments were run on a machine equipped with an AMD Ryzen 7 7700X 8 core CPU and 32 671
GB of RAM. The comparisons with TV AE and TabDDPM further made use of a GeForce RTX 3060 672
GPU with 12 GB of VRAM. 673
G Additional Results 674
G.1 Standard Errors 675
In Tables 10, 11 and 12 we report the sample standard deviations obtained for the main tables 676
presented in the paper. 677
G.2 Samples 678
In Figure G.2 we show the convergence of a Gibbs sampler sampling from a NRGBoost model. In 679
only a few samples each chain appears to have converged to the data manifold after starting at a 680
random sample from the initial model (a mixture between the product of training marginals and a 681
uniform). Note how consecutive samples are autocorrelated. In particular it can be rare for a chain 682
to switch between two different modes of the distribution (e.g., switching digits) even though a few 683
such transitions can be observed. 684
20Table 10: Single variable inference sample standard deviations.
R2AUC Accuracy
AB CH PR AD MBNE MNIST
XGBoost 0.0354 0.0092 0.0036 0.0004 0.0005 0.0017
RFDE 0.0963 0.0039 0.0071 0.0023 0.0078 0.0101
DEF (ISE) 0.0373 0.0080 0.0023 0.0026 0.0108 0.0107
DEF (KL) 0.0271 0.0083 0.0038 0.0005 0.0009 0.0073
NRGBoost 0.0358 0.0113 0.0087 0.0006 0.0007 0.0009
Table 11: ML Efficiency results sample standard deviations.
R2AUC Accuracy
AB CH PR AD MBNE MNIST CT
TV AE 0.0059 0.0054 0.0054 0.0011 0.0002 0.0088 0.0013
TabDDPM 0.0182 0.0049 0.0072 0.0007 0.0000 0.0250 0.0012
DEF (KL) 0.0131 0.0063 0.0073 0.0011 0.0022 0.0283 0.0029
NRGBoost 0.0161 0.0010 0.0076 0.0009 0.0009 0.0008 0.0011
Table 12: Discriminator measure sample standard deviations.
AB CH PR AD MBNE MNIST CT
TV AE 0.0039 0.0055 0.0017 0.0012 0.0001 0.0000 0.0001
TabDDPM 0.0146 0.0045 0.0043 0.0022 0.0024 0.0000 0.0074
DEF (KL) 0.0129 0.0081 0.0022 0.0016 0.0000 0.0000 0.0001
NRGBoost 0.0167 0.0115 0.0059 0.0032 0.0005 0.0026 0.0058
Figure 3: Downsampled MNIST samples generated by Gibbs sampling from a NRGBoost model.
Each row corresponds to an independent chain initialized with a sample from the initial model f0
(first column). Each column represents a consecutive sample from the chain.
21Table 13: Best NRGBoost model parameters per dataset and the wall time taken to train it. The
format is minutes:seconds.
AB CH PR AD MBNE MNIST CT
max_leaves 64 1024 1024 256 1024 4096 16384
shrinkage 0.14 0.063 0.14 0.09 0.199 0.199 0.098
Time 1:18 4:17 5:27 3:54 20:36 149:30 179:11
G.3 Time 685
In Table 13 we report the best hyperparameters found for NRGBoost for the first cross-validation 686
fold together with the time taken to train this best model. 687
22NeurIPS Paper Checklist 688
1.Claims 689
Question: Do the main claims made in the abstract and introduction accurately reflect the 690
paper’s contributions and scope? 691
Answer: [Yes] 692
Justification: Claims about proposal of novel methods are justified in Sections 3 and 4. 693
Claims about empirical results are justified in Section 6 and Appendix G. 694
Guidelines: 695
•The answer NA means that the abstract and introduction do not include the claims 696
made in the paper. 697
•The abstract and/or introduction should clearly state the claims made, including the 698
contributions made in the paper and important assumptions and limitations. A No or 699
NA answer to this question will not be perceived well by the reviewers. 700
•The claims made should match theoretical and experimental results, and reflect how 701
much the results can be expected to generalize to other settings. 702
•It is fine to include aspirational goals as motivation as long as it is clear that these goals 703
are not attained by the paper. 704
2.Limitations 705
Question: Does the paper discuss the limitations of the work performed by the authors? 706
Answer: [Yes] 707
Justification: We discuss limitations of our proposed method both in the section that intro- 708
duces it (Section 3) as well as in the experiments (Section 6) and discussion (Section 7) 709
sections. 710
Guidelines: 711
•The answer NA means that the paper has no limitation while the answer No means that 712
the paper has limitations, but those are not discussed in the paper. 713
• The authors are encouraged to create a separate "Limitations" section in their paper. 714
•The paper should point out any strong assumptions and how robust the results are to 715
violations of these assumptions (e.g., independence assumptions, noiseless settings, 716
model well-specification, asymptotic approximations only holding locally). The authors 717
should reflect on how these assumptions might be violated in practice and what the 718
implications would be. 719
•The authors should reflect on the scope of the claims made, e.g., if the approach was 720
only tested on a few datasets or with a few runs. In general, empirical results often 721
depend on implicit assumptions, which should be articulated. 722
•The authors should reflect on the factors that influence the performance of the approach. 723
For example, a facial recognition algorithm may perform poorly when image resolution 724
is low or images are taken in low lighting. Or a speech-to-text system might not be 725
used reliably to provide closed captions for online lectures because it fails to handle 726
technical jargon. 727
•The authors should discuss the computational efficiency of the proposed algorithms 728
and how they scale with dataset size. 729
•If applicable, the authors should discuss possible limitations of their approach to 730
address problems of privacy and fairness. 731
•While the authors might fear that complete honesty about limitations might be used by 732
reviewers as grounds for rejection, a worse outcome might be that reviewers discover 733
limitations that aren’t acknowledged in the paper. The authors should use their best 734
judgment and recognize that individual actions in favor of transparency play an impor- 735
tant role in developing norms that preserve the integrity of the community. Reviewers 736
will be specifically instructed to not penalize honesty concerning limitations. 737
3.Theory Assumptions and Proofs 738
Question: For each theoretical result, does the paper provide the full set of assumptions and 739
a complete (and correct) proof? 740
23Answer: [Yes] 741
Justification: All results presented in the main paper are justified in Appendices A and B. 742
Guidelines: 743
• The answer NA means that the paper does not include theoretical results. 744
•All the theorems, formulas, and proofs in the paper should be numbered and cross- 745
referenced. 746
•All assumptions should be clearly stated or referenced in the statement of any theorems. 747
•The proofs can either appear in the main paper or the supplemental material, but if 748
they appear in the supplemental material, the authors are encouraged to provide a short 749
proof sketch to provide intuition. 750
•Inversely, any informal proof provided in the core of the paper should be complemented 751
by formal proofs provided in appendix or supplemental material. 752
• Theorems and Lemmas that the proof relies upon should be properly referenced. 753
4.Experimental Result Reproducibility 754
Question: Does the paper fully disclose all the information needed to reproduce the main ex- 755
perimental results of the paper to the extent that it affects the main claims and/or conclusions 756
of the paper (regardless of whether the code and data are provided or not)? 757
Answer: [Yes] 758
Justification: Additional implementation details of our method are provided in Appendix D 759
and the full experimental setup is described in detail in Appendix F. 760
Guidelines: 761
• The answer NA means that the paper does not include experiments. 762
•If the paper includes experiments, a No answer to this question will not be perceived 763
well by the reviewers: Making the paper reproducible is important, regardless of 764
whether the code and data are provided or not. 765
•If the contribution is a dataset and/or model, the authors should describe the steps taken 766
to make their results reproducible or verifiable. 767
•Depending on the contribution, reproducibility can be accomplished in various ways. 768
For example, if the contribution is a novel architecture, describing the architecture fully 769
might suffice, or if the contribution is a specific model and empirical evaluation, it may 770
be necessary to either make it possible for others to replicate the model with the same 771
dataset, or provide access to the model. In general. releasing code and data is often 772
one good way to accomplish this, but reproducibility can also be provided via detailed 773
instructions for how to replicate the results, access to a hosted model (e.g., in the case 774
of a large language model), releasing of a model checkpoint, or other means that are 775
appropriate to the research performed. 776
•While NeurIPS does not require releasing code, the conference does require all submis- 777
sions to provide some reasonable avenue for reproducibility, which may depend on the 778
nature of the contribution. For example 779
(a)If the contribution is primarily a new algorithm, the paper should make it clear how 780
to reproduce that algorithm. 781
(b)If the contribution is primarily a new model architecture, the paper should describe 782
the architecture clearly and fully. 783
(c)If the contribution is a new model (e.g., a large language model), then there should 784
either be a way to access this model for reproducing the results or a way to reproduce 785
the model (e.g., with an open-source dataset or instructions for how to construct 786
the dataset). 787
(d)We recognize that reproducibility may be tricky in some cases, in which case 788
authors are welcome to describe the particular way they provide for reproducibility. 789
In the case of closed-source models, it may be that access to the model is limited in 790
some way (e.g., to registered users), but it should be possible for other researchers 791
to have some path to reproducing or verifying the results. 792
5.Open access to data and code 793
24Question: Does the paper provide open access to the data and code, with sufficient instruc- 794
tions to faithfully reproduce the main experimental results, as described in supplemental 795
material? 796
Answer: [No] 797
Justification: Unfortunately we did not have time to clean up the code and document it 798
so that it could be useful at the time of the paper deadline. But we intend to make our 799
implementations of the proposed algorithms available as a python library as soon as possible 800
and will also open source the full experimental setup on Github. 801
Guidelines: 802
• The answer NA means that paper does not include experiments requiring code. 803
•Please see the NeurIPS code and data submission guidelines ( https://nips.cc/ 804
public/guides/CodeSubmissionPolicy ) for more details. 805
•While we encourage the release of code and data, we understand that this might not be 806
possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not 807
including code, unless this is central to the contribution (e.g., for a new open-source 808
benchmark). 809
•The instructions should contain the exact command and environment needed to run to 810
reproduce the results. See the NeurIPS code and data submission guidelines ( https: 811
//nips.cc/public/guides/CodeSubmissionPolicy ) for more details. 812
•The authors should provide instructions on data access and preparation, including how 813
to access the raw data, preprocessed data, intermediate data, and generated data, etc. 814
•The authors should provide scripts to reproduce all experimental results for the new 815
proposed method and baselines. If only a subset of experiments are reproducible, they 816
should state which ones are omitted from the script and why. 817
•At submission time, to preserve anonymity, the authors should release anonymized 818
versions (if applicable). 819
•Providing as much information as possible in supplemental material (appended to the 820
paper) is recommended, but including URLs to data and code is permitted. 821
6.Experimental Setting/Details 822
Question: Does the paper specify all the training and test details (e.g., data splits, hyper- 823
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the 824
results? 825
Answer: [Yes] 826
Justification: The experimental setup is described in as much detail as the space allows in 827
Section 6. The full setup is described in Appendix F. 828
Guidelines: 829
• The answer NA means that the paper does not include experiments. 830
•The experimental setting should be presented in the core of the paper to a level of detail 831
that is necessary to appreciate the results and make sense of them. 832
•The full details can be provided either with the code, in appendix, or as supplemental 833
material. 834
7.Experiment Statistical Significance 835
Question: Does the paper report error bars suitably and correctly defined or other appropriate 836
information about the statistical significance of the experiments? 837
Answer: [Yes] 838
Justification: Sample standard deviations for all experiments are reported in Appendix G. 839
Guidelines: 840
• The answer NA means that the paper does not include experiments. 841
•The authors should answer "Yes" if the results are accompanied by error bars, confi- 842
dence intervals, or statistical significance tests, at least for the experiments that support 843
the main claims of the paper. 844
25•The factors of variability that the error bars are capturing should be clearly stated (for 845
example, train/test split, initialization, random drawing of some parameter, or overall 846
run with given experimental conditions). 847
•The method for calculating the error bars should be explained (closed form formula, 848
call to a library function, bootstrap, etc.) 849
• The assumptions made should be given (e.g., Normally distributed errors). 850
•It should be clear whether the error bar is the standard deviation or the standard error 851
of the mean. 852
•It is OK to report 1-sigma error bars, but one should state it. The authors should 853
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis 854
of Normality of errors is not verified. 855
•For asymmetric distributions, the authors should be careful not to show in tables or 856
figures symmetric error bars that would yield results that are out of range (e.g. negative 857
error rates). 858
•If error bars are reported in tables or plots, The authors should explain in the text how 859
they were calculated and reference the corresponding figures or tables in the text. 860
8.Experiments Compute Resources 861
Question: For each experiment, does the paper provide sufficient information on the com- 862
puter resources (type of compute workers, memory, time of execution) needed to reproduce 863
the experiments? 864
Answer: [Yes] 865
Justification: This information is provided in Appendix G. 866
Guidelines: 867
• The answer NA means that the paper does not include experiments. 868
•The paper should indicate the type of compute workers CPU or GPU, internal cluster, 869
or cloud provider, including relevant memory and storage. 870
•The paper should provide the amount of compute required for each of the individual 871
experimental runs as well as estimate the total compute. 872
•The paper should disclose whether the full research project required more compute 873
than the experiments reported in the paper (e.g., preliminary or failed experiments that 874
didn’t make it into the paper). 875
9.Code Of Ethics 876
Question: Does the research conducted in the paper conform, in every respect, with the 877
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines ? 878
Answer: [Yes] 879
Justification: As far as we are aware there are no violations of the Code of Ethics. 880
Guidelines: 881
•The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. 882
•If the authors answer No, they should explain the special circumstances that require a 883
deviation from the Code of Ethics. 884
•The authors should make sure to preserve anonymity (e.g., if there is a special consid- 885
eration due to laws or regulations in their jurisdiction). 886
10.Broader Impacts 887
Question: Does the paper discuss both potential positive societal impacts and negative 888
societal impacts of the work performed? 889
Answer: [Yes] 890
Justification: We discuss the main potential misuse of our work in Section 7. 891
Guidelines: 892
• The answer NA means that there is no societal impact of the work performed. 893
•If the authors answer NA or No, they should explain why their work has no societal 894
impact or why the paper does not address societal impact. 895
26•Examples of negative societal impacts include potential malicious or unintended uses 896
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations 897
(e.g., deployment of technologies that could make decisions that unfairly impact specific 898
groups), privacy considerations, and security considerations. 899
•The conference expects that many papers will be foundational research and not tied 900
to particular applications, let alone deployments. However, if there is a direct path to 901
any negative applications, the authors should point it out. For example, it is legitimate 902
to point out that an improvement in the quality of generative models could be used to 903
generate deepfakes for disinformation. On the other hand, it is not needed to point out 904
that a generic algorithm for optimizing neural networks could enable people to train 905
models that generate Deepfakes faster. 906
•The authors should consider possible harms that could arise when the technology is 907
being used as intended and functioning correctly, harms that could arise when the 908
technology is being used as intended but gives incorrect results, and harms following 909
from (intentional or unintentional) misuse of the technology. 910
•If there are negative societal impacts, the authors could also discuss possible mitigation 911
strategies (e.g., gated release of models, providing defenses in addition to attacks, 912
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from 913
feedback over time, improving the efficiency and accessibility of ML). 914
11.Safeguards 915
Question: Does the paper describe safeguards that have been put in place for responsible 916
release of data or models that have a high risk for misuse (e.g., pretrained language models, 917
image generators, or scraped datasets)? 918
Answer: [NA] 919
Justification: We do not believe our proposed models have a high risk of misuse but will 920
nonetheless highlight the potential risks in the documentation when we release the code. 921
Guidelines: 922
• The answer NA means that the paper poses no such risks. 923
•Released models that have a high risk for misuse or dual-use should be released with 924
necessary safeguards to allow for controlled use of the model, for example by requiring 925
that users adhere to usage guidelines or restrictions to access the model or implementing 926
safety filters. 927
•Datasets that have been scraped from the Internet could pose safety risks. The authors 928
should describe how they avoided releasing unsafe images. 929
•We recognize that providing effective safeguards is challenging, and many papers do 930
not require this, but we encourage authors to take this into account and make a best 931
faith effort. 932
12.Licenses for existing assets 933
Question: Are the creators or original owners of assets (e.g., code, data, models), used in 934
the paper, properly credited and are the license and terms of use explicitly mentioned and 935
properly respected? 936
Answer: [Yes] 937
Justification: As far as we are aware we cite all the sources of the data used in our experiments 938
as well the main software packages used. 939
Guidelines: 940
• The answer NA means that the paper does not use existing assets. 941
• The authors should cite the original paper that produced the code package or dataset. 942
•The authors should state which version of the asset is used and, if possible, include a 943
URL. 944
• The name of the license (e.g., CC-BY 4.0) should be included for each asset. 945
•For scraped data from a particular source (e.g., website), the copyright and terms of 946
service of that source should be provided. 947
27•If assets are released, the license, copyright information, and terms of use in the 948
package should be provided. For popular datasets, paperswithcode.com/datasets 949
has curated licenses for some datasets. Their licensing guide can help determine the 950
license of a dataset. 951
•For existing datasets that are re-packaged, both the original license and the license of 952
the derived asset (if it has changed) should be provided. 953
•If this information is not available online, the authors are encouraged to reach out to 954
the asset’s creators. 955
13.New Assets 956
Question: Are new assets introduced in the paper well documented and is the documentation 957
provided alongside the assets? 958
Answer: [NA] 959
Justification: We don’t release any new assets at the time of submission. We plan to release 960
the code later and will fully document it. 961
Guidelines: 962
• The answer NA means that the paper does not release new assets. 963
•Researchers should communicate the details of the dataset/code/model as part of their 964
submissions via structured templates. This includes details about training, license, 965
limitations, etc. 966
•The paper should discuss whether and how consent was obtained from people whose 967
asset is used. 968
•At submission time, remember to anonymize your assets (if applicable). You can either 969
create an anonymized URL or include an anonymized zip file. 970
14.Crowdsourcing and Research with Human Subjects 971
Question: For crowdsourcing experiments and research with human subjects, does the paper 972
include the full text of instructions given to participants and screenshots, if applicable, as 973
well as details about compensation (if any)? 974
Answer: [NA] 975
Justification: We don’t conduct any experiments involving human subjects. 976
Guidelines: 977
•The answer NA means that the paper does not involve crowdsourcing nor research with 978
human subjects. 979
•Including this information in the supplemental material is fine, but if the main contribu- 980
tion of the paper involves human subjects, then as much detail as possible should be 981
included in the main paper. 982
•According to the NeurIPS Code of Ethics, workers involved in data collection, curation, 983
or other labor should be paid at least the minimum wage in the country of the data 984
collector. 985
15.Institutional Review Board (IRB) Approvals or Equivalent for Research with Human 986
Subjects 987
Question: Does the paper describe potential risks incurred by study participants, whether 988
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) 989
approvals (or an equivalent approval/review based on the requirements of your country or 990
institution) were obtained? 991
Answer: [NA] 992
Justification: We don’t conduct any experiments involving human subjects. 993
Guidelines: 994
•The answer NA means that the paper does not involve crowdsourcing nor research with 995
human subjects. 996
•Depending on the country in which research is conducted, IRB approval (or equivalent) 997
may be required for any human subjects research. If you obtained IRB approval, you 998
should clearly state this in the paper. 999
28•We recognize that the procedures for this may vary significantly between institutions 1000
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the 1001
guidelines for their institution. 1002
•For initial submissions, do not include any information that would break anonymity (if 1003
applicable), such as the institution conducting the review. 1004
29