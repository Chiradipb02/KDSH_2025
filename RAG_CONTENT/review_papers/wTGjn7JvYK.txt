Published in Transactions on Machine Learning Research (04/2024)
On the Optimization and Generalization of
Multi-head Attention
Puneesh Deora∗puneeshdeora@ece.ubc.ca
University of British Columbia
Rouzbeh Ghaderi∗rghaderi@ece.ubc.ca
University of British Columbia
Hossein Taheri∗hossein@ucsb.edu
University of California, Santa Barbara
Christos Thrampoulidis cthrampo@ece.ubc.ca
University of British Columbia
Reviewed on OpenReview: https: // openreview. net/ forum? id= wTGjn7JvYK
Abstract
The training and generalization dynamics of the Transformer’s core mechanism, namely the
Attention mechanism, remain under-explored. Besides, existing analyses primarily focus
on single-head attention. Inspired by the demonstrated benefits of overparameterization
when training fully-connected networks, we investigate the potential optimization and
generalization advantages of using multiple attention heads. Towards this goal, we derive
convergence and generalization guarantees for gradient-descent training of a single-layer
multi-head self-attention model, under a suitable realizability condition on the data. We then
establish primitive conditions on the initialization that ensure realizability holds. Finally, we
demonstrate that these conditions are satisfied for a simple tokenized-mixture model. We
expect the analysis can be extended to various data-model and architecture variations.
1 Introduction
Transformers have emerged as a promising paradigm in deep learning, primarily attributable to their
distinctive self-attention mechanism. Motivated by the model’s state-of-the-art performance in natural
language processing (Devlin et al., 2019; Brown et al., 2020; Raffel et al., 2020) and computer vision
(Dosovitskiy et al., 2021; Radford et al., 2021; Touvron et al., 2021), the theoretical study of the attention
mechanism has seen a notable surge in interest recently. Numerous studies have already explored the
expressivity of Attention, e.g. (Baldi & Vershynin, 2022; Dong et al., 2021; Yun et al., 2020a;b; Sanford et al.,
2023; Bietti et al., 2023), and initial findings regarding memory capacity have been very recently studied
in (Baldi & Vershynin, 2022; Dong et al., 2021; Yun et al., 2020a;b; Mahdavi et al., 2023). In an attempt
to comprehend optimization aspects of training attention models, Sahiner et al. (2022); Ergen et al. (2022)
have investigated convex-relaxations, while Tarzanagh et al. (2023a) investigates the model’s implicit bias.
Additionally, Edelman et al. (2021) have presented capacity and Rademacher complexity-based generalization
bounds for Self-Attention. However, the exploration of the finite-time optimization and generalization
dynamics of gradient-descent (GD) for training attention models largely remains an open question.
Recent contributions in this direction, which serve as motivation for our work, include the studies by Jelassi
et al. (2022); Li et al. (2023a); Oymak et al. (2023). These works concentrate on single-layer attention models
with asingle attention head . Furthermore, despite necessary simplifying assumptions made for the data, the
∗These authors contributed equally. Alphabetical ordering used.
1Published in Transactions on Machine Learning Research (04/2024)
analyses are rather intricate and appear highly specialized on the individual attention and data model. These
direct and highly specialized analyses present certain challenges. First, it remains uncertain whether they
can be encompassed within a broader framework that can potentially be extended to more complex attention
architectures and diverse data models. Second, they appear disconnected from existing frameworks that
have been flourishing in recent years for conventional fully-connected and convolutional neural networks e.g.,
(Jacot et al., 2018; Ji & Telgarsky, 2020; Richards & Rabbat, 2021; Liu et al., 2020; Taheri & Thrampoulidis,
2023). Consequently, it is also unclear how the introduction of attention alters the analysis landscape.
In this work, we study the optimization and generalization properties of multi-head attention mechanism
trained by gradient methods. Our approach specifically leverages the use of multiple attention heads . Despite
the operational differences between attention heads in an attention model and hidden nodes in an MLP,
we demonstrate, from an analysis perspective, that this parallelism enables the exploitation of frameworks
developed for the latter to study the former. Particularly for the generalization analysis, we leverage recent
advancements in the application of the algorithmic-stability framework to overparameterized MLPs (Richards
& Kuzborskij, 2021; Taheri & Thrampoulidis, 2023).
Contributions. We study training and generalization of gradient descent optimization for a multi-head
attention (MHA) layer with Hheads in a binary classification setting. For this setting, detailed in Section 2,
we analyze training with logistic loss both the attention weights (parameterizing the softmax logits), as well
as, the linear decoder that turns output tokens to label prediction.
In Section 3, we characterize key properties of the empirical loss ̂L, specifically establishing that it is
self-bounded and satisfies a key self-bounded weak-convexity property, i.e. λmin(∇2̂L(θ))≳−κ√
ĤL(θ)for
a parameter κthat depends only mildly on the parameter vector θ. Establishing these properties (and
also quantifying κ) involves carefully computing and bounding the gradient and Hessian of the MHA layer,
calculations that can be useful beyond the context of our paper.
In Sections 4.1-4.2, we present our training and generalization bounds in their most general form. The
bounds are given in terms of the empirical loss ̂L(θ)and the distance ∥θ−θ0∥to initialization θ0of an
appropriately chosen target vector θ. The distance to initialization also controls the minimum number of
headsH≳∥θ−θ0∥6required for the bounds to hold. The choice of an appropriate parameter θthat makes
the bounds tight is generically specific to the data setting and the chosen initialization. To guide such a
choice, in Section 4.3, we formalize primitive and straightforward-to-check conditions on the initialization θ0
that ensure it is possible to find an appropriate θ. In short, provided the model output at initialization is
logarithmic on the train-set size nand the data are separable with respect to the neural-tangent kernel (NTK)
features of the MHA model with constant margin γ, then Corollary 2 shows that with step-size η=̃O(1)and
Θ(n)gradient descent steps, the train loss and generalization gap is bounded by ̃O(1/n)provided only a
polylogarithmic number of heads H=Ω(log6(n)). We remark that the aforementioned NTK separability
assumption, although related to, differs from the standard NTK analysis. Besides, while this assumption is
sufficient to apply our general bounds, it is not a necessary condition.
In Section 5, we investigate a tokenized mixture data model with label-(ir)relevant tokens. We show that after
one randomized gradient step from zero initialization, the NTK features of the MHA model separate the data
with margin γ⋆. Thus, applying our general analysis from Section 4.1, we establish training and generalization
bounds as described above, for a logarithmic number of heads. Towards assessing the optimality of these
bounds, we demonstrate that MHA is expressive enough to achieve margin γattnthat is superior to γ⋆. The
mechanism to reach γattninvolves selecting key-query weights of sufficiently large norm, which saturates the
softmax nonlinearity by suppressing label-irrelevant tokens. We identify the large-norm requirement as a
potential bottleneck in selecting those weights as target parameters in our theory framework and discuss
open questions regarding extending the analytical framework into this specific regime.
The remaining parts are organised as follows. Proof sketches of our main training/generalization bounds are
given in Section 6. The paper concludes in Section 7 with remarks on our findings’ implications and open
questions. Detailed proofs are in the appendix, where we also present synthetic numerical experiments.
Related work. We give a brief overview of the most relevant works on understanding optimiza-
tion/generalization of self-Attention or its variants. Please see Section H for more detailed exposition.
2Published in Transactions on Machine Learning Research (04/2024)
Oymak et al. (2023) diverges from traditional self-Attention by focusing on a variant called prompt-Attention,
aiming to gain understanding of prompt-tuning. Jelassi et al. (2022) shed light on how ViTs learn spatially
localized patterns using gradient-based methods. Li et al. (2023a) provides sample complexity bounds for
achieving zero generalization error on training three-layer ViTs for classification tasks for a similar tokenized
mixture data model as ours. Contemporaneous work Tian et al. (2023) presents SGD-dynamics of single-layer
attention for next-token prediction by re-parameterizing the original problem in terms of the softmax and
classification logit matrices, while Tarzanagh et al. (2023b;a) study the implicit bias of training the softmax
weightsWwith a fixed decoder U. All these works focus on a single attention head; instead, we leverage the
use of multiple heads to establish connections to the literature on GD training of overparameterized MLPs.
Conceptually, Hron et al. (2020) drew similar connections, linking multi-head attention to a Gaussian process
in the limit as the number of heads approaches infinity. In contrast, we study the more practical regime of
finite heads and obtain finite-time optimization and generalization bounds.
Among the extensive studies on training/generalization of overparameterized MLPs, our work closely aligns
with Nitanda et al. (2019); Ji & Telgarsky (2020); Cao & Gu (2019); Chen et al. (2020); Telgarsky (2022);
Taheri & Thrampoulidis (2023) focusing on classification with logistic loss. Conceptually, our findings extend
this research to attention models. The use of algorithmic-stability tools towards order-optimal generalization
bounds for overparameterized MLPs has been exploited recently by Richards & Kuzborskij (2021); Richards
& Rabbat (2021); Taheri & Thrampoulidis (2023); Lei et al. (2022). To adapt these tools to the MHA
layer, we critically utilize the smoothness of the softmax function and derive bounds on the growth of the
model’s gradient/Hessian, which establish a self-bounded weak convexity property for the empirical risk (see
Corollary 1). Our approach also involves training both the classifier and attention weights, necessitating
several technical adjustments detailed in Section 6 and Appendix B.1.
2 Preliminaries
Notation.φ(⋅)∶RT→RTdenotes the softmax map and φ′(v)∶=∇φ(v)=diag(φ(v))−φ(v)φ(v)⊺its
gradient atv∈RT. Fort∈[T],φt(v)is thet-th entry ofφ(v)∈RT. ForA∈Rn×m,Ai,∶is itsi-th row
andA∶,jis itsj-th column. Recall the induced matrix norm ∥A∥p,q=max∥v∥p=1∥Av∥qand particularly the
following: ∥A∥2,∞=maxi∈[n]∥Ai,∶∥,∥A∥1,2=maxj∈[m]∥A∶,j∥,and∥A∥1,∞=maxj∈[m]∥A∶,j∥∞. For simplicity,
∥A∥,∥v∥denote Euclidean norms and λmin(A)the minimum eigenvalue. We let a∧b=min{a,b}and
a∨b=max{a,b}.concatdenotes vector concatenation. All logarithms are natural logarithms (base e). We
represent the line segment between w1,w2∈Rd′as[w1,w2]={w∶w=αw1+(1−α)w2,α∈[0,1]}. Finally,
to simplify the exposition we use “ ≳” or “≲” notation to hide absolute constants. We also occasionally use
standard notations O,Ωand̃O,̃Ωto hide poly-log factors. Unless otherwise stated these order-wise notations
are with respect to the training-set size n. Whenever used, exact constants are specified in the appendix.
Single-head Self-attention. A single-layer self-attention head ATTN∶RT×d→RT×dwith context size T
and dimension dparameterized by key, query and value matrices WQ,WK∈Rd×dh,WV∈Rd×dvis given by:
ATTN(X;WQ,WK,WV)∶=φ(XWQW⊺
KX⊺)XWV.
Here,X=[x1,x2,...,xT]⊺∈RT×dis the input token matrix and φ(XWQW⊺
KX⊺)∈RT×Tis the attention
matrix. (Softmax applied to a matrix acts row-wise.) To turn the Attention output in a prediction label, we
compose ATTN with a linear projection head (aka decoder). Thus, the model’s output is∗
Φ(X;W,U)∶=⟨U,φ(XWX⊺)X⟩. (1)
Note that we absorb the value weight matrix WVinto the projector U=[u1,...,uT]⊺∈RT×d. Also, we
parameterize throughout the key-query product matrix as W∶=WQW⊺
K.
Multi-head Self-attention. Our focus is on the multi-head attention (MHA) model with Hheads:
∑
h∈[H]ATTN(X;WQh,WKh,WVh)WOh,
∗While we focus on (i) Full-projection: trainable matrix U∈RT×d, our results also apply to (ii) Pooling: U=u1⊺
Twith trainable
u∈Rd, and (iii) Last-token output: U=[0d×(T−1)u]⊺with trainable u∈Rd.
3Published in Transactions on Machine Learning Research (04/2024)
for output matrices WOh∈Rdv×d. AbsorbingWVhWOhinto a projection layer (similar to the single-head
attention) and parameterizing Wh∶=WQhWK⊺
hwe arrive at the following MHA model:
̃Φ(X;̃W,̃U)∶=1√
H∑h∈[H]Φ(X;Wh,Uh)=1√
H∑h∈[H]⟨Uh,φ(XWhX⊺)X⟩, (2)
parameterized by ̃W∶=concat({Wh}h∈[H])and̃U∶=concat({Uh}h∈[H]).The1/√
Hscaling is analogous to
the normalization in MLP literature e.g. (Du et al., 2019; Ji & Telgarsky, 2021; Richards & Kuzborskij, 2021),
ensuring the model variance is of constant order when Uhis initialized OH(1). Note that these relaxations
sacrifice some generality since it is common practice to set dhanddvsuch thatdv=d/H<d, thus imposing
low-rank restrictions on matrices WQhWK⊺
h,WVhWOh. We defer a treatment of these to future work.
Throughout, we will use θh∶=concat(Uh,Wh)∈RdT+d2,to denote the trainable parameters of the h-attention
head and ̃θ∶=concat({θh}h∈[H])∈RH(dT+d2)for the trainable parameters of the overall model. More generally,
we use the convention of applying “ ̃⋅” notation for quantities relating to the multi-head model. Finally,
with some slight abuse of notation, we define: ∥̃θ∥2,∞∶=maxh∈[H]∥θh∥.
Training. Given training set (Xi,yi)i∈[n], withnIIDsamples, we minimize logistic-loss based empirical risk
̂L(̃θ)∶=1
n∑
i∈[n]ℓ(yĩΦ(Xi;̃θ))∶=1
n∑
i∈[n]log(1+e−yĩΦ(Xi;̃θ)).
Our analysis extends to any convex, smooth, Lipschitz and self-bounded loss.†The empirical risk is minimized
as an approximation of the test loss defined asL(̃θ)∶=E(X,y)[ℓ(ỹΦ(X;̃θ))].We consider standard gradient-
descent (GD) applied to empirical risk ̂L. Formally, initialized at ̃θ(0)and equipped with step-size η>0, at
each iteration k≥0, GD performs the following update:
̃θ(k+1)=̃θ(k)−η∇̂L(̃θ(k)).
3 Gradient and Hessian bounds of soft-max attention
This section establishes bounds on the gradient and Hessian of the logistic empirical risk ̂L(.)evaluated on the
multi-headattentionmodel. Todothis, wefirstderiveboundsontheEuclideannormandspectral-normforthe
gradient and Hessian of the self-attention model. In order to simplify notations, we state here the bounds for
the single-head model (see App. A.1 for multi-head model): Φ(X;θ)∶=Φ(X;W,U)=⟨U,φ(XWX⊺)X⟩.
Lemma 1 (Gradient/Hessian formulas) .For alla∈RT,b,c∈Rdthe model’s gradients satisfy:
● ∇UΦ(X;θ)=φ(XWX⊺)X,and∇WΦ(X;θ)=T
∑
t=1xtu⊺
tX⊺φ′(XW⊺xt)X.
● ∇W⟨a,∇UΦ(X;θ)b⟩=T
∑
t=1xtatb⊺X⊺φ′(XW⊺xt)X,and
∇W⟨c,∇WΦ(X;θ)b⟩=T
∑
t=1(c⊺xt)xtd⊺φ′(XW⊺xt)X
where d∶=diag(Xb)Xut−Xutb⊺X⊺φ(XW⊺xt)−Xbu⊺
tX⊺φ(XW⊺xt).
These calculations imply the following useful bounds.
Proposition 1 (Model Gradient/Hessian bounds) .The Euclidean norm of the gradient and the spectral
norm of the Hessian of the single-head Attention model (1)are bounded as follows:
●∥∇θΦ(X;θ)∥≤2∥X∥2
2,∞T
∑
t=1∥Xut∥∞+√
T∥X∥2,∞.
●∥∇2
θΦ(X;θ)∥≤6d∥X∥2
2,∞∥X∥2
1,∞T
∑
t=1∥Xut∥∞+2√
Td∥X∥2
2,∞∥X∥1,∞.
†A function ℓ∶R→Ris self-bounded if ∃C>0such that ∣ℓ′(t)∣≤Cℓ(t).
4Published in Transactions on Machine Learning Research (04/2024)
Next, we focus on the empirical loss ̂L. To derive bounds on its gradient and Hessian, we leverage the model’s
bounds from Proposition 1 and the fact that logistic loss is self-bounded, i.e., ∣ℓ′(t)∣≤ℓ(t). To provide
concrete statements, we introduce first a mild boundedness assumption.
Assumption 1 (Bounded data) .Data(X,y)∈RT×d×Rsatisfy the following conditions almost surely:
y∈{±1}, and for some R≥1, it holds for all t∈[T]that∥xt∥≤R.
Corollary 1 (Loss properties) .Under Assumption 1, the objective’s gradient and Hessian satisfy the bounds:‡
(1)∥∇̂L(̃θ)∥≤β1(̃θ)̂L(̃θ), β 1(̃θ)∶=√
TR(2R2∥̃θ∥2,∞+1).
(2)∥∇2̂L(̃θ)∥≤β2(̃θ), β 2(̃θ)∶=1√
Hβ3(̃θ)+1
4β1(̃θ)2.
(3)λmin(∇2̂L(̃θ))≥−β3(̃θ)√
ĤL(̃θ)β3(̃θ)∶=2√
TdR3(3√
dR2∥̃θ∥2,∞+1).
The loss properties above are crucial for the training and generalization analysis. Property (1) establishes
self-boundedness of the empirical loss, which is used to analyze stability of GD updates for generalization.
Property (2) is used to establish descent of gradient updates for appropriate choice of step-size η. Note that
the smoothness upper bound is ̃θ-dependent, hence to show descent we need to also guarantee boundedness
of the updates. Finally, property (3) establishes a self-bounded weak-convexity property of the loss, which is
crucial to both the training and generalization analysis. Specifically, as the number of heads Hincreases, the
minimum eigenvalue becomes less negative, indicating an approach towards convex-like behavior.
4 Main results
In this section, we present our training and generalization bounds for multi-head attention.
4.1 Training bounds
We state our main result on train loss convergence in the following theorem. See App. B for exact constants
and the detailed proofs.
Theorem 1 (Training loss) .Fix iteration horizon K≥1and any ̃θ∈RH(dT+d2)andHsatisfying
√
H≳dT1/2R5∥̃θ∥2,∞∥̃θ−̃θ(0)∥3. (3)
Fix step-size η≤1∧1/ρ(̃θ)∧∥̃θ−̃θ(0)∥2
K̂L(̃θ)∧∥̃θ−̃θ(0)∥2
̂L(̃θ(0)),withρ(̃θ)≲d3/2T3/2R13∥̃θ∥2
2,∞∥̃θ−̃θ(0)∥2.Then, the
following bounds hold for the training loss and the weights’ norm at iteration Kof GD:
̂L(̃θ(K))≤1
KK
∑
k=1̂L(̃θk)≤2̂L(̃θ)+5∥̃θ−̃θ(0)∥2
4ηK, (4)
∥̃θ(K)−̃θ(0)∥≤4∥̃θ−̃θ(0)∥.
Yielding a concrete train loss bound requires an appropriate set of target parameters ̃θin the sense of
minimizing the bound in (4). Hence, ̃θshould simultaneously attain small loss ( ̂L(̃θ)) and distance to
initialization ( ∥̃θ−̃θ(0)∥). This desiderata is formalized in Assumption 2 below. The distance to initialization,
as well as ∥̃θ∥2,∞, determine how many heads are required for our bounds to hold. Also, in view of the bound
in(4), it is reasonable that an appropriate choice for ̃θattains ̂L(̃θ)of same order as ∥̃θ−̃θ(0)∥2/K.Hence,
the theorem’s restriction on the step-size is governed by the inverse local-smoothness of the loss: η≲1/ρ(̃θ).
4.2 Generalization bounds
Next we bound the expected generalization gap. Expectations are with respect to (w.r.t) randomness of the
train set. See App. C for the detailed proof, which is based on algorithmic-stability.
‡In all the bounds in this paper involving ∥̃θ∥2,∞, it is possible to substitute this term with maxh∈[H]∥Uh∥. However, for the
sake of notation simplicity, we opt for a slightly looser bound maxh∈[H]∥Uh∥≤maxh∈[H]∥θh∥=∶∥̃θ∥2,∞.
5Published in Transactions on Machine Learning Research (04/2024)
Theorem 2 (Generalization loss) .Fix anyK≥1, anỹθandHsatisfying (3), and any step-size ηsatisfying
the conditions of Thm. 1. Then the expected generalization gap at iteration Ksatisfies,
E[L(̃θ(K))−̂L(̃θ(K))]≤4
nE[2K̂L(̃θ)+9∥̃θ−̃θ(0)∥2
4η]. (5)
The condition on the number of heads is same up to constants to the corresponding condition in Theorem 1.
Also, the generalization-gap bound translates to test-loss bound by combining with Thm. 1. Finally, similar
to Thm. 1, we can get concrete bounds under the realizability assumption; see Cor. 4 in App. C.2. For the
generalization analysis, we require that the realizability assumption holds almost surely over all training sets
sampled from the data distribution.
The bounds on optimization and generalization are up to constants same as analogous bounds for logistic
regression (Soudry et al., 2018; Ji & Telgarsky, 2018; Shamir, 2021). Yet, for these bounds to be valid, we
require sufficiently large number of heads as well as the existence of an appropriate set of target parameters
̃θ, as stated in the conditions of theorem. Namely, these conditions are related to the realizability condition,
which guarantees small training error near initialization. The next assumption formalizes these conditions.
Assumption 2 (Realizability) .There exist non-increasing functions g∶R+→R+andg0∶R+→R+such
that∀ϵ>0, there exists model parameters ̃θ(ϵ)∈RH(dT+d2)for which: (i) the empirical loss over ndata
samples satisfies ̂L(̃θ(ε))≤ε, (ii)∥̃θ(ε)−̃θ(0)∥≤g0(ε), and, (iii) ∥̃θ(ε)∥2,∞≤g(ε).
With this assumption, we can specialize the result of Thms. above to specific data settings; see Cor. 3 and 4
in App. B.5 and C.2. In the next section we will further show how the realizability assumption is satisfied.
4.3 Primitive conditions for checking realizability
Here, weintroduceasetofmoreprimitiveandstraightforward-to-checkconditionsonthedataandinitialization
that ensure the realizability Assumption 2 holds.
Definition 1 (Good initialization) .We say ̃θ(0)=concat(θ(0)
1,...,θ(0)
H)is agoodinitialization with respect
to training data (Xi,yi)i∈[n]provided the following three properties hold.
P1.Parameter L2,∞-bound: There exists parameter B2≥1such that∀h∈[H]it holds ∥θ(0)
h∥2≤B2.
P2.Model bound: There exists parameter BΦ≥1such that∀i∈[n]it holds ∣̃Φ(Xi;̃θ(0))∣≤BΦ.
P3.NTK separability: There exists ̃θ⋆∈RH(dT+d2)andγ>0such that ∥̃θ⋆∥=√
2and∀i∈[n], it holds
yi⟨∇̃Φ(Xi;̃θ(0)),̃θ⋆⟩≥γ.
Prop. 7 in the appendix shows that starting from a goodinitialization we can always find ̃θ(ϵ)satisfying the
realizability Assumption 2 provided large enough number of heads. Thus, given goodinitialization, we can
immediately apply Theorems 1 and 2 to get the following concrete bounds.
Corollary 2 (General bounds under goodinitialization) .Suppose goodinitialization ̃θ(0)and let
√
H≳dT1/2R5B2
2(g0(1/K))3,whereg0(1
K)=2BΦ+log(K)
γ.
Further fix step-size η≤1∧1/ρ(K)∧4B2
Φ
γ2log(1+eBΦ)withρ(K)≳d3/2T3/2R13g0(1
K)4.Then, it holds that
̂L(̃θ(K))≤2
K+5(2BΦ+log(K))2
4γ2ηK,and E[L(̃θ(K))−̂L(̃θ(K))]≤17(2BΦ+log(K))2
γ2ηn.
Consider training loss after KGD steps: Assuming BΦ=̃OK(1)andγ=OK(1), then choosing η=̃OK(1),
the corollary guarantees train loss is ̃OK(1
K)provided polylogarithmic number of heads H=Ω(log6(K)).
Moreover, after K≈nGD steps the expected test loss is O(1/n).
6Published in Transactions on Machine Learning Research (04/2024)
Remark 1. The last two conditions (P2 and P3) for goodinitialization are similar to the conditions needed
in (Taheri & Thrampoulidis, 2023; Ji & Telgarsky, 2020; Nitanda et al., 2019) for analysis of two-layer MLPs.
Compared to (Ji & Telgarsky, 2020; Nitanda et al., 2019) which assume random Gaussian initialization
̃θ(0), and similar to (Taheri & Thrampoulidis, 2023) the NTK separability assumption (P3) can potentially
accommodate deterministic ̃θ(0). Condition (P1) appears because we allow training both layers of the model.
Specifically the L2,∞norm originates from the Hessian bounds in Corollary 1.
5 Application to tokenized-mixture model
We now demonstrate through an example how our results apply to specific data models.
Data model: An example. ConsiderM+2distinct patterns {µ+,µ−,ν1,ν2,...,νM}, where discriminative
patternsµ±correspond to labels y=±1. The tokens are split into (i) a label-relevant set ( R) and (ii) a
label-irrelevant set ( Rc∶=[T]/R). Conditioned on the label and R, the tokensxt,t∈[T]are IID as follows
xt∣y=⎧⎪⎪⎨⎪⎪⎩µy,t∈R
νjt+zt,jt∼Unif(1,...,M)andt∈Rc,(DM1)
whereztare noise vectors. Let Ddenote the joint distribution induced by the described (X,y)pairs.
Assumption 3. The labels are equi-probable and we further assume the following:
●Orthogonal, equal-energy means: All patterns are orthogonal to each other, i.e. µ+⊥µ−⊥νℓ⊥
νℓ′,∀ℓ,ℓ′∈[M].Also, for all y∈{±1},ℓ∈[M]that∥µy∥=∥νℓ∥=S, whereSdenotes the signal strength .
●Sparsity level: The number of label-relevant tokens is ∣R∣=ζT; for sparsity level ζ∈(0,1).
●Noise distribution: The noise tokens ztare sampled from a distribution Dz, such that it holds almost
surely forzt∼Dzthat∣⟨zt,µy⟩∣≤Zµ, y∈{±1}and∣⟨zt,νℓ⟩∣≤Zν/M,∀ℓ∈[M].Moreover, ∥zt∥≤Z.Overall,
Assumption 1 is satisfied with R=√
S2+Z2+2Zν/M.
The above assumptions can be relaxed, but without contributing new insights. We have chosen to present a
model that is representative and transparent in its analysis.
5.1 Finding a good initialization
To apply the general Corollary 2 to the specific data model DM1, it suffices to find goodinitialization. While
we cannot directly show that ̃θ(0)=0isgood, we can show this is the case for first step of gradient descent
̃θ(1). Thus, we consider training in two phases as follows.
First phase: One step of GD as initialization. We usen1training samples to update the model
parameters by running one-step of gradient descent starting from zero initialization. Specifically,
(U(1)
h,W(1)
h)=θ(1)
h=θ(0)
h−αh√
H⋅∇θĥLn1(θ(0)
h),whereθ(0)
h=0∀h∈[H].
Here,αhdenotes the step-size for head h∈[H]and the scaling by√
Hguarantees the update of each head
isO(1). The lemma below shows that at the end of this phase, we have ∥U(1)
h−ζαh
21Tu⊺
⋆∥F=O(1/√n1),
whereu⋆is the oracle classifier u⋆=µ+−µ−. On the other hand, the attention weight-matrix does notget
updated; the interesting aspect of the training lies in the second phase, which involves updating W.
Lemma 2 (First phase) .After the first-gradient step as described above, we have U(1)
h=αh1T(ζ
2u⊺
⋆+p⊺)
andW(1)
h=0. where with probability at least 1−δ∈(0,1)over the randomness of labels there exists positive
universal constant C>0such that
∥p∥≤C(2S+Z)(√
d
n1+√
log(1/δ)
n1)=∶P. (6)
7Published in Transactions on Machine Learning Research (04/2024)
Second phase: GD with constant step size. During the second phase, Kgradient steps are performed
onnnew samples (distinct from those used in the first phase). Concretely, ̃θ(k+1)=̃θ(k)−η⋅∇̃θ̂Ln(̃θ(k)), k=
1,...,K, with̃θ(1)=concat({θ(1)
h}h∈[H])the step obtained by the first-phase update and ηthe step-size of
the second phase. In order to analyze the second phase, during which both ̃Wand̃Uget updated, we employ
the general results of Section 4. To do so, we show that ̃θ(1)serves as goodinitialization as per Definition 1.
Proposition 2. Consider the first-phase iterate {θ(1)
h}h∈[H]and condition on the event ∥p∥≤P(depending
only on the data randomness in the first phase) of Lemma 2. Suppose the step-size of the first phase is chosen
IIDαh∼Unif(±1),h∈[H]. Then, the initialization ̃θ(1)=concat(θ(1)
1,...,θ(1)
H)isgoodwith respect to data
sampled from DM1 and satisfying Assumption 3. Specifically, the three desired properties hold as follows.
●Almost surely, P1holds withB2=√
T(S+P).
●With probability 1−δ∈(0,1),P2holds withBΦ=TR(S+P)√
2 log(n/δ).
●Suppose√
H≳R4T(S+P)
γ⋆⋅√
2 log(n/δ).Then, with probability 1−δ∈(0,1),P3holds withγ=γ⋆/2where
γ⋆∶=T(1−ζ)ζ(ζS4−7¯ZS2−12¯Z2−16¯Z3
S2)
4√
2(M+1)−PT5/2(S+Z)3+S√
T(ζ−2(1−ζ)Zµ
S2)
√
2, (7)
and ¯Z∶=Zµ∨Zν. The randomness is with respect to the sampling of αh,h∈[H].
The parameter γ⋆in(7)represents the NTK margin of the model at initialization ̃θ(1). By Corollary 2, larger
margin translates to better train/generalization bounds and smaller requirements on the number of heads.
For a concrete example, suppose T∨M=O(1)andZ∨¯Z=O(S). Then, provided first-phase sample
sizen1≳S2dso thatP=O(1), it holdsγ⋆=γlin+Ω(ζ2(1−ζ)S4), whereγlin=Ω(ζS)is the margin of a
linear model for the same dataset (see App. F). Overall, applying Cor. 2 for K=nand a polylogarithmic
polylog(n)number of heads leads to ˜O(1
ηγ2
⋆n)train loss and expected generalization gap.
5.2 Proof sketch of P3: NTK separability
It is instructive to see how P3follows as it sheds light on the choice of an appropriate target parameter ̃θas
per Thms. 1 and 2. We choose
W⋆=µ+µ⊺
++µ−µ⊺
−+∑
ℓ∈[M]νℓ(µ++µ−)⊺andU⋆=1Tu⊺
⋆=1T(µ+−µ−)⊺,
and normalize parameters such that θ⋆∶=(U⋆=1
∥U⋆∥FU⋆,sign(α)W⋆=sign(α)1
∥W⋆∥FW⋆). It is easy to
see thatU⋆is the optimal classifier for the label-relevant tokens. To gain intuition on the choice of W⋆, note
thatW⋆=WK,⋆W⊺
Q,⋆, with key-query matrices chosen as WK,⋆=[µ+µ−ν1⋯νM]∈Rd×(M+2)and
WQ,⋆=[µ+µ−µ++µ−⋯µ++µ−]∈Rd×(M+2). With these choices, the relevance scores (aka softmax
logits) of relevant tokens turn out to be strictly larger compared to the irrelevant tokens. Concretely, we show
in App. D.2.3 that the t-th rowrt(X;W⋆)=XW⊺
⋆xtof the softmax-logit matrix satisfies the following:
∀t∶[rt]t′=⎧⎪⎪⎨⎪⎪⎩O(S4),t′∈R,
O(S2),t′∈Rc.(8)
Thus, under this parameter choice, softmax can attend to label-relevant tokens and supresses noisy irrelevant
tokens. In turn, this increases the signal-to-noise ratio for classification using U⋆.
We now show how to compute Eθ(1)y⟨∇θΦ(X;θ(1)),θ⋆⟩for a single head. Recall θ⋆consists ofU⋆,W⋆. First,
sinceW(1)=0, using Assumption 3, a simple calculation shows y⟨∇UΦ(X;θ(1)),U⋆⟩≥S√
T√
2(ζ−2(1−ζ)Zµ
S2).
Second, to compute Eα∼Unif(±1)y⟨∇WΦ(X;θ(1)),sign(α)W⋆⟩it follows from Lemma 1 that
∇WΦ(X;θ(1))=αζ
2∑
t∈[T]xtu⊺
⋆X⊺φ′(0)X+α∑
t∈[T]xtp⊺X⊺φ′(0)X.
8Published in Transactions on Machine Learning Research (04/2024)
Note the first term is dominant here since the second term can be controlled by making ∥p∥2small as per
Lemma2. Thus, ignoringherethe second term (seeAppendixD.2.3forfull calculation) y⟨∇WΦ(X;θ(1)),W⋆⟩
is governed by the following term:αζ
2∑t∈[T]yu⊺
⋆X⊺φ′(0)XW⊺
⋆xt=αζ
2∑t∈[T]yu⊺
⋆X⊺φ′(0)rt.Note that
φ′(0)=I−1
T1T1⊺
T. To simplify the exposition here, let us focus on the identity component and leave
treatment of the the rank-one term to the detailed proof. The corresponding term then becomes
αζ
2∑
t∈[T]∑
t′∈[T](yu⊺
⋆xt′)
⌟⟨⟨⟪rl⟫l⟩⟩⟪⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟨⟨⟪rl⟫m⟩⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟨⟨⟪rl⟫r⟩⟩⟩⟪
class. logits⋅([rt]t′)
⌟⟨⟨⟪rl⟫l⟩⟩⟪⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟨⟨⟪rl⟫m⟩⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟨⟨⟪rl⟫r⟩⟩⟩⟪
softmax logits,
which involves for each output token t, the sum of products over all tokens t′∈[T]of softmax logits (i.e.
relevant scores [rt]t′) and corresponding classification logits (i.e. yu⊺
⋆xt′). Note that by choice of u⋆and
W⋆, both the classification and softmax logits are large from label-relevant tokens, while being small for noise
tokens. Intuitively, this allows for a positive margin γ⋆as stated in Proposition 2. We defer the detailed
calculations to Appendix D.2.3.
In the appendix, we also detail how to yield the computation for the MHA, which builds on the calculations
for the single-head attention model above. In short, we simply choose multi-head parameter ̃θ⋆as̃θ⋆=
1√
Hconcat(θ⋆(θ(1)
1),...,θ⋆(θ(1)
H)).This guarantees that ∥̃θ⋆∥=√
2and maintains the multi-head NTK
margin be at least γ⋆in expectation. To complete the proof, it remains to get a high-probability version of
this bound. To do this, notice that θ(1)
hareIID, hence we can apply Hoeffding’s inequality, which finally
gives the desired bound on the NTK margin provided sufficient number of heads H, which controls the degree
of concentration when applying Hoeffding’s inequality. See Lemmas 14 and 15 for details.
5.3 Is the NTK margin optimal?
Below, we discuss the optimality of the NTK margin γ⋆. First, define set of parameters θopt∶=(Uopt,Wopt):
Uopt∶=1
S√
2TU⋆andWopt∶=1
S2√
2(M+1)W⋆, (9)
normalized so that ∥θopt∥F=√
2. Recall here the definitions of U⋆,W⋆in the section above. As we already
explained above, this choice of parameters guarantees that relevant tokens are assigned larger relevance and
classification scores compared to irrelevant ones. Specifically about W⋆, we saw in Eq. (8)that it ensures a
gap of O(S2)between relevance scores of label-relevant and label-irrelevant tokens. Thanks to this gap, it is
possible for softmax to fully attend to the label-relevant tokens by saturating the softmax. To do this, it
suffices to scale-up W⋆by an amount ∝1/S2. This is formalized in the proposition below.
Proposition 3 (Attention expressivity for tokenized mixture model) .Consider single-head attention
model. Suppose the noise level is such that Zµ=Zν≤S2/8. For any ϵ>0, consider Γϵsatisfying
Γϵ≥8√
2(M+1)
3S2log(ζ−1−1
ϵ).Then, the attention scores corresponding to weights Γϵ⋅Woptsatisfy
∀t∈[T]∶0≤1−∑
t′∈Rφt′(x⊺
tΓϵWoptXT)=∑
t′∈Rcφt′(x⊺
tΓϵWoptXT)≤ϵ. (10)
Thus, almost surely over data (X,y)generated from data model DM1 the margin of single-head attention
with parameters (Uopt,Γϵ⋅Wopt)satisfies
yΦ(X;Uopt,Γϵ⋅Wopt)≥γattn∶=γattn(ϵ)∶=√
T√
2S(S2(1−ϵ)−2ϵZµ). (11)
From Eq. (10), note that as ϵ→0andΓϵ→∞, the softmax map saturates, i.e. it approaches a hard-max
map that attends only to the label-relevant tokens ( R) and suppress the rest ( Rc). As a consequence of
this, Eq. (11)shows that the achieved margin approaches γattn∶=S√
T/√
2. Note this is independent of the
sparsity level ζ. In particular, γattn≥γ⋆≥γlinand the gap increases with decreasing sparsity. See appendix
for experiments and discussion regarding the margin achieved by GD for data model DM1.
9Published in Transactions on Machine Learning Research (04/2024)
Following Proposition 3, a natural question arises: Is it possible to choose “good” parameters ̃θ=(̃U,̃W)
based on the set of optimal parameters θopt? This would then yield train-loss and expected generalization-gap
bounds ˜O(1/(ηγ2
attnn))after Θ(n)steps of GD starting at ̃θ(0)=0. To investigate this question, define the
following parameters for each head, aligning with the aforementioned “good” directions of Proposition 3:
Uh∶=log(n)
γattn1
H1/2Uopt,Wh∶=C
HpWopt,
for someC>0,p>0,and∀h∈[H]. To yield the margin γattnof(10), we need that each Whhas norm at
least Γϵ∝1/S2. Thus, we need ∥Wh∥≳1
S2/Leftr⫯g⊸tl⫯ne⇒S2≳1
C⋅Hp.Now, in order to apply Thms. 1 and 2, the
requirement on the number of heads Hin terms of distance of ̃θtõθ(0)=0yields the following condition:
H1/2≳S5∥̃θ∥3. (12)
Note that ∥̃U∥=log(n)
γattn≈log(n)
S,∥̃W∥=C⋅H1/2−p. Hence, in computing ∥̃θ∥, we distinguish two cases.
First, assume that ∥̃W∥≥∥̃U∥which implies that S≳log(n)
C⋅Hp−1/2and∥̃θ∥≳∥̃W∥∨∥̃U∥=C⋅H1/2−p. Since
S≳1
C1/2⋅Hp/2∨log(n)
C⋅Hp−1/2,
by using Eq. (12), we get the following conditions on H:
H1/2≳S5⋅C3⋅H3/2−3p≳C1/2⋅H3/2−p/2∨log5(n)
C2⋅H2p−1/Leftr⫯g⊸tl⫯ne⇒Hp−2≳CandHp−1/4≲C
log5/2(n).
Combining these two gives C≲C
log5/2(n)/Leftr⫯g⊸tl⫯ne⇒ log(n)≲1,a contradiction since n>1. Thus, there are no
possible choices for pandCthat satisfy both conditions. The case ∥̃W∥≤∥̃U∥can be treated similarly
leading to the same conclusion; thus, is omitted for brevity.
Intuitively, this contradiction arises because of the large ∥Wh∥requirement to achieve margin γattn. Finally,
one can ask if it is possible to resolve the contradiction by changing the scaling of normalization with respect
toHin the MHA model Eq. (2), from 1/H1/2to1/Hcforc>0. It can be shown via the same argument
that no such value of cexists for which ̃θconstructed above satisfies the overparameterization requirement
Hc≳S5∥̃θ∥3. We thus conclude that the construction of weights in Proposition 3 does not yield a target
parameter that simultaneously achieves low empirical loss and allows choosing Hlarge enough as per (3).
This triggers interesting questions for future research: Does GD converge to weights attaining margin γattn
as in Proposition 3? If so, under what conditions on initialization? See also the remarks in Section 7.
6 Proof Sketch of Section 4
Throughout this section we drop the ̃⋅iñθand̃Φ(Xi;̃θ)as everything refers to the full model. Moreover,
̃θ(K)and̃θ(0)are denoted by θKandθ0. Refer to Figure 1 in the App. for a summary of the sketch.
6.1 Training analysis
The proof begins by showing step-wise descent for any iteration k≥0of GD (see Lemma 7), where step-size
at each iteration ηk≤1
ρkdepends on the objective’s local smoothness parameters ρk=β2(θk)∨β2(θk+1):
̂L(θk+1)≤̂L(θk)−ηk
2∥∇̂L(θk)∥2. (13)
Now, using Taylor’s theorem we can link ̂L(θk)tôL(θ)for anyθas follows:
̂L(θ)≥̂L(θk)+⟨∇̂L(θk),θ−θk⟩+1
2min
θkαλmin(∇2̂L(θkα))∥θ−θk∥2, (14)
10Published in Transactions on Machine Learning Research (04/2024)
whereθkα∶=αθk+(1−α)θ, α∈[0,1]. We can plug this into (13)to relate the loss at iterates θkandθk+1.
To continue, we need to lower bound minθkαλmin(∇2̂L(θkα)). For this, we use the following property of
the loss objective from Corollary 1: ∀θ∶λmin(∇2̂L(θ))≥−κ(θ)⋅̂L(θ),whereκ(θ)∶=β3(θ)√
H.Note from the
definition of β3(⋅)that∀θ1,θ2∶maxθ∈[θ1,θ2]β3(θ)=β3(θ1)∨β3(θ2).Thus, the above property of the loss
implies the following local self-bounded weak convexity property on the line [θ1,θ2]for arbitrary points θ1,θ2:
∀θ1,θ2∶min
θ∈[θ1,θ2]λmin(∇2̂L(θ))≥−β3(θ1)∨β3(θ2)√
H⋅max
θ∈[θ1,θ2]̂L(θ). (15)
Therefore, using Eq. (15) in Eq. (14), we can get:
̂L(θ)≥̂L(θk)+⟨∇̂L(θk),θ−θk⟩−1
2β3(θ1)∨β3(θ2)√
H⋅max
α∈[0,1]̂L(θkα)∥θ−θk∥2. (16)
To apply the Descent Lemma in (13), we need to fix a step-size such that satisfies the condition of the Lemma
at each iteration η≤ηkfor allk<K. Then, combining with Eq. (16)and applying standard telescope
summation, we arrive at the following:
1
KK
∑
k=1̂L(θk)≤̂L(θ)+∥θ−θ0∥2
2ηK+1
2KK−1
∑
k=0β3(θ)∨β3(θk)√
H⋅max
α∈[0,1]̂L(θkα)∥θ−θk∥2. (17)
Next, we use the following generalized local quasi-convexity (GLQC) of the loss function.
Proposition 4 (GLQC property: Slight variation of Prop. 8 of Taheri & Thrampoulidis (2023)) .Letθ1and
θ2be two points that are sufficiently close to each other, such that
2(β3(θ1)∨β3(θ2))∥θ1−θ2∥2≤√
H. (18)
Then, maxθ∈[θ1,θ2]̂L(θ)≤4
3(̂L(θ1)∨̂L(θ2)).
Using Proposition 4 in Eq. (17)and assuming sufficiently large heads Hsuch that√
H≥
2(β3(θ)∨β3(θk))∥θ−θk∥2, we can get the advertised regret bound in (4).
In order to remove the dependence of Hon iteration k, by an induction argument we can show bounded
iterates-norm i.e. ∥θk−θ∥≤3∥θ−θ0∥(see Lemma 10). Using this and the definition of β3(⋅)we can
controlβ3(θ)∨β3(θk)as(β3(θ)∨β3(θk))≲∥θ−θ0∥+∥θ∥2,∞to get the desired requirement of heads√
H≳∥θ∥2,∞∥θ−θ0∥3stated in Eq. (3).
The remaining piece to guarantee descent at each step is establishing a ρ(θ)such thatρk≤ρ(θ)for
allk<K. To do this, we recall that ρk=β2(θk)∨β2(θk+1). By definition of β2(⋅)in Corollary 1,
we can control β2(θk)∨β2(θk+1)with controlling ∥θk∥2,∞∨∥θk+1∥2,∞as(∥θk∥2,∞∨∥θk+1∥2,∞)≲∥θ−
θk∥+∥θ∥2,∞+1. Using iterates-norm bound and setting ρ(θ)=(2√
TdR3
√
H+TR2
4)α(θ)2withα(θ)∶=
3√
dR2[3√
TR3(3∥θ−θ0∥+∥θ∥2,∞)+2√
TR], satisfies the desired condition for the Descent Lemma
completing the proof.
6.2 Generalization analysis
In order to bound the expected generalization gap, we leverage the algorithmic stability framework. To begin,
consider the leave-one-out (loo) training loss ̂L¬i(θ)∶=1
n∑j≠iℓj(θ)fori∈[n], whereℓj(θ)∶=ℓ(yjΦ(Xj;θ))
denotes the j-th sample loss. With these, define the loo model updates of GD on the loo loss for η>0:
θ¬i
k+1∶=θ¬i
k−η∇̂L¬i(θ¬i
k), k≥0,θ¬i
0=θ0.
The following lemma relates expected generalization loss to average model stability for any G-Lipschitz loss.
Lemma 3 (Lei & Ying (2020), Thm. 2) .ForG-Lipschitz loss and for all iterates K, it holds that
E[L(θK)−̂L(θK)]≤2G⋅E[1
n∑n
i=1∥θK−θ¬i
K∥].
11Published in Transactions on Machine Learning Research (04/2024)
To bound the average model-stability on the r.h.s of the lemma’s inequality, we use GD expansiveness.
Specifically applying (Taheri & Thrampoulidis, 2023, Lemma B.1.) to our setting, gives ∀θ,θ′:
∥(θ−η∇̂L(θ))−(θ′−η∇̂L(θ′))∥≤max
α∈[0,1]{(1+ηβ3(θα)√
ĤL(θα))∨ηβ2(θα)}∥θ−θ′∥,(19)
where,θα=αθ+(1−α)θ′,α∈[0,1]. Using this and gradient self-boundedness from Corollary 1, we get:
∥θk+1−θ¬i
k+1∥≤max
α∈[0,1]{(1+ηβ3(θ¬i
kα)√
ĤL¬i(θ¬i
kα))∨ηβ2(θ¬i
kα)}⋅∥θk−θ¬i
k∥+ηβ1(θk)
nℓi(θk),(20)
whereθ¬i
kα∶=αθk+(1−α)θ¬i
kforα∈[0,1]. Further using the bounded iterates-norm property from the
training analysis, we can control β2(θ¬i
kα)≤˜β2(θ)andβ3(θ¬i
kα)≤˜β3(θ)making them independent of k(See
Lemma 11 for the definitions of ˜β2(⋅),˜β3(⋅)). In order to invoke the Descent Lemma, we set the step-size
same as in the training analysis. Thus, (20) becomes:
∥θk+1−θ¬i
k+1∥≤((1+η˜β3(θ)√
H)max
α∈[0,1]̂L¬i(θ¬i
kα))∥θk−θ¬i
k∥+ηβ1(θk)
nℓi(θk). (21)
As in the training analysis, we can control the loo empirical loss ̂L¬ifor any point on the line [θk,θ¬i
k]of two
sufficiently close points satisfying√
H≥2(β3(θk)∨β3(θ¬i
k))∥θk−θ¬i
k∥2. Using Prop. 4, Eq. (21)becomes
∥θk+1−θ¬i
k+1∥≤(1+αk,i)∥θk−θ¬i
k∥+η˜β1(θ)
nℓi(θk), (22)
whereαk,i∶=4η˜β3(θ)
3√
H(̂L¬i(θk)+̂L¬i(θ¬i
k))andβ1(θk)≤˜β1(θ)similar toβ2(⋅),β3(⋅)using bounded iterates-
norm. Unrolling the iterates in (22), summing over i∈[n]and using training regret bounds, we have the
following average model stability bound for any iterate K:1
n∑n
i=1∥θK−θ¬i
K∥≤2η˜β1(θ)
n(2K̂L(θ)+9∥θ−θ0∥2
4η),
Combining this with an application of Lemma 3 for our objective, which is G≤˜β1(θ)-Lipschitz from Corollary
1, and using η≤1
ρ(θ)≤1
(˜β1(θ))2, we get the desired generalization gap stated in Thm. 2.
7 Concluding remarks
We studied convergence and generalization of GD for training a multi-head attention layer in a classification
task. Our training and generalization bounds hold under an appropriate realizability condition asking for the
existence of an a target model ̃θachieving good train loss while being sufficiently close to initialization. In
particular, from the condition on the number of heads Hin(3), we need ̃θis at most ˜O(d−1/3T−1/6R−5/3H1/6)
far from initialization (provided ∥̃θ∥2,∞=O(1)). In Sec. 4.3 we showed that such a model exists if the
initialization is chosen appropriately. Specifically it suffices that ∥̃θ(0)∥2,∞=O(1), the model output at
initialization is ˜O(1)-bounded and that the data are linearly separable with margin γwith respect to the
NTK features of the model at initialization. Then, O(d2TR10polylog(n)/γ6)number of heads guarantee
that Θ(n)GD steps result in train and test loss bounds ˜O(1/(ηγ2n)). In Sec. 5 we applied our results to a
tokenized-mixture model. We showed that after one randomized gradient step from 0, the model satisfies the
above conditions for goodintialization. For this initialization, we computed the NTK margin γ⋆which in
turn governs the guaranteed rate of convergence and generalization based on our general bounds. This opens
several interesting questions for future work.
First, does random initialization of attention weights satisfy NTK separability, and if so, what is the
corresponding margin? Second, are there other initialization strategies that guarantee the realizability
conditions are satisfied? Here, note that our conditions for goodinitialization are only shown to be sufficient
for realizability leaving room for improvements. Third, how suboptimal is the best NTK margin (among other
potential natural initializations) compared to the model’s global margin arg max∥̃θ∥=√
2mini∈[n]yĩΦ(Xi;̃θ)?
In Proposition 3 we showed for the data model DM1 that there exists single-head attention model θopt=
12Published in Transactions on Machine Learning Research (04/2024)
(Uopt,Wopt)with∥θopt∥=√
2such thatyΦ(X;Uopt,Γϵ⋅Wopt)=S√
T√
2((1−ϵ)−2ϵZµ/S)for all Γϵ≳
log((ζ−1−1)/ϵ)
Sand anyϵ∈(0,1)(see App. E). In particular, as ϵ→0andΓϵ→∞(for which the softmax
map gets saturated and attends to tokens with highest relevance score) the achieved margin approaches
γattn∶=S√
T/√
2, which is independent of the sparsity level ζ. In particular, γattn≥γ⋆≥γlinand the gap
increases with decreasing sparsity. Is it possible to establish finite-time convergence bounds to models with
margin≈γattnunder appropriate initialization? How is the answer affected by the fact that the optimal
attention weights in this case are diverging in norm ( Γϵ→∞)? Using our approach, we argued in Sec. 5.3 that
the key challenge is the saturation of norm of Wopt(Γϵ), which does not allow the appropriate realizability
condition to hold (at least for 0initialization). Finally, it is interesting to consider other data models for
which multiple heads are necessary to interpolate the data.
8 Acknowledgements
This work is supported by NSERC Discovery Grant RGPIN-2021-03677, NSF Grant CCF-2009030, and
a CIFAR AI Catalyst grant. The authors also acknowledge use of the Sockeye cluster by UBC Advanced
Research Computing. PD thanks Bhavya Vasudeva for the helpful discussions.
References
Ekin Akyürek, Dale Schuurmans, Jacob Andreas, Tengyu Ma, and Denny Zhou. What learning algorithm
is in-context learning? investigations with linear models. In The Eleventh International Conference on
Learning Representations , 2023. URL https://openreview.net/forum?id=0g0X4H8yN4I .
Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via over-
parameterization. In International Conference on Machine Learning , pp. 242–252. PMLR, 2019.
Sanjeev Arora, Simon Du, Wei Hu, Zhiyuan Li, and Ruosong Wang. Fine-grained analysis of optimization and
generalization for overparameterized two-layer neural networks. In International Conference on Machine
Learning , pp. 322–332. PMLR, 2019.
Pierre Baldi and Roman Vershynin. The quarks of attention. arXiv preprint arXiv:2202.08371 , 2022.
Arindam Banerjee, Pedro Cisneros-Velarde, Libin Zhu, and Mikhail Belkin. Restricted strong convexity of
deep learning models with smooth activations. arXiv preprint arXiv:2209.15106 , 2022.
Alberto Bietti, Vivien Cabannes, Diane Bouchacourt, Herve Jegou, and Leon Bottou. Birth of a transformer:
A memory viewpoint. arXiv preprint arXiv:2306.00802 , 2023.
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, PranavShyam, GirishSastry, AmandaAskell, SandhiniAgarwal, ArielHerbert-Voss, Gretchen
Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris
Hesse, MarkChen, EricSigler, MateuszLitwin, ScottGray, BenjaminChess, JackClark, ChristopherBerner,
Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners.
In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (eds.), Advances in Neural Information
Processing Systems , volume 33, pp. 1877–1901. Curran Associates, Inc., 2020. URL https://proceedings.
neurips.cc/paper_files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf .
Yuan Cao and Quanquan Gu. Generalization bounds of stochastic gradient descent for wide and deep neural
networks. Advances in neural information processing systems , 32, 2019.
Zixiang Chen, Yuan Cao, Difan Zou, and Quanquan Gu. How much over-parameterization is sufficient to
learn deep relu networks? In International Conference on Learning Representations , 2020.
Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine reading.
InProceedings of the 2016 Conference on Empirical Methods in Natural Language Processing , pp. 551–
561. Association for Computational Linguistics, November 2016. doi: 10.18653/v1/D16-1053. URL
https://aclanthology.org/D16-1053 .
13Published in Transactions on Machine Learning Research (04/2024)
JacobDevlin, Ming-WeiChang, KentonLee, andKristinaToutanova. BERT:Pre-trainingofdeepbidirectional
transformers for language understanding. pp. 4171–4186, Minneapolis, Minnesota, June 2019. Association
for Computational Linguistics. doi: 10.18653/v1/N19-1423. URL https://aclanthology.org/N19-1423 .
Yihe Dong, Jean-Baptiste Cordonnier, and Andreas Loukas. Attention is not all you need: Pure attention loses
rank doubly exponentially with depth. In International Conference on Machine Learning , pp. 2793–2803.
PMLR, 2021.
AlexeyDosovitskiy, LucasBeyer, AlexanderKolesnikov, DirkWeissenborn, XiaohuaZhai, ThomasUnterthiner,
Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby.
An image is worth 16x16 words: Transformers for image recognition at scale, 2021.
Simon Du, Jason Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai. Gradient descent finds global minima of
deep neural networks. In International conference on machine learning , pp. 1675–1685. PMLR, 2019.
Benjamin L Edelman, Surbhi Goel, Sham Kakade, and Cyril Zhang. Inductive biases and variable creation in
self-attention mechanisms. arXiv preprint arXiv:2110.10090 , 2021.
Tolga Ergen, Behnam Neyshabur, and Harsh Mehta. Convexifying transformers: Improving optimization and
understanding of transformer networks. arXiv preprint arXiv:2211.11052 , 2022.
Jiri Hron, Yasaman Bahri, Jascha Sohl-Dickstein, and Roman Novak. Infinite attention: Nngp and ntk for
deep attention networks. In International Conference on Machine Learning , pp. 4376–4386. PMLR, 2020.
Arthur Jacot, Franck Gabriel, and Clément Hongler. Neural tangent kernel: Convergence and generalization
in neural networks. Advances in neural information processing systems , 31, 2018.
Samy Jelassi, Michael Eli Sander, and Yuanzhi Li. Vision transformers provably learn spatial structure. In
Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (eds.), Advances in Neural Information
Processing Systems , 2022. URL https://openreview.net/forum?id=eMW9AkXaREI .
Ziwei Ji and Matus Telgarsky. Risk and parameter convergence of logistic regression. arXiv preprint
arXiv:1803.07300 , 2018.
Ziwei Ji and Matus Telgarsky. Polylogarithmic width suffices for gradient descent to achieve arbitrarily small
test error with shallow relu networks. In International Conference on Learning Representations , 2020.
Ziwei Ji and Matus Telgarsky. Characterizing the implicit bias via a primal-dual analysis. In Algorithmic
Learning Theory , pp. 772–804. PMLR, 2021.
Yunwen Lei and Yiming Ying. Fine-grained analysis of stability and generalization for stochastic gradient
descent. In International Conference on Machine Learning , pp. 5809–5819. PMLR, 2020.
Yunwen Lei, Rong Jin, and Yiming Ying. Stability and generalization analysis of gradient methods for shallow
neural networks. In Advances in Neural Information Processing Systems , 2022.
Hongkang Li, Meng Weng, Sijia Liu, and Pin-Yu Chen. A theoretical understanding of shallow vision
transformers: Learning, generalization, and sample complexity. In International Conference on Learning
Representations , 2023a.
Yingcong Li, M. Emrullah Ildiz, Dimitris Papailiopoulos, and Samet Oymak. Transformers as algorithms:
Generalization and stability in in-context learning, 2023b.
Valerii Likhosherstov, Krzysztof Choromanski, and Adrian Weller. On the expressive power of self-attention
matrices, 2021.
Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen Zhou, and Yoshua Bengio.
A structured self-attentive sentence embedding. In International Conference on Learning Representations ,
2017. URL https://openreview.net/forum?id=BJC_jUqxe .
14Published in Transactions on Machine Learning Research (04/2024)
Chaoyue Liu, Libin Zhu, and Misha Belkin. On the linearity of large non-linear models: when and why the
tangent kernel is constant. Advances in Neural Information Processing Systems , 33:15954–15964, 2020.
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke
Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach, 2019.
Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization, 2019.
Haoye Lu, Yongyi Mao, and Amiya Nayak. On the dynamics of training attention models. In International
Conference on Learning Representations , 2021. URL https://openreview.net/forum?id=1OCTOShAmqB .
Sadegh Mahdavi, Renjie Liao, and Christos Thrampoulidis. Memorization capacity of multi-head attention
in transformers. arXiv preprint arXiv:2306.02010 , 2023.
Quynh Nguyen, Marco Mondelli, and Guido F Montufar. Tight bounds on the smallest eigenvalue of the
neural tangent kernel for deep relu networks. In Marina Meila and Tong Zhang (eds.), Proceedings of
the 38th International Conference on Machine Learning , volume 139 of Proceedings of Machine Learning
Research , pp. 8119–8129. PMLR, 18–24 Jul 2021.
Quynh N Nguyen and Marco Mondelli. Global convergence of deep networks with one wide layer followed by
pyramidal topology. Advances in Neural Information Processing Systems , 33:11961–11972, 2020.
Konstantinos E Nikolakakis, Farzin Haddadpour, Amin Karbasi, and Dionysios S Kalogerias. Beyond
Lipschitz: Sharp generalization and excess risk bounds for full-batch gd. arXiv preprint arXiv:2204.12446 ,
2022.
Atsushi Nitanda, Geoffrey Chinot, and Taiji Suzuki. Gradient descent can learn less over-parameterized
two-layer neural networks on classification problems. arXiv preprint arXiv:1905.09870 , 2019.
OpenAI. Openai: Introducing chatgpt, 2022. URL https://openai.com/blog/chatgpt,2022 .
OpenAI. Gpt-4 technical report, 2023.
Samet Oymak and Mahdi Soltanolkotabi. Toward moderate overparameterization: Global convergence
guarantees for training shallow neural networks. IEEE Journal on Selected Areas in Information Theory , 1
(1):84–105, 2020.
Samet Oymak, Ankit Singh Rawat, Mahdi Soltanolkotabi, and Christos Thrampoulidis. On the role of
attention in prompt-tuning. In ICLR 2023 Workshop on Mathematical and Empirical Understanding of
Foundation Models , 2023.
Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention model
for natural language inference. In Proceedings of the 2016 Conference on Empirical Methods in Natural
Language Processing , pp. 2249–2255, Austin, Texas, November 2016. Association for Computational
Linguistics. doi: 10.18653/v1/D16-1244. URL https://aclanthology.org/D16-1244 .
Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish
Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning
transferable visual models from natural language supervision. In Marina Meila and Tong Zhang (eds.),
Proceedings of the 38th International Conference on Machine Learning , volume 139 of Proceedings of
Machine Learning Research , pp. 8748–8763. PMLR, 18–24 Jul 2021. URL https://proceedings.mlr.
press/v139/radford21a.html .
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou,
Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer.
21(1), 2020. ISSN 1532-4435.
Dominic Richards and Ilja Kuzborskij. Stability & generalisation of gradient descent for shallow neural
networks without the neural tangent kernel. Advances in Neural Information Processing Systems , 34:
8609–8621, 2021.
15Published in Transactions on Machine Learning Research (04/2024)
Dominic Richards and Mike Rabbat. Learning with gradient descent and weakly convex losses. In International
Conference on Artificial Intelligence and Statistics , pp. 1990–1998. PMLR, 2021.
Itay M Safran, Gilad Yehudai, and Ohad Shamir. The effects of mild over-parameterization on the optimization
landscape of shallow relu neural networks. In Mikhail Belkin and Samory Kpotufe (eds.), Proceedings of
Thirty Fourth Conference on Learning Theory , volume 134 of Proceedings of Machine Learning Research ,
pp. 3889–3934. PMLR, 15–19 Aug 2021.
Arda Sahiner, Tolga Ergen, Batu Ozturkler, John Pauly, Morteza Mardani, and Mert Pilanci. Unraveling
attention via convex duality: Analysis and interpretations of vision transformers. International Conference
on Machine Learning , 2022.
Clayton Sanford, Daniel Hsu, and Matus Telgarsky. Representational strengths and limitations of transformers.
arXiv preprint arXiv:2306.02896 , 2023.
Matan Schliserman and Tomer Koren. Stability vs implicit bias of gradient methods on separable data and
beyond. In Po-Ling Loh and Maxim Raginsky (eds.), Proceedings of Thirty Fifth Conference on Learning
Theory, volume 178 of Proceedings of Machine Learning Research , pp. 3380–3394. PMLR, 02–05 Jul 2022.
Ohad Shamir. Gradient methods never overfit on separable data. Journal of Machine Learning Research , 22
(85):1–20, 2021.
Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng, and
Christopher Potts. Recursive deep models for semantic compositionality over a sentiment treebank. In
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing , pp. 1631–1642,
Seattle, Washington, USA, October 2013. Association for Computational Linguistics.
Daniel Soudry, Elad Hoffer, Mor Shpigel Nacson, Suriya Gunasekar, and Nathan Srebro. The implicit bias of
gradient descent on separable data. The Journal of Machine Learning Research , 19(1):2822–2878, 2018.
Hossein Taheri and Christos Thrampoulidis. Generalization and stability of interpolating neural networks
with minimal width. arXiv preprint arXiv:2302.09235 , 2023.
Davoud Ataee Tarzanagh, Yingcong Li, Christos Thrampoulidis, and Samet Oymak. Transformers as support
vector machines, 2023a.
Davoud Ataee Tarzanagh, Yingcong Li, Xuechen Zhang, and Samet Oymak. Max-margin token selection in
attention mechanism, 2023b.
Matus Telgarsky. Margins, shrinkage, and boosting. In International Conference on Machine Learning , pp.
307–315. PMLR, 2013.
Matus Telgarsky. Feature selection and low test error in shallow low-rotation relu networks. In The Eleventh
International Conference on Learning Representations , 2022.
Yuandong Tian, Yiping Wang, Beidi Chen, and Simon Du. Scan and snap: Understanding training dynamics
and token composition in 1-layer transformer, 2023.
Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Herve
Jegou. Training data-efficient image transformers & distillation through attention. In Marina Meila
and Tong Zhang (eds.), Proceedings of the 38th International Conference on Machine Learning , volume
139 ofProceedings of Machine Learning Research , pp. 10347–10357. PMLR, 18–24 Jul 2021. URL
https://proceedings.mlr.press/v139/touvron21a.html .
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix,
Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard
Grave, and Guillaume Lample. Llama: Open and efficient foundation language models, 2023.
Ashish Vaswani, Noam M. Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz
Kaiser, and Illia Polosukhin. Attention is all you need. In NIPS, 2017.
16Published in Transactions on Machine Learning Research (04/2024)
Johannes von Oswald, Eyvind Niklasson, E. Randazzo, João Sacramento, Alexander Mordvintsev, Andrey Zh-
moginov, and Max Vladymyrov. Transformers learn in-context by gradient descent. ArXiv, abs/2212.07677,
2022.
Weihang Xu and Simon Du. Over-parameterization exponentially slows down gradient descent for learning
a single neuron. In Gergely Neu and Lorenzo Rosasco (eds.), Proceedings of Thirty Sixth Conference on
Learning Theory , volume 195 of Proceedings of Machine Learning Research , pp. 1155–1198. PMLR, 2023.
Chulhee Yun, Srinadh Bhojanapalli, Ankit Singh Rawat, Sashank J. Reddi, and Sanjiv Kumar. Are
transformers universal approximators of sequence-to-sequence functions?, 2020a.
Chulhee Yun, Yin-Wen Chang, Srinadh Bhojanapalli, Ankit Singh Rawat, Sashank J. Reddi, and Sanjiv
Kumar.o(n)connections are expressive enough: Universal approximability of sparse transformers, 2020b.
Ruiqi Zhang, Spencer Frei, and Peter L. Bartlett. Trained transformers learn linear models in-context, 2023.
Mo Zhou, Rong Ge, and Chi Jin. A local convergence theory for mildly over-parameterized two-layer neural
network. In Mikhail Belkin and Samory Kpotufe (eds.), Proceedings of Thirty Fourth Conference on
Learning Theory , volume 134 of Proceedings of Machine Learning Research , pp. 4577–4632. PMLR, 15–19
Aug 2021.
Zhenyu Zhu, Fanghui Liu, Grigorios Chrysos, Francesco Locatello, and Volkan Cevher. Benign overfitting
in deep neural networks under lazy training. In International Conference on Machine Learning , pp.
43105–43128. PMLR, 2023.
17Published in Transactions on Machine Learning Research (04/2024)
Contents
A Gradients and Hessian Calculations 18
A.1 Gradient/Hessian calculations for multihead-attention . . . . . . . . . . . . . . . . . . . . . . . 19
A.2 Proof of Corollary 1: Objective’s Gradient/Hessian . . . . . . . . . . . . . . . . . . . . . . . . . 25
B Training Analysis 26
B.1 Preliminaries . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26
B.2 Proof of Proposition 4 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26
B.3 Key Lemmas . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27
B.4 Proof of Theorem 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29
B.5 Corollary 3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30
C Generalization Analysis 31
C.1 Proof of Theorem 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34
C.2 Corollary 4 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35
C.3 From goodinitialization to realizability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35
C.4 Proof of Corollary 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36
D Proofs for Section 5 37
D.1 Useful facts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37
D.2 Proof of Proposition 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37
D.2.1 Proof of P1: Bounded norm per head . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38
D.2.2 Proof of P2: Bounded initialization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38
D.2.3 Proof of P3: NTK separability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38
D.3 Proof of Lemma 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43
E Optimal Model 44
E.1 Proof of Proposition 3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45
F Linear Model 46
G Experiments 46
H Related work 48
A Gradients and Hessian Calculations
We define the following for convenience
̂L′(θ)=1
n∑
i∈[n]∣ℓ′(yiΦ(Xi,θ))∣,and ̂L′′(θ)=1
n∑
i∈[n]∣ℓ′′(yiΦ(Xi,θ))∣.
18Published in Transactions on Machine Learning Research (04/2024)
For logistic loss ℓ(z)∶=log(1+e−z):
̂L′(θ)≤̂L(θ)≤1,and ̂L′′(θ)≤1
4. (24)
We use ⃗Ato denote the the vectorization of a matrix Aand⊙to denote the Hadamard product. Finally, we
define e(n)
ias thei-th standard basis vector in Rn.
A.1 Gradient/Hessian calculations for multihead-attention
Lemma 4. Let the softmax attention model Φ(X;θ)in Eq.(1). Then, for all vectors a∈RTandb,c∈Rdit
holds:
1.∇UΦ(X;θ)=φ(XWX⊺)X.
2.∇WΦ(X;θ)=T
∑
t=1xtu⊺
tX⊺φ′(XW⊺xt)X.
3.∇W⟨a,∇UΦ(X;θ)b⟩=T
∑
t=1xtatb⊺X⊺φ′(XW⊺xt)X.
4.∇W⟨c,∇WΦ(X;θ)b⟩=T
∑
t=1c⊺xtxt(u⊺
tX⊺diag(Xb)−φ(XW⊺xt)⊺Xb(Xut)⊺
−φ(XW⊺xt)⊺Xut(Xb)⊺)φ′(XW⊺xt)X.
Proof.For simplicity, we denote G∶=XWX⊺and the rows of φ(G)asφ(gt)=φ(XW⊺xt),t∈[T]. Recall,
for anyv∈Rd,
φ′(d)=∇vφ(v)=diag(φ(v))−φ(v)φ(v)⊺, (25)
i.e.,
φ(v+δ)=φ(v)+(diag(φ(v))−φ(v)φ(v)⊺)δ+o(∥δ∥2). (26)
We start with the gradient with respect to U,
∇UΦ(X;θ)=φ(G)X. (27)
Next step is to compute the gradient with respect to W,∇WΦ(X;θ)=∑T
t=1∇W(u⊺
tX⊺φ(gt)). Using Eq.
(26),
u⊺
tX⊺φ(X(W+∆)⊺xt)=u⊺
tX⊺(φ(gt)+(diag(φ(gt))−φ(gt)φ(gt)⊺)X∆⊺xt)+o(∥∆∥2)
=u⊺
tX⊺φ(gt)+tr(xtu⊺
tX⊺φ′(XW⊺xt)X∆⊺)+o(∥∆∥2).
Thus,
∇W(u⊺
tX⊺φ(gt))=xtu⊺
tX⊺φ′(XW⊺xt)X (28)
and
∇WΦ(X;θ)=T
∑
t=1xtu⊺
tX⊺φ′(XW⊺xt)X. (29)
19Published in Transactions on Machine Learning Research (04/2024)
For the third statement of the lemma, we have the following sequence of equalities,
∇W⟨a,∇UΦ(X;θ)b⟩=∇W⟨a,φ(G)Xb⟩=∇W(a⊺φ(G)Xb)
=∇Wtr(ba⊺φ(G)X)=T
∑
t=1∇W(atb⊺X⊺φt(g))
=T
∑
t=1xtatb⊺X⊺φ′(XW⊺xt)X, (30)
where in the last equality we used Eq. (28).
For the Hessian with respect to W, we have:
∇W⟨c,∇WΦ(X;θ)b⟩=T
∑
t=1∇W(c⊺xtu⊺
tX⊺
⌟⟨⟨⟪rl⟫l⟩⟩⟪⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟨⟨⟪rl⟫m⟩⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟨⟨⟪rl⟫r⟩⟩⟩⟪
p⊺
tφ′(XW⊺xt)Xb
⌟⟨⟨⟪rl⟫l⟩⟩⟪⌟⟨⟨⟪rl⟫m⟩⟨⌟⟨⟨⟪rl⟫r⟩⟩⟩⟪
qt)
=T
∑
t=1(∇Wp⊺
tdiag(qt)φ(gt)
⌟⟨⟨⟪rl⟫l⟩⟩⟪⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟨⟨⟪rl⟫m⟩⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟨⟨⟪rl⟫r⟩⟩⟩⟪
Term I−∇Wp⊺
tφ(gt)q⊺
tφ(gt)
⌟⟨⟨⟪rl⟫l⟩⟩⟪⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟨⟨⟪rl⟫m⟩⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟨⟨⟪rl⟫r⟩⟩⟩⟪
Term II), (31)
where we used the property diag(a1)a2=diag(a2)a1for vectorsa1,a2∈RT.
First, we compute the Term Iabove. Using Eq. (26):
p⊺
tdiag(qt)φ(X(W+∆)⊺xt)=p⊺
tdiag(qt)(φ(gt)+φ′(XW⊺xt)X∆⊺xt)+o(∥∆∥2)
=p⊺
tdiag(qt)φ(gt)+tr(xtp⊺
tdiag(qt)φ′(XW⊺xt)X∆⊺)+o(∥∆∥2).(32)
Therefore,
∇Wp⊺
tdiag(qt)φ(gt)=xtp⊺
tdiag(qt)φ′(XW⊺xt)X
=xtc⊺xtu⊺
tX⊺diag(Xb)φ′(XW⊺xt)X. (33)
Similarly for Term IIwe have:
p⊺
tφ(X(W+∆)⊺xt)q⊺
tφ(X(W+∆)⊺xt)=(p⊺
tφ(gt)+p⊺
tφ′(XW⊺xt)X∆⊺xt+o(∥∆∥2))
(q⊺
tφ(gt)+q⊺
tφ′(XW⊺xt)X∆⊺xt+o(∥∆∥2))
=p⊺
tφ(gt)q⊺
tφ(gt)+tr(xt(q⊺φ(gt)p⊺
+p⊺φ(gt)q⊺)φ′(XW⊺xt)X∆⊺)+o(∥∆∥2).(34)
Thus,
∇Wp⊺
tφ(gt)q⊺
tφ(gt)=xt(q⊺φ(gt)p⊺+p⊺φ(gt)q⊺)φ′(XW⊺xt)X
=c⊺xtxtφ(gt)⊺(Xut(Xb)⊺+Xb(Xut)⊺)φ′(XW⊺xt)X. (35)
Combining Eqs. (33) and (35) and plugging in Eq. (31) we conclude that
∇W⟨c,∇WΦ(X;θ)b⟩=T
∑
t=1c⊺xtxt(u⊺
tX⊺diag(Xb)−φ(gt)⊺Xb(Xut)⊺
−φ(gt)⊺Xut(Xb)⊺)φ′(XW⊺xt)X. (36)
We restate Proposition 1 here for convenience.
20Published in Transactions on Machine Learning Research (04/2024)
Proposition 5 (Restatement of Prop. 1) .Let the softmax attention model Φ(X;θ)in Eq.(1). Then, it
holds:
1.∥∇⃗UΦ(X;θ)∥≤√
T∥X∥2,∞.
2.∥∇⃗WΦ(X;θ)∥≤2∥X∥2
2,∞T
∑
t=1∥Xut∥∞.
3.∥∇θΦ(X;θ)∥≤2∥X∥2
2,∞T
∑
t=1∥Xut∥∞+√
T∥X∥2,∞.
4.∥∇2
θΦ(X;θ)∥≤6d∥X∥2
2,∞∥X∥2
1,∞T
∑
t=1∥Xut∥∞+2√
Td∥X∥2
2,∞∥X∥1,∞.
Moreover, if Assumption 1 holds,
1.∥∇θΦ(X;θ)∥≤√
TR(2R2∥U∥F+1).
2.∥∇2
θΦ(X;θ)∥≤2√
TdR3(3√
dR2∥U∥F+1).
Proof.Using the first statement of Lemma 4,
∥∇⃗UΦ(X;θ)∥=⌟roo⟪⟪op
⌟roo⟪mo⟨⌟roo⟪mo⟨⌟roo⟪⟨o⟪T
∑
t=1∥X⊺φ(gt)∥2
2≤⌟roo⟪⟪op
⌟roo⟪mo⟨⌟roo⟪mo⟨⌟roo⟪⟨o⟪T
∑
t=1∥X⊺∥2
1,2∥φ(gt)∥2
1
=√
Tmax
t∈[T]∥xt∥=√
T∥X∥2,∞, (37)
where we used that ∥φ(gt)∥1=1.
Using Lemma 4 for the gradient with respect to W,
∇WΦ(X;θ)=T
∑
t=1xtu⊺
tX⊺φ′(XW⊺xt)X. (38)
Therefore, by applying triangle inequality,
∥∇⃗WΦ(X;θ)∥≤T
∑
t=1∥xt∥∥X⊺φ′(gt)Xut∥.
In the next step we bound ∥X⊺φ′(gt)Xut∥,
∥X⊺φ′(XW⊺xt)Xut∥≤∥X⊺diag(φ(gt))Xut∥
⌟⟨⟨⟪rl⟫l⟩⟩⟪⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟨⟨⟪rl⟫m⟩⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟨⟨⟪rl⟫r⟩⟩⟩⟪
Term I+∥X⊺φ(gt)φ(gt)⊺Xut∥
⌟⟨⟨⟪rl⟫l⟩⟩⟪⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟨⟨⟪rl⟫m⟩⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟨⟨⟪rl⟫r⟩⟩⟩⟪
Term II. (39)
For Term Iwe do as follows:
∥X⊺diag(φ(gt))Xut∥= max
v∈Rdand∥v∥=1(Xv)⊺diag(φ(gt))Xut
=max
∥v∥=1T
∑
τ=1[diag(φ(gt))]ττ[Xut]τ[Xv]τ
=max
∥v∥=1φ(gt)⊺((Xut)⊙(Xv))
≤max
∥v∥=1∥φ(gt)∥1∥(Xut)⊙(Xv)∥∞
≤∥Xut∥∞max
∥v∥=1∥Xv∥∞
=∥Xut∥∞∥X∥2,∞, (40)
21Published in Transactions on Machine Learning Research (04/2024)
where we used Hölder’s inequality in the first inequality.
Then, we compute Term II:
∥X⊺φ(gt)φ(gt)⊺Xut∥= max
v∈Rdand∥v∥=1(Xv)⊺φ(gt)φ(gt)⊺Xut
≤max
∥v∥=1∥φ(gt)∥2
1∥Xut∥∞∥Xv∥∞
=∥Xut∥∞max
∥v∥=1∥Xv∥∞
=∥Xut∥∞∥X∥2,∞, (41)
where we used Hölder’s inequality in the first inequality. Combining Eqs. (40)and(41)and plugging in Eq.
(39) yields
∥(Xut)⊺φ′(gt)X∥≤2∥Xut∥∞∥X∥2,∞. (42)
Therefore,
∥∇⃗WΦ(X;θ)∥≤2∥X∥2,∞T
∑
t=1∥xt∥∥Xut∥∞. (43)
Using Eqs. (37) and (43) we conclude that
∥∇Φ(X;θ)∥≤2∥X∥2
2,∞T
∑
t=1∥Xut∥∞+√
T∥X∥2,∞. (44)
By using Assumption 1 to simplify Eq. (44),
∥∇θΦ(X;θ)∥≤2R2T
∑
t=1∥Xut∥∞+√
TR, (45)
or
∥∇θΦ(X;θ)∥≤2√
TR3∥U∥F+√
TR. (46)
We need to derive the Hessian to bound the greatest eigenvalue of it.
∇2
θΦ(X;θ)=[∇2
⃗UΦ(X;θ)∇2
⃗W⃗UΦ(X;θ)
∇2
⃗W⃗UΦ(X;θ)⊺∇2
⃗WΦ(X;θ)].
First, we compute the Hessian with respect to U,
∇2
⃗UΦ(X;θ)=∇U(φ(G)X)=0Td×Td. (47)
In the next step, we use the third statement of Lemma 4 and set a=e(T)
tandb=e(d)
jto compute the Hessian
with respect to WandU,
∇W([∇UΦ(X;θ)]tj)=xtX⊺
∶,jφ′(gt)X. (48)
recall,X∶,jis thej-th column of X. Therefore,
∇2
⃗W⃗UΦ(X;θ)=⎡⎢⎢⎢⎢⎢⎣∇W([∇UΦ(X;θ)]11)...∇W([∇UΦ(X;θ)]1d)
⋮ ⋱ ⋮
∇W([∇UΦ(X;θ)]T1)...∇W([∇UΦ(X;θ)]Td)⎤⎥⎥⎥⎥⎥⎦. (49)
Lastly, in order to compute the Hessian with respect to W, we use the last statement of Lemma 4 and set
c=e(d)
iandb=e(d)
j,
∇W([∇WΦ(X;θ)]ij)=T
∑
t=1Xtixt(u⊺
tX⊺diag(X∶,j)−φ(gt)⊺X∶,ju⊺
tX⊺
−φ(gt)⊺XutX⊺
∶,j)φ′(gt)X. (50)
22Published in Transactions on Machine Learning Research (04/2024)
Thus,
∇2
⃗WΦ(X;θ)=⎡⎢⎢⎢⎢⎢⎣∇W([∇WΦ(X;θ)]11)...∇W([∇WΦ(X;θ)]1d)
⋮ ⋱ ⋮
∇W([∇WΦ(X;θ)]d1)...∇W([∇WΦ(X;θ)]dd)⎤⎥⎥⎥⎥⎥⎦. (51)
To find the maximum eigenvalue of the Hessian we need to upper-bound
max
∥v∥=1⟨v,∇2
θΦ(X;θ)v⟩,
where
v=concat(p1,...,pT,q1,...,qd)∈RTd+d2andpt∈Rd,t∈[T]andqj∈Rd,j∈[d].
Then,
∥∇2
θΦ(X;θ)∥
≤max
∥v∥=1d
∑
i=1d
∑
j=1T
∑
t=1Xtiq⊺
ixt((Xut)⊺diag(X∶,j)−φ(gt)⊺X∶,j(Xut)⊺−φ(gt)⊺XutX⊺
∶,j)φ′(gt)Xqj
⌟⟨⟨⟪rl⟫l⟩⟩⟪⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟨⟨⟪rl⟫m⟩⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟨⟨⟪rl⟫r⟩⟩⟩⟪
Term 1
+2 max
∥v∥=1d
∑
j=1T
∑
t=1p⊺
txtX⊺
∶,jφ′(gt)Xqj
⌟⟨⟨⟪rl⟫l⟩⟩⟪⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟨⟨⟪rl⟫m⟩⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟨⟨⟪rl⟫r⟩⟩⟩⟪
Term 2. (52)
First, we bound Term 1:
max
∥v∥=1d
∑
i=1d
∑
j=1T
∑
t=1Xtiq⊺
ixt((Xut)⊺diag(X∶,j)−φ(gt)⊺X∶,j(Xut)⊺−φ(gt)⊺XutX⊺
∶,j)φ′(gt)Xqj
≤∥X∥1,∞∥X∥2,∞⎛
⎝max
∥v∥=1d
∑
i=1d
∑
j=1T
∑
t=1∥qi∥∥qj∥∥(Xut)⊺diag(X∶,j)φ′(gt)X∥
⌟⟨⟨⟪rl⟫l⟩⟩⟪⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟨⟨⟪rl⟫m⟩⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟨⟨⟪rl⟫r⟩⟩⟩⟪
Term I+
max
∥v∥=1d
∑
i=1d
∑
j=1T
∑
t=1∥qi∥∥qj∥∥φ(gt)⊺X∶,j(Xut)⊺φ′(gt)X∥
⌟⟨⟨⟪rl⟫l⟩⟩⟪⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟨⟨⟪rl⟫m⟩⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟨⟨⟪rl⟫r⟩⟩⟩⟪
Term II+
max
∥v∥=1d
∑
i=1d
∑
j=1T
∑
t=1∥qi∥∥qj∥∥φ(gt)⊺XutX⊺
∶,jφ′(gt)X∥
⌟⟨⟨⟪rl⟫l⟩⟩⟪⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟨⟨⟪rl⟫m⟩⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟨⟨⟪rl⟫r⟩⟩⟩⟪
Term III⎞
⎠. (53)
For Term I, we have:
max
∥v∥=1d
∑
i=1d
∑
j=1T
∑
t=1∥qi∥∥qj∥∥(Xut)⊺diag(X∶,j)φ′(gt)X∥
≤2 max
∥v∥=1d
∑
i=1d
∑
j=1T
∑
t=1∥qi∥∥qj∥∥X∥2,∞∥diag(X∶,j)Xut∥∞
≤2 max
∥v∥=1d
∑
i=1d
∑
j=1T
∑
t=1∥qi∥∥qj∥∥X∥2,∞∥X∶,j∥∞∥Xut∥∞≤2d∥X∥2,∞∥X∥1,∞T
∑
t=1∥Xut∥∞,(54)
23Published in Transactions on Machine Learning Research (04/2024)
where in the last inequality we used Cauchy-Schwarz inequality. Similarly for Term IIwe can write:
max
∥v∥=1d
∑
i=1d
∑
j=1T
∑
t=1∥qi∥∥qj∥∥φ(gt)⊺X∶,j(Xut)⊺φ′(gt)X∥
≤2 max
∥v∥=1d
∑
i=1d
∑
j=1T
∑
t=1∥qi∥∥qj∥∣φ(gt)⊺X∶,j∣∥X∥2,∞∥Xut∥∞≤2d∥X∥2,∞∥X∥1,∞T
∑
t=1∥Xut∥∞,(55)
where we used Hölder’s inequality in the last inequality. And at the end, for Term IIIwe have:
max
∥v∥=1d
∑
i=1d
∑
j=1T
∑
t=1∥qi∥∥qj∥∥φ(gt)⊺XutX⊺
∶,jφ′(gt)X∥
≤2 max
∥v∥=1d
∑
i=1d
∑
j=1T
∑
t=1∥qi∥∥qj∥∣φ(gt)⊺Xut∣∥X∥2,∞∥X∶,j∥∞≤2d∥X∥2,∞∥X∥1,∞T
∑
t=1∥Xut∥∞.(56)
Then, we upper-bound Term 2:
2 max
∥v∥=1d
∑
j=1T
∑
t=1p⊺
txtX⊺
∶,jφ′(gt)Xqj≤2 max
∥v∥=1d
∑
j=1T
∑
t=1∥pt∥∥qj∥∥X∥2,∞∥X⊺
∶,jφ′(gt)X∥
≤4√
Td
2∥X∥2
2,∞∥X∥1,∞=2√
Td∥X∥2
2,∞∥X∥1,∞.(57)
The last inequality comes from the calculation of the gradient concluded in Eq. (42). Plugging in Eqs. (54),
(55), (56), and (57) in Eq. (52), we conclude that
∥∇2
θΦ(X;θ)∥≤6d∥X∥2
2,∞∥X∥2
1,∞T
∑
t=1∥Xut∥∞+2√
Td∥X∥2
2,∞∥X∥1,∞. (58)
Applying Assumption 1, we have:
∥∇2
θΦ(X;θ)∥≤6dR4T
∑
t=1∥Xut∥∞+2√
TdR3, (59)
or
∥∇2
θΦ(X;θ)∥≤6dR4T
∑
t=1max
τ∈[T]x⊺
τut+2√
TdR3
≤6dR4T
∑
t=1max
τ∈[T]∥xτ∥∥ut∥+2√
TdR3
≤6√
TdR5∥U∥F+2√
TdR3, (60)
where in the second and the last inequalities we used Cauchy-Schwartz inequality.
We now derive bounds for the multi-head attention model.
Lemma 5. Recall the multihead model in Eq. (2).
̃Φ(X;̃θ)=1√
HH
∑
h=1⟨Uh,φ(XWhX⊺)X⟩. (61)
The following are true under Assumption 1.
1.∥∇̃θ̃Φ(X;̃θ)∥≤√
TR(2R2max
h∈[H]∥Uh∥F+1).
24Published in Transactions on Machine Learning Research (04/2024)
2.∥∇2
̃θ̃Φ(X;̃θ)∥≤2√
TdR3
√
H(3√
dR2max
h∈[H]∥Uh∥F+1).
Proof.From Equation (46) for single-head attention,
∥∇̃θ̃Φ(X;̃θ)∥2≤1
HH
∑
h=1(2√
TR3∥Uh∥F+√
TR)2
≤(2√
TR3max
h∈[H]∥Uh∥F+√
TR)2
. (62)
Denotev⊺=[v⊺
1⋯v⊺
H]∈RH(Td+d2), where ∥v∥=1. Using Equation (60),
⟨v,∇2
̃θ̃Φ(X;̃θ)v⟩≤1√
H(6√
TdR5max
h∈[H]∥Uh∥F+2√
TdR3)H
∑
h=1∥vh∥2
=1√
H(6√
TdR5max
h∈[H]∥Uh∥F+2√
TdR3). (63)
A.2 Proof of Corollary 1: Objective’s Gradient/Hessian
Corollary 1 follows immediately by the result of Lemma 6 below and using a more relaxed bound
maxh∈[H]∥Uh∥F≤∥̃θ∥2,∞.
Lemma 6 (Tight version of Corollary 1) .Let Assumption 1 hold and we use logistic loss function. Then, the
following are true for the loss gradient and Hessian:
1.∥∇̂L(̃θ)∥≤√
TR(2R2max
h∈[H]∥Uh∥F+1)̂L(̃θ).
2.∥∇2̂L(̃θ)∥≤2√
TdR3
√
H(3√
dR2max
h∈[H]∥Uh∥F+1)+TR2
4(2R2max
h∈[H]∥Uh∥F+1)2
.
3.λmin(∇2̂L(̃θ))≥−2√
TdR3
√
H(3√
dR2max
h∈[H]∥Uh∥F+1)̂L(̃θ).
Proof.The loss gradient is derived as follows,
∇̂L(̃θ)=1
nn
∑
i=1ℓ′(yĩΦ(Xi;̃θ))yi∇̃θ̃Φ(Xi;̃θ).
Recalling that yi∈{±1}, we can write
∥∇̂L(̃θ)∥=1
n∥n
∑
i=1ℓ′(yĩΦ(Xi;̃θ))yi∇̃θ̃Φ(Xi;̃θ)∥≤1
nn
∑
i=1∣ℓ′(yĩΦ(Xi;̃θ))∣∥∇̃θ̃Φ(Xi;̃θ)∥.
Thus, using Lemma 5 to bound the norm of the model’s gradient:
∥∇̂L(̃θ)∥≤√
TR(2R2max
h∈[H]∥Uh∥F+1)̂L(̃θ). (64)
For the Hessian of loss, note that
∇2̂L(̃θ)=1
nn
∑
i=1ℓ′′(yĩΦ(Xi;̃θ))∇̃θ̃Φ(Xi;̃θ)∇̃θ̃Φ(Xi;̃θ)⊺+ℓ′(yĩΦ(Xi;̃θ))yi∇2
̃θ̃Φ(Xi;̃θ).(65)
25Published in Transactions on Machine Learning Research (04/2024)
It follows that
∥∇2̂L(̃θ)∥=∥1
nn
∑
i=1ℓ′′(yĩΦ(Xi;̃θ))∇̃θ̃Φ(Xi;̃θ)∇̃θ̃Φ(Xi;̃θ)⊺+ℓ′(yiΦ(Xi;̃θ))yi∇2
̃θ̃Φ(Xi;̃θ)∥
≤1
nn
∑
i=1∣ℓ′(yĩΦ(Xi;̃θ))∣∥∇2
̃θ̃Φ(Xi;̃θ)∥+∣ℓ′′(yĩΦ(Xi;̃θ))∣∥∇̃θ̃Φ(Xi;̃θ)∥2
≤2√
TdR3
√
H(3√
dR2max
h∈[H]∥Uh∥F+1)+TR2
4(2R2max
h∈[H]∥Uh∥F+1)2
. (66)
To lower-bound the minimum eigenvalue of the Hessian of loss, note that ℓ(⋅)is convex and thus ℓ′′(⋅)≥0.
Therefore, the first term in (65)is positive semi-definite and the second term can be lower-bounded as follows,
λmin(∇2̂L(̃θ))≥−1
nn
∑
i=1∥ℓ′(yĩΦ(Xi;̃θ))yi∇2
̃θ̃Φ(Xi;̃θ)∥
≥−1
nn
∑
i=1∣ℓ′(yĩΦ(Xi;̃θ))∣∥∇2
̃θ̃Φ(Xi;̃θ)∥
≥−2√
TdR3
√
H(3√
dR2max
h∈[H]∥Uh∥F+1)̂L(̃θ).
B Training Analysis
B.1 Preliminaries
Throughout this section we drop the ̃⋅iñθand̃Φ(Xi;̃θ)as everything refers to the full model. Moreover,
̃θ(K)and̃θ(0)are denoted by θKandθ0.
The proof of both the training and generalization analysis follows the high-level steps outlined in Taheri
& Thrampoulidis (2023). However, our analysis focuses on the self-attention model, which differs from the
two-layer perceptron studied in the referenced work. Notably, we train both the attention weights and the
classifier head, whereas Taheri & Thrampoulidis (2023) assumes fixed outer layer weights. This introduces a
new challenge as the smoothness and curvature of the objective function at point θbecome dependent on θ
itself (see Corollary 1). Consequently, we make careful adjustments in the proof to account for this challenge.
Our analysis critically uses the following property of the loss objective from Corollary 1: ∀θ∶λmin(∇2̂L(θ))≥
−κ(θ)⋅̂L(θ),κ(θ)∶=β3(θ)√
H.Note from the definition of β3(⋅)that∀θ1,θ2∶maxθ∈[θ1,θ2]β3(θ)=β3(θ1)∨
β3(θ2).Thus, the above property of the loss implies the following local self-bounded weak convexity property
on the line [θ1,θ2]:
∀θ1,θ2∶min
θ∈[θ1,θ2]λmin(∇2̂L(θ))≥−β3(θ1)∨β3(θ2)√
H⋅̂L(θ). (67)
In turn, Equation (67)can be used to prove that the loss satisfies a generalized local quasi-convexity (GLQC)
property as formalized in the proposition below. The proposition is only a slight modification of (Taheri &
Thrampoulidis, 2023, Prop. 8). While, a direct application of their result is not possible since the self-bounded
weak convexity property in (67)holds only locally on the line [θ1,θ2], an inspection of their proof shows
that this is sufficient.
B.2 Proof of Proposition 4
First, we restate the proposition below for the reader’s convenience.
Proposition 6 (GLQC property) .Letθ1,θ2be points sufficiently close to each other, such that
2(β3(θ1)∨β3(θ2))∥θ1−θ2∥2≤√
H.
26Published in Transactions on Machine Learning Research (04/2024)
Then, the following generalized local quasi-convexity (GLQC) property holds:
max
θ∈[θ1,θ2]̂L(θ)≤4
3(̂L(θ1)∨̂L(θ2)).
Proof.Although the loss ̂Lis not uniformly self-bounded weakly convex as assumed in (Taheri & Thram-
poulidis, 2023, Prop. 8), an inspection of their proof shows that for every θ1,θ2it suffices that the loss is locally
self-bounded weakly convex. With this observation, we can apply their proposition with κ←κ(θ1)∨κ(θ2),
which gives the desired.
B.3 Key Lemmas
The proof of Theorem 1 consists of several intermediate lemma, which we state and prove in this section.
Lemma 7 (Descent Lemma) .Let Assumption 1 hold. Then, for any iteration k≥0we have step-wise
descent:
̂L(θk+1)≤̂L(θk)−η
2∥∇̂L(θk)∥2, (68)
providedη≤1
ρkwhereρkis the objective’s local smoothness parameter defined as below,
ρk∶=β2(θk)∨β2(θk+1).
Proof.By Taylor’s expansion, there exists a θ′∈[θk,θk+1]such that,
ˆL(θk+1)=̂L(θk)+⟨∇̂L(θk),θk+1−θk⟩+1
2⟨θk+1−θk,∇2̂L(θ′)(θk+1−θk)⟩
≤̂L(θk)+⟨∇̂L(θk),θk+1−θk⟩+1
2max
θ′∈[θk,θk+1]∥∇2̂L(θ′)∥⋅∥θk+1−θk∥2
≤̂L(θk)−η∥∇̂L(θk)∥2+η2ρk
2⋅∥∇̂L(θk)∥2,
where the last step follows from Corollary 1, and using maxθ′∈[θk,θk+1]β2(θ′)=β2(θ)∨β2(θk)=ρk. For
η≤1
ρk, we conclude the claim.
Lemma 8. Assumeη>0such that step-wise descent (68)holds for all k∈[K−1]. Then, for any θ, the
following holds:
1
KK
∑
k=1̂L(θk)≤̂L(θ)+∥θ−θ0∥2
2ηK+1
2KK−1
∑
k=0τk∥θ−θk∥2, (69)
whereτk∶=1√
H(β3(θ)∨β3(θk))maxα∈[0,1]̂L(θkα).
Proof.By Taylor’s theorem, for θkα∶=αθk+(1−α)θ, α∈[0,1]we know that:
̂L(θ)≥̂L(θk)+⟨∇̂L(θk),θ−θk⟩+1
2λmin(∇2̂L(θkα))∥θ−θk∥2.
Using (68), we get:
̂L(θk+1)≤̂L(θ)+⟨∇̂L(θk),θk−θ⟩−1
2λmin(∇2̂L(θkα))∥θ−θk∥2−η
2∥∇̂L(θk)∥2
≤̂L(θ)+∥θ−θk∥2
2η−∥θ−θk+1∥2
2η+1
2τk∥θ−θk∥2, (70)
where the last step follows by completion of squares using θk+1−θk=−η∇̂L(θk), and also uses Corollary
1 withτk∶=maxα∈[0,1]τkα, whereτkα=β3(θkα)√
ĤL(θkα). Telescoping in (70)fork=0,...,K−1, we get the
desired.
27Published in Transactions on Machine Learning Research (04/2024)
Next, we use the generalized local quasi-convexity property (Proposition 4) to obtain explicit regret bound
from Lemma 8.
Lemma 9. Suppose the assumptions of Lemma 8 hold. Moreover, assume for all k∈[K−1]it holds that√
H≥2(β3(θ)∨β3(θk)) ∥θ−θk∥2. Then,
1
KK
∑
k=1̂L(θk)≤2̂L(θ)+3∥θ−θ0∥2
4ηK+̂L(θ0)
2K. (71)
Proof.We have minθ′∈[θ,θk]λmin(∇2̂L(θ′))≥−β3(θ)∨β3(θk)√
Hmaxθ′∈[θ,θk]̂L(θ′)from Corollary 1. Thus, using
Proposition 4, we can control maxα∈[0,1]̂L(θkα). Specifically, by assumption we have for all k∈[K−1]that
√
H≥2(β3(θ)∨β3(θk)) ∥θ−θk∥2=2 max
α∈[0,1]β3(θkα)∥θ−θk∥2.
Then, by Proposition 4, we have
max
α∈[0,1]̂L(θkα)≤4
3max{̂L(θk),̂L(θ)}≤4
3̂L(θk)+4
3̂L(θ).
Thus, applying this to (69) we have:
1
KK
∑
k=1̂L(θk)≤̂L(θ)+∥θ−θ0∥2
2ηK+2
3KK−1
∑
k=01√
H(β3(θ)∨β3(θk))(̂L(θk)+̂L(θ))∥θ−θk∥2
≤̂L(θ)+∥θ−θ0∥2
2ηK+1
3KK−1
∑
k=0(̂L(θk)+̂L(θ))
≤4
3̂L(θ)+∥θ−θ0∥2
2ηK+1
3KK
∑
k=0̂L(θk), (72)
where we use the condition on Hin the second inequality. Rearranging terms above we conclude the claim of
the lemma.
Lemma 10 (Iterates-norm bound) .Suppose the descent property (68)holds∀k∈[K−1], and let Assumption
1 hold. Further, assume that
∥θ−θ0∥2≥max{ηK̂L(θ),η̂L(θ0)}. (73)
and
√
H≥36√
TdR3(3√
dR2(3∥θ−θ0∥+∥θ∥2,∞)+1) ∥θ−θ0∥2, (74)
Then, for all k∈[K],
∥θk−θ∥≤3∥θ−θ0∥. (75)
Proof.DenoteAk=∥θk−θ∥. Start by recalling from Eq. (70) that for all k:
A2
k+1≤A2
k+2η̂L(θ)−2η̂L(θk+1)+η(max
α∈[0,1]τkα)A2
k, (76)
where recall that τkα=β3(θkα)̂L(θkα)√
H. We will prove the desired statement (75)using induction. For k=0,
A0=∥θ−θ0∥. Thus, the assumption of induction holds. Now assume (75)is correct for j∈[k−1], i.e.
Aj≤3∥θ−θ0∥,∀j∈[k−1]. We will then prove it holds for k.
28Published in Transactions on Machine Learning Research (04/2024)
By induction hypothesis for all j∈[k−1],and allα∈[0,1]:
√
H≥36√
TdR3(3√
dR2(3∥θ−θ0∥+∥θ∥2,∞)+1) ∥θ−θ0∥2
≥4√
TdR3(3√
dR2(3∥θ−θ0∥+∥θ∥2,∞)+1) ∥θ−θj∥2
≥4√
TdR3(3√
dR2(∥θ−θj∥2,∞+∥θ∥2,∞)+1) ∥θ−θj∥2
≥4√
TdR3(3√
dR2∥θjα∥2,∞+1) ∥θ−θj∥2=2β3(θjα)∥θ−θj∥2.
Thus, by Proposition 4, ∀j∈[k−1]
max
α∈[0,1]̂L(θjα)≤4
3̂L(θj)+4
3̂L(θ). (77)
Using this in (76) we find for all j∈[k−1],
A2
j+1≤A2
j+2η̂L(θ)−2η̂L(θj+1)+ηmaxα∈[0,1]β3(θjα)A2
j√
H(4
3̂L(θj)+4
3̂L(θ))
≤A2
j+2η̂L(θ)−2η̂L(θj+1)+η(2
3̂L(θj)+2
3̂L(θ)),
where in the second inequality we used (77). We proceed by telescoping the above display over j=0,1,...,k−1
to get
A2
k≤A2
0+8
3ηk̂L(θ)+2
3η̂L(θ0)−4
3ηk−1
∑
j=1̂L(θj)−2η̂L(θk)
≤A2
0+8
3ηk̂L(θ)+2
3η̂L(θ0)
≤∥θ−θ0∥2+8
3∥θ−θ0∥2+2
3∥θ−θ0∥2
≤9∥θ−θ0∥2, (78)
where the second line follows by non-negativity of the loss. Thus, Ak≤3∥θ−θ0∥. This completes the induction
and proves the lemma.
B.4 Proof of Theorem 1
We restate the theorem here for convenience, this time also including exact constants.
Theorem 3 (Restatement of Thm. 1) .Fix anyθandHsatisfying
√
H≥36√
TdR3(3√
dR2(3∥θ−θ0∥+∥θ∥2,∞)+1) ∥θ−θ0∥2.
Further, denote for convenience
α(θ)∶=3√
dR2[3√
TR3(3∥θ−θ0∥+∥θ∥2,∞)+2√
TR]
andρ(θ)=(2√
TdR3
√
H+TR2
4)α(θ)2.Then, for any step-size η≤1∧1/ρ(θ)∧∥θ−θ0∥2
K̂L(θ)∧∥θ−θ0∥2
̂L(θ0), the following
bounds hold for the training loss and the weights’ norm at iteration Kof GD:
̂L(θK)≤1
KK
∑
k=1̂L(θk)≤2̂L(θ)+5∥θ−θ0∥2
4ηKand ∥θK−θ0∥≤4∥θ−θ0∥. (79)
29Published in Transactions on Machine Learning Research (04/2024)
Proof.Define
αη(θ)=3√
dR2[(2η√
TR3+1)(3∥θ−θ0∥+∥θ∥2,∞)+η√
TR+1],
and
ρη(θ)∶=(2√
TdR3
√
H+TR2
4)αη(θ)2.
From Lemma 7 recall that:
ρk∶=β2(θk)∨β2(θk+1).
Now, recalling the definition of β2(θ)from Corollary 1, we see that ρk, the objective’s smoothness parameter
at stepkdepends on ∥θk∥2,∞∨∥θk+1∥2,∞, where:
∥θk∥2,∞∨∥θk+1∥2,∞≤(∥θ−θk∥+∥θ∥2,∞)∨(∥θ−θk+1∥+∥θ∥2,∞)
≤(∥θ−θk∥+∥θ∥2,∞)∨(∥θ−θk∥+∥θk+1−θk∥+∥θ∥2,∞)
=∥θ−θk∥+∥θk+1−θk∥+∥θ∥2,∞
≤η√
TR(2R2∥θk∥2,∞+1)+∥θ−θk∥+∥θ∥2,∞
≤(2η√
TR3+1)(∥θ−θk∥+∥θ∥2,∞)+η√
TR=∶Θk,
where the second-last inequality follows from Corollary 1. For each ρk, define corresponding ρη(Θk)∶=
2√
TdR3
√
H(3√
dR2Θk+1)+TR2
4(2R2Θk+1)2. Consider ρ0, it is easy to see that ρ0≤ρη(Θ0)≤ρη(θ).
Hence, the descent property of GD holds in first iteration as per Lemma 7. Since, for η≤1,ρη(θ)≤ρ(θ).
Thus,η≤1∨1
ρ(θ)/Leftr⫯g⊸tl⫯ne⇒η≤1
ρ(θ)≤1
ρη(θ)≤1
ρη(Θ0)≤1
ρ0. Moreover, note that the assumptions of Lemma 10
are satisfied. Thus, by induction over Lemmas 9-10 and noting that ρk≤ρη(Θk)≤ρη(θ)for allk∈[K−1]
by using a similar argument as above, we obtain for any η≤1
ρ(θ),
∀k∈[K]∶∥θk−θ∥≤3∥θ−θ0∥,
and1
KK
∑
k=1̂L(θk)≤2̂L(θ)+3∥θ−θ0∥2
4ηK+̂L(θ0)
2K. (80)
Moreover, by assumptions of the theorem we immediately find that1
2K̂L(θ0)≤∥θ−θ0∥2
2ηK. We also have
∥θk−θ0∥≤∥θ−θk∥+∥θ−θ0∥≤4∥θ−θ0∥. This completes the proof.
The training proof is summarized in Figure 1.
B.5 Corollary 3
Corollary 3 (Training loss under realizability) .Let Assumptions 1 and 2 hold. Fix K≥1. Assume any H
such that
√
H≥36√
TdR3(3√
dR2(3g0(1
K)+g(1
K))+1)g0(1
K)2. (81)
Further, denote for convenience
α(K)∶=3√
dR2[3√
TR3(3g0(1
K)+g(1
K))+2√
TR]
andρ(K)=(2√
TdR3
√
H+TR2
4)α(K)2.Then, for any step-size η≤1∧1/ρ(K)∧g0(1)2∧g0(1)2
̂L(θ0), the following
bounds hold for the weights’ norm and objective at iteration Kof GD:
̂L(θK)≤2
K+5g0(1
K)2
4ηKand ∥θK−θ0∥≤4g0(1
K). (82)
30Published in Transactions on Machine Learning Research (04/2024)
Figure 1: Training proof schema.
Proof.According to Assumption 2, for any sufficiently small ε>0, there exists a θ(ε)such that ̂L(θ(ε))≤ε
and∥θ(ε)−θ0∥=g0(ε). Pickε=1
K. Let the step-size η>0, satisfy the assumption of Descent Lemma 7.
SincêL(θ(1/K))≤1
K, we have:
∥θ(1/K)−θ0∥2
K̂L(θ(1/K))≥∥θ(1/K)−θ0∥2=g0(1
K)2≥g0(1)2,
and
∥θ(1/K)−θ0∥2
̂L(θ0)=g0(1
K)2
̂L(θ0)≥g0(1)2
̂L(θ0).
Therefore, following our assumption on step-size η, we can conclude that
η≤g0(1)2∧g0(1)2
̂L(θ0)≤∥θ(1/K)−θ0∥2
K̂L(θ(1/K))∧∥θ(1/K)−θ0∥2
̂L(θ0). (83)
where in the second inequality we used the fact that g0(⋅)is a non-increasing function. The desired result is
obtained by Theorem 1.
C Generalization Analysis
Throughout this section we drop the ̃⋅iñθand̃Φ(Xi;̃θ)as everything refers to the full model. Moreover,
̃θ(K)and̃θ(0)are denoted by θKandθ0.
For the stability analysis below, recall the definition of the leave-one-out (loo) training loss for i∈[n]and
note that by denoting ℓj(θ)∶=ℓ(yjΦ(Xj;θ))to be thej-th sample loss: ̂L¬i(θ)∶=1
n∑j≠iℓj(θ). With these,
define the loo model updates of GD on the loo loss for some η>0:
θ¬i
k+1∶=θ¬i
k−η∇̂L¬i(θ¬i
k), k≥0,θ¬i
0=θ0.
31Published in Transactions on Machine Learning Research (04/2024)
Lemma 11. Assume the conditions of Theorem 1 hold and
√
H≥256√
TdR3(3√
dR2(3∥θ−θ0∥+∥θ∥2,∞)+1) ∥θ−θ0∥2. (84)
Then, the on-average model stability at iteration Kof GD satisfies,
1
nn
∑
i=1∥θK−θ¬i
K∥≤2η
n(√
TR(2R2(3∥θ−θ0∥+∥θ∥2,∞)+1)) (2K̂L(θ)+9∥θ−θ0∥2
4η).
Proof.First recall from Corollary 1 that gradient and hessian’s norm satisfy
∥∇̂L(θ)∥≤β1(θ)̂L(θ),
∥∇2̂L(θ)∥≤β2(θ),
λmin(∇2̂L(θ))≥−β3(θ)√
ĤL(θ).
Applying (Taheri & Thrampoulidis, 2023, Lemma B.1.), two arbitrary points θ,θ′satisfy the following
GD-expansiveness inequality:
∥(θ−η∇̂L(θ))−(θ′−η∇̂L(θ′))∥≤max
α∈[0,1]{(1+ηβ3(θα)√
ĤL(θα))∨ηβ2(θα)}∥θ−θ′∥,(85)
whereθα=αθ+(1−α)θ′denotes a point parameterized by α∈[0,1]in the line segment between θandθ′.
We aim to bound the on-average model stability in the r.h.s of the inequality in Lemma 3. Based on Eq. (85),
∥θk+1−θ¬i
k+1∥=∥(θk−η∇̂L¬i(θk))−(θ¬i
k−η∇̂L¬i(θ¬i
k))−η
n∇ℓi(θk)∥
≤∥(θk−η∇̂L¬i(θk))−(θ¬i
k−η∇̂L¬i(θ¬i
k))∥+η
n∥∇ℓi(θk)∥
≤∥(θk−η∇̂L¬i(θk))−(θ¬i
k−η∇̂L¬i(θ¬i
k))∥+ηβ1(θk)
nℓi(θk)
≤(max
α∈[0,1]{(1+ηβ3(θ¬i
kα)√
ĤL¬i(θ¬i
kα))∨ηβ2(θ¬i
kα)})∥θk−θ¬i
k∥+ηβ1(θk)
nℓi(θk).(86)
In the above we denoted θ¬i
kα∶=αθk+(1−α)θ¬i
k. We note that based on our guarantees for the weights’ norm
during training (75) it can be deduced that for all α∈[0,1],
β3(θ¬i
kα)=2√
TdR3(3√
dR2∥αθk+(1−α)θ¬i
k∥2,∞+1)
=2√
TdR3(3√
dR2(∥θk∥2,∞∨∥θ¬i
k∥2,∞)+1)
≤2√
TdR3(3√
dR2((∥θk−θ∥+∥θ∥2,∞)∨(∥θ¬i
k−θ∥+∥θ∥2,∞)+1)
≤2√
TdR3(3√
dR2(3∥θ−θ0∥+∥θ∥2,∞)+1)=∶˜β3(θ). (87)
Similarly, we obtain
β2(θ¬i
kα)=2√
TdR3
√
H(3√
dR2∥θ¬i
kα∥2,∞+1)+TR2
4(2R2∥θ¬i
kα∥2,∞+1)2
≤2√
TdR3
√
H(3√
dR2(3∥θ−θ0∥+∥θ∥2,∞)+1)
+TR2
4(2R2(3∥θ−θ0∥+∥θ∥2,∞)+1)2
=∶˜β2(θ). (88)
32Published in Transactions on Machine Learning Research (04/2024)
Hence, by the notation introduced above and noting that by our assumption on the step-size it holds
η≤1/˜β2(θ), we can rewrite Eq. (86) as follows,
∥θk+1−θ¬i
k+1∥≤((1+η˜β3(θ)√
H)max
α∈[0,1]̂L¬i(θ¬i
kα))∥θk−θ¬i
k∥+ηβ1(θk)
nℓi(θk).
Assume
√
H≥128˜β3(θ)∥θ−θ0∥2≥4˜β3(θ)(∥θk−θ0∥2+∥θ¬i
k−θ0∥2)≥2˜β3(θ)∥θk−θ¬i
k∥2
≥2(β3(θk)∨β3(θ¬i
k))∥θk−θ¬i
k∥2,
where we used Theorem 1 in the first inequality. We also have
min
α∈[0,1]λmin(∇2̂L¬i(θ¬i
kα))≥−β3(θk)∨β3(θ¬i
k)√
ĤL¬i(θ¬i
kα).
Thus, by applying Proposition 4 on the leave-one-out loss, it holds that for all α∈[0,1],
max
α∈[0,1]̂L¬i(θ¬i
kα)≤4
3(̂L¬i(θk)+̂L¬i(θ¬i
k)).
Thus,
∥θk+1−θ¬i
k+1∥≤((1+4η˜β3(θ)
3√
H)⋅(̂L¬i(θk)+̂L¬i(θ¬i
k)))∥θk−θ¬i
k∥+ηβ1(θk)
nℓi(θk). (89)
In order to remove the dependence on k, note that
β1(θk)≤√
TR(2R2∥θk∥2,∞+1)
≤√
TR(2R2(∥θ−θk∥2,∞+∥θ∥2,∞)+1)
≤√
TR(2R2(∥θ−θk∥+∥θ∥2,∞)+1)
≤√
TR(2R2(3∥θ−θ0∥+∥θ∥2,∞)+1)=∶˜β1(θ). (90)
For simplicity of exposition, denote αk,i∶=4η˜β3(θ)
3√
H(̂L¬i(θk)+̂L¬i(θ¬i
k)). Then by unrolling the iterates we
have,
∥θk+1−θ¬i
k+1∥≤(1+αk,i)∥θk−θ¬i
k∥+η˜β1(θ)
nℓi(θk)
≤(1+αk,i)(1+αk−1,i)∥θk−1−θ¬i
k−1∥+(1+αk,i)η˜β1(θ)
nℓi(θk−1)+η˜β1(θ)
nℓi(θk)
≤k
∑
j=1k
∏
l=j(1+αl,i)η˜β1(θ)ℓi(θj−1)
n+η˜β1(θ)ℓi(θk)
n
≤k
∑
j=1exp(k
∑
l=jαl,i)η˜β1(θ)ℓi(θj−1)
n+η˜β1(θ)ℓi(θk)
n
≤k
∑
j=1exp(k
∑
l=1αl,i)η˜β1(θ)ℓi(θj−1)
n+η˜β1(θ)ℓi(θk)
n, (91)
33Published in Transactions on Machine Learning Research (04/2024)
where in the above we used that θ0=θ¬i
0in unrolling the iterates as well as the fact that for x≥0∶1+x≤ex.
Note that by definition ̂L¬i(θk)≤̂L(θk). By training loss guarantees from Eq. (79), we have
k
∑
l=1αl,i≤4η˜β3(θ)
3√
Hk
∑
l=1(̂L¬i(θl)+̂L¬i(θ¬i
l))
≤4η˜β3(θ)
3√
Hk
∑
l=1(̂L(θl)+̂L(θ¬i
l))
≤4η˜β3(θ)
3√
H(5k̂L(θ)+5∥θ−θ0∥2
2η)
≤10˜β3(θ)∥θ−θ0∥2
√
H
≤1
10,
where the last step stems from the condition on√
H. Proceeding from Eq. (91), we find that for the last
iterate
∥θK−θ¬i
K∥≤2η˜β1(θ)K−1
∑
k=1ℓi(θk−1)
n+η˜β1(θ)ℓi(θK−1)
n
≤2η˜β1(θ)K−1
∑
k=0ℓi(θk)
n. (92)
It follows that the on-average model stability satisfies,
1
nn
∑
i=1∥θK−θ¬i
K∥≤2η˜β1(θ)
n2n
∑
i=1K−1
∑
k=0ℓi(θk)
=2η˜β1(θ)
nK−1
∑
k=0̂L(θk).
Applying our training loss guarantees from Eq. (80) to the r.h.s. above yields,
1
nn
∑
i=1∥θK−θ¬i
K∥≤2η˜β1(θ)
n(2K̂L(θ)+9∥θ−θ0∥2
4η),
where here we used the assumption that, ̂L(θ0)≤∥θ−θ0∥2/ηto simplify the final result. This completes the
proof.
C.1 Proof of Theorem 2
We restate the theorem here for convenience, this time also including exact constants.
Theorem 4 (Restatement of Thm. 2) .Fix anyθandHsatisfying
√
H≥256√
TdR3(3√
dR2(3∥θ−θ0∥+∥θ∥2,∞)+1) ∥θ−θ0∥2.
Further, denote for convenience
α(θ)∶=3√
dR2[3√
TR3(3∥θ−θ0∥+∥θ∥2,∞)+2√
TR]
andρ(θ)=(2√
TdR3
√
H+TR2
4)α(θ)2.Then, for any step-size η≤1∧1/ρ(θ)∧∥θ−θ0∥2
K̂L(θ)∧∥θ−θ0∥2
̂L(θ0), the expected
generalization gap at iteration Ksatisfies,
E[L(̃θ(K))−̂L(̃θ(K))]≤4
nE[2K̂L(̃θ)+9∥̃θ−̃θ(0)∥2
4η]. (93)
34Published in Transactions on Machine Learning Research (04/2024)
Proof.Note that the assumptions of Lemma 11 are satisfied. Moreover, as per Corollary 1 the objective is
Lipschitz at all iterates with parameter ˜β1(θ)since∀k∈[K]∶∥θk∥2,∞≤3∥θ−θ0∥+∥θ∥2,∞. Thus, by Lemma
3 and Lemma 11 we have,
E[L(θK)−̂L(θK)]≤4
nE[η(˜β1(θ))2(2K̂L(θ)+9∥θ−θ0∥2
4η)]. (94)
Recalling the condition on step-size η≤1
ρ(θ)≤1
(˜β1(θ))2concludes the proof.
C.2 Corollary 4
Corollary4 (Generalizationlossunderrealizability) .Let boundedness Assumption 1 hold. Also let realizability
assumption 2 holds almost surely over the data distribution. Fix K≥1. Assume any Hsuch that
√
H≥256√
TdR3(3√
dR2(3g0(1
K)+g(1
K))+1)g0(1
K)2. (95)
Let the step-size satisfy η≤1∧1/ρ(K)∧g0(1)2∧g0(1)2
̂L(θ0)whereρ(K)is as defined in Corollary 3. Then the
expected generalization gap at iteration Kof GD satisfies,
E[L(θK)−̂L(̃θK)]≤17g0(1
K)2
ηn. (96)
Proof.According to Assumption 2, for any sufficiently small ε>0, there exists a θ(ε)such that ̂L(θ(ε))≤ε
and∥θ(ε)−̃θ0∥=g0(ε). Pickε=1
K. Let the step-size, η>0satisfies the assumption of Descent Lemma 7.
SincêL(θ(1/K))≤1
K, we have:
∥θ(1/K)−θ0∥2
K̂L(θ(1/K))≥∥θ(1/K)−θ0∥2=g0(1
K)2≥g0(1)2
and
∥θ(1/K)−θ0∥2
̂L(θ0)=g0(1
K)2
̂L(θ0)≥g0(1)2
̂L(θ0).
Therefore, we can conclude that
η≤g0(1)2∧g0(1)2
̂L(θ0)≤∥θ(1/K)−θ0∥2
K̂L(θ(1/K))∧∥θ(1/K)−θ0∥2
̂L(θ0). (97)
where in the second inequality we used the fact that g0(⋅)is a non-increasing function. The desired result is
obtained by Theorem 2 and the fact that
K̂L(θ(1/K))≤∥θ(1/K)−θ0∥2
η=g0(1
K)2
η.
C.3 From good initialization to realizability
The proposition below shows that starting from a goodinitialization we can always find θ(ϵ)satisfying the
realizability Assumption 2 provided the number of heads is large enough.
Proposition 7 (From goodinitialization to realizability) .Suppose goodinitialization θ0as per Definition 1.
Fix any 1≥ε>0and let
√
H≥5√
TdR3B2
BΦ⋅(3√
dR2+1)⋅(2BΦ+log(1/ε)
γ)2
⋅(1∨2BΦ+log(1/ε)
γ). (98)
Then, the realizability Assumption 2 holds with g0(ε)=1
γ(2BΦ+log(1/ε))andg(ϵ)=B2+g0(ϵ).
35Published in Transactions on Machine Learning Research (04/2024)
Proof.By Taylor expansion there exists θ′∈[θ,θ0]such that,
yiΦ(Xi;θ)=yiΦ(Xi;θ0)+yi⟨∇Φ(Xi;θ0),θ−θ0⟩+1
2yi⟨θ−θ0,∇2Φ(Xi;θ′)(θ−θ0)⟩.(99)
Pick
θ∶=θ(ε)=θ0+2BΦ+log(1/ε)
γθ⋆.
Substituting this in (99) and using that θ0is agoodinitialization, we obtain for all i∈[n]:
yiΦ(Xi;θ)≥−∣yiΦ(Xi;θ0)∣+(2BΦ+log(1/ε))−1
2∥∇2Φ(Xi;θ′)∥2∥θ−θ0∥2
≥−BΦ+(2BΦ+log(1/ε))−1
2∥∇2Φ(Xi;θ′)∥2(2BΦ+log(1/ε)
γ)2
. (100)
To continue, we show in Lemma 5 in the appendix that ∥∇2Φ(Xi;θ′)∥2≤β3(θ′)/√
Hwhere recall β3(θ)∶=
2√
TdR3(3√
dR2∥θ∥2,∞+1)defined in Corollary 1. Now, note that
β3(θ′)≤β3(θ(ε))+β3(θ0)≤2BΦ+log(1/ε)
γ⋅β3(θ⋆)+2β3(θ0)
≤2√
2√
TdR3B2(3√
dR2+1)⋅(2+2BΦ+log(1/ε)
γ)
≤10√
TdR3B2(3√
dR2+1)⋅(1∨2BΦ+log(1/ε)
γ),
where the first two inequalities follow by triangle inequality and the inequality after those follows because
∥θ⋆∥2,∞≤∥θ⋆∥2≤√
2,1≤B2and also ∥θ0∥2,∞≤B2bygoodinitialization assumption.
Plugging in this bound in (100)and using the assumption on Hyields that yiΦ(Xi;θ)≥log(1/ϵ)for all
i∈[n].This in turn implies that ̂Li(θ)∶=ℓ(yiΦ(Xi;θ))≤log(1+ε)≤ε, and thus ̂L(θ)≤εas desired.
Furthermore, note by definition of θ(ϵ)thatg0(ϵ)=2BΦ+log(1/ε)
γandg(ϵ)=B2+g0(ϵ).For the latter, we
used triangle inequality and the rough bound ∥θ⋆∥2,∞≤∥θ⋆∥2≤√
2. This completes the proof.
C.4 Proof of Corollary 2
We restate the corollary here for convenience, this time also including exact constants.
Corollary 5 (Restatement of Cor. 2) .Suppose goodinitialization θ0and let
√
H≥256√
TdR3B2(3√
dR2(4g0(1
K)+B2)+1)g0(1
K)2,
whereg0(1
K)=2BΦ+log(K)
γ. Further, denote for convenience
α(K)∶=3√
dR2[3√
TR3(4g0(1
K)+B2)+2√
TR]
andρ(K)=(2√
TdR3
√
H+TR2
4)α(K)2.Then, for any fixed step-size
η≤1∧1/ρ(K)∧4B2
Φ
γ2⋅1
log(1+eBΦ),
the following bounds hold:
̂L(θK)≤2
K+5(2BΦ+log(K))2
4γ2ηKand E[L(θK)−̂L(θK)]≤17(2BΦ+log(K))2
γ2ηn.(101)
36Published in Transactions on Machine Learning Research (04/2024)
Proof.First, we prove that the given assumption on Hsatisfies the conditions of Proposition 7 for ε=1
K.
√
H≥256√
TdR3B2(3√
dR2(4g0(1
K)+B2)+1)g0(1
K)2
=256√
TdR3B2(3√
dR2(3g0(1
K)+g(1
K))+1)g0(1
K)2.
If1>g0(1
K),
√
H≥256√
TdR3B2(3√
dR2(4g0(1
K)+B2)+1)g0(1
K)2
≥5√
TdR3B2(3√
dR2+1)g0(1
K)2.
It means that the condition of Proposition 7 on√
His satisfied. Moreover, if 1≤g0(1
K),
√
H≥256√
TdR3B2(3√
dR2(4g0(1
K)+B2)+1)g0(1
K)2
≥256√
TdR3B2(12√
dR2g0(1
K))g0(1
K)2
≥5√
TdR3B2(3√
dR2+1)g0(1
K)3,
and again the condition of Proposition 7 on√
His satisfied. Then, we can apply the results of Corollaries 3
and 4 for a fixed Kwhich satisfies K≥1. Note that
g0(1)=2BΦ
γ,and̂L(θ0)≤log(1+eBΦ).
Thus, the condition on step-size simplifies to η≤1∧1/ρ(K)∧4B2
Φ
γ2⋅1
log(1+eBΦ). This completes the proof.
D Proofs for Section 5
D.1 Useful facts
Fact 1. Letx∈Rdbe subgaussian vector with ∥x∥ψ2≤K. Then, for any δ∈(0,1)and absolute constant
C>0it holds with probability at least 1−δthat∥x∥2≤CK(√
d+√
log(1/δ)).
Fact 2. SupposeXh,h∈[H]areIIDrealizations of random variable Xfor whichE[X]=µand∣X∣≤B
almost surely. Then, for any δ∈(0,1), with probability at least 1−δit holds that
⌟⟨rro⟪⟪⟩r⟪⌟⟨rro⟪⟪⟩r⟪⌟⟨rro⟪⟪⟩r⟪⌟⟨rro⟪⟪⟩r⟪⌟⟨rro⟪⟪⟩r⟪⌟⟨rro⟪⟪⟩r⟪⌟⟨rro⟪⟪⟩r⟪⌟⟨rro⟪⟪⟩r⟪⌟⟨rro⟪⟪⟩r⟪⌟⟨rro⟪⟪⟩r⟪⌟⟨rro⟪⟪⟩r⟪⌟⟨rro⟪⟪⟩r⟪1
H∑
h∈[H]Xh−µ⌟⟨rro⟪⟪⟩r⟪⌟⟨rro⟪⟪⟩r⟪⌟⟨rro⟪⟪⟩r⟪⌟⟨rro⟪⟪⟩r⟪⌟⟨rro⟪⟪⟩r⟪⌟⟨rro⟪⟪⟩r⟪⌟⟨rro⟪⟪⟩r⟪⌟⟨rro⟪⟪⟩r⟪⌟⟨rro⟪⟪⟩r⟪⌟⟨rro⟪⟪⟩r⟪⌟⟨rro⟪⟪⟩r⟪⌟⟨rro⟪⟪⟩r⟪≤2B√
log(1/δ)
2H.
D.2 Proof of Proposition 2
Recall
̃Φ(X;̃θ)=1√
H∑
h∈[H]Φh(X;Wh,Uh)=1√
H∑
h∈[H]⟨Uh,φ(XWhX⊺)X⟩,
wherẽθ=concat(θ1,θ2,...,θH)denotes the trainable parameters. After completing the first phase of training,
the initialization for the second phase is as follows for all h∈[H]:
θ(1)
h=concat(U(1)
h,W(1)
h)∶W(1)
h=0,U(1)
h=αh⎛
⎝ζ
21Tu⊺
⋆+1Tp⊺⎞
⎠, (102)
37Published in Transactions on Machine Learning Research (04/2024)
where recall that αh∼Unif(±1)and from Lemma 2 it holds with probability 1−δthat∥p∥≤Pwhere the
parameterPis defined in (6). Onwards, we condition on this good event.
We prove the three properties P1,P2, andP3in the order stated below.
D.2.1 Proof of P1: Bounded norm per head
This is straightforward by noting that for all h∈[H]:
∥θ(1)
h∥2=∥U(1)
h∥F≤ζ
2√
T∥u⋆∥2+√
T∥p∥2≤ζ
2√
T√
2S+√
TP.
D.2.2 Proof of P2: Bounded initialization
Lemma 12 (Initialization bound) .Let anyXsampled from DM1 and satisfying Assumption 3. Given the
initialization in (102), for anyδ∈(0,1)it holds with probability at least 1−δthat
∣̃Φ(X;̃θ(1))∣≤TR(S+P)√
2 log(1/δ).
Proof.
̃Φ(X;̃θ(1))=1√
H∑
h∈[H]Φh(X;θ(1)
h)=1√
H∑
h∈[H]⟨U(1)
h,φ(XW(1)
hX⊺)X⟩.
Using the initialization in (102) and recalling φ(0)=1T1⊺
T/T, we have
1√
H∑
h∈[H]⟨U(1)
h,φ(XW(1)
hX⊺)X⟩=1√
H∑
h∈[H]αh
T⟨U(1)
h,1T1⊺
TX⟩=∶1√
H∑
h∈[H]Xh.(103)
Note for each h∈[H]thatXhin(103)depends only on the random variable αh. Recall that αh,h∈[H]are
IIDUnif(±1). Thus, {Xh}h∈[H]are IID with 0mean as Eα=0. Further, note that
∣Xh∣=∣α
T⟨U(1),1T1⊺
TX⟩∣=∣α
T⟨ζ
21Tu⊺
⋆+1Tp⊺,1T1⊺
TX⟩∣
≤1
T(ζ
2√
T∥u⋆∥+√
T∥p∥)√
T∥∑
t∈[T]xt∥≤1
T(ζ
2√
T∥u⋆∥+√
T∥p∥)√
TRT≤TR(S+P).
Thus, using Hoeffding’s inequality (see Fact 2) we have for some absolute constant c>0, with probability at
least 1−δ:
∣̃Φ(X;̃θ(1))∣≤TR(S+P)√
2 log(1/δ).
D.2.3 Proof of P3: NTK separability
We prove property P3in two steps each stated in a separate lemma below
Lemma 13. Let
W⋆=µ+µ⊺
++µ−µ⊺
−+∑
ℓ∈[M]νℓ(µ++µ−)⊺, (104)
U⋆=1Tu⊺
⋆. (105)
Given the initialization θ(1)in(102)andθ⋆∶=(U⋆,sign(α)W⋆)we have for any (X,y)sampled from DM1
and satisfying Assumption 3:
Eθ(1)y⟨∇θΦ(X;θ(1)),θ⋆⟩≥γ⋆,
38Published in Transactions on Machine Learning Research (04/2024)
where the expectation is taken over α∼Unif(±1)and
γ⋆∶=T(1−ζ)ζ
4√
2(M+1)(ζS4−7¯ZS2−12¯Z2−16¯Z3
S2)−PT5/2(S+Z)3+S√
T√
2(ζ−2(1−ζ)Zµ
S2),
where ¯Z=Zµ∨Zν,andU⋆,W⋆denote normalized U⋆,W⋆,respectively.
Proof.
⟨∇θΦ(X;θ(1)),θ⋆⟩=sign(α)⟨∇WΦ(X;θ(1)),W⋆⟩+⟨∇UΦ(X;θ(1)),U⋆⟩.
Usingθ(1)=concat(U(1),0)andU⋆=U⋆
∥U⋆∥F=1
S√
2T1Tu⊺
⋆, we have:
y⟨∇UΦ(X;θ(1)),U⋆⟩=y⟨φ(XW(1)X⊺)X,U⋆⟩=y
TS√
2T⟨1T1⊺
TX,1Tu⊺
⋆⟩
=y
S√
2Tu⊺
⋆(∑
txt)≥S√
T√
2(ζ−2(1−ζ)Zµ
S2). (106)
The gradient with respect to Wevaluated at θ(1)=concat(U(1),0)is
∇WΦ(X;θ(1))=αζ
2∑
t∈[T]xtu⊺
⋆X⊺RtX+α∑
t∈[T]xtp⊺X⊺RtX,
where
Rt=R0∶=1
T⋅IT−1
T2⋅1T1⊺
T,∀t∈[T].
Thus,
y⟨∇WΦ(X;θ(1)),sign(α)W⋆⟩
=∣α∣
∥W⋆∥F(ζ
2y∑
t∈[T]u⊺
⋆X⊺R0rt(X;W⋆)
⌟⟨⟨⟪rl⟫l⟩⟩⟪⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟨⟨⟪rl⟫m⟩⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟨⟨⟪rl⟫r⟩⟩⟩⟪
Term I+y⟨∑
t∈[T]xtp⊺X⊺R0X,W⋆⟩
⌟⟨⟨⟪rl⟫l⟩⟩⟪⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟨⟨⟪rl⟫m⟩⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟨⟨⟪rl⟫r⟩⟩⟩⟪
Term II), (107)
where we set for convenience:
rt(X;W⋆)=XW⋆⊺xt∈RT.
To computert(X;W⋆), we consider two cases where row corresponds to a signal relevant of noisy token. We
denote the t′∈[T]entry ofrtas[rt]t′∈R.
Case 1. Relevance scores of signal tokens: Consider signal token t∈Rso thatxt=µy. Using orthogonality
in Assumption 3 we can compute for all t′∈[T]
t∈R∶[rt]t′=⎧⎪⎪⎨⎪⎪⎩S4,t′∈R,
S2(µ⊺
yzt′),t′∈Rc.(108)
Therefore, again using Assumption 3,
t∈R∶[rt]t′⎧⎪⎪⎨⎪⎪⎩=S4,t′∈R,
≤S2Zµ,t′∈Rc.(109)
Case 2. Relevance scores of noisy tokens: Similar to the calculations above, using Assumption 3 for parame-
tersW⋆as in (126) it holds for noisy tokens t∈Rcthat
t∈Rc∶[rt]t′=⎧⎪⎪⎪⎪⎨⎪⎪⎪⎪⎩S4+S2(µ⊺
yzt)+S2(∑ℓν⊺
ℓzt) ,t′∈R
(µ⊺
+zt)(µ⊺
+zt′)+(µ⊺
−zt)(µ⊺
−zt′) ,t′∈Rc
+∑ℓ(ν⊺
ℓzt)(µ⊺
+zt′)+∑ℓ(ν⊺
ℓzt)(µ⊺
−zt′)+S2(µ++µ−)⊺zt′(110)
39Published in Transactions on Machine Learning Research (04/2024)
Now, we can start to bound each of the two terms in (107) separately below.
Bounding Term I:Recall the data matrix complies with Assumption 3, hence.
[Xu⋆]t′=⎧⎪⎪⎨⎪⎪⎩yS2,t′∈R
z⊺
t′(µ+−µ−),t′∈Rc. (111)
Using this, we can compute:
y
T∑
t∈[T]u⊺
⋆X⊺rt(X;W∗)=y
T⎧⎪⎪⎨⎪⎪⎩∑
t∈Ru⊺
⋆X⊺rt(X;W⋆)+∑
t∈Rcu⊺
⋆X⊺rt(X;W⋆)⎫⎪⎪⎬⎪⎪⎭
=y
T⎧⎪⎪⎨⎪⎪⎩∑
t∈R(∑
t′∈R[Xu⋆]t′rt(X;W⋆)t′+∑
t′∈Rc[Xu⋆]t′rt(X;W⋆)t′)
+∑
t∈Rc(∑
t′∈R[Xu⋆]t′rt(X;W⋆)t′+∑
t′∈Rc[Xu⋆]t′rt(X;W⋆)t′)⎫⎪⎪⎬⎪⎪⎭
=y
T⎧⎪⎪⎨⎪⎪⎩ζT(ζTyS2S4+∑
t′∈Rcz⊺
t′(µ+−µ−)S2(µ⊺
yzt′))
+∑
t∈Rc[ζTyS2⋅⎛
⎝S4+S2(µ⊺
yzt)+S2(∑
jtν⊺
jtzt)⎞
⎠
+∑
t′∈Rcz⊺
t′(µ+−µ−)⋅((µ⊺
+zt)(µ⊺
+zt′)+(µ⊺
−zt)(µ⊺
−zt′)
+∑
jt(ν⊺
jtzt)(µ⊺
+zt′)+∑
jt(ν⊺
jtzt)(µ⊺
−zt′)+S2(µ++µ−)⊺zt′)]⎫⎪⎪⎬⎪⎪⎭.
Further using the noise bounds in Assumption 3, and rt(X;W⋆)from (127), (110) we have:
y
T∑
t∈[T]u⊺
⋆X⊺rt(X;W⋆)
≥ζT[ζS6−2(1−ζ)Z2
µS2]+(1−ζ)T[ζS6−ζ(Zµ+Zν)S4−2Zµ(1−ζ)(2Z2
µ+2ZµZν+2ZµS2)]
≥T[ζS6−ζ(1−ζ)(Zµ+Zν)S4−2(1−ζ)(ζ+2(1−ζ))Z2
µS2−4(1−ζ)2Z2
µ(Zµ+Zν)].(112)
For the second part of Term I:
−y
T2∑
t∈[T]u⊺
⋆X⊺1T1⊺
Trt(X;W⋆)=−y
T2∑
t∈[T]1⊺
TXu⋆1⊺
Trt(X;W⋆)
=−1
T2{∑
t′∈Ry2S2+∑
t′∈Rcyz⊺
t′(µ+−µ−)}⋅⎧⎪⎪⎨⎪⎪⎩∑
t∈R[∑
t′∈RS4+∑
t′∈RcS2z⊺
t′µy]
+∑
t∈Rc[∑
t′∈R⎛
⎝S4+S2z⊺
tµy+S2∑
jtν⊺
jtzt⎞
⎠+∑
t′∈Rc((z⊺
tµ+)(z⊺
t′µ+)+(z⊺
tµ−)µ−)
+(∑
jtν⊺
jtzt)(z⊺
t′µ+)+(∑
jtν⊺
jtzt)(z⊺
t′µ−)+S2z⊺
t′(µ++µ−))]⎫⎪⎪⎬⎪⎪⎭.
40Published in Transactions on Machine Learning Research (04/2024)
Using Assumption 3 to simplify the second term:
−y
T2∑
t∈[T]u⊺
⋆X⊺1T1⊺
Trt(X;W⋆)
≥T[−ζ3S6−ζ2(1−ζ)ZµS4−ζ2(1−ζ)S6−ζ2(1−ζ)(Zµ+Zν)S4−2ζ(1−ζ)2ZµS4
−2ζ(1−ζ)2Zµ(Zµ+Zν)S2−2ζ2(1−ζ)ZµS4−2ζ(1−ζ)2Z2
µS2−2ζ(1−ζ)2ZµS4
−2ζ(1−ζ)2Zµ(Zµ+Zν)S2−4(1−ζ)3Z2
µS2−4(1−ζ)3Z2
µ(Zµ+Zν)].
Therefore,
−y
T2∑
t∈[T]u⊺
⋆X⊺1T1⊺
Trt(X;W⋆)≥T[−ζ2S6−ζ(1−ζ)(4Zµ+ζZν)S4−2(1−ζ)2((2+ζ)Zµ
+2ζZν)ZµS2−4(1−ζ)3Z2
µ(Zµ+Zν)]. (113)
Bounding Term II:
y⟨∑
t∈[T]xtp⊺X⊺R0X,W⋆⟩≥−∑
t∈[T]∥xt∥∥p∥∥X⊺R0X∥F∥W⋆∥F
≥−P(ζTS+(1−ζ)T(S+Z))∥R0∥F∥X∥2
FS2√
2(M+1)
≥−√
2(M+1)S2PT5/2(ζS+(1−ζ)(S+Z))(ζS2+(1−ζ)(S+Z)2).(114)
Combining (112), (113), (114) in (107) we get:
y⟨∇WΦ(X;θ(1)),W⋆⟩
≥α⎧⎪⎪⎨⎪⎪⎩Tζ
2[ζS6−ζ(1−ζ)(Zµ+Zν)S4−2(1−ζ)(ζ+2(1−ζ))Z2
µS2−4(1−ζ)2Z2
µ(Zµ+Zν)]
−Tζ
2[ζ2S6+ζ(1−ζ)(4Zµ+ζZν)S4+2(1−ζ)2((2+ζ)Zµ+2ζZν)ZµS2+4(1−ζ)3Z2
µ(Zµ+Zν)]
−√
2(M+1)S2PT5/2(ζS+(1−ζ)(S+Z))(ζS2+(1−ζ)(S+Z)2)⎫⎪⎪⎬⎪⎪⎭
=α⎧⎪⎪⎨⎪⎪⎩T(1−ζ)ζ
2[ζS6−ζ(5Zµ+(1+ζ)Zν)S4−2((4−ζ2)Zµ+2ζ(1−ζ)Zν)ZµS2
−4(1−ζ)(2−ζ)(Zµ+Zν)Z2
µ]−√
2(M+1)S2PT5/2(S+(1−ζ)Z)(ζS2+(1−ζ)(S+Z)2)⎫⎪⎪⎬⎪⎪⎭
≥α⎧⎪⎪⎨⎪⎪⎩T(1−ζ)ζ
2[ζS6−(5Zµ+2Zν)S4−4(2Zµ+Zν)ZµS2−8(Zµ+Zν)Z2
µ]
−√
2(M+1)S2PT5/2(S+Z)(S2+(S+Z)2)⎫⎪⎪⎬⎪⎪⎭
≥α{T(1−ζ)ζ
2(ζS6−7¯ZS4−12¯Z2S2−16¯Z3)−2√
2(M+1)PT5/2S2(S+Z)3}, (115)
41Published in Transactions on Machine Learning Research (04/2024)
where ¯Z=Zµ∨Zν. Using this, we get
µ∶=Eα∼Unif(±1)y⟨∇WΦ(X;θ(1)),sign(α)W⋆
∥W⋆∥F⟩
≥Eα∼Unif(±1)∣α∣
∥W⋆∥{T(1−ζ)ζ
2(ζS6−7¯ZS4−12¯Z2S2−16¯Z3)−2√
2(M+1)PT5/2S2(S+Z)3}
≥T(1−ζ)ζ
4√
2(M+1)(ζS4−7¯ZS2−12¯Z2−16¯Z3
S2)−PT5/2(S+Z)3. (116)
Combining (106) and (116) concludes the proof.
Lemma 14 (NTK Separability) .Assume initialization ̃θ(1)=concat(θ(1)
1,...,θ(H)
1)as in(102)andIID
αh∼Unif(±1)for allh∈[H]. Recallγ⋆andθ⋆(⋅)from Lemma 13 above. Set
̃θ⋆=1√
Hconcat(θ⋆(θ(1)
1),...,θ⋆(θ(1)
H)).
Let any (X,y)from DM1. Then, with probability at least 1−δover the randomness of αh,h∈[H]it holds
y⟨∇̃Φ(X;̃θ(1)),̃θ⋆⟩≥γ⋆−2(2R3T(S+P)+√
TR)√
2 log(1/δ)
H.
Proof.We start by expanding the empirical margin:
y⟨∇̃Φ(X;̃θ(1)),̃θ⋆⟩=1
H∑
h∈[H]y⟨∇θΦ(X;θ(1)
h),θ⋆(θ(1)
h)⟩=∶1
H∑
h∈[H]Xh. (117)
Note that each summand Xhdefined above depends only on αh,h∈[H]. Thus, {Xh}h∈[H]areIIDrandom
variables because {αh}h∈[H]are IID random variables. Moreover, Xhis almost-surely bounded satisfying
∣Xh∣≤∥∇θΦ(X;θ(1)
h)∥∥θ⋆(θ(1)
h)∥≤√
2TR(2R2∥θ(1)
h∥F+1)≤2(2R3T(S+P)+√
TR).(118)
where the second inequality follows by Proposition 1 and by the assumption ∥θ⋆(θ(1)
h)∥2=√
2for allh∈[H],
and the last inequality follows because ∥θ(1)
h∥≤ζ
2√
T∥u⋆∥2+√
T∥p∥2≤√
T(S+P).
Finally, note that E[Xh]≥γ⋆from Lemma 13. Given these the desired claim follows by applying Hoeffding’s
inequality (see Fact 2) to (117).
Lemma 15 (Margin).Define ¯Z∶=Zµ∨Zνand
γ⋆∶=T(1−ζ)ζ
4√
2(M+1)(ζS4−7¯ZS2−12¯Z2−16¯Z3
S2)−PT5/2(S+Z)3+S√
T√
2(ζ−2(1−ζ)Zµ
S2).(119)
Suppose
√
H≥4⋅2R3T(S+P)+√
TR
γ⋆⋅√
2 log(n/δ).
Then, with probability 1−δ∈(0,1),P3holds withγ=γ⋆/2.
Proof.The proof is straightforward by using union bound and plugging the condition of Hin the result of
Lemma 14.
42Published in Transactions on Machine Learning Research (04/2024)
D.3 Proof of Lemma 2
The proof of the lemma follows directly by combining the two lemmas below and using ζ≤1.
Lemma 16. Fix anyh∈[H]. Suppose zero initialization θ(0)
h=0and consider first gradient step as in (102).
It then holds that W(1)
h=0andU(1)
h∈RT×dhas identical rows all equal to
ζαh
2u⋆+ζαh
2(1
n∑
i∈[n1]yiµyi−u⋆)
⌟⟨⟨⟪rl⟫l⟩⟩⟪⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟨⟨⟪rl⟫m⟩⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟨⟨⟪rl⟫r⟩⟩⟩⟪
p1+αh(1−ζ)
21
n∑
i∈[n1]yi1
(1−ζ)T∑
t∈Rc
i(νjt+zi,t)
⌟⟨⟨⟪rl⟫l⟩⟩⟪⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟨⟨⟪rl⟫m⟩⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟨⟨⟪rl⟫r⟩⟩⟩⟪
p2. (120)
where recall that u⋆=µ+−µ−.
Proof.We start by computing
n∇θĥL(θ(0)
h)=∑
i∈[n1]∇θhℓ(yĩΦ(X;θ(0)
h))=∑
i∈[n1]yiℓ′(yĩΦ(X;θ(0)
h))∇θh̃Φ(X;θ(0)
h)
=ℓ′(0)∑
i∈[n1]yi∇θh̃Φ(X;θ(0)
h)=ℓ′(0)√
H∑
i∈[n1]yi∇θhΦh(X;θ(0)
h)
=1
2√
H∑
i∈[n1]yi∇θhΦh(X;θ(0)
h)
where we used that Φh(X;θ(0)
h)=̃Φh(X;θ(0)
h)=0becauseUh=0and alsoℓ′(0)=1/2for the logistic
loss. Now, recall that Φh(X;θh)=⟨Uh,ATTNh(X;Wh)⟩. Hence,∇WhΦh(X;θ(0)
h)=0, which gives us
W(1)
h=W(0)
h. Also,
∇UhΦh(X;θ(0)
h)=ATTN(X;W(0)
h)=φ(0)X=1
T1T1⊺
TX.
Hence, the τ-th column of U(1)
hbecomes for all τ∈[T]:
[U(1)
h]∶,τ=1
2nTαh∑
i∈[n1]yi∑
t∈[T]xi,t
=ζ
2nαh∑
i∈[n1]yiµyi+1
2nTαh∑
i∈[n1]yi∑
t∈Rc
i(νjt+zi,t).
The claim of the lemma follows by rearranging the above.
Lemma 17. Suppose labels are IIDand equal probable, i.e. yi∼Rad(±1). Then for the two terms p1,p2in
(120)it holds with probability at least 1−2δand absolute constant C>0over the randomness of labels that
∥p1∥≤CS(√
d
n1+√
log(1/δ)
n1), (121)
and
∥p2∥≤C(S+Z)(√
d
n1+√
log(1/δ)
n1). (122)
Proof.For arbitrary ∥v∥=1, letXv=⟨v,1
n∑i∈[n1]yiµyi⟩. Then,
∥Xv∥ψ2=1
n∥∑
i∈[n1]yi⟨v,µyi⟩∥ψ2≤C
n√
∑
i∈[n1]∥yi⟨v,µyi⟩∥2
ψ2≤CS√n1.
43Published in Transactions on Machine Learning Research (04/2024)
where in the second inequality we used approximate rotation invariance of subgaussians and in the last step
we used that for all i∈[n1],∣yi⟨v,µyi⟩∣≤S, thus they are subgaussians with parameter CS. Further note
that Note that E[Xv]=⟨v,u⋆⟩. Thus, by centering property of subgaussians ⟨v,p1⟩is also subgaussian with
same constant CS/√n1. Since this holds for all von the sphere, we conclude that p1isCS/√n1-subgaussian.
From this, we can directly apply Fact 1 for concentration of Euclidean norm of random vectors to arrive at
(121).
We can follow exactly same steps to prove (122)forp2. The only difference is noting that for all i∈[n1]and
unit normv:⌟⟨rro⟪⟪⟩r⟪⌟⟨rro⟪⟪⟩r⟪⌟⟨rro⟪⟪⟩r⟪⌟⟨rro⟪⟪⟩r⟪⌟⟨rro⟪⟪⟩r⟪⌟⟨rro⟪⟪⟩r⟪⌟⟨rro⟪⟪⟩r⟪⌟⟨rro⟪⟪⟩r⟪⌟⟨rro⟪⟪⟩r⟪⌟⟨rro⟪⟪⟩r⟪⌟⟨rro⟪⟪⟩r⟪⌟⟨rro⟪⟪⟩r⟪yi1
(1−ζ)T∑
t∈Rc
i⟨νjt+zi,t,v⟩⌟⟨rro⟪⟪⟩r⟪⌟⟨rro⟪⟪⟩r⟪⌟⟨rro⟪⟪⟩r⟪⌟⟨rro⟪⟪⟩r⟪⌟⟨rro⟪⟪⟩r⟪⌟⟨rro⟪⟪⟩r⟪⌟⟨rro⟪⟪⟩r⟪⌟⟨rro⟪⟪⟩r⟪⌟⟨rro⟪⟪⟩r⟪⌟⟨rro⟪⟪⟩r⟪⌟⟨rro⟪⟪⟩r⟪⌟⟨rro⟪⟪⟩r⟪≤S+Z.
E Optimal Model
We first restate the optimal parameters θopt=(Uopt,Wopt):
Uopt∶=1
S√
T1d(µ+−µ−)⊺, (123)
Wopt∶=1
S2√
2(M+1)(µ+µ⊺
++µ−µ⊺
−+∑
ℓ∈[M]νℓ(µ++µ−)⊺), (124)
normalized so that ∥θopt∥F=√
2.
The following lemma about saturation in softmax scores is used to prove Proposition 3.
Lemma 18 (Softmax saturation) .Let relevance-scores vector r=[r1,...,rT]∈RTbe such that for some
L∈[T]andA,B∈R:
r1≥r2≥...≥rL≥AandB≥max{ri∣i=L+1,...,T}.
Further assume A>2B. Fix anyϵ>0and
Γ≥2
Alog(T/L−1
ϵ).
Then, for the attention weights a=[a1,...,aT]∶=φ(Γr)∈RTit holds that
0≤1−∑
i∈[L]ai=T
∑
i=L+1ai≤ϵ. (125)
Proof.For convenience denote D∶=∑j∈[T]eΓrj. Note that D≥LeΓA.Consider any i>L. Then,
ai=eΓri
D≤eΓB
D≤eΓB
LeΓA=1
LeΓ(A−B)≤1
LeΓA/2.
Suppose Γ≥2
Alog(C
ϵ), which ensures that
eΓA/2≥C/ϵ
SettingC=T−L
L=T
L−1, and combining the above two displays yields the desired:
ai≤ϵ
T−L, i>L.
Thus,∑i>Lai≤ϵ. The proof is complete by recalling that ∑i∈[T]ai=1, hence∑i∈[L]ai≥1−ϵ.
44Published in Transactions on Machine Learning Research (04/2024)
E.1 Proof of Proposition 3
First, we compute the attention matrix when the parameter Wis set to the value below:
W=µ+µ⊺
++µ−µ⊺
−+∑
ℓ∈[M]νℓ(µ++µ−)⊺. (126)
For convenience, use the notation r⊺
t∶=x⊺
tWX⊺∈RTfor thet-th row of matrix used to find attention scores.
Similar to the proof of Lemma 13 we consider two cases where row corresponds to a signal relevant of noisy
token. We denote the t′∈[T]entry ofrtas[rt]t′∈R.
Case 1. Relevance scores of signal tokens: Consider signal token t∈Rso thatxt=µy. Then for weights W
in (126), and using orthogonality in Assumption 3 we can compute for all t′∈[T]
t∈R∶[rt]t′=⎧⎪⎪⎨⎪⎪⎩S4,t′∈R,
S2(µ⊺
yzt′),t′∈Rc.
Therefore, again using Assumption 3,
t∈R∶[rt]t′⎧⎪⎪⎨⎪⎪⎩=S4,t′∈R,
≤S2Zµ,t′∈Rc.(127)
Case 2. Relevance scores of noisy tokens: Similar to the calculations above, using Assumption 3 for parame-
tersWas in (126) it holds for noisy tokens t∈Rcthat
t∈Rc∶[rt]t′=⎧⎪⎪⎪⎪⎨⎪⎪⎪⎪⎩S4+S2(µ⊺
yzt)+S2(∑ℓν⊺
ℓzt) ,t′∈R
(µ⊺
+zt)(µ⊺
+zt′)+(µ⊺
−zt)(µ⊺
−zt′) ,t′∈Rc
+∑ℓ(ν⊺
ℓzt)(µ⊺
+zt′)+∑ℓ(ν⊺
ℓzt)(µ⊺
−zt′)+S2(µ++µ−)⊺zt′
Therefore, using the noise bound assumptions, we have
t∈Rc∶[rt]t′⎧⎪⎪⎨⎪⎪⎩≥S4−S2(Zµ+Zν),t′∈R
≤2Z2
µ+2ZµZν+2S2Zµ,t′∈Rc.(128)
Combining the above two cases, specifically Equations (127)and(128), we find that for all t∈[T]the
relevance-score vectors rtare such that
t∈[T]∶[rt]t′⎧⎪⎪⎨⎪⎪⎩≥S4−S2(Zµ+Zν)∶=A,t′∈R
≤2(Z2
µ+ZµZν+S2Zµ)∶=B,t′∈Rc,(129)
where we defined the parameters AandBfor convenience. Note from assumption that
Zµ=Zν≤S2
8/Leftr⫯g⊸tl⫯ne⇒A≥3
4S4>1.25
4S4≥2B.
Thus, the conditions of Lemma 18 hold for L=∣R∣=ζT. Applying the lemma we can immediately conclude
that for
Γ∗≥8
3S4log(ζ−1−1
ϵ)≥2
Alog(ζ−1−1
ϵ).
it holds:
∀t∈[T]∶0≤1−∑
t′∈R[φ(Γ∗rt)]t′=∑
t′∈Rc[φ(Γ∗rt)]t′≤ϵ. (130)
45Published in Transactions on Machine Learning Research (04/2024)
Now, recall that
W⋆=̃ΓWfor̃Γ=Γ/(S2√
2(M+1)).
Thus, it holds for all t∈[T]thatφ(x⊺
tW⋆XT)=φ(̃Γrt).Combining this with (130)and the proposition’s
assumption on Γ(satisfying ̃Γ≥Γ∗), we have found that
∀t∈[T]∶0≤1−∑
t′∈R[φ(x⊺
tW⋆XT)]t′=∑
t′∈Rc[φ(x⊺
tW⋆XT)]t′≤ϵ. (131)
In the rest of the proof, we use (131) to lower-bound the margin:
yΦ(X;θ⋆)=y⟨U⋆,ATTN(X;W⋆)⟩=y
S√
2T∑
t∈[T]∑
t′∈[T][ϕ(x⊺
tW⋆XT)]t′(µ+−µ−)⊺xt′=∶y
S√
2T∑
t∈[T]ψt,
where we defined ψt,t∈[T]for convenience. For any t∈[T], we have
ψt=∑
t′∈R[ϕ(x⊺
tW⋆XT)]t′yS2+∑
t′∈Rc[ϕ(x⊺
tW⋆XT)]t′(µ+−µ−)⊺zt′
≥yS2∑
t′∈R[ϕ(x⊺
tW⋆XT)]t′−2Zµ∑
t′∈Rc[ϕ(x⊺
tW⋆XT)]t′
≥yS2(1−ϵ)−2ϵZµ.
Putting the last two displays together and using y2=1completes the proof of the proposition.
F Linear Model
To gain additional insights into the classification of the data model DM1 and also contrast our results to a
simplified model, we examine here a linear classifier:
Φlin(X;U)=⟨U,X⟩.
For this linear model, consider the oracle classifier
U⋆=1
S√
2T1Tu⊺
⋆,withu⋆=µ+−µ−,
and normalization such that ∥U⋆∥F=1. By using Assumption 3, almost surely for all examples (X,y)the
margin of the oracle classifier is lower bounded by:
yΦlin(X;U⋆)=1
S√
2T(∣R∣⋅S2+y∑
t∈Rc⟨µ+−µ−,zt⟩)
≥1
S√
2T(∣R∣⋅S2−2∣Rc∣⋅Zµ)=S√
T√
2(ζ−2(1−ζ)Zµ
S2)=∶γlin. (132)
G Experiments
In this section we provide some experiments discussing the role of number of heads Hin the training dynamics
on synthetic data models.
Data Model DM1 We set the number of tokens T=10and sparsity level ζ=0.1. We set
{µ+,µ−,ν1,ν2,...,νM}as the canonical basis vectors in Rd, withd=4,M=2and signal strength S=2.
Noisy tokens zare sampled from a Gaussian N(0,σ2Id), withσ=0.1. We usen=100training samples in
each experiment and evaluate on a test set of size 300(total 5 trials). All models are initialized as ̃θ(0)=0.
Figure 2 shows the effect of increasing the number of heads when running GD with constant step-size η=1.0
and data generated from data model DM1. Notice that rate of train loss decay reduces as we increase H,
highlighting a potential downside of overparameterization. A similar observation has been recently noted by
46Published in Transactions on Machine Learning Research (04/2024)
Xu & Du (2023) when optimizing with GD to learn a single neuron with ReLU activation under Gaussian
input. We also observe that at least for smaller H, GD indeed achieves margin γattn. It is worth noting that
these observations do not contradict our theoretical findings in Theorems 1 and 2 which guarantee training
convergence and generalization decay given sufficiently large number of heads H, without making an explicit
connection between the rates of convergence as we increase H. We also test how these rates change if we
scale step-size as η=O(√
H)for GD (Figure 3, left) or optimize with Adam (Figure 3, right) using constant
step-sizeη=0.06. It is interesting to observe that in both these cases, the convergence speeds up for larger
H, especially when optimizing with Adam, but somewhat strangely the margin attained by GD for larger H
continues to fall away from γattn. Further, note that our theory only covers step-size η=O(1)and the trends
observed in Figure 3 with η=O(√
H)for GD fall outside this regime. In essence, we believe that it would
be interesting future work to see how well these observations generalize to different datasets and develop
theory that explains the relation of overparameterization to rates of convergence.
GD withη=O(1)
Figure 2: For data model DM1 . Effect of number of heads Hon convergence rates when
trained with GD for constant step-size η=O(1). The average ∥⋅∥illustrates 1/Hand1/√
H
average forWandUacross heads, respectively. Attn-score denotes the softmax scores for the
relevant tokens averaged across all train samples and heads. The average ∥W∥indicates the
saturation of softmax scores and consequently the token-selection (attn-score), and the average
∥U∥controls the loss behaviour. Results demonstrate that overparameterization slows down
GD with constant step-size. The circled area shows a O(1/t)trend similar to what our training
and generalization bounds predict.
Planteddatamodel FixsomeW∗∈Rd×d,U∗∈RT×d. Entrieswithin Xaresampled IIDXij∼N(0,1),∀i∈
[T],j∈[d]. Given such an X∈RT×d, generate the label yusingW∗as the attention matrix and U∗as the
projection classifier:
y=sign(Φ(X;W∗,U∗))=sign(⟨U∗,φ(XW∗X⊺)X⟩). (DM2)
Data generated using model DM2 is used to train a multi-head self-attention model as given in equation
(2). Such a teacher-student setting (train the student network to learn the ground truth parent) has been
well explored in the past in the context of neural networks (Zhou et al., 2021; Safran et al., 2021). We set
d=5,T=10. The train set contains n=1000samples in each experiment and we evaluate on a test set of
size 3000. Each result is averaged over 5 trials. For numerical ease, while generating (example, label) pairs
we drop the samples for which ∣output logit ∣≤γattn, where we call γattn>0to be margin for the data model.
We setγattn=0.2in all the experiments. All models are initialized as ̃θ(0)=0. From Figure 4 we observe
that overparameterization somewhat improves convergence speeds for GD with step-size O(√
H), similar to
tokenized mixture model (Figure 3, left). Addition of momentum significantly helps speeding up convergence
(see Figure 5, left) and so does optimizing with Adam (Figure 5, right). Interesting to note that all the
models reach the expected margin γattnwhich was not the case for large Hfor the tokenized mixture model.
Further, we can observe that initializing at ̃θ(0)=0, all the optimizers find the planted model.
47Published in Transactions on Machine Learning Research (04/2024)
GD withη=O(√
H)
 Adam with η=O(1)
Figure 3: For data model DM1 . Effect of number of heads Hon convergence rates when
(left) trained with GD when scaling step-size as η=O(√
H); (right) trained with Adam with
constant step-size η=O(1). Quantities plotted are same as in Figure 2. Results demonstrate
that overparameterization speeds-up with train and test loss convergence in both the scenarios.
SST2 dataset We conduct an additional experiment on a simple real-world dataset. The SST2 dataset
(Socher et al., 2013) consists of sentences, with each sentence having a associated binary label to classify the
sentiment. We fine-tune RoBERTa based models with varying number of heads using AdamW (Loshchilov &
Hutter, 2019) optimizer with a learning rate of 5e−6. We train all the models for 5epochs, with the batch-size
set to 32. We use the Hugging Face pytorch-transformers implementation of the roberta-base
model, with pretrained weights (Liu et al., 2019). In Figure 6 we see that increasing the number of heads
speeds up the optimization and generalization. This behaviour is similarly observed for GD with momentum
and Adam in Figure 5.
GD withη=O(√
H)
Figure 4: For data model DM2 . Effect of number of heads Hon convergence rates when
trained with GD when scaling step-size as η=O(√
H). See Figure 2 caption for to get more
context on average ∥⋅∥. Alignment of Wwith the planted-head ̃W⋆at any iteration kis given
by⟨̃Wk,̃W⋆⟩
∥̃Wk∥∥̃W⋆∥, where ̃W⋆∶=concat({W⋆}h∈[H])containsW⋆repeatedHtimes. Alignment
between ̃Uand̃U⋆is computed similarly.
H Related work
This section elaborates on the paragraph on related work in Section 1.
48Published in Transactions on Machine Learning Research (04/2024)
GD withη=O(√
H)+ momentum
 Adam with η=O(1)
Figure 5: For data model DM2 . Effect of number of heads Hon convergence rates when
trained with (left) GD + momentum where step-size scales as η=O(√
H); (right) Adam with
constant step-size η=O(1). Quantities plotted are same as in Figure 4. Results demonstrate
that overparameterization speeds up convergence in both scenarios.
Figure 6: For SST-2 dataset (Socher et al., 2013) . Effect of number of heads Hon
convergence rates when trained with AdamW. Results demonstrate that increasing the number
of heads speeds up the training and generalization dynamics.
Transformers and Self-attention. The landscape of NLP and machine translation was profoundly
reshaped by the advent of Transformers, as pioneered by Vaswani et al. (2017) building upon earlier
investigations into self-attention as explored in the works of Cheng et al. (2016); Lin et al. (2017); Parikh
et al. (2016). More recent developments include the transformative success of large language models like,
LLaMA (Touvron et al., 2023), ChatGPT (OpenAI, 2022), and GPT4 (OpenAI, 2023). Despite this, the
learning dynamics of the self-attention mechanism remain largely unknown. Some recent works have focused
on understanding the expressive power (Baldi & Vershynin, 2022; Dong et al., 2021; Yun et al., 2020a;b;
Sanford et al., 2023; Bietti et al., 2023) and memory capacity of the attention mechanism (Baldi & Vershynin,
2022; Dong et al., 2021; Yun et al., 2020a;b; Mahdavi et al., 2023). Other aspects which are explored include
obtaining convex reformulations for the training problem (Sahiner et al., 2022; Ergen et al., 2022), studying
sparse function representations in self-attention mechanism (Edelman et al., 2021; Likhosherstov et al.,
2021) and investigating the inductive bias of masked self-attention models. Additionally, a sub-area gaining
increasing popularity is theoretical investigation of in-context learning, e.g. (von Oswald et al., 2022; Zhang
et al., 2023; Akyürek et al., 2023; Li et al., 2023b).
49Published in Transactions on Machine Learning Research (04/2024)
Here, we discuss works that aim to understand the optimization and generalization dynamics of self-Attention
or its variants. Oymak et al. (2023) diverges from traditional self-Attention by focusing on a variant called
prompt-Attention, aiming to gain understanding of prompt-tuning. Lu et al. (2021) show that for a bag
of words model, an attention model optimized with gradient flow for a topic classification task discovers
the “topic” word as training proceeds. However, they don’t provide finite-time optimization-generalization
rates. Jelassi et al. (2022) shed light on how ViTs learn spatially localized patterns, even though this spatial
structure is no longer explicitly represented after the image is split into patches. Specifically, they show that
for binary classification using gradient-based methods from random initialization, transformers implicitly
prefer the solution that learns the spatial structure of the dataset. Li et al. (2023a) provided theoretical
results on training three-layer ViTs for classification tasks for a similar data model as ours (tokenized mixture
data). They provide sample complexity for achieving zero generalization error, and also examined the degree
of sparsity in attention maps when trained using SGD. Contemporaneous works include (Tian et al., 2023;
Tarzanagh et al., 2023a): The former presents SGD-dynamics of single-layer transformer for the task of
next-token prediction by re-parameterizing the original problem in terms of the softmax and classification
logit matrices, and analyzing their training dynamics instead. The latter studies the implicit bias of training
the softmax weights Wwith a fixed decoder Uvia a regularization path analysis. All these works focus
on a single attention head. Instead, we leverage the use of multiple heads to establish connections to the
literature on GD training of overparameterized neural networks. Conceptually similar connections have also
been studied by Hron et al. (2020) who connect multi-head attention to a limiting Gaussian process when the
number of heads increase to infinity. In contrast, we study performance in the more practical regime of finite
number of heads and obtain and obtain finite-time optimization and generalization bounds.
Overparameterized MLPs. There has been an abundance of literature that discusses NN training
convergence and generalization dynamics via an NTK type approach, e.g. (Allen-Zhu et al., 2019; Oymak
& Soltanolkotabi, 2020; Arora et al., 2019; Nguyen & Mondelli, 2020; Banerjee et al., 2022; Nguyen et al.,
2021; Zhu et al., 2023). However, most of these works focus on GD dynamics on regression problems using
square loss and relate the training convergence and generalization to the minimum eigenvalue of the Hessian
of the NTK. On the other hand, relatively fewer works focus on classification with logistic loss under an NTK
separable data assumption, and (Nitanda et al., 2019; Ji & Telgarsky, 2020; Cao & Gu, 2019; Chen et al.,
2020; Telgarsky, 2013; Taheri & Thrampoulidis, 2023) are most relevant works that share overlapping ideas
with our work. We refer the reader to these for a more thorough overview of the NTK-regime analysis for
NNs.
Other than these, Richards & Kuzborskij (2021); Richards & Rabbat (2021); Taheri & Thrampoulidis (2023);
Lei et al. (2022) use algorithmic-stability based tools to understand the training dynamics and generalization
of GD in shallow NNs. Lei et al. (2022); Richards & Kuzborskij (2021) establish generalization bounds with
polynomial width ˜Ω(poly(n))requirement while minimizing square loss. Here, we make use of the tools
developed by Taheri & Thrampoulidis (2023) who work with self-bounded Lipschitz loss functions, like logistic
loss, similar to our analysis. The algorithmic stability arguments in the analysis of the above referenced
papers are rooted on a technique to bound generalization gap by directly relating it to train loss based on the
notion of average model stability introduced by Lei & Ying (2020). This technique has also been leveraged
by Schliserman & Koren (2022) to study linear logistic regression and Nikolakakis et al. (2022) who establish
generalization-risk bounds for Lipschitz function optimization with bounded optimal set.
50