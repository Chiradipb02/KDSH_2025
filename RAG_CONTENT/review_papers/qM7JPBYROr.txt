Published in Transactions on Machine Learning Research (11/2023)
Tight conditions for when the NTK approximation is valid
Enric Boix-Adsera eboix@mit.edu
MIT Electrical Engineering and Computer Science
Apple
Etai Littwin elittwin@apple.com
Apple
Reviewed on OpenReview: https: // openreview. net/ forum? id= qM7JPBYROr
Abstract
We study when the neural tangent kernel (NTK) approximation is valid for training a model
with the square loss. In the lazy training setting of Chizat et al. (2019), we show that
rescaling the model by a factor of α=O(T)suffices for the NTK approximation to be valid
until training time T. Our bound is tight and improves on the previous bound of Chizat
et al. (2019), which required a larger rescaling factor of α=O(T2).
1 Introduction
In the modern machine learning paradigm, practitioners train the weights wof a large neural network model
fw:Rdin→Rdoutvia a gradient-based optimizer. Theoretical understanding lags behind, since the training
dynamics are non-linear and hence difficult to analyze. To address this, Jacot et al. (2018) proposed an
approximation to the dynamics called the NTK approximation , and proved it was valid for infinitely-wide
networks trained by gradient descent1. The NTK approximation has been extremely influential, leading to
theoretical explanations for a range of questions, including why deep learning can memorize training data
(Du et al., 2018; 2019; Allen-Zhu et al., 2019a;b; Arora et al., 2019a; Cao & Gu, 2019; Lee et al., 2019),
why neural networks exhibit spectral bias (Cao et al., 2019; Basri et al., 2020; Canatar et al., 2021), and
why different architectures generalize differently (Bietti & Mairal, 2019; Mei et al., 2021; Wang et al., 2021).
Nevertheless, in practice the training dynamics of neural networks often diverge from the predictions of the
NTK approximation (see, e.g., Arora et al. (2019b)) Therefore, it is of interest to understand exactly under
which conditions the NTK approximation holds. In this paper, we ask the following question:
Can we give tight conditions for when the NTK approximation is valid?
1.1 The “lazy training" setting of Chizat et al. (2019)
The work of Chizat et al. (2019) showed that the NTK approximation actually holds for training any
differentiable model, as long as the model’s outputs are rescaled so that the model’s outputs change by a
large amount even when the weights change by a small amount. The correctness of the NTK approximation
for infinite-width models is a consequence of this observation, because by the default the model is rescaled as
the width tends to infinity; see the related work in Section 1.3 for more details.
Rescaling the model Leth:Rp→Fbe a smoothly-parameterized model, where Fis a separable Hilbert
space. Letα>0be a parameter which controls the rescaling of the model and which should be thought of as
large. We train the rescaled model αhwith gradient flow to minimize a smooth loss function R:F→R+.2
1Under a specific scaling of the initialization and learning rate as width tends to infinity.
2We use the Hilbert space notation as in Chizat et al. (2019). We can recover the setting of training a neural network fw:
Rd→Ron a finite training dataset {(x1,y1),..., (xn,yn)}⊆Rd×Rwith empirical loss function L(w) =1
n/summationtextn
i=1ℓ(fw(xi),yi)
as follows. LetH=Rnbe the Hilbert space, let h(w) = [fw(x1),...,f w(xn)], and letR(v) =1
n/summationtextn
i=1ℓ(vi,yi).
1Published in Transactions on Machine Learning Research (11/2023)
Namely, the weights w(t)∈Rpare initialized at w(0) = w0and evolve according to the gradient flow
dw
dt=−1
α2∇wR(αh(w(t))). (1)
NTK approximation Define the linear approximation of the model around the initial weights w0by
¯h(w) =h(w0) +Dh(w0)(w−w0), (2)
whereDhis the first derivative of hinw. Let ¯w(t)be weights initialized at ¯w(0) = w0that evolve according
to the gradient flow from training the rescaled linearized model α¯h:
d¯w
dt=−1
α2∇¯wR(α¯h(¯w(t))). (3)
The NTK approximation states that
αh(w(t))≈α¯h(¯w(t)).
In other words, it states that the linearization of the model his valid throughout training. This allows for
much simpler analysis of the training dynamics since the model ¯his linear in its parameters, and so the
evolution of ¯h(¯w)can be understood via a kernel gradient flow in function space.
When is the NTK approximation valid? Chizat et al. (2019) proves that if the rescaling parameter α
is large, then the NTK approximation is valid. The intuition is that the weights do not need to move far from
their initialization in order to change the output of the model significantly, so the linearization (2)is valid for
longer. Since the weights stay close to initialization, Chizat et al. (2019) refer to this regime of training as
“lazy training.” The following bound is proved.3Here
R0=R(αh(w0))
is the loss at initialization, and
κ=T
αLip(Dh)/radicalbig
R0,
is a quantity that will also appear in our main results.
Proposition 1.1 (Theorem 2.3 of Chizat et al. (2019)) .LetR(y) =1
2∥y−y∗∥2be the square loss, where
y∗∈Fare the target labels. Assume that hisLip(h)-Lipschitz and that DhisLip(Dh)-Lipschitz in a ball of
radiusρaround w0. Then, for any time 0≤T≤αρ/(Lip(h)√R0),
∥αh(w(T))−α¯h(¯w(T))∥≤TLip(h)2κ/radicalbig
R0. (4)
Notice that as we take the rescaling parameter αto infinity, then κgoes to 0, so the right-hand-side of (4)is
small and the NTK approximation is valid.
1.2 Our results
Our contribution is to refine the bound of Chizat et al. (2019) for large time scales. We prove:
Theorem 1.2 (NTK approximation error bound) .LetR(y) =1
2∥y−y∗∥2be the square loss. Assume that
DhisLip(Dh)-Lipschitz in a ball of radius ρaround w0. Then, at any time 0≤T≤α2ρ2/R0,
∥αh(w(T))−α¯h(¯w(T))∥≤min(6κ/radicalbig
R0,/radicalbig
8R0). (5)
Furthermore, the converse is true. Our bound is tight up to a constant factor.
Theorem 1.3 (Converse to Theorem 1.2) .For anyα,T, Lip(Dh), andR0, there is a model h:R→R,
an initialization w0∈R, and a target y∗∈Rsuch that, for the risk R(y) =1
2(y−y∗)2, the initial risk is
R(αh(w0)) =R0, the derivative map DhisLip(Dh)-Lipschitz, and
∥αh(w(T))−α¯h( ¯w(T))∥≥min(1
5κ/radicalbig
R0,1
5/radicalbig
R0).
3See Section 1.3 for discussion on the other results of Chizat et al. (2019).
2Published in Transactions on Machine Learning Research (11/2023)
Comparison to Chizat et al. (2019) In contrast to our theorem, the bound (4)depends on the Lipschitz
constant of h, and incurs an extra factor of TLip(h)2. So if Lip(Dh),Lip(h), andR0are bounded by constants,
our result shows that the NTK approximation (up to O(ϵ) error) is valid for times T=O(αϵ), while the
previously known bound is valid for T=O(√αϵ). Since the regime of interest is training for large times
T≫1, our result shows that the NTK approximation holds for much longer time horizons than previously
known.
1.3 Additional related literature
Other results of Chizat et al. (2019) In addition to the bound of Proposition 1.1 above, Chizat et al.
(2019) controls the error in the NTK approximation in two other settings: (a) for general losses, but αmust
be taken exponential in T, and (b) for strongly convex losses and infinite training time T, but the problem
must be “well-conditioned.” We work in the setting of Proposition 1.1 instead, since it is more aligned with
the situation in practice, where we have long training times and the problem is ill-conditioned. Indeed, the
experiments of Chizat et al. (2019) report that for convolutional neural networks on CIFAR10 trained in the
lazy regime, the problem is ill-conditioned, and training takes a long time to converge.
Other works on the validity of the NTK approximation The NTK approximation is valid for infinitely-
wide neural networks under a certain choice of hyperparameter scaling called the “NTK parametrization”
Jacot et al. (2018). However, there is another choice of hyperparameter scaling, called the “mean-field
parametrization”, under which the NTK approximation is not valid at infinite width (Chizat & Bach, 2018;
Rotskoff & Vanden-Eijnden, 2018; Sirignano & Spiliopoulos, 2022; Mei et al., 2018; 2019; Yang & Hu, 2021).
It was observed by Chizat et al. (2019) that one can interpolate between the “NTK parametrization” and the
“mean-field parametrization” by varying the lazy training parameter α. This inspired the works Woodworth
et al. (2020); Geiger et al. (2020; 2021), which study the effect of interpolating between lazy and non-lazy
training by varying α. Most work points towards provable benefits of non-lazy training (Allen-Zhu & Li,
2019; Bai & Lee, 2019; Bai et al., 2020; Chen et al., 2020; Nichani et al., 2022; Ghorbani et al., 2020; Malach
et al., 2021; Abbe et al., 2022; 2023; Mousavi-Hosseini et al., 2022; Damian et al., 2022; Bietti et al., 2022;
Ba et al., 2022) although interestingly there are settings where lazy training provably outperforms non-lazy
training (Petrini et al., 2022).
Finally, our results do not apply to ReLU activations because we require twice-differentiability of the model
as in Chizat et al. (2019). It is an interesting future direction to prove such an extension. One promising
approach could be to adapt a technique of Cao & Gu (2020), which analyzes ReLU network training in the
NTK regime by showing in Lemma 5.2 that around initialization the model is “almost” linear and “almost”
smooth, even though these assumptions are not strictly met because of the ReLU activations.
2 Application to neural networks
The bound in Theorem 1.2 applies to lazy training of any differentiable model. As a concrete example, we
describe its application to neural networks (a similar application was presented in Chizat et al. (2019)). We
parametrize the networks in the mean-field regime, so that the NTK approximation is not valid even as the
width tends to infinity. Therefore, the NTK approximation is valid only when we train with lazy training.
Letfw:Rd→Rbe a 2-layer network of width min the mean-field parametrization Chizat & Bach (2018);
Rotskoff & Vanden-Eijnden (2018); Sirignano & Spiliopoulos (2022); Mei et al. (2018; 2019), with activation
functionσ:R→R,
fw(x) =1√mm/summationdisplay
i=1aiσ(√m⟨xi,ui⟩).
The weights are w= (a,U)fora= [a1,...,am]andU= [u1,...,um]. These are initialized at w0with i.i.d.
Unif[−1/√m,1/√m]entries. Given training data (x1,y1),..., (xn,yn), we train the weights of the network
3Published in Transactions on Machine Learning Research (11/2023)
with the mean-squared loss
L(w) =1
nn/summationdisplay
i=1ℓ(fw(xi),yi), ℓ(a,b) =1
2(a−b)2. (6)
In the Hilbert space notation, we let H=Rn, so that the gradient flow training dynamics with loss (6)
correspond to the gradient flow dynamics (1) with the following model and loss function
h(w) =1√n[fw(x1),...,f w(xn)]∈Rn, R (v) =1
2∥v−y√n∥2.
Under some regularity assumptions on the activation function (which are satisfied, for example, by the
sigmoid function) and some bound on the weights, it holds that Lip(Dh)is bounded.
Lemma 2.1 (Bound on Lip(Dh)for mean-field 2-layer network) .Suppose that there is a constant Ksuch that
(i) the activation function σis bounded and has bounded derivatives ∥σ∥∞,∥σ′∥∞,∥σ′′∥∞,∥σ′′′∥∞≤K, (ii)
the weights have bounded norm ∥a∥+∥U∥≤K, and (iii) the data points have bounded norm maxi∥xi∥≤K.
Then there is a constant K′depending only Ksuch that
Lip(Dh)≤K′.
Proof.See Appendix C.
Note that our bounds hold at any finite width of the neural network, because we have taken initialization
uniformly bounded in the interval [−1/√m,1/√m]. Since the assumptions of Theorem 1.2 are met, we obtain
the following corollary for the lazy training dynamics of the 2-layer mean-field network.
Corollary 2.2 (Lazy training of 2-layer mean-field network) .Suppose that the conditions of Lemma 2.1, and
also that the labels are bounded in norm ∥y∥≤√
nK. Then there are constants c,C > 0depending only on
Ksuch that for any time 0≤T≤cα2,
∥αh(w(T))−α¯h(¯w(T))∥≤Cmin(T/α, 1).
Notice that training in the NTK parametrization corresponds to training the model√mfw, wherefwis the
network in the mean-field parametrization. This amounts to taking the lazy training parameter α=√min
the mean-field setting. Therefore, under the NTK parametrization with width m, the bound in Corollary 2.2
shows that the NTK approximation is valid until training time O(m)and the error bound is O(T/√m).
3 Proof ideas
3.1 Proof ideas for Theorem 1.2
Proof of Chizat et al. (2019) In order to give intuition for our proof, we first explain the idea behind
the proof in Chizat et al. (2019). Define residuals r(t),¯r(t)∈Funder training the original rescaled model
and the linearized rescaled model as r(t) =y∗−αh(w(t))and¯r(t) =y∗−α¯h(¯w(t)). It is well known that
these evolve according to
dr
dt=−Ktrandd¯r
dt=−K0¯r,
for the time-dependent kernel Kt:F→Fwhich is the linear operator given by Kt:=Dh(w(t))Dh(w(t))⊤.
To compare these trajectories, Chizat et al. (2019) observes that, since K0is p.s.d.,
1
2d
dt∥r−¯r∥2=−⟨r−¯r,Ktr−K0¯r⟩≤−⟨r−¯r,(Kt−K0)r⟩,
4Published in Transactions on Machine Learning Research (11/2023)
which, dividing both sides by ∥r−¯r∥and using that∥r∥≤√R0implies
d
dt∥r−¯r∥≤∥Kt−K0∥∥r∥≤2Lip(h)Lip(Dh)∥w−w0∥/radicalbig
R0. (7)
Using the Lipschitzness of the model, Chizat et al. (2019) furthermore proves that the weight change is
bounded by∥w(t)−w0∥≤t√R0Lip(h)/α. Plugging this into (7) yields the bound in Proposition 1.1,
∥αh(w(T))−α¯h(¯w(T))∥=∥r(T)−¯r(T)∥≤2Lip(h)2Lip(Dh)R0α−1/integraldisplayT
0tdt
=T2Lip(h)2Lip(Dh)R0/α.
First attempt: strengthening of the bound for long time horizons We show how to strengthen
this bound to hold for longer time horizons by using an improved bound on the movement of the weights.
Consider the following bound on the weight change.
Proposition 3.1 (Bound on weight change, implicit in proof of Theorem 2.2 in Chizat et al. (2019)) .
∥w(T)−w0∥≤/radicalbig
TR0/αand∥¯w(T)−w0∥≤/radicalbig
TR0/α. (8)
Proof of Proposition 3.1. By (a) Cauchy-Schwarz, and (b) the nonnegativity of the loss R,
∥w(T)−w(0)∥≤/integraldisplayT
0∥dw
dt∥dt(a)
≤/radicaligg
T/integraldisplayT
0∥dw
dt∥2dt=/radicaligg
−T
α2/integraldisplayT
0d
dtR(αh(w(t)))dt
(b)
≤/radicalbig
TR0/α.
The bound for ¯wis analogous.
This bound (8)has the benefit of√
tdependence (instead of linear tdependence), and also does not depend
onLip(h). So if we plug it into (7), we obtain
∥αh(w(T))−α¯h(¯w(T))∥≤2Lip(h)Lip(Dh)R0α−1/integraldisplayT
0√
tdt
=4
3T3/2Lip(h)Lip(Dh)R0/α.
This improves over Proposition 1.1 for long time horizons since the time dependence scales as T3/2instead of
asT2. However, it still depends on the Lipschitz constant Lip(h)ofh, and it also falls short of the linear in
Tdependence of Theorem 1.2.
Second attempt: new approach to prove Theorem 1.2 In order to avoid dependence on Lip(h)and
obtain a linear dependence in T, we develop a new approach. We cannot use (7), which was the core of
the proof in Chizat et al. (2019), since it depends on Lip(h). Furthermore, in order to achieve linear T
dependence using (7), we would need that ∥w−w0∥=O(1)for a constant that does not depend on the time
horizon, which is not true unless the problem is well-conditioned.
In the full proof in Appendix A, we bound ∥r(T)−¯r(T)∥=∥αh(w(T))−α¯h(¯w(T))∥, which requires working
with a product integral formulation of the dynamics of rto handle the time-varying kernels Kt(Dollard &
Friedman, 1984). The main technical innovation in the proof is Theorem A.8, which is a new, general bound
on the difference between product integrals.
To avoid the technical complications of the appendix, we provide some intuitions here by providing a proof of
a simplified theorem which does not imply the main result. We show:
Theorem 3.2 (Simplified variant of Theorem 1.2) .Considerr′(t)∈Fwhich is initialized as r′(0) =r(0)
and evolves asdr′
dt=−KTr′. Then,
∥r′(T)−¯r(T)∥≤min(3κ/radicalbig
R0,/radicalbig
8R0). (9)
5Published in Transactions on Machine Learning Research (11/2023)
It is at the final time t=Tthat the kernel Ktcan differ the most from K0. So, intuitively, if we can prove in
Theorem 3.2 that r′(T)and¯r(T)are close, then the same should be true for r(T)and¯r(T)as in Theorem 1.2.
For convenience, define the operators
A=Dh(w0)⊤andB=Dh(w(T))⊤−Dh(w0)⊤.
Since the kernels do not vary in time, the closed-form solution is
r′(t) =e−(A+B)⊤(A+B)tr(0)and ¯r(t) =e−A⊤Atr(0)
We prove that the time evolution operators for r′and¯rare close in operator norm.
Lemma 3.3. For anyt≥0, we have∥e−(A+B)⊤(A+B)t−e−A⊤At∥≤2∥B∥√
t.
Proof of Lemma 3.3. DefineZ(ζ) =−(A+ζB)⊤(A+ζB)t. By the fundamental theorem of calculus
∥e−(A+B)⊤(A+B)t−e−A⊤At∥=∥eZ(1)−eZ(0)∥=∥/integraldisplay1
0d
dζeZ(ζ)dζ∥≤ sup
ζ∈[0,1]∥d
dζeZ(ζ)∥.
Using the integral representation of the exponential map (see, e.g., Theorem 1.5.3 of (Dollard & Friedman,
1984)),
∥deZ(ζ)
dζ∥=∥/integraldisplay1
0e(1−τ)Z(ζ)(d
dζZ(ζ))eτZ(ζ)dτ∥
=∥t/integraldisplay1
0e(1−τ)Z(ζ)(A⊤B+B⊤A+ 2ζB⊤B)eτZ(ζ)dτ∥
≤∥t/integraldisplay1
0e(1−τ)Z(ζ)(A+ζB)⊤BeτZ(ζ)dτ∥
/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
(Term 1)+∥t/integraldisplay1
0e(1−τ)Z(ζ)B⊤(A+ζB)eτZ(ζ)dτ∥
/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
(Term 2).
By symmetry under transposing and reversing time, (Term 1) =(Term 2) , so it suffices to bound the first
term. Since∥eτZ(ζ)∥≤1,
(Term 1)≤t/integraldisplay1
0∥e(1−τ)Z(ζ)(A+ζB)⊤∥∥B∥∥eτZ(ζ)∥dτ
≤t∥B∥/integraldisplay1
0∥e(1−τ)Z(ζ)(A+ζB)⊤∥dτ
=t∥B∥/integraldisplay1
0/radicalig
∥e(1−τ)Z(ζ)(A+ζB)⊤(A+ζB)e(1−τ)Z(ζ)∥dτ
=√
t∥B∥/integraldisplay1
0/radicalig
∥e(1−τ)Z(ζ)Z(ζ)e(1−τ)Z(ζ)∥dτ
≤√
t∥B∥/integraldisplay1
0sup
λ≥0/radicalbig
λe−2(1−τ)λdτ
=√
t∥B∥/integraldisplay1
0/radicalbig
1/(2e(1−τ))dτ
=/radicalbig
2t/e∥B∥.
where in the third-to-last line we use the Courant-Fischer-Weyl theorem and the fact that Z(ζ)is negative
semidefinite. Combining these bounds ∥e−(A+B)⊤(A+B)t−e−A⊤At∥≤2/radicalbig
2t/e∥B∥≤2∥B∥√
t.
6Published in Transactions on Machine Learning Research (11/2023)
Finally, let us combine Lemma 3.3 with the weight-change bound in Proposition 3.1 to prove Theorem 3.2.
Notice that the weight-change bound in Proposition 3.1 implies
∥B∥≤Lip(Dh)∥w(T)−w0∥≤Lip(Dh)/radicalbig
TR0/α.
So Lemma 3.3 implies
∥r′(T)−¯r(T)∥≤2Lip(Dh)T/radicalbig
R0α−1∥r(0)∥= 2κ∥r(0)∥.
Combining this with ∥r′(T)−¯r(T)∥≤∥r′(T)∥+∥¯r(T)∥≤2∥r(0)∥= 2√2R0implies(9). Thus, we have
shown Theorem 3.2, which is the result of Theorem 1.2 if we replace rbyr′. The actual proof of the theorem
handles the time-varying kernel Kt, and is in Appendix A.
3.2 Proof ideas for Theorem 1.3
The converse in Theorem 1.3 is achieved in the simple case where h(w) =aw+1
2bw2fora=1√
Tand
b= Lip(Dh), andw0= 0andR(y) =1
2(y−√2R0)2, as we show in Appendix B by direct calculation.
4 Discussion
A limitation of our result is that it applies only to the gradient flow, which corresponds to SGD with
infinitesimally small step size. However, larger step sizes are beneficial for generalization in practice (see, e.g.,
Li et al. (2021); Andriushchenko et al. (2022)), so it would be interesting to understand the validity of the
NTK approximation in that setting. Another limitation is that our result applies only to the square loss, and
not to other popular losses such as the cross-entropy loss. Indeed, the known bounds in the setting of general
losses require either a “well-conditioning” assumption, or taking αexponential in the training time T(Chizat
et al., 2019). Can one prove bounds of analogous to Theorem 1.2 for more general losses, with αdepending
polynomially on T, and without conditioning assumptions?
A natural question raised by our bounds in Theorems 1.2 and 1.3 is: how do the dynamics behave just outside
the regime where the NTK approximation is valid? For models hwhere Lip(h)andLip(Dh)are bounded by
a constant, can we understand the dynamics in the regime where T≥Cαfor some large constant Cand
α≫C, at the edge of the lazy training regime?
Acknowledgements
We thank Emmanuel Abbe, Samy Bengio, and Joshua Susskind for stimulating and helpful discussions. EB
also thanks Apple for the company’s generous support through the AI/ML fellowship.
References
Emmanuel Abbe, Enric Boix-Adsera, and Theodor Misiakiewicz. The merged-staircase property: a necessary
and nearly sufficient condition for sgd learning of sparse functions on two-layer neural networks. In
Conference on Learning Theory , pp. 4782–4887. PMLR, 2022.
Emmanuel Abbe, Enric Boix-Adsera, and Theodor Misiakiewicz. Sgd learning on neural networks: leap
complexity and saddle-to-saddle dynamics. arXiv preprint arXiv:2302.11055 , 2023.
Zeyuan Allen-Zhu and Yuanzhi Li. What can resnet learn efficiently, going beyond kernels? Advances in
Neural Information Processing Systems , 32, 2019.
Zeyuan Allen-Zhu, Yuanzhi Li, and Yingyu Liang. Learning and generalization in overparameterized neural
networks, going beyond two layers. Advances in neural information processing systems , 32, 2019a.
Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via over-
parameterization. In International Conference on Machine Learning , pp. 242–252. PMLR, 2019b.
7Published in Transactions on Machine Learning Research (11/2023)
Maksym Andriushchenko, Aditya Varre, Loucas Pillaud-Vivien, and Nicolas Flammarion. Sgd with large step
sizes learns sparse features. arXiv preprint arXiv:2210.05337 , 2022.
Sanjeev Arora, Simon Du, Wei Hu, Zhiyuan Li, and Ruosong Wang. Fine-grained analysis of optimization and
generalization for overparameterized two-layer neural networks. In Proceedings of the 36th International
Conference on Machine Learning , pp. 322–332, 2019a.
Sanjeev Arora, Simon S Du, Wei Hu, Zhiyuan Li, Russ R Salakhutdinov, and Ruosong Wang. On exact
computation with an infinitely wide neural net. In Advances in Neural Information Processing Systems ,
2019b.
Jimmy Ba, Murat A Erdogdu, Taiji Suzuki, Zhichao Wang, Denny Wu, and Greg Yang. High-dimensional
asymptotics of feature learning: How one gradient step improves the representation. arXiv preprint
arXiv:2205.01445 , 2022.
Yu Bai and Jason D Lee. Beyond linearization: On quadratic and higher-order approximation of wide neural
networks. arXiv preprint arXiv:1910.01619 , 2019.
Yu Bai, Ben Krause, Huan Wang, Caiming Xiong, and Richard Socher. Taylorized training: Towards better
approximation of neural network training at finite width. arXiv preprint arXiv:2002.04010 , 2020.
Ronen Basri, Meirav Galun, Amnon Geifman, David Jacobs, Yoni Kasten, and Shira Kritchman. Frequency
bias in neural networks for input of non-uniform density. In International Conference on Machine Learning ,
pp. 685–694. PMLR, 2020.
Alberto Bietti and Julien Mairal. On the inductive bias of neural tangent kernels. Advances in Neural
Information Processing Systems , 32, 2019.
Alberto Bietti, Joan Bruna, Clayton Sanford, and Min Jae Song. Learning single-index models with shallow
neural networks. arXiv preprint arXiv:2210.15651 , 2022.
Abdulkadir Canatar, Blake Bordelon, and Cengiz Pehlevan. Spectral bias and task-model alignment explain
generalization in kernel regression and infinitely wide neural networks. Nature communications , 12(1):2914,
2021.
Yuan Cao and Quanquan Gu. Generalization bounds of stochastic gradient descent for wide and deep neural
networks. In Advances in Neural Information Processing Systems , 2019.
Yuan Cao and Quanquan Gu. Generalization error bounds of gradient descent for learning over-parameterized
deep relu networks. In Proceedings of the AAAI Conference on Artificial Intelligence , volume 34, pp.
3349–3356, 2020.
Yuan Cao, Zhiying Fang, Yue Wu, Ding-Xuan Zhou, and Quanquan Gu. Towards understanding the spectral
bias of deep learning. arXiv preprint arXiv:1912.01198 , 2019.
Minshuo Chen, Yu Bai, Jason D Lee, Tuo Zhao, Huan Wang, Caiming Xiong, and Richard Socher. Towards
understanding hierarchical learning: Benefits of neural representations. Advances in Neural Information
Processing Systems , 33:22134–22145, 2020.
Lenaic Chizat and Francis Bach. On the global convergence of gradient descent for over-parameterized models
using optimal transport. Advances in neural information processing systems , 31, 2018.
Lenaic Chizat, Edouard Oyallon, and Francis Bach. On lazy training in differentiable programming. Advances
in Neural Information Processing Systems , 32, 2019.
Alexandru Damian, Jason Lee, and Mahdi Soltanolkotabi. Neural networks can learn representations with
gradient descent. In Conference on Learning Theory , pp. 5413–5452. PMLR, 2022.
John Day Dollard and Charles N. Friedman. Product Integration with Application to Differential Equations .
Encyclopedia of Mathematics and its Applications. Cambridge University Press, 1984. doi: 10.1017/
CBO9781107340701.
8Published in Transactions on Machine Learning Research (11/2023)
Simon Du, Jason Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai. Gradient descent finds global minima of
deep neural networks. In International conference on machine learning , pp. 1675–1685. PMLR, 2019.
Simon S Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh. Gradient descent provably optimizes over-
parameterized neural networks. arXiv preprint arXiv:1810.02054 , 2018.
Mario Geiger, Stefano Spigler, Arthur Jacot, and Matthieu Wyart. Disentangling feature and lazy training in
deep neural networks. Journal of Statistical Mechanics: Theory and Experiment , 2020(11):113301, 2020.
Mario Geiger, Leonardo Petrini, and Matthieu Wyart. Landscape and training regimes in deep learning.
Physics Reports , 924:1–18, 2021.
Behrooz Ghorbani, Song Mei, Theodor Misiakiewicz, and Andrea Montanari. When do neural networks
outperform kernel methods? Advances in Neural Information Processing Systems , 33:14820–14830, 2020.
Arthur Jacot, Franck Gabriel, and Clément Hongler. Neural tangent kernel: Convergence and generalization
in neural networks. Advances in neural information processing systems , 31, 2018.
Jaehoon Lee, Lechao Xiao, Samuel Schoenholz, Yasaman Bahri, Roman Novak, Jascha Sohl-Dickstein, and
Jeffrey Pennington. Wide neural networks of any depth evolve as linear models under gradient descent.
Advances in neural information processing systems , 32, 2019.
Zhiyuan Li, Sadhika Malladi, and Sanjeev Arora. On the validity of modeling sgd with stochastic differential
equations (sdes). Advances in Neural Information Processing Systems , 34:12712–12725, 2021.
Eran Malach, Pritish Kamath, Emmanuel Abbe, and Nathan Srebro. Quantifying the benefit of using
differentiable learning over tangent kernels. In International Conference on Machine Learning , pp. 7379–
7389. PMLR, 2021.
Song Mei, Andrea Montanari, and Phan-Minh Nguyen. A mean field view of the landscape of two-layer
neural networks. Proceedings of the National Academy of Sciences , 115(33):E7665–E7671, 2018.
Song Mei, Theodor Misiakiewicz, and Andrea Montanari. Mean-field theory of two-layers neural networks:
dimension-free bounds and kernel limit. In Conference on Learning Theory , pp. 2388–2464. PMLR, 2019.
Song Mei, Theodor Misiakiewicz, and Andrea Montanari. Learning with invariances in random features and
kernel models. In Conference on Learning Theory , pp. 3351–3418. PMLR, 2021.
Alireza Mousavi-Hosseini, Sejun Park, Manuela Girotti, Ioannis Mitliagkas, and Murat A Erdogdu. Neural
networks efficiently learn low-dimensional representations with sgd. arXiv preprint arXiv:2209.14863 , 2022.
Eshaan Nichani, Yu Bai, and Jason D Lee. Identifying good directions to escape the ntk regime and efficiently
learn low-degree plus sparse polynomials. arXiv preprint arXiv:2206.03688 , 2022.
Leonardo Petrini, Francesco Cagnetta, Eric Vanden-Eijnden, and Matthieu Wyart. Learning sparse features
can lead to overfitting in neural networks. arXiv preprint arXiv:2206.12314 , 2022.
Grant M Rotskoff and Eric Vanden-Eijnden. Neural networks as interacting particle systems: Asymptotic
convexity of the loss landscape and universal scaling of the approximation error. stat, 1050:22, 2018.
Justin Sirignano and Konstantinos Spiliopoulos. Mean field analysis of deep neural networks. Mathematics of
Operations Research , 47(1):120–152, 2022.
Terence Tao. Topics in random matrix theory. Graduate Studies in Mathematics , 132, 2011.
Sifan Wang, Hanwen Wang, and Paris Perdikaris. On the eigenvector bias of fourier feature networks: From
regression to solving multi-scale pdes with physics-informed neural networks. Computer Methods in Applied
Mechanics and Engineering , 384:113938, 2021.
9Published in Transactions on Machine Learning Research (11/2023)
Blake Woodworth, Suriya Gunasekar, Jason D Lee, Edward Moroshko, Pedro Savarese, Itay Golan, Daniel
Soudry, and Nathan Srebro. Kernel and rich regimes in overparametrized models. In Conference on
Learning Theory , pp. 3635–3673. PMLR, 2020.
Greg Yang and Edward J Hu. Tensor programs IV: Feature learning in infinite-width neural networks. In
International Conference on Machine Learning , pp. 11727–11737. PMLR, 2021.
A Proof of Theorem 1.2
A.1 Notations
We letR0:=R(αh(w0))denote the loss at initialization. We define the residuals r(t),¯r(t)∈Funder training
the original model and the linearized model as
r(t) =y∗−αh(w(t)),and¯r(t) =y∗−α¯h(¯w(t)).
Since the evolution of wand ¯wis given by the gradient flow in (1)and(3), the residuals evolve as follows.
We writeDh⊤=/parenleftbigdh
dw/parenrightbig⊤to denote the adjoint of Dh=dh
dw.
dr
dt=dr
dwdw
dt=−αDh(w)(−∇Fα(w)) =αD(w)(1
αDh(w)⊤∇R(αh(w))) =−Dh(w)Dh(w)⊤r,
since∇R(αh(w)) =−(y∗−αh(w)) =−r. An analogous result can be derived for the residual ¯runder the
linearized dynamics:
d¯r
dt=−D¯h(¯w)D¯h(¯w)⊤¯r.
For any time t≥0, define the kernel Kt:F→Fas
Kt:=Dh(w(t))Dh(w(t))⊤.
SinceK0=Dh(w(0))Dh(w(0))⊤=D¯h(¯w(t))D¯h(¯w(t))⊤, we can write the dynamics in compact form:
dr
dt=−Ktrandd¯r
dt=−K0¯r. (10)
A.2 Proof overview
Our proof of Theorem 1.2 is outlined in the flowchart in Figure 1. First, we use Lemma A.1 to argue that
the change in the weights is bounded above during training. This implies several basic facts about the
continuity and boundedness of the kernel Ktover time, which are written as Lemmas A.2, A.3, A.4 A.5.
These lemmas allow us to write the dynamics of rand¯rusing product integrals Dollard & Friedman (1984)
in Lemma A.6. Product integrals are a standard tool in differential equations and quantum mechanics,
but known results on product integrals do not suffice to prove our theorem. Thus, in Theorem A.8, we
prove a new operator-norm bound between differences of product integrals. Theorem A.8 implies our main
Theorem 1.2 when we specialize to the case of gradient-flow training of a model, and combine it with the
weight-change bound from Lemma A.1. The bulk of the proof is dedicated to showing Theorem A.8. The
proof is an interpolation argument, where we interpolate between the two product integrals being compared,
and show that the operator norm of the derivative of the interpolation path is bounded. In order to show
this, we require several new technical bounds – most notably Claim 6, which shows that for any matrices
X,Y∈Rn×dand anyt≥0we have
∥e−(X+Y)(X+Y)⊤t(X+Y)−e−XX⊤tX∥≤3∥Y∥.
10Published in Transactions on Machine Learning Research (11/2023)
Theorem 1.2
(NTK approximation
error bound)
Theorem A.8
(Bound on difference
between product integrals)Lemma A.6
(Product integral
formulation of dynamics)
Finite-dimensional case of
Theorem A.8
(Bound on difference
between product integrals)Lemmas A.1, A.2, A.3, A.4, A.5
(Weight change bounded)
(Weights in ball around init)
(Dh−D¯hbounded)
(Ktbounded)
(t∝⇕⊣√∫⊔≀→Ktcontinuous)Lemma A.9
(Reduction to
finite-dimensional case)
Lemma A.10
(Bound on derivative of interpolation
between product integrals)
Claim 1
(Product integral
is a contraction)Claim 2
(Bound on product integral
times interpolant between kernels)
Claim 3
(Bound on∥e−XXTtX∥)Claim 4
(Approximate product
integral with exponential)
Claim 6
(Telescoping argument,
asymmetric matrices)
Claim 5
(Telescoping argument,
symmetric matrices)
Figure 1: Proof structure.
11Published in Transactions on Machine Learning Research (11/2023)
A.3 Basic facts about boundedness and continuity of the kernel
We begin with a series of simple propositions. Recall Proposition 3.1, which we restate below with a slight
strengthening in (11) which will be needed later on.
Lemma A.1 (Restatement of Proposition 3.1) .For any time t,
∥w(t)−w0∥≤/radicalbig
tR0/αand∥¯w(t)−w0∥≤/radicalbig
tR0/α.
and furthermore
/integraldisplayt
0∥dw
dτ∥dτ≤/radicalbig
tR0/αand/integraldisplayt
0∥d¯w
dτ∥dτ≤/radicalbig
tR0/α. (11)
Proof.The statement (11) is also implied by the proof of Proposition 3.1 in the main text.
The first implication is that the weights w(t)stay within the ball of radius ρaround w0, during the time-span
that we consider.
Lemma A.2. For any time 0≤t≤T, we have∥w(t)−w0∥≤ρ.
Proof.Immediate from Lemma A.1 and the fact that T≤ρ2α2/R0.
This allows to use the bounds Lip(h)andLip(Dh)on the Lipschitz constants of handDh. Specifically, the
kernelsKtandK0stay close during training, in the sense that the difference between DhandD¯hduring
training is bounded.
Lemma A.3. For any time 0≤t≤T, we have
∥Dh(w(t))−D¯h(¯w(t))∥≤Lip(Dh)/radicalbig
tR0/α
Proof.Since (a) ¯his the linearization of hatw0, and (b)∥w(t)−w0∥≤min(ρ,√tR0/α)by Lemma A.2
and Lemma A.1,
∥Dh(w(t))−D¯h(¯w(t))∥(a)=∥Dh(w(t))−Dh(w0)∥(b)
≤Lip(Dh)/radicalbig
tR0/α.
And therefore the kernel Ktis bounded at all times 0≤t≤Tin operator norm.
Lemma A.4. For any time 0≤t≤T, we have∥Kt∥≤3∥Dh(w(0))∥2+ 2Lip(Dh)tR0/α2.
Proof.By triangle inequality and Lemma A.3,
∥Kt−K0∥=∥Dh(w(t))Dh(w(t))⊤−Dh(w(0))Dh(w(0))⊤∥
≤∥Dh(w(t))∥2+∥Dh(w(0))∥2
≤(∥Dh(w(t))−Dh(w(0))∥+∥Dh(w(0))∥)2+∥Dh(w(0))∥2
≤3∥Dh(w(0))∥2+ 2Lip(Dh)tR0/α2.
And finally we note that the kernel evolves continously in time.
Lemma A.5. The mapt∝⇕⊣√∫⊔≀→Ktis continuous (in the operator norm topology) in the interval [0,T].
Proof.First,t∝⇕⊣√∫⊔≀→w(t)is continuous in time, since it solves the gradient flow. Second, we know that w(t)is
in the ball of radius ρaround w0by Lemma A.2, and in this ball the map w∝⇕⊣√∫⊔≀→Dh(w)is continuous because
Lip(Dh)<∞. Finally,Dh∝⇕⊣√∫⊔≀→DhDh⊤is continuous.
12Published in Transactions on Machine Learning Research (11/2023)
A.4 Product integral formulation of dynamics
Now we can present an equivalent formulation of the training dynamics (10)in terms of product integration.
For any 0≤x≤y≤T, letP(y,x) :Rp→Fsolve the operator integral equation
P(y,x) =I−/integraldisplayy
xKtP(t,x)dt. (12)
A solutionP(y,x)is guaranteed to exist and to be unique:
Lemma A.6. The unique solution to the integral equation (12)is given as follows. For any 0≤x≤y≤T
definesm,j= (y−x)(j/m) +xandδ= (y−x)/m, and letP(y,x)be the product integral
P(y,x) :=y/productdisplay
xe−Ksds:= lim
m→∞m/productdisplay
j=1e−δKsj= lim
m→∞e−δKsme−δKsm−1...e−δKs2e−δKs1.
Proof.Existence, uniqueness, and the expression as an infinite product are guaranteed by Theorems 3.4.1,
3.4.2, and 3.5.1 of Dollard & Friedman (1984), since t∝⇕⊣√∫⊔≀→Ktlies inL1
s(0,T), which is the space of “strongly
integrable” functions on [0,T]defined in Definition 3.3.1 of Dollard & Friedman (1984). This fact is guaranteed
by the separability of Fand the continuity and boundedness of t∝⇕⊣√∫⊔≀→Kt(Lemmas A.4 and A.5).
The operators P(y,x)are the time-evolution operators corresponding to the differential equation (10)for the
residual error rt. Namely, for any time 0≤t≤T,
rt=P(t,0)r0.
On the other hand, the solution to the linearized dynamics (10) is given by
¯rt=e−K0tr0,
sincee−K0tis the time-evolution operator when the kernel does not evolve with time.
A.5 Error bound between product integrals
To prove Theorem 1.2, it suffices to bound ∥P(t,0)−e−K0t∥, the difference of the time-evolution operators
under the full dynamics versus the linearized dynamics. We will do this via a general theorem. To state it,
we must define the total variation norm of a time-indexed sequence of operators:
Definition A.7. Let{Ct}t∈[x,y]be a sequence of time-bounded operators Ct:Rp→Fso thatt∝⇕⊣√∫⊔≀→Ctis
continuous in the interval [x,y]. Then the total variation norm of {Ct}t∈[x,y]is
V({Ct}t∈[x,y]) = sup
P∈PnP−1/summationdisplay
i=1∥Cti−Cti−1∥,
where the supremum is taken over partitions P={P={x=t1≤t2≤···≤tnP−1≤tnP=y}}of the
interval [x,y].
We may now state the general result.
Theorem A.8. LetFbe a separable Hilbert space, and let {At}t∈[0,T],{Bt}t∈[0,T]be time-indexed sequences
of bounded operators At:Rp→FandBt:Rp→Fsuch thatt∝⇕⊣√∫⊔≀→Atandt∝⇕⊣√∫⊔≀→Btare continuous in [0,T].
Then,
∥T/productdisplay
0e−AsA⊤
sds−T/productdisplay
0e−BsB⊤
sds∥≤( sup
t∈[0,T]∥At−Bt∥)(2√
T+ 3T·V({At−Bt}t∈[0,T])).
If we can establish Theorem A.8, then we may prove Theorem 1.2 as follows.
13Published in Transactions on Machine Learning Research (11/2023)
Proof of Theorem 1.2. for eacht≥0we choose the linear operators At,Bt:Rp→FbyAt=Dh(w(t))and
Bt=Dh(w(0))so thatAtA⊤
t=KtandBtB⊤
t=K0. We know that At,Btare bounded by Lemma A.4,
and thatt∝⇕⊣√∫⊔≀→Atis continuous by Lemma A.5. (Also t∝⇕⊣√∫⊔≀→Btis trivially continuous). So we may apply
Theorem A.8 to bound the difference in the residuals.
We first bound the total variation norm of {At−Bt}t∈[0,T], By (a) the fact from Lemma A.2 that w(t)is in
a ball of radius at most ρaround w0where w∝⇕⊣√∫⊔≀→Dh(w)isLip(Dh)-Lipschitz; (b) the fact that t∝⇕⊣√∫⊔≀→w(t)is
differentiable, since it solves a gradient flow; and (c) Lemma A.1,
V({At−Bt}t∈[0,T]) = sup
P∈PnP−1/summationdisplay
i=1∥Dh(w(ti+1))−Dh(w(0))−Dh(w(ti)) +Dh(w(0))∥
= sup
P∈PnP−1/summationdisplay
i=1∥Dh(w(ti+1))−Dh(w(ti))∥
(a)
≤Lip(Dh) sup
P∈PnP−1/summationdisplay
i=1∥w(ti+1)−w(ti)∥
(b)= Lip(Dh)/integraldisplayT
0∥dw
dt∥dt
(c)
≤Lip(Dh)/radicalbig
TR0/α
When we plug the above expression into Theorem A.8, along with the bound ∥At−Bt∥≤Lip(Dh)√tR0/α
from Lemma A.3, we obtain:
∥rT−¯rT∥=∥(T/productdisplay
s=0e−Ksds−T/productdisplay
s=0e−K0ds)r0∥
≤(Lip(Dh)/radicalbig
TR0/α)(2√
T+ 3Lip(Dh)T3/2/radicalbig
R0/α)/radicalbig
2R0
= (2κ+ 3κ2)/radicalbig
2R0,
whereκ=T
αLip(Dh)√R0.
Also note that
∥rT−¯rT∥≤∥rT∥+∥¯rT∥=/radicalbig
2R(αh(w(T))) +/radicalig
2R(α¯h(¯w(T)))≤2/radicalbig
2R0,
since the gradient flow does not increase the risk. So
∥rT−¯rT∥≤min(2κ+ 3κ2,2)/radicalbig
2R0≤6κ/radicalbig
R0.
It remains only to prove Theorem A.8.
A.6 Reduction to the finite-dimensional case
We first show that in order to prove Theorem A.8, it suffices to consider the case where Fis a finite-dimensional
Hilbert space. The argument is standard, and uses the fact that Fhas a countable orthonormal basis.
Lemma A.9. Theorem A.8 is true for general separable Hilbert spaces Fif it is true whenever Fis
finite-dimensional.
Proof.Suppose thatFis a separable Hilbertspace and At,Bt:Rp→Fsatisfy thehypotheses of Theorem A.8.
Let{fi}i∈Nbe a countable orthonormal basis for F, which is guaranteed by separability of F. For anyn, let
14Published in Transactions on Machine Learning Research (11/2023)
Pn:F→Fbe the linear projection operator defined by
Pn(fi) =/braceleftigg
fi,1≤i≤n
0,otherwise.
By (a) Duhamel’s formula in Theorem 3.5.8 of Dollard & Friedman (1984), (b) the fact that ∥e−AsA⊤
s∥≤1
and∥e−PnAsA⊤
sP⊤
n∥≤1becauseAsA⊤
sandPnAsA⊤
sP⊤
nare positive semidefinite.
∥T/productdisplay
0e−AsA⊤
sds−T/productdisplay
0e−PnAsA⊤
sP⊤
nds∥
(a)=∥/integraldisplayT
0/parenleftiggT/productdisplay
τe−AsA⊤
sds/parenrightigg
(AτA⊤
τ−PnAτA⊤
τP⊤
n)/parenleftiggτ/productdisplay
0e−PnAsA⊤
sP⊤
nds/parenrightigg
dτ∥
≤/integraldisplayT
0∥T/productdisplay
τe−AsA⊤
sds∥∥AτA⊤
τ−PnAτA⊤
τP⊤
n∥∥τ/productdisplay
0e−PnAsA⊤
sP⊤
nds∥dτ
(b)
≤/integraldisplayT
0∥AτA⊤
τ−PnAτA⊤
τP⊤
n∥dτ (13)
We have chosenPnso that for any bounded linear operator M:F→F, we have limn→∞PnMPn=M.
Sinceτ∝⇕⊣√∫⊔≀→AτA⊤
τis continuous in τ, andAτis bounded for each τ, the expression in (13)converges to 0 as
n→∞. By triangle inequality, we conclude that
∥T/productdisplay
0e−AsA⊤
sds−T/productdisplay
0e−BsB⊤
sds∥≤lim sup
n→∞∥T/productdisplay
0e−PnAsA⊤
sP⊤
nds−T/productdisplay
0e−PnBsB⊤
sP⊤
nds∥.
Notice thatPnAtandPnBtare bounded maps from Rptospan{f1,...,fn}, andt∝⇕⊣√∫⊔≀→PnAtandt∝⇕⊣√∫⊔≀→PnBt
are continuous and bounded. So using the theorem in the case where Fis finite-dimensional, the right-hand
side can be bounded by
lim sup
n→∞∥T/productdisplay
0e−PnAsA⊤
sP⊤
nds−T/productdisplay
0e−PnBsB⊤
sP⊤
nds∥
≤lim sup
n→∞( sup
t∈[0,T]∥PnAt−PnBt∥)(2√
T+ 3T·V({PnAt−PnBt}t∈[0,T])dt)
= ( sup
t∈[0,T]∥At−Bt∥)(2√
T+ 3T·V({At−Bt}t∈[0,T])dt).
A.7 Bound for the finite-dimensional case
We conclude by proving Theorem A.8 in the finite-dimensional case, where we write At,Bt∈Rn×das
time-indexed matrices. In order to prove a bound, we will interpolate between the dynamics we wish to
compare. Let
Ct=Bt−At∈Rn×d
For anyζ∈[0,1]andt∈[0,T], we define the kernel
Kt,ζ= (At+ζCt)(At+ζCt)⊤∈Rn×n.
This interpolates between the kernels of interest, since on the one hand, Kt,0=AtA⊤
tand on the other
Kt,1=BtB⊤
t. For anyx,y∈[0,T], let
P(y,x;ζ) =y/productdisplay
xe−Kt,ζdt∈Rn×n.
15Published in Transactions on Machine Learning Research (11/2023)
The derivative∂
∂ζP(y,x;ζ)exists by Theorem 1.5.3 of Dollard & Friedman (1984), since (i) Kt,ζis continuous
intfor each fixed ζ, (ii)Kt,ζis differentiable in ζin theL1sense4, and (iii) has a partial derivative∂Kt,ζ
∂ζ
that is integrable in t. The formula for∂
∂ζP(y,x;ζ)is given by the following formula, which generalizes the
integral representation of the exponential map:5
∂
∂ζP(y,x;ζ) =−/integraldisplayy
xP(y,t;ζ)∂Kt,ζ
∂ζP(t,x;ζ)dt (14)
Our main lemma is:
Lemma A.10. For allζ∈[0,1], we have the bound
∥∂P(T,0;ζ)
∂ζ∥≤( sup
t∈[0,T]∥Ct∥)(2√
T+ 3T·V({Ct}t∈[0,T])).
This lemma suffices to prove Theorem A.8.
Proof of Theorem A.8. Using the fundamental theorem of calculus,
∥T/productdisplay
0e−AtA⊤
tdt−T/productdisplay
0e−BtB⊤
tdt∥=∥P(T,0; 1)−P(T,0; 0)∥≤/integraldisplay1
0∥∂P(T,0;ζ)
∂ζ∥dζ,
which combined with Lemma A.10 proves Theorem A.8.
A.8 Proof of Lemma A.10
By a direct calculation,
∂Kt,ζ
∂ζ= (At+ζCt)C⊤
t+Ct(At+ζCt)⊤,
so, by (14),
∂
∂ζP(T,0;ζ) =−/integraldisplayT
0P(T,t;ζ)((At+ζCt)C⊤
t+Ct(At+ζCt)⊤)P(t,0;ζ)dt
=−/integraldisplayT
0P(T,t;ζ)(At+ζCt)C⊤
tP(t,0;ζ)dt
/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
M1−/integraldisplayT
0P(T,t;ζ)Ct(At+ζCt)⊤P(t,0;ζ)dt
/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
M2.
The arguments are similar for bounding M1andM2, so we only bound M1. We will need two technical
bounds, whose proofs are deferred to Section A.9.
Claim 1. For any 0≤t≤T, we have∥P(t,0;ζ)∥≤1.
Claim 2. For any 0≤t≤T, we have∥P(T,t;ζ)(At+ζCt)∥≤1
2√T−t+ 3V({Cs}s∈[t,T]).
Using (a) Claim 1, and (b) Claim 2,
∥M1∥(a)
≤( sup
t∈[0,T]∥Ct∥)/integraldisplayT
0∥P(T,t;ζ)(At+ζCt)∥dt (15)
(b)
≤( sup
t∈[0,T]∥Ct∥)/parenleftigg/integraldisplayT
01
2√
T−t+ 3V({Cs}s∈[t,T])dt/parenrightigg
(16)
= ( sup
t∈[0,T]∥Ct∥)(√
T+ 3/integraldisplayT
0V({Cs}s∈[t,T])dt). (17)
4For anyζ∈[0,1],limζ′→ζ/integraltextT
0∥Kt,ζ′−Kt,ζ
ζ′−ζ−∂Kt,ζ
∂ζ∥dt= 0, since the matrices At,Btare uniformly bounded.
5The proof is a consequence of Duhamel’s formula. This a tool used in a variety of contexts, including perturbative analysis
of path integrals in quantum mechanics Dollard & Friedman (1984).
16Published in Transactions on Machine Learning Research (11/2023)
Lemma A.10 is proved by noting that, symmetrically,
∥M2∥≤( sup
t∈[0,T]∥Ct∥)(√
T+ 3/integraldisplayT
0V({Cs}s∈[0,t])dt),
and, for any t∈[0,T],
V({Cs}s∈[0,t]) +V({Cs}s∈[t,T]) =V({Cs}s∈[0,T]).
A.9 Deferred proofs of Claims 1 and 2
Claim 1. For any 0≤t≤T, we have∥P(t,0;ζ)∥≤1.
Proof.This follows from the definition of the product integral as an infinite product, and the fact that each
terme−δKti,ζin the product has norm at most 1 because Kti,ζis positive semidefinite.
In order to prove Claim 2, we need two more claims:
Claim 3. For anyX∈Rn×dandt≥0,
∥e−XX⊤tX∥≤1
2√
t
Proof.SinceXX⊤is positive semidefinite,
∥e−XX⊤X∥=/radicalig
∥e−XX⊤tXX⊤e−XX⊤t∥≤sup
λ≥0√
e−λtλe−λt= sup
λ≥0e−λ/radicalbig
λ/t≤1
2√
t.
Claim 4. For any time-indexed sequence of matrices (Xt)t∈[0,T]inRn×dsuch thatt∝⇕⊣√∫⊔≀→Xtis continuous in
[0,T], and any 0≤a≤b≤T,
∥e−XbX⊤
b(b−a)Xb−/parenleftiggb/productdisplay
ae−XsX⊤
sds/parenrightigg
Xa∥≤3V({Xs}s∈[a,b])
This latter claim shows that we can approximate the product integral with an exponential. The proof is
involved, and is provided in Section A.10. Assuming the previous two claims, we may prove Claim 2.
Claim 2. For any 0≤t≤T, we have∥P(T,t;ζ)(At+ζCt)∥≤1
2√T−t+ 3V({Cs}s∈[t,T]).
Proof.By Claim 3, since KT,ζ= (AT+ζCT)(AT+ζCT)⊤,
∥e−KT,ζ(T−t)(AT+ζCT)∥≤1
2√
T−t.
By the triangle inequality, it remains to prove
∥e−KT,ζ(T−t)(AT+ζCT)−P(T,t;ζ)(At+ζCt)⊤∥≤3V({Cs}s∈[t,T]),
and this is implied by Claim 4, defining Xt=At+ζCtanda=t,b=T.
17Published in Transactions on Machine Learning Research (11/2023)
A.10 Deferred proof of Claim 4
The proof will be by interpolation, using the integral representation of the exponential map provided in (14),
similarly to the main body of the proof, but of course interpolating with respect to a different parameter. We
begin the following claim, which we will subsequently strengthen.
Claim 5. For any symmetric X,Y∈Rn×nandt≥0,
∥e−(X+Y)2t(X+Y)−e−X2tX∥≤3∥Y∥
Proof.For anyτ∈[0,1], defineX(τ) =X+τY, which interpolates between XandY. Then by (a) the
derivative of the exponential map in (14), and (b) the fact that X′(τ) =Yand∥e−X2(τ)t/2∥≤1,
∥e−(X+Y)2t(X+Y)−e−X2tX∥
=∥/integraldisplay1
0d
dτ/parenleftig
e−X2(τ)tX(τ)/parenrightig
dτ∥
=∥/integraldisplay1
0d
dτ/parenleftig
e−X2(τ)t/2X(τ)e−X2(τ)t/2/parenrightig
dτ∥
≤sup
τ∈[0,1]∥d
dτe−X2(τ)t/2X(τ)e−X2(τ)t/2∥
(a)= sup
τ∈[0,1]∥e−X2(τ)t/2X′(τ)e−X2(τ)t/2
−(t/2)/integraldisplay1
0e−(1−s)X2(τ)t/2(X′(τ)X(τ) +X(τ)X′(τ))e−sX2(τ)t/2X(τ)e−X2(τ)t/2ds
−(t/2)/integraldisplay1
0e−X2(τ)t/2X(τ)e−(1−s)X2(τ)t/2(X′(τ)X(τ) +X(τ)X′(τ))e−sX2(τ)t/2ds∥
(b)
≤sup
τ∈[0,1]∥Y∥+∥(t/2)/integraldisplay1
0e−(1−s)X2(τ)t/2(YX(τ) +X(τ)Y)e−sX2(τ)t/2X(τ)e−X2(τ)t/2ds∥
/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
T1
+∥(t/2)/integraldisplay1
0e−X2(τ)t/2X(τ)e−(1−s)X2(τ)t/2(YX(τ) +X(τ)Y)e−sX2(τ)t/2ds∥
/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
T2,
and by (a) supλ≥0λe−λ2t/2= 1/√
et, and (b)∥e−M∥≤1ifMis p.s.d.,
T1≤(t/2)/integraldisplay1
0∥e−(1−s)X2(τ)t/2(YX(τ) +X(τ)Y)e−sX2(τ)t/2∥∥X(τ)e−X2(τ)t/2∥ds
(a)
≤(t/2)/integraldisplay1
0∥e−(1−s)X2(τ)t/2(YX(τ) +X(τ)Y)e−sX2(τ)t/2∥1√
etds
≤√
t
2√e/integraldisplay1
0∥e−(1−s)X2(τ)t/2∥∥Y∥∥X(τ)e−sX2(τ)t/2∥+∥e−(1−s)X2(τ)t/2X(τ)∥∥Y∥∥e−sX2(τ)t/2∥ds
(a)
≤√
t
2e/integraldisplay1
0∥e−(1−s)X2(τ)t/2∥∥Y∥1√
st+1/radicalbig
(1−s)t∥Y∥∥e−sX2(τ)t/2∥ds
(b)
≤∥Y∥
2e/integraldisplay1
01√s+1/radicalbig
(1−s)ds
=∥Y∥
2e(2√s−2√
1−s)1
0
≤∥Y∥.
Similarly,T2≤∥Y∥.
18Published in Transactions on Machine Learning Research (11/2023)
Claim 6. For anyX,Y∈Rn×d(not necessarily symmetric) and t≥0,
∥e−(X+Y)(X+Y)⊤t(X+Y)−e−XX⊤tX∥≤3∥Y∥.
Proof.We will use Claim 5, combined with a general method for lifting facts from symmetric matrices to
asymmetric matrices (see, e.g., Tao (2011) for other similar arguments). Assume without loss of generality
thatn=d, since otherwise we can pad with zeros. Define ¯n= 2nand
¯X=/bracketleftbigg
0X⊤
X 0/bracketrightbigg
∈R¯n×¯nand ¯Y=/bracketleftbigg
0Y⊤
Y 0/bracketrightbigg
∈R¯n×¯n.
These are symmetric matrices by construction. Furthermore,
¯X2=/bracketleftbigg
X⊤X 0
0XX⊤/bracketrightbigg
and(¯X+¯Y)2=/bracketleftbigg
(X+Y)⊤(X+Y) 0
0 ( X+Y)(X+Y)⊤/bracketrightbigg
.
Because of the block-diagonal structure of these matrices,
e−¯X2t=/bracketleftigg
e−X⊤Xt0
0e−XX⊤t/bracketrightigg
ande−(¯X+¯Y)2t=/bracketleftigg
e−(X+Y)⊤(X+Y)t0
0e−(X+Y)(X+Y)⊤t/bracketrightigg
.
So
e−¯X2t¯X=/bracketleftigg
0e−X⊤XtX⊤
e−XX⊤tX 0/bracketrightigg
.
Similarly,
e−(¯X+¯Y)2t(¯X+¯Y) =/bracketleftigg
0 e−(X+Y)⊤(X+Y)t(X+Y)⊤
e−(X+Y)(X+Y)⊤t(X+Y) 0/bracketrightigg
.
For any matrix M∈Rn×n, we have∥M∥=supv∈Sn−1∥Mv∥. So for any matrices M1,M2∈Rn×n, we have
/vextenddouble/vextenddouble/vextenddouble/vextenddouble/bracketleftbigg0M1
M20/bracketrightbigg/vextenddouble/vextenddouble/vextenddouble/vextenddouble≥sup
v∈Sn−1/vextenddouble/vextenddouble/vextenddouble/vextenddouble/bracketleftbigg0M1
M20/bracketrightbigg/bracketleftbiggv
0/bracketrightbigg/vextenddouble/vextenddouble/vextenddouble/vextenddouble= sup
v∈Sn−1∥M2v∥=∥M2∥.
This means that, using Claim 5,
∥e−(X+Y)(X+Y)⊤t(X+Y)−e−XX⊤tX∥≤∥e−(¯X+¯Y)2t(¯X+¯Y)−e¯X2t¯X∥≤3∥¯Y∥.
Finally, using the symmetry of ¯Yand the block-diagonal structure of ¯Y2,
∥¯Y∥=∥¯Y2∥1/2=/vextenddouble/vextenddouble/vextenddouble/vextenddouble/bracketleftbiggY⊤Y 0
0YY⊤/bracketrightbigg/vextenddouble/vextenddouble/vextenddouble/vextenddouble1/2
= max(∥Y⊤Y∥1/2,∥YY⊤∥1/2) =∥Y∥.
We conclude by using these results to prove Claim 4 with a telescoping argument.
Claim 4. For any time-indexed sequence of matrices (Xt)t∈[0,T]inRn×dsuch thatt∝⇕⊣√∫⊔≀→Xtis continuous in
[0,T], and any 0≤a≤b≤T,
∥e−XbX⊤
b(b−a)Xb−/parenleftiggb/productdisplay
ae−XsX⊤
sds/parenrightigg
Xa∥≤3V({Xs}s∈[a,b])
19Published in Transactions on Machine Learning Research (11/2023)
Proof.We will do this by approximating the product integral by a finite product of mmatrices, and taking
the limitm→∞. For anym≥0andj∈{0,...,m}, andt∈[0,T], lettm,j= (b−a)(j/m) +a, and define
the finite-product approximation to the product integral for any k∈{0,...,m}
Pm,k=
m/productdisplay
j=k+1Qm,j
Qk
m,k,
where
Qm,j=e−Xtm,jX⊤
tm,j(b−a)/m.
Notice that for k= 0we have
Pm,0=
m/productdisplay
j=1e−Xtm,jX⊤
tm,j(b−a)/m
,
and so by continuity of s∝⇕⊣√∫⊔≀→Xsfors∈[0,T]and boundedness of Xs, we know from Theorem 1.1 of Dollard
& Friedman (1984) that
lim
m→∞Pm,0=b/productdisplay
ae−XsX⊤
sds.
This means that
∥e−XbX⊤
b(b−a)Xb−/parenleftiggb/productdisplay
ae−XsX⊤
sds/parenrightigg
Xa∥=∥Pm,mXtm,m−/parenleftiggb/productdisplay
ae−XsX⊤
sds/parenrightigg
Xa∥
= lim
m→∞∥Pm,mXtm,m−Pm,0Xtm,0∥.
Finally, telescoping and (a) using ∥Qm,j∥≤1for allj, and (b) Claim 6,
∥Pm,mXtm,m−Pm,1Xtm,1∥≤m/summationdisplay
k=1∥Pm,kXtm,k−Pm,k−1Xtm,k−1∥
=m/summationdisplay
k=1∥
m/productdisplay
j=k+1Qm,j
(Qk
m,kXtm,k−Qm,k(Qm,k−1)k−1Xtm,k−1)∥
(a)
≤m/summationdisplay
k=1∥(Qm,k)k−1Xtm,k−(Qm,k−1)k−1Xtm,k−1∥
(b)
≤m/summationdisplay
k=13∥Xtm,k−Xtm,k−1∥
≤3V({Xs}s∈[a,b]).
The lemma follows by taking m→∞, since the bound is independent of m.
B Proof of Theorem 1.3
The converse bound in Theorem 1.2 is achieved in the simple case where h:R→Ris given by h(w) =
aw+1
2bw2fora=1√
Tandb=Lip(Dh). We also let w0= 0andy∗=√2R0so that all conditions are
satisfied. The evolution of the residuals r(t) =y∗−αh(w(t))and¯r(t) =y∗−α¯h( ¯w(t))is given by
dr
dt=−Ktrandd¯r
dt=−K0¯r,
whereKt=Dh(w(t))Dh(w(t))⊤= (a+bw(t))2andK0=a2. Sincer=y∗−α(aw+b
2w2), we can express
the evolution of the residuals rand¯ras:
dr
dt=−(a2+ 2b(y∗−r)/α)randd¯r
dt=−a2¯r. (18)
20Published in Transactions on Machine Learning Research (11/2023)
Sinceb(y∗−r)/α≥0at all times, we must have a2+ 2b(y∗−r)/α≥a2. This means that at all times
r(t)≤¯r(t) =e−a2ty∗.
So, at any time t≥T/2,
r(t)≤r(T/2)≤¯r(T/2) =e−(1/√
T)2(T/2)y∗=e−1/2y∗,.
By plugging this into (18), for times t≥T/2,
dr
dt≤−(a2+ 2Lip(Dh)(1−e−1/2)/radicalbig
2R0/α)r≤−(a2+ 1.1κ/T)r.
So, at time T, assuming that κ≤1without loss of generality,
r(T)≤r(T/2)e−(1/T+1.1κ/T)(T/2)=r(T/2)e−1/2−0.55κ≤y∗e−1e−0.55κ≤y∗e−1(1−0.4κ).
So
|αh(w(T))−α¯h( ¯w(T))|=|r(T)−¯r(T)|≥|e−1y∗−(1−0.4κ)e−1y∗|≥0.4κe−1/radicalbig
2R0≥/radicalbig
R0/5.
C Deferred details from Section 2
The bound on Lip(Dh)for 2-layer networks is below.
Lemma C.1 (Bound on Lip(Dh)for mean-field 2-layer network) .Suppose that there is a constant Ksuch that
(i) the activation function σis bounded and has bounded derivatives ∥σ∥∞,∥σ′∥∞,∥σ′′∥∞,∥σ′′′∥∞≤K, (ii)
the weights have bounded norm ∥a∥+∥U∥≤K, and (iii) the data points have bounded norm maxi∥xi∥≤K.
Then there is a constant K′depending only Ksuch that
Lip(Dh)≤K′.
Proof.Letp=m+mdbe the number of parameters of the network. Then Dh∈Rn×pis
Dh=1√n
Dfw(x1)
...
Dfw(xn)
.
So
Lip(Dh)≤max
i∈[n]Lip(Dfw(xi)).
21Published in Transactions on Machine Learning Research (11/2023)
So for the 2-layer network,
Lip(Dh)≤max
i∈[n]1
mLip(/bracketleftbigσ(√m⟨u1,xi⟩)... σ (√m⟨um,xi⟩)/bracketrightbig
)
+1√mLip(/bracketleftbig
a1σ′(√m⟨u1,xi⟩)x⊤
i... amσ′(√m⟨um,xi⟩)x⊤
i/bracketrightbig
)
= max
i∈[n]1√m/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble
σ′(√m⟨u1,xi⟩)x⊤
i 0 0 ... 0
0 σ′(√m⟨u2,xi⟩)x⊤
i0... 0
...
0 ... 0σ′(√m⟨um,xi⟩)x⊤
i
/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble
+∥xi∥√mLip(/bracketleftbiga1σ′(√m⟨u1,xi⟩)... amσ′(√m⟨um,xi⟩)/bracketrightbig
)
≤∥σ′∥∞∥xi∥√m+∥xi∥√m∥diag(/bracketleftbigσ′(√m⟨u1,xi⟩)... σ′(√m⟨um,xi⟩)/bracketrightbig
)∥
+∥xi∥/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble
a1σ′′(√m⟨u1,xi⟩)x⊤
i0... 0
......
0 ... 0amσ′′(√m⟨um,xi⟩)x⊤
i
/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble
≤2∥σ′∥∞∥xi∥√m+∥σ′′∥∞∥xi∥2∥a∥∞
22