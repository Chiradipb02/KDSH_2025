Published in Transactions on Machine Learning Research (10/2023)
Non-Stationary Contextual Pricing with Safety Con-
straints
Dheeraj Baby∗dheeraj@cs.ucsb.edu
Department of Computer Science
University of California, Santa Barbara
Jianyu Xu∗xu_jy15@cs.ucsb.edu
Department of Computer Science
University of California, Santa Barbara
Yu-Xiang Wang yuxiangw@cs.ucsb.edu
Department of Computer Science
University of California, Santa Barbara
Reviewed on OpenReview: https: // openreview. net/ forum? id= fWIQ9Oaao0
Abstract
In a contextual pricing problem, a seller aims at maximizing the revenue over a sequence
of sales sessions (described by feature vectors) using binary-censored feedback of “sold” or
“not sold”. Existing methods often overlook two practical challenges (1) the best pricing
strategy could change over time; (2) the prices and pricing policies must conform to hard
constraints due to safety, ethical or legal restrictions. We address both challenges by solving
a more general problem of universal dynamic regret minimization in properonline learning
with exp-concave losses — an open problem posed by Baby & Wang (2021) that we partially
resolve in this paper, with attention restricted to loss functions coming from a generalized
linear model. Here “dynamic regret” measures the performance relative to a non-stationary
sequence of policies, and “proper” means that the learner must choose feasiblestrategies
within a pre-defined convex set, which we use to model the safety constraints. In this work,
we consider a linear noisy valuation model for the customers. With the market noises drawn
from aknown strictly log-concave distribution, our algorithm achieves ˜O(d3T1/3C2/3
T∨d3)
dynamic regret in comparison with the optimal policy series, where T,dandCTstand for the
time horizon, the feature dimension and the total variation (characterizing non-stationarity)
respectively. This regret is near-optimal with respect to T(withinO(logT)gaps) andCT,
and our algorithm is adaptable to unknownCTand remains feasiblethroughout. However,
the dependence on dis suboptimal and the minimax rate is still open.
1 Introduction
Feature-based dynamic pricing, or contextual pricing, is a problem where the seller sets prices for different
products based on their features and aims to maximize revenue. In general, a customer will make her decision
based on a comparison between the price and her own valuation of the product. Formally, many existing
works (e.g., Cohen et al., 2020; Javanmard & Nazerzadeh, 2019; Xu & Wang, 2021; Luo et al., 2021) adopt
the following linear-feature valuation model:
∗for equal contribution.
1Published in Transactions on Machine Learning Research (10/2023)
Contextual pricing. For t= 1,2,...,T :
1. A context xt∈Rdis revealed that describes a sales session (product, customer and context).
2. The customer valuates the product as yt=x⊤
tθ∗
t+Ntusingxt.
3. The seller proposes a price vt>0concurrently (according to xtand historical sales records).
4. The transaction is successful if vt≤yt, i.e., the seller gets a reward (payment) of rt=vt· 1(vt≤yt).
HereTis the unknown time horizon, xt’s are adversarial features (which can be stochastic or non-stochastic
series),θ∗
t’s arehidden parameters mapping features to valuations linearly, and Nt’s are i.i.d. noises drawn
from a known distribution D. Denote 1t:= 1(vt≤yt)as theBoolean-censored feedback that equals 1if
vt≤ytand0otherwise, and we only observe 1tinstead of the realized ytat each round. Our goal is to
maximize the cumulative expected reward, and the regretis defined as the difference of expected rewards
betweenvtand the best price at each round.
Time-variant Behavior and Dynamic Regret. Comparing with existing linear contextual pricing
problem settings (e.g., Cohen et al., 2020; Javanmard & Nazerzadeh, 2019; Xu & Wang, 2021) where the
linear valuation parameter θ∗
tis fixed as the same θ∗over allt, in this work we allow moderate changing
of customers’ valuations: i.e. θ∗
t’s can vary over time, and the total variation/summationtextT−1
t=1∥θ∗
t−θ∗
t+1∥1is upper
bounded by some CT(which could be unknown to the seller). Here we adopt the L1-norm bound because it
is a reasonable metric for capturing the non-stationarity of the valuation mechanism: For instance, suppose
each element of xtindicates the amount of one component of this product, and therefore each element
ofθ∗
tindicates the unit price of this component. In this example, ∥θ∗
t−θ∗
t+1∥1reflects the general price
fluctuations on the market, i.e., the sum of market-wise price changes over all components. To characterize
the performance of a pricing scheme under this non-stationary setting, we adopt the concept of dynamic
regret. In this notion, we compare the performance of vtwe propose with that of the optimal pricing policy
that knows the sequence of θ∗
tin advance. A rigorous definition of this dynamic regret will be presented in
Section 2.3.
Proper Learning. Usually, the actions/strategies we are allowed to adopt are restricted in some specific safe
domains. Taking any action/strategy outside this domain would probably cause risky, illegal or inconsistent
outcomes. Our algorithm works by maintaining an estimate, θt, for the true valuation parameter θ∗
tat each
roundt, and we in turn take θtas aparametric strategy for proposing the price vtaccording to a greedy policy
(see Section 2.3 for more details). In this work, we require that the estimate θtmust fall in a specific convex
and closed domain Dtat each round t. HereDtcan be chosen adversarially with the constraint imposed by
Assumption 1. As will be explained in Section 2.4, this is to address the fact that pricing strategies must
conform to hard constraints due to safety restrictions.
Universal Dynamic Regret and Proper OCO with co-variates. Next, we take a digression and
describe a general Online Convex Optimization (OCO) setting which will play a pivotal role in solving the
contextual pricing problem.
Proper OCO with co-variates. For t= 1,2,...,T :
1. Adversary reveals a co-variate xt∈Rd.
2. Learner makes a decision ˆθtin a convex domain Dt⊂Rd.
3. Adversary reveals a convex loss function ℓt(θ) =gt(θTxt).
This setting embodies OCO under a wide range of loss functions from the generalized linear model (GLM)
family for appropriate choices of gt. The co-variates xtcan be thought of as a feature that encodes valuable
information about the context in round twhich can be used by the learner to make its predictions. Examples
of this setting include (but are not limited to) linear regression and logistic regression.
The goal of the learner is to control its universal dynamic regret:
R(w1:T) :=T/summationdisplay
t=1ℓt(ˆθt)−ℓt(wt), (1)
2Published in Transactions on Machine Learning Research (10/2023)
wherew1:T={w1,w2,...,wT}isanycomparator sequence satisfying wt∈Dtfor allt∈[T]. This is known
to be a good metric in characterizing the performance of a learner in non-stationary environments (Zinkevich,
2003). Dynamic regret bounds are usually expressed in literature as functions of the time horizon Tand a path
length that captures the smoothness of the comparator sequence such as CT=/summationtextT−1
t=1∥wt−wt+1∥1.
1.1 Summary of Contributions
Our main contributions are given below.
1.We present an algorithm ProDR (Algorithm 1) that attains an optimal ˜O(d3(T1
3C2
3
T∨1))dynamic regret
(modulo dependencies in dandlogT) for the setting of proper OCO with co-variates under exp-concave
losses (see Section 3.1).
2.We construct an algorithm PDRP (Algorithm 2) with a base learner ProDR, which solves the non-stationary
contextual pricing problem with strictly log-concave noise. We define the dynamic regret of contextual
pricing as Eq.(3) and show that PDRP achieves a ˜O(d3(T1
3C2
3
T∨1))dynamic regret guarantee (see Section
3.2).
3.We show that any algorithm must incur a dynamic regret of Ω(T1
3C2
3
T∨1)in the contextual pricing
problem, which says that PDRP is minimax optimal up to dandlogTfactors (see Section 3.3).
Novelty. Owing to the reduction of Xu & Wang (2021), the non-stationary contextual pricing problem can
be reduced to an OCO problem with co-variates and exp-concave losses. The key subroutine we developed —
ProDR — is the first to achieve an optimaluniversal dynamic regret with exp-concave losses in the proper
OCO with covariate setting. ProDR makes considerable progress towards addressing the open problem posed
by Baby & Wang (2021) on the more general version of the above problem with general exp-concave losses
(rather than GLM with known covariates) . The only existing attempt to this open problem requires the
decision set to be an L∞ball (Baby & Wang, 2022b), which cannot be used to handle arbitrary convex
decision sets as we do.
Summary of techniques. The key technique in deriving ProDR is a novel “transfer theorem” which takes
the algorithm of Baby & Wang (2022b) ( L∞ball decision set) and converts it to an optimal algorithm for
the setting of proper OCO with co-variates under arbitrary convex decision sets . This idea is similar in spirit
to the improper-proper reduction in the work of Cutkosky & Orabona (2018) where they consider general
convex losses. However, a direct application of their reduction scheme cannot give fast rates for exp-concave
losses. To circumvent this issue, we propose new reduction schemes that carefully take the curvature of the
losses into account thereby allowing us to derive fast and optimal dynamic regret rates under exp-concave
losses (see Section 3.1 for a list of technical challenges). Such a “transfer theorem” could be of independent
interest and impactful in the general context of non-stationary online learning. That the non-stationary
dynamic pricing problem can be optimally solved using ProDR is a testament to this fact.
1.2 Related Works
Here we discuss how our work relates to the existing literature on dynamic pricing and dynamic regret.
Dynamic Pricing Dynamic pricing has been extensively studied under the single product (non-contextual)
setting (e.g. Kleinberg & Leighton, 2003; Besbes & Zeevi, 2015; Wang et al., 2021), where the goal is to find
and approach the best fixed price that maximizes the expected revenue. The problem is later generalized to
contextual pricing where a feature xtoccurs at each time tand the customer’s valuation is dependent on xt. A
widely adopted model is the linear valuation (e.g., Cohen et al., 2020; Javanmard & Nazerzadeh, 2019; Xu &
Wang, 2021), where they assume all customers’ valuations are a fixed feature-to-valuation mapping (i.e., θ∗
tis
fixed,∀t) adding i.i.d. noises drawn from a known distribution. As a result, the best price varies on different
features occurring over time, and the goal turns to approach the best price in every round. However, the
optimal pricing policy is static and the regret definition is a comparison of performance between our proposed
price and the optimal policy that knows θ∗and the noise distribution in advance. In this work, we adopt
this linear valuation setting and further generalize to non-stationary cases where the linear mapping θ∗
tis
changeable over time. As a result, the best pricing policy also changes according to θ∗
t, and we have to analyze
the algorithmic performance in the scale of dynamic regret . Leme et al. (2021) also studies non-stationary
3Published in Transactions on Machine Learning Research (10/2023)
pricing problems and adopts the dynamic regret metric. However, their loss function and constraints are
defined on realizedvaluations, different from our regret function defined on the expected valuations. Also,
they adopt a non-contextual problem setting and assume individual variation bounds on each pair of adjacent
valuations like|vt+1−vt|≤ϵtfor someϵt>0,t= 1,2,...,T. Under their assumptions, they achieve an
˜O(T1
2C1
2
T)optimal regret with a matching lower bound1. Given those differences in problem settings, their
regret bounds are not applicable to our problem.
Some existing works on stationary pricing problems adopt contextual bandits to achieve sub-linear static
regrets (Luo et al., 2021; Xu & Wang, 2022). Therefore, it is also possible to reduce our non-stationary pricing
problem to a non-stationary bandit problem (Chen et al., 2019; Cheung et al., 2022; Zhao et al., 2020a).
However, similarly to what Amin et al. (2014) indicates in stationary contextual pricing, a direct application
of non-stationary bandit algorithms to our known-noise-distribution setting might cause sub-optimality in
dynamic regret. Such a reduction might be optimal without the knowledge of noise distribution, but it is
beyond the scope of this work.
Besides, a recent stream of works study the pricing problems under constraints on inventory (Chen & Gallego,
2022), reserve-price (Niu et al., 2020), fairness (Cohen et al., 2021; 2022; Xu et al., 2023; Chen et al., 2023)
and budget (Salehi & Mirmohammadi, 2023), etc. Our work also contributes to this series as we enforce
safety constraints and adaptively measure the impact of those constraints on the dynamic regret.
Dynamic Regret. There is a rich body of literature aimed in minimizing the universal dynamic regret
(Eq.(1)) in OCO setting where the earliest works can be traced back to Zinkevich (2003). When the revealed
losses are convex, Zhang et al. (2018) proposes algorithms to attain an optimal dynamic regret rate of
O(/radicalbig
T(1 +PT))wherePT=/summationtextT−1
t=1∥wt−wt+1∥2. When the loss functions are gradient Lipschitz, problem
dependent regret bounds have been developed in the work of Zhao et al. (2020b). In addition to gradient
Lipschitzness, if the losses have extra curvature properties such as exp-concavity, Baby & Wang (2021)
proposes algorithms that attain a near optimal dynamic regret of ˜O∗(T1/3C2/3
T∨1)(˜O∗hides dependencies
on dimensions and factors of logT). The work of Baby & Wang (2022b) shows similar rates for non-smooth
and exp-concave losses in a proper learning setting when the decision set is an L∞ball. In contrast, our work
is able to attain near optimal rates for arbitrary convex decision sets for a large family of exp-concave losses.
Further Baby & Wang (2022b) also shows optimal rates for arbitrary bounded convex decision sets when the
losses are strongly convex.
If we take all the comparators wtin Eq.(1)to be same, one recovers the notion of static regret. There are
works that aim in controlling the static regret in any time window which makes them suitable for learning in
non-stationary environments. These algorithms fall into the category of adaptive / strongly adaptive regret
minimization algorithms. Examples of such methods include Hazan & Seshadhri (2007); Daniely et al. (2015);
Adamskiy et al. (2016); Jun et al. (2017); Cutkosky (2020); Baby et al. (2021); Zhang et al. (2021). We refer
the readers to Baby & Wang (2021) and references therein for a more inclusive survey on dynamic regret and
strongly adaptive algorithms.
There has also been recent advances (e.g. Zhao et al., 2022; Baby & Wang, 2022a) in applying online learning
techniques to design controllers. In particular, Baby & Wang (2022a) uses a reduction from Linear Quadratic
Regulator (LQR) problem to online (mini-batch) linear regression problem due to the work of Foster &
Simchowitz (2020). They employ a black-box reduction technique to convert the algorithm of Baby & Wang
(2022b) to one that attains optimal dynamic regret for online linear regression under the setting of proper
learning. This is facilitated by exploiting the constant hessian property of linear regression losses. However,
this property will not be satisfied for exp-concave losses in general. As such it is unclear that the black-box
reduction techniques of Baby & Wang (2022a) are generalisable beyond linear regression. Hence the results
in this paper are not directly implied by their results. We direct the readers to Baby & Wang (2022b) for an
elaborate discussion about the literature on online learning and control.
1Here we reduce their loss bound to our notations.
4Published in Transactions on Machine Learning Research (10/2023)
2 Notations and Problem Setup
In this section, we specify necessary mathematical symbols and notations, and define functions for algorithm
design and regret analysis. We also present three examples to illustrate the concept of proper learning in
contextual pricing.
2.1 Symbols and Notations.
The pricing process consists of Trounds.xt,θ∗
t∈Rd,yt∈R,vt∈R+andNt∈Rdenote the feature vector,
the linear valuation parameter, the customer’s valuation, the seller’s price and the noise at time t, sequentially.
At each round, we receive a payoff (reward) rt=vt· 1t, where the binary variable 1tindicates the customer’s
decision, i.e., 1t=1(vt≤yt).
2.2 Technical Assumptions
Denote a norm-bounded domain family DB
p={θ∈Rd,∥θ∥p≤B}. We firstly present assumptions on domain
constraints of xtandθ∗
t:
Assumption 1 (Domain Constraints) .Assumext∈DxwhereDx⊆D1
2is convex and closed, and θ∗
t∈Dt
where everyDt⊆DB
2⊂DB
∞is also convex and closed. Each Dtcan be chosen adversarially and is known to
the learner before time t.
Here we want the customers’ valuations to be bounded. Equivalently, we may also assume that Dx⊆DB1
2
andDt⊆DB/B 1
2for anyB1>0. With these assumptions, we know that |x⊤
tθ|≤B,∀θ∈DB
2,t= 1,2,...,T.
Next, we make a reasonable assumption on customers’ expected valuations:
Assumption 2 (Non-Negative Expected Valuation) .For a customer’s valuation yt=x⊤
tθ∗
t, we assume the
expected valuation x⊤
tθ∗
t≥0,t= 1,2,...,T.
Now we make assumptions on the distribution of noise Nt. We firstly present the definitions of log-concavity
andstrict log-concavity on1-dimensional distributions according to Prékopa (1973).
Definition 2.1 (Log-concavity and strict log-concavity) .A probability measure Pdefined on Ris said to be
log-concave if and only if for any pair A,B⊂Rof intervals, it holds that
P(λA+ (1−λ)B)≥{P(A)}λ{P(B)}1−λ,∀λ∈(0,1).
Here " +" denotes Minkowski addition. Also, Pisstrictly log-concave if and only if for any pair A,B⊂Rof
intervals,A̸=B, it holds that
P(λA+ (1−λ)B)>{P(A)}λ{P(B)}1−λ,∀λ∈(0,1).
Then we make the following assumption:
Assumption 3 (Valuation noise distribution) .At each time t= 1,2,...,T, the noiseNtis independently
and identically sampled from a fixed strictly log-concave distribution Dwith a twice continuously differentiable
cumulative distribution function (CDF) F. Furthermore, the first and second derivatives of the CDF, denoted
asfandf′, respectively are bounded by two finite constants Bf:=supω∈Rf(ω)andBf′:=supω∈R|f′(ω)|.
According to Definition 2.1, let (i) A= (−∞,x],B= (−∞,y]and (ii)A= (x,+∞),B= (y,+∞)respectively,
and we have Fand(1−F)are both strictly log-concave functions. Existing works on contextual pricing
also adopt log-concavity assumptions (see Javanmard & Nazerzadeh, 2019). For a detailed discussion on
log-concave distributions, we kindly refer the audience to Bagnoli & Bergstrom (2006).
All of those three assumptions are supposed to hold throughout the paper.
2.3 Functions and Key Quantities
Greedily Pricing. Here we adopt two functions defined by Xu & Wang (2021) and also make use
of their properties. Firstly, we introduce an expected reward functiong(v,u) :=E[rt|vt=v,x⊤
tθ∗=
u] =v·(1−F(v−u))that is unimodal w.r.t. v. Secondly, we introduce a greedily pricing function
J(u) :=argmaxv∈Rg(v,u).J(u)has two important properties: On the one hand, J(u)is strictly monotonically
5Published in Transactions on Machine Learning Research (10/2023)
increasing, with J′(u)∈(0,1). Therefore, J(u)andJ−1(v)are bijections,∀u∈R,v> 0. On the other hand,
we have∥∇θJ(x⊤θ)∥2=|J′(x⊤θ)|·∥x∥2≤1, which guarantees a low price-changing rate while modifying
parameterθ.
Restrictions on Actions/Parametric Strategies. When we take an action by presenting a price vt, there
always exists an θt∈Rdsuch thatx⊤
tθt=J−1(vt). Therefore, for any price vt>0, it is equivalent to firstly
propose a corresponding parametric strategy θt(satisfyingx⊤
tθt=J−1(vt)) and then set the price as J(x⊤
tθt).
Since we are approaching the optimal price (which is J(x⊤
tθ∗)) and thatθ∗
t∈Dt, we may restrict the strategy
θtto be taken within Dtat each time t. We will explain more on the motivation of the restrictions in Section
2.4.
Negative Log-likelihood. We define
ℓt(θ) =− 1t·log/parenleftbig
1−F(vt−x⊤
tθ)/parenrightbig
−(1− 1t) log/parenleftbig
F(vt−x⊤
tθ)/parenrightbig
(2)
as a negative log-likelihood function at round t. Also, we define an expected log-likelihood function Lt:=
ENt[ℓt(θ)|xt,θ∗
t]. For the simplicity of notations in the following sections, we denote ht(θ) :=∂ℓt(θ)
∂x⊤
tθ∈R, and
we show a property of ht(θ):
Lemma 2.2. Forθ∈DB
2, there exist constants 0<h min≤hmax<+∞such that
hmax= sup
θ∈DB
2|ht(θ)|,hmin= inf
θ∈DB
2|ht(θ)|,∀t= 1,2,...,T.
We prove this by noticing that h(θ)is continuous and DB
2is closed, and the details are in Appendix B.1.
With this lemma, we may know that ℓt(θ)is Lipschitz (see Lemma 3.8).
Dynamic Regret. Finally, we define the cumulative dynamic regret :
RegT=T/summationdisplay
t=1g(J(x⊤
tθ∗
t),x⊤
tθ∗
t)−g(vt,x⊤
tθ∗
t). (3)
We usually measure the regret as a function of T,dand the total variation CT:=/summationtextT−1
t=1∥θ∗
t−θ∗
t+1∥1.
2.4 Examples
Here we present three examples where the nature requires the strategies to lie in a “safe domain”, regarding
risk-taking, legal or consistency concerns.
Risk Control Adopting strategies outside a pre-defined and protected decision set can be very risky in
general. Concerning our contextual pricing problem, an extremely low price would lead to significant loss of
profit. Therefore, we have to set a lower pricing bar for each item. At each time t, suppose the lower bar is
ct>0, and therefore our parametric strategy θtshould satisfy ct≤J(x⊤
tθt). SinceJ(u)is monotonically
increasing, we have x⊤
tθt≥J−1(ct). By intersecting {θ∈Rd|x⊤
tθ≥J−1(ct)}with theL2-norm ballDB
2, we
get a convex and compact set Dt, in which any parametric strategy θsatisfies the norm bound and will lead
to a price not less than ctgiven theJ(x⊤
tθ)greedy pricing policy.
Legal Concern There exist laws or regulations regarding the highest price of some specific products.
For each item with feature xt, suppose that we cannot set a price exceeding ct>0. Equivalently, the
parametric strategy θtwe take must satisfy vt=J(x⊤
tθt)≤ct. SinceJ(u)is monotonically increasing, this
is further equivalent to x⊤
tθt≤J−1(ct). Therefore, the restricted strategy space Dtis the intersection of
{θ|x⊤
tθ≤J−1(ct)}with theL2-norm ballDB
2, which is a convex and compact set. Any parametric strategy
falling out ofDtwould lead to either vt>ctor∥θ∥>B.
Price Consistency It is important for the seller to be consistent on setting prices, or otherwise it might
cause pricing discrimination. Specifically, if two identical items with feature xoccur at time tandt+ 1, then
their prices must be close to each other. In other words, we require |J(x⊤θt)−J(x⊤θt+1)|≤C,∀x∈Dx⊂D1
2
for some constant C > 0. For eachx∈Dx, we may solve it and get
6Published in Transactions on Machine Learning Research (10/2023)
Algorithm 1 Proper Dynamic Regret minimization (ProDR)
1:Input:Base algorithmA, barrier multiplier G′>0, exp-concavity factor β.
2:fort= 1,2,...,T:do
3:Get iterate ˜θtfromA.
4:Featurextand proper domain Dtare revealed
5:Output ˆθt= argminθ∈Dt|x⊤
t(θ−˜θt)|.
6:Lossℓtis revealed.
7:Construct ˆℓt(θ)as in Eq.(4) and set
ft(θ) =ˆℓt(θ) +G′·St(θ),
whereSt(θ) = minη∈Dt|∇ˆℓt(ˆθt)⊤(η−θ)|;
8:Sendft(θ)toAas loss at time t.
9:end for
J−1(J(x⊤θt)−C)≤x⊤θt+1≤J−1(C+J(x⊤θt)).
Denote this set as St(x), and we haveDt+1⊆∩x∈DxSt(x). Sinceθt∈St(x),∀x, the intersection is non-
empty.
3 Main Results
In this section, we present and analyse our algorithms. In Section 3.1, we first study the more general problem
of universal dynamic regret (Eq. (1)) minimization in a proper OCO setting. Results of Section 3.1 will be
applied in Section 3.2 to derive an optimal algorithm for the non-stationary pricing problem. All omitted
proofs in this section are deferred to Appendix B.
3.1 Dynamic Regret of ProDR
In this section, we study the ProperDynamicRegret minimization (ProDR) algorithm (Algorithm 1). We
consider the protocol of proper OCO with co-variates introduced in Section 1.
The goal of this section is to control the universal dynamic regret as defined in Eq. (1). We start by listing
out the assumptions we made for the OCO problem.
Assumption 4. A constant B > 0is known such that maxθ∈Dt∥θ∥∞≤Bfor allt∈[n].
Assumption 5. The losses ℓtobey∥∇ℓt(θ)∥2≤Gfor allt∈[n]andθ∈Dt(recall thatDt⊆DB
2from
Section 2.2).
Assumption 6. The losses are αexp-concave. i.e ℓt(y)≥ℓt(x) +∇ℓt(x)⊤(y−x) +α
2/parenleftbig
ℓt(x)⊤(y−x)/parenrightbig2, for
α>0and for allx,y∈DB
2.
Assumption 4 puts a relatively mild constraint that a box enclosing all the decision sets is known ahead of
time. Lipschitzness assumptions like Assumption 5 are standard in online learning. Assumption 6 states that
the lossℓtexhibits a strong curvature in the direction of its gradients (see Hazan et al., 2007, as an example).
We will exploit this curvature to derive fast regret rates.
Qualitative description of ProDR. The base algorithm Ain ProDR is expected to optimally control the
dynamic regret under exp-concave losses and when the decision set is a box: DB
∞={x∈Rd:∥x∥∞≤B},
whereBis as in Assumption 4. The idea is to perform a black-box reduction that can convert the base
algorithmAto an algorithm that attains good dynamic regret guarantee on the domains Dt. Though similar
ideas have been already explored in the work of Cutkosky & Orabona (2018), our way of constructing such
reductions for the current problem is new and interesting in its own right in the context of exp-concave online
learning. Next, we expand upon this matter highlighting the differences from Cutkosky & Orabona (2018).
We construct losses ftin Line 7 of ProDR where the St(θ)term acts as a regularizer that penalizes Afor
7Published in Transactions on Machine Learning Research (10/2023)
predicting points outside Dt. We would like the losses ftto be exp-concave as the base algorithm Aexpects.
However, a direct application of the techniques of Cutkosky & Orabona (2018) does not satisfy this property.
We address this issue by carefully constructing ftas in Line 7 of Algorithm 1 such that: 1) gradients of
both ˆℓt(θ)andSt(θ)lie in the span of co-variate xtand 2) ˆℓt(θ)is exp-concave, meaning that it exhibits
strong curvature along the direction of xt. Now, 1 and 2 together implies that the surrogate losses ftstill
remains exp-concave as it exhibits strong curvature along the direction of its gradient (which is spanned by
xt). The particular choice of ˆℓt(θ)is found to be crucial in preventing the exp-concavity factor of losses ft
from collapsing to zero. We will show that the dynamic regret of ProDR w.r.t. losses ˆℓtis upper bounded by
the dynamic regret of the base algorithm Awrt lossesftwhich is well controlled.
We next describe the dynamic regret guarantees of Algorithm 1. We inherit all the notations used in the
algorithm description.
Theorem 3.1. Letβ=min{α/2,1/(8GB√
d)}andγ=1
4/parenleftbig
2GB√
dβ+1/(2√
β)/parenrightbig2andG′= 1 + 2GBβ√
d. Let
Ain ProDR algorithm be FLH-ONS (Fig.1 in Appendix A) instantiated with parameters ζ= 2γ/25,G=GG′
andϕ=B. Then ProDR (Algorithm 1) satisfes
T/summationdisplay
t=1ℓt(ˆθt)−ℓt(wt) =˜O/parenleftbigg
(d3γ+d2
γ)(T1/3C2/3
T∨1)/parenrightbigg
,
whereCT:=/summationtextT
t=2∥wt−wt−1∥1withwt∈Dt.a∨b:=max{a,b}and ˜Ohides dependence of constants
G,B,αand poly-logarithmic factors of T.
Remark 3.2 (Adaptivity to CT).In light of the Ω(dB2logT∨d1/3T1/3C2/3
TB4/3)lower bound (see Baby &
Wang, 2021, Proposition 11), we see that the ProDR algorithm adapts optimally to the path variation CTof
the comparator sequence, which may not be known ahead of time.
Proof.Due to the αexp-concavity of losses ℓtover the domainDB
2andβ≤α
2we have that:
ℓt(θ)≥ℓt(ˆθt) +∇ℓt(ˆθt)⊤(θ−ˆθt) +β/parenleftig
∇ℓt(ˆθt))⊤(θ−ˆθt)/parenrightig2
,
for anyθ∈DB
2. Hence following Hazan et al. (2007), we consider the linear-regression-type surrogate losses:
ˆℓt(θ) :=/parenleftigg
∇ℓt(ˆθt)⊤(θ−ˆθt)/radicalbig
β+1
2√β/parenrightigg2
. (4)
Hence for any θ∈DB
2we have that
ℓt(ˆθt)−ℓt(θ)≤1
4β−ˆℓt(θ) =ˆℓt(ˆθt)−ˆℓt(θ). (5)
where we used the fact that ˆℓt(ˆθt) =1
4β.
Given that St(θ∗
t) =St(ˆθt) = 0sinceθ∗
t,ˆθt∈Dt, we have
ft(θ∗
t) =ˆℓt(θ∗
t),ft(ˆθt) =ˆℓt(ˆθt). (6)
8Published in Transactions on Machine Learning Research (10/2023)
Let us denote∇ℓt(θ) =ht(θ)xtwhereht(θ) =g′
t(x⊤
tθ). Now, according to the definition of St(θ)andˆθt, we
have:
ft(˜θt) =ˆℓt(˜θt) +G′·St(˜θt)
=ˆℓt(˜θt) +G′·min
η∈Dt|∇ℓt(ˆθt)⊤(η−˜θt)|
=ˆℓt(˜θt) +G′·min
η∈Dt|ht(ˆθt)||x⊤
t(η−˜θt)|
=ˆℓt(˜θt) +G′·|ht(ˆθt)||x⊤
t(ˆθt−˜θt)|
=ˆℓt(˜θt) +G′·|∇ℓt(ˆθt)⊤(ˆθt−˜θt)|.
Next we proceed to upper bound the regret w.r.t. losses ˆℓtby the regret w.r.t. losses ft. We need the
following lemma.
Lemma 3.3. Under the assumptions of Theorem 3.1, we have that
|ˆℓt(θ)−ˆℓt(ˆθt)|≤G′|∇ℓt(ˆθt)⊤(θ−ˆθt)|,
for anyθ∈DB
∞whereG′:= (1 + 2GBβ√
d).
The proof is shown in Appendix B.2. With this lemma, we have
ˆℓt(ˆθt)≤ˆℓt(˜θt) +G′·|∇ℓt(ˆθt)⊤(ˆθt−˜θt)|=ft(˜θt).
Combining the above inequality with Eq.(6) we obtain
ˆℓt(ˆθt)−ˆℓt(θ∗
t)≤ft(˜θt)−ft(θ∗
t).
Now using Eq.(5) along with the previous relation yields that
T/summationdisplay
t=1ℓt(ˆθt)−ℓt(θ∗
t)≤T/summationdisplay
t=1ft(˜θt)−ft(θ∗
t).
The following lemma specifies how to compute the sub-gradient of the regularizer term St(θ)in Line 7 of
Algorithm 1. Further it highlights an important property that a sub-gradient of St(θ)lies in the span of
covariatext(recall that∇ℓt(θ) =h(θ)xt). This is also useful for proving the joint exp-concavity of the losses
ft.
Lemma 3.4. The function St(θ)is convex across Rd. Denote ηt(θ) := argminη|x⊤
t(η−θ)|. When
∇ℓt(ˆθt)⊤(ηt(θ)−θ)̸= 0, we have:
∇St(θ) =/braceleftigg
∇ℓt(ˆθt),if∇ℓt(ˆθt)⊤(ηt(θ)−θ)<0
−∇ℓt(ˆθt),if∇ℓt(ˆθt)⊤(ηt(θ)−θ)>0.
When∇ℓt(ˆθt)⊤(ηt(θ)−θ) = 0, we have 0∈∂St(θ).
The proof of Lemma 3.4 is in Appendix B.3. In the next lemma, we show that the losses ftremain exp-concave
with appropriate exp-concavity factor bounded away from zero. This is the key lemma that helps to control
the regret of ProDR.
Lemma 3.5. Defineγ:=1
4/parenleftbig
2GB√
dβ+1/(2√
β)/parenrightbig2. We have that the surrogate losses ftare2γ/25exp-concave
and2GG′Lipschitz in L2norm acrossDB
∞.
9Published in Transactions on Machine Learning Research (10/2023)
Algorithm 2 Proper Dynamic Regret Pricing (PDRP)
1:Input:Noise distribution D(including its CDF Fand PDFf).
ProDR algorithm Ainstantiated as in Theorem 3.1.
2:fort= 1,2,...,T:do
3:Featurextand proper domain Dtare revealed and sent to A.
4:Getˆθt∈DtfromA.
5:Seller proposes vt=J(x⊤
tˆθt)and receive 1t.
6:Send lossℓt(θ)defined in Eq.(2) to A.
7:end for
As is stated earlier in this section, the intuition of this lemma comes from two facts: (1) both ∇ˆℓt(θ)and
∇St(θ)are in the span of xt, and (2) ˆℓt(θ)is exp-concave. As a result, the strong curvature of ˆℓt(θ)along
thextdirection “absorbs” the plain convexity of St(θ)and therefore guarantees the exp-concavity of ft(θ).
We defer the detailed proof to Appendix B.4. Hence from Baby & Wang (2022b, Theorem 10) , FLH-ONS
algorithm (Fig.1 in Appendix A) run with parameters ζ= 2γ/25,G=GG′andϕ=Bcan be used to control
T/summationdisplay
t=1ft(˜θt)−ft(θ∗
t) =˜O/parenleftigg
d2(G2(G′)2B2γd+G2(G′)2B2+1
γ)(T1/3C2/3
T∨1)/parenrightigg
=˜O/parenleftig
d3(T1/3C2/3
T∨1)/parenrightig
,
where the last line is got by plugging in the values of γandG′and upper bounding further.
3.2 Dynamic regret of PDRP
In this section, we present our main algorithm for controlling the dynamic regret on contextual pricing
problem, the ProperDynamicRegretPricing (PDRP) (Algorithm 2).
Qualitative description of PDRP. Xu & Wang (2021) observes that the pricing problem can be reduced
to the setting of proper OCO with co-variates and exp-concave losses. This observation, armed with the
ProDR algorithm, naturally lends itself to the algorithm PDRP for controlling dynamic regret of the pricing
problem.
We are now ready to present regret guarantees for the non-stationary pricing problem.
Theorem 3.6. Consider the linear noisy contextual pricing problem defined in Section 1. Assume that
we know the noise distribution Dexactly. By properly initializing β,γandG′with pre-knowledge, PDRP
(Algorithm 2) obeys RegT=˜O(d3(T1
3C2
3
T∨1)), where RegTis as defined in Eq. (3),˜Ohides poly-logarithmic
factors ofTand(a∨b) = max{a,b}.
Proof.We start with the lemmas that help us leverage the OCO framework of Section 3.1.
Lemma 3.7. (Xu & Wang, 2021, Lemma 5 and 6) Under the assumptions in Theorem 3.6, for θ∈DB
2, we
have:
g(J(x⊤
tθ∗
t),x⊤
tθ∗
t)−g(J(x⊤
tθ),x⊤
tθ∗
t)≤2C
Cdown(E[ℓt(θ)−ℓt(θ∗
t)]),
whereℓtis defined in Eq. (2),C= 2Bf+ (B+J(0))Bf′and
Cdown := inf
ω∈[−B,B+J(0)]min/braceleftbiggd2log(1−F(ω))
dω2,d2log(F(ω))
dω2/bracerightbigg
>0.
So we have
RegT≤2C
CdownE[ℓt(ˆθt)−ℓt(θ∗
t)]. (7)
10Published in Transactions on Machine Learning Research (10/2023)
Next, we record the curvature and smoothness properties of losses ℓt.
Lemma 3.8. LetG=hmaxdefined in Lemma 2.2. Under the assumptions in Theorem 3.6, for θ∈
Dt, we have: (1) ℓt(θ)isG-Lipschitz in∥·∥ 2norm, and (2) ℓt(θ)Cdown
Cexp-exp-concave. Here Cexp:=
supω∈[−B,B+J(0)]max/braceleftig
f(ω)2
F(ω)2,f(ω)2
(1−F(ω))2/bracerightig
andCdownis defined in Lemma 3.7 .
This lemma is derived from Xu & Wang (2021) Lemma 7, and we defer the proof to Appendix B.5. The
lemma above implies that the losses satisfy Assumption 5 in Section 3.1. Further they satisfy Assumption 6
with exp-concavity factor of Cdown/Cexp. So we can use the ProDR algorithm (Algorithm 1) to control the
dynamic regret wrt losses ℓt. Letβ=min{Cdown/(2Cexp),1/(8GB√
d)}andγ=1
4/parenleftbig
2GB√
dβ+1/(2√
β)/parenrightbig2and
G′= 1 +GB√
dCdown/Cexp. Hence continuing from Eq.(7), we apply Theorem 3.1 to obtain
RegT≤˜O/parenleftig
d3(T1/3C2/3
T∨1)/parenrightig
.
This completes the proof of the theorem.
Remark3.9.Although noise distributions are known as we assumed, the coefficient of our regret upper bound
depends highly on the distribution. As is indicated by Xu & Wang (2021), when the noise Ntis an i.i.d.
Gaussian noise with zero mean and σstandard deviation, this coefficient is exponentially large w.r.t.1
σasσ
approaches 0, which is counter-intuitive.
3.3 Lower Bound on Dynamic Pricing Regret
So far, we have developed a ProDR algorithm that is suitable for domain-constraint optimization of generalized
linear model, and have constructed a PDRP algorithm to solve the linear contextual pricing problem where
PDRP achieves a ˜O(d3(T1
3C2
3
T∨1))dynamic regret. This upper regret bound is optimal for online exp-concave
optimization as is shown by Baby & Wang (2021), but is it still optimal for our feature-based dynamic pricing
setting in specific? The answer is Yes. This dynamic regret is near-optimal up to dandlogTfactors, and
here we present the following theorem.
Theorem 3.10 (Lower dynamic regret bound) .Letd= 1in the contextual pricing problem we consider.
For any algorithm A, there exists a specific problem setting where Ahas to suffer an Ω(T1
3C2
3
T∨1)expected
dynamic regret.
With this theorem, we may claim that our PDRP algorithm is near-optimal. We here show a proof sketch
and defer the full proof to Appendix B.6.
Proof Sketch. The proof is developed in three steps: Firstly, we construct a hypothesis set Θin which there
areNdifferent{θ∗
t}T
t=1series whose total variations are upper bounded by CT. For any pair of two different
series{θ∗
t}T
t=1’s inΘ, they are identical for T/3out ofTrounds in total, and are different by some small δ
for the rest 2T/3rounds. Secondly, we show that their corresponding feedback distributions are also “similar”
to each other under the metric of KL-divergence. Therefore, according to Fano’s Inequality, any algorithm
can hardly distinguish among these distributions. Finally, we show that a failure of correctly distinguish the
underlying distribution (i.e., the real {θ∗
t}T
t=1series) will result in an Ω(T1
3C2
3
T∨1)regret.
4 Conclusion
In this work, we studied the non-stationary contextual pricing problem under safety constraints. We first
presented the ProDR algorithm for minimizing universal dynamic regret in the framework of proper OCO
with co-variates and exp-concave losses. This contribution could be of independent interest in the context of
non-stationary online learning. As a concrete application, we constructed our pricing algorithm, PDRP, by
making use of ProDR as the base learner. We showed that PDRP attains a ˜O(d3(T1
3C2
3
T∨1))dynamic regret
in our pricing problem setting. Finally, we proved that this rate is information-theoretically optimal (modulo
dependencies on dandlogT).
11Published in Transactions on Machine Learning Research (10/2023)
Acknowledgements
The authors are very grateful to the associate editor András György and three anonymous reviewers for
their constructive comments. This research is partially supported by NSF Award #2007117, the Adobe Data
Science Research Award and a start-up grant from the UCSB Department of Computer Science.
References
Dmitry Adamskiy, Wouter M Koolen, Alexey Chernov, and Vladimir Vovk. A closer look at adaptive regret.
The Journal of Machine Learning Research , 17(1):706–726, 2016.
Kareem Amin, Afshin Rostamizadeh, and Umar Syed. Repeated contextual auctions with strategic buyers.
InAdvances in Neural Information Processing Systems (NIPS-14) , pp. 622–630, 2014.
Dheeraj Baby and Yu-Xiang Wang. Optimal dynamic regret in exp-concave online learning. In Conference
on Learning Theory , pp. 359–409. PMLR, 2021.
Dheeraj Baby and Yu-Xiang Wang. Optimal dynamic regret in LQR control. In Advances in Neural
Information Processing Systems , 2022a.
Dheeraj Baby and Yu-Xiang Wang. Optimal dynamic regret in proper online learning with strongly convex
losses and beyond. In International Conference on Artificial Intelligence and Statistics , pp. 1805–1845.
PMLR, 2022b.
Dheeraj Baby, Xuandong Zhao, and Yu-Xiang Wang. An optimal reduction of tv-denoising to adaptive online
learning. In Proceedings of The 24th International Conference on Artificial Intelligence and Statistics ,
Proceedings of Machine Learning Research, pp. 2899–2907, 2021.
MarkBagnoliandTedBergstrom. Log-concaveprobabilityanditsapplications. In Rationality and Equilibrium:
A Symposium in Honor of Marcel K. Richter , pp. 217–241. Springer, 2006.
Omar Besbes and Assaf Zeevi. On the (surprising) sufficiency of linear models for dynamic pricing with
demand learning. Management Science , 61(4):723–739, 2015.
Nicolo Cesa-Bianchi and Gabor Lugosi. Prediction, Learning, and Games . Cambridge University Press, New
York, NY, USA, 2006. ISBN 0521841089.
Ningyuan Chen and Guillermo Gallego. A primal–dual learning algorithm for personalized dynamic pricing
with an inventory constraint. Mathematics of Operations Research , 47(4):2585–2613, 2022.
Xin Chen, Zexing Xu, Zishuo Zhao, and Yuan Zhou. Personalized pricing with group fairness constraint. In
Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency , pp. 1520–1530,
2023.
Yifang Chen, Chung-Wei Lee, Haipeng Luo, and Chen-Yu Wei. A new algorithm for non-stationary contextual
bandits: Efficient, optimal and parameter-free. In Conference on Learning Theory , pp. 696–726. PMLR,
2019.
Wang Chi Cheung, David Simchi-Levi, and Ruihao Zhu. Hedging the drift: Learning to optimize under
nonstationarity. Management Science , 68(3):1696–1713, 2022.
Maxime C Cohen, Ilan Lobel, and Renato Paes Leme. Feature-based dynamic pricing. Management Science ,
66(11):4921–4943, 2020.
Maxime C Cohen, Sentao Miao, and Yining Wang. Dynamic pricing with fairness constraints. Available at
SSRN 3930622 , 2021.
Maxime C Cohen, Adam N Elmachtoub, and Xiao Lei. Price discrimination with fairness constraints.
Management Science , 68(12):8536–8552, 2022.
12Published in Transactions on Machine Learning Research (10/2023)
Ashok Cutkosky. Parameter-free, dynamic, and strongly-adaptive online learning. In International Conference
on Machine Learning , pp. 2250–2259. PMLR, 2020.
Ashok Cutkosky and Francesco Orabona. Black-box reductions for parameter-free online learning in banach
spaces. In Thirty-First Annual Conference on Learning Theory (COLT-18) , pp. 1493–1529. PMLR, 2018.
Amit Daniely, Alon Gonen, and Shai Shalev-Shwartz. Strongly adaptive online learning. In International
Conference on Machine Learning , pp. 1405–1411, 2015.
Dylan Foster and Max Simchowitz. Logarithmic regret for adversarial online control. In International
Conference on Machine Learning , pp. 3211–3221. PMLR, 2020.
Elad Hazan and Comandur Seshadhri. Adaptive algorithms for online decision problems. In Electronic
colloquium on computational complexity (ECCC) , volume 14, 2007.
Elad Hazan, Amit Agarwal, and Satyen Kale. Logarithmic regret algorithms for online convex optimization.
Machine Learning , 69(2-3):169–192, 2007.
Adel Javanmard and Hamid Nazerzadeh. Dynamic pricing in high-dimensions. The Journal of Machine
Learning Research , 20(1):315–363, 2019.
Kwang-Sung Jun, Francesco Orabona, Stephen Wright, and Rebecca Willett. Improved strongly adaptive
online learning using coin betting. In Artificial Intelligence and Statistics , pp. 943–951. PMLR, 2017.
Robert Kleinberg and Tom Leighton. The value of knowing a demand curve: Bounds on regret for online
posted-price auctions. In IEEE Symposium on Foundations of Computer Science (FOCS-03) , pp. 594–605.
IEEE, 2003.
Renato Paes Leme, Balasubramanian Sivan, Yifeng Teng, and Pratik Worah. Learning to price against a
moving target. In International Conference on Machine Learning , pp. 6223–6232. PMLR, 2021.
Yiyun Luo, Will Wei Sun, et al. Distribution-free contextual dynamic pricing. arXiv preprint arXiv:2109.07340 ,
2021.
Chaoyue Niu, Zhenzhe Zheng, Fan Wu, Shaojie Tang, and Guihai Chen. Online pricing with reserve price
constraint for personal data markets. IEEE Transactions on Knowledge and Data Engineering , 34(4):
1928–1943, 2020.
András Prékopa. On logarithmic concave measures and functions. Acta Scientiarum Mathematicarum , 34:
335–343, 1973.
Shabnam Salehi and S Hamid Mirmohammadi. A solution approach for sponsored search advertising and
dynamic pricing for a perishable product and an online retailer with budget constraint. Computers &
Industrial Engineering , 177:109086, 2023.
Yining Wang, Boxiao Chen, and David Simchi-Levi. Multimodal dynamic pricing. Management Science , 67
(10):6136–6152, 2021.
Jianyu Xu and Yu-Xiang Wang. Logarithmic regret in feature-based dynamic pricing. Advances in Neural
Information Processing Systems , 34:13898–13910, 2021.
Jianyu Xu and Yu-Xiang Wang. Towards agnostic feature-based dynamic pricing: Linear policies vs linear
valuation with unknown noise. In International Conference on Artificial Intelligence and Statistics , pp.
9643–9662. PMLR, 2022.
Jianyu Xu, Dan Qiao, and Yu-Xiang Wang. Doubly fair dynamic pricing. In International Conference on
Artificial Intelligence and Statistics , pp. 9941–9975. PMLR, 2023.
Lijun Zhang, Shiyin Lu, and Zhi-Hua Zhou. Adaptive online learning in dynamic environments. In Proceedings
of the 32nd International Conference on Neural Information Processing Systems , pp. 1330–1340, 2018.
13Published in Transactions on Machine Learning Research (10/2023)
Lijun Zhang, Guanghui Wang, Wei-Wei Tu, Wei Jiang, and Zhi-Hua Zhou. Dual adaptivity: A universal
algorithm for minimizing the adaptive regret of convex functions. Advances in Neural Information Processing
Systems, 34:24968–24980, 2021.
Peng Zhao, Lijun Zhang, Yuan Jiang, and Zhi-Hua Zhou. A simple approach for non-stationary linear bandits.
InInternational Conference on Artificial Intelligence and Statistics , pp. 746–755. PMLR, 2020a.
Peng Zhao, Yu-Jie Zhang, Lijun Zhang, and Zhi-Hua Zhou. Dynamic regret of convex and smooth functions.
Advances in Neural Information Processing Systems , 33:12510–12520, 2020b.
Peng Zhao, Yu-Xiang Wang, and Zhi-Hua Zhou. Non-stationary online learning with memory and non-
stochastic control. In International Conference on Artificial Intelligence and Statistics , pp. 2101–2133.
PMLR, 2022.
Martin Zinkevich. Online convex programming and generalized infinitesimal gradient ascent. In Proceedings
of the 20th international conference on machine learning (icml-03) , pp. 928–936, 2003.
14Published in Transactions on Machine Learning Research (10/2023)
A Preliminaries
For the sake of completeness, we recall the description of the Follow-the-Leading-History (FLH) algorithm
(see Hazan & Seshadhri, 2007).
FLH: inputs - Learning rate ζ,G,ϕ>0andTONS base learners E1,...,ET
initialized with parameters G=G,D= 2ϕ√
d,α=ζand decision setD={θ∈
Rd:∥θ∥∞≤ϕ}. The learner Etstarts operating from time t.
1.For eacht,vt= (v(1)
t,...,v(t)
t)is a probability vector in Rt. Initialize
v(1)
1= 1.
2.In roundt, set∀j≤t,θj
t←Ej(t)(the prediction of the jthbase learner
at timet). Playθalg
t=/summationtextt
j=1v(j)
tθ(j)
t.
3. After receiving loss ft, set ˆv(t+1)
t+1= 0and perform update for 1≤i≤t:
ˆv(i)
t+1=v(i)
te−ζft(θ(i)
t)
/summationtextt
j=1v(j)
te−ζft(θ(j)
t)
4. Addition step - Set v(t+1)
t+1to1/(t+ 1)and fori̸=t+ 1:
v(i)
t+1= (1−(t+ 1)−1)ˆv(i)
t+1
Figure 1: FLH algorithm
Next, we describe the Online Newton Step (ONS) algorithm (see Hazan et al., 2007).
ONS: inputs - Exp-concavity factor αandG,D> 0. Decision setD.
1. At round 1, predict 0.
2. Letβ=1
2min{1
4GD,α}. At iteration t>1predict:
wt∈argmin
θ∈D∥wt−1−1
βA−1
t−1∇t−1−θ∥At−1,
where∇i=∇fi(wi), At=Id
β2D2+/summationtextt
i=1∇i∇⊤
i.
Figure 2: ONS algorithm
B Detailed Proof
B.1 Proof of Lemma 2.2
Proof.To begin with, we know that
ht(θ) =− 1t·f(ω)
1−F(ω)+ (1− 1t)·f(ω)
F(ω),
whereω=vt−x⊤
tθ. Since∃θt∈ Dtsuch thatvt=J(x⊤
tθt), given that J′(u)∈(0,1)(see Xu &
Wang, 2021, Eq.(19)), we know that ω∈[J(−B)−B,J(B) +B]is bounded in a closed interval. Since
we assume that f(ω)>0,∀ω∈R, we know that fmin=infω∈[J(−B)−B,J(B)+B]f(ω)>0andF(ω)∈
[F(J(−B)−B),F(J(B) +B)]⊂(0,1). Remember that we denote Bf:=supω∈Rf(ω)<+∞. As a result,
we have
0<fmin≤f(ω)
1−F(ω)≤Bf
1−F(J(B) +B)<+∞
0<fmin≤f(ω)
F(ω)≤Bf
F(J(−B)−B)<+∞.
15Published in Transactions on Machine Learning Research (10/2023)
Sinceht(θ) =f(ω)
1−F(ω)for 1t= 1orh(t) =f(ω)
F(ω)for 1t= 0, we know that |ht(θ)| ∈
[fmin,Bf
min{1−F(J(B)+B),F(J(−B)−B)}]. Lethmax=Bf
min{1−F(J(B)+B),F(J(−B)−B)}andhmin=fmin, and the
lemma is therefore proved.
B.2 Proof of Lemma 3.3
Proof.We have that for any θ∈DB
∞,
|ˆℓt(θ)−ˆℓt(ˆθt)|=/vextendsingle/vextendsingle/vextendsingle1//radicalbig
β+/radicalbig
β·∇ℓt(ˆθt)⊤(θ−ˆθt)/vextendsingle/vextendsingle/vextendsingle·/vextendsingle/vextendsingle/vextendsingle/radicalbig
β∇ℓt(ˆθt)⊤(θ−ˆθt)/vextendsingle/vextendsingle/vextendsingle
≤/parenleftig
1 + 2GBβ√
d/parenrightig
|∇ℓt(ˆθt)⊤(θ−ˆθt)|,
where in the last line we apply triangle inequality and the facts that |∇ℓt(ˆθt)⊤(θ−ˆθt)|≤G∥θ−ˆθt∥2with
∥θ−ˆθt∥2≤2B√
d.
PuttingG′= 1 + 2GBβ√
dcompletes the lemma.
B.3 Proof of Lemma 3.4
Proof.For the simplicity of notation, we denote ∇t:=∇ℓt(ˆθt), and we have: St(θ) =minx∈Dt|∇⊤
t(x−θ)|.
SinceSt(θ)is convex in Rd, we have:
St(θ2)≥St(θ1) +⟨∇St(θ1),θ2−θ1⟩,∀θ1,θ2∈DB
∞.
Now we conduct an orthogonal decomposition: ∇St(θ1) =µ1∇t+∇⊥
t, where∇⊤
t∇⊥
t= 0. Letθ3=θ2+µ2∇⊥
t,
and we have|∇⊤
t(x−θ2)|=|∇⊤
t(x−θ3)|,∀x∈Rd. In other words, we have St(θ2) =St(θ3)and therefore
we have:
St(θ2) =St(θ3)≥St(θ1) +⟨∇St(θ1),θ3−θ1⟩
=St(θ1) +⟨µ1∇t+∇⊥
t,θ2+µ2∇⊥
t−θ1⟩
=St(θ1) +⟨∇St(θ1),θ2−θ1⟩+µ2⟨∇⊥
t,∇⊥
t⟩,∀θ2∈Rd,µ2∈R
In other words, µ2∥∇⊥
t∥2
2≤St(θ2)−St(θ1)−⟨∇St(θ1),θ2−θ1⟩. Denoteη1=argminx∈Dt|∇⊤
t(x−θ1)|, and
η2= argminx∈Dt|∇⊤
t(x−θ2)|. Notice that
St(θ2)−St(θ1) =|∇⊤
t(η2−θ2)|−|∇⊤
t(η1−θ1)|
≤|∇⊤
t(η1−θ2)|−|∇⊤
t(η1−θ1)|
≤|∇⊤
t(θ1−θ2)|
≤∥∇t∥2·∥θ1−θ2∥2
≤G·∥θ1−θ2∥2.(8)
Here the first inequality comes from the definition of η2, the second inequality is an application of the
triangular inequality, the third inequality is derived from Cauchy-Schwarz Inequality, and the last inequality
is from Assumption A5 on the Lipschitzness of ℓt(θ)overDt. Therefore, St(θ)isG-Lipschitz as well over
DB
∞, and we have:
µ2∥∇⊥
t∥2
2≤St(θ2)−St(θ1)−⟨∇St(θ1),θ2−θ1⟩
≤2G∥θ2−θ1∥2.
This holds for any θ1,θ2∈DB
∞. However, we may fix θ1andθ2while also let µ2→+∞since it holds for any
µ2∈R. If∥∇⊥
t∥2̸= 0then it will fall into a contradiction. Therefore, we know that ∇⊥
t=0and∇St(θ)is
always in the same direction of ∇t.
16Published in Transactions on Machine Learning Research (10/2023)
Without losing generality, denote ∇St(θ1) :=λ·∇t. In the following, we will prove that λ=±1or0. From
Eq.(8)line 3, we know that St(θ2)−St(θ1)≤|∇⊤
t(θ1−θ2)|. Combined with the convexity of St(θ), we have:
|∇⊤
t(θ1−θ2)|≥St(θ2)−St(θ1)
≥∇St(θ1)⊤(θ2−θ1)
=λ·∇⊤
t(θ2−θ1).(9)
Notice that we can choose arbitrary θ2without changing λ, we may let θ2= 0andθ2= 2θ1in Eq. (9):
±λ·∇⊤
tθ1≤|∇⊤
tθ1|
If∇⊤
tθ1̸= 0, then we have λ∈[−1,1]. Otherwise, we know from Eq. (9)that|∇⊤
tθ2|≥λ·∇⊤
tθ2,∀θ2, and
similarly we have λ∈[−1,1]. Now we denote θ4:=θ1+η1
2, and we have:
⟨∇St(θ1),θ4−θ1⟩+St(θ1)≤St(θ4) (10)
from the convexity of St. And we also have:
St(θ4) = min
x∈Dt|∇⊤
t(x−θ4)|
≤|∇⊤
t(η1−θ4)|
=|∇⊤
tθ1−η1
2|
=1
2St(θ1)
=|∇⊤
t(θ1−θ4)|
=St(θ1)−|∇⊤(θ1−θ4)|.(11)
Combine Eq. (10) and (11), we have:
⟨∇St(θ1),θ4−θ1⟩≤St(θ4)−St(θ1) =−|∇⊤
t(θ1−θ4)| (12)
Plug in∇St(θ1) =λ∇tto Eq. (12), and we have:
λ·∇⊤
t(θ4−θ1)≤−|∇⊤
t(θ1−θ4)|. (13)
According to Eq. (13), if∇⊤
t(θ4−θ1)>0, then we have λ≤−1; if∇⊤
t(θ4−θ1)<0, then we have λ≥1.
Since we already know that λ∈[−1,1], then for the two case we should have λ=−1orλ= 1.
Finally, what if ∇⊤
t(θ4−θ1) = 0? In this case, it means that ∇⊤
t(η1−θ1)/2 = 0. Sinceη1=
argminx∈Dt|∇⊤
t(x−θ1)|, we know that St(θ1) = 0at this time. Since St(θ)≥0,∀θ∈Rd, we know
thatSt(θ)≥St(θ1) +0⊤(θ−θ1)and as a result 0∈∂St(θ1). This in fact holds the lemma.
B.4 Proof of Lemma 3.5
Proof.We begin by noticing that ˆℓt(θ)is exp-concave over DB
∞. To see this, note that by the triangular
inequality and Cauchy-Schwarz Inequality,
|∇ℓt(ˆθt)⊤(θ−ˆθt)/radicalbig
β+ 1/(2/radicalbig
β)|≤|∇ℓt(ˆθt)⊤(θ−ˆθt)|/radicalbig
β+ 1/(2/radicalbig
β)≤2GB/radicalbig
dβ+ 1/(2/radicalbig
β),
where we use the fact that ∥∇ℓt(ˆθt)∥2≤Gby Assumption A5 and ∥θ−ˆθt∥2≤2B√
dasθ∈DB
∞and
ˆθt∈Dt⊂DB
∞.
17Published in Transactions on Machine Learning Research (10/2023)
Withγas defined in the statement of the lemma, we have that the losses ˆℓt(θ)are2γexp-concave over DB
∞.
(see Cesa-Bianchi & Lugosi, 2006, Section 3.3).
Now we proceed to show that the losses ft(θ)are in-fact exp-concave with appropriate exp-concavity factor.
For the sake of brevity, let us denote
∇ˆℓt(u) = 2/radicalbig
β/parenleftbigg
∇ℓt(ˆθt)⊤(u−ˆθt)/radicalbig
β+1
2√β/parenrightbigg
∇ℓt(ˆθt)
:=p(u)∇ℓt(ˆθt).
We have that for any u,v∈DB
∞,
ˆℓt(v)≥ˆℓt(u) +p(u)∇ℓt(ˆθt)⊤(v−u)
+γ/parenleftig
p(u)∇ℓt(ˆθt)⊤(v−u)/parenrightig2
. (14)
Due to convexity, we have
St(v)≥St(u) +λ∇ℓt(ˆθt)⊤(v−u), (15)
for someλ∈{− 1,0,1}as per Lemma 3.4.
Adding Eq.(14) and (15), we obtain
ft(v)≥ft(u) +∇ft(u)⊤(u−v)
+γp(u)2/parenleftig
∇ℓt(ˆθt)⊤(v−u)/parenrightig2
=ft(u) +∇ft(u)⊤(u−v)
+γ/parenleftbiggp(u)
λ+p(u)/parenrightbigg2/parenleftbig
∇ft(u)⊤(v−u)/parenrightbig2. (16)
Next, we proceed to obtain a lower bound on the exp-concavity factor. Note that
p(u)≥2/radicalbig
β/parenleftbigg
−2GB/radicalbig
dβ+1
2√β/parenrightbigg
≥2/radicalbig
β·1
4√β=1
2
where the first inequality is via Cauchy-Schwarz Inequality and the second inequality holds due to the fact
thatβ≤1/(8GB√
d)due to the setting in Theorem 3.1
Similarly we have that
|p(u) +λ|≤4GBβ√
d+ 2≤5/2,
where in the last line we used β≤1/(8GB√
d).
Combining the last two displays, we have that
γ/parenleftbiggp(u)
λ+p(u)/parenrightbigg2
≥γ/25.
Applying this lower bound to Eq.(16) now yields the exp-concavity of ft(θ)claimed in the lemma.
Next, we proceed to calculate the Lipschitz constant of ft. Since∥∇ℓt(ˆθt)∥2≤G, by Lemma 3.4 we conclude
thatG′St(θ)isG′GLipschitz in L2 norm across Rd. Now using Lemma 3.3 we conclude that the losses ft
are2G′Gare Lipschitz in L2 norm across DB
∞.
18Published in Transactions on Machine Learning Research (10/2023)
B.5 Proof of Lemma 3.8
Xu & Wang (2021, Lemma 7) has proved theCdown
Cexp-exp-concavity. Here we prove the other claim on
Lipschitzness.
Proof.Notice that ℓt(θ)is a continuous function. Therefore, for any θ1,θ2∈Dt, there exists a θ3=
ϵθ1+ (1−ϵ)θ2for someϵ∈[0,1]such that
ℓt(θ1)−ℓt(θ2) =∇ℓt(θ3)⊤(θ1−θ2)
=ht(θ3)x⊤
t(θ1−θ2)
≤hmax∥xt∥2∥θ1−θ2∥2
=hmax∥θ1−θ2∥2
=G∥θ1−θ2∥2(17)
wherehmaxis defined in Appendix B.1. In Eq. (17), the first equality is by Lagrange interpolation, the second
equality is by definition of ht(θ), the third inequality is by Cauchy-Schwarz Inequality, the fourth equality is
by the assumption that xt∈D1
2, and the last inequality is from the fact that hmax=G. SinceDtis convex,
we know that θ3∈Dt. Therefore, the lemma is proven.
B.6 Lower Bound Proof (Proof of Theorem 3.10)
Here we present and prove the following theorem, which is sufficient to show a Ω(T1
3C2
3
T)lower bound for
CT>1√
T.
Theorem B.1. Consider a feature-based dynamic pricing problem with d= 1,xt= 1,Nt∼i.i.d.N(0,1),t=
1,2,...,TandCT>1√
TFor any algorithm Athere exists a specific setting such that AsufferΩ(T1
3C2
3
T)
expected regret even with ytobservable.
The sufficiency comes from the fact that we cannot observe any realized yt’s in the pricing problem (but a
binary indicator instead). In comparison, the lower bound in Theorem B.1 even works for observable yt’s,
which is sufficient to derive the binary feedback (by comparing ytwithvt).
Proof.To summarize the procedure of proof: Denote [n] :={1,2,...,n}for any positive integer n. Define
θ0= 1,θ1= 1 +δ(T,CT)whereδ=1
40(CT
T)1
3is an additional amount. Then we construct a set S⊂{0,1}T
consisting of randomly-sampled β(i)∈{0,1}T,i= 1,2,...,Nthat we will use to construct θ∗
t(i)series (each
iindicating a specific {θ∗
t}series) later. Afterward, we will show that the {θ∗
t(i)}and the{θ∗
t(j)}series are
hard to distinguish by any algorithm, and we will further show that a large enough regret caused by this
misspecification. In this way, we can prove an expected lower regret bound (where the expectation is also
taken over different {θ∗
t(i)}).
The process to sample each β(i)is as follows: We split [T]uniformly into m=CT
4δintervals, with each length
4Tδ
CT. Sinceδ=1
40(CT
T)1
3andCT≥1√
T, we know that m≥10. Denote these intervals as I1,I2,...,Im.
For anyβ(i)∈S, we construct it in a stochastic process: For each index interval Ik,k= 1,2,...,m, we
generate a random variable Z(i)
k∼Ber(1
2)independently, and then let β(i)
l=Z(i)
k,∀l∈Ik. Denote the
vectorZ(i)= [Z(i)
1,Z(i)
2,...,Z(i)
m]⊤∈{0,1}m, and we know that E[∥Z(i)−Z(j)∥1] =m
2. Accordingly, we
haveE[∥β(i)−β(j)∥1] =m
2·4Tδ
CT=T
2.
Therefore, according to Hoeffding’s inequality, we have:
Pr[|∥Z(i)−Z(j)∥1−m
2|≤m
6]≥1−2·e−(m
6)2
2m
⇔Pr[|∥β(i)−β(j)∥1−T
2|≤T
6]≥1−2·e−m
72,∀i,j∈[N].(18)
19Published in Transactions on Machine Learning Research (10/2023)
By applying a union bound over all/parenleftbigN
2/parenrightbig
pairs ofi,j∈[N], we have:
Pr[|∥β(i)−β(j)∥1−T
2|≤T
6,∀i,j∈[N]]≥1−N2·e−m
72.
Also, we know that Pr[β(i)̸=β(j)] =Pr[Z(i)̸=Z(j)] = 1−1
2mfori̸=j. By applying a union bound over all/parenleftbig(N
2)/parenrightbig
pairs ofi,j, we have Pr[β(i)̸=β(j)]≥1−N2
2m+1. Combining these two probability bounds, we know that
in this way we can find a satisfactory set Swith probability at least Pr≥1−N2·(e−m
72+ 2−(m+1)). Let
N=em
200(and therefore logN=m
200=CT
800δ), and then Pr≥1−N2·(e−m
72+ 2−(m+1))≥1−(e−m
300+e−3
5m).
Since the total number of possible S(i.e., any set consisting N(repeatable) vectors β∈{0,1}T) is(2m)N
and we are uniformly sampling from this whole family, the expected total number of satisfactory Sis at least
(2m)N·(1−(e−m
300+e−3
5m)). Sincem≥10as we showed above, we have (2m)N·(1−(e−m
300+e−3
5m))≥
210×1·(1−e−1
30−e−6) = 31.0325>1. As a result, there must exist at least one satisfactory Sin the whole
possible set family, such that: (1)T
3≤∥β(i)−β(j)∥1≤2T
3, and (2)β(i)̸=β(j),∀i̸=j∈[N]. We here pick
this satisfactory Sand in the following we use it for further proof.
Now, for each β(i)∈S, we generate a sequence of parameter {θ∗
t(i)}according to β(i): Fort= 1,2,...,T, we
letθ∗
t(i) = 1 +δ·β(i)
t, i.e., ifβ(i)= 0, thenθ∗
t(i) =θ0= 1; ifβ(i)= 1, thenθ∗
t(i) = 1 +δ. Therefore, we have
the following result:
TV({θ∗
t(i)})≤m·δ=CT
4<CT.
This is because∥θ∗
t(i)−θ∗
t+1(i)∥>0only if∃k∈[m]s.t.t∈Ik,t+ 1∈Ik+1. As a result, the total variation
of this{θ∗
t(i)}satisfies the upper bound CT.
Now, let us consider the realized valuation sequence {yt}. For anyi∈[N], denote
y(i) := [x1(1 +β(i)
1δ) +N1,x2(1 +β(i)
2δ) +N2,...,xT(1 +β(i)
T) +NT]⊤
Let us denote the distribution of y(i)asPi,i= 1,2,...,N. Recall that xt= 1andNt∼N(0,1),∀t, and we
havePi= [N(1 +β(i)
1δ,1),N(1 +β(i)
2δ,1),...,N(1 +β(i)
Tδ)]⊤. Consider the difference between PiandPj
while fixing β(i)andβ(i), and for any i,j∈[N],i̸=jwe have:
KL[Pi||Pj] =T/summationdisplay
t=1KL[N(1 +β(i)
tδ,1)||N(1 +β(j)δ,1)]
=T/summationdisplay
t=1(β(i)
t−β(j)
t)2δ2
2
=δ2
2·∥β(i)−β(j)∥2
2
=δ2
2·∥β(i)−β(j)∥1.(19)
Again, the KL-divergence is conditioning on β(i)andβ(j). Here the first line is from the fact that
yt’s are independent for every t, the second line is by xt= 1, the third line is from the fact that
KL[N(µ1,σ1)||N(µ2,σ2)] =logσ2
σ1+σ2
1+(µ1−µ2)2
2σ2
2−1
2, the fourth line is by calculation and the fifth line is
from that|β(i)
t−β(j)
t|∈{0,1}.
Here we introduce a Fano’s Inequality as the following proposition:
Proposition B.2 (Fano’s Inequality) .LetX1,X2,...,Xn∼i.i.d.Pwhere P∈ {P1,P2,...,PN}is a
distribution family. Let Ψbe any function of X1,X2,...,Xntaking values in {1,2,...,N}. Letα=
maxj̸=kKL(Pj||Pk).2Then
1
NN/summationdisplay
j=1Pj(Ψ̸=j)≥1−nα+ log 2
logN.
2Usually it is denoted as β, but here we denote it as αfor clarity, since we have already defined β(i)as vectors in S.
20Published in Transactions on Machine Learning Research (10/2023)
According to Fano’s Inequality, we have:
inf
Ψ:RT→{1,2,...,N}sup
i∈{1,2,...,N}Pi(Ψ̸=i)≥inf
Ψ1
NN/summationdisplay
i=1Pi(Ψ̸=i)≥1−nα+ log 2
logN≥1
2−nα
logN.(20)
Heren= 1since only one specific y(i)covers the whole time series and is only sampled once, and
α=maxi,j∈[N],i̸=jKL[y(i)||y(j)] = maxi,j∈[N],i̸=jδ2
2·∥β(i)−β(j)∥1≤δ2T
3is the upper bound of KL-
divergences on different distributions. Now we specify the function ΨAfor any pricing algorithm A: At each
roundt= 1,2,...,T, suppose the algorithm Aproposes a price vA
t. Define a vector w= [w1,w2,...,wT]⊤
wherewt= 1[vA
t≥J(θ0)+J(θ1)
2]is a Boolean value. Then we let ΨA=argmini∈[N]∥w−β(i)∥1. Therefore,
we have:
2·∥w−β(j)∥1≥∥β(ΨA)−w∥1+∥w−β(j)∥1
≥∥β(ΨA)−β(j)∥1,∀j∈[N],j̸=ΨA
≥T
6
Here the first inequality is from the optimality of ΨA, the second inequality is from the triangular inequality,
and the third inequality is from the Hoeffding bound in Eq. (18). Therefore we know that if ΨA̸=ithen we
have∥w−β(i)∥1≥T
12, which further leads to
T/summationdisplay
t=1(vA
t−J(xtθ∗
t(i)))2
≥T/summationdisplay
t=1( 1[wt= 1] 1[β(i)
t= 0] + 1[wt= 0] 1[β(i)
t= 1])(vA
t−J(xtθ∗
t(i)))2
=T/summationdisplay
t=11[vA
t≥J(θ0) +J(θ1)
2] 1[β(i)
t= 0](vA
t−J(θ0))2+ 1[vA
t<J(θ0) +J(θ1)
2] 1[β(i)
t= 1](J(θ1−vA
t))2
≥T/summationdisplay
t=11[|wt−β(i)
t|= 1](J(θ1)−J(θ0)
2)2
=∥w−β(i)∥1(J(θ1)−J(θ0)
2)2
≥T
12·(J(θ0)−J(θ1)
2)2.
Here the first line is because we omit the case where 1[wt=β(i)
t], the second line is from the definition of
wt, the third line is from the facts of θ0<θ1andJ′(u)>0,∀u∈R, the fourth line is by the definition of
L1-norm and the last line is from the fact we mentioned prior to this equation. Now we propose a lemma of
properties:
Lemma B.3 (Properties of g(v,u)andJ(u)).Forg(v,u)andJ(u)withNt∼N(0,1), we have:
1.J(u)>uwhenu∈(0,/radicalbigπ
2)andJ(u)<uwhenu∈(/radicalig
Π
2,+∞).
2.∃cJ>0s.t.J′(u)≥cJ,∀u∈[−B,B ].
3.∃cg>0s.t.g(J(u),u)−g(v,u)≥cg(J(u)−v)2,∀v∈[0,B+J(B)].
21Published in Transactions on Machine Learning Research (10/2023)
We will show the proof of Lemma B.3 by the end of this section. With Lemma B.3, when ΨA̸=i, we have:
RegA=T/summationdisplay
t=1g(J(xtθ∗
t(i)),xtθ∗
t(i))−g(vA
t,xtθ∗
t(i))
≥T/summationdisplay
t=1cg(vA
t−J(xtθ∗
t(i)))2
≥cg·T
12·(J(θ0)−J(θ1)
2)2
≥cg·T
12·c2
J
4·(θ1−θ0)2
≥cgc2
J·Tδ2
48.(21)
Finally, let δ=1
40(CT
T)1
3, and according to Eq. (19),(20) and (21), we have:
E[RegA]≥sup
i∈[N]Pi(ΨA̸=i)·(T/summationdisplay
t=1g(J(xtθ∗
t(i)),xtθ∗
t(i))−g(vA
t,xtθ∗
t(i)))
≥sup
i∈[N]Pi(ΨA̸=i)·cgc2
J·Tδ2
48
≥(1
2−nα
logN)·cgc2
J·Tδ2
48
=(1
2−δ2T
3
CT
800δ)·cgc2
J·Tδ2
48
=cgc2
J(1
2−800
3·δ3T
CT)··Tδ2
48
=cgc2
J
48(1
2−1
240)T·(CT
T)2
3
144
≥cgc2
J
307200·C2
3
TT1
3.
This holds the theorem.
Also, since our upper regret bound w.r.t. TandCTis˜O(1)whenCT≤1√
T, which is trivial up to logTand
dfactors, we may conclude that our upper regret bound of ˜O(T1
3C2
3
T∨1)is optimal with respect to Tand
CT.
Proof of Lemma B.3. We here prove each of them.
1.According to Xu & Wang (2021, Lemma 14), we know that u−J(u)is monotonically increasing
sinceJ′(u)∈(0,1). Also, since∂g(v,u)
∂v|v=J(u)= 1−F(J(u)−u)−J(u)·f(J(u)−u) = 0, we have
J(/radicalbigπ
2) =/radicalbigπ
2. Therefore, u−J(u)>0whenu>/radicalbigπ
2andu−J(u)<0when 0<u</radicalbigπ
2.
2.From Xu & Wang (2021, Appendix B.2.1.), we know that J′(u) = 1 +1
ϕ′(ϕ−1(u))∈(0,1),∀u∈Rwhere
ϕ(ω) =1−F(ω)
f(ω)−ωis invertible and smooth for standard Gaussian distribution. Therefore, we know
thatJ′(u)is continuous. Therefore, ∃cJ>0such that infu∈[−B,B]J′(u) =cJ.
22Published in Transactions on Machine Learning Research (10/2023)
3.From the optimality of J(u)we know that∂g(v,u)
∂v|v=J(u)= 1−F(J(u)−u)−J(u)·f(J(u)−u) = 0.
Defineq(u) := 1−F(J(u)−u)−J(u)·f(J(u)−u). Sinceq(u) = 0,∀u∈R, we have:
∂q(u)
∂u= 0
⇔/parenleftbig
J′(u)(J(u)2−u·J(u)−2)−(J(u)2−u·J(u)−1)/parenrightbig
f(J(u)−u) = 0
⇔J′(u) = 1 +1
J(u)2−u·J(u)−2.
The second line is by standard Gaussian noises and some calculations, and the third line is from the
fact thatf(x)>0for standard Gaussian distribution. Since we already know that J′(u)∈(0,1), we
may then realized that J(u)2−u·J(u)−2<−1. Notice that∂2g(v,u)
∂v2= (v2−vu−2)f(v−u)for
standard gaussian noise. Therefore, we have∂2g(v,u)
∂v2= (J(u)2−u·J(u)−2)f(J(u)−u)≤(−1)·fmin<0
wherefminhas been defined in Appendix B.1 as the universal lower bound of f. This means that
g(v,u)isfmin-strongly concave at v=J(u), which further leads to the fact that there exists a
neighborhood v∈[J(u)−Bu,J(u) +Bu]with constant3Busuch that∂2g(v,u)
∂v2≤−fmin
2. As a result,
forv∈[J(u)−Bu,J(u) +Bu]we have
g(J(u),u)−g(v,u) =−∂g(v,u)
∂v|v=J(u)(J(u)−u)−1
2·∂2g(v,u)
∂v2|v=v′∈[J(u),v]or[v,J(u)](J(u)−v)2
≥−1
2(−fmin
2)(J(u)−v)2
=fmin
4(J(u)−v)2.
Now, let us consider the case when v∈[0,B+J(B)]butv /∈[J(u)−Bu,J(u) +Bu]. On the one
hand, (J(u)−v)2≤(B+J(B)−(−B))2= (2B+J(B))2. On the other hand, g(J(u),u)−g(v,u)≥
g(J(u),u)−max{g(J(u)−Bu,u),g(J(u) +B(u),u)}>0. Denotecu:=infu∈[−B,B]{g(J(u),u)−
max{g(J(u)−Bu,u),g(J(u) +B(u),u)}}, and we have cu>0. Therefore, we have:
g(J(u),u)−g(v,u)≥cu≥cu
(2B+J(B))2(2B+J(B))2≥cu
(2B+J(B))2(J(u)−v)2.
Finally, let cg= min{fmin
4,cu
(2B+J(B))2}, and we have proved the lemma.
3Bucan be defined as the inferior of all Buover allu∈[−B,B ]and is still a positive constant.
23