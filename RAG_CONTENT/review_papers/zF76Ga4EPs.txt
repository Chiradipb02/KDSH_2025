Published in Transactions on Machine Learning Research (06/2024)
On the Unreasonable Eﬀectiveness of Federated Averaging
with Heterogeneous Data
Jianyu Wang∗jianyu.wang.thu@gmail.com
Carnegie Mellon University
Rudrajit Das rdas@utexas.edu
University of Texas at Austin
Gauri Joshi gaurij@andrew.cmu.edu
Carnegie Mellon University
Satyen Kale satyen@satyenkale.com
Google Research
Zheng Xu xuzheng@google.com
Google Research
Tong Zhang tongzhang@tongzhang-ml.org
University of Illinois Urbana-Champaign
Reviewed on OpenReview: https: // openreview. net/ forum? id= zF76Ga4EPs
Abstract
Existing theoretical results (such as (Woodworth et al., 2020a)) predict that the performance
of federated averaging (FedAvg) is exacerbated by high data heterogeneity. However, in
practice, FedAvg converges pretty well on several naturally heterogeneous datasets. In order
to explain this seemingly unreasonable eﬀectiveness of FedAvg that contradicts previous
theoretical predictions, this paper introduces the client consensus hypothesis : the average of
local models updates on clients starting from the optimum is close to zero. We prove that
under this hypothesis, data heterogeneity does not exacerbate the convergence of FedAvg.
Moreover, we show that this hypothesis holds for a linear regression problem and some
naturally heterogeneous datasets such as FEMNIST and StackOverﬂow. Therefore, we
believe that this hypothesis can better explain the performance of FedAvg in practice.
1 Introduction
Federated learning (FL) is an emerging distributed training paradigm (Kairouz et al., 2019; Wang et al.,
2021), which enables a large number of clients to collaboratively train a powerful machine learning model
without the need of uploading any raw training data. One of the most popular FL algorithms is Federated
Averaging ( FedAvg ), proposed by McMahan et al. (2017). In each round of FedAvg , a small subset of
clients are randomly selected to perform local model training. Then, the local model changes from clients
are aggregated at the server to update the global model. This general local-update framework only requires
infrequent communication between server and clients, and thus, is especially suitable for FL settings where
the communication cost is a major bottleneck.
Due to its simplicity and empirical eﬀectiveness, FedAvg has become the basis of almost all subsequent
FL optimization algorithms. Nonetheless, its convergence behavior – especially when the clients have
∗Currently at Apple.
1Published in Transactions on Machine Learning Research (06/2024)
heterogeneous data – has not been fully understood yet. Existing theoretical results such as Woodworth
et al. (2020a); Glasgow et al. (2021) predict that FedAvg ’s convergence is greatly aﬀected by the data
heterogeneity. In particular, when the local gradients on clients become more diﬀerent from each other
(i.e., more data heterogeneity), FedAvg may require much more communication rounds to converge. These
theoretical predictions match with the observations on some datasets with artiﬁcially partitioned or synthetic
non-IID data (Hsu et al., 2019; Li et al., 2020a; Zhao et al., 2018). However, on many real-world FL training
tasks, FedAvg actually performs extremely well (McMahan et al., 2017; Charles et al., 2021), which appears
to be unreasonable based on the existing theory. In fact, many advanced methods aimed at mitigating the
negative eﬀects of data heterogeneity performs similar to FedAvg in real-world FL training tasks. For
example, Scaffold (Karimireddy et al., 2020b) needs much fewer communication rounds to converge than
FedAvg in theory and when run on a synthetic dataset. However, Scaffold andFedAvg are reported to
have roughly identical empirical performance on many realistic federated datasets, see the results in (Reddi
et al., 2021; Wu et al., 2023; Li et al., 2021).
The above gap between threotical results and practical observations motivates us to think about whether
the current theoretical analyses are too pessimistic about FedAvg , since most of them only focus on the
worst cases. It remains as an open problem whether the data heterogeneity modeled by theory and simulated
via artiﬁcially constructed non-IID datasets matches the heterogeneity in real-world applications. Is it
possible that the data heterogeneity in practice has some special properties that allow FedAvg to enjoy
good convergence? The answers to the above questions will serve as important guidelines for the design and
evaluation of future federated algorithms.
Main Contributions. In this paper, we take the ﬁrst step towards explaining the practical eﬀectiveness
ofFedAvg under a special but realistic hypothesis. In particular, our contributions are as follows.
•By performing experiments on naturally heterogeneous federated datasets, we show that previous
theoretical predictions do not align well with practice. FedAvg can have nearly identical performance
on both IID and non-IID versions of these datasets. Thus, previous worst-case analyses may be
too pessimistic for such datasets. This is likely because the heterogeneity metric used by existing
analyses may be too loose for naturally heterogeneous datasets.
•In order to explain the practical eﬀectiveness of FedAvg , we propose a client consensus hypothesis :
the average of local model updates starting from the optimum is close to zero. For smooth and
strongly-convex functions, we prove that under this hypothesis, there is no negative impact on the
convergence of FedAvg even with unbounded gradient dissimilarity.
•We further validate that the client consensus hypothesis can hold in many scenarios. We ﬁrstly
consider a linear regression problem where all the clients have the same conditional probability of
label given data, and show that the client consensus hypothesis holds. Besides, we empirically show
that natural federated datatsets such as FEMNIST and StackOverﬂow satisfy this hypothesis. Indeed,
data heterogeneity has very limited impact on these datasets.
We would like to clarify that we are nottrying to provide any practical approach/criterion to predict which
datasets FedAvg will converge on. We are just diving deep into the notion of heterogeneity and trying to
provide some insights on which notion seems to be more aligned with the behavior of FedAvg in practice.
2 Preliminaries and Related Work
Problem Formulation. We consider a FL setting with total Mclients, where each client chas a local
objective function Fc(w)deﬁned on its local dataset Dc. The goal of FL training is to minimize a global
objective function, deﬁned as a weighted average over all clients:
F(w) =M/summationdisplay
c=1pcFc(w) =Ec[Fc(w)], (1)
2Published in Transactions on Machine Learning Research (06/2024)
wherepcis the relative weight for client c. For the ease of writing, in the rest of this paper, we will use
Ec[ac] =/summationtextM
c=1pcacto represent the weighted average over clients.
Update Rule of FedAvg. FedAvg (McMahan et al., 2017) is a popular algorithm to minimize (1) without
the need of uploading raw training data. In round tofFedAvg , clientcperformsHsteps of SGD from the
current global model w(t)to a local model w(t,H)
cwith a local learning rate η. Then, at the server, the local
model changes are aggregated to update the global model as follows:
w(t+1)=w(t)−αEc[w(t)−w(t,H)
c]. (2)
Hereαdenotes the server learning rate. Our results in this paper are for the full-device participation case,
i.e., when all the clients participate in each round. We discuss how our results can be extended to the
partial-device participation case at the end of Section 4.2.
Theoretical Analysis of FedAvg. When clients have homogeneous data, many works have provided error
upper bounds to guarantee the convergence of FedAvg (also called Local SGD) (Stich, 2019; Yu et al., 2019b;
Wang & Joshi, 2018; Zhou & Cong, 2018; Khaled et al., 2020; Li et al., 2019). In these papers, FedAvg was
treated as a method to reduce the communication cost in distributed training. It has been shown that in the
stochastic setting, using a proper H > 1inFedAvg will not negatively inﬂuence the dominant convergence
rate. Hence FedAvg can save communication rounds compared to the algorithm with H= 1. Later in
Woodworth et al. (2020b), the authors compared FedAvg with the mini-batch SGD baseline, and showed
that in certain regimes, FedAvg provably improves over mini-batch SGD. These upper bounds on FedAvg
was later proved by Glasgow et al. (2021) to be tight and not improvable for general convex functions.
When clients have heterogeneous data, in order to analyze the convergence of FedAvg , it is common to make
the following assumption to bound the diﬀerence among local gradients.
Assumption 1 (Bounded Gradient Dissimilarity) .There exists a positive constant ζsuch that∀w∈Rd,
the diﬀerence between local and global gradients are uniformly bounded:
Ec/bardbl∇Fc(w)−∇F(w)/bardbl2≤ζ2. (3)
This assumption ﬁrst appeared in decentralized optimization literature (Lian et al., 2017; Yuan et al., 2016;
Assran et al., 2018; Koloskova et al., 2020; Wang et al., 2022), and has been subsequently used in the analysis
ofFedAvg and related algorithms (Yu et al., 2019a; Khaled et al., 2020; Karimireddy et al., 2020b; Reddi
et al., 2020; Wang et al., 2020a;b; Haddadpour & Mahdavi, 2019; Karimireddy et al., 2020a; Das et al., 2022;
Zindari et al., 2023; Crawshaw et al., 2024). Under the bounded gradient dissimilarity assumption, FedAvg
cannot outperform the simple mini-batch SGD baseline unless ζis extremely small ( ζ <1/TwhereTis the
total communication rounds) (Woodworth et al., 2020a); the deterministic version of FedAvg (i.e., Local
GD) has even slower convergence rate than vanilla GD (Khaled et al., 2020). Again, these upper bounds
match a lower bound constructed in Glasgow et al. (2021) for general convex functions, suggesting that they
are tight in the worst case. In this paper, we do not aim to improve these bounds, which are already tight.
Instead, we argue that since the existing analyses only consider the worst case, they may be too pessimistic
for practical applications. The data heterogeneity induced by the bounded gradient dissimilarity assumption
may be diﬀerent from the real-world heterogeneity.
Finally, we note that there is another line of works, namely, Malinovskiy et al. (2020); Charles & Konečn` y
(2021); Charles & Rush (2022), using a diﬀerent analysis technique from the above literature. They showed
thatFedAvg (with full client participation) is equivalent to performing gradient descent on a surrogate loss
function. However, so far this technique still has some limitations. It can only be applied to deterministic
settings with quadratic (or a very special class of) loss functions. Additionally, Wang et al. (2023a) propose a
heterogeneity-driven Lipschitz assumption to better capture the eﬀect of local steps, and derive a convergence
result for FedAvg with this assumption. Gu et al. (2023) discuss the reasons why FedAvg can have better
generalization than mini-batch SGD.
Comparisons of Data Heterogeneity Assumptions. In Table 1, we summarize some commonly used
data heterogeneity assumptions in literature. It is worth highlighting that previous literature, no matter
3Published in Transactions on Machine Learning Research (06/2024)
which heterogeneity measures they used, suggested that only when all local functions are the same or share
the same optimum, there are no additional error terms (i.e., ζ= 0) caused by data heterogeneity. However,
in our paper, we show that even if local functions are heterogeneous and have diﬀerent local optima, data
heterogeneity can have no negative impact in some regimes (which likely happen in practice). This new result
helps us gain a deeper understanding of the great performance of FedAvg observed in practice and cannot be
obtained from previous works.
Among all these heterogeneity measures, gradient dissimilarity (at optimum) is considered as the most general
one and widely used in literature (Wang et al., 2021). But as we will discuss in the paper, it can be pessimistic
in practice. Besides, the gradient diversity assumption used in (Li et al., 2020a; Haddadpour & Mahdavi,
2019) implicitly forces all local functions share the same optimum. So it is much more restrictive than the
gradient dissimilarity.
Table 1: Summary of data heterogeneity measures/assumptions used in literature. The above table is adapted
from Table 6 in the survey (Kairouz et al., 2019). In the above works, their error upper bounds have the
same dependency on ζ, thoughζ’s deﬁnition is diﬀerent.
NameDeﬁnition Example usage
grad. dissimilarity Ec/bardbl∇Fc(w)−∇F(w)/bardbl≤ζ2Woodworth et al. (2020a)
grad. dissimilarity at opt. Ec/bardbl∇Fc(w∗)/bardbl2≤ζ2Khaled et al. (2020); Koloskova et al. (2020)
grad. diversity Ec/bardbl∇Fc(w)/bardbl2≤λ2/bardbl∇F(w)/bardbl2Li et al. (2020a); Haddadpour & Mahdavi (2019)
general grad. diversity Ec/bardbl∇Fc(w)/bardbl2≤λ2/bardbl∇F(w)/bardbl2+ζ2Wang et al. (2020a); Karimireddy et al. (2020b)
grad. norm/bardbl∇Fc(w)/bardbl2≤ζ2Yu et al. (2019b); Li et al. (2020b)
opt. diﬀ/bardblEcw∗
c−w∗/bardbl2≤ζ2Wang et al. (2023b)
3 Mismatch Between Theory and Practice on FedAvg
In this section, we will introduce the existing convergence analysis in detail and compare the theory with
experimental results. We shall show that there is a gap between the theory and practice of FedAvg .
3.1 Existing Theoretical Analysis of FedAvg
Besides the bounded gradient dissimilarity assumption (3), the analysis of FedAvg relies on the following
common assumptions.
Assumption 2 (Lipschitz Smoothness ).There exists a constant Lsuch that,∀w,u∈Rd,∀c,
/bardbl∇Fc(w)−∇Fc(u)/bardbl2≤L/bardblw−u/bardbl2(4)
Assumption 3 (Local Unbiased Gradient ).Local stochastic gradient on each client cis unbiased with
expectation∇Fc(w)and bounded variance σ2.
Using the above assumptions, previous works derived an upper bound for the optimization error with
non-convex objective functions. We take the following theorem from (Jhunjhunwala et al., 2022) as an
example.
Theorem 1. Under Assumptions 1 to 3, if FedAvg’s learning rates satisfy η≤1/8LH,α≤1/24LH, then
the global gradient norm mintE/bardbl∇F(w(t))/bardbl2can be upper bounded by
O/parenleftbiggF(w(0))−F∗
αηHT/parenrightbigg
+O/parenleftbiggαηLσ2
M+η2L2Hσ2/parenrightbigg
+O/parenleftbig
η2L2H2ζ2/parenrightbig
. (5)
Theorem 1 shows that when the learning rates α,ηareﬁxedand the communication round Tis limited, data
heterogeneity always introduces an addition term O(ζ2)to the optimization error bound. It is worth noting
that, with an optimized learning rate, the above upper bound matches a lower bound constructed in Glasgow
et al. (2021) for general convex functions, suggesting that it is tight. In the worst case, the convergence of
4Published in Transactions on Machine Learning Research (06/2024)
FedAvg will become worse compared to the homogeneous setting. Although some papers argued that the
data heterogeneity only inﬂuences the higher order terms when using optimized learning rates (Yu et al.,
2019a; Khaled et al., 2020), this conclusion only holds asymptotically w.r.t. T. Also, in some special cases,
FedAvg ’s convergence rate with data heterogeneity can be substantially slower. For instance, when there is
no stochastic noise σ= 0,FedAvg in homogeneous setting can achieve a rate of T−1but with heterogeneous
data, the rate degrades to T−2/3(Woodworth et al., 2020a; Wang et al., 2021; Jhunjhunwala et al., 2022).
3.2 Empirical Observations on FedAvg
Results on Artiﬁcial Non-IID Datasets. In order to corroborate the above theoretical results, most of
previous works constructed datasets with artiﬁcially high data heterogeneity. Given a common benchmark
classiﬁcation dataset (such as MNIST, CIFAR-10/100), researchers simulated the data heterogeneity by
assigning diﬀerent class distributions to clients. For example, each client may only hold one or very few
classes of data (Zhao et al., 2018), or has data for all classes but the amount of each class is randomly drawn
from a Dirichlet distribution (Hsu et al., 2019). On these datasets, the empirical convergence of FedAvg is
greatly impacted and its ﬁnal accuracy is much lower than the one in the centralized training setting. As
shown in Zhao et al. (2018), FedAvg ’s ﬁnal test accuracy on non-IID versions of CIFAR-10 can be 10−40%
lower than its on the standard IID version.
Results on Natural Non-IID Datasets. While the negative results on artiﬁcial non-IID datasets are
widelycitedtoclaim FedAvg suﬀersduetodataheterogeneity, wedoubtwhethertheresultsarerepresentative
and general enough to cover all practical applications. Is it possible that there are some scenarios where
the data heterogeneity has benign eﬀects and has diﬀerent characteristics from the heterogeneity simulated
through these originally IID datasets?
We conduct some experiments on StackOverﬂow, a naturally non-IID split dataset for next-word prediction.
Each client in the dataset corresponds to a unique user on the Stack Overﬂow site. The data of each client
consists of questions and answers written by the corresponding user. More details about the experimental
setup, such as optimizer and learning rate choices, can be found in the Appendix. From the naturally
heterogeneous StackOverﬂow dataset, we create its IID version by aggregating and shuﬄing the data from all
clients, and then re-assigning the IID data back to clients. Surprisingly, the results in Figure 1 show that the
convergence of FedAvg is nearly identical (with limited communication rounds) on the new IID dataset and
its original non-IID version. This observation contradicts the conventional wisdom that data heterogeneity
always adds an additional error to the optimization procedure, as well as the empirical results on artiﬁcial
non-IID datasets. There is indeed a gap between the theory and practice of FedAvg .
0200 400 600 800 1000 1200 1400 1600
Round0.000.050.100.150.200.25AccuracyStackOverflow
Non-IID
IID
Figure 1:Results on the naturally non-IID dataset StackOverﬂow. The IID version of StackOverﬂow
is created via aggregating and shuﬄing all the data from clients. We observe that FedAvg achieves roughly
the same convergence in both IID and non-IID settings.
Experimental results from some previous papers serve as additional evidence to support our claim. For
instance, FedAvg converges signiﬁcantly faster than FedSGD on Shakespeare and StackOverﬂow datasets
5Published in Transactions on Machine Learning Research (06/2024)
Local updatesOptimumLocal minimumLocal updatesOptimumLocal minimum
ClientdriftClientconsensus
Figure 2: Comparison between client drift and client consensus. (Left) Previous works focus on the worst
case where the average of clients’ local updates (or drifts) drives the global model away from the optimum;
(Right) Our client consensus hypothesis states that the average local drifts at the optimum is very small or
close to zero. This structured heterogeneity does not impact the convergence of FedAvg .
even with strong heterogeneity (McMahan et al., 2017; Charles et al., 2021). Even though a more sophisticated
method like Scaffold does better on artiﬁcial datasets like EMNIST, it does not outperform FedAvg on
many naturally heterogeneous datasets like FEMNIST, StackOverﬂow, etc., as shown in (Reddi et al., 2021;
Wu et al., 2023; Li et al., 2021). Explanations to these observations still remain mysteries.
4 Client Consensus Hypothesis
In previous sections, we saw that there is a gap between the theory and practice of FedAvg . While theory
predicts that FedAvg performs worse in the presence of data heterogeneity, we did not observe this on
naturally non-IID datasets. Also, many advanced methods designed to tackle data heterogeneity problem do
not outperform FedAvg . In order to explain the above phenomenon, in this section, we propose the client
consensus hypothesis. Many realistic federated datasets may satisfy this special property such that data
heterogeneity has very limited impacts on the convergence of FedAvg .
4.1 Key Insights
FedAvg is commonly viewed as a perturbed version of vanilla SGD. To see this, we can consider the
accumulated local updates on each client as a pseudo-gradient, which is an approximation of the batch client
gradient. When clients have heterogeneous data, the average of the pseudo-gradients across all clients can be
very diﬀerent from the original global gradient. Especially, when the global model approaches the optimum
(or a stationary point) with a constant learning rate, since the average of pseudo-gradients is not zero, the
global model cannot stay and may drift away towards a diﬀerent point. This is referred to as client drift
problem in literature (Karimireddy et al., 2020b). An illustration is provided in Figure 2.
The above insight is true in the worst case. But it is possible that real-world FL datasets are far away from
this worst case. In this paper, we make a hypothesis about a special class of data heterogeneity. In particular,
clients can still have drastically diﬀerent data distributions and local minima. But, at the global optimum (or
stationary point), clients’ local updates (i.e., pseudo-gradients) cancel out with each other and hence, the
average pseudo-gradient at the optimum is or close to zero. As a consequence, when the global model reach
the global optimum, it can stay there and will not drift away. That is, clients reach some kind of consensus
at the optimum. For this reason, we name the hypothesis as client consensus hypothesis . We will show it in
Section 4.2 that this special class of non-IIDness has no negative impacts on the convergence of FedAvg .
Therefore, the hypothesis can serve as a possible explanation of the eﬀectiveness of FedAvg .
More formally, we deﬁne the deterministic pseudo-gradient for client cas follows:
Gc(w),1
ηH(w−w(H)
c,GD) (6)
6Published in Transactions on Machine Learning Research (06/2024)
where w(H)
c,GDdenotes the locally trained model at client cafter performing Hsteps of GD from wusing a
local learning rate η. Given this deﬁnition, the client consensus hypothesis is formally stated below.
Assumption 4 (Client Consensus Hypothesis ).On real-world federated datasets, for the values of η,H
used in FedAvg, the average pseudo-gradient at the optimum (i.e., average client model drift at the optimum)
ρ,/vextenddouble/vextenddouble/vextenddoubleEc[Gc(w∗)]/vextenddouble/vextenddouble/vextenddouble
is very small or close to zero, where w∗is the global optimum or stationary point of the global objective (1).
Note that the Client Consensus Hypothesis is satisﬁed whenever FedAvg converges with a constant learning
rate. Thus, in a sense, it is the minimal assumption under which one can expect to prove improved convergence
rates for FedAvg .ρcan be interpreted as the average drift at optimum ; this quantity is akin to a heterogeneity
metricand our theoretical results will be in terms of it.
Connections to Existing Analysis with Bounded Gradient Dissimilarity. Here, we discuss the
connections between client consensus hypothesis and previous convergence analysis with bounded gradient
dissimilarity assumption. As we mentioned before, FedAvg can be viewed as a perturbed version of vanilla
SGD. This alternative view is also critical in the convergence analysis. A key step in previous FedAvg
analysis is to bound the perturbations, i.e., the diﬀerence between the average pseudo-gradient Ec[Gc]and
the batch global gradient ∇F(w) =Ec[∇Fc(w)]. Previous works always upper bound this diﬀerence using
Jensen’s inequality as follows:
/bardblEc[Gc(w)−∇Fc(w)]/bardbl2≤Ec/bardblGc(w)−∇Fc(w)/bardbl2. (7)
Then, the right-hand-side (RHS) can be further bounded using the gradient dissimilarity assumption. However,
we note that the above upper bound omits the correlations among diﬀerent clients. While the RHS (i.e.,
average of squared /lscript2diﬀerences) can be large or unbounded, the LHS (i.e., squared /lscript2norm of the average
diﬀerence) can be small or just zero, especially at the optimum w∗. Therefore, using (7) in the analysis may
result in a pessimistic estimate.
Instead, in this paper, the client consensus hypothesis states that the LHS of (7) at the optimum is close to,
if not, zero. Using the hypothesis and our new analysis, we no longer need to provide an upper bound of the
RHS of (7) for all points. As a consequence, the new analysis does not require the gradient dissimilarity to
be bounded.
4.2 Convergence of FedAvg under Client Consensus Hypothesis
In this subsection, we will provide a convergence analysis for FedAvg under the client consensus hypothesis
(Assumption 4) for strongly-convex loss functions. In particular, we show that when ρ= 0in Assumption 4,
data heterogeneity has no negative impact on the convergence of FedAvg .
Besides the pseudo-gradient deﬁned in (6), we need to deﬁne its stochastic version:
ˆGc(w),1
ηH(w−w(H)
c) (8)
where w(H)
cdenotes the locally trained model at client cusingHsteps of SGD with learning rate η, starting
fromw. Let us deﬁneG:=Ec[Gc]and ˆG:=Ec[ˆGc]. Then, we have the following result.
Theorem 2. Under Assumptions 2 to 4, when each local objective function is µ-strongly convex and the
learning rates satisfy α≤1/4,η≤min{1/L,1/µH}, afterTrounds of FedAvg, we have
E/vextenddouble/vextenddouble/vextenddoublew(T)−w∗/vextenddouble/vextenddouble/vextenddouble2
≤(1−1
2αηHµ )T/vextenddouble/vextenddouble/vextenddoublew(0)−w∗/vextenddouble/vextenddouble/vextenddouble2
+2αηH
µmax
wVar[ˆG(w)]
+20
µ2max
wEc/bardblδc(w)/bardbl2+20ρ2
µ2(9)
where E[·],Var[·]are taken with respect to random noise in stochastic local updates, and δc(w) = (w(H)
c,GD−
E[w(H)
c])/(ηH)denotes the iterate bias between local GD and local SGD on client c.
7Published in Transactions on Machine Learning Research (06/2024)
From Theorem 2, we observe that the stochastic noise during local updates inﬂuences the second and the
third terms on the right-hand-side of (9). The upper bounds of these two terms only depend on the dynamics
of SGD, which has been well understood in literature. Speciﬁcally, in Khaled et al. (2020), the authors show
thatE/bardblw(H)−E[w(H)]/bardbl2≤2η2Hσ2. As a consequence, we directly obtain that
Var[ˆG(w)] =E/vextenddouble/vextenddouble/vextenddoublew(H)
c−E[w(H)
c]/vextenddouble/vextenddouble/vextenddouble2
η2H2M≤2σ2
MH. (10)
As for the iterate bias, one can obtain
Ec/bardblδc(w)/bardbl2≤η2L2σ2(H−1), (11)
the proof of which is provided in the Appendix. After substituting (10) and (11) into (9) and optimizing the
learning rates, we can obtain the following convergence rate for FedAvg .
Corollary 1 (Convergence Rate for Strongly Convex Functions ).In the same setting as Theorem 2,
whenα= 1/4,η=O(1/µHT ), the convergence rate of FedAvg is1
E/vextenddouble/vextenddouble/vextenddoublew(T)−w∗/vextenddouble/vextenddouble/vextenddouble2
=/tildewideO/parenleftbiggσ2
MHT+σ2
HT2+ρ2/parenrightbigg
. (12)
If clients perform local GD instead of local SGD, then when η= min{1/L,1/(µH)}, we have
/vextenddouble/vextenddouble/vextenddoublew(T)−w∗/vextenddouble/vextenddouble/vextenddouble2
=O/parenleftbigg
exp/parenleftbigg
−T
16κmin{κ,H}/parenrightbigg
+ρ2/parenrightbigg
(13)
whereκ=L/µdenotes the condition number.
Eﬀects of Data Heterogeneity. In the special regime of ρ≈0, i.e., when the client consensus hypothesis
holds, Theorem 2 and Corollary 1 state that data heterogeneity does not have any negative impact on the
convergence of FedAvg . However, in previous works based on gradient dissimilarity, even if ρ= 0, there is
an additional error caused by the positive gradient dissimilarity. Compared to the centralized setting where
M= 1, (12) suggests that FedAvg can provide linear speedup due to the usage of multiple workers.
Extensions. The above analysis can be extended to various settings. Below we provide several examples.
(1)Client Sampling : If we consider client sampling in FedAvg , then only the variance term in (9) will
change while the other terms will be unaﬀected. One can obtain new convergence guarantees by analyzing
the variance of diﬀerent sampling schemes and then simply substituting them into (9). Standard techniques
to analyze client sampling (Yang et al., 2020) can be directly applied.
(2)Third-order Smoothness : When the local objective functions satisfy third-order smoothness
(/vextenddouble/vextenddouble∇2Fc(w)−∇2Fc(u)/vextenddouble/vextenddouble≤Q/bardblw−u/bardbl), the bound of the iterate bias δ(w)can be further improved while all
other terms remain unchanged. According to Glasgow et al. (2021), one can obtain /bardblδ(w)/bardbl≤1
4η2HQσ2. In
the special case when local objective functions are quadratic, we have Q= 0. That is, there is no iterate bias.
As a consequence, the convergence rate of FedAvg can be signiﬁcantly improved.
Moreover, a result for the general convex case is provided in the Appendix. Analysis for non-convex functions
requires a diﬀerent framework and is non-trivial. So we leave it for future work.
Comparison with Previous Works. In Table 2, we summarize the convergence rates of FedAvg in
diﬀerent papers. All previous results depend on the gradient dissimilarity upper bound ζ, which is generally
large for heterogeneous data settings. However, in our result, under client consensus hypothesis, we show
that the eﬀects of data heterogeneity can be measured by average drift at optimum ρ, which can be close to
zero even in presence of strong data heterogeneity, as we showed in the quadratic example and experiments
on FEMNIST and StackOverﬂow. When the average drift at optimum is zero, we can get an improved
convergence rate compared to existing results.
1/tildewideO(.)hides log factors.
8Published in Transactions on Machine Learning Research (06/2024)
Table 2: Comparison with existing results for strongly convex objectives functions with deterministic local
updates. In the table, the error is measured by the distance to the optimum /bardblw−w∗/bardbl2, andκ=L/µis the
condition number. Also, we omit logarithmic factors. Compared to previous results, we show that in the
considered setting: (i) FedAvg enjoys linear convergence to the global optimum, and (ii) the multiple local
steps of FedAvg mitigate the impact of of ill-conditioning (high condition number).
Algorithm Worst-case error Comm. rounds to attain /epsilon1error when ρ= 0
GD exp(−T/κ) O(κlog(1//epsilon1))
FedAvg (Koloskova et al., 2020) ζ2/T2O(1//epsilon12)
FedAvg (Woodworth et al., 2020a) 1/(HT+H2T2) +ζ2/T2O(1//epsilon12)
FedAvg (Ours) exp(−Tmin{1,H/κ}) +ρ2O(max{1,κ/H}log(1//epsilon1))
5 Validating the Client Consensus Hypothesis
Here we will present some evidence to show that the client consensus hypothesis is realistic and practical; we
do so (a) theoretically, in a linear regression setting, and (b) empirically, on some naturally split datasets
such as FEMNIST and StackOverﬂow.
5.1 Linear Regression Example
IntuitiononWhenClientConsensusHypothesisHolds. One of the key insights of the client consensus
hypothesis is that clients do reach some consensus and a single global model can work reasonably well for
all clients though they have heterogeneous data. Inspired by this, we assume that all clients have the same
conditional probability of the label given data, i.e., pc(y|x) =p(y|x),∀c, where x,ydenote the input data
and its label, respectively. In this case, clients still have heterogeneous data distributions, as they may have
drastically diﬀerent pc(x)andpc(x,y). However, the client’s updates should not conﬂict with each other, as
the learning algorithms tend to learn the same p(y|x)on all clients.
Let us study a concrete linear regression setting satisfying the above property. Speciﬁcally, we assume that
the label of the ithdata sample on client cis generated as follows:
yc,i=/angbracketleftw∗,xc,i/angbracketright+/epsilon1c,i, (14)
where w∗∈Rddenotes the optimal model, and /epsilon1c,i∼P/epsilon1is a zero-mean random noise and independent from
xc,i(this is a common assumption in statistical learning). We also assume that all /bardblxc,i/bardblhave bounded
variance. Note that both w∗andP/epsilon1are the same across all the clients, i.e., all clients have the same label
generation process and hence, the same conditional probability p(y|x). Our goal is to ﬁnd the optimal
model w∗given a large amount of clients with ﬁnitedata samples (which is a common cross-device FL
setting (Kairouz et al., 2019)). We use the squared loss function which makes our objective function quadratic ;
speciﬁcally, it is:
Fc(w) =1
nn/summationdisplay
i=11
2/parenleftbig
yc,i−w/latticetopxc,i/parenrightbig2
=1
2(w−w∗)Ac(w−w∗)−b/latticetop
c(w−w∗) +const., (15)
where Ac=/summationtextn
i=1xc,ix/latticetop
c,i/n,bc=/summationtextn
i=1/epsilon1c,ixc,i/n. The minimizer of local objective Fc(w)isw∗
c=
w∗+A−1
cbc, which is diﬀerent from the global minimizer w∗asbc/negationslash= 0.
Client Consensus Hypothesis. In this problem, we can show that client consensus hypothesis holds. In
particular, one can show that the average pseudo-gradient at the optimum ρis
Ec[Gc(w∗)] =Ec/bracketleftBigg
1
HH−1/summationdisplay
h=0/bracketleftbig
I−(I−η∇2Ac)h/bracketrightbig
bc/bracketrightBigg
. (16)
9Published in Transactions on Machine Learning Research (06/2024)
Due to the independence of /epsilon1c,iandxc,i, for any choice of H,ρ=/bardblEc[Gc(w∗)]/bardbl→ 0almost surely when the
number of clients M→∞.
Unbounded Gradient Dissimilarity. In contrast, if we check the gradient dissimilarity, we have:
Ec/bardbl∇Fc(w∗)−∇F(w∗)/bardbl2=Ec/bardblbc/bardbl2. (17)
Observe that /epsilon1can have extremely large variance such that the gradient dissimilarity bound ζis arbitrarily
large. As a result, existing analyses, which rely on the bounded gradient dissimilarity, may predict that
FedAvg is much worse than its non-local counterparts. However, by simple manipulations on the update
rule of FedAvg , one can prove the following theorem.
Theorem 3. Suppose that the weighting of the clients is uniform, and each client has a small ﬁnite amount of
data. For the above linear regression setting, the iterates of Local GD (i.e., deterministic version of FedAvg)
satisfy the following equation almost surely as the number of clients goes to inﬁnity:
w(T)−w∗=/bracketleftBig
Ec/bracketleftBig
(I−ηAc)H/bracketrightBig/bracketrightBigT
(w(0)−w∗). (18)
The proof is relegated to the Appendix. From Theorem 3, it is clear that if the learning rate ηis properly set
such that (I−ηAc)is positive deﬁnite, then performing more local updates (larger H) will lead to faster
linearconvergence rate O(exp(−T))to the global optimum w∗. That is, local GD is strictly better than
vanilla GD. However, previous papers based on gradient dissimilarity will get a substantially slower rate of
O(1/T2). In this example, while gradient dissimilarity can be arbitrarily large, data heterogeneity does not
have any negative impacts on FedAvg as the client consensus hypothesis holds.
5.2 Experiments on Naturally Non-IID Datasets
In this subsection, we empirically show that the proposed client consensus hypothesis approximately holds
across multiple practical training tasks.
In Figure 3, we ﬁrst run mini-batch SGD on Federated EMNIST (FEMNIST) (McMahan et al., 2017) and
StackOverﬂow Next Word Prediction datasets (Reddi et al., 2019) to obtain an approximation for the optimal
model w∗. Then we evaluate the average drift at optimum ρ=/bardblEcBc(w∗)/bardbland its upper bound as given in
(7) on these datasets. We summarize the important observations below.
•As shown in Figures 3a and 3b, the average drift (or pseudo-gradient) at the optimum, i.e. ρ, is
very close to zero on naturally non-IID datasets FEMNIST and StackOverﬂow. The proposed client
consensus hypothesis is true on these two realistic datasets.
•From Figures 3a and 3b, we also observe that there is a large gap betweeen ρand its upper bound as
given in (7). This suggests that previous analyses using this upper bound can be loose.
•We run the same set of experiments on a non-IID CIFAR-100 dataset. Figure 3c shows that on this
artiﬁcial dataset, the client consensus hypothesis no longer holds. The average drﬁt at optimum ρis
pretty close to its upper bound and far from zero. This is the scenario where FedAvg fails with
heterogeneous data and faces the client drift problem.
The drastically diﬀerent observations on FEMNIST, StackOverﬂow and artiﬁcial CIFAR-100 highlight that
there are various kinds of data heterogeneity. For the heterogeneity satisfying client consensus hypothesis, it
may have very limited impacts on the convergence of FedAvg .
Furthermore, we run FedAvg on FEMNIST dataset following the same setup as (Reddi et al., 2021) and check
the diﬀerence between the average of pseudo-gradient and the true batch gradient at several intermediate
points on the optimization trajectory. For each point, we let clients perform local GD with the same local
learning rate for multiple steps. As shown in Figure 4, we observe a signiﬁcant gap between the norm of
average diﬀerences (red lines) and its upper bound (7) (i.e., the average of /lscript2diﬀerence, blue lines). Especially,
at the 50thand100throunds, the upper bound is about 10times larger. These observations suggest that the
upper bounds based on the gradient dissimilarity are loose in practice.
10Published in Transactions on Machine Learning Research (06/2024)
025 50 75100 125 150 175 200
# Local Steps0.00.20.40.60.81.01.21.41.6Bias
FEMNIST
avg. of L2 diff.
L2 of avg. diffs.
(a) FEMNIST.
025 50 75100 125 150 175 200
# Local Steps0.0000.0250.0500.0750.1000.1250.1500.1750.200Bias
StackOverflow
avg. of L2 diff.
L2 of avg. diffs.(b) StackOverﬂow.
025 50 75100 125 150 175 200
# Local Steps020406080100120Bias
CIFAR-100, Round 5000
avg. of L2 diff.
L2 of avg. diffs. (c) Artiﬁcial Non-IID CIFAR-100.
Figure 3: Diﬀerence between the average pseudo-gradient and the global gradient at the optimal point w∗on
three diﬀerent datasets. We observe that the norm of the average gradient diﬀerences at w∗(red line) nearly
remain zero on all natrual non-IID datasets but its upper bound used in previous analyses (7) (blue lines)
slowly become larger when Hincreases. On a artiﬁcial non-IID CIFAR-100 dataset, these two values are
pretty close to each other.
025 50 75100 125 150 175 200
# Local Steps0.00.20.40.60.81.0Bias
FEMNIST, Round 0
avg. of L2 diff.
L2 of avg. diffs.
(a) Round 0.
025 50 75100 125 150 175 200
# Local Steps0510152025Bias
FEMNIST, Round 50
avg. of L2 diff.
L2 of avg. diffs. (b) Round 50.
025 50 75100 125 150 175 200
# Local Steps0.02.55.07.510.012.515.017.5Bias
FEMNIST, Round 100
avg. of L2 diff.
L2 of avg. diffs. (c) Round 100.
Figure 4: Diﬀerence between the average pseudo-gradient and the global gradient on a FedAvg ’s optimization
trajectory. There is a signiﬁcant gap between the average gradient diﬀerences (red line) and its upper bound
(blue lines). Both of them increase and then saturate when increasing H.
6 Conclusions
In this paper, we aim to bridge the gap between theory and practice of the popular FedAvg algorithm. We
found that previous analyses based on the bounded gradient dissimilarity assumption can be too pessimistic
for practical applications. On some natural federated datasets, FedAvg may have identical performance on
both IID and non-IID settings. In order to explain this phenomenon, we introduced the client consensus
hypothesis and formally proved that under this hypothesis, data heterogeneity does not exacerbate the
convergence of FedAvg . More importantly, we show that this hypothesis holds for a linear regression problem
and many practical federated datasets, including FEMNIST and StackOverﬂow. So the proposed hypothesis
is realistic and can better explain the empirical success of FedAvg .
Given the above contributions, several future directions are ripe for exploration. Our client consensus
hypothesis is expressed in terms of a quantity ( ρ) that is akin to a heterogeneity metric; it would be interesting
to come up with a formal metric that applies to all settings and can replace the existing loose gradient
dissimilarity metric. Perhaps such metrics could also be helpful in guiding the design principles for FL
algorithms. Besides, it may be also worthwhile to design a more practical criterion to check whether a dataset
satisﬁes the client consensus hypothesis.
11Published in Transactions on Machine Learning Research (06/2024)
References
MahmoudAssran, NicolasLoizou, NicolasBallas, andMichaelRabbat. Stochasticgradientpushfordistributed
deep learning. arXiv preprint arXiv:1811.10792 , 2018.
Zachary Charles and Jakub Konečn` y. Convergence and accuracy trade-oﬀs in federated learning and meta-
learning. In International Conference on Artiﬁcial Intelligence and Statistics , pp. 2575–2583. PMLR,
2021.
Zachary Charles and Keith Rush. Iterated vector ﬁelds and conservatism, with applications to federated
learning. In International Conference on Algorithmic Learning Theory , pp. 130–147. PMLR, 2022.
Zachary Charles, Zachary Garrett, Zhouyuan Huo, Sergei Shmulyian, and Virginia Smith. On large-cohort
training for federated learning. Advances in Neural Information Processing Systems , 34, 2021.
MichaelCrawshaw, YajieBao, andMingruiLiu. Federatedlearningwithclientsubsampling, dataheterogeneity,
and unbounded smoothness: A new algorithm and lower bounds. Advances in Neural Information Processing
Systems, 36, 2024.
Rudrajit Das, Anish Acharya, Abolfazl Hashemi, Sujay Sanghavi, Inderjit S Dhillon, and Ufuk Topcu. Faster
non-convex federated learning via global and local momentum. In Uncertainty in Artiﬁcial Intelligence , pp.
496–506. PMLR, 2022.
Margalit Glasgow, Honglin Yuan, and Tengyu Ma. Sharp bounds for federated averaging (local sgd) and
continuous perspective. arXiv preprint arXiv:2111.03741 , 2021.
Xinran Gu, Kaifeng Lyu, Longbo Huang, and Sanjeev Arora. Why (and when) does local SGD generalize
better than SGD? In The Eleventh International Conference on Learning Representations , 2023. URL
https://openreview.net/forum?id=svCcui6Drl .
Farzin Haddadpour and Mehrdad Mahdavi. On the convergence of local descent methods in federated learning.
arXiv preprint arXiv:1910.14425 , 2019.
Tzu-Ming Harry Hsu, Hang Qi, and Matthew Brown. Measuring the eﬀects of non-identical data distribution
for federated visual classiﬁcation. arXiv preprint arXiv:1909.06335 , 2019.
Divyansh Jhunjhunwala, PRANAY SHARMA, Aushim Nagarkatti, and Gauri Joshi. Fedvarp: Tackling the
variance due to partial client participation in federated learning. In The 38th Conference on Uncertainty in
Artiﬁcial Intelligence , 2022.
Peter Kairouz, H Brendan McMahan, Brendan Avent, Aurélien Bellet, Mehdi Bennis, Arjun Nitin Bhagoji,
Keith Bonawitz, Zachary Charles, Graham Cormode, Rachel Cummings, et al. Advances and open problems
in federated learning. arXiv preprint arXiv:1912.04977 , 2019.
Sai Praneeth Karimireddy, Martin Jaggi, Satyen Kale, Mehryar Mohri, Sashank J Reddi, Sebastian U Stich,
and Ananda Theertha Suresh. Mime: Mimicking centralized stochastic algorithms in federated learning.
arXiv preprint arXiv:2008.03606 , 2020a.
Sai Praneeth Karimireddy, Satyen Kale, Mehryar Mohri, Sashank J Reddi, Sebastian U Stich, and
Ananda Theertha Suresh. SCAFFOLD: Stochastic controlled averaging for on-device federated learning.
InProceedings of the International Conference on Machine Learning , 2020b.
A Khaled, K Mishchenko, and P Richtárik. Tighter theory for local SGD on identical and heterogeneous data.
InProceedings of the Twenty Third International Conference on Artiﬁcial Intelligence and Statistics , 2020.
Anastasia Koloskova, Nicolas Loizou, Sadra Boreiri, Martin Jaggi, and Sebastian U Stich. A uniﬁed theory of
decentralized SGD with changing topology and local updates. In International Conference on Machine
Learning , 2020.
Qinbin Li, Bingsheng He, and Dawn Song. Model-contrastive federated learning. In Proceedings of the
IEEE/CVF conference on computer vision and pattern recognition , pp. 10713–10722, 2021.
12Published in Transactions on Machine Learning Research (06/2024)
Tian Li, Anit Kumar Sahu, Manzil Zaheer, Maziar Sanjabi, Ameet Talwalkar, and Virginia Smith. Federated
optimization in heterogeneous networks. In Proceedings of the Conference on Machine Learning and
Systems, 2020a.
Xiang Li, Kaixuan Huang, Wenhao Yang, Shusen Wang, and Zhihua Zhang. On the convergence of fedavg on
non-iid data. arXiv preprint arXiv:1907.02189 , 2019.
Xiang Li, Kaixuan Huang, Wenhao Yang, Shusen Wang, and Zhihua Zhang. On the convergence of
FedAvg on non-IID data. In International Conference on Learning Representations , 2020b. URL https:
//openreview.net/forum?id=HJxNAnVtDS .
Xiangru Lian, Ce Zhang, Huan Zhang, Cho-Jui Hsieh, Wei Zhang, and Ji Liu. Can decentralized algorithms
outperform centralized algorithms? a case study for decentralized parallel stochastic gradient descent. In
Advances in Neural Information Processing Systems , pp. 5336–5346, 2017.
Grigory Malinovskiy, Dmitry Kovalev, Elnur Gasanov, Laurent Condat, and Peter Richtarik. From local sgd
to local ﬁxed-point methods for federated learning. In International Conference on Machine Learning , pp.
6692–6701. PMLR, 2020.
H Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, et al. Communication-eﬃcient learning
of deep networks from decentralized data. In Proceedings of the 20th International Conference on Artiﬁcial
Intelligence and Statistics , 2017.
Sashank Reddi, Zachary Charles, Manzil Zaheer, Zachary Garrett, Keith Rush, Jakub Konečn` y, Sanjiv
Kumar, and H Brendan McMahan. Adaptive federated optimization. arXiv preprint arXiv:2003.00295 ,
2020.
Sashank J Reddi, Satyen Kale, and Sanjiv Kumar. On the convergence of adam and beyond. arXiv preprint
arXiv:1904.09237 , 2019.
Sashank J. Reddi, Zachary Charles, Manzil Zaheer, Zachary Garrett, Keith Rush, Jakub Konečný, Sanjiv
Kumar, and Hugh Brendan McMahan. Adaptive federated optimization. In International Conference on
Learning Representations , 2021. URL https://openreview.net/forum?id=LkFG3lB13U5 .
Sebastian U Stich. Local SGD converges fast and communicates little. In Proceedings of the International
Conference on Learning Representations (ICLR) , 2019.
Jianyu Wang and Gauri Joshi. Cooperative SGD: A uniﬁed framework for the design and analysis of
communication-eﬃcient SGD algorithms. arXiv preprint arXiv:1808.07576 , 2018.
Jianyu Wang, Qinghua Liu, Hao Liang, Gauri Joshi, and H Vincent Poor. Tackling the objective inconsistency
problem in heterogeneous federated optimization. Advances in Neural Information Processing Systems , 33,
2020a.
Jianyu Wang, Vinayak Tantia, Nicolas Ballas, and Michael Rabbat. SlowMo: Improving communication-
eﬃcient distributed SGD with slow momentum. In Proceedings of the International Conference on Learning
Representations (ICLR) , 2020b.
Jianyu Wang, Zachary Charles, Zheng Xu, Gauri Joshi, H Brendan McMahan, Maruan Al-Shedivat, Galen
Andrew, Salman Avestimehr, Katharine Daly, Deepesh Data, et al. A ﬁeld guide to federated optimization.
arXiv preprint arXiv:2107.06917 , 2021.
Jianyu Wang, Anit Kumar Sahu, Gauri Joshi, and Soummya Kar. Matcha: A matching-based link scheduling
strategy to speed up distributed optimization. IEEE Transactions on Signal Processing , 70:5208–5221,
2022.
Jiayi Wang, Shiqiang Wang, Rong-Rong Chen, and Mingyue Ji. A new theoretical perspective on data
heterogeneity in federated optimization. In Federated Learning and Analytics in Practice: Algorithms,
Systems, Applications, and Opportunities , 2023a.
13Published in Transactions on Machine Learning Research (06/2024)
Jiayi Wang, Shiqiang Wang, Rong-Rong Chen, and Mingyue Ji. Rethinking the data heterogeneity in
federated learning. In 2023 57th Asilomar Conference on Signals, Systems, and Computers , pp. 624–628.
IEEE, 2023b.
Blake Woodworth, Kumar Kshitij Patel, and Nathan Srebro. Minibatch vs local sgd for heterogeneous
distributed learning. arXiv preprint arXiv:2006.04735 , 2020a.
Blake Woodworth, Kumar Kshitij Patel, Sebastian U Stich, Zhen Dai, Brian Bullins, H Brendan McMahan,
Ohad Shamir, and Nathan Srebro. Is local SGD better than minibatch SGD? In International Conference
on Machine Learning , 2020b.
Feijie Wu, Song Guo, Zhihao Qu, Shiqi He, Ziming Liu, and Jing Gao. Anchor sampling for federated learning
with partial client participation. In International Conference on Machine Learning , pp. 37379–37416.
PMLR, 2023.
Haibo Yang, Minghong Fang, and Jia Liu. Achieving linear speedup with partial worker participation in
non-iid federated learning. In International Conference on Learning Representations , 2020.
Hao Yu, Rong Jin, and Sen Yang. On the linear speedup analysis of communication eﬃcient momentum
SGD for distributed non-convex optimization. In International Conference on Machine Learning , 2019a.
Hao Yu, Sen Yang, and Shenghuo Zhu. Parallel restarted SGD with faster convergence and less communication:
Demystifying why model averaging works for deep learning. In Proceedings of the AAAI Conference on
Artiﬁcial Intelligence , volume 33, pp. 5693–5700, 2019b.
Kun Yuan, Qing Ling, and Wotao Yin. On the convergence of decentralized gradient descent. SIAM Journal
on Optimization , 26(3):1835–1854, 2016.
Yue Zhao, Meng Li, Liangzhen Lai, Naveen Suda, Damon Civin, and Vikas Chandra. Federated learning
with non-IID data. arXiv preprint arXiv:1806.00582 , 2018.
Fan Zhou and Guojing Cong. On the convergence properties of a k-step averaging stochastic gradient
descent algorithm for nonconvex optimization. In Proceedings of the 27th International Joint Conference
on Artiﬁcial Intelligence (IJCAI) , pp. 3219–3227, 2018.
Ali Zindari, Ruichen Luo, and Sebastian U Stich. On the convergence of local sgd under third-order smoothness
and hessian similarity. In OPT 2023: Optimization for Machine Learning , 2023.
14Published in Transactions on Machine Learning Research (06/2024)
A Experimental Details
On FEMNIST, StackOverﬂow, and CIFAR-100 datasets, we strictly follow the training setup given in Reddi
et al. (2020). Both models are neural networks and hence the objective functions are non-convex. In Figure 3,
we used a trained model (obtained after training with mini-batch SGD) as an approximation of the global
optimum. Then, a large set of clients are selected to perform local model training from the optimum to
calculate the gradient bias. Details on the local training can be found in the following table.
Table 3: Details on local training in Figure 3.
Dataset Model Loss function # of clients Local optimizer Local learning rate
FEMNIST ConvNet Cross-Entropy 500 GD 0.1
StackOverﬂow LSTM Cross-Entropy 1000 GD 0.5
CIFAR-100 ConvNet Cross-Entropy 200 GD 0.5
B Proof of Theorem 2
B.1 Preliminaries
In this subsection, we will ﬁrst introduce several useful lemmas, which relate to the properties of the
deterministic pseudo-gradients. Before diving into the proof details, we would like to ﬁrst introduce a lemma,
which will be frequently used in the subsequent sections.
Lemma 1 (Mean Value Theorem ).Suppose function Fis twice diﬀerentiable, then
∇Fc(w)−∇Fc(u) =Ac(w,u)·(w−u) (19)
where Ac(w,u) =/integraltext1
0∇2Fc(u+s(w−u))ds. Ifµ/precedesequal∇2Fc/precedesequalL, then it follows that µ/precedesequalAc(w,u)/precedesequalLfor
anyw,u.
Lemma 2 (Convexity, Smoothness & Co-coercivity ).When each local objective function FcisL-
Lipschitz smooth and µ-strongly convex, for any w,u∈Rd, we have
/tildewideµ/bardblw−u/bardbl2≤/angbracketleftG(w)−G(u),w−u/angbracketright≤/tildewideL/bardblw−u/bardbl2, (20)
/bardblG(w)−G(u)/bardbl2≤/tildewideL/angbracketleftG(w)−G(u),w−u/angbracketright, (21)
where/tildewideµ= [1−(1−ηµ)H]/(ηH)and/tildewideL= [1 + (1−ηµ)H]/(ηH).
Proof.Let us ﬁrst focus on the pseudo-gradient on a speciﬁc client c. According to the deﬁnition of
pseudo-gradient, we have
ηH[Gc(w)−Gc(u)] =w−u−(w(H)
c−u(H)
c) (22)
=w−u
−[w(H−1)
c−u(H−1)
c−η(∇Fc(w(H−1)
c )−∇Fc(u(H−1)
c ))] (23)
=w−u−(I−ηD(H−1)
c )(w(H−1)
c−u(H−1)
c ) (24)
where (24) follows from Lemma 1 and Dcis a symmetric matrix satisfying µ/precedesequalDc/precedesequalL. Repeating the above
procedure, we can obtain that
ηH[Gc(w)−Gc(u)] =w−u−H−1/productdisplay
k=0(I−ηD(k)
c)(w−u) (25)
=/bracketleftBigg
I−H−1/productdisplay
k=0(I−ηD(k)
c)/bracketrightBigg
(w−u). (26)
15Published in Transactions on Machine Learning Research (06/2024)
As a consequence, we have
ηH/angbracketleftGc(w)−Gc(u),w−u/angbracketright=/bardblw−u/bardbl2−/angbracketleftBiggH−1/productdisplay
k=0(I−ηD(k)
c)(w−u),w−u/angbracketrightBigg
. (27)
Note that, due to Cauchy–Schwarz inequality,
/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/angbracketleftBiggH−1/productdisplay
k=0(I−ηD(k)
c)(w−u),w−u/angbracketrightBigg/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle≤H−1/productdisplay
k=0/vextenddouble/vextenddouble/vextenddoubleI−ηD(k)
c/vextenddouble/vextenddouble/vextenddouble/bardblw−u/bardbl2(28)
≤(1−ηµ)H/bardblw−u/bardbl2. (29)
That is,
−(1−ηµ)H/bardblw−u/bardbl2≤/angbracketleftBiggH−1/productdisplay
k=0(I−ηD(k)
c)(w−u),w−u/angbracketrightBigg
≤(1−ηµ)H/bardblw−u/bardbl2.(30)
It follows that,
1−(1−ηµ)H
ηH/bardblw−u/bardbl2≤/angbracketleftGc(w)−Gc(u),w−u/angbracketright≤1 + (1−ηµ)H
ηH/bardblw−u/bardbl2. (31)
Taking the average over all clients, we complete the proof of (20).
Next, we are going to prove (21). Note that
/vextenddouble/vextenddouble/vextenddoublew(H)
c−u(H)
c/vextenddouble/vextenddouble/vextenddouble=/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoubleH−1/productdisplay
k=0(I−ηD(k)
c)(w−u)/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble(32)
≤H−1/productdisplay
k=0/vextenddouble/vextenddouble/vextenddoubleI−ηD(k)
c/vextenddouble/vextenddouble/vextenddouble/bardblw−u/bardbl (33)
≤(1−ηµ)H/bardblw−u/bardbl. (34)
As a result, we have
/vextenddouble/vextenddouble/vextenddoublew(H)−u(H)/vextenddouble/vextenddouble/vextenddouble2
=/vextenddouble/vextenddouble/vextenddoubleEcw(H)
c−Ecu(H)
c/vextenddouble/vextenddouble/vextenddouble2
≤Ec/vextenddouble/vextenddouble/vextenddoublew(H)
c−u(H)
c/vextenddouble/vextenddouble/vextenddouble2
(35)
≤[(1−ηµ)H]2/bardblw−u/bardbl2. (36)
Then, according to the deﬁnition of pseudo-gradients, one can obtain
η2H2/bardblG(w)−G(u)/bardbl2=/vextenddouble/vextenddouble/vextenddoublew−u−w(H)+u(H)/vextenddouble/vextenddouble/vextenddouble2
(37)
=/bardblw−u/bardbl2+/vextenddouble/vextenddouble/vextenddoublew(H)−u(H)/vextenddouble/vextenddouble/vextenddouble2
−2/angbracketleftBig
w−u,w(H)−u(H)/angbracketrightBig
(38)
≤[1 + (1−ηµ)H]/bardblw−u/bardbl2−2/angbracketleftBig
w−u,w(H)−u(H)/angbracketrightBig
−(1−ηµ)H/bracketleftbig
1−(1−ηµ)H/bracketrightbig
/bardblw−u/bardbl2(39)
=[1 + (1−ηµ)H]/bracketleftBig
/bardblw−u/bardbl2−/angbracketleftBig
w−u,w(H)−u(H)/angbracketrightBig/bracketrightBig
−(1−(1−ηµ)H)/angbracketleftBig
w−u,w(H)−u(H)/angbracketrightBig
−(1−ηµ)H/bracketleftbig
1−(1−ηµ)H/bracketrightbig
/bardblw−u/bardbl2(40)
=[1 + (1−ηµ)H]/bracketleftBig
/bardblw−u/bardbl2−/angbracketleftBig
w−u,w(H)−u(H)/angbracketrightBig/bracketrightBig
+ (1−(1−ηµ)H)/bracketleftBig
/bardblw−u/bardbl2−/angbracketleftBig
w−u,w(H)−u(H)/angbracketrightBig/bracketrightBig
−/bracketleftbig
1 + (1−ηµ)H/bracketrightbig/bracketleftbig
1−(1−ηµ)H/bracketrightbig
/bardblw−u/bardbl2(41)
16Published in Transactions on Machine Learning Research (06/2024)
Note thatηH/angbracketleftw−u,G(w)−G(u)/angbracketright=/bardblw−u/bardbl2−/angbracketleftbig
w−u,w(H)−u(H)/angbracketrightbig
andηH/tildewideL= 1 + (1−ηµ)H,ηH/tildewideµ=
1−(1−ηµ)H, we have
/bardblG(w)−G(u)/bardbl2≤(/tildewideL+/tildewideµ)/angbracketleftw−u,G(w)−G(u)/angbracketright−/tildewideL/tildewideµ/bardblw−u/bardbl2(42)
=/tildewideL/angbracketleftw−u,G(w)−G(u)/angbracketright
−/tildewideµ/bracketleftBig
/tildewideL/bardblw−u/bardbl2−/angbracketleftw−u,G(w)−G(u)/angbracketright/bracketrightBig
(43)
≤/tildewideL/angbracketleftw−u,G(w)−G(u)/angbracketright (44)
where the last inequality is due to the smoothness of the pseudo-gradient (20).
B.2 Main Proof
In the analysis below, we ﬁrst analyze the training progress within one round. Suppose the current global
model is wand the next round’s global model is w+. Without otherwise stated, the expectation Eand
variance Varare conditioned on the current global model w. For the ease of writing, we deﬁne eﬀective
learning rate /tildewideα=αηH.
First, according to the update rule of FedAvg , we have
E/vextenddouble/vextenddoublew+−w∗/vextenddouble/vextenddouble2=E/vextenddouble/vextenddouble/vextenddoublew−/tildewideαˆG(w)−w∗/vextenddouble/vextenddouble/vextenddouble2
(45)
=/vextenddouble/vextenddouble/vextenddoublew−/tildewideαE[ˆG(w)]−w∗/vextenddouble/vextenddouble/vextenddouble2
+/tildewideα2Var[ˆG(w)] (46)
=/bardblw−w∗/bardbl2+/tildewideα2/vextenddouble/vextenddouble/vextenddoubleE[ˆG(w)]/vextenddouble/vextenddouble/vextenddouble2
−2/tildewideα/angbracketleftBig
w−w∗,E[ˆG(w)]/angbracketrightBig
+/tildewideα2Var[ˆG(w)](47)
Also, note that E[ˆG(w)] =G(w)−G(w∗) +G(w∗) +δ(w). So one can obtain
E/vextenddouble/vextenddoublew+−w∗/vextenddouble/vextenddouble2≤/bardblw−w∗/bardbl2+ 2/tildewideα2/bardblG(w)−G(w∗)/bardbl2−2/tildewideα/angbracketleftw−w∗,G(w)−G(w∗)/angbracketright
+ 2/tildewideα2/bardblG(w∗) +δ(w)/bardbl2−2/tildewideα/angbracketleftw−w∗,G(w∗) +δ(w)/angbracketright+/tildewideα2Var[ˆG(w)] (48)
≤(1−/tildewideα/tildewideµ)/bardblw−w∗/bardbl2+ 2/tildewideα2/bardblG(w)−G(w∗)/bardbl2−/tildewideα/angbracketleftw−w∗,G(w)−G(w∗)/angbracketright
+ 2/tildewideα2/bardblG(w∗) +δ(w)/bardbl2−2/tildewideα/angbracketleftw−w∗,G(w∗) +δ(w)/angbracketright+/tildewideα2Var[ˆG(w)] (49)
where the ﬁrst inequality uses the fact /bardbla+b/bardbl2≤2/bardbla/bardbl2+ 2/bardblb/bardbl2, and the second inequality comes from the
strongly-convexity of the pseudo-gradient. Now let us check the value of the following terms:
T1=2/tildewideα/bardblG(w)−G(w∗)/bardbl2−/angbracketleftw−w∗,G(w)−G(w∗)/angbracketright−2/angbracketleftw−w∗,G(w∗) +δ(w)/angbracketright. (50)
According to the co-coercivity of the pseudo-gradient, we have
T1≤/bracketleftBig
2/tildewideα/tildewideL−1/bracketrightBig
/angbracketleftw−w∗,G(w)−G(w∗)/angbracketright−2/angbracketleftw−w∗,G(w∗) +δ(w)/angbracketright (51)
≤/bracketleftBig
2/tildewideα/tildewideL−1/bracketrightBig
/angbracketleftw−w∗,G(w)−G(w∗)/angbracketright+/epsilon1/bardblw−w∗/bardbl2+1
/epsilon1/bardblG(w∗) +δ(w)/bardbl2(52)
where the last inequality comes from Young’s inequality. When /tildewideα/tildewideµ≤/tildewideα/tildewideL≤1/4, we have
T1≤−/tildewideµ
2/bardblw−w∗/bardbl2+/epsilon1/bardblw−w∗/bardbl2+1
/epsilon1/bardblG(w∗) +δ(w)/bardbl2≤2/bardblG(w∗) +δ(w)/bardbl2
/tildewideµ(53)
17Published in Transactions on Machine Learning Research (06/2024)
where the last inequality is obtained by setting /epsilon1=/tildewideµ/2. Then, substituting (53) into (49) and noting that
/tildewideα/tildewideµ≤/tildewideα/tildewideL≤1/4(that is,/tildewideα≤1/(4/tildewideµ)),
E/vextenddouble/vextenddoublew+−w∗/vextenddouble/vextenddouble2≤(1−/tildewideα/tildewideµ)/bardblw−w∗/bardbl2+/parenleftbigg
2/tildewideα2+2/tildewideα
/tildewideµ/parenrightbigg
/bardblG(w∗) +δ(w)/bardbl2+/tildewideα2Var[ˆG(w)](54)
≤(1−/tildewideα/tildewideµ)/bardblw−w∗/bardbl2+5/tildewideα
2/tildewideµ/bardblG(w∗) +δ(w)/bardbl2+/tildewideα2Var[ˆG(w)] (55)
≤(1−/tildewideα/tildewideµ)/bardblw−w∗/bardbl2+/tildewideα2Var[ˆG(w)] +5/tildewideα
/tildewideµ/bardblδ(w)/bardbl2+5/tildewideα
/tildewideµ/bardblG(w∗)/bardbl2(56)
≤(1−/tildewideα/tildewideµ)/bardblw−w∗/bardbl2+/tildewideα2max
wVar[ˆG(w)]
+5/tildewideα
/tildewideµmax
w/bardblδ(w)/bardbl2+5/tildewideα
/tildewideµ/bardblG(w∗)/bardbl2(57)
After total Tcommunication rounds and taking the total expectation, we end up with
E/vextenddouble/vextenddouble/vextenddoublew(T)−w∗/vextenddouble/vextenddouble/vextenddouble2
≤(1−/tildewideα/tildewideµ)T/vextenddouble/vextenddouble/vextenddoublew(0)−w∗/vextenddouble/vextenddouble/vextenddouble2
+/tildewideα
/tildewideµmax
wVar[ˆG(w)]
+5
/tildewideµ2max
wEc/bardblδc(w)/bardbl2+5ρ2
/tildewideµ2. (58)
WhenηHµ≤1, one can easily validate that
/tildewideµ=1−(1−ηµ)H
ηH≥µ
2. (59)
So it follows that
E/vextenddouble/vextenddouble/vextenddoublew(T)−w∗/vextenddouble/vextenddouble/vextenddouble2
≤(1−1
2αηHµ )T/vextenddouble/vextenddouble/vextenddoublew(0)−w∗/vextenddouble/vextenddouble/vextenddouble2
+2αηH
µmax
wVar[ˆG(w)]
+20
µ2max
wEc/bardblδc(w)/bardbl2+20ρ2
µ2. (60)
At last, in order to satisfy /tildewideα/tildewideL≤1/4, one can set α≤1/8, such that
/tildewideα/tildewideL=αηH·1 + (1−ηµ)H
ηH=α(1 + (1−ηµ)H)≤2α≤1
4. (61)
C Bound on Iterate Bias
In this section, we will provide an upper bound for the iterate bias (11). According to the local update rules,
we have/vextenddouble/vextenddouble/vextenddoublew(H)
c,GD−E[w(H)
c]/vextenddouble/vextenddouble/vextenddouble=/vextenddouble/vextenddouble/vextenddoublew(H−1)
c,GD−E[w(H−1)
c ]−η∇Fc(w(H−1)
c,GD) +ηE[∇Fc(w(H−1)
c )]/vextenddouble/vextenddouble/vextenddouble (62)
≤/vextenddouble/vextenddouble/vextenddoublew(H−1)
c,GD−E[w(H−1)
c ]−η∇Fc(w(H−1)
c,GD) +η∇Fc(E[w(H−1)
c ])/vextenddouble/vextenddouble/vextenddouble
+η/vextenddouble/vextenddouble/vextenddoubleE[∇Fc(w(H−1)
c )]−∇Fc(E[w(H−1)
c ])/vextenddouble/vextenddouble/vextenddouble (63)
≤(1−ηµ)/vextenddouble/vextenddouble/vextenddoublew(H−1)
c,GD−E[w(H−1)
c ]/vextenddouble/vextenddouble/vextenddouble
+η/vextenddouble/vextenddouble/vextenddoubleE[∇Fc(w(H−1)
c )]−∇Fc(E[w(H−1)
c ])/vextenddouble/vextenddouble/vextenddouble (64)
For the second term, we have
/vextenddouble/vextenddouble/vextenddoubleE[∇Fc(w(H−1)
c )]−∇Fc(E[w(H−1)
c ])/vextenddouble/vextenddouble/vextenddouble2
≤E/vextenddouble/vextenddouble/vextenddouble∇Fc(w(H−1)
c )−∇Fc(E[w(H−1)
c ])/vextenddouble/vextenddouble/vextenddouble2
(65)
≤L2E/vextenddouble/vextenddouble/vextenddoublewH−1
c−E[w(H−1)
c ]/vextenddouble/vextenddouble/vextenddouble2
(66)
≤2η2L2σ2(H−1) (67)
18Published in Transactions on Machine Learning Research (06/2024)
where the last inequality comes from previous works Khaled et al. (2020); Glasgow et al. (2021). As a result,
one can obtain
/vextenddouble/vextenddouble/vextenddoublew(H)
c,GD−E[w(H)
c]/vextenddouble/vextenddouble/vextenddouble≤(1−ηµ)/vextenddouble/vextenddouble/vextenddoublew(H−1)
c,GD−E[w(H−1)
c ]/vextenddouble/vextenddouble/vextenddouble+√
2η2Lσ(H−1)1
2 (68)
≤√
2η2LσH−1/summationdisplay
h=0(1−ηµ)H−1−hh1
2 (69)
≤√
2η2Lσ/bracketleftBiggH−1/summationdisplay
h=0(1−ηµ)2(H−1−h)/bracketrightBigg1
2/bracketleftBiggH−1/summationdisplay
h=0h/bracketrightBigg1
2
(70)
≤√
2η2Lσ/bracketleftBiggH−1/summationdisplay
h=0(1−ηµ)H−1−h/bracketrightBigg1
2/bracketleftBiggH−1/summationdisplay
h=0h/bracketrightBigg1
2
(71)
=/bracketleftbigg1−(1−ηµ)H
ηµH/bracketrightbigg1
2
η2LσH (H−1)1
2. (72)
According to the deﬁnition of δ(w), we obtain
Ec/bardblδc(w)/bardbl2≤Ec/vextenddouble/vextenddouble/vextenddoublew(H)
c,GD−E[w(H)
c]/vextenddouble/vextenddouble/vextenddouble2
≤/tildewideµη2L2σ2(H−1)
µ≤η2L2σ2(H−1) (73)
where the last inequality comes from the fact that /tildewideµ≤µ.
D Proof of Corollary 1
D.1 Deterministic Setting
When clients perform local GD in each round, there is no stochastic noise. So the error upper bound (9) can
be simpliﬁed as follows
/vextenddouble/vextenddouble/vextenddoublew(T)−w∗/vextenddouble/vextenddouble/vextenddouble2
≤(1−1
2αηHµ )T/vextenddouble/vextenddouble/vextenddoublew(0)−w∗/vextenddouble/vextenddouble/vextenddouble2
+20ρ2
µ2. (74)
IfHµ≥L, then the maximal learning rate is η= 1/Hµ. Whenα= 1/8, the upper bound becomes
/vextenddouble/vextenddouble/vextenddoublew(T)−w∗/vextenddouble/vextenddouble/vextenddouble2
≤/parenleftbigg
1−1
16/parenrightbiggT/vextenddouble/vextenddouble/vextenddoublew(0)−w∗/vextenddouble/vextenddouble/vextenddouble2
+20ρ2
µ2(75)
≤exp/parenleftbigg
−T
16/parenrightbigg/vextenddouble/vextenddouble/vextenddoublew(0)−w∗/vextenddouble/vextenddouble/vextenddouble2
+20ρ2
µ2. (76)
IfHµ≤L, then the maximal learning rate is η= 1/L. Whenα= 1/8, the upper bound becomes
/vextenddouble/vextenddouble/vextenddoublew(T)−w∗/vextenddouble/vextenddouble/vextenddouble2
≤/parenleftbigg
1−Hµ
16L/parenrightbiggT/vextenddouble/vextenddouble/vextenddoublew(0)−w∗/vextenddouble/vextenddouble/vextenddouble2
+20ρ2
µ2(77)
≤exp/parenleftbigg
−µHT
16L/parenrightbigg/vextenddouble/vextenddouble/vextenddoublew(0)−w∗/vextenddouble/vextenddouble/vextenddouble2
+20ρ2
µ2. (78)
We can summarize the above two bounds as follows:
/vextenddouble/vextenddouble/vextenddoublew(T)−w∗/vextenddouble/vextenddouble/vextenddouble2
≤exp/parenleftbigg
−T
16κmin{κ,H}/parenrightbigg/vextenddouble/vextenddouble/vextenddoublew(0)−w∗/vextenddouble/vextenddouble/vextenddouble2
+20ρ2
µ2(79)
=O/parenleftbigg
exp/parenleftbigg
−T
16κmin{κ,H}/parenrightbigg
+ρ2/parenrightbigg
. (80)
19Published in Transactions on Machine Learning Research (06/2024)
D.2 Stochastic Setting
Substituting the upper bounds for Var[G(w)]andδ(w)into (57) and setting α= 1/8,
E/vextenddouble/vextenddouble/vextenddoublew(t+1)−w∗/vextenddouble/vextenddouble/vextenddouble2
≤(1−/tildewideα/tildewideµ)/vextenddouble/vextenddouble/vextenddoublew(t)−w∗/vextenddouble/vextenddouble/vextenddouble2
+/tildewideα22σ2
MH+5/tildewideα3
/tildewideµσ2L2(H−1)
α2H2+5/tildewideα
/tildewideµ/bardblG(w∗)/bardbl2(81)
≤(1−/tildewideα/tildewideµ)/vextenddouble/vextenddouble/vextenddoublew(t)−w∗/vextenddouble/vextenddouble/vextenddouble2
+/tildewideα22σ2
MH+/tildewideα3320σ2L2
/tildewideµH+/tildewideα5ρ2
/tildewideµ. (82)
After minor rearrangement, we can get
E/vextenddouble/vextenddouble/vextenddoublew(t+1)−w∗/vextenddouble/vextenddouble/vextenddouble2
−5ρ2
/tildewideµ2≤(1−/tildewideα/tildewideµ)/bracketleftbigg/vextenddouble/vextenddouble/vextenddoublew(t)−w∗/vextenddouble/vextenddouble/vextenddouble2
−5ρ2
/tildewideµ2/bracketrightbigg
+/tildewideα22σ2
MH+/tildewideα3320σ2L2
/tildewideµH.(83)
After total Tcommunication rounds,
E/vextenddouble/vextenddouble/vextenddoublew(t+1)−w∗/vextenddouble/vextenddouble/vextenddouble2
−5ρ2
/tildewideµ2≤(1−/tildewideα/tildewideµ)T/bracketleftbigg/vextenddouble/vextenddouble/vextenddoublew(0)−w∗/vextenddouble/vextenddouble/vextenddouble2
−5ρ2
/tildewideµ2/bracketrightbigg
/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
r0+2/tildewideασ2
/tildewideµMH+320/tildewideα2σ2L2
/tildewideµ2H.(84)
If we set/tildewideα=ν
µT, whereν= 2 ln(max{r0µ2MHT/ (8σ2),r0µ4HT2/(1280L2σ2)}), then it follows that
E/vextenddouble/vextenddouble/vextenddoublew(t+1)−w∗/vextenddouble/vextenddouble/vextenddouble2
−20ρ2
µ2≤8σ2ν
µ2MHT+1280σ2L2ν
µ4HT2+ exp/parenleftBig
−ν
2/parenrightBig
r0 (85)
≤8σ2(ν+ 1)
µ2MHT+1280σ2L2(ν+ 1)
µ4HT2(86)
=/tildewideO/parenleftbiggσ2
MHT+σ2
HT2/parenrightbigg
(87)
where/tildewideOhides logarithmic factors.
E Extensions to General Convex Functions
In this section, we are going to extend Theorem 2 to general convex settings for deterministic FedAvg . This
extension can help to show that our conclusion “FedAvg can have identical convergence rate in homogeneous
and heterogeneous settings" is not only constrained to the strongly convex settings.
When the average client drift at optimum is zero and there is no stochastic noise, we have
/vextenddouble/vextenddoublew+−w∗/vextenddouble/vextenddouble2=/bardblw−w∗−αG(w)/bardbl2(88)
=/bardblw−w∗/bardbl2−2α/angbracketleftw−w∗,G(w)−G(w∗)/angbracketright+α2/bardblG(w)−G(w∗)/bardbl2(89)
≤(1−α/tildewideµ)/bardblw−w∗/bardbl2−α
/tildewideL(1−α/tildewideL)/bardblG(w)−G(w∗)/bardbl2(90)
≤(1−α/tildewideµ)/bardblw−w∗/bardbl2−α
2/tildewideL/bardblG(w)−G(w∗)/bardbl2(91)
where the last inequality is due to α/tildewideL≤1/2. In the general convex setting, we have µ= 0. According to the
deﬁnitions of /tildewideµ,/tildewideLin Lemma 2, it follows that /tildewideµ= 0and/tildewideL= 2/ηH. Substituting these values into (91), we
obtain
/vextenddouble/vextenddoublew+−w∗/vextenddouble/vextenddouble2≤/bardblw−w∗/bardbl2−αηH
4/bardblG(w)−G(w∗)/bardbl2. (92)
After minor rearrangement, we have
/bardblG(w)−G(w∗)/bardbl2≤4
αηH/bracketleftBig
/bardblw−w∗/bardbl2−/vextenddouble/vextenddoublew+−w∗/vextenddouble/vextenddouble2/bracketrightBig
. (93)
20Published in Transactions on Machine Learning Research (06/2024)
Taking the average from t= 0tot=T−1,
1
TT−1/summationdisplay
t=0/vextenddouble/vextenddouble/vextenddoubleG(w(t))−G(w∗)/vextenddouble/vextenddouble/vextenddouble2
≤4/vextenddouble/vextenddoublew(0)−w∗/vextenddouble/vextenddouble2
αηHT(94)
If we setα= 1/4,η= 1/L, then
1
TT−1/summationdisplay
t=0/vextenddouble/vextenddouble/vextenddoubleG(w(t))−G(w∗)/vextenddouble/vextenddouble/vextenddouble2
≤16L/vextenddouble/vextenddoublew(0)−w∗/vextenddouble/vextenddouble2
HT. (95)
The above rate is the same as GD for general convex functions and data heterogeneity does not have negative
impacts, as ρ=/bardblG(w)/bardbl= 0.
F Proof of Theorem 3
Proof.According to the local update rule, we have
w(t,h+1)
c =w(t,h)
c−η∇Fc(w(t,h)
c) (96)
=w(t,h)
c−η/bracketleftBig
Ac(w(t,h)
c−w∗)−bc/bracketrightBig
(97)
= (I−ηAc)w(t,h)
c+η(Acw∗+bc). (98)
Subtracting w∗
c=w∗+A−1
cbcon both sides, it follows that
w(t,h+1)
c−w∗
c= (I−ηAc)/parenleftBig
w(t,h)
c−w∗
c/parenrightBig
(99)
= (I−ηAc)h+1/parenleftBig
w(t)−w∗
c/parenrightBig
. (100)
Settingh=H, we have w(t,H)
c = (I−ηAc)H(w(t)−w∗
c) +w∗
c. Recall the deﬁnition of pseudo-gradient (6),
we get
Gc(w(t)) =1
ηH(w(t)−w(t,H)) (101)
=1
ηH/bracketleftbig
I−(I−ηAc)H/bracketrightbig
(w(t)−w∗
c). (102)
According to the global update rule of FedAvg , one can obtain that
w(t+1)=w(t)−αηHEcGc(w(t)) (103)
=w(t)−αEc/bracketleftBig
(I−(I−ηAc)H)/parenleftBig
w(t)−w∗
c/parenrightBig/bracketrightBig
(104)
=w(t)−αEc/bracketleftBig
(I−(I−ηAc)H)/parenleftBig
w(t)−w∗/parenrightBig/bracketrightBig
−αEc/bracketleftbig
(I−(I−ηAc)H) (w∗−w∗
c)/bracketrightbig
. (105)
Subtracting w∗on both sides and setting α= 1, we have
w(t+1)−w∗=Ec/bracketleftBig
(I−ηAc)H/bracketrightBig/parenleftBig
w(t)−w∗/parenrightBig
−Ec
(I−(I−ηAc)H) (w∗−w∗
c)/bracehtipupleft/bracehtipdownright/bracehtipdownleft /bracehtipupright
ηHGc(w∗)
 (106)
=Ec/bracketleftBig
(I−ηAc)H/bracketrightBig/parenleftBig
w(t)−w∗/parenrightBig
−ηHG(w∗) (107)
whereG(w∗) =EcGc(w∗).
21Published in Transactions on Machine Learning Research (06/2024)
Now, we prove that G(w∗) = 0almost surely as M→∞on this synthetic problem. According to the
deﬁnition of Ac,bc, we obtain that
G(w∗) =Ec/bracketleftbig
(I−(I−ηAc)H)A−1
cbc/bracketrightbig
(108)
=Ec
(I−(I−ηAc)H)A−1
c1
nn/summationdisplay
i=1xc,i/epsilon1c,i
/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
ξc
. (109)
Since the noise /epsilon1c,·are independent of xc,·,ξcis a zero-mean random variable that depends on client c. Since
we have assumed that all /bardblxc,i/bardbland/epsilon1c,ihave bounded variance, we know that E[ξ2
c]<∞. Since we have a
uniform weighting on the Mclients, it follows Ec[ξc] =O(1/√
M)with probability 1−oM(1), and asM→∞,
we have Ec[ξc]→0almost surely.
Hence, from (107), we conclude that
w(t+1)−w∗=/bracketleftBig
Ec/bracketleftBig
(I−ηAc)H/bracketrightBig/bracketrightBigt+1
(w(0)−w∗), (110)
almost surely as M→∞, which proves the desired result.
22