Approximate Size Targets Are Sufficient
for Accurate Semantic Segmentation
Anonymous Author(s)
Affiliation
Address
email
Abstract
We propose a new general form of image-level supervision for semantic segmenta- 1
tion based on approximate targets for the relative size of segments. At each training 2
image, such targets are represented by a categorical distribution for the “expected” 3
average prediction over the image pixels. We motivate the zero-avoiding variant of 4
KL divergence as a general training loss for any segmentation architecture leading 5
to quality on par with the full pixel-level supervision. However, our image-level 6
supervision is significantly less expensive, it needs to know only an approximate 7
fraction of an image occupied by each class. Such estimates are easy for a human 8
annotator compared to pixel-accurate labeling. Our loss shows significant robust- 9
ness to size target errors, which may even improve the generalization quality. The 10
proposed size targets can be seen as an extension of the standard class tags, which 11
correspond to non-zero size targets in each image. Using only a minimal amount 12
of extra information, our supervision improves and simplifies the training. It works 13
on standard segmentation architectures as is, unlike tag-based methods requiring 14
complex specialized modifications and multi-stage training. 15
1 Introduction 16
Our image-level supervision approach applies to any semantic segmentation model and does not 17
require any modification. It can be technically described in one paragraph, as follows. Soft-max 18
prediction Sp= (S1
p, . . . , SK
p)at any pixel pis a categorical distribution over Kclasses, including 19
background. At any image, the average prediction over all image pixels, denoted by set Ω, is 20
¯S:=1
|Ω|X
p∈ΩSp (1)
where ¯S= (¯S1, . . . , ¯SK)is also a categorical distribution over Kclasses. It is an image-level 21
prediction of the relative or normalized sizes (volume, area, or cardinality) of the objects in the image. 22
We assume that training images have approximate size targets represented by categorical distributions 23
v= (vk)K
k=1, e.g. v= (0, .15,0, . . . , 0, .75)for the middle image in Fig. 1 if “bird” is the second 24
class and “background” is the last. This representation also applies to multi-label images. For each 25
training image, our size-target loss 26
Lsize =KL(v∥¯S) =X
kvklnvk
¯Sk(2)
is based on Kullback–Leibler ( KL) divergence. Figure 2(b) shows some results for a generic 27
segmentation network (ResNet101 [ 4] backbone) trained on PASCAL [ 5] using only image-level 28
supervision with approximate size targets ( 8%mean relative errors). Our total loss is very simple: it 29
combines size-target loss (2) and a common CRF loss (3) [6]. 30
Submitted to 38th Conference on Neural Information Processing Systems (NeurIPS 2024). Do not distribute.TagBounding boxFull supervision
20.0 sec 47.1 sec 239.7 sec 78.2% (R101) Point
23.3 sec 66.6% (R101) 62.7% (WR38) mIOU:74.9% (R101) Size target (ours)
36.2 sec 72.6% (R101) bird:15%bird
sec. per image:size target cost = 1.5 classes/image * 11.8 s/class +  (20 – 1.5)s (non tag classes)= 36.2sbounding box cost = 2.8 instances/image * 10.2 s/instance + 18.5s (no tag) = 47.1sFor tag, point, full supervision cost, see the paper: What's the Point: Semantic Segmentation with Point Supervision Amy Bearman,Olga Russakovsky,Vittorio Ferrari,Li Fei-Fei, ECCVEach image has 2.8 instances and 1.5 classes on average (PASCAL VOC)20 sec32 sec30 sec151 sec1045 secFigure 1: Supervision types for segmentation: labeling speed and accuracy on PASCAL. The top-left
corner of each image shows its estimated labeling time based on observed instances. The table
shows per-image labeling times averaged over the data and mean Intersection-over-Union (mIoU) for
comparable end-to-end methods with similar ResNet backbones (ResNet101 or WideResNet38 [ 1]),
for fairness. We obtained mIoU scores, except for the “tag” and “box” scores from [ 2] and [ 3]. Our
supplemental materials detail evaluation of the labeling times and mIoU. For completeness, Tab.2
includes more complex architectures and multi-stage systems, e.g. for tags. This paper focuses on
standard segmentation architectures for size supervision.
1.1 Overview of weakly-supervised segmentation 31
Byweakly-supervised semantic segmentation we refer to all methods that do not use full pixel- 32
precise ground truth (GT) masks for training. Such full supervision is overwhelmingly expensive for 33
segmentation and is unrealistic for many practical purposes, see the right image in Fig. 1. There are 34
many forms of weak supervision for semantic segmentation, e.g. based on partial pixel-level ground 35
truth defined by “seeds” [ 6,7], boxes [ 3], or image-level class-tags [ 2,8,9], see Fig. 1. It is also 36
common to incorporate self-supervision based on various augmentation ideas and contrastive losses 37
[10–12]. 38
Lack of supervision also motivates unsupervised loss functions such as standard old-school regulariza- 39
tion objectives for low-level segmentation or clustering. For example, many methods [ 13,14,12] use 40
variants of K-means objective (squared errors) enforcing the compactness of each class representation. 41
It is also very common to use CRF-based pairwise loss functions [ 6,7] that encourage segment shape 42
regularity and alignment to intensity contrast edges in each image [ 15]. The last point addresses the 43
well-known limitation of standard segmentation networks that often output low-resolution segments. 44
Intensity contrast edges on the high-resolution input image is a good low-level cue of an object 45
boundary and it can improve the details and localization of the semantic segments. 46
Conditional or Markov random fields (CRF or MRF) are common basic examples of pairwise 47
graphical models. The corresponding unsupervised loss functions can be formulated for continuous 48
soft-max predictions Spproduced by segmentation networks, e.g. [ 6,7,9]. Thus, it is natural to use 49
relaxations of the standard discrete CRF/MRF models, such as Potts [16] or its dense-CRF version 50
[17]. We use a bilinear relaxation of the general Potts model 51
Lcrf(S) =X
k(1−Sk)⊤WSk(3)
where S:= (Sp|p∈Ω)is a field of all pixel-level soft-max predictions Spin a given image, and 52
Sk:= (Sk
p|p∈Ω)is a vector of all pixel predictions specifically for class k. Matrix W= [wpq] 53
typically represents some given non-negative affinities wpqbetween pairs of pixels p, q∈Ω. It is 54
easy to interpret loss (3)assuming, for simplicity, that all pixels have confident one-hot predictions 55
Spso that each Skis a binary indicator vector for segment k. The loss sums all weights wpqbetween 56
the pixels in different segments. Thus, the weights are interpreted as discontinuity penalties. The loss 57
minimizes the discontinuity costs [16]. 58
In practice, affinity weights wpqare set close to 1if two neighboring pixels p, qhave similar intensities, 59
and weight wpqis set close to zero either when two pixels are far from each other on the pixel grid or 60
if they have largely different intensities [ 6,16,17]. The affinity matrix Wcould be arbitrarily dense 61
or sparse, e.g. many zeros when representing a 4-connected pixel grid. The non-zero discontinuity 62
costs between neighboring pixels are often set by a Gaussian kernel wpq= exp−∥Ip−Iq∥2
2σ2 of given 63
bandwidth σ, which works as a soft threshold for detecting high-contrast intensity edges in the image. 64
Thus, loss (3)encourages both the alignment of the segmentation boundary to contrast edges in the 65
(high-resolution) input image and the shortness/regularity of this boundary. 66
2(b)(a)(c)
Figure 2: Semantic segmentation with standard DeepLabV3+(R101) segmentation models [ 18]:
PASCAL validation results for training with (a) log-barrier (9)using class tags, (b) KL-divergence
(2) using our approximate size targets, (c) cross-entropy with full (ground truth mask) supervision.
Weakly supervised segmentation methods may also use partial pixel-level ground truth where only 67
some subset Seeds ⊂Ωof image pixels has class labels [ 6,7,9]. In this case it is common to use 68
partial cross-entropy loss 69
Lpce(S) = −X
p∈SeedslnSyp
p (4)
where ypis the ground truth label at a seed pixel p. 70
1.2 Related balancing losses 71
Segmentation and classification methods often use “balancing” losses. In the context of classification, 72
image-level predictions can be balanced over the whole training data. For segmentation problems, 73
pixel-level predictions can be balanced within each training image. Our loss is an example of size 74
balancing. Below we review some examples of related balancing loss functions used in prior work. 75
Fully supervised classification. It is common to modify the standard cross-entropy loss to account 76
for unbalanced training data where some classes are represented more than others. One common 77
example is weighted cross-entropy , e.g. defined in [19] for image-level predictions Sias 78
Lwce(S) = −X
i∈DwyilnSyi
i (5)
where class weights wk∝1
1−βvkare motivated as a re-balancing factor based on the class distribution 79
vin the training dataset Dandβis a hyper-parameter. In the fully supervised setting, the purpose 80
of re-weighting cross-entropy is not to make the predictions even closer to the known labels, but to 81
decrease over-fitting to over-represented classes, which improves the model’s generality. 82
Unsupervised classification. In the context of clustering with soft-max models [ 20,21] it is common 83
to use fairness loss encouraging equal-size clusters. In this case, there is no ground truth and 84
fairness is one of the discriminative properties enforced by the total loss in order to improve the model 85
predictions on unlabeled training data. The fairness was motivated by information-theoretic arguments 86
in [20] deriving it as a negative entropy of the data-set-level average prediction ˆS:=1
|D|P
i∈DSi 87
for dataset D 88
Lfair(ˆS) = −H(ˆS)≡X
kˆSklnˆSk
c=X
kˆSklnˆSk
1/K≡KL(ˆS∥u) (6)
where u= (1
K, . . . ,1
K)is a uniform categorical distribution, and symbolc=indicates that the equality 89
is up to some additive constant independent of ˆS. Perona et al. [ 21] pointed out the equivalent KL- 90
divergence formulation of the fairness in (6) and generalized it to a balanced partitioning constraint 91
Lbal(ˆS) = KL(ˆS∥v) (7)
3with any given prior distribution vthat could be different from uniform. 92
Semantic segmentation with image-level supervision. Most weakly-supervised semantic segmenta- 93
tion methods use losses based on segment sizes. This is particularly true for image-level supervision 94
techniques [2, 9, 22, 23]. Clearly, segments for tag classes should have positive sizes, and segments 95
for non-tag classes should have zero sizes. 96
Similarly to our paper, size-based constraints are often defined for the image-level average prediction 97
¯S, see (1), computed from pixel-level predictions Sp. Many generalized forms of pixel-prediction 98
averaging can be found in the literature, where they are often referred to as prediction pooling . Some 99
decay parameter often provides a wide spectrum of options from basic averaging to max-pooling. 100
While the specific form of pooling matters, for simplicity, we discuss the corresponding balancing 101
loss functions assuming basic average prediction ¯Sin (1). 102
One of the earliest works on tag-supervised segmentation [ 9] uses log-barriers to “expand” tag 103
objects in each training image and to “suppress” the non-tag objects. Assuming image tags T, their 104
suppression loss is defined as 105
Lsuppress (¯S)∝ −X
k̸∈Tln(1−¯Sk) (8)
encouraging each non-tag class to have zero average prediction ¯Sk, which implies zero predictions 106
Sk
pat each pixel. Their expansion loss 107
Lexpand (¯S)∝ −X
k∈Tln¯Sk. (9)
encourages positive average predictions ¯Skand non-trivial tag class segments. 108
We observe that the expansion loss (9)may have a bias to equal-size segments, as particularly evident 109
in the case of average predictions. Indeed, (9) implies 110
Lexpand (¯S)∝KL(uT∥¯S) (10)
which is a special case of our size loss (2)when the size target v=uTis a uniform distribution over 111
tag classes. The intention of the log barrier loss (9)is to push image-level size prediction ¯Sfrom 112
the boundaries of the probability simplex ∆Kcorresponding to the zero-level for the tag classes 113
T. Figure 2(a) shows the results for training based on the total loss combining CRF loss (3)with 114
the log-barrier loss (9). Its unintended bias to equal-size segments (10) is obvious. Note that the 115
mentioned decay parameter used for generalized average predictions should reduce such bias. 116
Alternatively, it may be safer to use barriers for ¯Slike 117
Lflat =−X
k∈Tln max {¯Sk, ϵ} (11)
that have flat bottoms to avoid unintended bias to some specific size target inside the probability 118
simplex ∆K. Similar thresholded barriers are common [22]. 119
1.3 Contributions 120
In general, it would be great to have effective image-level supervision for segmentation that only uses 121
barriers like (9)or(11) since they do not require any specific size targets. This corresponds to tag-only 122
supervision. However, our empirical results for semantic segmentation using such barriers were 123
poor and comparable with those in [ 9]. A number of more recent semantic segmentation methods 124
for tag-level supervision have considerably improved such results [ 12,24–30], but they introduce 125
significantly more complex multi-stage training procedures and various architectural modifications, 126
which makes such methods hard to replicate, generalize, or to understand the results. We are focused 127
on general easy-to-understand end-to-end training methods. Our main contributions are: 128
•We propose and evaluate a new general form of weak supervision, size targets. The size- 129
target supervision can be approximate and is relatively easy to get from human annotators. 130
•We propose the zero-avoiding variant of KL divergence as a general training loss, allowing 131
our end-to-end size-target approach to be integrated with any segmentation architecture. 132
•Comprehensive experiments with our size-target method demonstrate state-of-the-art perfor- 133
mance across multiple datasets using standard segmentation models typically employed for 134
full supervision, without any architectural modifications. 135
42 Size-target loss and its properties 136
Our proposed total loss is very simple 137
Ltotal :=Lsize+Lcrf (12)
where the two terms are our size-target loss (2)and standard CRF loss (3). The core new com- 138
ponent is our size-target loss based on the forward KL-divergence. Our size-target loss (2)en- 139
courages specific target volumes for tag classes. Additionally, the size-target loss suppresses 140
non-tag classes, encouraging zero volumes for classes not in the image. The CRF loss also con- 141
tributes to the suppression of redundant classes. Therefore, unlike most prior work on image- 142
level supervision for semantic segmentation, e.g. [ 9,2,12], we do not need separate suppres- 143
sion loss terms like (8). We validated this claim experimentally, they did not change the results. 144
Figure 3: Forward vsreverse KL divergence. As-
suming binary classification K= 2, we can repre-
sent all possible probability distributions as points
on the interval [0,1]. The solid curves illustrate
our “strong” size constraint, i.e. the forward KL-
divergence KL(v∥¯S)for the average prediction
¯S. We show two examples of volumetric prior
v1= (0.9,0.1)(blue curve) and v2= (0.5,0.5)
(red curve). For comparison, the dashed curves
represent reverse KL divergence KL(¯S∥v).The size-target loss can also be integrated into 145
other weakly-supervised settings, e.g. partial 146
cross-entropy loss (4)commonly used for seeds. 147
We show that using approximate size targets 148
can significantly improve the seed-supervised 149
segmentation in [ 6] when the seed lengths are 150
short, see the right plot of Fig. 4. 151
L′
total :=Lsize+Lcrf+Lpce (13)
As is well known, KL divergence is asymmetric. 152
In our work on image-level supervised segmen- 153
tation, the order of the estimated and target dis- 154
tributions is crucial. The forward KL divergence 155
possesses a zero-avoiding property, as illustrated 156
in Fig. 3. Specifically, forward KL divergence 157
imposes an infinite penalty when any class with 158
a non-zero target is predicted as zero. In con- 159
trast, the penalty of the reverse KL divergence is 160
finite and much weaker. When using reverse KL 161
divergence, segmentation models tend to gener- 162
ate trivial solutions, predicting all pixels as the 163
background class. This issue likely arises due to 164
dataset imbalance, where the background class 165
is prevalent. The zero-avoiding property of forward KL divergence ensures that segmentation models 166
do not produce trivial solutions and predict all classes in the image tag sets. 167
3 Experiments 168
3.1 Experimental settings 169
Datasets. We evaluate our approach on three segmentation datasets: PASCAL VOC 2012 [ 5], MS 170
COCO 2014 [ 31], and 2017 ACDC Challenge1[32]. The PASCAL dataset contains 21 classes. We 171
adopt the augmented training set with 10,582 images [ 33], following the common practice [ 34,9]. 172
Validation and testing contain 1449 and 1456 images. Seed supervision of the PASCAL dataset is 173
from [ 7]. COCO has 81 classes with 80K training and 40K validation images. ACDC Challenge is 174
to segment the left ventricular endocardium. The training and validation sets contain 1674 and 228 175
images. The exact size targets are extracted from the ground truth masks. 176
Approximate size targets. We train segmentation models using approximate size targets v= 177
(vk)K
k=1generated for each image either by human annotators or by corrupting the exact size targets 178
ˆv= (ˆvk)K
k=1with different levels of noise. In all cases, we report the segmentation accuracy on 179
validation data together with mean relative error (mRE) of the corresponding corrupted size targets. 180
For each training image containing class k, the relative error for the size target vkis defined as 181
RE(vk) =|vk−ˆvk|
ˆvk(14)
1https://www.creatis.insa-lyon.fr/Challenge/acdc/
5where ˆvkis the exact size. mRE averages RE over all images and all classes. For human annotated 182
size targets v= (vk)K
k=1, the relative size errors are computed directly from the definition (14). 183
When used, synthetic targets v= (vk)K
k=1are generated by corrupting the exact targets ˆv= (ˆvk)K
k=1184
vk←−(1 +ϵ)ˆvkforϵ∼ N(0, σ) (15)
where ϵis white noise with standard deviation σcontrolling the level of corruption and operator ←− 185
represents re-normalization ensuring corrupted targets (vk)K
k=1add up to one. Equation (15) defines 186
random variable vkas a function of ϵ. Thus, in this case, mRE can be analytically estimated from σ 187
mRE =E|vk−ˆvk|
ˆvk
≈E(|ϵ|) =r
2
πσ (16)
where Eis the expectation operator. The approximation in the middle uses (15) as an equality 188
ignoring re-normalization of the corrupted sizes, and the last equality is a closed-form expression for 189
themean absolute deviation (MAD) of the Normal distribution N(0, σ). 190
Evaluation metrics for segmentation. We employ mean Intersection-over-Union (mIoU) as the 191
evaluation criteria for PASCAL and COCO, and mean Dice similarity coefficient (DSC) for the 192
ACDC dataset. The quality on the PASCAL test set is assessed on the online evaluation server. 193
Implementation details. We evaluate our approach with two types of ResNet-based [ 4] and one vision 194
transformer (ViT) based [ 35] segmentation models on the PASCAL and COCO datasets. ResNet- 195
based models follow the implementation of DeepLabV3+ [ 18] using the backbone of ResNet101 196
(R101) or the backbone of WideResNet-38 (WR38) [ 1]. For brevity, we name them R101-based or 197
WR38-based DeepLabV3+ models. For the ViT-based network, We follow the implementation of 198
Segmenter [ 36], adopting its ViT-B/16 backbone and linear decoder. For experiments on the ACDC 199
datasets, we use MobileNetV2-based [ 37] DeepLabv3+ model. The R101, WR38, and MobileNetV2 200
backbones are ImageNet [ 38] pre-trained. ViT-B/16 backbone is pre-trained on ImageNet-21K [ 39] 201
and fine-tuned on ImageNet-1k [ 38]. We directly evaluate our size-target approach on top of the 202
standard architectures without any modification. 203
Images are resized to 512×512for PASCAL and COCO, and 256×256for ACDC. We employ 204
color jittering and horizontal flipping for data augmentation. Segmentation models are trained with 205
stochastic gradient descent on one RTX A6000 GPU with 48 GB GDDR6: 60 epochs for PASCAL 206
and COCO, and 200 epochs for ACDC, with a polynomial learning rate scheduler (power of 0.9). 207
Batch sizes are set to 16 for ResNet and 20 for ViT models on PASCAL, 12 on ACDC, and 12 208
(ResNet) and 16 (ViT) for MS COCO. The initial learning rate is 0.005 for ACDC and PASCAL’s 209
ResNet models, and 0.0005 for PASCAL’s ViT models. The initial learning rate on COCO is 0.0005 210
for ResNet and 0.0001 for ViT models. Loss function (12) is employed for size-target supervision. 211
Loss (13) is only used for seed supervision in Sec. 3.3. The implementation of CRF loss (3)is the 212
same as [ 6]. We use 2e−9as the weight of the CRF term following the strategy in [ 6]. Size-target 213
loss (2) and pCE (4) are used for medical images. 214
3.2 Robustness to Size Errors 215
We show the size targets can be approximate. The left plot in Fig. 4 illustrates the robustness of our 216
approach to size errors. Segmentation models are trained with synthetic size targets subjected to 217
varying levels of corruption, as defined in (15). The validation accuracy (solid red line) only drops 218
slightly when mRE (16) remains below 16%. The CRF loss (3)further enhances the robustness 219
(solid blue line). When the relative error ( mRE ) is 4%, there is a noticeable increase in validation 220
accuracy. The downward trend of the training accuracy (dashed blue line) suggests that the observed 221
increases in validation accuracy at mRE = 4% stem from improved neural network generalization. 222
3.3 Enhancing seed-based segmentation with size targets 223
Our size-target approach can be integrated with partial ground truth mask supervision (seeds). The 224
right plot in Fig. 4 demonstrates the results of seed-supervised semantic segmentation with and without 225
size-target supervision. Size targets significantly enhance performance, especially when the seed 226
lengths are short. Without size targets, segmentation performance degrades dramatically as the seed 227
length decreases. Notably, when only one pixel is labeled for each object ( seed length ratio = 0.0), 228
size-target supervision boosts accuracy from 66.6% to 74%, approaching the performance of full 229
seed supervision (seed length ratio = 1.0). 230
6Figure 4: Segmentation results on the PASCAL dataset with R101-based DeeplabV3+ networks.
The green bar in both plots indicates the segmentation accuracy for full ground truth masks (i.e. full
supervision). The left plot shows the training and validation accuracy using approximate size targets.
The segmentation is trained using losses (2)(red curve) or (12) (blue curve), where size targets are
subject to various levels of corruption (15,16). The right plot shows validation accuracy for seed
supervision of varying lengths with (blue curve) and without (red curve) using size targets. The line
styles of the blue curves differentiate among various levels of corruption.
Figure 5: Left plot shows the quality of human annotations in terms of relative errors for the dog, cat,
and bird classes within the PASCAl dataset. The histograms are normalized by the number of images
in each class. The mean relative error for the three classes is 15.9%. For comparison, the dashed line
shows the relative error distribution of synthetic size targets as defined in (15) forσ= 20.0%which
aligns with the mRE of 15.9%, see (16). The right plot presents 4-way multi-class (cat, dog, bird,
and background) segmentation accuracy using human-annotated (red star at mRE = 15.9%) and
synthetic (blue curve) size targets, employing ResNet101-based DeeplabV3+ networks. Consistent
with experiments in Sec. 3.2, synthetic size targets are generated at various levels of corruption. The
green line indicates the segmentation accuracy of full supervision using ground truth masks.
3.4 Human-annotated size targets 231
Annotation tool. In this section, our approach is evaluated with size targets annotated by humans. 232
We annotated training images for a subset of PASCAL classes, including cat, dog, and bird. A 233
user interface with an assistance tool was developed to facilitate the annotation. The assistance tool 234
overlays grid lines partitioning the image into 5×4small rectangles or 3×3large rectangles. Users 235
can determine the size of a class in an image by counting rectangles (fractions allowed) or entering 236
the percentage relative to the image size. Annotators can choose finer or coarser partitioning for each 237
image depending on the object size. We evaluate relative errors with (14) for human annotations. 238
Empirical evidence shows that annotators are approximately two times more accurate with the 239
assistance tool, especially for small objects in the image. The last two columns of Table 1 report the 240
annotation speed per image and mean relative error (14) for each class. The left plot in Fig. 5 shows 241
the histograms of relative errors for human annotations. The histograms illustrate that annotated size 242
errors are mostly below 10%, but occasional large mistakes (heavy tails) raise the mean error. 243
Segmentation with human-annotated size. Segmentation models trained with human-annotated 244
size targets show robustness to human “heavy tail” errors. We compare the accuracy for human- 245
annotated and synthetic size targets in the right plot of Fig. 5. The accuracy for human-annotated 246
size (indicated by the red star in the plot) approaches 97.2% (89.6%/92.2%) of the full supervision 247
performance, demonstrating that size-target approach is significantly robust to human errors. Binary 248
segmentation accuracy for each class is reported in the shaded cells in Table 1. The performance of 249
7supervision gt mask gt size human-annotated size
mIoU mIoU mIoU speed mRE
cat 90.6% 88.8% 88.0% 12.6s 12.3%
dog 88.1% 84.3% 84.5% 9.1s 16.6%
bird 88.8% 86.2% 86.4% 15.2s 20.1%
Table 1: Human-annotated size targets. Two columns on the right show the average speed and relative
error for each class we annotated. The shaded cells compare the accuracy of binary segmentation
models trained with ground truth masks, ground truth size, and human-annotated size.
binary segmentation models trained with human-annotated size targets is comparable to those trained 250
with precise size targets. 251
3.5 Comparison with the state-of-the-art methods 252
Our general training losses are applied to three standard architectures (R101-DeepLabV3+, WR38- 253
DeepLabV3+, and ViT-Linear) for semantic segmentation as is, without any modification. Our results 254
are highlighted in Table 2. The models are trained using synthetic size targets with an approximate 255
mean relative error (mRE) of 8%. We chose this corruption level because its performance is close 256
to human annotations, as shown in the right plot of Figure 5. Since our single-stage (end-to-end) 257
approach is completely general, it is possible to use it in specialized architectures or complex 258
training procedures. Likely, this would further improve the results, but this is not the focus of 259
our work. The rest of Table 2 shows the results for semantic segmentation methods (of different 260
complexities) for weak and full supervision. Methods are divided into multi-stage and single-stage 261
methods, grouped by their backbones. Typical single-stage methods improve their results using 262
complex architectural or training modifications such as additional training branches, extra refinement 263
modules, or specialized training strategies. However, we achieve state-of-the-art using only standard 264
segmentation architectures, commonly used in full supervision. The R101-based DeepLabV3+ model 265
trained with approximate size targets approaches 92% (71.9/78.2) of its full supervision performance 266
on PASCAL. The WR38-based DeepLabV3+ model trained with approximate size-target supervision 267
surpasses other methods employing the same backbone by approximately 10%. Using the standard 268
vision transformer architecture [ 36], the size-target approach achieves approximately 96% of the 269
Backbone DecoderArchitectural/trainingSupervisionPASCAL COCO
modification Val Test Val
Multi-stage methods
R101 DeepLabV3+ MARS [40] arXiv’23 tags 77.7 77.2 49.4
R101 DeepLabV2 MatLabel [41] ICCV’23 tags 73.0 72.7 45.6
WR38 LargeFOV MCT [42] CVPR’22 tags 71.9 71.6 42.0
WR38 LargeFOV MCTOCR [43] CVPR’23 tags 72.7 72.0 42.5
SWIN DeepLabV2 ReCAM [44] CVPR’22 tags 71.8 72.2 47.9
ViT-S “Grad-clip” WeakTr [26] arXiv’23 tags 78.4 79.0 50.3
Single-stage (end-to-end) methods
R101 DeeplabV3+ - size (8%) 71.9 72.4 45.0
R101 DeeplabV3+ - full 78.2 78.2 60.4
WR38 DeepLabV3+ SSSS [2] CVPR’20 tags 62.7 64.3 -
WR38 Conv RRM [45] AAAI’20 tags 62.6 62.9 -
WR38 DeeplabV3+ - size (8%) 72.7 72.6 -
ViT-B LargeFOV ToCo [28] CVPR’23 tags 71.1 72.2 42.3
ViT-B Conv SeCo [29] arXiv’24 tags 74.0 73.8 46.7
ViT-B LargeFOV CoSA [30] arXiv’24 tags 76.2 75.1 51.0
ViT-B Linear - size (8%) 78.1 78.2 56.3
ViT-B Linear - full 81.4 80.7 -
Table 2: Semantic segmentation results (mIoU%) on PASCAL and COCO. The supervision column
indicates a form of supervision: image-level class tags,sizetargets (our highlighted results), or full
supervision with pixel-accurate masks. The percentage after “size” is the accuracy (mRE) of our
corrupted size targets (15,16). Our approach does not require any complex architectural modification
or multi-stage training procedures needed for tag supervision, see “Modification” column.
8[22]Figure 6: Size-targets (2)vs. size-barriers (17) on the ACDC dataset. The left plot shows the accuracy
of the binary segmentation models (MobileNetV2-based DeeplabV3+) measured by DSC. The blue
curve shows size-target accuracy with various levels of corruption. The dashed green line shows the
accuracy of the size-barrier technique [ 22]. The dashed red line shows the accuracy using the mean
size target for all training images. The gray line indicates the result of full supervision. The right
image shows randomly selected qualitative results of size-barrier [ 22] and approximate size target
(mRE = 8% ). Yellow shows true positive pixels, green is false positive, and red is false negatives.
full supervision performance on the Pascal dataset. Despite its simplicity, the size-target approach 270
outperforms other complex single-stage methods on both datasets. 271
3.6 Medical data: size-target vs. size-barrier 272
Our method is also promising for medical image segmentation, benefiting from the consistency in 273
object sizes across similar medical images, which healthcare professionals can easily estimate. We 274
compare our size-target approach with the thresholded size-barrier technique [ 22], proposed for the 275
weakly supervised medical image semantic segmentation. The size-barrier loss enforces inequality 276
size constraints. Given the lower bound of each class, the thresholded size-barrier loss is 277
Lflat _sq(S) =X
k 
max{ak−¯Sk,0}2, (17)
where akis a lower bound of class k. We train binary segmentation models with a combination 278
of partial cross-entropy loss (4)and size constraint loss: size-target (2)or size-barrier (17). Seeds 279
used in the experiments are obtained using the same method provided in [ 22]. The object and 280
background barrier, aobjandabgare set based on [ 22]. In the size-barrier experiments, similarly to 281
[22], we suppress the non-tag classes, using the loss Lsup(S) = ( ¯Sobj)2. Conversely, size-target 282
loss automatically suppresses non-tag classes as discussed in Sec. 2. The left plot in Fig. 6 displays 283
the segmentation accuracy against different levels of size target corruption. Our size-target loss 284
consistently outperforms size-barrier loss, maintaining its superiority even when using highly noisy 285
size targets. The peak in the accuracy curve aligns with the experimental results in Sec. 3.2 and 286
Sec. 3.4. The accuracy of the model trained using size targets with relative errors of 8% surpasses 287
the full supervision performance. Additionally, using a fixed average size target across all training 288
images can yield performance comparable to the size-barrier method, see the dashed red line in the 289
left plot of Fig. 6. The right image in Fig. 6 shows qualitative examples of both methods. 290
4 Conclusions 291
We proposed a new image-level supervision for semantic segmentation: size targets. Such targets 292
could be approximate. In fact, our results suggest that some errors can benefit generalization. The 293
size annotation by humans requires little extra effort compared to the standard image-level tags and it 294
is much cheaper than the full pixel-accurate ground truth masks. We proposed an effective size-target 295
loss based on forward KL divergence between the soft size targets and the average prediction. In 296
combination with the standard CRF-based regularization loss, our approximate size-target supervision 297
on standard segmentation architectures (DeepLab and ViT) achieves state-of-the-art performance. 298
Our general easy-to-understand approach outperforms significantly more complex weakly-supervised 299
techniques based on model modifications and multi-stage training procedures. 300
9References 301
[1]Zifeng Wu, Chunhua Shen, and Anton van den Hengel. Wider or deeper: Revisiting the resnet 302
model for visual recognition, 2016. 303
[2]Nikita Araslanov and Stefan Roth. Single-stage semantic segmentation from image labels. In 304
Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 305
4253–4262, 2020. 306
[3]V . Kulharia, S. Chandra, A. Agrawal, P. Torr, and A. Tyagi. Box2seg: Attention weighted loss 307
and discriminative feature learning for weakly supervised segmentation. In ECCV’20 , 2020. 308
[4]Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image 309
recognition, 2015. 310
[5]Mark Everingham, Luc Van Gool, Christopher KI Williams, John Winn, and Andrew Zisserman. 311
The pascal visual object classes (voc) challenge. International journal of computer vision , 312
88:303–308, 2009. 313
[6]Meng Tang, Federico Perazzi, Abdelaziz Djelouah, Ismail Ben Ayed, Christopher Schroers, and 314
Yuri Boykov. On regularized losses for weakly-supervised cnn segmentation. In Proceedings of 315
the European Conference on Computer Vision (ECCV) , pages 507–522, 2018. 316
[7]Di Lin, Jifeng Dai, Jiaya Jia, Kaiming He, and Jian Sun. Scribblesup: Scribble-supervised 317
convolutional networks for semantic segmentation. In Proceedings of the IEEE conference on 318
computer vision and pattern recognition , pages 3159–3167, 2016. 319
[8]George Papandreou, Liang-Chieh Chen, Kevin P Murphy, and Alan L Yuille. Weakly-and 320
semi-supervised learning of a deep convolutional network for semantic image segmentation. In 321
Proceedings of the IEEE international conference on computer vision , pages 1742–1750, 2015. 322
[9]Alexander Kolesnikov and Christoph H Lampert. Seed, expand and constrain: Three principles 323
for weakly-supervised image segmentation. In Computer Vision–ECCV 2016: 14th European 324
Conference, Amsterdam, The Netherlands, October 11–14, 2016, Proceedings, Part IV 14 , pages 325
695–711. Springer, 2016. 326
[10] Xu Ji, Joao F Henriques, and Andrea Vedaldi. Invariant information clustering for unsuper- 327
vised image classification and segmentation. In Proceedings of the IEEE/CVF International 328
Conference on Computer Vision , pages 9865–9874, 2019. 329
[11] Jang Hyun Cho, Utkarsh Mall, Kavita Bala, and Bharath Hariharan. Picie: Unsupervised 330
semantic segmentation using invariance and equivariance in clustering. In Proceedings of the 331
IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 16794–16804, 2021. 332
[12] Tianfei Zhou, Meijie Zhang, Fang Zhao, and Jianwu Li. Regional semantic contrast and 333
aggregation for weakly supervised semantic segmentation. In Proceedings of the IEEE/CVF 334
Conference on Computer Vision and Pattern Recognition , pages 4299–4309, 2022. 335
[13] Mathilde Caron, Piotr Bojanowski, Armand Joulin, and Matthijs Douze. Deep clustering 336
for unsupervised learning of visual features. In Proceedings of the European conference on 337
computer vision (ECCV) , pages 132–149, 2018. 338
[14] Jyh-Jing Hwang, Stella X Yu, Jianbo Shi, Maxwell D Collins, Tien-Ju Yang, Xiao Zhang, 339
and Liang-Chieh Chen. Segsort: Segmentation by discriminative sorting of segments. In 340
Proceedings of the IEEE/CVF International Conference on Computer Vision , pages 7334–7344, 341
2019. 342
[15] Yuri Y Boykov and M-P Jolly. Interactive graph cuts for optimal boundary & region segmenta- 343
tion of objects in nd images. In Proceedings eighth IEEE international conference on computer 344
vision. ICCV 2001 , volume 1, pages 105–112. IEEE, 2001. 345
[16] Yuri Boykov, Olga Veksler, and Ramin Zabih. Fast approximate energy minimization via graph 346
cuts. IEEE Transactions on pattern analysis and machine intelligence , 23(11):1222–1239, 347
2001. 348
10[17] Philipp Krähenbühl and Vladlen Koltun. Efficient inference in fully connected CRFs with 349
Gaussian edge potentials. Advances in neural information processing systems , 24, 2011. 350
[18] Liang-Chieh Chen, Yukun Zhu, George Papandreou, Florian Schroff, and Hartwig Adam. 351
Encoder-decoder with atrous separable convolution for semantic image segmentation. In 352
Proceedings of the European conference on computer vision (ECCV) , pages 801–818, 2018. 353
[19] Yin Cui, Menglin Jia, Tsung-Yi Lin, Yang Song, and Serge Belongie. Class-balanced loss 354
based on effective number of samples. In IEEE conference on Computer Vision and Pattern 355
Recognition (CVPR) , pages 9268–9277, 2019. 356
[20] John Bridle, Anthony Heading, and David MacKay. Unsupervised classifiers, mutual informa- 357
tion and’phantom targets. Advances in neural information processing systems , 4, 1991. 358
[21] Andreas Krause, Pietro Perona, and Ryan Gomes. Discriminative clustering by regularized 359
information maximization. Advances in neural information processing systems , 23, 2010. 360
[22] Hoel Kervadec, Jose Dolz, Meng Tang, Eric Granger, Yuri Boykov, and Ismail Ben Ayed. 361
Size-constraint loss for weakly supervised CNN segmentation. In Medical Imaging with Deep 362
Learning , 2018. 363
[23] Deepak Pathak, Philipp Krahenbuhl, and Trevor Darrell. Constrained convolutional neural 364
networks for weakly supervised segmentation. In Proceedings of the IEEE international 365
conference on computer vision , pages 1796–1804, 2015. 366
[24] Jiwoon Ahn and Suha Kwak. Learning pixel-level semantic affinity with image-level supervision 367
for weakly supervised semantic segmentation. In Proceedings of the IEEE conference on 368
computer vision and pattern recognition , pages 4981–4990, 2018. 369
[25] Zhaozheng Chen and Qianru Sun. Extracting class activation maps from non-discriminative 370
features as well. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern 371
Recognition , pages 3135–3144, 2023. 372
[26] Lianghui Zhu, Yingyue Li, Jieming Fang, Yan Liu, Hao Xin, Wenyu Liu, and Xinggang Wang. 373
Weaktr: Exploring plain vision transformer for weakly-supervised semantic segmentation. arXiv 374
preprint arXiv:2304.01184 , 2023. 375
[27] Xiaobo Yang and Xiaojin Gong. Foundation model assisted weakly supervised semantic 376
segmentation. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer 377
Vision , pages 523–532, 2024. 378
[28] Lixiang Ru, Heliang Zheng, Yibing Zhan, and Bo Du. Token contrast for weakly-supervised 379
semantic segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and 380
Pattern Recognition , pages 3093–3102, 2023. 381
[29] Zhiwei Yang, Kexue Fu, Minghong Duan, Linhao Qu, Shuo Wang, and Zhijian Song. Separate 382
and conquer: Decoupling co-occurrence via decomposition and representation for weakly 383
supervised semantic segmentation. arXiv preprint arXiv:2402.18467 , 2024. 384
[30] Xinyu Yang, Hossein Rahmani, Sue Black, and Bryan M Williams. Weakly super- 385
vised co-training with swapping assignments for semantic segmentation. arXiv preprint 386
arXiv:2402.17891 , 2024. 387
[31] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr 388
Dollár, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer 389
Vision–ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, 390
Proceedings, Part V 13 , pages 740–755. Springer, 2014. 391
[32] Olivier Bernard, Alain Lalande, Clement Zotti, Frederick Cervenansky, Xin Yang, Pheng-Ann 392
Heng, Irem Cetin, Karim Lekadir, Oscar Camara, Miguel Angel Gonzalez Ballester, et al. Deep 393
learning techniques for automatic mri cardiac multi-structures segmentation and diagnosis: is 394
the problem solved? IEEE transactions on medical imaging , 37(11):2514–2525, 2018. 395
11[33] Bharath Hariharan, Pablo Arbeláez, Lubomir Bourdev, Subhransu Maji, and Jitendra Malik. 396
Semantic contours from inverse detectors. In 2011 international conference on computer vision , 397
pages 991–998. IEEE, 2011. 398
[34] Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, and Alan L Yuille. 399
Semantic image segmentation with deep convolutional nets and fully connected crfs. arXiv 400
preprint arXiv:1412.7062 , 2014. 401
[35] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, 402
Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. 403
An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint 404
arXiv:2010.11929 , 2020. 405
[36] Robin Strudel, Ricardo Garcia, Ivan Laptev, and Cordelia Schmid. Segmenter: Transformer for 406
semantic segmentation. In Proceedings of the IEEE/CVF international conference on computer 407
vision , pages 7262–7272, 2021. 408
[37] Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen. 409
Mobilenetv2: Inverted residuals and linear bottlenecks. In Proceedings of the IEEE conference 410
on computer vision and pattern recognition , pages 4510–4520, 2018. 411
[38] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng 412
Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual 413
recognition challenge. International journal of computer vision , 115:211–252, 2015. 414
[39] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large- 415
scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern 416
recognition , pages 248–255. Ieee, 2009. 417
[40] Sanghyun Jo, In-Jae Yu, and Kyungsu Kim. Mars: Model-agnostic biased object removal 418
without additional supervision for weakly-supervised semantic segmentation. arXiv preprint 419
arXiv:2304.09913 , 2023. 420
[41] Changwei Wang, Rongtao Xu, Shibiao Xu, Weiliang Meng, and Xiaopeng Zhang. Treating 421
pseudo-labels generation as image matting for weakly supervised semantic segmentation. In 422
Proceedings of the IEEE/CVF International Conference on Computer Vision , pages 755–765, 423
2023. 424
[42] Lian Xu, Wanli Ouyang, Mohammed Bennamoun, Farid Boussaid, and Dan Xu. Multi- 425
class token transformer for weakly supervised semantic segmentation. In Proceedings of the 426
IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 4310–4319, 2022. 427
[43] Zesen Cheng, Pengchong Qiao, Kehan Li, Siheng Li, Pengxu Wei, Xiangyang Ji, Li Yuan, 428
Chang Liu, and Jie Chen. Out-of-candidate rectification for weakly supervised semantic 429
segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern 430
Recognition , pages 23673–23684, 2023. 431
[44] Zhaozheng Chen, Tan Wang, Xiongwei Wu, Xian-Sheng Hua, Hanwang Zhang, and Qianru 432
Sun. Class re-activation maps for weakly-supervised semantic segmentation. In Proceedings of 433
the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 969–978, 2022. 434
[45] Bingfeng Zhang, Jimin Xiao, Yunchao Wei, Mingjie Sun, and Kaizhu Huang. Reliability does 435
matter: An end-to-end weakly supervised semantic segmentation approach. In Proceedings of 436
the AAAI Conference on Artificial Intelligence , volume 34, pages 12765–12772, 2020. 437
[46] A. Bearman, O. Russakovsky, V . Ferrari, and F. Li. Semantic segmentation with point supervi- 438
sion. In ECCV , 2015. 439
12ImageGT mask
Prediction
WR38R101ViT-BViT-BPredictionPASCALCOCOFigure 7: Segmentation examples using size-target supervision ( mRE = 8% ). Model backbones are
shown in the top-left corner of the predictions, see Table 2 for decoders.
A Appendix / supplemental material 440
A.1 Labeling costs and accuracies reported in Figure 1 441
Labelling costs. Figure 1 in the paper shows labeling speed and accuracy for different forms of 442
supervision on PASCAL VOC. The table at the bottom of Figure 1 shows ballpark estimates of 443
average labeling time per image in the whole dataset. We use the data in [ 46], as well as Table 1 in 444
the paper, and aggregate all labeling speeds from “per class”, “per instance”, or “per point” to “per 445
image” using the average number of instances or classes in each image and the aggregation rules 446
formulated in [ 46], see their Section 4. The top-left corner in each picture shows the corresponding 447
estimated labeling times for the representative multi-instance image. All the labeling times are only 448
rough estimates, but they are intuitive. The relative costs for point supervision seem underestimated, 449
but they follow evaluation conventions detailed in [46]. 450
Accuracies. The values of “point”, “size target” and “full supervision” accuracy (mIOU%) are based 451
on the experiments in the paper (Figure 4). We follow the learning rate scheme in DeepLabV3+ [ 18] 452
for the training with full supervision. For fairness, we compare these with end-to-end methods using 453
similar ResNet backbones in tag-[2] and box-supervision [ 3]. Typical SOTA methods for tag and 454
box supervision use special architectural modifications, unlike our generic size-target loss, cannot be 455
seamlessly plugged into any segmentation model. 456
A.2 Qualitative results 457
Figure 7 presents the qualitative examples of our method on PASCAL (left) and COCO (right) 458
validation sets. Despite size targets providing only image-level information, segmentation models 459
can precisely identify object locations, eliminating the need for localization methods like CAM. 460
13NeurIPS Paper Checklist 461
1.Claims 462
Question: Do the main claims made in the abstract and introduction accurately reflect the 463
paper’s contributions and scope? 464
Answer: [Yes] 465
Justification: Contributions are included in the abstract and listed in Sec. 1.3 in the introduc- 466
tion. 467
Guidelines: 468
•The answer NA means that the abstract and introduction do not include the claims 469
made in the paper. 470
•The abstract and/or introduction should clearly state the claims made, including the 471
contributions made in the paper and important assumptions and limitations. A No or 472
NA answer to this question will not be perceived well by the reviewers. 473
•The claims made should match theoretical and experimental results, and reflect how 474
much the results can be expected to generalize to other settings. 475
•It is fine to include aspirational goals as motivation as long as it is clear that these goals 476
are not attained by the paper. 477
2.Limitations 478
Question: Does the paper discuss the limitations of the work performed by the authors? 479
Answer: [No] 480
Justification: Although the limitations were not explicitly detailed in the paper, we mentioned 481
that only a subset of the PASCAL dataset was labeled due to resource constraints, see Sec. 3.4. 482
To address this, we generated approximate synthetic size targets by corrupting the exact size 483
targets. This allowed us to evaluate our method on the entire PASCAL dataset, as well as on 484
COCO and ACDC datasets. 485
Guidelines: 486
•The answer NA means that the paper has no limitation while the answer No means that 487
the paper has limitations, but those are not discussed in the paper. 488
• The authors are encouraged to create a separate "Limitations" section in their paper. 489
•The paper should point out any strong assumptions and how robust the results are to 490
violations of these assumptions (e.g., independence assumptions, noiseless settings, 491
model well-specification, asymptotic approximations only holding locally). The authors 492
should reflect on how these assumptions might be violated in practice and what the 493
implications would be. 494
•The authors should reflect on the scope of the claims made, e.g., if the approach was 495
only tested on a few datasets or with a few runs. In general, empirical results often 496
depend on implicit assumptions, which should be articulated. 497
•The authors should reflect on the factors that influence the performance of the approach. 498
For example, a facial recognition algorithm may perform poorly when image resolution 499
is low or images are taken in low lighting. Or a speech-to-text system might not be 500
used reliably to provide closed captions for online lectures because it fails to handle 501
technical jargon. 502
•The authors should discuss the computational efficiency of the proposed algorithms 503
and how they scale with dataset size. 504
•If applicable, the authors should discuss possible limitations of their approach to 505
address problems of privacy and fairness. 506
•While the authors might fear that complete honesty about limitations might be used by 507
reviewers as grounds for rejection, a worse outcome might be that reviewers discover 508
limitations that aren’t acknowledged in the paper. The authors should use their best 509
judgment and recognize that individual actions in favor of transparency play an impor- 510
tant role in developing norms that preserve the integrity of the community. Reviewers 511
will be specifically instructed to not penalize honesty concerning limitations. 512
3.Theory Assumptions and Proofs 513
14Question: For each theoretical result, does the paper provide the full set of assumptions and 514
a complete (and correct) proof? 515
Answer:[NA] 516
Justification: The paper does not include theoretical results. 517
Guidelines: 518
• The answer NA means that the paper does not include theoretical results. 519
•All the theorems, formulas, and proofs in the paper should be numbered and cross- 520
referenced. 521
•All assumptions should be clearly stated or referenced in the statement of any theorems. 522
•The proofs can either appear in the main paper or the supplemental material, but if 523
they appear in the supplemental material, the authors are encouraged to provide a short 524
proof sketch to provide intuition. 525
•Inversely, any informal proof provided in the core of the paper should be complemented 526
by formal proofs provided in appendix or supplemental material. 527
• Theorems and Lemmas that the proof relies upon should be properly referenced. 528
4.Experimental Result Reproducibility 529
Question: Does the paper fully disclose all the information needed to reproduce the main ex- 530
perimental results of the paper to the extent that it affects the main claims and/or conclusions 531
of the paper (regardless of whether the code and data are provided or not)? 532
Answer: [Yes] 533
Justification: Our size-target loss function is discussed in the 2. The experimental settings 534
are discussed in the 3.1 535
Guidelines: 536
• The answer NA means that the paper does not include experiments. 537
•If the paper includes experiments, a No answer to this question will not be perceived 538
well by the reviewers: Making the paper reproducible is important, regardless of 539
whether the code and data are provided or not. 540
•If the contribution is a dataset and/or model, the authors should describe the steps taken 541
to make their results reproducible or verifiable. 542
•Depending on the contribution, reproducibility can be accomplished in various ways. 543
For example, if the contribution is a novel architecture, describing the architecture fully 544
might suffice, or if the contribution is a specific model and empirical evaluation, it may 545
be necessary to either make it possible for others to replicate the model with the same 546
dataset, or provide access to the model. In general. releasing code and data is often 547
one good way to accomplish this, but reproducibility can also be provided via detailed 548
instructions for how to replicate the results, access to a hosted model (e.g., in the case 549
of a large language model), releasing of a model checkpoint, or other means that are 550
appropriate to the research performed. 551
•While NeurIPS does not require releasing code, the conference does require all submis- 552
sions to provide some reasonable avenue for reproducibility, which may depend on the 553
nature of the contribution. For example 554
(a)If the contribution is primarily a new algorithm, the paper should make it clear how 555
to reproduce that algorithm. 556
(b)If the contribution is primarily a new model architecture, the paper should describe 557
the architecture clearly and fully. 558
(c)If the contribution is a new model (e.g., a large language model), then there should 559
either be a way to access this model for reproducing the results or a way to reproduce 560
the model (e.g., with an open-source dataset or instructions for how to construct 561
the dataset). 562
(d)We recognize that reproducibility may be tricky in some cases, in which case 563
authors are welcome to describe the particular way they provide for reproducibility. 564
In the case of closed-source models, it may be that access to the model is limited in 565
some way (e.g., to registered users), but it should be possible for other researchers 566
to have some path to reproducing or verifying the results. 567
155.Open access to data and code 568
Question: Does the paper provide open access to the data and code, with sufficient instruc- 569
tions to faithfully reproduce the main experimental results, as described in supplemental 570
material? 571
Answer: [No] 572
Justification: To preserve anonymity, the code will be released in the final version. 573
Guidelines: 574
• The answer NA means that paper does not include experiments requiring code. 575
•Please see the NeurIPS code and data submission guidelines ( https://nips.cc/ 576
public/guides/CodeSubmissionPolicy ) for more details. 577
•While we encourage the release of code and data, we understand that this might not be 578
possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not 579
including code, unless this is central to the contribution (e.g., for a new open-source 580
benchmark). 581
•The instructions should contain the exact command and environment needed to run to 582
reproduce the results. See the NeurIPS code and data submission guidelines ( https: 583
//nips.cc/public/guides/CodeSubmissionPolicy ) for more details. 584
•The authors should provide instructions on data access and preparation, including how 585
to access the raw data, preprocessed data, intermediate data, and generated data, etc. 586
•The authors should provide scripts to reproduce all experimental results for the new 587
proposed method and baselines. If only a subset of experiments are reproducible, they 588
should state which ones are omitted from the script and why. 589
•At submission time, to preserve anonymity, the authors should release anonymized 590
versions (if applicable). 591
•Providing as much information as possible in supplemental material (appended to the 592
paper) is recommended, but including URLs to data and code is permitted. 593
6.Experimental Setting/Details 594
Question: Does the paper specify all the training and test details (e.g., data splits, hyper- 595
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the 596
results? 597
Answer: [Yes] 598
Justification: The experimental setting is detailed in the Sec. 3.1 599
Guidelines: 600
• The answer NA means that the paper does not include experiments. 601
•The experimental setting should be presented in the core of the paper to a level of detail 602
that is necessary to appreciate the results and make sense of them. 603
•The full details can be provided either with the code, in appendix, or as supplemental 604
material. 605
7.Experiment Statistical Significance 606
Question: Does the paper report error bars suitably and correctly defined or other appropriate 607
information about the statistical significance of the experiments? 608
Answer: [No] 609
Justification: Error bars are not reported because it would be too computationally expensive. 610
Our plots in Figure 4, 5, 6 are smooth enough to verify our method. 611
Guidelines: 612
• The answer NA means that the paper does not include experiments. 613
•The authors should answer "Yes" if the results are accompanied by error bars, confi- 614
dence intervals, or statistical significance tests, at least for the experiments that support 615
the main claims of the paper. 616
•The factors of variability that the error bars are capturing should be clearly stated (for 617
example, train/test split, initialization, random drawing of some parameter, or overall 618
run with given experimental conditions). 619
16•The method for calculating the error bars should be explained (closed form formula, 620
call to a library function, bootstrap, etc.) 621
• The assumptions made should be given (e.g., Normally distributed errors). 622
•It should be clear whether the error bar is the standard deviation or the standard error 623
of the mean. 624
•It is OK to report 1-sigma error bars, but one should state it. The authors should 625
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis 626
of Normality of errors is not verified. 627
•For asymmetric distributions, the authors should be careful not to show in tables or 628
figures symmetric error bars that would yield results that are out of range (e.g. negative 629
error rates). 630
•If error bars are reported in tables or plots, The authors should explain in the text how 631
they were calculated and reference the corresponding figures or tables in the text. 632
8.Experiments Compute Resources 633
Question: For each experiment, does the paper provide sufficient information on the com- 634
puter resources (type of compute workers, memory, time of execution) needed to reproduce 635
the experiments? 636
Answer: [Yes] 637
Justification: The information on the computer resources is detailed in Sec. 3.1 638
Guidelines: 639
• The answer NA means that the paper does not include experiments. 640
•The paper should indicate the type of compute workers CPU or GPU, internal cluster, 641
or cloud provider, including relevant memory and storage. 642
•The paper should provide the amount of compute required for each of the individual 643
experimental runs as well as estimate the total compute. 644
•The paper should disclose whether the full research project required more compute 645
than the experiments reported in the paper (e.g., preliminary or failed experiments that 646
didn’t make it into the paper). 647
9.Code Of Ethics 648
Question: Does the research conducted in the paper conform, in every respect, with the 649
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines ? 650
Answer: [Yes] 651
Justification: The research in the paper conforms with the code of ethics. 652
Guidelines: 653
•The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. 654
•If the authors answer No, they should explain the special circumstances that require a 655
deviation from the Code of Ethics. 656
•The authors should make sure to preserve anonymity (e.g., if there is a special consid- 657
eration due to laws or regulations in their jurisdiction). 658
10.Broader Impacts 659
Question: Does the paper discuss both potential positive societal impacts and negative 660
societal impacts of the work performed? 661
Answer: [NA] 662
Justification: Our research on weakly-supervised semantic segmentation is a purely technical 663
advancement to improve image segmentation, with no direct societal impacts or associated 664
ethical concerns. 665
Guidelines: 666
• The answer NA means that there is no societal impact of the work performed. 667
•If the authors answer NA or No, they should explain why their work has no societal 668
impact or why the paper does not address societal impact. 669
17•Examples of negative societal impacts include potential malicious or unintended uses 670
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations 671
(e.g., deployment of technologies that could make decisions that unfairly impact specific 672
groups), privacy considerations, and security considerations. 673
•The conference expects that many papers will be foundational research and not tied 674
to particular applications, let alone deployments. However, if there is a direct path to 675
any negative applications, the authors should point it out. For example, it is legitimate 676
to point out that an improvement in the quality of generative models could be used to 677
generate deepfakes for disinformation. On the other hand, it is not needed to point out 678
that a generic algorithm for optimizing neural networks could enable people to train 679
models that generate Deepfakes faster. 680
•The authors should consider possible harms that could arise when the technology is 681
being used as intended and functioning correctly, harms that could arise when the 682
technology is being used as intended but gives incorrect results, and harms following 683
from (intentional or unintentional) misuse of the technology. 684
•If there are negative societal impacts, the authors could also discuss possible mitigation 685
strategies (e.g., gated release of models, providing defenses in addition to attacks, 686
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from 687
feedback over time, improving the efficiency and accessibility of ML). 688
11.Safeguards 689
Question: Does the paper describe safeguards that have been put in place for responsible 690
release of data or models that have a high risk for misuse (e.g., pretrained language models, 691
image generators, or scraped datasets)? 692
Answer: [NA] 693
Justification: This paper poses no such risks. 694
Guidelines: 695
• The answer NA means that the paper poses no such risks. 696
•Released models that have a high risk for misuse or dual-use should be released with 697
necessary safeguards to allow for controlled use of the model, for example by requiring 698
that users adhere to usage guidelines or restrictions to access the model or implementing 699
safety filters. 700
•Datasets that have been scraped from the Internet could pose safety risks. The authors 701
should describe how they avoided releasing unsafe images. 702
•We recognize that providing effective safeguards is challenging, and many papers do 703
not require this, but we encourage authors to take this into account and make a best 704
faith effort. 705
12.Licenses for existing assets 706
Question: Are the creators or original owners of assets (e.g., code, data, models), used in 707
the paper, properly credited and are the license and terms of use explicitly mentioned and 708
properly respected? 709
Answer: [Yes] 710
Justification: The owners of assets used in this paper are credited and the license is mentioned 711
and respected. 712
Guidelines: 713
• The answer NA means that the paper does not use existing assets. 714
• The authors should cite the original paper that produced the code package or dataset. 715
•The authors should state which version of the asset is used and, if possible, include a 716
URL. 717
• The name of the license (e.g., CC-BY 4.0) should be included for each asset. 718
•For scraped data from a particular source (e.g., website), the copyright and terms of 719
service of that source should be provided. 720
18•If assets are released, the license, copyright information, and terms of use in the 721
package should be provided. For popular datasets, paperswithcode.com/datasets 722
has curated licenses for some datasets. Their licensing guide can help determine the 723
license of a dataset. 724
•For existing datasets that are re-packaged, both the original license and the license of 725
the derived asset (if it has changed) should be provided. 726
•If this information is not available online, the authors are encouraged to reach out to 727
the asset’s creators. 728
13.New Assets 729
Question: Are new assets introduced in the paper well documented and is the documentation 730
provided alongside the assets? 731
Answer: [NA] 732
Justification: The paper does not release new assets. 733
Guidelines: 734
• The answer NA means that the paper does not release new assets. 735
•Researchers should communicate the details of the dataset/code/model as part of their 736
submissions via structured templates. This includes details about training, license, 737
limitations, etc. 738
•The paper should discuss whether and how consent was obtained from people whose 739
asset is used. 740
•At submission time, remember to anonymize your assets (if applicable). You can either 741
create an anonymized URL or include an anonymized zip file. 742
14.Crowdsourcing and Research with Human Subjects 743
Question: For crowdsourcing experiments and research with human subjects, does the paper 744
include the full text of instructions given to participants and screenshots, if applicable, as 745
well as details about compensation (if any)? 746
Answer: [NA] 747
Justification: The paper does not involve crowdsourcing or research with human subjects. 748
Guidelines: 749
•The answer NA means that the paper does not involve crowdsourcing nor research with 750
human subjects. 751
•Including this information in the supplemental material is fine, but if the main contribu- 752
tion of the paper involves human subjects, then as much detail as possible should be 753
included in the main paper. 754
•According to the NeurIPS Code of Ethics, workers involved in data collection, curation, 755
or other labor should be paid at least the minimum wage in the country of the data 756
collector. 757
15.Institutional Review Board (IRB) Approvals or Equivalent for Research with Human 758
Subjects 759
Question: Does the paper describe potential risks incurred by study participants, whether 760
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) 761
approvals (or an equivalent approval/review based on the requirements of your country or 762
institution) were obtained? 763
Answer: [NA] 764
Justification: The paper does not involve crowdsourcing or research with human subjects. 765
Guidelines: 766
•The answer NA means that the paper does not involve crowdsourcing nor research with 767
human subjects. 768
•Depending on the country in which research is conducted, IRB approval (or equivalent) 769
may be required for any human subjects research. If you obtained IRB approval, you 770
should clearly state this in the paper. 771
19•We recognize that the procedures for this may vary significantly between institutions 772
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the 773
guidelines for their institution. 774
•For initial submissions, do not include any information that would break anonymity (if 775
applicable), such as the institution conducting the review. 776
20