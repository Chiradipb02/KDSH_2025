Class Concept Representation from Contextual Texts
for Training-Free Multi-Label Recognition
Anonymous Author(s)
Affiliation
Address
email
Abstract
The power of large vision-language models (VLMs) has been demonstrated for 1
diverse vision tasks including multi-label recognition with training-free approach or 2
prompt tuning by measuring the cosine similarity between the text features related 3
to class names and the visual features of images. Prior works usually formed the 4
class-related text features by averaging simple hand-crafted text prompts with class 5
names (e.g., “a photo of {class name}” ). However, they may not fully exploit the 6
capability of VLMs considering how humans form the concepts on words using rich 7
contexts with the patterns of co-occurrence with other words. Inspired by that, we 8
propose class concept representation for zero-shot multi-label recognition to better 9
exploit rich contexts in the massive descriptions on images (e.g., captions from MS- 10
COCO) using large VLMs. Then, for better aligning visual features of VLMs to our 11
class concept representation, we propose context-guided visual representation that 12
is in the same linear space as class concept representation. Experimental results 13
on diverse benchmarks show that our proposed methods substantially improved 14
the performance of zero-shot methods like Zero-Shot CLIP and yielded better 15
performance than zero-shot prompt tunings that require additional training like 16
TaI-DPT. In addition, our proposed methods can synergetically work with existing 17
prompt tuning methods, consistently improving the performance of DualCoOp and 18
TaI-DPT in a training-free manner with negligible increase in inference time. 19
1 Introduction 20
The goal of multi-label image recognition is to assign all semantic labels (or class names) within an 21
image [ 10,44,48,11,27,33,31]. Differing from single-label recognition, multi-label recognition 22
addresses a broader range of practical applications such as image retrieval [ 36,39], recommendation 23
systems [ 52,8], medical diagnosis recognition [ 43] and retail checkout recognition [ 17,45]. However, 24
one of the challenges in multi-label recognition is the difficulty of collecting full label annotations, 25
which is laborious and prone to missing. To alleviate it, recent works have investigated training with 26
incomplete labels such as partial labels [37, 6, 31, 15, 9] or a single positive label [13, 46]. 27
Recent advances of large vision-language models (VLMs) [ 32,2,22,25,47,49] has demon- 28
strated their strong transferability on various downstream tasks with great performance. Contrastive 29
Language-Image Pretraining (CLIP) achieved impressive performance in zero-shot classification by 30
measuring the cosine similarity between images and class-related hand-crafted text prompts [ 32]. 31
Fine-tuning VLMs for adapting desired downstream datasets [ 32] can further improve performance 32
for targeted tasks, but tuning millions of parameters is usually undesirable due to computation burden 33
and possible forgetting. Prompt tuning has been investigated as an efficient and low-cost training 34
paradigm [ 54,53], learning only a few context tokens of VLMs for a given task. In multi-label 35
recognition, prompt tuning with CLIP has been investigated for distinguishing multiple objects in an 36
Submitted to 38th Conference on Neural Information Processing Systems (NeurIPS 2024). Do not distribute.Figure 1: Illustration of our methods applied to zero-shot CLIP (ZSCLIP) [ 32]. (a→b) Class concept
is formed from the text descriptions that contain rich contextual information with relevant class names
and other related words, yielding substantially improved performance without aligning with visual
features yet. (b →c) Context-guided visual feature is transformed from visual feature so that it is in
the same linear space as class concept representation, yielding significantly improved performance.
image [ 37,18,41], mitigating the difficulty of acquiring annotated samples. However, prompt tuning 37
inherently requires labeled data with additional training and may be susceptible to overfitting for 38
context tokens, hindering generalization. The capability of VLMs for label-free and/or training-free 39
classification has been exploited using prompt engineering [32, 34, 50, 4]. However, prompt ensem- 40
bles by averaging text features from simple hand-crafted prompts ( e.g.,“a sketch of {class name}” ) 41
yielded marginal improvements and struggled with multi-label recognition. Thus, the approach of 42
prior works on zero-shot or prompt-tuning based multi-label recognition using class names to obtain 43
class-related text features from VLMs may not use the full capacity of VLMs properly. 44
Humans form concepts on words from past experience, especially using their patterns of co-occurrence 45
with other words [5,29,20]. Inspired by this perspective in cognitive neuroscience, we propose a 46
novel approach of exploiting VLMs for multi-label recognition by replacing single class name -related 47
hand-crafted prompts with our proposed class concept representation using text descriptions such 48
as “A person holding a large pair of scissors ,” capturing rich contextual information with target 49
class names (e.g., person) as well as related words (e.g., holding, scissors). Our class concept will 50
be constructed from rich contextual descriptions on classes that may contain diverse and realistic 51
patterns of co-occurrence with target class name and other related class names. Then, this novel text 52
features with class concept representation requires aligned visual features with them for multi-label 53
recognition to properly match them with our class concepts. Thus, we propose context-guided visual 54
features to bring VLM’s visual features to the same representation domain as our class concept 55
representation by using our sequential attention. See Fig. 1 for the differences of performing multi- 56
label recognition using (a) prior zero-shot approach (ZS-CLIP), (b) our proposed class concepts from 57
text descriptions and (c) our proposed context-guided visual features on the same space as the class 58
concepts. We demonstrated that our proposed methods achieved improved performance on multiple 59
benchmark datasets without additional training (tuning), without additional labels (text-image pairs) 60
and with negligible increase in inference time. Here is the summary of the contributions: 61
•Proposing a novel class concept representation for training-free multi-label recognition tasks 62
using VLMs from massive text descriptions inspired by how human forms concept on words. 63
•Proposing a context-guided visual feature, transformed onto the same text feature space as 64
class concepts using sequential attention for better aligning multi-modal features. 65
•Demonstrating that our methods synergetically improve the performance of ZSCLIP and 66
other state-of-the-art prompt tuning methods with a negligible increase in inference time. 67
2 Related Works 68
Multi-label image recognition with CLIP. Multi-Label Recognition (MLR) aims to identify all 69
semantic labels within an image. However, it is difficult to collect the annotation of multi-label images 70
which involve complex scenes and diverse objects. Recently, prompt tuning with the pre-trained vision- 71
2language model CLIP has been developed to address the high labeling costs of multi-label images in 72
incomplete label setting. Among them, DualCoOp [ 37] proposed a novel prompt tuning approach 73
that trains positive and negative learnable contexts with class names in the partially labeled setting. 74
For mitigating data-limited or label-limited issues, TaI-DPT [ 18] proposed effective dual-grained 75
prompt tuning method using easily accessible text descriptions. It is worth noting that TaI-DPT 76
used the same text descriptions as ours not for performing training-free multi-label recognition 77
itself, but for label-free prompt tuning by replacing the image features with the contextual text 78
features (text as image) under the conventional framework of multi-label recognition with class 79
name. SCPNet [ 14] is designed to leverage the structured semantic prior from CLIP to complement 80
deficiency of label supervision for MLR with incomplete labels. CDUL [ 1] proposed unsupervised 81
multi-label recognition through pseudo-labeling using CLIP, alleviating the annotation burden. Even 82
though recent works has demonstrated outstanding performance of multi-label recognition task, they 83
still require tuning costs or labeled dataset to adapt pre-trained CLIP to various downstream tasks. In 84
this work, our method enables training-free and label-free adaptation of CLIP into downstream tasks, 85
utilizing the text descriptions. 86
Training-free enhancement with CLIP. For single-label recognition, recent works has developed 87
the training-free enhancement of CLIP. ZPE [ 4] weighted-averaged many prompts by automatically 88
scoring the importance of each prompt in zero-shot manner for improving prompt ensemble technique. 89
CALIP [ 19] designed a simple parameter-free attention module for zero-shot enhancement over CLIP 90
without any tuning of model parameter. With few-shot samples, Tip-Adapter [ 51] proposed training- 91
free approach for fast adaptation to target task, obtaining the weights of adapter using few-shot 92
samples during inference. Since these methods were originally developed for single-label recognition, 93
it is difficult to be directly applied to multi-label recognition. In multi-label recognition, our method 94
enables training-free enhancement and demonstrated its effectiveness on the benchmark dataset. 95
3 Method 96
First of all, we propose class concept representation as a training-free approach for multi-label 97
recognition instead of class name by exploiting pre-trained VLM and rich contextual text descriptions. 98
Secondly, we also propose context-guided visual feature that can enhance the alignment of the 99
visual feature of VLM with our novel class concept. Our proposed methods are label-free as well as 100
training-free so that they can be applicable synergetically for most existing VLM-based multi-label 101
recognition methods. The overall pipeline of our method is illustrated in Figure 2. 102
3.1 Class Concept Representation 103
Humans form concepts on words from past experience, often using their patterns of co-occurrence 104
with other words [5,29,20]. For example, the word “apple” does not exist alone, but often comes 105
with the verb “eat” or the noun “basket.” However, it may not well associate with other words such 106
as “fly” or “space.” Fortunately, we can easily obtain rich contextual text descriptions from various 107
public sources, including captions from benchmark datasets [ 26,23,24,30], web crawling and large 108
language models [ 38,7,40,28]. These text descriptions do not only contain class names , but also 109
include other words like class-related verbs and nouns in real-world contexts. 110
Assume that rich contextual text descriptions were gathered from the public sources that include one 111
or multiple class names. We denote the set of text descriptions as Zall={z1, z2, ..., z M}where zi 112
refers to an individual text description. Mdenotes the total number of text descriptions across all 113
classes. Note that Mcan be dynamically changed at inference since our proposed method does not 114
require additional training, thus can be seen as test-time adaptation. Assuming that the target task 115
uses the class names of person, scissors, clock, building and cake, the examples of the contextual text 116
descriptions from Zallare as follows: 117
“Aperson holding a large pair of scissors .” 118
“Aclock mounted on top of a building in the city.” 119
“Half of a white cake with coconuts on top.” 120
TaI-DPT [ 18] used these descriptions with rich contextual information as a surrogate for images 121
to propose a label-free prompt tuning. In this work, we propose to use these descriptions to form 122
concepts on class names to compare with images, so that ways of using them are completely different. 123
3Figure 2: (a) Overall pipeline of our method. 1) Class concept representation: VLM’s text features
from the rich contextual descriptions associated with each class name are used to construct the class
concept. 2) Context-guided visual features: VLM’s visual features are sequentially transformed onto
the class concept representation space using (b) sequential attention mechanism.
We define the class concept as a vector in the space constructed by the text descriptions as fol- 124
lows. Firstly, the linear space Zcan be constructed by spanning the VLM’s text features from 125
all text descriptions ziinZallusing the VLM’s text encoder Etxt(zi)∈R1×D, leading to 126
Z=span{Etxt(z1),Etxt(z2), . . . ,Etxt(zM)}. Secondly, we propose the class concept for a target 127
class name cas a vector tconcept
c in the space Zby defining it as follows: 128
tconcept
c =MX
i=1wc,i 1c(zi)Etxt(zi)∈R1×D(1)
where 1c(zi)an indicator function such that 1c(zi) = 1 if the text description zicontains the class 129
name cand 1c(zi) = 0 otherwise. The weight wc,iis assigned to the text feature of each text 130
description within a class cand it is assumed to be normalized within the class. In this work, we set 131
wc,i= 1/PM
j1c(zj)for∀i, thus will be the same for all ifor each class, which was guided by the 132
prior work on prompt ensembling [ 4], demonstrated that the prompt ensembling with equal weights 133
achieved significant performance gains that were comparable to weighted ensembling for single-label 134
recognition. Each class concept can be stored individually or together as a matrix. 135
Our class concept representation thus consists of various text features including diverse contextual 136
information related to the target class name. For instance, the descriptions for the class name “dog” 137
should contain the target class name as the following examples of the text descriptions: 138
“Adoggreets a sheep that is in a sheep pen.” 139
“A woman walks herdogon a city sidewalk .” 140
“Adogwith goggles is in a motorcycle side car .” 141
Note that the descriptions include the target class name (bold) as well as other related words in class- 142
related contexts (underline) as intended. We expect that our novel class concepts will be beneficial for 143
multi-label recognition due to other nouns (other class names) as well as other verbs to better explain 144
the context where the target class name is used. In this work, we obtain the texts from two sources to 145
collect the sufficient contextual text descriptions. The first source is the MS-COCO dataset [ 26] that is 146
publicly available and the second source is large language model( i.e., GPT-3.5[ 28]) that can generate 147
the several sentences quickly if the set of class names related to the target task were provided. 148
3.2 Context-Guided Visual Feature 149
Our novel class concept representation forms new vectors for diverse class names in the linear space 150
Zinstead of the embedding space of the VLM where the text and image encoders were relatively 151
well-aligned. Thus, it is expected that the class concept representation and the VLM’s visual feature 152
4Figure 3: Softmax values can be used to weigh the relevance with the given image. However, (a) naive
attention mechanisms yielded almost equal softmax values, thus may include texts with low relevance.
The proposed sequential attention method focuses on a subset of texts most relevant to the test image,
thus can transforms visual features to context-guided visual features for multi-label recognition by
assigning very high softmax value to the relevant text at index 0 while very low softmax value to the
irrelevant text at index 5000.
may not be aligned well. Here, we propose context-guided visual feature by transforming the visual 153
features of the VLM onto the same space as the class concept representation Zby using our sequential 154
attention with the text descriptions Zallthat were used for class concept construction. 155
For the target image qand the VLM’s visual encoder Eimg(q), the L2-normalized global visual feature 156
fis obtained by using Eimg(q)∈R1×Dand the flatten local visual feature F∈RHW×Dis also 157
constructed by using Eimg(Pi,j(q))where Pi,j(·)is an extractor of the (i, j)th patch of the input image. 158
Then, we aim to transform both the global visual feature vector fand the local visual feature matrix F 159
onto the same linear space Zas our class concept representation. One easy way is to “project” these 160
visual features fandFonto the space Zby computing the cosine similarity between visual features 161
(fand the column vectors of F) and all the text features ti=Etxt(zi)∈R1×D, i= 1, . . . , M 162
that constructed Z. Unfortunately, when the softmax function is applied to the cosine similarity 163
values, they tend to become similar, thus weigh both relevant and irrelevant texts almost equally 164
as illustrated in Figure 3 (a). To address this challenge, we propose sequential attention, applying 165
the softmax function to part of the cosine similarity values by dividing them into Ggroups. For the 166
text feature matrix T= [t1t2···tM]∈RM×D, let us determine Mifori= 1, . . . , G such that 167
M= ΠG
i=1Miand reshape the text feature matrix to be T∈RM1×···× MG×D. Then, propose to 168
sequentially apply the following attention process for Giterations for estimating both global and 169
local context-guided visual features v(k)andV(k), respectively, at the kth iteration: 170
v(k)=(
T ifk= 0,
Softmax dimk
f(v(k−1))t
αf
v(k−1)ifk >0,(2)
171
V(k)=(
T ifk= 0,
Softmax dimk(F(V(k−1))t
αF)V(k−1)ifk >0,(3)
where αfandαFdenote the modulation parameters, Softmax Mkrefers to the softmax operation 172
applied along the dimension corresponding to Mk. In this work, we utilize v(3)andV(3)to compute 173
classification score. The sequential attention process is illustrated in Figure 2 (b). Figure 3 further 174
demonstrates that our sequential attention is particularly effective in handling massive text descriptions. 175
Without sequential attention, weighted averaging essentially becomes equal averaging. 176
3.3 Multi-Label Recognition with Class Concepts 177
Architecture of model. Two encoders of CLIP are denoted as EimgandEtxtfor the visual encoder 178
and text encoder, respectively. Following TaI-DPT [ 18], we adopt the structure of double-grained 179
prompts (DPT), which has been shown effective for enhancing zero-shot multi-label recognition 180
performance. To obtain visual representations at both coarse-grained and fine-grained levels, we 181
5extract the local visual feature map F=Eimg(x)∈RHW×Dis extracted before attention pooling 182
layer, where HandWare spatial dimension of visual feature. After attention pooling layer, we 183
obtain the global visual feature f∈R1×D. Similarly, text features t=Etxt(z)∈R1×Dare obtained 184
by projecting the End-of-Sentence (EOS) token of the text prompt. Thus, we leverage both global 185
and local visual features for multi-label recognition. 186
Inference. Through our sequential attention, we obtain the context-guided visual features v(G)and 187
V(G)at both global and local levels, respectively. The similarity score SgloandSlocare calculated 188
between the transformed context-guided visual features v(G),V(G)and the class concepts tconcept
c189
using the cosine similarity Ψ(·,·) as follows: 190
Stot
c=Sglo
c+Sloc
c= Ψ( v(G), tconcept
c ) +PHW
j=1Softmax (sloc
c,j)·sloc
c,j (4)
where Stot
cis the classification score for the class candsloc
c,j= Ψ([ V(G)]j, tconcept
c )for the class c. 191
For obtaining Sloc
c, we employ the spatial aggregation over HW [37]. 192
Finally, we combined ZSCLIP[ 32] and other prompt tuning methods with our training-free approach 193
through simple logit ensemble. In our experiments, we demonstrate the effectiveness of integrating of 194
our method with existing methods, thereby boosting the performance of multi-label recognition. 195
4 Experiments 196
4.1 Implementation Details 197
Architecture. We empoly CLIP ResNet-50 in the Table. 2 and Table. 3 and ResNet-101 in other 198
experiments as the visual encoders and the CLIP transformer as the text encoder for ZSCLIP[ 32], 199
TaI-DPT [ 18], DualCoOP [ 37] and our method in the paper. In addition, ZSCLIP[ 32], TaI-DPT [ 18] 200
and our method are based on the double-grained prompt [18] for both global and local features1. 201
Datasets. For evaluation, we performed multi-label recognition experiments on 3 benchmark datasets. 202
MS-COCO [ 26] consists of 80 classes with 82,081 images for training and 40,504 images for test. 203
VOC2007[ 16] consists of 20 object classes with 5,011 image for training and 4,952 images for test. 204
NUS-WIDE[ 12] consists of 81 concepts with 161,789 image for training and 107,859 image for 205
test. For MS-COCO [ 26] and VOC2007 [ 26], text description source is from MS-COCO [ 26]. For 206
NUS-WIDE[ 12], we gathered the text descriptions from GPT-3.5. Note that there is example of text 207
template for extracting sentence from GPT-3.5 in supplementary. 208
Inference Details. In the paper, we set the total number of text descriptions, denoted as M, for 209
the MSCOCO[ 26], VOC2007[ 16], and NUS-WIDE[ 12] at 40,000, 64,000, and 57,600, respectively. 210
Note that we prepared the text embeddings of every text descriptions from CLIP text encoder in 211
advance. We set values of modulation parameter αvia validation. 212
4.2 Evaluation on Limited Data Setting 213
To evaluate our method, we conducted the experiments in limited data scenarios, including zero-shot 214
and few-shot settings for data-limited cases and partially labeled setting for label-limited cases. Note 215
that only our method provides training-free enhancement of CLIP without tuining cost for multi-label 216
recognition. Therefore, our method can be easily combined with existing methods to improve their 217
performance. 218
Evaluation on Zero-Shot Setting. We performed comparison studies for different zero-shot and fully 219
supervised methods in multi-label image recognition. To evaluate the effectiveness of our method 220
which, we combined our method with existing zero-shot methods, ZSCLIP[ 32] and TaI-DPT [ 18], 221
for zero-shot setting, as shown in Table 1. Additionally, we utilized the fully supervised method, 222
DualCoOp[37] with our method, for zero-shot learning setting (ZSL) as presented in Table 2. 223
Table 1 summarizes the results of the zero-shot experiment on benchmark datasets. In MS-COCO [ 26] 224
and VOC2007 [ 16], TaI-DPT [ 18] and our method utilized the public language data from MS- 225
COCO [ 26]. By applying our method to ZSCLIP[ 32] and TaI-DPT [ 18] during inference, we yield 226
performance improvements without tuning costs. Especially, the performance of ZSCLIP[ 32] with 227
1https://github.com/guozix/TaI-DPT
6Table 1: Multi-label recognition with zero-shot methods on MS-COCO [ 26], VOC2007 [ 16] and
NUS-WIDE [ 12]. Without training, our method significantly enhances the performance of existing
zero-shot methods. The evaluation is based on mAP.
Training-free Methods MS-COCO[26] VOC2007[16] NUS-WIDE[12]
✓ ZSCLIP[32] 57.4 82.8 37.3
✓ +Ours 70.0 (+12.6) 89.2 (+6.4) 46.6 (+9.3)
✗ TaI-DPT[18] 68.0 88.9 46.5
✓ +Ours 70.9 (+2.9) 90.1 (+1.2) 49.1 (+2.6)
Table 2: Multi-label recognition with 17 unseen classes on MS-COCO [ 26]. In zero-shot learning
(ZSL , recognizing only unseen classes) and generalized ZSL (GZSL, recognizing both seen and
unseen classes), our method effectively supplements the complementary information of unseen classes
to the supervised DualCoOp[37] on 48 seen classes. The evaluation is based on mAP.
MethodsResNet-50 ResNet-101
ZSL GZSL ZSL GZSL
DualCoOp[37] 78.2 70.2 82.9 74.9
+Ours 82.9 (+4.7) 73.2 (+3.0) 87.6 (+4.7) 78.0 (+3.1)
our method is notably enhanced, achieving better and comparable performance to TaI-DPT [ 18], 228
which requires mild tuning. In NUSWIDE [ 12], we incorporate contextual text descriptions from 229
a large language model (GPT-3.5) to validate the potential of utilizing generated texts instead of 230
well-curated caption data. With provided class name of NUSWIDE [ 12], we readily gathered the 231
massive set of text descriptions within a short amount of time. TaI-DPT [ 18] is trained with the 232
public caption data from OpenImages[ 23]. Our method exceeds the performance of ZSCLIP[ 32] and 233
TaI-DPT [18] by a large margin, with improvements of 9.3 mAP and 2.6 mAP, respectively. 234
Table 2 shows the results of the zero-shot learning setting for unseen classes. In MS-COCO [ 26], 235
we follow the DualCoOp[ 37] and split the dataset into 48 seen classes and 17 unseen classes. 236
The evaluation is conducted in both zero-shot setting (ZSL, recognizing only unseen classes) and 237
generalized zero-shot setting (GZSL, recognizing both seen and unseen classes). Based on prompt 238
tuning, DualCoOp[ 37] trains learnable context tokens on 48 seen classes and achieves the state-of-the- 239
art performance on both ZSL and GZSL. Our method was originally designed to handle novel classes 240
(unseen classes) by leveraging text descriptions. As a result, our method significantly improved 241
the ZSL and GZSL performance of the supervised DualCoOp[ 37] by providing complementary 242
information. Table 1 and Table 2 demonstrate the effectiveness of our method performing training- 243
free enhancement of CLIP with only text descriptions that are easily obtained. 244
Evaluation on Few-Shot Setting. We performed comparison study with few-shot methods in multi- 245
label recognition. In TaI-DPT [ 18], they have investigate to confirm the effectiveness of their zero-shot 246
method. Here, we further validate our method, which is zero-shot test-time task adaption without 247
tuning costs. 248
Table 3 summarizes the results of the few-shot methods on MS-COCO dataset [ 26], especially using 249
1 and 5 shot samples for all classes. While existing few-shot methods [ 3,35,54,51] demonstrated 250
the performance enhancements with an increase of labeled samples, TaI-DPT [ 18] and our method 251
are performed within the zero-shot setting. By applying our method with existing zero-shot methods 252
(ZSCLIP[ 32] and TaI-DPT [ 18]), we consistently enhance performance, as already demonstrated in a 253
zero-shot setting. In the absence of labeled samples and tuning, we achieved comparable performance 254
with ML-FSL[35] and better results than other few-shot methods utilizing 5-shot samples. 255
Evaluation on Partially Labeled Setting. Due to high costs of annotation in multi-label image 256
recognition, training with partially labeled samples [ 37,21,31,6] has been studied. Following 257
DualCoOp [ 37], we performed the evaluation of partially labeled setting. As shown in Table 4, 258
our method supplements the decreased performance of DualCoOp [ 37] caused by partially labeled 259
samples by providing complementary information during inference. Through zero-shot test time task 260
adaptation without tuning costs, we consistently enhance the the performance of DualCoOp [ 37] on 261
7Table 3: Comparison with few-shot methods on MS-COCO [ 26]. The evaluation is based on mAP
with 16 novel classes. For each shot, we highlighted the best performance in bold.
Training-free Methods 0-shot 1-shot 5-shot
✗ LaSO[3] - 45.3 58.1
✗ ML-FSL[35] - 54.4 63.6
✗ CoOp[54] - 46.9 55.6
✓ Tip-Adapter[51] - 53.8 59.7
✓ ZSCLIP[32] 49.7 - -
✓ +Ours 58.5 (+8.8) - -
✗ TaI-DPT[18] 59.2 - -
✓ +Ours 61.4 (+2.2) - -
Table 4: Performance of multi-label recognition based on the partially labeled dataset [ 26,16,
12]. Without training and labeled samples, our method consistently enhanced the performance of
supervised DualCoOp [ 37] over all partial label ratio. DualCoOp [ 37] is reproduced and the evaluation
is based on mAP.
Datasets Method Partial label
10% 20% 30% 40% 50% 60% 70% 80% 90% Avg.
MS-COCO SARB[31] 71.2 75.0 77.1 78.3 79.6 79.6 80.5 80.5 80.5 77.9
DualCoOp[37] 80.8 82.2 82.8 83.0 83.5 83.8 83.9 84.1 84.2 82.7
DualCoOp[37]+Ours 81.5 82.8 83.3 83.5 84.0 84.2 84.4 84.5 84.6 83.6
VOC2007 SARB[31] 83.5 88.6 90.7 91.4 91.9 92.2 92.6 92.8 92.9 90.7
DualCoOp[37] 91.6 93.3 93.7 94.3 94.5 94.7 94.8 94.9 94.8 94.0
DualCoOp[37]+Ours 92.5 93.9 94.3 94.7 94.9 95.0 95.1 95.2 95.1 94.5
NUS-WIDE DualCoOp[37] 54.0 56.1 56.9 57.4 57.9 57.8 58.0 58.4 58.8 57.3
DualCoOp[37]+Ours 55.0 56.9 57.7 58.2 58.6 58.6 58.8 59.2 59.5 58.1
all benchmark dataset. Furthermore, we achieved the performance of DualCoOp [ 37] trained with 262
90% labels by applying our method with DualCoOp trained with 60% labels from MS-COCO [ 26], 263
50% labels from VOC2007 [16], and 70% labels from NUSWIDE [12]. 264
4.3 Ablation Study and Analysis 265
4.3.1 Effectiveness of our method 266
To verify the effectiveness of components of our method, we conducted an ablation study for analyzing 267
our method. As shown in Table 5, we first proposed a novel class concept representation with text 268
descriptions by class to ZSCLIP[ 32]. Since the text descriptions contain the semantic meaning among 269
multiple class names and contextual information for multi-label recognition, the alignment between 270
visual features of test image and text features are improved compared to the hand-crafted prompts as 271
shown in the Fig.1. Thus, the performance is increased by 4.1 mAP and 1.1 mAP on MS-COCO [ 26] 272
and VOC2007 [ 16], respectively. Then, we performed the context-guided visual feature using a large 273
set of text descriptions, Zall. Transforming the visual features into same text feature space as our class 274
concept representation is essential to minimize the gap between visual feature from task-agnostic 275
visual encoder and text features for each class. Constructing context-guided visual feature, our method 276
yield remarkable performance gain by 8.5 mAP and 5.3 mAP on MS-COCO [ 26] and VOC2007 [ 16], 277
respectively. Thus, we effectively designed our method that improves the alignment between visual 278
and text features. 279
4.3.2 The Number of Text Descriptions 280
We investigate the effect of the number of text descriptions for our method. As shown in Table 6, 281
we evaluated performance by increasing the number of randomly selected text descriptions from 1K 282
to 32K texts. With only 1K text descriptions, our method enhances performance by approximately 283
8Table 5: Effectiveness of our method on MS-COCO [ 26] and VOC2007 [ 16]. Each component of
our method consistently improves performance, with significant enhancements achieved particularly
in context-guided visual feature through narrowing the gap between visual and text features. The
evaluation is based on mAP.
Method MS-COCO [26] VOC2007 [16]
Baseline (ZSCLIP[32]) 57.4 82.8
+Class concept representation 61.5(+4.1) 83.9(+1.1)
+Context-guided visual feature 70.0(+8.5) 89.2(+5.3)
Table 6: Ablation studies in terms of the number of the text descriptions. As increasing the number of
texts, we measured the performance of ZSCLIP[ 32] with our method in mAP on MS-COCO [ 26] and
VOC2007 [ 16]. Note that ZSCLIP[ 32] achieves 57.4 mAP and 82.8 mAP for MS-COCO [ 26] and
VOC2007 [16], respectively.
DatasetNumber of text descriptions
1K 2K 4K 8K 16K 32K
MS-COCO [26] 65.8 68.4 68.5 69.1 69.6 69.9
VOC2007 [16] 88.1 88.5 88.8 88.9 89.0 89.1
8 mAP on MS-COCO [ 26] and 5 mAP on VOC2007 [ 16], respectively. As the number of text 284
descriptions ranges from 1K to 32K, the text embeddings of Zallcan cover the wider range of test 285
dataset, resulting in increased performance gains. For adapting to novel classes during inference, our 286
method not only achieves a significant performance improvement with only 1K texts but also further 287
enhances performance as the quantity of texts increases. 288
4.3.3 Analysis of Inference Time 289
We analyzed the inference time of our method depending on the number of text descriptions. When 290
extracting text embeddings from the text descriptions in advance, we measure the inference time 291
as the number of text descriptions increases. ZSCLIP[ 32], as the baseline model, processes each 292
sample for classification in 7.2ms. When the number of texts increases from 1K to 32K, integrating 293
ZSCLIP[ 32] with our method only increases the inference time by 0.4-0.5ms, with tests conducted on 294
the RTX3090. In addition, Our method (6.8GB) requires slightly more memory than ZSCLIP (6.5GB) 295
on VOC2007 [ 16]. Therefore, our method presents a simple and efficient approach for training-free 296
enhancement approach at inference. 297
5 Conclusion 298
In this paper, we propose a novel class concept representation from massive text descriptions for 299
training-free multi-label recognition tasks. Inspired by how humans form concepts based on words, 300
as studied in cognitive neuroscience, we replace single class name prompts with the class concept 301
representation that capture various patterns of co-occurrence with other words. To further enhance 302
alignment between multi-modal features of VLMs, we propose a context-guided visual representation 303
that is transformed onto the same linear space as the class concept representation. Remarkably, 304
our proposed method outperforms zero-shot prompt tuning methods such as TaI-DPT and achieves 305
significant enhancements over ZSCLIP and other state-of-the-art prompt tuning methods without 306
requiring parameter tuning or labeled samples, and with minimal inference time overhead. 307
Limitations. While our method achieved impressive results with training-free enhancement of CLIP, 308
it exhibits limitations. First, a significant performance gap exists compared to prompt tuning methods 309
with full samples, like DualCoOp [ 37]. Second, the computational memory demands of our method 310
grow at a faster rate than ZSCLIP[32] as the batch size increases. 311
9References 312
[1]Rabab Abdelfattah, Qing Guo, Xiaoguang Li, Xiaofeng Wang, and Song Wang. Cdul: Clip-driven 313
unsupervised learning for multi-label image classification. In ICCV , 2023. 314
[2]Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, 315
Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: a visual language model for 316
few-shot learning. NeurIPS , 2022. 317
[3]Amit Alfassy, Leonid Karlinsky, Amit Aides, Joseph Shtok, Sivan Harary, Rogerio Feris, Raja Giryes, and 318
Alex M Bronstein. Laso: Label-set operations networks for multi-label few-shot learning. In CVPR , 2019. 319
[4]James Urquhart Allingham, Jie Ren, Michael W Dusenberry, Xiuye Gu, Yin Cui, Dustin Tran, Jeremiah Zhe 320
Liu, and Balaji Lakshminarayanan. A simple zero-shot prompt weighting technique to improve prompt 321
ensembling in text-image models. In ICML , 2023. 322
[5]Lawrence W Barsalou. Perceptual symbol systems. Behavioral and brain sciences , 22(4):577–660, 1999. 323
[6]Emanuel Ben-Baruch, Tal Ridnik, Itamar Friedman, Avi Ben-Cohen, Nadav Zamir, Asaf Noy, and Lihi 324
Zelnik-Manor. Multi-label classification with partial annotations using class-aware selective loss. In CVPR , 325
2022. 326
[7]Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind 327
Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. 328
InNeurIPS , 2020. 329
[8]Dolly Carrillo, Vivian F López, and María N Moreno. Multi-label classification for recommender systems. 330
InTrends in Practical Applications of Agents and Multiagent Systems: 11th International Conference on 331
Practical Applications of Agents and Multi-Agent Systems , pages 181–188. Springer, 2013. 332
[9]Tianshui Chen, Tao Pu, Hefeng Wu, Yuan Xie, and Liang Lin. Structured semantic transfer for multi-label 333
recognition with partial labels. In AAAI , 2022. 334
[10] Tianshui Chen, Muxin Xu, Xiaolu Hui, Hefeng Wu, and Liang Lin. Learning semantic-specific graph 335
representation for multi-label image recognition. In CVPR , pages 522–531, 2019. 336
[11] Zhao-Min Chen, Xiu-Shen Wei, Peng Wang, and Yanwen Guo. Multi-label image recognition with graph 337
convolutional networks. In CVPR , 2019. 338
[12] Tat-Seng Chua, Jinhui Tang, Richang Hong, Haojie Li, Zhiping Luo, and Yantao Zheng. Nus-wide: a 339
real-world web image database from national university of singapore. In CIVR , pages 1–9, 2009. 340
[13] Elijah Cole, Oisin Mac Aodha, Titouan Lorieul, Pietro Perona, Dan Morris, and Nebojsa Jojic. Multi-label 341
learning from single positive labels. In CVPR , 2021. 342
[14] Zixuan Ding, Ao Wang, Hui Chen, Qiang Zhang, Pengzhang Liu, Yongjun Bao, Weipeng Yan, and Jungong 343
Han. Exploring structured semantic prior for multi label recognition with incomplete labels. In CVPR , 344
2023. 345
[15] Thibaut Durand, Nazanin Mehrasa, and Greg Mori. Learning a deep convnet for multi-label classification 346
with partial labels. In CVPR , 2019. 347
[16] Mark Everingham, Luc Van Gool, Christopher KI Williams, John Winn, and Andrew Zisserman. The 348
pascal visual object classes (voc) challenge. IJCV , 2010. 349
[17] Marian George and Christian Floerkemeier. Recognizing products: A per-exemplar multi-label image 350
classification approach. In ECCV , 2014. 351
[18] Zixian Guo, Bowen Dong, Zhilong Ji, Jinfeng Bai, Yiwen Guo, and Wangmeng Zuo. Texts as images in 352
prompt tuning for multi-label image recognition. In CVPR , 2023. 353
[19] Ziyu Guo, Renrui Zhang, Longtian Qiu, Xianzheng Ma, Xupeng Miao, Xuming He, and Bin Cui. Calip: 354
Zero-shot enhancement of clip with parameter-free attention. In AAAI , 2023. 355
[20] Paul Hoffman, James L. McClelland, and Matthew A. Lambon Ralph. Concepts, control, and context: A 356
connectionist account of normal and disordered semantic cognition. Psychological Review , 125(3):293–328, 357
Apr. 358
[21] Ping Hu, Ximeng Sun, Stan Sclaroff, and Kate Saenko. Dualcoop++: Fast and effective adaptation to 359
multi-label recognition with limited annotations. arXiv preprint arXiv:2308.01890 , 2023. 360
10[22] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung, 361
Zhen Li, and Tom Duerig. Scaling up visual and vision-language representation learning with noisy text 362
supervision. In ICML . PMLR, 2021. 363
[23] Ivan Krasin, Tom Duerig, Neil Alldrin, Vittorio Ferrari, Sami Abu-El-Haija, Alina Kuznetsova, Hassan 364
Rom, Jasper Uijlings, Stefan Popov, Andreas Veit, et al. Openimages: A public dataset for large-scale 365
multi-label and multi-class image classification. Dataset available from https://github. com/openimages , 366
2017. 367
[24] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, 368
Yannis Kalantidis, Li-Jia Li, David A Shamma, et al. Visual genome: Connecting language and vision 369
using crowdsourced dense image annotations. IJCV , 2017. 370
[25] Yangguang Li, Feng Liang, Lichen Zhao, Yufeng Cui, Wanli Ouyang, Jing Shao, Fengwei Yu, and Junjie 371
Yan. Supervision exists everywhere: A data efficient contrastive language-image pre-training paradigm. 372
arXiv preprint arXiv:2110.05208 , 2021. 373
[26] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, 374
and C Lawrence Zitnick. Microsoft coco: Common objects in context. In ECCV , 2014. 375
[27] Yongcheng Liu, Lu Sheng, Jing Shao, Junjie Yan, Shiming Xiang, and Chunhong Pan. Multi-label image 376
classification via knowledge distillation from weakly-supervised detection. In ACMMM , 2018. 377
[28] OpenAI. Gpt-4 technical report, 2023. 378
[29] Karalyn Patterson, Peter J Nestor, and Timothy T Rogers. Where do you know what you know? the 379
representation of semantic knowledge in the human brain. Nature reviews neuroscience , 8(12):976–987, 380
2007. 381
[30] Bryan A Plummer, Liwei Wang, Chris M Cervantes, Juan C Caicedo, Julia Hockenmaier, and Svetlana 382
Lazebnik. Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence 383
models. In ICCV , 2015. 384
[31] Tao Pu, Tianshui Chen, Hefeng Wu, and Liang Lin. Semantic-aware representation blending for multi-label 385
image recognition with partial labels. In AAAI , 2022. 386
[32] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish 387
Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from 388
natural language supervision. In ICML , 2021. 389
[33] Nikolaos Sarafianos, Xiang Xu, and Ioannis A Kakadiaris. Deep imbalanced attribute classification using 390
visual attention aggregation. In ECCV , 2018. 391
[34] Manli Shu, Weili Nie, De-An Huang, Zhiding Yu, Tom Goldstein, Anima Anandkumar, and Chaowei 392
Xiao. Test-time prompt tuning for zero-shot generalization in vision-language models. Advances in Neural 393
Information Processing Systems , 35:14274–14289, 2022. 394
[35] Christian Simon, Piotr Koniusz, and Mehrtash Harandi. Meta-learning for multi-label few-shot classifica- 395
tion. In WACV , 2022. 396
[36] Josef Sivic and Andrew Zisserman. Video google: Efficient visual search of videos. Toward category-level 397
object recognition , pages 127–144, 2006. 398
[37] Ximeng Sun, Ping Hu, and Kate Saenko. Dualcoop: Fast adaptation to multi-label recognition with limited 399
annotations. NeurIPS , 2022. 400
[38] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, 401
and Tatsunori B Hashimoto. Alpaca: A strong, replicable instruction-following model. Stanford Center for 402
Research on Foundation Models. https://crfm.stanford.edu/2023/03/13/alpaca.html , 2023. 403
[39] Ivona Tautkute, Tomasz Trzci ´nski, Aleksander P Skorupa, Łukasz Brocki, and Krzysztof Marasek. Deep- 404
style: Multimodal search engine for fashion and interior design. IEEE Access , 2019. 405
[40] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay 406
Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and 407
fine-tuned chat models. arXiv preprint arXiv:2307.09288 , 2023. 408
[41] Ao Wang, Hui Chen, Zijia Lin, Zixuan Ding, Pengzhang Liu, Yongjun Bao, Weipeng Yan, and Guiguang 409
Ding. Hierarchical prompt learning using clip for multi-label classification with single positive labels. In 410
Proceedings of the 31st ACM International Conference on Multimedia , pages 5594–5604, 2023. 411
11[42] Shuai Wang, Daoan Zhang, Zipei Yan, Jianguo Zhang, and Rui Li. Feature alignment and uniformity for 412
test time adaptation. In CVPR , 2023. 413
[43] Xiaosong Wang, Yifan Peng, Le Lu, Zhiyong Lu, and Ronald M Summers. Tienet: Text-image embedding 414
network for common thorax disease classification and reporting in chest x-rays. In CVPR , 2018. 415
[44] Ya Wang, Dongliang He, Fu Li, Xiang Long, Zhichao Zhou, Jinwen Ma, and Shilei Wen. Multi-label 416
classification with label graph superimposing. In AAAI , 2020. 417
[45] Xiu-Shen Wei, Quan Cui, Lei Yang, Peng Wang, and Lingqiao Liu. Rpc: A large-scale retail product 418
checkout dataset. arXiv preprint arXiv:1901.07249 , 2019. 419
[46] Ning Xu, Congyu Qiao, Jiaqi Lv, Xin Geng, and Min-Ling Zhang. One positive label is sufficient: 420
Single-positive multi-label learning with label enhancement. NeurIPS , 2022. 421
[47] Lewei Yao, Runhui Huang, Lu Hou, Guansong Lu, Minzhe Niu, Hang Xu, Xiaodan Liang, Zhenguo Li, 422
Xin Jiang, and Chunjing Xu. Filip: Fine-grained interactive language-image pre-training. arXiv preprint 423
arXiv:2111.07783 , 2021. 424
[48] Vacit Oguz Yazici, Abel Gonzalez-Garcia, Arnau Ramisa, Bartlomiej Twardowski, and Joost van de Weijer. 425
Orderless recurrent models for multi-label classification. In CVPR , 2020. 426
[49] Lu Yuan, Dongdong Chen, Yi-Ling Chen, Noel Codella, Xiyang Dai, Jianfeng Gao, Houdong Hu, Xuedong 427
Huang, Boxin Li, Chunyuan Li, et al. Florence: A new foundation model for computer vision. arXiv 428
preprint arXiv:2111.11432 , 2021. 429
[50] Xiaohua Zhai, Xiao Wang, Basil Mustafa, Andreas Steiner, Daniel Keysers, Alexander Kolesnikov, and 430
Lucas Beyer. Lit: Zero-shot transfer with locked-image text tuning. In Proceedings of the IEEE/CVF 431
Conference on Computer Vision and Pattern Recognition , pages 18123–18133, 2022. 432
[51] Renrui Zhang, Rongyao Fang, Wei Zhang, Peng Gao, Kunchang Li, Jifeng Dai, Yu Qiao, and Hong- 433
sheng Li. Tip-adapter: Training-free clip-adapter for better vision-language modeling. arXiv preprint 434
arXiv:2111.03930 , 2021. 435
[52] Yong Zheng, Bamshad Mobasher, and Robin Burke. Context recommendation using multi-label classi- 436
fication. In IEEE/WIC/ACM International Joint Conferences on WI and IAT , volume 2, pages 288–295, 437
2014. 438
[53] Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei Liu. Conditional prompt learning for 439
vision-language models. In CVPR , 2022. 440
[54] Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei Liu. Learning to prompt for vision-language 441
models. IJCV , 2022. 442
A Generation of Text Descriptions using LLMs 443
Our proposed method leverages the text descriptions for enhancing the alignment between the 444
visual and text features. In practice, gathering the proper text descriptions is an essential process for 445
replacing the hand-crafted prompts. As mentioned in the main paper, the text descriptions can be 446
readily gathered from benchmark dataset, web crawling, or large language models. Recent advances 447
in large language models (LLMs) enable to rapidly generate text descriptions that are similar to 448
image captions in MS-COCO [ 26]. Therefore, we utilized the generated text descriptions from large 449
language model. With provided class name of NUSWIDE [ 12], Fig. 6 illustrates the example of input 450
prompt template and corresponding generated text descriptions using GPT3.5. We carefully designed 451
the instruction of input prompt including main description, constraints, examples of bad and good 452
cases, class names of target task and output format. 453
B Implementing Other Zero-Shot Training-Free Method 454
In single-label recognition, CALIP [ 19] proposed zero-shot alignment enhancement of CLIP for adapt- 455
ing target task without few-shot samples or additional training. The parameter-free attention module of 456
cross-modal interaction effectively enhances the alignment of visual and text features. CALIP utilized 457
the visual feature F=Encv(xk)∈RHW×Dvia reshaping and the text feature T=Enct(Ph)∈RC×D458
12Table 7: Ablation study of hyperparameter searching on validation set. We varied the modulation
parameters αf,tandαF,tand searched the proper values for context-guided visual feature.
1/αf,t,1/αF,t MS-COCO VOC2007 NUS-WIDE
100, 50 69.45 87.62 47.32
80, 40 69.51 87.94 48.31
60, 30 69.25 88.06 49.05
40, 20 67.47 87.37 49.82
20, 10 64.13 85.04 47.33
where Phis a hand-crafted description and Cdenotes the number of classes. The parameter-free 459
attention module is formulated as follows: 460
Fa=Softmax (A/α t)T, (5)
Ta=Softmax (AT/αv)F (6)
where the attention matrix is A=FTT∈RHW×C,αtandαvare the modulation parameters of textual 461
and visual features, respectively, and TaandFaare bidirectionally updated textual and visual features. 462
After pooling the updated visual feature Fa
v∈R1×Dand the global visual feature Fv∈R1×D, the 463
classification logit Sis obtained as below: 464
S=β1·FvTT+β2·FvTaT+β3·Fa
vTT, (7)
where β1, β2, β3are the weights for the three logits. 465
CALIP [ 19] tuned the hyperparameters β2, β3for each dataset while fixed β1to be 1 for simplicity. 466
As shown in Fig. 4, we have explored the value of β2, β3for multi-label recognition setting on 467
MS-COCO [ 26] and have observed that the parameter-free attention module consistently decreases 468
the mAP performance since multi-label recognition covers the identification of multiple objects 469
within an image, involving complex scene and diverse objects. 470
C Exploring Modulation Parameters 471
For hyperparameter searching, following existing methods for classification tasks, such as zero- 472
shot [ 18,19], training-free [ 51,19], and test-time adaptation [ 42], we explore the modulation pa- 473
rameters αtby conducting ablation studies on validation set. For simplicity, we set the value of 474
αf,tto be half of αF,t. As shown in Table 7, the value of ( 1/αf,t,1/αF,t) is suitable in the range 475
of (40∼80,20∼40). In the experiments of main paper, we set the ( 1/αf,t,1/αF,t) as (80,40) for 476
MS-COCO [26], (60,30) for VOC2007 [16] and (40,20) for NUSWIDE [12]. 477
D Examples of Local Alignment Enhancement 478
In Fig. 5, we visualized the examples of local alignment enhancement by applying our method. 479
Enhancing local alignment is important to recognize multiple objects in a test image [ 37]. Our 480
proposed method enhances the local alignment between the visual features of test image and the 481
text features of each class name, thereby suppressing the false-positive prediction. Therefore, Fig. 5 482
demonstrates the effectiveness of our method. 483
E Positive and Negative Societal Impacts 484
As a positive societal impact, our method can allow people with limited computing resources to 485
achieve better performance in multi-label classification using existing vision-language models. This 486
is because it does not require extensive training or labeled data. However, as a negative societal 487
impact, the failure of classification could produce the negative side effects. For example, in security 488
applications, incorrect classification of objects could lead to false alarms or missed detections, 489
potentially compromising safety and security. 490
13Figure 4: Results of hyperparameter searching of CALIP [ 19] on MS-COCO [ 26] onβ2andβ3.
Applying the parametric-free attention module of CALIP consistently decreases performance as
compared to the zero-shot CLIP (ZSCLIP) [32].
14Figure 5: Additional examples of local alignment enhancement via our method. We visualized the test
image in the left column and its corresponding spatial similarity map of each class name in the right
column. The yellow and red boxes refer to the bounding boxes for different labels in a multi-label
setting. By applying our method, the local alignment is enhanced across multiple objects in a test
image, thereby suppressing false-positive predictions.
15Figure 6: Example of text description generation using GPT3.5 for contextual text descriptions of
NUSWIDE [ 12]. We carefully designed the input prompt to ensure that the generated sentences
include the class name of the target task. The elements considered in designing the input prompt
include the main description, constraints, examples, class names, and the desired output format.
16NeurIPS Paper Checklist 491
1.Claims 492
Question: Do the main claims made in the abstract and introduction accurately reflect the 493
paper’s contributions and scope? 494
Answer: [Yes] 495
Justification: We clearly state our contributions in both the abstract and introduction. Espe- 496
cially, we summarize our contributions in the last part of the introduction. 497
Guidelines: 498
•The answer NA means that the abstract and introduction do not include the claims 499
made in the paper. 500
•The abstract and/or introduction should clearly state the claims made, including the 501
contributions made in the paper and important assumptions and limitations. A No or 502
NA answer to this question will not be perceived well by the reviewers. 503
•The claims made should match theoretical and experimental results, and reflect how 504
much the results can be expected to generalize to other settings. 505
•It is fine to include aspirational goals as motivation as long as it is clear that these goals 506
are not attained by the paper. 507
2.Limitations 508
Question: Does the paper discuss the limitations of the work performed by the authors? 509
Answer: [Yes] 510
Justification: We discuss the limitations of our method in conclusion. 511
Guidelines: 512
•The answer NA means that the paper has no limitation while the answer No means that 513
the paper has limitations, but those are not discussed in the paper. 514
• The authors are encouraged to create a separate "Limitations" section in their paper. 515
•The paper should point out any strong assumptions and how robust the results are to 516
violations of these assumptions (e.g., independence assumptions, noiseless settings, 517
model well-specification, asymptotic approximations only holding locally). The authors 518
should reflect on how these assumptions might be violated in practice and what the 519
implications would be. 520
•The authors should reflect on the scope of the claims made, e.g., if the approach was 521
only tested on a few datasets or with a few runs. In general, empirical results often 522
depend on implicit assumptions, which should be articulated. 523
•The authors should reflect on the factors that influence the performance of the approach. 524
For example, a facial recognition algorithm may perform poorly when image resolution 525
is low or images are taken in low lighting. Or a speech-to-text system might not be 526
used reliably to provide closed captions for online lectures because it fails to handle 527
technical jargon. 528
•The authors should discuss the computational efficiency of the proposed algorithms 529
and how they scale with dataset size. 530
•If applicable, the authors should discuss possible limitations of their approach to 531
address problems of privacy and fairness. 532
•While the authors might fear that complete honesty about limitations might be used by 533
reviewers as grounds for rejection, a worse outcome might be that reviewers discover 534
limitations that aren’t acknowledged in the paper. The authors should use their best 535
judgment and recognize that individual actions in favor of transparency play an impor- 536
tant role in developing norms that preserve the integrity of the community. Reviewers 537
will be specifically instructed to not penalize honesty concerning limitations. 538
3.Theory Assumptions and Proofs 539
Question: For each theoretical result, does the paper provide the full set of assumptions and 540
a complete (and correct) proof? 541
Answer: [NA] 542
17Justification: Our paper does not include theoretical results, assumptions and proof. 543
Guidelines: 544
• The answer NA means that the paper does not include theoretical results. 545
•All the theorems, formulas, and proofs in the paper should be numbered and cross- 546
referenced. 547
•All assumptions should be clearly stated or referenced in the statement of any theorems. 548
•The proofs can either appear in the main paper or the supplemental material, but if 549
they appear in the supplemental material, the authors are encouraged to provide a short 550
proof sketch to provide intuition. 551
•Inversely, any informal proof provided in the core of the paper should be complemented 552
by formal proofs provided in appendix or supplemental material. 553
• Theorems and Lemmas that the proof relies upon should be properly referenced. 554
4.Experimental Result Reproducibility 555
Question: Does the paper fully disclose all the information needed to reproduce the main ex- 556
perimental results of the paper to the extent that it affects the main claims and/or conclusions 557
of the paper (regardless of whether the code and data are provided or not)? 558
Answer: [Yes] 559
Justification: We provide the details of the used model, hyperparameters, source of datasets 560
and proposed algorithm for reproducing main experimental results. 561
Guidelines: 562
• The answer NA means that the paper does not include experiments. 563
•If the paper includes experiments, a No answer to this question will not be perceived 564
well by the reviewers: Making the paper reproducible is important, regardless of 565
whether the code and data are provided or not. 566
•If the contribution is a dataset and/or model, the authors should describe the steps taken 567
to make their results reproducible or verifiable. 568
•Depending on the contribution, reproducibility can be accomplished in various ways. 569
For example, if the contribution is a novel architecture, describing the architecture fully 570
might suffice, or if the contribution is a specific model and empirical evaluation, it may 571
be necessary to either make it possible for others to replicate the model with the same 572
dataset, or provide access to the model. In general. releasing code and data is often 573
one good way to accomplish this, but reproducibility can also be provided via detailed 574
instructions for how to replicate the results, access to a hosted model (e.g., in the case 575
of a large language model), releasing of a model checkpoint, or other means that are 576
appropriate to the research performed. 577
•While NeurIPS does not require releasing code, the conference does require all submis- 578
sions to provide some reasonable avenue for reproducibility, which may depend on the 579
nature of the contribution. For example 580
(a)If the contribution is primarily a new algorithm, the paper should make it clear how 581
to reproduce that algorithm. 582
(b)If the contribution is primarily a new model architecture, the paper should describe 583
the architecture clearly and fully. 584
(c)If the contribution is a new model (e.g., a large language model), then there should 585
either be a way to access this model for reproducing the results or a way to reproduce 586
the model (e.g., with an open-source dataset or instructions for how to construct 587
the dataset). 588
(d)We recognize that reproducibility may be tricky in some cases, in which case 589
authors are welcome to describe the particular way they provide for reproducibility. 590
In the case of closed-source models, it may be that access to the model is limited in 591
some way (e.g., to registered users), but it should be possible for other researchers 592
to have some path to reproducing or verifying the results. 593
5.Open access to data and code 594
Question: Does the paper provide open access to the data and code, with sufficient instruc- 595
tions to faithfully reproduce the main experimental results, as described in supplemental 596
material? 597
18Answer: [Yes] 598
Justification: We provide open access to the code for our proposed method. In our experi- 599
ments, we utilize a publicly accessible benchmark dataset described in Section ??. 600
Guidelines: 601
• The answer NA means that paper does not include experiments requiring code. 602
•Please see the NeurIPS code and data submission guidelines ( https://nips.cc/ 603
public/guides/CodeSubmissionPolicy ) for more details. 604
•While we encourage the release of code and data, we understand that this might not be 605
possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not 606
including code, unless this is central to the contribution (e.g., for a new open-source 607
benchmark). 608
•The instructions should contain the exact command and environment needed to run to 609
reproduce the results. See the NeurIPS code and data submission guidelines ( https: 610
//nips.cc/public/guides/CodeSubmissionPolicy ) for more details. 611
•The authors should provide instructions on data access and preparation, including how 612
to access the raw data, preprocessed data, intermediate data, and generated data, etc. 613
•The authors should provide scripts to reproduce all experimental results for the new 614
proposed method and baselines. If only a subset of experiments are reproducible, they 615
should state which ones are omitted from the script and why. 616
•At submission time, to preserve anonymity, the authors should release anonymized 617
versions (if applicable). 618
•Providing as much information as possible in supplemental material (appended to the 619
paper) is recommended, but including URLs to data and code is permitted. 620
6.Experimental Setting/Details 621
Question: Does the paper specify all the training and test details (e.g., data splits, hyper- 622
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the 623
results? 624
Answer: [Yes] 625
Justification: We provide the details of the experimental setting, including hyperparameters, 626
in Sections 4.1 and C. The generation details of the texts used in our method are also 627
provided in Section A. 628
Guidelines: 629
• The answer NA means that the paper does not include experiments. 630
•The experimental setting should be presented in the core of the paper to a level of detail 631
that is necessary to appreciate the results and make sense of them. 632
•The full details can be provided either with the code, in appendix, or as supplemental 633
material. 634
7.Experiment Statistical Significance 635
Question: Does the paper report error bars suitably and correctly defined or other appropriate 636
information about the statistical significance of the experiments? 637
Answer: [Yes] 638
Justification: We do not report the statistical significance of the experimental results, as our 639
method does not rely on statistical variables for inference. 640
Guidelines: 641
• The answer NA means that the paper does not include experiments. 642
•The authors should answer "Yes" if the results are accompanied by error bars, confi- 643
dence intervals, or statistical significance tests, at least for the experiments that support 644
the main claims of the paper. 645
•The factors of variability that the error bars are capturing should be clearly stated (for 646
example, train/test split, initialization, random drawing of some parameter, or overall 647
run with given experimental conditions). 648
19•The method for calculating the error bars should be explained (closed form formula, 649
call to a library function, bootstrap, etc.) 650
• The assumptions made should be given (e.g., Normally distributed errors). 651
•It should be clear whether the error bar is the standard deviation or the standard error 652
of the mean. 653
•It is OK to report 1-sigma error bars, but one should state it. The authors should 654
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis 655
of Normality of errors is not verified. 656
•For asymmetric distributions, the authors should be careful not to show in tables or 657
figures symmetric error bars that would yield results that are out of range (e.g. negative 658
error rates). 659
•If error bars are reported in tables or plots, The authors should explain in the text how 660
they were calculated and reference the corresponding figures or tables in the text. 661
8.Experiments Compute Resources 662
Question: For each experiment, does the paper provide sufficient information on the computer 663
resources (type of compute workers, memory, time of execution) needed to reproduce the 664
experiments? 665
Answer: [Yes] 666
Justification: We provide the information of types of compute worker (GPU model), memory 667
usage and inference time in Section. 4.3.3. 668
Guidelines: 669
• The answer NA means that the paper does not include experiments. 670
•The paper should indicate the type of compute workers CPU or GPU, internal cluster, 671
or cloud provider, including relevant memory and storage. 672
•The paper should provide the amount of compute required for each of the individual 673
experimental runs as well as estimate the total compute. 674
•The paper should disclose whether the full research project required more compute 675
than the experiments reported in the paper (e.g., preliminary or failed experiments that 676
didn’t make it into the paper). 677
9.Code Of Ethics 678
Question: Does the research conducted in the paper conform, in every respect, with the 679
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines ? 680
Answer: [Yes] 681
Justification: We abide by the NeurIPS Code of Ethics. 682
Guidelines: 683
•The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. 684
•If the authors answer No, they should explain the special circumstances that require a 685
deviation from the Code of Ethics. 686
•The authors should make sure to preserve anonymity (e.g., if there is a special consid- 687
eration due to laws or regulations in their jurisdiction). 688
10.Broader Impacts 689
Question: Does the paper discuss both potential positive societal impacts and negative 690
societal impacts of the work performed? 691
Answer: [Yes] 692
Justification: We discuss the positive and negative societal impacts of our paper in the 693
supplementary material. 694
Guidelines: 695
• The answer NA means that there is no societal impact of the work performed. 696
•If the authors answer NA or No, they should explain why their work has no societal 697
impact or why the paper does not address societal impact. 698
20•Examples of negative societal impacts include potential malicious or unintended uses 699
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations 700
(e.g., deployment of technologies that could make decisions that unfairly impact specific 701
groups), privacy considerations, and security considerations. 702
•The conference expects that many papers will be foundational research and not tied 703
to particular applications, let alone deployments. However, if there is a direct path to 704
any negative applications, the authors should point it out. For example, it is legitimate 705
to point out that an improvement in the quality of generative models could be used to 706
generate deepfakes for disinformation. On the other hand, it is not needed to point out 707
that a generic algorithm for optimizing neural networks could enable people to train 708
models that generate Deepfakes faster. 709
•The authors should consider possible harms that could arise when the technology is 710
being used as intended and functioning correctly, harms that could arise when the 711
technology is being used as intended but gives incorrect results, and harms following 712
from (intentional or unintentional) misuse of the technology. 713
•If there are negative societal impacts, the authors could also discuss possible mitigation 714
strategies (e.g., gated release of models, providing defenses in addition to attacks, 715
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from 716
feedback over time, improving the efficiency and accessibility of ML). 717
11.Safeguards 718
Question: Does the paper describe safeguards that have been put in place for responsible 719
release of data or models that have a high risk for misuse (e.g., pretrained language models, 720
image generators, or scraped datasets)? 721
Answer: [NA] 722
Justification: Our paper does not pose a high risk for misuse in terms of model and dataset. 723
Guidelines: 724
• The answer NA means that the paper poses no such risks. 725
•Released models that have a high risk for misuse or dual-use should be released with 726
necessary safeguards to allow for controlled use of the model, for example by requiring 727
that users adhere to usage guidelines or restrictions to access the model or implementing 728
safety filters. 729
•Datasets that have been scraped from the Internet could pose safety risks. The authors 730
should describe how they avoided releasing unsafe images. 731
•We recognize that providing effective safeguards is challenging, and many papers do 732
not require this, but we encourage authors to take this into account and make a best 733
faith effort. 734
12.Licenses for existing assets 735
Question: Are the creators or original owners of assets (e.g., code, data, models), used in 736
the paper, properly credited and are the license and terms of use explicitly mentioned and 737
properly respected? 738
Answer: [Yes] 739
Justification: We cite the papers that provide datasets, code and models in the Section. 4. 740
Guidelines: 741
• The answer NA means that the paper does not use existing assets. 742
• The authors should cite the original paper that produced the code package or dataset. 743
•The authors should state which version of the asset is used and, if possible, include a 744
URL. 745
• The name of the license (e.g., CC-BY 4.0) should be included for each asset. 746
•For scraped data from a particular source (e.g., website), the copyright and terms of 747
service of that source should be provided. 748
•If assets are released, the license, copyright information, and terms of use in the 749
package should be provided. For popular datasets, paperswithcode.com/datasets 750
has curated licenses for some datasets. Their licensing guide can help determine the 751
license of a dataset. 752
21•For existing datasets that are re-packaged, both the original license and the license of 753
the derived asset (if it has changed) should be provided. 754
•If this information is not available online, the authors are encouraged to reach out to 755
the asset’s creators. 756
13.New Assets 757
Question: Are new assets introduced in the paper well documented and is the documentation 758
provided alongside the assets? 759
Answer: [Yes] 760
Justification: We provide the detail documentation for the code in our submission. 761
Guidelines: 762
• The answer NA means that the paper does not release new assets. 763
•Researchers should communicate the details of the dataset/code/model as part of their 764
submissions via structured templates. This includes details about training, license, 765
limitations, etc. 766
•The paper should discuss whether and how consent was obtained from people whose 767
asset is used. 768
•At submission time, remember to anonymize your assets (if applicable). You can either 769
create an anonymized URL or include an anonymized zip file. 770
14.Crowdsourcing and Research with Human Subjects 771
Question: For crowdsourcing experiments and research with human subjects, does the paper 772
include the full text of instructions given to participants and screenshots, if applicable, as 773
well as details about compensation (if any)? 774
Answer: [NA] 775
Justification: Our paper does not involve neither crowdsourcing nor research with human 776
subjects. 777
Guidelines: 778
•The answer NA means that the paper does not involve crowdsourcing nor research with 779
human subjects. 780
•Including this information in the supplemental material is fine, but if the main contribu- 781
tion of the paper involves human subjects, then as much detail as possible should be 782
included in the main paper. 783
•According to the NeurIPS Code of Ethics, workers involved in data collection, curation, 784
or other labor should be paid at least the minimum wage in the country of the data 785
collector. 786
15.Institutional Review Board (IRB) Approvals or Equivalent for Research with Human 787
Subjects 788
Question: Does the paper describe potential risks incurred by study participants, whether 789
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) 790
approvals (or an equivalent approval/review based on the requirements of your country or 791
institution) were obtained? 792
Answer: [NA] 793
Justification: Our paper does not involve neither crowdsourcing nor research with human 794
subjects. 795
Guidelines: 796
•The answer NA means that the paper does not involve crowdsourcing nor research with 797
human subjects. 798
•Depending on the country in which research is conducted, IRB approval (or equivalent) 799
may be required for any human subjects research. If you obtained IRB approval, you 800
should clearly state this in the paper. 801
•We recognize that the procedures for this may vary significantly between institutions 802
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the 803
guidelines for their institution. 804
22•For initial submissions, do not include any information that would break anonymity (if 805
applicable), such as the institution conducting the review. 806
23