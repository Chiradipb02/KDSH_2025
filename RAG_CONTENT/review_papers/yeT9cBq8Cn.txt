Published in Transactions on Machine Learning Research (12/2022)
A Unified Domain Adaptation Framework with Distinctive
Divergence Analysis
Zhiri Yuan yuanzhiri2012@gmail.com
School of Data Science, City University of Hong Kong
Xixu Hu xixuhu2-c@my.cityu.edu.hk
School of Data Science, City University of Hong Kong
Qi Wu qi.wu@cityu.edu.hk
School of Data Science, City University of Hong Kong
Shumin Ma shuminma@uic.edu.cn
Guangdong Provincial Key Laboratory of Interdisciplinary Research and Application for Data Science, BNU-HKBU
United International College
Cheuk Hang Leung chleung87@cityu.edu.hk
School of Data Science, City University of Hong Kong
Xin Shen xshen@se.cuhk.edu.hk
Department of System Engineering and Engineering Management, The Chinese University of Hong Kong
Yiyan Huang yiyhuang3-c@my.cityu.edu.hk
School of Data Science, City University of Hong Kong
Reviewed on OpenReview: https: // openreview. net/ forum? id= yeT9cBq8Cn
Abstract
Unsupervised domain adaptation enables knowledge transfer from a labeled source domain
to an unlabeled target domain by aligning the learnt features of both domains. The idea
is theoretically supported by the generalization bound analysis in Ben-David et al. (2007),
which specifies the applicable task (binary classification) and designates a specific distribu-
tion divergence measure. Although most distribution-aligning domain adaptation models
seek theoretical grounds from this particular bound analysis, they do not actually fit into
the stringent conditions. In this paper, we bridge the long-standing theoretical gap in lit-
erature by providing a unified generalization bound. Our analysis can well accommodate
the classification/regression tasks and most commonly-used divergence measures, and more
importantly, it can theoretically recover a large amount of previous models. In addition, we
identify the key difference in the distribution divergence measures underlying the diverse
models and commit a comprehensive in-depth comparison of the commonly-used divergence
measures. Based on the unified generalization bound, we propose new domain adaptation
models that achieve transferability through domain-invariant representations and conduct
experiments on real-world datasets that corroborate our theoretical findings. We believe
these insights are helpful in guiding the future design of distribution-aligning domain adap-
tation algorithms.
1 Introduction
Domain adaptation (DA) originates from real-world situations where the source and target samples are
drawn from different distributions. In many applications, collecting full labels for the target domain is
1Published in Transactions on Machine Learning Research (12/2022)
prohibitively labor-expensive and time-consuming. To correctly label the target domain, DA transfers the
knowledge learned from the labeled source domain. Given that distributions of the two domains differ, a
remedy to address the distributional difference is to do transfer learning by learning a domain-invariant and
task-specific feature space (Ben-David et al., 2010; Tzeng et al., 2014; Ganin et al., 2016; Saito et al., 2018;
Xu et al., 2019; Zhang et al., 2019).
To do so, most distribution-aligning DA models base themselves on the theoretical analysis in Ben-David
et al. (2007) and use the source error regularized by domain discrepancy as the training objective function.
However, the generalization bound given by Ben-David et al. (2007) only applies to the specific binary
classification task and the H-divergence that measures the domain similarity. It theoretically restricts the
development of DA models for more general multi-class classification and regression tasks (Tzeng et al.,
2014; Ganin et al., 2016; Saito et al., 2018; Xu et al., 2019; Sun & Saenko, 2016). Moreover, to measure the
domain discrepancy, various distribution divergence measures other than the H-divergence have been applied
to DA models (Tzeng et al., 2014; Ganin et al., 2016; Saito et al., 2018; Xu et al., 2019; Long et al., 2017).
Thus, a gap exists between the proliferating DA models and the needed theoretical support. In this work,
we bridge the theoretical gap by providing a unified generalization bound, regardless of the task type and
the divergence measures. Over the various divergence measures, we commit a comprehensive comparison
and deliver helpful insights for guiding the future design of distribution-aligning DA algorithms. With our
analysis framework, we recover the classical DA models in literature and derive new DA model variants. We
empirically show that these model variants can help improve the transfer performance with experiments on
classical DA datasets.
We summarize our contributions of this work as follows. (1) We propose a unified generalization bound
that can easily adapt to both regression and classification tasks. Meanwhile, it can well accommodate the
commonly-used divergence measures, including f-divergence, Wasserstein distance, MMD, etc. Thus, it
greatly relaxes the stringent constraints imposed by the theoretical foundation in Ben-David et al. (2007).
(2) Our analysis underpins and can help recover most classical distribution-aligning DA models in literature.
More importantly, new DA models can be derived based on our framework. With experimental studies, we
show that these new models greatly enrich the model choices and can even improve the transfer performance.
(3) We make a comprehensive comparison of common discrepancies from the perspectives of norm topology,
convergence rate and computation cost, which heuristically provides solid theoretical contributions to the
DA community.
2 Related Work
It is a common belief in DA that minimizing the divergence between source and target domains and the
prediction error on the source domain simultaneously could improve the model performance on the target
domain. Thus, a large number of previous works focus on constructing suitable regularizing terms to measure
the divergence between domains (Redko et al., 2020). Total variation is commonly used to measure the
difference between two probability functions, and to our best knowledge it is the first divergence function
used in DA (Ben-David et al., 2010). MMD is also popular (Long et al., 2015) in that it can characterize more
complicated relations between domains by mapping the input data to a latent feature space. Wasserstein
distanceisanotherfamousdivergencecandidateforitsneatphisicalmeaningrelatedtotheoptimaltransport
theory (Shen et al., 2018). Recently, many researchers come up with fancier divergences, which may be not
natural at first glance but work well in terms of the model performance. For example, Saito et al. (2018) use
the task-specific classifiers to construct a regularizer, and Xu et al. (2019) try to align the classifiers of both
domains to a fixed scalar. In order to maintain the scale of learned features for regression tasks, Chen et al.
(2021) align the domains in terms of principal angles and the representation subspace distance. In general,
there are numerous divergence functions for DA, but a further discussion on why they could perform well in
DA remains untouched in literature.
The theoretical gaps between the target error analysis and vast numbers of DA models in literature have been
partially investigated. Ben-David et al. (2007; 2010) firstly prove that the target error could be bounded
by source error and H-divergence or the total variation between domains. Later, Long et al. (2015) prove
similar results for MMD. However, the theoretical results in Ben-David et al. (2007; 2010) and Long et al.
2Published in Transactions on Machine Learning Research (12/2022)
(2015) only hold for binary classification tasks. Acuna et al. (2021) fix this gap for f-divergences and
classification tasks, and Shen et al. (2018) generalize the result for Wasserstein- 1distance and regression
tasks. In general, it remains a question whether a specific divergence measure theoretically works in that
reducing the corresponding divergence between domains could indeed enhance the model performance on
the target domain. In this work, we propose a unified generalization bound analysis framework to make up
for the needed theoretical support in previous works.
The comparisons between divergence measures have been rarely investigated. Only a few researchers explain
the intuition why a discrepancy measure would outperform another, especially for f-divergences (Sason &
Verdú, 2016). Nowozin et al. (2016) and Arjovsky et al. (2017) argue that MMD and Wasserstein distance
could outperform the total variation, but they only compare them from one single aspect. It is quite natural
that one model could outperform or underperform others on different tasks, as we will see in the experiments.
In this work, we make a more comprehensive comparison over which divergence measure would be beneficial.
3 Preliminaries
LetXandYrepresent the input and output space, respectively. In unsupervised DA, we are given a source
domainDs={(xs
n,ys
n)}Ns
n=1⊂X×Y withNslabeled samples and a targetdomainDt={xt
n}Nt
n=1⊂Xwith
Ntunlabeled samples. It is assumed that the two domains share the same input space, but are characterized
by different distributions on the input space. In this work, we use Pto denote the distribution on the
input space of the source domain and Qto denote that of the target domain. Let hs,ht:X →Ybe the
true mapping functions on the source domain Dsand the target domain Dtrespectively. A hypothesis is
a function h:X →Y, and we denote the hypothesis set as H⊂{h:X →Y} . LetL:Y×Y → R+
be the loss function. Then the source error of a hypothesis hw.r.t.hsunder distribution Pis defined
as:LP(h,hs) :=Ex∼PL(h(x),hs(x)). Similarly, we can define the target error of a hypothesis h, namely,
LQ(h,ht) :=Ex∼QL(h(x),ht(x)). A DA task aims to learn a suitable h∈Hfrom the labeled source samples
as well as the unlabeled target samples such that the target error LQ(h,ht)is small.
Can a hypothesis that achieves the minimal source error generalize well on the target domain? At first glance
it sounds impossible in that the distributions characterizing the inputs differ and the mapping functions ( hs
andht) may also differ. With Theorem 1, Ben-David et al. (2007) verify that a hypothesis that can work
well on the source domain can indeed generalize well on the target domain given that the two domains are
similar. It thus theoretically underpins most unsupervised DA models that are later developed.
Theorem 1. (Ben-David et al., 2007) Let Hbe a hypothesis set such that H={h:X→{ 0,1}}and the
loss function be L(u,v) =|u−v|. Leth∗be the optimal hypothesis that achieves the minimum joint error
on both the source and target domains: h∗:=argminh∈H[LP(h,hs) +LQ(h,ht)], and letλ∗denote the joint
error of the optimal hypothesis h∗:λ∗:=LP(h∗,hs) +LQ(h∗,ht). Then, for any h∈H, we have
LQ(h,ht)≤LP(h,hs) +dH(P,Q) +λ∗. (1)
Here,dH(P,Q)is theH-divergence between distributions PandQ, and it measures the similarity of the
two domains. For binary classification tasks where H={h:X→{ 0,1}}, it is defined as
dH(P,Q)≜2sup
h∈H|Ex∼Ph(x)−Ex∼Qh(x)|.
Theorem 1 confirms the generalization bound in terms of the source error LP(h,hs)and the discrepancy
between domains dH(P,Q). But two elements have been made fixed: the task (binary classification) and
the measure of distributional difference ( H-divergence). Real-world DA applications seldom fit strictly into
the binary classification framework. To extend to more general settings like multi-class classification or even
regression tasks, Mansour et al. (2009) take one step further to establish the generalization bound by relaxing
the constraints over the hypothesis set Hwith the following Theorem 2.
Theorem 2. (Mansour et al., 2009) Assume that the loss function Lis symmetric and obeys the triangle
inequality. Define h∗
s:=argmin
h∈HLP(h,hs), andh∗
t:=argmin
h∈HLQ(h,ht). For any h∈H, the following
inequality holds,
LQ(h,ht)≤LP(h,h∗
s) +distH
L(P,Q) +LP(h∗
s,h∗
t) +LP(h∗
s,hs). (2)
3Published in Transactions on Machine Learning Research (12/2022)
InEq. (2), thedivergencemeasureused, namelydistH
L(P,Q), isthediscrepancydistance . Thediscrepancy
distance between two distributions P,QoverXis defined as
distH
L(P,Q)≜sup
h,h′∈H|LP(h,h′)−LQ(h,h′)|.
Compared to the H-divergence, discrepancy distance is more general. The discrepancy distance is pretty
flexible because its power on measuring the distribution distance can be controlled by the richness of the
hypothesis setHand loss functions L.
Although Mansour et al. (2009) remove the constraints over the hypothesis set Hto extend the generalization
bound analysis to more realistic applications, we identify by looking into Eq. (2) that a hypothesis hcan only
generalize well on the target domain when the objective function LP(h,h∗
s) +distH
L(P,Q)is minimized. On
one hand , compared to minimizing the true source error LP(h,hs)in Eq. (1), minimizing LP(h,h∗
s)results
in a suboptimal hypothesis because h∗
sis only an approximate to the true labeling function hsand is usually
implicit in real-world applications. On the other hand , in practice, the discrepancy distance distH
L(P,Q)is
hard to implement in the deep neural networks. In all of the DA applications, practitioners mainly choose a
specific divergence such as the Jensen-Shannon divergence, Wasserstein distance, MMD distance. etc. There
exists a gap between the proliferating DA models that takes various divergence measures and the needed
theoretical support to verify the effectiveness of the models. What’s more, considering there are multiple
divergence measure choices, the conundrum arises: what difference does it make if a practitioner chooses
MMD distance rather than Wasserstein distance? How to choose the best divergence measure? In this work,
we bridge the afforementioned theoretical gap by providing a unified generalization bound, regardless of the
task type and the divergence measures. In addition, we provide the detailed comparison of various divergence
measures in DA scenario and deliver helpful insights for guiding future design of DA algorithms.
Table 1: Popular f-divergences and corresponding ffunctions.
f-divergence f(t) f-divergence f(t)
Kullback-Leibler (KL) tlogt Reverse KL −logt
Neymanχ2 1
t−1 Pearsonχ2t2−1
Jensen-Shannon (JS) tlogt−(1 +t) log1+t
2Squared Hellinger (SH) (√
t−1)2
Total Variation (TV)1
2|t−1|
3.1 Divergence Measures between Distributions
Before proceeding to the detailed analysis of the generalization bound, we briefly introduce the divergence
measures that have been commonly used in literature.
f-divergence (Sason & Verdú, 2016). Letf:R+→Rbe a convex, semi-continuous function satisfying
f(1) = 0. LetP,Qbe two distributions, satisfying that dPis absolutely continuous w.r.t. dQ, and both are
absolutely continuous w.r.t. a base measure dx. Thef-divergence DffromPtoQis given by
Df(P,Q)≜/integraldisplay
f(dP
dQ)dQ.
Different choices of fhave been proposed in literature. For a good review, see Pardo (2018). Table 1 lists
the popular choices of the f-divergence and their corresponding ffunctions.
Wasserstein distance (Villani, 2009). The Wasserstein distance between distributions PandQcan be
interpreted as the optimal (minimal) transportation cost of moving the mass from Pinto the mass of Q.
Letp∈[1,+∞)andd(·,·)be a distance function of X. Let Π(P,Q)denote the set of all possible joint
distributions whose marginals are P,Qrespectively. The Wasserstein- pdistance between PandQis given
by
Wp(P,Q)≜inf
γ∈Π(P,Q)(E(x,x′)∼γ[d(x,x′)p])1
p.
In real applications, the case p= 1is of particular interest. Thus, we mainly consider the Wasserstein- 1
distance (W1) in the following context.
4Published in Transactions on Machine Learning Research (12/2022)
MMD (Sriperumbudur et al., 2009b). Given a kernel function k:X×X → R, letϕbe the feature
map such that k(x,x′) =ϕ(x)·ϕ(x′), andHkbe the reproducing kernel Hilbert space (RKHS) associated
with the kernel k. The MMD distance between distributions PandQis:
MMDk(P,Q)≜∥Ex∼Pϕ(x)−Ex∼Qϕ(x)∥Hk. (3)
4 A Unified Generalization Bound
In this section, we propose a unified generalization bound that can both accommodate the general DA
tasks (classification/regression) and the commonly-used distribution divergence measures. Based on this
unified bound, we provide the theoretical support for the DA models in the literature that lack the necessary
theoretical analysis. In addition, with this generalization bound in hand, we are able to propose new DA
models which replenish the current DA literature and greatly enrich the model choices. We will empirically
verify the efficiency of these new models on benchmark DA datasets in Section 6. In this section, we begin
with Theorem 3 where we prove that the target error can actually be upper bounded by the true source
error together with the domain difference plus a constant for any hypothesis set and any symmetric loss
function that satisfies triangular inequality. The proof of theorems and propositions in the following context
is deferred to Appendix A.
Theorem 3. Assume the loss function Lis symmetric and satisfies triangular inequality. For any hypothesis
setH, it always holds that
LQ(h,ht)≤LP(h,hs) +distH
L(P,Q) +λ∗. (4)
At first glance, it seems that Eq. (4) is nothing different compared with Eq. (1), except the divergence
term. There are actually improvements in our result. The unified bound in Theorem 3 does not only apply
to the binary classification task. We relax the constraints on the loss function and impose no constraints
on the hypothesis set, which improve the versatility of such an upper bound for any practical algorithm.
Compared to Eq. (2), we move one step further to directly minimize the true source error LP(h,hs)rather
thanLP(h,h∗
s), where estimating h∗
sfirst can amplify the source error and minimizing the corresponding
LP(h,h∗
s)will finally produce a suboptimal hypothesis. Hence our proposed Eq. (4) can produce a better
hypothesis to generalize well on the target domain.
Although we take the discrepancy distance as the divergence measure as in Eq. (2), we will not end here.
We will show the connections between the hypothetical distH
L(P,Q)and the commonly-used divergence
measures, including Wasserstein distance, MMD and f-divergence. It’s already known that Wasserstein- 1
distance (W1) is a special case of discrepancy distance (Shen et al., 2018; Villani, 2009; Sriperumbudur
et al., 2009b). In the following analysis, we focus on the other two divergence families and show that the
discrepancy distance can actually recover the MMD distance (in Proposition 4) and partially recover the
vastf-divergence family (in Proposition 6).
Proposition 4. LetL(u,v) =|u−v|andkbe a kernel function on X. LetHconsist of functions with norm
no greater than 1in RKHS associated with kernel k, then for any two distributions P,Q,distH
L(P,Q) =
2MMDk(P,Q).
Lemma 5. (Ben-David et al., 2010) Let L(u,v) =|u−v|andHconsists of functions on Xwith values in
[0,1], then for any two distributions P,Q,distH
L(P,Q) = 2DTV(P,Q).
Proposition 6. For anyf-divergence Dfsatisfying that fis continuous and strictly convex, there exists a
constantCf>0, such that for any two distributions P,Q,DTV(P,Q)≤CfDf(P,Q).
Proposition 4 verifies that MMD distance can be recovered by the discrepancy distance. With Proposition
6, one can check that the ffunctions for commonly-used f-divergence (including KL, JS, SH, etc.) indeed
fit into the continuous and strictly convex conditions. Thus, combining Lemma 5, Proposition 6 with
Theorem 3, we see that the inequality in Eq. (4) still holds for if the divergence measure is taken to be
thesef-divergence. It thus confirms that the generalization bound in Theorem 3 can well accommodate the
commonly-used f-divergence .
Compared with Theorem 1 and 2, Theorem 3 is more general in that it provides theoretical support to most
DA algorithms in literature. For example, if we take L(u,v) =|u−v|and letHconsist of all classifiers
5Published in Transactions on Machine Learning Research (12/2022)
takings values in {0,1}(namely, a binary classification task), then the divergence measure distH
L(P,Q)is
equal to theH-divergence and we can recover the classical result in Theorem 1. With our approach, we can
also theoretically recover the model proposed in Shen et al. (2018), where the authors prove the efficiency
of Wasserstein distance measuring the distributional difference for classification tasks. We defer the detailed
discussion over the connections with the classical models in the next subsection.
Theorem 3 is also novel in that the underlying analysis framework can overcome the limitations of the
generalization bound analysis in previous literature. For example, in Long et al. (2015), the authors prove
that using multiple kernel MMD is efficient in classification tasks with the help of Parzen window classifier,
which limits the analysis to only classification tasks thus regression tasks do not apply. With our analysis
methodology, we can generalize the efficiency analysis to any universal kernel MMD and to both classification
and regression tasks. Acuna et al. (2021) prove that f-divergence could be used to derive a generalization
bound for classifications under a pretty strong assumption on the hypothesis set, while we do not need
additional assumptions.
The target error bound analysis that we propose actually still holds irrespective of the problem setting
(covariate shift/conditional shift). The difference in solving these two kinds of problems lie in how to guide
the algorithm design with the same theoretical target error bound. In the covariate shift setting where it’s
assumed that the conditional distributions of Y|Xbetween the source and target domains are the same, the
third term λ∗in the target error bound in Eq. (4) vanishes. Thus, it suffices to just learn the invariant
representations while minimizing the source risk, as we have done in the experiments. In general settings
where there can be conditional shift, it is not sufficient to simply align the marginal distributions while
achieving small error on the source domain, since the third term λ∗can not be neglected (as supported by
Theorem 3 of [1] and the lower bound on the joint error of source and target domains in Theorem 4.3 of [2]).
But we should also note that some researchers point out that the third term can be reduced to a rather small
value if the hypothesis space is rich enough ([4]), making it still possible for the model to adapat well to the
target domain by only minimizing the domain discrepancy and the source error even if there is conditional
distribution shift.
4.1 Connections with Classical Models
In Table 2, we list and summarize some classical DA models in literature. From the last column, we see that
most models are proposed without the necessary theoretical explanation on why it can generalize well on
the target domain. In this section, we take three models (MCD (Saito et al., 2018), AFN (Xu et al., 2019)
and DAN (Long et al., 2015)) as examples to illustrate how our theoretical analysis can help explain the
underlying mechanism. Our analysis can also recover other benchmark models, which we will omit here.
4.1.1 Model 1: MCD (Saito et al., 2018)
Saito et al. (2018) propose to align source and target features with the task-specific classifiers as a discrimina-
tor for a multi-class classification task. Specifically, to incorporate the relationship between class boundaries
and target samples, the authors train two discriminators h1andh2to maximize the target features’ discrep-
ancy. Denote the number of classes by m, then the output space Y={0,1}m. LetL(u,v) =1
m/summationtextm
i=1|ui−vi|
where u,v∈Rm, then the training objective for the model MCD is
LP(h1,hs) +LP(h2,hs)−Ex∼QL(h1(x),h2(x)), (5)
Next, we show that minimizing the objective function in Eq. (5) is actually equivalent to minimizing the
source error together with measuring the total variation distance between the two domains. Typically, we
will prove that min−Ex∼QL(h1(x),h2(x))returns the domain divergence 2∗DTV(P,Q), given that h1and
h2can simultaneously perform well on the source domain. Expressing the total variation distance in the
form of discrepancy distance, we have
DTV(P,Q) =1
2sup
h1,h2∈H/vextendsingle/vextendsingleEx∼PL(h1(x),h2(x))−Ex∼QL(h1(x),h2(x))/vextendsingle/vextendsingle.
6Published in Transactions on Machine Learning Research (12/2022)
Table 2: Selected models in the literature.
Literature Model Hypothesis set Divergence measure Theoretical analysis
Ben-David et al. (2007) −{h:X→{ 0,1}}H-divergence ✓
Ben-David et al. (2010) −{h:X→{ 0,1}}Total variation ✓
Tzeng et al. (2014) DDC{h:X→{ 0,1}m}MMD
Long et al. (2015) DAN{h:X→{ 0,1}m}MK-MMD ✓
Sun & Saenko (2016) CORAL{h:X→{ 0,1}m}CORAL Loss
Ganin et al. (2016) DANN{h:X→{ 0,1}m}ProxyA-distance
Courty et al. (2017) JDOT{h:X→{ 0,1}m}Wasserstein distance ✓
Long et al. (2017) JAN{h:X→{ 0,1}m}JMMD
Tzeng et al. (2017) ADDA{h:X→{ 0,1}m}−
Long et al. (2018) CDAN{h:X→{ 0,1}m}−
Saito et al. (2018) MCD{h:X→{ 0,1}m}L1-norm
Sankaranarayanan et al. (2018) GTA{h:X→{ 0,1}m}−
Shen et al. (2018) WDGRL{h:X→{ 0,1}m}Wasserstein distance ✓
Xu et al. (2019) AFN{h:X→{ 0,1}m}MMFND
Zhang et al. (2019) MDD{h:X→{ 0,1}m}Disparity Discrepancy ✓
Kang et al. (2020) CAN{h:X→{ 0,1}m}CDD
Acuna et al. (2021) f-DAL{h:X→{ 0,1}m}Dϕ
h,Hdiscrepancy ✓
By assuming that the classifiers h1,h2will work well on the source domain, the authors suggest that the
termEx∼PL(h1(x),h2(x))is small enough to omit. Thus the following equivalence holds:
min
h1,h2LP(h1,hs) +LP(h2,hs)−Ex∼QL(h1(x),h2(x))
⇔min
h1,h2LP(h1,hs) +LP(h2,hs) +sup
h1,h2/vextendsingle/vextendsingleEx∼QL(h1(x),h2(x))/vextendsingle/vextendsingle
⇔min
h1,h2LP(h1,hs) +LP(h2,hs) +sup
h1,h2/vextendsingle/vextendsingleEx∼PL(h1(x),h2(x))−Ex∼QL(h1(x),h2(x))/vextendsingle/vextendsingle
⇔min
h1,h2LP(h1,hs) +LP(h2,hs) + 2DTV(P,Q).
4.1.2 Model 2: AFN (Xu et al., 2019)
Xu et al. (2019) point out that sometimes the hypothesis set His so rich that the upper bound will greatly
deviate from zero. To restrict H, the authors place a restrictive scalar Rto match the corresponding mean
feature norms. With a hyperparameter λ∈R+, the objective function to train the model AFN is
LP(h,hs) +λ/parenleftbig
LP(h,R) +LQ(h,R)/parenrightbig
. (6)
Now we use our analysis methodology to theoretically prove that Eq. (6) is actually an upper bound of the
target error. Notice that
LQ(h,ht)≤LQ(R,ht) +LQ(h,R)
≤LQ(R,ht) +LP(h,R) +/vextendsingle/vextendsingleLP(h,R)−LQ(h,R)/vextendsingle/vextendsingle
≤LP(h,hs) +LP(h,R) +LQ(h,R) +LQ(R,ht) +LP(R,hs).
Given thatLQ(R,ht)andLP(R,hs)are constants, the above calculations verify that Eq. (6) does work as
a generalization bound.
4.1.3 Model 3: DAN (Long et al., 2015)
Compared with Tzeng et al. (2014) who firstly use the Gaussian kernel MMD between features of the last
layer as the regularizer, the DAN model is novel in that the authors use the multiple-Gaussian kernel function
7Published in Transactions on Machine Learning Research (12/2022)
and consider the sum of MMD for features in multiple layers. Now we show that our analysis framework
can incorporate the DAN model.
First, a multiple-Gaussian kernel function is still a kernel function (Berlinet & Thomas-Agnan, 2011), thus
Theorem 3 and Proposition 4 guarantee that using the multiple-Gaussian kernel MMD can efficiently bound
the target error. By abusing the notations in Long et al. (2015), we denote the feature space after l-th layer
of the source domain (resp. target domain) by Dl
s(resp.Dl
t). The objective function of DAN is
LP(h,hs) +λ8/summationdisplay
l=6MMD2
k(Dl
s,Dl
t),
where the sum is taken through the squared MMDs between features of the 6th, 7th and 8th layer. Let
Dsum
s=8/circleplustext
l=6Dl
s,Dsum
t=8/circleplustext
l=6Dl
t, where/circleplustextis the direct sum, then it can be verified that MMD2
k(Dsum
s,Dsum
t) =
8/summationtext
l=6MMD2
k(Dl
s,Dl
t). Thus by viewing Dsum
sandDsum
tas the source and target domains, it is reasonable to
take MMD k(Dsum
s,Dsum
t)to construct the theoretical upper bound of the target domain, which means that
the objective function in Long et al. (2015) can upper-bound the target error.
5 Comparison of Divergence Measures
We have shown that the generalization bound in Eq. (4) can admit most of the distribution divergence
measures, and such conclusion is verified by the abundant literature in Table 2. A natural question is, is
there any difference on choosing another divergence measure? If so, will there be any guidance on how to
choose the suitable divergence measure? Through this section, we illustrate in detail the difference of the
various divergence measures. In Section 5.3, we conclude that there is a tradeoff in the selection of a specific
divergence measure, and thus we leave the second question for open discussion.
5.1 Topologies Induced by Metrics
When measuring the difference between distributions PandQ, we hope that the discrepancy measure could
truly reflect the extent of the difference. Otherwise, it is unclear whether minimizing the domain discrepancy
canindeedalignthefeaturespaces. Inthisrespect, totalvariationdistanceisunsatisfactory. Takeanextreme
example. LetX=R, andPθ=δθ,θ∈R, namely, the support of Pθcontains only the point {θ}. Then the
total variation distance between P0andPθis given by
DTV(P0,Pθ) =/braceleftigg
0, θ= 0
1, θ̸= 0
The corresponding neural network model will fail to minimize DTV(P0,Pθ)w.r.t.θifθis initialized non-
zero by backpropagation, making it impossible to align the two domains. However, Wasserstein distance and
MMD are known to be able to get rid of this problem (Arjovsky et al., 2017; Li et al., 2017).
To illustrate the underlying mechanism, we look into the norm topologies induced by the metrics. For a
spaceXwith a metric function d, thenorm topology τof(X,d)consists of the sets that could be written
as the union of open balls B(x,r) ={x′∈X :d(x,x′)<r}with center x∈Xand radius r∈R+. IfXis
known and fixed, we simply omit Xand say that τis induced by d. We say a topology τ1iscoarserthan
τ2, denoted by τ1⊂τ2, if any open ball in τ1is also open in τ2. Ifτ1⊂τ2andτ2⊂τ1, then we say τ1and
τ2areequivalent . Roughly speaking, the metric with coarser norm topology should perform better. With
the following lemma, we could establish the link between the norm topologies and the corresponding metric
functions1, which will facilitate the later analysis.
Lemma 7. Given two metrics d1,d2onX, if there exist a,b∈R+such thatd1(x,x′)≤a×d2(x,x′)b,∀x,x′∈
X, then the norm topology τ1of(X,d1)is coarser than the norm topology τ2of(X,d2).
1It is worth noting that some members (although not all) of the f-divergence are metric functions, such as the total variation
and the SH distance. Thus, the analysis over norm topology still applies to these f-divergence.
8Published in Transactions on Machine Learning Research (12/2022)
With Lemma 7, we are able to compare the norm topologies induced by the distance measures in Section
3.1. Assume that the underlying space Xis compact with diameter M <∞. By Theorem 6.15of Villani
(2009), it holds that
W1(P,Q)≤M×DTV(P,Q), (7)
implyingthetopologyinducedbytheWasserstein- 1distanceiscoarserthanthetopologyinducedbythetotal
variation. As for the MMD distance, for a kernel function k(·,·)satisfying that C:=supx∈Xk(x,x)<∞
(e.g., a Gaussian kernel, or a Laplacian kernel), by Theorem 14of Sriperumbudur et al. (2009b), we have
MMDk(P,Q)≤√
C×DTV(P,Q). (8)
It also suggests a coarser topology induced by the MMD distance than by the total variation. Thus, the
conclusions above help explain why in many DA applications, Wasserstein and MMD distance perform the
best. But as we will see in the following analysis and experiments, it is not the norm topology that has the
final say over the model performance. The implementation details of the domain divergence also significantly
affect the model performance.
5.2 Convergence Rate
In this part, we compare the various divergence measures from the perspective of the convergence rates w.r.t.
the sample size. In practice, the domain discrepancy between distributions PandQis estimated by the
finite samples from DsandDt. Thus, how the empirical distribution divergence distH
L(Ds,Dt)converges to
the true domain divergence distH
L(P,Q)is vital to the success of the algorithm. Notice that for arbitrary
loss function Land hypothesis class H, it holds that
|distH
L(Ds,Dt)−distH
L(P,Q)|≤distH
L(Ds,P) +distH
L(Dt,Q).
Thus, it is enough to analyze the convergence properties of the discrepancy between a typical distribution
and its empirical distribution, namely, distH
L(Ds,P)(or distH
L(Dt,Q)). The faster the empirical distribution
converges to its underlying distribution, the faster will the empirical domain difference converge to the true
domain divergence.
We introduce the convergence rate (CR) to measure the convergence performance. Fix the underlying space
X. Given a distribution Pand a set of NrealizationsD={x1,...,xN}, we denote by PN=/summationtextN
i=1δxi/Nthe
empirical distribution of P. The convergence rate of a probability distance metric D(·,·)is said beα, if for
any distribution PonXandδ∈(0,1), there exists a constant Csuch that the following inequality holds
with probability at least 1−δ,
D(PN,P)≤CN−α.
For notation simplicity, we denote a distance metric D(·,·)that has convergence rate αas CR (D) =α.
Obviously, for a probability metric D, the higher the convergence rate α, the faster D(PN,P)converges
w.r.t. the sample size N. And thus, the corresponding empirical domain difference will converge to the true
domain divergence.
In Proposition 8, we list the convergence rates of total variation distance, Wasserstein- 1distance and MMD.
We leave the discussion of Proposition 8 to the next subsection (Section 5.3).
Proposition 8. AssumeXis of dimension d.
(1).For anyf-divergence Dfthat is indeed metric function (e.g., total variation, SH distance, Pearson
χ2distance, etc.), CR(Df)≤1
2. In particular, CR(DTV) =1
2.
(2).IfXis compact, then CR(W1) =1
d.
(3).(Theorem 7 and Corollary 10 in Sriperumbudur et al. (2009a)) If the kernel function kis charac-
teristic and supx∈Xk(x,x)<∞, thenCR(MMDk) =1
2.
9Published in Transactions on Machine Learning Research (12/2022)
Table 3: Comparison of the divergence measures commonly used in literature.
f-divergences Wasserstein distance MMD
Computation cost Low Low High
Convergence rate ≤1
21
d1
2
Topology Fine Coarse Coarse
5.3 A Summary
In this part, we summarize the properties of commonly-used divergences and make a brief comparison
between them.
f-divergence isalargedivergencefamilythatcoversvariousdivergencemeasures. Thoughmanyresearchers
focus on only a few f-divergence, such as the total variation and JS divergence, there can be more divergence
choices in the applications. The topology determined by a f-divergence is rather fine. However, estimating
thef-divergence with neural networks can be time-consuming and unstable, as the implementation needs
the adversarial networks (Zhang et al., 2019; Acuna et al., 2021).
Wasserstein distance enjoys a very concrete intuition and a direct connection to the optimal transport
theory(Villani,2009). Comparedwith f-divergence, ithasaweakertopology. Giventhatdirectlycomputing
the Wasserstein- pdistance for arbitrary p≥1is challenging, researchers usually adopt the Wasserstein- 1
distance. The Wasserstein- 1distance is in some cases limited in that it only measures the difference between
the first-order moments of two distributions (Arjovsky et al., 2017).
MMDuses the kernel function to measure the domain discrepancy. Similar to the Wasserstein- 1distance,
MMD has a weaker topology than f-divergence, thus minimizing the domain discrepancy with MMD can
indeed align the feature space. However, unlike Wasserstein- 1distance, MMD (e.g. MMD with Gaussian
kernel) could measure higher-order moments of two distributions (Li et al., 2017). More importantly, it is
easy to implement in neural networks. In applications, instead of fixing a single kernel MMD, researchers
have the freedom to combine Gaussian MMDs with multiple bandwidths (Long et al., 2017; 2015; 2018;
Li et al., 2017). However, evaluating the MMD distance encounters the computational cost that grows
quadratically with sample size (Arjovsky et al., 2017), which limits the scalability of MMD distance.
We summarize the pros and cons of common divergences in Table 3. We observe that there is no consensus
on which divergence measure can uniformly outperform its competitors. That may explain why works taking
various divergence measures proliferate in the DA literature.
6 Experiments
In this section, we enrich the DA models by proposing new model variants that take divergence measures
which are not covered in literature before, including total variation (TV), Neyman χ2, Pearsonχ2, KL,
Reverse KL, SH, JS and MMD with Laplacian kernel functions.
Let’s first give a brief description over the proposed models. In the unsupervised DA setting, we are given
Nslabeled source samples {(xs
n,ys
n)}Ns
n=1, together with Ntunlabeled samples {xt
n}Nt
n=1. We’ll first train a
feature extractor F(·;θf)parameterized by θfto extract the features. We denote the features extracted as
Fs(for source features) and Ft(for target features). Namely, Fs:={F(xs
n;θf)}Ns
n=1,Ft:={F(xt
n;θf)}Nt
n=1.
The feature extractor is followed by a discriminator network D(·;θd)parameterized by θdto minimize the
source errorL({xs
n,ys
n};θf,θd). Here,Ltakes cross-entropy loss for a classification problem and mean
squared error for a regression problem. Meanwhile, good transferability requires the alignment of domains.
Thus, the feature extractor is required to extract the domain-invariant features, guaranteed by minimizing
the distance between the features distH
L(Fs,Ft). So the general objective for the proposed models is:
min
θf,θd/braceleftbig
L({xs
n,ys
n};θf,θd) +λdistH
L(Fs,Ft)/bracerightbig
,
10Published in Transactions on Machine Learning Research (12/2022)
Figure 1: The network flow of the proposed models. All samples, no matter from the source domain {xs
n}Ns
n=1
(blue) or the target domain {xs
n}Nt
n=1(green), are fed into a feature extractor F(·;θf)to extract features
that are both discriminative and domain-invariant. The discriminative features are achieved by training a
discriminator D(·;θd)to minimize the loss function L(·;θf,θd), while the domain invariance is measured by
distH
L(Fs,Ft).
whereλis a positive hyperparameter to be tuned. For illustration purposes, we sketch the general network
flow for the proposed models in Figure 1. For the domain divergence measure distH
Lin the proposed models,
we take specific divergence measures as mentioned in the last paragraph.
6.1 Image Classification
We test the efficacy of the new DA model variants and compare with a variety of baselines on three datasets:
Office-Home ,DigitsandOffice-31 . All experiments in this section are run on Dell Precision 7920 with
Intel®Xeon®Gold 6256 CPU at 3.6GHz, and a set of NVIDIA Quadro GV100 GPU cards.
Office-Home consists of 15,500 images in 65 object classes in office and home settings, forming four ex-
tremely dissimilar domains: Artistic images ( Ar), Clipart images ( CA), Product images ( Pr), and Real-
World images ( Rw). For the Digitsdataset, we investigate two digits datasets MNIST andUSPSwith
two transfer tasks ( M→UandU→M). We train and evaluate all the methods following the splits and
evaluation protocol from Long et al. (2018), which divides MNIST into 60,000 training images and 10,000
test images, and divides USPSinto 7,291 training images and 2,007 test images, respectively. Office-31 is
the most widely used dataset in DA literature. It consists of 4,652 images in 31 categories collected from
three sources: Amazon ( A), Webcam ( W) and DSLR ( D). We evaluate all methods on six transfer tasks
A→W,D→W,W→D,A→D,D→A,W→A.
Baseline methods. We compare with the three models discussed in Section 4.1 and the following classical
and state-of-the-art models. JAN (Long et al., 2017) and CAN (Kang et al., 2020) measure the domain
discrepancy with variants of MMD. MDD (Zhang et al., 2019) adopts γ-JSD (a member of f-divergence), to
align the two domains. f−DAL (Acuna et al., 2021) adopts a f−divergence variants of MDD to align the
two domains. WDGRL (Shen et al., 2018) estimates the domain discrepancy by the Wasserstein distance.
DANN (Ganin et al., 2016) analyzes the domain divergence by adversarial neural networks.
The models we propose. We propose to use multiple f-divergences as regularizers and test the cor-
responding DA models’ effects. Specifically, we use Squared Hellinger distance, Pearson χ2distance, KL
divergence, total variation, etc. We also test MMD with Laplacian kernel and compare it with the Gaussian
kernel. For the Office-Home and Office-31 experiments, ResNet-50 (He et al., 2016) pre-trained on the Ima-
geNet (Deng et al., 2009) is adopted as the backbone feature extractor. For the Digits experiments, LeNet
(LeCun et al., 1998) serves as the backbone feature extractor. The classifiers are 2-layer neural networks with
width 1024, 2048 and 500 correspondingly. We use separate neural networks to approximate the divergences.
11Published in Transactions on Machine Learning Research (12/2022)
For the Office datasets, we adopt 3-layer neural networks with the same width as the classifiers. For the
Digits dataset, we use a 2-layer neural network with width 500.
We follow the standard experiment protocol for unsupervised DA from Ganin et al. (2016) and report the
average accuracy for each experiment as in Acuna et al. (2021). All algorithms are implemented in PyTorch.
For models using f-divergences, we introduce a domain critic inspired by Shen et al. (2018) and Nowozin
et al. (2016) to estimate the f-divergence in a min-max manner. The domain critic is a 3-layer neural
network with ReLU as activation functions. We solve the min-max problem using gradient reversal layers
motivated by Ganin et al. (2016). We take the mini-batch SGD optimizer with the Nestorov momentum 0.9
and the learning rates of the classifier are set to be 10 times to that of the feature extractor.
Table 4: Accuracy (%) on the Office-Home datasets in two subtables. The performance with the baseline
models are recorded above the middle line and the models we propose below the line in each subtable.
Method Ar →Cl Ar→Pr Ar→Rw Cl→Ar Cl→Pr Cl→Rw Avg
ResNet-50 34.9 50.0 58.0 37.4 41.9 46.2 44.7
DAN(Gaussian) 46.3 66.8 74.2 57.5 63.1 66.8 62.5
DANN 45.6 59.3 70.1 47.0 58.5 60.9 56.9
JAN 45.9 61.2 68.9 50.4 59.7 61.0 57.9
MCD 53.2 73.8 77.7 62.8 68.8 70.7 67.8
AFN 52.0 71.7 76.3 64.2 69.9 71.9 67.7
MDD 54.9 73.7 77.8 60.0 71.4 71.8 68.3
WDGRL 48.2 61.5 73.0 54.6 64.0 63.9 60.9
CAN 50.3 71.7 76.0 60.9 69.9 70.3 66.5
f−DAL 53.7 71.2 76.3 60.2 68.4 69.0 66.6
DAN(Laplacian) 45.7 66.5 74.5 57.0 62.4 66.5 62.1
TV 46.4 65.1 73.6 55.6 64.6 66.5 62.0
Neymanχ250.0 66.8 75.6 56.5 66.5 67.9 63.9
Reverse KL 55.2 74.6 78.4 57.2 69.3 71.4 67.7
SH 55.0 73.3 77.1 60.4 69.1 70.7 67.6
JS 53.5 72.9 76.5 58.8 67.3 68.6 66.3
KL 55.7 73.4 77.9 60.0 70.8 72.5 68.4
Pearsonχ255.5 69.4 78.7 61.1 73.5 72.9 68.5
Method Pr →Ar Pr→Cl Pr→Rw Rw→Ar Rw→Cl Rw→Pr Avg
ResNet-50 38.5 31.2 60.4 53.9 41.2 59.9 47.5
DAN(Gaussian) 53.6 41.4 74.2 67.2 49.0 78.3 60.6
DANN 46.1 43.7 68.5 63.2 51.8 76.8 58.4
JAN 45.8 43.4 70.3 63.9 52.4 76.8 58.8
MCD 59.5 50.5 77.9 74.3 59.1 82.5 67.3
AFN 63.7 51.4 77.1 70.9 57.1 81.5 67.0
MDD 61.2 53.6 78.1 72.5 60.2 82.3 68.0
WDGRL 54.3 51.2 72.9 67.9 58.9 78.1 63.9
CAN 59.0 49.7 76.2 68.5 58.5 81.8 65.6
f−DAL 60.2 52.6 76.9 71.4 59.0 80.7 67.0
DAN(Laplacian) 53.4 40.7 73.3 66.7 48.5 78.1 60.1
TV 53.3 49.9 73.0 67.5 57.1 79.5 63.4
Neymanχ256.0 53.0 75.3 70.4 57.8 80.4 65.5
Reverse KL 60.7 47.0 79.7 72.2 58.6 82.2 66.7
SH 62.5 52.9 77.7 72.3 58.0 81.4 67.5
JS 61.7 52.1 76.3 71.5 57.2 80.8 66.6
KL 62.7 53.4 78.6 72.7 59.2 82.7 68.2
Pearsonχ263.0 54.3 80.0 73.9 60.6 83.6 69.2
We record the accuracy results on the Office-Home tasks in Table 4, the results on the Digits tasks in Table
5, and the results on the Office-31 transfer tasks in Table 6. We divide each table with a horizontal line to
12Published in Transactions on Machine Learning Research (12/2022)
Table 5: Accuracy (%) on the Digits datasets.
Method M →U U→M Avg
DAN (Gaussian) 88.9 87.6 88.3
DANN 91.6 95.9 93.8
JAN 84.8 90.7 87.7
MDD 93.6 97.395.5
MCD 94.7 96.7 95.7
AFN 89.9 96.3 93.1
WDGRL 97.4 94.195.8
CAN 90.9 94.1 92.5
f−DAL 91.7 95.9 93.8
DAN (Laplacian) 90.1 90.1 90.1
TV 91.3 96.6 94.0
Neymanχ292.4 95.3 93.9
Pearsonχ292.0 95.4 93.7
KL 93.0 95.8 94.4
Reverse KL 93.3 96.5 94.9
SH 94.8 97.3 96.1
JS 93.5 96.6 95.1
Table 6: Accuracy in (%) on the Office-31 datasets.
Method A →W D→W W→D A→D D→A W→A
ResNet-50 68.4 ±0.2 96.7±0.1 99.3±0.1 68.9±0.2 62.5±0.3 60.7±0.3
DAN(Gaussian) 84.3 ±0.8 98.5±0.4 100.0±0.0 86.1±0.6 65.6±0.5 64.4±0.4
DANN 82.0 ±0.4 96.9±0.2 99.1±0.1 79.7±0.4 68.2±0.4 67.4±0.5
JAN 85.4 ±0.3 97.4±0.2 99.8±0.2 84.7±0.3 68.6±0.3 70.0±0.4
MCD 88.6 ±0.2 98.5±0.1 100.0±0.0 92.2±0.2 69.5±0.1 69.7±0.3
AFN 92.5 ±0.7 98.9±0.2 100.0±0.094.8±0.3 72.7±0.4 70.8±0.6
MDD 94.5 ±0.3 98.4±0.1100.0±0.0 93.5±0.2 74.6±0.3 72.2±0.1
WDGRL 92.8 ±0.3 95.6±0.5 99.3±0.3 88.2±0.2 69.9±1.4 73.2±0.6
CAN 93.2 ±0.2 98.4±0.2 99.8±0.2 92.9±0.276.5±0.376.0±0.3
f-DAL 95.4±0.798.8±0.1 100.0±0.0 93.8±0.4 74.9±1.5 74.2±0.5
DAN(Laplacian) 83.0 ±0.8 98.6±0.3 100.0±.0 85.4±0.6 65.6±0.7 64.0±0.3
TV 91.2 ±1.8 98.3±0.1 100.0±.0 89.3±1.0 68.1±0.9 69.4±1.3
Neymanχ288.9±0.9 98.0±0.1 100.0±.0 89.8±0.8 72.7±1.6 72.7±1.3
Pearsonχ292.5±0.5 98.3±0.2 100.0±.0 87.7±0.4 72.3±0.674.4±0.2
KL 91.1 ±0.7 98.4±0.1 100.0±.0 88.1±0.7 75.0±0.00 74.3±0.1
Reverse KL 91.6 ±0.3 98.6±0.1 100.0±.0 88.6±0.8 75.1±0.1 74.0±0.4
SH 91.6 ±0.7 98.6±0.1 100.0±.0 89.9±0.8 75.0±0.1 74.1±0.4
JS 91.8 ±1.198.6±0.1100.0±.090.1±0.476.1±0.5 74.0±0.4
separately record the baseline models’ performance (above the line with the model name) and our proposed
models’ performance (below the line with the name of the divergence measure used). We highlight the
corresponding highest accuracy among the baseline models and among the models we propose.
From the 3 tables, we make the following observations:
(1) The framework we propose greatly enriches the transfer model choices. Furthermore, some models can
even beat the SOTA models. For example, in Table 4, the model with Pearson χ2distance achieves the
highest average accuracy score among all the DA models. In Table 5, the model using SH distance performs
13Published in Transactions on Machine Learning Research (12/2022)
the best on average. In Table 6, the model using JS divergence achieves the highest accuracy in 4 out of
6 tasks among all the models we propose. Such observations explicitly reveal the great potential of the
f-divergence family on transfer learning tasks, which fill in the blanks in literature.
(2) But we should also notice that in terms of each specific task in Table 4-6, there does not exist a typical
distribution divergence measure that can universally outperform. Besides, for some specific tasks (such as
Ar→Cl,Ar→Prin Table 4), other models rather than the model using Pearson χ2perform better. In
addition, there are also tasks in Table 6 where we fail to beat the baseline models. That reminds us to be
prudent in selecting a suitable DA model.
(3) Furthermore, DAN with Laplacian kernel MMD performs similarly to DAN with Gaussian kernel MMD,
and outperforms the latter one on the Digits dataset in Table 5. It suggests that a wider choice of the kernel
functions may be helpful, given that many previous works adopting MMD (Long et al., 2017; 2015; 2018)
only consider the Gaussian kernel MMD.
(4) We visualize the features learnt by the models using JS, SH and the other 7 baselines on Office-31 in
Figure 2 in the Appendix. It shows that the model with domain similarity measured by JS separates inputs
of different classes the best among all. That explains why the model with JS performs pretty well in Table
6.
6.2 Image Regression
In addition to the classification tasks, we also test the efficacy of the new DA model variants on two image
regressiondatasets: MPI3D anddSprites .MPI3D isasimulation-to-realdatasetof3Dobjects, consisting
of three domains: Toy ( T), RealistiC ( RC) and ReaL ( RL). Each domain contains 1,036,800 images, with
every image having two continuous factors of variations to be predicted: a rotation about a vertical axis
at the base and a second rotation about a horizontal axis. We evaluate all methods on six transfer tasks:
RL→RC, RL→T, RC→T, RC→RL, T→RL, and T→RC. For each task, we separately predict the two
rotation angles and record the sum of mean absolute error (MAE) over the two predictions as the transfer
performance of this task. dSprites is a 2D synthetic dataset composed of three domains: Color ( C), Noisy
(N) and Scream ( S). Each domain consists of 737,280 images with three variants of continuous factors to
be predicted: scale, position X, and position Y. We evaluate all methods on 6 transfer tasks: C→N,C→S,
N→C,N→S,S→C,S→N. Similar to the experiment on the previous dataset, we record the sum of MAE
over the three predictions in each transfer task as the transfer performance. All experiments in this section
are run with the same device as in the previous section.
We compare with the following DA models that can be applied to regression problems: DAN, DANN, MCD,
AFN and JDOT (Courty et al., 2017). For the MPI3D and dSprites experiments, ResNet-18 (He et al., 2016)
pretrained on ImageNet (Deng et al., 2009) is adopted as the backbone feature extractor. The regressors are
a 2-layer convolutional neural network with BatchNorm and ReLU as the activation function, followed by
an average pooling layer and a 1-layer neural network with width 1024. For models using f−divergences,
we introduce a domain critic inspired by (Nowozin et al., 2016) to estimate the f−divergence in a min-max
manner. The domain critic is a 2-layer convolutional neural network followed by a 1-layer neural network
with ReLU as activation functions. We solve the min-max problem using gradient reversal layers motivated
by (Ganin et al., 2016). We take the mini-batch SGD optimizer with the Nestorov momentum 0.95 and the
learning rates of the regressors are set to be 10 times that of the feature extractor.
We follow the standard experimental protocol for unsupervised DA and report the MAE on the two datasets
in Table 7 and 8. As in the previous tables, the baseline models’ performances are recorded above the line
with the model name and our proposed models’ performances are below the middle line with the name of
the divergence measure used. We highlight the corresponding lowest MAE among the baselines and among
the models we propose.
From Table 7 and 8, we can see that the framework we propose enables us to design more domain adaptation
regressionmodelswithbetterperformance. Forexample, inTable7, alltheproposedmodelsreducetheMAE
by a large margin with Total Variation divergence performing the best. In Table 8, our proposed models
also greatly improve the regression performance by lowering the MAE nearly by half. These all demonstrate
14Published in Transactions on Machine Learning Research (12/2022)
the powerful transferability of the proposed model guaranteed by our unified theoretical framework. We
should also notice that the performance of different f−divergences varies across datasets. For example, the
Total Variation divergence performs poorly in the previous two classification tasks, compared with other
f−divergences like Pearson χ2. However, the Total Variation divergence achieves the best results in the
MPI3D regression tasks. But for the second regression dataset dSprites, the KL divergence and the MMD
method with Laplacian kernel achieve the best. These inspire us to try as many as possible divergences in
the pilot study period to determine which is the best for our specific task. These could bring significant
improvement to the performance. And luckily, our theoretical framework provides a generalization guarantee
for a large variety of divergences, which greatly enriches people’s choices.
Table 7: MAE on the MPI3D dataset.
Method RL →RC RL→T RC→RL RC→T T→RL T→RC Avg
ResNet-18 0.17 ±0.02 0.44±0.04 0.19±0.02 0.45±0.03 0.51±0.01 0.50±0.03 0.377
DAN(Gaussian) 0.12 ±0.03 0.35±0.02 0.12±0.02 0.27±0.02 0.40±0.02 0.41±0.04 0.278
DANN 0.09±0.01 0.24±0.04 0.11±0.03 0.41±0.03 0.48±0.02 0.37±0.04 0.283
MCD 0.13 ±0.02 0.40±0.04 0.15±0.02 0.45±0.01 0.52±0.02 0.50±0.03 0.358
AFN 0.18 ±0.03 0.45±0.02 0.20±0.03 0.46±0.03 0.53±0.02 0.52±0.04 0.390
JDOT 0.16 ±0.02 0.41±0.01 0.16±0.02 0.41±0.02 0.47±0.02 0.47±0.02 0.353
DAN(Laplacian) 0.11 ±0.04 0.21±0.01 0.09±0.01 0.19±0.01 0.26±0.01 0.23±0.02 0.182
TV 0.06±0.00 0.18±0.01 0.07±0.02 0.13±0.02 0.18±0.01 0.16±0.01 0.132
Neymanχ20.09±0.02 0.19±0.00 0.09±0.01 0.18±0.01 0.19±0.00 0.19±0.00 0.157
Reverse KL 0.10 ±0.00 0.20±0.00 0.09±0.00 0.18±0.01 0.20±0.01 0.20±0.00 0.161
SH 0.11 ±0.00 0.21±0.01 0.10±0.01 0.20±0.00 0.21±0.00 0.21±0.01 0.176
JS 0.09 ±0.00 0.20±0.01 0.09±0.01 0.17±0.02 0.20±0.01 0.19±0.01 0.156
KL 0.10 ±0.01 0.19±0.00 0.10±0.01 0.18±0.01 0.20±0.00 0.20±0.01 0.161
Pearsonχ20.09±0.02 0.19±0.00 0.08±0.00 0.17±0.01 0.19±0.00 0.18±0.00 0.149
Table 8: MAE on the dSprites dataset.
Method C →N C→S N→C N→S S→C S→N Avg
ResNet-18 0.94 ±0.06 0.90±0.08 0.16±0.02 0.65±0.02 0.08±0.01 0.26±0.03 0.498
DAN(Gaussian) 0.70 ±0.05 0.77±0.09 0.12±0.03 0.50±0.05 0.06±0.02 0.11±0.04 0.377
DANN 0.47±0.07 0.46±0.07 0.16±0.02 0.65±0.05 0.05±0.00 0.10±0.01 0.315
MCD 0.81 ±0.09 0.81±0.12 0.17±0.12 0.65±0.03 0.07±0.02 0.19±0.04 0.450
AFN 1.00 ±0.04 0.96±0.05 0.16±0.03 0.62±0.04 0.08±0.01 0.32±0.06 0.523
JDOT 0.86 ±0.03 0.79±0.02 0.19±0.02 0.64±0.05 0.10±0.02 0.23±0.04 0.468
DAN(Laplacian) 0.18±0.01 0.28±0.00 0.08±0.00 0.23±0.01 0.05±0.00 0.07±0.01 0.151
TV 0.22 ±0.00 0.25±0.01 0.08±0.01 0.25±0.00 0.06±0.01 0.18±0.00 0.171
Neymanχ20.22±0.01 0.25±0.00 0.09±0.02 0.24±0.01 0.08±0.01 0.10±0.02 0.164
Reverse KL 0.22 ±0.01 0.25±0.01 0.10±0.02 0.22±0.04 0.10±0.04 0.08±0.02 0.161
SH 0.22 ±0.00 0.24±0.02 0.10±0.02 0.23±0.02 0.06±0.00 0.11±0.01 0.160
JS 0.22 ±0.00 0.24±0.01 0.08±0.00 0.23±0.02 0.05±0.00 0.11±0.01 0.155
KL 0.22 ±0.00 0.23±0.03 0.07±0.01 0.23±0.01 0.07±0.01 0.09±0.02 0.152
Pearsonχ20.23±0.03 0.25±0.00 0.08±0.01 0.23±0.03 0.11±0.03 0.11±0.00 0.168
7 Conclusion
In this work, we focus on the generalization bound for unsupervised domain adaptation (DA) problems. We
provide a unified generalization bound to bridge the theoretical gap between the stringent generalization
bound analysis in the pioneering work in Ben-David et al. (2007) and the diverse distribution-aligning DA
models in literature. This unified bound can well accommodate most commonly-used divergence measures
for both classification and regression tasks, and fully recover a large amount of previous models. We further
15Published in Transactions on Machine Learning Research (12/2022)
provide a comprehensive comparison of the commonly-used divergence measures in terms of the norm topol-
ogy, convergence rate and computation cost. In addition, we propose new DA models that are theoretically
guaranteed by the unified generalization bound. We empirically show that a wider range of divergence mea-
sures could greatly enrich the model choices and improve the model performance. Our work thus provides
theoretical support to many previous models that adopt specific divergence measures and sheds lights on
future studies of DA.
Acknowledgements
We thank the anonymous reviewers from TMLR and our Action Editor for their constructive feedback and
thorough suggestions for improvements.
Qi Wu acknowledges the support from the Hong Kong Research Grants Council [General Research Fund
14206117, 11219420, and 11200219], CityU SRG-Fd fund 7005300, and the support from the CityU-JD Digits
Laboratory in Financial Technology and Engineering, HK Institute of Data Science. The work described
in this paper was partially supported by the InnoHK initiative, The Government of the HKSAR, and the
Laboratory for AI-Powered Financial Technologies. Shumin Ma acknowledges the support from: Guangdong
Provincial Key Laboratory of Interdisciplinary Research and Application for Data Science, BNU-HKBU
United International College (2022B1212010006), Guangdong Higher Education Upgrading Plan (2021-2025)
of "Rushing to the Top, Making Up Shortcomings and Strengthening Special Features" with UIC research
grant (R0400001-22) and UIC (UICR0700019-22).
References
David Acuna, Guojun Zhang, Marc Teva Law, and Sanja Fidler. f-domain-adversarial learning: Theory and
algorithms. In ICML, 2021.
Martin Arjovsky, Soumith Chintala, and Léon Bottou. Wasserstein generative adversarial networks. In
International conference on machine learning , pp. 214–223, 2017.
Shai Ben-David, John Blitzer, Koby Crammer, Fernando Pereira, et al. Analysis of representations for
domain adaptation. Advances in neural information processing systems , 19:137, 2007.
Shai Ben-David, John Blitzer, Koby Crammer, Alex Kulesza, Fernando Pereira, and Jennifer Wortman
Vaughan. A theory of learning from different domains. Machine learning , 79(1):151–175, 2010.
Alain Berlinet and Christine Thomas-Agnan. Reproducing kernel Hilbert spaces in probability and statistics .
Springer Science & Business Media, 2011.
Xinyang Chen, Sinan Wang, Jianmin Wang, and Mingsheng Long. Representation subspace distance for
domain adaptation regression. In Marina Meila and Tong Zhang (eds.), Proceedings of the 38th Interna-
tional Conference on Machine Learning , volume 139 of Proceedings of Machine Learning Research , pp.
1749–1759, 18–24 Jul 2021.
Nicolas Courty, Rémi Flamary, Amaury Habrard, and Alain Rakotomamonjy. Joint distribution optimal
transportation for domain adaptation. In Proceedings of the 31st International Conference on Neural In-
formation Processing Systems , NIPS’17, pp. 3733–3742, Red Hook, NY, USA, 2017. ISBN 9781510860964.
ImreCsiszár. Aclassofmeasuresofinformativityofobservationchannels. Periodica Mathematica Hungarica ,
2(1-4):191–213, 1972.
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical
image database. In 2009 IEEE conference on computer vision and pattern recognition , pp. 248–255, 2009.
Nicolas Fournier and Arnaud Guillin. On the rate of convergence in wasserstein distance of the empirical
measure. Probability Theory and Related Fields , 162(3):707–738, 2015.
16Published in Transactions on Machine Learning Research (12/2022)
Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, François Laviolette,
Mario Marchand, and Victor Lempitsky. Domain-adversarial training of neural networks. The journal of
machine learning research , 17(1):2096–2030, 2016.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition.
In2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , pp. 770–778, 2016. doi:
10.1109/CVPR.2016.90.
GuoliangKang, LuJiang, YunchaoWei, YiYang, andAlexanderGHauptmann. Contrastiveadaptationnet-
work for single-and multi-source domain adaptation. IEEE transactions on pattern analysis and machine
intelligence , 2020.
Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to docu-
ment recognition. Proceedings of the IEEE , 86(11):2278–2324, 1998.
Chun-Liang Li, Wei-Cheng Chang, Yu Cheng, Yiming Yang, and Barnabás Póczos. Mmd gan: Towards
deeper understanding of moment matching network. In Proceedings of the 31st International Conference
on Neural Information Processing Systems , NIPS’17, pp. 2200–2210, 2017. ISBN 9781510860964.
Mingsheng Long, Yue Cao, Jianmin Wang, and Michael I. Jordan. Learning transferable features with deep
adaptation networks. In Proceedings of the 32nd International Conference on International Conference on
Machine Learning - Volume 37 , ICML’15, pp. 97–105, 2015.
Mingsheng Long, Han Zhu, Jianmin Wang, and Michael I. Jordan. Deep transfer learning with joint adap-
tation networks. In Proceedings of the 34th International Conference on Machine Learning - Volume 70 ,
ICML’17, pp. 2208–2217, 2017.
Mingsheng Long, ZHANGJIE CAO, Jianmin Wang, and Michael I Jordan. Conditional adversarial domain
adaptation. In Advances in Neural Information Processing Systems , volume 31, 2018.
Yishay Mansour, Mehryar Mohri, and Afshin Rostamizadeh. Domain adaptation: Learning bounds and
algorithms. In COLT, 2009.
Sebastian Nowozin, Botond Cseke, and Ryota Tomioka. f-gan: Training generative neural samplers us-
ing variational divergence minimization. In Proceedings of the 30th International Conference on Neural
Information Processing Systems , pp. 271–279, 2016.
Leandro Pardo. Statistical inference based on divergence measures . 2018.
Ievgen Redko, Emilie Morvant, Amaury Habrard, Marc Sebban, and Younès Bennani. A survey on domain
adaptation theory: learning bounds and theoretical guarantees. arXiv preprint arXiv:2004.11829 , 2020.
Paul Rubenstein, Olivier Bousquet, Josip Djolonga, Carlos Riquelme, and Ilya O Tolstikhin. Practical and
consistent estimation of f-divergences. Advances in Neural Information Processing Systems , 32:4070–4080,
2019.
Kuniaki Saito, Kohei Watanabe, Yoshitaka Ushiku, and Tatsuya Harada. Maximum classifier discrepancy for
unsupervised domain adaptation. In Proceedings of the IEEE conference on computer vision and pattern
recognition , pp. 3723–3732, 2018.
SwamiSankaranarayanan, YogeshBalaji, CarlosDomingoCastillo, andRamaChellappa. Generatetoadapt:
Aligning domains using generative adversarial networks. 2018 IEEE/CVF Conference on Computer Vision
and Pattern Recognition , pp. 8503–8512, 2018.
Igal Sason and Sergio Verdú. f-divergence inequalities. IEEE Transactions on Information Theory , 62(11):
5973–6006, 2016.
Jian Shen, Yanru Qu, Weinan Zhang, and Yong Yu. Wasserstein distance guided representation learning for
domain adaptation. In AAAI, 2018.
17Published in Transactions on Machine Learning Research (12/2022)
Bharath K Sriperumbudur, Kenji Fukumizu, Arthur Gretton, Gert RG Lanckriet, and Bernhard Schölkopf.
Kernel choice and classifiability for rkhs embeddings of probability distributions. In NIPS, volume 22, pp.
1750–1758, 2009a.
BharathK.Sriperumbudur, KenjiFukumizu, ArthurGretton, BernhardSchölkopf, andGertR.G.Lanckriet.
On integral probability metrics, ϕ-divergences and binary classification, 2009b.
Baochen Sun and Kate Saenko. Deep coral: Correlation alignment for deep domain adaptation. In European
conference on computer vision , pp. 443–450, 2016.
Eric Tzeng, Judy Hoffman, Ning Zhang, Kate Saenko, and Trevor Darrell. Deep domain confusion: Maxi-
mizing for domain invariance. arXiv preprint arXiv:1412.3474 , 2014.
Eric Tzeng, Judy Hoffman, Kate Saenko, and Trevor Darrell. Adversarial discriminative domain adaptation.
InProceedings of the IEEE conference on computer vision and pattern recognition , pp. 7167–7176, 2017.
Cédric Villani. Optimal transport: old and new , volume 338. 2009.
RuijiaXu, GuanbinLi, JihanYang, andLiangLin. Largernormmoretransferable: Anadaptivefeaturenorm
approach for unsupervised domain adaptation. In Proceedings of the IEEE/CVF International Conference
on Computer Vision , pp. 1426–1435, 2019.
Yuchen Zhang, Tianle Liu, Mingsheng Long, and Michael Jordan. Bridging theory and algorithm for domain
adaptation. In International Conference on Machine Learning , pp. 7404–7413, 2019.
A Appendix
A.1 Proof
Proof.(Theorem 3)
LQ(h,ht)≤LQ(h,h∗) +LQ(h∗,ht)
≤LP(h,h∗) +|LP(h,h∗)−LQ(h,h∗)|+ϵT
≤LP(h,hs) +LP(hs,h∗) +distH
L(P,Q) +ϵT
=LP(h,hs) +distH
L(P,Q) +ϵS+ϵT.
Proof.(Proposition 4) Given the kernel function k, by the description of maximum mean discrepancy in
Sriperumbudur et al. (2009b), MMD kcould be equivalently defined as
MMDk(P,Q) =sup
h∈H|Ex∼P(h(x))−Ex∼Q(h(x))|.
Note that∀h,h′∈H,h−h′is a function in RKHS. Further, we have
∥h−h′∥H≤∥h∥H+∥h′∥H≤2,
namely that h−h′is an element in RKHS with norm no greater than 2. Since there is an explicit bi-jection
betweenHand theH′:={h∈RKHS :∥h∥H≤2}, given byh∝⇕⊣√∫⊔≀→2h, we have
distH
L(P,Q) =sup
h,h′∈H|Ex∼P|h(x)−h′(x)|−Ex∼Q|h(x)−h′(x)||
=2×sup
h∈H|Ex∼Ph(x)−Ex∼Qh(x)|
=2MMDk(P,Q).
Thus we could conclude that in this case distH
Lis two times the MMD k.
18Published in Transactions on Machine Learning Research (12/2022)
Proof.(Proposition 6) We will refer to the proof of Theorem 3.1in Csiszár (1972). Let m= 1,wm=w0=
1,Π ={Q}, then in this case, Q∗=Q. Letδ=1
2,K= 2, and let/tildewidef(t) =tf(1
t), which is also continuous
fort>0and almost everywhere differentiable. Let /hatwideg(t) =/tildewidef′(t)at each point twhere ˜f(t)is differentiable.
Then,
αδ(u0) =min{˜f(u0+δ)−˜f(u0)−δ/hatwideg(u0),˜f(u0)−˜f(u0−δ)−δ/hatwideg(u0)}>0
foru0∈[δ,K]is continuous, and αδis positive and bounded in [δ,K]. Thus
ϵδ,K=1
δmin
δ≤u0≤Kαδ(u0)>0
is a positive finite constant depending on f. Here, we emphasize that the above constants w0,δ,ϵδ,Kare
independent of Q. Then for any distribution P, by Equation (3.25)of Csiszár (1972),
DTV(P,Q)≤1
ϵδ,K(1−δ)w0Df(P,Q).
Proof.(Lemma 7) Given any open ball
B=B(x0,r),x0∈X,r∈R+
in topology τ1, in order to show that Bis also open in τ2, we need to show that for any x1∈B, there is an
open ball in τ2that is inside Band contains x1. Indeed, for any x1∈B, denoteδ=d1(x0,x1), thenδ<r.
Consider the open ball B′={x∈X:d2(x1,x)<(r−δ
a)1
b}.It is direct that for any z∈B′, we have
d1(x0,z)≤d1(x0,x1) +d1(x1,z)
≤δ+ad2(x1,z)b
<δ+a×r−δ
a=r.
ThusB′⊂B, henceBis open inτ2. This shows that τ1⊂τ2.
Proof.(Proposition 8) Fix an integer N. Note that when distH
Ltakes the form of the total variation or
Wasserstein- 1distance, distH
Lis bounded. Thus distH
L(PN,P)as a function of x1,...,xNsatisfies the bounded
difference property. Indeed, for any (x1,...,xN),(x′
1,...,x′
N)∈XNthat only differs in the i-th coordinate
(namely,xj=x′
jfor allj̸=i), we denote their empirical distributions by PN,P′
Nrespectively, there exists
M > 0such that
|distH
L(PN,P)−distH
L(P′
N,P)|≤distH
L(PN,P′
N)≤M
N.
By McDiarmid’s inequality, with probability at least 1−δ, we shall have
distH
L(PN,P)≤EPNdistH
L(PN,P) +M/radicalbigg
1
2Nlog1
δ.
Thus we only need to figure out that how large EPNdistH
L(PN,P)is. If there exists a constant βand a
constantCsuch that
EPNdistH
L(PN,P)≤CN−β,
then the convergence rate will be min {1
2,β}. For the Wasserstein-1 distance, by Theorem 1in Fournier &
Guillin (2015), there exists a constant Csuch that
EPNW1(PN,P)≤CN−1
d.
For the total variation distance, by Theorem 1of Rubenstein et al. (2019), there exists a constant Csuch
that
EPNDTV(PN,P)≤CN−1
2.
19Published in Transactions on Machine Learning Research (12/2022)
Furthermore, by Proposition 6, for any f-divergence metric Dfsatisfying the assumptions, we shall have
DTV(P,Q)≤CfDf(P,Q).
ThusEPNDf(PN,P)must converge to 0with rateO(N−α)withα≤1
2. Otherwise, CfDf(PN,P)would
be smaller than DTV(PN,P)whenNis sufficiently large, which contradicts Proposition 6. Hence for such
metricDf, its convergence rate is no greater than1
2.
A.2 Supplementary figures
20Published in Transactions on Machine Learning Research (12/2022)
jensen
(a) JS
hellinger (b) SH
Wasserstein (c) WDGRL
DAN
(d) DAN
JAN (e) JAN
DANN (f) DANN
MDD
(g) MDD
MCD (h) MCD
AFN (i) AFN
Figure 2: t-SNE visualizations of features learnt from the 9 models on Office-31. The red (blue) points
represent the source (target) domain features and one clustering represents one class.
21