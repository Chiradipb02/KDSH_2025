Published in Transactions on Machine Learning Research (02/2024)
Data-Dependent Generalization Bounds for Neural Networks
with ReLU
Harsh Pandey bokharsh@gmail.com
Department of Computer Science
IIT Delhi
New Delhi, India
Amitabha Bagchi bagchi@cse.iitd.ac.in
Department of Computer Science
IIT Delhi
New Delhi, India
Srikanta Bedathur srikanta@cse.iitd.ac.in
Department of Computer Science
IIT Delhi
New Delhi, India
Arindam Bhattacharya arindambhattacharya@protonmail.com
Department of Computer Science
IIT Delhi
New Delhi, India
Reviewed on OpenReview: https: // openreview. net/ forum? id= mH6TelHVKD
Abstract
We try to establish that one of the correct data-dependent quantities to look at while trying
to prove generalization bounds, even for overparameterized neural networks, are the gradients
encountered by stochastic gradient descent while training the model. If these are small, then
the model generalizes. To make this conclusion rigorous, we weaken the notion of uniform
stability of a learning algorithm in a probabilistic way by positing the notion of almost sure
(a.s.) support stability and showing that algorithms that have this form of stability have
generalization error tending to 0 as the training set size increases. Further, we show that
for Stochastic Gradient Descent to be a.s. support stable we only need the loss function
to be a.s. locally Lipschitz and locally Smooth at the training points, thereby showing low
generalization error with weaker conditions than have been used in the literature. We then
show that Neural Networks with ReLU activation and a doubly differentiable loss function
possess these properties. Our notion of stability is the first data-dependent notion to be able
to show good generalization bounds for non-convex functions with learning rates strictly
slower than 1/tat thet-th step. Finally, we present experimental evidence to validate our
theoretical results.
1 Introduction
Deep neural networks are known to perform well on unseen data (test data), c.f. e.g. Jin et al. (2020)), but
theoretical explanations of this behaviour are still unsatisfactory. Under the assumption that the error on
the training set (empirical error) is low, studying the gap between the empirical error and the population
error is one route to investigating why this performance is good. In this paper we look at the gap between
the population error (risk) and empirical error (empirical risk) 1. Following works like Bousquet & Elisseeff
(2002), we use the term generalization error for this quantity. Chatterjee & Zielinski (2022) articulated the
1Published in Transactions on Machine Learning Research (02/2024)
Paper Number of
EpochsStep Size Neural Network
TypeKey Assumptions
Hardt et al. (2016) O(mc),c> 1/2O(1/t) No restrictions No data-dependence.
Kuzborskij &
Lampert (2018)1epoch O(1/t) No restrictions Bounded Hessian
Lei & Ying (2020) O(1) O(1/t) No restrictions Strongly convex
objective but non
convex loss function
Charles &
Papailiopoulos
(2018)O(m) O(1) 1-layered networks
with leaky ReLU or
linearPL and QG growth
conditions
Lei et al. (2022) O(m) O(1) 1-layered networks
with smooth
activation functionsSmooth loss function,
Bound in
expectation, lower
bound on number of
parameters
n>m3
(α+1),α> 0
Our Paper O(logm)O/parenleftig
1/t1−c
ρ(τ,m)/parenrightig
,
c∈(0,1)No restrictions Bounded Spectral
Complexity
Table 1: Recent related works addressing the question of generalization error and stability of neural networks
in comparison to the results in this paper.
main question as follows: why (or when) do neural networks generalize well when they have sufficient capacity
to memorize their training set? Although a number of formalisms have been used in an attempt to derive
theoretical bounds on the generalization error, e.g., VC dimension (Vapnik, 1998), Rademacher complexity
(Bartlett & Mendelson, 2003) and uniform stability (Bousquet & Elisseeff, 2002) but, as Zhang et al. (2017)
showed, all of these fail to resolve the conundrum thrown up by overly parameterized deep neural networks.
One clear failing identified in Zhang et al. (2017) was that many of these notions were data-independent. A
simple counterexample provided by Zhang et al. (2017) clearly established that a data-independent notion
was bound to fail to distinguish between data distributions on which deep NNs will generalize well and
those on which they will not. Further, the paper also raised doubts on the possibility of proving so-called
uniform convergence bounds for generalization error, i.e., bounds that were independent of the the size of the
training data. These doubts were concretized by Nagarajan & Kolter (2019) who constructed examples for
which the best possible algorithms were shown to not have uniform convergence bounds. We do not directly
address the issue of uniform convergence in this paper, instead focusing on the question that Chatterjee &
Zielinski (2022) formulated as follows: For a neural network, is there a property of the dataset that controls
the generalization error (assuming the size of the training set, architecture, learning rate, etc are held fixed)?
We give an affirmative answer to this question in one direction: We identify the data-dependent quantities,
namely SMSTrG, WCTrG and TeG and show that if these are bounded, we can guarantee the generalization
of neural networks. We show that the bound on these quantities depends on the difference between the initial
value and optimal value of the loss function and also do experiments to validate our results. Our techniques
are able to rigorously handle nonlinearities like ReLU and work for non-convex loss functions, and this holds
for classification case. We also allow for a learning rate that is asymptotically strictly slower than θ(1/t)at
thet-th step of SGD. All this holds for any bounded value loss function, which is twice differentiable.
2Published in Transactions on Machine Learning Research (02/2024)
Name Shorthand Mathematical Notation
Worst Case Training Gradient WCTrG LS
Second Moment of Step Training
GradientSMSTrG σS
Test Gradient TeG Lg
Training Smoothness Constant - - KS
Table 2: These are the important random variables which are used in the paper. All these are defined in
Section 4.1.1.
Our work is within the theoretical paradigm of stability. We asked the question, Is there an appropriate
version of stability that is flexible enough to incorporate dataset properties and can also adapt to most neural
networks? In a partial answer to this question, we introduce a notion called almost sure (a.s.) support
stability which is a data-dependent probabilistic weakening of uniform stability. Following the suggestions
made by Zhang et al. (2017), data-dependent notions of stability were defined in (Kuzborskij & Lampert,
2018, Definition 2) and (Lei & Ying, 2020, Definition 4) as well. However, a.s. support stability is a more
useful notion on three counts: it can handle SGD learning rates that are strictly slower than θ(1/t), its initial
learning rate is much higher, and, while these past works bound generalization error in expectation, a.s.
support stability can be used to show high probability bounds on generalization error. But, over and above
these technical benefits, our main contribution here is the identification of the data-dependent quantities as a
key indicator of generalization. Which in turn connects the generalization of neural networks to the difference
of initial and optimal loss values. A brief description of recent related works are summarized in table 1.
Earlier works for showing generalization (like Kuzborskij & Lampert (2018)) have a global Lipschitz constant
and show generalization using some other parameter. More recent works like Lei & Ying (2020) try to
completely remove the role of Lipschitz constants by taking some other assumptions on the structure or on
the loss function. We argue that the gradients are the correct quantities to look at for generalization.
We show generalization bounds for neural networks via two paths. One path uses Worst Case Training
Gradient (WCTrG) which is the the worst-case gradient across all steps of SGD and Test Gradient (TeG)
which is a gradient computed at the final parameter vector computed by the training. The other path uses
theSecond Moment of Step Training Gradient (SMSTrG) which is related to the second moment of the
gradients encountered during training and TeG. Using some results from Bottou et al. (2018) we show a
bound on SMSTrG in terms of the difference between the initial loss and optimal loss values. This directly
bounds the generalization error of neural networks in terms of this difference and makes this bound more
usable. We show examples of cases that take advantage of our bounds. We perform experiments to validate
the results and also empirically show that for random label case, WCTrG grows unboundedly, and so we
can’t guarantee generalization in this case, which is as expected.
We note that although we can say that when the data-dependent quantities are small, our results guarantee
good generalization performance, we do not establish that this condition is necessary.
In particular, our contributions are:
•In Section 3 we define a new notion of stability called a.s. support stability and show in Theorem 3.2 that
algorithms with a.s. support stability o(1/log2m)have generalization error tending to 0 as m→∞wherem
is the size of the training set.
•In Section 4 we first define the data-dependent quantities. We run SGD for τepochs with the slowest
learning rate of α0/t1−ρ(τ,m), whereρ(τ,m) =O(log logm/(logτ+logm)))for appropriate value of α0. For
reasonable values of mandτ, this marks a significant slowing down of the learning rate from θ(1/t). We use
two different ways to show a.s. support stability of SGD and we also show a bound on SMSTrG based on
SGD properties.
–In Section 4.1 we define the data-dependent quantities and show their existence.
3Published in Transactions on Machine Learning Research (02/2024)
–In Section 4.2 we show a.s. support stability of SGD using WCTrG and TeG. We show this for learning
rate ofα0/t1−ρ(τ,m), whereρ(τ,m)<1for reasonable size of training set ( m) and epochs ( τ) proportional
tolog(m).
–In Section 4.3 we show a.s. support stability of SGD using SMSTrG and TeG where τ= 1. This also
enjoys the small learning rate of α0/t1−ρ(τ,m).
–In Section 4.4, we show a bound on SMSTrG based on properties of SGD and a very minor assumption
(P1) as highlighted by Bottou et al. (2018). The main highlight of the bound on SMSTrG is that this
takes advantage of the fact that even for non-convex optimization the gradients of loss function decrease
as shown by Bottou et al. (2018).
•In Section 5, we combine results from Section 3 and 4 to show generalization error bound for neural
networks. We also translate these bound to the neural network setting. And show practical examples
highlighting the advantages of our data-dependent constants.
–In Section 5.1 using Section 4.2, we bound WCTrG by the spectral property (Proposition 5.3) and show
generalization bounds based on these spectral properties.
–In Section 5.2 using Sectoin 4.3 and 4.4, we show generalization via SMSTrG and show bound on it
which depends on the difference of initial loss and optimal loss values. The main advantage of this is
that this is much more practical and easy to verify as compared to the spectral property.
•Then, in Section 6, we experimentally verify the results showing that the bounded condition holds and plot
the generalization error. We also experimentally analyze the Test Gradient (TeG) for random labelling setting
suggested by Zhang et al. (2018) and conclude the Test Gradient is actually not bounded and increases with
the training set size. We relate this to the high variance of the loss function in random labelling case and
hence provide an explanation of which this example cannot be proved incorrectly to generalize using our
methods.
2 Related Work
Although NNs are known to generalize well in practice, many different theoretical approaches have been
tried without satisfactorily explaining this phenomenon, c.f., Jin et al. (2020); Chatterjee & Zielinski (2022).
We refer the reader to the work of Jin et al. (2020) which presents a concise taxonomy of these different
theoretical approaches. Several works seek to understand what a good theory of generalization should look
like, c.f. Kawaguchi et al. (2017); Chatterjee & Zielinski (2022). Our own work falls within the paradigm
that seeks to use notions of algorithmic stability to bound generalization error that began with Vapnik &
Chervonenkis (1974) but gathered steam with the publication of the work by Bousquet & Elisseeff (2002).
The applicability of the algorithmic stability paradigm to the study of generalization error in NNs was
brought to light by Hardt et al. (2016), who showed that functions optimized via Stochastic Gradient Descent
have the property of uniform stability defined by Bousquet & Elisseeff (2002), implying that NNs should
also have this property. Subsequently, there was renewed interest in uniform stability, and a sequence of
papers emerged using improved probabilistic tools to give better generalization bounds for uniformly stable
algorithms, e.g., Feldman & Vondrak (2018; 2019a) and Bousquet et al. (2020). Some other works, e.g.
Klochkov & Zhivotovskiy (2021), took this line forward by focussing on the relationship of uniform stability
with the excess risk. However, the work of Zhang et al. (2017) complicated the picture by pointing out
examples where the theory suggests the opposite of what happens in practice. This led to two different
strands of research. In one thread, an attempt was made to either discover those cases where uniform stability
fails (e.g. Charles & Papailiopoulos (2018)) or to show lower bounds on stability that ensure that uniform
stability does not exist (e.g. Zhang et al. (2022)). The other strand of research, a category in which our
work falls, focuses on weakening the notion of uniform stability, specifically by making it data-dependent,
thereby following the suggestion made by Zhang et al. (2017). (Kuzborskij & Lampert, 2018, Definition 2)
defined “on-average stability” which is weaker than our definition of a.s. support stability. Consequently,
their definition leads to a weaker in-expectation bound on the generalization error where the expectation is
over the training set as well as the random choices of the algorithm. Our Theorem 3.2, on the other hand,
provides a sharp concentration bound on the choice of the training set. (Lei & Ying, 2020, Definition 4)
4Published in Transactions on Machine Learning Research (02/2024)
define an “on-average model stability” that requires the average replace-one error over all the training points
to be bounded in expectation. While their smoothness requirements are less stringent, the problem is that
their generalization results are all relative to the optimal choice of the weight vector, which implies a high
generalization error in case of early stopping.
A key question in the study of generalization in NNs is that of the possibility of proving uniform convergence
bounds for norm-bounded classes of NNs, a question that can be traced to Bartlett (1996). This question
was answered negatively by Nagarajan & Kolter (2019) who showed that in general norm bounds on deep
networks show non-uniform growth, i.e., they grow with the training set size. For further discussion on this
question the reader is referred to Negrea et al. (2020a). Our current work is more focussed on identifying the
relevant quantities that bound generalization error, postponing the question of the exact relationship of these
quantities to the training set size to future study.
The role of the norm of gradients for bounding generalization error has been observed from a different angle by
Negrea et al. (2020b) and also by Haghifam et al. (2020). Their work focuses on giving information-theoretic
generalization bounds for Stochastic Gradient Langevin Dynamics (SGLD) and Langevin dynamics algorithm.
Although their setting and methods is different from ours, the general flavour is similar.
3 Almost Sure (a.s.) Support Stability and Generalization
In this section, we present a weakening of the notion of uniform stability defined by (Bousquet & Elisseeff,
2002, Definition 6) and show that exponential concentration bounds on the generalization error can be proved
for learning algorithms that have this weaker form of stability.
3.1 Terminology
LetXandYbe the input and output spaces respectively. We assume we have a training set S∈Zmof size
mwhere each point is chosen independently at random from an unknown distribution DoverZ⊂X×Y .
Forz= (x,y)∈Zwe will use the notation xzto denotexandyzto denotey. LetRbe the set of all finite
strings on some finite alphabet, and let us call the elements of Rdecision strings and let us assume that there
is some probability distribution Draccording to which we will select rrandomly fromR. This random string
abstracts the random choices of the algorithm. For example, in an NN trained with SGD it encapsulates the
random initial parameter vector and the random permutation of the training set as seen by SGD. For an
algorithm like Random Forest rwould abstract out the random points chosen to divide the space.
Further, letFbe the set of all functions from XtoY. In machine learning settings we typically compute a
map fromZm×RtoF. We will denote the function computed by this map as AS,r. Since the choice of S
andrare both random, AS,ris effectively a random function and can also be thought of as a randomized
algorithm.
Given a constant M > 0, we assume that we are given a bounded loss function ℓ:Y×Y→ [0,M]. We define
theriskofAS,ras
R(AS,r) =Ez∼D[ℓ(AS,r(xz),yz)],
where the expectation is over the random choice of point zaccording to data distribution D. Note that the
risk is a random variable since both Sandrare randomly chosen. The empirical risk ofAS,ris defined as
Re(AS,r) =1
|S|/summationdisplay
z∈Sℓ(AS,r(xz),yz).
We are interested in bounding the generalization error
|R(AS,r)−Re(AS,r)|. (1)
When talking about SGD we omit Aand just use R(S,r)andRe(S,r)to represent R(AS,r)andRe(AS,r)
respectively.
5Published in Transactions on Machine Learning Research (02/2024)
About the loss function l(·,·).When we talk about the loss function we refer to the commonly used loss
functions in machine learning, like cross-entropy, focal loss, mean squared error (for bounded inputs) etc.
Our results are valid for any bounded value loss function which is doubly differentiable. In machine learning
an implicit assumption is that the algorithm is able to successfully minimize the loss function chosen, i.e., a
loss function is used that can be minimized to a reasonable value over a training set. We also work with this
assumption.
3.2 A Weakening of Uniform Stability
GivenS={Z1,...,Zm}where all points are chosen randomly from D, we construct Sivia replacing the i-th
element ofSby an independently generated element from D. To quote it formally we choose {Z1+m,...,Z 2m}
points such that all are chosen randomly from Dsuch that they are independent from all points in S. For
eachi∈[m]we define
Si={Z1,...,Zi−1,Zi+m,Zi+1,...,Zm},
where [m]represents integer points from [1,m].
Definition 3.1 (Almost Sure (a.s.) Support Stability) .We say an algorithm AS,rhasalmost sure (a.s.)
support stability βwith respect to the loss function ℓ(·,·)if forZ1,...,Z 2mchosen i.i.d. according to an
unknown distribution Ddefined overZ,
∀i∈[m] :∀z∈supp (D) :Er/bracketleftbig
|ℓ(AS,r(xz),yz)−ℓ(ASi,r(xz),yz)|/bracketrightbig
≤β
with probability 1over the choice of points Z1,...,Z 2mwhere∀i,Zi∼Dor in other words {Z1,...,Z 2m}∼
D2m.
We note that this notion weakens the notion of uniform stability introduced by (Bousquet & Elisseeff, 2002,
Definition 6) by requiring the bound on the difference in losses to hold D2m- almost everywhere. This
probability is defined over the random choices of Z1,...,Z 2m. Besides the condition on the loss is required to
hold only for those data points that lie in the support of D. These conditions make a.s. support stability a
data-dependent quantity on the lines of the suggestion made by Zhang et al. (2017). We also observe that
a.s. support stability is comparable to but stronger than the hypothesis stability of Kearns & Ron (1999) as
formulated by Bousquet & Elisseeff (2002).
While the quantification of z, i.e.,∀z∼supp (D)appears to be a very strong condition it is a weakening
of uniform stability. In (Bousquet & Elisseeff, 2002, Section 5) it was shown that uniform stability (which
is∀z∼D) holds for several classical machine learning algorithms like soft margin SVM, bounded SVM
regression and regularized least square regression. Hence a.s. support stability also holds for these algorithms.
As we will see ahead, the weakening helps us fulfil key technical requirements when it comes to the study of
neural networks.
3.3 Exponential Convergence of Generalization Error
AlmostSure(a.s.) SupportStabilitycanbeusedinplaceofuniformstabilityinconjunctionwiththetechniques
of (Feldman & Vondrak, 2019a, Theorem 1.1) to give guarantees on generalization error for algorithms that are
symmetric in distribution . Afunction f(x1,...,xm)iscalledsymmetricif f(x1,...,xm) =f(σ(x1),...,σ (xm))
for any permutation σ. But if we have a function fwhich is not symmetric but the probability of choosing
any permutation of a given set of elements is equal then we use the term symmetric in distribution to refer to
such a function along with the distribution by which its inputs are picked. In (Bousquet & Elisseeff, 2002,
Section 2.1) the term “symmetric algorithm” was used but it was potentially misleading since what they
meant was symmetric in distribution in the sense that we have used it. Since SGD randomly permutes the
training points it is clearly symmetric in distribution .
In particular, we can derive the following theorem.
Theorem 3.2. LetAS,rbe an algorithm that is symmetric in distribution and has a.s. stability βwith
respect to the loss function ℓ(·,·)such that 0≤ℓ(AS,r(xz),yz)≤1for allS∈Zm, for allr∈Rand for all
6Published in Transactions on Machine Learning Research (02/2024)
z= (xz,yz)∈Z.Then, there exists a constant c>0independent of ms.t. for any m≥1andδ∈(0,1),
with probability 1−δ,
Er[R(S,r)−Re(S,r)]≤c/parenleftigg
βlog(m) log/parenleftigm
δ/parenrightig
+/radicalbigg
log(1/δ)
m/parenrightigg
.
The constant cis independent of mand, because our analysis is asymptotic in m, this is sufficient for us.
Proof outline. We give a high-level outline here. Our proof extends the proof of Feldman and Vondrak
((Feldman & Vondrak, 2019a, Theorem 1.1)) to accommodate the generalization of McDiarmid’s Lemma A.2
from (Combes, 2015, Proposition 2). Feldman & Vondrak (2019b) used two steps to get a better generalization
guarantee. The first step is range reduction, where the range of the loss function is reduced. For this, they
define a new clipping function in Lemma 3.1 Feldman & Vondrak (2019a) which preserves uniform stability
and hence it will also preserve a.s. support stability. They also use uniform stability in Lemma 3.2 Feldman
& Vondrak (2019a) where they show the shifted and clipped function will still be stable which is done by
applying McDiarmid’s inequality to βsensitive functions. Here use a modification of McDiarmid’s inequality
(Lemma A.2 given in Appendix A) to get bounds for a.s. support stability. The second step is dataset
size reduction (as described in Section 3.3 Feldman & Vondrak (2019a)) which will remain the same for a.s.
support stability as this only involves stating the result for a smaller dataset and the probability, and then
taking a union bound. Therefore both steps of the argument given in Feldman & Vondrak (2019a) go through
for a.s. support stability.
4 Almost Sure (a.s.) Support Stability of Stochastic Gradient Descent
In this section, we show how to bound the a.s. support stability of a model whose parameters are learned by
performing Stochastic Gradient Descent (SGD) using a training set. We will see that the bounds on stability
can be formulated in terms of the gradients encountered during training and the value of the gradient at the
end of the training. Since these gradients are determined by both the training set and the random choices
made by SGD, this section shows that stability can be understood not just by looking at the data distribution,
but by going beyond that and looking more carefully at the training process.
This section is organized as follows:
Section 4.1. We define three quantities associated with SGD that will be used to bound a.s. support
stability: Worst Case Training Gradient (WCTrG), the Test Gradient (TeG) and Second Moment
of Step Training Gradient (SMSTrG). We will also show that these quantities are bounded under
certain mild conditions in Section 4.1.2.
Section 4.2. We will show a bound on a.s. support stability that depends on WCTrG and TeG. This
bound is weak in the sense that WCTrG is related to the largest of the gradients encountered during
training and strong in the sense that the stability bound decreases as m−1.
Section 4.3. We will show a bound on a.s. support stability that depends on SMSTrG and TeG for one
epoch. This bound is stronger than the bound shown in Section 4.2 in the sense that SMSTrG is
associated with an averaging of the gradients encountered during training rather than the largest of
the gradient. However, the bound is weaker in the sense that it decreases as m−1/2.
Section 4.4. This section is devoted to making the bound presented in Section 4.3 more useable by
bounding Second Moment of Step Training Gradient in terms of the initial and final values of the
loss function.
Terminology. We assume that the learned function is parameterized by a vector w∈Rnfor somen≥1,
i.e., we have some fixed function f:Rn×X→Y . The training set is used to learn a suitable parameter
vectorw∈Rnsuch that the value f(w,xz)is a good estimate of yzfor allz∈Z. We assume that this value
ofwis learned by running SGD using a training set drawn from the unknown distribution. We will say that
7Published in Transactions on Machine Learning Research (02/2024)
the size of the training set is m, and the algorithm proceeds in epochsofmsteps each. The parameter vector
at steptis denoted by wtfor0≤t≤τ·m, whereτis the total number of epochs during training.
To frame the learned function output by this algorithm in the terms defined in Section 3.1, the random decision
stringrconsists of the pair (w0,π= (π0,...,πτ−1))which we also write as (rinit,rp), i.e., the random initial
parameter vector w0(orrinit) from which SGD begins and the sequence of τrandom permutations used in
theτepochs. The weights encountered at step tusing permutation πwill be represented as wt,π. For the
sake of brevity, except for Section 4.1.1 we omit the use of πand only write wt.
4.1 Some quantities associated with SGD gradients
4.1.1 Definitions
Definition 4.1. IfΠis the set of all permutations of S, then SGD as defined above has the following
quantities associated with it
1.Worst Case Training Gradient (WCTrG) LS.
Lz,A= max
w∈A/braceleftbigg/vextenddouble/vextenddouble/vextenddouble/vextenddouble∂
∂wf(w,z)/vextenddouble/vextenddouble/vextenddouble/vextenddouble/bracerightbigg
.
Where the set A=∪π∈Πτ{w1,π,...,wτm−1,π}, i.e. all the parameter vectors encountered across all
time steps across all possible permutations of S.LS=maxz∈S{Lz,A}which is a random variable
depending on the choice of Sandw0.
2.Training Smoothness Constant KS.
Kz,A= max
w,w′∈A2
w̸=w′/braceleftbigg∥f′(w,z)−f′(w′,z)∥
∥w−w′∥/bracerightbigg
.
Where the set A=∪π∈Πτ{w1,π,...,wτm−1,π}. We will say that KS=maxz∈S{Kz,A}. Note that
KSis a random variable depending on the choice of Sandw0.
3.Test Gradient (TeG) Lg.
Lz,g= max
w,w′∈A2
w̸=w′/braceleftbigg∥f(w,z)−f(w′,z)∥
∥w−w′∥/bracerightbigg
.
Where the set A=∪π∈Πτ{wτm,π}, i.e. all parameter vectors encountered at the end of the training
process across all possible permutation of S. We will say that Lg=maxz∈Z{Lz,g}. Note that Lgis
a random variable depending on the choice of Sandw0.
4.Second Moment of Step Training Gradient (SMSTrG) σS. This is defined for τ= 1.
σz=/radicaltp/radicalvertex/radicalvertex/radicalbt1
mm/summationdisplay
t=0/vextenddouble/vextenddouble/vextenddouble/vextenddoublemax
w∈At/braceleftbigg∂
∂wf(w,z)/bracerightbigg/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
.
Where the set At=∪π0∈Π{wt,π0}i.e. set of all possible parameter vectors encountered during
training at step tof SGD across all permutations. We say that σS=maxz∈S{σz}. Note that σSis a
random variable in Sandw0.
Our stability analysis of SGD will depend on these quantities. However, when we take this analysis and try
to apply it to NNs with discontinuous activation functions like ReLU, Training Smoothness Constant and
Test Gradient defined above are not guaranteed to be bounded. So, it is necessary to establish the conditions
under which they are bounded.
8Published in Transactions on Machine Learning Research (02/2024)
4.1.2 Boundedness
We now present the property under which the quantities defined above will be (almost surely) bounded.
Definition 4.2 (Almost surely Locally Parameter Lipschitzness of parametrized functions) .A parameterized
functionf:Rn×Z→ Ris said to be almost surely locally L-parameter Lipschitz w.r.tDif for a fixed
w∈Rnand forz∼Dthere exist constants L>0andϵ>0such that, with probability 1 (over the choice of
z), for allw′∈Rn,∥w′−w∥<ϵimplies
∥f(w′,z)−f(w,z)∥≤L∥w′−w∥.
We will use the abbreviation a.s.L-LPLor simply a.s. LPL for a function that satisfies Definition 4.2. Since
training is always run for a finite number of steps it is easy to observe that if the loss function is a.s. LPL,
thenLSandσSare bounded.
If∇wf(·,·)satisfies Definition 4.2 we will call such a function almost surely locally L-parameter Smooth ora.s.
L-LPSfor short. We now see that these two properties imply the almost sure boundedness of the quantities
defined in Section 4.1.1. The important insight is that if the function (or its gradient) is locally bounded,
and, if we only look at this function at a finite number of points, we get a “global” property within this finite
set of points. We state this as a lemma.
Lemma 4.3. Iff:Rn×Z→ Ris bounded and a.s. Ll-LPL, and Ais a finite subset of Rn, then with
probability 1 there is an L>0such that for every pair w,w′∈A, ifz∼D, then
∥f(w,z)−f(w′,z)∥≤L∥w−w′∥,
The proof is in Appendix B.
Corollary 4.4. Iff:Rn×Z→ Ris a.s.L-LPL then Lgis almost surely bounded. Further, if fis a.s.
K-LPS thenKSis almost surely bounded.
The corollary follows by observing that due to the finiteness of the training process, the parameters on which
the slopes are computed in Definition 4.1 are all drawn from finite sets.
4.2 Almost Sure (a.s.) Support Stability of SGD with WC TG
We now work towards the a.s. support stability of SGD using WCTrG and TeG. First, we state a theorem
that bounds the replace-one error of SGD up to a certain number of epochs. To make the theorem statement
easier to read, we first separate out our assumptions.
S1. We are given a space Z=X×Yand a probability distribution Ddefined over it. We have a
parameterized loss function f:Rn×Z→ Rthat is a.s. Ll-LPL w.r.t supp (D)and a.s.Kl-LPS
w.r.tS.
S2. For a training set Sof sizemfor eachi∈[m]chosen i.i.d. according to Dwe run SGD on fforτ
epochs with random decision string r. Parallelly, we do the same for set Siwith the same set of
random decision string r. ForSi, thei-th data point ziofShas been replaced by another data point
z′
ichosen from Dindependent of all other random choices.
S3. At step tof SGD let the learning rate αt≤α0/t(1−ρ(τ,m)),ρ(τ,m) =log logm
logτ+logm,wtandw′
tthe parameter
vectors obtained for the t-th step of SGD while training with set SandSirespectively.
Theorem 4.5. Given assumptions S1, S2 and S3, we have LSas WCTrG, Lgas TeG and KSas Training
Smoothness Constant. Let the random decision string r= (w0,π0), then with probability 1 over zwe have,
Er[f(wτm,z)−f(w′
τm,z)]≤2τα0·Er/bracketleftigg
LSLg·U(α0,KS,ρ(τ,m))
m/parenleftbig
1−α0KS
ρ(τ,m)/parenrightbig/bracketrightigg
, (2)
whereU(α0,KS,ρ(τ,m))≤1 +1
KSα0,and asα0→0, U(α0,KS,ρ(τ,m))→1 +mρ(τ,m)
ρ(τ,m).
9Published in Transactions on Machine Learning Research (02/2024)
Here, the expectation is over rfor random variables LS,LgandKS. Remember, the L.H.S. is still a random
variable in Sand a.s. support stability is almost surely over this S.
Proof outline. The proof follows the lines of the argument presented by (Hardt et al., 2016, Theorem 3.12)
with the difference that we allow for a probabilistic relaxation of the smoothness conditions and more
relaxed constraint on gradient bounds in line with our definition of a.s. stability. Also, note that we have to
account for an expectation over the random string rand that we have been able to extend the argument to
multiple epochs which was not possible in (Kuzborskij & Lampert, 2018, Theorem 4). The complete proof of
Theorem 4.5 is in Appendix B.
Data-dependence with WCTrG and TeG . A key feature of the bound presented in equation 2 is that the
dependence on the data is expressed through the data-dependent quantities WCTrG ( LS) and TeG ( Lg).
The WCTrG depends on the gradients at training points and the replacement point z′
i, which is also picked
from the data distribution, and TeG depends on the gradients of the trained network calculated at points
from distribution. The advantage of splitting TeG and WCTrG instead of a global bound is that TeG could
potentially be very small when the loss is converged as it’s calculated at the end of training. And, WCTrG
only depends on training points instead of the entire distribution.
Corollary 4.6. Given assumptions S1, S2 and S3, and under the condition that KSis a constant, w.r.t. m,
for allr, andEr[LgLS]is also constant w.r.t m, there is a constant c∈(0,1)that depends on α0andKS
such that if the number of epochs τis at mostclogmepochs, the expectation of the generalization error of the
algorithm taken over the random choices of the algorithm decreases as ˜O/parenleftbig
m−min(ϵ,1/2)/parenrightbig
(where tilde hides the
logarithmic factors) with probability at least 1−1/mover the choice of the training set ifα0KS
ρ(1,m)+clog2<1,
whereϵ= 1−clog 2−α0KS
ρ(1,m).
Proof.Let us consider two cases. In the first case when ϵ>1/2(i.e. we get the usual rate ˜O(m−1/2)), this
happens when α0<ρ(1,m)
2KSand we choose a small enough c. One the other hand for case where ϵ<1/2(i.e.
rate of ˜O(m−ϵ)), which allows for a larger learning rateρ(1,m)
2KS< α 0<ρ(1,m)
KS(for small enough c). This
clearly shows that a larger initial learning rate could be bad for generalization. It is easy to check from
Theorem 4.5 that with the conditions given in the statement of Corollary 4.6 the learning algorithm has a.s.
support stability βwhereβiso(1/mϵ)ifα0KS
ρ(1,m)+clogm< 1. We can therefore apply Theorem 3.2 with
δ= 1/mto get the result.
4.3 Almost Sure (a.s.) Support Stability of SGD with SMSTrG
We present a more fine-grained analysis that provides an alternate bound of Theorem 4.5 by replacing the
WCTrG (Worst Case Training Gradient), which was defined for all SGD steps, with the SMSTrG (Second
Moment of Step Training Gradient). The significance of this is that this expected value could be substantially
smaller than the worst case value in many cases, especially when the training converges rapidly from a high
value of the loss to a low value. We present the bounds using SMSTrG below.
Theorem 4.7. Given the assumption S1, S2 and S3 for τ= 1, we haveσSas SMSTrG, Lgas TeG and KS
as Training Smoothness Constant. Let the random decision string r= (w0,π0), then with probability 1overz
we have,
Er[f(wm,z)−f(w′
m,z)]≤α0·Ew0/bracketleftigg
Lg·σS·/radicalbig
U′(α0,KS,m)
m1
2−α0KS(logm−1)
log logm/bracketrightigg
. (3)
WhereU′(α0,KS,m)≤1 +1
2α0KSand asα0→0,U′(α0,KS,m)→1 +log(m)2
log logm.
Proof outline. The proof follows the proof of Theorem 4.5, but instead of using the WCTrG for bounding the
gradients encountered at the step where the training points differ, we use the exact gradient at that step, i.e.
f(wπ0(i),·)for every permutation π0∈Π. Then, the proof continues till msteps. Finally, when taking the
expectation over the permutations we apply Cauchy Schwarz to separate other random variables and obtain
the SMSTrG ( σS). The complete proof of Theorem 4.7 is in Appendix B.
10Published in Transactions on Machine Learning Research (02/2024)
Although the bound in Theorem 4.7 looks weaker than the bound in Theorem 4.5 because of m1
2term, the
key part here is the use of SMSTrG instead of WCTrG. SMSTrG could potentially be a lot smaller because
it’s averaged over the training steps. This will especially benefit the case when training converges fast and
gradients quickly become small. We later in Section 4.4 show that we can bound SMSTrG by the decrease in
loss during training under some very mild conditions of assumption P1.
We now present the alternate version of Corollary 4.6 using Theorem 4.7.
Corollary 4.8. Given assumption S1, S2 and S3 for τ= 1, and under the condition that KSis constant
w.r.t.mfor allr= (w0,π0), andEw0[Lg·σS]is also constant w.r.t. m, the expectation of generalization
error of algorithm taken over the random choices of the algorithm decreases as ˜O(mϵ)(were tilde hides the
logarithmic factors) with probability at least 1−1/mover the choice of training set ifα0KS(logm−1)
log logm<0.5,
whereϵ= 0.5−α0KS(logm−1)
log logm.
Proof.It’s direct to see when we apply the conditions stated in the above corollary in Theorem 4.7 we get
the desired result.
4.4 Bound on SMSTrG
We now proceed to show the bound on SMSTrG (i.e. Second Moment of Step Training Gradient) along with
the assumption required for the bound to hold. For this, we use a result from Bottou et al. (2018) and modify
it slightly to show a bound on the expectation of SMSTrG over Sandw0. Although this is a bound on
expectation, we later show in the discussion of Section 5.2.2 that we can use this as a high probability bound.
And this is sufficient because of the probabilistic form of a.s. support stability.
First we define FS(wt) =1
m/summationtextm
i=1f(wt,zi). Now, in order to bound the expectation of SMSTrG over the
training set, we use the result from (Bottou et al., 2018, Theorem 4.10) and modify it to get the required
bound. This theorem, combined with Assumption P1, provides a bound on the expectation of SMSTrG over
the training set (Corollary 4.10). The bound is in terms of the initial and optimal (final) values of the loss
function (and includes terms based on the learning rate), so unless the initial loss is very bad, the bound will
be useful. The theorem requires the following condition which is a kind of version of a bound on the variance
of the gradients encountered during training.
P1. (Assumption 4.3 (equation 4.9) of Bottou et al. (2018)). There exist constants M≥0and
MG≥0(independent of the size of the training set) such that,
1
m/summationdisplay
z∈S∥∇f(wt,z)∥2≤M+MG∥∇FS(wt)∥2. (4)
Theorem 4.9. (Bottou et al., 2018, Theorem 4.10) Suppose we run SGD under the assumption that KSis
constant for all rand P1 holds, for M≥0,MG≥0. Ifw0is the initialization weight, w∗is optimal value
forFS(w)andαtis any diminishing step size then,
T/summationdisplay
t=1αtE/bracketleftbig
∥∇FS(wt)∥2/bracketrightbig
≤2(E[FS(w0)]−E[FS(w∗)]) +E[KS]MT/summationdisplay
t=1α2
t, (5)
whereFS(wt) =1
m/summationtextm
i=1f(wt,zi).
Now, using Theorem 4.9 and assumption P1 we can get a bound on the expectation of SMSTrG. We state
this in the next corollary.
Corollary 4.10. If we run SGD for 1epoch under the assumption that KSis constant for all rand P1 holds,
forM≥0,MG≥0,w0is the initialization weight, w∗is optimal value for Ez∈S[f(w,z)],r= (w0,π0)is
the random string for SGD and αtis any diminishing learning rate then,
ES,w0[σS]≤M+ 2MGE[f(w0,z)−f(w∗,z)] +E[KS]MGM
α0T/summationdisplay
t=0α2
t (6)
11Published in Transactions on Machine Learning Research (02/2024)
The proof of this theorem is in Appendix B at the end. The R.H.S of this equation is constant w.r.t. mif/summationtextT
t=1α2
tis constant w.r.t. mand in our analysis this is true (c.f. assumption S3). Hence, this corollary
becomes useful for us since it shows that we can bound the expectation of SMSTrG under a mild assumption.
5 Neural Networks with ReLU Activation
We now proceed to show what the a.s. support stability bounds on SGD translates into for neural networks.
This includes two generalization bounds, one using WCTrG (i.e. Worst Case Training Gradient) in Section
5.1 and the other using SMSTrG (i.e. Second Moment of Step Training Gradient) in Section 5.2. We also
emphasise on the conditions required for generalization bounds to hold and translate those conditions to
more practical constraints.
5.1 Almost Sure (a.s.) Support Stability of Neural Networks with ReLU Activation: WCTrG
First, we present the generalization bound of neural networks via a.s. support stability using WCTrG (Worst
Case Training Gradient) in Section 5.1.1. We then, in Section 5.1.2, show the conditions required for the
bounds to hold. We also present a discussion at the end highlighting an example where the actual data
dimension is small, and our gradient bounds will be better.
5.1.1 Generalization Using WCTrG.
For ease of reading, we first state our assumptions and then, in Theorem 5.1 we present the bound.
N1.We have a fully connected neural network with ReLU activation and 1 output neuron.
N2.The NN is trained on set S∼Dmusing SGD for τepochs, where Dis over Rd×Y, such thatYis
countable and for each y∈Ywe get a countable set {x∈Rd:PrD{lab(x) =y}>0}, where lab(x)
is label of x.
N3.We have a doubly differentiable loss function with bounded first and second order derivatives and
learning rate αt=α0/t(1−ρ(τ,m)),whereρ(τ,m) =log logm
logτ+logmand the data points of Sand the
spectral norms of weight matrices explored by SGD are bounded
Theorem 5.1. If N1, N2 and N3 hold, then there is a constant c>0such that
Er[|R(S,r)−Re(S,r)|]≤c/parenleftigg
2τα0·Er[LSLg]·U(α0,KS,ρ(τ,m))log(m)2
m/parenleftbig
1−α0KS
ρ(τ,m)/parenrightbig+/radicalbigg
log(m)
m/parenrightigg
,
with probability at least 1−1/m, whereU(α0,KS,ρ(τ,m))≤1 +1
α0KSandα0→0implies
U(α0,KS,ρ(τ,m))→1 +mρ(τ,m)
ρ(τ,m).
The proof of the above theorem is at the end of Section 5.1.2 before discussion. Note that for some c1log(m)
epochs and with an initial learning rate of α0such thatα0KS
ρ(1,m)+c1log2<1, the RHS decreases as m
increases. It is important to note that TeG ( Lg) is the constant that depends on the actual distribution D
and is calculated for a trained neural network (i.e. at wτm). This aligns with the notion that if the network
has reached a “good enough” minima, then the gradient values should be less; hence, this will show better
generalization. Also, WCTrG ( LS) and Training Smoothness Constant ( KS) depend on the training set S.
These are “global” over the data set in the sense that the expectation is for the entire training process over
the (random) choice of initial parameters and the permutation that SGD chooses. For cases where SGD
chooses a good set of initial parameters with good probability, these are likely to be small.
For Training Smoothness Constant ( KS), we are constrained in the sense that we need this value to be small
throughout the training, even at the beginning. Also it is interesting to note that when LS→0andLg→0
the generalization error becomes zero, but when KS→0, we haveU(α0,KS,ρ(τ,m))→1 +mρ(τ,m)
ρ(τ,m)which
12Published in Transactions on Machine Learning Research (02/2024)
leads to generalization error behave like O/parenleftbigg
logm3
m+/radicalig
logm
m/parenrightbigg
. Although this is still a decreasing function of
m, this does not directly go to zero. This highlights that unlike LSandLg, justKS→0alone is insufficient
for zero generalization error.
The role of gradients in bounding generalization error that we identify has the same flavour, in a very different
context, as the work of (Negrea et al., 2020b, Theorem 3.1) and (Haghifam et al., 2020, Theorem 4.2) where
Information-Theoretic generalization bounds for SGLD (Stochastic Gradient Langevin Dynamics) are given
in terms of gradient values of the loss function. Next in Section 5.1.2, we first establish that the theory of
a.s. support stability applies to NNs under conditions specified, then we prove the Theorem 5.1 along with
empirical validation.
5.1.2 Conditions on NNs for Generalization
The key to showing the a.s. support stability of NNs with ReLU is to establish that they are locally
parameter-Lipschitz and locally parameter-smooth. First, we show the existence of these constants. Then we
will show an upper bound under some reasonable assumptions and finally present an example in discussion.
Theorem 5.2. For everyw∈Rn, a doubly differentiable loss function, ℓ:R×R→R, applied to the output
of a NN with ReLU activation is locally parameter-Lipschitz and locally parameter-smooth for all x∈Rd
except for a set of measure 0.
Proof outline. The proof of this theorem is based on the argument that for a given wa point of discontinuity
exists at a given neuron if the input xlies in the set of solutions to a family of equations, i.e., in a lower
dimensional subspace of Rd. This proof is an adaption of an idea of (Milne, 2019, Lemma 1) and can be
found in Appendix C.2.
Theorem 5.2 begs the question: How large are these WCTrG, TeG and Training Smoothness Constant? We
provide some general bounds that can be improved for specific architectures:
Proposition 5.3. Suppose we have a fully connected NN of depth H+ 1, with ReLU activation at the inner
nodes. Then, if the spectral norms of weight matrices are bounded for every layer i.e., ∥Wi∥σis bounded
∀i∈[H], and the size of each layer be {l0,...,lH}and the distribution of dataset is normalized with ∥x∥2≤1
then,
Lg≤/parenleftiggH/productdisplay
k=1∥Wk∥σ/parenrightigg
×A(M,W )1/2(7)
KS≤/parenleftiggH/productdisplay
k=1∥Wk∥σ/parenrightigg
×A(M,W ) (8)
where
A(M,W ) =H/summationdisplay
l=1∥Ml∥2
2,2
∥Wl−1∥2σ·∥Wl∥2σ·∥Wl+1∥2σ
where (i,j)thelement of matrix Ml[i,j] =∥M′(l,i,j)∥σ, and where M′(l,i,j)is a matrix such that (p,q)th
element isM′(l,i,j)[p,q] =w(l+1)
j,pw(l−1)
q,i. Note that equation 7 holds for both WCTrG ( LS) and TeG ( Lg).
The proof of the proposition is in Appendix C.2. Note that it’s possible to give a tighter bound for the above
theorem by not bounding the product of weight matrices (which we do after equation 19 in Appendix), but
we keep the above equation because of its clarity. The bounds on these should be compared to the bounds
given in the context of Rademacher complexity by (Bartlett et al., 2017, Equation 1.2) and Golowich et al.
(2018). Our bound is related to the spectral complexity and can potentially be independent of the size of the
network. We are now ready to prove our main theorem.
13Published in Transactions on Machine Learning Research (02/2024)
Proof of Theorem 5.1. Theorem 5.2 tells us that a NN with ReLU activations is locally parameter-Lipschitz
and locally parameter-smooth. From Proposition 5.3, we see that the boundedness of the first and second
derivatives of the loss function and the boundedness of the spectral norm of weight matrices and data points
ensures that the gradient bound and smoothness constants associated with the NN’s training are bounded
w.r.t.m. With all these in place, we can apply Theorem 4.5 to get the a.s. support stability followed by
Theorem 3.2 to get the desired result.
Discussion: An example showing the benefits of data-dependent gradient bounds. In general,
data-dependent gradients (WCTrG and TeG) can be much smaller than the global Lipschitz constant of
the space from which the data might appear. There are probably many scenarios in which this can be
demonstrated, but we turn to a well-appreciated scenario: a data set that has much smaller dimensionality
than the space in which it is embedded. We will now argue that in such scenarios, data-dependent gradients
(WCTrG and TeG) can be significantly smaller.
Suppose we have data as x∈Rd, but the actual dimension of the data is D≪d, a situation that is often
seen in many cases, for example, image data. For simplicity of presentations, we assume that each data point
hasx1,...,xDnon-zero and the remaining coordinates are 0. The arguments we make can be made even
without this assumption by considering the data points with coordinates based on their projection onto a
basis of the subspace they are taken from.
Suppose we have a neural network with 1hidden layer of d1neurons and a single output layer. Let W1∈Rd1×d
andW2∈R1×d1be the weights of 1stand2ndlayer respectively and we use w(l)
i,jto represent i,jweight of
lthlayer. For simplicity, let the output of the neural network O(w,x) =W2W1x. Now, assuming MSE loss,
we calculate the gradients and show that the effective upper bound of this could be smaller because of the
fact that our WCTrG and TeG are calculated only from Sandsupp(D). This is under the assumption that
the weights are upper bounded by some quantity B1. We will also assume that all data points have been
re-scaled so that their norm is at most 1.
Theorem 5.4. If the above conditions holds then we have a bound on ℓ2norm of the gradients of the
parameter vector was,
∥∇wf(w,x)∥2
2≤B2
1·d1·D(1 +D)
Proof of the Theorem 5.4 is in Appendix C.2.
Here, we see that we obtain a bound on the norm of the gradients that are related to D, which is significantly
smaller than d, whereas in general, we can expect the norm of the gradients to be of the order of deven
under the assumptions of bounded weights and rescaled data points.
Note that we show here the value of WCTrG ( LS) and TeG ( Lg), but for generalization, we actually need
Er[LS·Lg]to be bounded. Using Cauchy-Schwarz inequality we could see that we need bound on just the
expectation of square (or the second moment) of each term. This means that when we select the initial weight
parameter vector w0, we need the boundedness constraints on weights only, which is a fairly mild constraint.
5.2 Almost Sure (a.s.) Support Stability of Neural Networks with ReLU Activation: SMSTrG
This Section focuses on stating the generalization bound of neural networks via a.s. support stability using
the SMSTrG (i.e. Second Moment of Step Training Gradient). It’s already shown in Bottou et al. (2018)
that even for non-convex cases, the gradients of SGD decrease as the training proceeds. But earlier analysis
and even Section 5.1 could not use this fact. So, we use the SMSTrG (i.e. Second Moment of Step Training
Gradient), which will help us exploit this fact and provide a generalization bound for NNs. In Section 5.2.2,
we present the proof and discuss the conditions under which our bound holds. We also present a real-world
example highlighting the applicability of our results.
5.2.1 Generalization Using SMSTrG
We use the same conditions as defined in Section 5.1.1. Now, applying this to NNs and using SMSTrG, we
get a generalization bound, which is an alternate version of Theorem 5.1 for the 1-epoch case.
14Published in Transactions on Machine Learning Research (02/2024)
Theorem 5.5. If N1, N2 and N3 hold there is a constant c>0such that for all i∈[m]
Er[|R(S,r)−Re(S,r)|]≤c/parenleftigg
α0·/radicalig
Ew0/bracketleftbig
L2g/bracketrightbig
Ew0[σS]·U′(α0,KS,S)log(m)2
m/parenleftbig
1
2−α0KS(logm−1)
log logm/parenrightbig+/radicalbigg
log(m)
m/parenrightigg
,
with probability at least 1−1/m, whereU′(α0,KS,m)≤1+1
2α0KSand asα0→0,U′(α0,KS,m)→1+log(m)2
log logm
Note the presence of SMSTrG ( σS) on the R.H.S here as opposed to WCTrG ( LS) in the statement of
Theorem 5.1. We propose this is a much more practical quantity to bound. We have already shown in
Section 4.4 an expectation bound on SMSTrG by the difference between the initial and optimal values of the
loss function.
5.2.2 Conditions on NNs to Hold for Generalization
Proof outline of Theorem 5.5. Using Corollary 4.10 we already have a bound on expectation of SMSTrG.
So instead of Proposition 5.3, we use this Corollary to achieve the result. Although note that the result is
in expectation over the training set, we can simply apply Markov inequality as done in the discussion after
Theorem 5.6 to get a probability bound. This is sufficient because of the probabilistic form of a.s. support
stability. Also, the bound on the expectation of SMSTrG required only assumption (P1), which is a relatively
mild assumption.
It’s important to note that, like in the previous case (Section 5.1.2), we required the spectral norm of weight
matrices to be bounded, but it’s a difficult condition to ensure while training a neural network. On the other
hand, the bound on the expectation of SMSTrG depends on the difference between the initial and optimal
loss values, which is very easy to ensure and verify. This means we get a reasonable bound unless we have a
very bad initial loss. Moreover, the assumption P1 in Section 4.4 is also intuitive as a very bad training point
could badly affect the training, so bounding the variance of gradients (that too with respect to the average of
gradients) restricts such bad points to be present in the distribution.
We now move on to show the applicability of our bound for SMSTrG.
Discussion: Applicability in Classification with Two Layer Neural Network. We now use Corollary
4.10 to establish a generalization bound for the case of two-class classification with a two-layer Neural Network.
For simplicity of exposition, we have assumed that the data points are taken from R2. We fully specify the
problem through the following assumptions:
X1.We have a two class classification ( Y={−1,1}) in2dimension ( x= [x1,x2]) such that for expectations
of centers we have E[x1|y= 1] =E[x2|y= 1] = 2andE[x1|y=−1]=E[x2|y=−1]=−1, for
second moment for a constant σ>0we have E/bracketleftbig
x2
1/bracketrightbig
=E/bracketleftbig
x2
2/bracketrightbig
=σ2and also for a constant µp>0
E[|x1|] =E[|x2|] =µp.
X2.We use a single hidden layer feed-forward neural network, with kas hidden layer size. Its parameters
are initialized from w(l)
i,j=N(0,1). The total number of parameter values are n= 2k+k. For a
loss function f(w,z) =|y−O(w,x)|, whereO(w,x)is the output of the neural network, ∇f(w,z)j
denotes the j-th partial derivative of function fandwj1
j2,j3be the associated weight. Let αtbe any
diminishing step size at step tof SGD.
X3.We assume the ratio of absolute values of the weights of the Neural Network are bounded by Bwhere
B > 1.
Theorem 5.6. If X1,X2,X3 hold, for r= (w0,π0)we have,
ES,w0[σS]≤16B4σ2/parenleftbig
π+ 4kσ2µp/parenrightbig
(B−1)2π(9)
15Published in Transactions on Machine Learning Research (02/2024)
0 5000 10000 15000 20000
Number of Datapoints102030405060Empirical maxWCTrG
Training Smoothness constant
(a) MNIST
0 5000 10000 15000 20000
Number of Datapoints20406080100Empirical maxWCTrG
Training Smoothness constant (b) FashionMNIST
Figure 1: Experiment 1, Maximum of the Worst Case Training Gradient (WCTrG) and Training Smoothness
Constant at every p(= 20) interval of updates of SGD (we plot both the running average and the highest
value found so far). Notice that these constants have a clear upper bound throughout the training process.
The proof of this theorem is in Appendix C.3. Now we can directly use Markov inequality to get a high
probability bound. Assuming R.H.S of above is B′, so we will have,
PrS/braceleftbigg
Ew0[σS]≥16B4σ2logm×(kπ+ 4k2σ2µp)
(B−1)2π/bracerightbigg
≤1
klogm
Since the distance between the centers of the two classes is fixed at 2, the bound presented here satisfies our
intuition by showing that if the variances of the two classes are small, i.e., the classes are well-separated,
then the expectation of SMSTrG is small, and hence the generalization bound of Theorem 5.5 is small. It’s
important to note that since we have a probability bound, we need to apply the probabilistic version of
support stability, although we show generalization only through a.s. support stability. The probability version
can easily be derived using Theorem A.2 as we discuss in Section 3.3.
6 Experiments and Empirical Results
We now proceed to verify the results in a real-world case empirically. First, we perform the experiment to
show that WCTrG, SMSTrG and TeG are bounded. We also show in the case of random labelling that our
TeG are not bounded throughout the training, and hence, our results do not imply good generalization as
expected.
6.1 Experimental Validation of Results
Here, we will experimentally show that the WCTrG ( LS), TeG (Lg) and Training Smoothness Constant ( KS)
that we reasoned with are indeed bounded and that the theoretical upper bound that we derived for the
generalization error of a neural network holds in practice. Note that if LSis bounded, then σSwill also be
bounded. For simplicity in this experiment, we assume WCTrG to be a good proxy for TeG (as in general
Lg≤LS).
Setup.For our experiments we use MNIST andFashionMNIST datasets. In both datasets, we randomly
selected 20,000training and 1,000test points. All experiments were conducted using a fully connected feed
forward neural network with a single hidden layer and ReLU activation. We train the model using SGD
(batch size = 1), with cross-entropy loss, starting with randomly initialized weights. As suggested in our
analysis we use a decreasing learning rate αt=α0
t. In each epoch, we consider a random permutation of the
training set. WCTrG and Training Smoothness Constant are computed by calculating the norm of gradients
and Hessian across the training steps and taking their max.
16Published in Transactions on Machine Learning Research (02/2024)
0 5000 10000 15000 20000
Number of Datapoints107
105
103
101
101103Generalization errorvia SMSTrG
via WCTrG
Observed Generalization Error
(a) MNIST
0 5000 10000 15000 20000
Number of Datapoints102
100102Generalization errorvia SMSTrG
via WCTrG
Observed Generalization Error (b) FashionMNIST
Figure 2: Experiment 2, comparison of empirical generalization error (in green) vs. theoretical upper bound
via WCTrG (in red) and via SMSTrG (in blue) with varying training set size for different datasets.
Experiment 1. Our first experiment is aimed towards establishing that the WCTrG ( LS) and Training
Smoothness Constant ( KS) values estimated using local values at each step are bounded. Figure 1 summarizes
the results of these experiments over MNIST and FashionMNIST datasets ( α0= 0.001). The plots contain
the maximum of the gradients and smoothness values obtained after running each experiment 10times with
random weight initialization. These results support our Theorem 5.2 since the upper bound values quickly
stabilize and do not grow with the size of the training set in both datasets. Similarly, the bounded smoothness
constant supports our constraint on the learning rate, α0≤ρ(τ,m)
KS,ρ(τ,m) =log logm
logτ+logm. We findLSto be
8.1174(MNIST) & 12.5737(FashionMNIST), and KSto be 58.185(MNIST) and 102.7096(FashionMNIST).
Experiment 2. We now turn our attention to the experiment to support our main result, i.e., the empirical
generalization error estimated using the validation set is upper bounded by our theoretical upper bound. We
first split each dataset in a 20:1 ratio into training and validation sets and train the model at varying sizes of
training sets. We empirically compute the generalization error at each training set size using the validation
set. Figure 2 compares this empirical generalization error (in green) vs. the theoretical upper bound using
WCTrG (in red) and using SMSTrG (in blue). From these results, we can see that our bound decreases along
with the generalization error, thus empirically validating our reasoning. But clearly, the bound is not as tight
as we would like it to be. We conjecture that this is because of the upper bounding of the U(·)term and the
maximum which is taken across permutations of weights even for SMSTrG at t-th step.
6.2 Random Labelling Case
In making their case against the applicability of uniform stability as a tool for theoretically establishing the
good generalization properties of neural networks, Zhang et al. (2017) presented the following classification
problem: Given points picked from Euclidean space using some well-behaved distribution, say a Gaussian, each
point was assumed to have a class label picked uniformly at random from a finite set of labels independent of all
other points. Clearly, any classification algorithm trained on a finite training set will have ω(1)generalization
error for this problem. We now demonstrate that our results do notimply good generalization for this
problem. Specifically, we show empirically that the assumption of TeG ( Lg) being independent of mbreaks
in this case and this “constant” actually increases with m.
Setup. We pick images from the 0and1label class of MNIST dataset. For random labelling case, we
assign random labels to all the points. We then randomly sample a test set T(|T|= 50). We take a single
hidden layer ( 128neurons) fully connected neural network having ReLU activation in the hidden layer. We
take the loss function as l(ˆy,y) = 1−Softmax (c·ˆy,y)wherec= 6. We use a constant learning rate of 0.003,
batch size of size 8.
17Published in Transactions on Machine Learning Research (02/2024)
250 500 750 1000 1250
Number of Datapoints30405060T est Gradient
(a) Increasing Lgfor random label MNIST
250 500 750 1000 1250
Number of Datapoints681012T est Gradient (b) Decreasing Lgfor standard (non random) MNIST
Figure 3: Experiment 3, Test Gradient ( Lg) plot as training set size increases.
Experiment 3. The experiment proceeds by selecting initial random weights for a model say w0(we do
this10times). Then for every initialization we pick training set Sfrom our modified dataset (we do this
for5times). Now for every training set, we train the model either till accuracy is ≥98%or till 500epochs
whichever is reached first. Now we calculate the loss i.e. f(r,S,z )and the gradient∇wf(r,S,z )for allz∈T.
For the TeG we do Lg≃maxz∈T{∥∇wf(r,S,z )∥}. In figure 3a we can clearly see that the TeG ( Lg) scales
as the size of the training set ( m) increases. On the contrary for the standard (non-random) dataset the TeG
shows a decreasing trend with msee figure 3b. Therefore we can expect that in the random labelling case,
the upper bound in Theorem 5.1 becomes so large as to become vacuous.
Discussion. We note that the random labelling example has the property that the variance over the choice
of training sets of the loss of any algorithm, VarS[f(r,S,z )], is bound to be high. One possible direction for
theoretically showing that this implies that the TeG are likely to be high is by using Poincare-type inequalities.
This shows that the norm of the gradients of a function of a random vector is lower bounded by the variance
of the function. We do not pursue this direction further here, but we point out that it may help develop a
general theory for the limitations of what can be learned using parametrized methods trained using gradient
descent methods.
7 Applicability and Conclusion
7.1 Discussion on the Applicability of Our Results
•Removing the fully connectedness constraint. Although we considered a fully connected network for
Theorem 5.2 and Proposition 5.3, our data-dependent quantities are independent of the architecture of Neural
Networks. We only provide a bound on WCTrG and TeG using this. Note that even the bound on SMSTrG
is independent of the architecture of the network. We conjecture that it can be applied to architectures like
CNNs which have partially connected convolution layers with intermediate pooling, normalization and skip
connections (e.g., ResNet, DenseNet, etc). Our work provides a framework in which the study of the gradients
obtained during training such networks can help guide our understanding of their generalization properties.
•Adding regularization terms to the loss function. Several popular regularizers, the ℓ2regularizer being a
prominent example, are doubly differentiable and therefore Theorem 5.1 can be applied when such regularizers
are used along with a doubly differentiable loss function. Here as well a mild addition for bound on derivative
of regularization term in Theorem 5.3 may be able to help us prove results for this setting. However, it
requires further investigation to establish such a result.
•Activation functions apart from ReLU. We present a comprehensive treatment of ReLU activation but we
conjecture that results are not restricted to this kind of activation. Non-linearities like max-pool can also be
18Published in Transactions on Machine Learning Research (02/2024)
handled in our framework by proving that, like with ReLU, the points of discontinuity of such a non-linearity
also lie in a set of Lebesgue measure 0. This provides a direction for future research in this area.
•The case of multiple outputs Although we state the Theorem 5.1 and Theorem 5.5 for the case of a NN
with a single output, it is not difficult to extend the technique to cover the case of multiple outputs. However,
this requires a full treatment, which we postpone to future work.
What about other distributions? The data-dependent gradient bounds (SMSTrG, WCTrG and TeG) turn out
to be the deciding factor of generalization error. However, our analysis is limited to the bounds we derive
for them. There is a requirement for a more fine-grained analysis of these gradient bounds and we believe
that optimizing these data-dependent gradient bounds will be the right direction to proceed. This may be
made possible by looking at the network structures, the data distribution and the training set in more detail.
We hope that the polynomial characterization of the NN presented in Appendix C.1.2 will help this process.
We conjecture that it may be able to show that for certain distributions, the constants actually improve
(decrease) w.r.t. training set size as the training proceeds, resulting in a much slower decay of learning rate
and this could lead to a proof of a.s. support stability in these cases.
7.2 Conclusion
Using a.s. support stability, we derive the generalization error bounds for neural networks. We most
importantly identify the data-dependent quantities (WCTrG, SMSTrG, TeG) whose boundedness implies
generalization of neural networks. We also show how to upper bound these quantities either in terms of
spectral property orthe initial loss and variance of gradients of the neural network. So, this paper links the
generalization of neural networks directly with the gradients and shows a guarantee of better generalization
if we start with a small (constant w.r.t. training size) initial value of loss function and descent (defined in
assumption P1) value of variance of the gradient at each step of SGD. However, we feel it is possible to
prove stronger and more widely applicable results in this framework than the ones we have presented here.
Immediate lines of research are to apply our methods for CNNs and GNNs and to investigate what other
architectures can be approached with our method and whether the gradient bounds play some significant role
because of a different network structure. Also, we do provide empirical evidence to support the example of
the random labelling case but it lacks a theoretically concrete statement.
References
Peter L. Bartlett. For valid generalization, the size of the weights is more important than the size of the
network. In Proceedings of Advances in Neural Information Processing Systems, NeurIPS 1996 , 1996.
Peter L. Bartlett and Shahar Mendelson. Rademacher and Gaussian complexities: Risk bounds and structural
results.JMLR, 3:463–482, march 2003. ISSN 1532-4435.
Peter L. Bartlett, Dylan J. Foster, and Matus Telgarsky. Spectrally-normalized margin bounds for Neural
Networks. In Proc. Advances in Neural Information Processing Systems 30 , pp. 6240–6249, 2017.
Léon Bottou, Frank E. Curtis, and Jorge Nocedal. Optimization methods for large-scale machine learning.
SIAM Rev. , 60(2):221–487, 2018.
Olivier Bousquet and André Elisseeff. Stability and Generalization. J. Mach. Learn. Res. , 2(Mar):499–526,
2002.
Olivier Bousquet, Yegor Klochkov, and Nikita Zhivotovskiy. Sharper bounds for uniformly stable algorithms.
Proc. Machine Learning Research , 125:1–17, 2020.
Zachary Charles and Dimitris Papailiopoulos. Stability and generalization of learning algorithms that
converge to global optima. In Jennifer Dy and Andreas Krause (eds.), Proceedings of the 35th International
Conference on Machine Learning , volume 80 of Proceedings of Machine Learning Research , pp. 745–754.
PMLR, 10–15 Jul 2018. URL https://proceedings.mlr.press/v80/charles18a.html .
19Published in Transactions on Machine Learning Research (02/2024)
Satrajit Chatterjee and Piotr Zielinski. On the generalization mystery in Deep Learning. arXiv:2203.10036,
2022.
Richard Combes. An extension of McDiarmid’s inequality, 2015. URL https://arxiv.org/abs/1511.05240 .
Vitaly Feldman and Jan Vondrak. Generalization bounds for uniformly stable algorithms. In Advances in
Neural Information Processing Systems , 2018.
Vitaly Feldman and Jan Vondrak. High probability generalization bounds for uniformly stable algorithms
with nearly optimal rate. In Alina Beygelzimer and Daniel Hsu (eds.), Proceedings of the Thirty-Second
Conference on Learning Theory , volume 99 of Proceedings of Machine Learning Research , pp. 1270–1279.
PMLR, 25–28 Jun 2019a. URL https://proceedings.mlr.press/v99/feldman19a.html .
Vitaly Feldman and Jan Vondrak. High probability generalization bounds for uniformly stable algorithms
with nearly optimal rate. In Alina Beygelzimer and Daniel Hsu (eds.), Proceedings of the Thirty-Second
Conference on Learning Theory , volume 99 of Proceedings of Machine Learning Research , pp. 1270–1279.
PMLR,25–28Jun2019b. URL https://proceedings.mlr.press/v99/feldman19a.html . Supplementary
section.
N. Golowich, A. Rakhlin, and O. Shamir. Size-independent sample complexity of neural networks (extended
abstract). Proc. Machine Learning Research , 75:1–3, 2018.
Mahdi Haghifam, Jeffrey Negrea, Ashish Khisti, Daniel M. Roy, and Gintare Karolina Dziugaite. Sharpened
generalization bounds based on conditional mutual information and an application to noisy, iterative
algorithms, 2020.
Moritz Hardt, Ben Recht, and Yoram Singer. Train faster, generalize better: Stability of stochastic gradient
descent. In Maria Florina Balcan and Kilian Q. Weinberger (eds.), Proceedings of The 33rd International
Conference on Machine Learning , volume 48 of Proceedings of Machine Learning Research , pp. 1225–
1234, New York, New York, USA, 20–22 Jun 2016. PMLR. URL https://proceedings.mlr.press/v48/
hardt16.html .
Pengzhan Jin, Lu Lu, Yifa Tang, and George Em Karniadakis. Quantifying the generalization error in deep
learning in terms of data distribution and neural network smoothness. Neural Networks , 130:85–99, 2020.
ISSN 0893-6080. doi: https://doi.org/10.1016/j.neunet.2020.06.024. URL https://www.sciencedirect.
com/science/article/pii/S0893608020302392 .
Kenji Kawaguchi, Leslie Pack Kaelbling, and Yoshua Bengio. Generalization in deep learning. CoRR,
abs/1710.05468, 2017.
M. Kearns and D. Ron. Algorithmic stability and sanity-check bounds for leave-one-out cross- validation.
Neural Computation , 11(6):1427–1453, 1999.
Yegor Klochkov and Nikita Zhivotovskiy. Stability and deviation optimal risk bounds with convergence rate
o(1/n). In Advances in Neural Information Processing Systems , 2021.
Ilja Kuzborskij and Christoph Lampert. Data-dependent stability of stochastic gradient descent. In Jennifer
Dy and Andreas Krause (eds.), Proceedings of the 35th International Conference on Machine Learning ,
volume 80 of Proceedings of Machine Learning Research , pp. 2815–2824. PMLR, 10–15 Jul 2018. URL
https://proceedings.mlr.press/v80/kuzborskij18a.html .
Yunwen Lei and Yiming Ying. Fine-grained analysis of stability and generalization for stochastic gradient
descent. In Hal Daumé III and Aarti Singh (eds.), Proceedings of the 37th International Conference on
Machine Learning , volume 119 of Proceedings of Machine Learning Research , pp. 5809–5819. PMLR, 13–18
Jul 2020. URL https://proceedings.mlr.press/v119/lei20c.html .
Yunwen Lei, Rong Jin, and Yiming Ying. Stability and generalization analysis of gradient methods for shallow
neural networks. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (eds.), Advances in
Neural Information Processing Systems , 2022. URL https://openreview.net/forum?id=BWEGx_GFCbL .
20Published in Transactions on Machine Learning Research (02/2024)
Tristan Milne. Piecewise strong convexity of neural networks. In H. Wallach, H. Larochelle, A. Beygelz-
imer, F. d 'Alché-Buc, E. Fox, and R. Garnett (eds.), Advances in Neural Information Processing Sys-
tems, volume 32. Curran Associates, Inc., 2019. URL https://papers.nips.cc/paper/2019/file/
b33128cb0089003ddfb5199e1b679652-Paper.pdf .
Vaishnavh Nagarajan and J. Zico Kolter. Uniform convergence may be unable to explain general-
ization in deep learning. In Proceedings of Advances in Neural Information Processing Systems,
NeurIPS 2019 , pp. 11611–11622, 2019. URL https://proceedings.neurips.cc/paper/2019/hash/
05e97c207235d63ceb1db43c60db7bbb-Abstract.html .
Jeffrey Negrea, Gintare Karolina Dziugaite, and Daniel M. Roy. In defense of uniform convergence: general-
ization via derandomization with an application to interpolating predictors. In Proceeding of ICML 2020 ,
2020a.
Jeffrey Negrea, Mahdi Haghifam, Gintare Karolina Dziugaite, Ashish Khisti, and Daniel M. Roy. Information-
theoretic generalization bounds for sgld via data-dependent estimates, 2020b. URL https://arxiv.org/
abs/1911.02151v3 .
Vladimir Vapnik. Statistical learning theory . Wiley, 1998. ISBN 978-0-471-03003-4.
Vladimir Vapnik and Alexey Chervonenkis. Theory of Pattern Recognition . Nauka, Moscow, 1974.
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep
learning requires rethinking generalization. In International Conference on Learning Representations , 2017.
URL https://openreview.net/forum?id=Sy8gdB9xx .
Chiyuan Zhang, Qianli Liao, Alexander Rakhlin, Brando Miranda, Noah Golowich, and Tomaso A. Poggio.
Theory of deep learning iib: Optimization properties of sgd. ArXiv, abs/1801.02254, 2018.
Yikai Zhang, Wenjia Zhang, Sammy Bald, Vamsi Pritham Pingali, Chao Chen, and Mayank Goswami.
Stability of SGD: Tightness analysis and improved bounds. In The 38th Conference on Uncertainty in
Artificial Intelligence , 2022. URL https://openreview.net/forum?id=Sl-zmO8j5lq .
A Modification of McDiarmid’s Theorem
We first define a probabilistic weakening of bounded difference property.
Definition A.1. Given 2mi.i.d. random variables X1,...,X 2mdrawn from some domain Zaccording to
some probability distribution D, for someβ >0andη∈[0,1], a function f:Zm→Ris calledη-almostβ-
bounded difference w.r.t. Dif
∀i∈{1,···,m}:|f(X1,...,Xm)−f(X1,...,Xi−1,X′
i,Xi+1,...,Xm)|≤β,
with probability at least 1−η. In caseη= 0we say that fsatisfiesalmost surely β-bounded difference w.r.t
D. WhenDis understood we will omit it.
We now state a modified version of McDiarmid’s theorem that holds for η-almostβ- bounded difference
functions.
Lemma A.2. LetX1,...,Xmbe i.i.d. random variables. If fsatisfiesη-almostβ- bounded difference and
takes values between 0andM, then,
Pr{f(X1,...,Xm)−E[f(X1,...,Xm)]≥ϵ}≤exp/bracketleftigg
−2ϵ2
m(β+Mη)2/bracketrightigg
+η.
LemmaA.2follows directly from a result shown in (Combes, 2015, Proposition 2). Since the proof is available
in Combes (2015) we omit it here.
21Published in Transactions on Machine Learning Research (02/2024)
Symbol Explanation
LS Worst Case Training Gradient.
σS Second Moment of Step Training
Gradient.
Lg Test Gradient.
KS Training Smoothness Constant.
X,YInput and output Space.
D Distribution over Z⊂XY.
z=
(xz,yz)Input point and label picked from
distribution Ddefined overZ.
r∈R random string from a random set to
show randomness in an algorithm.
S Training set of size m.
SiTraining set Swithithpoint replaced by
another point picked i.i.d from D.
AS,r Training Algorithm.
ℓ(·,·)Bounded value Loss function with
domainY×Y→ [0,M].
R(AS,r)Risk (Population error).
Re(AS,r)Empirical Risk (Training error).
wt Weight of the parameterized function
trained by SGD at tthstep.
αt Learning rate at t-th step of SGD.
∥.∥σSpectral norm of matrix.Symbol Explanation
τ Total number of epochs, each epoch is
ofmstep,w0is the initial weight.
π∈Ππis some permutation of mpoints
picked from set of all possible
permutation Π.
rinit Random initialization i.e. w0.
rp A random permutation for mpoints.
r=
(rinit,rp)Random string rhavingw0and
{πi}τ−1
i=0i.e., for all epochs.
Ll Local parameter Lipschitz constant.
Kl Local parameter Smoothness constant.
WlWeight matrix of l-th layer on NN.
Wl
j,: Thej-th row ofl-th layer weight of
NN.
w(l)
i,j Weight value of l-th layer from ith
neuron ofl-th layer to j-th neuron of
l+ 1-th layer.
T Size of test set.
f(wt,z)Loss att-th step of SGD computed on
pointz.
f(r,S,z )Loss of NN trained on set Sand
evaluated on point z.
Table 3: Notation used in the body of the paper.
B Almost Sure (a.s.) Support Stability of SGD Proved
Proof of Lemma 4.3. Letfbe the partial function of w(i.e., assuming zis already given) is locally Lipschitz
atw∈A, there is an εw>0and anLw>0such that for all w′∈Rnwith∥w−w′∥≤εw,|f(w)−f(w′)|≤
Lw∥w−w′∥. So, let us turn our attention to those w′∈Athat lie outside the ball of radius εwaroundw.
Note that for such a w′, ifB > 0is the bound on f, we have that
|f(w)−f(w′)|
∥w−w′∥≤2B
εw.
Therefore the “global” Lipschitz constant for fwithinAismax{Lw,2B/εw:w∈A}which is bounded since
Ais finite. This is valid for all the partial functions (i.e., for all z∈Ω) and hence proves the theorem.
Proof of Theorem 4.5. For somei∈[m]we couple the trajectory of SGD on SandSiwherezi∈Shas been
replaced with z′
i. Our random decision string r, in this case, is a random choice of an initial parameter vector,
w0, and a random set of τi.i.d permutations π0,...,πτ−1of[m]chosen uniformly at random. We use these
random choices for training both the algorithms with SandSi. For 0≤j≤τ−1, we denote π−1
j(i)byIj,
i.e.,Ijis the (random) position where the ith training point is encountered in the jth training epoch. The
key quantity we will track through the coupled training process will be
δt=∥wt−w′
t∥,
for1≤t≤τm. If we can show that Er[Lgδτm]is bounded by some quantity Balmost surely, we can invoke
the fact that fis a.s.Ll-LPL to say that ∥Er[f(wt,z)−f(w′
t,z)]∥≤Er[Lgδτm]≤Bfor allz∈supp (D),
whereLgis the Test Gradient.
22Published in Transactions on Machine Learning Research (02/2024)
We argue differently for the first epoch and differently for later epochs. For the first epoch, we note that for
t≤I0,δt= 0since SGD performs identical moves in both cases. At t=I0+ 1
δI0+1=∥wI0−αI0∇f(wI0,zi)−(w′
I0−αI0∇f(w′
I0,z′
i))∥=αI0∥∇f(wI0,zi)−∇f(w′
I0,z′
i)∥,(10)
where the second equality follows from the fact that wI0=w′
I0by the definition of I0. Using Lemma 4.3
we can say that δI0+1≤2αI0LSalmost surely. Notice here we use data-dependent Worst Case Training
GradientLSwhich is only defined for points in set S, unlike Test Gradient. Now,
δI0+2≤∥wI0+1−w′
I0+1∥+αI0+1∥∇f(wI0+1,zi)−∇f(w′
I0+1,z′
i))∥.
Here although the parameter vectors wI0+1andw′
I0+1are not the same, zπ0(I0+1)andz′
π0(I0+1)are the same
by the definition of I0(assuming that I0̸=m). Therefore we get that
δI0+2≤δI0+1+αI0+1KSδI0+1
with probability 1 since from Lemma 4.3 we have that fhas a “global” smoothness property for the entire
set of at most 2τmparameter vectors that will be encountered during the coupled training of SandSi. So
we used Training Smoothness Constant ( KS). Noting that a similar recursion can be applied all the way to
the end of the first epoch, i.e. till t=mwe get
δm≤2αI0LSm/productdisplay
t=I0+1(1 +αtKS)≤2αI0LSexp/braceleftiggm/summationdisplay
t=I0+1αtKS/bracerightigg
, (11)
with probability 1. Moving on to the next epoch, we note that we can make the argument above till the next
point where the two training sequences differ, i.e., till the m+I1+ 1st step. At this point we have,
δm+I1+1≤δm+I1+αm+I1∥∇f(wm+I1,zi)−∇f(w′
m+I1,z′
i))∥.
Since neither the parameter vector nor the training points are the same in the second term, we have no option
but to use the data-dependent Worst Case Training Gradient to say that,
δm+I1+1≤δm+I1+αm+I12LS.
Sinceαm+I1<αI0, observing that our current bound for δm+I1is larger than αm+I12LS. Therefore
δm+I1+1≤2δm+I1.
So, we see that in the second and subsequent epochs, for time step jm+Ij+ 1,1≤j <τwe have the bound
δjm+Ij+1≤2δjm+Ij,
and for allt>m +I1,t̸=I1,...,Iτ−1we have, as before, by the smoothness property that
δt+1≤δt(1 +αt+1KS).
Therefore, we have that
δτm≤2αI0LS(2)τ−1exp/braceleftiggτm/summationdisplay
t=I0+1αtKS/bracerightigg
≤α0LS2τ 1
I1−ρ(τ,m)
0exp

α0KS/parenleftig
(τm)ρ(τ,m)−Iρ(τ,m)
0/parenrightig
ρ(τ,m)

.(12)
where, in the first inequality for ease of calculation we have retained the terms of the form (1 +αIjKS),
2≤j < τin the product on the right although we can ignore them. In the second inequality, we have
substituted αt=α0/t(1−ρ(τ,m))and bound the summation using integration.
Finally, in order to compute Er[LgδT]remember there were two source of randomness first is random
initialization w0or lets call it rinitand random permutation πlets call itrp. Now because rinitandrpare
23Published in Transactions on Machine Learning Research (02/2024)
independent we can write Er[Lgδτm]=Erinit/bracketleftbig
Erp[Lgδτm|rinit]/bracketrightbig
. Now in order to compute Erp[Lgδτm|rinit]
note thatLg,LSandKSare constant.
Note that, since π0is uniformly drawn from the set of permutations of [m],I0is uniformly distributed on
[m]. Summing up the last term of (12) over I0∈[m]and dividing further by mwe get
Erp[Lgδτm|rinit]≤2τα0LgLS×1
mm/summationdisplay
I0=11
I1−ρ(τ,m)
0exp

α0KS/parenleftig
(τm)ρ(τ,m)−Iρ(τ,m)
0/parenrightig
ρ(τ,m)


Using integration we bound the summation part and also using exp(−α0KS/ρ(τ,m))≤1we get the upper
bound for the summation part as
≤U(α0,KS,ρ(τ,m))·exp/parenleftbiggα0KS
ρ(τ,m)(τm)ρ(τ,m)/parenrightbigg
where
U(α0,KS,ρ(τ,m)) := 1 +1−exp(−α0KSmρ(τ,m)/ρ(τ,m))
α0KS,
we get
Erp[LgδT|rinit]≤2τα0LgLS·U(α0,KS,ρ(τ,m))·exp/braceleftig
α0KS
ρ(τ,m)(τm)ρ(τ,m)/bracerightig
m(13)
takingρ(τ,m) =log logm
logτ+logmand expectation over rinitwe get the desired result.
We also present the proof of Theorem 4.7, which is very similar to the proof of Theorem 4.5 with some
changes.
Proof of Theorem 4.7. The proof of this is very similar to the proof of Theorem 4.5. The main point is that
in the paragraph after Equation 10, we can actually use the exact gradient maxed over all permutation at
I0-th step (lets call it σS,I0) instead of LSas it’s the gradient at I0-th step. The rest of the steps follow
similarly till equation 11 so we get
δm≤2αI0σS,I0exp/braceleftiggm/summationdisplay
t=I0+1αtKS/bracerightigg
Now calculating Er[Lgδm], note that earlier (in line 622) LSwas constant w.r.t the permutation ( rp) but
hereσS,I0depends on the step, so taking expectation over permutation is equivalent to taking expectation
over random variable I0which is picked uniformly from 1tom.
Erp[Lgδm|rinit]≤α0Lg·Erp
σS,I0
I1−log logm
logm
0exp/braceleftiggm/summationdisplay
t=I0+1αtKS/bracerightigg

So using Cauchy Schwarz inequality to separate expectation over the random variable σS,I0
Erp[Lgδm|rinit]≤α0Lg·/radicalbigg
Erp/bracketleftig
σ2
S,I0/bracketrightig/radicaltp/radicalvertex/radicalvertex/radicalbt1
mm/summationdisplay
I0=11
I2−2 log logm
logm
0exp/braceleftiggm/summationdisplay
t=I0+12αtKS/bracerightigg
Upper bounding summation inside exponent by integration exactly like we did in equation 12. we get the
second square-root terms as
≤1
mm/summationdisplay
I0=11
I2−2 log logm
logm
0exp

2α0KS/parenleftbigg
(m)log logm
logm−Ilog logm
logm
0/parenrightbigg
logm
log logm


24Published in Transactions on Machine Learning Research (02/2024)
Assumingp=log logm
logmto simplify the equation,
≤1
mexp/braceleftbigg−2α0KS
p/bracerightbigg/parenleftigg
exp−2α0KS
p+m/summationdisplay
I0=21
I(2−2p)
0exp/braceleftbigg−2α0KSIp
0
p/bracerightbigg/parenrightigg
Now, we use integration to bound the summation term,
m/summationdisplay
I0=21
I(2−2p)
0exp/braceleftbigg−2α0KSIp
0
p/bracerightbigg
=/integraldisplaym
x=1x−2(1−p)exp/braceleftbigg−2α0KS
pxp/bracerightbigg
Substituting u=xp
p, and in next step upper bounding (u·p)(1−1/p)by1(asp<1andu·≥1throughout the
limit) and then putting the limits.
m/summationdisplay
I0=21
I(2−2p)
0exp/braceleftbigg−2α0KSIp
0
p/bracerightbigg
=/integraldisplaymp/p
u=1
p(u·p)(1−1/p)exp{−2α0KSu}
≤p1−1/p/integraldisplaymp/p
u=1/pexp{−2α0KSu}
≤p1−1/p
2α0KSexp/braceleftbigg−2α0KS
p/bracerightbigg
Using this and putting the value of pand taking p1−1/pwe get,
Erp[Lgδm|rinit]≤α0Lg·/radicalbig
U′(α0,KS,m)/radicalbigg
Erp/bracketleftig
σ2
S,I0/bracketrightig
·1
m1
2−α0KS(logm−1)
log logm(14)
where
U′(α0,KS,m) = 1 +1
2α0KS
and expectation over rpforσS,I0is just taking the average across SGD steps, so we get σS.
We also present the proof of Corollary!4.10.
Proof of Corollary 4.10. Writing assumption P1 averaged over msteps and multiplying both side by α0we
get,
1
mm/summationdisplay
t=1α0/bracketleftigg
1
m/summationdisplay
z∈S∥∇f(wt,z)∥2/bracketrightigg
≤α0M+α0MG
mm/summationdisplay
t=1/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble1
m/summationdisplay
z∈S∇f(wt,z)/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
Now we take total expectation over the above equation and use Theorem 4.9 to bound the R.H.S of the
equation. For L.H.S first, let the worst gradient (because of permutation) at t-th step computed on point ziis
Lzi,t=maxw∈At{∂
∂wf(w,zi)}, where set At=∪π0∈Π{wt,π0}. Keeping this in mind, we can write L.H.S as,
L.H.S =E/bracketleftigg
1
mm/summationdisplay
t=1/bracketleftigg
1
mm/summationdisplay
i=1L2
zi,t/bracketrightigg/bracketrightigg
(15)
=E/bracketleftigg
1
mm/summationdisplay
t=1/bracketleftigg
1
mm/summationdisplay
i=1Ezi∼Z/bracketleftbig
L2
zi,t/bracketrightbig/bracketrightigg/bracketrightigg
(16)
(17)
Now note that because all ziare identical (picked from the same distribution) so expectation over them will
be equal. So maxi∈[m](Ezi∼Z/bracketleftbig
L2
zi,t/bracketrightbig
) =Ezi∼Z/bracketleftbig
L2
zi,t/bracketrightbig
and therefore we can replace the above with SMSTrG
(i.e.σS). Using this, we get the statement of Corollary 4.10.
25Published in Transactions on Machine Learning Research (02/2024)
C Neural Networks: Characterization and Proofs
Symbol Explanation
kl Number of neurons in l-th layer.
H Depth of neural network.
ini,j(w,x) Input to the i-th neuron of j-th layer.
outi,j(w,x) Output of the i-th neuron of j-th layer.
outH,1=
out(w,x)Is the label returned by the neural network.
ϕ(w,x) Polynomial associated with fully connected NN.
ϕi,j(w,x) Base polynomial associated with j-th neuron of i-th layer.
Gw,x The set of weights need to set to zero, to apply all closed ReLU gate in NN (with
ReLU).
ϕ(w,x){Gw,x}A neural network with ReLU activation with closed ReLU gates set to 0.
lab(x) Label of point x.
Ij Position in permutation in j-th epoch when i-th training points (i.e. the replaced
point) is encountered based on πj.
δt Norm of difference between weights for SandSiatt-th step of SGD (i.e., ∥wt−w′
t∥).
Table 4: Notation used in section C
In order to prove Theorem 5.2 we first need to describe a characterization of neural networks that allows us
to get a better insight into their smoothness properties. We present the characterization in Section C.1 and
the proof in Section C.2.
After that, we continue presenting the proof of Section 5. In Sections C.2 we also present the proof of
Theorem 5.1, Proposition 5.3 and Theorem 5.4. Then finally in Section C.3 we present the proofs of Section 5.2
C.1 A Polynomial-based Characterization Neural Networks
C.1.1 Neural Network Terminology
Neural networks provide a family of parameterized functions of the form we have discussed in Section 4. The
parameter vector w∈Rnis applied over a network structure with layers. In this case, we specify Zto be
Rd×R, i.e., the data points are from Rdand the label is from R, i.e., the NN has a single output. We will
denote the depth of the network by H. The layers will be numbered 0 to Hwith layer 0 being the input layer.
The number of neurons in layer iwill beki. For this discussion, we assume a fully connected network. We
will denote by wi
j,kthe weight of the edge from the jneuron of the ith layer to the kth neuron of the i+ 1st
layer. For the NN with parameters wat a pointx∈Rdwe will denote the input into the jth neuron of the
ith layer by ini,j(w,x)and its output by outi,j(w,x). Further, we will assume that all neurons in all layers
of the network except the input layer and the output layer have ReLU activation applied to them. In case
the output of a node is 0 due to ReLU activation we will say the ReLU gate is closedotherwise we will say it
isopen. The label output by the network will be outH,1=out(w,x). For each exposition, we will assume
that out(w,x) = 1ifin(w,x)>0and 0 otherwise, i.e., there are only two labels in Y. For convenience we
will denote this architecture as N.
C.1.2 Multivariate Polynomials Associated with a Neural Network
Given a set of indeterminates x=x1,...,xl, letP(x)be the set of multivariate polynomials on x1,...,xl
with real coefficients. For any polynomial p(x),i1,...,iq∈[l]and anyα1,...,αq∈Rfor someq≤l, we will
denote byp(x)/braceleftbig
xij=αj:j∈[q]/bracerightbig
the polynomial in P(x\{xi1,...,xiq})that is obtained by setting all occurrences
ofxijtoαjinp(x). In particular, p(x){xi=0}is the polynomial p(x)with all monomials containing xi
removed, and p(x){xi=1}retains all the monomials of p(x)but those monomials that contain xiappear
without the term xi.
26Published in Transactions on Machine Learning Research (02/2024)
Returning to NNs, let us consider two sets of indeterminates: x={xi:i∈[d]}andw={w(i)
j,k: 0≤i <
H,1≤j≤ki,1≤k≤ki+1}andk0=d. Let us consider Ndefined in Sec. C.1.1 and create a version of it
that replaced the ReLU activation at each node with the identity activation function. We will call this the
identity version ofNand denote it I(N). We will say that I(N)has the following polynomial associated
with it:
ϕ(w,x) =k0/summationdisplay
j0=1k1/summationdisplay
j1=1···kH−1/summationdisplay
jH−1=1xj0w(0)
j0,j1w(1)
j1,j2···w(H−1)
jH−1,1.
Note that the output layer has only one neuron. We will refer to this as the base polynomial ofN. The base
polynomial associated with the jth neuron in layer ican be derived from the base polynomial of the network,
we express this in figure 4 and also write formally as follows
ϕi,j(w,x) =ϕ(w,x)/braceleftbig
w(i)
l1,l2=0,w(l3)
l4,l5=1:l1∈[ki]\{j},l2∈[ki+1],l3>i,l 4∈[kl3],l5∈[kl3+1]/bracerightbig
/producttextH
p=i+1ki. (18)
-th layer -th layer-th
node
Figure 4: The output of the j-th neuron of the i-th layer represented represented by the base polynomial.
Here the weights along the dotted red lines are set to zero and the weights along the green lines are set to
one. The output of the neuron is represented by the values on the connection. Notice that the output is
scaled by the product of the number of intermediate nodes because of which we divide it later in 18.
Also we could describe a Network whose say ithlayerjthneuron’s gate is closed by ϕ(w,x){wi
l1,j= 0,∀l1∈
ki−1}, This is represented by the figure 5. We will write Gw,xas the set of weights needed to be equated to
zero for all closed ReLU gates. It’s clearly visible that due to ReLU activations varying at different points,
there is no single polynomial that captures the output of the NN everywhere in Rn×Rd. However, the
following observation shows a way of defining polynomials that describe the output over certain subsets of
space.
Observation C.1. Givenw∈Rnandx̸= (0,..., 0)∈Rd,i∈[H],j∈[ki]andϕi,j(w,x){Gw,x= 0}be the
polynomial representing output and Gw,xbe the set of weights for closed ReLU gates as discussed above. For
the case where inl1,l2(w,x)̸= 0for all 1≤l1≤iand all 1≤l2≤kl1, there is an ϵ>0depending on w,x
such that, for all w′with∥w−w′∥<ϵ,
ϕi,j(w′,x){Gw′,x= 0}=ϕi,j(w′,x){Gw,x= 0}
i.e. the polynomial remains same for w′andw.
27Published in Transactions on Machine Learning Research (02/2024)
 -th
node
-th layer -th layer
Figure 5: For a neural network with ReLU, if the i-th layersj-th neuron’s ReLU gate is closed, this is
represented by the base polynomial. Here the dotted red lines are set to zero.
Proof.Since ini,j(w,x)is strictly separated from 0and there are only a finite number of neurons in the
network there must be an ϵsmall enough for which all open ReLU gates remain open and all closed gates
remain closed. And because of this, we can use the same polynomial with new weights as no ReLU gate
switches their state.
C.2 Proofs for Section 5.1
We use the NN characterization for the proof of Theorem 5.2.
Proof of Theorem 5.2. The idea behind this proof is due to (Milne, 2019, Lemma 1) who used it for a different
purpose. From Observation C.1 it follows that if we have x̸= (0,..., 0)∈Rdsuch that ini,j(w,x)̸= 0for all
1≤i≤Hand all 1≤j≤ki, then out(w,x)is, in fact, just the polynomial ϕ(w′,x){Gw,x= 0}within a
small neighbourhood of w. Therefore it is doubly differentiable. Since the loss function is also differentiable,
we are done for all such values of x.
So now let us consider the set of points xfor whichiis the smallest layer index such that ini,j(w,x) = 0.
In case there are two such indices, we break ties using the neuron index j. By Observation C.1, in a
neighbourhood of w,ini,j(w,x)is a polynomial in wandxfor eachx.
Now, we consider two cases. In the first case, outi−1,j′(w,x) = 0for allj′∈[ki−1], i.e., all the ReLU gates
from the previous layers are closed because ini−1,j′(w,x)<0for allj′∈[ki−1]. In this case out(w′,x) = 0
everywhere in the neighbourhood guaranteed by Observation C.1 and therefore ℓ(out(w′,x),lab(x))is doubly
differentiable in the parameter space at wfor all such x, where we assume that each data point has a label
lab(x)∈{0,1}associated with it. We note that this argument is easily portable to the case of a more general
label setYwith the property described in the statement of Theorem 5.1 since inH,1will be 0 everywhere in a
small neighbourhood.
In the second case we have some j′∈[ki−1]such that outi−1,j′(w,x)>0. LetCi,j⊆Rdbe thosexfor which
this case holds. Ci,jcontains the solutions to ini,j(w,x) = 0. Since we are working with a specific value of w,
this simply becomes a polynomial in x. In fact, inspecting the definition of base polynomials we note that
whenwis fixed ini,j(w,x)is simply a linear combination of x1,...,xd
R. This implies that Ci,jis a hyperplane
inRd. We note that this argument can also be made of the output node under the condition on the label
set given in the statement of Theorem 5.1 because for inH,1(w,x)to give a value that lies on the boundary
between two sets with different labels for a given w,xmust be drawn from a set of Lebesgue measure 0.
28Published in Transactions on Machine Learning Research (02/2024)
Since the network size is finite the set of all possible values of xfor which case 2 occurs, i.e.,/uniontext
i∈[H],j∈[ki]Ci,j
is a finite union of hyperplanes in Rdand therefore a set of Lebesgue measure 0.
Proof of Proposition 5.3. Let us consider the partial derivative w.r.t w(l)
i,j. For this let I(l)
i,j,A(l+1)
jandB(l−1)
i
be3matrices of size W(l),W(l+1)andW(l−1)respectively such that I(l)
i,j[i,i] = 1and reset all entries are 0,
A(l+1)
j[k,j] =W(l+1)[k,j],∀kand rest all entries are 0andB(l−1)
i[i,k] =W(l−1)[i,k],∀kand rest all entries
are one. Using these 3matrices and the weight matrices we can compute the gradient as
∂ϕ(w,x)
∂w(l)
i,j=W(H)···W(l+2)·A(l+1)
j·I(l)
i,j·B(l−1)
i·Wl−2···W1·x (19)
LetM′(l,i,j)be a matrix such that
M′
l,i,j=A(l+1)
j·I(l)
i,j·B(l−1)
i
Although we have scalar values taking spectral norm on both sides of eq 19 we get
/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle∂ϕ(w,x)
∂w(l)
i,j/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle=H/productdisplay
k=1∥W(k)∥σ∥M′
l,i,j∥σ
∥W(l+1)∥σ·∥W(l)∥σ·∥W(l−1)∥σ∥x∥
Now lets define another matrix Mlsuch that (p,q)thelement of matrix Ml[p,q] =∥M′
l,i,j∥σ. Now, the
expression for 2,2norm (Frobenius norm) of the gradient vector directly gives us the required expression for
bound on gradients.
We can give a similar argument for bounding KS, for somew(l1)
i1,j1andw(l2)
i2,j2we have
/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle∂2ϕ(w,x)
∂w(l2)
i2,j2∂w(l1)
i1,j1/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle≤H/productdisplay
k=1∥W(k)∥σ/parenleftbigg∥M′
l1,i1,j1∥σ
∥W(l1+1)∥σ·∥W(l1)∥σ·∥W(l1−1)∥σ/parenrightbigg
·/parenleftbigg∥M′
l2,i2,j2∥σ
∥W(l2+1)∥σ·∥W(l2)∥σ·∥W(l2−1)∥σ/parenrightbigg
∥x∥
Note that the above equation is exactly if l1+ 2<l2orl1−2>l2and for the rest of the case we can use
this as the upper bound this is because for a matrix Mspectral norm∥M∥σis upper bound for when we set
all except one row or column of matrix to zero and calculate the spectral norm. Now if we take the 2,2norm
(Frobenius norm) of the Hessian matrix we get the desired result.
We now show the proof of Theorem 5.4 present in the discussion of Section 5.1.2, where we bound the
gradients encountered.
Proof of Theorem 5.4. We first write the ℓ2norm of gradients of parameter vector was,
∥∇wf(w,x)∥2
2=∥∇W1f(w,x)∥2
2+∥∇W2f(w,x)∥2
2
Calculating gradients norm for both layers, assuming for given w0(i.e., weight at initialization, note that
this is part of rrandomness), we have a value of weights bounded above by B1
/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble∂f(w,x)
∂w(2)
j/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
2=|⟨W1
j,:,x⟩|2,WhereW1
j,:are thejthrow ofW1
≤B2
1·D2
29Published in Transactions on Machine Learning Research (02/2024)
The effective dimension of xisD, so the above dot product dimension will also be bounded by Dasxis0in
all other dimensions.
/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble∂f(w,x)
∂w(1)
i,j/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
2=|w(2)
ixj|2
≤B2
1
So, summing up the squared partial derivatives across all parameters we get,
∥∇wf(w,x)∥2
2≤d1·D·B2
1+d1·D2·B2
1
=B2
1·d1·D(1 +D)
C.3 Proofs for Section 5.2
Proof of Theorem 5.6. First lets assume that assumption P1 holds so we need to show Ew0,z∈S[f(w0,z)]is
bounded, so calculating the value for this,
f(w0,z) =|y−O(w,x)|
=/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingley−k/summationdisplay
i=1w(2)
iw(1)
i,1x1−k/summationdisplay
i=1w(2)
iw(1)
i,2x2/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
=|y|+/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsinglek/summationdisplay
i=1w(2)
iw(1)
i,1x1/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle+/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsinglek/summationdisplay
i=1w(2)
iw(1)
i,2x2/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
Leto1(w0) =/summationtextk
i=1w(2)
iw(1)
i,1ando2(w0) =/summationtextk
i=1w(2)
iw(1)
i,2for ease of writing, then take expectation over
w0, we directly place Ew0[|o1(w0)|]=Ew0[|o2(w0)|]≤2kσ2
πbecause of half normal distribution and i.i.d
assumption ,i.e.,
Ew0[f(w0,z)]≤|y|+2kσ2
π(|x1|+|x2|)
Taking expectation over z, we have
Ew0,z∈S[f(w0,z)] = 1 +4kσ2µp
π(20)
Now, we show the assumption P1 holds. Note that we ignore the case when weights are exactly zero or
weights become exactly equal to other weights to avoid zero in the denominator. We take M= 0from
assumption P1. Now we take the upper bound of L.H.S. of assumption P1 (without expectation),
∥∇f(w,z)∥2≤n·max
j(∇f(w,z)2
j)
and we take the lower bound of R.H.S. using
n·min
i(Ez∈S[∇f(w,z)i])2≤∥Ez∈S[∇f(w,z)]∥2
Using the above two inequalities and taking M= 0in assumption P1, we get,
/parenleftigg
Ez∈S/bracketleftbig
maxj∈[n]{∇f(w,z)2
j}/bracketrightbig
mini∈n{Ez∈S[∇f(w,z)i]2}/parenrightigg
≤MG
30Published in Transactions on Machine Learning Research (02/2024)
Now, calculating for numerator, we first write the max over the square of gradients,
max
j{(∇f(w,z)j)2}= max
j

/parenleftigg
∂O(w,x)
∂w(j1)
j2,j3/parenrightigg2


= max{(w(2)
j2)x2
j3}, ifj1= 1
= max{(w(1)
j2,1x1+w(1)
j2,2x2)2},ifj1= 2
Letwhbe the highest absolute value of weight(s) and wlbe the lowest absolute value of weight(s). To easily
calculate expectation, we take out the max weights across all, we get an upper bound for the numerator,
Ez∈S/bracketleftbigg
max
j/braceleftbig
(∇f(w,z)j)2/bracerightbig/bracketrightbigg
≤2w2
hσ2(21)
Now lower bounding denominator, so square of expectation of partial derivative, i.e.,
Ez∈S[∇f(w,z)i]2=/parenleftigg
Ez∈S/bracketleftigg
∂O(w,x)
∂w(i1)
i2,i3/bracketrightigg/parenrightigg2
= (w(2)
i2E[xi3])2, ifi1= 1
= (w(1)
i2,1E[x1] +w(1)
i2,2E[x2])2,ifi1= 2
Fori1= 2term, after taking expectation, we could write it as,
(w(1)
i2,1E[x1] +w(1)
i2,2E[x2])2=1
4/parenleftig
w(1)
i2,1+w(1)
i2,1/parenrightig2
Since we have Bfor all ratios of weights we could use this to bound below the absolute difference between
any pair of weights (i.e., |w′
l−wl|≥|wl/B−wl|), and we get
(w(1)
i2,1E[x1] +w(1)
i2,2E[x2])2≥w2
l(B−1)2/B2
So we can bound the whole denominator by,
min
i/braceleftbig
(Ez∈S[∇f(w,z)i])2/bracerightbig
≥1
4min/braceleftbigg
w2
l,w2
l(B−1)2
B2/bracerightbigg
(22)
≥w2
l(B−1)2
4B2(23)
Using 21 and 22 we get,
MG=8B4σ2
(B−1)2(24)
And from 20 and 24 we get the theorem statement.
31