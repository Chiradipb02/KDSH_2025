Published in Transactions on Machine Learning Research (05/2024)
Geometrical aspects of lattice gauge equivariant convolu-
tional neural networks
David I. Müller dmueller@hep.itp.tuwien.ac.at
TU Wien, Institute for Theoretical Physics, A-1040 Vienna, Austria
Jimmy Aronsson jimmyar@chalmers.se
Chalmers University of Technology, Department of Mathematical Sciences,
SE-412 96 Gothenburg, Sweden
Daniel Schuh schuh@hep.itp.tuwien.ac.at
TU Wien, Institute for Theoretical Physics, A-1040 Vienna, Austria
Reviewed on OpenReview: https: // openreview. net/ forum? id= zO4aAVHxPe
Abstract
Lattice gauge equivariant convolutional neural networks (L-CNNs) are a framework for
convolutional neural networks that can be applied to non-abelian lattice gauge theories
without violating gauge symmetry. We demonstrate how L-CNNs can be equipped with
global group equivariance. This allows us to extend the formulation to be equivariant not
just under translations but under global lattice symmetries such as rotations and reflections.
Additionally, we provide a geometric formulation of L-CNNs and show how convolutions in
L-CNNs arise as a special case of gauge equivariant neural networks on SU(N)principal
bundles.
1 Introduction
In recent years, machine learning methods incorporating ideas based on symmetry and geometry, often
summarized under the term geometric deep learning (Bronstein et al., 2021; Gerken et al., 2023), have
received much attention in both computer vision and physics. Most famously, convolutional neural networks
(CNNs) (LeCun et al., 1989) have proven to be an excellent machine learning architecture for computer
vision tasks such as object detection and classification. The classic examples include determining whether
an image contains a particular animal (e.g. cat or dog (Parkhi et al., 2012)), or identifying numbers in images
of hand-written digits (Deng, 2012). For these tasks, it has been demonstrated that CNN architectures excel
both in terms of accuracy and reduced model size, i.e. the number of model parameters. A key differentiating
feature of CNNs compared to generic neural networks is that they are formulated as stacks of convolutional
layers, which exhibit translational symmetry or, more accurately, translational equivariance. If a translation
is applied to the input of a convolutional layer, then the resulting output will be appropriately shifted as
well. This equivariance property is highly useful in the case of image classification, where the absolute
position of a particular feature (a cat; a hand-written digit) in the image is not important. Translational
equivariance further implies weight sharing, which reduces the number of required model parameters and
training time. Consequently, CNNs provide not just more accurate but also more robust models compared
to their translationally non-symmetric counterparts.
Whereas CNNs are only guaranteed to be translation equivariant, the concept of equivariance in neural net-
works can be extended to symmetries beyond translations, such as rotations or reflections. Group equivariant
CNNs (G-CNNs) (Cohen & Welling, 2016; Cohen et al., 2019a; Aronsson, 2022) and steerable CNNs (Cohen
&Welling,2017;Weileretal.,2018;Cesaetal.,2022)useconvolutionsongroupstoachieveequivariancewith
respect to general global symmetries. Analogous to the equivariance property of traditional CNNs, group
1Published in Transactions on Machine Learning Research (05/2024)
transformations (e.g. roto-translations) applied to the input of a group equivariant convolutional layer, lead
to the same group transformation being consistently applied to the output. Group convolutional layers thus
commute with group transformations. In certain applications where larger symmetries are important, these
networks have been shown to further improve performance compared to networks exhibiting less symmetry
(Graham et al., 2020; Gerken et al., 2022). From a physical perspective, the symmetries considered in CNNs
and, more generally, in G-CNNs are analogous to global symmetries of lattice field theories, which has led to
numerous applications of CNNs in high energy physics (see Boyda et al. (2022) for a review). For example,
CNNs have been applied to detect phase transitions and learn observables (Zhou et al., 2019; Blücher et al.,
2020; Bachtis et al., 2020; Bulusu et al., 2021; Bachtis et al., 2021) and as generative models (Nicoli et al.,
2021; Albergo et al., 2021a; de Haan et al., 2021; Gerdes et al., 2023; Albergo et al., 2021b) in both scalar
and fermionic lattice field theories.
In addition to global symmetries, the laws of physics of the fundamental interactions are based on the notion
of local symmetry, which is the foundation of gauge theories. Local symmetries allow for group transforma-
tions that can differ at every point in space-time. In machine learning, gauge equivariant neural networks
(Cohen et al., 2019b; Cheng et al., 2019) (see also Gerken et al. (2023) for a review) have been proposed
as architectures that are well-suited for data living on curved manifolds. In high energy physics, similar
methods have been applied to problems in lattice gauge theory. For example, gauge symmetric machine
learning models have been used as generative models (Kanwar et al., 2020; Boyda et al., 2021; Albergo et al.,
2021a; Abbott et al., 2022; Bacchio et al., 2023) to avoid the problem of critical slowing down inherent to
Markov Chain Monte Carlo simulations at large lattice sizes, or as machine-learned preconditioners for the
Dirac equation in lattice QCD (Lehner & Wettig, 2023a;b; Knüttel et al., 2024). Going beyond specific
applications, Lattice gauge equivariant CNNs (L-CNNs) (Favoni et al., 2022) have recently been proposed
as a general gauge equivariant architecture for generic machine learning problems in lattice gauge theory.
L-CNNs use gauge field configurations as input and can process data in a manner compatible with gauge
symmetry. They consist of a set of gauge equivariant layers to build up networks as stacks of individual
layers. In particular, gauge equivariant convolutional layers (L-Convs) are convolutional layers which use
parallel transport to preserve gauge symmetry while combining data at different lattice sites. Because of
their expressiveness, L-CNNs can be used as universal approximators of arbitrary gauge equivariant and in-
variant functions. It has been demonstrated in Favoni et al. (2022) that L-CNNs can accurately learn gauge
invariant observables such as Wilson loops from datasets of gauge field configurations. Similar to CNNs, the
convolutions used in L-CNNs are equivariant under lattice translations.
In this paper, we revisit L-CNNs from a geometric point of view and extend them by including a larger
degree of lattice symmetry. First, we review lattice gauge theory and the original formulation of L-CNNs
in Section 2. L-CNNs were originally constructed by incorporating local symmetry into ordinary CNNs,
which means that L-CNNs are equivariant under lattice translations but not under other lattice symmetries
such as rotations and reflections. We remedy this in Section 3 by applying methods from G-CNNs to L-
CNNs. Our main result is a gauge equivariant convolution that can be applied to tensor fields and that is
equivariant under translations, rotations, and reflections. Finally, in Section 4, we put the original L-CNNs
in a broader context by relating them to a mathematical theory for equivariant neural networks. In doing
so, we demonstrate how convolutions in L-CNNs can be understood as discretizations of convolutions on
SU(N)principal bundles.
2 Theoretical background
In this section, we review Yang-Mills theory and lattice gauge theory in the way these topics are usually
introduced within high-energy physics, largely following the conventions of Peskin & Schroeder (1995) and
Gattringer & Lang (2010). Having defined concepts such as gauge symmetry and gauge invariance, we then
review aspects of L-CNNs.
2.1 Yang-Mills theory
We consider SU(N)Yang-Mills theory on Euclidean space-time M=RDwithD−1>0spatial dimen-
sions. We choose Cartesian coordinates xµonMwithµ∈{1,2,...,D}such that the metric on Mis Eu-
2Published in Transactions on Machine Learning Research (05/2024)
clidean, i.e. gµν=δµν, whereδµνis the Kronecker symbol. The degrees of freedom in this theory are gauge
fieldsAµ(x), whichare su(N)-valuedvectorfieldson M. Wefurtherchooseamatrixrepresentationof su(N),
namely the fundamental representation spanned by the generators ta∈CN×Nwitha∈{1,2,...,N2−1},
which are traceless Hermitian matrices usually normalized to satisfy
Tr/bracketleftbig
tatb/bracketrightbig
=1
2δab. (1)
We note that the convention to use traceless Hermitian matrices to describe su(N), in contrast to anti-
Hermitian matrices, is often used in high-energy physics. These two conventions are related by replacing
ta→ita, whereiis the imaginary unit. With a basis for both Mand su(N), a gauge field can be written
as the 1-form
A(x) =Aµ(x)dxµ=Aa
µ(x)tadxµ, (2)
with components Aa
µ:M→ R. Two different gauge fields AandA′are considered to be gauge equivalent if
their components can be related via a gauge transformation TΩ,
A′
µ(x) =TΩAµ(x) = Ω( x)(Aµ(x)−i∂µ)Ω†(x), (3)
where Ω :M→ SU(N)is a differentiable function on space-time. Gauge fields that can be related via gauge
transformations form an equivalence class. Within gauge theory, (the components of) gauge fields are not
considered physical, observable fields. Rather, the physical state of a system is the same for all gauge fields
in any particular equivalence class. Observables within gauge theory therefore must be gauge invariant
functionals ofA. The most prominent example of such a gauge invariant functional is the Yang-Mills action
S[A] =1
2g2/integraldisplay
MdDxTr [Fµν(x)Fµν(x)], (4)
which maps a gauge field Ato a single real number S[A]∈R. Here,g > 0is the Yang-Mills coupling
constant, and the su(N)-valued field strength tensor is given by
Fµν(x) =∂µAν(x)−∂νAµ(x) +i[Aµ(x),Aν(x)], (5)
where [,]denotes the commutator of matrices in the fundamental representation of su(N). Under gauge
transformations, the field strength tensor is transformed according to
TΩFµν(x) = Ω( x)Fµν(x)Ω†(x). (6)
Because of the transformation behavior of the field strength tensor and the trace in the Yang-Mills action,
the value of the action is invariant under gauge transformations, i.e.
S[TΩA] =S[A]. (7)
The invariance of the Yang-Mills action under gauge transformations is called gauge symmetry.
2.2 Lattice gauge theory
Lattice discretizations of non-abelian Yang-Mills theory with exact lattice gauge symmetry can be con-
structed with the help of the link formalism of lattice gauge theory (Wilson, 1974). In this formalism, the
gauge fields Aµ(x)∈su(N)are replaced by gauge link variables Ux,µ∈SU(N)defined on the edges of a
finite hypercubic lattice Λwith periodic boundary conditions. We use the fundamental representation of
su(N)andSU(N)to represent gauge fields and gauge links as complex matrices. The links Ux,µconnect a
lattice site xto its neighboring sites x+µ=x+aˆeµ, separated by the lattice spacing aand the Euclidean
basis vector ˆeµ. The inverse link, which connects x+µtox, is written as Ux+µ,−µ=U†
x,µ.
In terms of the gauge field, a gauge link is given by the path-ordered exponential
Ux,µ=Pexp

i1/integraldisplay
0dsdxν(s)
dsAν(x(s))

. (8)
3Published in Transactions on Machine Learning Research (05/2024)
Intheconventionthatisusedhere, thepathorderingoperator Pshiftsfieldsearlierinthepathtotheleftand
fields later to the right of the product. The function x(s) : [0,1]→RDparameterizes the straight-line path
connecting xtox+µ. Geometrically, the gauge links prescribe how to parallel transport along the edges of
the lattice. They transform under general lattice gauge transformations TΩ,Ω : Λ→SU(N)according to
TΩUx,µ= Ω xUx,µΩ†
x+µ. (9)
Gauge links are the shortest possible Wilson lines on the lattice. Longer Wilson lines are formed by multi-
plying links that connect consecutive points to form an arbitrary path on the lattice. For closed paths, they
are referred to as Wilson loops, and the smallest loop, which is the 1×1loop, is called a plaquette and reads
Ux,µν=Ux,µUx+µ,νUx+µ+ν,−µUx+ν,−ν. (10)
It transforms under gauge transformations as given by
TΩUx,µν= Ω xUx,µνΩ†
x. (11)
The Wilson action (Wilson, 1974), which can be written in terms of plaquettes, reads
SW[U] =2
g2/summationdisplay
x∈Λ/summationdisplay
µ<νRe Tr [ 1−Ux,µν]. (12)
Note that (12) is invariant under global symmetries of the lattice such as translations, discrete rotations, and
reflections. Its invariance under lattice gauge transformations follows from the local transformation property
of the plaquettes and the trace.
In the continuum limit, for small lattice spacings a≪1, gauge links can be approximated by the matrix
exponential
Ux,µ≈exp/parenleftbigg
iaAµ/parenleftbigg
x+1
2µ/parenrightbigg/parenrightbigg
(13)
at the midpoint x+1
2µ. Furthermore, in this limit, plaquettes approximate the non-abelian field strength
tensor, given by Eq. (5),
Ux,µν≈exp/parenleftbigg
ia2Fµν/parenleftbigg
x+1
2µ+1
2ν/parenrightbigg/parenrightbigg
, (14)
and the Wilson action approximates the Yang-Mills action, introduced in Eq. (4).
2.3 Lattice gauge equivariant convolutional neural networks
An L-CNN is built up by individual layers Φ, which take as input at least one tuple (U,W). The first part of
the tuple,U={Ux,µ}, is a set of gauge links in the fundamental representation that transform non-locally
as in Eq. (9). The second part, W={Wx,a}, is a set of complex matrices Wx,a∈CN×Nthat transform
locally, like plaquettes, as in Eq. (11):
TΩWx,a= Ω xWx,aΩ†
x. (15)
Here, the index a∈{1,...N ch}refers to the channel, and Nchdenotes the total number of channels in the
layer in question. The output of the layer is, generally, again a tuple (U′,W′), with possibly a different
number of channels. We require every layer to be lattice gauge equivariant in the sense of
Φ(TΩU,TΩW) =T′
ΩΦ(U,W), (16)
whereT′
Ωdenotes the application of the gauge transformation Ωto the output of the layer. Generally, one
can consider layers where T′
Ω̸=TΩ. This would be the case if the representation of the input tuple (U,W)is
different from the one of the output tuple (U′,W′). In this work, we only consider layers that do not change
the representation of SU( N) or the transformation behavior of the links Uand the locally transforming
matricesWin any way. Additionally, we only focus on layers that do not modify the set of gauge links U,
i.e. we always require that U′=U. A network built from multiple gauge equivariant layers {Φ1,Φ2,..., ΦN}
4Published in Transactions on Machine Learning Research (05/2024)
through composition, ΦN◦···◦ Φ2◦Φ1, respects lattice gauge equivariance in the sense of Eq. (16). In the
following, we review some of the layers introduced in Favoni et al. (2022).
A convolutional layer usually aims to combine data from different locations with trainable weights in a trans-
lationally equivariant manner. In an L-CNN, such a layer is required to respect lattice gauge equivariance
as well. This is fulfilled by the Lattice gauge equivariant Convolutional (L-Conv) layer, which is a map
(U,W)∝⇕⊣√∫⊔≀→(U,W′), defined as
W′
x,a=/summationdisplay
b,µ,kψa,b,µ,kUx,k·µWx+k·µ,bU†
x,k·µ, (17)
with the trainable weights ψa,b,µ,k∈C, output channel index 1≤a≤Nch,out, input channel index
1≤b≤Nch,in, lattice directions 1≤µ≤Dand distances−Nk≤k≤Nk, whereNkdetermines the ker-
nel size. Note that the output channels associated with W′are a linear combination of the input channels of
W. In general, the number of input channels Nch,inmay differ from the number of output channels Nch,out.
The matrices Ux,k·µappearing in Eq. (17) describe parallel transport starting at the point xto the point
x+k·µ. They are given by
Ux,k·µ=k−1/productdisplay
i=0Ux+i·µ,µ=Ux,µUx+µ,µUx+2·µ,µ...U x+(k−1)·µ,µ (18)
for positive kand
Ux,k·µ=k−1/productdisplay
i=0Ux−i·µ,−µ=Ux,−µUx−µ,−µUx−2·µ,−µ...U x−(k−1)·µ,−µ (19)
fornegative k. Onlyparalleltransportsalongstraightpathsareconsideredbecausetheshortestpathbetween
two lattice sites is not unique otherwise. Data on lattice points that are not connected by straight paths can
be combined by stacking multiple layers. A bias term can be included by adding the unit element 1to the
setW. A further increase in expressivity can be achieved by also adding the Hermitian conjugates of Wx,i
toW. A general L-Conv layer thus may be written as
W′
x,a=/summationdisplay
b,µ,kψa,b,µ,kUx,k·µWx+k·µ,bU†
x,k·µ+/summationdisplay
b,µ,k˜ψa,b,µ,kUx,k·µW†
x+k·µ,bU†
x,k·µ+ψ01, (20)
with weights ψa,b,µ,k,˜ψa,b,µ,kand a bias term ψ0. For brevity, we will use the more compact form given
in Eq. (17). L-Conv layers are gauge equivariant by virtue of the transformation behavior of the parallel
transporters Ux,k·µ. From Eq. (9) it follows that
TΩUx,k·µ= Ω xUx,k·µΩ†
x+k·µ. (21)
The matrices Ux,k·µthus allow the L-Conv layer to combine data from various lattice sites without violating
gauge symmetry.
It follows that if data are combined only locally, there is no need for parallel transport to construct a lattice
gauge equivariant layer. This is realized by Lattice gauge equivariant Bilinear (L-Bilin) layers, which are
maps (U,W),(U,W′)∝⇕⊣√∫⊔≀→(U,W′′), given by
W′′
x,a=/summationdisplay
b,cαa,b,cWx,bW′
x,c. (22)
The weights αa,b,c∈Care trainable, have an output channel index 1≤a≤Nch,outand two input chan-
nel indices 1≤b≤Nch,inand1≤c≤N′
ch,in. Analogously to the L-Conv layer, the unit element and the
Hermitian conjugates can be added to Wto increase the expressivity of the L-Bilin layer.
Lattice gauge equivariant Activation functions (L-Act), which are maps (U,W)∝⇕⊣√∫⊔≀→(U,W′), are the gener-
alization of standard activation functions to the L-CNN. They can be applied at every lattice site and are
given by
W′
x,a=νx,a(W)Wx,a, (23)
5Published in Transactions on Machine Learning Research (05/2024)
whereνis any scalar-valued and gauge invariant function. One option is νx,a(W) = Θ(Re(Tr( Wx,a))), with
the Heaviside function Θ. For real-valued scalars s, this would lead to the well-known ReLU activation
function, which can also be written as ReLU(s) = Θ(s)s.
Finally, the last important layer to consider is the Trace layer. This layer maps (U,W)∝⇕⊣√∫⊔≀→T x,a, which
converts the lattice gauge equivariant quantities (U,W)to lattice gauge invariant quantities
Tx,a(U,W) = Tr(Wx,a). (24)
A gauge invariant layer such as this is necessary if the network output is supposed to approximate a (gauge
invariant) physical observable.
As an example, an L-CNN can compute the plaquettes as a pre-processing step and use them as local
variablesWx,ain the subsequent layers. With stacks of L-Conv and L-Bilin layers, it can build arbitrarily
shaped loops, depending on the number of these stacks (Favoni et al., 2022). The network expressivity can
be increased further by introducing L-Acts between said stacks, and if the output is a physical observable,
there should be a Trace layer at the end. After the Trace layer, a conventional CNN or other neural network
can be added without breaking lattice gauge equivariance.
3 Extending L-CNNs to general group equivariance
In the last section, we have reviewed the L-Conv operation on a hypercubic lattice Λ =ZD. Like a standard
convolutional layer, the L-Conv layer is equivariant under (integer) translations on ZD, which, when inter-
preted as a group T, can be identified with the lattice itself, T∼ZD. However, lattice gauge theories on
hypercubic lattices typically exhibit larger isometry groups Gthat include discrete rotations and reflections,
which we denote by the subgroup K⊂G. In this section, we explicitly construct L-CNN layers that are
compatible with general G-symmetry.
The original approach to G-CNNs (Cohen & Welling, 2016) is based on promoting feature maps from
functions on the lattice ZD, or, more generally, functions on some space M, to functions on the group G.
Group equivariant convolutions, or G-convolutions, are analogous to traditional convolutions, except that
integrals (or sums in the case of a discrete space M) are carried out over the whole group G. In contrast, the
modern approach to G-CNNs on homogeneous spaces Muses a fiber bundle formalism (Cohen et al., 2019a;
Aronsson, 2022) in which feature maps are modeled via fields on M, that is, via sections of associated vector
bundles overM. This approach is geometrically pleasing because it means that the inputs to and outputs
from convolutional layers live directly on M. It also offers computational advantages since convolutional
layersbecomeintegralsover M≃G/Kratherthanintegralsoverthelargerspace G. Here,Kisthesubgroup
ofGthat stabilizes an arbitrarily chosen origin in M.
However, in order to take advantage of this simplification, one needs to identify feature maps f:G→RN
with fields onM. This imposes a constraint given by
f(gk) =ρ(k)−1f(g), (25)
whereg∈G,k∈K, andρis a representation of K(see Aronsson (2022) for details). This constraint is
difficult to enforce numerically and is sometimes ignored, preventing the geometric view of fas a field on
M. Fortunately, ignoring the constraint effectively promotes fto a field on the group Ginstead, similar
to the approach laid out in Cohen & Welling (2016), and the bundle formalism still applies after changing
the homogeneous space from MtoG. Even though ignoring the constraint makes convolutional layers more
expensive to compute, it drastically simplifies their implementation in machine learning frameworks such as
PyTorch (Paszke et al., 2019). In our case, this is because the group Gof lattice symmetries is a semi-direct
productG=T⋊Kof translations x∈Tand rotoreflections r∈G/T=K. Group elements can thus be
split into products g=xr. Consequently, feature maps f:G→RNcan be viewed as “stacks” of feature
mapsfr(x)on the lattice ZD. It can be shown that G-convolutions can be expressed in terms of traditional
ZD-convolutions (Cohen & Welling, 2016), for which highly efficient implementations already exist.
Our strategy to develop a G-equivariant framework for L-CNNs is thus the following: We first review group
equivariant networks without gauge symmetry by working out explicit G-convolutions for scalar fields, vector
6Published in Transactions on Machine Learning Research (05/2024)
fields, and general tensor fields discretized on the lattice ZD, in the spirit of the original G-CNN formulation
(Cohen & Welling, 2016). We then show how G-convolutions can be combined with our approach to lattice
gauge equivariant convolutions to obtain fully G-equivariant L-Convs. We extend our approach to bilinear
layers (L-Bilin), activation layers (L-Act), trace layers, and pooling layers, which allows us to formulate fully
G-equivariant L-CNNs.
3.1 Group equivariant convolutions for scalars on the lattice
Convolutional layers in traditional CNNs act on feature maps, e.g. functions f:ZD→Rn, wherenis the
number of channels. For explicitness, we consider real-valued feature maps. A real-valued convolution with
ninput channels and n′output channels is given by
[ψ∗f]a(x) =n/summationdisplay
b=1/summationdisplay
y∈ZDψab(y−x)fb(y), a∈{1,2,...n′}, (26)
whereψ:ZD→Rn′×nare the kernel weights. Here we explicitly use boldfaced letters to denote points on
the lattice ZDand the symbol∗to denote a ZD-convolution. The convolution operation is equivariant with
respect to translations: applying a translation z∈T, which can be identified with the point z∈ZD, to
[ψ∗f]yields
Lz[ψ∗f]a(x) = [ψ∗f]a(x−z)
=n/summationdisplay
b=1/summationdisplay
y∈ZDψab(y−x+z)fb(y)
=n/summationdisplay
b=1/summationdisplay
y′∈ZDψab(y′−x)fb(y′−z)
= [ψ∗Lzf]a(x).(27)
The left translation Lzcommutes with the convolution operation. The main idea of Cohen & Welling (2016)
is to introduce convolutions that are G-equivariant, i.e. that commute with Lgforg∈G. More specifically,
two types of G-convolutions ( G-Convs) are introduced: the first-layer G-convolution acts on feature maps on
ZDand promotes them to feature maps on the group G, and the full G-convolution, which acts on feature
maps onGand outputs feature maps on G. To simplify notation and without loss of generality, we set the
number of channels to one.
The first-layer G-convolution acting on a feature map f:ZD→Ris given by (Cohen & Welling, 2016)
[ψ⋆f ](g) =/summationdisplay
y∈ZDψ(g−1·y)f(y), g∈G, (28)
whereψ:ZD→Rare the kernel weights, and g−1·ydenotes the action of the group element g−1on the
point y∈ZD. Notethatwedenote ZD-convolutions, asin(26), by ∗andG-convolutionsby ⋆. Uniquelysplit-
ting the group element ginto a translation x∈T(identified with x∈ZD) and a rotoreflection r∈K=G/T
about the origin, g=xr, we have
g−1·y=R−1(y−x), (29)
whereR∈ZD×Dis a matrix representation of the rotoreflection r. This split is unique because Gis a
semi-direct product G=T⋊K. In addition, the translations Tform a normal subgroup of G:
g−1xg∈T,∀x∈T, g∈G. (30)
Note that the result of the G-convolution in Eq. (28) is a function [ψ⋆f ] :G→Ron the group G. The
effect of the rotoreflection ris that the feature map fis convolved with the rotated kernel
Lrψ(y−x) =ψ(R−1(y−x)). (31)
7Published in Transactions on Machine Learning Research (05/2024)
The first-layer G-Conv can therefore be written as a convolution over ZD
[ψ⋆f ](xr) = [Lrψ∗f](x), (32)
which we refer to as the split form (see also section 7 of Cohen & Welling (2016)).
The fullG-convolution acts on feature maps f:G→Rand is used after the first-layer G-convolution. It is
given by
[ψ⋆f ](g) =/summationdisplay
h∈Gψ(g−1h)f(h), (33)
whereψ:G→Rare the kernel weights. Since we are dealing with discrete groups, we use a sum over the
group elements to define the convolution. Both gandhare elements of the group G, andg−1hdenotes the
group product. Just as before, we would like to write this operation in terms of ZD-convolutions (the split
form) using g=xrandh=yswithx,y∈Tandr,s∈G/T. In order to perform this split, we need to be
able to interpret functions on Gas “stacks” of functions on ZD. Givenf:G→Randh=yswe write
f(h) =f(ys) =fs(y), (34)
wherefs:ZD→Rfor each element s∈G/T. The function fis therefore equivalent to a stack of func-
tions{fs|s∈G/T}. A left translation acting on fwithg=xrandh=ysinduces
Lgf(h) =f(g−1h)
=f((xr)−1ys)
=f(r−1x−1yrr−1s)
=fr−1s(R−1(y−x)),(35)
wherez=r−1x−1yr∈T(asTis a normal subgroup of G) andr−1s∈G/T. In the last line we have made
use of the fact that pure translations x∈Tcan be uniquely identified with points x∈ZDvia the action of
the translation subgroup on the origin 0. The point zassociated with zis given by
z=z·0=r−1·((x−1y)·(r·0)) =r−1·((x−1y)·0) =r−1·(y−x) =R−1(y−x), (36)
wherer·0=0because rotoreflections form the stabilizer subgroup associated with the origin.
The kernel in Eq. (33) can thereby be written as
ψ(g−1h) =ψr−1s(R−1(y−x)), (37)
hence the split form of the full G-convolution is given by
[ψ⋆f ](xr) =/summationdisplay
s∈G/T/summationdisplay
y∈ZDψr−1s(R−1(y−x))fs(y)
=/summationdisplay
s∈G/T[(Lrψr−1s)∗fs](x),(38)
which is a sum of multiple ZD-convolutions with rotated kernels Lrψr−1s(y−x). The split forms Eqs. (32)
and (38) are particularly useful for concrete implementations in machine learning frameworks. Writing the
G-convolutions in terms of ZD-convolutions allows us to make use of highly optimized implementations such
as theConv2D andConv3D functions provided by PyTorch.
Both types of G-convolutions can be compactly written as
[ψ⋆f ](g) =/summationdisplay
h∈HLgψ(h)f(h), (39)
where we use H=ZDfor the first-layer G-Conv and H=Gfor the full G-Conv. In this form, it is evident
thatthetwotypesmerelydifferinthegroupthatisbeingsummedover(translations T∼ZDinthefirst-layer
8Published in Transactions on Machine Learning Research (05/2024)
G-Conv, the full group Gin the fullG-Conv) and how the left translation acts on the kernel ψ. Depending
on the choice of H, the left translated kernel Lgψis either a rotated ZD-kernel forH=ZDor a translated
kernel on the group for H=G. It is now easy to check that G-convolutions are in fact equivariant under
left translations Lg. Letk∈G, then we have
Lk[ψ⋆f ](g) = [ψ⋆f ](k−1g)
=/summationdisplay
h∈HLk−1gψ(h)f(h)
=/summationdisplay
h∈HLgψ(kh)f(h)
=/summationdisplay
h′∈HLgψ(h′)f(k−1h′)
=/summationdisplay
h′∈HLgψ(h′)Lkf(h′)
= [ψ⋆Lkf](g),(40)
where we have used the substitution h′=khin the fourth line. For the first-layer G-Conv (H=ZD), where
h∈ZD,h′=khis to be interpreted as a rotated and shifted coordinate on ZDas in Eq. (29), whereas for
the fullG-Conv (H=G),h′=khis simply a translated group element in G. Note that G-equivariance can
also be shown for the split forms Eqs. (32) and (38), but the proof is analogous to the one shown above.
3.2 Group equivariant convolutions for vector and tensor fields
We have explicitly shown that the G-convolutions are equivariant under general transformations gvia left
translations Lg. The feature maps f:ZD→R, on which the convolutions act, transform like scalar fields
underLgwithg=xr:
Lgf(y) =f(R−1(y−x)). (41)
In order to extend G-convolutions to vectors and tensors, we would expect transformations that act on
the vector structure. For example, consider a vector field v:ZD→RDon the lattice with components
vi:ZD→R. Acting on vwith a general transformation g=xryields
(Lgv)i(y) =Ri
jvj(R−1(y−x)), (42)
whereRijare the components of the matrix representation of r∈G/TonRD. More generally, a type (n,m)
tensor field w, i.e. withnvector and mco-vector components, transforms according to
(Lgw)i1...inj1...jm(y) =Ri1i′
1...Rini′nwi′
1...i′
nj′
1...j′m(R−1(y−x))(R−1)j′
1j1...(R−1)j′
mjm.(43)
Based on the compact form of scalar G-convolutions, Eq. (39), we now make the following guess at G-con-
volutions which map tensors of type (n,0)to tensors of the same type:
[ψ⋆w ]i1...in(g) =/summationdisplay
h∈H(Lgψ)i1...inj1...jn(h)wj1...jn(h), (44)
whereH=T∼ZDfor the first-layer and H=Gfor the full G-Conv. Here, the kernel ψis a tensor of
type (n,n)and acts as a general linear transformation of the tensor components of w. For the full G-Conv,
H=G, left translations acting on tensor fields ψof type (n,m)on the group are given by
(Lgψ)i1...inj1...jm(h) =Ri1i′
1...Rini′nψi′
1...i′
nj′
1...j′m(g−1h)(R−1)j′
1j1...(R−1)j′
mjm. (45)
G-convolutions for tensors of mixed type (n,m)are defined analogously.
9Published in Transactions on Machine Learning Research (05/2024)
The tensor G-convolutions can be cast into a more compact form by introducing condensed index notation:
we write the block of indices i1...inas multi-indices Iso that
wI
J:=wi1...inj1...jn,
δI
J:=δi1j1...δinjn,
RI
J:=Ri1j1...Rinjn.(46)
Contractions of RandR−1can be written as
RI
J(R−1)J
K=Ri1j1...Rinjn(R−1)j1k1...(R−1)jnkn
=Ri1j1(R−1)j1k1...Rinjn(R−1)jnkn
=δi1k1...δinkn
=δI
K.(47)
Using multi-indices, the tensor G-convolutions are given by
[ψ⋆w ]I(g) =/summationdisplay
h∈H(Lgψ)I
J(h)wJ(h), (48)
and left translations act on tensors according to
(Lgw)I
J(h) =RI
I′wI′
J′(g−1h)(R−1)J′
J. (49)
Equivariance of Eq. (48) follows from
(Lk[ψ⋆w ])I(g) =RI
I′[ψ⋆w ]I′(k−1g)
=/summationdisplay
h∈HRI
I′(Lk−1gψ)I′
J(h)wJ(h)
=/summationdisplay
h∈HRI
I′(R−1)I′
I′′(Lgψ)I′′
J′(kh)RJ′
JwJ(h)
=/summationdisplay
h′∈H(Lgψ)I
J(h′)RJ′
JwJ(k−1h′)
= [ψ⋆Lkw]I(g),(50)
where we have used the substitution h′=kh.
We can also define G-convolutions that change the tensor type. Given multi-indices In=i1...inand
Jm=j1...jmwithn̸=min general, we define
˜wIn= [ψ⋆w ]In(g) =/summationdisplay
h∈H(Lgψ)InJm(h)wJm(h), (51)
withH=ZDfor the first-layer and H=Gfor the full G-Conv. Here, the output feature map ˜wis a tensor
of type (n,0)while the input feature map wis a tensor of type (m,0). The kernel ψis of type (n,m).
For example, one can use these types of G-convolutions to reduce a rank 2 tensor to a vector field within a
G-CNN while keeping the equivariance property.
3.3 Split forms of tensor G-convolutions
As in the case of scalar G-convolutions, it is beneficial to write tensor G-convolutions Eqs. (48) in their split
forms, analogous to Eqs. (32) and (38). Using h=ys∈Gwithy∈Tands∈G/T, we write the tensor field
of type (n,m)
wI
J(h) =wI
J(ys) = (ws)I
J(y), (52)
10Published in Transactions on Machine Learning Research (05/2024)
where y∈ZD. Analogous to Eq. (35), left translations act on wvia
(Lgw)I
J(h) =RI
I′(wr−1s)I′
J′(R−1(y−x))(R−1)J′
J, (53)
whereg=xr,RIJis the (n,m)matrix representation of randRis its representation on ZD. The two types
of tensorG-convolutions can then be written as
[ψ⋆w ]I(g) =/summationdisplay
y∈ZDRI
I′ψI′
J′(R−1(y−x))(R−1)J′
JwJ(y)
= [(Lrψ)I
J∗wJ](x),(54)
[ψ⋆w ]I(g) =/summationdisplay
y∈ZD/summationdisplay
s∈G/TRI
I′(ψr−1s)I′
J′(R−1(y−x))(R−1)J′
JwJ
s(y)
=/summationdisplay
s∈G/T[(Lrψr−1s)I
J∗(ws)J](x),(55)
whereψIJdenotes the tensor components of the kernels, and ψIJ:ZD→Rfor the first-layer and
ψIJ:G→Rfor the full G-Conv, respectively. Analogously, similar split forms can be obtained for ten-
sors of mixed type and for G-convolutions that change the tensor type as in Eq. (51).
3.4 Implementing group equivariance in L-Convs for scalars and tensors
Having worked out G-convolutions for general tensors, we shift our focus back to gauge equivariance. The
L-Conv introduced in Eq. (17) can alternatively be written as
[ψ∗W](x) =/summationdisplay
y∈ZDψ(y−x)Ux→yW(y)Uy→x, (56)
where we write W(x)instead ofWxto denote a gauge dependent feature map. We keep the number of input
and output channels set to one for convenience. The paths x→yin the subscripts of the links Uare kept
general to further simplify notation. Since we only want to allow for straight paths, however, we set all other
elements of the kernel ψto zero. Note that we only need to consider how the convolution acts on the local
variablesWbecause the links are unaffected by an L-Conv.
The L-Conv operation is translationally equivariant, i.e. the gauge equivariant convolution commutes with
translations. When acting on [ψ∗W]with a translation z, we find
Lz[ψ∗W](x) = [ψ∗W](x−z) =/summationdisplay
y∈ZDψ(y−x+z)Ux−z→yW(y)Uy→x−z
=/summationdisplay
y′∈ZDψ(y′−x)Ux−z→y′−zW(y′−z)Uy′−z→x−z
= [ψ∗LzW](x),(57)
where the shift by zinduces a translation on both WandU. We note that our notation for gauge equivariant
convolutions explicitly hides the gauge links, but we want to stress that translations must also be applied
to the links Uand that links should be viewed as part of the input. Keeping in mind the transformation
propertiesof UandW,givenbyEqs.(9)and(15),itisstraightforwardtocheckthattheL-Convisequivariant
under gauge transformations:
[ψ∗TΩW](x) =/summationdisplay
y∈ZDψ(y−x)TΩUx→yTΩW(y)TΩUy→x= Ω(x)[ψ∗W](x)Ω†(x). (58)
Thus, the L-Conv commutes both with translations on ZDand with lattice gauge transformations.
Previously, we have determined the split forms of G-convolutions when the group Gis a semi-direct product
of translations T∼ZDand proper and improper rotations G/T. For scalar feature maps, we found Eqs. (32)
11Published in Transactions on Machine Learning Research (05/2024)
and (38), which reduce the G-convolution to ZD-convolutions with rotated kernels. Following the same
ideas, we now extend our lattice gauge equivariant convolutions so that they respect not only translations
but larger symmetry groups G. If we consider W:ZD→CN×Nto transform as a scalar under G,
LgW(x) =W(g−1·x) =W(R−1(x−y)), (59)
then we can easily extend G-convolutions to gauge dependent fields Wby replacing the standard ZD-con-
volution with the L-Conv operation. For the first-layer G-Conv of Eq. (32) we then have
[ψ⋆W ](xr) = [Lrψ∗W](x)
=/summationdisplay
y∈ZDψ(R−1(y−x))Ux→yW(y)Uy→x.(60)
The resulting feature map ˜W(g) = [ψ⋆W ](g)is now a feature map on the group Gand transforms under
Gaccording to
Lg˜W(h) =˜W(g−1h) = [ψ⋆LgW](h). (61)
Additionally, under lattice gauge transformations we have
TΩ˜W(g) =TΩ˜W(xr)
= [Lrψ∗TΩW](xr)
= Ω(x)[Lrψ∗W](xr)Ω†(x)
= Ω(x)˜W(xr)Ω†(x),(62)
i.e. the feature map ˜W(g) =˜W(xr)transforms locally at x. Similarly, the full G-Conv is given by
[ψ⋆W ](g) =/summationdisplay
h∈GLgψ(h)Uq(g)→q(h)W(h)Uq(h)→q(g), (63)
whereq:G→ZDprojects group elements g=xrdown to the lattice by acting on the origin:
q(g) =g·0=x·r·0=x·0=x. (64)
More compactly, both types of G-Conv can be written as
[ψ⋆W ](g) =/summationdisplay
h∈HLgψ(h)Uq(g)→q(h)W(h)Uq(h)→q(g), (65)
withH=T∼ZDorH=G. Similarly, we can generalize these scalar G-convolutions to tensor convolutions
in the same way as in the previous section. A general G-convolution for gauge dependent tensors is given by
˜WI(g) = [ψ⋆W ]I(g) =/summationdisplay
h∈H(Lgψ(h))I
JUq(g)→q(h)WJ(h)Uq(h)→q(g), (66)
whereWandψare of type (n,0)and(n,m), respectively, and the output feature map transforms as a
tensor of type (m,0). Note that gauge equivariant G-convolutions merely differ in the appearance of parallel
transporters compared to Eq. (48). Consequently, the proof of equivariance of Eq. (66) under left translations
inGlargely follows the steps performed in Eq. (50).
One aspect of G-equivariant L-Conv layers that has been missing in the discussion so far is the inclusion of
bias terms. Generally, bias terms are additional terms added to the convolution,
˜WI(g) = [ψ⋆W ]I(g) +bI(g), (67)
wherebI(g)is a tensor that is independent of the feature map WI(g). To retain group equivariance in
Eq. (67), we require bI(g)to be invariant under left translations:
(Lkb)I(g) =RI
JbJ(k−1g)!=bI(g),∀k∈G. (68)
12Published in Transactions on Machine Learning Research (05/2024)
Depending on the symmetry group G, the number of dimensions D, and the tensor type (n,0)of the
feature map, such invariant tensors may or may not exist. In the case of scalar feature maps, namely
W:ZD→CN×N(first layer) and W:G→CN×N(deeper layers), bias terms simply correspond to unit
matrices added to the output of the convolution:
˜W(g) = [ψ⋆W ](g) =/summationdisplay
h∈H(Lgψ(h))Uq(g)→q(h)W(h)Uq(h)→q(g)+b01, (69)
whereb0isatrainableparameter. However, thegeometricstructureofthesetermsbecomesmorecomplicated
for general tensor feature maps and depends on the symmetry group G. For example, if Gconsists of
rotations, reflections, and translations, then there is no vector-type (i.e. tensor of type (1,0)) bias term. For
rank 2 tensors, i.e. tensors of type (2,0), bias terms correspond to Kronecker deltas δij. InD= 3dimensions,
rank 3 bias terms may be given by the Levi-Civita tensor ϵijkassuming that reflections are not considered
as symmetries. More generally for the rotation group in D= 3, higher ranks are given by products and
contractions of δijandϵijk.
3.5G-equivariant bilinear layers
Bilinear layers, which map two feature maps into one, can be generalized to respect G-equivariance. Without
including channels, a G-equivariant bilinear layer for tensors reads
WI(g) = (Lgˆψ)I
JKVJ
1(g)VK
2(g), (70)
whereV1andV2are tensor feature maps of type (n1,0)and(n2,0), respectively. The product of V1andV2
is understood to be a matrix product with respect to their CN×Nmatrix structure. The weight tensor ˆψis
of type (m,n 1+n2)and the resulting tensor feature map Wis of type (m,0). Note that the weight tensor
is constant, i.e. it does not depend on the group element g. However, because of its index structure, it
transforms in the usual way
(Lgˆψ)I
JK=RI
I′ˆψI′
J′K′(R−1)J′
J(R−1)K′
K, (71)
whereRis the matrix representation of the rotational part rofg=xr. It follows that the bilinear layer is
equivariant under left translations:
(LkW)I(g) = (Lgˆψ)I
JK(LkV1)J(g)(LkV2)K(g). (72)
Gauge equivariance follows from the fact that when both V1andV2are locally transforming as in Eq. (11),
their matrix product also transforms locally. Consequently, the resulting feature map WItransforms locally
as well.
The bilinear layer can be expressed as a special case of a gauge equivariant tensor convolution. Consider a
tensorVof type (n1+n2,0)that factorizes into two tensors V1andV2with types (n1,0)and(n2,0):
VJK=VJ
1VK
2. (73)
A general lattice gauge equivariant convolution applied to Vthen reads
WI(g) =/summationdisplay
h∈H(Lgψ)I
JK(h)Uq(g)→q(h)VJ
1(h)VK
2(h)Uq(h)→q(g). (74)
The bilinear layer is local, i.e. feature maps are all evaluated at the same element g. We therefore assume
that the kernel has the particular form
ψI
JK(h) =ˆψI
JKδ(h), (75)
whereδ(e) = 1andδ(h) = 0forh̸=ewith the unit element e, and ˆψis a constant tensor. The left translated
kernelLgψ(h)only has a single non-zero contribution to the sum, namely when g−1his the unit element e
of the group, i.e. h=g. The convolution then reduces to Eq. (70), because the parallel transporters reduce
to unit matrices for constant paths q(g)→q(g):
Uq(g)→q(g)=1. (76)
Bias terms may also be included in the bilinear layer. As in the case of G-equivariant L-Conv layers, bias
terms must be invariant tensors.
13Published in Transactions on Machine Learning Research (05/2024)
3.6 Trace layers
It is straightforward to show that trace layers are compatible with G-equivariance. Given a locally gauge
transforming tensor field WI(g)we define the traced tensor wI(g)simply as
wI(g) = Tr[WI(g)], (77)
where the trace is taken over the CN×Nmatrix structure. The trace yields a gauge invariant tensor, which
can be shown via
TΩwI(g) = Tr[TΩWI(g)]
= Tr[Ω( x)WI(g)Ω†(x)]
= Tr[WI(g)]
=wI(g),(78)
where we have assumed q(g) =x. The traced tensor transforms under Gas a tensor because the left
translation Lk,∀k∈Gcommutes with the trace operation over CN×N. Note that the trace layer does not
have any trainable parameters. It can be used at the end of an L-CNN to obtain gauge invariant scalars and
tensors, which can then be further processed by standard group equivariant networks.
3.7 Activation functions
The purpose of an activation function is to introduce non-linearity into the network. In a CNN or G-CNN
with scalar input, it is usually applied point-wise, i.e. f′(x) =ν(f(x)), wheref,f′:G→Rare the feature
maps before and after applying the (scalar) activation function ν:R→R, respectively. When the input
feature map is a general tensor field wof type (n,0), we choose an ansatz similar to Eq. (23), namely
w′I=CνwI=ν(w)wI, (79)
with the operator Cν, which applies the activation function to the tensor. For G-equivariance to hold, Cν
has to commute with Lg
CνLgwI=LgCνwI. (80)
Keeping in mind the transformation property of w, which is given by Eq. (49), we find
ν(/tildewidew(h)) =ν(w(h)), (81)
where/tildewidewI(h) =RII′wI′(h). Thus, the activation function νhas to be invariant under transformations in the
groupG.
The generalization of Eq. (79) to lattice gauge invariant activation functions is straightforward. We make
the ansatz
W′I(g) =ν(w(g))WI(g), (82)
with the local variables WIandW′Ibefore and after the application of the activation function, respectively,
andwI(g) = Re(Tr/parenleftbig
WI(g)/parenrightbig
). The form of w(g)guarantees equivariance under lattice gauge transformations,
and Eq. (81) guarantees equivariance under transformations in G.
A possible choice for an activation function is a norm non-linearity (Gerken et al., 2023)
ν(w(g)) =α(∥w(g)∥), (83)
with a norm that satisfies Eq. (81), such as ∥w(g)∥=/radicalbig/summationtext
I(wI(g))2. Since the output of a norm is always
non-negative, choosing the Heaviside step function Θas the function α, which we have done in Section 2 to
mimic the well-known ReLU activation function, would not lead to a non-linearity. Therefore, we introduce
a trainable bias b≥0and set
α(∥w(g)∥) = Θ(∥w(g)∥−b). (84)
In the above example, the ReLU activation function becomes active if the norm of wexceeds the bias b.
14Published in Transactions on Machine Learning Research (05/2024)
3.8 Pooling
Pooling layers, which are often used to reduce the domain of a feature map, can also be generalized to
G-CNNs. In analogy to Cohen & Welling (2016), we split pooling layers into separate pooling and sub-
sampling steps. The pooling step performs a convolution-like operation on the feature map, but does not
change the domain. The domain reduction happens during the subsequent subsampling step with a par-
ticular stride. Typically, subsampling leads to a reduction in symmetry, depending on the stride. We first
review this procedure for scalar feature maps on the group Gbefore generalizing it to tensor fields on Gand
finally to the L-CNN.
As a motivating example, we consider sum pooling. In a traditional CNN in two dimensions, sum pooling
is performed by summing up all values of a feature map in a pooling domain D⊂ Z2, e.g. a 2×2region,
which is moved across the lattice. It differs from average pooling only by a constant factor determined by
the cardinality of D. Since sum pooling in a traditional CNN can be written as a special case of convolution
(every kernel coefficient in the pooling region is set to one), it is equivariant with respect to translations.
Analogously, sumpoolingonafeaturemap f:G→Rcanbeviewedasaspecialcaseofthefull G-convolution
in Eq. (33), when setting the kernel ψ(g)to one in the pooling domain D⊂Gand zero elsewhere:
f′(g) =/summationdisplay
h∈Gψ(g−1h)f(h) =/summationdisplay
h∈gDf(h). (85)
Here,gD={gd:d∈D}refers to the g-translated pooling region and f′:G→Ris the feature map after
the pooling step. Another well-known pooling operation is max pooling, given by
f′(g) = max
h∈gDf(h). (86)
Sum and max pooling can be generalized to other pooling operations by introducing an operator Pthat acts
on a feature map fby
f′(g) = (Pf)(g) =P(f(gd1),f(gd2),...,f (gdN)), (87)
whereP:RN→Risafunction,and Nisthecardinalityof D={d1,d2,...,dN}. Theoperator P g-translates
the setDover the feature map, so the same pooling operation is performed everywhere on G, rendering it
G-equivariant. That is, it commutes with the left translation operator Lk. This can be explicitly shown via
Lk(Pf)(g) = (Pf)(k−1g)
=P(f(k−1gd1),...,f (k−1gdN))
=P(Lkf(gd1),...,Lkf(gdN))
= (PLkf)(g).(88)
To achieve a pooling with stride sin a traditional CNN, after the pooling step, the feature map is subsampled
on the subgroup Ts⊂Tconsisting of all translations that are multiples of selementary translations. The
resulting feature map is then equivariant under Ts. Analogously, for feature maps f:G→R, subsampling
is performed on a subgroup H⊂G. This procedure is known as subgroup pooling and the resulting feature
map retains equivariance only under the subgroup H.
If the pooling region Dis itself a subgroup of G, thengDare left cosets ofD. They partition the group Ginto
disjoint, equally sized subsets. Since the left cosets are invariant under the right-action (or right translation)
ofD, i.e.gdD=gD,∀d∈D, the corresponding parts of the feature map are invariant under said action as
well, and we can pick one such part to be the resulting feature map without losing G-equivariance. This
type of pooling is called coset pooling. For example, in a network with scalar input on ZDthat is promoted
to the group G=T⋊Kby a first-layer G-convolution and further convolved with full G-convolutions, coset
pooling over Kwould yield a G-equivariant feature map on T∼ZD.
In order to generalize sum pooling to tensor fields w(g)onG, we take full tensor G-convolutions, which are
given by Eq. (48), with H=Gas a starting point and set the kernel ψIJ(g) =δIJψ(g), whereψ(g)is one
15Published in Transactions on Machine Learning Research (05/2024)
ifg∈Dand zero everywhere else. We get
w′I(g) = [ψ⋆w ]I(g) =/summationdisplay
h∈G(Lgψ)I
J(h)wJ(h)
=/summationdisplay
h∈GRI
I′ψI′
J′(g−1h)(R−1)J′
JwJ(h)
=/summationdisplay
h∈GRI
I′δI′
J′ψ(R−1)J′
JwJ(h)
=/summationdisplay
h∈GψwI(h) =/summationdisplay
h∈gDwI(h),(89)
which differs from sum pooling in the scalar case, Eq. (85), only by the tensor index I. We take this resem-
blance as motivation to define the general pooling operator Pon tensor fields of type (n,0), i.e.w:G→V
withV= (RD)n, as
w′I(g) = (Pw)I(g) =P(w(gd1),...,w (gdN))I, (90)
where the function PmapsP:VN→V. The pooling operation commutes with left translations if
P(Rw(d1),...,Rw (dN))I=RI
I′P(w(d1),...,w (dN))I′, (91)
which means that if the pooling operation commutes with the outer transformation R, then it is G-equiv-
ariant. This is the case for sum pooling defined in Eq. (89). Furthermore, if the norm ∥·∥is unaffected by
the outer transformation R, thenG-equivariance also holds for the max pooling operation
w′I(g) = max
h∈gDwI(h) =wI(g′), (92)
where
g′= argmax
h∈gD∥w(h)∥, (93)
corresponds to the element g′that maximizes the norm of win the translated pooling domain gD.1The
above definitions for tensor feature maps w:G→Vare consistent with the corresponding definitions for
scalar feature maps f:G→R, whereV=Rand the rotation matrices Rreduce to the identity operation.
To generalize sum pooling of tensor feature maps to L-CNNs, we start with the L-CNN full tensor G-con-
volution, which is given by Eq. (66), with H=G. We set the kernel ψIJ(g) =δIJψ(g), whereψ(g)is one
inside the pooling domain and zero elsewhere. Steps analogous to Eq. (89) lead to
W′I(g) =/summationdisplay
h∈gDUq(g)→q(h)WI(h)Uq(h)→q(g)=/summationdisplay
h∈gDWI
g(h), (94)
whereWandW′are the local variables before and after the pooling step, respectively, and
WI
g(h) =Uq(g)→q(h)WI(h)Uq(h)→q(g), (95)
denotes the variable W, parallel transported from htog. Note that the fact that we only consider straight
paths in the convolution restricts the shape of the pooling region D. In principle, however, it can be chosen
arbitrarily as long as the paths q(h)→q(g)of the parallel transporters Uq(h)→q(g)are chosen accordingly.
A general pooling step, represented by the aforementioned pooling operator P, on local tensor variables W
can be written as
W′I(g) = (PW)I(g) =P(Wg(gd1),...,Wg(gdN))I, (96)
whereP:VN→V. It isG-equivariant if the outer transformation commutes with the pooling operation
P(RWg(d1),...,RWg(dN))I=RI
I′P(Wg(d1),...,Wg(dN))I′. (97)
1For simplicity, we assume that the maximum of the feature map is unique. In practical applications of G-CNNs and
L-CNNs, it is highly unlikely to encounter cases where the maximum cannot be uniquely determined.
16Published in Transactions on Machine Learning Research (05/2024)
Similarly, we require the pooling layer to be equivariant under lattice gauge transformations
TΩ(PW)I(g) = Ω( x)(PW)I(g)Ω†(x)!= (PTΩW)I(g), (98)
which implies that the function Pmust also satisfy
P/parenleftbig
ΩWg(d1)Ω†,..., ΩWg(dN)Ω†/parenrightbigI= ΩP/parenleftbig
Wg(d1),...,Wg(dN)/parenrightbigIΩ†. (99)
We omitted the argument x=q(g) =q(xr)ofΩ = Ω( x)for simplicity. For example, max pooling can be
defined as
W′I(g) = max
h∈gDWI
g(h) =WI
g(g′), (100)
with
g′= argmax
h∈gD/vextenddouble/vextenddoubleRe/parenleftbig
Tr/parenleftbig
WI(h)/parenrightbig/parenrightbig/vextenddouble/vextenddouble, (101)
where the trace leads to the parallel transporters dropping out. Clearly, this form of max pooling satisfies
bothG-equivariance and equivariance under gauge transformations.
As a concrete example of how these pooling layers can be used, consider a network that uses local tensor
variablesWIonZDas input. In the first layer, the input feature maps are promoted to the group G=T⋊K
via a first-layer lattice G-convolution. After that, the feature maps are convolved by full lattice G-convo-
lutions. Coset pooling over Kcan then be applied to reduce the domain of the feature maps from Gback
toZDwhile retaining both gauge and global group symmetry. For example, if we use sum pooling, each
resulting feature map reads
W′I(g) =/summationdisplay
h∈gKWI(h), (102)
where the parallel transporters are trivial because the projections q(g) =q(h) =xcoincide at the same point
x∈ZD. The feature map is invariant in the sense of W′I(xr) =W′I(xr′)forr,r′∈K. Subsampling in K
thus leads to a G-equivariant tensor W′I(x)onZD. In this way, G-equivariant L-CNNs can be used to model
gauge and group equivariant functions which map input feature maps on the lattice ZDto new output feature
maps on ZD.
3.9 Computational requirements of G-equivariant L-CNNs
Having defined the G-equivariant generalizations of relevant L-CNN layers, we need to comment on the
computational resources required by these layers. There are two main differences to the original formulation
of L-CNNs: the first concerns the domain on which feature maps are defined, and the second concerns the
computational complexity of a G-convolution layer.
Regarding the first point, we recall that in standard L-CNNs, feature maps are functions on the lattice ZD
(with periodic boundary conditions) consisting of Nch·ND
lcomplex matrices in each layer, where Nchis the
number of channels and ND
lis the total number of lattice sites on the hypercubic lattice. Depending on the
dimensions of the lattice, our G-equivariant generalizations enlarge these domains considerably. After the
first-layerG-convolution, feature maps are promoted to functions on the global symmetry group G. Since
Gis a semi-direct product of the translation group T∼ZDand the stabilizer group K, we may write the
number of group elements as |G|=|K|ND
l. Feature maps on Gmay thus be represented by Nch·|K|·ND
l
complex matrices, which leads to an increased memory requirement by a factor of |K|compared to the
original L-CNN formulation. For example, if we consider rotoreflections in D= 2, we have|K|= 4·2 = 8
group elements since there are four possible rotations and two mirror operations about the two axes. For
D= 3andD= 4we have|K|= 48(octahedral group) and |K|= 384(hyperoctahedral group), respectively.
These factors are large, considering the fact that the models used in the original L-CNN study (Favoni
et al., 2022) easily saturated the available memory on modern GPUs. For the physically relevant case of
D= 4it may be argued that the symmetry subgroup Kmay be smaller than the full hyperoctahedral
group. In practice, one typically uses lattice sizes Nt·N3
l, whereNtis the number of cells along the time
directions with Nt̸=Nl, which reduces the symmetry. Thus, the group Kshould consist of rotations and
17Published in Transactions on Machine Learning Research (05/2024)
reflections in the spatial directions (octahedral group) and reflections along the fourth axis only, which leads
to|K|= 2·48 = 96. Nevertheless, the memory footprint of G-equivariant L-CNNs is generically much larger
than of their ZD-equivariant counterparts.
Additionally, one has to consider the computational complexity of a G-convolution layer, i.e. the number of
operations required to evaluate such convolutions. Similar to traditional CNNs, the original L-Conv layer
consists of a sum over the lattice ZD. We typically restrict the kernel of the L-Conv to the lattice axes,
such that in total D·(Nk−1) + 1terms have to be summed over to compute the result of the convolution.
G-convolutions extend this sum to run over the full symmetry group G. Thus, the number of terms to
consider is larger by a factor of |K|.
3.10 Universality of lattice gauge equivariant layers
Neural network architectures such as the multilayer perceptron (MLP) (Hornik et al., 1989), equivariant
MLPs (Ravanbakhsh, 2020), CNNs (Zhou, 2020) and G-CNNs (Sonoda et al., 2022) exhibit universality:
given enough depth (number of layers) or enough width (number of channels in a layer), these networks can
approximate any function in a suitable function space to arbitrary precision. One may ask to what extent
these universality properties also apply to lattice gauge equivariant networks.
Favoni et al. (2022) show that an L-CNN consisting of alternating convolutional and bilinear layers can be
used to construct any Wilson loop, that is, any closed path of gauge links. Combining this construction of
arbitrary loops with non-linear activation layers, a trace layer, and a CNN as a final network, the L-CNN
can represent any non-linear combination of loops. With the result of Durhuus (1980), which shows that the
linear span of products of traced Wilson loops is dense in the space of continuous gauge invariant functions,
one can conclude that the L-CNN is a universal approximator for continuous gauge invariant functions on
the lattice. Ostensibly, one can perform the same explicit construction using G-equivariant layers, which are
generalizations of the original ZD-equivariant L-CNN layers.
A related question is the universality of individual layers. For example, is the G-equivariant L-Conv layer the
most general G-equivariant, gauge equivariant linearmap with respect to the feature map W? It appears
that this is not the case due to the path dependence of parallel transporters employed in the convolution. The
non-universality of the linear layers can be demonstrated for the simpler case G=ZD. In this work, we have
always assumed that the paths along which feature maps are transported must be straight lines. Clearly,
in order to describe arbitrary linear maps, one must consider a generalization of the usual ZD-equivariant
L-Conv (see Eq. (56), which we repeat here for convenience)
[ψ∗W](x) =/summationdisplay
y∈ZDψ(y−x)Ux→yW(y)Uy→x, (103)
with the usual restriction of ψto only allow straight paths straight paths x→y. In principle, arbitrary
paths from xtoyare allowed, which leads to
[ψ∗W]′(x) =/summationdisplay
y∈ZD/summationdisplay
c∈Cy→xψcUcW(y)U−1
c, (104)
where the sum runs over the set of paths Cy→xwhich start at yand end at x. The kernel ψmust be promoted
to a mapψ:C→ R, whereCis the set of all paths on the lattice. We note that translation equivariance
requires that the kernel must itself be invariant under shifts, i.e. if two paths candc′are equivalent up to a
translation, then it holds that ψc=ψc′.
There is an obvious practical problem with Eq. (104): the sum over paths requires (countably) infinite terms
for full generality. Even if one restricts the kernel to a finite receptive field, i.e. allowing only paths that
stay close to the base point x, there are still infinitely many multi-winding loops to account for, and thus
infinitely many kernel coefficients. This is surprising since a standard ZD-convolution can be represented
by a finite number of trainable parameters when restricted to a finite receptive field. Clearly, linear gauge
equivariant layers such as the ZD-equivariant L-Conv are more complicated because they are only linear with
respect to feature maps W, but are generally non-linear functions of link variables U, which make up the
18Published in Transactions on Machine Learning Research (05/2024)
parallel transporters appearing in the convolution. For practical reasons, it is sensible to restrict the set of
paths, but this necessarily limits the expressivity of individual linear layers and renders them non-universal.
We note that this ambiguity in the choice of paths is not unique to the L-CNN. Gauge equivariant layers on
smooth manifolds (see e.g. Cohen (2021); Weiler et al. (2023)) typically use geodesics for parallel transport,
but any other choice would be compatible with gauge equivariance. Within lattice gauge theory, a practical
example along the lines of Eq. (104) can be found in Lehner & Wettig (2023a), where convolutional layers
over a restricted set of paths are applied to a regression problem.
4 L-CNNs from a bundle theoretic viewpoint
Having laid out the details of how to extend L-CNNs to larger global symmetries in previous sections, we
now focus on the mathematical foundations of gauge equivariant convolutional neural networks and how
they relate to the translationally equivariant L-CNN. There is a mathematical theory of equivariant neural
networks that uses fiber bundles to describe symmetries and to capture geometric information in data (Cohen
et al., 2019a; Aronsson, 2022). This theoretical framework models data points as fields or, more generally, as
sections of vector bundles associated to a principal bundle that specifies relevant symmetries. In Sections 4.1-
4.3 we show that the original L-CNN (Favoni et al., 2022) is a discretization of a continuous model within
this theory. Section 4.4 looks at how the original L-CNN can be directly generalized to other representations,
i.e. beyond locally transforming matrices W(x)as input. Finally, in Section 4.5, we discuss the possibility
of placing fully group-equivariant L-CNNs into this theoretical framework.
4.1 Bundle formalism
Geometric deep learning, and equivariant neural networks in particular, uses fiber bundles because they allow
nontrivial global geometries; fiber bundles generalize the (geometrically trivial) product M×Fbetween two
spacesMandF, respectively known as the base space and the characteristic fiber. We visualize this product
as attaching a fiber {x}×Fto each point x∈Mof the base space. A general fiber bundle is a collection
of fibers
E=/uniondisplay
x∈MFx, (105)
where each fiber Fx≃Fin the total space Eis equivalent to the characteristic fiber. There is also a
projectionπ:E→Mthat maps each element p∈Fx⊂Eto the point π(p) =xwhere its fiber is attached.
Although the total space Ecan be a more complicated object than a trivial bundle M×F, it is required
to look like a product D×Fon certain local regions D⊂M. A common example is the comparison
between a Möbius strip and a cylinder S1×[0,1], whereS1is the circle. Locally, both of these objects look
like segmentsD×[0,1]forD⊂S1but they have different global geometry. The Möbius strip is thus a
nontrivial fiber bundle.
Principal bundles are fiber bundles such that the characteristic fiber is a group K, often called the structure
group. Since the total space consists of fibers Kx≃K, the structure group acts as an internal degree of
freedom at each point x∈Mand principal bundles are therefore used to study local gauge symmetry.
Consider the trivial principal bundle on M=RDwith structure group K= SU(N),
π:RD×SU(N)→RD, π (x,Ω) = x, (106)
which we often refer to as P=RD×SU(N). This bundle describes an SU(N)gauge symmetry that is
incorporated into fields over RDin the following way: Let Vbe a linear space whose elements transform
according to a linear representation ρofSU(N),
v∝⇕⊣√∫⊔≀→ρ(Ω)v. (107)
Triples (x,Ω,v)are interpreted as an element v∈Vlocated at the position x∈RDand expressed in the
gauge Ω∈SU(N). The transformation in Eq. (107) means that if we let the identity matrix 1∈SU(N)
represent an initial gauge, then a gauge transformation 1∝⇕⊣√∫⊔≀→Ωcan be achieved by transforming v∝⇕⊣√∫⊔≀→ρ(Ω)v
19Published in Transactions on Machine Learning Research (05/2024)
instead, hence the triples (x,Ω,v)and(x,1,ρ(Ω)v)are gauge equivalent. We can remove the gauge degree
of freedom by defining an equivalence relation
(x,Ω,v)∼(x,1,ρ(Ω)v) (108)
and considering the set of equivalence classes Eρ=P×ρV= (P×V)/∼. The equivalence class
[x,Ω,v] = [x,1,ρ(Ω)v] (109)
is thus a gauge invariant expression for the element v∈Vat the position x∈RD.
By construction, the quotient space Eρis a so-called associated bundle
πρ:Eρ→RD, πρ([x,Ω,v]) =x, (110)
and such bundles are used extensively in the mathematical theory of equivariant neural networks. For
example, the inputs to and outputs from an equivariant neural network are called data points and are
defined as sections of associated bundles, i.e. generalized fields s:RD→Eρof the form
s(x) = [x,Ω,v(x)] = [x,1,ρ(Ω)v(x)], (111)
where Ωruns over all values of Ω∈SU(N)by definition of the equivalence class. The idea behind this
definition is that instead of using feature maps (which depend on the choice of gauge), we use gauge-
invariant data points sas input data, which map from the base space into the class of equivalent triplets.
Thus, one can consider s(x)at some point xas a gauge invariant object and use data points to describe
vector and tensor fields in a geometric (gauge and coordinate independent) way. In contrast, a feature map
of locally transforming matrices W(x)is a single representative of the gauge-invariant data point sW(x).
Similarly, one may also consider gauge-dependent tensor fields WI(x)as representatives of their respective
data points.
Each fiberExof the associated bundle Eρis a linear space with respect to linear combinations
α[x,Ω,v] +α′[x,Ω,v′] = [x,Ω,αv+α′v′], (112)
for scalarsα,α′andv,v′∈V. We can therefore take pointwise linear combinations αs(x) +α′s′(x)of data
points, making the set Γ(ρ)of all data points s:RD→Eρinto a linear space.
Layers in an equivariant neural network are maps
Φ : Γ(ρ1)→Γ(ρ2), (113)
between the spaces of data points for two possibly different representations (ρ1,V1)and (ρ2,V2)of the
structure group SU(N). Layers can be either linear or nonlinear, and we say that Φis gauge equivariant if
it commutes with gauge transformations
TΩs(x) =TΩ[x,˜Ω,v(x)]
= [x,˜ΩΩ(x),v(x)]
= [x,˜Ω,ρ(Ω†(x))v(x)].(114)
We define gauge equivariant neural networks as compositions of gauge equivariant layers such as Eq. (113).
Note that this definition makes no mention of convolutional layers or translation equivariance, allowing it
to be used even in the absence of global symmetry. Convolutional layers are one of possibly many different
types of layers. Moreover, (non-linear) activation functions, or the composition of a linear transformation
and an activation function, are also considered layers under this definition as there is no requirement of
linearity in Eq. (113). In the following, we will investigate how the original L-CNN relates to this theory.
20Published in Transactions on Machine Learning Research (05/2024)
4.2 Locally transforming variables
We claim that the locally transforming variables used in the original L-CNN are directly related to data
points of theassociated bundle EAd=P×AdCN×N. Here,V=CN×Nis the linearspace of complex N×N-
matrices and ρ= Adis the adjoint representation
Ad(Ω) :W∝⇕⊣√∫⊔≀→ΩWΩ†, W∈CN×N. (115)
In order to investigate how the data points, given by Eq. (111), are related to W(x)for this bundle, we fix
a (local) gauge
ω:D→P,D⊆RD, (116)
i.e. a local section of the principal bundle P=RD×SU(N). This principal bundle is trivial, and thus the
gauge is given by ω(x) = (x,g(x))for a unique function g:D→SU(N). Ifs:RD→EAdis a data point,
then the gauge selects a specific representative
f(x) = (x,g(x),W(x))∈P×CN×N(117)
of the equivalence class s(x)forx∈D. The triple in Eq. (117) describes a matrix W(x)∈CN×N, placed at
the position x∈RDand expressed in the gauge g(x). As our notation indicates, we argue that W(x)is the
matrix-valued, locally transforming variable used in the original L-CNN. For the purpose of verifying the
transformation behavior of W(x)under gauge transformations, we fix a second gauge ω′:D′→P, given by
ω′(x) = (x,g′(x)), and use it to select a representative
f′(x) = (x,g′(x),W′(x)) (118)
of the equivalence class s(x)∈Exforx∈D′. For each xin the intersection D∩D′, Eqs. (117) and (118)
select possibly different representatives f(x)∼f′(x)of the same equivalence class s(x)and must therefore
be related by
(x,g′(x),W′(x)) = ( x,g(x)Ω†(x),Ω(x)W(x)Ω†(x)). (119)
Here, Ω(x) =g′†(x)g(x)is the gauge transformation that transforms between ωandω′. This shows that
the matrices W(x)exhibit the correct transformation behavior
W′(x) = Ω( x)W(x)Ω(x)†. (120)
Channelsa= 1,...,mcan be introduced by taking direct sums: The multi-channel variable
W(x) =/parenleftbig
W1(x),...,Wm(x)/parenrightbig
, (121)
transforms under Ad⊕···⊕ Adand represents a data point sW(x) = [x,Ω,W(x)]of the bundle
EAd⊕···⊕ Ad≃EAd⊕···⊕EAd. (122)
Let us introduce the shorthand notations nAd =/circleplustextn
a=1AdandAdn=/circlemultiplytextn
a=1Ad.
4.3 Equivariant layers
The convolutional layer of Eq. (17) can be viewed as a discretization of a continuous convolution
[ψ⋆W]a(x) =/summationdisplay
b/integraldisplay
RDdyDψab(y−x)Ux→yWb(y)U†
x→y, (123)
with kernel components ψab:RD→Rthat are non-zero only on the coordinate axes and
Ux→y=Pexp

i1/integraldisplay
0dsdxν(s)
dsAν(x(s))

(124)
21Published in Transactions on Machine Learning Research (05/2024)
is the parallel transporter along the straight line from xtoy.
As discussed in Section 2.3, this design choice is due to parallel transport being path-dependent and the
non-uniqueness of shortest paths on the lattice. This convolution is a linear transformation by virtue of
being an integral operator, and it maps W= (W1,...,Wm)toW′= (W′1,...,W′n)in a gauge equivariant
manner:
[ψ⋆T ΩW]a(x) =TΩ[ψ⋆W]a(x). (125)
The action of Eq. (123) on data points sW(x) = [x,Ω,W(x)]is therefore well-defined and independent of
the choice of representative for the equivalence class. Thus, the continuous convolution defines a gauge
equivariant linear layer
ΦConv : Γ(mAd)→Γ(nAd), sW∝⇕⊣√∫⊔≀→sW′. (126)
Analogously, the original bilinear layer in Eq. (22) is a straightforward discretization of
W′′a(x) =m/summationdisplay
b=1m′/summationdisplay
c=1αabcWb(x)W′c(x), (127)
and can be linearized using tensor products. If sW∈Γ(mAd)andsW′∈Γ(m′Ad), then
sW⊗W′∈Γ(mAd⊗m′Ad) = Γ(mm′Ad2). (128)
That is,W⊗W′= (Wb⊗W′c). If we let nbe the number of output channels, the transformation in
Eq. (127) defines a gauge equivariant linear layer
ΦBilin: Γ(mm′Ad2)→Γ(nAd), sW⊗W′∝⇕⊣√∫⊔≀→sW′′. (129)
For each channel, trace layers W′a(x) = Tr(Wa(x,Ω))compute the trace along the N×Nmatrix struc-
ture and transform under the trivial representation ρ= Id. Gauge equivariance with respect to the trivial
representation is equivalent to gauge invariance, so trace layers are gauge equivariant linear layers
ΦTrace : Γ(mAd)→Γ(mId), sW∝⇕⊣√∫⊔≀→sW′ (130)
Finally, the non-linear activation functions W′a(x) =νa(W(x))Wa(x)transformW(x)by scaling each lo-
cally transforming variable Wa(x)using a non-linear and gauge invariant function νa. Since these activation
functions do not affect the transformation behavior of the input feature map W, they are gauge equivariant
non-linear layers
ΦAct: Γ(mAd)→Γ(mAd), sW∝⇕⊣√∫⊔≀→sW′. (131)
4.4 L-CNNs for data in other representations
The original L-CNN requires that (input) data take the form of CN×N-valued matrix variables W(x). How-
ever, the connection to the bundle theory for equivariant neural networks makes this requirement relatively
straightforward to generalize to functions f(x)taking values in a linear space V, and which transform under
gauge transformations as
f(x)∝⇕⊣√∫⊔≀→ρ(Ω(x))f(x). (132)
This more general L-CNN uses the same trivial principal bundle P=RD×SU(N)with the same gauge
links, only the data is more general. This allows us to also consider matter fields as input: for example,
fields in the fundamental representation CN(e.g. quark fields) transform according to
ρ(Ω(x))f(x) = Ω( x)f(x), (133)
where Ω(x)is aCN×Nmatrix. Similarly, for fields in the adjoint representation (in the sense of adjoint
fermions or bosons), we have
(ρ(Ω(x))f(x))a= Ω(x)abf(x)b, (134)
22Published in Transactions on Machine Learning Research (05/2024)
with color indices a,b∈{1,2,...,N2−1}and the adjoint matrix
Ω(x)ab= 2Tr/bracketleftbig
taΩ(x)†tbΩ(x)/bracketrightbig
. (135)
Inthesecases, wehaveanassociatedbundle Eρ=P×ρVconsistingofequivalenceclasses, Eq.(109), andthe
locally transforming input data f(x)are gauge-dependent representatives of data points sf(x) = [x,Ω,f(x)].
Allowing multiple channels,
F= (f1,...,fn), (136)
can be accomplished by using direct sum representations nρ=/circleplustextn
a=1ρ.
The convolution in Eq. (123) generalizes to a gauge equivariant layer Φ : Γ(mρ)→Γ(nρ)given by
[ψ⋆F]a(x,Ω) =n/summationdisplay
b=1/integraldisplay
RDdyDψab(y−x)ρ(Ux→y)fb(y,Ω), (137)
whereψab:RD→Rare kernel components.
Bilinear layers and trace layers do not generalize directly to data that, unlike W(x), are not matrix valued
and must instead be tailored to different representations. If f(x)is an SU(N) vector field, for instance, trace
layers could be defined as
f′a(x) = Tr/parenleftbig
fa(x)fa(x)†/parenrightbig
, (138)
which acts linearly on fa(x)⊗fa(x)†and therefore defines a linear layer Γ(m(ρ⊗ρ†))→Γ(mId). This
trace layer is gauge invariant and can be used to define activation functions as gauge equivariant non-linear
layers Γ(mρ)→Γ(mρ)given by
f′a(x) =νa(w(x))fa(x), (139)
for each channel a= 1,...,m. Here,wa(x) = Re/parenleftbig
Tr/parenleftbig
fa(x)fa(x)†/parenrightbig/parenrightbig
.
4.5 The difficulty in achieving full group equivariance
Now that we have connected the original L-CNN to the mathematical theory of equivariant neural networks,
it is desirable to do the same for the fully G-equivariant L-CNN discussed in Section 3. Fully adhering to
the existing theory would require us to identify a suitable principal bundle that describes both the global
symmetry under Gas well as the local gauge symmetry SU( N). The complication with this is that gauge
symmetry is described by the principal bundle P=RD×SU(N), whereas global symmetry and group
equivariance is more closely related to the principal bundle
q:G→RD, q (g) =q(xr) =x, (140)
with structure group K=G/T. It is not obvious whether a single principal bundle can correctly describe
both symmetries simultaneously. One interesting candidate is the principal bundle
q:G×SU(N)→G, q (g,Ω) =g. (141)
It is of the same general form q:G→G/Kas Eq. (140) but with the total space G=G×SU(N)and the
structure groupK=SU(N). The corresponding data points are sections of associated bundles Eρ=G×ρV
and are given by
s(g) = [g,Ω,f(g)] = [g,1,ρ(Ω)f(g)], (142)
forlinearrepresentations ρofSU(N). Gaugeequivarianceworkssimilarlyasfor P=RD×SU(N). However,
this bundle describes group equivariance with respect to G=G×SU(N), not with respect to Galone. This
means that SU(N)would have to represent a global symmetry in addition to the local gauge symmetry.
In particular, continuous convolutions would be integrals over G×SU(N). This is not compatible with
the group equivariant L-CNN, so Eq. (141) cannot be the correct principal bundle. The question, then, is
whether there is a more appropriate principal bundle or if the bundle theory for equivariant neural networks
can be appropriately broadened. We leave this question for future work.
23Published in Transactions on Machine Learning Research (05/2024)
5 Conclusions and outlook
In this work, we have reviewed the L-CNN framework (Favoni et al., 2022) from a geometrical perspective
and extended the original formulation by accounting for additional global symmetries on the lattice. The
L-CNN framework introduced a set of gauge equivariant layers which can be used to build machine learn-
ing models for performing computations on gauge link configurations {Ux,µ}. These layers, consisting of
convolutions, bilinear operations, activation functions, and trace layers, are equivariant under lattice gauge
transformations. This is achieved by accounting for parallel transport in the definition of gauge equivari-
ant convolutions. In addition, L-CNNs, which are fundamentally based on discrete convolutions on the
lattice ZD, are also equivariant under global translations of the input data.
The global symmetry group Gon a hypercubic lattice consists not only of translations, but also includes
discrete rotations and reflections. One of the drawbacks of L-CNNs is that they only respect the translational
part of the full global symmetry. To remedy this, we have revisited G-CNNs (Cohen & Welling, 2016),
which use convolutional layers compatible with general global symmetry transformations. We have first
reviewed how to use these G-convolutions on vector- and tensor-valued data and then combined the G-CNN
approachwiththeL-CNNframeworktoobtainnetworkarchitecturesthatarenotonlygaugeequivariant, but
also equivariant under the full global symmetry group on the lattice. There is a computational drawback
associated with this extension: the domain on which feature maps are defined must be enlarged to the
full symmetry group G, requiring more computational memory. Similarly, it increases the computational
complexity of convolutions, as they have to be carried out over feature maps on the group.
Finally,wehavelinkedL-CNNstothefiberbundletheoreticdescriptionofequivariantneuralnetworks(Aron-
sson, 2022) for the ZD-equivariant case. This has allowed us to determine the associated bundles used for
input data in the original L-CNN formulation and also consider more general input data for different repre-
sentations of the gauge group. More generally, we have shown how L-CNNs can be understood as a special
discretized case of gauge equivariant neural networks on fiber bundles (Gerken et al., 2023). Despite this,
a bundle description of G-equivariant L-CNNs is still lacking. Identifying the correct principal bundle to
simultaneously describe both global Gand local SU( N) symmetry would be a welcome extension of this
work.
To conclude, we list some potential applications that could benefit from full G-equivariance. Naturally, the
most interesting applications are found in four-dimensional SU( N) lattice gauge theory, which is globally
symmetric under discrete translations, rotations, and mirror transformations. Lattice gauge equivariant
networks have been used as a preconditioner for the Dirac operator in lattice QCD (Lehner & Wettig,
2023a;b; Knüttel et al., 2024). In these works, gauge equivariant convolution layers include multiple paths
and are translationally equivariant but not explicitly symmetric under rotations and reflections. Recently,
ZD-equivariant L-CNNs have been successfully used to learn a classically perfect fixed-point action (Holland
et al., 2024), which could help alleviate the problem of large autocorrelation times in Monte Carlo simula-
tions, also known as topological freezing and critical slowing down. Holland et al. (2024) demonstrate that
even well-trained ZD-equivariant L-CNNs violate rotational and mirror symmetry. Machine-learned actions
with fullG-symmetry would be more consistent with the physical theory and likely more accurate as well.
Beyond regression problems, G-equivariant L-CNNs could find applications in generative models. Equiv-
ariant normalizing (Kanwar et al., 2020; Boyda et al., 2021; Albergo et al., 2021b; Abbott et al., 2022) or
continuous (de Haan et al., 2021; Gerdes et al., 2023; Bacchio et al., 2023) flow models have been applied to
a wide range of lattice field theories in recent years. Flow models can generate statistically independent field
configurations and, ideally, exhibit vanishing autocorrelation. In most of these models, lattice symmetries
are only approximately realized – in some cases, even translational symmetry is broken down to a subgroup
(Boyda et al., 2021). The use of group and gauge equivariant layers would avoid these problems entirely, as
the relevant physical symmetries, both global and local, are manifest in the network architecture. Clearly,
more work towards an efficient implementation of G-equivariant L-CNNs is needed.
Acknowledgments
The authors thank Andreas Ipp for many helpful discussions regarding group equivariant neural networks
and comments on the manuscript. DM and DS have been supported by the Austrian Science Fund FWF
24Published in Transactions on Machine Learning Research (05/2024)
No. P32446, No. P34764 and No. P34455. DM acknowledges additional support from FWF No. P28352.
JA has been supported by the Wallenberg AI, Autonomous Systems and Software Program (WASP) funded
by the Knut and Alice Wallenberg Foundation. DM and JA thank the organizers of the Banach Center –
Oberwolfach Graduate Seminar “Mathematics of Deep Learning”, which took place in late 2019 and sparked
this collaboration.
References
Ryan Abbott et al. Gauge-equivariant flow models for sampling in lattice field theories with pseudofermions.
Phys. Rev. D , 106(7):074506, 2022. doi: 10.1103/PhysRevD.106.074506.
Michael S. Albergo, Denis Boyda, Daniel C. Hackett, Gurtej Kanwar, Kyle Cranmer, Sébastien Racanière,
Danilo Jimenez Rezende, and Phiala E. Shanahan. Introduction to Normalizing Flows for Lattice Field
Theory. art. arXiv:2101.08176, 2021a. doi: 10.48550/arXiv.2101.08176.
Michael S. Albergo, Gurtej Kanwar, Sébastien Racanière, Danilo J. Rezende, Julian M. Urban, Denis Boyda,
Kyle Cranmer, Daniel C. Hackett, and Phiala E. Shanahan. Flow-based sampling for fermionic lattice
field theories. Phys. Rev. D , 104(11):114507, 2021b. doi: 10.1103/PhysRevD.104.114507.
Jimmy Aronsson. Homogeneous vector bundles and G-equivariant convolutional neural networks. Sampling
Theory, Signal Processing, and Data Analysis , 20(2):1–35, 2022. doi: 10.1007/s43670-022-00029-3.
Simone Bacchio, Pan Kessel, Stefan Schaefer, and Lorenz Vaitl. Learning Trivializing Gradient Flows for
Lattice Gauge Theories. Phys. Rev. D , 107:L051504, Mar 2023. doi: 10.1103/PhysRevD.107.L051504.
Dimitrios Bachtis, Gert Aarts, and Biagio Lucini. Mapping distinct phase transitions to a neural network.
Phys. Rev. E , 102(5):053306, 2020. doi: 10.1103/PhysRevE.102.053306.
Dimitrios Bachtis, Gert Aarts, and Biagio Lucini. Quantum field-theoretic machine learning. Phys. Rev. D ,
103(7):074510, 2021. doi: 10.1103/PhysRevD.103.074510.
Stefan Blücher, Lukas Kades, Jan M. Pawlowski, Nils Strodthoff, and Julian M. Urban. Towards novel
insights in lattice field theory with explainable machine learning. Phys. Rev. D , 101(9):094507, 2020. doi:
10.1103/PhysRevD.101.094507.
Denis Boyda, Gurtej Kanwar, Sébastien Racanière, Danilo Jimenez Rezende, Michael S. Albergo, Kyle
Cranmer, Daniel C. Hackett, and Phiala E. Shanahan. Sampling using SU(N)gauge equivariant flows.
Phys. Rev. D , 103(7):074504, 2021. doi: 10.1103/PhysRevD.103.074504.
Denis Boyda et al. Applications of Machine Learning to Lattice Quantum Field Theory. In 2022 Snowmass
Summer Study , 2022.
Michael M. Bronstein, Joan Bruna, Taco Cohen, and Petar Veličković. Geometric Deep Learning: Grids,
Groups, Graphs, Geodesics, and Gauges. art. arXiv:2104.13478, 2021.
Srinath Bulusu, Matteo Favoni, Andreas Ipp, David I. Müller, and Daniel Schuh. Generalization capabilities
oftranslationallyequivariantneuralnetworks. Phys. Rev. D ,104(7):074504, 2021. doi: 10.1103/PhysRevD.
104.074504.
Gabriele Cesa, Leon Lang, and Maurice Weiler. A program to build E(N)-equivariant steerable CNNs. In
International Conference on Learning Representations , 2022.
Miranda CN Cheng, Vassilis Anagiannis, Maurice Weiler, Pim de Haan, Taco S Cohen, and Max Welling.
Covariance in physics and convolutional neural networks. art. arXiv:1906.02481, 2019.
Taco S. Cohen. Equivariant convolutional networks . Phd thesis, Universiteit van Amsterdam, 2021.
Taco S. Cohen and Max Welling. Group equivariant convolutional networks. In Proceedings of The 33rd
International Conference on Machine Learning , volume 48, pp. 2990–2999. JMLR, Jun 2016.
25Published in Transactions on Machine Learning Research (05/2024)
Taco S. Cohen and Max Welling. Steerable CNNs. In International Conference on Learning Representations ,
2017.
Taco S. Cohen, Mario Geiger, and Maurice Weiler. A general theory of equivariant CNNs on homogeneous
spaces. In Advances in Neural Information Processing Systems 32: Annual Conference on Neural In-
formation Processing Systems 2019, NeurIPS 2019, 8-14 December 2019, Vancouver, BC, Canada , pp.
9142–9153, 2019a.
Taco S. Cohen, Maurice Weiler, Berkay Kicanaoglu, and Max Welling. Gauge equivariant convolutional
networks and the icosahedral CNN. In Proceedings of the 36th International Conference on Machine
Learning , volume 97, pp. 1321–1330. JMLR, 2019b.
Pim de Haan, Corrado Rainone, Miranda C. N. Cheng, and Roberto Bondesan. Scaling up machine learning
for quantum field theory with equivariant continuous flows. CoRR, abs/2110.02673, 2021.
Li Deng. The MNIST Database of Handwritten Digit Images for Machine Learning Research [Best of the
Web].IEEE Signal Processing Magazine , 29(6):141–142, 2012. doi: 10.1109/MSP.2012.2211477.
B. Durhuus. On the structure of gauge invariant classical observables in lattice gauge theories. Letters in
Mathematical Physics , 4(6):515–522, Nov 1980. ISSN 1573-0530. doi: 10.1007/BF00943439.
Matteo Favoni, Andreas Ipp, David I. Müller, and Daniel Schuh. Lattice Gauge Equivariant Convolutional
Neural Networks. Phys. Rev. Lett. , 128(3):032003, 2022. doi: 10.1103/PhysRevLett.128.032003.
Christof Gattringer and Christian B. Lang. Quantum chromodynamics on the lattice , volume 788. Springer,
Berlin, 2010. ISBN 978-3-642-01849-7, 978-3-642-01850-3. doi: 10.1007/978-3-642-01850-3.
Mathis Gerdes, Pim de Haan, Corrado Rainone, Roberto Bondesan, and Miranda C. N. Cheng. Learning
lattice quantum field theories with equivariant continuous flows. SciPost Phys. , 15(6):238, 2023. doi:
10.21468/SciPostPhys.15.6.238.
Jan Gerken, Oscar Carlsson, Hampus Linander, Fredrik Ohlsson, Christoffer Petersson, and Daniel Persson.
Equivariance versus augmentation for spherical images. In International Conference on Machine Learning ,
pp. 7404–7421. PMLR, 2022.
Jan E. Gerken, Jimmy Aronsson, Oscar Carlsson, Hampus Linander, Fredrik Ohlsson, Christoffer Petersson,
and Daniel Persson. Geometric Deep Learning and Equivariant Neural Networks. Artif. Intell. Rev. , 56
(12):14605–14662, jun 2023. ISSN 0269-2821. doi: 10.1007/s10462-023-10502-7.
Simon Graham, David Epstein, and Nasir Rajpoot. Dense steerable filter CNNs for exploiting rotational
symmetry in histology images. IEEE Transactions on Medical Imaging , 39(12):4124–4136, 2020. doi:
10.1109/TMI.2020.3013246.
Kieran Holland, Andreas Ipp, David I. Müller, and Urs Wenger. Machine learning a fixed point action for
SU(3) gauge theory with a gauge equivariant convolutional neural network. 2024. doi: 10.48550/arXiv.
2401.06481.
Kurt Hornik, Maxwell Stinchcombe, and Halbert White. Multilayer feedforward networks are universal ap-
proximators. Neural Networks , 2(5):359–366, 1989. ISSN 0893-6080. doi: 10.1016/0893-6080(89)90020-8.
Gurtej Kanwar, Michael S. Albergo, Denis Boyda, Kyle Cranmer, Daniel C. Hackett, Sébastien Racanière,
Danilo Jimenez Rezende, and Phiala E. Shanahan. Equivariant flow-based sampling for lattice gauge
theory.Phys. Rev. Lett. , 125(12):121601, 2020. doi: 10.1103/PhysRevLett.125.121601.
Daniel Knüttel, Christoph Lehner, and Tilo Wettig. Gauge-equivariant multigrid neural networks. PoS,
LATTICE2023:037, 2024. doi: 10.22323/1.453.0037.
Yann LeCun, Bernhard Boser, John S Denker, Donnie Henderson, Richard E Howard, Wayne Hubbard, and
Lawrence D Jackel. Backpropagation applied to handwritten zip code recognition. Neural Computation ,
1(4):541–551, 1989.
26Published in Transactions on Machine Learning Research (05/2024)
Christoph Lehner and Tilo Wettig. Gauge-equivariant neural networks as preconditioners in lattice QCD.
Phys. Rev. D , 108:034503, 2023a. doi: 10.1103/PhysRevD.108.034503.
Christoph Lehner and Tilo Wettig. Gauge-equivariant pooling layers for preconditioners in lattice QCD.
2023b. doi: 10.48550/arXiv.2304.10438.
Kim A. Nicoli, Christopher J. Anders, Lena Funcke, Tobias Hartung, Karl Jansen, Pan Kessel, Shinichi
Nakajima, and Paolo Stornati. Estimation of Thermodynamic Observables in Lattice Field Theories with
Deep Generative Models. Phys. Rev. Lett. , 126(3):032001, 2021. doi: 10.1103/PhysRevLett.126.032001.
Omkar M Parkhi, Andrea Vedaldi, Andrew Zisserman, and C. V. Jawahar. Cats and dogs. In 2012 IEEE
Conference on Computer Vision and Pattern Recognition , pp. 3498–3505, 2012. doi: 10.1109/CVPR.2012.
6248092.
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen,
Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep
learning library. Advances in neural information processing systems , 32, 2019.
Michael E. Peskin and Daniel V. Schroeder. An Introduction to quantum field theory . Addison-Wesley,
Reading, USA, 1995. ISBN 978-0-201-50397-5.
Siamak Ravanbakhsh. Universal equivariant multilayer perceptrons. In Hal Daumé III and Aarti Singh
(eds.),Proceedings of the 37th International Conference on Machine Learning , volume 119 of Proceedings
of Machine Learning Research , pp. 7996–8006. PMLR, 13–18 Jul 2020.
Sho Sonoda, Isao Ishikawa, and Masahiro Ikeda. Universality of group convolutional neural networks based
on ridgelet analysis on groups. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho
(eds.),Advances in Neural Information Processing Systems , 2022.
Maurice Weiler, Fred A. Hamprecht, and Martin Storath. Learning steerable filters for rotation equivariant
CNNs. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pp. 849–858,
2018.
Maurice Weiler, Patrick Forré, Erik Verlinde, and Max Welling. Equivariant and Coordinate Independent
Convolutional Networks . 2023.
Kenneth G. Wilson. Confinement of quarks. Phys. Rev. D , 10:2445–2459, Oct 1974. doi: 10.1103/PhysRevD.
10.2445.
Ding-Xuan Zhou. Universality of deep convolutional neural networks. Applied and Computational Harmonic
Analysis, 48(2):787–794, 2020. ISSN 1063-5203. doi: 10.1016/j.acha.2019.06.004.
Kai Zhou, Gergely Endrődi, Long-Gang Pang, and Horst Stöcker. Regressive and generative neural networks
for scalar field theory. Phys. Rev. D , 100(1):011501, 2019. doi: 10.1103/PhysRevD.100.011501.
27