Under review as submission to TMLR
Implicit Bias and Fast Convergence Rates for Self-attention
Anonymous authors
Paper under double-blind review
Abstract
We study the fundamental optimization principles of self-attention, the defining mechanism
of transformers, by analyzing the implicit bias of gradient-based optimizers in training a
self-attention layer with a linear decoder in binary classification. Building on prior studies in
linear logistic regression, recent findings demonstrate that the key-query matrix Wtfrom
gradient-descent (GD) converges in direction towards Wmm, which maximizes the margin
between optimal and non-optimal tokens across sequences. However, this convergence is
local, dependent on initial conditions, only holds asymptotically as the number of iterations
increases, and leaves questions about the potential benefits of adaptive step-size rules
unaddressed. To bridge this gap, we first establish scenarios for which convergence is
provably global. We then analyze two adaptive step-size strategies: normalized GD and
Polyakstep-size, demonstrating finite-time convergenceratesfor WttoWmm, andquantifying
the sparsification rate of the attention map. These findings not only show that these strategies
can accelerate parameter convergence over standard GD in a non-convex setting but also
deepen the understanding of the implicit bias in self-attention, linking it more closely to the
phenomena observed in linear logistic regression despite its intricate non-convex nature.
1 Introduction
Self-attention serves as the fundamental building block of transformers, distinguishing them from traditional
neural networks (Vaswani et al., 2017) and driving their outstanding performance across various applications,
including natural language processing and generation (Devlin et al., 2019; Brown et al., 2020; Raffel et al.,
2020), as well as computer vision (Dosovitskiy et al., 2021; Radford et al., 2021; Touvron et al., 2021). With
transformers establishing themselves as the de-facto deep-learning architecture, driving advancements in
applications seamlessly integrated into society’s daily life at an unprecedented pace (OpenAI, 2022), there
has been a surge of recent interest in the mathematical study of the fundamental optimization and statistical
principles of the self-attention mechanism; see Section 6 on related work for an overview.
In pursuit of this objective, Tarzanagh et al. (2023b;a) have initiated an investigation into the implicit bias of
gradient descent (GD) in training a self-attention layer with fixed linear decoder in a binary classification
task. Concretely, the study paradigm of implicit bias seeks to characterize structural properties of the weights
learned by GD when the training objective has multiple solutions. The prototypical instance of this paradigm
is GD training of linear logistic regression on separable data: among infinitely many possible solutions to
logistic-loss minimization (each linear separator defines one such solution), GD learns weights that converge
in direction to the (unique) max-margin class separator (Soudry et al., 2018; Ji & Telgarsky, 2018). Notably,
convergence is global, holding irrespective of the initial weights’ direction, and comes with explicit rates that
characterize its speed with respect to the number of iterations. Drawing an analogy to this prototypical
instance, when training self-attention with linear decoder in a binary classification task, Tarzanagh et al.
(2023a) defines a hard-margin SVM problem (W-SVM) that separates, with maximal margin, optimalinput
tokens from non-optimal ones based on their respective softmax logits. For this, they show that the key-query
weightsWtfound by GD converge locally to the solution Wmmof(W-SVM) as the number of iterations t
grows to infinity.
Despite the intriguing analogy between the two settings, the findings of Tarzanagh et al. (2023a) are highly
non-trivial not only because the nature of the max-margin solution differs, but also because of the intricate
1Under review as submission to TMLR
Train loss Train accuracy Test loss Test accuracySGD SNGD SPS Adam
Figure 1: Comparison of train and test dynamics of various optimizers—SGD, stochastic normalized GD
(SNGD), stochastic Polyak step (SPS), and Adam—while fine-tuning a pre-trained BERT model on the MNLI
dataset; see App. C for details. SNGD and SPS, employing adaptive step-size rules, demonstrate significantly
faster training, closely resembling the performance of Adam. Motivated in part by this observation, our work
establishes fast convergence rates for NGD and PS for single-layer self-attention.
non-convex optimization landscape introduced by the presence of self-attention. The non-convexity, induced
by the softmax operator in the self-attention layer, complicates the analysis and is the reason why the
convergence result of Tarzanagh et al. (2023a) is: (i) local, holding only for an appropriate initialization
direction, and (ii) asymptotic , applicable only as iterations tapproach infinity.
Identifying these limitations, this work is motivated by the following questions:
Q1:Are there settings under which GD iterates converge globallyto the solution Wmmof(W-SVM) ?
Q2:Is it possible to obtain finite-time rates of convergence to Wmm?
Additionally, motivated by the practical benefit of using adaptive learning rates in transformer optimization
(see for example Fig. 1), we pose an additional open question:
Q3:Can using adaptive learning rates in self-attention optimization accelerate the convergence to Wmm?
Contributions. Our work addresses the above open questions, thus contributing fundamental insights on
the optimization properties of the self-attention mechanism.
Concretely, we study a single-layer self-attention model Φ(X,θ)=u⊺X⊺φ(XWx 1), where,φ(⋅)is the
softmax nonlinearity, X=[x1,⋯,xT]is the sequence of input tokens, Wis the key-query matrix, and uis
a linear decoder. Following the setup of Tarzanagh et al. (2023a), for each sequence X, we associate each
tokenxτwith a score γτ∶=yu⊺xτand let opt∈[T]be the index of the token with the largest score. Given a
training set of nsequencesXi, this defines a hard-margin SVM problem with solution Wmmthat separates
the optimal tokens with maximum margin; see Section 2 for details.
Motivated by question Q3, we study here the optimization properties of training Φ(X;θ)with exponential
loss using normalized GD , which sets the learning rate ηtat iteration tadaptively as ηt=η
∥∇θ̂L(θt)∥, for some
constantη>0. Our results also extend to Polyak step-size, another adaptive step-size rule.
Our first set of results (Section 3), for fixed decoder u(similar to Tarzanagh et al. (2023a)), answer questions
Q1-Q3 as follows. In response to Q1, we begin by identifying sufficient conditions on the input data under
which GD converges globally, in direction, to Wmm. Then, simultaneously addressing Q2 and Q3, we establish
fast finite-time rates of convergence for normalized GD by proving that the iterates Wt, at any time t, satisfy
∥Wt
∥Wt∥−Wmm
∥Wmm∥∥≤O(t−1/2). We identify two key ingredients towards establishing these results. First, we
show that the Euclidean norm ∥Wt∥of the iterates grows at a rate Θ(t). Second, and more intricate, we
demonstrate that even if the iterate at any time tviolates the constraints of (W-SVM) corresponding to any
training sample, the softmax score of the optimal token for that sample must be non-decreasing. In turn, this
establishes that the iterates, initialized in any direction, remain inside a cone around Wmm. Our convergence
results also imply an explicit rate at which softmax-attention gets sparsified as softmax scores of optimal
tokens converge to 1at an exponential rate O(exp(−ηt)).
Our second set of results (Section 4), raises the limitation of fixed linear decoder of prior work and applies to
joint training of uand of the attention weights W. In response to Q1, we construct a representative data
2Under review as submission to TMLR
model with Gaussian-distributed tokens and prove that GD converges globally, in direction, to Wmm. To
address Q2, we show that for normalized GD with an aggressive step-size, the iterates Wt, at any time t,
satisfy∥Wt
∥Wt∥−Wmm
∥Wmm∥∥≤O(1/logt).∗Further, we prove that the linear decoder uconverges, in direction, to
umm, the solution of the hard-margin SVM problem (u-SVM) that separates the examples with maximal
margin, using only the optimalinput tokens at a O(t−η)rate. Finally, to completely characterize the training
dynamics, we show fast train loss convergence at a O(exp(−t1/3))rate. We highlight three key technical
contributions towards proving these results: (i) a growing token score gap γopt,t−γτ,t>0, between optimal
optand non-optimal τ≠opttokens, for any time t>0; (ii) non-decreasing softmax scores of the optimal
tokens with time; and (iii) a constant loss ratio across all training sequences valid for any time. These
properties pave way for a PL-like inequality which is crucial to show the loss convergence rates.
Throughout, we validate our findings on both synthetic and real datasets (see Section 5). We end with
Section 7 discussing the implications of our results and the open questions they raise.
2 Preliminaries
Lowercase and uppercase bold letters represent vectors and matrices, respectively. ∥a∥and∥A∥denote
Euclidean norms. [a1,a2]={a∶a=βa1+(1−β)a2,β∈[0,1]}denotes the line segment between a1and
a2, and concat(⋅,⋅)denotes the concatenation operation. a∧bdenotes the minimum of numbers aandb.
a∨bdenotes their maximum. We use standard notation O,Ωto hide absolute constants, and ̃O,̃Ωto hide
poly-logarithmic factors. All logarithms are natural logarithms.
The output of a single-head self-attention layer, parameterized by key, query and value matrices WQ,WK∈
Rd×d1,WV∈Rd×d2, is given by φ(XWQW⊺
KX⊺)XWV,whereX∈RT×dis the input sequence, and the
softmax map φ(⋅)∶RT→RTis applied row-wise. We can compose the output of the attention layer with a
linear projection head to obtain the final prediction as
Φ(X;θ)∶=u⊺X⊺φ(XWx 1), (1)
whereθ∶=concat(u,W)denotes the set of trainable parameters. Here, similar to Tarzanagh et al. (2023a);
Deora et al. (2023); Tian et al. (2023a); Oymak et al. (2023) we reparameterize the key-query matrix as
W∶=WQW⊺
K∈Rd×d, use the first token for prediction†and subsume the value weights WVwithin the
prediction head u∈Rd. Leta(W)∶=XWx 1denote the vector of softmax logits, then φ′(a(W))∈RT×T
denotes the Jacobian at a(W).
Given training data {(Xi,yi)}n
i=1with labels yi∈{±1}andXi∶=[xi,1,xi,2,...,xi,T]⊺, and decreasing loss
ℓ∶R→R+, the empirical risk is ̂L(θ)∶=1
n∑i∈[n]ℓ(yiΦ(Xi;θ)). We focus on GD optimization with adaptive
time-dependent step-size ηt. Concretely, we study two variants: normalized gradient descent (NGD) (Hazan
et al., 2015; Nacson et al., 2019), and Polyak step-size (PS) (Loizou et al., 2021), which set
ηt∝1/∥∇θ̂L(θt)∥,andηt=(̂L(θt)−̂L∗)/(2∥∇θ̂L(θt)∥2)witĥL∗=min
θ̂L(θ),
respectively. Our results are applicable to both update rules. For specificity, we present them for NGD and
include remarks for PS.
We follow Tarzanagh et al. (2023b;a) in defining token scores as follows.
Definition 1 (Token scores and Optimality) .Given a fixed prediction head u∗∈Rd, the token score vector for
a sample (y,X)is given byγ=yXu∗. The optimal token index‡isopt=argmaxτ∈[T]γτ, whereγτ=yx⊺
τu∗
denotes the token score for the token xτ.
Similarly, optidenotes the optimal token index for a sample (yi,Xi),i∈[n]. Intuitively, these are the tokens
that minimize the training loss upon selection (Lemma 2 in Tarzanagh et al. (2023a)). Given a set of optimal
∗Note the rate here is slower compared to the fixed-decoder case. As detailed in Section 4, this is due to the additional (t+1)−1
factor in the step-size, which results in a slow down of the rate of growth of ∥Wt∥. Our proof requires this additional factor to
account for the non-smooth objective.
†This is without loss of generality as our results hold for any token τ∈[T].
‡Similar to Tarzanagh et al. (2023a), we assume unique optimal token which holds for almost all datasets.
3Under review as submission to TMLR
token indices OPT∶={opti}n
i=1, define the following hard-margin SVM problem, which separates, with maximal
margin, optimal tokens from the other tokens for every input sequence:
Wmm=argmin
W∥W∥s.t.(xi,opti−xi,τ)⊺Wxi,1≥1∀i∈[n],τ∈[T]∖{opti}. (W-SVM)
Throughout, we assume that (W-SVM) is feasible, i.e.softmax logits x⊺
i,optiWxi,1of optimal tokens can be
separated from logits x⊺
i,τWxi,1of the other tokens τ≠opti.
3 Training Dynamics of W
Here, let fixed prediction head u=u∗. This allows focusing first on the dynamics of token selection induced
by training key-query weight matrix W, which is the only trainable parameter:
Wt+1=Wt−η∇ŴL(Wt)
∥∇ŴL(Wt)∥,t>0. (2)
We start by setting up (mild) assumptions on the data and initialization, and then present our main results.
3.1 Setup
For convenience, first define Λ∶=∥Wmm∥,B∶=max
i,τ∥xi,τ∥,κ+∶=max
iexp(γi,opti−γi),κ−∶=min
iexp(γi,opti−γi),
andΥ=κ+⋅log(κ+)
log(κ−). Note 1/Λis the margin achieved by Wmmseparating optimal tokens from the rest, and
Bis a uniform bound on the tokens’ magnitudes. The parameters κ±represent the largest and smallest
degradation factors across sequences for each sequence’s individual loss term when suboptimal tokens are
selected. Respectively, Υcan be interpreted as a conditioning parameter for the problem, measuring the
variability in token gaps across different sequences.
Our first technical assumption towards ensuring global convergence requires that tokens are nearly orthogonal.§
This is often the case in high-dimensional settings; see Example 1 and the references below.
Assumption 1 (Nearly-orthogonal Tokens) .For anyi,j,k∈[n], and for any τ,τ′∈[T],
∥xi,τ∥2≥4nΥT∣⟨xi,τ,xj,τ′⟩∣, j≠i,τ′≠optj,
⟨xi,opti,xj,optj⟩≥4nΥT∣⟨xi,τ,xk,τ′⟩∣, yi=yj≠yk.
We will use Ass. 1 to prove that softmax scores of optimal tokens are lower bounded by a constant throughout
the optimization trajectory. Note that Ass. 1 itself makes nofurther guarantees on softmax scores of optimal
tokens approaching 1or even increasing during training, which is essential for global convergence and we
prove separately. Furthermore, we impose no assumptions on the direction of initialization W0. In particular,
our main results hold even when W0is aligned with “bad” stationary directions Wthat could asymptotically
saturate non-optimal tokens, i.e. ∇ŴL(αW)→0andφi,opti(αW)→0asα→∞. Thus, the only mild
requirement on the initialization W0regards its scale, rather than its direction, and is formalized in Lem. 2
in the App. See Fig. 8 in the App. for numerical validation of global convergence despite initializing in a
“bad” stationary direction. The following example illustrates the above assumption.
Example 1. Letµ±∈Rd,µ+⊥µ−,∥µ+∥=∥µ−∥=U, and data generated as follows:
y∼Unif({±1}),opt∼Unif([T]),ν∼N(0,σ2Σ),
xopt=ν+⎧⎪⎪⎨⎪⎪⎩µ+,y=1
µ−,y=−1,xτ∼N(0,ρ2Σ),∀τ∈[T]∖opt,
where Σ∶=Id−U−2µ+µ⊺
+−U−2µ−µ⊺
−. It is easy to show that when d=̃Ω(n2T2), the tokens are nearly
orthogonal with high probability. Further let high enough signal-to-noise ratio U2≥̃Ω((σ∨ρ)2nT√
d)and
§Our results also extend to cases where OPTtokens are antipodal and the remaining tokens are nearly orthogonal.
4Under review as submission to TMLR
appropriateu∗such that the token scores satisfy Υ=O(1), then, the data model satisfies Ass. 1 (see Lem. 6
in the App. for details). We remark that similar data models have been considered in prior works on the
analysis of linear overparameterized models (Muthukumar et al., 2019; Chatterji & Long, 2021; Wang &
Thrampoulidis, 2022; Cao & Gu, 2019; Wang et al., 2021), NNs with Leaky ReLU activation (Frei et al.,
2022a; 2023), CNNs (Cao et al., 2022; Kou et al., 2023a), and self-attention models (Deora et al., 2023; Li
et al., 2023a).
Our second technical assumption is similar to Tarzanagh et al. (2023a) with two key distinctions: Firstly, it
applies to self-attention rather than their simplified attention model. Secondly, and most importantly, under
this same assumption, we will prove a stronger results for global convergence and finite-time rates.
Assumption 2. For anyi∈[n], and anyτ≠opti∈[T], the token scores satisfy γi,τ=γi.
Note that in Ex. 1, Ass. 2 is satisfied by choosing an appropriate u∗(see Lem. 6 in the App. for details). It
has been recently shown that the optimization landscape of self-attention can lead GD to converge to local
directions that are different from Wmm(Tarzanagh et al., 2023a). Thus, global convergence necessitates
additional conditions. Assumptions 1 and 2, as outlined above, serve this purpose. While being only sufficient
conditions, they lead to the first-known global convergence result with finite-time guarantees, contrasting
with the local and asymptotic convergence in Tarzanagh et al. (2023a).
3.2 Main Results
We now present our main results. For ease of exposition, we use exponential loss, but our results directly
extend to continuously differentiable decreasing loss functions. First, we establish the rate of NGD’s directional
convergence to Wmm. A proof sketch is given in Sec. 3.4.
Theorem 1 (IB rate).Under small initialization scale (Lem. 2 in the App.) and Ass. 1-2, using the NGD
updates in Eq. (2)withη=̃O(B−2), it holds for any t≥t0=poly(η,B, Λ,T,nΥ),
⟨Wt,Wmm⟩≥1−C(logt)2
t,
whereC∶=C(η,B, Λ,t0)=poly(η,B, Λ,t0). In particular, assuming B=O(1),T=O(1)andΥ=O(1), we have
Λ=O(1). Thus, for any t≥t0=Ω(n), it holds ∥Wt−Wmm∥≤˜O(t−1/2).
Thm. 1 establishes that normalized iterates Wtof NGD converge globallytoWmm, starting from any
initialization direction. It also provides a lower bound on the rate of WtapproachingWmm. In Sec. 3.3, we
establish an upper bound for the rate of convergence of GD, demonstrating that NGD’s adaptive step-size
provably accelerates convergence. This is the first global convergence result and finite-time convergence rate
for self-attention using either GD or NGD. To the best of our knowledge, this is also the first demonstration of
directional convergence of NGD and its superiority over GD in a non-convex setting (Sec. 3.3 for comparisons
with existing results in convex linear settings).
We now discuss an implication of Thm. 1 concerning the evolution of the attention map during NGD training.
Specifically, we demonstrate that as training progresses, the attention map increasingly becomes sparse,
selecting optimal tokens at an exponential rate.
Lemma 1 (Softmax score rate) .Under the setting of Thm. 1, it holds for any i∈[n]and anyt≥
poly(η,B, Λ,t0)thatφt
i,opti≥(1+(T−1)e−η(8B2Λ2)−1t)−1.
To understand why the attention map becomes sparser during NGD training, recall from Thm. 1 that for
sufficiently large iterations, Wt≈∥Wt∥
∥Wmm∥Wmm, and thatWmmseparates optimal tokens from others. In
proving Thm. 1, we also show a norm-growth rate ∥Wt∥=Θ(t). Combining these yields that softmax scores
of optimal tokens φt
i,optiapproach 1, while the rest approach 0. Concretely, given Wt≈∥Wt∥
∥Wmm∥Wmm, the opt
softmax score φt
i,opti≥1
1+∑
τ≠optiexp(−∥Wt∥
∥Wmm ∥)approaches 1as∥Wt∥grows. Lem. 1 formalizes this intuition and
pairs it with an explicit sparsification rate.
5Under review as submission to TMLR
Figure 2: Training dynamics of a single-head self-attention model (Eq. (1)) when optimizing only Won
synthetic data with nearly orthogonal tokens (Example 1 with σ=0). The observed softmax score saturation,
norm growth of Wand directional alignment with Wmmclosely match with our theoretical results (Lemma 1,
Eq. (9) and Theorem 1, respectively).
Finally, we validate the predictions of the above results using synthetic data generated as per Ex. 1 with σ=0
(see App. C for details). In Fig. 2, we plot the train loss, the norm growth of Wt, the softmax score φi,opti
for the opttoken (averaged over i∈[n]), and the alignment of WtwithWmm. Note that since u∗is fixed,
the train loss does not converge to 0. Observe that the directional alignment for NGD is closely predicted by
Thm. 1. Similarly, the softmax-score saturation rate in Lem. 1 closely aligns with the empirical rate in Fig. 2.
Moreover, note that NGD is significantly faster than standard GD, which we formally prove below.
3.3 Remarks
NGD is faster than GD. We demonstrate that there exists a dataset, satisfying the assumptions of
Thm. 1, for which the correlation of vanilla GD parameters to Wmm,⟨Wt,Wmm⟩, is upper bounded by
≤1−Ω((logt)−2). In contrast, according to Thm. 1, the rate for NGD is ≥1−O((logt)2/t), thereby
highlighting a significant gap between the two.
To construct the dataset, let n=1,T=2,y=1,x1=[1,0]⊺,x2=[0,0]⊺andu∗=[1,0]⊺. Clearly, Ass. 2
holds since there is only two tokens; specifically, opt=1,γopt−γ=1. Also, Ass. 1 holds trivially since n=1.
For this dataset, the max-margin classifier can be easily expressed in closed form as Wmm=(x1−x2)x⊺
1.
Using this, we observe that ∇ŴL(Wt)=−̂L(Wt)φt
opt(1−φt
opt)Wmm, which implies
Wt+1=W0+ηGtWmm,where we set Gt∶=t
∑
t′=0∥∇ŴL(Wt′)∥.
Leveraging this, we now show that ∥Wt∥≤O(logt): Note that ∥Wt∥→∞ast→∞which leads to softmax
saturation φt
opt→1and hence, loss minimization. This implies that Gtgrows with t, and there exists
somet0>0, such that for any t≥t0, it holds that ∥Wt+1∥≈ηGt+W11
0, whereW11
0is the (1,1) entry
of the initialization matrix. Using this, we can show for t≥t0that 1−φt+1
opt=1
1+exp((x1−x2)⊺Wt+1x1)≤
exp(−(x1−x2)⊺Wt+1x1)=exp(−ηGt−W11
0)≈exp(−∥Wt+1∥). Thus,
∥Wt+1∥≤∥W0∥+ηt
∑
t′=0̂L(Wt′)exp(−∥Wt′∥)≤O(t
∑
t′=0exp(−∥Wt′∥)),
where we use ̂L(Wt)≤O(1), since the decoder is fixed. This proves that ∥Wt∥≤O(logt), which we use to
upper bound the correlation as follows:
⟨Wt,Wmm⟩=ηGt+W11
0√
(ηGt+W11
0)2+∥W0∥2−(W11
0)2≈1−∥W0∥2−(W11
0)2
2(ηGt+W11
0)2≈1−∥W0∥2−(W11
0)2
2∥Wt∥2.(3)
Thus,⟨Wt,Wmm⟩≤1−Ω((logt)−2), or equivalently ∥Wt−Wmm∥≥Ω((logt)−1). This shows that using an
adaptive step-size accelerates convergence to Wmm, compared to standard GD.
Extension to Polyak step-size. In Thm. 7 in the App., we establish an analogue of Thm. 1 specifically for
the Polyak step-size rule. This extension required us to further establish a lower bound on the gradient norm.
This is because PS has an additional ∥∇θ̂L(θt)∥−1factor compared to NGD. To accomplish this, we used the
variational form of the Euclidean norm for a “good” direction Wmm, yielding ∥∇θ̂L(θt)∥≥poly(Λ−1)̂L(θt).
6Under review as submission to TMLR
As seen in Fig. 2, PS leads to even faster convergence than NGD across all metrics, including the train loss.
In our implementation, we use a hyperparameter ηmaxas a threshold on the step-size value, which has been
shown to stabilize training (Loizou et al., 2021). Further, since the selection of opttokens minimizes the loss,
̂L∗is calculated by setting φi,opti=1for alli∈[n]. Note that after some time, when ηt≥ηmax, the PS updates
used in practice are identical to standard GD with step-size ηmax. Hence, the rates are much faster in the
beginning, but slow down as training progresses.
3.4 Proof Sketch of Theorem 1
The first key intermediate result we need to show is that after sufficient iterations, the negative gradient of
the loss atWtis more correlated with Wmmcompared to Wtitself. Formally, we show that for all ϵ>0,
there exists Rϵ∶=2Λϵ−1log(Cϵ−1)withC=poly(T,B2Λ,nΥ), such that for every tfor which ∥Wt∥≥Rϵ, it
holds that
⟨−∇ŴL(Wt),Wmm⟩≥(1−ϵ)⟨−∇ŴL(Wt),Wt⟩. (4)
A proof sketch of this follows; see Lem. 7 in the App. for details. First, under Ass. 2, for any W:
⟨−∇ŴL(Wt),W⟩=1
nn
∑
i=1ℓt,i(γi,opti−γi)φt
i,opti(1−φt
i,opti)ht,i, (5)
whereht,i=ai,opti−∑τ≠optiφt
i,τai,τ
∑τ≠optiφt
i,τ,ai=XiWxi,1. Analogously, define ˜h,h∗, using ˜a=ΛXWtx1,a∗=XW mmx1,
respectively. Then, to prove Eq. (4), it suffices that
˜ht,i≤(1+ν)h∗
t,i,whereν=ϵ(1−ϵ)−1. (6)
Suppose that ∥Wt−Wmm∥>ν
2B2Λ, since otherwise the claim follows easily. We categorize the training
samples into three subsets based on the satisfaction of the constraints in (W-SVM) . Concretely, define the
“minimum SVM-gap per sample” i∈[n]asδmin
i∶=minτ∉{opti}˜ai,opti−˜ai,τ−1and based on this consider three
sets of samples as follows:
•I1∶={i∈[n] ∣δmin
i≤0.3ν},for samples where the constraints are violated or weakly satisfied,
•I2∶={i∈[n] ∣0.3ν<δmin
i≤0.8ν},for samples where constraints are satisfied with small gap,
•I3∶={i∈[n] ∣δmin
i≥0.8ν}, for samples where the constraints are well-satisfied.
Since∥Wt−Wmm∥is large, there is at least one point i∈[n]that violates the constraints, i.e. I1is non-empty.
We first show the following for examples in I1andI2
˜ht,i≤(1+0.5ν)h∗
t,i,i∈I1,and ˜ht,i≤(1+ν)h∗
t,i,i∈I2. (7)
Note that we prove a tighter inequality for I1than required by (6)in order to use this residual to get the
desired inequality for examples in I3. Specifically, when Rϵis large enough, we wish to show (following
Eq. (5)) that
∑
i∈I3ℓt,i(γi,opti−γi)φt
i,opti(1−φt
i,opti)≤0.5ν
2B2Λ∑
i∈I1ℓt,i(γi,opti−γi)φt
i,opti(1−φt
i,opti). (8)
Intuitively, for the LHS, any sample i∈I3satisfies(W-SVM) constraints with large gap giving φi,opticlose to
1 and growing with ∥Wt∥. Similarly, for the RHS, we expect φi,optito be smaller. Thus, for a sufficiently
large∥Wt∥and controlling φi,optifor anyi∈I1andt>0would allow us to show the above inequality. This is
achieved by establishing a non-trivial lower bound on the softmax scores for i∈I1that holds throughout
training (Lem. 3 in the App.). Finally, since h∗≥1and˜h≤2B2Λ, we combine Eqs. (7)and(8)using the
formulation in (5) and complete the proof of Eq. (4).
7Under review as submission to TMLR
The next key ingredient in the proof of Thm. 1 is showing that the iterate norm grows at rate Θ(t).
Specifically, we prove in Lem. 5 in the App. that
2ηt≥∥Wt∥≥η(4B2Λ)−1t. (9)
The upper bound follows by applying Cauchy-Schwarz. For the lower bound, we write the iterate Wtusing
its gradient updates, use the dual norm characterization of the ℓ2-norm, and select the Wmmdirection to get
∥Wt∥≥⟨W0,Wmm⟩+t−1
∑
t′=0η⟨−∇ŴL(Wt)
∥∇ŴL(Wt)∥,Wmm
∥Wmm∥⟩,
where we use the key property that for any Wthe gradient correlates positively with Wmm(formalized in
Lem. 4 in the App.), i.e. ⟨−∇ŴL(W)
∥∇ŴL(W)∥,Wmm
∥Wmm∥⟩≥(2B2Λ)−1>0.
Next, using Eq. (2) to substitute the gradient update in Eq. (4), we can show that
⟨Wt+1−Wt,Wmm
∥Wmm∥⟩≥(1−ϵt)⟨Wt+1−Wt,Wt
∥Wt∥⟩=∥Wt+1∥2−∥Wt∥2−∥Wt+1−Wt∥2
2∥Wt∥−ϵt⟨Wt+1−Wt,Wt
∥Wt∥⟩
≥∥Wt+1∥−∥Wt∥−η2
2∥Wt∥−ϵtη.
We then select ϵt=(80B2Λ2logt)(ηt)−1, and use Eq. (9)and Lem. 7 to obtain a O((logt)2∥Wt∥−1)rate.
Using Eq. (9) then finishes the proof.
4 Training Dynamics for Joint Optimization
We now study training dynamics when jointly optimizing prediction head uand attention weights W.
Compared to Sec. 3, the key challenge is that the token scores γtevolve with time, driven by the changing
nature ofut. Additionally, the objective function becomes non-smooth in this context, given the dynamic
changes inut. Addressing these challenges necessitates additional technical considerations compared to Sec. 3,
which we also discuss in this section.
4.1 Setup
We consider the following updates for utandWt.
ut+1=ut−ηu
t∇ûL(θt), ηu
t=η(t+1)−2/3
∥∇ûL(θt)∥,Wt+1=Wt−ηW
t∇ŴL(θt), ηW
t=η(t+1)−1
∥∇ŴL(θt)∥.(10)
Here, we focus on exponential loss. Note that in ηu
t, the factor (t+1)−2/3can be replaced with (t+1)−p,
wherep∈[2/3,1). In our results, we present findings using p=2/3, as it yields the fastest rate. We also
remark that common adaptive learning rates like Adam (Kingma & Ba, 2014), AdaGrad (Duchi et al., 2011),
etc. similarly set per-parameter learning rates that vary dynamically.
Similar to Def. 1, given a trainable prediction head utat any iteration t>0, the token score vector for a
sample (y,X)is given byγt=yXut.
Data Model. We study the following data model. Within each example X, a single opt∼Unif([T])token
and an additional T−1tokens are sampled i.i.d. Gaussian as follows:
xopt∼N(0,α2ρ2Id),xτ∼N(0,ρ2Id),∀τ∈S, y=sign(u⊺
∗xopt), (DM)
where S∶=[T]∖{opt},u∗∈Rdis a fixed vector that generates the label y∈{±1}. Here,ρcontrols the token
norms, and α>1separates the opttoken from the other tokens. We consider the following mild conditions on
the initialization, token norms, overparameterization and step-size.
Condition 1. LetB=αρ√
1.5d≥1, andC0>1,C1,C2>0be some absolute constants. We consider
the following conditions for any δ∈(0,1): i) Zero initialization: ∥u0∥=0,∥W0∥=0; ii) opttoken has
larger norm: α≥C0; iii) Sufficient overparameterization: d≥C1α4n4log(10n2
δ); iv) Small step-size: η≤
(C2√nB4(B∨d))−1.
8Under review as submission to TMLR
The condition on the initialization is for ease of exposition; our analysis and results extend to random
initialization with small scale (similar to the conditions of Lem. 2 in Sec. 3). We also remark that under
Cond. 1 and data model DM, the tokens are nearly orthogonal, similar to Ass. 1 in Sec. 3. We formalize this
in Lem. 9 in the Appendix.
4.2 Main Results
We now present our main results for joint training. For ease of exposition, we consider T=2, but our results
directly extend to T>2assuming token scores γtsatisfy Ass. 2 at every iteration t. It is also possible to
extend our results to the case where Ass. 2 is not exactly satisfied, but the non- opttoken scores show small
deviation.
Our first result shows that train loss goes to 0atO(exp(−t1/3))rate; see Sec. 4.3 for a proof sketch.
Theorem 2 (Train loss convergence) .Under Cond. 1, data model DM, using the updates in Eq. (10), it
holds for any t>0that̂L(θt+1)≤O(exp(−ηBC 0√n(t+1)1/3)).
The next theorem illustrates that Wtconverges to Wmmin direction at an O(1/logt)rate.
Theorem 3 (IB rate ofW).Under Cond. 1 and the data model DM, using the updates in Eq. (10), for any
t≥tϵ=exp(poly(η,B, Λ,n,d, logδ−1,ϵ−1)),
⟨Wt,Wmm⟩≥1−ϵ−poly(η,B, Λ,ϵ)1
logt.
In particular, assuming ρ=O(η1/2d−3/4(nΛ)−1), for anyt≥tϵ=Ω(exp(ϵ−4/3)),∥Wt−Wmm∥≤O(ϵ∨(logt)−1).
Intuitively, since the updates for Wthave an additional (t+1)−1factor due to the smaller step-size ηt, the
iterate norm ∥Wt∥grows as Θ(logt)in contrast to the Θ(t)rate in Sec. 3, where we were only optimizing
Wt. Consequently, the convergence to implicit bias is slower — O(1/logt)instead of ˜O(t−1). The additional
(t+1)−1factor in the updates for Wthelps to account for the non-smoothness of the objective, which will
become apparent in Sec. 4.3.
We now discuss the implicit bias and convergence of ut. From prior work (Soudry et al., 2018; Ji &
Telgarsky, 2019), one could guess that utconverges to the max-margin predictor separating the set of samples
{(φt
i,opttxi,opti+(1−φt
i,opti)xi,τ,yi)n
i=1}. But as shown above, when t→∞,φopt→1. This motivates the
following hard-margin SVM problem,
umm=argmin
u∥u∥s.t.yiu⊺xi,opti≥1∀i∈[n]. (u-SVM)
The following result confirms that the learned model attains margin that is asymptotically a constant factor
(1/4) of the maximum margin γ∶=maxu∶∥u∥≤1miniyiu⊺xi,opti.
Theorem 4 (IB rate ofu).Under Cond. 1 and the data model DM, using the updates in Eq. (10), for any
t≥tϵ∨exp(C(η,B, Λ,ϵ)(ϵ−1∨(8B2Λ)4)), it holds that
min
iyiut⊺xi,opti≥γ
4−1
1+exp(η(8B2Λ2)−1logt).
4.3 Proof Sketch of Theorem 2
The main challenge in establishing the loss convergence rate in Thm. 2 is the non-smoothness of the objective
function. To overcome this, we show three key properties that hold throughout training: i) constant loss ratio
for any two samples, ii) growing token score gap between optand non- opttokens for every sample, and iii) a
non-decreasing opt-token softmax score. We formalize these in Lem. 11 in the App. Property (ii) is crucial
in proving the implicit bias rate in Thm. 3. Property (iii) is crucial in obtaining a PL-like inequality which is
the key challenging step in proving Thm. 2. Specifically, using properties (i) and (iii) we demonstrate that
the training loss satisfies the following (see Lem. 13 and Rem. 3 in the App. for details):
1. PL-inequality-like result: ∥∇ûL(θt)∥≥Ω(Bn−1/2)̂L(θt),
9Under review as submission to TMLR
2. controlled loss between current and next iterate: maxθ′∈[θt,θt+1]̂L(θ′)≤8̂L(θt),
3. second-order self-boundedness: max
θ′∈[θt,θt+1]∥∇2
θ̂L(θ′)∥≤8(ω(θt)∨ω(θt+1))̂L(θt),
whereω(θt)∶=13B5(B∨d)(∥ut∥∨1)2.
To prove the first point, we use the dual norm characterization of the ℓ2-norm,
∥∇ûL(θt)∥=sup
v∶∥v∥=1⟨1
nn
∑
i=1∣ℓ′
i,t∣yi∇uΦ(ut,Xi),v⟩≥̂L(θt)sup
v∶∥v∥=1min
iyi(φt
i,optixi,opti+(1−φt
i,opti)xi,τ)⊺v.
To proceed, we lower bound the softmax scores for the opttokens for every i∈[n]andt>0as
φt
i,opti≥1
2. (11)
Using this, we show that the set of samples {(0.5(xi,opti+xi,τ),yi)}n
i=1, whereτ≠opti, are separable using
˜u=∑i∈[n]yixi,opti(formalized in Lem. 12 in the App.). Then, selecting v=˜uproves the first point.
We now briefly discuss the process to prove Eq. (11). We can show this by induction. Since W0=0, then for
anyi∈[n],φ0
i,opti=1/2at initialization. To show the inductive step, note that for any i∈[n], if
(xi,opti−xi,τ)⊺(−∇ŴL(θt))xi,1>0, (12)
/Leftr⫯g⊸tl⫯ne⇒φt+1
j,optj/φt+1
j,τ
φt
j,optj/φt
j,τ=exp((xj,optj−xj,τ)⊺(Wt+1−Wt)xj,1)≥1.
We consider two cases. If φt
i,opti≥3/4, Eq. (12) follows as ηis small. Specifically, for any j∈[n],τ≠optj,
φt
j,optj=1
1+exp((xi,τ−xj,optj)⊺Wtxj,1)>3/4/Leftr⫯g⊸tl⫯ne⇒ (xj,τ−xj,optj)⊺Wt−1xj,1≤−log(3).
After the gradient step, since η≤(2B2)−1log(3), we have
(xj,τ−xj,optj)⊺Wt+1xj,1=(xj,τ−xj,optj)⊺Wtxj,1−ηt−1(xj,optj−xj,τ)⊺(−∇ŴL(Wt))xj,1≤2ηB2−log(3)≤0.
Ifφt
i,opti≤3/4, we proceed as follows. Consider the expansion of the LHS of Eq. (12),
(xi,opti−xi,τ)⊺(−∇ŴL(θt))xi,1=ℓt,i
n(γt
i,opti−γt
i)φt
i,opti(1−φt
i,opti)∥xi,1∥2∥xi,opti−xi,τ∥2
−1
n∑
j≠iℓt,j(γt
j,optj−γt
j)φt
j,optj(1−φt
j,optj)x⊺
i,1xj,1(xj,optj−xj,τ)⊺(xi,opti−xi,τ′).
To use this to prove Eq. (12), we first show that the token scores γtsatisfy Defn. 1 ( γt
opti>γt
ifor anyi∈[n]),
and that the loss ratio maxi,j∈[n]ℓt,i
ℓt,jis bounded by a constant throughout training, i.e., for anyi∈[n],
γt
i,opti−γt
i≥Ω(t1/3)and max
i,j∈[n]ℓt,i
ℓt,j≤C, (13)
whereC>0is a universal constant. This is formalized in Lem. 11 in the App. We then prove Eqs. (11)and
(13) jointly by induction.
For the second point, we first show that for any θ,θ′,
∣yΦ(θ,X)−yΦ(θ′,X)∣=∣u⊺X⊺φ(XW⊺x1)−u′⊺X⊺φ(XW′⊺x1)∣
≤∥X∥2,∞∥u−u′∥+2∥X∥3
2,∞∥u′∥∥W−W′∥.
10Under review as submission to TMLR
Since we are using exponential loss, we use this to get,
max
λ∈[0,1]ℓ(yΦ(θt+λ(θt+1−θt),X))
ℓ(yΦ(θt,X))≤max
λ∈[0,1]exp(2λ∥X∥3
2,∞∥ut∥∥Wt+1−Wt∥+λ∥X∥2,∞∥ut+1−ut∥).
Further, we show that the iterate norm ∥ut∥grows as O(t1/3)(formalized in Lem. 10 in the App.). Then,
using the updates in Eq. (10), we get
max
i∈[n]max
λ∈[0,1]ℓ(yiΦ(θt+λ(θt+1−θt),Xi))
ℓ(yiΦ(θt,Xi))≤exp(6η2B3+ηB).
Then, using Cond. 1 for ηfinishes the proof for the second point. For the third point, we first upper bound
∥∇2
θΦ∥as∥∇2
θΦ(θ,X)∥≤6dB5∥u∥+2√
dB3. Using this and the upper bound on ∥∇θΦ∥, we show that for
anyθ′∈[θt,θt+1],
∥∇2
θ̂L(θ′)∥≤max
i∈[n](∥∇θΦ(θt,Xi)∥2+∥∇2
θΦ(θt,Xi)∥)̂L(θ′)≤ω(θ′)̂L(θ′).
Then, we leverage the second point to finish proving the third point.
With these ingredients, we prove Thm. 2 by working with the second-order Taylor expansion of ̂L(θt+1).
Specifically, using the updates in Eq. (10)and the three key properties of the training loss established above,
we show that
̂L(θt+1)≤̂L(θt)−ηB
10√n(t+1)2/3̂L(θt)+8η2
(t+1)4/3(ω(θt)∨ω(θt+1))̂L(θt).
Further, since ∥ut∥≤O(t1/3), we show that ω(θt)≤O(t2/3). Using a small enough η, and telescoping, gives
the advertised result.
5 Experimental Results
To complement our theory, we present experiments on synthetic/ real-world data demonstrating that (S)NGD-
based training leads to faster convergence for various metrics compared to vanilla (S)GD.
Synthetic Data. We first consider the self-attention model in Eq. (1)and data generated as per data
model DM. Fig. 3 shows the train loss, iterate norms ∥ut∥and∥Wt∥, average softmax score for the opt
tokens, and alignment of iterates utandWtto the respective max-margin solutions Wmmandumm. We
compare standard GD, NGD updates in Eq. (10)(without the additional t−2/3,t−1factors in the step-size),
PS updates, and joint NGD updates. We observe that all other algorithms are faster than GD, with PS being
the fastest. We also see that the train loss converges at a similar rate for NGD and joint-NGD, while for the
other metrics, NGD is slightly faster than joint-NGD.
Language and Vision Datasets. Fig. 1 compares the train and test dynamics of various optimizers
while fine-tuning a pre-trained BERT model on the MNLI (Williams et al., 2018) dataset. As evident, both
gradient normalized step-sizes SNGD and SPS train significantly faster than SGD. Fig. 4 in the App. shows
similar results for the CivilComments dataset (Borkan et al., 2019). In App. C we present additional results
on vision datasets. Interestingly, in this setting, while SPS still outperforms SGD, SNGD trains slower, which
Figure 3: Training dynamics of a self-attention model (Eq. (1)) with data generated using model DM.
11Under review as submission to TMLR
could be explored further in future work. It is also worth noting that in this setting we see no significant gap
between Adam and SGD; this is consistent with the observations reported in Xie et al. (2023); Kunstner et al.
(2023).
6 Related Work
Implicit bias of NNs. There has been extensive work on the implicit bias of GD for both linear predictors
(see Sec.1) and NNs. Despite non-convexity, our results are more closely related to those for linear predictors
since we give an explicit formulation of the solution in the convergence limit (compare to implicit formulations
in terms of KKT points for nonlinear NNs; see App.D for a review).
Thm. 1 provides a non-trivial extension to self-attention of corresponding results for linear logistic regression.
Specifically, Ji & Telgarsky (2021) demonstrate that NGD on logistic loss under separable data converges
globally, in direction, at rate ̃O(1/t)to the max-margin separator of data points belonging to distinct classes.
While the convergence rate of Thm. 1 for self-attention is slower, it holds under a more intricate non-convex
landscape induced by the softmax nonlinearity; see App. A.4 for a discussion on the tightness of our results.
As we have seen, in addition to NGD, our convergence results also apply to Polyak step-size, which empirically
outperforms NGD (see Fig. 2). We also present results for NGD with momentum in Fig. 2, which results
in a performance boost. In the simpler setting of linear logistic regression, this method is proven to have
a faster implicit bias rate of ̃O(1/t2)(Ji et al., 2021). Extending our theory to incorporate momentum in
the self-attention setting is an interesting direction for future work. The loss convergence rate in Thm. 2
is analogous to the O(exp(−t))loss rate for linear predictors on separable data with ηt=η/∥∇̂L(θt)∥(Ji &
Telgarsky, 2019). The difference O(exp(−t1/3))vs.O(exp(−t))shows up as we use smaller step-size. Nacson
et al. (2019) similarly show a slower rate of O(exp(−t1/2))for linear predictors with ηt=η(t+1)−1/2/∥∇̂L(θt)∥.
Contrary to all these works, to the best of our knowledge, we are the first to prove parameter convergence of
NGD and PS in non-convex settings.
Transformers theory. To understand the optimization and generalization dynamics, (Jelassi et al., 2022)
shows that Vision Transformers (ViTs) learn spatially localized patterns in a binary classification task using
gradient-based methods, while Li et al. (2023b) shows that attention maps sparsify as SGD training progresses.
Oymak et al. (2023) studies the initial trajectory of GD for the closely related prompt-attention. Additionally,
Tian et al. (2023a) studies SGD-dynamics for the next-token prediction task in a one-layer transformer
with a linear decoder. More recently, Tian et al. (2023b) extends this analysis to the joint training of
multi-layer transformer with an MLP. As detailed in Sec. 1, our work is most closely related and improves
upon Tarzanagh et al. (2023b;a), which adopt an implicit-bias view to attention training. After completion of
this work, we became aware of contemporaneous work (Sheen et al., 2024) that also studies implicit bias in
the same problem setting (single-layer self-attention, fixed decoder, Ass. 2). Unlike ours, their results hold
only for gradient flow (i.e. GD with infinitesimal step-size), and are asymptotic. On the other hand, they
study separate optimization of key-query matrices, albeit with appropriate initialization assumptions. See
App.D for a full review.
7 Conclusion and Future Work
We studied implicit optimization bias of GD for self-attention in binary classification. For both fixed
and trainable decoder settings, we identified data conditions under which global convergence holds and
characterized, for the first time, the convergence rates. Our convergence rates hold for NGD and PS,
which are shown both theoretically and empirically to outperform vanilla GD. In future work, we aim to
further relax Ass. 2 and identify and investigate other sufficient conditions for global convergence, such
as overparameterization. Additionally, extending our theory to incorporate momentum and explain its
fast convergence is also an interesting future direction. Finally, motivated by our experiments, we want
to further investigate why NGD trains faster on language datasets, yet appears to be slower on vision
datasets. This may involve extending our findings to the next-token prediction setting, which is also of interest.
12Under review as submission to TMLR
References
Ekin Akyürek, Dale Schuurmans, Jacob Andreas, Tengyu Ma, and Denny Zhou. What learning algorithm is
in-context learning? Investigations with linear models. In Int. Conference on Learning Representations ,
2023. URL https://openreview.net/forum?id=0g0X4H8yN4I . 45
Sanjeev Arora, Nadav Cohen, Wei Hu, and Yuping Luo. Implicit regularization in deep matrix factorization.
InAdvances in Neural Information Processing Systems , volume 32, 2019. 45
Pierre Baldi and Roman Vershynin. The quarks of attention. arXiv preprint arXiv:2202.08371 , 2022. 45
Alberto Bietti, Vivien Cabannes, Diane Bouchacourt, Herve Jegou, and Leon Bottou. Birth of a transformer:
A memory viewpoint. arXiv preprint arXiv:2306.00802 , 2023. 45
Daniel Borkan, Lucas Dixon, Jeffrey Sorensen, Nithum Thain, and Lucy Vasserman. Nuanced metrics for
measuring unintended bias with real data for text classification. In Companion Proceedings of The 2019
World Wide Web Conference , 2019. 11, 42
Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, PranavShyam, GirishSastry, AmandaAskell, SandhiniAgarwal, ArielHerbert-Voss, Gretchen
Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter,
Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark,
Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models
are few-shot learners, 2020. 1
Yuan Cao and Quanquan Gu. Generalization bounds of stochastic gradient descent for wide and deep neural
networks. Advances in neural information processing systems , 32, 2019. 5
Yuan Cao, Zixiang Chen, Mikhail Belkin, and Quanquan Gu. Benign overfitting in two-layer convolutional
neural networks. arXiv preprint arXiv:2202.06526 , 2022. 5
Niladri S. Chatterji and Philip M. Long. Finite-sample analysis of interpolating linear classifiers in the
overparameterized regime. Journal of Machine Learning Research , 22(129):1–30, 2021. URL http:
//jmlr.org/papers/v22/20-974.html . 5
Lenaic Chizat and Francis Bach. Implicit bias of gradient descent for wide two-layer neural networks trained
with the logistic loss. In Conference on Learning Theory , pp. 1305–1338. PMLR, 2020. 45
Puneesh Deora, Rouzbeh Ghaderi, Hossein Taheri, and Christos Thrampoulidis. On the optimization and
generalization of multi-head attention, 2023. 3, 5, 37, 39, 45
JacobDevlin, Ming-WeiChang, KentonLee, andKristinaToutanova. BERT:Pre-trainingofdeepbidirectional
transformers for language understanding. In Proceedings of the 2019 Conference of the North American
Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long
and Short Papers) , pp. 4171–4186, Minneapolis, Minnesota, June 2019. Association for Computational
Linguistics. doi: 10.18653/v1/N19-1423. URL https://aclanthology.org/N19-1423 . 1, 42
Yihe Dong, Jean-Baptiste Cordonnier, and Andreas Loukas. Attention is not all you need: Pure attention loses
rank doubly exponentially with depth. In International Conference on Machine Learning , pp. 2793–2803.
PMLR, 2021. 45
AlexeyDosovitskiy, LucasBeyer, AlexanderKolesnikov, DirkWeissenborn, XiaohuaZhai, ThomasUnterthiner,
Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby.
An image is worth 16x16 words: Transformers for image recognition at scale, 2021. 1
John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and
stochastic optimization. Journal of Machine Learning Research , 12(61):2121–2159, 2011. URL http:
//jmlr.org/papers/v12/duchi11a.html . 8
13Under review as submission to TMLR
Tolga Ergen, Behnam Neyshabur, and Harsh Mehta. Convexifying transformers: Improving optimization and
understanding of transformer networks. arXiv preprint arXiv:2211.11052 , 2022. 45
Spencer Frei and Quanquan Gu. Proxy convexity: A unified framework for the analysis of neural networks
trained by gradient descent. In A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan (eds.),
Advances in Neural Information Processing Systems , 2021. URL https://openreview.net/forum?id=
NVpGLJUuPx5 . 37
Spencer Frei, Niladri S Chatterji, and Peter Bartlett. Benign overfitting without linearity: Neural network
classifiers trained by gradient descent for noisy linear data. In Conference on Learning Theory , pp.
2668–2703. PMLR, 2022a. 5
Spencer Frei, Gal Vardi, Peter L Bartlett, Nathan Srebro, and Wei Hu. Implicit bias in leaky relu networks
trained on high-dimensional data. arXiv preprint arXiv:2210.07082 , 2022b. 45
Spencer Frei, Gal Vardi, Peter Bartlett, and Nathan Srebro. Benign overfitting in linear classifiers and leaky
relu networks from kkt conditions for margin maximization. In Gergely Neu and Lorenzo Rosasco (eds.),
Proceedings of Thirty Sixth Conference on Learning Theory , volume 195 of Proceedings of Machine Learning
Research , pp. 3173–3228. PMLR, 12–15 Jul 2023. URL https://proceedings.mlr.press/v195/frei23a.
html. 5
Elad Hazan, Kfir Y. Levy, and Shai Shalev-Shwartz. Beyond convexity: Stochastic quasi-convex optimization.
InProceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1 ,
NIPS’15, pp. 1594–1602, Cambridge, MA, USA, 2015. MIT Press. 3
M. Emrullah Ildiz, Yixiao Huang, Yingcong Li, Ankit Singh Rawat, and Samet Oymak. From self-attention
to markov models: Unveiling the dynamics of generative transformers, 2024. 45
Samy Jelassi, Michael Eli Sander, and Yuanzhi Li. Vision transformers provably learn spatial structure. In
Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (eds.), Advances in Neural Information
Processing Systems , 2022. URL https://openreview.net/forum?id=eMW9AkXaREI . 12, 45
Ziwei Ji and Matus Telgarsky. Risk and parameter convergence of logistic regression. arXiv preprint
arXiv:1803.07300 , 2018. 1
Ziwei Ji and Matus Telgarsky. Characterizing the implicit bias via a primal-dual analysis. In International
Conference on Algorithmic Learning Theory , 2019. URL https://api.semanticscholar.org/CorpusID:
226245936 . 9, 12
Ziwei Ji and Matus Telgarsky. Directional convergence and alignment in deep learning. Advances in Neural
Information Processing Systems , 33:17176–17186, 2020. 45
Ziwei Ji and Matus Telgarsky. Characterizing the implicit bias via a primal-dual analysis. In Algorithmic
Learning Theory , pp. 772–804. PMLR, 2021. 12, 45
Ziwei Ji, Nathan Srebro, and Matus Telgarsky. Fast margin maximization via dual acceleration. In
International Conference on Machine Learning , pp. 4860–4869. PMLR, 2021. 12, 45
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980 , 2014. 8
Pang Wei Koh, Shiori Sagawa, Henrik Marklund, Sang Michael Xie, Marvin Zhang, Akshay Balsubramani,
Weihua Hu, Michihiro Yasunaga, Richard Lanas Phillips, Irena Gao, et al. Wilds: A benchmark of
in-the-wild distribution shifts. In International Conference on Machine Learning , pp. 5637–5664. PMLR,
2021. 42
Yiwen Kou, Zi-Yuan Chen, Yuanzhou Chen, and Quanquan Gu. Benign overfitting in two-layer relu
convolutional neural networks. In International Conference on Machine Learning , 2023a. URL https:
//api.semanticscholar.org/CorpusID:260871074 . 5
14Under review as submission to TMLR
Yiwen Kou, Zixiang Chen, and Quanquan Gu. Implicit bias of gradient descent for two-layer relu and leaky
relu networks on nearly-orthogonal data, 2023b. 45
Alex Krizhevsky. Learning multiple layers of features from tiny images. pp. 32–33, 2009. URL https:
//www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf . 44
Frederik Kunstner, Jacques Chen, Jonathan Wilder Lavington, and Mark Schmidt. Noise is not the main
factor behind the gap between sgd and adam on transformers, but sign descent might be. In The Eleventh
International Conference on Learning Representations , 2023. URL https://openreview.net/forum?id=
a65YK0cqH8g . 12, 44
Yann LeCun and Corinna Cortes. The mnist database of handwritten digits. 2005. 43
Seung Hoon Lee, Seunghyun Lee, and Byung Cheol Song. Vision transformer for small-size datasets, 2021. 43
HongkangLi, M.Wang, SijiaLiu, andPin-YuChen. Atheoreticalunderstandingofshallowvisiontransformers:
Learning, generalization, and sample complexity. ArXiv, abs/2302.06015, 2023a. 5
Hongkang Li, Meng Weng, Sijia Liu, and Pin-Yu Chen. A theoretical understanding of shallow vision
transformers: Learning, generalization, and sample complexity. In International Conference on Learning
Representations , 2023b. 12, 45
Yingcong Li, M. Emrullah Ildiz, Dimitris Papailiopoulos, and Samet Oymak. Transformers as algorithms:
Generalization and stability in in-context learning, 2023c. 45
Yingcong Li, Yixiao Huang, Muhammed E Ildiz, Ankit Singh Rawat, and Samet Oymak. Mechanics of
next token prediction with self-attention. In Sanjoy Dasgupta, Stephan Mandt, and Yingzhen Li (eds.),
Proceedings of The 27th International Conference on Artificial Intelligence and Statistics , volume 238 of
Proceedings of Machine Learning Research , pp. 685–693. PMLR, 02–04 May 2024. 45
Zhiyuan Li, Yuping Luo, and Kaifeng Lyu. Towards resolving the implicit bias of gradient descent for matrix
factorization: Greedy low-rank learning, 2021. 45
Chaoyue Liu, Libin Zhu, and Mikhail Belkin. Loss landscapes and optimization in over-parameterized
non-linear systems and neural networks. Applied and Computational Harmonic Analysis , 59:85–116, 2022.
37
Nicolas Loizou, Sharan Vaswani, Issam Hadj Laradji, and Simon Lacoste-Julien. Stochastic polyak step-size
for sgd: An adaptive learning rate for fast convergence. In Arindam Banerjee and Kenji Fukumizu (eds.),
Proceedings of The 24th International Conference on Artificial Intelligence and Statistics , volume 130
ofProceedings of Machine Learning Research , pp. 1306–1314. PMLR, 13–15 Apr 2021. URL https:
//proceedings.mlr.press/v130/loizou21a.html . 3, 7
Kaifeng Lyu and Jian Li. Gradient descent maximizes the margin of homogeneous neural networks. In
International Conference on Learning Representations , 2020. 45
Sadegh Mahdavi, Renjie Liao, and Christos Thrampoulidis. Memorization capacity of multi-head attention
in transformers. arXiv preprint arXiv:2306.02010 , 2023. 45
Ashok Vardhan Makkuva, Marco Bondaschi, Adway Girish, Alliot Nagle, Martin Jaggi, Hyeji Kim, and
Michael Gastpar. Attention with markov: A framework for principled analysis of transformers via markov
chains, 2024. 45
Vidya Muthukumar, Kailas Vodrahalli, and Anant Sahai. Harmless interpolation of noisy data in regression.
arXiv preprint arXiv:1903.09139 , 2019. 5
Mor Shpigel Nacson, Jason Lee, Suriya Gunasekar, Pedro Henrique Pamplona Savarese, Nathan Srebro, and
Daniel Soudry. Convergence of gradient descent on separable data. In The 22nd International Conference
on Artificial Intelligence and Statistics , pp. 3420–3428. PMLR, 2019. 3, 12, 30, 37, 45
15Under review as submission to TMLR
Quynh N Nguyen and Marco Mondelli. Global convergence of deep networks with one wide layer followed by
pyramidal topology. Advances in Neural Information Processing Systems , 33:11961–11972, 2020. 37
OpenAI. Openai: Introducing chatgpt, 2022. URL https://openai.com/blog/chatgpt,2022 . 1
Samet Oymak, Ankit Singh Rawat, Mahdi Soltanolkotabi, and Christos Thrampoulidis. On the role of
attention in prompt-tuning. In ICLR 2023 Workshop on Mathematical and Empirical Understanding of
Foundation Models , 2023. 3, 12, 45
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen,
Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Köpf, Edward Yang, Zach
DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and
Soumith Chintala. Pytorch: An imperative style, high-performance deep learning library, 2019. 42
Mary Phuong and Christoph H Lampert. The inductive bias of re{lu} networks on orthogonally separable
data. In International Conference on Learning Representations , 2021. URL https://openreview.net/
forum?id=krz7T0xU9Z_ . 45
Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish
Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning
transferable visual models from natural language supervision. In Marina Meila and Tong Zhang (eds.),
Proceedings of the 38th International Conference on Machine Learning , volume 139 of Proceedings of
Machine Learning Research , pp. 8748–8763. PMLR, 18–24 Jul 2021. URL https://proceedings.mlr.
press/v139/radford21a.html . 1
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou,
Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer.
21(1), 2020. ISSN 1532-4435. 1
Arda Sahiner, Tolga Ergen, Batu Ozturkler, John Pauly, Morteza Mardani, and Mert Pilanci. Unraveling
attention via convex duality: Analysis and interpretations of vision transformers. International Conference
on Machine Learning , 2022. 45
Clayton Sanford, Daniel Hsu, and Matus Telgarsky. Representational strengths and limitations of transformers.
arXiv preprint arXiv:2306.02896 , 2023. 45
Heejune Sheen, Siyu Chen, Tianhao Wang, and Harrison H. Zhou. Implicit regularization of gradient flow on
one-layer softmax attention, 2024. 12
Daniel Soudry, Elad Hoffer, Mor Shpigel Nacson, Suriya Gunasekar, and Nathan Srebro. The implicit bias of
gradient descent on separable data. The Journal of Machine Learning Research , 19(1):2822–2878, 2018. 1,
9
Hossein Taheri and Christos Thrampoulidis. Fast convergence in learning two-layer neural networks with
separable data. Proceedings of the AAAI Conference on Artificial Intelligence , 37(8):9944–9952, Jun. 2023a.
doi: 10.1609/aaai.v37i8.26186. URL https://ojs.aaai.org/index.php/AAAI/article/view/26186 . 37
Hossein Taheri and Christos Thrampoulidis. Generalization and stability of interpolating neural networks
with minimal width. arXiv preprint arXiv:2302.09235 , 2023b. 37
Davoud Ataee Tarzanagh, Yingcong Li, Christos Thrampoulidis, and Samet Oymak. Transformers as support
vector machines, 2023a. 1, 2, 3, 5, 12, 24, 26, 29, 41
Davoud Ataee Tarzanagh, Yingcong Li, Xuechen Zhang, and Samet Oymak. Max-margin token selection in
attention mechanism, 2023b. 1, 3, 12, 24, 26
Christos Thrampoulidis. Implicit bias of next-token prediction, 2024. 45
Yuandong Tian, Yiping Wang, Beidi Chen, and Simon Du. Scan and snap: Understanding training dynamics
and token composition in 1-layer transformer, 2023a. 3, 12, 45
16Under review as submission to TMLR
Yuandong Tian, Yiping Wang, Zhenyu Zhang, Beidi Chen, and Simon Du. Joma: Demystifying multilayer
transformers via joint dynamics of mlp and attention, 2023b. 12, 45
Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Herve
Jegou. Training data-efficient image transformers & distillation through attention. In Marina Meila
and Tong Zhang (eds.), Proceedings of the 38th International Conference on Machine Learning , volume
139 ofProceedings of Machine Learning Research , pp. 10347–10357. PMLR, 18–24 Jul 2021. URL
https://proceedings.mlr.press/v139/touvron21a.html . 1
Gal Vardi. On the implicit bias in deep-learning algorithms, 2022. 45
Gal Vardi and Ohad Shamir. Implicit regularization in relu networks with the square loss. In Conference on
Learning Theory , pp. 4224–4258. PMLR, 2021. 45
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser,
and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems , 30,
2017. 1
Johannes von Oswald, Eyvind Niklasson, Ettore Randazzo, João Sacramento, Alexander Mordvintsev, Andrey
Zhmoginov, and Max Vladymyrov. Transformers learn in-context by gradient descent. arXiv preprint
arXiv:2212.07677 , 2022. 45
Ke Wang and Christos Thrampoulidis. Binary classification of gaussian mixtures: Abundance of support
vectors, benign overfitting, and regularization. SIAM Journal on Mathematics of Data Science , 4(1):
260–284, 2022. doi: 10.1137/21M1415121. 5
Ke Wang, Vidya Muthukumar, and Christos Thrampoulidis. Benign overfitting in multiclass classification:
All roads lead to interpolation. Advances in Neural Information Processing Systems , 34:24164–24179, 2021.
5
Adina Williams, Nikita Nangia, and Samuel Bowman. A broad-coverage challenge corpus for sentence
understanding through inference. In Proceedings of the 2018 Conference of the North American Chapter of
the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers) ,
pp. 1112–1122. Association for Computational Linguistics, 2018. URL http://aclweb.org/anthology/
N18-1101 . 11, 42
Xingyu Xie, Pan Zhou, Huan Li, Zhouchen Lin, and Shuicheng YAN. Adan: Adaptive nesterov momentum al-
gorithm for faster optimizing deep models, 2023. URL https://openreview.net/forum?id=mPzpPv0geS2 .
12, 44
Chulhee Yun, Srinadh Bhojanapalli, Ankit Singh Rawat, Sashank J. Reddi, and Sanjiv Kumar. Are
transformers universal approximators of sequence-to-sequence functions?, 2020a. 45
Chulhee Yun, Yin-Wen Chang, Srinadh Bhojanapalli, Ankit Singh Rawat, Sashank J. Reddi, and Sanjiv
Kumar.o(n)connections are expressive enough: Universal approximability of sparse transformers, 2020b.
45
Ruiqi Zhang, Spencer Frei, and Peter L. Bartlett. Trained transformers learn linear models in-context, 2023.
45
17Under review as submission to TMLR
Appendix
A Training only W 18
A.1 Preliminaries . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18
A.2 Key Lemmas . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19
A.3 Proof of Theorem 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26
A.4 Optimal Rate for NGD . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27
A.5 Proofs for Additional Results in Section 3.2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28
B Joint Optimization 31
B.1 Preliminaries . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31
B.2 Key Lemmas . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32
B.3 Proof of Theorem 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39
B.4 Proofs of Theorems 3 and 4 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40
C Additional Experiments and Settings 42
C.1 Initializing in a Bad Stationary Direction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44
D Related Work 45
A Training only W
A.1 Preliminaries
∥A∥2denotes the spectral norm and λmin(A)denotes the minimum eigenvalue. Also, ∥A∥2,∞=max
i∥ai∥,
whereaiis theithrow ofA. We list the useful notations used in the Appendix in Table 1 for convenience.
Further, letφ′(v)∶=∇φ(v)=diag(φ(v))−φ(v)φ(v)⊺denote the Jacobian of the softmax vector φ(v)at
xi,ττthtoken in the ithsample
a(W) XWx 1
γ yXu∗
φ(⋅) softmax vector
φt
i,τφ(⋅)∣τ,τ∈[T]for sampleiat timet
Λ ∥Wmm∥
B maxi∥Xi∥2,∞
κmax
i(γi,opti−γi)
min
i(γi,opti−γi)
Υ κmin
iexp(γi,opti)
max
iexp(γi)
ζmax
i(γi,opti−γi)
max
iexp(γi)
Table 1: Useful Notation.
18Under review as submission to TMLR
v∈RT. We note that under Assumption 2, for any W, we have
⟨−∇ŴL(Wt),W⟩=−1
nn
∑
i=1ℓ′
t,i⟨yi∇WΦ(Wt,Xi),W⟩
=1
nn
∑
i=1ℓt,i(γi,opti−γi)φt
i,opti(1−φt
i,opti)[ai,opti−∑τ≠optiφt
i,τai,τ
∑τ≠optiφt
i,τ],(14)
whereai=XiWxi,1.
A.2 Key Lemmas
We first prove some key Lemmas that are useful in the proof of Theorem 1.
Lemma 2. Letw0
u,v∼N(0,σ2
0)for allu,v∈[d], with the initialization scale
σ0≤B−2d−1/2(C′−1log(kT−1
T−1)∧ηd−1/2(8Λ)−1),
for somek>1, and absolute constant C′>0. Further, if d≥log(2/δ)for anyδ∈(0,1), then with probability
at least 1−δ, we haveφ0
i,opti≥1
kTfor everyi∈[n].
Proof.Letw0
u,v∼N(0,σ2
0), for anyu,v∈[d]. Then for any i∈[n]andτ∈[T], for anyδ∈(0,1), with
probability at least 1−δwe have
(xi,τ−xi,opti)⊺W0xi,1≤∣(xi,τ−xi,opti)⊺Wxi,1∣
≤∥xi,τ−xi,opti∥∥W0∥2∥xi,1∥
≤2B2(σ0C(2√
d+√
log(2/δ)))
≤6CB2σ0√
d=∶B2σ0√
dC′.
Using this, we have
φ0
i,opti=1
1+∑
τ≠optiexp((xi,τ−xi,opti)⊺W0xi,1)
≥1
1+(T−1)exp(B2σ0√
dC′)
≥1
kT(usingσ0≤C′−1B−2d−1/2log(kT−1
T−1))
Also,∥W0∥≤2σ0d≤η(4B2Λ)−1.
Lemma 3. Under the conditions of Lemma 2, Assumptions 1 and 2, using the updates in Eq. (2)with
η≤(2B2)−1log(2T−1), at any time t≥0, for any sample j∈[n], the softmax weight for the optjtoken
satisfies
φt
j,optj≥φ0
j,optj∧1
T. (15)
Proof.We prove this by induction. Clearly, Eq. (15)is true fort=0. Next, we assume that it is true at time
t−1. Combining this with the conditions of Lemma 2, we have φt−1
j,optj≥1
2T.
Further, for any j∈[n]we can have one of the following two scenarios:
19Under review as submission to TMLR
Scenario 1: At timet−1,φt−1
j,optj>1−1
2T. This implies
φt−1
j,optj=1
1+∑τ≠optiexp((xi,τ−xj,optj)⊺Wt−1xj,1)>1−1
2T
/Leftr⫯g⊸tl⫯ne⇒ ∑
τ≠optjexp((xj,τ−xj,optj)⊺Wt−1xj,1)<1
2T−1
/Leftr⫯g⊸tl⫯ne⇒(xj,τ−xj,optj)⊺Wt−1xj,1≤−log(2T−1),∀τ≠optj.
After the gradient step at time t−1, for anyτ≠optj, we have,
(xj,τ−xj,optj)⊺Wtxj,1=(xj,τ−xj,optj)⊺Wt−1xj,1−ηt−1(xj,optj−xj,τ)⊺(−∇ŴL(Wt−1))xj,1
≤−log(2T−1)+η∥xj,τ−xj,optj∥∥xj,1∥
≤−log(2T−1)+2ηB2
≤0,
where the last step follows since η≤(2B2)−1log(2T−1). This gives
∑
τ≠optjexp((xj,τ−xj,optj)⊺Wtxj,1)≤T−1
/Leftr⫯g⊸tl⫯ne⇒φt
j,optj≥1
T.
Scenario 2: At timet−1,φt−1
j,optj≤1−1
2T. This gives
φt−1
j,optj(1−φt−1
j,optj)≥2T−1
4T2≥1
4T. (16)
We will use this to show that
(xj,optj−xj,τ)⊺(−∇ŴL(Wt))xj,1≥0. (17)
Leta′=a((xj,optj−xj,τ)x⊺
j,1)=X(xj,optj−xj,τ)x⊺
j,1x1. Using similar calculations as Eq. (14), we get
(xj,optj−xj,τ)⊺(−∇ŴL(Wt−1))xj,1
=1
nn
∑
i=1ℓt−1,i(γi,opti−γi)φt−1
i,opti(1−φt−1
i,opti)⎡⎢⎢⎢⎣a′
i,opti−∑τ≠optiφt−1
i,τa′
i,τ
∑τ≠optiφt−1
i,τ⎤⎥⎥⎥⎦. (18)
There are two cases:
Case 1: optj≠1. In this case, by splitting the sum over i∈[n]in Eq.(18)intoi=jandi≠j,
and using non-negativity of the softmax weights, we get
(xj,optj−xj,τ)⊺(−∇ŴL(Wt−1))xj,1
≥ℓt−1,j
n(γj,optj−γj)φt−1
j,optj(1−φt−1
j,optj)∥xj,1∥2min
τ′≠optj(xj,optj−xj,τ)⊺(xj,optj−xj,τ′)
−max
i≠jℓt−1,i(γi,opti−γi)φt−1
i,opti(1−φt−1
i,opti)∣x⊺
i,1xj,1∣max
τ′≠opti∣(xj,optj−xj,τ)⊺(xi,opti−xi,τ′)∣.(19)
Next, we use Assumption 1 and that nΥT≥1to get
max
i≠jmax
τ′≠opti(xj,optj−xj,τ)⊺(xi,opti−xi,τ′)≤max
i≠j∣x⊺
j,optjxi,opti∣+3 max
i,j∶i≠j,τ∈[T],
τ′∈[T]∖{optj}∣x⊺
i,τxj,τ′∣
≤4∥xj,optj∥2
4=∥xj,optj∥2. (20)
20Under review as submission to TMLR
Similarly,
min
τ′≠optj(xj,optj−xj,τ)⊺(xj,optj−xj,τ′)≥∥xj,optj∥2−3 max
τ∈[T],
τ′∈[T]∖{optj}∣x⊺
j,τxj,τ′∣
≥∥xj,optj∥2−3∥xj,optj∥2
4=1
4∥xj,optj∥2. (21)
Combining Eqs. (16), (20) and (21), we have
ζ∶=nmaxi≠jℓt,i(γi,opti−γi)φt−1
i,opti(1−φt−1
i,opti)
ℓt,j(γj,optj−γj)φt−1
j,optj(1−φt−1
j,optj)maxi,τ′≠opti(xj,optj−xj,τ)⊺(xi,opti−xi,τ′)
minτ′≠optj(xj,optj−xj,τ)⊺(xj,optj−xj,τ′)
≤nmaxiℓt−1,i(γi,opti−γi)
miniℓt−1,i(γi,opti−γi)1/4
1/(4T)∥xj,optj∥2
∥xj,optj∥2/4
≤4nTκmaxiℓt−1,i
miniℓt−1,i
≤4nTκmaxiexp(−(φt−1
i,optiγi,opti+(1−φt−1
i,opti)γi))
miniexp(−(φt−1
i,optiγi,opti+(1−φt−1
i,opti)γi))
≤4nTκmaxiexp(−γi)
miniexp(−γi,opti)=4nΥT. (22)
Using Assumption 1 and Eq. (22), we have
∥xj,1∥2≥ζmax
i≠j∣⟨xi,1,xj,1⟩∣,
which implies that Eq. (19) is non-negative.
Case 2: optj=1. In this case, let I∶={i∈[n]∶yi=yj∧opti=1}. Using Eq. (18), we
get
(xj,optj−xj,τ)⊺(−∇ŴL(Wt−1))xj,1
≥ℓt−1,j
n(γj,optj−γj)φt−1
j,optj(1−φt−1
j,optj)∥xj,1∥2min
τ′≠optj(xj,optj−xj,τ)⊺(xj,optj−xj,τ′)
+min
i≠j,i∈Iℓt−1,i(γi,opti−γi)φt−1
i,opti(1−φt−1
i,opti)x⊺
i,1xj,1min
τ′≠opti(xj,optj−xj,τ)⊺(xi,opti−xi,τ′)
−max
i≠j,i∉Iℓt−1,i(γi,opti−γi)φt−1
i,opti(1−φt−1
i,opti)∣x⊺
i,1xj,1∣max
τ′≠opti∣(xj,optj−xj,τ)⊺(xi,opti−xi,τ′)∣.(23)
Using Assumption 1, the second term is non-negative. For the remaining two terms, we can proceed in a
similar way as the previous case to show that using Assumption 1, these terms are also non-negative.
Using Eq. (17), we get
(xj,optj−xj,τ)⊺(Wt−Wt−1)xj,1=η(xj,optj−xj,τ)⊺(−∇ŴL(Wt−1))xj,1
∥∇ŴL(Wt−1)∥≥0.
Then, for any τ≠optj, we have
φt
j,optj/φt
j,τ
φt−1
j,optj/φt−1
j,τ=exp((xj,optj−xj,τ)⊺(Wt−Wt−1)xj,1)≥1.
By telescoping, we getφt
j,optj
φt
j,τ≥φ0
j,optj
φ0
j,τ, which implies that φt
j,optj≥φ0
j,optj.
21Under review as submission to TMLR
Lemma 4. Under Assumption 2, for Wmmas defined in (W-SVM) and anyW,
⟨−∇ŴL(W)
∥∇ŴL(W)∥F,Wmm
∥Wmm∥⟩≥(2B2Λ)−1>0.
Proof.First, using Eq. (14) for Wmm, we have
⟨−∇ŴL(W),Wmm⟩≥min
i(a∗
i,opti−max
τ≠optia∗
i,τ)1
nn
∑
i=1ℓi(γi,opti−γi)φi,opti(1−φi,opti)
≥1
nn
∑
i=1ℓi(γi,opti−γi)φi,opti(1−φi,opti), (24)
wherea∗
i=XiWmmxi,1, and the second inequality follows by the definition of Wmm.
Let ˆW∶=−∇ŴL(W)
∥∇ŴL(W)∥F, and ˆai=XiˆWxi,1, then by similar calculations as Eq. (14), we have
∥∇ŴL(W)∥F=⟨−∇ŴL(W),ˆW⟩
=1
nn
∑
i=1ℓi(γi,opti−γi)φi,opti(1−φi,opti)[ˆai,opti−∑τ≠optiφi,τˆai,τ
∑τ≠optiφi,τ]
≤max
i(ˆai,opti−min
τ≠optiˆai,τ)1
nn
∑
i=1ℓi(γi,opti−γi)φi,opti(1−φi,opti)
≤2B21
nn
∑
i=1ℓi(γi,opti−γi)φi,opti(1−φi,opti), (25)
where for the last step we use
max
i(ˆai,opti−min
τ≠optiˆai,τ)≤2 max
i,τ∣ˆai,τ∣=2 max
i,τ∣x⊺
i,τˆWxi,1∣
≤2 max
i∥Xi∥2,∞∥xi,1∥∥ˆW∥≤2B2.
Using Eqs. (24) and (25), we get
⟨−∇ŴL(W)
∥∇ŴL(W)∥F,Wmm
∥Wmm∥⟩≥(2B2Λ)−1.
Lemma 5 (Iterate Norm) .Using the updates in Eq. (2), under the conditions of Lemma 2 and Assumption 2,
at any time t>0, we have
2η((4B2Λ)−1∨1)t≥∥Wt∥F≥η(4B2Λ)−1t. (26)
Proof.Using Eq. (2) and Lemma 4 with W=Wt, we have
∥Wt∥=∥W0−t−1
∑
t′=0ηt′∇ŴL(Wt)∥
≥⟨W0,Wmm⟩+t−1
∑
t′=0η⟨−∇ŴL(Wt)
∥∇ŴL(Wt)∥F,Wmm
∥Wmm∥⟩
≥η(2B2Λ)−1t−∣⟨W0,Wmm⟩∣.
Under the conditions of Lemma 2,
∣⟨W0,Wmm⟩∣≤∥W0∥F≤η(4B2Λ)−1,
22Under review as submission to TMLR
which gives the required result for t>0.
Similarly, for the upper bound we have
∥Wt∥F=∥W0−t−1
∑
t′=0ηt′∇ŴL(Wt)∥
F
≤∥W0∥+t−1
∑
t′=0η
≤η(4B2Λ)−1+ηt.
Lemma 6 (Data Example 1) .Usingu∗=1
U2(µ+−µ−)andd≳Ω(n2T2log(4n
δ)), data generated as per
Example 1 satisfies Ass. 1 and 2 with probability at least 1−δ, for anyδ∈(0,1).
Proof.Usingu∗=1
U2(µ+−µ−), for alli∈[n]andτ≠opti, we have
γi,opti=yu⊺
∗xi,opti=1, γi,τ=yu⊺
∗xi,τ=0.
Using this, we have
Υ=max
i(γi,opti−γi)
min
i(γi,opti−γi)min
iexp(γi,opti)
max
iexp(γi)=e.
Next, by using Bernstein inequality, we know with probability at least 1−δ
2n, for everyi∈[n]andτ≠opti
we have
∣∥xi,opti∥2−σ2d−U2∣≤cσ2√
dlog(4n
δ)≤σ2d
2,
∣∥xi,τ∥2−ρ2d∣≤cρ2√
dlog(4n
δ)≤ρ2d
2.
wherec>0is an absolute constant and we use d≥4c2log(4n
δ).
Further, for any i,j∈[n],i≠jand anyτ,τ′∈[T]. By applying Bernstein’s inequality, each of the following is
true with probability at least 1−δ
2n2,
∣⟨xi,opti,xj,optj⟩−⟨µyi,µyj⟩∣≤2σ2√
dlog(4n
δ),∣⟨xi,τ′,xj,τ⟩∣≤2ρ2√
dlog(4n
δ),
∣⟨xi,opti,xj,τ⟩∣≤2σρ√
dlog(4n
δ),
whereτ′≠opti,τ≠optj.
In order to satisfy Assumption 1, ignoring the log factors, we require
(U2+σ2d)∧ρ2d≳nT(ρ2√
d∨σρ√
d)
U2+σ2√
d≳nT(σ2√
d∨ρ2√
d∨σρ√
d)
The above inequalities hold if we have d=̃Ω(n2T2)andU2≥̃Ω((σ∨ρ)2nT√
d). The proof finishes by the
application of a union bound.
We now state the key Lemma used in the proof of Thm. 1 below, with the expression for Rϵ.
23Under review as submission to TMLR
Lemma 7. Under the conditions of Lemma 2 and Assumptions 1 and 2, for any ϵ∈(0,1), there exists
Rϵ∶=2Λϵ−1(log(4n(B2Λ)−1ΥT3ϵ−1)∨5 log(20TB2Λϵ−1)),
such that for every twhere∥Wt∥≥Rϵ,
⟨−∇ŴL(Wt),Wmm
∥Wmm∥⟩≥(1−ϵ)⟨−∇ŴL(Wt),Wt
∥Wt∥⟩. (27)
Remark 1. Lem. 7 improves Lem. 10 of (Tarzanagh et al., 2023a), which only holds for n=1, i.e., for a
single training sample (same restriction as (Tarzanagh et al., 2023b)). Our key idea to extend the result to
n≥1is to divide the samples into three sets, based on whether the constraints in (W-SVM) are violated ( I1),
satisfied with a small margin ( I2) or well satisfied ( I3). We prove a tighter version for samples in I1and the
residual allows us to prove the result for the sum over all samples.
Proof.Let˜ai∶=XĩWtxi,1,a∗
i∶=XiWmmxi,1. We consider two scenarios based on whether ∥̃Wt−Wmm∥≤
ν
2B2(Scenario 1) or not (Scenario 2), where
̃Wt∶=Wt
∥Wt∥∥Wmm∥, ν∶=ϵ
(1−ϵ). (28)
Scenario 1 of the proof is the same as (Tarzanagh et al., 2023a). Below, we consider Scenario 2.
Using the definition of νand similar calculations to Eq. (14), Eq. (27) translates to
1
nn
∑
i=1ℓt,i(γi,opti−γi)φt
i,opti(1−φt
i,opti)˜ht,i≤(1+ν)1
nn
∑
i=1ℓt,i(γi,opti−γi)φt
i,opti(1−φt
i,opti)h∗
t,i,(29)
where ˜ht,i∶=˜ai,opti−∑τ≠optiφt
i,τ˜ai,τ
∑τ≠optiφt
i,τandh∗
t,iis defined similarly.
For any sample i∈[n], let
δmax
i(ϵ,ω)∶=max
τ∉{opti}˜ai,opti−˜ai,τ−1,
δmin
i(ϵ,ω)∶=min
τ∉{opti}˜ai,opti−˜ai,τ−1.
Define the following sets
I1∶={i∈[n] ∣δmin
i≤0.3ν},
I2∶={i∈[n] ∣0.3ν<δmin
i≤0.8ν},
I3∶={i∈[n] ∣δmin
i≥0.8ν}.
Intuitively, I1is the set of samples for which some (W-SVM) constraints are either violated or all constraints
are barely satisfied. Similarly, I3contains samples where all the (W-SVM) constraints are satisfied. Finally,
I2makes up the rest of the samples.
First, we will show that for any i∈I1,
[˜ai,opti−∑τ≠optiφt
i,τ˜ai,τ
∑τ≠optiφt
i,τ]≤(1+0.5ν)[a∗
i,opti−∑τ≠optiφt
i,τa∗
i,τ
∑τ≠optiφt
i,τ]. (30)
Next, we will show that for any i∈I2, we have ˜ht,i≤(1+ν)h∗
t,i, i.e.
[˜ai,opti−∑τ≠optiφt
i,τ˜ai,τ
∑τ≠optiφt
i,τ]≤(1+ν)[a∗
i,opti−∑τ≠optiφt
i,τa∗
i,τ
∑τ≠optiφt
i,τ]. (31)
24Under review as submission to TMLR
Finally, notice that in order to complete the proof for Eq. (29), using Eqs. (30) and (31) it suffices to show
that
∑
i∈I3ℓt,i(γi,opti−γi)φt
i,opti(1−φt
i,opti)(˜ht,i−(1+ν)h∗
t,i)
≤0.5ν∑
i∈I1ℓt,i(γi,opti−γi)φt
i,opti(1−φt
i,opti)h∗
t,i. (32)
Usingh∗
t,i≥1, Eq. (32) follows when
∑
i∈I3ℓt,i(γi,opti−γi)φt
i,opti(1−φt
i,opti)(˜ht,i−1−ν)≤0.5ν∑
i∈I1ℓt,i(γi,opti−γi)φt
i,opti(1−φt
i,opti).(33)
Case 1: Samplei∈I1, i.e.δmin
i≤0.3ν.
Ifδmax
i<0.5ν, (30) follows directly. For the rest of the samples iwhereδmax
i≥0.5ν, let
N∶={τ∈[T]∶ai,opti−ai,τ≤1+0.4ν}.
LetR′∶=∥Wt∥
∥Wmm∥, then by definition of N,δmin, we have
∑
τ≠opti,τ∉Nφt
i,τ
∑
τ≠optiφt
i,τ≤T maxτ≠opti,τ∉Nφt
i,τ
φt
i,τ≤T exp(−R′(1+0.4ν))
exp(−R′(1+δmin
i))
≤T exp(−0.1R′ν).
Combining these, we have
∑
τ≠opti(˜ai,opti−˜ai,τ)φt
i,τ
∑
τ≠optiφt
i,τ≤(1+0.4ν)∑
τ∈Nφt
i,τ
∑
τ≠optiφt
i,τ+(1+δmax
i)∑
τ≠opti,τ∉Nφt
i,τ
∑
τ≠optiφt
i,τ
≤(1+0.4ν)∑
τ≠optiφt
i,τ−∑
τ≠opti∉Nφt
i,τ
∑
τ≠optiφt
i,τ+(1+δmax
i)∑
τ≠opti,τ∉Nφt
i,τ
∑
τ≠optiφt
i,τ
≤1+0.4ν+(δmax
i−0.4ν)T exp(−0.1R′ν).
To satisfy (30), we need (δmax
i−0.4ν)T exp(−0.1R′ν)≤0.1ν, which is true when
R′≥(0.1ν)−1log(T(δmax
i/(0.1ν)−4)).
This is true since R′≥10ϵ−1log(10T maxi∈[n]δmax
i/ϵ), where we use the definition of νfrom Eq. (28).
Case 2: Samplei∈I2, i.e. 0.3ν<δmin≤0.8ν.
Ifδmax
i<ν,(31)follows directly. For the rest of the samples where δmax
i≥ν, we can proceed in a
similar way as Case 1. We use a threshold of 0.9νto split the tokens, and obtain
∑
τ≠opti(ai,opti−ai,τ)φt,i,τ
∑
τ≠optiφt,i,τ≤1+ν,
whenR′≥10ϵ−1log(10T maxi∈[n]δmax
i/ϵ).
25Under review as submission to TMLR
Case 3: Samplei∈I3, i.e.δmin
i≥0.8ν.
As∥̃Wt−Wmm∥≥ν
2B2, at least one sample i∈[n]violates the (W-SVM) constraints, i.e. ∣I1∣≥1. We have
∑
i∈I1ℓt,i(γi,opti−γi)φt
i,opti(1−φt
i,opti)≥min
i∈I1ℓt,i(γi,opti−γi)min
t(φt
i,opti)2∑
τ≠optiφt
i,τ/φt
i,opti
≥min
i∈I1ℓt,i(γi,opti−γi)(4T)−2exp(−(1+0.3ν)R′), (34)
where the last inequality follows by using Lemmas 2 and 3, and that δmin
i≤0.3νfor anyi∈I1. Also,
∑
i∈I3ℓt,i(γi,opti−γi)φt
i,opti(1−φt
i,opti)≤(n−1)max
i∈I3ℓt,i(γi,opti−γi)max
i∈I3∑
τ≠optiφt
i,τ/φt
i,opti
≤(n−1)max
i∈I3ℓt,i(γi,opti−γi)(T−1)exp(−(1+0.8ν)R′),(35)
where the last inequality follows because 0.8ν≤mini∈I3δmin
i. Combining Eqs. (34)and(35)gives us (33)
when
exp(0.5νR′)≥0.5ν(2B2Λ)−1(n−1)(T−1)(4T)2maxi∈I3ℓt,i(γi,opti−γi)
mini∈I1ℓt,i(γi,opti−γi), (36)
which is true when
exp(0.5νR′)≥4n(B2Λ)−1ΥT3ϵ−1,
which is true when R′≥2ϵ−1log(4n(B2Λ)−1ΥT3ϵ−1).
Combining all conditions on R′, and using δmax
i≤2B2Λfor alli∈[n], we get the desired result
since
Rϵ≥Λ−1=2Λϵ−1(log(4n(B2Λ)−1ΥT3ϵ−1)∨5 log(20TB2Λϵ−1)).
A.3 Proof of Theorem 1
We first restate Theorem 1, this time with the exact constants.
Theorem 5 (IB Rate).Under the conditions of Lemma 2 and Assumptions 1 and 2, using the updates in Eq.
(2)withη≤(2B2)−1log(2T−1), for anyt≥t0∶=(10BΛ√η)3
∨log(ηT(nΥ(B2Λ)−2T2∨5)
20Λ),
⟨Wt
∥Wt∥,Wmm
∥Wmm∥⟩≥1−C(η,B, Λ,t0)(logt)2
t,
whereC(η,B, Λ,t0)=4η−1B2Λ(∥Wt0∥−⟨Wt0,Wmm⟩+2B2Λ(40Λ+η)).
Remark 2 (Comparison to (Tarzanagh et al., 2023a).) .Thm. 1 establishes finite-time convergence of
NGD toWmmstarting from anyinitialization direction. This marks a clear improvement over prior work
(Tarzanagh et al., 2023a), which demonstrates that GD converges asymptotically (t→∞) toWmmonly under
an appropriate localinitialization direction. The only previously known global convergence result in Tarzanagh
et al. (2023b;a) requires Ass. 2 and n=1, limiting its applicability to training with a single sample. Thm.
1 addresses this limitation and additionally provides convergence rates. For completeness, we demonstrate
in Thm. 6 in the Appendix that GD (with constant step size) also converges globally to Wmmunder the
conditions specified in Thm. 1, without requiring n=1.
26Under review as submission to TMLR
Proof.First, we use Lemma 5 to show that Lemma 7 is true for any t≥t0, by selecting ϵ=ϵt∶=
(80B2Λ2logt)(ηt)−1. Since logt<1.25t1/3fort>0,ϵt≤1fort≥(10BΛη−1/2)3. We also have
2Λϵ−1(log(4nΥ(B2Λ)−1T3ϵ−1)∨5 log(20TB2Λϵ−1))≤10Λϵ−1log((4nΥ(B2Λ)−1T3∨20TB2Λ)ϵ−1)
=10Ληt
80B2Λ2logtlog((4nΥ(B2Λ)−1T3∨20TB2Λ)ηt
80B2Λ2logt)
≤η(4B2Λ)−1t
logt(logt∨log(ηT(nΥ(B2Λ)−2T2∨5)
20Λ))
≤η(4B2Λ)−1t.
Thus,∥Wt∥≥Rfort≥t0and we can use Lemma 7 with ϵ=ϵt,i.e.,
⟨−∇ŴL(Wt),Wmm
∥Wmm∥⟩≥(1−ϵt)⟨−∇ŴL(Wt),Wt
∥Wt∥⟩.
Using the update Eq. (2), we have
⟨Wt+1−Wt,Wmm
∥Wmm∥⟩≥(1−ϵt)⟨Wt+1−Wt,Wt
∥Wt∥⟩
=1
2∥Wt∥(∥Wt+1∥2−∥Wt∥2−∥Wt+1−Wt∥2)−ϵt⟨Wt+1−Wt,Wt
∥Wt∥⟩
=1
2∥Wt∥(∥Wt+1∥2−∥Wt∥2)−η2
2∥Wt∥+ϵtη⟨∇ŴL(Wt),Wt⟩(using Eq. (2))
≥∥Wt+1∥−∥Wt∥−η2
2∥Wt∥−ϵtη (sincea2−b2
2b≥a−b∀a,b>0)
≥∥Wt+1∥−∥Wt∥−80B2Λ2logt
t−2ηB2Λ(t+1)−1(substituting ϵt, using Lemma 5).
Summing over t≥t0, we get
⟨Wt,Wmm⟩≥∥Wt∥−∥Wt0∥+⟨Wt0,Wmm⟩−80B2Λ2t
∑
t′=t0(logt′)(t′)−1−2ηB2Λt
∑
t′=t0(t′+1)−1.(37)
We bound the last two terms as follows
t
∑
t′=t0(logt′)(t′)−1≤logtt
∑
t′=t0(t′)−1≤(logt)2,
t
∑
t′=t0(t′+1)−1≤logt.
Using these in Eq. (37) and dividing by ∥Wt∥throughout, we get
⟨Wt
∥Wt∥,Wmm
∥Wmm∥⟩≥1−1
∥Wt∥(∥Wt0∥−⟨Wt0,Wmm⟩+2B2Λ(40Λ+η)(logt)2)
≥1−C(η,B, Λ,t0)(logt)2
t,
where the last step follows by Lemma 5.
A.4 Optimal Rate for NGD
In Th. 1, we showed an upper bound on the correlation ⟨Wt,Wmm⟩≤1−˜Ω(t−1)for NGD. In this section,
we consider the example used in Sec. 3.3 to derive a lower bound on the correlation.
27Under review as submission to TMLR
We restate the example here for convenience. Let n=1,T=2,y=1,x1=[1,0]⊺,x2=[0,0]⊺andu∗=[1,0]⊺.
As we saw in Sec. 3.3, opt=1,γopt−γ=1,Wmm=(x1−x2)x⊺
1=XandΛ=1. Using this, we can write
∇ŴL(Wt)=−̂L(Wt)φt
opt(1−φt
opt)Wmm,andWt+1=W0+ηGtWmm,
whereGtWmm∶=∑t
t′=0∇ŴL(Wt′)
∥∇ŴL(Wt′)∥=(t+1)Wmm.
Clearly, ∥Wt∥≤ηt+∥W0∥≤O(t). Using Eq. (3), we have
⟨Wt,Wmm⟩≈1−∥W0∥2−(W11
0)2
2∥Wt∥2≤1−Ω(t−2).
Comparing this with the upper bound, we see that the rate in Th. 1 may not be the optimal rate for NGD.
However, it is the first finite-time convergence rate for self-attention. Improving this rate is an interesting
direction for future work.
A.5 Proofs for Additional Results in Section 3.2
Lemma 8 (Softmax score rate) .Under the conditions of Lemma 2 and Assumptions 1 and 2, using
the updates in Eq. (2), for anyi∈[n]and anyt≥211(B2Λ)2C(η,B, Λ,t0)∨t0, whereC(η,B, Λ,t0)∶=
4η−1B2Λ(∥Wt0∥−⟨Wt0,Wmm⟩+2B2Λ(40Λ+η)),t0∶=(10BΛ√η)3
∨log(ηT(nΥ(B2Λ)−2T2∨5)
20Λ),
φt
i,opti≥1
1+(T−1)exp(−η(8B2Λ2)−1t).
Proof.Using Theorem 1 and Lemma 5, we have
(xi,τ−xi,opti)⊺(Wt−Wmm∥Wt∥+Wmm∥Wt∥)xi,1≤2B2∥Wt∥∥Wt−Wmm∥−1
∥Wmm∥∥Wt∥
≤2√
2B2√
C(η,B, Λ,t0)
t1/2(2ηt)−η(4B2Λ2)−1t
=4ηB2√
2C(η,B, Λ,t0)√
t−η(4B2Λ2)−1t
≤−η(8B2Λ2)−1t, (38)
sincet≥211(B2Λ)2C(η,B, Λ,t0).
We use this to find the softmax rate as follows
φt
i,opti=1
1+∑τ≠optiexp((xi,τ−xi,opti)⊺Wtxi,1)
=1
1+∑τ≠optiexp((xi,τ−xi,opti)⊺(Wt−Wmm∥Wt∥+Wmm∥Wt∥)xi,1)
≥1
1+(T−1)exp(−η(8B2Λ2)−1t).
Theorem 6 (Asymptotic Convergence of GD) .Under the conditions of Lemma 2 and Assumptions 1 and 2,
using the standard GD updates with η≤log(2T−1)
B4ζ, for anyt≥tϵsuch that ∥Wt∥≥Rϵ∨1/2,
⟨Wt
∥Wt∥,Wmm
∥Wmm∥⟩≥1−ϵ−C(η,B, Λ,ϵ)
∥Wt∥,
whereC(η,B, Λ,ϵ)=2B2Λ
η(1−ϵ)∥Wtϵ∥(1−(1−ϵ)−1⟨Wtϵ,Wmm⟩−2η∥Wtϵ∥−1̂L(θtϵ)).
28Under review as submission to TMLR
Proof.First, we show that Lemma 3 holds for standard GD updates, i.e., for anyt≥0and anyj∈[n],
φt
j,optj≥φ0
j,optj∧1
T.
Following similar steps as the proof of Lemma 3, we consider two scenarios,
Scenario 1: At timet−1,φt−1
j,optj>1−1
2T. This implies
φt−1
j,optj≤−log(2T−1),∀τ≠optj.
After the gradient step at time t−1, for anyτ≠optj, we have,
(xj,τ−xj,optj)⊺Wtxj,1=(xj,τ−xj,optj)⊺Wt−1xj,1−η(xj,optj−xj,τ)⊺(−∇ŴL(Wt−1))xj,1
≤−log(2T−1)+η∥xj,τ−xj,optj∥∥xj,1∥∥∇ŴL(Wt−1)∥
≤−log(2T−1)+η(2B2)(B2̂L(Wt−1)max
i(γi,opti−γi)φt
i,opti(1−φt
i,opti))
≤−log(2T−1)+ηB4max
iexp(−γi)max
i(γi,opti−γi)
≤0,
where the last step follows since η≤log(2T−1)
B4ζ. This gives φt
j,optj≥1
T.
Scenario 2: At timet−1,φt−1
j,optj≤1−1
2T. The proof for this part is the same as Lemma 3.
Using this, we can easily show that Lemma 7 is true for GD updates, i.e., for anyϵ∈(0,1), there exists
Rϵ∶=2Λϵ−1(log(4n(B2Λ)−1ΥT3ϵ−1)∨5 log(20TB2Λϵ−1)),
such that for every twhere∥Wt∥≥Rϵ,
⟨−∇ŴL(Wt),Wmm
∥Wmm∥⟩≥(1−ϵ)⟨−∇ŴL(Wt),Wt
∥Wt∥⟩.
Once we have this, the remaining proof is the same as Theorem 4 in (Tarzanagh et al., 2023a).
Theorem 7 (IB rate for Polyak-step) .Under the conditions of Lemma 2 and Assumptions 1 and 2, using
GD updates with Polyak-step, with η≤(2B2)−1ω1log(2T−1), whereω1=β(1−β)Λ−1mini(γi,opti−γi), for
anyT0≥t≥t0∶=ω2ν((10BΛ√η)3
∨log(ηT(nΥ(B2Λ)−2T2∨5)
20Λ)), whereω2=0.5B2maxi(γi,opti−γi), such that for
anyi∈[n],φt
i,opti≤β,
⟨Wt
∥Wt∥,Wmm
∥Wmm∥⟩≥1−C(η,B, Λ,t0)(logt)2
t,
whereC(η,B, Λ,t0)=4η−1B2Λω2ν(∥Wt0∥−⟨Wt0,Wmm⟩+2B2Λω−1
1(40Λ+ηυω−1
1ω2)).
Proof.To extend our analysis and results for NGD updates to the Polyak-step, we mainly use the following
lower bound on the gradient norm,
∥∇ŴL(Wt)∥≥̂L(Wt)min
i⟨yi∇WΦ(θt,Xi),Wmm
∥Wmm∥⟩
=Λ−1̂L(Wt)min
i(γi,opti−γi)φt
i,opti(1−φt
i,opti)[a∗
i,opti−∑τ≠optiφt
i,τa∗
i,τ
∑τ≠optiφt
i,τ]
≥Λ−1̂L(Wt)min
i(γi,opti−γi)min
i,t′≤tφt′
i,opti(1−φt′
i,opti)
≥β(1−β)Λ−1min
i(γi,opti−γi)̂L(Wt)=ω1̂L(Wt).
29Under review as submission to TMLR
Note that similar lower bounds have appeared when deriving loss convergence rates of linear predictors
trained with NGD on separable data (Nacson et al., 2019).
We also have the following upper bound,
∥∇ŴL(Wt)∥≤̂L(Wt)(1/4)max
i(γi,opti−γi)(2B2)=ω2̂L(Wt).
First, we show that Lemma 3 holds for Polyak-step updates, i.e., for anyt≥0and anyj∈[n],
φt
j,optj≥φ0
j,optj∧1
T.
Following similar steps as the proof of Lemma 3, we consider two scenarios,
Scenario 1: At timet−1,φt−1
j,optj>1−1
2T. This implies
φt−1
j,optj≤−log(2T−1),∀τ≠optj.
After the gradient step at time t−1, for anyτ≠optj, we have,
(xj,τ−xj,optj)⊺Wtxj,1=(xj,τ−xj,optj)⊺Wt−1xj,1−ηt−1(xj,optj−xj,τ)⊺(−∇ŴL(Wt−1))xj,1
≤−log(2T−1)+η̂L(Wt−1)−̂L∗
∥∇ŴL(Wt−1)∥2∥xj,τ−xj,optj∥∥xj,1∥∥∇ŴL(Wt−1)∥
≤−log(2T−1)+η̂L(Wt−1)
ω1̂L(Wt−1)(2B2)
=−log(2T−1)+η2B2
ω1
≤0,
where the last step follows since η≤ω1log(2T−1)
2B2. This gives φt
j,optj≥1
T.
Scenario 2: At timet−1,φt−1
j,optj≤1−1
2T. The proof for this part is the same as Lemma 3.
Using this, we can easily show that Lemma 7 is true for Polyak-step updates, i.e., for anyϵ∈(0,1), there
exists
Rϵ∶=2Λϵ−1(log(4n(B2Λ)−1ΥT3ϵ−1)∨5 log(20TB2Λϵ−1)),
such that for every twhere∥Wt∥≥Rϵ,
⟨−∇ŴL(Wt),Wmm
∥Wmm∥⟩≥(1−ϵ)⟨−∇ŴL(Wt),Wt
∥Wt∥⟩. (39)
Next, following similar steps as the proof of Lemma 5, the iterate norm growth can be characterized as
follows. Using Lemma 4 with W=Wt, for anyt≤T0,
∥Wt∥=∥W0−t−1
∑
t′=0ηt′∇ŴL(Wt)∥
≥⟨W0,Wmm⟩+t−1
∑
t′=0η̂L(Wt)−̂L∗
∥∇ŴL(Wt)∥⟨−∇ŴL(Wt)
∥∇ŴL(Wt)∥,Wmm
∥Wmm∥⟩
≥η(2B2Λω2υ)−1t−∣⟨W0,Wmm⟩∣
≥η(4B2Λω2υ)−1t,
30Under review as submission to TMLR
sinceω2>1. Using this, and following similar steps as the proof of Theorem 1, we can show that Eq. (39)is
true for any t≥t0. Usingϵt=ϵt∶=(80B2Λ2logt)(ηt)−1, we have,
⟨Wt+1−Wt,Wmm
∥Wmm∥⟩≥(1−ϵt)⟨Wt+1−Wt,Wt
∥Wt∥⟩
=1
2∥Wt∥(∥Wt+1∥2−∥Wt∥2)−η2
2∥Wt∥(̂L(Wt)−̂L∗)2
∥∇ŴL(Wt)∥2+ϵtη̂L(Wt)−̂L∗
∥∇ŴL(Wt)∥⟨∇ŴL(Wt),Wt⟩
≥∥Wt+1∥−∥Wt∥−η2
2ω2
1∥Wt∥−ϵtη
ω1
≥∥Wt+1∥−∥Wt∥−80B2Λ2ω−1
1(logt)t−1−2ηυB2Λω−2
1ω2(t+1)−1.
Then, telescoping over t≥t0, we get
⟨Wt
∥Wt∥,Wmm
∥Wmm∥⟩≥1−1
∥Wt∥(∥Wt0∥−⟨Wt0,Wmm⟩+2B2Λω−1
1(40Λ+ηυω−1
1ω2)(logt)2)
≥1−C(η,B, Λ,t0)(logt)2
t.
B Joint Optimization
B.1 Preliminaries
We first state the complete version of Condition 1 below.
Condition 2 (Complete version of Condition 1) .LetB=αρ√
1.5d. We consider the following conditions for
anyδ∈(0,1),
•Zero initialization: ∥u0∥=0,∥W0∥=0,
•opttoken has larger norm: α≥6,
•Sufficient overparameterization: d≥C0α4n4log(10n2
δ), whereC0is an absolute constant,
•Small step-size: η≤1
18α2ρ2d∧αρ
160n∧log(2)
3B∧B
128ω0√
1.5n, whereω0=13(B∨1)5(B∨d).
Lemma 9. Under the data model DM and Condition 1, the following events are true with probability at least
1−δ,
•E1∶={α2ρ2d
2≤∥xi,opti∥2≤3α2ρ2d
2,∀i∈[n]},
•E2∶={ρ2d
2≤∥xi,τ∥2≤3ρ2d
2,∀i∈[n],τ≠opti},
•E3∶={∣⟨xi,opti,xj,optj⟩∣≤2α2ρ2√
dlog(10n2
δ),∀i≠j∈[n]},
•E4∶={∣⟨xi,τ′,xj,τ⟩∣≤2ρ2√
dlog(10n2
δ),∀i≠j∈[n],τ′≠opti,τ≠optj},
•E5∶={∣⟨xi,opti,xj,τ⟩∣≤2αρ2√
dlog(10n2
δ),∀i≠j∈[n],τ≠optj},
Proof.First, by applying Bernstein’s inequality, with probability at least 1−δ
5n, for everyi∈[n],
∣∥xi,opti∥2−α2ρ2d∣≤cα2ρ2√
dlog(10n
δ)≤α2ρ2d
2,
31Under review as submission to TMLR
wherec>0is an absolute constant and we use d≥(2c)2log(10n
δ)in the second inequality.
Similarly, since d≥(2c)2log(10n
δ), we also have that with probability at least 1−δ
5n, for every i∈[n],
τ≠opti,
∣∥xi,τ∥2−ρ2d∣≤cρ2√
dlog(10n
δ)≤ρ2d
2.
Further, for any i,j∈[n],i≠jand anyτ,τ′∈[T]since⟨xi,τ,xj,τ′⟩is zero-mean. By applying Bernstein’s
inequality, each of the following is true with probability at least 1−δ
5n2,
∣⟨xi,opti,xj,optj⟩∣≤2α2ρ2√
dlog(10n2
δ),∣⟨xi,τ′,xj,τ⟩∣≤2ρ2√
dlog(10n2
δ),
∣⟨xi,opti,xj,τ⟩∣≤2αρ2√
dlog(10n2
δ),
whereτ′≠opti,τ≠optj.
Applying a union bound over all these finishes the proof.
B.2 Key Lemmas
We first state some intermediate Lemmas that are useful in the proofs of Theorem 2 and 3.
Lemma 10 (Iterate Norm) .Using the updates in Eq. (10), at any time t>0,
∥ut∥≤3ηt1/3.
Proof.Using Eq. (10) and triangle inequality, we get
∥ut∥≤∥ut−1∥+η
t2/3∥∇ûL(θt)∥
∥∇ûL(θt)∥≤ηt−1
∑
t′=0(1+t′)−2/3≤3ηt1/3.
Lemma 11. Under Condition 1 and the data model DM, for any j∈[n], at anyt>0,
γt
j,optj−γt
j≥ηω2t−1
∑
t′=0(t+1)−2/3, φt
j,optj≥1
2and max
i,j∈[n]ℓt,i
ℓt,j≤C,
whereω2=ω2(α,ρ,n)∶=αρ
n√
log(10n2
δ)andC≥24is a universal constant.
Proof.We will prove this by induction. First, we prove the three parts at t=1as follows.
32Under review as submission to TMLR
Sinceu0=0, for anyj∈[n],ℓ0,j=1. Using this, We have
(γ1
j,optj−γ1
j)=−ηu
0(−∇ûL(θ0))⊺(xj,optj−xj,τ)
=ηu
0
nn
∑
i=1(yiX⊺
iφ(ai(Wt)))⊺(xj,optj−xj,τ)
≥ηu
0
n(φt
j,optjxj,optj+(1−φt
j,optj)xj,τ)⊺(xj,optj−xj,τ)
+ηu
0
n∑
i≠jℓt,iyiyj(φt
i,optixi,opti+(1−φt
i,opti)xi,τ)⊺(xj,optj−xj,τ)
≥ηu
0
n⎛
⎝1
2α2ρ2d
2−3ρ2d
2−αρ2√
dlog(10n2
δ)−2(n−1)Mtρ2α2√
dlog(10n2
δ)⎞
⎠
≥ηu
0
nα2ρ2d
16,
sinceα≥5,d≥(16n)2log(10n2
δ). Also, ∥∇ûL(θ0)∥≤maxi∈[n]∥Xi∥2,∞≤αρ√
1.5d. Using these, we get
(γ1
j,optj−γ1
j)≥η
16nα2ρ2√
d16n√
log(10n2
δ)
αρ√
1.5d
=ηαρ√
1.5√
log(10n2
δ)≥ηω2,
whenn>1. Next, we have
φ1
j,optj/φ1
j,τ
φ0
j,optj/φ0
j,τ=exp((xj,optj−xj,τ)⊺(W1−W0)xj,1)
=exp((xj,optj−xj,τ)⊺(−∇ŴL(θ0))xj,1)=1,
sinceu0=0. Therefore, φ1
j,optj=φ0
j,optj≥1
2.
Further, since η≤log(2)
3αρ√
1.5d, we have
max
i,j∈[n]ℓ1,i
ℓ1,j≤max
iexp(2∣u⊺
1Xi⊺φ1
i∣)≤exp(2∥u1∥αρ√
1.5d)
≤exp(6ηαρ√
1.5d)≤exp(2 log(2))=4≤C.
Next, we assume that these are true at iteration t. LetMt∶=maxiℓt,iandmt∶=miniℓt,i. We will first show
that for any j∈[n],
∆γt+1
j∶=(γt+1
j,optj−γt+1
j)−(γt
j,optj−γt
j)
=−ηu
t(−∇ûL(θt))⊺(xj,optj−xj,τ)≥η(t+1)−1/3ω2. (40)
33Under review as submission to TMLR
We have
n(−∇ûL(θt))⊺(xj,optj−xj,τ)=n
∑
i=1(ℓt,iyiX⊺
iφ(ai(Wt)))⊺(xj,optj−xj,τ)
=ℓt,j(φt
j,optjxj,optj+(1−φt
j,optj)xj,τ)⊺(xj,optj−xj,τ)
+∑
i≠jℓt,iyiyj(φt
i,optixi,opti+(1−φt
i,opti)xi,τ)⊺(xj,optj−xj,τ)
≥mt⎛
⎝1
2α2ρ2d
2−3ρ2d
2−αρ2√
dlog(10n2
δ)⎞
⎠−2(n−1)Mtρ2α2√
dlog(10n2
δ)
=2ρ2α2Mt√
dlog(10n2
δ)−2nρ2α2mt√
dlog(10n2
δ)⎛
⎜
⎝Mt
mt−α2ρ2d
4−3ρ2d
2−αρ2√
dlog(10n2
δ)
2nρ2α2√
dlog(10n2
δ)⎞
⎟
⎠
≥2ρ2α2̂L(θt)√
dlog(10n2
δ)−2nρ2α2mt√
dlog(10n2
δ)⎛
⎝Mt
mt−1
16n⌟roo⟪⟪op
⌟roo⟪mo⟨⌟roo⟪mo⟨⌟roo⟪⟨o⟪d
log(10n2
δ)⎞
⎠
≥2ρ2α2̂L(θt)√
dlog(10n2
δ), (41)
where we use the definitions of Mt,mt, and the first inequality follows by using Lemma 9 and the second part
of the IH, the second inequality follows since Mt≥̂L(θt),α≥6andd≥4log(10n2
δ), and the final inequality
follows by using the third part of the IH and that d≥(16Cn)2log(10n2
δ).
Next, we have ∥∇ûL(θt)∥≤maxi∈[n]∥Xi∥2,∞̂L(θt)≤αρ√
1.5d̂L(θt). Combining this with Eq. (41), we get
−∇ûL(θt)⊺(xj,optj−xj,τ)
∥∇ûL(θt)∥≥2ρα√
1.5n√
log(10n2
δ)≥ω1,
which implies Eq. (40). Using Eq. (40) and the first part of the IH, it follows that
γt+1
j,optj−γt+1
j≥γt
j,optj−γt
j+ηu
tω1≥ηω2t
∑
t′=0(t′+1)−2/3.
Next, we will show that for any j∈[n],φt+1
j,optj≥1
2. We consider two cases:
Case 1:φt
j,optj≥3
4. In this case, we can directly use the steps in Scenario 1 of the proof of Lemma 3, and
get thatφt+1
j,optj≥1
2sinceη≤log(3)
3α2ρ2d.
Case 2:φt
j,optj≤3
4. Using Lemma 10, we have
γt
i,opti−γt
i≤2∥ut∥max
i∥Xi∥2,∞≤6ηt−1
∑
t′=0(1+t′)−2/3αρ√
1.5d. (42)
34Under review as submission to TMLR
Next, using similar calculations as Eqs. (18) and (19), for any j∈[n], we have
(xj,optj−xj,τ)⊺(−∇ŴL(θt))xj,1
≥ℓt,j
n(γt
j,optj−γt
j)φt
j,optj(1−φt
j,optj)∥xj,1∥2∥xj,optj−xj,τ∥2
−(n−1)
nmax
i≠jℓt,i(γt
i,opti−γt
i)φt
i,opti(1−φt
i,opti)∣x⊺
i,1xj,1∣∣(xj,optj−xj,τ)⊺(xi,opti−xi,τ′)∣
≥mt
n(ηω2t−1
∑
t′=0(t′+1)−2/3)3
16ρ2d
2⎛
⎝α2ρ2d
2+ρ2d
2−2αρ2√
dlog(10n2
δ)⎞
⎠
−(n−1)
nMt(7.5ηαρ√
dt−1
∑
t′=0(t′+1)−2/3)α2ρ2√
dlog(10n2
δ)(α2+2α+1)ρ2√
dlog(10n2
δ)
≥α3ρ5d3/2
n(ηt−1
∑
t′=0(t′+1)−2/3)⎛
⎝3
26n√
dlog(10n2
δ)mt−32α2(n−1)Mtlog(10n2
δ)⎞
⎠
≥32α5ρ5d3/2
n(ηt−1
∑
t′=0(t′+1)−2/3)log(10n2
δ)⎛
⎝̂L(θt)+nmt⎛
⎝1
210α2n2⌟roo⟪⟪op
⌟roo⟪mo⟨⌟roo⟪mo⟨⌟roo⟪⟨o⟪d
log(10n2
δ)−Mt
mt⎞
⎠⎞
⎠
≥32α5ρ5d3/2
n(ηt−1
∑
t′=0(t′+1)−2/3)log(10n2
δ)̂L(θt), (43)
where we use the definitions of Mtandmt, and the second inequality follows by using Lemma 9, the
first two parts of the IH, and the final inequality uses the third part of the IH and follows when d≥
(210Cα2n2)2log(10n2
δ). This implies that
φt+1
j,optj/φt+1
j,τ
φt
j,optj/φt
j,τ=exp((xj,optj−xj,τ)⊺(Wt+1−Wt)xj,1)≥1.
Therefore,φt+1
j,optj≥φt
j,optj≥1
2.
Finally, we will show that Amax
t+1∶=maxi,j∈[n]ℓt+1,i
ℓt+1,j≤C. Letφt
i∶=φ(ai(Wt)). For anyi∈[n], we have
ℓt+1,i=exp(−yiu⊺
t+1X⊺
iφt+1
i)
=exp(−yiu⊺
t+1X⊺
i(φt+1
i−φt
i))exp(−yiu⊺
tX⊺
iφt
i)exp(yi(ηu
t∇ûL(θt))⊺X⊺
iφt
i)
=ℓt,iexp(−yiu⊺
t+1X⊺
i(φt+1
i−φt
i))exp⎛
⎝−ηu
t
nn
∑
j=1ℓt,j(yjX⊺
jφt
j)⊺yiX⊺
iφt
i⎞
⎠, (44)
whereSt,i∶=n
∑
j=1ℓt,j(yjX⊺
jφt
j)⊺yiX⊺
iφt
ican be expanded as
St,i=ℓt,i∥φt
i,optixi,opti+(1−φt
i,opti)xi,τ∥2
+∑
j≠iℓt,jyiyj(φt
j,optjxj,optj+(1−φt
j,optj)xj,τ′)⊺(φt
i,optixi,opti+(1−φt
i,opti)xi,τ). (45)
Using Lemma 9 and the second part of the IH, we have
∥φt
i,optixi,opti+(1−φt
i,opti)xi,τ∥2≤3α2ρ2d
2
∥φt
i,optixi,opti+(1−φt
i,opti)xi,τ∥2≥1
22α2ρ2d
2+12
22ρ2d
2−21
4αρ2√
dlog(10n2
δ)≥α2ρ2d
8,
∣(φt
j,optjxj,optj+(1−φt
j,optj)xj,τ′)⊺(φt
i,optixi,opti+(1−φt
i,opti)xi,τ)∣≤α2ρ2√
dlog(10n2
δ).
35Under review as submission to TMLR
Using these in Eq. (45), we get
St,i≤3α2ρ2d
2ℓt,i+nα2ρ2√
dlog(10n2
δ)Mt, St,i≥α2ρ2d
8ℓt,i−nα2ρ2√
dlog(10n2
δ)Mt.(46)
Next, for the second term, using similar calculations as Eq. (53) and Lemma 10, we have
Rt+1,i∶=−yiu⊺
t+1X⊺
i(φt+1
i−φt
i)≤∣u⊺
t+1X⊺
i(φt+1
i−φt
i)∣
≤2∥ut+1∥max
i∥Xi∥3
2,∞∥Wt+1−Wt∥
≤6η(αρ√
1.5d)3(t+1)1/3η(t+1)−1=2η2(αρ√
1.5d)3(t+1)−2/3. (47)
Consider the loss ratio for any two samples i,j∈[n],At∶=ℓt,i
ℓt,j. Using Eqs. (44) and (47), we have
At+1=Atexp(Rt+1,i)
exp(Rt+1,j)exp(−ηu
t
nSt,i)
exp(−ηu
t
nSt,j)≤Atexp(12η2(αρ√
1.5d)3(t+1)−2/3)exp(−ηu
t
nSt,i)
exp(−ηu
t
nSt,j)(48)
We consider two cases:
Case 1:At<C/2. In this case, we have
ηu
t
nSt,i≤η(t+1)−2/3max
i∥Xi∥2,∞≤ηαρ√
1.5d(t+1)−2/3.
Using this in Eq. (48), and that η≤log(2)
3αρ√
1.5d,η≤1
18α2ρ2d, we get
At+1≤Atexp(12η2(αρ√
1.5d)3)exp(2ηαρ√
1.5d)
≤Atexp(log(2)
3αρ√
1.5d1
18α2ρ2d(18α3ρ3d√
1.5d))exp(2 log(2)/3)
=2At≤C.
Case 2:At≥C/2.
In this case, using Eq. (46), we have
exp(−ηu
t
nSt,i)
exp(−ηu
t
nSt,j)≤exp⎛
⎝−ηu
t
n⎛
⎝α2ρ2d
8ℓt,i−3α2ρ2d
2ℓt,j−2nα2ρ2√
dlog(10n2
δ)Mt⎞
⎠⎞
⎠
≤exp⎛
⎝−ηu
tα2ρ2ℓt,j
n⎛
⎝d
8At−3d
2−2n√
dlog(10n2
δ)Amax
t⎞
⎠⎞
⎠.
Further,
ηu
tα2ρ2ℓt,j
n⎛
⎝d
8At−3d
2−2n√
dlog(10n2
δ)Amax
t⎞
⎠≥ηu
tα2ρ2mt
n⎛
⎝d
8C−3d
2−2n√
dlog(10n2
δ)C⎞
⎠
≥ηu
tα2ρ2mt
nC√
d
16⎛
⎝√
d−32n√
log(10n2
δ)⎞
⎠
≥ηu
tα2ρ2√
dMt
32n≥ηα2ρ2√
d
32n(t+1)−2/3̂L(θt)
αρ√
1.5d̂L(θt)
=ηαρ
40n(t+1)−2/3,
36Under review as submission to TMLR
asC≥24,d≥(32n)2log(10n2
δ)and∥∇ûL(θt)∥≤αρ√
1.5d̂L(θt). Further, since η≤αρ
160n, we have
exp(4η2(t+1)−2/3)exp(−ηαρ
40n(t+1)−2/3)≤1,
which gives At+1≤At≤C.
Lemma 12. Let˜u∶=∑
i∈[n]yixi,opti. Under Condition 1 and the data model DM, for any j∈[n],τ≠optj,
⟨yj(xj,optj+xj,τ),˜u
∥˜u∥⟩≥αρ
4√
d
n.
Proof.Using Lemma 9, we have
yj(xj,optj+xj,τ)⊺⎛
⎝∑
i∈[n]yixi,opti⎞
⎠=∥xj,optj∥2+yj∑
i≠j∈[n]yix⊺
i,optixj,optj+yj∑
i∈[n]yix⊺
i,optixj,τ
≥0.5α2ρ2d−2(n−1)α2ρ2√
dlog(10n2
δ)−2nαρ2√
dlog(10n2
δ)
≥0.5α2ρ2d−4α2ρ2√
dlog(10n2
δ)
≥α2ρ2d
3,
sinced≥(12n)2log(10n2
δ)Similarly, we also have
∥˜u∥≤√
nmax
i∥xi,opti∥2+n2max
i≠j∣⟨xi,opti,xj,optj⟩∣
≤⌟roo⟪⟪op
⌟roo⟪mo⟨⌟roo⟪mo⟨⌟roo⟪⟨o⟪n(1.5α2ρ2d)+2n2α2ρ2√
dlog(10n2
δ)
≤αρ√
1.75nd,
sinced≥(8n)2log(10n2
δ). Combining these and using 3√
1.75≤4finishes the proof.
Lemma 13. Under Condition 1 and the data model DM, using the updates in Eq. (10), exponential loss has
the following properties:
•∥∇θ̂L(θt)∥≥∥∇ûL(θt)∥≥ω1̂L(θt),
• max
θ′∈[θt,θt+1]̂L(θ′)≤8̂L(θt),
• max
θ′∈[θt,θt+1]∥∇2
θ̂L(θ′)∥≤8(ω(θt)∨ω(θt+1))̂L(θt),
whereω1=ω1(α,ρ,n,d )=B
8√
1.5n,ω(θt)∶=13(B∨1)5(B∨d)(∥ut∥∨1)2,B=αρ√
1.5d.
Remark 3. We note that the first property is the most challenging to show in the analysis and it yields a
PL-inequality-like form, e.g. for neural networks (Frei & Gu, 2021; Nguyen & Mondelli, 2020; Liu et al.,
2022). Analogous properties but for simpler settings, e.g. to obtain loss convergence results for linear predictors
using NGD on separable data have been shown in (Nacson et al., 2019). The second point is an analog of the
log-Lipschitzness property shown in (Taheri & Thrampoulidis, 2023a) for the analysis of two-layer neural
networks trained with NGD and controls the loss on a line between current iterate θtand the next one θt+1.
The third property of second-order self-boundedness has been seen previously in the convergence analysis of
multi-head self-attention (Deora et al., 2023) and MLPs (Taheri & Thrampoulidis, 2023b) trained with GD.
37Under review as submission to TMLR
Proof.We first obtain the lower bound on the gradient norm as follows.
∥∇θ̂L(θt)∥=sup
v∶∥v∥=1⟨1
nn
∑
i=1∣ℓ′
i,t∣yi∇θΦ(θt,Xi),v⟩
≥∥∇ûL(θt)∥=sup
v∶∥v∥=1⟨1
nn
∑
i=1∣ℓ′
i,t∣yi∇uΦ(θt,Xi),v⟩
≥̂L(θt)sup
v∶∥v∥=1min
iyi∇uΦ(θt,Xi)⊺v
≥̂L(θt)min
i⟨yi∇uΦ(θt,Xi),˜u
∥˜u∥⟩, (49)
where ˜u=∑
i∈[n]yixi,opti. Using Lemma 11 and 12,
min
i⟨yi∇uΦ(θt,Xi),˜u
∥˜u∥⟩=min
iyi(φt
i,optixi,opti+(1−φt
i,opti)xi,τ)⊺˜u
≥0.5 min
iyi(xi,opti+xi,τ)⊺˜u≥αρ
8√
d
n=ω1. (50)
Using Eq. (50) in Eq. (49), we get the first bullet point.
We show the second bullet point as follows. For any θ,θ′, we have
∣yΦ(θ,X)−yΦ(θ′,X)∣=∣u⊺X⊺φ(XW⊺x1)−u′⊺X⊺φ(XW′⊺x1)∣
≤∣(u−u′)⊺X⊺φ(XW⊺x1)∣+∣u′⊺X⊺(φ(XW⊺x1)−φ(XW′⊺x1))∣.(51)
These two terms can be bounded as follows.
∣(u−u′)⊺X⊺φ(XW⊺x1)∣≤∥u−u′∥∥X⊺∥1,2=∥X∥2,∞∥u−u′∥, (52)
∥X⊺(φ(XW⊺x1)−φ(XW′⊺x1))∥≤∥X⊺∥1,2∥φ(XW⊺x1)−φ(XW′⊺x1)∥1
≤2∥X∥2,∞∥XW⊺x1−XW′⊺x1∥∞
≤2∥X∥2
2,∞∥(W−W′)⊺x1∥
≤2∥X∥2
2,∞∥x1∥∥W−W′∥
≤2∥X∥3
2,∞∥W−W′∥. (53)
Using Eqs. (52) and (53) in Eq. (51) for θtandθt+λ(θt+1−θt), we get
max
λ∈[0,1]ℓ(yΦ(θt+λ(θt+1−θt),X))
ℓ(yΦ(θt,X))
≤max
λ∈[0,1]exp(∣yΦ(θt+λ(θt+1−θt),X)−yΦ(θt,X)∣)
≤max
λ∈[0,1]exp(2λ∥X∥3
2,∞∥ut∥∥Wt+1−Wt∥+λ∥X∥2,∞∥ut+1−ut∥)
≤exp(2ηW
t∥∇ŴL(θt)∥∥X∥3
2,∞∥ut∥+ηu
t∥∇ûL(θt)∥∥X∥2,∞) (using Eq. (2))
≤exp(6η∥X∥3
2,∞ηt1/3
(1+t)+η∥X∥2,∞) (using Lemma 10)
≤exp(6η2∥X∥3
2,∞+η∥X∥2,∞).
38Under review as submission to TMLR
Since this is true for every Xi, andη≤log(2)
3αρ√
1.5d∧log(3)
3α2ρ2d, we get
max
θ′∈[θt,θt+1]̂L(θ′)≤max
i∈[n]exp(6η2∥Xi∥3
2,∞+η∥Xi∥2,∞)̂L(θt)
=exp(6η2B3+ηB)̂L(θt)≤exp(6log(2)
3log(3)
2+log(2)
3)̂L(θt)
≤8̂L(θt).
Next, we obtain the upper bound on the Hessian norm as follows. Using Prop. 3 from (Deora et al., 2023),
we have
∥∇θΦ(θt,X)∥≤∥X∥2,∞+2∥X∥2
2,∞∥Xut∥∞,
∥∇2
θΦ(θt,X)∥≤6d∥X∥2
2,∞∥X∥2
1,∞∥Xut∥∞+2√
d∥X∥2
2,∞∥X∥1,∞.
Using these, we get
∥∇θΦ(θt,Xi)∥≤∥Xi∥2,∞+2∥Xi∥3
2,∞∥ut∥,
∥∇2
θΦ(θt,Xi)∥≤6d∥Xi∥5
2,∞∥ut∥+2√
d∥Xi∥3
2,∞,
/Leftr⫯g⊸tl⫯ne⇒∥∇θΦ(θt,Xi)∥2+∥∇2
θΦ(θt,Xi)∥≤max
i∈[n]4∥Xi∥6
2,∞∥ut∥2+6d∥Xi∥5
2,∞∥ut∥+2√
d∥Xi∥3
2,∞+∥Xi∥2
2,∞
=4B6∥ut∥2+6dB5∥ut∥+2√
dB3+B2
≤13(B∨1)5(B∨d)(∥ut∥∨1)2=∶ω(θt).
Using this, we get
max
θ′∈[θt,θt+1]∥∇2
θ̂L(θ′)∥≤max
θ′∈[θt,θt+1]ω(θ′)̂L(θ′)
≤8(ω(θt)∨ω(θt+1))̂L(θt).
B.3 Proof of Theorem 2
We first restate Theorem 2, this time with the exact constants.
Theorem 8 (Train loss convergence) .Under Condition 1 and the data model DM, using the updates in Eq.
(10), for anyt>0,
̂L(θt+1)≤O(exp(−ηω1
2(t+1)1/3)),
whereω1=αρ
8√
d
n.
Proof.First, using Lemma 10, we have
ω(θt)∨ω(θt+1)≤13(B∨1)5(B∨d)(∥ut∥∨∥ut+1∥∨1)2
≤13(B∨1)5(B∨d)(3η(t+1)1/3∨1)2
=13(B∨1)5(B∨d)(3η∨(t+1)−1/3)2(t+1)2/3
≤ω0(t+1)2/3, (54)
whereω0=13(B∨1)5(B∨d), sinceη≤1/3.
39Under review as submission to TMLR
Next, for some θ′∈[θt,θt+1], using the second order Taylor expansion of ̂L(θt+1), we have
̂L(θt+1)=̂L(θt)+⟨∇θ̂L(θt),θt+1−θt⟩+1
2(θt+1−θt)⊺∇2
θ̂L(θ′)(θt+1−θt)
≤̂L(θt)+⟨∇θ̂L(θt),θt+1−θt⟩+1
2∥θt+1−θt∥2max
θ′∈[θt,θt+1]∥∇2
θ̂L(θ′)∥
≤̂L(θt)−ηu
t∥∇ûL(θt)∥2−ηW
t∥∇ŴL(θt)∥2
+1
2((ηu
t)2∥∇ûL(θt)∥2+(ηW
t)2∥∇ŴL(θt)∥2)max
θ′∈[θt,θt+1]∥∇2
θ̂L(θ′)∥
≤̂L(θt)−η
(t+1)2/3∥∇ûL(θt)∥+1
2(η2
(t+1)4/3+η2
(t+1)2)max
θ′∈[θt,θt+1]∥∇2
θ̂L(θ′)∥(using Eq. (10))
≤̂L(θt)−ηω1
(t+1)2/3̂L(θt)+8η2
(t+1)4/3(ω(θt)∨ω(θt+1))̂L(θt) (using Lemma 13)
≤(1−ηω1
(t+1)2/3+8η2ω0(t+1)2/3)
(t+1)4/3))̂L(θt) (using Eq. (54))
≤(1−ηω1
2(t+1)2/3)̂L(θt)≤exp(−ηω1
2(t+1)−2/3)̂L(θt) (using Condition 1)
≤exp(−t
∑
t′=0ηω1
2(t+1)−2/3)̂L(θ0) (by telescoping)
≤exp(−ηω1
2(t+1)1/3)̂L(θ0).
B.4 Proofs of Theorems 3 and 4
We first restate Theorem 3, this time with the exact constants.
Theorem 9 (IB Rate under Joint Training) .Under Condition 1 and the data model DM, using the updates
in Eq.(10), for anyt≥tϵ∶=exp⎛
⎝(10BΛ)2
η(Cn2√
2d
250 log(10n2
δ)∨10B2Λ)1/3
ϵ−4/3⎞
⎠∨exp(B2Λ
η),
⟨Wt
∥Wt∥,Wmm
∥Wmm∥⟩≥1−ϵ−C(η,B, Λ,ϵ)
logt,
whereB=αρ√
1.5d,C≥24is an absolute constant, and
C(η,B, Λ,ϵ)=2B2Λ
η(1−ϵ)∥Wtϵ∥(1−(1−ϵ)−1⟨Wtϵ,Wmm⟩−2η∥Wtϵ∥−1̂L(θtϵ)).
Proof.In this case, Lemma 4 follows directly, using Lemma 11. Since ∥W0∥=0, using Eq. 10 and similar
calculations as the proof of Lemma 5, for any t>0,
∥Wt∥≥η(2B2Λ)−1logt. (55)
Next, using Lemma 11, we will show that for any ϵ∈(0,1), there exists
Rϵ∶=2Λϵ−1⎛
⎝log⎛
⎝4Cn2√
2d
log(10n2
δ)ϵ−1⎞
⎠∨5 log(40B2Λϵ−1)⎞
⎠, (56)
such that for every twhere∥Wt∥≥Rϵ,
⟨−∇ŴL(θt),Wmm
∥Wmm∥⟩≥(1−ϵ)⟨−∇ŴL(θt),Wt
∥Wt∥⟩. (57)
40Under review as submission to TMLR
Using similar calculations as the proof of Lemma 7, in the first two cases, since R′=R∥Wmm∥−1≥
10 log(40B2Λϵ−1), Eqs. (30) and (31) follow, respectively. Using Eq. (36), we can show that when
exp(0.5ϵ(1−ϵ)−1R′)≥0.5ν(n−1)(2)2maxi∈I3ℓt,i(γi,opti−γi)
mini∈I1ℓt,i(γi,opti−γi), (58)
Eq. (29) is satisfied. Using Lemma 11, Eq. (58) is true when
exp(0.5ϵ(1−ϵ)−1R′)≥4Cn2√
2d
log(10n2
δ)ϵ−1,
which is satisfied by Eq. (56).
Next, using Eqs. (55) and (58), we can show that for any t≥tϵ,∥Wt∥≥Rϵ∨1/2. We have
Rϵ=2Λϵ−1⎛
⎝log⎛
⎝4Cn2√
2d
log(10n2
δ)ϵ−1⎞
⎠∨5 log(40B2Λϵ−1)⎞
⎠
≤25Λ⎛
⎝Cn2√
2d
250 log(10n2
δ)∨10B2Λ⎞
⎠1/3
ϵ−4/3
≤η(4B2Λ)−1logt≤∥Wt∥.
In addition, ∥Wt∥≥1/2fort≥exp(B2Λ
η). Combining these and using similar steps as the proof of Theorem
4 in (Tarzanagh et al., 2023a), we get
⟨Wt
∥Wt∥,Wmm
∥Wmm∥⟩≥1−ϵ−η(2B2Λ)−1C(η,B,ϵ)
∥Wt∥
≥1−ϵ−C(η,B,ϵ)
logt,
where the last step follows by using Eq. (55).
Next, we restate Theorem 4 for convenience.
Theorem 10 (IB Rate ofu).Letγ∶=maxu∶∥u∥≤1miniyiu⊺xi,opti. Under Condition 1 and the data model
DM, using the updates in Eq. (10), for anyt≥tϵ∨exp(C(η,B, Λ,ϵ)(ϵ−1∨(8B2Λ)4)),
min
iyiut⊺xi,opti≥γ
4−1
1+exp(η(8B2Λ2)−1logt).
Proof.First, by definition of γ, we have
∥∇θ̂L(θt)∥≥γ
3̂L(θt).
Then, following similar steps as the proof of Theorem 2, we have
̂L(θt)≤exp(−ηγ
4(t+1)1/3)̂L(θ0).
Using this, we have
min
iexp(−yi(φi,optixi,opti+(1−φi,opti)xi,τ)⊺ut)≤exp(−ηγ
4(t+1)1/3),
/Leftr⫯g⊸tl⫯ne⇒ min
iyi(φi,optixi,opti+(1−φi,opti)xi,τ)⊺ut≥ηγ
4(t+1)1/3.
41Under review as submission to TMLR
Then, using the proof of Lemma 10, we get
min
iyi(φi,optixi,opti+(1−φi,opti)xi,τ)⊺ut≥γ
4
/Leftr⫯g⊸tl⫯ne⇒ min
iyiφi,optix⊺
i,optiut≥γ
4−max
i(1−φi,opti)∥xi,τ∥
≥γ
4−ρ√
1.5d
1+exp(−η(8B2Λ2)−1logt),
where for the last inequality, we use the following lower bound on the softmax scores,
φt
i,opti≥1−1
1+exp(η(8B2Λ2)−1logt)
which is obtained by using Theorem 3 and following similar steps as the proof of Lemma 1, as
(xi,τ−xi,opti)⊺(Wt−Wmm∥Wt∥+Wmm∥Wt∥)xi,1≤2B2∥Wt∥∥Wt−Wmm∥−1
∥Wmm∥∥Wt∥
≤4B2√
ϵ∨C(η,B, Λ,ϵ)
logt(2ηlogt)−η(4B2Λ2)−1logt
≤−η(8B2Λ2)−1logt,
sincet≥exp(C(η,B, Λ,ϵ)(ϵ−1∨(8B2Λ)4)).
Usingφi,opti≤1then finishes the proof.
C Additional Experiments and Settings
This section includes some additional experiments and details about the settings for the results included
in this work. We use the PyTorch (Paszke et al., 2019) library for our code, which is licensed under the
Modified BSD license.
Fig. 1 In this case, we use the MultiNLI (Williams et al., 2018) dataset, which contains sentence pairs
belonging to one of three classes: entailment, neutral, contradiction. The task is to predict whether the second
sentence entails, is neutral with, or contradicts the first sentence. It is released under the ODC-By license.
We use the Hugging Face pytorch-transformers implementation of the BERT bert-base-uncased
model, with pretrained weights (Devlin et al., 2019), released under Apache License 2.0. We use batch-size 32
to train all models. Learning rates are—Adam: 2e−5, SGD: 1e−3, SNGD: 0.01. Theηmaxfor SPS and
SNGD is set to 0.1.
Fig. 2 In this case, the samples are generated following Example 1, with U=1andρ=0.05. We setn=20,
d=100andT=6. We use a cap on the adaptive step-size, ηmax=100for NGD and NGD-mom, and ηmax=10
for Polyak-step. The learning rate ηis set as 0.025for NGD and NGD-mom, whereas for GD, it is set as
0.25. For NGD-mom, the momentum parameter is set to 0.9.
Fig. 3 In this case, the samples are generated based on the data model DM with ρ=0.1andα=3. We set
n=10,d=100andT=2. The learning rate ηis set as 0.002for NGD and NGD-joint, whereas for GD, it is
set as 0.02.
Fig. 4 We use the CivilComments dataset (Borkan et al., 2019), which consists of online comments and
the task is to classify if the comment is toxicornon-toxic . It is released under the CC BY-NC 4.0 license.
We use the BERT bert-base-uncased model , with pretrained weights (Devlin et al., 2019) using the
WILDS package (Koh et al., 2021). The learning rate for SGD is 10−3,10−2for NGD and 10−5for Adam.
The parameter ηmaxis set to 10−2for both SNGD and SPS. All models are trained with batch-size 32.
42Under review as submission to TMLR
SGD SNGD SPS AdamTrain loss Train accuracy Test loss Test accuracy
Figure 4: Comparison of train and test dynamics of various optimizers—SGD, SNGD, SPS, and Adam—while
fine-tuning a pre-trained BERT model on the CivilComments dataset.
Figure 5: Training dynamics when optimizing only Won synthetic data with antipodal opttokens.
Figure 6: Comparison of train and test dynamics of various optimizers—SGD, SNGD, SPS, and Adam—for a
ViT model on the MNIST dataset.
Figure 7: Comparison of train and test dynamics (top row) and parameter norm growth (bottom row) of
various optimizers—SGD, SNGD, SPS, and Adam—for a ViT model on the CIFAR-10 dataset.
Fig. 5 Training dynamics of a single-head self-attention model (Eq. (1)) when optimizing only Won
synthetic data (Example 1 with σ=0). In this case, the opttokens are antipodal instead of orthogonal. The
remaining settings are the same as those for Fig. 2, except we set ηmax=5for Polyak-step.
Fig. 6 We use the MNIST dataset (LeCun & Cortes, 2005), which contains gray-scale images of handwritten
digit 0−9. It is released under the CC BY-SA 3.0 license. We use the ViT for small-scale datasets
proposed in (Lee et al., 2021), referred to as ViT-small hereon. The implementation is available at https:
//github.com/lucidrains/vit-pytorch under the MIT license. We use patch-size 4, and set depth as 2,
number of heads as 8and MLP width as 128. All models are trained with a batch-size of 100. Learning
rates are set as follows. SGD: 0.1, SNGD: 0.001, Adam: 0.001.ηmaxfor SPS and SNGD is set to 0.01. We
observe that unlike language datasets SNGD trains slower than SGD, and Adam achieves no significant gain
43Under review as submission to TMLR
in terms of training speed over SGD (also reported in (Xie et al., 2023)), showcasing behaviour similar to
CNNs (Kunstner et al., 2023).
Fig. 7 We consider the CIFAR-10 dataset (Krizhevsky, 2009) which is a benchmark dataset for object
recongnition tasks and contains colored images from 10classes. It is released under the MIT license. We use
the ViT-small model with patch-size 4, and set depth as 8, number of heads as 32and MLP width as 512.
All models are trained with batch-size 100. Learning rates are set as follows. SGD: 0.1, SNGD: 0.001, Adam:
0.001. The parameter ηmaxis set to 10for SPS and 0.1for SNGD. As evident, SNGD portrays a slower
training speed similar to the observations for MNIST. Additionally, we plot the norm growth for various
layers of the ViT and see growth in parameters similar to our observations on synthetic datasets.
Compute and Runtimes. The experiments using synthetic data were run on Google Colab. The
experiments on vision and language datasets were run on an internal cluster with two NVIDIA V100 GPUs
with 32 GB memory each. We train for 40 epochs on MNIST and 80 epochs on CIFAR-10. The runtime for
the latter is about 2 hours for each setting. We fine-tune the models on language datasets for 10 epochs,
which takes about 32 hours for each run. In Figures 1 and 4, we plot the dynamics for every 200iterations.
C.1 Initializing in a Bad Stationary Direction
Fig. 8 shows a synthetic setting where we initialize in a bad stationary direction Winit—one that gives a
higher softmax score to the non-opt token. To be precise,
X1=[1 0.2
−1−0.2],X2=[−2.5 0.5
2.5−0.5],
with labels y1=−1,y2=1, respectively. We set u∗=[0,1]⊺. Using token optimality (Definition 1), this gives
opt1=2,opt2=1; see Fig. 8 (left) for illustration. The chosen initialization Winitviolates the (W-SVM)
constraints for i=2, that is
(x2,opt2−x2,τ≠opt2)⊺Winitx2,1<0.
This can be seen in Fig. 8 (left), where the — line correlates more with the non-opt token than the opt one.
This consequently forces the softmax score
φ0
2,opt2=1
1+exp((x2,τ≠opt2−x2,opt2)⊺Winitx2,1)<1
2,
which can be seen in Fig. 8 (plot 2). We call Winita "bad" stationary direction as starting with αWinit
andα→∞would result in φ2,opt2(αWinit)→0, and∇ŴL(αWinit)→0. On the other hand, for X1,Winit
behaves the opposite way (see plots 1-2 in Fig. 8).
Figure 8: The training dynamics for a synthetic setting with n=2,d=2,T=2. Attn-score denotes the
opt-token softmax score φt
i,opti, and alignment shows ⟨Wt,Wmm⟩. Plot 1 shows the initialization Winit, and
the(W-SVM) solutionWmmprojected onto the first token, respectively. For sample 2, the Winitis more
correlated to the non-opt token which consequently gives a small softmax score (plot 2). Plots 2-4 show that
despite this "bad" initialization direction GD converges to Wmmand achieves the loss minima ̂L∗.
Despite this “bad” starting direction, plots 3-4 in Fig. 8 show that GD still globally convergences to Wmm,
and minimizes the train loss. This behaviour aligns with our theoretical results (Thm. 1) showing global
parametric convergence starting in any direction Winit, with a small enough norm to avoid the bad stationary
directions in the limit as ∥Winit∥→∞.
44Under review as submission to TMLR
D Related Work
Implicit bias of NNs. Since the first few works characterizing the implicit bias of linear predictors on
separable data (see Introduction), there has been an abundance of work studying the implicit bias of gradient
based methods for both linear predictors and NNs. Nacson et al. (2019); Ji & Telgarsky (2021); Ji et al.
(2021) show fast convergence of GD-based methods to the max-margin predictor. For MLPs, early works
study the implicit bias of GD/gradient flow using exponentially-tailed classification losses towards the KKT
points of the corresponding max-margin problem in finite (Ji & Telgarsky, 2020; Lyu & Li, 2020) and infinite
width (Chizat & Bach, 2020). Additionally, works by Phuong & Lampert (2021); Frei et al. (2022b); Kou
et al. (2023b) study the implicit bias of GD trained ReLU/Leaky-ReLU networks on orthogonal data. Other
works also study the implicit bias towards rank minimization with square loss in regression settings (Vardi &
Shamir, 2021; Arora et al., 2019; Li et al., 2021). We encourage the reader to go through these works and a
recent survey (Vardi, 2022) for a thorough understanding.
Transformers theory. Several studies, such as those by Baldi & Vershynin (2022); Dong et al. (2021);
Yun et al. (2020a;b); Sanford et al. (2023); Bietti et al. (2023) have delved into exploring the expressivity of
attention, while Baldi & Vershynin (2022); Dong et al. (2021); Yun et al. (2020a;b); Mahdavi et al. (2023)
have initiated an investigation of its memory capacity. To gain insights into the optimization aspects of
training attention models, Sahiner et al. (2022); Ergen et al. (2022) have explored convex relaxations of
attention. Furthermore, a subdomain experiencing growing attention involves the theoretical exploration of
in-context learning, as evidenced by recent studies (von Oswald et al., 2022; Akyürek et al., 2023; Zhang
et al., 2023; Li et al., 2023c).
In this context, we discuss studies that seek to understand the optimization and generalization dynamics of
transformers. Jelassi et al. (2022) show that Vision Transformers (ViTs) learn spatially localized patterns in a
binary classification task using gradient-based methods. For a three-layer ViT starting from some structured
initialization to mimic a pre-trained network, Li et al. (2023b) show sample complexity bounds to achieve zero
generalization and show attention maps sparsify as SGD training proceeds. As a step towards understanding
the training dynamics of the closely related prompt-attention, Oymak et al. (2023) study the initial trajectory
of GD for one-layer attention. Further, there has also been work to obtain optimization and generalization
guarantees of GD in multi-head attention models (Deora et al., 2023). Additionally, recent work by Tian
et al. (2023a) attempts to understand SGD-dynamics for the task of next-token prediction for one-layer
transformer with a linear decoder showing a similar sparsifying effect in the attention map. More recently,
Tian et al. (2023b) extend this by analyzing the joint training dynamics of multi-layer transformer with an
MLP. Other than this, there has been recent progress towards a theoretical understanding of next-token
prediction dyanmics under GD training (Li et al., 2024; Thrampoulidis, 2024). Furthermore, Makkuva et al.
(2024) discussed the impact of the transformer architecture and distributional properties on the loss landscape
when modeling the data as a Markov source, and Ildiz et al. (2024) linked the dynamics of self-attention to
context-conditioned Markov chains.
45