Published in Transactions on Machine Learning Research (09/2024)
Decomposition of Equivariant Maps via Invariant Maps:
Application to Universal Approximation under Symmetry
Akiyoshi Sannai∗†sannai.akiyoshi.7z@kyoto-u.ac.jp
Department of Physics, Kyoto University, RIKEN
Kitashirakawa, Sakyo, 606-8502, Kyoto, Japan
Yuuki Takai∗takai@neptune.kanazawa-it.ac.jp
Kanazawa Institute of Technology
7-1 Ohgigaoka, Nonoichi, 921-8501, Ishikawa, Japan
Matthieu Cordonnier matthieu.cordonnier@gipsa-lab.fr
GIPSA-lab Grenoble INP, Grenoble INP, Université Grenoble Alpes
11 rue des Mathématiques, Grenoble Campus BP46, F-38402 SAINT MARTIN D’HERES CEDEX, France
Reviewed on OpenReview: https: // openreview. net/ forum? id= ycOLyHh1Ue
Abstract
In this paper, we develop a theory about the relationship between invariant and equivariant
maps with regard to a group G. We then leverage this theory in the context of deep neural
networks with group symmetries in order to obtain novel insight into their mechanisms.
More precisely, we establish a one-to-one relationship between equivariant maps and certain
invariant maps. This allows us to reduce arguments for equivariant maps to those for
invariant maps and vice versa. As an application, we propose a construction of universal
equivariant architectures built from universal invariant networks. We, in turn, explain how
the universal architectures arising from our construction differ from standard equivariant
architectures known to be universal. Furthermore, we explore the complexity, in terms of
the number of free parameters, of our models, and discuss the relation between invariant
and equivariant networks’ complexity. Finally, we also give an approximation rate for G-
equivariant deep neural networks with ReLU activation functions for finite group G.
1 Introduction
Symmetries play a fundamental role in many machine learning tasks. Incorporating these symmetries into
deep learning models has proven to be a successful strategy in various contexts. Notable examples include
the use of convolutional neural networks (CNNs) to address translation symmetries (LeCun et al., 2015;
Cohen & Welling, 2016), deep sets and graph neural networks for permutation symmetries (Zaheer et al.,
2017; Scarselli et al., 2008; Kipf & Welling, 2017; Defferrard et al., 2016), and spherical CNNs for rotation
symmetries (Cohen et al., 2018; Esteves et al., 2020). The underlying common principle in all these cases
is as follows: once the inherent symmetries of a target task are identified, the learning model is designed to
encode these symmetries. In doing so, we aim to improve the quality of learning by building a model that
fits the characteristics of the task better.
In mathematics, symmetries are represented by the concepts of groups and group actions. A group is a
set of transformations, and the action of a group results in a transformation of a given set. The symmetries
of group actions are usually divided into two categories: invariant tasks and equivariant tasks.Invariant
∗Both authors contributed equally to this research.
†This work includes results obtained while the author was affiliated with the Matsuo-Iwasawa Laboratory at The University
of Tokyo, Japan.
1Published in Transactions on Machine Learning Research (09/2024)
tasks require the output to remain unchanged by any transformation of the input via the group action. On
the other hand, in equivariant tasks, a transformation of the input results in a similar transformation of the
output. For example, in computer vision, object detection is an invariant task whereas image segmentation
is an equivariant task with regard to rotations and shift transformations.
Despite appearing in tasks of different natures, there are some mathematical relations between invariant
and equivariant maps. One can easily verify that the composition of an equivariant map followed by an
invariant one results in an invariant map. This relation is at the core of convolutional architectures, which
consist of layers made of an equivariant convolution followed by invariant pooling (LeCun et al., 2015). The
same is done by Maron et al. (2019b; 2020) to define G-invariant networks for a finite group G.
In this paper, we explore the relation between invariant and equivariant maps. Our main result Theorem 1
states that for a given group Gacting on a set, there is a one-to-one correspondence betweenG-equivariant
maps andHi-invariant maps, where the Hiare some stabilizer subgroups of G. This allows us to reduce any
equivariant tasks to some invariant tasks and vice versa.
AsamainapplicationofTheorem1,westudy universal approximation forequivariantmaps. The universal
approximation theorem (Pinkus, 1999), which is fundamental in deep learning, asserts that any reasonably
smoothfunctioncanbeapproximatedbyanartificialneuralnetworkwitharbitraryaccuracy. Inthepresence
of symmetries, we enforce the neural network architecture to match the symmetries of the target tasks, and,
by doing so, we significantly reduce the hypothesis class of neural networks. Naturally, it is essential to
ensure that the universal approximation property is not compromised.
It turns out that the universal approximation problem is more involved in the equivariant than in the
invariant setup. Indeed, for the symmetric group of permutations of nelementsSn, Zaheer et al. (2017)
showed that the DeepSets invariant architecture is universal via a representation theorem which is famous
as a solution for Hilbert’s 13th problem by Kolmogorov (1956) and Arnold (1957). However, they do not
provide such a theoretical guarantee for their equivariant architecture. Their result for invariance was then
extended to more general groups by Maron et al. (2019b) and Yarotsky (2022), but it is another series of
papers which later solved the equivariant case, each with their own techniques (Keriven & Peyré, 2019; Segol
& Lipman, 2019; Ravanbakhsh, 2020).
Using our decomposition Theorem 1, we propose, in Theorem 2 an alternative way to build universal
G-equivariant architectures via Hi-invariant maps for some suitable subgroups Hi⊂G. As we explain in
Remarks 1 and 2, the equivariant universal approximator that we obtain is different from others in the
literature.
As a second application of our main theorem, we examine the number of parameters as well as the rate of
approximation by invariant/equivariant neural networks with ReLU activation. In Theorem 3, we provide
both lower and upper bounds on the minimal number of parameters required to approximate an equivariant
map to a given accuracy with regard to the minimal number of parameters required to approximate an
invariant map in that same accuracy. Finally, in Theorem 4 and Corollary 1, we give an approximation rate
forG-equivariant neural networks among G-equivariant functions with Hölder smoothness condition. This
last result is an extension of a result from Sannai et al. (2021) from Snto any finite group G.
1.1 Contributions
Our contributions are summarized as follows:
•We introduce a relation between invariant maps and equivariant maps. This allows us to reduce
some problems for equivariant maps to those for invariant maps.
•We apply the relation to constructing universal approximators by equivariant deep neural network
modelsforequivariantmaps. Ourmodelsforuniversalapproximatorsaredifferentfromthestandard
models, such as the ones from Zaheer et al. (2017) or Maron et al. (2019a). However, the number
of parameters in our models can be very small compared to the fully connected models.
2Published in Transactions on Machine Learning Research (09/2024)
•As other applications of the relation, we show some inequalities among invariant and equivariant
deepneuralnetworksregardingtheirabilitytoapproximateanyinvariantandequivariantcontinuous
maps, respectively, for a given accuracy. Moreover, we show an approximation rate of G-invariant
andG-equivariant ReLU deep neural networks for elements of a Hölder space.
1.2 Related work
Symmetries in machine learning Symmetries have been considered since the early days of machine
learning by Shawe-Taylor (1989; 1993); Wood & Shawe-Taylor (1996). In contemporary deep learning,
they keep generating increasing interest following seminal works from Kondor (2008); Gens & Domingos
(2014); Qi et al. (2017); Ravanbakhsh et al. (2017); Zaheer et al. (2017). The most commonly encountered
symmetries involve translations, addressed by convolutional architectures (LeCun et al., 2015) and their
generalization to arbitrary compact groups (Cohen & Welling, 2016; Kondor, 2008); rotations (Cohen et al.,
2018; Esteves et al., 2020); as well as permutations. The latter are particularly relevant for set data (Zaheer
et al., 2017; Qi et al., 2017; Maron et al., 2020) as well as for graph data via most of the graph neural
network architectures (Scarselli et al., 2008; Defferrard et al., 2016; Kipf & Welling, 2017; Bruna et al., 2014).
PermutationsymmetriesarealsopresentinmodernTransformersofarchitectures, viaselfattention(Vaswani
etal.,2017)andpositionalencoding. RecentextensionofTransformersforgraphlearningasproposedbyKim
et al. (2021), utilize Laplacian eigenvectors for positional encoding features. Thus, to address ambiguity in
eigenvector choices, Lim et al. (2023) recently proposed an architecture that is invariant to change of basis in
the eigenspaces. Villar et al. (2021; 2024) are interested in the symmetries arousing from classical physics and
the distinction between “passive symmetries” coming from physical law and being empirically observed, and
“passive symmetries” coming from arbitrary choice of design such as labeling of elements of a set or nodes of
a graph. Regarding the connection between invariant and equivariant maps, this work is a direct follow-up
of Sannai et al. (2019). The methods and results from Theorems 1 and 2 extend the content of Sannai
et al. (2019) from the symmetric group Snto an arbitrary group. More recently, Blum-Smith & Villar
(2023) explain how to build equivariant maps from invariant polynomials using invariant theory. Finally,
we shall mention that invariant/equivariant deep learning is now a fast growing field with a plethora of
various architectures. Some researchers have recently proposed to unify most of existing approaches through
a general framework known as Geometric Deep Learning (Bronstein et al., 2021).
Symmetries and universal approximation Universal Approximation property is fundamental classical
deep learning, extensively studied since the pioneer works by Cybenko (1989); Hornik et al. (1989); Funa-
hashi (1989); Barron (1994); Kůrková (1992); Pinkus (1999) and more and further explored by Sonoda &
Murata (2017); Hanin & Sellke (2017); Hanin (2019). Concerning invariant architecture, Yarotsky (2022)
observed that it is straightforward to build a universal invariant architecture from universal classic architec-
ture just by group averaging when the group is finite. However, this is unfeasible for large groups such as Sn.
Additionally, this issue was recently addressed by Sannai et al. (2024) who showed that, it can sometimes
be enough to average over a subset of the group rather that the entire group. By doing so, they are, for
instance, able to reduce the averaging operation complexity from O(n!)toO(n2)in the case of graph neural
networks. Zaheer et al. (2017) prove universality of DeepSets via a representation theorem from Kolmogorov
(1956) and Arnold (1957) coupled with a sum decomposition. The effectiveness of this universal decompo-
sition for continuous functions on sets is discussed by Wagstaff et al. (2022), who argue that the underlying
latent dimension must be high-dimensional. Recently, Tabaghi & Wang (2023) improved the universality
result from Zaheer et al. (2017). Some generalizations to other groups have been proposed by Maron et al.
(2019b) and Yarotsky (2022), and universality of some invariant graph neural networks was proved by Maron
et al. (2019b) and Keriven & Peyré (2019).
In the case of equivariant networks, Keriven & Peyré (2019); Segol & Lipman (2019); Ravanbakhsh (2020)
establisheduniversalityresultsforfinitegroupsusinghighordertensors. Morerecently,Dym&Maron(2021)
suggested a universal architecture for rotation equivariance and Yarotsky (2022) for the semi-direct product
ofRnandSOn(R)(translation plus rotation). In this paper, we propose circumventing the challenge of
directly addressing equivariant universality by constructing equivariant universal architectures directly from
invariant networks, known to be universal, via our decomposition theorem. The resulting architecture differs
from others in the literature, as explained in more details in Remarks 1 and 2.
3Published in Transactions on Machine Learning Research (09/2024)
2 Preliminaries
In this section, we review some notions of group actions and introduce invariance/equivariance for maps. In
Appendix A, we summarize some necessary notions and various examples for groups.
For setsXandV, we consider the set Map(X,V ) =VX={f:X→V}of maps from XtoV. In many
situations, we regard Xas an index set and Vas a set of objects (such as channels or pictures). We show
some examples.
Example 1. (i) IfX={1,2,...,n}andV=R, then Map(X,V )is idenfied with RnbyMap(X,V )→
Rn:f∝⇕⊣√∫⊔≀→(f(1),f(2),...,f (n))⊤.
(ii)Letℓ,mbe positive integers, and X={1,2,...,ℓ}×{ 1,2,...,m}andV={0,1,..., 255}3. Then,
Map(X,V )can be regarded as the set of digital images of ℓ×mpixels with the RGB color channels.
Forf∈Map(X,V )and(i,j)∈X,f(i,j) = (rij,gij,bij)⊤(rij,gij,bij∈{1,2,..., 255}) represents
RGB color at (i,j)-th pixel.
(iii)LetV′⊂VXbe a set of some digital images as in Example 1(ii) and X′={1,2,...,n}. Then,
V′X′is the set of n-tuples of the digital images in V′.
(iv)ForX=R2andV=R3,Map(R2,R3) = (R3)R2can be regarded as the space of “ideal” images.
Here, the “ideal” means that the size of the image is “infinitely extended” and the pixels of the
image are “infinitely detailed” in the sense of Yarotsky (2022). This is a similar notion to the set
L2(Rν,Rm)of the “signals” introduced in Yarotsky (2022, Section 3.2).
Next, we consider a group action on the set VX. LetGbe a group and Xbe aG-set, i.e., a set on which
Gacts from the left. Then, Galso acts on VXfrom the left1for any set Vby
(σ·f)(x) =f(σ−1·x), (1)
forf∈VXandσ∈G. Forf∈VX, letOfbe theG-orbitG·f={σ·f|σ∈G}. A few examples of such
type of action are listed below.
Example 2. (i)The permutation group Snof the setX={1,2,...,n}acts onXby permutation.
Any unordered set {v1,v2,...,vn}ofnelements of Vcan be regarded as an Sn-orbit of the ordered
n-tuple (v1,v2,...,vn)∈VXforX={1,2,...,n}.
(ii)TheSn-orbit of an element finMap({1,2,...,n},R3)can be regarded as a point cloud consisting
ofnpoints in R3.
(iii)LetX={1,2,...,n}2andV=R. We consider the diagonal Sn-actionσ·(i,j) = (σ(i),σ(j))
(σ∈Sn) onX. Then,Snalso acts on VX=Rn×n. Let Sym(n)be the subset of symmetric
matrices in Rn×n. This Sym(n)is stable by the diagonal action of Sn. TheSn-orbitSn·Aof an
A∈Sym(n)⊂Rn×ncan be regarded as an isomorphism class of undirected weighted graph. Indeed,
Sym(n)is the set of adjacency matrices of undirected weighted graphs, and two graphs are isomorphic
if and only if the corresponding adjacency matrices A1,A2satisfyA1=σ·A2for an element σ∈Sn.
We define invariant and equivariant maps as follows:
Definition 1. LetGbe a group, X,Ybe twoG-sets, andV,Wbe two sets. Then, a map FfromVXto
WisG-invariant ifFsatisfiesF(σ·f) =F(f)for everyσ∈Gandf∈VX. A mapFfromVXtoWYis
G-equivariant ifFsatisfiesF(σ·f) =σ·F(f)for everyσ∈Gandf∈VX. We denote
InvG(VX,W) ={F:VX→W|G-invariant},
and
EquivG(VX,WY) ={F:VX→WY|G-equivariant}.
1Acting from the left means that for any σ,τ∈G, the formula σ·(τ·f) = (στ)·fholds. To ensure this, we must use σ−1,
rather than σ, on the right-hand part of equation (1). Otherwise we would have σ·(τ·f) = (τσ)·f.
4Published in Transactions on Machine Learning Research (09/2024)
Some examples of invariant or equivariant tasks are the following
Example 3. (i) LetX=Y={1,2,...,n}andV=R3. The classification task of point clouds
can be regarded as a task to find an appropriate Sn-invariant map from VXto a set of classes
W={c1,c2,...,cm}.
(ii) A task of anomaly detection from npictures is a permutation equivariant task as in Zaheer
et al. (2017, Appendix I). This is to find an appropriate Sn-equivariant map VXtoWYfor
V= ({0,1,..., 255}3)ℓ×m,W={0,1}, andX=Y={1,2,...,n}.
(iii) A task of classification of digital images of ℓ×ℓpixels is finding an appropriate 90-degree rotation
invariant map VXto a set of classes W={c1,c2,...,cm}, whereV={0,1,..., 255}3andX=
{1,2,...,ℓ}2. An image segmentation task can be regarded as finding an appropriate 90-degree
rotation equivariant map VXtoWYfor the same V,W,Xas Example 3 (ii) and Y={1,2,...,ℓ}2.
(iv) An extension of Example 3 (iii) to “ideal images” is finding an SO2(R)-invariant map from VX
toW(orSO2(R)-equivariant map from VXtoWY) for andX=R2,V=R3, andW=R(or
X=Y=R2,V=R3, andW=Rrespectively).
3 Invariant-equivariant relation and universal approximation
3.1 Warm up example with Sn
We begin this section with a warm-up example. In this subsection, G=Sn,X={1,...,n}andV,Ware
generic sets. We denote as fan element of VX. The action of SnonVXis as in equation (1)
(σ·f)(i) =f(σ−1(i)),∀1≤i≤n, (2)
and so is the action of SnonWX. LetFbe a map from VXtoWX, then there exist nmapsF1,...,Fn
fromVXtoWsuch that
F(f)(i) =Fi(f),∀1≤i≤n. (3)
Fiis just a shorter notation for the map F(·)(i). Now let us make the assumption that Fbelongs to
EquivSn(VX,WX), the equivariance property F(σ·f) =σ·(F(f))implies for all i,
Fi(σ·f) =Fσ−1(i)(f). (4)
Let us focus on i= 1. Notice that if σis a permutation such σ(1) = 1, it is straightforward from equation (4)
thatF1(σ·f) =F1(f)for allf∈VX. Since it is fairly known that the set of permutations such that σ(1) = 1
is a subgroup of Sn, denoted as StabSn(1), (this group is isomorphic to Sn−1), we deduce from equation (4)
that
F1=F(·)(1)∈InvStabSn(1)(VX,W). (5)
Moreover, if we let (1i)be the transposition that exchanges 1andiand fix any other j∈X, we obtain
easily from equation (4) that for all f
F1((1i)·f) =F(1i)·1(f) =Fi(f). (6)
To sum up, we have almost proven the following claim.
Proposition 1. F∈EquivSn(VX,WX)if and only if there exists F1∈InvStabSn(1)(VX,W)such that for
allf∈VX, fori= 1,...,n,
F(f)(i) =F1((1i)·f). (7)
Proof.The discussion above is a proof of the nontrivial implication. Reciprocally, one easily verifies that
pickingF1∈InvStabSn(1)(VX,W)and defining F:VX→WXby
F(f)(i) =F1((1i)·f,
yields a map F∈EquivSn(VX,WX).
The rest of this section is dedicated to a generalization of this result to a generic group G.
5Published in Transactions on Machine Learning Research (09/2024)
3.2 Invariant-equivariant relation
Our main theorem is a relation between a G-equivariant maps and some invariant maps for some subgroups
ofG. Recall that, for any yin aG-setY, its stabilizer, denoted as StabG(y), is the subgroup of all the σ∈G
such thatσ·y=y(see the recall en groups and group actions in Appendix A).
Theorem 1. LetGbe a group and X,Ybe twoG-sets. LetV,Wbe two sets. Let Y=/unionsqtext
i∈IOyibe the
G-orbit decomposition of Y. We fix a system of representatives {yi∈Y|i∈I}. Then, the following map is
bijective:
Φ : EquivG(VX,WY)−→/producttext
i∈IInvStabG(yi)(VX,W)
F∝⇕⊣√∫⊔≀−→ Φ(F) = (F(·)(yi))i∈I.
Moreover, its inverse map
Ψ :/producttext
i∈IInvStabG(yi)(VX,W)−→ EquivG(VX,WY)
(/tildewideFi)i∈I∝⇕⊣√∫⊔≀−→ Ψ/parenleftig
(/tildewideFi)i∈I/parenrightig
,
is defined by
Ψ/parenleftig
(/tildewideFi)i∈I/parenrightig
(f)(y) =/tildewideFi(σ·f), (8)
for anyf∈VXandy∈Ysuch thaty∈Oyiandy=σ−1·yifor someσ∈G.
In addition, if VandWare vector spaces over R, thenVX,WY,EquivG(VX,WY), and/producttext
i∈IInvStabG(yi)(VX,W)are also vector spaces over RandΦandΨareR-linear isomorphisms.
Proof.To prove Theorem 1, we shall prove that the maps ΦandΨare well-defined and these maps are
the inverse maps of each other, i.e., Ψ◦Φis the identity on EquivG(VX,WY)andΦ◦Ψis the identity on/producttext
i∈IInvStabG(yi)(VX,W).
We first show the well-definedness of Φ. That is, for Φ(F) = (F(·)(yi))i∈I, we shall prove that
F(·)(yi):VX→WisStabG(yi)-invariant for any i∈I. To do so, we check that for σ∈StabG(yi)
andf∈VX,F(σ·f)(yi) =F(f)(yi). LetFbe aG-equivariant map from VXtoWYandσbe an element
inStabG(yi). This implies σ−1·yi=yi, hence the inverse σ−1is also in StabG(yi). Then, by G-equivariance
ofF, we have
F(σ·f)(yi) = (σ·F)(f)(yi) =F(f)(σ−1·yi) =F(f)(yi).
Hence, the map F(·)(yi)isStabG(yi)-invariant. This implies that the map
Φ: EquivG(VX,WY)→/productdisplay
i∈IInvStabG(yi)(VX,W);F∝⇕⊣√∫⊔≀→(F(·)(yi))i∈I
is well-defined.
Next, we prove that the map Ψis well-defined. The well-definedness of Ψcan be rephrased that the
image Ψ((/tildewideFi)i∈I)(f)(y) =/tildewideFi(τ·f)forτ∈Gsuch thaty=τ−1·yiof(/tildewideFi)i∈I∈/producttext
i∈IInvStabG(yi)(VX,W)
is independent of the choice of τ∈Gand isG-equivariant. We first notice that such a τexists since, from
theG-orbit decomposition Y=/unionsqtext
i∈IOyi, there is an orbit representative yisuch thaty∈Oyi, and thus
y=τ−1·yiforsomeτ∈G. Ifτ′∈Gisanotherchoicesothat y=τ′−1·yi, thenwehave τ′−1·yi=y=τ−1·yi.
This implies that ττ′−1stabilizesyi. Thus, we can represent τ′=στfor someσ∈StabG(yi). The value of
/tildewideFiatτ′·fbecomes
/tildewideFi(τ′·f) =/tildewideFi((στ)·f)) =/tildewideFi(σ·(τ·f)) =/tildewideFi(τ·f).
Here, the last equality is deduced from StabG(yi)-invariance of /tildewideFi. This implies that the definition of the
value Ψ(/tildewideFi)(f)(y)is independent of the choice of τ∈Gsatisfyingyi=τ·y.
6Published in Transactions on Machine Learning Research (09/2024)
We setF= Ψ((/tildewideFi)i∈I). To show G-equivariance of F, it is sufficient to check that for any σ∈G, any
f∈VX, and anyy∈Y,σ·F(f)(y) =F(σ·f)(y)holds. Lety∈Oyiandy=τ−1·yifor someτ∈G. Then,
becauseσ−1·y= (σ−1τ−1)·yi∈Oyi,
(LHS) =σ·F(f)(y) =F(f)(σ−1·y) =F(f)((σ−1τ−1)·yi)
=F(f)((τσ)−1·yi) =/tildewideFi((τσ)·f).
Here, the last equality follows from the definition of the map of Ψin equation (8). On the other hand, we
have
(RHS) =F(σ·f)(y) =F(σ·f)(τ−1·yi) =/tildewideFi(τ·(σ·f)) =/tildewideFi((τσ)·f).
Here, the third equality also follows from the definition of the map of Ψin equation (8). Therefore, F
satisfiesσ·F(f)(y) =F(σ·f)(y)for anyσ∈G, anyf∈VX, and anyy∈Y. Hence,FisG-equivariant.
Finally, we show that the map Ψis the inverse map of the map Φand vice versa, i.e., both Ψ◦ΦandΦ◦Ψ
are identities. Let Fbe a map in EquivG(VX,WY). Then, the image of the map Φcan be written as (/tildewideFi)i∈I
such that/tildewideFi(f) =F(f)(yi). Then, the map Ψtakes (/tildewideFi)i∈ItoF′such thatF′(f)(y) =/tildewideFi(τ·f) =F(τ·f)(yi)
fory∈Yandτ∈Gsuch thaty=τ−1·yi. BecauseFisG-equivariant, we have
F(f)(y) =F(f)(τ−1·yi) = (τ·F)(f)(yi)
=F(τ·f)(yi) =F′(f)(y) = Ψ◦Φ(F(f))(y).
Hence,F=F′= (Ψ◦Φ)(F)holds for any F∈EquivG(VX,WY). In particular, Ψ◦Φis the identity.
Conversely, (/tildewideFi)i∈I∈/producttext
i∈IInvStabG(yi)(VX,W)is given. Let F= Ψ((/tildewideFi)i∈I). Then, the image of Fby
Φbecomes Φ(F)(f) = (F(f)(yi))i∈I. Now, fori0∈I,
F(f)(yi0) = Ψ((/tildewideFi)i∈I)(f)(yi0) =/tildewideFi0(τ·f)
for aτ∈Gsatisfyingyi0=τ·yi0. In particular, τis in StabG(yi0). By StabG(yi0)-invariance of /tildewideFi0,
/tildewideFi0(τ·f) =/tildewideFi0(f)holds. Thus, we have
(Φ◦Ψ)((/tildewideFj)j∈I)(f) = Φ(Ψ((/tildewideFj)j∈I)(f)) ={Ψ((/tildewideFj)j∈I)(f)(yi)}i∈I
= (/tildewideFi(f))i∈I= (/tildewideFi)i∈I(f).
Therefore, the composition Φ◦Ψis the identity.
Finally, we justify the final statement of the theorem in the case VandWare real vector spaces. Under
this assumption, it is fairly known that VX,WY,Map(VX,WY)as well as Map(VX,W)are real vector
spaces. Therefore,/producttext
i∈IMap(VX,W)is also a vector space as the Cartesian product of vector spaces. By
an abuse of notation, consider Φas a map between Map(VX,WY)and/producttext
i∈IMap(VX,W), and let us show
that it is linear.
LetF,F′∈Map(VX,WX)andλ∈R. It is clear that for all f∈VX,(F+λF′)(f) =F(f) +λF′(f)and
thus for all y∈Y,(F+λF′)(f)(y) =F(f)(y) +λF′(f)(y). Hence,
Φ(F+λF′) = ((F+λF′)(·)(yi))i∈I
= (F(·)(yi) +λF′(·)(yi))i∈I
= (F(·)(yi))i∈I+λ(F′(·)(yi))i∈I,
Which justifies the linearity of Φ : Map(VX,WY)→/producttext
i∈IMap(VX,W).
Next, we shall show that EquivG(VX,WY)is a linear subspace of Map(VX,WY). In order to do that,
we first have to show that the action of F′onMap(VX,WY)is itself linear. Let F,F′∈Map(VX,WY)and
7Published in Transactions on Machine Learning Research (09/2024)
λ∈R, for anyf∈VX, anyy∈Y, and anyσ∈G, we have :
σ·(F+λF′)(f)(y) = (F(f) +λF′(f))(σ−1·y)
=F(f)(σ−1·y) +λF′(f)(σ−1·y)
=σ·F(f)(y) +λσ·F′(f)(y),
Which yields σ·(F+λF′) =σ·F+λσ·F′. That being done, we go back on showing that EquivG(VX,WY)
is a linear subspace of Map(VX,WY). LetF,F′∈EquivG(VX,WY)andλ∈R, for anyσ∈G, we have
σ·(F+λF′)(f) = (σ·F+λσ·F′)(f)
=σ·F(f) +λσ·F′(f)
=F(σ·f) +λF′(σ·f)
= (F+λF′)(σ·f),
wherethethirdequalityisduetothe G-equivarianceproperty. Hence,sincethenullmapisclearlyequivariant
too, this makes EquivG(VX,WY)a real vector subspace of Map(VX,WY).
Consequently, theimageof EquivG(VX,WY)byΨmustbeavectorsubspaceof/producttext
i∈IMap(VX,W). Since
we have proven earlier that Φ : EquivG(VX,WY)→/producttext
i∈IInvStabG(yi)(VX,W)is a bijection, in particular, it
is a surjection. Hence, the aforementioned image is/producttext
i∈IInvStabG(yi)(VX,W), therefore it is a vector space.
To conclude, notice that we have shown that Φ : EquivG(VX,WY)→/producttext
i∈IInvStabG(yi)(VX,W)is a
linear bijection between vector spaces. Thus, by a fairly known fact from linear algebra, its inverse Ψmust
be linear too.
Theorem 1 implies that any G-equivariant map Fis determined by the StabG(yi)-invariant maps F(·)(yi)
fori∈I. In other words, the parts F(·)(y)fory̸∈{yi|i∈I}are redundant to construct G-equivariant
mapF.
This theorem is quite elementary, in the sense that its statement as well as its proof, only requires
basic knowledge in group theory. However, the idea of linking G-equivariant maps to invariant maps on
some subgroups of Gis not new and has more profound implications. For instance, it is central in group
representation theory as it relates to the so-called Frobenius reciprocity theorem (Curtis & Reiner, 1966),
abouttherelationshipsbetweentherepresentationof Gandtherepresentationsofitssubgroups. Inaddition,
the result from Theorem 1 can be recovered from some other more general theorems in more advanced topics,
such as Theorem 1.1.4 in Cap & Slovák (2009), about homogeneous vector bundles.
3.3 Construction of universal approximators
As an application of Theorem 1, we construct universal approximators for continuous G-equivariant maps.
In this section, we assume that VandWare normed vector spaces over Rwhose norms are ∥·∥Vand∥·∥W
respectively. Then, we define norms on VXandWYby the supremum norms
∥f∥VX= sup
x∈X∥f(x)∥V. (9)
forf∈VXand similarly for g∈WY. LetK⊂VXbe a compact subset. We assume that Kis stable by
the action of G, i.e.,σ·f∈Kfor anyσ∈Gandf∈K. We denote Equivcont
G(K,WY)(resp. Invcont
H(K,W )
for a subgroup H) the subset of continuous maps in EquivG(K,WY)(resp. InvH(K,W )):
Equivcont
G(K,WY) ={F∈EquivG(K,WY)|continuous}
Invcont
H(K,W ) ={/tildewideF∈InvH(K,W )|continuous}
For a subgroup H⊂G, letHH-invbe a hypothesis set in Invcont
H(K,W )consisting of some deep neural
networks. We assume that any maps in Invcont
H(K,W )can be approximated by an element in HH-inv,i.e.,
8Published in Transactions on Machine Learning Research (09/2024)
for any/tildewideF∈Invcont
H(K,W )and anyε>0, there is an element/hatwide/tildewideF∈HH-invsuch that
sup
f∈K∥/hatwide/tildewideF(f)−/tildewideF(f)∥W<ε.
We define a hypothesis set HG-equivinEquivG(K,WY)by aggregating the W-valued hypothesis sets
HStabG(yi)-invas follows:
HG-equiv = Ψ/parenleftigg/productdisplay
i∈IHStabG(yi)-inv/parenrightigg
, (10)
where Ψis the map defined in Theorem 1. Then, the set HG-equivis included in Equivcont
G(K,WY). Indeed,
by Theorem 1, this set is included in EquivG(K,WY)and we can show that F= (Fy)y∈Y:K→WYis
continuous if and only if Fy:K→Wis continuous for all y∈Ybecause we consider the supremum norm
onWY.
The following theorem constructs a universal approximator for G-equivariant map from some invariant
universal approximators:
Theorem 2. Any element in Equivcont
G(K,WY)can be approximated by an element of HG-equivdefined
in equation (10). More precisely, for any F∈Equivcont
G(K,WY)and anyε>0, there exists an element in
/hatwideF∈HG-equivsuch that
sup
f∈K∥/hatwideF(f)−F(f)∥WY≤ε.
Proof.By the definition of the hypothesis set HStabG(yi)-inv, for anyε>0and anyi∈I, there is an element
/hatwide/tildewideFi∈H StabG(yi)-invsuch that
sup
f∈K∥/hatwide/tildewideFi(f)−/tildewideFi(f)∥W≤ε (11)
We set/hatwideF= Ψ((/hatwide/tildewideFi)i∈I). The mapFcan also be written as F= Ψ((/tildewideFi)i∈I). Then, the norm of the difference
between/hatwideFandFforf∈Katy∈Ybecomes as follows: If y∈Oyiandy=σ−1·yiforσ∈G, then
∥/hatwideF(f)(y)−F(f)(y)∥W=∥/hatwideF(f)(σ−1·yi)−F(f)(σ−1·yi)∥W
=∥/hatwideF(σ·f)(yi)−F(σ·f)(yi)∥W
=∥/hatwide/tildewideFi(σ·f)−/tildewideFi(σ·f)∥W
≤sup
f∈K∥/hatwide/tildewideFi(σ·f)−/tildewideFi(σ·f)∥W
≤sup
f∈K∥/hatwide/tildewideFi(f)−/tildewideFi(f)∥W≤ε
The second inequality follows from the stability of Kby the action of G, and the last inequality follows from
inequality equation (11).
Hence, the difference between /hatwideFandFatf∈Kcan be written as
∥/hatwideF(f)−F(f)∥WY= sup
y∈Y∥/hatwideF(f)(y)−F(f)(y)∥W≤ε. (12)
This completes the proof because f∈Kis arbitrary in the above argument.
The conclusion of this Theorem relies on the assumption that some universal classes HH-invof invariant
maps do exist for the subgroups of G. We argue that this assumption is always fulfilled. Indeed, given any
9Published in Transactions on Machine Learning Research (09/2024)
universal class with a priorino symmetry, for instance, usual multi-layers perceptrons, one can always turn
it into a universal class of invariant maps by simple group averaging, as remarked by Yarotsky (2022).
However, such straightforward constructions are clearly unrealistic for large groups like Sn. The interest
of Theorem 2 is to enable reducing the study of efficient G-equivariant universal architecture to the study
ofStabG(yi)-invariant ones. It offers an alternative to the equivariant or invariant “symmetrization-based”
constructions in Yarotsky (2022, Section 2.1) by group averaging.
3.3.1 On the structural characteristics of universal equivariant networks arising from Theorem 2
On theG-action between the hidden layers. Several works have studied the design of multi-layer
universal equivariant architectures, including Zaheer et al. (2017), Maron et al. (2019b), Segol & Lipman
(2019), and Ravanbakhsh (2020). How do a universal architecture arising from our Theorem 2, differ from
the already existing aforementioned ones?
The main difference resides in the nature of the G-action in the hidden layers. In fact the way our
architecture is built involves a new G-action, noted “∗”, which is different from the original action “ ·”. We
provide below an explanation of this phenomenon in the form of a remark. The rigorous demonstrations are
left to Appendix B and require some background in group Representation Theory.
Remark 1. Say that we seek to design multi-layer architectures that are equivariant to some G-action
“·”. The usual strategy, as in the case of the previously mentioned works, is to focus on designing a single
layer block that is equivariant for “ ·”. Then by stacking these blocks, we obtain an equivariant multi-layer
architecture, because equivariance is preserved by composition.
Now, consider the scenario of our construction from Theorem 2. Assume that we are given some families
of multi-layers Hi-invariant maps, for the action “ ·”, where the Hi⊂Gare some subgroups that have been
identified thanks to the decomposition from Theorem 1. Then, by “aggregating” some of these Hi-invariant
maps, and applying Ψ, as is equation (10), a new multi-layer architecture /hatwideFis built. Moreover, by Theorem 1
again, the/hatwideFobtained by this procedure are guaranteed to be equivariant for the action “ ·”.
What can be said about the layer-wise structure of such /hatwideF? It turns out that the G-equivariance of the
hidden layers is no longer realized by “ ·” on both the input and the first hidden space. Instead, it involves
anotherG-action “∗” in the hidden space. If we look at the first layer φ1, which consists in “plugging-in”
the “aggregated” invariant maps to the input space (see Figure 2), its equivariance, from the input layer to
the first hidden layer of /hatwideF, will be written as φ1(σ·f) =σ∗φ1(f), as seen in Figure 1(b).
Overall, using commutative diagrams, Figure 1 highlights the differences between the group actions involved
in these architectures.
This new action “ ∗”, which is still an action of the group G, can be described as the “induced representation
of the restricted representation action on the hidden layers”. In Appendix B, a rigorous discussion about this
phenomenon and definition of this action “ ∗” are done using tools from Representation Theory.
On the number of free parameters. Another notorious advantage of multi-layer equivariant architec-
tures such as Zaheer et al. (2017); Maron et al. (2020), is that the design of equivariant blocks requires few
free parameters thanks to a phenomenon of parameters sharing enabled by the symmetry. Hence, in terms of
the total number of free parameters, there is a significant advantage in using layer-wise equivariant networks,
rather than naive fully connected ones, in order to approximate an equivariant map. Is it also the case for
our architecture?
We clarify that point in the following remark. We claim that, indeed, our equivariant architecture is more
efficient than naive fully connected networks, in terms of a number of free parameters. Proofs and rigorous
explanations are left in appendix C.
Remark 2. Although our equivariant model differs from the standard equivariant models, the number of
parameters of our models can also be less than that of a fully connected neural network. For example, let
G=Snact onX=Y={1,...,n}by permutation and V,Wbe vector spaces over R. ForSn-equivariant
mapF:Vn→Wn, we can take G-equivariant deep neural network /hatwideFapproximating Fby Theorem 2. Then,
10Published in Transactions on Machine Learning Research (09/2024)
V0 V1 V2···
V0 V1 V2···φ1
σ·⟳φ2
σ·⟳σ·
φ1 φ2
(a) Usual equivariant architecture: the action “ ·” is the same in the input space as well as in
each hidden spaces.
V0 V′
1 V′
2···
V0 V′
1 V′
2···φ1
σ·⟳φ2
σ∗⟳σ∗
φ1 φ2
(b) Equivariant architecture arising from Theorem 2: the action “ ·” on the input space is
different than the action “ ∗”, in red, on the hidden spaces.
Figure 1: Commutative diagrams for comparing between a usual equivariant multi-layer architecture, and
ours arising from Theorem 2. For each diagram, the two lines both represent the same multi-layer equivariant
architecture with the φibeing the equivariant layers between hidden spaces. Each square encapsulating a
⟳symbol is a commutative diagram that represents the equivariance of the layer φi,i.e, the fact that φi
commutes with some action of the group. This means that, within these squares, from the top left to the
bottom right the two possible paths are equal.
the first hidden layer of /hatwideFcan be written as the form Vn2
1for a vector space V1. The linear map Vn→Vn2
1
is equivariant for permutation action on Vnand the direct sum of “induced representations of restriction
representations to the stabilizer subgroup” on Vn2
1. By the argument on irreducible representations, the
number of parameters of the linear map between the input layer Vnand the first hidden layer Vn2
1becomes
5 dimVdimV1. Because the number of parameters of the linear map Vn→Vn2
1for the fully connected model
isn3dimVdimV1, our model can be constructed by much fewer parameters than the fully connected model.
In Appendix C, precise statement and proof of this fact are concluded using Schur’s Lemma and Young’s
diagrams.
4 Some specific cases
In this section, we propose an application of Theorems 1 and 2 to a few examples.
4.1 Permutation equivariant maps
As a first example, we consider Sn-equivariant maps. Recall the context from Section 3.1 where X=Y=
{1,2,...,n}andV,Ware generic sets. The set Yonly has a single orbit by the Snaction:O1=Sn·1,
which can be written as
O1={(1i)·1|i= 1,...,n}={1,2,...,n},
where (i j)∈Snis the transposition between iandj.
Hence, we recover Proposition 1 from Theorem 1. Namely, that an Sn-equivariant map F:VX→WXis
uniquely determined by its Stab(1)-invariant component /tildewideF1:VX→Wvia
F(f) = (/tildewideF1((1i)·f))n
i=1.
Bythisargument,anyanalysisof Sn-equivarianceofthemap Fcanbereducedtoanalyzeonlyone StabSn(1)-
invariant map /tildewideF1:VX→W(Figure 2).
11Published in Transactions on Machine Learning Research (09/2024)
Figure 2: The case of G=SnandX=Y={1,2,...,n}. TheSn-orbit of 1∈Yis the whole set Y, thus
Sn-equivariant map Fis determined by only one StabSn(1)-invariant map /tildewideF1.
In this case, as a hypothesis set HStabSn(1)ofStabSn(1)-invariant deep neural networks, we can use the
universal approximators defined in Maron et al. (2019b). By this HStabSn(1)and Theorem 2, we obtain
universal approximators for Sn-equivariant maps.
4.2 Finite groups
In this subsection, Gis any finite group. By Proposition 3 in Appendix A, Gis a subgroup of Snfor some
n. Hence,Gacts onX=Y={1,2,...,n}via permutation action of Sn.
TheG-orbit decomposition of Yis generally nontrivial. Let Y=/unionsqtextk
i=1Oyibe theG-orbit decomposition
ofYandy1,y2,...,ylbe elements of Gsuch thatOyi=Gyifori= 1,2,...,k. We represent Oyias
Oyi={yi1,yi2,...,yimi}, and choose σij∈Gsuch thatyij=σ−1
ij·yi. Then, we remark that/summationtextk
i=1mi=n.
We consider a G-equivariant map FfromVXtoWYfor vector spaces V,WoverR. By Theorem 1, F
can be written by
F(f) = (/tildewideFi(σij·f))i=1,...,k
j=1,...,mi, (13)
for anyf∈VX, where/tildewideFi(f) =F(f)(yi)isStabG(yi)-invariant. We remark that this map Fis determined
by the StabG(yi)-invariant map /tildewideFifromVXtoWfori= 1,2,...,l.
For general finite group G, as a hypothesis space HStabG(yi)-inv, we choose the set of models by using deep
neural networks with tensors introduced in Maron et al. (2019b), which is guaranteed to approximate any
StabG(yi)-invariant continuous maps. Keriven & Peyré (2019) also construct a universal approximator for
graph neural networks.
4.3 The special orthogonal group SO2(R)
As for Example 3 (iii), we consider the rotation actions on the set of the ideal images. The index sets are
X=Y=R2. Weconsider Map(X,R3) = (R3)XasthesetofidealimageswiththevaluesforidealRGBcolor
channelsV=R3. LetWbe a set of output labels. Then, the 2-dimensional special orthogonal group SO2(R)
acts onX=Y=R2. We consider an SO2(R)-equivariant (i.e., rotation equivariant) map F:VR2→WR2.
Then, the orbit Oyi= SO 2(R)·yiofyi= (i,0)∈R2fori∈I= [0,∞)isOyi={(a,b)∈R2|a2+b2=i2},
12Published in Transactions on Machine Learning Research (09/2024)
Figure 3: Rotation equivariant map Fis determined the maps {/tildewideFi}i∈IwhereIis the nonnegative real line
R≥0. We can recover Ffrom{/tildewideFi}i∈Iby “rotating”{/tildewideFi(g·)}i∈I,g∈SO2(R)
.
andY=/unionsqtext
i∈IOyiholds. On the other hand, the stabilizer subgroup of SO2(R)foryi= (i,0)is
Stab SO2(R)(yi) =/braceleftigg/braceleftig/parenleftig
1 0
0 1/parenrightig/bracerightig
ifi>0,
SO2(R)ifi= 0.
Then, by Theorem 1, the SO2(R)-equivariant map Fcan be written by
F(f) =/parenleftbigg
(/tildewideFi(g·f))i∈(0,∞)
g∈SO2(R),/tildewideF0(f)/parenrightbigg
.
This means that SO2(R)-equivariant map Fis determined by the maps /tildewideFi: ([0,1]3)R2→Wfori∈[0,∞)
as Figure 3. Unfortunately, /tildewideFifori >0does not have any invariance because the stabilizer subgroup of
SO2(R)for(i,0)is/braceleftbig/parenleftbig1 0
0 1/parenrightbig/bracerightbig
.
We assume that X,Yare the disk D={(x,y)|x2+y2≤R}of radiusR> 0. We can apply Theorem 2.
Then, if there is a universal approximators (/hatwide/tildewideFi)i∈Iof(/tildewideFi)i∈I,Ψ((/hatwide/tildewideFi)i∈I)approximates the map F=
Ψ((/tildewideFi)i∈I).
In this case, as a hypothesis set HStab SO2(R)(yi)-invofStab SO2(R)(yi)-invariant deep neural networks, we
can use the universal approximators defined in Yarotsky (2022, Definition 3.2). By this HStab SO2(R)(yi)-inv
and Theorem 2, we obtain universal approximators for SO2(R)-equivariant maps.
4.4 The translation group TforR2
It is known that the convolutional layers of convolution neural networks are translation equivariant. In this
subsection, we consider an example for X=Y=R2and the actions of the translation group TforR2on
VXandWY. The translation group Tis defined by
T={Tv|v∈R2},
where each translation Tvis defined by Tv(x) =x+v. Forf∈VX, the action of Tvonfis defined by
(Tv·f)(x) =f(T−1
v(x)) =f(x−v).
We consider an T-equivariant (i.e., translation equivariant) map F:VR2→WR2. In this case, for (0,0)∈
Y=R2, the orbitO(0,0)=T·(0,0)is same asY=R2andStab T((0,0)) ={Id}. Then, by Theorem 1, the
T-equivariant map Fcan be written by
F(f) = (/tildewideF(0,0)(Tv·f))v∈Y.
In particular, T-equivariant map Fis determined by one {Id}-invariant map (i.e., usual map without in-
variance)/tildewideF(0,0):VX→W. In this situation, by Theorem 2, if there is a universal approximators/hatwide/tildewideF(0,0)of
/tildewideF(0,0),Ψ(/hatwide/tildewideF(0,0))approximates the map F= Ψ(/tildewideF(0,0)).
13Published in Transactions on Machine Learning Research (09/2024)
5 Application to the numbers of parameters
5.1 A relation of the numbers of parameters for invariant/equivariant maps
In the case of finite group G, we investigate the number of parameters from the point of view of approxi-
mations. Let Gbe a group and X,Ybe two finite G-sets. LetY=/unionsqtext
i∈IOyibe theG-orbit decomposition
ofYandK⊂VXbe a compact subset which is stable by the G-action. For F∈Equivcont
G(K,WY), we
defineceq
G(ε;F)to be the minimum number of the parameters of G-equivariant deep neural networks that
can approximate Fin accuracy ε>0. More precisely, there exists a G-equivariant deep neural network /hatwideFof
which the number of parameters is equal to ceq
G(ε;F)andsupf∈K∥/hatwideF(f)−F(f)∥WY≤ε, and there is no such
G-equivariant deep neural network of which the number of parameters is less than ceq
G(ε;F). Similarly, for
subgroupHofGand/tildewideF∈Invcont
H(K,W ), we define cinv
H(ε;/tildewideF)to be the minimum number of the parameters
ofH-invariant deep neural networks that can approximate /tildewideFin accuracy ε > 0. The existences of such
cinv
H(ε;/tildewideF)andceq
G(ε;F)are guaranteed by the results for universal approximation theorems such as Maron
et al. (2019b) or Theorem 2 in this volume.
ForF∈Equivcont
G(K,WY),/tildewideFi∈InvStabG(yi)(VX,W)is defined by (/tildewideFi)i∈I= Φ(F)in the sense of
Theorem 1. Then, we have the following:
Theorem 3. For anyε>0, the following holds:
max
i∈Icinv
StabG(yi)(ε;/tildewideFi)≤ceq
G(ε;F)≤/summationdisplay
i∈Icinv
StabG(yi)(ε;/tildewideFi). (14)
Proof.We first show the left inequality. Let /tildewideFi0∈InvStabG(yi0)(VX,W)for ani0∈I. We can find a
G-equivariant deep neural network /hatwideFsuch that∥/hatwideF(f)−F(f)∥WY≤εfor anyf∈Kand the number of
parameters of /hatwideFis equal toceq
G(ε;F). The existence of /hatwideFis guaranteed by Theorem 2. For this F,/hatwideF(·)(yi0)
satisfies∥/hatwideF(f)(yi0)−/tildewideFi0(f)∥W≤εand the number of parameters of the deep neural network /hatwideF(·)(yi0)
is less than or equal to ceq
G(ε;F). Thus,/tildewideFi0can be approximated in accuracy εby a StabG(yi0)-invariant
deep neural network of which the number of parameters is less than or equal to ceq
G(ε;F). This implies
cinv
StabG(yi0)(ε;/tildewideFi0)≤ceq
G(ε;F). Becausei0∈Iis arbitrary, we have the first inequality of equation (14).
We next show the second inequality of equation (14). For any i∈I, there is a deep neural network/hatwide/tildewideFi
such that∥/hatwide/tildewideFi(f)−/tildewideFi(f)∥W≤εforf∈Kand the number of parameters of this is cinv
StabG(yi)(ε;/tildewideFi). We set
/hatwideF= Φ((/hatwide/tildewideFi)i∈I). Then, by Theorem 1 and inequality equation (12), we have
sup
f∈K∥/hatwideF(f)−F(f)∥WY= sup
f∈Ksup
y∈Y∥/hatwideF(f)(y)−F(f)(y)∥W
= sup
f∈K/parenleftbigg
sup
i∈I/parenleftbigg
sup
σ∈G∥/hatwideF(f)(σ−1·yi)−F(f)(σ−1·yi)∥W/parenrightbigg/parenrightbigg
= sup
f∈K/parenleftbigg
sup
i∈I/parenleftbigg
sup
σ∈G∥/hatwideF(σ·f)(yi)−F(σ·f)(yi)∥W/parenrightbigg/parenrightbigg
≤sup
i∈I/parenleftigg
sup
f∈K∥/hatwideF(f)(yi)−F(f)(yi)∥W/parenrightigg
= sup
i∈I/parenleftigg
sup
f∈K∥/hatwide/tildewideFi(f)−/tildewideFi(f)∥W/parenrightigg
≤ε. (15)
Because the number of parameters of /hatwideF(f)is less than or equal to/summationtext
i∈Icinv
StabG(yi)(ε;/tildewideFi), any elements
inEquivcont
G(K,WY)can be approximated in accuracy εby aG-equivariant deep neural network of
which the number of parameters is less than or equal to/summationtext
i∈Icinv
StabG(yi)(ε;/tildewideFi). This implies ceq
G(ε;F)≤
/summationtext
i∈Icinv
StabG(yi)(ε;/tildewideFi). This is the second inequality of equation (14).
14Published in Transactions on Machine Learning Research (09/2024)
5.2 Approximation rate for G-equivariant deep neural networks
Inthecaseof G=Sn,X={1,2,...,n},V=W=R, andK= [0,1]n⊂Vn, Sannaietal.(2021, Theorem4)
showed an approximation rate of Sn-invariant ReLU deep neural networks for elements in a Hölder space.
In this subsection, we generalize this result to G-invariant maps for a finite group G⊂Snand show an
approximation rate for G-equivariant ReLU deep neural networks.
Letαbe a positive constant. The Hölder space of order αis the space of continuous functions fsuch
that all the partial derivatives of fup to order⌊α⌋exist and the partial derivatives of order ⌊α⌋areα−⌊α⌋
Hölder-smooth. This space is usually denoted as Cα,α−⌊α⌋, but we denote it as Cαhere for convenience. For
f:K→R, the Hölder norm is defined by
∥f∥Cα:= max
β:|β|<⌊α⌋∥∂βf(x)∥L∞(K)+ max
β:|β|=⌊α⌋sup
x,x′∈K,x̸=x′|∂βf(x)−∂βf(x′)|
∥x−x′∥α−⌊α⌋
∞.
ForB > 0, aB-radius ball in the Hölder space on Kis defined asCα
B={f∈Cα|∥f∥Cα≤B}.
The following is a generalization of Sannai et al. (2021, Theorem 4) to finite group G⊂Sn.
Theorem 4. LetG⊂Snbe a finite group acting on the sets X=Y={1,2,...,n}andY=/unionsqtextk
i=1Oibe
theG-orbit decomposition, and K= [0,1]n⊂Rn. For anyε >0, letHG-invbe a set of G-invariant ReLU
deep neural networks from KtoRwhich has at most O(log(1/ε))layers and O(ε−n/αlog(1/ε))non-zero
parameters. Then, for any G-invariant function /tildewideF∈Cα
B, there exists/hatwide/tildewideF∈HG-invsuch that
sup
f∈K|/hatwide/tildewideF(f)−/tildewideF(f)|≤ε.
Proof.To generalize Sannai et al. (2021, Theorem 4), it is sufficient to define the fundamental domain ∆G
for finite group G⊂Snand a sorting map Sort: [0,1]n→∆G, and to show that Sortcan be realized by
ReLU deep neural networks. The remaining part of the proof is similar to the proofs of Sannai et al. (2021,
Proposition 9, Theorem 4)
Let{1,2,...,n}=/unionsqtextk
i=1Oibe aG-orbit decomposition and Ij={ij
1,...,ij
mj}satisfyingij
ℓ< ij
ℓ′for
ℓ<ℓ′.
Then, a fundamental domain ∆GforGis given as
∆G={(x1,...,xn)⊤∈[0,1]n|xij
ℓ≥xij
ℓ′ifℓ<ℓ′forj= 1,2,...,k}.
We consider the reordered index (i1
1,i1
2,...,i1
m1,i2
1,...,i2
m2,...,ik
1,...,ik
mk). We define the permutation
σ0from (1,2,...,n )to this index as
σ0=/parenleftbigg1 2... n
i1
1i1
2... ik
mk/parenrightbigg
.
Then, byσ−1
0,(x1,x2,...xn)⊤∈[0,1]nis permutated as
σ−1
0·(x1,x2,...,xn)⊤= (xσ0(1),xσ0(2),...,xσ0(n))⊤= (xi1
1,xi1
2,...,xikmk)⊤.
We remark that this permutation can be realized by a linear map. On the other hand, we define
SortG: [0,1]n→[0,1]nby
SortG(x1,...,xn) =
(max1
1(x1,...,xn),max1
2(x1,...,xn),..., maxk
mk(x1,...,xn)).
Here, maxj
ℓ(x1,...,xn)is theℓ-th largest value in {xpj−1+1,xpj−1+2...,xpj}, wherepj=/summationtextj−1
i=1mi. As
proven in Sannai et al. (2021, Proposition 8), we can show that the map SortGis represented by deep
15Published in Transactions on Machine Learning Research (09/2024)
neural networks with ReLU activation functions. We define /tildewidestSortG=σ0◦SortG◦σ−1
0. Then, the image
/tildewidestSortG(x1,...,xn)is in the fundamental domain ∆Gand/tildewidestSortGcan also be realized by a ReLU deep neural
network.
By applying Sannai et al. (2021, Proposition 9 and Proof of Theorem 4) with this /tildewidestSortG, we can complete
the proof.
As mentioned in Sannai et al. (2021) for G=Sn, also for finite group G⊂Sn, the approximation rate in
Theorem 4 is the optimal rate without invariance obtained by Yarotsky (2022, Theorem 1). Thus, we show
thatG-invariant deep neural networks can also achieve the optimal approximation rate even with invariance.
By combining Theorem 4 and similar argument in the proof of Theorem 3, we can show the approximation
rate forG-equivariant maps:
Corollary 1. LetG⊂Snbe a finite group acting on the sets X=Y={1,2,...,n}andY=/unionsqtextk
i=1Oibe the
G-orbit decomposition. Let V=W=RandK= [0,1]n⊂Rn. For anyε>0, letHG−equivbe a set of G-
equivariant ReLU deep neural networks from KtoRnwith at most O(log(1/ε))layers andO(kε−n/αlog(1/ε))
non-zero parameters. Then, for any G-equivariant map F∈(Cα
B)n, there exists /hatwideF∈HG−equivsuch that
sup
f∈K∥/hatwideF(f)−F(f)∥Rn≤ε.
Proof.LetF∈(Cα
B)nbe aG-equivariant map and (/tildewideFi)i∈I= Φ(F). Then, by Theorem 1, /tildewideFiis aStabG(yi)-
invariant continuous function on K. By Theorem 4, there is an element/hatwide/tildewideFiinHStabG(yi)-invsuch that
sup
f∈K|/hatwide/tildewideFi(f)−/tildewideFi(f)|≤ε.
We set/hatwideF= Ψ((/hatwide/tildewideFi)i∈I). Then, by the similar argument in inequality equation (15), we have
sup
f∈K∥/hatwideF(f)−F(f)∥Rn≤sup
i∈Isup
f∈K|/hatwide/tildewideFi(f)−/tildewideFi(f)|≤ε.
Now,/hatwideFhas at most O(log(1/ε)layers andO(kε−n/αlog(1/ε))non-zero parameters because of the assump-
tions for/hatwide/tildewideFi(i= 1,2,...,k). This concludes the proof.
6 Conclusion
In this study, we explained a fundamental relationship between equivariant maps and invariant maps for
stabilizer subgroups. The relation allows any equivariant tasks for a group to be reduced to some invariant
tasks for some stabilizer subgroups. As an application of this relation, we proposed the construction of
universal approximators for equivariant continuous maps by using some invariant deep neural networks.
Although our model is different from the other standard models introduced by Zaheer et al. (2017) or
Maronetal.(2019b), thenumberofparametersofourmodelsisfewerthanfullyconnectedmodels. Moreover,
by the relation, we showed inequalities of the number of parameters of invariant or equivariant deep neural
networks to approximate any continuous invariant/equivariant maps. We also showed an approximation rate
ofG-equivariant ReLU deep neural networks for elements of a Hölder space for finite group G.
The invariant-equivariant relation is fundamental and applicable to a wide variety of situations. We
believe that this study will lead to further research in the field of machine learning.
Acknowledgments
AT was supported in part by JSPS KAKENHI Grant No. JP20K03743, JP23H04484 and JST PRESTO
JPMJPR2123. YT was partly supported by JSPS KAKENHI Grant Numbers JP21K11763 and Grant for
16Published in Transactions on Machine Learning Research (09/2024)
Basic Science Research Projects from The Sumitomo Foundation Grant Number 200484. MC was partly
supported by the French National Research Agency in the framework of the LabEx PERSYVAL-Lab (ANR-
11-LABX-0025-01). The previous version of this work was done during MC’s visit to RIKEN AIP.
References
Vladimir I. Arnold. On functions of three variables. Proceeding of the USSR Academy of Sciences , 114:
679–681, 1957. English translation: Amer. Math. Soc. Transl., 28 (1963), pp. 51–54.
Andrew R Barron. Approximation and estimation bounds for artificial neural networks. Machine learning ,
14(1):115–133, 1994.
Ben Blum-Smith and Soledad Villar. Machine learning and invariant theory, 2023.
Michael M. Bronstein, Joan Bruna, Taco Cohen, and Petar Veličković. Geometric deep learning: Grids,
groups, graphs, geodesics, and gauges, 2021.
Joan Bruna, Wojciech Zaremba, Arthur Szlam, and Yann LeCun. Spectral networks and deep locally
connected networks on graphs. In 2nd International Conference on Learning Representations, ICLR 2014 ,
2014.
A. Cap and J. Slovák. Parabolic Geometries I . Mathematical surveys and monographs. American Mathe-
matical Society, 2009. ISBN 9780821826812. URL https://books.google.fr/books?id=NMyFAwAAQBAJ .
Taco Cohen and Max Welling. Group equivariant convolutional networks. In International conference on
machine learning , pp. 2990–2999, 2016.
Taco S. Cohen, Mario Geiger, Jonas Köhler, and Max Welling. Spherical CNNs. In International Conference
on Learning Representations , 2018. URL https://openreview.net/forum?id=Hkbd5xZRb .
Charles W Curtis and Irving Reiner. Representation theory of finite groups and associative algebras , volume
356. American Mathematical Soc., 1966.
George Cybenko. Approximation by superpositions of a sigmoidal function. Mathematics of control, signals
and systems , 2(4):303–314, 1989.
Michaël Defferrard, Xavier Bresson, and Pierre Vandergheynst. Convolutional neural networks on
graphs with fast localized spectral filtering. In D. Lee, M. Sugiyama, U. Luxburg, I. Guyon,
and R. Garnett (eds.), Advances in Neural Information Processing Systems , volume 29. Curran
Associates, Inc., 2016. URL https://proceedings.neurips.cc/paper_files/paper/2016/file/
04df4d434d481c5bb723be1b6df1ee65-Paper.pdf .
Nadav Dym and Haggai Maron. On the universality of rotation equivariant point cloud networks. In
International Conference on Learning Representations , 2021. URL https://openreview.net/forum?id=
6NFBvWlRXaG .
Carlos Esteves, Christine Allen-Blanchette, Ameesh Makadia, and Kostas Daniilidis. Learning so(3) equiv-
ariant representations with spherical cnns. International Journal of Computer Vision , 128(3):588–600,
Mar 2020. ISSN 1573-1405. doi: 10.1007/s11263-019-01220-1. URL https://doi.org/10.1007/
s11263-019-01220-1 .
Ken-Ichi Funahashi. On the approximate realization of continuous mappings by neural networks. Neural
networks , 2(3):183–192, 1989.
Robert Gens and Pedro M Domingos. Deep symmetry networks. In Advances in Neural Information Pro-
cessing Systems , pp. 2537–2545, 2014.
Boris Hanin. Universal function approximation by deep neural nets with bounded width and relu activations.
Mathematics , 7(10), 2019. ISSN 2227-7390. doi: 10.3390/math7100992. URL https://www.mdpi.com/
2227-7390/7/10/992 .
17Published in Transactions on Machine Learning Research (09/2024)
Boris Hanin and Mark Sellke. Approximating continuous functions by relu nets of minimal width. arXiv
preprint arXiv:1710.11278 , 2017.
Kurt Hornik, Maxwell Stinchcombe, and Halbert White. Multilayer feedforward networks are universal
approximators. Neural networks , 2(5):359–366, 1989.
Gordon D James. The representation theory of the symmetric groups. In Proc. Symposia in Pure Math ,
volume 47, pp. 111–126, 1987.
Nicolas Keriven and Gabriel Peyré. Universal invariant and equivariant graph neural networks. arXiv
preprint arXiv:1905.04943 , 2019.
Jinwoo Kim, Saeyoon Oh, and Seunghoon Hong. Transformers generalize deepsets and can be extended
to graphs & hypergraphs. In A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan (eds.),
Advances in Neural Information Processing Systems , 2021. URL https://openreview.net/forum?id=
scn3RYn1DYx .
Thomas N. Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. In
International Conference on Learning Representations , 2017. URL https://openreview.net/forum?id=
SJU4ayYgl .
Andrey Nikolaevich Kolmogorov. On the representation of continuous functions of several variables by
superpositions of continuous functions of a smaller number of variables. Proceeding of the USSR Academy
of Sciences , 108:176–182, 1956. English translation: Amer. Math. Soc. Transl., 17 (1961), pp. 369–373.
Imre Risi Kondor. Group theoretical methods in machine learning . Columbia University, 2008.
Věra Kůrková. Kolmogorov’s theorem and multilayer neural networks. Neural networks , 5(3):501–506, 1992.
Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. Nature, 521(7553):436–444, May 2015.
ISSN 1476-4687. doi: 10.1038/nature14539. URL https://doi.org/10.1038/nature14539 .
Derek Lim, Joshua David Robinson, Lingxiao Zhao, Tess Smidt, Suvrit Sra, Haggai Maron, and Stefanie
Jegelka. Sign and basis invariant networks for spectral graph representation learning. In The Eleventh
International Conference on Learning Representations , 2023. URL https://openreview.net/forum?id=
Q-UHqMorzil .
HaggaiMaron,HeliBen-Hamu,NadavShamir,andYaronLipman. Invariantandequivariantgraphnetworks.
InInternational Conference on Learning Representations , 2019a. URL https://openreview.net/forum?
id=Syx72jC9tm .
Haggai Maron, Ethan Fetaya, Nimrod Segol, and Yaron Lipman. On the universality of invariant networks.
Proceedings of the 36th International Conference on Machine Learning , 97, 2019b.
Haggai Maron, Or Litany, Gal Chechik, and Ethan Fetaya. On learning sets of symmetric elements. In
Hal Daumé III and Aarti Singh (eds.), Proceedings of the 37th International Conference on Machine
Learning , volume 119 of Proceedings of Machine Learning Research , pp. 6734–6744. PMLR, 13–18 Jul
2020. URL https://proceedings.mlr.press/v119/maron20a.html .
Allan Pinkus. Approximation theory of the mlp model in neural networks. Acta numerica , 8:143–195, 1999.
Charles R Qi, Hao Su, Kaichun Mo, and Leonidas J Guibas. Pointnet: Deep learning on point sets for 3d
classification and segmentation. In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition , pp. 652–660, 2017.
Siamak Ravanbakhsh. Universal equivariant multilayer perceptrons. In International Conference on Machine
Learning , pp. 7996–8006. PMLR, 2020.
18Published in Transactions on Machine Learning Research (09/2024)
Siamak Ravanbakhsh, Jeff Schneider, and Barnabás Póczos. Equivariance through parameter-sharing. In
Doina Precup and Yee Whye Teh (eds.), Proceedings of the 34th International Conference on Machine
Learning , volume 70 of Proceedings of Machine Learning Research , pp. 2892–2901. PMLR, 06–11 Aug
2017. URL https://proceedings.mlr.press/v70/ravanbakhsh17a.html .
Joseph J Rotman. An introduction to the theory of groups , volume 148. Springer Science & Business Media,
2012.
Akiyoshi Sannai, Yuuki Takai, and Matthieu Cordonnier. Universal approximations of permutation invari-
ant/equivariant functions by deep neural networks. arXiv preprint arXiv:1903.01939 , 2019.
Akiyoshi Sannai, Masaaki Imaizumi, and Makoto Kawano. Improved generalization bounds of group invari-
ant/equivariant deep networks via quotient feature spaces. In Uncertainty in Artificial Intelligence , pp.
771–780. PMLR, 2021.
Akiyoshi Sannai, Makoto Kawano, and Wataru Kumagai. Invariant and equivariant reynolds networks.
Journal of Machine Learning Research , 25(42):1–36, 2024. URL http://jmlr.org/papers/v25/22-0891.
html.
Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Hagenbuchner, and Gabriele Monfardini. The graph
neural network model. IEEE Transactions on Neural Networks , 20(1):61–80, 2008.
Nimrod Segol and Yaron Lipman. On universal equivariant set networks. In International Conference on
Learning Representations , 2019.
J.Shawe-Taylor. Buildingsymmetriesintofeedforwardnetworks. In 1989 First IEE International Conference
on Artificial Neural Networks, (Conf. Publ. No. 313) , pp. 158–162, 1989.
J. Shawe-Taylor. Symmetries and discriminability in feedforward network architectures. IEEE Transactions
on Neural Networks , 4(5):816–826, 1993. doi: 10.1109/72.248459.
Sho Sonoda and Noboru Murata. Neural network with unbounded activation functions is universal approx-
imator.Applied and Computational Harmonic Analysis , 43(2):233–268, 2017.
Puoya Tabaghi and Yusu Wang. Universal representation of permutation-invariant functions on vectors and
tensors, 2023.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz Kaiser,
and Illia Polosukhin. Attention is all you need. In I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach,
R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Advances in Neural Information Processing Systems ,
volume30.CurranAssociates,Inc.,2017. URL https://proceedings.neurips.cc/paper_files/paper/
2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf .
Soledad Villar, David W Hogg, Kate Storey-Fisher, Weichi Yao, and Ben Blum-Smith. Scalars are universal:
Equivariant machine learning, structured like classical physics. In A. Beygelzimer, Y. Dauphin, P. Liang,
and J. Wortman Vaughan (eds.), Advances in Neural Information Processing Systems , 2021. URL https:
//openreview.net/forum?id=NqYtJMX9g2t .
Soledad Villar, David W Hogg, Weichi Yao, George A Kevrekidis, and Bernhard Schölkopf. Towards fully
covariant machine learning. Transactions on Machine Learning Research , 2024. ISSN 2835-8856. URL
https://openreview.net/forum?id=gllUnpYuXg .
Edward Wagstaff, Fabian B. Fuchs, Martin Engelcke, Michael A. Osborne, and Ingmar Posner. Universal
approximation of functions on sets. Journal of Machine Learning Research , 23(151):1–56, 2022. URL
http://jmlr.org/papers/v23/21-0730.html .
Jeffrey Wood and John Shawe-Taylor. Representation theory and invariant neural networks. Discrete Applied
Mathematics , 69(1):33–60, 1996. ISSN 0166-218X. doi: https://doi.org/10.1016/0166-218X(95)00075-3.
URL https://www.sciencedirect.com/science/article/pii/0166218X95000753 .
19Published in Transactions on Machine Learning Research (09/2024)
Dmitry Yarotsky. Universal approximations of invariant maps by neural networks. Constructive Ap-
proximation , 55(1):407–474, Feb 2022. ISSN 1432-0940. doi: 10.1007/s00365-021-09546-1. URL
https://doi.org/10.1007/s00365-021-09546-1 .
Manzil Zaheer, Satwik Kottur, Siamak Ravanbakhsh, Barnabas Poczos, Ruslan R Salakhutdinov, and
Alexander J Smola. Deep sets. In Advances in neural information processing systems , pp. 3391–3401,
2017.
A Groups, actions, and orbits
Tointroduceaprecisestatementofthemaintheorem, webrieflyreviewseveralnotionsofgroups, actions, and
orbits. In general, a group is a set of transformations and an action of the group on a set is a transformation
rule. This is further explained in the following examples. For more details, we refer the readers to Rotman
(2012).
LetGbe a set with a product στ∈Gfor any elements σ,τofG. Then,Gis called a group if Gsatisfies
the following conditions:
1. (Existence of the unit) There is an element ϵ∈Gsuch thatϵσ=σϵ=xfor allσ∈G.
2. (Existence of the inverse element) For any σ∈G, there is an element σ−1∈Gsuch thatσσ−1=
σ−1σ=ϵ.
3. (Associativity) For any σ,τ,ϕ∈G,(στ)ϕ=σ(τϕ).
We callGa finite group if the number of elements of Gis finite. If the product of Gis commutative, i.e.,
for anyσ,τ∈G,στ=τσholds, then Gis called an abelian (or a commutative) group. Let Gbe a group
andHa subset of G. We callHa subgroup of GifHis a group with the same product as that of G.
Example 4. (i) The permutation group Snis one of the most important examples of finite groups:
Sn={σ:{1,...,n}→{ 1,...,n}|bijective}
and the product of σ,τ∈Snis given by the composition σ◦τas maps. The permutation group Snis also
called by the symmetric group.
(ii) The special orthogonal group SO2(R)is also an abelian group:
SO2(R) ={A∈R2×2|A⊤A=I,detA= 1}
=/braceleftbigg/parenleftbiggcosθsinθ
−sinθcosθ/parenrightbigg/vextendsingle/vextendsingle/vextendsingle/vextendsingleθ∈[0,2π)/bracerightbigg
,
whereIis the 2×2unit matrix. As seen above, the elements of SO2(R)include the rotations on R2.
Next, we review group actions on sets. Let Xbe a set. A left (resp. right) action of GonXis defined
as a mapX→X;x∝⇕⊣√∫⊔≀→σ·xforσ∈Gsatisfying the following:
1. For any x∈X,ϵ·x=x.
2. For any σ,τ∈Gandx∈X,(στ)·x=σ·(τ·x)(resp. (στ)·x=τ·(σ·x)).
Then, we say that Gacts onXfrom left (resp. right). Here, the reason why we call the latter a right action
is that the condition (στ)·x=τ·(σ·x)seems as if the condition “ x·(στ) = (x·σ)·τ”.
Example 5. (i) The permutation group Snacts on the set{1,2,...,n}from left by the permutation σ·i=
σ(i).
(ii) The special orthogonal group SO2(R)acts on R2from left by the rotation transformations.
20Published in Transactions on Machine Learning Research (09/2024)
We introduce a subgroup that is often considered in this article. Let Gbe a group acting on a set Xfrom
left. For an element x∈X, the subset StabG(x)ofGconsisting of elements stabilizing xis a subgroup of G:
StabG(x) ={σ∈G|σ·x=x}.
This group StabG(x)is called the stabilizer subgroup of Gwith respect to x.
Next, we introduce the notion of orbits by the group actions. An orbit is the set of elements obtained by
transformations of a fixed base element. Let Gbe a group acting on Xfrom left. Then, for x∈X, we define
theG-orbitOxofxas
Ox=G·x={σ·x|σ∈G}.
Then, itiseasytodemonstratethat Xcanbedecomposedintoadisjointunionofthe G-orbitsX=/unionsqtext
i∈IOxi.
We call this the G-orbit decomposition of X. We remark that the index set Imight be infinite.
Example 6. (i) We consider the permutation action of SnonX={1,2,...,n}. Then, the orbit of 1∈X
is same as X, i.e.,X=O1=Sn·1.
(ii) The orbit of X∈R2of the action of SO2(R)onX=R2is same as
OX=/braceleftigg
{(a,b)∈R2|a2+b2=∥X∥2
2}ifX̸=0,
{0} ifX=0.
When∥X∥2=r, then we can represent the orbits as O(r,0)=OX. Hence, the orbit decomposition of X=R2
is same as X=/unionsqtext
r≥0O(r,0).
LetHbe a subgroup of a finite group G. Then,Hacts onGfrom left and right by the product: For
τ∈Handσ∈G,τacts onσfrom left (resp. right) by σ∝⇕⊣√∫⊔≀→τσ(resp.σ∝⇕⊣√∫⊔≀→στ). For anyσ∈G, we define
the set
σH={στ∈G|τ∈H}.
This is nothing but the right H-orbit ofσfor the above right action of HonG. This is called the left coset
ofHwith respect to σ. Because the left coset is identical to the H-orbit for the right action of H, we can
decompose Ginto a disjoint union of the left cosets as a H-orbit decomposition:
G=/unionsqdisplay
i∈IσiH.
We refer to this as the left coset decomposition of GbyH. We define G/Has the set of the left cosets of G
byH:
G/H ={σiH|i∈I}.
The right cosets, the right coset decomposition, and the set of right cosets H\Gare also defined similarly.
Then, a relation exists between an orbit and a set of cosets:
Proposition 2. LetGbe a group acting on a set Xfrom left. For any x∈X, the map
G/StabG(x)→Ox;σStabG(x)→σ·x
is bijective. In particular, if G/StabG(x) ={σiStabG(x)|i∈I}, then the orbit of x inXcan be written by
Ox={σi·x|i∈I}.
Proof.It is easy to check well-definedness and bijectivity.
Example 7. (i) ForG=Snacting on{1,2,...,n}, theG-orbit of 1was only one, i.e.,
O1=σ·1 ={1,2,...,n}.
Hence, by Proposition 2, the following map is bijective:
Sn/StabSn(1)→{1,2,...,n};σStabSn(1)∝⇕⊣√∫⊔≀→σ(1).
21Published in Transactions on Machine Learning Research (09/2024)
(ii) ForG= SO 2(R), the orbit could be represented by O(r,0)= SO 2(R)·(r,0)forr≥0. Then, the stabilizer
subgroup of SO2(R)for(r,0)is
Stab SO2(R)((r,0)) =/braceleftigg
{I}ifr>0,
SO2(R)ifr= 0.
By Proposition 2, the map
SO2(R)/Stab SO2(R)((r,0))→O(r,0);σStab SO2(R)((r,0))∝⇕⊣√∫⊔≀→σ·(r,0)
is bijective for any r≥0. Indeed, this is compatible with Example 6 (ii).
One reason for the importance of the permutation group Snis the following proposition:
Proposition 3. Any finite group Gcan be realized as a subgroup of Snfor some positive integer n.
Proof.Letn:=|G|andG={σ1,σ2,...,σn}. For anyσ∈G,σσi=σjfor somej∈{1,2,...,n}, because
the product σσiis inG. We setτσ(i) =j. Then, we define a map τ:G→Sn;σ∝⇕⊣√∫⊔≀→τσ. This map is injective
and preserves the product, i.e., τσσ′=τστσ′. Through this map, Gcan be regarded as a subgroup of Sn.
B Theoretical explanation of Remark 1
To explain the architecture of our models, we introduce some notions of representation theory.
LetGbe a finite group, Vbe a vector space over R, and GL(V)be the group of the automorphisms
ofV. Here, an automorphism of Vis a linear bijection from VtoV. Then, a group homomorphism
ρ:G→GL(V)is called a (linear) representation of GonV. Defining a representation ρ:G→GL(V)is
equivalent to defining an action of GonVbyρ. Then, it is also known that considering a representation
ρ:G→GL(V)is equivalent to considering an R[G]-moduleV. Here, R[G]is the group algebra defined as
R[G] =/braceleftigg/summationdisplay
σ∈Gaσσ/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleaσ∈R/bracerightigg
.
Moreover, for an algebra Awith unit 1, anA-moduleMis an abelian group with summation +, endowed
with a “scalar product” by A, denoted as·, satisfying
σ·(x+x′) =σ·x+σ·x′,
(σ+τ)·x=σ·x+τ·x,
(στ)·x=σ·(τ·x),
1·x=x
forσ,τ∈Aandx,x′∈M. This means that an A-module is an analog of a vector space with the scalar
product by algebra A. When a representation ρ:G→GL(V)is given,Vcan be regarded as an R[G]-module
by the action/parenleftigg/summationdisplay
σ∈Gaσσ/parenrightigg
·v=/summationdisplay
σ∈Gaσρ(σ)(v).
for/summationtext
σ∈Gaσσ∈R[G]andv∈V.
Two representations (ρ,V),(ρ′,V′)of a groupGare called isomorphic, denoted (ρ,V)≃(ρ′,V′)if there
is a linear isomorphism Ξ:V→V′satisfying the commutativity
Ξ(ρ(σ)(v)) =ρ′(σ)(Ξ(v)) (16)
foranyσ∈Gandv∈V. Alinearmap (notnecessarily isomorphism) satisfyingcommutativityequation(16)
is called an intertwining operator. We set HomG(V,V′)as the set of intertwining operators from (ρ,V)to
(ρ′,V′).
22Published in Transactions on Machine Learning Research (09/2024)
For a subgroup H⊂Gand a representation ρ:G→GL(V), the restriction map ρ|H:H→GL(V)
is a representation of HonV. This representation is called the restricted representation of ρtoHand
denoted by ResG
H(ρ). Then, considering the restricted representation ρ|His equivalent to considering Vas
anR[H]-module.
On the other hand, for a subgroup H⊂Gand a representation ρ′:H→GL(W)ofHonW, the
coefficient extension of R[H]-moduleWtoR[G]
R[G]⊗R[H]W
is anR[G]-module. Here, the tensor product R[G]⊗R[H]Wis defined as the quotient (R[G]×W)/∼of the
direct product R[G]×W, by the equivalent relation ∼which is defined for σ,σ′∈R[G],τ∈H,w,w′∈W
andc∈Rby:
(σ+σ′,w)∼(σ,w) + (σ′,w),
(cσ,w )∼(σ,cw ),
(σ,w+w′)∼(σ,w) + (σ,w′),
(στ,w )∼(σ,ρ′(τ)(w)).
Then, the action of σ′∈Gon an element σ⊗w∈R[G]⊗R[H]Wis defined by
σ′(σ⊗w) = (σ′σ)⊗w.
We call the representation defined by this the induced representation of ρ′toGand denote by IndG
H(ρ′). We
remark that the following equality holds:
στ⊗w=σ⊗ρ′(τ)(w)
for anyσ∈G,τ∈H,w∈W. LetG=/unionsqtextm
i=1Hσibe the right coset decomposition of GbyH. Then, we
have the directed sum decomposition
R[G] =m/circleplusdisplay
i=1σ−1
iR[H]. (17)
By equation (17), we have
R[G]⊗R[H]W=/parenleftiggm/circleplusdisplay
i=1σ−1
iR[H]/parenrightigg
⊗R[H]W=m/circleplusdisplay
i=1σ−1
i⊗R[H]W≃Wm
Therefore, the dimension of R[G]⊗R[H]WismdimW= (G:H) dimW, where (G:H)is the index of Hin
G.
LetG⊂Snbe a finite group, F:Vn→Wnbe aG-equivariant continuous map, and let Y=
{1,2,...,n}=/unionsqtextm
i=1Oyibe aG-orbit decomposition with Oyi={σ−1(yi)|σ∈G}ofyi∈Y. By Theorem 1,
we have Φ(F) = (/tildewideFi)m
i=1for the StabG(yi)-invariant continuous map /tildewideFi(·) =F(·)(yi). Then, we have an
approximator/hatwide/tildewideFi:Vn→Was a StabG(yi)-invariant deep neural network of the StabG(yi)-invariant map
/tildewideFi(cf. Segol & Lipman (2019), Ravanbakhsh (2020)) and, by Theorem 2, can reconstruct an approximator
/hatwideF= Ψ((/hatwide/tildewideFi)m
i=1).
LetG=/unionsqtextℓi
j=1StabG(yi)σijbe a right coset decomposition for StabG(yi). Then, by Proposition 2, we
haveOyi={σ−1
ij(yi)|j= 1,...,ℓi}. By using this, an approximator /hatwideFofFis constructed from (/hatwide/tildewideFi)m
i=1by
/hatwideF(X) = ((/hatwide/tildewideFi(σij·X)ℓi
j=1)m
i=1 (18)
forX= (x1,...,xn)∈Vn.
23Published in Transactions on Machine Learning Research (09/2024)
We focus on a StabG(yi)-invariant approximator/hatwide/tildewideFi:Vn→W. By Segol & Lipman (2019), we assume
that/hatwide/tildewideFican be realized as a deep neural network model as
Vnφ(i)
1−→(V(i)
1)nφ(i)
2−→(V(i)
2)nφ(i)
3−→...φ(i)
d−→(V(i)
d)nφ(i)
d+1−→(V(i)
d+1)n=Wn,
whereV(i)
kis a finite dimensional vector space over Rand theφ(i)
kis a map defined by
φ(i)
k(X) = ReLU(W(i)
kX+B(i)
k)
forX∈(V(i)
k−1)n,W(i)
k∈RndimV(i)
k×ndimV(i)
k−1, and B(i)
k= (b(i)
k1,...,b(i)
kn)∈(V(i)
k)nsuch thatW(i)
k(resp.
B(i)
k) isStabG(yi)-equivariant (resp. StabG(yi)-invariant), i.e., for any X∈(V(i)
k−1)nand anyσ∈StabG(yi),
W(i)
k(σ·X) =σ·(W(i)
kX), σ·B(i)
k=B(i)
k
holds. Here, StabG(yi)⊂G⊂Snacts on (V(i)
k)nby the restriction of the permutation action.
By combining the equation (18) and this notation, the weight matrix from the input layer Vnto the first
hidden layer/circleplustextm
i=1/circleplustextℓi
j=1(V(i)
1)nof/hatwideFis
((W(i)
1σij)ℓi
j=1)m
i=1.
Then, by the action of σ∈G, we have
W(i)
kσij(σ·X) =W(i)
k((σijσ)·X)
=W(i)
k((τσij′)·X)
=W(i)
kτ(σij′·X)
=τW(i)
k(σij′·X),
wherej′∈Yandτ∈StabG(yi)are determined by σijσ=τσij′∈StabG(yi)σij′.
In particular, by the action of σ∈G, the (i,j)-th partW(i)
kσij(X)∈(V(i)
k)nis replaced by (i,j′)-th part
τW(i)
k(σij′·X)with the action of τ=σijσσ−1
ij′∈StabG(yi). From this observation, we define the action of
Gon thei-th part/circleplustextℓi
j=1(V(i)
k)nas follows: For σ∈Gandj= 1,2,...,ℓi, we define ϕ(i)
σ(j)∈{1,2,...,ℓi}
such that
σijσ=τσiϕ(i)
σ(j)∈Hiσiϕ(i)
σ(j). (19)
Then, for V(i)= (v(i)
j)ℓi
j=1∈/circleplustextℓi
j=1(V(i)
k)n⊂/circleplustextm
i=1/circleplustextℓi
j=1(V(i)
k)n, we define the action of σ∈Gby
σ∗V(i)= ((σijσσ−1
iϕ(i)
σ(j))·v(i)
ϕ(i)
σ(j))ℓi
j=1. (20)
The following theorem shows that this action ∗ofGis equivalent to the induced representation of the
restricted representation on (V(i)
k)n.
Theorem 5. For our proposed model, we define the action of Gon thek-th layer/circleplustextm
i=1/circleplustextℓi
j=1(V(i)
k)nby the
action∗defined in equation (20). Then, the representation defined by the action ∗is isomorphic to the sum
of the induced representation of the restrictions
m/circleplusdisplay
i=1IndG
StabG(yi)(ResG
StabG(yi)((V(i)
k)n)).
Moreover, for any k≥1, any affine maps from (k−1)-th layer to k-th layer are G-equivariant for the action
∗. (Only for the input layer, the action is the restriction of the permutation action to G.)
24Published in Transactions on Machine Learning Research (09/2024)
Proof.We setHi= StabG(yi)andG=/unionsqtextℓi
j=1Hiσij. Fori-th part, the induced representation of the
restricted representation of (V(i)
k)ncan be regarded as the R[G]-module
R[G]⊗R[Hi](V(i)
k)n=ℓi/circleplusdisplay
j=1σ−1
ij⊗(V(i)
k)n.
Then, by the action of σ, an element σ−1
ij⊗v∈/circleplustextℓi
j=1σ−1
ij⊗(V(i)
k)nis changed as
σ·(σ−1
ij⊗v) = (σσ−1
ij)⊗v= (σijσ−1)−1⊗v
= (τσij′)−1⊗v=σ−1
ij′τ−1⊗v=σij′⊗(τ−1·v), (21)
wherej′∈Yandτ∈Hiare determined by σijσ−1=τσij′∈Hiσij′. We remark that this j′is equal to
ϕ(i)
σ−1(j)by the notation defined in equation (19). This means that by the action of σ∈G, the (i,j)-th part
σ−1
ij⊗vis replaced to the (i,ϕ(i)
σ−1(j))-partσiϕ(i)
σ−1(j)⊗(τ·v)with the action of τ=σijσ−1σ−1
iϕ(i)
σ−1(j)∈Hi.
We define the map Ξfrom thei-th part of k-th hidden layer/circleplustextℓi
j=1(V(i)
k)ntoR[G]⊗R[Hi](V(i)
k)n=
/circleplustextℓi
j=1σ−1
ij⊗(V(i)
k)nby
Ξ:V(i)= (v(i)
j)ℓi
j=1∝⇕⊣√∫⊔≀−→ℓi/summationdisplay
j=1σ−1
ij⊗v(i)
j.
By definition, this map is bijective and equivariant for the action ∗ofσ∈Gon/circleplustextℓi
j=1(V(i)
k)nby equation (20)
and on R[G]⊗R[Hi](V(i)
k)nby equation (21). Indeed, we have
Ξ(σ∗V(i)) = Ξ(((σijσσ−1
iϕ(i)
σ(j))·v(i)
ϕ(i)
σ(j))ℓi
j=1)
=ℓi/summationdisplay
j=1σ−1
ij⊗(σijσσ−1
iϕ(i)
σ(j))·v(i)
ϕ(i)
σ(j)
=ℓi/summationdisplay
j=1σ−1
ij(σijσσ−1
iϕ(i)
σ(j))⊗v(i)
ϕ(i)
σ(j)
=ℓi/summationdisplay
j=1(σσ−1
iϕ(i)
σ(j))⊗v(i)
ϕ(i)
σ(j)
=σℓi/summationdisplay
j=1σ−1
iϕ(i)
σ(j)⊗v(i)
ϕ(i)
σ(j)
=σℓi/summationdisplay
j=1σ−1
ij⊗v(i)
j=σ·Ξ(V(i)).
This means that/circleplustextℓi
j=1(V(i)
k)nandR[G]⊗R[Hi](V(i)
k)nare isomorphic as R[G]-modules.
The equivariance of affine maps is deduced by the definition of the action ∗. This concludes the proof.
C Theoretical explanation of Remark 2
In the remaining part, we argue about the number of parameters of the affine maps between layers. If a
weight matrix between two layers of neural networks is equivariant by some G-action, this can be regarded as
an intertwining operator. To count the number of parameters of intertwining operators, we need the notion
of irreducible representations. We here review these notions briefly.
25Published in Transactions on Machine Learning Research (09/2024)
For a representation (ρ,V)of groupG, we call a subspace W⊂V G-invariant subspace if ρ(σ)(W)⊂
Wfor anyσ∈G. Then, (ρ,W)is also a representation of G. This representation (ρ,W)is called a
subrepresentation of (ρ,V). When a representation (ρ,V)has no nontrivial subrepresentation, this is called
irreducible representation. This means that if (ρ,W)is a subrepresentation of the irreducible representation
(ρ,V), then (ρ,W)isequalto (ρ,{0})or(ρ,V). ByMaschke’stheorem(Curtis&Reiner,1966, (10.7),(10.8)),
any finite-dimensional representation over Rcan be factorized to the direct sum of irreducible representations
up to isomorphic. In particular, irreducible representations are “building blocks” of representations.
By Schur’s Lemma (Curtis & Reiner, 1966, (27.3)), for two irreducible representations (ρ,V),(ρ′V′),
HomG(V,V′) = 0holds if and only if (ρ,V)is not isomorphic to (ρ′,V′). Thus, by this fact with Maschke’s
theorem, anyintertwiningoperatorcanbefactorizedtothesumofintertwiningoperatorsbetweenirreducible
representations.
ForG=Sn, by the argument with Young’s diagrams, we can calculate how the irreducible representations
change by induction and restriction of them. By combining such argument and Theorem 5, we can calculate
the number of parameters of the weight matrix between the input layer and the first hidden layer as follows:
Proposition 4. ForG=Sn,Sn-equivariant continuous map F:Vn→Wncan be recovered by one
StabSn(1)-invariant map /tildewideF1asF= Ψ(/tildewideF1). Let/hatwide/hatwideF1be a universal approximator of /tildewideF1and((V(1)
k)n)n
be thek-th hiddent layer of /hatwideF= Ψ(/hatwide/hatwideF1).
Then, the number of free parameters of the affine map from the input layer Vnto the first hidden layer
((V(1)
1)n)nis5 dimVdimV(1)
1+ 2 dimV(1)
1. Moreover, the number of free parameters of the affine map
from the (k−1)-th hidden layer ((V(1)
k−1)n)nto thek-th hidden layer ((V(1)
k)n)nfork= 2,...,dis at most
21 dimV(1)
k−1dimV(1)
k+ 2 dimV(1)
k.
Proof.ForG=Sn,StabG(1)≃Sn−1holds and the left coset decomposition is Sn=/unionsqtextn
i=1StabG(1)(1i).
Thus, the first hidden layer is isomorphic to R[Sn]⊗R[Sn−1](V(1)
1)nfor a finite vector space V(1)
1.
By permutation, Snacts on the part Rnpart ofVn≃Rn⊗RV. By the argument of Young tableau
(c.f. (James, 1987, 9.2)), the permutation representation on Rnis the sum of two irreducible representations
corresponding to the partitions λ1= (n)(trivial representation) and λ2= (n−1,1)ofn. On the other hand,
the representation on R[Sn]⊗R[Sn−1](V(1)
1)n≃R[Sn]⊗R[Sn−1]Rn⊗RV(1)
1is the induced representation of the
restricted representation of the permutation representation on Rn. By the argument of Young tableau again
(c.f. (James, 1987, 9.2)), the restricted representation is the sum of irreducible representations corresponding
to the partitions λ11= (n−1),λ21= (n−1), andλ22= (n−2,1)ofn−1. Then, the induced representation
of it is the sum of irreducible representations corresponding to the partitions λ111= (n),λ112= (n−1,1),
λ211= (n),λ212= (n−1,1),λ221= (n−1,1),λ222= (n−2,2), andλ223= (n−2,1,1). LetVλbe
the irreducible representation over Rcorresponding to the partition λofn. Then, we have the irreducible
representation decomposition
R[Sn]⊗R[Sn−1]Rn≃V⊕2
(n)⊕V⊕3
(n−1,1)⊕V(n−2,2)⊕V(n−2,1,1). (22)
The permutation representation can be decomposed into
Rn≃V(n)⊕V(n−1,1).
We set HomG(V,V′)as the set of G-equivariant linear maps from an R[G]-moduleVto an R[G]-moduleV′.
Then, by Schur’s lemma ((Curtis & Reiner, 1966, (27.3))), HomSn(Vλ,Vλ′) = 0holds ifλ̸=λ′. Moreover, it
is also known that
dimRHomSn(V(n),V(n)) = dim RHomSn(V(n−1,1),V(n−1,1)) = 1 (23)
holds. Indeed, it is known that
V(n)≃{c1∈Rn|c∈R}and
V(n−1,1)≃{X∈Rn|1⊤X= 0},
26Published in Transactions on Machine Learning Research (09/2024)
where 1∈Rnis the all one vector and 1⊤is the transposition of vector 1and we can show directly that
HomSn(V(n),V(n))≃RandHomSn(V(n−1,1),V(n−1,1)) =R. More explicitly, we can show that
HomSn/parenleftbig
V(n)⊕V(n−1,1),V(n)⊕V(n−1,1)/parenrightbig
≃R/parenleftbigg1
n11⊤/parenrightbigg
⊕R/parenleftbigg
I−1
n11⊤/parenrightbigg
⊂Rn×n
whereI∈Rn×nis the unit matrix, and (1/n)11⊤(resp.I−(1/n)11⊤) is the identity on V(n)(resp.V(n−1,1))
and the zero map on V(n−1,1)(resp.V(n)). This implies that
HomSn(Vn,R[Sn]⊗R[Sn−1](V(1)
1)n)
≃HomSn(Rn⊗RV,R[Sn]⊗R[Sn−1]Rn⊗RV(1)
1)
≃HomSn(Rn,R[Sn]⊗R[Sn−1]Rn)⊗RHom(V,V(1)
1)
≃(HomSn(V(n),V⊕2
(n))⊕HomSn(V(n−1,1),V⊕3
(n−1,1)))⊗RHom(V,V(1)
1). (24)
Thus, by equation (23), the dimension of HomSn(Vn,R[Sn]⊗R[Sn−1](V(1)
1)n)is equal to 5 dimVdimV(1)
1.
Next, we consider the bias vector (Bj)n
j=1∈((V(1)
1)n)n≃R[Sn]⊗R[Sn−1](V(1)
1)n, where Bj=
(bj1,...,bjn)∈(V(1)
1)n. Then, by the action ∗ofSn,(Bj)n
j=1is invariant, i.e.,
σ∗(Bj)n
j=1= ((σ1jσσ−1
1ϕ(1)
σ(j))·Bϕ(1)
σ(j))n
j=1= (Bj)n
j=1.
holds for any σ∈Sn.
Without of loss of generality, we may assume that σ11is the unit element e. We consider σ=σ−1
1j0τσ11
forτ∈H1= StabSn(1), we haveσ1j0σ=τσ11=τ. In particular, ϕ(1)
σ(j0) = 1Thus,j0-th entry of
(σ−1
1jτ)∗(Bj)n
j=1becomes
(σ1j0σσ−1
1ϕ(1)
σ(j0))·Bϕ(1)
σ(j0)=τ·B1. (25)
Because (Bj)n
j=1is invariant by the action ∗ofSn,
Bj0=τ·B1
holds for any j0= 1,2,...,nandτ∈StabSn(1). This implies that by τ=e,Bj=B1holds for any
j= 1,2,...,n. Furthermore, τ·B1=B1holds for any τ∈StabSn(1). Because the orbit decomposition of
{1,2,...,n}byH1= StabSn(1)is
{1,2,...,n}={1}⊔{ 2,3,...,n},
B1= (b11,b12,...,b1n)satisfies that b12=···=b1n. This means that the number of free parameters of
B1(hence, of (Bj)n
j=1) is2 dimV(1)
1.
Thus, the number of free parameters of the affine map from the input layer to the first hidden layer is
5 dimVdimV(1)
1+ 2 dimV(1)
1.
Next, we consider the linear part of the affine map from the (k−1)-th layer ((V(1)
k−1)n)nto thek-th layer
((V(1)
k)n)nfork>1. Then, by Theorem 5, Snacts on these layers by the action ∗, and the representations
definedbythisaction ∗isisomorphictothesumoftheinducedrepresentationoftherestrictedrepresentation.
by equation (22) and a similar argument as equation (24), we have
HomSn(R[Sn]⊗R[Sn−1](V(1)
k−1)n,R[Sn]⊗R[Sn−1](V(1)
k)n)
≃HomSn(R[Sn]⊗R[Sn−1]Rn⊗RV(1)
k−1,R[Sn]⊗R[Sn−1]Rn⊗RV(1)
k)
≃HomSn(R[Sn]⊗R[Sn−1]Rn,R[Sn]⊗R[Sn−1]Rn)⊗RHom(V(1)
k−1,V(1)
k)
≃(HomSn(V⊕2
(n),V⊕2
(n))⊕HomSn(V⊕3
(n−1,1),V⊕3
(n−1,1))
⊕HomSn(V(n−2,2),V(n−2,2))⊕HomSn(V(n−2,1,1),V(n−2,1,1)))⊗RHom(V(1)
k−1,V(1)
k).
27Published in Transactions on Machine Learning Research (09/2024)
Becausethedimensionofthedivisionalgebraover Risatmost 4, thedimensionsof HomSn(V(n−2,2),V(n−2,2))
andHomSn(V(n−2,1,1),V(n−2,1,1)))are at most 4. Thus, the dimension of HomSn(((V(1)
k−1)n)n,((V(1)
k)n)n)is
at most 21 dimV(1)
k−1dimV(1)
k.
On the other hand, the number of free parameters of the bias vectors is 2 dimV(1)
kby the similar argument
as above.
28