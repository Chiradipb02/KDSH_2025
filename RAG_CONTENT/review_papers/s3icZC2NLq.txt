A Nearly Optimal and Low-Switching Algorithm for
Reinforcement Learning with General Function
Approximation
Heyang Zhao
Department of Computer Science
University of California, Los Angeles
Los Angeles, CA 90095
hyzhao@cs.ucla.edu
Jiafan He
Department of Computer Science
University of California, Los Angeles
Los Angeles, CA 90095
jiafanhe19@ucla.eduQuanquan Gu
Department of Computer Science
University of California, Los Angeles
Los Angeles, CA 90095
qgu@cs.ucla.edu
Abstract
The exploration-exploitation dilemma has been a central challenge in reinforcement
learning (RL) with complex model classes. In this paper, we propose a new
algorithm, Monotonic Q-Learning with Upper Confidence Bound (MQL-UCB) for
RL with general function approximation. Our key algorithmic design includes (1)
a general deterministic policy-switching strategy that achieves low switching cost,
(2) a monotonic value function structure with carefully controlled function class
complexity, and (3) a variance-weighted regression scheme that exploits historical
trajectories with high data efficiency. MQL-UCB achieves minimax optimal regret
ofeO(d√
HK)when Kis sufficiently large and near-optimal policy switching cost
ofeO(dH), with dbeing the eluder dimension of the function class, Hbeing the
planning horizon, and Kbeing the number of episodes. Our work sheds light
on designing provably sample-efficient and deployment-efficient Q-learning with
nonlinear function approximation.
1 Introduction
In reinforcement learning (RL), a learner interacts with an unknown environment and aims to
maximize the cumulative reward. As one of the most mainstream paradigms for sequential decision-
making, RL has extensive applications in many real-world problems (Kober et al., 2013; Mnih et al.,
2015; Lillicrap et al., 2015; Zoph and Le, 2016; Zheng et al., 2018). Theoretically, the RL problem is
often formulated as a Markov Decision Process (MDP) (Puterman, 2014). Achieving the optimal
regret bound for various MDP settings has been a long-standing fundamental problem in RL research.
In tabular MDPs where the state space Sand the action space Aare finite, the optimal regret bound
has been well-established, ranging from the episodic setting (Azar et al., 2017; Zanette and Brunskill,
2019; Simchowitz and Jamieson, 2019; Zhang et al., 2020), average-reward setting (Zhang and Ji,
2019) to the discounted setting (He et al., 2021b). Nevertheless, these regret guarantees are intolerably
large in many real-world applications, where the state space Sand the action space Aare often large
or even infinite.
38th Conference on Neural Information Processing Systems (NeurIPS 2024).As is commonly applied in applications, function approximation has been widely studied by theorists
to demonstrate the generalization across large state-action spaces, proving the performance guarantees
of various RL algorithms for specific function classes. There are recent works on RL with linear
function approximation under different assumptions such as linear MDPs (Yang and Wang, 2019;
Jin et al., 2020), linear mixture MDPs (Modi et al., 2020; Ayoub et al., 2020; Zhou et al., 2021a).
Among them, Zhou et al. (2021a) achieved nearly optimal regret bounds for linear mixture MDPs
through a model-based approach adopting variance-weighted linear regression. Later, Hu et al.
(2022) proposed LSVI-UCB+ algorithm, making an attempt to improve the regret for linear MDP
through an over-optimistic value function approach. However, their analysis was later discovered
to suffer from a technical issue (Agarwal et al., 2022; He et al., 2022). To fix this issue, Agarwal
et al. (2022) introduced similar over-optimistic value functions to construct a monotonic variance
estimator and a non-Markov policy, achieving the first statistically optimal regret for linear MDPs.
They also proposed a novel algorithm dubbed VO QL for RL with general function approximation.
Concurrently, He et al. (2022) proposed LSVI-UCB++, which takes a different approach and employs
a rare-switching technique to obtain the optimal regret. In a parallel line of research, there has
been a growing body of literature proposing more general frameworks to unify sample efficient RL
algorithms, e.g., Bellman rank (Jiang et al., 2017), Witness rank (Sun et al., 2019), eluder dimension
(Russo and Van Roy, 2013), Bellman eluder dimension (Jin et al., 2021), Bilinear Classes (Du et al.,
2021), Decision-Estimation Coefficient (Foster et al., 2021), Admissible Bellman Characterization
(Chen et al., 2022) and Decoupling Coefficient (Agarwal and Zhang, 2022a,b). However, when
applying these frameworks to linear MDPs, none of them can achieve the minimax optimal regret. To
our knowledge, Agarwal et al. (2022) is the only algorithmic framework achieving optimal regret for
RL beyond linear function approximation. Since VO QL requires a non-Markov planning procedure,
where the resulting policy does not act greedily with respect to a single optimistic value function, it is
natural to ask
Can we develop a RL algorithm with Markov policy1to solve MDPs with general function
approximation and achieve the optimal regret?
While the sample efficiency of RL algorithms for MDPs with nonlinear function classes has been
comprehensively researched, deployment efficiency (Matsushima et al., 2020) is also a major concern
in many real-world application scenarios. For example, in recommendation systems (Afsar et al.,
2022), it may take several weeks to deploy a new recommendation policy. On the other hand, the
system is capable of collecting an enormous amount of data every minute implementing a fixed
policy. As a result, it is computationally inefficient to change the executed policy after each data
point is collected as is demanded by most of the online RL algorithms in theoretical studies. To
resolve this issue, Bai et al. (2019) first introduced the concept of switching cost, defined as the
number of policy updates. Following this concept, a series of RL algorithms have been proposed
on the theoretical side with low switching cost guarantees (e.g., Zhang et al., 2020; Wang et al.,
2021; Qiao et al., 2022; Kong et al., 2021; Velegkas et al., 2022; Li et al., 2023). Xiong et al. (2023)
considered low-switching RL with general function approximation, which achieves eO(dH)switching
cost. However, their algorithm has an intractable planning phase and is not statistically optimal.
Kong et al. (2021), Velegkas et al. (2022), and Li et al. (2023) considered RL with general function
approximation, all of which achieved the switching cost of O(d2Hpolylog (K)), where dis the eluder
dimension of the underlying function class, His the planning horizon, and Kis the number of
episodes. In contrast, Gao et al. (2021) proved an Ω(dH/logd)lower bound of switching cost for
any deterministic algorithms in learning linear MDPs. Therefore, the following question remains
open:
Can we design an efficient algorithm with eO(dH)switching cost for MDPs with bounded eluder
dimension?
In this paper, we answer the above two questions simultaneously by proposing a novel algorithm
Monotonic Q-Learning with UCB (MQL-UCB) with all the aforementioned appealing properties. At
the core of our algorithmic design are the following innovative techniques:
1A Markov policy means that the action chosen by the policy only depends on the current state instead of the
prefix trajectory. It is more aligned with the empirical RL approaches, since the estimated value function is not
well-defined under non-Markov policy.
2Table 1: A comparison of existing algorithms in terms of regret and switching cost for linear MDP
and general function class with bounded eluder dimension and Bellman completeness. The results
hold for in-homogeneous episodic RL with horizon length H, number of episodes Kwhere the total
reward obtained in an episode is not larger than 1. For regret, we only present the leading term when
Kis large enough compared to other variables and hide poly-logarithmic factors in K,dordim,H
and the constant. For linear MDPs, dis the dimension of the feature vectors. For general function
class, dim is a shorthand of the eluder dimension of the underlying function class, Nis the covering
number of the value function class, and NS,Ais the covering number of the state-action space.
Algorithm Regret # of Switches Model Class
LSVI-UCB
(Jin et al., 2020)d3/2H√
K K
LSVI-UCB-RareSwitch
(Wang et al., 2021)d3/2H√
K eO(dH)
LSVI-UCB++
(He et al., 2022)d√
HK eO(dH)Linear MDPs
F-LSVI
(Wang et al., 2020a)dim(F)p
logNlogNS,A·H√
K K
GOLF
(Jin et al., 2021)p
dim(F) logN·H√
K KBounded eluder dimension
VOQL + Completeness
(Agarwal et al., 2022)p
dim(F) logN·HK eO(dim(F)2H)
MQL-UCB
(Theorem 4.1)p
dim(F) logN·HK eO(dim(F)H)
•We propose a novel policy-switching strategy based on the cumulative sensitivity of historical data.
To the best of our knowledge, this is the first computationally-tractable deterministic rare-switching
algorithm for RL with general function approximation which achieves eO(dH)switching cost. We
also prove a nearly matching lower bound for any algorithm with arbitrary policies including both
deterministic and stochastic policies (See Theorem B.1). Previous approaches for low switching
cost in RL with general function approximation are sampling-based (Kong et al., 2021; Velegkas
et al., 2022; Li et al., 2023) and highly coupled with a sub-sampling technique used for regression,
making it less efficient. When restricted to the linear case, sampling-based rare-switching has a
worse switching cost of eO(d2H)(Section 3.3 in Kong et al. 2021; Theorem 3.3 in Velegkas et al.
2022; Section C in Li et al. 2023) than that in Wang et al. (2021).
•With the novel policy-switching scheme, we illustrate how to reduce the complexity of value
function classes while maintaining a series of monotonic value functions, strictly generalizing
the LSVI-UCB++ algorithm (He et al., 2022) to general function class with bounded eluder
dimension. Based on the structure of the value functions, we further demonstrate how MQL-UCB
achieves a variance-dependent regret bound that can gracefully reduce to a nearly constant regret
for deterministic MDPs. While in the worst case, our algorithm still attains a nearly minimax
optimal regret guarantee. Our work is the first work for RL with general function approximation
that achieves the nearly minimax optimal regret when specialized to linear MDPs, while still enjoys
simple Markov planning phase.
It is worth noting that recently, Qiao et al. (2023) also considered RL with low-switching cost beyond
linear function approximation, i.e., MDPs with low inherent Bellman error (Zanette et al., 2020b)
and generalized linear MDPs (Wang et al., 2020b). Their approach can be seen as a slight extension
of RL with low-switching cost for linear MDPs, since in both settings, the covariance matrix still
exists and they can still use the determinant of the covariance as a criterion for policy switching.
Notation. We use lower case letters to denote scalars and use lower and upper case bold face
letters to denote vectors and matrices respectively. We denote by [n]the set {1, . . . , n }. For two
positive sequences {an}and{bn}withn= 1,2, . . ., we write an=O(bn)if there exists an absolute
constant C > 0such that an≤Cbnholds for all n≥1and write an= Ω( bn)if there exists an
absolute constant C > 0such that an≥Cbnholds for all n≥1. We use eO(·)to further hide the
polylogarithmic factors except log-covering numbers. We use 1{·}to denote the indicator function.
32 Preliminaries
2.1 Time-Inhomogeneous Episodic MDP
We consider a time-inhomogeneous episodic Markov Decision Process (MDP), denoted by a tuple
M=M(S,A, H,{Ph}H
h=1,{rh}H
h=1). Here, SandAare the spaces of state and action, respectively,
His the length of each episode, Ph:S × A × S → [0,1]is the transition probability function at
stage hwhich denotes the probability for state sto transfer to the next state s′with current action a,
andrh:S × A → [0,1]is the deterministic reward function at stage h. A policy π:={πh}H
h=1is a
collection of mappings πhfrom an observed state s∈ Sto the simplex of action space A. For any
policy π={πh}H
h=1and stage h∈[H], we define the value function Vπ
h(s)and the action-value
function Qπ
h(s, a)as follows:
Qπ
h(s, a) =rh(s, a) +EHX
h′=h+1rh′ 
sh′, πh′(sh′)sh=s, ah=a
, Vπ
h(s) =Qπ
h(s, πh(s)),
where sh′+1∼Ph′(·|sh′, ah′). Then, we further define the optimal value function V∗
hand the
optimal action-value function Q∗
hasV∗
h(s) = max πVπ
h(s)andQ∗
h(s, a) = max πQπ
h(s, a). For
simplicity, we assume the total reward for each possible trajectory (s1, a1, ..., s H, aH)satisfiesPH
h=1rh(sh, ah)≤1. Under this assumption, the value function Vπ
h(·)andQπ
h(·,·)are bounded in
[0,1]. For any function V:S →Rand stage h∈[H], we define the following first-order Bellman
operator Thand second-order Bellman operator T2
hon function V:
ThV(sh, ah) =Esh+1
rh+V(sh+1)|sh, ah
,T2
hV(sh, ah) =Esh+1h 
rh+V(sh+1)2|sh, ahi
,
where sh+1∼Ph(·|sh, ah)andrh=rh(sh, ah). For simplicity, we further define [PhV](s, a) =
Es′∼Ph(·|s,a)V(s′)and[VhV](s, a) =T2
hV(sh, ah)− 
ThV(sh, ah)2. Using this notation, for each
stage h∈[H], the Bellman equation and Bellman optimality equation take the following forms:
Qπ
h(s, a) =ThVπ
h+1(s, a), Q∗(s, a) =ThV∗
h+1(s, a),
where Vπ
H+1(·) =V∗
H+1(·) = 0 . At the beginning of each episode k∈[K], the agent selects a
policy πkto be executed throughout the episode, and an initial state sk
1is arbitrarily selected by
the environment. For each stage h∈[H], the agent first observes the current state sk
h, chooses an
action following the policy πk
h, then transits to the next state with sk
h+1∼Ph(·|sk
h, ak
h)and reward
rh(sh, ah). Based on the protocol, we defined the suboptimality gap in episode kas the difference
between the value function for selected policy πkand the optimal value function V∗
1(sk
1)−Vπk
1(sk
1).
Based on these definitions, we can define the regret in the first Kepisodes as follows:
Definition 2.1. For any RL algorithm Alg, the regret in the first Kepisodes is denoted by the sum of
the suboptimality for episode k= 1, . . . , K ,
Regret (K) =KX
k=1V∗
1(sk
1)−Vπk
1(sk
1),
where πkis the agent’s policy at episode k.
2.2 Function Classes and Covering Numbers
Assumption 2.2 (Completeness) .Given F:={Fh}H
h=1which is composed of bounded functions
fh:S × A → [0, L]. We assume that for any function V:S → [0,1]there exists f1, f2∈ Fhsuch
that for any (s, a)∈ S × A ,
Es′∼Ph(·|s,a)
rh(s, a) +V(s′)
=f1(s, a),andEs′∼Ph(·|s,a)h 
rh(s, a) +V(s′)2i
=f2(s, a).
We assume that L=O(1)throughout the paper.
Remark 2.3. Completeness is a fundamental assumption in RL with general function approximation,
as recognized in Wang et al. (2020b); Jin et al. (2021); Agarwal et al. (2022). Our assumption is the
same as that in Agarwal et al. (2022) and is slightly stronger than that in Wang et al. (2020a) and
4Jin et al. (2021). More specifically, in Wang et al. (2020a), completeness is only required for the
first-order Bellman operator. In contrast, we necessitate completeness with respect to the second-order
Bellman operator, which becomes imperative during the construction of variance-based weights. Jin
et al. (2021) only requires the completeness for the function class Fh+1(ThFh+1⊆ F h). However,
the GOLF algorithm (Jin et al., 2021) requires solving an intricate optimization problem across
the entire episode. In contrast, we employ pointwise exploration bonuses as an alternative strategy,
which requires the completeness for function class V={V:S → [0,1]}, i.e.,ThV ⊆ F h. The
completeness assumption on the second moment is first introduced by Agarwal et al. (2022), and
is crucial for obtaining a tighter regret bound. More specifically, making use of the variance of the
value function at the next state is known to be crucial to achieve minimax-optimal regret bound in RL
ranging from tabular MDPs (Azar et al., 2017) to linear mixture MDPs (Zhou et al., 2021a) and linear
MDPs (He et al., 2022). In RL with general function approximation, the second-moment compleness
assumption makes the variance of value functions computationally tractable.
Definition 2.4 (Generalized Eluder dimension, Agarwal et al. 2022) .Letλ≥0, a sequence of
state-action pairs Z={zi}i∈[T]and a sequence of positive numbers σ={σi}i∈[T]. The gener-
alized Eluder dimension of a function class F:S × A → [0, L]with respect to λis defined by
dimα,T(F) := supZ,σ:|Z|=T,σ≥αdim(F,Z,σ),
dim(F,Z,σ) :=TX
i=1min
1,1
σ2
iD2
F(zi;z[i−1], σ[i−1])
,
D2
F(z;z[t−1], σ[t−1]) := sup
f1,f2∈F(f1(z)−f2(z))2
P
s∈[t−1]1
σ2s(f1(zs)−f2(zs))2+λ.
We write dimα,T(F) :=H−1·P
h∈[H]dimα,T(Fh)for short when Fis a collection of function
classes F={Fh}H
h=1in the context.
Remark 2.5. TheD2
Fquantity has been introduced in Agarwal et al. (2022) and Ye et al. (2023) to
quantify the uncertainty of a state-action pair given a collected dataset with corresponding weights. It
was inspired by Gentile et al. (2022) where an unweighted version of uncertainty has been defined
for active learning. Prior to that, Wang et al. (2020a) introduced a similar ‘sensitivity’ measure
to determine the sampling probability in their sub-sampling framework. As discussed in Agarwal
et al. (2022), when specialized to linear function classes, D2
F(zt;z[t−1], σ[t−1])can be written as the
elliptical norm ∥zt∥2
Σ−1
t−1, where Σt−1is the weighted covariance matrix of the feature vectors z[t−1].
Definition 2.6 (Bonus oracle ¯D2
F).In this paper, the bonus oracle is denoted by ¯D2
F, which computes
the estimated uncertainty of a state-action pair z= (s, a)∈ S × A with respect to historical
dataz[t−1]and corresponding weights σ[t−1]. In detail, we assume that a computable function
¯D2
F(z;z[t−1], σ[t−1])satisfies¯DF(z;z[t−1],σ[t−1])
DF(z;z[t−1],σ[t−1])∈[1, C], where Cis a fixed constant.
Remark 2.7. Agarwal et al. (2022) also assumed access to such a bonus oracle defined in Defini-
tion 2.6, where they assume that the bonus oracle finds a proper bonus from a finite bonus class
(Definition 3, Agarwal et al. 2022). Our definition is slightly different in the sense that the bonus class
is not assumed to be finite but with a finite covering number. Previous works by Kong et al. (2021)
and Wang et al. (2020a) proposed a sub-sampling idea to compute such a bonus function efficiently
in general cases, which is also applicable in our framework. In a similar nonlinear RL setting, Ye
et al. (2023) assumed that the uncertainly D2
Fcan be directly computed, which is a slightly stronger
assumption. But essentially, these differences in bonus assumption only lightly affect the algorithm
structure.
Definition 2.8 (Covering numbers of function classes) .For any ϵ >0, we define the following
covering numbers of the involved function classes:
1.For each h∈[H], there exists an ϵ-cover C(Fh, ϵ)⊆ F hwith size |C(Fh, ϵ)| ≤N(Fh, ϵ), such
that for any f∈ F, there exists f′∈ C(Fh, ϵ), such that ∥f−f′∥∞≤ϵ. For any ϵ >0, we
define the uniform covering number of Fwith respect to ϵasNF(ϵ) := max h∈[H]N(Fh, ϵ).
2.There exists a bonus class B:S × A → Rsuch that for any t≥0,z[t]∈(S × A )t,σ[t]∈Rt,
the oracle defined in Definition 2.6 ¯DF(·;z[t], σ[t])is inB.
3.For bonus class B, there exists an ϵ-cover C(B, ϵ)⊆ B with size |C(B, ϵ)| ≤N(B, ϵ), such that
for any b∈ B, there exists b′∈ C(B, ϵ), such that ∥b−b′∥∞≤ϵ.
5Remark 2.9. In general function approximation, it is common to introduce the additional assumption
on the covering number of bonus function classes. For example, in Ye et al. (2023), Agarwal and
Zhang (2022a), and Di et al. (2023), the covering number of the bonus function class is bounded.
3 Algorithm and Key Techniques
In this section, we will introduce our new algorithm, MQL-UCB. The detailed algorithm is provided
in Algorithm 1. Our algorithm’s foundational framework follows the Upper Confidence Bound
(UCB) approach. In detail, for each episode k∈[K], we construct an optimistic value function
Qk,h(s, a)during the planning phase. Subsequently, in the exploration phase, the agent interacts with
the environment, employing a greedy policy based on the current optimistic value function Qk,h(s, a).
Once the agent obtains the reward rk
hand transitions to the next state sk
h+1, these outcomes are
incorporated into the dataset, contributing to the subsequent planning phase. We will proceed to
elucidate the essential components of our method.
3.1 Rare Policy Switching
For MQL-UCB algorithm, the value functions Qk,h,qQk,h, along with their corresponding policy πk,
undergo updates when the agent collects a sufficient number of trajectories within the dataset that
could significantly diminish the uncertainty associated with the Bellman operator ThV(·,·)through
the weighted regression. In the context of linear bandits (Abbasi-Yadkori et al., 2011) or linear
MDPs (He et al., 2022), the uncertainty pertaining to the least-square regression is quantified by
the covariance matrix Σk. In this scenario, the agent adjusts its policy once the determinant of the
covariance matrix doubles, employing a determinant-based criterion. Nevertheless, in the general
function approximation setting, such a method is not applicable in the absence of the covariance
matrix which serves as a feature extractor in the linear setting. Circumventing this issue, Kong et al.
(2021) proposed a sub-sampling-based method to achieve low-switching properties in nonlinear RL.
Their subsampling technique is inspired by Wang et al. (2021), which showed that one only needs
to maintain a small subset of historical data to obtain a sufficiently accurate least-square estimator.
Such a subset can be generated sequentially according to the sensitivity of a new coming data point.
However, their approach leads to a switching cost of eO(d2H), which does not match the lower bound
in linear MDPs proposed by Gao et al. (2021).
To resolve this issue, we proposed a more general deterministic policy-updating framework for non-
linear RL. In detail, we use ¯D2
Fh(zi,h;z[klast−1],h,¯σ[klast−1],h)to evaluate the information collected
at the episode i, given the last updating klast. Once the collected information goes beyond a threshold
χfrom last updating, i.e.,
k−1X
i=klast1
¯σ2
i,h¯D2
Fh(zi,h;z[klast−1],h,¯σ[klast−1],h)≥χ. (3.1)
the agent will perform updates on both the optimistic estimated value function and the pessimistic
value function. Utilizing the D2
Fh-based criterion, we will show that the number of policy updates
can be bounded by O(H·dimα,K(F)). This further reduces the complexity of the optimistic value
function class and removes additional factors from a uniform convergence argument over the function
class. Specifically, we showcase under our rare-switching framework, the ϵ-covering number of the
optimistic and the pessimistic value function class at episode kis bounded by
Nϵ(k) := [ NF(ϵ/2)·N(B, ϵ/2pβK)]lk+1, (3.2)
where pβKis the maximum confidence radius as shown in Algorithm 1, which will be specified in
Lemmas G.3 and G.4, lkis the number of policy switches before the end of the k-th episode according
to Algorithm 1.
3.2 Weighted Regression
The estimation of Qfunction in MQL-UCB extends LSVI-UCB proposed by Jin et al. (2020) to
general function classes. While the estimators in LSVI-UCB are computed from the classic least
6squares regression, we construct the estimated value function pfk,hfor general function classes by
solving the following weighted regression:
pfk,h= argmin
fh∈FhX
i∈[k−1]1
¯σ2
i,h(fh(si
h, ai
h)−ri
h−Vk,h+1(si
h+1))2.
In the weighted regression, we set the weight ¯σk,has
¯σk,h= max
σk,h, α, γ·¯D1/2
Fh(z;z[k−1],h,¯σ[k−1],h)	
,
where σk,his the estimated variance for the stochastic transition process, ¯DFh(z;z[k−1],h,¯σ[k−1],h)
denotes the uncertainty of the estimated function pfk,hconditioned on the historical observations and
γ2:= log
2HK2 
2 log( L2k/α4) + 2
· 
log(4 L/α2) + 2
·N4
F(ϵ)·N2
ϵ(K)
δ
(3.3)
is used to properly balance the uncertainty across various state-action pairs. It is worth noting that Ye
et al. (2023) also introduced the uncertainty-aware variance in the general function approximation with
a distinct intention to deal with the adversarial corruption from the attacker. In addition, according to
the weighted regression, with high probability, the Bellman operator ThVk,hsatisfies:
λ+X
i∈[k−1]¯σ−2
i,h
pfk,h(si
h, ai
h)− ThVk,h+1(si
h, ai
h)2
≤pβ2
k,
where pβkis the exploration radius for the Qfunctions, which will be specified in Theorem 4.1.
According to the definition of Generalized Eluder dimension, the estimation error between the
estimated function pfk,hand the Bellman operator is upper bounded by:
pfk,h(s, a)− ThVk,h+1(s, a)≤pβkDFh(z;z[k−1],h,¯σ[k−1],h).
Therefore, we introduce the exploration bonus bk,hand construct the optimistic value
function Qk,h(s, a),i.e., Qk,h(s, a)≈pfk,h(s, a) + bk,h(s, a),where bk,h(s, a) = pβk·
¯DF 
(s, a);z[k−1],h, σ[k−1],h
. Inspired by Hu et al. (2022); He et al. (2022); Agarwal et al. (2022), in
order to estimate the gap between the optimistic value function Vk,h(s)and the optimal value function
V∗
h(s), we further construct the pessimistic estimator qfk,hby the following weighted regression
qfk,h= argmin
fh∈FhX
i∈[k−1]1
¯σ2
i,h(fh(si
h, ai
h)−ri
h−qVk,h+1(si
h+1))2,
and introduce a negative exploration bonus when generating the pessimistic estimator. qQk,h(s, a)≈
qfk,h(s, a)−qbk,h(s, a), where qbk,h(s, a) =qβk·¯DF((s, a);z[k−1],h, σ[k−1],h. Different from Agarwal
et al. (2022), the pessimistic value function qfk,his computed from a similar weighted-regression
scheme as in the case of the optimistic estimator, leading to a tighter confidence set.
3.3 Variance Estimator
In this subsection, we provide more details about the variance estimator σk,h, which measures the
variance of the value function Vk,h+1(sk
h+1)caused by the stochastic transition from state-action pair
(sk
h, ak
h). According to the definition of pfk,h, the difference between the estimator pfk,handThVk,h+1
satisfies
λ+X
i∈[k−1]1
¯σ2
i,h pfk,h(si
h, ai
h)− ThVk,h+1(si
h, ai
h)2≤2X
i∈[k−1]1
¯σ2
i,h 
f(si
h, ai
h)−pf∗
k(si
h, ai
h)
·pηi
h(Vk,h+1),
where the noise pηk
h(V) = rk
h+V(sk
h+1)−Es′∼Ph(sk
h,ak
h)[rh(sk
h, ak
h, s′) +V(s′)]denotes the
stochastic transition noise for the value function V. However, the generation of the target function
Vk,h+1relies on previously collected data z[klast], thus violating the conditional independence property.
Consequently, the noise term pηk
h(Vk,h+1)may not be unbiased. To address this challenge, it becomes
imperative to establish a uniform convergence property over the potential function class, which is
first introduced in linear MDPs by Jin et al. (2020).
Following the previous approach introduced by Azar et al. (2017); Hu et al. (2022); Agarwal et al.
(2022); He et al. (2022), we decompose the noise of optimistic value function pηk
h(Vk,h+1)into the
7Algorithm 1 Monotonic Q-Learning with UCB (MQL-UCB)
Require: Regularization parameter λ, confidence radius {eβk}k∈[K],{pβk}k∈[K]and{qβk}k∈[K].
1:Initialize klast= 0. For each stage h∈[H]and state-action (s, a)∈ S × A , setQ0,h(s, a)←
H,qQ0,h(s, a)←0.
2:forepisodes k= 1, . . . , K do
3: Received the initial state sk
1.
4: forstage h=H, . . . , 1do
5: ifthere exists a stage h′∈[H]such that (3.1) holds then
6: pfk,h←argminfh∈FhP
i∈[k−1]1
¯σ2
i,h(fh(si
h, ai
h)−ri
h−Vk,h+1(si
h+1))2.
7: qfk,h←argminfh∈FhP
i∈[k−1]1
¯σ2
i,h(fh(si
h, ai
h)−ri
h−qVk,h+1(si
h+1))2.
8: efk,h←argminfh∈FhP
i∈[k−1] 
fh(si
h, ai
h)− 
ri
h+Vk,h+1(si
h+1)22.
9: Qk,h(s, a)←minn
pfk,h(s, a) +bk,h(s, a), Qk−1,h(s, a),1o
.
10: qQk,h(s, a)←maxn
qfk,h(s, a)−qbk,h(s, a),qQk−1,h(s, a),0o
.
11: Set the last updating episode klast←kand number of policies as lk←lk−1+ 1.
12: else
13: Qk,h(s, a)←Qk−1,h(s, a),qQk,h(s, a)←qQk−1,h(s, a)andlk←lk−1.
14: end if
15: Set the policy πkasπk
h(·)←argmaxa∈AQk,h(·, a).Vk,h(s)←max aQk,h(s, a),
qVk,h(s)←max aqQk,h(s, a).
16: end for
17: forstage h= 1, . . . , H do
18: Take action ak
h←πk
h(sk
h)and receive next state sk
h+1.
19: Set the estimated variance σk,has in (3.4) and set ¯σk,h←max
σk,h, α, γ·
D1/2
Fh(z;z[k−1],h,¯σ[k−1],h)	
.
20: end for
21:end for
noise of optimal value function pηk
h(V∗
h+1)and the noise pηk
h 
Vk,h+1−V∗
h+1
to reduce the extra
log 
Nϵ(K)
dependency in the confidence radius. With the noise decomposition, we evaluate the
variances [VhV∗
h+1](s, a)and
Vh(Vk,h+1−V∗
h+1)
(s, a)separately.
For the variance of the optimal value function [VhV∗
h+1](s, a), since the optimal value function V∗
h+1
is independent with the collected data z[klast], it prevents a uniform convergence-based argument over
the function class. However, the optimal value function V∗
h+1is unobservable, and it requires several
steps to estimate the variance. In summary, we utilize the optimistic function Vk,h+1to approximate
the optimal value function V∗
h+1and calculate the estimated variance [¯VhVk,h]as the difference
between the second-order moment and the square of the first-order moment of Vk,h
[¯Vk,hVk,h+1] =efk,h−pf2
k,h.
Here, the approximate second-order moment efk,hand the approximate first-order moment pfk,his
generated by the least-square regression (Lines 6 and 8). In addition, we introduce the exploration
bonus Ek,hto control the estimation error between the estimated variance and the true variance of
Vk,h+1andFk,hto control the sub-optimality gap between Vk,h+1andV∗
h+1:
Ek,h= (2Lβk+eβk) min 
1,¯DFh(z;z[k−1],h,¯σ[k−1],h)
,
Fk,h= 
log(N(F, ϵ)·Nϵ(K))
·min 
1,2pfk,h(sk
h, ak
h)−2qfk,h(sk
h, ak
h) + 4βk¯DFh(z;z[k−1],h,¯σ[k−1],h)
,
where
eβk=r
128 logNϵ(k)·N(F, ϵ)·H
δ+ 64Lϵ·k, βk=r
128·logNϵ(k)·N(F, ϵ)H
δ+ 64Lϵ·k/α2.
For the variance of the sub-optimality gap,
Vh(Vk,h+1−V∗
h+1)
(s, a), based on the structure of
optimistic and pessimistic value function, it can be approximate and upper bounded by
[Vh(Vk,h+1−V∗
h+1)](sk
h, ak
h)≤2[Ph(Vk,h+1−V∗
h+1)](sk
h, ak
h)
8≤2[Ph(Vk,h+1−qVk,h+1)](sk
h, ak
h)≈2pfk,h(sk
h, ak
h)−2qfk,h(sk
h, ak
h),
where the approximate first-order moments pfk,h,qfk,hare generated by the least-square regression
(Lines 6 and 7) and can be dominated by the exploration bonus Fk,h.
In summary, we construct the estimated variance σk,has:
σk,h=q
[¯Vk,hVk,h+1](sk
h, ak
h) +Ek,h+Fk,h. (3.4)
3.4 Monotonic Value Function
As we discussed in the previous subsection, we decompose the value function Vk,hand evaluate
the variance [VhV∗
h+1](s, a),
Vh(Vk,h+1−V∗
h+1)
(s, a)separately. However, for each state-action
pair(sk
h, ak
h)and any subsequent episode i > k , the value function Vi,hand corresponding vari-
ance
Vh(Vi,h+1−V∗
h+1)
(sk
h, ak
h)may differ from the previous value function Vk,hand variance
Vh(Vk,h+1−V∗
h+1)
(sk
h, ak
h). Extending the idea proposed by He et al. (2022) for linear MDPs, we
ensure that the pessimistic value function qQk,hmaintains a monotonically increasing property during
updates, while the optimistic value function Qk,hmaintains a monotonically decreasing property.
Leveraging these monotonic properties, we can establish an upper bound on the variance as follows:

Vh(Vi,h+1−V∗
h+1)
(sk
h, ak
h)≤2[Ph(Vi,h+1−qVi,h+1)](sk
h, ak
h)≤2[Ph(Vk,h+1−qVk,h+1)](sk
h, ak
h)≤Fk,h.
In this scenario, the previously employed variance estimator σk,hoffers a consistent and uniform
upper bound for the variance across all subsequent episodes.
4 Main Results
In this section, we present the main theoretical results. In detail, we provide the regret upper bound of
Algorithm MQL-UCB in Theorem 4.1. As a complement, in Section B.1, we provide a lower bound
on the communication complexity for cooperative linear MDPs. Finally, in Section B.2, we discussed
the connection between the generalized eluder dimension and the standard eluder dimension.
The following theorem provides the regret upper bound of Algorithm MQL-UCB.
Theorem 4.1. Suppose Assumption 2.2 holds for function classes F:={Fh}H
h=1and Defini-
tion 2.4 holds with λ= 1. If we set α= 1/√
KH,ϵ= (KLH )−1, and set pβ2
k=qβ2
k:=
O 
log2k2(2 log( L2k/α4)+2)·(log(4L/α2)+2)
δ/H
·[log(NF(ϵ)) + 1] + O(λ) +O(ϵkL/α2), then with
probability 1−O(δ), the regret of MQL-UCB is upper bounded as follows:
Regret( K) =eO p
dim(F) logN·HVarK
+eO 
H2.5dim2(F)p
logNlog(N·Nb)
·p
HlogN+ dim( F) log( N·Nb)
where VarK:=PK
k=1PH
h=1[VhVπk
h+1](sk
h, ak
h) =eO(K), we denote the covering number of bonus
function class by Nb, the covering number of function class FbyN, and the dimension dimα,K(F)
bydim(F). Meanwhile, the switching cost of Algorithm 1 is O(dim α,K(F)·H).
In the worst case, when the number of episodes Kis sufficiently large, the leading term in our regret
bound is eO p
dim(F) logN·HK
. Our result matches the optimal regret achieved by Agarwal
et al. (2022). While their proposed algorithm involves the execution of a complicated and non-
Markovian policy with an action selection phase based on two series of optimistic value functions
and the prefix trajectory, MQL-UCB only requires the knowledge of the current state and a single
optimistic state-action value function Qto make a decision over the action space. In addition, our
theorem also provides a variance-dependent regret bound, which is adaptive to the randomness of the
underlying MDP encountered by the agent. Our definition of VarKis inspired by Zhao et al. (2023)
and Zhou et al. (2023), which studied variance-adaptive RL under tabular MDP setting and linear
mixture MDP setting, respectively.
As a direct application, we also present the regret guarantee of MQL-UCB for linear MDPs.
9Corollary 4.2. Under the same conditions of Theorem 4.1, assume that the underlying MDP is a
linear MDP such that F:={Fh}h∈[H]is composed of linear function classes with a known feature
mapping over the state-action space ϕ:S × A → Rd. If we set λ= 1,α= 1/√
K, then with
probability 1−O(δ), the following cumulative regret guarantee holds for MQL-UCB:
Regret( K) =eO 
d√
HK+H2.5d5p
H+d2
.
Remark 4.3. The leading term in our regret bound, as demonstrated in Corollary 4.2, matches the
lower bound proved in Zhou et al. (2021a) for linear MDPs. Similar optimal regrets have also been
accomplished by He et al. (2022) and Agarwal et al. (2022) for linear MDPs. Since we also apply
weighted regression to enhance the precision of our pessimistic value functions, the lower order
term (i.e., H2.5d5√
H+d2) in our regret has a better dependency on Hthan VO QL (Agarwal et al.,
2022) and LSVI-UCB++ (He et al., 2022), which may be of independent interest when considering
long-horizon MDPs. In addition, the switching cost of Algorithm 1 is bounded by eO(dH), which
matches the lower bound in Gao et al. (2021) for deterministic algorithms and our new lower bound
in Theorem B.1 for arbitrary algorithms up to logarithmic factors. For more details about our lower
bound, please refer to Appendix E.
5 Conclusion and Future Work
In this paper, we delve into the realm of RL with general function approximation. We proposed the
MQL-UCB algorithm with an innovative uncertainty-based rare-switching strategy in general function
approximation. Notably, our algorithm only requires eO(dH)updating times, which matches with
the lower bound established by Gao et al. (2021) up to logarithmic factors, and obtains a eO(d√
HK)
regret guarantee, which is near-optimal when restricted to the linear cases.
Acknowledgments and Disclosure of Funding
We thank the anonymous reviewers for their helpful comments. HZ is partially supported by the
research award from Cisco. JH and QG are partially supported by the research fund from UCLA-
Amazon Science Hub. The views and conclusions contained in this paper are those of the authors and
should not be interpreted as representing any funding agencies.
References
ABBASI -YADKORI , Y.,PÁL, D. andSZEPESVÁRI , C. (2011). Improved algorithms for linear
stochastic bandits. Advances in neural information processing systems 242312–2320.
AFSAR , M. M. ,CRUMP , T.andFAR, B. (2022). Reinforcement learning based recommender
systems: A survey. ACM Computing Surveys 551–38.
AGARWAL , A., J IN, Y. and Z HANG , T. (2022). V oql: Towards optimal regret in model-free rl with
nonlinear function approximation. arXiv preprint arXiv:2212.06069 .
AGARWAL , A.andZHANG , T.(2022a). Model-based rl with optimistic posterior sampling: Structural
conditions and sample complexity. Advances in Neural Information Processing Systems 3535284–
35297.
AGARWAL , A.andZHANG , T.(2022b). Non-linear reinforcement learning in large action spaces:
Structural conditions and sample-efficiency of posterior sampling. In Conference on Learning
Theory . PMLR.
AYOUB , A.,JIA, Z.,SZEPESVARI , C.,WANG, M. andYANG, L.(2020). Model-based reinforcement
learning with value-targeted regression. In International Conference on Machine Learning . PMLR.
AZAR, M. G. ,OSBAND , I.andMUNOS , R. (2017). Minimax regret bounds for reinforcement
learning. In International Conference on Machine Learning . PMLR.
BAI, Y.,XIE, T.,JIANG , N. andWANG , Y.-X. (2019). Provably efficient q-learning with low
switching cost. Advances in Neural Information Processing Systems 32.
10CHEN, Z.,LI, C. J. ,YUAN, H.,GU, Q. andJORDAN , M. (2022). A general framework for
sample-efficient function approximation in reinforcement learning. In The Eleventh International
Conference on Learning Representations .
DANN , C.,JIANG , N.,KRISHNAMURTHY , A.,AGARWAL , A.,LANGFORD , J.andSCHAPIRE ,
R. E. (2018). On oracle-efficient pac rl with rich observations. Advances in neural information
processing systems 31.
DI, Q.,ZHAO, H.,HE, J.andGU, Q.(2023). Pessimistic nonlinear least-squares value iteration for
offline reinforcement learning. arXiv preprint arXiv:2310.01380 .
DU, S.,KAKADE , S.,LEE, J.,LOVETT , S.,MAHAJAN , G.,SUN, W. andWANG, R.(2021). Bilinear
classes: A structural framework for provable generalization in rl. In International Conference on
Machine Learning . PMLR.
DU, S. S. ,KAKADE , S. M. ,WANG , R. andYANG , L. F. (2019). Is a good representation
sufficient for sample efficient reinforcement learning? In International Conference on Learning
Representations .
FOSTER , D. J. ,KAKADE , S. M. ,QIAN, J.andRAKHLIN , A.(2021). The statistical complexity of
interactive decision making. arXiv preprint arXiv:2112.13487 .
GAO, M.,XIE, T.,DU, S. S. andYANG , L. F. (2021). A provably efficient algorithm for linear
markov decision process with low switching cost. arXiv preprint arXiv:2101.00494 .
GENTILE , C.,WANG , Z.andZHANG , T.(2022). Achieving minimax rates in pool-based batch
active learning. In International Conference on Machine Learning . PMLR.
HAN, Y.,ZHOU, Z.,ZHOU, Z.,BLANCHET , J.,GLYNN , P. W. andYE, Y.(2020). Sequential batch
learning in finite-action linear contextual bandits. arXiv preprint arXiv:2004.06321 .
HE, J.,ZHAO, H.,ZHOU , D.andGU, Q.(2022). Nearly minimax optimal reinforcement learning
for linear markov decision processes. arXiv preprint arXiv:2212.06132 .
HE, J.,ZHOU , D.andGU, Q.(2021a). Logarithmic regret for reinforcement learning with linear
function approximation. In International Conference on Machine Learning . PMLR.
HE, J.,ZHOU , D. andGU, Q. (2021b). Nearly minimax optimal reinforcement learning for
discounted mdps. Advances in Neural Information Processing Systems 3422288–22300.
HU, P.,CHEN, Y.andHUANG , L.(2022). Nearly minimax optimal reinforcement learning with
linear function approximation. In International Conference on Machine Learning . PMLR.
HUANG , J.,CHEN, J.,ZHAO, L.,QIN, T.,JIANG , N.andLIU, T.-Y. (2022). Towards deployment-
efficient reinforcement learning: Lower bound and optimality. arXiv preprint arXiv:2202.06450
.
JIA, Z.,YANG , L.,SZEPESVARI , C.andWANG , M. (2020). Model-based reinforcement learning
with value-targeted regression. In Learning for Dynamics and Control . PMLR.
JIANG , N.,KRISHNAMURTHY , A.,AGARWAL , A.,LANGFORD , J.andSCHAPIRE , R. E. (2017).
Contextual decision processes with low bellman rank are pac-learnable. In International Conference
on Machine Learning . PMLR.
JIN, C.,ALLEN -ZHU, Z.,BUBECK , S.andJORDAN , M. I. (2018). Is q-learning provably efficient?
Advances in neural information processing systems 31.
JIN, C.,LIU, Q. andMIRYOOSEFI , S.(2021). Bellman eluder dimension: New rich classes of rl
problems, and sample-efficient algorithms. Advances in neural information processing systems 34
13406–13418.
JIN, C.,YANG, Z.,WANG, Z.andJORDAN , M. I. (2020). Provably efficient reinforcement learning
with linear function approximation. In Conference on Learning Theory . PMLR.
11KOBER , J.,BAGNELL , J. A. andPETERS , J.(2013). Reinforcement learning in robotics: A survey.
The International Journal of Robotics Research 321238–1274.
KONG , D.,SALAKHUTDINOV , R.,WANG , R.andYANG , L. F. (2021). Online sub-sampling for
reinforcement learning with general function approximation. arXiv preprint arXiv:2106.07203 .
LI, Y.,WANG, Y.,CHENG , Y.andYANG, L.(2023). Low-switching policy gradient with exploration
via online sensitivity sampling. In International Conference on Machine Learning . PMLR.
LILLICRAP , T. P. ,HUNT, J. J. ,PRITZEL , A.,HEESS , N.,EREZ, T.,TASSA , Y.,SILVER , D.and
WIERSTRA , D. (2015). Continuous control with deep reinforcement learning. arXiv preprint
arXiv:1509.02971 .
MATSUSHIMA , T.,FURUTA , H.,MATSUO , Y.,NACHUM , O. andGU, S.(2020). Deployment-
efficient reinforcement learning via model-based offline optimization. In International Conference
on Learning Representations .
MNIH, V.,KAVUKCUOGLU , K.,SILVER , D.,RUSU, A. A. ,VENESS , J.,BELLEMARE , M. G. ,
GRAVES , A.,RIEDMILLER , M.,FIDJELAND , A. K. ,OSTROVSKI , G. ET AL .(2015). Human-level
control through deep reinforcement learning. nature 518529–533.
MODI, A.,JIANG , N.,TEWARI , A. andSINGH , S.(2020). Sample complexity of reinforcement
learning using linearly combined model ensembles. In International Conference on Artificial
Intelligence and Statistics .
PUTERMAN , M. L. (2014). Markov decision processes: discrete stochastic dynamic programming .
John Wiley & Sons.
QIAO, D.,YIN, M.,MIN, M. andWANG , Y.-X. (2022). Sample-efficient reinforcement learning
with loglog (t) switching cost. In International Conference on Machine Learning . PMLR.
QIAO, D.,YIN, M. andWANG, Y.-X. (2023). Logarithmic switching cost in reinforcement learning
beyond linear mdps. arXiv preprint arXiv:2302.12456 .
RUAN, Y.,YANG , J.andZHOU , Y.(2021). Linear bandits with limited adaptivity and learning
distributional optimal design. In Proceedings of the 53rd Annual ACM SIGACT Symposium on
Theory of Computing .
RUSSO , D.andVANROY, B.(2013). Eluder dimension and the sample complexity of optimistic
exploration. Advances in Neural Information Processing Systems 26.
RUSSO , D.andVANROY, B.(2014). Learning to optimize via posterior sampling. Mathematics of
Operations Research 391221–1243.
SIMCHOWITZ , M. andJAMIESON , K. G. (2019). Non-asymptotic gap-dependent regret bounds for
tabular mdps. Advances in Neural Information Processing Systems 32.
SUN, W.,JIANG , N.,KRISHNAMURTHY , A.,AGARWAL , A.andLANGFORD , J.(2019). Model-
based rl in contextual decision processes: Pac bounds and exponential improvements over model-
free approaches. In Conference on learning theory . PMLR.
VELEGKAS , G.,YANG , Z.andKARBASI , A. (2022). Reinforcement learning with logarithmic
regret and policy switches. Advances in Neural Information Processing Systems 3536040–36053.
WANG , R.,SALAKHUTDINOV , R. R. andYANG , L.(2020a). Reinforcement learning with general
value function approximation: Provably efficient approach via bounded eluder dimension. Advances
in Neural Information Processing Systems 336123–6135.
WANG , T.,ZHOU , D. andGU, Q. (2021). Provably efficient reinforcement learning with linear
function approximation under adaptivity constraints. Advances in Neural Information Processing
Systems 3413524–13536.
WANG , Y.,WANG , R.,DU, S. S. andKRISHNAMURTHY , A.(2020b). Optimism in reinforcement
learning with generalized linear function approximation. In International Conference on Learning
Representations .
12XIONG , N.,YANG, Z.andWANG, Z.(2023). A general framework for sequential decision-making
under adaptivity constraints. arXiv preprint arXiv:2306.14468 .
YANG , L.andWANG , M. (2019). Sample-optimal parametric q-learning using linearly additive
features. In International Conference on Machine Learning . PMLR.
YANG , L.andWANG , M. (2020). Reinforcement learning in feature space: Matrix bandit, kernels,
and regret bound. In International Conference on Machine Learning . PMLR.
YE, C.,XIONG , W.,GU, Q.andZHANG , T.(2023). Corruption-robust algorithms with uncertainty
weighting for nonlinear contextual bandits and markov decision processes. In International
Conference on Machine Learning . PMLR.
ZANETTE , A.,BRANDFONBRENER , D.,BRUNSKILL , E.,PIROTTA , M. andLAZARIC , A.(2020a).
Frequentist regret bounds for randomized least-squares value iteration. In International Conference
on Artificial Intelligence and Statistics .
ZANETTE , A.andBRUNSKILL , E.(2019). Tighter problem-dependent regret bounds in reinforce-
ment learning without domain knowledge using value function bounds. In International Conference
on Machine Learning . PMLR.
ZANETTE , A.,LAZARIC , A.,KOCHENDERFER , M. andBRUNSKILL , E.(2020b). Learning near
optimal policies with low inherent bellman error. In International Conference on Machine Learning .
PMLR.
ZHANG , Z.andJI, X. (2019). Regret minimization for reinforcement learning by evaluating the
optimal bias function. Advances in Neural Information Processing Systems 32.
ZHANG , Z.,JI, X. andDU, S.(2021). Is reinforcement learning more difficult than bandits? a
near-optimal algorithm escaping the curse of horizon. In Conference on Learning Theory . PMLR.
ZHANG , Z.,JIANG , Y.,ZHOU , Y.andJI, X.(2022). Near-optimal regret bounds for multi-batch
reinforcement learning. Advances in Neural Information Processing Systems 3524586–24596.
ZHANG , Z.,ZHOU , Y.andJI, X. (2020). Almost optimal model-free reinforcement learningvia
reference-advantage decomposition. Advances in Neural Information Processing Systems 33
15198–15207.
ZHAO, H.,HE, J.,ZHOU, D.,ZHANG , T.andGU, Q.(2023). Variance-dependent regret bounds for
linear bandits and reinforcement learning: Adaptivity and computational efficiency. arXiv preprint
arXiv:2302.10371 .
ZHENG , G.,ZHANG , F.,ZHENG , Z.,XIANG , Y.,YUAN, N. J. ,XIE, X.andLI, Z.(2018). Drn: A
deep reinforcement learning framework for news recommendation. In Proceedings of the 2018
world wide web conference .
ZHOU , D. andGU, Q. (2022). Computationally efficient horizon-free reinforcement learning for
linear mixture mdps. arXiv preprint arXiv:2205.11507 .
ZHOU , D.,GU, Q.andSZEPESVARI , C.(2021a). Nearly minimax optimal reinforcement learning
for linear mixture markov decision processes. In Conference on Learning Theory . PMLR.
ZHOU , D.,HE, J.andGU, Q. (2021b). Provably efficient reinforcement learning for discounted
mdps with feature mapping. In International Conference on Machine Learning . PMLR.
ZHOU , R.,ZIHAN , Z.andDU, S. S. (2023). Sharp variance-dependent bounds in reinforcement
learning: Best of both worlds in stochastic and deterministic environments. In International
Conference on Machine Learning . PMLR.
ZOPH, B. andLE, Q. V. (2016). Neural architecture search with reinforcement learning. arXiv
preprint arXiv:1611.01578 .
13A Additional Related Work
A.1 RL with Linear Function Approximation
In recent years, a substantial body of research has emerged to address the challenges of solving
Markov Decision Processes (MDP) with linear function approximation, particularly to handle the vast
state and action spaces (Jiang et al., 2017; Dann et al., 2018; Yang and Wang, 2019; Du et al., 2019;
Sun et al., 2019; Jin et al., 2020; Wang et al., 2020b; Zanette et al., 2020a; Yang and Wang, 2020;
Modi et al., 2020; Ayoub et al., 2020; Zhou et al., 2021a; He et al., 2021a; Zhou and Gu, 2022; He
et al., 2022; Zhao et al., 2023). These works can be broadly categorized into two groups based on the
linear structures applied to the underlying MDP. One commonly employed linear structure is known
as the linear MDP (Jin et al., 2020), where the transition probability function Phand reward function
rhare represented as linear functions with respect to a given feature mapping ϕ:S × A → Rd.
Under this assumption, the LSVI-UCB algorithm (Jin et al., 2020) has been shown to achieve a regret
guarantee of eO(√
d3H4K). Subsequently, Zanette et al. (2020a) introduced the RLSVI algorithm,
utilizing the Thompson sampling method, to attain a regret bound of eO(√
d4H5K). More recently,
He et al. (2022) improved the regret guarantee to eO(√
d2H3K)with the LSVI-UCB++ algorithm,
aligning with the theoretical lower bound in Zhou et al. (2021a) up to logarithmic factors. Another
line of research has focused on linear mixture MDPs (Modi et al., 2020; Yang and Wang, 2020; Jia
et al., 2020; Ayoub et al., 2020; Zhou et al., 2021a), where the transition probability is expressed
as a linear combination of basic models P1,P2, ..,Pd. For linear mixture MDPs, Jia et al. (2020)
introduced the UCRL-VTR algorithm, achieving a regret guarantee of eO(√
d2H4K). Subsequently,
Zhou et al. (2021a) enhanced this result to eO(√
d2H3K), reaching a nearly minimax optimal regret
bound. Recently, several works focused on time-homogeneous linear mixture MDPs, and removed
the dependency on the episode length (horizon-free) (Zhang and Ji, 2019; Zhou et al., 2021b; Zhao
et al., 2023).
A.2 RL with General Function Approximation
Recent years have witnessed a flurry of progress on RL with nonlinear function classes. To explore the
theoretical limits of RL algorithms, various complexity measures have been developed to characterize
the hardness of RL instances such as Bellman rank (Jiang et al., 2017), Witness rank (Sun et al.,
2019), eluder dimension (Russo and Van Roy, 2013), Bellman eluder dimension (Jin et al., 2021),
Bilinear Classes (Du et al., 2021), Decision-Estimation Coefficient (Foster et al., 2021), Admissible
Bellman Characterization (Chen et al., 2022), generalized eluder dimension (Agarwal et al., 2022).
Among them, only Agarwal et al. (2022) yields a near-optimal regret guarantee when specialized to
linear MDPs. In their paper, they proposed a new framework named generalized eluder dimension
to handle the weighted objects in weighted regression, which can be seen as a variant of eluder
dimension. In their proposed algorithm VO QL, they adopted over-optimistic and over-pessimistic
value functions in order to bound the variance of regression targets, making it possible to apply a
weighted regression scheme in the model-free framework.
A.3 RL with Low Switching Cost
Most of the aforementioned approaches necessitate updating both the value function and the corre-
sponding policy in each episode, a practice that proves to be inefficient when dealing with substantial
datasets. To overcome this limitation, a widely adopted technique involves dividing the time sequence
into several epochs and updating the policy only between different epochs. In the context of the linear
bandit problem, Abbasi-Yadkori et al. (2011) introduced the rarely-switching OFUL algorithm, where
the agent updates the policy only when the determinant of the covariance matrix doubles. This method
enables the algorithm to achieve near-optimal regret of eO(√
K)while maintaining policy updates to
eO(dlogK)times. When the number of arms, denoted as |D|, is finite, Ruan et al. (2021) proposed
an algorithm with regret bounded by eO(√
dK)and a mere eO(dlogdlogK)policy-updating times.
In the realm of episodic reinforcement learning, Bai et al. (2019) and Zhang et al. (2021) delved
into tabular Markov Decision Processes, introducing algorithms that achieve a regret of eO(√
T)
andeO(SAlogK)updating times. Wang et al. (2021) later extended these results to linear MDPs,
unveiling the LSVI-UCB-RareSwitch algorithm. LSVI-UCB-RareSwitch delivers a regret bound
14ofeO(d3H4K)with a policy switching frequency of eO(dlogT), which matches the lower bound of
the switching cost (Gao et al., 2021) up to logarithmic terms. Furthermore, if the policy is updated
within fixed-length epochs (Han et al., 2020), the method is termed batch learning model, but this
falls beyond the scope of our current work. In addition, with the help of stochastic policies, Zhang
et al. (2022) porposed an algorithm with eO(√
K)regret guarantee and only eO(H)swithcing cost for
learning tabular MDPs. Later, Huang et al. (2022) employed the stochastic policy in learning linear
MDPs, which is able to find an ϵ-optimal policy with only eO(H)switching cost.
B Additional Results
B.1 Lower Bound for the Switching Cost
As a complement, we prove a new lower bound on the switching cost for RL with linear MDPs. Note
that linear MDPs lie in the class of MDPs studied in our paper with bounded generalized eluder
dimension. In particular, the generalized eluder dimension of linear MDPs is eO(d).
Theorem B.1. For any algorithm Algwith expected switching cost less than dH/(16 log K), there
exists a hard-to-learn linear MDP, such that the expected regret of Algis at least Ω(K).
Remark B.2. Theorem B.1 suggests that, to achieve a sublinear regret guarantee, an eΩ(dH)switch-
ing cost is inevitable. This lower bound does not violate the minimax upper bound of eO(H)proved
in Zhang et al. (2022); Huang et al. (2022), which additionally assume that the initial state sk
1is
either fixed or sampled from a fixed distribution. In contrast, our work and Gao et al. (2021) allow
the initial state to be adaptively chosen by an adversarial environment, bringing more challenges to
the learning of linear MDPs. When comparing our lower bound with the result in Gao et al. (2021),
it is worth noting that their focus is solely on deterministic algorithms, and they suggest that an
eΩ(dH)switching cost is necessary. As a comparison, our result holds for any algorithm with arbitrary
policies including both deterministic and stochastic policies.
B.2 Connection Between D2
F-Uncertainty and Eluder Dimension
Previous work by Agarwal et al. (2022) achieved the optimal regret bound eO(p
dim(F) logN·HK),
where dim(F)is defined as the generalized eluder dimension as stated in Definition 2.4. However,
the connection between generalized eluder dimension and eluder dimension proposed by Russo and
Van Roy (2013) is still under-discussed2. Consequently, their results could not be directly compared
with the results based on the classic eluder dimension measure (Wang et al., 2020a) or the more
general Bellman eluder dimension (Jin et al., 2021).
In this section, we make a first attempt to establish a connection between generalized eluder dimension
and eluder dimension in Theorem B.3.
Theorem B.3. For a function space G,α > 0, letdim be defined in Definition 2.4. WLOG,
we assume that for all g∈ G andz∈ Z ,|g(z)| ≤1. Then the following inequality between
dim(F,Z,σ)anddimE(G,1/√
T)holds for all Z:={zi}i∈[T]withzi∈ Z andσ:={σi}i∈[T]s.t.
α≤σi≤M∀i∈[T]:
X
i∈[T]min
1,1
σ2
iD2
G 
zi;z[i−1], σ[i−1]
≤O(dim E(F,1/√
T) logTlogλTlog(M/α ) +λ−1).
According to Theorem B.3, the generalized eluder dimension is upper bounded by eluder dimension
up to logarithmic terms. When the number of episodes Kis sufficiently large, the leading term
in our regret bound in Theorem 4.1 is eO p
dimE(F) logN ·HK
, where dimE(F)is the eluder
dimension of the function class F.
C Proof of Theorem B.3
To start with, we first recall the concept of eluder dimension as follows.
2Agarwal et al. (2022) (Remark 4) also discussed the relationship between the generalized eluder dimension
and eluder dimension. However, there exists a technique flaw in the proof and we will discuss it in Appendix D.
15Definition C.1 (Definition of eluder dimension, Russo and Van Roy 2013) .The eluder dimension of
a function class Gwith domain Zis defined as follows:
•A point z∈ Z isϵ-dependent on z1, z2,···, zk∈ Z with respect to G, if for all g1, g2∈ Gsuch
thatPk
i=1[g1(zi)−g2(zi)]2≤ϵ, it holds that |g1(z)−g2(z)| ≤ϵ.
•Further zis said to be ϵ-independent of z1, z2,···, zkwith respect to G, ifzis not dependent on
z1, z2,···, zk.
•The eluder dimension of G, denoted by dimE(G, ϵ), is the length of the longest sequence of
elements in Zsuch that every element is ϵ′-independent of its predecessors for some ϵ′≥ϵ.
With this definition, we can prove Theorem B.3.
Proof of Theorem B.3. LetIj(1≤j≤ ⌈log2M/α⌉) be the index set such that
Ij=
t∈[T]|σt∈[2j−1·α,2jα]	
.
Then we focus on the summation over Ijfor each j. For simplicity, we denote the subsequence
{zi}i∈Ijby{xi}i∈[|Ij|]. Then we have
X
i∈Ijmin
1,1
σ2
iD2
G 
zi;z[i−1], σ[i−1]
≤X
i∈[|Ij|]min 
1,4D2
G 
xi;x[i−1],1[i−1]
.
To boundP
i∈[|Ij|]min 
1,4D2
G 
xi;x[i−1],1[i−1]
,
X
i∈[|Ij|]min 
1,4D2
G 
xi;x[i−1],1[i−1]
≤4/λ+X
i∈[|Ij|]4Z1
1/λT1
D2
G 
xi;x[i−1],1[i−1]
≥ρ	
dρ
≤4/λ+ 4Z1
1/λTX
i∈[|Ij|]1
D2
G 
xi;x[i−1],1[i−1]
≥ρ	
dρ. (C.1)
Then we proceed by bounding the summationP
i∈[|Ij|]1
D2
G 
xi;x[i−1],1[i−1]
≥ρ	
for each
ρ≥1/(λT). For short, let d:= dim E(G,1/√
T). Essentially, it suffices to provide an upper bound
of the cardinality of the subset Jj:=
i∈ Ij|D2
G 
xi;x[i−1],1[i−1]
≥ρ	
.
For each i∈ Jj, since D2
G 
xi;x[i−1],1[i−1]
≥ρ, there exists g1, g2inGsuch that
(g1(xi)−g2(xi))2
P
t∈[i−1](g1(xt)−g2(xt))2+λ≥ρ/2.
Here (g1(xi)−g2(xi))2≥λρ≥1/T. Denote such value of (g1(xi)−g2(xi))2byL(xi). Then
we consider split cJjinto⌈log2T⌉layers such that in each layer Jk
j(k∈[⌈log2T⌉]), we have
1/T≤ξ≤L(xi)≤2ξfor some ξ.
Denote the cardinality of Jk
jbyAand the subsequence {xi}i∈Ik
jby{yi}i∈[A]. For the elements in
{y}, we dynamically maintain ⌊A/d⌋queues of elements. We enumerate through {y}in its original
order and put the current element yiinto the queue Qsuch that yiisξ-independent of the current
elements in Q. By the Pigeonhole principle and the definition of eluder dimension d, we can find an
element yiin{y}such that yiisξ-dependent of all the ⌊A/d⌋queues before i.
Then by the definition of L(yi)andξ, we can choose g1, g2such that
(g1(yi)−g2(yi))2
P
t∈[i−1](g1(yt)−g2(yt))2+λ≥ρ/2,2ξ≥(g1(yt)−g2(yt))2≥ξ. (C.2)
16By the ξ-dependencies, we haveP
t∈[i−1](g1(yt)−g2(yt))2≥ ⌊A/d⌋ ·ξ. At the same time,P
t∈[i−1](g1(yt)−g2(yt))2≤4ξ/ρdue to (C.2) . Thus, we deduce that A=|Jk
j| ≤O(d/ρ)for all
k∈[⌈log2T⌉]. Substituting it into (C.1), we have
X
i∈[|Ij|]min 
1,4D2
G 
xi;x[i−1],1[i−1]
≤4/λ+ 4Z1
1/λTO(d/ρ·log2T)dρ
=O(dlog2T·logλT+λ−1).
Here jis chosen arbitrarily. Hence, the generalized eluder dimension can be further bounded as
follows:
X
i∈[T]min
1,1
σ2
iD2
G 
zi;z[i−1], σ[i−1]
≤O(dim E(F,1/√
T) logTlogλTlog(M/α ) +λ−1).
In the following part, we will also discuss the issue in Agarwal et al. (2022) about the relationship
between the standard eluder dimension dimEand the generalized eluder dimension dim. In detail,
Agarwal et al. (2022) proposed the concept of Generalized Eluder dimension and made the following
claim:
Lemma C.2 (Remark 4, Agarwal et al. 2022) .If we set the weight σ= 1, then the Generalized
Eluder dimension dim = supZ,σ:|Z|=Tdim(F,Z,1)satisfied
dim≤dimE(F,p
λ/T) + 1,
where dimEdenotes the standard Eluder dimension proposed in Russo and Van Roy (2013).
In the proof of Remark 4, Agarwal et al. (2022) claims that given the standard Eluder dimen-
sion dimE(F,√
λ) = n, there are at most ndifferent (sorted) indices {t1, t2, ..}such that
D2
F(zti;zti−1],1)≥ϵ2/λ. However, according to the definition of D2
F-uncertainty, we only have
D2
F(z;z[t−1],1) := sup
f1,f2∈F(f1(z)−f2(z))2
P
s∈[t−1](f1(zs)−f2(zs))2+λ.
However, ztiisϵ-dependence with z1, .., z ti−1is only a sufficient condition for the uncertainty
D2
F(z;z[t−1],1)rather than necessary condition. In cases where both (f1(z)−f2(z))2≥ϵ2andP
s∈[t−1](f1(zs)−f2(zs))2≥ϵ2hold, the uncertainty D2
F(z;z[t−1],1)may exceed ϵ2/λ, yet it
will not be counted within the longest ϵ-independent sequence for the standard Eluder dimension.
D Proof of Theorem 4.1
In this section, we provide the proof of Theorem 4.1.
D.1 High Probability Events
In this subsection, we define the following high-probability events:
Eef
k,h=

λ+X
i∈[k−1]
efk,h(si
h, ai
h)− T2
hVk,h+1(si
h, ai
h)2
≤eβ2
k

, (D.1)
Epf
k,h=

λ+X
i∈[k−1]1
¯σ2
i,h
pfk,h(si
h, ai
h)− ThVk,h+1(si
h, ai
h)2
≤β2
k

, (D.2)
Eqf
k,h=

λ+X
i∈[k−1]1
¯σ2
i,h
qfk,h(si
h, ai
h)− ThqVk,h+1(si
h, ai
h)2
≤β2
k

, (D.3)
17where
eβk:=r
128 logNϵ(k)·NF(ϵ)·H
δ+ 64Lϵ·k, β k:=r
128·logNϵ(k)·NF(ϵ)H
δ+ 64Lϵ·k/α2.
Eef
k,h,Epf
k,handEqf
k,hare the hoeffding-type concentration results for efk,h,pfk,handqfk,hrespectively.
Then we define the following bellman-type concentration events for pfk,h,qfk,h, which implies a
tighter confidence set due to carefully designed variance estimators and an inverse-variance weighted
regression scheme for the general function classes.
Epf
h=(
λ+k−1X
i=11
(¯σi,h′)2
pfk,h′(si
h′, ai
h′)− Th′Vk,h′+1(si
h′, ai
h′)2
≤pβ2
k,∀h≤h′≤H, k∈[K])
,
Eqf
h=(
λ+k−1X
i=11
(¯σi,h′)2
qfk,h′(si
h′, ai
h′)− Th′Vk,h′+1(si
h′, ai
h′)2
≤qβ2
k,∀h≤h′≤H, k∈[K])
,
(D.4)
where
pβ2
k=qβ2
k:=O 
log2k2 
2 log( L2k/α4) + 2
· 
log(4L/α2) + 2
δ/H!
·[log(NF(ϵ)) + 1]
+O(λ) +O(ϵkL/α2).
We also define the following events which are later applied to prove the concentration of pfk,hand
qfk,hby induction.
¯Epf
k,h=
λ+X
i∈[k−1]p1i,h
(¯σi,h)2
pfk,h(si
h, ai
h)− ThVk,h+1(si
h, ai
h)2
≤pβ2
k
, (D.5)
where p1i,his the shorthand for the following product of indicator functions
p1i,h:=1 
[VhV∗
h+1](si
h, ai
h)≤¯σ2
i,h
·1 
[ThVi,h+1− TV∗
h+1]≤(logNF(ϵ) + log Nϵ(K))−1¯σ2
i,h
·1 
Vi,h+1(s)≥V∗
h+1(s)∀s∈ S
. (D.6)
¯Eqf
k,h=
λ+X
i∈[k−1]q1i,h
(¯σi,h)2
qfk,h(si
h, ai
h)− ThqVk,h+1(si
h, ai
h)2
≤qβ2
k
, (D.7)
where q1i,his the shorthand for the following product of indicator functions
q1i,h:=1 
[VhV∗
h+1](si
h, ai
h)≤¯σ2
i,h
·1 
[TV∗
h+1− ThqVi,h+1]≤(logNF(ϵ) + log Nϵ(K))−1¯σ2
i,h
·1
qVi,h+1(s)≤V∗
h+1(s)∀s∈ S
. (D.8)
The following Lemmas suggest that previous events hold with high probability.
Lemma D.1. Letefk,hbe defined as in line 8 of Algorithm 1, we have Eef:=T
k≥1,h∈[H]Eef
k,hholds
with probability at least 1−δ, where Eef
k,his defined in (D.1).
Lemma D.2. Letpfk,hbe defined as in line 6 of Algorithm 1, we have Epf:=T
k≥1,h∈[H]Epf
k,hholds
with probability at least 1−δ, where Epf
k,his defined in (D.2).
Lemma D.3. Letqfk,hbe defined as in line 7 of Algorithm 1, we have Eqf:=T
k≥1,h∈[H]Eqf
k,hholds
with probability at least 1−δ, where Eqf
k,his defined in (D.3).
Lemma D.4. Letpfk,hbe defined as in line 6 of Algorithm 1, we have ¯Epf:=T
k≥1,h∈[H]¯Epf
k,hholds
with probability at least 1−2δ, where Epf
k,his defined in (D.5).
Lemma D.5. Letqfk,hbe defined as in line ·of Algorithm 1, we have ¯Eqfholds with probability at
least1−2δ.
18D.2 Optimism and Pessimism
Based on the high probability events, we have the following lemmas for the function pfk,h(s, a)and
qfk,h(s, a)
Lemma D.6. On the events Epf
handEqf
h, for each episode k∈[K], we have
pfk,h(s, a)− ThVk,h+1(s, a)≤pβkDFh(z;z[k−1],h,¯σ[k−1],h),
qfk,h(s, a)− ThqVk,h+1(s, a)≤qβkDFh(z;z[k−1],h,¯σ[k−1],h),
where z= (s, a)andz[k−1],h={z1,h, z2,h, .., z k−1,h}.
Lemma D.7. On the events Epf
h+1andEqf
h+1, for each stage h≤h′≤Hand episode k∈[K], we
haveQk,h(s, a)≥Q∗
h(s, a)≥qQk,h(s, a). Furthermore, for the value functions Vk,h(s)andqVk,h(s),
we have Vk,h(s)≥V∗
h(s)≥qVk,h(s).
D.3 Monotonic Variance Estimator
In this subsection, we introduce the following lemmas to construct the monotonic variance estimator.
Lemma D.8. On the events Eef,EpfandEqf, for each episode k∈[K]and stage h∈[H], we have
pfk,h(s, a)− ThVk,h+1(s, a)≤βkDFh(z;z[k−1],h,¯σ[k−1],h),
qfk,h(s, a)− ThqVk,h+1(s, a)≤βkDFh(z;z[k−1],h,¯σ[k−1],h),
efk,h(s, a)− T2
hVk,h+1(s, a)≤eβkDFh(z;z[k−1],h,¯σ[k−1],h),
where z= (s, a)andz[k−1],h={z1,h, z2,h, .., z k−1,h}.
Lemma D.9. On the events Eef,Epf,Eqf,Epf
h+1,Eqf
h+1, for each episode k∈[K], the empirical
variance [¯VhVk,h+1](sk
h, ak
h)satisfies the following inequalities:
[¯VhVk,h+1](sk
h, ak
h)−[VhVk,h+1](sk
h, ak
h)≤Ek,h,[¯VhVk,h+1](sk
h, ak
h)−[VhV∗
h+1](sk
h, ak
h)≤Ek,h+Fk,h.
Lemma D.10. On the events Epf,Eqf,Epf
h+1,Eqf
h+1, for each episode k∈[K]andi > k , we have
(logNF(ϵ) + log Nϵ(K))·
Vh(Vi,h+1−V∗
h+1)
(sk
h, ak
h)≤Fi,h,
(logNF(ϵ) + log Nϵ(K))·
Vh(V∗
h+1−qVi,h+1)
(sk
h, ak
h)≤Fi,h,
Based on previous lemmas, we can construct an optimistic estimator σk,hfor the transition variance.
Under this situation, the weighted regression have the following guarantee.
Lemma D.11. If the events Eef,Epf,Eqf,¯Epfand¯Eqfhold, then events Epf=Epf
1andEqf=Eqf
1hold.
D.4 Proof of Regret Bound
In the subsection, we first define the following high proSbability events to control the stochastic noise
from the transition process:
E1=
∀h′∈[H],KX
k=1HX
h=h′[Ph(Vk,h+1−Vπk
h+1)
(sk
h, ak
h)−KX
k=1HX
h=h′ 
Vk,h+1(sk
h+1)−Vπk
h+1(sk
h+1)
≤2vuutKX
k=1HX
h=h′[Vh(Vk,h+1−Vπk
h+1)](sk
h, ak
h) log(2 K2H/δ) + 2p
log(2K2H/δ) + 2 log(2 K2H/δ)
,
E2=
∀h′∈[H],KX
k=1HX
h=h′[Ph(Vk,h+1−qVk,h+1)
(sk
h, ak
h)−KX
k=1HX
h=h′ 
Vk,h+1(sk
h+1)−qVk,h+1(sk
h+1)
19≤2vuutKX
k=1HX
h=h′[Vh(Vk,h+1−qVk,h+1)](sk
h, ak
h) log(2 K2H/δ) + 2p
log(2K2H/δ) + 2 log(2 K2H/δ)
.
According to Freedman inequality (Lemma H.5), we directly have the following results.
Lemma D.12. Events E1andE2hold with probability at least 1−2δ.
Proof. Fix an arbitrary h∈[H]. Applying Lemma H.5, we have the following inequality:
KX
k=1HX
h′=hPh(Vk,h+1−Vπk
h+1)
(sk
h, ak
h)−KX
k=1HX
h′=h 
Vk,h+1(sk
h+1)−Vπk
h+1(sk
h+1)
≤2vuutKX
k=1HX
h′=h[Vh(Vk,h+1−Vπk
h+1)](sk
h, ak
h) log(2 K2H/δ) + 2p
log(2K2H/δ) + 2 log(2 K2H/δ)
(D.9)
Applying a union bound for (D.9) across all h∈[H], k≥0, we have Event E1holds with probability
at least 1−2δ.
Similarly, we also have the corresponding high-probability bound for E2.
Next, we need the following lemma to control the summation of confidence radius.
Lemma D.13. For any parameters β≥1and stage h∈[H], the summation of confidence radius
over episode k∈[K]is upper bounded by
KX
k=1min
βDFh(z;z[k−1],h,¯σ[k−1],h),1
≤(1 +βγ2) dim α,K(Fh) + 2βq
dimα,K(Fh)vuutKX
k=1(σ2
k,h+α2),
where z= (s, a)andz[k−1],h={z1,h, z2,h, .., z k−1,h}.
Then, we can decompose the total regret in the first Kepisode to the summation of variancePK
k=1PH
h=1σ2
k,has the following lemma.
Lemma D.14. On the events Epf=Epf
1,Eqf=Eqf
1andE1, for each stage h∈[H], the regret in the
firstKepisodes can be decomposed and controlled as:
KX
k=1 
Vk,h(sk
h)−Vπk
h(sk
h)
≤2CH(1 +χ)(1 + pβkγ2) dim α,K(F)
+ 4C(1 +χ)pβkq
dimα,K(F)vuutHKX
k=1HX
h=1(σ2
k,h+α2)
+ 2vuutKX
k=1HX
h′=h[Vh(Vk,h+1−Vπk
h+1)](sk
h, ak
h) log(2 K2H/δ) + 2p
log(2K2H/δ) + 2 log(2 K2H/δ)
and for all stage h∈[H], we further have
KX
k=1HX
h=1
Ph(Vk,h+1−Vπk
h+1)
(sk
h, ak
h)
≤2CH2(1 +χ)(1 + pβkγ2) dim α,K(F) + 4CH(1 +χ)pβkq
dimα,K(Fh)vuutHKX
k=1HX
h=1(σ2
k,h+α2)
20+H
2vuutKX
k=1HX
h=1[Vh(Vk,h+1−Vπk
h+1)](sk
h, ak
h) log(2 K2H/δ) + 2p
log(2K2H/δ) + 2 log(2 K2H/δ)
.
In addition, the gap between the optimistic value function Vk,h(s)and pessimistic value function
qVk,h(s)can be upper bounded by the following lemma.
Lemma D.15. On the events Epf=Epf
1,Eqf=Eqf
1andE2, for each stage h∈[H], the regret in the
firstKepisodes can be decomposed and controlled as:
KX
k=1 
Vk,h(sk
h)−qVk,h(sk
h)
)≤4CH(1 +χ)(1 + pβkγ2) dim α,K(Fh) (D.10)
+ 8C(1 +χ)pβkq
dimα,K(F)vuutHKX
k=1HX
h=1(σ2
k,h+α2)
+ 2vuutKX
k=1HX
h′=h[Vh(Vk,h+1−qVk,h+1)](sk
h, ak
h) log(2 K2H/δ) + 2p
log(2K2H/δ) + 2 log(2 K2H/δ)
and for all stage h∈[H], we further have
KX
k=1HX
h=1
Ph(Vk,h+1−qVk,h+1)
(sk
h, ak
h)
≤4CH2(1 +χ)(1 + pβkγ2) dim α,K(Fh) + 8CH(1 +χ)pβkq
dimα,K(F)vuutHKX
k=1HX
h=1(σ2
k,h+α2)
+ 2H
2vuutKX
k=1HX
h′=h[Vh(Vk,h+1−qVk,h+1)](sk
h, ak
h) log(2 K2H/δ) + 2p
log(2K2H/δ) + 2 log(2 K2H/δ)
.
We define the following high probability event E3to control the summation of variance.
E3=KX
k=1HX
h=1[VhVπk
h+1](sk
h, ak
h)≤3K+ 3Hlog(1/δ)
.
According to Jin et al. (2018) (Lemma C.5)3, with probability at least 1−δ. Condition on this event,
the summation of variancePK
k=1PH
h=1σ2
k,hcan be upper bounded by the following lemma.
Lemma D.16. On the events E1,E2,E3,Epf=Epf
1andEqf=Eqf
1, the total estimated variance is upper
bounded by:
KX
k=1HX
h=1σ2
k,h≤(logNF(ϵ) + log Nϵ(K))×eO 
(1 +γ2)(βk+Hpβk+eβk)Hdimα,K(F)
+ (log NF(ϵ) + log Nϵ(K))2×eO 
(βk+Hpβk+eβk)2Hdimα,K(F)
+eO(Var K+KHα2).
With all previous lemmas, we can prove Theorem 4.1
Proof of Theorem 4.1. The low switching cost result is given by Lemma G.1.
3Jin et al. (2018) showed thatPK
k=1PH
h=1[VhVπk
h+1](sk
h, ak
h) = eO(KH2+H3)whenPh= 1Hrh(sh, ah)≤H. In this work, we assume the total reward satisfiedPH
h=1rh(sh, ah)≤1, and the
summation of variance is upper bounded by eO(K+H)
21After taking a union bound, the high probability events E1,E2,E3,Eef,Epf,Eqf,¯Epfand¯Eqfhold with
probability at least 1−10δ. Conditioned on these events, the regret is upper bounded by
Regret( K)
=KX
k=1 
V∗
1(sk
1)−Vπk
k,1(sk
1)
≤KX
k=1 
Vk,1(sk
1)−Vπk
k,1(sk
1)
≤2CH(1 +χ)(1 + pβKγ2) dim α,K(F) +eO
vuutKX
k=1HX
h=1[Vh(Vk,h+1−Vπk
h+1)](sk
h, ak
h)

+ 4C(1 +χ)pβKq
dimα,K(F)vuutHKX
k=1HX
h=1(σ2
k,h+α2)
≤2CH(1 +χ)(1 + pβKγ2) dim α,K(F) +eO
vuutKX
k=1HX
h=1[Ph(Vk,h+1−Vπk
h+1)](sk
h, ak
h)

+ 4C(1 +χ)pβKq
dimα,K(F)vuutHKX
k=1HX
h=1(σ2
k,h+α2)
≤2CH(1 +χ)(1 + pβKγ2) dim α,K(F) +eO
pβKq
dimα,K(F)vuutHKX
k=1HX
h=1(σ2
k,h+α2)

≤eO
Hdimα,K(F)·(1 + pβKγ2)
+eO
pβKq
dimα,K(F)·p
HVarK
+eO
Hdimα,K(F)pβK(βK+HpβK+eβK)·log[NF(ϵ)·Nϵ(K)]
+eO
Hdimα,K(F)pβK(1 +γ2)
=eO
H3dim2
α,K(F)·logNF1
2KHL
·log
NF1
2KHL
·N
B,1
2KHL pβK
+eO 
H2.5dim2.5
α,K(F)·s
logNF1
2KHL
·log1.5
NF1
2KHL
·N
B,1
2KHL pβK!
+eO s
dimα,K(F) logNF1
2KHL
·HVarK!
, (D.11)
where the first inequality holds due to Lemma D.7, the second inequality holds due to Lemma D.14,
the third inequality follows from Lemma D.7 and Var[X]≤E[X]·Mif random variable Xis in
[0, M], the fourth inequality holds due to (F.41) and2ab≤a2+b2, the last inequality holds due to
Lemma D.16. Thus, we complete the proof of Theorem 4.1.
We can reorganize (D.11) into the following upper bound for regret,
Regret( K) =eO(p
dim(F) logN·HVarK)
+eO
H2.5dim2(F)p
logNlog(N·Nb)·p
HlogN+ dim( F) log( N·Nb)
,
where we denote the covering number of bonus function class N
B,1
2KHL pβK
byNb, the covering
number of function class FbyNand the dimension dimα,K(F)bydim(F). Since E3occurs with
high probability according to Jin et al. (2018) (Lemma C.5), we have VarK=eO(K), which matches
the worst-case minimax optimal regret bound of MDPs with general function approximation.
22D.5 Proof Sketch of Regret Bound
In this subsection, we provide a proof sketch on the regret bound and show how our policy-switching
strategy works.
First, we introduce the following lemma which illustrates the stability of the bonus function after
using the uncertainty-based policy-switching condition.
Lemma D.17 (Restatement of Lemma G.2) .If the policy is not updated at episode k, the uncertainty
of all state-action pair z= (s, a)∈ S ×A and stage h∈[H]satisfies the following stability property:
D2
Fh(z;z[k−1],h,¯σ[k−1],h)≥1
1 +χD2
Fh(z;z[klast−1],h,¯σ[klast−1],h).
With Lemma G.2, we can then convert D2
Fh(z;z[klast−1],h,¯σ[klast−1],hin the bonus term into
D2
Fh(z;z[k−1],h,¯σ[k−1],h)with a cost of an additional 1 +χconstant.
As a direct consequence of Lemma G.2, We have the following lemma, which controls the gap
between optimistic value functions and real value functions resulting from the exploration bonuses.
Lemma D.18 (Restatement of Lemma D.14) .On the events Epf=Epf
1,Eqf=Eqf
1andE1, for each
stage h∈[H], the regret in the first Kepisodes can be decomposed and controlled as:
KX
k=1 
Vk,h(sk
h)−Vπk
h(sk
h)
≤2CH(1 +χ)(1 + pβkγ2) dim α,K(F)
+ 4C(1 +χ)pβkq
dimα,K(F)vuutHKX
k=1HX
h=1(σ2
k,h+α2)
+ 2vuutKX
k=1HX
h′=h[Vh(Vk,h+1−Vπk
h+1)](sk
h, ak
h) log(2 K2H/δ) + 2p
log(2K2H/δ) + 2 log(2 K2H/δ)
and for all stage h∈[H], we further have
KX
k=1HX
h=1
Ph(Vk,h+1−Vπk
h+1)
(sk
h, ak
h)
≤2CH2(1 +χ)(1 + pβkγ2) dim α,K(F) + 4CH(1 +χ)pβkq
dimα,K(Fh)vuutHKX
k=1HX
h=1(σ2
k,h+α2)
+H
2vuutKX
k=1HX
h=1[Vh(Vk,h+1−Vπk
h+1)](sk
h, ak
h) log(2 K2H/δ) + 2p
log(2K2H/δ) + 2 log(2 K2H/δ)
.
As a need to bound the sum of bonuses corresponding to a weighted regression target, we derive the
following results on the sum of inverse weights.
Lemma D.19 (Informal version of Lemma D.16) .With high probability, the total estimated variance
is upper bounded by:
KX
k=1HX
h=1σ2
k,h≤(logNF(ϵ) + log Nϵ(K))×eO 
(1 +γ2)(βk+Hpβk+eβk)Hdimα,K(F)
+ (log NF(ϵ) + log Nϵ(K))2×eO 
(βk+Hpβk+eβk)2Hdimα,K(F)
+eO(Var K+KHα2).
Proof sketch of Theorem 4.1. As the result of optimism, we have shown that with high probability,
for all k, h∈[K]×[H],Vk,his an upper bound for V∗
h. Hence, we further have,
Regret( K) =KX
k=1 
V∗
1(sk
1)−Vπk
k,1(sk
1)
≤KX
k=1 
Vk,1(sk
1)−Vπk
k,1(sk
1)
.
23To further proceed, we apply Lemma D.14 and obtain
Regret( K)≤2CH(1 +χ)(1 + pβKγ2) dim α,K(F) +eO
vuutKX
k=1HX
h=1[Vh(Vk,h+1−Vπk
h+1)](sk
h, ak
h)

+ 4C(1 +χ)pβKq
dimα,K(F)vuutHKX
k=1HX
h=1(σ2
k,h+α2),
where the second term can be bounded by applying Lemma D.14 repeatedly and the third term can
be controlled by Lemma D.16.
Then after substituting the value of pbeta K, γandα, we conclude that with high probability,
Regret( K) =eO(p
dim(F) logN·HVarK)
+eO
H2.5dim2(F)p
logNlog(N·Nb)·p
HlogN+ dim( F) log( N·Nb)
.
Please refer to Subsection D.4 for the detailed calculation of this part.
E Proof of Theorem B.1
In this section, we provide the proof of Theorem B.1. To prove the lower bound, we create a series
of hard-to-learn MDPs as follows. Each hard-to-learn MDP comprises d/4distinct sub-MDPs
denoted as M1, ..,Md/4. Each sub-MDP Miis characterized by two distinct states, initial state si,0
and absorbing state si,1, and shares the same action set A={a0, a1}. Since the state and action
spaces are finite, these tabular MDPs can always be represented as linear MDPs with dimension
|S| × |A| =d.
To generate each sub-MDP Mi, for all stage h∈[H], a special action ai,his uniformly randomly
selected from the action set {a0, a1}. Given the current state si,0, the agent transitions to the state
si,0if it takes the special action ai,h. Otherwise, the agent transitions to the absorbing state si,1and
remains in that state in subsequent stages. The agent will receive the reward 1if it takes the special
action ai,Hat the state si,0during the last stage H. Otherwise, the agent always receives reward 0.
In this scenario, for sub-MDP Mi, the optimal policy entails following the special action sequence
(ai,1, ai,2, ..., a i,H)to achieve a total reward of 1. In contrast, any other action sequence fails to yield
any reward.
Now, we partition the Kepisodes to d/4different distinct epochs. For each epoch (ranging from
episodes 4(i−1)K/d+ 1to episode 4iK/d ), we initialize the state as si,0and exclusively focus on
the sub-MDP Mi. The regret in each epoch can be lower bounded separately as follows:
Lemma E.1. For each epoch i∈[d/4]and any algorithm Algcapable of deploying arbitrary policies,
if the expected switching cost in epoch iis less than H/(2 log K), the expected regret of Algin the
i-th epoch is at least Ω(K/d).
Proof of Lemma E.1. Given that each sub-MDP is independently generated, policy updates before
epoch ionly offer information for the sub-MDPs M1, ...,Mi−1and do not provide any information
for the current epoch i. In this scenario, there is no distinction between epochs and for simplicity, we
only focus on the first epoch, encompassing episodes 1to4K/d .
Now, let k0= 0 and we denote K={k1, k2, ...}as the set of episodes where the algorithm Alg
updates the policy. If Algdoes not update the policy itimes, we set ki= 4K/d+ 1. For simplicity,
we set C= 2 log Kand for each i≤H/C , we define the events Eas the algorithm Alghas not
reached the state s1,0at the stage iCbefore the episode ki.
Conditioned on the events E1, ...,Ei−1, the algorithm Algdoes not gather any information about
the special action a1,hfor stage h≥(i−1)C+ 1. In this scenario, the special actions can still be
considered as uniformly randomly selected from the action set a0, a1. For each episode between ki−1
andki, the probability that a policy πarrives at state s1,0at stage iCis upper-bounded by:
Ea1,(i−1)C+1,...,a 1,iC
ΠiC
h=1Pr(πh(s1,0) =a1,h)
≤Ea1,(i−1)C+1,...,a 1,iC
ΠiC
h=(i−1)C+1Pr(πh(s1,0) =a1,h)
=1
2C,
24where the first inequality holds due to π(s1,0) =a1,h)≤1and the second equation holds due to the
random generation process of the special actions. Notice that there are at most Kepisodes between
theki−1andkiand after applying an union bound for all episodes, we have
Pr(Ei|E1, ...,Ei−1)≤1−K
2C= 1−1
K.
Furthermore, we have
Pr(EH/C)≤Pr(E1∩ E2∩ E3∩...∩ EH/C) = ΠH/C
i=1Pr(Ei|E1, ...,Ei−1)≤(1−1
1K)H/C≤1−H
CK.
(E.1)
Notice that the agent cannot receive any reward unless it has reached the state si,0at the last stage
H. Therefore, the expected regret for the algorithm Algfor the sub-MDP Mcan be bounded by the
switching cost δ:
EM[Regret (Alg)]≥EM[1(EH/C)×(kH/C−1)]
≥EM[kH/C−1]−H
C
≥EM
1(δ < H/C )·K/d
−H
C
≥4K/d−EM
δ
·4KC
dH−H
C, (E.2)
where the first inequality holds due to the fact that the agent receives no reward before kH/C
conditioned on the event EH/C, the second inequality holds due to (E.1) withkH/C−1≤K, the
third inequality holds due to the definition of kH/C and the last inequality holds due to E[1(x≥
a)]≤E[x]/afor any non-negative random variable x. According to the result in (E.1) , if the expected
switching E[δ]≤H/(2C), the expected regret is at least Ω(K), when Kis large enough compared
toH.
With Lemma B.1, we can prove Theorem B.1.
Proof of Theorem B.1. For these constructed hard-to-learn MDPs and any given algorithm Alg, we
denote the expected switching cost for sub-MDP Miasδi.
According to Lemma E.1, we have
EMi[Regret (Alg)]≥Pr 
δi< H/ (2 log K)
·K/d
≥ 
1−EMi[δi]·2 logK/H
·K/d, (E.3)
where the first inequality holds due to lemma B.1 and last inequality holds due to E[1(x≥a)]≤
E[x]/afor any non-negative random variable x. Taking a summation of (E.3) for all sub-Mdps, we
have
EM[Regret (Alg)] =d/4X
i=1EMi[Regret (Alg)]
≥d/4X
i=1 
1−EMi[δi]·2 logK/H
·K/d
= (d/4−EM[δ]·2 logK/H )·K/d,
where δis the total expected switching cost. Therefore, for any algorithm with total expected
switching cost less than dH/(16 log K), the expected is lower bounded by
EM[Regret (Alg)]≥(d/4−EM[δ]·2 logK/H )·K/d≥K/8.
Thus, we finish the proof of Theorem B.1.
25F Proof of Lemmas in Appendix D
In this section, we provide the detailed proof of lemmas in Appendix D.
F.1 Proof of High Probability Events
F.1.1 Proof of Lemma D.1
Proof of Lemma D.1. We first prove that for an arbitrarily chosen h∈[H],Eef
k,hholds with probabil-
ity at least 1−δ/H for all k.
For simplicity, in this proof, we denote T2
hVk,h+1(si
h, ai
h)asef∗
k(si
h, ai
h)where ef∗
k∈ F hexists
due to our assumption. For any function V:S → [0,1], leteηk
h(V) = 
rk
h+V(sk
h+1)2−
Es′∼sk
h,ak
hh 
r(sk
h, ak
h, s′) +V(s′)2i
.
By simple calculation, for all f∈ Fh, we have
X
i∈[k−1]
f(si
h, ai
h)−ef∗
k(si
h, ai
h)2
+ 2X
i∈[k−1]
f(si
h, ai
h)−ef∗
k(si
h, ai
h)
·eηi
h(Vk,h+1)
| {z }
I(f,ef∗
k,Vk,h+1)
=X
i∈[k−1]h 
ri
h+Vk,h+1(si
h)2−f(si
h, ai
h)i2
−X
i∈[k−1]h 
ri
h+Vk,h+1(si
h)2−ef∗
k(si
h, ai
h)i2
.
Due to the definition of efk,h, we have
X
(i,j)∈[k−1]×[H]
efk,h(si
j, ai
j)−ef∗
k(si
j, ai
j)2
+ 2I(efk,h,ef∗
k, Vk,h+1)≤0. (F.1)
Then we give a high probability bound for −I(f,ef∗
k, Vk,h+1)through the following calculation.
Applying Lemma H.4, for fixed f,¯fandV, with probability at least 1−δ,
−I(f,¯f, V) :=−X
i∈[k−1] 
f(si
h, ai
h)−¯f(si
h, ai
h)
·eηi
h(V)
≤8λX
i∈[k−1] 
f(si
h, ai
h)−¯f(si
h, ai
h)2+1
λ·log1
δ.
By the definition of Vin line ·of Algorithm 1, Vk,h+1lies in the optimistic value function class Vk
defined in (G.4) . Applying a union bound for all the value functions Vcin the corresponding ϵ-net
Vc, we have
−I(f,¯f, Vc)≤1
4X
i∈[k−1] 
f(si
h, ai
h)−¯f(si
h, ai
h)2+ 32·logNϵ(k)
δ
holds for all kwith probability at least 1−δ.
For all Vsuch that ∥V−Vc∥∞≤ϵ, we have |ηi
h(V)−ηi
h(Vc)| ≤4ϵ. Therefore, with probability
1−δ, the following bound holds for I(f,¯f, Vk,h+1):
−I(f,¯f, Vk,h+1)≤1
4X
i∈[k−1] 
f(si
h, ai
h)−¯f(si
h, ai
h)2+ 32·logNϵ(k)
δ+ 4ϵ·k.
To further bound I(efk,h,ef∗
k, Vk,h+1)in(F.1) , we apply an ϵ-covering argument on Fhand show that
with probability at least 1−δ,
−I(efk,h,ef∗
k, Vk,h+1)≤1
4X
i∈[k−1]
efk,h(si
h, ai
h)−ef∗
k(si
h, ai
h)2
+ 32·logNϵ(k)·NF(ϵ)
δ
26+ 16Lϵ·k (F.2)
for probability at least 1−δ.
Substituting (F.2) into (F.1) and rearranging the terms,
X
i∈[k−1]
efk,h(si
h, ai
h)−ef∗
k(si
h, ai
h)2
≤128 logNϵ(k)·NF(ϵ)·H
δ+ 64Lϵ·k
for all kwith probability at least 1−δ/H .
Finally, we apply a union bound over all h∈[H]to conclude that Eefholds with probability at least
1−δ.
F.1.2 Proof of Lemmas D.2 and D.3
Proof of Lemma D.2. Similar to the proof of Lemma D.1, we first prove that for an arbitrarily chosen
h∈[H],Epf
k,hholds with probability at least 1−δ/H for all k.
In this proof, we denote ThVk,h+1(si
h, ai
h)aspf∗
k(si
h, ai
h)where pf∗
k∈ Fhexists due to our assumption.
For any function V:S → [0,1], letpηk
h(V) = 
rk
h+V(sk
h+1)
−Es′∼sk
h,ak
h
rh(sk
h, ak
h, s′) +V(s′)
.
For all f∈ Fh, we have
X
i∈[k−1]1
(¯σi,h)2
f(si
h, ai
h)−pf∗
k(si
h, ai
h)2
+ 2X
i∈[k−1]1
(¯σi,h)2
f(si
h, ai
h)−pf∗
k(si
h, ai
h)
·pηi
h(Vk,h+1)
| {z }
I(f,pf∗
k,Vk,h+1)
=X
i∈[k−1]1
(¯σi,h)2
ri
h+Vk,h+1(si
h)2−f(si
h, ai
h)2−X
i∈[k−1]1
(¯σi,h)2h
ri
h+Vk,h+1(si
h)−pf∗
k(si
h, ai
h)i2
.
By the definition of pfk,h, we have
X
i∈[k−1]1
(¯σi,h)2
pfk,h(si
h, ai
h)−pf∗
k(si
h, ai
h)2
+ 2I(pfk,h, Vk,h+1)≤0. (F.3)
Then we give a high probability bound for −I(pfk,h,pf∗
k, Vk,h+1)through the following calculation.
Applying Lemma H.4, for fixed f,¯fandV, with probability at least 1−δ,
−I(f,¯f, V) :=−X
i∈[k−1]1
(¯σi,h)2 
f(si
h, ai
h)−¯f(si
h, ai
h)
·pηi
h(V)
≤8λ1
α2X
i∈[k−1]1
(¯σi,h)2 
f(si
h, ai
h)−¯f(si
h, ai
h)2+1
λ·log1
δ.
By the definition of Vin line ·of Algorithm 1, Vk,h+1lies in the optimistic value function class Vk
defined in (G.4) . Applying a union bound for all the value functions Vcin the corresponding ϵ-net
Vc, we have
−I(f,¯f, Vc)≤1
4X
i∈[k−1]1
(¯σi,h)2 
f(si
h, ai
h)−¯f(si
h, ai
h)2+32
α2·logNϵ(k)
δ
holds for all kwith probability at least 1−δ.
For all Vsuch that ∥V−Vc∥∞≤ϵ, we have |ηi
h(V)−ηi
h(Vc)| ≤4ϵ. Therefore, with probability
1−δ, the following bound holds for I(f,¯f, Vk,h+1):
−I(f,¯f, Vk,h+1)≤1
4X
i∈[k−1]1
(¯σi,h)2 
f(si
h, ai
h)−¯f(si
h, ai
h)2+32
α2·logNϵ(k)
δ+ 4ϵ·k/α2.
27To further bound I(pfk,h,pf∗
k, Vk,h+1)in(F.3) , we apply an ϵ-covering argument on Fhand show that
with probability at least 1−δ,
−I(pfk,h,pf∗
k, Vk,h+1)≤1
4X
i∈[k−1]1
(¯σi,h)2
pfk,h(si
h, ai
h)−pf∗
k(si
h, ai
h)2
+ 32·logNϵ(k)·NF(ϵ)
δ
+ 16Lϵ·k/α2. (F.4)
Substituting (F.4) into (F.3) and rearranging the terms, we have
X
i∈[k−1]1
(¯σi,h)2
pfk,h(si
h, ai
h)−pf∗
k(si
h, ai
h)2
≤128·logNϵ(k)·NF(ϵ)H
δ+ 64Lϵ·k/α2
for all kwith probability at least 1−δ/H . Then we can complete the proof by using a union bound
over all h∈[H].
Proof of Lemma D.3. The proof is almost identical to the proof of Lemma D.2.
F.1.3 Proof of Lemmas D.4 and D.5
Proof of Lemma D.4. Similar to the proof of Lemma D.1, we first prove that for an arbitrary h∈[H],
¯Epf
k,hholds with probability at least 1−δ/H for all k.
In this proof, we denote ThVk,h+1(si
h, ai
h)aspf∗
k(si
h, ai
h)where pf∗
k∈ F exists due to our assumption.
For any function V:S → [0,1], letpηk
h(V) = 
rk
h+V(sk
h+1)
−Es′∼sk
h,ak
h
rh(sk
h, ak
h, s′) +V(s′)
.
For all f∈ Fh, we have
X
i∈[k−1]p1i,h
(¯σi,h)2
f(si
h, ai
h)−pf∗
k(si
h, ai
h)2
+ 2X
i∈[k−1]p1i,h
(¯σi,h)2
f(si
h, ai
h)−pf∗
k(si
h, ai
h)
·pηi
h(Vk,h+1)
| {z }
I(f,pf∗
k,Vk,h+1)
=X
i∈[k−1]p1i,h
(¯σi,h)2
ri
h+Vk,h+1(si
h)2−f(si
h, ai
h)2−X
i∈[k−1]p1i,h
(¯σi,h)2h
ri
h+Vk,h+1(si
h)−pf∗
k(si
h, ai
h)i2
.
Due to the definition of pfk,h, we have
X
i∈[k−1]p1i,h
(¯σi,h)2
pfk,h(si
h, ai
h)−pf∗
k(si
h, ai
h)2
+ 2I(pfk,h, Vk,h+1)≤0. (F.5)
Then it suffices to bound the value of I(f,¯f, Vk,h+1)for all f,¯f∈ F.
Unlike the proof for Lemma D.1, we decompose I(f,¯f, Vk,h+1)into two parts:
I(f,¯f, Vk,h+1) =X
i∈[k−1]p1i,h
(¯σi,h)2 
f(si
h, ai
h)−¯f(si
h, ai
h)
·pηi
h(V∗
h+1)
+X
i∈[k−1]p1i,h
(¯σi,h)2 
f(si
h, ai
h)−¯f(si
h, ai
h)
·pηi
h(Vk,h+1−V∗
h+1). (F.6)
Then we bound the two terms separately.
For the first term, we first check the following conditions before applying Lemma H.2, which is a
variant of Freedman inequality. For fixed fand¯f, we have
E"
p1i,h
(¯σi,h)2 
f(si
h, ai
h)−¯f(si
h, ai
h)
·pηi
h(V∗
h+1)#
= 0,
since si
h+1is sampled from Ph(·|si
h, ai
h).
28Next, we need to derive a bound for the maximum absolute value of each ‘weighted’ transition noise:
max
i∈[k−1]p1i,h
(¯σi,h)2 
f(si
h, ai
h)−¯f(si
h, ai
h)
·pηi
h(V∗
h+1)
≤4 max
i∈[k−1]p1i,h
(¯σi,h)2 
f(si
h, ai
h)−¯f(si
h, ai
h)
≤4 max
i∈[k−1]1
(¯σi,h)2vuuutD2
Fh 
zi,h;z[i−1],h,¯σ[i−1],h
X
τ∈[k−1]p1τ,h
(¯στ,h)2 
f(sτ
h, aτ
h)−¯f(sτ
h, aτ
h)2+λ

≤4·γ−2vuutX
τ∈[k−1]p1τ,h
(¯στ,h)2 
f(sτ
h, aτ
h)−¯f(sτ
h, aτ
h)2+λ, (F.7)
where the second inequality follows from the definition of DFhin Definition 2.4, the last inequality
holds since ¯σi,h≥γ·D1/2
Fh 
zi,h;z[i−1],h,¯σ[i−1],h
according to line 19 in Algorithm 1. From the
definition of p1i,hin (D.6) we directly obtain the following upper bound of the variance:
X
i∈[k−1]E"
p1i,h
(¯σi,h)4 
f(si
h, ai
h)−¯f(si
h, ai
h)2·pηi
h(V∗
h+1)2#
≤4X
i∈[k−1]p1i,h
(¯σi,h)2 
f(si
h, ai
h)−¯f(si
h, ai
h)2≤4L2k/α2:=V.
Applying Lemma H.2 with V= 4L2k/α2,M= 2L/α2andv=m= 1, for fixed f,¯f,k, with
probability at least 1−δ/(2k2·NF(ϵ)·H),
−X
i∈[k−1]p1i,h
(¯σi,h)2 
f(si
h, ai
h)−¯f(si
h, ai
h)
·pηi
h(V∗
h+1) (F.8)
≤ιvuuut2
8X
i∈[k−1]p1i,h
(¯σi,h)2 
f(si
h, ai
h)−¯f(si
h, ai
h)2+ 1

+2
3ι2
8γ−2vuutX
τ∈[k−1]p1τ,h
(¯στ,h)2 
f(sτ
h, aτ
h)−¯f(sτ
h, aτ
h)2+λ+ 1
 (F.9)
≤
4ι+16
3ι2γ−2vuutX
τ∈[k−1]p1τ,h
(¯στ,h)2 
f(sτ
h, aτ
h)−¯f(sτ
h, aτ
h)2+λ+√
2ι+2
3ι2, (F.10)
where ι:=ι1(k, h, δ ) =r
log2k2
2 log(4L2k
α2)+2
·(log(4L
α2)+2)·NF(ϵ)
δ/H.
Using a union bound across all f,¯f∈ C(Fh, ϵ)andk≥1, with probability at least 1−δ/H ,
−X
i∈[k−1]p1i,h
(¯σi,h)2 
f(si
h, ai
h)−¯f(si
h, ai
h)
·pηi
h(V∗
h+1)
≤
4ι+16
3ι2γ−2vuutX
τ∈[k−1]p1τ,h
(¯στ,h)2 
f(sτ
h, aτ
h)−¯f(sτ
h, aτ
h)2+λ+√
2ι+2
3ι2(F.11)
holds for all f,¯f∈ C(Fh, ϵ)andk. Due to the definition of ϵ-net, we deduce that for pfk,handpf∗
k,
−X
i∈[k−1]p1i,h
(¯σi,h)2
pfk,h(si
h, ai
h)−pf∗
k(si
h, ai
h)
·pηi
h(V∗
h+1)
29≤
4ι+16
3ι2γ−2vuutX
τ∈[k−1]p1τ,h
(¯στ,h)2
pfk,h(sτ
h, aτ
h)−pf∗
k(sτ
h, aτ
h)2
+λ+√
2ι+2
3ι2
+
4ι+16
3ι2γ−2p
16kϵL/α2+ 8ϵk/α2. (F.12)
For the second term in (F.6), the following inequality holds for all V∈ Vk,h+1,
E1
(¯σi,h)2 
f(si
h, ai
h)−¯f(si
h, ai
h)
·pηi
h(V−V∗
h+1)
= 0,
max
i∈[k−1]1
(¯σi,h)2 
f(si
h, ai
h)−¯f(si
h, ai
h)
·pηi
h(V−V∗
h+1)
≤4 max
i∈[k−1]1
(¯σi,h)2 
f(si
h, ai
h)−¯f(si
h, ai
h)
≤4 max
i∈[k−1]1
(¯σi,h)2vuutD2
Fh 
zi,h;z[i−1],h,¯σ[i−1],hX
τ∈[k−1]1
(¯στ,h)2 
f(sτ
h, aτ
h)−¯f(sτ
h, aτ
h)2+λ
= 4·γ−2vuutX
τ∈[k−1]1
(¯στ,h)2 
f(sτ
h, aτ
h)−¯f(sτ
h, aτ
h)2+λ,
where the calculation is similar to that in (F.7).
We denote the sum of variance by Var(V−V∗
h+1)as follows for simplicity:
Var(V−V∗
h+1) :=X
i∈[k−1]E"
p1i,h
(¯σi,h)4
f(si
h, ai
h)−¯f(si
h, ai
h)2
·pηi
h(V−V∗
h+1)2#
(F.13)
≤k·L2/α4. (F.14)
ForVk,h+1, we have
Var(Vk,h+1−V∗
h+1) :=X
i∈[k−1]E"
p1i,h
(¯σi,h)4 
f(si
h, ai
h)−¯f(si
h, ai
h)2·pηi
h(Vk,h+1−V∗
h+1)2#
≤4
logNF(ϵ) + log Nϵ(K)X
i∈[k−1]p1i,h
(¯σi,h)2 
f(si
h, ai
h)−¯f(si
h, ai
h)2,
where the second inequality holds due to the definition of p1i,hin (D.6).
With a similar argument as shown in (F.8)∼(F.12) , we have with probability at least 1−δ/(2k2·
NF(ϵ)·Nϵ(k−1)·H), for a fixed f,¯f,kandV, (applying Lemma H.2, with V=k·L2/α4and
M= 2L/α2,v= (log NF(ϵ) + log Nϵ(K))−1/2,m=v2. )
−X
i∈[k−1]p1i,h
(¯σi,h)2 
f(si
h, ai
h)−¯f(si
h, ai
h)
·pηi
h(Vk,h+1−V∗
h+1)
≤ιr
2
2Var( V−V∗
h+1) + (log NF(ϵ) + log Nϵ(K))−1
+2
3ι2
8γ−2vuutX
τ∈[k−1]p1τ,h
(¯στ,h)2 
f(sτ
h, aτ
h)−¯f(sτ
h, aτ
h)2+λ+ (log NF(ϵ) + log Nϵ(K))−1
.
where
log2k2
2 logL2k(logNF(ϵ)·Nϵ(K))1/2
α4 + 2
·
log(4L(logNF(ϵ)·Nϵ(K))
α2 ) + 2
·NF(ϵ)·Nϵ(k)
δ/H
30≤log2k2
2 logL2k
α4+ 2
· 
log(4L
α2) + 2
·N4
F(ϵ)·N2
ϵ(K)
δ/H:=ι2
2(k, h, δ ).
Using a union bound over all (f,¯f, V)∈ C(Fh, ϵ)× C(Fh, ϵ)× Vc
k,h+1and all k≥1, we have the
inequality above holds for all such f,¯f, V, k with probability at least 1−δ/H .
There exists a Vc
k,h+1in the ϵ-net such that ∥Vk,h+1−Vc
k,h+1∥∞≤ϵ. Then we have
−X
i∈[k−1]p1i,h
(¯σi,h)2
pfk,h(si
h, ai
h)−pf∗
k(si
h, ai
h)
·pηi
h(Vk,h+1−V∗
h+1)
≤O 
ι2(k, h, δ )p
logNF(ϵ) + log Nϵ(K)+ι2(k, h, δ )2
γ−2!
·vuutX
τ∈[k−1]p1τ,h
(¯στ,h)2
pfk,h(sτ
h, aτ
h)−pf∗
k(sτ
h, aτ
h)2
+λ
+O(ϵkL/α )2) +Oι2
2(k, h, δ )
logNF(ϵ) + log Nϵ(K)
, (F.15)
for all kwith at least probability 1−δ/H .
Substituting (F.15) and (F.12) into (F.5), we can conclude that
X
i∈[k−1]p1i,h
(¯σi,h)2
pfk,h(si
h, ai
h)−pf∗
k(si
h, ai
h)2
≤O
 
ι2(k, h, δ )p
logNF(ϵ) + log Nϵ(K)+ι2(k, h, δ )2·γ−2!2
+O 
ι1(k, h, δ ) +ι1(k, h, δ )2/γ22
.
From the definition of γin (3.3), we can rewrite the upper bound of the squared loss
P
i∈[k−1]p1i,h
(¯σi,h)2
pfk,h(si
h, ai
h)−pf∗
k(si
h, ai
h)2
as follows:
λ+X
i∈[k−1]p1i,h
(¯σi,h)2
pfk,h(si
h, ai
h)−pf∗
k(si
h, ai
h)2
≤O
log2k2
2 logL2k
α4+ 2
· 
log(4L
α2) + 2
δ/H
·[log(NF(ϵ)) + 1] + O(λ) +O(ϵkL/α2).
Proof of Lemma D.5. The proof is almost identical to the proof of Lemma D.4.
F.2 Proof of Optimism and Pessimism
F.2.1 Proof of Lemma D.6
Proof of Lemma D.6. According to the definition of D2
Ffunction, we have
 pfk,h(s, a)− ThVk,h+1(s, a)2
≤D2
Fh(z;z[k−1],h,¯σ[k−1],h)× 
λ+k−1X
i=11
(¯σi,h)2
pfk,h(si
h, ai
h)− ThVk,h+1(si
h, ai
h)2!
≤pβ2
k×D2
Fh(z;z[k−1],h,¯σ[k−1],h),
31where the first inequality holds due the definition of D2
Ffunction with the Assumption 2.2 and the
second inequality holds due to the events Epf
h. Thus, we have
pfk,h(s, a)− ThVk,h+1(s, a)≤pβkDFh(z;z[k−1],h,¯σ[k−1],h).
With a similar argument, for the pessimistic value function qfk,h, we have
 qfk,h(s, a)− ThqVk,h(s, a)2
≤D2
Fh(z;z[k−1],h,¯σ[k−1],h)× 
λ+k−1X
i=11
(¯σi,h)2
qfk,h(si
h, ai
h)− ThqVk,h+1(si
h, ai
h)2!
≤qβ2
k×D2
Fh(z;z[k−1],h,¯σ[k−1],h),
where the first inequality holds due to the definition of D2
Ffunction with the Assumption 2.2 and the
second inequality holds due to the events Eqf
h. In addition, we have,
qfk,h(s, a)− ThqVk,h+1(s, a)≤pβkDFh(z;z[k−1],h,¯σ[k−1],h).
Thus, we complete the proof of Lemma D.6.
F.2.2 Proof of Lemma D.7
Proof of Lemma D.7. We use induction to prove the optimistic and pessimistic property. First, we
study the basic case with the last stage H+ 1. In this situation, Qk,H+1(s, a) =Q∗
h(s, a) =
qQk,h(s, a) = 0 andVk,h(s) =V∗
h(s) =qVk,h(s) = 0 hold for all state s∈ S and action a∈ A.
Therefore, Lemma D.7 holds for the basic case (stage H+ 1).
Second, if Lemma D.7 holds for stage h+ 1, then we focus on the stage h. Notice that the event
eEhdirectly implies the event eEh+1. Therefore, according to the induction assumption, the following
inequality holds for all state s∈ S and episode k∈[K].
Vk,h+1(s)≥V∗
h+1(s)≥qVk,h(s). (F.16)
Thus, for all episode k∈[K]and state-action pair (s, a)∈ S × A , we have
pfk,h(s, a) +bk,h(s, a)−Q∗
h(s, a)
≥ ThVk,h+1(s, a)−pβk·DFh(z;z[k−1],h,¯σ[k−1],h) +bk,h(s, a)−Q∗
h(s, a)
≥ ThVk,h+1(s, a)−Q∗
h(s, a)
=PhVk,h+1(s, a)−PhV∗
h(s, a)
≥0, (F.17)
where the first inequality holds due to Lemma D.6, the second inequality holds due to the definition
of the exploration bonus bk,hand the last inequality holds due to the (F.16) . Therefore, the optimal
value function Q∗
h(s, a)is upper bounded by
Q∗
h(s, a)≤minn
min
1≤i≤kpfi,h(s, a) +bi,h(s, a),1o
≤Qk,h(s, a), (F.18)
where the first inequality holds due to (F.17) with the fact that Q∗
h(s, a)≤1and the second inequality
holds due to the update rule of value function Qk,h.
With a similar argument, for the pessimistic estimator qfk,h, we have
qfk,h(s, a)−bk,h(s, a)−Q∗
h(s, a)
≤ ThqVk,h+1(s, a) +qβk·DFh(z;z[k−1],h,¯σ[k−1],h)−bk,h(s, a)−Q∗
h(s, a)
≤ ThqVk,h+1(s, a)−Q∗
h(s, a)
=PhqVk,h+1(s, a)−PhV∗
h(s, a)
≤0, (F.19)
32where the first inequality holds due to Lemma D.6, the second inequality holds due to the definition
of the exploration bonus bk,hand the last inequality holds due to the (F.19) . Therefore, the optimal
value function Q∗
h(s, a)is lower bounded by
Q∗
h(s, a)≥maxn
max
1≤i≤kqfi,h(s, a)−bi,h(s, a),0o
≥qQk,h(s, a), (F.20)
where the first inequality holds due to (F.19) with the fact that Q∗
h(s, a)≥0and the second inequality
holds due to the update rule of value function qQk,h.
Furthermore, for the value functions Vk,handqVk,h, we have
Vk,h(s) = max
aQk,h(s, a)≥max
aQ∗
h(s, a) =V∗
h(s),
qVk,h(s) = max
aqQk,h(s, a)≤max
aQ∗
h(s, a) =V∗
h(s),
where the first inequality holds due to (F.18) and the second inequality holds due to (F.20) . Thus, by
induction, we complete the proof of Lemma D.7.
F.3 Proof of Monotonic Variance Estimator
F.3.1 Proof of Lemma D.8
Proof of Lemma D.8. According to the definition of D2
Ffunction, we have
 pfk,h(s, a)− ThVk,h+1(s, a)2
≤D2
Fh(z;z[k−1],h,¯σ[k−1],h)× 
λ+k−1X
i=11
(¯σi,h)2
pfk,h(si
h, ai
h)− ThVk,h+1(si
h, ai
h)2!
≤β2
k×D2
Fh(z;z[k−1],h,¯σ[k−1],h),
where the first inequality holds due the definition of D2
Ffunction with the Assumption 2.2 and the
second inequality holds due to the events Epf. Thus, we have
pfk,h(s, a)− ThVk,h+1(s, a)≤βkDFh(z;z[k−1],h,¯σ[k−1],h).
For the pessimistic value function qfk,h, we have
 qfk,h(s, a)− ThqVk,h(s, a)2
≤D2
Fh(z;z[k−1],h,¯σ[k−1],h)× 
λ+k−1X
i=11
(¯σi,h)2
qfk,h(si
h, ai
h)− ThqVk,h+1(si
h, ai
h)2!
≤β2
k×D2
Fh(z;z[k−1],h,¯σ[k−1],h),
where the first inequality holds due the definition of D2
Ffunction with the Assumption 2.2 and the
second inequality holds due to the events Eqf. In addition, we have
qfk,h(s, a)− ThqVk,h+1(s, a)≤βkDFh(z;z[k−1],h,¯σ[k−1],h).
With a similar argument, for the second-order estimator efk,h, we have
 efk,h(s, a)− T2
hVk,h(s, a)2
≤D2
Fh(z;z[k−1],h,¯σ[k−1],h)× 
λ+k−1X
i=11
(¯σi,h)2
efk,h(si
h, ai
h)− T2
hVk,h+1(si
h, ai
h)2!
≤eβ2
k×D2
Fh(z;z[k−1],h,¯σ[k−1],h),
where the first inequality holds due the definition of D2
Ffunction with the Assumption 2.2 and the
second inequality holds due to the events Eef. Therefore, we have
efk,h(s, a)− T2
hVk,h(s, a)≤eβkDFh(z;z[k−1],h,¯σ[k−1],h).
Now, we complete the proof of Lemma D.8.
33F.3.2 Proof of Lemma D.9
Proof of Lemma D.9. First, according to Lemma D.8, we have
[¯VhVk,h+1](sk
h, ak
h)−[VhVk,h+1](sk
h, ak
h)
=efk,h−pf2
k,h−[PhV2
k,h+1](sk
h, ak
h) + 
[PhVk,h+1](sk
h, ak
h)2
≤pf2
k,h− 
[PhVk,h+1](sk
h, ak
h)2+efk,h−[PhV2
k,h+1](sk
h, ak
h)
≤2Lpfk,h− 
[PhVk,h+1](sk
h, ak
h)+efk,h−[PhV2
k,h+1](sk
h, ak
h)
≤(2Lβk+eβk)DFh(z;z[k−1],h,¯σ[k−1],h)
=Ek,h, (F.21)
where the first inequality holds due to |a+b| ≤ | a|+|b|, the second inequality holds due to
|a2−b2|=|a−b| · |a+b| ≤ |a−b| ·2 max( |a|,|b|)and the last inequalirtnholds due to Lemma
D.8.
For the difference between variances [VhVk,h+1](sk
h, ak
h)and[VhV∗
h+1](sk
h, ak
h), it can be upper
bounded by
[VhVk,h+1](sk
h, ak
h)−[VhV∗
h+1](sk
h, ak
h)
=[PhV2
k,h+1](sk
h, ak
h)− 
[PhVk,h+1](sk
h, ak
h)2−[Ph(V∗
h+1)2](sk
h, ak
h) + 
[PhV∗
h+1](sk
h, ak
h)2
≤[PhV2
k,h+1](sk
h, ak
h)−[Ph(V∗
h+1)2](sk
h, ak
h)+ 
[PhVk,h+1](sk
h, ak
h)2− 
[PhV∗
h+1](sk
h, ak
h)2
≤4 
[PhVk,h+1](sk
h, ak
h)−[PhV∗
h+1](sk
h, ak
h)
≤ 
[PhVk,h+1](sk
h, ak
h)−[PhqVk,h+1](sk
h, ak
h)
≤pfk,h(sk
h, ak
h)−qfk,h(sk
h, ak
h) + 2βkDFh(z;z[k−1],h,¯σ[k−1],h), (F.22)
where the first inequality holds due to |a+b| ≤ | a|+|b|, the second inequality holds due to
1≥Vk,h+1(·)≥V∗
h+1(·)≥0(Lemma D.7), the third inequality holds due to V∗
h+1(·)≥qVk,h+1(·)
(Lemma D.7) and the last inequality holds due to Lemma D.8. Combining the results in (F.21) and
(F.22) with the fact that 0≤Vk,h+1(·), V∗
h+1(·)≤1, we have
[¯VhVk,h+1](sk
h, ak
h)−[VhV∗
h+1](sk
h, ak
h)
≤[¯VhVk,h+1](sk
h, ak
h)−[VhVk,h+1](sk
h, ak
h)+[VhVk,h+1](sk
h, ak
h)−[VhV∗
h+1](sk
h, ak
h)
≤Ek,h+Fk,h.
Thus, we complete the proof of Lemma D.9.
F.3.3 Proof of Lemma D.10
Proof of Lemma D.10. On the events EpfandEpf
h+1, we have

Vh(Vi,h+1−V∗
h+1)
(sk
h, ak
h)
= [Ph(Vi,h+1−V∗
h+1)2](sk
h, ak
h)− 
[Ph(Vi,h+1−V∗
h+1)](sk
h, ak
h)2
≤[Ph(Vi,h+1−V∗
h+1)2](sk
h, ak
h)
≤2
Ph(Vi,h+1−V∗
h+1)
(sk
h, ak
h)
≤2 
[PhVi,h+1](sk
h, ak
h)−[PhqVk,h+1](sk
h, ak
h)
≤2 
[PhVk,h+1](sk
h, ak
h)−[PhqVk,h+1](sk
h, ak
h)
, (F.23)
where the first equation holds since the reward function is deterministic, the second inequality holds
due to Lemma D.7 with the fact that 0≤Vi,h+1(s), V∗
h+1(s)≤1and the third inequality holds due
to Lemma D.7 and the last inequality holds due to the fact that Vk,h+1≥Vi,h+1.
34With a similar argument, on the events EqfandEqf
h+1, we have

Vh(V∗
h+1−qVi,h+1)
(sk
h, ak
h)
= [Ph(V∗
h+1−qVi,h+1)2](sk
h, ak
h)− 
[Ph(V∗
h+1−qVi,h+1)](sk
h, ak
h)2
≤[Ph(V∗
h+1−qVi,h+1)2](sk
h, ak
h)
≤2
Ph(V∗
h+1−qVi,h+1)
(sk
h, ak
h)
≤2 
[PhVk,h+1](sk
h, ak
h)−[PhqVi,h+1](sk
h, ak
h)
≤2 
[PhVk,h+1](sk
h, ak
h)−[PhqVk,h+1](sk
h, ak
h)
, (F.24)
where the first inequality holds since the reward function is deterministic, the second and third
inequality holds due to Lemma D.7 with the fact that 0≤qVi,h+1(s), V∗
h+1(s)≤H, the last
inequality the fact qVk,h+1(s)≤qVi,h+1(s). For both variances
Vh(Vi,h+1−V∗
h+1)
(sk
h, ak
h)and
Vh(V∗
h+1−qVi,h+1)
(sk
h, ak
h), they are upper bounded by
2 
[PhVk,h+1](sk
h, ak
h)−[PhqVk,h+1](sk
h, ak
h)
= 2ThVk,h+1(sk
h, ak
h)−2ThqVk,h+1(sk
h, ak
h)
≤2pfk,h(sk
h, ak
h)−2qfk,h(sk
h, ak
h) + 4βkDFh(z;z[k−1],h,¯σ[k−1],h), (F.25)
where the first inequality holds due to Lemma D.8 and the second inequality holds due to the
definition of Fk,h. Substituting the result in (F.25) into (F.23) ,(F.24) and combining the fact that
Vh(Vi,h+1−V∗
h+1)
(sk
h, ak
h),
Vh(V∗
h+1−qVi,h+1)
(sk
h, ak
h)≤1, we finish the proof of Lemma
D.10.
F.4 Proof of Lemmas in Section D.4
F.4.1 Proof of Lemma D.11
Proof of Lemma D.11. We use induction to shows that the conclusion in Lemma D.7 and events
Epf
h,Eqf
Hhold for each stage h∈[H]. First, for the basic situation (stage H+ 1),Qk,H+1(s, a) =
Q∗
h(s, a) =qQk,h(s, a) = 0 andVk,h(s) =V∗
h(s) =qVk,h(s) = 0 hold for all state s∈ Sand action
a∈ A. Therefore, Lemma D.7 holds for the basic case (stage H+ 1)
Second, if Lemma D.7 holds for stage h+ 1, then we focus on the stage h. According to Lemmas
D.10 and Lemma D.9, we have the following inequalities:
σ2
i,h= [¯VhVi,h+1](si
h, ai
h) +Ei,h+Di,h≥[¯VhV∗
h+1](si
h, ai
h),
σ2
i,h≥Fi,h≥(logNF(ϵ) + log Nϵ(K))·
Vh(Vk,h+1−V∗
h+1)
(si
h, ai
h),
σ2
i,h≥Fi,h≥(logNF(ϵ) + log Nϵ(K))·
Vh(V∗
h+1−qVk,h+1)
(si
h, ai
h),
where the first inequality holds due to Lemma D.9, the second and third inequality holds due to
Lemma D.10. Thus, the indicator function in events ¯Epf
hand¯Eqf
hhold, which implies events Epf
h,Eqf
H
hold. Furthermore, when events Epf
h,Eqf
Hhold, then Lemma D.7 holds for stage h. Thus, we complete
the proof of Lemma D.11 by induction.
F.4.2 Proof of Lemma D.13
Proof of Lemma D.13. For each stage h, we divide the episodes {1,2, .., K}to the following sets:
I1={k∈[K] :DFh(z;z[k−1],h,¯σ[k−1],h)/¯σk,h≥1},
I2={k∈[K] :DFh(z;z[k−1],h,¯σ[k−1],h)/¯σk,h<1,¯σk,h=σk,h},
I3={k∈[K] :DFh(z;z[k−1],h,¯σ[k−1],h)/¯σk,h<1,¯σk,h=α},
I4={k∈[K] :DFh(z;z[k−1],h,¯σ[k−1],h)/¯σk,h<1,¯σk,h=γ×D1/2
Fh(z;z[k−1],h,¯σ[k−1],h)}.
35The number of episode in set I1is upper bounded by
|I1|=X
k∈I1min
D2
Fh(z;z[k−1],h,¯σ[k−1],h)/¯σ2
k,h,1
≤dimα,K(Fh),
where the equation holds due to D2
Fh(z;z[k−1],h,¯σ[k−1],h)/¯σ2
k,h≥1and the inequality holds due to
the definition of Generalized Eluder dimension. Thus, for set I1, the summation of confidence radius
is upper bounded by
X
k∈I1min
βDFh(z;z[k−1],h,¯σ[k−1],h),1
≤ |I 1| ≤dimα,K(Fh). (F.26)
For set I2, the summation of confidence radius is upper bounded by
X
k∈I2min
βDFh(z;z[k−1],h,¯σ[k−1],h),1
≤X
k∈I2βDFh(z;z[k−1],h,¯σ[k−1],h)
≤βsX
k∈I2σ2
k,h·sX
k∈I2D2
Fh(z;z[k−1],h,¯σ[k−1],h)/¯σ2
k,h
≤βq
dimα,K(Fh)sX
k∈I2σ2
k,h, (F.27)
where the second inequality holds due to Cauchy-Schwartz inequality with σk,h= ¯σk,hand the
last inequality holds due to the definition of Generalized Eluder dimension with the fact that
DFh(z;z[k−1],h,¯σ[k−1],h)/¯σk,h<1.
With a similar argument, the summation of confidence radius over set I3is upper bounded by
X
k∈I3min
βDFh(z;z[k−1],h,¯σ[k−1],h),1
≤X
k∈I3βDFh(z;z[k−1],h,¯σ[k−1],h)
≤βsX
k∈I3α2·sX
k∈I3D2
Fh(z;z[k−1],h,¯σ[k−1],h)/¯σ2
k,h
≤βq
dimα,K(Fh)sX
k∈I3α2, (F.28)
where the second inequality holds due to Cauchy-Schwartz inequality with ¯σk,h=αand the
last inequality holds due to the definition of Generalized Eluder dimension with the fact that
DFh(z;z[k−1],h,¯σ[k−1],h)/¯σk,h<1.
Finally, the summation of confidence radius over set I4is upper bounded by With a similar argument,
the summation of confidence radius over set I3is upper bounded by
X
k∈I4min
βDFh(z;z[k−1],h,¯σ[k−1],h),1
≤X
k∈I4βDFh(z;z[k−1],h,¯σ[k−1],h)
=X
k∈I4βγ2D2
Fh(z;z[k−1],h,¯σ[k−1],h)/¯σ2
k,h
≤βγ2dimα,K(Fh), (F.29)
where the first equation holds due to ¯σk,h=γ×D1/2
Fh(z;z[k−1],h,¯σ[k−1],h)and the last inequality
holds due to the definition of Generalized Eluder dimension with DFh(z;z[k−1],h,¯σ[k−1],h)/¯σk,h<
1.
36Combining the results in (F.26), (F.27), (F.28) and (F.29), we have
KX
k=1min
βDFh(z;z[k−1],h,¯σ[k−1],h),1
≤(1 +βγ2) dim α,K(Fh) + 2βq
dimα,K(Fh)vuutKX
k=1(σ2
k,h+α2).
Thus, we complete the proof of Lemma D.13.
F.4.3 Proof of Lemma D.14
Proof of Lemma D.14. First, for each stage h∈[H]and episode k∈[K], the gap between Vk,h(sk
h)
andVπk
h(sk
h)can be decomposed as:
Vk,h(sk
h)−Vπk
h(sk
h)
=Qk,h(sk
h, ak
h)−Qπk
h(sk
h, ak
h)
≤min
pfklast,h(s, a) +bklast,h(s, a),1
− ThVk,h+1(sk
h, ak
h)
+ThVk,h+1(sk
h, ak
h)− ThVπk
h+1(sk
h, ak
h)
≤
Ph(Vk,h+1−Vπk
h+1)
(sk
h, ak
h) + min pβklastDFh(z;z[klast−1],h,¯σ[klast−1],h),1
+ min 
bklast,h(sk
h, ak
h),1
≤
Ph(Vk,h+1−Vπk
h+1)
(sk
h, ak
h) + 2C·min pβklastDFh(z;z[klast−1],h,¯σ[klast−1],h),1
≤
Ph(Vk,h+1−Vπk
h+1)
(sk
h, ak
h) + 2C(1 +χ)·min pβkDFh(z;z[k−1],h,¯σ[k−1],h),1
=Vk,h+1(sk
h+1)−Vπk
h+1(sk
h+1) +
Ph(Vk,h+1−Vπk
h+1)
(sk
h, ak
h)− 
Vk,h+1(sk
h+1)−Vπk
h+1(sk
h+1)
+ 2C(1 +χ)·min pβkDFh(z;z[k−1],h,¯σ[k−1],h),1
, (F.30)
where the first inequality holds due to the definition of value function Qk,h(sk
h, ak
h), the sec-
ond inequality holds due to Lemma D.6, the third inequality holds due to bklast,h(sk
h, ak
h)≤
C·DFh(z;z[klast−1],h,¯σ[klast−1],h)and the last inequality holds due to Lemma G.2. Taking a summa-
tion of (F.30) over all episode k∈[K]and stage h′≥h, we have
KX
k=1 
Vk,h(sk
h)−Vπk
h(sk
h)
≤KX
k=1HX
h′=h
Ph(Vk,h+1−Vπk
h+1)
(sk
h, ak
h)− 
Vk,h+1(sk
h+1)−Vπk
h+1(sk
h+1)
+KX
k=1HX
h′=h2C(1 +χ)·min pβkDFh(z;z[k−1],h,¯σ[k−1],h),1
≤KX
k=1HX
h′=h2C(1 +χ)·min pβkDFh(z;z[k−1],h,¯σ[k−1],h),1
+ 2vuutKX
k=1HX
h′=h[Vh(Vk,h+1−Vπk
h+1)](sk
h, ak
h) log(2 K2H/δ) + 2p
log(2K2H/δ) + 2 log(2 K2H/δ)
≤HX
h′=h2C(1 +χ)(1 + pβkγ2) dim α,K(Fh′) +HX
h′=h4C(1 +χ)pβkq
dimα,K(Fh′)vuutKX
k=1(σ2
k,h′+α2)
+ 2vuutKX
k=1HX
h′=h[Vh(Vk,h+1−Vπk
h+1)](sk
h, ak
h) log(2 K2H/δ) + 2p
log(2K2H/δ) + 2 log(2 K2H/δ)
37≤2CH(1 +χ)(1 + pβkγ2) dim α,K(Fh) + 4C(1 +χ)pβkvuutHX
h′=hdimα,K(Fh′)vuutKX
k=1HX
h′=h(σ2
k,h′+α2)
+ 2vuutKX
k=1HX
h′=h[Vh(Vk,h+1−Vπk
h+1)](sk
h, ak
h) log(2 K2H/δ) + 2p
log(2K2H/δ) + 2 log(2 K2H/δ)
≤2CH(1 +χ)(1 + pβkγ2) dim α,K(Fh) + 4C(1 +χ)pβkq
dimα,K(F)vuutHKX
k=1HX
h=1(σ2
k,h+α2)
+ 2vuutKX
k=1HX
h′=h[Vh(Vk,h+1−Vπk
h+1)](sk
h, ak
h) log(2 K2H/δ) + 2p
log(2K2H/δ) + 2 log(2 K2H/δ),
(F.31)
where the first inequality holds due to (F.30) , the second inequality holds due to event E1, the third
inequality holds due to Lemma D.13, the fourth inequality holds due to Cauchy-Schwartz inequality
and the last inequality holds due toPH
h′=hdimα,K(Fh′)≤PH
h′=1dimα,K(Fh′) =Hdimα,K(F).
Furthermore, taking a summation of (F.31), we have
KX
k=1HX
h=1
Ph(Vk,h+1−Vπk
h+1)
(sk
h, ak
h)
=KX
k=1HX
h=1 
Vk,h+1(sk
h+1)−Vπk
h+1(sk
h+1)
+KX
k=1HX
h=1
Ph(Vk,h+1−Vπk
h+1)
(sk
h, ak
h)− 
Vk,h+1(sk
h+1)−Vπk
h+1(sk
h+1)
≤KX
k=1HX
h=1 
Vk,h+1(sk
h+1)−Vπk
h+1(sk
h+1)
+ 2vuutKX
k=1HX
h=1[Vh(Vk,h+1−Vπk
h+1)](sk
h, ak
h) log(2 K2H/δ) + 2p
log(2K2H/δ) + 2 log(2 K2H/δ)
≤2CH2(1 +χ)(1 + pβkγ2) dim α,K(F) + 4CH(1 +χ)pβkq
dimα,K(Fh)vuutHKX
k=1HX
h=1(σ2
k,h+α2)
+H
2vuutKX
k=1HX
h=1[Vh(Vk,h+1−Vπk
h+1)](sk
h, ak
h) log(2 K2H/δ) + 2p
log(2K2H/δ) + 2 log(2 K2H/δ)
,
where the first inequality holds due to event E1and the second inequality holds due to (F.31) . Thus,
we complete the proof of Lemma D.14.
F.4.4 Proof of Lemma D.15
Proof of Lemma D.15. Similar to the proof of Lemma D.14, for each stage h∈[H]and episode
k∈[K], the gap between Vk,h(sk
h)andqVk,h(sk
h)can be decomposed as:
Vk,h(sk
h)−qVk,h(sk
h)
≤Qk,h(sk
h, ak
h)−qQk,h(sk
h, ak
h)
≤min
pfklast,h(s, a) +bklast,h(s, a),1
− ThVk,h+1(sk
h, ak
h)
−max
qfklast,h(s, a)−bklast,h(s, a),0
+ThqVk,h+1(sk
h, ak
h)
38+ThVk,h+1(sk
h, ak
h)− ThqVk,h+1(sk
h, ak
h)
≤
Ph(Vk,h+1−qVk,h+1)
(sk
h, ak
h) + 2·min pβklastDFh(z;z[klast−1],h,¯σ[klast−1],h),1
+ 2·min 
bklast,h(sk
h, ak
h),1
≤
Ph(Vk,h+1−qVk,h+1)
(sk
h, ak
h) + 4C·min pβklastDFh(z;z[klast−1],h,¯σ[klast−1],h),1
≤
Ph(Vk,h+1−qVk,h+1)
(sk
h, ak
h) + 4C(1 +χ)·min pβkDFh(z;z[k−1],h,¯σ[k−1],h),1
=Vk,h+1(sk
h+1)−qVk,h+1(sk
h+1) +
Ph(Vk,h+1−qVk,h+1)
(sk
h, ak
h)− 
Vk,h+1(sk
h+1)−qVk,h+1(sk
h+1)
+ 4C(1 +χ)·min pβkDFh(z;z[k−1],h,¯σ[k−1],h),1
, (F.32)
where the first and second inequalities hold due to the definition of qVk,h(sk
h)andVk,h(sk
h), the
third inequality holds due to Lemma D.6 with pβklast=qβklast, the fourth inequality holds due to
bklast,h(sk
h, ak
h)≤C·DFh(z;z[klast−1],h,¯σ[klast−1],h)and the last inequality holds due to Lemma G.2.
Taking a summation of (F.32) over all episode k∈[K]and stage h′≥h, we have
KX
k=1 
Vk,h(sk
h)−qVk,h(sk
h)
≤KX
k=1HX
h′=h
Ph(Vk,h+1−qVk,h+1)
(sk
h, ak
h)− 
Vk,h+1(sk
h+1)−qVk,h+1(sk
h+1)
+KX
k=1HX
h′=h4C(1 +χ)·min pβkDFh(z;z[k−1],h,¯σ[k−1],h),1
≤KX
k=1HX
h′=h4C(1 +χ)·min pβkDFh(z;z[k−1],h,¯σ[k−1],h),1
+ 2vuutKX
k=1HX
h′=h[Vh(Vk,h+1−qVk,h+1)](sk
h, ak
h) log(2 K2H/δ) + 2p
log(2K2H/δ) + 2 log(2 K2H/δ)
≤HX
h′=h4C(1 +χ)(1 + pβkγ2) dim α,K(Fh′) +HX
h′=h8C(1 +χ)pβkq
dimα,K(Fh′)vuutKX
k=1(σ2
k,h′+α2)
+ 2vuutKX
k=1HX
h′=h[Vh(Vk,h+1−qVk,h+1)](sk
h, ak
h) log(2 K2H/δ) + 2p
log(2K2H/δ) + 2 log(2 K2H/δ)
≤4CH(1 +χ)(1 + pβkγ2) dim α,K(Fh) + 8C(1 +χ)pβkvuutHX
h′=hdimα,K(Fh′)vuutKX
k=1HX
h′=h(σ2
k,h′+α2)
+ 2vuutKX
k=1HX
h′=h[Vh(Vk,h+1−qVk,h+1)](sk
h, ak
h) log(2 K2H/δ) + 2p
log(2K2H/δ) + 2 log(2 K2H/δ)
≤4CH(1 +χ)(1 + pβkγ2) dim α,K(Fh) + 8C(1 +χ)pβkq
dimα,K(F)vuutHKX
k=1HX
h=1(σ2
k,h+α2)
+ 2vuutKX
k=1HX
h′=h[Vh(Vk,h+1−qVk,h+1)](sk
h, ak
h) log(2 K2H/δ) + 2p
log(2K2H/δ) + 2 log(2 K2H/δ),
(F.33)
where the first inequality holds due to (F.32) , the second inequality holds due to event E2, the third
inequality holds due to Lemma D.13, the fourth inequality holds due to Cauchy-Schwartz inequality
and the last inequality holds due toPH
h′=hdimα,K(Fh′)≤PH
h′=1dimα,K(Fh′) =Hdimα,K(F).
39Furthermore, taking a summation of (F.33), we have
KX
k=1HX
h=1
Ph(Vk,h+1−qVk,h+1)
(sk
h, ak
h)
=KX
k=1HX
h=1 
Vk,h+1(sk
h+1)−qVk,h+1(sk
h+1)
+KX
k=1HX
h=1
Ph(Vk,h+1−qVk,h+1)
(sk
h, ak
h)− 
Vk,h+1(sk
h+1)−qVk,h+1(sk
h+1)
≤KX
k=1HX
h=1 
Vk,h+1(sk
h+1)−qVk,h+1(sk
h+1)
+H
2vuutKX
k=1HX
h=1[Vh(Vk,h+1−qVk,h+1)](sk
h, ak
h) log(2 k2H/δ) + 2p
log(2K2H/δ) + 2 log(2 K2H/δ)

≤2CH2(1 +χ)(1 + pβkγ2) dim α,K(Fh) + 4CH(1 +χ)pβkq
dimα,K(F)vuutHKX
k=1HX
h=1(σ2
k,h+α2)
+ 2H
2vuutKX
k=1HX
h=1[Vh(Vk,h+1−qVk,h+1)](sk
h, ak
h) log(2 K2H/δ) + 2p
log(2K2H/δ) + 2 log(2 K2H/δ)
,
where the first inequality holds due to event E2and the last inequality holds due to (F.33). Thus, we
complete the proof of Lemma D.15.
F.4.5 Proof of Lemma D.16
Proof of Lemma D.16. According to the definition of estimated variance σk,h, the summation of
variance can be decomposed as following:
KX
k=1HX
h=1σ2
k,h=KX
k=1HX
h=1[¯Vk,hVk,h+1](sk
h, ak
h) +Ek,h+Fk,h
=KX
k=1HX
h=1 
[¯Vk,hVk,h+1](sk
h, ak
h)−[VhVk,h+1](sk
h, ak
h)
| {z }
I1+KX
k=1HX
h=1Ek,h
|{z}
I2+KX
k=1HX
h=1Fk,h
|{z}
I3
+KX
k=1HX
h=1 
[VhVk,h+1](sk
h, ak
h)−[VhVπk
h+1](sk
h, ak
h)
| {z }
I4+KX
k=1HX
h=1[VhVπk
h+1](sk
h, ak
h)
| {z }
I5.
(F.34)
For the term I1, it can be upper bounded by
I1=KX
k=1HX
h=1 
[¯Vk,hVk,h+1](sk
h, ak
h)−[VhVk,h+1](sk
h, ak
h)
≤KX
k=1HX
h=1Ek,h, (F.35)
where the inequality holds due to Lemma D.9.
For the second term I2=PK
k=1PH
h=1Ek,h, we have
KX
k=1HX
h=1Ek,h=KX
k=1HX
h=1(2Lβk+eβk) min
1, DFh(z;z[k−1],h,¯σ[k−1],h)
40≤HX
h=1(2Lβk+eβk)×(1 +γ2) dim α,K(Fh)
+HX
h=1(2Lβk+eβk)×2q
dimα,K(Fh)vuutKX
k=1(σ2
k,h+α2)
≤(2Lβk+eβk)H×(1 +γ2) dim α,K(F)
+ (2Lβk+eβk)×2vuutHX
h=1dimα,K(Fh)vuutKX
k=1HX
h=1(σ2
k,h+α2)
= (2Lβk+eβk)H×(1 +γ2) dim α,K(F)
+ (2Lβk+eβk)×2q
dimα,K(F)vuutHKX
k=1HX
h=1(σ2
k,h+α2), (F.36)
where the inequality holds due to Lemma D.13 and the second inequality holds due to Cauchy-
Schwartz inequality.
For the term I3, we have
I3=KX
k=1HX
h=1Fk,h
= (log NF(ϵ) + log Nϵ(K))
×KX
k=1HX
h=1min
1,2pfk,h(sk
h, ak
h)−2qfk,h(sk
h, ak
h) + 4βkDFh(z;z[k−1],h,¯σ[k−1],h)
≤(logNF(ϵ) + log Nϵ(K))
×KX
k=1HX
h=1min
1,2ThVk,h+1(sk
h, ak
h)−2ThqVk,h+1(sk
h, ak
h)(sk
h, ak
h) + 8βkDFh(z;z[k−1],h,¯σ[k−1],h)
≤(logNF(ϵ) + log Nϵ(K))×KX
k=1HX
h=1
Ph(Vk,h+1−qVk,h+1)
(sk
h, ak
h)
+ (log NF(ϵ) + log Nϵ(K))×KX
k=1HX
h=1min 
1,8βkDFh(z;z[k−1],h,¯σ[k−1],h)
≤(logNF(ϵ) + log Nϵ(K))×KX
k=1HX
h=1
Ph(Vk,h+1−qVk,h+1)
(sk
h, ak
h)
+ (log NF(ϵ) + log Nϵ(K))×(1 + 8 βkγ2)Hdimα,K(F)
+ ((log NF(ϵ) + log Nϵ(K))×16βkq
dimα,K(F)vuutHKX
k=1HX
h=1(σ2
k,h+α2) (F.37)
where the first inequality holds due to Lemma D.8, the second inequality holds due to Vk,h+1(·)≥
V∗
h+1(·)≥qVk,h+1(·), the third inequality holds due to Lemma D.13 with Cauchy-Schwartz inequality.
By Lemma D.15,
KX
k=1HX
h=1
Ph(Vk,h+1−qVk,h+1)
(sk
h, ak
h)≤4CH2(1 +χ)(1 + pβkγ2) dim α,K(Fh)
+ 8CH(1 +χ)pβkq
dimα,K(F)vuutHKX
k=1HX
h=1(σ2
k,h+α2)
41+ 2H
2vuutKX
k=1HX
h=1[Vh(Vk,h+1−qVk,h+1)](sk
h, ak
h) log(2 K2H/δ) + 2p
log(2K2H/δ) + 2 log(2 K2H/δ)

≤4CH2(1 +χ)(1 + pβkγ2) dim α,K(Fh) + 8CH(1 +χ)pβkq
dimα,K(F)vuutHKX
k=1HX
h=1(σ2
k,h+α2)
+eO
HvuutKX
k=1HX
h=1
Ph(Vk,h+1−qVk,h+1)
(sk
h, ak
h)
, (F.38)
where the last inequality follows from Lemma D.7.
Notice that for each variable x,x≤a√x+bimplies x≤a2+ 2b, from (F.38), we further have
KX
k=1HX
h=1
Ph(Vk,h+1−qVk,h+1)
(sk
h, ak
h)
≤eO
CH2(1 +χ)(1 + pβkγ2) dim α,K(Fh) +CH(1 +χ)pβkq
dimα,K(F)vuutHKX
k=1HX
h=1(σ2
k,h+α2)
.
(F.39)
Substituting (F.39) into (F.37), we obtain the following upper bound for I3,
I3≤eO
(logNF(ϵ) + log Nϵ(K))Hpβkq
dimα,K(F)vuutHKX
k=1HX
h=1(σ2
k,h+α2)

+eO
(logNF(ϵ) + log Nϵ(K))·H2(1 + pβkγ2) dim α,K(Fh)
. (F.40)
For the term I4, we have
I4=KX
k=1HX
h=1 
[VhVk,h+1](sk
h, ak
h)−[VhVπk
h+1](sk
h, ak
h)
=KX
k=1HX
h=1
[PhV2
k,h+1](sk
h, ak
h)− 
[PhVk,h+1](sk
h, ak
h)2−[Ph(Vπk
h+1)2](sk
h, ak
h) + 
[PhVπk
h+1](sk
h, ak
h)2
≤KX
k=1HX
h=1 
[PhV2
k,h+1](sk
h, ak
h)−[Ph(Vπk
h+1)2](sk
h, ak
h)
≤2KX
k=1HX
h=1 
[PhVk,h+1](sk
h, ak
h)−[PhVπk
h+1](sk
h, ak
h)
≤8CH2(1 +χ)(1 + pβkγ2) dim α,K(F) + 16 CH(1 +χ)pβkq
dimα,K(F)vuutHKX
k=1HX
h=1(σ2
k,h+α2)
+ 2H
2vuutKX
k=1HX
h=1[Vh(Vk,h+1−Vπk
h+1)](sk
h, ak
h) log(2 K2H/δ) + 2p
log(2K2H/δ) + 2 log(2 K2H/δ)
,
≤eO
H2(1 + pβkγ2) dim α,K(F) +Hpβkq
dimα,K(F)vuutHKX
k=1HX
h=1(σ2
k,h+α2)
 (F.41)
where the first inequality holds due to Vk,h+1(·)≥V∗
h+1(·)≥Vπk
h+1(·), the second inequality holds
due to 0≤V∗
h+1(·), Vπk
h+1(·)≤1, the third inequality holds due to Lemma D.14, and the last
inequality follows from Lemma D.7 and the fact that x≤a√x+bimplies x≤a2+ 2b.
42For the term I5, according to the definition of VarK, we have
I5=KX
k=1HX
h=1[VhVπk
h+1](sk
h, ak
h) = Var K. (F.42)
Substituting the results in (F.35), (F.36), (F.40), (F.41) and (F.42) into (F.34), we have
KX
k=1HX
h=1σ2
k,h
=I1+I2+I3+I4+I5
≤eO
(4Lβk+ 2eβk)H×(1 +γ2) dim α,K(F)
+ (4Lβk+ 2eβk)×2q
dimα,K(F)vuutHKX
k=1HX
h=1(σ2
k,h+α2) + Var K
+ 8CH2(1 +χ)(1 + pβkγ2) dim α,K(F) + 16 CH(1 +χ)pβkq
dimα,K(F)vuutHKX
k=1HX
h=1(σ2
k,h+α2)
+ (log NF(ϵ) + log Nϵ(K))Hpβkq
dimα,K(F)vuutHKX
k=1HX
h=1(σ2
k,h+α2)
+ (log NF(ϵ) + log Nϵ(K))·H2(1 + pβkγ2) dim α,K(Fh)
≤VarK+ (log NF(ϵ) + log Nϵ(K))×eO 
(1 +γ2)(βk+Hpβk+eβk)Hdimα,K(F)
+ (log NF(ϵ) + log Nϵ(K))×eO
(βk+Hpβk+eβk)q
dimα,K(F)vuutHKX
k=1HX
h=1(σ2
k,h+α2)

Notice that for each variable x,x≤a√x+bimplies x≤a2+ 2b. With this fact, we have
KX
k=1HX
h=1σ2
k,h≤(logNF(ϵ) + log Nϵ(K))×eO 
(1 +γ2)(βk+Hpβk+eβk)Hdimα,K(F)
+ (log NF(ϵ) + log Nϵ(K))2×eO 
(βk+Hpβk+eβk)2Hdimα,K(F)
+eO(Var K+KHα2).
Thus, we complete the proof of Lemma D.16.
G Covering Number Argument
G.1 Rare Switching
Based on the policy-updating criterion, the following lemma provides a upper bound of the switching
cost.
Lemma G.1. The number of episodes when the algorithm updates the value function is at most
O(dim α,K(F)·H).
Proof. According to line 9, the policy is updated at episode konly when there exists a stage h∈[H]
such that
X
i∈[klast,k−1]1
¯σ2
i,hD2
Fh(zi,h;z[klast−1],h,¯σ[klast−1],h)≥χ/C.
43and
X
i∈[klast,k−2]1
¯σ2
i,hD2
F(zi,h;z[klast−1],h,¯σ[klast−1],h)< χ. (G.1)
Then the following inequality holds,
sup
f1,f2∈FhP
i∈[1,k−2]1
¯σ2
i,h(f1(zi,h)−f2(zi,h))2+λ
P
i∈[1,klast−1]1
¯σ2
i,h(f1(zi,h)−f2(zi,h))2+λ(G.2)
= 1 + sup
f1,f2∈FhP
i∈[klast,k−2]1
¯σ2
i,h(f1(zi,h)−f2(zi,h))2
P
i∈[1,klast−1]1
¯σ2
i,h(f1(zi,h)−f2(zi,h))2+λ
≤1 +X
i∈[klast,k−2]1
¯σ2
i,hD2
Fh(zi,h;z[klast−1],h,¯σ[klast−1],h)
≤1 +χ, (G.3)
where the first inequality holds due to the definition of DFh(Definition 2.4), the second inequality
follows from (G.1).
(G.3) further gives a lower bound for the summation
X
i∈[klast,k−1]1
¯σ2
i,hD2
F(zi,h;z[i−1],h,¯σ[i−1],h)
≥1
1 +χX
i∈[klast,k−1]1
¯σ2
i,hD2
F(zi,h;z[klast−1],h,¯σ[klast−1],h)
≥χ/C
1 +χ.
Note thatχ/C
1+χ≤1, we also have
X
i∈[klast,k−1]min(
1,1
¯σ2
i,hD2
F(zi,h;z[i−1],h,¯σ[i−1],h))
≥χ/C
1 +χ.
Then we have an upper bound and lower bound for the following summation:
lK·χ/C
1 +χ≤KX
k=1HX
h=1min(
1,1
¯σ2
k,hD2
F(zk,h;z[k−1],h,¯σ[k−1],h))
≤dimα,K(F)·H.
Therefore, the number of policy switching lKis of order O(dim α,K(F)·H).
Lemma G.2 (Stability of uncertainty under rare switching strategy) .If the policy is not updated at
episode k, the uncertainty of all state-action pair z= (s, a)∈ S × A and stage h∈[H]satisfies the
following stability property:
D2
Fh(z;z[k−1],h,¯σ[k−1],h)≥1
1 +χD2
Fh(z;z[klast−1],h,¯σ[klast−1],h).
Proof. Due to the definition of klastin Algorithm 1, we have
X
i∈[klast,k−1]1
¯σ2
i,hD2
F(zi,h;z[klast−1],h,¯σ[klast−1],h)< χ.
As is shown in (G.3), here we also have
sup
f1,f2∈FhP
i∈[1,k−1]1
¯σ2
i,h(f1(zi,h)−f2(zi,h))2+λ
P
i∈[1,klast−1]1
¯σ2
i,h(f1(zi,h)−f2(zi,h))2+λ≤1 +χ.
44From the definition of DFh,
D2
Fh(z;z[k−1],h,¯σ[k−1],h)≥1
1 +χD2
Fh(z;z[klast−1],h,¯σ[klast−1],h).
The proof is then completed due to the arbitrariness of h.
G.2 Value Function Class and Its Covering Number
The optimistic value functions at episode kand stage h∈[H]in our construction belong to the
following function class:
Vk,h=
Vmax
a∈Amin
1≤i≤lk+1min (1 , fi(·, a) +β·b(·, a))
, (G.4)
where lkis the number of updated policies as defined in Algorithm 1, fi∈ Fhandb∈ B.
Similarly, we also define the following pessimistic value function classes for all k≥1:
qVk,h=
Vmax
a∈Amax
1≤i≤lk+1max (0 , fi(·, a)−β·b(·, a))
, (G.5)
Lemma G.3 (ϵ-covering number of optimistic value function classes) .For optimistic value function
classVk,hdefined in (G.4) , we define the distance between two value functions V1andV2as
∥V1−V2∥∞:= max s∈S|V1(s)−V2(s)|. Then the ϵ-covering number with respect to the distance
function can be upper bounded by
Nϵ(k) := [ NF(ϵ/2)·N(B, ϵ/2β)]lk+1. (G.6)
Proof. By the definition of N(F, ϵ), there exists an ϵ/2-net of F, denoted by C(F, ϵ/2), such that for
anyf∈ F, we can find f′∈ C(F, ϵ/2)such that ∥f−f′∥∞≤ϵ/2. Also, there exists an ϵ/2β-net
ofB,C(B, ϵ/2β).
Then we consider the following subset of Vk,
Vc=
Vmax
a∈Amin
1≤i≤lk+1min (1 , fi(·, a) +β·bi(·, a)), fi∈ C(Fh, ϵ/2), bi∈ C(B, ϵ/2β)
.
Consider an arbitrary V∈ V where V= max a∈Amin1≤i≤lk+1min(1 , fi(·, a) +β·bi(·, a)). For
eachfi, there exists fc
i∈ C(F, ϵ/2)such that ∥fi−fc
i∥∞≤ϵ/2. There also exists bc∈ C(B, ϵ/2β)
such that ∥b−bc∥∞≤ϵ/2β. LetVc= max a∈Amin1≤i≤lk+1min(1 , fc
i(·, a) +β·bc(·, a))∈ Vc.
It is then straightforward to check that ∥V−Vc∥∞≤ϵ/2 +β·ϵ/2β=ϵ.
By direct calculation, we have |Vc|= [N(Fh, ϵ/2)·N(B, ϵ/2β)]lk+1.
Lemma G.4 (ϵ-covering number of pessimistic value function classes) .For pessimistic value function
class qVk,hdefined in (G.5) , we define the distance between two value functions V1andV2as
∥V1−V2∥∞:= max s∈S|V1(s)−V2(s)|. Then the ϵ-covering number of qVkwith respect to the
distance function can be upper bounded by Nϵ(k)defined in (G.6).
Proof. The proof is nearly the same as that of Lemma G.3.
H Auxiliary Lemmas
Lemma H.1 (Azuma-Hoeffding inequality) .Let{xi}n
i=1be a martingale difference sequence with
respect to a filtration {Gi}satisfying |xi| ≤Mfor some constant M,xiisGi+1-measurable,
E[xi|Gi] = 0 . Then for any 0< δ < 1, with probability at least 1−δ, we have
nX
i=1xi≤Mp
2nlog(1/δ).
45Lemma H.2 (Corollary 2, Agarwal et al. 2022) .LetM > 0, V > v > 0be constants, and
{xi}i∈[t]be stochastic process adapted to a filtration {Hi}i∈[t]. Suppose E[xi|Hi−1] = 0 ,
|xi| ≤ MandP
i∈[t]E[x2
i|Hi−1]≤V2almost surely. Then for any δ, ϵ > 0, let ι=q
log(2 log( V/v)+2)·(log(M/m )+2)
δwe have
P
X
i∈[t]xi> ιvuuut2
2X
i∈[t]E[x2
i|Hi−1] +v2
+2
3ι2
2 max
i∈[t]|xi|+m
≤δ.
Lemma H.3 (Lemma 7, Russo and Van Roy 2014) .Consider random variables (Zn|n∈N)adapted
to the filtration (Hn:n= 0,1, ...). Assume E[exp{λZi}]is finite for all λ. Define the conditional
mean µi=E[Zi| Hi−1]. We define the conditional cumulant generating function of the centered
random variable [Zi−µi]byψi(λ) = log E[exp ( λ[Zi−µi])| Hi−1]. For all x≥0andλ≥0,
P nX
1λZi≤x+nX
1[λµi+ψi(λ)]∀n∈N!
≥1−e−x.
Lemma H.4 (Self-normalized bound for scalar-valued martingales) .Consider random variables
(vn|n∈N)adapted to the filtration (Hn:n= 0,1, ...). Let{ηi}∞
i=1be a sequence of real-valued
random variables which is Hi+1-measurable and is conditionally σ-sub-Gaussian. Then for an
arbitrarily chosen λ >0, for any δ >0, with probability at least 1−δ, it holds that
nX
i=1ϵivi≤λσ2
2·nX
i=1v2
i+ log(1 /δ)/λ ∀n∈N.
Lemma H.5 (Lemma 10, Zhang et al. 2020) .Let(Mn)n≥0be a martingale such that M0= 0and
|Mn−Mn−1| ≤cfor some c >0and any n≥1. Let Varn=Pn
k=1E[(Mk−Mk−1)2|Fk−1]for
n≥0, where Fk=σ(M1, M2, . . . , M k). Then for any positive integer nand any ε, p > 0, we have
that
P 
|Mn| ≥2s
Varnlog1
p
+ 2s
εlog1
p
+ 2clog1
p!
≤ 
2nc2/ε+ 2
p
NeurIPS Paper Checklist
1.Claims
Question: Do the main claims made in the abstract and introduction accurately reflect the
paper’s contributions and scope?
Answer:[Yes]
Justification: In both abstract and introduction, we highlight the contribution in our paper.
The proposed algorithm and the corresponding theoretical results are discussed in the
followed sections.
Guidelines:
•The answer NA means that the abstract and introduction do not include the claims
made in the paper.
•The abstract and/or introduction should clearly state the claims made, including the
contributions made in the paper and important assumptions and limitations. A No or
NA answer to this question will not be perceived well by the reviewers.
•The claims made should match theoretical and experimental results, and reflect how
much the results can be expected to generalize to other settings.
•It is fine to include aspirational goals as motivation as long as it is clear that these goals
are not attained by the paper.
2.Limitations
Question: Does the paper discuss the limitations of the work performed by the authors?
46Answer: [Yes]
Justification: We explicitly list all the necessary assumptions for our theoretical analysis.
Guidelines:
•The answer NA means that the paper has no limitation while the answer No means that
the paper has limitations, but those are not discussed in the paper.
• The authors are encouraged to create a separate "Limitations" section in their paper.
•The paper should point out any strong assumptions and how robust the results are to
violations of these assumptions (e.g., independence assumptions, noiseless settings,
model well-specification, asymptotic approximations only holding locally). The authors
should reflect on how these assumptions might be violated in practice and what the
implications would be.
•The authors should reflect on the scope of the claims made, e.g., if the approach was
only tested on a few datasets or with a few runs. In general, empirical results often
depend on implicit assumptions, which should be articulated.
•The authors should reflect on the factors that influence the performance of the approach.
For example, a facial recognition algorithm may perform poorly when image resolution
is low or images are taken in low lighting. Or a speech-to-text system might not be
used reliably to provide closed captions for online lectures because it fails to handle
technical jargon.
•The authors should discuss the computational efficiency of the proposed algorithms
and how they scale with dataset size.
•If applicable, the authors should discuss possible limitations of their approach to
address problems of privacy and fairness.
•While the authors might fear that complete honesty about limitations might be used by
reviewers as grounds for rejection, a worse outcome might be that reviewers discover
limitations that aren’t acknowledged in the paper. The authors should use their best
judgment and recognize that individual actions in favor of transparency play an impor-
tant role in developing norms that preserve the integrity of the community. Reviewers
will be specifically instructed to not penalize honesty concerning limitations.
3.Theory Assumptions and Proofs
Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?
Answer: [Yes]
Justification: We state all the assumptions in Section 2 and give the proof of theorems in the
appendix.
Guidelines:
• The answer NA means that the paper does not include theoretical results.
•All the theorems, formulas, and proofs in the paper should be numbered and cross-
referenced.
•All assumptions should be clearly stated or referenced in the statement of any theorems.
•The proofs can either appear in the main paper or the supplemental material, but if
they appear in the supplemental material, the authors are encouraged to provide a short
proof sketch to provide intuition.
•Inversely, any informal proof provided in the core of the paper should be complemented
by formal proofs provided in appendix or supplemental material.
• Theorems and Lemmas that the proof relies upon should be properly referenced.
4.Experimental Result Reproducibility
Question: Does the paper fully disclose all the information needed to reproduce the main ex-
perimental results of the paper to the extent that it affects the main claims and/or conclusions
of the paper (regardless of whether the code and data are provided or not)?
Answer: [NA]
Justification:The paper does not include experiments.
47Guidelines:
• The answer NA means that the paper does not include experiments.
•If the paper includes experiments, a No answer to this question will not be perceived
well by the reviewers: Making the paper reproducible is important, regardless of
whether the code and data are provided or not.
•If the contribution is a dataset and/or model, the authors should describe the steps taken
to make their results reproducible or verifiable.
•Depending on the contribution, reproducibility can be accomplished in various ways.
For example, if the contribution is a novel architecture, describing the architecture fully
might suffice, or if the contribution is a specific model and empirical evaluation, it may
be necessary to either make it possible for others to replicate the model with the same
dataset, or provide access to the model. In general. releasing code and data is often
one good way to accomplish this, but reproducibility can also be provided via detailed
instructions for how to replicate the results, access to a hosted model (e.g., in the case
of a large language model), releasing of a model checkpoint, or other means that are
appropriate to the research performed.
•While NeurIPS does not require releasing code, the conference does require all submis-
sions to provide some reasonable avenue for reproducibility, which may depend on the
nature of the contribution. For example
(a)If the contribution is primarily a new algorithm, the paper should make it clear how
to reproduce that algorithm.
(b)If the contribution is primarily a new model architecture, the paper should describe
the architecture clearly and fully.
(c)If the contribution is a new model (e.g., a large language model), then there should
either be a way to access this model for reproducing the results or a way to reproduce
the model (e.g., with an open-source dataset or instructions for how to construct
the dataset).
(d)We recognize that reproducibility may be tricky in some cases, in which case
authors are welcome to describe the particular way they provide for reproducibility.
In the case of closed-source models, it may be that access to the model is limited in
some way (e.g., to registered users), but it should be possible for other researchers
to have some path to reproducing or verifying the results.
5.Open access to data and code
Question: Does the paper provide open access to the data and code, with sufficient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?
Answer: [NA]
Justification: Paper does not include experiments requiring code.
Guidelines:
• The answer NA means that paper does not include experiments requiring code.
•Please see the NeurIPS code and data submission guidelines ( https://nips.cc/
public/guides/CodeSubmissionPolicy ) for more details.
•While we encourage the release of code and data, we understand that this might not be
possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
including code, unless this is central to the contribution (e.g., for a new open-source
benchmark).
•The instructions should contain the exact command and environment needed to run to
reproduce the results. See the NeurIPS code and data submission guidelines ( https:
//nips.cc/public/guides/CodeSubmissionPolicy ) for more details.
•The authors should provide instructions on data access and preparation, including how
to access the raw data, preprocessed data, intermediate data, and generated data, etc.
•The authors should provide scripts to reproduce all experimental results for the new
proposed method and baselines. If only a subset of experiments are reproducible, they
should state which ones are omitted from the script and why.
48•At submission time, to preserve anonymity, the authors should release anonymized
versions (if applicable).
•Providing as much information as possible in supplemental material (appended to the
paper) is recommended, but including URLs to data and code is permitted.
6.Experimental Setting/Details
Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?
Answer: [NA]
Justification: The paper does not include experiments.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The experimental setting should be presented in the core of the paper to a level of detail
that is necessary to appreciate the results and make sense of them.
•The full details can be provided either with the code, in appendix, or as supplemental
material.
7.Experiment Statistical Significance
Question: Does the paper report error bars suitably and correctly defined or other appropriate
information about the statistical significance of the experiments?
Answer: [NA]
Justification: The paper does not include experiments.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The authors should answer "Yes" if the results are accompanied by error bars, confi-
dence intervals, or statistical significance tests, at least for the experiments that support
the main claims of the paper.
•The factors of variability that the error bars are capturing should be clearly stated (for
example, train/test split, initialization, random drawing of some parameter, or overall
run with given experimental conditions).
•The method for calculating the error bars should be explained (closed form formula,
call to a library function, bootstrap, etc.)
• The assumptions made should be given (e.g., Normally distributed errors).
•It should be clear whether the error bar is the standard deviation or the standard error
of the mean.
•It is OK to report 1-sigma error bars, but one should state it. The authors should
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
of Normality of errors is not verified.
•For asymmetric distributions, the authors should be careful not to show in tables or
figures symmetric error bars that would yield results that are out of range (e.g. negative
error rates).
•If error bars are reported in tables or plots, The authors should explain in the text how
they were calculated and reference the corresponding figures or tables in the text.
8.Experiments Compute Resources
Question: For each experiment, does the paper provide sufficient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?
Answer: [NA]
Justification: The paper does not include experiments.
Guidelines:
• The answer NA means that the paper does not include experiments.
49•The paper should indicate the type of compute workers CPU or GPU, internal cluster,
or cloud provider, including relevant memory and storage.
•The paper should provide the amount of compute required for each of the individual
experimental runs as well as estimate the total compute.
•The paper should disclose whether the full research project required more compute
than the experiments reported in the paper (e.g., preliminary or failed experiments that
didn’t make it into the paper).
9.Code Of Ethics
Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines ?
Answer: [Yes]
Justification: The research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics.
Guidelines:
•The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
•If the authors answer No, they should explain the special circumstances that require a
deviation from the Code of Ethics.
•The authors should make sure to preserve anonymity (e.g., if there is a special consid-
eration due to laws or regulations in their jurisdiction).
10.Broader Impacts
Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?
Answer: [NA]
Justification: The paper is a theoretical work with no societal impact.
Guidelines:
• The answer NA means that there is no societal impact of the work performed.
•If the authors answer NA or No, they should explain why their work has no societal
impact or why the paper does not address societal impact.
•Examples of negative societal impacts include potential malicious or unintended uses
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
(e.g., deployment of technologies that could make decisions that unfairly impact specific
groups), privacy considerations, and security considerations.
•The conference expects that many papers will be foundational research and not tied
to particular applications, let alone deployments. However, if there is a direct path to
any negative applications, the authors should point it out. For example, it is legitimate
to point out that an improvement in the quality of generative models could be used to
generate deepfakes for disinformation. On the other hand, it is not needed to point out
that a generic algorithm for optimizing neural networks could enable people to train
models that generate Deepfakes faster.
•The authors should consider possible harms that could arise when the technology is
being used as intended and functioning correctly, harms that could arise when the
technology is being used as intended but gives incorrect results, and harms following
from (intentional or unintentional) misuse of the technology.
•If there are negative societal impacts, the authors could also discuss possible mitigation
strategies (e.g., gated release of models, providing defenses in addition to attacks,
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
feedback over time, improving the efficiency and accessibility of ML).
11.Safeguards
Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?
Answer: [NA]
50Justification: The paper poses no such risks.
Guidelines:
• The answer NA means that the paper poses no such risks.
•Released models that have a high risk for misuse or dual-use should be released with
necessary safeguards to allow for controlled use of the model, for example by requiring
that users adhere to usage guidelines or restrictions to access the model or implementing
safety filters.
•Datasets that have been scraped from the Internet could pose safety risks. The authors
should describe how they avoided releasing unsafe images.
•We recognize that providing effective safeguards is challenging, and many papers do
not require this, but we encourage authors to take this into account and make a best
faith effort.
12.Licenses for existing assets
Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?
Answer: [Yes]
Justification: We have described the related works, especially those work which our work is
based on with proper citations in corresponding sections.
Guidelines:
• The answer NA means that the paper does not use existing assets.
• The authors should cite the original paper that produced the code package or dataset.
•The authors should state which version of the asset is used and, if possible, include a
URL.
• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
•For scraped data from a particular source (e.g., website), the copyright and terms of
service of that source should be provided.
•If assets are released, the license, copyright information, and terms of use in the
package should be provided. For popular datasets, paperswithcode.com/datasets
has curated licenses for some datasets. Their licensing guide can help determine the
license of a dataset.
•For existing datasets that are re-packaged, both the original license and the license of
the derived asset (if it has changed) should be provided.
•If this information is not available online, the authors are encouraged to reach out to
the asset’s creators.
13.New Assets
Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?
Answer: [No]
Justification: This is a theoretical paper without experiments.
Guidelines:
• The answer NA means that the paper does not release new assets.
•Researchers should communicate the details of the dataset/code/model as part of their
submissions via structured templates. This includes details about training, license,
limitations, etc.
•The paper should discuss whether and how consent was obtained from people whose
asset is used.
•At submission time, remember to anonymize your assets (if applicable). You can either
create an anonymized URL or include an anonymized zip file.
14.Crowdsourcing and Research with Human Subjects
51Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as
well as details about compensation (if any)?
Answer: [NA]
Justification: This paper does not include crowdsourcing or human subjects.
Guidelines:
•The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
•Including this information in the supplemental material is fine, but if the main contribu-
tion of the paper involves human subjects, then as much detail as possible should be
included in the main paper.
•According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
or other labor should be paid at least the minimum wage in the country of the data
collector.
15.Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects
Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?
Answer: [NA]
Justification: This paper does not include crowdsourcing or human subjects.
Guidelines:
•The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
•Depending on the country in which research is conducted, IRB approval (or equivalent)
may be required for any human subjects research. If you obtained IRB approval, you
should clearly state this in the paper.
•We recognize that the procedures for this may vary significantly between institutions
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
guidelines for their institution.
•For initial submissions, do not include any information that would break anonymity (if
applicable), such as the institution conducting the review.
52