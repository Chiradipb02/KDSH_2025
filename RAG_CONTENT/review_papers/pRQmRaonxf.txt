Transformers as Game Players:
Provable In-context Game-playing Capabilities of
Pre-trained Models
Chengshuai Shi
University of Virginia
cs7ync@virginia.eduKun Yang
University of Virginia
ky9tc@virginia.edu
Jing Yang
The Pennsylvania State University
yangjing@psu.eduCong Shen
University of Virginia
cong@virginia.edu
Abstract
The in-context learning (ICL) capability of pre-trained models based on the trans-
former architecture has received growing interest in recent years. While theoretical
understanding has been obtained for ICL in reinforcement learning (RL), the pre-
vious results are largely confined to the single-agent setting. This work proposes
to further explore the in-context learning capabilities of pre-trained transformer
models in competitive multi-agent games, i.e., in-context game-playing (ICGP).
Focusing on the classical two-player zero-sum games, theoretical guarantees are
provided to demonstrate that pre-trained transformers can provably learn to ap-
proximate Nash equilibrium in an in-context manner for both decentralized and
centralized learning settings. As a key part of the proof, constructional results are
established to demonstrate that the transformer architecture is sufficiently rich to
realize celebrated multi-agent game-playing algorithms, in particular, decentralized
V-learning and centralized VI-ULCB.
1 Introduction
Since proposed in Vaswani et al. [57], the transformer architecture has received significant interest.
It has powered many recent breakthroughs in artificial intelligence [ 11,12,21,23], including the
extremely powerful large language models such as GPT [ 42] and Llama [ 55,56]. One of the most
striking observations from the research of these transformer-powered models is that they demonstrate
remarkable in-context learning (ICL) capabilities. In particular, after appropriate pre-training, the
models can handle new tasks when prompted by a few descriptions or demonstrations without any
parameter updates, e.g., Brown et al. [11], Chowdhery et al. [15], Liu et al. [39].
ICL is practically attractive as it provides strong generalization capabilities across different down-
stream tasks without requiring further training or a large amount of task-specific data. These appealing
properties have motivated many empirical studies to better understand ICL [ 18,26,59]; see the
survey by Dong et al. [22] for key findings and results. In addition to the empirical investigations,
recent years have witnessed growing efforts in gaining deeper theoretical insights into ICL, e.g., Ahn
et al. [2], Akyürek et al. [3], Bai et al. [7], Cheng et al. [14], Li et al. [37], Raventós et al. [44], Wu
et al. [64], Xie et al. [66], Zhang et al. [71].
Among these empirical and theoretical studies, one emerging direction focuses on the capability of pre-
trained transformer models to perform in-context reinforcement learning (ICRL) [ 28,34,35,50,73].
In particular, the transformer is pre-trained with interaction data from diverse environments, modeling
38th Conference on Neural Information Processing Systems (NeurIPS 2024).the interaction as a sequential prediction task. During inference, the pre-trained transformer is
prompted via the interaction trajectory in the current environment for it to select actions. The work
by Lin et al. [38] provides some theoretical understanding of ICRL, including both a general pre-
training guarantee and specific constructions of transformers to realize some well-known designs in
multi-armed bandits and RL (especially, LinUCB [ 1], Thompson sampling [ 54], and UCB-VI [ 6]).
Wang et al. [62] further provides understandings on the capability of transformers learning temporal
difference (TD) methods [ 52] via an in-context fashion. A detailed literature review can be found in
Sec. 6.
The insights from Lin et al. [38], Wang et al. [62] are largely confined to the single-agent scenario, i.e.,
a single-agent multi-armed bandit or Markov decision process (MDP). The power of RL, however,
extends to the much broader multi-agent scenario, especially the multi-player competitive games such
as GO [ 49], Starcraft [ 58], and Dota 2 [ 10]. To provide a more comprehensive understanding of ICRL,
this work targets further studying the in-context game-playing (ICGP) capabilities of transformers
in multi-agent competitive settings. To the best of our knowledge, this is the first work providing
theoretical analyses and empirical pieces of evidence on the ICGP capabilities of transformers. The
contributions of this work are further summarized as follows.
•A general framework is proposed to model in-context game-playing via transformers, where we
focus on the representative two-player zero-sum Markov games and target learning Nash equilibrium
(NE). Compared with the single-agent scenario [ 38], the multi-agent setting considered in this work
broadens the ICRL research scope while it is also more complicated due to its game-theoretic nature.
•The challenging decentralized learning setting is first studied, where two distinct transformers
are trained to learn NE, one for each player, without observing the opponent’s actions. A general
realizability-conditioned guarantee is first derived that characterizes the generalization error of the
pre-trained transformers. Then, the capability of the transformer architecture is demonstrated by
providing a concrete construction so that the famous V-learning algorithm [ 8] can be exactly realized.
Lastly, a finite-sample upper bound on the approximation error of NE is proved to establish the ICGP
capability of transformers. As a further implication, the result of realizing V-learning demonstrates
the capability of pre-trained transformers to perform model-free RL designs, in addition to the
model-based ones (e.g., UCB-VI [6] as studied in Lin et al. [38]).
•To obtain a complete understanding, the centralized learning setting is also investigated, where
one transformer is pre-trained to control both players’ actions. A similar set of results is provided: a
general pre-training guarantee, a constructional result to demonstrate realizability, and a finite-sample
upper bound on the approximation error of NE. Distinctly, the transformer construction is presented
as a specific parameterization to implement the renowned centralized VI-ULCB algorithm [8].
•Furthermore, experiments are also performed to practically test the ICGP capabilities of the pre-
trained transformers. The obtained results not only corroborate the derived theoretical claims, but
also empirically motivate this and further studies on the interesting direction of pre-trained models in
game-theoretic settings.
2 A Theoretical Framework for In-Context Game Playing
2.1 The Basic Setup of Environments
To demonstrate the ICGP capability of transformers, we focus on one of the most basic game-
theoretic settings: two-player zero-sum Markov games [ 47], while the discussions provided later
conceivably extend to more general games. An illustration of the different settings (i.e., decentralized
and centralized) considered in this work (with details explained later) is given in Fig. 1. The overall
framework is introduced in the following, which extends Lin et al. [38] from the single-agent
decision-making setting to the competitive multi-agent domain.
Considering a set of two-player zero-sum Markov games denoted as M. Each environment M∈ M
shares the number of episodes G, the number of steps Hin each episode, the state space S(with
|S|=S), the action spaces {A,B}(with|A|=Aand|B|=B), and the reward space R. Here A
andBdenote the action spaces of two players, respectively, which are referred to as the max-player
and the min-player for convenience.
2Figure 1: An overall view of the framework, where the in-context game-playing (ICGP) capabilities of
transformers are studied in both decentralized and centralized learning settings. The orange arrows denote the
supervised pre-training procedure and the blue arrows mark the inference procedure.
Each environment M={Th−1
M,Rh
M:h∈[H]}has its transition model Th
M:S × A × B → ∆(S)
and reward functions Rh
M:A × B → ∆(R), where T0
M(·)denotes the initial state distribution.
Particularly, overall Gepisodes of Hsteps happen in each environment M, with each episode starting
atsg,1∼T0
M(·). If the action pair (ag,h, bg,h)is taken upon state sg,hat step hin episode g, the state
is transited to sg,h+1∼Th
M(·|sg,h, ag,h, bg,h), and reward rg,h∼Rh
M(sg,h, ag,h, bg,h)(respectively,
−rg,h) is collected by the max-player (respectively, the min-player). For simplicity, we assume that
the max-player rewards are bounded in [0,1]and deterministic, i.e., for each (s, a, b, h ), there exists
r∈[0,1]such that Rg,h
M(r|s, a, b ) = 1 . Also, the initial state sg,1is assumed to be a fixed one s1,
i.e.,T0
M(s1) = 1 .
We further leverage the notation T:=GH, while using time tand episode-step pair (g, h)in an
interleaving manner with t:= (g−1)H+h. The partial interaction trajectory up to time tis
then denoted as Dt:={(sτ, aτ, bτ, rτ) :τ∈[t])and we use the abbreviated notation D:=DT.
Individually, for the max-player, we denote her observed interaction trajectory up to time tby
Dt
+:={(sτ, aτ, rτ) :τ∈[t])and write D+:=DT
+for short. Similarly, for the min-player, we
denote Dt
−:={(sτ, bτ, rτ) :τ∈[t])andD−:=DT
−.
2.2 Game-playing Algorithms and Nash Equilibrium
A game-playing algorithm Alg can map a partial trajectory Dt−1and state stto a distribution
over the actions, i.e., Alg(·,·|Dt−1, st)∈∆(A × B ). If one algorithm Algis decoupled for the
two players (as in the later decentralized setting), we denote it as Alg= (Alg+,Alg−), where
Alg+(·|Dt−1
+, st)∈∆(A)andAlg−(·|Dt−1
−, st)∈∆(B). Given an environment Mand an algo-
rithm Alg, the distribution over a full trajectory Dcan be expressed as
PAlg
M(D) =Y
t∈[T]Tt−1
M(st|st−1, at−1, bt−1)·Alg(at, bt|Dt−1, st)·Rt
M(rt).
If further considering an environment prior distribution Λ∈∆(M)such that M∼Λ, the joint
distribution of (M, D )is denoted as PAlg
Λ(D), where M∼Λ(·)andD∼PAlg
M(·).
For environment Mand a game-playing algorithm π, we define its value function over one episode
asVπ
M(s1) =EDH∼Pπ
M[P
t∈[H]rt]. With the marginalized policies of πdenoted as (µ, ν), we define
their best responses as
ν†(µ) := arg minν′Vµ,ν′
M(s1), µ†(ν) := arg maxµ′Vµ′,ν
M(s1),
whose corresponding values are
Vµ,†
M(s1) :=Vµ,ν†(µ)
M (s1), V†,ν
M(s1) :=Vµ†(ν),ν
M (s1).
With the notion of best responses, the following classical definition of approximate Nash equilibrium
(NE) can be introduced [8, 32, 40, 47].
Definition 2.1 (Approximate Nash equilibrium) .A decoupled policy pair (ˆµ,ˆν)is anε-approximate
Nash equilibrium for environment MifVˆµ,†
M(s1) +ε≥Vˆµ,ˆν
M(s1)≥V†,ˆν
M(s1)−ε, i.e., the Nash
equilibrium gap V†,ˆν
M(s1)−Vˆµ,†
M(s1)≤2ε.
3For each environment, our learning goal is to approximate its NE policy pair. In other words, we
target outputting a policy pair (ˆµ,ˆν)that is ε-approximate NE with an error εthat is as small as
possible, after interacting with the environment for an overall Trounds.
2.3 The Transformer Architecture
With the basics of the game-playing environment and the learning target established, we now introduce
the transformer architecture [ 57], which has demonstrated great potential in processing sequential
inputs. First, for an input vector x∈Rd, we denote σr(x) := ReLU (x) := max {x,0} ∈Rdas
the entry-wise ReLU activation function and σs(x) := softmax( x)∈Rdas the softmax activation
function, while using σ(·)to refer to a non-specified activation function (i.e., both ReLU and softmax
may be used). Then, the masked attention layer and the MLP layer can be defined as follows.
Definition 2.2 (Masked Attention Layer) .A masked attention layer with Mheads is denoted as
Attnθ(·)with parameters θ={(Vm,Qm,Km)}m∈[M]⊂Rd×d. On any input sequence H=
[h1,···,hN]∈Rd×N, we have H=Attnθ(H) = [h1,···,hN]∈Rd×N, where
hi=hi+X
m∈[M]1
iX
j∈[i]σr(⟨Qmhi,Kmhj⟩)·Vmhj.
Definition 2.3 (MLP Layer) .An MLP layer with hidden dimension d′is denoted as MLP θwith
parameters θ= (W1,W2)∈Rd′×d×Rd×d′. On any input sequence H= [h1,···,hN]∈Rd×N,
we have H=MLP θ(H) = [h1,···,hN]∈Rd×N, where
hi=hi+W2·σ(W1·hi).
The combination of masked attention layers and MLP layers leads to the overall decoder-based
transformer architecture studied in this work, as defined in the following.
Definition 2.4 (Decoder-based Transformer) .AnL-layer decoder-based transformer, denoted as
TFθ(·), is a composition of Lmasked attention layers, each followed by an MLP layer and a clip
operation: TFθ(H) =H(L)∈Rd×N, where H(L)is defined iteratively by taking H(0)=H∈
Rd×Nand for l∈[L],
H(l)=MLPθ(l)
mlp
Attnθ(l)
mattn
H(l−1)
∈Rd×N,
where parameter θ={(θ(l)
mattn,θ(l)
mlp) :l∈[L]}consists of θ(l)
mattn={(V(l)
m,Q(l)
m,K(l)
m) :m∈
[M]} ⊂Rd×dandθ(l)
mlp= (W(l)
1,W(l)
2)∈Rd′×d×Rd×d′.
We further define the parameter class of transformers as Θd,L,M,d′,F:={θ= (θ(1:L)
mattn,θ(1:L)
mlp) :
∥θ∥ ≤F}, where the norm of a transformer TF θis denoted as
∥θ∥:= max
l∈[L]
max
m∈[M]n
∥Q(l)
m∥op,∥K(l)
m∥opo
+X
m∈[M]∥V(l)
m∥op+∥W(l)
1∥op+∥W(l)
2∥op
.
Other Notations. The total variation distance between two algorithms {π, π′}upon Dt−1∪ {s}
is denoted as TV(π, π′|Dt−1, s) := TV(π(·|Dt−1, s), π′(·|Dt−1, s)). Also, the notation x≲y
indicates that xis a lower or equivalent order term compared with y, i.e., x=O(y),˜O(·)hides
poly-logarithmic terms in H, G, S, A, B , and poly(·)compactly denotes a polynomial term with
respect to the input.
3 Decentralized Learning
First, we study the decentralized learning setting, i.e., each player takes actions following her own
model independently without observing the opponent’s actions, as it better captures the unique
game-playing scenario considering in this work. This setting is aligned with the canonical study of
normal-form games [ 20,53], and has been extended to Markov games in recent years [ 32,41,51]. In
the following, we start by introducing the basic setup of supervised pre-training and provide a general
performance guarantee relying on a realizability assumption. Then, we provide a constructional result
4to demonstrate that the algorithms induced by transformers are rich enough to realize the celebrated
V-learning algorithm [ 32]. With these results, we finally establish that with V-learning providing
training data, the pre-trained transformer can effectively approximate NE when interacting with
different environments in an in-context fashion.
3.1 Supervised Pre-training Results
3.1.1 Basic Setups
Training Dataset. In the supervised pre-training, we use a context algorithm Alg0to collect the
offline trajectories. For the decentralized setting, the context algorithm Alg0used for data collection is
assumed to be consisted of two decoupled algorithms (Alg+,0,Alg−,0)for the max- and min-players,
respectively. With the context algorithm, we consider Ni.i.d. offline trajectories {Di:i∈[N]}are
collected, where Di:=Di∪D′
iwith
Di:={(st
i, at
i, bt
i, rt
i) :t∈[T]} ∼PAlg0
Λ(·);
D′
i:={(at
i,s, bt
i,s)∼Alg0(·,·|Dt−1
i, s) :t∈[T], s∈ S}.
It can be observed that Diis the commonly considered offline interaction trajectory of Alg0, while D′
i
is the sampled actions of each state sat each step twith Alg0. Compared with Lin et al. [38],D′
iis
an augmented component. We first note that collecting D′
iis relatively easy in practical applications,
as it only needs to additionally sample from the distribution Alg0(·,·|Dt−1
i, s)for each s∈ S
(i.e., no additional interactions with the environment). Moreover, the reason to incorporate such an
augmentation is to provide additional diverse pre-training data due to the unique game-theoretic
environment, with further discussions provided after the later Lemma 3.6. It has also been recognized
previously [ 16,72] that the data requirement for learning Markov games is typically much stronger
than that for single-agent RL.
To facilitate the decentralized training, the overall dataset is further split into two parts: {D+,i:i∈
[N]}and{D−,i:i∈[N]}, where
D+,i:=D+,i∪D′
+,i, D +,i:={(st
i, at
i, rt
i) :t∈[T]}, D′
+,i:={at
i,s:t∈[T], s∈ S};
D−,i:=D−,i∪D′
−,i, D−,i:={(st
i, bt
i, rt
i) :t∈[T]}, D′
−,i:={bt
i,s:t∈[T], s∈ S}.
In other words, Di,+denotes the observations of the max-player, while Di,−those of the min-layer.
Note that neither player can observe the opponent’s actions.
Algorithm Induced by Transformers. Due to the decentralized nature, two embedding mappings of
d+andd−dimensions are considered as h+:S ∪(A × R )→Rd+andh−:S ∪(B × R )→Rd−,
together with two transformers TFθ+andTFθ−. Taking the max-player’s transformer as representa-
tive, for trajectory (Dt−1
+, st), letH+=h+(Dt−1
+, st) = [ h+(s1),h+(a1, r1),···,h+(st)]be the
input to TFθ+, and the obtained output is H+=TFθ+(H+) = [h+,1,h+,2,···,h+,−2,h+,−1],
which has the same shape as H+. Similarly, the mapping h−is used for the min-player’s transformer
TFθ−to embed trajectory (Dt−1
−, st).
We further assume that two fixed linear extraction mappings, A∈RA×d+andB∈RB×d−, are used
to induce algorithms Algθ+andAlgθ−over the action spaces AandBof the max- and min-players,
respectively, as
Algθ+(·|Dt−1
+, st) =proj∆
A·TFθ+ 
h+(Dt−1
+, st)
−1
,
Algθ−(·|Dt−1
−, st) =proj∆
B·TFθ− 
h−(Dt−1
−, st)
−1
,(1)
where proj∆denotes the projection to a probability simplex.
Training Scheme. We consider the standard supervised pre-training to maximize the log-likelihood
of observing training datasets D+(resp., D−) over algorithms {Algθ+:θ+∈Θ+}(resp., {Algθ−:
θ−∈Θ−}) with Θ+:= Θ d+,L+,M+,d′
+,F+(resp., Θ−:= Θ d−,L−,M−,d′
−,F−). In particular, the
pre-training outputs bθ+andbθ−are determined as
bθ+= arg maxθ+∈Θ+1
NX
i∈[N]X
t∈[T]X
s∈Slog
Algθ+(at
i,s|Dt−1
+,i, s)
;
5bθ−= arg maxθ−∈Θ−1
NX
i∈[N]X
t∈[T]X
s∈Slog
Algθ−(bt
i,s|Dt−1
−,i, s)
.
3.1.2 Theoretical Guarantees
In this section, we provide a generalization guarantee of the algorithms Algbθ+andAlgbθ−pre-trained
following the scheme introduced above. First, the standard definition regarding the covering number
and an assumption of approximate realizability are introduced to facilitate the analysis, which are
also leveraged in Lin et al. [38].
Definition 3.1 (Decentralized Covering Number) .For a class of algorithms {Algθ+:θ+∈Θ+},
we say ˜Θ+⊆Θ+is aρ+-cover of Θ+, if˜Θ+is a finite set such that for any θ+∈Θ+, there exists
˜θ+∈˜Θ+such that for all Dt−1
+, s∈ S, t∈[T], it holds that
logAlg˜θ+(·|Dt−1
+, s)−logAlgθ+(·|Dt−1
+, s)
∞≤ρ+.
The covering number NΘ+(ρ+)is the minimal cardinality of ˜Θ+such that ˜Θ+is aρ+-cover of Θ+.
Similarly, we can define the ρ−-cover of Θ−and the covering number NΘ−(ρ−).
Assumption 3.2 (Decentralized Approximate Realizability) .There exist θ∗
+∈Θ+andε+,real>0
such that for all t∈[T], s∈ S, a∈ A, it holds that
log 
ED∼PAlg0
Λ"
Alg+,0(a|Dt−1
+, s)
Algθ∗
+(a|Dt−1
+, s)#!
≤ε+,real.
We also similarly assume ε−,real-approximate realizability of Alg−,0viaAlgθ∗
−withθ∗
−∈Θ−.
Then, we can establish the following generalization guarantee on the TV distance between
(Algbθ+,Algbθ−)andAlg0= (Alg0,+,Alg0,−), capturing their similarities.
Theorem 3.3 (Decentralized Pre-training Guarantee) .Letbθ+be the max-player’s pre-training output
defined in Sec. 3.1.1. Take NΘ+=NΘ+(1/N)as in Def. 3.1. Then, under Assumption 3.2, with
probability at least 1−δ, it holds that1
ED∼PAlg0
ΛX
t∈[T],s∈STV
Alg+,0,Algbθ+|Dt−1
+, s
≲TS√ε+,real+TSs
log 
NΘ+TS/δ
N.
A similar result holds for the min-players’ pre-training output bθ−.
Theorem 3.3 demonstrates that in expectation of the pre-training data distribution, i.e., PAlg0
Λ(D),
the TV distance between the pre-trained algorithm Algbθ+(resp, Algbθ−) and the context algorithm
Alg+,0(resp, Alg−,0) can be bounded via two terms: one from the approximate realizability, i.e.,
ε+,real(resp, ε−,real), and the other from the limited amount of pre-training trajectories, i.e., finite N.
While we can diminish the second term via a large pre-training dataset (i.e., sufficient pre-training
games), the key question is whether the transformer structure is sufficiently expressive to realize
the context algorithm, i.e., having a small ε+,real, which we affirmatively answer via an example of
realizing V-learning [32] in the next subsection.
3.2 Realizing V-learning
To demonstrate the capability of transformers in the decentralized game-playing setting, we choose
to prove that they can realize the renowned V-learning algorithm [ 32], the first design that breaks the
curse of multiple agents in learning Markov games. Particularly, V-learning leverages techniques
from adversarial bandits [ 5] to perform policy updates without observing the opponent’s actions. The
details of V-learning are provided in Appendix G.1, where its unique output rule is also elaborated.
In the following theorem, we demonstrate that a transformer can be constructed to exactly perform
V-learning with a suitable parameterization. One additional Assumption G.2 on the existence of
1The covering number NΘ+in Theorem 3.3 (and also the later Theorem C.3) is not concretely discussed in
the main paper to ease the presentation. A detailed illustration is provided in Appendix I.
6a transformer parameterized by the class of Θd,LD,MD,dD,FDto perform exact division is adopted
for the convenience of the proof, while in Appendix G.2, we further demonstrate that the required
division operation can be approximated to any arbitrary precision.
Theorem 3.4. With embedding mapping h+and extraction mapping Adefined in Appendix G.3,
under Assumption G.2, there exists a transformer TFθ+with
d≲HSA, L ≲GHL D,max
l∈[L]M(l)≲HS2+HSA +MD,
d′≲G+A+dD,∥θ∥≲GH2S+G3+FD,
which satisfies that for all Dt−1
+, s∈ S, t∈[T],Algθ+(·|Dt−1
+, s) = AlgV-learning (·|Dt−1
+, s). A
similar construction TFθ−exists for the min-player’s transformer such that for all Dt−1
−, s∈ S, t∈
[T],Algθ−(·|Dt−1
−, s) =AlgV-learning (·|Dt−1
−, s).
The proof of Theorem 3.4 (presented in Appendix G.3) is challenging because V-learning is a model-
free design, while UCB-VI [ 6] studied in Lin et al. [38] and VI-ULCB [ 8] later presented in Sec. 4.2
are both model-based ones. We believe this result deepens our understanding of the capability of
pre-trained transformers in decision-making, i.e., they can realize both model-based and model-free
designs, showcasing their further potentials.
More specifically, with the embedded trajectory as the input, the model-based philosophy is natural
for the masked attention mechanism, i.e., the value computation at each step is directly over all
raw inputs in previous steps. Thus, the construction procedure is straightforward as (input 1, input 2,
..., input t)→(value 1, value 2, ..., value t). However, the model-free designs are different, where
value computation at one step requires previous values (instead of raw inputs). In other words,
the construction procedure is a recursive one as (input 1, input 2, ..., input t)→(value 1, input 2, ...,
input t)→(value 1, value 2, ..., input t)→...→(value 1, value 2, ..., value t), whose realization requires
carefully crafted constructions.
3.3 The Overall ICGP Capablity
Finally, built upon the obtained results, the following theorem demonstrates the ICGP capability of
pre-trained transformers in the decentralized setting.
Theorem 3.5. LetΘ+andΘ−be the classes of transformers satisfying the requirements in Theo-
rem 3.4 and (Alg+,0,Alg−,0)both be V-learning. Let (ˆµ,ˆν)be the output policies via the output
rule of V-learning. Denoting Algbθ= (Algbθ+,Algbθ−)andNΘ=NΘ+NΘ−. Then, with probability
at least 1−δ, it holds that
ED∼PAlgbθ
Λh
V†,ˆν
M(s1)−Vˆµ,†
M(s1)i
≲r
H5S(A∨B) log( SABT )
G+THSr
log (TSNΘ/δ)
N.
With the obtained upper bound on the approximation error of NE, Theorem 3.5 demonstrates the
ICGP capability of pre-trained transformers as the algorithms Algˆθ+andAlgˆθ−are fixed during
interactions with varying inference games (i.e., no parameter updates). When prompted by the
interaction trajectory in the current game, they are capable of deciding the future interaction strategy
and finally provide policy pairs that are approximate NE. We further note that during both pre-training
and inference, each player’s transformer takes inputs of its own observed trajectories, but not the
opponent’s actions, which reflects the decentralized requirement. Moreover, the approximation error
in Theorem 3.5 depends on A∨Binstead of ABas in the later Theorem 4.2, evidencing the benefits
of decentralized learning.
Proof Sketch. The proof of Theorem 3.5 (presented in Appendix H) rely on the following, decompo-
sition, where E0[·]andEbθ[·]are with respect to PAlg0
ΛandPAlgbθ
Λ, respectively:
Ebθh
V†,ˆν
M(s1)−Vˆµ,†
M(s1)i
=E0h
V†,ˆν
M(s1)−Vˆµ,†
M(s1)i
(2)
+Ebθh
V†,ˆν
M(s1)i
−E0h
V†,ˆν
M(s1)i
+E0h
Vˆµ,†
M(s1)i
−Ebθh
Vˆµ,†
M(s1)i
, .
It can be observed that the first decomposed term is the performance of the considered V-learning,
which can be obtained following Jin et al. [32] as in Theorem G.1.
7Then, the remaining terms concern the performance of the policy pair (ˆµ,ˆν)learned from Alg0
andAlgbθagainst their own best responses , respectively. This is drastically different from the
consideration in Lin et al. [38], which only bounds the performance of the learned policies, i.e.,
E0h
Vˆµ,ˆν
M(s1)i
−Ebθh
Vˆµ,ˆν
M(s1)i
.
The involvement of best responses complicates the analysis. After careful treatments in Appendix H,
we obtain the following lemma to characterize these terms.
Lemma 3.6. For any two decentralized algorithms AlgαandAlgβ, we denote their performed
policies for episode gare(µg
α, νg
α)and(µg
β, νg
β), and their final output policies via the output rule of
V-learning (see Appendix G.1) are (ˆµα,ˆνα)and(ˆµβ,ˆνβ). For{ˆµα,ˆµβ}, it holds that
Eαh
Vˆµα,†
M(s1)i
−Eβh
Vˆµβ,†
M(s1)i
≲H·X
t∈[T],s∈SEα
TV 
µt
α, µt
β|Dt−1
+, s
+H·X
t∈[T],s∈SEα
TV 
νt
α, νt
β|Dt−1
−, s
,
where Eα[·]andEβ[·]are with respect to PAlgα
Λ andPAlgβ
Λ. A similar result holds for {ˆνα,ˆνβ}.
With Lemma 3.6, we can incorporate Theorem 3.3 to upper bound the TV distance between Alg0
andAlgbθ, which together with Theorem 3.4 establish εreal,+=εreal,−= 0in this case, leading to
the desired performance guarantee in Theorem 3.5. We here further note that the effectiveness of
Theorem 3.3 in capturing the bound in Lemma 3.6 over all s∈ Scredits to the augmented dataset
D′, which provides diverse data of all s∈ S.
4 Centralized Learning
In this section, we discuss the scenario of centralized learning, i.e., training one joint model to control
both players’ interactions. This is also known as the self-play setting [ 8,9,33,40,67]. Following a
similar procedure as the decentralized discussions, we first provide supervised pre-training guarantees
and then demonstrate that transformers are capable of realizing the renowned VI-ULCB algorithm
[8]. It is thus established that in a centralized learning setting, the pre-trained transformer can still
effectively perform ICGP and approximate NE.
4.1 Supervised Pre-training Results
The same training dataset {Di:i∈[N]}as in Section 3.1.1 is considered. As the centralized
setting is studied here, no further split of the dataset is needed. Moreover, one d-dimensional
mapping h:S ∪(A × B × R )→Rdcan be designed to embed the trajectories, and the induced
algorithm Algθ(·,·|Dt, st)from the transformer TFθcan be obtained via a fixed linear extraction
mapping Esimilarly as Eqn. (1). Finally, the MLE training is performed with Θ := Θ d,L,M,d′,Fas
bθ= arg maxθ∈Θ1
NP
i∈[N]P
t∈[T]P
s∈Slog 
Algθ(at
i,s, bt
i,s|Dt−1
i, s)
.
Then, a generalization guarantee of Algbθcan be provided similarly as Theorem 3.3, which is deferred
to Theorem C.3. This centralized result also implies that the pre-trained centralized algorithm
performs similarly as the context algorithm, with errors caused by the approximate realizability and
the finite pre-training data.
4.2 Realizing VI-ULCB
The VI-ULCB algorithm [ 8] is one of the first provably efficient centralized learning designs for
Markov games. It extends the key idea of using confidence bounds to incorporate uncertainties from
stochastic bandits and MDPs [ 4,6] to handle competitive environments, and has further inspired
many extensions in Markov games [ 9,30,33,40,65]. As VI-ULCB is highly representative, we
choose it as the example for realization in the centralized setting to demonstrate the capability of
transformers.
To make VI-ULCB practically implementable, we adopt an approximate CCE solver powered by
multiplicative weight update (MWU) in the place of its originally required general-sum NE solver
8(which is computationally demanding). This modification is demonstrated as provably efficient in
later works [ 9,40,65]. Then, the following result illustrates that a transformer can be constructed to
exactly perform the MWU-version of VI-ULCB.
Theorem 4.1. With embedding mapping hand extraction mapping Edefined in Appendix D.2, there
exists a transformer TFθwith
d≲HS2AB, L ≲GHS, max l∈[L]M(l)≲HS2AB,
d′≲G2HS2AB, ∥θ∥≲HS2AB+G3+GH,
which satisfies that for all Dt−1, s∈ S, t∈[T],Algθ(·,·|Dt−1, s) =AlgVI-ULCB (·,·|Dt−1, s).
One observation from the proof of Theorem 4.1 (presented in Appendix D.2) is that transformer
layers can perform MWU so that an approximate CCE can be found, which is not reported in Lin
et al. [38] and further demonstrates the in-context learning capability of transformers in playing
normal-form games (since MWU is one of the most basic designs).
4.3 The Overall ICGP Capability
With Theorem 4.1 showing VI-ULCB can be exactly realized (i.e., εreal= 0in Assumption C.2), we
can further prove an overall upper bound of the approximation error of NE by Algbθvia the following
theorem, demonstrating the ICGP capability of transformers.
Theorem 4.2. LetΘbe the class of transformers satisfying the requirements in Theorem 4.1 and
Alg0be VI-ULCB. For all (g, h, s )∈[G]×[H]× S, let(µg,h(·|s), νg,h(·|s))be the marginalized
policies of Algbθ(·,·|Dt−1, s). Then, with probability at least 1−δ, it holds that
EPAlgbθ
Λ,ˆµ,ˆνh
V†,ˆν
M(s1)−Vˆµ,†
M(s1)i
≲r
H4S2ABlog(SABT )
G+THSr
log(TSNΘ)/δ)
N,
where ˆµandˆνare uniformly sampled as ˆµ∼Unif{µ1,···, µG}andˆν∼Unif{ν1,···, νG}, with
µg:={µg,h(·|s) : (h, s)∈[H]× S} andνg:={νg,h(·|s) : (h, s)∈[H]× S} , andEˆµ,ˆνis with
respect to the process of policy sampling.
This result demonstrates the ICGP capability of pre-trained transformers in the centralized setting,
complementing the discussions in the decentralized results.
5 Empirical Experiments
Experiments are performed on two-player zero-sum normal-form games ( H= 1) and Markov games
(H= 2), with the decentralized EXP3 [ 5] (which can be viewed as a one-step V-learning) and
the centralized VI-ULCB being the context algorithms as demonstrations, respectively. Additional
experimental setups and details can be found in Appendix J. It can be first observed from Fig. 2
that, the transformers pre-trained with N= 20 games performs better on the inference tasks than
the ones pre-trained with N= 10 games. This observation empirically validates the theoretical
result that more pre-training games benefit the final game-playing performance during inference
(i.e., thep
1/N-dependencies established in Theorems 3.5 and 4.2). Moreover, when the number of
pre-training games is sufficient (i.e., N= 20 in Fig. 2), the obtained transformers can indeed learn to
approximate NE in an in-context manner (i.e., having a gradually decaying NE gap), and also the
obtained performance is similar to the context algorithm, i.e., EXP3 or VI-ULCB. These observations
provide empirical pieces of evidence to support the ICGP capabilities of pre-trained transformers,
motivating and validating the theoretical analyses performed in this work.
6 Related Works
In-context Learning. Since GPT-3 [ 11] demonstrates the ICL capability of pre-trained transformers,
growing attention has been paid to this direction. In particular, an emerging line of work targets
providing a deeper understanding of the fundamental mechanism behind the ICL capability [ 3,24,
29,31,37,44,59,60,64,66,68], where many interesting results have been obtained. In particular,
transformers have been shown to be capable of performing in-context gradient descent so that
90 500 1000 1500 2000 2500 3000
Episodes0.2250.2500.2750.3000.3250.3500.3750.400NE GapTransformer (20 envs)
Transformer (10 envs)
Exp3(a) Decentralized Comparisons With EXP3.
0 50 100 150 200 250 300
Episodes0.500.550.600.650.700.750.800.85NE GapTransformer (20 envs)
Transformer (10 envs)
VI-ULCB (b) Centralized Comparisons With VI-ULCB.
Figure 2: Comparisons of Nash equilibrium (NE) gaps over episodes in both decentralized and
centralized learning scenarios, averaged over 10inference games.
varying optimization-based algorithms can be realized [ 2,3,7,26,59]. Also, Giannou et al. [27]
demonstrates that looped transformers can emulate basic computing blocks, whose combinations can
lead to complex operations.
This work is more focused on the in-context reinforcement learning (ICRL) capability of pre-trained
transformers, as demonstrated in Grigsby et al. [28], Laskin et al. [34], Lee et al. [35], Wang et al. [62].
The recent work by Lin et al. [38] initiates the theoretical investigation of this topic. In particular, Lin
et al. [38] provides generalization guarantees after pre-training in the single-agent RL scenario, and
further constructs transformers to realize provably efficient single-agent bandits and RL algorithms
(in particular, LinUCB [ 1], Thompson sampling [ 54], UCB-VI [ 6]). This work extends Lin et al. [38]
to the domain of competitive multi-agent RL by studying the in-context game-playing setting. A
recent concurrent work [ 36] also touches upon the in-context game-playing capability of pre-trained
transformers, while focusing on practical aspects and exploiting different opponents.
Competitive Multi-agent RL. The study of RL in the competitive multi-agent domain has a long
and fruitful history [ 10,47,49,58,70]. In recent years, researchers have gained a deeper theoretical
understanding of this topic. The centralized setting (also known as self-play) has been investigated in
Bai and Jin [8], Bai et al. [9], Cui et al. [17], Huang et al. [30], Jin et al. [33], Liu et al. [40], Wang
et al. [63], Xiong et al. [67], Zhang et al. [69], and this work focuses on the representative VI-ULCB
design [ 9]. On the other hand, decentralized learning is more challenging, and the major breakthrough
is made by V-learning [9, 32, 41, 51], which is thus adopted as the target algorithm in this work.
7 Conclusions
This work investigated the in-context game-playing (ICGP) capabilities of pre-trained transformers,
broadening the research scope of in-context RL from the single-agent scenario to the more challenging
multi-agent competitive games. Focusing on the classical two-player zero-sum Markov games, a
general learning framework was first introduced, laying down a solid ground for this and later studies.
Through concrete theoretical results, this work further demonstrated that in both decentralized and
centralized learning settings, properly pre-trained transformers are capable of approximating Nash
equilibrium in an in-context manner. As a key part of the proof, concrete sets of parameterization
were provided to demonstrate that the transformer architecture can realize two famous designs,
decentralized V-learning and centralized VI-ULCB. Empirical experiments further validate the
theoretical results (especially that pre-trained transformers can indeed approximate NE in an in-
context manner) and motivate future studies on this under-explored research direction.
10Acknowledgments and Disclosure of Funding
The work of CSs and KY was supported in part by the US National Science Foundation (NSF) under
awards CNS-2002902, ECCS- 2029978, ECCS-2143559, and CNS-2313110, and the Bloomberg
Data Science Ph.D. Fellowship. The work of JY was supported in part by the US NSF under awards
CNS-1956276 and CNS-2114542.
References
[1]Abbasi-Yadkori, Y ., Pál, D., and Szepesvári, C. (2011). Improved algorithms for linear stochastic
bandits. Advances in neural information processing systems , 24.
[2]Ahn, K., Cheng, X., Daneshmand, H., and Sra, S. (2023). Transformers learn to implement
preconditioned gradient descent for in-context learning. arXiv preprint arXiv:2306.00297 .
[3]Akyürek, E., Schuurmans, D., Andreas, J., Ma, T., and Zhou, D. (2022). What learning algorithm
is in-context learning? investigations with linear models. arXiv preprint arXiv:2211.15661 .
[4]Auer, P., Cesa-Bianchi, N., and Fischer, P. (2002a). Finite-time analysis of the multiarmed bandit
problem. Machine learning , 47:235–256.
[5]Auer, P., Cesa-Bianchi, N., Freund, Y ., and Schapire, R. E. (2002b). The nonstochastic multiarmed
bandit problem. SIAM journal on computing , 32(1):48–77.
[6]Azar, M. G., Osband, I., and Munos, R. (2017). Minimax regret bounds for reinforcement
learning. In International Conference on Machine Learning , pages 263–272. PMLR.
[7]Bai, Y ., Chen, F., Wang, H., Xiong, C., and Mei, S. (2023). Transformers as statisticians: Provable
in-context learning with in-context algorithm selection. arXiv preprint arXiv:2306.04637 .
[8]Bai, Y . and Jin, C. (2020). Provable self-play algorithms for competitive reinforcement learning.
InInternational conference on machine learning , pages 551–560. PMLR.
[9]Bai, Y ., Jin, C., and Yu, T. (2020). Near-optimal reinforcement learning with self-play. Advances
in neural information processing systems , 33:2159–2170.
[10] Berner, C., Brockman, G., Chan, B., Cheung, V ., D˛ ebiak, P., Dennison, C., Farhi, D., Fischer,
Q., Hashme, S., Hesse, C., et al. (2019). Dota 2 with large scale deep reinforcement learning.
arXiv preprint arXiv:1912.06680 .
[11] Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A.,
Shyam, P., Sastry, G., Askell, A., et al. (2020). Language models are few-shot learners. Advances
in neural information processing systems , 33:1877–1901.
[12] Carion, N., Massa, F., Synnaeve, G., Usunier, N., Kirillov, A., and Zagoruyko, S. (2020).
End-to-end object detection with transformers. In European conference on computer vision , pages
213–229. Springer.
[13] Cesa-Bianchi, N. and Lugosi, G. (2006). Prediction, learning, and games . Cambridge university
press.
[14] Cheng, X., Chen, Y ., and Sra, S. (2023). Transformers implement functional gradient descent to
learn non-linear functions in context. arXiv preprint arXiv:2312.06528 .
[15] Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts, A., Barham, P., Chung,
H. W., Sutton, C., Gehrmann, S., et al. (2023). Palm: Scaling language modeling with pathways.
Journal of Machine Learning Research , 24(240):1–113.
[16] Cui, Q. and Du, S. S. (2022). When are offline two-player zero-sum markov games solvable?
Advances in Neural Information Processing Systems , 35:25779–25791.
[17] Cui, Q., Zhang, K., and Du, S. (2023). Breaking the curse of multiagents in a large state space:
Rl in markov games with independent linear function approximation. In The Thirty Sixth Annual
Conference on Learning Theory , pages 2651–2652. PMLR.
11[18] Dai, D., Sun, Y ., Dong, L., Hao, Y ., Ma, S., Sui, Z., and Wei, F. (2023). Why can gpt learn
in-context? language models secretly perform gradient descent as meta-optimizers. In Findings of
the Association for Computational Linguistics: ACL 2023 , pages 4005–4019.
[19] Daskalakis, C. (2013). On the complexity of approximating a nash equilibrium. ACM Transac-
tions on Algorithms (TALG) , 9(3):1–35.
[20] Daskalakis, C., Fishelson, M., and Golowich, N. (2021). Near-optimal no-regret learning in
general games. Advances in Neural Information Processing Systems , 34:27604–27616.
[21] Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. (2018). Bert: Pre-training of deep
bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805 .
[22] Dong, Q., Li, L., Dai, D., Zheng, C., Wu, Z., Chang, B., Sun, X., Xu, J., and Sui, Z. (2022). A
survey for in-context learning. arXiv preprint arXiv:2301.00234 .
[23] Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., De-
hghani, M., Minderer, M., Heigold, G., Gelly, S., et al. (2020). An image is worth 16x16 words:
Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929 .
[24] Fu, D., Chen, T.-Q., Jia, R., and Sharan, V . (2023). Transformers learn higher-order optimization
methods for in-context learning: A study with linear models. arXiv preprint arXiv:2310.17086 .
[25] Gao, B. and Pavel, L. (2017). On the properties of the softmax function with application in
game theory and reinforcement learning. arXiv preprint arXiv:1704.00805 .
[26] Garg, S., Tsipras, D., Liang, P. S., and Valiant, G. (2022). What can transformers learn in-
context? a case study of simple function classes. Advances in Neural Information Processing
Systems , 35:30583–30598.
[27] Giannou, A., Rajput, S., Sohn, J.-y., Lee, K., Lee, J. D., and Papailiopoulos, D. (2023). Looped
transformers as programmable computers. In International Conference on Machine Learning ,
pages 11398–11442. PMLR.
[28] Grigsby, J., Fan, L., and Zhu, Y . (2023). Amago: Scalable in-context reinforcement learning for
adaptive agents. arXiv preprint arXiv:2310.09971 .
[29] Guo, T., Hu, W., Mei, S., Wang, H., Xiong, C., Savarese, S., and Bai, Y . (2023). How do trans-
formers learn in-context beyond simple functions? a case study on learning with representations.
arXiv preprint arXiv:2310.10616 .
[30] Huang, B., Lee, J. D., Wang, Z., and Yang, Z. (2021). Towards general function approximation
in zero-sum markov games. arXiv preprint arXiv:2107.14702 .
[31] Huang, Y ., Cheng, Y ., and Liang, Y . (2023). In-context convergence of transformers. arXiv
preprint arXiv:2310.05249 .
[32] Jin, C., Liu, Q., Wang, Y ., and Yu, T. (2023). V-learning—a simple, efficient, decentralized
algorithm for multiagent reinforcement learning. Mathematics of Operations Research .
[33] Jin, C., Liu, Q., and Yu, T. (2022). The power of exploiter: Provable multi-agent rl in large state
spaces. In International Conference on Machine Learning , pages 10251–10279. PMLR.
[34] Laskin, M., Wang, L., Oh, J., Parisotto, E., Spencer, S., Steigerwald, R., Strouse, D., Hansen, S.,
Filos, A., Brooks, E., et al. (2022). In-context reinforcement learning with algorithm distillation.
arXiv preprint arXiv:2210.14215 .
[35] Lee, J. N., Xie, A., Pacchiano, A., Chandak, Y ., Finn, C., Nachum, O., and Brunskill, E.
(2023). Supervised pretraining can learn in-context reinforcement learning. arXiv preprint
arXiv:2306.14892 .
[36] Li, S., Yang, C., Zhang, Y ., Li, P., Wang, X., Huang, X., Chan, H., and An, B. (2024). In-context
exploiter for extensive-form games. arXiv preprint arXiv:2408.05575 .
12[37] Li, Y ., Ildiz, M. E., Papailiopoulos, D., and Oymak, S. (2023). Transformers as algorithms:
Generalization and stability in in-context learning. In International Conference on Machine
Learning , pages 19565–19594. PMLR.
[38] Lin, L., Bai, Y ., and Mei, S. (2023). Transformers as decision makers: Provable in-context
reinforcement learning via supervised pretraining. arXiv preprint arXiv:2310.08566 .
[39] Liu, J., Shen, D., Zhang, Y ., Dolan, B., Carin, L., and Chen, W. (2021a). What makes good
in-context examples for gpt- 3?arXiv preprint arXiv:2101.06804 .
[40] Liu, Q., Yu, T., Bai, Y ., and Jin, C. (2021b). A sharp analysis of model-based reinforcement
learning with self-play. In International Conference on Machine Learning , pages 7001–7010.
PMLR.
[41] Mao, W. and Ba¸ sar, T. (2023). Provably efficient reinforcement learning in decentralized
general-sum markov games. Dynamic Games and Applications , 13(1):165–186.
[42] OpenAI (2023). GPT-4 technical report. arXiv preprint , arXiv:2303.08774.
[43] Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., and Sutskever, I. (2019). Language
models are unsupervised multitask learners.
[44] Raventós, A., Paul, M., Chen, F., and Ganguli, S. (2023). Pretraining task diversity and the
emergence of non-bayesian in-context learning for regression. arXiv preprint arXiv:2306.15063 .
[45] Reddy, G. (2023). The mechanistic basis of data dependence and abrupt learning in an in-context
classification task. In The Twelfth International Conference on Learning Representations .
[46] Roughgarden, T. (2010). Algorithmic game theory. Communications of the ACM , 53(7):78–86.
[47] Shapley, L. S. (1953). Stochastic games. Proceedings of the national academy of sciences ,
39(10):1095–1100.
[48] Shoham, Y . and Leyton-Brown, K. (2008). Multiagent systems: Algorithmic, game-theoretic,
and logical foundations . Cambridge University Press.
[49] Silver, D., Schrittwieser, J., Simonyan, K., Antonoglou, I., Huang, A., Guez, A., Hubert, T.,
Baker, L., Lai, M., Bolton, A., et al. (2017). Mastering the game of go without human knowledge.
nature , 550(7676):354–359.
[50] Sinii, V ., Nikulin, A., Kurenkov, V ., Zisman, I., and Kolesnikov, S. (2023). In-context reinforce-
ment learning for variable action spaces. arXiv preprint arXiv:2312.13327 .
[51] Song, Z., Mei, S., and Bai, Y . (2021). When can we learn general-sum markov games with a
large number of players sample-efficiently? arXiv preprint arXiv:2110.04184 .
[52] Sutton, R. S. (1988). Learning to predict by the methods of temporal differences. Machine
learning , 3:9–44.
[53] Syrgkanis, V ., Agarwal, A., Luo, H., and Schapire, R. E. (2015). Fast convergence of regularized
learning in games. Advances in Neural Information Processing Systems , 28.
[54] Thompson, W. R. (1933). On the likelihood that one unknown probability exceeds another in
view of the evidence of two samples. Biometrika , 25(3/4):285–294.
[55] Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Rozière, B.,
Goyal, N., Hambro, E., Azhar, F., et al. (2023a). Llama: Open and efficient foundation language
models. arXiv preprint arXiv:2302.13971 .
[56] Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y ., Bashlykov, N., Batra,
S., Bhargava, P., Bhosale, S., et al. (2023b). Llama 2: Open foundation and fine-tuned chat models.
arXiv preprint arXiv:2307.09288 .
[57] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., and
Polosukhin, I. (2017). Attention is all you need. Advances in neural information processing
systems , 30.
13[58] Vinyals, O., Babuschkin, I., Czarnecki, W. M., Mathieu, M., Dudzik, A., Chung, J., Choi,
D. H., Powell, R., Ewalds, T., Georgiev, P., et al. (2019). Grandmaster level in starcraft ii using
multi-agent reinforcement learning. Nature , 575(7782):350–354.
[59] V on Oswald, J., Niklasson, E., Randazzo, E., Sacramento, J., Mordvintsev, A., Zhmoginov, A.,
and Vladymyrov, M. (2023a). Transformers learn in-context by gradient descent. In International
Conference on Machine Learning , pages 35151–35174. PMLR.
[60] V on Oswald, J., Niklasson, E., Schlegel, M., Kobayashi, S., Zucchet, N., Scherrer, N., Miller,
N., Sandler, M., Vladymyrov, M., Pascanu, R., et al. (2023b). Uncovering mesa-optimization
algorithms in transformers. arXiv preprint arXiv:2309.05858 .
[61] Wainwright, M. J. (2019). High-dimensional statistics: A non-asymptotic viewpoint , volume 48.
Cambridge university press.
[62] Wang, J., Blaser, E., Daneshmand, H., and Zhang, S. (2024). Transformers learn temporal
difference methods for in-context reinforcement learning. arXiv preprint arXiv:2405.13861 .
[63] Wang, Y ., Liu, Q., Bai, Y ., and Jin, C. (2023). Breaking the curse of multiagency: Provably effi-
cient decentralized multi-agent rl with function approximation. arXiv preprint arXiv:2302.06606 .
[64] Wu, J., Zou, D., Chen, Z., Braverman, V ., Gu, Q., and Bartlett, P. L. (2023). How many pretrain-
ing tasks are needed for in-context learning of linear regression? arXiv preprint arXiv:2310.08391 .
[65] Xie, Q., Chen, Y ., Wang, Z., and Yang, Z. (2020). Learning zero-sum simultaneous-move
markov games using function approximation and correlated equilibrium. In Conference on learning
theory , pages 3674–3682. PMLR.
[66] Xie, S. M., Raghunathan, A., Liang, P., and Ma, T. (2021). An explanation of in-context learning
as implicit bayesian inference. arXiv preprint arXiv:2111.02080 .
[67] Xiong, W., Zhong, H., Shi, C., Shen, C., and Zhang, T. (2022). A self-play posterior sampling
algorithm for zero-sum markov games. In International Conference on Machine Learning , pages
24496–24523. PMLR.
[68] Yun, C., Bhojanapalli, S., Rawat, A. S., Reddi, S. J., and Kumar, S. (2019). Are transformers
universal approximators of sequence-to-sequence functions? arXiv preprint arXiv:1912.10077 .
[69] Zhang, K., Kakade, S., Basar, T., and Yang, L. (2020). Model-based multi-agent rl in zero-sum
markov games with near-optimal sample complexity. Advances in Neural Information Processing
Systems , 33:1166–1178.
[70] Zhang, K., Yang, Z., and Ba¸ sar, T. (2021). Multi-agent reinforcement learning: A selective
overview of theories and algorithms. Handbook of reinforcement learning and control , pages
321–384.
[71] Zhang, R., Frei, S., and Bartlett, P. L. (2023). Trained transformers learn linear models
in-context. arXiv preprint arXiv:2306.09927 .
[72] Zhong, H., Xiong, W., Tan, J., Wang, L., Zhang, T., Wang, Z., and Yang, Z. (2022). Pes-
simistic minimax value iteration: Provably efficient equilibrium learning from offline datasets. In
International Conference on Machine Learning , pages 27117–27142. PMLR.
[73] Zisman, I., Kurenkov, V ., Nikulin, A., Sinii, V ., and Kolesnikov, S. (2023). Emergence of
in-context reinforcement learning from noise distillation. arXiv preprint arXiv:2312.12275 .
14A An Overview of the Appendix
In this section, an overview of the appendix is provided. First, additional discussions are presented in
Appendix B, which cover broader impacts of this work and our thoughts on the future directions.
Then, the proof details omitted in this main paper are provided. While the decentralized learning
setting is the major focus in the main paper, the discussions and proofs for the centralized learning
setting are first provided to facilitate the presentation and understanding as the decentralized learning
setting is more challenging.
•The supervised pre-training guarantee (i.e., Theorem C.3) for the centralized learning setting is
proved in Appendix C. The details and realization of VI-ULCB (i.e., Theorem 4.1) are presented in
Appendix D. The proofs for the overall performance guarantee (i.e., Theorem 4.2) can be found in
Appendix E.
•Subsequently, the proofs for the supervised pre-training guarantee (i.e., Theorem 3.3) in the
decentralized learning setting are provided in Appendix F. Appendix G contains the details and
realization of V-learning (i.e., Theorem 3.4). The overall performance guarantee (i.e., Theorem 3.5)
is proved in Appendix H.
• A detailed discussion of the covering number is provided in Appendix I.
Finally, the setups and details of the experiments presented in Sec. 5 are reported in Appendix J.
B Additional Discussions
B.1 Broader Impacts
This work mainly provides a theoretical understanding of the in-context game-playing capabilities of
pre-trained transformers, broadening the research scope of in-context reinforcement learning from
single-agent settings to multi-agent competitive games. Due to its theoretical nature, we do not
foresee major negative societal impacts; however, we still would like to acknowledge the need for
responsible usage of the practical implementation of the proposed game-playing transformers due to
their high capability in various environments.
B.2 Limitations and Future Works
The research direction of in-context game-playing is currently under-explored, and we believe that
there are many interesting topics to be further investigated.
•Different game forms and game-solving algorithms. This work mainly studies the classical two-
player zero-sum Markov games, which can be viewed as the most basic form of competitive games,
and has particular focuses on constructing transformers to realize V-learning [ 32] and VI-ULCB [ 8].
The cooperative games, on the other hand, are conceptually more similar to the single-agent setting
[38]. There are more complicated game forms [ 46,48], e.g., the mixed cooperative-competitive
games, which requires different game-solving algorithms. We believe the framework built in this work
is beneficial to further explore the capabilities of pre-trained models in game-theoretical scenarios.
•Pre-training dataset construction. This work considering the pre-training dataset is collected from
the context algorithm with an additional augmentation. First, while the current proofs rely on the
augmentation, it will be an interesting topic to understand whether it is necessary. As mentioned
Sec. 3.1.1, learning in Markov games typically require more diverse data than learning in single-
agent settings; however, the minimum requirement to perform effective pre-training is worth further
exploring. Moreover, in the study of single-agent RL [ 35], it is shown that pre-training with the
data from the optimal policy is more efficient, which is further theoretically investigated in Lin et al.
[38]. In multi-agent competitive games, it is currently unclear whether similar strategies can be
incorporated, e.g., pre-training with data collected by Nash equilibrium policies or best responses for
certain other policies.
•Large-scale empirical evaluations. Due to the limited computation resources, the experiments
reported in Sec. 5 are relatively small-scale compared with the current size of practically adopted
transformers. It would be an important and interesting direction to further evaluate the ICGP
capabilities of pre-trained transformers in large-scale experiments and practical game-theoretic
15applications. Also, the training dynamics are also worth further investigation, e.g., the sudden shifts
in learning effectiveness reported by Reddy [45].
Besides these directions, from the theoretical perspective, we believe it would be valuable to investi-
gate how to extend the current study on the tabular setting to incorporate function approximation,
where we conjecture it is sufficient for the pre-training dataset to cover information of certain rep-
resentative states and actions (e.g., a well-coverage of the feature space) [ 72]. Another attractive
theoretical question is how to learn from a dataset collected by multiple context algorithms. From
the practical perspective, a future study on the impact of the practical training recipe (e.g., model
structure, training hyperparameters, etc.) would be desirable to bring additional insights.
C Proofs for the Centralized Supervised Pre-training Guarantees
First, the definition of the centralized covering number and an assumption of centralized approximate
realizability are introduced to facilitate the analysis, which are also leveraged in Lin et al. [38].
Definition C.1 (Centralized Covering Number) .For a class of algorithms {Algθ:θ∈Θ}, we say
˜Θ⊆Θis aρ-cover of Θ, if˜Θis a finite set such that for any θ∈Θ, there exists ˜θ∈˜Θsuch that for
allDt−1, s∈ S, t∈[T], it holds that
logAlg˜θ(·,·|Dt−1, s)−logAlgθ(·,·|Dt−1, s)
∞≤ρ.
The covering number NΘ(ρ)is the minimal cardinality of ˜Θsuch that ˜Θis aρ-cover of Θ.
Assumption C.2 (Centralized Approximate Realizability) .There exist θ∗∈Θandεreal>0such
that for all s∈ S, t∈[T],(a, b)∈ A × B , it holds that
log
ED∼PAlg0
ΛAlg0(a, b|Dt−1, s)
Algθ∗(a, b|Dt−1, s)
≤εreal.
Then, the following pre-training guarantee can be established.
Theorem C.3 (Centralized Pre-training Guarantee) .Letbθbe the maximum likelihood pre-training
output. Take NΘ=NΘ(1/N)as in Definition C.1. Then, under Assumption C.2, with probability at
least1−δ, it holds that
ED∼PAlg0
ΛX
t∈[T],s∈STV(Alg0,Algbθ|Dt−1, s)
≲TS√εreal+TSr
log (NΘTS/δ )
N.
Proof of Theorem C.3. This proof extends that of Theorem 6 in Lin et al. [38] to the multi-agent
scenario. Let ˜Θbe a ρ-covering set of Θwith covering number NΘ=NΘ(ρ)as defined in
Definition C.1. With Lemma 15 in Lin et al. [38], we can obtain that for any θ∈Θ, there exists
˜θ∈˜Θsuch that for all Dt−1,t∈[T]ands∈ S,
TV 
Alg˜θ,Algθ|Dt−1, s)
≤ρ
Form∈[NΘ], t∈[T], i∈[N], s∈ S, we define that
ℓt
i,m(s) := log 
Alg0 
at
i,s, bt
i,s|Dt−1
i, s
Algθm 
at
i,s, bt
i,s|Dt−1
i, s!
.
According to Lemma 14 in Lin et al. [38], with probability at least 1−δ, for all m∈[NΘ], t∈
[T], s∈ S, it holds that
1
2X
i∈[N]ℓt
i,m(s) + log( NΘTS/δ )≥X
i∈[N]−log
E
exp
−ℓt
i,m(s)
2
.
Furthermore, it can be established that
E
exp
−ℓt
i,m(s)
2
|Dt−1
i
=E
vuutAlgθm 
at
i,s, bt
i,s|Dt−1
i, s
Alg0 
at
i,s, bt
i,s|Dt−1
i, s|Dt−1
i

16=X
(a,b)∈A×Bq
Algθm 
a, b|Dt−1
i, s
Alg0 
a, b|Dt−1
i, s
,
which implies that
E
exp
−ℓt
i,m(s)
2
= 1−1
2·E
X
(a,b)∈A×Bq
Algθm 
a, b|Dt−1
i, s
−q
Alg0 
a, b|Dt−1
i, s2

≤1−1
2·Eh
TV 
Algθm,Alg0|Dt−1
i, s2i
,
where the inequality is from the fact that the Hellinger distance is smaller than the TV distance.
Then, we can obtain that for any θcovered by θm, it holds that
 
ED
TV 
Alg0,Algθ|Dt−1, s2
≤ 
ED
TV 
Alg0,Algθm|Dt−1, s
+ED
TV 
Algθm,Algθ|Dt−1, s2
≤2 
ED
TV 
Alg0,Algθm|Dt−1, s2+ 2 
ED
TV 
Algθm,Algθ|Dt−1, s2
≤2EDh
TV 
Alg0,Algθ|Dt−1, s2i
+ 2ρ2
≤4−4E
exp
−ℓt
i,m(s)
2
+ 2ρ2
≤ −4 log
E
exp
−ℓt
i,m(s)
2
+ 2ρ2,
which further implies that
NX
s∈SX
t∈[T] 
ED
TV 
Alg0,Algθ|Dt−1, s2
≤ −4X
s∈SX
t∈[T]X
i∈[N]log
E
exp
−ℓt
i,m(s)
2
+ 2NSTρ2
≤2X
s∈SX
t∈[T]X
i∈[N]ℓt
i,m(s) + 2NSTρ2+ 4STlog(NΘTS/δ )
= 2X
s∈SX
t∈[T]X
i∈[N]log 
Alg0 
at
i,s, bt
i,s|Dt−1
i, s
Algθm 
at
i,s, bt
i,s|Dt−1
i, s!
+ 2NSTρ2+ 4STlog(NΘTS/δ )
≤2X
s∈SX
t∈[T]X
i∈[N]log 
Alg0 
at
i,s, bt
i,s|Dt−1
i, s
Algθ 
at
i,s, bt
i,s|Dt−1
i, s!
+ 2NSTρ2+ 2NSTρ + 4STlog(NΘTS/δ ).
Thus, for the obtained bθ, with probability at least 1−δ, it holds that
NX
s∈SX
t∈[T] 
ED
TV 
Alg0,Algbθ|Dt−1, s2
≤2X
s∈SX
t∈[T]X
i∈[N]log 
Alg0 
at
i,s, bt
i,s|Dt−1
i, s
Algbθ 
at
i,s, bt
i,s|Dt−1
i, s!
+ 2NSTρ2+ 2NSTρ + 4STlog(NΘTS/δ )
≤2X
s∈SX
t∈[T]X
i∈[N]log 
Alg0 
at
i,s, bt
i,s|Dt−1
i, s
Algθ∗ 
at
i,s, bt
i,s|Dt−1
i, s!
+ 2NSTρ2+ 2NSTρ + 4STlog(NΘTS/δ )
≤2X
s∈SX
t∈[T]X
i∈[N]log 
E"
Alg0 
at
i,s, bt
i,s|Dt−1
i, s
Algθ∗ 
at
i,s, bt
i,s|Dt−1
i, s#!
+STlog(TS/δ )
+ 2NSTρ2+ 2NSTρ + 4STlog(NΘTS/δ )
17≤2NSTε real+STlog(TS/δ ) + 2NSTρ2+ 2NSTρ + 4STlog(NΘTS/δ ),
Further by Cauchy-Schwarz inequality, we can obtain that
X
s∈SX
t∈[T] 
ED
TV 
Alg0,Algbθ|Dt−1, s
≤s
STX
s∈SX
t∈[T] 
ED
TV 
Alg0,Algbθ|Dt−1, s2
=O 
ST√εreal+STr
log(NΘST)
N+ST√ρ+STρ!
Taking ρ= 1/Nconcludes the proof.
D Proofs for Realizing VI-ULCB
D.1 Details of MWU VI-ULCB
We here note one distinction from the VI-ULCB design considered in this work from its vanilla
version proposed in Bai and Jin [8], which makes VI-ULCB practically implementable. Especially,
Bai and Jin [8]requires an oracle solver that can provide the exact NE policy pair (µ∗, ν∗)from any
two general input payoff matrices (Q, Q)∈RA×B×RA×B. However, it is known that approximating
such a general-sum NE is computationally hard (specifically, PPAD-complete) [ 19], which makes
this vanilla version impractical. Luckily, later studies [ 9,40,65] have demonstrated that a solver
finding one weaker notation of equilibrium, i.e., coarse correlated equilibrium (CCE), is already
sufficient. Following these recent results, we replace the NE solver with an approximate CCE solver
in VI-ULCB. Moreover, we consider finding such CCEs via no-regret learning.2In particular, both
players virtually run multiplicative weight update (MWU) (which is also known as Hedge ), a classical
no-regret algorithm, with payoff matrices (Q, Q)for several rounds; then, an aggregated policy can
be generated as an approximate CCE. The details of the VI-ULCB algorithm are provided in Alg. 1.
More specifically, we consider that an approximate CCE solver is adopted such that with each pair
of inputs (Qh(s,·,·), Qh(s,·,·)), we can obtain an εCCE-approximate CCE policy πh(·,·|s)which
satisfies that
E(a,b)∼πh(·,·|s)
Q(s, a, b )
≥max
a∗∈SE(a,b)∼πh(·,·|s)
Q(s, a∗, b)
−εCCE
E(a,b)∼πh(·,·|s)
Q(s, a, b )
≤min
b∗∈SE(a,b)∼πh(·,·|s)
Q(s, a, b∗)
+εCCE.
We also specifically choose to obtain such approximate CCEs by having both players (virtually)
perform MWU against each other. The details of MWU are included in Alg. 2, where we use the
following notations to denote normalized losses:
Lh(s, a, b ) :=H−Qh(s, a, b )
H, Lh(s, a, b ) :=H−Qh(s, a, b )
H.
Standard online learning results [ 13] guarantee that using learning rates ηA=p
log(A)/NMWU and
ηB=p
log(B)/NMWU, after NMWU rounds of MWU, the policy
πh(·,·|s) =1
NMWUX
n∈[N]µh
n(·|s)νh
n(·|s)
is anεCCE-approximate CCE policy, with
εCCE=Hs
log(A+B)
NMWU.
2Another common method to find CCEs is through linear programming (LP). It will be an interesting direction
to investigate whether transformers can be LP solvers, which is however out of the scope of this paper.
18Algorithm 1 VI-ULCB
1:Initialize : for any (s, a, b, h ),Qh(s, a, b )←H,Qh(s, a, b )←0,Nh(s, a, b )←0,
Nh(s, a, b, s′)←0
2:forepisode g= 1,···, Gdo
3: for(s, a, b )∈ S × A × B do
4: Compute Qh(s, a, b )←minn
ˆrh(s, a, b ) +h
ˆPhVh+1i
(s, a, b ) +cq
H2Sι
Nh(sh,ah,bh), Ho
5: Compute Qh(s, a, b )←maxn
ˆrh(s, a, b ) +h
ˆPhVh+1i
(s, a, b )−cq
H2Sι
Nh(sh,ah,bh),0o
6: end for
7: fors∈ S do
8: Update πh(·,·|s)←εN-approximate CCE
Qh(s,·,·), Qh(s,·,·)
solved by N-round
MWU
9: Compute Vh(s)←P
a,bπh(a, b|s)Qh(s, a, b )
10: Compute Vh(s)←P
a,bπh(a, b|s)Qh(s, a, b )
11: end for
12: forsteph= 1,···, Hdo
13: Take action (ah, bh)∼πh(·,·|sh)
14: Observe reward rhand next state sh+1
15: Update Nh(sh, ah, bh)andNh(sh, ah, bh, sh+1)
16: Update ˆPh(·|sh, ah, bh)andˆrh(sh, ah, bh)
17: end for
18:end for
Algorithm 2 MWU
1:Input : learning rates ηA=p
log(A)/NandηB=p
log(B)/N, action sets AandBwith size
AandB, loss matrices Lh(s,·,·)andLh(s,·,·)
2:Initialize : cumulative loss O+←0AandO−←0A
3:forn= 1,···, Ndo
4: Compute µh
n(·|s)←σs(−ηAO+)∈∆(A)
5: Compute νh
n(·|s)←σs(−ηBO−)∈∆(B)
6: Observe vectors o+,n∈RAwitho+,n(a) =νh
n(·|s)·Lh(s, a,·)
7: Observe vectors o−,n∈RBwitho−,n(b) =µh
n(·|s)·Lh(s,·, b)
8: Update O+=O++o+,nandO−=O−+o−,n
9:end for
10:Output: policyP
n∈[N]µh
n(·|s)νh
n(·|s)/N
Furthermore, for a certain bounded εCCE, Xie et al. [65] demonstrated that the performance degra-
dation can still be controlled. Following the results therein, the following theorem can be easily
established.
Theorem D.1 (Modified from Theorem 2 from Bai and Jin [8]).With probability at least 1−δ, in
any environment M, the output policies {(µg, νg) :g∈[G]}from the MWU-version of VI-ULCB
satisfy that
X
g∈[G]V†,νg
M(s1)−Vµg,†
M(s1) =Op
H3S2ABT log(SABT/δ ) +TεCCE
.
With δ= 1/T, to have a non-dominant loss caused by the approximate CCE solver, we can choose
NMWU=G. Then, for any environment M, it holds that
ED∼PVI-ULCB
MX
g∈[G]V†,νg
M(s1)−Vµg,†
M(s1)
=Op
H3S2ABT log(SABT )
.
19D.2 Proof of Theorem 4.1: The Realization Construction
D.2.1 Embeddings and Extraction Mapping
We consider each episode of observations to be embedded in 2Htokens. In particular, for each
t∈[T], we construct that
h2t−1=h(sg,h) =
0A
0B
0
sg,h
0AB
0
pos2t−1
=:
hpre,a
2t−1
hpre,b
2t−1
hpre,c
2t−1
hpre,d
2t−1
,
h2t=h(ag,h, bg,h, rg,h) =
ag,h
bg,h
rg,h
0S
0AB
0
pos2t
=:
hpre,a
2t
hpre,b
2t
hpre,c
2t
hpre,d
2t
,
where sg,h, ag,h, bg,hare represented via one-hot embedding. The positional embedding posiis
defined as
posi:=
g
h
t
eh
vi
i
i2
1
,
where ehis a one-hot vector with the h-th element being 1andvi:=1{ha
i=0}denote the tokens
that do not embed actions and rewards.
In summary, for observations Dt−1∪ {st}, we obtain the following tokens of length 2t−1:
H:=h(Dt−1, st) = [h1,h2,···,h2t−1] = [h(s1),h(a1, b1, r1),···,h(st)].
With the above input H, the transformer outputs H=TFθ+(H)of the same size as H. The
extraction mapping Eis directly set to satisfy the following
E·h−1=E·h2t−1=hc
2t−1∈RAB,
i.e., the part cof the output tokens is used to store the learned policy.
D.2.2 An Overview of the Proof
In the following, for the convenience of notations, we will consider step t+ 1, i.e., with observations
Dt∪ {st+1}. Given an input token matrix
H=h(Dt, st+1) = [h1,h2,···,h2t+1],
we construct a transformer to perform the following steps

hpre,a
2t+1
hpre,b
2t+1
hpre,c
2t+1
hpre,d
2t+1
step 1− − − →
hpre,{a,b,c}
2t+1
Nh(s, a, b )
Nh(s, a, b, s′)
Nh(s, a, b )rh(s, a, b )
⋆
0
pos2t+1
step 2− − − →
hpre,{a,b,c}
2t+1ˆPh(s′|s, a, b )
ˆrh(s, a, b )
⋆
0
pos2t+1
step 3− − − →
hpre,{a,b,c}
2t+1
Qh(s, a, b )
Qh(s, a, b )
⋆
0
pos2t+1

20step 4− − − →
hpre,{a,b,c}
2t+1
πh(a, b|s)
⋆
0
pos2t+1
step 5− − − →
hpre,{a,b,c}
2t+1
Vh(s)
Vh(s)
⋆
0
pos2t+1
step 6− − − →
hpre,{a,b}
2t+1
πh+1(·,·|sh+1)
hpost,d
2t+1
:=
hpost,a
2t+1
hpost,b
2t+1
hpost,c
2t+1
hpost,d
2t+1
,
where we use Nh(s, a, b ),Nh(s, a, b, s′),Nh(s, a, b )rh(s, a, b ),ˆPh(s′|s, a, b ),ˆrh(s, a, b ),
Qh(s, a, b ),Qh(s, a, b )andπh(a, b|s)to denote their entire vectors over h∈[H], s∈ S, a∈
A, b∈ B, s′∈ S. The notation ⋆denotes other quantities in hd
2(t+1).
The following provides a sketch of the proof.
Step 1 There exists an attention-only transformer TF θto complete Step 1 with
L=O(1),max
l∈[L]M(l)=O(HS2AB),∥θ∥=O(HG+HS2AB).
Step 2 There exists a transformer TF θto complete Step 2 with
L=O(1),max
l∈[L]M(l)=O(HS2AB), d′=O(G2HS2AB)
∥θ∥=O(HS2AB+G3+GH).
Step 3 There exists a transformer TF θto complete Step 3 with
L=O(H),max
l∈[L]M(l)=O(SAB ), d′(l)=O(SAB ),∥θ∥=O(H+SAB ).
Step 4 There exists a transformer TF θto complete Step 4 with
L=O(GHS ),max
l∈[L]M(l)=O(AB), d′(l)=O(AB),∥θ∥=O(H+AB).
Step 5 There exists an attention-only transformer TF θto complete Step 5 with
L=O(1),max
l∈[L]M(l)=O(HS),∥θ∥=O(HS).
Step 6 There exists an attention-only transformer TF θto complete Step 6 with
L=O(1),max
l∈[L]M(l)=O(HS),∥θ∥=O(HS+GH).
Thus, the overall transformer TF θcan be summarized as
L=O(GHS ),max
l∈[L]M(l)=O(HS2AB), d′=O(G2HS2AB),
∥θ∥=O(HS2AB+G3+GH).
Also, from the later construction, we can observe that log(R) =˜O(1).
D.2.3 Proof of Step 1: Update Nh(s, a, b ),Nh(s, a, b, s′)andNh(s, a, b )rh(s, a, b )
This can be similarly completed by an attention-only transformer constructed in Step 1 of realizing
UCB-VI in Lin et al. [38].
D.2.4 Proof of Step 2: Update ˆP(s′|s, a, b )andˆr(s, a, b )
This can be similarly completed by a transformer constructed in Step 2 of realizing UCB-VI in Lin
et al. [38].
D.2.5 Proof of Step 3: Compute Qh(s′|s, a, b )andQh(s′|s, a, b )
The computation of Qh(s′|s, a, b )can be similarly completed by a transformer constructed in Step
3 of realizing UCB-VI in Lin et al. [38]. The Qpart can also be obtained by modifying a few plus
signs to minuses.
21D.2.6 Proof of Step 4: Compute CCE
This is the most challenging part of realizing the VI-ULCB design, which distinguishes it from the
single-agent algorithms, e.g., UCB-VI [ 6]. As mentioned in Appendix D.1, we obtain an approximate
CCE via virtually playing MWU. In the following, for one tuple (s, h), we prove that one transformer
can be constructed to perform a one-step MWU update with that
L=O(1),max
l∈[L]M(l)=O(AB), d′=O(AB),∥θ∥=O(H+AB).
To obtain this result, we construct a transformer to perform the following computation from inputs to
output for all t′≤t:
h2t= [0],h2t+1=
Lh(s,·,·)
Lh(s,·,·)P
τ<no+,τ P
τ<no−,τ
µn(·|s)
νn(·|s)P
τ≤nµτ(·)ντ(·)
0

compute− − − − → h2t= [0],h2t+1=
Lh(s,·,·)
Lh(s,·,·)P
τ<n+1o+,τ P
τ<n+1o−,τ
µn+1(·|s)
νn+1(·|s)P
τ≤n+1µτ(·)ντ(·)
0
.
Note that here we again use the notations Lh(s,·,·) =H−Qh(s,·,·)
HandLh(s,·,·) =H−Qh(s,·,·)
Hto
denote the normalized losses. It can be seen that this computation can be performed via one ReLU
MLP layer.
Step 4.1: Get o+,nando−,n.
First, we can construct that for all t′≤t
Q(1)
a,1h2t′=
v2t′−1
0
t′
H
,Q(1)
a,1h2t′+1=
v2t′+1−1
νn(·|s)
t′+ 1
H
;
K(1)
a,1h2t′=
H
0
−H
t′
,K(1)
a,1h2t′+1=
H
Lh(a,·|s)
−H
t′+ 1
,
V(1)
a,1h2t′= 2t′,V(1)
a,1h2t′+1= 2t′+ 1.
With ReLU activation, this constructed transformer leads to updates that hd
2t′= 0 andhd
2t′+1=
o+,n(a). With another A−1paralleling heads, the whole vector o+,ncan be computed. Similarly,
withBmore paralleling heads, the whole vector o−,ncan be computed.
Then, with one ReLU MLP layer, we can obtain that
hd
2t′=0,hd
2t′+1=P
τ<no+,τ P
τ<no−,τ
+W(1)
2σr
W(1)
1h2t′+1
=P
τ<n+1o+,τ P
τ<n+1o−,τ
.
The required transformer can be summarized as
L= 1, M(1)=O(A+B), d′=O(A+B),∥θ∥=O(H).
22Step 4.2: Get µn+1(·|s)andνn+1(·|s).
We can construct a softmax MLP layer such that
W(1)
1h2t′=0A, σ s(W(1)
1h2t′) =1
A·1A
W(1)
1h2t′+1=
−ηAP
τ<n+1ot′
+,n
, σ s(W(1)
1h2t′+1) = [ µn+1(·|s)],
where ηA=p
log(A)/G. Thus, µn+1(·|s)can be provided. Similarly, another MLP layer
{W(2)
1, W(2)
2}with softmax activation can provide νn+1(·|s). The current output can be expressed as
hd
2t′=
0
1
A·1A
1
B·1B
,hd
2t′+1=
µn(·|s)
νn(·|s)
µn+1(·|s)
νn+1(·|s)
.
We can further construct one more attention layer as
Q(4)
1h2t′="1−v2t′
t′
1#
,Q(3)
1h2t′+1="1−v2t′+1
t′+ 1
1#
;
K(3)
1h2t′="1
−1
t′#
,K(3)
1h2t′+1="1
−1
t′+ 1#
;
V(3)
1h2t′= 2t′·
−1
A·1A
−1
B·1B
,V(3)
1h2t′+1= (2t′+ 1)·
−1
A·1A
−1
B·1B
.
With ReLU activation, this construction would result in that
h2t′=1
A·1A
1
B·1B
+
−1
A·1A
−1
B·1B
=0,h2t′+1=
µn+1(·|s)
νn+1(·|s)
+0=
µn+1(·|s)
νn+1(·|s)
.
At last, one ReLU MLP layer {W(3)
1,W(3)
2}can be constructed to replace µn(·|s)andνn(·|s)with
µn+1(·|s)andνn+1(·|s):
W(3)
2σr
W(3)
1h2t′
=0,W(3)
2σr
W(3)
1h2t′+1
=
µn+1(·|s)−µn(·|s)
νn+1(·|s)−νn(·|s)
.
The required transformer can be summarized as
L= 3,max
l∈[L]M(l)=O(1), d′=O(A+B),∥θ∥=O(p
log(A)/G+p
log(B)/G+ 1).
Step 4.3: GetP
τ≤n+1µτ(·|s)ντ(·|s)/N.
We can construct
Q(1)
1h2t′="0
t′
1#
,Q(1)
1h2t′+1="µn+1(a|s)
t′+ 1
1#
;
K(1)
1h2t′="0
−1
t′#
,K(1)
1h2t′+1="νn+1(b|s)
−1
t′+ 1#
;
V(5)
1h2t′= 2t′,V(5)
1h2t′+1= 2t′+ 1.
With ReLU activation, this construction can update that
h2t′= 0,h2t′+1=µn+1(a|s)νn+1(b|s).
Using an overall ABparalleling heads, we can then obtain µn+1(·|s)νn+1(·|s). Then, with a ReLU
MLP layer {W(5)
1, W(5)
2}, we can obtainP
τ≤n+1µτ(·|s)ντ(·|s)/N.
The required transformer can be summarized as
L= 1, M(1)=O(AB), d′=O(AB),∥θ∥=O(AB).
Combining all the sub-steps provides proof of a one-step MWU update. The same transformer can be
stacked for Gtimes, which completes the G-step MWU.
23D.2.7 Proof of Step 5: Compute Vh(s)andVh(s)
We can construct that
Q(1)
1h2t′="0
t′
H#
,Q(1)
1h2t′+1=
πh(·,·|s)
t′+ 1
H
;
K(1)
1h2t′="0
−H
t′#
,K(1)
1h2t′+1=
Qh(s,·,·)
−H
t′+ 1
;
V(1)
1h2t′= 2t′,V(1)
1h2t′+1= 2t′+ 1.
With ReLU activation, this construction leads to that
h2t′= 0,h2t′+1=πh(·,·|s)·Qh(s,·,·) =Vh(s).
Thus, with overall 2HSparalleling heads, the values of {Vh(s), Vh(s) :h∈[H], s∈ S} can be
computed.
The required transformer can be summarized as
L= 1, M(1)=O(HS),∥θ∥=O(HS).
D.2.8 Proof of Step 6: Obtain πh+1(·,·|sh+1)
We can construct one HS-head transformer that for all (s, h)∈ S × [H]
Q(1)
h,sh2t′=
0
eh′
t′
1
1
,Q(1)
h,sh2t′+1=
st′+1
eh′+1
t′+ 1
1
1
;
K(1)
h,sh2t′=
es
eh
−1
t′
−1
,K(1)
h,sh2t′+1=
es
eh
−1
t′+ 1
−1
;
V(1)
h,sh2t′=0,V(1)
h,sh2t′+1=πh(·,·|s).
With ReLU activation, this construction leads to the update that
h2t′=0,h2t′+1=1
2t′+ 1·πh′+1(·,·|st′+1).
Then, we can construct that
Q(1)
1h2t′="2t′
2GHt′
1#
,Q(1)
1h2t′+1="2t′+ 1
2GH(t′+ 1)
1#
;
K(1)
1h2t′="1
−1
2GHt′#
,K(1)
1h2t′+1="1
−1
2GH(t′+ 1)#
;
V(1)
1h2t′=0,V(1)
h,sh2t′+1=1
2t′+ 1·πh′+1(·,·|st′+1).
With ReLU activation, this construction leads to the update that
h2t′=0,h2t′+1=πh′+1(·,·|st′+1).
The required transformer can be summarized as
L= 2,max
l∈[L]M(l)=O(HS),∥θ∥=O(HS+GH).
24E Proofs for the Centralized Overall Performance
Proof of Theorem 4.2. First, we can obtain the decomposition that
EM∼Λ,D∼PAlgbθ
M
X
g∈[G]V†,νg
M(s1)−Vµg,†
M(s1)

=EM∼Λ,D∼PAlg0
M
X
g∈[G]V†,νg
M(s1)−Vµg,†
M(s1)

+EM∼Λ,D∼PAlgbθ
M
X
g∈[G]V†,νg
M(s1)
−EM∼Λ,D∼PAlg0
M
X
g∈[G]V†,νg
M(s1)

+EM∼Λ,D∼PAlg0
M
X
g∈[G]Vµg,†
M(s1)
−EM∼Λ,D∼PAlgbθ
M
X
g∈[G]Vµg,†
M(s1)
.
Via Theorem D.1, it holds that
EM∼Λ,D∼PAlg0
M
X
g∈[G]V†,νg
M(s1)−Vµg,†
M(s1)
=Op
H3S2ABT log(SABT )
.
Then, via Lemma E.1 and Theorem C.3, we can obtain that
EM∼Λ,D∼PAlg0
M
X
g∈[G]Vµg,†
M(s1)
−EM∼Λ,D∼PAlgbθ
M
X
g∈[G]Vµg,†
M(s1)

=O
T·EM∼Λ,D∼PAlg0
M
X
t∈[T]X
s∈S
TV 
Alg0,Algbθ|Dt−1, s


=O 
T2S√εreal+T2Sr
log (NΘTS/δ )
N!
,
and similarly,
EM∼Λ,D∼PAlgbθ
M
X
g∈[G]V†,νg
M(s1)
−EM∼Λ,D∼PAlg0
M
X
g∈[G]V†,νg
M(s1)

=O 
T2S√εreal+T2Sr
log (NΘTS/δ )
N!
.
With Theorem 4.1 providing that εreal= 0, combining the above terms completes the proof of the
regret bound. The bound on the covering number, i.e., log(NΘ)can be obtained via Lemma I.4.
Lemma E.1. For any two centralized algorithms AlgαandAlgβ, we denote their performed policies
for episode gare(πg
α, πg
β), whose marginal policies are (µg
α, νg
α)and(µg
β, νg
β). For{µg
α, νg
β}, it
holds that
EαX
g∈[G]Vµg
α,†
M(s1)
−EβX
g∈[G]Vµg
β,†
M(s1)
≲T·EαX
t∈[T],s∈STV 
πt
α, πt
β|Dt−1, s
,
where Eα[·]andEβ[·]are with respect to PAlgα
Λ andPAlgβ
Λ. A similar result holds for {νg
α, νg
β}.
Proof of Lemma E.1. It holds that
ED∼PAlgα
M
X
g∈[G]Vµg
α,†
M(s1)
−ED∼PAlgβ
M
X
g∈[G]Vµg
β,†
M(s1)

25=ED∼PAlgα
M
X
g∈[G]Vµg
α,†
M(s1)−X
g∈[G]Vµg
β,†
M(s1)

| {z }
:=(term I)
+ED∼PAlgα
M
X
g∈[G]Vµg
β,†
M(s1)
−ED∼PAlgβ
M
X
g∈[G]Vµg
β,†
M(s1)

| {z }
:=(term II)
Denoting DH:=D(g−1)H+1:gHandf(DH) =P
h∈[H]rg,h, we can further obtain that
Vµg
α,†
M(s1)−Vµg
β,†
M(s1)
(a)
≤Vµg
α,ν†(µg
β)
M (s1)−Vµg
β,ν†(µg
β)
M (s1)
=E
DH∼Pµg
α,ν†(µg
β)
M
f(DH)
−E
DH∼Pµg
β,ν†(µg
β)
M
f(DH)
=X
h∈[H]E
D1:h∼Pµg
α,ν†(µg
β)
M,Dh+1:H∼Pµg
β,ν†(µg
β)
M
f(DH)
−X
h∈[H]E
D1:h−1∼Pµg
α,ν†(µg
β)
M,Dh:H∼Pµg
β,ν†(µg
β)
M
f(DH)
(b)
≤2HX
h∈[H]E
D1:h−1∼Pµg
α,ν†(µg
β)
M,sg,hh
TV
µg,h
α×νh
†(µg
β)(·,·|sg,h), µg,h
β×νh
†(µg
β)(·,·|sg,h)i
(c)= 2HX
h∈[H]E
D1:h−1∼Pµg
α,ν†(µg
β)
M,sg,hh
TV
µg,h
α(·|sg,h), µg,h
β(·|sg,h)i
(d)
≤2HX
h∈[H]X
s∈STV
πg,h
α(·,·|s), πg,h
β(·,·|s)
,
where (a) is from the definition of best responses, (b) is from the variational representation of the
TV distance, (c) is from the fact that TV(µ×ν, µ′×ν) =TV(µ, µ′), and (d) is from the fact that
TV(P
bπ(·, b),P
bπ′(·, b))≤TV(π(·,·), π′(·,·)). The above relationship further leads to that
(term I) :=ED∼PAlgα
M
X
g∈[G]Vµg
α,†
M(s1)−X
g∈[G]Vµg
β,†
M(s1)

≤2H·ED∼PAlgα
M
X
t∈[T]X
s∈STV 
πt
α, πt
β|Dt−1, s
.
Also, denoting g(D) =P
g∈[G]Vµg
β,†(s1), it holds that
(term II) :=ED∼PAlgα
M
X
g∈[G]Vµg
β,†(s1)
−ED∼PAlgβ
M
X
g∈[G]Vµg
β,†(s1)

=X
t∈[T]EDt∼PAlgα
M,Dt+1:T∼PAlgβ
M[g(D)]−X
t∈[T]EDt−1∼PAlgα
M,Dt:T∼PAlgβ
M[g(D)]
≤2TX
t∈[T]EDt−1∼PAlgα
M,st
TV 
Algα,Algβ|Dt−1, st
≤2T·ED∼PAlgα
M
X
t∈[T]X
s∈STV 
πt
α, πt
β|Dt−1, s
.
Combining (term I) and (term II) finishes the proof.
26F Proofs for the Decentralized Supervised Pre-training Guarantees
Definition F.1 (The Complete Version of Definition 3.1) .For a class of algorithms {Algθ+:θ+∈
Θ+}, we say ˜Θ+⊆Θ+is aρ+-cover of Θ+, if˜Θ+is a finite set such that for any θ+∈Θ+, there
exists ˜θ+∈˜Θ+such that for all Dt−1
+, s∈ S, t∈[T], it holds that
logAlg˜θ+(·,·|Dt−1
+, s)−logAlgθ+(·|Dt−1
+, s)
∞≤ρ+.
The covering number NΘ+(ρ+)is the minimal cardinality of ˜Θ+such that ˜Θ+is aρ+-cover of Θ+.
Similarly, for a class of algorithms {Algθ−:θ−∈Θ−}, we say ˜Θ−⊆Θ−is aρ−-cover of
Θ−, if˜Θ−is a finite set such that for any θ−∈Θ−, there exists ˜θ−∈˜Θ−such that for all
Dt−1
−, s∈ S, t∈[T], it holds that
logAlg˜θ−(·,·|Dt−1
−, s)−logAlgθ−(·|Dt−1
−, s)
∞≤ρ−.
The covering number NΘ−(ρ−)is the minimal cardinality of ˜Θ−such that ˜Θ−is aρ−-cover of Θ−.
Assumption F.2 (The Complete Version of Assumption 3.2) .There exists θ∗
+∈Θ+such that there
exists ε+,real>0, for all t∈[T], s∈ S,a∈ A, it holds that
log 
EM∼Λ,D∼PAlg0
M"
Alg+,0(a|Dt−1
+, s)
Algθ∗
+(a|Dt−1
+, s)#!
≤ε+,real.
Similarly, there exists θ∗
−∈Θ−such that there exists ε−,real>0, for all t∈[T], s∈ S,b∈ B, it
holds that
log 
EM∼Λ,D∼PAlg0
M"
Alg−,0(b|Dt−1
−, s)
Algθ∗
−(b|Dt−1
−, s)#!
≤ε−,real.
Theorem F.3 (The Complete Version of Theorem 3.3) .Letbθ+be the max-player’s pre-training output
defined in Section 3.1.1. Take NΘ+=NΘ+(1/N)as in Definition F .1. Then, under Assumption F .2,
with probability at least 1−δ, it holds that
EM∼Λ,D∼PAlg0
M
X
t∈[T],s∈STV
Alg+,0,Algbθ+|Dt−1
+, s

=O
TS√ε+,real+TSs
log 
NΘ+TS/δ
N
.
Letbθ−be the min-player’s pre-training output defined in Section 3.1.1. Take NΘ−=NΘ−(1/N)as
in Definition F .1. Then, under Assumption F .2, with probability at least 1−δ, it holds that
EM∼Λ,D∼PAlg0
M
X
t∈[T],s∈STV
Alg−,0,Algbθ−|Dt−1
+, s

=O
TS√ε−,real+TSs
log 
NΘ−TS/δ
N
.
Proof of Theorem F .3. This theorem can be similarly proved as Theorem 3.3.
G Proofs for Realizing V-learning
In the following proof, we focus on the max-player’s perspective, which can be easily extended for
the min-player.
27G.1 Details of V-learning
The details of V-learning [ 32], discussed in Sec. 3.2, are presented in Alg. 3, where the following
notations are adopted
αn=H+ 1
H+n, βn=c·r
H3Alog(HSAG/δ )
n,
γn=ηn=r
Hlog(A)
An, ωn=αn nY
τ=2(1−ατ)!−1
.
After the learning process, V-learning requires an additional procedure to provide the output policy.
We include this procedure in Alg. 4, where the following notations are adopted
Ng,h(s) =the number of times sis visited at step hbefore episode g,
gh
i(s) =the index of the episode sis visited at step hfor the i-th time ,
αn,i=αinY
j=i+1(1−αj).
Following the results in Jin et al. [32], we can obtain the following performance guarantee of
V-learning.
Theorem G.1 (Theorem 4 in Jin et al. [32]).With probability at least 1−δ, in any environment M,
the output policies (ˆµ,ˆν)from V-learning satisfy that
V†,ˆν
M(s1)−Vˆµ,†
M(s1) =O r
H5S(A+B) log( SABT/δ )
G!
.
Thus, with δ= 1/T, we can obtain that
ED∼PV-learning
Mh
V†,ˆν
M(s1)−Vˆµ,†
M(s1)i
=O r
H5S(A+B) log( SABT )
G!
.
Algorithm 3 V-learning [32]
1:Initialize: for any (s, a, h )∈ S ×A× [H],Vh(s)←H+ 1−h,Nh(s)←0,µh(a|s)←1/A
2:forepisode g= 1,···, Gdo
3: receive sg,1
4: forsteph= 1,···, Hdo
5: Take action ag,h∼πh(·|sg,h), observe reward rg,hand next state sg,h+1
6: Update n=Nh(sg,h)←Nh(sg,h) + 1
7: Update ˜Vh(sg,h)←(1−αn)˜Vh(sg,h) +αn 
rg,h+Vh+1(sg,h+1) +βn
8: Update Vh(sg,h)←minn
H+ 1−h,˜Vh(sg,h)o
9: Compute ˜ℓh
n(sg,h, a)←H−rg,h−Vh+1(sg,h+1)
H·1{a=ag,h}
µh(ag,h|sg,h)+γnfor all a∈ A
10: Update µh(a|sg,h)∝exp
−ηn
ωn·P
τ∈[n]ωτ·˜ℓh
τ(sg,h, a)
for all a∈ A
11: end for
12:end for
G.2 An Additional Assumption About Transformers Performing Division
Before digging into the proof of Theorem 3.4, we first state the following assumption that there exists
one transformer that can perform the division operation.
Assumption G.2. There exists one transformer TFθwith
L=LD,max
l∈[L]M(l)=MD, d′=dD,∥θ∥=FD,
28Algorithm 4 V-learning Executing Output Policy ˆµ[32]
1:Sample g∼Unif(1,2,···, G)
2:forsteph= 1,···, Hdo
3: Observe shand set n←Ng,h(sh)
4: Setg←gh
i(sh)where i∈[n]is sampled with probability αn,i
5: Take action ah∼µg,h(·|sh)
6:end for
such that for any x∈[0,1], y∈[p
log(A)/(AG),p
log(A)/A+ 1], φ > 0, with input H=
[h1,···,he,···], where
hi="0
φ
0#
,∀i̸=e;he="x
y
0#
,
the transformer can provide output H= [h1,···,he,···]
hi=
0
φ
0
0
,∀i̸=e;he=
x
y
x/y
0
.
We note that this assumption on performing exact division is only for the convenience of the proof
as one can approximate the division to any arbitrary precision only via one MLP layer with ReLU
activation.
In particular, let Ballk
∞(R) = [−R, R]∞denote the standard ℓ∞ball in Rkwith radius R >0, we
introduce the following definitions and result.
Definition G.3 (Approximability by Sum of ReLUs, Definition 12 in Bai et al. [7]).A function
g:Rk→Ris(εapprox, R, M, C )-approximable by sum of ReLUs, if there exists a “ (M, C )-sum of
ReLUs” function
fM,C(z) =MX
m=1cmσ(a⊤
m[z; 1])
with
MX
m=1|cm| ≤C, max
m∈[M]∥am∥1≤1,am∈Rk+1, cm∈R
such that
sup
z∈Ballk
∞(R)|g(z)−fM,C(z)| ≤εapprox.
Definition G.4 (Sufficiently Smooth k-variable Function, Definition A.1 in Bai et al. [7]).We say a
function g:Rk→Ris(R, C ℓ)-smooth if for s=⌈(k−1)/2⌉+ 2,gis aCsfunction on Ballk
∞(R),
and
sup
z∈Ballk
∞(R)∥∇ig(z)∥∞= sup
z∈Ballk
∞(R)max
j1,···,ji∈[k]|∂zj1···zjig(z)| ≤Li
for all i∈ {0,1,···, s}, with
max
0≤i≤sLiRi≤Cℓ.
Proposition G.5 (Approximating Smooth k-variable Functions, Proposition A.1 in Bai et al. [7]).For
anyεapprox >0, R≥1, Cℓ>0, we have the following: Any (R, C ℓ)-smooth function g:Rk→R
is(εapprox, R, M, C )approximable by sum of ReLUs with M≤C(k)C2
ℓlog(1 + Cℓ/εapprox)/ε2
approx
andC≤C(k)Cℓ, where C(k)>0is a constant that depends only on k.
29Then, we can consider g(x, y) = (c1+x)/(c2+y)withc1≥1,c2≥1 +c′
2>1,x∈[−1,1]and
y∈[−1,1]. It can be verified that
|g(x, y)|=c1+x
c2+y≤1 +c1
c′
2;|∂xg(x, y)|=1
c+y≤1
c′
2;|∂yg(x, y)|=c1+x
(c2+y)2≤1 +c1
(c′
2)2;
|∂2
xg(x, y)|= 0;|∂2
yg(x, y)|=2(c1+x)
(c2+y)3≤2(1 + c1)
(c′
2)3;|∂x∂yg(x, y)|=1
(c+y)2≤1
(c′
2)2;
|∂3
xg(x, y)|= 0;|∂3
yg(x, y)|=6(c1+x)
(c2+y)4≤6(1 + c1)
(c′
2)4;
|∂2
x∂yg(x, y)|= 0;|∂x∂2
yg(x, y)|=2
(c+y)3≤2
(c′
2)3.
Then, from Definition G.4 there exists one Clsuch that g(x, y)is(1, Cl)-smooth. Thus, by Proposi-
tion G.5, this function can be approximated by a sum of ReLUs (defined in Definition G.3). With this
observation, the division operation required in Assumption G.2 can be approximated.
G.3 Proof of Theorem 3.4: The Realization Construction
G.3.1 Embeddings and Extraction Mappings
We consider each episode of the max-player’s observations to be embedded in 2Htokens. In
particular, for each t∈[T], we construct that
h2t−1=h+(sg,h) =
0A
0
sg,h
0A
0
pos2t−1
=:
hpre,a
2t−1
hpre,b
2t−1
hpre,c
2t−1
hpre,d
2t−1
,
h2t=h+(ag,h, rg,h) =
ag,h
rg,h
0S
0A
0
pos2t
=:
hpre,a
2t
hpre,b
2t
hpre,c
2t
hpre,d
2t
,
where sg,h, ag,hare represented via one-hot embedding. The positional embedding posiis defined as
posi:=
g
h
t
eh
vi
i
i2
1
,
where vi:=1{ha
i=0}denote the tokens that do not embed actions and rewards.
In summary, for observations Dt−1
+∪ {st}, we obtain the tokens of length 2t−1which can be
expressed as the following form:
H:=h+(Dt−1
+, st) = [h1,h2,···,h2t−1] = [h+(s1),h+(a1, r1),···,h+(st)].
With the above input H, the transformer outputs H=TFθ+(H)of the same size as H. The
extraction mapping Ais directly set to satisfy the following
A·h−1=A·h2t−1=hc
2t−1∈RA,
i.e., the part cof the output tokens is used to store the learned policy.
30G.3.2 An Overview of the Proof
In the following, for the convenience of notations, we will consider step t+ 1, i.e., with observations
Dt
+∪ {st+1}. Given an input token matrix
H=h+(Dt
+, st+1) = [h1,h2,···,h2t+1],
we construct a transformer to perform the following steps

hpre,a
2t+1
hpre,b
2t+1
hpre,c
2t+1
hpre,d
2t+1
step 1− − − →
hpre,{a,b,c}
2t+1
Nt(st)
⋆
0
pos2t+1
step 2− − − →
hpre,{a,b,c}
2t+1˜Vt(st)
Vt(st)
⋆
0
pos2t+1
step 3− − − →
hpre,{a,b,c}
2t+1˜ℓn(st,·)
µt(·|st)
⋆
0
pos2t+1

step 4− − − →
hpre,{a,b}
2t+1
µt+1(·|st+1)
hd
2t+1
:=
hpost,a
2t+1
hpost,b
2t+1
hpost,c
2t+1
hpost,d
2t+1
,
where n:=Nh(sg,h).
To ease the notations, in the proof, we will slightly abuse TFθasTFθ+. The following provides a
sketch of the proof.
Step 1 There exists an attention-only transformer TF θto complete Step 1 with
L=O(1),max
l∈[L]M(l)=O(1),∥θ∥=O(HG)
Step 2 There exists a transformer TF θto complete Step 2 with
L=O(HG),max
l∈[L]M(l)=O(HS2), d′=O(G),∥θ∥=O(G3+GH),
Step 3 There exists a transformer TF θto complete Step 3 with
L=O(GHL D),max
l∈[L]M(l)=O(HSA +MD), d′=O(dD+A+G),
∥θ∥=O(GH2S+FD+G3),
Step 4 There exists a transformer TF θto complete Step 4 with
L=O(1),max
l∈[L]M(l)=O(HS),∥θ∥=O(HS+GH).
Thus, the overall transformer TF θcan be summarized as
L=O(GHL D),max
l∈[L]M(l)=O(HS2+HSA +MD), d′=O(G+A+dD),
∥θ∥=O(GH2S+G3+FD).
Also, from the later construction, we can observe that log(R) =˜O(1). The bound on the covering
number, i.e., log(NΘ+)can be obtained via Lemma I.4.
G.3.3 Proof of Step 1: Update Nh(sg,h).
From the proof of Step 1 in realizing UCB-VI in Lin et al. [38], we know that there exists a
transformer with 3heads that for all t′≤t, can move st′,(at′, rt′)fromha
2t′−1andhb
2t′tohd
2t′+1
while maintaining hd
2t′not updated. This constructed transformer is shown in Lin et al. [38] to be an
attention-only one with ReLU activation and
L= 2,max
l∈[L]M(l)≤3,∥θ∥ ≤O(HG).
31Then, still following Lin et al. [38], another attention-only transformer can be constructed so that
Nt(st)can be computed in hd
2t+1. The constructed transformer has ReLU activation and
L= 2,max
l∈[L]M(l)=O(1),∥θ∥=O(H).
In summary, at the end of Step 1, we can obtain that for all t′≤t,
hd
2t′=
0
pos2t′
,hd
2t′+1=
st′
at′
rt′
Nt′(st′)
0
pos2t′+1
,
with an attention-only transformer that uses ReLU activation and
L= 4,max
l∈[L]M(l)=O(1),∥θ∥=O(HG).
G.3.4 Proof of Step 2: Compute ˜Vh(sg,h)andVh(sg,h)
In Step 2, let nt=Nt(st), the following computations will be performed:
˜Vt
new(st)←(1−αnt)˜Vt
old(st) +αnt 
rt+Vt+1
old(st+1) +βnt
,
Vt
new(st)←minn
H+ 1−h,˜Vt
new(st)o
.
First, similar to Step 2 in realizing UCB-VI in Lin et al. [38], we can obtain αnt′andβnt′in each
hd
2t′+1via a transformer with ReLU activation and
L=O(1),max
l∈[L]M(l)=O(1), d′=O(G),∥θ∥=O(G3).
Then, we can assume that the values of {˜Vh
old(s), Vh
old(s) :h∈[H], s∈ S} is already computed in
tokenhd
2τ−1, and prove via induction to show that the set of new values can be computed in token
hd
2τ+1, i.e.,
hd
2τ−1=
⋆
˜V1:H
old(·)
V1:H
old(·)
0
pos2τ−1
,hd
2τ=
0
pos2τ
,hd
2τ+1=
0
pos2τ+1
compute− − − − → hd
2τ=
0
pos2τ
,hd
2τ+1=
⋆
˜V1:H
new(·)
V1:H
new(·)
0
pos2τ+1
.
In the following, we will show that this one-step update can be completed via a transformer that
requires
L=O(1),max
l∈[L]M(l)=O(HS2), d′=O(G),∥θ∥=O(G3+GH),
and the overall updates can be completed via stacking Tsimilar transformers.
Step 2.1: Obtain τ− |t′−1−τ|.
First, we obtain an auxiliary value τ− |t′−1−τ|in each hd
2t′−1andhd
2t′for all
t′≤t. In particular, we can construct three MLP layers with ReLU activation, i.e.,
{W(1)
1,W(1)
2,W(2)
1,W(2)
2,W(3)
1,W(3)
2}to sequentially compute that
[0]→σr([t′−1−τ]) ;
32σr([t′−1−τ])→σr([t′−1−τ]) +σr([τ−t′+ 1]) = [ |t′−1−τ|]
[|t′−1−τ|]→[τ− |t′−1−τ|].
It can be observed that τ− |t′−1−τ|reaches its maximum τatt′=τ+ 1.
The required transformer can be summarized as
d′= 1,∥θ∥=O(GH).
Step 2.2: Move ˜V1:H
old(·)andV1:H
old(·).
Then, we move ˜V1:H
old(·)andV1:H
old(·)fromh2τ−1toh2τ+1. In particular, we can construct that
Q1h2t′=
v2t′−1
τ− |t′−1−τ|
−1
t′
1
,Q1h2t′+1=
v2t′+1−1
τ− |t′−τ|
−1
t′+ 1
1
;
K1h2t′=
1
1
τ
−1
t′+ 1
,K1h2t′+1=
1
1
τ
−1
t′+ 2
;
V1h2τ−1= (2τ+ 1)˜V1:h
old(·)
V1:H
old(·)
,V1h2τ=0,V1h2τ+1=0.
With ReLU activation, this construction leads to that
h2t′=h2t′+1
2t′X
i≤2t′σr(−1− |t′−1−τ| −t′+t(i) + 1) ·V1hi=0,∀t′≤t
h2t′+1=h2t′+1+1
2t′+ 1X
i≤2t′+1σr(−|t′−τ| −t′+t(i) + 1) ·V1hi
=

1
2t′+1(V1h2t′−1+V1h2t′+ 2V1h2t′+1) =˜V1:h
old(·)
V1:H
old(·)
ift′=τ
0 otherwise.
The transformer required in Step 2.3 can be summarized as
L= 1, M(1)=O(1),∥θ∥=O(GH).
Step 2.3: Compute ˜V1:H
new(·)andV1:H
new(·).
Finally, we get to compute ˜V1:H
new(·)andV1:H
new(·), where
˜Vτ
new(sτ) =˜Vτ
old(sτ)−αnτ˜Vτ
old(sτ) +αnτ 
rτ+Vτ+1
old(sτ+1) +βnτ
.
We can have a HS-head transformer that
Qh,sh2t′=
v2t′−1
τ− |t′−1−τ|
−1
t′
1
0
eh′
1
0
,Qh,sh2t′+1=
v2t′+1−1
τ− |t′−τ|
−1
t′+ 1
1
st′
eh′+1
1
αnt′
,
33Kh,sh2t′=
1
1
τ
−1
t′
es
eh+1
−2
1
,Kh,sh2t′+1=
1
1
τ
−1
t′+ 1
es
eh+1
−2
1
,
Vh,sh2τ+1=−(2τ+ 1)·Vh
old(s)·[eh,s].
It holds that for all t′≤t,
h2t′=h2t′+1
2t′X
h,sX
i≤2t′σr(−1− |t′−1−τ| −t′+t(i) +eh′·eh+1−2)Vh,shi=0
h2t′+1=h2t′+1+1
2t′+ 1X
h,sX
i≤2t′+1σr
−|t′−τ| −t′−1 +t(i) +st′·es+eh′+1·eh+αnt′−2
Vh,shi
=(
V1:H
old(·)−αnt′Vh′
old(st′)·eh′,st′ift′=τ
0 otherwise,
which completes the computation (1−αnτ)˜Vτ
old(sτ).
Two similar HS-head transformers can further perform (1−αnτ)˜Vτ
old(sτ) +αnτrτ+αnτβnτ. and
a similar HS2-head transformer can finalize ˜Vτ
new(sτ)as(1−αnτ)˜Vτ
old(sτ) +αnτrτ+αnτβnτ+
αnτVτ+1
old(sτ+1).
Finally, from the proof of Step 3 in realizing UCB-VI in Lin et al. [38], one MLP layer can perform
Vτ
old(sτ) = minn
˜Vτ
new(sτ), H−h+ 1o
.
The required transformer in step 2.4 can be summarized as
L= 4,max
l∈[L]M(l)=O(HS2), d′=O(1),∥θ∥=O(GH).
Thus, we can see that the update of h2τ+1can be done. Repeating the similar step for each τ∈[T]
would complete the overall updates.
G.3.5 Proof of Step 3: Compute ˜ℓnt(sg,h, a)andµh(a|sg,h)
In step 3, let nt=Nt(st), for step t, we update
˜ℓt
nt(st, a)←H−rt−Vt+1(st+1)
H·1{a=at}
µt
old(at|st) +γnt;
µt
new(a|st)∝exp
−ηnt
ωnt·X
i∈[nt]ωi·˜ℓt
i(st, a)
.
First, similar to Step 2, we can obtain ωnt′,ηnt′/ωnt′andγnt′in each hd
2t′+1via a transformer with
ReLU activation and
L=O(1),max
l∈[L]M(l)=O(1), d′=O(G),∥θ∥=O(G3).
Denoting vectors L ∈RHSAandµ∈RHSAcontaining the cumulative losses and policies, i.e.,P
i∈[n]ωi˜ℓh
i(s, a)andµh(a|s). Similar to step 2, we will assume that the values of {Lold,µold},
34which are computed via information before time τ, are already contained in token hd
2τ−1, and prove
via induction to show that the set of new values can be computed in token hd
2τ+1, i.e.,
hd
2τ−1=
⋆
Lold
µold
0
pos2τ−1
,hd
2τ=
0
pos2τ
,hd
2τ+1=
0
pos2τ+1
compute− − − − → hd
2τ=
0
pos2τ
,hd
2τ+1=
⋆
Lnew
µnew
0
pos2τ+1
.
In the following, we will show that this one-step update can be completed via a transformer that
requires
L=O(LD),max
l∈[L]M(l)=O(HSA +MD), d′=O(dD+A),∥θ∥=O(GH2S+FD),
and the overall updates can be completed via stacking Tsimilar transformers.
Step 3.1: Obtain τ− |t′−1−τ|.
First, similar to step 2.1, we can have an auxiliary value τ− |t′−1−τ|in each hd
2t′−1andhd
2t′for
allt′< tvia three MLP layers. The required transformer can be summarized as
d′= 1,∥θ∥=O(GH).
Step 3.2: Compute ˜ℓτ
nτ(sτ, a).
First, similar to Step 2.2, we can move Loldandµoldfromhd
2τ−1tohd
2τ+1while keeping other tokens
unchanged. In particular, we can construct that
Q1h2t′=
v2t′−1
τ− |t′−1−τ|
−1
t′
1
,Q1h2t′+1=
v2t′+1−1
τ− |t′−τ|
−1
t′+ 1
1
;
K1h2t′=
1
1
τ
−1
t′+ 1
,K1h2t′+1=
1
1
τ
−1
t′+ 2
;
V1h2τ−1= (2τ+ 1)
Lold
µold
,V1h2τ=0,V1h2τ+1=0.
With ReLU activation, this construction leads to that
h2t′=0,∀t′≤t;h2t′+1=

Lold
µold
ift′=τ
0 otherwise.
Then, using similar constructions as Step 2.3, we can specifically extract Vτ+1(sτ+1)andµτ
old(aτ|sτ)
tohd
2τ+1while keeping other tokens unchanged. Following the extraction, one MLP layer can com-
pute the valueH−rτ−Vτ+1(sτ+1)
Hand add it to hd
2τ+1. The required transformer can be summarized
as
L=O(1),max
l∈[L]M(l)=O(HSA ), d′= 1,∥θ∥=O(GH).
35With Assumption G.2, for token h2τ+1, we can further have one transformer to compute
˜ℓτ
nτ(sτ, aτ) =H−rτ−Vτ+1(sτ+1)
H·1
µτ
old(aτ|sτ) +γnτ.
The overall required transformer can be summarized as
L=O(LD),max
l∈[L]M(l)=O(HSA +MD), d′=O(dD),∥θ∥=O(GH+FD).
Step 3.3: Update µτ(a|sτ).
First, one transformer can be constructed to update LoldtoLnewinhd
2τ+1. In particular, we can have
Qh,sh2t′=
v2t′−1
τ− |t′−1−τ|
1
t′
1
0
eh′
1
,Qh,sh2t′+1=
v2t′+1−1
τ− |t′−τ|
1
t′+ 1
1
st′
eh′+1
1
,
Kh,sh2t′=
1
1
−τ
−1
t′
es
eh+1
−1
,Kh,sh2t′+1=
1
1
−τ
−1
t′+ 1
es
eh+1
−1
,
Vh,sh2τ+1=−(2τ+ 1)· Lold(h, s,·).
With ReLU activation, this construction leads to that
h2t′=0,∀t′≤t;h2t′+1=Lold(h(τ), sτ,·)ift′=τ
0 otherwise.
With a similar A-head transformer, we can add ωnτ·˜ℓτ
nτ(sτ, aτ)toLold(h(τ), sτ, aτ)and thus obtain
the new values Lnew(h(τ), sτ,·). Furthermore, a one-head transformer can obtain
h2τ+1=ηnτ
ωnτ· Lnew(h(τ), sτ,·)
Finally, with another softmax MLP layer, we can obtain µτ
new(·|sτ).
The required transformer can be summarized as
L=O(1),max
l∈[L]M(l)=O(HS+A), d′=O(A),∥θ∥=O(GH2S).
G.3.6 Proof of Step 4: Obtain µh+1(·|sh+1)
This step is essentially the same as Step 6 in realizing VI-ULCB, which can be completed with a
transformer that
L= 1, M(1)=HS,∥θ∥=O(HS).
H Proofs for the Decentralized Overall Performance
Proof of Theorem 3.5. We use the decomposition that
EM∼Λ,D∼PAlgbθ
Mh
V†,ˆν
M(s1)−Vˆµ,†
M(s1)i
=EM∼Λ,D∼PAlg0
Mh
V†,ˆν
M(s1)−Vˆµ,†
M(s1)i
36+EM∼Λ,D∼PAlgbθ
Mh
V†,ˆν
M(s1)i
−EM∼Λ,D∼PAlg0
Mh
V†,ˆν
M(s1)i
+EM∼Λ,D∼PAlg0
Mh
Vˆµ,†
M(s1)i
−EM∼Λ,D∼PAlgbθ
Mh
Vˆµ,†
M(s1)i
.
First, via Theorem G.1, it holds that
EM∼Λ,D∼PAlg0
Mh
V†,ˆν
M(s1)−Vˆµ,†
M(s1)i
=O r
H5S(A+B) log( SABT )
G!
.
Then, via Lemma 3.6 and Theorem 3.3, we can obtain that
EM∼Λ,D∼PAlg0
Mh
Vˆµ,†
M(s1)i
−EM∼Λ,D∼PAlgbθ
Mh
Vˆµ,†
M(s1)i
=O
H·X
t∈[T],s∈SEα
TV 
µt
α, µt
β|Dt−1
+, s
+TV 
νt
α, νt
β|Dt−1
−, s

=O
THS√ε+,real+THS√ε−,real+THSs
log 
NΘ+TS/δ
N+THSs
log 
NΘ−TS/δ
N
,
and similarly,
EM∼Λ,D∼PAlgbθ
Mh
V†,ˆν
M(s1)i
−EM∼Λ,D∼PAlg0
Mh
V†,ˆν
M(s1)i
=O
THS√ε+,real+THS√ε−,real+THSs
log 
NΘ+TS/δ
N+THSs
log 
NΘ−TS/δ
N
.
With Theorem 3.4 providing that ε+,real=ε−,real= 0, combining the above terms completes the
proof of the regret bound. The bound on the covering number, i.e., log(NΘ+)andlog(NΘ−)can be
obtained via Lemma I.4.
In the following, we provide the proof for the Lemma 3.6, which plays a key role in the above proof
of the overall performance.
Proof of Lemma 3.6. It holds that
ED∼PAlgα
Mh
Vˆµα,†
M(s1)i
−ED∼PAlgβ
Mh
Vˆµβ,†
M(s1)i
=ED∼PAlgα
Mh
Vˆµα,†
M(s1)−Vˆµβ,†
M(s1)i
| {z }
:=(term I)+ED∼PAlgα
Mh
Vˆµβ,†
M(s1)i
−ED∼PAlgβ
Mh
Vˆµβ,†
M(s1)i
| {z }
:=(term II).
Denoting f(DH) =P
h∈[H]rh, we can obtain that
Vˆµα,†
M(s1)−Vˆµβ,†
M(s1)
(a)
≤Vˆµα,ν†(ˆµβ)
M (s1)−Vˆµβ,ν†(ˆµβ)
M (s1)
(b)=1
GX
g∈[G]h
Vˆµg
α,ν†(ˆµβ)
M (s1)−Vˆµg
β,ν†(ˆµβ)
M (s1)i
≤1
GX
g∈[G]X
h∈[H]
E
D1:h∼Pˆµg
α,ν†(ˆµβ)
M,Dh+1:H∼Pˆµg
β,ν†(ˆµβ)
M
f(DH)
−1
GX
g∈[G]X
h∈[H]
E
D1:h−1∼Pˆµg
α,ν†(ˆµβ)
M,Dh:H∼Pˆµg
β,ν†(ˆµβ)
M
f(DH)
(c)
≤2H
GX
g∈[G]X
h∈[H]E
D1:h−1∼Pˆµg
α,ν†(ˆµβ)
M,shh
TV
ˆµg,h
α×νh
†(ˆµβ)(·,·|sh),ˆµg,h
β×νh
†(ˆµβ)(·,·|sh)i
37=2H
GX
g∈[G]X
h∈[H]E
D1:h−1∼Pˆµg
α,ν†(ˆµβ)
M,shh
TV
ˆµg,h
α(·|sh),ˆµg,h
β(·|sh)i
(d)=2H
GX
g∈[G]X
h∈[H]E
D1:h−1∼Pˆµg
α,ν†(ˆµβ)
M,sh
TV
X
i∈[n]αn,iµgi,h
α(·|sh),X
i∈[n]αn,iµgi,h
β(·|sh)


(e)
≤2H
GX
g∈[G]X
h∈[H]E
D1:h−1∼Pˆµg
α,ν†(ˆµβ)
M,sh
X
i∈[n]αn,iTV
µgi,h
α(·|sh), µgi,h
β(·|sh)

≤2H
GX
g∈[G]X
h∈[H]X
s∈SX
i∈[n]αn,iTV
µgi,h
α(·|s), µgi,h
β(·|s)
(f)
≤2HX
g∈[G]X
h∈[H]X
s∈STV
µg,h
α(·|s), µg,h
β(·|s)
,
where (a) is from the definition of best responses, (b) uses the notation ˆµgto denote the output policy
from Alg. 4 with the initial random sampling result to be g, (c) uses the variational representation
of the TV distance, (d) uses the abbreviations n=Ng,h(sh)andgi=gh
i(sh), (e) leverages the
property of TV distance, and (f) uses the fact that αn,i<1.
With the above result, we can further obtain that
(term I) :=ED∼PAlgα
Mh
Vˆµα,†
M(s1)−Vˆµβ,†
M(s1)i
≤ED∼PAlgα
M
2HX
t∈[T]X
s∈STV 
µt
α(·|s), µt
β(·|s)
.
Also, for term (II), denoting g(D) =Vˆµβ,†
M(s1), it holds that
(term II) :=ED∼PAlgα
Mh
Vˆµβ,†
M(s1)i
−ED∼PAlgβ
Mh
Vˆµβ,†
M(s1)i
=X
t∈[T]EDt∼PAlgα
M,Dt+1:T∼PAlgβ
M[g(D)]−X
t∈[T]EDt−1∼PAlgα
M,Dt:T∼PAlgβ
M[g(D)]
≤2HX
t∈[T]EDt−1∼PAlgα
M,st
TV 
Algα(·,·|Dt−1, st),Algβ(·,·|Dt−1, st)
≤2HX
t∈[T]X
s∈SEDt−1∼PAlgα
M
TV 
µt
α(·|Dt−1
+, st), µt
β(·,·|Dt−1
+, sh)
+ 2HX
t∈[T]X
s∈SEDt−1∼PAlgα
M
TV 
νt
α(·|Dt−1
+, s), νt
β(·,·|Dt−1
+, s)
.
Combining (term I) and (term II) concludes the proof.
I Discussions on the Covering Number
In the following, we characterize the covering number of algorithms induced by transformers parame-
terized by θ∈Θd,L,M,d′,F, i.e.,{Algθ:θ∈Θd,L,M,d′,F}. Here we will mainly take the perspective
of the centralized setting, while the extension to the decentralized setting is straightforward.
To facilitate the discussion, we introduce the following clipped transformer, where an additional clip
operator is adopted to bound the output of the transformer.
Definition I.1 (Clipped Decoder-based Transformer) .AnL-layer clipped decoder-based transformer,
denoted as TFR
θ(·), is a composition of Lmasked attention layers, each followed by an MLP layer
and a clip operation: TFR
θ(H) =H(L)∈Rd×N, where H(L)is defined iteratively by taking
H(0)=clipR(H)∈Rd×Nand for l∈[L],
H(l)=clipR
MLPθ(l)
mlp
Attnθ(l)
mattn
H(l−1)
∈Rd×N,
where clipR(H) = [ proj∥h∥2≤R(hi) :i∈[N]].
38Furthermore, for ζ∈(0,1], we define the following ζ-biased algorithm induced by transformer
TFR
θ(H)as
Algζ
θ(·,·|Dt−1, st) = (1 −ζ)·proj∆
E·TFR
θ 
h(Dt−1, st)
−1
+ζ
AB·1AB,
with1ABdenoting an all-one vector of dimension AB, which introduces a lower bound ζ/AB for
the probably each pair (a, b)∈ A × B to be sampled.
Finally, for any H= [h1,···,hN]∈Rd×N, we denote
∥H∥2,∞:= max
i∈[N]∥hi∥2.
Proposition I.2 (Modified from Proposition J.1 in Bai et al. [7]).For any θ1,θ2∈Θd,L,M,d′,F, we
have
∥TFR
θ1(H)−TFR
θ2(H)∥2,∞≤LFL−1
HFΘ∥θ1−θ2∥,
where FΘ:=FR(2 +FR2+F3R2)andFH:= (1 + F2)(1 + F2R3).
Proof. This proposition can be obtained similarly as Proposition J.1 in Bai et al. [7]with the following
Lemma I.3 in the place of Lemma J.1 in Bai et al. [7].
Lemma I.3 (Modified from Lemma J.1 in Bai et al. [7]).For a single MLP layer θmlp= (W1,W2),
we introduce its norm
∥θmlp∥=∥W1∥op+∥W2∥op.
For any fixed hidden dimension D′, we consider
Θmlp,F:={θmlp:∥θmlp∥ ≤F}.
Then, for any H∈ H R,θmlp∈Θmlp,F, it holds that MLP θmlp(H)is2FR-Lipschitz in θmlpand
(1 +F2)-Lipschitz in H.
Proof. It holds that
∥MLP θmlp(H)−MLP θ′
mlp(H)∥2,∞
= max
i∥W2σ(W1hi)−W′
2σ(W′
1hi)∥2
≤max
i∥W2−W′
2∥op∥σ(W′
1hi)∥2+∥W′
2∥op∥σ(W1hi)−σ(W′
1hi)∥2
(a)
≤max
i∥W2−W′
2∥opmax{1,∥W1hi∥2}+∥W′
2∥op∥W1hi−W′
1hi∥2
≤(1 +FR)∥W2−W′
2∥op+FR∥W1−W′
1∥2.
Inequality (a) is from that
∥σr(x)∥2≤ ∥x∥2,∥σs(x)∥2≤1,
and
∥σr(x)−σr(y)∥2≤ ∥x−y∥2,∥σs(x)−σs(y)∥2≤ ∥x−y∥2,
where the last result on the Lipschitzness of softmax is adopted from Gao and Pavel [25].
Similarly, we can obtain that
∥MLP θmlp(H)−MLP θmlp(H′)∥2,∞
= max
i∥hi+W1σ(W2hi)−h′
i−W1σ(W2h′
i)∥
≤ ∥H−H′∥2,∞+∥W1∥opmax
i∥σ(W2hi)−σ(W2h′
i)∥2
≤ ∥H−H′∥2,∞+∥W1∥opmax
i∥W2hi−W2h′
i∥2
≤ ∥H−H′∥2,∞+F2∥H−H′∥2,∞,
which concludes the proof.
39Lemma I.4 (Modified from Lemma 16 in Lin et al. [38]).For the space of transformers {TFR
θ:θ∈
Θd,L,M,d′,F}, the covering number of the induced algorithms {Algζ
θ:θ∈Θd,L,M,d′,F}satisfies
that
logNΘd,L,M,d ′,F(ρ) =O
L2d(Md+d′) log
1 +max{F, R, L }
ζρ
.
Proof. First, similar to Lemma 16 in Lin et al. [38], we can use Example 5.8 in Wainwright [61]
to obtain that the δ-covering number of the ball B∥·∥(F)with radius Funder norm ∥ · ∥ , i.e.,
B∥·∥(F) ={θ:∥θ∥ ≤F}, can be bounded as
logN(δ;B∥·∥(F),∥ · ∥)≤L(3Md2+ 2dd′) log(1 + 2 F/δ).
Recall that the projection operation to a probability simplex is Lipschitz continuous, i.e.,
∥proj∆(x)−proj∆(y)∥2≤ ∥x−y∥2.
Then, we can see that there exists a subset ˜Θ⊂Θd,L,M,d′,Fwith size
log|Θd,L,M,d′,F| ≤L(3Md2+ 2dd′) log(1 + 2 F/δ)
such that for any θ∈Θd,L,M,d′,F, there exists ˜θ∈˜ΘwithlogAlgζ
˜θ(·,·|Dt−1, s)−logAlgζ
θ(·,·|Dt−1, s)
∞
≤AB
ζAlgζ
˜θ(·,·|Dt−1, s)−Algζ
θ(·,·|Dt−1, s)
∞
≤AB
ζTFR
˜θ(H)−TFR
θ(H)
2,∞
≤AB
ζ·LFL−1
HFΘ· ∥θ−˜θ∥
≤AB
ζLFL−1
HFΘδ.
Letδ=ζρ
ABLFL−1
HFΘ, we can obtain ∥logAlgζ
˜θ(·,·|Dt−1, s)−logAlgζ
θ(·,·|Dt−1, s)∥∞≤ρ, which
proves that
log(NΘd,L,M,d ′,F(ρ))
≤L(3Md2+ 2dd′) log(1 + 2 F/δ)
=L(3Md2+ 2dd′) log 
1 +ABLFL−1
HFΘ
ζρ!
=O
L(3Md2+ 2dd′) log
1 +ABL (1 +F2)L−1(1 +F2R3)L−1FR(2 +FR2+F3R2)
ζρ
=O
L2(Md2+dd′) log
1 +max{A, B, F, R, L }
ζρ
,
which concludes the proof.
From the transformer construction in the proofs for Theorems 3.4 and 4.1, we can observe that it
is sufficient to specify one Rwithlog(R) =˜O(1)without impacting the transformers’ operations.
Also, in Theorems 3.3 and C.3, ρis taken to be 1/N.
Finally, for the introduced ζparameter, it can be recognized that the induced algorithms discussed
in the main paper, i.e. Algθ, can be interpreted as ζ= 0, which does not lead to a meaningful
log(NΘd,L,M,d ′,F(ρ))provided in Lemma I.4. However, a non-zero ζcan tackle this situation by only
introducing an additional realization error. Especially, assuming Assumption C.2 can be achieved
withεreal= 0, i.e., exactly realizing the context algorithm (as in Theorem 4.1), we can obtain that
log 
ED∼PAlg0
Λ"
Alg0(a, b|Dt−1, s)
Algζ
θ∗(a, b|Dt−1, s)#!
40= log 
ED∼PAlg0
Λ"
Alg0(a, b|Dt−1, s)
Algθ∗(a, b|Dt−1, s)·Algθ∗(a, b|Dt−1, s)
Algζ
θ∗(a, b|Dt−1, s)#!
= log
ED∼PAlg0
ΛAlgθ∗(a, b|Dt−1, s)
(1−ζ)·Algθ∗(a, b|Dt−1, s) +ζ/(AB)
≤log1
(1−ζ) +ζ/(AB)
≤log1
1−ζ
.
With ζ=O(1/N), an additional realization error εreal=O(1/N)occurs, whose impact
on the overall performance bound (i.e., Theorems 3.5 and 4.2) is non-dominating. As a re-
sult, for the parameterization provided in Theorem 4.1, a covering number satisfies log(NΘ) =
˜O(poly(GHSAB ) log( N))can be obtained. Similarly, the results can be extended to the decen-
tralized setting, where for the parameterization provided in Theorem 3.4, it holds that log(NΘ+) =
˜O(poly(GHSAL DMDdD) log( NFD)).
J Details of Experiments
J.1 Detail of Games
•The normal-form games for the decentralized setting. The normal-form games (i.e., matrix
games) used in decentralized experiments of Sec. 5 have A=B= 5actions for both players and
G= 3000 episodes (i.e., a horizon of T= 3000 withH= 1), which can be interpreted as having
S= 1state.
•The Markov games for the centralized setting. The Markov games used in centralized exper-
iments of Sec. 5 have A=B= 5 actions for both players, S= 4 states, H= 2 steps in each
episode and G= 300 episodes (i.e., a horizon of T=GH= 600 ). The transitions are fixed for
different games: when both players take the same actions, the state transits to the next one (i.e.,
1→2,···,4→1); otherwise, the state stays the same.
At the start of each game (during both training and inference), a A×Breward matrix Rh(s,·,·)
is generated for each step h∈[H]and state s∈ S with its elements independently sampled from
a standard Gaussian distribution truncated on [0,1]. Then, the interactions proceed as follows: at
each time step h∈[H], the players select action aandbon state sbased on their computed policy
distributions. After selecting their actions, the players receive rewards Rh(s, a, b )and−Rh(s, a, b ),
respectively, while the state transits to the new one.
J.2 Collection of Pre-training Data
•The EXP3 algorithm for the decentralized setting. In the decentralized setting, both players
are equipped with the EXP3 algorithm [ 5] to collect pre-training data. Up to time step t, the
trajectory of the max-player is recorded as “ a1, r1,···, at, rt”, and that of the min-player as “ b1,1−
r1,···, bt,1−rt”,
•The VI-ULCB algorithm for the centralized setting. In the centralized setting, both players
jointly follow the VI-ULCB algorithm [ 8] to collect pre-training data. Up to time step t, the trajectory
is recorded as “ s1, a1, b1, r1,1−r1,···, st, at, bt, rt,1−rt”.
We note that the decimal digits of the rewards are limited to two to facilitate tokenization, while
1−riinstead of −riis adopted for the min-player to avoid the additional complexity of negative
numbers.
J.3 Transformer Structure and Training
The transformer architecture employed in our experiments is primarily based on the well-known
GPT-2 model [ 43], and our implementation follows the miniGPT realization3for simplicity. The
3https://github.com/karpathy/minGPT
41numbers of transformer layers and attention heads have been modified to make the entire transformer
much smaller. In particular, we utilize a transformer with 2 layers and 4 heads. Given that the
transformer is used to compute the policy, we modify the output layer of the transformer to make it
aligned with the action dimension. We focus solely on the last output from this layer to determine the
action according to the computed transformer policy.
For the training procedure, we use one Nvidia 6000 Ada to train the transformer with a batch size of
32, trained for 100 epochs, and we set the learning rate as 5×10−4. The experimental codes are
available at https://github.com/ShenGroup/ICGP .
J.4 Performance Measurement
To test the performance of the pre-trained transformer (in particular, how well it approximates EXP3
and VI-ULCB), we adopt the measurement of Nash equilibrium gap. In particular, for either the
transformer induced policy or EXP3, we denote the max-player’s policy at time step tasµt, and the
min-player’s policy at time step tasνt. Furthermore, the average policy is computed as
¯µt=1
tX
τ∈[t]µt, ¯νt=1
tX
τ∈[t]νt.
The NE gap at step tis computed as
max
a∈AR¯νt−min
b∈B(¯µt)⊤R.
For VI-ULCB, the process is similar except that µtandνtare taken as the marginalized policies
of the joint policy learned at step twhile the NE gap is cumulated over one episode. The NE gaps
averaged over 10 randomly realized games at each step are plotted in Fig. 2.
42NeurIPS Paper Checklist
1.Claims
Question: Do the main claims made in the abstract and introduction accurately reflect the
paper’s contributions and scope?
Answer: [Yes]
Justification: This work investigates the in-context game-playing (ICGP) capabilities of pre-
trained transformers, where both the decentralized and centralized settings are considered.
The abstract and introduction (i.e., Sec. 1) accurately reflect the considered problem and
the obtained main results, highlighting our contributions in providing a comprehensive
understanding of ICGP.
Guidelines:
•The answer NA means that the abstract and introduction do not include the claims
made in the paper.
•The abstract and/or introduction should clearly state the claims made, including the
contributions made in the paper and important assumptions and limitations. A No or
NA answer to this question will not be perceived well by the reviewers.
•The claims made should match theoretical and experimental results, and reflect how
much the results can be expected to generalize to other settings.
•It is fine to include aspirational goals as motivation as long as it is clear that these goals
are not attained by the paper.
2.Limitations
Question: Does the paper discuss the limitations of the work performed by the authors?
Answer: [Yes]
Justification: The major limitations of this work in our mind are illustrated in Appendix B.2.
In particular, it is emphasized that this work mainly focuses on the two-player zero-sum
Markov games and two specific game-solving algorithms. Also, the construction of the
pre-trained dataset and the need of further large-scale experiments are discussed.
Guidelines:
•The answer NA means that the paper has no limitation while the answer No means that
the paper has limitations, but those are not discussed in the paper.
• The authors are encouraged to create a separate "Limitations" section in their paper.
•The paper should point out any strong assumptions and how robust the results are to
violations of these assumptions (e.g., independence assumptions, noiseless settings,
model well-specification, asymptotic approximations only holding locally). The authors
should reflect on how these assumptions might be violated in practice and what the
implications would be.
•The authors should reflect on the scope of the claims made, e.g., if the approach was
only tested on a few datasets or with a few runs. In general, empirical results often
depend on implicit assumptions, which should be articulated.
•The authors should reflect on the factors that influence the performance of the approach.
For example, a facial recognition algorithm may perform poorly when image resolution
is low or images are taken in low lighting. Or a speech-to-text system might not be
used reliably to provide closed captions for online lectures because it fails to handle
technical jargon.
•The authors should discuss the computational efficiency of the proposed algorithms
and how they scale with dataset size.
•If applicable, the authors should discuss possible limitations of their approach to
address problems of privacy and fairness.
•While the authors might fear that complete honesty about limitations might be used by
reviewers as grounds for rejection, a worse outcome might be that reviewers discover
limitations that aren’t acknowledged in the paper. The authors should use their best
judgment and recognize that individual actions in favor of transparency play an impor-
tant role in developing norms that preserve the integrity of the community. Reviewers
will be specifically instructed to not penalize honesty concerning limitations.
433.Theory Assumptions and Proofs
Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?
Answer: [Yes]
Justification: The main paper (particularly, Sec. 3.1.1 and the beginning of Sec. 4.1) provides
the main setups and assumptions considered in this work. Additional Assumptions C.2, F.2,
and G.2 are deferred to the appendix due to the page limits while being noted in the main
paper with corresponding pointers. The complete proofs are provided in the appendix (see
Appendix A for an overview), which have been carefully checked multiple times and are
correct to the best of our knowledge. A proof sketch is included in Sec. 4.3 to facilitate the
readers’ understanding.
Guidelines:
• The answer NA means that the paper does not include theoretical results.
•All the theorems, formulas, and proofs in the paper should be numbered and cross-
referenced.
•All assumptions should be clearly stated or referenced in the statement of any theorems.
•The proofs can either appear in the main paper or the supplemental material, but if
they appear in the supplemental material, the authors are encouraged to provide a short
proof sketch to provide intuition.
•Inversely, any informal proof provided in the core of the paper should be complemented
by formal proofs provided in appendix or supplemental material.
• Theorems and Lemmas that the proof relies upon should be properly referenced.
4.Experimental Result Reproducibility
Question: Does the paper fully disclose all the information needed to reproduce the main ex-
perimental results of the paper to the extent that it affects the main claims and/or conclusions
of the paper (regardless of whether the code and data are provided or not)?
Answer: [Yes]
Justification: A detailed description of the experimental setups and details is provided
in Appendix J. The complete set of codes for the experiments are available at https:
//github.com/ShenGroup/ICGP .
Guidelines:
• The answer NA means that the paper does not include experiments.
•If the paper includes experiments, a No answer to this question will not be perceived
well by the reviewers: Making the paper reproducible is important, regardless of
whether the code and data are provided or not.
•If the contribution is a dataset and/or model, the authors should describe the steps taken
to make their results reproducible or verifiable.
•Depending on the contribution, reproducibility can be accomplished in various ways.
For example, if the contribution is a novel architecture, describing the architecture fully
might suffice, or if the contribution is a specific model and empirical evaluation, it may
be necessary to either make it possible for others to replicate the model with the same
dataset, or provide access to the model. In general. releasing code and data is often
one good way to accomplish this, but reproducibility can also be provided via detailed
instructions for how to replicate the results, access to a hosted model (e.g., in the case
of a large language model), releasing of a model checkpoint, or other means that are
appropriate to the research performed.
•While NeurIPS does not require releasing code, the conference does require all submis-
sions to provide some reasonable avenue for reproducibility, which may depend on the
nature of the contribution. For example
(a)If the contribution is primarily a new algorithm, the paper should make it clear how
to reproduce that algorithm.
(b)If the contribution is primarily a new model architecture, the paper should describe
the architecture clearly and fully.
44(c)If the contribution is a new model (e.g., a large language model), then there should
either be a way to access this model for reproducing the results or a way to reproduce
the model (e.g., with an open-source dataset or instructions for how to construct
the dataset).
(d)We recognize that reproducibility may be tricky in some cases, in which case
authors are welcome to describe the particular way they provide for reproducibility.
In the case of closed-source models, it may be that access to the model is limited in
some way (e.g., to registered users), but it should be possible for other researchers
to have some path to reproducing or verifying the results.
5.Open access to data and code
Question: Does the paper provide open access to the data and code, with sufficient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?
Answer: [Yes]
Justification: The complete set of codes can be found at https://github.com/
ShenGroup/ICGP .
Guidelines:
• The answer NA means that paper does not include experiments requiring code.
•Please see the NeurIPS code and data submission guidelines ( https://nips.cc/
public/guides/CodeSubmissionPolicy ) for more details.
•While we encourage the release of code and data, we understand that this might not be
possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
including code, unless this is central to the contribution (e.g., for a new open-source
benchmark).
•The instructions should contain the exact command and environment needed to run to
reproduce the results. See the NeurIPS code and data submission guidelines ( https:
//nips.cc/public/guides/CodeSubmissionPolicy ) for more details.
•The authors should provide instructions on data access and preparation, including how
to access the raw data, preprocessed data, intermediate data, and generated data, etc.
•The authors should provide scripts to reproduce all experimental results for the new
proposed method and baselines. If only a subset of experiments are reproducible, they
should state which ones are omitted from the script and why.
•At submission time, to preserve anonymity, the authors should release anonymized
versions (if applicable).
•Providing as much information as possible in supplemental material (appended to the
paper) is recommended, but including URLs to data and code is permitted.
6.Experimental Setting/Details
Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?
Answer: [Yes]
Justification: A detailed description of the experimental setups and details is provided in
Appendix J. The experimental codes are available at https://github.com/ShenGroup/
ICGP .
Guidelines:
• The answer NA means that the paper does not include experiments.
•The experimental setting should be presented in the core of the paper to a level of detail
that is necessary to appreciate the results and make sense of them.
•The full details can be provided either with the code, in appendix, or as supplemental
material.
7.Experiment Statistical Significance
Question: Does the paper report error bars suitably and correctly defined or other appropriate
information about the statistical significance of the experiments?
45Answer: [Yes]
Justification: Error bars have been provided in Fig. 2, which is the main experimental results
in this work.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The authors should answer "Yes" if the results are accompanied by error bars, confi-
dence intervals, or statistical significance tests, at least for the experiments that support
the main claims of the paper.
•The factors of variability that the error bars are capturing should be clearly stated (for
example, train/test split, initialization, random drawing of some parameter, or overall
run with given experimental conditions).
•The method for calculating the error bars should be explained (closed form formula,
call to a library function, bootstrap, etc.)
• The assumptions made should be given (e.g., Normally distributed errors).
•It should be clear whether the error bar is the standard deviation or the standard error
of the mean.
•It is OK to report 1-sigma error bars, but one should state it. The authors should
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
of Normality of errors is not verified.
•For asymmetric distributions, the authors should be careful not to show in tables or
figures symmetric error bars that would yield results that are out of range (e.g. negative
error rates).
•If error bars are reported in tables or plots, The authors should explain in the text how
they were calculated and reference the corresponding figures or tables in the text.
8.Experiments Compute Resources
Question: For each experiment, does the paper provide sufficient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?
Answer: [Yes]
Justification: The computer resources used to perform the experiments reported in this work
are described in Appendix J.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The paper should indicate the type of compute workers CPU or GPU, internal cluster,
or cloud provider, including relevant memory and storage.
•The paper should provide the amount of compute required for each of the individual
experimental runs as well as estimate the total compute.
•The paper should disclose whether the full research project required more compute
than the experiments reported in the paper (e.g., preliminary or failed experiments that
didn’t make it into the paper).
9.Code Of Ethics
Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines ?
Answer: [Yes]
Justification: All the authors have reviewed the NeurIPS Code of Ethics and practiced it
during this submission.
Guidelines:
•The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
•If the authors answer No, they should explain the special circumstances that require a
deviation from the Code of Ethics.
•The authors should make sure to preserve anonymity (e.g., if there is a special consid-
eration due to laws or regulations in their jurisdiction).
4610.Broader Impacts
Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?
Answer: [Yes]
Justification: Discussions on the broader impacts of this work are provided in Appendix B.1.
Due to the theoretical focus of this work, we do not foresee major negative social impacts.
Guidelines:
• The answer NA means that there is no societal impact of the work performed.
•If the authors answer NA or No, they should explain why their work has no societal
impact or why the paper does not address societal impact.
•Examples of negative societal impacts include potential malicious or unintended uses
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
(e.g., deployment of technologies that could make decisions that unfairly impact specific
groups), privacy considerations, and security considerations.
•The conference expects that many papers will be foundational research and not tied
to particular applications, let alone deployments. However, if there is a direct path to
any negative applications, the authors should point it out. For example, it is legitimate
to point out that an improvement in the quality of generative models could be used to
generate deepfakes for disinformation. On the other hand, it is not needed to point out
that a generic algorithm for optimizing neural networks could enable people to train
models that generate Deepfakes faster.
•The authors should consider possible harms that could arise when the technology is
being used as intended and functioning correctly, harms that could arise when the
technology is being used as intended but gives incorrect results, and harms following
from (intentional or unintentional) misuse of the technology.
•If there are negative societal impacts, the authors could also discuss possible mitigation
strategies (e.g., gated release of models, providing defenses in addition to attacks,
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
feedback over time, improving the efficiency and accessibility of ML).
11.Safeguards
Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?
Answer: [NA]
Justification: The transformers trained in this work are only for the purpose of validating the
theoretical claims, which we do not foresee risks of misuse.
Guidelines:
• The answer NA means that the paper poses no such risks.
•Released models that have a high risk for misuse or dual-use should be released with
necessary safeguards to allow for controlled use of the model, for example by requiring
that users adhere to usage guidelines or restrictions to access the model or implementing
safety filters.
•Datasets that have been scraped from the Internet could pose safety risks. The authors
should describe how they avoided releasing unsafe images.
•We recognize that providing effective safeguards is challenging, and many papers do
not require this, but we encourage authors to take this into account and make a best
faith effort.
12.Licenses for existing assets
Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?
Answer: [Yes]
47Justification: We have included the URL of the adopted transformer model in Appendix J,
which is under a standard MIT license.
Guidelines:
• The answer NA means that the paper does not use existing assets.
• The authors should cite the original paper that produced the code package or dataset.
•The authors should state which version of the asset is used and, if possible, include a
URL.
• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
•For scraped data from a particular source (e.g., website), the copyright and terms of
service of that source should be provided.
•If assets are released, the license, copyright information, and terms of use in the
package should be provided. For popular datasets, paperswithcode.com/datasets
has curated licenses for some datasets. Their licensing guide can help determine the
license of a dataset.
•For existing datasets that are re-packaged, both the original license and the license of
the derived asset (if it has changed) should be provided.
•If this information is not available online, the authors are encouraged to reach out to
the asset’s creators.
13.New Assets
Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?
Answer: [NA]
Justification: No new assets have been released with this work.
Guidelines:
• The answer NA means that the paper does not release new assets.
•Researchers should communicate the details of the dataset/code/model as part of their
submissions via structured templates. This includes details about training, license,
limitations, etc.
•The paper should discuss whether and how consent was obtained from people whose
asset is used.
•At submission time, remember to anonymize your assets (if applicable). You can either
create an anonymized URL or include an anonymized zip file.
14.Crowdsourcing and Research with Human Subjects
Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as
well as details about compensation (if any)?
Answer: [NA]
Justification: No crowdsourcing and research with human subjects has been performed
during this work.
Guidelines:
•The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
•Including this information in the supplemental material is fine, but if the main contribu-
tion of the paper involves human subjects, then as much detail as possible should be
included in the main paper.
•According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
or other labor should be paid at least the minimum wage in the country of the data
collector.
15.Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects
48Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?
Answer: [NA]
Justification: No crowdsourcing and research with human subjects has been performed
during this work.
Guidelines:
•The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
•Depending on the country in which research is conducted, IRB approval (or equivalent)
may be required for any human subjects research. If you obtained IRB approval, you
should clearly state this in the paper.
•We recognize that the procedures for this may vary significantly between institutions
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
guidelines for their institution.
•For initial submissions, do not include any information that would break anonymity (if
applicable), such as the institution conducting the review.
49