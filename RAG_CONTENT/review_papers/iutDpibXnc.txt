Under review as submission to TMLR
On the Role of Initialization on the Implicit Bias in Deep
Linear Networks
Anonymous authors
Paper under double-blind review
Abstract
Despite Deep Learning’s (DL) empirical success, our theoretical understanding of its efficacy
remains limited. One notable paradox is that while conventional wisdom discourages
perfect data fitting, deep neural networks are designed to do just that, yet they generalize
effectively. This study focuses on exploring this phenomenon attributed to the implicit
bias at play. Various sources of implicit bias have been identified, such as step size, weight
initialization, optimization algorithm, and number of parameters. In this work, we focus on
investigating the implicit bias originating from weight initialization. To this end, we examine
the problem of solving underdetermined linear systems in various contexts, scrutinizing
the impact of initialization on the implicit regularization when using deep networks to
solve such systems. Our findings elucidate the role of initialization in the optimization
and generalization paradoxes, contributing to a more comprehensive understanding of DL’s
performance characteristics.
1 Introduction
Deep Learning (DL) has revolutionized many fields and is poised to radically transform the modern world.
DL is quickly becoming the best practice for many computer vision problems in commerce, finance, medicine,
entertainment, and many more fields that shape our daily lives. This explosion in popularity in recent years,
both in academia and in industry, is due to its practical success. Unfortunately, our understanding of DL and
why it is so successful is lagging far behind. Simply put, we do not have satisfactory explanations for why it
performs so well, or why it is even possible to optimize such non-convex models. These are two key examples
of unanswered questions on this subject matter, among many others.
When studying neural networks, it is tempting to consider underdetermined linear systems as an exploratory
model, as it retains many key characteristics that make Deep Neural Networks difficult to analyze (non-
convexity, overparameterization) while also having the advantages of being a simple linear model (and
thus theorems from linear algebra easily apply). Furthermore, there is a growing consensus that wide
neural networks are approximately linear and operate in the so-called lazy regime (Liu et al., 2022). Highly
overparameterized models that operate in the lazy regime are approximately Gaussian Processes, and thus
can be linked to kernel methods, which are linear in the weights (but not the input). Lee et al. (2018) have
established the link between wide networks and kernel machines. Thus, one can expect observations on linear
models to apply, at least approximately, on non-linear deep networks, which further motivates us to explore
overparametrized linear models and kernel methods.
A linear model attempts to find a weight vector ysuch that given an example-by-feature matrix A(or some
non-linear transformation of an original example-by-feature matrix, in the case of kernel machines) and a
target vector b, we have Ay=b. In other words, a system of linear equations. In this work, we consider
the problem of solving underdetermined systems of linear equations through the lens of DL, and specifically
through fully connected linear neural networks.
1Under review as submission to TMLR
A fully connected neural network is a model where we are given some example-by-feature matrix Aand
ground truth vector b, and our goal is to find weights W1,W2,...,Wh,xsuch that
LA,b(W1,W2,...,Wh,x) =1
2∥σ(σ(...(σ(AW 1)W2)...Wh)x)−b∥2
2
is minimized, where σis some activation function. Common choices are ReLU (x) =max(0,x)orSigmoid (x) =
1
1+e−x. The success of these models in real life scenarios can not be overstated.
However, as previously stated, there are several open questions about this framework that require answers.
Before we mention the most interesting questions and the connection to this work, a few key insights are
given:
1. Deep networks are highly over-parametrized. Many possible minimizers, some better, some worse.
2. Ifh>1thenLA,bis non-convex, even if σ(z) =z
These properties alongside the empirical success of of deep networks go against prevailing common wisdoms in
machine learning and statistical inference, that over-parametrized models tend to overfit, and that minimizing
a non convex objective is difficult. However, when we consider the success of DL, this intuition seems incorrect.
Deep networks are highly overparameterized, often having tens of billions of parameters, but, surprisingly,
they often predict well (the generalization paradox ). Deep networks are optimized by minimizing a non-convex
function, yet, often a minimizer is found quickly (the optimization paradox ). These two paradoxes have
suscited great interest and a vast literature that explores them.
Much like this work, contemporary research frequently focuses explicitly on linear networks since it is an
excellent model problem to comprehend for the reasons described above. In a linear model, the activation
functionσis simplyσ(x) =x, and so
LA,b(W1,W2,...,Wh,x) =1
2∥AW 1W2...Whx−b∥2
2
These linear models seem useless at first glance, as composition of linear functions is still linear. However, from
an optimization perspective, they can behave quite differently. Indeed, the objective function is non-convex
ifh>1with many possible saddle points, including a trivial one (∀i:Wi= 0). The trivial saddle point
(∀i:Wi= 0)proves non-convexity for h > 1and highlights a key difference between the naive linear
model and a deep linear model. Interestingly, this model can have advantages over the shallow model in
certain scenarios. For example, Bah et al. (2021) show that optimizing a deep linear network is equivalent
to Riemannian gradient flow on a manifold of low-rank matrices, with a suitable Riemannian metric. They
show that this Riemannian optimization converges to a global optimum of L1loss with very high probability
(given the rank constraint), and when the depth of the linear network is two, then with very high probability
it minimizes L2loss.
In this work, we attempt to shed light on interesting properties that arise from initialization in several
different linear network scenarios. To this end, we study ordinary linear regression in an overparameterized
setting. We prove a condition for convergence to optimal solution with respect to the Euclidean norm (which
is in line with the study in Bartlett et al. (2020)), provide an expression for the converged solution as a
function of the initial gradient descent guess, and outline an algorithm that is capable of controlling to which
solution gradient descent will converge (see Section 2.3). The reason we focus specifically on initialization is
due to both industry experience that this seemingly innocent choice can have a drastic effect on generalization
and theoretical results that used specific initialization schemes (Hu et al., 2020; Belkin et al., 2019; Bartlett
et al., 2020).
Next, we prove similar results for an overparameterized linear model that has a single hidden layer. We show
that it is possible to find a point where the solution gradient descent converges to is optimal, as well as having
every weight be optimal with respect to the other weights. We provide algorithms that take advantage of this
optimality to reduce the dimensionality of the problem (see Section 3). We then proceed to studying deep
linear networks, proving a condition for when gradient descent converges to optimum, providing an argument
2Under review as submission to TMLR
for why a balanced optimum point is unlikely to be found using our method, with more than two hidden
layers. We then study the stability of deep linear networks and prove properties regarding weight norms
(Section 4). Finally, we provide motivation and explanation for linear Riemannian models, study properties
of such models, and perform experiments that emphasize the interesting traits of such models (these results
are reported in Section 5).
Ourresultshintattheimportanceofinitializationwhendesigningdeeplearningsolutions. Properinitialization
of learning can be treated as another component controlled by a practitioner. By carefully selecting
initialization, the practitioner can bias the result towards desired outcomes (e.g. perhaps using data specific
initialization), reduce number of parameters, and accelerate convergence. However, additional research is
required to determine how the aforementioned ideas can be translated into practice.
1.1 Related Work
Our work is closely related to, and inspired by, Bartlett et al.’s work on benign overfitting in a shallow,
ordinary linear regression setting (Bartlett et al., 2020). It is shown there that in some cases, depending on
the dimensionality of the problem and the spectrum of the noise covariance, the minimum norm interpolating
solution generalizes well, in stark contradiction to common wisdom that says we should never interpolate,
certainlyinnoisysettings. Inthiswork, wefocusoninvestigatingtheimplicitbiasthatarisesfrominitialization
in deep linear networks, and it turns out that it is possible to easily bias the solution towards the minimum
norm interpolant.
Our work is also related to the work of Belkin et al. (2019) which attempts to explain the disconnect between
classical theory and the success of interpolating overparameterized solutions in practice, by suggesting a
single unified "double descent" performance curve. We also investigate performance curves of interpolating
overparametricized solutions but in a strictly linear setting. This is in contrast to Belkin et al. (2019) which
considered random Fourier features, which are non-linear in the input.
Related is also a series of papers by Arora et al. (2018; 2019) on the effects of depth on generalization,
optimization, and specifically on weight norms. They suggest that contrary to common wisdom,
overparameterization accelerates the convergence of optimization, which is something we also explore,
even managing to collapse a deep model into a shallow model, similarly to Ablin (2020) which shows
that deep orthogonal linear networks are shallow. Furthermore, Arora et al. (2019) explores the effects of
overparameterization via depth on which type of solution we converge to in matrix factorization, which is a
continuation of the foundation that Gunasekar et al. (2017) laid out, and whether we are implicitly biased to
minimize norm (and which type of norm), or rank.
Our work goes somewhat against Razin & Cohen (2020) and focuses exclusively on norms. That work has
shown that in a matrix completion setting (which is different from our setting), there are natural problems
where we can implicitly bias towards a solution that generalizes well, but that bias is not towards minimum
norm, but rather it minimizes rank, even at the cost of pushing the nuclear norm towards infinity. This is
another notion of implicit regularization, which does not apply in our setting (since we are dealing with linear
regression where the solution is a one-dimensional column vector), and we do not focus on it at all.
2 Preliminaries
2.1 Notation
We denote scalars using Greek letters or x,y,.... Vectors are denoted by x,y,...and matrices by A,B,....
Thes×sidentity matrix is denoted by Is. IfAis an×dmatrix then it has a singular value decomposition
A=UΣVTwhere U∈Rn×nis orthogonal, Σ∈Rn×dis rectangular diagonal and V∈Rd×dorthogonal.
For simplicity, we differentiate between column vectors and row vectors explicitly. A sdimensional row
vector is denoted as being in R1×s, and asdimensional column vector is denoted as being in Rs×1. If
xis a vector of any shape or dimension, we use ∥x∥2for the Euclidean norm. If Xis a matrix of any
shape or dimension, we use ∥X∥for the operator (spectral) norm and ∥X∥Fto mean the Frobenius norm
3Under review as submission to TMLR
(∥X∥F=/radicalig
trace (XXT)). The Moore-Penrose pseudoinverse of a matrix Mwith is denoted by M+. If the
columns of Mare independent, it is equal to M+= (MTM)−1MT.
When solving a linear system of equations, A∈Rn×dis our coefficient matrix where we assume d>n, and
rank(A) =nunless otherwise stated. We denote by b̸= 0∈Rn×1the target vector. We denote by α>0
the step size or the learning rate in the gradient descent iteration. We define θ⋆to be the minimum norm
solution
θ⋆:= arg min
Ax=b∥x∥2=AT(AAT)−1b
Iffis a function of two or more variables, we will explicitly write ∇xfto refer to the gradient with respect
to the variable x, and so on. In deep models with hidden weights, all hidden layer weights are assumed to be
d×dunless otherwise stated. The subscripts will be used to denote gradient descent iterations, so Wkis
the weight matrix Win iteration k, and we denote W∞:=limk→∞Wkif such a limit exists. In Section 4
we make a slight change of notation where W(k)
istands for the value of the matrix Wiin iteration k, and
W(∞)
i= limk→∞W(k)
i
2.2 Solving Underdetermined Least Squares using Gradient Descent
In this section we consider the classical task of finding a single vector y∈Rd×1such that Ay=b, where we
assume this is accomplished by defining a loss function
LA,b(y) =1
2∥Ay−b∥2
2
and applying gradient descent with fixed step size α>0. That is, an initial guess y0is picked, and then in
each iteration the algorithm moves in the direction directly opposite to the gradient
∇LA,b(y) =AT(Ay−b)
with fixed step size α. Thus, the iteration is
yk+1=yk−αAT(Ayk−b)
In many applications, it can occur that there is a single minimizer. However, since we are dealing with
underdetermined systems ( d>n) and assume rank(A) =n, there is an infinite set of solutions: θ=θ⋆+z
where zis any vector in ker (A).
Suppose A,bwere sampled from some population of features and targets for a problem for which we wish to
build a predictive model. Common statistical wisdom is that complex prediction rules are inferior to simple
ones. In this context, simplicity can refer to the solution vector’s norm. Hence, not all solutions to Ay=b
will be as useful for predictive purposes. Our goal and objective in many scenarios is to find θ⋆.
If the iteration starts from an arbitrary y0, it will converge to an arbitrary solution, and we can expect poor
generalization unless some explicit regularization is used. However, we now show that if we initialize smartly,
we can ensure convergence to θ⋆, or any other predetermined solution.
The following lemma, which shows that the minimum norm solution θ⋆is theonlysolution in row-space of
A, is a fundamental and classic result on underdetermined linear systems, which we make extensive use of
throughout this work. We stress at the outset that this lemma is true regardless of the optimization algorithm
used to reach a solution of the system.
Lemma 1. IfAy=bandy∈range/parenleftig
AT/parenrightig
theny=θ⋆.
Proof.Since y∈range/parenleftig
AT/parenrightig
there exists a v∈Rn×1such that y=ATv. SoAATv=b. Multiply the last
equation on the left by AT(AAT)−1to get y=θ⋆.
4Under review as submission to TMLR
The next lemma is specific to gradient descent. It shows that being in the row-space of Ais conserved
throughout gradient descent iterations.
Lemma 2. Ifyk∈range/parenleftig
AT/parenrightig
for somekthenyk+1∈range/parenleftig
AT/parenrightig
.
Proof.Since yk∈range/parenleftig
AT/parenrightig
there exists a vk∈Rn×1such that yk=ATvk. Then
yk+1=yk−αAT(Ayk−b) =AT(vk−α(AATvk−b))∈range/parenleftig
AT/parenrightig
.
An important consequence of Lemmas 1 and 2, and the fact that LA,bis convex, is the following corollary,
which demonstrates a key aspect of our work: if we initialize in an intelligent way, we are guaranteed to
converge to the optimal solution with respect to l2norm.
Corollary 3. Ify0∈range/parenleftig
AT/parenrightig
andα>0is such that the iteration converges to a stationary point, then
y∞=θ⋆. A trivial choice for y0that ensures that y0∈range/parenleftig
AT/parenrightig
isy0= 0.
Proof.Applying Lemma 2 in an inductive manner, we see that for all kthere exists a vksuch that yk=ATvk.
We also assumed convergence, so we know that y∞exists and is equal to some stationary point y. Note that
since AThas full rank, a stationary point must be a solution to Ax=b. Since AThas full column rank, we
also have vk= (AT)+yk, sov∞also exists. So,
y=y∞= lim
k→∞ATvk=ATv∞∈range/parenleftig
AT/parenrightig
To sum up, yis a solution and y∈range/parenleftig
AT/parenrightig
. Now apply Lemma 1 to get y=θ⋆
Though the last results are almost trivial, we mention them nonetheless as they lead to the central theme of
this work: smart choices for initializations will bias us towards better solutions, and it is possible to determine
properties of solutions we converge to simply by initializing in line with what we want to achieve. Notice that
we have not added explicit regularization; this is not ridge regression. Using only a clever initialization, we
have biased our solution to tend towards the minimal norm solution.
Furthermore, in this simple 0-depth case, it is possible to control exactly which solution the iteration will
converge to. Theorem 4 proves this is possible and provides an algorithm to do so. This is another example
of specific initializations yielding desirable solutions.
Theorem 4. Suppose that A∈Rn×dis a matrix of any rank, where d≥n, and let A=UΣVTbe a singular
value decomposition of A. Let V1be the first ncolumns of V, and V2the remaining columns. If ∥A∥2<2
α
then for any given initial guess y0we see that y∞exists and
y∞=V2VT
2y0+θ⋆
Proof.Write
Σ=/bracketleftbig˜Σ0n×(d−n)/bracketrightbig
5Under review as submission to TMLR
where ˜Σ∈Rn×nis diagonal. Notice that A=U˜ΣVT
1andθ⋆=V1˜Σ−1UTb. At stepkof gradient descent
we have
yk=yk−1−αAT(Ayk−1−b)
= (Id−αATA)yk−1+αATb
=...
= (Id−αATA)ky0+αk−1/summationdisplay
j=0(Id−αATA)jATb
= (V(Id−αΣTΣ)VT)ky0+αk−1/summationdisplay
j=0(V(Id−αΣTΣ)VT)jVΣTUTb
Since Id−αΣTΣis diagonal and Vis orthogonal, the last equation simplifies to
yk=V(Id−αΣTΣ)kVTy0+αk−1/summationdisplay
j=0V(Id−αΣTΣ)jΣTUTb.
Denote zk=VTyk, and multiply the last equation by VTon the left to get
zk= (Id−αΣTΣ)kz0+αk−1/summationdisplay
j=0(Id−αΣTΣ)jΣTUTb
=/bracketleftbigg
(In−α˜Σ2)k0n×(d−n)
0(d−n)×n Id−n/bracketrightbigg
z0+αk−1/summationdisplay
j=0/bracketleftbigg
(In−α˜Σ2)j˜Σ
0(d−n)×n/bracketrightbigg
UTb
The condition on αensures that all eigenvalues of In−α˜Σ2have absolute value strictly smaller than 1, which
is a sufficient condition for limk→∞(In−α˜Σ2)k= 0, which is in turn equivalent to the convergence of the
Neumann series/summationtext∞
j=0(In−α˜Σ2)jto(α˜Σ2)−1, so we have:
lim
k→∞zk=/bracketleftbigg0n 0n×(d−n)
0(d−n)×n Id−n/bracketrightbigg
z0+α∞/summationdisplay
j=0/bracketleftbigg
(In−α˜Σ2)j˜Σ
0(d−n)×n/bracketrightbigg
UTb
=/bracketleftbigg0n 0n×(d−n)
0(d−n)×n Id−n/bracketrightbigg
z0+/bracketleftbigg
α(α˜Σ2)−1˜Σ
0(d−n)×n/bracketrightbigg
UTb
=/bracketleftbigg0n 0n×(d−n)
0(d−n)×n Id−n/bracketrightbigg
z0+/bracketleftbigg˜Σ−1
0(d−n)×n/bracketrightbigg
UTb
Since yk=Vzkwe have
lim
k→∞yk=V( lim
k→∞zk)
=/bracketleftbigV1V2/bracketrightbig/bracketleftbigg0n 0n×(d−n)
0(d−n)×n Id−n/bracketrightbigg/bracketleftbiggVT
1
VT
2/bracketrightbigg
y0+/bracketleftbigV1V2/bracketrightbig/bracketleftbigg˜Σ−1
0(d−n)×n/bracketrightbigg
UTb
=V2VT
2y0+θ⋆
As a somewhat esoteric use of the last theorem, we can not only tell in advance to which solution the
iteration will converge to, we can also control to which solution. Theorem 4 tells us that if we want to reach
a solution θ, to get a valid initial guess y0that will lead to convergence to θ, we need to solve the system
V2VT
2y0=θ−θ⋆. This is a d×dsystem of rank d−n. We claim that this system has infinitely many
6Under review as submission to TMLR
solutions, because both θandθ⋆are solutions to Ay=b, soθ−θ⋆is a solution to Ay= 0, and so the
augmented matrix/bracketleftbig
V2VT
2θ−θ⋆/bracketrightbig
also has rank d−n, then the claim follows from the Rouche-Capelli
Theorem.
To convince ourselves that the augmented matrix/bracketleftbig
V2VT
2θ−θ⋆/bracketrightbig
indeed has rank d−n, we first notice
that since rank(XXT) =rank(X)for any real matrix X, we have rank(V2VT
2) =rank(V2) =d−n, since
the columns of V2are orthogonal, so they are also independent. Furthermore, notice that
AV 2VT
2=U/bracketleftbig˜Σ0n×(d−n)/bracketrightbig/bracketleftbigg
VT
1
VT
2/bracketrightbigg
V2VT
2
=U/bracketleftbig˜Σ0n×(d−n)/bracketrightbig/bracketleftbigg0n×(d−n)
Id−n/bracketrightbigg
VT
2
=U0n×(d−n)VT
2
= 0
That is to say, the columns of V2VT
2span ad−ndimensional subspace of vectors, where every vector in that
subspace is a solution to Ay= 0. By the rank-nullity theorem, we know that this subspace of homogeneous
solutions is d−ndimensional, so{y:Ay= 0}=span(V2VT
2), but (θ−θ⋆)∈{y:Ay= 0}and so it does
not add new information to V2VT
2, hence the rank of the augmented matrix/bracketleftbig
V2VT
2θ−θ⋆/bracketrightbig
isd−n.
Algorithm 1 Controlled ordinary linear regression.
Inputs: A∈Rn×d,b∈Rn×1,α∈R,θ∈Rd×1
_,_,/bracketleftbiggVT
1
VT
2/bracketrightbigg
←SVD(A)
θ⋆←AT(AAT)−1b
z0←arbitrary
foriterationk= 0,1,...until convergence do
zk+1←zk−αV2VT
2(V2VT
2zk−(θ−θ⋆))
end for
y0←zk
foriterationk= 0,1,...until convergence do
yk+1←yk−αAT(Ayk−b)
end for
output yk
7Under review as submission to TMLR
Figure 2.1: Illustration of solving x+y= 0using two initializations. The desired solution was (10,−10)
This possibility of controlling which solution we converge to is summarized in Algorithm 1. In Figure 2.1
we illustrate it by solving the trivial system x+y= 0with two initializations: one is a random point in
range/parenleftig
AT/parenrightig
and the other was the initial point suggested by Algorithm 1 when the desired solution was
(10,−10). We see that when initialized in range/parenleftig
AT/parenrightig
the iteration converges to the minimum norm solution
(0,0)and that Algorithm 1 works as intended.
The results of this subsection are striking, although simple, examples of the importance of initialization and
the role it plays on regularization. Although we did not introduce an explicit regularization at any point,
since we initialized in a clever way, we can reach the minimum norm solution. This is the central theme of
this work. As a consequence of Corollary 3, we point out that y0= 0is always a good choice for an initial
value if we want to converge to θ⋆for depth- 0linear networks.
2.3 Deep linear networks
A deep linear network of depth his a machine learning model that attempts to fit Atobby finding
W1,W2,...,Wh∈Rd×dandx∈Rd×1such that
∥AW 1W2...Whx−b∥2
is minimized. The minimization is done by gradient descent. The weights W1,W2,...,Whare called the
hidden layer weights, and the amount of hidden layer weights is defined as the depth of the model. A deep
linear network with depth hhashd2+dtrainable weights to optimize.
In a non-linear network, the bigger his, the more expressive power the model has. In linear networks, the
expressive power remains the same, but there is implicit acceleration at play (Arora et al., 2018), and the
model can take a different path from the usual ordinary linear regression. Furthermore, the increase in hover
the ordinary linear regression model leads to the inclusion of saddle points (a trivial one is at Wi= 0,x= 0),
which makes training more difficult in theory. In the following sections, we explore deep linear networks and
how initialization affects their characteristics.
8Under review as submission to TMLR
3 The Role of Initialization in Regularizing One Hidden Layer Linear Networks
In this section we consider the problem of learning both Wandxsuch that
LA,b(W,x) =1
2∥AWx−b∥2
2=1
2∥Ay−b∥2
2
is minimized, where we define y:=Wx. This is equivalent to linear neural network with a single hidden
layer. Similar to the previous section, now the gradients are
∇xLA,b(W,x) = WTAT(AWx−b)
∇WLA,b(W,x) = AT(AWx−b)xT
and so the iteration step is
xk+1 =xk−αWT
kAT(AWkxk−b)
Wk+1 =Wk−αAT(AWkxk−b)xT
k
While the use of the hidden layer may seem redundant (since composition of linear functions is still linear),
this model has highly non-trivial properties. One clear key difference is a dramatic increase in the level of
overparameterization. Indeed, now there are d2+d=O(d2)trainable parameters. This is important, as
modern machine learning theory is focused on the advantages in overparameterization (Arora et al. (2018);
Bartlett et al. (2020); Belkin et al. (2019) and many others). Another important difference from the model in
Section 2.3, is that similar to DNNs in any interesting setting, this loss function is non-convex. However, we
see that this non-convexity does not harm deep networks too much, empirically (Sejnowski, 2020), as a good
solution is often attained, and saddles and bad local minima are avoided.
We remark that single hidden layer networks have been studied extensively in the context of Neural Tangent
Kernels (NTKs). These models operate in two different regimes, depending on the norm of initialization and
how aggressive the overparameterization is in the hidden layer. If the deep model resembles it’s linearization,
then it operates in the kernel regime, also called lazy training. This shows that there is a connection between
deep networks and linear networks, which provides additional motivation for our study.
Indeed, previous works on NTKs show that if the overparameterization (width of the network in this sense)
is aggressive enough, and the weights have been initialized from a rotation-invariant distribution such as
a standard normal distribution, then the law of large numbers assures us that the model is guaranteed to
operate in this lazy regime and behave as a linear model, solving an under-determined system of equations,
and has all the disadvantages of that model (like bad local minima, large norm solutions). Hence, while
initialization does not matter for operating in this regime, it matters significantly from a generalization
point of view when training the model. This is in line with our work, which emphasizes the importance
of clever initializations. Furthermore, previous work on NTK also shows that one can choose to train an
NTK by solving the linear system indirectly by kernel regression, which is a convex problem with a square
matrix and a single solution, and there the initialization truly does not matter. Solving the kernel regression
problem yields the minimum norm solution and is equivalent to solving the linear system with a row-space
initialization. However, this does not preclude the possibility of other algorithms that will benefit from careful
initialization.
The following lemma is an example of the similarities between the ordinary linear regression model of Section
2.3 and the one hidden layer model. It is the one hidden layer equivalent of Lemma 2, giving us an easy and
good initialization for gradient descent to reach θ⋆.
Lemma 5. IfWk∈range/parenleftig
AT/parenrightig
for somekthenWk+1∈range/parenleftig
AT/parenrightig
.
Proof.In a similar way to Lemma 2, let Wk=ATZfor some Z∈Rn×d, so
Wk+1 =Wk−αAT(AWkxk−b)xT
k
=AT(Z−α(AATZxk−b)xT
k)∈range/parenleftig
AT/parenrightig
9Under review as submission to TMLR
As for the limit, suppose Wk=ATZkfor allk, and that the limits W∞andx∞exist (which also means
limk→∞Zk=Z∞exists since AThas full column rank). This trivially gives us
W∞= lim
k→∞[Wk−αAT(AWkxk−b)xT
k]
=AT(Z∞−α(AW∞x∞−b)xT
∞)
.
Since we also have W∞=ATZ∞we find that (AW∞x∞−b)xT
∞= 0, sox∞̸= 0implies AW∞x∞=b.
We lost convexity, so we are not sure we are converging to a solution (e.g. we could get stuck at a saddle
point, a trivial example is W0= 0,x0= 0), but the above discussion shows that if we do converge to a
solution, and W0∈range/parenleftig
AT/parenrightig
, we are guaranteed to converge to θ⋆regardless of what x0was.
Having W0∈range/parenleftig
AT/parenrightig
and assuming that we converge to a solution assures us that limk→∞Wkxkwill
be the minimal solution to the problem Ax=b, but each weight individually may not be optimal with
respect to the other. That is, it is possible that x∞is not the optimal solution to problem (AW∞)x=b
and vice versa.
We refer to a solution pair (W,x)asbi-optimal , ifWxis a minimum norm solution of Az=b,Wis a
minimum Frobenius norm solution of AZx=bwhere Zis the free parameter and xis a minimum norm
solution to AWz=b. The following theorem provides us with a criterion on the initial W0andx0that
ensures that W∞andx∞, if they exist, are bi-optimal.
Theorem 6 (bi-optimality) .LetW0=ATv0xT
0for some v0∈Rn×1and suppose xk̸= 0for allk. Then
for allkthere exists a vk∈Rn×1such that Wk=ATvkxT
k.
Proof.To make the proof more readable and less heavy on the use of subscripts, we make the following
temporary change of notation:
W :=W0
x:=x0
Z:=W1
y:=x1
v:=v0
r:=AW 0x0−b
First, we write that the iteration step in the new notation is
Z=W−αATrxT
y=x−αWTATr
then we claim that
yTyZ=ZyyT
To see this, we first write
yTyZ = (xT−αrTAW )(x−αWTATr)(W−αATrxT)
= (xTx−αxTWTATr−αrTAWx +α2rTAWWTATr)(W−αATrxT)
= (xTx)·W+ (−αxTx)·ATrxT+ (−αxTWTATr)·W+ (α2xTWTATr)·ATrxT+
(−αrTAWx )·W+ (α2rTAWx )·ATrxT+ (α2rTAWWTATr)·W+
(−α3rTAWWTATr)·ATrxT
10Under review as submission to TMLR
For reasons that will become clear, we will change the order of summation and instead write:
yTyZ = (xTx)·W+ (−αxTWTATr)·W+ (−αrTAWx )·W+ (α2rTAWWTATr)·W
+(−αxTx)·ATrxT+ (α2xTWTATr)·ATrxT+ (α2rTAWx )·ATrxT+
(−α3rTAWWTATr)·ATrxT
Similarly, write
ZyyT= (W−αATrxT)(x−αWTATr)(xT−αrTAW )
= (W−αATrxT)(xxT−αxrTAW−αWTATrxT+α2WTATrrTAW )
=WxxT−αWxrTAW−αWWTATrxT+α2WWTATrrTAW
−αATrxTxxT+α2ATrxTxrTAW +α2ATrxTWTATrxT−α3ATrxTWTATrrTAW
Both yTyZandZyyThave eight terms in their expressions. We claim that the equality is true, since each
term is equal to its equivalent term (with respect to order) in the other expression.
1. First term:
WxxT=ATvxTxxT
=ATv(xTx)xT
= (xTx)·ATvxT
= (xTx)·W
2. Second term:
−αWxrTAW =−αATvxTxrTAATvxT
=−αATv(xTx)(rTAATv)xT
=−α(xTx)(rTAATv)·ATvxT
=−α(xTx)(rTAATv)T·ATvxT
=−α(xTx)(vTAATr)·ATvxT
= (−αxT)(xvTA)(ATr)·ATvxT
= (−αxTWTATr)·W
3. Third term:
−αWWTATrxT=−αATvxTxvTAATrxT
=−αATv(xTx)(vTAATr)xT
=−α(vTAATr)(xTx)·ATvxT
=−α(vTAATr)T(xTx)·ATvxT
=−α(rTAATv)(xTx)·ATvxT
=−α(rTA)(ATvxT)(x)·ATvxT
= (−αrTAWx )·W
4. Fourth term:
α2WWTATrrTAW =α2ATvxTxvTAATrrTAATvxT
=α2ATv(xTx)(vTAATr)(rTAATv)xT
=α2(rTAATv)(xTx)(vTAATr)·ATvxT
=α2(rTA)(ATvxT)(xvTA)(ATr)·ATvxT
= (α2rTAWWTATr)·W
11Under review as submission to TMLR
5. Fifth term:
−αATrxTxxT=−αATr(xTx)xT
= (−αxTx)·ATrxT
6. Sixth term:
α2ATrxTxrTAW =α2ATrxTxrTAATvxT
=α2ATr(xTx)(rTAATv)xT
=α2(xTx)(rTAATv)·ATrxT
=α2(xTx)(rTAATv)T·ATrxT
=α2(xTx)(vTAATr)·ATrxT
= (α2xT)(xvTA)(ATr)·ATrxT
= (α2xTWTATr)·ATrxT
7. Seventh term:
α2ATrxTWTATrxT=α2ATrxTxvTAATrxT
=α2ATr(xTx)(vTAATr)xT
=α2(vTAATr)(xTx)·ATrxT
=α2(vTAATr)T(xTx)·ATrxT
=α2(rTAATv)(xTx)·ATrxT
=α2(rTA)(ATvxT)(x)·ATrxT
= (α2rTAWx )·ATrxT
8. Eighth term:
−α3ATrxTWTATrrTAW =−α3ATrxTxvTAATrrTAATvxT
=−α3ATr(xTx)(vTAATr)(rTAATv)xT
=−α3(rTAATv)(xTx)(vTAATr)·ATrxT
=−α3(rTA)(ATvxT)(xvTA)(ATr)·ATrxT
= (−α3rTAWWTATr)·ATrxT
This shows that yTyZ=ZyyT, which is another way of writing
Z=1
∥y∥2
2ZyyT
where we now used the assumption that xkis never zero, and thus y̸= 0and this division is valid. To
conclude the proof, let
u:=1
∥y∥2
2(v−αr)xTy
and check that
ATuyT=1
∥y∥2
2AT(v−αr)xTyyT
=1
∥y∥2
2(W−αATrxT)yyT
=1
∥y∥2
2ZyyT
=Z.
12Under review as submission to TMLR
Going back to our original notation, we have shown that if W0=ATv0xT
0for some v0, then W1=ATv1xT
1
for some v1which we called uand even gave an expression for. Applying this argument inductively exactly
in the same way from iteration kto iteration k+ 1, will give us the desired result.
Before proceeding, a short discussion on the case xk+1= 0, where an interesting phenomenon occurs, is in
order. Suppose that at iteration kthe weights are the non-zero pair (ATvkxT
k,xT
k). Ifxk+1= 0then at
iterationk+ 1the weights are the pair (ATvkxT
k−αATrkxT
k,0), which shows that the theorem does not
hold at this iteration. But we can notice that at iteration k+ 2we have
xk+2 =α(xkvT
kA−αxkrT
kA)ATb
Wk+2 =ATvkxT
k−αATrkxT
k
and if we define vk+2:=1
∥xk+2∥2
2AT+Wk+2xk+2then
ATvk+2xT
k+2 =1
∥xk+2∥2
2ATAT+Wk+2xk+2xT
k+2
=1
∥α(xkvT
kA−αxkrT
kA)ATb∥2
2ATAT+(ATvkxT
k−αATrkxT
k)xk+2xT
k+2
=1
∥α(xkvT
kA−αxkrT
kA)ATb∥2
2AT(vkxT
k−αrkxT
k)xk+2xT
k+2
=AT(vkxT
k−αrkxT
k)xk(vT
kA−αrT
kA)ATbbTA(ATvk−αATrk)xT
k
∥xk(vT
kA−αrT
kA)ATb∥2
2
=1
∥xk∥2
2(ATvkxT
k−αATrkxT
k)xkxT
k
=1
∥xk∥2
2(ATvk−αATrk)(xT
kxk)xT
k
=ATvkxT
k−αATrkxT
k
=Wk+2
which shows that in iteration k+1we don’t have the desired outcome, but one iteration later it course-corrects
and we return to the pattern Wk=ATvkxT
k.
The reason we refer to this theorem as bi-optimality is the following important corollary, which shows that if
we initialize as required by Theorem 6, then bi-optimality is guaranteed.
Corollary 7. If the conditions of Theorem 6 hold, and W∞,x∞exist and are non-zero, then AW∞x∞=b
by the discussion following the proof of Lemma 5, and the following statements are true:
1.W∞x∞is the minimum norm solution to the problem Az=b
2.x∞is the minimum norm solution to the problem (AW∞)z=b
3. vec (W∞)is the minimum norm solution to the problem (xT
∞⊗A)z=b, (i.e. W∞is the minimum
Frobenius norm solution to AZx∞=b).
13Under review as submission to TMLR
Proof. 1.The conditions of Theorem 6 are assumed to hold, so for all kxk̸= 0and there exists vk
such that Wk=ATvkxT
k. First, we mention that v∞must exist:
v∞= lim
k→∞1
∥xk∥2
2InvkxT
kxk
= lim
k→∞1
∥xk∥2
2(AT+AT)vkxT
kxk
= lim
k→∞1
∥xk∥2
2AT+(ATvkxT
k)xk
= lim
k→∞1
∥xk∥2
2AT+Wkxk
=1
∥x∥2
2AT+W∞x∞.
We emphasize that Ahas full rank and so In=AT+AT. From the existence of the limits
W∞,x∞,v∞, we can now safely deduce by simple multiplication.
W∞= lim
k→∞Wk= lim
k→∞ATvkxT
k=ATv∞xT
∞
SoW∞x∞∈range/parenleftig
AT/parenrightig
andAW∞x∞=b, and due to Lemma 1 we have θ⋆=W∞x∞.
2.x∞is trivially a solution of (AW∞)z=b. To see that it is the minimum norm solution, we prove
thatx∞∈range/parenleftbig
(AW∞)T/parenrightbig
by exploiting the facts that W=ATv∞xT
∞andv∞̸= 0, as that
would imply b= 0:
x∞=x∞(vT
∞AAT)(AATv∞)
∥AATv∞∥2
2
=(x∞vT
∞A)ATAATv∞
∥AATv∞∥2
2
=WT
∞ATAATv∞
∥AATv∞∥2
2
= (AW∞)T/parenleftbigg1
∥AATv∞∥2
2AATv∞/parenrightbigg
∈range/parenleftbig
(AW∞)T/parenrightbig
Now we apply Lemma 1.
3.W∞=ATv∞xT
∞implies by the properties of the Kronecker product that
vec(W∞) =vec(ATv∞xT
∞) = ( x∞⊗AT)v∞∈range/parenleftig
x∞⊗AT/parenrightig
=range/parenleftbig
(xT
∞⊗A)T/parenrightbig
.
Combine this with the fact that
(xT
∞⊗A)vec(W∞) =vec(AW∞x∞)
=AW∞x∞
=b
and apply Lemma 1 to conclude the proof
Even in this somewhat non-trivial model we see that initialization plays an important role, and if we initialize
intelligently, not only can we reach the minimum norm solution θ⋆that we desired, but we can factor it into
14Under review as submission to TMLR
Wxsuch that each variable is the minimal solution with respect to the other, a sort of bi-optimality where
every variable is optimal and no variable has any incentive to move. Of course, since we are still converging
toθ⋆, this method does not offer better generalization than other methods that converge to θ⋆, though we
hope that further research will yield useful properties of bi-optimal solutions.
An additional nice thing about Theorem 6 is that since Wk=ATvkxT
kin every iteration, instead of
optimizing over Wk, we can instead iterate over vk. The update step for xkremains the same (except that
we replace WkwithATvkxT
k), and the update step for vkis as denoted in the theorem. See Algorithm 2 for
a pseudocode description.
Algorithm 2 One hidden layer bi-optimal network
Inputs: A∈Rn×d,b∈Rn×1,α∈R
x0←arbitrary, not zero
v0←arbitrary, not zero
foriterationk= 0,1,...until convergence do
xk+1←xk−αxkvT
kAAT(AATvkxT
kxk−b)
vk+1←1
∥xk+1∥2
2(vk−α(AATvkxT
kxk−b))xT
kxk+1
end for
output ATvkxT
k,xk
We reduced the dimensions of our variables from d2+dtod+n, but we can improve further! Notice that
γk:=vT
kAAT(∥xk∥2
2AATvk−b)
is a scalar, so we can write
xk+1= (1−vT
kAAT(∥xk∥2
2AATvk−b))xk= (1−γk)xk
=k/productdisplay
i=0(1−γi)x0
=ρk+1x0
whereρk:=/producttextk−1
i=0(1−γi). Now,
vk+1 =1
ρ2
k+1∥x0∥2
2(vk−α(ρ2
k∥x0∥2
2AATvk−b))ρ2
k(1−γk)∥x0∥2
2
=1
1−γk(vk−α(ρk∥x0∥2
2AATvk−b))∥x0∥2
2.
Since x0is arbitrary, to simplify matters, we can sample x0from the unit sphere, and arrive at the concise
update step
vk+1 =1
1−γk(vk−α(ρkAATvk−b)).
Notice that we do not even need to iterate on xk, but rather only on γk, the product ρk, and vk. The number
of parameters is now n+ 2, which we remind the reader is possibly much lower than d, especially in the
setting we consider as d>n, and probably d≫nin many real-world settings. This yields the following very
interesting algorithm (Algorithm 3).
The time complexity of running Algorithm 3 for t>1iterations is O(t·max(n,TA)), where we abstract the
time it takes to multiply by AorATbyTA. One final observation regarding this algorithm is that since
lim
k→∞ρkATvk=θ⋆=AT(AAT)−1b
andAThas full column rank and thus has a left inverse, is that we know ahead of time that
v∞= lim
k→∞1
ρk(AAT)−1b.
15Under review as submission to TMLR
Algorithm 3 Compact hidden layer iteration
A∈Rn×d,b∈Rn×1,α∈Rinputs
v0←arbitrary, not zero O(n)
ρ0←1 O(1)
z0←ATv0 O(TA)
foriterationk= 0,1,...until convergence do
yk←Azk O(TA)
rk←α(ρ2
kyk−b) O(n)
γk←yT
krk O(n)
vk+1←1
1−γk(vk−rk) O(n)
ρk+1←ρk(1−γk) O(1)
zk+1←ATvk+1 O(TA)
end for
outputρ2
kzk
Of course (AAT)−1bis the unique solution to the problem AATz=b, which is a highly ill-conditioned
problem in most scenarios as κ(AAT) =κ(A)2. So this algorithm aims to solve Ax=bby way of solving
AATx=bwith a variable step size, but at the very low cost of optimizing a single layer, plus an additional
scalar. This shows that given an intelligent initialization, not only can we converge to the best solution, we
can collapse deep networks to have the same per iteration cost of shallow networks, up to a constant factor.
This is in the same spirit as the lottery ticket hypothesis (LTH, (Frankle & Carbin, 2019)), but there are two
key differences. The crux of the LTH is that at initialization, a randomly initialized neural network contains
a sub-network that if trained in isolation will reach the same or similar accuracy to the complete network
after training, and propose an iterative method for finding this sub-network. The similarity is clear in that
we reduce the cost of per iteration of tranining/testing, but the differences are that we propose an a priori
method for reducing the number of parameters and faster iterations, rather than an a posteriori iterative
one. Furthermore, our collapsed model is not composed of sub-weights of the original model, but rather
constraining the weights to be of a particular low rank form and optimizing the respective vectors that make
up this decomposition.
This algorithm is completely equivalent to a single hidden layer neural network, but does not give any
advantages in generalization. Does it have any advantages when it comes to optimization? Recent work
(Arora et al., 2018) suggests that overparameterization has advantages when it comes to optimization, and
that depth preconditions the problem. However, to the best of our knowledge, they did not consider an
underdetermined system, which is exactly our setting. We empirically test this idea in an underdetermined
setting. See Section 4.1.
4 The Role of Initialization in Deep Linear Networks
4.1 Collapsing two hidden layers linear networks
In this section, we consider the task of finding W1,W2,...,Wh,xsuch that
LA,b(W1,W2,...,Wh,x) =1
2∥AW 1W2...Whx−b∥2
2=1
2∥Ay−b∥2
2
is minimized where h>1and we define y:=W1W2...Whx. As one can expect, this model shares many
properties with the previous two models. Gradients are
∇WjLA,b(W1,W2,...,Wh,x) =

WT
j−1...WT
1AT(AW 1...Whx−b)xTWT
h...WT
j+1,1<j <h
(AW 1...Whx−b)xTWT
h...WT
2, j = 1
WT
h−1...WT
1AT(AW 1...Whx−b), j =h
16Under review as submission to TMLR
and
∇xLA,b(W1,W2,...,Wh,x) = WT
hWT
h−1...WT
1AT(AW 1W2...Whx−b).
The iteration step is
W(k+1)
j =W(k)
j−α∇WjLA,b(W(k)
1,W(k)
2,...,W(k)
h,xk)
xk+1 =xk−α∇xLA,b(W(k)
1,W(k)
2,...,W(k)
h,xk)
which leads us to this next very familiar lemma, which generalizes the previous 0-layer and 1-layer results to
arbitrary amount of layers. As one can expect, the property of the first weight being in the row-space of Ais
conserved throughout gradient descent iterations.
Lemma 8. IfW(k)
1∈range/parenleftig
AT/parenrightig
for somek, then W(k+1)
1∈range/parenleftig
AT/parenrightig
.
Proof.Identical to the proof of Lemma 5 with the respective change in matrices.
Unsurprisingly, the critical importance of initialization remains for deep models and is even exacerbated.
Although, naturally, many properties are shared with the model in Section 3, some things are also different.
The following series of results prove it is possible to collapse a linear network with h= 2and outline how to
do so. We start with Lemma 9, which describes how to initialize in a way that we will later use to achieve
bi-optimality in this model. This lemma is the basis for an induction we later use to prove Theorem 12.
Lemma 9. Whenh= 2, it is possible to find W1,W2,xall non-zero such that W1=ATvxTWT
2and
W2=WT
1ATuxTandx∈range/parenleftig
WT
2WT
1AT/parenrightig
for some v∈Rn×1andu∈Rn×1
Proof.Letvbe any non-zero vector. Set x=ATv
∥ATv∥2
2,u=AATv
∥AATv∥2
2, and
W1=
| 0 0... 0
ATv............
| 0 0... 0
.
Notice that ATv,AATv̸= 0from the rank-nullity theorem. Finally set W2=WT
1ATuxT. Let us verify the
other conditions:
ATvxTWT
2=ATvxTxuTAW 1
=ATvuTAW 1
= (( ATv)(ATu)T)W1
=
| 0 0... 0
((ATv)T(ATu))ATv............
| 0 0... 0

=
| 0 0... 0
1
∥AATv∥2
2((ATv)T(ATAATv))ATv............
| 0 0... 0

=
| 0 0... 0
ATv............
| 0 0... 0

=W1
17Under review as submission to TMLR
The matrix (ATv)(ATu)Thas eigenvector ATvwith eigenvalue (ATv)T(ATu) = 1. It is useful to mention
that if the matrices and vectors were constructed this way, we also have
W2=WT
1ATuxT
=1
∥AATv∥2
2
−vTA−
0... 0
.........
0... 0
ATAATvxT
=1
∥AATv∥2
2
(vTAAT)(AATv)
0
...
0
xT
=
1
0
...
0
xT
=
−xT−
0... 0
.........
0... 0

To finish the proof, note that
WT
2WT
1ATu=
|0... 0
x.........
|0... 0

−vTA−
0... 0
.........
0... 0
ATAATv
∥AATv∥2
2
=
|0... 0
x.........
|0... 0

−vTAAT−
0... 0
.........
0... 0
AATv
∥AATv∥2
2
=
|0... 0
x.........
|0... 0

1
0
...
0

=x.
sox∈range/parenleftig
WT
2WT
1AT/parenrightig
We now state and prove two technical lemmas, which are needed later only to prove the much more insightful
Theorem 12.
Lemma 10. Forh= 2, if at any iteration kwe have W(k)
1be all zeros except first column and W(k)
2be all
zeros except first row, then W(k+1)
1is all zeros except first column and W(k+1)
2is all zeros except first row.
Proof.Follows immediately from the iteration update steps:
W(k+1)
1 =W(k)
1−αAT(AW(k)
1W(k)
2xk−b)xT
kW(k)T
2
W(k+1)
2 =W(k)
2−αW(k)T
1AT(AW(k)
1W(k)
2xk−b)xT
k
18Under review as submission to TMLR
Lemma 11. Forh= 2, if in any iteration kwe have W(k)
1all zeros except for the first column, and
W(k)
2=
−xT
k−
0... 0
.........
0... 0

Then,
W(k+1)
2 =
−xT
k+1−
0... 0
.........
0... 0

Proof. W(k)
1TAT(AW(k)
1W(k)
2xk−b)is a column vector of length dwith a single non-zero entry in the first
index. Denote that non-zero value as β. Therefore, the first row of the matrix is
W(k+1)
2 =W(k)
2−αW(k)T
1AT(AW(k)
1W(k)
2xk−b)xT
k
isxT
k−αβxT
k. Similarly, (AW(k)
1W(k)
2xk−b)TAW(k)
1is a row vector of length dwith a single non-zero
entryβin the first index. Now the update step for xTis
xT
k+1 =xT
k−α(AW(k)
1W(k)
2xk−b)TAW(k)
1W(k)
2
=xT
k−α/parenleftbigβ0... 0/parenrightbig
−xT
k−
0... 0
.........
0... 0

=xT
k−αβxT
k
Both terms have the same update step, hence they are equal.
The next theorem is a key result, which builds on the previous lemmas, and shows that gradient descent
conserves during training the very special form of the weights described in Lemma 9. We then use this
theorem to prove the bi-optimality equivalent of this model in Corollary 14.
Theorem 12. Forh= 2, suppose that W(0)
1andW(0)
2andx0were constructed as described in Lemma
9 and assume that xkis never zero. Then, for all kthere exist vk∈Rn×1anduk∈Rn×1such that
W(k)
1=ATvkxT
kW(k)T
2andW(k)
2=W(k)T
1ATukxT
kandxk∈range/parenleftig
W(k)T
2W(k)T
1AT/parenrightig
Proof.The proof is inductive, just as in the h= 1case. The basis of our induction is given by Lemma 9.
Now suppose that the hypothesis is true up to k. By Lemma 10, we know that W(k+1)
1are all zeros except
the first column w(k+1)
1,W(k+1)
2is all zeros except first row, which is equal to xT
k+1by Lemma 11. Define
vk+1 :=1
∥xk+1∥4
2AT+W(k+1)
1W(k+1)
2xk+1
19Under review as submission to TMLR
and verify that
ATvk+1xT
k+1W(k+1)T
2 =1
∥xk+1∥4
2ATAT+W(k+1)
1W(k+1)
2xk+1xT
k+1W(k+1)T
2
=1
∥xk+1∥4
2ATAT+W(k+1)
1
∥xk+1∥2
2
0
...
0
/parenleftbig∥xk+1∥2
20... 0/parenrightbig
=ATAT+
| 0... 0
w(k+1)
1...... 0
| 0... 0

1 0... 0
0 0... 0
......... 0
0 0... 0

=ATAT+
| 0... 0
w(k+1)
1...... 0
| 0... 0

=ATAT+W(k+1)
1
=ATAT+(W(k)
1−αAT(AW(k)
1W(k)
2xk−b)xT
kW(k)T
2)
=ATAT+(ATvkxT
kW(k)T
2−αAT(AW(k)
1W(k)
2xk−b)xT
kW(k)T
2)
=ATvkxT
kW(k)T
2−αAT(AW(k)
1W(k)
2xk−b)xT
kW(k)T
2
=W(k)
1−αAT(AW(k)
1W(k)
2xk−b)xT
kW(k)T
2
=W(k+1)
1
Recall that Ahas full rank and less rows than columns, so we used the fact that AT+AT=In. As for
W(k+1)
2, the proof is similar. Denote:
uk+1 =1
∥xk+1∥2
2·∥AW(k+1)
1∥2
FAW(k+1)
1W(k+1)
2xk+1
remember that xkis never zero and notice that AW(k+1)
1 = (AAT)(vk+1xT
k+1W(k+1)T
2 )can’t be the zero
matrix because AATis full rank and only has the trivial solution. Now following a similar logic as before:
W(k+1)T
1 ATuk+1xT
k+1 =1
∥xk+1∥2
2·∥AW(k+1)
1∥2
FW(k+1)T
1 ATAW(k+1)
1W(k+1)
2xk+1xT
k+1
=1
∥xk+1∥2
2
1 0... 0
0 0... 0
............
0 0... 0
W(k+1)
2xk+1xT
k+1
=1
∥xk+1∥2
2(W(k+1)
2xk+1)xT
k+1
=
1
0
...
0
xT
k+1
=W(k+1)
2
This concludes the proofs for W1andW2. The claim of xk+1∈range/parenleftig
W(k+1)T
2 W(k+1)T
1 AT/parenrightig
follows the
same steps as in the proof of Lemma 9.
20Under review as submission to TMLR
Notice that we assumed in the previous theorem that xk̸= 0, and one reason for that assumption is that if
xk= 0then by Lemma 11 we have W(k)
2= 0as well, which leads to a saddle point (all gradients are zero)
and the iteration stops.
The following lemma extends Theorem 12 when k→∞.
Lemma 13. Forh= 2, consider the sequences {W(0)
1,W(1)
1,...},{W(0)
2,W(1)
2,...},{x0,x1,...}. If the
conditions of Theorem 12 are met and the sequences converge to W(∞)
1,W(∞)
2,x∞̸= 0respectively, then the
sequences{v1,v2,...},{u1,u2,...}defined by the result of Theorem 12 converge to v∞andu∞respectively,
and
W(∞)
1 =ATv∞xT
∞W(∞)
2T
W(∞)
2 =W(∞)
1TATu∞xT
∞
andx∞∈range/parenleftbigg
W(∞)
2TW(∞)
1TAT/parenrightbigg
Proof.Going by the definitions outlined in Theorem 12 we have
v∞= lim
k→∞vk= lim
k→∞1
∥xk∥4
2AT+W(k)
1W(k)
2xk=1
∥x∞∥4
2AT+W(∞)
1W(∞)
2x∞
and a similar logic for
u∞= lim
k→∞uk= lim
k→∞1
∥xk∥2
2·∥AW(k)
1∥2
FAW(k)
1W(k)
2xk=1
∥x∞∥2
2·∥AW(∞)
1∥2
FAW(∞)
1W(∞)
2x∞
Now we can simply multiply and see that
W(∞)
1 =ATv∞xT
∞W(∞)
2T
W(∞)
2 =W(∞)
1TATu∞xT
∞
as required. The proof for x∞follows the same steps as Lemma 9.
As expected, this initialization admits properties similar to those outlined in Corollary 7. The next corollary
describes the results of initializing in this special way and is the goal we built towards in this section.
Corollary 14. Denote W(∞)
1:=limk→∞W(k)
1,W(∞)
2:=limk→∞W(k)
2,x∞:=limk→∞xk. If the conditions
of Theorem 12 hold, the limits exist and are non-zero, and finally AW(∞)
1W(∞)
2x∞=b, then the following
statements are true:
1.W(∞)
1W(∞)
2x∞is the minimum norm solution to the problem Az=b
2.x∞is the minimum norm solution to the problem (AW(∞)
1W(∞)
2)z=b
3. vec (W(∞)
1)is the minimum norm solution to the problem (xT
∞W(∞)
2T⊗A)z=b
4. vec (W(∞)
2)is the minimum norm solution to the problem (xT
∞⊗AW(∞)
1)z=b
Proof.Use Lemma 13 and follow the same logic as Corollary 7
Another similarity to the h= 1linear model is that this can be collapsed to a more compact algorithm. We
do not need to iterate over W1andW2. Suppose that we know xkandvkat some iteration k. Then we can
construct
W(k)
2=
−xT
k−
0... 0
.........
0... 0

21Under review as submission to TMLR
trivially as we have shown from Lemma 11. This now allows us to compute W(k)
1=ATvkxT
kW(k)T
2.
Thus, if we wanted to stop at this iteration and produce a result, knowing xkandvkis all the information
we need. It is also all we need for the iteration step. We can write xk+1as follows:
xk+1 =xk−αW(k)T
2W(k)T
1AT(AW(k)
1W(k)
2xk−b)
=xk−αW(k)T
2W(k)
2xkvT
kAAT(AATvkxT
kW(k)T
2W(k)
2xk−b)
=xk−α
|0... 0
xk.........
|0... 0

∥xk∥2
2
0
...
0
vT
kAAT(∥xk∥4
2AATvk−b)
= (1−α∥xk∥2
2vT
kAAT(∥xk∥4
2AATvk−b))xk
and by using the iteration step for vk+1written in the proof of Theorem 12, we can write:
vk+1 =1
∥xk+1∥4
2AT+W(k+1)
1W(k+1)
2xk+1
=1
∥xk+1∥4
2AT+(W(k)
1−αAT(AW(k)
1W(k)
2xk−b)xT
kW(k)T
2)W(k+1)
2xk+1
=1
∥xk+1∥4
2AT+(ATvk/parenleftbig∥xk∥2
20... 0/parenrightbig
−αAT(∥xk∥4
2AATvk−b)/parenleftbig∥xk∥2
20... 0/parenrightbig
)
∥xk+1∥2
2
0
...
0

=1
∥xk+1∥2
2(vk−α(∥xk∥4
2AATvk−b))/parenleftbig
∥xk∥2
20... 0/parenrightbig
1
0
...
0

=∥xk∥2
2
∥xk+1∥2
2(vk−α(∥xk∥4
2AATvk−b))
So even for the iteration we just need xkandvk, and can iterate over them only, reducing the number of
parameters from 2d2+dtod+n, but just as before we can do better.
Notice that the iteration step for xkagain looks like
xk+1 = (1−γk)xk
=k/productdisplay
i=0(1−γi)x0
=ρk+1x0
whereγk=α∥xk∥2
2vT
kAAT(∥xk∥4
2AATvk−b)andρk=/producttextk−1
i=0(1−γi). We can use that ∥x0∥2= 1to rewrite
γkas
γk=αρ2
kvT
kAAT(ρ4
kAATvk−b)
We can use γkandρkto get a succinct and simple update step for vk:
vk+1 =∥xk∥2
2
∥xk+1∥2
2(vk−α(∥xk∥4
2AATvk−b))
=ρ2
k
ρ2
k+1(vk−α(ρ4
kAATvk−b))
=1
(1−γk)2(vk−α(ρ4
kAATvk−b))
22Under review as submission to TMLR
Figure 4.1: On rcv1 multiclass test set, each method with its largest learning rate in exponents of 10
(Algorithm 3 with α= 10−2, Algorithm 4 with α= 10−3, gradient descent with α= 10)
This allows us to effectively collapse a two hidden layers linear network to O(n)variables, much like we did
in the one hidden layer model. The algorithm is outlined below (Algorithm 4).
Algorithm 4 Compact two hidden layers iteration
A∈Rn×d,b∈Rn×1,α∈Rinputs
v0←arbitrary, not zero O(n)
ρ0←1 O(1)
z0←ATv0 O(TA)
foriterationk= 0,1,...until convergence do
yk←Azk O(TA)
ek←α(ρ4
kyk−b) O(n)
γk←ρ2
k(yT
kek) O(n)
vk+1←1
(1−γk)2(vk−ek) O(n)
ρk+1←ρk(1−γk) O(1)
zk+1←ATvk+1 O(TA)
end for
outputρ4
kzk
The time complexity of running Algorithm 4 for t>1iterations is O(t·max(n,TA). The similarities between
Algorithm 3 and Algorithm 4 are striking, but not entirely surprising.
We tested both these algorithms against the baseline gradient descent algorithm to answer two questions.
Can these two new algorithms outperform gradient descent and take different paths to θ⋆? To answer the
first question, we used the rcv1 multiclass test set, removed zero columns, and divided the feature matrix A
and the target vector bby52. We then trained three models using the methods mentioned above, and the
results in Figure 4.1 show that the new methods we propose are competitive and even beat gradient descent,
but begin to zigzag wildly after a certain amount of iterations.
Looking further into the matter, we see that Algorithm 3 begins to zigzag as soon as1
1−γk>1and Algorithm
4 begins to zigzag as soon as1
(1−γk)2>1, which both zigzag back and forth between a bit more than 1and a
bit less than 1. An illustration of this is shown in Figure 4.2.
23Under review as submission to TMLR
Figure 4.2: The new algorithms begin to zigzag when the corresponding coefficients zigzag between a bit
more and a bit less than 1.
Figure 4.3: Random projection of path. n= 100,d= 1000,condition number varies
As for the second question, the answer is a definite "No", as can be clearly seen in figure 4.3 where we solved
random 100by1000problems with specified condition numbers, and then projected the iteration path unto a
2d plane with a random projection to see if the two methods take the same path. They don’t take the same
path, Algorithm 4 seems to take a longer path, but it steps through that path more quickly as can be seen
empirically by the constraint on αin the experiments on the rcv1 dataset.
Is it possible to "collapse" deep models for h>2? We conjecture that no. We do not have a formal proof
but a heuristic argument. Consider, for example, the model when h= 3,AW 1W2W3x=b. As before, we
24Under review as submission to TMLR
would like W1=ATvxTWT
3WT
2,W2=WT
1ATuxTWT
3,W3=WT
2WT
1ATsxTfor someu,v,s. Using the
same strategy, we would have W1= (ATvxTWT
3)(ATuxTWT
3)TW1andW2= (WT
1ATu)(WT
1ATs)TW2
which means we need to choose u,v,ssuch that (ATvxTWT
3)T(ATuxTWT
3) = (WT
1ATu)T(WT
1ATs) = 1.
This does not seem feasible for weight matrices that are strongly coupled like that. It only worked in the
caseh= 2because the term for W1was without any mention of W2, but here there is seemingly no way to
decouple the weight matrices from each other. The key to solving the problem in the h= 2case does not
work in the h>2case, and there is no clear way of overcoming this problem. We do not claim the statement
is true, we leave it as an open problem. We only claim that the previous strategy does not work.
A final question is how our initialization methods compare to industry standard popular initializations.
Unfortunately, there is little relation, as our initializations, while random, are supported on a set of zero
measure. In contrast, most popular methods today sample scalar entries individually, and the support has a
positive measure (possibly even the entire space). Two prominent examples of industry standard initialization
are Xavier (Glorot & Bengio, 2010) and He Initializations (He et al., 2015).
In a Xavier Initialization we generate all entries from a uniform distribution on −1√sand1√s, wheresis
the number of neurons in the previous layer. The goal of this initialization, which is widely used for the
activation functions1
1+e−xandtanh(x), is to have constant variance across all layers. This prevents the
gradients from vanishing or exploding. He Initialization was invented to solve the problem that Xavier does
not work well when the activation function is ReLU. When performing He initialization, we generate numbers
from a normal distribution with mean 0and variance2
s.
Both of these initializations, and indeed most initialization techniques today, sample entries individually,
and so they miss the big picture of possible dependency on the data given and how to use it. They are
designed with optimization in mind, rather than generalization, and are very different from the methods we
propose. Initializing with these methods will almost surely not yield θ⋆and will not take advantage of the
collapsing property we have outlined. It is possible, however, that these initialization schemes avoid possible
exploding/vanishing gradient phenomena better than our proposed methods.
4.2 Stability analysis of deep linear networks
We have shown in Lemma 8 that if W(0)
1∈range/parenleftig
AT/parenrightig
andAW(∞)
1...W(∞)
hx∞=bthen the limit
W(∞)
1...W(∞)
hx∞exists and equals θ⋆. However, it is not always easy to achieve this perfectly, and due to
machine precision or other reasons we might have W(0)
1/∈range/parenleftig
AT/parenrightig
. Thus, a natural question to ask is
what would happen if W(0)
1/∈range/parenleftig
AT/parenrightig
, but is close to range/parenleftig
AT/parenrightig
in some sense. We formalize this
question by first writing
W(k)
1 =ATPk+Ck
where
Pk=(AT)+W(k)
1
Ck=W(k)
1−ATPk.
and we assume C0̸= 0. First, notice that ACk= 0is retained throughout our iterations. This is because
ACk=AW(k)
1−AAT(AAT)−1AW(k)
1
=AW(k)
1−AW(k)
1
= 0
25Under review as submission to TMLR
We can use this to arrive at the conclusion that Cknever changes, as
Ck+1 =W(k+1)
1−ATPk+1
=W(k+1)
1−ATAT+W(k+1)
1
=W(k)
1−AT(AW(k)
1xk−b)xT
k−ATAT+(W(k)
1−AT(AW(k)
1xk−b)xT
k)
=W(k)
1−AT(AW(k)
1xk−b)xT
k−ATAT+W(k)
1+AT(AW(k)
1xk−b)xT
k
=W(k)
1−ATAT+W(k)
1
=ATPk+Ck−ATAT+(ATPk+Ck)
=Ck−ATAT+Ck
=Ck−AT(AAT)−1ACk
=Ck−AT(AAT)−1·0
=Ck
Thus, we instead write W(k)
1=ATPk+Cwhere Cis constant and only Pkis being iterated upon.
A second observation is that if all limits are assumed to exist and AW(∞)
1W(∞)
2...W(∞)
hx∞=b, then
ATP∞W(∞)
2...W(∞)
hx∞=θ⋆. An easy way to see this is that
AW(∞)
1W(∞)
2...W(∞)
hx∞=AATP∞W(∞)
2...W(∞)
hx∞+ 0
=b
soATP∞W(∞)
2...W(∞)
hx∞is a solution and it is trivially in range/parenleftig
AT/parenrightig
so it is equal to θ⋆by Lemma 1.
Now observe that,
∥W(∞)
1W(∞)
2...W(∞)
hx∞−θ⋆∥2=∥ATP∞W(∞)
2...W(∞)
hx∞+CW(∞)
2...W(∞)
hx∞−θ⋆∥2
=∥CW(∞)
2...W(∞)
hx∞∥2
≤ ∥W(∞)
2∥...∥W(∞)
h∥·∥x∞∥2·∥C∥
We again see the importance of initialization on the constant ∥C∥. Can depth fix this constant however? The
inequality suggests that if his large and the weight norms are smaller than 1at convergence, then this fixes
large∥C∥. Conversely, if the norms are greater than 1, the bound explodes and a small perturbation during
initialization can result in radically different solutions. We tested this empirically on randomly generated
problems to see if depth helps. We created a linear neural network of varying depth, with W1=ATP0+C
where∥C∥= 1and tested whether depth helps or harms the distance to θ⋆. The other initial weights were
allIdexcept x0=random (Sd−1).
Quite surprisingly, we see that the product ∥W2∥...∥Wh∥∥x∥2increases as the depth increases, but the
distance to θ⋆could decrease nonetheless. It could increase, decrease, or be non-monotonic (see Figure 4.4).
In every experiment, the norm product always increased with depth. In the vast majority of experiments, the
distance to θ⋆increased monotonically with depth, signaling that depth causes the error to explode and does
not help with generalization.
26Under review as submission to TMLR
Figure 4.4: Norm product and distance to θ⋆at the end of training, with varying depths and seeds, exhibiting
different properties regarding distance to θ⋆
5 Riemannian Linear Neural Networks
In this section, we consider a deep linear model Ay=bwhere yis parameterized as y:=W1W2...Whx
where
W1,W2,...,Wh∈Stiefel (d,d) :={W∈Rd×d:WWT=Id}
andxremains unconstrained. The motivation for this model is clear from the previous section. The inequality
in Section 4.2 tempts us to enforce that ∥Wk∥= 1and then∥W1W2...Whx−θ⋆∥2≤∥x∥2·∥C∥, which
if∥x∥2is not large, hopefully fixes the damage by a poor initialization, or at the very least does not harm
it like deep linear networks might. This model makes it so that adding more layers does not increase the
upper bound on the error, which can often happen in regular deep linear networks, as shown in the figures in
Section 4.2, where in every model adding layers increased the product of norms (an upper bound). However,
we shall see that while the product of hidden weight norms is constant, depth in a Riemannian model can
have both a positive and negative effect, and results are inconclusive.
5.1 Brief Informal Background on Riemannian Optimization
This explanation, while simplistic and informal, is meant to convey the essential notion rather than to provide
a detailed and formal account of Riemannian optimization. Additional, formalized and detailed information
is provided by Absil et al. (2008); Boumal (2022).
Suppose we wish to find a vector x∈Sd−1that minimizes the function ∥Az−b∥2
2where∥z∥2= 1, like we
would encounter in Lagrange Multipliers for instance. Neural networks (whether linear or not) do not allow
us to specify which domain we want our weights to be in. It does not allow us to constrain them. But in
real-world applications, we often want to constrain the parameters. For instance, we might have a problem
where we are looking for the correct orientation of an object in space, thus our search domain is only rotation
matrices, which is not a linear space, but it is a smooth manifold that is locally linearizable at every point.
Back to our problem of minimizing f(z) =∥Az−b∥2
2over the unit sphere. The unit sphere is not a linear
space, so we cannot define an inner product on it, and as such there is no notion of gradient. However, it is
locally linearizable at every point. We can find the tangent space TzSd−1at every point z, choose an inner
product for it (there are many choices; conceptually, this is not far from preconditioning); an obvious choice
27Under review as submission to TMLR
is the standard inner product inherited from the Euclidean space Rd. This tangent space is now a linear
space endowed with an inner product, so we can now have a clear notion about the gradients in it.
The gradient of f(z)will not, in general, be in TzSd−1, so we will define the Riemannian gradient as the
vectorrgradf(z)which is the unique vector in TzSd−1such that⟨rgradf(z),v⟩=Df(x)vfor all vinTzSd−1,
where Df(x)v:=limδ→0f(x+δv)−f(x)
δ. As a consequence of this definition, we can easily calculate it with
rgradf(z) =Projz(∇f(z))whereProjzis the orthogonal projection operator from Rdto the tangent space
TzSd−1.
We now have z−rgradf(z)be inTzSd−1, but it is not on Sd−1. What we need is a mapping from the
tanget space onto the manifold. Such a mapping is called a retraction, and for this case an example is the
normalizing function. Now we can define a Riemannian version of gradient descent: move in the direction
opposite the Riemannian gradient and retract back to the manifold. This procedure allows us to optimize
functions over smooth non-linear manifolds, and not all Rd. This is also a form of regularization, as we can
choose "simple" manifolds and, we hope, get "simple" solutions.
This procedure for optimizing over the manifold Sd−1can be extended to any manifold we wish. All we need
is the tangent space at every point on the manifold, an inner product on that tangent space, the orthogonal
projection operator onto that tangent space, and a retraction. In Section 5.2 we consider Riemannian
optimization where our target manifold is the product of Stiefel manifolds (orthogonal matrices).
5.2 The Role of Initialization in Riemannian Linear Neural Networks
In this section we consider the problem of solving AWx=bwhere Wis either on the Stiefel manifold,
or overparametrized as a product of such matrices, and the effects of initialization on this problem. We
begin with a definition. The Frobenius distance of an orthogonal d×dmatrix Wfrom the range of a d×n
full-rank matrix Misdrange (M)(W) :=∥MM+W−W∥F. This definition is sensible because, indeed, M+W
minimizes∥MX−W∥Ffrom the properties of Moore-Penrose pseudoinverse. This definition motivates the
following theorem. This theorem is not specifically related to our use cases and models, but we use it to show
that we cannot initialize like in the previous sections, which is a key difference to the previous models.
Theorem 15. LetW∈Rd×dbe an orthogonal matrix and M∈Rd×nbe of full rank. Then drange (M)(W) =√
d−n.
Proof.The closest matrix to Winrange (M)is
Z=MM+W
=M(MTM)−1MTW.
All we need to do is calculate the distance between WandZ.
∥Z−W∥2
F=∥M(MTM)−1MTW−W∥2
F=∥(M(MTM)−1MT−Id)W∥2
F=∥M(MTM)−1MT−Id∥2
F
since Wis orthogonal.
Now
∥M(MTM)−1MT−Id∥2
F=trace [(M(MTM)−1MT−Id)T(M(MTM)−1MT−Id)]
=trace [M(MTM)−1MTM(MTM)−1MT−2M(MTM)−1MT+Id]
=trace [M(MTM)−1MT−2M(MTM)−1MT] +d
=d−trace [M(MTM)−1MT] =d−trace [MTM(MTM)−1] =d−trace [In]
=d−n
A consequence of the previous theorem is that it is impossible for us to have W(k)
1∈range/parenleftig
AT/parenrightig
in the
orthogonal network case. The theorem shows that if W(k)
1is orthogonal and W(k)
1=AT+Pk+Ck, then we
28Under review as submission to TMLR
always have∥Ck∥F=√
d−n, Hence Ck̸= 0for allk. While we have not shown that Ckis conserved like in
Section 4.2 (in fact, it is not conserved), but this proves that ∥Ck∥=√
d−nis conserved across iterations.
It also shows that we cannot ever have W(k)
1∈range/parenleftig
AT/parenrightig
. Thus, we do not necessarily find the minimum
norm solution!
W(k)
1may never be in range/parenleftig
AT/parenrightig
but we could still have W(∞)
1W(∞)
2...W(∞)
hx∞∈range/parenleftig
AT/parenrightig
. We
wanted to check whether this happens empirically in orthogonal linear networks, so we tested several problems
with pymanopt (Townsend et al., 2016), with the default QR retraction and no line search to keep things as
simple as possible (although this did not seem to have an effect regardless).
Figures 5.1, 5.2 are two examples of such experiments, where we solved the same problem (outlined below)
using deep orthogonal linear networks with two different initializations, and Figure 5.3 was entirely a different
problem. The goal of these experiments was to check whether depth helps us or not in orthogonal linear
networks and whether we converge to the minimum norm solution, perhaps regardless of initialization. We
clarify that in Figures 5.1, 5.2 both experiments solved the same problem
A=/parenleftbigg5−3 1
3 1−1/parenrightbigg
b=/parenleftbigg6
4/parenrightbigg
.
In all experiments the hidden weights were optimized on the d×dStiefel manifold while the outmost layer
was unconstrained. The only difference between Figures 5.1, 5.2 was the seed that governed the initialization.
We immediately see that the paths may diverge with depth. This disagrees with (Ablin, 2020), which states
that deep orthogonal networks are shallow and that depth has no effect. Ablin (2020)ś result holds only in
the matrix factorization case (that is, trying to decompose a given (square) matrix as a product of orthogonal
matrices, and to do so, they initialize the weights, all square matrices, to be orthogonal and strictly optimize
on Stiefel manifolds). This is unlike our setting, which is not matrix factorization, but rather regression,
where the outermost layer is a vector optimized on Rdand is unconstrained, only the inner layers have the
orthogonality constraint. These are two separate problems which are related in the sense that they are both
linear models where the weights are simply multiplied together but are in essence distinct in dimension of the
objective and the constraints on the parameters. Hence, the trajectories and biases are different as well, as
empirically shown in this work.
We also see in Figure 5.2 that even though we had a random initialization, as we have no choice on that
with orthogonal networks because of Theorem 15, we still converged to θ⋆. This is very interesting. In a
regular deep linear network with random initialization, the odds of converging to θ⋆are very low, we have
never encountered that happening randomly, but for orthogonal networks it happens quite frequently. This is
mysterious and we haven’t managed yet to find a convincing argument as to why this is the case.
We also see that depth can have both a positive and a negative effect, as seen in Figure 5.1 showing that
depth brings us closer to θ⋆while in Figure 5.3 depths displace us further from θ⋆
The experiments disprove the idea that the distance to θ⋆is related to the norms of the individual weights,
as indicated in Section 4.2. The inequality is very lenient, and depth does not fix a bad initialization, not
even in the orthogonal case, as illustrated in Figure 5.3, where depth even makes us farther away from θ⋆.
To further assess the behavior of Riemannian networks optimized on the product of Stiefel manifolds, we
have conducted 10000 trials with random initializations on the above linear system of equations, with the
goal of exploring whether statistically depth helps or harms the distance to the minimum norm solution.
Convergence to θ⋆is not guaranteed, and while it is interesting to explore when it convergence to θ⋆happens,
it is also useful to ask whether depth helps when it does not happen? Figures 5.4 and 5.5 aim to answer this
question.
In Figure 5.4 we draw the histogram of the distance from θ⋆for depths h= 1,3,6and see that as depth
increases, the histograms become more centered to the left (smaller error) and also more tightly clustered
(smaller variance). This indicates that while all options are possible, statistically when solving the above
problem, depth helps us. In Figure 5.5 we plot the 25th, 50th and 75th percentile distances for each respective
29Under review as submission to TMLR
Figure 5.1: Deep orthogonal linear network, seed = 5. Solving/parenleftbigg5−3 1
3 1−1/parenrightbigg
x=/parenleftbigg6
4/parenrightbigg
h, and the shaded area represents one variance. We observe that percentile distances decay and the shaded
areas become thinner as hincreases, indicating that statistically, depth is beneficial.
30Under review as submission to TMLR
Figure 5.2: Deep orthogonal linear network, seed = 351. Solving/parenleftbigg5−3 1
3 1−1/parenrightbigg
x=/parenleftbigg6
4/parenrightbigg
Figure 5.3: Deep orthogonal linear network, seed = 12. Solving a randomly generated problem.
31Under review as submission to TMLR
Figure 5.4: Histogram depicting amount of experiments vs distance to θ⋆observed for different hvalues in
Riemannian setting
Figure 5.5: Different percentile log distance to θ⋆for different hvalues, shaded area is possible values seen in
experiments.
32Under review as submission to TMLR
6 Conclusions
We hope that this work clearly illustrates the pivotal role of initialization in deep learning. For linear
networks, when we can control the initialization freely, we have clear advantages of choosing where to converge
(Theorem 4, Corollary 7, Corollary 14), and we can collapse the problem from a high-dimensional problem to
an equivalent problem with low dimensions (Algorithms 3 and 4). We can ensure convergence to an optimal
solution (since we converge to a solution rather than a saddle point) and give a very rough error bound if we
can not initialize exactly where we wish (Section 4.2). We saw that where we cannot control the initialization
(Section 5), the best we can do is hope to converge to a good solution, and depth often will not fix bad
initializations.
The implicit bias determined by initialization is a key question to solve in deep neural networks, and in our
work, we attempted to convey the importance of this seemingly innocent part of any parametric method, but
there is more work to be done. The new algorithms we propose (Algorithms 3 and 4) need to be looked at
further and given bounds on rate of convergence, and any other advantages these methods may have that we
hope will come to light. Specifically, we believe that there may be advantages to data-based initializations and
possibly other initializations apart from ours that take advantage of the data given to reach a desired bias,
but that is something that needs to be carefully and thoroughly researched further, as the industry standard
currently is simple random initialization that does not depend on the data. It is also very tempting to show
under which circumstances an orthogonal linear network will converge to the minimum-norm solution, as we
saw that it happens quite frequently, which is very surprising. A natural next step will be to try generalize
our work to the nonlinear case and prove a criterion that will assure convergence to the least norm solution
(or a low norm solution) in ordinary deep networks. The issue of extending our work to linear networks
of depth greater than h= 2is another matter that requires resolution - a general method for collapsing
deep linear networks, or proof that such a method does not exist when h>2. Finally, we hope to find an
explanation and perhaps a fix for the zigzag phenomenon that we see in Figure 4.1 that would make the new
algorithms even better, and test that solution on non-linear networks, which is the main motivation since it
is unlikely the new algorithms will be better than modern methods for linear regression like Krylov subspace
solutions.
References
Pierre Ablin. Deep orthogonal linear networks are shallow, 2020.
P.-A. Absil, R. Mahony, and R. Sepulchre. Optimization Algorithms on Matrix Manifolds . Princeton University
Press, Princeton, NJ, 2008. ISBN 978-0-691-13298-3.
Sanjeev Arora, Nadav Cohen, and Elad Hazan. On the optimization of deep networks: Implicit acceleration
by overparameterization. In International Conference on Machine Learning , pp. 244–253. PMLR, 2018.
Sanjeev Arora, Nadav Cohen, Wei Hu, and Yuping Luo. Implicit regularization in deep matrix factorization. In
H.Wallach, H.Larochelle, A.Beygelzimer, F.d 'Alché-Buc, E.Fox, andR.Garnett(eds.), Advances in Neural
Information Processing Systems , volume 32. Curran Associates, Inc., 2019. URL https://proceedings.
neurips.cc/paper/2019/file/c0c783b5fc0d7d808f1d14a6e9c8280d-Paper.pdf .
Bubacarr Bah, Holger Rauhut, Ulrich Terstiege, and Michael Westdickenberg. Learning deep linear neural
networks: Riemannian gradient flows and convergence to global minimizers. Information and Inference:
A Journal of the IMA , 11(1):307–353, 02 2021. ISSN 2049-8772. doi: 10.1093/imaiai/iaaa039. URL
https://doi.org/10.1093/imaiai/iaaa039 .
Peter L Bartlett, Philip M Long, Gábor Lugosi, and Alexander Tsigler. Benign overfitting in linear regression.
Proceedings of the National Academy of Sciences , 117(48):30063–30070, 2020.
Mikhail Belkin, Daniel Hsu, Siyuan Ma, and Soumik Mandal. Reconciling modern machine-learning practice
and the classical bias–variance trade-off. Proceedings of the National Academy of Sciences , 116(32):15849–
15854, 2019. ISSN 0027-8424. doi: 10.1073/pnas.1903070116. URL https://www.pnas.org/content/
116/32/15849 .
33Under review as submission to TMLR
Nicolas Boumal. An introduction to optimization on smooth manifolds. To appear with Cambridge University
Press, Jun 2022. URL https://www.nicolasboumal.net/book .
Jonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Finding sparse, trainable neural
networks. In ICLR. OpenReview.net, 2019. URL http://dblp.uni-trier.de/db/conf/iclr/
iclr2019.html#FrankleC19 .
Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural networks.
In Yee Whye Teh and Mike Titterington (eds.), Proceedings of the Thirteenth International Conference on
Artificial Intelligence and Statistics , volume 9 of Proceedings of Machine Learning Research , pp. 249–256,
Chia Laguna Resort, Sardinia, Italy, 13–15 May 2010. PMLR. URL https://proceedings.mlr.
press/v9/glorot10a.html .
Suriya Gunasekar, Blake E Woodworth, Srinadh Bhojanapalli, Behnam Neyshabur, and Nati Srebro.
Implicit regularization in matrix factorization. In I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach,
R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Advances in Neural Information Processing Systems ,
volume 30. Curran Associates, Inc., 2017. URL https://proceedings.neurips.cc/paper/2017/
file/58191d2a914c6dae66371c9dcdc91b41-Paper.pdf .
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing human-
level performance on imagenet classification. In Proceedings of the IEEE international conference on
computer vision , pp. 1026–1034, 2015.
Wei Hu, Lechao Xiao, and Jeffrey Pennington. Provable benefit of orthogonal initialization in optimizing
deep linear networks, 2020. URL https://arxiv.org/abs/2001.05992 .
Jaehoon Lee, Jascha Sohl-dickstein, Jeffrey Pennington, Roman Novak, Sam Schoenholz, and Yasaman Bahri.
Deep neural networks as gaussian processes. In International Conference on Learning Representations ,
2018. URL https://openreview.net/forum?id=B1EA-M-0Z .
Chaoyue Liu, Libin Zhu, and Misha Belkin. Transition to linearity of wide neural networks is an emerging
property of assembling weak models. In International Conference on Learning Representations , 2022. URL
https://openreview.net/forum?id=CyKHoKyvgnp .
Noam Razin and Nadav Cohen. Implicit regularization in deep learning may not be
explainable by norms. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and
H. Lin (eds.), Advances in Neural Information Processing Systems , volume 33, pp. 21174–21187.
Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/file/
f21e255f89e0f258accbe4e984eef486-Paper.pdf .
Terrence J. Sejnowski. The unreasonable effectiveness of deep learning in artificial intelligence. Proceedings
of the National Academy of Sciences , 117(48):30033–30038, 2020. doi: 10.1073/pnas.1907373117. URL
https://www.pnas.org/doi/abs/10.1073/pnas.1907373117 .
James Townsend, Niklas Koep, and Sebastian Weichwald. Pymanopt: A python toolbox for optimization
on manifolds using automatic differentiation. J. Mach. Learn. Res. , 17(1):4755â4759, jan 2016. ISSN
1532-4435.
34