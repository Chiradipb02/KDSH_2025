Published in Transactions on Machine Learning Research (01/2025)
A general framework of Riemannian adaptive optimization
methods with a convergence analysis
Hiroyuki Sakai sakai0815@cs.meiji.ac.jp
Meiji University
Hideaki Iiduka iiduka@cs.meiji.ac.jp
Meiji University
Reviewed on OpenReview: https: // openreview. net/ forum? id= knv4lQFVoE
Abstract
This paper proposes a general framework of Riemannian adaptive optimization methods.
TheframeworkencapsulatesseveralstochasticoptimizationalgorithmsonRiemannianman-
ifolds and incorporates the mini-batch strategy that is often used in deep learning. Within
this framework, we also propose AMSGrad on embedded submanifolds of Euclidean space.
Moreover, wegiveconvergenceanalysesvalidforbothaconstantandadiminishingstepsize.
Our analyses also reveal the relationship between the convergence rate and mini-batch size.
In numerical experiments, we applied the proposed algorithm to principal component analy-
sis and the low-rank matrix completion problem, which can be considered to be Riemannian
optimization problems. Python implementations of the methods used in the numerical ex-
periments are available at https://github.com/iiduka-researches/202408-adaptive .
1 Introduction
Riemannian optimization (Absil et al., 2008; Sato, 2021) has received much attention in machine learning.
For example, batch normalization (Cho & Lee, 2017), representation learning (Nickel & Kiela, 2017), and the
low-rank matrix completion problem (Vandereycken, 2013; Cambier & Absil, 2016; Boumal & Absil, 2015)
can be considered optimization problems on Riemannian manifolds. This paper focuses on Riemannian
adaptive optimization algorithms for solving stochastic optimization problems on Riemannian manifolds. In
particular, wetreatRiemanniansubmanifoldsofEuclideanspace(e.g., unitspheresandtheStiefelmanifold).
In Euclidean settings, adaptive optimization methods are widely used for training deep neural networks.
There are many adaptive optimization methods, such as Adaptive gradient (AdaGrad) (Duchi et al., 2011),
Adadelta (Zeiler, 2012), Root mean square propagation (RMSProp) (Hinton et al., 2012), Adaptive mo-
ment estimation (Adam) (Kingma & Ba, 2015), Yogi (Zaheer et al., 2018), Adaptive mean square gradient
(AMSGrad) (Reddi et al., 2018), AdaFom (Chen et al., 2019), AdaBound (Luo et al., 2019), Adam with
decoupled weight decay (AdamW) (Loshchilov & Hutter, 2019) and AdaBelief (Zhuang et al., 2020). Reddi
et al. (2018) proposed a general framework of adaptive optimization methods that encapsulates many of the
popular adaptive methods in Euclidean space.
Bonnabel (2013) proposed Riemannian stochastic gradient descent (RSGD), the most basic Riemannian
stochastic optimization algorithm. In particular, Riemannian stochastic variance reduction algorithms, such
as Riemannian stochastic variance-reduced gradient (RSVRG) (Zhang et al., 2016), Riemannian stochastic
recursive gradient (RSRG) (Kasai et al., 2018), and Riemannian stochastic path-integrated differential es-
timator (R-SPIDER) (Zhang et al., 2018; Zhou et al., 2019), are based on variance reduction methods in
Euclidean space. There are several prior studies on Riemannian adaptive optimization methods for spe-
cific Riemannian manifolds. In particular, Kasai et al. (2019) proposed a Riemannian adaptive stochastic
gradient algorithm on matrix manifolds (RASA). RASA is an adaptive optimization method on matrix
1Published in Transactions on Machine Learning Research (01/2025)
manifolds (e.g., the Stiefel manifold or the Grassmann manifold), and gave a convergence analysis under
the upper-Hessian bounded and retraction L-smooth assumptions (see (Kasai et al., 2019, Section 4) for
details). However, RASA is not a direct extension of the adaptive optimization methods commonly used
in deep learning, and it works only for diminishing step sizes. RAMSGrad (Bécigneul & Ganea, 2019) and
modified RAMSGrad (Sakai & Iiduka, 2021), direct extensions of AMSGrad, have been proposed as meth-
ods that work on Cartesian products of Riemannian manifolds. In particular, Roy et al. (2018) proposed
cRAMSProp and applied it to several Riemannian stochastic optimizations. However, they did not provide
a convergence analysis of it. More recently, Riemannian stochastic optimization methods, sharpness-aware
minimization on Riemannian manifolds (Riemannian SAM) (Yun & Yang, 2024) and Riemannian natural
gradient descent (RNGD) (Hu et al., 2024), were proposed.
1.1 Motivations
Kasai et al. (2019) presented useful results indicating RASA with a diminishing step size converges to a
stationary point of a continuously differentiable function defined on a matrix manifold. Meanwhile, other
results have indicated that using constant step sizes makes the algorithm useful for training deep neural
networks (Zaheer et al., 2018) and that the setting of the mini-batch size depends on the performance
of the algorithms used to train deep neural networks (Smith et al., 2018; Sato & Iiduka, 2023). The main
motivation of this paper is thus to gain an understanding of the theoretical performance relationship between
Riemannian adaptive optimization methods using constant step sizes and ones using the mini-batch size.
We are also interested in understanding the theoretical performance relationship between the Riemannian
adaptive optimization methods using diminishing step sizes and those using the mini-batch size.
1.2 Contributions
Our first contribution is to propose a framework of adaptive optimization methods on Riemannian subman-
ifolds of Euclidean space (Algorithm 1) that is based on the framework (Reddi et al., 2018, Algorithm 1)
proposed by Reddi, Kale and Kumar for Euclidean space. A key challenge in designing adaptive gradient
methods over Riemannian manifolds is the absence of a coordinate system like that in Euclidean space.
In this work, we specifically focus on manifolds that are embedded in Euclidean space, a common setting
in many applications. We address the issue by constructing algorithms that rely on projections onto the
tangent space at each point on the manifold. Our framework incorporates the mini-batch strategy that
is often used in deep learning. Important examples of Riemannian submanifolds of the Euclidean space
include the unit sphere and the Stiefel manifold. Note that (Kasai et al., 2019) focuses exclusively on the
RASA algorithm, which is defined only on matrix manifolds. In contrast, our framework can be defined
on any manifold embedded in Euclidean space. Moreover, within this framework, we propose AMSGrad on
embedded submanifolds of Euclidean space (Algorithm 2) as a direct extension of AMSGrad.
Our second contribution is to give convergence analyses of the proposed algorithm (Algorithm 1) by using
a mini-batch size bvalid for both a constant step size (Theorem 3.5) and diminishing step size (Theorem
3.6). In particular, Theorem 3.5 indicates that, under some conditions, the sequence (xk)∞
k=1generated by
the proposed algorithm with a constant step size αsatisfies
min
k=1,...,KE/bracketleftig
∥gradf(xk)∥2
2/bracketrightig
≤1
KK/summationdisplay
k=1E/bracketleftig
∥gradf(xk)∥2
2/bracketrightig
=O/parenleftbigg1
K+1
b/parenrightbigg
.
We should note that Theorem 3.5 is an extension of the result in Zaheer et al. (2018, Corollary 2). Theorem
3.6 indicates that, under some conditions, the sequence (xk)∞
k=1generated by the proposed algorithm with
a diminishing step size αksatisfies
min
k=1,...,KE/bracketleftig
∥gradf(xk)∥2
2/bracketrightig
≤1
KK/summationdisplay
k=1E/bracketleftig
∥gradf(xk)∥2
2/bracketrightig
=O/parenleftbigg/parenleftbigg
1 +1
b/parenrightbigglogK√
K/parenrightbigg
.
Since the stochastic gradient (see (1)) using large mini-batch sizes is approximately the full gradient of f;
intuitively, using large mini-batch sizes decreases the number of steps needed to find stationary points of
2Published in Transactions on Machine Learning Research (01/2025)
f. Theorems 3.5 and 3.6 indicate that, the larger the mini-batch size bis, the smaller the upper bound of
1
K/summationtextK
k=1E[∥gradf(xk)∥2
2]becomes and the fewer the required steps become. Hence, Theorems 3.5 and 3.6
would match our intuition.
Theorem 3.5 indicates that, although the proposed algorithm with a constant step size αand constant mini-
batch sizebdoes not always converge, we can expect that converges with an increasing mini-batch size bk
such thatbk≤bk+1. In practice, it has been shown that increasing the mini-batch size (Byrd et al., 2012;
Balles et al., 2017; De et al., 2017; Smith et al., 2018; Goyal et al., 2018) is useful for training deep neural
networks with mini-batch optimizers. Motivated by Theorems 3.5 and 3.6, we give convergence analyses
of the proposed algorithm by using an increasing mini-batch size bkvalid for both a constant step size
(Theorem 3.7) and diminishing step size (Theorem 3.8). Theorem 3.7 indicates that, under some conditions,
the sequence (xk)∞
k=1generated by the proposed algorithm with a constant step size αand an increasing
mini-batch size bksatisfies
min
k=1,...,KE/bracketleftig
∥gradf(xk)∥2
2/bracketrightig
≤1
KK/summationdisplay
k=1E/bracketleftig
∥gradf(xk)∥2
2/bracketrightig
=O/parenleftbigg1
K/parenrightbigg
.
That is, with increasing mini-batch sizes, it can find a stationary point of f, in contrast to Theorem 3.5.
Moreover, Theorem 3.8 indicates that, under some conditions, the sequence (xk)∞
k=1generated by the pro-
posed algorithm with a diminishing step size αkand an increasing mini-batch size bksatisfies
min
k=1,...,KE/bracketleftig
∥gradf(xk)∥2
2/bracketrightig
≤1/summationtextK
k=1αkK/summationdisplay
k=1αkE/bracketleftig
∥gradf(xk)∥2
2/bracketrightig
=O/parenleftbigg1√
K/parenrightbigg
.
That is, with increasing mini-batch sizes, it has a better convergence rate than with constant mini-batch
sizes (Theorem 3.6).
The third contribution is to numerically compare the performances of several methods based on Algorithm 1,
includingAlgorithm2, withtheexistingmethods. Inthenumericalexperiments, weappliedthealgorithmsto
principalcomponentanalysis(PCA)(Kasaietal.,2018;Royetal.,2018)andthelow-rankmatrixcompletion
(LRMC) problem (Boumal & Absil, 2015; Kasai et al., 2019; Hu et al., 2024), which can be considered to be
Riemannian optimization problems. Based on our experiments, we can summarize the following key findings.
For the PCA problem, RAMSGrad with a constant step size minimizes the objective function in fewer or
comparable iterations compared with RSGD, RASA-L, RASA-R and RASA-LR, regardless of the dataset.
For the LRMC problem, RAdam with a diminishing step size minimizes the objective function in fewer
or about the same number of iterations compared with RSGD, RASA-L and RASA-R, irrespective of the
dataset. As reported in Sakai & Iiduka (2021), RAMSGrad demonstrates robust performance regardless of
the initial step size. For the PCA and LRMC problems (where LRMC adopts a diminishing step size), a grid
search was conducted, allowing the selection of an initial step size that yielded even better results. This likely
contributed to the strong performance of RAMSGrad across datasets. However, for the LRMC problem,
RAMSGrad may underperform relative to RASA, depending on the dataset. It is worth noting that the
objective function in the LRMC problem is complex, and assumptions such as retraction L-smoothness may
not hold in this case. This could explain the less favorable results observed for RAMSGrad in this particular
setting. Moreover, to support our theoretical analyses, we show that increasing the batch size decreases the
number of steps needed to a certain index. In particular, Table 1 (resp. Table 2) supports the result shown
in Theorem 3.5, demonstrating that the proposed algorithms with a constant (resp. diminishing) step size
require fewer iterations to reduce the gradient norm below the threshold as the batch size increases. Tables 1
and 2 confirm that larger batch sizes lead to faster convergence in terms of the number of iterations, aligning
with our theoretical analysis. Similar tables (Tables 11–16) for the remaining datasets and experiments can
befoundinAppendixE.Notethatthechoiceofbatchsizeplaysacriticalroleinbalancingthecomputational
cost per iteration and the total number of iterations required for convergence. This is related to the concept
of the critical batch size, as discussed in Sakai & Iiduka (2024), which minimizes a stochastic first-order
oracle (SFO). However, estimating this optimal batch size is challenging due to the dependency on unknown
parameters such as the gradient Lipschitz constant and the variance of the stochastic gradient. To address
this challenge, an effective approach may involve dynamically increasing the batch size at regular intervals
3Published in Transactions on Machine Learning Research (01/2025)
during optimization, such as doubling or tripling it after a fixed number of iterations. This method has
theoretical guarantees for convergence (see Theorems 3.7 and 3.8) and can adaptively approach an efficient
balance between computational cost and convergence speed.
2 Mathematical preliminaries
LetRdbe ad-dimensional Euclidean space with inner product ⟨x,y⟩2:=x⊤y, which induces the norm ∥·∥2.
LetR++be the set of positive real numbers, i.e., R++:={x∈R|x>0}.Iddenotes ad×didentity matrix.
For square matrices X,Y∈Rd×d, we writeX≺Y(resp.X⪯Y) ifY−Xis a positive-definite (resp.
positive-semidefinite)matrix. Fortwomatrices XandYofthesamedimension, X⊙YdenotestheHadamard
product, i.e., element-wise product. Let max(X,Y )be the element-wise maximum. Let Sd(resp.Sd
+,Sd
++)
be the set of d×dsymmetric (resp. symmetric positive-semidefinite, symmetric positive-definite) matrices,
i.e.,Sd:={X∈Rd×d|X⊤=X},Sd
+:={X∈Rd×d|X⪰O}andSd
++:={X∈Rd×d|X≻O}. LetDd
be the set of d×ddiagonal matrices. Let Odbe the orthogonal group, i.e., Od:={X∈Rd×d|X⊤X=Id}.
We denote the trace of a square matrix Abytr(A). LetObe Landau’s symbol; i.e., for a sequence (xk)∞
k=0,
we writexk=O(g(k))ask→∞if there exist a constant C > 0and an index k0such that|xk|≤C|g(k)|
for allk≥k0.
In this paper, we focus on Riemannian manifolds (Sakai, 1996) embedded in Euclidean space. Let Mbe an
embedded submanifold of Rd. Moreover, let TxMbe the tangent space at a point x∈MandTMbe the
tangent bundle of M. Let 0xbe the zero element of TxM. The inner product ⟨·,·⟩2of a Euclidean space Rd
induces a Riemannian metric ⟨·,·⟩xofMatx∈Maccording to⟨ξ,η⟩x=⟨ξ,η⟩2=ξ⊤ηforξ,η∈TxM⊂
TxRd∼=Rd. The norm of η∈TxMis defined as∥η∥x=/radicalbig
η⊤η=∥η∥2. LetPx:TxRd∼=Rd→TxM
be the orthogonal projection onto TxM(see Absil et al. (2008)). For a smooth map F:M→Nbetween
two manifolds MandN,DF(x) :TxM→TF(x)Ndenotes the derivative of Fatx∈M. The Riemannian
gradient gradf(x)of a smooth function f:M→Ratx∈Mis defined as a unique tangent vector at x
satisfying⟨gradf(x),η⟩x= Df(x)[η]for anyη∈TxM.
Definition 2.1 (Retraction) .LetMbe a manifold. Any smooth map R:TM→Mis called a retraction
onMif it has the following properties.
•Rx(0x) =xfor allx∈M;
•With the canonical identification T0xTxM∼=TxM,DRx(0x) = id TxM:TxM→TxMfor allx∈M,
whereRxdenotes the restriction of RtoTxM.
2.1 Examples
The unit sphere Sd−1:={x∈Rd|∥x∥2= 1}is an embedded manifold of Rd. The tangent space TxSn−1at
x∈Sd−1is given by TxSn−1={η∈Rd|η⊤x= 0}. The induced Riemannian metric on Sd−1is given by
⟨ξ,η⟩x=⟨ξ,η⟩2:=ξ⊤ηforξ,η∈TxSd−1. The orthogonal projection Px:Rd→TxSd−1onto the tangent
spaceTxSn−1is given by Px(η) = (Id−xx⊤)ηforx∈Sd−1andη∈TxSd−1.
AnimportantexampleistheStiefelmanifold(Absiletal.,2008, Chapter3.3.2), whichisdefinedas St(p,n) :=
{X∈Rn×p|X⊤X=Ip}forn≥p.St(p,n)is an embedded manifold of Rn×d. The tangent space TxSt(p,n)
atX∈St(p,n)is given by
TXSt(p,n) ={η∈Rn×p|X⊤η+η⊤X=O}.
The induced Riemannian metric on St(p,n)is given by⟨ξ,η⟩X= tr(ξ⊤η)forξ,η∈TXSt(p,n). The
orthogonal projection onto the tangent space TXSt(p,n)is given by PX(η) =η−Xsym(X⊤η)forX∈
St(p,n),η∈TXSt(p,n), where sym(A) := (A+A⊤)/2. The Stiefel manifold St(p,n)reduces to the
orthogonal groups when n=p, i.e., St(p,p) =Op.
Moreover, we will also consider the Grassmann manifold (Absil et al., 2008, Chapter 3.4.4) Gr(p,n) :=
St(p,n)/Op. LetX∈St(p,n)be a representative of [X] :={XQ|Q∈O p}∈Gr(p,n). We denote the
4Published in Transactions on Machine Learning Research (01/2025)
horizontal lift of η∈T[X]Gr(p,n)atXby¯ηX∈TXSt(p,n). The Riemannian metric of the Grassmann
manifold Gr(p,n)is endowed with⟨ξ,η⟩[X]:=/angbracketleftbig¯ξX,¯ηX/angbracketrightbig
2forξ,η∈T[X]Gr(p,n). The orthogonal projection
onto the tangent space T[X]Gr(p,n)is defined through
P[X](η) = (In−XX⊤)¯ηX,
for[X]∈Gr(p,n)andη∈T[X]Gr(p,n). Note that while the Grassmann manifold itself is not an embedded
submanifold, in practical implementations, computations involving it is often performed via the Stiefel
manifold. This allows the proposed method to be applied. Such an approach was used in previous studies,
including Kasai et al. (2019); Hu et al. (2024).
2.2 Riemannian stochastic optimization problem
We focus on minimizing an objective function f:M→Rof the form,
f(x) =1
NN/summationdisplay
i=1fi(x),
wherefiis a smooth function for i= 1,...,N. We use the mini-batch strategy as follows (see Iiduka
(2024) for detail). sk,iis a random variable generated from the i-th sampling at the k-th iteration, and
sk:= (sk,1,...,s k,bk)⊤is independent of (xk)∞
k=1, wherebk(≤N)is the batch size. To simplify the notation,
we denote the expectation Eskwith respect to skbyEk. From the independence of s1,s2,..., sk, we can
define the total expectation EbyE1E2···Ek. We define the mini-batch stochastic gradient gradfBk(xk)of
fat thek-th iteration by
gradfBk(xk) :=1
bkbk/summationdisplay
i=1gradfsk,i(xk). (1)
Our main objective is to find a stationary point x⋆∈Msatisfying gradf(x⋆) = 0 x⋆.
2.3 Proposed general framework of Riemannian adaptive methods
Reddi et al. (2018) provided a general framework of adaptive gradient methods in Euclidean space. We
devised Algorithm 1 by generalizing that framework to an embedded manifold of Rd. The main difference
from the Euclidean setting is computing the projection of H−1
kmkonto the tangent space TxkMby the
orthogonal projection Pxk. Algorithm 1 requires sequences of maps, (ϕk)∞
k=1and(ψk)∞
k=1, such that ϕk:
Tx1M×···×TxkM→Rdandψk:Tx1M×···×TxkM→Dd∩Sd
++, respectively. Note that Algorithm
1 is still abstract because the maps (ϕk)∞
k=1and(ψk)∞
k=1are not specified. Algorithm 1 is the extension of
a general framework in Euclidean space proposed by Reddi, Kale and Kumar (Reddi et al., 2018). In the
Euclidean setting (i.e., M=Rd), the orthogonal projection Pxkyields an identity map and this corresponds
to the Euclidean version of the general framework.
By setting the maps (ϕn)∞
n=1and(ψn)∞
n=1in Algorithm 1 to those used by an adaptive optimization method
in Euclidean space, any adaptive optimization method can be extended to Riemannian manifolds. In the
following, we show some examples of extending the optimization algorithm in Euclidean space to Riemannian
manifolds by using Algorithm 1.
Here, SGD is the most basic method; it uses
ϕk(g1,...,g k) =gk, ψ k(g1,...,g k) =Id.
Algorithm 1 with these maps corresponds to RSGD (Bonnabel, 2013) in the Riemannian setting. AdaGrad
(Duchietal.,2011), thefirstadaptivegradientmethodinEuclideanspacethatpropelledresearchonadaptive
methods, uses the sequences of maps ϕk(g1,...,g k) =gkand
vk=vk−1+gk⊙gk,
ψk(g1,...,g k) = diag(√vk,1,...,√vk,d) +ϵId,
5Published in Transactions on Machine Learning Research (01/2025)
Algorithm 1 The general framework of Riemannian adaptive optimization methods on an embedded sub-
manifold of Rd.
Require: Initial point x1∈M, retraction R:TM→M, step sizes (αk)∞
k=1⊂R++, sequences of maps
(ϕk)∞
k=1,(ψk)∞
k=1.
Ensure: Sequence (xk)∞
k=1⊂M.
1:k←1.
2:loop
3:gk= gradfBk(xk).
4:mk=ϕk(g1,...,g k)∈Rd.
5:Hk=ψk(g1,...,g k)∈Dd∩Sd
++.
6:xk+1=Rxk(−αkPxk(H−1
kmk)).
7:k←k+ 1.
8:end loop
wherev0= 0∈Rdandϵ >0. Here, we will denote the i-th component of vkbyvk,i. The exponential
moving average variant of AdaGrad is often used in deep-learning training. The basic variant is RMSProp
(Hinton et al., 2012), which uses the sequences of maps ϕk(g1,...,g k) =gkand
vk=β2vk−1+ (1−β2)gk⊙gk,
ψk(g1,...,g k) = diag(√vk,1,...,√vk,d) +ϵId,
wherev0= 0∈Rdandϵ>0. Both Algorithm 1 with these maps and cRMSProp (Roy et al., 2018) can be
considered extensions of RMSProp to Riemannian manifolds. They differ from each other in that parallel
transport is needed to compute the search direction of cRMSProp, but it is not needed in our method.
Adam (Kingma & Ba, 2015) is one of the most common variants; it uses the sequence of maps,
mk=β1mk−1+ (1−β1)gk, ϕ k(g1,...,g k) =mk
1−βk+1
1, (2)
and
vk=β2vk−1+ (1−β2)gk⊙gk,ˆvk=vk
1−βk+1
2,
ψk(g1,...,g k) = diag(/radicalbig
ˆvk,1,...,/radicalbig
ˆvk,d) +ϵId, (3)
wherem0= 0∈Rdandv0= 0∈Rd.β1= 0.9,β2= 0.999andϵ= 10−8are typically recommended values.
Moreover, within the general framework (Algorithm 1), we propose the following algorithm as an extension
of AMSGrad (Reddi et al., 2018) in Euclidean space.
3 Convergence analysis
3.1 Assumptions and useful lemmas
We make the following Assumptions 3.1 (A1)–(A4). (A1) and (A2) include the standard conditions. (A3)
assumes the boundedness of the gradient. (A4) is an assumption on the Lipschitz continuity of the gradient.
(A5) assumes that a lower bound exists.
Assumption 3.1. Let(xk)∞
k=1be a sequence generated by Algorithm 1.
(A1) Ek[gradfsk,i(xk)] = gradf(xk)for allk≥1andi= 1,...,b k.
(A2) There exists σ2>0such that
Ek/bracketleftig/vextenddouble/vextenddoublegradfsk,i(xk)−gradf(xk)/vextenddouble/vextenddouble2
2/bracketrightig
≤σ2,
for allk≥1andi= 1,...,b k.
6Published in Transactions on Machine Learning Research (01/2025)
Algorithm 2 AMSGrad on an embedded submanifold of Rd.
Require: Initial point x1∈M, retraction R:TM→M, step sizes (αk)∞
k=1⊂R++, mini-batch sizes
(bk)∞
k=1⊂R++hyperparameters β1,β2∈[0,1),ϵ>0.
Ensure: Sequence (xk)∞
k=1⊂M.
1:Setm0= 0,v0= 0andˆv0= 0.
2:k←1.
3:loop
4:gk= gradfBk(xk).
5:mk=β1mk−1+ (1−β1)gk.
6:vk=β2vk−1+ (1−β2)gk⊙gk.
7: ˆvk= max(ˆvk−1,vk).
8:Hk= diag(/radicalbig
ˆvk,1,...,/radicalbig
ˆvk,d) +ϵId.
9:xk+1=Rxk(−αkPxk(H−1
kmk)).
10:k←k+ 1.
11:end loop
(A3) There exists G,B > 0such that∥gradf(xk)∥2≤Gand∥gradfBk(xk)∥2≤Bfor allk≥1.
(A4) There exists a constant L>0such that
|D(f◦Rx)(η)[η]−Df(x)[η]|≤L∥η∥2
2,
for allx∈M,η∈TxM.
(A5)fis bounded below by f⋆∈R.
Assumptions (A1) and (A2) are satisfied when the probability distribution is uniform (see Appendix F) or
when the full gradient is used, i.e., when the batch size equals the total number of data. (A1) and (A2) have
been used to analyze adaptive methods in Euclidean space (Zaheer et al., 2018, Pages 3, 12, and 16) and on
the Hadamard manifold (Sakai & Iiduka, 2024, (2.2) and (2.3)). Assumptions (A3)–(A5) are often used in
both Euclidean spaces and Riemannian manifolds. Intuitively, in practical applications, these assumptions
are satisfied because the search space of the algorithm is bounded. If fis unbounded on an unbounded search
spaceX, then (A5) does not hold. Although there is a possibility such that (A4) is satisfied, (A2) would
not hold, even when the probability distribution is uniform. When fis unbounded, there is no minimizer of
f. Hence, (A5) is essential in order to analyze optimization methods. Moreover, (A4) is needed in order to
use a retraction L-smooth (Proposition 3.2) that is an extension of the Euclidean-type descent lemma. (A3)
holds when the manifold is compact (Absil et al., 2008) or through a slight modification of the objective
function and the algorithm (Kasai et al., 2018).
It is known that if Assumption 3.1 (A4) holds, so does the following Proposition 3.2. This property is known
as retraction L-smoothness (see Huang et al. (2015); Kasai et al. (2018) for details).
Proposition 3.2. Suppose that Assumption 3.1 (A4) holds. Then,
f(Rx(η))≤f(x) +⟨gradf(x),η⟩2+L
2∥η∥2
2,
for allx∈Mandη∈TxM.
3.2 Convergence analysis of Algorithm 1
The main difficulty in analyzing the convergence of adaptive gradient methods is due to the stochastic
momentum mk=ϕk(g1,...,g k). As a way to overcome this challenge in Euclidean space, Zhou et al. (2024);
Yan et al. (2018); Chen et al. (2019) defined a new sequence zk,
zk=xk+β1
1−β1(xk−xk−1).
7Published in Transactions on Machine Learning Research (01/2025)
However, since the embedded submanifold is not closed under addition in Euclidean space, this strategy does
not work in the Riemannian setting. Therefore, by following the policy of Zaheer et al. (2018), let us analyze
the case in which ϕk(g1,...,g k) =gk. To simplify the notation, we denote the i-th component of gk(resp.
vk,ˆvk) bygk,i(resp.vk,i,ˆvk,i).
Lemma 3.3. Suppose that Assumption 3.1 (A4) holds. If ϕk(g1,...,g k) =gkandH−1
k⪯νIdfor allk≥1
and someν >0, then the sequence (xk)∞
k=1⊂Mgenerated by Algorithm 1 satisfies
f(xk+1)≤f(xk) +/angbracketleftbig
gradf(xk),−αkH−1
kgk/angbracketrightbig
2+Lα2
kν2
2∥gk∥2
2,
for allk≥1.
Proof.See Appendix B.
Theorem 3.4. Suppose that Assumptions 3.1 (A1)–(A5) hold. Moreover, let us assume that αk+1≤αk,
ϕk(g1,...,g k) =gk,αkH−1
k⪰αk+1H−1
k+1and there exist µ,ν > 0such thatµId⪯H−1
k⪯νIdfor allk≥1.
Then, the sequence (xk)∞
k=1⊂Mgenerated by Algorithm 1 satisfies
K/summationdisplay
k=1αk/parenleftbigg
µ−Lαkν2
2/parenrightbigg
E/bracketleftig
∥gradf(xk)∥2
2/bracketrightig
≤C1+C2K/summationdisplay
k=1α2
k
bk,
for some constant C1,C2>0.
Remark: Since Algorithm 2 satisfies ˆvk+1,i:= max(ˆvk,i,vk+1,i)≥ˆvk,i, it together with αk+1≤αk, leads to
αk/(/radicalbig
ˆvk,i+ϵ)≥αk+1/(/radicalbig
ˆvk+1,i+ϵ). Moreover, from Lemma A.3,
1
B+ϵ≤1/radicalbig
ˆvk,i+ϵ≤1
ϵ,
which implies (B+ϵ)−1Id⪯H−1
k⪯ϵ−1Id. Therefore, Algorithm 2 satisfies the assumption αkH−1
k⪰
αk+1H−1
k+1andµId⪯H−1
k⪯νIdwithµ= (B+ϵ)−1andν=ϵ−1.
Proof.We denote gradf(xk)byg(xk). First, let us consider the case of k= 1. From Lemma 3.3, we have
f(x2)≤f(x1) +/angbracketleftbig
g(x1),−α1H−1
1g1/angbracketrightbig
2+Lα2
1ν2
2∥g1∥2
2.
By taking E1[·]of both sides, we obtain
E1[f(x2)]≤f(x1) +/angbracketleftbig
g(x1),−α1E1[H−1
1g1]/angbracketrightbig
2+Lα2
1ν2
2E1/bracketleftig
∥g1∥2
2/bracketrightig
≤f(x1) +/angbracketleftbig
g(x1),−α1E1[H−1
1g1]/angbracketrightbig
2+Lα2
1ν2
2/parenleftbiggσ2
b1+∥g(x1)∥2
2/parenrightbigg
where the second inequality comes from Lemma A.2. By taking E[·]of both sides and rearranging terms, we
get
−Lα1ν2
2E/bracketleftig
∥g(x1)∥2
2/bracketrightig
≤f(x1)−E[f(x2)] +/angbracketleftbig
g(x1),−α1E[H−1
1g1]/angbracketrightbig
2+Lα2
1σ2ν2
2b1.
By addingα1µG2to both sides, we obtain
α1µG2−Lα1ν2
2E/bracketleftig
∥g(x1)∥2
2/bracketrightig
≤f(x1)−E[f(x2)] +Lα2
1σ2ν2
2b1+/angbracketleftbig
g(x1),−α1E[H−1
1g1]/angbracketrightbig
2+α1µG2
/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
C0.
8Published in Transactions on Machine Learning Research (01/2025)
Here, we note that
α1µE/bracketleftig
∥g(x1)∥2
2/bracketrightig
≤α1µG2.
Therefore, we have
α1/parenleftbigg
µ−Lα1ν2
2/parenrightbigg
E/bracketleftig
∥g(x1)∥2
2/bracketrightig
≤f(x1)−E[f(x2)] +Lα2
1σ2ν2
2b1+C0. (4)
Next, let us consider the case of k≥2. From Lemma 3.3, we have
f(xk+1)≤f(xk) +/angbracketleftbig
g(xk),−αk−1H−1
k−1gk/angbracketrightbig
2+/angbracketleftbig
g(xk),(αk−1H−1
k−1−αkH−1
k)gk/angbracketrightbig
2+Lα2
kν2
2∥gk∥2
2
for allk≥2. From Assumption 3.1 (A3), Lemma C.1, and αk−1H−1
k−1−αkH−1
k⪰O, we have
f(xk+1)≤f(xk) +/angbracketleftbig
g(xk),−αk−1H−1
k−1gk/angbracketrightbig
2+GBtr(αk−1H−1
k−1−αkH−1
k) +Lα2
kν2
2∥gk∥2
2.
By taking Ek[·]of both sides, we obtain
Ek[f(xk+1)]≤f(xk) +/angbracketleftbig
g(xk),−αk−1H−1
k−1Ek[gk]/angbracketrightbig
2
+GBEk[tr(αk−1H−1
k−1−αkH−1
k)] +Lα2
kν2
2Ek/bracketleftig
∥gk∥2
2/bracketrightig
≤f(xk)−αk−1/angbracketleftbig
g(xk),H−1
k−1g(xk)/angbracketrightbig
2
+GBEk[tr(αk−1H−1
k−1−αkH−1
k)] +Lα2
kν2
2/parenleftbiggσ2
bk+∥g(xk)∥2
2/parenrightbigg
,
where the first inequality comes from the independence of H−1
k−1forskand the second inequality comes from
Lemmas A.1 and A.2. Here, since H−1
k−1⪰µIdandαk≤αk−1, it follows that
−αk−1/angbracketleftbig
g(xk),H−1
k−1g(xk)/angbracketrightbig
2≤−αkµ∥g(xk)∥2
2,
which implies
Ek[f(xk+1)]≤f(xk)−αk/parenleftbigg
µ−Lαkν2
2/parenrightbigg
∥g(xk)∥2
2+GBEk[tr(αk−1H−1
k−1−αkH−1
k)] +Lα2
kσ2ν2
2bk.
By taking E[·]of both sides, we have
E[f(xk+1)]≤E[f(xk)]−αk/parenleftbigg
µ−Lαkν2
2/parenrightbigg
E/bracketleftig
∥g(xk)∥2
2/bracketrightig
+GBE[tr(αk−1H−1
k−1−αkH−1
k)] +Lα2
kσ2ν2
2bk.
Rearranging the above inequality gives us
αk/parenleftbigg
µ−Lαkν2
2/parenrightbigg
E/bracketleftig
∥g(xk)∥2
2/bracketrightig
≤E[f(xk)]−E[f(xk+1)] +GBE[tr(αk−1H−1
k−1−αkH−1
k)] +Lα2
kσ2ν2
2bk.(5)
By summing (5) from k= 2tok=K, we have
K/summationdisplay
k=2αk/parenleftbigg
µ−Lαkν2
2/parenrightbigg
E/bracketleftig
∥g(xk)∥2
2/bracketrightig
≤E[f(x2)]−E[f(xK+1)] +GBE[tr(α1H−1
1−αKH−1
K)] +K/summationdisplay
k=2Lα2
kσ2ν2
2bk.
9Published in Transactions on Machine Learning Research (01/2025)
SinceµId⪯H−1
K⪯νIdfor allk≥1, it follows that tr(α1H−1
1)≤α1νdandtr(αKH−1
K)≥0. Here, we note
thatE[f(xK+1)]≥f⋆, from Assumption 3.1 (A3). Therefore, we have
K/summationdisplay
k=2αk/parenleftbigg
µ−Lαkν2
2/parenrightbigg
E/bracketleftig
∥g(xk)∥2
2/bracketrightig
≤E[f(x2)]−f⋆+GBα 1νd+K/summationdisplay
k=2Lα2
kσ2ν2
2bk. (6)
Here, by adding both sides of (4) and (6), we have
K/summationdisplay
k=1αk/parenleftbigg
µ−Lαkν2
2/parenrightbigg
E/bracketleftig
∥g(xk)∥2
2/bracketrightig
≤f(x1)−f⋆+C0+GBα 1νd/bracehtipupleft/bracehtipdownright/bracehtipdownleft /bracehtipupright
C1+Lσ2ν2
2/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
C2K/summationdisplay
k=1α2
k
bk.
This completes the proof.
Our convergence analysis (Theorem 3.4) allows the proposed framework (Algorithm 1) to use both constant
and diminishing steps sizes. Theorems 3.5 and 3.6 are convergence analyses of Algorithm 1 with constant
and diminishing steps sizes and a fixed mini-batch size, respectively.
Theorem 3.5. Under the assumptions in Theorem 3.4 and assuming that the constant step size αk:=α
satisfies 0<α< 2µL−1ν−2and the mini-batch size bk:=bsatisfies 0<b≤N, the sequence (xk)∞
k=1⊂M
generated by Algorithm 1 satisfies
1
KK/summationdisplay
k=1E/bracketleftig
∥gradf(xk)∥2
2/bracketrightig
=O/parenleftbigg1
K+1
b/parenrightbigg
.
Proof.We denote gradf(xk)byg(xk). From Theorem 3.4, we obtain
1
KK/summationdisplay
k=1α/parenleftbigg
µ−Lαν2
2/parenrightbigg
E/bracketleftig
∥g(xk)∥2
2/bracketrightig
≤C1
K+C2α2
b. (7)
Since 0< α < 2µL−1ν−2, it follows that (2αµ−Lα2ν2)/2>0. Therefore, dividing both sides of (7) by
(2αµ−Lα2ν2)/2gives
1
KK/summationdisplay
k=1E/bracketleftig
∥g(xk)∥2
2/bracketrightig
≤2C1
2αµ−Lα2ν2·1
K+2C2α2
2αµ−Lα2ν2·1
b.
This completes the proof.
Theorem 3.6. Under the assumptions in Theorem 3.4 and assuming that the diminishing step size αk:=
α/√
ksatisfiesα∈(0,1]and the mini-batch size bk:=bsatisfies 0< b≤N, the sequence (xk)∞
k=1⊂M
generated by Algorithm 1 satisfies
1
KK/summationdisplay
k=1E/bracketleftig
∥gradf(xk)∥2
2/bracketrightig
=O/parenleftbigg/parenleftbigg
1 +1
b/parenrightbigglogK√
K/parenrightbigg
.
Proof.We denote gradf(xk)byg(xk). Since (αk)∞
k=1satisfiesαk→0 (k→∞ ), there exists a natural
numberk0≥1such that, for all k≥1, ifk≥k0, then 0<αk<2µL−1ν−2. Therefore, we obtain
0<µ−Lαkν2
2<µ,
for allk≥k0. From Theorem 3.4, we have
K/summationdisplay
k=k0αk/parenleftbigg
µ−Lαkν2
2/parenrightbigg
E/bracketleftig
∥g(xk)∥2
2/bracketrightig
≤C1+C2
bK/summationdisplay
k=1α2
k−k0−1/summationdisplay
k=1αk/parenleftbigg
µ−Lαkν2
2/parenrightbigg
E/bracketleftig
∥g(xk)∥2
2/bracketrightig
,
10Published in Transactions on Machine Learning Research (01/2025)
for allK≥k0. Since (αk)∞
k=1is monotone decreasing and αk>0, we obtain
αK/parenleftbigg
µ−Lαk0ν2
2/parenrightbiggK/summationdisplay
k=k0E/bracketleftig
∥g(xk)∥2
2/bracketrightig
≤C1+C2
bK/summationdisplay
k=1α2
k+k0−1/summationdisplay
k=1Lα2
kν2E/bracketleftig
∥g(xk)∥2
2/bracketrightig
.
Dividing both sides of this inequality by 2−1KαK(2µ−Lαk0ν2)>0yields
1
KK/summationdisplay
k=k0E/bracketleftig
∥g(xk)∥2
2/bracketrightig
≤2
KαK(2µ−Lαk0ν2)/parenleftigg
C1+C2
bK/summationdisplay
k=1α2
k+k0−1/summationdisplay
k=1Lα2
kν2E/bracketleftig
∥g(xk)∥2
2/bracketrightig/parenrightigg
=1
KαK·2
2µ−Lαk0ν2/parenleftigg
C1+k0−1/summationdisplay
k=1Lα2
kν2E/bracketleftig
∥g(xk)∥2
2/bracketrightig/parenrightigg
/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
C3
+1
bKα K·2C2
2µ−Lαk0ν2
/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
C4K/summationdisplay
k=1α2
k.
From this and αK:=α/√
K < 1, we obtain
1
KK/summationdisplay
k=1E/bracketleftig
∥g(xk)∥2
2/bracketrightig
≤1
KαK/parenleftigg
C3+C4
bK/summationdisplay
k=1α2
k/parenrightigg
+1
KαKk0−1/summationdisplay
k=1E/bracketleftig
∥g(xk)∥2
2/bracketrightig
=1
α√
K/parenleftigg
C3+k0−1/summationdisplay
k=1E/bracketleftig
∥g(xk)∥2
2/bracketrightig
+C4
bK/summationdisplay
k=1α2
k/parenrightigg
.
Fromα∈(0,1], we have that
K/summationdisplay
k=1α2
k=K/summationdisplay
k=1α2
k≤K/summationdisplay
k=11
k≤1 +/integraldisplayK
1dt
t= 1 + logK.
Therefore,
1
KK/summationdisplay
k=1E/bracketleftig
∥g(xk)∥2
2/bracketrightig
≤1
α√
K/parenleftigg
C3+k0−1/summationdisplay
k=1E/bracketleftig
∥g(xk)∥2
2/bracketrightig
+C4
b+C4
blogK/parenrightigg
.
This completes the proof.
Theorems 3.7 and 3.8 are convergence analyses of Algorithm 1 with constant and diminishing steps sizes and
increasing mini-batch sizes, respectively.
Theorem 3.7. Under the assumptions in Theorem 3.4 and assuming that the constant step size αk:=α
satisfies 0<α< 2µL−1ν−2and the mini-batch size bksatisfies 0<bk≤Nand/summationtext∞
k=1b−1
k<∞, the sequence
(xk)∞
k=1⊂Mgenerated by Algorithm 1 satisfies
min
k=1,...,KE/bracketleftig
∥gradf(xk)∥2
2/bracketrightig
≤1
KK/summationdisplay
k=1E/bracketleftig
∥gradf(xk)∥2
2/bracketrightig
=O/parenleftbigg1
K/parenrightbigg
.
Proof.We denote gradf(xk)byg(xk). From Theorem 3.4, we obtain
1
KK/summationdisplay
k=1α/parenleftbigg
µ−Lαν2
2/parenrightbigg
E/bracketleftig
∥g(xk)∥2
2/bracketrightig
≤C1
K+C2α2
KK/summationdisplay
k=11
bk. (8)
11Published in Transactions on Machine Learning Research (01/2025)
Since 0< α < 2µL−1ν−2, it follows that (2αµ−Lα2ν2)/2>0. Therefore, dividing both sides of (8) by
(2αµ−Lα2ν2)/2gives
1
KK/summationdisplay
k=1E/bracketleftig
∥g(xk)∥2
2/bracketrightig
≤2C1
2αµ−Lα2ν2·1
K+2C2α2
2αµ−Lα2ν2·1
KK/summationdisplay
k=11
bk.
This completes the proof.
An example of bksuch that/summationtext∞
k=1b−1
k<∞used in Theorem 3.7 is such that the mini-batch size is multiplied
byδ>1every step, i.e.,
bt=δtb0. (9)
For example, the mini-batch size defined by (9) with δ= 2makes batch size double each step. Moreover,
the mini-batch size defined by (9) and the diminishing step size αk:=α/√
k, whereα∈(0,1], satisfy that/summationtext∞
k=1α2
kb−1
k<∞(see the assumption in Theorem 3.8).
Theorem 3.8. Under the assumptions in Theorem 3.4 and assuming that the diminishing step size αk:=
α/√
ksatisfiesα∈(0,1]and the mini-batch size bksatisfies 0<bk≤Nand/summationtext∞
k=1α2
kb−1
k<∞, the sequence
(xk)∞
k=1⊂Mgenerated by Algorithm 1 satisfies
min
k=1,...,KE/bracketleftig
∥gradf(xk)∥2
2/bracketrightig
≤1/summationtextK
k=1αkK/summationdisplay
k=1αkE/bracketleftig
∥gradf(xk)∥2
2/bracketrightig
=O/parenleftbigg1√
K/parenrightbigg
.
Proof.We denote gradf(xk)byg(xk). Since (αk)∞
k=1satisfiesαk→0 (k→∞ ), there exists a natural
numberk0≥1such that, for all k≥1, ifk≥k0, then 0<αk<2µL−1ν−2. Therefore, we obtain
0<µ−Lαkν2
2<µ,
for allk≥k0. From Theorem 3.4, we have
K/summationdisplay
k=k0αk/parenleftbigg
µ−Lαkν2
2/parenrightbigg
E/bracketleftig
∥g(xk)∥2
2/bracketrightig
≤C1+C2K/summationdisplay
k=1α2
k
bk−k0−1/summationdisplay
k=1αk/parenleftbigg
µ−Lαkν2
2/parenrightbigg
E/bracketleftig
∥g(xk)∥2
2/bracketrightig
,
for allK≥k0. Since (αk)∞
k=1is monotone decreasing and αk>0, we obtain
/parenleftbigg
µ−Lαk0ν2
2/parenrightbiggK/summationdisplay
k=k0αkE/bracketleftig
∥g(xk)∥2
2/bracketrightig
≤C1+C2K/summationdisplay
k=1α2
k
bk+k0−1/summationdisplay
k=1Lα2
kν2E/bracketleftig
∥g(xk)∥2
2/bracketrightig
.
Dividing both sides of this inequality by 2−1(2µ−Lαk0ν2)>0yields
K/summationdisplay
k=k0αkE/bracketleftig
∥g(xk)∥2
2/bracketrightig
≤2
2µ−Lαk0ν2/parenleftigg
C1+C2K/summationdisplay
k=1α2
k
bk+k0−1/summationdisplay
k=1Lα2
kν2E/bracketleftig
∥g(xk)∥2
2/bracketrightig/parenrightigg
=2
2µ−Lαk0ν2/parenleftigg
C1+k0−1/summationdisplay
k=1Lα2
kν2E/bracketleftig
∥g(xk)∥2
2/bracketrightig/parenrightigg
/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
C3
+2C2
2µ−Lαk0ν2
/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
C4K/summationdisplay
k=1α2
k
bk.
12Published in Transactions on Machine Learning Research (01/2025)
From this and/summationtext∞
k=1α2
kbk−1<∞, we have that
K/summationdisplay
k=1αkE/bracketleftig
∥g(xk)∥2
2/bracketrightig
≤C3+k0−1/summationdisplay
k=1αkE/bracketleftig
∥g(xk)∥2
2/bracketrightig
+C4K/summationdisplay
k=1α2
k
bk
/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
C5,
which implies that
1/summationtextK
k=1αkK/summationdisplay
k=1αkE/bracketleftig
∥g(xk)∥2
2/bracketrightig
≤C5/summationtextK
k=1αk.
Fromαk:=α/√
k, we obtain
K/summationdisplay
k=1αk=K/summationdisplay
k=1α√
k≥α/integraldisplayK+1
1dt√
t= 2α(√
K+ 1−1).
Therefore,
1/summationtextK
k=1αkK/summationdisplay
k=1αkE/bracketleftig
∥g(xk)∥2
2/bracketrightig
≤C5
2α(√
K+ 1−1).
This completes the proof.
4 Numerical experiments
We experimentally compared our general framework of Riemannian adaptive optimization methods (Algo-
rithms 1) with several choices of (ϕn)∞
n=1and(ψn)∞
n=1with the following algorithms:
•RSGD (Bonnabel, 2013): Algorithm 1 with ϕk(g1,...,g k) =gkandψk(g1,...,g k) =Id.
•RASA-LR, RASA-L, RASA-R (Kasai et al., 2019, Algorithm 1): β= 0.99.
•RAdam: Algorithm 1 with (ϕn)∞
n=1defined by (2), (ψn)∞
n=1defined by (3), β1= 0.9,β2= 0.999and
ϵ= 10−8.
•RAMSGrad: Algorithm 2 with β1= 0.9,β2= 0.999andϵ= 10−8.
We experimented with both constant and diminishing step sizes. For each algorithm, we searched in the
set{10−1,10−2,..., 10−8}for the best initial step size α(both constant and diminishing). Note that the
constant (resp. diminishing) step size was determined to be αk=α(resp.αk=α/√
k) for allk≥1. For
each experiment, each data set was split into a training set and a test set. The experiments used a MacBook
Air (M1, 2020) and macOS Monterey version 12.2 operating system. The algorithms were written in Python
3.12.1 with the NumPy 1.26.0 package and the Matplotlib 3.9.1 package. The Python implementations of
the methods used in the numerical experiments are available at https://github.com/iiduka-researches/
202408-adaptive .
4.1 Principal component analysis
We applied the algorithms to a principal component analysis (PCA) problem (Kasai et al., 2018; Roy et al.,
2018). For Ngiven data points x1,...,x N∈Rnandp(≤n), the PCA problem is equivalent to minimizing
f(U) :=1
NN/summationdisplay
i=1/vextenddouble/vextenddoublexi−UU⊤xi/vextenddouble/vextenddouble2
2, (10)
13Published in Transactions on Machine Learning Research (01/2025)
on the Stiefel manifold St(p,n). Therefore, the PCA problem can be considered to be optimization problem
on the Stiefel manifold. In the experiments, we set pto10and the batch size bto210. We used the QR-based
retraction on the Stiefel manifold St(p,n)(Absil et al., 2008, Example 4.1.3), which is defined by
RX(η) := qf(X+η),
forX∈St(p,n)andη∈TXSt(p,n), where qf(·)returns the Q-factor of the QR decomposition.
We evaluated the algorithms on training images of the MNIST dataset (LeCun et al., 1998) and the COIL100
dataset (Nene et al., 1996). The MNIST dataset contains 28×28gray-scale images of handwritten digits.
It has a training set of 60,000 images and a test set of 10,000 images. We transformed every image into a
784-dimensional vector and normalized its pixel values to lie in the range of [0,1]. Thus, we set N= 60000
andn= 784. The COIL100 dataset contains 7,200 normalized color camera images of the 100 objects
taken from different angles. As in the previous study (Kasai et al., 2019), we resized them to 32 ×32 pixels.
Moreover, we split them into an 80%training set and a 20%test set. Thus, we set N= 5760andn= 1024.
Figure 1(a) (resp. Figure 1(b)) shows the performances of the algorithms with a constant (resp. diminishing)
step size for the objective function values defined by (10) with respect to the number of iterations on the
training set of the MNIST dataset, while Figures 2(a) and 2(b) present those on the test set. Moreover,
Figures 5(a) and 5(b) show those on the training set of the COIL100 dataset, while Figures 6(a) and 6(b)
present those on the test set. Figure 3(a) (resp. 3(b)) presents the performances of the algorithms with a
constant (resp. diminishing) step size for the norm of the gradient of objective function defined by (10) with
respect to the number of iterations on the training set of the MNIST dataset, while Figures 4(a) and 4(b)
present those on the test set of the MNIST dataset. Moreover, Figures 7(a) and 7(b) show those on the
training set of the COIL100 dataset, Figures 8(a) and 8(b) present those on the test set. The experiments
were performed for three random initial points, and the thick line plots the average of all experiments. The
area bounded by the maximum and minimum values is painted the same color as the corresponding line.
Note that the upper part of Figures 3(a), 3(b), 4(a) and 4(b) are cut off. The initial values of the gradient
norm in these figures are approximately 38, highlighting that the gradient norm was minimized during the
optimization process. Furthermore, our results differ from those presented in Kasai et al. (2019) because of
three key factors. In Kasai et al. (2019), the mini-batch size was fixed at 10, while in our experiments we
used 210. Furthermore, the scope of the grid search differs from that of Kasai et al. (2019). Moreover, we
experimented with a doubly increasing batch size every 100 steps from an initial batch size of b1= 27and
constant and diminishing step sizes. The results are given in Appendix G.
0 100 200 300 400 500
Number of iterations3×1014×101Objective function value
RSGD
RAdam
RAMSGrad
RASA-L
RASA-R
RASA-LR
(a) constant learning rate
0 100 200 300 400 500
Number of iterations3×1014×101Objective function value
RSGD
RAdam
RAMSGrad
RASA-L
RASA-R
RASA-LR (b) diminishing learning rate
Figure 1: Objective function value defined by (10) versus number of iterations on the training set of the
MNIST datasets.
Figures 1(a) and 2(a) indicate that RAdam and RAMSGrad (Algorithm 2) performed comparably to RASA-
LR in the sense of minimizing the objective function value. Figures 1(b) and 2(b) indicate that RAdam and
RAMSGrad(Algorithm2)outperformedRASA-LandRASA-R.Figures3(a)and4(a)showthatRAMSGrad
(Algorithm 2) performed better than RASA-LR in the sense of minimizing the full gradient norm of the
14Published in Transactions on Machine Learning Research (01/2025)
0 100 200 300 400 500
Number of iterations3×1014×1015×101
RSGD
RAdam
RAMSGrad
RASA-L
RASA-R
RASA-LR
(a) constant learning rate
0 100 200 300 400 500
Number of iterations3×1014×101
RSGD
RAdam
RAMSGrad
RASA-L
RASA-R
RASA-LR (b) diminishing learning rate
Figure 2: Objective function value defined by (10) versus number of iterations on the test set of the MNIST
datasets.
0 100 200 300 400 500
Number of iterations100101Norm of the gradient of the objective function
RSGD
RAdam
RAMSGrad
RASA-L
RASA-R
RASA-LR
(a) constant learning rate
0 100 200 300 400 500
Number of iterations100101Norm of the gradient of the objective function
RSGD
RAdam
RAMSGrad
RASA-L
RASA-R
RASA-LR (b) diminishing learning rate
Figure 3: Norm of the gradient of objective function defined by (10) versus number of iterations on the
training set of the MNIST datasets.
0 100 200 300 400 500
Number of iterations101
2×1003×1004×1006×100
RSGD
RAdam
RAMSGrad
RASA-L
RASA-R
RASA-LR
(a) constant learning rate
0 100 200 300 400 500
Number of iterations101
2×1003×1004×1006×100
RSGD
RAdam
RAMSGrad
RASA-L
RASA-R
RASA-LR (b) diminishing learning rate
Figure 4: Norm of the gradient of objective function defined by (10) versus number of iterations on the test
set of the MNIST datasets.
15Published in Transactions on Machine Learning Research (01/2025)
0 100 200 300 400 500
Number of iterations101
4×1006×1002×101Objective function value
RSGD
RAdam
RAMSGrad
RASA-L
RASA-R
RASA-LR
(a) constant learning rate
0 100 200 300 400 500
Number of iterations101
6×100Objective function value
RSGD
RAdam
RAMSGrad
RASA-L
RASA-R
RASA-LR (b) diminishing learning rate
Figure 5: Objective function value defined by (10) versus number of iterations on the training set of the
COIL100 datasets.
0 100 200 300 400 500
Number of iterations101
4×1006×100
RSGD
RAdam
RAMSGrad
RASA-L
RASA-R
RASA-LR
(a) constant learning rate
0 100 200 300 400 500
Number of iterations101
6×100
RSGD
RAdam
RAMSGrad
RASA-L
RASA-R
RASA-LR (b) diminishing learning rate
Figure 6: Objective function value defined by (10) versus number of iterations on the test set of the COIL100
datasets.
0 100 200 300 400 500
Number of iterations101Norm of the gradient of the objective function
RSGD
RAdam
RAMSGrad
RASA-L
RASA-R
RASA-LR
(a) constant learning rate
0 100 200 300 400 500
Number of iterations100101Norm of the gradient of the objective function
RSGD
RAdam
RAMSGrad
RASA-L
RASA-R
RASA-LR (b) diminishing learning rate
Figure 7: Norm of the gradient of objective function defined by (10) versus number of iterations on the
training set of the COIL100 datasets.
16Published in Transactions on Machine Learning Research (01/2025)
0 100 200 300 400 500
Number of iterations101
2×1003×1004×1006×100
RSGD
RAdam
RAMSGrad
RASA-L
RASA-R
RASA-LR
(a) constant learning rate
0 100 200 300 400 500
Number of iterations101
2×1003×1004×1006×100
RSGD
RAdam
RAMSGrad
RASA-L
RASA-R
RASA-LR (b) diminishing learning rate
Figure 8: Norm of the gradient of objective function defined by (10) versus number of iterations on the test
set of the COIL100 datasets.
objective function. Figures 5(a) and 6(a) indicate that RAdam and RAMSGrad (Algorithm 2) had the best
performance in the sense of minimizing the objective function value. Figures 5(b) and 6(b) indicate that
RAdam and RAMSGrad (Algorithm 2) performed comparably to RASA-LR. Figures 7(a) and 8(a) shows
that RAdam had the best performance in the sense of minimizing the full gradient norm of the objective
function. Figures 7(b) and 8(b) indicate that RAdam and RAMSGrad (Algorithm 2) performed comparably
to RASA-R and RASA-LR.
Table 1 compares performances across different batch sizes, showing the number of iterations and CPU time
(in seconds) required by each algorithm with a constant step size to reduce the gradient norm below 2 when
solving the PCA problem on the MNIST dataset. As our analysis (Theorem 3.5) indicates, increasing the
batch size reduces the number of iterations needed to decrease the norm.
Table 1: Iterations and CPU time (seconds) required by each algorithm with a constant step size to reduce
the gradient norm below 2 for solving the PCA problem on the MNIST dataset across different batch sizes.
“-” indicates cases where the algorithm did not reach the threshold within the maximum allowed (1,000)
iterations.
batch sizebnumber of iterations CPU time (seconds)
RSGD256 - -
512 303 0.531
1024 149 0.394
RAdam256 391 0.532
512 150 0.270
1024 126 0.349
RAMSGrad256 190 0.257
512 140 0.251
1024 114 0.323
RASA-L256 493 1.228
512 442 1.291
1024 373 1.476
RASA-R256 397 0.662
512 371 0.809
1024 300 0.901
RASA-LR256 - -
512 289 0.862
1024 113 0.447
17Published in Transactions on Machine Learning Research (01/2025)
Table 2 compares performances across different batch sizes, showing the number of iterations and CPU time
(in seconds) required by each algorithm with a diminishing step size to reduce the gradient norm below 2
when solving the PCA problem on the MNIST dataset. As our analysis (Theorem 3.6) indicates, increasing
the batch size reduces the number of iterations needed to decrease the norm. Similar tables (Tables 11–16)
for the remaining datasets and experiments can be found in Appendix E.
Table 2: Iterations and CPU time (seconds) required by each algorithm with a diminishing step size to
reduce the gradient norm below 2 for solving the PCA problem on the MNIST dataset across different batch
sizes.
batch sizebnumber of iterations CPU time (seconds)
RSGD256 239 0.315
512 140 0.327
1024 85 0.229
RAdam256 292 0.434
512 189 0.349
1024 101 0.280
RAMSGrad256 234 0.324
512 224 0.437
1024 141 0.384
RASA-L256 409 1.008
512 370 1.133
1024 282 1.102
RASA-R256 235 0.399
512 194 0.418
1024 156 0.472
RASA-LR256 92 0.229
512 63 0.192
1024 36 0.145
4.2 Low-rank matrix completion
Weappliedthealgorithmstothelow-rankmatrixcompletion(LRMC)problem(Boumal&Absil,2015;Kasai
et al., 2019; Hu et al., 2024). The LRMC problem aims to recover a low-rank matrix from an incomplete
matrixX= (Xij)∈Rn×N. We denote the set of observed entries by Ω⊂{1,...,n}×{1,...,N}, i.e., (i,j)∈
Ωif and only if Xijis known. Here, we defined the orthogonal projection PΩi:Rn→Rn:a∝⇕⊣√∫⊔≀→PΩi(a)such
that thej-th element of PΩi(a)isajif(i,j)∈Ω, and 0 otherwise. Moreover, we defined qi:Rn×p×Rn→Rp
as
qi(U,x) := arg min
a∈Rp∥PΩi(Ua−x)∥2, (11)
fori≥1. By partitioning X= (x1,...,x N), the rank-pLRMC problem is equivalent to minimizing
f(U) :=1
2NN/summationdisplay
i=1∥PΩi(Uqi(U,xi)−xi)∥2
2, (12)
on the Grassmann manifold Gr(p,n). Therefore, the rank- pLRMC problem can be considered to be an
optimization problem on the Grassmann manifold (see Hu et al. (2024, Section 1) or Kasai et al. (2019,
Section 6.3) for details).
We evaluated the algorithms on the MovieLens-1M1datasets (Harper & Konstan, 2015) and the Jester2
datasets for recommender systems. The MovieLens-1M datasets contains 1,000,209 ratings given by 6,040
1https://grouplens.org/datasets/movielens/
2https://grouplens.org/datasets/jester
18Published in Transactions on Machine Learning Research (01/2025)
users on 3,952 movies. Moreover, we split them into an 80%training set and a 20%test set. Thus, we
setN= 4832andn= 3952. The Jester dataset contains ratings of 100 jokes given by 24,983 users with
scores from−10to 10. In addition, we split them into an 80%training set and a 20%test set. Thus, we set
N= 19986 andn= 100.
In the experiments, we set pto10and the batch size bto28. We used numpy.linalg.lstsq3to solve
the least squares problem (11). We used a retraction based on a polar decomposition on the Grassmann
manifold Gr(p,n)(Absil et al., 2008, Example 4.1.3), which is defined through
R[X](η) := (X+ ¯ηX)(Ip+ ¯η⊤
X¯ηX)−1
2,
for[X]∈Gr(p,n)andη∈T[X]Gr(p,n).
Figure 9(a) (resp. Figure 9(b)) shows the performances of the algorithms with a constant (resp. diminishing)
step size for objective function values defined by (12) with respect to the number of iterations on the
training set of the MovieLens-1M dataset, while Figures 10(a) and 10(b) present those on the Jester dataset.
Moreover, Figures 13(a) and 13(b) show those on the training set of the Jester dataset, while Figures
14(a) and 14(b) present those on the test set of the Jester dataset. Figure 11(a) (resp. 11(b)) shows the
performances of the algorithms with a constant (resp. diminishing) step size for the norm of the gradient
of the objective function defined by (10) with respect to the number of iterations on the training set of the
MovieLens-1M dataset, while Figures 12(a) and 12(b) present those on the test set of the MovieLens-1M
dataset. Moreover, Figures 15(a) and 15(b) show those on the training set of the Jester dataset, while Figures
16(a) and 16(b) present those on the test set of the Jester dataset. The experiments were performed for
three random initial points, and the thick line plots the average results of all experiments. The area bounded
by the maximum and minimum values is painted the same color as the corresponding line. Moreover, we
experimented with a doubly increasing batch size every 20 steps from an initial batch size of b1= 26and
constant and diminishing step sizes. The results are given in Appendix G.
0 20 40 60 80 100
Number of iterations5×1006×1007×100Objective function value
RSGD
RAdam
RAMSGrad
RASA-L
RASA-R
RASA-LR
(a) constant learning rate
0 20 40 60 80 100
Number of iterations5×1006×1007×100Objective function value
RSGD
RAdam
RAMSGrad
RASA-L
RASA-R
RASA-LR (b) diminishing learning rate
Figure 9: Objective function value defined by (12) versus number of iterations on the training set of the
MovieLens-1M datasets.
Figures 9(a) and 10(a) indicate that RAMSGrad (Algorithm 2) performed better than RASA-L and RASA-
LR in the sense of minimizing the objective function value. Figures 9(b) and 10(b) show that RAdam
and RAMSGRad (Algorithm 2) performed comparably to RASA-L and RASA-LR. Figures 11(a) and 12(a)
indicate that RAdam performed comparably to RASA-R in the sense of minimizing the full gradient norm
of the objective function. Figures 11(b) and 12(b) show that RAdam outperformed RASA-R. Figures 13(a),
13(b), 14(a) and 14(b) indicate that RAMSGrad (Algorithm 2) performed better than RASA-LR in the
sense of minimizing the objective function value. Figures 15(a) and 16(a) indicate that RAdam performed
comparably to RASA-L and RASA-R in the sense of minimizing the full gradient norm of the objective
function. Figures 15(b) and 16(b) show that RAMSGrad (Algorithm 2) outperformed RASA-L and RASA-
R.
3https://numpy.org/doc/1.26/reference/generated/numpy.linalg.lstsq.html
19Published in Transactions on Machine Learning Research (01/2025)
0 20 40 60 80 100
Number of iterations5×1006×1007×100
RSGD
RAdam
RAMSGrad
RASA-L
RASA-R
RASA-LR
(a) constant learning rate
0 20 40 60 80 100
Number of iterations5×1006×1007×100
RSGD
RAdam
RAMSGrad
RASA-L
RASA-R
RASA-LR (b) diminishing learning rate
Figure 10: Objective function value defined by (12) versus number of iterations on the test set of the
MovieLens-1M datasets.
0 20 40 60 80 100
Number of iterations100101Norm of the gradient of the objective function
RSGD
RAdam
RAMSGrad
RASA-L
RASA-R
RASA-LR
(a) constant learning rate
0 20 40 60 80 100
Number of iterations100101Norm of the gradient of the objective function
RSGD
RAdam
RAMSGrad
RASA-L
RASA-R
RASA-LR (b) diminishing learning rate
Figure 11: Norm of the gradient of objective function defined by (12) versus number of iterations on the
training set of the MovieLens-1M datasets.
0 20 40 60 80 100
Number of iterations2×1003×1004×1006×100
RSGD
RAdam
RAMSGrad
RASA-L
RASA-R
RASA-LR
(a) constant learning rate
0 20 40 60 80 100
Number of iterations101
2×1003×1004×1006×100
RSGD
RAdam
RAMSGrad
RASA-L
RASA-R
RASA-LR (b) diminishing learning rate
Figure 12: Norm of the gradient of objective function defined by (12) versus number of iterations on the test
set of the MovieLens-1M datasets.
20Published in Transactions on Machine Learning Research (01/2025)
0 20 40 60 80 100
Number of iterations3×1004×100Objective function value
RSGD
RAdam
RAMSGrad
RASA-L
RASA-R
RASA-LR
(a) constant learning rate
0 20 40 60 80 100
Number of iterations3×1004×100Objective function value
RSGD
RAdam
RAMSGrad
RASA-L
RASA-R
RASA-LR (b) diminishing learning rate
Figure 13: Objective function value defined by (12) versus number of iterations on the training set of the
Jester datasets.
0 20 40 60 80 100
Number of iterations3×1004×100
RSGD
RAdam
RAMSGrad
RASA-L
RASA-R
RASA-LR
(a) constant learning rate
0 20 40 60 80 100
Number of iterations3×1004×100
RSGD
RAdam
RAMSGrad
RASA-L
RASA-R
RASA-LR (b) diminishing learning rate
Figure 14: Objective function value defined by (12) versus number of iterations on the test set of the Jester
datasets.
0 20 40 60 80 100
Number of iterations101
100Norm of the gradient of the objective function
RSGD
RAdam
RAMSGrad
RASA-L
RASA-R
RASA-LR
(a) constant learning rate
0 20 40 60 80 100
Number of iterations101
100Norm of the gradient of the objective function
RSGD
RAdam
RAMSGrad
RASA-L
RASA-R
RASA-LR (b) diminishing learning rate
Figure 15: Norm of the gradient of objective function defined by (12) versus number of iterations on the
training set of the Jester datasets.
21Published in Transactions on Machine Learning Research (01/2025)
0 20 40 60 80 100
Number of iterations101
100
RSGD
RAdam
RAMSGrad
RASA-L
RASA-R
RASA-LR
(a) constant learning rate
0 20 40 60 80 100
Number of iterations101
100
RSGD
RAdam
RAMSGrad
RASA-L
RASA-R
RASA-LR (b) diminishing learning rate
Figure 16: Norm of the gradient of objective function defined by (12) versus number of iterations on the test
set of the Jester datasets.
5 Conclusion
This paper proposed a general framework of Riemannian adaptive optimization methods, which encapsu-
lates several stochastic optimization algorithms on Riemannian manifolds. The framework incorporates the
mini-batch strategy often used in deep learning. We also proposed RAMSGrad (Algorithm 2) that works on
embedded submanifolds in Euclidean space within our framework. In addition, we gave convergence analyses
that are valid for both a constant and diminishing step size. The analyses also revealed the relationship
between the convergence rate and mini-batch size. We numerically compared the RAMSGrad (Algorithm 2)
with the existing algorithms by applying them to principal component analysis and low-rank matrix comple-
tion problems, which can be considered to be Riemannian optimization problems. Numerical experiments
showed that the proposed method performs well against PCA. RAdam and RAMSGrad performed well for
constant and diminishing step sizes especially on the COIL100 dataset.
Acknowledgments
We are sincerely grateful to the Action Editor, Stephen Becker, and the three anonymous reviewers for
helping us improve the original manuscript. This work was supported by a JSPS KAKENHI Grant, number
JP23KJ2003.
References
P-A Absil, Robert Mahony, and Rodolphe Sepulchre. Optimization Algorithms on Matrix Manifolds . Prince-
ton University Press, 2008.
Sheldon Axler. Linear Algebra Done Right . Springer, 4th edition, 2024.
Lukas Balles, Javier Romero, and Philipp Hennig. Coupling adaptive batch sizes with learning rates, 2017.
Heinz H Bauschke and Patrick L Combettes. Convex Analysis and Monotone Operator Theory in Hilbert
Spaces. Springer, 2011.
Gary Bécigneul and Octavian-Eugen Ganea. Riemannian adaptive optimization methods. Proceedings of
The International Conference on Learning Representations , 2019.
Silvere Bonnabel. Stochastic gradient descent on Riemannian manifolds. IEEE Transactions on Automatic
Control, 58(9):2217–2229, 2013.
Nicolas Boumal and P-A Absil. Low-rank matrix completion via preconditioned optimization on the Grass-
mann manifold. Linear Algebra and its Applications , 475:200–239, 2015.
22Published in Transactions on Machine Learning Research (01/2025)
Richard H. Byrd, Gillian M. Chin, Jorge Nocedal, and Yuchen Wu. Sample size selection in optimization
methods for machine learning. Mathematical Programming , 134(1):127–155, 2012.
Léopold Cambier and P-A Absil. Robust low-rank matrix completion by Riemannian optimization. SIAM
Journal on Scientific Computing , 38(5):S440–S460, 2016.
XiangyiChen, SijiaLiu, RuoyuSun, andMingyiHong. Ontheconvergenceofaclassofadam-typealgorithms
for non-convex optimization. Proceedings of The International Conference on Learning Representations ,
2019.
Minhyung Cho and Jaehyung Lee. Riemannian approach to batch normalization. Advances in Neural
Information Processing Systems , 30, 2017.
Soham De, Abhay Yadav, David Jacobs, and Tom Goldstein. Automated inference with adaptive batches.
InProceedings of the 20th International Conference on Artificial Intelligence and Statistics , volume 54 of
Proceedings of Machine Learning Research , pp. 1504–1513. PMLR, 2017.
JohnDuchi, EladHazan, andYoramSinger. Adaptivesubgradientmethodsforonlinelearningandstochastic
optimization. Journal of machine learning research , pp. 2121–2159, 2011.
Priya Goyal, Piotr Dollár, Ross Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola, Andrew
Tulloch, Yangqing Jia, and Kaiming He. Accurate, large minibatch SGD: Training imagenet in 1 hour,
2018.
F Maxwell Harper and Joseph A Konstan. The MovieLens datasets: History and context. Acm transactions
on interactive intelligent systems , 5(4):1–19, 2015.
Geoffrey Hinton, Nitish Srivastava, and Kevin Swersky. Neural networks for machine learning lecture 6a
overview of mini-batch gradient descent. COURSERA: Neural Networks for Machine Learning , pp. 26–31,
2012.
Jiang Hu, Ruicheng Ao, Anthony Man-Cho So, Minghan Yang, and Zaiwen Wen. Riemannian natural
gradient methods. SIAM Journal on Scientific Computing , 46(1):A204–A231, 2024.
Wen Huang, Kyle A Gallivan, and P-A Absil. A Broyden class of quasi-Newton methods for Riemannian
optimization. SIAM Journal on Optimization , 25(3):1660–1685, 2015.
Hideaki Iiduka. Theoretical analysis of Adam using hyperparameters close to one without Lipschitz smooth-
ness.Numerical Algorithms , 95(1):383–421, 2024.
Hiroyuki Kasai, Hiroyuki Sato, and Bamdev Mishra. Riemannian stochastic recursive gradient algorithm.
InInternational Conference on Machine Learning , pp. 2516–2524. PMLR, 2018.
Hiroyuki Kasai, Pratik Jawanpuria, and Bamdev Mishra. Riemannian adaptive stochastic gradient algo-
rithms on matrix manifolds. In International Conference on Machine Learning , pp. 3262–3271. PMLR,
2019.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. Proceedings of The
International Conference on Learning Representations , pp. 1–15, 2015.
Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to docu-
ment recognition. Proceedings of the IEEE , 86(11):2278–2324, 1998.
Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Conference on
Learning Representations , 2019.
Liangchen Luo, Yuanhao Xiong, Yan Liu, and Xu Sun. Adaptive gradient methods with dynamic bound of
learning rate. In Proceedings of the 7th International Conference on Learning Representations , 2019.
Sameer A Nene, Shree K Nayar, Hiroshi Murase, et al. Columbia object image library (COIL-100). Technical
Report CUCS-006-96 , 1996.
23Published in Transactions on Machine Learning Research (01/2025)
Maximillian Nickel and Douwe Kiela. Poincaré embeddings for learning hierarchical representations. In
Advances in neural information processing systems , pp. 6338–6347, 2017.
Sashank J Reddi, Satyen Kale, and Sanjiv Kumar. On the convergence of Adam and beyond. Proceedings
of The International Conference on Learning Representations , pp. 1–23, 2018.
SoumavaKumarRoy, ZakariaMhammedi, andMehrtashHarandi. Geometryawareconstrainedoptimization
techniques for deep learning. In Proceedings of the IEEE conference on computer vision and pattern
recognition , pp. 4460–4469, 2018.
Hiroyuki Sakai and Hideaki Iiduka. Riemannian adaptive optimization algorithm and its application to
natural language processing. IEEE Transactions on Cybernetics , 52(8):7328–7339, 2021.
Hiroyuki Sakai and Hideaki Iiduka. Convergence of Riemannian stochastic gradient descent on Hadamard
manifold. Pacific Journal of Optimization , 2024. doi: https://doi.org/10.61208/pjo-2024-005.
Takashi Sakai. Riemannian Geometry . American Mathematical Society, 1996.
Hiroyuki Sato. Riemannian Optimization and Its Applications . Springer, 2021.
Naoki Sato and Hideaki Iiduka. Existence and estimation of critical batch size for training generative
adversarial networks with two time-scale update rule. In International Conference on Machine Learning ,
pp. 30080–30104. PMLR, 2023.
Samuel L. Smith, Pieter-Jan Kindermans, and Quoc V. Le. Don’t decay the learning rate, increase the batch
size. InInternational Conference on Learning Representations , 2018. URL https://openreview.net/
forum?id=B1Yy1BxCZ .
Bart Vandereycken. Low-rank matrix completion by Riemannian optimization. SIAM Journal on Optimiza-
tion, 23(2):1214–1236, 2013.
Yan Yan, Tianbao Yang, Zhe Li, Qihang Lin, and Yi Yang. A unified analysis of stochastic momentum
methods for deep learning. In Proceedings of the Twenty-Seventh International Joint Conference on
Artificial Intelligence , pp. 2955–2961, 2018.
Jihun Yun and Eunho Yang. Riemannian SAM: Sharpness-aware minimization on riemannian manifolds.
Advances in Neural Information Processing Systems , 36, 2024.
Manzil Zaheer, Sashank Reddi, Devendra Sachan, Satyen Kale, and Sanjiv Kumar. Adaptive methods for
nonconvex optimization. Advances in neural information processing systems , 31, 2018.
Matthew D Zeiler. Adadelta: an adaptive learning rate method. arXiv preprint arXiv:1212.5701 , 2012.
Hongyi Zhang, Sashank J Reddi, and Suvrit Sra. Riemannian SVRG: Fast stochastic optimization on
Riemannian manifolds. Advances in Neural Information Processing Systems , 29, 2016.
Jingzhao Zhang, Hongyi Zhang, and Suvrit Sra. R-SPIDER: A fast Riemannian stochastic optimization
algorithm with curvature independent rate. arXiv preprint arXiv:1811.04194 , 2018.
Dongruo Zhou, Jinghui Chen, Yuan Cao, Ziyan Yang, and Quanquan Gu. On the convergence of adaptive
gradient methods for nonconvex optimization. Transactions on Machine Learning Research , 2024.
Pan Zhou, Xiao-Tong Yuan, and Jiashi Feng. Faster first-order methods for stochastic non-convex opti-
mization on Riemannian manifolds. In The 22nd International Conference on Artificial Intelligence and
Statistics , pp. 138–147. PMLR, 2019.
Juntang Zhuang, Tommy Tang, Yifan Ding, Sekhar C Tatikonda, Nicha Dvornek, Xenophon Papademetris,
and James Duncan. Adabelief optimizer: Adapting stepsizes by the belief in observed gradients. Advances
in Neural Information Processing Systems , 33:18795–18806, 2020.
24Published in Transactions on Machine Learning Research (01/2025)
A Useful Lemmas
Lemma A.1. Suppose that Assumption 3.1 (A1) holds. Let (xk)∞
k=1be a sequence generated by Algorithm
1. Then,
Ek[gradfBk(xk)] = gradf(xk),
for allk≥1.
Proof.From (1), Assumption 3.1 (A1) and the linearity of Ek[·], we have
Ek[gradfBk(xk)] =1
bkbk/summationdisplay
i=1Ek/bracketleftbig
gradfsk,i(xk)/bracketrightbig
= gradf(xk).
This completes the proof.
Lemma A.2. Suppose that Assumptions 3.1 (A1) and (A2) hold. Let (xk)∞
k=1be a sequence generated by
Algorithm 1. Then,
Ek/bracketleftig
∥gradfBk(xk)∥2
2/bracketrightig
≤σ2
bk+∥gradf(xk)∥2
2
for allk≥1.
Proof.From∥a+b∥2
2=∥a∥2
2+ 2⟨a,b⟩2+∥b∥2
2, we obtain
Ek/bracketleftig
∥gradfBk(xk)∥2
2/bracketrightig
=Ek/bracketleftig
∥gradfBk(xk)−gradf(xk)∥2
2/bracketrightig
+ 2Ek/bracketleftbig
⟨gradfBk(xk)−gradf(xk),gradf(xk)⟩2/bracketrightbig
+Ek/bracketleftig
∥gradf(xk)∥2
2/bracketrightig
,(13)
for allk≥1. From (1) and Assumption 3.1 (A1), the first term on the right-hand side of (13) yields
Ek/bracketleftig
∥gradfBk(xk)−gradf(xk)∥2
2/bracketrightig
=Ek
/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble1
bkbk/summationdisplay
i=1gradfsk,i(xk)−gradf(xk)/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
2

=1
b2
kEk/bracketleftiggbk/summationdisplay
i=1/vextenddouble/vextenddoublegradfsk,i(xk)−gradf(xk)/vextenddouble/vextenddouble2
2/bracketrightigg
≤σ2
bk,
where the second equality comes from Assumption 3.1 (A2). From Lemma A.1, the second term on the
right-hand side of (13) yields
2Ek/bracketleftbig
⟨gradfBk(xk)−gradf(xk),gradf(xk)⟩2/bracketrightbig
= 2⟨Ek[gradfBk(xk)]−gradf(xk),gradf(xk)⟩2
= 2⟨gradf(xk)−gradf(xk),gradf(xk)⟩2
= 0.
Therefore, we obtain
Ek/bracketleftig
∥gradfBk(xk)∥2
2/bracketrightig
≤σ2
bk+∥gradf(xk)∥2
2,
for allk≥1. This completes the proof.
25Published in Transactions on Machine Learning Research (01/2025)
Lemma A.3. Suppose that Assumption 3.1 (A3) holds. Then, the sequence (xk)∞
k=1⊂Mgenerated by
Algorithm 2 satisfies
ˆvk,i≤B2,
for allk≥1andi= 1,...,d.
Proof.Note that from Assumption 3.1 (A3), we have
g2
k,i≤g2
k,1+···+g2
k,d=∥gk∥2
2≤B2
for allk≥1andi= 1,...,d. The proof is by induction. For k= 1, from 0≤β2<1, we have
ˆv1,i=v1,i:=β2v0,i+ (1−β2)g2
1,i= (1−β2)g2
1,i≤g2
1,i≤B2.
Suppose that ˆvk−1,i≤B2. Fromvk−1,i≤ˆvk−1,i≤B2, we have
vk,i=β2vk−1,i+ (1−β2)g2
k,i≤β2B2+ (1−β2)B2=B2.
Thus, induction ensures that vk,i≤B2for allk≥1.
B Proof of Lemma 3.3
Proof.We denote gradf(xk)byg(xk). From Proposition 3.2, we have
f(xk+1)≤f(xk) +/angbracketleftbig
g(xk),−αkPxk(H−1
kgk)/angbracketrightbig
2+L
2/vextenddouble/vextenddouble−αkPxk(H−1
kgk)/vextenddouble/vextenddouble2
2,
for allk≥1. Here, we note that the tangent space TxkMof an embedded submanifold Min Euclidean
space is a subspace of Euclidean space. Therefore, according to the result in (Axler, 2024, 6.57 (a)), the
projectionPxkis a linear map. From the linearity and symmetry of Pxk, we obtain
/angbracketleftbig
g(xk),−αkPxk(H−1
kgk)/angbracketrightbig
2=/angbracketleftbig
Pxk(g(xk)),−αkH−1
kgk/angbracketrightbig
2=/angbracketleftbig
g(xk),−αkH−1
kgk/angbracketrightbig
2.
From the symmetry of PxkandPxk◦Pxk=Pxk, we have
/vextenddouble/vextenddouble−αkPxk(H−1
kgk)/vextenddouble/vextenddouble2
2=α2
k/vextenddouble/vextenddoublePxk(H−1
kgk)/vextenddouble/vextenddouble2
2
=α2
k/angbracketleftbig
Pxk(H−1
kgk),Pxk(H−1
kgk)/angbracketrightbig
2
=α2
k/angbracketleftbig
H−1
kgk,Pxk(Pxk(H−1
kgk))/angbracketrightbig
2
=α2
k/angbracketleftbig
H−1
kgk,Pxk(H−1
kgk)/angbracketrightbig
2
≤α2
k/vextenddouble/vextenddoubleH−1
kgk/vextenddouble/vextenddouble
2/vextenddouble/vextenddoublePxk(H−1
kgk)/vextenddouble/vextenddouble
2.
Here, when Pxk(H−1
kgk)̸= 0∈Rd, it follows that
/vextenddouble/vextenddouble−αkPxk(H−1
kgk)/vextenddouble/vextenddouble2
2≤α2
k/vextenddouble/vextenddoubleH−1
kgk/vextenddouble/vextenddouble2
2≤α2
kν2∥gk∥2
2,
where the first inequality comes from/vextenddouble/vextenddoublePxk(H−1
kgk)/vextenddouble/vextenddouble
2≤/vextenddouble/vextenddoubleH−1
kgk/vextenddouble/vextenddouble
2(see Bauschke & Combettes (2011,
Corollary 3.24 (ii)) and Axler (2024, 6.57 (h)) for details) and the second inequality comes from O≺H−1
k⪯
νId. On the other hand, this inequality clearly holds if Pxk(H−1
kgk) = 0∈Rd. Therefore, we obtain
f(xk+1)≤f(xk) +/angbracketleftbig
g(xk),−αkH−1
kgk/angbracketrightbig
2+Lα2
kν2
2∥gk∥2
2,
for allk≥1. This completes the proof.
26Published in Transactions on Machine Learning Research (01/2025)
C Linear algebra lemma
Lemma C.1. Leta= (a1,...,a n)⊤∈Rn,b= (b1,...,b n)⊤∈RnandD= diag(d1,...,d n)∈Sn
+∩Dn. If
∥a∥2≤Aand∥b∥2≤B, then
a⊤Db≤ABtr(D).
Proof.From∥a∥2≤Aand∥b∥2≤B, we have|ai|≤Aand|bi|≤Bfor alli= 1,...,n. Therefore, we
obtain
a⊤Db=n/summationdisplay
i=1aidibi≤n/summationdisplay
i=1|ai|·|bi|di≤ABn/summationdisplay
i=1di≤ABtr(D).
This completes the proof.
D Details of numerical experiments
Tables 3–10 summarize the initial step size, batch size, hyperparameters, and CPU time (seconds) per
iteration used in the numerical experiments. Table 3 (resp. Table 4) summarizes the details of solving the
PCA problem for the MNIST dataset by using the algorithms with constant (resp. diminishing) step sizes.
Table 5 (resp. Table 6) summarizes the details of solving the PCA problem for the COIL100 dataset by using
the algorithms with constant (resp. diminishing) step sizes. Table 7 (resp. Table 8) summarizes the details
of solving the LRMC problem for the MovieLens-1M dataset by using the algorithms with constant (resp.
diminishing) step sizes. Table 9 (resp. Table 10) summarizes the details of solving the LRMC problem for
the Jester dataset by using the algorithms with constant (resp. diminishing) step sizes.
Table 3: Summary of the initial step size, batch size, CPU time (seconds) per iteration, and hyperparameters
used in the case of constant step sizes for the PCA problem on the MNIST dataset.
α β 1β2β ϵ b CPU time per iteration (seconds)
RSGD 10−2- - - - 2101.4×10−2
RAdam 10−20.9 0.999 - 10−82101.481×10−2
RAMSGrad 10−30.9 0.999 - 10−82101.563×10−2
RASA-L 10−3- - 0.99 10−82102.155×10−2
RASA-R 10−3- - 0.99 10−82101.836×10−2
RASA-LR 10−3- - 0.99 10−82102.374×10−2
Table 4: Summary of the initial step size, batch size, CPU time (seconds) per iteration, and hyperparameters
used in the case of diminishing step sizes for the PCA problem on the MNIST dataset.
α β 1β2β ϵ b CPU time per iteration (seconds)
RSGD 10−1- - - - 2101.19×10−2
RAdam 10−10.9 0.999 - 10−82101.275×10−2
RAMSGrad 10−20.9 0.999 - 10−82101.28×10−2
RASA-L 10−2- - 0.99 10−82101.803×10−2
RASA-R 10−2- - 0.99 10−82101.629×10−2
RASA-LR 10−2- - 0.99 10−82102.222×10−2
E Extended results on batch size comparisons
This appendix presents a detailed comparison of using different batch sizes across various datasets and
experiments. Tables 11–16 show the performance of the algorithms under varying batch sizes, highlighting
theimpactonconvergencespeedandcomputationalcost. Theresultsfurthersupportouranalyses(Theorems
3.5 and 3.6) that larger batch sizes lead to faster convergence.
27Published in Transactions on Machine Learning Research (01/2025)
Table 5: Summary of the initial step size, batch size, CPU time (seconds) per iteration, and hyperparameters
used in the case of constant step sizes for the PCA problem on the COIL100 dataset.
α β 1β2β ϵ b CPU time per iteration (seconds)
RSGD 10−2- - - - 2103.937×10−2
RAdam 10−20.9 0.999 - 10−82105.256×10−2
RAMSGrad 10−30.9 0.999 - 10−82104.531×10−2
RASA-L 10−3- - 0.99 10−82105.697×10−2
RASA-R 10−3- - 0.99 10−82105.13×10−2
RASA-LR 10−3- - 0.99 10−82105.859×10−2
Table 6: Summary of the initial step size, batch size, CPU time (seconds) per iteration, and hyperparameters
used in the case of diminishing step sizes for the PCA problem on the COIL100 dataset.
α β 1β2β ϵ b CPU time per iteration (seconds)
RSGD 10−1- - - - 2103.638×10−2
RAdam 10−20.9 0.999 - 10−82105.339×10−2
RAMSGrad 10−30.9 0.999 - 10−82104.033×10−2
RASA-L 10−2- - 0.99 10−82106.805×10−2
RASA-R 10−2- - 0.99 10−82105.781×10−2
RASA-LR 10−2- - 0.99 10−82106.938×10−2
Table 7: Summary of the initial step size, batch size, CPU time (seconds) per iteration, and hyperparameters
used in the case of constant step sizes for the LRMC problem on the MovieLens-1M dataset.
α β 1β2β ϵ b CPU time per iteration (seconds)
RSGD 10−3- - - - 282.921×10−1
RAdam 10−30.9 0.999 - 10−8282.813×10−1
RAMSGrad 10−30.9 0.999 - 10−8282.806×10−1
RASA-L 10−3- - 0.99 10−8283.834×10−1
RASA-R 10−4- - 0.99 10−8288.512×10−2
RASA-LR 10−4- - 0.99 10−8281.285×10−2
Table 8: Summary of the initial step size, batch size, CPU time (seconds) per iteration, and hyperparameters
used in the case of diminishing step sizes for the LRMC problem on the MovieLens-1M dataset.
α β 1β2β ϵ b CPU time per iteration (seconds)
RSGD 10−3- - - - 281.931×10−1
RAdam 10−30.9 0.999 - 10−8281.953×10−1
RAMSGrad 10−30.9 0.999 - 10−8282.302×10−1
RASA-L 10−3- - 0.99 10−8283.11×10−1
RASA-R 10−4- - 0.99 10−8281.231×10−2
RASA-LR 10−4- - 0.99 10−8281.86×10−2
Table 9: Summary of the initial step size, batch size, CPU time (seconds) per iteration, and hyperparameters
used in the case of constant step sizes for the LRMC problem on the Jester dataset.
α β 1β2β ϵ b CPU time per iteration (seconds)
RSGD 10−3- - - - 286.03×10−2
RAdam 10−30.9 0.999 - 10−8285.973×10−2
RAMSGrad 10−30.9 0.999 - 10−8285.966×10−2
RASA-L 10−3- - 0.99 10−8285.955×10−2
RASA-R 10−3- - 0.99 10−8285.88×10−2
RASA-LR 10−3- - 0.99 10−8285.958×10−2
28Published in Transactions on Machine Learning Research (01/2025)
Table10: Summaryoftheinitialstepsize, batchsize, CPUtime(seconds)periteration, andhyperparameters
used in the case of diminishing step sizes for the LRMC problem on the Jester dataset.
α β 1β2β ϵ b CPU time per iteration (seconds)
RSGD 10−3- - - - 285.815×10−2
RAdam 10−30.9 0.999 - 10−8285.836×10−2
RAMSGrad 10−30.9 0.999 - 10−8285.885×10−2
RASA-L 10−3- - 0.99 10−8285.746×10−2
RASA-R 10−3- - 0.99 10−8285.853×10−2
RASA-LR 10−3- - 0.99 10−8285.829×10−2
Table 11: Iterations and CPU time (seconds) required by each algorithm with a constant step size to reduce
the gradient norm below 4 for solving the PCA problem on the COIL100 dataset, across different batch
sizes. A “-” indicates cases where the algorithm did not reach the threshold within the maximum allowed
1,000 iterations.
batch sizebnumber of iterations CPU time (seconds)
RSGD256 - -
512 - -
1024 - -
RAdam256 - -
512 19 0.178
1024 14 0.159
RAMSGrad256 132 1.105
512 23 0.220
1024 14 0.158
RASA-L256 297 3.558
512 122 1.636
1024 75 1.169
RASA-R256 53 0.490
512 34 0.358
1024 29 0.372
RASA-LR256 797 9.741
512 225 2.968
1024 60 0.936
29Published in Transactions on Machine Learning Research (01/2025)
Table 12: Iterations and CPU time (seconds) required by each algorithm with a diminishing step size to
reduce the gradient norm below 4 for solving the PCA problem on the COIL100 dataset across different
batch sizes.
batch sizebnumber of iterations CPU time (seconds)
RSGD256 237 1.695
512 171 1.526
1024 132 1.562
RAdam256 21 0.160
512 17 0.153
1024 16 0.179
RAMSGrad256 32 0.245
512 23 0.207
1024 20 0.247
RASA-L256 94 1.082
512 52 0.670
1024 36 0.542
RASA-R256 19 0.168
512 12 0.113
1024 9 0.123
RASA-LR256 36 0.420
512 31 0.400
1024 27 0.411
Table 13: Iterations and CPU time (seconds) required by each algorithm with a constant step size to reduce
thegradientnormbelow3forsolvingtheLRMCproblemontheMovieLens-1Mdatasetacrossdifferentbatch
sizes. “-” indicates cases where the algorithm did not reach the threshold within the maximum allowed (300)
iterations.
batch sizebnumber of iterations CPU time (seconds)
RSGD64 23 0.143
128 20 0.249
256 21 0.481
RAdam64 45 0.292
128 26 0.312
256 16 0.365
RAMSGrad64 - -
128 - -
256 155 3.717
RASA-L64 110 4.490
128 19 0.950
256 8 0.447
RASA-R64 37 0.637
128 34 0.745
256 34 1.106
RASA-LR64 19 0.723
128 9 0.424
256 5 0.232
30Published in Transactions on Machine Learning Research (01/2025)
Table 14: Iterations and CPU time (seconds) required by each algorithm with a diminishing step size
to reduce the gradient norm below 3 for solving the LRMC problem on the MovieLens-1M dataset across
differentbatchsizes. “-”indicatescaseswherethealgorithmdidnotreachthethresholdwithinthemaximum
allowed (300) iterations.
batch sizebnumber of iterations CPU time (seconds)
RSGD64 108 0.691
128 97 1.172
256 110 2.595
RAdam64 15 0.093
128 13 0.149
256 11 0.238
RAMSGrad64 50 0.322
128 32 0.385
256 22 0.503
RASA-L64 8 0.299
128 7 0.305
256 5 0.261
RASA-R64 - -
128 - -
256 - -
RASA-LR64 22 0.874
128 16 0.705
256 13 0.688
Table 15: Iterations and CPU time (seconds) required by each algorithm with a constant step size to reduce
the gradient norm below 1/4for solving the LRMC problem on the Jester dataset across different batch
sizes. “-” indicates cases where the algorithm did not reach the threshold within the maximum allowed (300)
iterations.
batch sizebnumber of iterations CPU time (seconds)
RSGD64 204 0.391
128 210 0.785
256 204 1.500
RAdam64 37 0.070
128 41 0.150
256 34 0.245
RAMSGrad64 - -
128 66 0.244
256 54 0.392
RASA-L64 32 0.062
128 36 0.134
256 32 0.231
RASA-R64 38 0.072
128 37 0.137
256 34 0.244
RASA-LR64 174 0.342
128 48 0.184
256 43 0.312
31Published in Transactions on Machine Learning Research (01/2025)
Table 16: Iterations and CPU time (seconds) required by each algorithm with a diminishing step size to
reduce the gradient norm below 1/2for solving the LRMC problem on the Jester dataset across different
batch sizes. “-” indicates cases where the algorithm did not reach the threshold within the maximum allowed
(300) iterations.
batch sizebnumber of iterations CPU time (seconds)
RSGD64 - -
128 - -
256 - -
RAdam64 188 0.376
128 225 0.840
256 167 1.232
RAMSGrad64 21 0.039
128 18 0.064
256 19 0.136
RASA-L64 184 0.372
128 249 0.943
256 178 1.325
RASA-R64 219 0.427
128 - -
256 195 1.441
RASA-LR64 5 0.008
128 4 0.012
256 3 0.015
F Conditions under which Assumption 3.1 (A2) holds
We introduce the assumptions (Assumptions F.1 (B1) and (B2)) under which Assumption 3.1 (A2) from
Section 3.1 holds, and demonstrate that Assumption 3.1 (A2) is valid under these assumptions (Lemma F.2).
Note that Assumptions F.1 (B1) and (B2) are simply the counterparts of Assumptions 3.1 (A4) and (A5)
forf, applied to each fi(i= 1,...,N ).
Assumption F.1. For alli= 1,...,N,
(B1) There exists a constant Li>0such that
|D(fi◦Rx)(η)[η]−Dfi(x)[η]|≤Li∥η∥2
2,
for allx∈M,η∈TxM.
(B2)fiis bounded below by f⋆∈R.
Lemma F.2. Suppose that Assumptions F.1 (B1) and (B2) hold. We assume that a random variable sk,i
takes a value from 1 to Nfollowing a uniform distribution. Then, the sequence (xk)∞
k=1⊂Mgenerated by
Algorithm 1 satisfies
Ek/bracketleftig/vextenddouble/vextenddoublegradfsk,i(xk)−gradf(xk)/vextenddouble/vextenddouble2
2/bracketrightig
≤2
NN/summationdisplay
i=1LiMi,
where
Mi:= sup{fi(xk)−f⋆|k≥1}∈[0,∞).
32Published in Transactions on Machine Learning Research (01/2025)
Proof.From Assumption 3.1 (A5) and Proposition 3.2, we have
f∗≤fi/parenleftbigg
Rxk/parenleftbigg
−1
Ligradfi(xk)/parenrightbigg/parenrightbigg
≤fi(xk) +/angbracketleftbigg
gradfi(xk),−1
Ligradfi(xk)/angbracketrightbigg
xk+Li
2/vextenddouble/vextenddouble/vextenddouble/vextenddouble−1
Ligradfi(xk)/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
xk
=fi(xk)−1
Li∥gradfi(xk)∥2
xk+1
2Li∥gradfi(xk)∥2
xk
=fi(xk)−1
2Li∥gradfi(xk)∥2
xk.
Therefore, we obtain
∥gradfi(xk)∥2
xk≤2Li(fi(xk)−f⋆)≤2LiMi, (14)
where
Mi:= sup{fi(xk)−f⋆|k≥1}∈[0,∞).
From∥a−b∥2
2=∥a∥2
2−2⟨a,b⟩2+∥b∥2
2, we obtain
Ek/bracketleftig/vextenddouble/vextenddoublegradfsk,i(xk)−gradf(xk)/vextenddouble/vextenddouble2
2/bracketrightig
=Ek/bracketleftig/vextenddouble/vextenddoublegradfsk,i(xk)/vextenddouble/vextenddouble2
2/bracketrightig
−2Ek/bracketleftbig/angbracketleftbig
gradfsk,i(xk),gradf(xk)/angbracketrightbig
2/bracketrightbig
+Ek/bracketleftig
∥gradf(xk)∥2
2/bracketrightig
=1
NN/summationdisplay
i=1∥gradfi(xk)∥2
2−2/angbracketleftbig
Ek[gradfsk,i(xk)],gradf(xk)/angbracketrightbig
2+∥gradf(xk)∥2
2
=1
NN/summationdisplay
i=1∥gradfi(xk)∥2
2−∥gradf(xk)∥2
2
≤1
NN/summationdisplay
i=1∥gradfi(xk)∥2
2.
Here, by using (14), it follows that
Ek/bracketleftig/vextenddouble/vextenddoublegradfsk,i(xk)−gradf(xk)/vextenddouble/vextenddouble2
2/bracketrightig
≤2
NN/summationdisplay
i=1LiMi.
G Experimental results on the PCA and LRMC with increasing batch sizes
33Published in Transactions on Machine Learning Research (01/2025)
0 100 200 300 400 500
Number of iterations3×1014×1015×101Objective function value
RSGD
RAdam
RAMSGrad
RASA-L
RASA-R
RASA-LR
(a) constant learning rate
0 100 200 300 400 500
Number of iterations3×1014×1015×101Objective function value
RSGD
RAdam
RAMSGrad
RASA-L
RASA-R
RASA-LR (b) diminishing learning rate
Figure 17: Objective function value defined by (10) versus number of iterations on the training set of the
MNIST datasets.
0 100 200 300 400 500
Number of iterations3×1014×101
RSGD
RAdam
RAMSGrad
RASA-L
RASA-R
RASA-LR
(a) constant learning rate
0 100 200 300 400 500
Number of iterations3×1014×101
RSGD
RAdam
RAMSGrad
RASA-L
RASA-R
RASA-LR (b) diminishing learning rate
Figure 18: Objective function value defined by (10) versus number of iterations on the test set of the MNIST
datasets.
0 100 200 300 400 500
Number of iterations100101Norm of the gradient of the objective function
RSGD
RAdam
RAMSGrad
RASA-L
RASA-R
RASA-LR
(a) constant learning rate
0 100 200 300 400 500
Number of iterations100101Norm of the gradient of the objective function
RSGD
RAdam
RAMSGrad
RASA-L
RASA-R
RASA-LR (b) diminishing learning rate
Figure 19: Norm of the gradient of objective function defined by (10) versus number of iterations on the
training set of the MNIST datasets.
34Published in Transactions on Machine Learning Research (01/2025)
0 100 200 300 400 500
Number of iterations101
2×1003×1004×1006×100
RSGD
RAdam
RAMSGrad
RASA-L
RASA-R
RASA-LR
(a) constant learning rate
0 100 200 300 400 500
Number of iterations101
2×1003×1004×1006×100
RSGD
RAdam
RAMSGrad
RASA-L
RASA-R
RASA-LR (b) diminishing learning rate
Figure 20: Norm of the gradient of objective function defined by (10) versus number of iterations on the test
set of the MNIST datasets.
0 100 200 300 400 500
Number of iterations101
4×1006×1002×101Objective function value
RSGD
RAdam
RAMSGrad
RASA-L
RASA-R
RASA-LR
(a) constant learning rate
0 100 200 300 400 500
Number of iterations101
6×1002×1013×101Objective function value
RSGD
RAdam
RAMSGrad
RASA-L
RASA-R
RASA-LR (b) diminishing learning rate
Figure 21: Objective function value defined by (10) versus number of iterations on the training set of the
COIL100 datasets.
0 100 200 300 400 500
Number of iterations101
4×1006×1002×101
RSGD
RAdam
RAMSGrad
RASA-L
RASA-R
RASA-LR
(a) constant learning rate
0 100 200 300 400 500
Number of iterations101
6×1002×101
RSGD
RAdam
RAMSGrad
RASA-L
RASA-R
RASA-LR (b) diminishing learning rate
Figure22: Objectivefunctionvaluedefinedby (10)versusnumberofiterationsonthetestsetoftheCOIL100
datasets.
35Published in Transactions on Machine Learning Research (01/2025)
0 100 200 300 400 500
Number of iterations100101Norm of the gradient of the objective function
RSGD
RAdam
RAMSGrad
RASA-L
RASA-R
RASA-LR
(a) constant learning rate
0 100 200 300 400 500
Number of iterations100101Norm of the gradient of the objective function
RSGD
RAdam
RAMSGrad
RASA-L
RASA-R
RASA-LR (b) diminishing learning rate
Figure 23: Norm of the gradient of objective function defined by (10) versus number of iterations on the
training set of the COIL100 datasets.
0 100 200 300 400 500
Number of iterations101
RSGD
RAdam
RAMSGrad
RASA-L
RASA-R
RASA-LR
(a) constant learning rate
0 100 200 300 400 500
Number of iterations101
RSGD
RAdam
RAMSGrad
RASA-L
RASA-R
RASA-LR (b) diminishing learning rate
Figure 24: Norm of the gradient of objective function defined by (10) versus number of iterations on the test
set of the COIL100 datasets.
0 20 40 60 80 100
Number of iterations5×1006×1007×1008×100Objective function value
RSGD
RAdam
RAMSGrad
RASA-L
RASA-R
RASA-LR
(a) constant learning rate
0 20 40 60 80 100
Number of iterations5×1006×1007×100Objective function value
RSGD
RAdam
RAMSGrad
RASA-L
RASA-R
RASA-LR (b) diminishing learning rate
Figure 25: Objective function value defined by (12) versus number of iterations on the training set of the
MovieLens-1M datasets.
36Published in Transactions on Machine Learning Research (01/2025)
0 20 40 60 80 100
Number of iterations5×1006×1007×1008×100
RSGD
RAdam
RAMSGrad
RASA-L
RASA-R
RASA-LR
(a) constant learning rate
0 20 40 60 80 100
Number of iterations5×1006×1007×1008×100
RSGD
RAdam
RAMSGrad
RASA-L
RASA-R
RASA-LR (b) diminishing learning rate
Figure 26: Objective function value defined by (12) versus number of iterations on the test set of the
MovieLens-1M datasets.
0 20 40 60 80 100
Number of iterations100101Norm of the gradient of the objective function
RSGD
RAdam
RAMSGrad
RASA-L
RASA-R
RASA-LR
(a) constant learning rate
0 20 40 60 80 100
Number of iterations100101Norm of the gradient of the objective function
RSGD
RAdam
RAMSGrad
RASA-L
RASA-R
RASA-LR (b) diminishing learning rate
Figure 27: Norm of the gradient of objective function defined by (12) versus number of iterations on the
training set of the MovieLens-1M datasets.
0 20 40 60 80 100
Number of iterations101
2×1003×1004×1006×100
RSGD
RAdam
RAMSGrad
RASA-L
RASA-R
RASA-LR
(a) constant learning rate
0 20 40 60 80 100
Number of iterations101
2×1003×1004×1006×100
RSGD
RAdam
RAMSGrad
RASA-L
RASA-R
RASA-LR (b) diminishing learning rate
Figure 28: Norm of the gradient of objective function defined by (12) versus number of iterations on the test
set of the MovieLens-1M datasets.
37Published in Transactions on Machine Learning Research (01/2025)
0 20 40 60 80 100
Number of iterations3×1004×100Objective function value
RSGD
RAdam
RAMSGrad
RASA-L
RASA-R
RASA-LR
(a) constant learning rate
0 20 40 60 80 100
Number of iterations3×1004×100Objective function value
RSGD
RAdam
RAMSGrad
RASA-L
RASA-R
RASA-LR (b) diminishing learning rate
Figure 29: Objective function value defined by (12) versus number of iterations on the training set of the
Jester datasets.
0 20 40 60 80 100
Number of iterations3×1004×100Objective function value
RSGD
RAdam
RAMSGrad
RASA-L
RASA-R
RASA-LR
(a) constant learning rate
0 20 40 60 80 100
Number of iterations3×1004×100Objective function value
RSGD
RAdam
RAMSGrad
RASA-L
RASA-R
RASA-LR (b) diminishing learning rate
Figure 30: Objective function value defined by (12) versus number of iterations on the test set of the Jester
datasets.
0 20 40 60 80 100
Number of iterations101
100Norm of the gradient of the objective function
RSGD
RAdam
RAMSGrad
RASA-L
RASA-R
RASA-LR
(a) constant learning rate
0 20 40 60 80 100
Number of iterations101
100Norm of the gradient of the objective function
RSGD
RAdam
RAMSGrad
RASA-L
RASA-R
RASA-LR (b) diminishing learning rate
Figure 31: Norm of the gradient of objective function defined by (12) versus number of iterations on the
training set of the Jester datasets.
38Published in Transactions on Machine Learning Research (01/2025)
0 20 40 60 80 100
Number of iterations101
100Norm of the gradient of the objective function
RSGD
RAdam
RAMSGrad
RASA-L
RASA-R
RASA-LR
(a) constant learning rate
0 20 40 60 80 100
Number of iterations101
100Norm of the gradient of the objective function
RSGD
RAdam
RAMSGrad
RASA-L
RASA-R
RASA-LR (b) diminishing learning rate
Figure 32: Norm of the gradient of objective function defined by (12) versus number of iterations on the test
set of the Jester datasets.
39