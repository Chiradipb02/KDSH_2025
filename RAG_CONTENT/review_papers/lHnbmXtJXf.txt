Under review as submission to TMLR
Deciphering Attention Mechanisms:
Optimization and Fenchel Dual Solutions
Anonymous authors
Paper under double-blind review
Abstract
Attention has been widely adopted in many state-of-the-art deep learning models. While the
significant performance improvements it brings have attracted great interest, the theoretical
understanding of attention remains limited. This paper presents a new perspective on
understanding attention by showing that it can be seen as a solver of a family of estimation
problems. Specifically, weexploreaconvexoptimizationproblemcentraltomanyestimation
tasks prevalent in the development of deep learning architectures. Instead of solving this
problem directly, we address its Fenchel dual and derive a closed-form approximation of
the optimal solution. This approach results in a generalized attention framework, with the
popular dot-product attention used in transformer networks being a special case. We show
thatT5transformerhasimplicitlyadoptedthegeneralformofthesolutionbydemonstrating
that this expression unifies the word mask and the positional encoding functions. Finally,
we discuss how these new attention structures can be practically applied in model design
and argue that the underlying convex optimization problem offers a principled justification
for the architectural choices in attention mechanisms.
1 Introduction
Attention-based deep neural networks are now integrated into cutting-edge language models that have revo-
lutionized a broad range of tasks: machine translation (Bahdanau et al., 2014; Luong et al., 2015), sentiment
classification (Wang et al., 2016), image captioning (Xu et al., 2015) and unsupervised representation learn-
ing (Devlin et al., 2019), etc. Especially, attention plays a pivotal role in the construction of the transformer
architecture (Vaswani et al., 2017), which has had a profound impact on the deep learning field.
Despite great empirical success, the design principle of attention has not been well studied in the literature,
and there is no in-depth understanding of why attention-based models (e.g. BERT (Devlin et al., 2019)) have
significantly better performance than other models. This gap in understanding limits practitioners’ ability
to effectively employ attention layers, posing challenges in developing new attention-based architectures.
In this paper, we offer a new perspective for understanding attention by showing that it is in fact a solver for
a certain type of optimization problem that corresponds to an inference task. We give several examples, all of
which can be characterized as follows: given 1) an unreliable estimate of the mean of an unknown distribution
ponRdand 2) a preference distribution uonRdencoding beliefs on p’s selection, the inference task is to
get a better estimate of p’s mean given its unreliable estimate and u. We derive a convex optimization
problem that is abstracted from the task and solve it by instead solving its Fenchel dual (Rockafellar, 1970,
p.104). Remarkably, the derived expression of the improved estimate of pgives a generalized attention
structure whose special case is equivalent to the popular dot-product attention (Luong et al., 2015) that is
also applied in the transformer network (Vaswani et al., 2017). In addition, we show that our generalized
attention expression has been implicitly adopted by T5 transformer (Raffel et al., 2020) as the expression
unifies the concept of word masks and its positional encoding functions. Extra examples are given to show
how the generalized attention structures can be used in practice, and a novel optimal transport (OT)-based
attentionisderivedtoshowhowourframeworkhelpsdevelopmoregeneralattentionstructures. Additionally,
experiments are performed, which validate our theoretical work.
1Under review as submission to TMLR
2 Related work
Since2019, severalauthorshaveinvestigatedthepropertiesandworkingmechanismsofattention. Thisseries
of works mainly addresses whether the attention mechanism can serve as a proxy of saliency (Michel et al.,
2019; Voita et al., 2019; Jain & Wallace, 2019; Wiegreffe & Pinter, 2019; Serrano & Smith, 2020; Vashishth
et al., 2020). Most of these works obtain insights into the attention mechanism by performing empirical
studies. The related methods include analyzing the behaviours of trained attention-based models (Clark
et al., 2019), pruning a few heads, analyzing the effects of altering the attention weights (Michel et al., 2019;
Voita et al., 2019), or a mixture of these (Jain & Wallace, 2019; Vashishth et al., 2020).
Beyond empirical understanding, theoretical results by Brunner et al. (2019) and Hahn (2020) indicate
that self-attention layers are not identifiable, meaning multiple combinations of attention weights can yield
equally good predictions. This non-uniqueness complicates interpretability. Additionally, Tsai et al. (2019)
reformulated attention using kernel theory, showing it can be viewed as applying a kernel smoother over
the inputs. Recent studies have also explored the expressivity of attention (Dong et al., 2021; Baldi &
Vershynin, 2022; Mahdavi et al., 2024). To understand the underpinning inductive bias of attention, Sahiner
et al. (2022) have investigated convex-relaxations through the lens of convex duality by replacing the softmax
function with element-wise nonlinear functions. While our work views the problem through a similar lens,
the framework covers the unaltered attention architecture and focuses more on the design motivation of
attention and its generalization.
Another important approach to understanding attention is to analyze its asymptotic behaviour when the
number of heads and the network width approach infinity (Yang, 2019; Hron et al., 2020). In this limit, the
entire network behaves as a Gaussian process (Lee et al., 2018) allowing for closed-form characterizations
not available in the finite regime. Since 2021, several theoretical works have explored attention outside
this asymptotic regime. Lu et al. (2021) set up a simple attention-based classification model and derive
a closed-form relationship between the word’s embedding norm and the product of its key and the query.
They empirically show that such a relationship also exists in a more practical configuration. Similarly, Jelassi
et al. (2022); Li et al. (2023); Deora et al. (2024) characterize optimization and generalization properties for
gradient-descent training. Ramsauer et al. (2021) established an equivalence between attention and a newly
proposed Hopfield network with continuous states, demonstrating that the new Hopfield network’s update
rule is equivalent to the attention mechanism used in transformers (Vaswani et al., 2017).
3 Setup of a design problem
We consiser a prediction task: given an input X, predict an output quantity Y= (Y(1),Y(2),...,Y(K)),
includingKcomponents. We will present several machine-learning problems and show they can be unified
andabstractedintoameanestimationproblem. Specifically, thegoalistoestimatethemeanofadistribution
p, given a prototype of pand a noisy estimate of the discrepancy between their means. By framing the
probleminthisway, wecandeviseaunifiedconvexoptimizationframeworktoaddressthesevariousscenarios.
The solutions derived under this framework yield attention-like structures, which can be used to tackle the
original prediction tasks. Furthermore, plugging in various functions for closeness constraints, we recover
the original dot-product attention (Sec 6) and derive a variant with added properties (Sec 9).
Translation Problem (TP). In this problem, the input Xis a sentence, or a sequence of words, in the
source language. Output Yis the sequence of words in the target sentence, where Y(k)is thekthword.
Image Captioning (IC). In this problem, the input Xis a raw image and output Yis the sequence of
words in the caption, where Y(k)is thekthword.
Filling in the Blanks Task (FB) . This task has been used to train the BERT model (Devlin et al., 2019).
The input Xis a sequence of words with a certain percentage of words masked. The output Yare the
predicted masked words, where Y(k)denotes the kthmasked one.
The objective of any of these problems and that we address in this paper is to learn a function F, mapping
from the space of Xto the space of Yso that Y=F(X). We will denote by F(k)the part ofFresponsible
2Under review as submission to TMLR
Y
X
F(1)
F(2)
F(k)
F(K)
...
...
Y(1)
Y(2)
Y(k)
Y(K)
Y(k)
f(k)
out
h(k)
F(k)
X
g(k)
(a)
(b)
(c)
c(k)
h(k)
g(k)
^h(k)=Pn
i=1t(k)
iu(k)
iexp(ht(k)
i;z(k)i)
Pn
j=1u(k)
jexp(ht(k)
j;z(k)i)
Generalized attention block
supp.
Templates
T=n
t(k)
1;t(k)
2;;t(k)
Mo
u(k)
Probabilities
n
u(k)
1;u(k)
2;;u(k)
Mo
z(k)
z(k)
f(k)
evd
f(k)
pref
u(k)
rel.
ppt.
F
Figure 1: A conceptual graph of the deep
learning model that we work with. The block
g(k)is the one we will investigate. (a) shows
the general structure of a sequence generation
model, with F(k)responsible for the k-th out-
put. Our focus is on the architecture in (b),
whereF(k)contains a component g(k)that in-
fers a distribution’s mean h(k)based on its
noisy estimations from two aspects: its pref-
erence (prior) distribution u(k)and a noisy es-
timation of its mean shift z(k)fromu(k)’s. We
show thatg(k)should implement the expres-
sion in (c), which includes the dot-product at-
tention as a special case (Luong et al., 2015).
for predicting Y(k)(Fig 1a), namely, Y(k)=F(k)(X). Although we here express Fas separate functions
(F(1),F(2),...,F(K)), we note that it is in fact possible that different F(k)’s share some component in
common. Without loss of generality, we now focus on the design of F(k).
3.1 The Design Problem
In deep learning research, a typical approach to solving the three running tasks is first to use a neural
network to extract vector representations {t(k)
1,t(k)
2,...,t(k)
M}⊆RdofX, which are referred as templates.
Collectively, we will denote the set {t(k)
1,t(k)
2,...,t(k)
M}of templates by T(k).1(IfXare words, typical choices
of neural network include RNN, LSTM, etc. If Xis an image, a typical choice is CNN.) Let A⊆Rddenote
the space containing all templates. For each Y(k), some mechanism g(k)is needed to adaptively combine the
representations of Xto obtain h(k), which is then fed into a classifier f(k)
outto predict Y(k).
To obtain an idea of how to produce h(k), consider TPtask, where h(k)corresponds to a vector (also known
as embedding) representing the k-th word in the target sentence, and T(k)={t(k)
1,t(k)
2,...,t(k)
M}are the
ones of the source sentence.2Then, the inference of h(k)corresponds to combining the semantic meanings
encoded in T(k)to produce the k-th word embedding in the target sentence. This can be simply modelled as
h(k)=/integraldisplay
Ωtp(k)(t)dts.t. p(k)(t)≥0for all t∈T(k)and/integraldisplay
Ωp(k)(t) dt= 1, (1)
where Ω =T(k). That is, h(k)is a convex combination of t∈T(k), or equivalently, the mean of an unknown
distribution p(k)onT(k). For generality, our following discussion extends the support of p(k)to all possible
templates by setting Ω =A. In Sec 9, we show that this extension enables optimal transport-based attention,
taking into account words having similar embeddings in T(k). That is, even if a word is not present in the
source sentence, its embedding will still be optimized if it has a similar embedding in T(k).
In practice, the cardinality of Amay be huge or infinite; therefore, it is important to design a mechanism
that allows the users to inject prior knowledge to guide the production of h(k). For example, in TPtask,A
would be the set of all word embeddings, which could contain more than 10K elements. However, h(k)should
largely depend on the templates associated with the words (in the input sentence) having similar locations
to thek-th word in the target sentence. If we could effectively inject this prior information, the inference
task would be largely simplified. One natural way to do so is to use a neural network module f(k)
prefto propose
a prototype of p(k), referred to as the preference distribution u(k), and letp(k)be closeu(k). Specifically
u(k)puts non-zero probability masses on templates t(k)
1,t(k)
2,...,t(k)
M, and their probabilities are respectively
u(k)
1,u(k)
2,...,u(k)
M(which sum to 1). For TP,u(k)is expected to have larger values for the words in a similar
1We add the superscript kto note that the inference of Y(k)does not necessarily share the set of templates.
2We mainly use TPto motivate the design and discussion. In Sec 3.2, we show the same ideas also apply to ICandFB.
3Under review as submission to TMLR
Templates available to be selected

tn
t1
t2
2Rd
LSTM
LSTM
LSTM

LSTM
LSTM
LSTM

T
g(k)
h(k)
word k 1
LSTM
word k 2
LSTM
f(k)
evd
(target)
(target)
Softmax
word k
Add
f(k)
out
(target)
z(k)
(source)
(source)
(source)
word 1
word 2
word n
f(k)
pref
Comb. supp. & pref. wt.
positional
encoding
u(k)
(a) Translation (TP)
T
g(k)
h(k)
word k 1
LSTM
word k 2
LSTM
f(k)
evd
Softmax
word k
Add
f(k)
out
z(k)
Comb. supp. & pref. wt.
positional
encoding
u(k)
Templates available to be selected

tn
t1
t2
2Rd
Image
CNN Layers
d
h
attened
Feature tensor
w
f(k)
pref (b) Image captioning (IC)
word 1
word 2
word n
Add & Norm
f(k)
out
FCN
Softmax
Templates available to be selected

tn
t1
t2
2Rd
recovered kthmasked word
word j

tj
(kthmasked)


V
V
V
V
Comb. supp. & pref. wt.
positional
encoding
u(k)
T
Q
z(k)
g(k)
h(k)
f(k)
evd
f(k)
pref
Transformation layers (c) Filling in the blanks (FB)
Figure 2: The model architectures of the three running examples. For the f(k)
evdin (a) and (b), the dashed
links exist throughout the training and are replaced by the dotted ones in the generation stage.
location of the k-th word of the target sentence. The preference distribution u(k)is considered as a good
approximation of p(k), in the sense that the support of p(k)is contained in the set T(k)of templates. Note
that if Rdis the word embedding space for a large vocabulary, and if the size Mof the template set T(k)is
relatively small, then restricting the support of p(k)to be within T(k)imposes a strong constraint on p(k).
On the other hand, u(k)is not a sufficiently accurate approximation of p(k), in the sense that u(k)may assign
probabilities to T(k)somewhat differently. For example, in TP, the choice of Y(k)depends on both Xand
the already generated words Y(i<k).3Whileu(k)provides a strong prior that p(k)should mainly focus on
the words appearing in the source sentence, it is inherently tough for u(k)to capture the semantic evolution
inY(i<k). The difficulty shifts the mean µ(k)ofu(k)from the mean h(k)ofp(k).
To alleviate the problem, we need another piece of information z(k)∈Rdthat is generated by another
network module f(k)
evdand provides information regarding the mean shift. In TP,z(k)depends on Y(i<k).)
In particular, we assume that z(k)is a noisy version of the shift, more precisely,
z(k)=h(k)−µ(k)+ϵ, (2)
whereϵ∼N(0,σ2I)is a spherical Gaussian noise in Rdwith covariance σ2I. We refer to z(k)as the evidence.
We summarize the problem setup in Fig 1b. Then the design problem is to construct a function, or a network
block,g, which infers the unknown distribution p(k)and hence its mean h(k)based on the evidence z(k)and
the preference distribution u(k).
3.2 Additional examples
Having demonstrated how the setup applies to the translation problem ( TP), we will now illustrate its
applicability to the other two examples.
Image Captioning (IC). The caption generation model in Fig 2b has a similar architecture adopted by Xu
et al. (2015), where f(k)
prefextracts templates from the image using a CNN. In this task, a word’s position
is independent of the object’s location, so all CNN-extracted templates have the same preference weight.
3We assume the sentence generation process is Markovian. More details are given in Sec 3.2.
4Under review as submission to TMLR
Similar objects in the image have similar CNN features. Allowing non- Ttemplates to influence h(k)could
introduce irrelevant information, harming word inference accuracy. To improve h(k)estimation, we constrain
p(k)to have support only within u(k). As generation progresses, h(k)should evolve to provide relevant image
information for the next word. This semantic evolution is captured by z(k)=f(k)
evd, which predicts the shift
ofµ(k)fromh(k). Soµ(k)+z(k)estimates h(k)and should be close to it, as should u(k)andp(k).
Filling in the Blanks Task (FB) . For filling-in-the-blank tasks, consider a BERT-like model (Fig 2c)
wheref(k)
prefandf(k)
evdshare transformation layers common to NLP tasks. f(k)
prefapplies a linear map Vto the
output sequence of the previous layer to form the template set Tsupporting u(k), with preference weights
specified by positional encoding. Concurrently, z(k)=f(k)
evdestimates the shift of h(k)from the mean µ(k)
due to local variation. As before, we need µ(k)+z(k)close to h(k)andp(k)close tou(k). Notably, the
formulation of the problem is based on the assumption that the network modules f(k)
evdandf(k)
prefare fixed
and generate z(k)andu(k)satisfying the above-assumed properties. In reality, f(k)
evdandf(k)
prefare obtained
via training. However, we argue that if gis made to satisfy our design objective, we can at least interpret
f(k)
evdandf(k)
prefobtained from training as serving to produce z(k)andu(k)with our desired properties.
4 Formulation of an optimization problem
The discussion made in the previous section implies that the key optimization problem we are about to focus
on should ensure
1.h(k)is not too far from µ(k)+z(k), where h(k)is constructed by p(k)according to (1) and µ(k)is
the mean of the preference distribution u(k).
2.p(k)is close tou(k)whilep(k)’s support is a subset of u(k)’s.
These two desiderata prompt us to optimize:
min
pα
2/vextenddouble/vextenddouble/vextenddouble/vextenddouble(µ+z)−/integraldisplay
Rdap(a)da/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
+K(p,u) (3)
whereα>0is responsible for the relative strength of the two terms (and can be interpreted as the reliability
ofµ+z),K(p,u)denotes the KL divergence of ufromp.4By definition,K(p,u)has a finite value if and
only ifphas zero values outside the support of u. Thus, both requirements in the second desideratum are
satisfied by using the KL divergence as a measure for the closeness of pandu. Let ˜pbe the minimizer of
(3). The estimate of his
ˆh=/integraldisplay
Rda˜p(a)da. (4)
Naturally, this optimization problem can be derived from three different, though related perspectives. Below,
we present a less commonly known view that demonstrates how αaffects the optimal solution from a hard
constraint perspective. The maximum likelihood and Bayesian perspectives are included in Appx B.
A Maximum Entropy on the Mean Perspective. Consider a problem that seeks a distribution psuch
that the expectation/integraltext
Rdap(a)dais not far from µ+z. Namely, we require/vextenddouble/vextenddouble(µ+z)−/integraltext
Rdap(a)da/vextenddouble/vextenddouble2≤1
2α.
Given z, there are infinitely many p’s that satisfy the constraints, making it difficult to select the “best” p.
A technique in information theory, maximum entropy on the mean (MEM) (Rioux et al., 2020; Gamboa,
1989), addresses this by selecting the best guess of the ground truth p∗that satisfies the constraint and
minimizes the KL divergence:
˜p= argmin
pK(p,u)s.t./vextenddouble/vextenddouble/vextenddouble/vextenddouble(µ+z)−/integraldisplay
Rdap(a)da/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
≤1
2α,
which also minimizes (3) according to (Rioux et al., 2020, Eq (18)) and (Borwein & Lewis, 1992, Cor 4.9).
4As we will focus on a single step of sequence predictions, we simplify our notations by omitting superscript (k)in the rest
of our discussions.
5Under review as submission to TMLR
5 A motivating example to find the optimal solution
To better illustrate our method for solving (3), we first examine a special case where the preference distri-
butionufollows a spherical Gaussian distribution, specifically u∼N (µ,Id). In this scenario, the convex
problem can be solved in closed form. The derivation provides valuable insights into how the problem can
be approached in a general context, as we will demonstrate in Sec 6.
Letb=µ+zserve as an unreliable observation of hp. Rioux et al. (Rioux et al., 2020) prove, via Fenchel
duality (Rockafellar, 1970, p.104) that the minimizer p∗of (3) takes the form
p∗(a) =u(a) exp⟨a,λ∗⟩/integraltext
u(a′) exp⟨a′,λ∗⟩da′, (5)
where
λ∗= argmax
λ∈Rd⟨b,λ⟩−1
2α∥λ∥2−log/integraldisplay
u(a) exp⟨a,λ⟩da. (6)
Note that/integraltext
u(a) exp⟨a,λ⟩da= exp(⟨µ,λ⟩+1
2∥λ∥2)as it is the moment generating function (MGF) of
u∼N (µ,Id). Substituting the expression into (6) followed by setting the derivative with respect to λto
zero yieldsλ∗=α
α+1(b−µ). By (5),p∗(a)∝exp(−1
2∥a−µ∥2+⟨a,λ∗⟩)∝exp(−1
2∥a−(µ+λ∗)∥2).
Substituting λ∗=α
α+1(b−µ)into it implies that p∗follows a Gaussian distribution N(1
1+αµ+α
1+αb,Id).
Thus, our estimate of hpis1
1+αµ+α
1+αb.
The valueαin (3) can also be considered as a measure of the reliability of the noisy observation b, where
a smallerαimplies a less reliable b. Then, the estimate of hpshould be less affected by basαapproaches
zero, which is well captured by our derived expression1
1+αµ+α
1+αb. We will also see this relationship in
a more general setting in our subsequent discussions. While a more complicated analysis is involved, the
underlying principles are essentially the same.
In Sec 6, we focus on a similar optimization problem that estimates hpassuming that uis instead a discrete
distribution. By solving the optimization problem, we derive a closed-form approximation for the estimate
ofhp, via Fenchel duality. The approximation then gives a generalized attention layer structure as shown in
Fig 1. A special case of it is equivalent to the familiar dot-product attention (Luong et al., 2015) that is also
adopted in transformers (Vaswani et al., 2017). Moreover, we will show that T5 transformer (Raffel et al.,
2020) implicitly adopts our generalized attention expression.
6 Attention as inference via Fenchel duality
Now we present how to solve (3) with general u, where the solution yields the standard attention mechanism.
Rioux et al. proved that the optimization problem stated in (3) has the following Fenchel dual:
Theorem 1. The dual of (3) is given by
max
λ∈Rd/braceleftbigg
⟨λ,µ+z⟩−1
2α∥λ∥2−logM(λ)/bracerightbigg
, (7)
where
M(λ) =/integraldisplay
Rdu(a) exp⟨a,λ⟩da. (8)
Given a maximizer λ∗of (7), one can recover the minimizer ˜pof (3) via
˜p(a) =u(a) exp⟨a,λ∗⟩/integraltext
Rdu(a′) exp⟨a′,λ∗⟩da′. (9)
6Under review as submission to TMLR
By Theorem 1, the estimated hdefined in (4) can be re-written as
ˆh=/integraldisplay
Rda˜p(a)da=/integraldisplay
Rdau(a) exp⟨a,λ∗⟩/integraltext
Rdu(a′) exp⟨a′,λ∗⟩da′da, (10)
whereλ∗is a maximizer of (7).
In general,λ∗does not have a closed-form expression in terms of α,uandz, and a standard paradigm is
to search for it using gradient ascent-based methods. In this paper, we will not search for λ∗in this way;
instead, we will derive a closed-form expression to approximate it. Remarkably, this takes the form of the
generalized attention presented in Fig 1. Note that M(λ)in (8) equals Eu[exp⟨W,λ⟩], the expectation of the
random variable exp⟨W,λ⟩whereWhas the probability distribution u. The expectation is just the moment
generating function (MGF) of W, and the value logM(λ)is called the cumulant of W(McCullagh, 1987,
p.26), which has an expansion (McCullagh, 1987, (2.4))
logM(λ) =⟨µ,λ⟩+1
2⟨λ,Σλ⟩+o(∥λ∥2), (11)
withµ=/integraltext
au(a)daandΣ=/integraltext
(a−µ) (a−µ)Tu(a)darespectively denote the expectation and the variance-
covariance matrix of W. Note that the expansion implicitly assumes that random variable Wfollowing
distribution uhas bounded moments. (Derivation of (11) is given in Appx A.)
Now we assume that αis small and we argue that this assumption is justified in practice. For instance, in
the translation task, all words in the dictionary can serve as candidate templates, which could be more than
10,000, but ureduces this size to the length of the source sentence (usually less than tens of words). The
inference of pshould strongly anchor around this prior information; consequently the information provided
byzshould weigh less. On the other hand, zcan hardly provide an accurate estimate of the mean shift, since
the generation of zis often ignorant of the templates selected by u(for example, in the example translation
and image captioning models) or generated by a low-capacity module (as in the example filling-in-the-blank
model). For these reasons, one should de-emphasize the constraint imposed by zand thus choose a small α.
Whenαis picked to be small enough (see (7)), the optimization of λgets a large penalty on its L2 norm
and thus,∥λ∗∥is close to zero. Then, by (11), we have
logM(λ∗)≈⟨µ,λ∗⟩+1
2⟨λ∗,Σλ∗⟩. (12)
Note that the approximation becomes exact for any α>0ifuis Gaussian, which is the case of the motivating
example in Sec 5. Substituting (12) into (7) followed by setting the derivative with respect to λto zero
yields
λ∗=α(Id+αΣ)−1z, (13)
whereIddenotes the d×didentity matrix.5Asαis assumed close to zero, (13) is further reduced to
λ∗=αz. (14)
Plugging the expression into (10) gives the result stated as follows:
Theorem 2. Givenuwith bounded moments, for a small enough α>0, the estimated hdefined in (4) can
be approximated by
ˆh=/integraldisplay
Rdau(a) exp(α⟨a,z⟩)/integraltext
Rdu(a′) exp(α⟨a′,z⟩)da′da. (15)
For the case that uis a discrete distribution with support {t1,t2,...,tn}and the preference probability
{u1,u2,...,un}, (15) becomes simply
ˆh=n/summationdisplay
i=1tiuiexp (α⟨ti,z⟩)/summationtextn
j=1ujexp (α⟨tj,z⟩). (16)
7Under review as submission to TMLR
1
 0 11.0
0.5
0.00.51.0 u(t1)=0.3
u(t2)=0.4
u(t3)=0.6+z
h (approx.)
h
=4
1
 0 1u(t1)=0.3
u(t2)=0.4
u(t3)=0.6+z
h (approx.)
h
=2
1
 0 1u(t1)=0.3
u(t2)=0.4
u(t3)=0.6+z
h (approx.)
h
=1
1
 0 1u(t1)=0.3
u(t2)=0.4
u(t3)=0.6+z
h (approx.)
h
=0.5
Figure 3: The approximation of ˆhfor different choices of α. The dots in orange compose the support of
discreteuwith the preference weights labelled above. The dark blue arrow starting from the mean µofu
denotes the evidence z. The red square marks the ˆhconstructed by (10) with the λ∗maximizing (7), while
the purple one marks the ˆhapproximated by (16). As we can observe, (16) gives a precise approximation of
ˆhwhenαis sufficiently small.
In Fig 3, we set d= 2and visualize the approximation of hfor various selections of α. We observe that, as α
decreases, (16) outputs a better approximation of ˆh. Besides, as a decreasing αimplies a less reliable µ+z,h
is less affected by µ+zand gets close to µ. Note that our results do not suggest that αshould be arbitrarily
close to zero for a perfect approximation (which leaves zuseless). Fig 3 shows a good approximation is
achieved when α= 0.5,1. And for these two choices, ˆhstill significantly deviates from µ(corresponding to
the case when α= 0andzis useless). Thus, zstill largely affects the final estimation results.
The derived solution in (16) aligns with the original attention mechanisms discussed by Bahdanau et al.
(2014) and Luong et al. (2015), where uis set to a uniform distribution. These models have incorporated
most of the crucial components of the modern transformer architecture. In Sec 8, we will demonstrate that
(16) also extends to more contemporary architectures, such as the BERT model (Devlin et al., 2019) and T5
(Raffel et al., 2020). Furthermore, we will show that a good approximation can be achieved in practice by
comparing the accurate solution with its approximated counterpart used in these pretrained models.
7 Discussion
In Section 6, we derived an alternative expression of ˆhdefined in (4) by solving the Fenchel dual of the
optimization problem (3). Although the expression is not in closed form, as we are only interested in the
case whenαis small, a closed-form approximation of ˆhis derived in Theorem 2 and reduced to the form
stated in (16) when considering a discrete distribution u.
As we pointed out, the block gin Fig 2a, Fig 2b and Fig 2c is expected to find the inferred ˜pminimizing (3)
followed by plugging it into (4) to construct ˆh. Thus, one can complete the architecture designs of the three
running examples by replacing gwith a network layer implementing (16), namely, the structure in Fig 1c.
The relationship between the optimal solution and attention models. Remarkably, the expression
stated in (16) gives a generalized attention block. In particular, based on our framework, researchers can
customize the implementations of f(k)
evdandf(k)
prefto generate zanduand feed them into (16) to get an
attention-like network architecture.6
For instance, by setting ui=1
nfor alli, the expression is equivalent to the well known dot-product attention
(Luong et al., 2015), which is also applied in the transformer network (Vaswani et al., 2017). The equivalence
of the expression of ˆhand the dot-product attention layer tells us: (a) by applying a dot-product attention
layer in a model, we essentially ask the model to perform an optimization task defined in (3) and construct the
output according to (4) . (b)the derivation of hdepends on two relatively independent pieces of information:
a preference distribution given the global information and an estimate of the output’s deviation from the
preference distribution’s mean according to some local information. This suggests that the design of attention-
based model can be decomposed into two parts that respectively estimate these two values.
5When Σ =Id, (13) becomes λ∗=α(Id+αId)−1z=α
1+αz. By (2), b=h+ϵ=z+µ. Thus,λ∗=α
1+α(b−µ)recovers
the expression of λ∗in the motivating example.
6Potential selectionss of f(k)
evdandf(k)
prefincludes constant functions, fixed formulas and neural networks.
8Under review as submission to TMLR
The model consisting of a stack of attention layers. Although our discussion focuses on the case that
contains a single attention layer, any attention layer Lin an attention stack fits our framework (see Fig 1).
In particular, all the attention layers closer to the input XthanLcan be grouped into the functions fpref
orfevd. For those layers that take the current layer’s output as input, we can group them into fout, where
cmay contain the outputs of other attention layers working in parallel.
Multi-head attention. For clarity, our derivation does not account for multi-head attention scenarios. In
essence, an n-head attention structure can be viewed as having ndistinct estimations of mean shift estimates.
Consequently, the outputs of n-head attention can be interpreted as the solutions to nunderlying convex
problems, which are subsequently stacked together at the end of the inference processes.
T5 transformer implicitly adopts the generalized attention structure. Recent studies in NLP have
shown that T5 (Raffel et al., 2020) can achieve state-of-the-art performance for many NLP benchmarks, in-
cluding text summarization, classification, question answering, etc. While their transformer implementations
are quite similar to the original transformer architecture (Vaswani et al., 2017; Devlin et al., 2019), they
adopt trainable relative position embeddings to replace the sinusoidal position signals.7The modification
provides the model with extra flexibility to encode the positional information with little computational cost.
We will see that in comparison to the original transformer implementation, T5 transformer can be seen as a
natural realization of the generalized attention in (16), where the preference weights uunifies the concepts of
word masks and T5’s positional encoding functions. Thus, the usefulness and the validity of our framework
are well-supported by the state-of-the-art performance of T5 in many NLP tasks (Raffel et al., 2020).
Consider the running example: filing in the blanks, with the preference distribution
u(ti) =/braceleftigg
0 if theithword is masked
exp(bj−i)/Zotherwise,(17)
whereZis a normalizing constant and bj−iis a trainable scalar that only depends on the relative position
of wordiand wordj(which is the kthmasked word that we are inferring). Substituting such uinto (16)
withα= 1yields
ˆh=n/summationdisplay
i=1tiexp (⟨ti,z⟩+bj−i+1masked (i))/summationtextn
l=1exp (⟨tl,z⟩+bj−l+1masked (l)), (18)
where 1masked (i)is an indicator function that equals −∞if wordiis masked and zero otherwise. The
expression in (18) has the same structure as that adopted in T5 transformer, where the indicator function
serves as the mask function to prevent the model from assigning weights to the masked words. In this way,
the concepts of word masks and the positional encoding functions are unified by uin (17). Conversely, T5
transformer is a realization of the generalized attention with the preference weights uspecified in (17).
Generalized attention structures suggested by the optimal solution. While T5 transformer has
implicitly adopted the generalized attention, (16) suggests potential for further generalizations. For instance,
in T5 transformer, the function that outputs the template’s preference weights only considers the word masks
and the word’s relative positions. This function could also be generalized to factor in the input sentence
contexts, and the output weights encode the importance of each word before giving the local information
storedin z. Thesameideacouldbeappliedtotheimagecaptioningexampletoreplacetheuniformpreference
weights. By adding a neural network taking the input image to generate non-uniform preference weights,
we devise a mechanism to estimate the importance of each part of the image before the caption generation.
In this way, the newly added network collects global information from the image to propose a preference
distribution, which could be updated locally based on the current generation stage encoded in z.
Besides, although we mainly focus on the case when uis discrete, we want to emphasize that the analysis
performed in Section 6 also covers continuous u. This hints that a continuous attention mechanism could
also be implemented, which might prove to be useful in some applications.
Moreover, our theoretical work enables the design of more general attention structures; for instance, KL-
divergence in the optimization problem (3) requires estimated distribution to share support with preference
7They also simplified the layer normalization (Lei Ba et al., 2016) for faster training and inference speed.
9Under review as submission to TMLR
0.00.51.0
0.074
Lay. 10.067
Lay. 20.019
Lay. 30.001
Lay. 40.000
Lay. 50.000
Lay. 6
0.0 0.50.00.51.0
0.000
Lay. 7
0.0 0.50.000
Lay. 8
0.0 0.50.000
Lay. 9
0.0 0.50.000
Lay. 10
0.0 0.50.000
Lay. 11
0.0 0.50.000
Lay. 12
(a) BERT
01Enc self0.029Layer 1
0.042Layer 2
0.061Layer 3
0.054Layer 4
0.050Layer 5
0.051Layer 6
01Dec self0.029 0.042 0.061 0.054 0.050 0.051
0.0 0.501Cross Attn0.057
0.0 0.50.062
0.0 0.50.071
0.0 0.50.069
0.0 0.50.074
0.0 0.50.075 (b) T5
Figure 4: The distribution of relative deviations∥λ∗−αz∥
∥λ∗∥for the attention in BERT and T5. The red vertical
lines mark the average of the errors.
distribution, whichmaynotbedesiredinmanytasks. (e.g. translation, wherethetargetshouldbeunaffected
if we replace some words in the source sentence with synonyms.) Using our theory, in Sec 9, we show that
this can be achieved by replacing KL divergence with an optimal transport (OT)-based measure that handles
word similarities in their embedding space.
8 Empirical evidence
To show the proposed optimization problem (3) indeed provides a principle justifying the design of attention
modules, we show that the maximizer λ∗of its dual problem (7) nearly coincides with its approximated
counterpart used in the pretrained BERT model (Devlin et al., 2019) and T5-small (Raffel et al., 2020).
Verification on other popular attention-based models yielded similar results.
Letxi∈Rdfori∈1,2...,n,yj∈Rdfori∈1,2...,mandK,Q,V∈Rd′×d. Thekthoutputs of BERT
attention and T5 are respectively,
BERT :n/summationdisplay
i=1Vxiexp/parenleftig
⟨Kxi,Qxk⟩/√
d′/parenrightig
/summationtextn
j=1exp/parenleftig
⟨Kxj,Qxk⟩/√
d′/parenrightig T5:n/summationdisplay
i=1Vxiuiexp/parenleftig
⟨Kxi,Qyk⟩/√
d′/parenrightig
/summationtextn
j=1exp/parenleftig
⟨Kxj,Qyk⟩/√
d′/parenrightig.(19)
Here, T5 has three types of attention, self-attentions in the encoder and the decoder and the cross-attention
connecting them. For the two self-attentions, xi=yiandm=n.
Following the reparameterization method used by Ramsauer et al. (2021), for BERT, setting α= 1,ti=xi√
d′,
z=K⊤Qxk,V′=V√
d′, andui∝1yieldsV′/summationtextn
i=1tiuiexp⟨ti,z⟩/summationtextn
j=1ujexp⟨tj,z⟩,where the summation part is the
one derived in (16).8Likewise, for T5, we use the same setting as BERT except that uiis computed based
on its positional encoding and z=K⊤Qykfor the cross-attention.
We findλ∗by plugging α,ui’s,ti’s and zinto (7) followed by performing gradient ascent. We then calculate
the relative deviation∥λ∗−αz∥
∥λ∗∥of its approximated counterpart αzand report its distribution in Fig 4 for
each attention layer by taking the average over the attention heads. We report the distributions for each
head in Appx C. As Fig 4 indicates, λ∗almost coincides with its approximated counterpart αzinferred by
BERT and T5. As a result, the T5 and BERT’s attention inference can be seen as solving the proposed
convex problem, which corroborates that problem (3) gives a principle justifying the design of attention.
8Templates tiabsorb the scaling factor d′−1
2so that their norms remain largely unchanged as d′increases. Thus, uhas
bounded moments, and Theorem 2 applies. Note that it is a common practice to scale outputs before performing theoretical
analysis (e.g., see the work of Arora et al. (2019)).
10Under review as submission to TMLR
9 An optimal transport-based attention
In Sec 7, we mentioned that our theoretical work enables the design of more general attention structures.
LetR+denote the set of non-negative real numbers. In this section, we provide an example by replacing the
KL-divergence in (3) with an entropy-regularized OT-based measure (Cuturi, 2013):
Wγ(p,u;M) = min
X∈U(p,u)⟨M,X⟩−γH(X), (20)
whereγ >0,H(X) =/summationtextN
i,j=1−XijlogXijis the entropy of X,U(p,u) ={X∈R|A|×|A|
+ ;X1=p,XT1=u}
andM∈R|A|×|A|
+is a cost matrix that measures the similarity between each pair of the templates in A.9
The entropy regularization makes the minimizer X∗in (20) change smoothly in terms of p,uandM, which
stabilizes and speeds up evaluation of W(Cuturi, 2013). When γ→0,M(t,t′) =dA(t,t′)ρ,W1/ρ
γis reduced
to the Wasserstein ρ-distance. We note that, due to the entropy term, for fixed uandM, the true preference
distribution ˜uthat minimizesWγ(˜u,u;M)is slightly deviated from uand will approach to uifγ→0. (see
Appx D for details.) Let ˜µdenote the expectation of ˜u. Then we can rewrite (3) as
min
pα
2/vextenddouble/vextenddouble/vextenddouble/vextenddouble(˜µ+z)−/integraldisplay
Rdap(a)da/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
+Wγ(p,u;M). (21)
Following a similar procedure presented in Sec 6 (the derivation is given in Appx D), we can derive and solve
its Fenchel dual problem and show that when both αandα
γare small, the minimizer p∗takes the form
p∗(t) =n/summationdisplay
i=1uiexp/parenleftbig/parenleftbig
αtTz−M(t,ti)/parenrightbig/slashbig
γ/parenrightbig
Zi(22)
withZi=/summationtext
t′∈Aexp/parenleftbig/parenleftbig
α(t′)Tz−M(t′,ti)/parenrightbig/slashbig
γ/parenrightbig
. Substituting (22) into (4), we get the OT-based attention.
The OT-based attention considers all templates in A.In comparison to the generalized attention
derived in Sec 6, the OT-based one assigns non-zero weights to all templates in A. To see how it works,
consider an extreme case in which the templates are partitioned into several groups. If two templates t,t′
belong to the same group, M(t,t′) = 0; otherwise, M(t,t) =∞. Moreover, templates within the same
groups are very similar in the sense that their inner products with zare approximately equal. Suppose ti
belongs to a group Gand other templates tj̸=ido not, then for all t∈G, we havep∗(t) =ui/|G|. That is,
all templates ofGshare the weight of tiand thus be potentially trained even if most of them do not appear
in the input. In general, if a template tis similar to some ti∈T(i.e.,M(t,ti)is small), it will share ti’s
weight although it does not appear in T. In contrast, for regular attention, only templates in Tcan be
assigned non-zero weights. The peculiar property of OT-based attention is desired in some practical tasks.
For example, in an NLP problem, synonyms intuitively have similar templates. Then, if a word appears in
the input sentence and is trained, its synonyms should be trained in a similar way and thus be assigned a
similar weight (because replacing a word with its synonym does not alter the input in a semantic sense).
Likewise, in the Vision Transformer (ViT) (Dosovitskiy et al., 2021), images are divided into small
patches, each of which is conceptually treated as a word. Consequently, an image composed of these
patches is analogous to a sentence. A multilayer transformer, similar to BERT, is then used to ex-
tract features from these patches. Finally, a special learnable token is incorporated to aggregate these
features (templates) using an attention mechanism, and the aggregated result is fed into a classi-
fier for image classification. Intuitively, images of the same class consist of visually similar patches
and replacing patches in an image with visually similar patches should not alter its class. Thus, it
is reasonable for the last attention layer to share the template sets for images of the same classes
and adopt the OT-based attention to train the templates associated with visually similar patches.
9A smallerMijimplies a larger similarity between tiandtj. While many OT-related problems define Mby embedding
templates into a metric space (A,dA)withM(t,t′) =dA(t,t′)ρ,ρ≥1, our discussion makes no assumption on Mother than
it is non-negative and symmetric, and M(t,t)<M (t′,t)for all t′̸=t.
11Under review as submission to TMLR
LR 3×10−33×10−43×10−5
ViT 0.228±0.010.463±0.01 0.452±0.01
OT-ViT 0.175±0.010.491±0.01 0.412±0.01
Table 1: Test accuracies of ViT and OT-ViT on CI-
FAR100 with various learning rates (LR).
Fashion-MNIST CIFAR10 CIFAR100
ViT 0.928±0.01 0.751±0.01 0.463±0.01
OT-ViT 0.937±0.01 0.772±0.020.491±0.01
Table 2: Test accuracies of ViT and OT-ViT on
Fashion-MNIST, CIFAR10 and CIFAR100.To corroborate our claims, we test the ViT and its
OT-based variant on Fashion-MNIST (Xiao et al.,
2017), CIFAR10 and CIFAR100 (Krizhevsky, 2009).
The OT-ViT model is identical to the ViT, except
that the final transformer layer is substituted with
OT-based attention where M(a,b) =−a⊤b+C,
α= 1andγ=√
hidden dim. ( Cis an upper bound
of all possible template pairs’ inner products, which
ensuresMis nonnegative.) To improve the train-
ing efficiency, when training OT-ViT, there is a 50%
chancethattheset Tconsistssolelyoftemplatesex-
tracted from the input image and a 50% chance that
Talso includes templates from another randomly
selected image of the same class. During testing, T
consists only of templates from the input image.
Throughout our experiments, we fixed the patch size to be 4×4and the dropout rate to be 0.2. To ensure a
fair and tractable comparison, we constrained both models to have 3.2M parameters. Under this constraint,
we traded off the number of layers and hidden dimensions of the Vision Transformer (ViT) model to achieve
the best performance on CIFAR100 (Krizhevsky, 2009). The study showed that a six-layer ViT model with
a hidden dimension of 512 had the optimal performance. We then used this setting for both the ViT and
OT-ViT models throughout the remaining experiments. (Note that for a fixed hidden dimension, the OT-
based attention has a nearly identical number of parameters to the regular transformer implementation.)
Similarly, we searched for the optimal learning rate (LR) of both models on CIFAR100 and reported the test
accuracy with the 95% confidence intervals in Table 1. The results indicate that both models achieved the
best performance when the learning rate was set to 3×10−4. We, therefore, used this learning rate selection
when training the models on the other datasets.
In Table 2, we compare the performances of ViT and OT-ViT on Fashion-MNIST (Xiao et al., 2017),
CIFAR10 and CIFAR100 (Krizhevsky, 2009) by reporting their test accuracies with the 95% confidence
interval. As demonstrated, OT-ViT consistently outperforms ViT, highlighting the effectiveness of OT-
based attention.
10 Conclusion
This paper presented a new perspective on understanding the attention mechanism by showing that it can
be viewed as a solver of a family of inference tasks. These tasks involve improving the noisy estimate of
a distribution p’s mean by a preference distribution that encodes some beliefs of p’s value. We have used
three running examples with the typical model architectures to show that such tasks naturally exist in
neural network design. We then abstracted a convex optimization problem from these tasks and derived
a closed-form approximation of the optimal solution by solving the problem’s Fenchel dual. We find that
the closed-form approximation can be seen as a generalized attention layer, and one of its special cases
is equivalent to the dot-product attention adopted in transformers. We further analyzed the general form
and showed that T5 transformer implicitly adopts the generalized attention structure with attention weights
unifying the concepts of the word masks and the positional encoding functions. We empirically show that our
framework can well-explain the attention inference in the pretrained BERT and T5 models. To demonstrate
the potential for designing more general attention structures, we replaced the KL divergence with an OT-
based measure, deriving an OT-based attention structure that removes the support constraints on p(k)
mentioned in the examples.
This paper also presents a principled justification for the design of attention modules in neural networks.
Specifically, there is a general assumption that because attention in humans narrows the search space, a
similar phenomenon is at play in transformers. In this paper, we have shown that the mechanism corresponds
to proposing a preference distribution over the templates, followed by adjusting it using a noisy mean shift
estimation. The generalized attention structure presented potentially opens the door to a wide design
12Under review as submission to TMLR
space. For example, the preference weights need not be derived from the positional encoding functions; they
could integrate a variety of information provided by other network components. Additionally, this research
successfully demonstrates a novel approach to analyze the functioning of a neural network component,
namely, via isolating the component from the complex network structure and asking: is there a “local
problem” that is solved by the design of this component?
Broader impact. This paper presents a new perspective on understanding attention and derives a gener-
alized attention structure. Our work is foundational, which we believe does not have direct negative societal
impacts. Due to the very wide range of applications of attention, such as self-driving (Kim & Canny, 2017),
healthcare (Ma et al., 2017) and protein interaction prediction (Tsubaki et al., 2018), we expect our works
can facilitate the algorithm developments in these areas, which may have unexpected impacts.
13Under review as submission to TMLR
References
SanjeevArora,SimonDu, WeiHu, ZhiyuanLi, andRuosongWang. Fine-grainedanalysisofoptimizationand
generalization for overparameterized two-layer neural networks. In International Conference on Machine
Learning (ICML) , pp. 322–332, 2019.
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural Machine Translation by Jointly Learning
to Align and Translate. In ICLR 2015 , pp. 1–15, 2014.
Pierre Baldi and Roman Vershynin. The quarks of attention, 2022. arXiv:2202.08371.
J. M. Borwein and A. S. Lewis. Partially finite convex programming, part i: Quasi relative interiors and
duality theory. Mathematical Programming , 57(1):15–48, 1992. doi: 10.1007/BF01581072. URL https:
//doi.org/10.1007/BF01581072 .
Gino Brunner, Yang Liu, Damián Pascual, Oliver Richter, Massimiliano Ciaramita, and Roger Wattenhofer.
On Identifiability in Transformers. In International Conference on Learning Representations (ICLR) ,
2019.
Kevin Clark, Urvashi Khandelwal, Omer Levy, and Christopher D. Manning. What Does BERT Look at?
An Analysis of BERT’s Attention. arXiv preprint arXiv:1906.04341 , 2019.
Marco Cuturi. Sinkhorn distances: Lightspeed computation of optimal transport. In C. J. C. Burges,
L. Bottou, M. Welling, Z. Ghahramani, and K. Q. Weinberger (eds.), Advances in Neural Information
Processing Systems , volume 26. Curran Associates, Inc., 2013. URL https://proceedings.neurips.cc/
paper/2013/file/af21d0c97db2e27e13572cbf59eb343d-Paper.pdf .
Marco Cuturi and Gabriel Peyre. A smoothed dual approach for variational wasserstein problems. SIAM
journal on imaging sciences , 9(1):320–343, 2016. ISSN 1936-4954.
Puneesh Deora, Rouzbeh Ghaderi, Hossein Taheri, and Christos Thrampoulidis. On the optimization and
generalizationofmulti-headattention. Transactions on Machine Learning Research , 2024. ISSN2835-8856.
URL https://openreview.net/forum?id=wTGjn7JvYK .
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirec-
tional transformers for language understanding. In Proceedings of the 2019 Conference of the North Amer-
ican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1
(Long and Short Papers) , pp. 4171–4186, Minneapolis, Minnesota, June 2019. Association for Computa-
tional Linguistics. doi: 10.18653/v1/N19-1423. URL https://www.aclweb.org/anthology/N19-1423 .
Yihe Dong, Jean-Baptiste Cordonnier, and Andreas Loukas. Attention is not all you need, pure attention
loses rank doubly exponentially with depth. 2021. URL https://arxiv.org/abs/2103.03404 .
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Un-
terthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil
Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In International
Conference on Learning Representations , 2021. URL https://openreview.net/forum?id=YicbFdNTTy .
Fabrice Gamboa. Methode du maximum d’entropie sur la moyenne et applications. Phd Thesis , 1989.
Michael Hahn. Theoretical Limitations of Self-Attention in Neural Sequence Models. Transactions of the
Association for Computational Linguistics , 8:156–171, 2020.
Jiri Hron, Yasaman Bahri, and Jascha Sohl-dickstein Roman. Infinite attention : NNGP and NTK for deep
attention networks. In International Conference on Machine Learning (ICML) , 2020.
Sarthak Jain and Byron C. Wallace. Attention is not Explanation. In Annual Conference of the North Amer-
ican Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-
HLT), 2019.
14Under review as submission to TMLR
Samy Jelassi, Michael Eli Sander, and Yuanzhi Li. Vision transformers provably learn spatial structure. In
Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (eds.), Advances in Neural Informa-
tion Processing Systems , 2022. URL https://openreview.net/forum?id=eMW9AkXaREI .
Jinkyu Kim and John Canny. Interpretable learning for self-driving cars by visualizing causal attention. In
2017 IEEE International Conference on Computer Vision (ICCV) , pp. 2961–2969, 2017. doi: 10.1109/
ICCV.2017.320.
Alex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, 2009.
JaehoonLee,JaschaSohl-dickstein,JeffreyPennington,RomanNovak,SamSchoenholz,andYasamanBahri.
Deep neural networks as gaussian processes. In International Conference on Learning Representations
(ICLR), 2018.
Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. Layer Normalization. arXiv e-prints , art.
arXiv:1607.06450, July 2016.
Hongkang Li, Meng Wang, Sijia Liu, and Pin-Yu Chen. A theoretical understanding of shallow vision
transformers: Learning, generalization, and sample complexity. In The Eleventh International Conference
on Learning Representations , 2023. URL https://openreview.net/forum?id=jClGv3Qjhb .
Haoye Lu, Yongyi Mao, and Amiya Nayak. On the dynamics of training attention models. In International
Conference on Learning Representations , 2021. URL https://openreview.net/forum?id=1OCTOShAmqB .
Thang Luong, Hieu Pham, and Christopher D. Manning. Effective approaches to attention-based neural
machine translation. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language
Processing , pp. 1412–1421, Lisbon, Portugal, September 2015. Association for Computational Linguistics.
doi: 10.18653/v1/D15-1166. URL https://www.aclweb.org/anthology/D15-1166 .
Fenglong Ma, Radha Chitta, Jing Zhou, Quanzeng You, Tong Sun, and Jing Gao. Dipole: Diagnosis
prediction in healthcare via attention-based bidirectional recurrent neural networks. In Proceedings of the
23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , KDD ’17, pp.
1903–1911, New York, NY, USA, 2017. Association for Computing Machinery. ISBN 9781450348874. doi:
10.1145/3097983.3098088. URL https://doi.org/10.1145/3097983.3098088 .
Sadegh Mahdavi, Renjie Liao, and Christos Thrampoulidis. Memorization capacity of multi-head attention
in transformers. In The Twelfth International Conference on Learning Representations , 2024. URL https:
//openreview.net/forum?id=MrR3rMxqqv .
P. McCullagh. Tensor Methods in Statistics : Monographs on Statistics and Applied Probability . Chapman
and Hall/CRC, Boca Raton, FL, first edition. edition, 1987. ISBN 9781351077118.
Paul Michel, Omer Levy, and Graham Neubig. Are sixteen heads really better than one? In Neural
Information Processing Systems (NIPS) , volume 32. Curran Associates, Inc., 2019.
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou,
Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer.
Journal of Machine Learning Research , 21(140):1–67, 2020. URL http://jmlr.org/papers/v21/20-074.
html.
HubertRamsauer, BernhardSchäfl, JohannesLehner, PhilippSeidl, MichaelWidrich, LukasGruber, Markus
Holzleitner, Thomas Adler, David Kreil, Michael K Kopp, Günter Klambauer, Johannes Brandstetter, and
Sepp Hochreiter. Hopfield networks is all you need. In International Conference on Learning Representa-
tions, 2021. URL https://openreview.net/forum?id=tL89RnzIiCd .
Gabriel Rioux, Rustum Choksi, Tim Hoheisel, Pierre Marechal, and Christopher Scarvelis. The maximum
entropy on the mean method for image deblurring. Inverse Problems , oct 2020. doi: 10.1088/1361-6420/
abc32e. URL https://doi.org/10.1088/1361-6420/abc32e .
15Under review as submission to TMLR
R. Tyrrell Rockafellar. Convex analysis . Princeton mathematical series ; 28. Princeton University Press,
Princeton, N.J, 1970. ISBN 0691080690.
Arda Sahiner, Tolga Ergen, Batu Ozturkler, John Pauly, Morteza Mardani, and Mert Pilanci. Unraveling
attention via convex duality: Analysis and interpretations of vision transformers. In Kamalika Chaudhuri,
Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato (eds.), Proceedings of the 39th
International Conference on Machine Learning , volume 162 of Proceedings of Machine Learning Research ,
pp. 19050–19088. PMLR, 17–23 Jul 2022. URL https://proceedings.mlr.press/v162/sahiner22a.
html.
Sofia Serrano and Noah A. Smith. Is attention interpretable? In Annual Meeting of the Association for
Computational Linguistics (ACL) , 2020.
Yao-Hung Hubert Tsai, Shaojie Bai, Makoto Yamada, Louis-Philippe Morency, and Ruslan Salakhutdinov.
Transformer dissection: An unified understanding for transformer’s attention via the lens of kernel. In
EMNLP, 2019.
Masashi Tsubaki, Kentaro Tomii, and Jun Sese. Compound–protein interaction prediction with end-to-end
learning of neural networks for graphs and sequences. Bioinformatics , 35(2):309–318, 07 2018.
Shikhar Vashishth, Shyam Upadhyay, Gaurav Singh Tomar, and Manaal Faruqui. Attention Interpretability
Across NLP Tasks. In International Conference on Learning Representations (ICLR) , 2020.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz Kaiser,
and Illia Polosukhin. Attention is all you need. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach,
R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Advances in Neural Information Processing Systems ,
volume 30. Curran Associates, Inc., 2017. URL https://proceedings.neurips.cc/paper/2017/file/
3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf .
Elena Voita, David Talbot, Fedor Moiseev, Rico Sennrich, and Ivan Titov. Analyzing multi-head self-
attention: Specialized heads do the heavy lifting, the rest can be pruned. In Proceedings of the 57th
Annual Meeting of the Association for Computational Linguistics , July 2019.
Yequan Wang, Minlie Huang, Xiaoyan Zhu, and Li Zhao. Attention-based LSTM for aspect-level sentiment
classification. In Empirical Methods in Natural Language Processing (EMNLP) , pp. 606–615, Austin,
Texas, November 2016. Association for Computational Linguistics.
Sarah Wiegreffe and Yuval Pinter. Attention is not not explanation. In 2019 Conference on Empirical
Methods in Natural Language Processing and 9th International Joint Conference on Natural Language
Processing (EMNLP-IJCNLP) , 2019.
Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: a novel image dataset for benchmarking
machine learning algorithms. CoRR, abs/1708.07747, 2017. URL http://arxiv.org/abs/1708.07747 .
K. Xu, J. Ba, R. Kiros, K. Cho, A. Courville, R. Salakhudinov, R. Zemel, and Y. Bengio. Show, Attend and
Tell: Neural Image Caption Generation with Visual Attention. In Proc. 32nd Int. Conf. Mach. Learn. ,
pp. 257–261, 2015. doi: 10.1109/EEEI.2002.1178445.
Greg Yang. Tensor Programs I : Wide Feedforward or Recurrent Neural Networks of Any Architecture are
Gaussian Processes. In Neural Information Processing Systems (NIPS) , 2019.
16Under review as submission to TMLR
A Derivation of (11) for preference distributions of bounded moments
Assume a preference distribution uhas bounded moments. Then its moment generating function
M(λ) =/integraldisplay
Rd⟨a,λ⟩u(a)da= 1 +⟨M′(0),λ⟩+1
2⟨λ,M′′(0)λ⟩+o(∥λ∥2), (23)
where
M′(0) =/integraldisplay
au(a)da=µ, (24)
M′′(0) =/integraldisplay
aa⊤u(a)da. (25)
Notice that
log(1 +x) =t−t2
2+t3
3−t4
4+···=t−t2
2+o(t2). (26)
Thus,
log(M(λ)) =/parenleftbigg
⟨M′(0),λ⟩+1
2⟨λ,M′′(0)λ⟩+o(∥λ∥2)/parenrightbigg
−1
2/parenleftbigg
⟨M′(0),λ⟩+1
2⟨λ,M′′(0)λ⟩+o(∥λ∥2)/parenrightbigg2
+o/parenleftbigg/parenleftbig
⟨M′(0),λ⟩+1
2⟨λ,M′′(0)λ⟩+o(∥λ∥2)/parenrightbig2/parenrightbigg
=⟨M′(0),λ⟩+1
2/parenleftig
⟨λ,M′′(0)λ⟩−⟨M′(0),λ⟩2/parenrightig
+o/parenleftbig
∥λ∥2/parenrightbig
=⟨µ,λ⟩+1
2λ⊤Σλ+o/parenleftbig
∥λ∥2/parenrightbig
,
where
Σ =M′′(0)−M′(0)M′(0)⊤=/integraldisplay
(a−µ) (a−µ)Tu(a)da.
17Under review as submission to TMLR
B Other perspectives to derive (3)
A Maximum Likelihood Perspective. The optimization problem in (3) can be derived using the max-
imum log likelihood method by treating the KL-divergence term as a regularizer. According to (2), the
difference (µ+z)−hfollows a Gaussian distribution N(0,σ2I). This implies the log likelihood function
ℓ(z)∝−1
2σ2∥(µ+z)−h∥2. Maximizing it with the KL-divergence term as a regularizer is the same as
minimizing
1
2σ2∥(µ+z)−h∥2+ηK(p,u), (27)
whereη>0controls the strength of the regularization. Substituting (1) into (27) followed by rearrangement
yields
min
p1
2ησ2/vextenddouble/vextenddouble/vextenddouble/vextenddouble(µ+z)−/integraldisplay
Rdap(a)da/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
+K(p,u), (28)
which is equivalent to (3) by setting α−1=ησ2.
A Bayesian perspective. Given observed data and prior belief about the distribution of parameters,
Bayesian inference allows us to update this distribution to reflect the new knowledge. Assume that the
distribution pis specified by parameters θ. By considering µ+zas the observed data, we will show that
picking the pθthat minimizes (3) is the same as choosing the θ∗that maximizes the posterior density of θ
given the observed data.
Letϑbe the parameters of the preference distribution uϑand suppose the prior distribution f(θ|ϑ)ofθ
satisfies
f(θ|ϑ)∝exp/parenleftig
−ηK(pθ,uϑ)/parenrightig
, (29)
whereη>0is a hyper-parameter that controls the decaying speed of the probability density as pθdeviates
fromuϑ.
In (2), we have assumed that given θ,(µ+z)−hθfollows a spherical Gaussian distribution N(0,σ2I), where
hθis the mean of pθ. Therefore, given its parameter θ, the probability density function of µ+zis
f(µ+z|θ) =f(µ+z|hθ)∝exp/parenleftbigg
−1
2σ2∥(µ+z)−hθ∥2/parenrightbigg
. (30)
Then the posterior distribution of θsatisfies
f(θ|µ+z,ϑ)∝f(µ+z|θ)f(θ|ϑ)
∝exp/parenleftbigg
−1
2σ2∥(µ+z)−hθ∥2−ηK(pθ,uϑ)/parenrightbigg
.
Findingθ∗that maximizes the posterior f(θ|µ+z,ϑ)is the same as finding
p∗
θ= argmin
pθ/braceleftbigg1
2σ2∥(µ+z)−hθ∥2+ηK/parenleftbig
pθ,uϑ/parenrightbig/bracerightbigg
= argmin
pθ/braceleftbigg1
2ησ2∥(µ+z)−hθ∥+K(pθ,uϑ)/bracerightbigg
,
which is equivalent to (3) by setting α−1=ησ2.
18Under review as submission to TMLR
C Extra experimental results
0.000.250.500.751.00Layer 10.092Head 1
0.040Head 2
0.034Head 3
0.093Head 4
0.102Head 5
0.093Head 6
0.052Head 7
0.033Head 8
0.069Head 9
0.040Head 10
0.074Head 11
0.142Head 12
0.000.250.500.751.00Layer 20.072 0.041 0.065 0.050 0.056 0.127 0.030 0.082 0.092 0.044 0.103 0.044
0.000.250.500.751.00Layer 30.022 0.016 0.015 0.024 0.023 0.013 0.026 0.020 0.012 0.019 0.016 0.017
0.000.250.500.751.00Layer 40.001 0.001 0.001 0.001 0.001 0.001 0.001 0.001 0.001 0.001 0.001 0.001
0.000.250.500.751.00Layer 50.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000
0.000.250.500.751.00Layer 60.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000
0.000.250.500.751.00Layer 70.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000
0.000.250.500.751.00Layer 80.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000
0.000.250.500.751.00Layer 90.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000
0.000.250.500.751.00Layer 100.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000
0.000.250.500.751.00Layer 110.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000
0.00 0.25 0.500.000.250.500.751.00Layer 120.000
0.00 0.25 0.500.000
0.00 0.25 0.500.000
0.00 0.25 0.500.000
0.00 0.25 0.500.000
0.00 0.25 0.500.000
0.00 0.25 0.500.000
0.00 0.25 0.500.000
0.00 0.25 0.500.000
0.00 0.25 0.500.000
0.00 0.25 0.500.000
0.00 0.25 0.500.000
Figure 5: The distribution of relative errors∥λ∗−αz∥
∥λ∗∥for the attention in BERT. The red vertical lines mark
the average of the errors.
19Under review as submission to TMLR
0.00.51.0Layer 10.038Head 1
0.025Head 2
0.038Head 3
0.020Head 4
0.022Head 5
0.029Head 6
0.028Head 7
0.030Head 8
0.00.51.0Layer 20.028 0.031 0.028 0.036 0.031 0.040 0.039 0.041
0.00.51.0Layer 30.044 0.044 0.046 0.054 0.043 0.051 0.060 0.052
0.00.51.0Layer 40.038 0.038 0.039 0.042 0.036 0.042 0.046 0.043
0.00.51.0Layer 50.036 0.039 0.036 0.043 0.035 0.040 0.042 0.043
0.0 0.50.00.51.0Layer 60.037
0.0 0.50.039
0.0 0.50.034
0.0 0.50.046
0.0 0.50.039
0.0 0.50.043
0.0 0.50.046
0.0 0.50.046
Figure 6: The distribution of relative errors∥λ∗−αz∥
∥λ∗∥for the self-attention of the encoder in T5. The red
vertical lines mark the average of the errors.
0.00.51.0Layer 10.016Head 1
0.026Head 2
0.023Head 3
0.018Head 4
0.018Head 5
0.023Head 6
0.028Head 7
0.019Head 8
0.00.51.0Layer 20.017 0.030 0.037 0.026 0.034 0.033 0.028 0.026
0.00.51.0Layer 30.014 0.029 0.033 0.027 0.029 0.030 0.029 0.030
0.00.51.0Layer 40.018 0.034 0.034 0.033 0.032 0.036 0.036 0.037
0.00.51.0Layer 50.019 0.038 0.035 0.031 0.033 0.033 0.034 0.030
0.0 0.50.00.51.0Layer 60.041
0.0 0.50.056
0.0 0.50.058
0.0 0.50.054
0.0 0.50.058
0.0 0.50.061
0.0 0.50.056
0.0 0.50.055
Figure 7: The distribution of relative errors∥λ∗−αz∥
∥λ∗∥for the self-attention of the decoder in T5. The red
vertical lines mark the average of the errors.
20Under review as submission to TMLR
0.000.250.500.75Layer 10.087Head 1
0.081Head 2
0.083Head 3
0.071Head 4
0.092Head 5
0.090Head 6
0.077Head 7
0.093Head 8
0.000.250.500.75Layer 20.072 0.075 0.083 0.072 0.073 0.076 0.072 0.072
0.000.250.500.75Layer 30.071 0.074 0.076 0.077 0.073 0.070 0.068 0.062
0.000.250.500.75Layer 40.066 0.076 0.073 0.077 0.069 0.064 0.069 0.058
0.000.250.500.75Layer 50.078 0.087 0.081 0.080 0.076 0.067 0.079 0.071
0.0 0.50.000.250.500.75Layer 60.076
0.0 0.50.077
0.0 0.50.089
0.0 0.50.082
0.0 0.50.070
0.0 0.50.070
0.0 0.50.087
0.0 0.50.078
Figure 8: The distribution of relative errors∥λ∗−αz∥
∥λ∗∥for the cross-attention in T5. The red vertical lines
mark the average of the errors.
D Details on the derivation of OT-based attention
According to the discussion in Sec 9, we consider the optimization problem
p∗= argmin
pα
2/vextenddouble/vextenddouble/vextenddouble/vextenddouble(˜µ+z)−/integraldisplay
Rdap(a)da/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
+Wγ(p,u;M) (31)
where ˜µdenotes the mean of the true preference distribution ˜uthat minimizes f(p) =Wγ(p,u;M). We will
show in Prop 1 that
˜µ=/summationdisplay
t,t′∈A×Au(t′)exp (−M(t,t′)/γ)/summationtext
t′′∈Aexp (−M(t′′,t′)/γ)t. (32)
Cuturi and Peyre Cuturi & Peyre (2016) proved that the Fenchel dual of Wγ(d;r,M)is
W∗
γ(p;u,M ) =γ/parenleftigg
H(u) +/summationdisplay
t∈Au(t) log/bracketleftigg/summationdisplay
t′∈Aexp/parenleftig
γ−1/parenleftbig
p(t)−M(t,t′)/parenrightbig/parenrightig/bracketrightigg/parenrightigg
(33)
forp∈RN; and, for t∈A
/bracketleftbig
∇pW∗
γ(p;u,M )/bracketrightbig
t=/summationdisplay
t′∈Au(t′) exp/parenleftig
γ−1/parenleftbig
p(t)−M(t,t′)/parenrightbig/parenrightig
/summationtext
t′′∈Aexp/parenleftig
γ−1/parenleftbig
p(t′′)−M(t′,t′′)/parenrightbig/parenrightig, (34)
where/bracketleftbig
∇pW∗
γ(p;u,M )/bracketrightbig
tdenote the entry in/bracketleftbig
∇pW∗
γ(p;u,M )/bracketrightbig
that is associated to template t. By the
Fenchel’s duality theorem, we know that p∗in (31) takes the form
p∗(t) =/summationdisplay
t′∈Au(t′) exp/parenleftig
γ−1/parenleftbig
t⊤λ∗−M(t,t′)/parenrightbig/parenrightig
/summationtext
t′′∈Aexp/parenleftig
γ−1/parenleftbig
(t′′)⊤λ∗−M(t′,t′′)/parenrightbig/parenrightig, (35)
21Under review as submission to TMLR
where
λ∗= arg max
λ∈Rd⟨˜µ+z,λ⟩−1
2α∥λ∥2−W∗
γ/parenleftbig
[tTλ|t∈A];u,M/parenrightbig
= arg max
λ∈Rd⟨˜µ+z,λ⟩−1
2α∥λ∥2−γ/summationdisplay
t∈Au(t) log/bracketleftigg/summationdisplay
t′∈Aexp/parenleftbigg(t′)⊤λ−M(t,t′)
γ/parenrightbigg/bracketrightigg
.(36)
The true preference distribution. The Fenchel dual perspective allows us to derive a closed-form
expression for the minimizer of f(p) =Wγ(p,u;M), which we refer as the true preference distribution ˜uin
the main text. We will also show that ˜uapproaches to the preference uasγ→0.
Notice that, by definition, ˜u→p∗whenα→0in (31). In this case, the optimization of λin (36) gets an
infinite penalty on its L2 norm and thus ∥λ∗∥2= 0. Therefore, we have
Proposition 1. Wγ(p;u,M )has the minimizer ˜u(t)taking the form
˜u(t) =/summationdisplay
t′∈Au(t′)exp (−M(t,t′)/γ)/summationtext
t′′∈Aexp (−M(t′,t′′)/γ), (37)
fort∈A. Besides, its mean
˜µ=/summationdisplay
t,t′∈A×Au(t′)exp (−M(t,t′)/γ)/summationtext
t′′∈Aexp (−M(t′′,t′)/γ)t. (38)
Whenγ→0,exp(−M(t,t′)/γ)/summationtext
t′′∈Aexp(−M(t′,t′′)/γ)approaches to 1ift=t′and0otherwise. Therefore, ˜u(t)→u(t)for
allt∈A.
The derivation of (22).Then we show how to derive (22) when αandα
γare assumed small.
Within the summation term of (36), for a fixed t
log/bracketleftigg/summationdisplay
t′∈Aexp/parenleftbigg(t′)⊤λ−M(t,t′)
γ/parenrightbigg/bracketrightigg
= log/bracketleftigg/summationdisplay
t′∈Aexp/parenleftbigg−M(t,t′)
γ/parenrightbigg
exp/parenleftbigg(t′)⊤λ
γ/parenrightbigg/bracketrightigg
= log/bracketleftigg/summationdisplay
t′∈Aqt(t′)Z(t) exp/parenleftbigg(t′)⊤λ
γ/parenrightbigg/bracketrightigg
= log/bracketleftigg/summationdisplay
t′∈Aqt(t′) exp/parenleftbigg(t′)⊤λ
γ/parenrightbigg/bracketrightigg
+ logZ(t)
= logMt(λ/γ) + logZ(t), (39)
where
qt(t′) = exp/parenleftbigg−M(t,t′)
γ/parenrightbigg/slashbigg
Z(t),
Z(t) =/summationdisplay
t′∈Aexp/parenleftbigg−M(t,t′)
γ/parenrightbigg
,
andMtdenotes the MGF of qt.
Note that logMt(λ/γ)is called the cumulant of qtand has the expansion
logMt(λ/γ) =µ⊤
t(λ/γ) +1
2(λ/γ)⊤Σt(λ/γ) +O(∥λ/γ∥3), (40)
where
µt=/summationdisplay
t′∈Aqt(t′)t′(41)
22Under review as submission to TMLR
and
Σt=/summationdisplay
t∈Aqt(t′) (t′−µt)(t′−µt)⊤(42)
respectively denote the mean and the variance-covariance matrix of qt.
Substituting (39) and (40) into (36) yields
λ∗= arg max
λ∈Rd⟨˜µ+z,λ⟩−1
2α∥λ∥2
−γ/bracketleftigg/parenleftig/summationdisplay
t∈Au(t)µt/parenrightig⊤
(λ/γ) +1
2/summationdisplay
t∈Au(t)/parenleftbig
(λ/γ)⊤Σt(λ/γ)/parenrightbig
+O(∥λ/γ∥3) +/summationdisplay
t∈Au(t) logZ(t)/bracketrightigg
= arg max
λ∈Rd⟨˜µ+z,λ⟩−1
2α∥λ∥2
−γ/bracketleftigg
(/summationdisplay
t∈Au(t)µt)⊤(λ/γ) +1
2/summationdisplay
t∈Au(t)/parenleftbig
(λ/γ)TΣt(λ/γ)/parenrightbig
+O(∥λ/γ∥3)/bracketrightigg
= arg max
λ∈Rd⟨˜µ+z,λ⟩−1
2α∥λ∥2
−/bracketleftigg
/parenleftbig/summationdisplay
iu(t)µt/parenrightbig⊤λ+1
2γ/summationdisplay
t∈Au(t)/parenleftbig
λ⊤Σtλ/parenrightbig
+γO(∥λ/γ∥3)/bracketrightigg
.
Whenαis assumed to be small, the optimization of λgets a large penalty on its L2 norm and thus, ∥λ∗∥2
is close to zero. So we have
λ∗≈arg max
λ∈Rd⟨˜µ+z,λ⟩−1
2α∥λ∥2
−/bracketleftigg/parenleftig/summationdisplay
t∈Au(t)µt/parenrightig⊤
λ+1
2γ/summationdisplay
t∈Au(t)/parenleftbig
λ⊤Σtλ/parenrightbig/bracketrightigg
Taking the derivative in terms of λand setting it to zero yields
(˜µ+z)−1
αλ∗−/summationdisplay
t∈Au(t)µt−1
γ/summationdisplay
t∈Au(t)Σtλ∗= 0.
As
/summationdisplay
t∈Au(t)µt=N/summationdisplay
i=1u(t)/summationdisplay
t′∈Aqt(t′)t′=/summationdisplay
t,t′∈A×Au(t)exp (−M(t,t′)/γ)/summationtext
t′′∈Aexp (−M(t,t′′)/γ)t′=˜µ, (43)
we also have
z−/parenleftigg
1
αId+1
γ/summationdisplay
t∈Au(t)Σt/parenrightigg
λ∗= 0.
That is,
λ∗=/parenleftigg
1
αId+1
γ/summationdisplay
t∈Au(t)Σt/parenrightigg−1
z
=/parenleftigg
Id+α
γ/summationdisplay
t∈Au(t)Σt/parenrightigg−1
(αz).
Whenα
γis small, the expression becomes simply
λ∗=αz.
23Under review as submission to TMLR
Plugging it into (35), we get
p∗(t) =/summationdisplay
t′∈Au(t′) exp/parenleftig
γ−1/parenleftbig
αt⊤z−M(t,t′)/parenrightbig/parenrightig
/summationtext
t′′∈Aexp/parenleftig
γ−1/parenleftbig
α(t′′)⊤z−M(t′,t′′)/parenrightbig/parenrightig, (44)
which is (22).
24