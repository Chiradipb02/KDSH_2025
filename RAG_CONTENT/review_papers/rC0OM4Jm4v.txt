GeNIe: Generative Hard Negative Images
Through Diffusion
Anonymous Author(s)
Affiliation
Address
email
Abstract
Data augmentation is crucial in training deep models, preventing them from over- 1
fitting to limited data. Recent advances in generative AI, e.g., diffusion mod- 2
els, have enabled more sophisticated augmentation techniques that produce data 3
resembling natural images. We introduce GeNIe a novel augmentation method 4
which leverages a latent diffusion model conditioned on a text prompt to combine 5
two contrasting data points (an image from the source category and a text prompt 6
from the target category) to generate challenging augmentations. To achieve this, 7
we adjust the noise level (equivalently, number of diffusion iterations) to ensure 8
the generated image retains low-level and background features from the source 9
image while representing the target category, resulting in a hard negative sample 10
for the source category. We further automate and enhance GeNIe by adaptively 11
adjusting the noise level selection on a per image basis (coined as GeNIe-Ada ), 12
leading to further performance improvements. Our extensive experiments, in both 13
few-shot and long-tail distribution settings, demonstrate the effectiveness of our 14
novel augmentation method and its superior performance over the prior art. 15
1 Introduction 16
Augmentation has become an integral part of training deep learning models, particularly when faced 17
with limited training data. For instance, when it comes to image classification with limited number 18
of samples per class, model generalization ability can be significantly hindered. Simple transfor- 19
mations like rotation, cropping, and adjustments in brightness artificially diversify the training set, 20
offering the model a more comprehensive grasp of potential data variations. Hence, augmentation 21
can serve as a practical strategy to boost the model’s learning capacity, minimizing the risk of overfit- 22
ting and facilitating effective knowledge transfer from limited labelled data to real-world scenarios. 23
Various image augmentation methods, encompassing standard transformations, and learning-based 24
approaches have been proposed [16, 15, 110, 111, 100]. Some augmentation strategies combine two 25
images possibly from two different categories to generate a new sample image. The simplest ones 26
in this category are MixUp [111] and CutMix [110] where two images are combined in the pixel 27
space. However, the resulting augmentations often do not lie within the manifold of natural images 28
and act as out-of-distribution samples that will not be encountered during testing. 29
Recently, leveraging generative models for data augmentation has gained an upsurge of attention 30
[100, 83, 63, 35]. These interesting studies, either based on fine-tuning or prompt engineering of 31
diffusion models, are mostly focused on generating generic augmentations without considering the 32
impact of other classes and incorporating that information into the generative process for a classifi- 33
cation context. We take a different approach to generate challenging augmentations near the decision 34
boundaries of a downstream classifier. Inspired by diffusion-based image editing methods [67, 63] 35
some of which are previously used for data augmentation, we propose to use conditional latent dif- 36
Submitted to 38th Conference on Neural Information Processing Systems (NeurIPS 2024). Do not distribute.Figure 1: Generative Hard N egative I mage s Through Diffusion ( GeNIe ):generates hard negative images
that belong to the target category but are similar to the source image from low-level feature and contextual
perspectives. GeNIe starts from a source image passing it through a partial noise addition process, and condi-
tioning it on a different target category. By controlling the amount of noise, the reverse latent diffusion process
generates images that serve as hard negatives for the source category.
fusion models [81] for generating hard negative images. Our core idea (coined as GeNIe ) is to 37
sample source images from various categories and prompt the diffusion model with a contradictory 38
text corresponding to a different target category. We demonstrate that the choice of noise level (or 39
equivalently number of iterations) for the diffusion process plays a pivotal role in generating images 40
that semantically belong to the target category while retaining low-level features from the source 41
image. We argue that these generated samples serve as hard negatives [108, 65] for the source cat- 42
egory (or from a dual perspective hard positives for the target category). To further enhance GeNIe , 43
we propose an adaptive noise level selection strategy (dubbed as GeNIe-Ada ) enabling it to adjust 44
noise levels automatically per sample. 45
To establish the impact of GeNIe , we focus on two challenging scenarios: long-tail andfew-shot 46
settings. In real-world applications, data often follows a long-tail distribution, where common sce- 47
narios dominate and rare occurrences are underrepresented. For instance, a person jaywalking a 48
highway causes models to struggle with such unusual scenarios. Combating such a bias or lack of 49
sufficient data samples during model training is essential in building robust models for self-driving 50
cars or surveillance systems, to name a few. Same challenge arises in few-shot learning settings 51
where the model has to learn from only a handful of samples. Our extensive quantitative and qual- 52
itative experimentation, on a suite of few-shot and long-tail distribution settings, corroborate the 53
effectiveness of the proposed novel augmentation method ( GeNIe ,GeNIe-Ada ) in generating hard 54
negatives, corroborating its significant impact on categories with a limited number of samples. A 55
high-level sketch of GeNIe is illustrated in Fig. 1. Our main contributions are summarized below: 56
- We introduce GeNIe , a novel yet elegantly simple diffusion-based augmentation method to cre- 57
ate challenging augmentations in the manifold of natural images. For the first time, to our best 58
knowledge, GeNIe achieves this by combining two sources of information (a source image, and a 59
contradictory target prompt) through a noise-level adjustment mechanism. 60
- We further extend GeNIe by automating the noise-level adjustment strategy on a per-sample basis 61
(called GeNIe-Ada ), to enable generating hard negative samples in the context of image classifica- 62
tion, leading also to further performance enhancement. 63
- To substantiate the impact of GeNIe , we present a suit of quantitative and qualitative results in- 64
cluding extensive experimentation on two challenging tasks: few-shot and long tail distribution 65
settings corroborating that GeNIe (and its extension GeNIe-Ada ) significantly improve the down- 66
stream classification performance. 67
2 Related Work 68
Data Augmentations. Simple flipping, cropping, colour jittering, and blurring are some forms of 69
image augmentations [91]. These augmentations are commonly adopted in training deep learning 70
models. However, using these data augmentations is not trivial in some domains. For example, 71
using blurring might remove important low-level information from medical images. More advanced 72
2approaches, such as MixUp [111] and CutMix [110], mix images and their labels accordingly [37, 73
59, 47, 17]. However, the resulting augmentations are not natural images anymore, and thus, act 74
as out-of-distribution samples that will not be seen at test time. Another strand of research tailors 75
the augmentation strategy through a learning process to fit the training data [23, 16, 15]. Unlike the 76
above methods, we propose to utilize pre-trained latent diffusion models to generate hard negatives 77
(in contrast to generic augmentations) through a noise adaptation strategy discussed in Section 3. 78
Data Augmentation with Generative Models. Using synthesized images from generative models 79
to augment training data has been studied before in many domains [30, 86], including domain adap- 80
tation [41], visual alignment [71], and mitigation of dataset bias [88, 36, 73]. For example, [73] 81
introduces a methodology aimed at enhancing test set evaluation through augmentation. While pre- 82
vious methods predominantly relied on GANs [114, 51, 101] as the generative model, more recent 83
studies promote using diffusion models to augment the data [81, 35, 89, 100, 4, 62, 83, 42, 28, 26, 8]. 84
More specifically, [100, 83, 35, 4] study the effectiveness of text-to-image diffusion models in data 85
augmentation by diversification of each class with synthetic images. [100] leverages a text-to-image 86
diffusion model and fine-tunes it on the downstream dataset using textual-inversion [31] to increase 87
the diversity of existing samples. [83] also utilizes a text-to-image diffusion model, but with a BLIP 88
[53] model to generate meaningful captions from the existing images. [42] utilizes diffusion models 89
for augmentation to correct model mistakes. [28] uses CLIP [76] to filter generated images. [26] 90
utilizes text-based diffusion and a large language model (LLM) to diversify the training data. [8] 91
uses an LLM to generate text descriptions of failure modes associated with spurious correlations, 92
which are then used to generate synthetic data through generative models. The challenge here is that 93
the LLM has little understanding of such failure scenarios and contexts. 94
We take a completely different approach here, without replying on any extra source of information 95
(e.g., through an LLM). Inspired by image editing approaches such as Boomerang [63] and SDEdit 96
[67], we propose to adaptively guide a latent diffusion model to generate hard negatives images 97
[65, 108] on a per-sample basis per category. In a nutshell, the aforementioned studies focus on im- 98
proving the diversity of each class with effective prompts and diffusion models, however, we focus 99
on generating effective hard negative samples for each class by combining two sources of contra- 100
dicting information (images from the source category and text prompt from the target category). 101
Language Guided Recognition Models. Vision-Language foundation models (VLMs) [2, 76, 81, 102
84, 77, 78] utilize human language to guide the generation of images or to extract features from 103
images that are aligned with human language. For example, CLIP [76] shows decent zero-shot 104
performance on many downstream tasks by matching images to their text descriptions. Some recent 105
works improve the utilization of human language in the prompt [25, 72], and others use a diffusion 106
model directly as a classifier [49]. Similar to the above, we use a foundation model (Stable Diffusion 107
1.5 [81]) to improve the downstream task. Concretely, we utilize category names of the downstream 108
tasks to augment their associate training data with hard negative samples. 109
Few-Shot Learning. In Few-shot Learning (FSL), we pre-train a model with abundant data to learn 110
a rich representation, then fine-tune it on new tasks with only a few available samples. In supervised 111
FSL [10, 1, 74, 109, 27, 54, 95, 116, 92], pretraining is done on a labeled dataset, whereas in 112
unsupervised FSL [43, 103, 61, 75, 3, 46, 39, 66, 90] the pre-training has to be conducted on an 113
unlabeled dataset. We assess the impact of GeNIe on a number of few-shot scenarios and state-of- 114
the-art baselines by accentuating on its impact on the few-shot inference stage. 115
3 Proposed Method: GeNIe 116
Given a source image XSfrom category S = <source category >, we are interested in generating a 117
target image Xrfrom category T=<target category >. In doing so, we intend to ensure the low- 118
level visual features or background context of the source image are preserved, so that we generate 119
samples that would serve as hard negatives for the source image. To this aim, we adopt a conditional 120
latent diffusion model (such as Stable Diffusion, [81]) conditioned on a text prompt of the following 121
format “A photo of a T=<target category >”. 122
Key Idea. GeNIe in its basic form is a simple yet effective augmentation sample generator for 123
improving a classifier fθ(.)with the following two key aspects: (i) inspired by [63, 67] instead of 124
adding the full amount of noise σmax and going through all Nmax (being typically 50) steps of 125
denoising, we use less amount of noise ( rσmax, with r∈(0,1)) and consequently fewer number 126
of denoising iterations ( ⌊rNmax⌋); (ii) we prompt the diffusion model with a Pmandating a target 127
3Figure 2: Effect of noise ratio, r, inGeNIe :we employ GeNIe to generate augmentations for the target classes
(motorcycle and cat) with varying r. Smaller ryields images closely resembling the source semantics, creating
an inconsistency with the intended target label. By tracing rfrom0to1, augmentations gradually transition
from source image characteristics to the target category. However, a distinct shift from the source to the target
occurs at a specific rthat may vary for different source images or target categories. For more examples, please
refer to Fig. A4.
category Tdifferent than the source S. Hence, we denote the conditional diffusion process as 128
Xr=STDiff (XS, P, r). In such a construct, the proximity of the final decoded image Xrto the 129
source image XSor the target category defined through the text prompt Pdepends on r. Hence, by 130
controlling the amount of noise, we can generate images that blend characteristics of both the text 131
prompt Pand the source image XS. If we do not provide much of visual details in the text prompt 132
(e.g., desired background, etc.), we expect the decoded image Xrto follow the details of XSwhile 133
reflecting the semantics of the text prompt P. We argue, and demonstrate later, that the newly 134
generated samples can serve as hard negative examples for the source category Ssince they share 135
the low-level features of XSwhile representing the semantics of the target category, T. Notably, the 136
source category Scan be randomly sampled or be carefully extracted from the confusion matrix of 137
fθ(.)based on real training data. The latter might result in even harder negative samples being now 138
cognizant of model confusions. Finally, we will append our initial dataset with the newly generated 139
hard negative samples through GeNIe and (re)train the classifier model. 140
Enhancing GeNIe :GeNIe-Ada .One of the remarkable aspects of GeNIe lies in its simple applica- 141
tion, requiring only XS,P, andr. However, selecting the appropriate value for rposes a challenge 142
as it profoundly influences the outcome. When ris small, the resulting Xrtends to closely resemble 143
XS, and conversely, when ris large (closer to 1), it tends to resemble the semantics of the target 144
category. This phenomenon arises because a smaller noise level restricts the capacity of the diffusion 145
model to deviate from the semantics of the input XS. Thus, a critical question emerges: how can we 146
select rfor a particular source image to generate samples that preserve the low-level semantics of 147
the source category SinXSwhile effectively representing the semantics of the target category T? 148
We propose a method to determine an ideal value for r. 149
Our intuition suggests that by varying the noise ratio rfrom 0to1,Xrwill progressively resemble 150
category Sin the beginning and category Ttowards the end. However, somewhere between 0 151
and1,Xrwill undergo a rapid transition from category StoT. This phenomenon is empirically 152
observed in our experiments with varying r, as depicted in Fig. 2. Although the exact reason for this 153
rapid change remains uncertain, one possible explanation is that the intermediate points between 154
two categories reside far from the natural image manifold, thus, challenging the diffusion model’s 155
capability to generate them. Ideally, we should select rcorresponding to just after this rapid semantic 156
transition, as at this point, Xrexhibits the highest similarity to the source image while belonging to 157
the target category. 158
We propose to trace the semantic trajectory between XSandXTthrough the lens of the classifier 159
fθ(.). As shown in Algorithm 1, assuming access to the classifier backbone fθ(.)and at least one 160
example XTfrom the target category, we convert both XSandXTinto their respective latent vectors 161
ZSandZTby passing them through fθ(.). Then, we sample Mvalues for runiformly distributed 162
∈(0,1), generating their corresponding Xrand their latent vectors Zrfor all those r. Subsequently, 163
we calculate dr=(Zr−ZS)T(ZT−ZS)
||ZT−ZS||2as the distance between ZrandZSprojected onto the vector 164
connecting ZSandZT. Our hypothesis posits that the rapid semantic transition corresponds to a 165
sharp change in this projected distance. Therefore, we sample nvalues for runiformly distributed 166
4Algorithm 1: GeNIe-Ada
Require: XS,XT,fθ(.),STDiff (.),M
Extract ZS←fθ(Xs),ZT←fθ(XT)
form∈[1, M]do
r←m
M,Zr←fθ(STDiff (X, P, r ) )
dm←(Zr−ZS)T(ZT−ZS)
||ZT−ZS||2
m∗←argmaxm|dm−dm−1|,∀m∈[2, M]
r∗←m∗
nReturn: Xr∗=STDiff (XS, P, r∗)
Figure 3: GeNIe-Ada: To choose radaptively for each (source image, target category) pair, we propose tracing
the semantic trajectory from ZS(source image embeddings) to ZT(target embeddings) through the lens of the
classifier fθ(·)(Algorithm 1). We adaptively select the sample right after the largest semantic shift.
between 0and1, and analyze the variations in dr. We identify the largest gap in drand select the r 167
value just after the gap when increasing r, as detailed in Algorithm 1 and illustrated in Fig. 3. 168
4 Experiments 169
Since the impact of augmentation is more pronounced when the training data is limited, we evaluate 170
the impact of GeNIe on Few-Shot classification in Section 4.1, Long-Tailed classification in Sec- 171
tion 4.2, and fine-grained classification in Section A.2. For GeNIe-Ada in all scenarios, we utilize 172
GeNIe to generate augmentations from the noise level set {0.5,0.6,0.7,0.8,0.9}. The selection of 173
the appropriate noise level per source image and target is adaptive, achieved through Algorithm 1. 174
Baselines. We use Stable Diffusion 1.5 [81] as our base diffusion model. In all settings, 175
we use the same prompt format to generate images for the target class: i.e., “A photo of a 176
<target category >”, where we replace the target category with the target category label. 177
We generate 512×512images for all methods. For fairness in comparison, we generate the same 178
number of new images for each class. We use a single NVIDIA RTX 3090 for image generation. 179
We consider 4diffusion-based baselines and a suite of traditional data augmentation baselines: 180
Img2Img [63, 67]: We sample an image from a target class, add noise to its latent representation and 181
then pass it along with a prompt for the target category through reverse diffusion. The focus here is 182
on a target class for which we generate extra positive samples. Adding large amount of noise leads 183
to generating an image less similar to the original image. We use two different noise magnitudes for 184
this baseline: r= 0.3andr= 0.7and denote them by Img2ImgLandImg2ImgH, respectively. 185
Txt2Img [4, 35]: For this baseline, we omit the forward diffusion process and only use the reverse 186
process starting from a text prompt for the target class of interest. This is similar to the base text- 187
to-image generation strategy adopted in [81, 35, 89, 4, 62]. Fig. 4 illustrates a set of generated 188
augmentation examples for Txt2Img ,Img2Img , and GeNIe . 189
DAFusion [100]: In this method, an embedding is optimized with a set of images for each class to 190
correspond to the classes in the dataset. This approach is introduced in Textual Inversion [32]. We 191
optimize an embedding for 5000 iterations for each class in the dataset, followed by augmentation 192
similar as the DAFusion method. 193
Cap2Aug[83]: It is a recent diffusion-based data augmentation strategy that uses image captions as 194
text prompts for an image-to-image diffusion model. 195
Traditional Data Augmentation: We consider both weak and strong traditional augmentations. 196
More specifically, for weak augmentation we use random resize crop with scaling ∈[0.2,1.0]and 197
horizontal flipping. For strong augmentation, we consider random color jitter, random grayscale, 198
and Gaussian blur. For the sake of completeness, we also compare against data augmentations such 199
as CutMix [110] and MixUp [111] that combine two images together. 200
4.1 Few-shot Classification 201
We assess the impact of GeNIe compared to other augmentations in a number of few-shot classifica- 202
tion (FSL) scenarios, where the model has to learn only from the samples contained in the ( N-way, 203
K-shot) support set and infer on the query set. Note that this corresponds to an inference-only FSL 204
5Figure 4: Visualization of Generative Samples: We compare GeNIe with two baselines: Img2ImgLaug-
mentation: both image and text prompt are from the same category. Adding noise does not change the image
much, so they are not hard examples. Txt2Img augmentation: We simply use the text prompt only to generate
an image for the desired category (e.g., using a text2image method). Such images may be far from the domain
of our task since the generation is not informed by any visual data from our task. GeNIe augmentation: We
use the target category name in the text prompt only along with the source image.
setting where a pretraining stage on an abundant dataset is discarded. The goal is to assess how well 205
the model can benefit from the augmentations while keeping the original N×Ksamples intact. 206
Datasets. We conduct our few-shot experiments on two most commonly adopted few-shot classi- 207
fication datasets: mini- Imagenet [79] and tiered- Imagenet [80]. mini- Imagenet is a subset of Ima- 208
geNet [22] for few-shot classification. It contains 100classes with 600samples each. We follow 209
the predominantly adopted settings of [79, 10] where we split the entire dataset into 64classes for 210
training, 16for validation and 20for testing. tiered -Imagenet is a larger subset of ImageNet with 211
608classes and a total of 779,165images, which are grouped into 34higher-level nodes in the Im- 212
ageNet human-curated hierarchy. This set of nodes is partitioned into 20,6, and 8disjoint sets of 213
training, validation, and testing nodes, and the corresponding classes form the respective meta-sets. 214
Evaluation. To quantify the impact of different augmentation methods, we evaluate the test-set ac- 215
curacies of a state-of-the-art unsupervised few-shot learning method with GeNIe and compare them 216
against the accuracies obtained using other augmentation methods. Specifically, we use UniSiam 217
[61] pre-trained with ResNet-18, ResNet-34 and ResNet-50 backbones and follow its evaluation 218
strategy of fine-tuning a logistic regressor to perform ( N-way, K-shot) classification on the test sets 219
ofmini- andtiered- Imagenet. Following [79], an episode consists of a labeled support-set and an un- 220
labelled query-set. The support-set contains Nrandomly sampled classes where each class contains 221
Ksamples, whereas the query-set contains Qrandomly sampled unlabeled images per class. We 222
conduct our experiments on the two most commonly adopted settings: ( 5-way, 1-shot) and ( 5-way, 223
5-shot) classification settings. Following the literature, we sample 16-shots per class for the query 224
set in both settings. We report the test accuracies along with the 95% confidence interval over 600 225
and1000 episodes for mini-ImageNet and tiered -ImageNet, respectively. 226
Implementation Details: GeNIe generates augmented images for each class using images from all 227
other classes as the source image. We use r= 0.8in our experiments. We generate 4samples per 228
class as augmentations in the 5-way, 1-shot setting and 20samples per class as augmentations in the 229
5-way, 5-shot setting. For the sake of a fair comparison, we ensure that the total number of labelled 230
samples in the support set after augmentation remains the same across all different traditional and 231
generative augmentation methodologies. Due to the expensive training of embeddings for each class 232
in each episode, we only evaluated the DA-Fusion baseline on the first 100 episodes. 233
Results: The results on mini- Imagenet and tiered- Imagenet for both ( 5-way, 1and5-shot) set- 234
tings are summarized in Table 1 and Table 2, respectively. Regardless of the choice of back- 235
bone, we observe that GeNIe helps consistently improve UniSiam’s performance and outperform 236
other supervised and unsupervised few-shot classification methods as well as other diffusion-based 237
[100, 63, 82, 35] and classical [110, 111] data augmentation techniques on both datasets, across both 238
(5-way, 1and5-shot) settings. Our noise adaptive method of selecting optimal augmentations per 239
source image ( GeNIe-Ada ) further improves GeNIe ’s performance across all three backbones, both 240
6Table 1: mini -ImageNet: We use our augmentations on ( 5-way, 1-shot) and ( 5-way, 5-shot) few-shot settings of
mini-Imagenet dataset with 3 different backbones (ResNet-18, 34, and 50). We compare with various baselines
and show that our augmentations with UniSiam outperform all the baselines including Txt2Img and DAFusion
augmentation. The number of generated images per class is 4 for 1-shot and 20 for 5-shot settings.
ResNet-18
Augmentation Method Pre-training 1-shot 5-shot
- iDeMe-Net [14] sup. 59.1 ±0.9 74.6 ±0.7
- Robust + dist [27] sup. 63.7 ±0.6 81.2 ±0.4
- AFHN [54] sup. 62.4 ±0.7 78.2 ±0.6
Weak ProtoNet+SSL [94] sup.+ssl - 76.6
Weak Neg-Cosine [57] sup. 62.3 ±0.8 80.9 ±0.6
- Centroid Align[1] sup. 59.9 ±0.7 80.4 ±0.7
- Baseline [10] sup. 59.6 ±0.8 77.3 ±0.6
- Baseline++ [10] sup. 59.0 ±0.8 76.7 ±0.6
Weak PSST [13] sup.+ssl 59.5 ±0.5 77.4 ±0.5
Weak UMTRA [46] unsup. 43.1 ±0.4 53.4 ±0.3
Weak ProtoCLR [66] unsup. 50.9 ±0.4 71.6 ±0.3
Weak SimCLR [9] unsup. 62.6 ±0.4 79.7 ±0.3
Weak SimSiam [12] unsup. 62.8 ±0.4 79.9 ±0.3
Weak UniSiam+dist [61] unsup. 64.1±0.4 82.3 ±0.3
Weak UniSiam [61] unsup. 63.1 ±0.8 81.4 ±0.5
Strong UniSiam [61] unsup. 62.8 ±0.8 81.2 ±0.6
CutMix [110] UniSiam [61] unsup. 62.7 ±0.8 80.6 ±0.6
MixUp [111] UniSiam [61] unsup. 62.1 ±0.8 80.7 ±0.6
Img2ImgL[63] UniSiam [61] unsup. 63.9 ±0.8 82.1 ±0.5
Img2ImgH[63] UniSiam [61] unsup. 69.1 ±0.7 84.0 ±0.5
Txt2Img [4, 35] UniSiam [61] unsup. 74.1 ±0.6 84.6 ±0.5
DAFusion [100] UniSiam [61] unsup. 64.3 ±1.8 82.0 ±1.4
GeNIe (Ours) UniSiam [61] unsup. 75.5±0.6 85.4±0.4
GeNIe-Ada (Ours) UniSiam [61] unsup. 76.8±0.6 85.9±0.4ResNet-34
Augmentation Method Pre-training 1-shot 5-shot
Weak Baseline [10] sup. 49.8 ±0.7 73.5 ±0.7
Weak Baseline++ [10] sup. 52.7 ±0.8 76.2 ±0.6
Weak SimCLR [9] unsup. 64.0 ±0.4 79.8 ±0.3
Weak SimSiam [12] unsup. 63.8 ±0.4 80.4 ±0.3
Weak UniSiam+dist [61] unsup. 65.6±0.4 83.4 ±0.2
Weak UniSiam [61] unsup. 64.3 ±0.8 82.3 ±0.5
Strong UniSiam [61] unsup. 64.5 ±0.8 82.1 ±0.6
CutMix [110] UniSiam [61] unsup. 64.0 ±0.8 81.7 ±0.6
MixUp [111] UniSiam [61] unsup. 63.7 ±0.8 80.1 ±0.8
Img2ImgL[63] UniSiam [61] unsup. 65.5 ±0.8 82.9 ±0.5
Img2ImgH[63] UniSiam [61] unsup. 70.5 ±0.8 84.8 ±0.5
Txt2Img [4, 35] UniSiam [61] unsup. 75.4 ±0.6 85.5 ±0.5
DAFusion [100] UniSiam [61] unsup. 64.7 ±1.9 83.2 ±1.4
GeNIe (Ours) UniSiam [61] unsup. 77.1±0.6 86.3±0.4
GeNIe-Ada (Ours) UniSiam [61] unsup. 78.5±0.6 86.6±0.4
ResNet-50
Weak PDA+Net [11] unsup. 63.8 ±0.9 83.1 ±0.6
Weak Meta-DM [40] unsup. 66.7 ±0.4 85.3 ±0.2
Weak UniSiam [61] unsup. 64.6 ±0.8 83.4 ±0.5
Strong UniSiam [61] unsup. 64.8 ±0.8 83.2 ±0.5
CutMix [110] UniSiam [61] unsup. 64.3 ±0.8 83.2 ±0.5
MixUp [111] UniSiam [61] unsup. 63.8 ±0.8 84.6 ±0.5
Img2ImgL[63] UniSiam [61] unsup. 66.0 ±0.8 84.0 ±0.5
Img2ImgH[63] UniSiam [61] unsup. 71.1 ±0.7 85.7 ±0.5
Txt2Img [4, 35] UniSiam [61] unsup. 76.4 ±0.6 86.5 ±0.4
DAFusion [100] UniSiam [61] unsup. 65.7 ±1.8 83.9 ±1.2
GeNIe (Ours) UniSiam [61] unsup. 77.3±0.6 87.2±0.4
GeNIe-Ada (Ours) UniSiam [61] unsup. 78.6±0.6 87.9±0.4
few-shot settings, and both datasets ( mini andtiered- Imagenet). Few-shot accuracies for ResNet- 241
34 computed on tiered Imagenet are reported in Section A.3 of the appendix. Note that employing 242
CutMix and MixUp seems to lead to performance degradation compared to weak augmentations, 243
probably due to overfitting since these methods can only choose from 4other classes to mix. 244
4.2 Long-Tailed Classification 245
We evaluate our method on long-tailed data, where the number of instances per class is unbalanced, 246
with most categories having limited samples (tail). Our goal is to mitigate this bias by augmenting 247
the tail of the distribution with generated samples. We evaluate GeNIe using two different backbones 248
and methods: the ViT architecture with LViT [107], and ResNet50 with VL-LTR [97]. 249
Following LViT [107], we first train an MAE [34] and ViT on the unbalanced dataset without any 250
augmentation. Next, we train the Balanced Fine-Tuning stage of LViT by incorporating the aug- 251
mentation data generated using GeNIe or other baselines. For ResNet50, we use VL-LTR code to 252
fine-tune the CLIP [76] ResNet50 pretrained backbone with generated augmentations by GeNIe . 253
Dataset: We perform experiments on ImageNet-LT [60]. It contains 115.8K images from 1,000 254
categories. The number of images per class varies from 1280 to5. Imagenet-LT classes can be 255
divided into 3groups: “Few” with less than 20images, “Med” with 20−100images, and “Many” 256
with more than 100images. Imagenet-LT uses the same validation set as ImageNet. We augment 257
“Few” categories only and limit the number of generated images to 50samples per class. For GeNIe , 258
instead of randomly sampling the source images from other classes, we use a confusion matrix on 259
the training data to find the top- 4most confused classes and only consider those classes for random 260
sampling of the source image. The source category may be from “Many”, “Med”, or “Few sets”. 261
Results: Augmenting training data with GeNIe-Ada improves accuracy on the “Few” set by 11.7% 262
and4.4%compared with LViT only and LViT with Txt2Img augmentation baselines respectively. 263
In ResNet50, GeNIe-Ada outperforms Cap2Aug baseline in “Few” categories by 7.6%. The results 264
are summarized in Table 3. Please refer to Section A.4 for implementation details. 265
4.3 Ablation and Analysis 266
Semantic Shift from Source to Target Class. The core motivation behind GeNIe-Ada is that by 267
varying the noise ratio rfrom 0to1, augmented sample Xrwill progressively shift its semantic cat- 268
egory from source ( S) in the beginning to target category ( T) towards the end. However, somewhere 269
between 0and1,Xrwill undergo a rapid transition from StoT. To demonstrate this hypothesis 270
empirically, in Figs. 5 and A5, we visualize pairs of source images and target categories with their re- 271
spective GeNIe generated augmentations for different noise ratios r, along with their corresponding 272
7Table 2: tiered- ImageNet: Accuracies (% ±std) for
5-way, 1-shot and 5-way, 5-shot classification settings
on the test-set. We compare against various SOTA su-
pervised and unsupervised few-shot classification base-
lines as well as other augmentation methods, with
UniSiam [61] pre-trained ResNet-18,50 backbones.
ResNet-18
Augmentation Method Pre-training 1-shot 5-shot
Weak SimCLR[9] unsup. 63.4 ±0.4 79.2 ±0.3
Weak SimSiam [12] unsup. 64.1 ±0.4 81.4 ±0.3
Weak UniSiam [61] unsup. 63.1 ±0.7 81.0 ±0.5
Strong UniSiam [61] unsup. 62.8 ±0.7 80.9 ±0.5
CutMix [110] UniSiam [61] unsup. 62.1 ±0.7 78.9 ±0.6
MixUp [111] UniSiam [61] unsup. 62.1 ±0.7 78.4 ±0.6
Img2ImgL[63] UniSiam [61] unsup. 63.9 ±0.7 81.8 ±0.5
Img2ImgH[63] UniSiam [61] unsup. 68.7 ±0.7 83.5 ±0.5
Txt2Img [35] UniSiam [61] unsup. 72.9 ±0.6 84.2 ±0.5
DAFusion [100] UniSiam [61] unsup. 62.6 ±2.1 81.0 ±1.5
GeNIe (Ours) UniSiam [61] unsup. 73.6±0.6 85.0±0.4
GeNIe-Ada (Ours) UniSiam [61] unsup. 75.1±0.6 85.5±0.5
ResNet-50
Weak PDA+Net [11] unsup. 69.0 ±0.9 84.2 ±0.7
Weak Meta-DM [40] unsup. 69.6 ±0.4 86.5 ±0.3
Weak UniSiam + dist [61] unsup. 69.6 ±0.4 86.5 ±0.3
Weak UniSiam [61] unsup. 66.8 ±0.7 84.7 ±0.5
Strong UniSiam [61] unsup. 66.5 ±0.7 84.5 ±0.5
CutMix [110] UniSiam [61] unsup. 66.0 ±0.7 83.3 ±0.5
MixUp [111] UniSiam [61] unsup. 66.1 ±0.5 84.1 ±0.8
Img2ImgL[63] UniSiam [61] unsup. 67.8 ±0.7 85.3 ±0.5
Img2ImgH[63] UniSiam [61] unsup. 72.4 ±0.7 86.7 ±0.4
Txt2Img [35] UniSiam [61] unsup. 77.1 ±0.6 87.3 ±0.4
DAFusion [100] UniSiam [61] unsup. 66.5 ±2.2 84.8 ±1.4
GeNIe (Ours) UniSiam [61] unsup. 78.0±0.6 88.0±0.4
GeNIe-Ada (Ours) UniSiam [61] unsup. 78.8±0.6 88.6±0.6Table 3: Long-Tailed ImageNet-LT: We
compare different augmentation methods on
ImageNet-LT and report Top-1 accuracy for
“Few”, “Medium”, and “Many” sets. On the
“Few” set and LiVT method, our augmentations
improve the accuracy by 11.7points compared
to LiVT original augmentation and 4.4points
compared to Txt2Img .GeNIe-Ada outperforms
Cap2Aug baseline in “Few” categories by 7.6%.
Refer to Table A4 for a full comparison with prior
Long-Tailed methods.
ResNet-50
Method Many Med. Few Overall Acc
ResLT [18] 63.3 53.3 40.3 55.1
PaCo [19] 68.2 58.7 41.0 60.0
LWS [44] 62.2 48.6 31.8 51.5
Zero-shot CLIP [76] 60.8 59.3 58.6 59.8
DRO-LT [85] 64.0 49.8 33.1 53.5
VL-LTR [97] 77.8 67.0 50.8 70.1
Cap2Aug [83] 78.5 67.7 51.9 70.9
GeNIe-Ada 79.2 64.6 59.5 71.5
ViT-B
Method Many Med. Few Overall Acc
ViT [24] 50.5 23.5 6.9 31.6
MAE [33] 74.7 48.2 19.4 54.5
DeiT [99] 70.4 40.9 12.8 48.4
LiVT [107] 73.6 56.4 41.0 60.9
LiVT + Img2ImgL74.3 56.4 34.3 60.5
LiVT + Img2ImgH73.8 56.4 45.3 61.6
LiVT + Txt2Img 74.9 55.6 48.3 62.2
LiVT + GeNIe-Ada 74.0 56.9 52.7 63.1
Figure 5: Embedding visualizations of generative augmentations: We pass all generative augmentations
through DINOv2 ViT-G (serving as an oracle) to extract their corresponding embeddings and visualize them
with PCA. As shown, the extent of semantic shifts varies based on both the source image and the target class.
PCA-projected embedding scatter plots (on the far left). We extract embeddings for all the images 273
using a DINOv2 ViT-G pretrained backbone, which we assume as an oracle model in identifying 274
the right category. We observe that as rincreases from 0.3to0.8, the images transition to embody 275
more of the target category’s semantics while preserving the contextual features of the source image. 276
This transition of semantics can also be observed in the embedding plots (on the left) where they 277
consistently shift from the proximity of the source image (blue star) to the target class’s centroid 278
(red cross) as the noise ratio rincreases. The sparse distribution of points within r= [0.4,0.6]for 279
the first image and r= [0.2,0.4]for the second image aligns with our intuition of a rapid transition 280
from category StoT, thus empirically affirming our motivation behind GeNIe-Ada . 281
To further establish this, in Fig. 6, we demonstrate the efficacy of GeNIe in generating hard negatives 282
at the decision boundaries of an SVM classifier, which is trained on the labelled support set of 283
the few-shot tasks of mini-Imagenet, without any augmentations. We then plot source and target 284
class probabilities ( P(YS|Xr)andP(YT|Xr), respectively) of the generated augmentation samples 285
Xr. For both r= 0.6and0.7, there is significant overlap between P(YS|Xr)andP(YT|Xr), 286
making it difficult for the classifier to decide the correct class. On the right-hand-side, GeNIe-Ada 287
automatically selects the best rresulting in the most overlap between the two distributions, thus 288
offering the hardest negative sample among the considered rvalues (for more details see A.1). 289
Note that a large overlap between distributions is not sufficient to call the generated samples hard 290
negatives because they should also belong to the target category. This is, however, confirmed by the 291
high Oracle accuracy in Table 4 (elaborated in detail in the following paragraph) which verifies that 292
majority of the generated augmentation samples do belong to the target category. 293
8Figure 6: Why GeNIe augmentations are challenging? While deciding which class the generated augmen-
tations ( Xr) belong to is already difficult within r= [0.6,0.7](due to high overlap between P(YS|Xr)and
P(YT|Xr)),GeNIe-Ada selects the best noise threshold ( r∗) offering the hardest negative sample.
Table 4: Effect of Noise in GeNIe :We use the same setting as in Table 1 to study the effect of the amount of
noise. As expected (also shown in Fig 5), small noise results in worse accuracy since some generated images
may be from the source category rather than the target one. For r= 0.5only73% of the generated data is
from the target category. This behaviour is also shown in Fig. 2. Notably, reducing the noise level below 0.7
is associated with a decline in oracle accuracy and subsequent degradation in the performance of the final few-
shot model. Note that the high oracle accuracy of GeNIe-Ada demonstrates its capability to adaptively select
the noise level per source and target, ensuring semantic consistency with the intended target.
Noise ResNet-18 ResNet-34 ResNet-50 Oracle
1-shot 5-shot 1-shot 5-shot 1-shot 5-shot Acc
GeNIe (r=0.5) 60.42±0.8 74.11 ±0.6 62.02±0.8 75.80 ±0.6 63.65±0.9 77.61 ±0.6 73.4±0.5
GeNIe (r=0.6) 69.66±0.7 80.65 ±0.5 71.13±0.7 82.21 ±0.5 72.10±0.7 82.79 ±0.5 85.8±0.4
GeNIe (r=0.7) 74.50±0.6 83.26 ±0.5 76.41±0.6 84.44 ±0.5 77.05±0.6 84.95 ±0.4 94.5±0.2
GeNIe (r=0.8) 75.45±0.6 85.38 ±0.4 77.08±0.6 86.28 ±0.4 77.28±0.6 87.22 ±0.4 98.2±0.1
GeNIe (r=0.9) 74.96±0.6 85.29 ±0.4 77.63±0.6 86.17 ±0.4 77.73±0.6 87.00 ±0.4 99.3±0.1
GeNIe-Ada 76.79±0.6 85.89±0.4 78.49±0.6 86.55±0.4 78.64±0.6 87.88±0.4 98.9±0.2
Label consistency of the generated samples. The choice of noise ratio ris important in producing 294
hard negative examples. In Table 4, we present the accuracy of the GeNIe model across various noise 295
ratios, alongside the oracle accuracy, which is an ImageNet pre-trained DeiT-Base [98] classifier. 296
We observe a decline in the label consistency of generated data (quantified by the performance of 297
the oracle model) when decreasing the noise level. Reducing ralso results in a degradation in the 298
performance of the final few-shot model ( 87.2%→77.6%) corroborating that an appropriate choice 299
ofrplays a crucial role in our design strategy. We investigate this further in the following paragraph. 300
Effect of Noise in GeNIe .We examine the impact of noise on the performance of the few-shot 301
model in Table 4. Noise levels r∈[0.7,0.8]yield the best performance. Conversely, utilizing noise 302
levels below 0.7diminishes performance due to label inconsistency, as is demonstrated in Table 4 303
and Fig 5. As such, determining the appropriate noise level is pivotal for the performance of GeNIe 304
to be able to generate challenging hard negatives while maintaining label consistency. An alternative 305
approach to finding the optimal noise level involves using GeNIe-Ada to adaptively select the noise 306
level for each source image and target class. As demonstrated in Tables 4 and A1, GeNIe-Ada 307
achieves performance that is comparable to or surpasses that of GeNIe with fixed noise levels. 308
5 Concluding Remarks 309
GeNIe , for the first time to our knowledge, combines contradictory sources of information (a source 310
image, and a different target category prompt) through a noise adjustment strategy into a conditional 311
latent diffusion model to generate challenging augmentations, which can serve as hard negatives. 312
Limitation. The required time to create augmentations through GeNIe is on par with any typical 313
diffusion-based competitors [4, 35]; however, this is naturally slower than traditional augmentation 314
techniques [110, 111]. This is not a bottleneck in offline augmentation strategies, but can be con- 315
sidered a limiting factor in real-time scenarios. Recent studies are already mitigating this through 316
advancements in diffusion model efficiency [87, 68, 58]. Another challenge present in any genera- 317
tive AI-based augmentation technique is the domain shift between the distribution of training data 318
and the downstream context they might be used for augmentation. A possible remedy is to fine-tune 319
the diffusion backbone on a rather small dataset from the downstream task. 320
Broader Impact. We believe ideas from GeNIe can have a significant impact when it comes to gen- 321
erating hard augmentations challenging and thus enhancing downstream tasks beyond classification. 322
At the same time, just like any other generative model, GeNIe can also introduce inherent biases 323
stemming from the training data used to build its diffusion backbone, which can reflect and amplify 324
societal prejudices or inaccuracies. Therefore, it is crucial to carefully mitigate potential biases in 325
generative models such as GeNIe to ensure a fair and ethical deployment of deep learning systems. 326
9References 327
[1] Afrasiyabi, A., Lalonde, J.F., Gagn ´e, C.: Associative alignment for few-shot image classifi- 328
cation. In: ECCV (2019) 329
[2] Alayrac, J.B., Donahue, J., Luc, P., Miech, A., Barr, I., Hasson, Y ., Lenc, K., Mensch, A., 330
Millican, K., Reynolds, M., Ring, R., Rutherford, E., Cabi, S., Han, T., Gong, Z., Saman- 331
gooei, S., Monteiro, M., Menick, J., Borgeaud, S., Brock, A., Nematzadeh, A., Sharifzadeh, 332
S., Binkowski, M., Barreira, R., Vinyals, O., Zisserman, A., Simonyan, K.: Flamingo: a 333
visual language model for few-shot learning (2022) 334
[3] Antoniou, A., Storkey, A.: Assume, augment and learn: Unsupervised few-shot meta- 335
learning via random labels and data augmentation. arxiv:1902.09884 (2019) 336
[4] Azizi, S., Kornblith, S., Saharia, C., Norouzi, M., Fleet, D.J.: Synthetic data from diffusion 337
models improves imagenet classification (2023) 338
[5] Bossard, L., Guillaumin, M., Van Gool, L.: Food-101 – mining discriminative components 339
with random forests. In: European Conference on Computer Vision (2014) 340
[6] Cai, J., Wang, Y ., Hwang, J.N., et al.: Ace: Ally complementary experts for solving long- 341
tailed recognition in one-shot. In: ICCV . pp. 112–121 (2021) 342
[7] Cao, K., Wei, C., Gaidon, A., Arechiga, N., Ma, T.: Learning imbalanced datasets with label- 343
distribution-aware margin loss. NeurIPS 32(2019) 344
[8] Chegini, A., Feizi, S.: Identifying and mitigating model failures through few-shot clip-aided 345
diffusion generation. arXiv preprint arXiv:2312.05464 (2023) 346
[9] Chen, T., Kornblith, S., Norouzi, M., Hinton, G.: A simple framework for contrastive learning 347
of visual representations. In: ICML (2020) 348
[10] Chen, W.Y ., Liu, Y .C., Kira, Z., Wang, Y .C.F., Huang, J.B.: A closer look at few-shot classi- 349
fication. In: ICLR (2019) 350
[11] Chen, W., Si, C., Wang, W., Wang, L., Wang, Z., Tan, T.: Few-shot learning with part discov- 351
ery and augmentation from unlabeled images. arXiv preprint arXiv:2105.11874 (2021) 352
[12] Chen, X., He, K.: Exploring simple siamese representation learning. In: CVPR (2021) 353
[13] Chen, Z., Ge, J., Zhan, H., Huang, S., Wang, D.: Pareto self-supervised training for few-shot 354
learning. In: CVPR (2021) 355
[14] Chen, Z., Fu, Y ., Wang, Y .X., Ma, L., Liu, W., Hebert, M.: Image deformation meta-networks 356
for one-shot learning. In: CVPR (2019) 357
[15] Cubuk, E.D., Zoph, B., Mane, D., Vasudevan, V ., Le, Q.V .: Autoaugment: Learning augmen- 358
tation policies from data (2019) 359
[16] Cubuk, E.D., Zoph, B., Shlens, J., Le, Q.V .: Randaugment: Practical automated data aug- 360
mentation with a reduced search space (2019) 361
[17] Cubuk, E.D., Zoph, B., Shlens, J., Le, Q.: Randaugment: Practical automated data aug- 362
mentation with a reduced search space. In: Larochelle, H., Ranzato, M., Hadsell, R., 363
Balcan, M., Lin, H. (eds.) Advances in Neural Information Processing Systems. vol. 33, 364
pp. 18613–18624. Curran Associates, Inc. (2020), https://proceedings.neurips.cc/ 365
paper/2020/file/d85b63ef0ccb114d0a3bb7b7d808028f-Paper.pdf 366
[18] Cui, J., Liu, S., Tian, Z., Zhong, Z., Jia, J.: Reslt: Residual learning for long-tailed recogni- 367
tion. IEEE transactions on pattern analysis and machine intelligence 45(3), 3695–3706 (2022) 368
[19] Cui, J., Zhong, Z., Liu, S., Yu, B., Jia, J.: Parametric contrastive learning. In: Proceedings of 369
the IEEE/CVF international conference on computer vision. pp. 715–724 (2021) 370
[20] Cui, J., Zhong, Z., Liu, S., Yu, B., Jia, J.: Parametric contrastive learning. In: ICCV . pp. 371
715–724 (2021) 372
[21] Cui, Y ., Jia, M., Lin, T.Y ., Song, Y ., Belongie, S.: Class-balanced loss based on effective 373
number of samples. In: CVPR. pp. 9268–9277 (2019) 374
[22] Deng, J., Dong, W., Socher, R., Li, L.J., Li, K., Fei-Fei, L.: Imagenet: A large-scale hierar- 375
chical image database. In: 2009 IEEE conference on computer vision and pattern recognition. 376
pp. 248–255. Ieee (2009) 377
10[23] Ding, M., An, B., Xu, Y ., Satheesh, A., Huang, F.: SAFLEX: Self-adaptive augmentation via 378
feature label extrapolation. In: The Twelfth International Conference on Learning Represen- 379
tations (2024), https://openreview.net/forum?id=qL6brrBDk2 380
[24] Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., De- 381
hghani, M., Minderer, M., Heigold, G., Gelly, S., Uszkoreit, J., Houlsby, N.: An image is 382
worth 16x16 words: Transformers for image recognition at scale. In: ICLR (2021) 383
[25] Dunlap, L., Mohri, C., Zhang, H., Guillory, D., Darrell, T., Gonzalez, J.E., Rohrbach, A., 384
Raghunathan, A.: Using language to extend to unseen domains. International Conference on 385
Learning Representations (ICLR) (2023) 386
[26] Dunlap, L., Umino, A., Zhang, H., Yang, J., Gonzalez, J.E., Darrell, T.: Diversify your vision 387
datasets with automatic diffusion-based augmentation (2023) 388
[27] Dvornik, N., Mairal, J., Schmid, C.: Diversity with cooperation: Ensemble methods for few- 389
shot classification. In: ICCV (2019) 390
[28] Feng, C.M., Yu, K., Liu, Y ., Khan, S., Zuo, W.: Diverse data augmentation with diffusions 391
for effective test-time prompt tuning (2023) 392
[29] Finn, C., Abbeel, P., Levine, S.: Model-agnostic meta-learning for fast adaptation of deep 393
networks. In: Proceedings of the 34th International Conference on Machine Learning. pp. 394
1126–1135 (2017) 395
[30] Frid-Adar, M., Diamant, I., Klang, E., Amitai, M., Goldberger, J., Greenspan, H.: Gan- 396
based synthetic medical image augmentation for increased cnn performance in liver lesion 397
classification. Neurocomputing (2018) 398
[31] Gal, R., Alaluf, Y ., Atzmon, Y ., Patashnik, O., Bermano, A.H., Chechik, G., Cohen-Or, D.: 399
An image is worth one word: Personalizing text-to-image generation using textual inversion. 400
arXiv preprint arXiv:2208.01618 (2022) 401
[32] Gal, R., Alaluf, Y ., Atzmon, Y ., Patashnik, O., Bermano, A.H., Chechik, G., Cohen-Or, 402
D.: An image is worth one word: Personalizing text-to-image generation using textual 403
inversion (2022). https://doi.org/10.48550/ARXIV .2208.01618, https://arxiv.org/abs/ 404
2208.01618 405
[33] He, K., Chen, X., Xie, S., Li, Y ., Doll ´ar, P., Girshick, R.B.: Masked autoencoders are scalable 406
vision learners. In: CVPR. pp. 15979–15988. IEEE (2022) 407
[34] He, K., Chen, X., Xie, S., Li, Y ., Doll ´ar, P., Girshick, R.: Masked autoencoders are scalable 408
vision learners (2021) 409
[35] He, R., Sun, S., Yu, X., Xue, C., Zhang, W., Torr, P., Bai, S., Qi, X.: Is synthetic data from 410
generative models ready for image recognition? arXiv preprint arXiv:2210.07574 (2022) 411
[36] Hemmat, R.A., Pezeshki, M., Bordes, F., Drozdzal, M., Romero-Soriano, A.: Feedback- 412
guided data synthesis for imbalanced classification (2023) 413
[37] Hendrycks, D., Mu, N., Cubuk, E.D., Zoph, B., Gilmer, J., Lakshminarayanan, B.: AugMix: 414
A simple data processing method to improve robustness and uncertainty. Proceedings of the 415
International Conference on Learning Representations (ICLR) (2020) 416
[38] Hong, Y ., Zhang, J., Sun, Z., Yan, K.: Safa: Sample-adaptive feature augmentation for long- 417
tailed image classification. In: ECCV (2022) 418
[39] Hsu, K., Levine, S., Finn, C.: Unsupervised learning via meta-learning. In: ICLR (2018) 419
[40] Hu, W., Jiang, X., Liu, J., Yang, Y ., Tian, H.: Meta-dm: Applications of diffusion models on 420
few-shot learning (2023) 421
[41] Huang, S.W., Lin, C.T., Chen, S.P., an Po-Hao Hsu, Y .Y .W., Lai, S.H.: Auggan: Cross do- 422
main adaptation with gan-based data augmentation. European Conference on Computer Vi- 423
sion (2018) 424
[42] Jain, S., Lawrence, H., Moitra, A., Madry, A.: Distilling model failures as directions in latent 425
space. In: ArXiv preprint arXiv:2206.14754 (2022) 426
[43] Jang, H., Lee, H., Shin, J.: Unsupervised meta-learning via few-shot pseudo-supervised con- 427
trastive learning. In: The Eleventh International Conference on Learning Representations 428
(2022) 429
11[44] Kang, B., Xie, S., Rohrbach, M., Yan, Z., Gordo, A., Feng, J., Kalantidis, Y .: Decoupling rep- 430
resentation and classifier for long-tailed recognition. arXiv preprint arXiv:1910.09217 (2019) 431
[45] Kang, B., Xie, S., Rohrbach, M., Yan, Z., Gordo, A., Feng, J., Kalantidis, Y .: Decoupling 432
representation and classifier for long-tailed recognition. In: ICLR (2020) 433
[46] Khodadadeh, S., Boloni, L., Shah, M.: Unsupervised meta-learning for few-shot image clas- 434
sification. In: NeurIPS (2019) 435
[47] Kim, J.H., Choo, W., Song, H.O.: Puzzle mix: Exploiting saliency and local statistics for 436
optimal mixup. In: International Conference on Machine Learning. pp. 5275–5285. PMLR 437
(2020) 438
[48] Krause, J., Stark, M., Deng, J., Fei-Fei, L.: 3D object representations for fine-grained catego- 439
rization. In: Workshop on 3D Representation and Recognition. Sydney, Australia (2013) 440
[49] Li, A.C., Prabhudesai, M., Duggal, S., Brown, E., Pathak, D.: Your diffusion model is secretly 441
a zero-shot classifier (2023) 442
[50] Li, B., Han, Z., Li, H., Fu, H., Zhang, C.: Trustworthy long-tailed classification. In: CVPR. 443
pp. 6970–6979 (2022) 444
[51] Li, D., Ling, H., Kim, S.W., Kreis, K., Barriuso, A., Fidler, S., Torralba, A.: Bigdatasetgan: 445
Synthesizing imagenet with pixel-wise annotations (2022) 446
[52] Li, J., Tan, Z., Wan, J., Lei, Z., Guo, G.: Nested collaborative learning for long-tailed visual 447
recognition. In: CVPR. pp. 6949–6958 (2022) 448
[53] Li, J., Li, D., Xiong, C., Hoi, S.: Blip: Bootstrapping language-image pre-training for unified 449
vision-language understanding and generation (2022) 450
[54] Li, K., Zhang, Y ., Li, K., Fu, Y .: Adversarial feature hallucination networks for few-shot 451
learning. In: CVPR (2020) 452
[55] Li, M., Cheung, Y .m., Lu, Y ., et al.: Long-tailed visual recognition via gaussian clouded logit 453
adjustment. In: CVPR. pp. 6929–6938 (2022) 454
[56] Li, T., Cao, P., Yuan, Y ., Fan, L., Yang, Y ., Feris, R.S., Indyk, P., Katabi, D.: Targeted 455
supervised contrastive learning for long-tailed recognition. In: CVPR. pp. 6918–6928 (2022) 456
[57] Liu, B., Cao, Y ., Lin, Y ., Li, Q., Zhang, Z., Long, M., Hu, H.: Negative margin matters: 457
Understanding margin in few-shot classification. In: ECCV (2020) 458
[58] Liu, X., Zhang, X., Ma, J., Peng, J., et al.: Instaflow: One step is enough for high-quality 459
diffusion-based text-to-image generation. In: The Twelfth International Conference on Learn- 460
ing Representations (2023) 461
[59] Liu, Z., Li, S., Wu, D., Liu, Z., Chen, Z., Wu, L., Li, S.Z.: Automix: Unveiling the power of 462
mixup for stronger classifiers. In: Computer Vision–ECCV 2022: 17th European Conference, 463
Tel Aviv, Israel, October 23–27, 2022, Proceedings, Part XXIV . pp. 441–458. Springer (2022) 464
[60] Liu, Z., Miao, Z., Zhan, X., Wang, J., Gong, B., Yu, S.X.: Large-scale long-tailed recognition 465
in an open world. In: CVPR (2019) 466
[61] Lu, Y ., Wen, L., Liu, J., Liu, Y ., Tian, X.: Self-supervision can be a good few-shot learner. In: 467
European Conference on Computer Vision. pp. 740–758. Springer (2022) 468
[62] Luo, X.J., Wang, S., Wu, Z., Sakaridis, C., Cheng, Y ., Fan, D.P., Gool, L.V .: Camdiff: Cam- 469
ouflage image augmentation via diffusion model (2023) 470
[63] Luzi, L., Siahkoohi, A., Mayer, P.M., Casco-Rodriguez, J., Baraniuk, R.: Boomerang: Local 471
sampling on image manifolds using diffusion models (2022) 472
[64] Maji, S., Rahtu, E., Kannala, J., Blaschko, M.B., Vedaldi, A.: Fine-grained visual classifica- 473
tion of aircraft. arXiv preprint arXiv:1306.5151 (2013) 474
[65] Mao, J., Xiao, T., Jiang, Y ., Cao, Z.: What can help pedestrian detection? (2017) 475
[66] Medina, C., Devos, A., Grossglauser, M.: Self-supervised prototypical transfer learning for 476
few-shot classification. In: ICMLW (2020) 477
[67] Meng, C., He, Y ., Song, Y ., Song, J., Wu, J., Zhu, J.Y ., Ermon, S.: Sdedit: Guided image 478
synthesis and editing with stochastic differential equations. arXiv preprint arXiv:2108.01073 479
(2021) 480
12[68] Meng, C., Rombach, R., Gao, R., Kingma, D., Ermon, S., Ho, J., Salimans, T.: On distilla- 481
tion of guided diffusion models. In: Proceedings of the IEEE/CVF Conference on Computer 482
Vision and Pattern Recognition. pp. 14297–14306 (2023) 483
[69] Menon, A.K., Jayasumana, S., Rawat, A.S., Jain, H., Veit, A., Kumar, S.: Long-tail learning 484
via logit adjustment. In: ICLR (2021) 485
[70] Oquab, M., Darcet, T., Moutakanni, T., V o, H., Szafraniec, M., Khalidov, V ., Fernandez, P., 486
Haziza, D., Massa, F., El-Nouby, A., Assran, M., Ballas, N., Galuba, W., Howes, R., Huang, 487
P.Y ., Li, S.W., Misra, I., Rabbat, M., Sharma, V ., Synnaeve, G., Xu, H., Jegou, H., Mairal, 488
J., Labatut, P., Joulin, A., Bojanowski, P.: Dinov2: Learning robust visual features without 489
supervision (2023) 490
[71] Peebles, W., Zhu, J.Y ., Zhang, R., Torralba, A., Efros, A., Shechtman, E.: Gan-supervised 491
dense visual alignment. In: CVPR (2022) 492
[72] Petryk, S., Dunlap, L., Nasseri, K., Gonzalez, J., Darrell, T., Rohrbach, A.: On guid- 493
ing visual attention with language specification. In: Conference on Computer Vision and 494
Pattern Recognition (CVPR) (2022). https://doi.org/10.48550/ARXIV .2202.08926, https: 495
//arxiv.org/abs/2202.08926 496
[73] Prabhu, V ., Yenamandra, S., Chattopadhyay, P., Hoffman, J.: Lance: Stress-testing visual 497
models by generating language-guided counterfactual images. Advances in Neural Informa- 498
tion Processing Systems 36(2024) 499
[74] Qiao, S., Liu, C., Shen, W., Yuille, A.: Few-shot image recognition by predicting parameters 500
from activations. In: CVPR (2018) 501
[75] Qin, T., Li, W., Shi, Y ., Yang, G.: Unsupervised few-shot learning via distribution shift-based 502
augmentation. arxiv:2004.05805 (2020) 503
[76] Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, 504
A., Mishkin, P., Clark, J., Krueger, G., Sutskever, I.: Learning transferable visual models 505
from natural language supervision. In: ICML (2021) 506
[77] Ramesh, A., Dhariwal, P., Nichol, A., Chu, C., Chen, M.: Hierarchical text-conditional image 507
generation with clip latents. arXiv preprint arXiv:2204.06125 1(2), 3 (2022) 508
[78] Ramesh, A., Pavlov, M., Goh, G., Gray, S., V oss, C., Radford, A., Chen, M., Sutskever, I.: 509
Zero-shot text-to-image generation. In: ICML (2021) 510
[79] Ravi, S., Larochelle, H.: Optimization as a model for few-shot learning. In: ICLR (2017) 511
[80] Ren, M., Ravi, S., Triantafillou, E., Snell, J., Swersky, K., Tenenbaum, J.B., Larochelle, 512
H., Zemel, R.S.: Meta-learning for semi-supervised few-shot classification. In: International 513
Conference on Learning Representations (2018), https://openreview.net/forum?id= 514
HJcSzz-CZ 515
[81] Rombach, R., Blattmann, A., Lorenz, D., Esser, P., Ommer, B.: High-resolution image syn- 516
thesis with latent diffusion models. In: CVPR (2022) 517
[82] Rombach, R., Blattmann, A., Lorenz, D., Esser, P., Ommer, B.: High-resolution image syn- 518
thesis with latent diffusion models (2021) 519
[83] Roy, A., Shah, A., Shah, K., Roy, A., Chellappa, R.: Cap2aug: Caption guided image to 520
image data augmentation (2023) 521
[84] Saharia, C., Chan, W., Saxena, S., Li, L., Whang, J., Denton, E.L., Ghasemipour, K., Gon- 522
tijo Lopes, R., Karagol Ayan, B., Salimans, T., et al.: Photorealistic text-to-image diffusion 523
models with deep language understanding. Advances in Neural Information Processing Sys- 524
tems 35, 36479–36494 (2022) 525
[85] Samuel, D., Chechik, G.: Distributional robustness loss for long-tail learning. In: ICCV 526
(2021) 527
[86] Sankaranarayanan, S., Balaji, Y ., Castillo, C.D., Chellappa, R.: Generate to adapt: Aligning 528
domains using generative adversarial networks. Conference on Computer Vision and Pattern 529
Recognition (CVPR) (2018) 530
[87] Sauer, A., Lorenz, D., Blattmann, A., Rombach, R.: Adversarial diffusion distillation. arXiv 531
preprint arXiv:2311.17042 (2023) 532
13[88] Sharmanska, V ., Hendricks, L.A., Darrell, T., Quadrianto, N.: Contrastive examples for ad- 533
dressing the tyranny of the majority. CoRR abs/2004.06524 (2020), https://arxiv.org/ 534
abs/2004.06524 535
[89] Shipard, J., Wiliem, A., Thanh, K.N., Xiang, W., Fookes, C.: Boosting zero-shot classification 536
with synthetic data diversity via stable diffusion. arXiv preprint arXiv:2302.03298 (2023) 537
[90] Shirekar, O.K., Singh, A., Jamali-Rad, H.: Self-attention message passing for contrastive 538
few-shot learning. In: Proceedings of the IEEE/CVF Winter Conference on Applications of 539
Computer Vision (WACV). pp. 5426–5436 (January 2023) 540
[91] Shorten, C., Khoshgoftaar, T.M.: A survey on image data augmentation for deep learning. 541
Journal of big data 6(1), 1–48 (2019) 542
[92] Singh, A.R., Jamali-Rad, H.: Transductive decoupled variational inference for few-shot 543
classification. Transactions on Machine Learning Research (2023), https://openreview. 544
net/forum?id=bomdTc9HyL 545
[93] Snell, J., Swersky, K., Zemel, R.: Prototypical networks for few-shot learning. In: Advances 546
in Neural Information Processing Systems (2017) 547
[94] Su, J.C., Maji, S., Hariharan, B.: When does self-supervision improve few-shot learning? In: 548
ECCV (2020) 549
[95] Sung, F., Yang, Y ., Zhang, L., Xiang, T., Torr, P.H., Hospedales, T.M.: Learning to compare: 550
Relation network for few-shot learning. In: CVPR (2018) 551
[96] Tang, K., Huang, J., Zhang, H.: Long-tailed classification by keeping the good and removing 552
the bad momentum causal effect. NeurIPS 33, 1513–1524 (2020) 553
[97] Tian, C., Wang, W., Zhu, X., Dai, J., Qiao, Y .: Vl-ltr: Learning class-wise visual-linguistic 554
representation for long-tailed visual recognition. In: ECCV 2022 (2022) 555
[98] Touvron, H., Cord, M., Douze, M., Massa, F., Sablayrolles, A., J ´egou, H.: Training data- 556
efficient image transformers and distillation through attention (2021) 557
[99] Touvron, H., Cord, M., J ´egou, H.: Deit iii: Revenge of the vit. In: ECCV (2022) 558
[100] Trabucco, B., Doherty, K., Gurinas, M.A., Salakhutdinov, R.: Effective data augmentation 559
with diffusion models. In: The Twelfth International Conference on Learning Representations 560
(2024), https://openreview.net/forum?id=ZWzUA9zeAg 561
[101] Tritrong, N., Rewatbowornwong, P., Suwajanakorn, S.: Repurposing gans for one-shot se- 562
mantic part segmentation. In: IEEE Conference on Computer Vision and Pattern Recognition 563
(CVPR) (2021) 564
[102] Wah, C., Branson, S., Welinder, P., Perona, P., Belongie, S.: The caltech-ucsd birds-200-2011 565
dataset (2011) 566
[103] Wang, H., Deng, Z.H.: Contrastive prototypical network with wasserstein confidence penalty. 567
In: European Conference on Computer Vision. pp. 665–682. Springer (2022) 568
[104] Wang, H., Fu, S., He, X., Fang, H., Liu, Z., Hu, H.: Towards calibrated hyper-sphere repre- 569
sentation via distribution overlap coefficient for long-tailed learning. In: ECCV (2022) 570
[105] Wang, X., Lian, L., Miao, Z., Liu, Z., Yu, S.X.: Long-tailed recognition by routing diverse 571
distribution-aware experts. In: ICLR. OpenReview.net (2021) 572
[106] Xu, Y ., Li, Y .L., Li, J., Lu, C.: Constructing balance from imbalance for long-tailed image 573
recognition. In: ECCV . pp. 38–56. Springer (2022) 574
[107] Xu, Z., Liu, R., Yang, S., Chai, Z., Yuan, C.: Learning imbalanced data with vision trans- 575
formers (2023) 576
[108] Xuan, H., Stylianou, A., Liu, X., Pless, R.: Hard negative examples are hard, but useful 577
(2021) 578
[109] Ye, H.J., Hu, H., Zhan, D.C., Sha, F.: Few-shot learning via embedding adaptation with 579
set-to-set functions. In: CVPR (2020) 580
[110] Yun, S., Han, D., Oh, S.J., Chun, S., Choe, J., Yoo, Y .: Cutmix: Regularization strategy to 581
train strong classifiers with localizable features. In: ICCV . pp. 6023–6032 (2019) 582
14[111] Zhang, H., Cisse, M., Dauphin, Y .N., Lopez-Paz, D.: mixup: Beyond empirical risk mini- 583
mization. In: ICLR (2018) 584
[112] Zhang, S., Li, Z., Yan, S., He, X., Sun, J.: Distribution alignment: A unified framework for 585
long-tail visual recognition. In: CVPR. pp. 2361–2370 (2021) 586
[113] Zhang, Y ., Hooi, B., Hong, L., Feng, J.: Test-agnostic long-tailed recognition by test-time 587
aggregating diverse experts with self-supervision. arXiv preprint arXiv:2107.09249 (2021) 588
[114] Zhang, Y ., Ling, H., Gao, J., Yin, K., Lafleche, J.F., Barriuso, A., Torralba, A., Fidler, S.: 589
Datasetgan: Efficient labeled data factory with minimal human effort. In: CVPR (2021) 590
[115] Zhong, Z., Cui, J., Liu, S., Jia, J.: Improving calibration for long-tailed recognition. In: 591
CVPR. pp. 16489–16498. Computer Vision Foundation / IEEE (2021) 592
[116] Zhou, Z., Qiu, X., Xie, J., Wu, J., Zhang, C.: Binocular mutual learning for improving few- 593
shot classification. In: ICCV (2021) 594
[117] Zhu, J., Wang, Z., Chen, J., Chen, Y .P.P., Jiang, Y .G.: Balanced contrastive learning for long- 595
tailed visual recognition. In: CVPR. pp. 6908–6917 (2022) 596
15A Appendix 597
A.1 Analyzing GeNIe ,GeNIe-Ada ’s Class-Probabilities 598
The core aim of GeNIe and GeNIe-Ada is to address the failure modes of a classifier 599
by generating challenging samples located near the decision boundary of each class pair, 600
which facilitates the learning process in effectively enhancing the decision boundary between 601
classes. As summarized in Table 4 and illustrated in Fig. 5, we have empirically corrob- 602
orated that GeNIe andGeNIe-Ada can respectively produce samples Xr, Xr∗that are nega- 603
tive with respect to the source image XS, while semantically belonging to the class T. To
Figure A1: P(YS|Xr)andP(YT|Xr)forr∈ {0.5,0.6,0.7,0.8,0.9}. On average, the classifier confidently
predicts the source class more than the target class for Xrforr= 0.5, and vice-versa for r= 0.8,0.9.
However, for r= 0.6,0.7, the classifier struggles to classify Xr, indicating that the augmented samples are
located closer to the decision boundary.
604
further analyze the effectiveness of GeNIe and GeNIe-Ada , we compare the source class- 605
probabilities P(YS|Xr)and target-class probabilities P(YS|Xr)of augmented samples Xr. 606
Figure A2: Significant over-
lap between P(YS|Xr∗)and
P(YT|Xr∗)indicates high class-
confusion for augmented sam-
ples generated by GeNIe-Ada .To compute these class probabilities, we first fit an SVM classifier 607
(as followed in UniSiam [61]) only on the labelled support set em- 608
beddings of each episode in the miniImagenet test dataset. Then, 609
we perform inference using each episode’s SVM classifier on its re- 610
spective Xr’s and extract its class probabilities of belonging to its 611
source class Sand target class T. These per augmentation-sample 612
source and target class probabilities are then averaged for each 613
episode for each r∈ {0.5,0.6,0.7,0.8,0.9}in the case of GeNIe 614
and for the optimal r=r∗per sample in the case of GeNIe-Ada , 615
plotted as density plots in Fig. A1, Fig. A2, respectively. Fig. A1 616
illustrates that P(YS|Xr)andP(YT|Xr)have significant overlap 617
in the case of r∈ {0.6,0.7}indicating class-confusion for Xr. 618
Furthermore, Fig. A2 illustrates that when using the optimal r=r∗619
found by GeNIe-Ada per sample, P(YS|Xr)andP(YT|Xr)signif- 620
icantly overlap around probability scores of 0.2−0.45, indicating 621
class confusion for GeNIe-Ada augmentations. This corroborates 622
with our analysis in Section 4.3, Table 4 and additionally empiri- 623
cally proves that the augmented samples generated by GeNIe for 624
r∈ {0.6,0.7}andGeNIe-Ada forr=r∗are actually located near 625
the decision boundary of each class pair. 626
A.2 Fine-grained Few-shot Classification 627
To further investigate the impact of the proposed method, we compare GeNIe with other text-based 628
data augmentation techniques across four distinct fine-grained datasets in a 20-way, 1-shot classifi- 629
cation setting. We employ the pre-trained DINOV2 ViT-G [70] backbone as a feature extractor to 630
derive features from training images. Subsequently, an SVM classifier is trained on these features, 631
and we report the Top- 1accuracy of the model on the test set. 632
Datasets: We assess our method on several datasets: Food101 [5] with 101classes of various foods, 633
CUB200 [102] with 200bird species classes, Cars196 [48] with 196car model classes, and FGVC- 634
Aircraft [64] with 41aircraft manufacturer classes. We provide detailed information around fine- 635
grained datasets in Table A2. The reported metric is the average Top- 1accuracy over 100episodes. 636
16Each episode involves sampling 20classes and 1-shot from the training set, with the final model 637
evaluated on the respective test set. 638
Implementation Details: We enhance the basic prompt by incorporating the superclass name for 639
the fine-grained dataset: “A photo of a <target class >, a type of <superclass >”. For instance, 640
in the food dataset and the burger class, our prompt reads: “A photo of a burger , a type of food.” No 641
additional augmentation is used for generative methods in this context. We generate 19samples for 642
both cases of our method and also the baseline with weak augmentation. 643
Results: Table A1 summarizes the results. GeNIe helps outperform all other baselines and aug- 644
mentations, including Txt2Img , by margins upto 0.5%on CUB200 [102], 6.6%on Cars196 [48], 645
0.1%on Food101 [5] and 5.3%on FGVC-Aircraft [64]. Notably, GeNIe exhibits great effectiveness 646
in more challenging datasets, outperforming the baseline with traditional augmentation by about 647
38% for the Cars dataset and by roughly 17% for the Aircraft dataset. It can be observed here that 648
GeNIe-Ada performs on-par with GeNIe with a fixed noise level, eliminating the necessity for noise 649
level search in GeNIe . 650
Table A1: Few-shot Learning on Fine-grained dataset: We utilize an SVM classifier trained atop the DI-
NOV2 ViT-G pretrained backbone, reporting Top-1 accuracy for the test set of each dataset. The baseline is
an SVM trained on the same backbone using weak augmentation. Across all datasets, GeNIe surpasses this
baseline.
Method Birds Cars Foods Aircraft
CUB200 [102] Cars196 [48] Food101 [5] Aircraft [64]
Baseline 90.3 49.8 82.9 29.2
Img2ImgL[63] 90.7 50.4 87.4 31.0
Img2ImgH[63] 91.3 56.4 91.7 34.7
Txt2Img [35] 92.0 81.3 93.0 41.7
GeNIe (r=0.5) 92.0 84.6 91.5 39.8
GeNIe (r=0.6) 92.2 87.1 92.5 45.0
GeNIe (r=0.7) 92.5 87.9 92.9 47.0
GeNIe (r=0.8) 92.5 87.7 93.1 46.5
GeNIe (r=0.9) 92.4 87.1 93.1 45.7
GeNIe-Ada 92.6 87.9 93.1 46.9
Table A2: Train and test split details of the fine-grained datasets. We use the provided train set for few-shot
task generation, and the provided test sets for our evaluation. For the Aircraft dataset we use manufacturer
hierarchy.
Dataset Classes Train Test
samples samples
CUB200 [102] 200 5994 5794
Food101 [5] 101 75750 25250
Cars [48] 196 8144 8041
Aircraft [64] 41 6,667 3333
A.3 Few-shot Classification with ResNet-34 on tiered Imagenet 651
We follow the same evaluation protocol here as mentioned in section 4.1. As summarized in Ta- 652
ble A3, GeNIe andGeNIe-Ada outperform all other classical and generative data augmentation 653
techniques. 654
A.4 Additional details of Long-Tail experiments 655
We present a comprehensive version of Table 3 to benchmark the performance with different back- 656
bone architectures (e.g., ResNet50) and to compare against previous long-tail baselines; this is de- 657
tailed in Table A4. 658
Implementation Details of LViT: We download the pre-trained ViT-B of LViT [107] and finetune 659
it with Bal-BCE loss proposed therein on the augmented dataset. Training takes 2 hours on four 660
NVIDIA RTX 3090 GPUs. We use the same hyperparameters as in [107] for finetuning: 100epochs, 661
lr= 0.008, batch size of 1024 , CutMix and MixUp for the data augmentation. 662
Implementation Details of VL-LTR: We use the official code of VL-LTR [97] for our experiments. 663
We use a pre-trained CLIP ResNet-50 backbone. We followed the hyperparameters reported in VL- 664
17Table A3: tiered- ImageNet: Accuracies (% ±std) for 5-way, 1-shot and 5-way, 5-shot classification set-
tings on the test-set. We compare against various SOTA supervised and unsupervised few-shot classification
baselines as well as other augmentation methods, with UniSiam [61] pre-trained ResNet-34 backbone.
ResNet-34
Augmentation Method Pre-training 1-shot 5-shot
Weak MAML + dist [29] sup. 51.7 ±1.8 70.3 ±1.7
Weak ProtoNet [93] sup. 52.0 ±1.2 72.1 ±1.5
Weak UniSiam + dist [61] unsup. 68.7 ±0.4 85.7 ±0.3
Weak UniSiam [61] unsup. 65.0 ±0.7 82.5 ±0.5
Strong UniSiam [61] unsup. 64.8 ±0.7 82.4 ±0.5
CutMix [110] UniSiam [61] unsup. 63.8 ±0.7 80.3 ±0.6
MixUp [111] UniSiam [61] unsup. 64.1 ±0.7 80.0 ±0.6
Img2ImgL[63] UniSiam [61] unsup. 66.1 ±0.7 83.1 ±0.5
Img2ImgH[63] UniSiam [61] unsup. 70.4 ±0.7 84.7 ±0.5
Txt2Img [35] UniSiam [61] unsup. 75.0 ±0.6 85.4 ±0.4
DAFusion [100] UniSiam [61] unsup. 64.1 ±2.1 82.8 ±1.4
GeNIe (Ours) UniSiam [61] unsup. 75.7±0.6 86.0±0.4
GeNIe-Ada (Ours) UniSiam [61] unsup. 76.9±0.6 86.3±0.2
LTR [97]. We augment only “Few” category and train the backbone with the VL-LTR [97] method. 665
Training takes 4 hours on 8NVIDIA RTX 3090 GPUs. 666
A.5 More Visualizations 667
Additional qualitative results resembling the style presented in Fig. 4 are presented in Fig. A3, and 668
more visuals akin to Fig. 2 can be found in Fig. A4. Moreover, we also present more visualization 669
similar to the style in Fig. 5 in Fig. A5. 670
18Table A4: Long-Tailed ImageNet-LT: We compare different augmentation methods on ImageNet-LT and
report Top-1 accuracy for “Few”, “Medium”, and “Many” sets. †indicates results with ResNeXt50. ∗: indicates
training with 384 resolution so is not directly comparable with other methods with 224 resolution. On the “Few”
set and LiVT method, our augmentations improve the accuracy by 11.7points compared to LiVT original
augmentation and 4.4points compared to Txt2Img .
ResNet-50
Method Many Med. Few Overall Acc
CE [21] 64.0 33.8 5.8 41.6
LDAM [7] 60.4 46.9 30.7 49.8
c-RT [45] 61.8 46.2 27.3 49.6
τ-Norm [45] 59.1 46.9 30.7 49.4
Causal [96] 62.7 48.8 31.6 51.8
Logit Adj. [69] 61.1 47.5 27.6 50.1
RIDE(4E) †[105] 68.3 53.5 35.9 56.8
MiSLAS [115] 62.9 50.7 34.3 52.7
DisAlign [112] 61.3 52.2 31.4 52.9
ACE†[6] 71.7 54.6 23.5 56.6
PaCo†[20] 68.0 56.4 37.2 58.2
TADE†[113] 66.5 57.0 43.5 58.8
TSC [56] 63.5 49.7 30.4 52.4
GCL [55] 63.0 52.7 37.1 54.5
TLC [50] 68.9 55.7 40.8 55.1
BCL†[117] 67.6 54.6 36.6 57.2
NCL [52] 67.3 55.4 39.0 57.7
SAFA [38] 63.8 49.9 33.4 53.1
DOC [104] 65.1 52.8 34.2 55.0
DLSA [106] 67.8 54.5 38.8 57.5
ResLT [18] 63.3 53.3 40.3 55.1
PaCo [19] 68.2 58.7 41.0 60.0
LWS [44] 62.2 48.6 31.8 51.5
Zero-shot CLIP [76] 60.8 59.3 58.6 59.8
DRO-LT [85] 64.0 49.8 33.1 53.5
VL-LTR [97] 77.8 67.0 50.8 70.1
Cap2Aug [83] 78.5 67.7 51.9 70.9
GeNIe-Ada 79.2 64.6 59.5 71.5
ViT-B
LiVT* [107] 76.4 59.7 42.7 63.8
ViT [24] 50.5 23.5 6.9 31.6
MAE [33] 74.7 48.2 19.4 54.5
DeiT [99] 70.4 40.9 12.8 48.4
LiVT [107] 73.6 56.4 41.0 60.9
LiVT + Img2ImgL74.3 56.4 34.3 60.5
LiVT + Img2ImgH73.8 56.4 45.3 61.6
LiVT + Txt2Img 74.9 55.6 48.3 62.2
LiVT + GeNIe (r=0.8)74.5 56.7 50.9 62.8
LiVT + GeNIe-Ada 74.0 56.9 52.7 63.1
19Figure A3: Visualization of Generative Samples: More visualization akin to Fig. 4. We compare GeNIe with
two baselines: Img2ImgLaugmentation uses both image and text prompt from the same category, resulting in
less challenging examples. Txt2Img augmentation generates images based solely on a text prompt, potentially
deviating from the task’s visual domain. GeNIe augmentation incorporates the target category name in the text
prompt along with the source image, producing desired images with an optimal amount of noise, and balancing
the impact of the source image and text prompt.
20Figure A4: Effect of noise in GeNIe :Akin to Fig. 2, we use GeNIe to create augmentations with varying noise
levels. As is illustrated in the examples above, a reduced amount of noise leads to images closely mirroring the
semantics of the source images, causing a misalignment with the intended target label.
21Figure A5: Effect of noise in GeNIe :Similar to Fig. 5, we pass all the generated augmentations through the
DinoV2 ViT-G model, which acts as our oracle model, to obtain their associated embeddings. Subsequently,
we employ PCA for visualization purposes. The visualization reveals that the magnitude of semantic transfor-
mations is contingent upon both the source image and the specified target category.
22NeurIPS Paper Checklist 671
1.Claims 672
Question: Do the main claims made in the abstract and introduction accurately reflect the paper’s 673
contributions and scope? 674
Answer: [Yes] 675
Justification: We demonstrate the effectiveness of our augmentation method through empirical 676
comparison with four different generative augmentation baselines across two scenarios: few-shot 677
and long-tail classification. Additionally, we perform analytical experiments on our augmented 678
samples to illustrate their nature as hard negatives. 679
Guidelines: 680
• The answer NA means that the abstract and introduction do not include the claims made in the 681
paper. 682
• The abstract and/or introduction should clearly state the claims made, including the contributions 683
made in the paper and important assumptions and limitations. A No or NA answer to this question 684
will not be perceived well by the reviewers. 685
• The claims made should match theoretical and experimental results, and reflect how much the 686
results can be expected to generalize to other settings. 687
• It is fine to include aspirational goals as motivation as long as it is clear that these goals are not 688
attained by the paper. 689
2.Limitations 690
Question: Does the paper discuss the limitations of the work performed by the authors? 691
Answer: [Yes] 692
Justification: We discuss about the limitations of our method in Sec 5 693
Guidelines: 694
• The answer NA means that the paper has no limitation while the answer No means that the paper 695
has limitations, but those are not discussed in the paper. 696
• The authors are encouraged to create a separate ”Limitations” section in their paper. 697
• The paper should point out any strong assumptions and how robust the results are to violations of 698
these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, 699
asymptotic approximations only holding locally). The authors should reflect on how these as- 700
sumptions might be violated in practice and what the implications would be. 701
• The authors should reflect on the scope of the claims made, e.g., if the approach was only tested 702
on a few datasets or with a few runs. In general, empirical results often depend on implicit 703
assumptions, which should be articulated. 704
• The authors should reflect on the factors that influence the performance of the approach. For 705
example, a facial recognition algorithm may perform poorly when image resolution is low or 706
images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide 707
closed captions for online lectures because it fails to handle technical jargon. 708
• The authors should discuss the computational efficiency of the proposed algorithms and how they 709
scale with dataset size. 710
• If applicable, the authors should discuss possible limitations of their approach to address prob- 711
lems of privacy and fairness. 712
• While the authors might fear that complete honesty about limitations might be used by reviewers 713
as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren’t 714
acknowledged in the paper. The authors should use their best judgment and recognize that indi- 715
vidual actions in favor of transparency play an important role in developing norms that preserve 716
the integrity of the community. Reviewers will be specifically instructed to not penalize honesty 717
concerning limitations. 718
3.Theory Assumptions and Proofs 719
Question: For each theoretical result, does the paper provide the full set of assumptions and a 720
complete (and correct) proof? 721
Answer: [NA] 722
23Justification: We do not have theoretical results. 723
Guidelines: 724
• The answer NA means that the paper does not include theoretical results. 725
• All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. 726
• All assumptions should be clearly stated or referenced in the statement of any theorems. 727
• The proofs can either appear in the main paper or the supplemental material, but if they appear in 728
the supplemental material, the authors are encouraged to provide a short proof sketch to provide 729
intuition. 730
• Inversely, any informal proof provided in the core of the paper should be complemented by formal 731
proofs provided in appendix or supplemental material. 732
• Theorems and Lemmas that the proof relies upon should be properly referenced. 733
4.Experimental Result Reproducibility 734
Question: Does the paper fully disclose all the information needed to reproduce the main exper- 735
imental results of the paper to the extent that it affects the main claims and/or conclusions of the 736
paper (regardless of whether the code and data are provided or not)? 737
Answer: [Yes] 738
Justification: We provide implementation details in each experimental section. Additionally, we 739
include the code as supplementary material and plan to release it publicly. 740
Guidelines: 741
• The answer NA means that the paper does not include experiments. 742
• If the paper includes experiments, a No answer to this question will not be perceived well by the 743
reviewers: Making the paper reproducible is important, regardless of whether the code and data 744
are provided or not. 745
• If the contribution is a dataset and/or model, the authors should describe the steps taken to make 746
their results reproducible or verifiable. 747
• Depending on the contribution, reproducibility can be accomplished in various ways. For exam- 748
ple, if the contribution is a novel architecture, describing the architecture fully might suffice, or if 749
the contribution is a specific model and empirical evaluation, it may be necessary to either make 750
it possible for others to replicate the model with the same dataset, or provide access to the model. 751
In general. releasing code and data is often one good way to accomplish this, but reproducibility 752
can also be provided via detailed instructions for how to replicate the results, access to a hosted 753
model (e.g., in the case of a large language model), releasing of a model checkpoint, or other 754
means that are appropriate to the research performed. 755
• While NeurIPS does not require releasing code, the conference does require all submissions to 756
provide some reasonable avenue for reproducibility, which may depend on the nature of the con- 757
tribution. For example 758
(a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce 759
that algorithm. 760
(b) If the contribution is primarily a new model architecture, the paper should describe the architec- 761
ture clearly and fully. 762
(c) If the contribution is a new model (e.g., a large language model), then there should either be a 763
way to access this model for reproducing the results or a way to reproduce the model (e.g., with 764
an open-source dataset or instructions for how to construct the dataset). 765
(d) We recognize that reproducibility may be tricky in some cases, in which case authors are wel- 766
come to describe the particular way they provide for reproducibility. In the case of closed-source 767
models, it may be that access to the model is limited in some way (e.g., to registered users), but 768
it should be possible for other researchers to have some path to reproducing or verifying the 769
results. 770
5.Open access to data and code 771
Question: Does the paper provide open access to the data and code, with sufficient instructions to 772
faithfully reproduce the main experimental results, as described in supplemental material? 773
Answer: [Yes] 774
Justification: We provide implementation details in each experimental section. Additionally, we 775
include the code as supplementary material and plan to release it publicly. 776
24Guidelines: 777
• The answer NA means that paper does not include experiments requiring code. 778
• Please see the NeurIPS code and data submission guidelines ( https://nips.cc/public/ 779
guides/CodeSubmissionPolicy ) for more details. 780
• While we encourage the release of code and data, we understand that this might not be possible, 781
so “No” is an acceptable answer. Papers cannot be rejected simply for not including code, unless 782
this is central to the contribution (e.g., for a new open-source benchmark). 783
• The instructions should contain the exact command and environment needed to run to reproduce 784
the results. See the NeurIPS code and data submission guidelines ( https://nips.cc/public/ 785
guides/CodeSubmissionPolicy ) for more details. 786
• The authors should provide instructions on data access and preparation, including how to access 787
the raw data, preprocessed data, intermediate data, and generated data, etc. 788
• The authors should provide scripts to reproduce all experimental results for the new proposed 789
method and baselines. If only a subset of experiments are reproducible, they should state which 790
ones are omitted from the script and why. 791
• At submission time, to preserve anonymity, the authors should release anonymized versions (if 792
applicable). 793
• Providing as much information as possible in supplemental material (appended to the paper) is 794
recommended, but including URLs to data and code is permitted. 795
6.Experimental Setting/Details 796
Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, 797
how they were chosen, type of optimizer, etc.) necessary to understand the results? 798
Answer: [Yes] 799
Justification: We provide implementation details and dataset details in each experimental section. 800
Guidelines: 801
• The answer NA means that the paper does not include experiments. 802
• The experimental setting should be presented in the core of the paper to a level of detail that is 803
necessary to appreciate the results and make sense of them. 804
• The full details can be provided either with the code, in appendix, or as supplemental material. 805
7.Experiment Statistical Significance 806
Question: Does the paper report error bars suitably and correctly defined or other appropriate 807
information about the statistical significance of the experiments? 808
Answer: [Yes] 809
Justification: We repeat few-shot training for 600 episodes on mini-ImageNet and 1000 episodes 810
on tiered-ImageNet, reporting the mean and variance for each method. 811
Guidelines: 812
• The answer NA means that the paper does not include experiments. 813
• The authors should answer ”Yes” if the results are accompanied by error bars, confidence inter- 814
vals, or statistical significance tests, at least for the experiments that support the main claims of 815
the paper. 816
• The factors of variability that the error bars are capturing should be clearly stated (for exam- 817
ple, train/test split, initialization, random drawing of some parameter, or overall run with given 818
experimental conditions). 819
• The method for calculating the error bars should be explained (closed form formula, call to a 820
library function, bootstrap, etc.) 821
• The assumptions made should be given (e.g., Normally distributed errors). 822
• It should be clear whether the error bar is the standard deviation or the standard error of the mean. 823
• It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report 824
a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is 825
not verified. 826
• For asymmetric distributions, the authors should be careful not to show in tables or figures sym- 827
metric error bars that would yield results that are out of range (e.g. negative error rates). 828
25• If error bars are reported in tables or plots, The authors should explain in the text how they were 829
calculated and reference the corresponding figures or tables in the text. 830
8.Experiments Compute Resources 831
Question: For each experiment, does the paper provide sufficient information on the computer 832
resources (type of compute workers, memory, time of execution) needed to reproduce the experi- 833
ments? 834
Answer: [Yes] 835
Justification: We provide implementation and dataset details in each experimental section. Ad- 836
ditionally, we elaborate on the required resources, including GPUs and training hours, for each 837
experiment. 838
Guidelines: 839
• The answer NA means that the paper does not include experiments. 840
• The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud 841
provider, including relevant memory and storage. 842
• The paper should provide the amount of compute required for each of the individual experimental 843
runs as well as estimate the total compute. 844
• The paper should disclose whether the full research project required more compute than the ex- 845
periments reported in the paper (e.g., preliminary or failed experiments that didn’t make it into 846
the paper). 847
9.Code Of Ethics 848
Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS 849
Code of Ethics https://neurips.cc/public/EthicsGuidelines ? 850
Answer: [Yes] 851
Justification: We reviewed the NeurIPS Code of Ethics. 852
Guidelines: 853
• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. 854
• If the authors answer No, they should explain the special circumstances that require a deviation 855
from the Code of Ethics. 856
• The authors should make sure to preserve anonymity (e.g., if there is a special consideration due 857
to laws or regulations in their jurisdiction). 858
10.Broader Impacts 859
Question: Does the paper discuss both potential positive societal impacts and negative societal 860
impacts of the work performed? 861
Answer: [Yes] 862
Justification: We discuss about broader impact in Conclusion. 863
Guidelines: 864
• The answer NA means that there is no societal impact of the work performed. 865
• If the authors answer NA or No, they should explain why their work has no societal impact or 866
why the paper does not address societal impact. 867
• Examples of negative societal impacts include potential malicious or unintended uses (e.g., dis- 868
information, generating fake profiles, surveillance), fairness considerations (e.g., deployment of 869
technologies that could make decisions that unfairly impact specific groups), privacy considera- 870
tions, and security considerations. 871
• The conference expects that many papers will be foundational research and not tied to particular 872
applications, let alone deployments. However, if there is a direct path to any negative applications, 873
the authors should point it out. For example, it is legitimate to point out that an improvement in 874
the quality of generative models could be used to generate deepfakes for disinformation. On the 875
other hand, it is not needed to point out that a generic algorithm for optimizing neural networks 876
could enable people to train models that generate Deepfakes faster. 877
26• The authors should consider possible harms that could arise when the technology is being used 878
as intended and functioning correctly, harms that could arise when the technology is being used 879
as intended but gives incorrect results, and harms following from (intentional or unintentional) 880
misuse of the technology. 881
• If there are negative societal impacts, the authors could also discuss possible mitigation strategies 882
(e.g., gated release of models, providing defenses in addition to attacks, mechanisms for moni- 883
toring misuse, mechanisms to monitor how a system learns from feedback over time, improving 884
the efficiency and accessibility of ML). 885
11.Safeguards 886
Question: Does the paper describe safeguards that have been put in place for responsible release of 887
data or models that have a high risk for misuse (e.g., pretrained language models, image generators, 888
or scraped datasets)? 889
Answer: [NA] 890
Justification: We believe our work does not have such risks. 891
Guidelines: 892
• The answer NA means that the paper poses no such risks. 893
• Released models that have a high risk for misuse or dual-use should be released with necessary 894
safeguards to allow for controlled use of the model, for example by requiring that users adhere to 895
usage guidelines or restrictions to access the model or implementing safety filters. 896
• Datasets that have been scraped from the Internet could pose safety risks. The authors should 897
describe how they avoided releasing unsafe images. 898
• We recognize that providing effective safeguards is challenging, and many papers do not require 899
this, but we encourage authors to take this into account and make a best faith effort. 900
12.Licenses for existing assets 901
Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, 902
properly credited and are the license and terms of use explicitly mentioned and properly respected? 903
Answer: [Yes] 904
Justification: We cited all datasets and code used in our paper. 905
Guidelines: 906
• The answer NA means that the paper does not use existing assets. 907
• The authors should cite the original paper that produced the code package or dataset. 908
• The authors should state which version of the asset is used and, if possible, include a URL. 909
• The name of the license (e.g., CC-BY 4.0) should be included for each asset. 910
• For scraped data from a particular source (e.g., website), the copyright and terms of service of 911
that source should be provided. 912
• If assets are released, the license, copyright information, and terms of use in the package should 913
be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for 914
some datasets. Their licensing guide can help determine the license of a dataset. 915
• For existing datasets that are re-packaged, both the original license and the license of the derived 916
asset (if it has changed) should be provided. 917
• If this information is not available online, the authors are encouraged to reach out to the asset’s 918
creators. 919
13.New Assets 920
Question: Are new assets introduced in the paper well documented and is the documentation pro- 921
vided alongside the assets? 922
Answer: [NA] 923
Justification: We do not release new assets. 924
Guidelines: 925
• The answer NA means that the paper does not release new assets. 926
• Researchers should communicate the details of the dataset/code/model as part of their submis- 927
sions via structured templates. This includes details about training, license, limitations, etc. 928
27• The paper should discuss whether and how consent was obtained from people whose asset is 929
used. 930
• At submission time, remember to anonymize your assets (if applicable). You can either create an 931
anonymized URL or include an anonymized zip file. 932
14.Crowdsourcing and Research with Human Subjects 933
Question: For crowdsourcing experiments and research with human subjects, does the paper in- 934
clude the full text of instructions given to participants and screenshots, if applicable, as well as 935
details about compensation (if any)? 936
Answer: [NA] 937
Justification: Our paper does not involve crowdsourcing nor research with human subjects. 938
Guidelines: 939
• The answer NA means that the paper does not involve crowdsourcing nor research with human 940
subjects. 941
• Including this information in the supplemental material is fine, but if the main contribution of the 942
paper involves human subjects, then as much detail as possible should be included in the main 943
paper. 944
• According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other 945
labor should be paid at least the minimum wage in the country of the data collector. 946
15.Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Sub- 947
jects 948
Question: Does the paper describe potential risks incurred by study participants, whether such 949
risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or 950
an equivalent approval/review based on the requirements of your country or institution) were ob- 951
tained? 952
Answer: [NA] 953
Justification: Our paper does not involve crowdsourcing nor research with human subjects. 954
Guidelines: 955
• The answer NA means that the paper does not involve crowdsourcing nor research with human 956
subjects. 957
• Depending on the country in which research is conducted, IRB approval (or equivalent) may be 958
required for any human subjects research. If you obtained IRB approval, you should clearly state 959
this in the paper. 960
• We recognize that the procedures for this may vary significantly between institutions and loca- 961
tions, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their 962
institution. 963
• For initial submissions, do not include any information that would break anonymity (if applica- 964
ble), such as the institution conducting the review. 965
28