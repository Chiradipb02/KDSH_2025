Published in Transactions on Machine Learning Research (12/2022)
Lazy vs hasty: linearization in deep networks impacts learning
schedule based on example difficulty
Thomas George georgeth@mila.quebec
Mila, Université de Montréal
Guillaume Lajoie g.lajoie@umontreal.ca
Mila, Université de Montréal
Canada CIFAR AI Chair
Aristide Baratin a.baratin@samsung.com
Samsung, SAIT AI Lab, Montréal
Reviewed on OpenReview: https: // openreview. net/ forum? id= lukVf4VrfP
Abstract
Among attempts at giving a theoretical account of the success of deep neural networks, a
recent line of work has identified a so-called ‘lazy’ training regime in which the network
can be well approximated by its linearization around initialization. Here we investigate
the comparative effect of the lazy (linear) and feature learning (non-linear) regimes on
subgroups of examples based on their difficulty. Specifically, we show that easier examples
are given more weight in feature learning mode, resulting in faster training compared to more
difficult ones. In other words, the non-linear dynamics tends to sequentialize the learning
of examples of increasing difficulty. We illustrate this phenomenon across different ways to
quantify example difficulty, including c-score, label noise, and in the presence of easy-to-learn
spurious correlations. Our results reveal a new understanding of how deep networks prioritize
resources across example difficulty.
1 Introduction
Understanding the performance of deep learning algorithms has been the subject of intense research efforts in
the past few years, driven in part by observed phenomena that seem to defy conventional statistical wisdom
(Neyshabur et al., 2015; Zhang et al., 2017; Belkin et al., 2019). Notably, many such phenomena have been
analyzed rigorously in simpler contexts of high dimensional linear or random feature models (e.g., Hastie
et al., 2022; Bartlett et al., 2021), which shed a new light on the crucial role of overparametrization in the
performance of such systems. These results also apply to the so-called lazy training regime (Chizat et al.,
2019), in which a deep network can be well approximated by its linearization at initialization, characterized
by the neural tangent kernel (NTK) (Jacot et al., 2018; Du et al., 2019b; Allen-Zhu et al., 2019). In this
regime, deep networks inherit the inductive bias and generalization properties of kernelized linear models.
It is also clear, however, that deep models cannot be understood solely through their kernel approximation.
Trained outside the lazy regime (e.g., Woodworth et al., 2020), they are able to learn adaptive representations,
with a time-varying tangent kernel (Fort et al., 2020) that specializes to the task during training (Kopitkov &
Indelman, 2020; Baratin et al., 2021; Paccolat et al., 2021; Ortiz-Jiménez et al., 2021). Several results showed
examples where their inductive bias cannot be characterized in terms of a kernel norm (Savarese et al., 2019;
Williams et al., 2019), or where they provably outperform any linear method (Malach et al., 2021). Yet,
the specific mechanisms by which the two regimes differ, which could explain the performance gaps often
observed in practice (Chizat et al., 2019; Geiger et al., 2020), are only partially understood.
1Published in Transactions on Machine Learning Research (12/2022)
Our work contributes new qualitative insights into this problem, by investigating the comparative
effect of the lazy and feature learning regimes on the training dynamics of various groups of
examples of increasing difficulty . We do so by means of a control parameter that modulates linearity of
the parametrization and smoothly interpolates the two training regimes (Chizat et al., 2019; Woodworth
et al., 2020). We provide empirical evidence and theoretical insights suggesting that the feature learning
regime puts higher weight on easy examples at the beginning of training, which results in an increased
learning speed compared to more difficult examples. This can be understood as an instance of the simplicity
biasbrought forward in recent work (Arpit et al., 2017; Rahaman et al., 2019; Kalimeris et al., 2019), which
we show here to be much more pronounced in the feature learning regime. It also resonates with the old idea
that generalization can benefit from some curriculum learning strategy (Elman, 1993; Bengio et al., 2009).
Contributions
•We introduce and test the hypothesis of qualitatively different example importance between linear
and non-linear regimes;
•Using adequately normalized plots, we present a unified picture using 4 different ways to quantify
example difficulty, where easy examples are prioritized in the non-linear regime. We illustrate
empirically this phenomenon across different ways to quantify example difficulty, including c-score
(Jiang et al., 2021), label noise, and spurious correlation to some easy-to-learn set of features.
•We illustrate some of our insights in a simple quadratic model amenable to analytical treatment.
The general setup of our experiments is introduced in Section 2. Section 3 is our empirical study, which
begins with an illustrative example on a toy dataset (Section 3.1), followed by experiments on CIFAR 10 in
two setups where example difficulty is quantified using respectively c-scores and label noise (Section 3.2). We
also examine standard setups where easy examples are those with strong correlations between their labels and
some spurious features (Section 3.3). Section 4 illustrates our findings with a theoretical analysis of a specific
class of quadratic models, whose training dynamics is solvable in both regimes. We conclude in Section 5.
Related work The neural tangent kernel was initially introduced in the context of infinitely wide networks,
for a specific parametrization that provably leads to the lazy regime (Jacot et al., 2018). Such a regime allows
to cast deep learning as a linear model using a fixed kernel, enabling import of well-known results from linear
models, such as guarantees of convergence to a global optimum (Du et al., 2019b;a; Allen-Zhu et al., 2019).
On the other hand, it is also clear that the kernel regime does not fully capture the behavior of deep models
– including, in fact, infinitely wide networks (Yang & Hu, 2021). For example, in the so-called mean field
limit, training two-layer networks by gradient descent learns adaptive representations (Chizat & Bach, 2018;
Mei et al., 2018) and it can be shown that the inductive bias cannot be characterized in terms of a RKHS
norm (Savarese et al., 2019; Williams et al., 2019). Performance gaps between the two regimes are also often
observed in practice (Chizat et al., 2019; Arora et al., 2019; Geiger et al., 2020).
Subsequent work showed and analyzed how, for a fixed (finite-width) network, the scaling of the model at
initialization also controls the transition between the lazy regime, governed by the empirical neural tangent
kernel, and the standard feature learning regime (Chizat et al., 2019; Woodworth et al., 2020; Agarwala et al.,
2020). In line with this prior work, in our experiments below we use a scaling parameter α>0that modulates
linearity of the parametrization and allows us to smoothly interpolates between the vanilla training ( α= 1)
where features are learned and the lazy ( α→∞) regime where they are not. We compare training runs with
various values of αand empirically assess linearity with several metrics described in Section 2 below.
Our results are in line with a group of work showing how deep networks learn patterns and functions of
increasing complexity during training (Arpit et al., 2017; Kalimeris et al., 2019). An instance of this is the
so-called spectral bias empirically observed in Rahaman et al. (2019) where a sum of sinusoidal signals is
incrementally learned from low frequencies to high frequencies, or in Zhang et al. (2021) that use Fourier
decomposition to analyze the function learned by a deep convolutional network on a vision task. The spectral
bias is well understood in linear regression, where the gradient dynamics favours the large singular directions
of the feature matrix. For neural networks in the lazy regime, spectral analysis of the neural tangent kernel
2Published in Transactions on Machine Learning Research (12/2022)
have been investigated for architectures and data (e.g, uniform data on the sphere) allowing for explicit
computations (e.g., decomposition of the NTK in terms of spherical harmonics) (Bietti & Mairal, 2019; Basri
et al., 2019; Yang & Salman, 2019). This is a setup where the spectral bias can be rigorously analyzed, i.e.,
we can get explicit information about the type of functions that are learned quickly and generalize well. In
this context, our work specifically focuses on comparing the lazy and the standard feature learning regimes.
Our theoretical model in Section 4 reproduces some of the key technical ingredients of known analytical
results (Saxe et al., 2014; Gidel et al., 2019) on deep linear networks. These results show how, in the context
of multiclass classification or matrix factorization, the principal components of the input-output correlation
matrix are learned sequentially from the highest to the lowest mode. Note however that in the framework
of these prior works, the number of modes is bounded by the output dimension - which thus reduces to
one in the context of regression or binary classification. By contrast, our theoretical analysis applies to the
components of the vector of labels Y∈Rnin the eigenbasis of the kernel XX⊤defined by the input matrix
X∈Rn×d. Thus, despite the technical similarities, our framework approaches the old problem of the relative
learning speed of different modes in factorized models from a novel angle. In particular, it allows us to frame
the notion of example difficulty in this context.
Example difficulty is a loosely defined concept that has been the subject of intense research recently. Ways to
quantify example difficulty for a model/algorithm to learn individual examples include e.g. self-influence (Koh
& Liang, 2017), example forgetting (Toneva et al., 2019), TracIn (Pruthi et al., 2020), C-scores (Jiang et al.,
2021), or prediction depth (Baldock et al., 2021). We believe that a comprehensive theory of generalization
in deep learning will require to understand how neural networks articulate learning and memorization to
both fit the head (easy examples) and the tail (difficult examples) of the data distribution (Hooker et al.,
2020; Feldman & Zhang, 2020; Sagawa et al., 2020a;b).
2 Setup
We consider neural networks fθparametrized by θ∈Rp(i.e. weights and biases for all layers), and trained
by minimizing some task-dependent loss function ℓ(θ) :=/summationtextn
i=1ℓi(fθ(xi))computed on a training dataset
{x1,···,xn}, using variants of gradient descent,
θ(t+1)=θ(t)−η∇θℓ(θ(t)), (1)
with some random initialization θ(0)and a chosen learning rate η>0.
Linearization A Taylor expansion and the chain rule give the corresponding updates f(t):=fθ(t)for any
network output, at first order in the learning rate,
f(t+1)(x)≃f(t)(x)−ηn/summationdisplay
i=1K(t)(x,xi)∇ℓi, (2)
which depend on the time-varying tangent kernel K(t)(x,x′) :=∇θf(t)(x)⊤∇θf(t)(x′). Thelazy regime is
one where this kernel remains nearly constant throughout training. Training the network in this regime thus
corresponds to training the linear predictor defined by
¯fθ(x) :=fθ(0)(x) + (θ−θ(0))⊤∇θfθ(0)(x). (3)
Modulating linearity In our experiments, following Chizat et al. (2019), we modulate the level of "non-
linearity" during training of a deep network with a scalar parameter α≥1, by replacing our prediction fθby
fα
θ(x) :=fθ(0)(x) +α(fθ(x)−fθ(0)(x)) (4)
and by rescaling the learning rate as ηα=η/α2. In this setup, gradient descent steps in parameter space are
rescaled by 1/αwhile steps in function space (up to first order) are in O(1)inα.1αcan also be viewed as
1Under some assumptions such as strong convexity of the loss, it was shown (Chizat et al., 2019, Thm 2.4) that as α→∞,
the gradient descent trajectory of fα
θ(t)gets uniformly close to that of the linearization ¯fθ(t).
3Published in Transactions on Machine Learning Research (12/2022)
102103
GD iterations0.00.20.40.6training loss(c)
(d)
(e)linearized (=100)
non-linear (=1)
(a) task +
example dataset
(c) loss(xtest) at
training loss=0.5
0.5
0.00.5
(d) loss(xtest) at
training loss=0.4
0.5
0.00.5
(e) loss(xtest) at
training loss=0.3
0.5
0.00.5
Figure 1: 100 randomly initialized runs of a 4 layers MLP trained on the yin-yang dataset (a)using gradient
descent in both the non-linear ( α= 1) and linearized ( α= 100) setting. The training losses (b)show a speed-
up in the non-linear regime: in order to compare both regimes at equal progress, we normalize by comparing
models extracted at equal training loss thresholds (c),(d)and(e). We visualize the differences ∆loss(xtest)=
lossfnon-linear (xtest)−lossflinear (xtest)for test points paving the 2d square [−1,1]2using a color scale. We observe that
these differences are not uniformly spread across examples: instead they suggest a comparative bias of the non-linear
regime towards correctly classifying easy examples (large areas of the same class), whereas difficult examples (e.g. the
small disks) are boosted in the linear regime.
controlling the level of feature adaptivity , where large values of αresult in linear training where features are
not learned. We will also experiment with α<1below, which enhances adaptivity compared the standard
regime (α= 1). The goal of this procedure is two-fold: (i)to be able to smoothly interpolate between the
standard regime at α= 1and the linearized one, (ii)to work with models that are nearly linearized yet
practically trainable with gradient descent.
Linearity measures In order to assess linearity of training runs for empirically chosen values of the
re-scaling factor α, we track three different metrics during training (fig 2 right):
•Sign similarity counts the proportion of ReLUs in all layers that have kept the same activation
status ( 0or>0) from initialization.
•Tangent kernel alignment measures the similarity between the Gram matrix K(t)
ij=K(t)(xi,xj)
of the tangent kernel with its initial value K(0), using kernel alignment (Cristianini et al., 2001)
KA(K(t),K(0)) =Tr[K(t)K(0)]
∥K(t)∥F∥K(0)∥F(∥·∥Fis the Froebenius norm) (5)
•Representation alignment measures the similarity of the last non-softmax layer representation
ϕ(t)
R(x)with its initial value ϕ(0)
R, in terms of the kernel alignment (eq. 5) of the corresponding Gram
matrices (K(t)
R)ij=ϕ(t)
R(xi)⊤ϕ(t)
R(xj).
3 Empirical Study
3.1 A motivating example on a toy dataset
We first explore the effect of modulating the training regime for a binary classification task on a toy dataset
with 2d inputs, for which we can get a visual intuition. We use a fully-connected network with 4 layers and
ReLU activations. For 100independent initial parameter values, we generate 100training examples uniformly
on the square [−1,1]2from the yin-yang dataset (fig. 1.a). We perform 2 training runs for α= 1andα= 100
using (full-batch) gradient descent with learning rate 0.01.
Global training speed-up and normalization After the very first few iterations, we observe a speed-up
in training progress of the non-linear regime (fig. 1.b). This is consistent with previously reported numerical
experiments on the lazy training regime (Chizat et al., 2019, section 3). This raises the question of whether
4Published in Transactions on Machine Learning Research (12/2022)
this acceleration comes from a global scaling in all directions, or if it prioritizes certain particular groups of
examples. We address this question by comparing the training dynamics at equal progress: we counteract
the difference in training speed by normalizing by the mean training loss, and we compare the linear and
non-linear regimes at common thresholds ((c), (d) and (e) horizontal lines in fig. 1.b).
Comparing linear and non-linear regimes At every threshold value, we compute the predictions on
test examples uniformly paving the 2d square. We compare both regimes on individual test examples by
plotting (fig. 1.c, 1.d, 1.e) the differences in loss values,
∆loss(xtest) =lossfnon-linear (xtest)−lossflinear (xtest) (6)
Red (resp. blue) areas indicate a lower test loss for the linearized (resp. non-linear) model. Remarkably,
the resulting picture is not uniform: these plots suggest that compared to the linear regime, the non-linear
training dynamics speeds up for specific groups of examples (the large top-right and bottom-left areas) at
the expense of examples in more intricate areas (both the disks and the areas between the disks and the
horizontal boundary).
3.2 Hastening easy examples
We now experiment with deeper convolutional networks on CIFAR10, in two setups where the training
examples are split into groups of varying difficulty. Additional experiments with various other choices of
hyperparameters and initialization seed are reported in Appendix F.
3.2.1 Example difficulty using C-scores
Inthissectionwequantifyexampledifficultyusingconsistencyscores(C-scores)(Jiangetal.,2021). Informally,
C-scores measure how likely an example is to be well-classified by models trained on subsets of the dataset
that do not contain it. Intuitively, examples with a high C-score share strong regularities with a large group
of examples in the dataset. Formally, given a choice of model f, for each (input, label) pair (x,y)in a dataset
D, Jiang et al. (2021) defines its empirical consistency profile as:
ˆCD,n(x,y) =ˆEr
Dn∼D\{ (x,y)}[P(f(x;D) =y)], (7)
where ˆEris the empirical average over rsubsetsDof sizenuniformly sampled from Dexcluding (x,y),
andf(·,D)is the model trained on D. A scalar C-score is obtained by averaging the consistency profile
over various values of n= 1,···,|D|− 1. For CIFAR10 we use pre-computed scores available as https:
//github.com/pluskid/structural-regularity .
While training, we compute the loss separately on 10 subsets of the training set ranked by increasing C-scores
deciles (e.g. examples in the last subset are top-10% C-scores), for both the training set and test set. We also
train a linearized copy of this network (so as to share the same initial conditions) with α= 100. Theα= 1
run is trained for 200epochs ( 64 000SGD iterations) whereas in order to converge the α= 100run is trained
for1 000epochs ( 320 000SGD iterations). Similarly to fig. 1, we normalize training progress using the mean
training loss in order to compare regimes at equal progress. We check (fig. 2 top right) that the model with
α= 100indeed stays in the linear regime during the whole training run, since all 3 linearity metrics that
we report essentially remain equal to 1. By contrast, in the non-linear regime ( α= 1), a steady decrease of
linearity metrics as training progresses indicates a rotation of the NTK and the representation kernel, as well
as lower sign similarity of the ReLUs.
The results are shown in fig. 2 (top left). As one might expect, examples with high C-scores are learned faster
during training than examples with low C-scores in both regimes. Remarkably, this effect is amplified in the
non-linear regime compared to the linear one, as we can observe by comparing e.g. the top (resp. bottom)
decile in light green (resp dark blue). This illustrates a relative acceleration of the non-linear regime in the
direction of easy examples.
5Published in Transactions on Machine Learning Research (12/2022)
0.5 1.0 1.5 2.0 2.5
mean training loss0.00.51.01.52.02.5per c-score bin training lossnon-linear (=1)
linear (=100)
top 10% c-scores
70-80% decile
20-30% decile
bottom 10%
0.0 0.5 1.0 1.5 2.0
mean training loss0.30.40.50.60.70.80.91.0linearity metrics
sign similarity
repr. alignment
ntk alignmentnon-linear (=1)
linear (=100)
0.0 0.5 1.0 1.5 2.0
clean examples loss0.00.51.01.52.02.53.03.5loss
non-linear (=1) noisy
non-linear (=1) test
linear (=100) noisy
linear (=100) test
0.0 0.5 1.0 1.5 2.0
clean examples loss0.20.30.40.50.60.70.80.91.0linearity metrics
sign similarity
repr. alignment
ntk alignmentnon-linear (=1)
linear (=100)
Figure 2: Starting from the same initial parameters, we train 2 ResNet18 models with α= 1(standard training) and
α= 100(linearized training) on CIFAR10 using SGD with momentum. (Top left) We compute the training loss
separately on 10 subgroups of examples ranked by their C-scores. Training progress is normalized by the mean training
loss on the x-axis. Unsurprisingly, in both regimes examples with high C-scores are learned faster. Remarkably, this
ranking is more pronounced in the non-linear regime as can be observed by comparing dashed and solid lines of the
same color. (Bottom left) We randomly flip the class of 15% of the training examples. At equal progress (measured
by equal clean examples loss), the non-linear regime prioritizes learning clean examples and nearly ignores noisy
examples compared to the linear regime since the solid curve remains higher for the non-linear regime. Concomitantly,
the non-linear test loss reaches a lower value. (Right) On the same training run, as a sanity check we observe that
theα= 100training run remains in the linear regime throughout since all metrics stay close to 1, whereas in the
α= 1run, the NTK and representation kernel rotate, and a large part of ReLU signs are flipped. These experiments
are completed in Appendix F with accuracy plots for the same experiments, and with other experiments with varying
initial model parameters and mini-batch order.
3.2.2 Example difficulty using label noise
In this section we use label noise to define difficult examples. We train a ResNet18 on CIFAR10 where 15% of
the training examples are assigned a wrong (random) label. We compute the loss and accuracy independently
on the regular examples with their true label, and the noisy examples whose label is flipped. In parallel, we
train a copy of the initial model in the linearized regime with α= 100. Fig. 2 bottom right shows the 3
linearity metrics during training in both regimes.
The results are shown in fig. 2 (bottom left). In both regimes, the training process begins with a phase where
only examples with true labels are learned, causing the loss on examples with wrong labels to increase. A
second phase starts (in this run) at 1.25clean examples loss for the non-linear regime and 1.5clean loss for
the linear regime, where random labels are getting memorized. We see that the first phase takes a larger
part of the training run in the non-linear regime than in the linear regime. We interpret this as the fact
that the non-linear regime prioritizes learning easy examples. As a consequence, the majority of the training
process is dedicated to learning the clean examples in the non-linear regime, whereas in the linear regime
both the clean and the noisy labels are learned simultaneously passed the 1.5clean loss mark. Concomitantly,
6Published in Transactions on Machine Learning Research (12/2022)
0.75 0.80 0.85 0.90 0.95 1.00
accuracy train0.500.550.600.650.700.75CelebA
accuracy testsuper adaptive (=0.5)
non-linear (=1)
linear (=100)
0.5 0.6 0.7 0.8 0.9 1.0
train acc. ex. with spurious correl.0.50.60.70.80.91.0train acc. ex. w/o spurious correl.super adaptive (=0.5)
non-linear (=1)
linear (=100)
0.5 0.6 0.7 0.8 0.9 1.0
test acc. ex. with spurious correl.0.50.60.70.80.91.0test acc. ex. w/o spurious correl.super adaptive (=0.5)
non-linear (=1)
linear (=100)
0.7 0.8 0.9 1.0
accuracy train0.500.550.600.650.700.750.80Waterbirds
accuracy testsuper adaptive (=0.5)
non-linear (=1)
linear (=100)
0.5 0.6 0.7 0.8 0.9 1.0
train acc. ex. with spurious correl.0.30.40.50.60.70.80.91.0train acc. ex. w/o spurious correl.super adaptive (=0.5)
non-linear (=1)
linear (=100)
0.5 0.6 0.7 0.8 0.9 1.0
test acc. ex. with spurious correl.0.30.40.50.60.70.80.91.0test acc. ex. w/o spurious correl.super adaptive (=0.5)
non-linear (=1)
linear (=100)
Figure 3: We visualize the trajectories of training runs on 2 spurious correlations setups, by computing the accuracy
on 2 separate subsets: one with examples that contain the spurious feature ( with spurious ), the other one without
spurious correlations ( w/o spurious ). On Celeb A (top row) , the attribute ’blond’ is spuriously correlated with
the gender ’woman’. In the first phase of training we observe that (left)the test accuracy is essentially higher for
the linear run, which can be further explained by observing that (middle) the training accuracy for w/o spurious
examples increases faster in the linear regime than in non-linear regimes at equal with spurious training accuracy.
(right)A similar trend holds for test examples. In this first part the linear regime is less sensitive to the spurious
correlation (easy examples) thus gets better robustness. (bottom row) On Waterbirds, the background (e.g. a lake)
is spuriously correlated with the label (e.g. a water bird). (left)We observe the same hierarchy between the linear
run and other runs. In the first training phase, the linear regime is less prone to learning the spurious correlation: the
w/o spurious accuracy stays higher while the with spurious examples are learned ( (middle) and(right)). These
experiments are completed in fig. 12 in Appendix F with varying initial model parameters and mini-batch order.
comparing the sweet spot (best test loss) of the two regimes indicates a higher robustness to label noise thus
a clear advantage for generalization in the non-linear regime.
3.3 Spurious correlations
We now examine setups where easy examples are those with strong correlations between their labels and
some spurious feature (Sagawa et al., 2020b). We experiment with CelebA (Liu et al., 2015) and Waterbirds
(Wah et al., 2011) datasets.
CelebA (Liu et al., 2015) is a collection of photographs of celebrities’ faces, each annotated with its
attributes, such as hair color or gender. Similarly to Sagawa et al. (2020b), our task is to classify pictures
based on whether the person is blond or not. In this dataset, the attribute "the person is a woman" is
spuriously correlated with the attribute "the person is blond", since the attribute "blond" is over-represented
among women ( 24%are blond) compared to men ( 2%are blond).
7Published in Transactions on Machine Learning Research (12/2022)
We use 20 000examples of CelebA to train a ResNet18 classifier on the task of predicting whether a person
is blond or not, using SGD with learning rate 0.01, momentum 0.9and batch size 100. We also extract a
balanced dataset with 180(i.e. the total number of blond men in the test set) examples in each of 4 categories:
blond man, blond woman, non-blond man and non-blond woman. While training progresses, we measure the
loss and accuracy on the subgroups man and woman. Starting from the same initial conditions, we train 3
different classifiers with α∈{.5,1,100}.
Waterbirds (Wah et al., 2011) is a smaller dataset of pictures of birds. We reproduce the experiments
of Sagawa et al. (2020a) where the task is to distinguish land birds and water birds. In this dataset, the
background is spuriously correlated with the type of bird: water birds are typically photographed on a
background such as a lake, and similarly there is generally no water in the background of land birds. There
are exceptions: a small part of the dataset consists of land birds on a water background and vice versa (e.g.
a duck walking in the grass).
We use 4 795training examples of the Waterbirds dataset. Since the dataset is smaller, we start from a
pre-trained ResNet18 classifier from default PyTorch models (pre-trained on ImageNet). We replace the last
layer with a freshly initialized binary classification layer, and we set batch norm layers to evaluation mode2
We train using SGD with learning rate 0.001, momentum 0.9and minibatch size 100.
From the training set, we extract a balanced dataset with 180examples in each of 4 groups: land birds on
land background, land birds on water background, water bird on land background, and water bird on land
background, which we group in two sets: with spurious when the type of bird and the background agree,
andw/o spurious otherwise. While training, we measure the accuracy separately on these 2 sets. We train
3 different classifiers with α∈{.5,1,100}.
Looking at fig. 3 left, for both the Waterbirds and the CelebA experiments we identify two phases: in the
first phase the test accuracy is higher for the linear α= 100run than for other runs. In the second phase all 3
runs seem to converge, with a slight advantage for non-linear runs in Waterbirds. Taking a closer look at the
first phase in fig. 3 middle and right, we understand this difference in test accuracy in light of spurious and
non-spurious features: in the non-linear regime, the training dynamics learns the majority examples faster, at
the cost of being more prone to spurious correlations. This can be seen both on the balanced training set and
the balanced test set.
4 Theoretical Insights
The goal here is to illustrate some of our insights in a simple setup amenable to analytical treatment.
4.1 A simple quadratic model
We consider a standard linear regression analysis: given ninput vectors xi∈Rdwith their corresponding
labelsyi∈R,1≤i≤n, the goal is to fit a linear function fθ(x)to the data by minimizing the least-squares
lossℓ(θ) :=1
2/summationtext
i(fθ(xi)−yi)2. We will focus on a specific quadratic parametrization, which can be viewed
as a subclass of two-layer networks with linear activations.
Notation We denote by X∈Rn×dthe matrix of inputs and by y∈Rnthe vector of labels. We consider
the singular value decomposition (SVD),
X=UMV⊤:=rX/summationdisplay
λ=1√µλuλv⊤
λ (8)
whereU∈Rn×n,V∈Rd×dare orthogonal and Mis rectangular diagonal. rXdenotes the rank of X,
µ1≥···≥µrX>0are the non zero (squared) singular values; we also set µλ= 0forrX<λ≤max(n,d).
2In order not to interfere with αscaling (see section 2), and since we consider that batch norm is a whole different theoretical
challenge by itself, we chose to turn it off, and instead keep the mean and variance buffers to their pre-trained value. See
appendix D for further discussion of this point.
8Published in Transactions on Machine Learning Research (12/2022)
Left and right singular vectors extend to orthonormal bases (u1,···,un)and(v1,···,vd)ofRnandRd,
respectively. In what follows we assume, without loss of generality3, that the vector of labels has positive
components in the basis uλ, i.e.,yλ:=u⊤
λy≥0forλ= 1,···n.
Parametrization We consider the following class of functions
fθ(x) =θ⊤x,θ:=1
2d/summationdisplay
λ=1w2
λvλ (9)
In this setting, the least-squares loss is minimized by gradient descent over the vector parameter w=
[w1,···wd]⊤. Given an initialization w0, we want to compare the solution of the vanilla gradient (non linear)
dynamics with the solution of the lazy regime, which corresponds to training the linearized function (3),
given in our setting by ¯f¯θ(x) =¯θ⊤xwhere ¯θ=/summationtextd
λ=1wλw0
λvλ.
4.2 Gradient dynamics
For simplicity, we analyze the continuous-time counterpart of gradient descent,
w(0) = w0,˙w(t) =−∇wℓ(θ(t)) (10)
where the dot denotes the time-derivative. Making the gradients explicit and differentiating (9) yield
˙θ(t) =−Σ(t)(θ(t)−θ∗),Σ(t) =VDiag(µ1w2
1(t),···,µdw2
d(t))V⊤(11)
whereθ∗is the solution of the linear dynamics.4Note how Σ(t)is obtained from the input correlation
matrixX⊤Xby rescaling each eigenvalue µλby the time-varying factor w2
λ. By contrast, the lazy regime is
described by the equation obtained from (11) by replacing Σ(t)the constant matrix Σ(0).
In the proposition below (proved in Appendix A), we consider the system (10, 11) initialized as θ0:=
1
2/summationtextd
λ=1(w0
λ)2vλwhere we assume that w0
λ̸= 0for allλ. We denote by ˜yλthe components of the input-label
correlation vector X⊤y∈Rdin the basisvλ: we have ˜yλ=√µλyλfor1≤λ≤rXand0whenrX<λ≤d.
Proposition 1 The solution of (10, 11) is given by,
θ(t) =d/summationdisplay
λ=1θλ(t)vλ, θλ(t) =

θ0
λθ∗
λ
θ0
λ−e−2˜yλt(θ0
λ−θ∗
λ)ifθ∗
λ̸= 0
θ0
λ
1 + 2µλθ0
λtifθ∗
λ= 0(12)
By contrast, the solution in the linearized regime where Σ(t)≈Σ(0)is,
θλ(t) =θ∗
λ+e−µλθ0
λt(θ0
λ−θ∗
λ) (13)
4.3 Discussion
While the gradient dynamics (12,13) converge to the same solution θ∗, we see that the convergence rates of
the various modes differ in the two regimes. To quantify this, for each dynamical mode 1≤λ≤rXsuch
that|θ∗
λ|̸= 0, lettλ(ϵ),tlinλ(ϵ)be the times required for θλto beϵ-close to convergence, i.e |θλ−θ∗
λ|=ϵ, in
the two regimes. Substituting into (12) and (13) we find that, close to convergence ϵ≪θ∗
λand for a small
initialization, θ0
λ≪θ∗
λ,
tλ(ϵ) =1
˜yλlogθ∗
λ
ϵθ0
λ, tlin
λ(ϵ) =1
µλθ0
λlogθ∗
λ
ϵ(14)
up to terms in O(ϵ/θ∗
λ,θ0
λ/θ∗
λ). Two remarks are in order:
3One can use the reflection invariance√µλuλv⊤
λ=√µλ(−uλ)(−vλ)⊤of the SVD to flip the sign of yλ.
4Explicitly,θ∗=Σ+X⊤y+P⊥(θ0)whereΣ+is the pseudoinverse of the input correlation matrix Σ=X⊤XandP⊥
projects onto the null space of X. It decomposes as θ∗=/summationtextd
λ=1θ∗
λvλin the basis vλ, whereθ∗
λ=yλ/√µλfor1≤λ≤rXand
θ∗
λ=θ0
λforrX<λ≤d.
9Published in Transactions on Machine Learning Research (12/2022)
0 50 100 150 200 250
time (arbitrary scale)02468per example squared errorlinear
non-linear
0 100 200
time (arbitrary scale)0.00.51.01.52.0Clean/noisy examples MSElinear
non-linear
clean
noisy
0 100 200
time (arbitrary scale)0.00.51.01.52.0Per group MSElinear
non-linear
spurious
no spur.
Figure 4: (left)Different input/label correlation (example 1): examples are learned in a flipped order in the two
regimes. (middle) Label noise (example 2): the non-linear dynamics prioritizes learning the clean labels (right)
Spurious correlations (example 3): the non-linear dynamics prioritizes learning the spuriously correlated feature.
These analytical curves are completed with numerical experiments on standard (dense) 2-layer MLP in figure 14 in
appendix G, which shows a similar qualitative behaviour.
Linearization impacts learning schedule. Specifically, while the learning speed of each mode depends
on the components of the label vector in the non linear regime, through ˜yλ=√µλyλ, it does not in the
linearized one. Thus, the ratio of ϵ-convergence times for two given modes λ,λ′istλ/tλ′=˜yλ/˜yλ′in the
non-linear case and tlinλ/tlinλ=µλ′θ0
λ′/µλθ0
λin the linearized case, up to logarithmic factors.
Sequentialization of learning. Non-linearity, together with a vanishingly small initialization, induces a
sequentialization of learning for the various modes (see also Gidel et al., 2019, Thm 2). To see this, pick
a modeλand consider, for any other mode λ′, the value θλ′(tλ)at the time tλ:=tλ(ϵ)whereλreaches
ϵ-convergence. Let us also write θ0
λ=σ˜θ0
λwhereσis small and ˜θ0
λ=O(1)asσ→0. Then elementary
manipulations show the following:
θλ′(tλ) =θ∗
λ′˜θ0
λ′
˜θ0
λ′+/bracketleftbig
ϵ˜θ0
λ′/θ∗
λ′/bracketrightbig2˜yλ′
˜yλσ2˜yλ′
˜yλ−1=σ→0/braceleftigg
θ∗
λ′ ˜yλ′>˜yλ
0 ˜yλ′<˜yλ(15)
In words, for fixed ϵ>0and in the limit of small initialization, the mode λgetsϵ-close to convergence before
any of the subdominant mode deviates from their (vanishing) initial value: the modes are learned sequentially.
4.4 Mode vs example difficulty
We close this section by illustrating the link between modeandexample difficulty on three concrete examples
of structure for the training data {xi,yi}n
i=1. In what follows, we consider the overparametrized setting where
d≥n. We denote by e1,···edthe canonical basis of Rd.
For each of the examples below, we consider an initialization w0=√σ[1···1]⊤with small σ >0and the
corresponding solutions in Prop. 1 for both regimes.
Example 1 We begin with a rather trivial setup where each mode corresponds to a training example. It
will illustrate how non-linearity can reversethe learning order of the examples. Given a sequence of strictly
positive numbers µ1≥···µn>0, we consider the training data,
xi=√µiei, yi=µn−i+1/√µi, 1≤i≤n (16)
In the linearized regime, fθ(t)(xi)converges to yiat the linear rate σµi; the model learns faster the examples
with higher µi, hence with lowerindexi. In the non linear regime, the examples are learned sequentially
according to the value ˜yi=√µiyi=µn−i+1, hence from high to low indexi. Thus in this setting, linearization
flips the learning order of the training examples (see fig. 4 left).
In examples 2 and 3 below we aim at modelling situations where the labels depend on low-dimensional (in
this case, one dimensional) projections on the inputs (e.g. an image classification problem where the labels
10Published in Transactions on Machine Learning Research (12/2022)
mainly depend on the low frequencies of the image). These two examples mirror the two sets of experiments
in Sections 3.2.2 and 3.3, respectively.
Example 2 We consider a simple classification setup on linearly separable data with label noise. Here we
assumed>n. Conditioned on a set of binary labels yi=±1, the inputs are given by
xi=κiyie1+ηei+11≤i≤n (17)
whereκi=±1is some ‘label flip’ variable and η>0. We assume we have q‘noisy’ examples with flipped
labels, i.e.κi=−1, where 1≤q<⌈n/2⌉.
The SVD of the feature matrix X∈Rn×ddefined by (17) can be made explicit. In particular, the top left
singular vector is u1=¯y/√n, where ¯y∈Rndenotes the vector of noisy labels ¯yi:=κiyi. This singles
out a dominant mode y1:= (u⊤
1y)u1of the label vector ythat islearned first by the non-linear dynamics.
Explicitly,
y1= (1−2q
n)¯y (18)
For a small noise ratio q/n≪1, this yields y1i≈yifor clean examples and −yifor noisy ones: fitting the
dominant mode amounts to learning the clean examples – while assigning the wrong label to the noisy ones.
This is illustrated in fig 4 (middle plot).
Example 3 We consider a simple spurious correlation setup (Sagawa et al., 2020b), obtained by adding to
(17) a ‘core’ feature that separates all training points. Here we assume d>n + 1. Conditioned on a set of
binary labels yi=±1, the inputs are given by
xi=κiyie1+λyie2+ηei+2,1≤i≤n (19)
for some binary variable κi=±1, scaling factor λ∈(0,1), andη>0. Given 1≤q<⌈n/2⌉, we assume we
have a majority group of n−qtraining examples with κi= 1, whose label is spuriously correlated with the
spurious feature e2, and a minority group of qexamples with κi=−1.
The analysis is similar as in Example 2. For small noise ratio and scaling factor λ, the non-linear dynamics
enhances the bias towards fitting first the majority group of (‘easy’) examples with spuriously correlated
labels. This illustrates an increased sensitivity of the non linear regime to the spurious feature – at least in
the first part of training. This is shown in fig 4 (right plot).
5 Conclusion
The recent emphasis on the lazy training regime, where deep networks behave as linear models amenable to
analytical treatment, begs the question of the specific mechanisms and implicit biases which differentiates it
from the full-fledged feature learning regime of the algorithms used in practice. In this paper, we investigated
the comparative effect of the two regimes on subgroups of examples based on their difficulty. We provided
experiments in various setups suggesting that easy examples are given more weight in non-linear training mode
(deep learning) than in linear training, resulting in a comparatively higher learning speed for these examples.
We illustrated this phenomenon across various ways to quantify examples difficulty, through c-scores, label
noise, and correlations to some easy-to-learn spurious features. We complemented these empirical observations
with a theoretical analysis of a quadratic model whose training dynamics is tractable in both regimes. We
believe that our findings makes a step towards a better understanding of the underlying mechanisms that
drive the good generalization properties observed in practice.
Broader Impact Statement
This work proposes to improve our understanding of the mechanisms behind deep learning models training.
There is no clear direct application of these theoretical observations to create harmful tools, however as for
any technology, machine learning models can be used for malicious intentions. We also acknowledge that
deep learning uses a significant amount of compute capacity when scaled to global tech companies, which in
some places is powered by carbon emitting power plants, thus contributes to global warming.
11Published in Transactions on Machine Learning Research (12/2022)
References
Atish Agarwala, Jeffrey Pennington, Yann Dauphin, and Sam Schoenholz. Temperature check: theory and
practice for training models with softmax-cross-entropy losses, 2020. URL https://arxiv.org/abs/2010.
07344.
Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via over-
parameterization. volume 97 of Proceedings of Machine Learning Research , pp. 242–252, Long Beach, Cali-
fornia, USA, 09–15 Jun 2019. PMLR. URL http://proceedings.mlr.press/v97/allen-zhu19a.html .
Sanjeev Arora, Simon S Du, Wei Hu, Zhiyuan Li, Russ R Salakhutdinov, and Ruosong Wang. On ex-
act computation with an infinitely wide neural net. In H. Wallach, H. Larochelle, A. Beygelzimer,
F. d'Alché-Buc, E. Fox, and R. Garnett (eds.), Advances in Neural Information Processing Systems ,
volume 32. Curran Associates, Inc., 2019. URL https://proceedings.neurips.cc/paper/2019/file/
dbc4d84bfcfe2284ba11beffb853a8c4-Paper.pdf .
Devansh Arpit, Stanislaw Jastrzebski, Nicolas Ballas, David Krueger, Emmanuel Bengio, Maxinder S. Kanwal,
Tegan Maharaj, Asja Fischer, Aaron Courville, Yoshua Bengio, and Simon Lacoste-Julien. A closer look
at memorization in deep networks. In Doina Precup and Yee Whye Teh (eds.), Proceedings of the 34th
International Conference on Machine Learning , volume 70 of Proceedings of Machine Learning Research ,
pp. 233–242. PMLR, 06–11 Aug 2017. URL https://proceedings.mlr.press/v70/arpit17a.html .
Robert John Nicholas Baldock, Hartmut Maennel, and Behnam Neyshabur. Deep learning through the lens of
example difficulty. In A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan (eds.), Advances in
Neural Information Processing Systems , 2021. URL https://openreview.net/forum?id=WWRBHhH158K .
Aristide Baratin, Thomas George, César Laurent, R Devon Hjelm, Guillaume Lajoie, Pascal Vincent, and
Simon Lacoste-Julien. Implicit regularization via neural feature alignment. In Arindam Banerjee and Kenji
Fukumizu (eds.), Proceedings of The 24th International Conference on Artificial Intelligence and Statistics ,
volume 130 of Proceedings of Machine Learning Research , pp. 2269–2277. PMLR, 13–15 Apr 2021. URL
https://proceedings.mlr.press/v130/baratin21a.html .
Peter L Bartlett, Andrea Montanari, and Alexander Rakhlin. Deep learning: a statistical viewpoint. Acta
numerica , 30:87–201, 2021.
Ronen Basri, David Jacobs, Yoni Kasten, and Shira Kritchman. The convergence rate of neu-
ral networks for learned functions of different frequencies. In Advances in Neural Infor-
mation Processing Systems 32 , pp. 4761–4771. 2019. URL http://papers.nips.cc/paper/
8723-the-convergence-rate-of-neural-networks-for-learned-functions-of-different-frequencies.
pdf.
Mikhail Belkin, Daniel Hsu, Siyuan Ma, and Soumik Mandal. Reconciling modern machine-learning practice
and the classical bias–variance trade-off. Proceedings of the National Academy of Sciences , 116(32):
15849–15854, 2019.
Yoshua Bengio, Jérôme Louradour, Ronan Collobert, and Jason Weston. Curriculum learning. In Proceedings
of the 26th annual international conference on machine learning , pp. 41–48, 2009.
Alberto Bietti and Julien Mairal. On the inductive bias of neural tangent kernels. In Advances in Neu-
ral Information Processing Systems 32 , pp. 12893–12904. 2019. URL http://papers.nips.cc/paper/
9449-on-the-inductive-bias-of-neural-tangent-kernels.pdf .
Lénaïc Chizat and Francis Bach. On the global convergence of gradient descent for over-parameterized
models using optimal transport. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-
Bianchi, and R. Garnett (eds.), Advances in Neural Information Processing Systems , volume 31, pp.
3036–3046. Curran Associates, Inc., 2018. URL https://proceedings.neurips.cc/paper/2018/file/
a1afc58c6ca9540d057299ec3016d726-Paper.pdf .
12Published in Transactions on Machine Learning Research (12/2022)
Lenaic Chizat, Edouard Oyallon, and Francis Bach. On lazy training in differentiable programming. Advances
in Neural Information Processing Systems , 32, 2019.
Nello Cristianini, John Shawe-Taylor, André Elisseeff, and Jaz Kandola. On kernel-target alignment.
In T. Dietterich, S. Becker, and Z. Ghahramani (eds.), Advances in Neural Information Processing
Systems, volume 14. MIT Press, 2001. URL https://proceedings.neurips.cc/paper/2001/file/
1f71e393b3809197ed66df836fe833e5-Paper.pdf .
Simon S. Du, Jason D. Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai. Gradient descent finds global
minima of deep neural networks. In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.), Proceedings
of the 36th International Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach,
California, USA , volume 97 of Proceedings of Machine Learning Research , pp. 1675–1685. PMLR, 2019a.
URL http://proceedings.mlr.press/v97/du19c.html .
Simon S. Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh. Gradient descent provably optimizes over-
parameterized neural networks. In International Conference on Learning Representations , 2019b. URL
https://openreview.net/forum?id=S1eK3i09YQ .
Jeffrey L. Elman. Learning and development in neural networks: The importance of starting small. Cognition ,
48:71–99, 1993.
Vitaly Feldman and Chiyuan Zhang. What neural networks memorize and why: Discovering the long tail via
influence estimation. Advances in Neural Information Processing Systems , 33:2881–2891, 2020.
Stanislav Fort, Gintare Karolina Dziugaite, Mansheej Paul, Sepideh Kharaghani, Daniel M Roy, and
Surya Ganguli. Deep learning versus kernel learning: an empirical study of loss landscape geome-
try and the time evolution of the neural tangent kernel. In H. Larochelle, M. Ranzato, R. Hadsell,
M.F. Balcan, and H. Lin (eds.), Advances in Neural Information Processing Systems , volume 33, pp.
5850–5861. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/file/
405075699f065e43581f27d67bb68478-Paper.pdf .
Mario Geiger, Stefano Spigler, Arthur Jacot, and Matthieu Wyart. Disentangling feature and lazy training in
deep neural networks. Journal of Statistical Mechanics: Theory and Experiment , 2020(11):113301, nov
2020. doi: 10.1088/1742-5468/abc4de. URL https://doi.org/10.1088%2F1742-5468%2Fabc4de .
Thomas George. NNGeometry: Easy and Fast Fisher Information Matrices and Neural Tangent Kernels in
PyTorch, February 2021. URL https://doi.org/10.5281/zenodo.4532597 .
Gauthier Gidel, Francis Bach, and Simon Lacoste-Julien. Implicit regularization of discrete gradi-
ent dynamics in linear neural networks. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d 'Alché-
Buc, E. Fox, and R. Garnett (eds.), Advances in Neural Information Processing Systems , vol-
ume 32. Curran Associates, Inc., 2019. URL https://proceedings.neurips.cc/paper/2019/file/
f39ae9ff3a81f499230c4126e01f421b-Paper.pdf .
Trevor Hastie, Andrea Montanari, Saharon Rosset, and Ryan J. Tibshirani. Surprises in high-dimensional
ridgeless least squares interpolation. The Annals of Statistics , 50(2):949 – 986, 2022. doi: 10.1214/
21-AOS2133. URL https://doi.org/10.1214/21-AOS2133 .
Sara Hooker, Aaron Courville, Gregory Clark, Yann Dauphin, and Andrea Frome. What do compressed deep
neural networks forget? arXiv preprint arXiv:1911.05248 [cs.LG] , 2020.
Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural tangent kernel: Convergence and generalization
in neural networks. In NIPS, pp. 8571–8580. 2018.
Ziheng Jiang, Chiyuan Zhang, Kunal Talwar, and Michael C. Mozer. Characterizing structural regularities
of labeled data in overparameterized models. In Marina Meila and Tong Zhang (eds.), Proceedings of
the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event ,
volume 139 of Proceedings of Machine Learning Research , pp. 5034–5044. PMLR, 2021. URL http:
//proceedings.mlr.press/v139/jiang21k.html .
13Published in Transactions on Machine Learning Research (12/2022)
Dimitris Kalimeris, Gal Kaplun, Preetum Nakkiran, Benjamin Edelman, Tristan Yang, Boaz Barak, and
Haofeng Zhang. Sgd on neural networks learns functions of increasing complexity. Advances in neural
information processing systems , 32, 2019.
Pang Wei Koh and Percy Liang. Understanding black-box predictions via influence functions. In International
conference on machine learning , pp. 1885–1894. PMLR, 2017.
Dmitry Kopitkov and Vadim Indelman. Neural spectrum alignment: Empirical study. In International
Conference on Artificial Neural Networks , pp. 168–179. Springer, 2020.
Yuanzhi Li, Colin Wei, and Tengyu Ma. Towards explaining the regularization effect of initial large learning
rate in training neural networks. Advances in Neural Information Processing Systems , 32, 2019.
Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild. In
Proceedings of International Conference on Computer Vision (ICCV) , December 2015.
Eran Malach, Pritish Kamath, Emmanuel Abbe, and Nathan Srebro. Quantifying the benefit of using
differentiable learning over tangent kernels. In Marina Meila and Tong Zhang (eds.), Proceedings of the 38th
International Conference on Machine Learning , volume 139 of Proceedings of Machine Learning Research ,
pp. 7379–7389. PMLR, 18–24 Jul 2021. URL https://proceedings.mlr.press/v139/malach21a.html .
Song Mei, Andrea Montanari, and Phan-Minh Nguyen. A mean field view of the landscape of two-layer
neural networks. Proceedings of the National Academy of Sciences , 115(33):E7665–E7671, 2018. ISSN
0027-8424. doi: 10.1073/pnas.1806579115. URL https://www.pnas.org/content/115/33/E7665 .
Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro. In search of the real inductive bias: On the role of
implicit regularization in deep learning. ICLR workshop track , 2015.
Guillermo Ortiz-Jiménez, Seyed-Mohsen Moosavi-Dezfooli, and Pascal Frossard. What can linearized neural
networks actually say about generalization? Advances in Neural Information Processing Systems , 34, 2021.
Jonas Paccolat, Leonardo Petrini, Mario Geiger, Kevin Tyloo, and Matthieu Wyart. Geometric compression
of invariant manifolds in neural networks. Journal of Statistical Mechanics: Theory and Experiment , 2021
(4):044001, 2021.
Garima Pruthi, Frederick Liu, Satyen Kale, and Mukund Sundararajan. Estimating training data influence
by tracing gradient descent. Advances in Neural Information Processing Systems , 33:19920–19930, 2020.
Nasim Rahaman, Aristide Baratin, Devansh Arpit, Felix Draxler, Min Lin, Fred Hamprecht, Yoshua Bengio,
and Aaron Courville. On the spectral bias of neural networks. In International Conference on Machine
Learning , pp. 5301–5310. PMLR, 2019.
Shiori Sagawa, Pang Wei Koh, Tatsunori B. Hashimoto, and Percy Liang. Distributionally robust neural
networks. In International Conference on Learning Representations , 2020a. URL https://openreview.
net/forum?id=ryxGuJrFvS .
Shiori Sagawa, Aditi Raghunathan, Pang Wei Koh, and Percy Liang. An investigation of why overparameteri-
zation exacerbates spurious correlations. In International Conference on Machine Learning , pp. 8346–8356.
PMLR, 2020b.
Pedro Savarese, Itay Evron, Daniel Soudry, and Nathan Srebro. How do infinite width bounded norm
networks look in function space? In Proceedings of the 32nd Annual Conference on Learning Theory
(COLT), volume PMLR 99, pp. 2667–2690, 2019.
Andrew M. Saxe, James L. Mcclelland, and Surya Ganguli. Exact solutions to the nonlinear dynamics of
learning in deep linear neural network. In In International Conference on Learning Representations , 2014.
Mariya Toneva, Alessandro Sordoni, Remi Tachet des Combes, Adam Trischler, Yoshua Bengio, and Geoffrey J.
Gordon. An empirical study of example forgetting during deep neural network learning. In International
Conference on Learning Representations , 2019. URL https://openreview.net/forum?id=BJlxm30cKm .
14Published in Transactions on Machine Learning Research (12/2022)
Catherine Wah, Steve Branson, Peter Welinder, Pietro Perona, and Serge Belongie. The caltech-ucsd
birds-200-2011 dataset. 2011.
Francis Williams, Matthew Trager, Daniele Panozzo, Claudio Silva, Denis Zorin, and Joan Bruna. Gra-
dient dynamics of shallow univariate relu networks. In H. Wallach, H. Larochelle, A. Beygelzimer,
F. d'Alché-Buc, E. Fox, and R. Garnett (eds.), Advances in Neural Information Processing Systems ,
volume 32. Curran Associates, Inc., 2019. URL https://proceedings.neurips.cc/paper/2019/file/
1f6419b1cbe79c71410cb320fc094775-Paper.pdf .
Blake Woodworth, Suriya Gunasekar, Jason D. Lee, Edward Moroshko, Pedro Savarese, Itay Golan, Daniel
Soudry, and Nathan Srebro. Kernel and rich regimes in overparametrized models. In Jacob Abernethy
and Shivani Agarwal (eds.), Proceedings of Thirty Third Conference on Learning Theory , volume 125
ofProceedings of Machine Learning Research , pp. 3635–3673. PMLR, 09–12 Jul 2020. URL https:
//proceedings.mlr.press/v125/woodworth20a.html .
Greg Yang and Edward J. Hu. Tensor programs IV: feature learning in infinite-width neural networks.
In Marina Meila and Tong Zhang (eds.), Proceedings of the 38th International Conference on Machine
Learning, ICML 2021, 18-24 July 2021, Virtual Event , volume 139 of Proceedings of Machine Learning
Research , pp. 11727–11737. PMLR, 2021. URL http://proceedings.mlr.press/v139/yang21c.html .
Greg Yang and Hadi Salman. A fine grained spectral perspective on neural networks. arxiv preprint
arXiv:1907.10599[cs.LG] , 2019.
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep
learning requires rethinking generalization. ICLR, 2017.
Xiao Zhang, Haoyi Xiong, and Dongrui Wu. Rethink the connections among generalization, memorization, and
the spectral bias of dnns. In Zhi-Hua Zhou (ed.), Proceedings of the Thirtieth International Joint Conference
on Artificial Intelligence, IJCAI-21 , pp. 3392–3398. International Joint Conferences on Artificial Intelligence
Organization, 8 2021. doi: 10.24963/ijcai.2021/467. URL https://doi.org/10.24963/ijcai.2021/467 .
Main Track.
15