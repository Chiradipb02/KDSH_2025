Under review as submission to TMLR
Stochastic Fractional Gradient Descent with Caputo L1
Scheme for Deep Neural Networks
Anonymous authors
Paper under double-blind review
Abstract
Stochastic gradient descent (SGD) has been used as a standard method to optimize deep
neuralnetworks(DNNs),whereitessentiallydealswithfirst-orderderivatives. Incorporating
fractional derivatives into learning algorithms is expected to improve model performance,
especially when the corresponding optimization problems involve objective functions with
memory effects or long-range dependencies. The Caputo derivative is a fractional derivative
that maintains consistency with integer-order calculus and produces more reliable solutions
than other fractional derivatives, especially for differential equations. In this paper, we
propose a novel Caputo-based SGD algorithm tailored for training DNNs. Our method
exploits the Caputo L1scheme to achieve highly effective training and accurate prediction
forlargedatabyusinggradientinformationfromitspasthistorytoguideparameterupdates
in a more informed direction. This allows it to avoid local minima and saddle points,
resulting in faster convergence to the target value. We conducted experiments on several
benchmark datasets to evaluate our method. The results show that our method can improve
the empirical performance over some traditional optimization methods in both accuracy and
convergence.
1 Introduction
The performance of deep neural networks (DNNs) depends heavily on the choice of optimization algorithms.
Stochastic gradient descent (SGD) (Bottou, 2010) has been used as a standard method for this purpose, and
is known for its computational efficiency especially when handling large data. Several improved variants of
SGD, such as Adam (Kingma & Ba, 2014), Yogi (Zaheer et al., 2018) and DiffGrad (Dubey et al., 2019)
have been proposed and been employed in various situations of learning DNNs. Note that these algorithms
essentially deal only with first-order derivatives.
On the other hand, fractional calculus, a branch of mathematical analysis that deals with non-integer order
derivatives and integrals, which dates back to the 19th century (Lacroix, 1819; Leibniz, 1849), has gained
recognition for its attractive properties such as long-term memory and nonlocality (Li & Zeng, 2015), and
has been widely applied in diverse fields, including image processing, signal processing, and the analysis of
neural networks (Kaslik & Sivasundaram, 2011; Rakkiyappan et al., 2016; Sun et al., 2018).
Some recent studies show that the integration of fractional calculus into optimization algorithms can lead to
promising performance improvements in the training of DNNs (Wang et al., 2017; Sheng et al., 2020; Kan
et al., 2021; Wang et al., 2022; Zhou et al., 2023; Shin et al., 2023; Altan et al., 2023; Wei et al., 2023).
Prior to these, Pu et al. pioneered the incorporation of fractional calculus into gradient descent methods by
directly replacing first-order derivatives in the methods with fractional-order derivatives (Pu et al., 2015).
Thereafter, in Wang et al. (2017); Bao et al. (2018), the authors developed a fractional gradient descent
(FGD) method for learning neural networks via back-propagation, where they use the Caputo derivative
to calculate fractional-order gradients of loss functions. In Sheng et al. (2020), a fractional-order gradient
approach for convolutional neural networks (CNNs) was proposed. Similarly in Taresh et al. (2022), the
authors proposed a FGD method for CNNs to improve efficiency using a fixed memory step and an adjustable
number of terms based on the Caputo definition. The authors of Yu et al. (2022) proposed a new fractional
1Under review as submission to TMLR
order momentum (FracM) optimizer for DNNs using the Grünwald-Letnikov (GL) definition of adaptive
optimization algorithms. They evaluated the FracM optimizer on CIFAR10/100 and IMDB datasets and
achieved improved performance over the baseline optimizers. And, deep learning optimizers based on the
GL derivative were proposed under the short-memory effect, named FCSGD_G_L and FCAdam_G_L,
respectively (Zhou et al., 2023), which are fusions of SGD and Adam respectively with the GL derivative,
allowing them to learn more effectively from past gradients.
In this paper, we propose a novel Caputo-based SGD algorithm tailored for training DNNs. Our method
exploits the Caputo L1scheme in the calculation of fractional gradients. The Caputo L1scheme is an
efficient way to calculate FGDs because it uses a linear combination of discrete differences to approximate
the Caputo fractional derivative. This approximation scheme does not lose information about the local
behavior of functions around the point where the derivative is calculated, resulting in a more accurate
approximation than the existing Caputo-based FGD method. These properties of our method are expected
to enhance the advantages of FGD methods over traditional gradient descent methods, such as robustness
to noise and improved ability to learn long-range dependencies in data. Finally, we performed experiments
on CIFAR-10/100 data using ResNet and VGG architectures to demonstrate the improved performance of
our Caputo-based SGD method over existing ones.
The remainder of this paper is organized as follows. First, in Section 2, we give a brief review of fractional
calculus and describe some related works. In Section 3, we provide the numerical approximation of the
Caputo derivative with the Caputo L1scheme. In Section 4, we propose a stochastic FGD algorithm using
the developed approximate Caputo derivative for learning DNNs. Finally, we show the numerical results of
experiments on benchmark datasets in Section 5, and then conclude the paper in Section 6.
2 Background and Related Works
In this section, we give a brief overview of fractional calculus, and then highlight some of the latest advances
in the field of FGD.
2.1 Fractional Calculus
Theq-th derivative, which we denote by Dqf, is clearly defined when qis a positive integer. Fractional
calculus is a branch of mathematical analysis that deals with non-integer order derivatives and integrals. It
is known that, while integer-order derivatives remain information about the rate of change of a function at a
single point, fractional derivatives provide the ability to describe the behavior of a function that reflects the
history of its trajectories (see, for example, Spanier & Oldham (1974)). This property is useful for describing
the behavior of a function with complex memory effects or long-range interactions.
Although the mathematical debate over the definition of the fractional derivative has not been settled, the
following three definitions are widely accepted by researchers: The Grünwald–Letnikov (GL), the Riemann-
Liouville (RL), and the Caputo derivatives (Kilbas et al., 2006). Despite the differences in these definitions,
they are all widely used in research and have their unique advantages and limitations.
Definition 1 (GL derivative) .The Grünwald–Letnikov (GL) fractional derivative with fractional-order
α>0of the given function f(w)is defined as
GLDα
a,wf(w) = lim
h→01
hαL/summationdisplay
k=0(−1)k/parenleftbiggα
k/parenrightbigg
f(w−kh),
where,L= [(w−a)
h], and/parenleftbiggp
q/parenrightbigg
=Γ(p+1)
Γ(q+1)Γ(p−k+1), p∈R, q∈Ndenotes the binomial coefficient. Γ(·)is
the Euler’s gamma function, Γ(τ) =/integraltext∞
0wτ−1e−wdw.
Definition 2 (RL derivative) .The Riemann-Liouville (RL) fractional derivative with fractional-order α>0
of the given function f(w)is defined as
RLDα
a,wf(w) =1
Γ(n−α)dn
dtn/integraldisplayw
a(w−τ)n−α−1f(τ)dτ,
2Under review as submission to TMLR
wherenis a positive integer satisfying n−1≤α<n.
Definition 3 (Caputo derivative) .The Caputo fractional derivative with fractional-order α>0of the given
functionf(w)is defined as
CDα
a,wf(w) =1
Γ(n−α)/integraldisplayw
a(w−τ)n−α−1f(n)(τ)dτ, (1)
wherenis a positive integer satisfying n−1<α≤n. The corresponding discreate expansion of Eq. (1) is
as follows (Sheng et al., 2020):
CDα
a,wf(w) =∞/summationdisplay
k=nf(k)(w)
Γ(k+ 1−α)(w−a)k−α.
Forα∈(0,1), we have
CDα
a,wf(w) =1
Γ(1−α)/integraldisplayw
a(w−τ)−αf(1)(τ)dτ. (2)
And the following gives the chain rule for the Caputo derivative:
Definition 4. (Wang et al., 2017) Suppose f(g(x))to be a composite function, then the α-th order Caputo
derivative with respect to xis
CDα
a,xf(g(x)) =∂f(g)
∂g·CDα
a,xg(x). (3)
2.2 Fractional Gradient Descent
As is mentioned above, fractional derivatives provide the ability to describe the behavior of a function that
reflects the history of its trajectories. Thus, their incorporation into gradient descent methods is expected to
guide parameter updates in a more informed direction. Basically, such an FGD method can be achieved by
replacing a conventional gradient in the update law with a fractional one, i.e., wk+1=wk−ϑDα
a,wkf(wk),
whereϑis the learning rate, Dα
a,wkis the fractional derivative operator and α > 0is the fractional-order
of the derivative. Dα
a,wkis determined based on the definitions of fractional derivatives such as the GL, RL
and Caputo derivatives (see Definitions 1, 2, and 3) and their numerical schemes. Recently, there has been
an increasing interest in the analysis of FGD approaches (Pu et al., 2015; Bao et al., 2018; Wei et al., 2019;
Lou et al., 2022; Taresh et al., 2022; Zhou et al., 2023).
The authors in Pu et al. (2015) have proposed the FGD optimization approach for NNs. The update law
was designed as follows:
wk+1=wk−ϑCDα
a,wkf(wk), (4)
whereCDα
a,wkis obtained based on the following discrete form of the Caputo derivative:
CDα
a,wf(w) =∞/summationdisplay
k=n/parenleftbiggα−n
k−n/parenrightbiggf(k)(w)
Γ(k+ 1−α)(w−a)k−α. (5)
Bao et al. have also proposed the FGD with the Caputo derivative for DNNs (Bao et al., 2018), where the
property of the fractional differentiation of power function CDα
a,wk(w−a)s=Γ(s+1)
Γ(s−α+1)(w−a)s−α, s>−1
was utilized to approximate Caputo derivatives.
The value of the lower terminal amay affect the convergent results, which were different from the real
extreme points when using the approach (4). Therefore, the authors in Wei et al. (2020) rewrote the update
law (4) by truncating higher order terms of Eq. (5) for 0<α< 1as
CDα
a,wkf(wk) =f(1)(wk)
Γ(2−α)|(wk−a) +ε|1−α,
3Under review as submission to TMLR
whereεrepresents the small non-negative number to avoid the non-convergence when wk=a. Meanwhile,
in Taresh et al. (2022), the authors proposed to iterate the value of the lower terminal of (4) to obtain the
real extreme point of wwith the modified update law:
wk+1=wk−ϑM/summationdisplay
k=1f(1)(wk−1)
Γ(n+ 1−α)|(wk−wk−1) +ε|n−α,
wheren<M. And more recently, the fractional SGD and fractional Adam algorithms have been proposed
in Zhou et al. (2023) by replacing the fractional derivative in Eq. (4) with the one of Definition 1.
3 Numerical Approximation for Caputo Derivative
In this paper, we propose a novel FGD algorithm based on the Caputo derivative. The Caputo derivative is a
recognized technique for tackling physical phenomena, especially initial value problems because it adheres to
the fundamental principles of integer-order calculus, where the fractional derivatives of a constant function
equals zero (Kilbas et al., 2006). Unlike prior methods, we employ the Caputo L1numerical scheme to
compute the fractional gradient update rule.
3.1 Caputo Derivative with L1scheme
The Caputo derivative (2) can be discretized based on the L1approximation, which relies on a linear inter-
polation formula for functions within each subinterval. Let us consider the fractional differential equations
as follows:
CDα
0,wY(w) =f(Y(w)),0<α< 1. (6)
Assume that Y(w)is the solution of (6) on the interval w∈Ξ := [0,T], whereT > 0. Discretizing the
domain Ξuniformly with step size ∆w=wi+1−wias follows: ΞN:={wi: 0 =w0<···<w 1<···<wi<
wi+1<···<wn=T},wherewi=i∆w.
Fori= 0,...,n−1, the discretization of (6) is given by using the L1-scheme:
CDα
0,wY(w)|w=wn=1
Γ(1−α)/integraldisplaywn
w0(wn−τ)−αY(1)(τ)dτ
=1
Γ(1−α)n−1/summationdisplay
i=0/integraldisplaywi+1
wi(wn−τ)−αY(1)(τ)dτ
≈−1
Γ(1−α)n−1/summationdisplay
i=0Y(wi+1)−Y(wi)
∆w/bracketleftbigg(wn−wi+1)−α+1−(wn−wi)−α+1
−α+ 1/bracketrightbigg
=n−1/summationdisplay
i=0δn−i−1(Y(wi+1)−Y(wi)), (7)
whereδk=(∆w)−α
Γ(2−α)[(k+ 1)1−α−k1−α].This method necessitates the storage of all previous function values,
creating a bottleneck for long-term simulations.
To visually represent the improved convergence of the suggested FGD (4) under the Caputo L1scheme (7),
the quadratic function—a popular kind of objective function—is selected. Let us assume that, ϑ= 0.08
(learning rate), µ= 0.4(momentum), α= 0.8(fractional-order), and ∆w= 0.01(step-size), we evaluate
the performance of Caputo-based SGD with momentum against two other widely adopted approaches -
gradient descent and SGD with momentum when applied to a basic quadratic objective function f(x,y) =
10x2+y2. The starting point for the optimization process is at (x,y) = (1,−10), with the origin as the
function’s minimizer. The comparison in Figure 1 clearly demonstrates that our proposed Caputo-based
SGD exhibits a superior convergence rate in achieving the desired optimization objective, outperforming
both the conventional gradient descent and SGD with momentum methodologies.
4Under review as submission to TMLR
−2.0 −1.5 −1.0 −0.5 0.0 0.5 1.0 1.5 2.0
x−10.0−7.5−5.0−2.50.02.55.07.510.0yContour plot of Objective Function with T rajectorie 
Gradient De cent
SGD with Momentum
Caputo-ba ed SGD with Momentum
0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5
Iteration020406080100LossLoss Graph
Gradient Descent
SGD with Momentum
Caputo-based SGD with Momentum
Figure 1: Convergence of descent approaches (Gradient Descent, SGD with Momentum, and Caputo-based
SGD with Momentum) on functions f(x,y) = 10x2+y2starting at (x,y) = (1,−10). The trajectory of the
objective functions is shown in the contour graph (left) and loss graph (right).
4 DNNs with Caputo-based FGD optimization
In this section, we establish a simple structure of DNNs that features end-to-end connections for all layers.
Followingthat, weproposetheCaputo-basedSGDwiththehelpoftheCaputo L1scheme. Next, weestablish
theoretical results by proving the convergence of the proposed method and its associated truncation error.
Then optimizing the addressed DNNs through the back-propagation algorithm which updates the parameters
of networks based on Caputo-based SGD.
4.1 Structure of DNNs
The simple structure of DNNs and how they work are discussed briefly as follows:
•Input layer: H0=xt
•Hidden layer: Hl=ςl(Al),Al=wlHl−1+bl, l= 1,2,···,L.
•Output layer: H(xt,ρ) =wL+1HL+bL+1.
Here,Ldenotes the number of hidden layers in NN. nl,l= 0,1,2,···,L+1,is the number of neurons in the
layerl, the weights and bias of the hidden layer lare depicted as wl∈Rnl,nl−1andbl∈Rnl.ςl(·)represents
the activation functions of the l-th hidden layer, and Hldenotes the output of the hidden layer l.ρis the
unknown parameter vector to be calculated.
The process of training NNs consists of two fundamental steps, including forward propagation and backward
propagation. In this paper, we aim to explore the use of FGD as a replacement for the traditional method
of training NNs via backward propagation. We believe that this alternative approach could offer significant
benefits, and we look forward to presenting our findings.
4.2 Caputo-based SGD approach
We aim to improve the accuracy, and speed of convergence, and minimize the total loss/error values of the
DNNs. To this end, the weights and biases are updated by using the Caputo-based FGD. Then the updating
formula at the iteration k∈Nis defined as
(wl)k+1= (wl)k−ϑCDα
(wl)kE,
(bl)k+1= (bl)k−ϑCDα
(bl)kE,(8)
whereϑdenotes the learning rate, and Eis the loss/objective function of DNNs.
5Under review as submission to TMLR
Proposition 1. For any DNN with a loss/objective function E, the fractional-order Caputo derivative of E
with respect to the DNN parameters (weights wland biasesbl) can be expressed as follows:
CDα
(wl)E=∂E
∂Hlς′
l(Al)Hl−1n−1/summationdisplay
s=0δn−s−1/parenleftbig
ws+1
l−ws
l/parenrightbig
,
CDα
(bl)E=∂E
∂Hlς′
l(Al)n−1/summationdisplay
s=0δn−s−1/parenleftbig
bs+1
l−bs
l/parenrightbig
,(9)
whereδr=(∆w)−α
Γ(2−α)/bracketleftbig
(r+ 1)1−α−r1−α/bracketrightbig
,and∆wis the step size used in the Caputo L1approximation. To
simplify the notation, we can replace wl(s+1),wl(s),bl(s+1), andbl(s)withws+1
l,ws
l,bs+1
l, andbs
l, respectively.
Proof.According to (3), one can obtain the Caputo fractional derivative of Ewith respect to the DNNs
parameters (weights and bias) as follows:
CDα
(wl)E=∂E
∂AlCDα
(wl)Al,
CDα
(bl)E=∂E
∂AlCDα
(bl)Al,(10)
From the structure of DNNs, the integer-order gradient term∂E
∂Alis expressed as
∂E
∂Al=∂E
∂Hl∂Hl
∂Al=∂E
∂Hlς′
l(Al),
∂E
∂Hl−1=∂E
∂Hl∂Hl
∂Al∂Al
∂Hl−1=∂E
∂Alwl.(11)
By substituting (11) into (10), we have
CDα
(wl)E=∂E
∂Hlς′
l(Al)CDα
(wl)Al,
CDα
(bl)E=∂E
∂Hlς′
l(Al)CDα
(bl)Al.(12)
Based on the Caputo L1scheme (7),
CDα
(wl)Al=Hl−1n−1/summationdisplay
s=0δn−s−1/parenleftbig
ws+1
l−ws
l/parenrightbig
,
CDα
(bl)Al=n−1/summationdisplay
s=0δn−s−1/parenleftbig
bs+1
l−bs
l/parenrightbig
,(13)
whereδr=(∆w)−α
Γ(2−α)/bracketleftbig
(r+ 1)1−α−r1−α/bracketrightbig
.From (12) and (13), the fractional-order updating gradient descent
(10) can be rewritten as
CDα
(wl)E=∂E
∂Hlς′
l(Al)Hl−1n−1/summationdisplay
s=0δn−s−1/parenleftbig
ws+1
l−ws
l/parenrightbig
,
CDα
(bl)E=∂E
∂Hlς′
l(Al)n−1/summationdisplay
s=0δn−s−1/parenleftbig
bs+1
l−bs
l/parenrightbig
.(14)
The following theorems illustrate the convergence and the truncation error analysis of objective function E
with Caputo L1scheme.
6Under review as submission to TMLR
Theorem 1. Let(ζl)kbe the layers of DNNs updated by Caputo-based SGD (8) converge to the real
extreme point ζ∗
lif and only if there exists ϵ >0and a learning rate ϑsuch that for all k, the following
inequality holds:
∥ek+1∥≤(1−ϑϵ)k∥e0∥,
whereek= (ζl)k−ζ∗
l,ϵ=κ(∆ζ)−α
Γ(2−α)/summationtextn−1
s=0/bracketleftbig
(n−s)1−α−(n−s−1)1−α/bracketrightbig
.
Proof.Letek= (ζl)k−ζ∗
l. Then the update law (8) can be expressed as:
ek+1=ek−ϑCDα
0,(ζl)kE((ζl)k)n)
According to (9), one can get
ek+1=ek−ϑ∂E
∂(ζl)k−1n−1/summationdisplay
s=0δn−s−1/parenleftbig
(ζl)s+1
k−(ζl)s
k/parenrightbig
, (15)
where∂E
∂(ζl)k−1=∂E
∂Hlς′
l(Al)Hl−1. It follows that, there exists a sufficiently large number N∈N, such that
for anyk−1> N, thenϱ= inf
k−1>N/vextendsingle/vextendsingle/vextendsingle/vextendsingle∂E
∂(ζl)k−1/vextendsingle/vextendsingle/vextendsingle/vextendsingle>0is guaranteed. From (15), the following results can be
obtained:
∥ek+1∥≤∥ek∥+ϑ/vextendsingle/vextendsingle/vextendsingle/vextendsingle∂E
∂(ζl)k−1/vextendsingle/vextendsingle/vextendsingle/vextendsinglen−1/summationdisplay
s=0δn−s−1∥/parenleftbig
(ζl)s+1
k−(ζl)s
k/parenrightbig
∥
≤∥ek∥+ϑϱ/radicaltp/radicalvertex/radicalvertex/radicalbtn−1/summationdisplay
s=0δ2
n−s−1n−1/summationdisplay
s=0/parenleftbig
∥(ζl)s+1
k−(ζl)s
k∥/parenrightbig2
≤∥ek∥+ϑϱ/radicaltp/radicalvertex/radicalvertex/radicalbtn−1/summationdisplay
s=0δ2
n−s−1n−1/summationdisplay
s=0∥ek∥2
≤∥ek∥+ϑϱ/radicaltp/radicalvertex/radicalvertex/radicalbtn−1/summationdisplay
s=0δ2
n−s−1n−1/summationdisplay
s=0∥ek∥2
≤∥ek∥+ϑϱ/radicaltp/radicalvertex/radicalvertex/radicalbt/parenleftiggn−1/summationdisplay
s=0δ2
n−s−1/parenrightigg
∥ek∥
≤
1 +ϑϱ/radicaltp/radicalvertex/radicalvertex/radicalbtn−1/summationdisplay
s=0δ2
n−s−1
∥ek∥
≤(1 +ϑϵ)∥ek∥,
whereϵ=ϱ/radicalig/summationtextn−1
s=0δ2
n−s−1, andδr=(∆ζ)−α
Γ(2−α)/bracketleftbig
(r+ 1)1−α−(r)1−α/bracketrightbig
.Now, for all k,
∥ek+1∥≤(1 +ϑϵ)k∥e0∥.
Next, analyze the convergence rate by rewriting the above inequality with the help of a natural logarithm:
ln(∥ek+1∥)≤kln(1 +ϑϵ) + ln(∥e0∥)
Based on Taylor’s series expansion, ln(1 +ϑϵ)≈ϑϵThis is a valid approximation for small ϑϵ. So, we have:
ln(∥ek+1∥)≤kϑϵ+ ln(∥e0∥)
7Under review as submission to TMLR
Taking the exponential on both sides, one can get
∥ek+1∥≤ekϑϵ∥e0∥
Thus, (ζl)kconverges to ζ∗
lexponentially with a rate of ekϑϵ.If0<ϑϵ< 1, the error will decrease with each
iteration, and the proposed method is convergent. The value of ϵdepends on the Caputo L1scheme with the
convergence rate (2−α)orO(∆ζ2−α). Achieving an optimal convergence rate requires careful adjustment
of both the learning rate ( ϑ) and the parameter ϵ. Theϑϵis closer to zero, the faster convergence. However,
choosing a very small ϑmay slow down the convergence due to small step sizes. This completes the proof of
the convergence of the proposed Caputo-based FGD with the Caputo L1scheme.
Theorem 2. For any 0< α < 1, theL1approximation is defined as (7). Then, there exists a positive
constantκ,κ= min
0≤i≤n−1|∂E/∂ζi
l|. The truncation error Ri=CDα
0,ζlE(ζl)|ζl=ζi
l−CDα
0,ζlE(ζl)|ζl=ζi
l, i=
0,...,n−1, satisfies:
|Ri|≤C (∆ζ)2−αmax
t∈[ζi
l,ζi+1
l]|A′′(tl)|, (16)
whereC=ακ
2Γ(3−α).
Proof.Fori= 1,2,···,n−1,ϖ1,iA(ζl) =A(ζi−1
l)ζi
l−ζl
∆ζ+A(ζi
l)ζl−ζi−1
l
∆ζ. Then, following theory of piecewise
linear interpolation, A(ζl)−ϖ1,iA(ζl) =A′′(si
l)
2(ζi
l−ζl)(ζl−ζi−1
l), sl∈(ζi−1
l,ζi
l), one can obtain
Ri=CDα
0,ζlE(ζl)|ζl=ζi
l−CDα
0,ζlE(ζl)|ζl=ζi
l
=1
Γ(1−α)∂E
∂(ζl)/integraldisplayζi
l
ζ0
l(ζi
l−τ)−α(ϖ1,iA(τ)−A(τ))′dτ
=1
Γ(1−α)∂E
∂(ζl)/bracketleftigg
(ϖ1,iA(τ)−A(τ))(ζi
l−τ)−α|ζi
l
ζ0
l−α/integraldisplayζi
l
ζ0
l(ζi
l−τ)−α−1(ϖ1,iA(τ)−A(τ))dτ/bracketrightigg
=1
Γ(1−α)∂E
∂(ζl)/bracketleftigg
−A′′(si
l)
2(τ−ζl)(ζi
l−τ)1−α|ζi
l
ζ0
l+α/integraldisplayζi
l
ζ0
l(ζi
l−τ)−αA′′(si
l)
2(τ−ζ0
l)dτ/bracketrightigg
=/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleακ
2Γ(1−α)A′′(tl)/integraldisplayζi
l
ζ0
l(ζi
l−τ)−α(τ−ζ0
l)dτ/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
|Ri|≤C (∆ζ)2−αmax
tl∈[ζi
l,ζi+1
l]|A′′(tl)|.
This completes the proof of the truncation error (16).
5 Numerical Experiments
The goal of this section is to show the performance of the Caputo-based SGD optimization method as
described in Algorithm 1, using the Caputo L1numerical approximation. For this purpose, we report the
results of our experiments on image recognition accuracy with the CIFAR-10 and CIFAR-100 datasets, which
are collections of images from 10 and 100 classes respectively. We use two deep learning architectures, ResNet
andVGG,whicharewell-knownfortheirhighaccuracyonimageclassificationtasks. Theseexperimentswere
conducted using GPUs, specifically the Tesla V100-SXM2-32GB. The deep learning models were developed
in Python 3.10.12, utilizing PyTorch 2.0.1, on the Windows 11 Home 64-bit operating system.
8Under review as submission to TMLR
Algorithm 1 The Caputo-based SGD algorithm
Input:ϑ(initial learning rate), ζ0(params),E(ζ)(objective), 0< α < 1(fractional-order), max-
imize=False ,nesterov=False ,∆ζ(step-size), λ= 0(weight decay), µ= 0(momentum), τ= 0
(dampening)
fork= 1to···do
∇k←CDα
(ζk)E(ζk−1) (FGD based on (9))
ifλ̸= 0then
∇k←∇k+λζk−1
end if
ifµ̸= 0then
ifk≥1then
Mk←µbk−1+ (1−τ)∇k
else
Mk←∇k
end if
ifnesterovthen
∇k←gk+µMk
else
∇k←Mk
end if
end if
ϑk←ϕ(k)·ϑ
ifmaximize then
ζk←ζk−1+ϑk∇k
else
ζk←ζk−1−ϑk∇k
end if
end for
returnζk
5.1 Datasets and Models
The CIFAR-10 and CIFAR-100 datasets (Krizhevsky et al., 2009) are widely utilized in computer vision and
image classification endeavors. These datasets consist of 32×32color images that are evenly distributed,
making them exceptionally well-suited for training and assessing image classification models. CIFAR-10
comprises 10 mutually exclusive categories, whereas CIFAR-100 comprises 100 categories that are sorted
into 20 superclasses.
CNNs are the most popular deep learning model for image classification. They use a series of convolutional
layers to extract features from images. Based on their architecture, CNNs can be classified into different
types, including residual networks (ResNets) (He et al., 2016) and visual geometry groups (VGG) (Simonyan
& Zisserman, 2014). ResNets and VGG are deep, efficient, and adaptable image classification models. In this
paper, we train and validate our proposed optimization strategy on the CIFAR-10 and CIFAR-100 datasets
using ResNet and VGG models to demonstrate its accuracy.
5.2 Experiments
More precisely, experiments involve training and validating the CIFAR-10 dataset using ResNet-110 and
ResNet-56 models, which have 110and56layers, respectively. Similarly, the CIFAR-100 dataset is trained
and validated using VGG19_bn and VGG16_bn models, which have 19 and 16 layers, respectively. Both
models indisputably dominate in image classification tasks, showcasing unparalleled and highly efficient
performance.
9Under review as submission to TMLR
Table 1: Test accuracy (%) of proposed Caputo-based SGD on CIFAR-10 datasets for different fractional-
ordersα
Test accuracy of CIFAR - 10
Model0<α< 1
0.90.80.70.6
Batch size: 128Resnet-110 94.35 94.2793.8193.75
Resnet56 94.09 93.6592.9493.05
Batch size: 64Resnet-110 93.76 93.5793.5093.63
Resnet56 93.98 93.6793.5793.36
Table 2: Comparing test accuracies (%) of traditional optimization techniques (SGD, FCSGD_G_L, FracM)
and the proposed Caputo-based SGD on CIFAR-10 datasets
Existing methods Proposed method
BatchModel SGDMFCSGD_G_L FracMCaputo-based SGDsize (Zhou et al., 2023) (Yu et al., 2022)
128Resnet-110 93.88 93.15 94.15 94.35
Resnet56 93.83 92.67 93.57 94.09
64Resnet-110 93.13 93.69 93.67 93.76
Resnet56 93.34 93.66 93.35 93.98
The proposed Caputo-based SGD optimizer was employed with a fractional-order 0<α< 1, learning rate
of0.1, and weight decay of 1e−5. The cross-entropy loss function was utilized. To train our models, we
used batch sizes of 128and64, respectively, over a total of 200epochs. We also used the ϕ= 0.1as a
learning rate scheduler with a warm-up period at epochs 100and150. After each epoch, we evaluated the
models’ performance on the test set. The purpose of these experiments was to demonstrate the efficacy of
the proposed Caputo-based SGD method, employing the Caputo L1scheme.
5.3 Results and Discussion
The results presented in our research article demonstrate the potential benefits of the method described in
Algorithm 1 (please refer to Tables 1 through 4). We report our findings as a mean value based on five trials
to offer a more precise estimation of the variability in classification outcomes caused by random initialization.
Tables 1 and 3 provide the test accuracy for image classification tasks on the CIFAR-10 and CIFAR-100
datasets, respectively. These results are obtained by applying the proposed Caputo-based SGD for various
values of the fractional-order differentiation parameter, denoted as α= 0.9,0.8,0.7,0.6, where 0< α < 1.
By adjusting the fractional order α, the algorithm’s behavior can be influenced, allowing authors to fine-
tune the optimization approach to suit the specific characteristics of the problem at hand. From Table 1, we
observe that the highest accuracy on the CIFAR-10 dataset is achieved when training ResNet-110/ResNet-56
models with batch sizes of 128 and 64, respectively, using α= 0.9. In Table 3, the highest accuracy on the
CIFAR-100 dataset is attained when training VGG19_bn and VGG16_bn models with a batch size of 64,
usingα= 0.9.
Table 2 and Table 4 compare the accuracy of image classification tasks on the CIFAR-10 and CIFAR-100
datasets, respectively. This comparison is made between the proposed Caputo-based SGD, conventional
SGD optimizers, SGD with momentum (SGDM), FCSGD_G_L (Zhou et al., 2023), and FracM (Yu et al.,
2022). The results clearly indicate that the proposed Caputo-based SGD optimizer outperforms conventional
optimizers and existing FGDs, achieving higher accuracy. In other words, Caputo-based SGD proves to be
a more effective optimizer for image classification tasks compared to conventional SGD optimizers. This
enhanced performance is likely attributable to the non-locality, degrees of freedom, and hereditary properties
10Under review as submission to TMLR
Table 3: Test accuracy (%) of proposed Caputo-based SGD on CIFAR-100 datasets for different fractional-
ordersα
Test accuracy of CIFAR - 100
Model0<α< 1
0.90.80.70.6
Batch size: 64VGG19_bn 71.15 70.7570.3269.82
VGG16_bn 73.42 73.1673.3072.96
Table 4: Comparing test accuracies (%) of traditional optimization techniques (SGD, FCSGD_G_L, FracM)
and the proposed Caputo-based SGD on CIFAR-100 datasets
Existing methods Proposed method
BatchModel SGDMFCSGD_G_L FracMCaputo-based SGDsize (Zhou et al., 2023) (Yu et al., 2022)
128VGG19_bn 73.00 71.57 72.16 73.18
VGG16_bn 74.01 70.96 72.03 74.34
of Caputo fractional derivatives, which enable them to capture complex patterns and dependencies within
the data.
6 Conclusion
In this study, we introduced the Caputo-based SGD algorithm for optimizing deep learning models. We
incorporated a fractional order within the interval of 0< α < 1, employing the Caputo L1numerical
approximation. These techniques were used to develop the Caputo-based SGD. The proposed algorithm was
then implemented in the back-propagation process to update the parameters of the input layers in DNNs.
Our method capitalizes on the memory and hereditary properties of fractional calculus to address the issue
of convergence to local minima and accelerate convergence towards the target value compared to traditional
gradient descent methods. To evaluate the effectiveness of our approach, we conducted experiments on
the CIFAR-10 and CIFAR-100 datasets using ResNet and VGG models. The results demonstrated that our
methodconsistentlyachievedhigheraccuracycomparedtoexistingapproaches, suchasSGD,FCSGD_G_L,
and FracM. Additionally, our research revealed that fine-tuning the fractional order αcan further enhance
the accuracy of deep learning models. The Caputo-based SGD algorithm is particularly valuable in scenarios
where long-term dependencies, noise, or anomalous behaviors significantly impact the optimization process.
References
Gokhan Altan, Sertan Alkan, and Dumitru Baleanu. A novel fractional operator application for neural
networks using proportional caputo derivative. Neural Computing and Applications , 35(4):3101–3114,
2023.
ChunhuiBao, YifeiPu, andYiZhang. Fractional-orderdeepbackpropagationneuralnetwork. Computational
intelligence and neuroscience , 2018, 2018.
Léon Bottou. Large-scale machine learning with stochastic gradient descent. In Proceedings of COMP-
STAT’2010: 19th International Conference on Computational StatisticsParis France, August 22-27, 2010
Keynote, Invited and Contributed Papers , pp. 177–186. Springer, 2010.
Shiv Ram Dubey, Soumendu Chakraborty, Swalpa Kumar Roy, Snehasis Mukherjee, Satish Kumar Singh,
and Bidyut Baran Chaudhuri. diffgrad: an optimization method for convolutional neural networks. IEEE
transactions on neural networks and learning systems , 31(11):4500–4511, 2019.
11Under review as submission to TMLR
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In
Proceedings of the IEEE conference on computer vision and pattern recognition , pp. 770–778, 2016.
Tao Kan, Zhe Gao, Chuang Yang, and Jing Jian. Convolutional neural networks based on fractional-order
momentum for parameter training. Neurocomputing , 449:85–99, 2021.
Eva Kaslik and Seenith Sivasundaram. Dynamics of fractional-order neural networks. In The 2011 Interna-
tional Joint Conference on Neural Networks , pp. 611–618. IEEE, 2011.
Anatolii Aleksandrovich Kilbas, Hari M Srivastava, and Juan J Trujillo. Theory and applications of fractional
differential equations , volume 204. elsevier, 2006.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980 , 2014.
Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton. Cifar-10 and cifar-100 datasets. URl: https://www. cs.
toronto. edu/kriz/cifar. html , 6(1):1, 2009.
S.F. Lacroix. Traité du calcul différentiel et du calcul intégral , volume 3. Mme. Ve Courcier, Paris, 2 edition,
1819.
Gottfried Wilhelm Leibniz. Letter from hanover, germany to gfa l’hospital, september 30, 1695. Mathema-
tische Schriften , 2:301–302, 1849.
Changpin Li and Fanhai Zeng. Numerical methods for fractional calculus , volume 24. CRC Press, 2015.
Weipu Lou, Wei Gao, Xianwei Han, and Yimin Zhang. Variable order fractional gradient descent method and
its application in neural networks optimization. In 2022 34th Chinese Control and Decision Conference
(CCDC), pp. 109–114. IEEE, 2022.
Yi-Fei Pu, Ji-Liu Zhou, Yi Zhang, Ni Zhang, Guo Huang, and Patrick Siarry. Fractional extreme value
adaptive training method: fractional steepest descent approach. IEEE transactions on neural networks
and learning systems , 26(4):653–662, 2015.
R Rakkiyappan, R Sivaranjani, G Velmurugan, and Jinde Cao. Analysis of global o (t- α) stability and
global asymptotical periodicity for a class of fractional-order complex-valued neural networks with time
varying delays. Neural Networks , 77:51–69, 2016.
Dian Sheng, Yiheng Wei, Yuquan Chen, and Yong Wang. Convolutional neural networks with fractional
order gradient method. Neurocomputing , 408:42–50, 2020.
Yeonjong Shin, Jérôme Darbon, and George Em Karniadakis. Accelerating gradient descent and adam via
fractional gradients. Neural Networks , 2023.
Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition.
CoRR, abs/1409.1556, 2014. URL http://arxiv.org/abs/1409.1556 .
Jerome Spanier and Keith B. Oldham. The Fractional Calculus . Academic Press, New York, 1974.
HongGuang Sun, Yong Zhang, Dumitru Baleanu, Wen Chen, and YangQuan Chen. A new collection of real
world applications of fractional calculus in science and engineering. Communications in Nonlinear Science
and Numerical Simulation , 64:213–231, 2018.
Mundher Mohammed Taresh, Ningbo Zhu, Talal Ahmed Ali Ali, Mohammed Alghaili, and Weihua Guo.
Usinganovelfractional-ordergradientmethodforcnnback-propagation. arXiv preprint arXiv:2205.00581 ,
2022.
Jian Wang, Yanqing Wen, Yida Gou, Zhenyun Ye, and Hua Chen. Fractional-order gradient descent learning
of bp neural networks with caputo derivative. Neural networks , 89:19–30, 2017.
12Under review as submission to TMLR
Yong Wang, Yuli He, and Zhiguang Zhu. Study on fast speed fractional order gradient descent method and
its application in neural networks. Neurocomputing , 489:366–376, 2022.
Jia-LiWei, Guo-ChengWu, Bao-QingLiu, andJuanJNieto. Anoptimalneuralnetworkdesignforfractional
deep learning of logistic growth. Neural Computing and Applications , 35(15):10837–10846, 2023.
Yiheng Wei, YangQuan Chen, Qing Gao, and Yong Wang. Infinite series representation of functions in
fractional calculus. In 2019 Chinese Automation Congress (CAC) , pp. 1697–1702. IEEE, 2019.
Yiheng Wei, Yu Kang, Weidi Yin, and Yong Wang. Generalization of the gradient method with fractional
order gradient direction. Journal of the Franklin Institute , 357(4):2514–2532, 2020.
ZhongLiang Yu, Guanghui Sun, and Jianfeng Lv. A fractional-order momentum optimization approach of
deep neural networks. Neural Computing and Applications , 34(9):7091–7111, 2022.
Manzil Zaheer, Sashank Reddi, Devendra Sachan, Satyen Kale, and Sanjiv Kumar. Adaptive methods for
nonconvex optimization. Advances in neural information processing systems , 31, 2018.
Xiaojun Zhou, Chunna Zhao, and Yaqun Huang. A deep learning optimizer based on grünwald–letnikov
fractional order definition. Mathematics , 11(2):316, 2023.
13