CogCoM: Train Large Vision-Language Models
Diving into Details through Chain of Manipulations
Anonymous Author(s)
Afﬁliation
Address
email
Can I turn left on this road?Answer with CoM.Find regions using GROUNDING(road signs) to get [[3,4,4,6],[12,2,15,5]] … Zoom into [3,4,4,6] cropped …
No, there is a sign showing one way.VLM with Chain of Manipulations
Detailed RecognitionCounting Objects
Math GeometryRead FiguresDe-hallucinationRead Time
Manipulation result (same color for referential relation)Manipulation creation
A
Q: How many wheels of the truck can be seen?
CogCoM: Using GROUNDING(all the individual wheels of the truck) to locate all the wheels in this picture, resulting in the boxes of [[56, 256, 96, 513], [105, 240, 144, 480], …]Based on the identified boxes of the wheels, the number of visible wheels of the truck is ten.
Q: What time does this photo show?
CogCoM: Applying GROUNDING(the clock) …,resulting in the region of [[814,531,940,1062]].Use CropZoomIn([[814,531,940,1062]],2) to crop … and then zoom in it bytwo times … re-input this new image.Using ReadTime(the clock in the newimage) to re-cognize the time on the clock, and the result is 1:45.
Q: In what year did the fatalities peak?
CogCoM: Firstly, using GROUNDING(fatalities values) to find the death toll, and the values at the position [[116, 114, 913, 755]] are 221, 213, 212, 214, 199, 194, 193, 193, 195, 190, 183, 173, 168, 160, 155, 151, 141, 121, 124, 118, and 85. The maximum number is 221.Draw a line using LINE[[([374,71), (374, 466])] to obtain newimage after drawing.The corresponding year is 2008.
Q: What is the brand of the plane?
CogCoM: Using GROUNDING(the plane) to find planes, and there are two regions [[255, 217, 312, 245], [193, 113, 481, 208]] that can be found.With the most notable one, using GROUNDING(the logo in [[193, 113, 481, 208]]) to find the logo, resulting in [[246,142,307,163]].Using CropZoomIn([[246, 142, 307,163]], 3) to get a new image.The brand name is AirFrance.
Q: What color is the umbrella the men are standing under?
CogCoM: Determine if there is an umbrella present.There are no men standing together; there is only one person visible.The person in the image is not standing but appears to be in a throwing stance.Since there is no umbrella present, its color cannot be determined.
Q: In circle O with radius of 5.0, the length of chord AB is 8.0, then the distance from the O to AB is?
CogCoM: Draw a straight line with LINE([[(54, 43), (13,70)]]) to obtain a new image after drawing.With the new image, set O perpe-ndicular to AB at point C. SinceC is the midpoint of AB, makingAC 4.0 long. In the right triangleAOC, OA=5.0, AC=4.0. Accordi-ng to the Pythagorean theorem,OC=3.0. So the answer is 3.0.BO
ABO
1
Figure 1: CogCoM solves various visual problems with Chain of Manipulations mechanism. Note
that the CoM reasoning generates evidential and explainable steps, without involving external tools.
Submitted to 38th Conference on Neural Information Processing Systems (NeurIPS 2024). Do not distribute.Abstract
Vision-Language Models (VLMs) have demonstrated their broad effectiveness 2
thanks to extensive training in aligning visual instructions to responses. However, 3
such training of conclusive alignment leads models to ignore essential visual rea- 4
soning, further resulting in failures in meticulous visual problems and unfaithful 5
responses. Drawing inspiration from human cognition in solving visual prob- 6
lems ( e.g.,marking ,zoom in ), this paper introduces Chain of Manipulations , 7
a mechanism that enables VLMs to solve problems step-by-step with evidence. 8
After training, models can solve various visual problems by eliciting intrinsic 9
manipulations ( e.g., grounding ,zoom in ) with results ( e.g., boxes ,image ) actively 10
without involving external tools, while also allowing users to trace error causes. We 11
study the roadmap to implement this mechanism, including (1) a ﬂexible design of 12
manipulations upon extensive analysis, (2) an efﬁcient automated data generation 13
pipeline, (3) a compatible VLM architecture capable of multi-turn multi-image, 14
and (4) a model training process for versatile capabilities. With the design, we also 15
manually annotate 6K high-quality samples for the challenging graphical mathemat- 16
ical problems. Our trained model, CogCoM , equipped with this mechanism with 17
17B parameters achieves state-of-the-art performance across 9 benchmarks from 18
4 categories, demonstrating the effectiveness while preserving the interpretability. 19
Our code, model weights, and collected data will be publicly available. 20
1 Introduction 21
Large Language Model
Image tokensWhat is written on the pillar in front of the man?No Smoking
Large Language ModelLarge Language Model
Image tokensWhat is written on the pillar in front of the man?First, use GROUNDING(man) to find regions [[4,5,7,12], [28,6,30,9]] …Then, use GROUNDING(pillar near [4,5,7,12]) to find [[8,0,17,13]] …Zoom into the cropped region [[9,7,11,9]] by two times and re-input …
Image tokensBased on the new image, the text written is QUICK DEPOSIT.W 
W 
Figure 2: In comparison with existing VLMs, CogCoM performs the multiple steps of evidential
reasoning with chain of manipulations (CoM) to achieve the faithful answer to visual scene.
Beneﬁting from the advantage of Large Language Models (LLMs) in broad world knowledge, large 22
Vision Language Models (VLMs) (Alayrac et al., 2022; Wang et al., 2023b) that are further trained 23
to understand visual inputs have demonstrated viabilities on broad multimodal scenarios, such as 24
visual question answering (Liu et al., 2023b), visual grounding (Peng et al., 2023), optical character 25
recognition (Zhang et al., 2023b). The research employing VLMs as foundation models (Bai et al., 26
2023; Sun et al., 2023b; Wang et al., 2023b) usually involves two main stages of training, where 27
the ﬁrst stage develops intrinsic visual understanding ability through exposure to massive image- 28
caption pairs, and the second stage endows the models with problem-solving capabilities through the 29
instruction tuning. 30
However, existing tuning methods train models to respond to instructions with conclusive language 31
responses upon visual inputs, which leads models to ignore the essential intermediate visual reasoning 32
and further results in failures in meticulous visual problems, unfaithful responses, and even hallucina- 33
tions. For example in the left subplot of Figure 2, we test the top-performing model CogVLM (Wang 34
et al., 2023b) about the details in the image ( i.e., texts written on a pillar ), and it directly responds 35
an incorrect answer ( i.e., NO SMOKING ), most likely from bias to visual or linguistic priors ( i.e., 36
typical scenes with a pillar in ofﬁce ). The absence of the essential reasoning on the visual scene may 37
lead to a rash response (Hwang et al., 2023). 38
2Humans solve problems regarding visual details by marking or processing the given images for 39
convenience and rigor, which we refer to as manipulations. For example, we ﬁnd targets by sequen- 40
tially locating references, and concentrate on subtle details by zooming into a corresponding region. 41
Most of VLMs have developed numerous intrinsic capabilities ( e.g., grounding boxes, recognizing 42
texts) during the ﬁrst stage of training. By further imitating the fundamental human behaviours ( e.g., 43
cropping, zoom in), models have the potential to perform this cognitive reasoning process. Three 44
major obstacles in eliciting VLMs with such reasoning are (1) ﬂexible deﬁnitions of manipulations 45
covering most visual problems, (2) an efﬁcient data collection pipeline capable of producing abundant 46
training data, and (3) a multi-turn multi-image VLM structure compatible with existing models. 47
Inspired by the human cognition in solving visual problems, we introduce Chain of Manipulations 48
(CoM) , a mechanism that enables VLMs to solve problems step-by-step with evidence, with each 49
step potentially involving a manipulation on the visual input and its corresponding result, both 50
generated by the model to facilitate the success and ﬁdelity. This paper studies a complete roadmap 51
with manipulations design, data collection, model architecture and training process for training 52
general VLMs with this mechanism. We ﬁrst formally design 6 basic manipulations upon the pilot 53
experiments, which are capable of handling diverse visual problems. Next, we propose a cascading 54
data generation pipeline based on reliable large language models ( e.g.,LLMs, the linguistic annotators) 55
and visual foundational models ( e.g., VFMs, the visual annotators), which can automatically produce 56
abundant error-free training data. We collect 70K CoM samples with this pipeline. We then devise 57
a multi-turn multi-image model architecture compatible with typical VLMs structures. Based on a 58
data recipe incorporating the curated corpus, we ﬁnally train a general VLM equipped with CoM 59
reasoning mechanism, named CogCoM, which possesses capabilities of chat, captioning, grounding 60
and reasoning. Additionally, beneﬁting from the expressive capability of the proposed mechanism, 61
we further manually annotated 6K high-quality samples of graphical mathematical problems, each 62
accompanied by a CoM reasoning process, to advance the research of VLMs in solving challenging 63
mathematical problems. 64
We conduct extensive experiments on 9 benchmarks from 4 categories, including TextVQA (Singh 65
et al., 2019), ST-VQA (Biten et al., 2019), TallyVQA (Acharya et al., 2019), and GQA Hudson & 66
Manning (2019) for detailed visual question answering, RefCOCO (Yu et al., 2016), RefCOCO+(Yu 67
et al., 2016), and RefCOCOg (Mao et al., 2016) for visual grounding, POPE (Li et al., 2023c) for 68
hallucination validation, and MM-Vet (Yu et al., 2023b) for general multimodal ability. Our model 69
achieves up to 9.0 and 1.09 accuracy improvement on the detailed VQA and grounding benchmarks, 70
respectively, and the superior performance on the general multimodal benchmark. The results 71
demonstrate the effectiveness of the mechanism while maintaining the interpretability of outputs. 72
2 Terminology 73
We ﬁrst conduct pilot experiments to investigate the possible manipulations capable of handling 74
diverse visual problems. 75
read, extract, ... locate, look for ...  zoom in, focus on ...01000020000300004000050000#FrequenciesOCR
GROUNDING
ZOOMIN
count, number, ... calculate, assess, ... draw, add, ...02004006008001000#FrequenciesCOUNTING
CALCULATE
LINE
Figure 3: Distribution of the generated 465 actions
base on GPT-4, mapped into 6 manipulations.Speciﬁcally, given a question about an image, 76
we prompt the advanced large language model, 77
GPT-4, to generate solving steps by optionally 78
utilizing possible actions on the image that fa- 79
cilitate problem-solving. We conduct this ex- 80
periment on 170K questions from TextVQA, a 81
dataset requiring detailed reasoning and recog- 82
nition on images. To ensure the stability, we 83
manually write 4 demonstrations as priors, The 84
detailed statistics are available at Appendix C.3. 85
We utilize the StanfordCoreNLP toolkit to ex- 86
tract verb phrases referring to the actions, and 87
the distribution of frequencies is shown in Fig- 88
ure 3. Through result analysis, we ﬁnd that most 89
of the actions can be mapped to 6 fundamen- 90
tal manipulations on images: OCR ,Grounding , 91
CropZoomIn ,Counting ,Calculate , and Line. 92
3Based on the observation, we formally predeﬁne a set of 6 manipulations, which can either be 93
developed from pre-training or be learned from ﬁne-tuning with the imitation to human behaviors: 94
M  {OCR(tgt)!txt,Grounding (tgt)!bbx,Counting (tgt)!num ,Calculate (tgt)! 95
num ,CropZoomIn (bbx; x )!img,Line(pts)!img}, where the parameters or results 96
tgt; txt; bbx; num; x; img; pts refer to the bounding boxes, zoom ratio, image, target description, 97
numbers, texts, and points, respectively. In addition to the predeﬁned manipulations, we also allow 98
trained models to create new manipulations during inference to facilitate problem-solving. We 99
empirically ﬁnd that more complicated goals can be derived from these fundamental manipulations. 100
We then deﬁne the standard CoM data structure to streamline the subsequent data construction 101
and validation process. Given a question Qabout an initial input image I0, a VLM equipped with 102
chain of manipulations mechanism solves the problem to achieve ﬁnal answer as VLM &(A; CjI0; Q), 103
where&refers to the reasoning chain with evidence, 104
&= (step 1; step 2; :::)
stepi= (fi; ci); f i2M(1)
where C= (ci; c2; :::; cjCj)refers to the free-form textual descriptions incorporating manipulation 105
names fiand corresponding results from utilizing fi. This deﬁnition explicitly declares the symbolic 106
execution process, while also being compatible with linguistic reasoning steps. Based on this 107
deﬁnition, we can clearly construct standard CoM samples that incorporating the manipulation 108
executions and linguistic steps with evidence. After the data construction, we can utilize a simple 109
method to convert the standard CoM samples to the compatible VQA samples . 110
3 Data Collection 111
Linguistic Annotators (LLMs)Generate solving steps for a given question, by optionally using chain of manipulations from      or newly created ones.
Using Grounding(the pillar in front of the man at         ) to ﬁnd .., the resultant region is         .Using CropZoomIn(         , 4) to crop it on        , and zoom in it by 4 times to get new image     .Using Grounding( the man in black shirt ) to locate .., the position is at         .
Using OCR(     )  recognize the text written on the image     , the result is  “QUICK DEPOSIT”.
<latexit sha1_base64="nquYd0jxdzSEaF8BNau6CPwxdhg=">AAACyHicjVHLSsNAFD3G97vq0k2wCK7CpNbY7opuxJWCqYKWkqRTHcyLZKKW4sYfcKtfJv6B/oV3xhR0UXRCkjvnnnNm7r1+GopcMvY+YUxOTc/Mzs0vLC4tr6xW1tbbeVJkAXeDJEyyC9/LeShi7kohQ36RZtyL/JCf+7eHKn9+x7NcJPGZHKS8E3nXseiLwJMEub7/0LW7lSqzmk1Wtx2TWXuM1ZwmBWy31nAc07aYXlWU6ySpvOEKPSQIUCACRwxJcQgPOT2XsMGQEtbBkLCMIqHzHI9YIG1BLE4Mj9Bb+l7T7rJEY9orz1yrAzolpDcjpYlt0iTEyyhWp5k6X2hnhY7zHmpPdbcB/f3SKyJU4obQv3Qj5n91qhaJPhq6BkE1pRpR1QWlS6G7om5u/qhKkkNKmIp7lM8oDrRy1GdTa3Jdu+qtp/MfmqlQtQ9KboFPdUsa8GiK5vigXbNsx7JP69XWQTnqOWxiCzs0z320cIQTuOQt8IwXvBrHRmrcG4NvqjFRajbwaxlPX98ykUs=</latexit>bbx1<latexit sha1_base64="nquYd0jxdzSEaF8BNau6CPwxdhg=">AAACyHicjVHLSsNAFD3G97vq0k2wCK7CpNbY7opuxJWCqYKWkqRTHcyLZKKW4sYfcKtfJv6B/oV3xhR0UXRCkjvnnnNm7r1+GopcMvY+YUxOTc/Mzs0vLC4tr6xW1tbbeVJkAXeDJEyyC9/LeShi7kohQ36RZtyL/JCf+7eHKn9+x7NcJPGZHKS8E3nXseiLwJMEub7/0LW7lSqzmk1Wtx2TWXuM1ZwmBWy31nAc07aYXlWU6ySpvOEKPSQIUCACRwxJcQgPOT2XsMGQEtbBkLCMIqHzHI9YIG1BLE4Mj9Bb+l7T7rJEY9orz1yrAzolpDcjpYlt0iTEyyhWp5k6X2hnhY7zHmpPdbcB/f3SKyJU4obQv3Qj5n91qhaJPhq6BkE1pRpR1QWlS6G7om5u/qhKkkNKmIp7lM8oDrRy1GdTa3Jdu+qtp/MfmqlQtQ9KboFPdUsa8GiK5vigXbNsx7JP69XWQTnqOWxiCzs0z320cIQTuOQt8IwXvBrHRmrcG4NvqjFRajbwaxlPX98ykUs=</latexit>bbx1
<latexit sha1_base64="2W62Q+SkAvdK1NjlqFl16kUP5hQ=">AAACyHicjVHLTsJAFD3UF+ILdemmkZi4IkNFlB3RjXGFiTwSJKQtA04obdNOVULc+ANu9cuMf6B/4Z2xJLogOk3bM+fec2buvU7oiVgy9p4xFhaXlleyq7m19Y3Nrfz2TjMOksjlDTfwgqjt2DH3hM8bUkiPt8OI22PH4y1ndK7irTsexSLwr+Uk5N2xPfTFQLi2JKrhOA89q5cvsGKlyqrWkUngiFmVMoHqcZlZzCwVmV4FpKse5N9wgz4CuEgwBocPSdiDjZieDkpgCInrYkpcREjoOMcjcqRNKItThk3siL5D2nVS1qe98oy12qVTPHojUpo4IE1AeRFhdZqp44l2Vuw876n2VHeb0N9JvcbEStwS+5dulvlfnapFYoBTXYOgmkLNqOrc1CXRXVE3N39UJckhJE7hPsUjwq5Wzvpsak2sa1e9tXX8Q2cqVu3dNDfBp7olDXg2RXM+aFrFUqVYuioXamfpqLPYwz4OaZ4nqOECdTTIW+AZL3g1Lo3QuDcm36lGJtXs4tcynr4A1CuRRg==</latexit>bbx2
<latexit sha1_base64="u1XfRsJCdu7luMOo4vc54wqulZc=">AAACzHicjVLLTsJAFD3UF+ILdemmkZi4wlYbwB3RjW4MJvIwSEhbBmzoK+3USAhbf8CtfpfxD/QvvDMWoyFGp2l759xzzsydO1boOjHXtNeMMje/sLiUXc6trK6tb+Q3txpxkEQ2q9uBG0Qty4yZ6/iszh3uslYYMdOzXNa0hqci37xjUewE/hUfhazjmQPf6Tu2yQm6tqz77tHBeVfv5gtaUS9plbKmakXj2KgYJVUvanJ8BQWkoxbkX3CDHgLYSOCBwQen2IWJmJ42dGgICetgTFhEkSPzDBPkSJsQixHDJHRI3wHN2inq01x4xlJt0youvREpVeyRJiBeRLFYTZX5RDoL9DfvsfQUexvR30q9PEI5bgn9Szdl/lcnauHooyJrcKimUCKiOjt1SeSpiJ2r36ri5BASJuIe5SOKbamcnrMqNbGsXZytKfNvkilQMbdTboJ3sUtq8Ew7Z4PGIV2Con5pFKonaauz2MEu9qmfZVRxhhrq5O3hEU94Vi4UroyVySdVyaSabfwYysMHzjCSaw==</latexit>bbx3/I1Using Grounding(the text written on the pillar in         ) to ﬁnd .., the resultant region is         .
<latexit sha1_base64="luFztvg4pkmUxMB01yKJs5viirE=">AAACznicjVHLSsNAFD2Nr/quunQTLIKrkoioy6Ibly3YB9Qik+m0DebFZFIopbj1B9zqZ4l/oH/hnTEFtYhOSHLm3HPuzL3XSwI/VY7zWrAWFpeWV4qra+sbm1vbpZ3dZhpnkosGj4NYtj2WisCPREP5KhDtRAoWeoFoeXeXOt4aCZn6cXStxonohmwQ+X2fM0VU5yZkashZMKlPb0tlp+KYZc8DNwdl5KsWl15wgx5icGQIIRBBEQ7AkNLTgQsHCXFdTIiThHwTF5hijbwZqQQpGLF39B3QrpOzEe11ztS4OZ0S0CvJaeOQPDHpJGF9mm3imcms2d9yT0xOfbcx/b08V0iswpDYv3wz5X99uhaFPs5NDT7VlBhGV8fzLJnpir65/aUqRRkS4jTuUVwS5sY567NtPKmpXfeWmfibUWpW73muzfCub0kDdn+Ocx40jyvuacWtn5SrF/moi9jHAY5onmeo4go1NEzHH/GEZ6tmjaypdf8ptQq5Zw/flvXwAY5Ck+4=</latexit>Q<latexit sha1_base64="Lz9T70LZH4j8t/WotJtOLW80Cn4=">AAACznicjVHLSsNAFD2Nr1pfVZdugkVwVRIRdVl147KCrYW2yGQ6rcG8mEwKpRS3/oBb/SzxD/QvvDOmoBbRCUnOnHvOnbn3ekngp8pxXgvW3PzC4lJxubSyura+Ud7caqZxJrlo8DiIZctjqQj8SDSUrwLRSqRgoReIa+/uXMevh0KmfhxdqVEiuiEbRH7f50wR1e6ETN1yFoxPJzflilN1zLJngZuDCvJVj8sv6KCHGBwZQghEUIQDMKT0tOHCQUJcF2PiJCHfxAUmKJE3I5UgBSP2jr4D2rVzNqK9zpkaN6dTAnolOW3skScmnSSsT7NNPDOZNftb7rHJqe82or+X5wqJVbgl9i/fVPlfn65FoY8TU4NPNSWG0dXxPEtmuqJvbn+pSlGGhDiNexSXhLlxTvtsG09qate9ZSb+ZpSa1XueazO861vSgN2f45wFzYOqe1R1Lw8rtbN81EXsYBf7NM9j1HCBOhqm4494wrNVt4bWxLr/lFqF3LONb8t6+ABoMpPe</latexit>A<latexit sha1_base64="Lz9T70LZH4j8t/WotJtOLW80Cn4=">AAACznicjVHLSsNAFD2Nr1pfVZdugkVwVRIRdVl147KCrYW2yGQ6rcG8mEwKpRS3/oBb/SzxD/QvvDOmoBbRCUnOnHvOnbn3ekngp8pxXgvW3PzC4lJxubSyura+Ud7caqZxJrlo8DiIZctjqQj8SDSUrwLRSqRgoReIa+/uXMevh0KmfhxdqVEiuiEbRH7f50wR1e6ETN1yFoxPJzflilN1zLJngZuDCvJVj8sv6KCHGBwZQghEUIQDMKT0tOHCQUJcF2PiJCHfxAUmKJE3I5UgBSP2jr4D2rVzNqK9zpkaN6dTAnolOW3skScmnSSsT7NNPDOZNftb7rHJqe82or+X5wqJVbgl9i/fVPlfn65FoY8TU4NPNSWG0dXxPEtmuqJvbn+pSlGGhDiNexSXhLlxTvtsG09qate9ZSb+ZpSa1XueazO861vSgN2f45wFzYOqe1R1Lw8rtbN81EXsYBf7NM9j1HCBOhqm4494wrNVt4bWxLr/lFqF3LONb8t6+ABoMpPe</latexit>A<latexit sha1_base64="luFztvg4pkmUxMB01yKJs5viirE=">AAACznicjVHLSsNAFD2Nr/quunQTLIKrkoioy6Ibly3YB9Qik+m0DebFZFIopbj1B9zqZ4l/oH/hnTEFtYhOSHLm3HPuzL3XSwI/VY7zWrAWFpeWV4qra+sbm1vbpZ3dZhpnkosGj4NYtj2WisCPREP5KhDtRAoWeoFoeXeXOt4aCZn6cXStxonohmwQ+X2fM0VU5yZkashZMKlPb0tlp+KYZc8DNwdl5KsWl15wgx5icGQIIRBBEQ7AkNLTgQsHCXFdTIiThHwTF5hijbwZqQQpGLF39B3QrpOzEe11ztS4OZ0S0CvJaeOQPDHpJGF9mm3imcms2d9yT0xOfbcx/b08V0iswpDYv3wz5X99uhaFPs5NDT7VlBhGV8fzLJnpir65/aUqRRkS4jTuUVwS5sY567NtPKmpXfeWmfibUWpW73muzfCub0kDdn+Ocx40jyvuacWtn5SrF/moi9jHAY5onmeo4go1NEzHH/GEZ6tmjaypdf8ptQq5Zw/flvXwAY5Ck+4=</latexit>Q
<latexit sha1_base64="BjPDv/IoisN4jPySDXrxkBnEZqQ=">AAACznicjVHLSsNAFD2Nr1pfVZdugkVwVRIRdVl040aoYGuhLTKZTmswLyaTQinFrT/gVj9L/AP9C++MKahFdEKSM+eec2fuvV4S+KlynNeCNTe/sLhUXC6trK6tb5Q3t5ppnEkuGjwOYtnyWCoCPxIN5atAtBIpWOgF4tq7O9Px66GQqR9HV2qUiG7IBpHf9zlTRLU7IVO3nAXji8lNueJUHbPsWeDmoIJ81ePyCzroIQZHhhACERThAAwpPW24cJAQ18WYOEnIN3GBCUrkzUglSMGIvaPvgHbtnI1or3Omxs3plIBeSU4be+SJSScJ69NsE89MZs3+lntscuq7jejv5blCYhVuif3LN1X+16drUejjxNTgU02JYXR1PM+Sma7om9tfqlKUISFO4x7FJWFunNM+28aTmtp1b5mJvxmlZvWe59oM7/qWNGD35zhnQfOg6h5V3cvDSu00H3URO9jFPs3zGDWco46G6fgjnvBs1a2hNbHuP6VWIfds49uyHj4AhL6T6g==</latexit>M
<latexit sha1_base64="/T/YLaO9AY6UEpfHnTU/ygqt5M8=">AAAC0HicjVHLSsNAFD2Nr/quunQTLIKrkoioy6obl1XsA2opk+m0DeZlMhFLKeLWH3CrXyX+gf6Fd8YU1CI6IcmZc+85M/deJ/LcRFrWa86Ymp6ZncvPLywuLa+sFtbWa0mYxlxUeeiFccNhifDcQFSlKz3RiGLBfMcTdefqRMXrNyJO3DC4kINItHzWC9yuy5kkqnXpM9nnzBsejdpWu1C0SpZe5iSwM1BEtiph4QWX6CAERwofAgEkYQ8MCT1N2LAQEdfCkLiYkKvjAiMskDalLEEZjNgr+vZo18zYgPbKM9FqTqd49MakNLFNmpDyYsLqNFPHU+2s2N+8h9pT3W1Afyfz8omV6BP7l26c+V+dqkWii0Ndg0s1RZpR1fHMJdVdUTc3v1QlySEiTuEOxWPCXCvHfTa1JtG1q94yHX/TmYpVe57lpnhXt6QB2z/HOQlquyV7v2Sf7RXLx9mo89jEFnZongco4xQVVMn7Go94wrNxbtwad8b9Z6qRyzQb+LaMhw8Q15SB</latexit>A0
<latexit sha1_base64="zM4q66wQW3lqPnlAeNIlhWc5UK4=">AAAC0HicjVHLSsNAFD2Nr/quunQTLIKrkoioy6obl1XsA2opk+m0DeZlMhFLKeLWH3CrXyX+gf6Fd8YU1CI6IcmZc+85M/deJ/LcRFrWa86Ymp6ZncvPLywuLa+sFtbWa0mYxlxUeeiFccNhifDcQFSlKz3RiGLBfMcTdefqRMXrNyJO3DC4kINItHzWC9yuy5kkqnXpM9nnzBsejdp2u1C0SpZe5iSwM1BEtiph4QWX6CAERwofAgEkYQ8MCT1N2LAQEdfCkLiYkKvjAiMskDalLEEZjNgr+vZo18zYgPbKM9FqTqd49MakNLFNmpDyYsLqNFPHU+2s2N+8h9pT3W1Afyfz8omV6BP7l26c+V+dqkWii0Ndg0s1RZpR1fHMJdVdUTc3v1QlySEiTuEOxWPCXCvHfTa1JtG1q94yHX/TmYpVe57lpnhXt6QB2z/HOQlquyV7v2Sf7RXLx9mo89jEFnZongco4xQVVMn7Go94wrNxbtwad8b9Z6qRyzQb+LaMhw8TN5SC</latexit>A1
<latexit sha1_base64="/KcGymOf6Y2mVdHFOKgNt1vAXiY=">AAAC0HicjVHLSsNAFD2Nr/quunQTLIKrkoioy6Ib3VWxD6ilTKbTNpiXyUQspYhbf8CtfpX4B/oX3hlTUIvohCRnzr3nzNx7nchzE2lZrzljanpmdi4/v7C4tLyyWlhbryVhGnNR5aEXxg2HJcJzA1GVrvREI4oF8x1P1J2rYxWv34g4ccPgQg4i0fJZL3C7LmeSqNalz2SfM294Ompb7ULRKll6mZPAzkAR2aqEhRdcooMQHCl8CASQhD0wJPQ0YcNCRFwLQ+JiQq6OC4ywQNqUsgRlMGKv6NujXTNjA9orz0SrOZ3i0RuT0sQ2aULKiwmr00wdT7WzYn/zHmpPdbcB/Z3MyydWok/sX7px5n91qhaJLg51DS7VFGlGVcczl1R3Rd3c/FKVJIeIOIU7FI8Jc60c99nUmkTXrnrLdPxNZypW7XmWm+Jd3ZIGbP8c5ySo7Zbs/ZJ9tlcsH2WjzmMTW9iheR6gjBNUUCXvazziCc/GuXFr3Bn3n6lGLtNs4NsyHj4AI++UiQ==</latexit>I0<latexit sha1_base64="ORjor/4Gs0BfUuuKlUjfF0aGIlE=">AAACxnicjVHLTsJAFD3UF+ILdemmkZi4aloCiDuiG9xhlEeChLTDgI2lbdqphhATf8CtfprxD/QvvDMWoyFGp2l759xzzsy91wk9Nxam+ZrRFhaXlleyq7m19Y3Nrfz2TisOkojxJgu8IOo4dsw91+dN4QqPd8KI22PH423n5lTm27c8it3AvxSTkPfG9sh3hy6zBUEXZ32rny+YhlWqlouWbhql44pZLuuWYar1FRSQrkaQf8EVBgjAkGAMDh+CYg82Ynq6sGAiJKyHKWERRa7Kc9wjR9qEWJwYNqE39B3RrpuiPu2lZ6zUjE7x6I1IqeOANAHxIorlabrKJ8pZor95T5WnvNuE/k7qNSZU4JrQv3Qz5n91shaBIaqqBpdqChUiq2OpS6K6Im+uf6tKkENImIwHlI8oZko567OuNLGqXfbWVvk3xZSo3LOUm+Bd3pIGPDfO+aBVNKyKYZ2XCrWTdNRZ7GEfhzTPI9RQRwNN8h7hEU941uqaryXa3SdVy6SaXfxY2sMHFmyQKw==</latexit>I1
<latexit sha1_base64="ORjor/4Gs0BfUuuKlUjfF0aGIlE=">AAACxnicjVHLTsJAFD3UF+ILdemmkZi4aloCiDuiG9xhlEeChLTDgI2lbdqphhATf8CtfprxD/QvvDMWoyFGp2l759xzzsy91wk9Nxam+ZrRFhaXlleyq7m19Y3Nrfz2TisOkojxJgu8IOo4dsw91+dN4QqPd8KI22PH423n5lTm27c8it3AvxSTkPfG9sh3hy6zBUEXZ32rny+YhlWqlouWbhql44pZLuuWYar1FRSQrkaQf8EVBgjAkGAMDh+CYg82Ynq6sGAiJKyHKWERRa7Kc9wjR9qEWJwYNqE39B3RrpuiPu2lZ6zUjE7x6I1IqeOANAHxIorlabrKJ8pZor95T5WnvNuE/k7qNSZU4JrQv3Qz5n91shaBIaqqBpdqChUiq2OpS6K6Im+uf6tKkENImIwHlI8oZko567OuNLGqXfbWVvk3xZSo3LOUm+Bd3pIGPDfO+aBVNKyKYZ2XCrWTdNRZ7GEfhzTPI9RQRwNN8h7hEU941uqaryXa3SdVy6SaXfxY2sMHFmyQKw==</latexit>I1<latexit sha1_base64="ORjor/4Gs0BfUuuKlUjfF0aGIlE=">AAACxnicjVHLTsJAFD3UF+ILdemmkZi4aloCiDuiG9xhlEeChLTDgI2lbdqphhATf8CtfprxD/QvvDMWoyFGp2l759xzzsy91wk9Nxam+ZrRFhaXlleyq7m19Y3Nrfz2TisOkojxJgu8IOo4dsw91+dN4QqPd8KI22PH423n5lTm27c8it3AvxSTkPfG9sh3hy6zBUEXZ32rny+YhlWqlouWbhql44pZLuuWYar1FRSQrkaQf8EVBgjAkGAMDh+CYg82Ynq6sGAiJKyHKWERRa7Kc9wjR9qEWJwYNqE39B3RrpuiPu2lZ6zUjE7x6I1IqeOANAHxIorlabrKJ8pZor95T5WnvNuE/k7qNSZU4JrQv3Qz5n91shaBIaqqBpdqChUiq2OpS6K6Im+uf6tKkENImIwHlI8oZko567OuNLGqXfbWVvk3xZSo3LOUm+Bd3pIGPDfO+aBVNKyKYZ2XCrWTdNRZ7GEfhzTPI9RQRwNN8h7hEU941uqaryXa3SdVy6SaXfxY2sMHFmyQKw==</latexit>I1
<latexit sha1_base64="nquYd0jxdzSEaF8BNau6CPwxdhg=">AAACyHicjVHLSsNAFD3G97vq0k2wCK7CpNbY7opuxJWCqYKWkqRTHcyLZKKW4sYfcKtfJv6B/oV3xhR0UXRCkjvnnnNm7r1+GopcMvY+YUxOTc/Mzs0vLC4tr6xW1tbbeVJkAXeDJEyyC9/LeShi7kohQ36RZtyL/JCf+7eHKn9+x7NcJPGZHKS8E3nXseiLwJMEub7/0LW7lSqzmk1Wtx2TWXuM1ZwmBWy31nAc07aYXlWU6ySpvOEKPSQIUCACRwxJcQgPOT2XsMGQEtbBkLCMIqHzHI9YIG1BLE4Mj9Bb+l7T7rJEY9orz1yrAzolpDcjpYlt0iTEyyhWp5k6X2hnhY7zHmpPdbcB/f3SKyJU4obQv3Qj5n91qhaJPhq6BkE1pRpR1QWlS6G7om5u/qhKkkNKmIp7lM8oDrRy1GdTa3Jdu+qtp/MfmqlQtQ9KboFPdUsa8GiK5vigXbNsx7JP69XWQTnqOWxiCzs0z320cIQTuOQt8IwXvBrHRmrcG4NvqjFRajbwaxlPX98ykUs=</latexit>bbx1
<latexit sha1_base64="nquYd0jxdzSEaF8BNau6CPwxdhg=">AAACyHicjVHLSsNAFD3G97vq0k2wCK7CpNbY7opuxJWCqYKWkqRTHcyLZKKW4sYfcKtfJv6B/oV3xhR0UXRCkjvnnnNm7r1+GopcMvY+YUxOTc/Mzs0vLC4tr6xW1tbbeVJkAXeDJEyyC9/LeShi7kohQ36RZtyL/JCf+7eHKn9+x7NcJPGZHKS8E3nXseiLwJMEub7/0LW7lSqzmk1Wtx2TWXuM1ZwmBWy31nAc07aYXlWU6ySpvOEKPSQIUCACRwxJcQgPOT2XsMGQEtbBkLCMIqHzHI9YIG1BLE4Mj9Bb+l7T7rJEY9orz1yrAzolpDcjpYlt0iTEyyhWp5k6X2hnhY7zHmpPdbcB/f3SKyJU4obQv3Qj5n91qhaJPhq6BkE1pRpR1QWlS6G7om5u/qhKkkNKmIp7lM8oDrRy1GdTa3Jdu+qtp/MfmqlQtQ9KboFPdUsa8GiK5vigXbNsx7JP69XWQTnqOWxiCzs0z320cIQTuOQt8IwXvBrHRmrcG4NvqjFRajbwaxlPX98ykUs=</latexit>bbx1<latexit sha1_base64="2W62Q+SkAvdK1NjlqFl16kUP5hQ=">AAACyHicjVHLTsJAFD3UF+ILdemmkZi4IkNFlB3RjXGFiTwSJKQtA04obdNOVULc+ANu9cuMf6B/4Z2xJLogOk3bM+fec2buvU7oiVgy9p4xFhaXlleyq7m19Y3Nrfz2TjMOksjlDTfwgqjt2DH3hM8bUkiPt8OI22PH4y1ndK7irTsexSLwr+Uk5N2xPfTFQLi2JKrhOA89q5cvsGKlyqrWkUngiFmVMoHqcZlZzCwVmV4FpKse5N9wgz4CuEgwBocPSdiDjZieDkpgCInrYkpcREjoOMcjcqRNKItThk3siL5D2nVS1qe98oy12qVTPHojUpo4IE1AeRFhdZqp44l2Vuw876n2VHeb0N9JvcbEStwS+5dulvlfnapFYoBTXYOgmkLNqOrc1CXRXVE3N39UJckhJE7hPsUjwq5Wzvpsak2sa1e9tXX8Q2cqVu3dNDfBp7olDXg2RXM+aFrFUqVYuioXamfpqLPYwz4OaZ4nqOECdTTIW+AZL3g1Lo3QuDcm36lGJtXs4tcynr4A1CuRRg==</latexit>bbx2
<latexit sha1_base64="7VClObItbF+/n0CnXZnOebL3Syo=">AAACyHicjVHLTsJAFD3UF+ILdemmkZi4aloExB3RjXGFiYAJEtKWASeUtmmnKiFu/AG3+mXGP9C/8M5YjIYYnabtnXPPOTP3Xif0eCxM8zWjzc0vLC5ll3Mrq2vrG/nNrWYcJJHLGm7gBdGlY8fM4z5rCC48dhlGzB45Hms5wxOZb92wKOaBfyHGIeuM7IHP+9y1BUENx7nrHnTzBdOwStVy0dJNo3RUMctl3TJMtb6CAtJVD/IvuEIPAVwkGIHBh6DYg42YnjYsmAgJ62BCWEQRV3mGe+RImxCLEcMmdEjfAe3aKerTXnrGSu3SKR69ESl17JEmIF5EsTxNV/lEOUv0N++J8pR3G9PfSb1GhApcE/qXbsr8r07WItBHVdXAqaZQIbI6N3VJVFfkzfVvVQlyCAmTcY/yEcWuUk77rCtNrGqXvbVV/k0xJSr3bspN8C5vSQOeGeds0CwaVsWwzkuF2nE66ix2sIt9muchajhFHQ3y5njEE561My3UbrXxJ1XLpJpt/FjawwerAJE0</latexit>bbx3
<latexit sha1_base64="2W62Q+SkAvdK1NjlqFl16kUP5hQ=">AAACyHicjVHLTsJAFD3UF+ILdemmkZi4IkNFlB3RjXGFiTwSJKQtA04obdNOVULc+ANu9cuMf6B/4Z2xJLogOk3bM+fec2buvU7oiVgy9p4xFhaXlleyq7m19Y3Nrfz2TjMOksjlDTfwgqjt2DH3hM8bUkiPt8OI22PH4y1ndK7irTsexSLwr+Uk5N2xPfTFQLi2JKrhOA89q5cvsGKlyqrWkUngiFmVMoHqcZlZzCwVmV4FpKse5N9wgz4CuEgwBocPSdiDjZieDkpgCInrYkpcREjoOMcjcqRNKItThk3siL5D2nVS1qe98oy12qVTPHojUpo4IE1AeRFhdZqp44l2Vuw876n2VHeb0N9JvcbEStwS+5dulvlfnapFYoBTXYOgmkLNqOrc1CXRXVE3N39UJckhJE7hPsUjwq5Wzvpsak2sa1e9tXX8Q2cqVu3dNDfBp7olDXg2RXM+aFrFUqVYuioXamfpqLPYwz4OaZ4nqOECdTTIW+AZL3g1Lo3QuDcm36lGJtXs4tcynr4A1CuRRg==</latexit>bbx2<latexit sha1_base64="7VClObItbF+/n0CnXZnOebL3Syo=">AAACyHicjVHLTsJAFD3UF+ILdemmkZi4aloExB3RjXGFiYAJEtKWASeUtmmnKiFu/AG3+mXGP9C/8M5YjIYYnabtnXPPOTP3Xif0eCxM8zWjzc0vLC5ll3Mrq2vrG/nNrWYcJJHLGm7gBdGlY8fM4z5rCC48dhlGzB45Hms5wxOZb92wKOaBfyHGIeuM7IHP+9y1BUENx7nrHnTzBdOwStVy0dJNo3RUMctl3TJMtb6CAtJVD/IvuEIPAVwkGIHBh6DYg42YnjYsmAgJ62BCWEQRV3mGe+RImxCLEcMmdEjfAe3aKerTXnrGSu3SKR69ESl17JEmIF5EsTxNV/lEOUv0N++J8pR3G9PfSb1GhApcE/qXbsr8r07WItBHVdXAqaZQIbI6N3VJVFfkzfVvVQlyCAmTcY/yEcWuUk77rCtNrGqXvbVV/k0xJSr3bspN8C5vSQOeGeds0CwaVsWwzkuF2nE66ix2sIt9muchajhFHQ3y5njEE561My3UbrXxJ1XLpJpt/FjawwerAJE0</latexit>bbx3<latexit sha1_base64="7VClObItbF+/n0CnXZnOebL3Syo=">AAACyHicjVHLTsJAFD3UF+ILdemmkZi4aloExB3RjXGFiYAJEtKWASeUtmmnKiFu/AG3+mXGP9C/8M5YjIYYnabtnXPPOTP3Xif0eCxM8zWjzc0vLC5ll3Mrq2vrG/nNrWYcJJHLGm7gBdGlY8fM4z5rCC48dhlGzB45Hms5wxOZb92wKOaBfyHGIeuM7IHP+9y1BUENx7nrHnTzBdOwStVy0dJNo3RUMctl3TJMtb6CAtJVD/IvuEIPAVwkGIHBh6DYg42YnjYsmAgJ62BCWEQRV3mGe+RImxCLEcMmdEjfAe3aKerTXnrGSu3SKR69ESl17JEmIF5EsTxNV/lEOUv0N++J8pR3G9PfSb1GhApcE/qXbsr8r07WItBHVdXAqaZQIbI6N3VJVFfkzfVvVQlyCAmTcY/yEcWuUk77rCtNrGqXvbVV/k0xJSr3bspN8C5vSQOeGeds0CwaVsWwzkuF2nE66ix2sIt9muchajhFHQ3y5njEE561My3UbrXxJ1XLpJpt/FjawwerAJE0</latexit>bbx3What is written on the pillar in front of man in black top?: QUICK DEPOSIT
<latexit sha1_base64="3uQ8QX2fue98Q2WsirfBsNLdvpE=">AAACyHicjVHLSsNAFD2Nr1pfVZdugkVwVRIRdVl0I64qmLagpSTTaR2aJmEyUUtx4w+41S8T/0D/wjtjCmoRnZDkzLn3nJl7b5CEIlWO81qwZmbn5heKi6Wl5ZXVtfL6RiONM8m4x+Iwlq3AT3koIu4poULeSiT3h0HIm8HgRMebN1ymIo4u1Cjh7aHfj0RPMF8R5ak71XE75YpTdcyyp4GbgwryVY/LL7hCFzEYMgzBEUERDuEjpecSLhwkxLUxJk4SEibOcY8SaTPK4pThEzugb592lzkb0V57pkbN6JSQXklKGzukiSlPEtan2SaeGWfN/uY9Np76biP6B7nXkFiFa2L/0k0y/6vTtSj0cGRqEFRTYhhdHctdMtMVfXP7S1WKHBLiNO5SXBJmRjnps200qald99Y38TeTqVm9Z3luhnd9Sxqw+3Oc06CxV3UPqu75fqV2nI+6iC1sY5fmeYgaTlGHR94Cj3jCs3VmJdatNfpMtQq5ZhPflvXwAYXbkSM=</latexit>txt1Linguistic AnnotationVQA SampleVisual AnnotationVisual Annotators (VFMs)<latexit sha1_base64="b2bQACR8spoFXUfIDRHakvG4W44=">AAAC73icjVHLSsNAFD3GV31XXboJFqG6KImIuhRd6LKCVcGWkqTTdDDNxMnEB8V/cOdO3PoDbvUvxD/Qv/DOGMEHohOSnDn3njNz7/WTiKfKcZ77rP6BwaHhwsjo2PjE5FRxemY/FZkMWC0QkZCHvpeyiMespriK2GEimdf1I3bgH2/p+MEpkykX8Z66SFij64Uxb/PAU0Q1i0t1xc5J19uWIotbPA4vyypUTXexLnnYUZ6U4sz2/fOm2yyWnIpjlv0TuDkoIV9VUXxCHS0IBMjQBUMMRTiCh5SeI7hwkBDXQI84SYibOMMlRkmbURajDI/YY/qGtDvK2Zj22jM16oBOieiVpLSxQBpBeZKwPs028cw4a/Y3757x1He7oL+fe3WJVegQ+5fuI/O/Ol2LQhvrpgZONSWG0dUFuUtmuqJvbn+qSpFDQpzGLYpLwoFRfvTZNprU1K5765n4i8nUrN4HeW6GV31LGrD7fZw/wf5yxV2tuLsrpY3NfNQFzGEeZZrnGjawgypq5H2Fezzg0Tqxrq0b6/Y91erLNbP4sqy7N0MpoTo=</latexit>Grounding(tgt1)!bbx1
<latexit sha1_base64="H0Ooc4Z/ZMj6HIyBcoV6b4A+CUA=">AAAC73icjVHLSsNAFD2N7/qqunQTLIK6KEkRdVl0oUsFq4KVkqRjHEwzcTLRluI/uHMnbv0Bt/oX4h/oX3hnjOAD0QlJzpx7z5m59/pJxFPlOM8Fq69/YHBoeKQ4OjY+MVmamt5LRSYDVg9EJOSB76Us4jGrK64idpBI5rX9iO37pxs6vn/OZMpFvKu6CTtqe2HMj3ngKaKapaWGYh3S9TalyOIWj8PLBRWqZnWxIXl4ojwpxYXt+51mtVkqOxXHLPsncHNQRr62RekJDbQgECBDGwwxFOEIHlJ6DuHCQULcEXrESULcxBkuUSRtRlmMMjxiT+kb0u4wZ2Paa8/UqAM6JaJXktLGPGkE5UnC+jTbxDPjrNnfvHvGU9+tS38/92oTq3BC7F+6j8z/6nQtCsdYMzVwqikxjK4uyF0y0xV9c/tTVYocEuI0blFcEg6M8qPPttGkpnbdW8/EX0ymZvU+yHMzvOpb0oDd7+P8CfaqFXel4u4sl2vr+aiHMYs5LNA8V1HDFrZRJ+8r3OMBj9aZdW3dWLfvqVYh18zgy7Lu3gBH+6E8</latexit>Grounding(tgt2)!bbx2
<latexit sha1_base64="61ZzdnoXO7upFU9SmdB0Ja42I2o=">AAAC73icjVHLSsNAFD2N73fVpZtgEdRFSayoS9GFLhVsK1gpSTrGwTQTJxMfFP/BnTtx6w+41b8Q/0D/wjtjCj4QnZDkzLn3nJl7r59EPFWO81Kwenr7+gcGh4ZHRsfGJ4qTU7VUZDJg1UBEQu77XsoiHrOq4ipi+4lkXtuPWN0/2dTx+hmTKRfxnrpM2GHbC2N+xANPEdUsLjYUuyBdZ0uKLG7xOLyaV6FqVhYakofHypNSnNu+f9GsNIslp+yYZf8Ebg5KyNeOKD6jgRYEAmRogyGGIhzBQ0rPAVw4SIg7RIc4SYibOMMVhkmbURajDI/YE/qGtDvI2Zj22jM16oBOieiVpLQxRxpBeZKwPs028cw4a/Y3747x1He7pL+fe7WJVTgm9i9dN/O/Ol2LwhHWTA2cakoMo6sLcpfMdEXf3P5UlSKHhDiNWxSXhAOj7PbZNprU1K5765n4q8nUrN4HeW6GN31LGrD7fZw/QW2p7K6U3d3l0vpGPupBzGAW8zTPVaxjGzuokvc1HvCIJ+vUurFurbuPVKuQa6bxZVn370zNoT4=</latexit>Grounding(tgt3)!bbx3
<latexit sha1_base64="76HXZux32y8Z7IeCXjMCYR2Nwsc=">AAAC6XicjVHLSsNAFD3GV31XXboJVqFuSiKiLovduPOB1UIrJUmndWheTCZaKf6AO3fi1h9wqz8i/oH+hXfGCGoRnZDkzLn3nJl7rxv7PJGW9TJkDI+Mjo3nJianpmdm5/LzC8dJlAqPVb3Ij0TNdRLm85BVJZc+q8WCOYHrsxO3W1Hxk3MmEh6FR/IyZqeB0wl5m3uOJKqZX2lI1iNdf69yeFXkQadprzUE75xJR4jowpQ92bSb+YJVsvQyB4GdgQKytR/ln9FACxE8pAjAEEIS9uEgoacOGxZi4k7RJ04Q4jrOcIVJ0qaUxSjDIbZL3w7t6hkb0l55Jlrt0Sk+vYKUJlZJE1GeIKxOM3U81c6K/c27rz3V3S7p72ZeAbESZ8T+pfvM/K9O1SLRxraugVNNsWZUdV7mkuquqJubX6qS5BATp3CL4oKwp5WffTa1JtG1q946Ov6qMxWr9l6Wm+JN3ZIGbP8c5yA4Xi/ZmyX7YKNQ3slGncMSllGkeW6hjF3so0re13jAI56MrnFj3Bp3H6nGUKZZxLdl3L8DgiyeRw==</latexit>OCR(img1)!txt1
DFS WRƉQGSRVLWLYHFKDLQV: Compatible VQA
[4,5,7,12][28,6,30,9][8,0,17,13]…[9,7,11,9]
QUICK DEPOSIT
<latexit sha1_base64="/KcGymOf6Y2mVdHFOKgNt1vAXiY=">AAAC0HicjVHLSsNAFD2Nr/quunQTLIKrkoioy6Ib3VWxD6ilTKbTNpiXyUQspYhbf8CtfpX4B/oX3hlTUIvohCRnzr3nzNx7nchzE2lZrzljanpmdi4/v7C4tLyyWlhbryVhGnNR5aEXxg2HJcJzA1GVrvREI4oF8x1P1J2rYxWv34g4ccPgQg4i0fJZL3C7LmeSqNalz2SfM294Ompb7ULRKll6mZPAzkAR2aqEhRdcooMQHCl8CASQhD0wJPQ0YcNCRFwLQ+JiQq6OC4ywQNqUsgRlMGKv6NujXTNjA9orz0SrOZ3i0RuT0sQ2aULKiwmr00wdT7WzYn/zHmpPdbcB/Z3MyydWok/sX7px5n91qhaJLg51DS7VFGlGVcczl1R3Rd3c/FKVJIeIOIU7FI8Jc60c99nUmkTXrnrLdPxNZypW7XmWm+Jd3ZIGbP8c5ySo7Zbs/ZJ9tlcsH2WjzmMTW9iheR6gjBNUUCXvazziCc/GuXFr3Bn3n6lGLtNs4NsyHj4AI++UiQ==</latexit>I0
<latexit sha1_base64="luFztvg4pkmUxMB01yKJs5viirE=">AAACznicjVHLSsNAFD2Nr/quunQTLIKrkoioy6Ibly3YB9Qik+m0DebFZFIopbj1B9zqZ4l/oH/hnTEFtYhOSHLm3HPuzL3XSwI/VY7zWrAWFpeWV4qra+sbm1vbpZ3dZhpnkosGj4NYtj2WisCPREP5KhDtRAoWeoFoeXeXOt4aCZn6cXStxonohmwQ+X2fM0VU5yZkashZMKlPb0tlp+KYZc8DNwdl5KsWl15wgx5icGQIIRBBEQ7AkNLTgQsHCXFdTIiThHwTF5hijbwZqQQpGLF39B3QrpOzEe11ztS4OZ0S0CvJaeOQPDHpJGF9mm3imcms2d9yT0xOfbcx/b08V0iswpDYv3wz5X99uhaFPs5NDT7VlBhGV8fzLJnpir65/aUqRRkS4jTuUVwS5sY567NtPKmpXfeWmfibUWpW73muzfCub0kDdn+Ocx40jyvuacWtn5SrF/moi9jHAY5onmeo4go1NEzHH/GEZ6tmjaypdf8ptQq5Zw/flvXwAY5Ck+4=</latexit>Q<latexit sha1_base64="/T/YLaO9AY6UEpfHnTU/ygqt5M8=">AAAC0HicjVHLSsNAFD2Nr/quunQTLIKrkoioy6obl1XsA2opk+m0DeZlMhFLKeLWH3CrXyX+gf6Fd8YU1CI6IcmZc+85M/deJ/LcRFrWa86Ymp6ZncvPLywuLa+sFtbWa0mYxlxUeeiFccNhifDcQFSlKz3RiGLBfMcTdefqRMXrNyJO3DC4kINItHzWC9yuy5kkqnXpM9nnzBsejdpWu1C0SpZe5iSwM1BEtiph4QWX6CAERwofAgEkYQ8MCT1N2LAQEdfCkLiYkKvjAiMskDalLEEZjNgr+vZo18zYgPbKM9FqTqd49MakNLFNmpDyYsLqNFPHU+2s2N+8h9pT3W1Afyfz8omV6BP7l26c+V+dqkWii0Ndg0s1RZpR1fHMJdVdUTc3v1QlySEiTuEOxWPCXCvHfTa1JtG1q94yHX/TmYpVe57lpnhXt6QB2z/HOQlquyV7v2Sf7RXLx9mo89jEFnZongco4xQVVMn7Go94wrNxbtwad8b9Z6qRyzQb+LaMhw8Q15SB</latexit>A0(   ,     ,      )(   ,     ,      )<latexit sha1_base64="zM4q66wQW3lqPnlAeNIlhWc5UK4=">AAAC0HicjVHLSsNAFD2Nr/quunQTLIKrkoioy6obl1XsA2opk+m0DeZlMhFLKeLWH3CrXyX+gf6Fd8YU1CI6IcmZc+85M/deJ/LcRFrWa86Ymp6ZncvPLywuLa+sFtbWa0mYxlxUeeiFccNhifDcQFSlKz3RiGLBfMcTdefqRMXrNyJO3DC4kINItHzWC9yuy5kkqnXpM9nnzBsejdp2u1C0SpZe5iSwM1BEtiph4QWX6CAERwofAgEkYQ8MCT1N2LAQEdfCkLiYkKvjAiMskDalLEEZjNgr+vZo18zYgPbKM9FqTqd49MakNLFNmpDyYsLqNFPHU+2s2N+8h9pT3W1Afyfz8omV6BP7l26c+V+dqkWii0Ndg0s1RZpR1fHMJdVdUTc3v1QlySEiTuEOxWPCXCvHfTa1JtG1q94yHX/TmYpVe57lpnhXt6QB2z/HOQlquyV7v2Sf7RXLx9mo89jEFnZongco4xQVVMn7Go94wrNxbtwad8b9Z6qRyzQb+LaMhw8TN5SC</latexit>A1
<latexit sha1_base64="NcauhQNgiNIOtVzvcG6THrKjLSY=">AAAC0HicjVHLSsNAFD2Nr/quunQTLIKrkoioy6Ib3VWxD6ilTKbTNpiXyUQspYhbf8CtfpX4B/oX3hlTUIvohCRnzr3nzNx7nchzE2lZrzljanpmdi4/v7C4tLyyWlhbryVhGnNR5aEXxg2HJcJzA1GVrvREI4oF8x1P1J2rYxWv34g4ccPgQg4i0fJZL3C7LmeSqNalz2SfM294Omrb7ULRKll6mZPAzkAR2aqEhRdcooMQHCl8CASQhD0wJPQ0YcNCRFwLQ+JiQq6OC4ywQNqUsgRlMGKv6NujXTNjA9orz0SrOZ3i0RuT0sQ2aULKiwmr00wdT7WzYn/zHmpPdbcB/Z3MyydWok/sX7px5n91qhaJLg51DS7VFGlGVcczl1R3Rd3c/FKVJIeIOIU7FI8Jc60c99nUmkTXrnrLdPxNZypW7XmWm+Jd3ZIGbP8c5ySo7Zbs/ZJ9tlcsH2WjzmMTW9iheR6gjBNUUCXvazziCc/GuXFr3Bn3n6lGLtNs4NsyHj4AJk+Uig==</latexit>I1
<latexit sha1_base64="B5Wvf7yhCi37aDSakUUps0PiuzI=">AAAC1nicjVHLSsNAFD3GV323unQTLIKrkoioy6Ibly3YB1iRyXSqwbyYTJQS6k7c+gNu9ZPEP9C/8M6Ygg9EJyQ5c+49Z+be6yWBnyrHeZmwJqemZ2ZLc/MLi0vLK+XKajuNM8lFi8dBLLseS0XgR6KlfBWIbiIFC71AdLzLQx3vXAmZ+nF0rIaJOA3ZeeQPfM4UUWflSs9jMu+FTF1wFuTN0eisXHVqjln2T+AWoIpiNeLyM3roIwZHhhACERThAAwpPSdw4SAh7hQ5cZKQb+ICI8yTNqMsQRmM2Ev6ntPupGAj2mvP1Kg5nRLQK0lpY5M0MeVJwvo028Qz46zZ37xz46nvNqS/V3iFxCpcEPuXbpz5X52uRWGAfVODTzUlhtHV8cIlM13RN7c/VaXIISFO4z7FJWFulOM+20aTmtp1b5mJv5pMzeo9L3IzvOlb0oDd7+P8CdrbNXe35jZ3qvWDYtQlrGMDWzTPPdRxhAZa5H2NBzziyepaN9atdfeRak0UmjV8Wdb9OyqgluQ=</latexit>¯Q
Figure 4: A cascading data generation pipeline that automatically produces standard CoM samples.
Given an original VQA sample, the linguistic annotator (LLMs) taught with usage of manipulations
(prompt) is ﬁrst asked to provide solving steps for the question Q, and the visual foundational models
(VFMs) are then engaged to replace the manipulations results, followed by a ﬁnal traversal on the
tree branched by the possible manipulation results to ﬁnd positive paths terminating to the answer A.
In this section, we ﬁrst introduces the automated data generation pipeline (illustrated in Figure 4), 112
that employs reliable LLMs as linguistic annotators and VFMs as the visual annotators to produce 113
error-free CoM samples upon prevalent VQA corpus, and then present the manual annotation of 114
high-quality CoM samples for the challenging graphical mathematical problems. 115
3.1 Automated Data Generation 116
Given a general corpus D=f(I; Q; A )gconsisting of triplet samples of images with corresponding 117
visual question-answer pairs, our automated data generation pipeline consists of a linguistic annotator 118
and several visual annotators according to the manipulations. For a question Qin each sample, we 119
ﬁrst engage the linguistic annotator to generate manipulations-assisted solving steps with the CoM 120
format (fi; ci), where the corresponding results of the instantiated manipulation executions are set 121
with variables as placeholders. In this paper, we adopt GPT-4 (OpenAI, 2023a), a large language 122
4model with reliable language understanding and generation abilities as the linguistic annotator. We 123
design a comprehensive prompt including the task requirements, usage of manipulations, and output 124
data format, and further manually annotate 5 demonstrations for a stable generation. The detailed 125
implementations are available at Appendix C.4. 126
We then employ essential visual annotators to supply the results of manipulations requested in the 127
solving steps by exactly performing the corresponding manipulations. By empirically analyzing 128
the manipulations from both predeﬁned set and newly created ones (refers to Appendix C.3 for 129
a detailed statistics), we reveal the Grounding andOCR are two fundamental manipulations, and 130
most of the others can be consequently derived ( e.g., CropZoomIn along a region of box, Counting 131
upon recognized boxes, and Calculate for the recognized formula). Therefore, we employ two 132
visual foundational models, GroundingDINO (Liu et al., 2023c) and PaddleOCR (Du et al., 2020), 133
and develop the implementations of these manipulations1. The execution of the manipulations will 134
transform the sequential reasoning steps into a treeT, as the input of current manipulation f1(xa) 135
may rely on one of the multiple results of previous manipulation f2!(xb; xc),i.e.,xarely on xb 136
(e.g., step 2 for ﬁnding pillars in Figure 5). We then perform a traversal on each produced tree with 137
Depth First Search (DFS) to ﬁnd all positive paths fPijPi2T; i= 1;2; :::gthat can terminate with 138
the ﬁnal answer Afrom the result of the last manipulation. Based on this method, the generated 139
CoM samples with positive paths are guaranteed to be error-free. We implement this pipeline on 3 140
existing datasets that require detailed recognition or objects counting, TextVQA (Singh et al., 2019), 141
ST-VQA (Biten et al., 2019), and TDIUC (Shrestha et al., 2019), to build 70K CoM samples2. The 142
designed prompt, a generated example with linguistic and visual results, and detailed algorithm 143
illustration are available at AppendixC.1. 144
3.2 Human Annotation 145
The analysis from Fig.1 of AlphaGeometry (Trinh et al., 2024) shows that outputting auxiliary lines 146
in linguistic reasoning process helps LLMs to solve complex geometry problems. Beneﬁting from the 147
expressive capability of CoM structure, we have also manually annotated high-quality CoM samples 148
for the graphical mathematical problems to facilitate VLMs in solving this challenging scenario. 149
Similar to the automated pipeline, we engage 10 human experts as the linguistic annotators and 150
visual annotators, where each expert is asked to annotate the linguistic solving steps and the use of 151
manipulations, as well as the results of manipulations on images. We perform this annotation on the 152
MathVista (Lu et al., 2023) and ChartQA (Masry et al., 2022), which include geometric and chart 153
math problems, resulting in the collection of 6K high-quality CoM math samples. 154
Finally, we adapt the CoM samples to be compatible with VQA-style training samples. For each CoM 155
sample including nimages from manipulations outputs (I0; Q; C 0; I1; C1; :::; I n; A), we convert it 156
into a multi-turn VQA sample segmented by the images [(I0; Q; C 0);(I1;Q; C 1); :::;(In;Q; A)], 157
where Cirepresents the intermediate steps between IiandIi+1, and Qis a simple prompt asking 158
model to answer question based on history. This transformation converts CoM samples into multi-turn 159
VQA samples that are compatible with existing VLMs training data. The detailed statistics of the 160
data generation are available at Appendix C.3. 161
4 Model Training 162
4.1 Architecture 163
We use the same model architecture as CogVLM (Wang et al., 2023b), a general VLM approach 164
that involves four fundamental components: (1) a Visual Encoder, (2) an MLP Adapter, (3) an LLM 165
Backbone, and (4) a Visual Expert Module, for a reliable multimodal understanding. Concretely, 166
the pre-trained EV A2-CLIP-E (Sun et al., 2023a) with 4B parameters and Vicuna-7B-v1.5 (Chiang 167
et al., 2023) are adopted as the visual encoder and LLM backbone, respectively. A two-layer MLP 168
(SwiGLU (Shazeer, 2020)) is further engaged to map the output of the visual encoder into the 169
linguistic space of the LLM backbone. The visual expert module adds the vision-speciﬁc weights 170
into the attention layer and feed-forward layer of each block in the LLM backbone, resulting in a 171
total of 6.5B additional parameters for the deep fusion of modalities. 172
1We simply implement the CropZoomIn referring to human behaviors with a local code interpreter.
2The success rate of GPT-4 to achieve the positive paths is 0.3555.
5Large Language ModelLLMW W KV-memory(Image_0, Prompt_0)ViTProjectorPrompt_1Image_1Response_0Response_1Stage 1: Pre-training- Foundational visual XQGHUVWDQGLQJ- Grounded XQGHUVWDQGLQJStage 2: Fine-tuning- Multimodal problems-solvingChat, Captioning, Grounding, ReasoningData structure:- {(image, caption)}- {(image, prompt, response)}Data structure- {(image, prompt, response)}Figure 5: Left: A compatible VLM architecture capable of multi-turn multi-image understanding.
Right : An effective training process to develop a general VLM with versatile capabilities.
Based on this general architecture, we develop a memory-based multi-turn multi-image VLM 173
approach. Speciﬁcally, for a multi-turn VQA sample [(It; Qt; At)jt= 1;2; :::], where Atrefers to 174
Ctin CoM, we keep the accumulated KV memories of each layer in the LLM backbone throughout 175
these turns. And at each turn tin training and inference, we calculate the attention function attas: 176
att(X) =softmax (QtK0T
tp
d)V0
t
K0
t=trunc(concat (K0;K1; :::;Kt))
V0
t=trunc(concat (V0;V1; :::;Vt))(2)
177
where Qt2Rsdis query representation of current layer, and the K0
t;V0
t2R(st)drefer to the 178
concatenation of accumulated representations and will be further truncated if the sequence length 179
stis greater than a predeﬁned threshold. At t >0, the new image Itwill be cropped from It 1 180
and ampliﬁed with the Bicubic Interpolation (Keys, 1981). 181
4.2 Training 182
The proposed CogCoM-17B relies on two main stages of training, to develop the capabilities of 183
general multimodal task-solving as well as the visual reasoning. 184
First Stage Pre-Training This stage consists of two ordinal sub-phases of training for foundational 185
visual understanding and grounded generation. Following the pre-training of CogVLM (Wang et al., 186
2023b), we ﬁrst train model on 1.5B image-text pairs cleaned from the LAION-2B (Schuhmann et al., 187
2022) and COYO-700M (Byeon et al., 2022) with 120,000 iterations and batch size of 8,192. We 188
then train model on 40M grounded image-question-answer triples cleaned from LAION-115M (Li 189
et al., 2023b) with 60,000 iterations and batch size of 1,024, where each noun phrase in the answer is 190
followed by a list of coordinates [[x0; y0; x1; y1]; :::]3referring the phrase to the grounded objects in 191
the image. Both phases adopt the next token prediction objective, and train the 6.5B parameters of 192
visual experts. 193
Second Stage Alignment This stage further trains the model to align with human preferences on 194
solving practical visual problems. We fuse the produced CoM data with 3 types of corpus, including 195
MultiInstruct (Xu et al., 2022), LLaV AR (Zhang et al., 2023b), and ShareGPT4V (Chen et al., 2023c), 196
referring the abilities of instruction-following, texts-recognizing, and detailed-captioning. This fusion 197
results in a total of 570K (I; Q; A )samples, where the answer Ain CoM data consists of multiple 198
turns. For the training data of CoM, we randomly prepend a lunching prompt4PMto questions 199
Q=PM+Qasking models to optionally use manipulations for the adaption of explicitly eliciting. 200
We empirically show that the model can effectively learn the evidential visual reasoning by ingesting 201
this portion of CoM data. We train model with 14,000 iterations and a batch size of 160, where the 202
learning rate reaches 10 5after 280 steps of warm-up and then decays linearly. The parameters 203
of 6.5B visual experts are trained with the objective of next token prediction. These two stages of 204
training result in our standard version of CogCoM involving both chat and reasoning capabilities. 205
More training details are available at Appendix D.2. 206
3xi; yi2[000;999] refer to the normalized pixel coordinates.
4See Appendix D.1 for examples.
65 Experiment 207
To quantitatively validate the suitability and efﬁciency of the proposed method, we conduct exper- 208
iments on 9 benchmarks corresponding to 4 categories of multimodal capabilities, as well as on a 209
newly constructed testbed that includes the evidential reasoning paths with a keypoints-aware metric. 210
Following previous works, we train two generalist versions of CogCoM for adapting to the different 211
scenarios of Visual Question Answering and Visual Grounding, and evaluate the standard version 212
with a qualitative analysis (Hwang et al., 2023). We also evaluate the time complexity. 213
•Detailed Visual Question Answering. This task involves models to perform detailed 214
reasoning or recognition on images. We use 4 prominent benchmarks including, GQA (Hud- 215
son & Manning, 2019), TextVQA (Singh et al., 2019), ST-VQA (Biten et al., 2019), and 216
TallyVQA (Acharya et al., 2019). 217
•Visual Grounding. Visual grounding evaluates the crucial abilities of VLMs on meticulous 218
position understanding. We evaluate our model on 3 standard benchmarks, RefCOCO (Yu 219
et al., 2016), RefCOCO+ (Yu et al., 2016), and RefCOCOg (Mao et al., 2016). 220
•General Multimodal Capabilities & Hallucination. We also evaluate on a general mul- 221
timodal benchmark, MM-Vet (Yu et al., 2023b), and a hallucination detection benchmark 222
POPE (Li et al., 2023c), to investigate the helpfulness of visual reasoning. 223
5.1 Experiments on Detailed VQA 224
VLMs have demonstrated the well-known superiority in visual scenes with salient content understand- 225
ing. We evaluate the effectiveness of CogCoM on VQAs on detailed understanding, which typically 226
require models to perform multiple actions ( ﬁnd, read ) or multiple reasoning steps ( recognizing and 227
then calculating ). Following previous studies (Wang et al., 2023b), we train our model obtained 228
from the ﬁrst-phase of stage-1 on a mixture of data, including an instruction corpus of MultiInstruct, 229
13 publicly available VQA datasets (only using training set), a newly created VQA dataset built 230
through promoting GPT-4V (OpenAI, 2023b) for image-oriented question-answer generation, and 231
the automatically generated 70K CoM corpus. This training results in a generalist VQA model 232
incorporating CoM reasoning. For all existing VQA tasks, we directly prompt CogCoM with given 233
questions and examine the correctness of outputted answers. 234
Type ModelGQA TallyVQA TextVQA ST-VQA
test-balanced simple complex test test
GeneralistFlamingo (Alayrac et al., 2022) - - - 54.1 -
GIT (Wang et al., 2022a) - - - 59.8 -
GI2 (Wang et al., 2022a) - - - 67.3 -
BLIP-2 (Li et al., 2023b) 44.7y- - - 21.7
InstructBLIP (Dai et al., 2023) 49.5y- - - 50.7y
Qwen-VL (Bai et al., 2023) 59.3 - - 63.8 -
CogVLM (Wang et al., 2023b) 65.2 79.8 68.0 69.7 61.0
CogCoM 71.7 84.0 70.1 71.1 70.0
Specialist
SOTAs72.1
(CFR)86.0
( PaLI-X)75.6
(PaLI-X)71.4
(PaLI-X)86.0
(SMoLA)
Table 1: Performance on Visual Question Answering benchmarks, where the results labeled with
yrefer to the few-shot setting. CogCoM achieves SOTA across the board, and demonstrates the
effectiveness on the visual reasoning and scene texts recognition benchmarks.
5.1.1 GQA, TextVQA, ST-VQA, TallyVQA 235
Settings GQA is a compositional VQA benchmark with diverse reasoning questions coming from 236
semantic functional programs. TallyVQA is an objects counting benchmark with human-annotated 237
complex counting questions involving challenging non-zero counterparts. TextVQA and ST-VQA are 238
two texts understanding benchmarks requiring models to answer questions through textual cues on 239
images. We use the ofﬁcial evaluation scripts for GQA and TallyVQA, which calculate the accuracy 240
score by the Exact Matching (EM) between model predictions and answers. For TextVQA and 241
ST-VQA, we submit our model predictions to the ofﬁcial online websites for calculating the accuracy 242
with VQA Score metric (Antol et al., 2015). 243
7Results As the results shown in Table 2, CogCoM achieves the state-of-the-art performance in 244
comparison with all generalist models, and achieves signiﬁcant improvements over the baseline model. 245
Speciﬁcally, compared to the baseline model, our model achieves up to 5.97 and 9.0 percentage 246
points improvement on the benchmarks that requires complex reasoning and detailed recognition, 247
respectively. On GQA and TextVQA, CogCoM also obtains comparable results with the large-scale 248
specialist SOTAs. This result demonstrates the effectiveness of the proposed approach in solving 249
details recognition problem. 250
5.1.2 Experiments for Reasoning Accuracy and Time Complexity 251
Due to the lack of resource, we build CoM-test, a benchmark with evidential reasoning chains on the 252
TextVQA test set based on the proposed data generation pipeline, and also introduce a keypoints- 253
aware metric to validate the correctness of reasoning paths (see Appendix C.3 for detailed statistics). 254
We also evaluate the time complexity for model generation on a held-out benchmark, MM-Vet. 255
0 2k 4k 6k 8k
Training Steps2.157.2720.0030.0040.0055.5960.00Accuracy Score
Zeroshot/answer
CogCoM/answer
CogCoM/explanation
1 2 3 4 5 6 7 8050100150200250300350#T okens141.7157.4190.9234.6262.9275.4292.5346.1
18.024.1 22.842.133.048.971.4189.8CogCoM/T okens
CogVLM/T okens
0.02.55.07.510.012.515.017.520.0
#Time (seconds)
CogCoM/Time
CogVLM/Time
Figure 6: Left: Results on a reasoning testbed CoM-test shows CogCoM achieves satisfactory
performance with only 70K training data and 2K steps. Right : Results on MM-Vet shows that
CogCoM produces comprehensive reasoning content without incurring excessive time overhead.
Reasoning Accuracy To validate the correctness of execution and results of manipulations in 256
reasoning paths, we introduce a keypoints-aware evaluation metric that concentrates on these contents 257
and their order. Concretely, given a predicted chain-answer pair (C0; A0)and the ground truth 258
pair(C; A), we ﬁrst extract the keypoints ( i.e.,the name, parameters, and results of manipulations) 259
inA0; Ato form two lists, and then discretize these two lists into K0andKbased on a bag- 260
of-words composed of all keypoints. Then, we calculate the normalized Levenshtein Distance 261
sK=Levenshtein (K0; K)=Nas the manipulation score. We also compute the BLEU (Papineni 262
et al., 2002) score sC=BLEU (C0; C)as the paragraph score. Finally, a weighted average of these 263
two scores serves as the ultimate reasoning score s acc= (0:6sK+ 0:4sC)=2. 264
We train our ﬁrst-stage model only using the 70K automated CoM data without other supervision 265
for qualitatively evaluate the effectiveness of chains, and the results are shown in the left subplot 266
of Figure 6. We ﬁnd that by training with the CoM chains, our model can swiftly achieve the 267
satisfactory performance of 48.41 accuracy score with 2k training steps, and obtain the optimal result 268
of 55.59 with 8K steps. Additionally, the explanation scores gradually improve along with the model 269
performance, indicating that successful reasoning steps contribute to the achieving of ﬁnal answer. 270
Time Complexity We also evaluate the time complexity and average length of tokens during model 271
reasoning on a held-out test set, MM-Vet. Speciﬁcally, we run CogCoM and the baseline model on 272
all 218 questions, and record the time overhead as well as the average number of outputted tokens 273
(using the Vicuna-7B-v1.5 tokenizer). We divide the 218 samples into 8 intervals based on the time 274
expenditure for each sample and calculate the average values of the time complexity and the number 275
of tokens for each interval, with the results presented in the right subplot of Figure 6. 276
From the results we ﬁnd that compared to baseline model, CogCoM produces information-intensive 277
reasoning content ( e.g., detection boxes, auxiliary lines) without incurring infeasible time overhead. 278
For example, without quantitive optimization, CogCoM outputs 262.9 informative tokens in approxi- 279
mately 9 seconds. With the advantages in long-context optimization techniques (Hooper et al., 2024), 280
we believe that it is crucial for models to produce informative content and accurate responses. 281
85.2 Experiments on Visual Grounding 282
The task of visual grounding requires models to precisely provide the corresponding coordinates 283
of regions in an image based on the given target description. Following the existing work (Wang 284
et al., 2023b), we train our model obtained by the ﬁrst stage on a mixture of datasets, including an 285
instruction corpus MultiInstruct, a high-quality grounded VQA corpus introduced in CogVLM, and 286
the 70K CoM data. This training results in a generalist grounding model that is excelling at visual 287
grounding while capable of reasoning. For all benchmarks, we prompt CogOM in a chat manner to 288
ask the model to provide grounded coordinates, such as “ Where ishexprianswer in [x0,y0,x1,y1] 289
format. ", where thehexprirefers to the target expression. We use the standard metric, that considers 290
a prediction as correct when the intersection-over-union (IoU) between boxes is greater than 0.5. 291
Type ModelRefCOCO RefCOCO+ RefCOCOg
val test-A test-B val test-A test-B val test
GeneralistOFA-L* (Wang et al., 2022b) 79.96 83.67 76.39 68.29 76.00 61.75 67.57 67.58
Shikra-7B (Chen et al., 2023b) 87.01 90.61 80.24 81.60 87.36 72.12 82.27 82.19
Shikra-13B (Chen et al., 2023b) 87.83 91.11 81.81 82.89 87.79 74.41 82.64 83.16
Qwen-VL (Bai et al., 2023) 89.36 92.26 85.34 83.12 88.25 77.21 85.58 85.48
CogVLM (Wang et al., 2023b) 92.51 93.95 88.73 87.52 91.81 81.43 89.46 90.09
CogCoM 92.34 94.57 89.15 88.19 92.80 82.08 89.32 90.45
Specialist
SOTAs92.64
(UNINEXT)94.33
(UNINEXT)91.46
(UNINEXT)88.77
(ONE-PEACE)92.21
(ONE-PEACE)83.23
(ONE-PEACE)89.22
(ONE-PEACE)89.37
(UNINEXT-H)
Table 2: Results on VG benchmarks, where the specialist SOTAs are quoted from (Bai et al., 2023).
Results As shown in Figure 2, CogCoM achieves the best performance in 6 out of all 8 sub-sets. 292
Based on the training with a mixture of broad capabilities, this result indicates that our model exhibits 293
a superior grounding abilities while offers potential to solve a variety of tasks. 294
5.3 Experiments on General Multimodal Evaluation and Hallucination Examination 295
We further examine the general multimodal capabilities, and the hallucination issue. We use the 296
generalist VQA model and obtain model predictions by directly asking the original questions in 297
benchmarks. We use the challenging adversarial version and ofﬁcial evaluation scripts for POPE. 298
Method LLM MM-Vet POPE adv
InstructBLIP (Dai et al., 2023) Vicuna-13B 25.6 77.3
LLaV A (Liu et al., 2023b) LLaMA2-7B 28.1 66.3
DreamLLM (Dong et al., 2023) Vicuna-7B 35.9 76.5
LLaV A-1.5 (Liu et al., 2023a) Vicuna-13B 36.3 84.5
CogVLM (Wang et al., 2023b) Vicuna-7B 45.5y87.2
CogCoM Vicuna-7B 46.1 87.8
Table 3: Evaluation results on the general and hallucination assessment benchmarks.
Results As shown in Table 3, we can see that CogCoM improves the performance by 0.6 points 299
compared to the baseline model on MM-Vet, and achieves the superior performance on POPE which 300
is in consistent with the baseline model. This result suggests that out model maintains superior 301
reasoning capabilities while preserving effectiveness in general multimodal tasks, and simultaneously 302
exhibits lower hallucination. 303
6 Conclusion 304
This paper studies the problems presented by the conclusive alignment training of VLMs, and 305
proposes a mechanism, Chain of Manipulations (CoM), that enables VLMs to solve problems step- 306
by-step by actively manipulating visual inputs as evidence. We realize this methodology by proposing 307
(1) a ﬂexible data structure, (2) an efﬁcient data generation framework capable of producing abundant 308
samples, (3) a memory-based architecture compatible with existing VLMs, and (4) a training process 309
for versatile capabilities. We also annotate 6K graphical math samples with reasoning chains to 310
facilitate the advancement of VLMs in solving mathematical problems. Experiments on 9 public 311
benchmarks show that our trained 17B general VLM can produce informative reasoning content 312
while achieving superior performance on diverse multimodal problems. 313
9References 314
Acharya, M., Kaﬂe, K., and Kanan, C. Tallyqa: Answering complex counting questions. In 315
Proceedings of the AAAI conference on artiﬁcial intelligence , 2019. 316
Alayrac, J.-B., Donahue, J., Luc, P., Miech, A., Barr, I., Hasson, Y ., Lenc, K., Mensch, A., Millican, 317
K., Reynolds, M., et al. Flamingo: a visual language model for few-shot learning. Advances in 318
Neural Information Processing Systems , 2022. 319
Antol, S., Agrawal, A., Lu, J., Mitchell, M., Batra, D., Zitnick, C. L., and Parikh, D. Vqa: Visual 320
question answering. In Proceedings of the IEEE international conference on computer vision , 321
2015. 322
Awadalla, A., Gao, I., Gardner, J., Hessel, J., Hanafy, Y ., Zhu, W., Marathe, K., Bitton, Y ., Gadre, 323
S., Sagawa, S., et al. Openﬂamingo: An open-source framework for training large autoregressive 324
vision-language models. arXiv preprint , 2023. 325
Bai, J., Bai, S., Yang, S., Wang, S., Tan, S., Wang, P., Lin, J., Zhou, C., and Zhou, J. Qwen-vl: A 326
frontier large vision-language model with versatile abilities. arXiv preprint , 2023. 327
Biten, A. F., Tito, R., Maﬂa, A., Gomez, L., Rusinol, M., Valveny, E., Jawahar, C., and Karatzas, D. 328
Scene text visual question answering. In Proceedings of the IEEE/CVF international conference 329
on computer vision , 2019. 330
Byeon, M., Park, B., Kim, H., Lee, S., Baek, W., and Kim, S. Coyo-700m: Image-text pair dataset, 331
2022. 332
Changpinyo, S., Sharma, P., Ding, N., and Soricut, R. Conceptual 12m: Pushing web-scale image-text 333
pre-training to recognize long-tail visual concepts. In Proceedings of the IEEE/CVF Conference 334
on Computer Vision and Pattern Recognition , 2021. 335
Chen, D., Liu, J., Dai, W., and Wang, B. Visual instruction tuning with polite ﬂamingo. arXiv 336
preprint arXiv:2307.01003 , 2023a. 337
Chen, K., Zhang, Z., Zeng, W., Zhang, R., Zhu, F., and Zhao, R. Shikra: Unleashing multimodal 338
llm’s referential dialogue magic. arXiv preprint , 2023b. 339
Chen, L., Li, J., Dong, X., Zhang, P., He, C., Wang, J., Zhao, F., and Lin, D. Sharegpt4v: Improving 340
large multi-modal models with better captions. arXiv preprint arXiv:2311.12793 , 2023c. 341
Chen, X., Wang, X., Changpinyo, S., Piergiovanni, A., Padlewski, P., Salz, D., Goodman, S., Grycner, 342
A., Mustafa, B., Beyer, L., et al. Pali: A jointly-scaled multilingual language-image model. In The 343
Eleventh International Conference on Learning Representations , 2022. 344
Chen, X., Djolonga, J., Padlewski, P., Mustafa, B., Changpinyo, S., Wu, J., Ruiz, C. R., Goodman, S., 345
Wang, X., Tay, Y ., et al. Pali-x: On scaling up a multilingual vision and language model. arXiv 346
preprint , 2023d. 347
Chiang, W.-L., Li, Z., Lin, Z., Sheng, Y ., Wu, Z., Zhang, H., Zheng, L., Zhuang, S., Zhuang, Y ., 348
Gonzalez, J. E., et al. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality. 349
See https://vicuna. lmsys. org (accessed 14 April 2023) , 2023. 350
Dai, W., Li, J., Li, D., Tiong, A., Zhao, J., Wang, W., Li, B., Fung, P., and Hoi, S. Instructblip: 351
Towards general-purpose vision-language models with instruction tuning. arxiv 2023. arXiv 352
preprint arXiv:2305.06500 . 353
Dai, W., Li, J., Li, D., Tiong, A., Zhao, J., Wang, W., Li, B., Fung, P., and Hoi, S. Instructblip: 354
Towards general-purpose vision-language models with instruction tuning. arxiv 2023. arXiv 355
preprint , 2023. 356
Dong, R., Han, C., Peng, Y ., Qi, Z., Ge, Z., Yang, J., Zhao, L., Sun, J., Zhou, H., Wei, H., et al. 357
Dreamllm: Synergistic multimodal comprehension and creation. arXiv preprint arXiv:2309.11499 , 358
2023. 359
10Du, Y ., Li, C., Guo, R., Yin, X., Liu, W., Zhou, J., Bai, Y ., Yu, Z., Yang, Y ., Dang, Q., et al. Pp-ocr: 360
A practical ultra lightweight ocr system. arXiv preprint arXiv:2009.09941 , 2020. 361
Hong, W., Wang, W., Lv, Q., Xu, J., Yu, W., Ji, J., Wang, Y ., Wang, Z., Dong, Y ., Ding, M., et al. 362
Cogagent: A visual language model for gui agents. arXiv preprint arXiv:2312.08914 , 2023. 363
Hooper, C., Kim, S., Mohammadzadeh, H., Mahoney, M. W., Shao, Y . S., Keutzer, K., and Gholami, 364
A. Kvquant: Towards 10 million context length llm inference with kv cache quantization. arXiv 365
preprint arXiv:2401.18079 , 2024. 366
Huang, K.-H., Zhou, M., Chan, H. P., Fung, Y . R., Wang, Z., Zhang, L., Chang, S.-F., and Ji, H. 367
Do lvlms understand charts? analyzing and correcting factual errors in chart captioning. arXiv 368
preprint arXiv:2312.10160 , 2023a. 369
Huang, S., Dong, L., Wang, W., Hao, Y ., Singhal, S., Ma, S., Lv, T., Cui, L., Mohammed, O. K., Liu, 370
Q., et al. Language is not all you need: Aligning perception with language models. arXiv preprint , 371
2023b. 372
Hudson, D. A. and Manning, C. D. Gqa: A new dataset for real-world visual reasoning and 373
compositional question answering. In Proceedings of the IEEE/CVF conference on computer 374
vision and pattern recognition , 2019. 375
Hwang, A., Head, A., and Callison-Burch, C. Grounded intuition of gpt-vision’s abilities with 376
scientiﬁc images. arXiv preprint , 2023. 377
Jia, C., Yang, Y ., Xia, Y ., Chen, Y .-T., Parekh, Z., Pham, H., Le, Q., Sung, Y .-H., Li, Z., and Duerig, 378
T. Scaling up visual and vision-language representation learning with noisy text supervision. In 379
International conference on machine learning , 2021. 380
Keys, R. Cubic convolution interpolation for digital image processing. IEEE transactions on 381
acoustics, speech, and signal processing , 1981. 382
Krishna, R., Zhu, Y ., Groth, O., Johnson, J., Hata, K., Kravitz, J., Chen, S., Kalantidis, Y ., Li, L.-J., 383
Shamma, D. A., et al. Visual genome: Connecting language and vision using crowdsourced dense 384
image annotations. International journal of computer vision , 2017. 385
Li, B., Zhang, Y ., Chen, L., Wang, J., Yang, J., and Liu, Z. Otter: A multi-modal model with 386
in-context instruction tuning. arXiv preprint arXiv:2305.03726 , 2023a. 387
Li, J., Li, D., Savarese, S., and Hoi, S. Blip-2: Bootstrapping language-image pre-training with 388
frozen image encoders and large language models. arXiv preprint , 2023b. 389
Li, Y ., Du, Y ., Zhou, K., Wang, J., Zhao, W. X., and Wen, J.-R. Evaluating object hallucination in 390
large vision-language models. arXiv preprint arXiv:2305.10355 , 2023c. 391
Li, Y ., Zhang, C., Yu, G., Wang, Z., Fu, B., Lin, G., Shen, C., Chen, L., and Wei, Y . Sta- 392
blellava: Enhanced visual instruction tuning with synthesized image-dialogue data. arXiv preprint 393
arXiv:2308.10253 , 2023d. 394
Lin, T.-Y ., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Dollár, P., and Zitnick, C. L. 395
Microsoft coco: Common objects in context. In Computer Vision–ECCV 2014: 13th European 396
Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13 , 2014. 397
Liu, H., Li, C., Li, Y ., and Lee, Y . J. Improved baselines with visual instruction tuning. arXiv preprint 398
arXiv:2310.03744 , 2023a. 399
Liu, H., Li, C., Wu, Q., and Lee, Y . J. Visual instruction tuning. arXiv preprint , 2023b. 400
Liu, S., Zeng, Z., Ren, T., Li, F., Zhang, H., Yang, J., Li, C., Yang, J., Su, H., Zhu, J., et al. Grounding 401
dino: Marrying dino with grounded pre-training for open-set object detection. arXiv preprint 402
arXiv:2303.05499 , 2023c. 403
Lu, P., Bansal, H., Xia, T., Liu, J., Li, C., Hajishirzi, H., Cheng, H., Chang, K.-W., Galley, M., and 404
Gao, J. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. 405
arXiv preprint arXiv:2310.02255 , 2023. 406
11Mao, J., Huang, J., Toshev, A., Camburu, O., Yuille, A. L., and Murphy, K. Generation and 407
comprehension of unambiguous object descriptions. In Proceedings of the IEEE conference on 408
computer vision and pattern recognition , 2016. 409
Masry, A., Long, D. X., Tan, J. Q., Joty, S., and Hoque, E. Chartqa: A benchmark for question 410
answering about charts with visual and logical reasoning. arXiv preprint arXiv:2203.10244 , 2022. 411
OpenAI, R. Gpt-4 technical report. arxiv 2303.08774. View in Article , 2023a. 412
OpenAI, R. Gpt-4v(ision) system card. Citekey: gptvision. , 2023b. 413
Ordonez, V ., Kulkarni, G., and Berg, T. Im2text: Describing images using 1 million captioned 414
photographs. Advances in neural information processing systems , 2011. 415
Papineni, K., Roukos, S., Ward, T., and Zhu, W.-J. Bleu: a method for automatic evaluation of machine 416
translation. In Proceedings of the 40th annual meeting of the Association for Computational 417
Linguistics , 2002. 418
Peng, Z., Wang, W., Dong, L., Hao, Y ., Huang, S., Ma, S., and Wei, F. Kosmos-2: Grounding 419
multimodal large language models to the world. arXiv preprint , 2023. 420
Schuhmann, C., Vencu, R., Beaumont, R., Kaczmarczyk, R., Mullis, C., Katta, A., Coombes, T., 421
Jitsev, J., and Komatsuzaki, A. Laion-400m: Open dataset of clip-ﬁltered 400 million image-text 422
pairs. arXiv preprint arXiv:2111.02114 , 2021. 423
Schuhmann, C., Beaumont, R., Vencu, R., Gordon, C., Wightman, R., Cherti, M., Coombes, T., 424
Katta, A., Mullis, C., Wortsman, M., et al. Laion-5b: An open large-scale dataset for training next 425
generation image-text models. Advances in Neural Information Processing Systems , 2022. 426
Sharma, P., Ding, N., Goodman, S., and Soricut, R. Conceptual captions: A cleaned, hypernymed, 427
image alt-text dataset for automatic image captioning. In Proceedings of the 56th Annual Meeting 428
of the Association for Computational Linguistics (Volume 1: Long Papers) , 2018. 429
Shazeer, N. Glu variants improve transformer. arXiv preprint arXiv:2002.05202 , 2020. 430
Shrestha, R., Kaﬂe, K., and Kanan, C. Answer them all! toward universal visual question answering 431
models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , 432
2019. 433
Singh, A., Natarajan, V ., Shah, M., Jiang, Y ., Chen, X., Batra, D., Parikh, D., and Rohrbach, M. 434
Towards vqa models that can read. In Proceedings of the IEEE/CVF conference on computer vision 435
and pattern recognition , 2019. 436
Sun, Q., Fang, Y ., Wu, L., Wang, X., and Cao, Y . Eva-clip: Improved training techniques for clip at 437
scale. arXiv preprint arXiv:2303.15389 , 2023a. 438
Sun, Q., Yu, Q., Cui, Y ., Zhang, F., Zhang, X., Wang, Y ., Gao, H., Liu, J., Huang, T., and Wang, X. 439
Generative pretraining in multimodality. arXiv preprint , 2023b. 440
Trinh, T. H., Wu, Y ., Le, Q. V ., He, H., and Luong, T. Solving olympiad geometry without human 441
demonstrations. Nature , 625(7995):476–482, 2024. 442
Wang, J., Yang, Z., Hu, X., Li, L., Lin, K., Gan, Z., Liu, Z., Liu, C., and Wang, L. Git: A generative 443
image-to-text transformer for vision and language. arXiv preprint arXiv:2205.14100 , 2022a. 444
Wang, P., Yang, A., Men, R., Lin, J., Bai, S., Li, Z., Ma, J., Zhou, C., Zhou, J., and Yang, H. Ofa: 445
Unifying architectures, tasks, and modalities through a simple sequence-to-sequence learning 446
framework. 2022b. 447
Wang, W., Chen, Z., Chen, X., Wu, J., Zhu, X., Zeng, G., Luo, P., Lu, T., Zhou, J., Qiao, Y ., et al. 448
Visionllm: Large language model is also an open-ended decoder for vision-centric tasks. arXiv 449
preprint , 2023a. 450
Wang, W., Lv, Q., Yu, W., Hong, W., Qi, J., Wang, Y ., Ji, J., Yang, Z., Zhao, L., Song, X., et al. 451
Cogvlm: Visual expert for pretrained language models. arXiv preprint , 2023b. 452
12Wang, Y ., Kordi, Y ., Mishra, S., Liu, A., Smith, N. A., Khashabi, D., and Hajishirzi, H. Self-instruct: 453
Aligning language model with self generated instructions. arXiv preprint arXiv:2212.10560 , 454
2022c. 455
Wu, P. and Xie, S. V*: Guided visual search as a core mechanism in multimodal llms. arXiv preprint 456
arXiv:2312.14135 , 2023. 457
Xu, Z., Shen, Y ., and Huang, L. Multiinstruct: Improving multi-modal zero-shot learning via 458
instruction tuning. arXiv preprint arXiv:2212.10773 , 2022. 459
Yin, S., Fu, C., Zhao, S., Xu, T., Wang, H., Sui, D., Shen, Y ., Li, K., Sun, X., and Chen, E. 460
Woodpecker: Hallucination correction for multimodal large language models. arXiv preprint 461
arXiv:2310.16045 , 2023. 462
Yu, L., Poirson, P., Yang, S., Berg, A. C., and Berg, T. L. Modeling context in referring expressions. In 463
Computer Vision–ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 464
11-14, 2016, Proceedings, Part II 14 , 2016. 465
Yu, Q., Li, J., Wei, L., Pang, L., Ye, W., Qin, B., Tang, S., Tian, Q., and Zhuang, Y . Hallucidoctor: 466
Mitigating hallucinatory toxicity in visual instruction data. arXiv preprint arXiv:2311.13614 , 467
2023a. 468
Yu, W., Yang, Z., Li, L., Wang, J., Lin, K., Liu, Z., Wang, X., and Wang, L. Mm-vet: Evaluating 469
large multimodal models for integrated capabilities. arXiv preprint arXiv:2308.02490 , 2023b. 470
Zeng, Y ., Zhang, H., Zheng, J., Xia, J., Wei, G., Wei, Y ., Zhang, Y ., and Kong, T. What matters in 471
training a gpt4-style language model with multimodal inputs? arXiv preprint arXiv:2307.02469 , 472
2023. 473
Zhang, S., Sun, P., Chen, S., Xiao, M., Shao, W., Zhang, W., Chen, K., and Luo, P. Gpt4roi: 474
Instruction tuning large language model on region-of-interest. arXiv preprint arXiv:2307.03601 , 475
2023a. 476
Zhang, Y ., Zhang, R., Gu, J., Zhou, Y ., Lipka, N., Yang, D., and Sun, T. Llavar: Enhanced visual 477
instruction tuning for text-rich image understanding. arXiv preprint , 2023b. 478
13A Related Works 479
A.1 Large Vision-Langauge Models as Foundations 480
Most of LVLMs rely on the training on publicly available image-caption pairs, including ALIGN (Jia 481
et al., 2021), MSCOCO (Lin et al., 2014), VG Krishna et al. (2017), CC3M Sharma et al. (2018), 482
CC12M (Changpinyo et al., 2021), SBU (Ordonez et al., 2011), LAION2B (Schuhmann et al., 2022), 483
LAION400M Schuhmann et al. (2021). Starting from Flamingo (Alayrac et al., 2022), a series of 484
LVLMs have focused on training the adaptation layers to align the visual representation to the frozen 485
LLMs on a mixture of image-text pairs with the above corpus, including BLIP2 Li et al. (2023b), 486
KOSMOS Huang et al. (2023b), and OpenFlamingo (Awadalla et al., 2023). Inspired by success of 487
instruction tuning in LLMs (Wang et al., 2022c), a line of works have devoted efforts to build vision- 488
oriented instruction-answer pairs through GPT4 and train models for imitation, such as LLA V A (Liu 489
et al., 2023b), Otter (Li et al., 2023a), VisionLLM (Wang et al., 2023a), MultiInstruct (Xu et al., 490
2022), Lynx (Zeng et al., 2023), InstructBLIP (Dai et al.), CleverFlamingo (Chen et al., 2023a) and 491
StableLLaV A (Li et al., 2023d). Recently, researchers have proven the efﬁciency of developing 492
LVLMs with two stages of training, the ﬁrst stage of abundant pretraining on image-caption pairs and 493
the second stage of alignment on image-question-answer triples, such as PALI (Chen et al., 2022), 494
PaLI-X (Chen et al., 2023d), Qwen-VL (Bai et al., 2023), and CogVLM Wang et al. (2023b). 495
A.2 Large Vision-Language Models with Reasoning 496
To further enhance the ability of LVLMs in solving high-level visual problems, research focusing 497
on various aspects of reasoning is attracting broad attention. We simply divide existing studies into 498
tree broad categories. The ﬁrst line of research focus on enhance train models with a mastery of 499
cross-modal grounded reasoning, where grounded instruction-following supervision is build through 500
public visual grounding dataset or GPT4-V for training, including KOSMOS-2 (Peng et al., 2023), 501
Shikra (Chen et al., 2023b), and GPT4ROI (Zhang et al., 2023a). The second aspect of efforts have 502
been devoted into promoting models to understand artiﬁcial visual scenes, such as ﬁgures, charts, and 503
receipts. These studies includes CogAgent (Hong et al., 2023) and CHARTVE (Huang et al., 2023a). 504
Some other studies address the crucial problem of hallucination in LVLMs with counterfactual or 505
interpretable reasoning (Yu et al., 2023a; Yin et al., 2023). V* (Wu & Xie, 2023) also contributes 506
efforts to enhance the details recognition of VLMs based the LLM-guided searching process. 507
B Limitation and Impact 508
Though we try to develop an accurate and robust framework that engages remarkable LLM to provide 509
basic solving steps, adopts reliable visual tools to obtain visual contents, and then acquires feasible 510
paths based on traversal, there are still limitations in our methodology that we hope to improve in the 511
future. First, We ﬁnd that the diversity of linguistic solving steps is insufﬁcient, and the inaccuracy of 512
visual tools ( e.g., the rough granularity of grounding boxes, OCR failures on slant letters) will lead 513
to a large amount of negative paths (effectively utilizing these paths would beneﬁcial). We suggest 514
to promote these limitations with dedicate prompts and improved visual tools. Second, our current 515
model re-input the manipulated images with a set of hard prompts, which may bring speed losses. 516
This is expected to be improved by implementing the physical manipuations into the calculations in 517
vector space. This work presents a general visual reasoning mechanism that alleviate the problems 518
caused by existing conclusion-alignment training for VLMs, introduces a data production framework 519
involving LLMs and visual tools as reliable annotators, and devises a memory-based compatible VLM 520
architecture. We expect this work to bring three beneﬁts to the community. First, the proposed visual 521
reasoning mechanism may push the progress of VLMs in solving complex visual problems. Second, 522
the introduced data production framework may be applied to widespread training scenarios to promote 523
the development of current data-driven machine learning. Third, we hope that the memory-based 524
architecture will be helpful for VLMs in multi-turn long contexts. 525
14C Details of Data Production 526
In this section, we further introduce the details of CoM data production, with the overall algorithm of 527
a pseudo code, an example of the solving steps generation with LLM and corresponding guideline, an 528
example of the reasoning chains completion with visual tools. We also list the details of data statistics 529
for the synthesised training data as well as the evaluation data of CoM-test, followed by a limitation 530
analysis for the current data production method. 531
C.1 Algorithm for the Automate Data Generation Pipeline 532
We provide the pseudocode of the CoM synthesis algorithm to clearly explain the process of data 533
generation, thereby facilitating understanding and reproduction 1. 534
Algorithm 1 Synthesising Chain of Manipulations
1:Deﬁne:8
<
:Manipulations :ffi:x!yjfi2Mg
Linguistic Annotator : 	L==We use GPT 4in this work
V isual Annotator : 	V==We use PaddleOCR and GroundingDINO in this work
2:Input: Image I, Question Q, Answer A
3:// Linguistic Annotation
4:Prompt 	Lwith guideline PLto generate reasoning steps:
&= 	L(QjPL); where&= (steps 1; steps 2; :::)
steps i= (fi; desc i)(3)
5:Deﬁne treeT
6:fori= 1toj&jdo
7: Extract xi; yiinstantiated with fiinstepi
8: Extract referential boxes Bfromxi
9: forbinBdo
10: Leverage 	Vto acquire corresponding visual content y0
i= 	(xijI; b), and apply yito
tree
T:level [i]:append (yi) (4)
11: end for
12:end for
13:TraverseTto obtain positive chains that leads to given answer with terminal return
[&1; &2; :::] =DFS (TjA) (5)
14:Return [&1; &2; :::]
C.2 The CoM-test Benchmark and Evaluation Metric 535
To measure the correctness of CoM chains, we introduce a keypoints-aware metric . The intuition 536
is that we care about the key elements including actions ( i.e.,manipulation name), targets ( i.e., 537
manipulation input), and visual contents ( i.e.,manipulation returns) of each step in the path, as well 538
as the logical execution order of manipulations. Given a pair of chain-answer annotation (c; a)and 539
corresponding model prediction (c0; a0), we ﬁrst sequentially extract the key elements from candc0540
to construct two ordered lists, and then replace the elements in the lists with their ﬁxed indices in a 541
Bag-of-ElementsE=c[c0to result in lists of kandk0. We thus calculate the score as the normalized 542
Levenshtein Distance sc=Levenshtein (k; k0)=Nbetween the two lists, where Nis the maximum 543
length between kandk0. We adopt this simple discretization strategy with low time complexity 544
to concentrate on the key points as well as the solving order. We further consider the linguistic 545
matching of paragraphs by calculating the BLEU (Papineni et al., 2002) score between two chains 546
sp=BLEU (c; c0), and the ﬁnal sore is a weighted combination as acc= (0:6sc+ 0:4sp)=2. 547
15C.3 Data Statistics 548
We develop a strategy to extract predicate phrases based constituency parsing with StandordCoreNLP, 549
in which we extract verb, conjunction-connected verb phrase, preposition-connected verb phrase. 550
Besides the standard CoM data incorporating manipulations with explicit visual evidences, the 551
proposed data synthesising framework is compatible of producing implicit visual reasoning steps 552
step0
i= (desc i)without involving the manipulations. We thereby also build this partial CoM data on 553
the corpus consisting of absurd visual questions ( i.e.,asking unanswerable questions based on the 554
given image) to further resist the toxic hallucinations. Speciﬁcally, given an image Iwith a question 555
Q,we prompt GPT-4V (OpenAI, 2023b) to solve the question step-by-step to acquire the reasoning 556
chains. 557
Data Source #QAs #Chains #Steps/Chain #Manipulations Types/Chain
TextVQA (Biten et al., 2019) 10782 13766 2.93 2.41
ST-VQA (Singh et al., 2019) 4814 3959 2.88 2.43
TDIUC-count (Shrestha et al., 2019) 53547 54523 2.35 0.74
TDIUC-absurd (Shrestha et al., 2019) 11677 11677 4.09 -
CoM-test 4609 8612 3.26 2.18
Table 4: Detailed statistics the the training data and evaluation data synthesised with CoM production.
locatereadidentifydeterminelook forobserveanalyzeexaminecountzoom incomparefind
focus onusezoomextractinterpretrecognizesearch forzoom onnotelook atresearchunderstandprovidemakelistcheckenhanceconductmeasurecheck formatchanswerconsultconvertconfirmdescribeinspectcalculatelookopenstaterefer
note downconsidergatherestimatetranslaterecord0.000.010.020.030.040.050.06
Figure 7: Distribution of the top-50 generated manipulations out of total 465based on 4-shot
prompting, where the ﬁrst three bars are scaled with 20% for a smooth visualization of all data.
16You are a visual assistant capable of generating solving steps for image-oriented visual questions. In each step, you can optionally use a manipulation to operate the image, which can be used to acquire speciﬁc information from the image or to acquire the processed new image (please be aware that these manipulations will not actually be performed when you generate the solving steps). The manipulation can be one from the predeﬁned ones, or can be a new one you create yourself (should there indeed be a need), where the predeﬁned manipulations with their descriptions are listed below:\n\n{MANIPULATIONS}.\n\nGiven a question Q abount an image, please generate a series of essential solving steps, where the output of each step is a tuple consisting of a Manipulation (leave it to None if the current step doesn't involve any manipulation) and a Description: (1) Manipulation f(x)->y, that is the manipulation `f` targeting `x` to obtain speciﬁc information or image `y`; (2) Description, which is a sentence describing the current solving step.\n Please adhere the following format: given an input of 'Q: xxx. The essential solving steps are: ', the output should like 'Step 1: (Manipulation, Description); Step 2: (Manipulation, Description); ...'.\n\n There are several examples:\n {DEMONSTRATIONS}\n\n Q: {QUESTION} The essential solving steps are:REQUIREMENTS (PROMPT)MANIPULATIONSi-th calculate manipulation, that calculate the formula speciﬁed by the target `tgt` in current image, and return the calculation result `res_i`.grounding_i(tgt)->bbx_i: i-th crop_and_zoomin manipulation which is useful to identify small and subtle objects in image, that ﬁrst crops the current image using the box `bbx` deﬁned by the top-left and bottom-right coordinates, and then zoom in the cropped image by two times and ﬁnally return the resulting image `img_i`.crop_and_zoomin_i(bbx)->img_i: i-th grounding manipulation, that locates the object(s) speciﬁed by the target noun phrase `tgt` in current image, and return the resulting bounding box(es) as `bbx_i` where each box is represented by the top-left and bottom-right coordinates.OCR_i(tgt)->txt_i:i-th OCR manipulation, that recognize the natural texts written on the target `tgt`, and return the recognized texts `txt_i`.calculate(tgt)->res_i:
Question: What number police station is on the building?Step 1: grounding_1(the building)->bbx_1,Identify the number of the police station on the building in box `bbx_1` and return the bounding box of the number as `bbx_2`.Recognize the number in the region `bbx_2` and return the recognized number as `txt_1`.Question: What number police station is on the building?Answer: 43
Locate the building in the image and return the bounding box of the building as `bbx_1`.Identify the number of the police station on the building in box `bbx_1` and return the bounding box of the number as `bbx_2`.Recognize the number in the region `bbx_2` and return the recognized number as `txt_1`.
<latexit sha1_base64="Eeipg0SaSzO/bMIo2fhu3xEhWXY=">AAACyXicjVHLSsNAFD2Nr1pfVZdugkVwVRIRdVl0owhSsWkLtUgyndbYvEwmYi2u/AG3+mPiH+hfeGdMQS2iE5KcOfeeM3PvdSLPTYRhvOa0icmp6Zn8bGFufmFxqbi8Uk/CNGbcYqEXxk3HTrjnBtwSrvB4M4q57Tsebzj9Axlv3PA4ccOgJgYRb/t2L3C7LrMFUfWjk6pVO7soloyyoZY+DswMlJCtalh8wTk6CMGQwgdHAEHYg42EnhZMGIiIa2NIXEzIVXGOexRIm1IWpwyb2D59e7RrZWxAe+mZKDWjUzx6Y1Lq2CBNSHkxYXmaruKpcpbsb95D5SnvNqC/k3n5xApcEvuXbpT5X52sRaCLPVWDSzVFipHVscwlVV2RN9e/VCXIISJO4g7FY8JMKUd91pUmUbXL3toq/qYyJSv3LMtN8S5vSQM2f45zHNS3yuZO2TzdLlX2s1HnsYZ1bNI8d1HBIaqwyPsKj3jCs3asXWu32t1nqpbLNKv4trSHD4zXkSA=</latexit>INPUTS
<latexit sha1_base64="z5pJ6KZLmsSkixjimeA0//1F+qY=">AAACzHicjVHLSsNAFD2Nr1pfVZdugkVwVRIRdVl040K0YmMrWkqSTuvQvEgmQind+gNu9bvEP9C/8M44glpEJyQ5c+49Z+be6yUBz4RlvRSMqemZ2bnifGlhcWl5pby6dpnFeeozx4+DOG15bsYCHjFHcBGwVpIyN/QC1vQGRzLevGNpxuOoIYYJa4duP+I97ruCqKszp1F3Ghedk065YlUttcxJYGtQgV71uPyMG3QRw0eOEAwRBOEALjJ6rmHDQkJcGyPiUkJcxRnGKJE2pyxGGS6xA/r2aXet2Yj20jNTap9OCehNSWliizQx5aWE5WmmiufKWbK/eY+Up7zbkP6e9gqJFbgl9i/dZ+Z/dbIWgR4OVA2cakoUI6vztUuuuiJvbn6pSpBDQpzEXYqnhH2l/OyzqTSZql321lXxV5UpWbn3dW6ON3lLGrD9c5yT4HKnau9V7fPdSu1Qj7qIDWxim+a5jxqOUYdD3iEe8Ign49QQxsgYf6QaBa1Zx7dl3L8Dgf2SSg==</latexit>OUTPUTSL<latexit sha1_base64="z5pJ6KZLmsSkixjimeA0//1F+qY=">AAACzHicjVHLSsNAFD2Nr1pfVZdugkVwVRIRdVl040K0YmMrWkqSTuvQvEgmQind+gNu9bvEP9C/8M44glpEJyQ5c+49Z+be6yUBz4RlvRSMqemZ2bnifGlhcWl5pby6dpnFeeozx4+DOG15bsYCHjFHcBGwVpIyN/QC1vQGRzLevGNpxuOoIYYJa4duP+I97ruCqKszp1F3Ghedk065YlUttcxJYGtQgV71uPyMG3QRw0eOEAwRBOEALjJ6rmHDQkJcGyPiUkJcxRnGKJE2pyxGGS6xA/r2aXet2Yj20jNTap9OCehNSWliizQx5aWE5WmmiufKWbK/eY+Up7zbkP6e9gqJFbgl9i/dZ+Z/dbIWgR4OVA2cakoUI6vztUuuuiJvbn6pSpBDQpzEXYqnhH2l/OyzqTSZql321lXxV5UpWbn3dW6ON3lLGrD9c5yT4HKnau9V7fPdSu1Qj7qIDWxim+a5jxqOUYdD3iEe8Ign49QQxsgYf6QaBa1Zx7dl3L8Dgf2SSg==</latexit>OUTPUTSL
<latexit sha1_base64="gNwd0WkqYb/+16EBVpkoNrWmwYE=">AAACz3icjVHLTsJAFD3UF+ILdemmkZi4Iq0x6pLoxoWJECmQACHtMGBDaZt2qiEE49YfcKt/ZfwD/QvvjCVRidFp2p45954zc+91Qs+NhWG8ZrS5+YXFpexybmV1bX0jv7lVi4MkYtxigRdEDceOuef63BKu8HgjjLg9dDxedwZnMl6/4VHsBn5VjELeHtp93+25zBZEtS6tatmqXnXGtYtJJ18wioZa+iwwU1BAuspB/gUtdBGAIcEQHD4EYQ82YnqaMGEgJK6NMXERIVfFOSbIkTahLE4ZNrED+vZp10xZn/bSM1ZqRqd49Eak1LFHmoDyIsLyNF3FE+Us2d+8x8pT3m1Efyf1GhIrcE3sX7pp5n91shaBHk5UDS7VFCpGVsdSl0R1Rd5c/1KVIIeQOIm7FI8IM6Wc9llXmljVLntrq/ibypSs3LM0N8G7vCUN2Pw5zllQOyiaR0WzclgonaajzmIHu9ineR6jhHOUYZF3iEc84VmraLfanXb/maplUs02vi3t4QMXQZO2</latexit>OUTPUTSVL
<latexit sha1_base64="Eeipg0SaSzO/bMIo2fhu3xEhWXY=">AAACyXicjVHLSsNAFD2Nr1pfVZdugkVwVRIRdVl0owhSsWkLtUgyndbYvEwmYi2u/AG3+mPiH+hfeGdMQS2iE5KcOfeeM3PvdSLPTYRhvOa0icmp6Zn8bGFufmFxqbi8Uk/CNGbcYqEXxk3HTrjnBtwSrvB4M4q57Tsebzj9Axlv3PA4ccOgJgYRb/t2L3C7LrMFUfWjk6pVO7soloyyoZY+DswMlJCtalh8wTk6CMGQwgdHAEHYg42EnhZMGIiIa2NIXEzIVXGOexRIm1IWpwyb2D59e7RrZWxAe+mZKDWjUzx6Y1Lq2CBNSHkxYXmaruKpcpbsb95D5SnvNqC/k3n5xApcEvuXbpT5X52sRaCLPVWDSzVFipHVscwlVV2RN9e/VCXIISJO4g7FY8JMKUd91pUmUbXL3toq/qYyJSv3LMtN8S5vSQM2f45zHNS3yuZO2TzdLlX2s1HnsYZ1bNI8d1HBIaqwyPsKj3jCs3asXWu32t1nqpbLNKv4trSHD4zXkSA=</latexit>INPUTSLocate the building in the image and return the bounding box of the building as `bbx_1`.Step 2: (grounding_2(number police station on               the building in box `bbx_1`)->bbx_2,Step 3: (OCR_1(number in region `bbx_2`)               ->txt_1,LinguisticAnnotation
bbx_1bbx_1bbx_2txt_1bbx_1bbx_2VisualAnnotationManipulations Deﬁnition and Linguistic Annotation Guideline
An Example to show the linguistic annotation results and Visual annotation resultsFigure 8: An example shows the conﬁguration, inputs, outputs of the linguistic annotation and visual
annotation.
C.4 Details of the Linguistic/Visual Annotations 558
In this work, we adopt the GPT4-turbo as the linguistic annotator for generating problems solving 559
steps, and the API call was conducted during the period of 2023.9 - 2023.12. For the visual annotators, 560
we leverage the the currently best-performing tools, GroundingDINO and PaddleOCR, to acquire all 561
visual contents requested by the manipulations. For a clear description to the production setting and 562
results, we illustrate the guiding prompt, and an example-based linguistic annotation results as well 563
as the visual annotation results in Figure 8. 564
C.5 Limitation Analysis for the Data Production 565
For the implemented data framework, we engage the remarkable LLM to provide basic solving steps, 566
adopt two reliable visual tools ( i.e.,GroundingDINO and PaddleOCR) to acquire corresponding 567
visual contents, and then perform the traversal to achieve feasible reasoning paths, which ensures the 568
correctness and robustness of data synthesizing. However, we also ﬁnd that there are three major 569
limitations caused by the employed models and could be improved in future: 570
17•The lack of diversity in linguistic reasoning steps. The 5-shot prompting to the GPT-4 gains 571
a stable solving steps, but it also results in the descriptions for executing manipulations or 572
general thinking are similar. We suggest that this can be addressed by employing diversiﬁed 573
prompts or requirements. 574
•The inaccuracy of visual tools. We ﬁnd that there are a considerable amount of negative 575
paths caused by the failures of visual tools, such as the rough granularity of bounding boxes 576
and the error recognition of slated letters or long sentences. This issue can be relieved by 577
improving the semantic understanding capabilities of visual tools. 578
D Details of Training 579
D.1 Launching Prompts 580
•Please solve the problem gradually via a chain of manipulations, where in each 581
step you can selectively adopt one of the following manipulations GROUNDING(a 582
phrase)!boxes, OCR(an image or a region) !texts, CROP_AND_ZOOMIN(a region on 583
given image)!new_image, CALCULATE(a computable target) !numbers, or invent a new 584
manipulation, if that seems helpful. {QUESTION} 585
•Please tackle a given question in a stepbystep manner. For each step one of the following 586
manipulations (depicted as Name(Input) !Retrun) can be optionally used: GROUNDING(a 587
phrase)!boxes, OCR(an image or a region) !texts, CROP_AND_ZOOMIN(a region on 588
given image)!new_image, CALCULATE(a computable target) !numbers, or develop a 589
new manipulation yourself (if it is indeed required). {QUESTION} 590
•Please go through the question incrementally with chain of manipulations (optionally use 591
manipulation when needed) such as GROUNDING(a phrase) !boxes, OCR(an image or 592
a region)!texts, CROP_AND_ZOOMIN(a region on given image) !new_image, CAL- 593
CULATE(a computable target) !numbers, and create a new manipulation if necessary. 594
{QUESTION} 595
D.2 Training settings 596
Parameters Stage1-1 State1-2 Stage-2
Hardware Environment 3,840 A100xdays 256 A100xdays 160 A100xdays
Objective next token prediction next token prediction next token prediction
Images 1.5B 40M 576K
Batch size 8192 1024 160
Iterations 120,000 60000 14000
Optimizer AdamW AdamW AdamW
Learning rate 1e-4 1e-5 1e-5
Warm up steps 7200 1200 280
Trainable weights 6.5B visual expert 6.5B visual expert 6.5B visual expert
Table 5: Training details of all stages.
E Details of Qualitative Analysis 597
E.1 Qualitative Analysis 598
We investigate the evidential reasoning capability of CogCoM on scenarios that requires different 599
types of meticulous reasoning, including recognizing textual details, reading time, understanding 600
charts and counting objects. The results are shown in Figure 1. The ﬁrst case demonstrates that 601
CogCoM ﬁnds the region corresponding to the plane logo through two steps of grounding and then 602
achieves the answer based on zooming in the cropped region. The second case illustrates the ability 603
of CogCoM in reading time, by locating the device that displays time and then transforming the time 604
into words based on the read_timne manipulation. In the forth example, CogCoM ﬁrst identiﬁes all 605
18visible truck wheels through grounding, and then leverages the counting manipulation to consolidate 606
the total number. 607
E.2 Representative Cases 608
Question: What is written on the trafﬁc sign near the bus?Model:     Using GROUNDING(the trafﬁc sign near the bus) to locate the trafﬁc sign near the bus in current image, which is [[084,157,926,703;676,146,740,273]]. Utilizing OCR(texts in region [[084,157,926,703]]) to read the texts written on the trafﬁc sign within region [[084,157,926,703]], resulting Taxi. So ultimately, the conclusive answer to the question in discussion is Taxi.
CoM:
Visual Grounding:     Question: Find the region in image that "a trafﬁc sign" describes.     Model:     [[678,145,739,270]Grounded Captioning:    Question: The goal of this task is to generate description for one part of the image. The part is speciﬁed by [[678,145,739,270]].     Model:     A no tax signImage Captioning:    Question: In this task, you are given an image and you will need to generate some text to describe it. Please give me a concise answer.    Answer:   Taxi bus with rainbow colors on side.Multi Choice:    Question: Select the color that does not appear in the rainbow sign of this bus. (1) red. (2) yellow. (3) pink. (4) black. Please give me a direct answer.    Answer:   blackDetailed Captioning:    Question: Explain the visual content of the image in great detail.    Answer:   The image captures a scene on a city street, where a white bus with red and blue accents is parked. The bus bears the text "Trafalgar" in bold red letters across its side, indicating it's likely part of a tourist or travel company. The bus is positioned next to a trafﬁc sign that reads "TAXI", suggesting it might be waiting for passengers or ready to embark on a journey. In the background, there are buildings and trees, adding depth to the urban landscape. The overall impression is one of a typical day in an urban setting, with a bus waiting for its passengers amidst the hustle and bustle of city life.Given a question, please solve the question step-by-step with a chain of manipulations, where in each step you can optionally use one of the following manipulations (deﬁned as Name(Input)->Return) on the image: GROUNDING(a phrase)->boxes, OCR(an image or a region)->texts, CROP_AND_ZOOMIN(a region on given image)->new_image, CALCULATE(a computable target)->numbers, or a new manipulation created by yourself (if it is indeed needed). 
Figure 9: CogCoM demonstrates the ﬂexible capabilities for adapting to different multimodal
scenarios, including evidential visual reasoning, Visual Grounding, Grounded Captioning, Image
Captioning, Multi Choice, and Detailed Captioning.
19NeurIPS Paper Checklist 609
1.Claims 610
Question: Do the main claims made in the abstract and introduction accurately reﬂect the 611
paper’s contributions and scope? 612
Answer: [Yes] 613
Justiﬁcation: Please refer to Line 6 to Line 20, and Line 48 to Line 72. 614
Guidelines: 615
•The answer NA means that the abstract and introduction do not include the claims 616
made in the paper. 617
•The abstract and/or introduction should clearly state the claims made, including the 618
contributions made in the paper and important assumptions and limitations. A No or 619
NA answer to this question will not be perceived well by the reviewers. 620
•The claims made should match theoretical and experimental results, and reﬂect how 621
much the results can be expected to generalize to other settings. 622
•It is ﬁne to include aspirational goals as motivation as long as it is clear that these goals 623
are not attained by the paper. 624
2.Limitations 625
Question: Does the paper discuss the limitations of the work performed by the authors? 626
Answer: [Yes] 627
Justiﬁcation: Please refer to Appendix B. 628
Guidelines: 629
•The answer NA means that the paper has no limitation while the answer No means that 630
the paper has limitations, but those are not discussed in the paper. 631
• The authors are encouraged to create a separate "Limitations" section in their paper. 632
•The paper should point out any strong assumptions and how robust the results are to 633
violations of these assumptions (e.g., independence assumptions, noiseless settings, 634
model well-speciﬁcation, asymptotic approximations only holding locally). The authors 635
should reﬂect on how these assumptions might be violated in practice and what the 636
implications would be. 637
•The authors should reﬂect on the scope of the claims made, e.g., if the approach was 638
only tested on a few datasets or with a few runs. In general, empirical results often 639
depend on implicit assumptions, which should be articulated. 640
•The authors should reﬂect on the factors that inﬂuence the performance of the approach. 641
For example, a facial recognition algorithm may perform poorly when image resolution 642
is low or images are taken in low lighting. Or a speech-to-text system might not be 643
used reliably to provide closed captions for online lectures because it fails to handle 644
technical jargon. 645
•The authors should discuss the computational efﬁciency of the proposed algorithms 646
and how they scale with dataset size. 647
•If applicable, the authors should discuss possible limitations of their approach to 648
address problems of privacy and fairness. 649
•While the authors might fear that complete honesty about limitations might be used by 650
reviewers as grounds for rejection, a worse outcome might be that reviewers discover 651
limitations that aren’t acknowledged in the paper. The authors should use their best 652
judgment and recognize that individual actions in favor of transparency play an impor- 653
tant role in developing norms that preserve the integrity of the community. Reviewers 654
will be speciﬁcally instructed to not penalize honesty concerning limitations. 655
3.Theory Assumptions and Proofs 656
Question: For each theoretical result, does the paper provide the full set of assumptions and 657
a complete (and correct) proof? 658
Answer: [Yes] 659
20Justiﬁcation: Please refer to Line 74 to Line 92 for pilot experiments. 660
Guidelines: 661
• The answer NA means that the paper does not include theoretical results. 662
•All the theorems, formulas, and proofs in the paper should be numbered and cross- 663
referenced. 664
•All assumptions should be clearly stated or referenced in the statement of any theorems. 665
•The proofs can either appear in the main paper or the supplemental material, but if 666
they appear in the supplemental material, the authors are encouraged to provide a short 667
proof sketch to provide intuition. 668
•Inversely, any informal proof provided in the core of the paper should be complemented 669
by formal proofs provided in appendix or supplemental material. 670
• Theorems and Lemmas that the proof relies upon should be properly referenced. 671
4.Experimental Result Reproducibility 672
Question: Does the paper fully disclose all the information needed to reproduce the main ex- 673
perimental results of the paper to the extent that it affects the main claims and/or conclusions 674
of the paper (regardless of whether the code and data are provided or not)? 675
Answer: [Yes] 676
Justiﬁcation: The data collection with Section 3, model training process with Section 4, and 677
the experimental settings with Section 5. 678
Guidelines: 679
• The answer NA means that the paper does not include experiments. 680
•If the paper includes experiments, a No answer to this question will not be perceived 681
well by the reviewers: Making the paper reproducible is important, regardless of 682
whether the code and data are provided or not. 683
•If the contribution is a dataset and/or model, the authors should describe the steps taken 684
to make their results reproducible or veriﬁable. 685
•Depending on the contribution, reproducibility can be accomplished in various ways. 686
For example, if the contribution is a novel architecture, describing the architecture fully 687
might sufﬁce, or if the contribution is a speciﬁc model and empirical evaluation, it may 688
be necessary to either make it possible for others to replicate the model with the same 689
dataset, or provide access to the model. In general. releasing code and data is often 690
one good way to accomplish this, but reproducibility can also be provided via detailed 691
instructions for how to replicate the results, access to a hosted model (e.g., in the case 692
of a large language model), releasing of a model checkpoint, or other means that are 693
appropriate to the research performed. 694
•While NeurIPS does not require releasing code, the conference does require all submis- 695
sions to provide some reasonable avenue for reproducibility, which may depend on the 696
nature of the contribution. For example 697
(a)If the contribution is primarily a new algorithm, the paper should make it clear how 698
to reproduce that algorithm. 699
(b)If the contribution is primarily a new model architecture, the paper should describe 700
the architecture clearly and fully. 701
(c)If the contribution is a new model (e.g., a large language model), then there should 702
either be a way to access this model for reproducing the results or a way to reproduce 703
the model (e.g., with an open-source dataset or instructions for how to construct 704
the dataset). 705
(d)We recognize that reproducibility may be tricky in some cases, in which case 706
authors are welcome to describe the particular way they provide for reproducibility. 707
In the case of closed-source models, it may be that access to the model is limited in 708
some way (e.g., to registered users), but it should be possible for other researchers 709
to have some path to reproducing or verifying the results. 710
5.Open access to data and code 711
Question: Does the paper provide open access to the data and code, with sufﬁcient instruc- 712
tions to faithfully reproduce the main experimental results, as described in supplemental 713
material? 714
21Answer: [Yes] 715
Justiﬁcation: We will open-source the code, model weights, and all collected data. The 716
generation process and statistics of data are available at Section 3 and Appendix C. 717
Guidelines: 718
• The answer NA means that paper does not include experiments requiring code. 719
•Please see the NeurIPS code and data submission guidelines ( https://nips.cc/ 720
public/guides/CodeSubmissionPolicy ) for more details. 721
•While we encourage the release of code and data, we understand that this might not be 722
possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not 723
including code, unless this is central to the contribution (e.g., for a new open-source 724
benchmark). 725
•The instructions should contain the exact command and environment needed to run to 726
reproduce the results. See the NeurIPS code and data submission guidelines ( https: 727
//nips.cc/public/guides/CodeSubmissionPolicy ) for more details. 728
•The authors should provide instructions on data access and preparation, including how 729
to access the raw data, preprocessed data, intermediate data, and generated data, etc. 730
•The authors should provide scripts to reproduce all experimental results for the new 731
proposed method and baselines. If only a subset of experiments are reproducible, they 732
should state which ones are omitted from the script and why. 733
•At submission time, to preserve anonymity, the authors should release anonymized 734
versions (if applicable). 735
•Providing as much information as possible in supplemental material (appended to the 736
paper) is recommended, but including URLs to data and code is permitted. 737
6.Experimental Setting/Details 738
Question: Does the paper specify all the training and test details (e.g., data splits, hyper- 739
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the 740
results? 741
Answer: [Yes] 742
Justiﬁcation: The experimental settings are listed in each subsection of benchmark eval- 743
uation, which is Section 5.1, Section 5.2 and Section 5.3. The training settings with 744
hyperparameters and optimizations are listed at Appendix D.2. 745
Guidelines: 746
• The answer NA means that the paper does not include experiments. 747
•The experimental setting should be presented in the core of the paper to a level of detail 748
that is necessary to appreciate the results and make sense of them. 749
•The full details can be provided either with the code, in appendix, or as supplemental 750
material. 751
7.Experiment Statistical Signiﬁcance 752
Question: Does the paper report error bars suitably and correctly deﬁned or other appropriate 753
information about the statistical signiﬁcance of the experiments? 754
Answer: [Yes] 755
Justiﬁcation: We include the error analysis with textual descriptions in Appendix C.5, as 756
well as the reasoning accuracy with limitations in Section 5.1.2. 757
Guidelines: 758
• The answer NA means that the paper does not include experiments. 759
•The authors should answer "Yes" if the results are accompanied by error bars, conﬁ- 760
dence intervals, or statistical signiﬁcance tests, at least for the experiments that support 761
the main claims of the paper. 762
• The factors of variability that the error bars are capturing should be clearly stated (for 763
example, train/test split, initialization, random drawing of some parameter, or overall 764
run with given experimental conditions). 765
22•The method for calculating the error bars should be explained (closed form formula, 766
call to a library function, bootstrap, etc.) 767
• The assumptions made should be given (e.g., Normally distributed errors). 768
•It should be clear whether the error bar is the standard deviation or the standard error 769
of the mean. 770
•It is OK to report 1-sigma error bars, but one should state it. The authors should 771
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis 772
of Normality of errors is not veriﬁed. 773
•For asymmetric distributions, the authors should be careful not to show in tables or 774
ﬁgures symmetric error bars that would yield results that are out of range (e.g. negative 775
error rates). 776
•If error bars are reported in tables or plots, The authors should explain in the text how 777
they were calculated and reference the corresponding ﬁgures or tables in the text. 778
8.Experiments Compute Resources 779
Question: For each experiment, does the paper provide sufﬁcient information on the com- 780
puter resources (type of compute workers, memory, time of execution) needed to reproduce 781
the experiments? 782
Answer: [Yes] 783
Justiﬁcation: Detailed compute resources are listed in Section D.2. 784
Guidelines: 785
• The answer NA means that the paper does not include experiments. 786
•The paper should indicate the type of compute workers CPU or GPU, internal cluster, 787
or cloud provider, including relevant memory and storage. 788
•The paper should provide the amount of compute required for each of the individual 789
experimental runs as well as estimate the total compute. 790
•The paper should disclose whether the full research project required more compute 791
than the experiments reported in the paper (e.g., preliminary or failed experiments that 792
didn’t make it into the paper). 793
9.Code Of Ethics 794
Question: Does the research conducted in the paper conform, in every respect, with the 795
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines ? 796
Answer: [Yes] 797
Justiﬁcation: All code anonymity. 798
Guidelines: 799
•The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. 800
•If the authors answer No, they should explain the special circumstances that require a 801
deviation from the Code of Ethics. 802
•The authors should make sure to preserve anonymity (e.g., if there is a special consid- 803
eration due to laws or regulations in their jurisdiction). 804
10.Broader Impacts 805
Question: Does the paper discuss both potential positive societal impacts and negative 806
societal impacts of the work performed? 807
Answer: [Yes] 808
Justiﬁcation: We discussed the societal impacts in details in Appendix B. 809
Guidelines: 810
• The answer NA means that there is no societal impact of the work performed. 811
•If the authors answer NA or No, they should explain why their work has no societal 812
impact or why the paper does not address societal impact. 813
•Examples of negative societal impacts include potential malicious or unintended uses 814
(e.g., disinformation, generating fake proﬁles, surveillance), fairness considerations 815
(e.g., deployment of technologies that could make decisions that unfairly impact speciﬁc 816
groups), privacy considerations, and security considerations. 817
23•The conference expects that many papers will be foundational research and not tied 818
to particular applications, let alone deployments. However, if there is a direct path to 819
any negative applications, the authors should point it out. For example, it is legitimate 820
to point out that an improvement in the quality of generative models could be used to 821
generate deepfakes for disinformation. On the other hand, it is not needed to point out 822
that a generic algorithm for optimizing neural networks could enable people to train 823
models that generate Deepfakes faster. 824
•The authors should consider possible harms that could arise when the technology is 825
being used as intended and functioning correctly, harms that could arise when the 826
technology is being used as intended but gives incorrect results, and harms following 827
from (intentional or unintentional) misuse of the technology. 828
•If there are negative societal impacts, the authors could also discuss possible mitigation 829
strategies (e.g., gated release of models, providing defenses in addition to attacks, 830
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from 831
feedback over time, improving the efﬁciency and accessibility of ML). 832
11.Safeguards 833
Question: Does the paper describe safeguards that have been put in place for responsible 834
release of data or models that have a high risk for misuse (e.g., pretrained language models, 835
image generators, or scraped datasets)? 836
Answer: [NA] 837
Justiﬁcation: The training data collected from public datasets with research purpose does 838
not face the safety risks. 839
Guidelines: 840
• The answer NA means that the paper poses no such risks. 841
•Released models that have a high risk for misuse or dual-use should be released with 842
necessary safeguards to allow for controlled use of the model, for example by requiring 843
that users adhere to usage guidelines or restrictions to access the model or implementing 844
safety ﬁlters. 845
•Datasets that have been scraped from the Internet could pose safety risks. The authors 846
should describe how they avoided releasing unsafe images. 847
•We recognize that providing effective safeguards is challenging, and many papers do 848
not require this, but we encourage authors to take this into account and make a best 849
faith effort. 850
12.Licenses for existing assets 851
Question: Are the creators or original owners of assets (e.g., code, data, models), used in 852
the paper, properly credited and are the license and terms of use explicitly mentioned and 853
properly respected? 854
Answer: [Yes] 855
Justiﬁcation: The public available datasets used in this paper are cited properly. 856
Guidelines: 857
• The answer NA means that the paper does not use existing assets. 858
• The authors should cite the original paper that produced the code package or dataset. 859
•The authors should state which version of the asset is used and, if possible, include a 860
URL. 861
• The name of the license (e.g., CC-BY 4.0) should be included for each asset. 862
•For scraped data from a particular source (e.g., website), the copyright and terms of 863
service of that source should be provided. 864
•If assets are released, the license, copyright information, and terms of use in the 865
package should be provided. For popular datasets, paperswithcode.com/datasets 866
has curated licenses for some datasets. Their licensing guide can help determine the 867
license of a dataset. 868
•For existing datasets that are re-packaged, both the original license and the license of 869
the derived asset (if it has changed) should be provided. 870
24•If this information is not available online, the authors are encouraged to reach out to 871
the asset’s creators. 872
13.New Assets 873
Question: Are new assets introduced in the paper well documented and is the documentation 874
provided alongside the assets? 875
Answer: [Yes] 876
Justiﬁcation: The newly created benchmark CoM-test is documented in detailed in Section 877
5 and Appendix C.2, and the manually annotated math data is described in Section 3.2. 878
Guidelines: 879
• The answer NA means that the paper does not release new assets. 880
•Researchers should communicate the details of the dataset/code/model as part of their 881
submissions via structured templates. This includes details about training, license, 882
limitations, etc. 883
•The paper should discuss whether and how consent was obtained from people whose 884
asset is used. 885
•At submission time, remember to anonymize your assets (if applicable). You can either 886
create an anonymized URL or include an anonymized zip ﬁle. 887
14.Crowdsourcing and Research with Human Subjects 888
Question: For crowdsourcing experiments and research with human subjects, does the paper 889
include the full text of instructions given to participants and screenshots, if applicable, as 890
well as details about compensation (if any)? 891
Answer: [NA] 892
Justiﬁcation: This study does not involve crowdsourcing nor research with human subjects. 893
Guidelines: 894
•The answer NA means that the paper does not involve crowdsourcing nor research with 895
human subjects. 896
•Including this information in the supplemental material is ﬁne, but if the main contribu- 897
tion of the paper involves human subjects, then as much detail as possible should be 898
included in the main paper. 899
•According to the NeurIPS Code of Ethics, workers involved in data collection, curation, 900
or other labor should be paid at least the minimum wage in the country of the data 901
collector. 902
15.Institutional Review Board (IRB) Approvals or Equivalent for Research with Human 903
Subjects 904
Question: Does the paper describe potential risks incurred by study participants, whether 905
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) 906
approvals (or an equivalent approval/review based on the requirements of your country or 907
institution) were obtained? 908
Answer: [NA] 909
Justiﬁcation: This study does not involve crowdsourcing nor research with human subjects. 910
Guidelines: 911
•The answer NA means that the paper does not involve crowdsourcing nor research with 912
human subjects. 913
•Depending on the country in which research is conducted, IRB approval (or equivalent) 914
may be required for any human subjects research. If you obtained IRB approval, you 915
should clearly state this in the paper. 916
•We recognize that the procedures for this may vary signiﬁcantly between institutions 917
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the 918
guidelines for their institution. 919
•For initial submissions, do not include any information that would break anonymity (if 920
applicable), such as the institution conducting the review. 921
25