BMRS: Bayesian Model Reduction for Structured
Pruning
Dustin Wright, Christian Igel, and Raghavendra Selvan
Department of Computer Science, University of Copenhagen
{dw,igel,raghav}@di.ku.dk
Abstract
Modern neural networks are often massively overparameterized leading to high
compute costs during training and at inference. One effective method to improve
both the compute and energy efficiency of neural networks while maintaining good
performance is structured pruning, where full network structures (e.g. neurons or
convolutional filters) that have limited impact on the model output are removed. In
this work, we propose Bayesian Model Reduction for Structured pruning (BMRS),
a fully end-to-end Bayesian method of structured pruning. BMRS is based on
two recent methods: Bayesian structured pruning with multiplicative noise, and
Bayesian model reduction (BMR), a method which allows efficient comparison of
Bayesian models under a change in prior. We present two realizations of BMRS de-
rived from different priors which yield different structured pruning characteristics:
1) BMRS Nwith the truncated log-normal prior, which offers reliable compression
rates and accuracy without the need for tuning any thresholds and 2) BMRS U
with the truncated log-uniform prior that can achieve more aggressive compres-
sion based on the boundaries of truncation. Overall, we find that BMRS offers a
theoretically grounded approach to structured pruning of neural networks yielding
both high compression rates and accuracy. Experiments on multiple datasets and
neural networks of varying complexity showed that the two BMRS methods offer a
competitive performance-efficiency trade-off compared to other pruning methods.1
1 Introduction
Modern neural networks come with an increasing computational burden, as scale is often seen to be
associated with performance [ 34]. The response to this has been a focus on research around the topic
of neural network efficiency [ 3], where the goal is to reduce the computational cost of a system while
maintaining other desirable metrics. As such, selecting a method to improve efficiency comes with
many tradeoffs, including how to balance compute and energy consumption with accuracy [35].
Neural network pruning seeks to do this by removing parts of a network which have limited impact
on its output. This comes in two primary forms: unstructured pruning, where individual weights
are removed, and structured pruning, where entire neural network structures such as neurons and
convolutional filters are removed [ 27]. Structured pruning is often desirable as unstructured pruning
can result in sparse computations which are energy intensive on current hardware, while structured
pruning can maintain more energy efficient dense operations [ 14,17,31]. Many ways to perform
structured pruning have been proposed, but the challenge of how to appropriately balance accuracy
and complexity in a principled manner has remained.
In this work, we address this challenge by proposing BMRS: Bayesian Model Reduction for
Structured pruning. BMRS is a principled method based on combining two complementary lines
1Source code: https://github.com/saintslab/bmrs-structured-pruning/
38th Conference on Neural Information Processing Systems (NeurIPS 2024).Calculate	ŒîùêπPrune	Œîùêπ‚â•0Figure 1: BMRS uses BMR to perform structured pruning under multiplicative noise by calculating
the change in log-evidence of noise variables Œ∏under a prior which would shrink them to 0.
of work: Bayesian structured pruning with multiplicative noise [ 30] and Bayesian model reduction
(BMR) [ 6], a method of efficient Bayesian model comparison under a change in prior. Multiplicative
noise allows one to flexibly induce sparsity at any structural level without the need to use computa-
tionally complex spike-and-slab priors [ 16,29], while BMR enables principled pruning rules without
the need for task-specific threshold tuning. Starting with the approach from [ 30], we derive two
versions of BMRS using different priors which offer their own benefits. BMRS Nis based on the
truncated log-normal prior and has the benefit of achieving a high compression rate without needing
to tune a threshold for compression, while BMRS Uoffers tunable compression by controlling the
allowable precision of noise variables in the network. In sum, our contributions are:
‚Ä¢BMRS: a method for Bayesian structured pruning based on multiplicative noise and Bayesian
model reduction;
‚Ä¢ Derivations of pruning algorithms for two priors with theoretical motivation;
‚Ä¢Empirical results on a range of neural networks and datasets demonstrating high compression
rates without any threshold tuning, with more extreme compression achievable via a parameter
controlling allowable precision.
2 Related work
The primary goal of neural network pruning is to determine the elements of a network which can
be removed with minimal impact on the output. Ideally, a pruning method ranks all elements in the
order in which they can be removed and provides a criterion for truncating the resulting ordered
list. Since the early works on gradient based methods for pruning [ 23,12], the literature around
neural network pruning has expanded rapidly, with the two main lines of work exploring pruning
individual weights (unstructured pruning) and pruning full network structures (structured pruning).
For a recent survey, see [ 27]. The closest related works to ours are those pruning methods which
perform Bayesian pruning [ 30,28,10,16,26], and those which use Bayesian model reduction to
determine what elements to remove from a neural network [5, 29].
Bayesian pruning. Bayesian structured pruning was first explored in Kingma et al. [20], where
the authors demonstrate that dropout has a Bayesian interpretation as multiplicative noise with a
sparsity inducing prior. The studies of [ 30,28,10] follow this work by explicitly modeling the
random noise in dropout with different priors, [ 30] using a truncated log-uniform prior and [ 28,10]
using horseshoe priors. Following this, the works of [ 2,15,16,33,29,26] have explored pruning
of Bayesian neural networks (BNNs) with spike-and-slab priors to induce both weight sparsity and
group sparsity with flat and hierarchical priors, respectively. [ 16] demonstrate that thresholdless
pruning is achievable by placing an explicit spike-and-slab prior on the nodes of a BNN to induce
group sparsity. However, this setup requires complex and carefully constructed posteriors due to the
discrete nature of spike-and-slab distributions and is thus computationally inefficient [29, 16].
Bayesian model reduction. Bayesian model reduction, discussed in detail in ¬ß3.2, is an efficient
method of Bayesian model comparison which allows for analytic solutions for the model evidence
under a change in priors. BMR has found application across multiple scientific disciplines [ 9,8,18],
and has recently been used as a method for neural network pruning [ 29,5]. More specifically, [ 5]
demonstrate the benefits of BMR-based pruning for the case of a BNN with a Gaussian prior on
2the weights, and [ 29] demonstrate the utility of BMR for unstructured pruning of BNNs with priors
inducing both weight and group sparsity.
3 Problem formulation
3.1 Structured pruning with multiplicative noise and variational inference
We approach the problem of structured pruning using sparsity inducing multiplicative noise as
described in [ 30]. In this setting, we have a dataset consisting of Ni.i.d. input-output pairs,
D={(xj, yj)‚àÄj= 1, . . . , N }. We consider a parametric model, here a deep neural network,
that maps the input data xjto their output yjusing the trainable parameters Wgiving rise to the
likelihood function p(D|Œò,W) =QN
j=1p(yj|xj,Œò,W). In addition to the trainable weights, W,
the model consists of the sparsity inducing multiplicative noise given by the random variable, Œò, with
prior p(Œò). This is in contrast to BNNs where the weights are random variables but aligns with the
setting when using multiplicative noise for Bayesian pruning [30].
The effect of the multiplicative noise Œ∏i‚ààŒòfor a structural element in a neural network with index i,
parameters wi, and input hi‚àí1is given as
hi=Œ∏i¬∑(wihi‚àí1), Œ∏ i‚àºp(Œ∏i). (1)
We note that wicould be the parameters of any structural element in the network, for example, a single
neuron or an entire convolutional filter. Given this, we would like to learn the maximum likelihood
estimate (MLE) ÀÜwiof the weights as well as the posterior distribution over the multiplicative noise,
p(Œ∏i|D,ÀÜwi), when using a sparsity inducing prior p(Œ∏i)such that Œ∏ifavors values closer to 0.
Following [ 30], the neural network weights are learned via gradient descent as in standard deep learn-
ing model optimization. The posterior distribution, p(Œ∏|D,ÀÜwi) =p(D|Œ∏i,ÀÜwi)p(Œ∏i)/p(D), however,
is intractable. We resort to a variational approximation from a tractable family of approximating
distributions, qœï(Œ∏), parameterized by œï(for the sake of brevity we do not indicate the dependence
onwiand omit the subscript i). The parameters œïare obtained by optimizing the following objective
w.r.t. Œ∏:
F[p, q] =DKL[qœï(Œ∏)||p(Œ∏|D)]c=DKL[qœï(Œ∏)||p(Œ∏)]‚àíEqœï[logp(yj|xj, Œ∏,ÀÜw)] (2)
This is the commonly used variational free energy (VFE) or negative evidence lower bound
(ELBO) [4]. Herec=denotes equality up to a positive constant.
The expectation Eqœï[¬∑]is approximated by a Monte Carlo estimator acting on minibatch samples from
D, and reparameterization allows to backpropagate gradients through stochastic variables [ 19,28].
Under this reparameterization, the variational distribution, qœï(Œ∏), becomes a deterministic function
of the non-parametric noise œµ‚àºp(œµ)and the VFE is calculated as
F[p, q]c=DKL[qœï(Œ∏)||p(Œ∏)]‚àíX
(xj,yj)‚ààDlogp(yj|xj, Œ∏=f(œï, œµ);ÀÜw), (3)
where fis a function that allows us to sample Œ∏via deterministic parameters œïand the non-parametric
stochastic variable, œµ. Optimization of Equation 3 allows us to jointly learn ÀÜwandŒ∏. The particular
choice of priors and the approximating distributions to induce sparsity are discussed in ¬ß4.1.
3.2 Bayesian model reduction
Bayesian model reduction (BMR) allows one to efficiently compute the change in VFE (Equation 2)
under a change in prior without the need to re-estimate model parameters. To perform pruning, one
can start out by selecting a broad prior for the original model estimation and then pick a narrower prior
(i.e. reduced prior) with the density concentrated around 0. Then, BMR can be used to determine if
the VFE is greater under the reduced model, and prune those parameters for which this condition
holds. We briefly describe how this is achieved in the general case, followed by the specific realization
for BMRS in ¬ß4.2; for further details see [6].
Consider the likelihood function p(D|Œ∏)and a prior p(Œ∏)on the variable Œ∏. We can introduce a new
prior Àúp(Œ∏)which shares the same likelihood as the original model (i.e. p(D|Œ∏) = Àúp(D|Œ∏)) and get:
p(D|Œ∏) =p(Œ∏|D)p(D)
p(Œ∏)=Àúp(Œ∏|D)Àúp(D)
Àúp(Œ∏)‚áíÀúp(Œ∏|D) =p(D)
Àúp(D)p(Œ∏|D)Àúp(Œ∏)
p(Œ∏)(4)
3By marginalizing over Œ∏and taking the log, we obtain the difference in log evidence as:
log Àúp(D)‚àílogp(D) = logZ
p(Œ∏|D)Àúp(Œ∏)
p(Œ∏)dŒ∏‚âàlogZ
qœï(Œ∏)Àúp(Œ∏)
p(Œ∏)dŒ∏= log EÀúpqœï(Œ∏)
p(Œ∏)
(5)
More concisely, we call the change in log evidence ‚àÜFand thus have:
‚àÜF‚âúlog Àúp(D)‚àílogp(D)‚âàlogEÀúpqœï(Œ∏)
p(Œ∏)
(6)
If the new prior, Àúp(Œ∏), is selected so that Œ∏would be removed, pruning can be performed when
‚àÜF‚â•0. Additionally, when the type of distributions between p(Œ∏),Àúp(Œ∏), andqœï(Œ∏)are the same or
similar (e.g. Gaussian), ‚àÜFcan be calculated efficiently in closed form (see [7]).
We presented BMR for a general likelihood function p(D|Œ∏); it holds analogously for the likelihood
function p(D|Œ∏,W)introduced with the multiplicative noise described in ¬ß3.1.
4 Bayesian model reduction for structured pruning (BMRS)
Our goal is to derive a principled structured pruning algorithm starting from the general formulation
in ¬ß 3 which can automatically determine which structures to prune. BMRS accomplishes this by
following the multiplicative noise setup in [ 30] with BMR used on the noise terms. Figure 1 illustrates
the general approach, where ‚àÜFis calculated for a model trained with multiplicative noise under a
reduced prior, and elements of the model are removed if the new VFE is greater. We next describe
the multiplicative noise layer trained using Equation 2, and then derive two variants of BMRS from
Equation 6 using different reduced priors.
4.1 Multiplicative noise layer
The concept of multiplicative noise inducing sparsity in neural networks was first introduced with
variational dropout, where [ 20] show that dropout has a Bayesian interpretation as multiplicative
noise with a log-uniform prior. One can use this interpretation of dropout in order to explicitly learn
dropout parameters, Œ∏i, as in ¬ß3, by selecting appropriate prior and variational distributions and
optimizing Equation 2 directly. [ 30] propose to do so by using the truncated log-uniform distribution
as a prior and the truncated log-normal distribution as the variational distribution. As such, the
variational approximation can be performed using
hi=Œ∏i¬∑(wihi‚àí1);qœï(Œ∏i) =LogN[a,b](Œ∏i|¬µi, œÉ2
i);p(Œ∏i) =LogU[a,b](Œ∏i) (7)
with bounded support between aandband0< a < b ‚â§1. We refer to [ 30] for details on how to
learn qœï, which is obtained by optimizing Equation 3.
The log-uniform distribution serves as a sparsity inducing prior as most of its density is concentrated
around 0 (see panel 2 in Figure 1). Additionally, it acts as a regularizer on the floating point precision
of the multiplicative noise terms [ 20]. In [ 30] this is used to perform structured pruning by removing
all structures hiwhere the signal-to-noise ratio of the noise term Œ∏ifalls below a pre-defined threshold.
We next show how to derive principled pruning algorithms based on BMR which induce sparsity
while maintaining accuracy without the need for tuning pruning thresholds.
4.2 Deriving BMRS
Our goal is to use BMR in order to perform structured pruning of models trained with multiplicative
noise. To do so, we must select a new prior Àúp(Œ∏)from which we can: 1) induce sparsity; 2) efficiently
calculate ‚àÜFand; 3) prune the network while maintaining good performance.
Selecting the reduced prior, Àúp(Œ∏), is straightforward when the prior and approximate posterior are
the same type of distributions. For example, in a fully BNN where one assumes a prior distribution
ofN(Œò|0,I)over all the model weights with a mean-field variational approximation resulting in a
factorisation over individual weights, N(Œ∏|¬µ, œÉ2), the three criteria above can be met when one selects
a Gaussian reduced prior with slight variance around 0 i.e. N(Œ∏|0, œµ), œµ‚âà0.2However, in the case of
2It can be shown that ‚àÜFis calculable efficiently in closed form for this setup; see e.g. [7]
4multiplicative noise, our prior and variational distributions are of different types and thus the selection
of the reduced prior is not immediately obvious. Here, we derive and compare the characteristics of
two different reduced priors: one based on a truncated log-normal distribution, which we can use to
approximate a Dirac delta at 0 (BMRS N), and one based on a truncated log-uniform distribution with
reduced support (BMRS U).
4.2.1 BMRS with log-normal reduced prior (BMRS N)
First, we derive ‚àÜFwhen using the log-uniform distribution as the original prior, p(Œ∏) =
LogU[a,b](Œ∏), and the truncated log-normal distribution as the reduced prior, Àúp(Œ∏) =
LogN[a,b](Œ∏|Àú¬µp,ÀúœÉ2
p). We select a truncated log-normal distribution, as it matches the variational
distribution qœï(Œ∏), and the log-uniform prior, because it is a special case of the log-normal distribution
when the variance goes to infinity. Because of this, we expect that ‚àÜFwill have a closed form
solution, and that the computation will be efficient. We briefly present the results of the derivation
here; for the full derivation see ¬ß A.1.
We can use the specific forms of p(Œ∏)andÀúp(Œ∏)for the truncated log-uniform and truncated log-normal
distributions, respectively, in Equation 6 to determine ‚àÜF:
‚àÜF‚âàlogEÀúpqœï(Œ∏)
p(Œ∏)
= logZÀúq(logb‚àíloga)
ZÀúpZq+1
2logÀúœÉ2
q
2œÄÀúœÉ2pœÉ2q‚àí1
2 
¬µ2
q
œÉ2q+Àú¬µ2
p
ÀúœÉ2p‚àíÀú¬µ2
q
ÀúœÉ2q!
(8)
with ÀúœÉ2
q=1
œÉ2q+1
ÀúœÉ2p‚àí1
andÀú¬µq= ÀúœÉ2
q¬µq
œÉ2q+Àú¬µp
ÀúœÉ2p
. Here, Zp= Œ¶( Œ≤p)‚àíŒ¶(Œ±p);Œ¶(t) =
1
2[1 + erf(t‚àö
2)]is the CDF of the standard Normal distribution, t‚àº N(0,1);Œ±p= (a‚àí¬µp)/œÉp;
andŒ≤p= (b‚àí¬µp)/œÉp.
As can be seen from Equation 8, the calculation for ‚àÜFcan be performed directly using only the
statistics of the priors and variational distribution. In order for Equation 8 to induce sparsity, we must
select a Àú¬µpandÀúœÉ2
pthat effectively collapse Œ∏to 0. To achieve this, we can approximate a Dirac delta
at 0 by selecting Àú¬µpto be close to 0 (e.g., the lower bound of truncation a), and ÀúœÉ2
pto be sufficiently
small. We will demonstrate in ¬ß5 that this reduced prior results in high sparsity while maintaining
performance without any need for tuning pruning thresholds.
4.2.2 BMRS with log-uniform reduced prior (BMRS U)
Next, we derive the change in VFE, ‚àÜF, when using a truncated log-uniform distribution as the
original prior, p(Œ∏) =LogU[a,b](Œ∏), and a truncated log-uniform distribution with reduced support as
the reduced prior, Àúp(Œ∏) =LogU[a‚Ä≤,b‚Ä≤](Œ∏). We select a reduced truncated log-uniform distribution for
the same reasons as the truncated log-normal: we expect that ‚àÜFwill have an efficiently calculable
closed form, given that the priors are of the same type and are a special case of the variational
distribution. The PDF of the reduced truncated log-uniform distribution is given as follows:
Àúp(Œ∏) =LogU[a‚Ä≤,b‚Ä≤](Œ∏) =(
Œ∏logb‚Ä≤
a‚Ä≤‚àí1
, a < a‚Ä≤‚â§Œ∏‚â§b‚Ä≤< b
0, otherwise(9)
Using this, we can directly solve the integral under the expectation given in Equation 6 for ‚àÜF(full
details in ¬ß A.2):
exp ‚àÜ F‚âàEÀúpqœï(Œ∏)
p(Œ∏)
=Zb
aLogU[a‚Ä≤,b‚Ä≤](Œ∏)qœï(Œ∏)
LogU[a,b](Œ∏)dŒ∏=logb
a
logb‚Ä≤
a‚Ä≤qœï(a‚Ä≤‚â§Œ∏i‚â§b‚Ä≤)(10)
where qœï(a‚Ä≤‚â§Œ∏i‚â§b‚Ä≤)is the CDF of the variational distribution evaluated between a‚Ä≤andb‚Ä≤. We
know from Equation 5 that the VFE under Àúp(Œ∏)is greater when exp ‚àÜ F‚â•1. Plugging this in:
1‚â§logb
a
logb‚Ä≤
a‚Ä≤qœï(a‚Ä≤‚â§Œ∏i‚â§b‚Ä≤)‚áílogb‚Ä≤
a‚Ä≤
logb
a‚â§qœï(a‚Ä≤‚â§Œ∏‚â§b‚Ä≤) (11)
Here, the left hand side of the inequality is the CDF of the truncated log-uniform distribution between
a‚Ä≤andb‚Ä≤. In other words, when the new prior is a log-uniform distribution with reduced support,
5Algorithm 1: Training and pruning with BMRS
input : dataset D; neural network with deterministic weights Wand variational parameters œï;
original prior p( Œò); reduced prior Àúp(Œò); number of training epochs eT; number of
fine-tuning epochs eF; number of pruning epochs P
j‚Üê0
while j < e TandW, œïnot converged do
Train œïandWonDusing Equation 3
ifjmod P= 0then
forall Œ∏iinŒòdo
dF‚Üê‚àÜF(p(Œ∏i),Àúp(Œ∏i), qœï(Œ∏i))
ifdF‚â•0then
œï‚Üêœï\œïi
W‚ÜêW\wi
j‚Üíj+ 1
Fine tune WandœïonDforeFepochs
the BMR pruning criterion amounts to a comparison between the CDF of the original prior and
the variational distribution along the interval [a‚Ä≤, b‚Ä≤]. Additionally, Equation 11 shows that this is
generalizable to any variational distribution with support broader than the reduced prior.
BMRS Upruning and connection to floating point precision. To see how BMRS Ucan be used
for pruning, we first briefly summarize the relationship between the log-uniform distribution and
floating point numbers. Floating point numbers are commonly encoded in binary as a combination of
a sign bit s, a set of exponent bits e, and a set of mantissa bits mdenoting the fractional part of a
real-number: r=s¬∑(m/2p‚àí1)¬∑2e, where pdetermines the precision of the encoding. As discussed
in [11], the mantissae of ‚Äúnaturally observed‚Äù floating point numbers (e.g., natural constants) tend
to follow a log-uniform distribution and repeated multiplications/divisions on a digital computer
transform a broad class of distributions towards a log-uniform distribution
m‚àº(mlogB)‚àí1,1/B‚â§m‚â§1, (12)
where Bis the base of the number system. In the case where the mantissa uses pbits, there are 2p
representable fractional numbers so B= 2p. As such, pdetermines the smallest fractional value
which can be represented. We can use this to have the reduced prior cover a finite range of high
precision values which are acceptable to prune. To accomplish this, we use a reduced log-uniform
prior of the following form by selecting two integers p1, p2where 0‚â§p1< p2:
Àúp(Œ∏) =( 
Œ∏log2p2
2p1‚àí1,1/2p2‚â§Œ∏‚â§1/2p1
0, otherwise(13)
Thus we can reduce the prior to a range of precision between p1andp2by selecting a‚Ä≤= 1/2p2and
b‚Ä≤= 1/2p1. Equation 11 then has a natural interpretation as comparing the probability of drawing a
random mantissa from the interval [1/2p2,1/2p1]to the probability of drawing a value within that
interval from the variational distribution. If we select p2to be the limit of the precision of values in
the number system used ( p2= 23 for single-point precision), then we can interpret p1as determining
the prunable range of precision of all variables Œò. The accuracy-complexity tradeoff inherent in the
selection of Àúp(Œ∏)is then controlled through p1, the desired cutoff of the precision of the network.
4.3 Training and pruning
The details on how to train a network and use BMRS for pruning are given in Algorithm 1. We train a
model for a fixed number of epochs (or until convergence) and perform pruning every Pepochs. This
lends itself to either post-training pruning, where the network is fully trained followed by pruning
and fine-tuning, or continuous pruning, where pruning is performed during model training. In our
experiments, we explore both of these setups and contrast BMRS with alternative pruning methods.
65 Experiments
We demonstrate the pruning behavior of BMRS through several experiments with neural networks
of varying complexity measured as the number of trainable parameters. We use the following
datasets (full details in Appendix B): MNIST [ 22], Fashion-MNIST [ 37], CIFAR10 [ 21], and
TinyImagenet. For MNIST, Fashion-MNIST, and CIFAR10 we experiment with both a multi-layer
perceptron (MLP) and a small CNN (Lenet5 [ 24]). Pruning layers are applied after each fully
connected layer for the MLP, and for each convolutional filter and fully connected layer for Lenet5.
For CIFAR10 and TinyImagenet, we further experiment with a pretrained ResNet-50 [ 13] and a
pretrained vision transformer (ViT) [ 36]. For ResNet-50, we apply pruning layers after each layer of
batch normalization, and for ViT we apply pruning layers to the output of each transformer block.
For the multiplicative noise layers, we set the left bound of truncation to be loga=‚àí20and the
right bound of truncation to be logb= 0. Hyperparameters for the MLPs and Lenet5 are tuned on a
model with no pruning performed and kept the same for each variant (see Appendix C). We perform
experiments using the following model variants and baselines which cover both Bayesian pruning
criteria (e.g. ours, SNR, and Eqœï[Œ∏]) as well as magnitude pruning (L2):
‚Ä¢None: A baseline with no compression and no multiplicative noise.
‚Ä¢L2:A magnitude pruning baseline based on the L2 Norm of weight vectors (matrices in the case
of convolutional filters) at the input of each neuron to be pruned [ 25]. We set the pruning threshold
to the compression rate achieved by BMRS Nusing the same settings of a given experiment.
‚Ä¢Eqœï[Œ∏]: The expected value of noise variables Œ∏. For continuous pruning, we use a set threshold of
0.1.
‚Ä¢SNR: The signal-to-noise ratio Eqœï[Œ∏]p
Var[Œ∏]as used in [ 30]. For continuous pruning, we use a
set threshold of 1 as recommended in [30].
‚Ä¢BMRS N:BMRS using the log-normal prior from Equation 6. In order to reduce the prior to 0, we
setÀú¬µpto the left bound of truncation ( a), and ÀúœÉ2
pto10‚àí12.
‚Ä¢BMRS U-p1:BMRS using the log-uniform prior from Equation 11. In our experiments, we set a‚Ä≤
to be the limit of the precision of single-point floats ( p2= 23 so a‚Ä≤= 1/223) and b‚Ä≤to either p1=
8-bit precision ( b‚Ä≤= 1/28) orp1= 4-bit ( b‚Ä≤= 1/24).
Post-training pruning. We first look at the behavior of BMRS when used in the post-training
setting. To do so, we first train a model on a given dataset, then use each method to rank the neurons
based on their pruning function (L2 norm, signal-to-noise ratio, or ‚àÜF). To observe the accuracy at
different compression rates, neurons are progressively removed based on their rank, and the model is
fine-tuned for one epoch before measuring the test accuracy. For BMRS methods, we additionally
stop pruning once ‚àÜF < 0for a given structure. The plots of accuracy vs. compression for 10
different random seeds are given in Figure 2 (Further experiments given in Appendix E).
First, we find that BMRS generally stops compressing near the knee point of the trade-off curve ‚Äì
a preferred solution of a Pareto front if there is no a priori preferences ‚Äì in all settings except for
BMRS U-4 which only does so in 4 out of 6 settings. Notably, BMRS Naccomplishes this with no
need to tune additional thresholds as is common in pruning literature. To further visualize this, the
right plot in each subfigure shows a scatter plot of the accuracy at the maximum compression rate
(pruning all neurons where ‚àÜF‚â•0) along with the curve of accuracy vs. compression for SNR
pruning near the knee point. We can see that the density of points for BMRS is concentrated near the
optimal point in all cases except for the MLP on MNIST, indicating the robustness of the proposed
methods.
We additionally observe much similarity in the curves for BMRS and SNR pruning, suggesting that
they may be performing similar functions. To further investigate this, we look at the Spearman rank
correlation coefficient [ 32] of the neurons based on their respective functions in Figure 3 (plots for
additional datasets in Appendix E). We see that BMRS Ntends to have a high correlation with SNR,
suggesting that it learns a qualitatively similar function with the benefit of providing a threshold for
compression. BMRS U, on the other hand, tends to have very low or even negative correlation. This,
combined with the more rapidly declining accuracy for a given level of compression, suggests that
BMRS Uis only apt for determining a single split into elements to keep and to remove, but does not
provide an accurate ranking of the elements..
70 50 100
Compression %0.20.40.6AccuracyL2
SNR
Etheta
BMRS_N
BMRS_U-8
BMRS_U-4
55 60 65 70 75
Compression %0.30.40.50.60.7AccuracySNR
BMRS_N
BMRS_U-8
BMRS_U-4
(a) CIFAR10 Lenet5
0 50 100
Compression %0.10.20.30.40.5AccuracyL2
SNR
Etheta
BMRS_N
BMRS_U-8
BMRS_U-4
75 80 85
Compression %0.350.400.450.50AccuracySNR
BMRS_N
BMRS_U-8
BMRS_U-4
 (b) CIFAR10 MLP
0 50 100
Compression %0.00.20.40.60.8AccuracyL2
SNR
Etheta
BMRS_N
BMRS_U-8
BMRS_U-4
85 90 95
Compression %0.00.20.40.60.8AccuracySNR
BMRS_N
BMRS_U-8
BMRS_U-4
(c) Fashion-MNIST Lenet5
0 50 100
Compression %0.20.40.60.8AccuracyL2
SNR
BMRS_N
BMRS_U-8
BMRS_U-4
90 95 100
Compression %0.00.20.40.60.8AccuracySNR
BMRS_N
BMRS_U-8
BMRS_U-4
 (d) Fashion-MNIST MLP
0 50 100
Compression %0.00.20.40.60.81.0AccuracyL2
SNR
Etheta
BMRS_N
BMRS_U-8
BMRS_U-4
80 85 90 95
Compression %0.00.20.40.60.81.0AccuracySNR
BMRS_N
BMRS_U-8
BMRS_U-4
(e) MNIST Lenet5
0 50 100
Compression %0.00.20.40.60.81.0AccuracyL2
SNR
Etheta
BMRS_N
BMRS_U-8
BMRS_U-4
50 60
Compression %0.900.951.001.05AccuracySNR
BMRS_N
BMRS_U-8
BMRS_U-4
 (f) MNIST MLP
Figure 2: Accuracy vs. compression for post-training pruning on CIFAR10, Fashion-MNIST, and
MNIST. The left plot in each subfigure shows the average accuracy across 10 seeds, shading shows
the standard deviation. For BMRS, we mark the maximum compression rate based on when ‚àÜF‚â•0.
The right plot in each subfigure shows a scatter plot and kernel density estimation of accuracy vs.
compression of BMRS compared to SNR accuracy. BMRS Nand BMRS U-8 consistently stop pruning
near the knee point, a preferred trade-off solution.
SNR
Etheta
L2
BMRS_N
BMRS_U-8
BMRS_U-4SNR
Etheta
L2
BMRS_N
BMRS_U-8
BMRS_U-41 1 0.86 0.95 0.47 0.27
1 1 0.86 0.95 0.47 0.27
0.86 0.86 1 0.81 0.33 0.21
0.95 0.95 0.81 1 0.49 0.32
0.47 0.47 0.33 0.49 1 0.8
0.27 0.27 0.21 0.32 0.8 1
(a) CIFAR10 Lenet5
SNR
Etheta
L2
BMRS_N
BMRS_U-8
BMRS_U-4SNR
Etheta
L2
BMRS_N
BMRS_U-8
BMRS_U-41 1 0.37 0.71 -0.39 -0.41
1 1 0.37 0.71 -0.39 -0.41
0.37 0.37 1 0.29 -0.13 -0.14
0.71 0.71 0.29 1-0.12 -0.12
-0.39 -0.39 -0.13 -0.12 1 0.99
-0.41 -0.41 -0.14 -0.12 0.99 1 (b) CIFAR10 MLP
Figure 3: Average Spearman‚Äôs rank correlation
between the ranks of neurons for pruning when
using different methods on CIFAR10 (plots for
additional datasets are given in Appendix E).
2 4 6 8 10 12 14 16
p166676869Accuracy
0204060
Compression %
Accuracy
Compression %Figure 4: Accuracy and compression rate vs. p1
for BMRS Uon CIFAR10 with Lenet5. Results
are averaged across 10 seeds with standard devia-
tion indicated by the error bars.
Continuous pruning. Next, we experiment with continuous pruning, where neurons are pruned
continuously throughout training based on either a provided pruning threshold (SNR, Eqœï[Œ∏]) or‚àÜF
(BMRS). For SNR, we prune a neuron when its SNR falls below 1 (as in [ 30]), and for Eqœï[Œ∏]we set
a threshold of 0.1. For L2 pruning, we perform post-training pruning based on the compression rate
8Table 1: Parameter compression % and accuracy for different baseline methods and settings of the
proposed method. Standard deviations over ten runs are included. Best accuracy for the compression
methods is given in bold.
MNIST Fash-MNIST CIFAR10
Pruning Method Comp. (%) Acc. Comp. (%) Acc. Comp. (%) Acc.
MLP
None 0.00¬±0.00 97 .43¬±0.14 0 .00¬±0.00 88 .17¬±0.20 0 .00¬±0.00 44 .94¬±0.40
L2 43.11¬±2.06 10 .39¬±0.32 87 .86¬±2.27 18 .23¬±10.22 42 .89¬±2.64 10 .00¬±0.00
E[Œ∏] 52 .08¬±1.71 96 .88¬±0.15 91 .76¬±0.81 85 .59¬±0.26 77 .99¬±1.54 43 .39¬±0.46
SNR 58.57¬±2.01 96 .92¬±0.08 99 .83¬±0.00 10 .00¬±0.00 75 .93¬±1.26 43 .97¬±0.46
BMRS N 48.86¬±1.32 96 .95¬±0.19 93 .20¬±0.66 84 .99¬±0.35 76 .36¬±1.08 43 .59¬±0.29
BMRS U-8 48.73¬±1.90 96 .93¬±0.16 93 .02¬±0.81 85 .01¬±0.32 77 .17¬±0.98 43 .45¬±0.42
BMRS U-4 54.47¬±1.74 96.99¬±0.13 91.57¬±0.71 85.79¬±0.34 76.63¬±0.94 44.06¬±0.40
Lenet5
None 0.00¬±0.00 99 .07¬±0.09 0 .00¬±0.00 89 .16¬±0.27 0 .00¬±0.00 67 .62¬±0.77
L2 83.42¬±1.92 11 .35¬±0.00 83 .62¬±1.69 10 .00¬±0.00 52 .29¬±2.18 10 .00¬±0.00
E[Œ∏] 88 .29¬±1.00 51 .30¬±41.12 89 .71¬±0.56 50 .93¬±33.45 66 .19¬±1.36 65 .83¬±0.90
SNR 92.66¬±5.77 62 .70¬±41.93 98 .47¬±3.45 17 .01¬±21.03 70 .29¬±2.02 67.68¬±0.52
BMRS N 86.90¬±1.15 95 .59¬±0.94 88 .02¬±1.00 77 .90¬±2.44 62 .87¬±1.64 66 .14¬±0.70
BMRS U-8 86.11¬±1.37 95 .27¬±1.02 87 .61¬±0.72 77 .23¬±3.49 62 .54¬±1.49 66 .28¬±1.07
BMRS U-4 87.58¬±1.01 96.66¬±0.59 88.72¬±0.73 81.10¬±1.50 68.07¬±1.95 67 .66¬±0.59
achieved by BMRS N. Neurons are pruned after every epoch during training, followed by 10 epochs
of fine-tuning at the very end of training.
We compare the raw performance of each variant using an MLP and Lenet5 on MNIST, Fashion-
MNIST, and CIFAR10 in Table 1. First, we note that using the L2 norm with the same compression
rate as BMRS Nresults in a degenerate model; the accuracy degrades to random in all settings.
Additionally, we see that using the SNR as a pruning criterion with the recommended threshold of 1
from [ 30] is also inconsistent, resulting in large drops in performance for 3 out of 6 settings. BMRS N
and BMRS Uresult in both high compression rate and high performance in all settings. BMRS N
accomplishes this without the need for tuning any pruning thresholds, in one case yielding a higher
compression rate than BMRS U-4 while keeping the accuracy high. BMRS U-4 results in both the
highest compression rate among the three BMRS variants in 4 out of 6 settings, and the highest
accuracy in 5 out of 6 settings, with the caveat of needing to select p1as a hyperparameter. To explore
the effect of this hyperparameter further, we plot accuracy and compression vs. p1for Lenet5 trained
on CIFAR10 in Figure 4. We see that compression rapidly increases after p1= 11 , continuing until
p1= 1. Additionally, we find that accuracy also steadily increases with a higher compression rate,
indicating that BMRS Ureduces complexity while increasing the generalization capacity of the model.
Finally, a comparison of ResNet-50 and ViT on CIFAR10 and TinyImagenet is given in Table 2.
Here, SNR and the three BMRS variants achieve similar accuracies at different compression rates.
BMRS Nachieves a modest compression rate compared to SNR with a threshold of 1 for each case.
BMRS U-4 yields a higher compression rate than BMRS Nand BMRS U-8 in all settings. As such, we
show that BMRS Nis capable of achieving high compression with no threshold tuning, while a more
extreme compression rate is possible by selecting p1for BMRS U.
6 Discussion and conclusion
Our experimental results demonstrate the pruning characteristics of BMRS in two settings: continuous
pruning and post-training pruning. One of the benefits of BMRS, and BMRS Nin particular, is that
no threshold tuning is needed, making it particularly useful for the continuous pruning setting.
Here, the compression rate improves as the model converges, allowing one to gradually increase the
compression rate without sacrificing accuracy. The choice of BMRS Nvs. BMRS Uis then dependent
on the problem, where more complex scenarios (e.g., over-parameterized models) are suitable for
BMRS Uif a higher compression rate is desired. In this case, a hyperparameter search over p1in
the range [œµ,23]can be done to select BMRS U(see Figure 4 for an example), or one can simply
use BMRS Nto achieve good compression with no hyperparameter tuning. Additionally, we expect
that the more over-specified the model is, the lower we can set p1and maintain accuracy (i.e., lower
bit-rate).
9Table 2: Parameter compression % and accuracy for different baseline methods and settings of
the proposed method. Standard deviations over three runs are included. Best accuracy for the
compression methods is given in bold.
CIFAR10 TinyImagenet
Pruning Method Comp. % Acc. Comp. % Acc.
Res50-Pretrained
None 0.00¬±0.00 90 .65¬±0.05 0 .00¬±0.00 53 .01¬±0.35
L2 63.79¬±4.21 10 .00¬±0.00 55 .71¬±1.08 0 .50¬±0.00
E[Œ∏] 89 .85¬±0.09 16 .27¬±1.86 83 .59¬±1.50 21 .11¬±6.47
SNR 91.73¬±0.16 89 .24¬±0.40 77 .85¬±0.14 50 .54¬±0.59
BMRS N 87.10¬±1.55 89.62¬±0.41 74.98¬±0.16 50 .56¬±0.33
BMRS U-8 88.18¬±0.22 89 .29¬±0.20 74 .99¬±0.04 50.84¬±0.34
BMRS U-4 89.85¬±0.05 89 .26¬±0.28 76 .12¬±0.13 50 .82¬±0.50
Vision Transformer
None 0.00¬±0.00 94 .80¬±0.17 0 .00¬±0.00 63 .14¬±0.42
L2 57.74¬±0.26 54 .36¬±1.84 47 .47¬±0.12 7 .08¬±0.57
E[Œ∏] 68 .18¬±0.09 10 .00¬±0.00 57 .08¬±0.18 0 .50¬±0.00
SNR 73.03¬±0.03 94 .78¬±0.10 62 .75¬±0.04 64 .60¬±0.07
BMRS N 57.74¬±0.25 94 .60¬±0.01 47 .48¬±0.12 65 .00¬±0.21
BMRS U-8 58.14¬±0.11 94 .69¬±0.04 47 .34¬±0.14 65.14¬±0.13
BMRS U-4 67.16¬±0.21 94.83¬±0.25 56.13¬±0.14 65 .13¬±0.10
Limitations: We note a few of the limitations of BMRS. First, while multiplicative noise pruning
allows for the flexible application of pruning at different structural levels, BNNs may offer more
aggressive compression rates as they apply sparsity inducing priors at multiple hierarchical levels [ 2,
15,16,33,29]. BMR based approaches may be derived for such networks; as of this work and
as far we we know, it has only been successfully applied in practice to models with flat priors for
unstructured pruning [ 5,29]. Additionally, multiplicative noise creates an overhead of additional
parameters œïwhich increase the training time and storage requirements. Third, more exhaustive
baseline pruning criteria could be used for comparison in the future, for example classic methods
based on the Hessian or gradient of weights (e.g. see Figure 5 in Appendix E) [ 23,12]. Fourth,
we apply multiplicative noise to linear layers and convolutional filters, while it could be useful to
explore pruning more complex structures in the future. Finally, while structured pruning can reduce
the inference time and energy consumption of neural networks, improvements in efficiency have been
shown to have potential negative consequences in terms of energy consumption and carbon emissions
based on how efficiency can affect how a model is used in practice [35].
Conclusions: In this work we presented BMRS, an efficiently calculable method for threshold-free
structured pruning of neural networks. We derived two versions of BMRS: BMRS Nbased on the
truncated log-normal prior, and BMRS Ubased on a reduced truncated log-uniform prior. BMRS offers
several key features over existing work: by basing the method off of the approach of multiplicative
noise [ 30], the structured pruning aspect is flexible as it is not dependent on assuming any prior over
individual weights and can be easily applied at any structural level [ 16,2]. Additionally, the prior and
variational posterior in the multiplicative noise approach lend themselves to the derivation of BMRS
using multiple reduced priors which have different pruning properties, allowing for flexibility in the
compression rate when desired and threshold free pruning otherwise. Finally, our experimental results
demonstrate the competitive compression and accuracy of BMRS compared to baseline compression
methods on multiple networks of varying complexity and across multiple datasets. The methods
presented here based on BMR could form a template for developing more aggressive pruning schemes
by incorporating more complex hierarchical priors on both structures and individual weights, as well
as for studying limits on the number of neural network structures required to solve a given dataset.
Acknowledgments DW, CI and RS are partly funded by European Union‚Äôs Horizon Europe Research
and Innovation Programme under grant agreements No. 101070284 and No. 101070408. DW is also
partly supported by a Danish Data Science Academy postdoctoral fellowship (grant: 2023-1425).
10References
[1]L. F. W. Anthony, B. Kanding, and R. Selvan. Carbontracker: Tracking and predicting the carbon
footprint of training deep learning models. In ICML Workshop on Challenges in Deploying and
Monitoring Machine Learning Systems , 2020.
[2]J. Bai, Q. Song, and G. Cheng. Efficient variational inference for sparse deep learning with
theoretical guarantee. In Advances in Neural Information Processing Systems (NeurIPS) , 2020.
[3]B. R. Bartoldson, B. Kailkhura, and D. Blalock. Compute-efficient deep learning: Algorithmic
trends and opportunities. Journal of Machine Learning Research , 24:122‚Äì1, 2023.
[4]M. J. Beal. Variational algorithms for approximate Bayesian inference . University of London,
University College London (United Kingdom), 2003.
[5]J. Beckers, B. Van Erp, Z. Zhao, K. Kondrashov, and B. De Vries. Principled pruning of
bayesian neural networks through variational free energy minimization. IEEE Open Journal of
Signal Processing , 2024.
[6]K. Friston, T. Parr, and P. Zeidman. Bayesian model reduction. arXiv:1805.07092 [stat.ME],
2018.
[7]K. J. Friston and W. D. Penny. Post hoc Bayesian model selection. NeuroImage , 56(4):
2089‚Äì2099, 2011.
[8]K. J. Friston, V . Litvak, A. Oswal, A. Razi, K. E. Stephan, B. C. M. van Wijk, G. Ziegler,
and P. Zeidman. Bayesian model reduction and empirical bayes for group (DCM) studies.
NeuroImage , 128:413‚Äì431, 2016.
[9]K. J. Friston, M. Lin, C. D. Frith, G. Pezzulo, J. A. Hobson, and S. Ondobaka. Active inference,
curiosity and insight. Neural Computation , 29(10):2633‚Äì2683, 2017.
[10] S. Ghosh, J. Yao, and F. Doshi-Velez. Model selection in bayesian neural networks via horseshoe
priors. Journal of Machine Learning Research , 20:182:1‚Äì182:46, 2019.
[11] R. W. Hamming. On the distribution of numbers. The Bell System Technical Journal , 49(8):
1609‚Äì1625, 1970.
[12] B. Hassibi and D. G. Stork. Second order derivatives for network pruning: Optimal brain
surgeon. In Advances in Neural Information Processing Systems (NeurIPS) , pages 164‚Äì171.
Morgan Kaufmann, 1992.
[13] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In Computer
Vision and Pattern Recognition (CVPR) , pages 770‚Äì778, 2016.
[14] P. Henderson, J. Hu, J. Romoff, E. Brunskill, D. Jurafsky, and J. Pineau. Towards the Systematic
Reporting of the Energy and Carbon Footprints of Machine Learning. Journal of Machine
Learning Research , 21(1):10039‚Äì10081, 2020.
[15] A. Hubin and G. Storvik. Variational inference for Bayesian neural networks under model and
parameter uncertainty. arXiv:2305.00934 [stat.ML], 2023.
[16] S. R. Jantre, S. Bhattacharya, and T. Maiti. Layer adaptive node selection in bayesian neural
networks: Statistical guarantees and implementation details. Neural Networks , 167:309‚Äì330,
2023.
[17] Y . Jeon and J. Kim. Constructing fast network through deconstruction of convolution. In
Advances in Neural Information Processing Systems (NeurIPS) , pages 5955‚Äì5965, 2018.
[18] S. J. Kiebel, M. I. Garrido, R. J. Moran, and K. J. Friston. Dynamic causal modelling for EEG
and MEG. Cognitive Neurodynamics , 2:121‚Äì136, 2008.
[19] D. P. Kingma and M. Welling. Auto-encoding variational bayes. In International Conference
on Learning Representation (ICLR) , 2014.
11[20] D. P. Kingma, T. Salimans, and M. Welling. Variational dropout and the local reparameterization
trick. In Advances in Neural Information Processing Systems (NeurIPS) , pages 2575‚Äì2583,
2015.
[21] A. Krizhevsky. Learning multiple layers of features from tiny images. Technical report,
University of Toronto, 2009.
[22] Y . LeCun. The MNIST database of handwritten digits. http://yann.lecun.com/exdb/
mnist/ , 1998.
[23] Y . LeCun, J. S. Denker, and S. A. Solla. Optimal brain damage. In Advances in Neural
Information Processing Systems (NeurIPS) , pages 598‚Äì605. Morgan Kaufmann, 1989.
[24] Y . LeCun, L. Bottou, Y . Bengio, and P. Haffner. Gradient-based learning applied to document
recognition. Proceedings of the IEEE , 86(11):2278‚Äì2324, 1998.
[25] H. Li, A. Kadav, I. Durdanovic, H. Samet, and H. P. Graf. Pruning filters for efficient convnets.
InInternational Conference on Learning Representation (ICLR) , 2017.
[26] J. Li, Z. Miao, Q. Qiu, and R. Zhang. Training bayesian neural networks with sparse subspace
variational inference. In The Twelfth International Conference on Learning Representations,
ICLR , 2024.
[27] T. Liang, J. Glossner, L. Wang, S. Shi, and X. Zhang. Pruning and quantization for deep neural
network acceleration: A survey. Neurocomputing , 461:370‚Äì403, 2021.
[28] C. Louizos, K. Ullrich, and M. Welling. Bayesian compression for deep learning. In Advances
in Neural Information Processing Systems (NeurIPS) , pages 3288‚Äì3298, 2017.
[29] D. Markovic, K. J. Friston, and S. J. Kiebel. Bayesian sparsification for deep neural networks
with Bayesian model reduction. arXiv:2309.12095 [stat.ML], 2023.
[30] K. Neklyudov, D. Molchanov, A. Ashukha, and D. P. Vetrov. Structured bayesian pruning
via log-normal multiplicative noise. In Advances in Neural Information Processing Systems
(NeurIPS) , pages 6775‚Äì6784, 2017.
[31] H. Peng, Q. Cao, J. Dodge, M. E. Peters, J. Fernandez, T. Sherborne, K. Lo, S. Skjonsberg,
E. Strubell, D. Plessas, I. B. end Evan Pete Walsh, N. A. Smith, and H. Hajishirzi. Efficiency
Pentathlon: A standardized arena for efficiency evaluation. arXiv:2307.09701 [cs.CL], 2023.
[32] P. Sedgwick. Spearman‚Äôs rank correlation coefficient. BMJ , 349, 2014.
[33] Y . Sun, Q. Song, and F. Liang. Learning sparse deep neural networks with a spike-and-slab
prior. Statistics & Probability Letters , 180:109246, 2022.
[34] N. Thompson, K. Greenewald, K. Lee, and G. F. Manso. The Computational Limits of Deep
Learning. In Workshop on Computing within Limits , 2023.
[35] D. Wright, C. Igel, G. Samuel, and R. Selvan. Efficiency is not enough: A critical perspective
of environmentally sustainable AI. arXiv:2309.02065 [cs.LG], 2023.
[36] B. Wu, C. Xu, X. Dai, A. Wan, P. Zhang, Z. Yan, M. Tomizuka, J. Gonzalez, K. Keutzer, and
P. Vajda. Visual transformers: Token-based image representation and processing for computer
vision. arXiv:2006.03677 [cs.CV], 2020.
[37] H. Xiao, K. Rasul, and R. V ollgraf. Fashion-MNIST: a novel image dataset for benchmarking
machine learning algorithms. arXiv:1708.07747 [cs.LG], 2017.
12A Derivations
We use the following notation and distributions:
Œ¶(x) =1
2
1 +erfx‚àö
2
(CDF of N(0,1)evaluated at x)
Œ±p=a‚àí¬µp
œÉp
Œ≤p=b‚àí¬µp
œÉp
Zp= Œ¶(Œ≤p)‚àíŒ¶(Œ±p)
LogN[a,b](Œ∏|¬µp, œÉ2
p) =(1
ZpŒ∏‚àö
2œÄœÉ2pexpn
‚àí1
2(logŒ∏‚àí¬µp)2
œÉ2po
a‚â§Œ∏‚â§b
0, otherwise
LogU[a,b](Œ∏) =( 
Œ∏logb
a‚àí1, a‚â§Œ∏‚â§b
0, otherwise
qœï(Œ∏) =LogN[a,b](Œ∏|¬µq, œÉ2
q)
Àúp(Œ∏) =LogN[a,b](Œ∏|Àú¬µp,ÀúœÉ2
p)
p(Œ∏) =LogU[a,b](Œ∏)
A.1 BMRS N
We start by finding qœï(Œ∏)Àúp(Œ∏)
p(Œ∏).
1. First we look at
qœï(Œ∏)Àúp(Œ∏) =1
Œ∏22œÄZqÀúZpq
œÉ2qÀúœÉ2pexp
‚àí1
2(logŒ∏‚àí¬µq)2
œÉ2q+(logŒ∏‚àíÀú¬µp)2
ÀúœÉ2p
.
The exponent can be rewritten as
‚àí1
2(logŒ∏‚àí¬µq)2
œÉ2q+(logŒ∏‚àíÀú¬µp)2
ÀúœÉ2p
=
‚àí1
2ÀúœÉ2
p(log2Œ∏‚àí2 logŒ∏¬µq+¬µ2
q) +œÉ2
q(log2Œ∏‚àí2 logŒ∏Àú¬µp+ Àú¬µ2
p)
œÉ2qÀúœÉ2p=
‚àí1
2(œÉ2
q+ ÀúœÉ2
p)
logŒ∏‚àíœÉ2
qÀúœÉ2
p
œÉ2q+ÀúœÉ2p
¬µq
œÉ2q+Àú¬µp
ÀúœÉ2p2
+ ÀúœÉ2
p¬µ2
q+œÉ2
qÀú¬µ2
p‚àí(œÉ2
q+ ÀúœÉ2
p)œÉ2
qÀúœÉ2
p
œÉ2q+ÀúœÉ2p
¬µq
œÉ2q+Àú¬µp
ÀúœÉ2p2
œÉ2qÀúœÉ2p.
Defining
ÀúœÉ2
q:=œÉ2
qÀúœÉ2
p
œÉ2q+ ÀúœÉ2p=1
œÉ2q+1
ÀúœÉ2p‚àí1
andÀú¬µq:= ÀúœÉ2
q¬µq
œÉ2q+Àú¬µp
ÀúœÉ2p
we get
qœï(Œ∏)Àúp(Œ∏) =1
Œ∏22œÄZqÀúZpq
œÉ2qÀúœÉ2pexp
‚àí1
2(logŒ∏‚àíÀú¬µq)2
ÀúœÉ2q
exp(
‚àí1
2 
¬µ2
q
œÉ2q+Àú¬µ2
p
ÀúœÉ2p‚àíÀú¬µ2
q
ÀúœÉ2q!)
=Œ∏ÀúZqq
2œÄÀúœÉ2q
Œ∏22œÄZqÀúZpq
œÉ2qÀúœÉ2p1
Œ∏ÀúZqq
2œÄÀúœÉ2qexp
‚àí1
2(logŒ∏‚àíÀú¬µq)2
ÀúœÉ2q
exp(
‚àí1
2 
¬µ2
q
œÉ2q+Àú¬µ2
p
ÀúœÉ2p‚àíÀú¬µ2
q
ÀúœÉ2q!)
=ÀúZqq
ÀúœÉ2q
Œ∏ZqÀúZpq
2œÄœÉ2qÀúœÉ2pexp(
‚àí1
2 
¬µ2
q
œÉ2q+Àú¬µ2
p
ÀúœÉ2p‚àíÀú¬µ2
q
ÀúœÉ2q!)
LogN[a,b](Œ∏|Àú¬µq,ÀúœÉ2
q).
132. Then we divide out p(Œ∏):
qœï(Œ∏)Àúp(Œ∏)
p(Œ∏)=(logb‚àíloga)ÀúZqq
ÀúœÉ2q
ZqÀúZpq
2œÄœÉ2qÀúœÉ2pexp(
‚àí1
2 
¬µ2
q
œÉ2q+Àú¬µ2
p
ÀúœÉ2p‚àíÀú¬µ2
q
ÀúœÉ2q!)
LogN[a,b](Œ∏|Àú¬µq,ÀúœÉ2
q)
3. Now we can find Àúq(Œ∏)and‚àÜFusing Equation 6. Start with Àúq(Œ∏):
Àúq(Œ∏) =qœï(Œ∏)Àúp(Œ∏)
p(Œ∏)
exp ‚àÜ F
=qœï(Œ∏)Àúp(Œ∏)
p(Œ∏)R
qœï(Œ∏)Àúp(Œ∏)
p(Œ∏)dŒ∏
=(logb‚àíloga)ÀúZq‚àö
ÀúœÉ2q
ZqÀúZp‚àö
2œÄœÉ2qÀúœÉ2pexpn
‚àí1
2¬µ2
q
œÉ2q+Àú¬µ2
p
ÀúœÉ2p‚àíÀú¬µ2
q
ÀúœÉ2qo
LogN[a,b](Œ∏|Àú¬µq,ÀúœÉ2
q)
Z(logb‚àíloga)ÀúZqq
ÀúœÉ2q
ZqÀúZpq
2œÄœÉ2qÀúœÉ2pexp(
‚àí1
2 
¬µ2
q
œÉ2q+Àú¬µ2
p
ÀúœÉ2p‚àíÀú¬µ2
q
ÀúœÉ2q!)
LogN[a,b](Œ∏|Àú¬µq,ÀúœÉ2
q)dŒ∏
=(logb‚àíloga)ÀúZq‚àö
ÀúœÉ2q
ZqÀúZp‚àö
2œÄœÉ2qÀúœÉ2pexpn
‚àí1
2¬µ2
q
œÉ2q+Àú¬µ2
p
ÀúœÉ2p‚àíÀú¬µ2
q
ÀúœÉ2qo
LogN[a,b](Œ∏|Àú¬µq,ÀúœÉ2
q)
(logb‚àíloga)ÀúZq‚àö
ÀúœÉ2q
ZqÀúZp‚àö
2œÄœÉ2qÀúœÉ2pexpn
‚àí1
2¬µ2q
œÉ2q+Àú¬µ2p
ÀúœÉ2p‚àíÀú¬µ2q
ÀúœÉ2qoR
LogN[a,b](Œ∏|Àú¬µq,ÀúœÉ2q)dŒ∏
=LogN[a,b](Œ∏|Àú¬µq,ÀúœÉ2
q)
4. Finally we get ‚àÜF:
‚àÜF= logqœï(Œ∏)Àúp(Œ∏)
p(Œ∏)
Àúq(Œ∏)
= log(logb‚àíloga)ÀúZqq
ÀúœÉ2q
ZqÀúZpq
2œÄœÉ2qÀúœÉ2pexp(
‚àí1
2 
¬µ2
q
œÉ2q+Àú¬µ2
p
ÀúœÉ2p‚àíÀú¬µ2
q
ÀúœÉ2q!)
= logÀúZq(logb‚àíloga)
ZqÀúZp+1
2logÀúœÉ2
q
2œÄœÉ2qÀúœÉ2p‚àí1
2 
¬µ2
q
œÉ2q+Àú¬µ2
p
ÀúœÉ2p‚àíÀú¬µ2
q
ÀúœÉ2q!
A.2 BMRS U
The PDF of the reduced truncated log-uniform distribution is given as follows:
Àúp(Œ∏) =LogU[a‚Ä≤,b‚Ä≤](Œ∏) =(
Œ∏logb‚Ä≤
a‚Ä≤‚àí1
, a‚â§a‚Ä≤< b‚Ä≤‚â§b
0, otherwise(14)
Using this, we can directly solve the integral under the expectation given in Equation 6 for ‚àÜF
exp ‚àÜ F=EÀúpqœï(Œ∏)
p(Œ∏)
=Zb
aLogU[a‚Ä≤,b‚Ä≤](Œ∏)qœï(Œ∏)
LogU[a,b](Œ∏)dŒ∏=Zb
aŒ∏logb
a
Œ∏logb‚Ä≤
a‚Ä≤qœï(Œ∏)dŒ∏
=Za‚Ä≤
a0dŒ∏+Zb‚Ä≤
a‚Ä≤logb
a
logb‚Ä≤
a‚Ä≤qœï(Œ∏)dŒ∏+Zb
b‚Ä≤0dŒ∏=logb
a
logb‚Ä≤
a‚Ä≤qœï(a‚Ä≤‚â§Œ∏i‚â§b‚Ä≤)
Plugging this in to Equation 5 where exp ‚àÜ F‚â•1:
1‚â§logb
a
logb‚Ä≤
a‚Ä≤qœï(a‚Ä≤‚â§Œ∏i‚â§b‚Ä≤)‚áílogb‚Ä≤
a‚Ä≤
logb
a‚â§qœï(a‚Ä≤‚â§Œ∏‚â§b‚Ä≤) (15)
14B Dataset details
MNIST MNIST [ 22] is a classic image classification dataset consisting of 70,000 28x28 black and
white images of handwritten digits (10 classes). We use the original 10,000 image test set for testing
and split the 60,000 image train set into 80% training and 20% validation images.
Fashion-MNIST Fashion-MNIST [ 37] is a modernized version of MNIST using images of different
articles of clothing as opposed to handwritten digits. The dataset statistics are the same as MNIST:
28x28 greyscale images, 60,000 training images, 10,000 test images, 10 classes. Similar to MNIST,
we split the training set into 80% training and 20% validation images.
CIFAR10 CIFAR10 [ 21] is an image classification dataset of 32x32 color images with 10 classes.
There are 50,000 training images and 10,000 test images. Again, we split the training set to 80%
training and 20% validation.
TinyImagenet TinyImagenet is a reduced version of ImageNet consisting of 110,000 64x64 color
images in 200 classes. We use the 10,000 image validation split for testing, and split the 100,000
image train set into 80% training and 20% validation images.
C Model details
For each model and dataset we use the Adam optimizer with no weight decay. We train for 50 epochs
for each experiment with an MLP and Lenet5, and for 100 epochs for each experiment with Resnet50
and ViT. Further details about each model are given as follows:
C.1 MLP
We use a multilayer perceptron (MLP) for several experiments, with different network sizes based on
a hyperparameter sweep for each dataset. Multiplicative noise for pruning is applied to every neuron
in the network. We sweep through the following hyperparmeters:
‚Ä¢ Number of layers = {1,3,5,7,9}
‚Ä¢ Hidden dimension = {10, 30, 50, 100, 150}
‚Ä¢ Batch size = {16, 32, 64, 128}
‚Ä¢ Learning rate [0.0001, 0.1].
The final network settings for each dataset are given as follows:
MNIST: Number of layers: 7; Hidden dimension: 100; Batch size: 128; Learning rate: 8.5 ¬∑10‚àí4.
Fashion-MNIST Number of layers: 1; Hidden dimension: 150; Batch size: 128; Learning rate:
1.5¬∑10‚àí3.
CIFAR10 Number of layers: 5; Hidden dimension: 150; Batch size: 32; Learning rate: 6.8 ¬∑10‚àí4.
C.2 Lenet5
Lenet5 [ 24] is an early CNN architecture consisting of 2 convolutional layers with 6 and 16 filters
per channel, respectively, each followed by a ReLU activation and max pooling layer, followed by 3
linear layers. Multiplicative noise is applied to each convolutional filter map, as well as each neuron
the the linear layers. We use the same architecture for each experiments and tune hyperparameters
based on the dataset. We sweep through the following hyperparameters:
‚Ä¢ Batch size = {16, 32, 64, 128}
‚Ä¢ Learning rate [0.0001, 0.1].
The final settings for each dataset are given as follows:
MNIST: Batch size 128; Learning rate 1.4 ¬∑10‚àí3.
15Table 3: Training and inference runtimes in milliseconds. Each runtime is averaged across 1000
forward passes of a batch of 32 images. Inference runtimes are before pruning (i.e., with the full
network).
MNIST Fash-MNIST CIFAR10
Pruning Method Train Inf. Train Inf. Train Inf.
MLP
None 2.41 0 .73 2.43 0 .73 4.61 5 .30
E[Œ∏] 19 .76 1 .17 20.00 1 .14 21.21 5 .39
SNR 20.00 1 .14 19.99 1 .15 22.00 4 .94
BMRS N 20.02 1 .14 20.14 1 .15 21.76 5 .57
BMRS U 19.99 1 .16 20.11 1 .16 22.14 5 .15
Lenet5
None 2.31 0 .69 2.24 0 .69 4.73 5 .15
E[Œ∏] 11 .45 0 .94 11.34 0 .94 12.11 4 .90
SNR 11.18 0 .93 11.42 0 .94 15.05 4 .95
BMRS N 11.39 0 .94 11.47 0 .94 13.34 5 .07
BMRS U 11.44 0 .94 11.08 0 .93 12.86 4 .43
Fashion-MNIST Batch size 32; Learning rate 1.4 ¬∑10‚àí3.
CIFAR10 Batch size 64; Learning rate 1 ¬∑10‚àí3.
C.3 Resnet50
Resnet50 [ 13] is a deep 50-layer CNN which uses residual connections to stabilize optimization
and improve accuracy. We start with a model pretrained on ImageNet-1k,3then fine-tuned on the
downstream dataset with pruning layers added to each output layer after batch normalization. We
use a learning rate of 6.8e-4, a batch size of 32, and train for 100 epochs for both CIFAR10 and
TinyImagenet.
C.4 Vision Transformer (ViT)
Vision Transformer (ViT) [ 36] is a transformer model tailored for image data based on tokenizing an
image as 16x16 image patches. We use a ViT which is pretrained on ImageNet-21k (14M images
and 21,843 classes) as well as ImageNet-1k.4We add multiplicative noise to the output layer of each
transformer block for pruning. We use a learning rate of 6.8e-4, a batch size of 32, and train for 100
epochs for both CIFAR10 and TinyImagenet.
D Compute resources
All experiments were run on a shared cluster. Requested jobs consisted of 16GB of RAM and 4
Intel Xeon Silver 4110 CPUs. We used a single NVIDIA Titan X GPU with 24GB of RAM for all
experiments, though utilization was generally much lower due the the average size of each network.
Runtimes for each experiment ranged from approx. 7 minutes for Lenet5 on MNIST with no pruning
layers to approx. 44 hours for ViT on TinyImagenet with multiplicative noise trained for 100 epochs.
The training of models in this work over the course of the entire project (prototyping, experimentation,
etc.) is estimated to have used 3773.785 kWh of electricity contributing to 599.892 kg of CO2eq (as
measured by carbontracker [1]; this is equivalent to 5580.395 km travelled by car).
We additionally benchmark the training and inference runtimes of each model before pruning in
Table 3 and Table 4 (i.e., the inference runtimes are without removing structures).
3https://pytorch.org/vision/stable/models.html
4https://huggingface.co/google/vit-base-patch16-224
16Table 4: Training and inference runtimes in milliseconds. Each runtime is averaged across 1000
forward passes of a batch of 32 images. Inference runtimes are before pruning (i.e., with the full
network).
CIFAR10 TinyImagenet
Pruning Method Train Inf. Train Inf.
Res50-Pretrained
None 47.78 34 .30 74.89 59 .11
E[Œ∏] 329 .11 35 .24 346.80 64 .02
SNR 328.29 33 .97 346.56 64 .00
BMRS N 329.84 32 .99 345.63 64 .01
BMRS U-4 329.81 34 .85 346.78 64 .06
Vision Transformer
None 254.50 87 .15 252.91 86 .70
E[Œ∏] 343 .51 88 .80 351.19 87 .49
SNR 343.50 88 .92 350.93 87 .55
BMRS N 343.55 88 .74 350.99 87 .55
BMRS U-4 343.60 88 .56 351.39 87 .47
E Additional plots
An additional experiment for post-training pruning including gradient-based thersholding for pruning
is given in Figure 5. For gradient based pruning, we use two thresholds: one for the L2-Norm
(magnitude) of structures, and one for the L2-Norm of the gradients of weights in a given structure.
The neuron rank correlations for Fashion-MNIST are given in Figure 6. and for MNIST in Figure 7.
170 50 100
Compression %0.00.20.40.6AccuracyL2
Grad
SNR
Etheta
BMRS_N
BMRS_U-8
BMRS_U-4
55 60 65 70 75
Compression %0.30.40.50.60.7AccuracySNR
BMRS_N
BMRS_U-8
BMRS_U-4
(a) CIFAR10 Lenet5
0 50 100
Compression %0.10.20.30.40.5AccuracyL2
Grad
SNR
Etheta
BMRS_N
BMRS_U-8
BMRS_U-4
75 80 85
Compression %0.350.400.450.50AccuracySNR
BMRS_N
BMRS_U-8
BMRS_U-4
 (b) CIFAR10 MLP
0 50 100
Compression %0.00.20.40.60.8AccuracyL2
Grad
SNR
Etheta
BMRS_N
BMRS_U-8
BMRS_U-4
85 90 95
Compression %0.00.20.40.60.8AccuracySNR
BMRS_N
BMRS_U-8
BMRS_U-4
(c) Fashion-MNIST Lenet5
0 50 100
Compression %0.20.40.60.8AccuracyL2
Grad
SNR
Etheta
BMRS_N
BMRS_U-8
BMRS_U-4
90 95 100
Compression %0.00.20.40.60.8AccuracySNR
BMRS_N
BMRS_U-8
BMRS_U-4
 (d) Fashion-MNIST MLP
0 50 100
Compression %0.00.20.40.60.81.0AccuracyL2
Grad
SNR
Etheta
BMRS_N
BMRS_U-8
BMRS_U-4
80 85 90 95
Compression %0.00.20.40.60.81.0AccuracySNR
BMRS_N
BMRS_U-8
BMRS_U-4
(e) MNIST Lenet5
0 50 100
Compression %0.00.20.40.60.81.0AccuracyL2
Grad
SNR
Etheta
BMRS_N
BMRS_U-8
BMRS_U-4
50 60
Compression %0.900.951.001.05AccuracySNR
BMRS_N
BMRS_U-8
BMRS_U-4
 (f) MNIST MLP
Figure 5: Additional accuracy vs. compression results for post-training pruning including gradient-
based pruning.
SNR
Etheta
L2
BMRS_N
BMRS_U-8
BMRS_U-4SNR
Etheta
L2
BMRS_N
BMRS_U-8
BMRS_U-41 1 0.59 0.67 -0.1 -0.45
1 1 0.59 0.67 -0.1 -0.45
0.59 0.59 1 0.34 -0.069 -0.24
0.67 0.67 0.34 1 0.2 -0.12
-0.1 -0.1 -0.069 0.2 1 0.66
-0.45 -0.45 -0.24 -0.12 0.66 1
(a) Fashion-MNIST Lenet5
SNR
Etheta
L2
BMRS_N
BMRS_U-8
BMRS_U-4SNR
Etheta
L2
BMRS_N
BMRS_U-8
BMRS_U-41 1-0.23 0.94 -0.63 -0.71
1 1-0.23 0.94 -0.63 -0.71
-0.23 -0.23 1-0.21 0.33 0.34
0.94 0.94 -0.21 1-0.59 -0.66
-0.63 -0.63 0.33 -0.59 1 0.93
-0.71 -0.71 0.34 -0.66 0.93 1 (b) Fashion-MNIST MLP
Figure 6: Average correlation between the ranks of neurons for pruning when using different methods
on Fashion-MNIST.
18SNR
Etheta
L2
BMRS_N
BMRS_U-8
BMRS_U-4SNR
Etheta
L2
BMRS_N
BMRS_U-8
BMRS_U-41 1 0.77 0.94 -0.016 -0.36
1 1 0.77 0.93 -0.015 -0.36
0.77 0.77 1 0.74 -0.089 -0.31
0.94 0.93 0.74 10.034 -0.3
-0.016 -0.015 -0.089 0.034 1 0.65
-0.36 -0.36 -0.31 -0.3 0.65 1(a) MNIST Lenet5
SNR
Etheta
L2
BMRS_N
BMRS_U-8
BMRS_U-4SNR
Etheta
L2
BMRS_N
BMRS_U-8
BMRS_U-41 1 0.48 0.93 0.17 -0.034
1 1 0.48 0.93 0.17 -0.034
0.48 0.48 1 0.44 0.39 0.23
0.93 0.93 0.44 1 0.22 0.033
0.17 0.17 0.39 0.22 1 0.8
-0.034 -0.034 0.23 0.033 0.8 1 (b) MNIST MLP
Figure 7: Average correlation between the ranks of neurons for pruning when using different methods
on MNIST.
19NeurIPS Paper Checklist
1.Claims
Question: Do the main claims made in the abstract and introduction accurately reflect the
paper‚Äôs contributions and scope?
Answer: [Yes]
Justification: The abstract accurately reflects the main claims of the paper; see the last
paragraph of ¬ß1 for the primary contributions for comparison.
Guidelines:
‚Ä¢The answer NA means that the abstract and introduction do not include the claims
made in the paper.
‚Ä¢The abstract and/or introduction should clearly state the claims made, including the
contributions made in the paper and important assumptions and limitations. A No or
NA answer to this question will not be perceived well by the reviewers.
‚Ä¢The claims made should match theoretical and experimental results, and reflect how
much the results can be expected to generalize to other settings.
‚Ä¢It is fine to include aspirational goals as motivation as long as it is clear that these goals
are not attained by the paper.
2.Limitations
Question: Does the paper discuss the limitations of the work performed by the authors?
Answer: [Yes]
Justification: The limitations are discussed after ¬ß6 in a paragraph marked ‚ÄúLimitations‚Äù.
Guidelines:
‚Ä¢The answer NA means that the paper has no limitation while the answer No means that
the paper has limitations, but those are not discussed in the paper.
‚Ä¢ The authors are encouraged to create a separate "Limitations" section in their paper.
‚Ä¢The paper should point out any strong assumptions and how robust the results are to
violations of these assumptions (e.g., independence assumptions, noiseless settings,
model well-specification, asymptotic approximations only holding locally). The authors
should reflect on how these assumptions might be violated in practice and what the
implications would be.
‚Ä¢The authors should reflect on the scope of the claims made, e.g., if the approach was
only tested on a few datasets or with a few runs. In general, empirical results often
depend on implicit assumptions, which should be articulated.
‚Ä¢The authors should reflect on the factors that influence the performance of the approach.
For example, a facial recognition algorithm may perform poorly when image resolution
is low or images are taken in low lighting. Or a speech-to-text system might not be
used reliably to provide closed captions for online lectures because it fails to handle
technical jargon.
‚Ä¢The authors should discuss the computational efficiency of the proposed algorithms
and how they scale with dataset size.
‚Ä¢If applicable, the authors should discuss possible limitations of their approach to
address problems of privacy and fairness.
‚Ä¢While the authors might fear that complete honesty about limitations might be used by
reviewers as grounds for rejection, a worse outcome might be that reviewers discover
limitations that aren‚Äôt acknowledged in the paper. The authors should use their best
judgment and recognize that individual actions in favor of transparency play an impor-
tant role in developing norms that preserve the integrity of the community. Reviewers
will be specifically instructed to not penalize honesty concerning limitations.
3.Theory Assumptions and Proofs
Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?
Answer: [Yes]
20Justification: We provide detailed derivations and describe assumptions for each new
theoretical result in ¬ß4.2 and Appendix A.
Guidelines:
‚Ä¢ The answer NA means that the paper does not include theoretical results.
‚Ä¢All the theorems, formulas, and proofs in the paper should be numbered and cross-
referenced.
‚Ä¢All assumptions should be clearly stated or referenced in the statement of any theorems.
‚Ä¢The proofs can either appear in the main paper or the supplemental material, but if
they appear in the supplemental material, the authors are encouraged to provide a short
proof sketch to provide intuition.
‚Ä¢Inversely, any informal proof provided in the core of the paper should be complemented
by formal proofs provided in appendix or supplemental material.
‚Ä¢ Theorems and Lemmas that the proof relies upon should be properly referenced.
4.Experimental Result Reproducibility
Question: Does the paper fully disclose all the information needed to reproduce the main ex-
perimental results of the paper to the extent that it affects the main claims and/or conclusions
of the paper (regardless of whether the code and data are provided or not)?
Answer: [Yes]
Justification: The full methodology is given in ¬ß3 and ¬ß4; the experimental setting is given in
¬ß5; the full dataset and model details are given in Appendix B and Appendix C, respectively.
Guidelines:
‚Ä¢ The answer NA means that the paper does not include experiments.
‚Ä¢If the paper includes experiments, a No answer to this question will not be perceived
well by the reviewers: Making the paper reproducible is important, regardless of
whether the code and data are provided or not.
‚Ä¢If the contribution is a dataset and/or model, the authors should describe the steps taken
to make their results reproducible or verifiable.
‚Ä¢Depending on the contribution, reproducibility can be accomplished in various ways.
For example, if the contribution is a novel architecture, describing the architecture fully
might suffice, or if the contribution is a specific model and empirical evaluation, it may
be necessary to either make it possible for others to replicate the model with the same
dataset, or provide access to the model. In general. releasing code and data is often
one good way to accomplish this, but reproducibility can also be provided via detailed
instructions for how to replicate the results, access to a hosted model (e.g., in the case
of a large language model), releasing of a model checkpoint, or other means that are
appropriate to the research performed.
‚Ä¢While NeurIPS does not require releasing code, the conference does require all submis-
sions to provide some reasonable avenue for reproducibility, which may depend on the
nature of the contribution. For example
(a)If the contribution is primarily a new algorithm, the paper should make it clear how
to reproduce that algorithm.
(b)If the contribution is primarily a new model architecture, the paper should describe
the architecture clearly and fully.
(c)If the contribution is a new model (e.g., a large language model), then there should
either be a way to access this model for reproducing the results or a way to reproduce
the model (e.g., with an open-source dataset or instructions for how to construct
the dataset).
(d)We recognize that reproducibility may be tricky in some cases, in which case
authors are welcome to describe the particular way they provide for reproducibility.
In the case of closed-source models, it may be that access to the model is limited in
some way (e.g., to registered users), but it should be possible for other researchers
to have some path to reproducing or verifying the results.
5.Open access to data and code
21Question: Does the paper provide open access to the data and code, with sufficient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?
Answer: [Yes]
Justification: The datasets we use are all open source, and details are given in Appendix B.
The code is made available and can be downloaded/viewed at https://github.com/
saintslab/bmrs-structured-pruning/ .
Guidelines:
‚Ä¢ The answer NA means that paper does not include experiments requiring code.
‚Ä¢Please see the NeurIPS code and data submission guidelines ( https://nips.cc/
public/guides/CodeSubmissionPolicy ) for more details.
‚Ä¢While we encourage the release of code and data, we understand that this might not be
possible, so ‚ÄúNo‚Äù is an acceptable answer. Papers cannot be rejected simply for not
including code, unless this is central to the contribution (e.g., for a new open-source
benchmark).
‚Ä¢The instructions should contain the exact command and environment needed to run to
reproduce the results. See the NeurIPS code and data submission guidelines ( https:
//nips.cc/public/guides/CodeSubmissionPolicy ) for more details.
‚Ä¢The authors should provide instructions on data access and preparation, including how
to access the raw data, preprocessed data, intermediate data, and generated data, etc.
‚Ä¢The authors should provide scripts to reproduce all experimental results for the new
proposed method and baselines. If only a subset of experiments are reproducible, they
should state which ones are omitted from the script and why.
‚Ä¢At submission time, to preserve anonymity, the authors should release anonymized
versions (if applicable).
‚Ä¢Providing as much information as possible in supplemental material (appended to the
paper) is recommended, but including URLs to data and code is permitted.
6.Experimental Setting/Details
Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?
Answer: [Yes]
Justification: The experimental setting is given in ¬ß5; the full dataset and model details are
given in Appendix B and Appendix C, respectively.
Guidelines:
‚Ä¢ The answer NA means that the paper does not include experiments.
‚Ä¢The experimental setting should be presented in the core of the paper to a level of detail
that is necessary to appreciate the results and make sense of them.
‚Ä¢The full details can be provided either with the code, in appendix, or as supplemental
material.
7.Experiment Statistical Significance
Question: Does the paper report error bars suitably and correctly defined or other appropriate
information about the statistical significance of the experiments?
Answer: [Yes]
Justification: All of our experiments are run across multiple seeds. All plots include error
bars (areas of standard deviation or confidence intervals). All tables include the mean and
standard deviation across multiple seeds.
Guidelines:
‚Ä¢ The answer NA means that the paper does not include experiments.
‚Ä¢The authors should answer "Yes" if the results are accompanied by error bars, confi-
dence intervals, or statistical significance tests, at least for the experiments that support
the main claims of the paper.
22‚Ä¢The factors of variability that the error bars are capturing should be clearly stated (for
example, train/test split, initialization, random drawing of some parameter, or overall
run with given experimental conditions).
‚Ä¢The method for calculating the error bars should be explained (closed form formula,
call to a library function, bootstrap, etc.)
‚Ä¢ The assumptions made should be given (e.g., Normally distributed errors).
‚Ä¢It should be clear whether the error bar is the standard deviation or the standard error
of the mean.
‚Ä¢It is OK to report 1-sigma error bars, but one should state it. The authors should
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
of Normality of errors is not verified.
‚Ä¢For asymmetric distributions, the authors should be careful not to show in tables or
figures symmetric error bars that would yield results that are out of range (e.g. negative
error rates).
‚Ä¢If error bars are reported in tables or plots, The authors should explain in the text how
they were calculated and reference the corresponding figures or tables in the text.
8.Experiments Compute Resources
Question: For each experiment, does the paper provide sufficient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?
Answer: [Yes]
Justification: Information on the compute infrastructure and resources are given in Ap-
pendix D.
Guidelines:
‚Ä¢ The answer NA means that the paper does not include experiments.
‚Ä¢The paper should indicate the type of compute workers CPU or GPU, internal cluster,
or cloud provider, including relevant memory and storage.
‚Ä¢The paper should provide the amount of compute required for each of the individual
experimental runs as well as estimate the total compute.
‚Ä¢The paper should disclose whether the full research project required more compute
than the experiments reported in the paper (e.g., preliminary or failed experiments that
didn‚Äôt make it into the paper).
9.Code Of Ethics
Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines ?
Answer: [Yes]
Justification: We conform to the NeurIPS code of ethics.
Guidelines:
‚Ä¢The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
‚Ä¢If the authors answer No, they should explain the special circumstances that require a
deviation from the Code of Ethics.
‚Ä¢The authors should make sure to preserve anonymity (e.g., if there is a special consid-
eration due to laws or regulations in their jurisdiction).
10.Broader Impacts
Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?
Answer: [Yes]
Justification: We point out in the limitations that efficiency can also have potential negative
environmental aspects due to e.g. the rebound effect.
Guidelines:
‚Ä¢ The answer NA means that there is no societal impact of the work performed.
23‚Ä¢If the authors answer NA or No, they should explain why their work has no societal
impact or why the paper does not address societal impact.
‚Ä¢Examples of negative societal impacts include potential malicious or unintended uses
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
(e.g., deployment of technologies that could make decisions that unfairly impact specific
groups), privacy considerations, and security considerations.
‚Ä¢The conference expects that many papers will be foundational research and not tied
to particular applications, let alone deployments. However, if there is a direct path to
any negative applications, the authors should point it out. For example, it is legitimate
to point out that an improvement in the quality of generative models could be used to
generate deepfakes for disinformation. On the other hand, it is not needed to point out
that a generic algorithm for optimizing neural networks could enable people to train
models that generate Deepfakes faster.
‚Ä¢The authors should consider possible harms that could arise when the technology is
being used as intended and functioning correctly, harms that could arise when the
technology is being used as intended but gives incorrect results, and harms following
from (intentional or unintentional) misuse of the technology.
‚Ä¢If there are negative societal impacts, the authors could also discuss possible mitigation
strategies (e.g., gated release of models, providing defenses in addition to attacks,
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
feedback over time, improving the efficiency and accessibility of ML).
11.Safeguards
Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?
Answer: [NA]
Justification: As we propose a general method for neural network pruning, we do not see
where safeguards could be put in place or that there is a high risk of misuse.
Guidelines:
‚Ä¢ The answer NA means that the paper poses no such risks.
‚Ä¢Released models that have a high risk for misuse or dual-use should be released with
necessary safeguards to allow for controlled use of the model, for example by requiring
that users adhere to usage guidelines or restrictions to access the model or implementing
safety filters.
‚Ä¢Datasets that have been scraped from the Internet could pose safety risks. The authors
should describe how they avoided releasing unsafe images.
‚Ä¢We recognize that providing effective safeguards is challenging, and many papers do
not require this, but we encourage authors to take this into account and make a best
faith effort.
12.Licenses for existing assets
Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?
Answer: [Yes]
Justification: We clearly mark where any non-original code is used (see e.g. modules/utils.py
in the code).
Guidelines:
‚Ä¢ The answer NA means that the paper does not use existing assets.
‚Ä¢ The authors should cite the original paper that produced the code package or dataset.
‚Ä¢The authors should state which version of the asset is used and, if possible, include a
URL.
‚Ä¢ The name of the license (e.g., CC-BY 4.0) should be included for each asset.
24‚Ä¢For scraped data from a particular source (e.g., website), the copyright and terms of
service of that source should be provided.
‚Ä¢If assets are released, the license, copyright information, and terms of use in the
package should be provided. For popular datasets, paperswithcode.com/datasets
has curated licenses for some datasets. Their licensing guide can help determine the
license of a dataset.
‚Ä¢For existing datasets that are re-packaged, both the original license and the license of
the derived asset (if it has changed) should be provided.
‚Ä¢If this information is not available online, the authors are encouraged to reach out to
the asset‚Äôs creators.
13.New Assets
Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?
Answer: [Yes]
Justification: We provide links to the code in the paper and have documented the code for
ease of use/reproducibility.
Guidelines:
‚Ä¢ The answer NA means that the paper does not release new assets.
‚Ä¢Researchers should communicate the details of the dataset/code/model as part of their
submissions via structured templates. This includes details about training, license,
limitations, etc.
‚Ä¢The paper should discuss whether and how consent was obtained from people whose
asset is used.
‚Ä¢At submission time, remember to anonymize your assets (if applicable). You can either
create an anonymized URL or include an anonymized zip file.
14.Crowdsourcing and Research with Human Subjects
Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as
well as details about compensation (if any)?
Answer: [NA]
Justification: We do not perform crowdsourced experiments.
Guidelines:
‚Ä¢The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
‚Ä¢Including this information in the supplemental material is fine, but if the main contribu-
tion of the paper involves human subjects, then as much detail as possible should be
included in the main paper.
‚Ä¢According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
or other labor should be paid at least the minimum wage in the country of the data
collector.
15.Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects
Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?
Answer: [NA]
Justification: We do not do crowdsourcing or involve human subjects.
Guidelines:
‚Ä¢The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
25‚Ä¢Depending on the country in which research is conducted, IRB approval (or equivalent)
may be required for any human subjects research. If you obtained IRB approval, you
should clearly state this in the paper.
‚Ä¢We recognize that the procedures for this may vary significantly between institutions
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
guidelines for their institution.
‚Ä¢For initial submissions, do not include any information that would break anonymity (if
applicable), such as the institution conducting the review.
26