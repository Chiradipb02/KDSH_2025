How Transformers Utilize Multi-Head Attention in
In-Context Learning?
A Case Study on Sparse Linear Regression
Xingwu Chen∗
The University of Hong Kong
xingwu@connect.hku.hkLei Zhao∗
University of Pennsylvania
leizhao7@wharton.upenn.edu
Difan Zou
The University of Hong Kong
dzou@cs.hku.hk
Abstract
Despite the remarkable success of transformer-based models in various real-world
tasks, their underlying mechanisms remain poorly understood. Recent studies
have suggested that transformers can implement gradient descent as an in-context
learner for linear regression problems and have developed various theoretical anal-
yses accordingly. However, these works mostly focus on the expressive power of
transformers by designing specific parameter constructions, lacking a comprehen-
sive understanding of their inherent working mechanisms post-training. In this
study, we consider a sparse linear regression problem and investigate how a trained
multi-head transformer performs in-context learning. We experimentally discover
that the utilization of multi-heads exhibits different patterns across layers: multiple
heads are utilized and essential in the first layer, while usually one single head
is dominantly utilized for subsequent layers. We provide a theoretical rationale
for this observation: the first layer undertakes data preprocessing on the context
examples, and the following layers execute simple optimization steps based on the
preprocessed context. Moreover, we prove that such a preprocess-then-optimize
algorithm can outperform naive gradient descent and ridge regression algorithms,
which is also supported by our further experiments. Our findings offer insights
into the benefits of multi-head attention and contribute to understanding the more
intricate mechanisms hidden within trained transformers.
1 Introduction
Transformers [ 45] have emerged as a dominant force in machine learning, particularly in natural
language processing. Transformer-based large language models such as Llama [ 42,43] and the GPT
family [ 36,2,12,38], equipped with multiple heads and layers, showcasing exceptional learning
and reasoning capabilities. One of the fundamental capabilities is in-context learning [ 12,52], i.e.,
transformer can solve new tasks after prompting with a few context examples, without any further
parameter training. Understanding their working mechanisms and developing reasonable theoretical
explanations for their performance is vital and has gathered considerable research attention.
Numerous studies have been conducted to explore the expressive power of transformers, aiming to
showcase their ability to tackle challenging tasks related to memorization [ 33], reasoning [ 24,8,
∗Equal contribution.
38th Conference on Neural Information Processing Systems (NeurIPS 2024).28,14], function approximation [ 26,39], causal relationship [ 35], and simulating complex circuits
[21,31]. These endeavors typically aim to enhance our understanding of the capabilities and
limitations of transformers when configured with varying numbers of heads[ 16] and layers. However,
it’s important to note that the findings regarding expressive power and complexity may not directly
translate into explanations or insights into the behavior of trained transformer models in practical
applications.
To perform a deeper understanding of the learning ability of transformer, a line of recent studies
has been made to study the in-context learning performance of transformer by connecting it to
certain iterative optimization algorithms [ 17,3,47]. These investigations have primarily focused
on linear regression tasks with a Gaussian prior, demonstrating that a transformer with Llayers
can mimic Lsteps of gradient descent on the loss defined by contextual examples both theoret-
ically and empirically[ 3,53]. These observations have immediately triggered a series of further
theoretical research, revealing that multi-layer and multi-head transformers can emulate a broad
range of algorithms, including proximal gradient descent [ 8], preconditioned gradient descent [ 3,50],
functional gradient descent [ 15], Newton methods [ 22,19], and ridge regression [ 8,4]. However,
these theoretical works are mostly built by designing specific parameter constructions, which may
not reflect the key mechanism of trained transformers in practice. The precise roles of different
transformer modules, especially for the various attention layers and heads, remain largely opaque,
even within the context of linear regression tasks.
To this end, we take a deeper exploration regarding the working mechanism of transformer by
investigating how transformers utilize multi-heads, at different layers, to perform the in-context
learning. In particular, we consider the sparse linear regression task, i.e., the data is generated from a
noisy linear model with sparse ground truth w∗∈Rdwith∥w∗∥0≤s≪d, and train a transformer
model with multiple layers and heads. While a line of works also investigates this problem [ 20,8,1],
understanding the key mechanisms behind trained transformers always requires more experimental
and theoretical insights. Consequently, we empirically assess the importance of different heads at
varying layers by selectively masking individual heads and evaluating the resulting performance
degradation. Surprisingly, our observations reveal distinct utilization patterns of multi-head attention
across layers of a trained transformer: in the first attention layer, all heads appear to be significant;
in the subsequent layers, only one head appears to be significant. This phenomenon suggests that (1)
employing multiple heads, particularly in the first layer, plays a crucial role in enhancing in-context
learning performance; and (2) the working mechanisms of the transformer may be different for the
first and subsequent layers.
Based on the experimental findings, we conjecture that muti-layer transformer may exhibit a
preprocess-then-optimize algorithm on the context examples. Specifically, transformers utilize
all heads in the initial layer for data preprocessing and subsequently employ a single head in sub-
sequent layers to execute simple iterative optimization algorithms, such as gradient descent, on the
preprocessed data. We then develop the theory to demonstrate that such an algorithm can be indeed
implemented by a transformer with multiple heads in the first layer and one head in the remaining
layers, and can achieve substantially lower excess risk than gradient descent and ridge regression
(without data preprocessing). The main contributions of this paper are highlighted as follows:
•We empirically investigate the role of different heads within transformers in performing in-context
learning. We train a transformer model based on the data points generated by the noisy sparse
linear model. Then, we reveal a distinct utilization pattern of multi-head attention across layers:
while the first attention layer tended to evenly utilize all heads, subsequent layers predominantly
relied on a single head. This observation suggests that the working mechanisms of multi-head
transformers may vary between the first and subsequent layers.
•Building upon our empirical findings, we proposed a possible working mechanism for multi-head
transformers. Specifically, we hypothesized that transformers use the first layer for data prepro-
cessing on in-context examples, followed by subsequent layers performing iterative optimizations
on the preprocessed data. To substantiate this hypothesis, we theoretically demonstrated that,
by constructing a transformer with mild size, such a preprocess-then-optimize algorithm can be
implemented using multiple heads in the first layers and a single head in subsequent layers.
2•We further validated our proposed mechanism by comparing the performance of the preprocess-
then-optimize algorithm with multi-step gradient descent and ridge regression solution, which
can be implemented by the single-head transformers. We prove that the preprocess-then-optimize
algorithm can achieve lower excess risk compared to these traditional methods, which is also
verified by our numerical experiments. This aligns with our empirical findings, which indicated
that multi-head transformers outperformed ridge regression in terms of excess risk.
•To further validate our theoretical framework, we conducted additional experiments. Specifically,
we performed probing on the output of the first layer of the transformer and demonstrated that
representations generated by transformers with more heads led to lower excess risk after gradient
descent. These experiments provided further support for our explanation on the working mechanism
of transformers.
2 Preliminaries
Sparse Linear Regression. We consider sparse linear models where (x, y)∼P=Plin
w⋆is sampled
asx∼N(0,Σ),y=⟨w⋆,x⟩+N 
0, σ2
, where the Σis a diagonal matrix and ground truth
w⋆∈Rdsatisfies ∥w⋆∥0≤s. Then, we define the population risk of a parameter was follows:
L(w) :=E(x,y)∼P
(⟨x,w⟩ −y)2
.
Moreover, we are interested in the excess risk, i.e., the gap between the population risk achieved by
wand the optimal one:
E(w) :=L(w)−min
wL(w).
Multi-head Transformers. Transformers are a type of neural network with stacked attention and
multi-layer perceptron (MLP) blocks. In each layer, the transformer first utilizes multi-head attention
Attn to process the input sequence (or hidden states) H= [h1,h2, . . . ,hm]∈Rdhid×m. It computes
hdifferent queries, keys, and values, and then concatenates the output of each head:
Attn(H, θ1) =H+Concat [V1sfmx(K⊤
1Q1),···,Vhsfmx(K⊤
hQh)],
where Vi=WViH,Qi=WQiH,Vi=VViHandθ1=
WVi,WKi,WQi∈Rdhid/h×dhid	h
i=1are learnable parameters. The MLP then applies a nonlinear element-wise operation:
MLP(H, θ2) =W1ReLU (W2Attn(H, θ1)), (2.1)
where θ2={W1,W2}denotes the parameters of MLP. We remark that here some modules, such as
layernorm and bias, are ignored for simplicity.
Linear Attention-only Transformers To perform an tractable theoretical investigation on the role
of multi-head in the attention layer, we make further simplification on the transformer model by
considering linear attention-only transformers. These simplifications are widely adopted in many
recent works to study the behavior of transformer models [ 47,53,32,3]. In particular, the i-th layer
TFiperforms the following update on the input sequence (or hidden state) H(i−1)as follows:
H(i)=TFi(H(i−1)) =W1 
H(i−1)+Concat [{ViMK⊤
iQi}h
i=1]
,M:=
In0
00
∈Rm×m,
(2.2)
where {WVi,WKi,WQi∈Rdhid
h×dhid}h
i=1andW1∈Rdhid×dhidare learnable parameters, note that
as we ignore the ReLU activation in Eq. (2.1) , so we merge the parameter W1andW2into one matrix
W1. Besides, the mask matrix Mis included in the attention to constrain the model focus the first n
in-context examples rather than the subsequent m−nqueries [ 3,32,54]. To adapt the transformer for
solving sparse linear regression problems, we introduce additional linear layers WE∈R(d+1)×dhid
andWO∈Rdhid×1for input embedding and output projection, respectively. Mathematically, let E
denotes the input sequences with nin-context example followed by qqueries,
E=
x1x2···xnxn+1···xn+q
y1y2··· yn 0··· 0
. (2.3)
3Multi -Head AttentionAdd & Norm
Embedding ProjectionFFNAdd & NormOutput Projection
𝐿×Multi -Head Attention
InputInput
First layer ( l = 1 )
preprocessing
subsequent layer s ( l > 1 )
optimizing
𝒙1… 𝒙𝑛 𝒙𝑛+1 … 𝒙𝑛+𝑞
𝑦1… 𝑦𝑛 0 ⋯ 0 𝑦𝑖=𝒙𝑖,𝒘 +𝜉𝑖𝒘 0≤𝑠
In-Context Sparse Linear regression𝒙𝑖 ~ 𝑁0,Σ 𝜉𝑖~ 𝑁0,𝜎2(a) Overview of the experiments, including task, data, transformer architecture, and our insights.
5 10 15 20 25
in-context examples012345excess riskn_layer = 4,  = 0.1
ols
lasso
ridge
h = 1
h = 2
h = 4
h = 8
(b) ICL with Varying Heads
1 2 3 4
i-th head4 3 2 1j-th layerh_num = 4, n_layer 4
0.10.20.30.40.50.6
 (c) Heads Assessment
input 1 2 3 4
i-th layer012345excess risk
h_num = 4, n_layer 4  = 0.1
pruned h = 4
h = 4
h = 1 (d) Pruning and Probing
Figure 1: Experimental Insights into Multi-head Attention for In-context Learning
Then model processes the input sequence E, resulting in the output by∈R1×(n+q):
by=WO◦TFL◦ ··· ◦ TF1◦WE(E),
here, Lis the layer number of the transformer, and byi+nis the prediction value for the query xi+n.
During training, we set q >1for efficiency, and for inference and theoretical analysis, we set q= 1
and define the in-context learning excess risk EICLas:
EICL:=E(x,y)∼P(byn+1−yn+1)2−σ2.
Notations. For two functions f(x)≥0andg(x)≥0defined on the positive real numbers ( x >0),
we write f(x)≲g(x)if there exists two constants c, x0>0such that ∀x≥x0,f(x)≤c·g(x);
we write f(x)≳g(x)ifg(x)≲f(x); we write f(x)⋍g(x)iff(x)≲g(x)andg(x)≲f(x). If
f(x)≲g(x), we can write f(x)asO(g(x)). We can also write write f(x)aseO(g(x))if there exists
a constant k >0such that f(x)≲g(x) logk(x).
3 Experimental Insights into Multi-head Attention for In-context Learning
While previous work has demonstrated the in-context learning ability for sparse linear regression
[20,8], the hidden mechanism behind the trained transformer for solving this problem remains
unclear. To this end, we design a series of experiments, utilizing techniques like probing [ 5] and
pruning [ 27] to help us gain initial insights into how the trained transformer utilizes multi-head
attention for this problem. For all experiments in Sections 3 and 6, we choose an encoder-based
architecture as the backbone (see Figure 1a), set the hidden dimension dhidto 256, and use the input
sequence format shown in Eq. (2.3) , where d= 16 ,s= 4,x∼N(0,I), with varying noise levels,
layers, and heads, Additional experimental details can be found in Appendix B. The experiments we
designed are as follows:
ICL with Varying Heads: First, based on the experiment results by [ 8], we further investigate
the performance of transformers in solving the in-context sparse linear regression problem with
varying attention heads. An example can be found in Figure 1b, where we display the excess risk for
different models when using different numbers of in-context examples. We can observe that given
few-shot in-context examples, transformers can outperform OLS and ridge. Moreover, we can also
clearly observe the benefit of using multiple heads, which leads to lower excess risk when increasing
4the number of heads. This highlights the importance of multi-head attention in transformer to
perform in-context learning .
Heads Assessment: Based on Eq. (2.2) , we know that the j-th head at the i-th layer corresponds
to the subspace of the intermediate output from (j−1)·dhid/htoj·dhid/h−1. To assess the
importance of each attention head, we can mask the particular head by zeroing out the corresponding
output entries, while keeping other dimensions unchanged. Then, let (i, j)be the layer and head
indices, we evaluate the risk change before and after head masking, denoted by ∆EICL(i,j). Then we
normalize the risk changes in the same layer to evaluate their relative importance:
Wi,j=∆EICL(i,j)Ph
k=1∆EICL(i,k). (3.1)
An example can be found in Figure 1c. We can observe that in the first layer, no head distinctly
outweighs the others, while in the subsequent layers, there always exists a head that exhibits higher
importance than others. This gives us insight that in the first attention layer, all heads appear to be
significant, while in the subsequent layers, only one head appears to be significant .
Pruning and Probing: To further validate our findings in the previous experiments, we prune the
trained model by (1) retaining all heads in the first layer; and (2) only keeping the most important
head and zeroing out others for the subsequent layers. Then the pruned model, referred to as the
“pruned transformer”, will be fine-tuned with with the same training data. We then use linear probes
[6] to evaluate the prediction performance for different layers. An example can be found in Figure 1d,
we can find that the “pruned transformer” and the original model exhibit almost the same performance
for each layer. Additionally, compared to the model with single-head attention, we observe that the
probing result is largely different between single-head transformers and the “pruned transformers”,
the latter has better performances compared to the former. Noting that the main difference between
them is the number of heads in the first layer (subsequent layers have the same structure), it can be
deduced that the working mechanisms of the multi-head transformer may be different for the
first and subsequent layers .
4 Potential Mechanism Behind Trained transformer
Based on the experimental insights from Section 3, we found that all heads in the first layer of the
trained transformer are crucial, while in subsequent layers, only one head plays a significant role.
Furthermore, by checking the result for probing and pruning, we can find that the working mechanisms
of the transformer may be different for the first and subsequent layers. To this end, we hypothesize
that the multi-layer transformer may implement a preprocess-then-optimize to perform the in-context
learning, i.e., the transformer first performs preprocessing on the in-context examples using the first
layer and then implements multi-step iterative optimization algorithms on the preprocessed in-context
examples using the subsequent layers.
We note that [ 24] adapts a similar two-phase idea to explain how transformer learning specific
functions in context, in their constructed transformers, the first few layers utilize MLPs to compute
an appropriate representation for each entry, while the subsequent layers utilize the attention module
to implement gradient descent over the context. We highlight that our algorithm mainly focus on
utilizing multihead attention, and it aligns well with the our experimental observation and intuition.
The details of our algorithm are as follows:
4.1 Preprocessing on In-context Examples
First, as the multihead attention is designed to facilitate to model to capture features from different
representation subspaces [ 45], we abstract the algorithm implementation by the first layer of the
transformers as a preprocessing procedure. In general, for the sparse linear regression, a possible data
preprocessing method is to perform reweighting of the data features by emphasizing the features that
correspond to the nonzero entries of the ground truth w∗and disregard the remaining features. In the
idealized case, if we know the nonzero support of w∗, we can trivially zero out the date features of x
on the complement of the nonzero support, as a data preprocessing procedure, and perform projected
gradient descent to obtain the optimal solution.
5In general, the nonzero support of w∗is intractable to the learner, so that one cannot perform idealized
masking-related data preprocessing. However, one can still perform estimations on the importance
of data features by examining their correlation with the target. In particular, note that we have
y=⟨w∗,x⟩+ξi=Pd
i=1w∗
ixi+ξi, implying that ri:=E[xiy] =E[Pd
i=1w∗
ixi·xi] +E[ξxi] =
w∗
iE[x2
i]if considering independent data features. Then it is clear that such a correlation between the
feature and label will be nonzero only when |w∗
i| ̸= 0. Therefore, instead of knowing the nonzero
support of w∗, we can instead calculate such a correlation to perform reweighting on the data features.
Noting that the transformer is provided with nin-context examples {(xi, yi)}n
i=1, such correlations
can be estimated accordingly: brj=1
nPn
i=1xijyi, which will be further used to perform the data
preprocessing on the in-context examples. We summarize this procedure in Alg. 1.
Algorithm 1 Data preprocessing for in-context examples
1:Input : Sequence with {(xi, yi)}n
i=1,{(xi,0)}n+q
i=n+1as in-context examples/queries.
2:fork= 1, . . . , n do
3: Compute exkbyexk=bRxk, wherebR=diag{br1,br2, . . . ,brd}, wherebrjis given by
brj=1
nnX
i=1xijyi. (4.1)
4:end for
5:Output : Sequence with the preprocessed in-context examples/queries {(exi, yi)}n
i=1,{(exi,0)}n+q
i=n+1.
The preprocessing procedure aligns well with the structure of a multi-head attention layer with linear
attention, which motivates our theoretical construction of the desired transformer. In particular, each
head of the attention layer can be conceptualized as executing specific operations on a distinct subset
of data entries. Then, the linear query-key calculation, represented as (WKiH)⊤WQiH, where
H=Edenotes the input sequence embedding matrix, effectively estimates correlations between the
i-th subset of data entries and the corresponding label yi. Here, WKiandWQiselectively extract
entries from the i-th subset of features and the label, respectively, akin to an "entries selection"
process. Furthermore, when combined with the value calculation WViH, each head of the attention
layer conducts correlation calculations for the i-th subset of features and subsequently employs them
to reweight the original features within the same subset. Consequently, by stacking the outputs of
multiple heads, all data features can be reweighted accordingly, which matches the design of the
proposed preprocessing procedure in Alg. 1. We formally prove this in the following theorem.
Proposition 4.1 (Single-layer multi-head transformer implements Alg. 1) .There exists a single-layer
transformer function TF1, with dheads and dhid= 3dhidden dimension, together with an input
embedding layer with weight WE∈Rdhid×d, that can implement Alg. 1. Let Ebe the input sequence
defined in Eq. (2.3) andexi=bRxbe the preprocessed features defined in Alg. 1, it holds that
H(1):=TF1◦WE(E) =
ex1ex2···exnexn+1···exn+q
y1y2··· yn 0··· 0
.....................
, (4.2)
where ···in third row implies arbitrary values.
4.2 Optimizing Over Preprocessed In-Context Examples
Based on the experimental results, we observe that the subsequent layers of transformers dominantly
rely on one single head, suggesting their different but potentially simpler behavior compared to the
first one. Motivated by a series of recent work [ 47,15,53,3] that reveal the connection between
gradient descent steps and multi-layer single-head transformer in the in-context learning tasks, we
conjecture that the subsequent layers also implement iterative optimization algorithms such as gradient
descent on the (preprocessed) in-context examples.
To maintain clarity in our construction and explanation, in each layer, we use a linear projection W(i)
1
to rearrange the dimensions of the sequence processed by the multi-head attention, resulting in the
hidden state H(i)of each layer. We refer to the first drows of the input data as x, and the (d+ 1) -th
row as the corresponding y. For example, in Eq. (4.2) , we take the first drows, together with the
6(d+ 1)-th row, as the input data entry {exi, yi}n+1
i=1. Then, the following proposition shows that the
subsequent layers of transformer can implement multi-step gradient descent on the preprocessed
in-context examples {(exi, yi)}i=1,...,n.
Proposition 4.2 (Subsequent single-head transformer implements multi-step GD) .There exists a
transformer with klayers, 1head, dhid= 3d, letbyℓ
n+1be the prediction representation of the ℓ-th
layer, then it holds that byℓ
(n+1)=⟨wℓ
gd,exn+1⟩, where exn+1=bRxn+1denotes the preprocessed
data feature, wℓ
gdis defined as w0
gd= 0and as follows for ℓ= 0, . . . , k −1:
wℓ+1
gd=wℓ
gd−η∇eL(wℓ
gd),where eL(w) =1
2nnX
i=1(yi− ⟨w,exi⟩)2. (4.3)
The proof of Propositions 4.1 and 4.2 can be found in Appendix C, combining these two propositions
, we show that the multi-layer transformer with multiple heads in the first layer and one head in
the subsequent layers can implement the proposed preprocess-then-optimization algorithm. In the
next section, we will establish theories to demonstrate that such an algorithm can indeed achieve
smaller excess risk than standard gradient descent and ridge regression solutions of the sparse linear
regression problem.
5 Excess Risk of the Preprocess-then-optimize Algorithm
In this section, we will develop the theory to demonstrate the improved performance of the preprocess-
then-optimize algorithm compared to the gradient descent algorithm on the raw inputs. The proof for
Theorem 5.1, 5.2, and 5.3 can be found in Appendix D, E, and F, respectively.
We first denote ewt
gdas the estimator obtained by t-step GD on {(exi, yi)}n
i=1, which can be viewed as
the solution generated by the t+ 1-layer transformer based on our discussion in Section 4, and wt
gd
as the estimator obtained by t-step GD on {(xi, yi)}n
i=1. Before presenting our main theorem, we
first need to redefine the excess risk of GD on {(exi, yi)}n
i=1. Note that in our algorithm, the learned
predictor takes the form x→ ⟨bRx,ewt
gd⟩. Consequently, the population risk of a parameter ewt
gdis
naturally defined as eL(ewt
gd):=1
2·E(x,y)∼P
(⟨bRx,ewt
gd⟩ −y)2
, and the excess risk is then defined
asE(w):=eL(w)−minweL(w)2. Next, we provide the upper bound of the excess risk for E(ewt
gd)
andE(wt
gd)respectively.
Theorem 5.1. Denote S:={i:w⋆
i̸= 0}andR=diag{r1, . . . , r d}, where rj=Pd
i=1w⋆
iΣij.
Suppose that there exist a β > 0such that mini∈S|ri| ≥β,∥R∥2,∥Σ∥2,∥w⋆∥2≃O(1)and
n≳1/β2·t2s· 
Tr2/3(Σ) + Tr( RΣR )
·poly(log ( d/δ)). Then set η≲1/∥RΣR∥2and
ηt≃1
β· σ2Tr(RΣR ) log ( d/δ)
n+σ2sTr(Σ) log2(d/δ)
n2−1/2,
it holds that
E ewt
gd
≲logt
βs
σ2Tr(RΣR ) log ( d/δ)
n+σ2sTr(Σ) log2(d/δ)
n2,
with probability at least 1−δ.
Theorem 5.1 provides an upper bound on the excess risk achieved by the preprocess-then-optimize
algorithm, where we tuned learning rate ηto balance the bias and variance error. Then, it can be seen
that the risk bound is valid if Tr(RΣR )/n→0andTr(Σ)s/n2→0when n→ ∞ . This can be
readily satisfied if we have ∥w∗∥2andTr(Σ)be bounded by some reasonable quantities that are
independent of the sample size n, which are the common assumptions made in many prior works
[58,57,9]. Besides, it can be also seen that the excess risk bound explicitly depends on the sparsity
parameter sand lower sparsity implies better performance. This implies the ability of the proposed
preprocess-then-optimize for discovering and leveraging the nice sparse structure of the ground truth.
2Here for the ease to presentation and comparison, we slightly abuse the notation of E(w)by extending it to
ewt
gd, although E(w)is originally defined for the estimator for the raw feature vector x.
7As a comparison, the following theorem states the excess risk bound for the standard gradient descents
on the raw features. To make a fair comparison, we consider using the same number of steps but
allow the step size to be tuned separately.
Theorem 5.2. Suppose that ∥Σ∥,∥w⋆∥2≃O(1)andn≳t2(Tr(Σ) + log (1 /δ)). When η≲
1/∥Σ∥2andηt≃
σ2Tr(Σ) log ( d/δ)
n−1/2
, it holds that
E 
wt
gd
≲logt·r
σ2Tr(Σ) log ( d/δ)
n,
with probability at least 1−δ.
We are now able to make a rough comparison between the excess risk bounds in Theorems 5.1 and
5.2. Then, it is clear that E(ewt
gd)≲E(wt
gd)requires Tr(RΣR )/β2≲Tr(Σ)ands/(n2β2)≤1/n.
Specifically, we can consider the case that Σto be a diagonal matrix, assume w⋆
i∼U{−1/√s,1/√s}
has a restricted uniform prior for i∈ S andmini∈SΣii≥1/κfor some constant κ >1, we can get
β≥p
1/(sκ2), thus Tr(RΣR )/β2≤κ2P
i:w⋆
i̸=0Σiiands/(n2β2)≤κ2s2/n2. Note that |S|=
s≪d, then if the covariance matrix Σhas a flat eigenspectrum such thatP
i∈SΣii≪P
i∈[d]Σii=
Tr(Σ), we have Tr(RΣR )/β2≤Tr(Σ)ands/(n2β2)≤κ2s2/nifs=o 
min{d,√n}
. This
suggests that the preprocess-then-optimization algorithm can outperform the standard gradient descent
for solving a sparse linear regression problem with s=o 
min{d,√n}
.
To make a more rigorous comparison, we next consider the example where xii.i.d.∼N(0,I), based on
which we can get the upper bound for our algorithm and the lower bound for OLS, ridge regression,
and finite-step GD.
Theorem 5.3. Suppose Swith|S|=sis selected such that each element is chosen with equal
probability from the set {1,2, . . . , d }andw⋆
i∼U{−1/√s,1/√s}has a restricted uniform prior
fori∈ S,∥w⋆∥2≃Θ(1) andn≳t2s3d2/3. Then there exists a choice of ηandtsuch that
E ewt
gd
≲σ2log2 
ns/σ2
log2(d/δ)·s
n+ds2
n2
,
with probability at least 1−δ. Besides, let bwλbe the ridge regression estimator with regularized
parameter λ, andwolsbe the OLS estimator, it holds that
Ew⋆[E(w)]≳(
σ2d
nn≳d+ log (1 /δ)
1−n
d+σ2n
dd≳n+ log (1 /δ),
with probability at least 1−δ, where w∈ {bwλ,wols,wt
gd}.
It can be seen that for a wide range of under-parameterized and over-parameterized cases, ewt
gd
has a smaller excess risk than ridge regression, standard gradient descent, and OLS. In particular,
consider the setting σ2= 1, in the over-parameterized setting that d≳n, the excess risk bound of
preprocess-then-optimize is eO(ds/n2), which also outperforms the eΩ(1) bound achieved by OLS,
ridge regression, and standard gradient descent if the sparsity satisfies s=O(n2/d)(in fact, this
condition can be certainly removed as E(ewt
gd)also has a naive upper bound eO(1)). In the under-
parameterized case that d≲n, it can be readily verified that the data preprocessing can lead to a
eO(s/n)excess risk, which is strictly better than the eΩ(d/n)risk achieved by OLS, ridge regression,
and standard gradient descent. Moreover, it is well known that Lasso can achieve eO(s/n)excess risk
bound in the setting of Theorem 5.3. Then, by comparing with our results, we can also conclude that
the proprocess-then-optimize algorithm can be comparable to Lasso up to logarithmic factors when
d≲n, while becomes worse when d≳n.
6 Experiments
In Section 3, we conduct several experiments, and based on the observations, we propose that a trained
transformer can apply a preprocess-then-optimize algorithm: (1) In the first layer, the transformer can
apply a preprocessing algorithm (Alg. 1) on the in-context examples utilizing multi-head attention.
8𝑇𝐹1
𝑥1 𝑥2 … 𝑥𝑛…
Embedding  Projection
𝑥𝑛+1 … 𝑥𝑛+𝑞𝑇𝐹2𝑇𝐹𝐿
𝐸 =𝐻1=
෤𝑥𝑛+1…෤𝑥𝑛+𝑞−1෤𝑥𝑛+𝑞
𝑦𝑛+1 … 𝑦𝑛+𝑞−1
𝑦𝑛+𝑞K-step GD
𝑤gd𝑘
ො𝑦𝑛+𝑞𝑘
ℰ𝑘= (ො𝑦𝑛+𝑞𝑘- 𝑦𝑛+𝑞)2−𝜎2 extract 
query 
entriesTrained Transformer
Preprocessing Probing(a) P-probing
0123456789×10
steps2.02.53.03.54.0excess risk  = 0
h = 1
h = 4
h = 8
0123456789×10
steps2.02.53.03.54.0  = 0.1
h = 1
h = 4
h = 8
0123456789×10
steps2.02.53.03.54.0  = 0.2
h = 1
h = 4
h = 8
0123456789×10
steps2.02.53.03.54.0  = 0.4
h = 1
h = 4
h = 8
0123456789×10
steps2.02.53.03.54.0  = 0.8
h = 1
h = 4
h = 8(b) P-Probing result for Transformers trained on varying heads and data settings
0123456789×103
steps1.01.52.02.53.03.54.0excess risk  = 0
gd
pre-gd
0123456789×103
steps1.01.52.02.53.03.54.0  = 0.1
gd
pre-gd
0123456789×103
steps1.01.52.02.53.03.54.0  = 0.2
gd
pre-gd
0123456789×103
steps1.52.02.53.03.54.0  = 0.4
gd
pre-gd
0123456789×103
steps2.02.53.03.54.0  = 0.8
gd
pre-gd
(c) Directly apply Alg. 1 on input data for gradient descent
Figure 2: Supporting experiments for our preprocess-then-optimize algorithm and theoretical analysis
(2) In the subsequent layers, the transformer applies a gradient descent algorithm on the preprocessed
data utilizing single-head attention. While the second part is supported by extensive theoretical
analysis and experimental evidence [ 47,15,53,3], here we develop a technique called preprocessing
probing (P-probing) on the trained transformer to support the first part of our algorithm. We also
directly apply Alg. 1 on the in-context examples and then check the excess risk for multiple-step
gradient descent to verify the effectiveness of our algorithm and theoretical analysis.
P-probing: To verify the existence of a preprocessing procedure in the trained transformer, we
develop a “preprocessing probing” (P-probing) technique on the trained transformers, as illustrated
in Figure 2a. For a trained transformer, we first set the input sequence as in Eq. (2.3) , where the
firstnexamples {x}n
i=1have the corresponding labels {y}n
i=1, and the following qquery entries
only have {xi}n+q
i=n+1in the sequence. Then, we extract the last qvectors in the output hidden state
H1from the first layer of the transformer and treat these data as processed query entries. Next, we
conduct gradient descent on the first q−1query entries with their corresponding y, computing the
excess risk on the last query. Additional experimental details can be found in Appendix B. We
adapt this technique based on the intuition that, according to our theoretical analysis, we can extract
the preprocessed entry {exi}n+q
i=n+1fromH1, besides, the excess risk computed by the preprocessed
data has a better upper bound guarantee compared to raw data without preprocessing under the
same number of gradient descent steps, so if the trained transformer utilize multihead attention for
preprocess, compared with single head attention, the queries entries extract from H1by multihead
attention can have better gradient descent performance compared with single head attention.
Verifying the benefit of preprocessing: To further support the effectiveness of our algorithm,
we directly apply Alg. 1 on the input data {xi, yi}n+1
i=1, and then implement gradient descent on
the example entries {xi, yi}n
i=1and compute the excess risk with the last query {bxn+1, yn+1}, we
refer this procedure as pre-gd . We compare pre-gd with the excess risk obtained by directly
applying gradient descent without preprocessing (referred to as gd). For all experiments (both
P-probing and this), we set w0
gd=0and tune the learning rate ηfor each model by choosing from
[1,10−1,10−2,10−3,10−4,10−5,10−6]with the lowest average excess risk.
Based on Figure 2b, we can observe that compared to the transformer with single-head attention ( h=
1), the query entries extracted from the transformer with multiple heads ( h= 4,8) preserve better
convergence performance and can dive into a lower risk. This aligns well with our experiment result
in Figure 2c, where compared to gd, the data preprocessed by Alg. 1 preserves better convergence
performance and can dive into a lower risk space, supporting the existence of the preprocessing
procedure in the trained transformer. Moreover, Figure 2c also aligns well with our theoretical
analysis, where we provide a better upper bound for convergence guarantee for our algorithm
compared to ridge regression and OLS.
97 Conclusions and Limitations
In this paper, we investigate a sparse linear regression problem and explore how a trained transformer
leverages multi-head attention for in-context learning. Based on our empirical investigations, we
propose a preprocess-then-optimize algorithm, where the trained transformer utilizes multi-head
attention in the first layer for data preprocessing, and subsequent layers employ only a single head
for optimization. We theoretically prove the effectiveness of our algorithm compared to OLS, ridge
regression, and gradient descent, and provide additional experiments to support our findings.
While our findings provide promising insights into the hidden mechanisms of multi-head attention for
in-context learning, there is still much to be explored. First, our work focuses on the case of sparse
linear regression, and it may be beneficial to implement our experiment for more challenging or even
real-world tasks. Additionally, as we adapt attention-only transformers for analysis simplification,
the role of other modules, such as MLPs, are neglected. How these modules incorporate in real-world
tasks remains unclear. Moreover, our analysis does not consider the training dynamics of transformers,
while the theoretical analysis in [13] provides valuable insights into the convergence of single-layer
transformers with multi-head attention, the training dynamics for multi-layer transformers remain
unclear. How transformers learn to implement these algorithms is worth further investigation.
Acknowledgements
We would like to thank the anonymous reviewers and area chairs for their helpful comments. This
work is supported by NSFC 62306252, Guangdong NSF 2024A1515012444, Hong Kong ECS awards
27309624, and the central fund from HKU IDS.
References
[1]Jacob Abernethy, Alekh Agarwal, Teodor Vanislavov Marinov, and Manfred K. Warmuth. A
mechanism for sample-efficient in-context learning for sparse retrieval tasks. In Claire Vernade
and Daniel Hsu, editors, Proceedings of The 35th International Conference on Algorithmic
Learning Theory , volume 237 of Proceedings of Machine Learning Research , pages 3–46.
PMLR, 25–28 Feb 2024. URL https://proceedings.mlr.press/v237/abernethy24a.
html .
[2]Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni
Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4
technical report. arXiv preprint arXiv:2303.08774 , 2023.
[3]Kwangjun Ahn, Xiang Cheng, Hadi Daneshmand, and Suvrit Sra. Transformers learn to imple-
ment preconditioned gradient descent for in-context learning. Advances in Neural Information
Processing Systems , 36, 2024.
[4]Ekin Akyürek, Dale Schuurmans, Jacob Andreas, Tengyu Ma, and Denny Zhou. What learn-
ing algorithm is in-context learning? investigations with linear models. In The Eleventh
International Conference on Learning Representations , 2022.
[5]Guillaume Alain and Yoshua Bengio. Understanding intermediate layers using linear classifier
probes. arXiv preprint arXiv:1610.01644 , 2016.
[6]Guillaume Alain and Yoshua Bengio. Understanding intermediate layers using linear classifier
probes, 2017. URL https://openreview.net/forum?id=ryF7rTqgl .
[7]Zeyuan Allen-Zhu and Yuanzhi Li. Physics of language models: Part 3.2, knowledge manipula-
tion. arXiv preprint arXiv:2309.14402 , 2023.
[8]Yu Bai, Fan Chen, Huan Wang, Caiming Xiong, and Song Mei. Transformers as Statisticians:
Provable In-Context Learning with In-Context Algorithm Selection, July 2023.
[9]Peter L Bartlett, Philip M Long, Gábor Lugosi, and Alexander Tsigler. Benign overfitting in
linear regression. Proceedings of the National Academy of Sciences , 117(48):30063–30070,
2020.
10[10] Steven Bills, Nick Cammarata, Dan Mossing, Henk Tillman, Leo Gao, Gabriel Goh, Ilya
Sutskever, Jan Leike, Jeff Wu, and William Saunders. Language models can explain
neurons in language models. URL https://openaipublic. blob. core. windows. net/neuron-
explainer/paper/index. html.(Date accessed: 14.05. 2023) , 2023.
[11] Trenton Bricken, Adly Templeton, Joshua Batson, Brian Chen, Adam Jermyn, Tom Conerly,
Nick Turner, Cem Anil, Carson Denison, Amanda Askell, et al. Towards monosemanticity:
Decomposing language models with dictionary learning. Transformer Circuits Thread , page 2,
2023.
[12] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,
Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are
few-shot learners. Advances in neural information processing systems , 33:1877–1901, 2020.
[13] Siyu Chen, Heejune Sheen, Tianhao Wang, and Zhuoran Yang. Training dynamics of multi-
head softmax attention for in-context learning: Emergence, convergence, and optimality. arXiv
preprint arXiv:2402.19442 , 2024.
[14] Xingwu Chen and Difan Zou. What can transformer learn with varying depth? case studies on
sequence learning tasks. arXiv preprint arXiv:2404.01601 , 2024.
[15] Xiang Cheng, Yuxin Chen, and Suvrit Sra. Transformers implement functional gradient descent
to learn non-linear functions in context. arXiv preprint arXiv:2312.06528 , 2023.
[16] Yingqian Cui, Jie Ren, Pengfei He, Jiliang Tang, and Yue Xing. Superiority of multi-head
attention in in-context linear regression. arXiv preprint arXiv:2401.17426 , 2024.
[17] Nan Ding, Tomer Levinboim, Jialin Wu, Sebastian Goodman, and Radu Soricut. CausalLM is
not optimal for in-context learning, September 2023.
[18] Dan Friedman, Alexander Wettig, and Danqi Chen. Learning transformer programs. Advances
in Neural Information Processing Systems , 36, 2024.
[19] Deqing Fu, Tian-Qi Chen, Robin Jia, and Vatsal Sharan. Transformers Learn Higher-Order
Optimization Methods for In-Context Learning: A Study with Linear Models, October 2023.
[20] Shivam Garg, Dimitris Tsipras, Percy Liang, and Gregory Valiant. What Can Transformers
Learn In-Context? A Case Study of Simple Function Classes, August 2023.
[21] Angeliki Giannou, Shashank Rajput, Jy-yong Sohn, Kangwook Lee, Jason D. Lee, and Dimitris
Papailiopoulos. Looped Transformers as Programmable Computers, January 2023.
[22] Angeliki Giannou, Liu Yang, Tianhao Wang, Dimitris Papailiopoulos, and Jason D. Lee. How
Well Can Transformers Emulate In-context Newton’s Method?, March 2024.
[23] Andrey Gromov, Kushal Tirumala, Hassan Shapourian, Paolo Glorioso, and Daniel A Roberts.
The unreasonable ineffectiveness of the deeper layers. arXiv preprint arXiv:2403.17887 , 2024.
[24] Tianyu Guo, Wei Hu, Song Mei, Huan Wang, Caiming Xiong, Silvio Savarese, and Yu Bai.
How Do Transformers Learn In-Context Beyond Simple Functions? A Case Study on Learning
with Representations, October 2023.
[25] Yu Huang, Yuan Cheng, and Yingbin Liang. In-context convergence of transformers. arXiv
preprint arXiv:2310.05249 , 2023.
[26] Tokio Kajitsuka and Issei Sato. Are Transformers with One Layer Self-Attention Using Low-
Rank Weight Matrices Universal Approximators?, July 2023.
[27] Hao Li, Asim Kadav, Igor Durdanovic, Hanan Samet, and Hans Peter Graf. Pruning filters
for efficient convnets. In International Conference on Learning Representations , 2017. URL
https://openreview.net/forum?id=rJqFGTslg .
11[28] Yingcong Li, Muhammed Emrullah Ildiz, Dimitris Papailiopoulos, and Samet Oymak. Trans-
formers as algorithms: Generalization and stability in in-context learning. In International
Conference on Machine Learning , pages 19565–19594. PMLR, 2023.
[29] Yuchen Li, Yuanzhi Li, and Andrej Risteski. How do transformers learn topic structure:
Towards a mechanistic understanding. In International Conference on Machine Learning , pages
19689–19729. PMLR, 2023.
[30] David Lindner, János Kramár, Sebastian Farquhar, Matthew Rahtz, Thomas McGrath, and
Vladimir Mikulik. Tracr: Compiled Transformers as a Laboratory for Interpretability, November
2023.
[31] Bingbin Liu, Jordan T. Ash, Surbhi Goel, Akshay Krishnamurthy, and Cyril Zhang. Trans-
formers learn shortcuts to automata. In The Eleventh International Conference on Learning
Representations , 2023. URL https://openreview.net/forum?id=De4FYqjFueZ .
[32] Arvind Mahankali, Tatsunori B. Hashimoto, and Tengyu Ma. One Step of Gradient Descent is
Provably the Optimal In-Context Learner with One Layer of Linear Self-Attention, July 2023.
[33] Sadegh Mahdavi, Renjie Liao, and Christos Thrampoulidis. Memorization Capacity of Multi-
Head Attention in Transformers, October 2023.
[34] Xin Men, Mingyu Xu, Qingyu Zhang, Bingning Wang, Hongyu Lin, Yaojie Lu, Xianpei Han,
and Weipeng Chen. Shortgpt: Layers in large language models are more redundant than you
expect. arXiv preprint arXiv:2403.03853 , 2024.
[35] Eshaan Nichani, Alex Damian, and Jason D Lee. How transformers learn causal structure with
gradient descent. arXiv preprint arXiv:2402.14735 , 2024.
[36] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin,
Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to
follow instructions with human feedback. Advances in neural information processing systems ,
35:27730–27744, 2022.
[37] Onkar Pandit and Yufang Hou. Probing for bridging inference in transformer language models.
arXiv preprint arXiv:2104.09400 , 2021.
[38] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al.
Language models are unsupervised multitask learners. OpenAI blog , 1(8):9, 2019.
[39] Shokichi Takakura and Taiji Suzuki. Approximation and Estimation Ability of Transformers
for Sequence-to-Sequence Functions with Infinite Dimensional Input, May 2023.
[40] Davoud Ataee Tarzanagh, Yingcong Li, Christos Thrampoulidis, and Samet Oymak. Transform-
ers as Support Vector Machines, September 2023.
[41] Yuandong Tian, Yiping Wang, Beidi Chen, and Simon Du. Scan and Snap: Understanding
Training Dynamics and Token Composition in 1-layer Transformer, July 2023.
[42] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timo-
thée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open
and efficient foundation language models. arXiv preprint arXiv:2302.13971 , 2023.
[43] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei,
Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open
foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288 , 2023.
[44] Alexander Tsigler and Peter L Bartlett. Benign overfitting in ridge regression. Journal of
Machine Learning Research , 24(123):1–76, 2023.
[45] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez,
Lukasz Kaiser, and Illia Polosukhin. Attention Is All You Need, August 2023.
12[46] Roman Vershynin. High-dimensional probability. University of California, Irvine , 10:11, 2020.
[47] Johannes von Oswald, Eyvind Niklasson, Ettore Randazzo, João Sacramento, Alexander Mord-
vintsev, Andrey Zhmoginov, and Max Vladymyrov. Transformers learn in-context by gradient
descent, May 2023.
[48] Gail Weiss, Yoav Goldberg, and Eran Yahav. Thinking Like Transformers, July 2021.
[49] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony
Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, et al. Huggingface’s transform-
ers: State-of-the-art natural language processing. arXiv preprint arXiv:1910.03771 , 2019.
[50] Jingfeng Wu, Difan Zou, Zixiang Chen, Vladimir Braverman, Quanquan Gu, and Peter Bartlett.
How many pretraining tasks are needed for in-context learning of linear regression? In The
Twelfth International Conference on Learning Representations , 2023.
[51] Zhiyong Wu, Yun Chen, Ben Kao, and Qun Liu. Perturbed masking: Parameter-free probing
for analyzing and interpreting bert. arXiv preprint arXiv:2004.14786 , 2020.
[52] Sang Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu Ma. An explanation of
in-context learning as implicit bayesian inference. In International Conference on Learning
Representations , 2021.
[53] Ruiqi Zhang, Spencer Frei, and Peter L Bartlett. Trained transformers learn linear models
in-context. arXiv preprint arXiv:2306.09927 , 2023.
[54] Ruiqi Zhang, Jingfeng Wu, and Peter L Bartlett. In-context learning of a linear transformer block:
Benefits of the mlp component and one-step gd initialization. arXiv preprint arXiv:2402.14951 ,
2024.
[55] Hattie Zhou, Arwen Bradley, Etai Littwin, Noam Razin, Omid Saremi, Joshua M. Susskind,
Samy Bengio, and Preetum Nakkiran. What algorithms can transformers learn? a study in
length generalization. In The Twelfth International Conference on Learning Representations ,
2024. URL https://openreview.net/forum?id=AssIuHnmHX .
[56] Zeyuan Allen Zhu and Yuanzhi Li. Physics of language models: Part 3.1, knowledge storage
and extraction. arXiv preprint arXiv:2309.14316 , 2023.
[57] Difan Zou, Jingfeng Wu, Vladimir Braverman, Quanquan Gu, Dean P Foster, and Sham Kakade.
The benefits of implicit regularization from sgd in least squares problems. Advances in neural
information processing systems , 34:5456–5468, 2021.
[58] Difan Zou, Jingfeng Wu, Vladimir Braverman, Quanquan Gu, and Sham Kakade. Risk bounds
of multi-pass sgd for least squares in the interpolation regime. Advances in Neural Information
Processing Systems , 35:12909–12920, 2022.
13A Additional Related Work
In addition to works towards understanding the expressive power of transformers that we introduced
before, there is also a body of research on the mechanism interpretation and the training dynamics of
transformers:
Mechanism interpretation of trained transformers To understand the mechanisms in trained
transformers, researchers have developed various techniques, including interpreting transformers into
programming languages [ 18,30,48,55], probing the behavior of individual layers [ 37,51,11,7,56],
and incorporating transformers with other large language models to interpret individual neurons
[10]. While these techniques provide high-level insights into transformer mechanism understanding,
providing a clear algorithms behind the trained transformers is still very challenging.
Training dynamics of transformers In parallel, a body of work has also investigated how trans-
formers learn these algorithms, i.e., the training dynamics of transformers. Tarzanagh et al. [40]
shows an equivalence between a single attention layer and a support vector machine. Zhang et al.
[53], Ahn et al. [3]analyze the training dynamics of a single-head attention layer for in-context
linear regression, where [53] demonstrates that it can converge to implement one-step gradient over
in-context examples. Huang et al. [25], Chen et al. [13] extended these findings from linear attention
to softmax settings, with [ 13] revealing that trained transformers with multi-head attention tend to
utilize different heads for distinct tasks in various subspaces. Additionally, Tian et al. [41], Li et al.
[29] study the convergence of transformers on sequences of discrete tokens. Gromov et al. [23], Men
et al. [34] use experiments to show that in large language models, parameters in deeper layers are
less critical compared to those in shallower layers. These works provide valuable insights towards
the theoretical understanding of the training dynamics of transformers, which offer potential future
extension aspects for our work.
B Additional Details for Sections 3 and 6
Architecture and Optimization We conduct extensive experiments on encoder-only transformers
withdhid= 256 , varying the number of heads h∈ {1,2,4,8}, layers l∈ {3,4,5,6}, and noise
levels σ∈ {0,0.1,0.2,0.4,0.8}. For the input sequence, we sample x∼N(0,I). Forw, we first
sample w∼N(0,I)∈R16, and randomly choose s= 4entries, setting the other elements to zero.
Note that We don’t apply positional encodings in our setting, as no positional information is needed
in our input setting. To further support our preprocessing-then-optimize algorithm, we also try a
decoder-only architecture(Figure 9), train models on other settings like stardand linear regression
tasks=d= 16 (Figure 10) and non-orthogonal data distributions (Figure 11) as comparisons in
Appendix G. During training, we set n= 12 andq= 4, with a batch size of 64. We utilize the
Adam optimizer with a learning rate γ= 10−4for320000 updates. Each experiment takes about two
hours on a single NVIDIA GeForce RTX 4090 GPU. We fix the random seed such that each model
is trained and evaluated with the same training and evaluation dataset. We use HuggingFace [ 49]
library to implement our models.
ICL with Varying Heads We compare the model’s performance with ridge regression, OLS, and
lasso. For ridge regression and lasso, we tune λ, α∈
1,10−1,10−2,10−3,10−4	
respectively for
the lowest risk, as in [20].
From Figure 3, we can find that in most cases, transformers with single head ( h= 1) exhibits higher
risk compared to models with multiple heads ( h= 4,8). Note that in thesame subplot, models with
different numbers of heads have the same number of parameters. This experiment highlights the
importance of multi-head attention for transformers in in-context learning.
Heads Assessment Here, we set n= 10 andq= 1, with an evaluation data size of 8192 . For a
model with hheads and llayers, we train |σ|models under different noise levels. We first compute
theWh,l,σunder different noise levels σ, then sort each row in Wh,l,σ, and add them together as
Wh,l
avg=1
|σ|P
σ∈σWh,l,σ, resulting in the final weight for each head. An example can be found in
Fig 1c. In Fig 4, we present more results for different handl, and we also present the heat map for
the decode-only transformers in Figure 9.
145 10 15 20 25012345excess riskn_layer = 3,  = 0
ols
lasso
ridge
h = 1
h = 2
h = 4
h = 8
5 10 15 20 25012345n_layer = 4,  = 0
ols
lasso
ridge
h = 1
h = 2
h = 4
h = 8
5 10 15 20 25012345n_layer = 5,  = 0
ols
lasso
ridge
h = 1
h = 2
h = 4
h = 8
5 10 15 20 25012345n_layer = 6,  = 0
ols
lasso
ridge
h = 1
h = 2
h = 4
h = 8
5 10 15 20 25012345excess riskn_layer = 3,  = 0.1
ols
lasso
ridge
h = 1
h = 2
h = 4
h = 8
5 10 15 20 25012345n_layer = 4,  = 0.1
ols
lasso
ridge
h = 1
h = 2
h = 4
h = 8
5 10 15 20 25012345n_layer = 5,  = 0.1
ols
lasso
ridge
h = 1
h = 2
h = 4
h = 8
5 10 15 20 25012345n_layer = 6,  = 0.1
ols
lasso
ridge
h = 1
h = 2
h = 4
h = 8
5 10 15 20 25012345excess riskn_layer = 3,  = 0.2
ols
lasso
ridge
h = 1
h = 2
h = 4
h = 8
5 10 15 20 25012345n_layer = 4,  = 0.2
ols
lasso
ridge
h = 1
h = 2
h = 4
h = 8
5 10 15 20 25012345n_layer = 5,  = 0.2
ols
lasso
ridge
h = 1
h = 2
h = 4
h = 8
5 10 15 20 25012345n_layer = 6,  = 0.2
ols
lasso
ridge
h = 1
h = 2
h = 4
h = 8
5 10 15 20 25012345excess riskn_layer = 3,  = 0.4
ols
lasso
ridge
h = 1
h = 2
h = 4
h = 8
5 10 15 20 25012345n_layer = 4,  = 0.4
ols
lasso
ridge
h = 1
h = 2
h = 4
h = 8
5 10 15 20 25012345n_layer = 5,  = 0.4
ols
lasso
ridge
h = 1
h = 2
h = 4
h = 8
5 10 15 20 25012345n_layer = 6,  = 0.4
ols
lasso
ridge
h = 1
h = 2
h = 4
h = 8
5 10 15 20 25
in-context examples012345excess riskn_layer = 3,  = 0.8
ols
lasso
ridge
h = 1
h = 2
h = 4
h = 8
5 10 15 20 25
in-context examples012345n_layer = 4,  = 0.8
ols
lasso
ridge
h = 1
h = 2
h = 4
h = 8
5 10 15 20 25
in-context examples012345n_layer = 5,  = 0.8
ols
lasso
ridge
h = 1
h = 2
h = 4
h = 8
5 10 15 20 25
in-context examples012345n_layer = 6,  = 0.8
ols
lasso
ridge
h = 1
h = 2
h = 4
h = 8Figure 3: ICL with varying heads, layers and noise levels
1 2 3 43 2 1j-th layerh_num = 4, n_layer 3 avg
1 2 3 44 3 2 1h_num = 4, n_layer 4 avg
1 2 3 45 4 3 2 1h_num = 4, n_layer 5 avg
1 2 3 46 5 4 3 2 1h_num = 4, n_layer 6 avg
12345678
i-th head321j-th layerh_num = 8, n_layer 3 avg
12345678
i-th head4321h_num = 8, n_layer 4 avg
12345678
i-th head54321h_num = 8, n_layer 5 avg
12345678
i-th head654321h_num = 8, n_layer 6 avg0.150.200.250.300.350.400.45
0.10.20.30.40.5
0.10.20.30.40.50.6
0.10.20.30.40.50.6
0.10.20.30.40.50.6
0.10.20.30.40.50.6
0.10.20.30.40.50.60.7
0.10.20.30.40.50.60.7
Figure 4: Head Assessment with varying heads, layers
From Fig 4, we can find that in most settings, each head contributes almost equally, while in the
subsequent layers, there always exists a head that has a much larger weight than the others. This
15indicates that in trained transformers for in-context learning, in the first attention layer, all heads
appear to be significant, while in the subsequent layers, only one head appears to be significant.
Pruning and Probing Here, we also set n= 10 andq= 1, with an evaluation data size of 8192 .
To further support our finding from the Head Assessment, we first prune the model based on our
computed head weight Wh,l
avg, where we keep all heads in the first layer, whereas we only keep the
head with the highest score weight and mask the others. We then train the pruned model with the
same method as before for 60000 steps. In Fig 5, 6, 7, 8, we provide the Pruning and Probing results
for different numbers of heads h∈ {4,8}and noise levels σ∈ {0,0.1,0.2,0.4,0.8}. It can be found
that in almost all cases, the pruned model exhibits almost the same performance in each layer, while
being largely different from the single-layer transformer. This further supports the results in the
Heads Assessment and indicates that the working mechanisms of the multi-head transformer may be
different for the first and subsequent layers.
input 1 2 3012345excess risk
n_layer 3 avg
pruned h = 4
h = 4
h = 1
input 1 2 3012345
n_layer 3,  = 0
pruned h = 4
h = 4
h = 1
input 1 2 3012345
n_layer 3,  = 0.1
pruned h = 4
h = 4
h = 1
input 1 2 3012345
n_layer 3,  = 0.2
pruned h = 4
h = 4
h = 1
input 1 2 3012345
n_layer 3,  = 0.4
pruned h = 4
h = 4
h = 1
input 1 2 3012345
n_layer 3,  = 0.8
pruned h = 4
h = 4
h = 1
input 1 2 3
i-th layer012345excess risk
n_layer 3 avg
pruned h = 8
h = 8
h = 1
input 1 2 3
i-th layer012345
n_layer 3,  = 0
pruned h = 8
h = 8
h = 1
input 1 2 3
i-th layer012345
n_layer 3,  = 0.1
pruned h = 8
h = 8
h = 1
input 1 2 3
i-th layer012345
n_layer 3,  = 0.2
pruned h = 8
h = 8
h = 1
input 1 2 3
i-th layer012345
n_layer 3,  = 0.4
pruned h = 8
h = 8
h = 1
input 1 2 3
i-th layer012345
n_layer 3,  = 0.8
pruned h = 8
h = 8
h = 1
Figure 5: Pruning and Probing, 3 layers
input 1 2 3 4012345excess risk
n_layer 4 avg
pruned h = 4
h = 4
h = 1
input 1 2 3 4012345
n_layer 4,  = 0
pruned h = 4
h = 4
h = 1
input 1 2 3 4012345
n_layer 4,  = 0.1
pruned h = 4
h = 4
h = 1
input 1 2 3 4012345
n_layer 4,  = 0.2
pruned h = 4
h = 4
h = 1
input 1 2 3 4012345
n_layer 4,  = 0.4
pruned h = 4
h = 4
h = 1
input 1 2 3 4012345
n_layer 4,  = 0.8
pruned h = 4
h = 4
h = 1
input 1 2 3 4
i-th layer012345excess risk
n_layer 4 avg
pruned h = 8
h = 8
h = 1
input 1 2 3 4
i-th layer012345
n_layer 4,  = 0
pruned h = 8
h = 8
h = 1
input 1 2 3 4
i-th layer012345
n_layer 4,  = 0.1
pruned h = 8
h = 8
h = 1
input 1 2 3 4
i-th layer012345
n_layer 4,  = 0.2
pruned h = 8
h = 8
h = 1
input 1 2 3 4
i-th layer012345
n_layer 4,  = 0.4
pruned h = 8
h = 8
h = 1
input 1 2 3 4
i-th layer012345
n_layer 4,  = 0.8
pruned h = 8
h = 8
h = 1
Figure 6: Pruning and Probing, 4 layers
16input 1 2 3 4 5012345excess risk
n_layer 5 avg
pruned h = 4
h = 4
h = 1
input 1 2 3 4 5012345
n_layer 5,  = 0
pruned h = 4
h = 4
h = 1
input 1 2 3 4 5012345
n_layer 5,  = 0.1
pruned h = 4
h = 4
h = 1
input 1 2 3 4 5012345
n_layer 5,  = 0.2
pruned h = 4
h = 4
h = 1
input 1 2 3 4 5012345
n_layer 5,  = 0.4
pruned h = 4
h = 4
h = 1
input 1 2 3 4 5012345
n_layer 5,  = 0.8
pruned h = 4
h = 4
h = 1
input 1 2 3 4 5
i-th layer012345excess risk
n_layer 5 avg
pruned h = 8
h = 8
h = 1
input 1 2 3 4 5
i-th layer012345
n_layer 5,  = 0
pruned h = 8
h = 8
h = 1
input 1 2 3 4 5
i-th layer012345
n_layer 5,  = 0.1
pruned h = 8
h = 8
h = 1
input 1 2 3 4 5
i-th layer012345
n_layer 5,  = 0.2
pruned h = 8
h = 8
h = 1
input 1 2 3 4 5
i-th layer012345
n_layer 5,  = 0.4
pruned h = 8
h = 8
h = 1
input 1 2 3 4 5
i-th layer012345
n_layer 5,  = 0.8
pruned h = 8
h = 8
h = 1Figure 7: Pruning and Probing, 5 layers
input 1 2 3 4 5 6012345excess risk
n_layer 6 avg
pruned h = 4
h = 4
h = 1
input 1 2 3 4 5 6012345
n_layer 6,  = 0
pruned h = 4
h = 4
h = 1
input 1 2 3 4 5 6012345
n_layer 6,  = 0.1
pruned h = 4
h = 4
h = 1
input 1 2 3 4 5 6012345
n_layer 6,  = 0.2
pruned h = 4
h = 4
h = 1
input 1 2 3 4 5 6012345
n_layer 6,  = 0.4
pruned h = 4
h = 4
h = 1
input 1 2 3 4 5 6012345
n_layer 6,  = 0.8
pruned h = 4
h = 4
h = 1
input 1 2 3 4 5 6
i-th layer012345excess risk
n_layer 6 avg
pruned h = 8
h = 8
h = 1
input 1 2 3 4 5 6
i-th layer012345
n_layer 6,  = 0
pruned h = 8
h = 8
h = 1
input 1 2 3 4 5 6
i-th layer012345
n_layer 6,  = 0.1
pruned h = 8
h = 8
h = 1
input 1 2 3 4 5 6
i-th layer012345
n_layer 6,  = 0.2
pruned h = 8
h = 8
h = 1
input 1 2 3 4 5 6
i-th layer012345
n_layer 6,  = 0.4
pruned h = 8
h = 8
h = 1
input 1 2 3 4 5 6
i-th layer012345
n_layer 6,  = 0.8
pruned h = 8
h = 8
h = 1
Figure 8: Pruning and Probing, 6 layers
P-probing Here, we also set n= 117 andq= 11 , with an evaluation data size of 1024 . We choose
n≫qsuch that the model can handle more queries ( q= 11 ) than those in the training ( q= 4)
process.
C Proof for Section 4
C.1 Proof for Proposition 4.1
Proposition C.1 (Restate of Proposition 4.1) .There exists a transformer with 1layers, h=dheads,
dhid= 3dand the input projection WE∈R(d+1)×dhidsuch that with the input sequence Eset as
Equation 2.3 the first attention layer can implement Algorithm 1 so that each of the enhanced data
{brixi,j}i∈[d]can be found in the output representation H(1):
H(1)=TF1◦WE(E) =
ex1ex2···exnexn+1···exn+q
y1y2··· yn 0··· 0
.....................
.
Proof. Here we first explain the key steps of our constructed transformer: the model first re-
arrange the input entries with a input projection to divide the input data into dsubspace WE,
each subspace includes an entry of xand the corresponding y(step C.2), then use hparameters
{WVi,WKi,WQi}h
i=1to calculate hqueries, keys and values (step C.3), and compute the attention
output for each head and concatenate them together (step C.4), finally use a projection matrix W1
17rearrange the result, resulting the target output (step C.5):
E=
x1x2···xnxn+1···xn+q
y1y2··· yn 0··· 0
 (C.1)
input projection− − − − − − − − − − − →
WE∈R(d+1)×dhidH=
x1,1x2,1···xn,1x(n+1),1···x(n+q),1
y1 y2··· yn 0 ··· 0
0 0··· 0 0 ··· 0
.....................
(C.2)
compute Qi,Ki,Vi− − − − − − − − − − − − − − − − →
WVi,WKi,WQi∈R3×dhidKi=1
n
0··· 00···
0··· 00···
y1···yn0···
;Qi,Vi=
0··· 0 0 ···
0··· 0 0 ···
x1,i···xn,ix(n+1),i···

(C.3)
Attn(WE(E))− − − − − − − − − − − − − − − →
H+Concat {ViMK⊤
iQi}
x1,1x2,1···xn,1x(n+1),1···x(n+q),1
y1 y2··· yn 0 0 0
ex1,1ex2,1···exn,1ex(n+1),1···ex(n+q),1
.....................
(C.4)
H(1)=TF1◦WE(E)− − − − − − − − − − − − →
W1∈Rdhid×dhid
ex1ex2···exnexn+1···exn+q
y1y2··· yn 0··· 0
.....................
(C.5)
. The detailed parameters and calculation process for each step are as follows:
• we set WE∈R(d+1)×dhidto rearrange the entries:
WE=
 1[1] 1[d+ 1] 0 1[2] 1[d+ 1] 0··· 1[d] 1[d+ 1] 0
⊤
,
where 1[k]is an1×dhidvector with 1ati-th entry and 0elsewhere, such that
H=WEE=
x1,1x2,1···xn,1x(n+1),1···x(n+q),1
y1 y2··· yn 0 ··· 0
0 0··· 0 0 ··· 0
x1,2x2,2···xn,2x(n+1),2···x(n+q),2
.....................
.
• we set WVi,WKi,WQi∈R3×dhidfor values, keys and queries:
WKi=1
n
0
0
1[3i−1]
;WVi,WQi=
0
0
1[3i−2]
,
such that the i-th head extract i-th entry of xand corresponding y
Ki=1
n
0
0
1[3i−1]

x1,1x2,1···xn,1x(n+1),1···
y1 y2··· yn 0 ···
0 0··· 0 0 ···
x1,2x2,2···xn,2x(n+1),2···
..................
=1
n
0··· 00···
0··· 00···
y1···yn0···
,
Qi,Vi=
0
0
1[3i−2]

x1,1x2,1···xn,1x(n+1),1···
y1 y2··· yn 0 ···
0 0··· 0 0 ···
x1,2x2,2···xn,2x(n+1),2···
..................
=
0··· 0 0 ···
0··· 0 0 ···
x1,i···xn,ix(n+1),i···
,
18ViMK⊤
iQi=
0··· 0 0 ··· 0
0··· 0 0 ··· 0
x1,i···xn,ix(n+1),i···x(n+q),i

In0
00
·

0··· 00··· 0
0··· 00··· 0
y1···yn0··· 0
⊤
0··· 0 0 ··· 0
0··· 0 0 ··· 0
x1,i···xn,ix(n+1),i···x(n+q),i

=
0··· 0 0 ··· 0
0··· 0 0 ··· 0
ex1,i···exn,iex(n+1),i···ex(n+q),i
.
• Then concatenate the output of each head {ViMK⊤
iQi}h
i=1together with residue:
H+Concat [{ViMK⊤
iQi}h
i=1] =
x1,1x2,1···xn,1x(n+1),1···x(n+q),1
y1 y2··· yn 0 ··· 0
ex1,1ex2,1···exn,1ex(n+1),1···ex(n+q),1
.....................
.
(C.6)
• Finally, W1is applied to rearrange the entries:
W1=
 1[3]··· 1[3d] 1[2]···
⊤
,
where the first ···implies the omitted d−2vectors { 1[3i]|i= 2,3, . . . , (d−1)}, the second ···
implies arbitrary values, then resulting the final output:
H(1)=W1
H+Concat [{ViMK⊤
iQi}h
i=1]
=
ex1ex2···exnexn+1···exn+q
y1y2··· yn 0··· 0
.....................
.
in this way we construct a transformer that can apply Alg. 1 so that each of the enhanced data
{brixi,j}i∈[d]can be found in the output representation H(1).
C.2 Proof for Proposition 4.2
Proposition C.2 (Restate of Proposition 4.2) .There exists a transformer with klayers, 1head, dhid=
3d, let{(exi,byℓ
(i))}n+1
i=1be the ℓ-th layer input data entry, then it holds that byℓ
(n+1)=⟨wℓ
gd,exn+1⟩,
where wgdis defined as w0
gd= 0and as follows for ℓ= 0, ..., k−1:
wℓ+1
gd=wℓ
gd−η∇eL(wℓ
gd), where eL(w) =1
2nnX
i=1(yi− ⟨w,exi⟩)2.
Proof. Here we directly provide the parameters Wℓ
V,Wℓ
K,Wℓ
Q∈Rdhid×dhidandWℓ
1∈Rdhid×dhid
for each layer TFℓ,
Wℓ
V=−η
n
00
01
;Wℓ
K,Wℓ
Q=
Id×d0
0 0
;Wℓ
1=Idhid×dhid (C.7)
As we set Wℓ
1as the identity matrix, we can ignore it and then apply Lemma 1 in [ 3]. By replacing
(Wℓ
K⊤Wℓ
Q)asQiandWℓ
VwithPi, then it holds that byℓ
(n+1)=⟨wℓ
gd,exn+1⟩, where wgdis defined
asw0
gd= 0and as follows for ℓ= 0, ..., k−1:
wℓ+1
gd=wℓ
gd−η∇eL(wℓ
gd), where eL(w) =1
2nnX
i=1(yi− ⟨w,exi⟩)2.
19D Proof of Theorem 5.1
To simplify the notations, we use bwtto denote ewt
gd. We first prove that with a high probability, there
exists a R∈Rd×dsuch that RbR=bRR=Is, where Is=diag{a1, . . . , a d}withaj= 1{j∈S}.
Lemma D.1. Denote R=diag{r1, . . . , r d}, where rj=Pd
i=1w⋆
iΣij. Suppose n≥ O(log ( d/δ)),
then for any δ∈(0,1)with probability at least 1−δ, we have
∥bR−R∥2≲K·r
slog (d/δ)
n,
where K:=C 
max iΣii+σ2
, where Cis an absolute constant.
Lemma D.2. Define the event ERbyER=
|br|i≥1
2|ri|,∀i∈ S	
. Suppose that n≳
slog (d/δ)/β2, thenP(E1)≥1−δ.
We define RbyR=diag{r1, . . . , rd}, where rjis given by
rj=(
0 j /∈ S,
1/brjj∈ S.
It is easy to see RbR=bRR=Is. On the event E1, we have thatR≲1/β. Hereafter, we
condition on E1.
D.1 Bias-variance Decomposition
LeteX=XbRwithexi=bRxi. Forbwt, we have
bwt+1−Rw⋆=bwt−Rw⋆−η·1
nnX
i=1exi ex⊤
ibwt−yi
=bwt−Rw⋆−η·1
nnX
i=1exi ex⊤
ibwt−ex⊤
iRw⋆+ϵ
=
I−ηbΣ bwt−Rw⋆
+η·1
neX⊤ϵ.
Hence, we have
bwt=
I−
I−ηbΣt
Rw⋆+1
ntX
i=1
I−ηbΣi−1eX⊤ϵ. (D.1)
We can decompose the risk L(bwt)by
E(bwt) =E(x,y)∼P
⟨bRx,bwt⟩ − ⟨bRx,Rw⋆⟩ −ϵ2
−σ2(D.2)
=E(x,y)∼P
⟨bRx,bwt⟩ − ⟨bRx,Rw⋆⟩2
=Σ1/2bR bwt−Rw⋆2
2
=Σ1/2bR 
−
I−ηbΣt
Rw⋆+η·1
ntX
i=1
I−ηbΣi−1eX⊤ϵ!2
2
=Σ1/2bR
I−ηbΣt
Rw⋆2
2| {z }
Bias+η2Σ1/2bR 
1
ntX
i=1
I−ηbΣi−1eX⊤ϵ!2
2| {z }
Variance. (D.3)
Next, we present some lemmas.
20Lemma D.3 (Theorem 9 in Bartlett et al. [9]).There is an absolute constant csuch that for any
δ∈(0,1)with probability at least 1−δ,
∥bΣ−Σ∥2≤c∥Σ∥2·max(r
r(Σ)
n,r(Σ)
n,r
log (1 /δ)
n,log (1 /δ)
n)
,
where r(Σ) = Tr( Σ)/λ1.
Lemma D.4. With probability at least 1−δ, we have
∥bRbΣbR−RΣR∥2≲√s·poly(log ( d/δ))· r
r(RΣR )
n+p
r(Σ) +r(RΣR )
n+r(Σ)
n3/2!
.
As a result, when n≳st2 
r2/3(Σ) +r(RΣR )
·poly(log ( d/δ)), with probability at least 1−δ,
we have
∥bRbΣbR−RΣR∥2≤1/t.
We define the event E2as follows:
E2:=n
∥RΣR∥2≲eα(n, δ)≤1/to
,
where
eα(n, δ) =√s·poly(log ( d/δ))· r
r(RΣR )
n+p
r(Σ) +r(RΣR )
n+r(Σ)
n3/2!
.
By Lemma D.4, P(E2)≥1−δ. Hereafter, we condition on E1∩ E2.
D.2 Bounding the Bias
OnE1∩ E2, we have
Bias =Σ1/2bR
I−ηbΣt
Rw⋆2
2
=w⋆⊤R
I−ηbΣtbRΣbR
I−ηbΣt
Rw⋆
=w⋆⊤R
I−ηbΣtbR
Σ−bΣ
bR
I−ηbΣt
Rw⋆
| {z }
I+w⋆⊤R
I−ηbΣtbRbΣbR
I−ηbΣt
Rw⋆
| {z }
II..
(D.4)
Lemma D.5. OnE1∩ E2, we have
I≲1
tβ2
and
II≲1
ηtβ2·
hold with probability at least 1−δ.
By Lemma D.5, we obtain that with probability at least 1−δ,
Bias≲I + II≤1
tβ2+1
ηtβ2≲1
ηtβ2(D.5)
where the last inequality is by η≲1/∥Σ∥≲1.
21D.3 Bounding the Variance
Variance = η2Σ1/2bR 
1
ntX
i=1
I−ηbΣi−1eX⊤ϵ!2
2
=η2
n2ϵ⊤XbRtX
i=1
I−ηbΣi−1bRΣbRtX
i=1
I−ηbΣi−1bRX⊤ϵ
=η2
n2ϵ⊤XbRtX
i=1
I−ηbΣi−1bR
Σ−bΣ
bRtX
i=1
I−ηbΣi−1bRX⊤ϵ
| {z }
I
+η2
n2ϵ⊤XbRtX
i=1
I−ηbΣi−1bRbΣbRtX
i=1
I−ηbΣi−1bRX⊤ϵ
| {z }
II. (D.6)
Lemma D.6. OnE1∩ E2, with probability at least 1−δ, we have
I≲η2t
n2·bRX⊤ϵ2
2
and
II≲ηtlogt
n2·bRX⊤ϵ2
2.
By applying Lemma D.6 to Eq.(D.6), we obtain that
Variance = I + II ≲η2t
n2·bRX⊤ϵ2
2+ηtlogt
n2·bRX⊤ϵ2
2≲ηtlogt
n2·bRX⊤ϵ2
2.(D.7)
Lemma D.7. with probability at least 1−δ, we have
1
n·bRX⊤ϵ2
2≲σ2Tr(RΣR ) log ( d/δ)
n+σ2sTr(Σ) log2(d/δ)
n2
By applying Lemma D.7 to Eq.(D.7), we obtain that
Variance ≲ηtlogt·σ2Tr(RΣR ) log ( d/δ)
n+σ2sTr(Σ) log2(d/δ)
n2
. (D.8)
D.4 Final Bound
Combining Eq.(D.5) and Eq.(D.8), we obtain that
E(bwt)≤1
ηtβ2+ηtlogt·σ2Tr(RΣR ) log ( d/δ)
n+σ2sTr(Σ) log2(d/δ)
n2
≲logt
βs
σ2Tr(RΣR ) log ( d/δ)
n+σ2sTr(Σ) log2(d/δ)
n2,
when ηt≃1
β·
σ2Tr(RΣR ) log ( d/δ)
n+σ2sTr(Σ) log2(d/δ)
n2−1/2
.
D.5 Proof for Appendix D
Proof of Lemma D.1. Since yi=Pd
j=1w⋆
jxij+ϵi, then we have
bri=1
nnX
j=1xjiyj=1
nnX
j=1xji· dX
k=1w⋆
kxjk+ϵj!
=dX
k=1w⋆
k
nnX
j=1xjkxji+1
nnX
j=1xjiϵj.(D.9)
22Since xji∼N(0,Σii)for any i, j, by Lemma 2.7.7 in Vershynin [46], there exists an absolute
constant Csuch that xjkxjiis a sub-exponential random variable with
∥xjkxji∥Ψ1≤Cp
ΣkkΣii≤K,
where ∥ · ∥ Ψ1denotes the sub-exponential norm and the last inequality comes from the definition of
K. By applying Bernstein’s inequality [46, Theorem 2.8.1], we have
1
nnX
j=1xjkxji−E[x1kx1i]=1
nnX
j=1xjkxji−Σki
≤K·max(r
log (d/δ)
n,log (d/δ)
n)
=K·r
log (d/δ)
n, (D.10)
where the last equality due to n≥ O(log ( d/δ)). We also note that xjiϵjis a sub-exponential random
variable with ∥xjiϵj∥Ψ1≤K.Hence, we also have
1
nX
j=1xjiϵj≲K·r
log (d/δ)
n. (D.11)
Combining Eq.(D.9), Eq.(D.10) and Eq.(D.11), we have
|bri−ri|≲K·r
log (d/δ)
ndX
k=1|w⋆
k|+K·r
log (d/δ)
n= (∥w⋆∥1+ 1)K·r
log (d/δ)
n.
By definition of bRandR, we obtain
∥bR−R∥2= max
i|bri−ri| ≤K(∥w⋆∥1+ 1)·r
log (d/δ)
n
≤Kq
s∥w⋆∥2
2+ 1
·r
log (d/δ)
n≲K·r
slog (d/δ)
n,
which completes the proof.
Proof of Lemma D.2. By Lemma D.1, for any j∈ S, with probability at least 1−δ, we have
|ri−brj|≲r
slog (d/δ)
n≲β/2≤ |rj|/2, (D.12)
where the last inequality is due to the definition of β.
Proof of Lemma D.4. We can decompose ∥bRbΣbR−RΣR∥2as follows:
∥bRbΣbR−RΣR∥2=∥bRbΣbR−RbΣbR+RbΣbR−RΣbR+RΣbR−RΣR∥2
≤ ∥bRbΣbR−RbΣbR∥2| {z }
I+∥RbΣbR−RΣbR∥2| {z }
II+∥RΣbR−RΣR∥2| {z }
III.(D.13)
Next, we proof the bound for I,IIandIIIseparately.
For term I,
I =∥bRbΣbR−RbΣbR∥2=∥
bR−R
bΣbR∥2
≤ ∥bR−R∥2· ∥bΣ∥2· ∥bR∥2
≤ ∥bR−R∥2·
∥Σ∥2+∥bΣ−Σ∥2
·
∥R∥2+∥R−bR∥2
, (D.14)
23where the last line is due to triangle inequality. By Lemma D.3, with probability at least 1−δ/3, we
have
∥bΣ−Σ∥2≲∥Σ∥2·max(r
r(Σ)
n,r(Σ)
n,r
log (1 /δ)
n,log (1 /δ)
n)
≲∥Σ∥2·max(r
r(Σ) + log (1 /δ)
n,r(Σ) + log (1 /δ)
n)
. (D.15)
By Lemma D.1, we obtain that
∥bR−R∥2≤K·r
slog (d/δ)
n≲1 (D.16)
holds with probability at least 1−δ/3, where the last inequality is valid since n≳K2s∥R∥2
2log (d/δ).
Combing Eq.(D.14), Eq.(D.15) and Eq.(D.16), we have
I≲K∥Σ∥2r
slog (d/δ)
n· 
1 + max(r
r(Σ) + log (1 /δ)
n,r(Σ) + log (1 /δ)
n)!
≤K∥Σ∥2r
slog (d/δ)
n· 
1 +r
r(Σ) + log (1 /δ)
n+r(Σ) + log (1 /δ)
n!
. (D.17)
For term II, we can decompose IIas follows:
∥R
bΣ−Σ
bR∥2≤ ∥R
bΣ−Σ
R∥2
| {z }
II.a+∥R
bΣ−Σ
bR−R
∥2
| {z }
II.b.
For term II.a, by using Lemma D.3, we have with probability at least 1−δ/3,
II.a≲∥RΣR∥2·max(r
r(RΣR )
n,r(RΣR )
n,r
log (1 /δ)
n,log (1 /δ)
n)
≲∥RΣR∥2·max(r
r(RΣR ) + log (1 /δ)
n,r(RΣR ) + log (1 /δ)
n)
≤ ∥RΣR∥2· r
r(RΣR ) + log (1 /δ)
n+r(RΣR ) + log (1 /δ)
n!
(D.18)
Similar to the proof for bounding I, we can obtain that
II.b≲K∥Σ∥2r
slog (d/δ)
n· 
1 +r
r(Σ) + log (1 /δ)
n+r(Σ) + log (1 /δ)
n!
. (D.19)
For term III, we have
III =∥RΣ
bR−R
∥2≤ |R∥2∥Σ∥2K(∥w⋆∥1+ 1)·r
slog (d/δ)
n, (D.20)
where the last inequality is by Eq.(D.16).
24Combining Eq. (D.17) , Eq. (D.18) , Eq. (D.19) and Eq. (D.20) and taking the union bound, we obtain
that with probability at least 1−δ,
∥bRbΣbR−RΣR∥2≤I + II + III
≲K∥Σ∥2(∥w⋆∥1+ 1)r
log (d/δ)
n· 
1 +r
r(Σ) + log (1 /δ)
n+r(Σ) + log (1 /δ)
n!
+∥RΣR∥2· r
r(RΣR ) + log (1 /δ)
n+r(RΣR ) + log (1 /δ)
n!
+∥R∥2∥Σ∥2K(∥w⋆∥1+ 1)·r
log (d/δ)
n
≤(K∥Σ∥2(∥w⋆∥1+ 1) + ∥RΣR∥2+∥R∥2∥Σ∥2K(∥w⋆∥1+ 1))
· r
log (d/δ)
n· 
2 +r
r(Σ) + log (1 /δ)
n+r(Σ) + log (1 /δ)
n!
+r
r(RΣR ) + log (1 /δ)
n+r(RΣR ) + log (1 /δ)
n!
≲eCcov· r
r(RΣR ) + log (1 /δ)
n+p
r(Σ) log ( d/δ) +r(RΣR ) + log( d/δ)
n
+r(Σ)p
log (d/δ) + log3/2(d/δ)
n3/2!
≲eCcov·poly(log ( d/δ))· r
r(RΣR )
n+p
r(Σ) +r(RΣR )
n+r(Σ)
n3/2!
,
where the second last inequality is by aa′+bb′+cc′≤(a+b+c)(a′+b′+c′)fora, a′, b, b′, c, c′≥
0. Here eCcov=K∥Σ∥2(∥w⋆∥1+ 1) + ∥RΣR∥2+∥R∥2∥Σ∥2K(∥w⋆∥1+ 1)≲√s.
Proof of Lemma D.5. By the triangle inequality, we have
bR
Σ−bΣ
bR
2
=R
Σ−bΣ
R+R
Σ−bΣ
bR−R
+
bR−R
Σ−bΣ
R+
bR−R
Σ−bΣ
bR−R
2
≤R
Σ−bΣ
R
2+R
Σ−bΣ
bR−R
2+
bR−R
Σ−bΣ
R
2+
bR−R
Σ−bΣ
bR−R
2.
Following the proof of Lemma D.4, we can prove that with probability at least 1−δ,
bR
Σ−bΣ
bR
2≲eα(n, δ)≤1/t, (D.21)
where the last inequality is by E2. By Eq.(D.21), we have
bR
Σ−bΣ
bR⪯1/t·I.
Hence, we obtain that
I≲w⋆⊤R
I−ηbΣt
·1/t·I·
I−ηbΣt
Rw⋆
=1
tw⋆⊤R
I−ηbΣ2t
Rw⋆
≤1
tw⋆⊤RRw⋆(by
I−ηbΣ2t
⪯I)
≤1
t∥w⋆∥2
2, (D.22)
25where the last line by R⪯2
β·I. For the term II, we have
II =w⋆⊤R
I−ηbΣtbRbΣbR
I−ηbΣt
Rw⋆
≲1
ηtw⋆⊤RRw⋆
1
ηtβ2∥w⋆∥2
2≤1
ηtβ2, (D.23)
where the second last line is by the fact that x(1−x)k≤1/(k+ 1) for all x∈[0,1]and all
k >0.
Proof of Lemma D.6. Similar to the proof of Lemma D.5, with probability at least 1−δ, we have
bR
Σ−bΣ
bR⪯1
t·I. Then we have
I =η2
n2ϵ⊤XbRtX
i=1
I−ηbΣi−1bR
Σ−bΣ
bRtX
i=1
I−ηbΣi−1bRX⊤ϵ
≲η2
tn2ϵ⊤XbRtX
i=1
I−ηbΣi−1tX
i=1
I−ηbΣi−1bRX⊤ϵ
≤η2t
n2ϵ⊤XbRbRX⊤ϵ
=η2t
n2·bRX⊤ϵ2
2,
where the second last line is byPt
i=1
I−ηbΣi−1
⪯t·I. By the fact that x(1−x)k≤1/(k+ 1)
for all x∈[0,1]and all k >0, we have
II =η2
n2ϵ⊤XbRtX
i=1
I−ηbΣi−1bRbΣbRtX
i=1
I−ηbΣi−1bRX⊤ϵ
=η
n2ϵ⊤XbR
tX
i,j=1
I−ηbΣi+j−2
ηbRbΣ
bRX⊤ϵ
≤η
n2·(tX
i,j=11
i+j−1)bRX⊤ϵ2
2
≤ηt
n2·(tX
i=11
i)bRX⊤ϵ2
2
≲ηtlogt
n2·bRX⊤ϵ2
2,
where the last inequality is by the fact thatPt
i=11
i≲logt.
Proof of Lemma D.7. First, we can decompose1
n·bRX⊤ϵ2
2by
1
n·bRX⊤ϵ2
2≲1
n·RX⊤ϵ2
2+1
n·
bR−R
X⊤ϵ2
2.
Letzi=Rxi, then zi∼N(G), where G:=RΣR . For any i, j, by Lemma 2.7.7 in Vershynin
[46], there exists an absolute constant Csuch that ϵjzjiis a sub-exponential random variable with
∥ϵjzji∥Ψ1≤Cσp
Gii.
26By applying Bernstein’s inequality Vershynin [46, Theorem 2.8.1], for any 1≤i≤d, we have that1
nnX
j=1ϵjzji−E[ϵ1z1i]=1
nnX
j=1ϵjzji
≲σp
Gii·max(r
log (d/δ)
n,log (d/δ)
n)
=σp
Gii·r
log (d/δ)
n(D.24)
hold with probability 1−δ
3d, where the last inequality is due to n≥ O(log(d/δ)). By taking the
union bound, we obtain that1
nnX
j=1ϵjzji≲σp
Gii·r
log (d/δ)
n
holds for any i, with probability 1−δ
3. Then we have
I =dX
i=1
1
nnX
j=1ϵjzji
2
≲dX
i=1σ2Gii·log(d/δ)
n=σ2Tr(RΣR ) log( d/δ)
n.
In the same way, we can prove that with probability at least 1−δ/3,
1
nX⊤ϵ2
2≲σ2Tr(Σ) log( d/δ)
n. (D.25)
By applying Lemma D.1, with probability at least 1−δ/3, we have
bR−R2
2≲slog (d/δ)
n. (D.26)
By Eq.(D.25) and Eq.(D.26), with probability 1−2δ/3, we have
1
n·
bR−R
X⊤ϵ2
2≤bR−R2
21
nX⊤ϵ2
2≲σ2sTr(Σ) log2(d/δ)
n2.
By taking the union bound, we derive the desired result.
E Proof for Theorem 5.2
To simplify the notations, we use wtto denote wt
gd.
Lemma E.1. with probability at least 1−δ, we havebΣ−Σ≲α(n, δ), (E.1)
where α(n, δ) =q
Tr(Σ)+log (1 /δ)
n+Tr(Σ)+log (1 /δ)
n. As a result, when n≳t2(Tr(Σ) + log (1 /δ)),
with probability at least 1−δ,bΣ−Σ≲1/t.
Proof of Lemma E.1. By Lemma D.3, we have
∥bΣ−Σ∥2≤c∥Σ∥2·max(r
r(Σ)
n,r(Σ)
n,r
log (1 /δ)
n,log (1 /δ)
n)
≲max(r
r(Σ) + log (1 /δ)
n,r(Σ) + log (1 /δ)
n)
≤r
r(Σ) + log (1 /δ)
n+r(Σ) + log (1 /δ)
n(E.2)
holds with probability at least 1−δ, where the last line is by the inequality that max{a, b} ≤a+b
for all a, b≥0.
27We define the event Eas follows:
E:=n
RΣR∥2≲α(n, δ)≤1/to
.
By Lemma E.1, P(E)≥1−δ. Hereafter, we condition on E.
Bias-variance Decomposition Similar to Eq.(D.1), we have
wt=
I−
I−ηbΣt
w⋆+1
ntX
i=1
I−ηbΣi−1
X⊤ϵ. (E.3)
In the same way, we can decompose the risk E(wt)by
E(wt) =Σ1/2
I−ηbΣt
w⋆2
2| {z }
Bias+η2Σ1/2 
1
ntX
i=1
I−ηbΣi−1
X⊤ϵ!2
2| {z }
Variance. (E.4)
Bounding the Bias
Bias = w⋆⊤
I−ηbΣt
Σ
I−ηbΣt
w⋆
=w⋆⊤
I−ηbΣt
Σ−bΣ
I−ηbΣt
w⋆
| {z }
I+w⋆⊤
I−ηbΣtbΣ
I−ηbΣt
w⋆
| {z }
II.
Similar to the proof of Lemma D.5, we have the following lemma.
Lemma E.2. OnE, we have
I≲1
t
and
II≲1
ηt
hold with probability at least 1−δ.
As a result, the bound of the bias term is given by
Bias≤1
ηt+1
t≲1
ηt. (E.5)
Bounding the Variance By using the same way of the proof for bounding the variance term of
Theorem 5.1, we have the following lemma.
Lemma E.3. OnE, with probability at least 1−δ, we have that
Variance ≲ηtlogt·1
n·X⊤ϵ2
2≲ηtlogt·σ2Tr(Σ) log ( d/δ)
n. (E.6)
Combining Eq.(E.5) and Eq.(E.6), we obtain that
E(wt)≲1
ηt+ηtlogt·σ2Tr(Σ) log ( d/δ)
n≲logt·r
σ2Tr(Σ) log ( d/δ)
n,
when ηt≃
σ2Tr(Σ) log ( d/δ)
n−1/2
F Proof for Theorem 5.3
To simplify the notation, we use bwtto denote ewt
gdandwtto denote wt
gd.
28F.1 Proof for the upper bound of the excess risk
When Σ=I, by Eq.(D.2), we have
E(bwt) =bR
I−ηbΣt
Rw⋆2
2| {z }
Bias+η2bR 
1
ntX
i=1
I−ηbΣi−1eX⊤ϵ!2
2| {z }
Variance.
Following the proof of Theorem 5.1, it holds that
Variance ≲ηtlogt·σ2log (d/δ)
n+σ2sdlog2(d/δ)
n2
with probability at least 1−δ, when n≳t2sd2/3
Similar to the proof of Lemma D.2, we can prove that
bri≥ri
2∀i∈ S, bri≲1∀i,
with probability at least 1−δ.
When Σ=I, by Lemma D.4, we have that
bRbΣbR−RΣR
2≲β2
t
holds with probability at least 1−δ, when n≳t2∥w⋆∥2
1d2/3
β4 . As a result, RΣR −β2
t·I⪯bRbΣbR.
Hereafter, we condition on the above events. For the bias term, we have
bR
I−ηbΣt
Rw⋆2
2≤bR2
2·
I−ηbΣt
Rw⋆2
2
≤w⋆⊤R
I−ηbΣ2t
Rw⋆
≲w⋆⊤R
I−η
RΣR −β2
t·I2t
Rw⋆
=X
i∈S(w⋆
i/bri)2·
1−η
(w⋆
i)2−β2
t2t
≤s· 
1−ηβ2/22t,
where the last line is by the definition of β. When t≳log
σ2
ns
/ 
2 log 
1−ηβ2/2
, we have
Bias =bR
I−ηbΣt
Rw⋆2
2≤σ2
n. (F.1)
When ηβ2/2≤1/2, there exist a c >0, such that
log 
1−ηβ2/2
≥cηβ2/2.
Hence, the variance term is bounded by
Variance ≲ηtlogt· 
σ2log (d/δ)
n+σ2∥w⋆∥2
1dlog2(d/δ)
n2!
≲σ2log2 
ns/σ2
log2(d/δ)
β2·s
n+ds
n2
, (F.2)
where the last line is by ∥w⋆∥1≤s· ∥w⋆∥2
2=sandηt≲log(ns/σ2)
β2 . Combining Eq. (F.1) and
Eq.(F.2), we have that
E(bwt)≲σ2
n+σ2log2 
ns/σ2
log2(d/δ)
β2·1
n+ds
n2
≲σ2log2 
ns/σ2
log2(d/δ)
β2·1
n+ds
n2
,
29when n≳t2sd2/3
β4≥t2∥w⋆∥2
1d2/3
β4 andt≳log (ns)
ηβ2. When w⋆
i∈U{−1/√s,1/√s},β= 1/√s. In
this case, we have that
E(bwt)≲σ2log2 
ns/σ2
log2(d/δ)·s
n+ds2
n2
,
when n≳t2s3d2/3andt≳log (ns)
ηs.
F.2 Lower bound for Ridge Regression
When n≳d+ log (1 /δ), by Lemma D.3, we have that1
2·I⪯bΣ⪯2·IFor the ridge estimator
bwλ=1
n·
bΣ+λ·I−1
X⊤y, we have
Ew⋆[E(bwλ)] =
I−
bΣ+λI−1bΣ
w⋆2
2+1
n·
bΣ+λ·I−1
X⊤ϵ2
2
≥1
n·
bΣ+λ·I−1
X⊤ϵ2
2.
By Lemma D.3, when1
2·I⪯bΣ⪯2·I, with probability at least 1−δ, we have
Ew⋆[E(bwλ)]≥1
n·
bΣ+λ·I−1
X⊤ϵ2
2
=1
n2·ϵ⊤X
bΣ+λI−2
X⊤ϵ
≥1
n2(2 +λ)2·ϵ⊤XX⊤ϵ,
where the last line is due to the fact that bΣ+λI⪯(2 +λ)·I.
Lemma F.1. Given Xsuch that1
2I⪯bΣ⪯2I, it holds that
1
nX⊤ϵ2
2≳σ2d
n,
with probability at least 1−δ, when n≥ O(log (1 /δ)).
Proof of Lemma F .1. We consider the singular value decomposition of1√nX⊤:1√nX⊤=UΛV⊤,
where U∈Rd×dis an orthogonal matrix, Λ∈Rd×nis a rectangular diagonal matrix with non-
negative real numbers on the diagonal, V∈Rn×nis an orthogonal matrix. Let {σ1, . . . , σ d}be the
singular values of1√nX⊤. Then we have
1
nX⊤ϵ2
2=1√nUΛV⊤ϵ2
2=1√nΛV⊤ϵ2
2
=1√nΛeϵ2
2=1
ndX
i=1σ2
ieϵ2
i,
whereeϵ=V⊤ϵ∼N(0,I). By ][Lemma 22], we have
1
nX⊤ϵ2
2−E"1
nX⊤ϵ2
2#≲σ2max

qPd
i=1σ4
ilog (1 /δ)
n,max iσ2
ilog (1 /δ)
n


≲σ2max(p
dlog (1 /δ)
n,log (1 /δ)
n)
, (F.3)
30where the last line is valid since
σ2
1, . . . , σ2
d	
is the eigenvalues of bΣ=1
nX⊤Xand1
2I⪯bΣ⪯2I.
By Eq.(F.3), we obtain that
1
nX⊤ϵ2
2≥E"1
nX⊤ϵ2
2#
−σ2max(p
dlog (1 /δ)
n,log (1 /δ)
n)
=σ2dX
i=1σ2
i−σ2max(p
dlog (1 /δ)
n,log (1 /δ)
n)
=σ2d
n−σ2max(p
dlog (1 /δ)
n,log (1 /δ)
n)
(by1
2I⪯bΣ⪯2I)
≲σ2d
n,
where the last line is due to d≥ O(log (1 /δ)).
Next, we define the event Eas follows:
Eridge:=(
1
2I⪯bΣ⪯2I,1
nX⊤ϵ2
2≳σ2d
n)
.
By Lemma F.1, we have P(E)≥1−δwhen n≥ O(d)≥ O(log (1 /δ)). OnEridge, we have
Ew⋆[E(bwλ)]≳σ2d
(1 +λ)2n. (F.4)
When d≳n+ log (1 /δ), by Lemma D.3, with probability at least 1−δ, we have thatd
2·I⪯
XX⊤⪯2d·I. Hereafter, we condition on this event. By direct calculation, we can decompose the
excess risk by
Ew⋆[E(bwλ)] =Ew⋆
I−
bΣ+λI−1bΣ
w⋆2
2+1
n·
bΣ+λ·I−1
X⊤ϵ2
2.
For the first term, we have
Ew⋆
I−
bΣ+λI−1bΣ
w⋆2
2=Ew⋆
I−X⊤ 
XX⊤+nλI−1X
w⋆2
2
= (1−n
d)Ew⋆h
∥w⋆∥2
2i
, (F.5)
= 1−n
d(F.6)
where the last line is due to
I−X⊤ 
XX⊤+nλI−1X
is ad−nspace.
1
n·
bΣ+λ·I−1
X⊤ϵ2
2=ϵ⊤XX⊤ 
XX⊤+nλI−2ϵ
≥dn
2(2d+nλ)2·1
nnX
i=1ϵ2
i, (F.7)
where the first line is by 
X⊤X+nλI−1X⊤=X⊤ 
XX⊤+nλI−1and the last line is by
d
2(d+nλ)2·I⪯XX⊤ 
XX⊤+nλI−2. By Tsigler and Bartlett [44, Lemma 22], we obatain that
nX
i=1ϵ2
i−nσ2≲σ2p
nlog (1 /δ) +σ2
holds with probability at least 1−δ. When n≳log (1 /δ), we havePn
i=1ϵ2
i−nσ2≥nσ2
2holds
with probability at least 1−δ. Taking the union bound, we obtain that
Ew⋆[E(bwλ)]≳1−n
d+σ2·dn
2(2d+nλ)2≳1−n
d+σ2 n
(1 +λ)2d. (F.8)
31F.3 Lower Bound for Finite-Step GD
We first consider the case where n≳d+ log (1 /δ). Define the event EGDbyEGD=n
1
2·I⪯bΣ⪯2Io
. By Lemma D.3, P(EGD)≥1−δ. By Eq.(E.4), we have
Ew⋆[E(wt)] =Ew⋆
I−ηbΣt
w⋆2
2+η2 
1
ntX
i=1
I−ηbΣi−1
X⊤ϵ!2
2
≥η 
1
ntX
i=1
I−ηbΣi−1
X⊤ϵ!2
2
=η2
n2· 
bΣ
I−
I−ηbΣt−1!−1
X⊤ϵ2
2
≳η2
n2·
bΣ+1
ηt·I−1
X⊤ϵ2
2
≳σ2 η2d
(1 + 1 /(ηt))2n,
where the second last line is by bΣ
I−
I−ηbΣt−1
⪯Σ+2
tη·Iand the last line is by Eq. (F.4) .
We then consider the case where d≳n+log (1 /δ). Define the event E′
GD=d
2·I⪯XX⊤≺2dI	
.
By Lemma D.3, P(E′
GD)≥1−δ. Following the proof of Zou et al. [58, Theorem 4.3], we have
Ew⋆[E(wt)]≥Ew⋆ 
I−X⊤
XX⊤+n
ηtI−1
X!2
2+1
nX⊤
XX⊤+n
ηtI−1
ϵ2
2
= 1−n
d+σ2n

1 +1
ηt2
d,
where we use the results from Appendix F.2.
F.4 Lower bound of OLS
Letwolsbe the OLS estimator. It is easy to see wols=w0. Hence, we have
Ew⋆[E(wols)]≳(
σ2d
nn≳d+ log (1 /δ)
1−n
d+σ2n
dd≳n+ log (1 /δ),
holds with probability at least 1−δ.
G Additional Experiments
Here, we provide additional experiments on the decoder-only architecture and train models with
different settings.
Training Decoder-Only Transformer In this experiment, we adapt the same input setting and
training objective as in [ 20]. During training, we set n= 24 andk= 8 in Eq. (G.1) (where in yi,
we use zero padding to align with xi),dhid= 256 . We choose h= 8andl∈ {4,5,6}.3We then
conduct heads assessment experiments on the trained decoder-only transformers with 10in-context
3We also tried other settings with fewer heads or layers, but even with delicate hyperparameter tuning,
decoder-only transformers with fewer heads or layers consistently failed to learn how to solve our sparse linear
regression problem. A possible reason is that decoder-only transformers first need to learn the causal structure
[35] and then apply an optimization algorithm to the in-context entries, which is more challenging than our
encoder-based settings.
32examples, as in the previous settings. The result is shown in Figure 9. We can observe that the
decoder-only transformer exhibits the similar weight distribution for each layer as the encoder-based
models, indicating that our algorithm may extend to decoder-only based models.
E=
x1y1x2y2. . .xnyn
, L =nX
i=k(byi−yi)2. (G.1)
0 1 2 3 4 5 6 70 1 2 3j-th layerh_num = 8, n_layer 4 avg
0 1 2 3 4 5 6 70 1 2 3 4h_num = 8, n_layer 5 avg
0 1 2 3 4 5 6 70 1 2 3 4 5h_num = 8, n_layer 6 avg
0.10.20.30.40.50.60.7
0.10.20.30.40.50.6
0.10.20.30.40.50.60.7
Figure 9: Heads Assessment for decoder-only transformers
Training Models with s=d= 16 Here, we adapt the encoder-only transformer and the same
settings as introduced in B, but set s=d= 16 . We observe that in these cases, there is no distinct
performance difference between models with different numbers of heads. As shown in Figure 3,
when we set s= 4, d= 16 , transformers with more heads ( h= 4,8) always perform better than
models with fewer heads ( h= 1,2). However, in Figure 10, such a difference is unclear, which
aligns well with the theoretical analysis. When sis close to d, a clear better upper bound guarantee,
as ensured in cases where s≪dmay not hold.
Training Models with non-orthogonal design To further demonstrate the applicability of our
experimental results to more general non-orthogonal settings, we conducted additional experiments
by modifying the distribution of xtoN(0,Σ), where Σ =I+ζS, andSis a matrix of ones. We
varied ζacross the values [0,0.1,0.2,0.4]to validate our findings. The results are presented in
Figure 11, which reveals patterns similar to those observed in orthogonal design settings.
335 10 15 20 250246810excess riskn_layer = 3,  = 0
ols
lasso
ridge
h = 1
h = 2
h = 4
h = 8
5 10 15 20 250246810n_layer = 4,  = 0
ols
lasso
ridge
h = 1
h = 2
h = 4
h = 8
5 10 15 20 250246810n_layer = 5,  = 0
ols
lasso
ridge
h = 1
h = 2
h = 4
h = 8
5 10 15 20 250246810n_layer = 6,  = 0
ols
lasso
ridge
h = 1
h = 2
h = 4
h = 8
5 10 15 20 250246810excess riskn_layer = 3,  = 0.1
ols
lasso
ridge
h = 1
h = 2
h = 4
h = 8
5 10 15 20 250246810n_layer = 4,  = 0.1
ols
lasso
ridge
h = 1
h = 2
h = 4
h = 8
5 10 15 20 250246810n_layer = 5,  = 0.1
ols
lasso
ridge
h = 1
h = 2
h = 4
h = 8
5 10 15 20 250246810n_layer = 6,  = 0.1
ols
lasso
ridge
h = 1
h = 2
h = 4
h = 8
5 10 15 20 250246810excess riskn_layer = 3,  = 0.2
ols
lasso
ridge
h = 1
h = 2
h = 4
h = 8
5 10 15 20 250246810n_layer = 4,  = 0.2
ols
lasso
ridge
h = 1
h = 2
h = 4
h = 8
5 10 15 20 250246810n_layer = 5,  = 0.2
ols
lasso
ridge
h = 1
h = 2
h = 4
h = 8
5 10 15 20 250246810n_layer = 6,  = 0.2
ols
lasso
ridge
h = 1
h = 2
h = 4
h = 8
5 10 15 20 250246810excess riskn_layer = 3,  = 0.4
ols
lasso
ridge
h = 1
h = 2
h = 4
h = 8
5 10 15 20 250246810n_layer = 4,  = 0.4
ols
lasso
ridge
h = 1
h = 2
h = 4
h = 8
5 10 15 20 250246810n_layer = 5,  = 0.4
ols
lasso
ridge
h = 1
h = 2
h = 4
h = 8
5 10 15 20 250246810n_layer = 6,  = 0.4
ols
lasso
ridge
h = 1
h = 2
h = 4
h = 8
5 10 15 20 25
in-context examples0246810excess riskn_layer = 3,  = 0.8
ols
lasso
ridge
h = 1
h = 2
h = 4
h = 8
5 10 15 20 25
in-context examples0246810n_layer = 4,  = 0.8
ols
lasso
ridge
h = 1
h = 2
h = 4
h = 8
5 10 15 20 25
in-context examples0246810n_layer = 5,  = 0.8
ols
lasso
ridge
h = 1
h = 2
h = 4
h = 8
5 10 15 20 25
in-context examples0246810n_layer = 6,  = 0.8
ols
lasso
ridge
h = 1
h = 2
h = 4
h = 8Figure 10: Train Models with s=d= 16
1 2 3 43 2 1j-th layer=0.0, avg
1 2 3 43 2 1=0.1, avg
1 2 3 43 2 1=0.2, avg
1 2 3 43 2 1=0.4, avg
12345678321j-th layer=0.0, avg
12345678321=0.1, avg
12345678321=0.2, avg
12345678321=0.4, avg
1 2 3 44 3 2 1j-th layer=0.0, avg
1 2 3 44 3 2 1=0.1, avg
1 2 3 44 3 2 1=0.2, avg
1 2 3 44 3 2 1=0.4, avg
12345678
i-th head4321j-th layer=0.0, avg
12345678
i-th head4321=0.1, avg
12345678
i-th head4321=0.2, avg
12345678
i-th head4321=0.4, avg
0.10.20.30.40.5
0.150.200.250.300.350.400.45
0.10.20.30.40.5
0.10.20.30.40.5
0.10.20.30.4
0.10.20.30.40.5
0.050.100.150.200.250.300.35
0.050.100.150.200.250.300.350.400.45
0.10.20.30.40.5
0.10.20.30.40.50.6
0.10.20.30.40.50.6
0.10.20.30.40.50.6
0.050.100.150.200.250.300.350.40
0.10.20.30.40.5
0.10.20.30.40.50.60.7
0.10.20.30.40.50.60.7
Figure 11: Train Models with x∼N(0,Σ), where Σ =I+ζS.
34NeurIPS Paper Checklist
1.Claims
Question: Do the main claims made in the abstract and introduction accurately reflect the
paper’s contributions and scope?
Answer: [Yes]
Justification: Main claims made in the abstract and introduction accurately reflect the paper’s
contributions and scope.
Guidelines:
•The answer NA means that the abstract and introduction do not include the claims
made in the paper.
•The abstract and/or introduction should clearly state the claims made, including the
contributions made in the paper and important assumptions and limitations. A No or
NA answer to this question will not be perceived well by the reviewers.
•The claims made should match theoretical and experimental results, and reflect how
much the results can be expected to generalize to other settings.
•It is fine to include aspirational goals as motivation as long as it is clear that these goals
are not attained by the paper.
2.Limitations
Question: Does the paper discuss the limitations of the work performed by the authors?
Answer: [Yes]
Justification: In Section 7, we discuss the limitation and possible future works for this paper.
Guidelines:
•The answer NA means that the paper has no limitation while the answer No means that
the paper has limitations, but those are not discussed in the paper.
• The authors are encouraged to create a separate "Limitations" section in their paper.
•The paper should point out any strong assumptions and how robust the results are to
violations of these assumptions (e.g., independence assumptions, noiseless settings,
model well-specification, asymptotic approximations only holding locally). The authors
should reflect on how these assumptions might be violated in practice and what the
implications would be.
•The authors should reflect on the scope of the claims made, e.g., if the approach was
only tested on a few datasets or with a few runs. In general, empirical results often
depend on implicit assumptions, which should be articulated.
•The authors should reflect on the factors that influence the performance of the approach.
For example, a facial recognition algorithm may perform poorly when image resolution
is low or images are taken in low lighting. Or a speech-to-text system might not be
used reliably to provide closed captions for online lectures because it fails to handle
technical jargon.
•The authors should discuss the computational efficiency of the proposed algorithms
and how they scale with dataset size.
•If applicable, the authors should discuss possible limitations of their approach to
address problems of privacy and fairness.
•While the authors might fear that complete honesty about limitations might be used by
reviewers as grounds for rejection, a worse outcome might be that reviewers discover
limitations that aren’t acknowledged in the paper. The authors should use their best
35judgment and recognize that individual actions in favor of transparency play an impor-
tant role in developing norms that preserve the integrity of the community. Reviewers
will be specifically instructed to not penalize honesty concerning limitations.
3.Theory Assumptions and Proofs
Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?
Answer: [Yes]
Justification: We provide proofs for every theories and propositions in appendix.
Guidelines:
• The answer NA means that the paper does not include theoretical results.
•All the theorems, formulas, and proofs in the paper should be numbered and cross-
referenced.
•All assumptions should be clearly stated or referenced in the statement of any theorems.
•The proofs can either appear in the main paper or the supplemental material, but if
they appear in the supplemental material, the authors are encouraged to provide a short
proof sketch to provide intuition.
•Inversely, any informal proof provided in the core of the paper should be complemented
by formal proofs provided in appendix or supplemental material.
• Theorems and Lemmas that the proof relies upon should be properly referenced.
4.Experimental Result Reproducibility
Question: Does the paper fully disclose all the information needed to reproduce the main ex-
perimental results of the paper to the extent that it affects the main claims and/or conclusions
of the paper (regardless of whether the code and data are provided or not)?
Answer: [Yes]
Justification: We provide experimental settings and training details in Appendix B.
Guidelines:
• The answer NA means that the paper does not include experiments.
•If the paper includes experiments, a No answer to this question will not be perceived
well by the reviewers: Making the paper reproducible is important, regardless of
whether the code and data are provided or not.
•If the contribution is a dataset and/or model, the authors should describe the steps taken
to make their results reproducible or verifiable.
•Depending on the contribution, reproducibility can be accomplished in various ways.
For example, if the contribution is a novel architecture, describing the architecture fully
might suffice, or if the contribution is a specific model and empirical evaluation, it may
be necessary to either make it possible for others to replicate the model with the same
dataset, or provide access to the model. In general. releasing code and data is often
one good way to accomplish this, but reproducibility can also be provided via detailed
instructions for how to replicate the results, access to a hosted model (e.g., in the case
of a large language model), releasing of a model checkpoint, or other means that are
appropriate to the research performed.
•While NeurIPS does not require releasing code, the conference does require all submis-
sions to provide some reasonable avenue for reproducibility, which may depend on the
nature of the contribution. For example
36(a)If the contribution is primarily a new algorithm, the paper should make it clear how
to reproduce that algorithm.
(b)If the contribution is primarily a new model architecture, the paper should describe
the architecture clearly and fully.
(c)If the contribution is a new model (e.g., a large language model), then there should
either be a way to access this model for reproducing the results or a way to reproduce
the model (e.g., with an open-source dataset or instructions for how to construct
the dataset).
(d)We recognize that reproducibility may be tricky in some cases, in which case
authors are welcome to describe the particular way they provide for reproducibility.
In the case of closed-source models, it may be that access to the model is limited in
some way (e.g., to registered users), but it should be possible for other researchers
to have some path to reproducing or verifying the results.
5.Open access to data and code
Question: Does the paper provide open access to the data and code, with sufficient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?
Answer: [No]
Justification: the code is being organized and we can provide it at an appropriate time.
Guidelines:
• The answer NA means that paper does not include experiments requiring code.
•Please see the NeurIPS code and data submission guidelines ( https://nips.cc/
public/guides/CodeSubmissionPolicy ) for more details.
•While we encourage the release of code and data, we understand that this might not be
possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
including code, unless this is central to the contribution (e.g., for a new open-source
benchmark).
•The instructions should contain the exact command and environment needed to run to
reproduce the results. See the NeurIPS code and data submission guidelines ( https:
//nips.cc/public/guides/CodeSubmissionPolicy ) for more details.
•The authors should provide instructions on data access and preparation, including how
to access the raw data, preprocessed data, intermediate data, and generated data, etc.
•The authors should provide scripts to reproduce all experimental results for the new
proposed method and baselines. If only a subset of experiments are reproducible, they
should state which ones are omitted from the script and why.
•At submission time, to preserve anonymity, the authors should release anonymized
versions (if applicable).
•Providing as much information as possible in supplemental material (appended to the
paper) is recommended, but including URLs to data and code is permitted.
6.Experimental Setting/Details
Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?
Answer: [Yes]
Justification: We provide experimental settings and training details in Appendix B.
Guidelines:
37• The answer NA means that the paper does not include experiments.
•The experimental setting should be presented in the core of the paper to a level of detail
that is necessary to appreciate the results and make sense of them.
•The full details can be provided either with the code, in appendix, or as supplemental
material.
7.Experiment Statistical Significance
Question: Does the paper report error bars suitably and correctly defined or other appropriate
information about the statistical significance of the experiments?
Answer: [No]
Justification: The reported mean behaviors in our experiments are sufficient to support our
theoretical results.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The authors should answer "Yes" if the results are accompanied by error bars, confi-
dence intervals, or statistical significance tests, at least for the experiments that support
the main claims of the paper.
•The factors of variability that the error bars are capturing should be clearly stated (for
example, train/test split, initialization, random drawing of some parameter, or overall
run with given experimental conditions).
•The method for calculating the error bars should be explained (closed form formula,
call to a library function, bootstrap, etc.)
• The assumptions made should be given (e.g., Normally distributed errors).
•It should be clear whether the error bar is the standard deviation or the standard error
of the mean.
•It is OK to report 1-sigma error bars, but one should state it. The authors should
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
of Normality of errors is not verified.
•For asymmetric distributions, the authors should be careful not to show in tables or
figures symmetric error bars that would yield results that are out of range (e.g. negative
error rates).
•If error bars are reported in tables or plots, The authors should explain in the text how
they were calculated and reference the corresponding figures or tables in the text.
8.Experiments Compute Resources
Question: For each experiment, does the paper provide sufficient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?
Answer: [Yes]
Justification: We provide experimental settings and training details in Appendix B.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The paper should indicate the type of compute workers CPU or GPU, internal cluster,
or cloud provider, including relevant memory and storage.
•The paper should provide the amount of compute required for each of the individual
experimental runs as well as estimate the total compute.
38•The paper should disclose whether the full research project required more compute
than the experiments reported in the paper (e.g., preliminary or failed experiments that
didn’t make it into the paper).
9.Code Of Ethics
Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines ?
Answer: [Yes]
Justification: the research conducted in the paper conform, in every respect, with the NeurIPS
Code of Ethics.
Guidelines:
•The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
•If the authors answer No, they should explain the special circumstances that require a
deviation from the Code of Ethics.
•The authors should make sure to preserve anonymity (e.g., if there is a special consid-
eration due to laws or regulations in their jurisdiction).
10.Broader Impacts
Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?
Answer: [NA]
Justification: no societal impact of the work performed.
Guidelines:
• The answer NA means that there is no societal impact of the work performed.
•If the authors answer NA or No, they should explain why their work has no societal
impact or why the paper does not address societal impact.
•Examples of negative societal impacts include potential malicious or unintended uses
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
(e.g., deployment of technologies that could make decisions that unfairly impact specific
groups), privacy considerations, and security considerations.
•The conference expects that many papers will be foundational research and not tied
to particular applications, let alone deployments. However, if there is a direct path to
any negative applications, the authors should point it out. For example, it is legitimate
to point out that an improvement in the quality of generative models could be used to
generate deepfakes for disinformation. On the other hand, it is not needed to point out
that a generic algorithm for optimizing neural networks could enable people to train
models that generate Deepfakes faster.
•The authors should consider possible harms that could arise when the technology is
being used as intended and functioning correctly, harms that could arise when the
technology is being used as intended but gives incorrect results, and harms following
from (intentional or unintentional) misuse of the technology.
•If there are negative societal impacts, the authors could also discuss possible mitigation
strategies (e.g., gated release of models, providing defenses in addition to attacks,
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
feedback over time, improving the efficiency and accessibility of ML).
11.Safeguards
39Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?
Answer: [NA]
Justification: the paper poses no such risks.
Guidelines:
• The answer NA means that the paper poses no such risks.
•Released models that have a high risk for misuse or dual-use should be released with
necessary safeguards to allow for controlled use of the model, for example by requiring
that users adhere to usage guidelines or restrictions to access the model or implementing
safety filters.
•Datasets that have been scraped from the Internet could pose safety risks. The authors
should describe how they avoided releasing unsafe images.
•We recognize that providing effective safeguards is challenging, and many papers do
not require this, but we encourage authors to take this into account and make a best
faith effort.
12.Licenses for existing assets
Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?
Answer: [Yes]
Justification: the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected
Guidelines:
• The answer NA means that the paper does not use existing assets.
• The authors should cite the original paper that produced the code package or dataset.
•The authors should state which version of the asset is used and, if possible, include a
URL.
• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
•For scraped data from a particular source (e.g., website), the copyright and terms of
service of that source should be provided.
•If assets are released, the license, copyright information, and terms of use in the
package should be provided. For popular datasets, paperswithcode.com/datasets
has curated licenses for some datasets. Their licensing guide can help determine the
license of a dataset.
•For existing datasets that are re-packaged, both the original license and the license of
the derived asset (if it has changed) should be provided.
•If this information is not available online, the authors are encouraged to reach out to
the asset’s creators.
13.New Assets
Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?
Answer: [NA]
40Justification: the paper does not release new assets.
Guidelines:
• The answer NA means that the paper does not release new assets.
•Researchers should communicate the details of the dataset/code/model as part of their
submissions via structured templates. This includes details about training, license,
limitations, etc.
•The paper should discuss whether and how consent was obtained from people whose
asset is used.
•At submission time, remember to anonymize your assets (if applicable). You can either
create an anonymized URL or include an anonymized zip file.
14.Crowdsourcing and Research with Human Subjects
Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as
well as details about compensation (if any)?
Answer: [NA]
Justification: the paper does not involve crowdsourcing nor research with human subjects.
Guidelines:
•The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
•Including this information in the supplemental material is fine, but if the main contribu-
tion of the paper involves human subjects, then as much detail as possible should be
included in the main paper.
•According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
or other labor should be paid at least the minimum wage in the country of the data
collector.
15.Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects
Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?
Answer: [NA]
Justification: the paper does not involve crowdsourcing nor research with human subjects.
Guidelines:
•The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
•Depending on the country in which research is conducted, IRB approval (or equivalent)
may be required for any human subjects research. If you obtained IRB approval, you
should clearly state this in the paper.
•We recognize that the procedures for this may vary significantly between institutions
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
guidelines for their institution.
•For initial submissions, do not include any information that would break anonymity (if
applicable), such as the institution conducting the review.
41