Published in Transactions on Machine Learning Research (12/2024)
Variational Pseudo Marginal Methods for Jet Reconstruction in
Particle Physics
Hanming Yang1,*hy2781@columbia.edu
Antonio Khalil Moretti1,2,*antoniomoretti@spelman.edu
Sebastian Macaluso3seb.macaluso@gmail.com
Philippe Chlenski1pac@cs.columbia.edu
Christian A. Naesseth4c.a.naesseth@uva.nl
Itsik Pe’er1itsik@cs.columbia.edu
1Department of Computer Science, Columbia University
2Department of Computer Science, Spelman College
3Telefonica Research
4University of Amsterdam
*Equal contribution
Reviewed on OpenReview: https: // openreview. net/ forum? id= pCapRF2vFf
Abstract
Reconstructing jets, which provide vital insights into the properties and histories of sub-
atomic particles produced in high-energy collisions, is a main problem in data analyses of
collider physics. This intricate task deals with estimating the latent structure of a jet (bi-
nary tree) and involves parameters such as particle energy, momentum, and types. While
Bayesian methods offer a natural approach for handling uncertainty and leveraging prior
knowledge, they face significant challenges due to the super-exponential growth of potential
jet topologies as the number of observed particles increases. To address this, we introduce
a Combinatorial Sequential Monte Carlo approach for inferring jet latent structures. As a
second contribution, we leverage the resulting estimator to develop a variational inference
algorithm for parameter learning. Building on this, we introduce a variational family using a
pseudo-marginal framework for a fully Bayesian treatment of all variables, unifying the gen-
erative model with the inference process. We illustrate our method’s effectiveness through
experiments using data generated with a collider physics generative model, highlighting
superior speed and accuracy across a range of tasks.
1 Introduction
Reconstructing jets in particle physics deals with estimating a high-quality hierarchical clustering. A com-
prehensive approach to this process also involves inference on model parameters, which could provide insights
intoourunderstandingofquantumchromodynamics(QCD),i.e. thetheoryofthestronginteractionbetween
quarks mediated by gluons. Hierarchical clustering forms a natural data representation of data generated by
a Markov tree, and has been applied in a wide variety of settings such as entity resolution for knowledge-bases
(Green et al., 2012; Vashishth et al., 2018), personalization (Zhang et al., 2014), and jet physics (Cacciari
et al., 2008; Catani et al., 1993; Dokshitzer et al., 1997; Ellis & Soper, 1993). Typically, work has focused
on approximate methods for relatively large datasets (Bateni et al., 2017; Monath et al., 2019; Naumov
et al., 2020; Dubey et al., 2014; Hu et al., 2015; Monath et al., 2020; Dubey et al., 2020; Monath et al.,
2021). However, there are relevant use cases for hierarchical clustering that require exact or high-quality
approximations on small to medium-sized datasets (Greenberg et al., 2020; 2021). This paper deals with
one of these use cases: reconstructing the latent hierarchy of jetsin particle physics. Within this context,
1Published in Transactions on Machine Learning Research (12/2024)
Jet generation Jet reconstruction
Proton
Proton=⇒
Leaves =
Observed
Particles
Figure 1: Jets as binary trees. Left: Schematic representation of the production of a jet at CERN’s LHC.
Incoming protons collide, producing two new particles (light blue). Each new particle undergoes a sequence
of binary splittings until stable particles (solid blue) are produced and measured by a detector. Right: In jet
reconstruction, only the leaf nodes are observed and the tree topology is inferred. Each latent tree topology
represents a different possible splitting history.
Bayesian methods provide a natural approach for handling uncertainty, but the super-exponential scaling
of the number of hierarchies with the size of the datasets presents significant difficulties, i.e. the number of
topologies grows as (2N−3)!!, withNbeing the number of leaves. This super-exponential growth in the
space of configurations makes brute force and exact methods intractable.
1.1 Jet physics
During high-energy particle collisions, such as those observed at the Large Hadron Collider (LHC) at CERN,
collimated sprays of particles called jetsare produced. The jet constituents are the observed final-state
particles that hit the detector and are originated by a showering process (described by QCD) where an
initial (unstable) particle goes through successive binary splittings. Intermediate (latent) particles can be
identified as internal nodes of a hierarchical clustering and the final-state (observed) particles correspond to
the leaves in a binary tree. Fig. 1 provides a schematic representation of this process. This results in several
possible latent topologies corresponding to a set of leaves. This representation, first suggested in Louppe
et al. (2019), connects jets physics with natural language processing (NLP) and biology.
1.2 Collider data analysis
A main problem in data analyses of collider physics deals with estimating the latent showering process
(hierarchical clustering) of a jet, which is needed for subsequent tasks that aim to identify the creation
of different types of sub-atomic particles. The final goal is to perform precision measurements to test the
predictions of the Standard Model of Particle Physics and explore potential models of new physics particles,
therebyadvancingourcomprehensionoftheuniverse’sfundamentalconstituents. Theimprovedperformance
of deep learning jet classifiers (Butter et al., 2019) (for the initial state particles) over traditional clustering-
based physics observables gives evidence of the limitedness of current clustering algorithms since the right
clustering should be optimal for classification tasks. Thus, high-quality approximations in this context would
be highly beneficial for data analyses in experimental particle physics.
1.3 Jet simulators
Currently, there are high-fidelity simulations for jets, such as Pythia(Sjostrand et al., 2006), Herwig(Bellm
et al., 2016), and Sherpa(Gleisberg et al., 2009). These simulators are grounded in QCD but make several
approximations that introduce parametric modeling choices that are not predicted from the underlying the-
ory, so they are commonly tunedto match the data. Many tasks in jet physics can be framed in probabilistic
terms (Cranmer et al., 2021). In particular, we consider the challenges of calculating the maximum likelihood
hierarchy given a set of leaves (jet constituents), the posterior distribution of hierarchies, as well as estimat-
ing the marginal likelihood. Also, these quantities are relevant to tune the parameters of the simulators.
While these formulations are helpful conceptually, they are not practical in current high-fidelity simulations
2Published in Transactions on Machine Learning Research (12/2024)
for jets, given that the likelihood is typically intractable (they are implicit models). Thus, we consider
Ginkgo(Cranmer, Kyle et al., 2021; Cranmer et al., 2019b): a semi-realistic generative model for jets with
a tractable joint likelihood and captures essential ingredients of parton shower generators in full physics
simulations. In particular, Ginkgowas designed to enable implementations of probabilistic programming,
differentiable programming, dynamic programming and variational inference.Within the analogy between
jets and NLP, Ginkgocan be considered as ground-truth parse trees with a known language model.
1.4 Related work
1.4.1 Jet clustering
For each jet produced at the LHC, there is an inference task on the latent hierarchy that typically involves
10 to 100 particles (leaves). Though this is a relatively small number of elements, exhaustive solutions
are intractable, and current exact methods, e.g., Greenberg et al. (2020; 2021), have limited scalability.
The industry standard uses agglomerative clustering techniques, which are greedy and based on heuris-
tics (Cacciari et al., 2008; Catani et al., 1993; Dokshitzer et al., 1997; Ellis & Soper, 1993), typically finding
low-quality hierarchical clusterings. Regarding likelihood-based clustering (applied to Ginkgodatasets),
previous work Greenberg et al. (2020) introduced a classical data structure and dynamic programming al-
gorithm (the cluster trellis ) that exactly finds the marginal likelihood over the space of configurations and
the maximum likelihood hierarchy. Also, an A* search algorithm combined with a trellisdata structure that
finds the exact maximum likelihood hierarchy was introduced in Greenberg et al. (2021). Finally, Cranmer
et al. (2022) pairs Ginkgowith the cluster trellis (Greenberg et al., 2020), to use the marginal likelihood to
directly characterize the discrimination power of the optimal classifier (J. Stuart & Arnold, 1994; Cranmer
& Plehn, 2007) as well as to compute the exact maximum likelihood estimate for the simulator’s parameters.
While these works provide exact algorithms that extend the reach of brute force methods, they have an
exponential space and time complexity, becoming intractable for datasets with as few as 15 leaves. For this
reason, Greenberg et al. (2020; 2021) also provide approximate solutions at the cost of finding lower-quality
hierarchies.
1.4.2 Bayesian inference
A recent body of research has melded variational inference (VI) and sequential search. These connections are
realized through the development of a variational family for hidden Markov models, employing Sequential
Monte Carlo ( Smc) as the marginal likelihood estimator (Maddison et al., 2017; Naesseth et al., 2018;
Le et al., 2018; Moretti et al., 2019; 2020; 2021). Within the field of Bayesian phylogenetics (the study
of evolutionary histories), various methods have been proposed for inference on tree structures. Common
approaches include local search algorithms like random-walk Mcmc(Ronquist et al., 2012) and sequential
search algorithms like Combinatorial Sequential Monte Carlo ( Csmc) (Bouchard-Côté et al., 2012; Wang
et al., 2015). Mcmcmethods also handle model learning. Dinh et al. (2017) proposes ppHmcwhich extends
Hamiltonian Monte Carlo to phylogenies. Evaluating the likelihood term in Mcmcacceptance ratios can
be challenging. As a workaround, particle Mcmc(Pmcmc) algorithms use Smcto estimate the marginal
likelihood and define Mcmcproposals for parameter learning (Wang & Wang, 2020).
Pseudo-marginal methods are a class of statistical techniques used to approximate difficult-to-compute prob-
abilities, typically by introducing auxiliary random variables to form an unbiased estimate of the target prob-
ability (Andrieu & Roberts, 2009). Beaumont (2003) introduced a method in genetics to sample genealogies
in a fully Bayesian framework. Tran et al. (2016) utilizes pseudo-marginal methods to perform variational
Bayesian inference with an intractable likelihood. Our work is a synthesis of Wang et al. (2015) and Moretti
et al. (2021) in that we introduce a variational approximation on topologies using Smcand a VI framework
to learn parameters.
1.5 Contributions of this Paper:
1. We expand upon the Csmctechnique introduced by Wang et al. (2015) and the Ncsmcmethod from
Moretti et al. (2021) to introduce a Combinatorial Sequential Monte Carlo framework for inferring
hierarchical clusterings for jets. The resulting estimators are unbiased and consistent. To the best
3Published in Transactions on Machine Learning Research (12/2024)
of our knowledge, this marks the first adaptation of Smcmethods to jet reconstruction in particle
physics.
2. We leverage the resulting Smcestimators to develop two approximate posteriors on jet hierarchies
and correspondingly two VI methods for parameter learning. We illustrate the effectiveness of
both methods through experiments using data generated with Ginkgo(Cranmer, Kyle et al., 2021),
highlighting superior speed and accuracy across various tasks.
3. In order to circumvent parametric modeling assumptions, we propose a unification of the generative
model and the inference process. Building upon the point estimators, we define a distinct variational
family over global and local parameters for a fully Bayesian treatment of all variables.
4. We show how partial states and re-sampled indices generated by Smccan be interpreted as auxil-
iary random variables within a pseudo-marginal framework, thus establishing connections between
variational pseudo-marginal methods and Vsmc(Naesseth et al., 2018; Moretti et al., 2021).
2 Background
Section 2.1 provides an overview of the Ginkgogenerative model for jet physics (Cranmer, Kyle et al., 2021).
Section 2.3.1 summarizes the approximate inference techniques this work builds upon.
2.1 Ginkgo generative model
In this subsection we provide an overview of the generative process in Ginkgoas well as jet (binary tree)
reconstruction during inference. As mentioned in the introduction, Ginkgois a semi-realistic model designed
to simulate a jet. The branching history of a jet is depicted as a binary tree structure τ= (V,E)where
τdenotes topology, Vcomprises the set of vertices, and Ethe set of edges. Each node is characterized
by a 4D (energy-momentum) vector z= (E∈R+,⃗ p∈R3)whereEdenotes energy and ⃗ p= (px,py,pz)
denotes momentum in the respective dimensions. The squared mass t=t(z):=E2−|⃗ p|2is calculated
using the energy-momentum vector z. The terminal nodes (or leaf nodes), represented as X={x1,···,xN},
correspond to the observed energy-momentum vectors measured at the detector. The tree topology τand
the energy-momentum vectors associated with internal nodes, denoted as Z={z1,···,zN−1}, are latent
variables in the model.
2.1.1 Generative process
InGinkgothegenerativeprocessbeginswiththesplittingofaparent(root)nodewithinvariantmasssquared
tPinto two children, as shown schematically in Fig. 2 (left). The process is characterized by a cutoff mass
squaredtcut, and a rate parameter λfor the exponential distribution governing the decay. During generation,
as long as the invariant mass squared of a node exceeds the cutoff value ( tP>tcut), that node is promoted
to be a parent and the algorithm recursively splits it. The squared masses of each new left (L) and right (R)
child nodes ( tL,tR) are obtained from sampling from the exponential distribution f(t|λ,tP)defined in Eq. 1,
with parameters specific to each child ( L,R). Finally, once tLandtRare sampled, the corresponding energy-
momentum vectors ( zL,zR) for the (L,R) nodes are derived from tP,tLandtRfollowing energy-momentum
conservation rules, i.e. applying a 2-body particle decay (see (Cranmer, Kyle et al., 2021; Cranmer et al.,
2019b) for more details). Next, we specify the exponential distribution
f(t|λ,ti
P) =1
1−e−λλ
ti
Pe−λt
ti
P, (1)
where the first term (1−e−λ)−1λ/ti
Pis a normalization factor, i∈{L,R},tL
P=tP, andtR
P= (√tP−√tL)2.
To satisfy energy-momentum conservation (√tL+√tR<√tP),tLis sampled before tR. Thus, in Ginkgo
the left child mass squared tLis sampled first with tL
P=tPand then an auxiliary value tR
p= (√tP−√tL)2
is calculated to sample the right child mass squared tR(see Fig. 2 (left)).
There are different types of jets, depending on the type of initial state particle (root of the binary tree).
To simulate QCD-like jets, a single λparameter is employed for the entire process. However, for a heavy
4Published in Transactions on Machine Learning Research (12/2024)
tP=t(zP)>tcut
P
tL∼f(·|λ,tP)tR∼f(·|λ,(√tP−√tL)2)L R
(a)Ginkgo generative process.tP=t(zL+zR)
P ℓ(tL,λ,tcut,tL
P,tP)
=f(t|λ,tP)ℓ(tR,λ,tcut,tR
P,tP)
=f(t|λ,(√tP−√tL)2)
L
tL=t(zL)R
tR=t(zR)
(b) Node splitting likelihood reconstruction process.
Figure 2: Illustration of the Ginkgogenerative and reconstruction processes. (a) Ginkgostarts with a parent
node characterized by a 4-vector zp= (E,⃗ p)and invariant mass squared tP=E2−|⃗ p|2. IftPis greater
than the cut off value tcut, then the parent node splits (we have a particle decay). The left and right nodes
invariant mass squared ( tL,tR) are sampled from a truncated exponential distribution defined in Eq. 1. (b)
The splitting likelihood reconstruction process of a node, defined in Eq. 4 begins with two child nodes L
andRalong with their respective 4-vectors zLandzR. The 4-vector for the parent node Pis calculated
aszP=zL+zRand thentP=t(zP). Next, we obtain tL=t(zL),tR=t(zR)and define tL
P=tP,
andtR
P= (√tP−√tL)2. Finally, the left splitting term likelihood ℓ(tL,λ,tcut,tL
P,tP)and the right one
ℓ(tR,λ,tcut,tR
P,tP)are evaluated.
resonance particle decay, such as a Wboson jet, the initial (root node) splitting is governed by a model
parameterλ1, while the subsequent ones are characterized by λ2.
2.1.2 Jet reconstruction during inference
In addition to the generative model, we also need to be able to assign a likelihood value to a proposed jet
clustering (binary tree) during inference. To do this we use the same general form for the jet’s likelihood
based on a product of likelihoods over each splitting. In order to evaluate this we need to first reconstruct
each parent from its left and right children. Different tree topologies give rise to different tPvalues for the
inner nodes and thus different likelihoods. Notably, in Ginkgothe likelihood of a tree is expressed in terms
of the product (in linear space) of all splitting likelihoods (specified in Eq. 4 and referred to as the partial
likelihood ) of a parent into two children. The likelihood of a splitting connects parent with child nodes (i.e.
we have a likelihood of sampling a child with squared mass tgiven a parent with squared mass tP). Thus,
parent and child nodes are not independent. However, splitting likelihoods of different parent nodes are
independent, given a tree. For a set of observed energy-momentum vectors X={x1,···,xN}(leaf nodes),
parameters θ, and a tree topology τ, the likelihood of a splitting history can be evaluated efficiently.
2.2 Parent node splitting likelihood reconstruction
At inference time, a parent node with energy-momentum vector zPis obtained by adding its children values
(zL,zR) as shown schematically in Fig. 2 (right), and tPis calculated deterministically given zP. The
likelihood of a parent splitting into a left (right) child is defined as follows:
ℓ(ti,λ,tcut,ti
P,tP) =/braceleftigg
f(ti|λ,ti
P), t P>tcut
Fs(tcut,tP), tP≤tcut,(2)
wherei∈{L,R}(note that to fix a degree of freedom, tL
P=tP, andtR
P= (√tP−√tL)2) andtcutis the
cutoff mass squared scale for the binary splitting process to stop (if ti≤tcut, the corresponding node is a
leaf of the binary tree). We introduce the cumulative density function Fs(tcut,tP)for a given generative
process to stop, i.e. the probability of having sampled a value of tP<tcut, as
Fs(tcut,tP) =

1−e−λtcut/tP
1−e−λ, tP>tcut
1, t P≤tcut,(3)
5Published in Transactions on Machine Learning Research (12/2024)
Resample
A
B
C
D
A
B
C
D
A
B
C
DPropose
A
B
C
D
A
B
C
D
A
B
C
DA
B
C
D
A
B
C
D
A
B
C
DWeighting
A
B
C
D
A
B
C
D
A
B
C
D
Figure 3: Summary of the Csmcframework: A total of Kpartial states{sk
r}K
k=1are retained as collections
of tree structures encompassing the data set. A partial state is defined as a collection of trees, which start
out as singleton particles A,B,CandD. Each iteration within Algorithm 2 comprises three key stages: (1)
resampling partial states based on their importance weights {wk
r}K
k=1, (2) proposing an expansion of each
partial state to form a new one by linking two trees within the forest, and (3) determining the new weights
for these new partial states. The illustration above depicts three samples across a jet consisting of observed
four particles, denoted as A,B,C, andD.
Taking this into account, the probability F(tL,tR,λ,tcut,tP)of a parent node splitting at inference time
can be reconstructed as the product of the probability of splitting (1−Fs(tcut,tP))times the likelihood of
a parent splitting into left and right children as follows:
F(tL,tR,λ,tcut,tP) =1
4π(1−Fs(tcut,tP))·ℓ(tL,λ,tcut,tL
P)·ℓ(tR,λ,tcut,tR
P). (4)
where the factor1
4πcomes from the likelihood of sampling uniformly over the two-sphere during the 2-
body particle decay process. Also, at inference time, given two particles, we assign tL→max{tL,tR}and
tR→min{tL,tR}.
2.3 Approximate Inference
2.3.1 Bayesian jet reconstruction.
The goal of jet reconstruction is to infer the splitting history and properties of subatomic particles pro-
duced during high-energy collisions. This involves estimating various parameters such as particle momenta,
positions, and types. Bayesian methods provide a natural framework for modeling uncertainty and incor-
porating prior information into the reconstruction process. Let X={x1,···,xN}denote the matrix of
observed energy-momentum vectors of the Ginkgo model. The posterior distribution can be expressed as
follows:
P(τ|X,λ) =P(X|τ,λ)P(τ|λ)
P(X|λ). (5)
Calculatingthedenominatorrequiresmarginalizingover (2N−3)!!distinctjettopologieswhichisintractable.
The two interconnected tasks for Bayesian jet reconstruction are: (i) computing the normalization constant
P(X|λ)by marginalizing all possible candidate topologies:
P(X|λ) =/summationdisplay
τP(X|τ,λ)P(τ|λ), (6)
6Published in Transactions on Machine Learning Research (12/2024)
and (ii): learning or optimizing the likelihood in Eq. 5 obtained by marginalizing Eq. 6: ˆλ=
arg max
ˆλ∈λlogP(X|λ).Variational Inference (VI) offers an approach to tackle both tasks for non-trivial poste-
rior distributions.
2.3.2 Variational Inference.
VI is a method used to estimate the posterior distribution P(τ,λ|X)when its direct computation is in-
tractable (due to the complexity of marginalizing the latent variables τ). To address this challenge, VI
introduces a tractable distribution Q(λ,τ|X)to create a lower bound LELBOon the log-likelihood:
logP(X)≥LELBO (X):=E
Q/bracketleftbiggP(τ,λ,X)
Q(τ,λ|X)/bracketrightbigg
(7)
In the context of Auto-Encoding Variational Bayes ( Aevb, bothQ(τ,λ|X)andP(λ,τ,X)are jointly
trained (Kingma & Welling, 2013; Rezende et al., 2014). To approximate the expectation in Eq.7, Monte
Carlo samples from Q(τ,λ|X)are averaged, and these samples are reparameterized using a deterministic
function of a random variable that is independent of τ.
Obtaining a feasible approximation for jet structures can be a complex task, leading us to modify and adapt
Csmc.
2.3.3 Combinatorial Sequential Monte Carlo.
Csmc, tailored for phylogenetic tree models, approximates a sequence of increasing probability spaces,
ultimately aligning with Eq. 5 (Wang et al., 2015). Csmcemploys sequential importance resampling across
{r}N−1
r=1steps to approximate both the unnormalized target distribution πand its normalization constant,
denoted as∥π∥, constituting the numerator and denominator in Eq. 5, by Kpartial states{sk
r}K
k=1∈Srto
form a distribution (see Wang et al. (2015) or Appendix B),
/hatwideπr=∥/hatwideπr−1∥1
KK/summationdisplay
k=1wk
rδskr(s)∀s∈S. (8)
Csmc, in contrast to standard Smctechniques, manages a combinatorial set representing the realm of tree
topologies alongside the continuous branch lengths—both of which are characteristic features of phyloge-
nies (Wang et al., 2015). Partial states (Monte Carlo samples) are resampled at each rank r, ensuring
samples remain in high-probability regions, and importance weights are defined as:
wk
r=w(sak
r−1
r−1,sk
r) =π(sk
r)
π(sak
r−1
r−1)·ν−(sak
r−1
r−1)
q(skr|sak
r−1
r−1), (9)
whereq(sk
r|sak
r−1
r−1)specifies a proposal distribution, ak
r−1∈{1,···,K}denote resampled ancestor indices
withP(ak
r−1=i) =wi
r−1//summationtextK
l=1wl
r−1, andν−is an overcounting correction defined in Wang et al. (2015).
Resampled states are then extended via proposal distribution simulations (see Fig. 3). This framework
allows for the construction of an unbiased estimate of the marginal likelihood, converging in L2norm:
/hatwideZCSMC :=∥/hatwideπR∥=R/productdisplay
r=1/parenleftigg
1
KK/summationdisplay
k=1wk
r/parenrightigg
→∥π∥. (10)
TheCsmcmethod is only applicable for sampling toplogies to marginalize over the space of phylogenetic
trees, leading us to adapt Vcsmcto perform VI.
Variational Combinatorial Sequential Monte Carlo. Expanding on the foundation laid by Csmc,
Moretti et al. (2021) introduces Variational Combinatorial Sequential Monte Carlo ( Vcsmc) as an approach
7Published in Transactions on Machine Learning Research (12/2024)
to learn distributions over phylogenetic trees. Vcsmcemploys Csmcas a means to create an unbiased
estimator for the marginal likelihood:
LCSMC :=E
Q/bracketleftig
ˆZCSMC/bracketrightig
. (11)
Inthesamework,Morettietal.(2021)introducesNestedCombinatorialSequentialMonteCarlo( Ncsmc),an
efficient proposal distribution, providing an exact approximation to the intractable locally optimal proposal
forCsmc. We provide a review of Ncsmcin Appendix C. The VI algorithms VcsmcandVncsmc that
utilize the estimators ˆZCSMCand ˆZNCSMCeach introduce a structured approximate posterior that exhibits
factorization across rank events. Each state, denoted as sr, is uniquely characterized by its topology, a
collection of trees forming a forest, and the corresponding branch lengths. To facilitate reparameterization,
discrete terms are either removed from gradient estimates or transformed into Gumbel-Softmax random
variables.
3 Methods
A
C B
DEFGHIπ(s) =/producttext
ζi∈sπ(ζi)
= (/producttext
i∈{A...G}Fi)·FH·FI
=FA·˜FB·˜FC=˜FA
Figure 4: The likelihood FAfor a sub-tree defined on
leaf nodesD,E,FandGis defined as the recursive
product of splitting likelihoods FBandFC. The in-
termediate target π(s3)for the partial state s3also
includes the probability of singletons HandIdenoted
FHandFI.Section 3.1 adapts the Csmcapproach to perform
inference on jet tree structures. Section 3.2 refor-
mulates Vcsmcfor inference on global parameters.
Section 3.2.1 utilizes Vcsmcmethodology to learn
parameters as point estimates. Section 3.2.2 defines
a prior on the model parameters to construct a vari-
ational approximation on both global and local pa-
rameters. The resulting approach is interpreted as
a variational pseudo-marginal method establishing
connectionsbetweenpseudo-marginalmethods(An-
drieu & Roberts, 2009) and Variational Combinato-
rial Sequential Monte Carlo (Naesseth et al., 2018;
Moretti et al., 2021).
3.1 Inference on Tree Structures
SequentialMonteCarlo( Smc)methods(Naessethetal.,2019;Chopin&Papaspiliopoulos,2020)aredesigned
to sample from a sequence of probability spaces, where the final iteration converges to the target distribution.
However, adapting Csmcfor the Ginkgomodel requires a crucial modification: ensuring that the splitting
likelihood at the final coalescent event reflects the dependence on the entire sub-tree splitting history , not
just the most recent split. This dependence is essential for accurately capturing the recursive structure of
the tree. Recall that the splitting likelihood is defined as the product of all splitting likelihoods (see Eq. 4).
To achieve this, we reformulate the splitting likelihood to explicitly account for previous splits, as depicted
in the recurrence relation:
˜F(tL,tR,λ,tcut) =1
4π·(1−Fs(tcut,tP))×ℓ(tL,λ,tcut,tL
P)·ℓ(tR,λ,tcut,tR
P) (12)
×˜F(tLL,tLR,λ,t cut)·˜F(tRL,tRR,λ,t cut).
In the above, the pair i,j∈(L,R)×(L,R)definestijas the mass squared of the jchild of the current i
coalescent node and ˜F(tL,tR,λ,t cut)for leaf nodes simply evaluates to 1. Eq. 12 represents the tree (sub-
tree) likelihood (with root node having squared mass tP) as the recursive product of splitting likelihoods.
Each term brings its normalization, and the overall normalization is correctly expressed as the product of
individual ones. In practice we use dynamic programming to maintain a running sum of cumulative log
probabilities across rank events. The Csmcresampling step illustrated in Fig. 3 is now dependent upon the
full sub-tree splitting history, reflecting the recursive nature of the coalescent process.
In a slight abuse of notation, the probability π(sk
r)of partial state sk
r(recallr∈{1,···,N−1}denotes the
coalescent event and kdenotes the Monte Carlo sample) is defined as the product of the probabilities of all
8Published in Transactions on Machine Learning Research (12/2024)
disjoint trees ζiin the forest s:π(s) =/producttext
ζi∈sπ(ζi).An illustration of a likelihood for a sub-tree along with
the likelihood of a partial state is provided in Fig. 4. The Ncsmcalgorithm defined in Moretti et al. (2021)
can similarly be adjusted to ensure compatibility with this modified framework. The resulting estimators
are unbiased and consistent, for proofs see Wang et al. (2015) and Moretti et al. (2021).
3.2 Inference on Global Parameters
In Section 3.2.1, we define a variational approximation on topologies, while in Section 3.2.2, we outline fully
Bayesian inference using a variational pseudo-marginal framework.
3.2.1 Maximum Likelihood
We introduce a variational approximation on τ, using Csmcand the Aevbframework to optimize λ. Using
Eq. 9 and Eq. 10 to define the weights and estimator respectively, along with Eq. 11 and Eq. 12 to evaluate
π(sk
r)and form the ELBO, we define a variational family:
Qϕ,ψ/parenleftbig
s1:K
1:R,a1:K
1:R−1/parenrightbig:=K/productdisplay
k=1qϕ,ψ(sk
1)×R/productdisplay
r=2K/productdisplay
k=1
wak
r−1
r−1/summationtextK
l=1wl
r−1·qϕ,ψ/parenleftbigg
sk
r|sak
r−1
r−1/parenrightbigg
. (13)
The full factorization of qϕ,ψ(sk
r|sak
r−1
r−1)is written in Eq. 22 and Eq. 23 of the Appendix and depends on the
choice of CsmcorNcsmcas an inference algorithm. We utilize our adaptation of CsmcandNcsmcto
form the two objectives LCSMCandLNCSMC.
3.2.2 Fully Bayesian Inference Using a Variational Pseudo-Marginal Framework
Simulators rooted in quantum chromodynamics are frequently calibrated to align with the data. However,
training a simulator separately from the inference process can lead to inefficiencies. To address this, we
propose a modification of the posterior distribution defined in Eq. 5 to include a prior on λalong with
variational parameters to learn the proposal distribution in Eq. 13. We define a log-normal distribution over
λ∼logN(λ|µ,Σ)so thatλcan be marginalized along with τ. The target distribution can now be specified
as follows:
P(τ,λ|X) =P(X|τ,λ)P(τ|λ)P(λ)
P(X). (14)
The generative model parameters can be defined as θ:=λ={µ,Σ}or as the output of a neural network
and the proposal parameters ϕ={˜µ,˜Σ}used in the variational approximation to Eq. 14 can be shared or
separately trained.
The pseudo-marginal framework is designed to sample from a posterior distribution such as the one defined in
Eq. 5 when the marginal likelihood p(X|λ)cannot be evaluated directly. We would normally be interested in
computingtheposteriordistributionoversplittingtopologiesanddecayparametersdefinedinEq.5; however,
themarginallikelihood p(X|λ)isintractable. Givenaccesstoafunction ˆg(u;X,λ)acceptingrandomnumbers
u∼r(u)that can be evaluated pointwise, assume ˆg(u;X,λ)returns a non-negative unbiased estimate of
P(X|λ):
E
r(u)[ˆg(u;X,λ)] =/integraldisplay
ˆg(u;X,λ)r(u)du=p(X|λ). (15)
In our setup, ˆg(u;X,λ)is defined in Eq. 13 and the auxiliary random variables u:= (s1:K
1:R,a1:K
1:R−1)are
generated via the CsmcorNcsmcalgorithm. Let p(λ,u)be a joint target distribution over λandu:
p(λ,u) =g(u;X,λ)r(u)p(λ)
p(X), (16)
The integral of the expression above equals one, and its marginal distribution corresponds to the posterior
distribution:
π(λ) =/integraldisplayg(u;X,λ)r(u)p(λ)
p(X)du=p(X|λ)p(λ)
p(X). (17)
9Published in Transactions on Machine Learning Research (12/2024)
Figure 5: Left: Scatterplot comparing log-conditional likelihood of Vncsmc withK,M = (256,1)vs Greedy
Search and Center: Scatterplot comparing log-conditional likelihood of Vncsmc withK,M = (256,1)vs
Beam Search. Across 100 simulated jets, Vncsmc returns higher likelihood on all 100 cases against Greedy
Search and 99 cases against Beam Search. Right: Log-conditional likelihood values for Vcsmc(blue) and
Vncsmc (red) withK={256}(andM= 1) samples averaged across 5 random seeds. Vncsmc achieves
convergence in fewer epochs than Vcsmcand yields higher values, all while maintaining lower stochastic
gradient noise. Additional experiments demonstrating the effect of Kappear in Fig. 9 of the Appendix.
This implies that by employing nearly any approximate inference technique to Eq. 16, the resulting marginal
distribution of that approximation will serve as an estimate of the actual target distribution. The pseudo-
marginal framework by Andrieu & Roberts (2009) designs a Markov Chain (θi,ui)with Eq. 16 as its target
distribution.
Given the conditional likelihood p(X|λ,τ)we could run MCMC only on the parameter p(λ). Instead, we
take the approach of sampling Ktopologiesτk∼p(τ|λ)so that
1
KK/summationdisplay
k=1p(X|τk,λ)p(λ) =⇒
asK→∞p(X|λ)p(λ),
where an explicit approximation to p(τ|λ)is defined by the modified Csmcalgorithm. This is a form
ofvariational pseudo-marginal setup where we are interested in approximately marginalizing out all jet
structures. Our distribution estimator which marginalizes λis interpreted analogously.
4 Experiments
Theindustrystandardinparticlephysicsusesagglomerativeclusteringtechniques,whicharegreedy(Cacciari
et al., 2012). Beam Search provides a straightforward and significant improvement. Thus, we consider both
Greedy and Beam Search as standard, relevant and efficient baselines, also applied in cited works Greenberg
et al. (2020; 2021). We simulated 100 jets using Ginkgorunning comparisons with Greedy Search, Beam
Search and Cluster Trellis.
4.1 MAP estimate
Fig. 5 (left) provides a scatterplot comparing log-conditional likelihood values. Across 100 simulated jets,
Vncsmc withK,M = (256,1)returns a higher likelihood on all 100 cases against Greedy Search (left) and
99 cases against Beam Search (center). Notably, Vncsmc simultaneously conducts inference and λlearning,
a feature lacking in Greedy Search and Beam Search, which rely on the user providing λvalues. Furthermore,
Vncsmc yields probability distributions over topologies, while Greedy Search and Beam Search yield single
topologies.
10Published in Transactions on Machine Learning Research (12/2024)
Figure 6: Left: CsmcandNcsmcconverge to the exact marginal likelihood as Kincreases (log scale).
Center: Log conditional likelihood returned by Vcsmccompared with the exact MAP clustering returned
by the cluster trellis Greenberg et al. (2020), Greedy Search and Beam Search, for a dataset of 100 simulated
jets generated with Ginkgo. Even with K= 256andN= 12,Vcsmcclosely approximates exact cluster
trellis values for specific jets. Right: A comparison of the running times, highlighting that Vcsmcon
N= 64significantly outperforms cluster trellis on N= 12. AsNincreases, the cluster trellis quickly
becomes impractical due to its exponential complexity.
Fig. 5 (right) shows the log conditional likelihood log ˆp(X|τ,λ)forVcsmc(blue) and Vncsmc (red) with
K= 256(andM= 1forVncsmc ) samples averaged across 5 random seeds. Vncsmc converges in fewer
epochs and achieves higher likelihood values with lower gradient noise. Additional experiments illustrating
the effect of Kappear in Fig. 9 in the Appendix. Larger Kyields higher log ˆp(X|τ,λ)values and reduces
stochastic gradient noise.
4.2 Running Time and Complexity.
We generated jets with N={4,···,64}leaf nodes and profiled the running time of Vcsmc, Cluster Trellis,
Greedy Search and Beam Search averaged across 3 random seeds. All experiments were performed on
a Google Cloud Platform n1-standard-4 instance with an Intel Xeon CPU 4 vCPUs and 15 GB RAM
without leveraging GPU utilization.
Fig. 6 (left) illustrates convergence of the CsmcandNcsmcconditional likelihood to the exact cluster trellis
marginal likelihood as Kincreases. Fig. 7 plots the inferred Log-Normal Pseudo-Marginal Distribution
for the parameters µ= (µ1,µ2)of the Heavy Resonance Jet, estimated through Ncsmc. Contours of the
log-conditional likelihood are shown with stochastic gradient steps taken on LNCSMChighlighted in red.
Next, we compare in Fig. 6 (center) the log conditional likelihood returned by VCSMC, Greedy Search and
Beam Search with the exact Maximum a Posteriori (MAP) clustering value calculated with the cluster trellis
technique described in Greenberg et al. (2020), for a dataset of 100 jets. We see that VCSMC returns high
quality hierarchies, with log likelihood values close to the exact ones.
Fig. 6 (right) reports the running times on a log scale in seconds. Vcsmcis an order of magnitude faster
than Beam Search on N= 20leaf nodes. Beam search entails managing a list of log-likelihood pairs at each
level and for each beam size b. The given list is sorted, iterated over, and selectively only a single topology
is retained at a time. This incurs a complexity of O(b2Nlogb+bN3logN). Typically b > N, in which
case the complexity can be prohibitively slow, but for b<N2logN, the complexity becomes O(bN3logN).
In contrast, NcsmcisO(KN3M)and the CsmcisO(KNM ), whereK,NandMdenote the number of
Monte Carlo samples, the number of leaf nodes (observed particles), and the number of subsamples.
5 Discussion
11Published in Transactions on Machine Learning Research (12/2024)
In all experiments, tcutis an exogenous variable, and in full physics simulations its value is determined by the
energy scale where there is a qualitative change in the theory that describes the process, i.e. we switch from
a showering to a hadronization process, described by different models of physics. The difference between the
energy of the initial state particle (root of the binary tree) and this energy scale ( tcut) affects the number
of final state particles in the Monte Carlo sampling. Given that Ginkgo does not include a full quantum
chromodynamics (QCD) modeling of the splitting likelihood, we choose tcutto control the distribution on
the number of final state particles and mimic full physics simulation processes. As such, the exact value is
not relevant and the developed algorithms capabilities are independent of it.
Figure 7: Inferred log-normal pseudo-marginal distri-
bution for the parameters ˆµ= (ˆµ1,ˆµ2)of the heavy
resonance jet, estimated using Ncsmc. Contours rep-
resent the log-conditional likelihood, with stochastic
gradient steps on LNCSMChighlighted in red.Simulators rooted in QCD present significant chal-
lenges due to their complex likelihoods. Rewriting
these simulators is a substantial endeavor, demand-
ing considerable expertise and effort, with special-
ists often dedicating entire careers to mastering the
intricacies of QCD. Real-world data utilization ne-
cessitates strict adherence to the models embedded
within these simulators, further complicating the
task. Moreover, the lack of currently available pow-
erful quantum computers adds another layer of dif-
ficulty, prompting high-energy researchers, such as
those at Iris-Hep , to rely on approximations like
Ginkgo (Cranmer et al., 2019a).
TheCsmcmethod (Wang et al., 2015) and the
Variational Combinatorial Sequential Monte Carlo
(Vcsmc) method (Moretti et al., 2021) both use
a specific generative model for biology based on a
continuous time Markov chain. Our paper repre-
sents the first adaptation of Sequential Monte Carlo
methods for jet reconstruction in particle physics.
This is achieved by adapting the Ginkgo model and
redefining the parent node splitting likelihood re-
cursively (Eq. 12) to ensure that, at the final Csmc
rank event, the support of the proposal distribution
aligns with that of the target distribution.
5.1 Conclusion
Variationalandpseudo-marginalmethodsarebroadlyapplicableacrossarangeofcombinatorialandcontinu-
ous problems. In Bayesian phylogenetics, these methods facilitate efficient estimation of marginal likelihoods
over an equivalent factorial search space of tree topologies, (2N−3)!!, allowing for uncertainty in both tree
structure and branch lengths (Moretti et al., 2021; Zhang & Matsen IV, 2019; Zhang et al., 2021). Addi-
tionally, in social and biological network analysis, variational techniques are central to hierarchical clustering
and stochastic block models for complex, structured latent spaces (Zhou, 2015; Zhang & IV, 2018; Greenberg
et al., 2021). Monte Carlo methods, widely used to model self-avoiding random walks on combinatorial struc-
tures, provide insight into diffusion and spatial distribution phenomena that bridge into statistical mechanics
and more complex particle interaction models (Madras & Slade, 1996; Grosberg et al., 2006). These tech-
niques also extend to probabilistic graphical models and Bayesian networks, where factorized approximations
reduce computational burden, supporting inference on high-dimensional data (Zhang & IV, 2018).
We have introduced the first adaptation of Csmcfor unbiased and consistent jet reconstruction, proposing
approximate posteriors and variational inference (VI) techniques for both point and distribution estimators.
Our approach significantly improves both speed and accuracy, paving the way for broader adoption of
variational methods in collider data analyses. This work not only provides a robust framework for jet
reconstruction but also sets the stage for future advancements in the application of advanced approximate
inference methods to high-energy physics experiments.
12Published in Transactions on Machine Learning Research (12/2024)
References
Christophe Andrieu and Gareth O. Roberts. The pseudo-marginal approach for efficient Monte Carlo
computations. The Annals of Statistics , 37(2):697 – 725, 2009. doi: 10.1214/07-AOS574. URL
https://doi.org/10.1214/07-AOS574 .
MohammadHossein Bateni, Soheil Behnezhad, Mahsa Derakhshan, MohammadTaghi Hajiaghayi, Raimon-
das Kiveris, Silvio Lattanzi, and Vahab Mirrokni. Affinity clustering: Hierarchical clustering at scale. In
Advances in Neural Information Processing Systems (NeurIPS) , 2017.
Mark A Beaumont. Estimation of Population Growth or Decline in Genetically Monitored Populations.
Genetics , 164(3):1139–1160, 07 2003. ISSN 1943-2631. doi: 10.1093/genetics/164.3.1139. URL https:
//doi.org/10.1093/genetics/164.3.1139 .
Johannes Bellm et al. Herwig 7.0/Herwig++ 3.0 release note. Eur. Phys. J. C , 76(4):196, 2016. doi:
10.1140/epjc/s10052-016-4018-8.
Alexandre Bouchard-Côté, Sriram Sankararaman, and Michael Jordan. Phylogenetic inference via sequential
Monte Carlo. Systematic biology , 61:579–93, 01 2012.
Anja Butter et al. The Machine Learning landscape of top taggers. SciPost Phys. , 7:014, 2019. doi:
10.21468/SciPostPhys.7.1.014.
Matteo Cacciari, Gavin P. Salam, and Gregory Soyez. The anti- ktjet clustering algorithm. JHEP, 04:063,
2008. doi: 10.1088/1126-6708/2008/04/063.
Matteo Cacciari, Gavin P. Salam, and Gregory Soyez. Fastjet user manual: (for version 3.0.2). The European
Physical Journal C , 72(3), March 2012. ISSN 1434-6052. doi: 10.1140/epjc/s10052-012-1896-2. URL
http://dx.doi.org/10.1140/epjc/s10052-012-1896-2 .
S Catani, Yuri L Dokshitzer, M H Seymour, and B R Webber. Longitudinally invariant ktclustering
algorithms for hadron hadron collisions. Nucl. Phys. B , 406:187–224, 1993. doi: 10.1016/0550-3213(93)
90166-M.
NicolasChopinandOmirosPapaspiliopoulos. An introduction to sequential Monte Carlo , volume4. Springer,
2020.
K. Cranmer and T. Plehn. Maximum significance at the lhc and higgs decays to muons. The European
Physical Journal C , 51(2):415–420, Jun 2007. ISSN 1434-6052. doi: 10.1140/epjc/s10052-007-0309-4.
URL http://dx.doi.org/10.1140/epjc/s10052-007-0309-4 .
K.Cranmer, S.Macaluso, andD.Pappadopulo. Toygenerativemodelforjets, 2019a. URL https://indico.
cern.ch/event/841905/contributions/3533259/attachments/1915283/3166245/ToyJetsModel.pdf .
The Institute for Research and Innovation in Software for High Energy Physics (IRIS-HEP) Workshop on
ML4Jets.
Kyle Cranmer, Sebastian Macaluso, and Duccio Pappadopulo. Toy Generative Model for Jets Package.
https://github.com/SebastianMacaluso/ToyJetsShower , 2019b.
Kyle Cranmer, Matthew Drnevich, Sebastian Macaluso, and Duccio Pappadopulo. Reframing jet physics
with new computational methods. EPJ Web of Conferences , 251:03059, 2021. doi: 10.1051/epjconf/
202125103059. URL https://doi.org/10.1051%2Fepjconf%2F202125103059 .
Kyle Cranmer, Matthew Drnevich, Lauren Greenspan, Sebastian Macaluso, and Duccio Pappadopulo. Com-
putingthebayes-optimalclassifierandexactmaximumlikelihoodestimatorwithasemi-realisticgenerative
model for jet physics. Machine Learning and the Physical Sciences, NeurIPS , abs/2002.11661, 2022. URL
https://ml4physicalsciences.github.io/2022/files/NeurIPS_ML4PS_2022_32.pdf .
Cranmer, Kyle, Drnevich, Matthew, Macaluso, Sebastian, and Pappadopulo, Duccio. Reframing jet physics
with new computational methods. EPJ Web Conf. , 251:03059, 2021. doi: 10.1051/epjconf/202125103059.
URL https://doi.org/10.1051/epjconf/202125103059 .
13Published in Transactions on Machine Learning Research (12/2024)
Vu Dinh, Arman Bilge, Cheng Zhang, and Frederick A. Matsen, IV. Probabilistic path Hamiltonian Monte
Carlo. volume 70 of Proceedings of Machine Learning Research , pp. 1009–1018, International Convention
Centre, Sydney, Australia, 06–11 Aug 2017. PMLR.
Yuri L Dokshitzer, G D Leder, S Moretti, and B R Webber. Better jet clustering algorithms. JHEP, 08:1,
1997. doi: 10.1088/1126-6708/1997/08/001.
AvinavaDubey, QirongHo, SineadWilliamson, andEricPXing. Dependentnonparametrictreesfordynamic
hierarchical clustering. Advances in Neural Information Processing Systems (NeurIPS) , 2014.
Kumar Avinava Dubey, Michael Zhang, Eric Xing, and Sinead Williamson. Distributed, partially collapsed
mcmc for bayesian nonparametrics. In International Conference on Artificial Intelligence and Statistics ,
2020.
Stephen D Ellis and Davison E Soper. Successive combination jet algorithm for hadron collisions. Phys.
Rev. D, 48:3160–3166, 1993. doi: 10.1103/PhysRevD.48.3160.
T. Gleisberg, Stefan. Hoeche, F. Krauss, M. Schonherr, S. Schumann, F. Siegert, and J. Winter. Event
generation with SHERPA 1.1. JHEP, 02:007, 2009. doi: 10.1088/1126-6708/2009/02/007.
Spence Green, Nicholas Andrews, Matthew R. Gormley, Mark Dredze, and Christopher D. Manning. En-
tity clustering across languages. In Conference of the North American Chapter of the Association for
Computational Linguistics: Human Language Technologies (NAACL-HLT) , 2012.
Craig S. Greenberg, Sebastian Macaluso, Nicholas Monath, Ji-Ah Lee, Patrick Flaherty, Kyle Cranmer,
Andrew McGregor, and Andrew McCallum. Data Structures \& Algorithms for Exact Inference in Hier-
archical Clustering. 2 2020. URL https://arxiv.org/abs/2002.11661 .
Craig S. Greenberg, Sebastian Macaluso, Nicholas Monath, Avinava Dubey, Patrick Flaherty, Manzil Zaheer,
Amr Ahmed, Kyle Cranmer, and Andrew McCallum. Exact and approximate hierarchical clustering using
a*, 2021.
A. Y. Grosberg, K. Kremer, and M. Kardar. The physics of polymers: from gaussian chains to polymer gels.
Reviews of Modern Physics , 78(2):393–418, 2006.
Zhiting Hu, Ho Qirong, Avinava Dubey, and Eric Xing. Large-scale distributed dependent nonparametric
trees.International Conference on Machine Learning (ICML) , 2015.
A. Ord J. Stuart and S. Arnold. Kendall’s advanced theory of statistics. In Vol 2A (6th Ed.) (Oxford
University Press, New York , 1994.
Diederik P Kingma and Max Welling. Auto-encoding variational Bayes, 2013.
Tuan Anh Le, Maximilian Igl, Tom Rainforth, Tom Jin, and Frank Wood. Auto-encoding sequential Monte
Carlo. In International Conference on Learning Representations , 2018.
Gilles Louppe, Kyunghyun Cho, Cyril Becot, and Kyle Cranmer. QCD-Aware Recursive Neural Networks
for Jet Physics. JHEP, 01:057, 2019. doi: 10.1007/JHEP01(2019)057.
Chris J. Maddison, Dieterich Lawson, George Tucker, Nicolas Heess, Mohammad Norouzi, Andriy Mnih,
Arnaud Doucet, and Yee Whye Teh. Filtering variational objectives. 2017.
Neal Madras and Gordon Slade. The Self-Avoiding Walk . Birkhäuser, 1996.
Nicholas Monath, Ari Kobren, Akshay Krishnamurthy, Michael R Glass, and Andrew McCallum. Scalable
hierarchical clustering with tree grafting. In Knowledge Discovery & Data Mining (KDD) , 2019.
Nicholas Monath, Avinava Dubey, Guru Guruganesh, Manzil Zaheer, Amr Ahmed, Andrew McCallum,
Gokhan Mergen, Marc Najork, Mert Terzihan, Bryon Tjanaka, et al. Scalable bottom-up hierarchical
clustering. arXiv preprint arXiv:2010.11821 , 2020.
14Published in Transactions on Machine Learning Research (12/2024)
Nicholas Monath, Manzil Zaheer, Kumar Avinava Dubey, Amr Ahmed, and Andrew McCallum. Dag-
structured clustering by nearest neighbors. In International Conference on Artificial Intelligence and
Statistics , 2021.
AntonioKhalilMoretti,ZizhaoWang,LuhuanWu,IddoDrori,andItsikPe’er. Particlesmoothingvariational
objectives. CoRR, abs/1909.09734, 2019.
Antonio Khalil Moretti, Zizhao Wang, Luhuan Wu, Iddo Drori, and Itsik Pe’er. Variational objectives for
Markovian dynamics with backward simulation. European Conference on Artificial Intelligence , 2020.
Antonio Khalil Moretti, Liyi Zhang, Christian A. Naesseth, Hadiah Venner, David Blei, and Itsik Pe’er.
variational combinatorial sequential monte carlo methods for bayesian phylogenetic inference. In Cassio
de Campos and Marloes H. Maathuis (eds.), Proceedings of the Thirty-Seventh Conference on Uncertainty
in Artificial Intelligence , volume 161 of Proceedings of Machine Learning Research , pp. 971–981. PMLR,
27–30 Jul 2021. URL https://proceedings.mlr.press/v161/moretti21a.html .
C. A. Naesseth, S. Linderman, R. Ranganath, and D. Blei. Variational sequential Monte Carlo. volume 84
ofProceedings of Machine Learning Research , 2018.
C.A. Naesseth, F.Lindsten, and T.B. Schön. Elements ofsequential MonteCarlo. Foundations and Trends ®
in Machine Learning , 12(3):307–392, 2019.
Stanislav Naumov, Grigory Yaroslavtsev, and Dmitrii Avdiukhin. Objective-based hierarchical clustering of
deep embedding vectors. arXiv preprint arXiv:2012.08466 , 2020.
DaniloJimenezRezende,ShakirMohamed,andDaanWierstra. Stochasticbackpropagationandapproximate
inference in deep generative models, 2014.
Fredrik Ronquist, Maxim Teslenko, Paul Mark, Daniel Ayres, Aaron Darling, Sebastian Höhna, Bret Larget,
Liang Liu, Marc Suchard, and John Huelsenbeck. MrBayes 3.2: Efficient bayesian phylogenetic inference
and model choice across a large model space. Systematic biology , 61:539–42, 03 2012.
Torbjorn Sjostrand, Stephen Mrenna, and Peter Z. Skands. PYTHIA 6.4 Physics and Manual. JHEP, 05:
026, 2006. doi: 10.1088/1126-6708/2006/05/026.
Minh-Ngoc Tran, David J. Nott, and Robert Kohn. Variational bayes with intractable likelihood, 2016.
Shikhar Vashishth, Prince Jain, and Partha Talukdar. Cesi: Canonicalizing open knowledge bases using
embeddings and side information. In Proceedings of the 2018 World Wide Web Conference on World Wide
Web, pp. 1317–1327. International World Wide Web Conferences Steering Committee, 2018.
Liangliang Wang, Alexandre Bouchard-Côté, and Arnaud Doucet. Bayesian phylogenetic inference using a
combinatorial sequential Monte Carlo method. Journal of the American Statistical Association , 01 2015.
Shijia Wang and Liangliang Wang. Particle Gibbs sampling for Bayesian phylogenetic inference, 2020.
Cheng Zhang and Frederick A. Matsen IV. Generalizing tree probability estimation via bayesian networks,
2018. URL https://arxiv.org/abs/1805.07834 .
Cheng Zhang and Frederick A Matsen IV. Variational Bayesian phylogenetic inference. In International
Conference on Learning Representations , 2019.
Yixin Zhang, Vladimir N Minin, and Frederick A Matsen IV. Variational inference in phylogenetics. In
Proceedings of the 38th International Conference on Machine Learning , pp. 12311–12321. PMLR, 2021.
Yuchen Zhang, Amr Ahmed, Vanja Josifovski, and Alexander Smola. Taxonomy discovery for personalized
recommendation. In Proceedings of the 7th ACM international conference on Web search and data mining .
ACM, 2014.
MiZhou. Infiniteedgepartitionmodelsforoverlappingcommunitydetectionandlinkprediction. Proceedings
of the 18th International Conference on Artificial Intelligence and Statistics , 38:1135–1143, 2015.
15Published in Transactions on Machine Learning Research (12/2024)
A Ginkgo Generative Model
The Ginkgogenerative process is outlined below: The 2-body decay in the parent rest frame is defined using
Algorithm 1 Toy Parton Shower Generator
Require: parent momentum pµ
p, parent mass squared tP, cut-off mass squared tcut, rate for the exponential
distribution λ, binary tree tree
1:Function NodeProcessing (pµ
p,tP,tcut,λ,tree)
2:Add parent node to tree.
3:iftP>tcutthen
4:SampletLandtRfrom the decaying exponential distribution.
5:Sample a unit vector from a uniform distribution over the 2-sphere.
6:Compute the 2-body decay of the parent node in the parent rest frame.
7:Apply a Lorentz boost to the lab frame to each child.
8:callNodeProcessing (pµ
p,tL,tcut,λ,tree)
9:callNodeProcessing (pµ
p,tR,tcut,λ,tree)
10:end if
momentum pµ
p=pµ
L+pµ
R= (√s,0,0,0). Due to energy-momentum conservation the child energies are given
by
EL=√s
2/parenleftbigg
1 +tL
s−tR
s/parenrightbigg
(18)
ER=√s
2/parenleftbigg
1 +tR
s−tL
s/parenrightbigg
(19)
and the magnitude of their 3-momentum is defined
|⃗ p|=√s
2¯β=√s
2/radicalbigg
1−2(tL+tR)
s+(tL−tR)2
s2(20)
The left and right child momentum are given by pµ
L= (EL,⃗ p)andpµ
R= (ER,−⃗ p)in the parent rest frame.
The Lorentz boost γ=Ep√
tpandγβ=|⃗ pp|/√tp. For more information see Cranmer, Kyle et al. (2021);
Cranmer et al. (2019b).
16Published in Transactions on Machine Learning Research (12/2024)
B Combinatorial Sequential Monte Carlo
We provide an overview of the Csmcalgorithm from Wang et al. (2015).
B.1 Partial States and the Natural Forest Extension
Definition 1 (Partial State) .A rankr∈{0,···,N−1}partial state, symbolized as sr= (ti,Xi), represents
a collection of rooted trees and adheres to the following three conditions:
1. Partial states of different ranks are disjoint, meaning that for any two distinct ranks, rands, there
is no overlap between the sets of partial states, written as ∀r̸=s,Sr∩Ss=∅.
2. The set of partial states at the smallest rank consists of only one element, denoted as S0=⊥.
3. The set of partial states at the final rank R=N−1corresponds to the target space X.
The likelihood, as represented in Eq. 12, and the probability measure πare specifically defined within the
scope of the target space, denoted as SR=X. It’s important to note that these definitions apply exclusively
to the target space of trees and not to the broader sample space encompassing partial states, denoted as
Sr<R, which consists of forests containing disjoint trees. The Sum-Product algorithm is primarily utilized
to derive a maximum likelihood estimate for a tree. However, partial states are explicitly characterized as
collections of these disjoint trees or leaf nodes. To extend the target measure πto encompass the sample
spaceSr<R, a practical approach is to treat all elements of the jump chain as trees, as elaborated in Wang
et al. (2015).
Definition 2 (Natural Forest Extension) .The natural forest extension, denoted as Nfe, expands the target
measureπinto forests by forming a product over the trees contained within the forest:
π(s):=/productdisplay
(ti,Xi)πYi(xi)(ti). (21)
One notable advantage of the Nfeis its ability to transmit information from non-coalescing elements to the
local weight update.
Algorithm 2 Combinatorial Sequential Monte Carlo
Input: Y={Y1,···,YM}∈ΩNxM,θ
1:Initialization.∀k,sk
0←⊥,wk
0←1/K.
2:forr= 0toR=N−1do
3:fork= 1toKdo
4: Resample
P(ak
r−1=i) =wi
r−1/summationtextK
l=1wl
r−1
5: Extend partial state
sk
r∼q(·|sak
r−1
r−1)
6: Compute weights
wk
r=w(sak
r−1
r−1,sk
r) =π(sk
r)
π(sak
r−1
r−1)·ν−(sak
r−1
r−1)
q(skr|sak
r−1
r−1)
7:end for
8:end for
9:Output:s1:K
R,w1:K
1:R
17Published in Transactions on Machine Learning Research (12/2024)
C Nested Combinatorial Sequential Monte Carlo
We provide a review of the Ncsmcalgorithm from Moretti et al. (2021). The Ncsmcmethod performs a
standard Resample step (line 4), similar to Csmcmethods, iterating over rank events. In each iteration,
Ncsmcexplores all possible one-step ahead topologies (/parenleftbigN−r
2/parenrightbig
) and samples sub-branch lengths for each of
them (line 5-7). Importance sub-weights orpotential functions are evaluated for these sampled look-ahead
states (line 8). The ancestral partial state is then extended to a new partial state by choosing a topology
and branch length based on their respective weights ( line 11). The final weight for each sample is calculated
by averaging over the potential functions ( line 12). For a visual representation of this procedure, please refer
to Fig. 8.
Algorithm 3 Nested Combinatorial Sequential Monte Carlo
Input: Y={Y1,···,YN}∈ΩNxM,θ
1:Initialization.∀k,sk
0←⊥,wk
0←1/K.
2:forr= 1toR=N−1do
3:fork= 1toKdo
4: Resample P(ak
r−1=i) =wi
r−1/summationtextK
l=1wl
r−1
5:fori= 1toL=/parenleftbigN−r
2/parenrightbig
do
6:form= 1toMdo
7: Form look-ahead partial state
sk,m
r[i]∼q(·|sak
r−1
r−1)
8: Compute potentials
wk,m
r[i] =π(sk,m
r[i])
π(sak
r−1
r−1)·ν−(sak
r−1
r−1)
q(sk,m
r[i]|sak
r−1
r−1)
9:end for
10:end for
11: Extend partial state
sk
r=sk,J
r[I],
P(I=i,J=j) =wk,j
r[i]/summationtextL
l=1/summationtextM
m=1wk,m
r[i]
12: Compute weights
wk
r=1
MLL/summationdisplay
i=1M/summationdisplay
m=1wk,m
r[i]
13:end for
14:end for
Output:s1:K
R,w1:K
1:R
D Approximate Posteriors
The proposal distribution for our point estimator adaptation of Csmcand the corresponding approximate
posterior for Vcsmccorresponding to Eq. 5 can be written explicitly as follows:
Qϕ/parenleftbig
T1:K
1:R,a1:K
1:R−1/parenrightbig:=/parenleftiggK/productdisplay
k=1qϕ(Tk
1)/parenrightigg
·R/productdisplay
r=2K/productdisplay
k=1
wak
r−1
r−1/summationtextK
l=1wl
r−1·qϕ/parenleftbigg
Tk
r|Tak
r−1
r−1/parenrightbigg
. (22)
18Published in Transactions on Machine Learning Research (12/2024)
Enumerate Topologies
A
B
C
DSubsample Branch Lengths
A
B
C
D
A
B
C
D
B
A
C
DA
B
C
D
A
B
C
D
A
B
C
DCompute Potentials
A
B
C
D
A
B
C
D
A
B
C
D
Figure 8: Illustration of the Ncsmcframework: In Ncsmc, all possible one-step ahead topologies, which
amountto/parenleftbigN−r
2/parenrightbig
intotal, aresystematicallyenumerated. Forthestate A,B,C,D , theenumeratedtopologies
include ( top):A,B,C,D , (center):A,B,C,D , and (bottom):B,A,C,D . Subsequently, M= 1sub-branch
lengths are stochastically sampled for each edge. Following this, the sub-weights orpotentials are computed
(right), and a single candidate is randomly selected proportional to its sub-weight (or potential) to create
the new partial state.
In the above, ak
r−1denotes the ancestor index of the resampled random variable and the partial state sk
r=Tk
r
is sampled by proposing forest Tk
r∼qϕ(·|Tak
r−1
r−1)from a Uniform distribution. Similarly, the proposal
distribution for the global posterior defined in Eq. 14 is expressed where λk
r∼qψ(·|λak
r−1
r−1) = logN(·|˜µ,˜Σ):
Qϕ,ψ/parenleftbig
T1:K
1:R,Λ1:K
1:R,a1:K
1:R−1/parenrightbig:=/parenleftiggK/productdisplay
k=1qϕ(Tk
1)·qψ(λk
1)/parenrightigg
·R/productdisplay
r=2K/productdisplay
k=1
wak
r−1
r−1/summationtextK
l=1wl
r−1·qϕ/parenleftbigg
Tk
r|Tak
r−1
r−1/parenrightbigg
·qψ/parenleftbigg
λk
r|λak
r−1
r−1/parenrightbigg
.
(23)
TheNcsmcmethod detailed in Algorithm 3 can also be used to form an unbiased and consistent estimator
of the log-marginal likelihood /hatwideZNCSMCand a variational objective which we refer to as LNCSMC:
LNCSMC :=E
Q/bracketleftig
logˆZNCSMC/bracketrightig
,/hatwideZNCSMC :=R/productdisplay
r=1/parenleftigg
1
KK/summationdisplay
k=1wk
r/parenrightigg
. (24)
19Published in Transactions on Machine Learning Research (12/2024)
E Log Conditional Likelihood ˆP(X|τ,λ)
Figure 9: Log-conditional likelihood ˆP(X|τ,λ)values for Vcsmc(blue) with K={8,16,32,64,128,256}
samples and Vncsmc (red) withK={8,16,32,64,128,256}andM= 1samples averaged across 5 random
seeds. Greater values of Kresult in a more constrained ELBO and higher log-likelihood values while reducing
stochastic gradient noise. Vncsmc withK≥8explores higher probability spaces than the likelihood
returned by the simulator, as depicted by the green trace for reference. Vncsmc achieves convergence in
fewer epochs than Vcsmcand yields higher values, all while maintaining lower stochastic gradient noise.
Notably, even Vncsmc with (K,M ) = (8,1)in the top-left plot (red) outperforms Vcsmcwith K = 256 in
the bottom-right plot (blue).
F Implementation Details
F.1 Invalid Partial States when Coalescing Particles
Physics imposes several constraints on which pairs of particles are impossible to coalesce. We must consider
these constraints as we are building trees from leaf to root, coalescing particles (represented as nodes) at
every iteration. The following are the conditions
1.t>0for any node.
2.tp>tcutfor all inner nodes.
3.tp>max (tl,tr)for all inner nodes.
Recall that the ELBO is a function of the weight matrix, which is of dimensions (R,K ), and contains all
weights of the Kparticles across Riterations. Each entry in the matrix represents the corresponding weight
of some partial state.
InVcsmcandVncsmc , resampling ensures that we not only extend upon partial states of valid non-zero
probability, but we also arrive at Kvalid trees at the final rank event. We note that both Greedy Search
and Beam Search often fail to find any valid trees because they reach a set of partial states where no viable
tree can be constructed.
20