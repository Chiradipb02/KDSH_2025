Interventional Causal Discovery in a Mixture of DAGs
Burak Varıcı∗
Carnegie Mellon UniversityDmitriy A. Katz
IBM ResearchDennis Wei
IBM Research
Prasanna Sattigeri
IBM ResearchAli Tajer
Rensselaer Polytechnic Institute
Abstract
Causal interactions among a group of variables are often modeled by a single
causal graph. In some domains, however, these interactions are best described
by multiple co-existing causal graphs, e.g., in dynamical systems or genomics.
This paper addresses the hitherto unknown role of interventions in learning causal
interactions among variables governed by a mixture of causal systems, each
modeled by one directed acyclic graph (DAG). Causal discovery from mixtures
is fundamentally more challenging than single-DAG causal discovery. Two
major difficulties stem from (i) an inherent uncertainty about the skeletons of the
component DAGs that constitute the mixture and (ii) possibly cyclic relationships
across these component DAGs. This paper addresses these challenges and aims to
identify edges that exist in at least one component DAG of the mixture, referred to
as the trueedges. First, it establishes matching necessary and sufficient conditions
on the size of interventions required to identify the true edges. Next, guided by
the necessity results, an adaptive algorithm is designed that learns all true edges
usingO(n2)interventions, where nis the number of nodes. Remarkably, the size
of the interventions is optimal if the underlying mixture model does not contain
cycles across its components. More generally, the gap between the intervention
size used by the algorithm and the optimal size is quantified. It is shown to be
bounded by the cyclic complexity number of the mixture model, defined as the
size of the minimal intervention that can break the cycles in the mixture, which
is upper bounded by the number of cycles among the ancestors of a node.
1 Introduction
The causal interactions in a system of causally related variables are often abstracted by a directed
acyclic graph (DAG). This is the common practice in various disciplines, including biology [ 1], social
sciences [ 2], and economics [ 3]. In a wide range of applications, however, the complexities of the ob-
served data cannot be reduced to conform to a single DAG, and they are best described by a mixture of
multiple co-existing DAGs over the same set of variables. For instance, gene expression of certain can-
cer types comprises multiple subtypes with different causal relationships [ 4]. In another example, mix-
ture models are often more accurate than unimodal distributions in representing dynamical systems [ 5],
including time-series trajectories in psychology [ 6] and data from complex robotics environments [ 7].
Despite the widespread applications, causal discovery for a mixture of DAGs remains an under-
investigated domain. Furthermore, the existing studies on the subject are also limited to using
only observational data [ 8–11]. Observational data alone is highly insufficient in uncovering causal
relationships. It is well-established that even for learning a single DAG, observational data can learn
a DAG only up to its Markov equivalence class (MEC) [ 12]. Hence, interventions , which refer to
∗Work was done when BV was a Ph.D. student at Rensselaer Polytechnic Institute.
38th Conference on Neural Information Processing Systems (NeurIPS 2024).altering the causal mechanisms of a set of target nodes, have a potentially significant role in improving
identifiability guarantees in mixture DAG models. Specifically, interventional data can be used to
learn specific cause-effect relationships and refine the equivalence classes.
Using interventions for learning a single DAG is well-investigated for various causal models and
interventions [ 13–17]. In this paper, we investigate using interventions for causal discovery in a
mixture of DAGs, a fundamentally more challenging problem. The major difficulties stem from
(i) an inherent uncertainty about the skeletons of the DAGs that constitute the mixture and (ii)
possibly cyclic relationships across these DAGs. For a single DAG, the skeleton can be learned from
observational data via conditional independence (CI) tests and the role of interventions is limited
to orienting the edges. On the contrary, in a mixture of DAGs, the skeleton cannot be learned from
observational data alone, making interventions essential for both learning the skeleton and orienting
the edges. Uncertainty in the skeleton arises because, in addition to true edges present in at least
one individual DAG, there are inseparable random variable pairs that cannot be made conditionally
independent via CI tests, even though they are nonadjacent in every DAG of the mixture. These types
of inseparable node pairs, referred to as emergent edges [11], cannot be distinguished from true edges
using observational data alone.
In this paper, we aim to characterize the fundamental limits of interventions needed for learning
the true edges in a mixture of DAGs. The two main aspects of these limits are the minimum size
andnumber of the interventions. To this end, we first investigate the necessary and sufficient size
of interventions for identifying a true edge. Subsequently, we design an adaptive algorithm that
learns the true edges using interventions guided by the necessary and sufficient intervention sizes.
We quantify the optimality gap of the maximum intervention size used by the algorithm as a function
of the structure of the cyclic relationships across the mixture model. We note that the component
DAGs of the mixture cannot be identified without further assumptions even when using interventions
(see examples in Appendix D.1). Hence, our focus is on learning the set of true edges in the mixture
as specified above. Our contributions are summarized as follows.
•Intervention size: We establish matching necessary and sufficient intervention size to identify
each node’s mixture parents (i.e., the union of its parents across all DAGs). Specifically, we
show that this size is one more than the number of mixture parents of the said node.
•Tree DAGs: For the special case of a mixture of directed trees, we show that the necessary and
sufficient intervention size is one more than the number of DAGs in the mixture.
•Algorithm: We design an adaptive algorithm that identifies all directed edges of the individual
DAGs in the mixture by using O(n2)interventions, where nis the number of variables. Re-
markably, the maximum size of the interventions used in our algorithm is optimal if the mixture
ancestors of a node (i.e., the union of its ancestors across all DAGs) do not form a cycle.
•Optimality gap: We show that the gap between the maximum intervention size used by the
proposed algorithm for a given node and the optimal size is bounded by the cyclic complexity
number of the node, which is defined as the number of nodes needing intervention to break
cycles among the ancestors of the node, and is upper bounded by the number of such cycles.
We provide an overview of the closely related literature, the majority of which is focused on the
causal discovery of single DAGs.
Causal discovery of a mixture of DAGs. The relevant literature on the causal discovery of a mixture
of DAGs focuses on developing graphical models to represent CI relationships in the observed
mixture distribution [ 8–11]. Among them, [ 8] proposes a fused graph and shows that the mixture
distribution is Markov with respect to it. The study in [ 9] proposes a similar mixture graph but relies
on longitudinal data to orient any edges. The study in [ 10] constructs a mixture DAG that represents
the mixture distribution and designed an algorithm for learning a maximal ancestral graph. The
algorithm of [ 10] requires the component DAGs of the mixture to be poset compatible, which rules
out any cyclic relationships across the DAGs. The study in [ 11] introduces the notion of emergent
edges to investigate the inseparability conditions arising in the mixture of DAGs. The study in [ 18]
proposes a variational inference-based approach for causal discovery from a mixture of time-series
data. Despite their differences, all these studies are limited to using observational data.
Intervention design for causal discovery of a single DAG. We note that the structure of a single
DAG without latent variables can be learned using single-node interventions. Hence, the majority of
the literature focuses on minimizing the number of interventions. Worst-case bounds on the number
2of interventions with unconstrained size are established in [ 13], and heuristic adaptive algorithms are
proposed in [ 14]. Intervention design on causal graphs with latent variables is studied in [ 19–21]. The
study in [ 20] also shows that single-node interventions are not sufficient for exact graph recovery in
the presence of latent variables. In another direction, [ 16] studies interventions under size constraints,
establishes a lower bound for the number of interventions, and shows that O(n
klog log k)randomized
interventions with size ksuffice for identifying the DAG with high probability. In the case of
single-node interventions, adaptive and non-adaptive algorithms are proposed in [ 22], active learning
of directed trees is studied in [ 23], and a universal lower bound for the number of interventions is
established in [ 17]. [24] also studies the universal lower bound problem and [ 25] provides an exact
characterization for the number of interventions required to recover the DAG from the observational
essential graph. A linear cost model, where the cost of an intervention is proportional to its size, is
proposed in [ 26]. It is shown that learning the DAG with optimal cost under the linear cost model
is NP-hard [ 27]. The size of the minimal intervention sets is studied for cyclic directed models
in [28]. Specifically, it is shown that the required intervention size is at least ζ−1where ζdenotes
the size of the largest strongly connected component in the cyclic model. A related problem to
intervention design is causal discovery from a combination of observational and interventional data.
In this setting, the characterization of the equivalence classes and designing algorithms for learning
them is well-explored for a single DAG [29–32].
Causal discovery from multiple clusters/contexts. Another approach to causal discovery from
a mixture of DAGs is clustering the observed samples and performing structure learning on each
cluster separately [ 33–37]. Learning from multiple contexts is also studied in the interventional causal
discovery literature [ 38–41]. However, these studies assume that domain indexes are known. In a
similar problem, [ 42] aims to learn the domain indexes and perform causal discovery simultaneously.
2 Preliminaries and definitions
2.1 Observational mixture model
DAG models. We consider K≥2distinct DAGs Gℓ≜(V,Eℓ)forℓ∈ {1, . . . , K }defined over
the same set of nodes V≜{1, . . . , n }.Eℓdenotes the set of directed edges in graph Gℓ. Throughout
the paper, we refer to these as the mixture component DAGs. We use paℓ(i),chℓ(i),anℓ(i), and
deℓ(i)to refer to the parents, children, ancestors, and descendants of node iin DAG Gℓ, respectively.
We also use iℓ⇝jto denote i∈anm(j). For each node i∈V, we define pam(i)as the union of the
nodes that are parents of iin at least one component DAG and refer to pam(i)as the mixture parents
of node i. Similarly, for each node i∈Vwe define chm(i),anm(i), and dem(i).
Mixture model. Each of the component DAGs represents a Bayesian network. We denote the
random variable generated by node i∈VbyXiand define the random vector X≜(X1, . . . , X n)⊤.
For any subset of nodes A⊆V, we use XAto denote the vector formed by Xifori∈A. We denote
the probability density function (pdf) of XunderGℓbypℓ, which factorizes according to Gℓas
pℓ(x) =Y
i∈[n]pℓ(xi|xpaℓ(i)),∀ℓ∈[K]. (1)
For distinct ℓ, ℓ′∈[K],pℓandpℓ′can be distinct even when Eℓ=Eℓ′. The differences between any
two DAGs are captured by the nodes with distinct causal mechanisms (i.e., conditional distributions)
in the DAGs. To formalize such distinctions, we define the following set, which contains all the nodes
with at least two different conditional distributions across component distributions.
∆≜
i∈V:∃ℓ, ℓ′∈[K] :pℓ(Xi|Xpaℓ(i))̸=pℓ′(Xi|Xpaℓ′(i))	
. (2)
We adopt the same mixture model as the prior work on causal discovery of mixture of DAGs [ 8–
11,18]. Specifically, observed data is generated by a mixture of distributions {pℓ:ℓ∈[K]}. It
is unknown to the learner which model is generating the observations X. To formalize this, we
define L∈ {1, . . . , K }as a latent random variable where L=ℓspecifies that the true model is pℓ.
We denote the probability mass function (pmf) of Lbyr. Hence, we have the following mixture
distribution for the observed samples X.
pm(x)≜X
ℓ∈[K]r(ℓ)·pℓ(x). (3)
3Next, we provide several definitions that are instrumental to formalizing causal discovery objectives.
Definition 1 (True edge) .We say that j→iis atrueedge if j∈pam(i). The set of all true edges is
denoted by
Et≜{(j→i) :i, j∈V,∃ Gℓ:j∈paℓ(i)}. (4)
A common approach to causal discovery is the class of constraint-based approaches, which perform
conditional independence (CI) tests on the observed data to infer (partial) knowledge about the
DAGs’ structure [ 43–45]. In this paper, we adopt a constraint-based CI testing approach. Following
this approach, the following definition formally specifies the set of node pairs that cannot be made
conditionally independent in the mixture distribution.
Definition 2 (Inseparable pair) .The node pair (i, j)is called inseparable ifXiandXjare always sta-
tistically dependent in the mixture distribution pmunder any conditioning set. The set of inseparable
node pairs is specified by
Ei≜{(i−j) :i, j∈V,∄A⊆V\ {i, j}:Xi⊥ ⊥Xj|XAinpm}. (5)
Note that when (j→i)is a true edge, the pair (i, j)will be inseparable. A significant difference
between independence tests for mixture models and single-DAG models is that not all inseparable
pairs have an associated true edge in the former. More specifically, due to the mixing of multiple
distributions, a pair of nodes can be nonadjacent in all component DAGs but still be inseparable in
mixture distribution pm. We refer to such inseparable node pairs as emergent pairs , formalized next.
Definition 3 (Emergent pair) .An inseparable pair (i, j)∈Eiis called an emergent pair if there is
no true edge associated with the pair. The set of emergent pairs is denoted by
Ee≜{(i, j)∈Ei:i /∈pam(j)∧j /∈pam(i)}. (6)
The conditions under which emergent edges arise in mixture models are recently investigated in [ 11],
where it is shown that the causal paths that pass through a node in the set ∆defined in (2)are
instrumental for their analysis. These paths are specified next.
Definition 4 (∆-through path) .We say that a causal path in Gℓbetween iandjis a∆-through
path if it passes through at least one node in ∆, i.e., there exists u∈∆such that iℓ⇝uℓ⇝j. If
u∈chℓ(i), the path is also called a ∆-child-through path.
2.2 Intervention model
In this section, we describe the intervention model we use for causal discovery on a mixture of
DAGs. We consider stochastic hard interventions on component DAGs of the mixture model. A hard
intervention on a set of nodes I ⊆Vcuts off the edges incident on nodes i∈ Iin all component
DAGs Gℓforℓ∈[K]. We denote the post-intervention component DAGs upon an intervention Iby
{Gℓ,I:ℓ∈[K]}. We note that hard interventions are less restrictive than dointerventions, which
not only remove ancestral dependencies but also remove randomness by assigning constant values
to the intervened nodes. Specifically, in Gℓ,I, the causal mechanism of an intervened node i∈ I
changes from pℓ(xi|xpaℓ(i))toqi(xi). Therefore, upon an intervention I ⊆V, the interventional
component DAG distributions are given by
pℓ,I(x)≜Y
i∈Iqi(xi)Y
i∈V\Ipℓ(xi|xpaℓ(i)),∀ℓ∈[K]. (7)
Subsequently, the interventional mixture distribution pm,I(x)is given by
pm,I(x)≜X
ℓ∈[K]r(ℓ)·pℓ,I(x). (8)
We note that an intervened node i∈ Ihas the same causal mechanism qi(xi)for all interventions
I ⊆Vthat contain i. This is because an intervention procedure targets a set of nodes in all mixture
components at the same time. Hence, resulting qi(Xi)is shared for all component models, owing
to the same intervention mechanism, e.g., gene knockout experiments [ 46]. Hence, the set of nodes
with distinct causal mechanisms across the components of the interventional mixture model becomes
∆I≜∆\ I. Next, we specify the I-mixture DAG, which extends the mixture DAG defined for
observational data in [10, 11] and will facilitate our analysis.
4<latexit sha1_base64="KNioJ/sWib5kzWFrKmvyqxdqVkQ=">AAACuXicjVFRSxtBEN6crdpoNbGPfVkaBAUJdyKt4EvEgj6IGGiikASZ20ySNXt7x+5cMBz5Bb62f8B/5b9xLwmlSfrgwMLH9818M7MTJkpa8v3Xgrf24eP6xuan4tb2553dUnmvaePUCGyIWMXmPgSLSmpskCSF94lBiEKFd+HwItfvRmisjPUvGifYiaCvZU8KIEfVg4dSxa/60+CrIJiDCpvH7UO58NLuxiKNUJNQYG0r8BPqZGBICoWTYju1mIAYQh9bDmqI0Hay6aQTvu+YLu/Fxj1NfMr+W5FBZO04Cl1mBDSwy1pO/k9rpdQ77WRSJymhFrNGvVRxinm+Nu9Kg4LU2AEQRrpZuRiAAUHuc4oLbabmCYqFVbKnVEsRd3GJVfREBibF/bZFikDqfK/sCtUIXQ/gN5jiX9X55vLBT9mXZI+u3Q300aVBHB6ulCz4nRsJ6l02s0x31WD5hqugeVwNvldP6ieV2vH8vpvsK/vGDljAfrAau2K3rMEEQ/bMfrM/3pkH3sB7nKV6hXnNF7YQnn0DCI7bfQ==</latexit>1<latexit sha1_base64="pecWCJildrHVdhbAMoSawdIoZ5A=">AAACuXicjVFNSyNBEO2MuqtZXb+OXhqDoCBhJgRd8KIo6EFEwaiQBKnpVJI2PT1Dd40YhvwCr/oH/Ff7b7YnCWISD1vQ8Hiv6lVVV5goacn3/xa8ufmFHz8Xl4q/lld+r66tb9zZODUCayJWsXkIwaKSGmskSeFDYhCiUOF92DvN9ftnNFbG+pb6CTYj6GjZlgLIUTeVx7WSX/aHwWdBMAYlNo7rx/XCR6MVizRCTUKBtfXAT6iZgSEpFA6KjdRiAqIHHaw7qCFC28yGkw74jmNavB0b9zTxIfu1IoPI2n4UuswIqGuntZz8Tqun1P7TzKROUkItRo3aqeIU83xt3pIGBam+AyCMdLNy0QUDgtznFCfaDM0TFBOrZC+pliJu4RSr6IUMDIo7DYsUgdT5XtkFqmd0PYBfYYqfqvPN5d0z2ZFk9y/dDfT+uUHs7c2UTPidGAnqv2xGme6qwfQNZ8FdpRwclKs31dJxZXzfRbbFttkuC9ghO2YX7JrVmGDIXtkbe/eOPPC63tMo1SuMazbZRHj2Hwrj234=</latexit>2
<latexit sha1_base64="KkNs6T3iXH/7Jwuf1K7bPD75nK4=">AAACuXicjVHfSxtBEN5cbWvTH2p99GUxBFKQcKfSFvpiaUEfRBRMFJIQ5jaTZM3e3rE7J4Yjf0Ff7T/gf+V/073LISbxoQMLH983883MTpgoacn3Hyveq7XXb96uv6u+//Dx08bm1ue2jVMjsCViFZvrECwqqbFFkhReJwYhChVehZNfuX51i8bKWF/SNMFeBCMth1IAOerioL9Z85t+EXwVBCWosTLO+1uVh+4gFmmEmoQCazuBn1AvA0NSKJxVu6nFBMQERthxUEOEtpcVk8543TEDPoyNe5p4wT6vyCCydhqFLjMCGttlLSdf0jopDb/3MqmTlFCLeaNhqjjFPF+bD6RBQWrqAAgj3axcjMGAIPc51YU2hXmCYmGV7C7VUsQDXGIV3ZGBWbXetUgRSJ3vlZ2gukXXA/gZpvikOt9cbvyWI0l279TdQO8dG8TJl5WSBb+fRoL6L5t5prtqsHzDVdDebwZfm4cXh7Wj/fK+62yH7bIGC9g3dsRO2DlrMcGQ/WH37K/3wwNv7N3MU71KWbPNFsKz/wANONt/</latexit>3<latexit sha1_base64="6nQzhJC9Npz6VYjvX2LtnvEx2Vs=">AAACtXicjVHbSiNBEO2M1x3vvu5LYxAUJMxIcAVfFAV9WETBqJAEqelUkiY9PbPdNWIY8gW+6g/sX+3fbE8SxCQ+WNBwOKfqVFVXlCppKQj+lby5+YXFpeUf/srq2vrGpr91b5PMCKyJRCXmMQKLSmqskSSFj6lBiCOFD1HvvNAfntFYmeg76qfYjKGjZVsKIEfdVp82y0ElGAafBeEYlNk4bp62Sn8brURkMWoSCqyth0FKzRwMSaFw4DcyiymIHnSw7qCGGG0zH0464LuOafF2YtzTxIfs54ocYmv7ceQyY6CundYK8iutnlH7uJlLnWaEWowatTPFKeHF2rwlDQpSfQdAGOlm5aILBgS5z/En2gzNUxQTq+QvmZYiaeEUq+iFDAz83YZFikHqYq/8CtUzuh7ArzHDD9X5FvLehexIsge/3Q30waVB7O3PlEz4nRkJ6ls2o0x31XD6hrPg/rASHlWq5dPD8XWX2U+2w/ZYyH6xU3bFbliNCYbslb2xd+/EA687SvRK44ptNhHen/9mutqE</latexit>4(a)G1
<latexit sha1_base64="KNioJ/sWib5kzWFrKmvyqxdqVkQ=">AAACuXicjVFRSxtBEN6crdpoNbGPfVkaBAUJdyKt4EvEgj6IGGiikASZ20ySNXt7x+5cMBz5Bb62f8B/5b9xLwmlSfrgwMLH9818M7MTJkpa8v3Xgrf24eP6xuan4tb2553dUnmvaePUCGyIWMXmPgSLSmpskCSF94lBiEKFd+HwItfvRmisjPUvGifYiaCvZU8KIEfVg4dSxa/60+CrIJiDCpvH7UO58NLuxiKNUJNQYG0r8BPqZGBICoWTYju1mIAYQh9bDmqI0Hay6aQTvu+YLu/Fxj1NfMr+W5FBZO04Cl1mBDSwy1pO/k9rpdQ77WRSJymhFrNGvVRxinm+Nu9Kg4LU2AEQRrpZuRiAAUHuc4oLbabmCYqFVbKnVEsRd3GJVfREBibF/bZFikDqfK/sCtUIXQ/gN5jiX9X55vLBT9mXZI+u3Q300aVBHB6ulCz4nRsJ6l02s0x31WD5hqugeVwNvldP6ieV2vH8vpvsK/vGDljAfrAau2K3rMEEQ/bMfrM/3pkH3sB7nKV6hXnNF7YQnn0DCI7bfQ==</latexit>1<latexit sha1_base64="pecWCJildrHVdhbAMoSawdIoZ5A=">AAACuXicjVFNSyNBEO2MuqtZXb+OXhqDoCBhJgRd8KIo6EFEwaiQBKnpVJI2PT1Dd40YhvwCr/oH/Ff7b7YnCWISD1vQ8Hiv6lVVV5goacn3/xa8ufmFHz8Xl4q/lld+r66tb9zZODUCayJWsXkIwaKSGmskSeFDYhCiUOF92DvN9ftnNFbG+pb6CTYj6GjZlgLIUTeVx7WSX/aHwWdBMAYlNo7rx/XCR6MVizRCTUKBtfXAT6iZgSEpFA6KjdRiAqIHHaw7qCFC28yGkw74jmNavB0b9zTxIfu1IoPI2n4UuswIqGuntZz8Tqun1P7TzKROUkItRo3aqeIU83xt3pIGBam+AyCMdLNy0QUDgtznFCfaDM0TFBOrZC+pliJu4RSr6IUMDIo7DYsUgdT5XtkFqmd0PYBfYYqfqvPN5d0z2ZFk9y/dDfT+uUHs7c2UTPidGAnqv2xGme6qwfQNZ8FdpRwclKs31dJxZXzfRbbFttkuC9ghO2YX7JrVmGDIXtkbe/eOPPC63tMo1SuMazbZRHj2Hwrj234=</latexit>2
<latexit sha1_base64="KkNs6T3iXH/7Jwuf1K7bPD75nK4=">AAACuXicjVHfSxtBEN5cbWvTH2p99GUxBFKQcKfSFvpiaUEfRBRMFJIQ5jaTZM3e3rE7J4Yjf0Ff7T/gf+V/073LISbxoQMLH983883MTpgoacn3Hyveq7XXb96uv6u+//Dx08bm1ue2jVMjsCViFZvrECwqqbFFkhReJwYhChVehZNfuX51i8bKWF/SNMFeBCMth1IAOerioL9Z85t+EXwVBCWosTLO+1uVh+4gFmmEmoQCazuBn1AvA0NSKJxVu6nFBMQERthxUEOEtpcVk8543TEDPoyNe5p4wT6vyCCydhqFLjMCGttlLSdf0jopDb/3MqmTlFCLeaNhqjjFPF+bD6RBQWrqAAgj3axcjMGAIPc51YU2hXmCYmGV7C7VUsQDXGIV3ZGBWbXetUgRSJ3vlZ2gukXXA/gZpvikOt9cbvyWI0l279TdQO8dG8TJl5WSBb+fRoL6L5t5prtqsHzDVdDebwZfm4cXh7Wj/fK+62yH7bIGC9g3dsRO2DlrMcGQ/WH37K/3wwNv7N3MU71KWbPNFsKz/wANONt/</latexit>3<latexit sha1_base64="6nQzhJC9Npz6VYjvX2LtnvEx2Vs=">AAACtXicjVHbSiNBEO2M1x3vvu5LYxAUJMxIcAVfFAV9WETBqJAEqelUkiY9PbPdNWIY8gW+6g/sX+3fbE8SxCQ+WNBwOKfqVFVXlCppKQj+lby5+YXFpeUf/srq2vrGpr91b5PMCKyJRCXmMQKLSmqskSSFj6lBiCOFD1HvvNAfntFYmeg76qfYjKGjZVsKIEfdVp82y0ElGAafBeEYlNk4bp62Sn8brURkMWoSCqyth0FKzRwMSaFw4DcyiymIHnSw7qCGGG0zH0464LuOafF2YtzTxIfs54ocYmv7ceQyY6CundYK8iutnlH7uJlLnWaEWowatTPFKeHF2rwlDQpSfQdAGOlm5aILBgS5z/En2gzNUxQTq+QvmZYiaeEUq+iFDAz83YZFikHqYq/8CtUzuh7ArzHDD9X5FvLehexIsge/3Q30waVB7O3PlEz4nRkJ6ls2o0x31XD6hrPg/rASHlWq5dPD8XWX2U+2w/ZYyH6xU3bFbliNCYbslb2xd+/EA687SvRK44ptNhHen/9mutqE</latexit>4 (b)G2
<latexit sha1_base64="7D2hCvOUx17eEaAEgY4/0uiKRE8=">AAACuXicjVHbSiNBEO3Mes2ut/XRl8YgZCGEGREVfFFWWB+WRcFoIAmhplNJ2vT0DN01YhjyBfuqP7B/5d/YkwQxiQ9b0HA4p+pUVVeYKGnJ918L3pel5ZXVtfXi128bm1vbO9/vbJwagTURq9jUQ7CopMYaSVJYTwxCFCq8Dwc/c/3+EY2Vsb6lYYKtCHpadqUActTNsL1d8qv+OPgiCKagxKZx3d4p/Gt2YpFGqEkosLYR+Am1MjAkhcJRsZlaTEAMoIcNBzVEaFvZeNIRP3BMh3dj454mPmY/VmQQWTuMQpcZAfXtvJaTn2mNlLqnrUzqJCXUYtKomypOMc/X5h1pUJAaOgDCSDcrF30wIMh9TnGmzdg8QTGzSvaUainiDs6xip7IwKh40LRIEUid75VdoXpE1wP4H0zxXXW+uVy+lD1JtvLb3UBXfhnEwY+Fkhm/CyNB/ZfNJNNdNZi/4SK4O6wGx9Wjm6PS+eH0vmtsj+2zMgvYCTtnV+ya1ZhgyP6yZ/binXng9b2HSapXmNbsspnw7BuwdtvF</latexit>y
<latexit sha1_base64="ITZaJfsMf6+k09wCgHRV6O7CMyc=">AAACu3icjVHBSiNBEO3MuupG19X16KUxCAoSZkR2vQiKwnoQUTQqJCHUdCqxNz09Q3e1GIZ8gtfd+/6Vf7M9SRCTeLCg4fFe1auqrjhT0lIYvpSCT3Of5xcWv5SXlr+ufFtd+35rU2cE1kSqUnMfg0UlNdZIksL7zCAkscK7uHdS6HePaKxM9Q31M2wm0NWyIwWQp66jVtRarYTVcBh8FkRjUGHjuGytlf412qlwCWoSCqytR2FGzRwMSaFwUG44ixmIHnSx7qGGBG0zH8464FueafNOavzTxIfs24ocEmv7SewzE6AHO60V5Hta3VHnoJlLnTlCLUaNOk5xSnmxOG9Lg4JU3wMQRvpZuXgAA4L895Qn2gzNMxQTq+RPTkuRtnGKVfREBgblrYZFSkDqYq/8DNUj+h7AL9Dhq+p9C3n7VHYl2d1zfwW9+8sg9nZmSib8jo0E9SGbUaa/ajR9w1lwu1eNflT3r/YrR3vj+y6yDbbJtlnEfrIjdsYuWY0J1mXP7A/7GxwGIvgdqFFqUBrXrLOJCNx/pJrcIQ==</latexit>11
<latexit sha1_base64="9UEkfh74UxSxmISi3/7TqE82C/o=">AAACu3icjVHfSxtBEN6c2mrU1h+PviwGQUHCXZC2LwVLhfogomhUSEKY20ziNnt7x+6sGI78CX1t3/tf+d+4lwQxiQ8OLHx838w3MztxpqSlMHwqBQuLSx8+Lq+UV9fWP33e2Ny6sakzAusiVam5i8GikhrrJEnhXWYQkljhbdz/Wei3D2isTPU1DTJsJdDTsisFkKeuau2ovVEJq+Eo+DyIJqDCJnHR3iz9b3ZS4RLUJBRY24jCjFo5GJJC4bDcdBYzEH3oYcNDDQnaVj6adcj3PNPh3dT4p4mP2NcVOSTWDpLYZyZA93ZWK8i3tIaj7rdWLnXmCLUYN+o6xSnlxeK8Iw0KUgMPQBjpZ+XiHgwI8t9TnmozMs9QTK2SPzotRdrBGVbRIxkYlveaFikBqYu98lNUD+h7AD9Hhy+q9y3k/RPZk2QPz/wV9OEvg9g/mCuZ8vthJKh32Ywz/VWj2RvOg5taNfpSPbo8qhzXJvddZjtsl+2ziH1lx+yUXbA6E6zH/rC/7F/wPRDB70CNU4PSpGabTUXgngGm8dwi</latexit>21
<latexit sha1_base64="3Tj8+misl9G86r3oAJMH8aiN8iw=">AAACu3icjVHfSxtBEN6cttpY66/HviwNgoKEOxX1RVAs6EMpio0KSQhzm0lcs7d37M6K4cif0Nf23f/K/8a9JBST+NCBhY/vm/lmZifOlLQUhi+lYG7+w8eFxU/lpc/LX1ZW19ZvbOqMwJpIVWruYrCopMYaSVJ4lxmEJFZ4G/fOCv32EY2Vqf5F/QybCXS17EgB5KnrvVbUWq2E1XAYfBZEY1Bh47hsrZWeG+1UuAQ1CQXW1qMwo2YOhqRQOCg3nMUMRA+6WPdQQ4K2mQ9nHfBNz7R5JzX+aeJD9m1FDom1/ST2mQnQvZ3WCvI9re6oc9TMpc4coRajRh2nOKW8WJy3pUFBqu8BCCP9rFzcgwFB/nvKE22G5hmKiVXyJ6elSNs4xSp6IgOD8mbDIiUgdbFXfoHqEX0P4D/R4T/V+xby1nfZlWR3fvgr6J1zg9jbnimZ8Ds1EtR/2Ywy/VWj6RvOgpvdanRQ3b/ar5zsju+7yL6yb2yLReyQnbALdslqTLAu+83+sL/BcSCCh0CNUoPSuGaDTUTgXgGpSNwj</latexit>31
<latexit sha1_base64="1U3l/sViK9U/ZhDfKHen/1m8ZNE=">AAACu3icjVHfSxtBEN6c2mrU1h+PviwGQUHCnYS2LwVLhfogomhUSEKY20ziNnt7x+6sGI78CX1t3/tf+d+4lwQxiQ8OLHx838w3MztxpqSlMHwqBQuLSx8+Lq+UV9fWP33e2Ny6sakzAusiVam5i8GikhrrJEnhXWYQkljhbdz/Wei3D2isTPU1DTJsJdDTsisFkKeuau2ovVEJq+Eo+DyIJqDCJnHR3iz9b3ZS4RLUJBRY24jCjFo5GJJC4bDcdBYzEH3oYcNDDQnaVj6adcj3PNPh3dT4p4mP2NcVOSTWDpLYZyZA93ZWK8i3tIaj7rdWLnXmCLUYN+o6xSnlxeK8Iw0KUgMPQBjpZ+XiHgwI8t9TnmozMs9QTK2SPzotRdrBGVbRIxkYlveaFikBqYu98lNUD+h7AD9Hhy+q9y3k/RPZk2QPz/wV9OEvg9g/mCuZ8vthJKh32Ywz/VWj2RvOg5ujavSlWrusVY6PJvddZjtsl+2ziH1lx+yUXbA6E6zH/rC/7F/wPRDB70CNU4PSpGabTUXgngGrn9wk</latexit>41<latexit sha1_base64="95DzUCzk+BYmQtD0I4KZYqSsrYA=">AAACu3icjVFNaxsxEJU3zUfd5rPHXkRNIIVgdo1Jcim4tJAcSklpnARsY2blsa1Yq12kkYlZ/BN6be/5V/031dqm1HYOHRA83pt5M6OJMyUtheHvUrDxYnNre+dl+dXr3b39g8OjW5s6I7ApUpWa+xgsKqmxSZIU3mcGIYkV3sWjT4V+N0ZjZapvaJJhJ4GBln0pgDz1vd6tdQ8qYTWcBV8H0QJU2CKuu4elp3YvFS5BTUKBta0ozKiTgyEpFE7LbWcxAzGCAbY81JCg7eSzWaf82DM93k+Nf5r4jP23IofE2kkS+8wEaGhXtYJ8Tms56l90cqkzR6jFvFHfKU4pLxbnPWlQkJp4AMJIPysXQzAgyH9PeanNzDxDsbRK/ui0FGkPV1hFj2RgWj5uW6QEpC72yq9QjdH3AP4VHf5VvW8hn3yWA0n29Iu/gj69NIij92slS34fjQT1XzbzTH/VaPWG6+C2Vo3OqvVv9UqjtrjvDnvL3rETFrFz1mBX7Jo1mWAD9oP9ZL+CD4EIHgI1Tw1Ki5o3bCkC9wet9Nwl</latexit>42
<latexit sha1_base64="zvv+ufuJrFFV3IByUZRYJLHZgD0=">AAACu3icjVFRSxtBEN6c1tq0Wq2PfVkMggUJd1GqLwVFQR9KsdSokIQwt5nEbfb2jt1ZMRz5Cb7qu/+q/6Z7SRCT+ODAwsf3zXwzsxNnSloKw3+lYGHx3dL75Q/lj59WVj+vrX+5tKkzAusiVam5jsGikhrrJEnhdWYQkljhVdw/LvSrWzRWpvqCBhm2Euhp2ZUCyFN/dtu19lolrIaj4PMgmoAKm8R5e7301OykwiWoSSiwthGFGbVyMCSFwmG56SxmIPrQw4aHGhK0rXw065BveabDu6nxTxMfsS8rckisHSSxz0yAbuysVpCvaQ1H3YNWLnXmCLUYN+o6xSnlxeK8Iw0KUgMPQBjpZ+XiBgwI8t9TnmozMs9QTK2S3zktRdrBGVbRHRkYlreaFikBqYu98jNUt+h7AP+FDp9V71vI2yeyJ8nu/PRX0DunBrH/ba5kyu/ISFBvshln+qtGszecB5e1avS9uvd7r3JYm9x3mX1lm2ybRWyfHbIzds7qTLAeu2cP7DH4EYjgb6DGqUFpUrPBpiJw/wGrndwk</latexit>32
<latexit sha1_base64="RvKa28OGvIz8tsEPhXDIh/UxzRQ=">AAACu3icjVHfSxtBEN6c1mpq/fnoy2IQFCTcBWn7UrBUqA8iikaFJIS5zSSu2ds7dmfFcORP6Gv73v/K/8a9JIhJfOjAwsf3zXwzsxNnSloKw+dSsLD4Yenj8kr50+rntfWNza0bmzojsC5SlZq7GCwqqbFOkhTeZQYhiRXexv2fhX77iMbKVF/TIMNWAj0tu1IAeeqq1q61NyphNRwFnwfRBFTYJC7am6V/zU4qXIKahAJrG1GYUSsHQ1IoHJabzmIGog89bHioIUHbykezDvmeZzq8mxr/NPER+7Yih8TaQRL7zATo3s5qBfme1nDU/dbKpc4coRbjRl2nOKW8WJx3pEFBauABCCP9rFzcgwFB/nvKU21G5hmKqVXyJ6elSDs4wyp6IgPD8l7TIiUgdbFXforqEX0P4Ofo8FX1voW8fyJ7kuzhmb+CPvxlEPsHcyVTfj+MBPVfNuNMf9Vo9obz4KZWjb5Ujy6PKse1yX2X2Q7bZfssYl/ZMTtlF6zOBOux3+wP+xt8D0TwEKhxalCa1GyzqQjcC6lG3CM=</latexit>22
<latexit sha1_base64="GP2ihqxmBtt+TDUW2M5JA8bANYI=">AAACu3icjVHfSxtBEN6c2mrU1h+PviwGQUHCXZC2LwVLhfogomhUSEKY20ziNnt7x+6sGI78CX1t3/tf+d+4lwQxiQ8OLHx838w3MztxpqSlMHwqBQuLSx8+Lq+UV9fWP33e2Ny6sakzAusiVam5i8GikhrrJEnhXWYQkljhbdz/Wei3D2isTPU1DTJsJdDTsisFkKeuonatvVEJq+Eo+DyIJqDCJnHR3iz9b3ZS4RLUJBRY24jCjFo5GJJC4bDcdBYzEH3oYcNDDQnaVj6adcj3PNPh3dT4p4mP2NcVOSTWDpLYZyZA93ZWK8i3tIaj7rdWLnXmCLUYN+o6xSnlxeK8Iw0KUgMPQBjpZ+XiHgwI8t9TnmozMs9QTK2SPzotRdrBGVbRIxkYlveaFikBqYu98lNUD+h7AD9Hhy+q9y3k/RPZk2QPz/wV9OEvg9g/mCuZ8vthJKh32Ywz/VWj2RvOg5taNfpSPbo8qhzXJvddZjtsl+2ziH1lx+yUXbA6E6zH/rC/7F/wPRDB70CNU4PSpGabTUXgngGm79wi</latexit>12 (c)Gm
<latexit sha1_base64="KNioJ/sWib5kzWFrKmvyqxdqVkQ=">AAACuXicjVFRSxtBEN6crdpoNbGPfVkaBAUJdyKt4EvEgj6IGGiikASZ20ySNXt7x+5cMBz5Bb62f8B/5b9xLwmlSfrgwMLH9818M7MTJkpa8v3Xgrf24eP6xuan4tb2553dUnmvaePUCGyIWMXmPgSLSmpskCSF94lBiEKFd+HwItfvRmisjPUvGifYiaCvZU8KIEfVg4dSxa/60+CrIJiDCpvH7UO58NLuxiKNUJNQYG0r8BPqZGBICoWTYju1mIAYQh9bDmqI0Hay6aQTvu+YLu/Fxj1NfMr+W5FBZO04Cl1mBDSwy1pO/k9rpdQ77WRSJymhFrNGvVRxinm+Nu9Kg4LU2AEQRrpZuRiAAUHuc4oLbabmCYqFVbKnVEsRd3GJVfREBibF/bZFikDqfK/sCtUIXQ/gN5jiX9X55vLBT9mXZI+u3Q300aVBHB6ulCz4nRsJ6l02s0x31WD5hqugeVwNvldP6ieV2vH8vpvsK/vGDljAfrAau2K3rMEEQ/bMfrM/3pkH3sB7nKV6hXnNF7YQnn0DCI7bfQ==</latexit>1
<latexit sha1_base64="pecWCJildrHVdhbAMoSawdIoZ5A=">AAACuXicjVFNSyNBEO2MuqtZXb+OXhqDoCBhJgRd8KIo6EFEwaiQBKnpVJI2PT1Dd40YhvwCr/oH/Ff7b7YnCWISD1vQ8Hiv6lVVV5goacn3/xa8ufmFHz8Xl4q/lld+r66tb9zZODUCayJWsXkIwaKSGmskSeFDYhCiUOF92DvN9ftnNFbG+pb6CTYj6GjZlgLIUTeVx7WSX/aHwWdBMAYlNo7rx/XCR6MVizRCTUKBtfXAT6iZgSEpFA6KjdRiAqIHHaw7qCFC28yGkw74jmNavB0b9zTxIfu1IoPI2n4UuswIqGuntZz8Tqun1P7TzKROUkItRo3aqeIU83xt3pIGBam+AyCMdLNy0QUDgtznFCfaDM0TFBOrZC+pliJu4RSr6IUMDIo7DYsUgdT5XtkFqmd0PYBfYYqfqvPN5d0z2ZFk9y/dDfT+uUHs7c2UTPidGAnqv2xGme6qwfQNZ8FdpRwclKs31dJxZXzfRbbFttkuC9ghO2YX7JrVmGDIXtkbe/eOPPC63tMo1SuMazbZRHj2Hwrj234=</latexit>2
<latexit sha1_base64="KkNs6T3iXH/7Jwuf1K7bPD75nK4=">AAACuXicjVHfSxtBEN5cbWvTH2p99GUxBFKQcKfSFvpiaUEfRBRMFJIQ5jaTZM3e3rE7J4Yjf0Ff7T/gf+V/073LISbxoQMLH983883MTpgoacn3Hyveq7XXb96uv6u+//Dx08bm1ue2jVMjsCViFZvrECwqqbFFkhReJwYhChVehZNfuX51i8bKWF/SNMFeBCMth1IAOerioL9Z85t+EXwVBCWosTLO+1uVh+4gFmmEmoQCazuBn1AvA0NSKJxVu6nFBMQERthxUEOEtpcVk8543TEDPoyNe5p4wT6vyCCydhqFLjMCGttlLSdf0jopDb/3MqmTlFCLeaNhqjjFPF+bD6RBQWrqAAgj3axcjMGAIPc51YU2hXmCYmGV7C7VUsQDXGIV3ZGBWbXetUgRSJ3vlZ2gukXXA/gZpvikOt9cbvyWI0l279TdQO8dG8TJl5WSBb+fRoL6L5t5prtqsHzDVdDebwZfm4cXh7Wj/fK+62yH7bIGC9g3dsRO2DlrMcGQ/WH37K/3wwNv7N3MU71KWbPNFsKz/wANONt/</latexit>3<latexit sha1_base64="6nQzhJC9Npz6VYjvX2LtnvEx2Vs=">AAACtXicjVHbSiNBEO2M1x3vvu5LYxAUJMxIcAVfFAV9WETBqJAEqelUkiY9PbPdNWIY8gW+6g/sX+3fbE8SxCQ+WNBwOKfqVFVXlCppKQj+lby5+YXFpeUf/srq2vrGpr91b5PMCKyJRCXmMQKLSmqskSSFj6lBiCOFD1HvvNAfntFYmeg76qfYjKGjZVsKIEfdVp82y0ElGAafBeEYlNk4bp62Sn8brURkMWoSCqyth0FKzRwMSaFw4DcyiymIHnSw7qCGGG0zH0464LuOafF2YtzTxIfs54ocYmv7ceQyY6CundYK8iutnlH7uJlLnWaEWowatTPFKeHF2rwlDQpSfQdAGOlm5aILBgS5z/En2gzNUxQTq+QvmZYiaeEUq+iFDAz83YZFikHqYq/8CtUzuh7ArzHDD9X5FvLehexIsge/3Q30waVB7O3PlEz4nRkJ6ls2o0x31XD6hrPg/rASHlWq5dPD8XWX2U+2w/ZYyH6xU3bFbliNCYbslb2xd+/EA687SvRK44ptNhHen/9mutqE</latexit>4 (d)G1,I
<latexit sha1_base64="KNioJ/sWib5kzWFrKmvyqxdqVkQ=">AAACuXicjVFRSxtBEN6crdpoNbGPfVkaBAUJdyKt4EvEgj6IGGiikASZ20ySNXt7x+5cMBz5Bb62f8B/5b9xLwmlSfrgwMLH9818M7MTJkpa8v3Xgrf24eP6xuan4tb2553dUnmvaePUCGyIWMXmPgSLSmpskCSF94lBiEKFd+HwItfvRmisjPUvGifYiaCvZU8KIEfVg4dSxa/60+CrIJiDCpvH7UO58NLuxiKNUJNQYG0r8BPqZGBICoWTYju1mIAYQh9bDmqI0Hay6aQTvu+YLu/Fxj1NfMr+W5FBZO04Cl1mBDSwy1pO/k9rpdQ77WRSJymhFrNGvVRxinm+Nu9Kg4LU2AEQRrpZuRiAAUHuc4oLbabmCYqFVbKnVEsRd3GJVfREBibF/bZFikDqfK/sCtUIXQ/gN5jiX9X55vLBT9mXZI+u3Q300aVBHB6ulCz4nRsJ6l02s0x31WD5hqugeVwNvldP6ieV2vH8vpvsK/vGDljAfrAau2K3rMEEQ/bMfrM/3pkH3sB7nKV6hXnNF7YQnn0DCI7bfQ==</latexit>1
<latexit sha1_base64="pecWCJildrHVdhbAMoSawdIoZ5A=">AAACuXicjVFNSyNBEO2MuqtZXb+OXhqDoCBhJgRd8KIo6EFEwaiQBKnpVJI2PT1Dd40YhvwCr/oH/Ff7b7YnCWISD1vQ8Hiv6lVVV5goacn3/xa8ufmFHz8Xl4q/lld+r66tb9zZODUCayJWsXkIwaKSGmskSeFDYhCiUOF92DvN9ftnNFbG+pb6CTYj6GjZlgLIUTeVx7WSX/aHwWdBMAYlNo7rx/XCR6MVizRCTUKBtfXAT6iZgSEpFA6KjdRiAqIHHaw7qCFC28yGkw74jmNavB0b9zTxIfu1IoPI2n4UuswIqGuntZz8Tqun1P7TzKROUkItRo3aqeIU83xt3pIGBam+AyCMdLNy0QUDgtznFCfaDM0TFBOrZC+pliJu4RSr6IUMDIo7DYsUgdT5XtkFqmd0PYBfYYqfqvPN5d0z2ZFk9y/dDfT+uUHs7c2UTPidGAnqv2xGme6qwfQNZ8FdpRwclKs31dJxZXzfRbbFttkuC9ghO2YX7JrVmGDIXtkbe/eOPPC63tMo1SuMazbZRHj2Hwrj234=</latexit>2
<latexit sha1_base64="KkNs6T3iXH/7Jwuf1K7bPD75nK4=">AAACuXicjVHfSxtBEN5cbWvTH2p99GUxBFKQcKfSFvpiaUEfRBRMFJIQ5jaTZM3e3rE7J4Yjf0Ff7T/gf+V/073LISbxoQMLH983883MTpgoacn3Hyveq7XXb96uv6u+//Dx08bm1ue2jVMjsCViFZvrECwqqbFFkhReJwYhChVehZNfuX51i8bKWF/SNMFeBCMth1IAOerioL9Z85t+EXwVBCWosTLO+1uVh+4gFmmEmoQCazuBn1AvA0NSKJxVu6nFBMQERthxUEOEtpcVk8543TEDPoyNe5p4wT6vyCCydhqFLjMCGttlLSdf0jopDb/3MqmTlFCLeaNhqjjFPF+bD6RBQWrqAAgj3axcjMGAIPc51YU2hXmCYmGV7C7VUsQDXGIV3ZGBWbXetUgRSJ3vlZ2gukXXA/gZpvikOt9cbvyWI0l279TdQO8dG8TJl5WSBb+fRoL6L5t5prtqsHzDVdDebwZfm4cXh7Wj/fK+62yH7bIGC9g3dsRO2DlrMcGQ/WH37K/3wwNv7N3MU71KWbPNFsKz/wANONt/</latexit>3<latexit sha1_base64="6nQzhJC9Npz6VYjvX2LtnvEx2Vs=">AAACtXicjVHbSiNBEO2M1x3vvu5LYxAUJMxIcAVfFAV9WETBqJAEqelUkiY9PbPdNWIY8gW+6g/sX+3fbE8SxCQ+WNBwOKfqVFVXlCppKQj+lby5+YXFpeUf/srq2vrGpr91b5PMCKyJRCXmMQKLSmqskSSFj6lBiCOFD1HvvNAfntFYmeg76qfYjKGjZVsKIEfdVp82y0ElGAafBeEYlNk4bp62Sn8brURkMWoSCqyth0FKzRwMSaFw4DcyiymIHnSw7qCGGG0zH0464LuOafF2YtzTxIfs54ocYmv7ceQyY6CundYK8iutnlH7uJlLnWaEWowatTPFKeHF2rwlDQpSfQdAGOlm5aILBgS5z/En2gzNUxQTq+QvmZYiaeEUq+iFDAz83YZFikHqYq/8CtUzuh7ArzHDD9X5FvLehexIsge/3Q30waVB7O3PlEz4nRkJ6ls2o0x31XD6hrPg/rASHlWq5dPD8XWX2U+2w/ZYyH6xU3bFbliNCYbslb2xd+/EA687SvRK44ptNhHen/9mutqE</latexit>4 (e)G2,I
<latexit sha1_base64="7D2hCvOUx17eEaAEgY4/0uiKRE8=">AAACuXicjVHbSiNBEO3Mes2ut/XRl8YgZCGEGREVfFFWWB+WRcFoIAmhplNJ2vT0DN01YhjyBfuqP7B/5d/YkwQxiQ9b0HA4p+pUVVeYKGnJ918L3pel5ZXVtfXi128bm1vbO9/vbJwagTURq9jUQ7CopMYaSVJYTwxCFCq8Dwc/c/3+EY2Vsb6lYYKtCHpadqUActTNsL1d8qv+OPgiCKagxKZx3d4p/Gt2YpFGqEkosLYR+Am1MjAkhcJRsZlaTEAMoIcNBzVEaFvZeNIRP3BMh3dj454mPmY/VmQQWTuMQpcZAfXtvJaTn2mNlLqnrUzqJCXUYtKomypOMc/X5h1pUJAaOgDCSDcrF30wIMh9TnGmzdg8QTGzSvaUainiDs6xip7IwKh40LRIEUid75VdoXpE1wP4H0zxXXW+uVy+lD1JtvLb3UBXfhnEwY+Fkhm/CyNB/ZfNJNNdNZi/4SK4O6wGx9Wjm6PS+eH0vmtsj+2zMgvYCTtnV+ya1ZhgyP6yZ/binXng9b2HSapXmNbsspnw7BuwdtvF</latexit>y
<latexit sha1_base64="ITZaJfsMf6+k09wCgHRV6O7CMyc=">AAACu3icjVHBSiNBEO3MuupG19X16KUxCAoSZkR2vQiKwnoQUTQqJCHUdCqxNz09Q3e1GIZ8gtfd+/6Vf7M9SRCTeLCg4fFe1auqrjhT0lIYvpSCT3Of5xcWv5SXlr+ufFtd+35rU2cE1kSqUnMfg0UlNdZIksL7zCAkscK7uHdS6HePaKxM9Q31M2wm0NWyIwWQp66jVtRarYTVcBh8FkRjUGHjuGytlf412qlwCWoSCqytR2FGzRwMSaFwUG44ixmIHnSx7qGGBG0zH8464FueafNOavzTxIfs24ocEmv7SewzE6AHO60V5Hta3VHnoJlLnTlCLUaNOk5xSnmxOG9Lg4JU3wMQRvpZuXgAA4L895Qn2gzNMxQTq+RPTkuRtnGKVfREBgblrYZFSkDqYq/8DNUj+h7AL9Dhq+p9C3n7VHYl2d1zfwW9+8sg9nZmSib8jo0E9SGbUaa/ajR9w1lwu1eNflT3r/YrR3vj+y6yDbbJtlnEfrIjdsYuWY0J1mXP7A/7GxwGIvgdqFFqUBrXrLOJCNx/pJrcIQ==</latexit>11
<latexit sha1_base64="9UEkfh74UxSxmISi3/7TqE82C/o=">AAACu3icjVHfSxtBEN6c2mrU1h+PviwGQUHCXZC2LwVLhfogomhUSEKY20ziNnt7x+6sGI78CX1t3/tf+d+4lwQxiQ8OLHx838w3MztxpqSlMHwqBQuLSx8+Lq+UV9fWP33e2Ny6sakzAusiVam5i8GikhrrJEnhXWYQkljhbdz/Wei3D2isTPU1DTJsJdDTsisFkKeuau2ovVEJq+Eo+DyIJqDCJnHR3iz9b3ZS4RLUJBRY24jCjFo5GJJC4bDcdBYzEH3oYcNDDQnaVj6adcj3PNPh3dT4p4mP2NcVOSTWDpLYZyZA93ZWK8i3tIaj7rdWLnXmCLUYN+o6xSnlxeK8Iw0KUgMPQBjpZ+XiHgwI8t9TnmozMs9QTK2SPzotRdrBGVbRIxkYlveaFikBqYu98lNUD+h7AD9Hhy+q9y3k/RPZk2QPz/wV9OEvg9g/mCuZ8vthJKh32Ywz/VWj2RvOg5taNfpSPbo8qhzXJvddZjtsl+2ziH1lx+yUXbA6E6zH/rC/7F/wPRDB70CNU4PSpGabTUXgngGm8dwi</latexit>21
<latexit sha1_base64="3Tj8+misl9G86r3oAJMH8aiN8iw=">AAACu3icjVHfSxtBEN6cttpY66/HviwNgoKEOxX1RVAs6EMpio0KSQhzm0lcs7d37M6K4cif0Nf23f/K/8a9JBST+NCBhY/vm/lmZifOlLQUhi+lYG7+w8eFxU/lpc/LX1ZW19ZvbOqMwJpIVWruYrCopMYaSVJ4lxmEJFZ4G/fOCv32EY2Vqf5F/QybCXS17EgB5KnrvVbUWq2E1XAYfBZEY1Bh47hsrZWeG+1UuAQ1CQXW1qMwo2YOhqRQOCg3nMUMRA+6WPdQQ4K2mQ9nHfBNz7R5JzX+aeJD9m1FDom1/ST2mQnQvZ3WCvI9re6oc9TMpc4coRajRh2nOKW8WJy3pUFBqu8BCCP9rFzcgwFB/nvKE22G5hmKiVXyJ6elSNs4xSp6IgOD8mbDIiUgdbFXfoHqEX0P4D/R4T/V+xby1nfZlWR3fvgr6J1zg9jbnimZ8Ds1EtR/2Ywy/VWj6RvOgpvdanRQ3b/ar5zsju+7yL6yb2yLReyQnbALdslqTLAu+83+sL/BcSCCh0CNUoPSuGaDTUTgXgGpSNwj</latexit>31
<latexit sha1_base64="1U3l/sViK9U/ZhDfKHen/1m8ZNE=">AAACu3icjVHfSxtBEN6c2mrU1h+PviwGQUHCnYS2LwVLhfogomhUSEKY20ziNnt7x+6sGI78CX1t3/tf+d+4lwQxiQ8OLHx838w3MztxpqSlMHwqBQuLSx8+Lq+UV9fWP33e2Ny6sakzAusiVam5i8GikhrrJEnhXWYQkljhbdz/Wei3D2isTPU1DTJsJdDTsisFkKeuau2ovVEJq+Eo+DyIJqDCJnHR3iz9b3ZS4RLUJBRY24jCjFo5GJJC4bDcdBYzEH3oYcNDDQnaVj6adcj3PNPh3dT4p4mP2NcVOSTWDpLYZyZA93ZWK8i3tIaj7rdWLnXmCLUYN+o6xSnlxeK8Iw0KUgMPQBjpZ+XiHgwI8t9TnmozMs9QTK2SPzotRdrBGVbRIxkYlveaFikBqYu98lNUD+h7AD9Hhy+q9y3k/RPZk2QPz/wV9OEvg9g/mCuZ8vthJKh32Ywz/VWj2RvOg5ujavSlWrusVY6PJvddZjtsl+2ziH1lx+yUXbA6E6zH/rC/7F/wPRDB70CNU4PSpGabTUXgngGrn9wk</latexit>41<latexit sha1_base64="95DzUCzk+BYmQtD0I4KZYqSsrYA=">AAACu3icjVFNaxsxEJU3zUfd5rPHXkRNIIVgdo1Jcim4tJAcSklpnARsY2blsa1Yq12kkYlZ/BN6be/5V/031dqm1HYOHRA83pt5M6OJMyUtheHvUrDxYnNre+dl+dXr3b39g8OjW5s6I7ApUpWa+xgsKqmxSZIU3mcGIYkV3sWjT4V+N0ZjZapvaJJhJ4GBln0pgDz1vd6tdQ8qYTWcBV8H0QJU2CKuu4elp3YvFS5BTUKBta0ozKiTgyEpFE7LbWcxAzGCAbY81JCg7eSzWaf82DM93k+Nf5r4jP23IofE2kkS+8wEaGhXtYJ8Tms56l90cqkzR6jFvFHfKU4pLxbnPWlQkJp4AMJIPysXQzAgyH9PeanNzDxDsbRK/ui0FGkPV1hFj2RgWj5uW6QEpC72yq9QjdH3AP4VHf5VvW8hn3yWA0n29Iu/gj69NIij92slS34fjQT1XzbzTH/VaPWG6+C2Vo3OqvVv9UqjtrjvDnvL3rETFrFz1mBX7Jo1mWAD9oP9ZL+CD4EIHgI1Tw1Ki5o3bCkC9wet9Nwl</latexit>42
<latexit sha1_base64="zvv+ufuJrFFV3IByUZRYJLHZgD0=">AAACu3icjVFRSxtBEN6c1tq0Wq2PfVkMggUJd1GqLwVFQR9KsdSokIQwt5nEbfb2jt1ZMRz5Cb7qu/+q/6Z7SRCT+ODAwsf3zXwzsxNnSloKw3+lYGHx3dL75Q/lj59WVj+vrX+5tKkzAusiVam5jsGikhrrJEnhdWYQkljhVdw/LvSrWzRWpvqCBhm2Euhp2ZUCyFN/dtu19lolrIaj4PMgmoAKm8R5e7301OykwiWoSSiwthGFGbVyMCSFwmG56SxmIPrQw4aHGhK0rXw065BveabDu6nxTxMfsS8rckisHSSxz0yAbuysVpCvaQ1H3YNWLnXmCLUYN+o6xSnlxeK8Iw0KUgMPQBjpZ+XiBgwI8t9TnmozMs9QTK2S3zktRdrBGVbRHRkYlreaFikBqYu98jNUt+h7AP+FDp9V71vI2yeyJ8nu/PRX0DunBrH/ba5kyu/ISFBvshln+qtGszecB5e1avS9uvd7r3JYm9x3mX1lm2ybRWyfHbIzds7qTLAeu2cP7DH4EYjgb6DGqUFpUrPBpiJw/wGrndwk</latexit>32
<latexit sha1_base64="RvKa28OGvIz8tsEPhXDIh/UxzRQ=">AAACu3icjVHfSxtBEN6c1mpq/fnoy2IQFCTcBWn7UrBUqA8iikaFJIS5zSSu2ds7dmfFcORP6Gv73v/K/8a9JIhJfOjAwsf3zXwzsxNnSloKw+dSsLD4Yenj8kr50+rntfWNza0bmzojsC5SlZq7GCwqqbFOkhTeZQYhiRXexv2fhX77iMbKVF/TIMNWAj0tu1IAeeqq1q61NyphNRwFnwfRBFTYJC7am6V/zU4qXIKahAJrG1GYUSsHQ1IoHJabzmIGog89bHioIUHbykezDvmeZzq8mxr/NPER+7Yih8TaQRL7zATo3s5qBfme1nDU/dbKpc4coRbjRl2nOKW8WJx3pEFBauABCCP9rFzcgwFB/nvKU21G5hmKqVXyJ6elSDs4wyp6IgPD8l7TIiUgdbFXforqEX0P4Ofo8FX1voW8fyJ7kuzhmb+CPvxlEPsHcyVTfj+MBPVfNuNMf9Vo9obz4KZWjb5Ujy6PKse1yX2X2Q7bZfssYl/ZMTtlF6zOBOux3+wP+xt8D0TwEKhxalCa1GyzqQjcC6lG3CM=</latexit>22
<latexit sha1_base64="GP2ihqxmBtt+TDUW2M5JA8bANYI=">AAACu3icjVHfSxtBEN6c2mrU1h+PviwGQUHCXZC2LwVLhfogomhUSEKY20ziNnt7x+6sGI78CX1t3/tf+d+4lwQxiQ8OLHx838w3MztxpqSlMHwqBQuLSx8+Lq+UV9fWP33e2Ny6sakzAusiVam5i8GikhrrJEnhXWYQkljhbdz/Wei3D2isTPU1DTJsJdDTsisFkKeuonatvVEJq+Eo+DyIJqDCJnHR3iz9b3ZS4RLUJBRY24jCjFo5GJJC4bDcdBYzEH3oYcNDDQnaVj6adcj3PNPh3dT4p4mP2NcVOSTWDpLYZyZA93ZWK8i3tIaj7rdWLnXmCLUYN+o6xSnlxeK8Iw0KUgMPQBjpZ+XiHgwI8t9TnmozMs9QTK2SPzotRdrBGVbRIxkYlveaFikBqYu98lNUD+h7AD9Hhy+q9y3k/RPZk2QPz/wV9OEvg9g/mCuZ8vthJKh32Ywz/VWj2RvOg5taNfpSPbo8qhzXJvddZjtsl+2ziH1lx+yUXbA6E6zH/rC/7F/wPRDB70CNU4PSpGabTUXgngGm79wi</latexit>12 (f)Gm,I
Figure 1: (a)-(b): sample component DAGs; (c)the mixture DAG for I=∅, note that ∆ ={2,3,4}
(when the distribution of node 1remains the same) ; (d)-(e): post-intervention component DAGs
forI={2};(f): corresponding I-mixture DAG. Also note that true edges Et={(1→2),(2→
3),(3→2),(3→4),(1→4)}, inseparable pairs Ei={(1−2),(1−3),(1−4),(2−3),(2−
4),(3−4)}, and emergent edges Ee={(1,3),(2,4)}.
Definition 5 (I-mixture DAG) .Given an intervention Ion a mixture of DAGs, I-mixture DAG Gm,I
is a graph with nK+ 1nodes constructed by first concatenating the Kcomponent DAGs and then
adding a single node yto the concatenation. Furthermore, there will be a directed edge from yto
every node in ∆Iin every DAG {Gℓ,I:ℓ∈[K]}. In the I-mixture DAG Gm,I, we use iℓto denote
the copy of node iinGℓ,I. Accordingly, for any A⊆Vwe define ¯A≜{iℓ:i∈A, ℓ∈[K]}.
Figure 1 illustrates an example of a mixture of K= 2component DAGs, different edge types, an
intervention Ion the mixture, and the construction of the I-mixture DAG from post-intervention
component DAGs G1,IandG2,I. We define the observational mixture DAG as theI-mixture DAG
when the intervention set is I=∅and denote it by Gm. It is known that pmspecified in (3)satisfies
the global Markov property with respect to observational mixture DAG [ 10, Theorem 3.2]. It can be
readily verified that this result extends to the interventional setting for pm,IandGm,I. We make the
following faithfulness assumption to facilitate causal discovery via statistical independence tests.
Assumption 1 (I-mixture faithfulness) .For any intervention I ⊆V, the interventional mixture
distribution pm,I(x)is faithful to Gm,I, that is if XA⊥ ⊥XB|XCinpm,I(x), then ¯Aand¯Bare
d-separated given ¯CinGm,I.
Finally, we note that the observational counterpart of Assumption 1, i.e., when I=∅, is standard in
the literature for analyzing a mixture of DAGs [ 9–11]. In working with interventions, we naturally
extend it to interventional mixture distributions. Also note that Assumption 1 does not compare
observational and interventional distributions. Hence, it is not comparable to various faithfulness
assumptions in the literature on the interventional causal discovery of a single DAG, e.g., [29, 31].
2.3 Causal discovery objectives
We aim to address the following question: how can we use interventions to perform causal discovery
in a mixture of DAGs , with the objectives specified next.
The counterpart of this question is well-studied for the causal discovery of a single DAG. Since the
unoriented skeleton of the single DAG can already be identified by CI tests on observational data,
interventions are leveraged to orient the edges. Interventions are generally bounded by a pre-specified
budget, measured by the number of interventions. The extent of causal relationships that observational
data can uncover in a mixture of DAGs is significantly narrower than those in single DAGs. The
striking difference is the existence of emergent pairs specified in (6). Therefore, the objective of
intervention design extends to distinguishing truecause-effect relationships from the emergent pairs
as well as determining the direction of causality. Specifically, we focus on identifying the true edges
specified in (4)as the edges exist in at least one component DAG of the mixture. For this purpose,
two central objectives of our investigation are:
1.Determining the necessary and sufficient size of the interventions for identifying true edges Et,
2. Designing efficient algorithms with near-optimal intervention sizes.
53 Interventions for causal discovery of a mixture of DAGs
In this section, we investigate the first key question of interventional causal discovery on a mixture of
DAGs and investigate the size of the necessary and sufficient interventions for identifying mixture
parents of a node. First, we consider a mixture of general DAGs without imposing structural
constraints and establish matching necessary and sufficient intervention size for distinguishing a
true edge from an emergent pair. Then, we strengthen the results for a mixture of directed trees.
The results established in this section are pivotal for understanding the fundamental limits of causal
discovery of a mixture of DAGs. These results guide the intervention design in Section 4.
Our analysis uncovers the connections between the mixture distribution under an intervention Iand
the structure of post-intervention component DAGs {Gℓ,I:ℓ∈[K]}. We know that the interventional
mixture distribution pm,Isatisfies the Markov property with respect to I-mixture DAG Gm,Ispecified
in Definition 5. Therefore, in conjunction with the I-mixture faithfulness assumption, the separation
statements in Gm,Ican be inferred exactly by testing the conditional independencies in pm,I. To
establish the necessary and sufficient intervention sizes, we recall that set ∆plays an important role in
the separability conditions in I-mixture DAG Gm,Isince ∆allows paths across different component
DAGs. The following result serves as an intermediate step in obtaining our main result.
Lemma 1. Consider an inseparable pair (i, j)∈Eiand an intervention I ⊆V. We have the
following identifiability guarantees using the interventional mixture distribution pm,I(x).
(i)Identifiability: It is possible to determine whether j∈pam(i)ifj∈ Iand there do not exist
∆-through paths from jtoiinGℓ,Ifor any ℓ∈[K].
(ii)Non-identifiability: It is impossible to determine whether j∈pam(i)ifj∈∆Ior there exists
a∆-child-through path from jtoiin at least one Gℓ,Iwhere ℓ∈[K].
Lemma 1 provides intuition for characterizing sufficient and necessary conditions for identifying a
true edge. The identifiability result implies that it suffices to choose an intervention Ithat reduces
the viable ∆-through paths in Gm,Ito true edges from jtoi. Similarly, the non-identifiability result
implies the necessity of intervening on ∆-child nodes. Building on these properties, our main result in
this section establishes matching necessary and sufficient intervention sizes for identifying true edges.
Theorem 1 (Intervention sizes) .Consider nodes i, j∈Vin a mixture of DAGs.
(i)Sufficiency: For any mixture of DAGs, there exists an intervention Iwith|I| ≤ | pam(i)|+ 1
that ensures the determination of whether j∈pam(i)using CI tests on pm,I.
(ii)Necessity: There exist DAG mixtures for which it is impossible to determine whether j∈pam(i)
using CI tests on pm,I(x)for any intervention Iwith|I| ≤ | pam(i)|.
Theorem 1 represents a fundamental step for understanding the intricacies of mixture causal discovery
and serves as a guide for evaluating the optimality and efficiency of any learning algorithm. We also
note that the necessity statement reflects a worst-case scenario. As such, we present the following
refined sufficiency results that can guide efficient algorithm designs.
Lemma 2. Consider nodes i, j∈Vin a mixture of DAGs. It is possible to determine whether
j∈pam(i)using CI tests on pm,Iand any of the following interventions:
(i)I={j} ∪S
ℓ∈[K]
paℓ(i)∩deℓ(j)	
; or
(ii)I={j} ∪S
ℓ∈[K]
anℓ(i)∩chℓ(j)	
; or
(iii)I={j} ∪S
ℓ∈[K]
anℓ(i)∩deℓ(j)∩∆	
.
Note that the three interventions in Lemma 2 can coincide when parents of iin a component
DAG are also children of jand are in ∆. This case yields the set I= pam(i)∪ {j}with size
(|pam(i)|+ 1). Since this can be a rare occurrence for realistic mixture models, partial knowledge
about the underlying component DAGs, e.g., ancestral relations or the knowledge of ∆, can prove
to be useful for identifying pam(i)using interventions with smaller sizes. Finally, we note that
our results in Theorem 1 and Lemma 2 are given for a mixture of general DAGs, and they can be
improved for special classes of DAGs. In the next result, we focus on mixtures of directed trees.
Theorem 2 (Intervention sizes – trees) .Consider nodes i, j∈Vin a mixture of Kdirected trees.
(i)Sufficiency: For any mixture of directed trees, there exists an intervention Iwith|I| ≤ K+ 1
such that it is possible to determine whether j∈pam(i)using CI tests on pm,I.
6(ii)Necessity: There exist mixtures of directed trees such that it is impossible to determine whether
j∈pam(i)using CI tests on pm,Ifor any intervention Iwith|I| ≤ K.
Theorem 2 shows that, unlike the general result in Theorem 1, the number of mixture components
plays a key role when considering a mixture of directed trees. Hence, prior knowledge of the number
of mixture components can be useful for the causal discovery of a mixture of directed trees.
4 Learning algorithm and its analysis
In this section, we design an adaptive algorithm that identifies and orients all true edges, referred to as
CausalDiscovery from Interventions on Mixture Models (CADIM). The algorithm is summarized in
Algorithm 1, and its steps are described in Section 4.1. We also analyze the performance guarantees
of the algorithm and the optimality of the interventions used in the algorithm in Section 4.2.
4.1 Causal discovery from interventions on mixture models
The proposed CADIM algorithm designs interventions for performing causal discovery on a mixture
of DAGs. The algorithm is designed to be general and demonstrate feasible time complexity for any
mixture of DAGs without imposing structural constraints. Therefore, we forego the computationally
expensive task of learning the inseparable pairs from observational data, which requires O(n2·2n)CI
tests [ 11], and entirely focus on leveraging interventions for discovering the true causal relationships.
The key idea of the algorithm is to use interventions to decompose the ancestors of a node into topo-
logical layers and identify the mixture parents by sequentially processing the topological layers using
carefully selected interventions. The algorithm consists of four main steps, which are described next.
Step 1: Identifying mixture ancestors. We start by identifying the set of mixture ancestors anm(i)
for each node i∈V, i.e., the union of ancestors of iin the component DAGs. For this purpose,
we use single-node interventions. Specifically, for each node i∈V, we intervene on I={i}and
construct the set of nodes that are marginally dependent on Xiinpm,I, i.e.,
ˆde(i) ={j:Xj̸⊥ ⊥Xiinpm,{i}},∀i∈V. (9)
Then, we construct the sets ˆ an(i) ={j:i∈ˆde(j)}for all i∈V. Under I-mixture faithfulness,
this procedure ensures that ˆde(i) = de m(i), and ˆ an(i) = an m(i)(see Lemma 3). The rest of the
algorithm steps aim to identify mixture parents of a single node i,pam(i), within the set ˆ an(i). Hence,
the following steps can be repeated for all i∈Vto identify all true edges.
Step 2: Obtaining cycle-free descendants. In this step, we consider a given node i∈Vand aim
to break the cycles across the nodes in ˆ an(i)by careful interventions. Once this is achieved, for all
j∈ˆ an(i), we will refine j’s descendant set ˆde(j)tocycle-free descendant set dei(j). The motivation
is that these refined descendant sets can be used to topologically order the nodes in ˆ an(i). The details
of this step work as follows. First, we construct the set of cycles
C(i)≜{π= (π1, . . . , π ℓ) :π1=πℓ,∀u∈[ℓ−1]πu∈ˆ an(i)∧πu∈ˆ an(πu+1)}. (10)
Subsequently, if C(i)is not empty, we define a minimal set that shares at least one node with each
cycle in C(i),
B(i)≜a minimal set such that ∀π∈ C(i)|B(i)∩π| ≥1. (11)
We refer to B(i)as the breaking set of node isince intervening on any set Ithat contains B(i)
breaks all the cyclic relationships in C(i). Then, if C(i)is not empty, we sequentially intervene on
I=B(i)∪ {j}for all j∈ˆ an(i), and construct the cycle-free descendant sets defined as
dei(j)← {k∈ˆ an(i)∪ {i}:Xj̸⊥ ⊥Xkinpm,I},where I=B(i)∪ {j}. (12)
Note that dei(j)is a subset of dem(j)since intervening on jmakes it independent of all its non-
descendants. Finally, we construct the set A={j∈ˆ an(i) :i∈dei(j)}.
Step 3: Topological layering. In this step, we decompose ˆ an(i)into topological layers by using the
cycle-free descendant sets constructed in Step 2. We start by constructing the first layer as
S1(i) ={j∈ A: dei(j)∩ A=∅}. (13)
7Algorithm 1 Ca usalDiscovery from Interventions on Mixture Models (CADIM)
1:Step 1: Identify mixture ancestors
2:fori∈Vdo
3: Intervene on I={i}, observe samples from pm,I
4: ˆde(i)← {j:Xj̸⊥ ⊥Xiinpm,I} ▷mixture descendants of node i
5:fori∈Vdo
6: ˆ an(i)← {j:i∈ˆde(j)} ▷mixture ancestors of node i
7:Repeat Steps 2, 3, 4 for all i∈V
8:Step 2: Obtain cycle-free descendants
9:Find cycles among ˆ an(i)
10:C(i)← {π= (π1, . . . , π t+1) :π1=πt+1,∀u∈[t]πu∈ˆ an(i)∧πu∈ˆ an(πu+1)}
11:ifC(i)is empty then
12: B(i)← ∅
13: forj∈ˆ an(i)do
14: dei(j)←ˆde(j)∩ˆ an(i)	
∪ {i}
15:else
16: B(i)←a minimal set such that ∀π∈ C(i)|B(i)∩π| ≥1
17: forj∈ˆ an(i)do
18: Intervene on I=B(i)∪ {j} ▷break cycles among ˆ an(i)
19: dei(j)← {k∈ˆ an(i)∪ {i}:Xj̸⊥ ⊥Xkinpm,I}▷cycle-free descendants of node j
20:A ← { j∈ˆ an(i) :i∈dei(j)} ▷refined ancestors
21:Step 3: Topological layering
22:t←0
23:while|A| ≥ 1do
24: t←t+ 1
25: St(i)← {j∈ A: dei(j)∩ A=∅}
26: A ← A \ St(i)
27:Step 4: Identify mixture parents
28:ˆ pa(i)← ∅
29:foru∈(1, . . . , t )do
30: forj∈Su(i)do
31: Intervene on I= ˆ pa( i)∪ B(i)∪ {j}
32: ifXj̸⊥ ⊥Xiinpm,Ithen
33: ˆ pa(i)←ˆ pa(i)∪ {j}
34:Return ˆ pa(i)
The construction of cycle-free descendant sets ensures that S1(i)is not empty. Next, we update
A ← A \ S1(i)by removing layer S1(i)to conclude the first step. Then, we iteratively construct the
layers Su(i) ={j∈ A:ˆde(j)∩ A=∅}and update A ← A \ Su(i)as in Line 26 of the algorithm.
We continue until the set Ais exhausted, and denote these topological layers by {S1(i), . . . , S t(i)}.
Step 4: Identifying the mixture parents. Finally, we process the topological layers sequentially
to identify the mixture parents in each layer. For a node j∈S1(i), whether j∈pam(i)can be
determined from a marginal independence test on pm,Iwhere I=B(i)∪ {j}. Leveraging this
result, when processing each Su(i), we consider the nodes j∈Su(i)sequentially and intervene on
I= ˆ pa( i)∪ B(i)∪ {j}, where ˆ pa(i)denotes the estimated mixture parents. Under this intervention,
a statistical dependence implies a true edge from jtoi. Hence, we update the set ˆ pa(i)as follows.
ˆ pa(i)←ˆ pa(i)∪ {j}ifXj̸⊥ ⊥Xiinpm,Iwhere I= ˆ pa( i)∪ B(i)∪ {j}. (14)
After the last layer St(i)is processed, the algorithm returns the estimated mixture parents ˆ pa(i). By
repeating Steps 2, 3, and 4 for all i∈V, we determine the true edges with their orientations.
84.2 Guarantees of the CADIM algorithm
In this section, we establish the guarantees of the CADIM algorithm and interpret them vis-à-vis the
results in Section 3. We start by providing the following result to show the correctness of identifying
mixture ancestors.
Lemma 3. GivenI-mixture faithfulness, Step 1 of Algorithm 1 identifies {anm(i) :i∈[n]}using n
single-node interventions.
Note that the mixture ancestor sets {anm(i)}do not imply a topological order over the nodes V, e.g.,
there may exist nodes u, vsuch that u∈anm(v)andv∈anm(u). As such, a major difficulty in
learning a mixture of DAGs compared to learning a single DAG is the possible cyclic relationships
formed by the combination of components of the mixture. Recall that the breaking set is specified
in(11) to treat such possible cycles carefully. We refer to the size of B(i)as the cyclic complexity
number of node i, denoted by τi, and the size of the largest breaking set by τmas
τi≜|B(i)|,∀i∈V, and τm≜max
i∈Vτi. (15)
Note that τiis readily bounded by the number of cycles in C(i). Next, we analyze the guarantees of the
algorithm for a node iin two cases: τi= 0(cycle-free case) and τi≥1(nonzero cyclic complexity).
Cycle-free case. Our next result shows that if τi= 0, i.e., there are no cycles among the nodes in
anm(i), then we identify the mixture parents pam(i), i.e., the union of the nodes that are parents of i
in at least one component DAG, using interventions with the optimal size.
Theorem 3 (Guarantees for cycle-free ancestors) .If the cyclic complexity of node iis zero, then
Algorithm 1 ensures that ˆ pa(i) = pam(i)by using |anm(i)|interventions where the size of each
intervention is at most |pam(i)|+ 1.
Theorem 3 shows that by repeating the algorithm steps for each node i∈V, we can identify all true
edges with their orientations using n+P
i∈V|anm(i)| ≤n+n(n−1) = n2interventions, where
the size of each intervention is bounded by the worst-case necessary size established in Theorem 1.
Nonzero cyclic complexity. Finally, we address the most general case, in which the mixture
ancestors of node imight contain cycles. In this case, our algorithm performs additional interventions
to break the cycles among anm(i). Hence, the number and size of the interventions will be greater
than the cycle-free case, which is established in the following result.
Theorem 4 (Guarantees for general mixtures) .Algorithm 1 ensures that ˆ pa(i) = pam(i)by using
|anm(i)|interventions with size τi+1, and|anm(i)|interventions with size at most |pam(i)|+τi+1.
Theorem 4 shows that, Algorithm 1 achieves the causal discovery objectives by using a total of
n+ 2P
i∈V|anm(i)| ≤n+ 2n(n−1) =O(n2)interventions, where the maximum intervention
size for learning each pam(i)is at most τilarger than the necessary and sufficient size |pam(i)|+ 1.
This optimality gap reflects the challenges of accommodating cyclic relationships in intervention
design for learning in mixtures while also maintaining a quadratic number of interventions O(n2).
5 Experiments
We evaluate the performance of Algorithm 1 for estimating the true edges in a mixture of DAGs using
synthetic data and investigate the need for interventions, the effect of the graph size, and the cyclic
complexity. Additional results for varying the number of components, parameterization, and number
of samples are provided in Appendix E1.
Experimental setup. We use an Erd ˝os-Rényi model G(n, p)with density p= 2/nto generate
the component DAGs {Gℓ:ℓ∈[K]}for different values of nodes nand mixture components K.
We adopt linear structural equation models (SEMs) with Gaussian noise for the causal models, in
which the noise for node iis sampled from N(µi, σ2
i)where µiis sampled uniformly in [−1,1]and
σ2
iis sampled uniformly in [0.5,1.5]. The edge weights are sampled uniformly in ±[0.25,2]. We
1The codebase for the experiments can be found at https://github.com/bvarici/
intervention-mixture-DAG .
95 6 7 8 9 10
Number of nodes0.60.70.80.91.0F1 score
Ours (intervention-based)
Observation-based(a) Skeleton recovery for observa-
tional and interventional methods
5 10 15 20 25 30
Number of nodes0.900.920.940.960.981.00Recovery of true edges
precision
recall(b) Varying the number of nodes
nfor a mixture of K= 3DAGs
5 6 7 8 9 10
Number of nodes0.00.51.01.52.02.53.03.54.0Cyclic complexity
K=2 - true
K=3 - true
K=4 - trueK=2 - est.
K=3 - est.
K=4 - est.(c) Empirical cyclic complexity
for varying nandKvalues
Figure 2: Mean true edge recovery rates and quantification of mean cyclic complexity of a node.
consider the case where a change in the conditional distribution of node iis only caused by changes
in the parents of iacross different DAGs. We use a partial correlation test to check (conditional)
independence in the algorithm steps, similar to the related work [ 10,11]. We repeat this procedure
for100randomly generated DAG mixtures for each of the following settings.
Need for interventions. We demonstrate the need for interventions for learning the skeleton in the
mixture of DAGs, unlike the case of single DAGs. To this end, we consider a mixture of K= 2
DAGs and learn the inseparable node pairs via exhaustive CI tests (see Algorithm 2 in Appendix E).
Figure 2a empirically verifies the claim that true edges (even their undirected versions) cannot be
learned using observational data only.
Recovery of true edges. We evaluate the performance of Algorithm 1 on the central task of learning
the true edges in the mixture. For this purpose, we report average precision and recall rates for
recovering the true edges. We look into the performance of Algorithm 1 under a varying number of
nodes n∈[5,30]for a mixture of K= 3DAGs and using 5000 samples from each DAG. Figure 2b
demonstrates that Algorithm 1 maintains a strong performance even under n= 30 nodes. We provide
additional results for the number of DAGs in the range K∈[2,10]and varying number of samples
in Appendix E.
Quantification of cyclic complexity. We recall that for finding the mixture parents of a node i, the
maximum size of the intervention used in Algorithm 1 is at most τi, i.e., cyclic complexity, larger
than the necessary size. In Figure 2c, we plot the empirical values of average cyclic complexity –
both the ground truth and estimated by the algorithm. Figure 2c shows that even though average τi
increases with K, it still remains very small, e.g., approximately 1.5for a mixture of K= 3DAGs
withn= 10 nodes. Furthermore, on average, the estimated τivalues used in the algorithm are almost
identical to the ground truth τi. Therefore, Algorithm 1 maintains its close to optimal intervention
size guarantees in the finite-sample regime.
6 Conclusion
In this paper, we have conducted the first analysis of using interventions to learn causal relationships
in a mixture of DAGs. First, we have established the matching necessary and sufficient size of
interventions needed for learning the true edges in a mixture. Subsequently, guided by this result,
we have designed an algorithm that learns the true edges using interventions with close to optimal
sizes. We have also analyzed the optimality gap of our algorithm in terms of the cyclic relationships
within the mixture model. The proposed algorithm uses a total of O(n2)interventions. Establishing
lower bounds for the number of interventions with constrained sizes remains an important direction
for future work, which can draw connections to intervention design for single-DAG and further
characterize the differences of causal discovery in mixtures. Finally, generalizing the mixture model
to accommodate partial knowledge of the underlying domains can be useful in disciplines where such
knowledge can be acquired a priori.
Acknowledgments and disclosure of funding
This work was supported by IBM through the IBM-Rensselaer Future of Computing Research
Collaboration.
10References
[1]Nir Friedman, Michal Linial, Iftach Nachman, and Dana Pe’er. Using Bayesian networks
to analyze expression data. In Proc. International Conference on Computational Molecular
Biology , Tokyo, Japan, April 2000.
[2]Nicolai Meinshausen, Alain Hauser, Joris M Mooij, Jonas Peters, Philip Versteeg, and Peter
Bühlmann. Methods for causal inference from gene perturbation experiments and validation.
Proceedings of the National Academy of Sciences , 113(27):7361–7368, 2016.
[3]Guido W Imbens. Potential outcome and directed acyclic graph approaches to causality:
Relevance for empirical practice in economics. Journal of Economic Literature , 58(4):1129–
1179, 2020.
[4]Brett M Reid, Jennifer B Permuth, and Thomas A Sellers. Epidemiology of ovarian cancer: A
review. Cancer Biology & Medicine , 14(1):9, 2017.
[5]Yanxi Chen and H. Vincent Poor. Learning mixtures of linear dynamical systems. In Proc.
International Conference on Machine Learning , Baltimore, Maryland, July 2022.
[6]Kirsten Bulteel, Francis Tuerlinckx, Annette Brose, and Eva Ceulemans. Clustering vector
autoregressive models: Capturing qualitative differences in within-person dynamics. Frontiers
in Psychology , 7:1540, 2016.
[7]Emma Brunskill, Bethany R. Leffler, Lihong Li, Michael L. Littman, and Nicholas Roy. Provably
efficient learning with typed parametric models. Journal of Machine Learning Research , 10
(68):1955–1988, 2009.
[8]Peter Spirtes. Directed cyclic graphical representations of feedback models. In Proc. Conference
on Uncertainty in Artificial Intelligence , Montréal, Canada, August 1995.
[9]Eric V . Strobl. Causal discovery with a mixture of dags. Machine Learning , 112(11):4201–4225,
2023.
[10] Basil Saeed, Snigdha Panigrahi, and Caroline Uhler. Causal structure discovery from distribu-
tions arising from mixtures of dags. In Proc. International Conference on Machine Learning ,
virtual, July 2020.
[11] Burak Varıcı, Dmitriy Katz, Dennis Wei, Prasanna Sattigeri, and Ali Tajer. Separability analysis
for causal discovery in mixture of DAGs. Transactions on Machine Learning Research , 2024.
ISSN 2835-8856. URL https://openreview.net/forum?id=ALRWXT1RLZ .
[12] Thomas Verma and Judea Pearl. An algorithm for deciding if a set of observed independencies
has a causal explanation. In Proc. Conference on Uncertainty in Artificial Intelligence , Stanford,
CA, July 1992.
[13] Frederick Eberhardt and Richard Scheines. Interventions and causal inference. Philosopy of
Science , 74(5):981–995, December 2007.
[14] Alain Hauser and Peter Bühlmann. Two optimal strategies for active learning of causal models
from interventional data. International Journal of Approximate Reasoning , 55(4):926–939, June
2014.
[15] Antti Hyttinen, Frederick Eberhardt, and Patrik O Hoyer. Experiment selection for causal
discovery. Journal of Machine Learning Research , 14(1):3041–3071, 2013.
[16] Karthikeyan Shanmugam, Murat Kocaoglu, Alexandros G Dimakis, and Sriram Vishwanath.
Learning causal graphs with small interventions. In Proc. Advances in Neural Information
Processing Systems , Montréal, Canada, December 2015.
[17] Chandler Squires, Sara Magliacane, Kristjan Greenewald, Dmitriy Katz, Murat Kocaoglu, and
Karthikeyan Shanmugam. Active structure learning of causal DAGs via directed clique trees.
InProc. Advances in Neural Information Processing Systems , December 2020.
11[18] Sumanth Varambally, Yi-An Ma, and Rose Yu. Discovering mixtures of structural causal models
from time series data. arXiv:2310.06312 , 2023.
[19] Murat Kocaoglu, Karthikeyan Shanmugam, and Elias Bareinboim. Experimental design for
learning causal graphs with latent variables. Proc. Advances in Neural Information Processing
Systems , December 2017.
[20] Raghavendra Addanki, Shiva Kasiviswanathan, Andrew McGregor, and Cameron Musco.
Efficient intervention design for causal discovery with latents. In Proc. International Conference
on Machine Learning , July 2020.
[21] Raghavendra Addanki, Andrew McGregor, and Cameron Musco. Intervention efficient algo-
rithms for approximate learning of causal graphs. In Algorithmic Learning Theory , March
2021.
[22] Yang-Bo He and Zhi Geng. Active learning of causal networks with intervention experiments
and optimal designs. Journal of Machine Learning Research , 9(November):2523–2547, 2008.
[23] Kristjan Greenewald, Dmitriy Katz, Karthikeyan Shanmugam, Sara Magliacane, Murat Ko-
caoglu, Enric Boix Adsera, and Guy Bresler. Sample efficient active learning of causal trees.
InProc. Advances in Neural Information Processing Systems , Vancouver, Canada, December
2019.
[24] Vibhor Porwal, Piyush Srivastava, and Gaurav Sinha. Almost optimal universal lower bound for
learning causal dags with atomic interventions. In Proc. International Conference on Artificial
Intelligence and Statistics , virtual, March 2022.
[25] Davin Choo, Kirankumar Shiragur, and Arnab Bhattacharyya. Verification and search algorithms
for causal dags. In Proc. Advances in Neural Information Processing Systems , New Orleans,
LA, December 2022.
[26] Murat Kocaoglu, Alex Dimakis, and Sriram Vishwanath. Cost-optimal learning of causal graphs.
InProc. International Conference on Machine Learning , Sydney, Australia, August 2017.
[27] Erik Lindgren, Murat Kocaoglu, Alexandros G Dimakis, and Sriram Vishwanath. Experimental
design for cost-aware learning of causal graphs. In Proc. Advances in Neural Information
Processing Systems , Montréal, Canada, December 2018.
[28] Ehsan Mokhtarian, Saber Salehkaleybar, AmirEmad Ghassami, and Negar Kiyavash. A unified
experiment design approach for cyclic and acyclic causal models. Journal of Machine Learning
Research , 24(354):1–31, 2023.
[29] Karren Yang, Abigail Katcoff, and Caroline Uhler. Characterizing and learning equivalence
classes of causal DAGs under interventions. In Proc. International Conference on Machine
Learning , Stockholm, Sweden, July 2018.
[30] Amin Jaber, Murat Kocaoglu, Karthikeyan Shanmugam, and Elias Bareinboim. Causal discovery
from soft interventions with unknown targets: Characterization and learning. In Proc. Advances
in Neural Information Processing Systems , December 2020.
[31] Chandler Squires, Yuhao Wang, and Caroline Uhler. Permutation-based causal structure learning
with unknown intervention targets. In Proc. Conference in Uncertainty in Artificial Intelligence ,
August 2020.
[32] Philippe Brouillard, Sébastien Lachapelle, Alexandre Lacoste, Simon Lacoste-Julien, and
Alexandre Drouin. Differentiable causal discovery from interventional data. In Proc. Advances
in Neural Information Processing Systems , December 2020.
[33] Bo Thiesson, Christopher Meek, David Maxwell Chickering, and David Heckerman. Learning
mixtures of DAG models. In Proc. Conference on Uncertainty in Artificial Intelligence , Madison,
WI, July 1998.
[34] Biwei Huang, Kun Zhang, Pengtao Xie, Mingming Gong, Eric P Xing, and Clark Glymour. Spe-
cific and shared causal relation modeling and mechanism-based clustering. In Proc. Advances
in Neural Information Processing Systems , Vancouver, Canada, December 2019.
12[35] Kun Zhang and Madelyn RK Glymour. Unmixing for causal inference: Thoughts on Mccaffrey
and Danks. The British Journal for the Philosophy of Science , 2020.
[36] Wei Chen, Yunjin Wu, Ruichu Cai, Yueguo Chen, and Zhifeng Hao. CCSL: A causal structure
learning method from multiple unknown environments. arXiv:2111.09666 , 2021.
[37] Alex Markham, Richeek Das, and Moritz Grosse-Wentrup. A distance covariance-based kernel
for nonlinear causal clustering in heterogeneous populations. In Proc. Conference on Causal
Learning and Reasoning , Eureka, CA, April 2022.
[38] Biwei Huang, Kun Zhang, Jiji Zhang, Joseph D Ramsey, Ruben Sanchez-Romero, Clark
Glymour, and Bernhard Schölkopf. Causal discovery from heterogeneous/nonstationary data.
Journal of Machine Learning Research , 21(89):1–53, 2020.
[39] Kun Zhang, Biwei Huang, Jiji Zhang, Clark Glymour, and Bernhard Schölkopf. Causal discov-
ery from nonstationary/heterogeneous data: Skeleton estimation and orientation determination.
InProc. International Joint Conference on Artificial Intelligence , Melbourne, Australia, August
2017.
[40] Joris M. Mooij, Sara Magliacane, and Tom Claassen. Joint causal inference from multiple
contexts. Journal of Machine Learning Research , 21(99):1–108, 2020.
[41] Amin Jaber, Murat Kocaoglu, Karthikeyan Shanmugam, and Elias Bareinboim. Causal discovery
from soft interventions with unknown targets: Characterization and learning. In Proc. Advances
in Neural Information Processing Systems , virtual, December 2020.
[42] Chenxi Liu and Kun Kuang. Causal structure learning for latent intervened non-stationary data.
InProc. International Conference on Machine Learning , Honolulu, Hawaii, July 2023.
[43] Peter Spirtes, Clark N Glymour, Richard Scheines, and David Heckerman. Causation, Predic-
tion, and Search . MIT Press, Cambridge, MA, 2000.
[44] Judea Pearl. Causality . Cambridge University Press, Cambridge, UK, 2009.
[45] Joris M Mooij and Tom Claassen. Constraint-based causal discovery using partial ancestral
graphs in the presence of cycles. In Proc. Conference on Uncertainty in Artificial Intelligence ,
virtual, August 2020.
[46] F Ann Ran, Patrick D Hsu, Jason Wright, Vineeta Agarwala, David A Scott, and Feng Zhang.
Genome engineering using the crispr-cas9 system. Nature Protocols , 8(11):2281–2308, 2013.
[47] Geoffrey J McLachlan, Sharon X Lee, and Suren I Rathnayake. Finite mixture models. Annual
Review of Statistics and Its Application , 6(1):355–378, 2019.
[48] Abhinav Kumar and Gaurav Sinha. Disentangling mixtures of unknown causal interventions.
InProc. Uncertainty in Artificial Intelligence , July 2021.
13Interventional Causal Discovery in a Mixture of DAGs
Appendices
Table of Contents
A Auxiliary results 14
B Proofs for Section 3 15
B.1 Proof of Lemma 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15
B.2 Proof of Lemma 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15
B.3 Proof of Theorem 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15
B.4 Proof of Theorem 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16
C Proofs for Section 4 16
C.1 Proof of Lemma 3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16
C.2 Proof of Theorem 3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16
C.3 Proof of Theorem 4 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18
D Additional examples 20
D.1 Partitioning true edges into component DAGs . . . . . . . . . . . . . . . . . . . 20
D.2 An example of cyclic complexity . . . . . . . . . . . . . . . . . . . . . . . . . 20
E Additional experiments 21
A Auxiliary results
Lemma 4 (Markov property, [ 10, Theorem 3.2]) .LetA, B, C ⊆Vbe disjoint. If ¯Aand¯Bare
d-separated given ¯Cin the mixture DAG, then XAandXBare conditionally independent given XC
in mixture distribution.
Lemma 5 ([11, Theorem 5]) .Consider nodes i, j∈Vsuch that iandjare not adjacent in any of
the component DAGs, i.e., i /∈pam(j)andj /∈pam(i).
(i) If i∈∆andj∈∆:iandjare always inseparable, i.e., (i−j)is an emergent edge.
(ii)Ifi /∈∆andj /∈∆: Ifiandjare inseparable, then there exist two component DAGs Gℓ,Gℓ′
such that Gℓcontains a ∆-through path from itojandGℓ′contains a ∆-through path from
jtoi.
(iii) Ifi /∈∆andj∈∆: Ifiandjare inseparable, then at least one component DAG contains a
∆-through path from itoj.
Lemma 6 ([11, Theorem 6]) .Suppose that G1, . . . ,GKare directed trees. Consider nodes i, j∈V
such that iandjare not adjacent in any component DAG, i.e., i /∈pam(j)andj /∈pam(i).
(i) If i∈∆andj∈∆:iandjare always inseparable.
(ii)Ifi /∈∆andj /∈∆:iandjare separable if and only if there does not exist Gℓ,Gℓ′such that
the two DAGs contain ∆-child-through paths between iandjin opposite directions.
(iii) Ifi /∈∆andj∈∆:iandjare separable if and only if none of the component DAGs contains
a∆-child-through path from itoj.
14B Proofs for Section 3
B.1 Proof of Lemma 1
Proof of identifiability: Since j∈ I, we have j /∈∆I. Then, Lemma 5 (iii) implies that if (j→i)
is not a true edge and there does not exist a ∆-through path from jtoiin any Gℓ,I, then iandjare
separable in pm,I. Consequently, whether j∈pam(i)can be determined from pm,I.
Proof of non-identifiability: First, note that using Lemma 4, for any intervention I,pm,Iis Markov
with respect to the I-mixture DAG Gm,I. Then, for j∈∆I, if also i∈∆I,iandjare inseparable in
pm,Iregardless of whether there is a true edge between iandj. Hence, whether j∈pam(i)cannot
be determined using CI tests on pm,I. For the second statement, recall that intervention Icuts off
all incoming edges to nodes of Iin all component DAGs. Then, if i∈ I, we cannot determine
whether j∈pam(i)since the possible influence of joniis cut off by the intervention. Suppose
thati∈∆I, and let πbe a∆-child-through path from jtoiin some component DAG Gℓ,I, i.e., π
is given by jℓ→kℓ⇝ifor some k∈∆I. Since i∈∆I, theI-mixture DAG Gm,Ialso contains
the path jℓ→kℓ←yℓ→i. Since these two paths cannot be blocked simultaneously by conditioning
on a set of nodes, iandjare inseparable regardless of whether j∈pam(i). Therefore, whether
j∈pam(i)cannot be determined using CI tests on pm,I.
B.2 Proof of Lemma 2
We start by providing a general statement that will be used for the proof of the three subcases. Let
Ibe an intervention such that j∈ Iand there does not exist a ∆-through path from jtoiin any
component DAG Gℓ,I. In this case, using Lemma 5, if (j→i)is not a true edge, then iandjare
separable in pm,I. Subsequently, if iandjare inseparable in pm,I, then (j→i)is a true edge.
Letπbe a∆-through path from jtoiin some Gℓ,I. Note that, for any of the following three
intervention sets,
(i)I={j} ∪S
ℓ∈[K]
paℓ(i)∩deℓ(j)	
,
(ii)I={j} ∪S
ℓ∈[K]
anℓ(i)∩chℓ(j)	
,
(iii)I={j} ∪S
ℓ∈[K]
anℓ(i)∩deℓ(j)∩∆	
,
πcannot contain a node between jandi. Therefore, if j /∈pam(i), there does not exist a ∆-through
path from jtoi. Subsequently, if iandjare not separable in pm,Ifor any of these three interventions,
it means there exists jℓ→ifor some component DAG Gℓ. Therefore, whether j∈pam(i)can be
determined by checking whether iandjare separable in pm,Ifor any of these three interventions.
B.3 Proof of Theorem 1
The sufficiency result immediately follows from Lemma 2 since each of the three interventions in
stated in Lemma 2 is a subset of pam(i)∪ {j}. To show the worst-case necessity of an intervention
with size at least |pam(i)|+ 1, we construct the following example. Consider component DAGs
{Gℓ:ℓ∈[K]}such that G1contains a single edge i1→j. In the rest of the component DAGs, for any
kℓ→iedge, let us also draw jℓ→k. We do not put any constraints on the other possible connections
in{Gℓ:ℓ∈ {2, . . . , K }}. Note that this construction yields that pam(i)∪ {i, j} ⊆∆. Consider the
paths
j1←y1→i , and{jℓ→kℓ→i:k∈pam(i)}. (16)
For any intervention I ⊆V\ {i}that does not contain all nodes in pam(i)∪ {j}, at least one of
these paths will be active in the I-mixture DAG Gm,I, regardless of whether there exists a jℓ→i,
ℓ∈ {2, . . . , K }edge. Therefore, at the worst-case, whether j∈pam(i)cannot be determined from
pm,Ifor any intervention Iwith size |I| ≤ | pam(i)|.
15B.4 Proof of Theorem 2
Proof of sufficiency. Consider an inseparable pair (i−j)inpm. For a mixture of directed trees,
Lemma 6 implies that if there does not exist a ∆-child-through path from jtoiin any component
DAG, then (i−j)corresponds to a true edge. Consider the intervention
I={j} ∪[
ℓ∈[K]{anℓ(i)∩chℓ(j)∩∆}, (17)
which cuts off all ∆-child-through paths from jtoiin component DAGs. Also note that |anℓ(i)∩
chℓ(j)∩∆| ≤1since each Gℓcontains at most one causal path from jtoi. Then, iandjare
inseparable in pm,Iif and only if j∈pam(i), since j∈ Icuts off all iℓ→jedges. Therefore, we
can determine whether j∈pam(i)frompm,Iwith an intervention Iwhere
|I|= 1 +X
ℓ∈[K]1 
{anℓ(i)∩chℓ(j)∩∆} ̸=∅
≤K+ 1. (18)
Proof of necessity. For the worst-case necessity, consider component trees {Gℓ:ℓ∈[K]}such that
each graph contains a ∆-child-through path from jtoiin which the children of jin each graph is
distinct. Also, let k∈pa1(j)butk /∈paℓ(j)for all ℓ∈ {2, . . . , K }. This construction yields that
J≜{i, j} ∪[
ℓ∈[K]{anℓ(i)∩chℓ(j)∩∆} ⊆∆. (19)
Consider the paths
j1←y1→i , and{jℓ→kℓ⇝i:k∈ J \ { i}}. (20)
For any intervention I ⊆V\{i}that does not contain all nodes in J \{i}, at least one of these paths
will be active in the I-mixture DAG Gm,I, regardless of whether one of the ∆-child-through paths is
jℓ→iitself. Note that if j /∈pam(i), then this specific construction yields that |J \ { i}|=K+ 1.
Therefore, at the worst-case, whether j∈pam(i)cannot be determined from pm,Ifor any intervention
Iwith size |I| ≤ K.
C Proofs for Section 4
C.1 Proof of Lemma 3
Consider intervention I={i}and the corresponding interventional mixture distribution pm,IandI-
mixture DAG Gm,I. We will show that {j:Xj̸⊥ ⊥Xiinpm,I}is equal to the mixture descendants
dem(i). First, let i∈anm(j), i.e., there exists a path iℓ⇝jfor some ℓ∈[K]. Then, by I-mixture
faithfulness, Xj̸⊥ ⊥Xiinpm,I. For the other direction, let Xj̸⊥ ⊥Xiinpm,I. Then, there must
be an active path between ¯jand¯iinI-mixture DAG Gm,I. Since iis intervened and there is no
conditioning set, the path cannot contain any collider, which implies that the path is in the form iℓ⇝j
for some ℓ∈[K]. Therefore, we have
i∈anm(j)⇐⇒ Xj̸⊥ ⊥Xiinpm,{i}, (21)
which concludes the proof.
The choice of single-node interventions in Step 1 . We note that the choice of single-node interven-
tions for learning the mixture ancestors {anm(i) :i∈V}is deliberate for simplicity. It is possible to
achieve the same guarantee using fewer than ninterventions by designing multi-node interventions,
e.g., using separating systems with restricted intervention sizes [ 16]. Nevertheless, using ninterven-
tion does not compromise our main results since as we show in the sequel, the number of total inter-
ventions in the algorithm will be dominated by the number of interventions in the subsequent steps.
C.2 Proof of Theorem 3
Lemma 3 ensures that Step 1 of Algorithm 1 identifies anm(i)anddem(i)correctly for all i∈[n].
Hence, in this proof we use anm(i)anddem(i)forˆ an(i)andˆde(i), respectively. We consider the
16case where there are no cycles among the mixture ancestors of node iinduced by the mixture ancestral
relationships, i.e., the following set of cycles is empty.
C(i)← {π= (π1, . . . , π ℓ) :π1=πℓ,∀u∈[ℓ−1]πu∈anm(i)∧πu∈anm(πu+1)}.(22)
In this case, Step 2 of Algorithm 1 only creates the sets
dei(j)≜ˆde(j)∩ {ˆ an(i)∪i}= de m(j)∩ {anm(i)∪i},∀i∈anm(i), (23)
andA= an m(i). The lack of cycles implies that the nodes in Acan be topologically ordered, i.e.,
there exists an ordering (A1, . . . ,A|anm(i)|)such that Aj∈dem(Ak)implies j < k . In Step 3, we
leverage this key property for constructing hierarchically ordered topological layers.
Next, recall the definition of S1(i)in (13),
S1(i) ={j∈anm(i) : de m(j)∩anm(i) =∅}. (24)
Then, since there are no cycles among the nodes in anm(i),S1(i)is not empty, e.g., the first node of
the topological order described above has no mixture descendant within anm(i). Consider j∈S1(i).
Since anm(i)∩dem(j) =∅,jmust be a mixture parent of i. Therefore, S1(i)⊆pam(i). We will
use induction to prove that topological layering in Step 3 and the sequential interventions in Step 4
ensure identifying pam(i).
Base case. Consider S2(i)defined as
S2(i) ={j∈anm(i)\S1(i) : de m(j)∩ {anm(i)\S1(i)}=∅}. (25)
Note that we have ˆ pa(i) =S1(i). Consider a node j∈S2(i)and intervene on I=S1(i)∪ {j}. If
j∈pam(i), then by I-mixture faithfulness, Xj̸⊥ ⊥Xiinpm,I. On the other direction, suppose that
Xj̸⊥ ⊥Xiinpm,I. Ifj /∈pam(i), then there exists an active path between ¯jand¯iinGm,I. Since
the conditioning set is empty, there cannot be any colliders on the path. Then, since j /∈pam(i),
the path has the form jℓ⇝kℓ→ifor some k∈anm(i)andℓ∈[K]. We know that k∈S1(i)
since intervention Icontains S1(i). Then, k∈dem(j)∩ {anm(i)\S1(i)}, which contradicts
withj∈S2(i)by definition of S2(i). Therefore, Xj̸⊥ ⊥Xiinpm,Iimplies that j∈pam(i).
Subsequently, for j∈S2(i), we have
j∈pam(i)⇐⇒ Xj̸⊥ ⊥Xiinpm,I. (26)
Note that this step uses |S2(i)|interventions, one for each j∈S2(i), and each intervention has size
|I|=|S1(i)|+ 1.
Induction hypothesis. Assume that we have identified the set Su(i)∩pam(i)correctly for u∈
{1, . . . , v −1}, i.e., we have ˆ pa(i) =v−1S
k=1Sk(i)∩pam(i). We will show that the algorithm also
identifies Sv(i)∩pam(i)correctly.
LetA= an m(i)\v−1S
k=1Sk(i)and consider Sv(i)defined as
Sv(i) ={j∈ A: dem(j)∩ A=∅}. (27)
Consider a node j∈Sv(i)and intervene on
I={j} ∪ˆ pa(i) ={j} ∪v−1[
k=1Sk(i)∩pam(i). (28)
Ifj∈pam(i), then by I-mixture faithfulness, Xj̸⊥ ⊥Xiinpm,I. For the other direction, suppose
thatXj̸⊥ ⊥Xiinpm,I. Ifj /∈pam(i), then there exists an active path between ¯jand¯iinI-mixture
DAGGm,I. Since the conditioning set is empty, this path has the form jℓ⇝kℓ→ifor some
k∈dem(j)∩ A, which contradicts with j∈Sv(i)by the definition of Sv(i). Subsequently, for
j∈Sv(i), we have
j∈pam(i)⇐⇒ Xj̸⊥ ⊥Xiinpm,I. (29)
Therefore, by induction, the algorithm identifies Su(i)∩pam(i)correctly for all u∈ {1, . . . , t}.
17Finally, note that while processing each layer Su(i), the algorithm uses |Su(i)|interventions, one for
eachj∈Su(i), with size |I|=pam(i)∩u−1S
k=1Sk(i)+ 1. This is upper bounded by |pam(i)|+ 1,
which is shown to be the worst-case necessary intervention size in Theorem 1. Then, including n
single-node interventions performed in Step 1, for identifying pam(i)for all i∈V, Algorithm 1 uses
a total of
n+nX
i=1tX
u=1|Su(i)|=n+nX
i=1|anm(i)|=O(n2) (30)
interventions, which completes the proof of the theorem.
C.3 Proof of Theorem 4
We start by giving a synopsis of the proof. Lemma 3 ensures that Step 1 of Algorithm 1 identifies
anm(i)anddem(i)correctly for all i∈V. Hence, in this proof we use anm(i)anddem(i)forˆ an(i)
and ˆdem(i), respectively. In this theorem, we consider the most general case in which the nodes in
mixture ancestors anm(i)can form cycles via their mixture ancestral relationships. These cycles will
be accommodated by the procedure in Step 2. Intuitively, by intervening on a small number of nodes,
we can break all the cycles in C(i)in the new interventional mixture graphs. Then, we would be able
to follow Steps 3 and 4 similarly to the proof of Theorem 3, albeit using interventions with larger
sizes.
Step 2. First, we recall the definition of cycles among mixture ancestors of i,
C(i)← {π= (π1, . . . , π ℓ) :π1=πℓ,∀u∈[ℓ−1]πu∈ˆ an(i)∧πu∈ˆ an(πu+1)},(31)
and the associated breaking set,
B(i)≜a minimal set s.t. ∀π∈ C(i),|B(i)∩π| ≥1. (32)
We denote the size of the breaking set by τi≜|B(i)|and refer to it as the cyclic complexity of node i.
The intervention I=B(i)breaks all cycles in C(i). To see this consider a cycle π= (π1, . . . , π ℓ)in
C(i)and suppose that piu∈ B(i). Then, intervening on πubreaks all causal paths from πu−1toπu,
which breaks the cycle. In Step 2, we leverage this property to obtain cycle-free descendants of each
nodej∈anm(i). Specifically, for each each j∈anm(i), we intervene on I=B(i)∪ {j}and set
dei(j) ={k∈anm(i)∪ {i}:Xj̸⊥ ⊥Xkinpm,I}. (33)
Note that if j∈pam(i), then i∈dei(j). Hence, after constructing these cycle-free descendant sets,
we refine the ancestor set
A={j∈anm(i) :i∈dei(j)}, (34)
which contains all pam(i). We will use induction to prove that topological layering in Step 3 and the
sequential interventions on Step 4 ensure to identify pam(i)fromA.
Base case. Consider S1(i)defined as
S1(i) ={j∈ A: dei(j)∩ A=∅}. (35)
First, we show that S1(i)is not empty. Otherwise, starting from a node π1∈ A, we would have
dei(π1)∩ A ̸=∅=⇒ ∃ π2∈dei(π1)∩ A (36)
dei(π2)∩ A ̸=∅=⇒ ∃ π3∈dei(π2)∩ A (37)
... (38)
dei(πℓ)∩ A ̸=∅=⇒ ∃ π1∈dei(πℓ)∩ A (39)
sinceAhas finite elements. However, this implies that none of the {π1, . . . , π ℓ}are contained in
B(i)due to the construction of dei(j)sets with interventions I=B(i)∪ {j}. This contradicts with
the definition of the breaking set B(i)as it does not contain any node from the cycle {π1, . . . , π ℓ}.
18Next, consider a node j∈S1(i)and intervene on I=B(i)∪ {j}. We will show that
j∈pam(i)⇐⇒ Xj̸⊥ ⊥Xiinpm,I. (40)
Ifj∈pam(i), then by I-mixture faithfulness, Xj̸⊥ ⊥Xiinpm,I. We prove the other direction,
that is Xj̸⊥ ⊥Xiinpm,Iimplies that j∈pam(i)as follows. First, note that Xj̸⊥ ⊥Xiinpm,I
does not have a conditioning set. Then, it implies that there exists an active path jℓ⇝iinGℓ,Ifor
some ℓ∈[K]. Suppose that j /∈pam(i), which implies that jℓ→kℓ⇝iinGℓ,I, and k /∈ B(i)for
path being active. However, in this case we have k∈dei(j)∩ A, which contradicts with j∈S1(i)
due to definition of S1(i). Hence, for j∈S1(i),Xj̸⊥ ⊥Xiinpm,Iimplies that j∈pam(i), which
concludes the proof of the base case.
Induction step. Assume that we have identified the set Su(i)∩pam(i)correctly for u∈
{1, . . . , v −1}. LetA= an m(i)\v−1S
k=1Sk(i)and consider Sv(i)defined as
Sv(i) ={j∈ A: dei(j)∩ A=∅}. (41)
Note that, after processing {S1, . . . , S v−1}correctly, we have
ˆ pa(i) = pam(i)∩v−1[
k=1Sk(i). (42)
Consider a node j∈Sv(i)and intervene on
I={j} ∪ˆ pa(i)∪ B(i) ={j} ∪v−1[
k=1Sk(i)∩pam(i). (43)
We will show that
j∈pam(i)⇐⇒ Xj̸⊥ ⊥Xiinpm,I. (44)
Ifj∈pam(i), then by I-mixture faithfulness, Xj̸⊥ ⊥Xiinpm,I. We will prove the other direction,
that is Xj̸⊥ ⊥Xiinpm,Iimplies j∈pam(i), similarly to the base case. First, note that Xj̸⊥ ⊥Xi
inpm,Idoes not have a conditioning set. Then, it implies that there exists an active path jℓ⇝i
inGℓ,Ifor some ℓ∈[K]. Now, suppose that j /∈pam(i), which implies that the active path has
the form jℓ⇝kℓ→iinGℓ,Ifor some ℓ∈[K]andk /∈ I. Since k∈pam(i),k /∈ I implies that
k /∈v−1S
u=1Su(i). Then, we have k∈dei(j)∩ A, which contradicts with k∈Sv(i)due to definition
ofSv(i). Therefore, for j∈Sv(i)andI=B(i)∪ˆ pa(i)∪ {j},Xj̸⊥ ⊥Xiinpm,Iimplies that
j∈pam(i), which concludes the proof of the induction step. Therefore, by induction, the algorithm
identifies Su∩pam(i)correctly for all u∈ {1, . . . , t}.
Finally, note that while processing each layer Su(i), the algorithm uses |Su(i)|interventions, one for
eachj∈Su(i), with size |I|=pam(i)∩u−1S
k=1Sk(i)+B(i) + 1 , where τi=|B(i)|is referred to as
the cyclic complexity of node i. Therefore, the size of the largest intervention set is
|pam(i)|+τi+ 1. (45)
We note that this upper bound on the intervention size is τilarger than the necessary size |pam(i)|+ 1
shown in Theorem 1. This optimality gap reflects the effect of the cyclic complexity of the problem.
Finally, adding nsingle-node interventions performed in Step 1, for identifying pam(i)for all i∈V,
Algorithm 1 uses a total of
n+nX
i=1|anm(i)|+nX
i=1tX
u=1|Su(i)|=n+ 2nX
i=1|anm(i)| ≤2n2−n=O(n2) (46)
interventions, which completes the proof of the theorem.
19D Additional examples
D.1 Partitioning true edges into component DAGs
We have emphasized in Section 1 that the component DAGs of the mixture cannot be identified
without further assumptions even under our interventional setting. We discuss a few examples of this
matter.
1. Consider the following two mixtures of K= 2DAGs
• Mixture 1: Edge sets E1={1→2,1→3}andE2=∅
• Mixture 2: Edge sets E1={1→2}andE2={1→3}
In this case, distributions of the two mixtures can still be the same under all intervention sets
I ⊆ { 1,2,3}. Hence, without additional assumptions (e.g., model parameterization), we cannot
distinguish the two mixtures via only conditional independence tests. Instead, we can only learn
the set of true edges, Et={1→2,1→3}for both mixtures.
2.Consider a mixture of two DAGs with edge sets E1={1→2,2→3}andE2={3→1}.
Recall that in Stage 1 of Algorithm 1, we learn the mixture ancestors of the nodes as an
intermediate step. Hence, after learning the set of true edges in the mixture via the rest of the
algorithm, we have the following information:
1∈anm(3),1/∈pam(3),1∈pam(2), (47)
2∈pam(3),2/∈anm(1), (48)
3∈pam(1),3/∈anm(2) (49)
Suppose that we know K= 2. Then, we can see that the only possible component DAGs that
result in this mixture are E1={1→2,2→3}andE2={3→1}. This is because 3→1
cannot be in the same DAG as the other two edges due to the known ancestral relationships.
Hence, we learn the component DAGs in this case. However, without learning the true edges,
we would not be able to know that we can learn the component DAGs of the mixture model.
These examples show that our work is a necessary first step into the interventional causal discovery
of mixtures. Furthermore, we hope that it can inspire future work for the use of interventions in a
mixture of models, e.g., establishing graphical conditions for (partial) recovery of individual DAGs,
leveraging the side information.
Finally, we note two things regarding the possible side information that can enable stronger results.
First, mixture distributions (the underlying component distributions pℓ(x)s) can be identified under
some assumptions, e.g., Gaussian mixture models [ 47]. Second, in a related line of work, disen-
tangling mixtures of unknown interventional datasets is studied under specific conditions on the
intervention sets and given the distribution of the observational DAG [ 48]. Establishing the necessary
and sufficient conditions for achieving similar disentangling objectives tasks in our mixture model is
an open problem for future work.
D.2 An example of cyclic complexity
We have empirically quantified the average cyclic complexity in Section 5. In addition, we give
a visual example here. Consider the mixture of two DAGs in Figure 3. By definition of mixture
ancestors, we have
anm(1) ={2,5}, (50)
anm(2) ={3,5}, (51)
anm(3) =∅, (52)
anm(4) ={1,2,3,5}, (53)
anm(5) ={1,2}. (54)
20<latexit sha1_base64="KNioJ/sWib5kzWFrKmvyqxdqVkQ=">AAACuXicjVFRSxtBEN6crdpoNbGPfVkaBAUJdyKt4EvEgj6IGGiikASZ20ySNXt7x+5cMBz5Bb62f8B/5b9xLwmlSfrgwMLH9818M7MTJkpa8v3Xgrf24eP6xuan4tb2553dUnmvaePUCGyIWMXmPgSLSmpskCSF94lBiEKFd+HwItfvRmisjPUvGifYiaCvZU8KIEfVg4dSxa/60+CrIJiDCpvH7UO58NLuxiKNUJNQYG0r8BPqZGBICoWTYju1mIAYQh9bDmqI0Hay6aQTvu+YLu/Fxj1NfMr+W5FBZO04Cl1mBDSwy1pO/k9rpdQ77WRSJymhFrNGvVRxinm+Nu9Kg4LU2AEQRrpZuRiAAUHuc4oLbabmCYqFVbKnVEsRd3GJVfREBibF/bZFikDqfK/sCtUIXQ/gN5jiX9X55vLBT9mXZI+u3Q300aVBHB6ulCz4nRsJ6l02s0x31WD5hqugeVwNvldP6ieV2vH8vpvsK/vGDljAfrAau2K3rMEEQ/bMfrM/3pkH3sB7nKV6hXnNF7YQnn0DCI7bfQ==</latexit>1<latexit sha1_base64="pecWCJildrHVdhbAMoSawdIoZ5A=">AAACuXicjVFNSyNBEO2MuqtZXb+OXhqDoCBhJgRd8KIo6EFEwaiQBKnpVJI2PT1Dd40YhvwCr/oH/Ff7b7YnCWISD1vQ8Hiv6lVVV5goacn3/xa8ufmFHz8Xl4q/lld+r66tb9zZODUCayJWsXkIwaKSGmskSeFDYhCiUOF92DvN9ftnNFbG+pb6CTYj6GjZlgLIUTeVx7WSX/aHwWdBMAYlNo7rx/XCR6MVizRCTUKBtfXAT6iZgSEpFA6KjdRiAqIHHaw7qCFC28yGkw74jmNavB0b9zTxIfu1IoPI2n4UuswIqGuntZz8Tqun1P7TzKROUkItRo3aqeIU83xt3pIGBam+AyCMdLNy0QUDgtznFCfaDM0TFBOrZC+pliJu4RSr6IUMDIo7DYsUgdT5XtkFqmd0PYBfYYqfqvPN5d0z2ZFk9y/dDfT+uUHs7c2UTPidGAnqv2xGme6qwfQNZ8FdpRwclKs31dJxZXzfRbbFttkuC9ghO2YX7JrVmGDIXtkbe/eOPPC63tMo1SuMazbZRHj2Hwrj234=</latexit>2
<latexit sha1_base64="KkNs6T3iXH/7Jwuf1K7bPD75nK4=">AAACuXicjVHfSxtBEN5cbWvTH2p99GUxBFKQcKfSFvpiaUEfRBRMFJIQ5jaTZM3e3rE7J4Yjf0Ff7T/gf+V/073LISbxoQMLH983883MTpgoacn3Hyveq7XXb96uv6u+//Dx08bm1ue2jVMjsCViFZvrECwqqbFFkhReJwYhChVehZNfuX51i8bKWF/SNMFeBCMth1IAOerioL9Z85t+EXwVBCWosTLO+1uVh+4gFmmEmoQCazuBn1AvA0NSKJxVu6nFBMQERthxUEOEtpcVk8543TEDPoyNe5p4wT6vyCCydhqFLjMCGttlLSdf0jopDb/3MqmTlFCLeaNhqjjFPF+bD6RBQWrqAAgj3axcjMGAIPc51YU2hXmCYmGV7C7VUsQDXGIV3ZGBWbXetUgRSJ3vlZ2gukXXA/gZpvikOt9cbvyWI0l279TdQO8dG8TJl5WSBb+fRoL6L5t5prtqsHzDVdDebwZfm4cXh7Wj/fK+62yH7bIGC9g3dsRO2DlrMcGQ/WH37K/3wwNv7N3MU71KWbPNFsKz/wANONt/</latexit>3
<latexit sha1_base64="6nQzhJC9Npz6VYjvX2LtnvEx2Vs=">AAACtXicjVHbSiNBEO2M1x3vvu5LYxAUJMxIcAVfFAV9WETBqJAEqelUkiY9PbPdNWIY8gW+6g/sX+3fbE8SxCQ+WNBwOKfqVFVXlCppKQj+lby5+YXFpeUf/srq2vrGpr91b5PMCKyJRCXmMQKLSmqskSSFj6lBiCOFD1HvvNAfntFYmeg76qfYjKGjZVsKIEfdVp82y0ElGAafBeEYlNk4bp62Sn8brURkMWoSCqyth0FKzRwMSaFw4DcyiymIHnSw7qCGGG0zH0464LuOafF2YtzTxIfs54ocYmv7ceQyY6CundYK8iutnlH7uJlLnWaEWowatTPFKeHF2rwlDQpSfQdAGOlm5aILBgS5z/En2gzNUxQTq+QvmZYiaeEUq+iFDAz83YZFikHqYq/8CtUzuh7ArzHDD9X5FvLehexIsge/3Q30waVB7O3PlEz4nRkJ6ls2o0x31XD6hrPg/rASHlWq5dPD8XWX2U+2w/ZYyH6xU3bFbliNCYbslb2xd+/EA687SvRK44ptNhHen/9mutqE</latexit>4<latexit sha1_base64="cPGp2UuswnhiFsy6ql/WviG9bZM=">AAAB8XicbVDLSgNBEOyNrxhfUY9eBoPgKewGHzkGPOgxgnlgsoTZSScZMju7zMwKYclfePGgiFf/xpt/4yTZgyYWNBRV3XR3BbHg2rjut5NbW9/Y3MpvF3Z29/YPiodHTR0limGDRSJS7YBqFFxiw3AjsB0rpGEgsBWMb2Z+6wmV5pF8MJMY/ZAOJR9wRo2VHtMuo4LcTnuVXrHklt05yCrxMlKCDPVe8avbj1gSojRMUK07nhsbP6XKcCZwWugmGmPKxnSIHUslDVH76fziKTmzSp8MImVLGjJXf0+kNNR6Ega2M6RmpJe9mfif10nMoOqnXMaJQckWiwaJICYis/dJnytkRkwsoUxxeythI6ooMzakgg3BW355lTQrZe+qfHl/UapVszjycAKncA4eXEMN7qAODWAg4Rle4c3Rzovz7nwsWnNONnMMf+B8/gDWJZBa</latexit>G2
<latexit sha1_base64="l23oEDfxQDo7M9aSNY7xxIAzfZI=">AAAB8XicbVDLSgNBEOyNrxhfUY9eBoPgKeyKjxwDHvQYwTwwWcLsZDYZMju7zPQKYclfePGgiFf/xpt/4yTZg0YLGoqqbrq7gkQKg6775RRWVtfWN4qbpa3tnd298v5By8SpZrzJYhnrTkANl0LxJgqUvJNoTqNA8nYwvp757UeujYjVPU4S7kd0qEQoGEUrPWQ9RiW5mfa9frniVt05yF/i5aQCORr98mdvELM04gqZpMZ0PTdBP6MaBZN8WuqlhieUjemQdy1VNOLGz+YXT8mJVQYkjLUthWSu/pzIaGTMJApsZ0RxZJa9mfif100xrPmZUEmKXLHFojCVBGMye58MhOYM5cQSyrSwtxI2opoytCGVbAje8st/Seus6l1WL+7OK/VaHkcRjuAYTsGDK6jDLTSgCQwUPMELvDrGeXbenPdFa8HJZw7hF5yPb9ShkFk=</latexit>G1<latexit sha1_base64="JeAr3F/YtJbBvAqKr8CYWjYm4cI=">AAACuXicjVHfSxtBEN5cbWvTH2p99GUxBFKQcCfaFvpiaUEfRBRMFJIQ5jaTZM3e3rE7J4Yjf0Ff7T/gf+V/073LISbxoQMLH983883MTpgoacn3Hyveq7XXb96uv6u+//Dx08bm1ue2jVMjsCViFZvrECwqqbFFkhReJwYhChVehZNfuX51i8bKWF/SNMFeBCMth1IAOerisL9Z85t+EXwVBCWosTLO+1uVh+4gFmmEmoQCazuBn1AvA0NSKJxVu6nFBMQERthxUEOEtpcVk8543TEDPoyNe5p4wT6vyCCydhqFLjMCGttlLSdf0jopDb/3MqmTlFCLeaNhqjjFPF+bD6RBQWrqAAgj3axcjMGAIPc51YU2hXmCYmGV7C7VUsQDXGIV3ZGBWbXetUgRSJ3vlZ2gukXXA/gZpvikOt9cbvyWI0l279TdQO8dG8TJl5WSBb+fRoL6L5t5prtqsHzDVdDebwZfm4cXB7Wj/fK+62yH7bIGC9g3dsRO2DlrMcGQ/WH37K/3wwNv7N3MU71KWbPNFsKz/wASNNuC</latexit>5<latexit sha1_base64="KNioJ/sWib5kzWFrKmvyqxdqVkQ=">AAACuXicjVFRSxtBEN6crdpoNbGPfVkaBAUJdyKt4EvEgj6IGGiikASZ20ySNXt7x+5cMBz5Bb62f8B/5b9xLwmlSfrgwMLH9818M7MTJkpa8v3Xgrf24eP6xuan4tb2553dUnmvaePUCGyIWMXmPgSLSmpskCSF94lBiEKFd+HwItfvRmisjPUvGifYiaCvZU8KIEfVg4dSxa/60+CrIJiDCpvH7UO58NLuxiKNUJNQYG0r8BPqZGBICoWTYju1mIAYQh9bDmqI0Hay6aQTvu+YLu/Fxj1NfMr+W5FBZO04Cl1mBDSwy1pO/k9rpdQ77WRSJymhFrNGvVRxinm+Nu9Kg4LU2AEQRrpZuRiAAUHuc4oLbabmCYqFVbKnVEsRd3GJVfREBibF/bZFikDqfK/sCtUIXQ/gN5jiX9X55vLBT9mXZI+u3Q300aVBHB6ulCz4nRsJ6l02s0x31WD5hqugeVwNvldP6ieV2vH8vpvsK/vGDljAfrAau2K3rMEEQ/bMfrM/3pkH3sB7nKV6hXnNF7YQnn0DCI7bfQ==</latexit>1<latexit sha1_base64="pecWCJildrHVdhbAMoSawdIoZ5A=">AAACuXicjVFNSyNBEO2MuqtZXb+OXhqDoCBhJgRd8KIo6EFEwaiQBKnpVJI2PT1Dd40YhvwCr/oH/Ff7b7YnCWISD1vQ8Hiv6lVVV5goacn3/xa8ufmFHz8Xl4q/lld+r66tb9zZODUCayJWsXkIwaKSGmskSeFDYhCiUOF92DvN9ftnNFbG+pb6CTYj6GjZlgLIUTeVx7WSX/aHwWdBMAYlNo7rx/XCR6MVizRCTUKBtfXAT6iZgSEpFA6KjdRiAqIHHaw7qCFC28yGkw74jmNavB0b9zTxIfu1IoPI2n4UuswIqGuntZz8Tqun1P7TzKROUkItRo3aqeIU83xt3pIGBam+AyCMdLNy0QUDgtznFCfaDM0TFBOrZC+pliJu4RSr6IUMDIo7DYsUgdT5XtkFqmd0PYBfYYqfqvPN5d0z2ZFk9y/dDfT+uUHs7c2UTPidGAnqv2xGme6qwfQNZ8FdpRwclKs31dJxZXzfRbbFttkuC9ghO2YX7JrVmGDIXtkbe/eOPPC63tMo1SuMazbZRHj2Hwrj234=</latexit>2
<latexit sha1_base64="KkNs6T3iXH/7Jwuf1K7bPD75nK4=">AAACuXicjVHfSxtBEN5cbWvTH2p99GUxBFKQcKfSFvpiaUEfRBRMFJIQ5jaTZM3e3rE7J4Yjf0Ff7T/gf+V/073LISbxoQMLH983883MTpgoacn3Hyveq7XXb96uv6u+//Dx08bm1ue2jVMjsCViFZvrECwqqbFFkhReJwYhChVehZNfuX51i8bKWF/SNMFeBCMth1IAOerioL9Z85t+EXwVBCWosTLO+1uVh+4gFmmEmoQCazuBn1AvA0NSKJxVu6nFBMQERthxUEOEtpcVk8543TEDPoyNe5p4wT6vyCCydhqFLjMCGttlLSdf0jopDb/3MqmTlFCLeaNhqjjFPF+bD6RBQWrqAAgj3axcjMGAIPc51YU2hXmCYmGV7C7VUsQDXGIV3ZGBWbXetUgRSJ3vlZ2gukXXA/gZpvikOt9cbvyWI0l279TdQO8dG8TJl5WSBb+fRoL6L5t5prtqsHzDVdDebwZfm4cXh7Wj/fK+62yH7bIGC9g3dsRO2DlrMcGQ/WH37K/3wwNv7N3MU71KWbPNFsKz/wANONt/</latexit>3
<latexit sha1_base64="6nQzhJC9Npz6VYjvX2LtnvEx2Vs=">AAACtXicjVHbSiNBEO2M1x3vvu5LYxAUJMxIcAVfFAV9WETBqJAEqelUkiY9PbPdNWIY8gW+6g/sX+3fbE8SxCQ+WNBwOKfqVFVXlCppKQj+lby5+YXFpeUf/srq2vrGpr91b5PMCKyJRCXmMQKLSmqskSSFj6lBiCOFD1HvvNAfntFYmeg76qfYjKGjZVsKIEfdVp82y0ElGAafBeEYlNk4bp62Sn8brURkMWoSCqyth0FKzRwMSaFw4DcyiymIHnSw7qCGGG0zH0464LuOafF2YtzTxIfs54ocYmv7ceQyY6CundYK8iutnlH7uJlLnWaEWowatTPFKeHF2rwlDQpSfQdAGOlm5aILBgS5z/En2gzNUxQTq+QvmZYiaeEUq+iFDAz83YZFikHqYq/8CtUzuh7ArzHDD9X5FvLehexIsge/3Q30waVB7O3PlEz4nRkJ6ls2o0x31XD6hrPg/rASHlWq5dPD8XWX2U+2w/ZYyH6xU3bFbliNCYbslb2xd+/EA687SvRK44ptNhHen/9mutqE</latexit>4<latexit sha1_base64="JeAr3F/YtJbBvAqKr8CYWjYm4cI=">AAACuXicjVHfSxtBEN5cbWvTH2p99GUxBFKQcCfaFvpiaUEfRBRMFJIQ5jaTZM3e3rE7J4Yjf0Ff7T/gf+V/073LISbxoQMLH983883MTpgoacn3Hyveq7XXb96uv6u+//Dx08bm1ue2jVMjsCViFZvrECwqqbFFkhReJwYhChVehZNfuX51i8bKWF/SNMFeBCMth1IAOerisL9Z85t+EXwVBCWosTLO+1uVh+4gFmmEmoQCazuBn1AvA0NSKJxVu6nFBMQERthxUEOEtpcVk8543TEDPoyNe5p4wT6vyCCydhqFLjMCGttlLSdf0jopDb/3MqmTlFCLeaNhqjjFPF+bD6RBQWrqAAgj3axcjMGAIPc51YU2hXmCYmGV7C7VUsQDXGIV3ZGBWbXetUgRSJ3vlZ2gukXXA/gZpvikOt9cbvyWI0l279TdQO8dG8TJl5WSBb+fRoL6L5t5prtqsHzDVdDebwZfm4cXB7Wj/fK+62yH7bIGC9g3dsRO2DlrMcGQ/WH37K/3wwNv7N3MU71KWbPNFsKz/wASNNuC</latexit>5Figure 3: Sample DAGs for a mixture of two DAGs
Then, by definition of C(i)in (10), we have
C(1) ={(2,5,2)}, (55)
C(2) =∅, (56)
C(3) =∅, (57)
C(4) ={(2,5,2),(2,1,5,2),(1,5,1)}, (58)
C(5) =∅. (59)
Subsequently, an example of minimal breaking sets and cycle complexities are given by
B(1) ={2}=⇒ τ1= 1, (60)
B(2) =∅=⇒ τ2= 0, (61)
B(3) =∅=⇒ τ3= 0, (62)
B(4) ={5}=⇒ τ4= 1, (63)
B(5) =∅=⇒ τ5= 0. (64)
This example illustrates that even though the mixture model can contain many cycles, the cyclic
complexity of the nodes can be small.
E Additional experiments
Effect of the number of samples. For the same experimental setting in Section 5, we investigate
the effect of the number of samples on the performance of Algorithm 1. Figure 4a demonstrates that
the algorithm achieves almost perfect precision even with as few as s= 1000 samples under the
parameterization described in Section 5. The recall rates are lower than the precision; however, when
the number of samples is increased to s= 10000 , the gap is closed, and the recall rates also become
closer to perfect.
Varying the true edge weights. Recall that in Section 5, we have considered the case where the
weight of a true edge is constant across the component DAGs it belongs to. However, our theory
and algorithm can handle the general case, in which conditional distributions pℓ(Xi|Xpaℓ(i))and
pℓ′(Xi|Xpaℓ′(i))can be different even if the parent sets paℓ(i) = paℓ′(i)for two component DAGs
GℓandGℓ′. For instance, when considering a mixture of DAGs where (1→2)∈E1,(1→2)/∈E1,
and(1→2)∈E3relations, the edge weight from node 1to node 2can be different in G1andG3.
To investigate the performance of our algorithm in this general setting, we consider the following
parameterization. When randomly generating the weight of a true edge (i→j)∈Et, it has two
options: (i) With probability 0.5, it is constant across the component DAGs it belongs to, (ii) with
probability 0.5, it is different (randomly sampled) for every component DAG it belongs to. Figure 4b
shows that the performance of the algorithm is virtually the same for this setting compared to the
main setting considered in Section 5.
Skeleton (true edges) versus inseparable pairs. In Section 5, we demonstrate the need for
interventions by learning inseparable pairs from observational mixture data and comparing them to
the skeleton of the true edges. To learn the inseparable pairs specified in (5), we perform exhaustive
conditional independent tests as in [ 11], summarized in Algorithm 2. Note that as mentioned in
Section 4.1, this exhaustive search requires O(n2·2n)CI tests, which we perform only for this
experiment setting and omit in our proposed Algorithm 1.
211000 2000 5000 10000
Number of samples0.800.850.900.951.00Precision & recall of true edges
n=5 - precision
n=10 - precision
n=5 - recall
n=10 - recall(a) Varying number of samples for a mixture
ofK= 3DAGs
10 15 20
Number of nodes0.900.920.940.960.981.00Recovery of true edges
precision
recall(b) Recovery rates for varying true edge
weights for a mixture of K= 3DAGs
Figure 4: Additional experiment results for true edge recovery
Algorithm 2 Mixture skeleton learning via observational data [11, Algorithm 1 - Stage 1]
1:Input: Samples from mixture distribution pm
2:Return: Inseparable pairs of the mixture Ei
3:Form complete undirected graph: Ei← {(i−j) :i, j∈V}
4:foralli, j∈Vdo
5: forallS∈V\ {i, j}do
6: ifXi⊥ ⊥Xj|XSthen
7: Remove (i−j)edge:EM←EM\(i−j), and SepSet( i, j)←S.
8: break
9: end if
10: end for
11:end for
NeurIPS Paper Checklist
1.Claims
Question: Do the main claims made in the abstract and introduction accurately reflect the
paper’s contributions and scope?
Answer: [Yes]
Justification: Sections 3 and 4 establish the results claimed in the abstract and introduction.
Guidelines:
•The answer NA means that the abstract and introduction do not include the claims
made in the paper.
•The abstract and/or introduction should clearly state the claims made, including the
contributions made in the paper and important assumptions and limitations. A No or
NA answer to this question will not be perceived well by the reviewers.
•The claims made should match theoretical and experimental results, and reflect how
much the results can be expected to generalize to other settings.
•It is fine to include aspirational goals as motivation as long as it is clear that these goals
are not attained by the paper.
2.Limitations
Question: Does the paper discuss the limitations of the work performed by the authors?
Answer: [Yes]
Justification: The limitations of the work are clarified throughout the paper and discussed in
Section 6.
Guidelines:
•The answer NA means that the paper has no limitation while the answer No means that
the paper has limitations, but those are not discussed in the paper.
22• The authors are encouraged to create a separate "Limitations" section in their paper.
•The paper should point out any strong assumptions and how robust the results are to
violations of these assumptions (e.g., independence assumptions, noiseless settings,
model well-specification, asymptotic approximations only holding locally). The authors
should reflect on how these assumptions might be violated in practice and what the
implications would be.
•The authors should reflect on the scope of the claims made, e.g., if the approach was
only tested on a few datasets or with a few runs. In general, empirical results often
depend on implicit assumptions, which should be articulated.
•The authors should reflect on the factors that influence the performance of the approach.
For example, a facial recognition algorithm may perform poorly when image resolution
is low or images are taken in low lighting. Or a speech-to-text system might not be
used reliably to provide closed captions for online lectures because it fails to handle
technical jargon.
•The authors should discuss the computational efficiency of the proposed algorithms
and how they scale with dataset size.
•If applicable, the authors should discuss possible limitations of their approach to
address problems of privacy and fairness.
•While the authors might fear that complete honesty about limitations might be used by
reviewers as grounds for rejection, a worse outcome might be that reviewers discover
limitations that aren’t acknowledged in the paper. The authors should use their best
judgment and recognize that individual actions in favor of transparency play an impor-
tant role in developing norms that preserve the integrity of the community. Reviewers
will be specifically instructed to not penalize honesty concerning limitations.
3.Theory Assumptions and Proofs
Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?
Answer: [Yes]
Justification: The assumptions for the theoretical results are clearly stated in the theorem
statements, and the proofs of the results are provided in Appendix B and C.
Guidelines:
• The answer NA means that the paper does not include theoretical results.
•All the theorems, formulas, and proofs in the paper should be numbered and cross-
referenced.
•All assumptions should be clearly stated or referenced in the statement of any theorems.
•The proofs can either appear in the main paper or the supplemental material, but if
they appear in the supplemental material, the authors are encouraged to provide a short
proof sketch to provide intuition.
•Inversely, any informal proof provided in the core of the paper should be complemented
by formal proofs provided in appendix or supplemental material.
• Theorems and Lemmas that the proof relies upon should be properly referenced.
4.Experimental Result Reproducibility
Question: Does the paper fully disclose all the information needed to reproduce the main ex-
perimental results of the paper to the extent that it affects the main claims and/or conclusions
of the paper (regardless of whether the code and data are provided or not)?
Answer: [Yes]
Justification: The algorithm details are summarized in Algorithm 1, and described in Sec-
tion 4. The experimental procedure is described in Section 5, and detailed parameterization
is given in Appendix E. The code for reproducing the main experimental results can be
found at https://github.com/bvarici/intervention-mixture-DAG .
Guidelines:
• The answer NA means that the paper does not include experiments.
23•If the paper includes experiments, a No answer to this question will not be perceived
well by the reviewers: Making the paper reproducible is important, regardless of
whether the code and data are provided or not.
•If the contribution is a dataset and/or model, the authors should describe the steps taken
to make their results reproducible or verifiable.
•Depending on the contribution, reproducibility can be accomplished in various ways.
For example, if the contribution is a novel architecture, describing the architecture fully
might suffice, or if the contribution is a specific model and empirical evaluation, it may
be necessary to either make it possible for others to replicate the model with the same
dataset, or provide access to the model. In general. releasing code and data is often
one good way to accomplish this, but reproducibility can also be provided via detailed
instructions for how to replicate the results, access to a hosted model (e.g., in the case
of a large language model), releasing of a model checkpoint, or other means that are
appropriate to the research performed.
•While NeurIPS does not require releasing code, the conference does require all submis-
sions to provide some reasonable avenue for reproducibility, which may depend on the
nature of the contribution. For example
(a)If the contribution is primarily a new algorithm, the paper should make it clear how
to reproduce that algorithm.
(b)If the contribution is primarily a new model architecture, the paper should describe
the architecture clearly and fully.
(c)If the contribution is a new model (e.g., a large language model), then there should
either be a way to access this model for reproducing the results or a way to reproduce
the model (e.g., with an open-source dataset or instructions for how to construct
the dataset).
(d)We recognize that reproducibility may be tricky in some cases, in which case
authors are welcome to describe the particular way they provide for reproducibility.
In the case of closed-source models, it may be that access to the model is limited in
some way (e.g., to registered users), but it should be possible for other researchers
to have some path to reproducing or verifying the results.
5.Open access to data and code
Question: Does the paper provide open access to the data and code, with sufficient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?
Answer: [Yes]
Justification: The code for reproducing the main experimental results can be found at
https://github.com/bvarici/intervention-mixture-DAG .
Guidelines:
• The answer NA means that paper does not include experiments requiring code.
•Please see the NeurIPS code and data submission guidelines ( https://nips.cc/
public/guides/CodeSubmissionPolicy ) for more details.
•While we encourage the release of code and data, we understand that this might not be
possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
including code, unless this is central to the contribution (e.g., for a new open-source
benchmark).
•The instructions should contain the exact command and environment needed to run to
reproduce the results. See the NeurIPS code and data submission guidelines ( https:
//nips.cc/public/guides/CodeSubmissionPolicy ) for more details.
•The authors should provide instructions on data access and preparation, including how
to access the raw data, preprocessed data, intermediate data, and generated data, etc.
•The authors should provide scripts to reproduce all experimental results for the new
proposed method and baselines. If only a subset of experiments are reproducible, they
should state which ones are omitted from the script and why.
•At submission time, to preserve anonymity, the authors should release anonymized
versions (if applicable).
24•Providing as much information as possible in supplemental material (appended to the
paper) is recommended, but including URLs to data and code is permitted.
6.Experimental Setting/Details
Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?
Answer: [Yes]
Justification: The experimental procedure is described in Section 5, and detailed parameteri-
zation is provided in Appendix E.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The experimental setting should be presented in the core of the paper to a level of detail
that is necessary to appreciate the results and make sense of them.
•The full details can be provided either with the code, in appendix, or as supplemental
material.
7.Experiment Statistical Significance
Question: Does the paper report error bars suitably and correctly defined or other appropriate
information about the statistical significance of the experiments?
Answer: [Yes]
Justification: The plots in all figures are given for an average of 100 experiments.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The authors should answer "Yes" if the results are accompanied by error bars, confi-
dence intervals, or statistical significance tests, at least for the experiments that support
the main claims of the paper.
•The factors of variability that the error bars are capturing should be clearly stated (for
example, train/test split, initialization, random drawing of some parameter, or overall
run with given experimental conditions).
•The method for calculating the error bars should be explained (closed form formula,
call to a library function, bootstrap, etc.)
• The assumptions made should be given (e.g., Normally distributed errors).
•It should be clear whether the error bar is the standard deviation or the standard error
of the mean.
•It is OK to report 1-sigma error bars, but one should state it. The authors should
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
of Normality of errors is not verified.
•For asymmetric distributions, the authors should be careful not to show in tables or
figures symmetric error bars that would yield results that are out of range (e.g. negative
error rates).
•If error bars are reported in tables or plots, The authors should explain in the text how
they were calculated and reference the corresponding figures or tables in the text.
8.Experiments Compute Resources
Question: For each experiment, does the paper provide sufficient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?
Answer: [Yes]
Justification: Experiments are run on a single commercial CPU.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The paper should indicate the type of compute workers CPU or GPU, internal cluster,
or cloud provider, including relevant memory and storage.
25•The paper should provide the amount of compute required for each of the individual
experimental runs as well as estimate the total compute.
•The paper should disclose whether the full research project required more compute
than the experiments reported in the paper (e.g., preliminary or failed experiments that
didn’t make it into the paper).
9.Code Of Ethics
Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines ?
Answer: [Yes]
Justification: The authors have reviewed the NeurIPS Code of Ethics and confirm that the
research in this paper conforms with the code of ethics.
Guidelines:
•The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
•If the authors answer No, they should explain the special circumstances that require a
deviation from the Code of Ethics.
•The authors should make sure to preserve anonymity (e.g., if there is a special consid-
eration due to laws or regulations in their jurisdiction).
10.Broader Impacts
Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?
Answer: [NA]
Justification: This paper is mostly theoretical and does not pose potential negative societal
impacts.
Guidelines:
• The answer NA means that there is no societal impact of the work performed.
•If the authors answer NA or No, they should explain why their work has no societal
impact or why the paper does not address societal impact.
•Examples of negative societal impacts include potential malicious or unintended uses
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
(e.g., deployment of technologies that could make decisions that unfairly impact specific
groups), privacy considerations, and security considerations.
•The conference expects that many papers will be foundational research and not tied
to particular applications, let alone deployments. However, if there is a direct path to
any negative applications, the authors should point it out. For example, it is legitimate
to point out that an improvement in the quality of generative models could be used to
generate deepfakes for disinformation. On the other hand, it is not needed to point out
that a generic algorithm for optimizing neural networks could enable people to train
models that generate Deepfakes faster.
•The authors should consider possible harms that could arise when the technology is
being used as intended and functioning correctly, harms that could arise when the
technology is being used as intended but gives incorrect results, and harms following
from (intentional or unintentional) misuse of the technology.
•If there are negative societal impacts, the authors could also discuss possible mitigation
strategies (e.g., gated release of models, providing defenses in addition to attacks,
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
feedback over time, improving the efficiency and accessibility of ML).
11.Safeguards
Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?
Answer: [NA]
Justification: This paper is mostly theoretical and does not pose such risks.
26Guidelines:
• The answer NA means that the paper poses no such risks.
•Released models that have a high risk for misuse or dual-use should be released with
necessary safeguards to allow for controlled use of the model, for example by requiring
that users adhere to usage guidelines or restrictions to access the model or implementing
safety filters.
•Datasets that have been scraped from the Internet could pose safety risks. The authors
should describe how they avoided releasing unsafe images.
•We recognize that providing effective safeguards is challenging, and many papers do
not require this, but we encourage authors to take this into account and make a best
faith effort.
12.Licenses for existing assets
Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?
Answer: [Yes]
Justification: Portions of the publicly available code of [ 11] is adopted in the code of our
experiments.
Guidelines:
• The answer NA means that the paper does not use existing assets.
• The authors should cite the original paper that produced the code package or dataset.
•The authors should state which version of the asset is used and, if possible, include a
URL.
• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
•For scraped data from a particular source (e.g., website), the copyright and terms of
service of that source should be provided.
•If assets are released, the license, copyright information, and terms of use in the package
should be provided. For popular datasets, paperswithcode.com/datasets has
curated licenses for some datasets. Their licensing guide can help determine the license
of a dataset.
•For existing datasets that are re-packaged, both the original license and the license of
the derived asset (if it has changed) should be provided.
•If this information is not available online, the authors are encouraged to reach out to
the asset’s creators.
13.New Assets
Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?
Answer: [Yes]
Justification: The codebase for the experiments can be found at https://github.com/
bvarici/intervention-mixture-DAG , and is released under Apache 2.0 license.
Guidelines:
• The answer NA means that the paper does not release new assets.
•Researchers should communicate the details of the dataset/code/model as part of their
submissions via structured templates. This includes details about training, license,
limitations, etc.
•The paper should discuss whether and how consent was obtained from people whose
asset is used.
•At submission time, remember to anonymize your assets (if applicable). You can either
create an anonymized URL or include an anonymized zip file.
14.Crowdsourcing and Research with Human Subjects
Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as
well as details about compensation (if any)?
27Answer: [NA]
Justification: [NA]
Guidelines:
•The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
•Including this information in the supplemental material is fine, but if the main contribu-
tion of the paper involves human subjects, then as much detail as possible should be
included in the main paper.
•According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
or other labor should be paid at least the minimum wage in the country of the data
collector.
15.Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects
Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?
Answer: [NA]
Justification: [NA]
Guidelines:
•The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
•Depending on the country in which research is conducted, IRB approval (or equivalent)
may be required for any human subjects research. If you obtained IRB approval, you
should clearly state this in the paper.
•We recognize that the procedures for this may vary significantly between institutions
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
guidelines for their institution.
•For initial submissions, do not include any information that would break anonymity (if
applicable), such as the institution conducting the review.
28