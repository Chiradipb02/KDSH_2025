Published in Transactions on Machine Learning Research (04/2024)
Navigating Noise: A Study of How Noise Influences
Generalisation and Calibration of Neural Networks
Martin Ferianc∗martin.ferianc.19@ucl.ac.uk
Department of Electronic and Electrical Engineering
University College London
Ondrej Bohdal∗ondrej.bohdal@ed.ac.uk
School of Informatics
University of Edinburgh
Timothy Hospedales t.hospedales@ed.ac.uk
School of Informatics
University of Edinburgh
Samsung AI Center Cambridge
Miguel Rodrigues m.rodrigues@ucl.ac.uk
Department of Electronic and Electrical Engineering
University College London
Reviewed on OpenReview: https://openreview.net/forum?id=zn3fB4VVF0
Abstract
Enhancing the generalisation abilities of neural networks (NNs) through integrating noise
such as MixUp or Dropout during training has emerged as a powerful and adaptable tech-
nique. Despite the proven efficacy of noise in NN training, there is no consensus regarding
which noise sources, types and placements yield maximal benefits in generalisation and con-
fidence calibration. This study thoroughly explores diverse noise modalities to evaluate their
impacts on NN’s generalisation and calibration under in-distribution or out-of-distribution
settings, paired with experiments investigating the metric landscapes of the learnt repre-
sentations across a spectrum of NN architectures, tasks, and datasets. Our study shows
that AugMix and weak augmentation exhibit cross-task effectiveness in computer vision,
emphasising the need to tailor noise to specific domains. Our findings emphasise the ef-
ficacy of combining noises and successful hyperparameter transfer within a single domain
but the difficulties in transferring the benefits to other domains. Furthermore, the study
underscores the complexity of simultaneously optimising for both generalisation and cali-
bration, emphasising the need for practitioners to carefully consider noise combinations and
hyperparameter tuning for optimal performance in specific tasks and datasets.
1 Introduction
Neuralnetworks(NNs)havedemonstratedremarkablecapabilitiesacrossvarioustasks,yettheyoftengrapple
with overfitting to training data, resulting in suboptimal generalisation performance on unseen samples (Sri-
vastava et al., 2014; Bishop, 1995; Sietsma & Dow, 1991). Addressing this issue, conventional techniques
such as weight decay (Krogh & Hertz, 1991) and early stopping (Prechelt, 2002) have been employed to
regularise NN training. Alongside these methods, the introduction of noise during the NN’s training has
∗Joint first authors.
1Published in Transactions on Machine Learning Research (04/2024)
emerged as a potent strategy to enhance generalisation (Sietsma & Dow, 1991; Neelakantan et al., 2017;
Camuto, 2021; Kukačka et al., 2017). The concept of noise injections refers to the deliberate introduction of
artificial perturbations into different aspects of NN training. Note that this is distinct from the concept of
noise in the data itself which originates from the data collection process (Song et al., 2022). Diverging from
weight decay and early stopping that modulate the model’s search within the hypothesis space, noise injec-
tions embrace randomness during training, fostering exploration of a broader array of representations (He
et al., 2019). The appeal of noise injections extends further due to their versatile applicability across diverse
tasks, datasets, and NN architectures. These attributes establish noise injections as a convenient approach
for enhancing NN’s generalisation.
In addition to generalisation, confidence calibration is a desirable model property, especially in safety-critical
applications where confidence scores must be aligned with the model’s accuracy to make informed deci-
sions (Guo et al., 2017). Empirically, noise injections have been shown to improve confidence calibration
by improving the generalisation of the NNs in previously unseen circumstances and inherently reducing
overconfidence in predictions (Guo et al., 2017; Müller et al., 2019; Hendrycks et al., 2020; Zhang et al.,
2018; Gal & Ghahramani, 2016). However, the relationship between generalisation and calibration is not
straightforward, and the two properties are often at odds with each other (Guo et al., 2017).
Various noise injection methodologies have been proposed, encompassing activation techniques such as
Dropout (Srivastava et al., 2014; Gal & Ghahramani, 2016) and Gaussian Dropout (Kingma et al., 2015),
weightnoises such as DropConnect (Wan et al., 2013) or additive Gaussian noise (Blundell et al., 2015),
targetmethods such as label smoothing (Szegedy et al., 2016), input-target strategies exemplified by
MixUp (Zhang et al., 2018), inputmodifications such as AugMix (Hendrycks et al., 2020) or the standard
horizontal flipping and center cropping (Krizhevsky et al., 2009), modelapproaches including weight per-
turbation (Ash & Adams, 2020), and gradient perturbations involving Gaussian noise (Neelakantan et al.,
2017). Despite the diversity of these techniques, comprehensive and fair comparisons are scarce, leaving a
gap in understanding which approach is helpful for specific datasets, tasks and models in conjunction with
generalisation and calibration.
This study aims to systematically and comprehensively investigate the effects of widely used noise injection
methods on NN generalisation and calibration across multiple datasets, tasks, and architectures. This explo-
ration is predicated on the premise that while generalisation focuses on reducing overfitting and improving
the model’s predictive accuracy, calibration deals with aligning the model’s confidence with its actual perfor-
mance. Rather than focusing on improving state-of-the-art performance, we aim to provide a holistic view
of the effects of noise injections on NNs’ generalisation and calibration for the benefit of practitioners. To
this end, we present the following contributions:
1. The first systematic empirical investigation into the impact of noise injections on NN generalisation and
calibration across diverse datasets, tasks and NN architectures. Our exploration extends to evaluation
under in-distribution (ID) and out-of-distribution (OOD) scenarios and their transferability across archi-
tectures and datasets.
2. A methodological framework for simultaneously combining various noise injection approaches.
3. Visualisation of the learnt representation landscape across noises, jointly comparing calibration and gen-
eralisation performance.
Our investigation reveals that certain types of noise aid in generalisation by introducing robustness against
overfitting and variability in data and potentially improve calibration by mitigating overconfidence in predic-
tions. The findings show that AugMix, weak augmentation and Dropout prove effective across diverse tasks,
emphasising their versatility. Task-specific nuances in noise effectiveness, such as AugMix’s superiority in
computer vision (CV), Dropout in natural language processing (NLP) and Gaussian noise in tabular data
regression, highlight the need for tailored approaches. Combining noises, careful hyperparameter tuning,
and task-specific considerations are crucial for optimising NN’s performance. Our code is publicly available
athttps://github.com/martinferianc/noise .
2Published in Transactions on Machine Learning Research (04/2024)
2 Related Work
In this study, we consider artificial addition of noise into various facets of NN training – including input,
target,input-target ,activations ,weights,gradients , and modelparameters. The noise application
is denoted by α<place> (·,δ), whereα<place>is the noise application methodology which can be executed
at different places, e.g. αinputfor input noise, αtargetfor target noise along with ·arbitrary arguments,
depending on the noise injection methodology. For example, under this definition, additive input Gaussian
noise samples a Gaussian and adds it to the input xasαinput(x,δ) =x+ϵ;ϵ∼N (0,σ2), whereσ2is
the hyperparameter in δ. Note that this study focuses on artificial noise injections, which are purposely
introduced during training, and not on naturalnoise, inherent in the data, e.g. label noise in the targets
where the classifying label is incorrect. The natural noise needs to be addressed separately and we refer the
reader to Song et al. (2022) for a review of strategies for learning with noisy labels. Under different noise
placements, we review several noise injection strategies. The review focused on the most fundamental noise
injection methodologies, which constitute the building blocks of more complex approaches and represent the
noise injection category.
Input Noise : Pioneering work by Sietsma & Dow (1991) demonstrated the benefits of training with added
input Gaussian noise, while Bishop (1995) established its linkage to regularisation in the least squares
problems. In CV, weak augmentation, such as random cropping and horizontal flipping, has improved gen-
eralisation (Krizhevsky et al., 2009). AugMix, domain-specific to CV, applies a sequence of image processing
operations to the input, bolstering robustness in OOD settings. From the adversarial robustness domain,
ODS augments inputs conditioned on the prediction, aiming to diversify the inputs (Tashiro et al., 2020).
Target Noise : Label smoothing (Pereyra et al., 2017) softens the one-hot classification targets by replac-
ing the targets with a categorical distribution with the most mass on the correct class and the rest spread
across the other classes through the addition of constant uniform noise, effectively improving NN’s robust-
ness (Müller et al., 2019). This differs from label noise, where the entire probability mass is on an incorrect
class, and the NN must learn to ignore the errors (Song et al., 2022). Input-Target Noise : Variants of
MixUp have exhibited efficacy in augmenting both generalisation and calibration (Zhang et al., 2018; Müller
et al., 2019; Guo et al., 2019; Yao et al., 2022; Guo et al., 2017). MixUp adds noise to both the input and the
target via linear interpolation between two samples and their targets, while CMixUp expands this approach
to regression problems. Activation Noise: Widespread activation noise includes Dropout or Gaussian noise
injections. Dropout (Srivastava et al., 2014; Noh et al., 2017) randomly deactivates activations through 0-1
noise, while Gaussian noise injections add noise to activations (Kingma et al., 2015; DeVries & Taylor, 2017).
Bayesian NNs (Gal & Ghahramani, 2016) incorporate these injections during training and evaluation, in
contrast to our work’s focus solely on their application in training. Weight Noise: Unlike Dropout, Drop-
Connect (Wan et al., 2013) randomly deactivates weights or connections between neurons, while Gaussian
noise injections add noise to weights (Blundell et al., 2015). Note that we do not model the variance of
the Gaussian noise through learnable parameters, as in (Blundell et al., 2015), but rather fix it through a
searchable hyperparameter. We do this to ensure a fair comparison with other noise injection approaches,
such as Dropout, which do not have learnable parameters and would require changing the model architecture
to accommodate them. Gradient Noise: Annealed Gaussian noise added to gradients during training has
demonstrated its efficacy in improving NN generalisation Neelakantan et al. (2017); Welling & Teh (2011);
Zhou et al. (2019); Chaudhari & Soatto (2015); Wu et al. (2020). Model Noise: A recent contribution,
Gaussian noise injection through periodic weight shrinking and perturbation Ash & Adams (2020), improves
retraining generalisation.
In previous work, the impact of noise per injection type was studied. Poole et al. (2014) show that inject-
ing noise at different layers of autoencoders implements various regularisation techniques and can improve
feature learning and classification performance. Cohen et al. (2019) show that smoothing classifiers with
Gaussian noise naturally induces robustness in the L2 norm. Wei et al. (2020) disentangle and analyti-
cally characterise the explicit regularisation effect from modifying the expected training objective and the
implicit regularisation effect from the stochasticity of Dropout noise in NNs. Camuto (2021); Camuto et al.
(2020) show that training NNs with Gaussian noise injections on inputs and activations regularises them
to learn lower frequency functions, improves generalisation and calibration on unseen data but also confers
robustness to perturbation. On one hand, Jang et al. (2021) show that training NNs on noisy images can
3Published in Transactions on Machine Learning Research (04/2024)
improve their robustness and match human behavioural and neural responses. On the other hand, Geirhos
et al. (2018) demonstrate that adding specific noise to the input can surpass humans in generalisation on
that specific noise, but not to other types of noise, while human vision is robust to a wide range of noise
types. The results of Geirhos et al. (2018) are confirmed by the results of Kang et al. (2019), who show that
robustness against one type of noise does not necessarily transfer to robustness against other types of noise.
Furthermore, Kang et al. (2019) consider adversarial training, where a model is trained to be robust against
noise-based adversarial attacks (Goodfellow et al., 2015). An adversarial attack is a specific type of noise
injection during evaluation, where the noise is designed to fool the model. In comparison, our work focuses
on the generalisation and confidence calibration performance of NNs with respect to domain shift in the
data distribution, rather than adversarial attacks. We consider enhancing robustness to adversarial attacks
through artificial noise injections as future work. Moreover, (Kukačka et al., 2017) provided a taxonomy of
regularisation in NNs, covering multiple noise-based approaches.
The closest work to ours is (Chun et al., 2020), which considered regularisation commonly used during
traininganditsimpactongeneralisation,confidencecalibrationandout-of-distributiondetectionincomputer
vision. While their focus was not noise-specific, as in our work, they overlap with our work by considering
input noise: weak augmentation and Gaussian noise, target noise: label smoothing (Müller et al., 2019),
input-target noise: MixUp (Zhang et al., 2018). They show that common regularisation techniques improve
generalisation, confidence calibration and out-of-distribution detection. In comparison to (Chun et al., 2020),
our work focuses on a broader set of noise injections, network architectures, datasets and tasks, evaluation of
the weight landscapes, and in-depth noise combinations paired with comprehensive hyperparameter search.
Past work has studied noise injection techniques in isolation, mainly focused on generalisation alone, lacked
comprehensive hyperparameter optimisation, and rarely evaluated the robustness of distribution shift. For
example, only MixUp, AugMix and label smoothing have been studied in calibration (Guo et al., 2017;
Müller et al., 2019; Guo et al., 2019; Yao et al., 2022; Chun et al., 2020). An exception to this is Chun et al.
(2020), who studied generalisation, calibration and out-of-distribution detection for some noise injections.
While promising, these methods require further unified analysis to determine their relationships across
datasets, tasks, architectures and across a broader set of noise injections. Our work addresses these gaps
by1.)studying the impact across datasets, tasks and architectures; 2.)benchmarking the impact of noise
injections’ hyperparameters on transferability between datasets and architectures; 3.)studying confidence-
calibration in addition to generalisation; 4.)performing a comprehensive hyperparameter search with fair
comparisons; 5.)evaluating robustness to distribution shift; 6.)providing a methodological framework for
combining and tuning various noise injection approaches across categories; and lastly 7.)visualising the
learnt representation or learning landscape across noise injections in 1D or 2D (Goodfellow et al., 2014; Li
et al., 2018) across both generalisation and calibration.
3 Methodology
We establish a structured methodology to investigate noise injections’ effects on NNs. The noise types are
divided into input,input-target ,target,activation ,weight,gradient andmodel, and we enable their
conditional deployment through probabilities {pi
noise}S
i=1in the range 0≤pi
noise≤1, whereSdenotes the
number of noises.
The training allows simultaneous consideration of Snoise types, each associated with specific hyperpa-
rameters{δi}S
i=1and an application function {αi
<place> (·,δ)}S
i=1, whereαi
<place>is the noise application
methodology which can be executed at different places, e.g. αinputfor input noise, αtargetfor target noise
along with·arbitrary arguments, depending on the noise injection methodology. The different noise types
implement only the relevant αi
<place> (·,δ)function, while others are ignored. We encourage the reader to
refer to the code for the implementation details for each noise type. The probabilities {pi
noise}S
i=1allow us to
tune the frequency of applying each noise type, while the hyperparameters {δi}S
i=1enable us to adjust the
magnitude of each noise type. This enables us to tune both the magnitude and frequency of noise injections,
unlike, for example, Dropout (Srivastava et al., 2014), which only allows the tuning of the magnitude, and
it is applied every batch. The tuning of the frequency allows us to avoid conflicts between noises, as it can
be set to 0 if the noise is conflicting with other noises.
4Published in Transactions on Machine Learning Research (04/2024)
Algorithm 1 provides a comprehensive overview of the training process, executed throughout Eepochs with
Lbatches processed per epoch. For every batch, input and target data (xb,yb)are randomly drawn from the
training datasetD={(xb,yb)}L
b=1. For each noise in S, we sample a uniform random variable ϵ∼U(0,1),
and ifϵ<pi
noise, we enable noise iwith hyperparameters δifor the current batch b.
Algorithm 1 Training of a Neural Network with Noise
Require: Training dataset D={(xb,yb)}L
b=1,Lbatches, number
of epochsE, network depth D, weightsW={Wd}D
d=1, hid-
den stateszb={zd
b}D
d=1, activations ϕ={ϕd(·)}D
d=1, weighted
operations f={fd(·,Wd)}D
d=1,Snoise types, probabilities of
applying noise to a batch pnoise ={pi
noise}S
i=1, Noise hyperpa-
rametersδ={δi}S
i=1, Noise application functions α<place> =
{αi
<place> (·,δ)}S
i=1.
1:InitialiseWrandomly
2:fore= 1toEdo
3:forb= 1toLdo
4:Randomly select a batch (xb,yb)fromD
5:Sampleϵ={ϵi∼U(0,1)}S
i=1
6:Set toggles t={ti=ϵi<pi
noise}S
i=1
7: Input noise : apply_αinput(xb,t,δ)
8: Target noise : apply_αtarget(yb,t,δ)
9: Input-target noise : apply_αinput-target (xb,yb,t,δ)
10:z0
b=xb
11: ford= 1toDdo
12: Weight noise : apply_αweight (Wd,t,δ)
13: Compute hidden state zd
b=fd(zd−1
b,Wd)
14: Activation noise : apply_αactivation (zd
b,t,δ)ifd<D
15:zd
b=ϕd(zd
b)
16: end for
17:Assign predictions ˆyb=zd
b
18:Compute lossL(ˆyi,yi)and gradients∇WL
19: Gradient noise : apply_αgradient (∇WL,t,δ)
20:Update weights W
21: end for
22: Model noise : apply_αmodel(W,e,t,δ )
23:end for
24:Procedure: apply_α<place> (·,t,δ)
25:fori= 1toSdo
26: iftiandαi
<place>exists then
27:αi
<place> (·,δi)
28: end if
29:end forFor each noise in S, we sample a uni-
form random variable ϵ∼U(0,1), and
ifϵ<pi
noise, we enable noise ithrough
setting the toggle tito 1 for the current
batchb. Theenablednoisesareapplied
in the order: 1.)input, target, input-
target,2.)weights,3.)activations, 4.)
gradients and 5.)model through the
apply_α<place> (·,t,δ)procedure. The
proceduresequentiallyiteratesoverthe
noise types in Sand applies the noise
if the noise is enabled and the applica-
tion function αi
<place>exists. The user
specifies the order of the noises in S.
Our approach accounts for networks of
depthD, denoted by{fd(·,Wd)}D
d=1,
involving weights together with bi-
asesW={Wd}D
d=1and activations
{ϕd(·)}D
d=1to produce hidden states
{zd
b}D
d=1.zb
0corresponds to the input
xb, whilezD
brepresents the output pre-
diction ˆyb.
Forinputnoise, we explore Aug-
Mix, ODS, weak augmentation: ran-
dom cropping and horizontal flipping,
and additive Gaussian noise injec-
tions (Hendrycks et al., 2020; Tashiro
etal.,2020;Sietsma&Dow,1991). For
input-target we explore MixUp and
CMixUp(Zhangetal.,2018;Yaoetal.,
2022). For targetnoise, we consider
label smoothing, and the target noise
also inherently involves MixUp and
CMixUp(Zhangetal.,2018;Yaoetal.,
2022; Müller et al., 2019). The activa-
tionnoise examines Dropout and ad-
ditive Gaussian noise (Srivastava et al.,
2014; Kingma et al., 2015) prior to ac-
tivations for all linear or convolutional
layers, except the last layer. For weightnoise, we consider Gaussian noise added to the weights (Blundell
et al., 2015) or DropConnect (Wan et al., 2013) for all linear or convolutional layers, except the last layer.
We consider gradient Gaussian noise added to all gradients of the loss function (Neelakantan et al., 2017).
After the update of the weights, the modelnoise is applied to the weights, for which we consider shrink-
ing the weights and adding Gaussian noise (Ash & Adams, 2020), but not in the last 25% of the training
epochs. Out of these noises, label smoothing, MixUp and ODS are exclusive to classification, and CMixUp
is applicable only in regression. AugMix and weak augmentation are exclusive to the CV data. The other
noises are broadly applicable across tasks.
5Published in Transactions on Machine Learning Research (04/2024)
4 Experiments
Next, in Section 4.1 we present the concrete datasets, tasks and architectures used in our experiments,
followed by experiments on ID data in Section 4.2, OOD data in Section 4.3, combined noises in Section 4.4,
transferability in Section 4.5 and lastly the metric landscape visualisations in Section 4.6.
4.1 Experimental Settings
Tasks, Architectures and Datasets: We consider various setups, including computer vision (CV) clas-
sification and regression, tabular data classification and regression, and natural language processing (NLP)
classification. For CV classification we include datasets such as CIFAR-10, CIFAR-100 (Krizhevsky et al.,
2009), SVHN (Netzer et al., 2011), and TinyImageNet (Le & Yang, 2015), along with neural architectures
such as a fully-connected (FC) net and ResNet (He et al., 2016). For CV regression, we introduce a rotated
versionofCIFAR-100topredicttherotationangle, andwealsousetheWikiFacedataset(Rotheetal.,2015),
where the aim is to predict the age based on the image of the face. We use the ResNet model in both cases.
We deem the rotation prediction task compelling to evaluate since it is a common task in the literature for
self-supervised pre-training (Gidaris et al., 2018). In the realm of tabular data classification and regression,
we use an FC network and evaluate noises on diverse datasets, including Wine, Toxicity, Abalone, Students,
Adult for classification and Concrete, Energy, Boston, Wine, Yacht for regression (Asuncion & Newman,
2007). We explore NLP classification using the NewsGroup and SST-2 datasets (Lang, 1995; Socher et al.,
2013) paired with global pooling convolutional NN (Kim, 2014) and a transformer (Vaswani et al., 2017).
The Appendix details the datasets and architectures and gives the complete numerical results.
Metrics: To assess the effectiveness of the noise injection methods in classification, we measure their
performance using three metrics: Error ( ↓,%), Expected Calibration Error (ECE) (Guo et al., 2017) ( ↓,%)
with10binsandthecategoricalNegativeLog-Likelihood(NLL)( ↓). Forregression, weusetheMeanSquared
Error (MSE) (↓) and the Gaussian NLL ( ↓). We test the generalisation of the models by evaluating their
performance on the ID test set. For CV classification and regression, we test the robustness of the models
by assessing their performance on an OOD test set by applying corruptions (Hendrycks & Dietterich, 2019)
to the ID test set. These corruptions include, for example, adding snow or fog to the image, changing the
brightness or saturation of the image or blurring the image across 5 intensities. We created the OOD test
set for tabular data by adding or multiplying the inputs with Gaussian or Uniform noise or by zeroing some
of the input features with Bernoulli noise, similarly across 5 intensities. While vision data has ImageNet-
C (Hendrycks & Dietterich, 2019), to the best of our knowledge, there is no similar benchmark for tabular
data. Our methodology for introducing perturbations and zeroing out features is designed to simulate a wide
range of potential distribution shifts in real-world scenarios, such as instrumentation errors, missing data,
and adversarial attacks. We crafted the OOD evaluation to be similar to (Hendrycks & Dietterich, 2019),
in terms of the magnitude and severity of the noise, allowing us to systematically evaluate the robustness of
the models. To summarise the results, we collect the results for each approach for each dataset and metric
and rank them relative to the no noise baseline. For example, -1 means that the approach is one rank better
than the no noise baseline, and 1 means that the approach is one rank worse than the no noise baseline. We
then average the ranks across the datasets for each task and metric.
Hyperparameter Optimisation: We first tune the learning rate and L2 regularisation of a no noise
network, which are reused when tuning the HPs of each noise injection method. By tuning the learning
rate and L2 regularisation, we wanted to simulate a realistic scenario where the practitioner seeks to add
noise to their existing model and does not want to jointly tune the model’s hyperparameters and the noise
injection method. The tuning was performed with 1 seed, and the winning hyperparameters were retrained
3 times with different seeds. 10% of the training data was used as the validation set to select the best model,
with validation NLL used as the selection objective to combine both generalisation and calibration. The
tuning is performed using model-based Tree-structured Parzen Estimator method (Bergstra et al., 2011) with
successive halving pruning strategy (Jamieson & Talwalkar, 2016). We evaluate 50 trials for each setting,
which allows us to manage the trade-off between compute costs and a reasonable number of trials.
6Published in Transactions on Machine Learning Research (04/2024)
Input Weak Aug.Input GaussianInput ODS
Input AugMix
Input-T arget MixUpLabel Smoothing
Activation GaussianActivation Dropout Gradient GaussianModel
Weight Gaussian
Weight DropConnectT op-2 Direct T op-3 Direct
T op-2 Optimised T op-3 OptimisedError
ECE
NLL-8.0 -0.5 0.2 -8.5 -2.0 -2.2 -1.5 -6.5 3.0 -5.0 -1.2 -1.2 -11.5 -10.2 -11.2 -10.0
-6.0 -1.0 1.0 -7.5 1.0 3.2 -0.8 -3.5 2.8 -0.2 2.8 0.0 -5.8 -3.8 -6.0 -6.0
-7.8 -1.2 -0.5 -7.5 -2.0 1.2 -1.2 -6.2 2.8 -5.0 -0.2 -1.8 -11.8 -10.2 -11.2 -9.5
10
010
(a) CV classification.
Input Weak Aug.Input GaussianInput AugMix
Input-T arget CMixUp Activation GaussianActivation Dropout Gradient GaussianModel
Weight Gaussian
Weight DropConnectT op-2 Direct T op-3 Direct
T op-2 Optimised T op-3 OptimisedMSE
NLL1.5 8.5 0.5 5.5 2.5 4.0 6.5 10.0 3.0 9.0 1.0 -0.5 1.5 7.0
-2.0 1.0 -3.5 0.0 1.0 -1.0 5.0 -2.0 1.0 4.0 -1.0 -0.5 -3.5 1.5
10
010 (b) CV regression.
Input GaussianInput ODS
Input-T arget MixUpLabel Smoothing
Activation GaussianActivation Dropout Gradient GaussianModel
Weight Gaussian
Weight DropConnectT op-2 Direct T op-3 Direct
T op-2 Optimised T op-3 OptimisedError
ECE
NLL0.8 -0.6 3.5 -0.3 -1.1 3.5 2.0 0.1 -1.6 0.5 -3.9 -0.9 0.4 2.1
0.4 -1.8 -5.2 -4.6 -0.8 -6.6 2.2 -6.2 -0.6 -1.2 -7.8 -9.0 -6.2 -6.6
-0.6 -4.0 -3.6 -5.2 1.0 -4.4 2.6 -6.2 0.2 -1.8 -7.2 -4.4 -6.0 -2.4
10
010
(c) Tabular classification.
Input Gaussian
Input-T arget CMixUp Activation GaussianActivation Dropout Gradient GaussianModel
Weight Gaussian
Weight DropConnectT op-2 Direct T op-3 Direct
T op-2 Optimised T op-3 OptimisedMSE
NLL-2.4 4.2 -2.0 4.0 -0.8 2.2 -2.4 1.2 1.2 2.0 -0.4 3.6
-1.6 -1.2 -2.8 -0.8 -1.6 -1.0 -5.0 -3.8 -1.8 -1.6 -3.0 -4.4
10
010 (d) Tabular regression.
Input GaussianInput ODS
Input-T arget MixUpLabel Smoothing
Activation GaussianActivation Dropout Gradient GaussianModel
Weight Gaussian
Weight DropConnectT op-2 Direct T op-3 Direct
T op-2 Optimised T op-3 OptimisedError
ECE
NLL0.2 -5.8 -1.9 0.2 -1.1 -1.6 2.6 1.5 1.1 -1.1 -3.6 -5.7 -0.5 -5.2
0.2 0.0 -3.2 -2.5 -0.2 -4.0 3.0 -2.0 1.0 1.8 -4.5 -1.0 -5.5 -5.5
-1.8 -3.8 -3.0 -4.8 -2.0 -4.2 3.2 1.0 1.5 -0.5 -5.0 -2.2 -4.8 -7.5
10
010
(e) NLP classification.
Figure 1: In-domain evaluation of the differences in rankings compared to not using any noise.
4.2 In-Domain Evaluation
In Figure 1, we show the in-domain (ID) performance of NNs trained with various noise injection methods
across CV classification and regression, tabular data classification and regression, and NLP classification.
Overall, we observe that the noise injection methods significantly improve the generalisation and calibration
in many cases, but different noise types are needed for various tasks. In CV classification, almost all noises
improve the error rate, with many simultaneously improving calibration. The most beneficial noises are
AugMix, weak augmentation and Dropout. MixUp and label smoothing are a surprise to a certain extent as
theyimprovedgeneralisationbutnotcalibration. InCVregression, improvinggeneralisationwaschallenging,
with no improvement. However, several noises have improved NLL, with AugMix, weak augmentation, and
Dropout achieving the best balance. These results suggest that image augmentation broadly benefits CV,
confirming expectations.
Several noises have improved the error rate to a lesser extent or kept it at a similar level in tabular data clas-
sification. In contrast, almost all noises have improved ECE and NLL. The improvements were particularly
impactful in several cases, with model noise, label smoothing, and Dropout being the best. While ODS is
designed to improve adversarial robustness, it improved ECE and NLL while slightly improving error rates.
All noises improve NLL for tabular regression, and some significantly improve MSE. Gaussian noises applied
to the weights, activations, or inputs are the most useful types of noise for improving the two metrics. In
NLP classification, about half of the noises improve error, with some also improving calibration simultane-
ously. The best noises are Dropout, label smoothing and ODS, which differs from what was the best for CV.
These noises significantly lowered error and NLL, while MixUp and model noise were particularly useful for
reducing ECE. ODS was beneficial for improving error and calibration via NLL, which can be a surprise as
this technique was not previously considered for improving generalisation or calibration.
In Figure 2, we show detailed results for selecting representative datasets across the 5 tasks. We see the
improvements in error can be large for CIFAR-10, for example, halving it in some of the best cases –
weak augmentation and AugMix, with Dropout also leading to a few percentage point improvements. The
situation is similar for ECE, where weak augmentation and AugMix make the ECE one-half or one-third.
Many errors are slightly better, but certain noises, such as MixUp, label smoothing, or Gaussian noise added
7Published in Transactions on Machine Learning Research (04/2024)
0246810121416Error (%)
0123456ECE (%)
(a) CIFAR-10 CV classification.
0.000.020.040.060.080.10MSE
0510152025NLL (b) WikiFace CV regression.
0246810121416Error (%)
0.00.51.01.52.02.53.03.5ECE (%)
(c) Adult tabular data classification.
0.00.10.20.30.40.5MSE
2.00
1.75
1.50
1.25
1.00
0.75
0.50
0.25
0.00NLL (d) Yacht tabular data regression.
0510152025303540Error (%)
0510152025ECE (%)
(e) NewsGroup NLP classification.
No Noise Input Weak Aug. Input Gaussian Input ODS Input AugMix Input-Target MixUp Label Smoothing Activation Gaussian Activation Dropout
Gradient Gaussian Model Weight Gaussian Weight DropConnect Top-2 Direct Top-3 Direct Top-2 Optimised Top-3 Optimised
Figure 2: Detailed in-domain performance of NNs trained with various noises across the five tasks.
to the activations, worsen the calibration. For WikiFace, there are more minor improvements in error from
weak augmentation and AugMix with overall similar MSE across different noises. Still, the differences in
calibration as measured using NLL can be considerable, with most noises improving the NLL significantly.
Moving the focus to tabular data, most noises applied to the Adult classification dataset improve the error
marginally. In contrast, many improve ECE significantly, with the best ones being Dropout, model noise and
DropConnect. Most noises have significantly improved MSE for the Yacht regression dataset, but CMixUp
and model noise led to significant increases. The best ones have been gradient Gaussian and Gaussian
noise added to the weights. NLL has been improved in several cases, including gradient Gaussian and
weight Gaussian, demonstrating solid improvements in MSE and NLL. The errors stay similar for NLP
classification on NewsGroup using the global pooling CNN model. ODS leads to the best improvement,
while several noises, specifically Dropout, gradient Gaussian, and model noise, lead to worse generalisation.
ODS and label smoothing have also noticeably improved ECE.
Main Observations: The noises are effective across various tasks and datasets. The shortlist of the most
effective methods is AugMix and weak augmentation in CV, model noise, and Gaussian noise added to
weights for tabular data and dropout in NLP. Different task types benefit from different types of noise.
4.3 Out-of-Domain Evaluation
We evaluate the performance on the ID test set and an augmented OOD set, including an average over
visual corruptions across 19 categories and 5 severities (Hendrycks & Dietterich, 2019). Likewise, we average
the performance across 5 categories and 5 severities for tabular data. The summary of the results is in
Figure 3, with analysis of correlations between ID and OOD rankings via Kendall Tau score in Table 1.
For CV classification, we observe that the generalisation improvements also remain for OOD, but improving
calibration in terms of ECE turns out to be much more challenging. The overall ranking of the best noises
remains similar, with AugMix and weak augmentation remaining the best. MixUp rose to prominence thanks
8Published in Transactions on Machine Learning Research (04/2024)
Input Weak Aug.Input GaussianInput ODS
Input AugMix
Input-T arget MixUpLabel Smoothing
Activation GaussianActivation Dropout Gradient GaussianModel
Weight Gaussian
Weight DropConnectT op-2 Direct T op-3 Direct
T op-2 Optimised T op-3 OptimisedError
ECE
NLL-7.5 -1.2 -1.2 -9.8 -3.5 -3.8 -1.5 -6.5 2.5 -3.8 -0.8 -2.8 -11.8 -11.0 -12.2 -10.2
2.2 1.8 2.2 0.5 -1.5 4.0 1.0 4.0 7.0 1.2 3.0 1.2 0.2 2.2 0.0 0.5
-6.0 -1.2 -0.5 -8.8 -3.0 -1.0 -0.2 -4.2 3.0 -2.8 0.8 -2.8 -10.8 -9.0 -11.2 -10.2
10
010
(a) CV classification.
Input Weak Aug.Input GaussianInput AugMix
Input-T arget CMixUp Activation GaussianActivation Dropout Gradient GaussianModel
Weight Gaussian
Weight DropConnectT op-2 Direct T op-3 Direct
T op-2 Optimised T op-3 OptimisedMSE
NLL1.0 7.5 -3.5 1.5 6.5 7.0 1.5 5.0 6.5 6.0 -1.0 1.5 -2.5 8.0
-4.0 1.0 -8.0 -3.5 -3.0 -7.0 -4.5 -5.5 -4.5 -1.0 -12.0 -12.0 -10.5 -8.0
10
010 (b) CV regression.
Input GaussianInput ODS
Input-T arget MixUpLabel Smoothing
Activation GaussianActivation Dropout Gradient GaussianModel
Weight Gaussian
Weight DropConnectT op-2 Direct T op-3 Direct
T op-2 Optimised T op-3 OptimisedError
ECE
NLL0.4 1.0 2.6 0.4 1.2 4.6 3.6 0.4 1.0 1.2 -1.6 0.4 0.2 2.6
-0.6 -3.2 -5.8 -4.8 -1.0 -8.2 1.4 -7.0 -2.0 -3.8 -9.2 -10.4 -6.6 -7.8
-0.8 -5.4 -5.8 -5.2 0.0 -5.6 1.8 -6.6 -0.8 -3.0 -7.8 -5.6 -6.0 -3.2
10
010
(c) Tabular classification.
Input Gaussian
Input-T arget CMixUp Activation GaussianActivation Dropout Gradient GaussianModel
Weight Gaussian
Weight DropConnectT op-2 Direct T op-3 Direct
T op-2 Optimised T op-3 OptimisedMSE
NLL-2.4 0.4 -1.8 -1.0 -3.0 2.2 -2.2 -1.2 1.6 1.8 -1.6 2.0
-0.8 -1.6 0.0 -4.0 1.8 0.4 -0.2 -4.6 0.0 -2.2 0.4 0.4
10
010 (d) Tabular regression.
Figure 3: OOD evaluation of the differences in rankings compared to not using any noise.
Metric SVHN CIFAR-10 CIFAR-100 TinyImageNet Average
Error 0.912 ±0.000 0.706 ±0.000 0.897 ±0.000 0.912 ±0.000 0.857
ECE 1.000 ±0.000 0.618 ±0.000 0.250 ±0.177 0.118 ±0.542 0.496
NLL 0.971 ±0.000 0.515 ±0.003 0.868 ±0.000 0.882 ±0.000 0.809
(a) CV classification.Metric Rotated CIFAR-100 WikiFace Average
MSE 0.257 ±0.202 0.581 ±0.002 0.419
NLL -0.238 ±0.239 0.810 ±0.000 0.286
(b) CV regression.
Metric Wine Toxicity Abalone Students Adult Average
Error 0.861 ±0.000 0.743 ±0.000 0.529 ±0.006 0.532 ±0.007 0.924 ±0.000 0.718
ECE 0.905 ±0.000 0.905 ±0.000 0.867 ±0.000 0.867 ±0.000 0.962 ±0.000 0.901
NLL 0.924 ±0.000 0.981 ±0.000 0.790 ±0.000 0.810 ±0.000 0.962 ±0.000 0.893
(c) Tabular data classification.Metric Energy Boston Wine Yacht Concrete Average
MSE 0.667 ±0.001 0.923 ±0.000 -0.564 ±0.007 0.641 ±0.002 0.872 ±0.000 0.508
NLL -0.615 ±0.003 0.974 ±0.000 0.154 ±0.510 0.590 ±0.004 0.436 ±0.042 0.308
(d) Tabular data regression.
Table 1: Kendall Tau correlation between ID and OOD rankings of different noise types for various tasks.
to the best OOD calibration and improved errors and NLL. Analysis of Kendall Tau correlation in Table 1a
shows that ID and OOD rankings are strongly correlated for error and NLL, while only moderately for ECE.
CV regression is similar to classification ranking the best noises, with only AugMix leading to improvements
in OOD generalisation. However, calibration is improved by most noises, with AugMix excelling. Only a
minor correlation exists between ID and OOD rankings for MSE and NLL metrics. For tabular classification,
the noises generally improve ECE and NLL but not the error rate under OOD settings, with model noise,
label smoothing, and Dropout being the best. This suggests all of these are among the best noises for
both ID and OOD. ID and OOD rankings show a strong correlation overall. Several noises improve OOD
generalisation and calibration for tabular regression, with DropConnect, Dropout and Gaussian noise added
to the inputs, leading to the best overall improvements. The ID and OOD ranking Kendall Tau correlation
is low in this case. MixUp and CMixUp have improved OOD calibration for both tabular classification and
regression.
We study selected representative datasets regarding OOD performance in Figure 4. OOD results on CIFAR-
10 show that AugMix significantly improves both error and ECE, making ECE one-third of the no noise
equivalent. MixUp leads to similarly considerable improvements in ECE and more minor yet significant
improvements in error. Several noises, e.g., Dropout and Gaussian noise, added to activations or weights lead
to a few percentages worse ECE. On WikiFace, most OOD MSE values are similar, but OOD calibration
in NLL is improved significantly for several noises, including AugMix, weak augmentation or Dropout.
Improvements in generalisation for the Adult tabular classification dataset are minor, but the improvements
in calibration can be significant, for example, Dropout and model noise halving the OOD ECE value. For
the Yacht tabular regression dataset, the improvements in generalisation have been more critical, with the
same being true for calibration measured in terms of OOD NLL.
9Published in Transactions on Machine Learning Research (04/2024)
05101520253035Error (%)
05101520ECE (%)
(a) CIFAR-10 CV classification.
0.000.020.040.060.080.10MSE
051015202530NLL (b) WikiFace CV regression.
0246810121416Error (%)
0.00.51.01.52.02.53.03.54.0ECE (%)
(c) Adult tabular data classification.
0.00.10.20.30.40.5MSE
2
0246810NLL (d) Yacht tabular data regression.
No Noise Input Weak Aug. Input Gaussian Input ODS Input AugMix Input-Target MixUp Label Smoothing Activation Gaussian Activation Dropout
Gradient Gaussian Model Weight Gaussian Weight DropConnect Top-2 Direct Top-3 Direct Top-2 Optimised Top-3 Optimised
Figure 4: Detailed OOD performance of NNs trained with various noises across the four tasks.
WeincludeanadditionalOODinvestigationonTinyImageNet, wherewestudytheperformanceonblackand
whitesketches. WeusethesameclassesasinTinyImageNet(Wangetal.,2019)andreportthefulldetailsand
results in the Appendix. The shift from natural images to sketches represents a more significant domain shift
than our standard OOD shifts (Hendrycks & Dietterich, 2019), and hence also tests the generalisation of the
noises to a larger degree. We do this to disambiguate the performance of AugMix, which uses augmentations
that may be similar, but are still distinct (Hendrycks et al., 2020), to the ones used for our OOD evaluation.
The results clearly show that AugMix obtains similar rankings for all three metrics on both the synthetic
OOD and sketch domain evaluations. Overall we see strong Kendall Tau correlation in terms of the error
across all noises, but smaller in terms of ECE and NLL.
Main Observations: We see consistent improvements in OOD generalisation and calibration for tabular
data. Errors and NLL are improved for CV classification, but calibration is generally not improved when
measured via ECE. CV regression usually sees improvements in OOD NLL only. The best ID noise types
have often remained the best OOD, but overall, the correlations between ID and OOD rankings were not
high in all cases. MixUp, or CMixUp for regression, showed surprising behaviour as it was much more helpful
for improving OOD calibration than ID calibration.
4.4 Combination of Noises
Next, we evaluate the combination of noises. We construct them from empirical combinations of the Top
2 or 3 noises from the ID evaluation for each task, based on average rank across respective datasets and
metrics. We consider two cases: 1.)the found hyperparameters of the noises are directly applied, and 2.)
the hyperparameters of the noises are jointly tuned. We utilise the same 50-trial budget to tune the selected
noises jointly. We restricted our experiments to combinations of two or three noises, as we empirically found
that tuning all the noises jointly in our study is not feasible even with a larger computational budget. We
consider the individual noise hyperparameter tuning and then the direct application or joint tuning of their
combination to be a realistic scenario where a practitioner has a limited compute budget and wants to
improve their model’s performance (Godbole et al., 2023) iteratively.
The results are already in Figures 1, 2, 3 and 4 and denoted as Top-2 Direct, Top-3 Direct for 1.), Top-
2 Optimised and Top-3 Optimised for 2.). The combinations for Top-2 and Top-3 are in Table 2. To
simplify the analysis of how effective the different combinations of noises are, we compute their average rank
improvement compared to no noise and report it in Table 3. Notice that when we choose a combination of
noises to involve noises from the same category, for example, ODS and input Gaussian are both input noises,
these are applied sequentially.
10Published in Transactions on Machine Learning Research (04/2024)
Task Top-2 Third Method
CV classification Input AugMix , Input Weak Augmentation Activation Dropout
NLP classification Activation Dropout, Target Label Smoothing Input ODS
Tabular classification Model, Target Label Smoothing Activation Dropout
CV regression Input AugMix , Input Weak Augmentation Activation Dropout
Tabular regression Weight Gaussian, Activation Gaussian Input Gaussian
Table 2: Top task and noise combinations. Underlined methods are from the same type.
Scenario Top-2 Direct Top-3 Direct Top-2 Optimised Top-3 Optimised
ID -4.75 -3.69 -4.34 -3.30
OOD -5.24 -4.43 -5.00 -2.59
Table 3: Average rank improvement over no noise for the different combination strategies.
We can draw several observations from Table 3. 1.)Directly combining hyperparameters for the top two or
three noises is a good strategy when considering the same budget for hyperparameter tuning as for one noise.
A significantly larger budget is likely needed for jointly optimising hyperparameters of multiple noises. 2.)
A combination of two noises performs better than a combination of three noises, suggesting there may be
negative interactions when too many noise sources are used without extensive hyperparameter tuning. 3.)
The behaviour of different combination strategies is consistent across ID and OOD settings.
Commenting on the overall performance of the combinations of noises, the combinations are typically better
for classification tasks than the individual noises. Still, the opposite may be true for regression. As observed
in Figures 1a, 1c and 1e, the combinations are consistently ranked lowest in comparison to using no noise
for classification, showing the effectiveness of the combinations. However, Figures 1b and 1d show that
regression can benefit from only using one noise at a time. OOD analysis in Figure 3 confirms the benefits
of combinations of noises for classification tasks, and it also shows that it can be beneficial for regression,
contrary to the ID behaviour. The combinations are generally ranked lower and can improve calibration and
generalisation, as seen in lower MSE, NLL, or error and ECE simultaneously.
Main Observations: The combination of noises can improve both calibration and generalisation simulta-
neously. Directly combining two noises is better than three, as too many can lead to conflicts. Combining
noises directly with their hyperparameters is generally reasonable and a significantly larger tuning budget
for optimising hyperparameters would be needed for optimising their hyperparameters jointly.
4.5 Transferability of Hyperparameters Across Datasets and Models
Furthermore, weevaluatethetransferabilityofthehyperparametersacrossdatasetsandmodels. Weconsider
two cases: the transfer of hyperparameters to a new dataset and the transfer of hyperparameters to a
new architecture. For the dataset transfer, we consider the following combinations: SVHN to CIFAR-10,
CIFAR-10 to CIFAR-100, CIFAR-100 to TinyImageNet, and 3 tabular regression datasets combinations,
Concrete to Energy, Boston to Wine, Yacht to Concrete. We consider the following combinations for the
architecture transfer: FC to ResNet-18 for SVHN and ResNet-18 to ResNet-34 for CIFAR-10, CIFAR-100
and TinyImageNet. We use a NN with an additional layer for tabular data, i.e., five layers instead of four.
4.5.1 Dataset Transfer
Figures 5a and 5c show the dataset transfer results for ID settings, with OOD settings shown in Figures 6a
and 6c. We observe generally good transferability of hyperparameters across datasets for CV classification in
ID and OOD settings. In particular, weak augmentation, AugMix and Dropout lead to solid improvements
in the ID setting. AugMix also excels in OOD scenarios under dataset transfer, but weak augmentation and
Dropout are not as strong in calibration measured using ECE. Certain noises are less transferable, including
Gaussian noise added to the input and DropConnect. Hyperparameters for noise in tabular regression are
less transferable because of worse generalisation measured using MSE.
11Published in Transactions on Machine Learning Research (04/2024)
Input Weak Aug.Input GaussianInput ODS
Input AugMix
Input-T arget MixUpLabel Smoothing
Activation GaussianActivation Dropout Gradient GaussianModel
Weight Gaussian
Weight DropConnectError
ECE
NLL-7.3 1.0 -1.0 -6.3 -1.3 1.3 -2.0 -4.7 4.3 -1.7 -1.3 1.7
-7.7 0.7 -2.3 -6.3 -0.7 -0.7 -4.3 -3.7 -2.7 -6.0 -1.3 0.3
-7.7 0.3 -1.3 -7.7 -4.3 0.3 -4.0 -3.0 2.0 -3.0 -2.7 0.7
10
010
(a) ID CV classification in dataset transfer.
Input Weak Aug.Input GaussianInput AugMix
Input-T arget MixUpLabel Smoothing
Activation GaussianActivation Dropout Gradient GaussianModel
Weight Gaussian
Weight DropConnectError
ECE
NLL-6.0 -1.0 -6.0 0.7 -1.7 1.3 -4.0 4.0 -4.7 0.7 2.7
-3.0 -2.7 -3.7 -1.7 0.0 -2.7 0.0 1.7 -3.0 -3.0 0.0
-6.7 -3.0 -7.0 -2.0 0.0 -1.0 -2.3 2.0 -5.7 -2.0 1.7
10
010 (b) ID CV classification in architecture transfer.
Input Gaussian
Input-T arget CMixUp Activation GaussianActivation Dropout Gradient GaussianModel
Weight Gaussian
Weight DropConnectMSE
NLL-0.7 2.3 1.3 3.0 2.7 4.7 -0.3 2.0
-0.3 -2.7 -1.3 -2.0 0.3 0.0 -1.7 -4.3
10
010
(c) ID tabular regression in dataset transfer.
Input Gaussian
Input-T arget CMixUp Activation GaussianActivation Dropout Gradient GaussianModel
Weight Gaussian
Weight DropConnectMSE
NLL-1.5 2.2 0.2 0.2 -1.7 1.8 -1.8 -0.8
-2.2 -2.5 0.5 -3.8 -1.0 -2.2 -2.5 -2.8
10
010 (d) ID tabular regression in architecture transfer.
Figure 5: Transfer of hyperparameters on in-domain (ID) data.
Input Weak Aug.Input GaussianInput ODS
Input AugMix
Input-T arget MixUpLabel Smoothing
Activation GaussianActivation Dropout Gradient GaussianModel
Weight Gaussian
Weight DropConnectError
ECE
NLL-8.3 -2.0 -4.0 -9.3 -3.3 -2.0 -3.3 -6.3 2.3 -1.3 -3.0 -2.7
1.3 -2.7 -3.0 -3.0 -4.3 -0.7 -5.3 2.0 0.0 -2.7 -0.3 -3.0
-5.3 -2.7 -2.7 -9.3 -7.3 -1.7 -4.7 -3.0 1.7 -3.3 -2.7 -2.3
10
010
(a) OOD CV classification in dataset transfer.
Input Weak Aug.Input GaussianInput AugMix
Input-T arget MixUpLabel Smoothing
Activation GaussianActivation Dropout Gradient GaussianModel
Weight Gaussian
Weight DropConnectError
ECE
NLL-7.7 -3.0 -8.7 -1.0 -5.3 -1.3 -7.0 0.0 -7.7 -2.3 -2.0
4.3 -1.0 2.0 -1.7 -2.3 0.0 3.3 5.0 1.7 -2.0 0.7
-4.7 -4.0 -8.3 -3.7 -2.3 -3.0 -3.7 0.7 -4.0 -3.7 -1.3
10
010 (b) OOD CV classification in architecture transfer.
Input Gaussian
Input-T arget CMixUp Activation GaussianActivation Dropout Gradient GaussianModel
Weight Gaussian
Weight DropConnectMSE
NLL0.7 5.3 1.3 2.0 4.0 4.0 -0.3 4.0
-0.3 0.7 1.0 -1.0 0.7 -3.3 -1.3 -2.3
10
010
(c) OOD tabular regression in dataset transfer.
Input Gaussian
Input-T arget CMixUp Activation GaussianActivation Dropout Gradient GaussianModel
Weight Gaussian
Weight DropConnectMSE
NLL-1.5 2.2 0.5 0.2 -1.7 1.8 -1.8 -1.2
-1.5 -3.2 -0.5 -2.5 -2.0 -2.5 -1.2 -3.2
10
010 (d) OOD tabular regression in architecture transfer.
Figure 6: Transfer of hyperparameters on out-of-domain (OOD) data.
Main Observations: The transfer of hyperparameters from dataset to dataset generally works well for
CV classification. However, caution is advised as it is not the case for all noise types. For tabular data
regression, tuning of hyperparameters is recommended.
4.5.2 Architecture Transfer
In Figures 5b and 5d, we show the ID results for the architecture transfer, with Figures 6b and 6d reporting
the OOD results. The transferability of noise hyperparameters is lower than across datasets for CV classi-
fication, but it is still successful, especially for weak augmentation and AugMix for ID settings. Transfer of
hyperparameters for tabular data regression works for certain noise types in the ID setting, including adding
Gaussian noise to the input or the weights, which are among the Top-3 noises for tabular data regression.
12Published in Transactions on Machine Learning Research (04/2024)
Main Observations: Transfer of hyperparameters across architectures appears more challenging than
across datasets but can be successful in some instances. Caution is advised, and tuning is recommended.
4.6 Learnt Representation Landscapes
We study the learnt representation landscapes of NNs trained with various noises through the lenses of ID
and OOD performance in terms of error, ECE, NLL or MSE. We consider the noises individually, with the
ID-found hyperparameters starting from the same weight initialisation for fairness. We visualise linear inter-
polation modulated through an αparameter between the final, α= 0and initial model, α= 1(Goodfellow
et al., 2014). The interpolation empirically investigates the smoothness of the training process. We also
visualise the landscape in 2D (Li et al., 2018; Holbrook, 2020) by saving the network after each epoch and
concatenating the weights. Instead of using random coordinates, we use the first two principal components
of the weights as the coordinates. We normalise them based on the magnitude of the original weights, and
we project all the weights onto these two components in the vicinity of αandβ. The 2D visualisations
show us the exploration and exploitation of the training process. In Figures 7 and 8, we compare the metric
landscapes of no noise with AugMix and Dropout noises, respectively on CIFAR-10 and WikiFace datasets.
We used 20 points for linear interpolation and 100 points for the 2D plots across five selected OOD augmen-
tations and 1000 test data samples for compute efficiency. In red, we show the error or MSE; in green, we
offer the NLL or ECE. In the 1D plots, and▲stand for ID and OOD error or MSE, and ×and■stand for
ID and OOD ECE or NLL. In the 2D plots, the darker combined contours signify worse performance than
the lighter parts, and the in blue or black denotes the start or end weights, respectively. The Appendix
contains the metric landscapes for all other noises, tabular classification – Adult, and regression – Yacht
datasets.
Observing Figures 7 and 8, we first notice the ID and OOD results are similar, with the OOD results
being slightly worse across all metrics. This includes both the 1D plots and the 2D plots. Second, as seen in
Figures 7a and 7b, the curves for error, representing generalisation, and ECE or NLL, representing confidence
calibration, do not share the same shapes or curvatures. MSE and NLL curves in Figure 8a are more similar
than error and ECE curves. Looking at the 1D plots, for example in Figures 7a 7b and 8a, the error or MSE
can be more smoothly interpolated than ECE or NLL. Figure 8a shows models trained without noise can
become overconfident, reflected in large NLL and small MSE. Adding noise such as Dropout can fix this,
leading to low NLL for the final model in Figure 8d. Looking at the 2D plots in Figures 7c and 8b, the error
or MSE valley is wider than the ECE or NLL valley, and they are not aligned. From a detailed comparison
between no noise and AugMix or Dropout in Figures 7 and 8, we observe that AugMix and Dropout can
smoothen the optimisation in the 1D plots, but not for ECE, and decrease the gap between ID and OOD
performance. The 2D plots show that AugMix and Dropout can explore broader metric landscapes than no
noise, shown in ranges of αandβin the 2D plots, and marginally align the error or MSE with NLL. Seen
in the lightness of the 2D contour plots, the noises navigate lower NLL or ECE landscapes than no noise.
Our general observations considering both CV and tabular datasets show that while noises such as Aug-
Mix, weak augmentation, MixUp or activation and weight noises based around Dropout can smoothen the
optimisation regarding error or MSE, they rarely smoothen the optimisation regarding ECE. The metric
landscapes often look similar to no noise, but the optimisation ends in more profound valleys. Across the
datasets and tasks, label smoothing, input additive Gaussian and ODS have minimal effect on the 2D land-
scapes or 1D interpolation. The model shrink and perturb make the optimisation more “stairs-like”, and the
metric landscape explored is broader. Together with gradient Gaussian noise, the shrink and perturb noises
explore broader metric landscapes than the others. No method drastically changes the metric landscape or
the interpolation from the default, but they can make the optimisation smoother or broader.
Main Observations: The metric landscapes for error or MSE and ECE or NLL are different, and the
noises can smoothen the optimisation in terms of error or MSE but not necessarily in terms of ECE or NLL.
When a model trained without noise is overconfident, adding noise to the training can resolve it and lead to
a significantly better-calibrated model at the end of training.
13Published in Transactions on Machine Learning Research (04/2024)
0.0 0.2 0.4 0.6 0.8 1.0
α102030405060708090Error [%]
ID OOD
0510152025
Expected Calibration Error [%]
ID OOD
(a) Error and ECE.
0.0 0.2 0.4 0.6 0.8 1.0
α102030405060708090Error [%]
ID OOD
0.500.751.001.251.501.752.002.25
NLL [nats]
ID OOD (b) Error and NLL.
−2.5 0.0 2.5 5.0 7.5 10.012.5
α−8−6−4−202β162432404856647280
8888
961020
20303030
40 4040
505050
606060
707070
8080Start weights End weights
152535455565758595
Error [%]
51525354555657585
Expected Calibration Error [%] (c) Error and ECE on ID.
−2.5 0.0 2.5 5.0 7.5 10.012.5
α−8−6−4−202β2432404856647280
88
961020
303030
404040
505050
606060
7070
8080Start weights End weights
24324048566472808896
Error [%]
51525354555657585
Expected Calibration Error [%] (d) Error and ECE on OOD.
0.0 0.2 0.4 0.6 0.8 1.0
α20406080Error [%]
ID OOD
24681012
Expected Calibration Error [%]
ID OOD
(e) Error and ECE.
0.0 0.2 0.4 0.6 0.8 1.0
α20406080Error [%]
ID OOD
0.51.01.52.0
NLL [nats]
ID OOD (f) Error and NLL.
0 2 4 6
α−6−5−4−3−2−101β10
20304050607080
908
888
1616
1616
2424
3232
4040
4848
5656
6464
72
80Start weights End weights
102030405060708090
Error [%]
4122028364452606876
Expected Calibration Error [%] (g) Error and ECE on ID.
0 2 4 6
α−6−5−4−3−2−101β
1624324048566472
8088
968
888
1616
16
2424
3232
4040
4848
5656
6464
72
80Start weights End weights
152535455565758595
Error [%]
4122028364452606876
Expected Calibration Error [%] (h) Error and ECE on OOD.
Figure 7: No noise (top) and Input AugMix (bottom) on CIFAR-10.
0.0 0.2 0.4 0.6 0.8 1.0
α0.030.040.050.060.070.080.09Mean Squared Error
ID OOD
05101520253035
NLL [nats]
ID OOD
(a) MSE and NLL.
−0.0020.000 0.002 0.004 0.006 0.008 0.010
α−0.00250.00000.00250.00500.00750.01000.01250.01500.0175β0.0306
0.03080.0308
0.03100.03100.03120.0314
0.03160.03180.03200.03220.0324
2428 323640 44485256Start weights End weights
0.03060.03090.03120.03150.03180.03210.0324
Mean Squared Error
242832364044485256
NLL [nats] (b) MSE and NLL on ID.
−0.0020.000 0.002 0.004 0.006 0.008 0.010
α−0.00250.00000.00250.00500.00750.01000.01250.01500.0175β0.03250
0.032750.03300
0.033250.033500.033750.034000.03425
323640 44 4852566064Start weights End weights
0.03240.03270.03300.03330.03360.03390.03420.03450.0348
Mean Squared Error
323640444852566064
NLL [nats] (c) MSE and NLL on OOD.
0.0 0.2 0.4 0.6 0.8 1.0
α0.030.040.050.060.070.080.09Mean Squared Error
ID OOD
−1.4−1.2−1.0−0.8−0.6−0.4−0.2
NLL [nats]
ID OOD
(d) MSE and NLL.
0.00 0.02 0.04 0.06
α−0.0050.0000.0050.0100.0150.020β
0.03200.03280.03360.03440.03520.0360 0.03600.0368
−1.25−1.00−0.75−0.50−0.250.000.25Start weights End weights
0.03160.03240.03320.03400.03480.03560.03640.03720.0380
Mean Squared Error
−1.2−0.9−0.6−0.30.00.30.6
NLL [nats] (e) MSE and NLL on ID.
0.00 0.02 0.04 0.06
α−0.0050.0000.0050.0100.0150.020β
0.03420.03480.03540.03600.03660.0372 0.0378 0.03780.0384 0.0384
−1.25−1.00−0.75−0.50−0.250.000.250.50Start weights End weights
0.03360.03450.03540.03630.03720.03810.0390
Mean Squared Error
−1.2−0.9−0.6−0.30.00.30.60.9
NLL [nats] (f) MSE and NLL on OOD.
Figure 8: No noise (top) and Activation Dropout (bottom) on WikiFace.
5 Conclusion
Key Takeaways : Noise injection methods can improve NN performance across various tasks and datasets.
This is despite the fact L2 regularisation was already tuned to prevent overfitting, indicating noise injection
methods can provide additional benefits beyond standard regularisation. The methods were not equally
efficient across all tasks and datasets, with significant differences in performance between regression and
classification. Nevertheless, out of the considered noise types, the proposed methodology identified at least
one noise or a combination of noises that demonstrated improvements in both calibration and generalisation
over the no noise baseline for all tasks. The most effective noise for CV was AugMix, model shrink and
perturb and Gaussian noise added to weights for tabular data classification and regression, respectively. At
the same time, Dropout and label smoothing worked the best for NLP. Even though ODS was not designed
to improve calibration and generalisation, it has shown promising performance in several cases. Combining
noisesoutperformedindividualnoisesinmostclassificationcases, withregressionoftenbenefittingfromusing
only one noise at a time. While directly combining hyperparameters of noises is a reasonable strategy, tuning
them can still be valuable if a large budget is used. The noises improved ID and OOD performance, but
14Published in Transactions on Machine Learning Research (04/2024)
the ID rankings were sometimes inconsistent with the OOD evaluation. AugMix remained highly ranked for
robustness, demonstrating that domain-specific inductive biases are beneficial when crafting noise injection
methodsastheycanimprovegeneralisationandcalibration. Notethatouraimwasnottoaddnewknowledge
about AugMix’s performance against image corruptions, given its well-documented performance (Hendrycks
et al., 2020), but to demonstrate that inductive biases should be considered when generating noise for better
performance. The visualisation showed noises can smoothen the optimisation in terms of error or MSE but
not necessarily in terms of ECE or NLL. It also showed noise can help mitigate overconfidence. Overall,
the results indicate practitioners should consider combining noises, e.g. AugMix and Dropout, and tuning
hyperparameters for their specific problem.
Limitations : To conduct this study, we had to restrict the experiments’ scope. Our scope was limited to
experimental datasets, tasks such as classification and regression, and standard NN architectures. Testing on
more complex data and downstream tasks such as object detection, segmentation, or reinforcement learning
would reveal more profound impacts of noise injection. Furthermore, diving deeper into one particular
domain, suchasNLP, couldprovidemoreinsights intotheeffectivenessofnoiseinjectionmethods. Moreover,
we also limited the optimisation to SGD with momentum and a cosine learning rate schedule, which were
tuned beforehand to make the hyperparameter search tractable. To draw practical conclusions, we evaluated
thenoiseperformancebyminimisingtheNLLratherthanexploringallpossiblesettings. Thecostsassociated
with adjusting and evaluating different noises limited the scope of the experiments. Consequently, certain
noises might prove more effective with more thorough tuning and a larger budget. This is especially true
for noise combinations, where the number of possible combinations grows exponentially with the number
of noises. Nonetheless, the existing findings offer valuable insights for practitioners by giving a preliminary
indication of the most promising noise sources. This enables users to concentrate their efforts and compute
the budget required for tuning. For example, AugMix, incorporating domain-specific inductive biases, was
transferable and effective even with a limited budget. Developing methods to choose hyperparameters
without the need for extensive tuning would enhance the accessibility of these techniques. Lastly, the out-of-
distribution evaluation was focused on synthetic augmentations, which may not fully capture the real-world
distribution shift, as noticed in (Taori et al., 2020) for computer vision tasks.
Future Directions : The strong performance of AugMix highlights the potential for developing specialised,
domain-specific noise techniques such as DeepAugment (Hendrycks et al., 2021). For example, tailored
domain-specific noise methods could benefit tabular data-based problems and NLP. Future work should also
explore specific data-architecture noise interactions, as the transferability of hyperparameters was limited.
Inspired by the annealed gradient noise, annealing noise levels overtraining may also prove helpful, as early
noise could encourage robustness. In contrast, low late-stage noise could enable convergence on a high-
accuracy solution. The potential for combining noises from the same category should also be investigated
further. The noises affected the entire architecture, but targeting noise injection methods that only affect
specific layers or sections of the network may be possible, requiring more or less regularisation. Specific
noise-based approaches for simultaneously exploring the generalisation and confidence calibration trade-off
should be investigated further. As demonstrated across image and tabular domains, the empirical differ-
ences between many methods seem minor. This indicates that there are more fundamental determinants of
performance. Therefore, theoretical analysis of noise injection methods would be beneficial in understanding
the underlying mechanisms and providing guidance for future research. Lastly, testing on wider-scale out-
of-distribution datasets and real-world distribution shifts would provide a more comprehensive evaluation
of the noise injection methods’ impact on robustness. We hope our study and framework, embedded in our
codebase, will assist further research in this area.
Acknowledgements
Martin Ferianc was sponsored through a scholarship from the Institute of Communications and Connected
Systems at UCL. Ondrej Bohdal was supported by the EPSRC Centre for Doctoral Training in Data Science,
funded by the UK Engineering and Physical Sciences Research Council (grant EP/L016427/1) and the
University of Edinburgh.
15Published in Transactions on Machine Learning Research (04/2024)
References
Jordan Ash and Ryan P Adams. On warm-starting neural network training. In NeurIPS , 2020.
Arthur Asuncion and David Newman. Uci machine learning repository, 2007.
James Bergstra, Rémi Bardenet, Yoshua Bengio, and Balázs Kégl. Algorithms for hyper-parameter opti-
mization. In NeurIPS , 2011.
Chris M Bishop. Training with noise is equivalent to tikhonov regularization. Neural computation , 7(1):
108–116, 1995.
Charles Blundell, Julien Cornebise, Koray Kavukcuoglu, and Daan Wierstra. Weight uncertainty in neural
network. In ICML, 2015.
Alexander Camuto. Understanding Gaussian noise injections in neural networks . PhD thesis, University of
Oxford, 2021.
Alexander Camuto, Matthew Willetts, Umut Simsekli, Stephen J Roberts, and Chris C Holmes. Explicit
regularisation in gaussian noise injections. In NeurIPS , 2020.
Pratik Chaudhari and Stefano Soatto. On the energy landscape of deep networks. arXiv, 2015.
Sanghyuk Chun, Seong Joon Oh, Sangdoo Yun, Dongyoon Han, Junsuk Choe, and Youngjoon Yoo. An
empirical evaluation on robustness and uncertainty of regularization methods. arXiv, 2020.
Jeremy Cohen, Elan Rosenfeld, and Zico Kolter. Certified adversarial robustness via randomized smoothing.
InICML, 2019.
Terrance DeVries and Graham W Taylor. Dataset augmentation in feature space. arXiv, 2017.
Yarin Gal and Zoubin Ghahramani. Dropout as a bayesian approximation: Representing model uncertainty
in deep learning. In ICML, 2016.
Robert Geirhos, Carlos RM Temme, Jonas Rauber, Heiko H Schütt, Matthias Bethge, and Felix A Wich-
mann. Generalisation in humans and deep neural networks. In NeurIPS , 2018.
Spyros Gidaris, Praveer Singh, and Nikos Komodakis. Unsupervised representation learning by predicting
image rotations. In ICLR, 2018.
Varun Godbole, George E. Dahl, Justin Gilmer, Christopher J. Shallue, and Zachary Nado. Deep learning
tuning playbook, 2023. URL http://github.com/google-research/tuning_playbook . Version 1.0.
Ian J Goodfellow, Oriol Vinyals, and Andrew M Saxe. Qualitatively characterizing neural network optimiza-
tion problems. arXiv, 2014.
Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples.
InICLR, 2015.
Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q Weinberger. On calibration of modern neural networks. In
ICML, 2017.
Hongyu Guo, Yongyi Mao, and Richong Zhang. Augmenting data with mixup for sentence classification:
An empirical study. arXiv, 2019.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In
CVPR, 2016.
Zhezhi He, Adnan Siraj Rakin, and Deliang Fan. Parametric noise injection: Trainable randomness to
improve deep neural network robustness against adversarial attack. In CVPR, 2019.
16Published in Transactions on Machine Learning Research (04/2024)
Dan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common corruptions
and perturbations. In ICLR, 2019.
Dan Hendrycks, Norman Mu, Ekin D Cubuk, Barret Zoph, Justin Gilmer, and Balaji Lakshminarayanan.
Augmix: A simple data processing method to improve robustness and uncertainty. In ICLR, 2020.
Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan Dorundo, Rahul Desai,
Tyler Zhu, Samyak Parajuli, Mike Guo, et al. The many faces of robustness: A critical analysis of
out-of-distribution generalization. In CVPR, 2021.
Ryan Holbrook, 2020. URL https://mathformachines.com/posts/visualizing-the-loss-landscape/ .
Kevin Jamieson and Ameet Talwalkar. Non-stochastic best arm identification and hyperparameter optimiza-
tion. InAISTATS , 2016.
Hojin Jang, Devin McCormack, and Frank Tong. Noise-trained deep neural networks effectively predict
human vision and its neural responses to challenging images. PLoS biology , 19(12):e3001418, 2021.
Daniel Kang, Yi Sun, Tom Brown, Dan Hendrycks, and Jacob Steinhardt. Transfer of adversarial robustness
between perturbation types. arXiv, 2019.
Yoon Kim. Convolutional neural networks for sentence classification. arXiv, 2014.
Durk P Kingma, Tim Salimans, and Max Welling. Variational dropout and the local reparameterization
trick. In NeurIPS , 2015.
Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. Technical
report, University of Toronto, 2009.
Anders Krogh and John Hertz. A simple weight decay can improve generalization. In NeurIPS , 1991.
Jan Kukačka, Vladimir Golkov, and Daniel Cremers. Regularization for deep learning: A taxonomy. arXiv,
2017.
Ken Lang. Newsweeder: Learning to filter netnews. In ICML, 1995.
Ya Le and Xuan Yang. Tiny imagenet visual recognition challenge. CS 231N , 7(7):3, 2015.
Hao Li, Zheng Xu, Gavin Taylor, Christoph Studer, and Tom Goldstein. Visualizing the loss landscape of
neural nets. In NeurIPS , 2018.
Ilya Loshchilov and Frank Hutter. Sgdr: Stochastic gradient descent with warm restarts. In ICLR, 2017.
Rafael Müller, Simon Kornblith, and Geoffrey E Hinton. When does label smoothing help? In NeurIPS ,
2019.
Arvind Neelakantan, Luke Vilnis, Quoc V. Le, Lukasz Kaiser, Karol Kurach, Ilya Sutskever, and James
Martens. Adding gradient noise improves learning for very deep networks. In OpenReview , 2017.
Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y Ng. Reading digits in
natural images with unsupervised feature learning. In NIPS Workshop on Deep Learning and Unsupervised
Feature Learning , 2011.
Hyeonwoo Noh, Tackgeun You, Jonghwan Mun, and Bohyung Han. Regularizing deep neural networks by
noise: Its interpretation and optimization. In NeurIPS , 2017.
Jeffrey Pennington, Richard Socher, and Christopher D Manning. Glove: Global vectors for word represen-
tation. In EMNLP, 2014.
Gabriel Pereyra, George Tucker, Jan Chorowski, Łukasz Kaiser, and Geoffrey Hinton. Regularizing neural
networks by penalizing confident output distributions. In ICLR Workshop , 2017.
17Published in Transactions on Machine Learning Research (04/2024)
Ben Poole, Jascha Sohl-Dickstein, and Surya Ganguli. Analyzing noise in autoencoders and deep networks.
arXiv, 2014.
Lutz Prechelt. Early stopping-but when? In Neural Networks: Tricks of the trade . Springer, 2002.
Rasmus Rothe, Radu Timofte, and Luc Van Gool. DEX: Deep EXpectation of apparent age from a single
image. In ICCV Workshop , 2015.
Jocelyn Sietsma and Robert JF Dow. Creating artificial neural networks that generalize. Neural networks ,
4(1):67–79, 1991.
Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Y Ng, and
Christopher Potts. Recursive deep models for semantic compositionality over a sentiment treebank. In
EMNLP, 2013.
Hwanjun Song, Minseok Kim, Dongmin Park, Yooju Shin, and Jae-Gil Lee. Learning from noisy labels with
deep neural networks: A survey. Transactions on Neural Networks and Learning Systems , 2022.
Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: a
simple way to prevent neural networks from overfitting. JMLR, 2014.
Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethinking the
inception architecture for computer vision. In CVPR, 2016.
Rohan Taori, Achal Dave, Vaishaal Shankar, Nicholas Carlini, Benjamin Recht, and Ludwig Schmidt. Mea-
suring robustness to natural distribution shifts in image classification. In NeurIPS , 2020.
Yusuke Tashiro, Yang Song, and Stefano Ermon. Diversity can be transferred: Output diversification for
white-and black-box attacks. In NeurIPS , 2020.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser,
and Illia Polosukhin. Attention is all you need. In NeurIPS , 2017.
Li Wan, Matthew Zeiler, Sixin Zhang, Yann Le Cun, and Rob Fergus. Regularization of neural networks
using dropconnect. In ICML, 2013.
Haohan Wang, Songwei Ge, Zachary Lipton, and Eric P Xing. Learning robust global representations by
penalizing local predictive power. In NeurIPS , 2019.
Colin Wei, Sham Kakade, and Tengyu Ma. The implicit and explicit regularization effects of dropout. In
ICML, 2020.
Max Welling and Yee W Teh. Bayesian learning via stochastic gradient langevin dynamics. In ICML, 2011.
Jingfeng Wu, Wenqing Hu, Haoyi Xiong, Jun Huan, Vladimir Braverman, and Zhanxing Zhu. On the noisy
gradient descent that generalizes as SGD. In ICML, 2020.
Huaxiu Yao, Yiping Wang, Linjun Zhang, James Y Zou, and Chelsea Finn. C-mixup: Improving general-
ization in regression. In NeurIPS , 2022.
Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond empirical risk
minimization. In ICLR, 2018.
Mo Zhou, Tianyi Liu, Yan Li, Dachao Lin, Enlu Zhou, and Tuo Zhao. Toward understanding the importance
of noise in training neural networks. In ICML, 2019.
18Published in Transactions on Machine Learning Research (04/2024)
Appendix
In the Appendix, we first provide the experimental settings and the hyperparameter ranges for all the
experimentsinSectionA.Wethenprovidethefullnumericalresultsandvisualisationsforalltheexperiments
in Section B.
A Settings
A.1 General Settings
We used stochastic gradient descent with a momentum of 0.9 to train all the networks. The learning rate
and L2 regularisation were tuned and reused for each noise injection method. We used a cosine annealing
learning rate schedule without restarts (Loshchilov & Hutter, 2017) for all experiments. In most cases,
we used gradient norm clipping of 20.0 to stabilise the training, with gradient clipping of 10.0 for tabular
regression and 5.0 for WikiFace. The batch size was set to 256 for all experiments. The final results are the
average of 3 runs with 3 different seeds. We used cross-entropy loss for all classification experiments. For
regression, we used the Gaussian negative log-likelihood (NLL) loss, where we modelled the variance as an
additional output passed through an exponential function to ensure positivity. We added a small ϵof1e−8
to the softmax probabilities to avoid NaNs. We clipped the variance between 1e−4and1e4to avoid NaNs.
The hyperparameter ranges, and the sampling scale for each dataset-architecture pair are in Table 4. The
hyperparameters and implementations of all the noises and experiments can be found in the code, which will
be open-sourced. We used the default PyTorch weight initialisation for all layers.
For the tabular OOD experiments, we constructed custom augmentations where we applied Gaussian or
Uniform noise scaled by the magnitude of the input features across 5 severities for addition: [0.02, 0.04, 0.06,
0.08, 0.1] or multiplication [0.04, 0.08, 0.12, 0.16, 0.2] where the severity scaled the range or the standard
deviation of the noise applied to the input. Additionally, we zeroed out some input features with probability
[0.04, 0.08, 0.12, 0.16, 0.2], denoting 5 severities. In total, there were 5 different input shifts across 5 severities
each. To avoid label-flipping in applying these augmentations, we have introduced an empirically determined
scaling factor for the severity of all noises for a particular dataset. They multiply the [0.02, 0.04, 0.06, 0.08,
0.1] or [0.04, 0.08, 0.12, 0.16, 0.2] by a scaling factor to determine the severity of the noise applied to the
input based on the dataset. We use a K-nearest neighbour (KNN) classifier, specifically a 1-neighbour KNN,
trained on a dataset’s original, unmodified data. This classifier then predicts labels for the augmented data.
We adjust the scaling factor for each dataset so that the KNN classifier’s accuracy on the augmented data
exceeds 99% or the mean squared error is less than 0.01. This approach ensures that the augmentations are
subtle enough to maintain the integrity of the data, meaning the nearest neighbour—the closest match in
the original dataset—remains the same. However, this is not a perfect solution, with an empirical guarantee
that the augmentations are not too severe, and the nearest neighbour’s label remains the same. The found
scaling factors are shown in Table 5.
Regarding noise implementation details, Dropout, DropConnect, additive weight or activation Gaussian
noise, are applied to all linear and convolutional weights throughout the network, excluding the last layer
and normalisation layers. Both model and Gaussian gradient noise are implemented on all weights within
the network, encompassing affine parameters in normalisation layers. Rotation was omitted from AugMix,
given that one of our tasks involved predicting the rotation angle.
A.2 Vision Experiments
For SVHN, we used a fully connected network with 4 hidden layers of 150 units followed by ReLU activations.
When we used ResNet-18 we used it with [64, 128, 256, 512] channels in 4 stages with [2, 2, 2, 2] blocks
with strides [1, 2, 2, 2]. When we used ResNet-34, we used it with [64, 128, 256, 512] channels in 4 stages
with [3, 4, 6, 3] blocks with strides [1, 2, 2, 2]. In all cases, we trained the networks for 200 epochs. We
only used 0-1 truncation followed by normalisation for each dataset without further data augmentations
for training, validation and test sets. For rotation experiments, we enabled uniform rotations between (0,
90°) degrees and rescaled the targets accordingly to [-1, 1]. Gaussian noise, motion blur, snow, elastic
19Published in Transactions on Machine Learning Research (04/2024)
transformation, and JPEG compression were selected as OOD augmentations for visualisation experiments
across all 5 severities. For CIFAR-10, CIFAR-100, and SVHN we used the dedicated test sets as the test
set, while for TinyImageNet we used the official validation set as the test set. We used 10% of the training
data to construct the validation sets. For WikiFace, we used 10% of the data as the test set and 10% of the
remaining data as the validation set and the rest as the training set.
For evaluation on the sketch domain, we utilise ImageNet-Sketch dataset from Wang et al. (2019) and derive
our own TinyImageNet-Sketch dataset from it. As the ImageNet-Sketch images are not square, we crop
the centre and then resize to 64×64, the same size as TinyImageNet. We only keep the images of the
same 200 classes as used in TinyImageNet, allowing us to directly evaluate pre-trained models on the new
TinyImageNet-Sketch dataset.
A.3 NLP Classification Experiments
For our NLP experiments we used NewsGroup and binary SST datasets. In the NewsGroup dataset we aim
to classify news texts into one of the 20 available categories based on the topic. The task in the binary SST
dataset is to predict the sentiment of a movie review into 2 categories: positive or negative. For SST we
only consider the text itself, rather than also considering the available parse trees, making the task more
challenging.
Each dataset was first pre-processed with respect to glove embeddings (Pennington et al., 2014) into embed-
dings of dimension 100 and sequence length 100 and 50 for NewsGroup and SST respectively. In both cases,
we trained the networks for 100 epochs. We used the global-pooling convolutional network architecture
from Kim (2014) with planes [128, 128, 128] and a transformer decoder (Vaswani et al., 2017) with embed-
ding dimensions 100, 6 layers, 8 heads, 1024 hidden dimensions, 64 dimensions per head and no dropout. For
the NewsGroup experiments, we used about 5% of the data as the test set, 10% of the remaining data as the
validation set and the rest as the training set. For the SST experiments, we used the original development
set as the test set, for validation we took 10% of the original training set and used the remainder as the
training set. No OOD test was set for the NLP task due to a lack of suitable perturbations to construct
OOD data.
A.4 Tabular Regression Experiments
For the tabular experiments, we used a fully connected network with [100, 100, 100, 100] hidden units and
ReLU activations. In all cases, we trained the networks for 100 epochs. We normalised the input features and
targets to zero mean and unit variance by using the training set statistics and applied the same normalisation
to the validation and test sets. For the tabular experiments, we used 20% of the data as the test set and
10% of the remaining data as the validation set and the rest as the training set. The regression targets were
normalised to zero mean and unit variance.
B Full Results
We provide full results of all experiments in the paper, where the main reported value is the mean across 3
repetitions, followed by the standard deviation. The ranks presented in the main body of the paper can be
obtained by ranking the results in each table by the metric of interest. Following the tables, there are the
visualisations of metric landscapes for CIFAR-10, Adult, WikiFace and Yacht datasets. We encourage the
reader to look at our code for other datasets to regenerate them from there.
20Published in Transactions on Machine Learning Research (04/2024)
Hyperparameter ( δ) Range Scale
Learning rate (LR) [10−4,10−1]Log
L2 weight [10−7,10−1]Log
Input Gaussian noise std. [10−4,10−1]Log
Input AugMix alpha [0,1]Linear
Input AugMix severity [1,10] Linear
Input AugMix width [1,5]Linear
Input AugMix chain-depth [−1,3]Linear
Input ODS epsilon [10−4,10−1]Log
Input ODS temperature [0.5,5.0]Log
Input-Target MixUp alpha [0,1]Linear
Input-Target CMixUp alpha [0,1]Linear
Input-Target CMixUp sigma [10−4,102]Log
Target Label Smoothing [0,0.25]Linear
Activation Gaussian noise std [10−4,10−1]Log
Activation Dropout rate [0,1]Linear
Gradient Gaussian noise η [0,1]Linear
Gradient Gaussian noise γ [0,1]Linear
Weight Gaussian noise std [10−4,10−1]Log
Weight DropConnect rate [0,1]Linear
Model noise shrink factor [0.0,1.0]Linear
Model noise std [10−7,10−3]Log
Model noise frequency [0,20] Linear
Table 4: Hyperparameters (HPs) optimised for individual noises and their range.
Dataset Task Scaling Factor
Adult Classification 0.1438
Abalone Classification 0.0886
Concrete Regression 0.0207
Energy Regression 0.0546
Wine Classification 0.0078
Wine Regression 0.0048
Yacht Regression 0.0886
Toxicity Classification 0.3793
Students Classification 0.2336
Boston Regression 0.0886
Table 5: Scaling factors for tabular data.
21Published in Transactions on Machine Learning Research (04/2024)
Noise TypeSVHN CIFAR-10 CIFAR-100 TinyImageNet
ID OOD ID OOD ID OOD ID OOD
No Noise 16.33 ±0.07 20.18 ±0.09 12.04 ±0.21 32.32 ±0.24 44.69 ±0.84 61.52 ±0.52 54.15 ±0.36 75.65 ±0.19
Input Weak Aug. 14.67 ±0.14 18.59 ±0.14 5.39±0.17 27.86 ±0.37 26.51 ±0.10 52.57 ±0.13 39.73 ±0.27 67.73 ±0.13
Input Gaussian 16.45 ±0.23 20.23 ±0.22 12.02 ±0.05 31.97 ±0.31 44.34 ±1.70 61.14 ±1.60 53.28 ±0.24 75.12 ±0.06
Input ODS 16.35 ±0.20 20.13 ±0.15 12.01 ±0.16 30.42 ±0.45 44.28 ±0.64 61.34 ±0.56 66.47 ±15.18 82.46 ±8.72
Input AugMix 12.28 ±0.07 15.67 ±0.07 7.48±0.06 18.75 ±0.25 30.09 ±0.25 46.05 ±0.13 42.49 ±0.24 61.58 ±0.16
Input-Target MixUp 13.95 ±0.01 17.71 ±0.11 10.97 ±0.13 28.38 ±0.70 46.15 ±0.05 63.00 ±0.02 54.02 ±0.53 75.33 ±0.44
Label Smoothing 16.35 ±0.17 20.10 ±0.12 11.88 ±0.39 31.63 ±0.29 42.48 ±0.47 58.41 ±0.55 52.47 ±0.21 73.96 ±0.15
Activation Gaussian 16.33 ±0.17 20.14 ±0.13 11.44 ±0.12 30.71 ±0.20 44.58 ±0.28 61.81 ±0.15 53.96 ±0.16 75.45 ±0.14
Activation Dropout 13.83 ±0.13 17.43 ±0.10 8.93±0.25 29.85 ±0.86 41.51 ±0.83 58.92 ±0.80 43.26 ±0.35 69.32 ±0.32
Gradient Gaussian 17.59 ±0.15 22.49 ±0.07 16.41 ±0.15 37.57 ±0.12 45.33 ±0.52 62.49 ±0.24 59.99 ±0.38 80.15 ±0.21
Model 16.17 ±0.25 20.08 ±0.17 10.65 ±0.19 32.83 ±0.60 35.88 ±0.19 56.73 ±0.32 49.66 ±0.34 72.44 ±0.15
Weight Gaussian 16.60 ±0.16 20.29 ±0.11 10.53 ±0.24 30.76 ±0.55 42.97 ±0.54 60.68 ±0.24 54.20 ±0.11 75.71 ±0.13
Weight DropConnect 15.82 ±0.06 19.53 ±0.04 12.20 ±0.20 31.29 ±0.63 42.08 ±0.75 59.16 ±0.30 54.33 ±0.70 75.70 ±0.43
Top-2 Direct Combination 12.57 ±0.18 16.04 ±0.19 4.78±0.12 16.47 ±0.24 24.69 ±0.22 43.04 ±0.24 37.18 ±0.04 58.16 ±0.12
Top-3 Direct Combination 13.49 ±0.19 16.95 ±0.10 5.07±0.07 17.72 ±0.07 24.88 ±0.13 42.99 ±0.21 37.43 ±0.12 58.73 ±0.19
Top-2 Optimised Combination 12.14 ±0.10 15.51 ±0.11 5.15±0.22 15.63 ±0.27 25.28 ±0.15 43.86 ±0.16 36.88 ±0.08 57.90 ±0.24
Top-3 Optimised Combination 12.66 ±0.04 16.08 ±0.04 6.60±0.16 17.91 ±0.63 26.19 ±0.23 43.84 ±0.30 36.59 ±0.06 60.36 ±0.17
Table 6: CV classification: Error (↓,%)comparison on in-distribution (ID) and out-of-distribution (OOD)
test sets and with tuned hyperparameters.
Noise TypeSVHN CIFAR-10 CIFAR-100 TinyImageNet
ID OOD ID OOD ID OOD ID OOD
No Noise 13.10 ±0.05 15.59 ±0.06 4.02±0.15 16.40 ±0.36 5.76±0.25 7.43±0.09 16.43 ±0.27 10.11 ±0.11
Input Weak Aug. 6.35 ±0.19 7.85±0.24 1.92±0.23 15.91 ±0.67 4.83±0.19 13.28 ±0.14 6.49±0.28 11.46 ±0.25
Input Gaussian 13.24 ±0.17 15.67 ±0.15 4.22±0.38 16.65 ±0.59 5.42±0.46 7.70±0.48 15.96 ±1.37 10.15 ±0.45
Input ODS 13.03 ±0.13 15.43 ±0.12 4.26±0.13 15.38 ±0.27 5.86±0.30 7.48±0.17 26.97 ±19.11 28.20 ±26.53
Input AugMix 4.81 ±0.07 6.03±0.09 1.18±0.06 6.27±0.39 4.41±0.39 11.38 ±0.24 4.87±0.31 15.78 ±0.31
Input-Target MixUp 2.81 ±0.08 3.49±0.07 5.41±3.04 7.31±0.66 14.20 ±0.13 8.57±0.42 16.33 ±1.40 10.26 ±0.32
Label Smoothing 8.66 ±0.10 10.61 ±0.08 5.11±0.12 9.46±0.02 21.55 ±0.15 17.10 ±0.15 29.45 ±0.17 16.62 ±0.03
Activation Gaussian 13.09 ±0.14 15.56 ±0.09 5.50±0.13 18.80 ±0.25 5.12±0.32 7.58±0.25 14.77 ±0.76 9.83±0.29
Activation Dropout 5.33 ±0.19 6.61±0.17 4.48±0.15 18.85 ±0.62 5.48±0.91 8.64±0.85 10.31 ±0.66 20.13 ±0.86
Gradient Gaussian 14.84 ±0.09 18.39 ±0.04 6.03±0.27 18.79 ±0.14 5.54±0.42 9.70±0.27 23.56 ±0.24 34.33 ±0.22
Model 10.93 ±0.19 12.82 ±0.13 4.42±0.01 18.43 ±0.54 9.06±0.32 11.00 ±0.39 10.80 ±0.24 9.01±0.03
Weight Gaussian 13.38 ±0.12 15.72 ±0.08 6.48±0.24 21.54 ±0.66 5.99±0.26 7.79±0.36 14.95 ±1.03 9.98±0.33
Weight DropConnect 12.51 ±0.10 14.87 ±0.05 4.94±0.20 16.99 ±0.66 5.79±0.36 8.21±0.47 15.50 ±0.83 10.12 ±0.35
Top-2 Direct Combination 1.79 ±0.15 2.76±0.17 1.25±0.15 6.91±0.26 6.19±0.19 14.21 ±0.44 4.04±0.27 15.69 ±0.34
Top-3 Direct Combination 1.31 ±0.17 1.86±0.09 2.08±0.12 8.57±0.33 6.66±0.04 14.67 ±0.21 13.05 ±1.09 22.67 ±1.96
Top-2 Optimised Combination 2.55 ±0.07 3.48±0.10 0.96±0.13 5.80±0.16 6.68±0.22 14.54 ±0.46 3.21±0.39 15.33 ±0.05
Top-3 Optimised Combination 2.89 ±0.11 3.98±0.08 1.15±0.34 4.97±1.05 5.60±0.31 12.56 ±0.61 9.90±0.42 20.11 ±0.26
Table 7: CV classification: ECE (↓,%)comparison on in-distribution (ID) and out-of-distribution (OOD)
test sets and with tuned hyperparameters.
22Published in Transactions on Machine Learning Research (04/2024)
Noise TypeSVHN CIFAR-10 CIFAR-100 TinyImageNet
ID OOD ID OOD ID OOD ID OOD
No Noise 1.43 ±0.00 1.64±0.01 0.42±0.01 1.21±0.02 1.85±0.03 2.69±0.03 2.80±0.02 3.90±0.01
Input Weak Aug. 0.62 ±0.01 0.75±0.01 0.20±0.00 1.10±0.03 1.07±0.01 2.44±0.00 1.84±0.02 3.46±0.02
Input Gaussian 1.43 ±0.01 1.65±0.01 0.42±0.01 1.21±0.03 1.84±0.08 2.67±0.08 2.75±0.06 3.86±0.02
Input ODS 1.40 ±0.02 1.60±0.02 0.42±0.01 1.14±0.02 1.84±0.03 2.69±0.03 4.79±2.79 6.23±3.27
Input AugMix 0.49 ±0.01 0.61±0.00 0.25±0.00 0.63±0.01 1.17±0.00 2.04±0.01 1.87±0.01 3.14±0.01
Input-Target MixUp 0.52 ±0.00 0.64±0.00 0.40±0.03 0.91±0.02 2.03±0.00 2.77±0.01 2.79±0.07 3.88±0.04
Label Smoothing 0.75 ±0.01 0.90±0.01 0.46±0.01 1.09±0.01 2.18±0.02 2.87±0.03 3.25±0.02 4.15±0.01
Activation Gaussian 1.41 ±0.00 1.63±0.00 0.43±0.00 1.27±0.01 1.84±0.01 2.71±0.01 2.74±0.02 3.87±0.01
Activation Dropout 0.51 ±0.01 0.63±0.01 0.33±0.01 1.26±0.04 1.71±0.03 2.58±0.04 1.97±0.02 3.69±0.05
Gradient Gaussian 1.76 ±0.01 2.12±0.01 0.56±0.01 1.39±0.01 1.86±0.02 2.76±0.01 3.20±0.02 5.36±0.03
Model 1.02 ±0.01 1.17±0.01 0.37±0.00 1.27±0.03 1.60±0.02 2.63±0.02 2.38±0.01 3.64±0.01
Weight Gaussian 1.44 ±0.02 1.65±0.02 0.44±0.01 1.41±0.04 1.79±0.03 2.66±0.02 2.76±0.03 3.89±0.02
Weight DropConnect 1.32 ±0.01 1.52±0.01 0.43±0.01 1.21±0.03 1.73±0.04 2.58±0.02 2.78±0.06 3.90±0.04
Top-2 Direct Combination 0.44 ±0.00 0.54±0.01 0.16±0.00 0.57±0.01 0.97±0.00 1.96±0.02 1.60±0.00 2.98±0.01
Top-3 Direct Combination 0.45 ±0.00 0.55±0.00 0.17±0.00 0.63±0.01 0.97±0.00 1.97±0.01 1.73±0.02 3.25±0.10
Top-2 Optimised Combination 0.43 ±0.00 0.54±0.00 0.17±0.01 0.53±0.01 0.98±0.01 2.00±0.02 1.58±0.01 2.96±0.02
Top-3 Optimised Combination 0.44 ±0.00 0.55±0.00 0.21±0.00 0.57±0.00 0.99±0.01 1.93±0.03 1.63±0.01 3.25±0.01
Table 8: CV classification: NLL (↓)comparison on in-distribution (ID) and out-of-distribution (OOD) test
sets and with tuned hyperparameters.
Noise TypeScores Ranks
ID OOD Sketch ID OOD Sketch
No Noise 54.15 ±0.36 75.65 ±0.19 89.61 ±0.35 13.0 13.0 16.0
Input Weak Aug. 39.73 ±0.27 67.73 ±0.13 85.90 ±0.56 5.0 6.0 6.0
Input Gaussian 53.28 ±0.24 75.12 ±0.06 89.29 ±0.07 10.0 10.0 13.0
Input ODS 66.47 ±15.18 82.46 ±8.72 89.54 ±0.61 17.0 17.0 15.0
Input AugMix 42.49 ±0.24 61.58 ±0.16 83.12 ±0.23 6.0 5.0 4.0
Input-Target MixUp 54.02 ±0.53 75.33 ±0.44 89.27 ±0.09 12.0 11.0 12.0
Label Smoothing 52.47 ±0.21 73.96 ±0.15 88.15 ±0.18 9.0 9.0 9.0
Activation Gaussian 53.96 ±0.16 75.45 ±0.14 89.01 ±0.35 11.0 12.0 10.0
Activation Dropout 43.26 ±0.35 69.32 ±0.32 87.60 ±0.61 7.0 7.0 8.0
Gradient Gaussian 59.99 ±0.38 80.15 ±0.21 92.16 ±0.16 16.0 16.0 17.0
Model 49.66 ±0.34 72.44 ±0.15 86.98 ±0.15 8.0 8.0 7.0
Weight Gaussian 54.20 ±0.11 75.71 ±0.13 89.46 ±0.32 14.0 15.0 14.0
Weight DropConnect 54.33 ±0.70 75.70 ±0.43 89.24 ±0.27 15.0 14.0 11.0
Top-2 Direct Combination 37.18 ±0.04 58.16 ±0.12 82.52 ±0.65 3.0 2.0 2.0
Top-3 Direct Combination 37.43 ±0.12 58.73 ±0.19 83.46 ±0.17 4.0 3.0 5.0
Top-2 Optimised Combination 36.88 ±0.08 57.90 ±0.24 82.30 ±0.15 2.0 1.0 1.0
Top-3 Optimised Combination 36.59 ±0.06 60.36 ±0.17 83.02 ±0.46 1.0 4.0 3.0
Table9: TinyImageNetclassification: Error (↓,%)comparisononin-distribution(ID)andout-of-distribution
(OOD) and Sketch test sets and with tuned hyperparameters.
23Published in Transactions on Machine Learning Research (04/2024)
Noise TypeScores Ranks
ID OOD Sketch ID OOD Sketch
No Noise 16.43 ±0.27 10.11 ±0.11 8.67±0.46 14.0 4.0 5.0
Input Weak Aug. 6.49 ±0.28 11.46 ±0.25 18.10 ±0.52 4.0 8.0 10.0
Input Gaussian 15.96 ±1.37 10.15 ±0.45 7.53±1.13 12.0 6.0 3.0
Input ODS 26.97 ±19.11 28.20 ±26.53 11.31 ±1.74 16.0 16.0 9.0
Input AugMix 4.87 ±0.31 15.78 ±0.31 20.08 ±0.82 3.0 11.0 11.0
Input-Target MixUp 16.33 ±1.40 10.26 ±0.32 7.28±1.06 13.0 7.0 2.0
Label Smoothing 29.45 ±0.17 16.62 ±0.03 5.28±0.21 17.0 12.0 1.0
Activation Gaussian 14.77 ±0.76 9.83±0.29 9.04±0.99 9.0 2.0 7.0
Activation Dropout 10.31 ±0.66 20.13 ±0.86 34.71 ±2.38 6.0 14.0 15.0
Gradient Gaussian 23.56 ±0.24 34.33 ±0.22 47.82 ±1.24 15.0 17.0 17.0
Model 10.80 ±0.24 9.01±0.03 9.90±0.12 7.0 1.0 8.0
Weight Gaussian 14.95 ±1.03 9.98±0.33 8.99±1.99 10.0 3.0 6.0
Weight DropConnect 15.50 ±0.83 10.12 ±0.35 7.76±0.72 11.0 5.0 4.0
Top-2 Direct Combination 4.04 ±0.27 15.69 ±0.34 25.12 ±0.91 2.0 10.0 13.0
Top-3 Direct Combination 13.05 ±1.09 22.67 ±1.96 39.54 ±0.43 8.0 15.0 16.0
Top-2 Optimised Combination 3.21 ±0.39 15.33 ±0.05 24.93 ±1.23 1.0 9.0 12.0
Top-3 Optimised Combination 9.90 ±0.42 20.11 ±0.26 34.64 ±0.59 5.0 13.0 14.0
Table 10: TinyImageNet classification: ECE (↓,%)comparison on in-distribution (ID) and out-of-
distribution (OOD) and Sketch test sets and with tuned hyperparameters.
Noise TypeScores Ranks
ID OOD Sketch ID OOD Sketch
No Noise 2.80 ±0.02 3.90±0.01 4.68±0.01 14.0 14.0 8.0
Input Weak Aug. 1.84 ±0.02 3.46±0.02 4.69±0.02 5.0 6.0 9.0
Input Gaussian 2.75 ±0.06 3.86±0.02 4.65±0.02 10.0 9.0 5.0
Input ODS 4.79 ±2.79 6.23±3.27 4.72±0.06 17.0 17.0 12.0
Input AugMix 1.87 ±0.01 3.14±0.01 4.52±0.01 6.0 3.0 1.0
Input-Target MixUp 2.79 ±0.07 3.88±0.04 4.66±0.01 13.0 11.0 6.0
Label Smoothing 3.25 ±0.02 4.15±0.01 4.70±0.00 16.0 15.0 10.0
Activation Gaussian 2.74 ±0.02 3.87±0.01 4.64±0.03 9.0 10.0 3.0
Activation Dropout 1.97 ±0.02 3.69±0.05 5.48±0.19 7.0 8.0 15.0
Gradient Gaussian 3.20 ±0.02 5.36±0.03 7.53±0.11 15.0 16.0 17.0
Model 2.38 ±0.01 3.64±0.01 4.54±0.01 8.0 7.0 2.0
Weight Gaussian 2.76 ±0.03 3.89±0.02 4.67±0.05 11.0 12.0 7.0
Weight DropConnect 2.78 ±0.06 3.90±0.04 4.64±0.02 12.0 13.0 4.0
Top-2 Direct Combination 1.60 ±0.00 2.98±0.01 4.72±0.09 2.0 2.0 13.0
Top-3 Direct Combination 1.73 ±0.02 3.25±0.10 5.75±0.03 4.0 5.0 16.0
Top-2 Optimised Combination 1.58 ±0.01 2.96±0.02 4.71±0.05 1.0 1.0 11.0
Top-3 Optimised Combination 1.63 ±0.01 3.25±0.01 5.22±0.07 3.0 4.0 14.0
Table 11: TinyImageNet classification: NLL (↓)comparison on in-distribution (ID), out-of-distribution
(OOD) and Sketch test sets and with tuned hyperparameters.
Metric ID vs OOD ID vs Sketch OOD vs Sketch
Error 0.912 ±0.000 0.794 ±0.000 0.824 ±0.000
ECE 0.118 ±0.542 -0.397 ±0.027 0.397 ±0.027
NLL 0.878 ±0.000 0.029 ±0.903 0.081 ±0.650
Table 12: TinyImageNet classification: Kendall Tau correlation between ID, OOD and Sketch rankings of
different noise types.
24Published in Transactions on Machine Learning Research (04/2024)
Noise TypeCIFAR-10 CIFAR-100 TinyImageNet
ID OOD ID OOD ID OOD
No Noise 16.09 ±0.18 30.81 ±0.76 40.97 ±0.40 61.56 ±0.11 54.70 ±0.81 75.96 ±0.52
Input Weak Aug. 7.99 ±0.07 27.60 ±0.42 24.03 ±0.09 52.82 ±0.13 39.60 ±0.26 67.41 ±0.02
Input Gaussian 16.72 ±0.13 30.28 ±0.85 41.07 ±0.25 61.04 ±0.14 54.01 ±0.60 75.31 ±0.33
Input ODS 15.79 ±0.13 29.54 ±0.71 41.13 ±0.43 60.43 ±0.39 52.77 ±0.54 74.61 ±0.11
Input AugMix 10.26 ±0.04 20.95 ±0.06 30.18 ±0.46 45.94 ±0.29 40.04 ±0.29 60.67 ±0.15
Input-Target MixUp 16.90 ±0.17 32.62 ±0.36 39.04 ±0.30 58.48 ±0.03 51.49 ±0.27 72.87 ±0.12
Label Smoothing 17.18 ±0.13 31.10 ±0.68 42.00 ±0.09 61.49 ±0.30 52.33 ±0.14 73.95 ±0.08
Activation Gaussian 16.34 ±0.15 30.52 ±0.79 38.98 ±0.19 59.70 ±0.22 52.49 ±0.23 74.62 ±0.21
Activation Dropout 12.45 ±0.12 27.90 ±0.10 31.58 ±0.54 56.38 ±0.36 51.85 ±0.10 74.08 ±0.01
Gradient Gaussian 18.70 ±0.15 34.04 ±0.49 47.64 ±0.31 67.79 ±0.22 55.91 ±0.25 76.96 ±0.06
Model 13.11 ±0.25 32.40 ±0.27 79.31 ±27.85 87.90 ±15.70 48.86 ±0.13 72.07 ±0.12
Weight Gaussian 16.54 ±0.11 30.86 ±0.29 37.14 ±0.19 58.32 ±0.48 52.88 ±0.45 74.82 ±0.18
Weight DropConnect 16.79 ±0.28 28.80 ±0.80 41.18 ±0.56 61.39 ±0.02 53.67 ±0.19 75.28 ±0.14
Table 13: CV classification: Error (↓,%)comparison on in-distribution (ID) and out-of-distribution (OOD)
test sets and with hyperparameters transferred across datasets.
Noise TypeCIFAR-10 CIFAR-100 TinyImageNet
ID OOD ID OOD ID OOD
No Noise 9.76 ±0.07 20.73 ±0.67 12.77 ±0.86 10.73 ±0.38 20.11 ±2.25 11.66 ±0.63
Input Weak Aug. 5.40 ±0.09 20.98 ±0.62 3.65±0.38 11.12 ±0.59 5.97±0.40 11.75 ±0.48
Input Gaussian 10.39 ±0.15 20.21 ±0.76 12.81 ±0.25 10.66 ±0.16 16.74 ±0.67 10.72 ±0.16
Input ODS 9.89 ±0.12 20.40 ±0.75 11.18 ±2.18 10.28 ±1.07 15.98 ±0.37 10.52 ±0.22
Input AugMix 5.53 ±0.08 11.65 ±0.07 8.94±0.07 8.86±0.05 3.97±0.20 15.43 ±0.53
Input-Target MixUp 6.27 ±0.51 8.12±0.20 14.76 ±0.48 10.99 ±0.06 19.33 ±0.27 10.08 ±0.32
Label Smoothing 1.65 ±0.20 6.05±0.46 21.53 ±0.30 15.64 ±0.20 27.18 ±0.27 15.49 ±0.11
Activation Gaussian 10.03 ±0.06 20.36 ±0.69 5.07±0.19 9.29±0.35 14.04 ±0.72 9.88±0.11
Activation Dropout 9.41 ±0.17 21.79 ±0.11 7.94±0.39 19.81 ±0.47 16.67 ±0.07 11.00 ±0.12
Gradient Gaussian 12.77 ±0.20 24.59 ±0.50 8.38±0.33 18.02 ±0.19 13.41 ±0.73 10.07 ±0.14
Model 7.53 ±0.26 21.33 ±0.36 0.80±1.10 4.07±5.73 14.45 ±0.23 10.68 ±0.07
Weight Gaussian 10.05 ±0.11 20.70 ±0.23 11.79 ±1.29 22.28 ±1.77 15.55 ±0.55 10.38 ±0.30
Weight DropConnect 10.93 ±0.26 19.75 ±0.67 11.24 ±0.70 9.93±0.18 18.10 ±0.20 11.11 ±0.10
Table 14: CV classification: ECE (↓,%)comparison on in-distribution (ID) and out-of-distribution (OOD)
test sets and with hyperparameters transferred across datasets.
25Published in Transactions on Machine Learning Research (04/2024)
Noise TypeCIFAR-10 CIFAR-100 TinyImageNet
ID OOD ID OOD ID OOD
No Noise 0.71 ±0.01 1.55±0.05 1.88±0.02 2.87±0.01 2.96±0.12 3.99±0.06
Input Weak Aug. 0.40 ±0.00 1.84±0.09 1.02±0.01 2.45±0.02 1.80±0.01 3.43±0.02
Input Gaussian 0.72 ±0.01 1.50±0.06 1.89±0.01 2.84±0.01 2.81±0.06 3.91±0.03
Input ODS 0.71 ±0.00 1.55±0.07 1.87±0.07 2.80±0.05 2.72±0.02 3.85±0.00
Input AugMix 0.39 ±0.00 0.84±0.00 1.30±0.01 2.02±0.01 1.73±0.01 3.10±0.02
Input-Target MixUp 0.56 ±0.01 1.04±0.02 1.71±0.01 2.59±0.00 2.62±0.02 3.65±0.01
Label Smoothing 0.56 ±0.00 1.01±0.02 2.16±0.01 3.04±0.01 3.16±0.00 4.10±0.01
Activation Gaussian 0.70 ±0.01 1.51±0.06 1.66±0.01 2.73±0.02 2.64±0.03 3.81±0.02
Activation Dropout 0.74 ±0.02 1.96±0.02 1.24±0.02 2.69±0.03 2.70±0.01 3.83±0.00
Gradient Gaussian 0.93 ±0.01 1.99±0.04 1.93±0.01 3.20±0.02 2.83±0.02 3.98±0.01
Model 0.52 ±0.00 1.43±0.02 3.59±1.44 4.07±0.75 2.46±0.01 3.69±0.01
Weight Gaussian 0.72 ±0.01 1.54±0.03 1.54±0.03 2.81±0.07 2.72±0.04 3.86±0.02
Weight DropConnect 0.79 ±0.01 1.48±0.06 1.86±0.01 2.85±0.00 2.83±0.01 3.92±0.00
Table 15: CV classification: NLL (↓)comparison on in-distribution (ID) and out-of-distribution (OOD) test
sets and with hyperparameters transferred across datasets.
Noise TypeSVHN CIFAR-100 TinyImageNet
ID OOD ID OOD ID OOD
No Noise 5.12 ±0.13 9.20±0.10 43.87 ±0.48 61.25 ±0.46 53.63 ±0.17 75.20 ±0.32
Input Weak Aug. 4.13 ±0.10 8.51±0.23 27.33 ±0.35 51.09 ±0.18 38.21 ±0.24 64.75 ±0.29
Input Gaussian 5.01 ±0.08 9.09±0.06 43.05 ±0.35 58.71 ±0.32 54.15 ±1.59 74.61 ±1.09
Input AugMix 3.51 ±0.05 8.27±0.04 30.23 ±0.06 45.51 ±0.17 42.05 ±0.29 59.93 ±0.10
Input-Target MixUp 5.58 ±0.12 12.75 ±0.11 43.93 ±0.68 59.98 ±0.23 52.95 ±1.26 73.82 ±0.54
Label Smoothing 5.04 ±0.03 8.88±0.01 42.99 ±0.36 57.27 ±0.43 53.77 ±0.32 74.25 ±0.11
Activation Gaussian 5.14 ±0.08 9.17±0.06 43.74 ±0.31 59.29 ±0.65 56.32 ±2.03 76.49 ±1.23
Activation Dropout 4.37 ±0.02 8.16±0.04 42.89 ±0.90 58.26 ±0.34 42.47 ±0.35 67.78 ±0.25
Gradient Gaussian 6.25 ±0.10 11.43 ±0.14 44.39 ±0.73 59.56 ±0.87 57.92 ±0.75 77.81 ±0.34
Model 3.98 ±0.02 8.11±0.04 37.14 ±0.33 56.97 ±0.16 47.11 ±0.53 69.76 ±0.09
Weight Gaussian 5.04 ±0.04 9.08±0.06 44.28 ±0.31 59.84 ±0.08 54.11 ±2.41 74.52 ±1.42
Weight DropConnect 5.11 ±0.11 9.03±0.11 45.75 ±0.89 59.71 ±1.15 56.44 ±3.41 76.12 ±2.55
Table 16: CV classification: Error (↓,%)comparison on in-distribution (ID) and out-of-distribution (OOD)
test sets and with hyperparameters transferred across architectures.
26Published in Transactions on Machine Learning Research (04/2024)
Noise TypeSVHN CIFAR-100 TinyImageNet
ID OOD ID OOD ID OOD
No Noise 2.73 ±0.10 5.13±0.06 5.76±0.34 7.50±0.11 14.68 ±0.47 9.93±0.08
Input Weak Aug. 2.66 ±0.05 5.75±0.09 6.33±0.32 15.26 ±0.80 8.20±1.23 9.88±0.38
Input Gaussian 2.68 ±0.05 5.09±0.03 5.07±0.61 9.59±1.63 13.79 ±0.49 8.75±0.46
Input AugMix 1.46 ±0.04 3.04±0.07 6.56±0.71 14.17 ±0.59 6.23±0.63 16.60 ±0.23
Input-Target MixUp 5.61 ±1.19 5.27±0.90 2.68±0.26 6.77±0.43 12.24 ±0.82 8.32±0.10
Label Smoothing 0.95 ±0.07 1.16±0.03 10.18 ±2.84 8.54±1.72 17.47 ±0.22 9.76±0.12
Activation Gaussian 2.75 ±0.05 5.11±0.07 5.45±1.23 10.46 ±1.78 10.66 ±3.63 8.42±0.89
Activation Dropout 3.03 ±0.02 5.72±0.02 5.49±0.32 10.00 ±0.41 14.68 ±0.17 25.16 ±0.75
Gradient Gaussian 3.61 ±0.11 7.07±0.15 5.59±1.20 11.16 ±1.73 21.10 ±0.45 31.33 ±0.35
Model 2.38 ±0.05 4.78±0.04 7.59±0.52 13.21 ±0.40 5.63±0.31 10.34 ±0.44
Weight Gaussian 2.67 ±0.03 5.06±0.02 5.30±0.20 9.87±0.53 12.42 ±0.87 8.24±0.34
Weight DropConnect 2.91 ±0.08 5.38±0.07 6.98±1.36 11.51 ±1.69 11.42 ±2.14 7.99±0.82
Table 17: CV classification: ECE (↓,%)comparison on in-distribution (ID) and out-of-distribution (OOD)
test sets and with hyperparameters transferred across architectures.
Noise TypeSVHN CIFAR-100 TinyImageNet
ID OOD ID OOD ID OOD
No Noise 0.23 ±0.01 0.41±0.00 1.83±0.03 2.69±0.02 2.72±0.03 3.86±0.02
Input Weak Aug. 0.23 ±0.00 0.48±0.00 1.09±0.01 2.36±0.02 1.84±0.05 3.28±0.04
Input Gaussian 0.23 ±0.00 0.41±0.00 1.75±0.03 2.56±0.02 2.67±0.08 3.76±0.06
Input AugMix 0.16 ±0.00 0.31±0.00 1.19±0.01 2.05±0.00 1.90±0.01 3.05±0.01
Input-Target MixUp 0.24 ±0.01 0.46±0.01 1.76±0.03 2.58±0.01 2.56±0.03 3.68±0.03
Label Smoothing 0.19 ±0.00 0.31±0.00 1.99±0.06 2.65±0.06 2.82±0.02 3.86±0.01
Activation Gaussian 0.23 ±0.00 0.41±0.00 1.78±0.02 2.59±0.04 2.72±0.19 3.84±0.09
Activation Dropout 0.26 ±0.00 0.48±0.00 1.76±0.04 2.55±0.01 2.03±0.00 3.70±0.04
Gradient Gaussian 0.30 ±0.01 0.57±0.02 1.79±0.03 2.60±0.04 2.94±0.05 4.79±0.04
Model 0.18 ±0.00 0.35±0.00 1.59±0.01 2.67±0.01 2.10±0.02 3.46±0.00
Weight Gaussian 0.23 ±0.00 0.40±0.00 1.81±0.02 2.61±0.01 2.63±0.13 3.74±0.09
Weight DropConnect 0.24 ±0.01 0.43±0.01 1.87±0.04 2.61±0.04 2.73±0.16 3.82±0.15
Table 18: CV classification: NLL (↓)comparison on in-distribution (ID) and out-of-distribution (OOD) test
set and with hyperparameters transferred across architectures.
Noise TypeWine Toxicity Abalone Students Adult
ID OOD ID OOD ID OOD ID OOD ID OOD
No Noise 35.73 ±2.20 36.37 ±2.07 47.06 ±8.32 44.98 ±7.10 43.30 ±0.29 43.34 ±0.20 65.40 ±4.88 65.89 ±3.72 15.54 ±0.23 15.73 ±0.19
Input Gaussian 35.94 ±2.43 36.28 ±2.15 47.06 ±8.32 44.78 ±7.26 43.30 ±0.43 43.53 ±0.26 65.40 ±4.88 65.81 ±3.62 15.58 ±0.42 15.74 ±0.37
Input ODS 35.83 ±1.64 36.17 ±1.53 48.04 ±5.55 45.65 ±5.41 43.38 ±0.73 43.61 ±0.60 64.56 ±3.73 65.74 ±3.30 15.47 ±0.39 15.67 ±0.34
Input-Target MixUp 38.02 ±0.82 38.12 ±0.80 50.98 ±5.00 46.47 ±4.29 43.46 ±0.45 43.55 ±0.40 65.40 ±2.60 66.13 ±2.87 15.28 ±0.33 15.44 ±0.33
Label Smoothing 35.52 ±2.37 36.05 ±2.30 50.00 ±4.16 46.59 ±4.20 43.22 ±0.20 43.41 ±0.24 64.98 ±4.30 65.69 ±3.52 15.55 ±0.35 15.74 ±0.31
Activation Gaussian 35.83 ±1.74 36.12 ±1.49 47.06 ±8.32 44.82 ±7.43 43.22 ±0.54 43.41 ±0.29 64.56 ±4.51 66.46 ±4.38 15.77 ±0.40 15.91 ±0.42
Activation Dropout 36.04 ±2.23 36.40 ±1.84 56.86 ±15.62 56.27 ±15.36 43.54 ±1.28 43.69 ±1.06 71.73 ±3.32 72.44 ±3.51 14.82 ±0.32 14.92 ±0.31
Gradient Gaussian 32.60 ±0.53 33.97 ±1.19 50.00 ±4.16 49.10 ±1.22 43.74 ±0.11 43.86 ±0.11 68.78 ±5.69 68.56 ±4.60 15.38 ±0.31 15.52 ±0.29
Model 37.92 ±1.28 38.15 ±1.31 45.10 ±6.04 43.49 ±6.16 43.42 ±0.20 43.66 ±0.26 65.82 ±4.51 65.86 ±3.57 14.72 ±0.46 14.81 ±0.38
Weight Gaussian 35.52 ±1.95 36.16 ±1.75 50.00 ±6.35 49.06 ±5.41 43.10 ±0.39 43.36 ±0.29 64.98 ±4.88 66.80 ±4.25 15.13 ±0.31 15.23 ±0.24
Weight DropConnect 38.85 ±0.53 38.99 ±0.77 47.06 ±8.32 44.90 ±6.87 43.42 ±0.78 43.36 ±0.59 64.56 ±4.51 66.58 ±4.40 14.93 ±0.32 15.04 ±0.29
Top-2 Direct Combination 37.50 ±1.53 37.88 ±1.28 46.08 ±5.00 44.00 ±5.17 43.18 ±0.68 43.41 ±0.67 64.56 ±4.51 65.82 ±3.69 14.61 ±0.30 14.77 ±0.27
Top-3 Direct Combination 38.23 ±1.03 38.46 ±0.94 31.37 ±7.72 31.37 ±7.72 43.06 ±0.88 43.39 ±0.82 69.20 ±2.60 69.35 ±2.21 14.83 ±0.31 14.91 ±0.29
Top-2 Optimised Combination 37.60 ±2.12 37.79 ±2.36 50.00 ±6.35 47.02 ±5.47 43.22 ±0.56 43.40 ±0.32 65.82 ±4.51 66.23 ±3.77 14.64 ±0.35 14.74 ±0.33
Top-3 Optimised Combination 39.90 ±2.08 39.96 ±1.80 49.02 ±6.93 44.47 ±7.36 43.26 ±0.66 43.56 ±0.41 82.70 ±5.69 82.75 ±5.58 14.64 ±0.32 14.74 ±0.33
Table 19: Tabular data classification: error (↓,%)comparison on in-distribution (ID) and out-of-distribution
(OOD) test sets and with tuned hyperparameters.
27Published in Transactions on Machine Learning Research (04/2024)
Noise TypeWine Toxicity Abalone Students Adult
ID OOD ID OOD ID OOD ID OOD ID OOD
No Noise 8.75 ±1.34 9.07±0.96 45.78 ±8.12 44.18 ±6.80 3.94±0.59 4.19±0.61 12.46 ±2.92 13.81 ±2.17 3.47±0.29 3.85±0.34
Input Gaussian 8.56 ±0.60 8.82±1.13 45.84 ±8.40 43.93 ±7.07 3.60±0.41 4.05±0.67 13.36 ±0.44 14.37 ±1.68 3.50±0.44 3.84±0.42
Input ODS 5.75 ±0.71 6.02±0.52 42.82 ±4.17 39.93 ±3.67 3.04±0.75 3.55±1.01 14.04 ±2.38 14.07 ±1.77 3.64±0.46 3.89±0.41
Input-Target MixUp 4.43 ±0.77 4.76±0.77 43.18 ±5.18 41.22 ±3.89 3.41±1.00 3.86±0.92 11.07 ±3.74 11.74 ±3.19 2.79±0.36 3.06±0.43
Label Smoothing 4.82 ±0.61 5.47±1.60 42.08 ±4.15 38.75 ±4.52 2.77±0.74 3.42±0.88 12.69 ±2.59 13.81 ±1.98 3.32±0.32 3.62±0.36
Activation Gaussian 9.27 ±1.24 9.52±1.12 46.46 ±8.05 44.11 ±7.04 3.17±0.96 3.79±0.87 11.91 ±0.96 13.62 ±0.49 3.74±0.57 4.05±0.56
Activation Dropout 6.93 ±1.24 7.26±1.51 23.26 ±16.83 23.81 ±16.23 3.48±1.96 3.75±1.55 9.75±2.95 9.25±3.16 0.95±0.06 1.10±0.10
Gradient Gaussian 16.96 ±1.93 18.15 ±1.56 48.32 ±3.18 48.00 ±0.99 5.30±1.57 5.87±1.71 17.58 ±4.99 18.03 ±4.63 2.81±0.53 3.13±0.52
Model 4.73 ±1.73 5.30±1.36 39.66 ±2.95 37.93 ±3.67 2.80±1.09 3.40±0.90 12.35 ±2.63 13.65 ±1.99 1.49±0.51 1.62±0.43
Weight Gaussian 9.56 ±1.17 9.36±1.35 48.77 ±4.79 46.12 ±4.70 3.48±0.55 3.90±0.54 12.27 ±1.18 13.49 ±0.76 2.36±0.25 2.61±0.22
Weight DropConnect 5.65 ±3.34 5.26±2.58 46.47 ±7.40 44.01 ±6.62 3.72±1.47 3.96±1.12 12.73 ±1.63 13.75 ±0.73 1.70±0.40 1.92±0.41
Top-2 Direct Combination 4.58 ±0.47 4.19±0.42 32.77 ±1.78 32.93 ±2.41 2.80±0.81 3.07±0.86 11.78 ±1.43 11.99 ±0.40 1.39±0.33 1.50±0.32
Top-3 Direct Combination 3.77 ±0.90 3.95±0.62 15.99 ±9.87 16.13 ±9.60 2.65±1.51 2.88±1.24 12.44 ±1.68 12.01 ±1.35 0.95±0.21 1.10±0.19
Top-2 Optimised Combination 5.01 ±1.16 5.51±1.07 45.30 ±7.41 43.41 ±6.21 2.98±0.83 3.69±0.87 10.68 ±0.62 11.72 ±0.08 1.25±0.38 1.43±0.35
Top-3 Optimised Combination 7.39 ±1.06 7.26±1.48 38.88 ±6.65 36.78 ±6.77 3.08±1.34 3.62±0.92 2.56±1.82 2.70±1.82 1.17±0.19 1.38±0.23
Table 20: Tabular data classification: ECE (↓,%)comparison on in-distribution (ID) and out-of-distribution
(OOD) test sets and with tuned hyperparameters.
Noise TypeWine Toxicity Abalone Students Adult
ID OOD ID OOD ID OOD ID OOD ID OOD
No Noise 0.94 ±0.05 0.95±0.04 4.85±0.23 4.87±0.27 0.84±0.02 0.86±0.02 1.87±0.08 1.98±0.07 0.35±0.01 0.36±0.01
Input Gaussian 0.94 ±0.05 0.95±0.05 4.85±0.27 4.87±0.28 0.84±0.02 0.86±0.02 1.87±0.08 1.97±0.07 0.34±0.01 0.36±0.01
Input ODS 0.91 ±0.05 0.92±0.05 2.20±0.33 2.35±0.39 0.84±0.02 0.85±0.02 1.87±0.08 1.98±0.07 0.35±0.01 0.36±0.01
Input-Target MixUp 0.91 ±0.04 0.91±0.04 2.39±0.27 2.44±0.19 0.84±0.02 0.86±0.02 1.88±0.04 1.96±0.04 0.34±0.01 0.35±0.01
Label Smoothing 0.93 ±0.04 0.94±0.04 1.76±0.22 1.78±0.31 0.84±0.02 0.85±0.02 1.87±0.08 1.97±0.07 0.34±0.01 0.35±0.01
Activation Gaussian 0.94 ±0.05 0.96±0.05 4.78±0.26 4.82±0.30 0.84±0.02 0.86±0.02 1.90±0.09 2.00±0.08 0.35±0.01 0.36±0.01
Activation Dropout 0.92 ±0.05 0.93±0.05 1.03±0.24 1.03±0.23 0.84±0.02 0.85±0.02 2.23±0.04 2.25±0.05 0.32±0.01 0.32±0.01
Gradient Gaussian 1.26 ±0.09 1.30±0.08 5.41±0.16 5.65±0.05 0.86±0.03 0.88±0.04 1.95±0.06 2.05±0.05 0.34±0.01 0.35±0.01
Model 0.92 ±0.04 0.93±0.04 2.06±0.37 2.14±0.43 0.84±0.02 0.85±0.02 1.87±0.08 1.98±0.07 0.31±0.01 0.32±0.01
Weight Gaussian 0.95 ±0.05 0.96±0.05 3.02±0.20 3.15±0.25 0.84±0.02 0.86±0.02 1.91±0.08 2.01±0.08 0.33±0.01 0.34±0.01
Weight DropConnect 0.94 ±0.04 0.94±0.04 4.82±0.17 4.83±0.23 0.84±0.02 0.85±0.02 1.91±0.09 2.01±0.08 0.32±0.01 0.33±0.01
Top-2 Direct Combination 0.94 ±0.03 0.95±0.03 1.57±0.24 1.62±0.27 0.84±0.02 0.85±0.02 1.87±0.06 1.97±0.05 0.31±0.01 0.32±0.01
Top-3 Direct Combination 0.94 ±0.03 0.95±0.03 0.93±0.22 0.93±0.22 0.84±0.02 0.84±0.02 2.20±0.05 2.23±0.05 0.32±0.01 0.32±0.01
Top-2 Optimised Combination 0.92 ±0.05 0.93±0.04 2.47±0.41 2.50±0.45 0.84±0.02 0.85±0.02 1.86±0.05 1.96±0.05 0.31±0.01 0.32±0.01
Top-3 Optimised Combination 0.96 ±0.04 0.96±0.04 2.05±0.27 2.25±0.34 0.84±0.02 0.85±0.02 2.54±0.15 2.54±0.14 0.31±0.01 0.32±0.01
Table 21: Tabular data classification: NLL (↓)comparison on in-distribution (ID) and out-of-distribution
(OOD) test sets and with tuned hyperparameters.
Noise TypeNewsGroup SST
GP-CNN Transformer GP-CNN Transformer
No Noise 35.67 ±1.13 36.56 ±0.83 19.15 ±0.75 21.60 ±0.24
Input Gaussian 35.44 ±0.87 36.59 ±0.84 21.14 ±0.33 21.52 ±0.24
Input ODS 33.56 ±0.33 34.44 ±0.74 18.46 ±1.03 21.52 ±1.22
Input-Target MixUp 35.44 ±0.78 36.70 ±0.92 18.85 ±0.11 21.33 ±0.41
Label Smoothing 35.59 ±0.69 36.56 ±1.10 21.75 ±0.53 21.29 ±0.38
Activation Gaussian 35.56 ±1.16 36.44 ±0.87 19.61 ±1.06 21.64 ±0.24
Activation Dropout 39.19 ±0.92 36.48 ±0.29 19.61 ±0.25 21.02 ±0.11
Gradient Gaussian 40.56 ±0.18 36.52 ±0.89 21.90 ±0.82 21.41 ±0.91
Model 40.22 ±0.64 36.52 ±0.76 19.57 ±0.59 21.67 ±0.25
Weight Gaussian 35.48 ±0.53 36.89 ±0.79 20.18 ±0.19 21.56 ±0.32
Weight DropConnect 35.19 ±0.69 37.04 ±0.68 20.15 ±0.81 21.18 ±0.39
Top-2 Direct Combination 39.52 ±0.69 36.41 ±0.52 18.77 ±1.00 20.64 ±0.50
Top-3 Direct Combination 36.96 ±0.73 34.67 ±0.74 17.58 ±0.44 20.22 ±0.61
Top-2 Optimised Combination 38.04 ±1.12 36.52 ±1.30 20.41 ±0.50 21.06 ±0.33
Top-3 Optimised Combination 36.00 ±0.48 35.19 ±0.37 18.85 ±0.78 19.07 ±0.61
Table 22: NewsGroup NLP classification: Error (↓,%)comparison on in-distribution (ID) test set and with
tuned hyperparameters.
28Published in Transactions on Machine Learning Research (04/2024)
Noise TypeNewsGroup SST
GP-CNN Transformer GP-CNN Transformer
No Noise 5.12 ±0.53 3.47±0.98 13.99 ±0.47 11.61 ±3.97
Input Gaussian 4.78 ±1.30 3.59±0.83 15.14 ±0.17 11.32 ±3.72
Input ODS 2.57 ±0.81 7.99±0.59 11.80 ±0.93 14.70 ±2.64
Input-Target MixUp 5.07 ±0.59 2.54±1.27 7.76±0.32 11.75 ±3.55
Label Smoothing 3.78 ±0.50 4.13±0.94 9.10±0.87 10.26 ±3.30
Activation Gaussian 5.75 ±0.50 3.42±1.05 13.99 ±1.22 11.39 ±3.71
Activation Dropout 7.19 ±1.29 2.26±0.20 11.50 ±0.11 7.42±1.24
Gradient Gaussian 24.26 ±1.01 3.24±1.07 17.40 ±0.93 12.20 ±3.46
Model 2.91 ±0.90 3.55±0.84 14.00 ±0.50 11.16 ±3.62
Weight Gaussian 4.57 ±0.48 4.01±0.79 14.77 ±0.04 12.09 ±3.68
Weight DropConnect 5.41 ±0.76 4.43±0.72 16.08 ±1.02 11.27 ±3.01
Top-2 Direct Combination 8.25 ±0.67 2.91±0.33 4.55±1.11 6.40±0.93
Top-3 Direct Combination 13.76 ±0.67 11.19 ±0.88 2.12±0.78 8.70±2.85
Top-2 Optimised Combination 3.44 ±0.51 3.07±0.52 8.86±0.38 8.02±2.12
Top-3 Optimised Combination 2.75 ±0.05 2.89±0.42 9.33±0.90 9.18±0.61
Table 23: NewsGroup NLP classification: ECE (↓,%)comparison on in-distribution (ID) test set and with
tuned hyperparameters.
Noise TypeNewsGroup SST
GP-CNN Transformer GP-CNN Transformer
No Noise 1.14 ±0.01 1.13±0.02 0.81±0.04 0.61±0.10
Input Gaussian 1.12 ±0.01 1.13±0.02 0.85±0.05 0.59±0.09
Input ODS 1.03 ±0.00 1.10±0.01 0.62±0.02 0.73±0.12
Input-Target MixUp 1.13 ±0.01 1.13±0.02 0.48±0.00 0.59±0.08
Label Smoothing 1.12 ±0.01 1.13±0.02 0.52±0.01 0.54±0.05
Activation Gaussian 1.14 ±0.01 1.13±0.02 0.83±0.03 0.59±0.09
Activation Dropout 1.18 ±0.01 1.11±0.02 0.55±0.01 0.48±0.01
Gradient Gaussian 1.87 ±0.07 1.13±0.02 1.11±0.06 0.60±0.08
Model 1.21 ±0.01 1.13±0.02 0.85±0.02 0.59±0.09
Weight Gaussian 1.14 ±0.01 1.13±0.02 0.84±0.02 0.62±0.11
Weight DropConnect 1.13 ±0.01 1.13±0.02 1.05±0.07 0.58±0.07
Top-2 Direct Combination 1.19 ±0.01 1.12±0.01 0.43±0.02 0.46±0.01
Top-3 Direct Combination 1.18 ±0.01 1.15±0.01 0.40±0.01 0.51±0.04
Top-2 Optimised Combination 1.14 ±0.02 1.12±0.01 0.51±0.01 0.49±0.02
Top-3 Optimised Combination 1.09 ±0.01 1.10±0.01 0.48±0.02 0.50±0.01
Table 24: NewsGroup NLP classification: NLL (↓)comparison on in-distribution (ID) test set and with
tuned hyperparameters.
29Published in Transactions on Machine Learning Research (04/2024)
Noise TypeRotated CIFAR-100 WikiFace
ID OOD ID OOD
No Noise 0.03 ±0.00 0.13±0.01 0.03±0.00 0.04±0.00
Input Weak Aug. 0.03 ±0.00 0.13±0.01 0.03±0.00 0.04±0.00
Input Gaussian 0.03 ±0.00 0.14±0.01 0.04±0.00 0.05±0.00
Input AugMix 0.03 ±0.00 0.07±0.00 0.03±0.00 0.04±0.00
Input-Target CMixUp 0.03 ±0.00 0.09±0.00 0.04±0.00 0.04±0.00
Activation Gaussian 0.03 ±0.00 0.14±0.01 0.04±0.00 0.04±0.00
Activation Dropout 0.03 ±0.00 0.14±0.00 0.04±0.00 0.05±0.00
Gradient Gaussian 0.04 ±0.00 0.11±0.00 0.04±0.00 0.04±0.00
Model 0.04 ±0.00 0.16±0.01 0.04±0.00 0.04±0.00
Weight Gaussian 0.03 ±0.00 0.15±0.00 0.04±0.00 0.04±0.00
Weight DropConnect 0.03 ±0.00 0.12±0.01 0.10±0.04 0.11±0.04
Top-2 Direct Combination 0.03 ±0.00 0.08±0.00 0.04±0.00 0.04±0.00
Top-3 Direct Combination 0.03 ±0.00 0.08±0.01 0.04±0.00 0.04±0.00
Top-2 Optimised Combination 0.03 ±0.00 0.06±0.00 0.03±0.00 0.04±0.00
Top-3 Optimised Combination 0.24 ±0.15 0.29±0.10 0.04±0.00 0.04±0.00
Table25: RotatedCVregression: MSE (↓)comparisononin-distribution(ID)andout-of-distribution(OOD)
test sets and with tuned hyperparameters.
Noise TypeRotated CIFAR-100 WikiFace
ID OOD ID OOD
No Noise -4.81 ±0.00 6.90±1.52 27.03 ±2.94 31.43 ±3.80
Input Weak Aug. -4.60 ±0.06 3.41±0.53 -0.82 ±0.24 0.31±0.66
Input Gaussian -4.67 ±0.11 7.57±2.14 27.96 ±2.66 31.50 ±2.15
Input AugMix -4.82 ±0.01 -1.70 ±0.04 0.78±0.27 -0.12 ±0.11
Input-Target CMixUp -4.62 ±0.04 1.86±0.35 21.83 ±2.35 25.62 ±0.93
Activation Gaussian -4.27 ±0.07 2.94±0.18 14.93 ±1.09 17.61 ±1.01
Activation Dropout -3.81 ±0.55 1.04±0.59 -1.35 ±0.02 -0.53 ±0.36
Gradient Gaussian -3.70 ±0.00 -0.44 ±0.00 25.87 ±2.19 29.69 ±3.75
Model -4.36 ±0.05 3.44±0.76 -1.08 ±0.03 -1.08 ±0.03
Weight Gaussian -4.23 ±0.12 2.53±0.36 4.42±0.10 5.65±0.60
Weight DropConnect -2.28 ±1.93 39.88 ±24.73 4.83±3.37 6.00±4.13
Top-2 Direct Combination -4.14 ±0.23 -1.93 ±0.13 -1.34 ±0.01 -1.15 ±0.02
Top-3 Direct Combination -4.21 ±0.07 -1.76 ±0.01 -1.32 ±0.01 -1.16 ±0.02
Top-2 Optimised Combination -4.27 ±0.03 -1.71 ±0.05 -1.39 ±0.01 -1.12 ±0.02
Top-3 Optimised Combination 1.84 ±2.73 0.57±0.85 -1.34 ±0.03 -1.07 ±0.10
Table26: RotatedCVregression: NLL (↓)comparisononin-distribution(ID)andout-of-distribution(OOD)
test sets and with tuned hyperparameters.
30Published in Transactions on Machine Learning Research (04/2024)
Noise TypeEnergy Boston Wine Yacht Concrete
ID OOD ID OOD ID OOD ID OOD ID OOD
No Noise 0.04 ±0.00 0.05±0.00 0.13±0.02 0.15±0.02 0.25±0.04 29.42 ±25.71 0.07±0.07 0.16±0.05 0.10±0.01 0.10±0.01
Input Gaussian 0.04 ±0.00 0.05±0.00 0.11±0.02 0.13±0.02 0.25±0.04 29.39 ±25.66 0.02±0.02 0.09±0.07 0.10±0.01 0.10±0.01
Input-Target CMixUp 0.04 ±0.00 0.05±0.00 0.13±0.01 0.15±0.01 0.25±0.04 28.95 ±25.23 0.15±0.17 0.19±0.16 0.10±0.00 0.11±0.00
Activation Gaussian 0.04 ±0.00 0.05±0.00 0.13±0.03 0.15±0.02 0.25±0.04 29.57 ±25.82 0.03±0.02 0.09±0.07 0.10±0.01 0.10±0.01
Activation Dropout 0.04 ±0.00 0.05±0.00 0.16±0.05 0.18±0.04 0.38±0.06 17.02 ±13.89 0.03±0.01 0.09±0.07 0.10±0.01 0.11±0.01
Gradient Gaussian 0.04 ±0.00 0.04±0.00 0.24±0.13 0.29±0.11 0.25±0.04 28.34 ±24.70 0.01±0.01 0.09±0.08 0.10±0.01 0.10±0.01
Model 0.04 ±0.00 0.05±0.00 0.13±0.02 0.15±0.02 0.25±0.04 29.41 ±25.69 0.54±0.70 0.55±0.69 0.10±0.01 0.11±0.01
Weight Gaussian 0.04 ±0.00 0.05±0.00 0.12±0.04 0.15±0.03 0.25±0.04 29.51 ±25.77 0.02±0.01 0.06±0.05 0.10±0.01 0.10±0.01
Weight DropConnect 0.04 ±0.00 0.05±0.00 0.13±0.04 0.16±0.04 0.25±0.04 29.53 ±25.81 0.05±0.05 0.07±0.04 0.10±0.01 0.10±0.01
Top-2 Direct 0.04 ±0.00 0.05±0.00 0.15±0.02 0.18±0.02 0.25±0.04 29.43 ±25.69 0.03±0.03 0.10±0.07 0.10±0.01 0.10±0.01
Top-3 Direct 0.04 ±0.00 0.05±0.00 0.13±0.02 0.15±0.02 0.25±0.04 29.51 ±25.69 0.04±0.03 0.09±0.06 0.10±0.01 0.11±0.01
Top-2 Optimised 0.04 ±0.00 0.05±0.00 0.13±0.02 0.16±0.03 0.25±0.04 29.35 ±25.55 0.01±0.01 0.07±0.06 0.10±0.01 0.10±0.01
Top-3 Optimised 0.04 ±0.00 0.05±0.00 0.14±0.01 0.17±0.01 0.25±0.04 29.31 ±25.46 0.04±0.03 0.10±0.07 0.10±0.01 0.11±0.01
Table 27: Tabular regression: MSE (↓)comparison on in-distribution (ID) and out-of-distribution (OOD)
test sets and with tuned hyperparameters.
Noise TypeEnergy Boston Wine Yacht Concrete
ID OOD ID OOD ID OOD ID OOD ID OOD
No Noise -1.54 ±0.05 6.64±3.38 -0.19 ±0.12 0.04±0.12 -0.22 ±0.08 0.03±0.12 -1.18 ±0.22 10.45 ±16.46 -0.56 ±0.10 -0.01 ±0.24
Input Gaussian -1.55 ±0.03 6.55±1.53 -0.54 ±0.14 -0.40 ±0.18 -0.22 ±0.08 0.03±0.12 -1.46 ±0.19 -1.04 ±0.30 -0.53 ±0.11 0.21±0.41
Input-Target CMixUp -1.53 ±0.03 6.03±2.17 -0.42 ±0.12 -0.29 ±0.14 -0.19 ±0.09 81.98 ±58.22 -1.05 ±0.20 -0.87 ±0.31 -0.63 ±0.01 -0.26 ±0.26
Activation Gaussian -1.56 ±0.03 6.64±0.32 -0.20 ±0.17 0.04±0.11 -0.22 ±0.08 0.04±0.10 -1.31 ±0.15 -1.21 ±0.14 -0.56 ±0.09 0.08±0.44
Activation Dropout -1.53 ±0.05 4.44±2.01 -0.62 ±0.06 -0.59 ±0.06 0.01±0.05 0.03±0.04 -1.18 ±0.38 -1.07 ±0.39 -0.58 ±0.08 0.07±0.30
Gradient Gaussian -1.56 ±0.07 8.61±3.39 0.55±0.79 0.80±0.98 -0.22 ±0.08 0.04±0.09 -2.04 ±0.30 -1.49 ±0.32 -0.55 ±0.09 0.13±0.43
Model -1.55 ±0.04 7.20±3.32 -0.16 ±0.18 0.06±0.19 -0.22 ±0.08 0.03±0.11 -0.77 ±1.13 0.15±0.80 -0.59 ±0.06 -0.05 ±0.45
Weight Gaussian -1.56 ±0.04 6.82±2.72 -0.36 ±0.18 -0.11 ±0.21 -0.22 ±0.08 0.04±0.11 -2.08 ±0.51 -1.71 ±0.23 -0.54 ±0.08 0.14±0.39
Weight DropConnect -1.55 ±0.03 5.53±1.27 -0.62 ±0.05 -0.49 ±0.06 -0.22 ±0.08 0.02±0.12 -1.36 ±0.43 -1.25 ±0.33 -0.57 ±0.08 0.07±0.35
Top-2 Direct -1.55 ±0.04 6.09±2.35 0.32±0.19 0.63±0.30 -0.22 ±0.08 0.02±0.11 -1.37 ±0.34 0.35±2.06 -0.57 ±0.10 0.11±0.32
Top-3 Direct -1.53 ±0.03 4.53±2.51 -0.18 ±0.22 0.06±0.24 -0.22 ±0.08 0.03±0.12 -1.82 ±0.67 -1.39 ±0.22 -0.59 ±0.09 0.04±0.40
Top-2 Optimised -1.56 ±0.03 8.60±3.54 0.16±0.18 0.46±0.29 -0.22 ±0.08 0.03±0.11 -2.03 ±0.14 -1.55 ±0.21 -0.57 ±0.09 0.22±0.55
Top-3 Optimised -1.55 ±0.02 6.68±1.97 -0.27 ±0.46 -0.04 ±0.53 -0.22 ±0.08 0.03±0.12 -1.77 ±0.38 -0.40 ±1.62 -0.60 ±0.07 0.17±0.61
Table 28: Tabular regression: NLL (↓)comparison on in-distribution (ID) and out-of-distribution (OOD)
test sets and with tuned hyperparameters.
Noise TypeEnergy Wine Concrete
ID OOD ID OOD ID OOD
No Noise 0.03 ±0.01 0.03±0.01 0.21±0.01 37.20 ±33.29 0.09±0.01 0.10±0.01
Input Gaussian 0.03 ±0.01 0.03±0.01 0.21±0.02 45.36 ±43.45 0.08±0.01 0.09±0.01
Input-Target CMixUp 0.03 ±0.00 0.04±0.00 0.18±0.01 49.23 ±37.16 0.10±0.01 0.11±0.00
Activation Gaussian 0.03 ±0.01 0.03±0.01 0.22±0.01 43.40 ±45.61 0.09±0.02 0.10±0.01
Activation Dropout 0.03 ±0.01 0.03±0.00 0.24±0.02 42.16 ±33.28 0.09±0.02 0.10±0.01
Gradient Gaussian 0.03 ±0.01 0.04±0.01 311.38 ±405.17 1858.44 ±2585.50 0.09±0.02 0.10±0.02
Model 0.03 ±0.00 0.04±0.00 0.21±0.01 37.68 ±32.86 0.11±0.01 0.12±0.01
Weight Gaussian 0.03 ±0.01 0.03±0.01 0.20±0.02 41.60 ±40.06 0.09±0.02 0.10±0.01
Weight DropConnect 0.03 ±0.01 0.03±0.01 0.20±0.02 46.44 ±50.40 0.09±0.02 0.10±0.01
Table 29: Tabular regression: MSE (↓)comparison on in-distribution (ID) and out-of-distribution (OOD)
test sets and with hyperparameters transferred across datasets.
31Published in Transactions on Machine Learning Research (04/2024)
Noise TypeEnergy Wine Concrete
ID OOD ID OOD ID OOD
No Noise -1.70 ±0.11 1.24±0.61 4.78±1.31 6.66±1.82 -0.54 ±0.15 0.32±0.69
Input Gaussian -1.74 ±0.07 1.06±0.88 5.24±1.58 19.29 ±19.08 -0.40 ±0.15 0.23±0.47
Input-Target CMixUp -1.66 ±0.10 0.42±1.30 -0.06 ±0.15 95014.61 ±134370.19 -0.65 ±0.05 0.53±1.27
Activation Gaussian -1.71 ±0.09 0.51±1.20 1.84±1.25 177076.49 ±250421.18 -0.50 ±0.14 0.46±0.74
Activation Dropout -1.66 ±0.12 0.02±1.35 -0.27 ±0.05 -0.19 ±0.05 -0.56 ±0.11 1.19±2.07
Gradient Gaussian -1.70 ±0.11 5.71±6.29 1.97±1.94 2.06±2.03 -0.11 ±0.10 0.63±0.34
Model -1.64 ±0.02 -0.16 ±0.64 4.68±1.33 6.22±1.31 -0.64 ±0.03 -0.49 ±0.17
Weight Gaussian -1.71 ±0.06 -0.26 ±0.33 4.51±1.62 206785.40 ±200338.85 -0.55 ±0.07 -0.03 ±0.47
Weight DropConnect -1.73 ±0.12 0.00±0.39 0.37±0.43 37100.06 ±52466.28 -0.67 ±0.13 -0.56 ±0.11
Table 30: Tabular regression: NLL (↓)comparison on in-distribution (ID) and out-of-distribution (OOD)
test sets and with hyperparameters transferred across datasets.
Noise TypeBoston Yacht Concrete
ID OOD ID OOD ID OOD
No Noise 0.13 ±0.04 0.16±0.04 0.56±0.74 0.59±0.72 0.11±0.01 0.11±0.00
Input Gaussian 0.13 ±0.03 0.15±0.03 0.55±0.75 0.59±0.72 0.10±0.01 0.11±0.01
Input-Target CMixUp 0.50 ±0.47 0.52±0.46 0.05±0.02 0.11±0.06 0.11±0.01 0.12±0.01
Activation Gaussian 0.11 ±0.03 0.13±0.03 0.56±0.75 0.62±0.71 0.11±0.01 0.11±0.01
Activation Dropout 0.17 ±0.07 0.18±0.06 0.05±0.02 0.08±0.01 0.11±0.01 0.11±0.01
Gradient Gaussian 0.16 ±0.04 0.18±0.04 0.02±0.01 0.06±0.04 0.11±0.01 0.11±0.00
Model 0.15 ±0.06 0.17±0.06 0.11±0.04 0.13±0.04 0.12±0.01 0.12±0.01
Weight Gaussian 0.12 ±0.04 0.14±0.03 0.03±0.00 0.07±0.02 0.11±0.01 0.11±0.00
Weight DropConnect 0.13 ±0.03 0.15±0.03 0.04±0.02 0.10±0.07 0.11±0.01 0.11±0.01
Table 31: Tabular regression: MSE (↓)comparison on in-distribution (ID) and out-of-distribution (OOD)
test sets and with hyperparameters transferred across architectures.
Noise TypeBoston Yacht Concrete
ID OOD ID OOD ID OOD
No Noise -0.25 ±0.21 -0.01 ±0.17 -0.42 ±0.88 -0.02 ±0.80 -0.57 ±0.06 -0.02 ±0.54
Input Gaussian -0.35 ±0.10 -0.17 ±0.09 -0.94 ±1.25 -0.72 ±1.09 -0.58 ±0.08 0.06±0.63
Input-Target CMixUp -0.09 ±0.29 -0.02 ±0.23 -1.20 ±0.07 -1.06 ±0.11 -0.60 ±0.04 -0.25 ±0.32
Activation Gaussian -0.18 ±0.66 0.02±0.72 -0.44 ±0.93 -0.09 ±0.65 -0.56 ±0.07 -0.09 ±0.36
Activation Dropout -0.56 ±0.04 -0.54 ±0.04 -1.11 ±0.31 -0.81 ±0.11 -0.58 ±0.07 0.08±0.66
Gradient Gaussian 0.97 ±1.13 1.22±1.16 -1.52 ±0.43 -1.25 ±0.17 -0.57 ±0.06 -0.02 ±0.54
Model -0.31 ±0.25 -0.10 ±0.19 -0.57 ±0.10 -0.56 ±0.11 -0.58 ±0.03 -0.33 ±0.15
Weight Gaussian -0.42 ±0.18 -0.28 ±0.18 -1.62 ±0.34 -0.77 ±1.00 -0.56 ±0.07 0.11±0.69
Weight DropConnect -0.46 ±0.09 -0.34 ±0.10 -1.72 ±0.31 -1.20 ±0.26 -0.55 ±0.06 0.07±0.60
Table 32: Tabular regression: NLL (↓)comparison on in-distribution (ID) and out-of-distribution (OOD)
test sets and with hyperparameters transferred across architectures.
32Published in Transactions on Machine Learning Research (04/2024)
0.0 0.2 0.4 0.6 0.8 1.0
α20406080Error [%]
ID OOD
2.55.07.510.012.515.017.520.0
Expected Calibration Error [%]
ID OOD
(a) Error and ECE.
0.0 0.2 0.4 0.6 0.8 1.0
α20406080Error [%]
ID OOD
0.51.01.52.0
NLL [nats]
ID OOD (b) Error and NLL.
−2 0 2 4 6 8
α−6−5−4−3−2−101β
1020304050607080
9090908
8816
16
2424
24
323232
404040
484848
565656
6464
72Start weights End weights
102030405060708090
Error [%]
4122028364452606876
Expected Calibration Error [%] (c) Error and ECE on ID.
−2 0 2 4 6 8
α−6−5−4−3−2−101β
2432404856647280
888888
968
88816
2424
24
3232
32
404040
484848
565656
646464
72Start weights End weights
20283644526068768492
Error [%]
4122028364452606876
Expected Calibration Error [%] (d) Error and ECE on OOD.
Figure 9: Input Random Crop, Horizontal Flip on CIFAR-10. Observations : Did not change the smoothness
of the 1D curves or the 2D metric landscape trajectory compared to no noise.
0.0 0.2 0.4 0.6 0.8 1.0
α102030405060708090Error [%]
ID OOD
51015202530
Expected Calibration Error [%]
ID OOD
(a) Error and ECE.
0.0 0.2 0.4 0.6 0.8 1.0
α102030405060708090Error [%]
ID OOD
0.500.751.001.251.501.752.002.25
NLL [nats]
ID OOD (b) Error and NLL.
−2.50.0 2.5 5.0 7.5 10.012.5
α−8−6−4−202β
162432404856647280
888888
96 88 8816
1624
2424
32
3232
40 4040
484848
565656
646464
7272
80Start weights End weights
152535455565758595
Error [%]
4122028364452606876
Expected Calibration Error [%] (c) Error and ECE on ID.
−2.50.0 2.5 5.0 7.5 10.012.5
α−8−6−4−202β24
3240485664728088 88
96 8816
242424
323232
40 4040
484848
565656
646464
7272
8080Start weights End weights
24324048566472808896
Error [%]
4122028364452606876
Expected Calibration Error [%] (d) Error and ECE on OOD.
Figure 10: Input Additive Gaussian on CIFAR-10. Observations : Changed the smoothness of the 1D curves
where NLL became less smooth and removed the bumps in ECE for αapproaching the initial model. The
2D metric landscape trajectory did not change in comparison to no noise.
0.0 0.2 0.4 0.6 0.8 1.0
α102030405060708090Error [%]
ID OOD
05101520
Expected Calibration Error [%]
ID OOD
(a) Error and ECE.
0.0 0.2 0.4 0.6 0.8 1.0
α102030405060708090Error [%]
ID OOD
0.500.751.001.251.501.752.002.25
NLL [nats]
ID OOD (b) Error and NLL.
−2.5 0.0 2.5 5.0 7.5 10.0 12.5
α−8−6−4−202β20304050607080
90
9090
88
16
1616
24
2424
323232
40 4040
484848
565656
646464
7272
80Start weights End weights
152535455565758595
Error [%]
4122028364452606876
Expected Calibration Error [%] (c) Error and ECE on ID.
−2.5 0.0 2.5 5.0 7.5 10.0 12.5
α−8−6−4−202β2432404856647280
8888
88
9688
161624
2424
323232
40 4040
484848
565656
646464
7272
80Start weights End weights
24324048566472808896
Error [%]
4122028364452606876
Expected Calibration Error [%] (d) Error and ECE on OOD.
Figure 11: Input ODS on CIFAR-10. Observations : Marginally changed the smoothness of the 1D curves.
The 2D metric landscape trajectory did not change in comparison to no noise.
0.0 0.2 0.4 0.6 0.8 1.0
α102030405060708090Error [%]
ID OOD
24681012
Expected Calibration Error [%]
ID OOD
(a) Error and ECE.
0.0 0.2 0.4 0.6 0.8 1.0
α102030405060708090Error [%]
ID OOD
0.250.500.751.001.251.501.752.002.25
NLL [nats]
ID OOD (b) Error and NLL.
−2 0 2 4 6 8
α−6−5−4−3−2−101β20304050607080
909090
908
88
16
1616
242424
323232
4040
48
5664
72Start weights End weights
152535455565758595
Error [%]
4122028364452606876
Expected Calibration Error [%] (c) Error and ECE on ID.
−2 0 2 4 6 8
α−6−5−4−3−2−101β
2432404856647280
8888
968
88
16
1616
2424
24
323232
404040
48
5664
72Start weights End weights
24324048566472808896
Error [%]
4122028364452606876
Expected Calibration Error [%] (d) Error and ECE on OOD.
Figure 12: Input-Target MixUp on CIFAR-10. Observations : Both the NLL and ECE 1D curves changed in
comparison to no noise, and the 2D plots seem to explore wider valleys compared to no noise.
33Published in Transactions on Machine Learning Research (04/2024)
0.0 0.2 0.4 0.6 0.8 1.0
α102030405060708090Error [%]
ID OOD
0246810121416
Expected Calibration Error [%]
ID OOD
(a) Error and ECE.
0.0 0.2 0.4 0.6 0.8 1.0
α102030405060708090Error [%]
ID OOD
0.500.751.001.251.501.752.002.25
NLL [nats]
ID OOD (b) Error and NLL.
−2.5 0.0 2.5 5.0 7.5 10.012.5
α−8−6−4−202β
1624324048566472808888
968
16 1616
24 2424
32 3232
404040
484848
566472
80Start weights End weights
152535455565758595
Error [%]
515253545556575
Expected Calibration Error [%] (c) Error and ECE on ID.
−2.5 0.0 2.5 5.0 7.5 10.012.5
α−8−6−4−202β
243240485664728088
968
1616
242424
32 3232
404040
484848
5656
6472
80Start weights End weights
24324048566472808896
Error [%]
4122028364452606876
Expected Calibration Error [%] (d) Error and ECE on OOD.
Figure 13: Target Smoothing on CIFAR-10. Observations : The NLL became more aligned with the error,
not the ECE. The 2D plots show slightly more variation in the trajectory than no noise.
0.0 0.2 0.4 0.6 0.8 1.0
α102030405060708090Error [%]
ID OOD
05101520
Expected Calibration Error [%]
ID OOD
(a) Error and ECE.
0.0 0.2 0.4 0.6 0.8 1.0
α102030405060708090Error [%]
ID OOD
0.500.751.001.251.501.752.002.25
NLL [nats]
ID OOD (b) Error and NLL.
−2.5 0.0 2.5 5.0 7.5 10.0
α−8−6−4−202β
203040506070809090
8888 16
161624
242424
32
323232
40
404040
484848
5656
6464
7272
80Start weights End weights
152535455565758595
Error [%]
51525354555657585
Expected Calibration Error [%] (c) Error and ECE on ID.
−2.5 0.0 2.5 5.0 7.5 10.0
α−8−6−4−202β
243240485664728088
88
968888
1616
24
242424
32
323232
40
404040
484848
565656
6464
7272
80Start weights End weights
24324048566472808896
Error [%]
51525354555657585
Expected Calibration Error [%] (d) Error and ECE on OOD.
Figure 14: Activation Additive Gaussian on CIFAR-10. Observations : Did not change the smoothness of
the 1D curves or the 2D metric landscape trajectory compared to no noise.
0.0 0.2 0.4 0.6 0.8 1.0
α20406080Error [%]
ID OOD
510152025
Expected Calibration Error [%]
ID OOD
(a) Error and ECE.
0.0 0.2 0.4 0.6 0.8 1.0
α20406080Error [%]
ID OOD
0.250.500.751.001.251.501.752.002.25
NLL [nats]
ID OOD (b) Error and NLL.
0 2 4 6
α−6−5−4−3−2−101β
102030405060708090
909090
8
88
88
16
1616
16
24
242424
3232
3232
4040
4848
5656
6464
72Start weights End weights
102030405060708090
Error [%]
4122028364452606876
Expected Calibration Error [%] (c) Error and ECE on ID.
0 2 4 6
α−6−5−4−3−2−101β
2432404856647280
88
968
88
88
16
161616
24
2424
2424
3232
3232
4040
4848
5656
6464
72Start weights End weights
20283644526068768492
Error [%]
4122028364452606876
Expected Calibration Error [%] (d) Error and ECE on OOD.
Figure 15: Activation Dropout on CIFAR-10. Observations : Dropout narrowed the gap between ID and
OOD results; nevertheless, the shape of the 1D curves is similar to no noise. The trajectories in 2D plots
did not seem to converge into a narrow local minimum.
0.0 0.2 0.4 0.6 0.8 1.0
α2030405060708090Error [%]
ID OOD
510152025
Expected Calibration Error [%]
ID OOD
(a) Error and ECE.
0.0 0.2 0.4 0.6 0.8 1.0
α2030405060708090Error [%]
ID OOD
0.751.001.251.501.752.002.25
NLL [nats]
ID OOD (b) Error and NLL.
−0.75−0.50−0.25 0.00 0.25 0.50
α−0.250.000.250.500.751.001.251.50β
16
2432
4048
566472808855
1010
10
1515
2020
20
2525
3030
3535
4045Start weights End weights
16243240485664728088
Error [%]
2.57.512.517.522.527.532.537.542.547.5
Expected Calibration Error [%] (c) Error and ECE on ID.
−0.75−0.50−0.25 0.00 0.25 0.50
α−0.250.000.250.500.751.001.251.50β
32
40
4856
647280
88488
1212
1616
2020
20
2424
24
2828
28
3232
3636Start weights End weights
2836445260687684
Error [%]
261014182226303438
Expected Calibration Error [%] (d) Error and ECE on OOD.
Figure 16: Gradient Gaussian on CIFAR-10. Observations : The 1D and 2D figures changed curvature and
shape drastically, and NLL and ECE follow a non-linear pattern. The 2D plots show a circular curvature,
perhaps suggesting difficulty in convergence.
34Published in Transactions on Machine Learning Research (04/2024)
0.0 0.2 0.4 0.6 0.8 1.0
α102030405060708090Error [%]
ID OOD
5101520253035
Expected Calibration Error [%]
ID OOD
(a) Error and ECE.
0.0 0.2 0.4 0.6 0.8 1.0
α102030405060708090Error [%]
ID OOD
0.500.751.001.251.501.752.002.25
NLL [nats]
ID OOD (b) Error and NLL.
−2.5 0.0 2.5 5.0 7.5 10.012.5
α−8−6−4−202β16
2432404856647280
888888
88
961020
20303030
40 4040
505050
606060
707070
80Start weights End weights
152535455565758595
Error [%]
51525354555657585
Expected Calibration Error [%] (c) Error and ECE on ID.
−2.5 0.0 2.5 5.0 7.5 10.012.5
α−8−6−4−202β32404856647280
8888
8888
9688161624
32 3232
40 4040
484848
565656
646464
727272
80Start weights End weights
283644526068768492
Error [%]
51525354555657585
Expected Calibration Error [%] (d) Error and ECE on OOD.
Figure 17: Model Shrinkand Perturb on CIFAR-10. Observations : The1D and 2D figureschanged curvature
and shape drastically, and all metrics show a non-linear optimisation path as hypothesised. The point cluster
around centres created by shrinking and perturbing the weights.
0.0 0.2 0.4 0.6 0.8 1.0
α102030405060708090Error [%]
ID OOD
51015202530
Expected Calibration Error [%]
ID OOD
(a) Error and ECE.
0.0 0.2 0.4 0.6 0.8 1.0
α102030405060708090Error [%]
ID OOD
0.500.751.001.251.501.752.002.25
NLL [nats]
ID OOD (b) Error and NLL.
0 2 4 6 8
α−6−5−4−3−2−101β
162432404856647280
88
9688 8
1616
16
162424
24
24323232
32 4040
48566472Start weights End weights
152535455565758595
Error [%]
4122028364452606876
Expected Calibration Error [%] (c) Error and ECE on ID.
0 2 4 6 8
α−6−5−4−3−2−101β
2432404856647280
88
96888
1616
2424
24
243232
32
4040
4848
5656
6472Start weights End weights
24324048566472808896
Error [%]
4122028364452606876
Expected Calibration Error [%] (d) Error and ECE on OOD.
Figure 18: Weight Additive Gaussian on CIFAR-10. Observations : The 1D curves marginally changed their
shape. However, the difference between ID and OOD metrics became more profound. The 2D plots suggest
that the optimisation was not able to converge.
0.0 0.2 0.4 0.6 0.8 1.0
α102030405060708090Error [%]
ID OOD
05101520
Expected Calibration Error [%]
ID OOD
(a) Error and ECE.
0.0 0.2 0.4 0.6 0.8 1.0
α102030405060708090Error [%]
ID OOD
0.500.751.001.251.501.752.002.25
NLL [nats]
ID OOD (b) Error and NLL.
−2.5 0.0 2.5 5.0 7.5 10.012.5
α−8−6−4−202β 162432404856647280
888888
96 888
16
1616
24
2424
323232
40
404040
48
484848
565656
6464
727280Start weights End weights
152535455565758595
Error [%]
4122028364452606876
Expected Calibration Error [%] (c) Error and ECE on ID.
−2.5 0.0 2.5 5.0 7.5 10.012.5
α−8−6−4−202β
243240485664728088
96 888
1616
2424
24
323232
40
404040
48
484848
565656
646464
727280Start weights End weights
24324048566472808896
Error [%]
51525354555657585
Expected Calibration Error [%] (d) Error and ECE on OOD.
Figure 19: Weight DropConnect on CIFAR-10. Observations : Did not change the smoothness of the 1D
curves or the 2D metric landscape trajectory compared to no noise.
0.0 0.2 0.4 0.6 0.8 1.0
a203040506070Error [%]
ID OOD
051015202530
Expected Calibration Error [%]
ID OOD
(a) Error and ECE.
0.0 0.2 0.4 0.6 0.8 1.0
a203040506070Error [%]
ID OOD
0.40.50.60.7
NLL [nats]
ID OOD (b) Error and NLL.
−0.10−0.05 0.00
a−0.6−0.5−0.4−0.3−0.2−0.10.00.1b
161818
2020
222224
26
28
302.5
5.05.07.57.510
.010
.0
12
.515
.017
.520
.0
22
.5Start weights End weights
16.018.420.823.225.628.030.4
Error [%]
1.54.57.510.513.516.519.522.5
Expected Calibration Error [%] (c) Error and ECE on ID.
−0.10−0.05 0.00
a−0.6−0.5−0.4−0.3−0.2−0.10.00.1b
161618
18
2020
2222
2424
26
28
302.5
5.05.0
7.57.510
.010
.0
12
.515
.017
.520
.0
22.5Start weights End weights
161820222426283032
Error [%]
36912151821
Expected Calibration Error [%] (d) Error and ECE on OOD.
Figure 20: No noise on Adult.
35Published in Transactions on Machine Learning Research (04/2024)
0.0 0.2 0.4 0.6 0.8 1.0
a203040506070Error [%]
ID OOD
051015202530
Expected Calibration Error [%]
ID OOD
(a) Error and ECE.
0.0 0.2 0.4 0.6 0.8 1.0
a203040506070Error [%]
ID OOD
0.40.50.60.7
NLL [nats]
ID OOD (b) Error and NLL.
−0.10−0.05 0.00
a−0.6−0.5−0.4−0.3−0.2−0.10.00.1b16
1618
18
20202224
2628
302.5
5.05.0
7.57.510
.010
.012
.515
.017
.520
.022
.525
.0Start weights End weights
151719212325272931
Error [%]
1.54.57.510.513.516.519.522.525.5
Expected Calibration Error [%] (c) Error and ECE on ID.
−0.10−0.05 0.00
a−0.6−0.5−0.4−0.3−0.2−0.10.00.1b16
1618
18
2020222426
28
302.5
5.05.0
7.57.510
.010
.012
.515
.017
.520
.022
.525
.0Start weights End weights
151719212325272931
Error [%]
3691215182124
Expected Calibration Error [%] (d) Error and ECE on OOD.
Figure 21: Input Additive Gaussian on Adult. Observations : Did not change the smoothness of the 1D
curves or the 2D metric landscape trajectory compared to no noise.
0.0 0.2 0.4 0.6 0.8 1.0
a203040506070Error [%]
ID OOD
51015202530
Expected Calibration Error [%]
ID OOD
(a) Error and ECE.
0.0 0.2 0.4 0.6 0.8 1.0
a203040506070Error [%]
ID OOD
0.40.50.60.7
NLL [nats]
ID OOD (b) Error and NLL.
−0.10−0.05 0.00
a−0.6−0.5−0.4−0.3−0.2−0.10.00.1b16
1618 18
202022
24
26
28
302.5
5.05.0
5.07.57.5 10
.012
.515
.017
.520
.022
.525
.0Start weights End weights
151719212325272931
Error [%]
1.54.57.510.513.516.519.522.525.5
Expected Calibration Error [%] (c) Error and ECE on ID.
−0.10−0.05 0.00
a−0.6−0.5−0.4−0.3−0.2−0.10.00.1b16
1618 18
202022
24
26
28
302.52.5
5.05.0
7.57.510
.010
.0
12
.515
.017
.520
.022
.525
.0Start weights End weights
1618202224262830
Error [%]
3691215182124
Expected Calibration Error [%] (d) Error and ECE on OOD.
Figure 22: Input ODS on Adult. Observations : Did not change the smoothness of the 1D curves or the 2D
metric landscape trajectory compared to no noise.
0.0 0.2 0.4 0.6 0.8 1.0
a203040506070Error [%]
ID OOD
51015202530
Expected Calibration Error [%]
ID OOD
(a) Error and ECE.
0.0 0.2 0.4 0.6 0.8 1.0
a203040506070Error [%]
ID OOD
0.40.50.60.7
NLL [nats]
ID OOD (b) Error and NLL.
−0.100−0.075−0.050−0.025 0.000 0.025
a−0.6−0.5−0.4−0.3−0.2−0.10.00.1b16
.5
16.518.0
19.5
21.0
22.5
24.0
25.5
27.02.5
5.05.07.57.510
.012
.515
.017
.520
.022
.525
.0Start weights End weights
15.216.818.420.021.623.224.826.428.0
Error [%]
369121518212427
Expected Calibration Error [%] (c) Error and ECE on ID.
−0.100−0.075−0.050−0.025 0.000 0.025
a−0.6−0.5−0.4−0.3−0.2−0.10.00.1b16
.5 16
.518.0
19.5
21.0
22.5
24.0
25.5
27.02.5 5.05.0
7.57.510
.012
.515
.017
.520
.022
.525
.0Start weights End weights
16.017.619.220.822.424.025.627.228.8
Error [%]
369121518212427
Expected Calibration Error [%] (d) Error and ECE on OOD.
Figure 23: Input-Target MixUp on Adult. Observations : Did not change the smoothness of the 1D curves
or the 2D metric landscape trajectory compared to no noise.
0.0 0.2 0.4 0.6 0.8 1.0
a203040506070Error [%]
ID OOD
51015202530
Expected Calibration Error [%]
ID OOD
(a) Error and ECE.
0.0 0.2 0.4 0.6 0.8 1.0
a203040506070Error [%]
ID OOD
0.40.50.60.7
NLL [nats]
ID OOD (b) Error and NLL.
−0.125−0.100−0.075−0.050−0.025 0.000 0.025
a−0.6−0.5−0.4−0.3−0.2−0.10.00.1b16
1616
1818
202224
26
28
30322.5
5.05.0
7.57.510
.012
.515
.017
.520
.022
.525
.027
.5Start weights End weights
16182022242628303234
Error [%]
1.54.57.510.513.516.519.522.525.5
Expected Calibration Error [%] (c) Error and ECE on ID.
−0.125−0.100−0.075−0.050−0.025 0.000 0.025
a−0.6−0.5−0.4−0.3−0.2−0.10.00.1b1616
1818
202224
26
28
30322.5
5.05.0
7.57.510
.012
.515
.017
.520
.022
.525
.027
.5Start weights End weights
16182022242628303234
Error [%]
1.54.57.510.513.516.519.522.525.528.5
Expected Calibration Error [%] (d) Error and ECE on OOD.
Figure 24: Target Smoothing on Adult. Observations : Did not change the smoothness of the 1D curves or
the 2D metric landscape trajectory compared to no noise.
36Published in Transactions on Machine Learning Research (04/2024)
0.0 0.2 0.4 0.6 0.8 1.0
a203040506070Error [%]
ID OOD
051015202530
Expected Calibration Error [%]
ID OOD
(a) Error and ECE.
0.0 0.2 0.4 0.6 0.8 1.0
a203040506070Error [%]
ID OOD
0.350.400.450.500.550.600.650.700.75
NLL [nats]
ID OOD (b) Error and NLL.
−0.15−0.10−0.05 0.00
a−0.6−0.5−0.4−0.3−0.2−0.10.00.1b16
1616
1818
20202224
26
2830
32342.5
5.05.0
7.57.510
.0
10.010
.0
12
.515
.017
.520
.022
.525
.0Start weights End weights
1518212427303336
Error [%]
1.54.57.510.513.516.519.522.525.5
Expected Calibration Error [%] (c) Error and ECE on ID.
−0.15−0.10−0.05 0.00
a−0.6−0.5−0.4−0.3−0.2−0.10.00.1b16
161818
20202224
26
2830
32342.5
5.05.0
7.57.510
.010.012
.515
.017
.520
.022
.525
.0Start weights End weights
16.519.522.525.528.531.534.5
Error [%]
1.54.57.510.513.516.519.522.525.5
Expected Calibration Error [%] (d) Error and ECE on OOD.
Figure 25: Activation Additive Gaussian on Adult. Observations : Did not change the smoothness of the 1D
curves or the 2D metric landscape trajectory compared to no noise.
0.0 0.2 0.4 0.6 0.8 1.0
a203040506070Error [%]
ID OOD
51015202530
Expected Calibration Error [%]
ID OOD
(a) Error and ECE.
0.0 0.2 0.4 0.6 0.8 1.0
a203040506070Error [%]
ID OOD
0.30.40.50.60.7
NLL [nats]
ID OOD (b) Error and NLL.
−0.025 0.000 0.025 0.050 0.075 0.100 0.125
a−0.8−0.6−0.4−0.20.0b 15.215.2
16
.016
.817
.618
.419
.220
.020.8
21.63.04.54.56.06.0
7.57.5
9.010
.512
.0
13.5
15.0Start weights End weights
14.815.616.417.218.018.819.620.421.222.0
Error [%]
2.44.05.67.28.810.412.013.615.2
Expected Calibration Error [%] (c) Error and ECE on ID.
−0.025 0.000 0.025 0.050 0.075 0.100 0.125
a−0.8−0.6−0.4−0.20.0b15.215
.2
16
.016
.817
.618
.419
.220
.020.8
21.63.03.04.5 4.5 6.06.07.57.5
9.010
.512
.013.5
15.0Start weights End weights
15.216.016.817.618.419.220.020.821.6
Error [%]
2.44.05.67.28.810.412.013.615.2
Expected Calibration Error [%] (d) Error and ECE on OOD.
Figure 26: Activation Dropout on Adult. Observations : Changed the ECE curvature and made the NLL
plots smoother in the 1D case. In the 2D plots, the ECE and error appear aligned during optimisation. The
curvature of the 2D plots has changed and there is a higher alignment between the ECE and error.
0.0 0.2 0.4 0.6 0.8 1.0
a203040506070Error [%]
ID OOD
51015202530
Expected Calibration Error [%]
ID OOD
(a) Error and ECE.
0.0 0.2 0.4 0.6 0.8 1.0
a203040506070Error [%]
ID OOD
0.30.40.50.60.7
NLL [nats]
ID OOD (b) Error and NLL.
−1.00−0.75−0.50−0.25 0.00 0.25
a−0.6−0.4−0.20.0b18
24 3030
3636
4248546066722.5
5.05.05.07.5
7.5
10.010.0
10.0
12.512.512.5
15.015.015.0
17.5 17.517.5
17.5
20.020.0
20.0
22.522
.5
25.0Start weights End weights
18243036424854606672
Error [%]
369121518212427
Expected Calibration Error [%] (c) Error and ECE on ID.
−1.00−0.75−0.50−0.25 0.00 0.25
a−0.6−0.4−0.20.0b18
243030
3636
4248546066722.5
5.05.0 5.07.5
7.5
10.010.0
10.0
12.512.512.5
15.015.015.0
17.5 17.517.5
17.5
20.020.0
20.0
22.522.5
25.0Start weights End weights
18243036424854606672
Error [%]
369121518212427
Expected Calibration Error [%] (d) Error and ECE on OOD.
Figure 27: Gradient Gaussian on Adult. Observations : Did not change the smoothness of the 1D curves,
but the 2D trajectory appears more exploratory compared to no noise.
0.0 0.2 0.4 0.6 0.8 1.0
a203040506070Error [%]
ID OOD
051015202530
Expected Calibration Error [%]
ID OOD
(a) Error and ECE.
0.0 0.2 0.4 0.6 0.8 1.0
a203040506070Error [%]
ID OOD
0.30.40.50.60.7
NLL [nats]
ID OOD (b) Error and NLL.
−0.5 0.0 0.5 1.0 1.5
a−2.0−1.5−1.0−0.50.0b
1824
24
3030364248
4854
545460
60606066
66
66
6666666
9 9 9 1212
15151515
15181818
1821
212121
2124
2424
272727Start weights End weights
15243342516069
Error [%]
36912151821242730
Expected Calibration Error [%] (c) Error and ECE on ID.
−0.5 0.0 0.5 1.0 1.5
a−2.0−1.5−1.0−0.50.0b
1824
24
3030364248
4854
545460
60606066
66
66
6666666
9 9 9 1212
15151515
15181818
1821
212121
2124
2424
272727Start weights End weights
15243342516069
Error [%]
36912151821242730
Expected Calibration Error [%] (d) Error and ECE on OOD.
Figure 28: Model Shrink and Perturb on Adult. Observations : Did not change the smoothness of the 1D
curves, but the 2D trajectory appears more exploratory compared to no noise.
37Published in Transactions on Machine Learning Research (04/2024)
0.0 0.2 0.4 0.6 0.8 1.0
a203040506070Error [%]
ID OOD
051015202530
Expected Calibration Error [%]
ID OOD
(a) Error and ECE.
0.0 0.2 0.4 0.6 0.8 1.0
a203040506070Error [%]
ID OOD
0.30.40.50.60.7
NLL [nats]
ID OOD (b) Error and NLL.
−0.10−0.08−0.06−0.04−0.02 0.00 0.02
a−0.6−0.5−0.4−0.3−0.2−0.10.00.1b1616
1818 2022
24
2628
302
2
44
66
81012
1416
1618
182020Start weights End weights
15171921232527293133
Error [%]
14710131619
Expected Calibration Error [%] (c) Error and ECE on ID.
−0.10−0.08−0.06−0.04−0.02 0.00 0.02
a−0.6−0.5−0.4−0.3−0.2−0.10.00.1b16
1618
182022
24
2628
302
2
44
66
8
1012
1416
1618
182020Start weights End weights
15171921232527293133
Error [%]
2468101214161820
Expected Calibration Error [%] (d) Error and ECE on OOD.
Figure 29: Weight Additive Gaussian on Adult. Observations : Did not change the smoothness of the 1D
curves or the 2D metric landscape trajectory compared to no noise.
0.0 0.2 0.4 0.6 0.8 1.0
a203040506070Error [%]
ID OOD
51015202530
Expected Calibration Error [%]
ID OOD
(a) Error and ECE.
0.0 0.2 0.4 0.6 0.8 1.0
a203040506070Error [%]
ID OOD
0.30.40.50.60.7
NLL [nats]
ID OOD (b) Error and NLL.
−0.02 0.00 0.02 0.04 0.06 0.08 0.10
a−0.5−0.4−0.3−0.2−0.10.00.1b
151616 17
171819
20212223
24
1.53.0
3.03.04.54.5
6.06.0
7.57.5
9.010
.510.5
12
.012
.013.5Start weights End weights
14.515.516.517.518.519.520.521.522.523.5
Error [%]
1.63.24.86.48.09.611.212.814.4
Expected Calibration Error [%] (c) Error and ECE on ID.
−0.02 0.00 0.02 0.04 0.06 0.08 0.10
a−0.5−0.4−0.3−0.2−0.10.00.1b 16 1617
171819
20212223243.03.04.54.56.06.0
7.57.5
9.010
.510.5
12.012
.013.5Start weights End weights
15161718192021222324
Error [%]
2.44.05.67.28.810.412.013.6
Expected Calibration Error [%] (d) Error and ECE on OOD.
Figure 30: Weight DropConnect on Adult. Observations : Changed the ECE curvature and made the
NLL and ECE plots smoother in the 1D case. In the 2D plots, the ECE and error appear aligned during
optimisation.
0.0 0.2 0.4 0.6 0.8 1.0
α0.040.050.060.070.08Mean Squared Error
ID OOD
051015202530
NLL [nats]
ID OOD
(a) MSE and NLL.
−0.0020.000 0.002 0.004 0.006 0.008 0.010
α−0.00250.00000.00250.00500.00750.01000.01250.0150β0.03420.03480.03540.03600.03660.03720.03780.03840.03900.0396
20
.022
.525
.027
.530
.032
.535.037.540.042.5Start weights End weights
0.03360.03440.03520.03600.03680.03760.03840.03920.0400
Mean Squared Error
19.522.525.528.531.534.537.540.543.5
NLL [nats] (b) MSE and NLL on ID.
−0.0020.000 0.002 0.004 0.006 0.008 0.010
α−0.00250.00000.00250.00500.00750.01000.01250.0150β0.03600.0366
0.03720.03780.03840.03900.03960.04020.04080.0414
27.530
.032.535
.037.540.042.545.047.5Start weights End weights
0.03560.03640.03720.03800.03880.03960.04040.0412
Mean Squared Error
25.528.531.534.537.540.543.546.549.5
NLL [nats] (c) MSE and NLL on OOD.
Figure 31: Input Additive Gaussian on WikiFace. Observations : Did not change the smoothness of the 1D
curves, or the 2D trajectory.
38Published in Transactions on Machine Learning Research (04/2024)
0.0 0.2 0.4 0.6 0.8 1.0
α0.030.040.050.060.070.080.09Mean Squared Error
ID OOD
−1.00−0.75−0.50−0.250.000.250.50
NLL [nats]
ID OOD
(a) MSE and NLL.
−0.010.00 0.01 0.02 0.03 0.04 0.05
α−0.025−0.020−0.015−0.010−0.0050.0000.005β
0.03120.03180.03240.03300.0336
0.03420.0342
0.03480.0354
012345 678Start weights End weights
0.03120.03180.03240.03300.03360.03420.03480.03540.03600.0366
Mean Squared Error
0123456789
NLL [nats] (b) MSE and NLL on ID.
−0.010.00 0.01 0.02 0.03 0.04 0.05
α−0.025−0.020−0.015−0.010−0.0050.0000.005β
0.03400.03450.03500.03550.03600.0365
0.03700.0370 0.0375
123456789Start weights End weights
0.033750.034250.034750.035250.035750.036250.036750.037250.037750.03825
Mean Squared Error
0.51.52.53.54.55.56.57.58.59.5
NLL [nats] (c) MSE and NLL on OOD.
Figure 32: Input Random Crop, Horizontal Flip on WikiFace. Observations : Surprisingly, the NLL starts
decreasing compared to MSE as the model is interpolated between the final and the initial model in the 1D
plots. The 2D plots demonstrate that the model was able to explore a deeper optimal from the start where
NLL was slower to converge than MSE.
0.0 0.2 0.4 0.6 0.8 1.0
α0.030.040.050.060.070.080.090.10Mean Squared Error
ID OOD
−1.2−1.0−0.8−0.6−0.4−0.2
NLL [nats]
ID OOD
(a) MSE and NLL.
0.00 0.01 0.02 0.03 0.04
α−0.0050.0000.0050.0100.0150.020β 0.03040.03080.0312
0.03160.03200.03240.03280.0332
−0.6−0.4−0.20.00.20.40.60.8 1.0
1.01.2Start weights End weights
0.03020.03060.03100.03140.03180.03220.03260.03300.03340.0338
Mean Squared Error
−0.6−0.30.00.30.60.91.2
NLL [nats] (b) MSE and NLL on ID.
0.00 0.01 0.02 0.03 0.04
α−0.0050.0000.0050.0100.0150.020β0.03200.03240.0328
0.03280.03320.03360.03400.0344
−0.50−0.250.000.250.500.751.00 1.251.50Start weights End weights
0.03180.03220.03260.03300.03340.03380.03420.03460.0350
Mean Squared Error
−0.30.00.30.60.91.21.5
NLL [nats] (c) MSE and NLL on OOD.
Figure 33: Input AugMix on WikiFace. Observations : Surprisingly, the NLL starts decreasing compared to
MSE as the model is interpolated between the final and the initial model in the 1D plots. The 2D plots
demonstrate that the model was able to explore a deeper optimal from the start where NLL was slower to
converge than MSE, and it did not converge in the optima from the perspective of NLL.
0.0 0.2 0.4 0.6 0.8 1.0
α0.040.050.060.070.080.09Mean Squared Error
ID OOD
0510152025
NLL [nats]
ID OOD
(a) MSE and NLL.
−0.0025 0.0000 0.0025 0.0050 0.0075 0.0100
α0.0000.0050.0100.015β0.0369 0.03720.03750.03780.03810.03840.03870.03900.0393
212427
303336394245Start weights End weights
0.036750.037050.037350.037650.037950.038250.038550.038850.039150.03945
Mean Squared Error
18212427303336394245
NLL [nats] (b) MSE and NLL on ID.
−0.0025 0.0000 0.0025 0.0050 0.0075 0.0100
α0.0000.0050.0100.015β0.03870.03900.03930.03960.0399
0.04020.04050.04080.0411
24273033
363942454851Start weights End weights
0.03860.03900.03940.03980.04020.04060.04100.0414
Mean Squared Error
24273033363942454851
NLL [nats] (c) MSE and NLL on OOD.
Figure 34: Input-Target CMixUp on WikiFace. Observations : Did not change the smoothness of the 1D
curves, or the 2D trajectory appears more exploratory compared to no noise.
39Published in Transactions on Machine Learning Research (04/2024)
0.0 0.2 0.4 0.6 0.8 1.0
α0.040.050.060.070.080.09Mean Squared Error
ID OOD
02468101214
NLL [nats]
ID OOD
(a) MSE and NLL.
−0.0020.000 0.002 0.004 0.006 0.008 0.010
α0.0000.0050.0100.015β0.03600.03650.03700.03750.03800.03850.03900.03950.0400
11121314 1516171819Start weights End weights
0.035750.036250.036750.037250.037750.038250.038750.039250.039750.04025
Mean Squared Error
11121314151617181920
NLL [nats] (b) MSE and NLL on ID.
−0.0020.000 0.002 0.004 0.006 0.008 0.010
α0.0000.0050.0100.015β0.03800.0385 0.03900.03950.04000.04050.04100.0415
14.415.216.016.817.618.419.220.020.821.6Start weights End weights
0.03750.03800.03850.03900.03950.04000.04050.04100.04150.0420
Mean Squared Error
13.514.515.516.517.518.519.520.521.5
NLL [nats] (c) MSE and NLL on OOD.
Figure 35: Activation Additive Gaussian on WikiFace. Observations : Did not change the smoothness of the
1D curves, but the 2D trajectory appears more exploratory compared to no noise.
0.0 0.2 0.4 0.6 0.8 1.0
α0.030.040.050.060.070.080.09Mean Squared Error
ID OOD
0510152025
NLL [nats]
ID OOD
(a) MSE and NLL.
−0.008−0.006−0.004−0.002 0.000 0.002
α−0.010.000.010.020.030.04β0.02940.02940.0300
0.03000.0306
0.03060.0312
0.03120.0318
0.03180.0324
0.03240.03300.03300.03360.0342
25 30354045505560Start weights End weights
0.02910.02970.03030.03090.03150.03210.03270.03330.03390.0345
Mean Squared Error
22.527.532.537.542.547.552.557.562.5
NLL [nats] (b) MSE and NLL on ID.
−0.008−0.006−0.004−0.002 0.000 0.002
α−0.010.000.010.020.030.04β
0.03200.03250.03250.03300.03300.03350.0335
0.03400.03400.03450.03450.03500.0350
0.03550.03550.03600.0360
2530
354045
5055606570Start weights End weights
0.03200.03250.03300.03350.03400.03450.03500.03550.03600.0365
Mean Squared Error
212733394551576369
NLL [nats] (c) MSE and NLL on OOD.
Figure 36: Gradient Gaussian on WikiFace. Observations : Did not change the smoothness of the 1D curves,
but the 2D trajectory appears to align MSE and NLL. However, it seems that the optimisation missed a
local minimum during training.
0.0 0.2 0.4 0.6 0.8 1.0
α0.040.060.080.100.120.140.16Mean Squared Error
ID OOD
0100020003000400050006000
NLL [nats]
ID OOD
(a) MSE and NLL.
0 5 10 15
α02468β5050100100150150 200200250250
300300
350
0002500000025000000
5000000050000000
7500000075000000Start weights End weights
3090150210270330390450510
Mean Squared Error
0.00.30.60.91.21.51.82.12.4
NLL [nats]×108 (b) MSE and NLL on ID.
0 5 10 15
α02468β5050100100150150 200200250250
300300
350
0002500000025000000
5000000050000000
7500000075000000Start weights End weights
3090150210270330390450510
Mean Squared Error
0.00.30.60.91.21.51.82.12.4
NLL [nats]×108 (c) MSE and NLL on OOD.
Figure 37: Model Shrink and Perturb on WikiFace. Observations : Due to shrinking and perturbation,
the experiment appears to converge in a narrow basin and as seed in the 1D plots, the optimisation was
completely non-linear and unrecoverable.
40Published in Transactions on Machine Learning Research (04/2024)
0.0 0.2 0.4 0.6 0.8 1.0
α0.040.050.060.070.080.09Mean Squared Error
ID OOD
−1012345
NLL [nats]
ID OOD
(a) MSE and NLL.
0.000 0.005 0.010
α−0.025−0.020−0.015−0.010−0.0050.0000.005β
0.03510.0354
0.03570.03600.03630.03660.03690.03720.0375
1.62.43.24.04.8
5.66.47.28.0Start weights End weights
0.03510.03540.03570.03600.03630.03660.03690.03720.03750.0378
Mean Squared Error
1.22.02.83.64.45.26.06.87.6
NLL [nats] (b) MSE and NLL on ID.
0.000 0.005 0.010
α−0.025−0.020−0.015−0.010−0.0050.0000.005β
0.03720.03740.03760.0378
0.03800.03800.03820.0382
0.03840.0384 0.0386
0.03860.0388
0.0388
1.82.4
3.03.64.24.85.46.0Start weights End weights
0.03720.03750.03780.03810.03840.03870.0390
Mean Squared Error
1.21.82.43.03.64.24.85.46.06.6
NLL [nats] (c) MSE and NLL on OOD.
Figure 38: Weight Additive Gaussian on WikiFace. Observations : The 1D curves look similar to no noise,
although with respect to a different scale for NLL. The 2D plots explore a similar trajectory to no noise;
however, the 2D landscape appears more distorted.
0.0 0.2 0.4 0.6 0.8 1.0
α0.050.060.070.080.090.10Mean Squared Error
ID OOD
−10123456
NLL [nats]
ID OOD
(a) MSE and NLL.
0.00 0.01 0.02 0.03
α−0.0050.0000.0050.0100.0150.020β
0.04480.0456
0.04640.04720.0480
0.04880.04960.05040.0512
1.53.04.56.07.59.010.512.0
13.5Start weights End weights
0.04440.04520.04600.04680.04760.04840.04920.05000.05080.0516
Mean Squared Error
0.82.44.05.67.28.810.412.013.615.2
NLL [nats] (b) MSE and NLL on ID.
0.00 0.01 0.02 0.03
α−0.0050.0000.0050.0100.0150.020β
0.04560.04620.0468
0.04740.0480
0.04860.04920.04980.0504
1.53.0
4.56.07.59.010.512.0Start weights End weights
0.04560.04620.04680.04740.04800.04860.04920.04980.05040.0510
Mean Squared Error
0.01.63.24.86.48.09.611.212.8
NLL [nats] (c) MSE and NLL on OOD.
Figure 39: Weight DropConnect on WikiFace. Observations : The 1D curves look similar to no noise,
although with respect to a different scale for NLL. The 2D plots explore a similar trajectory to no noise.
0.0 0.2 0.4 0.6 0.8 1.0
a0.00.20.40.60.81.0Mean Squared Error
ID OOD
−1.5−1.0−0.50.00.5
NLL [nats]
ID OOD
(a) MSE and NLL.
0.0 0.1 0.2 0.3
a−0.10.00.10.20.30.40.50.6b
0.30.3
0.60.6
0.90.9
1.21.2
1.51.5
−1.2−0.6−0.6
0.00.0
0.60.60.6
1.2
1.21.2
1.8
1.82.4
2.43.03.6Start weights End weights
0.150.450.751.051.351.651.952.252.552.85
Mean Squared Error
−1.2−0.60.00.61.21.82.43.03.64.2
NLL [nats] (b) MSE and NLL on ID.
0.0 0.1 0.2 0.3
a−0.10.00.10.20.30.40.50.6b
0.40.4
0.80.8
1.21.2
1.61.6
2.00051015Start weights End weights
0.20.61.01.41.82.22.63.03.4
Mean Squared Error
0510152025303540
NLL [nats] (c) MSE and NLL on OOD.
Figure 40: No noise on Yacht.
41Published in Transactions on Machine Learning Research (04/2024)
0.0 0.2 0.4 0.6 0.8 1.0
a0.00.20.40.60.81.0Mean Squared Error
ID OOD
−1.5−1.0−0.50.00.51.01.5
NLL [nats]
ID OOD
(a) MSE and NLL.
0.0 0.1 0.2 0.3
a−0.10.00.10.20.30.40.50.6b
0.40.4
0.80.8
1.21.2
1.62.02.400
44
4
88
12162024Start weights End weights
0.20.61.01.41.82.22.63.03.43.8
Mean Squared Error
061218243036
NLL [nats] (b) MSE and NLL on ID.
0.0 0.1 0.2 0.3
a−0.10.00.10.20.30.40.50.6b
0.50.5
1.01.0
1.52.02.500
44
44
888
1216202428Start weights End weights
0.250.751.251.752.252.753.253.754.25
Mean Squared Error
0510152025303540
NLL [nats] (c) MSE and NLL on OOD.
Figure 41: Input Additive Gaussian on Yacht. Observations : While the shape of the 1D curves looks similar
to no noise, the MSE and NLL magnitudes are different. The OOD NLL is substantially higher than the
OOD NLL for no noise. The 2D plots demonstrate a wider landscape of feasible solutions than no noise.
0.0 0.2 0.4 0.6 0.8 1.0
a0.00.20.40.60.81.0Mean Squared Error
ID OOD
−1.00−0.75−0.50−0.250.000.250.500.75
NLL [nats]
ID OOD
(a) MSE and NLL.
0.00 0.05 0.10 0.15 0.20
a−0.10.00.10.20.30.4b
0.30.3
0.60.6
0.90.9
1.21.50.00.0
2.55.0
7.510
.0Start weights End weights
0.150.450.751.051.351.651.952.252.552.85
Mean Squared Error
036912151821
NLL [nats] (b) MSE and NLL on ID.
0.00 0.05 0.10 0.15 0.20
a−0.10.00.10.20.30.4b
0.30.3
0.60.6
0.90.9
1.21.51.80.00.0
2.55.07.510
.0Start weights End weights
0.150.450.751.051.351.651.952.252.552.85
Mean Squared Error
03691215182124
NLL [nats] (c) MSE and NLL on OOD.
Figure 42: Input-Target CMixUp on Yacht. Observations : While the shape of the 1D curves looks similar
to no noise, the MSE and NLL magnitudes are different. The 2D plots demonstrate a wider landscape of
feasible solutions than no noise.
0.0 0.2 0.4 0.6 0.8 1.0
a0.00.20.40.60.81.0Mean Squared Error
ID OOD
−1.5−1.0−0.50.00.5
NLL [nats]
ID OOD
(a) MSE and NLL.
0.0 0.1 0.2 0.3
a−0.10.00.10.20.30.40.50.6b
0.30.3
0.60.6
0.90.9
1.21.2
1.5−1.2−0.6−0.6
0.00.0
0.60.60.6
1.2
1.21.2
1.8
1.82.4
2.43.03.6Start weights End weights
0.150.450.751.051.351.651.952.252.552.85
Mean Squared Error
−1.2−0.60.00.61.21.82.43.03.64.2
NLL [nats] (b) MSE and NLL on ID.
0.0 0.1 0.2 0.3
a−0.10.00.10.20.30.40.50.6b
0.40.4
0.80.8
1.21.2
1.61.6
2.0004
481216Start weights End weights
0.20.61.01.41.82.22.63.03.4
Mean Squared Error
0510152025303540
NLL [nats] (c) MSE and NLL on OOD.
Figure 43: Activation Additive Gaussian on Yacht. Observations : While the shape of the 1D curves looks
similar to no noise, the MSE and NLL magnitudes are different. The 2D plots are close to the no-noise ones,
showing marginal differences.
42Published in Transactions on Machine Learning Research (04/2024)
0.0 0.2 0.4 0.6 0.8 1.0
a0.00.20.40.60.81.0Mean Squared Error
ID OOD
−1.0−0.50.00.5
NLL [nats]
ID OOD
(a) MSE and NLL.
0.0 0.1 0.2 0.3
a−0.10.00.10.20.30.40.50.6b
0.20.2
0.40.4
0.60.6
0.80.8
1.01.0
1.21.21.4
−1.0−1.0−0.50.00.51.0
1.01.0
1.5
1.51.5
2.02.5Start weights End weights
0.150.450.751.051.351.651.95
Mean Squared Error
−1.0−0.50.00.51.01.52.02.53.0
NLL [nats] (b) MSE and NLL on ID.
0.0 0.1 0.2 0.3
a−0.10.00.10.20.30.40.50.6b
0.25
0.500.50
0.750.75
1.001.00
1.251.251.50
005101520Start weights End weights
0.150.450.751.051.351.651.952.252.55
Mean Squared Error
0612182430364248
NLL [nats] (c) MSE and NLL on OOD.
Figure 44: Activation Dropout on Yacht. Observations : The 1D curves look similar to no noise but Dropout
converged in a narrow valley, as demonstrated in the 2D plots.
0.0 0.2 0.4 0.6 0.8 1.0
a0.00.20.40.60.81.0Mean Squared Error
ID OOD
−1.5−1.0−0.50.00.5
NLL [nats]
ID OOD
(a) MSE and NLL.
0.0 0.2 0.4 0.6 0.8
a−0.10.00.10.20.30.40.50.6b
0.60.6
1.21.82.43.03.64.24.8 5.4 5.4 6.00.00.0
1.51.5
3.03.0
4.54.5
6.07.5Start weights End weights
0.41.22.02.83.64.45.26.0
Mean Squared Error
−1.60.01.63.24.86.48.09.611.212.8
NLL [nats] (b) MSE and NLL on ID.
0.0 0.2 0.4 0.6 0.8
a−0.10.00.10.20.30.40.50.6b
0.80.8
1.62.43.24.04.8 5.6 5.6 6.4 6.4 7.20.00.0
1.51.5
3.03.0
4.54.5
6.07.59.0Start weights End weights
0.41.22.02.83.64.45.26.06.87.6
Mean Squared Error
0.01.63.24.86.48.09.611.212.8
NLL [nats] (c) MSE and NLL on OOD.
Figure 45: Gradient Gaussian on Yacht. Observations : The 1D curves remained unchanged except for the
magnitude of NLL or MSE. Nevertheless, the 2D plots show us that the optimisation trajectory significantly
differed from no noise where the landscape of potential optimal solutions was wider.
0.0 0.2 0.4 0.6 0.8 1.0
a0.20.40.60.81.0Mean Squared Error
ID OOD
−2024681012
NLL [nats]
ID OOD
(a) MSE and NLL.
−1.0−0.5 0.0
a−0.20.00.20.40.60.81.01.21.4b
500
10001500200080000160000
240000Start weights End weights
25075012501750225027503250375042504750
Mean Squared Error
40000120000200000280000360000440000520000600000680000760000
NLL [nats] (b) MSE and NLL on ID.
−1.0−0.5 0.0
a−0.20.00.20.40.60.81.01.21.4b
500
10001500200080000160000
240000Start weights End weights
3009001500210027003300390045005100
Mean Squared Error
40000120000200000280000360000440000520000600000680000760000
NLL [nats] (c) MSE and NLL on OOD.
Figure 46: Model Shrink and Perturb on Yacht. Observations : The model jumped between narrow valleys
as seed in the 2D plots and the 1D plots show smoother behaviour from the OOD perspective for MSE but
not NLL.
43Published in Transactions on Machine Learning Research (04/2024)
0.0 0.2 0.4 0.6 0.8 1.0
a0.00.20.40.60.81.0Mean Squared Error
ID OOD
−1.5−1.0−0.50.00.51.0
NLL [nats]
ID OOD
(a) MSE and NLL.
0.0 0.1 0.2 0.3
a−0.10.00.10.20.30.40.50.6b
0.40.4
0.80.8
1.21.2
1.62.0−0.8−0.8
0.00.0
0.80.80.8
1.6
1.61.6
2.4
2.42.43.2 3.2
3.2Start weights End weights
0.20.61.01.41.82.22.63.03.4
Mean Squared Error
−1.6−0.80.00.81.62.43.24.04.8
NLL [nats] (b) MSE and NLL on ID.
0.0 0.1 0.2 0.3
a−0.10.00.10.20.30.40.50.6b
0.40.4
0.80.8
1.21.2
1.61.6
2.02.4004080120Start weights End weights
0.20.61.01.41.82.22.63.03.43.8
Mean Squared Error
04080120160200240280320
NLL [nats] (c) MSE and NLL on OOD.
Figure 47: Weight Additive Gaussian on Yacht. Observations : Did not change the smoothness of the 1D
curves or the 2D trajectory.
0.0 0.2 0.4 0.6 0.8 1.0
a0.00.20.40.60.81.0Mean Squared Error
ID OOD
−2.0−1.5−1.0−0.50.00.51.0
NLL [nats]
ID OOD
(a) MSE and NLL.
0.0 0.1 0.2 0.3
a−0.10.00.10.20.30.40.50.6b
0.40.4
0.80.8
1.21.2
1.62.000
5101520Start weights End weights
0.20.61.01.41.82.22.63.03.4
Mean Squared Error
0510152025303540
NLL [nats] (b) MSE and NLL on ID.
0.0 0.1 0.2 0.3
a−0.10.00.10.20.30.40.50.6b
0.40.4
0.80.8
1.21.2
1.61.6
2.000
88
1616
2424
3232404856Start weights End weights
0.20.61.01.41.82.22.63.03.4
Mean Squared Error
0122436486072
NLL [nats] (c) MSE and NLL on OOD.
Figure 48: Weight DropConnect on Yacht. Observations : While the shape of the 1D curves looks similar
to no noise, the MSE and NLL magnitudes are different. The 2D plots demonstrate a wider landscape of
feasible solutions than no noise.
44