Published in Transactions on Machine Learning Research (06/2024)
Improving Variational Autoencoder Estimation from
Incomplete Data with Mixture Variational Families
Vaidotas Simkus vaidotas.simkus@ed.ac.uk
Michael U. Gutmann michael.gutmann@ed.ac.uk
School of Informatics
University of Edinburgh
Reviewed on OpenReview: https: // openreview. net/ forum? id= lLVmIvZfry
Abstract
We consider the task of estimating variational autoencoders (VAEs) when the training data
is incomplete. We show that missing data increases the complexity of the model’s posterior
distribution over the latent variables compared to the fully-observed case. The increased
complexity may adversely affect the fit of the model due to a mismatch between the vari-
ational and model posterior distributions. We introduce two strategies based on (i) finite
variational-mixture and (ii) imputation-based variational-mixture distributions to address
the increased posterior complexity. Through a comprehensive evaluation of the proposed
approaches, we show that variational mixtures are effective at improving the accuracy of
VAE estimation from incomplete data.
1 Introduction
Deep latent variable models, as introduced by Kingma & Welling (2013); Rezende et al. (2014); Goodfellow
et al. (2014); Sohl-Dickstein et al. (2015); Krishnan et al. (2016); Dinh et al. (2017), have emerged as a
predominant approach to model real-world data. The models excel in capturing the intricate nature of data
by representing it within a well-structured latent space. However, they typically require large amounts of
fully-observed data at training time, while practitioners in many domains often only have access to incomplete
data sets .
In this paper we focus on the class of variational autoencoders (VAEs, Kingma & Welling, 2013; Rezende
et al., 2014) and investigate the implications of incomplete training data on model estimation. Our contri-
butions are as follows:
•We show that data missingness can add significant complexity to the model posterior of the latent
variables, hencerequiringmoreflexiblevariationalfamiliescomparedtoscenarioswithfully-observed
data (section 3).
•We propose finite variational-mixture approaches to deal with the increased complexity due to
missingness for both standard and importance-weighted ELBOs (section 4.1).
•We further propose an imputation-based variational-mixture approach, which decouples model esti-
mation from data missingness problems, and as a result, improves model estimation when using the
standard ELBO (section 4.2).
•We evaluate the proposed methods for VAE estimation on synthetic and realistic data sets with
missing data (section 6).
The proposed methods achieve better or similar estimation performance compared to existing methods that
do not use variational mixtures. Moreover, the mixtures are formed by the variational families that are used
1Published in Transactions on Machine Learning Research (06/2024)
in the fully-observed case, which allows us to seamlessly re-use the inductive biases from the well-studied
scenarios with fully-observed data (see e.g. Miao et al., 2022, for the importance of inductive biases in VAEs).
2 Background: Standard approach for VAEs estimation from incomplete data
We consider the situation where some part of the training data-points might be missing. We denote the
observedandmissingpartsofthe i-thdata-point xibyxi
obsandxi
mis, respectively, where xiisD-dimensional
and the dimensions of xi
obsandxi
mismust add to D. This split into observed and missing components
corresponds to a missingness pattern mi∈{0,1}Dwithmi
j= 1if thej-th dimension is observed and mi
j= 0
if the dimension is missing. The missingness pattern miis generally different for each data-point and is a
realisation of a random variable mthat follows a typically unknown missingness distribution p∗(m|xi). We
make the common assumption that the missingness distribution does not depend on the missing variables,
which is known as the ignorable missingness or missing-at-random assumption (MAR, e.g. Little & Rubin,
2002, Section 1.3).1The MAR assumption allows us to ignore the missingness pattern miwhen fitting a
modelpθ(x)of the true distribution p∗(x)from incomplete data (see e.g. Seaman et al., 2013, Theorem 1), as
well as when performing multiple imputation of the missing data (see e.g. van Buuren, 2018, Section 2.2.6).
The VAE model with parameters θis typically specified using a decoder distribution pθ(x|z), parametrised
using a neural network, and a prior pθ(z)over the latents zthat can either be fixed or learnt. A principled
approachtohandlingincompletetrainingdataisthentomarginalisethemissingvariablesfromthelikelihood
pθ(x), which yields the marginal likelihood
pθ(xi
obs) =/integraldisplay
pθ(xi
obs,xi
mis) dxi
mis=/integraldisplay/integraldisplay
pθ(xi
obs,xi
mis|z)pθ(z) dzdxi
mis=/integraldisplay
pθ(xi
obs|z)pθ(z) dz,(1)
wheretheinnerintegral/integraltext
pθ(xobs,xmis|z) dxmisisoftencomputationallytractableinVAEsduetostandard
assumptions, such as the conditional independence of xgivenzor the use of the Gaussian family for the
decoderpθ(x|z). Similar to existing work, we also make the assumption that the marginalisation of the
missing variables is tractable. However, the marginal likelihood above remains intractable to compute as a
consequence of the integral over the latents z.
Due to the intractable integral, VAEs are typically fitted via a variational evidence lower-bound (ELBO)
logpθ(y)≥Eqϕ(z|y)/bracketleftigg
logpθ(y|z)pθ(z)
qϕ(z|y)/bracketrightigg
= logpθ(y)−DKL(qϕ(z|y)||pθ(z|y)), (2)
whereyrefers toxiin the fully-observed case, and to xi
obsin the incomplete-data case, and qϕ(z|y)
is an (amortised) variational distribution with parameters ϕthat is shared for all data-points in the data
set (Gershman & Goodman, 2014). The amortised distribution is parametrised using a neural network
(the encoder), which takes the data-point yas the input and predicts the distributional parameters of the
variational family. Moreover, when the data is incomplete, i.e. y=xi
obs, sharing of the encoder for any
pattern of missingness is often achieved by fixing the input dimensionality of the encoder to twice the size of
xand providing γ(xi
obs)andmias the inputs,2whereγ(·)is a function that takes the incomplete data-point
xobsand produces a vector of length Dwith the missing dimensions set to zero3(Nazábal et al., 2020; Mattei
& Frellsen, 2019).
From eq. (2), we see that the training objective for incomplete and fully-observed data has the same form,
and therefore it may seem that fitting VAEs from incomplete data would be similarly difficult to the fully-
observed case. However, as we will see next, data missingness can make model estimation much harder than
in the complete data case.
2Published in Transactions on Machine Learning Research (06/2024)
p(z|x) p(z|xobs) q(z|x) Ep(xmis|xobs)[q(z|x)]
Figure 1: Illustration of the posterior complexity due to missing data. Each colour represents a different data-
pointxi. First: the model posterior pθ(z|x)under complete data x. Second: the model posterior pθ(z|
xobs)under incomplete data xobs. Third: variational approximation qϕ(z|x)of the complete-data posterior
pθ(z|x). Fourth: an imputation-mixture variational approximation Epθ(xmis|xobs)[qϕ(z|xobs,xmis)]of
the incomplete posterior pθ(z|xobs). In these figures, we use a VAE with Gaussian variational, prior,
and decoder distributions fitted on complete data, then the incomplete data-points xobsare obtained by
randomly masking 50% of the values from the complete data-points x.
3 Implications of incomplete data for VAE estimation
The decomposition of the ELBO in eq. (2) emphasises that accurate estimation of the VAE model requires
the variational distribution qϕ(z|xobs)to accurately approximate the model posterior pθ(z|xobs). While
it might appear that the marginalisation of the missing variables in eq. (1) comes at no cost since the ELBO
maintains the same form as in the complete case, we here illustrate that his is not the case.
In the two left-most columns of fig. 1 we illustrate the model posteriors pθ(z|·)under fully-observed data
xand partially-observed data xobs.4We discover that the model posteriors pθ(z|x), which exhibited a
certain regularity in the complete-data scenario, have become irregular multimodal distributions pθ(z|xobs)
in the case of incomplete data.5Hence, accurate estimation of VAEs from incomplete data may require more
flexible variational families than in the fully-observed case: while a family may sufficiently well approximate
the model posterior in the fully-observed case, it may no longer be sufficiently flexible in the incomplete
data case. We provide a further explanation when this situation may occur in appendix A. As a result of
the mismatch between the model posterior pθ(z|xobs)and the variational distribution qϕ(z|xobs), the
incomplete-data KL divergence term DKL(qϕ(z|xobs)||pθ(z|xobs))in eq. (2) may be large compared to
the analogous KL term DKL(qϕ(z|x)||pθ(z|x))in the complete-data case, subsequently introducing a
bias to the fit of the model.
In the two right-most columns of fig. 1 we illustrate the variational approximations of the aforementioned
model posterior distributions, pθ(z|x)andpθ(z|xobs). The first of the two plots shows the complete-
data variational distribution qϕ(z|x)obtained after training, which well-approximates the model posterior
pθ(z|x). Inthesecondofthetwoplots,weconstructtheincomplete-dataposteriorapproximationasfollows:
pθ(z|xobs) =Epθ(xmis|xobs)[pθ(z|xobs,xmis)]≈Epθ(xmis|xobs)[qϕ(z|xobs,xmis)], which well-approximates
the model posterior pθ(z|xobs)too. Taken together, the two plots show that if the variational family
used in the fully-observed case well-approximates the model posterior, i.e. qϕ(z|x)≈pθ(z|x), then the
imputation-mixture Epθ(xmis|xobs)[qϕ(z|xobs,xmis)]will also be a good approximation of the incomplete-
data posterior pθ(z|xobs). This observation suggests that we can work with the same variational family
in both the fully-observed and incomplete data scenarios if we adopt a mixture approach. In the rest of
1While there is some historical disparity between MAR assumptions in the statistics literature, we can here adopt the weakest
MAR assumption, also known as the realisedMAR (Seaman et al., 2013).
2Alternative encoder architectures, such as, permutation-invariant networks (Ma et al., 2019) are also used.
3Equivalent to setting the missing dimensions to the empirical mean for zero-centered data.
4In fig. 1 we use a VAE with Gaussian variational, prior, and decoder distributions fitted on complete data.
5A related phenomenon, called posterior inconsistency, has been recently reported in concurrent work by Sudak & Tschi-
atschek (2023), relating pθ(z|xobs)andpθ(z|xobs\u), where uis a subset of the observed dimensions (see section 5).
3Published in Transactions on Machine Learning Research (06/2024)
this paper, we investigate opportunities to improve VAE estimation from incomplete data by constructing
variational mixture approximations of the incomplete-data posterior.
4 Fitting VAEs from incomplete data using mixture variational families
We propose working with mixture variational families to mitigate the increase in posterior complexity and
to improve the estimation accuracy of VAEs when the training data are incomplete. This allows us to
use families of distributions for the mixture components that are known to work well when the data is
fully-observed, and use the mixtures to handle the increased posterior complexity due to missing data.
We propose two approaches for constructing variational mixtures. In section 4.1 we specify qϕ(z|xobs)
as a finite-mixture distribution that can be learnt directly using the reparametrisation trick. In section 4.2
we investigate an imputation-based variational-mixture to approximate Epθ(xmis|xobs)[qϕ(z|xobs,xmis)].
Detailed evaluation of the proposed methods is provided in section 6.
4.1 Using finite mixture variational distributions to fit VAEs from incomplete data
In section 3 we saw that the imputation-mixture Epθ(xmis|xobs)[qϕ(z|xobs,xmis)]is a good approximation of
the incomplete-data posterior pθ(z|xobs)and would thus be a suitable variational distribution qϕ(z|xobs).
However, estimationoftheimputationdistribution pθ(xmis|xobs)isgenerallyintractableforVAEs(Rezende
et al., 2014; Mattei & Frellsen, 2018a; Simkus & Gutmann, 2023). Hence, we here consider a more tractable
approach and specify the variational distribution qϕ(z|xobs)in terms of a finite-mixture distribution:
qϕ(z|xobs) =K/summationdisplay
k=1qϕ(k|xobs)qk
ϕ(z|xobs), (3)
whereqϕ(k|xobs)is a categorical distribution over the components k∈{1,...,K}and each component
distribution qk
ϕ(z|xobs)belongs to any reparametrisable distribution family. Both qϕ(k|xobs)andqk
ϕ(z|
xobs)are amortised using an encoder network, similar to section 2.
The “reparametrisation trick” is typically used in VAEs to efficiently optimise the parameters ϕof the
variational distribution qϕ(z|xobs). This requires that the random variable zcan be parametrised as a
learnable differentiable transformation t(ϵ;xobs,ϕ)of another random variable ϵthat follows a distribution
with no learnable parameters. However, reparametrising mixture-families requires extra care: sampling the
mixtureqϕ(z|xobs)in eq. (3) is typically done via ancestral sampling by first drawing k∼qϕ(k|xobs)
and thenz∼qk
ϕ(z|xobs), but the sampling of the categorical distribution qϕ(k|xobs)is non-differentiable,
making the direct application of the “reparametrisation trick” generally infeasible.
To make the fitting of VAEs using mixture-variational distributions feasible we consider two objectives based
on the variational ELBO (Kingma & Welling, 2013; Rezende et al., 2014):
LELBO (xobs) =Eqϕ(z|xobs)[logw(z)],and (4)
LSELBO (xobs) =K/summationdisplay
k=1qϕ(k|xobs)Eqk
ϕ(z|xobs)[logw(z)], (5)
wherew(z) =pθ(xobs,z)
qϕ(z|xobs). (6)
The first objective LELBOcorresponds to the standard ELBO, while LSELBOis the stratified ELBO (Roeder
et al., 2017, Section 4; Morningstar et al., 2021). When working with LELBO, due to the mixture varia-
tional family, we will need to optimise ϕwithimplicitreparametrisation (Figurnov et al., 2019). Implicit
reparametrisation of mixture distributions requires that the component distributions qk
ϕ(z|xobs)can be
factorised using the chain rule, i.e. qk
ϕ(z|xobs) =/producttext
dqk
ϕ(zd|z<d,xobs), and that we have access to the
CDF (or other standardisation function) of each factor qk
ϕ(zd|z<d,xobs). However, the chain rule require-
ment can be difficult to satisfy for some highly flexible variational families, such as normalising flows (e.g.
4Published in Transactions on Machine Learning Research (06/2024)
Papamakarios et al., 2021), and finding the (conditional) CDF of the factors can also be hard if not already
known in closed form. Consequently, LELBOwith implicit reparametrisation may not be usable with all
distribution families as components of the variational mixture.
The second objective LSELBO, on the other hand, samples the mixture distribution with stratified sampling,6
which avoids the non-differentiability of sampling qϕ(k|xobs), and as a result allows us to use any family of
reparametrisable distributions as the mixture components.
Theimportance-weightedELBO(IWELBO,Burdaetal.,2015)isoftenusedasanalternativetothestandard
ELBOasitcanbemadetighter. Weherealsoconsideranordinaryversion, LIWELBO, andastratifiedversion,
LSIWELBO (Shi et al., 2019, Appendix A; Morningstar et al., 2021):
LI
IWELBO (xobs) =E{zj}I
j=1∼qϕ(z|xobs)
log1
II/summationdisplay
j=1w(zj)
,and (7)
LI
SIWELBO (xobs) =E{{zk
j}I
j=1∼qk
ϕ(z|xobs)}K
k=1
logK/summationdisplay
k=1qϕ(k|xobs)1
II/summationdisplay
j=1w(zk
j)
,7(8)
whereIisthenumberofimportancesamplesin LIWELBOandthenumberofsamplesper-mixture-component
inLSIWELBO .
When the number of mixture-components is K= 1the lower-bounds in eqs. (4-5) and eqs. (7-8) correspond
to the MVAE and MIWAE bounds in Mattei & Frellsen (2019) which are among the most popular bounds
for fitting VAEs from incomplete data. However, as K > 1the proposed bounds can be tighter due to
an increased flexibility of the variational distribution qϕ(z|xobs)(Morningstar et al., 2021, Appendix A),
which potentially mitigates the problems caused by the missing data (see section 3). Finally, the importance-
weighted bounds in eqs. (7) and (8) maintain the asymptotic consistency guarantees of Burda et al. (2015)
and approaches the true marginal log-likelihood logpθ(xobs)asK·I→∞, allowing for more accurate
estimation of the model with increasing computational budget.
We denote the four methods based on eqs. (4), (5), (7) and (8) by MissVAE ,MissSVAE ,MissIWAE ,
andMissSIWAE respectively.
4.2 Using imputation-mixture distributions to fit VAEs from incomplete data
Insection4.1, wejointlydealtwithboththeinferenceofthelatents z(section2)andtheposteriorcomplexity
increaseduetomissingdata(section3)bylearningafinite-mixturevariationaldistribution. Here, wepropose
a second “decomposed” approach to deal with the complexities of missing data.
Intuitively, if we had an oracle that were able to generate imputations of the missing data from the ground
truth conditional distribution p∗(xmis|xobs), then the VAE estimation task would reduce to the case
of complete-data. This suggests that an effective strategy is to decompose the task of model estimation
from incomplete data into two (iterative) tasks: (i) data imputation and (ii) model estimation , akin to the
Monte Carlo EM algorithm (Wei & Tanner, 1990; Dempster et al., 1977). However, access to the oracle
p∗(xmis|xobs)is unrealistic and the exact sampling of pθ(xmis|xobs), as required in EM, is generally
intractable. To address this, we resort to (i) approximate but computationally cheap conditional sampling
methods for VAEs to generate imputations (Rezende et al., 2014; Mattei & Frellsen, 2018a; Simkus &
6Stratified sampling of mixture distributions typically draws an equal number of samples from each component and weighs
the samples by the component probabilities qϕ(k|xobs)when estimating expectations. It is commonly used to reduce Monte
Carlo variance (Robert & Casella, 2004).
7In multimodal-domain VAE literature, Shi et al. (2019) proposed a looser bound related to LSIWELBO :
LI
SIWELBO(xobs)≥˜LI
SIWELBO(xobs)def=K/summationdisplay
k=1qϕ(k|xobs)E{zk
j}I
j=1∼qk
ϕ(z|xobs)/bracketleftigg
log1
II/summationdisplay
j=1w(zk
j)/bracketrightigg
I=1=LSELBO (xobs),
and empirically showed that it may alleviate potential mixture collapse to a subset of the mixture components. Therefore, the
looser bound may be useful when variational mixture collapse is observed.
5Published in Transactions on Machine Learning Research (06/2024)
Gutmann, 2023) and (ii) separate learning objectives for the model pθand the variational distribution qϕ
to compensate for potential sampling errors. We call the proposed approach DeMissVAE (decomposed
approach for handling missing data in VAEs).
We construct the variational distribution qϕ,ft(z|xobs)for an incomplete data-point xobsusing a
completed-data variational distribution qϕ(z|xobs,xmis)and an (approximate) imputation distribution
ft(xmis|xobs)≈pθ(xmis|xobs):
qϕ,ft(z|xobs) =Eft(xmis|xobs)/bracketleftbig
qϕ(z|xobs,xmis)/bracketrightbig
. (9)
The intuition for this construction comes from the decomposition of the model posterior pθ(z|xobs) =
Epθ(xmis|xobs)[pθ(z|xobs,xmis)]. Assuming that the completed-data variational distribution qϕ(z|
xobs,xmis)well-represents the model posterior pθ(z|xobs,xmis), and that the imputation distribution
ft(xmis|xobs)draws plausible imputations of the missing variables, then qϕ,ft(z|xobs)will reasonably
representpθ(z|xobs)(see the two right-most columns of fig. 1). A more technical justification is provided
below, in the paragraph after eq. (13). In contrast to section 4.1 we here use a continuous-mixture variational
distribution, which is more flexible than a finite-mixture distribution, albeit at an extra computational cost
due to sampling the (approximate) imputations (see appendix D).
We now derive the DeMissVAE objectives for fitting the generative model pθ(x)and the completed-data
variational distribution qϕ(z|xobs,xmis), see appendix C for a more in-depth treatment.
Objective for pθ(x,z).With the variational distribution in eq. (9), we derive an ELBO on the marginal
log-likelihood, similar to eq. (2), to learn the parameters θof the generative model:
logpθ(xobs)≥Eft(xmis|xobs)qϕ(z|xobs,xmis)
logpθ(xobs,z)
Eft(xmis|xobs)/bracketleftig
qϕ(z|xobs,xmis)/bracketrightig

=Eft(xmis|xobs)qϕ(z|xobs,xmis)[logpθ(xobs,z)]
/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
def=Lθ
CVI(xobs;ϕ,θ,ft)+H/bracketleftbig
qϕ,ft(z|xobs)/bracketrightbig
/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
Const. w.r.t. θ. (10)
This lower-bound can be further decomposed into log-likelihood and KL divergence terms
Lθ
CVI(xobs;ϕ,θ,ft) +H/bracketleftbig
qϕ,ft(z|xobs)/bracketrightbig
= logpθ(xobs)−DKL(qϕ,ft(z|xobs)||pθ(z|xobs)),(11)
which means that if qϕ,ft(z|xobs)≈pθ(z|xobs)then maximising eq. (10) w.r.t. θperforms approximate
maximum-likelihood estimation. Importantly, the missing variables xmisare marginalised-out, which adds
robustness to the potential sampling errors in ft(xmis|xobs).
Objective for qϕ(z|x).We obtain the objective for learning the variational distribution qϕ(z|x)by
marginalising the missing variables xmisfrom the complete-data ELBO in eq. (2) and then lower-bounding
the integral using ft(xmis|xobs)(see appendix B):
logpθ(xobs)≥Eft(xmis|xobs)qϕ(z|xobs,xmis)/bracketleftigg
logpθ(xobs,xmis,z)
qϕ(z|xobs,xmis)/bracketrightigg
/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
def=Lϕ
LMVB(xobs;ϕ,θ,ft)+H/bracketleftbig
ft(xmis|xobs)/bracketrightbig
/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
Const. w.r.t. ϕ.(12)
This lower-bound can also be decomposed into the log-likelihood term and two KL divergence terms
Lϕ
LMVB (xobs;ϕ,θ,ft) +H/bracketleftbig
ft(xmis|xobs)/bracketrightbig
= logpθ(xobs)−DKL(ft(xmis|xobs)||pθ(xmis|xobs))
−Eft(xmis|xobs)/bracketleftbig
DKL(qϕ(z|xobs,xmis)||pθ(z|xobs,xmis))/bracketrightbig
,
(13)
which means that the bound is maximised w.r.t. ϕiffqϕ(z|xobs,xmis) =pθ(z|xobs,xmis)for allxmis.
Therefore, using the above objective to fit qϕcorresponds directly to the complete-data case, and hence
avoids having to approximate complex posteriors that arise due to missing data (see 3).
6Published in Transactions on Machine Learning Research (06/2024)
Ifqϕ(z|xobs,xmis) =pθ(z|xobs,xmis)for allxmis, then maximising either of the bounds in eqs. (10)
or (12) w.r.t. the imputation distribution ft(xmis|xobs)would correspond to setting ft(xmis|xobs) =
pθ(xmis|xobs). However, directly learning an imputation distribution ft(xmis|xobs)≈pθ(xmis|xobs)
is challenging (Simkus et al., 2023, Section 2.2). This motivates using sampling methods to approximate
the optimal imputation distribution ft(xmis|xobs)≈pθ(xmis|xobs)with samples. We draw samples from
ft(xmis|xobs)using (cheap) approximate conditional sampling methods for VAEs to obtain Kimputations
{xk
mis}K
kand then use them to approximate the expectations w.r.t. ft(xmis|xobs)in the above objectives.
We discuss the implementation of the algorithm in detail in appendix D.
Finally, we note that the Lθ
CVIandLϕ
LMVBobjectives in eqs. (10) and (12) are based on the standard
ELBO. Extensions to the importance-weighted ELBO might improve the method further by increasing the
flexibility of the variational posterior. However, unlike the standard ELBO used in eq. (10) where the density
of the imputation-based variational-mixture qϕ,ft(z|xobs)can be dropped, IWELBO requires computing
the density of the proposal distribution qϕ,ft(z|xobs), which is generally intractable. We hence leave this
direction for future work.
5 Related work
FittingVAEsfromincompletedata. SincetheseminalworksofKingma&Welling(2013)andRezende
et al. (2014), VAEs have been widely used for density estimation from incomplete data and various down-
streamtasks,primarilyduetothecomputationallyefficientmarginalisationofthemodelineq.(1). Vedantam
et al. (2017) and Wu & Goodman (2018) explored the use of product-of-experts variational distributions,
drawing inspiration from findings in the factor analysis case with incomplete data (Williams et al., 2018).
Mattei & Frellsen (2019) used the importance-weighted ELBO (Burda et al., 2015) for training VAEs on
incomplete training data sets. Ma et al. (2019) proposed the use of permutation invariant neural networks
to parametrise the encoder network instead of relying on zero-masking. Nazábal et al. (2020) introduced
hierarchical priors to handle incomplete heterogeneous training data. Simkus et al. (2023) proposed a
general-purpose approach that is applicable to VAEs, not requiring the decoder distribution to be easily
marginalisable. Here, we further develop the understanding of VAEs in the presence missing values in the
training data set, and propose variational-mixtures as a natural approach to improve VAE estimation from
incomplete data, building upon the motivation from imputation-mixtures discussed in section 3.
Variational mixture distributions. Mixture distributions have found widespread application in vari-
ational inference and VAE literature. Roeder et al. (2017) introduced the stratified ELBO corresponding
to eq. (5). In the context of VAEs in multimodal domains, Shi et al. (2019, Appendix A) introduced the
stratified IWELBO corresponding to eq. (8), but opted to use a looser bound instead, see footnote 7. These
bounds were subsequently rediscovered by Morningstar et al. (2021) and Kviman et al. (2023), who inves-
tigated their use for VAE estimation in fully-observed data scenarios. Figurnov et al. (2019) introduced
implicit reparametrisation, enabling gradient estimation for ancestrally-sampled mixtures, allowing the es-
timation of variational mixtures using eqs. (4) and (7). Here, we build on this prior work, asserting that
variational-mixtures are well-suited for handling the posterior complexity increase due to missing data (see
section 3). Moreover, the imputation-mixture distribution used in DeMissVAE is a novel type of variational
mixtures specifically designed for incomplete data scenarios.
Posterior complexity increase due to missing data. Concurrent to this study, Sudak & Tschiatschek
(2023) have brought attention to a phenomenon related to the increase in posterior complexity due to
incomplete data, discussed in section 3. They noted that, for any xobsandxobs\u, whereuis a subset of the
observeddimensions,themodelposteriors pθ(z|xobs)andpθ(z|xobs\u)shouldexhibitastrongdependency.
However, because of the approximations in the variational posterior (see e.g. Cremer et al., 2018; Zhang
et al., 2021), the variational approximations qϕ(z|xobs)andqϕ(z|xobs\u)may not consistently capture
this dependency. They refer to the lack of dependency between qϕ(z|xobs)andqϕ(z|xobs\u), compared
topθ(z|xobs)andpθ(z|xobs\u), as posterior inconsistency. Focused on improving downstream task
performance, theyintroduceregularisationintotheVAEtrainingobjectivetoaddressposteriorinconsistency.
In contrast to their work, we compare the fully-observed and incomplete-data posteriors, pθ(z|x)and
7Published in Transactions on Machine Learning Research (06/2024)
Method pθobjective qϕobjective # of components Mixture sampling
MVAE†eq. (4) eq. (4) K= 1 —
MissVAE eq. (4) eq. (4) K > 1 Ancestral
MissSVAE eq. (5) eq. (5) K > 1 Stratified
MIWAE†eq. (7) eq. (7) K= 1 —
MissIWAE eq. (7) eq. (7) K > 1 Ancestral
MissSIWAE eq. (8) eq. (8) K > 1 Stratified
DeMissVAE eq. (10) eq. (12) K > 1 Conditional VAE
Table 1: Summary of the proposed and baseline methods. The non-mixture baselines ( †) are based on Mattei
& Frellsen (2019) and the other methods are proposed in this paper. Moreover, the methods using ancestral
sampling require implicit reparametrisation (Figurnov et al., 2019), whereas the other methods work with
the standard reparametrisation trick.
pθ(z|xobs), respectively. Observing that the incomplete-data posterior pθ(z|xobs)can be expressed as a
mixture of fully-observed posteriors pθ(z|x), that is,pθ(z|xobs) =Epθ(xmis|xobs)[pθ(z|x)], we propose
using variational-mixtures to improve the match between the variational and model posteriors when dealing
with incomplete data in order to improve model estimation performance.
Marginalised variational bound. In the standard ELBO derivation for incomplete data in eq. (2) the
missing variables are first marginalised (collapsed) from the likelihood, and then a variational ELBO is
established. This approach is sometimes referred to as collapsed variational inference (CVI). In contrast,
in the derivation of the DeMissVAE encoder objective in eq. (12) we swap the order of marginalisation
and variational inference. Specifically, we start with the variational ELBO on completed-data, and then
marginalise the missing variables (see appendix B). This approach bears similarity to the marginalised
variational bound (MVB, or KL-corrected bound) in exponential-conjugate variational inference literature
(King & Lawrence, 2006; Lázaro-Gredilla & Titsias, 2011; Hensman et al., 2012). In these works, MVB
has been preferred over CVI due to improved convergence and guarantees that, for appropriately formulated
conjugatemodels, MVBisanalyticallytractableincaseswhereCVIisnot(Hensmanetal.,2012, Section3.3).
While MVB remains intractable in the VAE setting with incomplete data, similar to how the standard ELBO
is intractable in fully-observed case, we find the motivation behind MVB and DeMissVAE to be similar.
6 Evaluation
We here evaluate the proposed methods, MissVAE, MissSVAE, MissIWAE, MissSIWAE (section 4.1), and
DeMissVAE (section 4.2), on synthetic and real-world data, and compare them to the popular methods
MVAE and MIWAE that do not use mixture variational distributions (Mattei & Frellsen, 2019). The
methods are summarised in table 1 and the code implementation is available at https://github.com/
vsimkus/demiss-vae .
6.1 Mixture-of-Gaussians data with a 2D latent VAE
Evaluating log-likelihood on held-out data is generally intractable for VAEs due to the intractable integral
in eq. (1). We hence here choose a VAE with 2D latent space, where numerical integration can be used to
estimate the log-likelihood of the model accurately (see appendix E.1 for more details). We fit the model
on incomplete data drawn from a mixture-of-Gaussians distribution. By introducing uniform missingness
of 50% in the mixture-of-Gaussians data we introduce multi-modality in the latent space (see fig. 1), which
allows us to verify the efficacy of mixture-variational distributions when the posteriors are multi-modal due
to missing data.
Results are shown in fig. 2. We first note that the stratified MissSVAE approach performed better than
MissVAE that uses ancestral sampling. The reason for this is likely that stratified sampling reduces Monte
Carlo variance of the gradients w.r.t. ϕand hence enables a better fit of the variational distribution qϕ(z|
8Published in Transactions on Machine Learning Research (06/2024)
Z=5
Z=15
Z=25
K=5 Z=5
K=15 Z=15
K=25 Z=25
K=5 Z=1
K=15 Z=1
K=25 Z=1
I=5
I=15
I=25
K=5 I=5
K=15 I=15
K=25 I=25
K=5 I=1
K=15 I=1
K=25 I=1
K=5
K=15
K=25−4.25−4.00−3.75−3.50−3.25−3.00−2.75−2.50Log-likelihood
MVAE†MissVAE MissSVAE MIWAE†MissIWAE MissSIWAE DeMissVAE
Figure 2: Log-likelihood on held out data evaluated by numerically integrating the 2D latent variables. VAEs
were fitted on mixture-of-Gaussians data with 50% missingness. Each model is fitted with a computational
budget of 5/15/25 samples from the variational distribution. The box plots show 1st and 3rd quartiles, the
black lines are the medians, the dashed lines are the means, and the whiskers show the data range over 5
independent runs. MVAE and MIWAE ( †) are baseline methods by Mattei & Frellsen (2019). The other
five methods are proposed in this paper.
20% 50% 80%
Missingness0.02.55.07.510.0Log-likelihoodGAS
20% 50% 80%
Missingness−3−2−10POWER
20% 50% 80%
Missingness−28−26−24−22−20HEPMASS
20% 50% 80%
Missingness−30−25−20−15MINIBOONEMVAE
MissVAE
MissSVAEMIWAE
MissIWAE
MissSIWAEDeMissVAE
Figure 3: Estimate of the test log-likelihood using the IWELBO with I= 50000, on four UCI data sets. Each
data set was rendered incomplete by applying uniform missingness of 20/50/80%. The curves show average
performance over 5 independent runs of the algorithms and the intervals show the 90% centered interval.
xobs)(see a further investigation in appendix F.1.1). In line with this intuition, the MissVAE results exhibit
significantly larger variance than MissSVAE. Similarly, we observe that the stratified MissSIWAE approach
performed better than MissIWAE. Importantly, we see that the use of mixture variational distributions
in MissSVAE and MissSIWAE improve the model fit over the MVAE and MIWAE baselines that do not
use mixtures to deal with the increased posterior complexity due to missingness. Finally, we observe that
DeMissVAE is capable of achieving comparable performance to MIWAE and MissSIWAE, despite using a
looser ELBO bound, which shows that the decomposed approach to handling data missingness can be used
to achieve an improved fit of the model.
In appendix F.1.2, we analyse the model and variational posteriors of the learnt models. We observe that
the mixture approaches better-approximate the incomplete-data posteriors, compared to the approaches
that do not use variational-mixtures. Moreover, we also observe that the structure of the latent space is
better-behaved when fitted using the decomposed approach in DeMissVAE.
9Published in Transactions on Machine Learning Research (06/2024)
MVAE
MissVAE
MissSVAE
MIWAE
MissIWAE
MissSIWAE
DeMissVAE−98−96−94−92−90Log-likelihood
MNIST
MVAE
MissVAE
MissSVAE
MIWAE
MissIWAE
MissSIWAE
DeMissVAE−130−125−120−115
Omniglot
Figure 4: Estimate of the test log-likelihood using the IWELBO with I= 1000, MNIST and Omniglot data
sets.Each image in the training data set was missing 2 out of 4 random quadrants. The box plots show 1st
and 3rd quartiles, the black lines are the medians, the dashed lines are the means, and the whiskers show
the data range over 5 independent runs.
6.2 Real-world UCI data sets
We here evaluate the proposed methods on real-world data sets from the UCI repository (Dua & Graff, 2017;
Papamakarios et al., 2017). We train a VAE model with ResNet architecture on incomplete data sets with
20/50/80% uniform missingness (see appendix E.2 for more details). We then estimate the log-likelihood on
complete test data set using the IWELBO bound with I= 50K importance samples.8For additional metrics
see appendix F.2.
The results are shown in fig. 3. We first note that, similar to before, the stratified MissSVAE approach
performed better than MissVAE which uses ancestral sampling. Importantly, we observe that using mixture
variational distributions in MissSVAE improves the fit of the model over MVAE (with the exception on
the Miniboone data set) that uses non-mixture variational distributions. Furthermore, the gains in model
accuracy typically increase with data missingness, which verifies that MissSVAE performs better because it
handles the increased posterior complexity due to missing data better (see fig. 1). Next, we observe that the
performance of MIWAE, MissIWAE, and MissSIWAE is similar, although we can note a small improvement
by using MissIWAE and MissSIWAE in large missingness settings. We observe only a relatively small
difference between the IWAE methods because the use of importance weighted bound already corresponds
tousingamoreflexiblesemi-implicitlydefinedvariationaldistribution(Cremeretal.,2017),whichhereseems
to be sufficient to deal with the complexities arising due to missingness. Finally, we note that DeMissVAE
results are in-between MissSVAE and MIWAE. This verifies that the decomposed approach can be used to
deal with data missingness and, as a result, can improve the fit of the model. Nonetheless, DeMissVAE is
surpassed by the IWAE methods, which is likely due to using the ELBO in DeMissVAE versus IWELBO in
IWAE methods that can tighten the bound more effectively.
6.3 MNIST and Omniglot data sets
In this section we evaluate the proposed methods on binarised MNIST (Garris et al., 1991) and Omniglot
(Lake et al., 2015) data sets of handwritten characters. We fit a VAE model with a convolutional ResNet
encoder and decoder networks (see appendix E.3 for more details). The data is made incomplete by masking
2 out of 4 quadrants of an image at random. Similar to the previous section, we estimate the log-likelihood
8AsI→∞IWELBO approaches logpθ(x). Moreover, as suggested by Mattei & Frellsen (2018b), to improve the estimate
on held-out data we fine-tune the encoder on complete test data before estimating the log-likelihood.
10Published in Transactions on Machine Learning Research (06/2024)
on a complete test data set using the IWELBO bound with I= 1000importance samples. In appendix F.3
we report additional results with varying dimensionality of the latent variables.
On the MNIST data set we see that MVAE ≤MissVAE<MissSVAE similar to the previous results but
MIWAE<MissSIWAE <MissIWAE.ThissuggeststhatMissIWAE,whichusesancestralsampling, wasable
to tighten the bound more effectively compared to stratified MissSIWAE, and was able to fit the variational
distribution qϕ(z|xobs)well despite the potentially larger variance w.r.t. ϕ. Moreover, we also see that
MVAE<DeMissVAE <MIWAE, which further verifies that the decomposed approach is able to handle the
data missingness well.
On the Omniglot data we observe that the mixture approaches perform similarly to MVAE and MIWAE,
which do not use mixture variational distributions. This suggests that either the posterior multi-modality is
less prominent in the Omniglot data set or that due to the reverse KL optimisation of the variational distri-
bution all mixture components have degenerated to a single mode. Finally, DeMissVAE slightly outperforms
MVAE, MissVAE, and MissSVAE, but is surpassed by the importance-weighted approaches.
Interestingly, in this evaluation the stratified approaches (MissSVAE and MissSIWAE) were outperformed
by the approaches using standard ELBO and implicit reparametrisation (MissVAE and MissIWAE). This
suggests that the performance of each approach can be data- and model-dependent and hence both should
be evaluated when possible.
7 Discussion
Handling missing data is a key challenge in modern machine learning, as many real-world applications involve
incomplete data. In the context of variational autoencoders, we have shown that incomplete data increases
the complexity of the latent variables’ posterior distribution. Therefore, accurately fitting models from
incomplete data requires more flexible variational families than in the complete-data case. We stipulated
that variational-mixtures are a natural approach for handling missing data. One benefit is that it allows us
to work with the same variational families as in the fully-observed case, which enables the transfer of useful
known inductive biases (Miao et al., 2022) from the fully-observed to the incomplete data scenario.
Subsequently, we have introduced two methodologies grounded in variational mixtures. First, we proposed
using finite variational mixtures with the standard and importance-weighted ELBOs using ancestral and
stratified sampling of the mixtures. Second, we have proposed a novel “decomposed” variational-mixture
approach, that uses cost-effective yet often coarse conditional sampling methods for VAEs to generate im-
putations and ELBO-based objectives that are robust to the sampling errors.
Our evaluation shows that using variational mixtures can improve the fit of VAEs when dealing with incom-
plete data, surpassing the performance of models without variational mixtures. Moreover, our observations
indicate that, although stratified sampling of the finite mixtures often yields better results compared to
ancestral sampling, the effectiveness of these methods can be data- and model-dependent and hence both
approaches should be evaluated when possible. Our results further indicate that variational mixtures provide
relatively little improvement with the IWELBO-based methods compared the ELBO-based methods. We
believe that this is mainly because IWELBO can be seen to be working with semi-implicitly defined varia-
tional distributions that are flexible enough to handle the posterior complexity increase due to missing data.
Alternatively, this may be related to an observation by Shi et al. (2019, Appendix A) that the reverse-KL
formulationoftheimportance-weightedboundmayleadtosituationswherethemixturecomponentscollapse
to a single mode. Hence, a future direction would be to investigate alternative formulations of importance-
weighted bounds that avoid the mode-seeking nature (Bornschein & Bengio, 2015; Mnih & Rezende, 2016;
Wan et al., 2020). Furthermore, we note that the decomposed approach in DeMissVAE outperforms all
ELBO-based methods but falls short of surpassing IWELBO-based methods. These results point towards
promising research avenues, suggesting potential improvements in VAE model estimation from incomplete
data. Future directions include extending the DeMissVAE approach to incorporate IWELBO-based objec-
tives and developing improved cost-effective conditional sampling methods for VAEs.
11Published in Transactions on Machine Learning Research (06/2024)
Budget Variational
familiesLatent
structure*Evaluation rank
Method MoG UCI MNIST +Omniglot
MissVAE small limited well-behaved 5 5 5
MissSVAE medium any well-behaved 4 4 4
MissIWAE medium limited potentially irregular 3 2 1
MissSIWAE medium any potentially irregular 1 1 2
DeMissVAE medium/high any well-behaved+2 3 3
Table 2: A coarse summary of advantages and disadvantages of the proposed methods. Budget :
small/medium/high depending on the number of latent samples required or whether conditional sampling of
VAEs is needed. Variational families : which families of distributions can be used as mixture components—
any reparametrisable families, or limited families, as discussed in section 4.1. Latent structure : methods
with potential to learn irregular latent spaces may have decreased downstream performance in certain tasks.
We have found ( +) that DeMissVAE is able to achieve the most well-behaved latent structures on the MoG
data in appendix F.1.2. Please note (*) that the learnt latent structure will depend on the chosen model
architecture. Evaluation rank : the rank of the proposed methods in the evaluations in sections 6.1 to 6.3.
The choice between the proposed methods for fitting VAEs from incomplete data depends on various factors
such as computational budget, variational families, model accuracy goals, and the specific requirements of
downstream tasks, discussed next and summarised in table 2.
Computational and memory budget. The standard ELBO with ancestral sampling is the most suitable
method for small computational and memory budgets, since the objective can be estimated using a
single latent sample for each data point. On the other hand, methods using stratified sampling or
the importance-weighted ELBO require multiple latent samples for each data-point and hence may
only be used if the memory and compute budget allows. Moreover, for a fixed budget, stratified
approaches may limit the number of components Kthat may be used. Lastly, akin to the standard
ELBO, the DeMissVAE objectives can be estimated using a single latent sample, but the approach
incurs extra cost in sampling the imputations.
Variational families. While the stratified and DeMissVAE approaches can use any reparametrisable dis-
tribution family for the mixture components, the ancestral sampling methods require the use of
implicitreparametrisation (Figurnov et al., 2019) and as a result may not work with all distribution
families (see discussion in section 4.1).
Model accuracy. Stratified sampling of mixtures can improve the model accuracy, compared to ancestral
sampling, by reducing Monte Carlo gradient variance. Additionally, methods using the importance-
weighted ELBO, compared to the standard ELBO, are often able to tighten the bound more ef-
fectively by using multiple importance samples, leading to improved model accuracy. DeMissVAE
performance lies in between the standard ELBO and importance-weighted ELBO approaches. Al-
though the introduced DeMissVAE objectives exhibit robustness to some imputation distribution
error, improved model accuracy can often be achieved by improving the accuracy of imputations by
using a larger budget for the imputation step.
Latent structure. Different downstream tasks may prefer distinct latent structures, for example, condi-
tional generation from unconditional VAEs is often easier if the latent space is well-structured (Engel
et al., 2017; Gómez-Bombarelli et al., 2018). To this end, observations in appendix F.1.2 show that
the latent space of DeMissVAE behaves well, and is comparable to a model fitted with complete
data. This characteristic makes it preferable for downstream tasks requiring well-structured latent
spaces. On the other hand, as noted by Burda et al. (2015, Appendix C) and Cremer et al. (2018,
Section 5.4), the use of importance-weighted ELBO to mitigate the increased posterior complex-
ity due to missing data may make the latent space less regular, compared to a model trained on
fully-observed data set, which potentially decreases the model’s performance on downstream tasks.
12Published in Transactions on Machine Learning Research (06/2024)
Finally, we step back to note that this paper is focused on the class of variational autoencoder models, a
subset of the broader family of deep latent variable models (DLVMs). Much like VAEs, DLVMs usually
aim to efficiently represent the intricate nature of data through a well-structured latent space, implicitly
defined by a learnable generative process. Building on our findings in VAEs, where incomplete data led to
an increased complexity in the posterior distribution compared to the fully-observed case, we conjecture that
a similar effect may occur within the wider family of DLVMs, affecting the fit of the model. We therefore
believe that there is substantial scope to explore the implications of incomplete data in other DLVM classes,
particularly focusing on the effects of marginalisation on latent space representations and the associated
generative processes. Investigating decomposed approaches, similar to DeMissVAE or Monte Carlo EM
(Wei & Tanner, 1990), presents promising avenues for further research in this direction.
References
Jörg Bornschein and Yoshua Bengio. Reweighted Wake-Sleep. In International Conference on Learning
Representations (ICLR) , April 2015. doi: 10.48550/arXiv.1406.2751. (Cited on 11)
Yuri Burda, Roger Grosse, and Ruslan Salakhutdinov. Importance Weighted Autoencoders. In International
Conference on Learning Representations (ICLR) , San Juan, Puerto Rico, September 2015. (Cited on 5,
7, 12, 17, 20, 25)
Chris Cremer, Quaid Morris, and David Duvenaud. Reinterpreting Importance-Weighted Autoencoders. In
ICLR Workshop , February 2017. (Cited on 10, 17, 25, 27)
Chris Cremer, Xuechen Li, and David Duvenaud. Inference Suboptimality in Variational Autoencoders. In
International Conference on Machine Learning (ICML) , May 2018. (Cited on 7, 12, 25)
Arthur P. Dempster, Nan M. Laird, and Donald B. Rubin. Maximum Likelihood from Incomplete Data Via
the EM Algorithm. Journal of the Royal Statistical Society: Series B (Methodological) , 39(1):1–22, 1977.
doi: 10.1111/j.2517-6161.1977.tb01600.x. (Cited on 5)
LaurentDinh, JaschaSohl-Dickstein, andSamyBengio. DensityestimationusingRealNVP. In International
Conference on Learning Representations (ICLR) , February 2017. (Cited on 1)
Dheeru Dua and Casey Graff. UCI Machine Learning Repository, 2017. (Cited on 10, 23)
Jesse Engel, Matthew Hoffman, and Adam Roberts. Latent Constraints: Learning to Generate Conditionally
from Unconditional Generative Models. In International Conference on Learning Representations (ICLR) ,
December 2017. (Cited on 12)
Michael Figurnov, Shakir Mohamed, and Andriy Mnih. Implicit Reparameterization Gradients. In Advances
in Neural Information Processing Systems (NeurIPS) , January 2019. (Cited on 4, 7, 8, 12)
Michael D. Garris, R. A. Wilkinson, and Charles L. Wilson. Methods for enhancing neural network hand-
written character recognition. In International Joint Conference on Neural Networks (IJCNN) , volume 1,
pp. 695–700, Seattle, WA, USA, 1991. IEEE. ISBN 978-0-7803-0164-1. doi: 10.1109/IJCNN.1991.155265.
(Cited on 10)
Samuel J. Gershman and Noah D. Goodman. Amortized Inference in Probabilistic Reasoning. In Annual
Meeting of the Cognitive Science Society , volume 36, 2014. (Cited on 2)
Rafael Gómez-Bombarelli, Jennifer N. Wei, David Duvenaud, José Miguel Hernández-Lobato, Benjamín
Sánchez-Lengeling, Dennis Sheberla, Jorge Aguilera-Iparraguirre, Timothy D. Hirzel, Ryan P. Adams,
and Alán Aspuru-Guzik. Automatic Chemical Design Using a Data-Driven Continuous Representation of
Molecules. ACS Central Science , 4(2):268–276, February 2018. ISSN 2374-7943. doi: 10.1021/acscentsci.
7b00572. (Cited on 12)
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron
Courville, and Yoshua Bengio. Generative Adversarial Networks. In Advances in Neural Information
Processing Systems (NeurIPS) , June 2014. (Cited on 1)
13Published in Transactions on Machine Learning Research (06/2024)
James Hensman, Magnus Rattray, and Neil D. Lawrence. Fast Variational Inference in the Conjugate
Exponential Family. In Advances in Neural Information Processing Systems (NeurIPS) , December 2012.
(Cited on 8)
Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. GANs
Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium. In Advances in Neural
Information Processing Systems (NeurIPS) , 2017. (Cited on 28)
Niels Bruun Ipsen, Pierre-Alexandre Mattei, and Jes Frellsen. Not-MIWAE: Deep Generative Modelling
with Missing not at Random Data. In International Conference on Learning Representations (ICLR) ,
June 2020. (Cited on 17)
Nathaniel J. King and Neil D. Lawrence. Fast Variational Inference for Gaussian Process Models Through
KL-Correction. In European Conference on Machine Learning (ECML) , 2006. (Cited on 8)
Diederik P. Kingma and Max Welling. Auto-Encoding Variational Bayes. In International Conference on
Learning Representations (ICLR) , December 2013. (Cited on 1, 4, 7)
Rahul G. Krishnan, Uri Shalit, and David Sontag. Structured Inference Networks for Nonlinear State Space
Models. In AAAI Conference on Artificial Intelligence , December 2016. doi: 10.48550/arXiv.1609.09869.
(Cited on 1)
Oskar Kviman, Ricky Molén, Alexandra Hotti, Semih Kurt, Víctor Elvira, and Jens Lagergren. Cooperation
in the Latent Space: The Benefits of Adding Mixture Components in Variational Autoencoders. In Inter-
national Conference on Machine Learning (ICML) , July 2023. doi: 10.48550/arXiv.2209.15514. (Cited on
7)
Brenden M. Lake, Ruslan Salakhutdinov, and Joshua B. Tenenbaum. Human-level concept learning through
probabilistic program induction. Science, 2015. doi: 10.1126/science.aab3050. (Cited on 10, 23)
Miguel Lázaro-Gredilla and Michalis K. Titsias. Variational heteroscedastic Gaussian process regression. In
International Conference on Machine Learning (ICML) , June 2011. (Cited on 8)
Roderick J. A. Little and Donald B. Rubin. Statistical Analysis with Missing Data: Second Edition . Wiley-
Interscience, 2002. ISBN 0-471-18386-5. (Cited on 2)
Chao Ma and Cheng Zhang. Identifiable Generative Models for Missing Not at Random Data Imputation.
InAdvances in Neural Information Processing Systems (NeurIPS) , October 2021. (Cited on 17)
Chao Ma, Sebastian Tschiatschek, Konstantina Palla, José Miguel Hernández-Lobato, Sebastian Nowozin,
and Cheng Zhang. EDDI: Efficient dynamic discovery of high-value information with partial VAE. In
International Conference on Machine Learning (ICML) , pp. 7483–7504, 2019. ISBN 9781510886988.
(Cited on 3, 7)
Pierre-Alexandre Mattei and Jes Frellsen. Leveraging the Exact Likelihood of Deep Latent Variable Models.
InAdvances in Neural Information Processing Systems (NeurIPS) , February 2018a. (Cited on 4, 5, 20,
23)
Pierre-Alexandre Mattei and Jes Frellsen. Refit your Encoder when New Data Comes by. In Workshop on
Bayesian Deep Learning at Neural Information Processing Systems (NeurIPS) , pp. 4, Montreal, Canada,
2018b. (Cited on 10)
Pierre-AlexandreMatteiandJesFrellsen. MIWAE:DeepGenerativeModellingandImputationofIncomplete
Data Sets. In International Conference on Machine Learning (ICML) , 2019. (Cited on 2, 5, 7, 8, 9, 17)
Xiao-Li Meng. On the Rate of Convergence of the ECM Algorithm. The Annals of Statistics , 22(1):326–339,
March 1994. ISSN 0090-5364, 2168-8966. doi: 10.1214/aos/1176325371. (Cited on 18)
14Published in Transactions on Machine Learning Research (06/2024)
Ning Miao, Emile Mathieu, N. Siddharth, Yee Whye Teh, and Tom Rainforth. On Incorporating Inductive
Biases into VAEs. In International Conference on Learning Representations (ICLR) , February 2022. doi:
10.48550/arXiv.2106.13746. (Cited on 2, 11)
Andriy Mnih and Danilo J. Rezende. Variational inference for Monte Carlo objectives. arXiv:1602.06725
[cs, stat], June 2016. (Cited on 11)
Warren Morningstar, Sharad Vikram, Cusuh Ham, Andrew Gallagher, and Joshua Dillon. Automatic Differ-
entiation Variational Inference with Mixtures. In International Conference on Artificial Intelligence and
Statistics (AISTATS) , pp. 3250–3258. PMLR, March 2021. (Cited on 4, 5, 7)
Alfredo Nazábal, Pablo M. Olmos, Zoubin Ghahramani, and Isabel Valera. Handling Incomplete Heteroge-
neous Data using VAEs. Pattern Recognition , 107, 2020. ISSN 0031-3203. doi: 10.1016/j.patcog.2020.
107501. (Cited on 2, 7)
GeorgePapamakarios, TheoPavlakou, andIainMurray. MaskedAutoregressiveFlowforDensityEstimation.
Advances in Neural Information Processing Systems (NeurIPS) , 30, 2017. (Cited on 10, 23)
George Papamakarios, Eric Nalisnick, Danilo Jimenez Rezende, Shakir Mohamed, and Balaji Lakshmi-
narayanan. Normalizing Flows for Probabilistic Modeling and Inference. Journal of Machine Learning
Research , 22(57):1–64, 2021. (Cited on 5)
Tom Rainforth, Adam R. Kosiorek, Tuan Anh Le, Chris J. Maddison, Maximilian Igl, Frank Wood, and
Yee Whye Teh. Tighter Variational Bounds are Not Necessarily Better. In International Conference on
Machine Learning (ICML) , March 2019. (Cited on 24)
SashankJ.Reddi, SatyenKale, andSanjivKumar. OntheconvergenceofAdamandbeyond. In International
Conference on Learning Representations (ICLR) , pp. 1–23, 2018. (Cited on 22, 23)
Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic Backpropagation and Approxi-
mate Inference. In International Conference on Machine Learning (ICML) , Beijing, China, 2014. (Cited
on 1, 4, 5, 7, 20, 23)
Christian P. Robert and George Casella. Monte Carlo Statistical Methods . Springer, 2004. ISBN 0-387-
21239-6. (Cited on 5)
Geoffrey Roeder, Yuhuai Wu, and David K. Duvenaud. Sticking the Landing: Simple, Lower-Variance
Gradient Estimators for Variational Inference. In Advances in Neural Information Processing Systems
(NeurIPS) , volume 30, 2017. (Cited on 4, 7, 22, 23)
Shaun Seaman, John Galati, Dan Jackson, and John Carlin. What Is Meant by “Missing at Random”?
Statistical Science , 28(2):257–268, May 2013. ISSN 0883-4237, 2168-8745. doi: 10.1214/13-STS415. (Cited
on 2, 3)
Yuge Shi, N. Siddharth, Brooks Paige, and Philip Torr. Variational Mixture-of-Experts Autoencoders for
Multi-Modal Deep Generative Models. In Advances in Neural Information Processing Systems (NeurIPS) ,
2019. (Cited on 5, 7, 11)
Vaidotas Simkus and Michael U. Gutmann. Conditional Sampling of Variational Autoencoders via Iterated
Approximate Ancestral Sampling. Transactions on Machine Learning Research , 2023. ISSN 2835-8856.
(Cited on 4, 5, 19, 20, 23)
Vaidotas Simkus, Benjamin Rhodes, and Michael U. Gutmann. Variational Gibbs Inference for Statistical
Model Estimation from Incomplete Data. Journal of Machine Learning Research , 24(196):1–72, 2023.
ISSN 1533-7928. (Cited on 7, 21)
JaschaSohl-Dickstein, EricA.Weiss, NiruMaheswaranathan, andSuryaGanguli. DeepUnsupervisedLearn-
ing using Nonequilibrium Thermodynamics. In International Conference on Machine Learning (ICML) ,
November 2015. doi: 10.48550/arXiv.1503.03585. (Cited on 1)
15Published in Transactions on Machine Learning Research (06/2024)
Timur Sudak and Sebastian Tschiatschek. Posterior Consistency for Missing Data in Variational Autoen-
coders. In European Conference on Machine Learning and Principles and Practice of Knowledge Discovery
in Databases (ECML-PKDD) , October 2023. doi: 10.48550/arXiv.2310.16648. (Cited on 3, 7)
Tijmen Tieleman. Training restricted Boltzmann machines using approximations to the likelihood gradient.
InInternational Conference on Machine Learning (ICML) , pp. 1064–1071, 2008. ISBN 9781605582054.
doi: 10.1145/1390156.1390290. (Cited on 21)
George Tucker, Dieterich Lawson, Shixiang Gu, and Chris J. Maddison. Doubly Reparameterized Gradient
Estimators for Monte Carlo Objectives. In International Conference on Learning Representations (ICLR) ,
November 2018. (Cited on 22)
Stef van Buuren. Flexible Imputation of Missing Data . CRC Press LLC, 2 edition, 2018. ISBN 978-1-138-
58831-8. (Cited on 2)
Ramakrishna Vedantam, Ian Fischer, Jonathan Huang, and Kevin P. Murphy. Generative Models of Visually
Grounded Imagination. International Conference on Learning Representations (ICLR) , May 2017. (Cited
on 7)
Neng Wan, Dapeng Li, and Naira Hovakimyan. F-Divergence Variational Inference. In Advances in Neural
Information Processing Systems , volume 33, pp. 17370–17379. Curran Associates, Inc., 2020. (Cited on
11)
Greg C. G. Wei and Martin A. Tanner. A Monte Carlo Implementation of the EM Algorithm and the Poor
Man’s Data Augmentation Algorithms. Journal of the American Statistical Association , 85(411):699–704,
September 1990. doi: 10.1080/01621459.1990.10474930. (Cited on 5, 13)
Christopher K. I. Williams, Charlie Nash, and Alfredo Nazábal. Autoencoders and Probabilistic Inference
with Missing Data: An Exact Solution for The Factor Analysis Case. arXiv preprint , 1801.03851, January
2018. (Cited on 7)
Mike Wu and Noah D. Goodman. Multimodal Generative Models for Scalable Weakly-Supervised Learning.
InNeurIPS 2018 , February 2018. (Cited on 7)
Laurent Younes. On the convergence of markovian stochastic algorithms with rapidly decreasing ergodicity
rates.Stochastics and Stochastic Reports , 65(3-4):177–228, February 1999. ISSN 1045-1129. doi: 10.1080/
17442509908834179. (Cited on 21)
Mingtian Zhang, Peter Hayes, and David Barber. Generalization Gap in Amortized Inference. In Workshop
on Bayesian Deep Learning at Neural Information Processing Systems (NeurIPS) , pp. 6, 2021. (Cited on
7)
16Published in Transactions on Machine Learning Research (06/2024)
A Posterior complexity due to missing information
The complexity increase of the model posterior due to missing data, shown in fig. 1, explains why flexible
variational distributions (Burda et al., 2015; Cremer et al., 2017) have been preferred when fitting VAEs
from incomplete data (Mattei & Frellsen, 2019; Ipsen et al., 2020; Ma & Zhang, 2021). We here define the
increase of the posterior complexity via the expected Kullback–Leibler (KL) divergence as follows
Ep∗(x)[DKL(pθ(z|x)||pθ(z|xobs))] =Ep∗(x)Epθ(z|x)/bracketleftbigg
logpθ(z|xobs,xmis)
pθ(z|xobs)/bracketrightbigg
=I(z,xmis|xobs).9
As shown above the expected KL divergence equals the (conditional) mutual information (MI) between the
latentszand the missing variables xmis.
The mutual information interpretation allows us to reason when a more flexible variational family may
be necessary to accurately estimate VAEs from incomplete data. Specifically, when the MI is small then
the two posterior distributions, pθ(z|x)andpθ(z|xobs)are similar, in which case a simple variational
distribution may work sufficiently well. This situation might appear when the observed xobsand unobserved
xmisvariables are highly related and xmisprovides little additional information about zover justxobs,
for example, when random pixels of an image are masked it is “easy” to infer the complete image due to
strong relationship between neighbouring pixels. On the other hand, when the MI is high then xmisprovides
significant additional information about zover justxobs, in which case a more flexible variational family may
be needed, for example, when the pixels of an image are masked in blocks such that it introduces significant
uncertainty about what is missing.
B DeMissVAE: Encoder objective derivation
The standard (complete-data) ELBO in eq. (2) gives the inequality
logpθ(xobs,xmis)≥Eqϕ(z|xobs,xmis)/bracketleftigg
logpθ(xobs,xmis,z)
qϕ(z|xobs,xmis)/bracketrightigg
,
which, together with the identity
logpθ(xobs) = log/integraldisplay
exp{logpθ(xobs,xmis)}dxmis,
yields
logpθ(xobs)≥log/integraldisplay
exp/braceleftigg
Eqϕ(z|xobs,xmis)/bracketleftigg
logpθ(xobs,xmis,z)
qϕ(z|xobs,xmis)/bracketrightigg/bracerightigg
dxmis.
Astheintegralonther.h.s.isintractable, welower-bounditusingtheimputationdistribution ft(xmis|xobs)
and Jensen’s inequality
logpθ(xobs)≥log/integraldisplayft(xmis|xobs)
ft(xmis|xobs)exp/braceleftigg
Eqϕ(z|xobs,xmis)/bracketleftigg
logpθ(xobs,xmis,z)
qϕ(z|xobs,xmis)/bracketrightigg/bracerightigg
dxmis
= log Eft(xmis|xobs)/bracketleftigg
exp/parenleftbig
−logft(xmis|xobs)/parenrightbig
exp/braceleftigg
Eqϕ(z|xobs,xmis)/bracketleftigg
logpθ(xobs,xmis,z)
qϕ(z|xobs,xmis)/bracketrightigg/bracerightigg/bracketrightigg
= log Eft(xmis|xobs)/bracketleftigg
exp/braceleftigg
Eqϕ(z|xobs,xmis)/bracketleftigg
logpθ(xobs,xmis,z)
qϕ(z|xobs,xmis)ft(xmis|xobs)/bracketrightigg/bracerightigg/bracketrightigg
9Where notation of mis suppressed due to MAR assumption. In case of missing-not-at-random (MNAR) assumption there
would be an additional dependency on m.
17Published in Transactions on Machine Learning Research (06/2024)
≥Eft(xmis|xobs)qϕ(z|xobs,xmis)/bracketleftigg
logpθ(xobs,xmis,z)
qϕ(z|xobs,xmis)ft(xmis|xobs)/bracketrightigg
=Eft(xmis|xobs)qϕ(z|xobs,xmis)/bracketleftigg
logpθ(xobs,xmis,z)
qϕ(z|xobs,xmis)/bracketrightigg
/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
def=Lϕ
LMVB(xobs;ϕ,θ,ft)+H/bracketleftbig
ft(xmis|xobs)/bracketrightbig
/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
Const. w.r.t. ϕ.
C DeMissVAE: Motivating the separation of objectives
The two DeMissVAE objectives Lθ
CVIandLϕ
LMVBin eqs. (10) and (12) correspond to valid lower-bounds on
logpθ(xobs)irrespective of ft(xmis|xobs). Moreover, both of them are tight at ft(xmis|xobs) =pθ(xmis|
xobs)andqϕ(z|xobs,xmis) =pθ(z|xobs,xmis).So, a natural question is why do we prefer Lθ
CVIto learn
pθandLϕ
LMVBto learnqϕ?
Why useLθ
CVIin eq. (10) over Lϕ
LMVBin eq. (12) to learn pθ(x)?Maximisation of the objective
Lϕ
LMVBin iteration tw.r.t.θwould have to compromise between maximising the log-likelihood logpθ(xobs)
and keeping the other two KL divergence terms in eq. (13) low. Specifically, the compromise between max-
imising logpθ(xobs)and keeping DKL(ft(xmis|xobs)||pθ(xmis|xobs))low is equivalent to the compromise
in the EM algorithm, which is known to affect the convergence of the model (Meng, 1994). Moreover,
ifft(xmis|xobs)̸=pθ(xmis|xobs)then minimising the DKL(ft(xmis|xobs)||pθ(xmis|xobs))will fit the
modelpθ(x)tothebiasedsamplesfrom ft(xmis|xobs). Ontheotherhand,in Lθ
CVIthemissingvariables xmis
are marginalised from the model, therefore it avoids the compromise with DKL(ft(xmis|xobs)||pθ(xmis|
xobs))and the potential bias of the imputation distribution ft(xmis|xobs)affects the model onlyvia the
latentsz∼qϕ,ft(z|xobs), increasing the robustness to sub-optimal imputations.
Why useLϕ
LMVBin eq. (12) over Lθ
CVIin eq. (10) to learn qϕ(z|x)?In the case ofLθ
CVI, if
ft(xmis|xobs) =pθ(xmis|xobs)then the bound is tightened when qϕ(z|xobs,xmis) =pθ(z|xobs,xmis)for
allxmis, which is the same optimal qϕif we usedLϕ
LMVB. But, there is also at least one more possible optimal
solutionqϕ(z|xobs,xmis) =pθ(z|xobs), which ignores the imputations and corresponds to the optimal
solutionofthestandardapproachinsection2,andthusitmeansthattheoptimumis(partially)unidentifiable
and can make optimisation of qϕusingLθ
CVIdifficult. Moreover, if ft(xmis|xobs)̸=pθ(xmis|xobs)then in
order to minimise DKL(qϕ,ft(z|xobs)||pθ(z|xobs))w.r.t.ϕthe variational distribution qϕ(z|xobs,xmis)
would have to compensate for the inaccuracies of ft(xmis|xobs)by adjusting the probability mass over
the latentsz, such that qϕ(z|xobs,xmis)is correct on average, i.e. qϕ,ft(z|xobs) =Eft(xmis|xobs)[qϕ(z|
xobs,xmis)]≈pθ(z|xobs). These two issues make optimising ϕviaLθ
CVIsuch thatqϕ(z|xobs,xmis)≈
pθ(z|xobs,xmis)difficult. On the other hand, in Lϕ
LMVBthe optimal qϕis always at qϕ(z|xobs,xmis) =
pθ(z|xobs,xmis), irrespective of the imputation distribution ft(xmis|xobs), hence theLϕ
LMVBobjective
in eq. (12) is well-defined and more robust to inaccuracies of ft(xmis|xobs)for the optimisation of qϕ(z|
xobs,xmis).
In fig. 5 we verify the efficacy of DeMissVAE via a control study on a small VAE model pθ(x)with 2D
latent space fitted on incomplete samples from a ground truth mixture-of-Gaussians (MoG) distribution
p∗(x). We evaluate fitting the VAE using only Lθ
CVIin eq. (10) (CVI-VAE, blue), only Lϕ
LMVBin eq. (12)
(MVB-VAE, yellow), and using the proposed two-objective approach (DeMissVAE, green). In the left-most
figure we evaluate the three methods where we represent the imputation distribution ft(xmis|xobs) =
pθ(xmis|xobs)using rejection sampling, which corresponds to the optimal imputation distribution w.r.t.
DKL(ft(xmis|xobs)||pθ(xmis|xobs)) = 0. We see that the proposed approach (green) dominates over the
other two control methods (blue and yellow), and importantly that marginalisation of the missing variables
in DeMissVAE (green) improves the model accuracy compared to an EM-type handling of the missing
variables (yellow). Furthermore, in the remaining two figures we investigate the sensitivity of the methods
to the accuracy of imputations in ft(xmis|xobs). In Oracle 1 we start with the ground-truth conditional
18Published in Transactions on Machine Learning Research (06/2024)
−4.5−4.0−3.5−3.0Log-likelihood
pθ(xmis|xobs)
0.0 0.2 0.4 0.6 0.8
JSD(ft(xmis|xobs)||p∗(xmis|xobs))−8−6−4
Oracle 1
0.0 0.1 0.2 0.3 0.4 0.5 0.6
JSD(ft(xmis|xobs)||p∗(xmis|xobs))−5−4−3
Oracle 2CVI-VAE MVB-VAE DeMissVAE
Figure 5: A control study on a VAE model with 2D latent space (see additional details in appendix E.1),
examining the sensitivity of the proposed method (DeMissVAE, green) and two control methods (blue and
yellow) to the accuracy of the imputation distribution ft(xmis|xobs).Left:ft(xmis|xobs) =pθ(xmis|xobs)
represented using rejection sampling. Center: an oracle imputation function that gets progressively “wider”
from left-to-right of the figure. Right: an oracle imputation distribution that towards the right of the figure
more significantly oversamples low-probability posterior modes. The log-likelihood is computed on a held-
out test data set by numerically integrating the 2D latent space of the VAE. The horizontal axis on the
two right-most figures shows the Jensen–Shannon divergence between the imputation distribution and the
ground-truth conditional p∗(xmis|xobs).
p∗(xmis|xobs)and, along the x-axis of the figure, investigate how the methods perform when the imputation
distribution becomes “wider”: first interpolating from p∗(xmis|xobs)to an independent unconditional
distribution/producttext
d∈idx(m)p∗(xd)and then further towards and independent Gaussian distribution. And in
Oracle 2 we investigate what happens when the sampler “oversamples” posterior modes: we interpolate
the imputation distribution from p∗(xmis|xobs)to1
C/summationtextC
cp∗(xmis|xobs,c), wherecis the component of
the mixture distribution with a total of Ccomponents. As we see in the figure, the proposed DeMissVAE
approach (green) performs similar or better than the MVB-VAE (yellow) and CVI-VAE (blue) control
methods, with an exception when the ft(xmis|xobs)are extremely inaccurate (last two points on the middle
figure) which is expected since qϕ,ft(z|xobs)in eq. (9) can be arbitrarily far from pθ(z|xobs)when
qϕ(z|xobs,xmis) =pθ(z|xobs,xmis)butft(xmis|xobs)̸≈pθ(xmis|xobs).
Finally, in fig. 6 we investigate what happens if we used only Lθ
CVIin eq. (10) orLϕ
LMVBin eq. (12) to fit
the VAE model, in contrast to the two separate objectives for encoder and decoder in DeMissVAE. We use
the LAIR sampling method (Simkus & Gutmann, 2023) as detailed in appendix D to obtain approximate
samples from ft(xmis|xobs)(xmis|xobs)≈pθ(xmis|xobs). And, we observe that DeMissVAE achieves a
better fit of the model, in line with our motivation in this section.
D DeMissVAE: Implementing the training procedure
DeMissVAE requires optimising two objectives Lθ
CVIandLϕ
LMVBin eqs. (10) and (12) and drawing (approx-
imate) samples to represent ft(xmis|xobs)≈pθ(xmis|xobs). Our aim is to implement this efficiently to
minimise redundant computation.
The algorithm starts with a randomly-initialised target VAE model pθ(x,z), an amortised variational dis-
tributionqϕ(z|x), and an incomplete data set D={xi
obs}i. And then, to represent the imputation
distribution f0(xmis|xobs),Kimputations{xik
mis}K
k=1are generated for each xi
obs∈Dusing some simple
imputation function such as sampling the marginal empirical distributions of the missing variables. The
algorithm then iterates between the following two steps:
1.Imputation. Update the Kimputations{xik
mis}K
k=1representing samples from the imputation
distribution ft(xmis|xobs), such that ft(xmis|xobs)is “closer” to pθ(xmis|xobs). For this, we
19Published in Transactions on Machine Learning Research (06/2024)
K=5
K=15
K=25
K=5
K=15
K=25
K=5
K=15
K=25−4.0−3.8−3.6−3.4−3.2−3.0−2.8−2.6Log-likelihood
CVI-VAE MVB-VAE DeMissVAE
Figure 6: A control study on a VAE model with 2D latent space (see additional details in appendix E.1),
investigating the importance of the two-objective approach in DeMissVAE (green) and two control methods
(blue and yellow). In CVI-VAE (blue) we fit both the encoder and decoder using eq. (10), and in MVB-VAE
(yellow) we fit both the encoder and decoder using eq. (12). The log-likelihood is computed on a held-out
test data set by numerically integrating the 2D latent space of the VAE.
Algorithm 1 Shared computation of the DeMissVAE learning objectives
Input:parametersθandϕ, number of latent samples L, completed data-point (xi
obs,xik
mis)
1:ψik←Encoder (xi
obs,xik
mis;ϕ) ▷Compute parameters of the variational distribution
2:z1,...,zL∼q(z|xi
obs,xik
mis;ψik) ▷Sample latents z
3:ηl←Decoder (zl;θ)for∀l∈[1,L] ▷Compute parameters of the generative distribution
4:defLθ
CVI(z1,...,zL,η1,...,ηL): ▷Procedure for estimating eq. (10)
5:return1
L/summationtextL
l=1logp(xi
obs,zl;ηl)
6:defLϕ
LMVB(ψik,z1,...,zL,η1,...,ηL): ▷Procedure for estimating eq. (12)
7:return1
L/summationtextL
l=1logp(xi
obs,xik
mis,zl;ηl)−logq(zl|xi
obs,xik
mis;ψik)
returnLθ
CVI(z1,...,zL,η1,...,ηL),Lϕ
LMVB (ψik,z1,...,zL,η1,...,ηL)
use cheap approximate iterative sampling methods such as pseudo-Gibbs (Rezende et al., 2014,
Appendix F), Metropolis-within-Gibbs (MWG, Mattei & Frellsen, 2018a), or latent-adaptive impor-
tance resampling (LAIR, Simkus & Gutmann, 2023). Moreover, since the model and the variational
distributions are initialised randomly, we skip the imputation step during the first epoch over the
data.
2.Parameter update. Update the parameters using stochastic gradient ascent on Lθ
CVIandLϕ
LMVB
in eqs. (10) and (12) with the imputations from ft(xmis|xobs).
Efficient parameter update. While the two objectives for pθandqϕin eqs. (10) and (12) are different,
a major part of the computation can be shared, as shown in algorithm 1. As usual, the objectives are
approximatedusingMonteCarloaveragingandrequireonlyoneevaluationofthegenerativemodel, including
the encoder, decoder, and prior, for each completed data-point (xi
obs,xik
mis). Therefore, only backpropagation
needs to be performed separately and the overall per-iteration computational cost of optimising the two
objectivesisabout1.67timesthecostofafully-observedVAEoptimisation(insteadof2timesifimplemented
naïvely).10
Efficient imputation. To make the imputation step efficient, the imputation distribution ft(xmis|xobs)
is “persitent” between iterations, that is, the imputation distribution from the previous iteration
10The cost of backpropagation is about 2 times the cost of a forward pass (Burda et al., 2015).
20Published in Transactions on Machine Learning Research (06/2024)
ft−1(xmis|xobs)is used to initialise the iterative approximate VAE sampler at iteration t.11Moreover,
an iteration of a pseudo-Gibbs, MWG, or LAIR samplers uses the same quantities as the objectives Lθ
CVI
andLϕ
LMVBin eqs. (10) and (12), and hence the cost of one iteration of the sampler in the imputation step
can be shared with the cost of computation of the learning objectives. However, it is important to note that
the accuracy of imputations affects the accuracy of the estimated model, and hence better estimation can
be achieved by increasing the computational budget for imputation or using better imputation methods.
11Persistent samplers have been used in the past to increase efficiency of maximum-likelihood estimation methods (Younes,
1999; Tieleman, 2008; Simkus et al., 2023).
21Published in Transactions on Machine Learning Research (06/2024)
E Experiment details
In this appendix we provide additional details on the experiments.
E.1 Mixture-of-Gaussians data with a 2D latent VAE
We generated a random 5D mixture-of-Gaussians model with 15 components by sampling the mixture
covariance matrices from the inverse Wishart distribution W−1(ν=D,Ψ =I), means from the Gaussian
distributionN(µ=0,σ=3)and the component probabilities from Dirichlet distribution Dir(α=1)
(uniform). The model was then standardised to have a zero mean and a standard deviation of one. The
pairwise marginal densities of the generated distribution is visualised in fig. 7 showing a highly-complex
and multimodal distribution, and the generated parameters and data used in this paper are available in the
shared code repository. We simulated a 20K sample data set used to fit the VAEs.
dim=0dim=1
 dim=2
 dim=3
 dim=4
dim=1
 dim=2
 dim=3
Figure 7: The pairwise marginals of the ground-truth Mixture-of-Gaussians distribution.
We then fitted a VAE model with 2-dimensional latent space using diagonal Gaussian encoder and decoder
distributions, and a fixed standard Normal prior. For the decoder and encoder networks we used fully-
connected residual neural networks with 3 residual blocks, 200 hidden dimensions, and ReLU activations.
To optimise the model parameters we have used AMSGrad optimiser (Reddi et al., 2018) with a learning
rate of 10−3for a total of 500 epochs.
The hyperameters are listed in table 3, note that the total number of samples was the same for all methods
(i.e.5/15/25). Moreover, we have used “sticking-the-landing” (STL) gradients (Roeder et al., 2017) to reduce
gradient variance for all methods.12
12We have also evaluated the doubly-reparametrised gradients (DReG, Tucker et al., 2018) for IWAE methods but found STL
to perform similar or better.
22Published in Transactions on Machine Learning Research (06/2024)
Method Z K I Mixture sampling
MVAE 5/15/251 — —
MissVAE 5/15/25 5/15/25— Ancestral
MissSVAE 1 5/15/25— Stratified
MIWAE 1 1 5/15/25 —
MissIWAE 1 5/15/25 5/15/25 Ancestral
MissSIWAE 1 5/15/251 Stratified
DeMissVAE 1 5/15/25—LAIR (1 iteration, R= 0)
(Simkus & Gutmann, 2023)
Table 3: Method hyperparameters on MoG data.
E.2 UCI data sets
We fit the VAEs on four data sets from the UCI repository (Dua & Graff, 2017) with the preprocessing
of (Papamakarios et al., 2017). The VAE model uses diagonal Gaussian encoder and decoder distributions
regularised such that the standard deviation ≥10−5(Mattei & Frellsen, 2018a), and a fixed standard Normal
prior. The latent space is 16-dimensional, except for the MINIBOONE where 32 dimensions were used.
The encoder and decoder networks used fully-connected residual neural networks with 2 residual blocks
(except for on the MINIBOONE dataset where 5 blocks were used in the encoder) with 256 hidden dimen-
sionality, and ReLU activations. A dropout of 0.5 was used on the MINIBOONE dataset. The parameters
wereoptimisedusingAMSGradoptimiser(Reddietal.,2018)withalearningrateof 10−3andcosinelearning
rate schedule for a total of 200K iterations (or 22K iterations on MINIBOONE). As before, STL gradients
(Roeder et al., 2017) were used to reduce the variance for all methods. DeMissVAE used the LAIR sampler
(Simkus & Gutmann, 2023) with K= 5R= 1and 10 iterations. Moreover we have used gradient norm
clipping to stabilise DeMissVAE with the maximum norm set to 1(except for POWER dataset where we
set it to 0.5).
E.3 MNIST and Omniglot data sets
WefitaVAEonstaticallybinarisedMNISTandOmniglotdatasets(Lakeetal.,2015)downsampledto28x28
pixels. The VAE uses diagonal Gaussian decoder distributions regularised such that the standard deviation
≥10−5(Mattei & Frellsen, 2018a), a fixed standard Normal prior, and a Bernoulli decoder distribution.
The latent space is 50-dimensional.
For both MNIST and Omniglot we have used convolutional ResNet neural networks for the encoder and
decoder with 4 residual blocks, ReLU activations, and dropout probability of 0.3. For MNIST, the encoder
the residual block hidden dimensionalities were 32, 64, 128, 256, and for the decoder they were 128,64,32,32.
For Omniglot, the encoder the residual block hidden dimensionalities were 64, 128, 256, 512, and for the
decoder they were 256,128,64,64. We used AMSGrad optimiser (Reddi et al., 2018) with 10−4learning rate,
cosine learning rate schedule, and STL gradients (Roeder et al., 2017) for 500 epochs for MNIST and 200
epochs for Omniglot.
For MVAE, we use 5 latent samples and for MIWAE we use 5 importance samples. For MissVAE we use
K= 5mixture components and sample 5 latent samples. For MissSVAE we use K= 5mixture components
andsample1samplefromeachcomponent, foratotalof5samples. ForMissIWAEweuse K= 5components
and sample 5 importance samples. for MissSIWAE we use K= 5components and sample 1 sample from each
component. For DeMissVAE we use K= 5imputations and update them using a single step of pseudo-Gibbs
(Rezende et al., 2014).
23Published in Transactions on Machine Learning Research (06/2024)
F Additional figures
In this appendix we provide additional figures for the experiments in this paper.
F.1 Mixture-of-Gaussians data with a 2D latent VAE
In this section we show additional analysis on the mixture-of-Gaussians data, supplementing the results in
section 6.1.
F.1.1 Analysis of gradient variance with ancestral and stratified sampling
In section 6.1 we observed that the model estimation performance can depend on whether ancestral sampling
(with implicit reparametrisation) or stratified sampling is used to approximate the expectations in eqs. (4),
(5), (7) and (8), corresponding to MissVAE/MissIWAE and MissSVAE/MissSIWAE, respectively.
We analyse the signal-to-noise ratio (SNR) of the gradients w.r.t. ϕandθfor the two approaches, which is
defined as follows (Rainforth et al., 2019)
SNR(ϕ) =/vextendsingle/vextendsingle/vextendsingle/vextendsingleE[∆(ϕ)]
σ[∆(ϕ)]/vextendsingle/vextendsingle/vextendsingle/vextendsingle,and SNR (θ) =/vextendsingle/vextendsingle/vextendsingle/vextendsingleE[∆(θ)]
σ[∆(θ)]/vextendsingle/vextendsingle/vextendsingle/vextendsingle,
where ∆(·)denotes the gradient estimate, and σ[·]is the standard deviation of a random variable. We
estimate the SNR by computing the expectation and standard deviation over the entire training epoch.
The SNR for ϕandθis plotted in fig. 8. We observe that the stratified approaches (MissSVAE and
MissSIWAE) generally have higher SNR. This is possibly the reason why MissSVAE and MissSIWAE have
achieved better model accuracy than the ancestral approaches (MissVAE and MissIWAE) in section 6.1.
0 100 200 300 400 500
Training epoch20004000600080001000012000SNRφ
0 100 200 300 400 500
Training epoch020004000600080001000012000θMissVAE MissSVAE MissIWAE MissSIWAE
Figure 8: Signal-to-noise ratio (SNR, higher is better) of the gradients w.r.t. encoder parameters ϕ(left)
and decoder parameters θ(right). For all methods we used a budget of 5 samples from the variational
distribution (see appendix E.1 for more details). We show the median SNR over 5 independent runs and a
90% confidence interval.
F.1.2 Analysis of the model posteriors
In figs. 9 to 11 we visualise the model posteriors with complete and incomplete data, pθ(z|x)andpθ(z|
xobs), respectively, and the variational distribution qϕ(z|·)that was used to fit the model via the variational
ELBO.Foreachmethodwehaveusedabudgetof25samplesfromthevariationaldistributionduringtraining
(additional details are in appendix E.1). Each figure shows the posteriors for 5 training data-points using
distinct colours.
Figure 9 shows MVAE, MissVAE, and MissSVAE model posteriors pθ(z|x)andpθ(z|xobs), as well as
the variational distribution qϕ(z|xobs), which approximates the incomplete-data posterior. As motivated
24Published in Transactions on Machine Learning Research (06/2024)
in section 3 we observe that the Gaussian posterior in MVAE (first row) is not sufficiently flexible to ap-
proximate the complex incomplete-data posteriors pθ(z|xobs). On the other hand, the mixture-variational
approaches, MissVAE (second row) and MissSVAE (third row), are able to well-approximate the incomplete-
data posteriors.
Figure 10 shows MIWAE, MissIWAE, and MissSIWAE model posteriors pθ(z|x)andpθ(z|xobs), as well
as the variational proposal qϕ(z|xobs)and the importance-weighted semi-implicit distribution qIW
ϕ,θ,I=25(z|
xobs)that arises from sampling the variational proposal qϕ(z|xobs)and re-sampling using importance
weightsw(z) =pθ(xobs,z)/qϕ(z|xobs)(Cremer et al., 2017). Similar to the MVAE case above, the
variational proposal qϕ(z|xobs)in MIWAE (first row) is quite far from the model posterior pθ(z|xobs), but
the importance-weighted bound in eq. (7) is able to re-weigh the samples to sufficiently-well match the model
posterior, as shown in the fourth column. However, as efficiency of importance sampling depends on the
discrepancy between the proposal and the target distributions, we can expect that more flexible variational
distributions may improve the performance of the importance-weighted ELBO methods. Importantly, we
show that the variational-mixture approaches, MissIWAE (second row) and MissSIWAE (third row), are
able to adapt the variational proposals to the incomplete-data posteriors well, and as a result achieve better
efficiency than MIWAE.
Figure 11 shows DeMissVAE model posteriors pθ(z|x)andpθ(z|xobs), the variational distribution
qϕ(z|x), which approximates the complete -data posterior, and the imputation-mixture qϕ,ft(z|xobs)
approximatedusingthe25imputationsin ft(xmis|xobs)attheendoftraining. Weobservesimilarbehaviour
tofig.1,wherethecompletedataposteriors pθ(z|x)areclosetoGaussianbuttheincomplete-dataposteriors
pθ(z|xobs)are irregular. As we show in section 6.1, DeMissVAE is capable of fitting the model well by
learning the completed-data variational distribution qϕ(z|x)(third column) and using the imputation-
mixture in eq. (9) to approximate the incomplete data posterior pθ(z|xobs). Moreover, we observe that the
imputation-mixture qϕ,ft(z|xobs)(fourth column) captures only one of the modes of the model posterior
pθ(z|xobs). This is a result of using a small imputation sampling budget, that is, using only a single
iteration of LAIR to update the imputations (see more details in appendix D), and hence better accuracy
can be achieved by trading-off some computational cost to obtain better imputations that would ensure a
better representation of the imputation distribution. Nonetheless, as observed in fig. 2, DeMissVAE achieves
good model accuracy despite potentially sub-optimal imputations, further signifying the importance of the
two learning objectives for the encoder and decoder distributions in section 4.2 and appendix C.
Interestingly, by comparing the complete-data posteriors pθ(z|x)(first column) in figs. 9 to 11, we ob-
serve that they are slightly more irregular than in the complete case in fig. 1, except for DeMissVAE whose
posteriors are nearly Gaussian. The irregularity is stronger for the importance-weighted ELBO-based meth-
ods in fig. 10. This is in line with the observation by Burda et al. (2015, Appendix C) and Cremer et al.
(2018, Section 5.4) that VAEs trained with more flexible variational distributions tend to learn a more
complex model posterior. This means that using the importance-weighted bounds, and to a lesser extent
the finite variational-mixture approaches from section 4.1, to fit VAEs on incomplete data may result in
worse-structured latent spaces, compared to models fitted on complete data. On the other hand, we observe
that DeMissVAE learns a better-structured latent space, with the posterior pθ(z|x)close to a Gaussian,
that is comparable to the complete case. This suggests that the decomposed approach in DeMissVAE may
be important in cases where the latent space needs to be regular, at the additional cost of obtaining missing
data imputations (see appendix D).
25Published in Transactions on Machine Learning Research (06/2024)
MVAEp(z|x) p(z|xobs) q(z|xobs)MissVAE MissSVAE
Figure 9: Posterior distributions of MVAE, MissVAE, and MissSVAE. First column: the model posterior
pθ(z|x)under complete data x. Second column: the model posterior pθ(z|xobs)under incomplete data
xobs. Third column: variational approximation qϕ(z|xobs)of the incomplete posterior pθ(z|xobs)obtained
at the end of training.
26Published in Transactions on Machine Learning Research (06/2024)
−3−2−1012MIWAEp(z|x) p(z|xobs) q(z|xobs) qIW
I=25(z|xobs)
−3−2−1012MissIWAE
−3−2−1 0 1 2−3−2−1012MissSIWAE
−3−2−1 0 1 2−3−2−1 0 1 2−3−2−1 0 1 2
Figure 10: Posterior distributions of MIWAE, MissIWAE, and MissSIWAE. First column: the model pos-
teriorpθ(z|x)under complete data x. Second column: the model posterior pθ(z|xobs)under incomplete
dataxobs. Third column: variational proposal qϕ(z|xobs)for an incomplete data-point xobsobtained at
the end of training. Fourth column: importance-weighted variational distribution qϕIW
I=25(z|xobs)for an
incomplete data-point xobsobtained after re-weighting samples from qϕ(z|xobs)(Cremer et al., 2017).
DeMissVAEp(z|x) p(z|xobs) q(z|x) qft(z|xobs)
Figure 11: Posterior distributions of DeMissVAE. First: the model posterior pθ(z|x)under complete data
x. Second: the model posterior pθ(z|xobs)under incomplete data xobs. Third: variational approximation
qϕ(z|x)of the complete-data posterior pθ(z|x)obtained at the end of training. Fourth: the variational
imputation-mixture distribution in eq. (9) using the imputation distribution ft(xmis|xobs)obtained at the
end of training, approximated using a Monte Carlo average with 25 imputations.
27Published in Transactions on Machine Learning Research (06/2024)
F.2 UCI data sets
In fig. 12 we plot the Fréchet inception distance (FID, Heusel et al., 2017) versus training iteration on the
UCI datasets. The results closely mimic the log-likelihood results in section 6.2. Importantly, we observe
that using mixture variational distributions becomes more important as the missingness fraction increases,
causing the posterior distributions to be more complex.
0 50000 100000 150000 20000010−410−310−210−1100101102103GAS20%
0 50000 100000 150000 20000010−510−410−310−210−110010110210350%
0 50000 100000 150000 20000010−310−210−110010110210380%
0 50000 100000 150000 20000010−710−610−510−410−310−210−1100101102103104105106107108POWER
0 50000 100000 150000 20000010−710−610−510−410−310−210−1100101102103104105106107108
0 50000 100000 150000 20000010−510−410−310−210−1100101102103104105106107108
0 50000 100000 150000 20000010−510−410−310−210−1100101HEPMASS
0 50000 100000 150000 20000010−410−310−210−1100101
0 50000 100000 150000 20000010−210−1100101
0 5000 10000 15000 20000
Iteration10−310−210−1100101102MINIBOONE
0 5000 10000 15000 20000
Iteration10−210−1100101102
0 5000 10000 15000 20000
Iteration10−210−1100101102MissingnessMVAE
MissVAE
MissSVAEMIWAE
MissIWAE
MissSIWAEDeMissVAE
Figure 12: FID (lower is better) between the model and the complete test data versus training iterations. The
FID is computed using features of the last encoder layer of an independent VAE model trained on complete
data. Lines show the median of 5 independent runs and the intervals show 90% confidence.
28Published in Transactions on Machine Learning Research (06/2024)
F.3 MNIST and Omniglot data sets, latent dimensionality
Rather than handling the posterior complexity due to missingness with variational mixtures, an alternative
approach may be to increase the capacity of the model by making the latent space dimensionality larger. In
fig. 13 we plot the estimated test log-likelihood against latent variable dimensionality.
On MNIST data, the effect from increasing the latent space is small, with the exception of DeMissVAE. The
reason why DeMissVAE improves quite significantly with latent space dimensionality may be more due to
easier sampling than a larger capacity of the model. This again highlights that DeMissVAE, when used with
efficient sampling methods, can be an efficient approach to handle data missingness.
On Omniglot data, we observe a small improvement for MIWAE, MVAE, and the stratified MissSIWAE,
while the other methods either remained at about the same accuracy or declined. However, there is no
significant change from the results in section 6.3.
Hence, while increasing latent dimensionality generally increases the capacity of the model, enabling the
modelling of more complex distributions, overall we observe that it provides minimal effect for dealing with
data missingness.
25 50 75 100 125 150 175 200
Latent dimensionality−100−98−96−94−92−90−88Log-likelihoodMNIST
25 50 75 100 125 150 175 200
Latent dimensionality−135−130−125−120−115OmniglotMVAE
MissVAE
MissSVAEMIWAE
MissIWAE
MissSIWAEDeMissVAE
Figure 13: Estimate of the test log-likelihood against latent dimensionality on MNIST and Omniglot data
sets.The log-likelihood was estimated on complete test data using the IWELBO with I= 1000. The curves
show median performance over 5 independent runs of the methods and the intervals show the 90% centered
interval.
29