Under review as submission to TMLR
Thompson Sampling for Non-Stationary Bandit Problems
Anonymous authors
Paper under double-blind review
Abstract
Non-stationary multi-armed bandit (MAB) problems have recently attracted extensive
attention. We focus on the abruptly changing scenario where reward distributions remain
constant for a certain period and change at unknown time steps. Although Thompson
Sampling (TS) has shown empirical success in non-stationary settings, there is currently no
regretboundanalysisforTSwithGaussianpriors. Toaddressthis, weproposetwoalgorithms,
discounted TS and sliding-window TS, designed for sub-Gaussian reward distributions. For
these algorithms, we establish an upper bound for the expected regret by bounding the
expected number of times a suboptimal arm is played. We show that the regret order of both
algorithms is ˜O(√TBT), whereTis the time horizon, BTis the number of breakpoints. This
upper bound matches the lower bound for abruptly changing problems up to a logarithmic
factor. Empirical comparisons with other non-stationary bandit algorithms highlight the
competitive performance of our proposed methods.
1 Introduction
MAB is a classic sequential decision problem. At each time step, the learner selects an arm from a finite
set of arms (also known as actions) based on its past observations, and she only observes the reward of the
chosen action. The learner’s goal is to maximize its expected cumulative reward or minimize the regret
incurred during the learning process. The regret is defined as the difference between the expected reward of
the optimal arm and the expected reward achieved by the MAB algorithm.
MAB has found practical use in various scenarios, with one of the earliest applications being the diagnosis
and treatment experiments proposed by Robbins (1952). In this experiment, each patient’s treatment plan
corresponds to an arm in the MAB problem, and the goal is to minimize the patient’s health loss by making
optimal treatment decisions. Recently, MAB has gained wide-ranging applicability. For example, MAB
algorithms have been used in online recommendation systems to improve user experiences and increase
engagement (Li et al., 2011; Bouneffouf et al., 2012; Li et al., 2016). Similarly, MAB has been employed
in online advertising campaigns to optimize the allocation of resources and maximize the effectiveness of
ad placements (Schwartz et al., 2017). While the standard MAB model assumes fixed reward distributions,
real-world scenarios often involve changing distributions over time. For instance, in online recommendation
systems, the collected data gradually becomes outdated, and user preferences are likely to evolve (Wu et al.,
2018). This dynamic nature necessitates the development of algorithms that can adapt to these changes,
leading to the exploration of non-stationary MAB problems.
In recent years, there has been much research on non-stationary multi-armed bandit problems. These methods
can be roughly divided into two categories: they either detect changes in the reward distribution using
change-point detection algorithms (Liu et al., 2018; Cao et al., 2019; Auer et al., 2019; Chen et al., 2019;
Besson et al., 2022), or they passively reduce the effect of past observations (Garivier & Moulines, 2011;
Raj & Kalyani, 2017; Trovo et al., 2020; Baudry et al., 2021). The former method needs to make some
assumptions about the change of arms distribution to ensure the effectiveness of the change-point detection
algorithm. For instance, (Liu et al., 2018; Cao et al., 2019) require a lower bound on the amplitude of change
of each arm’s expected rewards. The latter requires fewer assumptions about the characteristics of the change.
They often use a sliding window or discount factor to forget past information to adapt to the change of arms
distribution.
1Under review as submission to TMLR
These methods all provide the theoretical guarantees for regret upper bounds. However, the known Thompson
sampling, have received little theoretical analysis of regret in non-stationary MAB problems, despite the fact
TS algorithms often have superior or comparable performance to frequentist algorithms in most non-stationary
scenarios. Raj & Kalyani (2017) have studied the discounted Thompson sampling with Beta priors. However,
they only derive the probability of picking a suboptimal arm for the simple case of a two-armed bandit. To
the best of our knowledge, only sliding-window Thompson sampling with Beta priors (Trovo et al., 2020)
provides the regret upper bounds. However, their proof is incorrect with a wrong application of a well-known
result (Lemma A.3). We analyze their mistakes in detail and provide a counterexample in Appendix C.
There are two main challenges in analyzing Thompson sampling algorithm in non-stationary setting. The
first challenge is that the DS-TS algorithm cannot fully forget previous information and the second is
theunder-estimation of the optimal arm . In non-stationary setting, solving these problems is highly
challenging due to the changing reward distribution. We define a UCB-like function serving as the upper
confidence bound to tackle the first challenge, as detailed in Lemma 5.1 and Lemma 5.2. Along with the
defined function, we employ a new regret decomposition to bound the regret comes from the under-estimation
of the optimal arm, as presented in the proof of Lemma 5.3. We provide details about these challenges and
their solutions in the theoretical analysis section (Section 5).
Our contributions are as follows: we propose discounted TS (DS-TS) and sliding-window TS (SW-TS) with
Gaussian priors for abruptly changing settings. We adopt a unified method to analyze the regret upper
bound for both algorithms. The theoretical analysis results show that their regret upper bounds are of
order ˜O(√TBT), whereTis the number of time steps, BTis the number of breakpoints. This regret bound
matches the Ω(√
T)lower bound proven by Garivier & Moulines (2011) in an order sense. We also verify
the algorithms in various environmental settings with Gaussian and Bernoulli rewards, and both DS-TS and
SW-TS achieve competitive performance.
2 Related Works
Many works are based on the idea of forgetting past observations. Discounted UCB (DS-UCB) (Kocsis &
Szepesvári, 2006; Garivier & Moulines, 2011) uses a discounted factor to average the past rewards. In order to
achieve the purpose of forgetting information, the weight of the early reward is smaller. Garivier & Moulines
(2011) also propose the sliding-window UCB (SW-UCB) by only using a few recent rewards to compute the
UCB index. They calculate the regret upper bound for DS-UCB and SW-UCB as ˜O(√TBT). EXP3.S, as
proposed in (Auer et al., 2002), has been shown to achieve the regret upper bound by ˜O(√TBT). Under the
assumption that the total variation of the expected rewards over the time horizon is bounded by a budget
VT, Besbes et al. (2014) introduce REXP3 with regret ˜O(T2/3). Combes & Proutiere (2014) propose the
SW-OSUB algorithm, specifically for the case of smoothly changing with an upper bound of ˜O(σ1/4T), where
σis the Lipschitz constant of the evolve process. Raj & Kalyani (2017) propose the discounted Thompson
sampling for Bernoulli priors without providing the regret upper bound. They only calculate the probability
of picking a sub-optimal arm for the simple case of a two-armed bandit. Trovo et al. (2020) propose the
sliding-window Thompson sampling algorithm with regret ˜O(T1+α
2)for abruptly changing settings and ˜O(Tβ)
for smoothly changing settngs. Baudry et al. (2021) propose a novel algorithm named Sliding Window Last
Block Subsampling Duelling Algorithm (SW-LB-SDA) with regret ˜O(√TBT). They only assume that the
reward distributions belong to the same one-parameter exponential family for all arms during each stationary
phase.
There are also many works that exploit techniques from the field of change detection to deal with reward
distributions varying over time. Mellor & Shapiro (2013) combine a Bayesian change point mechanism
and Thompson sampling strategy to tackle the non-stationary problem. Their algorithm can detect global
switching and per-arm switching. Liu et al. (2018) propose a change-detection framework that combines UCB
and a change-detection algorithm named CUSUM. They obtain an upper bound for the average detection
delay and a lower bound for the average time between false alarms. Cao et al. (2019) propose M-UCB, which
is similar to CUSUM but uses another simpler change-detection algorithm. M-UCB and CUSUM are nearly
optimal, their regret bounds are ˜O(√TBT).
2Under review as submission to TMLR
Recently, there are also some works deriving regret bounds without knowing the number of changes. For
example, Auer et al. (2019) propose an algorithm called ADSWITCH with optimal regret bound ˜O(√BTT).
Suk & Kpotufe (2022) improve the work (Auer et al., 2019) so that the obtained regret bound is smaller than
˜O(√
ST), whereSonly counts the best arms switches.
3 Problem Formulation
Assume that the non-stationary MAB problem has KarmsA:={1,2,...,K}with finite time horizon T. At
each round t, the learner must select an arm it∈Aand obtain the corresponding reward Xt(it). The rewards
are generated from σ-subGaussian distributions. The expectation of Xt(i)is denoted as µt(i) =E[Xt(i)].
A policyπis a function that selects arm itto play at round t. Letµt(∗) :=maxi∈{1,...,K}µt(i)denote the
expected reward of the optimal arm i∗
tat roundt. Unlike the stationary MAB settings, where an arm is
optimal all of the time (i.e. ∀t∈{1,...,T},i∗
t=i∗), while in the non-stationary settings, the optimal arms
might change over time. The performance of a policy πis measured in terms of cumulative expected regret:
Rπ
T=E/bracketleftiggT/summationdisplay
t=1(µt(∗)−µt(it))/bracketrightigg
, (1)
where E[·]is the expectation with respect to randomness of π. Let ∆t(i) =µt(∗)−µt(i)and let
kT(i) =T/summationdisplay
t=11{it=i,i̸=i∗
t}
denote the number of plays of arm iwhen it is not the best arm until time T. When we analyze the upper
bound ofRπ
T, we can directly analyze E[kT(i)]to get the upper bound of each arm.
Abruptly Changing Setting The abruptly changing setting is introduced by Garivier & Moulines (2011)
for the first time. The number of breakpoints is denoted as BT=/summationtextT−1
t=11{∃i∈A:µt(i)̸=µt+1(i)}. Suppose
the set of breakpoints isB={b1,...,bBT}(we defineb1= 1). At each breakpoint, the reward distribution
changes for at least one arm. The rounds between two adjacent breakpoints are called stationary phase .
Abruptly changing bandits pose a more challenging problem as the learner needs to balance exploration
and exploitation within each stationary phase and during the changes between different phases. Trovo et al.
(2020) makes assumption about the number of breakpoints to facilitate more generalized analysis, while we
explicitly use BTto represent the number of breakpoints for analysis.
4 Algorithms
In this section, we propose the DS-TS and SW-TS with Gaussian priors for the non-stationary stochastic
MAB problems. Different from Agrawal & Goyal (2013), we assume that the reward distribution follows a
σ-subGaussian distribution rather than a bounded distribution. Assume that X1,...,Xnare independently
and identically distributed, following a σ-subGaussian distribution with mean µ. Assume further that the
prior distribution is a Gaussian distribution µ∼N (0,σ2
0). The posterior distribution is also Gaussian
distributionN(µ1,σ2
1)where
µ1=σ2
1(0
σ2
0+/summationtextn
i=1Xi
σ2),σ2
1=1
1
σ2
0+n
σ2.
Letσ0= +∞, we get the posterior distribution as N(1
n/summationtextn
i=1Xi,σ2
n).
4.1 DS-TS
DS-TS uses a discount factor γ(0<γ < 1) to dynamically adjust the estimate of each arm’s distribution. The
key to our algorithm is to decrease the sampling variance of the selected arm while increasing the sampling
variance of the unselected arms.
3Under review as submission to TMLR
Algorithm 1: DS-TS
Input:discounted factor γ,ˆµ1(i) = 0,˜µ1(i) = 0,Nt(γ,i) = 0
1fort= 1,...,T do
2fori= 1,..,K do
3 sampleθt(i)∼N(ˆµt(γ,i),4σ2
Nt(γ,i))
4end
5Pull armit= arg max iθt(i), observe reward Xt(it);
6fori= 1,...,K do
7 ˜µt+1(γ,i) =γ˜µt(γ,i) + 1{it=i}Xt(i)
8Nt+1(γ,i) =γNt(γ,i) + 1{it=i}
9 ˆµt+1(γ,i) =˜µt+1(γ,i)
Nt+1(γ,i)
10 end
11end
Specifically, let
Nt(γ,i) =t/summationdisplay
j=1γt−j1{ij=i}
denote the discounted number of plays of arm iuntil timet. We use
ˆµt(γ,i) =1
Nt(γ,i)t/summationdisplay
j=1γt−jXj(i) 1{ij=i}
called discounted empirical average to estimate the expected rewards of arm i. In non-stationary settings, we
use the discounted average and discounted number of plays instead of the true average and number of plays
respectively. Therefore, the posterior distribution is N(ˆµt(γ,i),σ2
Nt(γ,i)).
Algorithm 2: SW-TS
Input: sliding window τ,ˆµ1(i) = 0,˜µ1(i) = 0,Nt(τ,i) = 0
1fort= 1,...,T do
2fori= 1,..,K do
3 sampleθt(i)∼N(ˆµt(τ,i),4σ2
Nt(τ,i))
4end
5Pull armit= arg max iθt(i), observe reward Xt(it)
6fori= 1,...,K do
7Nt+1(τ,i) =Nt(τ,i) + 1{it=i}− 1{it−τ=i}
8 ˜µt+1(τ,i) = ˜µt(τ,i) + 1{it=i}Xt(i)− 1{it−τ=i}Xt−τ(i)
9 ˆµt+1(τ,i) =˜µt+1(τ,i)
Nt+1(τ,i)
10 end
11end
Algorithm 1 shows the pseudocode of DS-TS. Step 3 is the Thompson sampling. For each arm, we draw a
random sample θt(i)fromN(ˆµt(γ,i),4σ2
Nt(γ,i)). We use4σ2
Nt(γ,i)as the posterior variance instead ofσ2
Nt(γ,i), which
helps the subsequent analysis. Then we select arm itwith the maximum sample value and obtain the reward
Xt(it)(Step 5). To avoid the time complexity going to O(T2), we introduce ˜µt(γ,i) =/summationtextt
j=1γt−jXj(i) 1{ij=
i}to calculate ˆµt(γ,i)using an iterative method(Step 7-9).
If armiis selected at round t, the posterior distribution is updated as follows:
ˆµt+1(γ,i) =γˆµt(γ,i)Nt(γ,i) +Xt(i)
γNt(γ,i) + 1=˜µt+1(γ,i)
Nt+1(γ,i)
4Under review as submission to TMLR
If armiisn’t selected at round t, the posterior distribution is updated as
ˆµt+1(γ,i) =˜µt+1(γ,i)
Nt+1(γ,i)=γ˜µt(γ,i)
γNt(γ,i)= ˆµt(γ,i)
i.e. the expectation of posterior distribution remains unchanged.
4.2 SW-TS
SW-TS uses a sliding window τto adapt to changes in the reward distribution. Let
Nt(τ,i) =t/summationdisplay
j=t−τ+11{ij=i},ˆµt(τ,i) =1
Nt(τ,i)t/summationdisplay
j=t−τ+1Xj(i) 1{ij=i}.
Ift < τ, the range of summation is from 1tot. Similar to DS-TS, the posterior distribution is
N(ˆµt(τ,i),4σ2
Nt(τ,i)). Algorithm 2 shows the pseudocode of SW-TS. To avoid the time complexity going
toO(T2), we introduce ˜µt(τ,i) =/summationtextt
j=t−τ+1Xj(i) 1{ij=i}to update ˆµt(τ,i).
4.3 Results
In this section, we give the regret upper bounds of DS-TS and SW-TS. Then we discuss how to take the
values of the parameters so that these algorithms reach the optimal upper bound.
Recall that ∆t(i) =µt(∗)−µt(i). Let ∆T(i) =min{∆t(i) :t≤T,i̸=i∗
t}, be the minimum difference between
the expected reward of the best arm i∗
tand the expected reward of arm iin all timeTwhen the arm iis not
the best arm. Let ∆T
max=max{µt1(i)−µt2(i) :t1̸=t2,i∈[K]}denote the maximum expected variation of
arms.
Theorem 4.1 (DS-TS) .Letγ∈(0,1)satisfyingσ2
∆Tmax(1−γ)2log1
1−γ<1. For any suboptimal arm i,
E[kT(i)]≤BTD(γ) +C1(γ)L1(γ)γ−1
1−γT(1−γ),
where
D(γ) =log((σ
∆Tmax)2(1−γ)2log1
1−γ)
logγ,C1(γ) =e17+ 12 + 3 log1
1−γ,L1(γ) =1152 log(1
1−γ+e17)σ2
γ1/(1−γ)(∆T(i))2.
Corollary 4.2. Whenγis close to 1,γ−1
1−γis arounde. If the time horizon Tand number of breakpoints
BTare known in advance, the discounted factor can be chosen as γ= 1−1
σ/radicalig
BT
TlogT. IfBT≪T,
σ2
∆Tmax(1−γ)2log1
1−γ<σ/e
∆Tmax/radicaligg
BT
TlogT<1.
We have
E[kT(i)] =O(/radicalbig
TBT(logT)3
2).
Theorem 4.3 (SW-TS) .Letτ >0, for any suboptimal arm i,
E[kT(i)]≤BTτ+C2(τ)L2(τ)T
τ,
where
C2(τ) =e11+ 12 + 3 log τ,L2(τ) =1152 log(τ+e11)σ2
(∆T(i))2.
Corollary 4.4. If the time horizon Tand number of breakpoints BTare known in advance, the sliding
window can be chosen as τ=σ/radicalbig
T/BTlogT, then
E[kT(i)] =O(/radicalbig
TBTlogT).
5Under review as submission to TMLR
5 Proofs of Upper Bounds
Before giving the detailed proof, we discuss the main challenges in regret analysis of Thompson sampling in
non-stationary setting. These challenges are addressed by Lemmas 5.1 to 5.3.
5.1 Challenges in Regret Analysis
Existing analyses of regret bounds for Thompson sampling (Agrawal & Goyal, 2013; Jin et al., 2021; 2022)
decompose the regret into two parts. The first part of regret comes from the over-estimation of suboptimal
arm, which can be dealt with by the concentration properties of the sampling distribution and rewards
distribution. The second part is the under-estimation of the optimal arm, which mainly relies on bounding
the following equation.
T/summationdisplay
t=1E[1−pi,t
pi,t1{it=i∗
t,θt(∗)≤µt(∗)−ϵi}], (2)
wherepi,t=P(θt(∗)>µt(∗)−ϵi)is the probability that the best arm will not be under-estimated from the
mean reward by a margin ϵi.
The first challenge is specific to the DS-TS algorithm. Unlike SW-TS, which completely forgets previous
information after τrounds following a breakpoint, DS-TS cannot fully forget past information . This
makes it challenging to utilize the concentration properties of the reward distribution to bound regret comes
from the over-estimate of the suboptimal arm. And this will further affect the analysis of Equation (2).
The second challenge is the under-estimation of the optimal arm. In stationary settings, pi,tchanges
only when the optimal arm is selected, Equation (2) can be bounded by the method proposed by Agrawal
& Goyal (2013). However, the distribution of θt(∗)may vary over time in non-stationary settings. It is
challenging and nontrivial to obtain a tight bound of Equation (2).
To overcome the first challenges, we adjust the posterior variance to be4σ2
Nt(γ,i). This slightly larger variance
is specifically designed for the σ2-subGaussian distribution, which helps to bound E[1
pi,t]. Then, we define
Ut(γ,i), which serves a role similar to the upper confidence bound in the UCB algorithm. We solve this
problem through Lemma 5.1 and Lemma 5.2.
For the second challenge, we use the new defined Ut(γ,i)and employ a new regret decomposition for
Equation (2) based on whether the event {Nt(γ,∗)> L 1(γ)}occurs. Intuitively, if Nt(γ,∗)> L 1(γ),pi,t
is close to 1, which will lead to a sharp bound. If Nt(γ,∗)≤L1(γ), using Lemma A.3 we can also get the
upper bound of Equation (2). We derive the upper bound of E[1
pi,t]for non-stationary settings, with an
extra logarithmic term compared with the stationary settings. The proof of Lemma 5.3 in Appendix B.3
demonstrates these details.
5.2 Proofs of Theorem 4.1
For armi̸=i∗
t, we choose two threshold xt(i),yt(i)such thatxt(i) =µt(i) +∆t(i)
3,yt(i) =µt(∗)−∆t(i)
3. Then
µt(i)<xt(i)<yt(i)<µt(∗)andyt(i)−xt(i) =∆t(i)
3. The historyFtis defined as the plays and rewards of
the previous tplays. ˆµt(γ,i),itand the distribution of θt(i)are determined by the history Ft−1.
The abruptly changing setting is in fact piecewise-stationary. The rounds between two adjacent breakpoints
is stable stationary. Based on this observation, we define the pseudo-stationary phase as
T(γ) ={t≤T:∀s∈(t−D(γ),t],µs(·) =µt(·)}.
LetS(γ) ={t≤T:t /∈T(γ)}. Note that, on the right side of any breakpoint, there will be at most D(γ)
rounds belonging to S(γ). Therefore, the number of elements in the set S(γ)has an upper bound BTD(γ),
i.e.
|S(γ)|≤BTD(γ) (3)
Figure 1 showsT(γ)andS(γ)in two different situations.
6Under review as submission to TMLR
𝑏𝑖𝑏𝑖+𝐷(𝛾) 𝑏𝑖+1𝒮(𝛾) 𝒯(𝛾)
𝑏𝑖𝑏𝑖+𝐷(𝛾) 𝑏𝑖+2𝒮(𝛾) 𝒯(𝛾)
𝑏𝑖+1 𝑏𝑖+1+𝐷(𝛾)𝐷𝛾<𝑏𝑖+1−𝑏𝑖
𝐷𝛾>𝑏𝑖+1−𝑏𝑖
Figure 1: Illustration of T(γ)andS(γ)in two different situations. bi,bi+1,bi+2are the breakpoints. The
situation that bi+1−bi>D(γ)is shown in the top figure, and bi+1−bi≤D(γ)is in the bottom.
To facilitate the analysis, we define the following quantities
n= 6√
2 + 3/radicalbig
1−γ,A(γ) =n2log(1
1−γ)σ2
(∆T(i))2,Ut(γ,i) =σ/radicaligg
(1−γ) log1
1−γ
Nt(γ,i). (4)
Now we list some useful lemmas. The detailed proofs are provided in the appendix. The following lemma
depicts that after finite rounds at the breakpoint, i.e., in the pseudo-stationary phase, the distance between
µt(i)and discounted average of expectation for arm ican be bounded by Ut(γ,i).Ut(γ,i)is analogous to
the upper confidence bound in the UCB algorithm.
Lemma 5.1. Let¨µt(γ,i) =1
Nt(γ,i)/summationtextt
j=1γt−j1{ij=i}µj(i)denote the discounted average of expectation for
armiat time step t.∀t∈T(γ), the distance between µt(i)and¨µt(γ,i)is less than Ut(γ,i).
|µt(i)−¨µt(γ,i)|≤Ut(γ,i), (5)
UsingLemma5.1andtheself-normalizedHoeffding-typeinequalityforsubGaussiandistributions(LemmaA.1),
we have the following lemma, which helps to bound regret comes from the over-estimation of suboptimal arm.
Lemma 5.2.∀t∈T(γ),i̸=i∗
t,
P(ˆµt(γ,i)>xt(i),Nt(γ,i)>A(γ))≤(1−γ)2
The following key lemma helps bound the regret comes from the under-estimation of the optimal arm. This
is the most tricky part of analyzing TS. Note that, the proof in Trovo et al. (2020) does not prove the result
of the following lemma.
Lemma 5.3. Letpi,t=P(θt(∗)>yt(i)|Ft−1). For anyt∈T(γ)andi̸=i∗
t,
/summationdisplay
t∈T(γ)E[1−pi,t
pi,t1{it=i∗
t,θt(i)<yt(i)}]≤(e17+ 9 + 3 log1
1−γ)T(1−γ)L1(γ)γ−1/(1−γ).
Now we can give the detailed proof. The proof is in 5steps:
Step 1We can divide the rounds t∈{1,...,T}into two parts:{t∈T(γ)}and{t /∈T(γ)}. Equation (3)
shows that the number of elements in the second part is smaller than BTD(γ), we have
E[kT(i)]≤BTD(γ) +/summationdisplay
t∈T(γ)P(it=i). (6)
7Under review as submission to TMLR
Step 2Then we consider the event {Nt(γ,i)>A(γ)}.
/summationdisplay
t∈T(γ)P(it=i) =/summationdisplay
t∈T(γ)P(it=i,Nt(γ,i)<A(γ)) +/summationdisplay
t∈T(γ)P(it=i,Nt(γ,i)>A(γ)).
We first bound/summationtext
t∈T(γ)P(it=i,Nt(γ,i)<A(γ)).
/summationdisplay
t∈T(γ)P(it=i,Nt(γ,i)<A(γ)) =/summationdisplay
t∈T(γ)E/bracketleftbig
P(it=i,Nt(γ,i)<A(γ)|Ft−1)/bracketrightbig
=/summationdisplay
t∈T(γ)E/bracketleftbig
E/bracketleftbig
1(it=i,Nt(γ,i)<A(γ)|Ft−1)/bracketrightbig/bracketrightbig
=/summationdisplay
t∈T(γ)E/bracketleftbig
1(it=i,Nt(γ,i)<A(γ))/bracketrightbig
,(7)
where the last equation uses the tower rule of expectation.
Using Lemma A.3, we have
/summationdisplay
t∈T(γ)P(it=i,Nt(γ,i)<A(γ))≤T(1−γ)A(γ)γ−1/(1−γ)(8)
Therefore,
E[kT(i)]≤T(1−γ)A(γ)γ−1/(1−γ)+BTD(γ) +/summationdisplay
t∈T(γ)P(it=i,Nt(γ,i)>A(γ))(9)
Step 3 DefineEt(γ,i)as the event{it=i,Nt(γ,i)> A(γ)}. DefineEθ
t(i)as the event θt(i)< yt(i).
Equation (9) may be decomposed as follows:
/summationdisplay
t∈T(γ)P(Et(γ,i)) =/summationdisplay
t∈T(γ)P(Et(γ,i),ˆµt(γ,i)>xt(i)) +/summationdisplay
t∈T(γ)P(Et(γ,i),ˆµt(γ,i)<xt(i),Eθ
t(i))
+/summationdisplay
t∈T(γ)P(Et(γ,i),ˆµt(γ,i)<xt(i),Eθ
t(i))(10)
Using Lemma 5.2, the first part in Equation (10) can be bounded by T(1−γ)2.
Step 4Then we bound the second part in Equation (10). Use the fact that Nt(γ,i)andˆµt(i)are determined
by the historyFt−1, we have
/summationdisplay
t∈T(γ)P(Et(γ,i),ˆµt(γ,i)<xt(i),Eθ
t(i))
=E/bracketleftbigg/summationdisplay
t∈T(γ)E/bracketleftbig
1{it=i,Nt(γ,i)>A(γ),ˆµt(γ,i)<xt(i),Eθ
t(i)}|Ft−1/bracketrightbig/bracketrightbigg
=E/bracketleftbigg/summationdisplay
t∈T(γ)1{Nt(γ,i)>A(γ),ˆµt(γ,i)<xt(i)}P(it=i,Eθ
t(i)|Ft−1)/bracketrightbigg
≤E/bracketleftbigg/summationdisplay
t∈T(γ)1{Nt(γ,i)>A(γ),ˆµt(γ,i)<xt(i)}P(θt(i)>yt(i)|Ft−1)/bracketrightbigg
.(11)
Given the history Ft−1such thatNt(γ,i)>A(γ)andˆµt(γ,i)<xt(i), we have
yt(i)−ˆµt(γ,i)>yt(i)−xt(i) =∆t(i)
3≥∆T(i)
3.
8Under review as submission to TMLR
Therefore,
P(θt(i)>yt(i)|Ft−1))≤P(θt(i)−ˆµt(γ,i)>∆T(i)
3|Ft−1)≤1
2exp(−(∆T(i))2A(γ)
72σ2)≤1
2(1−γ),(12)
where the second inequality follows θt(i)∼N/parenleftbig
ˆµt(γ,i),4σ2
Nt(γ,i)/parenrightbig
and Fact 1.
For otherFt−1, the indicator term 1{Nt(γ,i)>A(γ),ˆµt(γ,i)<xt(i)}will be 0. Hence, we can bound the
second part byT
2(1−γ)
Step 5Finally, we focus the third term in Equation (10). Using Lemma A.2 and the fact that pi,tis fixed
givenFt−1,
/summationdisplay
t∈T(γ)P(Et(γ,i),ˆµt(γ,i)<xt(i),Eθ
t(i))≤/summationdisplay
t∈T(γ)E/bracketleftbigg1−pi,t
pi,tP(it=i∗
t,Eθ
t(i)|Ft−1)/bracketrightbigg
=/summationdisplay
t∈T(γ)E/bracketleftbigg
E[1−pi,t
pi,t1{it=i∗
t,Eθ
t(i)|Ft−1}]/bracketrightbigg
=/summationdisplay
t∈T(γ)E[1−pi,t
pi,t1{it=i∗
t,Eθ
t(i)}]
Then by Lemma 5.3, we have
/summationdisplay
t∈T(γ)P(Et(γ,i),ˆµt(γ,i)<xt(i),Eθ
t(i))≤(e17+ 9 + 3 log1
1−γ)T(1−γ)L1(γ)γ−1/(1−γ).(13)
Substituting the results in Step 3-5 to Equation (10) and Equation (9),
E[kT(i)]≤T(1−γ)A(γ)γ−1
1−γ+BTD(γ) + 2T(1−γ) + (e17+ 9 + 3 log1
1−γ)T(1−γ)L1(γ)γ−1
1−γ
≤BTD(γ) + (e17+ 12 + 3 log1
1−γ)L1(γ)γ−1
1−γT(1−γ).
5.3 Proofs of Theorem 4.3
The proof of Theorem 4.3 is similar to Theorem 4.1. The main difference is that the pseudo-stationary phase
is now defined as T(τ) ={t≤T:∀s∈(t−τ,t],µs(·) =µt(·)}. Let
¨µt(τ,i) =1
Nt(τ,i)t/summationdisplay
j=t−τ+11{ij=i}µj(i).
Ift∈T(τ),
¨µt(τ,i) =1
Nt(τ,i)t/summationdisplay
j=t−τ+11{ij=i}µt(i) =µt(i).
This means the bias ( Ut(γ,i)) vanishes. We no longer need an nrelated toτto deal with the bias issue. We
only need to define A(τ)as
A(τ) =72 log(τ)σ2
(∆T(i))2.
We directly list the following two lemmas, corresponding to Lemma 5.2 and Lemma 5.3, respectively. The
detailed proofs can be found in Appendix B.4 and Appendix B.5.
9Under review as submission to TMLR
Lemma 5.4.∀t∈T(τ),t̸=i∗
t,
P(ˆµt(τ,i)>xt(i),Nt(τ,i)>A(τ))≤1
τ2.
Lemma 5.5. Letpi,t=P(θt(∗)>yt(i)|Ft−1). For anyt∈T(τ)andi̸=i∗
t,
/summationdisplay
t∈T(γ)E[1−pi,t
pi,t1{it=i∗
t,θt(i)<yt(i)}]≤(e11+ 9 + 3 logτ)T
τL2(τ).
LetS(τ) ={t≤T:t /∈T(τ)}. Similar to Equation (3), we have |S(τ)|≤BTτ. Then the proof of step 1 is
E[kT(i)]≤BTτ+/summationdisplay
t∈T(τ)P(it=i).
The rest of the proof is nearly identical to the proof of Theorem 4.1.
6 Experiments
In this section, we empirically compare the performance of our method with state-of-the-art algorithms on
Bernoulli and a Gaussian reward distributions. Specifically, we compare DS-TS and SW-TS with Thompson
Sampling to evaluate the improvement obtained thanks to the employment of the discounted factor γand
sliding window τ. We also compare our method with the UCB method, DS-UCB and SW-UCB (Garivier
& Moulines, 2011) to evaluate the effect of Thompson Sampling and UCB. Furthermore, we compare our
method with some novel and efficient algorithms such as CUSUM (Liu et al., 2018), M-UCB (Cao et al., 2019)
and SW-LB-SDA (Baudry et al., 2021). We measure the performance of each algorithm with the cumulative
expected regret defined in Equation Equation (1). The expected regret is averaged over 100independently
runs. The 95% confidence interval is obtained by performing 100independent runs and is depicted as a
semi-transparent region in the figure.
0 20000 40000 60000 80000 100000
Round t−10−50510µt(i)Arm 1
Arm 2
Arm 3
Arm 4
Arm 5
(a)
0 20000 40000 60000 80000 100000
Round t0.00.20.40.60.81.0µt(i)Arm 1
Arm 2
Arm 3
Arm 4
Arm 5 (b)
Figure 2:K= 5,BT= 5. Gauss arms (a), Bernoulli arms (b).
6.1 Gaussian Arms
Experimental setting for Gaussian arms We fix the time horizon as T= 100000 . The mean and variance
are drawn from distributions N(0,52)andU(1,5). For Gaussian rewards, we conduct two experiments. In
the first experiment, we split the time horizon into 5phases and use a number of arms K= 5. While in the
second experiment, we split the time horizon into 10phases and use a number of arms K= 10.
10Under review as submission to TMLR
The analysis of SW-UCB and DS-UCB is conducted under the bounded reward assumption, but the algorithms
can adapt to Gaussian scenarios. To achieve reasonable performance, it is necessary to adjust the discounted
factor and the sliding-window appropriately. We use the settings recommended in (Baudry et al., 2021),
whereτ= 2(1 + 2σ)/radicalbig
Tlog(T)/BTfor SW-UCB and γ= 1−1/(4(1 + 2σ))/radicalbig
BT/Tfor DS-UCB.
Results Figure 3 illustrates the performance of these algorithms for Gaussian rewards under two different
settings. Notably, CUSUM and M-UCB are not applicable to Gaussian rewards: CUSUM is designed for
Bernoulli distributions, while M-UCB assumes bounded distributions. The discounted methods tend to
perform better than sliding-windows methods in Gaussian rewards. Among these algorithms, only our
algorithms and SW-LB-SDA provide regret analysis for unbounded rewards. Our algorithm (DS-TS) and
SW-LB-SDA have demonstrated highly competitive experimental performance.
0 20000 40000 60000 80000 100000
Round t050000100000150000200000250000300000350000Regret
SW-LB-SDA
DS-UCB
SW-UCB
TS
SW-TS
DS-TS
(a)
0 20000 40000 60000 80000 100000
Round t0100000200000300000400000500000Regret
SW-LB-SDA
DS-UCB
SW-UCB
TS
SW-TS
DS-TS (b)
Figure 3: Gaussian arms. (a) K= 5,BT= 5. (b)K= 10,BT= 10
6.2 Bernoulli Arms
Experimental setting for Bernoulli arms The time horizon is set as T= 100000 . We split the time
horizon into 5,10phases of equal length and use a number of arms K={5,10},respectively.
For Bernoulli rewards, the expected value µt(i)of each arm iis drawn from a uniform distribution over [0,1].
In the stationary phase, the rewards distributions remain unchanged. The Bernoulli arms for each phase are
generated as µt(i)∼U(0,1).Figure 2 depicts the expected rewards for Gaussian arms and Bernoulli arms
withK= 5andBT= 5.
For Bernoulli distribution, we modify the Thompson sampling (step 3) in our algorithm as θt(i)∼
N(ˆµt(γ,i),1
Nt(γ,i))andθt(i)∼ N (ˆµt(τ,i),1
Nt(τ,i)). Based on Corollary 4.2 and Corollary 4.4, we set
γ= 1−/radicalig
BT
TlogTandτ=/radicalbig
T/BTlogT. To allow for fair comparison, DS-UCB uses the discount fac-
torγ= 1−/radicalbig
BT/T/4, SW-UCB uses the sliding window τ= 2/radicalbig
TlogT/BTsuggested by (Garivier &
Moulines, 2011). Based on (Baudry et al., 2021), we set τ= 2/radicalbig
Tlog(T)/BTfor LB-SDA. For changepoint
detection algorithm M-UCB, we set w= 800,b=/radicalbig
w/2 log(2KT2)suggested by (Cao et al., 2019). But
set the amount of exploration γ=/radicalbig
KBTlog(T)/T. In practice, it has been found that using this value
instead of the one guaranteed in (Cao et al., 2019) will improve empirical performance (Baudry et al., 2021).
For CUSUM, following from (Liu et al., 2018), we set α=/radicalbig
BT/Tlog(T/BT)andh=log(T/BT). For our
experiment settings, we choose M= 50,ϵ= 0.05.
Results Figure 4 presents the results for Bernoulli arms in abruptly changing settings. It can be observed
that our method (SW-TS) and SW-LB-SDA exhibit almost identical performance. Thompson Sampling,
designed for stationary MAB problems, shows significant oscillations at the breakpoints. The changepoint
11Under review as submission to TMLR
0 20000 40000 60000 80000 100000
Round t02500500075001000012500150001750020000Regret
SW-LB-SDA
CUSUM
DS-UCB
SW-UCB
M-UCB
SW-TS
DS-TS
TS
(a)
0 20000 40000 60000 80000 100000
Round t05000100001500020000Regret
SW-LB-SDA
CUSUM
DS-UCB
SW-UCB
M-UCB
SW-TS
DS-TS
TS (b)
Figure 4: Bernoulli arms. Settings with K= 5,BT= 5(a),K= 10,BT= 10(b)
detection algorithm CUSUM (Liu et al., 2018) also shows competitive performance. Note that, our experiment
does not satisfy the detectability assumption of CUSUM. As the number of arms and breakpoints increase,
the performance of UCB-class algorithms (DS-UCB, SW-UCB) declines, while two TS-based algorithms
(DS-TS, SW-TS) still work well.
Storage and Compute Cost These algorithms can be divided into three class: UCB, TS and SW-LB-SDA.
At each round, UCB-class and TS-class algorithms require O(K)storage and spend O(K)time complexity for
computational cost. However, for round T, SW-LB-SDA require O(K(logT)2)storage and spend O(KlogT)
time cost. Although the experimental performance of SW-LB-SDA is similar to our algorithms, our algorithm
has less storage space and lower computational complexity.
7 Conclusion
In this paper, we analyze the regret upper bound of the TS algorithm with Gaussian prior in non-stationary
settings, filling a research gap in this field. Our approach builds upon previous works while tackling two key
challenges specific to non-stationary environments: under-estimation of the optimal arm and the inability of
DS-TS algorithm to fully forget previous information. Finally, we conduct some experiments to verify theory
results. Below we discuss the results and propose directions for future research.
(1) The standard posterior update rule for Thompson Sampling has a sampling variance asσ2
N. We use4σ2
N
only for ease of analysis. While this discrepancy is significant only for relatively small values of N. It would
be valuable to develop proof techniques that leverage the variance of standard Bayesian updates.
(2) Our regret upper bound includes an additional logarithmic term compared to DS-UCB and SW-UCB,
along with coefficients of e17ande11. It would be interesting to explore whether the additional logarithm
and large coefficients are intrinsic to DS-TS and SW-TS algorithms or is a limitation of our analysis.
References
Milton Abramowitz and Irene A Stegun. Handbook of mathematical functions with formulas, graphs, and
mathematical tables , volume 55. US Government printing office, 1964.
Shipra Agrawal and Navin Goyal. Further optimal regret bounds for thompson sampling. In Artificial
intelligence and statistics , pp. 99–107. PMLR, 2013.
Peter Auer, Nicolo Cesa-Bianchi, Yoav Freund, and Robert E Schapire. The nonstochastic multiarmed bandit
problem. SIAM journal on computing , 32(1):48–77, 2002.
12Under review as submission to TMLR
Peter Auer, Pratik Gajane, and Ronald Ortner. Adaptively tracking the best bandit arm with an unknown
number of distribution changes. In Conference on Learning Theory , pp. 138–158. PMLR, 2019.
Dorian Baudry, Yoan Russac, and Olivier Cappé. On limited-memory subsampling strategies for bandits. In
International Conference on Machine Learning , pp. 727–737. PMLR, 2021.
Omar Besbes, Yonatan Gur, and Assaf Zeevi. Stochastic multi-armed-bandit problem with non-stationary
rewards. Advances in neural information processing systems , 27, 2014.
Lilian Besson, Emilie Kaufmann, Odalric-Ambrym Maillard, and Julien Seznec. Efficient change-point
detection for tackling piecewise-stationary bandits. Journal of Machine Learning Research , 23(77):1–40,
2022.
Djallel Bouneffouf, Amel Bouzeghoub, and Alda Lopes Ganarski. A contextual-bandit algorithm for mobile
context-aware recommender system. In International conference on neural information processing , pp.
324–331. Springer, 2012.
Yang Cao, Zheng Wen, Branislav Kveton, and Yao Xie. Nearly optimal adaptive procedure with change
detection for piecewise-stationary bandit. In The 22nd International Conference on Artificial Intelligence
and Statistics , pp. 418–427. PMLR, 2019.
Yifang Chen, Chung-Wei Lee, Haipeng Luo, and Chen-Yu Wei. A new algorithm for non-stationary contextual
bandits: Efficient, optimal and parameter-free. In Conference on Learning Theory , pp. 696–726. PMLR,
2019.
Richard Combes and Alexandre Proutiere. Unimodal bandits: Regret lower bounds and optimal algorithms.
InInternational Conference on Machine Learning , pp. 521–529. PMLR, 2014.
Aurélien Garivier and Eric Moulines. On upper-confidence bound policies for switching bandit problems. In
International Conference on Algorithmic Learning Theory , pp. 174–188. Springer, 2011.
Tianyuan Jin, Pan Xu, Jieming Shi, Xiaokui Xiao, and Quanquan Gu. Mots: Minimax optimal thompson
sampling. In International Conference on Machine Learning , pp. 5074–5083. PMLR, 2021.
Tianyuan Jin, Pan Xu, Xiaokui Xiao, and Anima Anandkumar. Finite-time regret of thompson sampling
algorithms for exponential family multi-armed bandits. Advances in Neural Information Processing Systems ,
35:38475–38487, 2022.
Levente Kocsis and Csaba Szepesvári. Discounted ucb. In 2nd PASCAL Challenges Workshop , volume 2, pp.
51–134, 2006.
Lihong Li, Wei Chu, John Langford, and Xuanhui Wang. Unbiased offline evaluation of contextual-bandit-
based news article recommendation algorithms. In Proceedings of the fourth ACM international conference
on Web search and data mining , pp. 297–306, 2011.
Shuai Li, Alexandros Karatzoglou, and Claudio Gentile. Collaborative filtering bandits. In Proceedings of the
39th International ACM SIGIR conference on Research and Development in Information Retrieval , pp.
539–548, 2016.
Fang Liu, Joohyun Lee, and Ness Shroff. A change-detection based framework for piecewise-stationary
multi-armed bandit problem. In Proceedings of the AAAI Conference on Artificial Intelligence , 2018.
Joseph Mellor and Jonathan Shapiro. Thompson sampling in switching environments with bayesian online
change detection. In Artificial intelligence and statistics , pp. 442–450. PMLR, 2013.
Vishnu Raj and Sheetal Kalyani. Taming non-stationary bandits: A bayesian approach. arXiv preprint
arXiv:1707.09727 , 2017.
Herbert Robbins. Some aspects of the sequential design of experiments. Bulletin of the American Mathematical
Society, 58(5):527–535, 1952.
13Under review as submission to TMLR
Eric M Schwartz, Eric T Bradlow, and Peter S Fader. Customer acquisition via display advertising using
multi-armed bandit experiments. Marketing Science , 36(4):500–522, 2017.
Joe Suk and Samory Kpotufe. Tracking most significant arm switches in bandits. In Conference on Learning
Theory, pp. 2160–2182. PMLR, 2022.
Francesco Trovo, Stefano Paladino, Marcello Restelli, and Nicola Gatti. Sliding-window thompson sampling
for non-stationary settings. Journal of Artificial Intelligence Research , 68:311–364, 2020.
Qingyun Wu, Naveen Iyer, and Hongning Wang. Learning contextual bandits in a non-stationary environment.
InThe 41st International ACM SIGIR Conference on Research & Development in Information Retrieval ,
pp. 495–504, 2018.
A Facts and Lemmas
In this section, we list some well-known lemmas.
Garivier & Moulines (2011) has derived a Hoeffding-type inequality for self-normalized means with a random
number of summands. Their bound is for bounded distribution. Leveraging the properties of σ-subGaussian
distributions, we have the following bound for σ-subGaussian. Recall that
Nt(γ,i) =t/summationdisplay
j=1γt−j1{ij=i},ˆµt(γ,i) =1
Nt(γ,i)t/summationdisplay
j=1γt−jXj(i) 1{ij=i}
¨µt(γ,i) =1
Nt(γ,i)t/summationdisplay
j=1γt−j1{ij=i}µj(i)
Lemma A.1. Lett∈T(γ),δ> 0,
P(Nt(γ,i)(ˆµ(γ,i)−¨µ(γ,i))/radicalbig
Nt(γ2,i)>δ)≤log(1
1−γ) exp (−3δ2
8σ2).
Lett∈T(τ),δ> 0,
P(/radicalbig
Nt(τ,i)(ˆµ(τ,i)−µt(i))>δ)≤logτexp (−3δ2
8σ2),
The following inequality is the anti-concentration and concentration bound for Gaussian distributed random
variables.
Fact 1(Abramowitz & Stegun (1964)) .For a Gaussian distributed random variable Xwith mean µand
varianceσ2, for anya>0
1√
2πa
1 +a2e−a2/2≤P(X−µ>aσ )≤1
a+√
a2+ 4e−a2/2
Since1
a+√
a2+4≤1
2, we also have the following well-known result:
P(X−µ>aσ )≤1
2e−a2/2
The following lemma is adapted from Agrawal & Goyal (2013) and is often used in the analysis of Thompson
Sampling, which can transform the probability of selecting the ith arm into the probability of selecting the
optimal arm i∗
t.
Lemma A.2. Letpi,t=P(θt(∗)>yt(i)|Ft−1). For anyA>0,i̸=i∗
t,
P(it=i,θt(i)<yt(i)|Ft−1)≤(1−pi,t)
pi,tP(it=i∗
t,θt(i)<yt(i)|Ft−1)
14Under review as submission to TMLR
Lemma A.3 (Garivier & Moulines (2011)) .For anyi∈{1,...,K},γ∈(0,1)andA>0,
T/summationdisplay
t=11{it=i,Nt(γ,i)<A}≤⌈T(1−γ)⌉Aγ−1/(1−γ),
T/summationdisplay
t=11{it=i,Nt(τ,i)<A}≤/ceilingleftbiggT
τ/ceilingrightbigg
A.
B Detailed Proofs of Lemmas and Theorems
In this section, we provide the detailed proofs of Lemma 5.1,Lemma 5.2,Lemma 5.3, Lemma 5.4 and Lemma 5.5.
The proof of Theorem 4.3 is almost identical to that of Theorem 4.1, so we have omitted the details of the
proof.
B.1 Proof of Lemma 5.1
Recall that ¨µt(γ,i) =1
Nt(γ,i)/summationtextt
j=1γt−j1{ij=i}µj(i). Since ¨µt(γ,i)is a convex combination of elements
µj(i),j= 1,...,t, we have
|µt(i)−¨µt(γ,i)|≤∆T
max (14)
We can write µt(i)asµt(i) =1
Nt(γ,i)/summationtextt
j=1γt−j1{ij=i}µt(i). Thus, we have
|µt(i)−¨µt(γ,i)|=1
Nt(γ,i)|t/summationdisplay
j=1γt−j(µj(i)−µt(i)) 1{ij=i}|.
Recall thatT(γ) ={t≤T:∀s∈(t−D(γ),t],µs(·) =µt(·)}. Ift∈T(γ), we haveµj(i) =µt(i),∀j∈
(t−D(γ),t).
Therefore,∀t∈T(γ), we have
|µt(i)−¨µt(γ,i)|=1
Nt(γ,i)|t−D(γ)/summationdisplay
j=1γt−j(µj(i)−µt(i)) 1{ij=i}|
≤∆T
max
Nt(γ,i)t−D(γ)/summationdisplay
j=1γt−j1{ij=i}
=∆T
max
Nt(γ,i)γD(γ)Nt−D(γ)(γ,i)
≤∆T
maxγD(γ)
Nt(γ,i)(1−γ)
where the last inequality follows from Nt−D(γ)(γ,i)≤1
1−γ.
IfγD(γ)
Nt(γ,i)(1−γ)<1,γD(γ)
Nt(γ,i)(1−γ)</radicalig
γD(γ)
Nt(γ,i)(1−γ), we have
|µt(i)−¨µt(γ,i)|≤∆T
max/radicaligg
γD(γ)
Nt(γ,i)(1−γ).
IfγD(γ)
Nt(γ,i)(1−γ)≥1, from Equation (14), we also have
|µt(i)−¨µt(γ,i)|≤∆T
max≤∆T
max/radicaligg
γD(γ)
Nt(γ,i)(1−γ).
15Under review as submission to TMLR
By the definition of D(γ) =log((σ
∆Tmax)2(1−γ)2log1
1−γ)
logγ,
|µt(i)−¨µt(γ,i)|≤σ/radicaligg
(1−γ) log1
1−γ
Nt(γ,i)
B.2 Proof of Lemma 5.2
From the definition of n,A(γ),Ut(γ,i)in Equation (4), we can get
Ut(γ,i) =√1−γ∆T(i)
n/radicaligg
A(γ)
Nt(γ,i). (15)
IfNt(γ,i)>A(γ),Ut(γ,i)<√1−γ
n∆T(i). Thus, we have
∆t(i)
3−Ut(γ,i)>∆T(i)
3−√1−γ
n∆T(i) =2√
2
n∆T(i). (16)
Therefore,
P(ˆµt(γ,i)>µt(i) +∆t(i)
3,Nt(γ,i)>A(γ))
(a)
≤P(ˆµt(γ,i)−¨µt(γ,i)>∆t(i)
3−Ut(γ,i),Nt(γ,i)>A(γ))
(b)
≤P(ˆµt(γ,i)−¨µt(γ,i)>2√
2
n∆T(i),Nt(γ,i)>A(γ))
(c)
≤P(Nt(γ,i)(ˆµt(γ,i)−¨µt(γ,i))/radicalbig
Nt(γ2,i)>2√
2
n∆T(i)/radicalbig
A(γ))
(d)
≤log1
1−γexp(−3(∆T(i))2
n2σ2A(γ))
≤(1−γ)3log1
1−γ(17)
where (a) uses Lemma 5.1, (b) uses Equation (16), (c) follows from Nt(γ,i)>Nt(γ2,i), (d) uses Lemma A.1.
Since (1−γ) log1
1−γ≤1
e<1, this ends the proof.
B.3 Proof of Lemma 5.3
This proof is adapted from Agrawal & Goyal (2013) for the stationary settings. However, there are some
technical problems that are difficult to overcome in non-stationary settings. The tricky problem is to lower
bound the probability of the mean’s estimation of optimal arm Equation (21). By designing the function
Ut(γ,i)and decomposing the regret to use Lemma A.3 again, we solve this challenge. We use blue font to
emphasize the techniques used in the proof.
The proof is in 3steps.
Step 1We first prove that E[1
pi,t]has an upper bound independent of t.
Define a Bernoulli experiment as sampling from N(ˆµt(∗),4σ2
Nt(γ,∗)), where success implies that θt(∗)>yt(i).
LetGtdenote the number of experiments performed when the event {θt(∗)>yt(i)}first occurs. Then
E[1
pi,t] =E[E[Gt|Ft−1]] =E[Gt]
16Under review as submission to TMLR
Letz=√logr+1
2(r≥1is an integer ) and let MAXrdenote the maximum of rindependent Bernoulli
experiment. Then
P(Gt≤r)≥P(MAXr>ˆµt(∗) +z·2σ/radicalbig
Nt(γ,∗)≥yt(i))
=E[E[ 1{MAXr>ˆµt(∗) +z·2σ/radicalbig
Nt(γ,∗)≥yt(i)}|Ft−1]]
=E[ 1{ˆµt(∗) +z·2σ/radicalbig
Nt(γ,∗)≥yt(i)}P(MAXr>ˆµt(∗) +z·2σ/radicalbig
Nt(γ,∗)|Ft−1)](18)
Using Fact 1,
P(MAXr>ˆµt(∗) +z·2σ/radicalbig
Nt(γ,∗)|Ft−1)≥1−(1−1√
2πz
z2+ 1e−z2/2)r
= 1−(1−1√
2π√logr+1
2
(√logr+1
2)2+ 1e−1/4−√
logr/2
√r)r
≥1−e−√re−√
logr/2
e0.25√
2π(√
logr+1)(19)
For anyr≥e17,e−√re−√
logr/2
e0.25√
2π(√
logr+1)≤1
r2. Hence, for any r≥e17,
P(MAXr>ˆµt(∗) +z·2σ/radicalbig
Nt(γ,∗)|Ft−1)≥1−1
r2.
Therefore, for any r≥e17,
P(Gt≤r)≥(1−1
r2)P(ˆµt(∗) +z/radicalbig
Nt(γ,∗)≥yt(i))
Next, we apply Lemma A.1 to lower bound P(ˆµt(∗) +z·2σ√
Nt(γ,∗)≥yt(i)).
P(ˆµt(∗) +z·2σ/radicalbig
Nt(γ,∗)≥yt(i))≥1−P(ˆµt(∗) +z·2σ/radicalbig
Nt(γ,∗)≤µt(∗))
≥1−P(ˆµt(∗)−¨µt(∗)≤Ut(γ,∗)−z·2σ/radicalbig
Nt(γ,∗))(20)
SinceUt(γ,∗) =σ/radicalbig
(1−γ) log1
1−γ√
Nt(γ,∗),z= logr+1
2,
Ut(γ,∗)−z·2σ/radicalbig
Nt(γ,∗)=σ/radicalig
(1−γ) log1
1−γ−σ−2σ√logr
/radicalbig
Nt(γ,∗)<−2σ√logr/radicalbig
Nt(γ,∗).
Then we have
P(ˆµt(∗) +z·2σ/radicalbig
Nt(γ,∗)≥yt(i))≥1−P(ˆµt(∗)−¨µt(∗)<−2σ√logr/radicalbig
Nt(γ,∗))
≥1−log(1
1−γ)e−3
2logr
≥1−log1
1−γ1
r1.5.(21)
Substituting, for any r>e17,
P(Gt≤r)≥1−log1
1−γ1
r1.5−1
r2(22)
17Under review as submission to TMLR
Therefore,
E[Gt] =∞/summationdisplay
r=0P(Gt≥r)
≤1 +e17+/summationdisplay
r>e17(log1
1−γ1
r1.5+1
r2)
≤e17+ 3 + 3 log1
1−γ
This proves a bound of E[1
pi,t]≤e17+ 3 + 3 log1
1−γindependent of t.
Step 2. DefineL(γ) =1152 log(1
1−γ+e17)σ2
(∆T(i))2. We consider the upper bound of E[1
pi,t]whenNt(γ,∗)>L(γ).
P(Gt≤r)≥P(MAXr>ˆµt(∗) +z·2σ/radicalbig
Nt(γ,∗)−∆t(i)
6≥yt(i))
=E[ 1{ˆµt(∗) +z·2σ/radicalbig
Nt(γ,∗)−∆t(i)
6≥yt(i)}P(MAXr>ˆµt(∗) +z·2σ/radicalbig
Nt(γ,∗)−∆t(i)
6|Ft−1)](23)
Now, since Nt(γ,∗)>L(γ),1√
Nt(γ,∗)<∆t(i)
48/radicalbig
log(1
1−γ+e17)σ. Therefore, for any r≤(1
1−γ+e17)2,
z·2σ/radicalbig
Nt(γ,∗)−∆t(i)
6=2σ√logr+σ/radicalbig
Nt(γ,∗)−∆t(i)
6≤−∆t(i)
12.
Using Fact 1,
P(θt(i)>ˆµt(i)−∆t(i)
12|Ft−1)≤1−1
2e−Nt(γ,∗)
4σ2∆t(i)2
288≥1−1
2(1/(1−γ) +e17).
This implies
P(MAXr>ˆµt(∗) +z/radicalbig
Nt(γ,∗)−∆t(i)
6|Ft−1)≥1−1
2r(1/(1−γ) +e17)r.
Also, apply the self-normalized Hoeffding-type inequality,
P(ˆµt(∗) +z·2σ/radicalbig
Nt(γ,∗)−∆t(i)
6≥yt(i))≥1−P(ˆµt(∗)≤µt(∗)−∆t(i)
6)
≥1−P(ˆµt(∗)−¨µt(∗)≥−Ut(γ,∗) +∆t(i)
6)
>1−P(ˆµt(∗)−¨µt(∗)≥∆T(i)
8/radicaligg
L(γ)
Nt(γ,∗))
≥1−log(1
1−γ+e17)1
(1/(1−γ) +e17)3.
Letγ′= (1
1−γ+e17)2. Therefore,for any 1≤r≤γ′,
P(Gt≤r)≥1−1
2rγ′r/2−log(1
1−γ+e17)1
γ′1.5.
Whenr≥γ′>e17, we can use Equation (22) to obtain,
P(Gt≤r)≥1−log1
1−γ1
r1.5−1
r2
18Under review as submission to TMLR
Combining these results,
E[Gt]≤∞/summationdisplay
r=0P(Gt≥r)
≤1 +γ′/summationdisplay
r=1P(Gt≥r) +∞/summationdisplay
r=γ′P(Gt≥r)
≤1 +γ′/summationdisplay
r=1(1
2rγ′r/2+ log(1
1−γ+e17)1
γ′1.5) +∞/summationdisplay
r=γ′(log1
1−γ1
r1.5+1
r2)
≤1 +1
2√γ′+ log(1
1−γ+e17)1√γ′+2
γ′+ log(1
1−γ)3√γ′
≤1 + 6(1−γ) log(1
1−γ+e17).
Therefore, when Nt(γ,∗)>L(γ), it holds that
E[1
pi,t]−1 =E[Gt]−1≤6(1−γ) log(1
1−γ+e17).
Step 3 LetA(γ,∗) ={t∈{1,...,T}:Nt(γ,∗)≤L(γ)}.
/summationdisplay
t∈T(γ)E[1−pi,t
pi,t1{it=i∗
t,θt(i)<yt(i)}]
≤/parenleftbigg/summationdisplay
t∈T(γ)∩A(γ,∗)+/summationdisplay
t∈T(γ)\A(γ,∗)/parenrightbigg
E/bracketleftbigg1−pi,t
pi,t1{it=i∗
t,θt(i)<yt(i)}/bracketrightbigg
≤/vextendsingle/vextendsingle{t:it=i∗
t,Nt(γ,∗)≤L(γ)}/vextendsingle/vextendsingle(e17+ 3 + 3 log1
1−γ) +/summationdisplay
t∈T(γ)\A(γ,∗)E/bracketleftbigg1−pi,t
pi,t/bracketrightbigg
≤T(1−γ)L(γ)γ−1/(1−γ)(e17+ 3 + 3 log1
1−γ) + 6T(1−γ) log(1
1−γ+e17)
≤(e17+ 9 + 3 log1
1−γ)T(1−γ)L(γ)γ−1/(1−γ).(24)
Lemma A.1 has a stricter upper bound aslog1
1−γ
log(1+η)exp(−1
2σ2(1−η2
16)). Suppose the variance of Thompson
sampling isξσ2
Nt(γ,i). The lower bound of Equation (21) becomes
log1
1−γ
log(1 +η)exp(−ξlogr
2(1−η2
16)).
To ensure that E[Gt]has a finite upper bound, our analysis method requires ξ>2, i.e. the sampling variance
needs to be strictly greater than2σ2
Nt(γ,i).
19Under review as submission to TMLR
B.4 Proof of Lemma 5.4
Recall that A(τ) =72 log(τ)σ2
(∆T(i))2. Using Lemma A.1, we have
P(ˆµt(τ,i)>xt(i),Nt(τ,i)>A(τ)) =P(ˆµt(τ,i)−µt(i)>∆t(i)
3,Nt(τ,i)>A(γ))
≤P(/radicalbig
Nt(τ,i)(ˆµt(τ,i)−µt(i))>∆T(i)
3/radicalbig
A(γ))
≤logτexp(−3(∆T(i))2
72σ2A(γ))
≤1
τ2(25)
B.5 Proof of Lemma 5.5
The proof is similar to the proof of Lemma 5.3.
Step 1We first prove that E[1
pi,t]has an upper bound independent of t.
Letz=√logr(r≥1is an integer ). Then
P(Gt≤r)≥P(MAXr>ˆµt(∗) +z·2σ/radicalbig
Nt(τ,∗)≥yt(i))
=E[ 1{ˆµt(∗) +z·2σ/radicalbig
Nt(τ,∗)≥yt(i)}P(MAXr>ˆµt(∗) +z·2σ/radicalbig
Nt(τ,∗)|Ft−1)](26)
Using Fact 1,
P(MAXr>ˆµt(∗) +z·2σ/radicalbig
Nt(τ,∗)|Ft−1)≥1−(1−1√
2πz
z2+ 1e−z2/2)r
= 1−(1−1√
2π√logr
(√logr)2+ 11√r)r
≥1−e−√r√
2π(√
logr+1)(27)
For anyr≥e11,e−√r√
2π(√
logr+1)≤1
r2. Hence, for any r≥e11,
P(MAXr>ˆµt(∗) +z·2σ/radicalbig
Nt(τ,∗)|Ft−1)≥1−1
r2.
Therefore, for any r≥e11,
P(Gt≤r)≥(1−1
r2)P(ˆµt(∗) +z/radicalbig
Nt(τ,∗)≥yt(i))
Next, we apply Lemma A.1 to lower bound P(ˆµt(∗) +z·2σ√
Nt(τ,∗)≥yt(i)).
P(ˆµt(∗) +z·2σ/radicalbig
Nt(τ,∗)≥yt(i))≥1−P(ˆµt(∗) +z·2σ/radicalbig
Nt(τ,∗)≤µt(∗))
≥1−P(ˆµt(∗)−µt(∗)<−2σ√logr/radicalbig
Nt(τ,∗))
≥1−logτe−3
2logr
= 1−logτ1
r1.5.
20Under review as submission to TMLR
Substituting, for any r>e11,
P(Gt≤r)≥1−logτ1
r1.5−1
r2(28)
Therefore,
E[Gt] =∞/summationdisplay
r=0P(Gt≥r)
≤1 +e11+/summationdisplay
r>e11(logτ1
r1.5+1
r2)
≤e11+ 3 + 3 logτ
This proves a bound of E[1
pi,t]≤e11+ 3 + 3 logτindependent of t.
Step 2. DefineL(τ) =1152 log(τ+e11)σ2
(∆T(i))2. We consider the upper bound of E[1
pi,t]whenNt(τ,∗)>L(τ).
P(Gt≤r)≥P(MAXr>ˆµt(∗) +z·2σ/radicalbig
Nt(τ,∗)−∆t(i)
6≥yt(i))
=E[ 1{ˆµt(∗) +z·2σ/radicalbig
Nt(τ,∗)−∆t(i)
6≥yt(i)}P(MAXr>ˆµt(∗) +z·2σ/radicalbig
Nt(τ,∗)−∆t(i)
6|Ft−1)](29)
Now, since Nt(τ,∗)>L(τ),1√
Nt(τ,∗)<∆t(i)
48√
log(τ+e11)σ. Therefore, for any r≤(τ+e11)2,
z·2σ/radicalbig
Nt(τ,∗)−∆t(i)
6=2σ√logr+σ/radicalbig
Nt(τ,∗)−∆t(i)
6≤−∆t(i)
12.
Using Fact 1,
P(θt(i)>ˆµt(i)−∆t(i)
12|Ft−1)≤1−1
2e−Nt(τ,∗)
4σ2∆t(i)2
288≥1−1
2(τ+e11).
This implies
P(MAXr>ˆµt(∗) +z/radicalbig
Nt(τ,∗)−∆t(i)
6|Ft−1)≥1−1
2r(τ+e11)r.
Also, apply Lemma A.1,
P(ˆµt(∗) +z/radicalbig
Nt(τ,∗)−∆t(i)
6≥yt(i))≥1−P(ˆµt(∗)−µt(∗)≥∆t(i)
6)
≥1−log(τ+e11)1
(τ+e11)3.
Letτ′= (τ+e11)2. Therefore, for any 1≤r≤τ′,
P(Gt≤r)≥1−1
2rτ′r/2−log(τ+e11)1
τ′1.5.
Whenr≥τ′>e11, we can use Equation (22) to obtain,
P(Gt≤r)≥1−logτ1
r1.5−1
r2
Combining these results,
E[Gt]≤∞/summationdisplay
r=0P(Gt≥r)
≤1 +τ′/summationdisplay
r=1P(Gt≥r) +∞/summationdisplay
r=τ′P(Gt≥r)
≤1 +6
τlog(τ+e11).
21Under review as submission to TMLR
Therefore, when Nt(τ,∗)>L(τ), it holds that
E[1
pi,t]−1 =E[Gt]−1≤6
τlog(τ+e11).
Step 3 LetA(τ,∗) ={t∈{1,...,T}:Nt(τ,∗)≤L(τ)}andC=e11+ 9.
/summationdisplay
t∈T(τ)E[1−pi,t
pi,t1{it=i∗
t,θt(i)<yt(i)}]
≤/summationdisplay
t∈T(τ)∩A(τ,∗)E[1−pi,t
pi,t1{it=i∗
t,θt(i)<yt(i)}] +/summationdisplay
t∈T(τ)\A(τ,∗)E[1−pi,t
pi,t1{it=i∗
t,θt(i)<yt(i)}]
≤|{t∈{1,...,T}:it=i∗
t,Nt(τ,∗)≤L(τ)}|(e11+ 3 + 3 logτ) +/summationdisplay
t∈T(τ)\A(τ,∗)E[1−pi,t
pi,t]
≤T
τL(τ)(e11+ 3 + 3 logτ) + 6T
τlog(τ+e11)
≤(e11+ 9 + 3 logτ)T
τL(τ).(30)
C Incorrect Proof of SW-TS with Beta Priors
Here, we discuss the mistakes in proof of Trovo et al. (2020). It is precisely because of these errors that they
bypassed the analysis of under-estimation of the optimal arm ( i.e. Lemma 5.3).
We first define the same notions in Trovo et al. (2020).
LetF′
ϕ:={t:bϕ−1+τ≤t<bϕ},bϕis theϕ-th breakpoints. Ti(F′
ϕ) :=/summationtext
t∈F′
ϕ1{it=i,i̸=i∗
ϕ}denote the
number of times a suboptimal arm is played during phase F′
ϕ.
Ti,t,r:=/summationtextt
s=max{t−τ+1,1}1{is=i}.ϑi,tis the result of Thompson sampling from the Beta distribution.
Then we cite the same equations in Trovo et al. (2020). Use Lemma A.3, they also have the following result:
/summationdisplay
t∈F′
ϕE/bracketleftbig
1{it=i,Ti,t,τ≤¯nA}/bracketrightbig
≤¯nANϕ
τ, (31)
whereF′
ϕ≤Nϕ. Thus by choosing ¯nA=/ceilingleftig
19
logτ/ceilingrightig
, we have:
RA=/summationdisplay
t∈F′
ϕP/parenleftbigg
ϑi∗
ϕ,t≤µi∗
ϕ,t−/radicaligg
5 logτ
Ti∗
ϕ,t,τ/parenrightbigg
(32)
≤/summationdisplay
t∈F′
ϕP/parenleftbigg
ϑi∗
ϕ,t≤µi∗
ϕ,t−/radicaligg
5 logτ
Ti∗
ϕ,t,τ,Ti∗
ϕ,t,τ>¯nA/parenrightbigg
+/summationdisplay
t∈F′
ϕP/parenleftbig
Ti∗
ϕ,t,τ≤¯nA/parenrightbig
(33)
≤/summationdisplay
t∈F′
ϕP/parenleftbigg
ϑi∗
ϕ,t≤µi∗
ϕ,t−/radicaligg
5 logτ
Ti∗
ϕ,t,τ,Ti∗
ϕ,t,τ>¯nA/parenrightbigg
+/summationdisplay
t∈F′
ϕE/bracketleftbig
1{Ti∗
ϕ,t,τ≤¯nA}/bracketrightbig
(34)
≤/summationdisplay
t∈F′
ϕP/parenleftbigg
ϑi∗
ϕ,t≤µi∗
ϕ,t−/radicaligg
5 logτ
Ti∗
ϕ,t,τ,Ti∗
ϕ,t,τ>¯nA/parenrightbigg
+ ¯nANϕ
τ(35)
The first mistake appears in the blue part. They claim that
/summationdisplay
t∈F′
ϕE/bracketleftbig
1{Ti∗
ϕ,t,τ≤¯nA}/bracketrightbig
≤¯nANϕ
τ. (36)
22Under review as submission to TMLR
This inequality is not true, as it lacks one condition it=i∗
ϕ. And from the context in their proof, we know
that onlyit=iholds, notit=i∗
ϕ.
To see why Equation (36) is wrong, consider the simple example: the algorithm always select the suboptimal
arm in phaseF′
ϕ. Thus, 1{Ti∗
ϕ,t,τ≤¯nA}= 1,∀t∈F′
ϕ. We have
/summationdisplay
t∈F′
ϕE/bracketleftbig
1{Ti∗
ϕ,t,τ≤¯nA}/bracketrightbig
=|F′
ϕ|.
This implies Equation (36) is not true.
In their proof, there exists other three mistakes related to Equation (36)(the numbering of the equations
below is the same as in their proof):
Eq 43→Eq 44 :/summationdisplay
t∈F′
ϕP/parenleftbig
Ti∗
ϕ,t,τ≤¯nB∗/parenrightbig
≤¯nB∗Nϕ
τ(37)
Eq 71→Eq 72 :/summationdisplay
t∈F∆C,NP/parenleftbig
Ti∗
ϕ,t,τ≤¯nA/parenrightbig
≤¯nB/ceilingleftbiggN
τ/ceilingrightbigg
(38)
Eq 95→Eq 96 :/summationdisplay
t∈F∆C,NP/parenleftbig
Ti∗
ϕ,t,τ≤¯nB∗/parenrightbig
≤¯nB∗/ceilingleftbiggNϕ
τ/ceilingrightbigg
(39)
The last two inequalities are for smoothly changes. We speculate that fixing these errors would also require
proving conclusions similar to Lemma 5.3 and Lemma 5.5. However, since the Beta distribution does not
have concentration properties like the Gaussian distribution (Fact 1), fixing these errors is challenging.
23