Under review as submission to TMLR
Shedding Light on Problems with Hyperbolic Graph Learning
Anonymous authors
Paper under double-blind review
Abstract
Recent papers in the graph machine learning literature have introduced a number of ap-
proaches for hyperbolic representation learning. The asserted benefits are improved perfor-
mance on a variety of graph tasks, node classification and link prediction included. Claims
have also been made about the geometric suitability of particular hierarchical graph datasets
to representation in hyperbolic space. Despite these claims, our work makes a surprising
discovery: when simple Euclidean models with comparable numbers of parameters are
properly trained in the same environment, in most cases, they perform as well, if not better,
than all introduced hyperbolic graph representation learning models, even on graph datasets
previously claimed to be the most hyperbolic (Chami et al., 2019) as measured by Gromov
δ-hyperbolicity (i.e., perfect trees). This observation gives rise to a simple question: how can
this be? We answer this question by taking a careful look at the field of hyperbolic graph
representation learning as it stands today, and find that a number of results fail to diligently
present baselines, make faulty modelling assumptions when constructing algorithms, and use
misleading metrics to quantify geometry of graph datasets. We take a closer look at each of
these three problems, elucidate the issues, perform an analysis of methods, and introduce a
parametric family of benchmark datasets to ascertain the applicability of (hyperbolic) graph
neural networks.
1 Introduction
Graph machine learning has undergone a meteoric rise in popularity over the course of the last several years.
Given the ubiquity of graphs, and the demonstrated capabilities of machine learning in modern large-scale
data contexts, it has been unsurprising to see widespread interest in designing machine learning models that
can do predictive inference over these fundamental structures. Relevant graph tasks that researchers have
focused on include link prediction (when a model must predict whether two nodes are connected or not) as
well as node classification (when a model must predict one of a discrete set of classes for a given graph node).
Simultaneously, recent research from Bronstein et al. (2017) and others has brought attention to the fact that
several kinds of complex data benefit from manifold considerations. In particular, Nickel & Kiela (2017) has
shown that tree-like graphs (that is, graphs with a hierarchical structure) benefit from representation through
hyperbolic node embeddings, in that such embeddings frequently yield lower distortion than their Euclidean
analogs. This line of work is well motivated by Sarkar (2011), which first showed that trees can be embedded
with arbitrarily low distortion in hyperbolic space—something that does not hold for Euclidean space.
In an attempt to capitalize on these improved non-Euclidean representations for tree-like graphs, a number of
graph machine learning papers began to incorporate non-Euclidean representations into their pipelines. This
resulted in a number of highly influential papers, namely Ganea et al. (2018) and Chami et al. (2019), both
of which later inspired even more related geometric graph machine learning work (Shimizu et al., 2020; Chen
et al., 2021; Zhang et al., 2019; 2021).
In the process of expanding and generalizing models, several papers strayed from the original theoretical and
primarily geometric motivations behind non-Euclidean embeddings for trees, frequently pursuing generality
at the expense of appropriateness for data. We posit this as one of the reasons for the following main
discovery of our paper: on prior benchmark tasks, when simple Euclidean models with a comparable number
1Under review as submission to TMLR
Figure 1: Above is an example where we give the best Euclidean embedding of a tripodal graph, associated
with a toy graph dataset (comprised of the tripodal tree and associated node features, fi), inR2, i.e., the
embedding that has lowest average distortion. We see that the largest amount of separation we can get
between the leaf nodes while maintaining the distances between the root and leaf nodes is√
3≈1.7, falling
short of 2. In contrast, we give the best hyperbolic embedding of the same tree in H2. Note that the
embedding has high geometric fidelity, i.e., all distances can be preserved. We compute this embedding
explicitly in Appendix A.
of parameters are trained in the same environment as these state-of-the-art hyperbolic models, the Euclidean
models match or outperform the hyperbolic models on the “most hyperbolic” datasets, as measured by
Gromovδ-hyperbolicity. We demonstrate this specifically for the link prediction and node classification tasks
presented originally in Chami et al. (2019), and used in a number of papers thereafter (Chen et al., 2021;
Zhang et al., 2019; 2021). Investigating this further, we find there are three fundamental problems that led
to this unfortunate state of affairs:
(i)The Euclidean baselines used in these papers were either buggy, or insufficiently trained, thereby
yielding a misleading representation of inferiority.
On a more fundamental level, this led us to the observation that the original test tasks were selected
incorrectly. Explicitly, we observed that the test tasks were too easy. They were easy enough to not
yield statistically significant separation between the performance of Euclidean baselines and state-
of-the-art hyperbolic models, they were even too weak to separate the performance of a Euclidean
baseline that uses only features from the performance of a Euclidean baseline that incorporates graph
information. Most of the hyperbolic test tasks in Chami et al. (2019) are in fact trivially solvable with
a simple Euclidean multi-layer perceptron that does not utilize any graph information. This suggests
the need for much harder test tasks, if we are to make any conclusions about the effectiveness of
incorporating graph information, let alone the effectiveness of hyperbolic methods in these contexts.
(ii)Graph machine learning models began using hyperbolic representations for Euclidean features without
any justification for the hyperbolic nature of said features (in contrast to the extant hyperbolic
nature of nodes in tree-like graphs). Additionally, modelling assumptions were frequently made that
sacrificed geometric fidelity for the sake of convenience.
2Under review as submission to TMLR
(iii)Using Gromov δ-hyperbolicity (Gromov, 1987) as a proxy for graph dataset suitability to learning
in hyperbolic space is flawed. Notably, δ-hyperbolicity is a characteristic of the graph alone, yet a
graph dataset includes node features and frequently node labels. Moreover, Gromov δ-hyperbolicity
is too coarse a metric for the purpose of understanding graph geometry at a finer level.
We expound on each of these three points in our paper, conducting rigorous experiments that give ample
evidence to demonstrate each point, while resolving a subset of key issues and laying the grounds for a more
comprehensive study. Our contributions are as follows:
1.On several of the original most hyperbolic test tasks, we demonstrate that a simple Euclidean model
outperforms or matches a variety of state-of-the-art hyperbolic (and non-trivial Euclidean) models.
2.We explain the underlying causes of this phenomenon, expounding upon each of the problems (i),
(ii), and (iii) above, providing relevant evidence and resolving a core subset of issues.
3.We perform an analysis of existing methods, and introduce a parametric family of benchmark datasets
that help establish the applicability of (hyperbolic) graph neural networks.
2 Related Work
Our work addresses and is related to a specific subset of the hyperbolic machine learning (Bronstein et al.,
2017) literature that deals with graphs in the context of intragraph prediction. Specifically, we deal with
tasks such as link prediction, in which one must predict whether two nodes are connected (Kipf & Welling,
2017), and node classification, in which one must predict one of a set of classes for individual nodes in a
graph (Kipf & Welling, 2017). That being said, we believe several problems we point out about the way
hyperbolic graph learning is being done currently may hold more generally.
Distortion Distortion is fundamental to the discussion of geometric representation learning but is not used
in a number of the aforementioned papers. One must recall the original theoretical motivation from Sarkar
(2011) was to provide a natural embedding for trees (undirected, connected, acyclic graphs) in which distortion
of graph distances could be made arbitrarily small. That is, we seek an embedding of the graph into a metric
space (M,dM)in which the distance metric dMcaptures the original graph distances as well as possible.
Typically, the continuous nature of Myields considerable benefits over the original discrete graph structure.
For a given dimension, it may be impossible to obtain a Euclidean embedding with high geometric fidelity
but this may be possible in a hyperbolic space. Take for example the task of a two-dimensional embedding of
the tripodal tree, with one root and three children, shown in Figure 1. The optimal embedding in R2does
not have high geometric fidelity as it does not preserve the graph distances faithfully. In contrast, we see in
the same figure that when the same graph is embedded in H2, we can recover distances of 2between the
children (see Appendix A for full details), staying faithful to the original graph. This example provides some
intuition for the suitability of hyperbolic space for tree embeddings.
Graph neural networks Graph neural networks, and perhaps more broadly, the application of modern
machine learning to graph tasks, begins with the paper Kipf & Welling (2017), which introduced Graph
Convolutional Networks (GCNs). The ubiquity of graphs and the power of large scale modern machine
learning quickly transformed graph machine learning into an incredibly large and diverse field (Velickovic
et al., 2018). That being said, these methods were not without their problems. Wu et al. (2019) pointed out
that many of the modifications being made to graph neural networks (GNNs) were at their core, superficial,
and presented a way to considerably simplify GNNs. Additionally, Huang et al. (2020) showed that a very
simple graph neural network construction with few parameters could outperform most state-of-the-art models
with an order of magnitude more parameters. Such papers proved very useful at forcing the field to step back
and think about what methodological changes were truly useful. In a similar sense, our paper aims to push
current hyperbolic machine learning papers, particularly in graph contexts, for more rigorous research and
careful methodological proposals.
Hyperbolic graph machine learning Hyperbolic graph machine learning can be viewed either as a subfield
of geometric machine learning (Bronstein et al., 2017) or as a subfield of graph machine learning. Either way,
3Under review as submission to TMLR
it has enjoyed increasing popularity over the course of the last several years. The original line of work from
Nickel & Kiela (2017) was principled in that it wanted to generate better embeddings for trees (WordNet
(Fellbaum, 2000) hierarchies, to be precise) via hyperbolic embedding. Concurrently, Ganea et al. (2018)
began to extend classical Euclidean neural networks to hyperbolic space. Issues in the principled nature
of the work began to arise when researchers began to push hyperbolic neural networks to do intragraph
prediction. A prominent early paper exemplifying this is Chami et al. (2019). While mentioning distortion
several times for motivation, it loses sight of what it actually means to minimize distortion (i.e., distortion is
with respect to a graph embedding). In this work, features are mapped into hyperbolic space despite the
fact that there is no direct evidence for their hyperbolicity, in contrast to the nodes themselves that are best
suited to hyperbolic space assuming the graph is a tree. Further papers that build upon Chami et al. (2019),
such as Zhang et al. (2019), Zhang et al. (2021), and Chen et al. (2021) follow the framework of initially
mapping the features into hyperbolic space. Their innovations are primarily architectural: Zhang et al. (2019)
introduces Hyperbolic Graph Attention Networks (HATs), Zhang et al. (2021) introduces Lorentzian Graph
Convolutional Networks (LGCNs), and Chen et al. (2021) introduces a hyperbolic neural network capable of
expressively capturing Lorentz boosts, in contrast to Ganea et al. (2018). Katsman et al. (2023) introduces a
general manifold version of a residual neural network and uses hyperbolic space as a use case, comparing
against the above methods.
Graph curvature One of the claims we put forth is that geometric graph measures previously used are
insufficient to capture the geometry of the graph, let alone the graph dataset (comprised of the graph, features,
and frequently node labels). Notably, Gromov δ-hyperbolicity has been previously used in Chami et al. (2019);
Chen et al. (2021); in particular, the same number, 0, is assigned to all trees despite considerable potential
differences in geometry (e.g. branch factor). Graph curvature measures, such as Ollivier-Ricci curvature (Lin
et al., 2011), have been previously studied as a way to more granularly characterize graphs, but are seldom
used in this context. Please see Appendix C for further details.
3 Background
We give the primary background necessary to understand the key results of this paper. In particular, we
introduce some essential definitions from Riemannian geometry, give necessary background on hyperbolic
space, and demonstrate explicitly how Chami et al. (2019); Liu et al. (2019); Zhang et al. (2019; 2021) map
features from Euclidean to hyperbolic space; we also define Gromov δ-hyperbolicity.
3.1 Riemannian Geometry
We establish relevant definitions from Riemannian geometry (Lee, 1997).
Manifold Ann-dimensional manifold Mis a topological space that is locally homeomorphic1toRn.
Tangent space The tangent space TxMatxof a manifoldMis defined as the vector space of all tangent
vectors atxand is isomorphic to Rn.
Riemannian manifold A Riemannian manifold is a manifold Mequipped with a Riemannian metric,
ρ= (ρx)x∈M, a smooth collection of inner products ρx:TxM×TxM→ Rfor everyx∈M.
Geodesicsandinduceddistancefunction GivenaRiemannianmanifold (M,ρ)andacurve γ:[a,b]→M,
we define the length of γto beL(γ) =/integraltextb
a||γ′(y)||ρdt. Forx,y∈M, the distance d(x,y) =infL(γ), whereγ
is any curve such that γ(a) =x,γ(b) =y. A geodesic γxyfromxtoyshould be thought of as a curve that
minimizes this length.
Riemannian exponential map For each point x∈Mand vector v∈TxM, there exists a unique
geodesicγ:[0,1]→M, whereγ(0) =x,γ′(0) =v. The exponential map expx:TxM→M is defined by
expx(v) =γ(1).
1A homeomorphism is a continuous bijection with continuous inverse.
4Under review as submission to TMLR
3.2 Hyperbolic Space
We give some necessary definitions regarding hyperbolic space. A more detailed treatment can be found in
Lee (1997).
Hyperbolic space Hyperbolic space is the Riemannian manifold of constant negative sectional curvature
K < 0. Here we deal with the hyperboloid model of hyperbolic space: Hn
K={x∈Rn+1:⟨x,x⟩L=1
K},
where⟨x,x⟩Lis the hyperbolic inner product:
⟨x,y⟩L=−x0y0+x1y1+x2y2+···+xnyn
forx,y∈Hn
K. It is equipped with the pullback metric from Rn+1(Lee, 1997).
Hyperbolic exponential map The Riemannian exponential map for the hyperboloid model has a closed
form given by:
expK
x(v) = cosh(/radicalbig
|K|||v||L)x+vsinh(/radicalbig
|K|||v||L)/radicalbig
|K|||v||L
wherex∈Hn
Kandv∈TxHn
K.
3.3 Mapping Features from Euclidean to Hyperbolic Space
Hyperbolic graph machine learning models are usually trained on graph datasets that have Euclidean features.
In order for the models to process these features, they map them to the hyperboloid via the exponential map.
This is the procedure followed by Chami et al. (2019); Liu et al. (2019); Zhang et al. (2019; 2021); Chen
et al. (2021). Explicitly, the procedure is as follows. Let xE∈Rndenote the input Euclidean features. Let
o={√
K,0,..., 0}∈Hn
Kdenote the north pole (origin) in Hn
K, which is the reference point used to perform
tangent space operations. They interpret (0,xE)as a point in ToHn
Kand use the exponential map to map it
toHn
K:
xH= expK
o((0,xE))
=
√
Kcosh/parenleftigg
||xE||2√
K/parenrightigg
,√
Ksinh/parenleftigg
||xE||2√
K/parenrightigg
xE
||xE||2

Notice how the origin for exponentiation is chosen arbitrarily. Moreover, observe that this use of hyperbolic
space to represent the features is separate from any notion of improved distortion of graph embedding,
discussed at length in Section 2. To give a concrete example, this would map the features fifrom Figure 1
into hyperbolic space, instead of producing embeddings of the graph nodes. One can perhaps posit that the
features are in some way representative of the original hierarchical relationship between the nodes, but this
need not be the case. In other words, this ad hoc exponentiation is not well-motivated.
3.4 Gromov δ-Hyperbolicity
LetG= (V,E)be a connected graph with distance function ddefined as the number of edges on a shortest
path between a pair of vertices. Formally, the Gromov δ-hyperbolicity can be defined using the following
four-point condition (Gromov, 1987).
Given a graph G= (V,E)and four vertices x,y,u,v∈Vwith:
d(x,y) +d(u,v)≥d(x,u) +d(y,v)≥d(x,v) +d(y,u)
the hyperbolicity of the quadruple x,y,u,v denoted as δ(x,y,u,v )is defined as:
δ(x,y,u,v ) = (d(x,y) +d(u,v)−(d(x,u) +d(y,v)))/2
and theδ-hyperbolicity of the graph is δ(G) =supx,y,u,v∈Vδ(x,y,u,v ). Note that Gromov δ-hyperbolicity is
nonnegative and can be thought of as measuring how “tree-like” a graph is. The closer it is to 0, the closer a
graph is to being a perfect tree. We reference the interested reader to Appendix B for additional intuition.
5Under review as submission to TMLR
4 Elucidating Problems
In this section, we revisit the problems mentioned in Section 1 and provide relevant evidence for our claims.
4.1 Misleading Presentation of Strength of Hyperbolic Graph Models
Properly-tuned Euclidean models outperform hyperbolic models A somewhat persistent issue across
a number of prominent geometric graph machine learning papers is either buggy or poor implementation of
Euclidean baselines. Such an issue does indeed arise in the publicly available code for Chami et al. (2019).
We discuss the details of the bug and the fix in Appendix F. In short, the central issue is that two lines in
the implementation of the MLP normalize all Euclidean features to lie within the unit ball; commenting
these two lines alleviates this issue. Although this introduces some numerical instability to the Fermi-Dirac
decoder, there is also a simple one line fix for this that we also give in Appendix F. The removal of this bug
allows for the Euclidean representation space to be fully utilized, i.e., the features are no longer constrained
to a unit ball.
If we fix bugs in a Euclidean baseline of Chami et al. (2019), said model outperforms or matches all hyperbolic
models on nearly all of the most hyperbolic datasets of that paper. The results for the non-buggy version of
the multi-layer perceptron Euclidean model in Chami et al. (2019) are given in the “MLP (debugged)” row of
Table 1. All results are given by mean and standard deviation over 5trials. As is evident, the debugged
Euclidean model trivially solves nearly all of the most hyperbolic tasks. The Euclidean MLP presented in
the original GitHub repository2attains 98.7±0.2test ROC AUC for link prediction (vs. 72.6±0.6for the
original) on Disease (Chami et al., 2019) and 80.3±0.7test F1 score for node classification (vs. 28.8±2.5for
the original) on the same dataset (as shown in Table 1 above). These are increases of 43.5and20.6standard
deviations, by the standards of the originally presented link prediction and node classification results for this
model, respectively. The link prediction result for Disease-M is 99.1±0.3test ROC AUC (vs. 55.3±0.5for
the original) after the correction, up 87.6standard deviations from the original result. The one exception is
Disease NC, for which the difference between the debugged and original version is staggering ( 80.3±0.7versus
28.8±2.5), though the result is not state-of-the-art. It is also worth noting that the Euclidean MLP attains
these results without the graph, since it uses features alone! The graph neural network baselines on the table
(under subheading “GNN”) are standard near state-of-the-art baselines used in the graph neural network
literature. The “Hyp NN” models are all various different hyperbolic neural network models, with either
HyboNet (Chen et al., 2021) or G-RResNet Horo (Katsman et al., 2023) being the current state-of-the-art on
these test tasks.
It should be noted that a number of papers (e.g. HyboNet (Chen et al., 2021)) make direct use of the
experimental setup provided originally in Chami et al. (2019), hence we effectively have that the original
issues with baselines cascade and affect the experimental setup for many papers at once. To the best of our
knowledge, none of these papers noticed the issues with the Euclidean MLP baseline. The experimental
details for Table 1 are given in Appendix G.
Baselines for hyperbolic knowledge graph tasks It should be noted that Euclidean baselines in other
papers can frequently be made stronger. We show a more typical scenario via a subset of the knowledge
graph tasks in Chami et al. (2020), specifically those that “exhibit hierarchical structures.” In particular, we
note that carefully tuned Euclidean models considerably decrease the gap between the baseline Euclidean
models in that paper and the introduced hyperbolic models. Results are given in Appendix D (Table 3).
4.2 How Did this Happen?
Poor dataset selection The results in Table 1 suggest that the graph structure, and perhaps geometric
fidelity more broadly, is irrelevant to solving node classification and link prediction on the “most hyperbolic”
tasks presented in the cited subset of hyperbolic machine learning papers. Please note that both Disease
and Disease-M are the most hyperbolic datasets presented, as measured by Gromov δ-hyperbolicity (the
δ-hyperbolicity is 0, meaning the graphs from both datasets are perfect trees). The fact that a Euclidean
2Please find the original GitHub repository for Chami et al. (2019) at https://github.com/HazyResearch/hgcn.
3The authors were unable to provide the dataset to obtain this result.
6Under review as submission to TMLR
Dataset Disease Disease-M Airport
Hyperbolicity δ= 0 δ= 0 δ= 1
Task LP NC LP NC LP NCShallowEuc (Chami et al., 2019) 59.8±2.032.5±1.1– – 92.0±0.060.9±3.4
Hyp (Nickel & Kiela, 2017) 63.5±0.645.5±3.3– – 94.5±0.070.2±0.1
Euc-Mixed 49.6±1.135.2±3.4– – 91.5±0.168.3±2.3
Hyp-Mixed 55.1±1.356.9±1.5– – 93.3±0.069.6±0.1NNMLP (original) 72.6±0.6 28.8±2.555.3±0.555.9±0.389.8±0.568.6±0.6
MLP (debugged) 98.7±0.280.3±0.799.1±0.3–396.4±0.195.9±0.8GNNGCN (Kipf & Welling, 2017) 64.7±0.569.7±0.466.0±0.859.4±3.489.3±0.481.4±0.6
GAT (Velickovic et al., 2018) 69.8±0.370.4±0.469.5±0.462.5±0.790.5±0.381.5±0.3
SAGE (Hamilton et al., 2017) 65.9±0.369.1±0.667.4±0.561.3±0.490.4±0.582.1±0.5
SGC (Wu et al., 2019) 65.1±0.269.5±0.266.2±0.260.5±0.389.8±0.380.6±0.1Hyp NNHNN (Ganea et al., 2018) 75.1±0.341.0±1.860.9±0.456.2±0.390.8±0.280.5±0.5
HGCN (Chami et al., 2019) 90.8±0.374.5±0.978.1±0.472.2±0.596.4±0.190.6±0.2
HAT (Zhang et al., 2019) 91.8±0.583.6±0.9– – – –
LGCN (Zhang et al., 2021) 96.6±0.684.4±0.8– – 96.0±0.690.9±1.7
HyboNet (Chen et al., 2021) 96.8±0.496.0±1.0– – 97.3±0.390.9±1.4
G-RResNet Horo (Katsman et al., 2023) 98.4±0.395.4±1.0– – 95.2±0.197.4±0.1
Table 1: Above we give graph task results for a debugged version of the Euclidean model from Chami et al.
(2019) compared to other models, including the current state-of-the-art models from Chen et al. (2021) and
Katsman et al. (2023). The metrics reported are test ROC AUC for link prediction and test F1 score for
node classification. Means and standard deviations are given over 5trials. After the bug fix, the Euclidean
“MLP (debugged)” entry above attains or nearly matches state-of-the-art results on most of the hyperbolic
test tasks in the paper. We highlight the best result only if the result gives a p-value less than 0.01after
running a paired significance t-test against the next best result.
model with no graph information (i.e. a model that only uses features) effectively solves the most hyperbolic
tasks led us to recognize what is perhaps an even more fundamental problem of the original work: the graph
tasks selected were quite poor, in that they could not serve their purpose of distinguishing the hyperbolic
models from baseline Euclidean models, simply because the multi-layer perceptron Euclidean model already
solved the tasks with no graph information. To avoid this, we claim one must first ensure that the graph
tasks benefit from geometry, i.e., that usage of the graph markedly improves performance. We elaborate on
this further in Section 5.
Poorly motivated model design A model design feature that has persisted in hyperbolic graph neural
networks due to very early adoption from Kipf & Welling (2017) is the failure to distinguish between nodes
and node features. Graph convolution convolves the node features as de facto substitutes for the nodes
themselves. However, when we deal with these graphs in a geometric context, it is important to recall that
the benefit we obtain from Sarkar (2011) in terms of embedding trees in hyperbolic space only holds if we
explicitly embed the nodes in hyperbolic space with respect to the original graph distances. Simply mapping
the node features to hyperbolic space, as discussed in Section 3.3, does not necessarily capture the original
graph geometry. One may perhaps make the argument that the features should by proxy capture some of
the original hyperbolicity of the graph, but there are myriad counterexamples4for this and hyperbolicity of
features has never been defined, let alone demonstrated. The failure to recognize and/or to explicitly address
this is what makes the decision to exponentiate features in Chami et al. (2019); Zhang et al. (2019; 2021);
Liu et al. (2019) and derivative work not methodologically principled. The construction and the theoretical
motivation presented originally by Sarkar (2011) diverge.
4Take for example a dataset of airports for which separate nodes, say major airports in New York City and Shanghai, may be
very different and far apart in the graph while being very close in feature space (e.g. similar land area, occupancy, etc.). The
point more generally is that there is no guarantee that the feature space structure will mimic the graph structure.
7Under review as submission to TMLR
Poor geometric characterization of graph datasets Another problem is that the metric used to quantify
the “hyperbolicity” of the so-called “most hyperbolic” graph datasets characterizes only the graph, and
moreover, does so in rather a coarse way. As we saw above, characterizing graph datasets via the graph
alone can be extremely misleading, simply because the features alone are often enough to solve the relevant
tasks. A proper characterization should take into account both the graph and associated node features.
Additionally, using Gromov δ-hyperbolicty is quite a coarse characterization of the graph, since δ= 0for all
trees, regardless of considerable differences in geometry (e.g. differing branch factor). One must use more
granular graph curvature metrics, for example Ollivier-Ricci curvature, to more precisely characterize graph
geometry. We refer the interested reader to Appendix B for a definition of Ollivier-Ricci curvature together
with intuition. That being said, Ollivier-Ricci is still a graph-level characterization and does not incorporate
analysis of node features, which in some sense is a more central and pressing issue.
Dataset Tree1111 (γ= 0)ModelMLP (debugged) 54.4±3.3
GCN (Kipf & Welling, 2017) 55.5±3.5
HyboNet (Chen et al., 2021) 68.4±4.6
Table 2: Link prediction results (test ROC AUC) for
our synthesized Tree1111 dataset, meant to illustrate
a context in which Euclidean models fail.
0.0 0.2 0.4 0.6 0.8 1.0
Parent Feature Coefficient ( )
5060708090100T est ROC AUC
MLP (debugged) ( =0)
GCN ( =0)
HyboNet ( =0)
Introducing Parental Dependence in FeaturesFigure 2: Introducing parental dependence in node
features very quickly leads the MLP to solve link
prediction. Each data point is obtained by tuning the
MLP on the relevant synthetic dataset. The average
of5trials is reported and the error region specifies
one standard deviation.
5 Initial Solutions
In response to the above presented problems, our solutions are to test a well-tuned graph-less Euclidean MLP
baseline on a given graph dataset before making use of a graph machine learning method, using synthetic
datasets to benchmark graph methods (or selecting tasks where graph structure is actually important for the
task at hand, i.e., where using the features alone is not sufficient to solve the task), designing models well,
based on theory (Sarkar, 2011) (which implies usefulness of graph positional embeddings), and lastly, using
quantitative graph dataset characterizations that capture both the graph and associated graph features to
determine suitability of a graph dataset to a geometric machine learning method.
Euclidean MLP baseline Crucially, we recommend a well-tuned Euclidean MLP be applied first to a
graph dataset (i.e. the features alone); if this method is insufficient, one should seek to use a graph neural
network that makes use of the graph information together with the features. Even in this case, one should
still be careful, since more recently Bechler-Speicher et al. (2024) points out different graph neural networks
can perform quite differently on various benchmarks and it may be the case that the graph is not necessary
(perhaps even detrimental) for a given task using a certain graph neural network.
Synthetic graph benchmark datasets The results in Table 1 show that these tasks make for quite a poor
proxy to measure the benefit of using the graph, let alone the geometric benefit of the proposed hyperbolic
approach. For proper benchmarking, one must attempt to select tasks that necessitate graph information.
Defining Tree (b,ℓ,γ,δ,D)For the purpose of illustration, and to introduce an important set of synthetic
graph machine learning benchmark tasks, we describe designing better alternative datasets to Disease and
Disease-M (Chami et al., 2019). We do this by introducing a parametric family of dataset distributions that
8Under review as submission to TMLR
we denote by Tree (b,ℓ,γ,δ,D); drawing a sample from a given dataset distribution implies generating a graph
dataset whose graph is an ℓ-level tree with branch factor band whose features are generated procedurally,
level-by-level, in the following way:
x(0)∼D
x(n)←γ·p(x(n)) +δ·v,wherev∼D
wherex(0)is the root node’s feature vector and p(x(n))is the feature vector of the parent node of x(n). For
the sake of concreteness, we will fix D=N(0,I1000)(i.e. the multivariate normal distribution over R1000with
mean 0and identity covariance matrix) and δ= 1. Now note that γcontrols the level of parental dependence,
which in effect controls the extent to which the features carry the graph structure (i.e. the extent to which
the relationships of the Euclidean distances between features mimic the relationships of the graph distances
between the corresponding nodes). If γ= 0, the features are entirely i.i.d. and carry no graph structure. To
perform link prediction well on such a dataset, a graph machine learning method must make explicit use of
the graph.
Tree1111 dataset For illustration (and as an initial starting point), we sample from
Tree(10,3,0,1,N(0,I1000))and call this sampled dataset Tree1111. That is, we take a graph with high branch
factor and synthesize features simply by drawing them i.i.d. from the specified Gaussian; the graph and these
features together comprise the dataset, which is suitable for link prediction. Note that the features contain no
graph information, thereby forcing use of the graph for nontrivial performance. Moreover, the larger branch
factor (10) relative to Disease (Chami et al., 2019) makes this dataset a far worse fit for Euclidean graph
machine learning models. We summarize the performance for three key models on this dataset in Table 2. In
particular, the Euclidean multi-layer perceptron that trivially solves Disease in Table 1 only obtains 54.4±3.3
test ROC AUC for Tree1111 link prediction after thorough tuning. Moreover, adding graph information via
the traditional Euclidean GCN also does not help, yielding a result of 55.5±3.5test ROC AUC. Finally, we
note that the state-of-the-art hyperbolic model HyboNet (Chen et al., 2021) obtains 68.4±4.6test ROC
AUC, a considerable improvement over both Euclidean models. This Tree1111 result is significant, in that it
demonstrates a situation where graph information is not only relevant, but the straightforward Euclidean
GCN treatment fails, and a hyperbolic treatment (of the graph) truly helps yield better results.
Analysis of existing methods Moreover, we note that when the features are completely uninformative,
a number of standard Euclidean graph models do not use graph information in a more complex way than
a simple GCN (when performing a per-layer analysis at initialization); the full details of this analysis are
given in Appendix E. This highlights the overreliance of existing graph machine learning methods on the
adjacency matrix alone, and indicates that more explicit usage of the graph information via embeddings can
yield considerable benefit in situations where the graph is important.
Tree1111 γdatasets To further illustrate this, as well as how when graph information is leaked into the
features, graph machine learning can quickly becomes entirely unnecessary, we vary γfrom 0to1in increments
of0.2, sampling a graph dataset from each parametric distribution. This produces six datasets, that we call
{Tree1111γ|γ∈{0.2k|k∈0∪[5]}}. We tune a Euclidean MLP on these datasets and give the results in Figure
2. As is clearly visible, as soon as γ≥0.4, the MLP trivially solves the link prediction task with features alone.
To produce more challenging benchmark datasets, we also generate Tree1111 0.05,Tree1111 0.1,Tree1111 0.15;
as can be seen from Figure 2, these datasets are not as trivially solved by the MLP and thus are reasonable
additional benchmarks for hyperbolic graph machine learning models.
Note that the feature synthesis process for γ >0can be viewed as sampling a positional embedding for the
nodes of the graph, which we see here, produces a considerable benefit and is a potential solution (yielding
complementary benefit) to the fact that many graph methods do not make use of graph information in a
more nuanced manner.
6 Conclusion
In this work, we have demonstrated that state-of-the-art geometric graph learning results are misleading,
in that, for most of the hyperbolic tasks in Chami et al. (2019), a simple Euclidean model can outperform
or match state-of-the-art models while using the features alone. We explain the problems that led to this
9Under review as submission to TMLR
state of affairs. Namely, that hyperbolic graph machine learning papers (i) suffer from buggy or weak
Euclidean baselines, (ii) select test tasks that can be solved with the features alone, (iii) stray from the
theoretical motivation behind hyperbolic embedding for trees, and (iv) use a graph measure ( δ-hyperbolicity)
to characterize an entire graph dataset, as opposed to characterizing the features jointly with the graph
(moreover, the graph measure used is coarse).
Limitations We present initial solutions in Section 5 as a substantial first step towards resolving the
aforementioned problems, but recognize that we fall short of a complete understanding of all aspects.
Future work We resolve the first three of these four problems above. Additional work on the third problem
would consist of finding real world benchmarks to match the synthetic benchmarks we introduced in this
paper. The largest remaining amount of work is to be done on the fourth problem, that of introducing a
metric to characterize graph datasets (that is, the graph and the associated node features). For this, one
will have to characterize the interaction of the node features and the graph jointly, and may have to use
more granular metrics than Gromov δ-hyperbolicity (e.g. Ollivier-Ricci curvature) to gain a better geometric
understanding of the underlying structure.
Impact statement This paper, among other things, demonstrates the need for greater care when dealing
with baseline evaluation in the context of hyperbolic machine learning. We do not have many concerns about
negative societal impact, and hope our results will improve the quality of evaluation in the geometric graph
machine learning literature, thereby improving the overall quality of work in the field. It should be noted
that we do make use of a synthetic dataset to demonstrate some of our core claims and note that we do not
want to push the field entirely to the extreme of only synthetic benchmarks; ideally, we would use relevant
practical real-world datasets for evaluation and finding such test tasks should be a priority for the field.
References
Asuman Güven Aksoy and Sixian Jin. The apple doesn’t fall far from the (metric) tree: Equivalence
of definitions. arXiv: Metric Geometry , 2013. URL https://api.semanticscholar.org/CorpusID:
14720409 .
Maya Bechler-Speicher, Ido Amos, Ran Gilad-Bachrach, and Amir Globerson. Graph neural networks use
graphs when they shouldn’t, 2024.
Antoine Bordes, Nicolas Usunier, Alberto García-Durán, Jason Weston, and Oksana Yakhnenko. Translating
embeddings for modeling multi-relational data. In Neural Information Processing Systems , 2013. URL
https://api.semanticscholar.org/CorpusID:14941970 .
M. Bronstein, Joan Bruna, Y. LeCun, Arthur D. Szlam, and P. Vandergheynst. Geometric deep learning:
Going beyond euclidean data. IEEE Signal Processing Magazine , 34:18–42, 2017.
Ines Chami, Zhitao Ying, Christopher Ré, and Jure Leskovec. Hyperbolic graph convolutional neural networks.
InAdvances in neural information processing systems , pp. 4868–4879, 2019.
Ines Chami, Adva Wolf, Da-Cheng Juan, Frederic Sala, Sujith Ravi, and Christopher Ré. Low-dimensional
hyperbolic knowledge graph embeddings. In Annual Meeting of the Association for Computational
Linguistics , 2020.
Weize Chen, Xu Han, Yankai Lin, Hexu Zhao, Zhiyuan Liu, Peng Li, Maosong Sun, and Jie Zhou. Fully
hyperbolic neural networks. ArXiv, abs/2105.14686, 2021. URL https://api.semanticscholar.org/
CorpusID:235254732 .
Christiane D. Fellbaum. Wordnet : an electronic lexical database. Language , 76:706, 2000. URL https:
//api.semanticscholar.org/CorpusID:5958691 .
Octavian Ganea, Gary Bécigneul, and Thomas Hofmann. Hyperbolic neural networks. In Advances in neural
information processing systems , pp. 5345–5355, 2018.
10Under review as submission to TMLR
M. Gromov. Hyperbolic Groups , pp. 75–263. Springer New York, New York, NY, 1987. ISBN 978-1-4613-9586-7.
doi: 10.1007/978-1-4613-9586-7_3. URL https://doi.org/10.1007/978-1-4613-9586-7_3 .
William L. Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large graphs. In
NIPS, 2017.
Qian Huang, Horace He, Abhay Singh, Ser-Nam Lim, and Austin R. Benson. Combining label propagation
and simple models out-performs graph neural networks. ArXiv, abs/2010.13993, 2020.
Isay Katsman, Eric Chen, Sidhanth Holalkere, Anna Asch, Aaron Lou, Ser-Nam Lim, and Christopher De Sa.
Riemannian residual neural networks. ArXiv, abs/2310.10013, 2023. URL https://api.semanticscholar.
org/CorpusID:264145900 .
Thomas Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. ArXiv,
abs/1609.02907, 2017.
John M. Lee. Riemannian Manifolds: An Introduction to Curvature . 1997.
Yong Lin, Linyuan Lu, and Shing-Tung Yau. Ricci curvature of graphs. Tohoku Mathematical Journal , 63:
605–627, 2011.
Qi Liu, Maximilian Nickel, and Douwe Kiela. Hyperbolic graph neural networks. In Advances in Neural
Information Processing Systems , pp. 8230–8241, 2019.
Aaron Lou, Isay Katsman, Qingxuan Jiang, Serge Belongie, Ser-Nam Lim, and Christopher De Sa. Differenti-
ating through the fréchet mean. In International Conference on Machine Learning , 2020.
Maximillian Nickel and Douwe Kiela. Poincaré embeddings for learning hierarchical representations. In
Advances in neural information processing systems , pp. 6338–6347, 2017.
Rik Sarkar. Low distortion delaunay embedding of trees in hyperbolic plane. In International Symposium on
Graph Drawing , pp. 355–366. Springer, 2011.
Ryohei Shimizu, Yusuke Mukuta, and Tatsuya Harada. Hyperbolic neural networks++, 2020.
Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio’, and Yoshua Bengio.
Graph attention networks. ArXiv, abs/1710.10903, 2018.
Felix Wu, Tianyi Zhang, Amauri H. de Souza, Christopher Fifty, Tao Yu, and Kilian Q. Weinberger.
Simplifying graph convolutional networks. ArXiv, abs/1902.07153, 2019.
Yiding Zhang, Xiao Wang, Xunqiang Jiang, Chuan Shi, and Yanfang Ye. Hyperbolic graph attention network.
IEEE Transactions on Big Data , 8:1690–1701, 2019. URL https://api.semanticscholar.org/CorpusID:
208857525 .
Yiding Zhang, Xiao Wang, Chuan Shi, Nian Liu, and Guojie Song. Lorentzian graph convolutional networks.
Proceedings of the Web Conference 2021 , 2021. URL https://api.semanticscholar.org/CorpusID:
233241168 .
11Under review as submission to TMLR
Appendix
A Hyperbolic Embedding Example Computation
In this section we specify some fundamental Riemannian constructs in the context of the hyperboloid model
of hyperbolic space (Lee, 1997), use them to compute an H2embedding of the tripodal graph in Figure
1, and show that the distances can be preserved with arbitrarily low distortion in hyperbolic space. We
will deal with the hyperboloid model of hyperbolic space with curvature K < 0; the relevant point set is
Hn
K={x∈Rn+1:⟨x,x⟩L=1
K}, where⟨x,x⟩Lis the hyperbolic inner product:
⟨x,y⟩L=−x0y0+x1y1+x2y2+···+xnyn
forx,y∈Hn
K. Referencing Lou et al. (2020), we note that the hyperbolic distance in this model is given by:
dK
H(x,y) =1/radicalbig
|K|cosh−1(K⟨x,y⟩L)
and the Riemannian exponential map has a closed form given by:
expK
x(v) = cosh(/radicalbig
|K|||v||L)x+vsinh(/radicalbig
|K|||v||L)/radicalbig
|K|||v||L
wherex∈Hn
Kandv∈TxHn
K. Recall we seek a low distortion embedding for the tripodal graph shown
in Figure 1 with distances d(µ,xi) = 1andd(xi,xj) = 2fori̸=j. It will be convenient to give explicit
coordinates vectors for the x1,x2,x3,µembeddings. Hence, for reference, we begin by labeling xE
1,xE
2,xE
3,µE
as the Euclidean embeddings given in Figure 1, for the sake of disambiguation:
µE=/bracketleftbigg0
0/bracketrightbigg
,xE
1=/bracketleftbigg1
0/bracketrightbigg
,xE
2=/bracketleftigg
−1
2√
3
2/bracketrightigg
,xE
3=/bracketleftigg
−1
2
−√
3
2/bracketrightigg
We will now embed these points into H2
Kusing the Riemannian exponential map at the origin of the
hyperboloid, µH, which has coordinate form:
µH=
1√
|K|
0
0

We must prepend a 0to the Euclidean vectors prior to using them with the above exponential map formulation
in order to properly treat them as tangent vectors (vectors lying in the tangent space of H2
Kwith point of
tangencyµH). We abuse notation slightly and treat the original Euclidean vectors equivalently to those with
this augmentation. Computing the embedding, we see:
expK
µH(µE) = cosh(/radicalbig
|K|·0)µH+ 0 =µH
xH
1:= expK
µH(xE
1) = expK
µH

0
1
0

= cosh(/radicalbig
|K|)µH+sinh(/radicalbig
|K|)/radicalbig
|K|
0
1
0
=
cosh(√
|K|)√
|K|
sinh(√
|K|)√
|K|
0

xH
2:= expK
µH(xE
2) = expK
µH

0
−1
2√
3
2

= cosh(/radicalbig
|K|)µH+sinh(/radicalbig
|K|)/radicalbig
|K|
0
−1
2√
3
2
=
cosh(√
|K|)√
|K|
−sinh(√
|K|)
2√
|K|√
3 sinh(√
|K|)
2√
|K|

12Under review as submission to TMLR
xH
3:= expK
µH(xE
3) = expK
µH

0
−1
2
−√
3
2

= cosh(/radicalbig
|K|)µH+sinh(/radicalbig
|K|)/radicalbig
|K|
0
−1
2
−√
3
2
=
cosh(√
|K|)√
|K|
−sinh(√
|K|)
2√
|K|
−√
3 sinh(√
|K|)
2√
|K|

Notice that because the Riemannian expis a local isometry, we have dK
H(µH,xH
i) = 1, preserving the original
Euclidean distances to the origin in the tangent space. Indeed, we can confirm for µHandxH
1:
dK
H(µH,xH
1) =dK
H

1√
|K|
0
0
,
cosh(√
|K|)√
|K|
sinh(√
|K|)√
|K|
0

=1/radicalbig
|K|cosh−1/parenleftigg
K·−cosh(/radicalbig
|K|)
|K|/parenrightigg
= 1
Thus we preserve the distances from µto the children with no distortion. Now for the leaf node distortion
analysis, we select xH
1andxH
2, without loss of generality (by symmetry). Note:
dK
H(xH
1,xH
2) =dK
H

cosh(√
|K|)√
|K|
sinh(√
|K|)√
|K|
0
,
cosh(√
|K|)√
|K|
−sinh(√
|K|)
2√
|K|√
3 sinh(√
|K|)
2√
|K|


=1/radicalbig
|K|cosh−1/parenleftigg
−Kcosh2(/radicalbig
|K|)
|K|−Ksinh2(/radicalbig
|K|)
2|K|/parenrightigg
=1/radicalbig
|K|/parenleftbigg
cosh2(/radicalbig
|K|) +1
2sinh2(/radicalbig
|K|)/parenrightbigg
Now observe:
lim
|K|→0+dK
H(xH
1,xH
2) = lim
|K|→0+1/radicalbig
|K|/parenleftbigg
cosh2(/radicalbig
|K|) +1
2sinh2(/radicalbig
|K|)/parenrightbigg
=√
3
and
lim
|K|→∞dK
H(xH
1,xH
2) = lim
|K|→∞1/radicalbig
|K|/parenleftbigg
cosh2(/radicalbig
|K|) +1
2sinh2(/radicalbig
|K|)/parenrightbigg
= 2
Thus we see when the curvature is 0, we obtain the distorted Euclidean distance√
3, as anticipated. Yet as the
curvature becomes more and more negative, dK
H(xH
1,xH
2)approaches 2, which is the original graph distance
between any two leaf nodes. Thus we have manifested a hyperbolic embedding and have demonstrated that
it is suitable to preserve the original graph distances in Figure 1 with arbitrarily low distortion.
B Hyperbolic Geometry: Gromov δ-Hyperbolicity Intuition
In this section, we follow up on the algebraic definition of Gromov δ-hyperbolicity given in Section 3.4 and
aim to give a more intuitive way of reasoning about this commonly used graph characterization metric. In
service of attaining this end, we give a less algebraic and more geometrically intuitive definition of Gromov
13Under review as submission to TMLR
Figure 3: An illustration of the δ-slim triangle condition for the metric space (X,d). RecallBδ(S):=
{p|infs∈Sd(s,p)<δ}.
δ-hyperbolicity in what follows.5Let(X,d)be a geodesic metric space (a space in which any two points can
be connected via a geodesic, not necessarily unique). Let x,y,z∈X. A geodesic triangle with vertices x,y,z
is the union of the three geodesic segments [x,y],[y,z],[z,x](where [p,q]denotes a geodesic segment with
endpointspandq). If for any point m∈[x,y]there is a point in [y,z]∪[z,x]at distance less than δofm,
and similarly for points on the other edges, and δ≥0, then the triangle is said to be δ-slim. Please see Figure
3 for an illustration of this condition. We can then define a δ-hyperbolic space as a geodesic metric space in
which all geodesic triangles are δ-slim. Notably, if δ= 0, we see the triangles collapse to tripods, implying
the space is isomorphic to a tree.
C Graph Characterization via Ollivier-Ricci Curvature
Though Gromov δ-hyperbolicity is commonly used to characterize the geometry of graphs, we note that it
assigns a single integer to an entire graph, and fails to distinguish finer graph geometry that may be relevant
in a machine learning context. In particular, two trees with very different structure (e.g. very different
branch factor) will both be assigned a Gromov δ-hyperbolicity value of 0. Thus, one may desire a finer way
to characterize graph geometry in this context. One such way to obtain a more fine characterization is via
Ollivier-Ricci curvature (Lin et al., 2011). Ollivier-Ricci curvature is a discretization of Ricci curvature, and
can be used to locally characterize graph edges. The definition is given below.
Definition 1 (Ollivier-Ricci Curvature). Let(X,d)be a separable, completely metrizable, metric space.
Let(mx)x∈Xbe a family of probability measures on Xsuch that (i) the measure mxdepends measurably
onx∈Xand (ii) for every x∈X, the first moment/integraltext
d(x,y)dmx(y)is finite. Let x,y∈X,x̸=y. The
Ollivier-Ricci curvature κ(x,y)of(X,d, (mx))along (xy)is defined by:
κ(x,y):= 1−T1(mx,my)
d(x,y)
whereT1(mx,my)is theL1transportation distance from mxtomy.
In the context of graphs, we measure the Ollivier-Ricci curvature of an edge (x,y)by takingmxandmyto
be discrete uniform distributions over the neighbor sets Nx,Ny, respectively. The metric dis induced by
the graph (i.e. shortest path). Computing the curvature boils down to computing the L1transport distance
between these distributions, which boils down to solving a linear program (and can be automated with
a simple linear program solver). Computing the Ollivier-Ricci curvature for all edges can give a nuanced
view of graph geometry that is not offered by Gromov δ-hyperbolicity. In particular, note that while the
graphs from Disease (Chami et al., 2019) and Tree1111 (a dataset we introduce) are both trees, i.e. Gromov
δ-hyperbolicity is 0for both, the Ollivier-Ricci curvature profiles given in Figure 4 differ considerably. As
such, Ollivier-Ricci curvature provides a more granular approach to graph characterization that would benefit
5This definition of δ-hyperbolicity is equivalent to the prior definition up to a constant factor (Aksoy & Jin, 2013).
14Under review as submission to TMLR
Figure 4: Ollivier-Ricci curvature (Lin et al., 2011) for two graphs. The left graph is the Disease (Chami
et al., 2019) dataset and the right graph is a tree with a branch factor of 10from the synthetic graph dataset
Tree1111 we introduce.
the hyperbolic graph machine learning community in particular (this community currently largely relies on
Gromovδ-hyperbolicity).
D Tuning Euclidean Baselines
In cases where Euclidean baselines are not well presented, most of the time Euclidean baselines are not
severely buggy, but rather, are not tuned to maximally present the extent of their performance. In this
section, we investigate baselines in the application of geometric machine learning to the knowledge graph
tasks in Chami et al. (2020). In particular, we note that further tuning the Euclidean models for one of the
datasets that “exhibits hierarchical structures” (Chami et al., 2020) considerably bridges the gap between the
baseline Euclidean models and the introduced hyperbolic models. Results are given in Table 3 (best of 5
trials is given; only the best result is reported, following conventions set by prior work for these tasks).
Dataset WN18RR (Bordes et al., 2013)
Space Model MRR H@1 H@3 H@10
RdRefE 0.455 0.419 0.470 0.521
RotE 0.463 0.426 0.477 0.529
RdRefE (tuned) 0.457 0.425 0.469 0.512
RotE (tuned) 0.466 0.429 0.481 0.534
HdRefH 0.447 0.408 0.464 0.518
RotH 0.472 0.428 0.490 0.553
Table 3: Above we give link prediction results for WN18RR (Bordes et al., 2013), a dataset that “exhibits
hierarchical structures” (Chami et al., 2020). We deal with low-dimensional embeddings ( d= 32) in the
filtered setting, as was the case in the original paper (Chami et al., 2020). The properly tuned Euclidean
models considerably close the gap between the baseline Euclidean results and the hyperbolic results. While
tuning, we focus on MRR, but most other metrics also exhibit a concomitant increase.
E Analysis of Graph Methods when Features are Uninformative
In this section, under some assumptions, we analyze a variety of graph machine learning methods in a context
where the features are uninformative of the graph structure. We do this to better understand how the graph
is being used when the features offer no distinction between nodes for the task at hand.
15Under review as submission to TMLR
Per-layer Analysis
We analyze a variety of common graph machine learning baselines on a graph dataset where the features are
i.i.d.; this is a very strong setting, in that when this is the case, graph methods must use the graph to obtain
nontrivial performance. Note that our introduced synthetic Tree1111 ( γ= 0) dataset falls under this setting.
Recall that a graph dataset is given by the graph structure G= (V,E), whence the adjacency matrix Ais
derived, together with the associated feature vectors X∈R|V|×n; that is, a feature vector in Rnis given for
each of the vertices. We will make the simplifying assumption that the input features are drawn i.i.d. from
N(0,Id).
Multi-layer Perceptron (MLP) Recall that a layer of the MLP is simply:
X(n)=σ(X(n−1)W)
whereWare the learned weights, X(n−1)is the matrix of input features, and σis a non-linearity. This is
one of the simplest models used in this context and is a baseline that uses no graph information (only the
features).
Graph Convolutional Network (GCN) A layer of the GCN (Kipf & Welling, 2017) is given by:
X(n)=σ(˜AX(n−1)W)
whereWandX(n−1)are as given above, and ˜Ais the normalized adjacency matrix of the underlying graph.
We have:
˜A=¯D−1/2¯A¯D−1/2
¯A=A+I|V|
where ¯Dis the degree matrix of ¯A. The GCN is the simplest modern graph machine learning method that
uses graph information by way of a normalized adjacency matrix multiplication.
Graph Attention Network (GAT) A layer of GAT (Velickovic et al., 2018) is given by:
X(n)
i=σ
/summationdisplay
j∈NiαijWX(n−1)
j

whereNirepresents the graph neighbors of node iand:
αij=exp
LeakyReLU
aT/bracketleftigg
WX(n−1)
i
WX(n−1)
j/bracketrightigg


/summationtext
k∈Niexp
LeakyReLU
aT/bracketleftigg
WX(n−1)
i
WX(n−1)
k/bracketrightigg


are the attention weights. Note that if all node features Xiare i.i.d. Gaussian, the vectors/bracketleftigg
WX(n−1)
i
WX(n−1)
j/bracketrightigg
are
all equal in distribution for any i,j. By properties of softmax, this implies αijconcentrates on1
|Ni|, and hence
X(n)
iconcentrates on σ/parenleftig/summationtext
j∈Ni1
|Ni|WX(n−1)
j/parenrightig
, and since Wis constant, the features over which learning
happens approximately follow N(0,1
|Ni|Id).
That is to say, the attention scores are spread evenly across all neighbors, and use of the graph structure
reduces simply to scaling down covariance by degree.
Simple Graph Convolution Network (SGC) A K-layer SGC (Wu et al., 2019) is given by:
X(n)=softmax (˜AKXW)
16Under review as submission to TMLR
where ˜Ais the normalized adjacency matrix. On a per-layer basis (K= 1)it reduces to:
X(n)=softmax (˜AXW )
that is, the graph is used in no more complicated of a way than how it is used for the GCN.
Graph Sample and Aggregation (SAGE) A layer of SAGE (Hamilton et al., 2017) is given by:
X(n)
i=σ
W/bracketleftigg
X(n−1)
i
X(n)
N(i)/bracketrightigg

where:
X(n)
N(i)=Agg(n)({X(n−1)
j,∀j∈N(i)})
The aggregation function Aggis selected to be either a mean or max pool operator. For the sake of simplicity,
let us assume the aggregation operator is the mean. Note if the aggregation is the mean function, we have:
X(n)
N(i)d=N(0,1
|Ni|Id)
That is to say, X(n)
iis learned from the previous feature vector and a feature drawn from a distribution with
covariance scaled down by the degree. In other words, merely the degree is used from the graph and the
model uses graph structure in no more a sophisticated manner than the GCN.
A summary of this analysis is provided in Table 4, highlighting how each method does (or does not) use the
graph. As we can clearly see, when dealing with a dataset where the features are not informative, most graph
learning methods do not actually use the graph information in a more sophisticated way than the GCN.
This may perhaps be surprising and showcases the overreliance on features of most graph methods. If graph
information is truly necessary to solve the task at hand, this analysis indicates that coming up with positional
embeddings may prove to be highly beneficial (and this is confirmed in Figure 2 of the main paper).
Table 4: Summary Analysis of Various Graph Machine Learning Methods on a Dataset with Independent,
Identically Distributed Gaussian Features
Method Layer Structure Graph Dependence Reduction (first layer at initialization)
MLP X(n)=σ(X(n−1)W) None
GCN X(n)=σ(˜AX(n−1)W) ˜AX(n−1)W
GATX(n)
i=σ/parenleftig/summationtext
j∈NiαijWX(n−1)
j/parenrightig
, whereαij=exp
LeakyReLU
aT/bracketleftigg
WX(n−1)
i
WX(n−1)
j/bracketrightigg


/summationtext
k∈Niexp
LeakyReLU
aT/bracketleftigg
WX(n−1)
i
WX(n−1)
k/bracketrightigg

N(0,1
|Ni|Id)
SGC X(n)=softmax (˜AKX(n−1)W) ˜AX(n−1)W
SAGE X(n)
i=σ
W/bracketleftigg
X(n−1)
i
X(n)
N(i)/bracketrightigg
, whereX(n)
N(i)=Agg(n)({X(n−1)
j,∀j∈N(i)}) N(0,1
|Ni|Id), if Agg(n)is mean
F HGCN (Chami et al., 2019) Euclidean Multi-layer Perceptron Bug
A fairly prominent, and motivating, discovery in our paper is that in Chami et al. (2019), there was a bug
present for Euclidean baselines, most prominently for the Euclidean MLP. This bug is manifested by a single
line of code in the publicly released implementation; this line constrained Euclidean model representations to
lie within the unit ball, perhaps a vestigial feature from the Poincaré ball implementation. For reference, the
line, together with the relevant conditional statement, is given below (lines 104-105 of ‘models/base_models.py’
of the original repository, accessible at https://github.com/HazyResearch/hgcn):
17Under review as submission to TMLR
ifself.manifold_name == 'Euclidean ':
h = self.manifold.normalize(h)
If you simply comment these lines, as we did above for Table 1, and slightly modify the Fermi-Dirac decoder
in ‘layers/layers.py’ by replacing line 86:
probs = 1. / (torch.exp(((dist - self.r) / self.t)) + 1.0)
with:
probs = 1. / (torch.exp(((dist - self.r) / self.t).clamp_max(20)) + 1.0)
for better numerical stability, the very Euclidean MLP presented in the original GitHub repository6attains
98.7%test ROC AUC for link prediction on Disease (up from 72.6%for the original version) and 80.3%
test F1 score for node classification on Disease (up from 28.8%for the original version). The Euclidean
MLP also attains 99.1%test ROC AUC for link prediction on Disease-M (up from 55.3%). Note that
Disease and Disease-M (Chami et al., 2019) are the most hyperbolic datasets presented, as measured by
Gromovδ-hyperbolicity (the δ-hyperbolicity is 0, meaning the graphs from both datasets are trees). This is a
considerable issue, since the HGCN GitHub repository has been used in a number of hyperbolic machine
learning papers (Chen et al., 2021; Lou et al., 2020; Katsman et al., 2023) that introduce their own hyperbolic
models.
G Experimental Details
Our runs for Table 1, specifically the “MLP (debugged)” row, were performed on a single NVIDIA GeForce
RTX 3090 GPU. All reported results in Table 1 give mean and standard deviation over 5trials; trials differ
only in terms of the seed used. The same procedure is followed for the three rows of Table 2 and each data
point of Figure 2. Data for other rows in Table 1 was taken from existing results reported in Chami et al.
(2019) and Chen et al. (2021).
A complete repository with our code (and a bug-free version of the Euclidean multi-layer perceptron model
from Chami et al. (2019)) is available at <redacted for purposes of double-blind review, but please find
our code attached in the supplementary material, together with a README file with exact commands to
replicate results>.
6Please find the original GitHub repository for Chami et al. (2019) at https://github.com/HazyResearch/hgcn.
18