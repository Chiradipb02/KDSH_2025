Differentially Private Optimizers Can Learn Adversarially
Robust Models
Zhiqi Bu∗woodyx218@gmail.com
Yuan Zhang∗ewyuanzhang@gmail.com
Reviewed on OpenReview: https: // openreview. net/ forum? id= o8VgRNYh6n
Abstract
Machine learning models have shone in a variety of domains and attracted increasing at-
tention from both the security and the privacy communities. One important yet worrying
question is: Will training models under the differential privacy (DP) constraint have an
unfavorable impact on their adversarial robustness? While previous works have postulated
that privacy comes at the cost of worse robustness, we give the first theoretical analysis
to show that DP models can indeed be robust and accurate, even sometimes more robust
than their naturally-trained non-private counterparts. We observe three key factors that
influence the privacy-robustness-accuracy tradeoff: (1) hyper-parameters for DP optimizers
are critical; (2) pre-training on public data significantly mitigates the accuracy and robust-
ness drop; (3) choice of DP optimizers makes a difference. With these factors set properly,
we achieve 90% natural accuracy, 72% robust accuracy ( +9%than the non-private model)
underl2(0.5)attack, and 69% robust accuracy ( +16%than the non-private model) with
pre-trained SimCLRv2 model under l∞(4/255)attack on CIFAR10 with ϵ= 2. In fact, we
show both theoretically and empirically that DP models are Pareto optimal on the accuracy-
robustnesstradeoff. Empirically, therobustnessofDPmodelsisconsistentlyobservedacross
various datasets and models. We believe our encouraging results are a significant step to-
wards training models that are private as well as robust.
1 Introduction
Machine learning models trained on large amount of data can be vulnerable to privacy attacks and leak
sensitive information. For example, Carlini et al. (2021) shows that attackers can extract text input from
the training set via GPT2 (Radford et al., 2019), that contains private information such as address, phone
number, name and so on; Zhu et al. (2019) shows that attackers can recover both the image input and the
label from gradients of ResNet (He et al., 2016) trained on CIFAR100 (Krizhevsky et al., 2009), SVHN
(Netzer et al., 2011), and LFW (Huang et al., 2008).
To protect against the privacy risk rigorously, the differential privacy (DP) is widely applied in various
deep learning tasks (Abadi et al., 2016; McMahan et al., 2017; Bu et al., 2020; Nori et al., 2021; Li et al.,
2021), including but not limited to computer vision, natural language processing, recommendation system,
federated learning and so on. At high level, the privacy is protected via DP optimizers such as DP-SGD and
DP-Adam, while allowing the models to remain highly accurate. In other words, the privacy concerns have
been largely alleviated by switching from regular optimizers to DP ones.
∗Equal contribution.
1An equally important concern from the security community is that, many models such as deep neural
networks are known to be vulnerable against adversarial attacks. This robustness risk can be severe when
the attackers can successfully fool models to make the wrong prediction, through modifying the input data
by a negligible amount. An example from Engstrom et al. (2019) shows that a strong ResNet50 trained on
ImageNet (Deng et al., 2009) with 76.13% accuracy can degrade to 0.00% accuracy, even if the input image
is merely perturbed by 4/255 at each pixel.
However, at the intersection of these two concerns, previous works have empirically observed an upsetting
privacy-robustness tradeoff under some scenarios, implying the implausibility of achieving both robustness
and privacy at the same time. In Song et al. (2019) and Mejia et al. (2019), adversarially trained models are
shown to be more vulnerable to privacy attacks, such as the membership inference attack, than naturally
trained models. In Tursynbek et al. (2020) and Boenisch et al. (2021), DP trained models were more
vulnerable to robustness attacks than naturally trained models on MNIST and CIFAR10. This leads to the
following concern:
Does DP optimization necessarily lead to less adversarially robust models?
On the contrary, we show that DP models can be adversarially robust, sometimes even more robust than the
naturally trained models. Indeed, we illustrate that DP models can be Pareto optimal, so that any model
with higher accuracy than DP ones must have worse robustness. We observe that:
1. DP training itself does not worsen adversarial robustness in comparison to the natural training.
2. The robustness is largely affected by the DP optimization hyper-parameters (R,η), where different hyper-
parameters are equally privacy-preserving but significantly different in accuracy and robustness.
3. DP optimization hyper-parameters (R,η)achieving the best accuracy (which can be much less robust
than the natural training) is different to those achieving the best robustness.
4. The robustness is less affected by the privacy budget (e.g. Table 4 and Table 6). Even when we consider
non-DP optimization (i.e. ϵ=∞, no noise but with clipping), the models could be comparably or more
robust than naturally trained models (see the blue boxplots in Figure 3).
Insharpcontrasttotheempiricalnatureofpreviousarts,weenhanceourunderstandingabouttheadversarial
robustness of DP models from a theoretical angle. Our analysis shows that DP classifiers without adversarial
training may in fact be the most adversarially robust classifier. Motivated by our theoretical analysis, we
claim that the hyperparameter tuning is vital to successfully learning a robust and private model, where
the optimal choice is to use small clipping norms and large learning rates. This is interesting as such a
hyperparameter choice is also observed to be the most effective in learning highly accurate models under
DP (Li et al., 2021; Kurakin et al., 2022; De et al., 2022). In fact, using a small clipping norm is equivalent
to normalizing all per-sample gradients, which allows a clear demonstration in Table 1 that the optimal
hyperparameter for natural accuracy is different to that for robust accuracy (only ηis present because Ris
absorbed in the automatic DP-SGD (Bu et al., 2023b)).
Table 1: Robust and natural accuracy are achieved by different hyperparameter ηon CIFAR10. Same setting
as in Tramer & Boneh (2020), under (ϵ,δ) =(2,1e-5) and attacked by 20 steps of l∞(2/255)PGD.
DPlearning rate η 2−82−72−62−52−42−32−22−1
natural accuracy 84.36 87.33 89.45 90.76 91.76 92.50 92.54 92.70
robust accuracy 75.45 78.92 81.03 80.96 78.87 72.77 58.29 26.97
non-DPlearning rate η 2−52−42−32−22−1202122
natural accuracy 94.23 94.31 94.38 94.46 94.60 94.39 94.21 93.84
robust accuracy 79.3279.57 74.18 66.20 55.87 46.00 41.75 45.21
2Additionally, we advocate pretraining the model and selecting proper optimizers for DP training, which
allow us to max out the performance on MNIST (LeCun et al., 1998), Fashion MNIST(Xiao et al., 2017),
CIFAR10(Krizhevsky et al., 2009), and CelebA(Liu et al., 2015).
Remark 1.Most of this work does not use adversarial training (except in Table 2 and Table 3) and should
be distinguished from the certified robustness (Lecuyer et al., 2019), which does not guarantee DP in a
per-sample sense rather than use the mathematical tools from DP in a per-pixel way.
2 Preliminaries
Notation. We usef:X →Y to denote the model mapping from data space Xto label spaceY. We
denote the datapoints as {xi}∈Rdand the labels as {yi}, following i.i.d. from the distributions xandy
respectively. We denote the gradient of the i-th sample at step tasgt(xi,yi;w,b), where wis the weights
andbis the bias of model f.
To start, we introduce the definition of DP, particularly the (ϵ,δ)-DP (Dwork et al., 2014).
Definition 1. A randomized algorithm Mis(ε,δ)-DP if for any neighboring datasets S,S′that differ by
one arbitrary sample, and for any event E, it holds that
P[M(S)∈E]⩽eεP[M(S′)∈E] +δ. (1)
In words, DP guarantees in the worst case that adding or removing one single datapoint (xi,yi)does not
affect the model much, as quantified by the small constants (ϵ,δ). Therefore DP limits the information
possibly leaked from such datapoint.
In deep learning, DP is guaranteed by privatizing the gradient in two steps: (1) the per-sample gradient
clipping1(specified by the clipping norm R, to bound the sensitivity of/summationtextgt(xi)); (2) the random noising
(specified by the noise multiplier σDP, to randomize the outcome so that each sample’s contribution is
indistinguishable; σDPcan be determined by the privacy accountants Dwork et al. (2014); Abadi et al.
(2016)). From an algorithmic viewpoint, DP training simply applies any optimizer on the private gradients
instead of on the regular gradients.
Non-DP training on regular gradient:/summationtext
igt(xi,yi) (2)
DP training on private gradient:/summationtext
iCR(gt(xi,yi)) +σDPR·N(0,I) (3)
We are interested in the adversarial robustness of models trained by DP optimizers. To be sure, we consider
the adversarially robust classification error and the natural classification error as
Rγ(f) :=P(∃||p||∞<γ,s.t.f(x+p)̸=y),R0(f) :=P(f(x)̸=y). (4)
wherep∈Rdis the adversarial perturbation, γis the attack magnitude, and l∞(andl2) attack is considered.
Notice that when γ= 0, the robust error in (4) reduces to the natural error.
3 Theoretical Analysis on Linear Classifiers
To theoretically understand the adversarial robustness of DP learning, we study the robustness of the DP
and non-DP linear models on a binary classification problem. We consider a mixed Gaussian distribution,
where the positive class y= +1has a larger variance (i.e. it is more difficult to be classified correctly2) than
the negative class y=−1:
x∼/braceleftbiggN/parenleftbig
θd,K2σ2Id/parenrightbig
ify= +1
N/parenleftbig
−θd,σ2Id/parenrightbig
ify=−1(5)
1We use CR(gt(xi)) := gt·min{R/||gt||2,1}as in Abadi et al. (2016) to denote the gradient clipping, after which each
per-sample gradient has norm ≤R. Note that in Table 1 we use the automatic clipping such that CR(gt(xi)) := gt/||gt||2.
2The fact that larger variance indicates lower intra-class accuracy is proven by Xu et al. (2021, Theorem 1).
3whereyunif∼ {− 1,+1},θd= (θ,···,θ)∈Rd,σ>0andK > 1.
−2−1 0 1 2−2−1012Adversarial
Natural
DP
−1.5−1.0−0.5 0.0 0.5−1.50−1.25−1.00−0.75−0.50−0.250.000.250.50
Adversarial
Natural
DP
Figure 1: Decision boundaries of linear classifiers for (5), K= 4,σ= 0.2,θ= 1.
The setting3in (5) is analyzable because of the data symmetry along the diagonal axis Ex1=···=Exd,
as illustrated in Figure 1. We will show that this symmetry leads to an explicit decision hyperplane of
linear classifiers in Theorem 1, which further characterizes the strongest adversarial perturbation p∗≡
arg sup∥p∥∞<γP(f(x+p)̸=y)explicitly.
In the following analysis, we focus on the linear classifiers f(x;w,b) =sign(/summationtextd
j=1wjxj+b)with weights wj
and biasb.
Remark2.Within the family of linear classifiers, by the symmetry of data in (5), it can be rigorously shown
by (6) that the optimal weights with respect to the natural and robust errors are always w1=···=wd,
which matches our empirical calculation on DP models. That is, the weights do not distinguish between the
robust and natural models. Consequently, the key to the adversarial robustness lies in the intercept b, which
is analyzed in the subsequent sections.
3.1 Optimal Robust and Natural Linear Classifiers
We start by reviewing the robust error of robust classifier and the explicit formula of its intercept.
Theorem 1 (Extended from Theorem 2 in Xu et al. (2021)) .For data distribution (x,y)in Equation (5)
and under the γattack magnitude, we define the optimal robust linear classifier as
fγ= arg min
fis linearP(∃||p||∞<γ,s.t.f(x+p)̸=y) = arg min
fis linearRγ(f).
The optimal robust error is
Rγ(fγ) =1
2Φ/parenleftig
B(K,γ)−K/radicalbig
B(K,γ)2+q(K)/parenrightig
+1
2Φ/parenleftig
−KB(K,γ) +/radicalbig
B(K,γ)2+q(K)/parenrightig
,
where Φis the cumulative distribution function of standard normal, B(K,γ) =2
K2−1√
d(θ−γ)
σandq(K) =
2 logK
K2−1. Furthermore, by the symmetry of the data distribution, we have
1,···,1,bγ= arg min
w,bRγ(f(·;w,b)), (6)
bγ=K2+ 1
K2−1d(θ−γ)−K/radicaligg
4d2(θ−γ)2
(K2−1)2+dσ2q(K). (7)
3This setting is also studied in Xu et al. (2021), which focuses on the robustness-fairness tradeoff, and thus is different to
our interest.
4Theorem 1 gives the closed form of the optimal robust classifier fγ, or equivalently its intercept bγ, and the
optimal robust error. The special case of natural error can be easily recovered by setting γ= 0:
1,···,1,b0= arg min
w,bR0(f(·;w,b)).
2
 1
 0 1 2 3
b102
101
b00(f0)
 b0.20.2(f0.2)
b0.40.4(f0.4)
=0
=0.2
=0.4
Figure 2: Intercepts and robust/natural accuracy
under (5), with K= 4,σ= 0.2,θ= 1.We know for sure from Theorem 1 that there exists a
tradeoff between robustness and accuracy: it is impos-
sible for the natural classifier f0to be optimally robust
or the robust classifier fγto be optimally accurate, since
b0̸=bγ(c.f. Figure 2).
Fact 1.bγin(7)is strictly decreasing in γ, ranging
fromb0to−∞.
Proof of Fact 1. Proof 5 in Xu et al. (2021) shows that
dbγ
dγ≤−K−1
K+1d < 0, thusbγis strictly decreasing in γ.
Therefore, the range of bγis(b∞,b0]. Finally, we note
thatbγ<K2+1
K2−1(θ−γ), henceb∞=−∞.
3.2 Adversarially Robust Errors of Private Linear classifiers
Now we analyze the robust error of DP classifiers, which requires a different analysis from Theorem 1 because
⟨1⟩DP modifies the optimization instead of the objective function, unlike Rγwhich modifies the natural
errorR0;⟨2⟩consequently, deriving the DP parameters wandbis much more difficult: we need to solve
arg minw,bRγ(f(·;w,b))under the additional constraint (imposed by DP) that (3) equals 0 in expectation.
While it is possible to simplify the problem, for example, by analyzing the deterministic gradient flow Bu
et al. (2023a) or by formulating the actual objective that DP-SGD optimizes Song et al. (2021), this is
beyond the scope of this work.
We consider a specific linear classifier f(·;1,b)where only the intercept bis learned and privatized4. This is
known as the bias term fine-tuning (Zaken et al., 2022; Bu et al., 2022b), a popular approach in fine-tuning
the DP or standard neural networks.
For this classifier and any b, the robust error ( γ̸= 0) and the natural error ( γ= 0) are:
Rγ(f) =P(∃∥p∥∞≤γs.t.f(x+p)̸=y) = max
∥p∥∞≤γP(f(x+p)̸=y)
=1
2P(f(x+γd)̸=−1|y=−1) +1
2P(f(x−γd)̸= +1|y= +1)
=1
2P/parenleftiggd/summationdisplay
i=1wj(xj+γ) +b>0|y=−1/parenrightigg
+1
2P/parenleftiggd/summationdisplay
i=1wj(xj−γ) +b<0|y= +1/parenrightigg
=1
2Φ/parenleftigg
−√
d(θ−γ)
σ+1√
dσ·b/parenrightigg
+1
2Φ/parenleftigg
−√
d(θ−γ)
Kσ−1
K√
dσ·b/parenrightigg
(8)
whereγd≡(γ,···,γ). With (8), we can analyze the robust and natural errors for any intercept b(private
or not, robust or natural) and any attack magnitude γ.
Our next result answers the following question: fixing a DP classifier fDP:=f(·;1,bDP), or equivalently its
interceptbDP, under which attack magnitude is the classifier robust? We show that, although bDPis not
available in the closed form, it is possible for some attack magnitude γ∗thatbDP=bγ∗, and thus the DP
classifier is the most robust classifier among all.
4Note that DP is only required on trainable parameters that are learned from data; otherwise no data privacy can be leaked.
Therefore this specific classifier is guaranteed to be DP.
5Theorem 2. For data distribution (x,y)in Equation (5)and for any bDP< b0, there exists γ∗>0such
thatbγ∗=bDP, and therefore
min
fis linearRγ∗(f)≡Rγ∗(fγ∗) =Rγ∗(fDP).
Proof of Theorem 2. ByFact1,bγ−bDPisdecreasingin γ, rangingfrom b0−bDPto−∞. Bytheintermediate
value theorem, there exists γ∗>0such thatbγ∗=bDP, i.e.fDP=fγ∗(·;1,bγ∗).
By Theorem 2, as long as the DP intercept is sufficiently small, the DP classifier must be the most robust
under some attack magnitude, among all linear classifiers. We visualize in Figure 3 at γ∗= 0.075, that
indeedbDP≈bγ∗(grey solid line).
0.1 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0
R0.951.001.051.101.151.201.25
b0
bγ,γ= 0.1
bγ,γ= 0.075
bDP,σDP= 0
bDP,σDP= 1
Figure 3: Intercepts (decision boundaries) for (5), same setup as Figure 2. For DP classifiers, we use DP-SGD
withη= 8, epochs=50, batch size=1000, sample size=10000, (ϵ,δ) = (15,1e−4).
To validate the condition of Theorem 2, we now demonstrate the achievability of bDP<b0as a result of the
per-sample gradient clipping in DP optimizers. In words, we show that the robust intercept is stationary by
the DP gradient descent but not so by the regular gradient descent.
We consider the above setting with an attack magnitude γandσDP= 0. We temporarily ignore the noise
because it only adds variance to the gradient but does not affect the mean (see the blue and orange boxes
in Figure 3), and when learning rate is small, σDPhas little effect on convergence (Bu et al., 2023a).
In DP gradient descent, if the clipping norm Ris sufficiently small, then each per-sample gradient
gt(xi,yi;1,bγ)has the same magnitude Rafter clipping. Therefore the positive samples, pushing the inter-
ceptbto increase, can balance with the negative ones that pull bto descrease. Thus b=bγis a stationary
point learnable by DP training, even though it is smaller than b0by Fact 1. However, in the non-DP gradient
descent,bγis not stationary. This is because the positive class gradient/summationtext
igt(xi,+1;1,bγ)is larger than
the negative class gradient/summationtext
igt(xi,−1;1,bγ), so as to push the decision boundary bγtowardsb0, where
the natural classifier f0is defined. We visualize our analysis in Figure 4.
Next, suppose we only require the DP classifier to be more robust than the natural classifier, without
requiring it to be the most robust among all linear classifiers. We can answer the question: fixing the attack
magnitude γ, under which condition is fDPmore robust than the natural classifier f0?
6−0.100−0.075−0.050−0.025 0.000 0.025 0.050 0.075 0.100
CR(g(xi,yi;bDP))100101102103Countyi= +1
yi=−1
−1.00−0.75−0.50−0.25 0.00 0.25 0.50 0.75 1.00
g(xi,yi;bDP)100101102103Countyi= +1
yi=−1Figure 4: Distribution of gradient with clipping (left, which is balanced) and without clipping (right, which
is unbalanced) of linear classifiers for (5), K= 4,σ= 0.2,θ= 1, and clipping norm R= 0.1.
Theorem3. Fixing the attack magnitude γ, if data distribution in Equation (5)satisfiesK2+1
2Kγ <|θ−γ|+|θ|,
then whenever bγ<bDP<b0, we have
min
fis linearRγ(f)≡Rγ(fγ)<Rγ(fDP)<Rγ(f0).
Furthermore, any intercept bwith better natural accuracy than bDPmust have worse robust accuracy:
R0(f)<R0(fDP) =⇒Rγ(f)>Rγ(fDP).
Theorem 3 shows that, under some conditions, DP models are more robust than natural models, and cannot
be dominated in the Pareto optimal sense. That is, DP linear classifier can be Pareto optimal in terms of
the robust and natural accuracy. We visualize the premise bγ<bDP<b0in Figure 3 as well as in Figure 1,
and the resultRγ(fγ)<Rγ(fDP)<Rγ(f0)in Figure 2.
We emphasize that, our results on the l∞attacks is generally extendable to l2attacks. Put differently, we
show that DP models can be adversarially robust and Pareto optimal under both l∞andl2attacks5.
Corollary 1 (Extension to l2attacks).All theorems hold for l2attacks by changing γ→γ/√
d.
Remark 3.Theorem 2 gives sufficient and necessary condition for the DP classifier to be more robust than
all classifiers at one attack magnitude γ∗; Theorem 3 gives sufficient but not necessary condition for the DP
classifier to be more robust than one classifier (the natural one) at many attack magnitudes.
4 Training Private and Robust Neural Networks
In this section, we extend our investigation beyond the linear classifiers in Theorem 1, Theorem 2, and
Theorem 3, and study the robustness of DP neural networks. We emphasize that several state-of-the-art
(SOTA) advances are actually achieved by linear classifiers within the deep neural networks (Mehta et al.,
2022; Tramer & Boneh, 2020), i.e. by finetuning only the last linear layer of Wide ResNet, SimCLR, and
vision transformers. Therefore these advances fall in the same setting as our theorems, although the neural
networks are non-linear models in general.
By experimenting with real datasets MNIST, CIFAR10 and CelebA, we corroborate our insights gained
from theoretically analyzing the linear models and empirically show that the DP neural networks can be
adversarially robust in practice (despite being much more challenging to analyze). We use one Nvidia GTX
1080Ti GPU and the Renyi privacy accountant to calculate the privacy loss.
5In DP deep learning, the clipping is on the gradient level under l2norm (see Footnote 1), regardless of the lnorm in
adversarial attacks, which are on the sample level, i.e. on xi.
7In summary, we have three key observations.
1. By selecting the hyper-parameters carefully, we can remain exactly the same level of DP but vastly
stronger robustness. Especially, we visualize the distinct landscapes of robust accuracy and natural
accuracy over (R,η), which also depend on the choice of optimizers (see Appendix E).
2. We observe a privacy-accuracy-robustness tradeoff, showing that DP models is Pareto optimal (with or
without pre-training), thus extending Theorem 3 to deep learning.
3. The robustness of DP models holds for general attacks, including single-step or multi-step (FGSM v.s.
PGD), single method or ensemble (PGD v.s. APGD), and l∞orl2.
Remark 4.Our analysis also implies that DP models can be resilient to data poisoning attacks, since
adversarial examples can serve as strong data poisons (Fowl et al., 2021). Such resilience is empirically
observed in (Yang et al., 2022; Hong et al., 2020).
4.1 Hyper-parameters are keys to robustness
In DP deep learning, the training hyper-parameters can be divided into two categories: some are related to
the privacy accounting, including the batch size B, the noise multiplier σDP, the number of iterations T; the
others are only related to the optimization but not to the privacy, including the clipping norm Rand the
learning rate η. That is, changing (R,η)can influence the accuracy and the robustness without affecting the
DP guarantee.
On one hand, Rhas to be small to achieve SOTA natural accuracy. Large models such as ResNet and GPT2
are optimally trained at R < 1, even though the gradient’s dimension is of hundreds of millions Kurakin
et al. (2022); Li et al. (2021); Klause et al. (2022); Mehta et al. (2022). In fact, Bu et al. (2023b) adopts
an infinitely small R= 0+to achieve SOTA results, essentially applying per-sample gradient normalization
instead of the clipping.
On the other hand, DP training empirically benefits from large learning rate, usually 10 times larger than
the non-DP training. This pattern is observed for DP-Adam (Li et al., 2021, Figure 4) and for DP-SGD
(Kurakin et al., 2022) over text and image datasets.
Interestingly, we also observe such choice of (R,η)performs strongly in the adversarial robustness context
(though not the same hyper-parameters). By the ablation study in Figure 5 for CIFAR10 and in Appendix B
for MNIST, Fashion MNIST and CelebA, it is clear that robust accuracy and natural accuracy have distinc-
tively different landscapes over (R,η). We observe that the optimal (R,η)should be carefully selected along
the diagonal ridge for DP-SGD to obtain high robust and high natural accuracy. Otherwise, even small
deviation can lead to a sharp drop in the robustness, despite that the natural accuracy may remain similar
(see upper right corner of 2D plots in Figure 5).
Remark 5.From Figure 5 (see also Figure 7, Figure 8 and Figure 9 in the appendix), it is empirically
sufficient to set R≈0+and to only tune the learning rate ηfor both robust and natural accuracy, when
using DP optimizers such as DP-SGD, DP-Adam, DP-RMSprop and DP-Adagrad. This approach, termed as
automatic clipping by Bu et al. (2023b), reduces the 2-dimension hyperparameter search to a much cheaper
1-dimension search (c.f. Table 1).
8R
22202−22−4
η23
21
2−1
2−3Robust Accuracy %1020304050607080
2−52−42−32−22−1202122
R23
22
21
20
2−1
2−2
2−3
2−4η58.29 26.69 2.58 0.76 7.28 45.84 68.79 79.77
72.97 55.32 20.97 2.14 0.57 6.18 43.59 68.52
78.87 71.57 51.69 16.58 1.48 0.72 7.22 43.33
80.97 78.64 69.94 47.55 11.50 0.92 0.39 6.55
81.04 80.70 78.00 68.16 42.73 7.15 0.45 0.36
78.95 80.96 80.60 77.06 65.77 36.55 4.20 0.32
75.54 79.41 81.03 80.56 75.89 62.48 28.70 2.40
73.32 75.93 79.66 81.04 80.20 75.05 57.46 19.55
NonDP46.55
45.21
41.75
46.00
55.87
66.20
74.18
79.57
1020304050607080
R
22202−22−4
η23
21
2−1
2−3Natural Accuracy %
8486889092
2−52−42−32−22−1202122
R23
22
21
20
2−1
2−2
2−3
2−4η92.69 92.43 91.32 89.74 87.71 88.04 86.85 87.56
92.42 92.91 92.41 90.67 89.05 88.25 87.52 87.00
91.76 92.41 92.72 92.36 90.17 88.97 88.18 87.88
90.67 91.76 92.45 92.85 91.88 90.08 88.68 87.57
89.44 90.90 91.94 92.32 92.59 91.95 89.72 88.30
87.31 89.61 90.98 91.98 92.57 92.46 91.41 89.50
84.26 87.55 89.71 91.11 91.97 92.67 92.43 90.78
83.42 84.70 87.81 89.86 91.27 92.07 92.53 92.28
NonDP93.77
93.84
94.21
94.39
94.60
94.46
94.38
94.31
 8486889092Figure 5: Robust and natural accuracy by ηandRon CIFAR10. We use the same setting as in Tramer &
Boneh (2020): pretraining SimCLR on ImageNet and then privately training using DP-SGD with momen-
tum=0.9, under (ϵ,δ) = (2,1e−5)and attacked by 20 steps of l∞(2/255)PGD.
Our ablation study demonstrates that the DP neural network, with 81.04%robust accuracy and 89.86%
natural accuracy, can be more robust than the most robust of naturally trained networks ( 79.59%robust
accuracy and 94.31%natural accuracy). If we trade some robustness for the natural accuracy, we can achieve
the same level of robustness (80.20%)at 91.27% natural accuracy, thus closing the gap between the natural
accuracy of DP and non-DP models without sacrificing the robustness.
While Figure 5 presents the result of a single attack magnitude, we further study the influence of hyper-
parameters under different attack magnitudes, with and without the adversarial training. We illustrate on
CIFAR10 the l∞attack performance in Table 2 and the l2one in Table 3.
Table 2: Natural and robust accuracy of SimCLRv2 (Chen et al., 2020) and ResNet50 (Engstrom et al.,
2019) on CIFAR10 under 20 steps l∞PGD attack. Here cyan columns are adversarial training and white
columns are natural training. Adversarial training and robusthyper-parameters are obtained by grid search
overηandRagainstl∞(2/255), andnaturalhyper-parameters are adopted from Tramer & Boneh (2020).
SimCLRv2 pre-trained on unlabelled ImageNet ResNet50
DP DP DP DP DP DP DP DP DP Non-DP Non-DP Non-DP Non-DP Non-DP
attackϵ= 2ϵ= 2ϵ= 2ϵ= 4ϵ= 4ϵ= 4ϵ= 8ϵ= 8ϵ= 8ϵ=∞ϵ=∞ϵ=∞ϵ=∞ϵ=∞
magnitude adv2/255robust accurate adv2/255robust accurate adv2/255robust accurate adv2/255robust accurate adv8/255accurate
γ= 090.46% 89.69% 92.87%91.12% 90.91% 93.41%91.70% 91.22% 93.64%93.42% 94.29% 94.55% 87.03% 95.25%
γ=2/25583.83% 81.05% 33.21%85.28% 82.53% 57.80%86.12% 83.02% 68.90%89.07% 79.79% 59.56% – –
γ=4/25575.56% 68.85% 0.16%77.73% 70.21% 9.69%78.62% 71.08% 28.09%83.07% 53.56% 15.99% – –
γ=8/25553.61% 39.63% 0.00%56.90% 38.39% 0.00%57.84% 39.28% 0.01%66.99% 8.14% 0.00% 53.49% 0.00%
γ=16/2558.05% 1.20% 0.00%10.30% 0.65% 0.00%11.31% 0.91% 0.00%20.67% 0.00% 0.00% 18.13% 0.00%
9Table 3: Natural and robust accuracy of SimCLRv2 (Chen et al., 2020) and ResNet50 (Engstrom et al.,
2019) on CIFAR10 under 20 steps l2PGD attack. Here cyan columns are adversarial training and white
columns are natural training. Adversarial training and robusthyper-parameters are obtained by grid search
overηandRagainstl2(0.25), andnaturalhyper-parameters are adopted from Tramer & Boneh (2020).
SimCLRv2 pre-trained on unlabelled ImageNet ResNet50
DP DP DP DP DP DP DP DP DP Non-DP Non-DP Non-DP Non-DP Non-DP
attackϵ= 2ϵ= 2ϵ= 2ϵ= 4ϵ= 4ϵ= 4ϵ= 8ϵ= 8ϵ= 8ϵ=∞ϵ=∞ϵ=∞ϵ=∞ϵ=∞
magnitude adv 0.25 robust accurate adv 0.25 robust accurate adv 0.25 robust accurate adv 0.25 robust accurate adv0.5accurate
γ= 091.07% 89.69% 92.87%91.19% 90.91% 93.41%91.87% 91.22% 93.64%93.88% 94.29% 94.55% 90.83% 95.25%
γ= 0.2582.69% 82.12% 59.91%83.69% 83.35% 74.10%84.27% 83.77% 79.03%85.20% 82.91% 72.63% 82.34% 8.66%
γ= 0.570.54% 71.99% 12.76%72.42% 72.79% 40.97%72.00% 73.08% 54.53%71.22% 63.32% 35.95% 70.17% 0.28%
γ= 1.038.57% 46.30% 9.49% 42.97% 44.46% 8.97% 40.17% 44.65% 9.68% 39.74% 33.34% 0.98% 40.47% 0.00%
We evaluate the robust and natural accuracy on the DP models in Tramer & Boneh (2020), considering two
groups of hyper-parameters: the naturalhyper-parameters reproduced from Tramer & Boneh (2020) that
has highest natural accuracy, and the robusthyper-parameters from a grid search on (R,η)for the highest
robust accuracy. From Table 2 and Table 3, we see that even under the same privacy constraint (including
the non-DP scenario), the robustness from different hyper-parameters can be fundamentally different. For
example, DP SimCLR at ϵ= 2can be either very robust ( ≈70%accuracy at γ= 4/255) or not robust at
all (0.16%accuracy). Consequently, our results may explain the mis-understanding of previous researches
by the improper choice of the hyper-parameters.
Scrutinizing the natural training with robust hyper-parameters, we see that, across all l∞attack magnitudes
γ={2/255,4/255,8/255,16/255}andl2onesγ={0,0.25,0.5,1.0}, DP SimCLR can be more robust than
the non-DP SimCLR, in fact comparable to the adversarially trained ResNet50 that is benchmarked in
Engstrom et al. (2019) and to the adversarially trained DP SimCLR. To be assured, we further demonstrate
that our choice of small Rand largeηis consistently robust on Fashion MNIST, CIFAR10 and CelebA in
Appendix C.
4.2 Pareto optimality on accuracy and robustness
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
Clean Accuracy0.00.10.20.30.40.50.60.7Robust AccuracyPre-train, DP-SGD, R= 0.1
Pre-train, DP-SGD, R= 2
Pre-train, SGD
No pre-train, DP-SGD, R= 0.1
No pre-train, DP-SGD, R= 2
No pre-train, SGD
Figure 6: Robust and natural accuracy on CIFAR10 at different iterations. Dots are CNN from Papernot
et al. (2020). Crosses are SimCLR from Tramer & Boneh (2020). See details in Appendix D.
In the standard non-DP regime, the tradeoff between the accuracy and the robustness is well-known (En-
gstrom et al., 2019). We extend the Pareto statement in Theorem 3 to DP deep learning, thus adding the
privacy dimension into the privacy-accuracy-robustness tradeoff. In Figure 6, we show that two strong DP
models on CIFAR10 (one pre-trained, the other not) achieve Pareto optimality with proper hyperparamters,
10and thus cannot be dominated by any natural classifiers. This can be observed by the fact that no green
cross (or dot) is to the top right of all blue crosses (or dots), meaning that any natural classifier may have
better robustness or higher accuracy, but not both. Therefore, our observation supports the claim that DP
neural networks can be Pareto optimal in terms of the robustness and the accuracy.
4.3 DP neural networks can be robust against general attacks
Following the claim in Section 4.1 that DP neural networks can be robust against l2andl∞PGD attacks,
we now demonstrate the transferability of DP neural networks’ robustness against different attacks.
Table 4: Natural and robust accuracy of FGSM(Goodfellow et al., 2014), BIM(Kurakin et al., 2018),
PGD∞(Madry et al., 2017), APGD ∞(Croce & Hein, 2020), PGD 2(Madry et al., 2017) and APGD 2(Croce &
Hein, 2020) on CIFAR10 under general adversarial attacks. Same model as Table 2 with the robusthyper-
parameters. See detailed attack settings in Appendix D.
Natural FGSM BIM PGD ∞APGD∞PGD 2APGD 2
DP ,ϵ= 2 89.86% 69.73% 68.85% 68.85% 68.85% 72.10% 71.97%
DP ,ϵ= 4 90.91% 71.13% 70.21% 70.21% 70.21% 72.79% 72.63%
DP ,ϵ= 8 91.22% 71.88% 71.08% 71.08% 71.06% 73.08% 72.99%
Non-DP 94.29% 56.24% 53.56% 53.56% 53.56% 63.32% 62.93%
This is interesting in the sense that DP mechanism does not intentionally defend against any adversarial
attack, while the adversarial training (Goodfellow et al., 2014) usually specifically targets a particular attack,
e.g. PGD attack is defensed by PGD adversarial training. In Table 4, we attack on the robust models from
Table 2, with the robusthyper-parameters. We consistently observe that DP models can be adversarially
robust and more so than the non-DP ones on MNIST/Fashion MNIST/CelebA in Appendix C, if the hyper-
parameters (R,η)are set properly.
4.4 Large scale experiments on CelebA face datasets
We further validate our claims on CelebA (Liu et al., 2015), a public high-resolution ( 178×218pixels) image
dataset, consisting of over 200,000 real human faces that are supposed to be protected against privacy risks.
We train ResNet18 (He et al. (2016), 11 million parameters) and Vision Transformer (ViT, Dosovitskiy et al.
(2020), 6 million parameters) with DP-RMSprop. Both models are implemented by Wightman (2019) and
pretrained on ImageNet. The experiment can be reproduced using the DP vision codebase Private Vision
by (Bu et al., 2022a).
Table 5: Natural and robust accuracy on CelebA with label ‘Smiling’, DP-RMSprop, under 20 steps l∞PGD
attack. Here the hyper-parameters have notbeen carefully searched for the best robustness. See details in
Appendix D.
ResNet18 ViT
attack DP DP DP Non-DP DP DP DP Non-DP
magnitude ϵ= 2ϵ= 4ϵ= 8ϵ=∞ϵ= 2ϵ= 4ϵ= 8ϵ=∞
γ= 0 80.10% 85.10% 88.48% 91.91% 92.30% 92.33% 92.09% 92.87%
γ=2/255 1.26% 0.47% 1.03% 1.19% 1.42% 2.02% 10.35% 0.08%
γ=4/255 0.01% 0.01% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00%
γ=8/255 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00%
γ=16/255 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00%
11Table 6: Natural and robust accuracy of FGSM(Goodfellow et al., 2014), BIM(Kurakin et al., 2018),
PGD∞(Madry et al., 2017), APGD ∞(Croce & Hein, 2020), PGD 2(Madry et al., 2017) and APGD 2(Croce
& Hein, 2020) on CelebA with label ‘Smiling’ under general adversarial attacks. Same ResNet18 as Table 5.
See detailed attack settings in Appendix D.
Natural FGSM BIM PGD ∞APGD∞PGD 2APGD 2
DP ,ϵ= 2 80.10% 24.47% 1.24% 1.26% 1.18% 47.02% 46.25%
DP ,ϵ= 4 85.10% 24.40% 0.45% 0.47% 0.41% 56.92% 56.09%
DP ,ϵ= 8 88.48% 29.32% 0.97% 1.03% 0.41% 57.40% 56.69%
Non-DP 91.91% 22.94% 1.09% 1.19% 0.68% 66.89% 66.13%
In Table 5 and Table 6, we observe that DP ResNet18 and ViT are almost as adversarially robust as their
non-DP counterparts, if not more robust. These observations are consistent with those of simpler models on
tiny images (c.f. Table 2 and Table 4). We note that unlike the linear classifiers on CIFAR10 in Table 2,
training all layers on CelebA in Table 5 are much more vulnerable even at γ= 2/255.
5 Discussion
Through the lens of theoretical analysis and extensive experiments, we have shown that differentially private
models can be adversarially robust and sometimes even more robust than the naturally trained models.
Moreover, DP models can be Pareto optimal in the sense that a more accurate natural model must be less
robust (see Theorem 3 and Figure 6). Our conclusion holds for various attacks with different magnitudes,
from linear models to large vision models, from grey-scale images to real face datasets, and from SGD to
adaptive optimizers. We not only are the first to reveal this possibility of achieving privacy and robustness
simultaneously, but also are the first to offer practical guidelines for such important goal (see Section 4).
To be concrete, we demonstrate that hyper-parameters – clipping norm Rand learning rate η– exert a lot
of influence on the robustness and accuracy, while remaining equally private. We hope that our insights
will encourage the practitioners to adopt techniques that protect the privacy and robustness in real-world
applications.
For future directions, a more thorough study of private and robustness learning is desirable, by extending
to language models, recommendation systems, and so on. We believe a new analysis when all parameters
are trainable will be challenging but enlightening. Especially, given that larger models are empirically more
accurate under the fixed privacy budget, it would be interesting to understand whether the robustness
also improves, or at least persists, with larger model sizes. Another direction is to further investigate the
adversarial training with DP optimizers, whose performance may go beyond our Pareto frontier (of the
robustness and the accuracy) that is based on the natural training.
References
MartinAbadi, AndyChu, IanGoodfellow, HBrendanMcMahan, IlyaMironov, KunalTalwar, andLiZhang.
Deep learning with differential privacy. In Proceedings of the 2016 ACM SIGSAC conference on computer
and communications security , pp. 308–318, 2016.
Franziska Boenisch, Philip Sperl, and Konstantin Böttinger. Gradient masking and the underestimated
robustness threats of differential privacy in deep learning. arXiv preprint arXiv:2105.07985 , 2021.
Zhiqi Bu, Jinshuo Dong, Qi Long, and Weijie J Su. Deep learning with gaussian differential privacy. Harvard
data science review , 2020(23), 2020.
Zhiqi Bu, Jialin Mao, and Shiyun Xu. Scalable and efficient training of large convolutional neural networks
with differential privacy. Advances in Neural Information Processing Systems , 35:38305–38318, 2022a.
12Zhiqi Bu, Yu-Xiang Wang, Sheng Zha, and George Karypis. Differentially private bias-term only fine-tuning
of foundation models. In Workshop on Trustworthy and Socially Responsible Machine Learning, NeurIPS
2022, 2022b.
Zhiqi Bu, Hua Wang, Zongyu Dai, and Qi Long. On the convergence and calibration of deep learning with
differential privacy. Transactions on Machine Learning Research , 2023a.
Zhiqi Bu, Yu-Xiang Wang, Sheng Zha, and George Karypis. Automatic clipping: Differentially private deep
learning made easier and stronger. Advances in Neural Information Processing Systems , 2023b.
Nicholas Carlini, Florian Tramer, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss, Katherine Lee, Adam
Roberts, Tom Brown, Dawn Song, Ulfar Erlingsson, et al. Extracting training data from large language
models. In 30th USENIX Security Symposium (USENIX Security 21) , pp. 2633–2650, 2021.
Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive
learning of visual representations. In International conference on machine learning , pp. 1597–1607. PMLR,
2020.
FrancescoCroceandMatthiasHein. Reliableevaluationofadversarialrobustnesswithanensembleofdiverse
parameter-free attacks. In International conference on machine learning , pp. 2206–2216. PMLR, 2020.
Soham De, Leonard Berrada, Jamie Hayes, Samuel L Smith, and Borja Balle. Unlocking high-accuracy
differentially private image classification through scale. arXiv preprint arXiv:2204.13650 , 2022.
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical
image database. In 2009 IEEE Conference on Computer Vision and Pattern Recognition , pp. 248–255,
2009. doi: 10.1109/CVPR.2009.5206848.
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Un-
terthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth
16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929 , 2020.
Cynthia Dwork, Aaron Roth, et al. The algorithmic foundations of differential privacy. Found. Trends Theor.
Comput. Sci. , 9(3-4):211–407, 2014.
Logan Engstrom, Andrew Ilyas, Hadi Salman, Shibani Santurkar, and Dimitris Tsipras. Robustness (python
library), 2019. URL https://github.com/MadryLab/robustness .
Liam Fowl, Micah Goldblum, Ping-yeh Chiang, Jonas Geiping, Wojciech Czaja, and Tom Goldstein. Adver-
sarialexamplesmakestrongpoisons. Advances in Neural Information Processing Systems , 34:30339–30351,
2021.
Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples.
arXiv preprint arXiv:1412.6572 , 2014.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In
Proceedings of the IEEE conference on computer vision and pattern recognition , pp. 770–778, 2016.
Sanghyun Hong, Varun Chandrasekaran, Yiğitcan Kaya, Tudor Dumitraş, and Nicolas Papernot. On the
effectiveness of mitigating data poisoning attacks with gradient shaping. arXiv preprint arXiv:2002.11497 ,
2020.
Gary B Huang, Marwan Mattar, Tamara Berg, and Eric Learned-Miller. Labeled faces in the wild: A
database forstudying face recognition in unconstrained environments. In Workshop on faces in’Real-
Life’Images: detection, alignment, and recognition , 2008.
Helena Klause, Alexander Ziller, Daniel Rueckert, Kerstin Hammernik, and Georgios Kaissis. Differentially
private training of residual networks with scale normalisation. arXiv preprint arXiv:2203.00324 , 2022.
13Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.
Alexey Kurakin, Ian J Goodfellow, and Samy Bengio. Adversarial examples in the physical world. In
Artificial intelligence safety and security , pp. 99–112. Chapman and Hall/CRC, 2018.
Alexey Kurakin, Steve Chien, Shuang Song, Roxana Geambasu, Andreas Terzis, and Abhradeep Thakurta.
Toward training at imagenet scale with differential privacy. arXiv preprint arXiv:2201.12328 , 2022.
Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to docu-
ment recognition. Proceedings of the IEEE , 86(11):2278–2324, 1998. doi: 10.1109/5.726791.
Mathias Lecuyer, Vaggelis Atlidakis, Roxana Geambasu, Daniel Hsu, and Suman Jana. Certified robustness
to adversarial examples with differential privacy. In 2019 IEEE Symposium on Security and Privacy (SP) ,
pp. 656–672. IEEE, 2019.
Xuechen Li, Florian Tramer, Percy Liang, and Tatsunori Hashimoto. Large language models can be strong
differentially private learners. arXiv preprint arXiv:2110.05679 , 2021.
Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild. In
Proceedings of International Conference on Computer Vision (ICCV) , December 2015.
Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards
deep learning models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083 , 2017.
H Brendan McMahan, Daniel Ramage, Kunal Talwar, and Li Zhang. Learning differentially private recurrent
language models. arXiv preprint arXiv:1710.06963 , 2017.
Harsh Mehta, Abhradeep Thakurta, Alexey Kurakin, and Ashok Cutkosky. Large scale transfer learning for
differentially private image classification. arXiv preprint arXiv:2205.02973 , 2022.
Felipe A Mejia, Paul Gamble, Zigfried Hampel-Arias, Michael Lomnitz, Nina Lopatina, Lucas Tindall, and
Maria Alejandra Barrios. Robust or private? adversarial training makes models more vulnerable to privacy
attacks. arXiv preprint arXiv:1906.06449 , 2019.
Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y Ng. Reading digits in
natural images with unsupervised feature learning. 2011.
Harsha Nori, Rich Caruana, Zhiqi Bu, Judy Hanwen Shen, and Janardhan Kulkarni. Accuracy, interpretabil-
ity, and differential privacy via explainable boosting. In International Conference on Machine Learning ,
pp. 8227–8237. PMLR, 2021.
Nicolas Papernot, Abhradeep Thakurta, Shuang Song, Steve Chien, and Ulfar Erlingsson. Tempered sigmoid
activations for deep learning with differential privacy. arXiv preprint arXiv:2007.14191 , pp. 10, 2020.
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models
are unsupervised multitask learners. OpenAI blog , 1(8):9, 2019.
Liwei Song, Reza Shokri, and Prateek Mittal. Privacy risks of securing machine learning models against
adversarial examples. In Proceedings of the 2019 ACM SIGSAC Conference on Computer and Communi-
cations Security , pp. 241–257, 2019.
Shuang Song, Thomas Steinke, Om Thakkar, and Abhradeep Thakurta. Evading the curse of dimensionality
in unconstrained private glms. In International Conference on Artificial Intelligence and Statistics , pp.
2638–2646. PMLR, 2021.
Tijmen Tieleman, Geoffrey Hinton, et al. Lecture 6.5-rmsprop: Divide the gradient by a running average of
its recent magnitude. COURSERA: Neural Networks for Machine Learning , 4(2):26–31, 2012.
14Florian Tramer and Dan Boneh. Differentially private learning needs better features (or much more data).
arXiv preprint arXiv:2011.11660 , 2020.
Nurislam Tursynbek, Aleksandr Petiushko, and Ivan Oseledets. Robustness threats of differential privacy.
arXiv preprint arXiv:2012.07828 , 2020.
Ross Wightman. Pytorch image models. https://github.com/rwightman/pytorch-image-models , 2019.
Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: a novel image dataset for benchmarking
machine learning algorithms, 2017.
Han Xu, Xiaorui Liu, Yaxin Li, Anil Jain, and Jiliang Tang. To be robust or to be fair: Towards fairness in
adversarial training. In International Conference on Machine Learning , pp. 11492–11501. PMLR, 2021.
Yu Yang, Tian Yu Liu, and Baharan Mirzasoleiman. Not all poisons are created equal: Robust training
against data poisoning. In International Conference on Machine Learning , pp. 25154–25165. PMLR, 2022.
Elad Ben Zaken, Yoav Goldberg, and Shauli Ravfogel. Bitfit: Simple parameter-efficient fine-tuning for
transformer-based masked language-models. In Proceedings of the 60th Annual Meeting of the Association
for Computational Linguistics (Volume 2: Short Papers) , pp. 1–9, 2022.
Ligeng Zhu, Zhijian Liu, and Song Han. Deep leakage from gradients. Advances in Neural Information
Processing Systems , 32, 2019.
15A Proofs
Proof of Theorem 1. [Extended from Xu et al. (2021)] By Xu et al. (2021, Lemma 2), according to the data
symmetry in (5), the optimal linear classifier has the form
1,···,1,bγ= arg min
w,bRγ(f(·;w,b)).
Recall that (8) proves that for such linear classifier, the robust error is
Rγ(f) =1
2Φ/parenleftigg
−√
d(θ−γ)
σ+1√
dσ·b/parenrightigg
+1
2Φ/parenleftigg
−√
d(θ−γ)
Kσ−1
K√
dσ·b/parenrightigg
.
where Φis the cumulative distribution function of standard normal.
The optimal bγto minimizeRγ(f)is achieved at the point that∂Rγ(f)
∂b= 0. Thus,bγsatisfies:
ϕ/parenleftigg
−√
d(θ−γ)
σ+bγ√
dσ/parenrightigg
·1√
dσ−ϕ/parenleftigg
−√
d(θ−γ)
Kσ−bγ
K√
dσ/parenrightigg
·1
K√
dσ= 0
whereϕis the probability density function of standard normal. This equals to
ϕ/parenleftigg
−√
d(θ−γ)
σ+bγ√
dσ/parenrightigg
=ϕ/parenleftigg
−√
d(θ−γ)
Kσ−bγ
K√
dσ/parenrightigg
/K
and
K=ϕ/parenleftigg
−√
d(θ−γ)
Kσ−bγ
K√
dσ/parenrightigg
/ϕ/parenleftigg
−√
d(θ−γ)
σ+bγ√
dσ/parenrightigg
=e−1
2/bracketleftig/parenleftig
−√
d(θ−γ)
Kσ−bγ
K√
dσ/parenrightig2
−/parenleftig
−√
d(θ−γ)
σ+bγ√
dσ/parenrightig2/bracketrightig
It is not hard to see
−2 logK=/parenleftigg
−√
d(θ−γ)
Kσ−bγ
K√
dσ/parenrightigg2
−/parenleftigg
−√
d(θ−γ)
σ+bγ√
dσ/parenrightigg2
which re-arranges to a quadratic equation
b2
γ1
dσ2(1−1
K2)−bγ2(θ−γ)
σ2(1 +1
K2) +d(θ−γ)2
σ2(1−1
K2) = 2 logK.
The solution is therefore explicit as
bγ=K2+ 1
K2−1d(θ−γ)−K/radicaligg
4d2(θ−γ)2
(K2−1)2+dσ2q(K),
whereq(K) =2 logK
K2−1which is a positive constant and only depends on K. By incorporating bγinto (8), we
can get the optimal robust error Rγ(fγ):
Rγ(fγ) =1
2Φ/parenleftig
B(K,γ)−K/radicalbig
B(K,γ)2+q(K)/parenrightig
+1
2Φ/parenleftig
−KB(K,γ) +/radicalbig
B(K,γ)2+q(K)/parenrightig
,
whereB(K,γ) =2
K2−1√
d(θ−γ)
σ.
16Proof of Theorem 3. We denote the two roots of∂Rγ(f(b))
∂b= 0asb+
γandb−
γ. Herebγ≡b−
γ. ClearlyRγ(b)
is increasing in (b−
γ,b+
γ). We hope to show b0∈(b−
γ,b+
γ)∀γ >0, so thatRγ(b)is also increasing in (b−
γ,b0).
Note their Equation (17)
Rγ(b) =1
2Φ(−√
d(θ−γ)
σ+1√
dσb) +1
2Φ(−√
d(θ−γ)
Kσ−1
K√
dσb)
Taking derivative w.r.t. b
∂Rγ(b)
∂b=1
2√
dσϕ(−√
d(θ−γ)
σ+1√
dσb)−1
2K√
dσϕ(−√
d(θ−γ)
Kσ−1
K√
dσb)
Setting this derivative to 0:
0 =Kϕ(−√
d(θ−γ)
σ+1√
dσb)−ϕ(−√
d(θ−γ)
Kσ−1
K√
dσb)
which means
ϕ(−√
d(θ−γ)
Kσ−1
K√
dσb)
ϕ(−√
d(θ−γ)
σ+1√
dσb)=K
Using the standard normal density ϕ(u) =e−u2/2andϕ(u)
ϕ(v)=e(v2−u2)/2, we have
(−√
d(θ−γ)
σ+1√
dσb)2−(−√
d(θ−γ)
Kσ−1
K√
dσb)2= 2 logK
=⇒K2(−d(θ−γ) +b)2−(−d(θ−γ)−b)2= 2dσ2K2logK
=⇒(K2−1)b2−2d(θ−γ)(K2+ 1)b+d2(θ−γ)2(K2−1)−2dσ2K2logK= 0
Byx=−b
2a±√
b2−4ac
2a=−b
2a±/radicalig
(b
2a)2−c
a, we know
b±
γ=K2+ 1
K2−1d(θ−γ)±/radicalbigg
(K2+ 1
K2−1d(θ−γ))2−d2(θ−γ)2+K2dσ2q(K)
=K2+ 1
K2−1d(θ−γ)±K/radicaligg
4d2(θ−γ)2
(K2−1)2+dσ2q(K)
We now derive the sufficient condition that b0<b+
γ:
K2+ 1
K2−1d(θ)−K/radicaligg
4d2(θ)2
(K2−1)2+dσ2q(K)<K2+ 1
K2−1d(θ−γ) +K/radicaligg
4d2(θ−γ)2
(K2−1)2+dσ2q(K).
This is equivalent to
K2+ 1
K2−1dγ <K/parenleftigg/radicaligg
4d2(θ−γ)2
(K2−1)2+dσ2q(K) +/radicaligg
4d2θ2
(K2−1)2+dσ2q(K)/parenrightigg
.
Therefore, it suffices to have
K2+ 1
2Kγ <|θ−γ|+|θ|
Finally, it is easy to see the Pareto statement R0(f)<R0(fDP)−→Rγ(f)>Rγ(fDP). A necessary but
not sufficient condition for R0(f)<R0(fDP)given thatb0>bDPisb>b DP, sinceb0is a minimizer which
meansR0is decreasing on the interval (−∞,b0). Similarly,Rγis increasing on the right of bγand thusb
has higher robust error.
17Proof of Corollary 1. We can characterize the robust errors based on l2attacks in a similar fashion to (8).
We notice that
Rγ(f) =P(∃∥p∥2≤ϵs.t.f(x+p)̸=y) = max
∥p∥2≤γP(f(x+p)̸=y)
=1
2P(f(x+γd/√
d)̸=−1|y=−1) +1
2P(f(x−γd/√
d)̸= +1|y= +1)
In short, the same analysis is in place except γ→γ/√
dwhen we switch from l∞tol2attacks.
B Ablation Studies
B.1 CelebA
R
22202−22−4
η23×10−2
2−1×10−2
2−5×10−2
2−9×10−2Robust Accuracy % 40506070
2−52−42−32−22−1202122
R23×10−2
21×10−2
2−1×10−2
2−3×10−2
2−5×10−2
2−7×10−2
2−9×10−2
2−11×10−2η61.81 61.80 61.46 61.80 61.84 62.04 61.85 39.40
43.79 45.24 46.56 45.22 37.09 37.24 30.40 39.25
45.58 47.98 43.38 40.69 33.85 34.99 34.79 34.89
66.58 63.98 68.29 65.34 63.40 57.75 60.47 54.00
73.52 73.65 74.13 74.23 74.46 76.56 70.59 74.55
71.65 72.22 72.84 72.52 72.08 72.38 72.80 73.10
62.01 66.75 67.65 66.87 68.65 66.34 68.51 68.35
61.80 61.80 61.80 61.80 61.80 61.80 61.80 61.80
NonDP61.80
61.80
29.36
30.80
41.02
64.17
75.70
75.00
40506070
R
22202−22−4
η23×10−2
2−1×10−2
2−5×10−2
2−9×10−2Natural Accuracy %
405060708090
2−52−42−32−22−1202122
R23×10−2
21×10−2
2−1×10−2
2−3×10−2
2−5×10−2
2−7×10−2
2−9×10−2
2−11×10−2η61.84 61.80 61.96 61.71 61.81 61.85 61.89 39.43
90.02 91.51 91.10 90.92 91.14 90.40 90.55 88.95
94.19 94.14 94.37 94.22 94.43 94.05 94.16 94.04
94.73 94.44 94.71 94.72 94.79 94.75 94.67 94.95
85.68 85.06 86.57 87.26 87.35 90.64 90.12 89.46
79.03 79.67 80.27 80.27 79.62 81.32 80.89 81.69
62.83 71.70 72.20 70.72 73.33 70.98 74.06 74.50
61.80 61.80 61.80 61.80 61.80 61.80 61.82 61.87
NonDP61.80
61.80
95.20
96.48
97.08
96.26
94.46
88.67
405060708090
Figure 7: Robust and natural accuracy of ηandRon CelebA with label ‘Male’. We train a 2-layer CNN
using DP-Adam and attack by l∞(2/255)PGD attack. Same as in Figure 13. Here ϵ= 2, batch size = 512,
epochs = 10.
180.1 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0
R3.2
2.8
2.4
2.0
1.6
1.2
0.8
0.4η75.26 61.24 48.25 35.11 39.16 63.06 41.72 68.83 63.11
77.18 60.40 49.46 37.62 33.04 50.30 60.34 63.07 72.65
75.82 65.83 48.84 39.96 41.80 63.08 68.24 63.02 74.83
74.65 69.00 46.80 42.33 33.79 39.63 41.20 35.84 83.88
74.68 72.38 58.27 46.04 45.38 39.31 42.20 63.08 63.07
73.84 75.45 63.03 51.81 49.39 44.78 40.37 24.14 55.11
72.69 74.42 74.12 64.44 50.63 47.57 42.98 41.07 41.50
71.04 76.10 75.36 73.29 71.60 65.54 59.82 54.74 49.15
NonDP63.09
63.09
63.09
63.09
63.09
63.09
63.09
63.09
304050607080
0.1 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0
R0.08
0.07
0.06
0.05
0.04
0.03
0.02
0.01η47.61 40.59 38.27 39.12 38.45 34.94 31.45 30.55 28.77
50.83 46.88 41.86 39.41 36.81 35.75 35.43 34.71 30.05
53.38 52.31 43.44 46.54 33.47 40.86 37.92 40.10 38.21
62.74 50.36 46.43 50.80 43.49 44.20 44.41 44.44 44.03
63.07 60.67 56.94 52.52 52.92 47.58 49.69 45.53 45.24
67.50 64.53 62.00 61.24 60.62 61.10 58.64 58.35 62.20
72.95 71.15 69.32 69.75 72.37 67.24 67.49 71.55 68.56
76.52 71.00 69.83 73.63 76.66 76.01 73.34 75.85 75.31
NonDP52.13
55.92
31.22
43.06
42.07
50.53
58.82
55.83
3040506070
0.1 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0
R0.32
0.28
0.24
0.2
0.16
0.12
0.08
0.04η76.11 58.86 36.99 34.30 40.92 34.96 34.40 32.77 39.26
75.15 63.11 43.21 40.51 40.61 38.24 42.91 42.48 43.35
74.86 66.45 42.91 39.01 36.86 35.10 36.53 38.11 37.71
74.85 66.13 51.21 44.10 40.67 31.79 33.95 36.18 39.32
74.60 72.11 54.90 47.46 38.49 37.29 38.35 37.55 36.56
74.07 74.88 61.95 53.72 41.74 38.45 37.68 32.41 39.55
72.22 77.09 72.26 60.75 51.75 42.59 37.28 37.86 35.31
70.81 75.00 75.79 73.89 71.52 65.86 60.88 53.31 48.91
NonDP63.09
63.09
63.09
63.09
63.09
63.09
32.91
39.71
40506070
0.01 0.02 0.05 0.1 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0
R0.004
0.0035
0.003
0.0025
0.002
0.0015
0.001
0.0005η49.86 48.40 43.62 42.10 40.91 43.34 38.98 36.59 34.33 35.61 32.76 42.35
45.52 46.98 44.07 45.92 42.00 37.99 39.54 38.85 35.68 37.62 34.47 32.84
53.61 49.32 48.83 52.91 41.86 45.16 40.62 38.27 38.26 40.72 36.38 35.08
52.98 53.22 55.29 53.60 46.95 47.67 42.85 40.55 36.72 35.44 45.44 42.50
63.19 63.42 57.93 56.91 52.92 46.23 47.59 45.88 41.22 40.94 43.25 42.88
68.94 69.54 62.40 66.66 61.51 58.72 58.66 55.02 53.30 54.79 52.48 53.97
71.89 72.30 66.92 69.38 66.96 67.83 66.38 66.99 66.97 65.62 64.67 67.60
74.51 73.98 75.93 76.30 73.90 75.25 75.94 76.01 73.93 75.50 76.39 76.14
NonDP63.09
32.31
34.80
28.46
24.70
31.77
30.07
30.38
354045505560657075Figure 8: Robust accuracy of CelebA with label ‘Male’ under different optimizer, trained with a 2-layer CNN
and attacked by l∞(2/255)PGD attack. Top left: SGD. Top right: Adagrad. Bottom left: SGD momentum.
Bottom right: Adam. Here ϵ= 2, batch size = 512, epochs = 10.
19R
22202−22−4
η29×10−4
25×10−4
21×10−4
2−3×10−4Robust Accuracy % 1020304050
2−52−42−32−22−1202122
R29×10−2
27×10−2
25×10−2
23×10−2
21×10−2
2−1×10−2
2−3×10−2
2−5×10−2η50.03 49.97 49.97 49.97 49.97 49.68 50.03 49.97
35.55 49.97 50.03 49.97 49.97 49.97 49.97 49.97
49.97 49.97 50.03 49.97 49.97 49.97 50.03 49.97
49.97 31.47 49.97 49.97 49.97 49.97 49.97 49.97
32.76 30.50 27.63 25.96 38.96 23.38 21.50 30.30
24.39 21.44 23.88 17.83 14.68 20.39 15.59 9.05
18.31 11.65 10.42 7.08 8.27 9.67 14.84 2.23
1.43 1.36 1.70 1.73 1.26 0.41 10.42 1.77
1020304050
R
22202−22−4
η29×10−4
25×10−4
21×10−4
2−3×10−4Natural Accuracy %
505560657075808590
2−52−42−32−22−1202122
R29×10−2
27×10−2
25×10−2
23×10−2
21×10−2
2−1×10−2
2−3×10−2
2−5×10−2η50.03 49.97 49.97 49.97 49.97 49.97 50.03 49.97
49.98 49.97 50.03 49.97 49.97 49.97 49.97 49.97
49.97 49.97 50.03 49.97 49.97 49.97 50.03 49.97
49.97 63.09 49.97 49.97 49.97 49.97 49.97 49.97
91.63 91.79 91.77 91.57 90.93 91.80 91.60 91.32
90.68 90.84 90.87 90.93 90.87 90.69 90.60 90.35
88.84 88.52 88.15 86.29 87.06 88.37 87.61 81.72
71.81 56.15 66.28 73.59 56.13 59.35 60.42 61.47
505560657075808590Figure 9: Robust and natural accuracy of ηandRon CelebA with label ‘Smiling’. We train ViT-tiny using
DP-RMSprop and attack by l∞(2/255)PGD attack. Here ϵ= 2, batch size = 1024, epoch = 1.
20B.2 CIFAR10
R
22202−22−4
η24
22
20
2−2Robust Accuracy %1020304050607080
2−52−42−32−22−1202122
R24
23
22
21
20
2−1
2−2
2−3η79.97 74.12 59.31 25.44 2.52 0.28 1.35 30.30
80.97 79.71 73.48 55.99 19.07 1.90 0.39 1.13
80.52 81.11 79.10 71.87 51.20 14.31 1.02 0.21
78.00 80.68 80.93 78.49 69.74 46.34 9.41 0.65
74.99 78.40 80.80 80.81 77.86 67.57 38.47 4.28
71.82 74.94 78.68 80.88 80.60 76.91 64.78 30.14
64.49 71.92 74.98 79.16 80.97 80.28 75.25 59.08
46.11 64.44 71.66 75.40 79.65 80.76 79.59 72.90
NonDP62.81
59.77
60.53
69.56
76.21
80.79
82.69
83.69
1020304050607080
R
22202−22−4
η24
22
20
2−2Natural Accuracy %828486889092
2−52−42−32−22−1202122
R24
23
22
21
20
2−1
2−2
2−3η91.33 92.22 92.82 92.58 90.88 88.96 88.03 87.69
90.31 91.48 92.28 92.76 92.28 90.94 88.78 87.55
88.75 90.36 91.63 92.48 92.85 91.90 89.69 88.44
86.52 88.95 90.72 91.81 92.26 92.77 91.45 89.72
83.94 86.78 89.20 90.72 91.75 92.55 92.31 91.18
83.12 83.95 87.13 89.46 90.88 92.04 92.61 92.34
82.11 83.35 84.08 87.40 89.65 91.07 91.99 92.55
80.71 82.50 83.22 84.46 87.75 89.79 91.18 91.90
NonDP93.79
94.35
94.02
94.49
94.44
94.20
93.60
92.87
828486889092
Figure 10: Robust and clean accuracy of ηandRon CIFAR10, transferred from SimCLRv2 pre-trained on
unlabelled ImageNet. We use DP-SGD and attack by l∞(2/255)PGD attack. Here ϵ= 2, batch size = 1024,
epochs = 50.
21B.3 MNIST
R
22202−22−4
η20
2−2
2−4
2−6Robust Accuracy %
010203040
2−52−42−32−22−1202122
R20
2−1
2−2
2−3
2−4
2−5
2−6
2−7η43.97 45.03 35.15 37.84 18.42 8.58 33.91 43.83
37.80 41.24 39.34 35.10 30.09 19.85 19.00 27.72
28.76 34.34 31.72 29.97 47.22 29.05 25.10 15.23
29.18 33.65 35.76 38.64 47.41 32.88 30.33 15.52
12.15 29.84 36.43 31.77 35.24 42.10 43.55 31.88
10.42 15.80 33.53 31.41 36.03 34.03 40.30 37.96
0.12 9.43 13.06 26.22 33.60 34.37 35.81 40.79
0.00 3.09 5.65 18.21 22.39 30.86 29.93 40.65
NonDP10.32
20.06
23.91
22.61
25.46
28.39
32.87
33.61
010203040
R
22202−22−4
η20
2−2
2−4
2−6Natural Accuracy %
102030405060708090
2−52−42−32−22−1202122
R20
2−1
2−2
2−3
2−4
2−5
2−6
2−7η92.84 95.42 96.35 97.72 97.99 97.55 94.37 91.77
82.19 92.40 95.48 96.81 97.74 97.69 97.58 94.56
64.52 84.33 92.55 95.12 97.03 97.46 97.84 97.11
64.14 74.40 85.87 93.20 95.69 96.55 97.59 97.77
42.94 65.79 73.92 86.05 92.22 95.36 97.03 97.60
14.58 40.15 62.00 74.27 87.03 91.69 95.66 96.80
11.09 14.27 45.24 59.27 75.49 89.84 93.90 95.37
8.02 10.25 25.35 42.29 54.01 69.89 88.86 93.83
NonDP10.32
99.16
99.02
98.63
98.74
98.47
97.65
95.96
102030405060708090
Figure 11: Robust and clean accuracy of ηandRon MNIST. We train the CNN from Tramer & Boneh
(2020) using DP-SGD and attack by l∞(32/255)PGD attack. Here ϵ= 2, batch size = 512, epochs = 40.
22B.4 Fashion MNIST
R
22202−22−42−6
η23
21
2−1
2−3
2−5Robust Accuracy %
1020304050607080
2−72−62−52−42−32−22−1202122
R23
22
21
20
2−1
2−2
2−3
2−4
2−5
2−6η75.67 78.76 80.09 76.81 75.45 76.00 74.20 57.69 62.66 50.62
73.21 76.68 79.29 79.62 76.65 75.63 75.83 74.65 71.03 58.28
70.76 73.28 75.64 78.92 79.92 76.69 75.88 75.26 74.21 67.01
66.13 70.86 72.74 76.36 79.53 79.88 76.81 74.59 75.53 73.73
63.08 69.07 71.15 73.18 76.77 79.17 79.35 75.96 74.69 74.29
51.76 60.54 65.95 71.07 73.84 76.65 79.18 79.59 76.37 72.61
44.61 54.49 57.15 66.42 71.36 73.06 76.81 79.47 79.90 74.63
44.26 52.57 54.89 61.37 69.65 70.96 74.30 77.40 79.85 79.63
13.05 32.26 48.79 53.87 58.35 68.97 71.42 73.99 77.50 79.74
8.3024.78 50.95 51.07 55.23 59.00 68.72 71.28 74.91 78.12
NonDP10.00
10.00
10.00
10.00
72.65
76.56
79.22
79.59
81.72
81.63
1020304050607080
R
22202−22−42−6
η23
21
2−1
2−3
2−5Natural Accuracy %
20304050607080
2−72−62−52−42−32−22−1202122
R23
22
21
20
2−1
2−2
2−3
2−4
2−5
2−6η78.54 82.44 85.42 85.59 83.80 80.69 75.21 59.38 49.08 38.12
75.43 80.05 83.02 84.96 85.53 83.60 79.54 76.58 67.87 54.29
73.16 75.43 78.45 82.68 85.20 86.02 82.71 81.12 75.69 67.10
68.37 73.23 75.40 79.89 83.08 85.29 85.72 83.65 80.22 76.13
65.00 71.32 73.26 75.81 80.67 83.23 85.37 85.72 83.17 79.37
53.54 62.79 68.46 73.31 76.74 80.14 83.43 85.29 85.51 82.21
47.67 55.67 59.21 68.58 73.59 75.34 80.03 83.85 86.21 85.35
48.52 54.66 56.25 63.43 71.89 73.05 76.91 81.14 84.09 86.34
15.74 35.68 51.15 55.62 60.26 71.04 73.59 76.82 80.88 84.32
13.87 26.05 53.61 53.30 57.08 60.91 70.80 73.94 77.96 81.92
NonDP10.00
10.00
10.00
10.00
86.75
89.41
90.12
89.00
88.78
87.35
20304050607080
Figure 12: Robust and clean accuracy of ηandRon Fashion MNIST. We train the CNN from Tramer &
Boneh (2020) using DP-SGD and attack by l∞(2/255)PGD attack. Here ϵ= 2, batch size = 2048, epochs
= 40.
23C More tables
Natural FGSM BIM PGD∞APGD∞PGD 2APGD 2
Non-DP 94.55% 18.71% 15.97% 15.96% 16.04% 35.95% 35.89%
DP ,ϵ= 2 92.73% 10.35% 0.03% 0.03% 0.03% 12.76% 12.68%
DP ,ϵ= 4 93.49% 30.10% 9.10% 9.09% 9.12% 40.97% 41.01%
DP ,ϵ= 8 93.74% 31.86% 28.08% 28.09% 28.09% 54.53% 54.54%
Table7: NaturalandrobustaccuracyofmodelstransferredfromunlabelledImageNetpre-trainedSIMCLRv2
on CIFAR10 under general adversarial attacks with γ∞= 4/255andγ2= 0.5. Attack steps are 20 if
applicable. Model hyper-parameters are directly adopted from Tramer & Boneh (2020) for highest natural
accuracy. DP models are trained using DP-SGD, R= 0.1,ηDP= 4, momentum = 0.9, batch size = 1024.
Non-DP models are trained using SGD with the same hyper-parameters except ηnon−DP= 0.4.
Non-DP DP DP DP
attack magnitude ϵ=∞ϵ= 2ϵ= 4ϵ= 8
γ= 0.0 99.24% 98.01% 98.32% 98.50%
γ= 0.25 97.57% 95.29% 95.94% 96.65%
γ= 0.5 93.32% 90.28% 91.71% 92.97%
γ= 1.0 66.58% 63.95% 73.32% 77.08%
γ= 2.0 36.28% 39.88% 51.48% 52.74%
Table 8: Robust accuracy on MNIST under 20 steps l2PGD attack. Model hyper-parameters are directly
adopted from Tramer & Boneh (2020) for highest natural accuracy. DP models are trained using DP-SGD,
R= 0.1,ηDP= 0.5, momentum = 0.9, batch size = 512. Non-DP models are trained using SGD with the
same hyper-parameters except ηnon−DP= 0.05.
Non-DP DP DP DP
attack magnitude ϵ=∞ϵ= 2ϵ= 4ϵ= 8
γ= 0.0 99.24% 98.01% 98.32% 98.50%
γ= 2/255 98.73% 97.12% 97.43% 97.84%
γ= 4/255 97.88% 95.78% 96.32% 97.13%
γ= 8/255 95.32% 92.31% 93.51% 94.74%
γ= 16/255 82.06% 77.67% 80.28% 85.82%
Table 9: Robust accuracy on MNIST under 20 steps l∞PGD attack. Model hyper-parameters are directly
adopted from Tramer & Boneh (2020) for highest natural accuracy. DP models are trained using DP-SGD,
R= 0.1,ηDP= 0.5, momentum = 0.9, batch size = 512. Non-DP models are trained using SGD with the
same hyper-parameters except ηnon−DP= 0.05.
24Natural FGSM BIM PGD∞APGD∞PGD 2APGD 2
Non-DP 99.24% 97.92% 97.88% 97.88% 97.77% 93.32% 93.27%
DP ,ϵ= 2 98.01% 95.89% 95.80% 95.79% 95.63% 90.28% 90.15%
DP ,ϵ= 4 98.32% 96.45% 96.32% 96.33% 96.27% 91.71% 91.68%
DP ,ϵ= 8 98.50% 97.19% 97.15% 97.15% 97.06% 92.97% 92.94%
Table 10: Natural and robust accuracy of CNN models on MNIST under general adversarial attacks with
γ∞= 4/255andγ2= 0.5. Attack steps are 20 if applicable. Model hyper-parameters are directly adopted
from Tramer & Boneh (2020) for highest natural accuracy. DP models are trained using DP-SGD, R= 0.1,
ηDP= 0.5, momentum = 0.9, batch size = 512. Non-DP models are trained using SGD with the same
hyper-parameters except ηnon−DP= 0.05.
Non-DP DP DP DP
attack magnitude ϵ=∞ϵ= 2ϵ= 4ϵ= 8
γ= 0.0 89.75% 85.95% 86.60% 86.74%
γ= 0.25 57.37% 69.24% 72.93% 75.35%
γ= 0.5 25.21% 46.09% 54.30% 59.23%
γ= 1.0 7.87% 16.77% 25.95% 29.08%
γ= 2.0 7.47% 11.77% 16.85% 17.00%
Table 11: Robust accuracy on Fashion MNIST under 20 steps l2PGD attack. Model hyper-parameters are
directly adopted from Tramer & Boneh (2020) for highest natural accuracy. DP models are trained using
DP-SGD,R= 0.1,ηDP= 4, momentum = 0.9, batch size = 2048. Non-DP models are trained using SGD
with the same hyper-parameters except ηnon−DP= 0.4.
Non-DP DP DP DP
attack magnitude ϵ=∞ϵ= 2ϵ= 4ϵ= 8
γ= 0.0 89.75% 85.95% 86.60% 86.74%
γ= 2/255 76.19% 78.29% 79.84% 81.47%
γ= 4/255 64.46% 69.75% 72.60% 74.72%
γ= 8/255 47.24% 54.62% 57.87% 60.52%
γ= 16/255 23.26% 28.51% 31.68% 30.90%
Table 12: Robust accuracy on Fashion MNIST under 20 steps l∞PGD attack. Model hyper-parameters are
directly adopted from Tramer & Boneh (2020) for highest natural accuracy. DP models are trained using
DP-SGD,R= 0.1,ηDP= 4, momentum = 0.9, batch size = 2048. Non-DP models are trained using SGD
with the same hyper-parameters except ηnon−DP= 0.4.
Natural FGSM BIM PGD∞APGD∞PGD 2APGD 2
Non-DP 89.75% 70.41% 64.56% 64.44% 53.41% 25.21% 23.13%
DP ,ϵ= 2 85.95% 72.11% 69.76% 69.71% 67.13% 46.09% 45.41%
DP ,ϵ= 4 86.60% 73.67% 72.68% 72.69% 70.84% 54.30% 53.92%
DP ,ϵ= 8 86.74% 75.45% 74.75% 74.74% 73.71% 59.23% 58.98%
Table 13: Natural and robust accuracy of CNN models on Fashion MNIST under general adversarial attacks
withγ∞= 4/255andγ2= 0.5. Attack steps are 20 if applicable. Model hyper-parameters are directly
adopted from Tramer & Boneh (2020) for highest natural accuracy. DP models are trained using DP-SGD,
R= 0.1,ηDP= 4, momentum = 0.9, batch size = 2048. Non-DP models are trained using SGD with the
same hyper-parameters except ηnon−DP= 0.4.
25Natural FGSM BIM PGD∞APGD∞PGD 2APGD 2
Non-DP 94.29% 14.48% 12.02% 12.00% 12.03% 31.36% 31.28%
DP ,ϵ= 2 92.73% 15.70% 1.59% 1.61% 1.62% 28.05% 28.06%
DP ,ϵ= 4 93.49% 30.89% 5.23% 5.27% 5.25% 35.96% 35.98%
DP ,ϵ= 8 93.74% 9.66% 4.30% 4.29% 4.31% 33.21% 33.23%
Table 14: Natural and robust accuracy of models transferred from unlabelled ImageNet pre-trained SIM-
CLRv2 on CIFAR10 under general adversarial attacks with γ∞= 4/255andγ2= 0.5. Attack steps are 20 if
applicable. Model in each row is the most accurate model obtained by simple grid search: Non-DP: η= 0.5;
DPϵ=2:η= 1.0,R= 0.25;DPϵ=4:η= 8,R= 0.0625,DPϵ=8:η= 0.5,R= 1.0. All models are trained using
SGD or DP-SGD, momentum = 0.9and batch size = 1024.
D Hyper-parameter setup
In Table 2, SimCLRv2 models are pre-trained on unlabelled ImageNet and fine-tuned on CIFAR10. Natural
models are directly adopted from Tramer & Boneh (2020) for highest natural accuracy, where optimizer
is DP-SGD and SGD, R= 0.1,ηDP= 4,ηnon−DP= 0.4, momentum = 0.9, batch size = 1024.Robust
models are obtained by grid search over ηandRagainstl∞(2/255), where Non-DP: η= 0.0625;DPϵ=2:
η= 4,R= 0.0625;DPϵ=4:η= 0.5,R= 0.0625,DPϵ=8:η= 0.125,R= 0.25. Similarly, adversarial training
models are obtained by grid search over ηand a fixed R= 0.0625, where Non-DP: η= 0.25;DPϵ=2:η= 0.5;
DPϵ=4:η= 1,DPϵ=8:η= 2. Other settings are the same as the naturalones. Adversarial attack is l∞, 20
steps, alpha = 0.1.
In Table 3, SimCLRv2 models are pre-trained on unlabelled ImageNet and fine-tuned on CIFAR10. Natural
models are directly adopted from Tramer & Boneh (2020) for highest natural accuracy, where optimizer
is DP-SGD and SGD, R= 0.1,ηDP= 4,ηnon−DP= 0.4, momentum = 0.9, batch size = 1024.Robust
models are obtained by grid search over ηandRagainstl2(0.25), where Non-DP: η= 0.0625;DPϵ=2:
η= 0.0625,R= 0.25;DPϵ=4:η= 0.5,R= 0.0625,DPϵ=8:η= 0.125,R= 0.25. Similarly, adversarial
training models are obtained by grid search over ηand a fixed R= 0.0625, where Non-DP: η= 0.0625;
DPϵ=2:η= 1;DPϵ=4:η= 1,DPϵ=8:η= 2. Other settings are the same as the naturalones. Adversarial
attack isl2, 20 steps, alpha = 0.1.
In Figure 6, models are SimCLRv2 pre-trained on unlabelled ImageNet and fine-tuned on CIFAR10 using
DP-SGD, with ϵ= 8, batch size = 1024. Adversarial attack is l∞PGD,γ= 4/255, alpha=0.1.
In Table 4, models the same as in Table 2 with robusthyper-parameters, where optimizer is DP-SGD and
SGD, momentum = 0.9, batch size = 1024, Non-DP: η= 0.0625;DPϵ=2:η= 4,R= 0.0625;DPϵ=4:
η= 0.5,R= 0.0625,DPϵ=8:η= 0.125,R= 0.25. Adversarial attack steps = 20, alpha = 0.1if applicable.
In Table 5, models are ResNet18 and ViT-tiny trained on CelebA, label Smiling. Images are resized to
224×224. Optimizer is DP-RMSprop with epochs = 5, batch size = 1024,η= 0.0002,R= 0.1, delta=5e-6.
Adversarial attack is l∞PGD, 20 steps, alpha = 1/255.
In Table 6, models are ResNet18 as in Table 5, trained on CelebA, label ’Smiling’. Images are resized to
224×224. Optimizer is DP-RMSprop with epochs = 5, batch size = 1024,η= 0.0002,R= 0.1, delta=5e-6.
Adversarial attack is l∞(2/255)with alpha∞= 1/255andl2(0.25)with alpha2= 0.2, 20 steps, if applicable.
In Figure 13, models are 2-layer CNN trained on CelebA label ‘Male’ using DP-Adam, where ϵ= 2, batch
size= 512, epochs = 10. Adversarial attack is l∞(2/255)PGD, 20 steps, alpha = 0.1.
26R
22202−22−4
η23×10−2
2−1×10−2
2−5×10−2
2−9×10−2Robust Accuracy % 40506070
R
22202−22−4
η23×10−2
2−1×10−2
2−5×10−2
2−9×10−2Natural Accuracy %
405060708090Figure 13: Robust and natural accuracy by ηandRon CelebA with label ‘Male’. We train a simple CNN
with DP-Adam and test under 20 steps of l∞(2/255)PGD attack. See details in Appendix D.
E Extra
E.1 Robust and accuracy landscapes of DP optimizers
We note that the diagonal pattern of the accuracy landscapes observed in Figure 5 (CIFAR10 & DP-
Heavyball), Figure 10 (CIFAR10 & DP-SGD), Figure 11 (MNIST & DP-SGD) and Figure 12 (Fashion
MNIST & DP-SGD) is not universal. For example, in Figure 7, we show that adaptive optimizers are much
less sensitive to the clipping norm R, as the landscapes are characterized by the row-wise pattern instead
of the diagonal pattern. This pattern is particularly obvious in the small Rregime, where the robust and
natural accuracy are high (see right panel in Figure 7.).
To rigorously analyze the insensitivity to the clipping norm in a simplified manner, we take the RM-
Sprop(Tieleman et al., 2012) as an example, similar to the analysis in (Bu et al., 2023b) on DP-Adam.
WhenRis sufficiently small, the private gradient in (3) becomes
˜gt=/summationdisplay
igt(xi)
max(1,||gt(xi)||2/R)+σRN(0,I) =/summationdisplay
igt(xi)
||gt(xi)||2/R+σRN(0,I)
=R·/parenleftigg/summationdisplay
igt(xi)
||gt(xi)||2+σN(0,I)/parenrightigg
:=R·ˆgt.
With private gradient ˜gt, DP-RMSprop updates the parameters θtby
θt=θt−1+ηt˜gt√˜vt, (9)
where ˜vis the squared average of ˜gt, written as
˜vt=α˜vt−1+ (1−α)˜g2
t=t/summationdisplay
s(1−α)αt−s˜g2
s=R2·t/summationdisplay
s(1−α)αt−sˆg2
s (10)
Substitute (10) into (9), we obtain an updating rule that is independent of the clipping norm R,
θt=θt−1+ηtR·ˆgt/radicalig
R2·/summationtextt
s(1−α)αt−sˆg2s=θt−1+ηtˆgt/radicalig/summationtextt
s(1−α)αt−sˆg2s.
As a result, DP optimizers can have fundamentally different landscapes with respect to the hyper-parameters
(R,η), which in turn may affect the accuracy and robustness as illustrated in Figure 8.
27