Published in Transactions on Machine Learning Research (11/2024)
Zero-Order One-Point Gradient Estimate in Consensus-
Based Distributed Stochastic Optimization
Elissa Mhanna elissa.mhanna@centralesupelec.fr
Mohamad Assaad mohamad.assaad@centralesupelec.fr
Laboratoire des Signaux & Systèmes
CentraleSupélec
3 rue Joliot Curie
91190 Gif-sur-Yvette, France
Reviewed on OpenReview: https: // openreview. net/ forum? id= tkswb7XoYB
Abstract
In this work, we consider a distributed multi-agent stochastic optimization problem, where
each agent holds a local objective function that is smooth and strongly convex and that is
subject to a stochastic process. The goal is for all agents to collaborate to ﬁnd a common
solution that optimizes the sum of these local functions. With the practical assumption
that agents can only obtain noisy numerical function queries at precisely one point at a
time, we consider an extention of a standard consensus-based distributed stochastic gradient
(DSG) method to the bandit setting where we do not have access to the gradient, and we
introduce a zero-order (ZO) one-point estimate (1P-DSG). We analyze the convergence of
this techniques using stochastic approximation tools, and we prove that it converges almost
surely to the optimum despite the biasedness of our gradient estimate. We then study the
convergence rate of our method. With constant step sizes, our method competes with its
ﬁrst-order (FO) counterparts by achieving a linear rate O(/rho1k)as a function of number of
iterationsk. To the best of our knowledge, this is the ﬁrst work that proves this rate in
the noisy estimation setting or with one-point estimators. With vanishing step sizes, we
establish a rate of O(1√
k)after a suﬃcient number of iterations k>K 0. This rate matches
the lower bound of centralized techniques utilizing one-point estimators. We then provide
a regret bound of O(√
k)with vanishing step sizes. We further illustrate the usefulness of
the proposed technique using numerical experiments.
1 Introduction
Gradient-free optimization is an old topic in the research community; however, there has been an increased
interest recently, especially in machine learning applications, where optimization problems are typically
solvedwithgradientdescentalgorithms. Successfulapplicationsofgradient-freemethodsinmachinelearning
include competing with an adversary in bandit problems (Flaxman et al., 2004; Agarwal et al., 2010),
generating adversarial attacks for deep learning models (Chen et al., 2019; Liu et al., 2019) and reinforcement
learning (Vemula et al., 2019). Gradient-free optimization aims to solve optimization problems with only
functional ZO information rather than FO gradient information. These techniques are essential in settings
where explicit gradient computation may be impractical, expensive, or impossible. Instances of such settings
include high data dimensionality, time or resource straining function diﬀerentiation, or the cost function not
having a closed-form. ZO information-based methods include direct search methods (Golovin et al., 2019),
1-point methods (Flaxman et al., 2004; Bach & Perchet, 2016; Vemula et al., 2019; Li & Assaad, 2021) where
a functionf(·,S) :Rd→Ris evaluated at a single point with some randomization to estimate the gradient
as such
g(1)
γ,z(x,S) =d
γf(x+γz,S )z, (1)
1Published in Transactions on Machine Learning Research (11/2024)
withxthe optimization variable, γ > 0a small value, and za random vector following a symmetrical
distribution. ZO also includes 2- or more point methods (Duchi et al., 2015; Nesterov & Spokoiny, 2017;
Gorbunov et al., 2018; Bach & Perchet, 2016; Hajinezhad et al., 2019; Kumar Sahu et al., 2018; Agarwal
et al., 2010; Chen et al., 2019; Liu et al., 2019; Vemula et al., 2019), where functional diﬀerence at various
points is employed for estimation, generally having the respective structures
g(2)
γ,z(x,S) =df(x+γz,S )−f(x−γz,S )
2γz (2)
andg(2d)
γ(x,S) =d/summationdisplay
j=1f(x+γej,S)−f(x−γej,S)
2γej (3)
where{ej}j=1,...,dis the canonical basis, and other methods such as sign information of gradient estimates
(Liu et al., 2019).
Another area of great interest is distributed multi-agent optimization, where agents try to cooperatively
solve a problem with information exchange only limited to immediate neighbors in the network. Distributed
computing and data storing are particularly essential in ﬁelds such as vehicular communications and coordi-
nation, data processing and distributed control in sensor networks (Shi & Eryilmaz, 2020), big-data analytics
(Daneshmand et al., 2015), and federated learning (McMahan et al., 2017). More speciﬁcally, one direction
of research integrates (sub)gradient-based methods with a consensus/averaging strategy; the local agent
incorporates one or multiple consensus steps alongside evaluating the local gradient during optimization.
Hence, these algorithms can tackle a fundamental challenge: overcoming diﬀerences between agents’ local
data distributions.
1.1 Problem Description
Consider a set of agents N={1,2,...,n}connected by a communication network. Each agent iis associated
with a local objective function fi(·,S) :K→R, whereK⊂Rdis a convex feasible set. The global goal
of the agents is to collaboratively locate the decision variable x∈Kthat solves the stochastic optimization
problem:
min
x∈KF(x) =1
nn/summationdisplay
i=1Fi(x) (4)
where
Fi(x) =ESfi(x,S),
withS∈Sdenoting an i.i.d. ergodic stochastic process describing uncertainties in the communication
system.
We assume that at each time step, agent ican only query the function values of fiat exactly one point, and
can only communicate with its neighbors. Further, we assume that the function queries are noisy ˜fi=fi+ζi
withζisome additive noise. Agent imust then employ this query to estimate the gradient of the form
gi(x,Si).
1.2 Function Classes and Gradient Estimate Assumptions
Consider the following ﬁve classes of functions:
•The convex class Ccvxcontaining all functions f:Rd→Rthat are convex.
•The strongly convex class Csccontaining all functions f:Rd→Rthat are continuously diﬀerentiable
and admit a constant λfsuch that
/angbracketleft∇f(x)−∇f(y),x−y/angbracketright≥λf/bardblx−y/bardbl2,∀x,y∈Rd.
2Published in Transactions on Machine Learning Research (11/2024)
•The Lipschitz continuous class Clipcontaining all functions f:Rd→Rthat admit a constant Lf
such that
|f(x)−f(y)|≤Lf/bardblx−y/bardbl,∀x,y∈Rd.
•The smooth class Csmocontaining all functions f:Rd→Rthat are continuously diﬀerentiable and
admit a constant Gfsuch that
/bardbl∇f(x)−∇f(y)/bardbl≤Gf/bardblx−y/bardbl,∀x,y∈Rd.
•The gradient dominated class Cgdcontaining all functions f:Rd→Rthat are diﬀerentiable, have
a global minimizer x∗, and admit a constant νfsuch that
2νf(f(x)−f(x∗))≤/bardbl∇f(x)/bardbl2,∀x∈Rd.
This gradient domination property can be viewed as a nonconvex analogy of strong convexity.
In addition, consider the following assumptions on the gradient estimate:
•A gradient estimate gis said to be unbiased w.r.t. the true gradient ∇fif for allx∈Rdand
independent S∈S, it satisﬁes the following equality
ES[g(x,S)|x] =∇f(x).
•Otherwise, it is said to be biased and satisﬁes
ES[g(x,S)|x] =∇f(x) +b(x),
withb(x)some bias term.
•A gradient estimate gis said to have bounded variance when for all x∈Rdand independent S∈S,
ES[/bardblg(x,S)−∇f(x)/bardbl2|x]≤σfor someσ>0.
•Otherwise, when this bound is unknown or does not exist, it is said to have unbounded variance.
In general, FO stochastic gradient estimates are unbiased and have bounded variance. ZO estimates, on the
other hand, are biased. While multi-point ZO estimates have bounded or even vanishing variance, one-point
estimates have unbounded variance Liu et al. (2020).
1.3 Related Work
FO Consensus-Based Distributed Methods: The optimal convergence rate for solving problem (4), as-
suming the objective function Fis strongly convex with Lipschitz continuous gradients, has been established
asO(1
k)under a diminishing step size with full gradient information Pu & Nedić (2018); Nemirovski et al.
(2009). However, when employing a constant step size α>0that is suﬃciently small, the iterates produced
by a stochastic gradient method converge exponentially fast (in expectation) to an O(α)-neighborhood of
the optimal solution (Pu & Nedić, 2018); this is known as the linear rate O(/rho1k). The literature dedicated to
solving problem (4) is vast. In what follows, we highlight some of the contributions.
Towﬁc et al. (2016); Tu & Sayed (2012) study distributed stochastic gradient methods where they compare
the adapt-then-combine (ATC) and combine-then-adapt (CTA) strategies, and prove that the ATC strategy
outperforms CTA one in terms of convergence rate, whether with vanishing or with constant step sizes and
that it is more robust against data distribution drifts and network topology. Jakovetic et al. (2018) consider
the CTA strategy with noisy FO gradients over random networks and establish an O(1
k)convergence rate
for strongly convex and smooth objectives and vanishing step size. Matei & Baras (2011); Yuan et al. (2016)
also consider random networks and solve problem (4) using a noise-free (sub)gradient instead and achieve a
3Published in Transactions on Machine Learning Research (11/2024)
ESTIMATE OPFUNCTION
CLASSSTEP
SIZEREGRET
BOUNDCONVERGENCE
RATE
ZOOne-
pointCentralized Ccvx/intersectiontext
Clip f. O(k3
4) O(1
4√
k)Flaxman et al. (2004)
Centralized Csc/intersectiontext
Clip/intersectiontext
Csmo v. O(√
k) O(1√
k)Bach & Perchet (2016)
Distributed Csc/intersectiontext
Csmo v. O(√
k) O(1√
k)1P-DSG
Distributed Csc/intersectiontext
Csmo f. - O(/rho1k)1P-DSG
Two-
pointCentralized Ccvx/intersectiontext
Clip v. O(√
k) O(1√
k)Agarwal et al. (2010)
Centralized Csc/intersectiontext
Clip v. O(logk) O(logk
k)Agarwal et al. (2010)
Distributed Clip/intersectiontext
Csmo v. - O(1√
klogk)Tang et al. (2021)
Distributed Csmo/intersectiontext
Cgd v. - O(1
k)Tang et al. (2021)
2d-
pointDistributed Csmo f. - O(1
k)Tang et al. (2021)
Distributed Csmo/intersectiontext
Cgd f. - O(/rho1k)Tang et al. (2021)
Distributed Csc/intersectiontext
Csmo v. - O(1√
k)Kumar Sahu et al. (2018)
FOUnbiased
/BVDistributed Csc/intersectiontext
Csmo f. - O(/rho1k)Matei & Baras (2011) ;
Yuan et al. (2016); Pu & Nedić (2018)
Distributed Csc/intersectiontext
Csmo v. - O(1
k)Jakovetic et al. (2018) ;
Pu & Nedić (2018)
Table 1: Convergence rates for various algorithms related to our work, classiﬁed according to the nature of
the gradient estimate, whether the optimization problem (OP) is centralized or distributed, the assumptions
on the objective function, whether the step size is ﬁxed (f.) or varying (v.), and the achieved regret bound
and convergence rate
linear rate to a neighborhood of the optimum with constant step sizes. Nedić & Olshevsky (2016) consider
time-varying and directed networks and present a subgradient-push method based on noisy FO gradients
that achieves an O(lnk
k)rate under the same assumptions on the objective function and vanishing step size.
Both the works of Shi et al. (2015) and Qu & Li (2018) consider a static version of the objective function and
proposemethodsthatemployhistoryinformationofthegradient. Theybothobtainarateof O(1
k)forgeneral
convex and smooth objectives with constant step sizes. Under the further strong convexity assumption, the
static nature of the objective allows them to establish a linear convergence rate to the exact solution instead
of a neighborhood of it. Qu & Li (2018) inspire the vast literature on gradient tracking extended to the
stochastic setting (Pu & Nedić, 2018; Pu, 2020; Xin et al., 2019) that utilizes local auxiliary variables to
track the average of all agents’ gradients, the linear rate, however, is established to a neighborhood of the
optimum.
ZO Centralized Methods: ZO methods are known to have worse convergence rates than their FO coun-
terparts under the same conditions. For example, under a convex centralized setting, Flaxman et al. (2004)
prove a regret bound of O(k3
4)(or equivalently a rate of O(1
4√
k)) with a one-point estimator for Lipschitz
continuous functions. For strongly convex and smooth objective functions, Hazan & Levy (2014) and Ito
(2020) improve upon this result by proving a regret of O(√klogk)and Bach & Perchet (2016) that of O(√
k).
In the work of Agarwal et al. (2010), when the number of points is two, they prove regret bounds of ˜O(√
k)
with high probability and of O(log(k))in expectation for strongly convex loss functions. When the number
isd+ 1point, they prove regret bounds of O(√
k)and ofO(log(k))with strong convexity. The reason why
the performance improves with the addition of number of points in the estimate, is that their variance can be
bounded, unlike one-point estimates whose variance cannot be bounded (Liu et al., 2020). However, when
the function queries are subjected to noise, multi-point estimates start behaving like one-point ones. In noisy
function queries (centralized) scenario, it has been proven that gradient-free methods cannot achieve a better
4Published in Transactions on Machine Learning Research (11/2024)
convergence rate than Ω(1√
k)which is the lower bound derived by Duchi et al. (2015); Jamieson et al. (2012);
Shamir (2013) for strongly convex and smooth objective functions. In the work of Bubeck et al. (2021), a
kernelized loss estimator is proposed where a generalization of Bernoulli convolutions is adopted, and an
annealing schedule for exponential weights is used to control the estimator’s variance in a focus region for
dimensions higher than 1. Their method achieves a regret bound of O(√
k).
ZO Consensus-Based Distributed Methods: In distributed settings, Tang et al. (2021) develop two
algorithms for a noise-free nonconvex multi-agent optimization problem aiming at consensus. One of them
is gradient-tracking based on a 2d-point estimator of the gradient with vanishing variance that achieves a
rate ofO(1
k)with smoothness assumptions and a linear rate for an extra ν-gradient dominated objective
assumption and for ﬁxed step sizes. The other is based on a 2-point estimator following an ATC strategy
instead of gradient tracking and achieves a rate of O(1√
klogk)under Lipschitz continuity and smoothness
conditions and O(1
k)under an extra gradient dominated function structure. In the nonconvex setting, a
gradient-tracking method is also proposed, but with a one-point estimator (Mhanna & Assaad, 2023) where a
convergence rate of O(1
3√
k)is established with Lipschitz continuous and smooth functions. Kumar Sahu et al.
(2018) propose a standard CTA method where they consider a 2d-point estimate with noisy function queries
over random networks. Under smoothness and strong convexity, they establish an O(1√
k)convergence rate
with vanishing step sizes. Wan et al. (2020; 2022) propose a projection-free method with one-point gradient
estimate where a linear optimization step is performed instead of projection. They prove a regret bound of
O(k3
4)for convex losses and an improved regret of O(k2
3(logk)1
3)for strongly convex ones.
We highlight some of the mentioned convergence rates from the literature in Table 1.
1.4 Contributions
While consensus-based distributed methods have been extended to the ZO case (Tang et al., 2021; Ku-
mar Sahu et al., 2018), their approach relies on a multi-point gradient estimator and in the case of Mhanna
& Assaad (2023); Wan et al. (2020; 2022), the rates established for one-point estimates are slow. The multi-
point estimation technique assumes the ability to observe multiple instances of the objective function under
identical system conditions, i.e., many function queries are done for the same realization of Sin (2) and (3).
However, this assumption needs to be revised in applications such as mobile edge computing (Mao et al.,
2017; Chen et al., 2021; Zhou et al., 2022) where computational tasks from mobile users are oﬄoaded to
servers within the cellular network. Thus, queries requested from the servers by the users are subject to
the wireless environment and are corrupted by noise not necessarily additive. Other applications include
sensor selection for an accurate parameter estimation (Liu et al., 2018) where the observation of each sensor
is continuously changing. Thus, in such scenarios, one-point estimates oﬀer a vital alternative to solving
online optimization/learning problems. Yet, one-point estimators are not generally used because of their
slow convergence rate. The main reason is due to their unbounded variance. To avoid this unbounded
variance, in this work, we don’t use the estimate given in (1), we extend the one point approach in Li &
Assaad (2021)’s work where the action of the agent is a scalar and diﬀerent agents have diﬀerent variables,
to our consensual problem with vector variables. The diﬀerence is that in our gradient estimate, we don’t
divide byγ. This brings additional challenges in proving that our algorithm converges and a consensus can
be achieved by all agents. And even with bounded variance, there’s still a diﬃculty achieving good (linear)
convergence rates with two-point estimates due to the constant upper bound of the variance (Tang et al.,
2021). Here, despite this constant bound, we were able to go beyond two-point estimates to achieve a linear
rate. Moreover, while it requires 2dpointswiththe gradient tracking method to achieve a linear rate in Tang
et al. (2021)’s work, which is twice the dimension of the gradient itself, here we only need one scalar point
or query. This is much more computationally eﬃcient. We further replace the gradient tracking method by
a standard ATC strategy which is more communication eﬃcient as it requires the sharing of only one vector
instead of two.
We summarize our contribution in the following points,
•We consider smooth and strongly convex local objectives, and we consider the distributed stochastic
gradient method in the case where we do not have access to the gradient in the noisy setting. Under
5Published in Transactions on Machine Learning Research (11/2024)
the realistic assumption that the agent only has access to a single noisy function value at each
time without necessarily knowing the form of this function, we propose a one-point estimator in a
stochastic framework.
•Naturally, one-point estimators are biased with respect to the true gradient and suﬀer from high
variance (Liu et al., 2020); Despite this, in this work, we analyze and indeed prove the algorithm’s
convergence almost surely with a biased estimate. This convergence is stronger than expected
convergence analysis usually established for ZO optimization. We also consider that a stochastic
process inﬂuences the objective function from one iteration to the other, which provides a practical
modelingforreal-worldscenariosthatinvolvevarioussourcesofstochasticity, notnecessarilyadditive
noise.
•We then study the convergence rate and we demonstrate that with ﬁxed step sizes, the algorithm
achieves a linear convergence rate O(/rho1k)to a neighborhood of the optimal solution, marking the
ﬁrst instance where this rate is attained in ZO optimization with one-point/two-point estimates and
in a noisy query setting, to the best of our knowledge. This linear rate competes with FO methods
and even centralized algorithms in terms of convergence speed (Pu & Nedić, 2018).
•Whenthestep-sizesarevanishing, weprovethatarateof O(1√
k)isattainabletoconvergetoanexact
solution after a suﬃcient number of iterations k>K 0. This rate satisﬁes the lower bounds achieved
by its centralized counterparts in the same derivative-free setting (Duchi et al., 2015; Jamieson et al.,
2012; Shamir, 2013).
•We then show that a regret bound of O(√
k)is achieved for this algorithm.
•Finally, we support our theoretical claims by providing numerical evidence and comparing the algo-
rithm’s performance to its FO and centralized counterparts.
The rest of this paper is divided as follow. In subsection 1.5, we present the mathematical notation followed
in this paper. In subsection 1.6, we present the main assumptions of our optimization problem. We then
describe our gradient estimate followed by the proposed algorithm in subsection 2.1. We then prove the
almost sure convergence of our algorithm in subsection 3.1 and study its rate in subsection 3.2 with varying
step sizes. In subsection 3.3, we ﬁnd its regret bound. And in subsection 3.4, we consider the case of ﬁxed
step sizes, study the convergence of our algorithm and its rate. Finally, in section 4, we provide numerical
evidence and conclude the paper in section 5.
1.5 Notation
In all that follows, vectors are column-shaped unless deﬁned otherwise and 1denotes the vector of all entries
equal to 1. For two vectors a,bof the same dimension, /angbracketlefta,b/angbracketrightis the inner product. For two matrices A,
B∈Rn×d, we deﬁne
/angbracketleftA,B/angbracketright=n/summationdisplay
i=1/angbracketleftAi,Bi/angbracketright
whereAi(respectively, Bi) represents the i-th row ofA(respectively, B). This matrix product is the Hilbert-
Schmidt inner product which is written as /angbracketleftA,B/angbracketright= tr(ABT)./bardbl./bardbldenotes the 2-norm for vectors and the
Frobenius norm for matrices.
We next let ΠK(·)denote the Euclidean projection of a vector on the set K. We know that this projection
on a closed convex set Kis nonexpansive (Kinderlehrer & Stampacchia (2000) - Corollary 2.4), i.e.,
/bardblΠK(x)−ΠK(x/prime)/bardbl≤/bardblx−x/prime/bardbl,∀x,x/prime∈Rd. (5)
We assume that each agent imaintains a local copy xi∈Kof the decision variable and each agent’s local
function is subject to the stochastic variable Si∈Rm. At iteration k, the respective values are denoted as
xi,kandSi,k. Bold notations denote the concatenated version of the variables, i.e.,
x:= [x1,x2,...,xn]Tis of dimension n×dandS:= [S1,S2,...,Sn]Tof dimension n×m.
6Published in Transactions on Machine Learning Research (11/2024)
We then deﬁne the mean of the decision variable as ¯x:=1
n1Txwhose dimension is 1×d.
We deﬁne the gradient of Fiat the local variable ∇Fi(xi)∈Rdand its Hessian matrix ∇2Fi(xi)∈Rd×dand
we let
∇F(x) := [∇F1(x1),∇F2(x2),...,∇Fn(xn)]T∈Rn×d
and
g:=g(x,S) := [g1(x1,S1),g2(x2,S2),...,gn(xn,Sn)]T∈Rn×d.
We deﬁne its mean ¯g:=1
n1Tg∈R1×dand we denote each agent’s gradient estimate at time kbygi,k=
gi(xi,k,Si,k).
1.6 Basic Assumptions
In this subsection, we introduce the fundamental assumptions that ensure the performance of the 1P-DSG
algorithm.
Assumption 1.1. (on the graph) The topology of the network is represented by the graph G= (N,E)where
the edges inE⊆N×N represent communication links. A graph Gis undirected, i.e., (i,j)∈Eiﬀ(j,i)∈E,
and connected (there exists a path of links between any two agents).
W= [wij]∈Rn×ndenotes the agents’ coupling matrix, where agents iandjare connected iﬀ wij=wji>0
(wij=wji= 0otherwise). Wis a nonnegative matrix and doubly stochastic, i.e., W1=1and1TW=1T.
All diagonal elements wiiare strictly positive.
Assumption 1.2. (on the objective function) We assume the existence and the continuity of both ∇Fi(x)
and∇2Fi(x). Letx∗∈Kdenote the solution of the problem (4) such that F(x∗) = minx∈KF(x). We next
assume thatF(x)isλ-strongly convex where
F(y)≥F(x) +/angbracketleft∇F(x),y−x/angbracketright+λ
2/bardbly−x/bardbl2,∀x,y∈K.
We further assume the boundedness of the local Hessian where there exists a constant c1∈R+such that
/bardbl∇2Fi(x)/bardbl2≤c1,∀x∈K,∀i∈N,
where here it suﬃces to the spectral norm (keeping in mind for a matrix A,/bardblA/bardbl2≤/bardblA/bardblF).
Assumption 1.3. (on the additive noise) ζi,kis a zero-mean uncorrelated noise with bounded variance,
whereE(ζi,k) = 0andE(ζ2
i,k) =c2<∞,∀i∈N.
Lemma 1.4. (Qu & Li, 2018) Let ρwbe the spectral norm of W−1
n11T. When Assumption 1.1 is satisﬁed,
we have the following inequality
/bardblWω−1¯ω/bardbl≤ρw/bardblω−1¯ω/bardbl,∀ω∈Rn×dand¯ω=1
n1Tω,
andρw<1.
Lemma 1.5. (Pu & Nedić, 2018) Deﬁne h(x) :=1
n1T∇F(x)∈R1×d. Due to the boundedness of the second
derivative in Assumption 1.2, there exists a scalar L>0such that the objective function is L-smooth, and
/bardbl∇F (¯x)−h(x)/bardbl≤L√n/bardblx−1¯x/bardbl.
Proof: See Appendix A.
2 Distributed Stochastic Gradient Methods
We propose to employ a zero-order one-point estimate of the gradient subject to the stochastic process S
and an additive noise ζwhile a stochastic perturbation and a step size are introduced, and we assume that
7Published in Transactions on Machine Learning Research (11/2024)
each agent can perform this estimation at each iteration. To elaborate, let gi,kdenote the aforementioned
gradient estimate for agent iat timek, then we deﬁne it as
gi,k= Φi,k˜fi(xi,k+γkΦi,k,Si,k)
= Φi,k(fi(xi,k+γkΦi,k,Si,k) +ζi,k),(6)
whereγk>0is a vanishing step size and Φi,k∈Rdis a perturbation randomly and independently generated
by each agent i.gi,kis in fact a biased estimation of the gradient ∇Fi(xi,k)and the algorithm can converge
under the condition that all parameters are properly chosen. For clariﬁcation on the form of this bias and
more on the properties of this estimate, refer to Appendix B.
2.1 The 1P-DSG Algorithm
We consider a zero-order distributed stochastic gradient algorithm aiming for consensus with a one-point
estimate. We denote it as 1P-DSG employing the gradient estimate gi,kin (6). Every agent iinitializes its
variables with an arbitrary valued vector xi,0∈Kand computes gi,0at that variable. Then, at each time
k∈N, agentiupdates its variables independently according to the following steps:
zi,k+1=n/summationdisplay
j=1wij(xj,k−αkgj,k)
xi,k+1= ΠK(zi,k+1)
perform the action: xi,k+1+γk+1Φi,k+1(7)
whereαk>0is a step size. Algorithm (7) can then be written in the following compact matrix form for
clarity of analysis:
zk+1=W(xk−αkgk)
xk+1= [x1,k+1,x2,k+1,...,xn,k+1]T
perform the action: xk+1+γk+1Φk+1(8)
where Φk∈Rn×dis deﬁned as Φk= [Φ 1,k,Φ2,k,..., Φn,k]T.
As is evident from the update of the variables, the exchange between agents is limited to neighboring nodes,
and it encompasses the value xk−αkgkor the local gradient descent step.
We remark the eﬀect of the gradient estimate variance on the convergence by carefully examining the steps
in (8). Naturally, when the estimates have a large variance, the estimated gradients can vary widely from
one sample to another. This means that the norm of xk+1, which is directly aﬀected by this variance,
may also grow considerably. Thus, it may then take longer to converge to the optimal solution because it
cannot reliably discern the direction of the steepest descent. In the worst case, the huge variance causes
instability as the optimizer may oscillate around the optimum or even diverge if the variance is too high,
making converging to a satisfactory solution diﬃcult. In this work, we use the fact that the local functions
and the noise variance are bounded to prove that the variance of gradient estimate presented in (6) is indeed
bounded. This boundedness, alongside the properties of the matrix Win Assumption 1.1, allows us to ﬁnd
then an upper bound on the variation of xk+1with respect to its mean and the variation of this mean with
respect to the optimizer at every iteration and analyze the convergence of both.
We then consider the following assumptions for the subsequent convergence analysis. We must note that the
ﬁrst assumption is only taken into account when we study the algorithm’s behavior with varying step sizes,
otherwise it is dropped.
Assumption 2.1. (on the step-sizes) Both αkandγkvanish to 0ask→∞, and satisfy the the following
sums∞/summationdisplay
k=1αkγk=∞,∞/summationdisplay
k=1α2
k<∞,and∞/summationdisplay
k=1αkγ2
k<∞.
8Published in Transactions on Machine Learning Research (11/2024)
Assumption 2.2. (on the random perturbation) Let Φi,k= (φ1
i,k,φ2
i,k,...,φd
i,k)T.
Each agent ichooses its Φi,kvector independently from other agents j/negationslash=i. In addition, the elements of Φi,k
are assumed i.i.d with E(φd1
i,kφd2
i,k) = 0ford1/negationslash=d2and there exists c3>0such that E(φdj
i,k)2=c3,∀dj,∀i,
almost surely. We further assume that there exists a constant c4>0where/bardblΦi,k/bardbl≤c4,∀i, almost surely.
Example 2.3. One example is to take αk=α0(k+ 1)−υ1andγk=γ0(k+ 1)−υ2with the constants α0,
γ0,υ1,υ2∈R+. As/summationtext∞
k=1αkγkdiverges for υ1+υ2≤1,/summationtext∞
k=1α2
kconverges for υ1>0.5, and/summationtext∞
k=1αkγ2
k
converges for υ1+ 2υ2>1, we can ﬁnd pairs of υ1andυ2so that Assumption 2.1 is satisﬁed.
To achieve the conditions in Assumption 2.2, we can choose the probability distribution of φdj
i,kto be the
symmetrical Bernoulli distribution where φdj
i,k∈{−1√
d,1√
d}withP(φdj
i,k=−1√
d) =P(φdj
i,k=1√
d) = 0.5,∀dj,
∀i.
Assumption 2.4. (on the local functions) Kis a compact convex set and all local functions x/mapsto→fi(x,S)
are bounded on the c4γ0-neighborhood ofK, i.e.,
|fi(x,S)|<∞,∀x∈Nc4γ0(K),∀S∈Rm,∀i∈N,
whereNc4γ0(K) ={x∈Rd|infa∈K/bardblx−a/bardbl<c4γ0}is thec4γ0-neighborhood ofK.
3 The 1P-DSG Algorithm
In this section, we analyze Algorithm 1P-DSG presented in (7) and (8).
3.1 Convergence Results
ThegoalofthispartistoanalyzetheasymptoticbehaviorofAlgorithm(8). Westarttheanalysisbydeﬁning
Hkas the history sequence {x0,y0,S0,...,xk−1,yk−1,Sk−1,xk}and denoting by E[.|Hk]as the conditional
expectation given Hk.
We deﬁne ˜gkto be the expected value of ¯gkwith respect to all the stochastic terms S,Φ,ζgivenHk, i.e.,
˜gk=ES,Φ,ζ[¯gk|Hk].
In what follows, we use ˜gk=E[¯gk|Hk]for shorthand notation.
We deﬁne the error ekto be the diﬀerence between the value of a single realization of ¯gkand its conditional
expectation ˜gk, i.e.,
ek= ¯gk−˜gk,
whereekcan be seen as a stochastic noise. The following lemma describing the vanishing of the stochastic
noise is essential for our main result.
Lemma 3.1. If all Assumptions 1.2, 1.3, 2.1, 2.2, and 2.4 hold, then for any constant ν >0, we have
P( lim
K→∞sup
K/prime≥K/bardblK/prime/summationdisplay
k=Kαkek/bardbl≥ν) = 0,∀ν >0.
Proof: See Appendix C.
For any integer k≥0, we deﬁne the divergence, or the error between the average action taken by the agents
¯xkand the optimal solution x∗withinKas
dk=/bardbl¯xk−x∗/bardbl2. (9)
The following theorem describes the main convergence result.
Theorem 3.2. If all Assumptions 1.1-1.3, 2.1-2.2, and 2.4 hold, then as k→∞,dk→0,¯xk→x∗, and
xi,k→¯xk, for alli∈N, almost surely by applying the Algorithm.
Proof: See Appendix D.
9Published in Transactions on Machine Learning Research (11/2024)
3.2 Convergence Rate with Vanishing Step Sizes
This part deals with how fast the expected divergence vanishes to ﬁnd the proposed algorithm’s expected
convergence rate. To do so, we deﬁne the expected divergence as
Dk=E[/bardbl¯xk−x∗/bardbl2].
The goal is to bound this divergence from above by sequences whose convergence rate is known. The analysis
is highly associated with the parameters αkandγkthat play a signiﬁcant role in determining this upper
bound. Hence, in what follows, the analysis starts with a general form of αkandγk, then a particular case
is considered.
3.2.1 General Form of αkandγk
We ﬁrst study the rate of convergence of the consensus error by introducing the following lemma.
Lemma 3.3. Let Assumptions 1.1-1.3, 2.1-2.2, and 2.4 hold. Deﬁne
K1= arg min
α2
k+1
α2
k>1+ρ2w
2k.
Then, fork≥K1, there exist 0<ϑ1,ϑ2<∞, such that
/bardblxk−1¯xk/bardbl2<ϑ2
1α2
kand/bardblzk+1−1¯xk/bardbl2≤ϑ2
2α2
k. (10)
Proof: Refer to Appendix D.3.
Our main result regarding the convergence rate is summarized in the following theorem.
Theorem 3.4. Let Assumptions 1.1-1.3, 2.1-2.2, and 2.4 hold. We then deﬁne the constants A=λc3
2,
B=4c3L2ϑ2
1
λn,C=c2
1c6
4
c3λ,E=ϑ2
n,
K2= arg min
Aαkγk<1k,andK0= max{K1,K2}.
We ﬁnally deﬁne the following parameters:
κk=1−(γk+1
γk)2
αkγk, σ 1= max
k≥K0κk, σ 2= max
k≥K0α2
k
γ2
k, σ 3= max
k≥K0αk
γ3
k,
τk=1−αk+1γ−1
k+1
αkγ−1
k
αkγk, σ 4= max
k≥K0τk, σ 5= max
k≥K0αkγk, σ 6= max
k≥K0γ3
k
αk.(11)
Ifκk<Afor anyk≥K0, then
Dk≤ς1γ2
k,∀k≥K0, (12)
with
ς1≥max/braceleftBigg
DK0
γ2
K0,Bσ2+Eσ3+C
A−σ1/bracerightBigg
. (13)
Ifτk<Afor anyk≥K0, then
Dk≤ς2αk
γk,∀k≥K0, (14)
with
ς2≥max/braceleftBigg
DK0γK0
αK0,Bσ5+Cσ6+E
A−σ4/bracerightBigg
. (15)
Proof: See Appendix E.1.
10Published in Transactions on Machine Learning Research (11/2024)
3.2.2 A Special Case of αkandγk
We now consider the special case mentioned in Example 2.3:
αk=α0(k+ 1)−υ1andγk=γ0(k+ 1)−υ2, (16)
where 0.5<υ1<1,0<υ2≤1−υ1, andυ1+ 2υ2>1.
Theorem 3.5. Letαkandγkhave the forms given in (16) and consider the same assumptions of Theorem
3.4. Ifα0γ0≥max{2υ2,υ1−υ2}/A, then we can say that there exists Υ<∞, where
Dk≤Υ(k+ 1)−min{2υ2,υ1−υ2},∀k≥K0.
Proof: See Appendix E.4.
The parameters clearly aﬀect the upper bound of the convergence rate or rate of expected divergence decay
in Theorem 3.5. As it is evident that
max{2υ2,υ1−υ2}≤0.5,
the best choice is when equality holds for υ1= 0.75andυ2= 0.25. With the suﬃcient condition on the
parameters in Theorem 3.5, we can ﬁnally state that our algorithm converges with a rate of O(1√
k)after a
suﬃcient number of iterations k>K 0when the step sizes are vanishing.
3.3 Regret Bound
To further examine the performance of our algorithm, we present the following theorem on the achieved
regret bound.
Theorem 3.6. Let the assumptions of Theorem 3.4 hold. When αkandγkhave the forms of (16) with
υ1= 0.75andυ2= 0.25, the regret bound is given by
E/bracketleftbigg1
nK/summationdisplay
k=1n/summationdisplay
i=1Fi(xi,k)−Fi(x∗)/bracketrightbigg
≤O(√
K).
Proof: See Appendix F.
3.4 Convergence Rate with Constant Step Sizes
In this subsection, we ﬁx the step sizes to αk=α > 0andγk=γ > 0,∀k≥0, and we assume them
to be two arbitrarily small values. We also deﬁne the following terms, A=λc3
2,B=4c3L2
λn,C=c2
1c6
4
c3λ,
andR=/bardblx0−1¯x0/bardbl2. We letMdenote the upper bound on /bardbl¯gk/bardbl2. We then let G1=2nM(1+ρ2
w)
(1−ρ2w)2and
G2=nM/parenleftbigg/parenleftBig
1+ρ2
w
1−ρ2w/parenrightBig2
+1+ρ2
w
1−ρ2w/parenrightbigg
. We ﬁnally deﬁne /rho11= 1−Aαγand/rho12=1+ρ2
w
2.
Theorem 3.7. Assumeαγ <1
Aandα<γ. Let Assumptions 1.1-1.3, 2.2, and 2.4 hold, then
/bardblxk+1−1¯xk+1/bardbl2≤/rho1k+1
2R+α2G1and/bardblzk+1−1¯xk/bardbl2≤/rho1k+1
2R+α2G2. (17)
Meaning,/bardblxk+1−1¯xk+1/bardbl2converges with the linear rate of O/parenleftbig
/rho1k
2/parenrightbig
for an arbitrary small αalmost surely.
Further,
•When/rho11≤/rho12,
Dk+1≤/rho1k+1
1D0+/rho1k+1
22R/parenleftBig
Bαγ +/rho12
n/parenrightBig
2Aαγ +ρ2w−1+α2BG1
A+α
γG2
nA+γ2C
A.(18)
Then, for arbitrary small step sizes, Dkconverges with the linear rate of O/parenleftbig
/rho1k
2/parenrightbig
.
11Published in Transactions on Machine Learning Research (11/2024)
•When/rho11>/rho12,
Dk+1≤/rho1k+1
1/parenleftBig
D0+2RBαγ +2R/rho12
n
1−2Aαγ−ρ2w/parenrightBig
+α2BG1
A+α
γG2
nA+γ2C
A. (19)
Then, for arbitrary small step sizes, Dkconverges with the linear rate of O/parenleftbig
/rho1k
1/parenrightbig
.
Proof: See Appendix G.
Taking arbitrarily small values of α,γsatisfyingαγ <1
Aandα < γ, and setting /rho1= max{/rho11,/rho12}, the
convergence rate becomes O(/rho1k), achieving the same rate as with FO information.
4 Numerical Results
In this section, we provide numerical examples to illustrate the performance of the algorithm 1P-DSG. We
compare it with FO distributed methods aiming to achieve consensus, namely DSGT (Pu & Nedić, 2018)
and EXTRA (Shi et al., 2015), a ZO distributed algorithm denoted 2P-DSG based on the two-point estimate
in (2) (Tang et al., 2021), and a ZO centralized algorithm based on gradient descent (e.g. Flaxman et al.
(2004) and Bach & Perchet (2016)) using another one-point estimate which is presented in (1). We denote
the ZO centralized algorithm by 1P-GD. We also compare with a centralized version of our algorithm where
we use the estimate in (6). For DSGT and EXTRA, we calculate the exact gradient and add white noise to
it to form an unbiased FO estimator and for all the ZO algorithms, we consider that the function queries
are noisy. The network topology is a connected Erdős-Rényi random graph with a probability of 0.05.
We consider a logistic classiﬁcation problem to classify mimages of the two digits, labeled as yij= +1or
−1from the MNIST data set (LeCun & Cortes, 2005). Each image, Xij, is a 785-dimensional vector and
is compressed using a lossy autoencoder to become 10-dimensional denoted as X/prime
ij, i.e.,d= 10. The total
images are split equally among the agents such that each agent has mi=m
nimages and no access to other
ones for privacy constraints. However, the goal is still to make use of all images and to solve collaboratively
min
θ∈K1
nn/summationdisplay
i=11
mmi/summationdisplay
j=1Eu∼N(1,σu)ln(1 + exp(−uijyij.X/primeT
ijθ)) +c/bardblθ/bardbl2,
while reaching consensus on the decision variable θ∈KwithK= [−10,10]d. We note here that umodels
some perturbation on the local querying of every example to add to the randomization of the communication
process.
We consider classifying the digits 1and2withm= 12700 images. There are n= 100agents in the network
and thus each has a local batch of mi= 127images. We take σu= 0.01and letαk= 0.05(k+ 1)−0.75
andγk= 0.8(k+ 1)−0.25for 1P-DSG with vanishing step sizes, and α= 0.05andγ= 0.6with constant
step sizes. We choose Φk∈{−1√
d,1√
d}dwith equal probability. Also, every function query is subject to
a white noise generated by the standard normal distribution. For the DSGT algorithm, we set the step
size toαk= 0.015(k+ 1)−1when it is vanishing and α= 0.015when constant, and we do not consider
the perturbation on the objective function nor the noise on the objective function, only the noise on the
exact gradient. Similarly for EXTRA and we set its step size to α= 0.01. For 2P-DSG, we consider
αk= 0.01(k+ 1)−0.75andγk= 0.01(k+ 1)−0.25. For the centralized 1P-GD algorithm, we set α= 0.005
andγ= 0.5(α= 0.03andγ= 0.6with estimator (6)). We let c= 0.1, and the initialization be the same
for all algorithms, with θi,0uniformly chosen from [−0.5,0.5]d,∀i∈N, per instance. We ﬁnally average the
simulations over 30instances.
The expected evolution of the loss objective function is presented in Figure 1 and the graphs are zoomed
in on in Figure 2. Experimental results seem to validate our theoretical results: Our proposed algorithm
converges linearly fast with constant step sizes, however the ﬁnal gap is due to converging to an O(α)-
neighborhood of the optimal solution. 1P-DSG with vanishing step sizes converges with an O(1√
k)while
DSGT with vanishing step size converges at a rate of O(1
k). Using constant vs vanishing step size does not
12Published in Transactions on Machine Learning Research (11/2024)
seem to aﬀect the convergence rate of the loss function of DSGT. EXTRA consistently performs similarly
to DSGT. The most interesting point is that 1P-DSG, with vanishing and constant step sizes, outperforms
the centralized ZO counterpart 1P-GD highlighting an advantage of our gradient estimate. In addition,
there seems to be an evident advantage to using our one-point estimate to two-point one when the queries
are noisy, as our algorithm outperforms 2P-DSG. We also note that the estimate we use seem more stable
than the other ZO counterparts as shown in Figure 3 where we plot the average loss error bar of these
algorithms. A possible explanation is that not dividing by γin the estimate provides some stability against
noisy queries (this is slightly evident in 1P-GD with the diﬀerent estimators and in 1P-DSG vs. 2P-DSG).
Figure 1: Expected loss function evolution of the
proposedalgorithmvs. DSGT,EXTRA,and1P-
GD considering vanishing vs. constant step sizes.
Figure 2: Expected loss function evolution of the
proposedalgorithmvs. DSGT,EXTRA,and1P-
GD considering vanishing vs. constant step sizes.
Figure 3: The average loss error bar evolution of
the proposed algorithm vs. DSGT.
Figure4: Expectedtestaccuracyevolutionofthe
proposedalgorithmvs. DSGT,EXTRA,and1P-
GD considering vanishing vs. constant step sizes.
In Figure 4, we measure at every iteration the classiﬁcation accuracy against an independent test set of
2167images using the updated mean vector ¯θk=1
n/summationtextn
i=1θi,kof the local decision variables. The interest
of the constant step sizes appears in the convergence rate of this accuracy, where our algorithm is able to
compete with DSGT with full FO information, and to outperform DSGT with a vanishing step size. This
is an important result as it shows that the classiﬁcation goal with ZO is well met despite the limiting upper
bounds of convergence rate and that O(α)-neighborhood of the optimal solution achieved linearly fast can
be suﬃcient to achieve the best possible accuracy.
The reason for this better accuracy attainment is generally because the step sizes aﬀect the bound on the
generalizationerror. Forexample, Hardtetal.(2016)provetheoreticallythattheboundonthegeneralization
error for strongly convex objectives is smaller when the step sizes are constant (theorem 3.9) than when they
13Published in Transactions on Machine Learning Research (11/2024)
are vanishing (theorem 3.10), wherein the latter, there is an extra element containing the supremum of the
function. Naturally, the step sizes seem to play a role in aﬀecting the bound on the evolution of iterates,
which in turn aﬀects the uniform stability of the SGD method (stable means that the loss function is not
aﬀected much if one datapoint is diﬀerent) and the generalization error of an SGD-trained model is upper
bounded by the uniform stability bound.
This result seems to be conﬁrmed by the centralized 1P-GD vs 1P-DSG with vanishing step sizes. Despite
the latter outperforming the ﬁrst in convergence speed (of the objective function), the ﬁrst with constant
step sizes seems to generalize better.
In Figures 5, 6, and 7 the curves are those of the evolution of the expected consensus error, or E/bracketleftbig/summationtextn
i=1/bardblθi,k−
¯θk/bardbl2/bracketrightbig
which is the expected error between the local decision variables and their average. For all algorithms,
the error again validates the theoretical bounds and decreases quite fast. Generally, as evident in Figure 7
for all algorithms (expect 2P-DSG where a noise term is always multiplied by1
γk), vanishing step sizes allow
the consensus error to completely vanish while constant step sizes leave an O(α2)-gap.
We add other numerical examples for diﬀerent image labels in Appendix H.
5 Conclusion
In this work, we extended the distributed stochastic gradient algorithm to present a practical solution to
a relevant problem with realistic assumptions. A novel ZO algorithm was studied and proved to converge
with a biased and high variance one-point gradient estimate and a stochastic perturbation on the objective
function. In the context of noisy ZO optimization, we have successfully established a linear convergence rate
ofO(/rho1k)using ﬁxed step sizes and O(1√
k)with vanishing step sizes. These rates align with the optimal
expectations examined in the existing literature. We also prove a regret bound that of O(√
k)with vanishing
step sizes. A numerical application conﬁrmed the success and eﬃciency of the algorithm.
6 Acknowledgment
We are grateful for the comments made by the editor and the reviewers that have substantially improved
the quality of this work.
Figure 5: Expected consensus error evolution of
the proposed algorithm vs. DSGT and EXTRA
considering vanishing vs. constant step sizes.
Figure 6: Expected consensus error evolution of
the proposed algorithm vs. DSGT and EXTRA
considering vanishing vs. constant step sizes.
14Published in Transactions on Machine Learning Research (11/2024)
Figure 7: Expected consensus error evolution of
the proposed algorithm vs. DSGT and EXTRA
considering vanishing vs. constant step sizes.
References
Alekh Agarwal, Ofer Dekel, and Lin Xiao. Optimal algorithms for online convex optimization with multi-
point bandit feedback. In COLT, 2010.
Francis Bach and Vianney Perchet. Highly-smooth zero-th order online optimization vianney perchet, 2016.
URL https://arxiv.org/abs/1605.08165 .
Sébastien Bubeck, Ronen Eldan, and Yin Tat Lee. Kernel-based methods for bandit convex optimization. J.
ACM,68(4), jun2021. ISSN0004-5411. doi: 10.1145/3453721. URL https://doi.org/10.1145/3453721 .
Xiangyi Chen, Sijia Liu, Kaidi Xu, Xingguo Li, Xue Lin, Mingyi Hong, and David Cox. Zo-adamm: Zeroth-
order adaptive momentum method for black-box optimization. In NeurIPS , 2019.
YingChen, ZhiyongLiu, YongchaoZhang, YuanWu, XinChen, andLianZhao. Deepreinforcementlearning-
based dynamic resource management for mobile edge computing in industrial internet of things. IEEE
Transactions on Industrial Informatics , 17(7):4925–4934, 2021. doi: 10.1109/TII.2020.3028963.
Amir Daneshmand, Francisco Facchinei, Vyacheslav Kungurtsev, and Gesualdo Scutari. Hybrid ran-
dom/deterministicparallelalgorithmsforconvexandnonconvexbigdataoptimization. IEEE Transactions
on Signal Processing , 63(15):3914–3929, 2015. doi: 10.1109/TSP.2015.2436357.
Joseph L. Doob. Stochastic processes. 1953.
John C. Duchi, Michael I. Jordan, Martin J. Wainwright, and Andre Wibisono. Optimal rates for zero-order
convex optimization: The power of two function evaluations. IEEE Transactions on Information Theory ,
61(5):2788–2806, 2015. doi: 10.1109/TIT.2015.2409256.
Abraham Flaxman, Adam Tauman Kalai, and H. Brendan McMahan. Online convex optimization in the
bandit setting: gradient descent without a gradient. CoRR, cs.LG/0408007, 2004. URL http://arxiv.
org/abs/cs.LG/0408007 .
Daniel Golovin, John Karro, Greg Kochanski, Chansoo Lee, Xingyou Song, and Qiuyi (Richard) Zhang.
Gradientless descent: High-dimensional zeroth-order optimization. CoRR, abs/1911.06317, 2019. URL
http://arxiv.org/abs/1911.06317 .
Eduard Gorbunov, Pavel Dvurechensky, and Alexander Gasnikov. An accelerated method for derivative-free
smooth stochastic convex optimization, 2018. URL https://arxiv.org/abs/1802.09022 .
15Published in Transactions on Machine Learning Research (11/2024)
Davood Hajinezhad, Mingyi Hong, and Alfredo Garcia. Zone: Zeroth-order nonconvex multiagent op-
timization over networks. IEEE Transactions on Automatic Control , 64(10):3995–4010, 2019. doi:
10.1109/TAC.2019.2896025.
Moritz Hardt, Benjamin Recht, and Yoram Singer. Train faster, generalize better: Stability of stochastic
gradient descent, 2016. URL https://arxiv.org/abs/1509.01240 .
Elad Hazan and Kﬁr Levy. Bandit convex optimization: Towards tight bounds. In Z. Ghahramani,
M. Welling, C. Cortes, N. Lawrence, and K.Q. Weinberger (eds.), Advances in Neural Information Pro-
cessing Systems , volume 27. Curran Associates, Inc., 2014. URL https://proceedings.neurips.cc/
paper_files/paper/2014/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf .
Shinji Ito. An optimal algorithm for bandit convex optimization with strongly-convex and smooth loss. In
Silvia Chiappa and Roberto Calandra (eds.), Proceedings of the Twenty Third International Conference
on Artiﬁcial Intelligence and Statistics , volume 108 of Proceedings of Machine Learning Research , pp.
2229–2239. PMLR, 26–28 Aug 2020. URL https://proceedings.mlr.press/v108/ito20a.html .
Dusan Jakovetic, Dragana Bajovic, Anit Kumar Sahu, and Soummya Kar. Convergence rates for distributed
stochastic optimization over random networks. In 2018 IEEE Conference on Decision and Control (CDC) ,
pp. 4238–4245, 2018. doi: 10.1109/CDC.2018.8619228.
Kevin G. Jamieson, Robert D. Nowak, and Benjamin Recht. Query complexity of derivative-free optimiza-
tion. InNIPS, 2012.
DavidKinderlehrerandGuidoStampacchia. Anintroductiontovariationalinequalitiesandtheirapplication.
31, 01 2000. doi: 10.1137/1.9780898719451.
Anit Kumar Sahu, Dusan Jakovetic, Dragana Bajovic, and Soummya Kar. Distributed zeroth order opti-
mization over random networks: A kiefer-wolfowitz stochastic approximation approach. In 2018 IEEE
Conference on Decision and Control (CDC) , pp. 4951–4958, 2018. doi: 10.1109/CDC.2018.8619044.
Yann LeCun and Corinna Cortes. The mnist database of handwritten digits. 2005.
Wenjie Li and Mohamad Assaad. Distributed stochastic optimization in networks with low informational
exchange. IEEE Transactions on Information Theory , 67(5):2989–3008, 2021. doi: 10.1109/TIT.2021.
3064925.
Sijia Liu, Jie Chen, Pin-Yu Chen, and Alfred Hero. Zeroth-order online alternating direction method of
multipliers: Convergence analysis and applications. In Amos Storkey and Fernando Perez-Cruz (eds.),
Proceedings of the Twenty-First International Conference on Artiﬁcial Intelligence and Statistics , vol-
ume 84 of Proceedings of Machine Learning Research , pp. 288–297. PMLR, 09–11 Apr 2018. URL
https://proceedings.mlr.press/v84/liu18a.html .
Sijia Liu, Pin-Yu Chen, Xiangyi Chen, and Mingyi Hong. signsgd via zeroth-order oracle. In ICLR, 2019.
Sijia Liu, Pin-Yu Chen, Bhavya Kailkhura, Gaoyuan Zhang, Alfred O. Hero III, and Pramod K. Varshney. A
primeronzeroth-orderoptimizationinsignalprocessingandmachinelearning: Principals, recentadvances,
and applications. IEEE Signal Processing Magazine , 37(5):43–54, 2020. doi: 10.1109/MSP.2020.3003837.
Yuyi Mao, Changsheng You, Jun Zhang, Kaibin Huang, and Khaled B. Letaief. A survey on mobile edge
computing: The communication perspective. IEEE Communications Surveys Tutorials , 19(4):2322–2358,
2017. doi: 10.1109/COMST.2017.2745201.
Ion Matei and John S. Baras. Performance evaluation of the consensus-based distributed subgradient method
under random communication topologies. IEEE Journal of Selected Topics in Signal Processing , 5(4):754–
771, 2011. doi: 10.1109/JSTSP.2011.2120593.
16Published in Transactions on Machine Learning Research (11/2024)
Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas.
Communication-Eﬃcient Learning of Deep Networks from Decentralized Data. In Aarti Singh and Jerry
Zhu (eds.), Proceedings of the 20th International Conference on Artiﬁcial Intelligence and Statistics , vol-
ume 54 of Proceedings of Machine Learning Research , pp. 1273–1282. PMLR, 20–22 Apr 2017. URL
https://proceedings.mlr.press/v54/mcmahan17a.html .
Elissa Mhanna and Mohamad Assaad. Single point-based distributed zeroth-order optimization with a non-
convex stochastic objective function. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara
Engelhardt, Sivan Sabato, and Jonathan Scarlett (eds.), Proceedings of the 40th International Conference
on Machine Learning , volume 202 of Proceedings of Machine Learning Research , pp. 24701–24719. PMLR,
23–29 Jul 2023. URL https://proceedings.mlr.press/v202/mhanna23a.html .
Angelia Nedić and Alex Olshevsky. Stochastic gradient-push for strongly convex functions on time-varying
directed graphs. IEEE Transactions on Automatic Control , 61(12):3936–3947, 2016. doi: 10.1109/TAC.
2016.2529285.
A. Nemirovski, A. Juditsky, G. Lan, and A. Shapiro. Robust stochastic approximation approach to stochastic
programming. SIAM Journal on Optimization , 19(4):1574–1609, 2009. doi: 10.1137/070704277. URL
https://doi.org/10.1137/070704277 .
Yurii Nesterov and Vladimir G. Spokoiny. Random gradient-free minimization of convex functions. Foun-
dations of Computational Mathematics , 17:527–566, 2017.
Shi Pu. A robust gradient tracking method for distributed optimization over directed networks. In 2020
59th IEEE Conference on Decision and Control (CDC) , pp. 2335–2341, 2020. doi: 10.1109/CDC42340.
2020.9303917.
Shi Pu and Angelia Nedić. Distributed stochastic gradient tracking methods, 2018. URL https://arxiv.
org/abs/1805.11454 .
Guannan Qu and Na Li. Harnessing smoothness to accelerate distributed optimization. IEEE Transactions
on Control of Network Systems , 5(3):1245–1260, 2018. doi: 10.1109/TCNS.2017.2698261.
Ohad Shamir. On the complexity of bandit and derivative-free stochastic convex optimization. In Shai
Shalev-Shwartz and Ingo Steinwart (eds.), Proceedings of the 26th Annual Conference on Learning Theory ,
volume 30 of Proceedings of Machine Learning Research , pp. 3–24, Princeton, NJ, USA, 12–14 Jun 2013.
PMLR. URL https://proceedings.mlr.press/v30/Shamir13.html .
Wei Shi, Qing Ling, Gang Wu, and Wotao Yin. Extra: An exact ﬁrst-order algorithm for decentralized
consensus optimization. SIAM Journal on Optimization , 25(2):944–966, 2015. doi: 10.1137/14096668X.
URL https://doi.org/10.1137/14096668X .
Zai Shi and Atilla Eryilmaz. A zeroth-order admm algorithm for stochastic optimization over distributed
processing networks. In IEEE INFOCOM 2020 - IEEE Conference on Computer Communications , pp.
726–735, 2020. doi: 10.1109/INFOCOM41043.2020.9155520.
Yujie Tang, Junshan Zhang, and Na Li. Distributed zero-order algorithms for nonconvex multiagent opti-
mization. IEEE Transactions on Control of Network Systems , 8(1):269–281, 2021. doi: 10.1109/TCNS.
2020.3024321.
Zaid J. Towﬁc, Jianshu Chen, and Ali H. Sayed. Excess-risk of distributed stochastic learners. IEEE
Transactions on Information Theory , 62(10):5753–5785, 2016. doi: 10.1109/TIT.2016.2593769.
Sheng-Yuan Tu and Ali H. Sayed. Diﬀusion strategies outperform consensus strategies for distributed es-
timation over adaptive networks. IEEE Transactions on Signal Processing , 60(12):6217–6234, 2012. doi:
10.1109/TSP.2012.2217338.
Anirudh Vemula, Wen Sun, and J. Andrew Bagnell. Contrasting exploration in parameter and action space:
A zeroth-order optimization perspective. ArXiv, abs/1901.11503, 2019.
17Published in Transactions on Machine Learning Research (11/2024)
Yuanyu Wan, Wei-Wei Tu, and Lijun Zhang. Projection-free distributed online convex optimization with
o(√
T)communication complexity. In Hal Daumé III and Aarti Singh (eds.), Proceedings of the 37th
International Conference on Machine Learning , volume 119 of Proceedings of Machine Learning Research ,
pp. 9818–9828. PMLR, 13–18 Jul 2020. URL https://proceedings.mlr.press/v119/wan20b.html .
Yuanyu Wan, Guanghui Wang, Wei-Wei Tu, and Lijun Zhang. Projection-free distributed online learning
with sublinear communication complexity. Journal of Machine Learning Research , 23(172):1–53, 2022.
URL http://jmlr.org/papers/v23/20-1239.html .
Ran Xin, Anit Kumar Sahu, Usman A. Khan, and Soummya Kar. Distributed stochastic optimization
with gradient tracking over strongly-connected networks. In 2019 IEEE 58th Conference on Decision and
Control (CDC) , pp. 8353–8358, 2019. doi: 10.1109/CDC40024.2019.9029217.
Kun Yuan, Qing Ling, and Wotao Yin. On the convergence of decentralized gradient descent. SIAM Journal
on Optimization , 26(3):1835–1854, 2016. doi: 10.1137/130943170. URL https://doi.org/10.1137/
130943170 .
Fanqin Zhou, Lei Feng, Michel Kadoch, Peng Yu, Wenjing Li, and Zhili Wang. Multiagent rl aided task
oﬄoading and resource management in wi-ﬁ 6 and 5g coexisting industrial wireless environment. IEEE
Transactions on Industrial Informatics , 18(5):2923–2933, 2022. doi: 10.1109/TII.2021.3106973.
A L-Smoothness Property
/bardbl∇F (¯x)−h(x)/bardbl=/vextenddouble/vextenddouble/vextenddouble1
nn/summationdisplay
i=1/parenleftBig
∇Fi(¯x)−∇Fi(xi)/parenrightBig/vextenddouble/vextenddouble/vextenddouble
≤1
nn/summationdisplay
i=1/vextenddouble/vextenddouble/vextenddouble∇Fi(¯x)−∇Fi(xi)/vextenddouble/vextenddouble/vextenddouble
≤L
nn/summationdisplay
i=1/bardbl¯x−xi/bardbl
=L
nn/summationdisplay
i=1/bardblxi−¯x/bardbl
(a)
≤L
n/radicaltp/radicalvertex/radicalvertex/radicalbtnn/summationdisplay
i=1/bardblxi−¯x/bardbl2
(b)=L√n/bardblx−1¯x/bardbl,
where (a)is by applying the Cauchy-Schwarz inequality, |/summationtextn
i=1ai·1| ≤ (/summationtextn
i=1a2
i)1
2·(/summationtextn
i=112)1
2=
n1
2(/summationtextn
i=1a2
i)1
2, and (b)is by deﬁnition of the Frobenius norm, /bardblx−1¯x/bardbl2=/summationtextn
i=1/bardblxi−¯x/bardbl2.
B Estimated Gradient
In this section, we derive the bias of the gradient estimate with respect to the real gradient of the local
objective function. Let
˘gi,k=ES,Φ,ζ[gi,k|Hk].
Thus, by Assumption 1.3 and the deﬁnition in (4),
˘gi,k=ES,Φ,ζ[Φi,k(fi(xi,k+γkΦi,k,Si,k) +ζi,k)|Hk]
=ES,Φ[Φi,kfi(xi,k+γkΦi,k,Si,k)|Hk]
=EΦ[Φi,kFi(xi,k+γkΦi,k)|Hk].
18Published in Transactions on Machine Learning Research (11/2024)
By Taylor’s theorem and the mean-valued theorem, there exists ˜xi,klocated between xi,kandxi,k+γkΦi,k
where
Fi(xi,k+γkΦi,k) =Fi(xi,k) +γk/angbracketleftΦi,k,∇Fi(xi,k)/angbracketright+γ2
k
2/angbracketleftΦi,k,∇2Fi(˜xi,k)Φi,k/angbracketright,
substituting in the previous deﬁnition,
˘gi,k=Fi(xi,k)EΦ[Φi,k] +γkEΦ[Φi,kΦT
i,k]∇Fi(xi,k) +γ2
k
2EΦ[Φi,kΦT
i,k∇2Fi(˜xi,k)Φi,k|Hk]
=c3γk[∇Fi(xi,k) +bi,k].
Thus, the estimation bias has the form
bi,k=˘gi,k
c3γk−∇Fi(xi,k)
=γk
2c3EΦ[Φi,kΦT
i,k∇2Fi(˜xi,k)Φi,k|Hk].
Let Assumptions 1.2 and 2.2 hold. Then, we can bound the bias as
/bardblbi,k/bardbl≤γk
2c3EΦ[/bardblΦi,k/bardbl2/bardblΦT
i,k/bardbl2/bardbl∇2Fi(˜xi,k)/bardbl2/bardblΦi,k/bardbl2|Hk]
≤γkc3
4c1
2c3.(20)
We can see/bardblbi,k/bardbl→0ask→∞sinceγkis vanishing. We remark that
˜gk=E[¯gk|Hk]
=1
nn/summationdisplay
i=1E[gi,k|Hk]
=1
nn/summationdisplay
i=1c3γk[∇Fi(xi,k) +bi,k]
=c3γk[h(xk) +¯bk](21)
is also a biased estimator of h(xk)with
/bardbl¯bk/bardbl=/bardbl1
nn/summationdisplay
i=1bi,k/bardbl
≤1
nn/summationdisplay
i=1/bardblbi,k/bardbl
≤1
nn/summationdisplay
i=1γkc3
4c1
2c3
=γkc3
4c1
2c3.(22)
Lemma B.1. Let all Assumptions 1.3, 2.2, and 2.4 hold, then there exists a bounded constant ¯M > 0, such
thatE[/bardbl¯gk/bardbl2|Hk]<¯M.
Proof.∀i∈N, we have
E[/bardblgi,k/bardbl2|Hk] =E[/bardblΦi,k(fi(xi,k+γkΦi,k,Si,k) +ζi,k)/bardbl2|Hk]
=E[/bardblΦi,k/bardbl2/bardblfi(xi,k+γkΦi,k,Si,k) +ζi,k/bardbl2|Hk]
(a)
≤c2
4E[(fi(xi,k+γkΦi,k,Si,k) +ζi,k)2|Hk]
(b)=c2
4E[f2
i(xi,k+γkΦi,k,Si,k)|Hk] +c2
4c2
(f)
<∞,
19Published in Transactions on Machine Learning Research (11/2024)
where (a)is due to Assumption 2.2, (b)Assumption 1.3, and (c)Assumption 2.4.
Then, E[/bardblgk/bardbl2|Hk] =E/bracketleftBig/summationtextn
i=1/bardblgi,k/bardbl2/vextendsingle/vextendsingle/vextendsingleHk/bracketrightBig
=/summationtextn
i=1E[/bardblgi,k/bardbl2|Hk]<∞and
E[/bardbl¯gk/bardbl2|Hk] =E/bracketleftBig
/bardbl1
nn/summationdisplay
i=1gi,k/bardbl2/vextendsingle/vextendsingle/vextendsingleHk/bracketrightBig
=1
n2E/bracketleftBig
/bardbln/summationdisplay
i=1gi,k/bardbl2/vextendsingle/vextendsingle/vextendsingleHk/bracketrightBig
≤n
n2E/bracketleftBign/summationdisplay
i=1/bardblgi,k/bardbl2/vextendsingle/vextendsingle/vextendsingleHk/bracketrightBig
=1
nn/summationdisplay
i=1E[/bardblgi,k/bardbl2|Hk]
<∞.
C Stochastic Noise
To prove Lemma 3.1, we begin by demonstrating that the sequence {/summationtextK/prime
k=Kαkek}K/prime≥Kis a martingale. To
do so, we have to prove that for all K/prime≥K,XK/prime=/summationtextK/prime
k=Kαkeksatisﬁes the following two conditions:
(i)E[XK/prime+1|XK/prime] =XK/prime
(ii)E[/bardblXK/prime/bardbl2]<∞
We know that
E[ek] =E[¯gk−E[¯gk|Hk]] =EHk/bracketleftBig
E/bracketleftBig
¯gk−E[¯gk|Hk]/vextendsingle/vextendsingle/vextendsingleHk/bracketrightBig/bracketrightBig
= 0
by the law of total expectation. Hence,
E[XK/prime+1|XK/prime] =E/bracketleftBig
αK/prime+1eK/prime+1+K/prime/summationdisplay
k=Kαkek/vextendsingle/vextendsingle/vextendsingleK/prime/summationdisplay
k=Kαkek/bracketrightBig
= 0 +K/prime/summationdisplay
k=Kαkek=XK/prime. (23)
In addition, ekandek/primeare uncorrelated for any k/negationslash=k/primesince (assuming k>k/prime)E/bracketleftbig
eT
kek/prime/bracketrightbig
=E/bracketleftbig
E[eT
kek/prime|Hk]/bracketrightbig
=
E/bracketleftbig
ek/primeE[eT
k|Hk]/bracketrightbig
= 0. Thus,
E(/bardblK/prime/summationdisplay
k=Kαkek/bardbl2) =E(K/prime/summationdisplay
k=KK/prime/summationdisplay
k/prime=Kαkαk/prime/angbracketleftek,ek/prime/angbracketright)
(a)=E(K/prime/summationdisplay
k=K/bardblαkek/bardbl2)
≤∞/summationdisplay
k=KE(α2
k/bardbl¯gk−E[¯gk|Hk]/bardbl2)
=∞/summationdisplay
k=Kα2
kE(/bardbl¯gk/bardbl2)−EHk(/bardblE[¯gk|Hk]/bardbl2)
≤∞/summationdisplay
k=Kα2
kE(/bardbl¯gk/bardbl2)
(b)
≤M∞/summationdisplay
k=Kα2
k(c)
<∞,(24)
20Published in Transactions on Machine Learning Research (11/2024)
where (a)is due to the uncorrelatedness E[/angbracketleftek,ek/prime/angbracketright] = 0,(b)is by Lemma B.1, and (c)is by Assumption
2.1. Therefore, both (i) and (ii) are satisﬁed and we can say that {/summationtextK/prime
k=Kαkek}K/prime≥Kis a martingale. This
permits us to use Doob’s martingale inequality Doob (1953):
For any constant ν >0,
P( sup
K/prime≥K/bardblK/prime/summationdisplay
k=Kαkek/bardbl≥ν)≤1
ν2E(/bardblK/prime/summationdisplay
k=Kαkek/bardbl2)
(a)
≤M
ν2∞/summationdisplay
k=Kα2
k,(25)
where (a)is following the exact same steps as (24).
SinceMis a bounded constant and limK→∞/summationtext∞
k=Kα2
k= 0by Assumption 2.1, we get
limK→∞M
ν2/summationtext∞
k=Kα2
k= 0for any bounded constant ν. Hence, the probability that /bardbl/summationtextK/prime
k=Kαkek/bardbl≥ν
also vanishes as K→∞, which concludes the proof.
D Proof of Convergence
We start by stating the following lemma that will be useful for the proof of convergence.
Lemma D.1. If all Assumptions 1.1-1.3, 2.1-2.2, and 2.4 hold, then limk→∞/bardblxk−1¯xk/bardbl2= 0. In fact, we
have
∞/summationdisplay
k=0/bardblxk−1¯xk/bardbl2<∞,∞/summationdisplay
k=0/bardblzk+1−1¯xk/bardbl2<∞,and∞/summationdisplay
k=0γkαk/bardblxk−1¯xk/bardbl<∞,
almost surely.
Proof: See Appendix D.2.
D.1 Proof of Theorem 3.2
By using the compact form of the algorithm in (8), we know that
¯zk+1=1
n1TW(xk−αkgk)(a)=1
n1T(xk−αkgk) = ¯xk−αk¯gk, (26)
where (a)is again due to the doubly stochastic property of W.
The divergence at time k+ 1can then be written as
dk+1=/bardbl¯xk+1−x∗/bardbl2
=/bardbl1
nn/summationdisplay
i=1(xi,k+1−x∗)/bardbl2
≤n
n2n/summationdisplay
i=1/bardblxi,k+1−x∗/bardbl2
(a)
≤1
nn/summationdisplay
i=1/bardblzi,k+1−x∗/bardbl2
21Published in Transactions on Machine Learning Research (11/2024)
=1
nn/summationdisplay
i=1/bardblzi,k+1−¯xk+ ¯xk−x∗/bardbl2
=1
nn/summationdisplay
i=1/bardblzi,k+1−¯xk/bardbl2+ 21
nn/summationdisplay
i=1/angbracketleftzi,k+1−¯xk,¯xk−x∗/angbracketright+1
nn/summationdisplay
i=1/bardbl¯xk−x∗/bardbl2
=1
n/bardblzk+1−1¯xk/bardbl2+ 2/angbracketleft¯zk+1−¯xk,¯xk−x∗/angbracketright+/bardbl¯xk−x∗/bardbl2
(b)=1
n/bardblzk+1−1¯xk/bardbl2+ 2/angbracketleft−αk¯gk,¯xk−x∗/angbracketright+dk
=dk−2αk/angbracketleft¯xk−x∗,¯gk−E[¯gk|Hk] +E[¯gk|Hk]/angbracketright+1
n/bardblzk+1−1¯xk/bardbl2
=dk−2αk/angbracketleft¯xk−x∗,E[¯gk|Hk]/angbracketright−2αk/angbracketleft¯xk−x∗,ek/angbracketright+1
n/bardblzk+1−1¯xk/bardbl2
(c)=dk−2c3γkαk/angbracketleft¯xk−x∗,h(xk) +¯bk/angbracketright−2αk/angbracketleft¯xk−x∗,ek/angbracketright+1
n/bardblzk+1−1¯xk/bardbl2
=dk−2c3γkαk/angbracketleft¯xk−x∗,∇F(¯xk)/angbracketright+ 2c3γkαk/angbracketleft¯xk−x∗,∇F(¯xk)−h(xk)/angbracketright
−2c3γkαk/angbracketleft¯xk−x∗,¯bk/angbracketright−2αk/angbracketleft¯xk−x∗,ek/angbracketright+1
n/bardblzk+1−1¯xk/bardbl2
(d)
≤dk−2c3γkαk/angbracketleft¯xk−x∗,∇F(¯xk)/angbracketright+2c3Lγkαk√n/bardbl¯xk−x∗/bardbl/bardblxk−1¯xk/bardbl
+ 2c3γkαk/bardbl¯xk−x∗/bardbl/bardbl¯bk/bardbl−2αk/angbracketleft¯xk−x∗,ek/angbracketright+1
n/bardblzk+1−1¯xk/bardbl2,(27)
where (a)is by the projection inequality (5) noting that x∗∈K(so projecting it onto Kgives us the same
point), (b)is by (26), (c)is due to (21), and (d)is due to Lemma 1.5.
By recursion of inequality (27), we have
dK+1≤d0−2c3K/summationdisplay
k=0γkαk/angbracketleft¯xk−x∗,∇F(¯xk) +¯bk/angbracketright+2c3L√nK/summationdisplay
k=0γkαk/bardbl¯xk−x∗/bardbl/bardblxk−1¯xk/bardbl
+ 2c3K/summationdisplay
k=0γkαk/bardbl¯xk−x∗/bardbl/bardbl¯bk/bardbl−2K/summationdisplay
k=0αk/angbracketleft¯xk−x∗,ek/angbracketright+1
nK/summationdisplay
k=0/bardblzk+1−1¯xk/bardbl2.(28)
By Lemma 3.1, we have limK→∞/bardbl/summationtextK
k=0αkek/bardbl<∞almost surely. Since /bardbl¯xk−x∗/bardbl<∞by the compactness
ofKin Assumption 2.4, hence
lim
K→∞/bardblK/summationdisplay
k=0αk/angbracketleft¯xk−x∗,ek/angbracketright/bardbl<∞. (29)
From (42) in Lemma D.1, we have
lim
K→∞K/summationdisplay
k=0/bardblzk+1−1¯xk/bardbl2<∞. (30)
As stated in Lemma D.1, we have/summationtext∞
k=0γkαk/bardblxk−1¯xk/bardbl<∞, adding to/bardbl¯xk−x∗/bardbl<∞by Assumption
2.4, then
lim
K→∞K/summationdisplay
k=0γkαk/bardbl¯xk−x∗/bardbl/bardblxk−1¯xk/bardbl<∞. (31)
By (22), we know that /bardbl¯bk/bardbl≤c3
4c1
2c3γkand/bardbl¯xk−x∗/bardbl<∞by Assumption 2.4,
lim
K→∞K/summationdisplay
k=0γ2
kαk/bardbl¯xk−x∗/bardbl<∞, (32)
22Published in Transactions on Machine Learning Research (11/2024)
by Assumption 2.1.
From the above inequalities (28)-(32), we see that there exists 0<D/prime<∞such thatdK+1≤D/prime+zK, with
zKdeﬁned as
zK=−2c3K/summationdisplay
k=0γkαk/angbracketleft¯xk−x∗,∇F(¯xk)/angbracketright. (33)
By the strong convexity, we have
−/angbracketleft¯xk−x∗,∇F(¯xk)/angbracketright≤F (x∗)−F(¯xk)−λ
2/bardbl¯xk−x∗/bardbl2≤0, (34)
asF(¯xk)≥F(x∗)by the deﬁnition of x∗being the optimum in Kand¯xk∈K(by the property of a convex
set).
Thus,zK≤0, conﬁrming dK+1<∞.
Let’s assume that ∀/epsilon1h>0,∃Khsuch that/bardbl¯xk−x∗/bardbl2>/epsilon1hfork≥Kh, meaning
lim
K→∞−K/summationdisplay
k=Khγkαk/bardbl¯xk−x∗/bardbl2<−/epsilon1hlim
K→∞K/summationdisplay
k=Khγkαk<−∞, (35)
since/summationtext
kαkγkdiverges by Assumption 2.1. However, this implies that zK<−∞and as a consequence,
dK+1≤D/prime+zK<−∞which is a contradiction as dK+1≥0. We conclude that limk→∞dk= 0and
limk→∞¯xk=x∗, almost surely.
D.2 Proof of Lemma D.1
The goal is to bound /bardblxk+1−1¯xk+1/bardbl2by/bardblxk−1¯xk/bardbl2and other vanishing terms.
/bardblxk+1−1¯xk+1/bardbl2=/bardblxk+1−1¯xk+1¯xk−1¯xk+1/bardbl2
=/bardblxk+1−1¯xk/bardbl2+ 2/angbracketleftxk+1−1¯xk,1¯xk−1¯xk+1/angbracketright+/bardbl1¯xk−1¯xk+1/bardbl2
(a)=/bardblxk+1−1¯xk/bardbl2−/bardbl1¯xk−1¯xk+1/bardbl2
≤/bardblxk+1−1¯xk/bardbl2
=n/summationdisplay
i=1/bardblxi,k+1−¯xk/bardbl2
(b)
≤n/summationdisplay
i=1/bardblzi,k+1−¯xk/bardbl2
=/bardblzk+1−1¯xk/bardbl2
=/bardblWxk−αkWgk−1¯xk/bardbl2
=/bardblWxk−1¯xk/bardbl2−2αk/angbracketleftWxk−1¯xk,Wgk/angbracketright+α2
k/bardblWgk/bardbl2
(c)
≤/bardblWxk−1¯xk/bardbl2+αk[1−ρ2
w
2ρ2wαk/bardblWxk−1¯xk/bardbl2+2ρ2
wαk
1−ρ2w/bardblWgk/bardbl2] +α2
k/bardblWgk/bardbl2
(d)
≤ρ2
w/bardblxk−1¯xk/bardbl2+αk[1−ρ2
w
2αk/bardblxk−1¯xk/bardbl2+2ρ2
wαk
1−ρ2w/bardblWgk/bardbl2] +α2
k/bardblWgk/bardbl2
=1 +ρ2
w
2/bardblxk−1¯xk/bardbl2+α2
k1 +ρ2
w
1−ρ2w/bardblWgk/bardbl2
=1 +ρ2
w
2/bardblxk−1¯xk/bardbl2+α2
k1 +ρ2
w
1−ρ2w/bardblWgk−1¯gk+1¯gk/bardbl2
(e)=1 +ρ2
w
2/bardblxk−1¯xk/bardbl2+α2
k1 +ρ2
w
1−ρ2w/bardblWgk−1¯gk/bardbl2+α2
kn(1 +ρ2
w)
1−ρ2w/bardbl¯gk/bardbl2
23Published in Transactions on Machine Learning Research (11/2024)
≤1 +ρ2
w
2/bardblxk−1¯xk/bardbl2+α2
kρ2
w(1 +ρ2
w)
1−ρ2w/bardblgk−1¯gk/bardbl2+α2
kn(1 +ρ2
w)
1−ρ2w/bardbl¯gk/bardbl2
(f)
≤1 +ρ2
w
2/bardblxk−1¯xk/bardbl2+α2
kn(1 +ρ2
w)
1−ρ2wM.(36)
where (a)is by (37), (b)is the projection inequality (5) noting that ¯xk∈KsinceKis a convex set (so
projecting it onto Kgives us the same point), (c)is by−2/epsilon1×1
/epsilon1/angbracketlefta,b/angbracketright=−2/angbracketleft/epsilon1a,1
/epsilon1b/angbracketright≤/epsilon12/bardbla/bardbl2+1
/epsilon12/bardblb/bardbl2(d)is
by Lemma 1.4, (e)is by (38), and (f)is by (39) and (40).
2/angbracketleftxk+1−1¯xk,1¯xk−1¯xk+1/angbracketright=2n/summationdisplay
i=1/angbracketleftxi,k+1−¯xk,¯xk−¯xk+1/angbracketright
=2/angbracketleftn/summationdisplay
i=1(xi,k+1−¯xk),¯xk−¯xk+1/angbracketright
=2/angbracketleftn(¯xk+1−¯xk),¯xk−¯xk+1/angbracketright
=−2n/angbracketleft¯xk−¯xk+1,¯xk−¯xk+1/angbracketright
=−2n/bardbl¯xk−¯xk+1/bardbl2
=−2/bardbl1¯xk−1¯xk+1/bardbl2.(37)
/angbracketleftWgk−1¯gk,1¯gk/angbracketright=n/summationdisplay
i=1/angbracketleftn/summationdisplay
j=1wijgj,k−¯gk,¯gk/angbracketright
=/angbracketleftn/summationdisplay
i=1n/summationdisplay
j=1wijgj,k−n¯gk,¯gk/angbracketright
=/angbracketleftn/summationdisplay
j=1(n/summationdisplay
i=1wij)gj,k−n¯gk,¯gk/angbracketright
=/angbracketleftn/summationdisplay
j=1gj,k−n¯gk,¯gk/angbracketright
=0.(38)
From Lemma B.1, we know that /bardbl¯gk/bardbl2≤M <∞almost surely,
/bardblgk−1¯gk/bardbl2=n/summationdisplay
i=1/bardblgi,k−1
nn/summationdisplay
j=1gj,k/bardbl2
=n/summationdisplay
i=1/parenleftbigg
/bardblgi,k/bardbl2−2/angbracketleftgi,k,1
nn/summationdisplay
j=1gj,k/angbracketright+/bardbl¯gk/bardbl2/parenrightbigg
=/bardblgk/bardbl2−2n/bardbl¯gk/bardbl2+n/bardbl¯gk/bardbl2
=/bardblgk/bardbl2−n/bardbl¯gk/bardbl2(39)
Then,
ρ2
w/bardblgk−1¯gk/bardbl2+n/bardbl¯gk/bardbl2=ρ2
w/bardblgk/bardbl2+n(1−ρ2
w)/bardbl¯gk/bardbl2
≤ρ2
wnM+n(1−ρ2
w)M
=nM.(40)
24Published in Transactions on Machine Learning Research (11/2024)
1.Proving limK→∞/summationtextK
k=0/bardblxk−1¯xk/bardbl2<∞,limK→∞/summationtextK
k=0/bardblzk+1−1¯xk/bardbl2<∞, and limk→∞/bardblxk−
1¯xk/bardbl2= 0
Reconsider (36),
/bardblxk+1−1¯xk+1/bardbl2≤1 +ρ2
w
2/bardblxk−1¯xk/bardbl2+α2
kn(1 +ρ2
w)
1−ρ2wM
/bardblxk−1¯xk/bardbl2≤1 +ρ2
w
2/bardblxk−1−1¯xk−1/bardbl2+α2
k−1n(1 +ρ2
w)
1−ρ2wM
...
/bardblx1−1¯x1/bardbl2≤1 +ρ2
w
2/bardblx0−1¯x0/bardbl2+α2
0n(1 +ρ2
w)
1−ρ2wM.(41)
Adding all inequalities in (41), we obtain
/bardblxk+1−1¯xk+1/bardbl2≤−1−ρ2
w
2k/summationdisplay
l=1/bardblxl−1¯xl/bardbl2+1 +ρ2
w
2/bardblx0−1¯x0/bardbl2+n(1 +ρ2
w)
1−ρ2wMk/summationdisplay
l=0α2
l
Letk→∞, then the second and third terms are bounded due to Assumption 2.1. There are then
2 cases:/summationtext
l/bardblxl−1¯xl/bardbl2either diverges or converges. Assume the validity of the hypothesis H2)/summationtext
l/bardblxl−1¯xl/bardbl2diverges, i.e.,/summationtext∞
l=1/bardblxl−1¯xl/bardbl2→∞. This leads to
/bardblxk+1−1¯xk+1/bardbl2<−∞,
as−1−ρ2
w
2<0. However,/bardblxk+1−1¯xk+1/bardbl2should be positive. Thus, hypothesis H2cannot be true
and/summationtext
l/bardblxl−1¯xl/bardbl2converges. Hence, limk→∞/bardblxk−1¯xk/bardbl2= 0almost surely.
Thus, reconsider (36),
/bardblzk+1−1¯xk/bardbl2≤1 +ρ2
w
2/bardblxk−1¯xk/bardbl2+α2
kn(1 +ρ2
w)
1−ρ2wM
K/summationdisplay
k=0/bardblzk+1−1¯xk/bardbl2≤1 +ρ2
w
2K/summationdisplay
k=0/bardblxk−1¯xk/bardbl2+n(1 +ρ2
w)
1−ρ2wMK/summationdisplay
k=0α2
k
<∞.(42)
2.Proving/summationtext∞
k=0γkαk/bardblxk−1¯xk/bardbl<∞
By induction from (36), we have
/bardblxk+1−1¯xk+1/bardbl2≤/parenleftbig1 +ρ2
w
2/parenrightbigk+1/bardblx0−1¯x0/bardbl2+2nM
1−ρ2wk/summationdisplay
j=0/parenleftbig1 +ρ2
w
2/parenrightbigj+1α2
k−j.(43)
Since√
a+b<√a+√
b,
/bardblxk+1−1¯xk+1/bardbl≤/parenleftbig1 +ρ2
w
2/parenrightbigk+1
2/bardblx0−1¯x0/bardbl+/radicalBigg
2nM
1−ρ2wk/summationdisplay
j=0/parenleftbig1 +ρ2
w
2/parenrightbigj+1
2αk−j.(44)
Then, substituting into the sum/summationtext∞
k=0γkαk/bardblxk−1¯xk/bardbl,
∞/summationdisplay
k=1γkαk/parenleftBigg
/parenleftbig1 +ρ2
w
2/parenrightbigk
2/bardblx0−1¯x0/bardbl+/radicalBigg
2nM
1−ρ2wk−1/summationdisplay
j=0/parenleftbig1 +ρ2
w
2/parenrightbigj+1
2αk−1−j/parenrightBigg
≤γ0α0/bardblx0−1¯x0/bardbl/radicalbig
1 +ρ2w√
2−/radicalbig
1 +ρ2w+/radicalBigg
2nM
1−ρ2w∞/summationdisplay
k=1γkαkk−1/summationdisplay
j=0/parenleftbig1 +ρ2
w
2/parenrightbigj+1
2αk−1−j,
25Published in Transactions on Machine Learning Research (11/2024)
where the inequality is due to the fact that γkandαkare both decreasing step-sizes and we have a
geometric sum of ratio/radicalBig
1+ρ2
2<1. We then study the sums in the second term,
∞/summationdisplay
k=1γkαkk−1/summationdisplay
j=0/parenleftbig1 +ρ2
w
2/parenrightbigj+1
2αk−1−j≤∞/summationdisplay
k=1γkk−1/summationdisplay
j=0/parenleftbig1 +ρ2
w
2/parenrightbigj+1
2α2
k−1−j
=∞/summationdisplay
k=1γkk/summationdisplay
j=1/parenleftbig1 +ρ2
w
2/parenrightbigk−j+1
2α2
j−1
=∞/summationdisplay
j=1α2
j−1∞/summationdisplay
k=jγk/parenleftbig1 +ρ2
w
2/parenrightbigk−j+1
2
≤γ0∞/summationdisplay
j=1α2
j−1∞/summationdisplay
k=j/parenleftbig1 +ρ2
w
2/parenrightbigk−j+1
2
=γ0/radicalbig
1 +ρ2w√
2−/radicalbig
1 +ρ2w∞/summationdisplay
j=1α2
j−1
<∞,
as/summationtextα2
kconverges by Assumption 2.1.
Finally,/summationtext∞
k=0γkαk/bardblxk−1¯xk/bardbl<∞.
D.3 Convergence Rate of the Consensus Error /bardblxk−1¯xk/bardbl2and of/bardblzk+1−1¯xk/bardbl2
As/summationtext
k/bardblxk−1¯xk/bardbl2<∞, let us assume that /bardblxk−1¯xk/bardbl2vanishes with the same rate as α2
k. Then, there
must be a scalar ϑ1>0such that/bardblxk−1¯xk/bardbl2<ϑ2
1α2
k. To test if such ϑ1exists, we employ (36) to check
whether/bardblxk+1−1¯xk+1/bardbl2<ϑ2
1α2
k+1holds,
/bardblxk+1−1¯xk+1/bardbl2≤1 +ρ2
w
2/bardblxk−1¯xk/bardbl2+n(1 +ρ2
w)M
1−ρ2wα2
k
≤1 +ρ2
w
2ϑ2
1α2
k+n(1 +ρ2
w)M
1−ρ2wα2
k
=/parenleftBig1 +ρ2
w
2ϑ2
1+n(1 +ρ2
w)M
1−ρ2w/parenrightBig
α2
k.(45)
Then, testing
/parenleftBig1 +ρ2
w
2ϑ2
1+n(1 +ρ2
w)M
1−ρ2w/parenrightBig
α2
k≤ϑ2
1α2
k+1
n(1 +ρ2
w)M
1−ρ2w≤ϑ2
1/parenleftbiggα2
k+1
α2
k−1 +ρ2
w
2/parenrightbigg
n(1+ρ2
w)M
1−ρ2w
α2
k+1
α2
k−1+ρ2w
2≤ϑ2
1.(46)
Thus, 0</rho12<∞wheneverα2
k+1
α2
k−1+ρ2
w
2>0.
Let us consider αkhaving the form in Example 2.3, thenα2
k+1
α2
k=/parenleftBig
k+1
k+2/parenrightBig2υ1
is an increasing function of k
taking values between 0and1, and deﬁne
K1= arg min
α2
k+1
α2
k>1+ρ2w
2k.
26Published in Transactions on Machine Learning Research (11/2024)
To test whether K1grows very large, we ﬁnd the intersectionα2
k+1
α2
k=1+ρ2
w
2,
/parenleftBigk+ 1
k+ 2/parenrightBig2υ1
=1 +ρ2
w
2
k+ 1
k+ 2=/parenleftBig1 +ρ2
w
2/parenrightBig1
2υ1
k+ 1 = (k+ 2)/parenleftBig1 +ρ2
w
2/parenrightBig1
2υ1
k=2/parenleftBig
1+ρ2
w
2/parenrightBig1
2υ1−1
1−/parenleftBig
1+ρ2w
2/parenrightBig1
2υ1.(47)
Deﬁne the function h(x,υ1) =2x1
2υ1−1
1−x1
2υ1for0<x< 1and0.5<υ< 1.
∂h(x,υ1)
∂υ1=−exp(lnx
2υ1) lnx
2υ2
1(1−x1
2υ1)2>0for a ﬁxed 0<x< 1.
∂h(x,υ1)
∂x=x−2υ1+1
2υ1
2υ1(1−x1
2υ1)2>0for a ﬁxed 0.5<υ1<1.
Taking an extreme case of x=υ1= 0.99, we obtain h(0.99,0.99)≈196iterations. For x=υ1= 0.95,
h(0.95,0.95)≈36iterations. It decreases even more drastically for realistic choices of ρwandυ1. Thus, it
is reasonable to study the rate for k≥K1.
We conclude that for k≥K1, there exists 0<ϑ1<∞, such that
/bardblxk−1¯xk/bardbl2<ϑ2
1α2
k. (48)
Thus, from (36), for k≥K1, we also have
/bardblzk+1−1¯xk/bardbl2≤1 +ρ2
w
2/bardblxk−1¯xk/bardbl2+α2
kn(1 +ρ2
w)M
1−ρ2w
≤/parenleftBig1 +ρ2
w
2ϑ2
1+n(1 +ρ2
w)M
1−ρ2w/parenrightBig
α2
k
:=ϑ2
2α2
k.(49)
E Convergence Rate
Our primary result, stated in the following Lemma, is based on ﬁnding a relation between two successive
iterations of the expected divergence.
Lemma E.1. LetA=λc3
2,B=4c3L2ϑ2
1
λn,C=c2
1c6
4
c3λ, andE=ϑ2
n. Then, for k>K 1,
Dk+1≤(1−Aαkγk)Dk+Bα3
kγk+Cαkγ3
k+Eα2
k. (50)
Proof: See Appendix E.2.
Next, we let
K2= arg min
Aαkγk<1k
andK0= max{K1,K2}. For the ensuing part, the purpose is to locate a vanishing upper bound of Dk,
making use of the inequality (50). The idea is to propose a decreasing sequence Uk+1≤Ukand suppose that
Dk≤Uk,∀k≥K0, and then verify that Dk+1≤Uk+1by induction. The choice of Ukis the most diﬃcult
component as one has to keep in mind the general forms of αkandγkin (50) and what kind of decisions to
take regarding these forms. An essential property of Ukis presented in the subsequent lemma.
27Published in Transactions on Machine Learning Research (11/2024)
Lemma E.2. If a decreasing sequence Uk+1≤Ukfork≥K0exists such that Dk+1≤Uk+1can be deduced
fromDk≤Ukand (50), then
Uk≥B
Aα2
k+C
Aγ2
k+E
Aαk
γk. (51)
Proof: See Appendix E.3.
An important remark is that the lower bound of Ukin (51) is vanishing as α2
k,γ2
k, andαk
γkare all vanishing.
This lower bound provides an insight on the convergence rate of Dkas it cannot be better than that of
α2
k,γ2
k, orαk
γk.
The previous Lemma allows us to move forward in conﬁrming the existence of the constants ς1andς2that
permitDk≤ς1γ2
kandDk≤ς2αk
γkin Theorem 3.4, respectively.
E.1 Proof of Theorem 3.4
1.Proof of (12)
By deﬁnition of ς1,DK0≤ς1γ2
K0. The next step is to make sure that Dk+1≤Uk+1can be obtained
fromDk≤Uk,∀k≥K0. TakeUk=ς1γ2
k, letDk≤Ukhold, and substitute in (50),
Dk+1≤(1−Aαkγk)ς1γ2
k+Bα3
kγk+Cαkγ3
k+Eα2
k.
We solveDk+1≤Uk+1forς1∈R+
(1−Aαkγk)ς1γ2
k+Bα3
kγk+Cαkγ3
k+Eα2
k≤Uk+1=ς1γ2
k+1.
Then, by considering κk=1−(γk+1
γk)2
αkγk>0as given in (11),
Bα2
kγ−2
k+Eαkγ−3
k+C≤ς1(A−κk),
and assuming A−κk>0, we ﬁnd a constant ¯ς1such that
ς1≥¯ς1=Bα2
kγ−2
k+Eαkγ−3
k+C
A−κk,
keeping in mind that Bα2
kγ−2
k+Eαkγ−3
k+Cis positive by deﬁnition. Examine the parameters σ1,
σ2, andσ3as they are introduced in (11), then
¯ς1≤Bσ2+Eσ3+C
A−σ1,
We conclude that Dk≤ς1γ2
kwhereς1satisﬁes the deﬁnition (13).
2.Proof of (14)
DK0≤ς2γK0
αK0by deﬁnition of ς2.∀k≥K0, letDk≤ς2αk
γk, then
Dk+1≤(1−Aαkγk)ς2αk
γk+Bα3
kγk+Cαkγ3
k+Eα2
k.
SolvingDk+1≤ς2αk+1
γk+1forς2∈R+,
(1−Aαkγk)ς2αk
γk+Bα3
kγk+Cαkγ3
k+Eα2
k≤ς2αk+1
γk+1.
Takeτk=αk
γk−αk+1
γk+1
α2
k>0as given in (11), then
Bαkγk+Cα−1
kγ3
k+E≤(A−τk)ς2.
28Published in Transactions on Machine Learning Research (11/2024)
Ifαk
γk−αk+1
γk+1<Aα2
k, then∃¯ς2such that
ς2≥¯ς2=Bαkγk+Cα−1
kγ3
k+E
(A−τk).
Examineσ4,σ5, andσ6that are deﬁned in (11), we can say
¯ς2≤Bσ5+Cσ6+E
(A−σ4).
We conclude that Dk≤ς2αk
γkwithς2satisfying (15).
E.2 Proof of Lemma E.1
Starting with the same steps as in (27),
Dk+1=E[/bardbl¯xk+1−x∗/bardbl2]
≤E[1
n/bardblzk+1−1¯xk/bardbl2+ 2/angbracketleft−αk¯gk,¯xk−x∗/angbracketright+dk]
=Dk+1
nE[/bardblzk+1−1¯xk/bardbl2]−2αkE[/angbracketleft¯xk−x∗,¯gk/angbracketright]
(a)=Dk+1
nE[/bardblzk+1−1¯xk/bardbl2]−2c3αkγkE[/angbracketleft¯xk−x∗,h(xk) +¯bk/angbracketright]
=Dk+1
nE[/bardblzk+1−1¯xk/bardbl2]−2c3αkγkE[/angbracketleft¯xk−x∗,∇F(¯xk)/angbracketright] + 2c3αkγkE[/angbracketleft¯xk−x∗,∇F(¯xk)−h(xk)/angbracketright]
−2c3αkγkE[/angbracketleft¯xk−x∗,¯bk/angbracketright]
(52)
where (a)is due to both E[ek|Hk] = 0and (21):
E[/angbracketleft¯xk−x∗,¯gk/angbracketright] =E[/angbracketleft¯xk−x∗,¯gk−E[¯gk|Hk] +E[¯gk|Hk]/angbracketright]
=E[/angbracketleft¯xk−x∗,ek/angbracketright] +E[/angbracketleft¯xk−x∗,E[¯gk|Hk]/angbracketright]
=EHk[E[/angbracketleft¯xk−x∗,ek/angbracketright|Hk]] +E[/angbracketleft¯xk−x∗,E[¯gk|Hk]/angbracketright]
= 0 + E[/angbracketleft¯xk−x∗,E[¯gk|Hk]/angbracketright].
From Lemma B.1, we have E[/bardbl¯gk/bardbl2]<¯Mwith ¯Ma bounded constant.
By the strong convexity in Assumption 1.2, we have
−2c3αkγkE[/angbracketleft¯xk−x∗,∇F(¯xk)/angbracketright]≤2c3αkγkE[F(x∗)−F(¯xk)]−λc3αkγkE[/bardbl¯xk−x∗/bardbl2]
≤−λc3αkγkE[/bardbl¯xk−x∗/bardbl2]
=−λc3αkγkDk,(53)
where we used the fact that F(x∗)−F(¯xk)≤0.
Next, from Lemma 1.5, we have
2c3αkγk/angbracketleft¯xk−x∗,∇F(¯xk)−h(xk)/angbracketright≤2c3αkγkL√n/bardbl¯xk−x∗/bardbl/bardblxk−1¯xk/bardbl
(a)
≤λc3αkγk
4/bardbl¯xk−x∗/bardbl2+ 4c3αkγkL2
λn/bardblxk−1¯xk/bardbl2,
where (a)is due to 2√/epsilon1×1√/epsilon1/angbracketlefta,b/angbracketright= 2/angbracketleft√/epsilon1a,1√/epsilon1b/angbracketright≤/epsilon1/bardbla/bardbl2+1
/epsilon1/bardblb/bardbl2. From (48), we have for k≥K1,
/bardblxk−1¯xk/bardbl2≤ϑ2
1α2
k.
29Published in Transactions on Machine Learning Research (11/2024)
Hence,
2c3αkγkE[/angbracketleft¯xk−x∗,∇F(¯xk)−h(xk)/angbracketright]≤λc3αkγk
4Dk+4c3L2ϑ2
1
λnα3
kγk. (54)
From (22),
−2c3αkγkE[/angbracketleft¯xk−x∗,¯bk/angbracketright]≤λc3αkγk
4Dk+4c3αkγk
λE[/bardbl¯bk/bardbl2]
≤λc3αkγk
4Dk+c2
1c6
4αkγ3
k
c3λ(55)
From (49), for k≥K1, we have
1
nE[/bardblzk+1−1¯xk/bardbl2]≤ϑ2
nα2
k. (56)
Finally, by combining (52), (53), (54), (55), and (56) we get (50).
E.3 Proof of Lemma E.2
Since 1−Aαkγk>0whenk≥K0, we may substitute Dk≤Ukin (50),
Dk+1≤(1−Aαkγk)Uk+Bα3
kγk+Cαkγ3
k+Eα2
k.
TestingDk+1≤Uk+1in the previous inequality, we get
(1−Aαkγk)Uk+Bα3
kγk+Cαkγ3
k+Eα2
k≤Uk+1≤Uk
B
Aα2
k+C
Aγ2
k+E
Aαk
γk≤Uk. (57)
E.4 Proof of Theorem 3.5
Theorem 3.4 indicates that the convergence rate is a function of υ1andυ2, asγ2
k∝(k+ 1)−2υ2and
αk
γk∝(k+ 1)−(υ1−υ2). Nonetheless, we must still verify the validity of the assumptions presented in the
theorem, meaning:
•Areσ1<Aandσ4<Afulﬁlled?
•Areς1andς2bounded?
We must remark that in what follows, the analysis is done for k≥K0.
Letαkandγkhave the forms given in (16).
1.Verifying that σ1<Aandσ4<A
The idea is to ﬁnd a bound on α0andγ0to guarantee σ1<Aandσ4<A. We start by bounding
σ1andσ4from above, i.e.,
σ1= max
k≥K01−(γk+1
γk)2
αkγk= max
k≥K01−(1 +1
k+1)−2υ2
α0γ0(k+ 1)−υ1−υ2
and
σ4= max
k≥K01−αk+1γ−1
k+1
αkγ−1
k
αkγk= max
k≥K01−(1 +1
k+1)−(υ1−υ2)
α0γ0(k+ 1)−υ1−υ2.
30Published in Transactions on Machine Learning Research (11/2024)
To do so, we deﬁne a function q(x) =x−a(1−(1 +x)−b)witha,b,x∈(0,1]. Sincex−a≤x−1, we
haveq(x)≤x−1(1−(1 +x)−b) =r(x). To further bound q(x), We study the derivative of r(x)as
it is simpler to do so,
r/prime(x) =x−2/parenleftbigg
((b+ 1)x+ 1)(1 +x)−b−1−1/parenrightbigg
=x−2s(x).
Hence the sign of r/prime(x)is that ofs(x). We again calculate the derivative of s(x)to ﬁnd its sign,
s/prime(x) =−b(b+ 1)x(1 +x)−b−2≤0
sinceb > 0andx > 0. Then,s(x)is a decreasing function of xover (0,1]. We remark that
limx→0s(x) = 0, meanings(x)<0andr/prime(x)<0,∀x∈(0,1]. Finally,
r(x)<lim
x→0r(x) =1−(1 +x)−b
x=b,
andq(x)≤r(x)< b, noting that limx→0q(x) =bfora= 1. We conclude that σ1<2υ2
α0γ0and
σ4<υ1−υ2
α0γ0. Forσ1<Aandσ4<Ato be valid, we must have
α0γ0≥max{2υ2,υ1−υ2}/A. (58)
2.Verifying that ς1andς2are bounded
The goal is to verify that the constant term in the convergence rate is bounded. Thus, we must
check that the lower bounds given in (13) and (15) are indeed ﬁnite. We start by analyzing σ2and
σ5,
σ2=α2
0γ−2
0max
k≥K0(1 +k)−2(υ1−υ2)=α2
0γ−2
0(1 +K0)−2(υ1−υ2),as0<υ2≤υ1,
and
σ5=α0γ0max
k≥K0(1 +k)−(υ1+υ2)=α0γ0(1 +K0)−(υ1+υ2),as0<υ2+υ1.
We end with the analysis of σ3andσ6, i.e.,
σ3=α0γ−3
0max
k≥K0(1 +k)−(υ1−3υ2)=/braceleftBigg
α0γ−3
0(1 +K0)−(υ1−3υ2),ifυ1≥3υ2,
∞, ifυ1<3υ2,
and
σ6=α−1
0γ3
0max
k≥K0(1 +k)υ1−3υ2=/braceleftBigg
α−1
0γ3
0(1 +K0)υ1−3υ2,ifυ1≤3υ2,
∞, ifυ1>3υ2.
There are clearly 3 cases:
•υ1>3υ2
Thus,σ3is bounded.
Sinceσ2andς1(by deﬁnition) are also bounded provided that α0γ0≥2υ2
Ain (58).
However,ς2→∞sinceσ6→∞resulting in a loose upper bound in (14).
To that end, we can write Dk≤Υ1(1 +k)−2υ2with Υ1a bounded constant.
•υ1<3υ2
Similarly,σ6is bounded while σ3→∞. Then,∃Υ2<∞, whereDk≤Υ2(1 +k)−(υ1−υ2)
provided that α0γ0≥υ1−υ2
A.
•υ1= 3υ2
Bothσ3andσ6are bounded allowing both previous inequalities corresponding to Dkto be
valid.
31Published in Transactions on Machine Learning Research (11/2024)
By this analysis, we conclude the proof of Theorem 3.5.
We present Figure 8 for easier reading of the conditions on
the step sizes’ exponents where we plot υ2vs.υ1.
Figure 8: Plot of υ2vs.υ1where the
yellowshadedareaisthefeasibilityre-
gion determined by Assumption 2.1.
F Regret Analysis
Consider the following modiﬁed deﬁnition of expected divergence that we denote by D/prime
k.
D/prime
k=E/bracketleftBig1
nn/summationdisplay
i=1/bardblxi,k−x∗/bardbl2/bracketrightBig
.
We then develop this entity,
D/prime
k+1=E/bracketleftBig1
nn/summationdisplay
i=1/bardblxi,k+1−x∗/bardbl2/bracketrightBig
(a)
≤E/bracketleftBig1
nn/summationdisplay
i=1/bardblzi,k+1−x∗/bardbl2/bracketrightBig
=E/bracketleftBig1
nn/summationdisplay
i=1/vextenddouble/vextenddouble/vextenddoublen/summationdisplay
j=1wij(xj,k−αkgj,k)−x∗/vextenddouble/vextenddouble/vextenddouble2/bracketrightBig
=E/bracketleftBig1
nn/summationdisplay
i=1/vextenddouble/vextenddouble/vextenddoublen/summationdisplay
j=1wij(xj,k−αkgj,k−x∗)/vextenddouble/vextenddouble/vextenddouble2/bracketrightBig
(b)
≤E/bracketleftBig1
nn/summationdisplay
i=1n/summationdisplay
j=1wij/bardblxj,k−αkgj,k−x∗/bardbl2/bracketrightBig
(c)=E/bracketleftBig1
nn/summationdisplay
j=1/bardblxj,k−αkgj,k−x∗/bardbl2/bracketrightBig
=D/prime
k−2αk1
nn/summationdisplay
j=1E[/angbracketleftxj,k−x∗,gj,k/angbracketright] +α2
k1
nn/summationdisplay
j=1E[/bardblgj,k/bardbl2]
=D/prime
k−2c3αkγk1
nn/summationdisplay
j=1E[/angbracketleftxj,k−x∗,∇Fj(xj,k) +bj,k/angbracketright] +α2
k1
nn/summationdisplay
j=1E[/bardblgj,k/bardbl2]
(d)
≤D/prime
k−2c3αkγk1
nn/summationdisplay
j=1E[Fj(xj,k)−Fj(x∗)] +c3αkγk1
nn/summationdisplay
j=1E[/bardblxj,k−x∗/bardbl2+/bardblbj,k/bardbl2] +α2
kM
(e)
≤D/prime
k−2c3αkγk1
nn/summationdisplay
j=1E[Fj(xj,k)−Fj(x∗)] +c3αkγkD/prime
k+c3αkγ3
kc6
4c2
1
4c2
3+α2
kM,
32Published in Transactions on Machine Learning Research (11/2024)
where (a)is by applying the projection inequality (5), (b)is by the convexity of the norm square function,
(c)is by the doubly stochastic nature of the matrix W,(d)is by the convexity of the objective function and
Lemma B.1, and (e)is by (20).
Then,
1
nn/summationdisplay
i=1E[Fi(xi,k)−Fi(x∗)]≤D/prime
k−D/prime
k+1
c3αkγk+D/prime
k+c6
4c2
1
4c2
3γ2
k+αk
c3γkM. (59)
We know that D/prime
kcan be written as
D/prime
k=E/bracketleftBig1
nn/summationdisplay
i=1/bardblxi,k−x∗/bardbl2/bracketrightBig
=E/bracketleftBig1
nn/summationdisplay
i=1/bardblxi,k−¯xk+ ¯xk−x∗/bardbl2/bracketrightBig
=E/bracketleftBig1
nn/summationdisplay
i=1/parenleftBig
/bardblxi,k−¯xk/bardbl2+ 2/angbracketleftxi,k−¯xk,¯xk−x∗/angbracketright+/bardbl¯xk−x∗/bardbl2/parenrightBig/bracketrightBig
=E/bracketleftBig1
n/bardblxk−1¯xk/bardbl2+ 2/angbracketleft¯xk−¯xk,¯xk−x∗/angbracketright+/bardbl¯xk−x∗/bardbl2/bracketrightBig
=E/bracketleftBig1
n/bardblxk−1¯xk/bardbl2+/bardbl¯xk−x∗/bardbl2/bracketrightBig
.(60)
Hence, to ﬁnd the regret bound, we write
E/bracketleftbigg1
nK/summationdisplay
k=K0n/summationdisplay
i=1Fi(xi,k)−Fi(x∗)/bracketrightbigg
(a)
≤K/summationdisplay
k=K0/parenleftBigg
D/prime
k−D/prime
k+1
c3αkγk+D/prime
k+c6
4c2
1
4c2
3γ2
k+αk
c3γkM/parenrightBigg
=K/summationdisplay
k=K0+1D/prime
k/parenleftBig1
c3αkγk−1
c3αk−1γk−1/parenrightBig
+D/prime
K0
c3αK0γK0+D/prime
K+1
c3αK+1γK+1+K/summationdisplay
k=K0/parenleftBig
D/prime
k+c6
4c2
1
4c2
3γ2
k+αk
c3γkM/parenrightBig
(b)=/parenleftBig1
c3α0γ0+ 1/parenrightBigK/summationdisplay
k=K0+1D/prime
k+/parenleftBig1
c3αK0γK0+ 1/parenrightBig
D/prime
K0+D/prime
K+1
c3αK+1γK+1+K/summationdisplay
k=K0/parenleftBigc6
4c2
1γ2
0
4c2
31√
k+ 1+Mα 0
c3γ01√
k+ 1/parenrightBig
(c)
≤/parenleftBig1
c3α0γ0+ 1/parenrightBigK/summationdisplay
k=K0+1/parenleftBigϑ2
1α2
0
n1
(k+ 1)3
2+ Υ1√
k+ 1/parenrightBig
+(K+ 2)
c3α0γ0/parenleftBigϑ2
1α2
0
n1
(K+ 2)3
2+ Υ1√
K+ 2/parenrightBig
+/parenleftBig1
c3αK0γK0+ 1/parenrightBig
D/prime
K0+/parenleftBigc6
4c2
1γ2
0
4c2
3+Mα 0
c3γ0/parenrightBigK/summationdisplay
k=K01√
k+ 1
where (a)is following up from (59), (b)is by substituting αk=α0(k+ 1)−3
4andγk=γ0(k+ 1)−1
4, and (c)
is by (60), Lemma 3.3, and Theorem 3.5.
To ﬁnd an upper bound, we interpret the sums over K0+1≤k≤Kas Riemann sums in which the functions
1
(u+1)3
2and1√u+1are evaluated at the right endpoint of the interval [i−1,i]fori=K0+ 1,K0+ 2,...,K.
Since the functions1
(u+1)3
2and1√u+1are monotonically decreasing, the sums are in fact lowerRiemann
sums and therefore bounded from above by the integrals/integraltextK
K01
(u+1)3
2duand/integraltextK
K01√u+1du, respectively.
K/summationdisplay
k=K0+11
(k+ 1)3
2≤/integraldisplayK
K01
(u+ 1)3
2du= 2/parenleftBig1√K0+ 1−1√
K+ 1/parenrightBig
33Published in Transactions on Machine Learning Research (11/2024)
K/summationdisplay
k=K0+11√
k+ 1≤/integraldisplayK
K01√u+ 1du= 2(√
K+ 1−/radicalbig
K0+ 1)
Finally,
E/bracketleftbigg1
nK/summationdisplay
k=K0n/summationdisplay
i=1Fi(xi,k)−Fi(x∗)/bracketrightbigg
≤2/parenleftBig1
c3α0γ0+ 1/parenrightBig/parenleftBigg
ϑ2
1α2
0
n/parenleftBig1√K0+ 1−1√
K+ 1/parenrightBig
+ Υ(√
K+ 1−/radicalbig
K0+ 1)/parenrightBigg
+1
c3α0γ0/parenleftBigϑ2
1α2
0
n1√
K+ 2+ Υ√
K+ 2/parenrightBig
+/parenleftBig1
c3αK0γK0+ 1/parenrightBig
D/prime
K0+/parenleftBigc6
4c2
1γ2
0
2c2
3+2Mα 0
c3γ0/parenrightBig
(√
K+ 1−/radicalbig
K0)
G Convergence Rate with Constant Step Sizes
We start by going over previous derivations,
˘gi,k=ES,Φ,ζ[Φi,k(fi(xi,k+γΦi,k,Si,k) +ζi,k)|Hk]
=EΦ[Φi,kFi(xi,k+γΦi,k)|Hk]
=Fi(xi,k)EΦ[Φi,k] +γEΦ[Φi,kΦT
i,k|Hk]∇Fi(xi,k) +γ2
2EΦ[Φi,kΦT
i,k∇2Fi(˜xi,k)Φi,k|Hk]
=c3γ[∇Fi(xi,k) +bi,k].
Thus,bi,k=γ
2c3EΦ[Φi,kΦT
i,k∇2Fi(˜xi,k)Φi,k|Hk].
Let Assumptions 1.2 and 2.2 hold. Then, we can bound the bias as
/bardblbi,k/bardbl≤γ
2c3EΦ[/bardblΦi,k/bardbl2/bardblΦT
i,k/bardbl2/bardbl∇2Fi(˜xi,k)/bardbl2/bardblΦi,k/bardbl2|Hk]
≤γc3
4c1
2c3.
We remark that
˜gk=E[¯gk|Hk]
=1
nn/summationdisplay
i=1E[gi,k|Hk]
=1
nn/summationdisplay
i=1c3γ[∇Fi(xi,k) +bi,k]
=c3γ[h(xk) +¯bk](61)
is also a biased estimator of h(xk)with
/bardbl¯bk/bardbl=/bardbl1
nn/summationdisplay
i=1bi,k/bardbl
≤1
nn/summationdisplay
i=1/bardblbi,k/bardbl
≤1
nn/summationdisplay
i=1γc3
4c1
2c3
=γc3
4c1
2c3.(62)
34Published in Transactions on Machine Learning Research (11/2024)
Lemma G.1. Let all Assumptions 1.3, 2.2, and 2.4 hold, then there exists a bounded constant ¯M > 0, such
thatE[/bardbl¯gk/bardbl2]<¯M.
Proof.∀i∈N, we have
E[/bardblgi,k/bardbl2|Hk] =E[/bardblΦi,k(fi(xi,k+γΦi,k,Si,k) +ζi,k)/bardbl2|Hk]
=E[/bardblΦi,k/bardbl2/bardblfi(xi,k+γΦi,k,Si,k) +ζi,k/bardbl2|Hk]
(a)
≤c2
4E[(fi(xi,k+γΦi,k,Si,k) +ζi,k)2|Hk]
(b)=c2
4E[f2
i(xi,k+γΦi,k,Si,k)|Hk] +c2
4c2
(c)
<∞,
where (a)is due to Assumption 2.2, (b)Assumption 1.3, and (c)Assumption 2.4.
The stochastic noise is still deﬁned as ek= ¯gk−˜gkand retains its property
E[ek] =E[¯gk−E[¯gk|Hk]] =EHk/bracketleftBig
E/bracketleftBig
¯gk−E[¯gk|Hk]/vextendsingle/vextendsingle/vextendsingleHk/bracketrightBig/bracketrightBig
= 0.
1.Proving/bardblxk−1¯xk/bardbl2and/bardblzk+1−1¯xk/bardbl2converge linearly
/bardblxk+1−1¯xk+1/bardbl2=/bardblxk+1−1¯xk+1¯xk−1¯xk+1/bardbl2
=/bardblxk+1−1¯xk/bardbl2+ 2/angbracketleftxk+1−1¯xk,1¯xk−1¯xk+1/angbracketright+/bardbl1¯xk−1¯xk+1/bardbl2
(a)=/bardblxk+1−1¯xk/bardbl2−/bardbl1¯xk−1¯xk+1/bardbl2
≤/bardblxk+1−1¯xk/bardbl2
=n/summationdisplay
i=1/bardblxi,k+1−¯xk/bardbl2
(b)
≤n/summationdisplay
i=1/bardblzi,k+1−¯xk/bardbl2
=/bardblzk+1−1¯xk/bardbl2
=/bardblWxk−αWgk−1¯xk/bardbl2
=/bardblWxk−1¯xk/bardbl2−2α/angbracketleftWxk−1¯xk,Wgk/angbracketright+α2/bardblWgk/bardbl2
(c)
≤/bardblWxk−1¯xk/bardbl2+α[1−ρ2
w
2ρ2wα/bardblWxk−1¯xk/bardbl2+2ρ2
wα
1−ρ2w/bardblWgk/bardbl2] +α2/bardblWgk/bardbl2
(d)
≤ρ2
w/bardblxk−1¯xk/bardbl2+α[1−ρ2
w
2α/bardblxk−1¯xk/bardbl2+2ρ2
wα
1−ρ2w/bardblWgk/bardbl2] +α2/bardblWgk/bardbl2
=1 +ρ2
w
2/bardblxk−1¯xk/bardbl2+α21 +ρ2
w
1−ρ2w/bardblWgk/bardbl2
=1 +ρ2
w
2/bardblxk−1¯xk/bardbl2+α21 +ρ2
w
1−ρ2w/bardblWgk−1¯gk+1¯gk/bardbl2
(e)=1 +ρ2
w
2/bardblxk−1¯xk/bardbl2+α21 +ρ2
w
1−ρ2w/bardblWgk−1¯gk/bardbl2+α2n(1 +ρ2
w)
1−ρ2w/bardbl¯gk/bardbl2
≤1 +ρ2
w
2/bardblxk−1¯xk/bardbl2+α2ρ2
w(1 +ρ2
w)
1−ρ2w/bardblgk−1¯gk/bardbl2+α2n(1 +ρ2
w)
1−ρ2w/bardbl¯gk/bardbl2
(f)
≤1 +ρ2
w
2/bardblxk−1¯xk/bardbl2+α2n(1 +ρ2
w)
1−ρ2wM.
(63)
35Published in Transactions on Machine Learning Research (11/2024)
where (a)is by (37), (b)is the projection inequality (5) noting that ¯xk∈KsinceKis a convex set (so
projecting it onto Kgives us the same point), (c)is by−2/epsilon1×1
/epsilon1/angbracketlefta,b/angbracketright=−2/angbracketleft/epsilon1a,1
/epsilon1b/angbracketright≤/epsilon12/bardbla/bardbl2+1
/epsilon12/bardblb/bardbl2
(d)is by Lemma 1.4, (e)is by (38), and (f)is by (39) and (40).
By induction, we have
/bardblxk+1−1¯xk+1/bardbl2≤/parenleftbig1 +ρ2
w
2/parenrightbigk+1/bardblx0−1¯x0/bardbl2+α22nM
1−ρ2wk/summationdisplay
j=0/parenleftbig1 +ρ2
w
2/parenrightbigj+1
≤/parenleftbig1 +ρ2
w
2/parenrightbigk+1/bardblx0−1¯x0/bardbl2+α22nM(1 +ρ2
w)
(1−ρ2w)2,(64)
where the last inequality is due to the geometric sum with1+ρ2
w
2<1
We conclude that /bardblxk−1¯xk/bardbl2converges linearly to an α2neighborhood of 0, almost surely.
Substituting in (63),
/bardblzk+1−1¯xk/bardbl2≤1 +ρ2
w
2/bardblxk−1¯xk/bardbl2+α2n(1 +ρ2
w)
1−ρ2wM
≤1 +ρ2
w
2/parenleftBigg
/parenleftbig1 +ρ2
w
2/parenrightbigk/bardblx0−1¯x0/bardbl2+α22nM(1 +ρ2
w)
(1−ρ2w)2/parenrightBigg
+α2n(1 +ρ2
w)
1−ρ2wM
=/parenleftbig1 +ρ2
w
2/parenrightbigk+1/bardblx0−1¯x0/bardbl2+α2nM/parenleftbigg/parenleftBig1 +ρ2
w
1−ρ2w/parenrightBig2
+1 +ρ2
w
1−ρ2w/parenrightbigg(65)
Finally,/bardblzk+1−1¯xk/bardbl2converges linearly to an α2neighborhood of 0, almost surely, as well.
2.ProvingDk=E[/bardbl¯xk−x∗/bardbl2]converges linearly
Dk+1=E[/bardbl¯xk+1−x∗/bardbl2]
≤E[1
n/bardblzk+1−1¯xk/bardbl2+ 2/angbracketleft−α¯gk,¯xk−x∗/angbracketright+dk]
=Dk+1
nE[/bardblzk+1−1¯xk/bardbl2]−2αE[/angbracketleft¯xk−x∗,¯gk/angbracketright]
(a)=Dk+1
nE[/bardblzk+1−1¯xk/bardbl2]−2c3αγE[/angbracketleft¯xk−x∗,h(xk) +¯bk/angbracketright]
=Dk+1
nE[/bardblzk+1−1¯xk/bardbl2]−2c3αγE[/angbracketleft¯xk−x∗,∇F(¯xk)/angbracketright] + 2c3αγE[/angbracketleft¯xk−x∗,∇F(¯xk)−h(xk)/angbracketright]
−2c3αγE[/angbracketleft¯xk−x∗,¯bk/angbracketright]
(66)
where (a)is due to both E[ek|Hk] = 0and (21):
E[/angbracketleft¯xk−x∗,¯gk/angbracketright] =E[/angbracketleft¯xk−x∗,¯gk−E[¯gk|Hk] +E[¯gk|Hk]/angbracketright]
=E[/angbracketleft¯xk−x∗,ek/angbracketright] +E[/angbracketleft¯xk−x∗,E[¯gk|Hk]/angbracketright]
=EHk[E[/angbracketleft¯xk−x∗,ek/angbracketright|Hk]] +E[/angbracketleft¯xk−x∗,E[¯gk|Hk]/angbracketright]
= 0 + E[/angbracketleft¯xk−x∗,E[¯gk|Hk]/angbracketright].
From Lemma G.1, we have E[/bardbl¯gk/bardbl2]<¯Mwith ¯Ma bounded constant.
By the strong convexity in Assumption 1.2, we have
−2c3αγE[/angbracketleft¯xk−x∗,∇F(¯xk)/angbracketright]≤2c3αγE[F(x∗)−F(¯xk)]−λc3αγE[/bardbl¯xk−x∗/bardbl2]
≤−λc3αγE[/bardbl¯xk−x∗/bardbl2]
=−λc3αγDk,(67)
36Published in Transactions on Machine Learning Research (11/2024)
where we used the fact that F(x∗)−F(¯xk)≤0.
Next, from Lemma 1.5, we have
2c3αγ/angbracketleft¯xk−x∗,∇F(¯xk)−h(xk)/angbracketright≤2c3αγL√n/bardbl¯xk−x∗/bardbl/bardblxk−1¯xk/bardbl
(a)
≤λc3αγ
4/bardbl¯xk−x∗/bardbl2+ 4c3αγL2
λn/bardblxk−1¯xk/bardbl2,
where (a)is due to 2√/epsilon1×1√/epsilon1/angbracketlefta,b/angbracketright= 2/angbracketleft√/epsilon1a,1√/epsilon1b/angbracketright≤/epsilon1/bardbla/bardbl2+1
/epsilon1/bardblb/bardbl2.
In (64) and (65), we let R=/bardblx0−1¯x0/bardbl2, andG1=2nM(1+ρ2
w)
(1−ρ2w)2,G2=nM/parenleftbigg/parenleftBig
1+ρ2
w
1−ρ2w/parenrightBig2
+1+ρ2
w
1−ρ2w/parenrightbigg
,
/bardblxk−1¯xk/bardbl2≤/parenleftbig1 +ρ2
w
2/parenrightbigkR+α2G1and/bardblzk+1−1¯xk/bardbl2≤/parenleftbig1 +ρ2
w
2/parenrightbigk+1R+α2G2.(68)
Hence,
2c3αγE[/angbracketleft¯xk−x∗,∇F(¯xk)−h(xk)/angbracketright]≤λc3αγ
4Dk+ 4c3αγL2
λn/bracketleftBig/parenleftbig1 +ρ2
w
2/parenrightbigkR+α2G1/bracketrightBig
.(69)
From (62),
−2c3αγE[/angbracketleft¯xk−x∗,¯bk/angbracketright]≤λc3αγ
4Dk+4c3αγ
λE[/bardbl¯bk/bardbl2]
≤λc3αγ
4Dk+αγ3c2
1c6
4
c3λ(70)
Finally, by combining (66), (67), (68), (69), and (70), and setting now A=λc3
2,B=4c3L2
λn, and
C=c2
1c6
4
c3λ, we get
Dk+1≤(1−Aαγ)Dk+Bαγ/bracketleftBig/parenleftbig1 +ρ2
w
2/parenrightbigkR+α2G1/bracketrightBig
+1
n/bracketleftBig/parenleftbig1 +ρ2
w
2/parenrightbigk+1R+α2G2/bracketrightBig
+Cαγ3
=(1−Aαγ)Dk+R/bracketleftBig
Bαγ +1
n/parenleftbig1 +ρ2
w
2/parenrightbig/bracketrightBig/parenleftbig1 +ρ2
w
2/parenrightbigk+α3γBG 1+α21
nG2+αγ3C.(71)
Let/rho11= 1−Aαγand/rho12=/parenleftbig1+ρ2
w
2/parenrightbig
. Then, assuming αγ <1
Aand taking the telescoping sum
Dk+1≤/rho1k+1
1D0+R/parenleftBig
Bαγ +/rho12
n/parenrightBigk/summationdisplay
i=0/rho1i
1/rho1k−i
2+ (α3γBG 1+α21
nG2+αγ3C)k/summationdisplay
i=0/rho1i
1
=/rho1k+1
1D0+R/parenleftBig
Bαγ +/rho12
n/parenrightBigk/summationdisplay
i=0/rho1i
1/rho1k−i
2+ (α3γBG 1+α21
nG2+αγ3C)/parenleftBig1−/rho1k+1
1
1−/rho11/parenrightBig
=/rho1k+1
1D0+R/parenleftBig
Bαγ +/rho12
n/parenrightBigk/summationdisplay
i=0/rho1i
1/rho1k−i
2+/parenleftBig
α2BG1
A+α
γG2
nA+γ2C
A/parenrightBig
(1−/rho1k+1
1)
≤/rho1k+1
1D0+R/parenleftBig
Bαγ +/rho12
n/parenrightBigk/summationdisplay
i=0/rho1i
1/rho1k−i
2+α2BG1
A+α
γG2
nA+γ2C
A(72)
where in the last equality, we further imposed the step sizes to satisfy α<γ.
In what follows, we discuss the summation in the second term of the inequality to avoid setting loose
bounds. We know that this summation can be written as follows,
k/summationdisplay
i=0/rho1i
1/rho1k−i
2=k/summationdisplay
i=0/rho1k−i
1/rho1i
2 (73)
37Published in Transactions on Machine Learning Research (11/2024)
Thus, without imposing further assumptions on the step sizes, we consider the following function
the two cases:
•When/rho11≤/rho12, we use the left hand side of the previous equality
Dk+1≤/rho1k+1
1D0+R/parenleftBig
Bαγ +/rho12
n/parenrightBig
/rho1k
2k/summationdisplay
i=0/rho1i
1/rho1−i
2+α2BG1
A+α
γG2
nA+γ2C
A
≤/rho1k+1
1D0+R/parenleftBig
Bαγ +/rho12
n/parenrightBig
/rho1k
21
1−/rho11
/rho12+α2BG1
A+α
γG2
nA+γ2C
A
=/rho1k+1
1D0+/rho1k+1
22R/parenleftBig
Bαγ +/rho12
n/parenrightBig
2Aαγ +ρ2w−1+α2BG1
A+α
γG2
nA+γ2C
A(74)
Then, for arbitrary small step sizes satisfying αγ <1
Aandα<γ,Dkconverges with the linear
rate ofO/parenleftbig
/rho1k
2/parenrightbig
.
•When/rho11>/rho12, we use the right hand side
Dk+1≤/rho1k+1
1D0+R/parenleftBig
Bαγ +/rho12
n/parenrightBig
/rho1k
1k/summationdisplay
i=0/rho1−i/rho1i
2+α2BG1
A+α
γG2
nA+γ2C
A
≤/rho1k+1
1D0+R/parenleftBig
Bαγ +/rho12
n/parenrightBig
/rho1k
11
1−/rho12
/rho11+α2BG1
A+α
γG2
nA+γ2C
A
=/rho1k+1
1/parenleftBig
D0+2RBαγ +2R/rho12
n
1−2Aαγ−ρ2w/parenrightBig
+α2BG1
A+α
γG2
nA+γ2C
A(75)
Then, for arbitrary small step sizes satisfying αγ <1
Aandα<γ,Dkconverges with the linear
rate ofO/parenleftbig
/rho1k
1/parenrightbig
.
H Additional Numerical Examples
Figures 9-11 depict the classiﬁcation of images with the labels 2and3and Figures 12-14 depict those with
the labels 3and4.
Figure 9: Expected loss function evolution of the
proposedalgorithmvs. DSGT,EXTRA,and1P-
GD considering vanishing vs. constant step sizes
classifying images with labels 2and3.
Figure 10: Expected test accuracy evolution of
the proposed algorithm vs. DSGT, EXTRA, and
1P-GD considering vanishing vs. constant step
sizes classifying images with labels 2and3.
38Published in Transactions on Machine Learning Research (11/2024)
Figure 11: Expected consensus error evolution of
the proposed algorithm vs. DSGT and EXTRA
consideringvanishingvs. constantstepsizesclas-
sifying images with labels 2and3.
Figure 12: Expected loss function evolution of
the proposed algorithm vs. DSGT, EXTRA, and
1P-GD considering vanishing vs. constant step
sizes classifying images with labels 3and4.
Figure 13: Expected test accuracy evolution of
the proposed algorithm vs. DSGT, EXTRA, and
1P-GD considering vanishing vs. constant step
sizes classifying images with labels 3and4.
Figure 14: Expected consensus error evolution of
the proposed algorithm vs. DSGT and EXTRA
consideringvanishingvs. constantstepsizesclas-
sifying images with labels 3and4.
39