Published in Transactions on Machine Learning Research (11/2024)
Gaussian-Smoothed Sliced Probability Divergences
Mokhtar Z. Alaya alayaelm@utc.fr
Université de Technologie de Compiègne,
LMAC (Laboratoire de Mathématiques Appliquées de Compiègne), CS 60 319 - 60 203 Compiègne Cedex
Alain Rakotomamonjy a.rakotomamonjy@criteo.com
Criteo AI Lab, Paris, France,
Maxime Berar maxime.berar@univ-rouen.fr
Univ Rouen Normandie, INSA Rouen Normandie, Universite Le Havre Normandie
Normandie Univ, LITIS UR4108, Rouen, France
Gilles Gasso gilles.gasso@insa-rouen.fr
INSA Rouen Normandie, Univ Rouen Normandie, Universite Le Havre Normandie,
Normandie Univ, LITIS UR4108, Rouen, France
Reviewed on OpenReview: https: // openreview. net/ forum? id= weuALLWUV2
Abstract
Gaussian smoothed sliced Wasserstein distance has been recently introduced for comparing
probability distributions, while preserving privacy on the data. It has been shown that it
provides performances similar to its non-smoothed (non-private) counterpart. However, the
computational and statistical properties of such a metric have not yet been well-established.
This work investigates the theoretical properties of this distance as well as those of generalized
versions denoted as Gaussian-smoothed sliced divergences GσSDp. We ﬁrst show that
smoothing and slicing preserve the metric property and the weak topology. To study
the sample complexity of such divergences, we then introduce ˆˆµnthe double empirical
distribution for the smoothed-projected µ. The distribution ˆˆµnis a result of a double
sampling process: one from sampling according to the origin distribution µand the second
according to the convolution of the projection of µon the unit sphere and the Gaussian
smoothing. We particularly focus on the Gaussian smoothed sliced Wasserstein distance
GσSWpand prove that it converges with a rate O(n−1/2p). We also derive other properties,
including continuity, of diﬀerent divergences with respect to the smoothing parameter. We
support our theoretical ﬁndings with empirical studies in the context of privacy-preserving
domain adaptation.
1 Introduction
Divergences for comparing two distributions have been shown to be important for achieving good performance
in the contexts of generative modeling (Arjovsky et al., 2017; Salimans et al., 2018), domain adaptation (Long
et al., 2015; Courty et al., 2016; Lee et al., 2019), and in computer vision (Bonneel et al., 2011; Solomon
et al., 2015) among many more applications (Kolouri et al., 2017; Peyré & Cuturi, 2019; Nguyen et al., 2023).
Examples of divergences that have proved useful for these tasks are the Maximum Mean Discrepancy (Gretton
et al., 2012; Long et al., 2015; Sutherland et al., 2017), the Wasserstein distance (Monge, 1781; Kantorovich,
1942; Villani, 2009) or its variant the sliced Wasserstein distance (SW) (Kolouri et al., 2016; Bonneel &
Coeurjolly, 2019; Kolouri et al., 2019b; Nguyen et al., 2021; 2022; 2024).
The SW distance has the advantage of being computationally eﬃcient, since it uses a closed-form solution for
distributions with support on R, by computing the expectation of one-dimensional (1D) random projections
of distributions in Rd. Owing to this eﬃciency and the resulting scalability, this distance has been successfully
applied in several applications ranging from generative models to domain adaptation (Kolouri et al., 2019a;
1Published in Transactions on Machine Learning Research (11/2024)
Deshpande et al., 2019; Wu et al., 2019; Lee et al., 2019) and its statistical properties have been well-studied
in Nadjahi et al. (2020).
Recently, Gaussian smoothed variants of the Wasserstein distance and the sliced Wasserstein distance have
been introduced respectively in (Nietert et al., 2021) and in Rakotomamonjy & Ralaivola (2021). One main
motivation behind these variants is to provide a privacy guarantee for the distribution comparison task as
Gaussian smoothing is known to be a mechanism for achieving diﬀerential privacy (Dwork et al., 2014). While
the properties of the Gaussian smoothed Wasserstein distance have been extensively studied by Nietert et al.
(2021), the properties of the Gaussian smoothed sliced Wasserstein distance have not been fully investigated
yet although they are known to be more computationally eﬃcient.
In this work, we focus on the slicing of Gaussian-smoothed measure discrepancies by providing theoretical
properties of more general divergences induced by some base distances or divergences for distributions deﬁned
inRd. These base distances/divergences encompass Wasserstein, maximum mean discrepancy, Sinkhorn
divergence. As for a main contribution, we ﬁrst establish the topological properties of these divergences in
term of a metrization of the weak topology and a semi-lower continuous property. Then we focus on the sample
complexity of such divergences by introducing the double empirical distribution ˆˆµnfor the smoothed-projected
origin distribution µ. The new empirical distribution is a result of double sampling process: one from sampling
according to the origin distribution and the second according to the convolution of the projection of µon
the unit sphere and the Gaussian smoothing. The introducing of ˆˆµnis inspired from the implementation
part: we sample X1, . . . , X nfrom the raw distribution µto deﬁne ˆµnthen project it on the unit sphere and
smooth this projection with a Gaussian distribution. This smoothing is a continuous measure that needs to
be sampled. For that reason, we add a double sampling and then provide ˆˆµn. We particularly focus on the
Gaussian smoothed sliced Wasserstein distance.
Given the importance of the noise level in the privacy/utility trade-oﬀ achieved by the divergence, we
investigate an order relation and a continuity result with respect to the noise level. These properties are of
high impact as it supports a computationally cheap warm-start/ﬁne-tuning procedure when looking for a
privacy/utility compromise of the divergence. Our theoretical study is backed by some numerical experiments
on toy problems and on domain adaptation illustrating how owing to the topology induced by our metric and
its continuity, diﬀerential privacy comes almost for free (without loss of performance) and multiple models
with diﬀerent level of privacy can be cheaply computed.
Comparison with previous works. Here we highlight the position of this work compared to the most
linked previous ones, in particular Nadjahi et al. (2020) and Rakotomamonjy & Ralaivola (2021). The work
of Nadjahi et al. (2020) is focused on sliced Wasserstein distance and its statistical properties, however our
work is based on the properties of the Gaussian smoothed with general divergences (e.g. Wasserstein, MMD,
Sinkhorn divergence). We argue that the properties cannot be directly derived from (Nadjahi et al., 2020),
especially the sample complexity result. In Rakotomamonjy & Ralaivola (2021), the authors investigated the
smoothed Wasserstein distance and their theoretical ﬁnding was principally on proving the metric property,
whereas we further investigate sample and projection complexities and the continuity properties w.r.t. the
smoothing noise level. We emphasize that the novelty of the present paper consists in the theoretical
properties derived from the deﬁnition of the empirical measure ˆˆµn. The smoothing of the raw measures, from
a theoretical point view, is a continuous measure (see Lemma 3.5) that needs to be sampled. This entails to
deﬁne the second sampling step and construct ˆˆµn, an empirical version for the smoothing projection of µ. To
the best of our knowledge, this work is the ﬁrst introducing the double randomness in the case of smoothing
optimal transport discrepancies. Recent works (Goldfeld et al., 2020; Nietert et al., 2021) addressed the
smoothing Wasserstein an their theoretical results relied only on ˆµn.
Layout of the paper. The paper is organized as follows: after introducing the notation and some
background in Section 2, we detail the topological properties of Gaussian-smoothed sliced divergence in
Section 3.1 while the double sampling process and its statistical properties are established in Section 3.2. The
noise analyses are provided in Section 3.3. Experimental analyses for supporting the theory and showcasing the
relevance of our divergences in domain adaptation are depicted in Section 4. Discussions on the perspectives
and limitations are in Section 5. All the proofs of the theoretical results and some additional experiments are
postponed to the appendices in the supplementary.
2Published in Transactions on Machine Learning Research (11/2024)
2 Preliminaries
For the reader’s convenience, we provide a brief summary of standard notations and deﬁnitions used throughout
the paper.
Notation. Ford∈N∗, letP(Rd)be the set of Borel probability measures on RdandPp(Rd)⊂ P(Rd), those
with ﬁnite moment of order p, i.e.,Pp(Rd),{µ∈ P:/integraltext
/bardblx/bardblpdµ(x)<∞}, where /bardbl · /bardbl is the Euclidean norm.
We denote Mp(µ) =/integraltext
x/bardblx/bardblpdµ(x).For two probability distributions µandν, we denote their convolution
asµ∗ν∈ P(Rd), namely (µ∗ν)(A) =/integraltext
x/integraltext
y1A(x+y)dµ(x)dν(y), where 1A(·)is the indicator function
over A. Given two independent random variables X∼µandY∼ν, we remind that X+Y∼µ∗ν. The
d-dimensional unit-sphere is noted as Sd−1,{θ∈Rd:/bardblθ/bardbl= 1}. We denote by udthe uniform distribution
onSd−1and we use δ(·)to denote the Kronecker delta function. We note as Eµfthe expectation of the
function fwith respect to µ.
LetΓ :R→Rbe the Gamma function expressed as Γ(v) =/integraltext∞
0tv−1e−tdtforv >0. For k∈N,(·)kdenoted
the Pochhammer symbol, also known in the literature as a rising factorial, namely (α)0= 1,(α)1=α, and
(α)k=Γ(α+k)
Γ(k)=α(α+ 1)···(α+k−1), for k≥1. We denote by 1F1(α, γ;z)the Kummers conﬂuent
hypergeometric function (Olver, 2010) and deﬁned by 1F1(α, γ;z) =/summationtext∞
k=0(α)k
(γ)kzk
k!.
Sliced Wasserstein distance. We remind in this paragraph several measures of similarity between two
distributions. The Wasserstein distance of order p∈[1,∞)between two measures in Pp(Rd)is given by the
relaxation of the optimal transport problem, and it is deﬁned as
Wp(µ, ν) =/parenleftbigg
inf
γ∈Π(µ,ν)/integraldisplay
Rd×Rd/bardblx−x/prime/bardblpγ(x, x/prime)dxdx/prime/parenrightbigg1/p
where Π(µ, ν),{γ∈ P(Rd×Rd)|π1#γ=µ, π 2#γ=ν}andπ1, π2are the marginal projectors of γon each
of its coordinates. When d= 1, the Wasserstein distance can be calculated in closed-form owing to the
cumulative distributions of µandν(Rachev & Rüschendorf, 1998). In practice for empirical distributions,
the closed-form solution requires only the sorting of the samples, which makes it very eﬃcient. Because
of this eﬃciency, eﬀorts have been devoted to derive a metric for high-dimensional distributions based
on 1D Wasserstein distance. The main idea is to project high-dimensional probability distributions onto
a random one-dimensional space and then to compute the Wasserstein distance. This operation can be
theoretically formalized through the use of the Radon transform, leading to the so-called sliced Wasserstein
distance (Kolouri et al., 2016; Bonneel & Coeurjolly, 2019; Kolouri et al., 2019b; Nguyen et al., 2021).
Deﬁnition 2.1. For any p∈[1,∞)and two measures µ,ν∈ Pp(Rd), the sliced Wasserstein distance (SW)
reads as
SWp(µ, ν),/parenleftbigg/integraldisplay
Sd−1Wp
p(Ruµ,Ruν)ud(u)du/parenrightbigg1/p
.
where Ruis the Radon transform of a probability distribution, namely Ruµ(·) =/integraltext
Rdµ(s)δ(· −s/latticetopu)ds. In
practice, the integral is approximated through a Monte-Carlo simulation leading to a sum of 1D Wasserstein
distances over a ﬁxed number of random directions u.
Gaussian-smoothed sliced Wasserstein distance. Based on this deﬁnition of SW, replacing the Radon
projected measures with their Gaussian-smoothed counterpart leads to the following deﬁnition:
Deﬁnition 2.2. Theσ-Gaussian-smoothed p-Sliced Wasserstein distance between probability distributions µ
andνinPp(Rd)writes as
GσSWp(µ, ν),/parenleftbigg/integraldisplay
Sd−1Wp
p(Ruµ∗ Nσ,Ruν∗ Nσ)ud(u)du/parenrightbigg1/p
,
3Published in Transactions on Machine Learning Research (11/2024)
where Nσ=N(0, σ2)is the zero-mean σ2-variance Gaussian measure. It is important to note here that the
smoothing (convolution) operation occurs after projection onto the one-dimensional space. Hence, assuming
X∼µ,Y∼ν, for a given direction u, we compute in the integral the one-dimensional Wasserstein distance
between the probability laws of u/latticetopX+Zandu/latticetopY+Z/primewhere Z, Z/prime∼ N σare independent random variables.
The metric properties of GσSWpforp≥1have been discussed in a recent work (Rakotomamonjy & Ralaivola,
2021). This latter work has also shown, in the context of diﬀerential privacy, the importance of convolving
the Radon projected distribution with a Gaussian instead of computing the SW distance of the original
distribution smoothed with a d-dimensional Gaussian µ∗ NσId, where Iddenotes the d×didentity matrix.
Gaussian-smoothed sliced divergence. The idea of slicing high-dimensional distributions before feeding
them to a divergence between probability distributions can be extended to distances other than the Wasserstein
distance. These sliced divergences have been studied by Nadjahi et al. (2020). Similarly, we can deﬁne a
Gaussian-smoothed sliced divergence, given a divergence DRd:Pp(Rd)× P p(Rd)→R+ford≥1as:
Deﬁnition 2.3. Theσ-Gaussian-smoothed p-Sliced Divergence between probability distributions µandνin
Pp(Rd)associated to the base divergence D,DR,p≥1is
GσSDp(µ, ν),/parenleftbigg/integraldisplay
Sd−1Dp(Ruµ∗ Nσ,Ruν∗ Nσ)ud(u)du/parenrightbigg1/p
.
Typical relevant divergences are the maximum mean discrepancy (MMD) (Gretton et al., 2012) or the
Sinkhorn divergence (Genevay et al., 2018; Peyré & Cuturi, 2019). In Section 4, we report empirical ﬁndings
based on these divergences as well as on the Wasserstein distance.
3 Theoretical properties
In this section, we analyze the properties of the Gaussian-smoothed sliced divergence, in terms of topological
and statistical properties and the inﬂuence of the Gaussian smoothing parameter σon the distance.
3.1 Topology
It has already been shown in Rakotomamonjy & Ralaivola (2021) that the Gaussian-smoothed sliced
Wasserstein is a metric on P(Rd). In the next, we extend these results to any divergence D(·,·)under certain
assumptions.
Theorem 3.1. For any σ >0, p≥1, the following properties hold:
1.ifD(·,·)is non-negative (or symmetric), then GσSDp(·,·)is non-negative (or symmetric);
2.ifD(·,·)satisﬁes the identity of indiscernibles, i.e. for µ/prime, ν/prime∈ P(R),D(µ/prime, ν/prime) = 0 if and only if
µ/prime=ν/prime, then this identity also holds for GσSDp(·,·)for any µ, ν∈ Pp(Rd);
3.ifD(·,·)satisﬁes the triangle inequality then GσSDp(·,·)satisﬁes the triangle inequality.
The above theorem shows that under mild hypotheses over the base divergence D, as being a metric for
instance, the metric property of its Gaussian-smoothed sliced version naturally derives. As exposed in the
appendix, the more involved property to prove is the identity of indiscernibles.
We further postponed to the appendix the proofs of the two other topological properties: (i) GσSDmetrizes
the weak topology on Pp(Rd)and (ii) GσSDis lower semi-continuous with respect to the weak topology in
Pp(Rd).
Now, we establish under which conditions on the divergence D, the convergence of a sequence in GσSD
implies weak convergence in Pp(Rd). We say that {µk}k∈Nconverges weakly toµand write, µk⇒µ, if/integraltext
f(x)dµk(x)→/integraltext
f(x)dµ(x), ask→ ∞ ,for every fin the space of all bounded continuous real functions.
Theorem 3.2. Letσ >0, p≥1,µ∈ Pp(Rd), and {µk∈ Pp(Rd)}k∈Na sequence of distributions. Assume
that the divergence Dis bounded and metrizes the weak topology on P(R). Then, limk→∞GσSDp(µk, µ) = 0
if and only if µk⇒µ.
4Published in Transactions on Machine Learning Research (11/2024)
Note that Theorem 3.2 extends the results of Nadjahi et al. (2020) to Gaussian-smoothed distributions, as we
retrieve them as a special case for σ= 0. In addition, based on Theorem 3.2 by Lin et al. (2021) and the
above, we can also claim that the Gaussian-smoothed SWD metrizes the weak convergence.
Proposition 3.3. Letσ >0, p≥1and assume that the base divergence Dis lower semi-continuous w.r.t. the
weak topology in P(R). Then, GσSDpis lower semi-continuous with respect to the weak topology in Pp(Rd).
When the base divergence Dis equal to the Wasserstein distance Wp, that is lower semi-continuous (Villani,
2009), then Proposition 3.3 shows that the smoothed sliced Wasserstein distance is semi-lower continuous too.
3.2 Statistical properties
The next theoretical question we are interested in is about the incurred error when the true distribution µis
approximated by its empirical distribution ˆµn. Such a case is common in practical applications where only (high-
dimensional) empirical samples are at disposal. Speciﬁcally, we are interested in quantifying two key properties
of empirical Gaussian-smoothed divergence: (i)the convergence of the double empirical ˆGσSDp(ˆµn,ˆνn)
(see Deﬁnition 3.6) to GσSDp(µ, ν)(ii)the convergence of \GσSDp(µ, ν)(see (1)) to GσSDp(µ, ν), when
approximating the expectation over the random projection with sample mean.
Letˆµn=1
n/summationtextn
i=1δXiandˆνn=1
n/summationtextn
i=1δYibe the empirical probability measures of independent observations.
The smoothed Gaussian sliced divergence between ˆµnandˆνnis given by
GσSDp(ˆµn,ˆνn) =/parenleftbigg/integraldisplay
Sd−1Dp/parenleftbig
Ruˆµn∗ Nσ,Ruˆνn∗ Nσ/parenrightbig
ud(u)du/parenrightbigg1/p
.
Remark 3.4.Remark that for a ﬁxed u∈Sd−1, the distributions Ruˆµn∗ NσandRuˆνn∗ Nσarecontinuous ,
in particular they are a mixture of Gaussian distributions centered on the projected samples with variance σ2.
Lemma 3.5. Conditionally on the samples {Xi}i=1,...,n and{Yi}i=1,...,n, one has: Ruˆµn∗ N σ=
1
n/summationtextn
i=1N(u/latticetopXi, σ2)andRuˆνn∗ Nσ=1
n/summationtextn
i=1N(u/latticetopYi, σ2).
Note that we further need to sample with respect to the continuous mixture Gaussian measures in Lemma 3.5
in order to get a fully empirical measure version of GσSD(µ, ν). To this end, we next deﬁne the double
empirical divergence of GσSD.
3.2.1 Double empirical divergence of GσSD
LetTx
1, . . . , Tx
nandTy
1, . . . , Ty
nbe i.i.d. observations of Ruˆµn∗NσandRuˆνn∗Nσ,respectively. Sampling i.i.d.
{Tx
i}i=1,...,n is given by the following scheme: for i= 1, . . . , n , we ﬁrst choose the component N(u/latticetopXi, σ2)
from the mixture1
n/summationtextn
i=1N(u/latticetopXi, σ2)then we generate Tx
i=u/latticetopXi+Zx
i, where Zx
i∼ N σ.Hence, we set,
for a given u
ˆˆµn=1
nn/summationdisplay
i=1δTx
i=1
nn/summationdisplay
i=1δu/latticetopXi+Zx
iandˆˆνn=1
nn/summationdisplay
i=1δTy
i=1
nn/summationdisplay
i=1δu/latticetopYi+Zy
i.
The measure ˆˆµn∈ P(R)deﬁnes an empirical version of the continuous Ruˆµn∗ Nσdenoted as \Ruˆµn∗ Nσ
(similarly ˆˆνn=\Ruˆνn∗ Nσ). Using the aforementioned notation, we deﬁne.
Deﬁnition 3.6. The double empirical smoothed Gaussian sliced divergence reads as
ˆGσSDp(ˆµn,ˆνn),/parenleftbigg/integraldisplay
Sd−1Dp(ˆˆµn,ˆˆνn)ud(u)du/parenrightbigg1/p
.
Remark 3.7.(i)It is worth to comment the double randomnesses showing in the deﬁnition of ˆGσSDp(ˆµn,ˆνn):
the ﬁrst comes from sampling according to the original probability measure ( µorν) whereas the second takes
place from sampling according to the mixture1
n/summationtextn
i=1N(u/latticetopXi, σ2).
(ii)The empirical measure of the convolution \Ruµ∗ Nσcould be written as1
n/summationtextn
i=1δUx
i+Qx
iallowing to
5Published in Transactions on Machine Learning Research (11/2024)
sample in a one shot ni.i.d. samples Ux
i+Qx
isuch that Ux
i∼ R uµandQx
i∼ N σ. From an empirical
view, sampling according to Ruµ∗ Nσis intractable. For that reason, our theoretical results and numerical
experiments are based on ˆˆµn,ˆˆνn, and hence with respect to ˆGσSDp(ˆµn,ˆνn).
3.2.2 Sample complexity of GσSWp
Herein, our goal is to quantify the error made when approximating GσSWp(µ, ν)with ˆGσSWp(ˆµn,ˆνn).
More precisely, we are interested in establishing an order of the convergence rate of ˆGσSDp(ˆµn,ˆνn)towards
GσSDp(µ, ν), according to the sample size n.This rate stands for the so-called sample complexity.
The convergence results in the sequel are given in expectation. Recall that the empirical distributions
are derived from a double sampling process, which leads to consider a double expectations, wrt the origin
distribution Eµ⊗nand wrt the sampling from the Gaussian smoothing EN⊗nσwhere µ⊗nandN⊗nσare the
n-fold product extensions of µandNσ, respectively. We ﬁrst consider the conditional expectation given the
samples X1, . . . , X n, i.e.EN⊗nσ[·|X1, . . . , X n], and then apply Eµ⊗n. We denote by
Eµ⊗n|N⊗nσ[·] =Eµ⊗n/bracketleftbig
EN⊗nσ[·|X1, . . . , X n]/bracketrightbig
.
Next, we focus on the sample complexity for the special case of Gaussian-smoothed sliced Wasserstein
distance.
Proposition 3.8. Fixσ >0, p≥1andϑ >√
2. For X∼µ, assume that/integraltext∞
0e2ξ2
σ2ϑ2P/bracketleftbig
/bardblX/bardbl> ξ/bracketrightbig
dξ <∞.
Then,
Eµ⊗n|N⊗nσ[ˆGσSWp(ˆµn, µ)]≤Ξp,σ,ϑ1
n1/2p+ Υ p,σ,µ(logn)1/p
n1/p,
where Ξp,σ,ϑ =25
2−5
4p
π1/2pσ1−1
4pϑ1+1
p/parenleftbig
Γ/parenleftbig
p+1
2/parenrightbig/parenleftbig/radicalBig
4πσ2ϑ2
ϑ2−2+ 4/integraltext∞
0e2ξ2
σ2ϑ2P[/bardblX/bardbl> ξ]dξ/parenrightbig/parenrightbig1/2pand Υp,σ,µ =
22−1
2pCp
π1/2pσ2/parenleftbig
Γ(p+1
2)/summationtext∞
k=0(−p)k
(1
2)k(−1)k
(2σ2)kk!M2k(µ)/parenrightbig1/pwith Cpis a positive constant depending only on p.
It is worth to note that for p∈N∗, e.g. p= 2(standard choice for numerical experiments), the (pseudo)
conﬂuent hypergeometric function/summationtext∞
k=0(−p)k
(1
2)k(−1)k
(2σ2)kk!M2k(µ)is only depending on the 2k-th moments of µ
fork= 1, . . . , p, since (−p)(k)= 0fork≥p+ 1.Now, let us sketch the proof of Proposition 3.8: we ﬁrst insert
the proxy term of mixture Gaussian distribution1
n/summationtextn
i=1N(u/latticetopXi, σ2), then by an application of the triangle
inequality on the Wasserstein distance we are faced to control two terms (i) Wp
p(ˆˆµn,1
n/summationtextn
i=1N(u/latticetopXi, σ2))
and (ii) Wp
p(1
n/summationtextn
i=1N(u/latticetopXi, σ2), µ). For (i) we get a standard order of O(logn
n), which comes from a
by-product of Fournier & Guillin (2015). For (ii), through a coupling via the maximal coupling using the total
variation distance (Theorem 6.15 in Villani (2009)), we obtain the order O(n−1/2). The control technique for
(ii) was inspired from Goldfeld et al. (2020) and Nietert et al. (2021).
Remark 3.9.The condition/integraltext∞
0e2ξ2
σ2ϑ2P/bracketleftbig
/bardblX/bardbl> ξ/bracketrightbig
dξ <∞needs P/bracketleftbig
/bardblX/bardbl> ξ/bracketrightbig
goes to 0faster than e−κξ2for
κ <2/σ2ϑ2. This can be satisﬁed when /bardblX/bardblis aω-sub-gausssian ( ω≥0). Namely, E[eη/latticetop(X−E[X])]≤eω/bardblη/bardbl2
2
for all η∈Rd.If the parameter ωveriﬁes ω < σϑ/ 2, then the latter condition holds.
Remark 3.10.Note that the sample complexity depends on the amount of smoothing through the moment of
the Gaussian noise : the larger the amount of smoothing (and thus the privacy), the worse is the constant
of the complexity. Hence, a trade-oﬀ on privacy and statistical estimation appears here as a reasonable
guarantee on the diﬀerential privacy usually requires a large Gaussian variance.
Proposition 3.11. Under the same conditions of Proposition 3.8, we have
Eµ⊗n|N⊗nσEν⊗n|N⊗nσ[ˆGσSWp(ˆµn,ˆνn)]≤31−1
pGσSWp(µ, ν) + 3Ξ p,σ,ϑ1
n1/2p+ 31−1
p(Υp,σ,µ + Υ p,σ,ν)(logn)1/p
n1/p
6Published in Transactions on Machine Learning Research (11/2024)
Figure 1: Measuring the divergence between two sets of samples in R50, of increasing size, randomly drawn
fromN(0,I). We compare three sliced divergences and their Gaussian-smoothed sliced versions with a σ= 3:
(top) dimension has been set to d= 50 ; (bottom) sample complexity with diﬀerent dimensions. This plot
conﬁrms that the complexity is dimension-independent.
and
GσSWp(µ, ν)≤31−1
pEµ⊗n|N⊗nσEν⊗n|N⊗nσ[ˆGσSWp(ˆµn,ˆνn)] + 3Ξ p,σ,ϑ1
n1/2p+ 31−1
p(Υp,σ,µ + Υ p,σ,ν)(logn)1/p
n1/p.
Proof of Proposition 3.11 relies on a double application of triangle inequality satisﬁed by Wasserstein distance
as follows: Wp(ˆˆµn,ˆˆνn)≤Wp(ˆˆµn,Ruµ∗ Nσ) +Wp(Ruµ∗ Nσ,Ruν∗ Nσ) +Wp(Ruν∗ Nσ,ˆˆνn), combined
with Proposition 3.8. This gives a non sharp convergence result since we get the constant 31−1
pin front of
Eµ⊗n|N⊗nσEν⊗n|N⊗nσ[ˆGσSWp(ˆµn,ˆνn)]orGσSWp(µ, ν). However, when the power p= 1we obtain a sharp
convergence result with O(n−1/2), namely
Eµ⊗n|N⊗nσEν⊗n|N⊗nσ[|ˆGσSW(ˆµn,ˆνn)−GσSW(µ, ν)|]≤3Ξ1,σ,ϑ1√n+ (Υ 1,σ,µ+ Υ 1,σ,ν)logn
n
Despite that our theoretical results hold only for Gaussian-smoothed sliced Wasserstein distance, our empirical
results show that given other base divergences D, shows that the sample complexity of GσSDpis proportional
to the one dimensional sample complexity of Dp(p= 2). Figure 1 provides an empirical illustration of this
statement.
3.2.3 Projection complexity
To compute the Gaussian-smoothed sliced divergence, one may resort to a Monte Carlo scheme to numerically
approximate the integral in GσSDp(µ, ν). Towards this, let deﬁne the following sum:
\GσSDp(µ, ν) =/parenleftBig1
LL/summationdisplay
l=1Dp(Rulµ∗ Nσ,Rulν∗ Nσ)/parenrightBig1/p
, (1)
where ulis a random vector uniformly drawn from Sd−1, for l= 1, . . . , L. Theorem 3.12 shows that for
a ﬁxed dimension d, the root mean square error of Monte Carlo (MC) approximation is of order O/parenleftbig1√
L/parenrightbig
,
which corresponds to the projection complexity. We denote by u⊗L
dand the L-fold product extensions of the
uniform measure udon the unit sphere.
Proposition 3.12. Letσ >0, p≥1. Then the error related to the MC-estimation of GσSDpis bounded as
follows
Eu⊗L
d[|\GσSDpp
(µ, ν)−GσSDp
p(µ, ν)|]≤A(p, σ)√
L,
7Published in Transactions on Machine Learning Research (11/2024)
where A2(p, σ) =/integraltext
Sd−1/parenleftbig
Dp(Ruµ∗ Nσ,Ruν∗ Nσ)−¯τp/parenrightbig2ud(u)du,with ¯τp=/integraltext
Sd−1Dp(Ruµ∗ Nσ,Ruν∗
Nσ)ud(u)du.
The term A2(p, σ)corresponds to the variance of Dp(Ruµ∗Nσ,Ruν∗Nσ)with respect to u∼ud. It is worth
to note that the precision of the Monte Carlo scheme approximation depends on the number of projections L
and the variance of the evaluations of the divergence Dp.The estimation error decreases at the rate L−1/2
according to the number of projections used to compute the smoothed sliced divergence.
Given the above results, we provide a ﬁner analysis of GσSWp(µ, ν)’s sample complexity. Towards this ends,
for a ﬁxed random projection ul,(1≤l≤L)we deﬁne ˆˆµn,l=1
n/summationtextn
i=1δu/latticetop
lXi+Zx
i(similarly for ˆˆνn,l)and set
\ˆGσSDp(ˆµn,ˆνn) =/parenleftBig1
LL/summationdisplay
l=1Wp
p(ˆˆµn,l,ˆˆνn,l)/parenrightBig1/p
The overall complexity of GσSDp(µ, ν)consists in its approximation by sampling and projection of the origin
probability measures µ, ν, i.e. through\ˆGσSDp(ˆµn,ˆνn).By application of triangle inequality, one has
|\ˆGσSWpp
(ˆµn,ˆνn)−GσSWp
p(µ, ν)| ≤ |\ˆGσSWpp
(ˆµn,ˆνn)−ˆGσSWp
p(ˆµn,ˆνn)|+|ˆGσSWp
p(ˆµn,ˆνn)−GσSWp(µ, ν)|.
The ﬁrst term in the right-hand-side (RHS) of the latter decomposition can be controlled by Proposition 3.12
in the following way:
Eu⊗L
d/bracketleftbig
|\ˆGσSWpp
(ˆµn,ˆνn)−ˆGσSWp
p(ˆµn,ˆνn)|/bracketrightbig
≤ˆA(p, σ)√
L,{Vu∼ud[Wp
p(ˆˆµn,ˆˆνn)]}1/2
√
L.
However we don’t have a proper control for p≥2of the second term in the RHS, |ˆGσSWp
p(ˆµn,ˆνn)−
GσSWp(µ, ν)|, as it can be seen from Proposition 3.11. For that reason, we derive an overall complexity in
the case of p= 1.
Corollary 3.13. The sample and projection complexities of GσSW(µ, ν)reads as complexity (GσSW) =
O(n−1/2+L−1/2).If we consider the number of projections as L=⌊nβ⌋for some β∈(0,1)then the overall
complexity complexity(G σSW(µ, ν)) =O(n−β/2).
3.3 Noise-level dependencies
The parameter σof the Gaussian smoothing function Nσmay signiﬁcantly inﬂuence the attained privacy level.
Hence, we provide theoretical results analyzing the eﬀect of the noise level σon the induced Gaussian-smoothed
sliced divergence.
3.4 Order relation
We ﬁrst show that the noise level tends to reduce the diﬀerence between two distributions as measured using
GσSDp(µ, ν)provided the base divergence Dsatisﬁes some mild assumptions.
Proposition 3.14. Letµ, ν∈ P p(Rd)and consider the noise levels σ1, σ2such that 0≤σ1≤σ2<∞.
Assume that the base divergence Dsatisﬁes D(µ/prime∗Nσ2, ν/prime∗Nσ2)≤D(µ/prime∗Nσ1, ν/prime∗Nσ1),for any µ/prime, ν/prime∈ P(R).
Then, Gσ2SDp(µ, ν)≤Gσ1SDp(µ, ν).
Note that the assumption for the base divergence inequality holds for the Gaussian-smoothed Wasserstein
distance Nietert et al. (2021). While we conjecture that it holds also for smoothed Sinkhorn and MMD, we
leave the proofs for future works. Based on the property in Proposition 3.14, we show some speciﬁc properties
of the metric with respect to the noise level σ.
Proposition 3.15. GσSDp(µ, ν)is decreasing with respect to σand we have limσ→0GσSDp(µ, ν) =Dp(µ, ν).
The proof of Proposition 3.15 comes straightforwardly from Proposition 3.14 by taking σ2=σand letting
σ1→0. This property interestingly states that the GσSDprecovers the sliced divergence when the noise
level vanishes. We end up this section by providing a relation between Gaussian-smoothed sliced Wasserstein
distances under two noise levels.
8Published in Transactions on Machine Learning Research (11/2024)
Figure 2: Absolute diﬀerence between the approximated Monte Carlo approximation of all divergences
compared to the true one (evaluated with 10,000number of projections). The two sets of 500samples in R50
are randomly drawn from N(0,I). The Gaussian-smoothed sliced divergences are parameterized with σ= 3.
Proposition 3.16. Let0≤σ1≤σ2be two noise levels. Then, one has Gσ2SWp(µ, ν)≤Gσ1SWp(µ, ν)and
|Gσ1SWp(µ, ν)−Gσ2SWp(µ, ν)| ≤(21−1
p−1) G σ2SWp(µ, ν) + 25
2(σ2
2−σ2
1),
in particular for p= 1,|GσSW(µ, ν)−Gσ2SW(µ, ν)| ≤25
2(σ2
2−σ2
1).
3.4.1 Continuity
Now we analyze the continuity properties of some GσSDp(µ, ν)w.r.t. the noise level.
Proposition 3.17. For any two distributions µandνfor which the sliced Wasserstein is well-deﬁned, the
Gaussian-smoothed sliced Wasserstein distance is continuous w.r.t. to σ.
Proposition 3.18. Assume that the kernel deﬁning the maximum mean discrepancy (MMD )divergence is
bounded. Then the Gaussian-smoothed sliced GσMMD is continuous w.r.t. to σ.
The above propositions show that most distribution divergences are continuous with respect to σunder mild
conditions.
4 Numerical Experiments
In this section, we report on a series of experiments that support the established theoretical results. We also
highlight the usefulness of the ﬁndings in a context of privacy-preserving domain adaptation problem.
4.1 Supporting the theoretical results
Sample complexity. The ﬁrst experiment (see Figure 1) analyzes the sample complexity of diﬀerent
base divergences. It shows that the sample complexity stays similar to the one of their original and sliced
counterparts up to a constant (see Proposition 3.8). For this purpose, we have considered samples in Rd
randomly drawn from a Normal distribution N(0,I). For the Sinkhorn divergence, the entropy regularization
has been set to 0.1and for MMD, we used a Gaussian kernel for which the bandwidth has been set to the
mean of all pairwise distances between samples. The number of projections has been ﬁxed to L= 50 and we
perform 20 runs per experiment. For the ﬁrst study, the convergence rate has been evaluated by increasing the
samples number up to 25,000 with ﬁxed dimension d= 50 . For the second one, we vary both the dimension
and the number of samples.
Figure 1 shows the sample complexity of some sliced divergences, respectively noted as SWD, SKD and
MMD for Sliced Wasserstein distance, Sinkhorn divergence and Maximum Mean discrepancy and their
Gaussian-smoothed sliced versions, named as GS SWD, GS SKD and GS MMD. On the top plot, we can see
that all Gaussian-smoothed sliced divergences preserve the complexity rate with just a slight to moderate
9Published in Transactions on Machine Learning Research (11/2024)
Figure 3: Measuring the divergence between two sets of samples in R50drawn from N(0,I). We plot the
sample complexity for diﬀerent Gaussian-smoothed sliced divergence at diﬀerent level of noises.
overhead. The worst diﬀerence is for Sinkhorn divergence, while MMD almost comes for free in term of
complexity. From the bottom plot where sample complexities for diﬀerent dimensions dare given, we conﬁrm
the ﬁnding that Gaussian smoothing keeps the independence of the convergence rate to the dimension of
sliced divergences.
Two other experiments on the sample complexity and identity of indiscernibles are also reported in the
supplementary material.
Projection complexity. We have also investigated the impact of the number of projections when estimating
the distance between two sets of 500samples drawn from the same distribution, N(0,I). Figure 2 plots
the approximation error between the true expectation of the sliced divergences (computed for a number of
L= 10,000projections) and its approximated versions. We remark that, for all methods, the error ranges
within 10-fold when approximating with 50projections and decreases with the number of projections.
Performance path on the impact of the noise parameter. Since the Gaussian smoothing parameter
σis key in a privacy preserving context, as it impacts on the level of privacy of the Gaussian mechanism, we
have analyzed its impact on the smoothed sliced divergence. We have reproduced the experiment for the
sample complexity but with diﬀerent values of σ. The number of projections has been set to 50. Figure 3
shows these sample complexities. The ﬁrst very interesting point to note is that the smoothing parameter has
almost no eﬀect on the GS MMD sample complexity. For the GS SWD and GS SKD divergences, instead,
the smoothing tends to increase the divergence at ﬁxed number of samples. Another interpretation is that
to achieve a given value of divergence, one needs more far samples when the smoothing is larger ( i.e.for
getting a given divergence value at σ= 5, one needs almost 10-fold more samples for σ= 15 ). This overhead
of samples needed when smoothing increases is properly described, for the Gaussian-smoothed sliced SWD in
our Proposition 3.8, as the sample complexity depends on the moments of the Gaussian.
As for conclusion from these analyses, we highlight that the Gaussian-smoothed sliced MMD seems to present
several strong beneﬁts: its sample complexity does not depend on the dimension and seems to be the best
one among the divergence we considered. More interestingly, it is not impacted by the amount of Gaussian
smoothing and thus not impacted by a desired privacy level.
4.2 Domain adaptation with GσSW
As an application, we have considered the problem of unsupervised domain adaptation for a classiﬁcation task.
In this context, given source examples Xsand their label ysand unlabeled target examples Xt, our goal is to
design a classiﬁer h(·)learned from the source examples that generalizes well on the target ones. A classical
approach consists in learning a representation mapping g(·)that leads to invariant latent representations,
invariance being measured as a distance between empirical distributions of mapped source and target samples.
10Published in Transactions on Machine Learning Research (11/2024)
Figure 4: Domain adaptation performances using diﬀerent divergences on distributions with respect to
the Gaussian smoothing. (Left) USPS to MNIST. (Middle) Oﬃce-31 Webcam to DSLR. (Right) Oﬃce-31
Amazon to Webcam.
Figure 5: Domain adaptation performances using diﬀerent divergences on distributions with respect to
the Gaussian smoothing using one-epoch-ﬁne-tuned models. (Left) USPS to MNIST. (Middle) Oﬃce-31
Webcam to DSLR. (Right) Oﬃce-31 Amazon to Webcam.
Formally, this leads to the following problem
min
g,h/braceleftbig
Lc(h(g(Xs)),ys) +D(g(Xs), g(Xt))/bracerightbig
where Lccan be the cross-entropy loss or a quadratic loss and Da divergence between empirical distributions,
in our case, Dwill be any Gaussian-smoothed sliced divergence. We solve this problem through stochastic
gradient descent, similarly to many approaches that use sliced Wasserstein distance as a distribution
distance Lee et al. (2019). Note that, in practice, using a smoothed divergence preserves the privacy of the
target samples as shown by (Rakotomamonjy & Ralaivola, 2021).
When performing such model adaptation, a privacy/utility trade-oﬀ that has to be handled. In practice, one
would prefer the most private model while not hurting its performance. Hence, one would seek the largest
noise level σ >0to use while preserving accuracy on target domain. Hence, it is useful to evaluate how the
model performs on a range of noise level (hence, privacy level). This can be computationally expensive at it
requires to fully train several models on hundreds of epochs. Instead, we leverage on the continuity of our
GσSDto employ a ﬁne-tuning strategy: we train a domain adaptation model for the largest desired value of
σ(over the full number of epochs) and when σis decreased, we just ﬁne-tune the lasted model by training on
only one epoch.
Our experiments evaluate the studied Gaussian-smoothed sliced divergences in classical unsupervised domain
adaptation. We have considered two datasets: a handwritten digit recognition (USPS/MNIST) and Oﬃce 31
datasets.
In our ﬁrst analysis, we have compared our GσSDperformances with non-smoothed divergences. The ﬁrst
one is the sliced Wasserstein distance (SWD) Lee et al. (2019) and the second one is the Jenssen-Shannon
approximation based on adversarial approach, known as DANN Ganin & Lempitsky (2015). For all methods
11Published in Transactions on Machine Learning Research (11/2024)
and for each dataset, we used the same neural network architecture for representation mapping and for
classiﬁcation. Approaches diﬀer only on how distance between distributions have been computed. Here for
each noise value σ, we have trained the model from scratch for 100epochs. Results are depicted in Figure 4.
For the two problems, we can see that performances obtained with the Gaussian-smoothed sliced Wasserstein
or MMD divergences are similar to those obtained with DANN or SWD across all ranges of noise. The
smoothed version of Sinkhorn is less stable and induces a slight loss of performance. Owing to the metric
property and the induced weak topology, the privacy preservation comes almost without loss of performance
in this domain adaptation context.
In the second analysis, we have studied the privacy/utility trade-oﬀ when ﬁne-tuning models, using only one
epoch, for decreasing values of σ. Results are shown in Figure 5. They highlight that depending on the data
and the used smoothed divergence, performance varies between one percent for Oﬃce 31 to four percent for
USPS to MNIST. Note that except for the largest value of σ, we are training a model using only one epoch
instead of a hundred. A very large gain in complexity is thus achieved for swiping the full range of noise level.
Hence depending on the importance this slight drop in performance will have, it is worth using a large value
ofσand preserving strong privacy or go through a validation procedure of several (cheaply obtained) models.
5 Conclusion
This work provided the properties of Gaussian-smoothed sliced divergences for comparing distributions. We
derived several theoretical results related to their topological and statistical properties and showed, under
mild conditions on their base divergences, the smoothing and slicing operations preserves the metric property.
From a statistical point of view, we introduced the double empirical distribution and focused on the sample
complexity of the smoothed sliced Wasserstein distance and we proved that it converges with a rate O(n−1/2p).
We furhter analyzed the behavior of these divergences on domain adaptation problems and conﬁrm the fact
that using those divergences yields only to slight loss of performances while preserving privacy. Note that in
the obtained bound we use upper bound of higher moments of the smoothing distribution. An important
direction for future research is considering non Gaussian smoothing distribution enjoying this property.
12Published in Transactions on Machine Learning Research (11/2024)
References
Martin Arjovsky, Soumith Chintala, and Léon Bottou. Wasserstein generative adversarial networks. In
International conference on machine learning , pp. 214–223. PMLR, 2017.
Krishna B. Athreya and Soumendra N. Lahiri. Measure Theory and Probability Theory . Springer Texts in
Statistics. Springer, 2006. ISBN 9780387329031.
Nicolas Bonneel and David Coeurjolly. Spot: Sliced partial optimal transport. 38(4), 2019. ISSN 0730-0301.
Nicolas Bonneel, Michiel van de Panne, Sylvain Paris, and Wolfgang Heidrich. Displacement interpolation
using lagrangian mass transport. ACM Trans. Graph. , 30(6):158:1–158:12, 2011. ISSN 0730-0301.
Nicolas Bonnotte. Unidimensional and Evolution Methods for Optimal Transportation . Theses, Université
Paris Sud - Paris XI ; Scuola normale superiore (Pise, Italie), December 2013.
Adam Bowers and Nigel Kalton. An Introductory Course in Functional Analysis . Universitext. Springer New
York, 2014. ISBN 9781493919444.
Nicolas Courty, Rémi Flamary, Devis Tuia, and Alain Rakotomamonjy. Optimal transport for domain
adaptation. IEEE transactions on pattern analysis and machine intelligence , 39(9):1853–1865, 2016.
H. Cramér and H. Wold. Some theorems on distribution functions. Journal of the London Mathematical
Society , s1-11(4):290–294, 1936.
Ishan Deshpande, Yuan-Ting Hu, Ruoyu Sun, Ayis Pyrros, Nasir Siddiqui, Sanmi Koyejo, Zhizhen Zhao,
David Forsyth, and Alexander G. Schwing. Max-sliced Wasserstein distance and its use for gans. In 2019
IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , pp. 10640–10648, 2019.
Cynthia Dwork, Aaron Roth, et al. The algorithmic foundations of diﬀerential privacy. Foundations and
Trends ®in Theoretical Computer Science , 9(3–4):211–407, 2014.
Nicolas Fournier and Arnaud Guillin. On the rate of convergence in Wasserstein distance of the empirical
measure. Probability Theory and Related Fields , 162(3):707–738, Aug 2015. ISSN 1432-2064.
Yaroslav Ganin and Victor Lempitsky. Unsupervised domain adaptation by backpropagation. In International
conference on machine learning , pp. 1180–1189. PMLR, 2015.
Aude Genevay, Gabriel Peyré, and Marco Cuturi. Learning generative models with sinkhorn divergences. In
International Conference on Artiﬁcial Intelligence and Statistics , pp. 1608–1617. PMLR, 2018.
Ziv Goldfeld, Kristjan Greenewald, Jonathan Niles-Weed, and Yury Polyanskiy. Convergence of smoothed
empirical measures with applications to entropy estimation. IEEE Transactions on Information Theory , 66
(7):4368–4391, 2020.
Arthur Gretton, Karsten M Borgwardt, Malte J Rasch, Bernhard Schölkopf, and Alexander Smola. A kernel
two-sample test. The Journal of Machine Learning Research , 13(1):723–773, 2012.
Peter J. Huber. Robust Statistics , pp. 1248–1251. Springer Berlin Heidelberg, Berlin, Heidelberg, 2011. ISBN
978-3-642-04898-2.
Leonid V. Kantorovich. On the transfer of masses (in russian). Doklady Akademii Nauk , 2:227–229, 1942.
Davar Khoshnevisan. Probability . Graduate studies in mathematics. American Mathematical Soc., 2007.
ISBN 9780821884010.
Soheil Kolouri, Yang Zou, and Gustavo K. Rohde. Sliced Wasserstein kernels for probability distributions. In
2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , pp. 5258–5267, 2016.
Soheil Kolouri, Se Rim Park, Matthew Thorpe, Dejan Slepcev, and Gustavo K. Rohde. Optimal mass
transport: Signal processing and machine-learning applications. IEEE Signal Processing Magazine , 34(4):
43–59, July 2017. ISSN 1053-5888.
13Published in Transactions on Machine Learning Research (11/2024)
Soheil Kolouri, Phillip E. Pope , Charles E. Martin, and Gustavo K. Rohde. Sliced Wasserstein auto-encoders.
InInternational Conference on Learning Representations , 2019a.
Soheil Kolouri, Kimia Nadjahi, Umut Simsekli, Roland Badeau, and Gustavo Rohde. Generalized sliced
Wasserstein distances. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d /quotesingle.ts1Alché-Buc, E. Fox, and R. Garnett
(eds.), Advances in Neural Information Processing Systems , volume 32, pp. 261–272. Curran Associates,
Inc., 2019b.
Chen-Yu Lee, Tanmay Batra, Mohammad Haris Baig, and Daniel Ulbricht. Sliced wasserstein discrepancy
for unsupervised domain adaptation. In Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition , pp. 10285–10295, 2019.
Tianyi Lin, Zeyu Zheng, Elynn Chen, Marco Cuturi, and Michael Jordan. On projection robust optimal
transport: Sample complexity and model misspeciﬁcation. In International Conference on Artiﬁcial
Intelligence and Statistics , pp. 262–270. PMLR, 2021.
Mingsheng Long, Yue Cao, Jianmin Wang, and Michael Jordan. Learning transferable features with deep
adaptation networks. In International conference on machine learning , pp. 97–105. PMLR, 2015.
Gaspard Monge. Mémoire sur la théotie des déblais et des remblais. Histoire de l’Académie Royale des
Sciences , pp. 666–704, 1781.
Kimia Nadjahi, Alain Durmus, Lénaïc Chizat, Soheil Kolouri, Shahin Shahrampour, and Umut Şimşekli.
Statistical and topological properties of sliced probability divergences. In Hugo Larochelle, Marc’Aurelio
Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin (eds.), Advances in Neural Information
Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020,
December 6-12, 2020, virtual , 2020.
Khai Nguyen, Nhat Ho, Tung Pham, and Hung Bui. Distributional sliced-wasserstein and applications to
generative modeling. In International Conference on Learning Representations , 2021.
Khai Nguyen, Tongzheng Ren, Huy Nguyen, Litu Rout, Tan Nguyen, and Nhat Ho. Hierarchical sliced
wasserstein distance. arXiv preprint arXiv:2209.13570 , 2022.
Khai Nguyen, Dang Nguyen, and Nhat Ho. Self-attention amortized distributional projection optimization
for sliced wasserstein point-cloud reconstruction. In International Conference on Machine Learning , pp.
26008–26030. PMLR, 2023.
Khai Nguyen, Tongzheng Ren, and Nhat Ho. Markovian sliced wasserstein distances: Beyond independent
projections. Advances in Neural Information Processing Systems , 36, 2024.
Sloan Nietert, Ziv Goldfeld, and Kengo Kato. Smooth p-wasserstein distance: Structure, empirical approx-
imation, and statistical applications. In Marina Meila and Tong Zhang (eds.), Proceedings of the 38th
International Conference on Machine Learning , volume 139 of Proceedings of Machine Learning Research ,
pp. 8172–8183. PMLR, 18–24 Jul 2021.
Frank W. J. Olver. NIST handbook of mathematical functions hardback and CD-ROM . Cambridge university
press, 2010.
Gabriel Peyré and Marco Cuturi. Computational optimal transport. Foundations and Trendső in Machine
Learning , 11(5-6):355–607, 2019. ISSN 1935-8237.
Svetlozar T. Rachev and Ludger Rüschendorf. Mass Transportation Problems: Volume I: Theory . Mass
Transportation Problems. Springer, 1998. ISBN 9780387983509.
Alain Rakotomamonjy and Liva Ralaivola. Diﬀerentially private sliced wasserstein distance. In Marina Meila
and Tong Zhang (eds.), Proceedings of the 38th International Conference on Machine Learning , volume
139 of Proceedings of Machine Learning Research , pp. 8810–8820. PMLR, 18–24 Jul 2021.
14Published in Transactions on Machine Learning Research (11/2024)
Tim Salimans, Han Zhang, Alec Radford, and Dimitris Metaxas. Improving GANs using optimal transport.
InInternational Conference on Learning Representations , 2018.
Justin Solomon, Fernando d de Goes, Gabriel Peyré, Marco Cuturi, Adrian Butscher, Andy Nguyen, Tao Du,
and Leonidas Guibas. Convolutional Wasserstein distances: Eﬃcient optimal transportation on geometric
domains. ACM Trans. Graph. , 34(4):66:1–66:11, 2015.
Dougal J Sutherland, Hsiao-Yu Tung, Heiko Strathmann, Soumyajit De, Aaditya Ramdas, Alexander J Smola,
and Arthur Gretton. Generative models and model criticism via optimized maximum mean discrepancy.
InICLR (Poster) , 2017.
Cédric Villani. Optimal Transport: Old and New , volume 338 of Grundlehren der mathematischen Wis-
senschaften . Springer Berlin Heidelberg, 2009. ISBN 9783540710509.
Andreas Winkelbauer. Moments and absolute moments of the normal distribution, 2014.
Jiqing Wu, Zhiwu Huang, Dinesh Acharya, Wen Li, Janine Thoma, Danda Pani Paudel, and Luc Van Gool.
Sliced wasserstein generative models. In Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition , pp. 3713–3722, 2019.
15Published in Transactions on Machine Learning Research (11/2024)
A Proofs
In the following sections, we give the proofs of the theoretical guarantees given in the main of the paper.
A.1 Proof of Theorem 3.1: GσSDpis a proper metric on Pp(Rd)× P p(Rd)
Before starting the proof, we add this notation: the characteristic function of a probability distribution
µ∈ P(Rd)isϕµ(t) =Eµ[eiX/latticetopt]. Given this deﬁnition, similarly to the Fourier transform, the characteristic
function of the convolution of two probability distributions readsas ϕν∗µ(t) =ϕν(t)·ϕµ(t).
•Non-negativity (or symmetry). The non-negativity (or symmetry) follows directly from the non-negativity
(or symmetry) of Dp, see Deﬁnition 2.3.
•Identity property. If the base divergence Dpsatisﬁes the identity property in one dimensional measures,
then for any µ∈ P p(Rd)andu∈Sd−1, one has that Dp(Ruµ∗ Nσ,Ruµ∗ Nσ) = 0 ,hence, by Deﬁnition
2.3,GσSDp(µ, µ) = 0 .Let us now prove the fact that for any µ, ν∈ Pp(Rd),GσSDp(µ, ν) = 0 entails µ=ν
a.s. On one hand, GσSDp(µ, ν) = 0 gives the fact that Dp(Ruµ∗ Nσ,Ruν∗ Nσ) = 0 forud-almost every
u∈Sd−1,hence Ruµ∗ Nσ=Ruν∗ Nσforud-almost every u∈Sd−1.Following the techniques in proof
of Proposition 5.1.2 in Bonnotte (2013), for any measure η∈ P(Rm)(with m≥1),F[η](·)stands for the
Fourier transform of ηand is given as F[η](v) =/integraltext
Rme−is/latticetopvdη(s)for any v∈Rm.Then
F[Ruµ∗ Nσ](v) =/integraldisplay
Re−ivtd(Ruµ∗ Nσ)(t)
=/integraldisplay
R/integraldisplay
Re−i(r+t)vdRuµ(r)dNσ(t) (by the deﬁnition of the convolution operator )
=/integraldisplay
Rd/integraldisplay
Re−i(/angbracketleftu,s/angbracketright+t)vdµ(s)dNσ(t) (by the deﬁnition of Radon Transform )
=/integraldisplay
Re−itvdNσ(t)/integraldisplay
Rde−i(/angbracketleftu,s/angbracketright)vdµ(s)
=F[Nσ](v)F[µ](vu).
Since for ud-almost every u∈Sd−1,Ruµ∗ Nσ=Ruν∗ Nσ, and hence F[Ruµ∗ Nσ] =F[Ruν∗ Nσ]⇔
F[Nσ]F[µ] =F[Nσ]F[ν] (by the Fourier transform of the convolution )⇔ F [µ] =F[ν]. Since the Fourier
transform is injective, we conclude that µ=ν.
•Triangle inequality. Assume that Dis a metric and let µ, ν, η ∈ Pp(Rd).We then have
GσSDp(µ, ν) =/parenleftBig/integraldisplay
Sd−1Dp(Ruµ∗ Nσ,Ruν∗ Nσ)ud(u)du/parenrightBig1/p
≤/parenleftBig/integraldisplay
Sd−1/parenleftbig
D(Ruµ∗ Nσ,Ruη∗ Nσ) + D( Ruη∗ Nσ,Ruν∗ Nσ)/parenrightBigp
ud(u)du/parenrightbig1/p
≤/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
(⋆)/parenleftBig/integraldisplay
Sd−1/parenleftbig
Dp(Ruµ∗ Nσ,Ruη∗ Nσ)ud(u)du/parenrightBig1/p
+/parenleftBig/integraldisplay
Sd−1Dp(Ruη∗ Nσ,Ruν∗ Nσ)/parenrightBigp
ud(u)du/parenrightBig1/p
= G σSDp(µ, η) + G σSDp(η, ν),
where inequality in (⋆)follows from the application of Minkowski inequality.
A.2 Proof of Theorem 3.2: GσSDpmetrizes the weak topology
The proof is done by double implications and the technical material relies on the continuous mapping
theorem (Athreya & Lahiri, 2006) and bounded convergence theorem for the ﬁrst direct implication “ ⇒” . The
second one, “ ⇐”, is based on the fact that weak convergence is equivalent to the convergence corresponding
to Lévy-Prokhorov distance (Huber, 2011)
“⇒” Assume that µk⇒µ. Fixu∈Sd−1,the mapping u/mapsto→ R uis continuous from RdtoR, then an application
of continuous mapping theorem (Athreya & Lahiri, 2006) entails that Ruµk⇒ R uµ. By Lévy’s continuity
16Published in Transactions on Machine Learning Research (11/2024)
theorem (Athreya & Lahiri, 2006) Ruµk∗ Nσ⇒ R uµ∗ Nσ. Therefore, limk→∞D(Ruµk,Ruµ∗ Nσ) =
0.Since we suppose that the divergence Dis bounded, then there exists K≥0such that for any k,
Dp(Ruµk,Ruµ∗ Nσ)≤K.An application of bounded convergence theorem yields
lim
k→∞GσSDp(µk, µ) =/parenleftBig/integraldisplay
Sd−1lim
k→∞Dp(Ruµk∗ Nσ,Ruµ∗ Nσ)ud(u)du/parenrightBig1/p
= 0.
“⇐” (By contrapositive). Suppose that µkdoesn’t converge weakly to µand assume that
limk→∞GσSDp(µk, µ) = 0 . On one hand, since Rdis a complete separable space then the weak con-
vergence is equivalent to the convergence corresponding to Lévy-Prokhorov distance Λdeﬁned as: The
Lévy-Prokhorov distance Λ(η, ζ)between η, ζ∈P((E, ρ),T)(space of probability measures on a measurable
metric space) is given by:
Λ(η, ζ) = inf
ε>0{η(A)< ζ(Aε) +ε, ζ (A)< η(Aε) +ε,for all A∈ T } ,where Aε={x∈E:ρ(x, A)< ε}.
Hence there exists ε > 0and a subsequence {µs(k)}k∈Nsuch that Λ(µs(k), µ)> ε. One the other
hand, we have limk→∞GσSDp(µs(k), µ) = 0 , that is equivalent to {D(Ruµs(k)∗ Nσ,Ruν∗ Nσ)}kcon-
verges to 0inLp(Sd−1) ={f:Sd−1→R|/integraltext
Sd−1f(u)ud(u)du <∞}.Since the Lp-convergence en-
tails the point-wise convergence (Khoshnevisan, 2007), there exists a subsequence {µs(t(k))}ksuch that
lim
k→∞D(Ruµs(t(k))∗ Nσ,Ruµ∗ Nσ) = 0 almost everywhere for all u∈Sd−1.Recall that the divergence D
metrizes the weak convergence in P(R)thenRuµs(t(k))∗Nσ⇒ R uµ∗Nσalmost everywhere for all u∈Sd−1.
Therefore, Ruµs(t(k))⇒ R uµalmost everywhere for all u∈Sd−1.Using Cramér-Wold device (Huber, 2011),
we get µs(t(k))⇒µ.Since the Lévy-Prokhorov distance metrizes the weak convergence, it entails that
lim
k→∞Λ(µs(t(k)), µk) = 0 , that contradicts the fact that Λ(µs(k), µ)> ε. We then conclude by contrapositive
that µk⇒µ.
A.3 Proof of Proposition 3.3: GσSDpis lower semi-continuous
Recall that the base divergence Dis lower semi-continuous w.r.t. the weak topology in P(R), namely
for every sequence of measures {µ/prime
k}k∈Nand{ν/prime
k}k∈NinP(R)such that µ/prime
k⇒µ/primeandν/prime
k⇒ν/prime, one has
D(µ/prime, ν/prime)≤lim inf
k→∞D(µ/prime
k, ν/prime
k).
Now, let {µk}k∈Nand{νk}k∈Nare two sequences of measure in Pp(Rd)such that µk⇒µandνk⇒ν.
By continuous mapping theorem (Bowers & Kalton, 2014) and Levy’s continuity theorem, we obtain
Ruµk∗ Nσ⇒ R uµ∗ NσandRuνk∗ Nσ⇒ R uν∗ Nσfor all u∈Sd−1.Since the base divergence Dis a lower
semi-continuous with respect to weak topology in P(R), then
Dp(Ruµ∗ Nσ,Ruν∗ Nσ)≤/parenleftbig
lim inf
k→∞D(Ruµk∗ Nσ,Ruνk∗ Nσ)/parenrightbigp≤lim inf
k→∞Dp(Ruµk∗ Nσ,Ruνk∗ Nσ).
It gives
GσSDp(µ, ν)≤/parenleftBig/integraldisplay
Sd−1lim inf
k→∞Dp(Ruµk∗ Nσ,Ruνk∗ Nσ)ud(u)du/parenrightBig1/p
.
Furthermore, by application of Fatou’s lemma (Bowers & Kalton, 2014), we get
GσSDp(µ, ν)≤lim inf
k→∞/parenleftBig/integraldisplay
Sd−1Dp(Ruµk∗ Nσ,Ruνk∗ Nσ)ud(u)du/parenrightBig1/p
= lim inf
k→∞GσSDp(µk, νk),
which is the desired result.
17Published in Transactions on Machine Learning Research (11/2024)
A.4 Proofs of statistical properties
A.4.1 Proof of Lemma 3.5: Ruˆµn∗ Nσis an average of Gaussian mixture
Straighforwardly, for every Borelian I∈ B(R), we have
Ruˆµn∗ Nσ(I) =/integraldisplay
r/integraldisplay
s1I(r+s)d{1
nn/summationdisplay
i=1δu/latticetopXi}(r)dNσ(s)
=1
nn/summationdisplay
i=1/integraldisplay
s1I(u/latticetopXi+s)fNσ(s)ds
=1
nn/summationdisplay
i=1/integraldisplay
s/prime1I(s/prime)fNσ(s/prime−u/latticetopXi)ds/prime
=1
nn/summationdisplay
i=1/integraldisplay
s/prime1I(s/prime)fN(u/latticetopXi,σ2)(s/prime)ds/prime(since fNσ(s/prime−u/latticetopXi) =fN(u/latticetopXi,σ2)(s/prime))
=1
nn/summationdisplay
i=1N(u/latticetopXi, σ2)(I).
Thanks to Theorem of Cramér and Wold (Cramér & Wold, 1936), we conclude the equality between the
measures Ruˆµn∗ Nσ=1
n/summationtextn
i=1N(u/latticetopXi, σ2).
A.4.2 Proof of Proposition 3.8
Let us give ﬁrst the overall structure of the proof. We we use frequently the triangle inequality for Wasserstein
distances between the quantities ˆˆµn,1
nNσ(u/latticetopXi, σ2)andRuµ∗ Nσ.We then obtain two quantities, Iand
II(see below for explicit), bounding Eµ⊗n|N⊗nσ[ˆGσSWp(ˆµn, µ)]. To control Ibound, we use a well known
converging bound in Fournier & Guillin (2015) of Wasserstein distance between empirical and true measure.
ForIIbound, we consider maximal TV-coupling in Villani (2009)] and use result of the 2p-moment of absolute
Gaussian random variable founded in Winkelbauer (2014).
On one hand, using triangle inequality of Wasserstein distance, we have
Eµ⊗n|N⊗nσ[ˆGσSWp(ˆµn, µ)] =Eµ⊗n|N⊗nσ/bracketleftBig/parenleftBig/integraldisplay
Sd−1Wp
p(ˆˆµn, Ruµ∗ Nσ)ud(u)du/parenrightBig1/p/bracketrightBig
≤/parenleftBig
Eµ⊗n|N⊗nσ/bracketleftBig/integraldisplay
Sd−1Wp
p(ˆˆµn, Ruµ∗ Nσ)ud(u)du/bracketrightBig/parenrightBig1/p
≤/parenleftBig/integraldisplay
Sd−1Eµ⊗n|N⊗nσ[Wp
p(ˆˆµn, Ruµ∗ Nσ)]ud(u)du/parenrightBig1/p
≤(I+II)1/p
where
I,2p−1/integraldisplay
Sd−1Eµ⊗n|N⊗nσ/bracketleftBig
Wp
p/parenleftBig
ˆˆµn,1
nn/summationdisplay
i=1N(u/latticetopXi, σ2)/parenrightBig/bracketrightBig
ud(u)du
and
II,2p−1/integraldisplay
Sd−1Eµ⊗n|N⊗nσ/bracketleftbig
Wp
p/parenleftbig1
nn/summationdisplay
i=1N(u/latticetopXi, σ2), Ruµ∗ Nσ)/parenrightbig/bracketrightbig
ud(u)du
The proof is based on two steps to control the quantities IandII.
Step 1: Control of I.
Let us state the following lemma:
18Published in Transactions on Machine Learning Research (11/2024)
Lemma A.1 (See proof of Theorem 1 in Fournier & Guillin (2015)) .Letη∈ P(R)and let p≥1. Assume
thatMq(η)<∞for some q > p. There exists a constant Cp,qdepending only on p, qsuch that, for all n≥1,
E[Wp
p(ˆηn, η)]≤Cp,qMq(η)p/q∆n(p, q),
where
∆n(p, q) =

n−1/21q>2p,
n−1/2log(n)1q=2p
n−(q−p)/q1p<q< 2p..
We note that ˆˆµnis an empirical version of the Gausian mixture1
n/summationtextn
i=1Nσ(u/latticetopXi, σ2). Then, by application
of Lemma A.1, we get
Eµ⊗n|N⊗nσ/bracketleftbig
Wp
p/parenleftbigˆˆµn,1
nn/summationdisplay
i=1N(u/latticetopXi, σ2)/parenrightbig/bracketrightbig
≤Cp,qEµ⊗n/bracketleftBig
Mp/q
q/parenleftBig1
nn/summationdisplay
i=1N(u/latticetopXi, σ2)/parenrightBig/bracketrightBig
∆n(p, q).
Let us ﬁrst upper bound the q-th moment of Mq/parenleftBig
1
n/summationtextn
i=1N(u/latticetopXi, σ2)/parenrightBig
, for all q≥1.For all u∈Sd−1, we
have
Mq/parenleftBig1
nn/summationdisplay
i=1N(u/latticetopXi, σ2)/parenrightBig
=/integraldisplay
R|t|qd(1
nn/summationdisplay
i=1N(u/latticetopXi, σ2))(t) =1
nn/summationdisplay
i=1Mq(|Zi,u|q),
where Zi,u∼ N(u/latticetopXi, σ2)).By Equation (17) in Winkelbauer (2014) we have
Mq/parenleftBig1
nn/summationdisplay
i=1N(u/latticetopXi, σ2)/parenrightBig
=1
n2q/2σq
√πΓ(q+ 1
2)n/summationdisplay
i=11F1/parenleftbig
−q
2,1
2;−(u/latticetopXi)2
2σ2/parenrightbig
.
Since X1, . . . , X nare i.i.d samples from µ, it yields
Eµ⊗n/bracketleftBig
Mp/q
q/parenleftBig1
nn/summationdisplay
i=1N(u/latticetopXi, σ2)/parenrightBig/bracketrightBig
=2q/2σq
√πΓ(q+ 1
2)Eµ/bracketleftbig
1F1/parenleftbig
−q
2,1
2;−(u/latticetopX)2
2σ2/parenrightbig/bracketrightbig
(X∼µ)
=2q/2σq
√πΓ(q+ 1
2)∞/summationdisplay
k=0(−q
2)k
(1
2)k(−1)k
(2σ2)kk!Eµ[(u/latticetopX)2k]
≤2q/2σq
√πΓ(q+ 1
2)∞/summationdisplay
k=0(−q
2)k
(1
2)k(−1)k
(2σ2)kk!M2k(µ).
Setting q= 2pwe have ∆n(p, q) =logn
n, then
I≤22p−1Cpσ2p
√πΓ(2p+ 1
2)∞/summationdisplay
k=0(−p)k
(1
2)k(−1)k
(2σ2)kk!M2k(µ)log(n)
n.
Step 2: Control of II.
We follow the lines of proofs of Proposition 1 in Goldfeld et al. (2020) and Theorem 2 in Nietert et al. (2021).
Using a coupling ˆˆµnandRuµ∗ Nσ)via the maximal TV-coupling (see Theorem 6.15 in Villani (2009)]), the
control of the total variation of the Wasserstein distance, we get for any ﬁxed u∈Sd−1
Wp
p/parenleftbig1
nn/summationdisplay
i=1N(u/latticetopXi, σ2), Ruµ∗ Nσ)/parenrightbig
≤2p−1/integraldisplay
R|t|p|hn,u(t)−gu(t)|dt,
19Published in Transactions on Machine Learning Research (11/2024)
where hn,uandguare the densities associated with µnandRuµ∗ Nσ, respectively. Let fσ,ϑthe probability
density function of Nσ,ϑ, i.e, fσ,ϑ(t) =1/radicalbig
2π(σϑ)2e−t2
2(σϑ)2forϑ >0to be speciﬁed later. An application of
Cauchy-Schwarz inequality gives
Eµ⊗n|N⊗nσ/bracketleftBig
Wp
p/parenleftbig1
nn/summationdisplay
i=1N(u/latticetopXi, σ2), Ruµ∗ Nσ)/parenrightbig/bracketrightBig
≤2p−1Eµ⊗n|N⊗nσ/integraldisplay
R|t|p/radicalBig
fσ,ϑ(t)|hn,u(t)−gu(t)|/radicalbig
fσ,ϑ(t)dt
≤2p−1Eµ⊗n|N⊗nσ/parenleftBig/integraldisplay
R|t|2pfσ,ϑ(t)dt/parenrightBig1
2/parenleftBig/integraldisplay
R(hn,u(t)−gu(t))2
fσ,ϑ(t)dt/parenrightBig1
2
≤2p−1/parenleftBig/integraldisplay
R|t|2pfσ,ϑ(t)dt/parenrightBig1
2/parenleftBig/integraldisplay
REµ⊗n|N⊗nσ(hn,u(t)−gu(t))2
fσ,ϑ(t)dt/parenrightBig1
2.
Note that/integraltext
R|t|2pfσ,ϑ(t)dtis the 2p-th moment of |Nσ,ϑ(t)|equals to (see Equation (18) in Winkelbauer
(2014))
/integraldisplay
R|t|2pfσ,ϑ(t)dt=(σϑ)2p2p
√πΓ/parenleftbig2p+ 1
2/parenrightbig
.
Moreover,
hn,u(t) =1
nn/summationdisplay
i=1dN(u/latticetopXi, σ2)(t) =1
nn/summationdisplay
i=1fσ,ϑ(t−u/latticetopXi),
It is clear to see that hn,u(t)is a sum of i.i.d. terms with expectation gu(t), which implies
Eµ⊗n|N⊗nσ/bracketleftbig
(hn,u(t)−gu(t))2/bracketrightbig
=Vµ⊗n/bracketleftBig1
nn/summationdisplay
i=1fσ,ϑ(t−u/latticetopXi)/bracketrightBig
=1
nVµ[fσ,ϑ(t−u/latticetopX]
≤1
nEµ[(fσ,ϑ(t−u/latticetopX)2]
≤(2πσ2)−1
nEµ[e−1
σ2(t−u/latticetopX)2].
Now
Eµ[e−(t−u/latticetopX)2
σ2 ] =/integraldisplay
/bardblx/bardbl≤|t|
2e−1
σ2(t−u/latticetopx)2dµ(x) +/integraldisplay
/bardblx/bardbl>|t|
2e−1
σ2(t−u/latticetopx)2dµ(x).
Remark that when /bardblx/bardbl ≤|t|
2, then (t−u/latticetopX)2≥ |t|2− |u/latticetopx|2≥ |t|2− /bardblx/bardbl2(since /bardblu/bardbl2= 1). We get
(t−u/latticetopX)2≥|t|2
4and hence
/integraldisplay
/bardblx/bardbl≤|t|
2e−1
σ2(t−u/latticetopx)2dµ(x)≤e−t2
4σ2and/integraldisplay
/bardblx/bardbl>|t|
2e−1
σ2(t−u/latticetopx)2dµ(x)≤P/bracketleftbig
/bardblX/bardbl>|t|
2/bracketrightbig
This gives,
/integraldisplay
REµ⊗n|N⊗nσ(hn,u(t)−gu(t))2
fσ,ϑ(t)dt≤(2πσ2)−1(√
2πσϑ)
n/parenleftBig/integraldisplay
Ret2
2(σϑ)2e−t2
4σ2dt+/integraldisplay
Ret2
2(σϑ)2P/bracketleftbig
/bardblX/bardbl>|t|
2/bracketrightbig
dt/parenrightBig
.
20Published in Transactions on Machine Learning Research (11/2024)
Note that the integral/integraltext
Ret2
2(σϑ)2e−t2
4σ2dt=/integraltext
Re−/parenleftbig
1
2−1
ϑ2/parenrightbig
t2
2σ2dtis ﬁnite if and only if1
2−1
ϑ2>0namely ϑ >√
2
and its value is given by
/integraldisplay
Ret2
2(σϑ)2e−t2
4σ2dt=/radicalBigg
2πσ2
1
2−1
ϑ2=/radicalbigg
4πσ2ϑ2
ϑ2−2.
For the second integral
/integraldisplay
Ret2
2(σϑ)2P/bracketleftbig
/bardblX/bardbl>|t|
2/bracketrightbig
dt= 2/integraldisplay∞
0et2
2(σϑ)2P/bracketleftbig
/bardblX/bardbl>t
2/bracketrightbig
dt= 4/integraldisplay∞
0e2ξ2
σ2ϑ2P/bracketleftbig
/bardblX/bardbl> ξ/bracketrightbig
dξ
Then,
II≤n−1/24p−1/braceleftBig
(2πσ2)−1(√
2πσϑ)(σϑ)2p2p
√πΓ/parenleftbig2p+ 1
2/parenrightbig/bracerightBig1
2/parenleftBig/radicalbigg
4πσ2ϑ2
ϑ2−2+ 4/integraldisplay∞
0e2ξ2
σ2ϑ2P/bracketleftbig
/bardblX/bardbl> ξ/bracketrightbig
dξ/parenrightBig1
2.
this gives the desired result using the fact that (a+b)1/p≤a1/p+b1/p,fora, b≥0.
A.4.3 Proof of Proposition 3.11
Using triangle inequality, we have
Wp(ˆˆµn,ˆˆνn)≤Wp(ˆˆµn,Ruµ∗ Nσ) + W p(Ruµ∗ Nσ,Ruν∗ Nσ) + W p(Ruν∗ Nσ,ˆˆνn).
and then
Wp
p(ˆˆµn,ˆˆνn)≤3p−1/braceleftbig
Wp
p(ˆˆµn,Ruµ∗ Nσ) + Wp
p(Ruµ∗ Nσ,Ruν∗ Nσ) + Wp
p(Ruν∗ Nσ,ˆˆνn)/bracerightbig
.
This implies that
Eµ⊗n|N⊗nσEν⊗n|N⊗nσ[ˆGσSWp(ˆµn,ˆνn)]
≤31−1
pGσSWp(µ, ν) + 31−1
pEµ⊗n|N⊗nσ[ˆGσSWp(ˆµn, µ)] + 31−1
pEν⊗n|N⊗nσ[ˆGσSWp(ˆνn, ν)].
By application of Proposition 3.8, it yields This gives that
Eµ⊗n|N⊗nσEν⊗n|N⊗nσ[ˆGσSWp(ˆµn,ˆνn)]≤31−1
pGσSWp(µ, ν) + 3Ξ p,σ,ϑ1
n1/2p+ 31−1
p(Υp,σ,µ + Υ p,σ,ν)(logn)1/p
n1/p
This ends the proof of the ﬁrst statement in Proposition 3.11. For the second one, we also use a triangle
inequality
Wp
p(Ruµ∗ Nσ,Ruν∗ Nσ)≤3p−1/braceleftbig
Wp
p(Ruµ∗ Nσ,ˆˆµn) + Wp
p(ˆˆµn,ˆˆνn) + Wp
p(ˆˆνn),Ruν∗ Nσ/bracerightbig
.
Then we control each term as we did before.
A.5 Proof of Proposition 3.12: projection complexity
Using Holder’s inequality, we have
Eu⊗L
d/bracketleftbig/vextendsingle/vextendsingle\GσSDpp
(µ, ν)−GσSDpp(µ, ν)/vextendsingle/vextendsingle/bracketrightbig
≤/parenleftBig
Eu⊗L
d[/bracketleftbig/vextendsingle/vextendsingle\GσSDpp
(µ, ν)−GσSDp
p(µ, ν)/vextendsingle/vextendsingle2/bracketrightbig/parenrightBig1/2
=/parenleftBig
Vu⊗L
d[/bracketleftbig\GσSDpp
(µ, ν)/bracketrightbig/parenrightBig1/2
=A(p, σ)
L1/2.
21Published in Transactions on Machine Learning Research (11/2024)
A.6 Proof of Corollary 3.13: overall complexity (p= 1)
By application of triangle inequality, one has
|\ˆGσSW(ˆµn,ˆνn)−GσSW(µ, ν)| ≤ |\ˆGσSW(ˆµn,ˆνn)−ˆGσSW(ˆµn,ˆνn)|+|ˆGσSW(ˆµn,ˆνn)−GσSW(µ, ν)|
Using Proposition 3.12, we have
Eu⊗L
d/bracketleftbig
|\ˆGσSW(ˆµn,ˆνn)−ˆGσSW(ˆµn,ˆνn)|/bracketrightbig
≤ˆAσ√
L:={Vu∼ud[W(ˆˆµn,ˆˆνn)]}1/2
√
L.
Using Proposition 3.11 for p= 1we get,
Eµ⊗n|N⊗nσEν⊗n|N⊗nσ[|ˆGσSW(ˆµn,ˆνn)−GσSW(µ, ν)|]≤3Ξ1,σ,ϑ1√n+ (Υ 1,σ,µ+ Υ 1,σ,ν)logn
n.
Therefore, by applying the expectations with respect to the projection and sampling we obtain
Eu⊗L
dEµ⊗n|N⊗nσEν⊗n|N⊗nσ/bracketleftbig
|\ˆGσSW(ˆµn,ˆνn)−GσSW(µ, ν)|/bracketrightbig
≤1√
LEµ⊗n|N⊗nσEν⊗n|N⊗nσ[ˆAσ] + 3Ξ 1,σ,ϑ1√n+ (Υ 1,σ,µ+ Υ 1,σ,ν)logn
n.
By Jensen inequality, we have
Eµ⊗n|N⊗nσEν⊗n|N⊗nσ[ˆAσ]≤/braceleftbig
Eµ⊗n|N⊗nσEν⊗n|N⊗nσ[Vu∼ud[W(ˆˆµn,ˆˆνn)]]/bracerightbig1/2.
A.7 Proof of Proposition 3.14
For all u∈Sd−1we have Ruµ,Ruν∈ P(R). By application of the inequality of noise level satisﬁed by Din
one dimension we get
Dp(Ruµ∗ Nσ2,Ruν∗ Nσ2)≤Dp(Ruµ∗ Nσ1,Ruν∗ Nσ1).
Then, computing the expectation over the projections usince the divergence is non-negative concludes the
proof.
A.8 Proof of Proposition 3.16: relation between GσSWp(µ, ν)under two noise levels
First, using the contractive property of convolution (see Lemma 3 in Nietert et al. (2021)), stating that for any
probability measure α∈ P(R),Wp(µ∗α, ν∗α)≤Wp(µ, ν). Hence Wp
p(µ∗Nσ2, ν∗Nσ2)≤Wp
p(µ∗Nσ1, ν∗Nσ1).
Now using Proposition 3.14 of the oreder relation satisﬁed by GσSWpyields
Gσ2SWp(µ, ν)≤Gσ1SWp(µ, ν).
In the other direction, we have that Nσ2=Nσ1∗ N/radicalbig
σ2
2−σ2
1(similarly for Nσ1). Setting the following random
variables: Xu∼ R uµ, Yu∼ R uν, ZX∼ N σ1, ZY∼ N σ1, Z/prime
X∼ N/radicalbig
σ2
2−σ2
1, Z/prime
Y∼ N/radicalbig
σ2
2−σ2
1. The sliced
Wasserstein distance Wp
p(Ruµ∗ Nσ2,Ruν∗ Nσ2)is given as a minimization over couplings (Xu, ZX, Z/prime
X)and
(Yu, ZY, Z/prime
Y), namely
Wp
p(Ruµ∗ Nσ2,Ruν∗ Nσ2) = inf
Xu,ZX,Z/prime
X
Yu,ZY,Z/prime
YE/bracketleftbig/vextendsingle/vextendsingle/parenleftbig
(Xu+ZX)−(Yu+ZY)/parenrightbig
+ (Z/prime
X−Z/prime
Y)/vextendsingle/vextendsinglep/bracketrightbig
Using the inequality E[|U+V|p]−2p−1E[|W|p]≤2p−1E[|U+V+W|p]for any random variables U, V, W ∈Lp
integrable, we obtain,
2p−1E/bracketleftbig
|(Xu+ZX)−(Yu+ZY) + (Z/prime
X+Z/prime
Y)|p/bracketrightbig
≥E/bracketleftbig
|(Xu+ZX)−(Yu+ZY)|p/bracketrightbig
−2p−1E/bracketleftbig
|(Z/prime
X−Z/prime
Y)|p/bracketrightbig/parenrightbig
.
22Published in Transactions on Machine Learning Research (11/2024)
Hence,
2p−1Wp
p(Ruµ∗ Nσ2,Ruν∗ Nσ2)≥inf/parenleftBig
E/bracketleftbig
|(Xu+ZX)−(Yu+ZY)|p/bracketrightbig
−2p−1E/bracketleftbig
|(Z/prime
X−Z/prime
Y)|p/bracketrightbig/parenrightbig/parenrightBig
≥Wp
p(Ruµ∗ Nσ1,Ruν∗ Nσ1)−2p−1supE/bracketleftbig
|(Z/prime
X−Z/prime
Y)|p/bracketrightbig
≥Wp
p(Ruµ∗ Nσ1,Ruν∗ Nσ1)−22psupE/bracketleftbig
|(Z/prime
X)|p/bracketrightbig
.
Hence,
Gσ1SWp(µ, ν)≤21−1
pGσ2SWp(µ, ν) + 4/parenleftbig
supE/bracketleftbig
|(Z/prime
X)|p/bracketrightbig
)/parenrightbig1/p.
Finally, for any p≥1thep-th moment of |Nσ|satisﬁes E[|Nσ|p] =2pΓ((p+1)/2)
Γ(1/2)σ2p≤2p/2σ2p,then
Gσ1SWp(µ, ν)≤21−1
pGσ2SWp(µ, ν) + 25
2(σ2
2−σ2
1),
and concludes the proof.
A.9 Proof of Proposition 3.17: continuity of the smoothed Gaussian sliced Wasserstein w.r.t. σ
From Lemma 1 in (Nietert et al., 2021), we know that the Gaussian-smoothed Wasserstein is continuous with
respect to σ, for any distribution RuνandRuµ. In addition, for any u, we have Wp(Ruν∗ Nσ,Ruµ∗ Nσ)≤
Wp(Ruν,Ruµ). Then by applying Lebesgue’s dominated convergence theorem (Bowers & Kalton, 2014) to
the above inequality with Wp(Ruν,Ruµ)as a dominating function, that is ud-almost everywhere integrable
because both measures are in Pp(Rd), we then conclude that the Gaussian-smoothed SWD is continuous
w.r.t. σ.
A.10 Proof of Proposition 3.18: continuity of the smoothed sliced squared-MMD w.r.t. σ
Let us ﬁrst recall the deﬁnition of the MMD divergence. Let k:R×R→Rbe a measurable bounded
kernel on Rand consider the reproducing kernel Hilbert space (RKHS) Hkassociated with kand equipped
with inner product <·,·>Hkand norm /bardbl · /bardblHk. Let PHk(R)be the set of probability measures ηsuch
that/integraltext
R/radicalbig
k(t, t)dη(x)<∞.The kernel mean embedding is deﬁned as Φk(η) =/integraltext
Rk(·, t)dη(t).The squared-
maximum mean discrepancy between η, ζ∈ P(R)denoted as MMD :PHk(R)× PHk(R)→R+is expressed
as the distance between two such kernel mean embeddings. It is deﬁned as Gretton et al. (2012)
MMD2(η, ζ) =/bardblΦk(η)−Φk(ζ)/bardbl2
Hk=ET,T/prime∼η[k(T, T/prime)]−2ET∼η,R∼ζ[k(T, R)] +ER,R/prime∼ζ[k(R, R/prime)]
where TandT/primeare independent random variables drawn according to η,RandR/primeare independent random
variables drawn according to ζ, and Tis independent of R. We deﬁne the Gaussian Smoothed Sliced
squared- MMD as follows:
GσMMD2(µ, ν) =/integraldisplay
Sd−1/bardblΦk(Ruµ∗ Nσ)−Φk(Ruν∗ Nσ)/bardbl2
Hkud(u)du
=/integraldisplay
Sd−1/parenleftbig
ET,T/prime∼Ruµ∗Nσ[k(T, T/prime)]−2ET∼Ruµ∗Nσ,R∼Ruν∗Nσ[k(T, R)]
+ER,R/prime∼Ruν∗Nσ[k(R, R/prime)]/parenrightbig
ud(u)du.
From the deﬁnition of the smoothed sliced squared-MMD, we have
ET,T/prime∼Ruµ∗Nσ[k(T, T/prime)] =/integraldisplay/integraldisplay
R×Rk(t, t/prime)dRuµ∗ Nσ(t)dRuµ∗ Nσ(t/prime)
=/integraldisplay/integraldisplay
R×R/parenleftBig/integraldisplay
Rk(t+z, t/prime)dRuµ(z)Nσ(t)/parenrightBig
dRuµ∗ Nσ(t/prime)
=/integraldisplay/integraldisplay
R×R/parenleftBig/integraldisplay
Rdk(t+u/latticetopx, t/prime)dµ(x)Nσ(t)/parenrightBig
dRuµ∗ Nσ(t/prime)
=/integraldisplay/integraldisplay
R×R/integraldisplay/integraldisplay
Rd×Rdk(t+u/latticetopx, t/prime+u/latticetopx/prime)dµ(x)dµ(x/prime)dNσ(t)dNσ(t/prime).
23Published in Transactions on Machine Learning Research (11/2024)
Similarly,
ER,R/prime∼Ruν∗Nσ[k(R, R/prime)] =/integraldisplay/integraldisplay
R×R/integraldisplay/integraldisplay
Rd×Rdk(r+u/latticetopy, r/prime+u/latticetopy/prime)dν(y)dν(y/prime)dNσ(r)dNσ(r/prime)
and
ET∼Ruµ∗Nσ,R∼Ruν∗Nσ[k(T, R)] =/integraldisplay/integraldisplay
R×R/integraldisplay/integraldisplay
Rd×Rdk(t+u/latticetopx, r+u/latticetopy)dµ(x)dν(y)dNσ(t)dNσ(r).
Together the assumption of boundness of the kernel function kand the continuity of integrals, the three
latter terms are continuous functions w.r.t. σ∈(0,∞).Again by the boundness of the kernel function k,
there exists a positive ﬁnite constant Cksuch that
/vextendsingle/vextendsingleET,T/prime∼Ruµ∗Nσ[k(T, T/prime)]−2ET∼Ruµ∗Nσ,R∼Ruν∗Nσ[k(T, R)] +ER,R/prime∼Ruν∗Nσ[k(R, R/prime)]/vextendsingle/vextendsingle≤4Ck.
We conclude the continuity of σ/mapsto→GσMMD2(µ, ν)by an application of the continuity of integrals.
B Additional experiments
B.1 Sample complexity on CIFAR dataset
We have also evaluated the sample complexity for the CIFAR dataset by sampling sets of increasing size.
Results reported in Figure 6 conﬁrms the ﬁndings obtained from the toy dataset.
Figure 6: Measuring the divergence between two sets of samples drawn iid from the CIFAR10 dataset. We
compare three sliced divergences and their Gaussian smoothed versions with a σ= 3.
B.2 Identity of indiscernibles
The second experiment aims at checking whether our divergences converge towards a small value when the
distributions to be compared are the same. For this, we consider samples from distributions µandνchosen
as normal distributions with respectively mean 2×1dands1dwith varying s(noted as the displacement).
Results are depicted in Figure 7. We can see that all methods are able to attain their minimum when s= 2.
Interestingly, the gap between the Gaussian smoothed and non-smoothed divergences for Wasserstein and
Sinkhorn is almost indiscernible as the distance between distribution increases.
24Published in Transactions on Machine Learning Research (11/2024)
Figure 7: Measuring the divergence between two sets of samples in R50, one with mean 21dand the other
with mean s1dwith increasing s. We compare three sliced divergences and their Gaussian smoothed version
with a σ= 3.
25