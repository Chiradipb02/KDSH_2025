Published in Transactions on Machine Learning Research (02/2024)
An optimal control perspective
on diffusion-based generative modeling
Julius Berner⋆†jberner@caltech.edu
Caltech
Lorenz Richter⋆richter@zib.de
Zuse Institute Berlin
dida Datenschmiede GmbH
Karen Ullrich karenu@meta.com
Meta AI
Reviewed on OpenReview: https: // openreview. net/ forum? id= oYIjw37pTP
Abstract
We establish a connection between stochastic optimal control and generative models based
on stochastic differential equations (SDEs), such as recently developed diffusion probabilistic
models. In particular, we derive a Hamilton–Jacobi–Bellman equation that governs the
evolution of the log-densities of the underlying SDE marginals. This perspective allows to
transfer methods from optimal control theory to generative modeling. First, we show that
the evidence lower bound is a direct consequence of the well-known verification theorem
from control theory. Further, we can formulate diffusion-based generative modeling as a
minimization of the Kullback–Leibler divergence between suitable measures in path space.
Finally, we develop a novel diffusion-based method for sampling from unnormalized densities
– a problem frequently occurring in statistics and computational sciences. We demonstrate
that our time-reversed diffusion sampler (DIS) can outperform other diffusion-based sampling
approaches on multiple numerical examples.
1 Introduction
Diffusion (probabilistic) models (DPMs) have established themselves as state-of-the-art in generative modeling
and likelihood estimation of high-dimensional image data (Ho et al., 2020; Kingma et al., 2021; Nichol &
Dhariwal, 2021). In order to better understand their inner mechanisms, multiple connections to adjacent
fields have already been proposed: For instance, in a discrete-time setting, one can interpret DPMs as types
ofvariational autoencoders (VAEs) for which the evidence lower bound (ELBO) corresponds to a multi-scale
denoising score matching objective (Ho et al., 2020). In continuous time, DPMs can be interpreted in terms
of stochastic differential equations (SDEs) (Song et al., 2021; Huang et al., 2021; Vahdat et al., 2021) or as
infinitely deep VAEs (Kingma et al., 2021). Both interpretations allow for the derivation of a continuous-time
ELBO, and encapsulate various methods, such as denoising score matching with Langevin dynamics (SMLD),
denoising diffusion probabilistic models (DDPM), and continuous-time normalizing flows as special cases (Song
et al., 2021; Huang et al., 2021).
In this work, we suggest another perspective. We show that the SDE framework naturally connects
diffusion models to partial differential equations (PDEs) typically appearing in stochastic optimal control and
reinforcement learning . The underlying insight is that the Hopf–Cole transformation serves as a means of
⋆Equal contribution (the author order was determined by numpy.random.rand(1) ).
†Work done during an internship at Meta AI.
1Published in Transactions on Machine Learning Research (02/2024)
Diffusion
models
Generative SDE  with
density 
 solves Fokker-Planck
equation
 solves (generalized)
Kolmogorov equation
 solves HJB
equation
Optimal
control
Path space
measures
ELBO
Sampling from
(unnormalized)
densities
Figure 1: The connection between diffusion models
and optimal control is outlined in Section 2. Three
possibleimplicationsofthisconnectionarethederiva-
tion of the ELBO via the verification theorem (Sec-
tion 2.2), a path space measure interpretation of dif-
fusion models (Section 2.3), and a novel approach for
sampling from (unnormalized) densities (Section 3).
densityY0 XT
YT
3X2T
3
Y2T
3XT
3
YT X0
drift of Y drift of XXY
Figure 2: For a stochastic process Yand its time-
reversed process X, we show the density pY(·,t) =
pX(·,T−t)(top), the drift f(·,t)ofY(middle), and
the driftµ(·,t)ofX(bottom) for t∈/bracketleftbig
0,T
3,2T
3,T/bracketrightbig
,
see (2) and (3).
showing that the time-reversed log-density of the diffusion process satisfies a Hamilton–Jacobi–Bellman (HJB)
equation (Section 2.1). This relation allows us to connect diffusion models to a control problem that minimizes
specific control costs with respect to a given controlled dynamics. We show that this readily yields the ELBO
of the generative model (Section 2.2), which can subsequently be interpreted in terms of Kullback-Leibler
(KL) divergences between measures on the path space of continuous-time stochastic processes (Section 2.3).
While one of our contributions lies in the formal connection between stochastic optimal control and diffusion
models, as described in Section 2.4, we moreover demonstrate the practical relevance of our analysis by
transferring methods from control theory to generative modeling. More specifically, in Section 3, we design a
novel algorithm for sampling from (unnormalized) densities – a problem which frequently occurs in Bayesian
statistics, computational physics, chemistry, and biology (Liu & Liu, 2001; Stoltz et al., 2010). As opposed
to related approaches (Dai Pra, 1991; Richter, 2021; Zhang & Chen, 2022a), our method allows for more
flexibility in choosing the initial distribution and reference SDE, offering the possibility to incorporate specific
prior knowledge. Finally, in Section 4, we show that our sampling strategy can significantly outperform
related approaches across a number of relevant numerical examples.
In summary, our work establishes new connections between generative modeling, sampling, and optimal
control, which brings the following theoretical insights and practical algorithms (see Figure 1 for an overview):
•PDE and optimal control perspectives: We identify the HJB equation as the governing PDE of
the score function, which rigorously establishes the connection of generative modeling to optimal
control (Section 2.1). The HJB equation can further be used for theoretic analyses as well as for
practical algorithms for the numerical approximation of the score, e.g., based on neural PDE solvers.
•ELBO: Using the HJB equation, we show that the objectives in diffusion-based generative modeling
can be solely derived from the principles of control theory (Section 2.2).
•Path space perspective: This perspective on diffusion models offers an intuitive representation
of the objective (and the variational gap) in terms of KL divergences between measures on path
space (Section 2.3). Crucially, it also allows to consider alternative divergences, such as, e.g., the
log-variance divergence, leading to improved loss functions and algorithms.
•Sampling: Our control perspective leads to a novel, diffusion-based method to sample from
unnormalized densities (Section 3). This method can already outperform previous diffusion-based
state-of-the-art samplers. Moreover, the connection to diffusion models allows to transfer noise
schedules, integrators, and further popular techniques from generative modeling to sampling.
2Published in Transactions on Machine Learning Research (02/2024)
1.1 Related work
Diffusion models: Starting from DPMs (Sohl-Dickstein et al., 2015), a number of works have contributed
to the success of diffusion-based generative modeling, see, e.g., Ho et al. (2020); Kingma et al. (2021); Nichol
& Dhariwal (2021); Vahdat et al. (2021); Song & Ermon (2020). We are building upon the SDE-based
formulation developed by Song et al. (2021) which connects diffusion models to score matching (Hyvärinen
& Dayan, 2005). The underlying idea of time-reversing a stochastic process dates back to work by Nelson
(1967); Anderson (1982); Haussmann & Pardoux (1986); Föllmer (1988). Notably, Pavon (1989) connects the
log-density of such a reverse-time SDE to an HJB equation. In this setting, we extend the results of Huang
et al. (2021) on the (continuous-time) ELBO of diffusion models and provide further insights from the
perspective of optimal control and path space measures.
For further previous work on optimal control in the context of generative modeling, we refer the reader
to Tzen & Raginsky (2019); De Bortoli et al. (2021); Pavon (2022); Holdijk et al. (2022). Connections between
optimal control and time-reversals of stochastic processes have also been analyzed by Maoutsa & Opper
(2022); Maoutsa (2023). Finally, we refer to Richter & Berner (2024) for further generalizations based on
the path space perspective and to Zhang & Katsoulakis (2023) for recent work on interpreting generative
modeling via mean-field games.
Sampling from (unnormalized) densities: Monte Carlo (MC) techniques are arguably the most common
methods to sample from unnormalized densities and compute normalizing constants. Specifically, variations
ofAnnealed Importance Sampling (AIS) (Neal, 2001) and Sequential Monte Carlo (Del Moral et al., 2006;
Doucet et al., 2009) (SMC) are often referred to as the “gold standard” in the literature. They apply a
combination of Markov chain Monte Carlo (MCMC) methods and importance sampling to a sequence of
distributions interpolating between a tractable initial distribution and the target density. Even though MCMC
methods are guaranteed to converge to the target distribution under mild assumptions, the convergence speed
might be too slow in many practical settings (Robert et al., 1999). Variational methods such as mean-field
approximations (Wainwright et al., 2008) and normalizing flows provide an alternative1(Papamakarios et al.,
2021). By fitting a parametric family of tractable distributions to the target density, the problem of density
estimation is cast into an optimization problem. In Salimans et al. (2015) it is noted that stochastic Markov
chains can be interpreted as the variational approximation in an extended state space, thus bridging the two
techniques and allowing to apply them jointly.
In this context, diffusion models have been employed to approximate the extended target distribution
needed in the importance sampling step of AIS methods (Doucet et al., 2022). Moreover, diffusion models
have been trained on densities by simulating samples using importance sampling with the likelihood of the
partially-trained model (computed via the probability flow ODE) as proposal distribution (Jing et al., 2022).
In this work, we propose a novel variational method based on diffusion models, which we call time-reversed
diffusion sampler (DIS). It is based on minimizing the reverse KL divergence between a controlled SDE
and the reverse-time SDE. This is intimately connected to methods developed in the field of stochastic
optimal control based on minimizing the reverse KL divergence to a reference process starting at a Dirac
delta distribution (Dai Pra, 1991; Richter, 2021; Zhang & Chen, 2022a; Tzen & Raginsky, 2019; Vargas et al.,
2023b). Finally, we mention that concurrently to our work, Denoising Diffusion Samplers (Vargas et al.,
2023a) have been proposed, which can be viewed as a special case of our approach, see Section A.10.
Schrödinger bridges: Schrödinger bridge (SB) problems (Schrödinger, 1931) aim to find the minimizer of
the KL divergence to a reference process, typically a Brownian motion, subject to the constraint of satisfying
given marginal distributions at the initial and terminal time. Such problems include diffusion-based methods
as a special case where the reference process is given by the uncontrolled inference process (De Bortoli et al.,
2021; Chen et al., 2021; Koshizuka & Sato, 2022). Since in this case only the reverse-time process is controlled,
its initial distribution must correspond to the terminal distribution of the inference process. In practice, this
constraint is usually only fulfilled in an approximate sense, resulting in a prior loss, see Section 2.3. Note
that the previously mentioned sampling methods in the field of optimal control rely on a Dirac delta as the
1Note that diffusion models give rise to a continuous-time normalizing flow, the so-called probability flow ordinary differential
equation (ODE) , having the same marginals as the reverse-time diffusion process, see Song et al. (2021) and Section A.3.
3Published in Transactions on Machine Learning Research (02/2024)
initial distribution and solve the so-called Schrödinger half-bridge problem , constraining the KL minimization
problem only to the terminal distribution (Dai Pra, 1991).
1.2 Notation
We denote the density of a random variable ZbypZ. For an Rd-valued stochastic process Z= (Zt)t∈[0,T]
we define the function pZbypZ(·,t):=pZtfor everyt∈[0,T]. We further denote by PZthe law of Z
on the space of continuous functions C([0,T],Rd). For a time-dependent function f, we denote by ⃗fthe
time-reversal given by ⃗f(t):=f(T−t). Finally, we define the divergence of matrix-valued functions row-wise.
More details on our notation can be found in Section A.1.
2 SDE-based generative modeling as an optimal control problem
Diffusion models can naturally be interpreted through the lens of continuous-time stochastic processes (Song
et al., 2021). To this end, let us formalize our setting in the general context of SDE-based generative models.
We define our model as the stochastic process X= (Xs)s∈[0,T]characterized by the SDE
dXs=⃗µ(Xs,s) ds+⃗σ(s) dBs (1)
with suitable2drift and diffusion coefficients µ:Rd×[0,T]→Rdandσ:[0,T]→Rd×d. Learning the model
in (1) now corresponds to solving the following problem.
Problem 2.1 (SDE-based generative modeling) .Learn an initial condition X0as well as coefficient functions
µandσsuch that the distribution of XTapproximates a given data distribution D.
While, in general, the initial condition X0as well as both the coefficient functions µandσcan be learned,
typical applications often focus on learning only the drift µ(Ho et al., 2020; Kong et al., 2021; Corso et al.,
2022). The following remark justifies that this is sufficient to represent an arbitrary distribution D.
Remark 2.2 (Reverse-time SDE) .Naively, one can achieve XT∼Dby setting
X0∼YTandµ:=σσ⊤∇logpY−f, (2)
wheref:Rd×[0,T]→RdandYis a solution to the SDE
dYs=f(Ys,s) ds+σ(s) dBs, Y 0∼D. (3)
This well-known result dates back to Nelson (1967); Anderson (1982); Haussmann & Pardoux (1986); Föllmer
(1988). More generally, it states that Xcan be interpreted as the time-reversal of Y, which we will denote by
⃗Y, in the sense that pX=⃗pYalmost everywhere, see Figure 2 and Section A.3. Even though the reverse-time
SDE provides a valid solution to Problem 2.1, apparent practical challenges are to sample from the terminal
distribution of the generative SDE, X0∼YT, and to compute the so-called score∇logpY. In most scenarios,
we either only have access to samples from the true data distribution Dor access to its density. If samples
fromDare available, diffusion models provide a working solution to solving Problem 2.1 (more details
in Section 2.4). When, instead, having access to the density of D, general time-reversed diffusions have, to
the best of our knowledge, not yet been considered. In Section 3, we thus propose a novel strategy for this
scenario.
As already apparent from the optimal drift µin the previous remark, and noting again that pY=⃗pXin this
case, the reverse-time log-density log⃗pXof the process Xwill play a prominent role in deriving a suitable
objective for solving Problem 2.1. Hence, in the next section, we derive the HJB equation governing the
evolution of log⃗pX, which then provides the bridge to the fields of optimal control and reinforcement learning.
2Motivated by Remark 2.2, we start with time-reversed drift and diffusion coefficients ⃗µand⃗σ. Further, we assume certain
regularity on the coefficient functions of all appearing SDEs, see Section A.1
4Published in Transactions on Machine Learning Research (02/2024)
2.1 PDE perspective: HJB equation for log-density
We start with the well-known Fokker-Planck equation , which describes the evolution of the density of the
solutionXto the SDE in (1) via the PDE
∂tpX= div/parenleftig
div/parenleftig
⃗DpX/parenrightig
−⃗µpX/parenrightig
, (4)
where we set D:=1
2σσ⊤for notational convenience. This implies that the time-reversed density ⃗pXsatisfies
a (generalized) Kolmogorov backward equation given by
∂t⃗pX= div (−div (D⃗pX) +µ⃗pX) =−Tr/parenleftbig
D∇2⃗pX/parenrightbig
+µ·∇⃗pX+ div(µ)⃗pX. (5)
The second equality follows from the identities for divergences in Section A.2 and the fact that σdoes not
depend on the spatial variable3x. Now we use the Hopf–Cole transformation to convert the linear PDE
in(5)to an HJB equation which is prominent in control theory, see Pavon (1989); Fleming & Rishel (2012),
and Section A.5.
Lemma 2.3 (HJB equation for log-density) .Let us define V:=−log⃗pX. ThenVis a solution to the HJB
equation
∂tV=−Tr/parenleftbig
D∇2V/parenrightbig
+µ·∇V−div(µ) +1
2/vextenddouble/vextenddoubleσ⊤∇V/vextenddouble/vextenddouble2(6)
with terminal condition V(·,T) =−logpX0.
For approaches to directly solve Kolmogorov backward or HJB equations via deep learning in rather high
dimensions, we refer to, e.g., Richter (2021); Berner et al. (2020); Zhou et al. (2021); Nüsken & Richter
(2023; 2021); Richter & Berner (2022), see also Sections A.4 and A.11. In the following, we leverage the HJB
equation and tools from stochastic control theory to derive a suitable objective for Problem 2.1.
2.2 Optimal control perspective: ELBO derivation
We will derive the ELBO for our generative model (1)using the following fundamental result from control
theory, which shows that the solution to an HJB equation, such as the one stated in Lemma 2.3, is related to
an optimal control problem, see Dai Pra (1991); Pavon (1989); Nüsken & Richter (2021); Fleming & Soner
(2006); Pham (2009) and Section A.7. For a general introduction to stochastic optimal control theory we
refer to Section A.6.
Theorem 2.4 (Verification theorem) .LetVbe a solution to the HJB equation in Lemma 2.3. Further, let
U⊂C1(Rd×[0,T],Rd)be a suitable set of admissible controls and for every control u∈UletYube the
solution to the controlled SDE4
dYu
s= (σu−µ) (Yu
s,s) ds+σ(s) dBs. (7)
Then it holds almost surely that
V(Yu
0,0) = min
u∈UE/bracketleftbig
Ru
µ(Yu)−logpX0(Yu
T)/vextendsingle/vextendsingleYu
0/bracketrightbig
, (8)
where−logpX0(Yu
T)constitutes the terminal costs, and the running costs are defined as
Ru
µ(Yu):=/integraldisplayT
0/parenleftbigg
div(µ) +1
2∥u∥2/parenrightbigg
(Yu
s,s) ds. (9)
Moreover, the unique minimum is attained by u∗:=−σ⊤∇V.
3In Section A.3, we provide a proof for the reverse-time SDE in Remark 2.2 for general σdepending on xandt. However, for
simplicity, we restrict ourselves to σonly depending on the time variable tin the following.
4As usually done, we assume that the initial condition Yu
0of a solution Yuto a controlled SDE does not depend on the
controlu.
5Published in Transactions on Machine Learning Research (02/2024)
Plugging in the definition of Vfrom Lemma 2.3, this readily yields the following ELBO of our generative
model in (1). The variational gap can be found in Proposition 2.6 and Remark A.6.
Corollary 2.5 (Evidence lower bound) .For everyu∈Uit holds almost surely that
logpXT(Yu
0)≥ E/bracketleftbig
logpX0(Yu
T)−Ru
µ(Yu)/vextendsingle/vextendsingleYu
0/bracketrightbig
, (10)
where equality is obtained for u∗:=σ⊤∇log⃗pX.
Comparing (8)and(10), we see that the ELBO equals the negative control costs. With the initial condition
Yu
0∼D, it represents a lower bound on the negative log-likelihood of our generative model. In practice,
one can now parametrize uwith, for instance, a neural network and rely on gradient-based optimization
to maximize the ELBO using samples from D. The optimality condition in Corollary 2.5 guarantees that
⃗pX=pYu∗almost everywhere if we ensure that X0∼Yu∗
T, see Section A.3. In particular, this implies that
XT∼D, i.e., our generative model solves Problem 2.1. However, we still face the problem of sampling
X0∼Yu∗
Tsince the distribution of Yu∗
Tdepends5on the initial distribution D. In Sections 2.4 and 3 we will
demonstrate ways to circumvent this problem.
2.3 Path space perspective: KL divergence in continuous time
In this section, we show that the variational gap corresponding to Corollary 2.5 can be interpreted as a KL
divergence between measures on the space of continuous trajectories, also known as path space (Üstünel &
Zakai, 2013). Later, we can use this result to develop our sampling method. To this end, let us define the
path space measure PYuas the distribution of the trajectories associated with the controlled process Yuas
defined in (7), see also Section A.1. Consequently, we denote by PY0the path space measure associated with
the uncontrolled process Y0with the choice u≡0.
We can now state a path space measure perspective on Problem 2.1 by identifying a formula for the target
measure PYu∗, which corresponds to the process in (7)with the optimal control u=u∗=σ⊤∇log⃗pX. We
further show that, with the correct initial condition Yu
0∼XT, this target measure corresponds to the measure
of the time-reversed process, i.e., PYu∗= P⃗X, see Remark 2.2 and Section A.3. For the proof and further
details, see Proposition A.9 in the appendix.
Proposition 2.6 (Optimal path space measure) .The optimal path space measure can be defined via the
work functionalW:C([0,T],Rd)→Rand the Radon-Nikodym derivative
d PYu∗
d PY0= exp (−W)withW(Y0):=R0
µ(Y0)−logpX0/parenleftbig
Y0
T/parenrightbig
pXT(Y0
0), (11)
whereR0
µ(Y0)is as in(9)withu= 0. Moreover, for any u∈U, the expected variational gap
G(u):= E[logpXT(Yu
0)]− E/bracketleftbig
logpX0(Yu
T)−Ru
µ(Yu)/bracketrightbig
(12)
of the ELBO in Corollary 2.5 satisfies that
G(u) =DKL( PYu| PYu∗) =DKL( PYu| P⃗X)−DKL( PYu
0| PXT). (13)
Note that the optimal change of measure (11)can be seen as a version of Doob’sh-transform , see Dai Pra
& Pavon (1990). Furthermore, it can be interpreted as Bayes’ rule for conditional probabilities, with the
target PYu∗denoting the posterior, PY0the prior measure, and exp (−W)being the likelihood. Formula (13)
emphasizes again that we can solve Problem 2.1 by approximating the optimal control u∗and having matching
initial conditions, see also Remark 2.2. The next section provides an objective for this goal that can be used
in practice.
5In case one has access to samples from the data distribution D, one could use these as initial data Yu∗
0in order to simulate
X0∼Yu∗
T. In doing so, however, one cannot expect to recover the entire distribution D, but only the empirical distribution of
the samples.
6Published in Transactions on Machine Learning Research (02/2024)
2.4 Connection to denoising score matching objective
This section outlines that, under a reparametrization of the generative model in (1), the ELBO in Corollary 2.5
corresponds to the objective typically used for the training of continuous-time diffusion models. We note
that the ELBO in Corollary 2.5 in fact equals the one derived in Huang et al. (2021, Theorem 3). Following
the arguments therein and motivated by Remark 2.2, we can now use the reparametrization µ:=σu−fto
arrive at an uncontrolled inference SDE and a controlled generative SDE
dYs=f(Ys,s) ds+σ(s) dBs, Y 0∼D,and dXu
s=/parenleftbig
⃗σ⃗u−⃗f/parenrightbig
(Xu
s,s) ds+⃗σ(s) dBs.(14)
In practice, the coefficients fandσare usually6constructed such that Yis an Ornstein-Uhlenbeck (OU)
process with YTbeing approximately distributed according to a standard normal distribution, see also (139)
in Section A.13. This is why the process Yis said to diffusethe data. Setting Xu
0∼N(0,I)thus satisfies
thatpXu
0≈pYTand allows to easily sample Xu
0.
The corresponding ELBO in Corollary 2.5 now takes the form
logpXu
T(Y0)≥ E/bracketleftbig
logpXu
0(YT)−Ru
σu−f(Y)/vextendsingle/vextendsingleY0/bracketrightbig
, (15)
where, in analogy to (13), the expected variational gap is given by the forward KL divergence G(u) =
DKL( PY| P⃗Xu)−DKL( PY0| PXu
T), see Proposition A.10. We note that the process Yin the ELBO does not
depend on the control uanymore. Under suitable assumptions, this allows us to rewrite the expected negative
ELBO (up to a constant not depending on u) as a denoising score matching objective (Vincent, 2011), i.e.,
LDSM(u):=T
2E/bracketleftig/vextenddouble/vextenddoubleu(Yτ,τ)−σ⊤(τ)∇logpYτ|Y0(Yτ|Y0)/vextenddouble/vextenddouble2/bracketrightig
, (16)
whereτ∼U([0,T])andpYτ|Y0denotes the conditional density of YτgivenY0, see Section A.9. We emphasize
that the conditional density can be explicitly computed for the OU process, see also (137)in Section A.13.
Due to its simplicity, variants of the objective (16)are typically used in implementations. Note, however, that
the setting in this section requires that one has access to samples of Din order to simulate the process Y. In
the next section, we consider a different scenario, where instead we only have access to the (unnormalized)
density ofD.
3 Sampling from unnormalized densities
In many practical settings, for instance, in Bayesian statistics or computational physics, the data distribution
Dadmits the density ρ/Z, whereρis known, but computing the normalizing constant Z:=/integraltext
Rdρ(x) dxis
intractable and samples from Dare not easily available. In this section, we propose a novel method based on
diffusion models, called time-reversed diffusion sampler (DIS), which allows to sample from D, see Figure 3.
To this end, we interchange the roles of XandYuin our derivation in Section 2, i.e., consider
dYs=f(Ys,s) ds+σ(s) dBs, Y 0∼D,and dXu
s=/parenleftbig
⃗σ⃗u−⃗f/parenrightbig
(Xu
s,s) ds+⃗σ(s) dBs,(17)
where we also renamed µtofto stay consistent with (14). Analogously to Theorem 2.4, the following
corollary specifies the control objective, see Corollary A.12 in the appendix for details and the proof.
Corollary 3.1 (Reverse KL divergence) .LetXuandYbe defined by (17). Then it holds
DKL( PXu| P⃗Y) =DKL( PXu| PXu∗) +DKL( PXu
0| PYT) (18a)
= E/bracketleftbigg
R⃗u
⃗f(Xu) + logpYT(Xu
0)
ρ(Xu
T)/bracketrightbigg
+ logZ+DKL( PXu
0| PYT) (18b)
=LDIS(u) + logZ, (18c)
6For coefficients typically used in practice (leading, for instance, to continuous-time analogs of SMLD and DDPM) we refer
to Song et al. (2021) and Section A.13.
7Published in Transactions on Machine Learning Research (02/2024)
0.0 0.2 0.4
initial densityx2
0.0 0.2 0.4 0.6 0.8 1.0
t0.0 0.5 1.0
terminal density2
 0 2
x1
Figure 3: Illustration of our DIS algorithm for the double well example in Section 4 with d= 20,w= 5,
δ= 3. The process Xustarts from a Gaussian (approximately distributed as YT) and the control uis trained
such that the distribution at terminal time Xu
Tapproximates the target density ρ/Z. The plot displays some
trajectories as well as histograms at initial and terminal times. In the right panel, we show a KDE density
estimation of a 2dmarginal of the corresponding double well.
where
LDIS(u):= E/bracketleftbigg
R⃗u
⃗f(Xu) + logpXu
0(Xu
0)
ρ(Xu
T)/bracketrightbigg
. (19)
In particular, this implies that
DKL( PXu
0| PYT)−logZ= min
u∈ULDIS(u) (20)
and, assuming that Xu
0∼YT, the minimizing control u∗:=σ⊤∇logpYguarantees that Xu∗
T∼D.
As in the previous chapter, Xu
0∼YTcan be approximately achieved by choosing Xu
0∼N(0,I)andfandσ
such thatpYT≈N(0,I), which incurs an irreducible prior loss given by DKL( PXu
0| PYT). In practice, one can
now minimize the control objective (19)using gradient-based optimization techniques. Alternatively, one can
solve the corresponding HJB equation to obtain an approximation to u∗, see Section A.11. Note that, in
contrast to (15)or(16), we do not need access to samples Y0∼DasYdoes not need to be simulated for the
objective in (19). However, since the controlled process Xunow appears in the objective, we need to simulate
the whole trajectory of Xuand cannot resort to a Monte Carlo approximation, such as in the denoising score
matching objective in (16). In Section A.14, we show that our path space perspective allows us to introduce
divergences different from the reverse KL-divergence in (18a), which allows for off-policy training and can
result in improved numerical performance.
3.1 Comparison to Schrödinger half-bridges
The idea to rely on controlled diffusions in order to sample from prescribed target densities is closely related
to Schrödinger bridges (Dai Pra, 1991). This problem has been particularly well studied in the case where the
initial density follows a Dirac distribution, i.e., the stochastic process starts at a pre-specified point—often
referred to as Schrödinger half-bridge. Corresponding numerical algorithms based on deep learning, referred
to asPath Integral Sampler (PIS) in Zhang & Chen (2022a), have been independently presented in Richter
(2021); Zhang & Chen (2022a); Vargas et al. (2023b). Combining ideas from Dai Pra (1991) and Fleming &
Soner (2006), it can be shown that a corresponding control problem can be formulated as
−logZ= min
u∈ULPIS(u),withLPIS(u):= E/bracketleftigg
R⃗u
0(Xu) + logpX0
T(Xu
T)
ρ(Xu
T)/bracketrightigg
, (21)
where the controlled diffusion Xuis defined as in (17)with a fixed initial value Xu
0=x0∈Rd. In the above,
pX0
Tdenotes the density of the uncontrolled process X0
Tat timeT. Equivalently, one can show that PXu∗
8Published in Transactions on Machine Learning Research (02/2024)
satisfies the optimal change of measure, given by
d PXu∗
d PX0(X) =ρ
ZpX0
T(XT), (22)
for all suitable stochastic processes Xon the path space. Using the Girsanov theorem (Theorem A.7), we see
that
DKL( PXu| PXu∗) =LPIS(u) + logZ, (23)
see also Zhang & Chen (2022a); Nüsken & Richter (2021). Similar to Corollary 3.1, one can thus show
that the optimally controlled process satisfies Xu∗
T∼D, see also Tzen & Raginsky (2019) and Pavon (2022).
Comparing the two attempts, i.e., the objectives (19) and (21), we can identify multiple differences:
•Different initial distributions, running costs, and terminal costs are considered. In the Schrödinger
half-bridge, the terminal costs consist of ρandpX0
T, whereas for the diffusion model they only consist of ρ.
•For the Schrödinger half-bridge, fneeds to be chosen such that pX0
Tis known analytically (up to a
constant). For the time-reversed diffusion sampler, fneeds to be chosen such that pYT≈pXu
0in order to
have a small prior loss DKL( PXu
0| PYT).
•In the Schrödinger half-bridge, Xu
0starts from an arbitrary, but fixed, point, whereas for the diffusion-based
generative modeling attempt Xu
0must be (approximately) distributed as YT.
In Appendix A.10, we show that the optimal control u∗of our objective in (19)can be numerically more
stable than the one in (21). In the next section, we demonstrates that this can also lead to better sample
quality and more accurate estimates of normalizing constants.
4 Numerical examples
The numerical experiments displayed in Figures 4 and 5 show that our time-reversed diffusion sampler (DIS)
succeeds in sampling from high-dimensional multimodal distributions. We compare our method against the
Path Integral Sampler (PIS) introduced in Zhang & Chen (2022a), which uses the objective from Section 3.1.
As shown in Zhang & Chen (2022a), the latter method can already outperform various state-of-the-art
sampling methods. This includes gradient-guided MCMC methods without the annealing trick, such as
Hamiltonian Monte Carlo (HMC) (MacKay, 2003) and No-U-Turn Sampler (NUTS) (Hoffman & Gelman,
2014), SMC with annealing trick such as Annealed Flow Transport Monte Carlo (AFT) (Arbel et al., 2021),
and variational normalizing flows (VINF) (Rezende & Mohamed, 2015).
Letussummarizeoursettinginthefollowingandnotethatfurtherdetails, aswellastheexacthyperparameters,
can be found in Section A.13 and Table 2. Our PyTorch implementation7is based on Zhang & Chen (2022a)
with the following main differences: We train with larger batch sizes and more gradient steps to guarantee
better convergence. We further use a clipping schedule for the neural network outputs, and an exponential
moving average of the parameters for evaluation (as is often done for diffusion models (Nichol & Dhariwal,
2021)). Moreover, we train with a varying number of step sizes for simulating the SDEs using the Euler-
Maruyama (EM) scheme in order to amortize the cost of training separate models for different evaluation
settings. To have a fair comparison, we trained PIS using the same setup. We also present a comparison to
the original code in Figure 11, showing that our setup leads to increased performance and stability. The
optimization routine is summarized in Algorithm 1.
Similar to PIS, we also use the score of the density ∇logρ(typically given in closed-form or evaluated via
automatic differentiation) for our parametrization of the control uθ, given by
uθ(x,t):= Φ(1)
θ(x,t) + Φ(2)
θ(t)σ(t)s(x,t). (24)
Here,sis a linear interpolation between the initial and terminal scores ∇logρand∇logpXu
0, thus approxi-
mating the optimal score ∇logpYat the boundary values, see Corollary 3.1. This parametrization yielded
7The associated repository can be found at https://github.com/juliusberner/sde_sampler .
9Published in Transactions on Machine Learning Research (02/2024)
Algorithm 1 Time-reversed diffusion sampler (DIS): Training the control in Section 3 via deep learning.
inputneural network uθwith initial parameters θ(0), optimizer method stepfor updating the parameters,
number of steps K, batch size m
output parameters (θ(k))K
k=1
fork←0,...,K−1do
(x(i))m
i=1←sample fromN(0,I)⊗m
/hatwideLDIS(uθ(k))←estimate the cost in (19) using the EM scheme with /hatwideXθ
0=x(i), i= 1,...,m
θ(k+1)←step/parenleftig
θ(k),∇/hatwideLDIS(uθ(k))/parenrightig
end for
the best results in practice, see also the comparison to other parametrizations in Section A.14. For the drift
and diffusion coefficients, we can make use of successful noise schedules for diffusion models and choose the
variance-preserving SDE from Song et al. (2021).
We evaluate DIS and PIS on a number of examples that we outline in the sequel. Specifically, we compare
the approximation of the log-normalizing constant logZusing (approximately) unbiased estimators given
by importance sampling in path space, see Section A.12 for DIS and Zhang & Chen (2022a) for PIS. To
evaluate the sample quality, we compare Monte Carlo approximations of expectations as well as estimates of
standard deviations. We further refer to Figure 7 in the appendix for preliminary experiments using Physics
informed neural networks (PINNs) for solving the corresponding HJB equation, as well as to Figure 6, where
we employ a divergence different from the KL divergence, which is possible due to our path space perspective.
4.1 Examples
Let us present the numerical examples on which we evaluate our method.
Gaussian mixture model (GMM): We consider
ρ(x) =M/summationdisplay
m=1αmN(x;/tildewideµm,Σm), (25)
with/summationtextM
m=1αm= 1. Specifically, we choose m= 9,Σm= 0.3 I, and (/tildewideµm)9
m=1={−5,0,5}×{− 5,0,5}⊂R2.
The density and the optimal drift are depicted in Figure 2.
Funnel: The10-dimensional Funnel distribution (Neal, 2003) is a challenging example often used to test
MCMC methods. It is given by
ρ(x) =N(x1; 0,ν2)d/productdisplay
i=2N(xi; 0,ex1) (26)
forx= (xi)10
i=1∈R10with8ν= 3.
Double well (DW): A typical problem in molecular dynamics considers sampling from the stationary
distribution of a Langevin dynamics, where the drift of the SDE is given by the negative gradient of a
potential Ψ, namely
dXs=−∇Ψ(Xs) ds+σ(s) dBs, (27)
see, e.g., (Leimkuhler & Matthews, 2015). Given certain assumptions on the potential, the stationary density
of the process can be shown to be pX∞=e−Ψ/Z. Potentials often contain multiple energy barriers , and
resulting local minima correspond to meaningful configurations of a molecule. However, the convergence
speed ofpXtcan be very slow – in particular for large energy barriers. Our time-reversed diffusion sampler,
8Due to a typo in the code, the results presented by Zhang & Chen (2022a) seem to consider ν= 3for the baselines, but the
favorable choice of ν= 1for evaluating their method.
10Published in Transactions on Machine Learning Research (02/2024)
100 200 400 800
integrator steps0.8
0.6
0.4
0.2
0.0log() (rw)
GMM
100 200 400 800
integrator steps0.1
0.00.10.2Funnel
100 200 400 800
integrator steps10111213DW (d=20,w=5,=4)
DIS
PIS
ground-truth
Figure 4: We compare our DIS method against PIS on the ability to compute the log-normalizing constant
logZ(median and interquartile range over 10training seeds) when using N∈{100,200,400,800}steps of
the Euler-Maruyama scheme. Our method outperforms PIS clearly for the GMM and Funnel examples and
offers a slight improvement for the DW example. Each model has been trained with each Nfor1/4of the
total gradient steps (starting with 100and ending with 800steps). See also Figure 10 for a comparison of
models trained on a single step size.
on the other hand, can (at least in principle) sample from densities already at a fixed time T. In our example,
we shall consider a d-dimensional double well potential, corresponding to an (unnormalized) density ρ=e−Ψ
given by
ρ(x) = exp/parenleftigg
−w/summationdisplay
i=1(x2
i−δ)2−1
2d/summationdisplay
i=w+1x2
i/parenrightigg
(28)
withw∈Ncombined double wells and a separation parameter δ∈(0,∞), see also Wu et al. (2020). Note
that, due to the double well structure of the potential, the density contains 2wmodes. For these multimodal
examples, we can compute a reference solution of the log-normalizing constant logZand other statistics of
interest by numerical integration since ρfactorizes in the dimensions.
4.2 Results
Our experiments show that DIS can offer improvements over PIS for all the tasks we considered, i.e., estimation
of normalizing constants, expectations, and standard deviations. Figures 4 and 5 display direct comparisons
for the examples in Section 4.1, noting that we use the same training techniques but different objectives
for the respective methods. Moreover, we recall that our setup of the PIS method already outperforms
the default implementation by Zhang & Chen (2022a), see Figure 11 in the appendix. We also emphasize
that the reference process of PIS satisfies that X0
T∼N (0,I), which is already the correct distribution
ford−wdimensions of the double well example. Nevertheless, our proposed DIS method converges to
a better approximation and provides strong results even for multimodal, high-dimensional distributions,
such asd= 50with 32well-separated modes, see Figure 5. Finally, we provide further results for two
promising future directions: We show that replacing the KL divergence in Corollary 3.1 with the log-variance
divergence (Nüsken & Richter, 2021; Richter et al., 2020) can further improve performance, see Figure 6
and Section A.14 for details. In Figure 7, we show that solving the HJB equation in Lemma 2.3 using
physics-informed neural networks (PINNs) can provide competitive results, see Section A.11 for further
explanations. We present more numerical results and comparisons in Sections A.13 and A.14.
4.3 Limitations
In this section we shall discuss limitations of the optimal control perspective and our resulting sampling
algorithm. First, note that the control perspective so far only yields new algorithms if the target density is
known (up to the normalization constant). In particular, we note that the direct minimization of divergences,
such as the KL or log-variance divergence, needs access to the (unnormalized) target density. This is typically
not the case for problems in generative modeling where one has only access to data samples and where one
resorts to optimizing the ELBO, as outlined in Section 2.4. However, we emphasize that classical sampling
11Published in Transactions on Machine Learning Research (02/2024)
12.3512.4012.4512.5012.5512.60expectation |x|2DW (d=10,w=3,=2)
272829DW (d=20,w=5,=3)
53.2553.5053.7554.0054.25DW (d=50,w=5,=2)
9.469.489.509.529.549.56expectation |x|1
19.2519.5019.7520.0020.2520.50
42.142.242.342.442.542.6
100 200 400 800
integrator steps1.10001.10251.10501.10751.1100avg. standard deviation
100 200 400 800
integrator steps1.101.121.141.161.18
100 200 400 800
integrator steps1.02751.03001.03251.03501.0375DIS
PIS
ground-truth
Figure 5: We compare our DIS method against PIS on the ability to estimate the expectations E/bracketleftbig
∥Y0∥2/bracketrightbig
and
E[∥Y0∥1]and the average standard deviation1
d/summationtextd
i=1/radicalbig
V[(Y0)i], see(148). Each model has been trained
with eachN∈{100,200,400,800}for1/4of the total gradient steps (starting with 100and ending with 800
steps). We compare the methods using Nsteps of the Euler-Maruyama scheme when computing our estimates
(median and interquartile range over 10training seeds). Our method outperforms PIS in all considered
settings in terms of accuracy.
problems (without any samples from the data distribution) can arguably be more challenging, e.g., due to
exploration-exploitation trade-offs and potential mode collapse.
Compared to classical sampling methods such as, e.g., MCMC and SMC methods, the sampling time of our
algorithm DIS is typically much faster. However, this comes at the cost of training a model first, which might
only amortize when a larger number of samples is needed. While a well-trained model enjoys good sampling
guarantees (cf. De Bortoli (2022); Chen et al. (2022); Lee et al. (2023)), approximating the optimal control
(along the controlled dynamics) can be challenging and sensitive to the chosen hyperparameters. We have
observed that clever choices of alternative divergences can improve convergence and counteract mode collapse,
however, it is generally difficult to provide convergence guarantees.
5 Conclusion and outlook
We propose a connection of diffusion models to the fields of optimal control and reinforcement learning,
which provides valuable new insights and allows to transfer established tools from one field to the respective
other. As first steps, we have shown how to readily derive the ELBO for continuous-time diffusion models
solely from control arguments, we provided an interpretation of diffusion models via measures on path space
(eventually leading to improved losses), and we extended the diffusion modeling framework to sampling from
unnormalized densities. We could demonstrate that our framework offers significant numerical advantages
over existing diffusion-based sampling approaches across a series of challenging, high-dimensional problems.
12Published in Transactions on Machine Learning Research (02/2024)
100 200 400 800
integrator steps0.8
0.6
0.4
0.2
0.0log() (rw)
GMM
100 200 400 800
integrator steps0.1
0.00.10.2Funnel
100 200 400 800
integrator steps2.5
2.0
1.5
1.0
0.5
DW (d=3,w=3,=4)
DIS (log-variance)
DIS (KL)
PIS
ground-truth
Figure 6: We use the settings in Figure 4 and only change the reverse KL divergence in Corollary 3.1 to
the log-variance divergence (Nüsken & Richter, 2021). Training with the latter objective can significantly
improve the estimate of the log-normalizing constant logZfor the GMM, the Funnel, and a DW example.
With the new connections at hand, we anticipate further fruitful theoretical as well as practical insights. For
instance, the PDE perspective offers to rely on PDE techniques and solvers that have been developed in the
past, see Section A.11 for details. For high-dimensional settings, tensor-based methods (Richter et al., 2023),
as well as SDE-based solvers (Nüsken & Richter, 2021), seem to be well suited for numerical approximation.
Alternatively, one can readily apply numerical methods from control theory to approximate the score function,
such as, for instance, methods based on tensor trains (Oster et al., 2022) or policy iteration, which has
recently been combined with deep learning (Zhou et al., 2021).
Note that one usually controls linear, Ornstein-Uhlenbeck type processes. This motivates to approximate
the target density by a Gaussian density (which corresponds to quadratic costs in the control problem).
Then, one can pretrain the score model by solving a linear-quadratic-control problem, for which very efficient
numerical methods have been developed extensively. On the other hand, we may add additional running costs
to the control objective in order to promote or prevent certain regions in the domain, which can incorporate
domain knowledge and improve sampling for respective applications. Interesting future perspectives also
include the usage of other divergences on path space (motivated by our promising results with the log-variance
divergence), as well as the extension to Schrödinger bridges, for instance based on the work by Chen et al.
(2021). These methods are particularly well-suited for the task of sampling from unnormalized densities,
where we cannot leverage the efficient denoising score matching objective.
Acknowledgments
We would like to thank Nikolas Nüsken and Ricky T. Q. Chen for many useful discussions. The research of
Lorenz Richter has been partially funded by Deutsche Forschungsgemeinschaft (DFG) through the grant
CRC 1114“Scaling Cascades in Complex Systems” (project A 05, project number 235221301 ). Julius Berner
is grateful to G-Research for the travel grant.
13Published in Transactions on Machine Learning Research (02/2024)
References
Brian DO Anderson. Reverse-time diffusion equation models. Stochastic Processes and their Applications , 12
(3):313–326, 1982.
Michael Arbel, Alex Matthews, and Arnaud Doucet. Annealed flow transport Monte Carlo. In International
Conference on Machine Learning , pp. 318–330. PMLR, 2021.
Ludwig Arnold. Stochastic Differential Equations: Theory and Applications . A Wiley-Interscience publication.
Wiley, 1974.
P. Baldi. Stochastic Calculus: An Introduction Through Theory and Exercises . Universitext. Springer
International Publishing, 2017.
Christian Beck, Sebastian Becker, Philipp Grohs, Nor Jaafari, and Arnulf Jentzen. Solving the Kolmogorov
PDE by means of deep learning. Journal of Scientific Computing , 88:1–28, 2021.
Richard Bellman. Dynamic programming . Princeton University Press, 1957.
Nils Berglund. Kramers’ law: Validity, derivations and generalisations. arXiv preprint arXiv:1106.5799 , 2011.
Julius Berner, Markus Dablander, and Philipp Grohs. Numerically solving parametric families of high-
dimensional Kolmogorov partial differential equations via deep learning. In Advances in Neural Information
Processing Systems , volume 33, pp. 16615–16627, 2020.
Sitan Chen, Sinho Chewi, Jerry Li, Yuanzhi Li, Adil Salim, and Anru R Zhang. Sampling is as easy as learning
the score: theory for diffusion models with minimal data assumptions. arXiv preprint arXiv:2209.11215 ,
2022.
Tianrong Chen, Guan-Horng Liu, and Evangelos A Theodorou. Likelihood training of Schrödinger Bridge
using forward-backward SDEs theory. arXiv preprint arXiv:2110.11291 , 2021.
Gabriele Corso, Hannes Stärk, Bowen Jing, Regina Barzilay, and Tommi Jaakkola. Diffdock: Diffusion steps,
twists, and turns for molecular docking. arXiv preprint arXiv:2210.01776 , 2022.
Marco Cuturi. Sinkhorn distances: Lightspeed computation of optimal transport. Advances in neural
information processing systems , pp. 2292–2300, 2013.
Paolo Dai Pra. A stochastic control approach to reciprocal diffusion processes. Applied mathematics and
Optimization , 23(1):313–329, 1991.
Paolo Dai Pra and Michele Pavon. On the Markov processes of Schrödinger, the Feynman-Kac formula and
stochastic control. In Realization and Modelling in System Theory , pp. 497–504. Springer, 1990.
Valentin De Bortoli. Convergence of denoising diffusion models under the manifold hypothesis. arXiv preprint
arXiv:2208.05314 , 2022.
Valentin De Bortoli, James Thornton, Jeremy Heng, and Arnaud Doucet. Diffusion Schrödinger bridge with
applications to score-based generative modeling. Advances in Neural Information Processing Systems , 34:
17695–17709, 2021.
Pierre Del Moral, Arnaud Doucet, and Ajay Jasra. Sequential Monte Carlo samplers. Journal of the Royal
Statistical Society: Series B (Statistical Methodology) , 68(3):411–436, 2006.
Arnaud Doucet, Adam M Johansen, et al. A tutorial on particle filtering and smoothing: Fifteen years later.
Handbook of nonlinear filtering , 12(656-704):3, 2009.
Arnaud Doucet, Will Sussman Grathwohl, Alexander GDG Matthews, and Heiko Strathmann. Score-based
diffusion meets annealed importance sampling. In Advances in Neural Information Processing Systems ,
2022.
14Published in Transactions on Machine Learning Research (02/2024)
Lawrence C. Evans. Partial differential equations . American Mathematical Society, Providence, R.I., 2010.
Wendell H Fleming and Raymond W Rishel. Deterministic and stochastic optimal control , volume 1. Springer
Science & Business Media, 2012.
Wendell H Fleming and Halil Mete Soner. Controlled Markov processes and viscosity solutions , volume 25.
Springer Science & Business Media, 2006.
Hans Föllmer. Random fields and diffusion processes. In École d’Été de Probabilités de Saint-Flour XV–XVII,
1985–87, pp. 101–203. Springer, 1988.
Carsten Hartmann and Lorenz Richter. Nonasymptotic bounds for suboptimal importance sampling. arXiv
preprint arXiv:2102.09606 , 2021.
Carsten Hartmann, Lorenz Richter, Christof Schütte, and Wei Zhang. Variational characterization of free
energy: Theory and algorithms. Entropy, 19(11):626, 2017.
Ulrich G Haussmann and Etienne Pardoux. Time reversal of diffusions. The Annals of Probability , pp.
1188–1205, 1986.
Dan Hendrycks and Kevin Gimpel. Gaussian error linear units (GELUs). arXiv preprint arXiv:1606.08415 ,
2016.
Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in Neural
Information Processing Systems , 33:6840–6851, 2020.
Matthew D Hoffman and Andrew Gelman. The No-U-Turn sampler: adaptively setting path lengths in
Hamiltonian Monte Carlo. Journal of Machine Learning Research , 15(1):1593–1623, 2014.
Lars Holdijk, Yuanqi Du, Ferry Hooft, Priyank Jaini, Bernd Ensing, and Max Welling. Path integral stochastic
optimal control for sampling transition paths. arXiv preprint arXiv:2207.02149 , 2022.
Chin-Wei Huang, Jae Hyun Lim, and Aaron C Courville. A variational perspective on diffusion-based
generative models and score matching. Advances in Neural Information Processing Systems , 34, 2021.
Aapo Hyvärinen and Peter Dayan. Estimation of non-normalized statistical models by score matching.
Journal of Machine Learning Research , 6(4), 2005.
Bowen Jing, Gabriele Corso, Jeffrey Chang, Regina Barzilay, and Tommi Jaakkola. Torsional diffusion for
molecular conformer generation. arXiv preprint arXiv:2206.01729 , 2022.
Patrick Kidger, James Foster, Xuechen Chen Li, and Terry Lyons. Efficient and accurate gradients for neural
SDEs.Advances in Neural Information Processing Systems , 34:18747–18761, 2021.
Diederik Kingma, Tim Salimans, Ben Poole, and Jonathan Ho. Variational diffusion models. Advances in
Neural Information Processing Systems , 34:21696–21707, 2021.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980 , 2014.
Peter E Kloeden and Eckhard Platen. Stochastic differential equations. In Numerical Solution of Stochastic
Differential Equations , pp. 103–160. Springer, 1992.
Zhifeng Kong, Wei Ping, Jiaji Huang, Kexin Zhao, and Bryan Catanzaro. Diffwave: A versatile diffusion
model for audio synthesis. In International Conference on Learning Representations , 2021.
Takeshi Koshizuka and Issei Sato. Neural Lagrangian Schrödinger bridge. arXiv preprint arXiv:2204.04853 ,
2022.
Isaac E Lagaris, Aristidis Likas, and Dimitrios I Fotiadis. Artificial neural networks for solving ordinary and
partial differential equations. IEEE transactions on neural networks , 9(5):987–1000, 1998.
15Published in Transactions on Machine Learning Research (02/2024)
Holden Lee, Jianfeng Lu, and Yixin Tan. Convergence of score-based generative modeling for general data
distributions. In International Conference on Algorithmic Learning Theory , pp. 946–985. PMLR, 2023.
Flavien Léger and Wuchen Li. Hopf–Cole transformation via generalized Schrödinger bridge problem. Journal
of Differential Equations , 274:788–827, 2021.
Ben Leimkuhler and Charles Matthews. Molecular dynamics. Interdisciplinary applied mathematics , 39:443,
2015.
Christian Léonard. Some properties of path measures. In Séminaire de Probabilités XLVI , pp. 207–230.
Springer, 2014.
Xuechen Li, Ting-Kam Leonard Wong, Ricky TQ Chen, and David Duvenaud. Scalable gradients for
stochastic differential equations. In International Conference on Artificial Intelligence and Statistics , pp.
3870–3882. PMLR, 2020.
Pierre-Louis Lions. Optimal control of diffusion processes and Hamilton-Jacobi–Bellman equations part 2:
viscosity solutions and uniqueness. Communications in partial differential equations , 8(11):1229–1276, 1983.
Jun S Liu and Jun S Liu. Monte Carlo strategies in scientific computing , volume 10. Springer, 2001.
David MacKay. Information theory, pattern recognition and neural networks. In Proceedings of the 1st
International Conference on Evolutionary Computation, Cambridge University Press , 2003.
Dimitra Maoutsa. Geometric constraints improve inference of sparsely observed stochastic dynamics. arXiv
preprint arXiv:2304.00423 , 2023.
Dimitra Maoutsa and Manfred Opper. Deterministic particle flows for constraining stochastic nonlinear
systems. Physical Review Research , 4(4):043035, 2022.
Laurence Illing Midgley, Vincent Stimper, Gregor NC Simm, Bernhard Schölkopf, and José Miguel Hernández-
Lobato. Flow annealed importance sampling bootstrap. In NeurIPS 2022 AI for Science: Progress and
Promises , 2022.
Tom Minka et al. Divergence measures and message passing. Technical report, Citeseer, 2005.
Radford M Neal. Annealed importance sampling. Statistics and computing , 11(2):125–139, 2001.
Radford M Neal. Slice sampling. The Annals of Statistics , 31(3):705–767, 2003.
E Nelson. Dynamical theories of Brownian motion. Press, Princeton, NJ , 1967.
Alexander Quinn Nichol and Prafulla Dhariwal. Improved denoising diffusion probabilistic models. In
International Conference on Machine Learning , pp. 8162–8171. PMLR, 2021.
Nikolas Nüsken and Lorenz Richter. Solving high-dimensional Hamilton–Jacobi–Bellman PDEs using neural
networks: perspectives from the theory of controlled diffusions and measures on path space. Partial
Differential Equations and Applications , 2(4):1–48, 2021.
Nikolas Nüsken and Lorenz Richter. Interpolating between BSDEs and PINNs: deep learning for elliptic and
parabolic boundary value problems. Journal of Machine Learning , 2023.
Bernt Øksendal and Bernt Øksendal. Stochastic differential equations . Springer, 2003.
Mathias Oster, Leon Sallandt, and Reinhold Schneider. Approximating optimal feedback controllers of finite
horizon control problems using hierarchical tensor formats. SIAM Journal on Scientific Computing , 44(3):
B746–B770, 2022.
George Papamakarios, Eric T Nalisnick, Danilo Jimenez Rezende, Shakir Mohamed, and Balaji Lakshmi-
narayanan. Normalizing flows for probabilistic modeling and inference. J. Mach. Learn. Res. , 22(57):1–64,
2021.
16Published in Transactions on Machine Learning Research (02/2024)
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen,
Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep
learning library. Advances in neural information processing systems , 32, 2019.
Michele Pavon. Stochastic control and nonequilibrium thermodynamical systems. Applied Mathematics and
Optimization , 19(1):187–202, 1989.
MichelePavon. Onlocalentropy, stochasticcontrolanddeepneuralnetworks. arXiv preprint arXiv:2204.13049 ,
2022.
H. Pham. Continuous-time Stochastic Control and Optimization with Financial Applications . Stochastic
Modelling and Applied Probability. Springer Berlin Heidelberg, 2009.
Maziar Raissi, Paris Perdikaris, and George Em Karniadakis. Physics informed deep learning (part I):
Data-driven solutions of nonlinear partial differential equations. arXiv preprint arXiv:1711.10561 , 2017.
Danilo Rezende and Shakir Mohamed. Variational inference with normalizing flows. In International
conference on machine learning , pp. 1530–1538. PMLR, 2015.
Lorenz Richter. Solving high-dimensional PDEs, approximation of path space measures and importance
sampling of diffusions . PhD thesis, BTU Cottbus-Senftenberg, 2021.
Lorenz Richter and Julius Berner. Robust SDE-based variational formulations for solving linear PDEs via
deep learning. In International Conference on Machine Learning , pp. 18649–18666. PMLR, 2022.
Lorenz Richter and Julius Berner. Improved sampling via learned diffusions. In International Conference on
Learning Representations , 2024.
Lorenz Richter, Ayman Boustati, Nikolas Nüsken, Francisco Ruiz, and Omer Deniz Akyildiz. VarGrad:
a low-variance gradient estimator for variational inference. Advances in Neural Information Processing
Systems, 33:13481–13492, 2020.
Lorenz Richter, Leon Sallandt, and Nikolas Nüsken. From continuous-time formulations to discretization
schemes: tensor trains and robust regression for bsdes and parabolic PDEs. arXiv preprint arXiv:2307.15496 ,
2023.
Christian P Robert, George Casella, and George Casella. Monte Carlo statistical methods , volume 2. Springer,
1999.
Geoffrey Roeder, Yuhuai Wu, and David K Duvenaud. Sticking the landing: Simple, lower-variance gradient
estimators for variational inference. Advances in Neural Information Processing Systems , 30, 2017.
Tim Salimans, Diederik Kingma, and Max Welling. Markov chain Monte Carlo and variational inference:
Bridging the gap. In International conference on machine learning , pp. 1218–1226. PMLR, 2015.
E.Schrödinger. Über die Umkehrung der Naturgesetze . VerlagderAkademiederWissenschafteninKommission
bei Walter De Gruyter u. Company, 1931.
JustinSirignanoandKonstantinosSpiliopoulos. DGM:Adeeplearningalgorithmforsolvingpartialdifferential
equations. Journal of computational physics , 375:1339–1364, 2018.
Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning
using nonequilibrium thermodynamics. In International Conference on Machine Learning , pp. 2256–2265.
PMLR, 2015.
Yang Song and Stefano Ermon. Improved techniques for training score-based generative models. Advances in
neural information processing systems , 33:12438–12448, 2020.
Yang Song, Sahaj Garg, Jiaxin Shi, and Stefano Ermon. Sliced score matching: A scalable approach to
density and score estimation. In Uncertainty in Artificial Intelligence , pp. 574–584. PMLR, 2020.
17Published in Transactions on Machine Learning Research (02/2024)
Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole.
Score-based generative modeling through stochastic differential equations. In International Conference on
Learning Representations , 2021.
Gabriel Stoltz, Mathias Rousset, et al. Free energy computations: A mathematical perspective . World
Scientific, 2010.
Matthew Tancik, Pratul Srinivasan, Ben Mildenhall, Sara Fridovich-Keil, Nithin Raghavan, Utkarsh Singhal,
Ravi Ramamoorthi, Jonathan Barron, and Ren Ng. Fourier features let networks learn high frequency
functions in low dimensional domains. Advances in Neural Information Processing Systems , 33:7537–7547,
2020.
Sep Thijssen and HJ Kappen. Path integral control and state-dependent feedback. Physical Review E , 91(3):
032104, 2015.
Belinda Tzen and Maxim Raginsky. Theoretical guarantees for sampling and inference in generative models
with latent diffusions. In Conference on Learning Theory , pp. 3084–3114. PMLR, 2019.
A Süleyman Üstünel and Moshe Zakai. Transformation of measure on Wiener space . Springer Science &
Business Media, 2013.
Arash Vahdat, Karsten Kreis, and Jan Kautz. Score-based generative modeling in latent space. Advances in
Neural Information Processing Systems , 34:11287–11302, 2021.
Ramon Van Handel. Stochastic calculus, filtering, and stochastic control. Lecture Notes , 2007.
Francisco Vargas and Nikolas Nüsken. Transport, variational inference and diffusions: with applications to
annealed flows and Schrödinger bridges. arXiv preprint arXiv:2307.01050 , 2023.
Francisco Vargas, Will Grathwohl, and Arnaud Doucet. Denoising diffusion samplers. In International
Conference on Learning Representations , 2023a.
Francisco Vargas, Andrius Ovsianas, David Fernandes, Mark Girolami, Neil D Lawrence, and Nikolas Nüsken.
Bayesian learning via neural Schrödinger–Föllmer flows. Statistics and Computing , 33(1):1–22, 2023b.
Pascal Vincent. A connection between score matching and denoising autoencoders. Neural computation , 23
(7):1661–1674, 2011.
Martin J Wainwright, Michael I Jordan, et al. Graphical models, exponential families, and variational
inference. Foundations and Trends in Machine Learning , 1(1–2):1–305, 2008.
Hao Wu, Jonas Köhler, and Frank Noé. Stochastic normalizing flows. Advances in Neural Information
Processing Systems , 33:5933–5944, 2020.
Benjamin J Zhang and Markos A Katsoulakis. A mean-field games laboratory for generative modeling. arXiv
preprint arXiv:2304.13534 , 2023.
Qinsheng Zhang and Yongxin Chen. Path integral sampler: a stochastic control approach for sampling. In
International Conference on Learning Representations , 2022a.
Qinsheng Zhang and Yongxin Chen. Fast sampling of diffusion models with exponential integrator. arXiv
preprint arXiv:2204.13902 , 2022b.
Mo Zhou, Jiequn Han, and Jianfeng Lu. Actor-critic method for high dimensional static Hamilton-Jacobi-
Bellman partial differential equations based on neural networks. SIAM Journal on Scientific Computing ,
43(6):A4043–A4066, 2021.
18Published in Transactions on Machine Learning Research (02/2024)
A Appendix
Contents
A.1 Setting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19
A.2 Identities for divergences . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20
A.3 Reverse-time SDEs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20
A.4 Further details on the HJB equation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21
A.5 Hopf–Cole transformation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21
A.6 Brief introduction to stochastic optimal control . . . . . . . . . . . . . . . . . . . . . . . . . . 22
A.7 Verification theorem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23
A.8 Measures on path space . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24
A.9 ELBO formulations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27
A.10 Diffusion-based sampling from unnormalized densities . . . . . . . . . . . . . . . . . . . . . . 28
A.10.1 DDS as a special case of DIS . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29
A.10.2 Optimal drifts of DIS and PIS . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30
A.11 PDE-based methods for sampling from unnormalized densities . . . . . . . . . . . . . . . . . 30
A.11.1 Physics informed neural networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31
A.11.2 BSDE-based methods and Feynman-Kac formula . . . . . . . . . . . . . . . . . . . . . 32
A.12 Importance sampling in path space . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33
A.13 Details on implementation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34
A.14 Further numerical results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37
A.1 Setting
Letd,k∈NandT∈(0,∞). For a random variable X, which is absolutely continuous w.r.t. to the Lebesgue
measure, we write pXfor its density. We denote by Ba standard d-dimensional Brownian motion. We say
that a continuous Rd-valued stochastic process Y= (Yt)t∈[0,T]has density pY:Rd×[0,T]→[0,∞)if for all
t∈[0,T]the random variable Ythas density pY(·,t)w.r.t. to the d-dimensional Lebesgue measure, i.e., for
allt∈[0,T]and all measurable A⊂Rdit holds that
P[Yt∈A] =/integraldisplay
ApY(x,t) dx=/integraldisplay
ApYt(x) dx. (29)
We denote by PYthe law of Yon the space of continuous functions C([0,T],Rd)equipped with the
Borel measure. We assume that the coefficient functions and initial conditions of all appearing SDEs are
sufficiently regular such that the SDEs admit unique strong solutions and Novikov’s condition is satisfied,
see, e.g., Øksendal & Øksendal (2003, Section 8.6). Furthermore, we assume that the SDE solutions have
densities that are smooth and strictly positive for t∈(0,T)and that can be written as unique solutions to
corresponding Fokker-Planck equations, see, for instance, Arnold (1974, Section 2.6) and Baldi (2017, Section
10.5) for the details. For a function f:Rd×[0,T]→Rk, we write ⃗ffor the time-reversed function given by
⃗f(x,t) =f(x,T−t),(x,t)∈Rd×[0,T]. (30)
For a scalar-valued function g:Rd×[0,T]→R, we denote by∇g,∇2g, and div(g)its gradient, Hessian
matrix, and divergence w.r.t. to the spatial variable x. For matrix-valued functions A:Rd→Rd×d, we denote
19Published in Transactions on Machine Learning Research (02/2024)
by
Tr(A):=d/summationdisplay
i=1Aii (31)
the trace of its output. Finally, we define the divergence of matrix-valued functions row-wise, see Section A.2.
A.2 Identities for divergences
LetA:Rd→Rd×d,v:Rd→Rd, andg:Rd→R. We define the divergence of Arow-wise, i.e.,
div(A):= (div(Ai·))d
i=1=d/summationdisplay
j=1∂xjA·j, (32)
whereAi·andA·jdenote thei-th row and j-th column, respectively. Then the following identities hold true:
1.div(div(A)) =/summationtextd
i,j=1∂xi∂xjAij
2.div(vg) = div(v)g+v·∇g
3.div(Ag) = div(A)g+A∇g
4.div(Av) = div(A⊤)·v+ Tr(A∇v).
A.3 Reverse-time SDEs
The next theorem shows that the marginals of a time-reversed Itô process can be represented as marginals of
another Itô process, see Huang et al. (2021); Song et al. (2021); Nelson (1967); Anderson (1982); Haussmann
& Pardoux (1986); Föllmer (1988). We present a formulation from Huang et al. (2021, Appendix G) which
derives a whole family of processes (parametrized by a function λ). The relations stated in equation (2)
follow from the choice λ= 0and the fact that div(D) = 0ifσdoes not depend on the spatial variable x.
Theorem A.1 (Reverse-time SDE) .Letf∈Rd×[0,T]→Rdandσ:Rd×[0,T]→Rd×d, letY= (Ys)s∈[0,T]
be the solution to the SDE
dYs=f(Ys,s) ds+σ(Ys,s) dBs, (33)
and assume that Yhas density pY, which satisfies the Fokker-Planck equation given by
∂tpY= div (div ( DpY)−fpY), (34)
whereD:=1
2σσ⊤. For every λ∈C2([0,T],[0,1])the solution ⃗Y= (⃗Ys)s∈[0,T]to the reverse-time SDE
d⃗Ys=⃗µ(λ)(⃗Ys,s) ds+⃗σ(λ)(⃗Ys,s) dBs,⃗Y0∼YT, (35)
with
µ(λ):= (2−λ) div/parenleftbig
D/parenrightbig
+ (2−λ)D∇logpY−f, (36)
and
σ(λ):=√
1−λσ (37)
has density p⃗Ygiven by
p⃗Y(·,t) =⃗pY(·,t) (38)
almost everywhere for every t∈[0,T]. In other words, for every t∈[0,T]it holds that YT−t∼⃗Yt.
Proof.Using the Fokker-Planck equation in (34), we observe that
∂t⃗pY= div/parenleftig
−div/parenleftig
⃗D⃗pY/parenrightig
+⃗f⃗pY/parenrightig
. (39)
20Published in Transactions on Machine Learning Research (02/2024)
The negative divergence, originating from the chain rule, prohibits us from directly viewing the above equation
as a Fokker-Planck equation. We can, however, use the identities in Section A.2, to show that
div/parenleftig
⃗D⃗pY/parenrightig
= div/parenleftig
⃗D/parenrightig
⃗pY+⃗D∇⃗pY=/parenleftig
div/parenleftig
⃗D/parenrightig
+⃗D∇log⃗pY/parenrightig
⃗pY. (40)
This implies that we can rewrite (39) as
∂t⃗pY= div/parenleftig
(1−λ) div/parenleftig
⃗D⃗pY/parenrightig
−(2−λ) div/parenleftig
⃗D⃗pY/parenrightig
+⃗f⃗pY/parenrightig
(41a)
= div/parenleftbig
div/parenleftbig⃗D(λ)⃗pY/parenrightbig
−⃗µ(λ)⃗pY/parenrightbig
, (41b)
whereD(λ):=1
2σ(λ)(σ(λ))⊤. As the PDE in (41b)defines a valid Fokker-Planck equation associated to the
reverse-time SDE given by (35), this proves the claim.
A.4 Further details on the HJB equation
In order to solve Problem 2.1, one might be tempted to rely on classical methods to approximate the solution
of the HJB equation from Lemma 2.3 directly. However, in the setting of Remark 2.2, one should note that
the optimal drift,
µ=σσ⊤∇logpY−f=−σσ⊤∇V−f, (42)
contains the solution Vitself. Plugging it into the HJB equation in Lemma 2.3, we get the equation
∂tV= Tr/parenleftbig
D∇2V/parenrightbig
−f·∇V+ div(f)−1
2∥σ⊤∇V∥2, V (·,T) =−logpX0. (43)
Likewise, when applying the Hopf–Cole transformation from Section A.5 to pYdirectly, i.e., considering
V:=−logpY, whereYis the solution to SDE (3), we get the same PDE. We note that the signs in (43)
do not match with typical HJB equations from control theory. In order to obtain an HJB equation, we can
consider the time-reversed function ⃗V, which satisfies
∂t⃗V=−Tr/parenleftig
⃗D∇2⃗V/parenrightig
+⃗f·∇⃗V−div(⃗f) +1
2∥⃗σ⊤∇⃗V∥2,⃗V(·,T) =−logpXT. (44)
Unfortunately, the terminal conditions in both (43)and(44)are typically not available in the context of
generative modeling. Specifically, for the optimal drift in (42), they correspond to the intractable marginal
densities of the inference process Y, since it holds that pX=⃗pY, see Remark 2.2. However, the situation is
different in the case of sampling from (unnormalized) densities, see Section 3.
A.5 Hopf–Cole transformation
Recall that the Kolmogorov backward equation follows from the Fokker-Planck equation by using the
divergence identities from Section A.2, i.e.
∂t⃗pX= div (−div (D⃗pX) +µ⃗pX) = div (−D∇⃗pX) +µ·∇⃗pX+ div(µ)⃗pX (45a)
=−Tr/parenleftbig
D∇2⃗pX/parenrightbig
+µ·∇⃗pX+ div(µ)⃗pX. (45b)
The following lemma details the relation of the HJB equation in Lemma 2.3 and the linear Kolmogorov
backward equation9in(5). A proof of the Hopf-Cole transformation can, e.g., be found in Evans (2010,
Section 4.4.1) and Richter & Berner (2022, Appendix G). Lemma 2.3 follows with the choices b:=−µand
h:= div(µ).
Lemma A.2 (Hopf–Cole transformation) .Leth:Rd×[0,T]→Rand letp∈C2,1(Rd×[0,T],R)solve the
linear PDE
∂tp=−1
2Tr(σσ⊤∇2p)−b·∇p+hp. (46)
ThenV:=−logpsatisfies the HJB equation
∂tV=−1
2Tr(σσ⊤∇2V)−b·∇V−h+1
2/vextenddouble/vextenddoubleσ⊤∇V/vextenddouble/vextenddouble2. (47)
9Fordiv(µ) = 0this can be viewed as the adjoint of the Fokker-Planck equation.
21Published in Transactions on Machine Learning Research (02/2024)
For further applications of the Hopf–Cole transformation we refer, for instance, to Fleming & Soner (2006);
Hartmann et al. (2017); Léger & Li (2021).
A.6 Brief introduction to stochastic optimal control
In this section, we shall provide a brief introduction to stochastic optimal control. For details and further
reading, we refer the interested reader to the monographs by Fleming & Rishel (2012); Fleming & Soner
(2006); Pham (2009); Van Handel (2007). Loosely speaking, stochastic control theory deals with identifying
optimal strategies in noisy environments, in our case, continuous-time stochastic processes defined by the
SDE
dXU
s=/tildewideµ(XU
s,s,Us) ds+/tildewideσ(XU
s,s,Us) dBs, (48)
where/tildewideµand/tildewideσare suitable functions, Bis ad-dimensional Brownian motion, and Uis a progressively
measurable, Rd-valued random control process. For ease of presentation, we focus on the frequent case where
Uis aMarkov control , which means that there exists a deterministic function u∈U⊂C(Rd×[0,T],Rd),
such thatUs=u(XU
s,s). In other words, the randomness of the process Uis only coming from the stochastic
processXU. The function class Uthen defines the set of admissible controls . Very often, one considers the
special cases
/tildewideµ(x,s,u ):=µ(x,s) +σ(s)u(x,s)and/tildewideσ(x,s,u ):=σ(s), (49)
whereµandσmight correspond to the choices taken in Section 2.4, and one might think of uas a steering
force as to reach a certain target.
The goal is now to minimize specified control costs with respect to the control u. To this end, we can define
the cost functional
J(u;xinit,0) = E/bracketleftigg/integraldisplayT
0/tildewideh(XU
s,s,u(XU
s,s)) ds+g(XU
T)/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleXU
0=xinit/bracketrightigg
, (50)
where/tildewideh:Rd×[0,T]×Rd→Rspecifiesrunning costs andg:Rd→Rrepresents terminal costs . Furthermore
we can define the cost-to-go as
J(u;x,t) = E/bracketleftigg/integraldisplayT
t/tildewideh(XU
s,s,u(XU
s,s)) ds+g(XU
T)/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleXU
t=x/bracketrightigg
, (51)
now depending on respective initial values (x,t)∈Rd×[0,T]. The objective in optimal control is now to
minimize this quantity over all admissible controls u∈Uand we, therefore, introduce the so-called value
function
V(x,t) = inf
u∈UJ(u;x,t) (52)
as the optimal costs conditioned on being in position xat timet.
Motivated by the dynamic programming principle (Bellman, 1957), one can then derive the main result
from control theory, namely that the function Vdefined in (52)fulfills a nonlinear PDE, which can thus be
interpreted as the determining equation for optimality10.
Theorem A.3 (Verification theorem for general HJB equation) .LetV∈C2,1(Rd×[0,T],R)fulfill the PDE
∂tV=−inf
α∈Rd/braceleftbigg
/tildewideh(·,·,α) +/tildewideµ(·,·,α)·∇V+1
2Tr/parenleftbig
(/tildewideσ/tildewideσ⊤)(·,·,α)∇2V/parenrightbig/bracerightbigg
, V(·,T) =g, (53)
such that
sup
(x,t)∈Rd×[0,T]∥V(x,t)∥
1 +∥x∥2<∞ (54)
10In practice, solutions to optimal control problems may not posses enough regularity in order to formally fulfill the HJB
equation, such that a complete theory of optimal control needs to introduce an appropriate concept of weak solutions, leading to
so-called viscosity solutions that have been studied, for instance, in Fleming & Soner (2006); Lions (1983).
22Published in Transactions on Machine Learning Research (02/2024)
and assume that there exists a measurable function U∋u∗:Rd×[0,T]→Rdthat attains the above infimum
for all (x,t)∈Rd×[0,T]. Further, let the correspondingly controlled SDE in (48)withU∗
s:=u∗(XU∗
s,s)
have a strong solution XU∗. ThenVcoincides with the value function as defined in (52)andu∗is an optimal
Markovian control.
Let us appreciate the fact that the infimum in the HJB equation in (53)is merely over the set Rdand not
over the function space Uas in(52), so the minimization reduces to a pointwise operation. A proof of
Theorem A.3 can, for instance, be found in Pham (2009, Theorem 3.5.2).
In many applications, in addition to the choices (49), one considers the special form of running costs
/tildewideh(x,s,u (x,s)):=h(x,s) +1
2∥u(x,s)∥2, (55)
whereh:Rd×[0,T]→Rd. In this setting, the minimization appearing in the general HJB equation (53)can
be solved explicitly, therefore leading to a closed-form PDE, as made precise with the following Corollary.
Corollary A.4 (HJB equation with quadratic running costs) .If the diffusion coefficient /tildewideσdoes not depend
on the control, the control enters additively in the drift as in (49), and the running costs take the form
/tildewideh(x,s,u (x,s)) =h(x,s) +1
2∥u(x,s)∥2, (56)
then the general HJB equation in (53)can be stated in closed form as the HJB equation in (47).
Proof.We formally compute
inf
α∈Rd/braceleftig
/tildewideh(·,·,α) +/tildewideµ(·,·,α)·∇V/bracerightig
=h+µ·∇V+ inf
α∈Rd/braceleftbigg1
2∥α∥2+σα·∇V/bracerightbigg
, (57)
and realize that the infimum is attained when choosing α∗=−σ⊤∇V(x,t)for each corresponding (x,t)∈
Rd×[0,T], resulting in the optimal control u∗=−σ⊤∇V. Plugging this into the general HJB equation (53),
we readily get the PDE in (47).
A.7 Verification theorem
Theverification theorem is a classical result in optimal control, and the proof can, for instance, be found
in Nüsken & Richter (2021, Theorem 2.2), Fleming & Soner (2006, Theorem IV.4.4), and Pham (2009,
Theorem 3.5.2), see also Section A.6. For the interested reader, we provide the theorem and a self-contained
proof using Itô’s lemma in the following. Theorem 2.4 follows with the choices t:= 0,b:=−µ,h:=div(µ),
andg:=−logpX0.
Theorem A.5 (Verification theorem) .LetVbe a solution to the HJB equation in (47)with terminal
conditionV(·,T) =g. Further, let t∈[0,T]and define the set of admissible controls by
U:=/braceleftigg
u∈C1(Rd×[t,T],Rd): sup
(x,s)∈Rd×[t,T]∥u(x,s)∥
1 +∥x∥<∞/bracerightigg
. (58)
For every control u∈UletZu= (Zu
s)s∈[t,T]be the solution to the controlled SDE
dZu
s= (σu+b) (Zu
s,s) ds+σ(s) dBs (59)
and let the cost of the control ube defined by
J(u;t):= E/bracketleftigg/integraldisplayT
t/parenleftbigg
h+1
2∥u∥2/parenrightbigg
(Zu
s,s) ds+g(Zu
T)/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleZu
t/bracketrightigg
. (60)
Then for every u∈Uit holds almost surely that
V(Zu
t,t) + E/bracketleftigg
1
2/integraldisplayT
t/vextenddouble/vextenddoubleσ⊤∇V+u/vextenddouble/vextenddouble2(Zu
s,s) ds/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleZu
t/bracketrightigg
=J(u;t). (61)
23Published in Transactions on Machine Learning Research (02/2024)
In particular, this implies that V(Zu
t,t) =minu∈UJ(u;t)almost surely, where the unique minimum is attained
byu∗:=−σ⊤∇V.
Proof.Let us derive the verification theorem directly from Itô’s lemma, which, under suitable assumptions,
states that
V(Zu
T,T)−V(Zu
t,t) =/integraldisplayT
t/parenleftbig
∂sV+ (σu+b)·∇V+ Tr/parenleftbig
D∇2
xV/parenrightbig/parenrightbig
(Zu
s,s) ds+S (62)
almost surely, where
S:=/integraldisplayT
t(σ⊤∇V)(Zu
s,s)·dBs, (63)
see, e.g., Theorem 8.3 in Baldi (2017). Combining this with the fact that Vsolves the HJB equation in (47)
and the simple calculation
1
2∥σ⊤∇V+u∥2=1
2/parenleftbig
σ⊤∇V+u/parenrightbig
·/parenleftbig
σ⊤∇V+u/parenrightbig
(64a)
=1
2∥σ⊤∇V∥2+/parenleftbig
σ⊤∇V/parenrightbig
·u+1
2∥u∥2, (64b)
shows that
V(Zu
t,t) =/integraldisplayT
t/parenleftbigg
h+1
2∥u∥2−1
2/vextenddouble/vextenddoubleσ⊤∇V+u/vextenddouble/vextenddouble2/parenrightbigg
(Zu
s,s) ds+g(Zu
T)−S (65)
almost surely. Under mild regularity assumptions, the stochastic integral Shas zero expectation conditioned
onZu
t, which proves the claim.
Remark A.6 (Variational gap) .We can interpret the term
E/bracketleftigg
1
2/integraldisplayT
t/vextenddouble/vextenddoubleσ⊤∇V+u/vextenddouble/vextenddouble2(Zu
s,s) ds/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleZu
t/bracketrightigg
(66)
in(61)as the variational gap specifying the misfit of the current and the optimal control objective. In the
setting of Corollary 2.5 it takes the form
E/bracketleftigg
1
2/integraldisplayT
0/vextenddouble/vextenddoubleu−σ⊤∇log⃗pX/vextenddouble/vextenddouble2(Yu
s,s) ds/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleY0/bracketrightigg
(67)
and can be compared to Huang et al. (2021, Theorem 4), where, however, the factor 1/2seems to be missing.
A.8 Measures on path space
In this section, we elaborate on the path space measure perspective on diffusion-based generative modeling,
as introduced in Section 2.3. Recalling our setting in Section A.1, we first state the Girsanov theorem that
will be helpful in the following, see, for instance, Üstünel & Zakai (2013, Proposition 2.2.1 and Theorem
2.1.1) for a proof.
Theorem A.7 (Girsanov theorem) .For every control u∈UletYu= (Yu
s)s∈[0,T]be the solution to the
controlled SDE
dYu
s= (σu−µ) (Yu
s,s) ds+σ(s) dBs. (68)
Foru,v∈U, it then holds that
logd PYu
d PYv(Yu) =Ru−v
0(Yu) +/integraldisplayT
0(u−v)(Yu
s,s)·dBs (69)
and, in particular, that
DKL( PYu| PYv) = E/bracketleftbigg
logd PYu
d PYv(Yu)/bracketrightbigg
= E/bracketleftbig
Ru−v
0(Yu)/bracketrightbig
. (70)
24Published in Transactions on Machine Learning Research (02/2024)
Note that the expression for the KL divergence in (70)follows from the fact that, under mild regularity
assumptions, the stochastic integral in (69)is a martingale and has vanishing expectation. Now, we present
a lemma that specifies how the KL divergence behaves when we change the initial value of a path space
measure.
Lemma A.8 (KL divergence and disintegration) .LetX,Y,Zbe diffusion processes. Further, let Zbe such
thatZ0∼Y0and11dZ= dX. Then it holds that
DKL( PY| PX) =DKL( PY| PZ) +DKL( PY0| PX0). (71)
Proof.Let PYxbe the path space measure of the process Ywith initial condition Y0=x∈Rd, let PY0be
the marginal at time t= 0, and similarly define the corresponding quantities for the processes XandZ. Since
our assumptions guarantee that PXx= PZxand PZ0= PY0, the disintegration theorem, see, e.g., Léonard
(2014), shows that
d PY
d PX=d PYx
d PXxd PY0
d PX0=d PYx
d PZxd PY0
d PZ0d PY0
d PX0=d PY
d PZd PY0
d PX0, (72)
which implies the claim.
Using the previous two results, we can now provide a path space perspective on the optimal control problem
in Theorem 2.4. Note that Proposition 2.6 follows directly from this result. For further connections between
optimal control and path space measures, we refer to Hartmann et al. (2017); Thijssen & Kappen (2015).
Proposition A.9 (Optimal path space measure) .We define the work functional W:C([0,T],Rd)→Rfor
all suitable stochastic processes Yby
W(Y):=R0
µ(Y)−logpX0(YT)
pXT(Y0), (73)
whereR0
µ(Y0)is as in(9)withu= 0. Further, let the path space measure Qbe defined via the Radon-Nikodym
derivative
d Q
d PY0= exp (−W). (74)
Then it holds that Q= PYu∗, whereu∗:=σ⊤∇log⃗pXas in Corollary 2.5, and for every u∈Uwe have that
logd PYu
d PYu∗(Yu) =Ru
µ(Yu) +/integraldisplayT
0u(Yu
s)·dBs+ logpXT(Yu
0)
pX0(Yu
T). (75)
In particular, the expected variational gap
G(u):= E[logpXT(Yu
0)]− E/bracketleftbig
logpX0(Yu
T)−Ru
µ(Yu)/bracketrightbig
(76)
of the ELBO in Corollary 2.5 satisfies that
G(u) =DKL( PYu| PYu∗) =DKL( PYu| P⃗X)−DKL( PYu
0| PXT). (77)
Proof.Similar to computations in Nüsken & Richter (2021), we may compute
logd PYu
d Q(Yu) = log/parenleftbiggd PYu
d PY0d PY0
d Q/parenrightbigg
(Yu) (78a)
=Ru
0(Yu) +/integraldisplayT
0u(Yu
s)·dBs+R0
µ(Yu)−logpX0(Yu
T)
pXT(Yu
0)(78b)
=Ru
µ(Yu) +/integraldisplayT
0u(Yu
s)·dBs+ logpXT(Yu
0)
pX0(Yu
T), (78c)
11This means that ZandXare governed by the same SDE, however, their initial conditions could be different.
25Published in Transactions on Machine Learning Research (02/2024)
where we used the Girsanov theorem (Theorem A.7) and (74) in (78b). This implies that
DKL( PYu| Q):= E/bracketleftbigg
logd PYu
d Q(Yu)/bracketrightbigg
= E/bracketleftbig
Ru
µ(Yu)−logpX0(Yu
T)/bracketrightbig
+ E[logpXT(Yu
0)].(79)
Comparing to Theorem 2.4, we realize that the above KL divergence is equivalent to the control costs up to
E[−V(Yu
0,0)] = E[logpXT(Yu
0)]. Using (8), we thus conclude that
DKL( PYu∗| Q) = 0 (80)
foru∗=−σ⊤∇V=σ⊤∇log⃗pX, which implies that Q= PYu∗. Together with (78), this proves the claim
in (75).
Now, we can express the expected variational gap G(u)in(76)in terms of KL divergences. We can prove
the first equality in (77)by combining the identity Q= PYu∗with(79). The second equality follows
from Lemma A.8 and the observations that Yu∗
0∼Yu
0anddYu∗= d⃗X, see Theorem A.1. This concludes
the proof.
Note that the Girsanov theorem (Theorem A.7) shows that the expected variational gap G(u) =
DKL( PYu| PYu∗)as in(77)equals the quantity derived in Remark A.6. The following proposition states the
path space measure perspective on diffusion-based generative modeling.
Proposition A.10 (Forward KL divergence) .Let us consider the inference SDE and the controlled generative
SDE as in (14), i.e.,
dYs=f(Ys,s) ds+σ(s) dBs, Y 0∼D,dXu
s=/parenleftbig
⃗σ⃗u−⃗f/parenrightbig
(Xu
s,s) ds+⃗σ(s) dBs, (81)
and the associated path space measures PYand P⃗Xu. Then the expected variational gap is given by12
G(u):= E/bracketleftbig
logpXu
T(Y0)/bracketrightbig
− E/bracketleftbig
logpXu
0(YT)−Ru
σu−f(Y)/bracketrightbig
(82a)
=DKL( PY| P⃗Xu)−DKL( PY0| PXu
T). (82b)
Proof.Let us consider the SDE
dYu,µ
s= (σu−µ)(Yu,µ
s,s) ds+σ(s) dBs, Yu,µ
0∼D. (83)
With the choice µ=σu−f, we then have Yu,σu−f=Y, which actually does not depend on uanymore,
anddYu∗,σu−f= d⃗Xu, whereu∗=σ⊤∇log⃗pXu. Together with the fact that Y0∼Yu∗,σu−f
0, Lemma A.8
implies that
DKL( PYu,σu−f| PYu∗,σu−f) =DKL( PY| P⃗Xu)−DKL( PY0| PXu
T) (84)
and Proposition A.9 (with µ=σu−f) yields the desired expression.
In practice, the expected variational gap G(u)in(82a)cannot be minimized directly since the evidence
logpXu
T(Y0)is intractable13and one resorts to maximizing the ELBO instead. The same problem occurs when
considering divergences other than the (forward) KL divergence DKLin(82b). The situation is different in the
context of sampling from densities, where we directly minimize the (reverse) KL divergence, see Corollary 3.1,
allowing us to use other divergences, such as the log-variance divergence, see Section A.14.
12In view of Lemma A.8, the variational gap can also be written as G(u) =DKL( PY| PZu), where dZu= d⃗XuandZu
0=Y0,
i.e., the process Zuis governed by the same SDE as ⃗Xu, however, has the initial condition of Y.
13One could, however, use the probability flow ODE, corresponding to the choice λ= 1in Theorem A.1, which then resembles
training of a normalizing flow.
26Published in Transactions on Machine Learning Research (02/2024)
A.9 ELBO formulations
Here we provide details on the connection of the ELBO to the denoising score matching objective. Using the
reparametrization in (14), i.e. taking µ:=σu−f, Corollary 2.5 yields that
logpXu
T(Y0)≥ E/bracketleftig
logpXu
0(YT)−Ru
σu−f(Y)/vextendsingle/vextendsingle/vextendsingleY0/bracketrightig
. (85)
The next lemma shows that the expected negative ELBO in (85)equals the denoising score matching
objective (Vincent, 2011) up to a constant, which does not depend on the control u.
Lemma A.11 (Connection to denoising score matching) .Let us define the denoising score matching objective
by
LDSM(u):=T
2E/bracketleftig/vextenddouble/vextenddoubleu(Yτ,τ)−σ⊤(τ)∇logpYτ|Y0(Yτ|Y0)/vextenddouble/vextenddouble2/bracketrightig
. (86)
Then it holds that
LDSM(u) =− E/bracketleftbig
logpXu
0(YT)−Ru
σu−f(Y)/bracketrightbig
+C, (87)
with
C:= E/bracketleftig
logpXu
0(YT) +Tdiv(f)(Yτ,τ) +T
2/vextenddouble/vextenddoubleσ⊤(τ)∇logpYτ|Y0(Yτ|Y0)/vextenddouble/vextenddouble2/bracketrightig
, (88)
whereτ∼U([0,T])andpYτ|Y0denotes the conditional density of YτgivenY0.
Proof.The proof closely follows the one in Huang et al. (2021, Appendix A). For notational convenience, let
us first define the abbreviations
p(x,s):=pYs|Y0(x|Y0)andr(x,s):=u(x,s)·/parenleftbig
σ⊤(s)∇logp(x,s)/parenrightbig
(89)
for everyx∈Rdands∈[0,T]. Note that we have
1
2/vextenddouble/vextenddoubleu−σ⊤∇logp/vextenddouble/vextenddouble2=1
2∥u∥2−r+1
2/vextenddouble/vextenddoubleσ⊤∇logp/vextenddouble/vextenddouble2, (90)
which shows that
LDSM(u) = E/bracketleftbigg
T/parenleftbigg1
2∥u∥2−r/parenrightbigg
(Yτ,τ)/bracketrightbigg
+ E/bracketleftbiggT
2/vextenddouble/vextenddoubleσ⊤(τ)∇logp(Yτ,τ)/vextenddouble/vextenddouble2/bracketrightbigg
. (91)
Focusing on the first term on the right-hand side, Fubini’s theorem and a Monte Carlo approximation establish
that, under mild regularity conditions, it holds that
E/bracketleftbigg
T/parenleftbigg1
2∥u∥2−r/parenrightbigg
(Yτ,τ)/bracketrightbigg
= E/bracketleftbig
Ru
−f(Y) +Tdiv(f)(Yτ,τ)/bracketrightbig
−/integraldisplayT
0E[r(Ys,s)] ds, (92)
where the expectation on the left-hand side is over the random variable (Yτ,τ)∼pYτ|τpτ, wherepτis the
density ofτ∼U([0,T])andpYτ|τis the conditional density of Yτgivenτ.
Together with (91), this implies that
LDSM(u) =− E/bracketleftbig
logpXu
0(YT)−Ru
−f(Y)/bracketrightbig
−/integraldisplayT
0E[r(Ys,s)] ds+C. (93)
Focusing on the term r(Ys,s)for fixeds∈[0,T], it remains to show that
E[r(Ys,s)] =− E[div(σu)(Ys,s)]. (94)
Using the identities for divergences in Section A.2, one can show that
div(σup)−div(σu)p= (σu)·∇p= (σu)·(p∇logp) =rp. (95)
27Published in Transactions on Machine Learning Research (02/2024)
Further, Stokes’ theorem guarantees that under suitable assumptions it holds that
/integraldisplay
Rddiv(σup)(x,s) dx= 0. (96)
Thus, using (95) and (96), we have that
− E[div(σu)(Ys,s)|Y0] =−/integraldisplay
Rddiv(σu)(x,s)p(x,s) dx (97a)
=/integraldisplay
Rdr(x,s)p(x,s)dx= E/bracketleftbig
r(Ys,s)/vextendsingle/vextendsingleY0/bracketrightbig
. (97b)
Combining this with (93) finishes the proof.
Note that one can also establish equivalences to explicit, implicit, and sliced score matching (Hyvärinen &
Dayan, 2005; Song et al., 2020), see Huang et al. (2021, Appendix A). Using the interpretation of the ELBO
in terms of KL divergences, see Huang et al. (2021, Theorem 5) and also Proposition A.10, one can further
derive the ELBO for discrete-time diffusion models as presented in Ho et al. (2020); Kingma et al. (2021).
A.10 Diffusion-based sampling from unnormalized densities
In the following, we formulate an optimization problem for sampling from (unnormalized) densities. The
proof and statement are similar to Theorem 2.4 and Proposition A.9 with the roles of the generative and
inference SDEs interchanged. Note that Corollary 3.1 is a direct consequence of this result.
Corollary A.12 (Reverse KL divergence) .LetXuandYbe defined by (17). Then, for every u∈Uit holds
that
logd PXu
d PXu∗(Xu) =R⃗u
⃗f(Xu) +/integraldisplayT
0⃗u(Xu
s)·dBs+ logpYT(Xu
0)
ρ(Xu
T)+ logZ, (98)
whereu∗:=σ⊤∇logpY. In particular, it holds that
DKL( PXu| P⃗Y) =DKL( PXu| PXu∗) +DKL( PXu
0| PYT) =LDIS(u) + logZ, (99)
where
LDIS(u):= E/bracketleftbigg
R⃗u
⃗f(Xu) + logpXu
0(Xu
0)
ρ(Xu
T)/bracketrightbigg
. (100)
This further implies that
DKL( PXu
0| PYT)−logZ= min
u∈ULDIS(u), (101)
and, assuming that Xu
0∼YT, the minimizing control u∗guarantees that Xu∗
T∼D.
Proof.Analogously to the proof of Lemma 2.3 and noting the interchanged roles of XuandY, we see that
log⃗pYsatisfies the HJB equation
∂tp=−Tr/parenleftbig⃗D∇2p/parenrightbig
+⃗f·∇p+ div(⃗f)p, p (·,T) = logpY0=ρ
Z. (102)
We can now proceed analogously to the proofs of Theorem 2.4 and Proposition 2.6. Using the identity
DKL( PXu
0| PYT) = E/bracketleftbigg
logpXu
0(Xu
0)
pYT(Xu
0)/bracketrightbigg
, (103)
this proves the statements in (99)and(101). The last statement follows from Theorem A.1 and the fact that
the minimizer u∗:=σ⊤∇logpYis the scaled score function.
28Published in Transactions on Machine Learning Research (02/2024)
Table 1: We compare the error of DDS (Vargas et al., 2023a) and DIS in estimating the log-normalizing
constant and the average standard deviation on two problems. We report the average error over three
independent runs. For a fair comparison, we use the same prior N(0,I), batch size m= 2048, and number
of gradient steps K= 20000, and a fixed number of integrator steps N= 256, see Table 2. For DDS, we
present the best results for different cosine schedules with αmax∈{0.25,0.5,1,1.5,2,2.5}(see Vargas et al.
(2023a) for the precise definition). For comparison, we present results in their training setup as well as ours,
i.e., with exponentially moving average of the parameters, clipping, and learning rate scheduler. For DIS, we
also present results for the log-variance divergence, see Section A.14.
Problem Method ∆ logZ(rw)↓∆std↓
GMM DDS ( α= 0.5) 0.176 2.48
DDS (α= 0.5, our setup) 0.090 2.52
DIS (KL) 0.046 2.83
DIS (log-variance) 0.013 0.01
Funnel DDS ( αmax= 0.5) 0.121 7.20
DDS (αmax= 1, our setup) 0.046 6.49
DIS (KL) 0.017 5.50
DIS (log-variance) 0.021 5.46
A.10.1 DDS as a special case of DIS
In this section, we compare the denoising diffusion sampler (DDS) suggested in Vargas et al. (2023a) to our
time-reversed diffusion sampler (DIS). We present a numerical comparison in Table 1 and refer to Richter &
Berner (2024, Section 3) and Vargas & Nüsken (2023) for further details. In particular, these works show
that DDS can be derived using a reference process with known time-reversal. In the following, we outline
how DDS can be considered a special case of DIS. To this end, we note that DDS considers the process
dXs=/parenleftbigg
−⃗β(s)Xs+ 2η2⃗β(s)/parenleftbigg
⃗Φ(Xs,s) +Xs
η2/parenrightbigg/parenrightbigg
ds+η/radicalig
2⃗β(s) dBs (104a)
=/parenleftig
⃗β(s)Xs+ 2η2⃗β(s)⃗Φ(Xs,s)/parenrightig
ds+η/radicalig
2⃗β(s) dBs, (104b)
withX0∼N(0,η2I), see also (136). To ease notation, let us define σ(t):=η/radicalbig
2β(t)I,f(x,t) =−β(t)x, and
u(x,t):=σ(t)⊤Φ(x,t)to get
dXu
s= (−⃗f+⃗σ⃗u)(Xu
s,s) ds+⃗σ(s) dBs (105)
as in(17). We added the superscript uto the process in order to indicate the dependence on the control u.
For DDS, the loss
LDDS(u):= E/bracketleftigg/integraldisplayT
01
2/vextenddouble/vextenddouble/vextenddouble/vextenddouble⃗u(Xu
s,s) +⃗σ(s)Xu
s
η2/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
ds+ logN(Xu
T; 0,η2I)
ρ(Xu
T)/bracketrightigg
(106)
is considered. In order to show the mathematical equivalence of DDS and DIS for the special choice of σand
f, we may apply Itô’s lemma to the function V(x,t) =1
2∥x∥2. This yields that
E/bracketleftbigg1
2∥Xu
T∥2−1
2∥Xu
0∥2/bracketrightbigg
= E/bracketleftigg/integraldisplayT
0/parenleftig
−⃗f+⃗σ⃗u/parenrightig
(Xu
s,s)·Xu
s+η2d⃗β(s) ds/bracketrightigg
, (107)
whereXuis defined in (105). Noting that
logN(x; 0,η2I) =−1
2η2∥x∥2−1
2log/parenleftbig
2πη2/parenrightbig
(108)
and
∇·⃗f(x,t) =∇·(−⃗β(t)x) =−d⃗β(t), (109)
29Published in Transactions on Machine Learning Research (02/2024)
we can rewrite (107) as
E/bracketleftbigg
logN(Xu
T; 0,η2I)
N(Xu
0; 0,η2I)/bracketrightbigg
= E/bracketleftigg/integraldisplayT
0/parenleftigg⃗f−⃗σ⃗u
η2/parenrightigg
(Xu
s,s)·Xu
s+ div/parenleftig
⃗f/parenrightig
(Xu
s,s) ds/bracketrightigg
. (110)
Now, we observe that the special choice of σandfimplies that
/parenleftigg⃗f−⃗σ⃗u
η2/parenrightigg
(Xu
s,s)·Xu
s=1
2∥⃗u(Xu
s,s)∥2−1
2/vextenddouble/vextenddouble/vextenddouble/vextenddouble⃗u(Xu
s,s) +⃗σ(s)Xu
s
η2/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
, (111)
which, together with (110), shows that the DDS objective in (106) can be written as
LDDS(u) = E/bracketleftbigg
R⃗u
⃗f(Xu) + logN(Xu
0; 0,η2I)
ρ(Xu
T)/bracketrightbigg
. (112)
Recalling that pXu
0=N(0,η2I), we obtain that LDDS(u) =LDIS(u), where the DIS objective is given as
in (19).
We emphasize, however, that the derivation of the DDS objective in (106)relies on a specific choice of the
drift and diffusion coefficients fandµin(106). In contrast, the DIS objective in (19)is, in principle, valid
for an arbitrary diffusion process.
A.10.2 Optimal drifts of DIS and PIS
In this section, we analyze the optimal drift for DIS and PIS in the tractable case where the target
ρ=N(m,ν2I)is a multidimensional Gaussian. This reveals that the drift of DIS can exhibit preferable
numerical properties in practice. We refer to Figures 14 and 15 for visualizations.
Using the VP SDE in Section A.13 with η= 1(as in our experiments), the optimal drift for DIS is given by
drift DIS(x,t)(17)=⃗σ(t)⃗u∗(x,t)−⃗f(t) =⃗σ2(t)e−⃗α(t)m−x
1 +e−2⃗α(t)(ν2−1)/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
=∇log⃗pY(x,t)+1
2⃗σ2(t)x, (113)
whereαis as in (138), see (137). Note that for ν= 1, we can rewrite the score as
∇log⃗pY(x,t) =e−⃗α(t)m−x= (1−e−⃗α(t))∇logpXu
0(x) +e−⃗α(t)∇logρ(x), (114)
which is reminiscent of our initialization given by a linear interpolation and, in fact, the same for the
(admittedly exotic) choice σ2(t) =2
T−t, see Section A.13.
In comparison, the optimal drift of PIS (using f= 0, i.e., a pinned Brownian motion scaled by σ∈(0,∞)
as in the original paper) can be calculated via the Feynman-Kac formula (Zhang & Chen, 2022a; Tzen &
Raginsky, 2019) and is given by
drift PIS(x,t) =σ⃗u∗(x,t) =σ2T
tν2+σ2T(T−t)/parenleftbigg
m−Tx
t/parenrightbigg
+x
t, (115)
see Vargas et al. (2023a, Appendix A.2). Due to the initial delta distribution, the drift of PIS can become
unbounded for t→0, which causes instabilities in practice. Note that this is not the case for DIS.
A.11 PDE-based methods for sampling from unnormalized densities
In principle, the different perspectives on diffusion-based generative modeling introduced in Section 2 allow
for different numerical methods. While our suggested method for sampling from (unnormalized) densities in
Section 3 directly follows from the optimal control viewpoint, an alternative route is motivated by the PDE
perspective outlined in Section 2.1. While classical numerical methods for approximating PDEs suffer from
30Published in Transactions on Machine Learning Research (02/2024)
the curse of dimensionality, we shall briefly discuss methods that may be computationally feasible even in
higher dimensions.
Let us first rewrite the PDEs from Section 2.1 in a way that makes their numerical approximation feasible.
To this end, we recall that the Fokker-Planck equation is a linear PDE and its time-reversal can be written
as a generalized Kolmogorov backward equation. Specifically, in the setting of Section 3, it holds for the
time-reversed density ⃗pYthat
∂t⃗pY= div/parenleftig
−div/parenleftig
⃗D⃗pY/parenrightig
+⃗f⃗pY/parenrightig
=−Tr/parenleftbig⃗D∇2⃗pY/parenrightbig
+⃗f·∇⃗pY+ div(⃗f)⃗pY, (116)
analogously to (5). We note that, due to the linearity of the PDE in (116), the scaled density p:=Z⃗pYalso
satisfies a Kolmogorov backward equation given by
∂tp=−Tr/parenleftbig⃗D∇2p/parenrightbig
+⃗f·∇p+ div(⃗f)p, p (·,T) =ρ. (117)
The corresponding HJB equation following from the Hopf–Cole transformation V:=−log (Z⃗pY)(as outlined
in Section A.5) equals
∂tV=−Tr(⃗D∇2V) +⃗f·∇V−div(⃗f) +1
2/vextenddouble/vextenddouble⃗σ⊤∇V/vextenddouble/vextenddouble2, V (·,T) =−logρ, (118)
in analogy to Lemma 2.3, however, with a modified terminal condition since we omitted the normalizing
constant. Now, as ρis known, viable strategies to learn u∗would be to solve the PDEs for porVvia deep
learning.
The general idea for the approximation of PDE solutions via deep learning is to define loss functionals Lsuch
that
L(p)is minimal⇐⇒pis a solution to (117), (119)
or, respectively,
L(V)is minimal⇐⇒Vis a solution to (118). (120)
This variational perspective then allows to parametrize the solution porVby a neural network and to
minimize suitable estimator versions of the loss functional Lvia gradient-based methods. We can then use
automatic differentiation to compute
u∗:=σ⊤∇logpY=σ⊤∇log⃗p=σ⊤∇⃗V (121)
as the score is invariant to rescaling. This adds an alternative to directly optimizing the control costs in (19)
in order to approximately learn u≈u∗.
A.11.1 Physics informed neural networks
A general approach to obtain suitable loss functionals Lfor learning the PDEs, dating back to the 1990s (see,
e.g., Lagaris et al. (1998)), is often referred to as Physics informed neural networks (PINNs) or deep Galerkin
method(DGM). These approaches have recently become popular for approximating solutions to PDEs via
a combination of Monte Carlo simulation and deep learning (Nüsken & Richter, 2023; Raissi et al., 2017;
Sirignano & Spiliopoulos, 2018). The idea is to minimize the squared residual of the PDE in (118)for an
approximation /tildewideV≈V, i.e.,
LPDE(/tildewideV) = E/bracketleftbigg/parenleftig
∂t/tildewideV+ Tr/parenleftig
⃗D∇2/tildewideV/parenrightig
−⃗f·∇/tildewideV+ div(⃗f)−1
2/vextenddouble/vextenddouble⃗σ⊤∇/tildewideV/vextenddouble/vextenddouble2/parenrightig2
(ξ,τ)/bracketrightbigg
, (122)
as well as the squared residual of the terminal condition, i.e.,
Lboundary (/tildewideV) = E/bracketleftbigg/parenleftig
/tildewideV(ξ,T) + logρ(ξ)/parenrightig2/bracketrightbigg
, (123)
where (ξ,τ)is a suitable random variable distributed on Rd×[0,T]. This results in a loss
LPINN(/tildewideV) =LPDE(/tildewideV) +cLboundary (/tildewideV), (124)
31Published in Transactions on Machine Learning Research (02/2024)
100 200 400 800
integrator steps7.307.357.407.457.50log() (rw)
100 200 400 800
integrator steps12.3512.4012.4512.5012.5512.60expectation |x|2DW (d=10,w=3,=2)
100 200 400 800
integrator steps9.469.489.509.529.549.56expectation |x|1DIS
PIS
PINN
ground-truth
Figure 7: We use the settings in Figures 4 and 5 and display the estimates (median and interquartile range
over 10training seeds) of DIS, PIS, and the PINN method for logZand the expected values of two choices
ofγin (148). We find that the PINN method can provide competitive results.
wherec∈(0,∞)is a suitably chosen penalty parameter. Using automatic differentiation, one can then
compute an approximation of the control as in (121). In Figure 7, we compare this approach to DIS and PIS
on a 10-dimensional double well example. While DIS provides better results overall, the PINN approach
outperforms PIS and holds promise for further development. A clear advantage of PINN is the fact that
no time-discretization is necessary. On the other hand, one needs to compute higher-order derivatives in
the PINN objective (122)during training, and one also needs to evaluate the derivative of /tildewideVto obtain the
approximated score during sampling.
A.11.2 BSDE-based methods and Feynman-Kac formula
To reduce the computational cost for the higher-order derivatives in the PINN objective, one can also develop
loss functionalsLfor our specific PDEs by considering suitable stochastic representations. This can be done
by applying Itô’s lemma to the corresponding solutions. For instance, for the HJB equation in (118), we
obtain
−logρ(XT) =V(ξ,τ) +R⃗σ⊤∇V
−⃗f(X) +SV(X), (125)
where we abbreviate
Sp(X):=/integraldisplayT
τ/parenleftbig
σ⊤∇p/parenrightbig
(Xs,s)·dBs. (126)
In the above, the stochastic process Xis given by the SDE
dXs=−⃗f(Xs,s) ds+⃗σdBs, Xτ∼ξ, (127)
where (ξ,τ)is again a suitable random variable distributed on Rd×[0,T]. Minimizing the squared residual
in (125) for an approximation /tildewideV≈V, we can thus define a viable loss functional by
LBSDE (/tildewideV):= E/bracketleftbigg/parenleftig
/tildewideV(ξ,τ) +R⃗σ⊤∇/tildewideV
−⃗f(X) +S/tildewideV(X) + logρ(XT)/parenrightig2/bracketrightbigg
, (128)
see, e.g., Richter (2021); Nüsken & Richter (2021). The name of the loss functional LBSDEstems from the
fact that (129)can also be seen as a backward stochastic differential equation (BSDE). A method to combine
the PINN loss in A.11.1 with the BSDE-based loss in (128)is presented in Nüsken & Richter (2023), leading
to the so-called diffusion loss , which allows to consider arbitrary SDE trajectory lengths.
One can argue in a similar fashion for the linear PDE in (117), where Itô’s lemma establishes that
ρ(XT) =p(ξ,τ) +/integraldisplayT
τ/parenleftig
div(⃗f)p/parenrightig
(Xs,s) ds+Sp(X). (129)
However, for the linear Kolmogorov backward equation, we can also make use of the celebrated Feynman-Kac
formula, i.e.,
p(ξ,τ) = E/bracketleftigg
exp/parenleftigg
−/integraldisplayT
τdiv(⃗f)(Xs,s) ds/parenrightigg
ρ(XT)/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle(ξ,τ)/bracketrightigg
(130)
32Published in Transactions on Machine Learning Research (02/2024)
2
 1
 0 1 2
x10.00.51.0density
2
 1
 0 1 2
x1DW (d=20,w=5,=3)
marginal
histogram of Xu
T
reweighted histogram of Xu
T
Figure 8: Illustration of importance sampling in path space. The left panel shows a histogram of the first
component of the controlled process Xu
Tat terminal time Tcompared to the corresponding marginal of the
target density ρ/Z. While we cannot correct the error caused by pXu
0≈pYT, we can mitigate the error in the
controlu≈u∗by reweighting Xu
Twith the importance weights /tildewidewu, see Section A.12. The right panel shows
that this indeed results in an improved approximation of the target density.
to establish the loss
LFK(/tildewidep) = E
/parenleftigg
/tildewidep(ξ,τ)−exp/parenleftigg
−/integraldisplayT
τdiv(⃗f)(Xs,s) ds/parenrightigg
ρ(XT)/parenrightigg2
, (131)
see, e.g., Berner et al. (2020); Richter & Berner (2022); Beck et al. (2021). Since the stochastic integral Sp(X)
has vanishing expectation conditioned on (ξ,τ), it can also be included in the FK-based loss in (131)to
reduce the variance of corresponding estimators and improve the performance, see Richter & Berner (2022).
A.12 Importance sampling in path space
In practice, we will usually not have u∗available but must rely on an approximation u≈u∗. In consequence,
this then yields samples Xu
Tthat are only approximately distributed according to the target density ρand
thus leads to biased estimates. However, we can correct for this bias by employing importance sampling.
Importance sampling is a classical method for variance-reduction in Monte Carlo approximation that leads to
unbiased estimates of corresponding quantities of interest (Liu & Liu, 2001). The idea is to sample from a
proposal distribution and correct for the corresponding bias by a likelihood ratio between the proposal and
the target distribution. While importance sampling is commonly used for densities, one can also employ it
in path space, thereby reweighting continuous trajectories of a stochastic process (Hartmann et al., 2017).
In our application, we can make use of this in order to correct for the bias introduced by the fact that
uis only an approximation of the optimal u∗. Note that, in general, it holds for any suitable functional
φ:C([0,T],Rd)→Rthat
E/bracketleftig
φ(Xu∗)/bracketrightig
= E/bracketleftbigg
φ(Xu)d PXu∗
d PXu(Xu)/bracketrightbigg
, (132)
i.e., the expectation can be computed w.r.t. samples from an optimally controlled process Xu∗, even though
the actual samples originate from the process Xu. The weights necessary for this can be calculated via
wu:=d PXu∗
d PXu(Xu) =1
Zexp/parenleftigg
−R⃗u
⃗f(Xu)−/integraldisplayT
0⃗u(Xu
s)·dBs−logpYT(Xu
0)
ρ(Xu
T)/parenrightigg
, (133)
where we used the Radon-Nikodym derivative from Proposition A.9. The identity in (132)then yields an
unbiased estimator, however, with potentially increased variance. In fact, the variance of importance sampling
estimators can scale exponentially in the dimension dand in the deviation of ufrom the optimal u∗, thereby
being very sensitive to the approximation quality of the control u, see, for instance, Hartmann & Richter
(2021).
Note that, in practice, we do not know Z, i.e., we need to consider the unnormalized weights
/tildewidewu:=wuZ. (134)
33Published in Transactions on Machine Learning Research (02/2024)
We can then make use of the identity
E/bracketleftig
φ(Xu∗)/bracketrightig
=E[φ(Xu)/tildewidew(Xu)]
E[/tildewidew(Xu)]. (135)
While for normalized weights as in (133)the variance of the corresponding importance sampling estimator
vanishes at the optimum u=u∗, this is, in general, not the case for the unnormalized weights defined in
(134).
In implementations, we introduce a bias in the importance sampling estimator by simulating the SDE Xu
using a time-discretization, such as the Euler-Maruyama scheme, see Section A.13. Moreover, the quantity
pYTin(133)is typically intractable and we need to use the approximation pXu
0≈pYTas in the lossLDIS,
see Corollary 3.1. We have illustrated the effect of importance sampling in Figure 8.
A.13 Details on implementation
In this section, we provide further details on the implementation.
SDE:For the inference SDE Ywe employ the variance-preserving (VP) SDE from Song et al. (2021) given
by
σ(t):=η/radicalbig
2β(t) Iandf(x,t):=−β(t)x (136)
withβ∈C([0,T],(0,∞))andη∈(0,∞). Note that this constitutes an Ornstein-Uhlenbeck process with
conditional density
pYt|Y0(·|Y0) =N/parenleftig
e−α(t)Y0,η2/parenleftig
1−e−2α(t)/parenrightig
I/parenrightig
, (137)
where
α(t):=/integraldisplayt
0β(s)ds. (138)
In particular, we observe that
pYT= E/bracketleftbig
pYT|Y0(·|Y0)/bracketrightbig
≈N/parenleftbig
0,η2I/parenrightbig
(139)
for suitable βand sufficiently large T. In practice, we choose the schedule
β(t):=1
2/parenleftbigg/parenleftbigg
1−t
T/parenrightbigg
σmin+t
Tσmax/parenrightbigg
(140)
withη= 1and sufficiently large 0< σ min< σ max. This motivates our choice Xu
0∼N (0,I). We can
numerically check whether (the unconditional) YTis indeed close to a standard normal distribution. To this
end, we consider samples from Dand let the process Yrun according to the inference SDE in (3). We can now
compare the empirical distributions of the different components, which each need to follow a one-dimensional
Gaussian. Figure 9 shows that this is indeed the case.
Model: Similar to PIS, we employ the initial score of the inference SDE, i.e., the score of the data
distribution,∇logpY0=∇logρ, in the parametrization of our control. Additionally, in our DIS method, we
also make use of the (tractable) initial score of the generative SDE ∇logpXu
0=∇logN(0,I). Specifically, we
choose
uθ(x,t):= Φ(1)
θ(x,t) + Φ(2)
θ(t)σ(t)s(x,t), (141)
where
s(x,t):=t
T∇logpXu
0(x) +/parenleftbigg
1−t
T/parenrightbigg
∇logρ(x) (142)
andΦ(1)
θandΦ(2)
θare neural networks with parameters θ. We use the same network architectures as PIS,
which in particular uses a Fourier feature embedding (Tancik et al., 2020) for the time variable t. We initialize
θ(0)such that Φ(1)
θ(0)≡0andΦ(2)
θ(0)≡1. The parametrization in (141)establishes that our initial control
(approximately) matches the optimal control at the initial and terminal times, i.e.,
uθ(0)(·,0) =σ(0)⊤∇logρ=σ(0)⊤∇logpY0=u∗(·,0) (143)
34Published in Transactions on Machine Learning Research (02/2024)
2
 0 2
x00.00.10.20.30.4GMM
2
 0 2
x0Funnel
2
 0 2
x1Funnelhistogram of YT
(0,1)
Figure 9: Histogram of the first component of YTfor the Gaussian mixture model and the first two coordinates
of the Funnel distribution (in blue) when starting the corresponding process YinY0∼Dand usingN= 800
steps for the simulation, compared to the density of a standard normal (in orange). We do not depict the
double well as we cannot sample from the ground truth distribution.
and
uθ(0)(·,T) =σ(T)⊤∇logpXu
0≈σ(T)⊤∇logpYT=u∗(·,T). (144)
In our experiments, we found it beneficial to detach the evaluation of sfrom the computational graph and to
use a clipping schedule for the output of the neural networks.
Training: We minimize the mapping θ∝⇕⊣√∫⊔≀→/hatwideLDIS(uθ)using a variant of gradient descent, i.e., the Adam
optimizer (Kingma & Ba, 2014), where /hatwideLDISis a Monte Carlo estimator of LDISin(19). Given that we
are now optimizing a reverse KL divergence, the expectation in LDIS(uθ)is over the controlled process Xuθ
in(17)and we need to discretize Xuθand the running costs R⃗uθ
⃗f(Xuθ)on a time grid 0 =t0<···<tN=T.
We use the Euler-Maruyama (EM) scheme given by
/hatwideXθ
n+1=/hatwideXθ
n+/parenleftig
⃗σ⃗uθ−⃗f/parenrightig
(/hatwideXθ
n,tn)∆t+⃗σ(tn) (Btn+1−Btn)/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
∼N(0,∆tI), (145)
where ∆t:=T/N =tn+1−tnis the step size. Given /hatwideXθ
0∼Xuθ
0, it can be shown that /hatwideXθ
Nconvergences
toXuθ
Tfor∆t→0in an appropriate sense (Kloeden & Platen, 1992). This motivates why we increase the
number of steps Nfor our methods when the training progresses, see Table 2. A smaller step size typically
leads to a better approximation but incurs increased computational costs. We also trained separate models
with a fixed number of steps, but did not observe benefits justifying the additional computational effort,
see Figure 10.
We compute the derivatives w.r.t. θby automatic differentiation. In a memory-restricted setting, one could
alternatively use the stochastic adjoint sensitivity method (Li et al., 2020; Kidger et al., 2021) to compute the
gradients using adaptive SDE solvers. For a given time constraint, we did not observe better performance of
stochastic adjoint sensitivity methods, higher-order (adaptive) SDE solvers, or the exponential integrator
by Zhang & Chen (2022b, Section 5) over the Euler-Maruyama scheme. Our training scheme yields better
results compared to the models obtained when using the default configuration in the code of Zhang & Chen
(2022a), see Figure 11. For our comparisons, we thus trained models for PIS according to our scheme, see
also Table 2. Note that in Section A.11 we present an alternative approach of training a control uθbased on
physics-informed neural networks (PINNs).
Sampling: In order to reduce the variance, we compute an exponential moving average ¯θof the parameters
(θ(k))K
k=1in Algorithm 1 during training and use it for evaluation. We evaluate our model by sampling from
the prior/hatwideX¯θ
0∼N(0,I)and simulating the generative SDE using the EM scheme, such that /hatwideX¯θ
Nrepresents an
approximate sample from D. Note that we can, in principle, choose the number of steps for the EM scheme
independent from the steps used for training. We used an increasing number of steps during training, and
our trained model provides good results for a range of steps, see Figures 4 and 5. Thus, we amortized the
35Published in Transactions on Machine Learning Research (02/2024)
100 200 400 800
integrator steps0.75
0.50
0.25
0.00log() (rw)
GMM
100 200 400 800
integrator steps0.2
0.1
0.00.1Funnel
100 200 400 800
integrator steps10111213DW (d=20,w=5,=4)
 PIS
DIS
ground-truth
Figure 10: We use the same setting as in Figure 4, except that for each choice of SDE integrator steps Na
separate model is trained, i.e., all gradient steps use the same step size ∆t. This yields qualitatively similar
results. For our other experiments, we thus amortize the cost and train a model which can be evaluated for a
range of steps N.
cost of training different models for different step sizes. Finally, note that similar to Song et al. (2021), one
could alternatively obtain approximate samples from Dby simulating the probability flow ODE , originating
from the choice λ= 1in Theorem A.1, using suitable ODE solvers (Zhang & Chen, 2022b).
Evaluation: We evaluate the performance of our proposed method, DIS, against PIS on the following two
metrics:
•approximating normalizing constants: We obtain a lower bound14on the log-normalizing constant
logZby evaluating the negative control costs, i.e.,
logZ≥ logZ−DKL( PXu
0| PYT)≥−L DIS(u¯θ) (146)
see(20). We note that, similar to the PIS method, adding the stochastic integral (which has zero
expectation), i.e., considering
R⃗u
⃗µ(Xu) + logpXu
0(Xu
0)
ρ(Xu
T)+/integraldisplayT
0u(Yu
s,s)·dBs (147)
yields an estimator with lower variance, see also Corollary A.12. We will compare unbiased estimates
usingimportance sampling in path space, see Section A.12.
•approximating expectations and standard deviations: In applications, one is often interested in
computing expected values of the form
Γ:= E[γ(Y0)], (148)
whereY0∼Dfollows some distribution and γ:Rd→Rspecifies an observable of interest. We
considerγ(x):=∥x∥2andγ(x):=∥x∥1=/summationtextd
i=1|xi|. We approximate (148)by creating samples
(/hatwideX¯θ,(k)
N)K
k=1from our model as described in the previous paragraph, where K∈Nspecifies the sample
size. We can then approximate Γin (148) via Monte Carlo sampling by
/hatwideΓ:=1
KK/summationdisplay
k=1γ(/hatwideX¯θ,(k)
N). (149)
Given a reference solution Γof(148), we can now compute the relative error |(/hatwideΓ−Γ)/Γ|, which can
be viewed as an evaluation of the sample quality on a global level. Analogously, we analyze the
error when approximating coordinate-wise standard deviations of the target distribution Y0using the
samples (/hatwideX¯θ,(k)
N)K
k=1.
14We note that an Euler-Maruyama discretization of −LDIS(u¯θ)in(146)can potentially overestimate the log-normalizing
constant logZ. We refer to Vargas & Nüsken (2023) for reformulations that guarantee a lower bound using backward integration.
36Published in Transactions on Machine Learning Research (02/2024)
2.0
1.5
1.0
0.5
0.0log() (rw)
GMM
0.75
0.50
0.25
0.000.25Funnel
0510DW (d=20,w=5,=4)
 ground-truth
DIS
PIS
PIS (original)
Figure 11: The boxplot compares our models to the results obtained from the repository of Zhang & Chen
(2022a) using their settings. As they only train with N= 100steps and present approximations of the
log-normalizing constants, we show corresponding results. The figure shows that our implementation of
the PIS method already provides significantly improved results. Compared to their settings, we train until
convergence with larger batch sizes, use increasingly finer step sizes, a clipping schedule, and an exponential
moving average of the parameters.
A.14 Further numerical results
In this section, we present further empirical results and comparisons.
Other divergences on path space: Our principled framework of considering path space measures
in Section 2.3 allows us to consider divergences other than the (reverse) KL divergence, which is known to
suffer from mode collapse (Minka et al., 2005; Midgley et al., 2022) or a potentially high variance of Monte
Carlo estimators (Roeder et al., 2017). We conducted preliminary experiments and want to highlight results
using the log-variance divergence that has been suggested in Nüsken & Richter (2021) and often exhibits
favorable numerical properties. In fact, we can see in Figure 6 that, for some experiments, this divergence can
improve performance significantly – sometimes by more than an order of magnitude. We, therefore, suspect
that further improvements will be possible by leveraging our path space perspective, see Richter & Berner
(2024) for an extensive comparison.
Other parametrizations and initializations: We tried a number of parametrizations and initializations
for our neural networks. Specifically, we tried to initialize the control usuch that the generative SDE Xu
1. has approximately zero drift (as in PIS),
2. corresponds to the SDE of the unadjusted Langevin algorithm (ULA),
3. has the correct drift at the initial and terminal times, i.e., the interpolation described in (141).
For Item 1, we also experimented with omitting the score ∇logρin the parametrization. We present a
comparison of the learned densities for the Funnel problem in Figure 12 and a GMM problem in Figure 13,
where we chose a non-symmetric GMM example to emphasize the different behavior. While there are
problem-specific performance differences, we observed that all the parametrizations incorporating the score
∇logρyielded qualitatively similar final results. Thus, we opted for the most stable combination, i.e., Item 3,
which is detailed in (141)above. As further benchmarks and metrics, we also present the results of running
ULA, see the next paragraph for details, and the Sinkhorn distance15(Cuturi, 2013) to ground truth samples.
Finally, we provide a comparison of the corresponding learned drifts in Figure 14 and Figure 15. We note
that the learned drifts are closer to the optimal drift for larger t∈[0,T]. It would be an interesting future
direction to try different loss weighting schemes (as have been employed for the denoising score matching
objective) to facilitate learning the score for small t.
Unadjusted Langevin Algorithm (ULA): Note that Langevin-based algorithms (which are not model-
based), such as ULA or the Metropolis-adjusted Langevin algorithm (MALA), are only guaranteed to converge
15Our implementation is based on https://github.com/fwilliams/scalable-pytorch-sinkhorn with the default parameters.
37Published in Transactions on Machine Learning Research (02/2024)
to the target distribution Din infinite time. In contrast, DIS and PIS are guaranteed to sample from the target
after finite time T(given that the learned model converged). This is particularly relevant for distributions D
for which the convergence speed of the Langevin-based algorithms is slow – which often appears if the modes
of the corresponding densities are very disconnected (or, equivalently, if the potential consists of multiple
minima, which are separated by rather high “energy barriers”). In fact, by Kramer’s law, the time to escape
such a minimum via the Langevin SDE satisfies the large deviations asymptotics E[τ]≍exp(2∆Ψ/η), where
τis the exit time from the well, ∆Ψis the energy barrier, i.e., the difference between the minimum and the
corresponding maximum in the potential function, and ηis a temperature scaling (Berglund, 2011). Here,
the crucial part is the exponential scaling in the energy barriers, which results in the Langevin diffusion
“being stuck” in the potential minima for a very long time. We visualize such behavior in the experiments
provided in Figures 12 and 13. In our GMM experiment, only three modes are sufficiently covered, even for
large terminal times, such as T= 2000, which takes roughly ten times longer than DIS or PIS to sample 6000
points on a GPU. Trying larger step sizes leads to bias and divergence, as can be seen in the last plot. We
observe similar results for the Funnel distribution, although we need smaller step sizes to even converge. We
note that one can counteract the bias by additionally using MCMC methods (e.g., MALA), but it remains
hard to tune the parameters for a given problem.
38Published in Transactions on Machine Learning Research (02/2024)
Table 2: Hyperparameters
DIS SDE
inference SDE (corresponding to Y) Variance-Preserving SDE with linear schedule (Song et al., 2021)
min. diffusivity σmin 0.1
max. diffusivity σmax 10
terminal time T 1
initial distribution Xu
0 N(0,I)(truncated to 99.99%of mass)
PIS SDE (Zhang & Chen, 2022a)
uncontrolled process X0scaled Brownian motion
drift⃗f 0(constant)
diffusivity ⃗σ√
0.2(constant)
terminal time T 5
initial distribution Dirac delta δ0at the origin
SDE Solver
type Euler-Maruyama (Kloeden & Platen, 1992)
stepsN(see also figure descriptions) [100,200,400,800](each for 1/4of the total gradient steps K)
Training
optimizer Adam (Kingma & Ba, 2014)
weight decay 10−7
learning rate 0.005
batch sizem 2048
gradient clipping 1(ℓ2-norm)
clipΦ(1)
θ,s∈[−c,c],Φ(2)
θ∈[−c,c]dc= 10(step≤200),c= 50(200<step≤400),c= 250(else)
gradient steps K 20000(d≤10),80000(else)
framework PyTorch (Paszke et al., 2019)
GPU Tesla V100 ( 32GiB)
number of seeds 10
Network fourier-emb c(Tancik et al., 2020)
dimensions [0,T]→R2c→Rc→Rc
architecture linear◦[ϱ◦linear]◦fourier-features
activation function ϱ GELU (Hendrycks & Gimpel, 2016)
Network Φ(1)
θ(Zhang & Chen, 2022a)
widthc 64(d≤10),128(else)
dimensions Rd×[0,T]→Rc×Rc→Rc→Rc→Rc→Rd
architecture linear◦[ϱ◦linear]◦[ϱ◦linear]◦[ϱ◦sum]◦[linear,fourier-emb c]
activation function GELU (Hendrycks & Gimpel, 2016)
bias initialization in last layer 0
weight initialization in last layer 0
Network Φ(2)
θ(Zhang & Chen, 2022a)
dimensions [0,T]→R64→R64→R
architecture [linear◦ϱ]◦[linear◦ϱ]◦fourier-emb 64
activation function ϱ GELU (Hendrycks & Gimpel, 2016)
bias initialization in last layer 1
weight initialization in last layer 0
Evaluation
exponentially moving average ¯θ last1500steps (updated every 5-th) with decay 1−1
1+ema _step/0.9
samplesM 6000
39Published in Transactions on Machine Learning Research (02/2024)
5
05x2GroundtruthDIS
(our setting)
Sample time=0.3s
Sinkhorn dist.=6.51
rel. [|X|1] error=0.26
DIS
(Langevin init.)
Sample time=0.3s
Sinkhorn dist.=6.52
rel. [|X|1] error=0.38
DIS
(same init. as PIS)
Sample time=0.3s
Sinkhorn dist.=6.54
rel. [|X|1] error=0.19
PIS
(our setting)
Sample time=0.3s
Sinkhorn dist.=6.64
rel. [|X|1] error=0.54
5
 0 5
x15
05x2DIS
(without log)
Sample time=0.2s
Sinkhorn dist.=6.50
rel. [|X|1] error=0.33
5
 0 5
x1PIS
(without log)
Sample time=0.2s
Sinkhorn dist.=6.74
rel. [|X|1] error=0.59
5
 0 5
x1ULA
(T=5, t=0.01)
Sample time=0.4s
Sinkhorn dist.=7.39
rel. [|X|1] error=0.55
5
 0 5
x1ULA
(T=50, t=0.01)
Sample time=2.8s
Sinkhorn dist.=7.27
rel. [|X|1] error=0.41
5
 0 5
x1ULA
(T=500, t=0.1)
Sample time=2.8s
Sinkhorn dist.=9.47
rel. [|X|1] error=2.05
Figure 12: KDE plots visualizing samples from DIS and PIS trained on the Funnel example with different
parametrizations, as described in Section A.13. For comparison, we also present results of ULA with two
different settings and the runtimes to sample 6000points on a single GPU.
5.0
2.5
0.02.55.0x2GroundtruthDIS
(our setting)
Sample time=0.4s
Sinkhorn dist.=0.07
rel. [|X|1] error=0.02
DIS
(Langevin init.)
Sample time=0.4s
Sinkhorn dist.=0.07
rel. [|X|1] error=0.02
DIS
(same init. as PIS)
Sample time=0.4s
Sinkhorn dist.=0.07
rel. [|X|1] error=0.01
PIS
(our setting)
Sample time=0.4s
Sinkhorn dist.=0.84
rel. [|X|1] error=0.16
5
 0 5
x15.0
2.5
0.02.55.0x2DIS
(without log)
Sample time=0.2s
Sinkhorn dist.=1.42
rel. [|X|1] error=0.33
5
 0 5
x1PIS
(without log)
Sample time=0.2s
Sinkhorn dist.=1.36
rel. [|X|1] error=0.34
5
 0 5
x1ULA
(T=50, t=0.1)
Sample time=1.0s
Sinkhorn dist.=0.08
rel. [|X|1] error=0.32
5
 0 5
x1ULA
(T=500, t=0.1)
Sample time=9.0s
Sinkhorn dist.=0.07
rel. [|X|1] error=0.23
5
 0 5
x1ULA
(T=2000, t=0.4)
Sample time=9.0s
Sinkhorn dist.=0.17
rel. [|X|1] error=0.18
Figure 13: The same experiment as in Figure 12 for a GMM example with means at (0,0),(0,2),(3,0),
(−4,0),(0,−5)and covariance matrix1
5I.
40Published in Transactions on Machine Learning Research (02/2024)
10
5
0510
DIS
(our setting)01
4T1
2T3
4T T
10
5
0510
DIS
(Langevin init.)
10
5
0510
DIS
(same init. as PIS)
10
5
0510
PIS
(our setting)
10
5
0510
DIS
(without log)
10
5
0510
PIS
(without log)
10
 0 1010
5
0510
ULA
10
 0 10 10
 0 10 10
 0 10 10
 0 10
Figure 14: The SDE drifts of the different methods in Figure 12 for x1,x2∈[−10,10]andx3,...,x 10= 0.
41Published in Transactions on Machine Learning Research (02/2024)
5
05
DIS
(optimal)01
4T1
2T3
4T T
5
05
DIS
(our setting)
5
05
DIS
(Langevin init.)
5
05
DIS
(same init. as PIS)
5
05
PIS
(our setting)
5
05
DIS
(without log)
5
05
PIS
(without log)
5
 0 55
05
ULA
5
 0 5 5
 0 5 5
 0 5 5
 0 5
Figure 15: The same visualization as in Figure 14 for the GMM example in Figure 13 and x1,x2∈[−8,8].
We compute the optimal drift for DIS similar to Section A.10.
42