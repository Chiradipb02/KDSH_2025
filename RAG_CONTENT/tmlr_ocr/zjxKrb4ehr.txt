Under review as submission to TMLR
Ondiffusion-basedgenerativemodelsandtheirerrorbounds:
The log-concave case with full convergence estimates
Anonymous authors
Paper under double-blind review
Abstract
We provide full theoretical guarantees for the convergence behaviour of diffusion-based gen-
erative models under the assumption of strongly log-concave data distributions while our
approximating class of functions used for score estimation is made of Lipschitz continuous
functions avoiding any Lipschitzness assumption on the score function. We demonstrate
via a motivating example, sampling from a Gaussian distribution with unknown mean, the
powerfulness of our approach. In this case, explicit estimates are provided for the associated
optimization problem, i.e. score approximation, while these are combined with the corre-
sponding sampling estimates. As a result, we obtain the best known upper bound estimates
in terms of key quantities of interest, such as the dimension and rates of convergence, for
the Wasserstein-2 distance between the data distribution (Gaussian with unknown mean)
and our sampling algorithm. Beyond the motivating example and in order to allow for the
use of a diverse range of stochastic optimizers, we present our results using an L2-accurate
score estimation assumption, which crucially is formed under an expectation with respect to
the stochastic optimizer and our novel auxiliary process that uses only known information.
This approach yields the best known convergence rate for our sampling algorithm.
1 Introduction
Diffusion-based generative models, also known as score-based generative models (SGMs) (Song & Ermon,
2019; Song et al., 2021; Sohl-Dickstein et al., 2015; Ho et al., 2020), have become over the past few years one
of the most popular approaches in generative modelling due to their empirical successes in data generation
tasks. These models have achieved state-of-the-art results in image generation (Dhariwal & Nichol, 2021;
Rombach et al., 2022), audio generation (Kong et al., 2020) and inverse problems (Chung et al., 2022; Song
et al., 2022; Cardoso et al., 2024; Boys et al., 2023) outperforming other generative models like generative
adversarial networks (GANs) (Goodfellow et al., 2014), variational autoencoders (VAEs) (Kingma & Welling,
2014), normalizing flows (Rezende & Mohamed, 2015) and energy-based methods (Zhao et al., 2017).
SGMs generate approximate data samples from high-dimensional data distributions by combining two dif-
fusion processes, a forward and a backward in time process. The former process is used to iteratively and
smoothly transform samples from the unknown data distribution into (Gaussian) noise, while the associated
backward in time process reverses the noising procedure and generates new samples from the starting un-
known data distribution. A key role in these models is played by the score function, i.e. the gradient of the
log-density of the solution of the forward process, which appears in the drift of the stochastic differential
equation (SDE) associated to the backward process. Since this quantity depends on the unknown data
distribution, an estimator of the score has to be constructed during the noising step using score-matching
techniques (Hyvärinen, 2005; Vincent, 2011). These techniques have the advantage of not suffering from
known problems of traditional pushforward generative models, such as mode collapse (Salmona et al., 2022).
The widespread applicability and success of SGMs have been accompanied by a growing interest in the
theoretical understandings of these models, particularly in their convergence theory e.g. in Bortoli (2022);
Block et al. (2020); Lee et al. (2022; 2023); Chen et al. (2023a;d); Li et al. (2024); Conforti et al. (2023);
Benton et al. (2024); Oko et al. (2023); Pedrotti et al. (2023). At its core, this new generative modeling
1Under review as submission to TMLR
approach combines optimisation and sampling procedures – specifically, the approximation of the score and
the creation of new samples, which make its theoretical analysis both an interesting and a rich challenge.
Some of the recent advances in the study of the theoretical properties of SGMs concentrate around the
sampling procedure by assuming suitable control for the score estimation procedure. For instance, the
analysis in Lee et al. (2022); Chen et al. (2023d) assumes that the score estimate is L2-accurate, meaning
that theL2error between the score and its estimate is small, and provides estimates in total variation (TV)
distance. Under the same assumption, the more recent contribution Conforti et al. (2023) establishes non-
asymptoticboundsinKullbackLeibler(KL)divergencebyassumingfiniterelativeFisherofdatadistribution
with respect to a Gaussian distribution.
The main drawback of the aforementioned L2-accurate (and in some cases L∞-accurate) score estimation
assumption is that the corresponding expectation is given with respect to density of the solution of the
forward process, which depends on the unknown data distribution.
Ourapproachintroducesanovelauxiliaryprocessthatreliessolelyonknowninformationandusesthedensity
of the solution of this process in the L2-accurate score estimation. To further highlight the powerfulness
of our approach, we present a motivating example on the case of sampling from a Gaussian distribution
with unknown mean. Full theoretical estimates for the convergence properties of the SGM are provided
in Wasserstein-2 distance, while our choice of stochastic optimizer for the score approximation is a simple
Langevin-based algorithm. Our estimates are explicit and contain the best known optimal dependencies
in terms of dimension and rate of convergence (Theorem 1). To the best of the authors’ knowledge, these
are the first such explicit results with transparent dependence on the parameters involved in the sampling
and optimization combined procedures of the diffusion models. By connecting the diffusion models with
the theoretical guarantees of machine learning optimizers via standard stochastic calculus tools, the results
in Theorem 1, together with the bounds achieved in the more general setting in Theorem 10, provide the
theoretical justification for the empirical success of the diffusion models.
We close this introductory section by highlighting some other, alternative approaches which were recently
developed. One may consult Yang & Wibisono (2023) for an approach based on the assumption that the
score estimation error has a sub-Gaussian tail. This is a stronger assumption than L2-accurate. In Chen
et al. (2023b), non-asymptotic bounds in TV and Wassertein distance of order 2 are derived when the data
distribution is supported on a low-dimensional linear subspace. Finally, convergence guarantees in TV were
developedinChenetal.(2023c)fortheprobabilityflowordinarydifferentialequation(ODE)implementation
of SGMs under the L2-accurate score estimate assumption.
Notation. Let(Ω,F,P)be a fixed probability space. We denote by E[X]the expectation of a random variable
X. For 1≤p<∞,Lpis used to denote the usual space of p-integrable real-valued random variables. The
Lp-integrability of a random variable Xis defined as E[|X|p]<∞. Fix an integer M≥1. For an RM-valued
random variable X, its law onB(RM), i.e. the Borel sigma-algebra of RMis denoted byL(X). LetT > 0
denotes some time horizon. For a positive real number a, we denote its integer part by ⌊a⌋. The Euclidean
scalar product is denoted by ⟨·,·⟩, with|·|standing for the corresponding norm (where the dimension of the
space may vary depending on the context). Let R>0:={x∈R|x>0}. Letf:RM→Rbe a continuously
differentiable function. The gradient of fis denote by∇f. For any integer q≥1, letP(Rq)be the set of
probability measures on B(Rq). Forµ,ν∈P(RM), letC(µ,ν)denote the set of probability measures ζon
B(R2M)such that its respective marginals are µandν. For anyµandν∈P(RM), the Wasserstein distance
of order 2 is defined as
W2(µ,ν) =/parenleftbigg
inf
ζ∈C(µ,ν)/integraldisplay
RM/integraldisplay
RM|x−y|2dζ(x,y)/parenrightbigg1
2
.
2 Technical Background
In this section, we provide a brief summary behind the construction of score-based generative models (SGMs)
based on diffusion introduced in Song et al. (2021). The fundamental concept behind SGMs centers around
the use of an ergodic (forward) diffusion process to diffuse the unknown data distribution πD∈P(RM)to a
known prior distribution and then learn a backward process to transform the prior to the target distribution
2Under review as submission to TMLR
πDby estimating the score function of the forward process. In our analysis, we focus on the forward process
(Xt)t∈[0,T]given by an Ornstein-Uhlenbeck (OU) process
dXt=−Xtdt+√
2dBt, X 0∼πD, (1)
where (Bt)t∈[0,T]is anM-dimensional Brownian motion and we assume that E[|X0|2]<∞. The process 1
is chosen to match the forward process in the original paper (Song et al., 2021), which is also referred to
as Variance Preserving Stochastic Differential Equation. The noising process 1 can also be represented as
follows
Xta.s.=mtX0+σtZt, mt=e−t, σ2
t= 1−e−2t, Zt∼N(0,IM), (2)
wherea.s.=denotes almost sure equality and IMdenotes the identity matrix of dimension M. Under mild
assumptions on the target data distribution πD(Haussmann & Pardoux, 1986; Cattiaux et al., 2023), the
backward process (Yt)t∈[0,T]= (XT−t)t∈[0,T]is given by
dYt= (Yt+ 2∇logpT−t(Yt))dt+√
2d¯Bt, Y 0∼L(XT), (3)
where{pt}t∈[0,T]is the family of densities of {L(Xt)}t∈(0,T]with respect to the Lebesgue measure, ¯Btis an
another Brownian motion independent of Btin 1 defined on (Ω,F,P). However, the sampling is done in
practice from the invariant distribution of the forward process, which, in this case, is a standard Gaussian
distribution. Therefore, the backward process 3 becomes
d/tildewideYt= (/tildewideYt+ 2∇logpT−t(/tildewideYt))dt+√
2d¯Bt,/tildewideY0∼π∞=N(0,IM). (4)
Here, we have /tildewideY0a.s.=ZT. SinceπDis unknown, the score function ∇logptin 3 cannot be computed exactly.
Toaddressthisissue,anestimator s(·,θ∗,·)islearnedbasedonafamilyoffunctions s: [0,T]×Rd×RM→RM
parametrized in θ, aiming at approximating the score of the ergodic (forward) diffusion process over a fixed
timewindow [0,T]. Inpractice, thefunctions sareneuralnetworksandinparticularcaseslikethemotivating
example in Section 3.1, the functions scan be wisely designed. The optimal value θ∗of the parameter θis
determined by optimizing the following score-matching objective
Rd∋θ∝⇕⊣√∫⊔≀→E/bracketleftigg/integraldisplayT
0|∇logpt(Xt)−s(t,θ,Xt)|2dt/bracketrightigg
. (5)
To account for numerical instability issues for training and sampling at t= 0as observed in practice in Song
et al. (2021, Appendix C) and for the possibility that the integral of the score function in 5 may diverge
whent= 0(see Appendix A), a discretised version of the score-matching optimization problem is usually
considered
minimize Rd∋θ∝⇕⊣√∫⊔≀→U(θ) :=/integraldisplayT
ϵκ(t)
T−ϵ/integraldisplay
RM|∇logpt(x)−s(t,θ,x )|2pt(x) dxdt, (6)
whereϵ >0andκ: [0,T]→R>0is a weighting function. The score-matching objective Uin 6 can be
rewritten via denoising score matching (Vincent, 2011) as follows
U(θ) =E[κ(τ)|σ−1
τZ+s(τ,θ,mτX0+στZ)|2] +C, (7)
wheretheexpectationisover τ∼Uniform ([ϵ,T]),X0∼πDandZ∼N(0,IM), andwhere C∈Risaconstant
independent of θ(see Appendix A for the derivation with the OU representation 30). The stochastic gradient
H:Rd×Rm→Rdof 6 deduced using 7 is given by
H(θ,x) = 2κ(t)M/summationdisplay
i=1/parenleftig
σ−1
tz(i)+s(i)(t,θ,mtx0+σtz)/parenrightig
∇θs(i)(t,θ,mtx0+σtz), (8)
where x= (t,x0,z)∈Rmwithm= 2M+ 1. As contribution to this analysis, we introduce an auxiliary
process (Yaux
t)t∈[0,T]containing the approximating function sdepending on the (random) estimator of θ∗
denoted by ˆθ, fort∈[0,T],
dYaux
t= (Yaux
t+ 2s(T−t,ˆθ,Yaux
t))dt+√
2d¯Bt, Yaux
0∼π∞=N(0,IM). (9)
3Under review as submission to TMLR
The process 9 will play an important role in the derivation of the nonasymptotic estimates in Wasserstein
distance of order two between the target data distribution and the generative distribution of the diffusion
model. Indeed, itconnectsthebackwardprocess4andthenumericalscheme11, whichfacilitatestheanalysis
of the convergence of the diffusion model (see Appendix C.2 and Appendix D.2 for more details). For this
reason, we introduce a sequence of stepsizes {γk}k∈{0,...,K}such that/summationtextK
k=0γk=T. For anyk∈{0,...,K},
lettk+1=/summationtextk
j=0γjwitht0= 0andtK+1=T. Letγk=γ∈(0,1)for eachk= 0,...,K. The discrete
process (YEM
k)k∈{0,...,K +1}of the Euler–Maruyama approximation of 9 is given, for any k∈{0,...,K}, as
follows
YEM
k+1=YEM
k+γ(YEM
k+ 2s(T−tk,ˆθ,YEM
k)) +/radicalbig
2γ¯Zk+1, YEM
0∼π∞=N(0,IM),(10)
where{¯Zk}k∈{0,...,K +1}is a sequence of independent M-dimensional Gaussian random variables with zero
mean and identity covariance matrix. We emphasize that the approximation 10 is the one chosen in the
original paper Song et al. (2021). Finally, the continuous-time interpolation of 10, for t∈[0,T], is given by
d/hatwideYEM
t= (/hatwideYEM
⌊t/γ⌋γ+ 2s(T−⌊t/γ⌋γ,ˆθ,/hatwideYEM
⌊t/γ⌋γ))dt+√
2d¯Bt,/hatwideYEM
0∼π∞=N(0,IM),(11)
whereL(/hatwideYEM
k) =L(YEM
k)at grid points for each k∈{0,...,K + 1}.
3 Main Results
Before introducing the main assumptions of the paper, we present a motivating example.
3.1 A Motivating Example: Full Estimates for Multivariate Gaussian Initial Data with Unknown Mean
In this section, we consider the case where the data distribution follows a multivariate normal distribution
withunknownmeanandidentitycovariance, i.e., X0∼πD=N(µ,Id)forsomeunknown µ∈RdwithM=d.
We show that, by using diffusion models, we are able to generate new data from an approximate distribution
that is close to πD. More precisely, we provide a non-asymptotic convergence bound with explicit constants
in Wasserstein-2 distance between the law of the diffusion model and πD, which can be made arbitrarily
small by appropriately choosing key parameters on the upper bound. In this example, the estimation of the
score function reduces to the estimation of the unknown mean by the methods of convex optimization with
optimal dependence of the dimension combined with the most efficient sampling method for high-dimensional
Gaussian data.
In this setting by using 1, we can derive the score function given by
∇logpt(x) =−x+mtµ, (12)
which can be approximated using
s(t,θ,x ) =−x+mtθ,(t,θ,x )∈[0,T]×Rd×Rd. (13)
To obtain an approximated score, i.e., to obtain an optimal value of θin 13, we opt for a popular class of
algorithms called stochastic gradient Langevin dynamics (SGLD) to solve the optimisation problem 6. In
addition, we choose the weighting function κ(t) =σ2
tas in Song & Ermon (2019) and we set ϵ= 0in 6.
Using the approximating function 13 in 8, we can obtain the following expression for the stochastic gradient
H(θ,x) = 2σ2
td/summationdisplay
i=1/parenleftig
σ−1
tz(i)+ (−mtx(i)
0−σtz(i)+mtθ(i))/parenrightig
mtei
= 2σ2
tmt/parenleftbig
σ−1
tz−mtx0−σtz+mtθ/parenrightbig
,(14)
where x= (t,x0,z)∈Rmwithm= 2d+ 1andeidenotes the unit vector with i-th entry being 1. Fixing
the so-called inverse temperature parameter β >0, the associated SGLD algorithm is given by
θλ
0:=θ0, θλ
n+1=θλ
n−λH(θλ
n,Xn+1) +/radicalbig
2λ/β ξn+1, n∈N0, (15)
4Under review as submission to TMLR
whereλ>0is often called the stepsize or gain of the algorithm, (ξn)n∈N0is a sequence of standard Gaussian
vectors and
(Xn)n∈N0= (τn,X0,n,Zn)n∈N0, (16)
is a sequence of i.i.d. random variables generated as follows. For each n∈N0, we sample τnfrom
Uniform ([0,T])such thatL(τn) =L(τ), sampleX0,nfromπD=N(µ,Id)such thatL(X0,n) =L(X0),
and sample ZnfromN(0,Id)such thatL(Zn) =L(Z). In addition, we consider the case where θ0,(ξn)n∈N0,
and(Xn)n∈N0in 15 are independent and we have E[H(θ,Xn+1)] =∇U(θ).
Throughout this section, we fix
0<λ≤min{E[σ2
τm2
τ]/(4E[σ4
τm4
τ]),1/(2E[σ2
τm2
τ])}. (17)
Table 1: Explicit expressions for the constants in Theorem 1.
Constant Full Expression
CSGLD,1 1/E[σ2
τm2
τ]
CSGLD,2 4E[σ4
τm2
τ(σ−1
τ|Z|+mτ|X0|+στ|Z|+mτ|θ∗|)2]/E[σ2
τm2
τ]
Tδ 2−1ln/parenleftig
4√
2/parenleftig/radicalbig
E[|X0|2] +√
d/parenrightig
/δ/parenrightig
βδ 144d(/radicalbig
4/3 + 2√
33)2/(δ2E[κ(τ)m2
τ])
λδmin/braceleftbig
E[σ2
τm2
τ]/(4E[σ4
τm4
τ]),1/(2E[σ2
τm2
τ]),δ2E[σ2
τm2
τ]
×(576(/radicalbig
4/3 + 2√
33)2E[σ4
τm2
τ/parenleftbig
σ−1
τ|Z|+mτ|X0|+στ|Z|+mτ|θ∗|/parenrightbig2])−1/bracerightig
nδ (λE[σ2
τm2
τ])−1ln/parenleftig
12(/radicalbig
4/3 + 2√
33)/radicalbig
E[|θ0−θ∗|2]/δ/parenrightig
with fixed λ(≤λδ)
γδ min/braceleftbig
δ/(4(18d+ 132|θ∗|2)1/2),1/2/bracerightbig
Theorem 1 states the non-asymptotic (upper) bounds between the generative distribution of the diffusion
modelL(/hatwideYEM
K+1)and the data distribution πD. An overview of the proof can be found in Appendix C.2.
Theorem 1. Under the setting described in this section, then, for any T >0andγ∈(0,1/2],
W2(L(/hatwideYEM
K+1),πD)
≤√
2e−2T(/radicalbig
E[|X0|2] +√
d)
+ (/radicalbig
4/3 + 2√
33)(e−nλE[σ2
τm2
τ]/radicalbig
E[|θ0−θ∗|2] +/radicalig
dCSGLD,1/β+/radicalbig
λCSGLD,2)
+γ(√
18d+/radicalbig
132|θ∗|2),(18)
whereCSGLD,1andCSGLD,2are given explicitly in Table 1. In addition, the result in 18 implies that for any
δ>0, if we choose T >Tδ,β≥βδ,0<λ≤λδ,n≥nδ, and 0<γ <γδ, then
W2(L(/hatwideYEM
K+1),πD)< δ,
whereTδ,βδ,λδ,nδandγδare given explicitly in Table 1.
Remark 2. The result in Theorem 1 achieves the optimal rate of convergence of order one for Euler or
Milstein schemes of SDEs with constant diffusion coefficients. Furthermore, one notes that the dependence
of the dimension on the upper bound in 18 is in the form of√
d. To the best of the authors’ knowledge,
the result in Theorem 1 is the first convergence bound where the parameters involved in the sampling and
optimization steps of the diffusion models appear explicitly. In the optimization procedure, we use SGLD
algorithm 15 to solve the problem 6. Since the stochastic gradient Hin 14 is strongly convex by Proposition
13, it has been proved, for instance in Barkhagen et al. (2021), that, for large enough β >0, the output of
SGLD is an almost minimizer of 6 when nis large. Thus, in the diffusion model, we set ˆθ=θλ
nindicating
5Under review as submission to TMLR
∇logpt(x)≈s(t,ˆθ,x) =s(t,θλ
n,x)for large values of nand for all tandx. Crucially, this allows us to use
the established convergence results for SGLD to deduce a sampling upper bound for W2(L(/hatwideYEM
K+1),πD)with
explicit constants in 18. Consequently, this bound can be controlled by any given precision level δ >0by
appropriately choosing T,β,λ,n andγ.
This motivating example has focused on exploring the convergence of diffusion-based generative models
using a Langevin-based algorithm, specifically SGLD, which is well-known for its theoretical guarantees in
achieving global convergence. However, in the general case discussed in Section 3.2, we do not prescribe a
specific optimizer to choose to minimise the distance between ˆθandθ∗.
3.2 General Case
In this section, we derive the full non-asymptotic estimates in Wasserstein distance of order two between
the target data distribution πDand the generative distribution of the diffusion model under the assumptions
stated below. As explained in Section 2 (see also Appendix A), it could be necessary in the general setting
to restrictt∈[ϵ,T]forϵ∈(0,1)in 6. Therefore, we truncate the integration in the backward diffusion at
T−ϵand run the process (Yt)t∈[0,T−ϵ].
3.2.1 Assumptions for the General Case
In the motivating example in Section 3.1, we have chosen the SGLD algorithm to solve the optimisation
problem 6. Other algorithms, such as ADAM (Kingma & Ba, 2015), TheoPouLa (Lim & Sabanis, 2024)
and stochastic gradient descent (Jentzen et al., 2021), can be chosen as long as they satisfy the following
assumption. Fix ϵ>0.
Assumption 1. Letθ∗be a minimiser1of 6 and let ˆθbe the (random) estimator of θ∗obtained through
some approximation procedure such that E[|ˆθ|4]<∞. There exists /tildewideεAL>0such that
E[|ˆθ−θ∗|2]</tildewideεAL.
Remark 3. As a consequence of Assumption 1, we have E[|ˆθ|2]<2/tildewideεAL+ 2|θ∗|2.
We consider the following assumption on the data distribution.
Assumption 2. The data distribution πDhas a finite second moment, is strongly log-concave, and/integraltextT
ϵ|∇logpt(0)|2dt<∞.
Remark 4. As a consequence of Assumption 2 and the preservation of strong log-concavity under convolu-
tion, see, e.g., Saumard & Wellner (2014, Proposition 3.7), there exists LMO: [0,T]→(0,∞]such that for
allt∈[0,T]andx,¯x∈RM, we have
⟨∇logpt(x)−∇logpt(¯x),x−¯x⟩≤−LMO(t)|x−¯x|2. (19)
The function LMO(t)in 19 has a lower bound for all t∈[0,T], which we denote by /hatwideLMO. Moreover,
Assumption 2 with the estimate 19 implies that the processes in 3 and in 4 have a unique strong solution,
see, e.g., Krylov (1991, Theorem 1).
Next, we consider the following assumption on the approximating function swhich is used in Remark 12.
Assumption 3.a. The function s: [0,T]×Rd×RM→RMis continuously differentiable in x∈RM. Let
D1:Rd×Rd→R+,D2: [0,T]×[0,T]→R+andD3: [0,T]×[0,T]→R+be such that/integraltextT
ϵ/integraltextT
ϵD2(t,¯t)dtd¯t<
∞and/integraltextT
ϵ/integraltextT
ϵD3(t,¯t)dtd¯t<∞. Forα∈/bracketleftbig1
2,1/bracketrightbig
and for all t,¯t∈[0,T],x,¯x∈RM, andθ,¯θ∈Rd, we have
that
|s(t,θ,x )−s(¯t,¯θ,¯x)|≤D1(θ,¯θ)|t−¯t|α+D2(t,¯t)|θ−¯θ|+D3(t,¯t)|x−¯x|,
1The score-matching optimization problem 6 is not necessarily (strongly) convex.
6Under review as submission to TMLR
whereD1,D2andD3have the following growth in each variable: i.e. there exist K1,K2, and K3>0such
that for each t,¯t∈[0,T]andθ,¯θ∈Rd,
|D1(θ,¯θ)|≤K1(1 +|θ|+|¯θ|),|D2(t,¯t)|≤K2(1 +|t|α+|¯t|α),
|D3(t,¯t)|≤K3(1 +|t|α+|¯t|α).
By adding a further condition on the gradient of sin Assumption 3.a we are able to achieve the optimal rate
of convergence in Theorem 10 below.
Assumption 3.b. Letsbe as in Assumption 3.a and there exists K4>0such that, for all x,¯x∈RMand
for anyk= 1,...M,
|∇xs(k)(t,θ,x )−∇ ¯xs(k)(t,θ,¯x)|≤K4(1 + 2|t|α)|x−¯x|.
Remark 5. LetKTotal :=K1+K2+K3+|s(0,0,0)|>0. Using Assumption 3.b, we have
|s(t,θ,x )|≤KTotal(1 +|t|α)(1 +|θ|+|x|).
We postpone the proof of Remark 5 to Appendix D.1.
Remark 6. Assumption 3.a and 3.b impose Lipschitz continuity on a family of approximating functions
s(·,·,·)with respect to the input variables, tandx, as well as the parameters θ. It is well-known that the
continuity properties of neural networks with respect to t, andxare largely determined by the activation
function at the last layer. For instance, neural networks with activation functions such as hyberbolic tangent
and sigmoid functions at the last layer satisfy the Lipschitz continuity with respect to tandx(Virmaux &
Scaman, 2018; Fazlyab et al., 2019).
For the following assumption on the score approximation, we let ˆθbe as in Assumption 1 and we let
(Yaux
t)t∈[0,T]be the auxiliary process defined in 9.
Assumption 4. There exists εSN>0such that
E/integraldisplayT−ϵ
0|∇logpT−r(Yaux
r)−s(T−r,ˆθ,Yaux
r)|2dr<ε SN. (20)
Remark 7. We highlight that the expectation in Assumption 4 is taken over the auxiliary process 9. This
density is known since the approximating function sand the estimator ˆθare known. To the best of authors’
knowledge, this is a novelty with respect to previous works (Bortoli, 2022; Chen et al., 2023d; Lee et al., 2022;
2023; Chen et al., 2023a; Conforti et al., 2023; Benton et al., 2024) which consider the unknown density of
the forward process (or its numerical discretization).
Remark 8. Assumption 4 is satisfied, along with Assumption 3.b, for data distributions satisfying Assump-
tion 2, beyond the multivariate Gaussian case discussed in the motivating example. Indeed,
E/integraldisplayT−ϵ
0|∇logpT−r(Yaux
r)−s(T−r,ˆθ,Yaux
r)|2dr
≤2E/integraldisplayT−ϵ
0|∇logpT−r(Yaux
r)−s(T−r,θ∗,Yaux
r)|2dr
+ 2E/integraldisplayT−ϵ
0|s(T−r,θ∗,Yaux
r)−s(T−r,ˆθ,Yaux
r)|2dr.(21)
Ifθ∗is such that s(t,θ∗,x) =∇logpt(x)as in the motivating example in Section 3.1, then the first term on
the right-hand side of 21 vanishes. Otherwise, we expect that the first term on the right-hand side above to
be small. Indeed, by the definition of strong log-concavity, see e.g. (Saumard & Wellner, 2014, Definition
2.9), we have
∇logpt(x) =∇log(g(x)) +∇log(ϕt(x)), (22)
7Under review as submission to TMLR
(a) Error with Assumption 4.
 (b) Error using U(θ)in 7.
Figure 1: The quality of generated samples with respect to (a) the error with Assumption 4 and (b) the
error obtained through denoising score matching using U(θ)in 7.
wheregis some log-concave function and ϕtis a multivariate Gaussian density. If gis a multivariate logistic
distribution2Malik & Abraham (1973), then its score function is given by
∂
∂xklog(g(x)) =−1−(M+ 1)−exp(−xk)
1 +/summationtextM
k=1exp(−xk),−∞<xk<∞, k = 1,...,M, (23)
while the Hessian of log(g(x))is bounded (see Appendix B for more details). Therefore, the score function of
the multivariate logistic distribution 23 is Lipschitz, and, as a consequence, ∇logpt(x)in 22 is still Lipschitz.
Thus, we expect to have a good control on the first term on the right-hand side of 21 since the function s
satisfying Assumption 3.b approximates a Lipschitz score function. In general, there exists a function
c(t,x) :=∇logpt(x)−s(t,θ∗,x), t> 0, x∈RM, (24)
and therefore one has to define a log-concave function gsuch that the first term on the right-hand side of
21 is small. Clearly, this is a problem specific challenge and it may not always have a good solution. The
second term on the right-hand side of 21 is controlled by Assumption 3.b and Assumption 1 (see Appendix
B for more details).
Remark 9. We conduct a numerical experiment to show the convergence of diffusion-based generative
models under Assumption 1, 2, 3.b and 4. We consider the case where X0∼πD=N(µ,Id)with
µ= (−1.2347,−0.89244)andd= 2. We use SGLD algorithm 15 to solve the optimization problem 7,
withκ(t) =σ2
t,ϵ= 0,T= 2,λ= 5×10−5,β= 1012, andn= 4×104. At each iteration, 128mini-batch
samples are used to estimate the stochastic gradient. Then, we generate samples using the Euler-Maruyama
approximation 10 with γ= 10−3andsgiven in 13. At each iteration n, we evaluate the quality of 100,000
generated samples using the Wasserstein distance of order two. In addition, we compute the L2error between
the score function and the approximated function swithθλ
n, using the auxiliary process as in Assumption
4. Figure 1 (a) demonstrates the error in Assumption 4 vanishes as the generative model converges, where
the degree of convergence is measured by W2(L(YEM
K),πD). This empirical observation justifies Assumption
4. Moreover, we explore the relationship between the quality of generated samples and the error using 7 via
denoising score matching in Figure 1 (b).
3.2.2 Full Estimates for the General Case
The main result under the general setting is stated as follows. An overview of the proof can be found in
Appendix D.2.
2The multivariate logistic distribution is an example of elliptical distribution widely used in portfolio risk management Xiao
& Valdez (2015); Owen & Rabinovitch (1983).
8Under review as submission to TMLR
Theorem 10. Let Assumptions 1, 2, 3.b and 4 hold. Then, there exist constants C1,C2,C3andC4>0
such that for any T >0andγ,ϵ∈(0,1),
W2(L(YEM
K),πD)≤C1√ϵ+C2e−2/hatwideLMO(T−ϵ)−ϵ+C3(T,ϵ)√εSN+C4(T,ϵ)γα, (25)
whereC1,C2,C3andC4are given explicitly in Table 4 (Appendix E). In addition, the result in 25 implies
that for any δ>0, if we choose 0≤ϵ<ϵδ,T >Tδ,0<εSN<εSN,δand0<γ <γδwithϵδ,Tδ,εSN,δ, and
γδgiven in Table 4, then
W2(L(YEM
K),πD)< δ.
Remark 11. The error bounds in 25 are not as good as the ones provided in Theorem 1 due to the general
form of the approximating function s. We emphasize that the optimal rate of convergence of order α∈/bracketleftbig1
2,1/bracketrightbig
for the Euler or Milstein scheme of SDEs with constant diffusion coefficients is achieved using the Lipschitz
continuity on the derivative of sin Assumption 3.b. In the explicit expression of C4in Table 4, the dependence
of the dimension is O(M)due to numerical techniques from Kumar & Sabanis (2019) used in the proof of
Theorem 10 to achieve the optimal rate of convergence.
Remark 12. If we replace Assumption 3.b with Assumption 3.a in Theorem 10, then the bound 25 becomes
W2(L(YEM
K),πD)≤C1√ϵ+C2e−2/hatwideLMO(T−ϵ)−ϵ+C3(T,ϵ)√εSN+/tildewideC4(T,ϵ)γ1/2,
whereC1,C2,C3are the same as in Theorem 10 and /tildewideC4(T,ϵ)contains a better dependence on the dimension,
namelyO(√
M), than the one achieved in Theorem 10. Although this relaxation achieves the same dependence
on the data dimension as in the motivating example in Theorem 1, it leads to a worse rate of convergence
of order 1/2.
4 Related Work and Comparison
We describe assumptions and results of various existing works in our notation and framework to facilitate
the comparisons with our results which are provided in 2-Wasserstein distance. Beyond being theoretical
relevant, the choice of the use of this metric is motivated by its applications in generative modeling. A
popular performance metric currently used to examine the quality of the images produced by generative
models is the Fréchet Inception Distance (FID) which was originally introduced in Heusel et al. (2017). This
metric measures the Fréchet distance between the distribution of generated samples and the distribution of
real samples, assuming Gaussian distributions, which is equivalent to the Wasserstein distance of order two.
Our results provided under this metric are therefore also relevant for practical applications.
We can classify the previous approaches based on their assumptions on two key quantities: (i) score ap-
proximation error and (ii) assumptions on the data distribution. Based on these approaches, we give a brief
overview of some of the most relevant recent contributions in the field.
4.1 Score Approximation Assumptions
Most of the recent analysis of score-based generative models, see, e.g., Chen et al. (2023d); Lee et al. (2022;
2023); Chen et al. (2023a); Conforti et al. (2023); Benton et al. (2024), have considered assumptions (in L2)
on the absolute error of the following type, i.e. for any k∈{0,...,N−1}, there exists ε>0
E/bracketleftig
|∇logptk(Xtk)−s(tk,ˆθ,Xtk)|2/bracketrightig
≤ε, (26)
where the expectation is taken with respect to the unknown {pt}t∈[0,T]andˆθis a deterministic quantity. In
Chen et al. (2023a); Conforti et al. (2023); Benton et al. (2024), the assumption 26 is written in integral
(averaged) form. In Lee et al. (2023), the bound in 26 is not uniform over t, i.e.εt:=ε
σ4
tand this allows
the score function to grow as1
σ2
tast→0. The authors in Conforti et al. (2023) use the relative L2-score
approximation error such as
E/bracketleftig
|2∇log ˜ptk(Xtk)−˜s(tk,ˆθ,Xtk)|2/bracketrightig
≤εE/bracketleftbig
|2∇log ˜ptk(Xtk)|2/bracketrightbig
, (27)
9Under review as submission to TMLR
where the expectation is with respect to the unknown density of the law of Xtagainst the Gaussian dis-
tributionπ∞(x), i.e. ˜pt(x) :=pt(x)/π∞(x), and ˜s(t,θ,x )in 27 is the approximating function for the score
function of ˜pt. A pointwise assumption in Bortoli (2022), given by
|∇logpt(x)−s(t,ˆθ,x)|≤C(1 +|x|)/σ2
t, (28)
forC≥0, is considered under the manifold (compact) setting. The assumption 28 takes into account the
explosive behaviour of the score function as t→0. In an attempt to obtain a weaker control than 28, the
assumption Bortoli (2022, A5) is used, namely
E/bracketleftig
|∇logpT−tk(YEI
k)−s(T−tk,ˆθ,YEI
k)|2/bracketrightig
≤C2E/bracketleftbig/parenleftbig
1 +|YEI
k|2/parenrightbig/bracketrightbig
/σ4
T−tk. (29)
We note that, unlike 26 and 27, the expectation in 29 is taken with respect to the algorithm YEI
kgiven by the
exponential integrator (EI) discretization scheme3, which has known density. However, the bounds of Bortoli
(2022, Theorem H.1) in Wasserstein distance of order one derived under this assumption scale exponentially
in problem parameters such as the diameter of the manifold Mand the inverse of the early stopping time
parameterϵ, i.e. exp(O(diam (M)2/ϵ)). Furthermore, an assumption similar to 29 is considered in a very
recent and concurrent result4Gao et al. (2023):
sup
k=1,...,K/parenleftig
E/bracketleftig
|∇logpT−(k−1)η(YEI
k)−s(T−(k−1)η,ˆθ,YEI
k)|2/bracketrightig/parenrightig1/2
≤ε,
whereη>0is the stepsize and T=KηwithK∈N.
We emphasize that the existing results in the literature do not take the expectation with respect to the
stochastic optimizer ˆθ. This, together with the use of our novel auxiliary process 9 that uses only known
information, allows us to deduce state-of-the-art bounds in the Wasserstein distance of order two in the
following sense. The bounds scale polynomially in the data dimension M, i.e.O(√
M)in the motivating
example (Theorem 1) and O(M)in the general case (Theorem 10) while achieving the optimal rate of
convergence γin both cases.
4.2 Assumptions on the Data Distribution
The vast majority of the results available in the literature are in KL divergence and TV distance. For two
general data distributions µandν, there is no known relationship between their KL divergence and their
W2. However, for strongly log-concave data distributions, a bound on the Wasserstein distance of order two
in terms of KL divergence follows from an extension of Talagrand’s inequality (Gozlan & Léonard, 2010,
Corollary 7.2).
Some convergence results in different metrics can be deduced under the following types of assumptions.
Convergence bounds in Wasserstein distance of order one with exponential complexity has been obtained
in Bortoli (2022) under the so-called manifold hypothesis, namely assuming that the target distribution
is supported on a lower-dimensional manifold or is given by some empirical distribution. Moreover, the
results in TV distance in Lee et al. (2022) and in KL divergence Yang & Wibisono (2023) assumed that
the data distribution satisfies a logarithmic Sobolev inequality and the score function is Lipschitz resulting
in convergence bounds characterized by polynomial complexity. By replacing the requirement that the
data distribution satisfies a functional inequality with the assumption that πDhas finite KL divergence
with respect to the standard Gaussian and by assuming that the score function for the forward process
is Lipschitz, the authors in Chen et al. (2023d) managed to derive bounds in TV distance which scale
polynomially in all the problem parameters. By requiring only the Lipschitzness of the score at the initial
time instead of the whole trajectory, the authors in Chen et al. (2023a, Theorem 2.5) managed to show,
using an exponentially decreasing then linear step size, convergence bounds in KL divergence with quadratic
dimensional dependence and logarithmic complexity in the Lipschitz constant. In the work by Benton et al.
3This analysis can be extended to the Euler-Maruyama numerical scheme in YEM
k10.
4The concurrent paper Gao et al. (2023) appeared few days earlier when the first draft of this work was made available
online.
10Under review as submission to TMLR
(2024), the authors provide convergence bounds in KL divergence that are linear in the data dimension up
to logarithmic factors. The authors in Conforti et al. (2023) derive bounds in KL for any data distribution
with finite Fisher information with respect to the standard Gaussian distribution and without using the
early stopping rule. Their bounds depend only on the score approximation error when πD∼N(0,IM).
We summarise the results of Chen et al. (2023a); Benton et al. (2024); Conforti et al. (2023) and compare
them to ours in Table 2 and Table 3, making the distinction based on whether the early stopping rule
is applied. A careful examination of Chen et al. (2023a, Proof of Theorem 2.2) and Benton et al. (2024,
Proof of Theorem 1 and Corollary 1) reveals that the authors require to have uniqueness of solutions for the
backward SDE 3. Therefore, additional integrability conditions on the score function depending on the type
of uniqueness of solutions considered are still needed.
We emphasize that in Theorem 10, we do not assume the score function to be Lipschitz continuous with
a uniformly bounded Lipschitz constant. This is particularly useful for future work in nonconvex set-
tings, where the upper bound estimates will be independent of the (potentially large) Lipschitz constant
of the score function, which could otherwise hide additional dimensional dependencies. The requirement/integraltextT
ϵ|∇logpt(0)|2dt<∞in Assumption 2 is weaker than the Lipschitz assumption on the score function, but
it is still difficult to verify in practical applications.
As pointed out in Chen et al. (2023d, Section 4), some type of log-concavity assumption on the data
distribution is needed to derive polynomial convergence rates in 2-Wasserstein distance. This justifies the
need for our Assumption 2 under which bounds linear in the data dimension are derived in Theorem 10.
The concurrent result Gao et al. (2023) has a similar assumption.
Table2: SummaryofpreviousboundswithoutearlystoppingandourresultinTheorem1. Boundsexpressed
in terms of the number of steps required to guarantee an error of at most δin the stated metric. The relative
Fisher information of the target πDagainst standard Gaussian measure π∞is denoted by FI (πD|π∞). All
the bounds assume that πDhas finite second moments.
Optimization
estimateRegularity condi-
tionMetric Complexity Result
None∀t,∇logptL-
Lipschitz/radicalig
KL(L(/hatwideYEI
K+1)||πD)˜O/parenleftig
ML2
δ/parenrightig
(Chen et al.,
2023a, Theo-
rem 2.1)
None FI(πD|π∞)<∞
Conforti et al.
(2023, H2)/radicalig
KL(L(/hatwideYEI
K+1)||πD)˜O/parenleftbigg/radicalig
M+E[|X0|2]
δlog2(L)/parenrightbigg
,
withL:=M−1FI(πD|π∞)(Conforti
et al., 2023,
Theorem 3)
Yes πD∼N(µ,IM)W2(L(/hatwideYEM
K+1),πD) ˜O(√
M
δ) Theorem 1
11Under review as submission to TMLR
Table 3: Summary of previous bounds with early stopping and our results in Theorem 10. Bounds expressed
in terms of the number of steps required to guarantee an error of at most δin the stated metric. The results
in (Chen et al., 2023a, Theorem 2.2) and (Benton et al., 2024, Corollary 1) are stated for the smoothed
version of the data, denoted by πϵ
D. Therefore, an additional error should be added to their bounds, as the
distance between πϵ
DandπDscales with√
MinW2(see Appendix D.2). All the bounds assume that πD
has finite second moments.
Optimization
estimateRegularity condi-
tionMetric Complexity Result
None Integrability condi-
tions on∇logptfor
the uniqueness of
(Yt)t∈[0,T]/radicalig
KL(L(/hatwideYEI
K)||πϵ
D) ˜O/parenleftig
M2log2(1/ϵ)
δ/parenrightig
+ bounds
betweenπϵ
DandπD(Chen et al.,
2023a, Theo-
rem 2.2)
None Integrability condi-
tions on∇logptfor
the uniqueness of
(Yt)t∈[0,T]/radicalig
KL(L(/hatwideYEI
K)||πϵ
D) ˜O/parenleftig
Mlog2(1/ϵ)
δ/parenrightig
+ bounds be-
tweenπϵ
DandπD(Benton
et al., 2024,
Corollary 1)
Yes Assumption 2 W2(L(/hatwideYEM
K),πD) ˜O/parenleftig
M1/αϵ1/(2α)
δ1/α/parenrightig
,
forα∈/bracketleftbig1
2,1/bracketrightbig
in Assumption
3.bTheorem 10
References
Mathias Barkhagen, Sotirios Sabanis, Ying Zhang, Huy N Chau, Éric Moulines, and Miklós Rásonyi. On
stochastic gradient Langevin dynamics with dependent data streams in the logconcave case. Bernoulli , 27
(1):1–33, 2021.
Joe Benton, Valentin De Bortoli, Arnaud Doucet, and George Deligiannidis. Nearly d-linear convergence
bounds for diffusion models via stochastic localization. In The Twelfth International Conference on Learn-
ing Representations , 2024.
Adam Block, Youssef Mroueh, and Alexander Rakhlin. Generative modeling with denoising auto-encoders
and Langevin sampling. arXiv preprint arXiv:2002.00107 , 2020.
Valentin De Bortoli. Convergence of denoising diffusion models under the manifold hypothesis. Transactions
on Machine Learning Research , 2022.
Benjamin Boys, Mark Girolami, Jakiw Pidstrigach, Sebastian Reich, Alan Mosca, and O Deniz Akyildiz.
Tweedie moment projected diffusions for inverse problems. arXiv preprint arXiv:2310.06721 , 2023.
Gabriel Cardoso, Yazid Janati El Idrissi, Eric Moulines, and Sylvain Le Corff. Monte carlo guided denoising
diffusionmodelsforBayesianlinearinverseproblems. In The Twelfth International Conference on Learning
Representations , 2024.
Patrick Cattiaux, Giovanni Conforti, Ivan Gentil, and ChristianLéonard. Time reversal ofdiffusion processes
under a finite entropy condition. In Annales de l’Institut Henri Poincaré (B) Probabilités et Statistiques ,
volume 59, pp. 1844–1881, 2023.
Hongrui Chen, Holden Lee, and Jianfeng Lu. Improved analysis of score-based generative modeling: User-
friendly bounds under minimal smoothness assumptions. In International Conference on Machine Learn-
ing, pp. 4735–4763, 2023a.
Minshuo Chen, Kaixuan Huang, Tuo Zhao, and Mengdi Wang. Score approximation, estimation and dis-
tribution recovery of diffusion models on low-dimensional data. In Proceedings of the 40th International
Conference on Machine Learning , volume 202, pp. 4672–4712, 2023b.
12Under review as submission to TMLR
Sitan Chen, Sinho Chewi, Holden Lee, Yuanzhi Li, Jianfeng Lu, and Adil Salim. The probability flow ODE
is provably fast. In Thirty-seventh Conference on Neural Information Processing Systems , 2023c.
Sitan Chen, Sinho Chewi, Jerry Li, Yuanzhi Li, Adil Salim, and Anru Zhang. Sampling is as easy as learning
the score: theory for diffusion models with minimal data assumptions. In The Eleventh International
Conference on Learning Representations , 2023d.
Hyungjin Chung, Jeongsol Kim, Michael T Mccann, Marc L Klasky, and Jong Chul Ye. Diffusion poste-
rior sampling for general noisy inverse problems. In The Eleventh International Conference on Learning
Representations , 2022.
Giovanni Conforti, Alain Durmus, and Marta G Silveri. KL convergence guarantees for score diffusion models
under minimal data assumptions. arXiv preprint arXiv:2308.12240 , 2023.
Prafulla Dhariwal and Alexander Nichol. Diffusion models beat GANs on image synthesis. Advances in
neural information processing systems , 34, 2021.
Mahyar Fazlyab, Alexander Robey, Hamed Hassani, Manfred Morari, and George Pappas. Efficient and
accurate estimation of Lipschitz constants for deep neural networks. Advances in neural information
processing systems , 32, 2019.
Xuefeng Gao, Hoang M. Nguyen, and Lingjiong Zhu. Wasserstein convergence guarantees for a general class
of score-based generative models. arXiv preprint arXiv:2311.11003 , 2023.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron
Courville, and Yoshua Bengio. Generative adversarial nets. Advances in neural information processing
systems, 27, 2014.
Nathael Gozlan and Christian Léonard. Transport inequalities. A survey. Markov Processes and Related
Fields, 16:635–736, 2010.
Ulrich G Haussmann and Etienne Pardoux. Time reversal of diffusions. The Annals of Probability , pp.
1188–1205, 1986.
Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. GANs
trained by a two time-scale update rule converge to a local Nash equilibrium. Advances in neural infor-
mation processing systems , 30, 2017.
Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural
information processing systems , 33:6840–6851, 2020.
Aapo Hyvärinen. Estimation of non-normalized statistical models by score matching. Journal of Machine
Learning Research , 6(24):695–709, 2005.
Arnulf Jentzen, Benno Kuckuck, Ariel Neufeld, and Philippe von Wurstemberger. Strong error analysis for
stochastic gradient descent optimization algorithms. IMA Journal of Numerical Analysis , 41(1):455–492,
2021.
Diederik Kingma and Jimmy Ba. ADAM: A method for stochastic optimization. In International Conference
on Learning Representations , 2015.
Diederik P Kingma and Max Welling. Auto-encoding variational Bayes. In The Eleventh International
Conference on Learning Representations , 2014.
Zhifeng Kong, Wei Ping, Jiaji Huang, Kexin Zhao, and Bryan Catanzaro. Diffwave: A versatile diffusion
model for audio synthesis. In International Conference on Learning Representations , 2020.
Nikolai V Krylov. A simple proof of the existence of a solution of Itô’s equation with monotone coefficients.
Theory of Probability & Its Applications , 35(3):583–587, 1991.
13Under review as submission to TMLR
Chaman Kumar and Sotirios Sabanis. On Milstein approximations with varying coefficients: The case of
super-linear diffusion coefficients. BIT Numerical Mathematics , 59(4):929–968, 2019.
Holden Lee, Jianfeng Lu, and Yixin Tan. Convergence for score-based generative modeling with polynomial
complexity. Advances in Neural Information Processing Systems , 35:22870–22882, 2022.
Holden Lee, Jianfeng Lu, and Yixin Tan. Convergence of score-based generative modeling for general data
distributions. In International Conference on Algorithmic Learning Theory , pp. 946–985, 2023.
Gen Li, Yuting Wei, Yuxin Chen, and Yuejie Chi. Towards non-asymptotic convergence for diffusion-based
generative models. In The Twelfth International Conference on Learning Representations , 2024.
Dong-Young Lim and Sotirios Sabanis. Polygonal unadjusted Langevin algorithms: Creating stable and
efficient adaptive algorithms for neural networks. Journal of Machine Learning Research , 25(53):1–52,
2024.
Henrick J Malik and Bovas Abraham. Multivariate logistic distributions. The Annals of Statistics , 1973.
Kazusato Oko, Shunta Akiyama, and Taiji Suzuki. Diffusion models are minimax optimal distribution
estimators. In Proceedings of the 40th International Conference on Machine Learning , volume 202, pp.
26517–26582, 2023.
Joel Owen and Ramon Rabinovitch. On the class of elliptical distributions and their applications to the
theory of portfolio choice. The Journal of Finance , 1983.
Francesco Pedrotti, Jan Maas, and Marco Mondelli. Improved convergence of score-based diffusion models
via prediction-correction. arXiv preprint arXiv:2305.14164 , 2023.
Danilo Rezende and Shakir Mohamed. Variational inference with normalizing flows. In International con-
ference on machine learning , pp. 1530–1538, 2015.
Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution
image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer
vision and pattern recognition , pp. 10684–10695, 2022.
Antoine Salmona, Valentin De Bortoli, Julie Delon, and Agnès Desolneux. Can push-forward generative
models fit multimodal distributions? Advances in Neural Information Processing Systems , 35:10766–
10779, 2022.
Adrien Saumard and Jon A Wellner. Log-concavity and strong log-concavity: A review. Statistics Surveys ,
pp. 45 – 114, 2014.
Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning
using nonequilibrium thermodynamics. In International Conference on Machine Learning , pp. 2256–2265,
2015.
Jiaming Song, Arash Vahdat, Morteza Mardani, and Jan Kautz. Pseudoinverse-guided diffusion models for
inverse problems. In International Conference on Learning Representations , 2022.
Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution. In
Advances in Neural Information Processing Systems , volume 32, 2019.
Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole.
Score-based generative modeling through stochastic differential equations. In International Conference on
Learning Representations , 2021.
Pascal Vincent. A connection between score matching and denoising autoencoders. Neural computation , 23
(7):1661–1674, 2011.
Aladin Virmaux and Kevin Scaman. Lipschitz regularity of deep neural networks: Analysis and efficient
estimation. In Advances in Neural Information Processing Systems , 2018.
14Under review as submission to TMLR
Yugu Xiao and Emiliano A Valdez. A black–litterman asset allocation model under elliptical distributions.
Quantitative Finance , 2015.
Kaylee Yingxi Yang and Andre Wibisono. Convergence of the inexact Langevin algorithm and score-based
generative models in KL divergence. arXiv preprint arXiv:2211.01512 , 2023.
Junbo Zhao, Michael Mathieu, and Yann LeCun. Energy-based generative adversarial networks. In Inter-
national Conference on Learning Representations , 2017.
Appendix
A Objective Function via Denoising Score Matching
In this section, we show that the objective function Uin 6 can be written into 7 using denoising score
matching (Vincent, 2011) and the OU representation
Xtd=mtX0+σtZ, mt=e−t, σ2
t= 1−e−2t, Z∼N(0,IM), (30)
whered=denotes equality in distribution. We start by noticing that
/integraldisplay
RM⟨∇logpt(x),s(t,θ,x )⟩pt(x)dx
=/integraldisplay
RM⟨∇pt(x), s(t,θ,x )⟩dx
=/integraldisplay
RM/angbracketleftbigg
∇x/integraldisplay
RMp0(˜x)pt|0(x|˜x)d˜x, s(t,θ,x )/angbracketrightbigg
dx
=/integraldisplay
RM/angbracketleftbigg/integraldisplay
RMp0(˜x)pt|0(x|˜x)∇xlog(pt|0(x|˜x))d˜x, s(t,θ,x )/angbracketrightbigg
dx
=/integraldisplay
RM/integraldisplay
RMpt,0(x,˜x)/angbracketleftbig
∇xlog(pt|0(x|˜x)), s(t,θ,x )/angbracketrightbig
dxd˜x,(31)
wherept|0is the density of the transition kernel associated with 1 and pt,0is the joint density of XtandX0.
Using 31 in the objective function given in 6 and the OU representation 30, we have
U(θ) =/integraldisplayT
ϵκ(t)
T−ϵ/integraldisplay
RM/parenleftbig
|∇logpt(x)|2−2⟨∇logpt(x),s(t,θ,x )⟩+|s(t,θ,x )|2/parenrightbig
pt(x)dxdt
=/integraldisplayT
ϵκ(t)
T−ϵ/integraldisplay
RM/integraldisplay
RM/parenleftigg
|∇logpt(x)|2−2/angbracketleftbig
∇xlog(pt|0(x|˜x)), s(t,θ,x )/angbracketrightbig
+|s(t,θ,x )|2/parenrightigg
pt,0(x,˜x)dxd˜xdt
=/integraldisplayT
ϵκ(t)
T−ϵ/integraldisplay
RM/integraldisplay
RM|∇xlogpt|0(x|˜x)−s(t,θ,x )|2pt,0(x,˜x)dxd˜xdt
+/integraldisplayT
ϵκ(t)
T−ϵ/integraldisplay
RM|∇logpt(x)|2pt(x)dxdt
−/integraldisplayT
ϵκ(t)
T−ϵ/integraldisplay
RM/integraldisplay
RM|∇xlogpt|0(x|˜x)|2pt,0(x,˜x)dxd˜xdt
=E[κ(τ)|σ−1
τZ+s(τ,θ,mτX0+στZ)|2] +/integraldisplayT
ϵκ(t)
T−ϵ/integraldisplay
RM|∇logpt(x)|2pt(x)dxdt
−/integraldisplayT
ϵκ(t)
T−ϵ/integraldisplay
RM/integraldisplay
RM|∇xlogpt|0(x|˜x)|2pt,0(x,˜x)dxd˜xdt.(32)
15Under review as submission to TMLR
We emphasize that we may choose to restrict t∈[ϵ,T]withϵ∈(0,1)to prevent the divergence of the
integrals on the right-hand side of 32.
B Additional Discussions about Assumption 4
In this section, we provide additional details about Remark 8, in which we used the multivariate logistic
distribution Malik & Abraham (1973)
g(x) =M! exp/parenleftigg
−M/summationdisplay
k=1xk/parenrightigg/parenleftigg
1 +M/summationdisplay
k=1exp(−xk)/parenrightigg−(M+1)
, k = 1,...,M,−∞<xk<∞,
and derived its score function in 23. The Hessian of log(g(x))is
∂2
∂x2
klog(g(x)) =−(M+ 1)/parenleftigg
exp(−xk)
(1 +/summationtextM
k=1exp(−xk))−exp(−2xk)
(1 +/summationtextM
k=1exp(−xk))2/parenrightigg
,
∂2
∂xk∂xjlog(g(x)) =−(M+ 1)/parenleftigg
exp(−xk) exp(−xj)
(1 +/summationtextM
k=1exp(−xk))2/parenrightigg
, j,k = 1,...,M, j̸=k,(33)
Since the Hessian 33 is bounded, we can conclude that the score function of the multivariate logistic distri-
bution defined in 23 is Lipschitz continuous. By using the same arguments as in Remark 8, we can conclude
that∇logpt(x)defined in 22 is Lipschitz.
Using Assumption 3.b and Assumption 1, we have
E/integraldisplayT−ϵ
0|∇logpT−r(Yaux
r)−s(T−r,ˆθ,Yaux
r)|2dr
≤2E/integraldisplayT−ϵ
0|∇logpT−r(Yaux
r)−s(T−r,θ∗,Yaux
r)|2dr+ 2K2
2/integraldisplayT−ϵ
0(1 + 2|T−r|α)2E[|ˆθ−θ∗|2] dr
≤2E/integraldisplayT−ϵ
0|∇logpT−r(Yaux
r)−s(T−r,θ∗,Yaux
r)|2dr+ 4K2
2(T−ϵ)(1 + 4T2α)/tildewideεAL<εSN,
where the error of the first term on the right-hand side above is expected to be small since the Lipschitz
∇logpt(x)is approximated by the function ssatisfying Assumption 1. This satisfies Assumption 4.
In addition, we remark that the motivating example in section 3.1 is a special case of the general setting.
Indeed, Assumption 1 is satisfied due to Lemma 14 with ˆθ=θλ
nbeing thenth-iterate of the SGLD algorithm
14 andθ∗:=µ. Assumption 2 is satisfied by the score function 12. Assumption 3.b is satisfied by the
approximating function 13 with d=M,α= 1,D1(θ,¯θ) :=|¯θ|,D2(t,¯t) :=e−t,D3(t,¯t) := 1, for anyθ,
¯θ∈Rd,t,¯t∈[0,T]andK1=K2=K3=K4= 1. Furthermore, both functions satisfy Assumption 4 with
d=M. Indeed, by Assumption 1 with θ∗=µ, we have
E/integraldisplayT−ϵ
0|∇logpT−r(Yaux
r)−s(T−r,ˆθ,Yaux
r)|2dr
≤2E/integraldisplayT−ϵ
0|∇logpT−r(Yaux
r)−s(T−r,θ∗,Yaux
r)|2dr
+ 2E/integraldisplayT−ϵ
0|s(T−r,θ∗,Yaux
r)−s(T−r,ˆθ,Yaux
r)|2dr
= (e−2ϵ−e−2T)E[|ˆθ−θ∗|2]<(e−2ϵ−e−2T)/tildewideεAL<εSN.
C Proofs of the Results for the Multivariate Gaussian Initial Data with Unknown
Mean
In this section, we provide the proof of Theorem 1. We start by introducing the results which will be used
in the proof of Theorem 1.
16Under review as submission to TMLR
C.1 Preliminary Estimates
We provide the results for the SGLD algorithm 15 with λgiven in 17, β >0,CSGLD,1andCSGLD,2given in
Table 1, as well as for the auxiliary process 9, the discrete process 10, and the continuous-time interpolation
11 withsgiven in 13 and γ∈(0,1/2].
Proposition 13. For anyθ,¯θ∈Rdandx∈Rm,
|H(θ,x)−H(¯θ,x)|= 2σ2
tm2
t|θ−¯θ|,
⟨H(θ,x)−H(¯θ,x),θ−¯θ⟩= 2σ2
tm2
t|θ−¯θ|2.
Proposition 13 is derived from the definition of the stochastic gradient 14.
The proof of the following lemmas are postponed to Section C.3.
Lemma 14. For anyn∈N0,
E/bracketleftbig
|θλ
n−θ∗|2/bracketrightbig
≤(1−2λE[σ2
τm2
τ])nE/bracketleftbig
|θ0−θ∗|2/bracketrightbig
+dCSGLD,1/β+λCSGLD,2.
As a consequence of Lemma 14, we derive the following corollary.
Corollary 15. For anyn∈N0,
E/bracketleftbig
|θλ
n|2/bracketrightbig
≤2e−2nλE[σ2
τm2
τ]E/bracketleftbig
|θ0−θ∗|2/bracketrightbig
+ 2dCSGLD,1/β+ 2λCSGLD,2+ 2|θ∗|2.
Lemma 16. It holds that
sup
t≥0E/bracketleftbig
|Yaux
t|2/bracketrightbig
≤Caux,
whereCaux:= (8/3)(e−2nλE[σ2
τm2
τ]E/bracketleftbig
|θ0−θ∗|2/bracketrightbig
+dCSGLD,1/β+λCSGLD,2+|θ∗|2) + 2d.
Lemma 17. It holds that
sup
k∈NE/bracketleftbig
|YEM
k|2/bracketrightbig
≤CEM,
whereCEM:= 3d+ 20(e−2nλE[σ2
τm2
τ]E/bracketleftbig
|θ0−θ∗|2/bracketrightbig
+dCSGLD,1/β+λCSGLD,2+|θ∗|2).
Lemma 18. For any 0<γ≤1/2, one obtains that
sup
t≥0E/bracketleftig
|/hatwideYEM
t−/hatwideYEM
⌊t/γ⌋γ|2/bracketrightig
≤γCEMose,
whereCEMose := 8d+ 56(e−2nλE[σ2
τm2
τ]E/bracketleftbig
|θ0−θ∗|2/bracketrightbig
+dCSGLD,1/β+λCSGLD,2+|θ∗|2).
Lemma 19. It holds that
sup
t≥0E/bracketleftig
|/hatwideYEM
t|2/bracketrightig
≤/hatwideCEM,
where/hatwideCEM:= 18d+ 128(e−2nλE[σ2
τm2
τ]E/bracketleftbig
|θ0−θ∗|2/bracketrightbig
+dCSGLD,1/β+λCSGLD,2+|θ∗|2).
C.2 Proof of the Main Result in the Motivating Example
Proof of Theorem 1. We derive the non-asymptotic estimate for W2(L(/hatwideYEM
K+1),πD)using the splitting
W2(L(YEM
K+1),πD)≤W2(πD,L(YtK+1)) +W2(L(YtK+1),L(/tildewideYtK+1))
+W2(L(/tildewideYtK+1),L(Yaux
tK+1)) +W2(L(Yaux
tK+1),L(YEM
K+1)).(34)
SincetK+1=T, we haveW2(πD,L(YtK+1)) = 0. We provide upper bounds on the error made by approximat-
ing the initial condition of the backward process Y0∼L(XT)with/tildewideY0∼π∞, i.e.W2(L(YtK+1),L(/tildewideYtK+1)), the
error made by approximating the score function with s, i.e.W2(L(/tildewideYtK+1),L(Yaux
tK+1)), and the discretisation
error, i.e.W2(L(Yaux
tK+1),L(YEM
K+1)), separately.
17Under review as submission to TMLR
Upper bound on W2(L(YtK+1),L(/tildewideYtK+1)).Applying Itô’s formula and using 3 and 4 with the score
function given in 12, we have, for any t∈[0,T],
d|Yt−/tildewideYt|2= 2⟨Yt−/tildewideYt,Yt−/tildewideYt+ 2(∇logpT−t(Yt)−∇logpT−t(/tildewideYt))⟩dt
=−2|Yt−/tildewideYt|2dt.(35)
Integrating and taking expectation both sides in 35 yields
E[|Yt−/tildewideYt|2] =E[|Y0−/tildewideY0|2]−2/integraldisplayt
0E[|Ys−/tildewideYs|2]ds
≤e−2tE[|Y0−/tildewideY0|2].(36)
Using 36, the representation 2 with ZTd=/tildewideY0and1−σt≤mt, we have
E[|YtK+1−/tildewideYtK+1|2]≤e−2tK+1E[|Y0−/tildewideY0|2]
=e−2tK+1E[|mTX0+ (σT−1)/tildewideY0|2]
≤2e−4tK+1/parenleftbig
E[|X0|2] +d/parenrightbig
.(37)
Using 37, we have
W2(L(YtK+1),L(/tildewideYtK+1))≤/radicalig
E[|YtK+1−/tildewideYtK+1|2]
≤√
2e−2T(/radicalbig
E[|X0|2] +√
d).(38)
Upper bound on W2(L(/tildewideYtK+1),L(Yaux
tK+1)).Applying Itô’s formula and using the process 4 with the score
function 12 and the process 9 with the approximating function 13, we have, for any t∈[0,T],
d|/tildewideYt−Yaux
t|2= 2⟨/tildewideYt−Yaux
t,/tildewideYt−Yaux
t+ 2(∇logpT−t(/tildewideYt)−s(T−t,ˆθ,Yaux
t))⟩dt
=−2|/tildewideYt−Yaux
t|2dt+ 4⟨/tildewideYt−Yaux
t,mT−t(µ−ˆθ)⟩dt.(39)
Integrating and taking expectation on both sides in 39 and using that the minimiser θ⋆=µ, we have
E[|/tildewideYt−Yaux
t|2] =−2/integraldisplayt
0E[|/tildewideYs−Yaux
s|2]ds+ 4/integraldisplayt
0E[⟨/tildewideYs−Yaux
s,mT−s(θ∗−ˆθ)⟩]ds. (40)
Differentiating both sides of 40 and using Young’s inequality, we obtain
d
dtE[|/tildewideYt−Yaux
t|2] =−2E[|/tildewideYt−Yaux
t|2] + 4E[⟨/tildewideYt−Yaux
t,mT−t(θ∗−ˆθ)⟩]
≤−E[|/tildewideYt−Yaux
t|2] + 4e−2Te2tE[|θ∗−ˆθ|2],
which implies that
d
dt(etE[|/tildewideYt−Yaux
t|2])≤4e−2Te3tE[|θ∗−ˆθ|2].
Integrating both sides and using Lemma 14 yields
E[|/tildewideYt−Yaux
t|2]
≤(4/3)e−2T(e2t−e−t)E[|θ∗−ˆθ|2]
≤(4/3)e−2(T−t)(e−2nλE[σ2
τm2
τ]E[|θ0−θ∗|2] +dCSGLD,1/β+λCSGLD,2).(41)
Using 41, we have
W2(L(/tildewideYtK+1),L(Yaux
tK+1))≤/radicalig
E[|/tildewideYtK+1−Yaux
tK+1|2]
≤/radicalbig
4/3(e−2nλE[σ2
τm2
τ]E[|θ0−θ∗|2] +dCSGLD,1/β+λCSGLD,2)1/2.(42)
18Under review as submission to TMLR
Upper bound on W2(L(Yaux
tK+1),L(/hatwideYEM
K+1)).Applying Itô’s formula and using the processes 9 and 11 with
the approximating function sgiven in 13, we have, for any t∈[0,T],
d|Yaux
t−/hatwideYEM
t|2= 2⟨Yaux
t−/hatwideYEM
t,Yaux
t−/hatwideYEM
⌊t/γ⌋γ⟩dt
+ 4⟨Yaux
t−/hatwideYEM
t,s(T−t,ˆθ,Yaux
t)−s(T−⌊t/γ⌋γ,ˆθ,/hatwideYEM
⌊t/γ⌋γ)⟩dt
=−2⟨Yaux
t−/hatwideYEM
t,Yaux
t−/hatwideYEM
⌊t/γ⌋γ⟩dt
+ 4⟨Yaux
t−/hatwideYEM
t,(mT−t−mT−⌊t/γ⌋γ)ˆθ⟩dt.(43)
Integrating both sides and taking expectation in 43 yields
E/bracketleftig
|Yaux
t−/hatwideYEM
t|2/bracketrightig
=−2/integraldisplayt
0E/bracketleftig
|Yaux
s−/hatwideYEM
s|2/bracketrightig
ds−2/integraldisplayt
0E/bracketleftig/angbracketleftig
Yaux
s−/hatwideYEM
s,/hatwideYEM
s−/hatwideYEM
⌊s/γ⌋γ/angbracketrightig/bracketrightig
ds
+ 4/integraldisplayt
0E/bracketleftig/angbracketleftig
Yaux
s−/hatwideYEM
s,(mT−s−mT−⌊s/γ⌋γ)ˆθ/angbracketrightig/bracketrightig
ds.(44)
Differentiating both sides in 44, using Young’s inequality and mT−t−mT−⌊t/γ⌋γ≤γmT−t, we have
d
dtE[|Yaux
t−/hatwideYEM
t|2] =−2E[|Yaux
t−/hatwideYEM
t|2]−2E/bracketleftigg/angbracketleftigg
Yaux
t−/hatwideYEM
t,√
2/integraldisplayt
⌊t/γ⌋γdBs/angbracketrightigg/bracketrightigg
−2E/bracketleftigg/angbracketleftigg
Yaux
t−/hatwideYEM
t,/integraldisplayt
⌊t/γ⌋γ(−/hatwideYEM
⌊s/γ⌋γ+ 2mT−⌊s/γ⌋γˆθ)ds/angbracketrightigg/bracketrightigg
+ 4E[⟨Yaux
t−/hatwideYEM
t,(mT−t−mT−⌊t/γ⌋γ)ˆθ⟩]
≤−E[|Yaux
t−/hatwideYEM
t|2]−2E/bracketleftigg/angbracketleftigg
Yaux
t−/hatwideYEM
t,√
2/integraldisplayt
⌊t/γ⌋γdBs/angbracketrightigg/bracketrightigg
+ 2γ2E[(−/hatwideYEM
⌊t/γ⌋γ+ 2mT−⌊t/γ⌋γˆθ)2] + 8γ2e−2(T−t)E[|ˆθ|2]
≤−E[|Yaux
t−/hatwideYEM
t|2]−2E/bracketleftigg/angbracketleftigg
Yaux
t−/hatwideYEM
t,√
2/integraldisplayt
⌊t/γ⌋γdBs/angbracketrightigg/bracketrightigg
+ 4γ2E[|/hatwideYEM
⌊t/γ⌋γ|2] + 24γ2E[|ˆθ|2].(45)
Wederiveanupperboundforthesecondtermontheright-handsidein45. UsingCauchy-Schwarzinequality,
Itô formula applied to tBtand Young’s inequality, we have
−2E/bracketleftigg/angbracketleftigg
Yaux
t−/hatwideYEM
t,√
2/integraldisplayt
⌊t/γ⌋γdBs/angbracketrightigg/bracketrightigg
=−2E/bracketleftigg/angbracketleftigg
(Yaux
t−Yaux
⌊t/γ⌋γ)−(/hatwideYEM
t−/hatwideYEM
⌊t/γ⌋γ),√
2/integraldisplayt
⌊t/γ⌋γdBs/angbracketrightigg/bracketrightigg
=−2E/bracketleftigg/angbracketleftigg/integraldisplayt
⌊t/γ⌋γ(−Yaux
s+ 2mT−sˆθ)ds,√
2/integraldisplayt
⌊t/γ⌋γdBs/angbracketrightigg/bracketrightigg
+ 2E/bracketleftigg/angbracketleftigg/integraldisplayt
⌊t/γ⌋γ(−/hatwideYEM
⌊s/γ⌋γ+ 2mT−⌊s/γ⌋γˆθ)ds,√
2/integraldisplayt
⌊t/γ⌋γdBs/angbracketrightigg/bracketrightigg
19Under review as submission to TMLR
= 2E/bracketleftigg/angbracketleftigg/integraldisplayt
⌊t/γ⌋γ(Yaux
s−/hatwideYEM
⌊s/γ⌋γ)ds,√
2/integraldisplayt
⌊t/γ⌋γdBs/angbracketrightigg/bracketrightigg
= 2E/bracketleftigg/angbracketleftigg/integraldisplayt
⌊t/γ⌋γ(Yaux
s−Yaux
⌊s/γ⌋γ)ds,√
2/integraldisplayt
⌊t/γ⌋γdBs/angbracketrightigg/bracketrightigg
= 2E/bracketleftigg/angbracketleftigg/integraldisplayt
⌊t/γ⌋γ/integraldisplays
⌊s/γ⌋γ(−Yaux
ν)dνds,√
2/integraldisplayt
⌊t/γ⌋γdBs/angbracketrightigg/bracketrightigg
+ 2E/bracketleftigg/angbracketleftigg/integraldisplayt
⌊t/γ⌋γ√
2/integraldisplays
⌊s/γ⌋γdBνds,√
2/integraldisplayt
⌊t/γ⌋γdBs/angbracketrightigg/bracketrightigg
≤2√
2
E
/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/integraldisplayt
⌊t/γ⌋γ/integraldisplays
⌊s/γ⌋γ(−Yaux
ν)dνds/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle2

1/2
E
/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/integraldisplayt
⌊t/γ⌋γdBs/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle2

1/2
+ 4E/bracketleftigg/angbracketleftigg
t/integraldisplayt
⌊t/γ⌋γdBs−/integraldisplayt
⌊t/γ⌋γsdBs,/integraldisplayt
⌊t/γ⌋γdBs/angbracketrightigg/bracketrightigg
≤√
2γ5/2/parenleftbigg
sup
t≥0E/bracketleftbig
|Yaux
t|2/bracketrightbig
+d/parenrightbigg
+ 2dγ2. (46)
Substituting 46 into 45 yields
d
dtE[|Yaux
t−/hatwideYEM
t|2]≤−E[|Yaux
t−/hatwideYEM
t|2] +√
2γ5/2sup
t≥0E[|Yaux
t|2]
+√
2dγ5/2+ 2dγ2+ 4γ2E[|/hatwideYEM
⌊t/γ⌋γ|2] + 24γ2E[|ˆθ|2].
Thus,
d
dt(etE[|Yaux
t−/hatwideYEM
t|2])≤etγ2/parenleftbigg√
2 sup
t≥0E[|Yaux
t|2] + 3d+ 4E[|/hatwideYEM
⌊t/γ⌋γ|2] + 24E[|ˆθ|2]/parenrightbigg
.(47)
Integrating both sides in 47 and using Lemma 16, Lemma 17 and Corollary 15, yield
E[|Yaux
t−/hatwideYEM
t|2]
≤√
2γ2(2d+ (8/3)(e−2nλE[σ2
τm2
τ]E[|θ0−θ∗|2] +dCSGLD,1/β+λCSGLD,2+|θ∗|2))
+ 3dγ2+ 4γ2(3d+ 20(e−2nλE[σ2
τm2
τ]E[|θ0−θ∗|2] +dCSGLD,1/β+λCSGLD,2+|θ∗|2))
+ 48γ2(e−2nλE[σ2
τm2
τ]E[|θ0−θ∗|2] +dCSGLD,1/β+λCSGLD,2+|θ∗|2)
≤18dγ2+ 132γ2(e−2nλE[σ2
τm2
τ]E[|θ0−θ∗|2] +dCSGLD,1/β+λCSGLD,2+|θ∗|2).(48)
By using 48, we have
W2(L(Yaux
tK+1),L(/hatwideYEM
K+1))
≤/radicalig
E[|Yaux
tK+1−/hatwideYEM
tK+1|2]
≤γ(18d+ 132(e−2nλE[σ2
τm2
τ]E[|θ0−θ∗|2] +dCSGLD,1/β+λCSGLD,2+|θ∗|2))1/2.(49)
20Under review as submission to TMLR
Final upper bound on W2(L(/hatwideYEM
K+1),πD).Substituting 38, 42, 49 into 34 yields
W2(L(/hatwideYEM
K+1),πD)
≤√
2e−2T(/radicalbig
E[|X0|2] +√
d)
+/radicalbig
4/3(e−2nλE[σ2
τm2
τ]E[|θ0−θ∗|2] +dCSGLD,1/β+λCSGLD,2)1/2
+γ(18d+ 132(e−2nλE[σ2
τm2
τ]E[|θ0−θ∗|2] +dCSGLD,1/β+λCSGLD,2+|θ∗|2))1/2
≤√
2e−2T(/radicalbig
E[|X0|2] +√
d)
+ (/radicalbig
4/3 + 2√
33)(e−2nλE[σ2
τm2
τ]E[|θ0−θ∗|2] +dCSGLD,1/β+λCSGLD,2)1/2
+γ(18d+ 132|θ∗|2)1/2.(50)
The bound for W2(L(/hatwideYEM
K+1),πD)in 50 can be made arbitrarily small by appropriately choosing parameters
includingT,β,λ,n andγ. More precisely, for any δ >0, we first choose T >TδwithTδgiven explicitly in
Table 1 such that√
2e−2T(/radicalbig
E[|X0|2] +√
d)<δ/4. (51)
Next, we choose β≥βδand0<λ≤λδwithβδandλδgiven explicitly in Table 1, and, in the case where
λ=λδ, we choose n≥nδwithnδgiven explicitly in Table 1 such that
(/radicalbig
4/3 + 2√
33)(e−2nλE[σ2
τm2
τ]E[|θ0−θ∗|2] +dCSGLD,1/β+λCSGLD,2)1/2
≤(/radicalbig
4/3 + 2√
33)/radicalbig
d/(βE[σ2τm2τ])
+ (/radicalbig
4/3 + 2√
33)(4λE[σ4
τm2
τ(σ−1
τ|Z|+mτ|X0|+στ|Z|+mτ|θ∗|)2]/(E[σ2
τm2
τ]))1/2
+ (/radicalbig
4/3 + 2√
33)e−nλE[σ2
τm2
τ]/radicalbig
E[|θ0−θ∗|2]
≤δ/12 +δ/12 +δ/12 =δ/4.(52)
Finally, we choose 0<γ <γδwithγδgiven explicitly in Table 1 such that
γ(18d+ 132|θ∗|2)1/2<δ/4. (53)
Using 51, 52 and 53 in 50, we obtain W2(L(/hatwideYEM
K+1),πD)<δ.
C.3 Proof of the Preliminary Results
We provide the proofs of the results of Appendix C.1.
Proof of Lemma 14. By 15, we have for any n∈N0,
|θλ
n+1−θ∗|2=/vextendsingle/vextendsingle/vextendsingleθλ
n−θ∗−λH(θλ
n,Xn+1) +/radicalbig
2λ/β ξn+1/vextendsingle/vextendsingle/vextendsingle2
=/vextendsingle/vextendsingleθλ
n−θ∗/vextendsingle/vextendsingle2+ 2/angbracketleftig
θλ
n−θ∗,−λH(θλ
n,Xn+1) +/radicalbig
2λ/β ξn+1/angbracketrightig
+/vextendsingle/vextendsingle/vextendsingle−λH(θλ
n,Xn+1) +/radicalbig
2λ/βξn+1/vextendsingle/vextendsingle/vextendsingle2
=/vextendsingle/vextendsingleθλ
n−θ∗/vextendsingle/vextendsingle2−2λ/angbracketleftbig
θλ
n−θ∗,H(θλ
n,Xn+1)−H(θ∗,Xn+1)/angbracketrightbig
−2λ/angbracketleftbig
θλ
n−θ∗,H(θ∗,Xn+1)/angbracketrightbig
+ 2/radicalbig
2λ/β/angbracketleftbig
θλ
n−θ∗,ξn+1/angbracketrightbig
+λ2/vextendsingle/vextendsingleH(θλ
n,Xn+1)/vextendsingle/vextendsingle2
−2λ/radicalbig
2λ/β/angbracketleftbig
H(θλ
n,Xn+1),ξn+1/angbracketrightbig
+ (2λ/β)|ξn+1|2.(54)
Taking conditional expectation on both sides in 54 yields
E/bracketleftbig
|θλ
n+1−θ∗|2|θλ
n/bracketrightbig
=/vextendsingle/vextendsingleθλ
n−θ∗/vextendsingle/vextendsingle2−2λE/bracketleftbig/angbracketleftbig
θλ
n−θ∗,H(θλ
n,Xn+1)−H(θ∗,Xn+1)/angbracketrightbig
|θλ
n/bracketrightbig
+ 2λ2E/bracketleftig/vextendsingle/vextendsingleH(θλ
n,Xn+1)−H(θ∗,Xn+1)/vextendsingle/vextendsingle2|θλ
n/bracketrightig
+ 2λ2E/bracketleftig
|H(θ∗,Xn+1)|2|θλ
n/bracketrightig
+ 2λd/β.
21Under review as submission to TMLR
Recall that 0< λ≤min{E[σ2
τm2
τ]/(4E[σ4
τm4
τ]),1/(2E[σ2
τm2
τ])}. Using Proposition 13 and the stochastic
gradient 14, we have
E/bracketleftbig
|θλ
n+1−θ∗|2|θλ
n/bracketrightbig
≤/parenleftbig
1−2λE/bracketleftbig
σ2
τm2
τ/bracketrightbig/parenrightbig
|θλ
n−θ∗|2+ 2λd/β
−2λE/bracketleftbig
σ2
τm2
τ/bracketrightbig
|θλ
n−θ∗|2+ 8λ2E/bracketleftbig
σ4
τm4
τ/bracketrightbig/vextendsingle/vextendsingleθλ
n−θ∗/vextendsingle/vextendsingle2
+ 8λ2E/bracketleftig
σ4
τm2
τ/parenleftbig
σ−1
τ|Z|+mτ|X0|+στ|Z|+mτ|θ∗|/parenrightbig2/bracketrightig
≤/parenleftbig
1−2λE/bracketleftbig
σ2
τm2
τ/bracketrightbig/parenrightbig
|θλ
n−θ∗|2+ 2λd/β
+ 8λ2E/bracketleftig
σ4
τm2
τ/parenleftbig
σ−1
τ|Z|+mτ|X0|+στ|Z|+mτ|θ∗|/parenrightbig2/bracketrightig
.
This implies that
E/bracketleftbig
|θλ
n+1−θ∗|2/bracketrightbig
≤/parenleftbig
1−2λE/bracketleftbig
σ2
τm2
τ/bracketrightbig/parenrightbign+1E/bracketleftbig
|θ0−θ∗|2/bracketrightbig
+dCSGLD,1/β+λCSGLD,2.
Proof of Lemma 16. Applying Itô’s formula and using the process 9 with sgiven in 13, we have, for any
t∈[0,T],
d|Yaux
t|2=−2|Yaux
t|2dt+ 2⟨Yaux
t,2mT−tˆθ⟩dt+ 2⟨Yaux
t,√
2dBt⟩+ 2ddt.
Integrating both sides and taking expectation, we have
E/bracketleftbig
|Yaux
t|2/bracketrightbig
=−2/integraldisplayt
0E/bracketleftbig
|Yaux
s|2/bracketrightbig
ds+ 2/integraldisplayt
0E[⟨Yaux
s,2mT−sˆθ⟩]ds+E/bracketleftbig
|Yaux
0|2/bracketrightbig
+ 2dt.
Then, differentiating both sides, we have
d
dtE/bracketleftbig
|Yaux
t|2/bracketrightbig
=−2E/bracketleftbig
|Yaux
t|2/bracketrightbig
+ 2E[⟨Yaux
t,2mT−tˆθ⟩] + 2d
≤−E/bracketleftbig
|Yaux
t|2/bracketrightbig
+ 4m2
T−tE[|ˆθ|2] + 2d,
which, by rearranging the terms, yields
d
dt(etE/bracketleftbig
|Yaux
t|2/bracketrightbig
)≤4e−2Te3tE[|ˆθ|2] + 2det.
Integrating both side and using Corollary 15, we obtain
E/bracketleftbig
|Yaux
t|2/bracketrightbig
≤e−t(E/bracketleftbig
|Yaux
0|2/bracketrightbig
+ (4/3)e−2T(e3t−1)E[|ˆθ|2] + 2d(et−1))
≤d(2−e−t) + (4/3)e−2T(e2t−e−t)E[|ˆθ|2]
≤2d+e−2(T−t)(8/3)(e−2nλE[σ2
τm2
τ]E[|θ0−θ∗|2] +dCSGLD,1/β+λCSGLD,2+|θ∗|2).
Proof of Lemma 17. Using the process 10 with the approximating function sgiven in 13, we have
|YEM
k+1|2=|YEM
k+γ(−YEM
k+ 2mT−kˆθ)|2+ 2γ|¯Zk+1|2
+ 2⟨YEM
k+γ(−YEM
k+ 2mT−kˆθ),/radicalbig
2γ¯Zk+1⟩.(55)
22Under review as submission to TMLR
Taking conditional expectation on both sides in 55, using the independence of YEM
kand ¯Zk+1, Young’s
inequality and 0<γ≤1/2, we obtain
E/bracketleftbig
|YEM
k+1|2|YEM
k/bracketrightbig
=|YEM
k|2+ 2⟨YEM
k,γ(−YEM
k+ 2mT−tkˆθ)⟩
+γ2|YEM
k|2+ 4γ2m2
T−k|ˆθ|2−2γ2⟨YEM
k,2mT−tkˆθ⟩+ 2γd
= (1−2γ)|YEM
k|2+ 4γ2m2
T−k|ˆθ|2+ 2γd
+γ2|YEM
k|2+ 2γ(1−γ)⟨YEM
k,2mT−kˆθ⟩
≤(1−γ)|YEM
k|2+ 4γ(γ+ 2)m2
T−k|ˆθ|2+ 2γd
−γ|YEM
k|2+γ2|YEM
k|2+ (γ/2)|YEM
k|2
≤(1−γ)|YEM
k|2+ 4γ(γ+ 2)|ˆθ|2+ 2γd.
Thus,
E/bracketleftbig
|YEM
k+1|2/bracketrightbig
≤(1−γ)k+1d+ 10|ˆθ|2+ 2d. (56)
Using Corollary 15 in 56 yields
E/bracketleftbig
|YEM
k|2/bracketrightbig
≤(1−γ)kd+ 2d+ 20(e−2nλE[σ2
τm2
τ]E/bracketleftbig
|θ0−θ∗|2/bracketrightbig
+dCSGLD,1/β+λCSGLD,2+|θ∗|2).
Proof of Lemma 18. Using the process 11 with the approximating function sgiven in 13, Lemma 17, Corol-
lary 15 and γ∈(0,1/2], we have, for any t∈[0,T],
E/bracketleftig
|/hatwideYEM
t−/hatwideYEM
⌊t/γ⌋γ|2/bracketrightig
=E
/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/integraldisplayt
⌊t/γ⌋γ(−/hatwideYEM
⌊s/γ⌋γ+ 2mT−⌊s/γ⌋γˆθ)ds+√
2/integraldisplayt
⌊t/γ⌋γdBs/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle2

≤γ2E/bracketleftbigg/vextendsingle/vextendsingle/vextendsingle−/hatwideYEM
⌊t/γ⌋γ+ 2mT−⌊t/γ⌋γˆθ/vextendsingle/vextendsingle/vextendsingle2/bracketrightbigg
+ 2dγ
≤2γ2E/bracketleftig
|/hatwideYEM
⌊t/γ⌋γ|2/bracketrightig
+ 8γ2E/bracketleftig
|ˆθ|2/bracketrightig
+ 2dγ
≤6γ2d+ 40γ2(e−2nλE[σ2
τm2
τ]E/bracketleftbig
|θ0−θ∗|2/bracketrightbig
+dCSGLD,1/β+λCSGLD,2+|θ∗|2)
+ 16γ2(e−2nλE[σ2
τm2
τ]E/bracketleftbig
|θ0−θ∗|2/bracketrightbig
+dCSGLD,1/β+λCSGLD,2+|θ∗|2) + 2dγ
≤γCEMose,
whereCEMose = 8d+ 56(e−2nλE[σ2
τm2
τ]E/bracketleftbig
|θ0−θ∗|2/bracketrightbig
+dCSGLD,1/β+λCSGLD,2+|θ∗|2).
Proof of Lemma 19. Applying Itô’s formula to the process 11 with the approximating function sgiven in
13, we have, for any t∈[0,T],
d|/hatwideYEM
t|2= 2⟨/hatwideYEM
t,−/hatwideYEM
⌊t/γ⌋γ+ 2mT−⌊t/γ⌋γˆθ⟩dt+ 2⟨/hatwideYEM
t,√
2dBt⟩+ 2ddt
=−2|/hatwideYEM
t|2dt+ 2⟨/hatwideYEM
t,/hatwideYEM
t−/hatwideYEM
⌊t/γ⌋γ⟩dt+ 2⟨/hatwideYEM
t,2mT−⌊t/γ⌋γˆθ⟩dt
+ 2⟨/hatwideYEM
t,√
2dBt⟩+ 2ddt.
Integrating both sides and taking expectation, we have
E/bracketleftig
|/hatwideYEM
t|2/bracketrightig
=−2/integraldisplayt
0E/bracketleftig
|/hatwideYEM
s|2/bracketrightig
ds+ 2/integraldisplayt
0E/bracketleftig
⟨/hatwideYEM
s,/hatwideYEM
s−/hatwideYEM
⌊s/γ⌋γ⟩/bracketrightig
ds
+ 2/integraldisplayt
0E/bracketleftig
⟨/hatwideYEM
s,2mT−⌊s/γ⌋γˆθ⟩/bracketrightig
ds+E/bracketleftig
|/hatwideYEM
0|2/bracketrightig
+ 2dt.
23Under review as submission to TMLR
Then, differentiating both sides and using Young’s inequality, yield
d
dtE/bracketleftig
|/hatwideYEM
t|2/bracketrightig
=−2E/bracketleftig
|/hatwideYEM
t|2/bracketrightig
+ 2E/bracketleftig
⟨/hatwideYEM
t,/hatwideYEM
t−/hatwideYEM
⌊t/γ⌋γ⟩/bracketrightig
+ 2E/bracketleftig
⟨/hatwideYEM
t,2mT−⌊t/γ⌋γˆθ⟩/bracketrightig
+ 2d
≤−E/bracketleftig
|/hatwideYEM
t|2/bracketrightig
+ 2E/bracketleftig
|/hatwideYEM
t−/hatwideYEM
⌊t/γ⌋γ|2/bracketrightig
+ 8E[|ˆθ|2] + 2d.
Using Lemma 18, we have
d
dt/parenleftig
etE/bracketleftig
|/hatwideYEM
t|2/bracketrightig/parenrightig
≤et(2γCEMose + 8E[|ˆθ|2] + 2d).
Integrating both sides, using γ∈(0,1/2]and Corollary 15 yields
E/bracketleftig
|/hatwideYEM
t|2/bracketrightig
≤e−td+ (1−e−t)(2γCEMose + 8E[|ˆθ|2] + 2d)
≤18d+ 128(e−2nλE[σ2
τm2
τ]E/bracketleftbig
|θ0−θ∗|2/bracketrightbig
+dCSGLD,1/β+λCSGLD,2+|θ∗|2).
D Proofs of the Results in the General Case
In this section, we provide the proof of Theorem 10. We start by introducing the results which will be used
in the proof of Theorem 10.
D.1 Preliminary Estimates for the General Case
Throughout this section, we fix ϵ∈(0,1). The following auxiliary results will be used in the proof of Theorem
10 and their proofs are postponed to Appendix D.3.
We provide an upper bound for the moments of (/hatwideYEM
t)t∈[0,T]defined in 11.
Lemma 20. Let Assumptions 1 and 3.b hold. For any p∈[2,4]andt∈[0,T−ϵ],
sup
0≤s≤tE/bracketleftig
|/hatwideYEM
s|p/bracketrightig
≤CEM,p(t),
where
CEM,p(t) :=et(3p−1−2
p+22p−1Kp
Total(1+Tαp))
×/parenleftbigg
E/bracketleftig
|/hatwideYEM
0|p/bracketrightig
+ 23p−2Kp
Totalt(1 +E[|ˆθ|p])(1 +Tαp) +2
p(pM+p(p−2))p
2t/parenrightbigg
.
The following result provides an estimate for the one step error associated with (/hatwideYEM
t)t∈[0,T]defined in 11.
Lemma 21. Let Assumptions 1 and 3.b hold. For any p∈[2,4]andt∈[0,T−ϵ],
E/bracketleftig
|/hatwideYEM
t−/hatwideYEM
⌊t/γ⌋γ|p/bracketrightig
≤γp
2CEMose,p,
where
CEMose,p:= 2p−1(CEM,p(T) +Kp
Total(1 +Tαp)(23p−2CEM,p(T) + 24p−3(1 +E[|ˆθ|p])))
+ (Mp(p−1))p
2.
The following result is a modification of Kumar & Sabanis (2019, Lemma 4.1).
24Under review as submission to TMLR
Lemma 22. Let Assumption 3.b hold and let b: [0,T]×Rd×RM→RMsuch that
b(t,θ,x ) :=x+ 2s(t,θ,x ). (57)
Then, for any x,¯x∈RM,t∈[0,T],θ∈Rd,α∈[1
2,1], andk= 1,...M,
/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleb(k)(t,θ,x )−b(k)(t,θ,¯x)−M/summationdisplay
i=1∂b(k)(t,θ,¯x)
∂yi(xi−¯xi)/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle≤K4(1 + 2|t|α)|x−¯x|2.
Lemma 23. Let Assumption 3.b hold and let bbe as in 57. Then, one obtains that, for any (t,θ,x )∈
[0,T]×Rd×RMandk= 1,...M,
|∇xb(k)(t,θ,x )|≤1 + 2K3(1 + 2|t|α). (58)
D.2 Proof of the Main Result for the General Case
Proof of Theorem 10. We proceed as in the proof of Theorem 1 using the splitting
W2(L(YEM
K),πD)≤W2(πD,L(YtK)) +W2(L(YtK),L(/tildewideYtK))
+W2(L(/tildewideYtK),L(Yaux
tK)) +W2(L(Yaux
tK),L(YEM
K)),(59)
where the first term on the right-hand side in 59 corresponds to the error made by the early stopping and
the remaining terms have the same interpretation of the corresponding ones in 34.
Upper bound on W2(πD,L(YtK)).For anyt∈[0,T], note that
1−mt≤σt, σ2
t= 1−e−2t≤2t. (60)
Recall that tK=T−ϵ. Using the representation of the OU process 2 and the inequalities 60, we have
W2(πD,L(YtK)) =W2(πD,L(XT−tK))
≤/radicalbig
E[|X0−XT−tK|2]
≤√
2/bracketleftig
(1−mT−tK)/radicalbig
E[|X0|2] +σT−tK√
M/bracketrightig
≤√
2σT−tK(/radicalbig
E[|X0|2] +√
M)
≤2√ϵ(/radicalbig
E[|X0|2] +√
M).(61)
Upper bound on W2(L(YtK),L(/tildewideYtK)).Using Itô’s formula, we have, for any t∈[0,T−ϵ],
d|Yt−/tildewideYt|2= 2⟨Yt−/tildewideYt,Yt+ 2∇logpT−t(Yt)−/tildewideYt−2∇logpT−t(/tildewideYt)⟩dt
= 2|Yt−/tildewideYt|2dt+ 4⟨Yt−/tildewideYt,∇logpT−t(Yt)−∇logpT−t(/tildewideYt)⟩dt.(62)
By integrating, taking expectations on both sides in 62, and using Remark 4 with the lower bound /hatwideLMOin
the estimate 19, we have
E[|YtK−/tildewideYtK|2]≤E[|Y0−/tildewideY0|2] +/integraldisplaytK
02(1−2/hatwideLMO)E/bracketleftig
|Yt−/tildewideYt|2/bracketrightig
dt
≤E[|Y0−/tildewideY0|2]e2(1−2/hatwideLMO)tK.(63)
Using 63, the representation 2 with ZTd=/tildewideY0and 60, we have
E[|YtK−/tildewideYtK|2]≤E[|Y0−/tildewideY0|2]e2(1−2/hatwideLMO)tK
=E[|mTX0+ (σT−1)/tildewideY0|2]e2(1−2/hatwideLMO)tK
≤2/parenleftbig
E[|X0|2] +M/parenrightbig
e2(1−2/hatwideLMO)tK−2T.(64)
25Under review as submission to TMLR
Using 64, we have
W2(L(YtK),L(/tildewideYtK))≤/radicalig
E[|YtK−/tildewideYtK|2]
≤√
2(/radicalbig
E[|X0|2] +√
M)e−2/hatwideLMO(T−ϵ)−ϵ.(65)
Upper bound on W2(L(/tildewideYtK),L(Yaux
tK)).Using Itô’s formula, we have, for t∈[0,T−ϵ],
d|/tildewideYt−Yaux
t|2= 2⟨/tildewideYt−Yaux
t,/tildewideYt+ 2∇logpT−t(/tildewideYt)−Yaux
t−2s(T−t,ˆθ,Yaux
t)⟩dt
= 2|/tildewideYt−Yaux
t|2dt+ 4⟨/tildewideYt−Yaux
t,∇logpT−t(/tildewideYt)−∇logpT−t(Yaux
t)⟩dt
+ 4⟨/tildewideYt−Yaux
t,∇logpT−t(Yaux
t)−s(T−t,ˆθ,Yaux
t)⟩dt.(66)
By integrating and taking the expectation on both sides in 66, and using the Remark 4 with the lower bound
/hatwideLMOin the estimate 19, Young’s inequality with ζ∈(0,1)and Assumption 4, we have
E[|/tildewideYT−ϵ−Yaux
T−ϵ|2] = 2/integraldisplayT−ϵ
0E[|/tildewideYs−Yaux
s|2]ds
+ 4/integraldisplayT−ϵ
0E[⟨/tildewideYs−Yaux
s,∇logpT−s(/tildewideYs)−∇logpT−s(Yaux
s)⟩]ds
+ 4/integraldisplayT−ϵ
0E[⟨/tildewideYs−Yaux
s,∇logpT−s(Yaux
s)−s(T−s,ˆθ,Yaux
s)⟩]ds
≤/integraldisplayT−ϵ
02(1 +ζ−2/hatwideLMO)E[|/tildewideYs−Yaux
s|2]ds
+ 2ζ−1/integraldisplayT−ϵ
0E[|∇logpT−s(Yaux
s)−s(T−s,ˆθ,Yaux
s)|2]ds
≤/integraldisplayT−ϵ
02(1 +ζ−2/hatwideLMO)E[|/tildewideYs−Yaux
s|2]ds+ 2ζ−1εSN
≤2e2(1+ζ−2/hatwideLMO)(T−ϵ)ζ−1εSN.(67)
Using 67 and tK=T−ϵ, we have
W2(L(/tildewideYtK),L(Yaux
tK))≤/radicalig
E[|/tildewideYtK−Yaux
tK|2]
≤/radicalbig
2ζ−1e(1+ζ−2/hatwideLMO)(T−ϵ)√εSN.(68)
Upper bound on W2(L(Yaux
tK),L(YEM
K)).The following bound is derived by modifying Kumar & Sabanis
(2019, Lemma 4.7). For the sake of presentation, let b: [0,T]×Rd×RM→RMsuch that
b(t,θ,x ) =x+ 2s(t,θ,x ). (69)
Consequently, (/hatwideYEM
t)t∈[0,T]can be expressed using 69 as
d/hatwideYEM
t=b(T−⌊t/γ⌋γ,ˆθ,/hatwideYEM
⌊t/γ⌋γ)dt+√
2d¯Bt,/hatwideYEM
0∼π∞=N(0,IM). (70)
Using Itô’s formula, we have, for t∈[0,T−ϵ],
d|Yaux
t−/hatwideYEM
t|2
= 2⟨Yaux
t−/hatwideYEM
t,Yaux
t+ 2s(T−t,ˆθ,Yaux
t)−/hatwideYEM
⌊t/γ⌋γ−2s(T−⌊t/γ⌋γ,ˆθ,/hatwideYEM
⌊t/γ⌋γ)⟩dt
= 2|Yaux
t−/hatwideYEM
t|2dt+ 4⟨Yaux
t−/hatwideYEM
t,s(T−t,ˆθ,Yaux
t)−s(T−t,ˆθ,/hatwideYEM
t)⟩dt
+ 4⟨Yaux
t−/hatwideYEM
t,s(T−t,ˆθ,/hatwideYEM
⌊t/γ⌋γ)−s(T−⌊t/γ⌋γ,ˆθ,/hatwideYEM
⌊t/γ⌋γ)⟩dt
+ 2⟨Yaux
t−/hatwideYEM
t,b(T−t,ˆθ,/hatwideYEM
t)−b(T−t,ˆθ,/hatwideYEM
⌊t/γ⌋γ))⟩dt.(71)
26Under review as submission to TMLR
Integrating and taking the expectation on both sides in 71, using Cauchy–Schwarz inequality, Young’s
inequality with ζ∈(0,1), Assumption 3.b and Remark 3, yield
E/bracketleftig
|Yaux
t−/hatwideYEM
t|2/bracketrightig
= 2/integraldisplayt
0E/bracketleftig
|Yaux
s−/hatwideYEM
s|2/bracketrightig
ds+ 4/integraldisplayt
0E/bracketleftig
⟨Yaux
s−/hatwideYEM
s,s(T−s,ˆθ,Yaux
s)−s(T−s,ˆθ,/hatwideYEM
s)⟩/bracketrightig
ds
+ 4/integraldisplayt
0E/bracketleftig
⟨Yaux
s−/hatwideYEM
s,s(T−s,ˆθ,/hatwideYEM
⌊s/γ⌋γ)−s(T−⌊s/γ⌋γ,ˆθ,/hatwideYEM
⌊s/γ⌋γ)⟩/bracketrightig
ds
+ 2/integraldisplayt
0E/bracketleftig
⟨Yaux
s−/hatwideYEM
s,b(T−s,ˆθ,/hatwideYEM
s)−b(T−s,ˆθ,/hatwideYEM
⌊s/γ⌋γ)⟩/bracketrightig
ds
≤/integraldisplayt
02(1 +ζ+ 2K3(1 + 2|T−s|α))E/bracketleftig
|Yaux
s−/hatwideYEM
s|2/bracketrightig
ds
+ 2ζ−1/integraldisplayt
0E/bracketleftig
|s(T−s,ˆθ,/hatwideYEM
⌊s/γ⌋γ)−s(T−⌊s/γ⌋γ,ˆθ,/hatwideYEM
⌊s/γ⌋γ)|2/bracketrightig
ds
+ 2/integraldisplayt
0E/bracketleftig
⟨Yaux
s−/hatwideYEM
s,b(T−s,ˆθ,/hatwideYEM
s)−b(T−s,ˆθ,/hatwideYEM
⌊s/γ⌋γ))⟩/bracketrightig
ds
≤/integraldisplayt
02(1 +ζ+ 2K3(1 + 2Tα))E/bracketleftig
|Yaux
s−/hatwideYEM
s|2/bracketrightig
ds
+ 2ζ−1K2
1E/bracketleftig
(1 + 2|ˆθ|)2/bracketrightig/integraldisplayt
0|s−⌊s/γ⌋γ|2αds
+ 2/integraldisplayt
0E/bracketleftig
⟨Yaux
s−/hatwideYEM
s,b(T−s,ˆθ,/hatwideYEM
s)−b(T−s,ˆθ,/hatwideYEM
⌊s/γ⌋γ))⟩/bracketrightig
ds
≤/integraldisplayt
02(1 +ζ+ 2K3(1 + 2Tα))E/bracketleftig
|Yaux
s−/hatwideYEM
s|2/bracketrightig
ds+ 4ζ−1K2
1(1 + 8(/tildewideεAL+|θ∗|2))γ2αt
+ 2/integraldisplayt
0E/bracketleftig
⟨Yaux
s−/hatwideYEM
s,b(T−s,ˆθ,/hatwideYEM
s)−b(T−s,ˆθ,/hatwideYEM
⌊s/γ⌋γ)⟩/bracketrightig
ds.(72)
We proceed estimating the third term on the right-hand side of 72. Using 70, Young’s inequality with
ζ∈(0,1), Lemma 22 and Lemma 21, yields, for any t∈[0,T−ϵ]
/integraldisplayt
0E/bracketleftig
⟨Yaux
s−/hatwideYEM
s,b(T−s,ˆθ,/hatwideYEM
s)−b(T−s,ˆθ,/hatwideYEM
⌊s/γ⌋γ)⟩/bracketrightig
ds
=M/summationdisplay
k=1E/bracketleftigg/integraldisplayt
0/parenleftig
Yaux,(k)
s−/hatwideYEM,(k)
s/parenrightig/parenleftigg
b(k)(T−s,ˆθ,/hatwideYEM
s)−b(k)(T−s,ˆθ,/hatwideYEM
⌊s/γ⌋γ)
−M/summationdisplay
j=1∂b(k)(T−s,ˆθ,/hatwideYEM
⌊s/γ⌋γ)
∂xj(/hatwideYEM,(j)
s−/hatwideYEM,(j)
⌊s/γ⌋γ)/parenrightigg
ds/bracketrightigg
+M/summationdisplay
k=1E
/integraldisplayt
0/parenleftig
Yaux,(k)
s−/hatwideYEM,(k)
s/parenrightig
M/summationdisplay
j=1∂b(k)(T−s,ˆθ,/hatwideYEM
⌊s/γ⌋γ)
∂xj(/hatwideYEM,(j)
s−/hatwideYEM,(j)
⌊s/γ⌋γ)
ds

≤ζ
2/integraldisplayt
0E/bracketleftig
|Yaux
s−/hatwideYEM
s|2/bracketrightig
ds
+1
2ζM/summationdisplay
k=1E/bracketleftigg/integraldisplayt
0|b(k)(T−s,ˆθ,/hatwideYEM
s)−b(k)(T−s,ˆθ,/hatwideYEM
⌊s/γ⌋γ)
27Under review as submission to TMLR
−M/summationdisplay
j=1∂b(k)(T−s,ˆθ,/hatwideYEM
⌊s/γ⌋γ)
∂xj/parenleftig
/hatwideYEM,(j)
s−/hatwideYEM,(j)
⌊s/γ⌋γ/parenrightig
|2ds/bracketrightigg
+M/summationdisplay
k=1E/bracketleftigg/integraldisplayt
0/parenleftig
Yaux,(k)
s−/hatwideYEM,(k)
s/parenrightig
×
M/summationdisplay
j=1∂b(k)(T−s,ˆθ,/hatwideYEM
⌊s/γ⌋γ)
∂xj/integraldisplays
⌊s/γ⌋γb(j)(T−⌊r/γ⌋γ,ˆθ,/hatwideYEM
⌊r/γ⌋γ) dr
ds/bracketrightigg
+M/summationdisplay
k=1E
/integraldisplayt
0/parenleftig
Yaux,(k)
s−/hatwideYEM,(k)
s/parenrightig
M/summationdisplay
j=1∂b(k)(T−s,ˆθ,/hatwideYEM
⌊s/γ⌋γ)
∂xj/integraldisplays
⌊s/γ⌋γ√
2 d¯B(j)
r
ds

≤ζ
2/integraldisplayt
0E/bracketleftig
|Yaux
s−/hatwideYEM
s|2/bracketrightig
ds+K2
4
2ζ/integraldisplayt
0(1 + 2|T−s|α)2E/bracketleftig
|/hatwideYEM
s−/hatwideYEM
⌊s/γ⌋γ|4/bracketrightig
ds
+M/summationdisplay
k=1E/bracketleftigg/integraldisplayt
0/parenleftig
Yaux,(k)
s−/hatwideYEM,(k)
s/parenrightig
×
M/summationdisplay
j=1∂b(k)(T−s,ˆθ,/hatwideYEM
⌊s/γ⌋γ)
∂xj/integraldisplays
⌊s/γ⌋γb(j)(T−⌊r/γ⌋γ,ˆθ,/hatwideYEM
⌊r/γ⌋γ) dr
ds/bracketrightigg
+M/summationdisplay
k=1E
/integraldisplayt
0/parenleftig
Yaux,(k)
s−/hatwideYEM,(k)
s/parenrightig
M/summationdisplay
j=1∂b(k)(T−s,ˆθ,/hatwideYEM
⌊s/γ⌋γ)
∂xj/integraldisplays
⌊s/γ⌋γ√
2 d¯B(j)
r
ds

≤ζ
2/integraldisplayt
0E/bracketleftig
|Yaux
s−/hatwideYEM
s|2/bracketrightig
ds+γ2K2
4
ζt(1 + 4T2α)CEMose,4
+M/summationdisplay
k=1E/bracketleftigg/integraldisplayt
0/parenleftig
Yaux,(k)
s−/hatwideYEM,(k)
s/parenrightig
×
M/summationdisplay
j=1∂b(k)(T−s,ˆθ,/hatwideYEM
⌊s/γ⌋γ)
∂xj/integraldisplays
⌊s/γ⌋γb(j)(T−⌊r/γ⌋γ,ˆθ,/hatwideYEM
⌊r/γ⌋γ) dr
ds/bracketrightigg
+M/summationdisplay
k=1E
/integraldisplayt
0/parenleftig
Yaux,(k)
s−/hatwideYEM,(k)
s/parenrightig
M/summationdisplay
j=1∂b(k)(T−s,ˆθ,/hatwideYEM
⌊s/γ⌋γ)
∂xj/integraldisplays
⌊s/γ⌋γ√
2 d¯B(j)
r
ds
.(73)
We proceed estimating the third and fourth term on the right-hand side of 73, separately.
The third term is estimated using Young’s inequality with ζ∈(0,1), Lemma 23, Remark 5, Remark 3 and
Lemma 20, and for any t∈[0,T−ϵ], we obtain that
M/summationdisplay
k=1E/bracketleftigg/integraldisplayt
0/parenleftig
Yaux,(k)
s−/hatwideYEM,(k)
s/parenrightig
×
M/summationdisplay
j=1∂b(k)(T−s,ˆθ,/hatwideYEM
⌊s/γ⌋γ)
∂xj/integraldisplays
⌊s/γ⌋γb(j)(T−⌊r/γ⌋γ,ˆθ,/hatwideYEM
⌊r/γ⌋γ)dr
ds/bracketrightigg
≤ζ
2/integraldisplayt
0E/bracketleftig
|Yaux
s−/hatwideYEM
s|2/bracketrightig
ds
+2γ
ζM(1 + 8 K2
3(1 + 4T2α))E
/integraldisplayt
0/integraldisplays
⌊s/γ⌋γM/summationdisplay
j=1|b(j)(T−⌊r/γ⌋γ,ˆθ,/hatwideYEM
⌊r/γ⌋γ)|2drds

28Under review as submission to TMLR
≤ζ
2/integraldisplayt
0E/bracketleftig
|Yaux
s−/hatwideYEM
s|2/bracketrightig
ds
+4γ
ζM(1 + 8 K2
3(1 + 4T2α))
×/integraldisplayt
0/integraldisplays
⌊s/γ⌋γ/bracketleftig
(1 + 16 K2
Total(1 +T2α))E[|/hatwideYEM
⌊r/γ⌋γ|2] + 32K2
Total(1 +T2α)(1 +E[|ˆθ|2])/bracketrightig
drds
≤ζ
2/integraldisplayt
0E/bracketleftig
|Yaux
s−/hatwideYEM
s|2/bracketrightig
ds
+γ2
ζ4M(1 + 8 K2
3(1 + 4T2α))
×t/bracketleftbigg
(1 + 16 K2
Total(1 +T2α)) sup
0≤s≤tE[|/hatwideYEM
s|2] + 32K2
Total(1 +T2α)(1 + 2/tildewideεAL+ 2|θ∗|2)/bracketrightbigg
≤ζ
2/integraldisplayt
0E/bracketleftig
|Yaux
s−/hatwideYEM
s|2/bracketrightig
ds
+tγ2
ζ4M(1 + 8 K2
3(1 + 4T2α))
×/bracketleftbig
(1 + 16 K2
Total(1 +T2α))CEM,2(T) + 32 K2
Total(1 +T2α)(1 + 2/tildewideεAL+ 2|θ∗|2)/bracketrightbig
.
(74)
We proceed with the estimate of the fourth term on the right-hand side of 73. For k= 1,...,M, we note
Yaux,(k)
s−/hatwideYEM,(k)
s =Yaux,(k)
⌊s/γ⌋γ−/hatwideYEM,(k)
⌊s/γ⌋γ
+/integraldisplays
⌊s/γ⌋γ(b(k)(T−r,ˆθ,Yaux
r)−b(k)(T−r,ˆθ,/hatwideYEM
r) dr
+/integraldisplays
⌊s/γ⌋γ(b(k)(T−r,ˆθ,/hatwideYEM
r)−b(k)(T−r,ˆθ,/hatwideYEM
⌊r/γ⌋γ)) dr
+/integraldisplays
⌊s/γ⌋γ2(s(k)(T−r,ˆθ,/hatwideYEM
⌊r/γ⌋γ)−s(k)(T−⌊r/γ⌋γ,ˆθ,/hatwideYEM
⌊r/γ⌋γ)) dr.(75)
By using 75, Cauchy–Schwarz inequality, Young’s inequality, Assumption 3.b and Lemma 23, we have
M/summationdisplay
k=1E
/integraldisplayt
0/parenleftig
Yaux,(k)
s−/hatwideYEM,(k)
s/parenrightig
M/summationdisplay
j=1∂b(k)(T−s,ˆθ,/hatwideYEM
⌊s/γ⌋γ)
∂xj/integraldisplays
⌊s/γ⌋γ√
2 d¯B(j)
r
ds

=M/summationdisplay
k=1E
/integraldisplayt
0/parenleftig
Yaux,(k)
⌊s/γ⌋γ−/hatwideYEM,(k)
⌊s/γ⌋γ/parenrightig
M/summationdisplay
j=1∂b(k)(T−s,ˆθ,/hatwideYEM
⌊s/γ⌋γ)
∂xj/integraldisplays
⌊s/γ⌋γ√
2 d¯B(j)
r
ds

+M/summationdisplay
k=1E/bracketleftigg/integraldisplayt
0/parenleftigg/integraldisplays
⌊s/γ⌋γ(b(k)(T−r,ˆθ,Yaux
r)−b(k)(T−r,ˆθ,/hatwideYEM
r)) dr/parenrightigg
×
M/summationdisplay
j=1∂b(k)(T−s,ˆθ,/hatwideYEM
⌊s/γ⌋γ)
∂xj/integraldisplays
⌊s/γ⌋γ√
2 d¯B(j)
r
ds/bracketrightigg
29Under review as submission to TMLR
+M/summationdisplay
k=1E/bracketleftigg/integraldisplayt
0/parenleftigg/integraldisplays
⌊s/γ⌋γ(b(k)(T−r,ˆθ,/hatwideYEM
r)−b(k)(T−r,ˆθ,/hatwideYEM
⌊r/γ⌋γ)) dr/parenrightigg
×
M/summationdisplay
j=1∂b(k)(T−s,ˆθ,/hatwideYEM
⌊s/γ⌋γ)
∂xj/integraldisplays
⌊s/γ⌋γ√
2 d¯B(j)
r
ds/bracketrightigg
+M/summationdisplay
k=1E/bracketleftigg/integraldisplayt
0/parenleftigg/integraldisplays
⌊s/γ⌋γ2(s(k)(T−r,ˆθ,/hatwideYEM
⌊r/γ⌋γ)−s(k)(T−⌊r/γ⌋γ,ˆθ,/hatwideYEM
⌊r/γ⌋γ)) dr/parenrightigg
×
M/summationdisplay
j=1∂b(k)(T−s,ˆθ,/hatwideYEM
⌊s/γ⌋γ)
∂xj/integraldisplays
⌊s/γ⌋γ√
2 d¯B(j)
r
ds/bracketrightigg
≤M/summationdisplay
k=1E/bracketleftigg/integraldisplayt
0/integraldisplays
⌊s/γ⌋γγ−1/2|b(k)(T−r,ˆθ,Yaux
r)−b(k)(T−r,ˆθ,/hatwideYEM
r)|dr
×γ1/2/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/integraldisplays
⌊s/γ⌋γM/summationdisplay
j=1∂b(k)(T−s,ˆθ,/hatwideYEM
⌊s/γ⌋γ)
∂xj√
2 d¯B(j)
r/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleds/bracketrightigg
+M/summationdisplay
k=1E/bracketleftigg/integraldisplayt
0/integraldisplays
⌊s/γ⌋γ|b(k)(T−r,ˆθ,/hatwideYEM
r)−b(k)(T−r,ˆθ,/hatwideYEM
⌊r/γ⌋γ)|dr
×/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/integraldisplays
⌊s/γ⌋γM/summationdisplay
j=1∂b(k)(T−s,ˆθ,/hatwideYEM
⌊s/γ⌋γ)
∂xj√
2 d¯B(j)
r/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleds/bracketrightigg
+M/summationdisplay
k=1E/bracketleftigg/integraldisplayt
0/integraldisplays
⌊s/γ⌋γ2|s(k)(T−r,ˆθ,/hatwideYEM
⌊r/γ⌋γ)−s(k)(T−⌊r/γ⌋γ,ˆθ,/hatwideYEM
⌊r/γ⌋γ)|dr
×/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/integraldisplays
⌊s/γ⌋γM/summationdisplay
j=1∂b(k)(T−s,ˆθ,/hatwideYEM
⌊s/γ⌋γ)
∂xj√
2 d¯B(j)
r/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleds/bracketrightigg
≤γ−1
2M/summationdisplay
k=1E/bracketleftigg/integraldisplayt
0/integraldisplays
⌊s/γ⌋γ|b(k)(T−r,ˆθ,Yaux
r)−b(k)(T−r,ˆθ,/hatwideYEM
r)|2drds/bracketrightigg
+γ
2M/summationdisplay
k=1/integraldisplayt
0E
/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/integraldisplays
⌊s/γ⌋γM/summationdisplay
j=1∂b(k)(T−s,ˆθ,/hatwideYEM
⌊s/γ⌋γ)
∂xj√
2 d¯B(j)
r/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle2
ds

+M/summationdisplay
k=1/integraldisplayt
0
E/parenleftigg/integraldisplays
⌊s/γ⌋γ|b(k)(T−r,ˆθ,/hatwideYEM
r)−b(k)(T−r,ˆθ,/hatwideYEM
⌊r/γ⌋γ)|dr/parenrightigg2
1/2
×
E/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/integraldisplays
⌊s/γ⌋γM/summationdisplay
j=1∂b(k)(T−s,ˆθ,/hatwideYEM
⌊s/γ⌋γ)
∂xj√
2 d¯B(j)
r/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle2
1/2
ds
+M/summationdisplay
k=1/integraldisplayt
0
E/parenleftigg/integraldisplays
⌊s/γ⌋γ2|s(k)(T−r,ˆθ,/hatwideYEM
⌊r/γ⌋γ)−s(k)(T−⌊r/γ⌋γ,ˆθ,/hatwideYEM
⌊r/γ⌋γ)|dr/parenrightigg2
1/2
×
E/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/integraldisplays
⌊s/γ⌋γM/summationdisplay
j=1∂b(k)(T−s,ˆθ,/hatwideYEM
⌊s/γ⌋γ)
∂xj√
2 d¯B(j)
r/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle2
1/2
ds
30Under review as submission to TMLR
≤E/bracketleftigg/integraldisplayt
0/integraldisplays
⌊s/γ⌋γγ−1(1 + 8 K2
3(1 + 4T2α))|Yaux
r−/hatwideYEM
r|2drds/bracketrightigg
+ 2γ2M/summationdisplay
k=1E
/integraldisplayt
0M/summationdisplay
j=1/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle∂b(k)(T−s,ˆθ,/hatwideYEM
⌊s/γ⌋γ)
∂xj/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle2
ds

+γ1/22(1 + 8 K2
3(1 + 4T2α))1/2
×M/summationdisplay
k=1/integraldisplayt
0/bracketleftigg/integraldisplays
⌊s/γ⌋γE[|/hatwideYEM
r−/hatwideYEM
⌊r/γ⌋γ|2] dr/bracketrightigg1/2
E/integraldisplays
⌊s/γ⌋γ/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleM/summationdisplay
j=1∂b(k)(T−s,ˆθ,/hatwideYEM
⌊s/γ⌋γ)
∂xj/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle2
dr
1/2
ds
+γ1+α4K1(1 + 8/tildewideεAL+ 8|θ∗|2)1/2M/summationdisplay
k=1/integraldisplayt
0
E/integraldisplays
⌊s/γ⌋γ/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleM/summationdisplay
j=1∂b(k)(T−s,ˆθ,/hatwideYEM
⌊s/γ⌋γ)
∂xj/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle2
dr
1/2
ds
≤(1 + 8 K2
3(1 + 4T2α))/integraldisplayt
0sup
0≤r≤sE/bracketleftig
|Yaux
r−/hatwideYEM
r|2/bracketrightig
ds+ 4γ2tM(1 + 8 K2
3(1 + 4T2α))
+γ3/22(1 + 8 K2
3(1 + 4T2α))1/2C1/2
EMose,2M/summationdisplay
k=1/integraldisplayt
0
E/integraldisplays
⌊s/γ⌋γ/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleM/summationdisplay
j=1∂b(k)(T−s,ˆθ,/hatwideYEM
⌊s/γ⌋γ)
∂xj/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle2
dr
1/2
ds
+γ1+α4K1(1 + 8/tildewideεAL+ 8|θ∗|2)1/2M/summationdisplay
k=1/integraldisplayt
0
E/integraldisplays
⌊s/γ⌋γ/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleM/summationdisplay
j=1∂b(k)(T−s,ˆθ,/hatwideYEM
⌊s/γ⌋γ)
∂xj/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle2
dr
1/2
ds
≤(1 + 8 K2
3(1 + 4T2α))/integraldisplayt
0sup
0≤r≤sE/bracketleftig
|Yaux
r−/hatwideYEM
r|2/bracketrightig
ds+ 4γ2tM(1 + 8 K2
3(1 + 4T2α))
+ [γ22(1 + 8 K2
3(1 + 4T2α))1/2C1/2
EMose,2+γ3/2+α4K1(1 + 8/tildewideεAL+ 8|θ∗|2)1/2]
×[tM√
2(1 + 8 K2
3(1 + 4T2α))1/2].(76)
Using 73, 74 and 76 in 72, we have
E/bracketleftig
|Yaux
t−/hatwideYEM
t|2/bracketrightig
≤/integraldisplayt
04(1 +ζ+K3(1 + 2Tα+ 4K3(1 + 4T2α))) sup
0≤r≤sE/bracketleftig
|Yaux
r−/hatwideYEM
r|2/bracketrightig
ds
+ 8γ2tζ−1M(1 + 8 K2
3(1 + 4T2α))
×/bracketleftbig
(1 + 16 K2
Total(1 +T2α))CEM,2(T) + 32 K2
Total(1 +T2α)(1 + 2/tildewideεAL+ 2|θ∗|2)/bracketrightbig
+ 2γ2tK2
4ζ−1(1 + 4T2α)CEMose,4+ 8γ2tM(1 + 8 K2
3(1 + 4T2α))
+γ2αt4ζ−1K2
1(1 + 8(/tildewideεAL+|θ∗|2))
+ 4[γ2(1 + 8 K2
3(1 + 4T2α))1/2C1/2
EMose,2+γ3/2+α2K1(1 + 8/tildewideεAL+ 8|θ∗|2)1/2]
×[tM√
2(1 + 8 K2
3(1 + 4T2α))1/2].
Thus,
sup
0≤s≤tE/bracketleftig
|Yaux
s−/hatwideYEM
s|2/bracketrightig
≤/integraldisplayt
04(1 +ζ+K3(1 + 2Tα+ 4K3(1 + 4T2α))) sup
0≤r≤sE/bracketleftig
|Yaux
r−/hatwideYEM
r|2/bracketrightig
ds
31Under review as submission to TMLR
+γ2αt2/parenleftigg
K2
4ζ−1(1 + 4T2α)CEMose,4+ 4M(1 + 8 K2
3(1 + 4T2α)) + 2ζ−1K2
1(1 + 8(/tildewideεAL+|θ∗|2))
+ 4ζ−1M(1 + 8 K2
3(1 + 4T2α))
×[(1 + 16 K2
Total(1 +T2α))CEM,2(T) + 32 K2
Total(1 +T2α)(1 + 2/tildewideεAL+ 2|θ∗|2)]
+ 2[(1 + 8 K2
3(1 + 4T2α))1/2C1/2
EMose,2+ 2K1(1 + 8/tildewideεAL+ 8|θ∗|2)1/2]
×[M√
2(1 + 8 K2
3(1 + 4T2α))1/2]/parenrightigg
≤2e4(1+ζ+K3(1+2Tα+4K3(1+4T2α)))tγ2αt
×/parenleftigg
K2
4ζ−1(1 + 4T2α)CEMose,4+ 4M(1 + 8 K2
3(1 + 4T2α)) + 2ζ−1K2
1(1 + 8(/tildewideεAL+|θ∗|2))
+ 4ζ−1M(1 + 8 K2
3(1 + 4T2α))
×[(1 + 16 K2
Total(1 +T2α))CEM,2(T) + 32 K2
Total(1 +T2α)(1 + 2/tildewideεAL+ 2|θ∗|2)]
+ 2[(1 + 8 K2
3(1 + 4T2α))1/2C1/2
EMose,2+ 2K1(1 + 8/tildewideεAL+ 8|θ∗|2)1/2]
×[M√
2(1 + 8 K2
3(1 + 4T2α))1/2]/parenrightigg
.(77)
Using 77 and tK=T−ϵ, we have
W2(L(Yaux
tK),L(YEM
K))
≤/radicalbigg
E/bracketleftig
|Yaux
tK−/hatwideYEM
tK|2/bracketrightig
≤√
2e2(1+ζ+K3(1+2Tα+4K3(1+4T2α)))(T−ϵ)γα√
T−ϵ
×/parenleftigg
K2
4ζ−1(1 + 4T2α)CEMose,4+ 4M(1 + 8 K2
3(1 + 4T2α)) + 2ζ−1K2
1(1 + 8(/tildewideεAL+|θ∗|2))
+ 4ζ−1M(1 + 8 K2
3(1 + 4T2α))
×[(1 + 16 K2
Total(1 +T2α))CEM,2(T) + 32 K2
Total(1 +T2α)(1 + 2/tildewideεAL+ 2|θ∗|2)]
+ 2[(1 + 8 K2
3(1 + 4T2α))1/2C1/2
EMose,2+ 2K1(1 + 8/tildewideεAL+ 8|θ∗|2)1/2]
×[M√
2(1 + 8 K2
3(1 + 4T2α))1/2]/parenrightigg1/2
.(78)
Final upper bound on W2(L(YEM
K),πD).Substituting 61, 65, 68, and 78 into 59, we have
W2(L(YEM
K),πD)
≤(/radicalbig
E[|X0|2] +√
M)2√ϵ
+√
2(/radicalbig
E[|X0|2] +√
M)e−2/hatwideLMO(T−ϵ)−ϵ
+/radicalbig
2ζ−1e(1+ζ−2/hatwideLMO)(T−ϵ)√εSN
+√
2e2(1+ζ+K3(1+2Tα+4K3(1+4T2α)))(T−ϵ)γα√
T−ϵ
32Under review as submission to TMLR
×/parenleftigg
K2
4ζ−1(1 + 4T2α)CEMose,4+ 4M(1 + 8 K2
3(1 + 4T2α)) + 2ζ−1K2
1(1 + 8(/tildewideεAL+|θ∗|2))
+ 4ζ−1M(1 + 8 K2
3(1 + 4T2α))
×[(1 + 16 K2
Total(1 +T2α))CEM,2(T) + 32 K2
Total(1 +T2α)(1 + 2/tildewideεAL+ 2|θ∗|2)]
+ 2[(1 + 8 K2
3(1 + 4T2α))1/2C1/2
EMose,2+ 2K1(1 + 8/tildewideεAL+ 8|θ∗|2)1/2]
×[M√
2(1 + 8 K2
3(1 + 4T2α))1/2]/parenrightigg1/2
.(79)
The bound for W2(L(/hatwideYEM
K),πD)in 79 can be made arbitrarily small by appropriately choosing parameters
includingϵ,T,ε SNandγ. More precisely, for any δ >0, we first choose 0≤ϵ<ϵδwithϵδgiven in Table 4
such that the first term on the right-hand side of 79 is
(/radicalbig
E[|X0|2] +√
M)2√ϵ<δ/ 4. (80)
Next, we choose T >TδwithTδgiven in Table 4 such that the second term on the right-hand side of 79 is
√
2(/radicalbig
E[|X0|2] +√
M)e−2/hatwideLMO(T−ϵ)−ϵ<δ/4. (81)
Next, we turn to the third term on the right-hand side of 79. We choose 0<εSN<εSN,δwithεSN,δgiven
in Table 4 such that
/radicalbig
2ζ−1e(1+ζ−2/hatwideLMO)(T−ϵ)√εSN<δ/4. (82)
Finally, we choose 0<γ <γδwithγδgiven in Table 4 such that the fourth term on the right-hand side of
79 is
√
2e2(1+ζ+K3(1+2Tα+4K3(1+4T2α)))(T−ϵ)γα√
T−ϵ
×/parenleftigg
K2
4ζ−1(1 + 4T2α)CEMose,4+ 4M(1 + 8 K2
3(1 + 4T2α)) + 2ζ−1K2
1(1 + 8(/tildewideεAL+|θ∗|2))
+ 4ζ−1M(1 + 8 K2
3(1 + 4T2α))
×[(1 + 16 K2
Total(1 +T2α))CEM,2(T) + 32 K2
Total(1 +T2α)(1 + 2/tildewideεAL+ 2|θ∗|2)]
+ 2[(1 + 8 K2
3(1 + 4T2α))1/2C1/2
EMose,2+ 2K1(1 + 8/tildewideεAL+ 8|θ∗|2)1/2]
×[M√
2(1 + 8 K2
3(1 + 4T2α))1/2]/parenrightigg1/2
<δ/4.(83)
Using 80, 81, 82 and 83, we obtain W2(L(/hatwideYEM
K),πD)<δ.
D.3 Proof of the Preliminary Results for the General Case
We provide the proofs of Section 3.2 and Appendix D.1.
Proof of Remark 5. Using Assumption 3.b, we have
|s(t,θ,x )|≤|s(t,θ,x )−s(0,0,0)|+|s(0,0,0)|
≤K1(1 +|θ|)|t|α+K2(1 +|t|α)|θ|+K3(1 +|t|α)|x|+|s(0,0,0)|
≤KTotal(1 +|t|α)(1 +|θ|+|x|),
where KTotal :=K1+K2+K3+|s(0,0,0)|.
33Under review as submission to TMLR
Proof of Lemma 20. Using Itô’s formula, we have, for any t∈[0,T−ϵ]andp∈[2,4],
d|/hatwideYEM
t|p=p/angbracketleftig
|/hatwideYEM
t|p−2/hatwideYEM
t,/hatwideYEM
⌊t/γ⌋γ/angbracketrightig
dt+p/angbracketleftig
|/hatwideYEM
t|p−2/hatwideYEM
t,2s(T−⌊t/γ⌋γ,ˆθ,/hatwideYEM
⌊t/γ⌋γ)/angbracketrightig
dt
+p/angbracketleftig
|/hatwideYEM
t|p−2/hatwideYEM
t,√
2 dBt/angbracketrightig
+p
2|/hatwideYEM
t|p−2(2M) dt+p(p−2)
2|/hatwideYEM
t|p−42|/hatwideYEM
t|2dt.(84)
Integrating and taking expectation on both sides in 84, using Young’s inequality and Remark 5, we have
E/bracketleftig
|/hatwideYEM
t|p/bracketrightig
=E/bracketleftig
|/hatwideYEM
0|p/bracketrightig
+p/integraldisplayt
0E/bracketleftig
⟨|/hatwideYEM
s|p−2/hatwideYEM
s,/hatwideYEM
⌊s/γ⌋γ⟩/bracketrightig
ds
+ 2p/integraldisplayt
0E/bracketleftig
⟨|/hatwideYEM
s|p−2/hatwideYEM
s,s(T−⌊s/γ⌋γ,ˆθ,/hatwideYEM
⌊s/γ⌋γ)⟩/bracketrightig
ds
+p(M+p−2)/integraldisplayt
0E/bracketleftig
|/hatwideYEM
s|p−2/bracketrightig
ds
≤E/bracketleftig
|/hatwideYEM
0|p/bracketrightig
+ 3(p−1)/integraldisplayt
0E/bracketleftig
|/hatwideYEM
s|p/bracketrightig
ds+/integraldisplayt
0E/bracketleftig
|/hatwideYEM
⌊s/γ⌋γ|p/bracketrightig
ds
+ 2/integraldisplayt
0E/bracketleftig
|s(T−⌊s/γ⌋γ,ˆθ,/hatwideYEM
⌊s/γ⌋γ)|p/bracketrightig
ds
+2
p(pM+p(p−2))p
2t+p−2
p/integraldisplayt
0E/bracketleftig
|/hatwideYEM
s|p/bracketrightig
ds
≤E/bracketleftig
|/hatwideYEM
0|p/bracketrightig
+/parenleftbigg
3p−2−2
p/parenrightbigg/integraldisplayt
0E/bracketleftig
|/hatwideYEM
s|p/bracketrightig
ds+/integraldisplayt
0E/bracketleftig
|/hatwideYEM
⌊s/γ⌋γ|p/bracketrightig
ds
+ 2pKp
Total/integraldisplayt
0(1 +|T−⌊s/γ⌋γ|α)pE/bracketleftig
|/hatwideYEM
⌊s/γ⌋γ|p/bracketrightig
ds
+ 22p−1Kp
Total(1 +E[|ˆθ|p])/integraldisplayt
0(1 +|T−⌊s/γ⌋γ|α)pds+2
p(pM+p(p−2))p
2t
≤E/bracketleftig
|/hatwideYEM
0|p/bracketrightig
+ (3p−1−2
p+ 22p−1Kp
Total(1 +Tαp))/integraldisplayt
0sup
0≤r≤sE/bracketleftig
|/hatwideYEM
r|p/bracketrightig
ds
+ 23p−2Kp
Total(1 +E[|ˆθ|p])(1 +Tαp)t+2
p(pM+p(p−2))p
2t.
Using Grönwall’s inequality, we have
sup
0≤s≤tE/bracketleftig
|/hatwideYEM
s|p/bracketrightig
≤E/bracketleftig
|/hatwideYEM
0|p/bracketrightig
+ (3p−1−2
p+ 22p−1Kp
Total(1 +Tαp))/integraldisplayt
0sup
0≤r≤sE/bracketleftig
|/hatwideYEM
r|p/bracketrightig
ds
+ 23p−2Kp
Total(1 +E[|ˆθ|p])(1 +Tαp)t+2
p(pM+p(p−2))p
2t
≤et(3p−1−2
p+22p−1Kp
Total(1+Tαp))
×(E/bracketleftig
|/hatwideYEM
0|p/bracketrightig
+ 23p−2Kp
Totalt(1 +E[|ˆθ|p])(1 +Tαp) +2
p(pM+p(p−2))p
2t).
Proof of Lemma 21. Using 11, Lemma 20 and Remark 5, we have, for any t∈[0,T−ϵ]andp∈[2,4],
E/bracketleftig
|/hatwideYEM
t−/hatwideYEM
⌊t/γ⌋γ|p/bracketrightig
≤γpE/bracketleftig/vextendsingle/vextendsingle/vextendsingle/hatwideYEM
⌊t/γ⌋γ+ 2s(T−⌊t/γ⌋γ,ˆθ,/hatwideYEM
⌊t/γ⌋γ)/vextendsingle/vextendsingle/vextendsinglep/bracketrightig
+E/bracketleftigg/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/integraldisplayt
⌊t/γ⌋γ√
2 dBs/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsinglep/bracketrightigg
34Under review as submission to TMLR
≤2p−1γp/parenleftig
E/bracketleftig
|/hatwideYEM
⌊t/γ⌋γ|p/bracketrightig
+ 2pE/bracketleftig
|s(T−⌊t/γ⌋γ,ˆθ,/hatwideYEM
⌊t/γ⌋γ)|p/bracketrightig/parenrightig
+γp
2(Mp(p−1))p
2
≤2p−1γp(CEM,p(t) + 23p−2Kp
Total(1 +Tαp)CEM,p(t) + 24p−3Kp
Total(1 +Tαp)(1 +E[|ˆθ|p]))
+γp
2(Mp(p−1))p
2
≤γp
2CEMose,p,
where
CEMose,p= 2p−1(CEM,p(T) +Kp
Total(1 +Tαp)(23p−2CEM,p(T) + 24p−3(1 +E[|ˆθ|p]))) + (Mp(p−1))p
2.
Proof of Lemma 22. By the mean value theorem, for any k= 1,...,M, we have,
b(k)(t,θ,x )−b(k)(t,θ,¯x) =M/summationdisplay
i=1∂b(k)(t,θ,qx + (1−q)¯x)
∂yi(xi−¯x(i)),
for someq∈(0,1). Hence, for a fixed q∈(0,1), we have
/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleb(k)(t,θ,x )−b(k)(t,θ,¯x)−M/summationdisplay
i=1∂b(k)(t,θ,¯x)
∂yi(xi−¯xi)/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
=/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleM/summationdisplay
i=1∂b(k)(t,θ,qx + (1−q)¯x)
∂yi(xi−¯x(i))−M/summationdisplay
i=1∂b(k)(t,θ,¯x)
∂yi(xi−¯xi)/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
≤M/summationdisplay
i=1/vextendsingle/vextendsingle/vextendsingle/vextendsingle∂b(k)(t,θ,qx + (1−q)¯x)
∂yi−∂b(k)(t,θ,¯x)
∂yi/vextendsingle/vextendsingle/vextendsingle/vextendsingle|xi−¯xi|.
The proof is completed using Assumption 3.b.
Proof of Lemma 23. Atx∈RMand for any v∈RM, we have, for any k= 1,...M,
⟨∇xs(k)(t,θ,x ),v⟩= lim
h→0s(k)(t,θ,x +vh)−s(k)(t,θ,x )
h.
Using Assumption 3.b, we have
|⟨∇xs(k)(t,θ,x ),v⟩|≤ lim
h→0/vextendsingle/vextendsingle/vextendsingle/vextendsingles(k)(t,θ,x +vh)−s(k)(t,θ,x )
h/vextendsingle/vextendsingle/vextendsingle/vextendsingle
≤lim
h→0|D3(t,t)||x+vh−x|
|h|
≤K3(1 + 2|t|α)|v|.(85)
Takingv=∇xs(k)(t,θ,x)
|∇xs(k)(t,θ,x)|in 85, we have
|∇xs(k)(t,θ,x )|≤K3(1 + 2|t|α). (86)
Using 57 and 86, we obtain
|∇xb(k)(t,θ,x )|≤1 + 2|∇xs(k)(t,θ,x )|
≤1 + 2K3(1 + 2|t|α).
35Under review as submission to TMLR
E Table of Constants
Table 4 displays full expressions for constants which appear in Theorem 10.
Table 4: Explicit expressions for the constants in Theorem 10.
Constant Dependency Full Expression
C1 O(√
M) 2(/radicalbig
E[|X0|2] +√
M)
C2 O(√
M)√
2/parenleftig/radicalbig
E[|X0|2] +√
M/parenrightig
C3(T,ϵ)O(e(1+ζ−2/hatwideLMO)(T−ϵ))/radicalbig
2ζ−1e(1+ζ−2/hatwideLMO)(T−ϵ)
CEM,2(T)O(MeT2α+1T2α+1/tildewideεAL)eT(4+8 K2
Total(1+T2α))
×(E[|/hatwideYEM
0|2] + 16 K2
TotalT(1 + 2/tildewideεAL+ 2|θ∗|2)(1 +T2α) + 2MT)
CEM,4(T)O(M2eT4α+1T4α+1)eT(21
2+128 K4
Total(1+T4α))
×(E[|/hatwideYEM
0|4] + 1024 K4
TotalT(1 +E[|ˆθ|4])(1 +T4α) + 8(M2+ 4M+ 4)T)
CEMose,2O(MeT2α+1T4α+1/tildewideεAL) 2(CEM,2(T) +K2
Total(1 +T2α)(16CEM,2(T) + 32(1 + 2/tildewideεAL+ 2|θ∗|2))) + 2M
CEMose,4O(M2eT4α+1T8α+1) 8(CEM,4(T) +K4
Total(1 +T4α)(1024CEM,4(T) + 8192(1 + E[|ˆθ|4]))) + 144M2
C4(T,ϵ)O(MeT4α+1T4α+1/tildewideε1/4
AL)√
2e2(1+ζ+K3(1+2Tα+4K3(1+4T2α)))(T−ϵ)/radicalbig
T−ϵ
×/parenleftbigg
K2
4ζ−1(1 + 4T2α)CEMose,4+ 4M(1 + 8 K2
3(1 + 4T2α))
+ 2ζ−1K2
1(1 + 8(/tildewideεAL+|θ∗|2))
+ 4ζ−1M(1 + 8 K2
3(1 + 4T2α))
×[(1 + 16 K2
Total(1 +T2α))CEM,2(T)
+ 32 K2
Total(1 +T2α)(1 + 2/tildewideεAL+ 2|θ∗|2)]
+ 2[(1 + 8 K2
3(1 + 4T2α))1/2C1/2
EMose,2+ 2K1(1 + 8/tildewideεAL+ 8|θ∗|2)1/2]
×[M√
2(1 + 8 K2
3(1 + 4T2α))1/2]/parenrightbigg1/2
.
ϵδ - δ2/(64(/radicalbig
E[|X0|2] +√
M)2)
Tδ - (2/hatwideLMO)−1/bracketleftig
ln/parenleftig
4√
2/parenleftig/radicalbig
E[|X0|2] +√
M/parenrightig
/δ/parenrightig
−ϵ/bracketrightig
+ϵ
εSN,δ - (ζδ2/32)e−2(1+ζ−2/hatwideLMO)(T−ϵ)
γδ -min/braceleftbigg
(δ/(4√
2))1/α(T−ϵ)−1/(2α)e−(2/α)(1+ζ+K3(1+2Tα+4K3(1+4T2α)))(T−ϵ)
×/parenleftbigg
K2
4ζ−1(1 + 4T2α)CEMose,4+ 4M(1 + 8 K2
3(1 + 4T2α))
+ 2ζ−1K2
1(1 + 8(/tildewideεAL+|θ∗|2))
+ 4ζ−1M(1 + 8 K2
3(1 + 4T2α))
×[(1 + 16 K2
Total(1 +T2α))CEM,2(T)
+ 32 K2
Total(1 +T2α)(1 + 2/tildewideεAL+ 2|θ∗|2)]
+ 2[(1 + 8 K2
3(1 + 4T2α))1/2C1/2
EMose,2+ 2K1(1 + 8/tildewideεAL+ 8|θ∗|2)1/2]
×[M√
2(1 + 8 K2
3(1 + 4T2α))1/2]/parenrightbigg−1/(2α)
,1/bracerightbigg
.
36