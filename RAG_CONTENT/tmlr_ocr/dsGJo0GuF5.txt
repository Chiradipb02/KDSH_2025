UnderreviewassubmissiontoTMLRContinualfew-shotlearningwithHippocampal-inspiredre-playAnonymousauthorsPaperunderdouble-blindreviewAbstractContinuallearningandfew-shotlearningareimportantfrontiersinthequesttoimproveMachineLearning.Thereisagrowingbodyofworkineachfrontier,butverylittlecombiningthetwo.Recentlyhowever,Antoniouetal.(2020)introducedaContinualFew-shotLearningframework,CFSL,thatcombinesboth.Inthisstudy,weextendedCFSLtomakeitmorecomparabletostandardcontinuallearningexperiments,whereusuallyamuchlargernumberofclassesarepresented.Wealsointroducedan‘instancetest’toclassifyverysimilarspeciﬁcinstances-acapabilityofanimalcognitionthatisusuallyneglectedinML.WeselectedrepresentativebaselinemodelsfromtheoriginalCFSLworkandcomparedtoamodelwithHippocampal-inspiredreplay,astheHippocampusisconsideredtobevitaltothistypeoflearninginanimals.Asexpected,learningmoreclassesismorediﬃcultthantheoriginalCFSLexperiments,andinterestingly,thewayinwhichtheyarepresentedmakesadiﬀerencetoperformance.Accuracyintheinstancetestiscomparabletotheclassiﬁcationtasks.Theuseofreplayforconsolidationimprovesperformancesubstantiallyforbothtypesoftasks,particularlytheinstancetest.1IntroductionOverthepastdecade,MachineLearning(ML)hasmadeimpressiveprogressinmanyareas.Theareasinwhichprogresshasbeenmostdramaticsharesomecommoncharacteristics.Typically,amodellearnsfromalargeiiddatasetwithmanysamplesperclassandafteratrainingphase,theweightsareﬁxedi.e.itdoesnotcontinuetolearn.Thisislimitingformanyapplicationsandasaresultdistinctsubﬁeldshaveemergedwhichembracediﬀerentcharacteristics,suchascontinuallearningandfew-shotlearning.Incontinuallearning(alsoknownaslifelonglearning),thechallengeistocontinuallylearnnewtaskswhilemaintainingperformanceonpreviousones.Awellknowndiﬃcultyiscatastrophicforgetting(McCloskey&Cohen,1989)inwhichnewlearningdisruptsexistingknowledge.Therearemanyapproachestotacklecatastrophicforgettingthatfallbroadlyinto3categories(Delangeetal.,2021):Regularization-basedmeth-ods,ParameterisolationmethodsandReplaymethodswhichareinspiredbyHippocampalreplay(Parisietal.,2018).Toourknowledge,noneofthereportedworksexplorecontinuallearningwithfewsamplesperclass.Infew-shotlearning,onlyafewsamplesofeachclassareavailable.Inthestandardframework(Lakeetal.,2015;Vinyalsetal.,2017),backgroundknowledgeisﬁrstacquiredinapre-trainingphasewithmanyclasses.Thenoneorafewexamplesofanovelclassarepresentedforlearning,andthetaskistoidentifythisclassinatestset(typically5or20samplesofdiﬀerentclasses).Knowledgeofnovelclassesisnotpermanentlyintegratedintothenetwork,whichprecludescontinuallearning.Aspecialcaseoffew-shotlearningisreasoningaboutspeciﬁcinstances.Thisiseasyforanimals,buttypicallyneglectedbyMLresearch.Forexampleyouusuallyknowwhichcoﬀeecupisyours,evenifitappearssimilartothecupofteathatbelongstoyourcolleague.Itiseasytoseehowthiscapabilityhasapplicationsacrossdomainsfromautonomousroboticstodialoguewithhumanstofrauddetection.1UnderreviewassubmissiontoTMLRAnotherenviablecharacteristicofhumanandanimallearning,istheabilitytoperformbothcontinualandfew-shotlearningsimultaneously.Weneedtoaccumulateknowledgequicklyandmayonlyeverreceiveafewexamplestolearnfrom.Forexample,givenknowledgeofvehicles(e.g.trucks,cars,bikesetc.),wecanlearnaboutanynumberofadditionalnovelvehicles(e.g.motorbike,thenskateboard)fromonlyafewexamples.Thisabilityiscriticalforeverydaylife,particularlyartiﬁcialagentsinchangingenvironmentsandmanyindustryapplications.Inordertocombinecontinualandfew-shotlearning,Antoniouetal.(2020)introducedtheContinualFew-ShotLearning(CFSL)framework.Itisﬂexible,allowingthedescriptionofdiversescenarioswithonlyasmallsetofparameters:thenumberofsmalltrainingsets,referredtoas‘supportsets’(NSS),howmanysupportsetsbeforeclasschanges(CCI),numberofclasses(n-way),andnumberofexposures(k-shot).Theoriginalstudycomparedstandardfew-shotlearningalgorithmsandtheexperimentswerelimitedtoonly5classespersupportsetandamaximumof10supportsets,whereasincontinuallearningproblems,therearetypicallyamuchlargernumberofclassesandsupportsets.Inthisstudyweextendedtheworkof(Antoniouetal.,2020)inthreemaindirections.First,weranexperimentswithanorderofmagnitudemoreclassestogiveresultsthatarecomparablewithtypicalcontinuallearningstudies.Second,weintroducedaninstancetest,whichisaspecialcaseoftheCFSLframework.Third,weinvestigatedtheeﬀectofHippocampal-inspiredreplayundertheseconditions.2ExperimentalMethodWeﬁrstgiveanoverviewoftheCFSLframework(Antoniouetal.,2020),uponwhichourstudyisbased,inSection2.1.Second,wedescribeexperimentstoscaleselectedtestsin(Antoniouetal.,2020)toagreaternumberofclasses,referredtoas‘CFSLatscale’,inSection2.2.Third,weintroducetheinstancetestinSection2.3.Finally,wedescribethemodelsusedintheexperimentsinSection2.4.Thesourcecodeforallexperimentsislocatedathttps://github.com/xxxx.2.1Continualfew-shotlearningframework-backgroundAscontext,wewillrecaptheCFSLframeworkandterminology.Incontinuallearning,newtasksareintroducedinastreamandoldtrainingsamplesarenevershownagain.Performanceiscontinuallyassessedonnewandoldtasks.IntheCFSLframework,thenewdataispresentedwithcollectionsofsamplesdeﬁnedas‘supportsets’,andthenthemodelmustclassifyasetoftestsamplesina‘targetset’.TheexperimentisparameterisedbyasmallsetofparametersdescribedinTable1.Byvaryingtheseparameters,theexperimentercancontrolthetotalnumberofclasses,NC,samplesperclass,andthemannerinwhichtheyarepresentedtothelearner.AvisualrepresentationisshowninFigure1.Table1:TheparametersthatfullydeﬁneanexperimentintheCFSLframeworkbyAntoniouetal.(2020)ParameterDescriptionNSSNumberofsupportsetsCCIClass-changeintervale.g.ifCCI=2,thentheclasswillchangeevery2supportsetsn-wayNumberclassespersetk-shotNumberofsamplespersupportclassinasupportset2.2CFSLatscaleWechosetobaseourexperimentsontheparametersofTaskDasdescribedintheoriginalCFSLbenchmark(Antoniouetal.,2020).TaskD(seeFigure1)introducesbothnewclassesandmultipleinstancesofeachclass,whichweargueisthemostcommon,applicablereal-worldscenario.Weempiricallyoptimizedtheexperimentalmethodforspeedandeﬃciency.Weobservedthelearningcurves,andreducedthenumberofepochsanditerationswhereitdidnotsacriﬁceaccuracy.Inthecaseofoneof2UnderreviewassubmissiontoTMLR
Figure1:VisualrepresentationofCFSLexperimentparameterisation.Reproducedfrom(Antoniouetal.,2020).themodels,VGG(Simonyan&Zisserman,2014),thatmeantloweringfrom250epochsand500iterationsperepochto10epochsand100iterationsperepoch.Inaddition,wedecreasedthenumberoftasksfrom600to100,withoutnoticeablyaﬀectingthemeanandstandarddeviationaccuracyacrosstasks.IntheoriginalCFSLbenchmark,Antoniouetal.(2020)tookanensembleofmodelsfromthetop5performingiterations.Ensemblingiswell-knowntoimproveperformanceinML,andsowepreferredtoseeamoredirectmeasureofperformancebysimplytakingthemeanacrosstasks.Weincludetheensemblescoresforcomparison.TheoriginalexperimentswereconductedonbothOmniglotandSlimagenetdatasets.OurstudyutilizesOmniglotasastartingpoint.2.2.1ReplicationDuringourworkwithCFSL,weidentiﬁedandﬁxedanumberofissuesintheoriginalCFSLcodebase(Antoniouetal.,2020),andcollaboratedwiththeauthorstohavethemreviewedandmergedupstream.Giventhesigniﬁcanceofsomeoftheseissues,weoptedtoreplicateaselectednumberoftheoriginalexperimentstoproperlycontextualiseournewexperimentsandresults.Themainissuesrelatedtoa)VGGweightupdatesandb)mislabellingofnewinstanceswhichbecameanissuewhereCCI>1.2.2.2ScalingOneofthelimitationsin(Antoniouetal.,2020)isthesmalltotalnumberofclassesineachexperiment(5classespersupportsetandamaximumof10supportsets).Itiscommoninthecontinuallearningﬁeld,forthenumberofclassestorangefrom20to200,evenifthenumberoftasksinasequencemaybesmall(approximately10).Therefore,weintroducedexperimentswithupto200classes,presentedintwoways.Wide,inwhichthenumberofsupportsetswassmallbutwithalargernumberofclassesperset,andDeep,wheretherewerealargernumberofsupportsetsbutwithasmallernumberofclassesperset.SeeFigure2.Twoofthereplicationconﬁgurationswereusedasbaselines,with10and20classes.Then,wecreatedWideandDeepconﬁgurations,with20totalclasseslikethe2ndbaseline,butwemodiﬁedthewaythattheclasseswerepresented.Finally,thenumberofclasseswasincreasedten-foldto200,presentedinbothWideandDeepconﬁgurations.Inalloftheexperiments,k-shotissetto1,soforanysupportset,thereisonlyoneexemplarperclass.Toensurethatthescalingexperimentsareafaircomparisontothereplicationexperiments,particularlygiventhattheexperimentsizeisdramaticallyincreased,weconductedextensivehyperparametersearchtoempiricallyoptimizetheresults.3UnderreviewassubmissiontoTMLR
Figure2:WidevsDeep.AnillustrationofWidevsDeepexperiments.Widehavebigsupportsetsandfewtasks,Deephassmallsupportsetsandmanytasks.2.3InstancetestThestandarddeﬁnitionof‘classiﬁcation’concernsgeneralization.Inotherwords,theobjectiveistolearntoignoresmallvariationsthatarenotstatisticallysigniﬁcant.Incontrast,learningspeciﬁcinstancesrequiresanabilitytolearnthedistinctcharacteristicsofaspeciﬁcinstanceofaclass,todiﬀerentiatebetweenverysimilarsamples,eventodiﬀerentiatesamplesofthesameclass.Itimpliesmemorization,butstillrequiresadegreeofgeneralizationi.e.youneedtobeabletorecognizethesameinstancedespiteobservationalvariationcausedbyfactorssuchaslightingchangesorocclusion.Reasoningaboutinstancesisalsocrucialforintelligentagents,andissomethingthatwetakeforgrantedinhumansandotheranimals.Forexample,knowingyourownmugfromothers’cupsormugs,inadditiontorecognisingthatitbelongstothe‘cup’class.Moregenerally,itunderpinsmemoryforsingularfactsandanindividual’sownautobiographicalhistory,importantforfuturedecisionmaking.Toevaluatethecapabilitiesdescribedabove,weinventedan‘instancetest’versionofCFSLbasedontheinstancetestinAHA(Kowadloetal.,2020).Inourinstancetest,thelearnermustlearntorecognizespeciﬁcexemplarsamongstsetswherealltheexemplarsaredrawnfromthesameclass.ItisademonstrationoftheﬂexibilityoftheCFSLframework,thattheinstancetestcanbeimplementedasaspecialcaseoftheexistingparameters.n-wayissetto1,sothatthereisonlyoneclassineachsupportset.CCIisequaltoNSSsothatthereisnoclasschangebetweensupportsets.Thenthek-shotorsamplesperclass,determineshowmanyinstancesareshownforagivenclass,whichwerefertoasNumberofInstances,NI.Weusedaconstanttotalnumberofinstancesforallexperiments,20,butexperimentedwithpresentingthesamplesdiﬀerently,intermsofnumberandsizeofsupportsets.WereusedempiricallyoptimalhyperparametersfromtheScalingExperiments.2.4Models2.4.1BaselinearchitecturesWeselectedthreemodelsfromtheoriginalCFSLpapertouseasbaselines,torepresenteachfamilyofalgorithmthatwastested.TheﬁrstmodelwasVGG(Simonyan&Zisserman,2014),whichisastandard4UnderreviewassubmissiontoTMLRCNNarchitecture,trainedwithconventionalminibatchstochasticgradientdescent.ThesecondmodelwasProtoNet(Snelletal.,2017),whichisameta-learningapproach.WeintendedtoalsoevaluateSCA,whichisacomplexandhighperformingmeta-learningapproach.Unfortunatelyhowever,weencounteredscalabilityandresourceissueswithSCA.WewereunabletosuccessfullycompletethelargervariantofeachexperimenttypefortheSCAmodel.TheexperimentsfortheVGGbaselineandProtoNetswereachievablewithouravailablecomputationalresources.2.4.2LearningwithreplayAcoreobjectiveofthisresearchistotesttheconceptofCLS(McClellandetal.,1995;O’Reillyetal.,2014;Kumaranetal.,2016)replayinCFSL.InCLS,ashorttermmemory(STM)storesrecentrepresentationsofstimuliinahighlynon-interferingmanner.Interleavedreplaytoalongtermmemory(LTM),whichisassumedtobeaniterativestatisticallearner,enablesimprovedlearningandretentionofthatknowledge.WeutiliseasimplecircularbuﬀerasSTM.ItisanidealisedversionofaHippocampalSTM,andassuchsetsanupperboundonreplaybeneﬁt,whichservesasausefulﬁrsttestoftheconcept.Thebuﬀerisidealisedinthesensethatitprovidesperfectmemorizationandrecallofinput.ItwouldbeinterestingtouseabiologicallyplausibleSTMsuchasAHA(Kowadloetal.,2019;2020;2021),whichweleaveforfuturework.Adisadvantageofthebuﬀerimplementationistheuseofanaivenearestneighbourlookupforrecall,whereasamorebiologicallyrealisticalgorithmsuchasAHAwouldbebetterabletogeneralisei.e.recallapreviousexemplarfromanovelexemplarpresentedasacue.Anotherdisadvantageisthatthebuﬀerstoresthefullinputimages.AnalgorithmsuchasAHAhasthepotentialtostoremoreabstractembeddings,withadditionalmemoryeﬃciencybeneﬁts.TheVGGbaselineisthemoststraightforwardandappropriatemodeltopairwithashorttermmemoryinaCLSarchitecture.ThearchitectureisshowninFigure3.IntheoriginalCFSLtaskwithoutareplaybuﬀer,theVGGispre-trainedonabackgroundsetofimages.DuringtheCFSLfew-shotphase,whenasupportsetispresentedduringtraining,itisusedtoﬁne-tunetheVGG.Whenusingreplay,therearetwostages.First,thecurrentsupportsetisstoredintheSTM,addingtorecentsupportsets.bisthebuﬀersize,measuredinsupportsets,andisatuneablehyperparameter.Second,theVGGistrainedusingthecurrentsupportset,aswellassamplesrandomlydrawnfromthereplaybuﬀer.Thenumberofsamplesisdeterminedbyasecondhyperparameterk.Weconductedahyperparametersearchtooptimiseforbandkacrossthediﬀerentexperiments.Addingthereplaybuﬀerincreasedthememoryrequirements.Forsomeexperiments,wereducedthenumberofﬁne-tuningtrainingstepstomakeitpossibletorunwithinourhardwareconstraints.Replayrequiresmorethan1supportset,andsoitisnotrelevantforoneoftheconﬁgurationsoftheinstancetests,Experiment1,whichcontainsonly1supportset.3Results3.1CFSLatscale3.1.1ReplicationTheresultsofthereplicationexperimentsaresummarisedinTable2,whichincludesreferencevaluesfrom(Antoniouetal.,2020)forcomparison.Intheexperimentsthatwereaﬀectedbycodeﬁxes(allVGGrunsandProtoNetswhereCCI>1),performanceimprovedsubstantiallyfromunusuallylowvalues,andtheperfor-manceacrossexperimentsfollowedamoreexpectedtrend(i.e.increasingaccuracyofVGGwithdecreasingnumberofclasses).ProtoNetsaresubstantiallymoreaccuratethanVGG,andperformconsistentlyacrossdiﬀerentvariationsofthepresentationof10-50totalclasses.5UnderreviewassubmissiontoTMLR
Replay BufferMemorize support set
Support set i+1
VGGVGG
Replay Buffer
Support set iCapacity = ‘b’ support sets, where b is a small number typically 2-3Support set i, i-1 … i-RSample ‘k’ samples from replay buffer
Sample from current support setComposite support set
1.Store samples2. Train VGG
Train
Figure3:Learningwithreplay.CLSsetupwithVGGLongTermMemory(LTM),pairedwithacircularbuﬀerShortTermMemory(STM).First,inamemorizationstep,theSTMtemporarilystoresrecentsupportsets.Second,inarecallstep,thememorizaeddataareusedinLTMtraining.Table2:Replicationexperiments.ReplicationofTaskDfrom(Antoniouetal.,2020)aftercorrectingerrorsintheframeworkcode.Accuracyisshownasmean±standarddeviationacross3randomseeds.Allaccuraciesarein%.ModelnameNSSCCIn-wayk-shotNumberofclassesEnsembleAccuracyAccuracyReferenceEnsembleAccuracyVGG42521037.81±0.7736.7±0.807.91±0.15VGG82522027.92±0.1026.41±0.143.86±0.06VGG31521517.76±0.3217.40±0.339.97±0.14VGG51522513.76±0.0813.10±0.036.02±0.02VGG10152509.73±0.068.36±0.053.13±0.03ProtoNets42521097.93±0.0596.98±0.0548.98±0.03ProtoNets82522096.66±0.0395.22±0.0648.44±0.03ProtoNets31521597.12±0.0695.88±0.1295.30±0.12ProtoNets51522595.93±0.1294.36±0.0591.52±0.20ProtoNets101525092.43±0.2790.24±0.1083.72±0.193.1.2ScalingTheresultsaresummarisedinTable3andFigure4.ThebestsetofhyperparametersareshowninAp-pendix8.1.Thenumberofﬁne-tuningtrainingstepsforthereplayexperiments,whichhadtobereducedtoallowittorunwithinourhardwareRAMconstraints,areshowninAppendix8.2.Increasingthenumberofclassesbyanorderofmagnitude(to200)ledtoadramaticdecreaseinaccuracy(toonlyapproximately5%).Themannerinwhichtheclasseswerepresentedmakesadiﬀerencetolearning.Firstly,rearrangingthepresentationof20classesfromBaseline2toWideorDeepimprovedperformance.6UnderreviewassubmissiontoTMLRTable3:Scalingtest.Thetableshowsexperiments(columns)foreachmodel(rows).Theseresultsareforthebestconﬁgurationsfoundthroughhyperparametersearch.Accuracyisshownin%,asmean±standarddeviationacross5randomseeds.NC=numberofclasses.ModelnameBaseline1(NSS=4,CCI=2,n-way=5,NC=10)Baseline2(NSS=8,CCI=2,n-way=5,NC=20)Wide1(NSS=4,CCI=2,n-way=10,NC=20)Wide2(NSS=4,CCI=2,n-way=100,NC=200)Deep1(NSS=20,CCI=2,n-way=2,NC=20)Deep2(NSS=80,CCI=2,n-way=5,NC=200)VGG64.95±1.0033.71±3.5454.25±0.594.22±0.5033.44±1.186.64±0.68ProtoNets86.67±1.3588.04±1.1286.92±0.4265.61±9.3188.56±0.6180.30±1.15VGG+replay81.15±0.8173.5±0.4380.74±0.7031.78±0.6560.94±0.9018.62±0.71
Figure4:Scalingexperiments.Theseresultsareforthebestconﬁgurationsfoundthroughhyperparam-etersearch.Errorbarsshow1standarddeviationacross5randomseeds.Secondly,thedecreasedperformancefromBaselines1and2toWideandDeepismorepronouncedfortheDeepexperiment.ProtoNetsshowedasimilarproﬁlewithafewexceptions.TheoverallaccuracyismuchbetterthanVGG.DoublingthenumberofclassesbetweenBaseline1and2didnotdecreaseperformance.Infactitimprovedslightly(althoughthemeansarewithinastandarddeviation).Decreaseinperformancewith200classesislargebutmodestincomparisontoVGG,approximately23%and10%proportionaldecreaseforWideandDeeprespectively.ForbothVGGandProtoNets,theWideexperimentsweremorediﬃcultcomparedtoDeep.ForVGG,replaysubstantiallyimprovedperformance,rangingfromapproximately12%to40%,withoutacleartrendovertheexperimentalconﬁgurations.7UnderreviewassubmissiontoTMLR3.2InstancetestTheresultsfortheinstancetestaresummarisedinTable4andFigure5.Weidentiﬁedthemostcom-parableconﬁgurationfromthescalingexperimentsasBaseline2,andusedthesamehyperparameters(seeAppendix8.1).Thenumberofﬁne-tuningtrainingstepsforthereplayexperiments,whichhadtobereducedtoallowittorunwithinourhardwareRAMconstraints,areshowninAppendix8.2.Thenumberof‘itemstoidentify’,whichinthiscaseareseparateinstances,isconstantat20foralloftheexperiments.ForVGG,theperformanceisvariabledependingonhowtheinstancesarepresented.Itisinthesamerangeasthescalingexperimentswiththesamenumberof‘itemstoidentify’,whichinthatcase,aretotalclasses.Accuracyincreasesastheinstancesaredistributedovermore,smallersupportsets.Thetrendofincreasingaccuracywithincreasedsupportsetsstartstoreversewhenthenumberofsupportsetsbecomestoolarge.However,thisislikelyduetothefactthatthesubsequentdecreaseinsupportsetsizebecomespathologicalatonlyone.Replayboostsperformancesubstantially,fromapproximately40%to96%forExperiment2.Theimprove-mentremainssubstantialbutdecreasesasthenumberofsupportsetsincreases,andtheirsizedecreases.ProtoNetsareagaineﬀectiveandinsensitivetoexperimentconﬁguration,evenmoresothanforthescalingexperiments.ForVGG,replaysubstantiallyimprovedperformance,rangingfrom12%to40%,beatingProtoNetsinsomecases.Table4:Instancetest.Theseresultsareforthebestconﬁgurationsfoundthroughhyperparametersearch.Accuracyisshownin%,asmean±standarddeviationacross5randomseeds.n-way=1forallexperiments,torestrictdistinguishingbetweensimilarinstancesofasingleclass.Intheinstancetest,k-shottranslatestothesizeofthesupportset.Itis1-shotinthesensethateachinstanceisonlyshownonce.NI,numberofinstances=20foralltheexperiments.ModelnameExp.1(NSS=1,k-shot=20,NI=20)Exp.2(NSS=2,k-shot=10,NI=20)Exp.3(NSS=4,k-shot=5,NI=20)Exp.4(NSS=10,k-shot=2,NI=20)Exp.5(NSS=20,k-shot=1,NI=20)VGG18.33±0.7640.35±1.4047.63±5.0047.72±2.7743.88±2.47ProtoNets92.72±1.0192.38±1.0091.94±1.0991.79±1.1691.79±1.16VGG+replayN/A96.39±1.3689.19±2.7982.77±2.1579.46±3.014DiscussionInthisstudywefoundthatforthearchitecturestested,few-shotcontinuallearningofnewclassesismorediﬃcultatscale(the‘scalingtest’)i.e.asthenumberofclassesincreasedfrom20to200.Anovel‘instancetest’wasapproximatelyasdiﬃcultforeachmodelassimilarsizedclassiﬁcationtasksinthe‘scalingtest’.ProtoNetsoutperformedVGGinalltasks(scalingtestandinstancetest),butwiththeadditionofreplay,VGG+replayaccuracyimprovedsubstantiallybecomingcomparabletoProtoNetsontheinstancetest.4.1ModelcomparisonsTheexperimentsinvolvedtwomodeltypes:VGGandProtoNets(Snelletal.,2017).It’snaturaltocomparetheirperformance,butcomparisonshouldbecautiousasProtoNetsandVGGsdonotperformthesamelearningtask.VGG(Simonyan&Zisserman,2014)isaCNNarchitecture,andistrainedforclassiﬁcation.Ontheotherhand,ProtoNets(Snelletal.,2017)learnembeddingsthatcanbeusedforclassiﬁcation.Classiﬁcationisachievedbycomparingembeddings,whichnecessitatesashort-termmemory(STM)ofthereferenceembeddingbeingmatched.Byconvention,thatmemoryisinthetestingframeworkratherthantheProtoNetarchitectureitself.Fromthisperspective,addingtheSTM(replaybuﬀer)toVGGmakesitsimilarto8UnderreviewassubmissiontoTMLR
Figure5:Instancetest.Theseresultsareforthebestconﬁgurationsfoundthroughhyperparametersearch.Errorbarsshow1standarddeviationacross5randomseeds.Intheinstancetest,k-shottranslatestothesizeofthesupportset.Itis1-shotinthesensethateachinstanceisonlyshownonce.NI,numberofinstances=20foralltheexperiments.
9UnderreviewassubmissiontoTMLRProtoNets.InthisstudytheSTMisusedforreplayonly.Infuturework,itwillalsobeusedforclassiﬁcationforrecentsamplesstillinshorttermmemory,aswasdoneinAHA(Kowadloetal.,2021).InthecaseofProtoNets,theobjectiveofrepresentationlearningistooptimizeone-shotclassgeneralization.TheobjectiveofAHAistolearnrepresentationsthatarealsohighlydiﬀerentiated,whichintheoryshouldbebetterattheinstancetest.AnotherwayinwhichVGGandProtoNetlearningdiﬀers,isthattheProtoNetarchitecturein(Antoniouetal.,2020)andourstudydoesnotactuallyacquirenewknowledgeduringtrainingandthereforedoesnotcontinuallylearn.Thereisjustoneoptimizer(forthemeta-learning‘outerloop’)thatgetstriggeredduringthe‘pre-training’phase,meaningthatitlearnsmetaparametersforanembeddingrepresentationthatwillbeoptimalfordownstreamtasks.SincetheseProtoNetsdonotlearn,theyalsodonotforgetolderknowledgeandVGG+replaymayprovemoreeﬀectivewhenconsolidatingnewknowledgecomparedtoProtoNets’ﬁne-tuningonadditionaltrainingdata(exploredfurtherbelowinthecontextofreplay,Section4.4).WenotethatintheoriginalProtoNetspaper(Snelletal.,2017),thereisﬁne-tuningofweightsaftertheinitialpre-training.4.2WidevsDeep-bigbatchesvsmanytasks(noreplay)Thewaythatdataarepresented,notjustthenumberclasses,madeadiﬀerencetolearning.Whenthenumberofclasseswasheldconstantat20,butthenumberandsizeofsupportsetswasvaried(Baseline2vsWide1orDeep1),performancewasbetterforthewideconﬁguration.However,whenthenumberofclasseswasincreasedbyanorderofmagnitudeto200(Wide2vsDeep2),theperformancewasbetterinthedeepconﬁguration,wheretheclasseswerespreadoutacrosssmallerbatches.Thiswasunexpectedgiventhatweightupdates(inVGG)occuraftereachsupportset,andthemoresupportsetsthereare,themoreitcould‘forget’earlierlearning.Itispossiblethatasthenumberofclassesincrease,thelargersupportsets(intheWideexperiments)arehardertolearn,orcausesharperforgettingbyvirtueofthefactthatmoreknowledgeisbeingacquiredinoneupdate.ProtoNets(Snelletal.,2017)areveryeﬀectiveinthescalingtest,despitenotactuallylearningduringthesetasks.Theresultsshowthattheylearntaneﬀectiveembeddingspaceduringpre-trainingforthetask.Sincenolearningtakesplace,performancecannotsuﬀerduetoforgetting.Thereforelowerperformancewhentherearealotofclasses,asinWide2andDeep2,islikelyduetogenerationofsimilarembeddingsfordiﬀerentclasses.4.3Speciﬁcinstances(noreplay)Generalperformanceontheinstancetestwassurprisinglygood.Comparedtoclassiﬁcationinexperimentconﬁgurationswithacomparable‘numberofitems’,ProtoNetsweremoreaccurateandstable.VGG+replayaccuracywasinasimilarrange,insomecasesbetterandothersworse.Theresultssuggestthatone-shotdistinguishingofspeciﬁc(verysimilar)instancesisnotmorediﬃcultthanclassiﬁcation,forthesearchitectures.ThefactthatVGGaccuracyincreasesastheinstancesaredistributedovermoresupportsets,furtherhintsthatCNNsmaybemoreeﬀectiveatcontinuallearningwithsmallersupportsets,whichisin-linewithourinterpretationofwhyDeep(more,smallersupportsets)waseasierthanWideinthescalingexperiments(explainedinmoredetailinSection4.2above).ProtoNetsareeﬀectiveintheinstancetestaswellasclassiﬁcation.Intheinstancetest,generalizationisnotrequired,andsotherepresentationsarelesslikelytooverlapreducingthepossibilityofclashes.Inaddition,noﬁne-tuningoccurs(seeearlierintheDiscussion,Section4.1),soperformanceisverystableacrossallconﬁgurations.Ifnoiseorocclusionwereintroduced,adegreeofgeneralizationwouldberequired,anditislikelythattheperformancewouldsuﬀer.Theseconditionswereexploredin(Kowadloetal.,2020).10UnderreviewassubmissiontoTMLR4.4EﬀectofreplayAshypothesized,addingreplaytoVGGenabledastrongimprovementacrosstasks.Despitetheimprove-ment,performancedidnotreachthesamelevelasProtoNetsinallscalingtests(classiﬁcation).Replayhadamoresubstantialimpactintheinstancetest,comparedtothescalingtest.VGG+replayoutperformedProtoNetswhereNSS=2,theaccuracywaswithinonestandarddeviationforNSS=4,andgraduallyfelllowerthanProtoNetsforNSS=10or20.However,thisdeteriorationmaybeduetothefactthatwereducedthenumberofﬁne-tuningiterationstoallowittorunwithinourresourceconstraints,speciﬁcallyGPURAM(seeAppendix8.2).Whyisreplaymoreeﬀectiveintheinstancetest?Thenumberofsamplesavailableforreplayislimitedtoonly2supportsets,makingitpronetooverﬁtting,whichlikelyprovidedanadvantageintheinstancetest.AlthoughVGG+replaydidnotconvincinglyallowVGGtooutperformProtoNetsontheseexperiments,theenhancedperformancehasanumberofimplicationsforfuturework.Replayimprovesperformanceofastatisticallearner(i.e.anLTM)infew-shotcontinuallearningandtheVGGlongtermmemory(LTM)couldeasilybereplacedwithothermoresophisticatedmodelssuchasResNet(Heetal.,2015).MoreoveritispossiblethattherelativeadvantageofProtoNetswilldecreaseformorecomplexdatasets,whichdidoccurforseveralmodelsintheoriginalCFSLexperiments(Antoniouetal.,2020).UndertheseconditionsaneﬀectiveLTMwithreplaycouldbemoreeﬀectivethanProtoNetsorotheralternatives.Finally,andperhapsmostsigniﬁcantly,ProtoNetsasimplementeddonotacquirenewknowledgeduringtraining,asexplainedearlierinthissection,andthereforedonotactuallydemonstratecontinuallearning.Theinabilitytoadaptisverylikelytolimitperformanceifthereisashiftinthestatisticsofdatadistribution.PushingtheselimitsandexploringweightadaptationduringtraininginthecontextofCFSLisanimportantareaforfuturework.5LimitationsThebaseframeworkmeasuresperformanceafterallthelearninghasoccurred.Incontrast,moststudiesinthecontinuallearningliteraturedocumentprogressiveperformanceasnewtasksareintroduced,whichgivesmorevisibilityintolearning.Also,themodelsandtasksusedareverycomputationallyexpensive,limitingourabilitytocomparetosomeofthemodelsusedintheoriginalwork.6FutureWorkApromisingdirectionforfutureworkistouseordevelopamoresophisticatedHippocampalmodeltoimproveCFSL.Forexample,likeinAHA(Kowadloetal.,2019;2021),anabilitytostorecompressedrepresentationsformoreeﬃcientstorage,useoftheSTM(togetherwiththeLTM)forinference,notjustconsolidation,andmultiplepathwaystosupportbothpatternseparation(instancetest)andgeneralization(classiﬁcation),andlikein(Luetal.,2022),anabilitytoselectivelyencodeandretrievememoriesformoreeﬃcientstorageandtraining.7ConclusionInthisstudywescaledtheCFSLframeworktomakeitmorecomparabletotypicalcontinuallearningexperiments.Weintroducedtwovariants,Widewithfewerlargertraining‘supportsets’andDeepwithagreaternumberofsmallersupportsets.Wealsointroducedafew-shotcontinualinstancetest,whichisimportantineverydaylife,butoftenneglectedinMachineLearning.Increasingthenumberofclassesdecreasedclassiﬁcationperformance(scalingtest)andthewaythatthedatawerepresenteddidmakeadiﬀerencetoaccuracy.Performanceinthefew-shotinstancetestwascomparabletofew-shotclassiﬁcation.AugmentingVGGwithareplayalgorithmimprovedperformancesubstantially,especiallyintheinstancetest.Inmostexperiments,replaydidnotboostperformanceaboveProtoNets,buttheutilityofreplayisclearlydemonstrated,andanLTMwithreplayarchitecturemaybemoreeﬀectivethanProtoNetsorothermodelsunderdiﬀerentexperimentalconditions.11UnderreviewassubmissiontoTMLRContinualfew-shotlearning,forbothclassesandinstances,isanecessarycapabilityforagentsoperatinginunfamiliarandchangingenvironmentsaswellasprovidingnewpossibilitiesforbusinessapplicationswheretheseconditionsmayoftenoccur.Thisstudyisoneoftheﬁrststepsinthatdirection,combiningcontinualandfew-shotlearning,anditdemonstratesthatHippocampal-inspiredreplayisapromisingapproach.ReferencesAntreasAntoniou,MassimilianoPatacchiola,MateuszOchal,andAmosStorkey.DeﬁningBenchmarksforContinualFew-ShotLearning.arXivpreprintarXiv:2004.11967,2020.MatthiasDelange,RahafAljundi,MarcMasana,SarahParisot,XuJia,AlesLeonardis,GregSlabaugh,andTinneTuytelaars.Acontinuallearningsurvey:Defyingforgettinginclassiﬁcationtasks.IEEETransactionsonPatternAnalysisandMachineIntelligence,pp.1–29,2021.ISSN19393539.doi:10.1109/TPAMI.2021.3057446.KaimingHe,XiangyuZhang,ShaoqingRen,andJianSun.DeepResidualLearningforImageRecognition.ProceedingsoftheIEEEComputerSocietyConferenceonComputerVisionandPatternRecognition,pp.770–778,dec2015.ISSN10636919.doi:10.48550/arxiv.1512.03385.URLhttps://arxiv.org/abs/1512.03385v1.GideonKowadlo,AbdelrahmanAhmed,andDavidRawlinson.AHA!an’ArtiﬁcialHippocampalAlgorithm’forEpisodicMachineLearning.arXivpreprintarxiv:1909.10340,2019.GideonKowadlo,AbdelrahmanAhmed,andDavidRawlinson.UnsupervisedOne-ShotLearningofBothSpeciﬁcInstancesandGeneralisedClasseswithaHippocampalArchitecture.InMarcusGallagher,NourMoustafa,andErandiLakshika(eds.),AI2020:AdvancesinArtiﬁcialIntelligence,pp.395–406,Cham,2020.SpringerInternationalPublishing.ISBN978-3-030-64984-5.GideonKowadlo,AbdelrahmanAhmed,andDavidRawlinson.One-shotlearningforthelongterm:consol-idationwithanartiﬁcialhippocampalalgorithm,2021.URLhttp://arxiv.org/abs/2102.07503.DharshanKumaran,DemisHassabis,andJamesLMcClelland.WhatLearningSystemsdoIntelligentAgentsNeed?ComplementaryLearningSystemsTheoryUpdated.TrendsinCognitiveSciences,20(7):512–534,2016.BrendenMLake,RuslanSalakhutdinov,andJoshuaBTenenbaum.Human-levelconceptlearningthroughprobabilisticprograminduction.Science,350(6266):1332–1338,2015.ISSN10959203.doi:10.1126/science.aab3050.QihongLu,UriHasson,andKennethANorman.Aneuralnetworkmodelofwhentoretrieveandencodeepisodicmemories.eLife,11:e74445,feb2022.ISSN2050-084X.doi:10.7554/eLife.74445.URLhttps://doi.org/10.7554/eLife.74445.JamesLMcClelland,BruceLMcNaughton,andRandallCO’Reilly.Whytherearecomplementarylearningsystemsinthehippocampusandneocortex:Insightsfromthesuccessesandfailuresofconnectionistmodelsoflearningandmemory.PsychologicalReview,102(3):419–457,1995.ISSN0033295X.doi:10.1037/0033-295X.102.3.419.MichaelMcCloskeyandNealJ.Cohen.CatastrophicInterferenceinConnectionistNetworks:TheSequentialLearningProblem.PsychologyofLearningandMotivation-AdvancesinResearchandTheory,24(C):109–165,jan1989.ISSN0079-7421.doi:10.1016/S0079-7421(08)60536-8.RandallCO’Reilly,RajanBhattacharyya,MichaelDHoward,andNicholasKetz.Complementarylearningsystems.CognitiveScience,38(6):1229–1248,2014.ISSN03640213.doi:10.1111/j.1551-6709.2011.01214.x.GermanIParisi,JunTani,CorneliusWeber,andStefanWermter.Lifelonglearningofspatiotemporalrepresentationswithdual-memoryrecurrentself-organization.Frontiersinneurorobotics,12:78,2018.12UnderreviewassubmissiontoTMLRKarenSimonyanandAndrewZisserman.Verydeepconvolutionalnetworksforlarge-scaleimagerecognition.3rdInternationalConferenceonLearningRepresentations,ICLR2015-ConferenceTrackProceedings,92014.doi:10.48550/arxiv.1409.1556.URLhttps://arxiv.org/abs/1409.1556v6.JakeSnell,KevinSwersky,andRichardSZemel.PrototypicalNetworksforFew-shotLearning.InAdvancesinneuralinformationprocessingsystems,pp.4077–4087,2017.OriolVinyals,CharlesBlundell,TimothyLillicrap,andKorayKavukcuoglu.MatchingNetworksforOneShotLearning.arXiv,2017.8Appendix8.1HyperparametersforVGG+replayTable5:Hyperparameters.OptimizedHyperparametersforthescalingtestandVGG+replay.lristhelearningrateoftheLongTermMemory,bisthereplaybuﬀersizemeasuredinnumberofsupportsets,andkisthenumberofsamplesrandomlydrawnfromthereplaybuﬀerfortrainingtheLongTermMemory.ExperimentHyperparametersBaseline1512ﬁlters,3stages,lr=0.01,b=2,k=10Baseline2128ﬁlters,3stages,lr=0.01,b=4,k=10Wide1256ﬁlters,3stages,lr=0.01,b=2,k=20Wide2128ﬁlters,2stages,lr=0.01,b=2,k=50Deep1256ﬁlters,3stages,lr=0.01,b=5,k=10Deep2256ﬁlters,3stages,lr=0.01,b=5,k=108.2Fine-tuningstepsTable6:Fine-tuningforscalingtest.Thenumberofﬁne-tuningtrainingstepsfortheVGG+replayscalingtest.ExperimentFine-tuningtrainingstepsBaseline1120Baseline260Wide130Wide230Deep15Deep25Table7:Fine-tuningfortheinstancetest.Thenumberofﬁne-tuningtrainingstepsfortheVGG+replayinstancetest.ExperimentFine-tuningtrainingstepsExp.1N/AExp.2120Exp.3120Exp.460Exp.53013