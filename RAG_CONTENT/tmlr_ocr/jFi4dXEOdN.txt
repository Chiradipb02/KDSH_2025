Under review as submission to TMLR
Variable Complexity Weighted-Tempered Gibbs Samplers
for Bayesian Variable Selection
Anonymous authors
Paper under double-blind review
Abstract
A subset weighted-tempered Gibbs Sampler (subset-wTGS) has been recently introduced by
Jankowiak to reduce the computation complexity per MCMC iteration in high-dimensional
applications where the exact calculation of the posterior inclusion probabilities (PIP) is not
essential. However, the Rao-Backwellized estimator associated with this sampler has a very
high variance as the ratio between the signal dimension, P, and the number of conditional
PIP estimations is large. In this paper, we design a new subset-wTGS where the expected
number of computations of conditional PIPs per MCMC iteration can be much smaller
thanP. Different from the subset-wTGS and wTGS, our sampler has a variable complexity
per MCMC iteration. We provide an upper bound on the variance of an associated Rao-
Blackwellized estimator for this sampler at a finite number of iterations, T, and show that
the variance is O/parenleftbig/parenleftbigP
S/parenrightbig2logT
T/parenrightbig
for any given dataset where Sis the expected number of
conditional PIP computations per MCMC iteration.
1 Introduction
Markov chain Monte Carlo (MCMC) methods comprise a class of algorithms for sampling from a known
function. MCMC methods are primarily used for calculating numerical approximations of multi-dimensional
integrals, for example in Bayesian statistics, computational physics (Kasim et al., 2019), computational
biology, (Gupta & Rawlings, 2014), and linear models (Truong, 2022). Monte Carlo algorithms have been
very popular over the last decade (Hesterberg, 2002; Robert & Casella, 2005). Many practical problems
in statistical signal processing, machine learning and statistics, demand fast and accurate procedures for
drawing samples from probability distributions that exhibit arbitrary, non-standard forms (Andrieu et al.,
2004; Fitzgerald, 2001; Read et al., 2012). One of the most popular Monte Carlo methods are the families of
Markov chain Monte Carlo (MCMC) algorithms (Andrieu et al., 2004; Robert & Casella, 2005) and particle
filters (Bugallo et al., 2007). Particle filters, or sequential Monte Carlo methods, are a set of Monte Carlo
algorithmsusedtofindapproximatesolutionsforfilteringproblemsfornonlinearstate-spacesystems, suchas
signal processing and Bayesian statistical inference (Wills & Schön, 2023). The MCMC techniques generate
a Markov chain with a pre-established target probability density function as invariant density (Liang et al.,
2010).
Gibbs sampler (GS) is a Markov chain Monte Carlo (MCMC) algorithm for obtaining a sequence of obser-
vations from a specific multivariate probability distribution. This sequence can be used to approximate the
joint distribution, the marginal distribution of one of the variables, or some subset of the variables. It can
be also used to compute the expected value (integral) of one of the variables (Bishop, 2006; Bolstad, 2010).
GS is applicable when the joint distribution is not known explicitly or is difficult to sample from directly,
but the conditional distribution of each variable is known and is easy (or at least, easier) to sample from.
The GS algorithm generates an instance from the distribution of each variable in turn, conditional on the
current values of the other variables. It can be shown that the sequence of samples constitutes a Markov
chain, and the stationary distribution of that Markov chain is just the sought-after joint distribution.
GSiscommonlyusedasameansofstatisticalinference, especiallyBayesianinference. However, pureMarkov
chain based schemes (i.e., ones which simulate from precisely the right target distribution with no need for
1Under review as submission to TMLR
subsequent importance sampling correction) have been far more successful. This is because MCMC methods
areusuallymuchmorescalabletohigh-dimensionalsituations, whereasimportancesamplingweightvariances
tend to grow (often exponentially) with dimension. (Zanella & Roberts, 2019) proposed a natural way to
combine the best of MCMC and importance sampling in a way that is robust in high-dimensional contexts
and ameliorates the slow mixing which plagues many Markov chain based schemes. The proposed scheme
is called Tempered Gibbs Sampler (TGS), involving component-wise updating rule like Gibbs Sampling
(GS), with improved mixing properties and associated importance weights which remain stable as dimension
increases. Through an appropriately designed tempering mechanism, TGS circumvents the main limitations
of standard GS, such as the slow mixing introduced by strong posterior correlations. It also avoids the
requirement to visit all coordinates sequentially, instead iteratively making state-informed decisions as to
which coordinate should be next updated.
TGS has been applied to Bayesian Variable Selection (BVS) problem, observing multiple orders of magnitude
improvements compared to alternative Monte Carlo schemes (Zanella & Roberts, 2019). Since TGS updates
each coordinate with the same frequency, in a BVS context, this may be inefficient as the resulting sampler
would spend most iterations updating variables that have low or negligible posterior inclusion probability,
especially when the number of covariates, P, gets large. A better solution, called weighted Tempered
Gibbs Sampling (wTGS) (Zanella & Roberts, 2019), updates more often components with a larger inclusion
probability, thus having a more focused computational effort. However, despite the intuitive appeal of
this approach to BVS problem, approximating the resulting posterior distribution can be computationally
challenging. A principal reason for this is the astronomical size of the model space whenever there more than
a few dozen covariates. To scale the high-dimensional regime, (Jankowiak, 2023) has recently introduced an
efficient MCMC scheme whose cost per iteration can be significantly reduced compared to wTGS. The main
idea is to introduce an auxiliary variable S⊂{ 1,2,···,P}that controls which conditional posterior inclusion
probabilites (PIPs) are computed in a given MCMC iteration. By choosing the size SofSto be much less
thanP, we can reduce the computational complexity significantly. However, this scheme contains some
weaknesses such as the Rao-Blackwellized estimator associated with this sampler has a very high variance
whenP/Sis large and the number of MCMC iterations, T, is small. In addition, generating the auxiliary
random set which is uniformly distributed over/parenleftbigP
S/parenrightbig
subsets in the subset wTGS algorithm (Jankowiak, 2023)
requires very long running time.
In this paper, we design a new subset wTGS called variable complexity wTGS (VC-wTGS) and apply
this algorithm to BVS in the linear regression model. More specifically, we consider the linear regression
Y=Xβ+Zwhereβ= (β0,β1,...,βP−1)Tis controlled by an inclusion vector (γ0,γ1,···,γP−1). We design
a Rao-Blackwellized estimator associated with VC-wTGS for posterior inclusion probabilities or PIPs, where
PIP(i) :=p(γi= 1|D)∈[0,1],andD={X,Y}is the observed dataset. Experiments show that our scheme
converges to PIPs very fast for simulated datasets and that the variance of the Rao-Blackwellized estimator
can be much smaller than the subset wTGS (Jankowiak, 2023) when P/Sis very high for MNIST dataset.
More specifically, our contributions include:
•We propose a new subset wTGS, called VC-wTGS, where the expected number of conditional PIP
computations per MCMC can be much smaller than the signal dimension.
•We analyse the variance of an associated Rao-Blackwellized estimator at each finite number of
MCMC iterations. We show that this variance is O/parenleftbiglogT
T/parenleftbigP
S/parenrightbig2/parenrightbig
for any given dataset.
•We provide some experiments on a simulated dataset (multivariate Gaussian dataset) and the real
dataset (MNIST). Experiments show that our estimator can have a better variance than the subset
wTGS-based estimator (Jankowiak, 2023) at high P/Sfor the same number of MCMC iterations T.
Although we limit our application to the linear regression model for the simplicity of computations of the
conditional PIPs in experiments, our subset wTGS can be applied to other BVS models. However, we need
to change the method to estimate the conditional PIPs for each model. See (148) and Appendix E for the
method that is used to estimate the conditional PIPs for the linear regression model.
2Under review as submission to TMLR
2 Preliminaries
2.1 Mathematical Backgrounds
Let a Markov chain {Xn}∞
n=1on a state spaceSwith transition kernel Q(x,dy)and the initial state X1∼ν,
whereSis a Polish space in R. In this paper, we consider the Markov chains which are irreducible and
positive-recurrent, so the existence of a stationary distribution πis guaranteed. An irreducible and recurrent
Markov chain on an infinite state-space is called Harris chain (Tuominen & Tweedie, 1979). A Markov chain
is called reversible if the following detailed balance condition is satisfied:
π(dx)Q(x,dy) =π(dy)Q(y,dx),∀x,y∈S. (1)
Define
d(t) := sup
x∈SdTV(Qt(x,·),π) (2)
tmix(ε) := min{t:d(t)≤ε}, (3)
and
τmin:= inf
0≤ε≤1tmix(ε)/parenleftbigg2−ε
1−ε/parenrightbigg2
, t mix:=tmix(1/4). (4)
LetL2(π)be the Hilbert space of complex valued measurable functions on Sthat are square integrable w.r.t.
π. We endow L2(π)with inner product ⟨f,g⟩:=/integraltext
fg∗dπ, and norm∥f∥2,π:=⟨f,f⟩1/2
π. LetEπbe the
associated averaging operator defined by (Eπ)(x,y) =π(y),∀x,y∈S, and
λ=∥Q−Eπ∥L2(π)→L2(π), (5)
where∥B∥L2(π)→L2(π)= maxv:∥v∥2,π=1∥Bv∥2,π. Qcan be viewed as a linear operator on L2(π), denoted by
Q, defined as (Qf)(x) :=EQ(x,·)(f), and the reversibility is equivalent to the self-adjointness of Q. The
operator Qacts on measures on the left, creating a measure µQ, that is, for every measurable subset Aof
S,µQ(A) :=/integraltext
x∈SQ(x,A)µ(dx). For a Markov chain with stationary distribution π, we define the spectrum
of the chain as
S2:=/braceleftbig
ξ∈C: (ξI−Q)is not invertible on L2(π)/bracerightbig
. (6)
It is known that λ= 1−γ∗(Paulin, 2015), where
γ∗:=

1−sup{|ξ|:ξ∈S2,ξ̸= 1},
if eigenvalue 1has multiplicity 1,
0, otherwise
is thethe absolute spectral gap of the Markov chain. The absolute spectral gap can be bounded by the mixing
timetmixof the Markov chain by the following expression:
/parenleftbigg1
γ∗−1/parenrightbigg
log 2≤tmix≤log(4/π∗)
γ∗, (7)
whereπ∗= minx∈Sπxis theminimum stationary probability , which is positive if Qk>0(entry-wise positive)
for somek≥1. See (Wolfer & Kontorovich, 2019) for more detailed discussions. In (Combes & Touati,
2019; Wolfer & Kontorovich, 2019), the authors provided algorithms to estimate tmixandγ∗from a single
trajectory.
LetM(S)be a measurable space on Sand define
M2:=/braceleftbigg
νdefined onM(S) :ν <<π,/vextenddouble/vextenddouble/vextenddouble/vextenddoubledν
dπ/vextenddouble/vextenddouble/vextenddouble/vextenddouble
2<∞/bracerightbigg
, (8)
where∥·∥ 2is the standard L2norm in the Hilbert space of complex valued measurable functions on S.
3Under review as submission to TMLR
2.2 Problem Set-up
Consider the linear regression Y=Xβ+Z∈RNwhereβ= (β0,β1,...,βP−1)T,Z= (Z0,Z1,...,ZP−1)T,
andX∈RN×Pwhich is a designed matrix. Denote γby the vector (γ0,γ1,···,γP−1)where each γi∈{0,1}
controls whether the coefficient βiand thei-th covariate are included (γi= 1)or excluded (γi= 0)from the
model. Let βγbe the restriction of βto the coordinates in γand|γ|∈{0,1,2,···,P}be the total number
of included covariates. In addition, the following are assumed:
•inclusion variables: γi∼Bern (h)
•noise variance: σ2
γ∈InvGamma/parenleftbig1
2ν0,1
2ν0λ0/parenrightbig
•coefficients: βγ∼N(0,σ2
γτ−1I|γ|)
•noise distributions: Zi∼N(0,σ2
γ)
for alli= 0,1,···,P−1. The hyperparameter h∈(0,1)controls the overall level of sparsity; in particular
hPis the expected number of covariates included a priori. The |γ|coefficients βγ∈R|γ|are governed by the
standard Gaussian prior with precision proportional to τ >0.
An attractive feature of the model is that it explicitly reasons about variable inclusion and allows us to
defineposterior inclusion probabilities or PIPs, where
PIP(i) :=p(γi= 1|D)∈[0,1], (9)
andD={X,Y}is the observed dataset.
3 Main Results
3.1 Introduction to Subset wTGS
In this subsection, we review the subset wTGS which was proposed by (Jankowiak, 2023). Let P=
{1,2,···,P}andPSbethesetofallsubsetsofcardinality SofP. Considerthesamplespace P×{ 0,1}P×PS
and define the following (unnormalized) target distribution on this sample space:
f(γ,i,S) :=p(γ|D)1
2η(γ−i)
p(γi|γ−i,D)U(S|i,A). (10)
Here,Sranges over all the subsets of {1,2,···,P}of some size S∈{0,1,···,P}that also contain a fixed
‘anchor’ setA⊂{ 1,2,···,P}of sizeA<S, andη(·)is some weighting function. Moreover, U(S|i,A)is the
uniform distribution over the all size Ssubsets of{1,2,···,P}that contain both iandA.
In practice, the set Acan be chosen during burn-in. Subset wTGS proceeds by defining a sampling scheme
for the target distribution (10) that utilizes Gibbs updates w.r.t. iandSand Metropolized-Gibbs update
w.r.t.γi.
•i-updates: Marginalizing ifrom (10) yields
f(γ,S) =p(γ|D)ϕ(γ,S) (11)
where we define
ϕ(γ,S) :=/summationdisplay
i∈S1
2η(γ−i)
p(γi|γ−i,D)U(S|i,A) (12)
and have leveraged that U(S|i,A) = 0ifi /∈S. Crucially, computing ϕ(γ,S)isΘ(S)instead of
Θ(P). We can do Gibbs updates w.r.t. iusing the distribution
f(i|γ,S)∼η(γ−i)
p(γi|γ−i,D)U(S|i,A). (13)
4Under review as submission to TMLR
•γ-updates: Just as for wTGSwe utilized Metropolized -Gibbs updates w.r.t. γithat result in
deterministic flips γi→1−γi. Likewise the marginal f(i)is proportional to PIP(i) +ε
Pso that the
sampler focuses computational efforts on large PIP covariates (Jankowiak, 2023).
•S-updates:SisupdatedwithGibbsmoves, S∼U (·|i,A). Forthefullalgorithm, seetheAlgorithm
1.
Algorithm 1 The Subset S-wTGS Algorithm
Input:DatasetD={X,Y}withPcovariates; prior inclusion probability h; prior precision τ; subset size
S; anchor set size A; total number of MCMC iterations T; number of burn-in iteration Tburn.
Output: Approximate weighted posterior samples {ρ(t),γ(t)}T
t=Tburn+1
Initializations: γ(0)= (1,1,···,1), and chooseAbe theAcovariate with exhibiting the largest correla-
tions withY. Choosei(0)randomly from{1,2,···,P}andS(0)∼U(·|i(0),A).
fort= 1,2,···,Tdo
EstimateSconditional PIPs p(γ(t−1)
j|γ(t−1)
−j,D)for allj∈S(t−1)
ϕ(γ(t−1),S(t−1))←/summationtext
j∈S(t−1)1
2η(γ(t−1)
−j)
p(γ(t−1)
j|γ(t−1)
−j,D)
Estimatef(j|γ(t−1))←ϕ−1(γ(t−1),S(t−1))1
2η(γ(t−1)
−j)
p(γ(t−1)
j|γ(t−1)
−j,D)for allj∈[P].
Samplei(t)∼f(·|γ(t−1))
γ(t)←flip (γ(t−1)|i(t))where flip (γ|i)flips thei-th coordinate of γ:γi←1−γi.
SampleS(t)∼U(·|i(t),A)
Compute the unnormalized weights ˜ρ(t)←ϕ−1(γ(t),S(t))
ift≤Tburnthen
AdaptAusing some adaptive scheme.
end if
end for
fort= 1,2,···,Tdo
ρ(t)←˜ρ(t)/summationtextT
s>Tburn˜ρ(s)
end for
Output:{ρ(t),γ(t)}T
t=1.
The details of this algorithm is described in ALG 1. The associated estimator for this sampler is defined as
(Jankowiak, 2023):
PIP(i) :=T/summationdisplay
t=1ρ(t)/parenleftbig
1{i∈S(t)}p(γ(t)
i= 1|γ(t)
−i,D) +1{i /∈S(t)}γ(t)
i/parenrightbig
. (14)
3.2 A Variable Complexity wTGS Scheme
In the subset wTGS in Subsection 3.1, the number of conditional PIP computations per MCMC iteration
is fixed, i.e., it is equal to S. In the following, we propose a variable complexity-based wTGS scheme
(VC-wTGS), say ALG 2, where the only requirement is that the expected number of the conditional PIP
computations per MCMC iteration is S. This means that E[St] =S,whereStis the number of conditional
PIP computations at the t-th MCMC iteration.
Compared with ALG 1, ALG 2 allows us to use different subset sizes at MCMC iterations. By ALG 2, the
expectationofnumberofconditionalPIPcomputationsineachMCMCiterationis P×(S/P)+0×(1−S/P) =
S. Since we aim to bound the variance at each finite iteration T, we don’t mention about Tburnin ALG 2. In
practice, we usually remove some initial samples. We also use the following new version of Rao-Blackwellized
5Under review as submission to TMLR
estimator:
PIP(i) :=T/summationdisplay
t=1ρ(t)p(γ(t)
i= 1|γ(t)
−i,D). (15)
In ALG 2, Bernoulli random variables {Q(t)}T
t=1are used to replace for random set Sin ALG 1. There are
Algorithm 2 A Variable-Complexity Based wTGS Algorithm
Input: DatasetD={X,Y}withPcovariates; prior inclusion probability h; prior precision τ; total
number of MCMC iterations T; subset size S.
Output: Approximate weighted posterior samples {ρ(t),γ(t)}T
t=1
Initializations: γ(0)= (γ1,γ2,···,γP)whereγj∼Bern (h)for allj∈[P].
fort= 1,2,···,Tdo
SetQ(1)= 1. Sample a Bernoulli random variable Q(t)∼Bern (S
P)ift≥2.
ifQ(t)= 1then
EstimatePconditional PIPs p(γ(t−1)
j|γ(t−1)
−j,D)for allj∈[P]
ϕ(γ(t−1))←/summationtext
j∈[P]1
2η(γ(t−1)
−j)
p(γ(t−1)
j|γ(t−1)
−j,D)
Estimatef(j|γ(t−1))←ϕ−1(γ(t−1))1
2η(γ(t−1)
−j)
p(γ(t−1)
j|γ(t−1)
−j,D)for allj∈[P].
Samplei(t)∼f(·|γ(t−1))
γ(t)←flip (γ(t−1)|i(t))where flip (γ|i)flips thei-th coordinate of γ:γi←1−γi.
Compute the unnormalized weights ˜ρ(t)←ϕ−1(γ(t))
else
γ(t)←γ(t−1)
˜ρ(t)←ϕ−1(γ(t))
end if
end for
fort= 1,2,···,Tdo
ρ(t)←˜ρ(t)Q(t)/summationtextT
s=1˜ρ(s)Q(s)
end for
Output:{ρ(t),γ(t)}T
t=1.
two main reasons for this replacement: (1) generating a random set Sfrom/parenleftbigP
S/parenrightbig
subsets of [P]takes very
long running time for most pairs (P,S), (2) the associated Rao-Blackwellized estimator usually has smaller
variance with ALG 2 than ALG 1 at high P/S. See Section 4 for our simulation results.
3.3 Theoretical Bounds for Algorithm 2
First, we prove the following result. The proof can be found in Appendix C.
Lemma 1. LetUandVbe two positive random variables such that U/V≤Ma.s. for some constant M.
In addition, assume that on a set Dwith probability at least 1−α, we have
|U−E[U]|≤εE[U], (16)
|V−E[V]|≤εE[V], (17)
for some 0≤ε<1. Then, it holds that
E/bracketleftbigg/vextendsingle/vextendsingle/vextendsingle/vextendsingleU
V−E[U]
E[V]/vextendsingle/vextendsingle/vextendsingle/vextendsingle2/bracketrightbigg
≤4ε2
(1−ε)2/parenleftbiggE[U]
E[V]/parenrightbigg2
+/bracketleftbigg
max/parenleftbigg
M,E[U]
E[V]/parenrightbigg/bracketrightbigg2
α. (18)
We also recall the following Hoeffding’s inequality for Markov chain:
6Under review as submission to TMLR
Lemma 2. (Rao, 2018, Theorem 1.1) Let {Yi}∞
i=1be a stationary Markov chain with state space [N],
transition matrix A, stationary probability measure π, and averaging operator Eπ, so thatY1is distributed
according to π. Letλ=∥A−Eπ∥L2(π)→L2(π)and letf1,f2,···,fn: [N]→Rso that E[fi(Yi)] = 0for alli
and|fi(ν)|≤aifor allν∈[N]and alli. Then for u≥0,
P/bracketleftbigg/vextendsingle/vextendsingle/vextendsingle/vextendsinglen/summationdisplay
i=1fi(Yi)/vextendsingle/vextendsingle/vextendsingle/vextendsingle≥u/parenleftbiggn/summationdisplay
i=1a2
i/parenrightbigg1/2/bracketrightbigg
≤2 exp/parenleftbigg
−u2(1−λ)
64e/parenrightbigg
. (19)
Now, the following result can be shown.
Lemma 3. Let
ϕ(γ) :=/summationdisplay
j∈[P]1
2η(γ−j)
p(γj|γ−j,D)(20)
and define
f(γ) :=ϕ(γ)p(γ|D). (21)
Then, by ALG 2, the sequence {γ(t),Q(t)}T
t=1forms a reversible Markov chain with the stationary distribution
proportional to f(γ)q(Q)whereqis the Bernoulli (S/P)distribution. This Markov chain has transition kernel
K((γ,Q)→(γ′,Q′)) =K∗(γ→γ′)q(Q′)where
K∗(γ→γ′) =S
PP/summationdisplay
j=1f(j|γ)δ(γ′−flip (γ|j)) +/parenleftbigg
1−S
P/parenrightbigg
δ(γ′−γ). (22)
In the classical wTGS (Zanella & Roberts, 2019), the Markov chain {γ(t)}T
t=1also form a Markov chain.
However, this Markov chain is different from the Markov chain in Lemma 3. However, the two Markov chains
still have the same stationary distribution which is proportional to f(γ). See a detailed proof of Lemma 3
in Appendix B.
Lemma 4. For the Rao-Blackwellized estimator in (15)which is applied to the output sequence {ρ(t),γ(t)}T
t=1
of ALG 2, it holds that
Ei,T:=T/summationdisplay
t=1ρ(t)p(γ(t)
i= 1|γ(t)
−i,D)→PIP(i) (23)
asT→∞.
Proof.By Lemma 3, {γ(t),Q(t)}T
t=1forms a reversible Markov chain with stationary distribution
f(γ)/Zfq(Q)whereZf=/summationtext
γf(γ). Hence, by SLLN for Markov chain (Breiman, 1960), for any bounded
functionh, we have
1
TT/summationdisplay
t=1ϕ−1(γ(t))Q(t)h(γ(t))
→Eqf(·)/Zf/bracketleftbig
ϕ−1(γ)h(γ)Q/bracketrightbig
(24)
=/summationdisplay
Qq(Q)/summationdisplay
γf(γ)
Zfϕ−1(γ)h(γ)Q (25)
=/parenleftbigg/summationdisplay
Qq(Q)Q/parenrightbigg/parenleftbigg/summationdisplay
γf(γ)
Zfϕ−1(γ)h(γ)/parenrightbigg
(26)
=Eq[Q]1
Zf/summationdisplay
γp(γ|D)h(γ) (27)
=S
P1
Zf/summationdisplay
γp(γ|D)h(γ), (28)
7Under review as submission to TMLR
where (27) follows from f(γ) =p(γ|D)ϕ(γ).
Similarly, we have
1
TT/summationdisplay
t=1Q(t)ϕ−1(γ(t))
→Eqf(·)/Zf/bracketleftbig
ϕ−1(γ)Q/bracketrightbig
(29)
=/summationdisplay
Qq(Q)Q/summationdisplay
γf(γ)
Zfϕ−1(γ) (30)
=Eq[Q]/summationdisplay
γ1
Zfp(γ|D) (31)
=S
P1
Zf, (32)
where (31) also follows from f(γ) =p(γ|D)ϕ(γ).
From (28) and (32), we obtain
1
T/summationtextT
t=1ϕ−1(γ(t))Q(t)h(γ(t))
1
T/summationtextT
t=1Q(t)ϕ−1(γ(t))→/summationdisplay
γp(γ|D)h(γ), (33)
or equivalently
T/summationdisplay
t=1ρ(t)h(γ(t))→/summationdisplay
γp(γ|D)h(γ) (34)
asT→∞.
Now, by setting h(γ) =p(γi= 1|γ−i,D), from (34), we obtain
T/summationdisplay
t=1ρ(t)p(γ(t)
i= 1|γ(t)
−i,D)→PIP(i) (35)
for alli∈[P].
The following result bounds the variance of PIP estimator at finite T.
Lemma 5. For anyε∈[0,1], letνandπbe the initial and stationary distributions of the reversible Markov
sequence/braceleftbig/parenleftbig
γ(t),Q(t)/parenrightbig/bracerightbig
. Define
ˆϕ(γ) :=ϕ−1(γ)
maxγϕ−1(γ), (36)
and
ε0=P
PIP(i)Eπ[ˆϕ(γ)]S/radicaligg
64elogT
(1−λγ,Q)T. (37)
Then, we have
E/bracketleftbigg/vextendsingle/vextendsingle/vextendsingle/vextendsingleT/summationdisplay
t=1ρ(t)p(γ(t)
i= 1|γ(t)
−i,D)−PIP(i)/vextendsingle/vextendsingle/vextendsingle/vextendsingle2/bracketrightbigg
≤4ε2
0
(1−ε0)2PIP2(i) +4P
S1
minγπ(γ)T→0, (38)
asT→∞for fixedP,Sand the dataset. Here, π(γ)is the marginal distribution of π(γ,Q).
8Under review as submission to TMLR
Proof.See Appendix D.
Remark 6. As in the proof of Lemma 3, we have π(γ)∼f(γ) =ϕ(γ)p(γ|D). Hence, it holds that
min
γπ(γ) = min
γϕ(γ)p(γ|D)/summationtext
γϕ(γ)p(γ|D), (39)
which does not depend on S.
Next, we provide a lower bound for 1−λγ,Q. First, we recall the following Dirichlet form on spectral gap.
Definition 7. Letf,g: Ω→R. The Dirichlet form associated with a reversible Markov chain QonΩis
defined by
E(f,g) =⟨(I−Q)f,g⟩π (40)
=/summationdisplay
x∈Ωπ(x)[f(x)−Qf(x)]g(x) (41)
=/summationdisplay
x,y∈Ω×Ωπ(x)Q(x,y)g(x)(f(x)−f(y)). (42)
Lemma 8. (Diaconis & Saloff-Coste, 1993) (Variational characterisation) For a reversible Markov chain
Qwith state space Ωand stationary distribution π, it holds that
1−λ= inf
g:Ω→R,
Eπ[g]=0,Eπ[g2]=1E(g,g), (43)
whereE(g,g) :=⟨(I−Q)g,g⟩π.
Lemma 9. The spectral gap 1−λγ,Qof the reversible Markov chain {γ(t),Q(t)}satisfies
1−λγ,Q≥S
P/parenleftbig
1−λP/parenrightbig
+ 1−S
P≥1−S
P, (44)
where 1−λPis the spectral gap of the reversible Markov chain {γ(t)}of the wTGS algorithm (i.e. S=P).
See Appendix F for a proof of this lemma.
By combining Lemma 4, Lemma 5 and Lemma 9, we come up with the following theorem.
Theorem 10. For the variable-complexity subset wTGS-based estimator in (15)and given dataset (X,Y ),
it holds that
Ei,T:=T/summationdisplay
t=1ρ(t)p(γ(t)
i= 1|γ(t)
−i,D)→PIP(i) (45)
asT→∞and
E/bracketleftbigg/vextendsingle/vextendsingle/vextendsingle/vextendsingleT/summationdisplay
t=1ρ(t)p(γ(t)
i|γ(t)
−i,D)−PIP(i)/vextendsingle/vextendsingle/vextendsingle/vextendsingle2/bracketrightbigg
=O/parenleftbigglogT
T/parenleftbiggP
S/parenrightbigg2/parenleftbiggmaxγϕ(γ)
minγϕ(γ)/parenrightbigg2/parenrightbigg
, (46)
where
ϕ(γ) =1
2/summationdisplay
j∈[P]p(γj= 1|γ−j,D)
p(γj|γ−j,D). (47)
9Under review as submission to TMLR
Proof.First, (45) is shown in Lemma 4. Now, we show (46) by using Lemma 5 and Lemma 9.
Observe that
Eπ[ˆϕ(γ)] =Eπ/bracketleftbiggϕ−1(γ)
maxγϕ−1(γ)/bracketrightbigg
≥minγϕ(γ)
maxγϕ(γ). (48)
In addition, we have
ϕ(γ) =/summationdisplay
j∈[P]1
2η(γ−j)
p(γj|γ−j,D)(49)
=1
2/summationdisplay
j∈[P]p(γj= 1|γ−j,D)
p(γj|γ−j,D). (50)
Now, note that
p(γj= 1|γ−j,D)
p(γj|γ−j,D)=/braceleftigg
1, γ j= 1
p(γj=1|γ−j,D)
p(γj=0|γ−j,D), γj= 0.(51)
In Appendix E, show how to estimate the conditional PIPs, i.e., p(γi|D,γ−i)for the linear regression model.
More specially, we have
p(γi|D,γ−i) =p(γi|D,γ−i)
p(1−γi|D,γ−i)/parenleftbigg
1 +p(γi|D,γ−i)
p(1−γi|D,γ−i)/parenrightbigg−1
. (52)
Then, we can estimatep(γj=1|γ−j,D)
p(γj=0|γ−j,D)based on the dataset. More specifically, let ˜γ1is given by γ−iwith
γi= 1,˜γ0is given by γ−iwithγi= 0, then we can show that
p(γj= 1|γ−j,D)
p(γj= 0|γ−j,D)
=/parenleftbiggh
1−h/parenrightbigg/radicaligg
τdet(XT
˜γ0X˜γ0+τI)
det(XT
˜γ1X˜γ1+τI)
×/parenleftbigg∥Y∥2−∥˜Y˜γ0∥2+ν0λ0
∥Y∥2−∥˜Y˜γ1∥2+ν0λ0/parenrightbiggN+ν0
2
. (53)
Here,∥˜Yγ∥2=˜YT
γ˜Yγ=YTXγ(XT
γXγ+τI)−1XT
γY.
Using this algorithm, if pre-computing XTXis not possible, the computational complexity per conditional
PIP isO(N|γ|2+|γ|3+P|γ|2). Otherwise, if pre-computing XTXis possible, the computational complexity
per conditional PIP is O(|γ|3+P|γ|2).
Remark 11. As we can see in Appendix E, for the linear regression model in Section 2.2, if pre-computing
XTXis not possible, the computational complexity for a conditional PIP is O(N|γ|2+|γ|3+P|γ|2). Other-
wise, if pre-computing XTXis possible, the computational complexity for a conditional PIP is O(|γ|3+P|γ|2).
Here,|γ|≈hP. Hence, the average computational complexity for our algorithm is O(S(N|γ|2+|γ|3+P|γ|2))
orO(S(|γ|3+P|γ|2))which depends on whether the precomputing of XTXis possible or not. To reduce
the computational complexity, we can reduce S, or we are only interested in the case P/Sis large. This
computational complexity reductions is more meaningful if |γ|≈Ph << P , i.e., we consider the sparse
linear regression regimes. However, the variance of the associated Rao-Blackwellized estimator is increased
asSbecomes small. Hence, there is a trade-off between the computational complexity per MCMC iteration
10Under review as submission to TMLR
vs. the variance of of the Rao-Blackwellized estimator. The most interesting fact is that the newly-designed
Rao-Blackwellized estimator converges to PIPs for any value of S. In practice, the choice of Sdepends on
each application and the availability of computational resources. We can choose Svery small (eg., S= 2)
to have a low complexity estimator and low convergence rate. We can choose S≈Pfor a high complexity
estimator with high convergence rate. Furthermore, both our and Jankowiak algorithms are degenerated to
the wTGS (Zanella & Roberts (2019)) at S≈P.
4 Experiments
In this section, we show by simulation that the PIP-estimator is convergent as T→ ∞. In addition,
we compare the variance of associated Rao-Blackwellized estimators for VC-wTGS and subset wTGS on
simulated and real datasets. To compute p(γi|γ−i,Y), we use the same trick as (Zanella & Roberts, 2019,
Appendix B.1) for the new setting. See our derivations of this posterior distribution in Appendix E. As
(Jankowiak, 2023), in ALG 1 and ALG 2, we choose
η(γ−i) =P(γi= 1|γ−i,D). (54)
4.1 Simulated Datasets
First, we perform a simulated experiment. Let X∈RN×Pbe a realization of a multivariate (random)
Gaussian matrix. We consider the case N= 100andP= 200. We runT= 20000 iterations.
Fig. 1 shows the number of conditional PIP computations per MCMC iteration over Titerations. As we can
see, our algorithm (Algorithm 2) has variable complexity where the number of conditional PIP computations
per MCMC is a random variable Ywhich takes value on {0,P}where P(Y=P) =S/P. For Jankowiak’s
algorithm, the number of conditional PIP computations per MCMC is always fixed, which is equal to S.
Fig.2showsthattheRao-Blackwellizedestimatorin(15)convergestothevalueofPIPat T→∞fordifferent
values ofS. Since the number of PIPs, P, is very large, we only run simulations for PIP(0) and PIP(1). The
behavior of PIP(0) and PIP(1) represents the behavior of other PIPs. Since VC-wTGS converges very fast
atTbig enough, the variance of variable-complexity wTGS is very small in the long term.
In Fig. 4, we plot the estimators of VC-wTGS, subset wTGS, and wTGS for estimating PIP(0). It can
our estimator converges to wTGS estimator faster than subset wTGS. This also means that the variance of
VC-wTGS is smaller than the variance of subset wTGS for the same sample complexity S.
11Under review as submission to TMLR
0 2500 5000 7500 10000 12500 15000 17500 20000
Sampler iteration02004006008001000Computational ComplexityJanko iak algorithm (ALG 1)
Our algorithm (ALG 2)
Figure 1: Computational Complexity Evolution
12Under review as submission to TMLR
0 2500 5000 7500 10000 12500 15000 17500 20000
Sampler iteration0.100.110.120.130.140.15PIP(0)S=P
S=P/20
S=2
0 2500 5000 7500 10000 12500 15000 17500 20000
Sampler iteration0.100.120.140.160.180.200.220.24PIP(1)S=P
S=P/20
S=2
Figure 2: VC-wTGS Rao-Blackwellized Estimators (ALG 2)
0 2500 5000 7500 10000 12500 15000 17500 20000
Sampler iteration0.00.20.40.60.81.0PIP(0)i=0,S=2, variable
i=0,S=2,fixed
i=0,S=P
Figure 3: Convergence of Rao-Blackwellized Estimators
4.2 Real Datasets
In this simulation, we run ALG 2 on MNIST dataset.
As Fig. 1, Fig. 4 shows the number of conditional PIP computations per MCMC iteration over Titerations.
It shows that our algorithm has variable computational complexity per MCMC iteration, which is different
from Jankowiak’s algorithm.
Fig. 5 plots PIP(0) and PIP(1) and the estimated variances for the Rao-Blackwellized estimator in (15)
at different values of S, respectively. Here, PIP(0) and PIP(1) are defined in (9), which are posterior
inclusion probabilities that the components β0andβ1affect the output. These plots show a trade-off
between the computational complexity and the estimated variance for estimating PIP(0) and PIP(1). The
13Under review as submission to TMLR
0 10000 20000 30000 40000 50000
Sampler iteration02004006008001000Computational ComplexityJanko iak algorithm (ALG 1)
Our algorithm (ALG 2)
Figure 4: Computational Complexity Evolution
expected number of PIP computations is only STin ALG 2 but TPin wTGS if we run TMCMC iterations.
However, we suffer an increasing in variance. By Theorem 10, the variance is O/parenleftbig/parenleftbigP
S/parenrightbig2logT
T/parenrightbig
for a given
dataset, i.e., increasing at most (P/S)2times. For many applications, we don’t need to estimate PIPs
exactly, hence VC-wTGS can be used to reduce computational complexity especially when Pis very large
(million covariates). Fig. 6 shows that VC-wTGS outperforms subset wTGS (Jankowiak, 2023) at high
values ofP/S, which shows that our newly-designed Rao-Blackwellized estimator converges to PIP faster
than Jankowiak’s estimator at high P/S.
5 Conclusion
This paper proposed a variable complexity wTGS for Bayesian Variable Selection which can improve the
computational complexity of the well-known wTGS. Experiments show that our Rao-Blackwellized estimator
can give a smaller variance than its counterpart associated with the subset-wTGS at high P/S.
14Under review as submission to TMLR
0 10000 20000 30000 40000 50000
Sampler iteration0.000.020.040.060.080.100.120.14V ariance of estimating PIP(0)S=P/4
S=P/20
S=2
0 10000 20000 30000 40000 50000
Sampler iteration0.000.050.100.150.200.250.30V ariance of estimating PIP(1)S=P/4
S=P/20
S=2
Figure 5: The variance of VC-wTGS Rao-Blackwellized Estimators (ALG 2)
0 10000 20000 30000 40000 50000
Sampler iteration0e+001e-022e-023e-024e-025e-02Estimation V ariance of PIP(0) at S=2fixed
variable
0 10000 20000 30000 40000 50000
Sampler iteration0.000.010.020.030.040.05Estimation V ariance of PIP(P/2) at S=2fixed
variable
Figure 6: Comparing the variance between subset wTGS and VC-wTGS at S= 2.
References
Christophe Andrieu, Nando de Freitas, A. Doucet, and Michael I. Jordan. An introduction to MCMC for
machine learning. Machine Learning , 50:5–43, 2004.
C. M. Bishop. Pattern Recognition and Machine Learning . Springer, 2006.
William M. Bolstad. Understanding Computational Bayesian Statistics . John Wiley, 2010.
L. Breiman. The strong law of large numbers for a class of Markov chains. Annals of Mathematical Statistics ,
31:801–803, 1960.
Mónica F. Bugallo, Shanshan Xu, and Petar M. Djurić. Performance comparison of EKF and particle
filtering methods for maneuvering targets. Digit. Signal Process. , 17:774–786, 2007.
R. Combes and M. Touati. Computationally efficient estimation of the spectral gap of a markov chain.
Proceedings of the ACM on Measurement and Analysis of Computing Systems , 3:1 – 21, 2019.
Persi Diaconis and Laurent Saloff-Coste. Comparison theorems for reversible markov chains. Annals of
Applied Probability , 3:696–730, 1993.
15Under review as submission to TMLR
William J. Fitzgerald. Markov chain Monte Carlo methods with applications to signal processing. Signal
Process., 81:3–18, 2001.
Ankur Gupta and James B. Rawlings. Comparison of parameter estimation methods in stochastic chemical
kinetic models: Examples in systems biology. AIChE journal. American Institute of Chemical Engineers ,
60 4:1253–1268, 2014.
Tim Hesterberg. Monte carlo strategies in scientific computing. Technometrics , 44:403 – 404, 2002.
Martin Jankowiak. Bayesian variable selection in a million dimensions. In International Conference on
Artificial Intelligence and Statistics , 2023.
Muhammad F. Kasim, A. F. A. Bott, Petros Tzeferacos, Donald Q. Lamb, Gianluca Gregori, and Sam M.
Vinko. Retrieving fields from proton radiography without source profiles. Physical review. E , 100 3-1:
033208, 2019.
Faming Liang, Chuanhai Liu, and Raymond J. Carroll. Advanced Markov chain Monte Carlo methods:
Learning from past samples. 2010.
Daniel Paulin. Concentration inequalities for Markov chains by Marton couplings and spectral methods.
Electronic Journal of Probability , 20(79):1 – 32, 2015.
Shravas Rao. A Hoeffding inequality for Markov chains. Electronic Communications in Probability , 2018.
Jesse Read, Luca Martino, and David Luengo. Efficient Monte Carlo methods for multi-dimensional learning
with classifier chains. Pattern Recognit. , 47:1535–1546, 2012.
Christian P. Robert and George Casella. Monte carlo statistical methods. Technometrics , 47:243 – 243,
2005.
Lan V. Truong. On linear model with markov signal priors. In AISTATS , 2022.
Pekka Tuominen and Richard L. Tweedie. Markov Chains with Continuous Components. Proceedings of the
London Mathematical Society , s3-38(1):89–114, 01 1979.
Adrian G. Wills and Thomas Bo Schön. Sequential monte carlo: A unified review. Annu. Rev. Control.
Robotics Auton. Syst. , 6:159–182, 2023.
G. Wolfer and A. Kontorovich. Estimating the mixing time of ergodic Markov chains. In 32nd Annual
Conference on Learning Theory , 2019.
Giacomo Zanella and Gareth O. Roberts. Scalable importance tempering and Bayesian variable selection.
Journal of the Royal Statistical Society: Series B (Statistical Methodology) , 81, 2019.
A Appendix
B Proof of Lemma 3
The transition kernel for the sequence {γ(t)}can be written as
K∗(γ→γ′) =S
PP/summationdisplay
j=1f(j|γ)δ(γ′−flip (γ|j)) +/parenleftbigg
1−S
P/parenrightbigg
δ(γ′−γ). (55)
This implies that for any pair (γ,γ′)such thatγ′=flip (γ|i)for somei∈[P], we have
K∗(γ→γ′) =S
PP/summationdisplay
j=1f(j|γ)δ(γ′−flip (γ|j)) (56)
=S
Pf(i|γ). (57)
16Under review as submission to TMLR
Now, by ALG 2, we also have
f(i|γ) =ϕ−1(γ)1
2η(γ−i)
p(γi|γ−i,D)(58)
and
f(i|γ′) =ϕ−1(γ′)1
2η(γ′
−i)
p(γ′
i|γ′
−i,D). (59)
From (58) and (59) and γ−i=γ′
−i, we obtain
K∗(γ→γ′)
K∗(γ′→γ)=S
Pf(i|γ)
S
Pf(i|γ′)(60)
=f(i|γ)
f(i|γ′)(61)
=ϕ(γ′)p(γ′|D)
ϕ(γ)p(γ|D)(62)
=f(γ′)
f(γ). (63)
In addition, we also have K∗(γ→γ′) =K∗(γ′→γ) = 0ifγ′̸=γandγ′̸=flip (γ|i)for anyi∈[P].
Furthermore, K∗(γ→γ′) =K∗(γ′→γ) = 1−S
Pifγ=γ′.
By combining all these cases, it holds that
f(γ)K∗(γ→γ′) =f(γ′)K∗(γ′→γ) (64)
for allγ′,γ.
This means that {γ(t)}T
t=1form a reversible Markov chain with stationary distribution f(γ)/Zfwhere
Zf=/summationdisplay
γf(γ). (65)
Since{Qt}T
t=1is an i.i.d. Bernoulli sequence with q(1) =S/Pand independent of {γ(t)}T
t=1,{γ(t),Q(t)}T
t=1
forms a Markov chain with the transition kernel satisfying:
K((γ,Q)→(γ′,Q′)) =q(Q′)K∗(γ→γ′). (66)
It follows from (66) that
q(Q)f(γ)/ZfK((γ,Q)→(γ′,Q′)) =/bracketleftbig
K∗(γ→γ′)f(γ)/Zf/bracketrightbig
q(Q)q(Q′) (67)
for any pair (γ,Q)and(γ′,Q′).
Finally, from (64) and (67), we have
q(Q)f(γ)/ZfK((γ,Q)→(γ′,Q′)) =q(Q′)f(γ)/ZfK((γ′,Q′)→(γ,Q)). (68)
This means that {γt,Q(t)}T
t=1forms a reversible Markov chain with stationary distribution q(Q)f(γ)/Zf.
C Proof of Lemma 1
Observe that with probability at least 1−α, we have
(1−ε)E[U]≤U≤(1 +ε)E[U] (69)
(1−ε)E[V]≤V≤(1 +ε)E[V]. (70)
17Under review as submission to TMLR
Hence, we have
/parenleftbigg1−ε
1 +ε/parenrightbiggE[U]
E[V]≤U
V≤/parenleftbigg1 +ε
1−ε/parenrightbiggE[U]
E[V]. (71)
From (71), with probability at least 1−α, we have
/vextendsingle/vextendsingle/vextendsingle/vextendsingleU
V−E[U]
E[V]/vextendsingle/vextendsingle/vextendsingle/vextendsingle≤2ε
1−ε/parenleftbiggE[U]
E[V]/parenrightbigg
. (72)
It follows from (72) that
E/bracketleftbigg/vextendsingle/vextendsingle/vextendsingle/vextendsingleU
V−E[U]
E[V]/vextendsingle/vextendsingle/vextendsingle/vextendsingle2/bracketrightbigg
=E/bracketleftbigg/vextendsingle/vextendsingle/vextendsingle/vextendsingleU
V−E[U]
E[V]/vextendsingle/vextendsingle/vextendsingle/vextendsingle2/vextendsingle/vextendsingle/vextendsingle/vextendsingleD/bracketrightbigg
P(D) +E/bracketleftbigg/vextendsingle/vextendsingle/vextendsingle/vextendsingleU
V−E[U]
E[V]/vextendsingle/vextendsingle/vextendsingle/vextendsingle2/vextendsingle/vextendsingle/vextendsingle/vextendsingleDc/bracketrightbigg
P(Dc) (73)
≤4ε2
(1−ε)2/parenleftbiggE[U]
E[V]/parenrightbigg2
+/bracketleftbigg
max/parenleftbigg
M,E[U]
E[V]/parenrightbigg/bracketrightbigg2
α. (74)
D Proof of Lemma 5
First, by definition of ˆϕ(γ)in (36) we have
ρ(t)=ˆϕ(γ(t))Q(t)
/summationtextT
t=1ˆϕ/parenleftbig
γ(t)/parenrightbig
Q(t). (75)
In addition, observe that
0≤ˆϕ(γ)≤1. (76)
Now, letg:{0,1}P→R+such thatg(γ)≤1for allγ. Then, by applying Lemma 2 and a change of
measure, with probability 1−2dν
dπexp(−ζ2T(1−λ)
64e), we have
1
T/vextendsingle/vextendsingle/vextendsingle/vextendsingleT/summationdisplay
t=1ˆϕ(γ(t))g(γ(t))Q(t)−Eπ/bracketleftbiggT/summationdisplay
t=1ˆϕ(γ(t))g(γ(t))Q(t)/bracketrightbigg/vextendsingle/vextendsingle/vextendsingle/vextendsingle≤ζ (77)
for anyζ >0.
Similarly, by using Lemma 2, with probability at least 1−2dν
dπexp(−ζ2T(1−λ)
64e), it holds that
1
T/vextendsingle/vextendsingle/vextendsingle/vextendsingleT/summationdisplay
t=1ˆϕ(γ(t))Q(t)−Eπ/bracketleftbiggT/summationdisplay
t=1ˆϕ(γ(t))Q(t)/bracketrightbigg/vextendsingle/vextendsingle/vextendsingle/vextendsingle≤ζ. (78)
By using the union bound, with probability at least 1−4dν
dπexp(−ζ2T(1−λ)
64e), it holds that
1
T/vextendsingle/vextendsingle/vextendsingle/vextendsingleT/summationdisplay
t=1ˆϕ(γ(t))g(γ(t))Q(t)−Eπ/bracketleftbiggT/summationdisplay
t=1ˆϕ(γ(t))g(γ(t))Q(t)/bracketrightbigg/vextendsingle/vextendsingle/vextendsingle/vextendsingle≤ζ, (79)
1
T/vextendsingle/vextendsingle/vextendsingle/vextendsingleT/summationdisplay
t=1ˆϕ(γ(t))−Eπ/bracketleftbiggT/summationdisplay
t=1ˆϕ(γ(t))/bracketrightbigg/vextendsingle/vextendsingle/vextendsingle/vextendsingle≤ζ. (80)
18Under review as submission to TMLR
Now, by setting ζ=ζ0:=ε
Tmin/braceleftbig
Eπ/bracketleftbig/summationtextT
t=1ˆϕ(γ(t))g(γ(t))Q(t)/bracketrightbig
,Eπ/bracketleftbig/summationtextT
t=1ˆϕ(γ(t))/bracketrightbig/bracerightbig
for someε >0(to be
chosen later), with probability at least 1−4dν
dπexp(−ζ2
0T(1−λ)
64e), it holds that
1
T/vextendsingle/vextendsingle/vextendsingle/vextendsingleT/summationdisplay
t=1ˆϕ(γ(t))g(γ(t))Q(t)−Eπ/bracketleftbiggT/summationdisplay
t=1ˆϕ(γ(t))g(γ(t))Q(t)/bracketrightbigg/vextendsingle/vextendsingle/vextendsingle/vextendsingle
≤ε
TEπ/bracketleftbiggT/summationdisplay
t=1ˆϕ(γ(t))g(γ(t))Q(t)/bracketrightbigg
, (81)
1
T/vextendsingle/vextendsingle/vextendsingle/vextendsingleT/summationdisplay
t=1ˆϕ(γ(t))Q(t)−Eπ/bracketleftbiggT/summationdisplay
t=1ˆϕ(γ(t))Q(t)/bracketrightbigg/vextendsingle/vextendsingle/vextendsingle/vextendsingle≤ε
TEπ/bracketleftbiggT/summationdisplay
t=1ˆϕ(γ(t))Q(t)/bracketrightbigg
. (82)
Furthermore, by setting
U:=1
TT/summationdisplay
t=1ˆϕ(γ(t))g(γ(t))Q(t), (83)
V:=1
TT/summationdisplay
t=1ˆϕ(γ(t))Q(t), (84)
we have
U
V=/summationtextT
t=1ϕ−1(γ(t))g(γ(t))Q(t)
/summationtextT
t=1ϕ−1(γ(t))Q(t)(85)
=T/summationdisplay
t=1ρ(t)g(γ(t)) (86)
and
M:= sup(U/V)≤1 (87)
since/summationtextT
t=1ρ(t)= 1andg(γ(t))≤1for allγ(t).
From (80)-(87), by Lemma 1, we have
E/bracketleftbigg/vextendsingle/vextendsingle/vextendsingle/vextendsingleT/summationdisplay
t=1ρ(t)g(γ(t))Q(t)−Eπ[U]
Eπ[V]/vextendsingle/vextendsingle/vextendsingle/vextendsingle2/bracketrightbigg
≤4ε2
(1−ε)2/parenleftbiggEπ[U]
Eπ[V]/parenrightbigg2
+/bracketleftbigg
max/parenleftbigg
1,Eπ[U]
Eπ[V]/parenrightbigg/bracketrightbigg2
α, (88)
whereα:= 4dν
dπexp/parenleftbig
−ε2T(1−λγ,Q) min{Eπ[U],Eπ[V]}2
64e/parenrightbig
,whereλγ,Qisthestationarydistributionofthereversible
Markov chain{γ(t),Q(t)}.
Now, by setting
ε=ε0=1
min{Eπ[U],Eπ[V]}/radicaligg
64elogT
(1−λγ,Q)T, (89)
we haveα= 4dν
dπ1
T. Then, we obtain
E/bracketleftbigg/vextendsingle/vextendsingle/vextendsingle/vextendsingleT/summationdisplay
t=1ρ(t)g(γ(t))−Eπ[U]
Eπ[V]/vextendsingle/vextendsingle/vextendsingle/vextendsingle2/bracketrightbigg
≤4ε2
0
(1−ε0)2/parenleftbiggEπ[U]
Eπ[V]/parenrightbigg2
+/bracketleftbigg
max/parenleftbigg
1,Eπ[U]
Eπ[V]/parenrightbigg/bracketrightbigg2
α. (90)
Now, observe that
Eπ[U]
Eπ[V]=Eπ/bracketleftbig
g(γ)Qˆϕ(γ)/bracketrightbig
Eπ/bracketleftbigˆϕ(γ)Q/bracketrightbig (91)
=Eπ/bracketleftbig
g(γ)Qϕ−1(γ)/bracketrightbig
Eπ/bracketleftbig
ϕ−1(γ)Q/bracketrightbig. (92)
19Under review as submission to TMLR
On the other hand, by Lemma 3, we have π(γ,Q) =q(Q)f(γ)
ZfwhereZf:=/summationtext
γf(γ)andf(γ) =p(γ|D)ϕ(γ).
It follows that
Eπ/bracketleftbig
g(γ)Qϕ−1(γ)/bracketrightbig
=Eq(Q)f(γ)/Zf/bracketleftbig
g(γ)Qϕ−1(γ)/bracketrightbig
(93)
=/summationdisplay
γ/summationdisplay
Qg(γ)Qϕ−1(γ)f(γ)
Zfq(Q) (94)
=1
Zf/summationdisplay
γ/summationdisplay
Qg(γ)q(Q)Qp(γ|D) (95)
=1
ZfEp(γ|D)/bracketleftbig
g(γ)/bracketrightbig
Eq[Q]. (96)
Similarly, we have
Eπ/bracketleftbig
ϕ−1(γ)Q/bracketrightbig
=Eq(Q)f(γ)/Zf/bracketleftbig
ϕ−1(γ)Q/bracketrightbig
(97)
=/summationdisplay
Q/summationdisplay
γϕ−1(γ)Qf(γ)
Zfq(Q) (98)
=1
Zf/parenleftbigg/summationdisplay
γP(γ|D)/parenrightbigg
Eq[Q]. (99)
From (92), (96) and (99), we obtain
Eπ[U]
Eπ[V]=Ep(γ|D)/bracketleftbig
g(γ)/bracketrightbig
. (100)
For the given problem, by setting g(γ) =p(γi= 1|γ−i,D), from (100), we have
Eπ[U]
Eπ[V]=PIP(i). (101)
In addition, we have
Eπ[V] =Eπ/bracketleftbigˆϕ(γ)Q/bracketrightbig
(102)
=/summationdisplay
γ,Qˆϕ(γ)Qf(γ)
Zfq(Q) (103)
=/parenleftbigg/summationdisplay
γˆϕ(γ)f(γ)
Zf/parenrightbigg/parenleftbigg/summationdisplay
QQq(Q)/parenrightbigg
(104)
=Eπ[ˆϕ(γ)]EQ[Q] (105)
=S
PEπ[ˆϕ(γ)]. (106)
Hence, we obtain
min{Eπ[U],Eπ[V]}=Eπ[V] min/braceleftbigg
1,Eπ[U]
Eπ[V]/bracerightbigg
(107)
=Eπ[V] min/braceleftbigg
1,PIP(i)/bracerightbigg
(108)
=Eπ[V]PIP(i) (109)
=S
PEπ[ˆϕ(γ)]PIP(i). (110)
20Under review as submission to TMLR
From (90), (101), and (110), we have
E/bracketleftbigg/vextendsingle/vextendsingle/vextendsingle/vextendsingleT/summationdisplay
t=1ρ(t)p(γ(t)
i= 1|γ(t)
−i,D)−PIP(i)/vextendsingle/vextendsingle/vextendsingle/vextendsingle2/bracketrightbigg
≤4ε2
0
(1−ε0)2PIP2(i) + 4dν
dπ1
T, (111)
and
ε0=P
PIP(i)Eπ[ˆϕ(γ)]S/radicaligg
64elogT
(1−λγ,Q)T. (112)
Now, observe that
dν
dπ(γ,Q) =pγ1,Q1(γ,Q)
π(γ,Q)(113)
≤1
π(γ,Q)(114)
=1
π(γ)q(Q)(115)
≤P
S1
minγπ(γ). (116)
By combining (111) and (116), we have
E/bracketleftbigg/vextendsingle/vextendsingle/vextendsingle/vextendsingleT/summationdisplay
t=1ρ(t)p(γ(t)
i= 1|γ(t)
−i,D)−PIP(i)/vextendsingle/vextendsingle/vextendsingle/vextendsingle2/bracketrightbigg
≤4ε2
0
(1−ε0)2PIP2(i) +4P
S1
minγπ(γ)T. (117)
E Derive p(γi|D,γ−i)
Observe that
p(γi|D,γ−i) =p(γi|D,γ−i)
p(1−γi|D,γ−i)/parenleftbigg
1 +p(γi|D,γ−i)
p(1−γi|D,γ−i)/parenrightbigg−1
. (118)
In addition, we have
p(γi= 1|D,γ−i)
p(γi= 0|D,γ−i)=p(γi= 1,D|γ−i)
p(γi= 0,D|γ−i)(119)
=p(γi= 1|γ−i,X)
p(γi= 0|γ−i,X)p(Y|γi= 1,γ−i,X)
p(Y|γi= 0,γ−i,X)(120)
=/parenleftbiggp(γi= 1)
p(γi= 0)/parenrightbigg/parenleftbiggp(Y|γi= 1,γ−i,X)
p(Y|γi= 0,γ−i,X)/parenrightbigg
(121)
=/parenleftbiggh
1−h/parenrightbigg/parenleftbiggp(Y|γi= 1,γ−i,X)
p(Y|γi= 0,γ−i,X)/parenrightbigg
. (122)
On the other hand, for any tuple γ= (γ1,γ2,···,γP)such thatγi= 1(so|γ|≥1), we have
p(Y|γi= 1,γ−i,βγ,σ2
γ,X) =1
/parenleftbig
σγ√
2π/parenrightbigNexp/parenleftbigg
−∥Y−Xγβγ∥2
2σ2γ/parenrightbigg
. (123)
21Under review as submission to TMLR
It follows that
p(Y|γi= 1,γ−i,X/parenrightbig
=/integraldisplay
βγ/integraldisplay∞
σ2γ=01
/parenleftbig
σγ√
2π/parenrightbigNexp/parenleftbigg
−∥Y−Xγβγ∥2
2σ2γ/parenrightbigg
p(βγ|γi= 1,γ−i)p(σ2
γ|γi= 1,γ−i)dβγdσ2
γ(124)
=/integraldisplay∞
σ2γ=0InvGamma/parenleftbigg1
2ν0,1
2ν0λ0/parenrightbigg/integraldisplay
βγ1
/parenleftbig
σγ√
2π/parenrightbigNexp/parenleftbigg
−∥Y−Xγβγ∥2
2σ2γ/parenrightbigg
×1
/parenleftbig
σγ√
2πτ−1/parenrightbig|γ|exp/parenleftbigg
−∥βγ∥2
2σ2γτ−1/parenrightbigg
dβγdσ2
γ. (125)
Now, observe that
∥Y−Xγβγ∥2+τ∥βγ∥2
= (Y−Xγβγ)T(Y−Xγβγ) +τβT
γβγ (126)
=YTY−2YTXγβγ+βT
γXT
γXγβγ+τβT
γβγ (127)
=YTY−2YTXγβγ+βT
γ(XT
γXγ+τI)βγ. (128)
Now, consider the EVD (singular value decomposition) of the positive definite matrix XT
γXγ+τI(note that
τ >0):
XT
γXγ+τI=UTΛU (129)
where Λis the a diagonal matrix consisting of all positive eigenvalue of XT
γXγ+τI. Let
˜βγ:=√
ΛUβγ, (130)
˜Yγ:=√
Λ−1UXT
γY. (131)
Then, we have
∥Y−Xγβγ∥2+τ∥βγ∥2
=YTY−2YTXγβγ+βT
γ(XT
γXγ+τI)βγ (132)
=YTY−2YTXγ√
Λ−1UT˜βγ+˜βT
γ˜βγ (133)
=YTY−2˜YT
γ˜βγ+˜βT
γ˜βγ (134)
=/parenleftbig
∥Y∥2−∥˜Yγ|2/parenrightbig
+/parenleftbig˜YT
γ˜Yγ−2˜YT
γ˜βγ+˜βT
γ˜βγ/parenrightbig
(135)
=/parenleftbig
∥Y∥2−∥˜Yγ|2/parenrightbig
+∥˜Yγ−˜βγ∥2. (136)
Hence, we have
dβγ= det(UTΛ−1/2)d˜βγ (137)
= det(XT
γXγ+τI)−1/2d˜βγ. (138)
Hence, we have
/integraldisplay
βγ1
/parenleftbig
σγ√
2π/parenrightbigNexp/parenleftbigg
−∥Y−Xγβγ∥2
2σ2γ/parenrightbigg1
/parenleftbig
σγ√
2πτ−1/parenrightbig|γ|exp/parenleftbigg
−∥βγ∥2
2σ2γτ−1/parenrightbigg
dβγ (139)
=/integraldisplay
˜βγ1
/parenleftbig
σγ√
2π/parenrightbigNexp/parenleftbigg
−/parenleftbig
∥Y∥2−∥˜Yγ|2/parenrightbig
+∥˜Yγ−˜βγ∥2
2σ2γ/parenrightbigg
×1
/parenleftbig
σγ√
2πτ−1/parenrightbig|γ|det(XT
γXγ+τI)−1/2d˜βγ (140)
=1
/parenleftbig
σγ√
2π/parenrightbigNτ|γ|/2exp/parenleftbigg
−/parenleftbig
∥Y∥2−∥˜Yγ|2/parenrightbig
2σ2γ/parenrightbigg
det(XT
γXγ+τI)−1/2. (141)
22Under review as submission to TMLR
By combining (125) and (141), we obtain
p(Y|γi= 1,γ−i,X/parenrightbig
=/integraldisplay
βγ/integraldisplay∞
σ2γ=01
/parenleftbig
σγ√
2π/parenrightbigNexp/parenleftbigg
−∥Y−Xγβγ∥2
2σ2γ/parenrightbigg
p(βγ|γi= 1,γ−i)p(σ2
γ|γi= 1,γ−i)dβγdσ2
γ(142)
=/integraldisplay∞
σ2γ=0InvGamma/parenleftbigg1
2ν0,1
2ν0λ0/parenrightbigg1
/parenleftbig
σγ√
2π/parenrightbigNτ|γ|/2
×exp/parenleftbigg
−/parenleftbig
∥Y∥2−∥˜Yγ|2/parenrightbig
2σ2γ/parenrightbigg
det(XT
γXγ+τI)−1/2dσ2
γ (143)
= det(XT
γXγ+τI)−1/2τ|γ|/2(2π)−N/2/integraldisplay∞
σ2γ=0InvGamma/parenleftbigg1
2ν0,1
2ν0λ0/parenrightbigg
(σ2
γ)−N/2
×exp/parenleftbigg
−/parenleftbig
∥Y∥2−∥˜Yγ∥2/parenrightbig
2σ2γ/parenrightbigg
dσ2
γ (144)
= det(XT
γXγ+τI)−1/2τ|γ|/2(2π)−N/2
×/integraldisplay∞
σ2γ=0(1/2λ0ν0)1/2ν0
Γ(1/2ν0)(1/σ2
γ)1/2ν0+1exp/parenleftbigg
−1/2ν0λ0/σ2
γ/parenrightbigg
(σ2
γ)−N/2
×exp/parenleftbigg
−/parenleftbig
∥Y∥2−∥˜Yγ∥2/parenrightbig
2σ2γ/parenrightbigg
dσ2
γ (145)
= det(XT
γXγ+τI)−1/2τ|γ|/2(2π)−N/2(1/2λ0ν0)1/2ν0
Γ(1/2ν0)
×/integraldisplay∞
σ2
γ=0(1/σ2
γ)1/2ν0+1+N/2exp/parenleftbigg
−/parenleftbig
∥Y∥2−∥˜Yγ∥2+ν0λ0/parenrightbig
2σ2γ/parenrightbigg
dσ2
γ (146)
= det(XT
γXγ+τI)−1/2τ|γ|/2(2π)−N/2(1/2λ0ν0)1/2ν0
Γ(1/2ν0)
×Γ/parenleftbiggN+ν0
2/parenrightbigg/parenleftbigg∥Y∥2−∥˜Yγ∥2+ν0λ0
2/parenrightbigg−N+ν0
2
. (147)
Let˜γ1is given by γ−iwithγi= 1,˜γ0is given by γ−iwithγi= 0. It follows that
p(Y|γi= 1,γ−i,X)
p(Y|γi= 0,γ−i,X)=√τ/radicaligg
det(XT
˜γ0X˜γ0+τI)
det(XT
˜γ1X˜γ1+τI)/parenleftbigg∥Y∥2−∥˜Y˜γ0∥2+ν0λ0
∥Y∥2−∥˜Y˜γ1∥2+ν0λ0/parenrightbiggN+ν0
2
. (148)
On the other hand, we have
∥˜Yγ∥2=˜YT
γ˜Yγ (149)
=YTXγ(XT
γXγ+τI)−1XT
γY. (150)
Hence, we finally have
p(Y|γi= 1,γ−i,X)
p(Y|γi= 0,γ−i,X)=/radicaligg
τdet(XT
˜γ0X˜γ0+τI)
det(XT
˜γ1X˜γ1+τI)/parenleftbiggS˜γ0
S˜γ1/parenrightbiggN+ν0
, (151)
where
Sγ:=YTY−YTXγ(XT
γXγ+τI)−1XT
γY+ν0λ0. (152)
Based on this, we can estimate
p(γi|D,γ−i) =p(γi|D,γ−i)
p(1−γi|D,γ−i)/parenleftbigg
1 +p(γi|D,γ−i)
p(1−γi|D,γ−i)/parenrightbigg−1
. (153)
23Under review as submission to TMLR
Denote the set of included variables in ˜γ0asI={j: ˜γ0,j= 1}. DefineF=/parenleftbig
XT
˜γ0X˜γ0+τI/parenrightbig−1,ν=XTY
andν˜γ0= (νj)j∈I. Also define A=XTXandai= (Aji)j∈I. Then, by using the same arguments as (Zanella
& Roberts, 2019, Appendix B1), we can show that
S(˜γ1) =S(˜γ0)−di/parenleftbig
νT
˜γ0Fai−νi/parenrightbig2, (154)
wheredi= (Aii+τ−aT
iFai)−1. In addition, we can compute aT
iFaiby using the Cholesky decomposition
ofF=LLTand
aT
iFai=∥aT
iL∥2(155)
=/summationdisplay
j∈I(BL)2
ij, (156)
whereBis thep×|γ|matrix made of the columns of Acorresponding to variables included in γ.
In addition, we have
XT
˜γ1X˜γ1+τI=/parenleftbiggXT
˜γ0X˜γ0+τI ai
aT
iAii+τ/parenrightbigg
(157)
Hence, by using Schur’s formula for the determinant of block matrix, we are easy to see that
det(XT
˜γ0X˜γ0+τI)
det(XT
˜γ1X˜γ1+τI)=di. (158)
Using this algorithm, if pre-computing XTXis not possible, the computational complexity per conditional
PIP isO(N|γ|2+|γ|3+P|γ|2). Otherwise, if pre-computing XTXis possible, the computational complexity
per conditional PIP is O(|γ|3+P|γ|2).
F Proof of Lemma 9
From Lemma 8 and the fact that {γ(t),Q(t)}forms a reversible Markov chain with transition kernel
K((γ,Q)→(γ′,Q′)) =K∗(γ→γ′)q(Q′), we have
1−λγ,Q
= inf
g(γ,Q):Eπ[g]=0,Eπ[g2]=1⟨g,g⟩π−⟨Kg,g⟩ (159)
= 1− sup
g(γ,Q):Eπ[g]=0,Eπ[g2]=1⟨Kg,g⟩ (160)
= 1− sup
g(γ,Q):Eπ[g]=0,Eπ[g2]=1/summationdisplay
γ,QKg(γ,Q)g(γ,Q)π(γ,Q) (161)
= 1− sup
g(γ,Q):Eπ[g]=0,Eπ[g2]=1/summationdisplay
γ,Q/summationdisplay
γ′,Q′K((γ,Q)→(γ′,Q′))g(γ′,Q′)g(γ,Q)π(γ,Q) (162)
= 1−S
Psup
g(γ,Q):Eπ[g]=0,Eπ[g2]=1/summationdisplay
γ,Q/summationdisplay
γ′,Q′K∗(γ→γ′)q(Q′)g(γ′,Q′)g(γ,Q)π(γ,Q) (163)
= 1−S
Psup
g(γ,Q):Eπ[g]=0,Eπ[g2]=1/summationdisplay
γ,Q/summationdisplay
γ′,Q′K∗(γ→γ′)f(γ)
Zfq(Q)g(γ′,Q′)g(γ,Q)q(Q′) (164)
= 1−S
Psup
g(γ,Q):Eπ[g]=0,Eπ[g2]=1/summationdisplay
γ,γ′K∗(γ→γ′)f(γ)
Zf/summationdisplay
Q,Q′g(γ′,Q′)g(γ,Q)q(Q)q(Q′) (165)
= 1−S
Psup
g(γ,Q):Eπ[g]=0,Eπ[g2]=1/summationdisplay
γ,γ′K∗(γ→γ′)π(γ)/parenleftbigg/summationdisplay
Qg(γ,Q)q(Q)/parenrightbigg/parenleftbigg/summationdisplay
Q′π(γ′,Q′)q(Q′)/parenrightbigg
(166)
= 1−S
Psup
g(γ,Q):Eπ[g]=0,Eπ[g2]=1/summationdisplay
γ,γ′K∗(γ→γ′)π(γ)h(γ)h(γ′) (167)
24Under review as submission to TMLR
where
π(γ) =f(γ)
Zf, (168)
Zf=/summationdisplay
γf(γ), (169)
h(γ) :=/summationdisplay
Qg(γ,Q)q(Q). (170)
Observe that
Eπ[h(γ)] =/summationdisplay
γh(γ)π(γ) (171)
=/summationdisplay
γ/summationdisplay
Qg(γ,Q)q(Q)π(γ) (172)
=/summationdisplay
γ,Qg(γ,Q)π(γ,Q) (173)
=Eπ[g(γ,Q)] (174)
= 0. (175)
On the other hand, we also have
Eπ/bracketleftbig
h2(γ)/bracketrightbig
=/summationdisplay
γ/parenleftbigg/summationdisplay
Qg(γ,Q)q(Q)/parenrightbigg2
π(γ) (176)
≤/summationdisplay
γ/parenleftbigg/summationdisplay
Qg(γ,Q)2q(Q)/parenrightbigg
π(γ) (177)
=/summationdisplay
γ,Qg(γ,Q)2π(γ,Q) (178)
=Eπ/bracketleftbig
g(γ,Q)2/bracketrightbig
(179)
= 1, (180)
where (177) follows from the convexity of the function x2on[0,∞).
From (175), (180), and (167), we obtain
1−λγ,Q≥1− sup
h(γ):Eπ[h]=0,Eπ[h2]≤1/summationdisplay
γ,γ′K∗(γ→γ′)π(γ)h(γ)h(γ′). (181)
Now, note that Eπ[h] = 0is equivalent to h⊥π1. Let|Ω|= 2P+1:=nandh1,h2,···,hnare eigenfunctions
ofK∗corresponding to the decreasing ordered eigenvalues λ1≥λ2≥···≥λnand are orthogonal since K∗is
self-adjoint. Set h1=1. Since∥h∥2,π= 1andh⊥π1, we haveh=/summationtextn
j=2ajhjbecause it is perpendicular to
h1so it can be only represented by these eigenvectors. By taking l2-norm on both sizes we have/summationtextn
j=2a2
j≤1
since the form like ⟨hi,hj⟩π= 0and⟨hi,hi⟩=∥hi∥2
2,π= 1. Thus,
sup
h:Eπ[h]=0,Eπ[h2]≤1/summationdisplay
γ,γ′K∗(γ→γ′)π(γ)h(γ)h(γ′)≤ max
a2,a3,···,an:/summationtextn
j=2a2
j≤1n/summationdisplay
j=1a2
jλj (182)
≤λ2n/summationdisplay
j=2a2
j (183)
=λ2, (184)
25Under review as submission to TMLR
where/summationtextn
j=2a2
j≤1andλj∈spec (P)such thatλ2≥λ3···≥λn. Hence, from (184), we obtain
1−λγ,Q≥1−S
Pλ2 (185)
=S
P(1−λP) + 1−S
P(186)
≥1−S
P. (187)
26