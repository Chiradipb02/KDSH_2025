Under review as submission to TMLR
EffectofRandomLearningRate: TheoreticalAnalysisofSGD
Dynamics in Non-Convex Optimization via Stationary Distri-
bution
Anonymous authors
Paper under double-blind review
Abstract
We consider a variant of the stochastic gradient descent (SGD) with a random learning rate and
revealitsconvergenceproperties. SGDisawidelyusedstochasticoptimizationalgorithminmachine
learning,especiallydeeplearning. NumerousstudiesrevealtheconvergencepropertiesofSGDand
its simplified variants. Among these, the analysis of convergence using a stationary distribution of
updated parameters provides generalizable results. However, to obtain a stationary distribution, the
updatedirectionoftheparametersmustnotdegenerate,whichlimitstheapplicablevariantsofSGD.
In this study, we consider a novel SGD variant, Poisson SGD, which has degenerated parameter
update directions and instead utilizes a random learning rate. Consequently, we demonstrate that
a distribution of a parameter updated by Poisson SGD converges to a stationary distribution under
weakassumptionsonalossfunction. Basedonthis,wefurthershowthatPoissonSGDfindsglobal
minima in non-convex optimization problems and also evaluate the generalization error using this
method. As a proof technique, we approximate the distribution by Poisson SGD with that of the
bouncy particle sampler (BPS) and derive its stationary distribution, using the theoretical advance
of the piece-wise deterministic Markov process (PDMP).
1 Introduction
Stochastic gradient descent (SGD) stands out as a widely employed optimization algorithm in machine learning. It
fallsunderthecategoryofstochasticoptimization,whereparametersareupdatedwithrandomnessfromthemini-batch
sampling. SGD is valued for two main reasons in optimization: (i) it is memory-efficient and requires only low
computational resources by updating parameters from a fraction of the training data at each iteration (Bottou, 1991),
and (ii) models optimized with SGD have less generalization error than those optimized by other algorithms such as
gradient descent (GD) for neural networks (Wu et al., 2020; Zhu et al., 2019). Owing to these advantages, SGD has
beenoneofthestandardmethodsfortrainingdeeplearningmodels(Hofferetal.,2017;Keskaretal.,2016;Zhuetal.,
2019).
To understand the properties of SGD, the characteristics of parameters updated by SGD or its variants have been
actively studied. As for the usual SGD, Garrigos & Gower (2023) surveyed the results about the convergence rate of
SGD in convex and non-convex settings. It also mentions the global convergence property of SGD under the strong
convexity setting. Li et al. (2017); Jastrzebski et al. (2017) clarified that the parameter updating process of SGD can
be approximated by a stochastic differential equation. Zhu et al. (2019); Nguyen et al. (2019) discussed the relation
betweentherandomnoiseofSGDandtheescapeefficiencyfromthesharpminimaofthelossfunction. Oneexample
ofavariantofSGDisstochasticgradientLangevindynamics(SGLD),whichisanextensionofSGDthataddsGaussian
noise to the update formula of SGD. Raginsky et al. (2017) analyzed the dynamics of stochastic gradient Langevin
dynamics (SGLD) as a variant of SGD and proved the parameters optimized by SGLD converge to the global minima
ofthegeneralizationerror. Asanotherexample,Jastrzebskietal.(2017);Heetal.(2019);Mandtetal.(2017)analyzed
thedynamicsofSGDwithaconstantlearningrateundertheassumptionsthatthenoiseofSGDonthegradientinduced
by the mini-batch sampling is isotropic, and derived the probability distribution of the parameters obtained by SGD.
Latz (2021) analyze SGD both in the case of the constant learning rate and of the decreasing learning rate.
1Under review as submission to TMLR
Among the methods analyzing the properties of SGD, one of the most general approaches is to study a stationary
distribution of parameters updated by SGD and its variants. The stationary distribution is a distribution that remains
unchangedwhentheparameterisupdatedbyonestep. Itisusefulintheoreticalanalysis,because(i)itcananalyzethe
global dynamics of the optimization algorithm, and (ii) it can be applied to a wide range of loss functions regardless
of its shape. For these reasons, we can use it to investigate the optimization of complex loss functions such as those
used for training deep neural networks. For example, (Dieuleveut et al., 2020) studied the stationary distribution of
the parameter optimized by SGD when the loss function is strongly convex, and (Raginsky et al., 2017) studied the
stationary distribution of SGLD when the loss function is non-convex.
Despitetheaboveadvantages,therearenotmanySGDvariantstowhichstationarydistributionanalysiscanbeapplied.
Thisisbecause,tousetheanalysisbyastationarydistribution,itisrequiredthatthedirectionofparameterupdatesby
analgorithmdoesnotdegenerate;inotherwords,theremustbenodirectionsthatarenotbeingexplored. Examplesof
suchvariantsare(i)SGLD(Welling&Teh,2011;Dalalyan,2017;Durmus&Moulines,2016),whichaddsaGaussian
noise to the parameter update of SGD and (ii) Gaussian SGD (Jastrzebski et al., 2017; He et al., 2019; Mandt et al.,
2017), which assumes that the noise of SGD on the gradient induced by the mini-batch sampling is non-degenerate
Gaussian. In contrast, the parameter update of SGD degenerates in many practical cases, such as deep learning (Zhu
et al., 2019; Nguyen et al., 2019; Simsekli et al., 2019). We remark that we focus on the degeneracy of the update
directionofSGD,notonthedistributionofitsincethereisnoclearagreementthatgradientnoisefollowsaparticular
distribution (the mathematical definition of degeneracy is in Remark 2). Hence, there is a gap between the variants of
SGDconsideredinthetheoreticalanalysisandtheempiricalfactsaboutSGD.Thisgapfostersthefollowingquestion:
Do parameters optimized by a variant of SGD have a stationary distribution
even if the update direction degenerates - and if so, what is the form of it?
1.1 Our Contribution
We theoretically prove that a variant of SGD has a stationary distribution even if the update direction degenerates.
Specifically,wedevelopanovelSGDvariantwitha randomlearningrate ,whichfollowsthePoissonprocessdepending
on a mini-batch gradient. We call the variant Poisson SGD , and prove that the distribution of a parameter updated
by Poisson SGD converges to a stationary distribution. As a result, we provide a positive answer to the question
posed above: even with a degenerated parameter update, it is possible to construct a variant of SGD that reaches a
stationarydistributionbyusingarandomlearningrate. Wenotethatourlearningratehasaroleofefficientlyexploring
parameters, which differs from conventional methods with adaptive step size such as Adam.
Ourspecificcontributionsareasfollows. Weconsidertheempiricalriskminimizationproblemandprovethefollowing
results under weak assumptions on the loss function such as absolute continuity: (i) the distribution of the parameters
updated by Poisson SGD converges to a stationary distribution, and (ii) an output of Poisson SGD converges to the
global minima of the empirical risk, applying the stationary distribution while controlling the inverse-temperature
parameter. Furthermore,weevaluatethegeneralizationerroroftheupdatedparameterforpredictionwithunseendata
by studying an expectation of the risk function in terms of the obtained stationary distribution.
Onthetechnicalside,weutilizeanalgorithmcalledtheBouncyParticleSampler(BPS)todemonstratetheconvergence
tothestationarydistributionbyPoissonSGD.BPSisapiecewisedeterministicMarkovprocess(PDMP)thatachieves
ergodicity using stochastically occurring jumps (Davis, 1984; 1993). In our proof, we show that the distribution
of parameters updated by Poisson SGD can be well approximated by that of BPS, and we concretely construct the
stationary distribution using the theory of BPS.
1.2 Related Work
There are many works which investigate the stationary distribution of SGD or its variants. Dieuleveut et al. (2020);
Chen et al. (2022) derived the stationary distribution of the parameters obtained by SGD when the loss function is
strongly-convex,throughthetheoriesaboutMarkovprocesses. TheparametersobtainedthroughtheSGLDalgorithm
aretheoreticallyproventoconvergetotheGibbsdistributionandgeneralizewell(Raginskyetal.,2017). Heetal.(2019)
andMandtetal.(2017)assumedthenoiseofSGDisGaussianwhosecovariancematrixisconstantandapproximatethe
process of optimization through SGD by Ornstein-Uhlenbeck process and derive its stationary distribution. Gradient
Langevindynamics(GLD),whichisafull-batchversionofSGLD,canalsobeseenasavariantofSGDwhichassumes
2Under review as submission to TMLR
that the noise of SGD is Gaussian with a covariance matrix of constant multiples of the identity matrix. Like SGLD,
it converges to a stationary distribution even in non-convex scenarios (Dalalyan, 2017; Durmus & Moulines, 2016).
Intermsofarandomlearningrate,thereareseveralempiricalstudies. Musso(2020)investigatedthedynamicsofSGD
with a random learning rate by analyzing the stochastic differential equation and its Fokker-Planck equation. Blier
et al. (2019) showed experimentally that SGD with random learning rates performs well in optimizing deep neural
networks. We remark that these studies and ours have several major differences. The first difference is in the design
of a learning rate. Our method considers Poisson processes, whereas existing methods consider uniform distributions
and heterogeneous learning rates for each subneural network. The second difference is the objective of the study. We
aim to evaluate global convergence, whileexisting studies aim at interpretability, speed of convergence, etc., and have
very different motivations.
As for BPS, (Deligiannidis et al., 2019; Durmus et al., 2020) proved that the parameters updated by continuous-time
BPSconvergetoastationarydistributionandderivedtheconcreteformofthestationarydistributionanditsconvergence
rate. (Sherlock & Thiery, 2022) clarified the relation between discrete-time BPS and continuous-time BPS.
1.3 Notation
For a natural number 𝑎∈N, we define[𝑎]:={1,2,...,𝑎}. For a real𝑧∈R,⌊𝑧⌋denotes the largest integer which is
nomore than 𝑧.𝐼𝑑isa𝑑-dimensionalidentity matrix. ⟨𝑎,𝑏⟩meansthe innerproductin Euclidspace, i.e.,sum ofthe
product of each component. ∥·∥ 1and∥·∥mean the vector norms which represent 1-norm and 2-norm respectively.
S𝑑−1is a unit sphere in R𝑑. For probability measures 𝑃,𝑃′onR𝑑and𝑝∈ [1,∞], the𝑝−Wasserstein distance
is defined asW𝑝(𝑃,𝑃′):=inf𝜋∈Π(𝑃,𝑃′)(∫
R𝑑∥𝑧−𝑧′∥𝑝
𝑝𝑑𝜋(𝑧,𝑧′))1/𝑝, whereΠ(𝑃,𝑃′)is a set of coupling measure
between𝑃and𝑃′.∥𝑃−𝑃′∥TVdenotes the total variation of 𝑃−𝑃′.Γ:R→Rdenotes the gamma function, i.e.,
Γ(𝑧)=∫∞
0𝑡𝑧−1𝑒−𝑡𝑑𝑡.B :R×R→Rdenotesthebetafunction, i.e.,B(𝑥,𝑦)=∫1
0𝑡𝑥−1(1−𝑡)𝑦−1𝑑𝑡. Foracompactset
Θ,wedenote diam(Θ)=sup𝜃1,𝜃2∈Θ∥𝜃1−𝜃2∥. Forarandomvariable 𝑋∈X,E𝑋[𝑋]denotestheexpectedvaluewith
regard to𝑋,i.e.,∫
X𝑥𝑑𝜇𝑋(𝑑𝑥), where𝜇𝑋is the probability measure of 𝑋. 1[·]denotes an indicator function, which
takes 1if the condition in the bracket is satisfied, and 0otherwise. We denote 𝑎+=max{0,𝑎}.
2 Preliminary
2.1 Problem Setup: Empirical Risk Minimization
We consider the following stochastic optimization problem. Let Zbe a compact sample space, and consider a
probability measure 𝑃∗onZ. Suppose that we observe 𝑛samplesz={𝑧1,...,𝑧𝑛}⊂Z, that are independently and
identically generated from the measure 𝑃∗. Using the samples, we consider an empirical risk with a loss function.
LetΘ⊂R𝑑be a compact parameter space, and define 𝑊:=diam(Θ). With a (potentially non-convex) loss function
ℓ:Z×Θ→R, we consider the following empirical risk with the samples:
𝐿z(𝜃)=1
𝑛𝑛∑︁
𝑖=1ℓ(𝑧𝑖;𝜃), 𝜃∈Θ. (1)
Ourgoalistofindaglobalminimumoftheempiricalrisk 𝐿z(·),whichisdefinedasaparameter 𝜃∗∈Θwhichsatisfies
𝐿z(𝜃∗)=min
𝜃′∈Θ𝐿z(𝜃′). (2)
In this study, we consider a torus as the parameter space Θin order to skip the argument of constraining parameter
updates to a compact set. If we consider another compact parameter space such as a hypercube, a projection onto the
spaceneedstobeadded. Wealsomentionthattheboundednessofparameterspacesisanacceptedsettinginprevious
studies on SGD, e.g. Ljung (1977); Kushner & Yin (2003); Bonnabel (2013); Tripuraneni et al. (2018); Lan (2020);
Boumal (2023).
3Under review as submission to TMLR
2.2 Gradient Descent Algorithm and Variants
To find the global minimum 𝜃∗as defined in (2), we often use the optimization algorithm called stochastic gradient
descent (SGD) with momentum.
2.2.1 General Form of Stochastic Gradient Descent
WegiveaformaldefinitionofSGDwithamomentumtermassociatedwithempiricalrisk 𝐿z(𝜃)in(1). Let𝐾∈Nbe
the number of iterations. The SGD with momentum generates a sequence of Θ-valued random parameters 𝜃1,...,𝜃𝐾
andR𝑑-valued random vectors 𝑣1,...,𝑣𝐾, by the following procedure.
Let𝜃0∈Θbe an arbitrary parameter for the initialization, 𝑣0∈R𝑑as an initial velocity vector, and 𝑚∈[𝑛]be a
numberofsub-samples,i.e.,thebatchsize. Supposethatweobservethe 𝑛samples𝑧:={𝑧1,...,𝑧𝑛},i.e.,thefull-batch.
For𝑘=1,...,𝐾, we uniformly sample 𝑚integers𝐼(𝑘)={𝑖1,𝑖2,...,𝑖𝑚}from[𝑛], which is called mini-batch sampling
with the batch-size 𝑚. We define an associated mini-batch risk as
b𝐿(𝑘)
z(𝜃):=1
𝑚∑︁
𝑖∈𝐼(𝑘)ℓ(𝑧𝑖;𝜃). (3)
Then, with initial values 𝜃0∈Θand𝑣0∈R𝑑, the SGD with momentum generates the parameter and the velocity
vector by the following recursive formula for 𝑘=1,...,𝐾:
𝜃𝑘=𝜃𝑘−1+𝜂𝑘𝑣𝑘−1,and
𝑣𝑘=𝑣𝑘−1−𝛼𝑘∇b𝐿(𝑘)
z(𝜃𝑘), (4)
where𝜂𝑘>0is a learning rate and 𝛼𝑘∈Ris a momentum coefficient. This form is generic and can be identical to
other forms of SGD with momentum (Qian, 1999; Sutskever et al., 2013) by adjusting the parameters 𝜂and𝛼.
Remark 1 (Gradient Noise) .For the sake of technical discussions below, we define a notion of gradient noise
𝜉(𝑚,𝑛)
𝑘(𝜃):=∇b𝐿(𝑘)
z(𝜃)−∇𝐿z(𝜃)for𝑘=1,...,𝑁and𝜃∈Θ, which is caused by sub-sampling of the SGD. If one
assumes that 𝜉(𝑚,𝑛)
𝑘(𝜃)follows a centered Gaussian distribution with an identity covariance, the SGD corresponds to
thegradientLangevindynamics(GLD).However,itisempiricallyobservedthatthecovariancematrixofthegradient
noise often degenerates (Zhu et al., 2019; Cheng et al., 2020; HaoChen et al., 2021). In addition, there is still much
discussion on a distribution that gradient noise follows, e.g. Simsekli et al. (2019) and Battash et al. (2024) reports
the non-Gaussianity of the gradient noise in empirical studies. Due to these situations, we do not consider a full-rank
covariance matrix nor a particular distribution of the gradient noise.
Remark 2 (Degeneracy of the gradient noise) .We briefly explain the degeneracy of the gradient noise. Since the
covariance matrix of the gradient noise with the batch-size 𝑚is written as
1
𝑚𝑚∑︁
𝑖=1(∇ℓ(𝑧𝑖;𝜃)−∇𝐿z(𝜃))(∇ℓ(𝑧𝑖;𝜃)−∇𝐿z(𝜃))⊤,
whereeachterminthesumisarank-1matrix,therankofatotalcovariancematrixisnogreaterthan 𝑚. Hence,inthe
over-parameterized models like neural networks, the matrix becomes rank-deficient, which we refer to as degeneracy
of the noise.
3 Our SGD Variant: Poisson SGD
Inthissection,weintroduceouralgorithm, PoissonSGD ,whichisavariantofSGDwitharandomlearningrate 𝜂and
momentumcoefficient 𝛼. Wedesignourmethodsothattheparametercansearchthewholeparameterspaceowingto
the design.
Wedescribetherandomlearningrate. Inpreparation,wedefinethefollowingexponentialdistributionfunctionwitha
function𝑓:Θ→R𝑑and parameters 𝜃∈Θ,𝑣∈S𝑑−1:
𝐸(𝑓(·),𝜃,𝑣):=exp
−∫𝑡
0{max{⟨𝑓(𝜃+𝑟𝑣),𝑣⟩,0}+𝐶𝑃}𝑑𝑟
,
4Under review as submission to TMLR
Algorithm 1 Poisson SGD
1:Initialize(𝜃0,𝑣0)as∥𝑣0∥=1.
2:for𝑘=1,2,...,𝐾do
3:Sample𝐼(𝑘)⊂[𝑛]and obtain∇b𝐿(𝑘)
z(𝜃𝑘)as (3).
4:Sample𝜂𝑘as (5).
5:Obtain𝜃𝑘as𝜃𝑘=𝜃𝑘−1+𝜂𝑘𝑣𝑘−1.
6:Obtain𝑣𝑘as𝑣𝑘=𝑣𝑘−1−𝛼𝑘∇b𝐿(𝑘)
z(𝜃𝑘)with𝛼𝑘as (6).
7:end for
8:Return(𝜃𝐾,𝑣𝐾).
where𝐶𝑃>0is some constant. Then, for each update 𝑘=1,...,𝐾, we design the random learning rate 𝜂𝑘following
the exponential distribution:
𝑃(𝜂𝑘≥𝑡)=𝐸(𝛽∇b𝐿(𝑘)
z(·),𝜃𝑘−1,𝑣𝑘−1), (5)
where𝛽>0is the hyper-parameter of Poisson SGD, called an inverse temperature parameter.
Second, we select the momentum coefficient 𝛼𝑘for each𝑘=1,...,𝐾as
𝛼𝑘=2⟨∇b𝐿(𝑘)
z(𝜃𝑘),𝑣𝑘−1⟩
∥∇b𝐿(𝑘)
z(𝜃𝑘)∥2+𝐶𝛼, (6)
where𝐶𝛼≥0is the hyper-parameter. While 𝐶𝛼has the function of enhancing the effect of the gradient for practical
use, we set𝐶𝛼=0in the theoretical analysis of this paper (in experiments in Section 7, we set 𝐶𝛼>0). This setup
keepsthelengthofthevelocityvectorconstantas ∥𝑣𝑘∥=1forevery𝑘(SeeProposition7inAppendix),andonlyuses
itsangletoupdatetheparameters. Weupdatetheparameterbychanging 𝜂𝑘and𝛼𝑘ineveryiteration. Thepseudo-code
of Poisson SGD is shown in Algorithm 1.
The algorithm is designed to effectively explore large regions of the parameter space Θ. Specifically, the update
directionisdeterminedbythevelocityvector 𝑣𝑘normalisedby 𝛼𝑘as(6),andthesizeoftheupdateisrandomlysetby
therandomlearningrate 𝜂𝑘as(5). Whenthegradient ∇b𝐿(𝑘)
z(·)issmall,thelearningrate 𝜂𝑘ischosentobelarge,thus
the updated parameter tends to escape local minima or saddle points. Figure 1 illustrates that Poisson GD, which we
refer to as the full-batch version of Poisson SGD explores a wider parameter space and discovers the global minimum
owing to the random learning rate, while the parameters updated by GD converge to the local minimum. Here, we set
the learning rate of GD as 𝜂=0.02and the hyper-parameter of Poisson GD as 𝐶𝑃=100and𝛽=10000.
Remark 3 (Moments of Poisson SGD) .We claim that even if the learning rate is random, the actual updates are
not too large, by studying its moments. That is, if 𝐶𝑃is sufficiently large, there is little chance of sampling a
large learning rate 𝜂, since the first and second moments of 𝜂𝑘are given as E[𝜂𝑘]≤∫∞
0exp(−𝐶𝑃𝑡)𝑑𝑡=1
𝐶𝑃and
E[𝜂2
𝑘]−E[𝜂𝑘]2≤∫∞
02𝑠exp(−𝐶𝑃𝑠)𝑑𝑠−1
𝐶2
𝑃=1
𝐶2
𝑃. By this property, we can avoid the case in which 𝜂𝑘diverges. In
addition,evenifalarge 𝜂𝑘issampled,theparameterdoesnotexitfromtheparameterspacesinceweconsideratorus
as the parameter space.
4 Convergence Theory for Poisson SGD
We provide theoretical results on the convergence of Poisson SGD (Algorithm 1). Our main interest is a distribution
of the generated parameter 𝜃𝐾by Poisson SGD associated with the empirical risk minimization problem (2).
4.1 Stationary Distribution of Poisson SGD
In this section, we show that the parameter 𝜃𝐾by the Poisson SGD follows a stationary distribution. Formally, we
define the stationary distribution of the Markov process. In preparation, we utilize the notion of transition probability
𝑄(𝜃,𝑑𝑤)from a distribution 𝑝0(𝜃)to another𝑝1(𝜃)onΘ, that is,𝑝1(𝑤)=∫
Θ𝑄(𝜃,𝑑𝑤)𝑝0(𝑑𝜃)holds.
5Under review as submission to TMLR
Figure 1: The comparison of the trajectories of GD (with a fixed learning rate) and Poisson GD (with the random
learning rate) in optimizing the function 𝑧=𝑥4−4𝑥3−36𝑥2+𝑦2. Poisson GD represents replacing the mini-batch
lossb𝐿zby the full-batch loss 𝐿zin Poisson SGD, where we set 𝐿z(𝑥,𝑦)=𝑥4−4𝑥3−36𝑥2+𝑦2. The point(−3,0)
representsalocalminimumandthepoint (6,0)isidentifiedastheglobalminimum. Agreenpointindicatestheinitial
position, a black line represents the trajectory of GD, and a red line represents the trajectory of Poisson GD.
Definition1. Let𝑄(𝜃,𝑑𝑤)bethetransitionprobabilityofaMarkovprocessin Θ. Ifthefollowingequationholds,we
call the probability distribution 𝜋(𝜃)stationary distribution of the Markov process:
𝜋(𝑑𝑤)=∫
Θ𝑄(𝜃,𝑑𝑤)𝜋(𝑑𝜃).
A stationary distribution is an useful notion to represent a limit of the parameter distribution, and it enables us
to analyze where the parameter converges by algorithms. For example, see the theoretical framework to analyze
stochastic optimization algorithms by (Raginsky et al., 2017).
4.1.1 Assumption
We provide several principal assumptions. First, we consider the basis assumptions on the loss function ℓ(·;·). The
followingconditionsarefairlygeneralfortheanalysisofstochasticoptimizationalgorithms,e.g. Bertazzietal.(2022).
Assumption 1 (Loss function) .The loss function ℓ:Z×Θ→R≥0satisfies the following conditions:
•ℓ(𝑧;𝜃)is absolutely continuous and differentiable with respect to 𝜃∈Θfor every𝑧∈Z.
•∇𝜃ℓ(𝑧;𝜃)is continuous in 𝜃and𝑧for all𝜃∈Θand𝑧∈Z.
The first condition is satisfied by a large class of models, such as linear regression model, or deep neural networks
whose activation function is ReLU or sigmoid function. From the second condition, we define an upper bound
𝑀ℓ:=max𝜃∈Θ,𝑧∈Z∥∇𝜃ℓ(𝑧;𝜃)∥sinceΘandZare compact.
4.1.2 Statement of Convergence
Let𝜇z,𝐾beadistributionoftheoutput 𝜃𝐾fromthePoissonSGDinAlgorithm1withthegivendataset z. Wediscuss
the convergence of 𝜇z,𝐾as𝐾increases.
In preparation, we define a probability measure on Θfor arbitrary 𝛽,𝜀> 0, whose density is written as follows:
𝜇(𝛽,𝜀)
z(𝑑𝜃)∝
𝛽𝑀ℓ+1
𝜀+𝑎𝑑𝛽∥∇𝐿z(𝜃)∥
exp(−𝛽𝐿z(𝜃))𝑑𝜃, (7)
6Under review as submission to TMLR
where𝑎𝑑:= Γ(𝑑/2)/(√𝜋Γ(𝑑/2+1/2)). The probability measure (7) is concentrated around the global minima
of𝐿z(𝜃), since the dominant exponential term exp(−𝛽𝐿z(𝜃))in (7) increases in 𝐿z(𝜃). In addition, as the inverse
temperature parameter 𝛽increases, the measure 𝜇(𝛽,𝜀)
zconcentrates more around the global minimum.
Weshowourresultsontheconvergenceofthestationarydistribution. ThediscrepancyismeasuredbytheWasserstein
distanceW1(·,·). We remark that this theorem is the integration of Theorem 3 and Theorem 4 appearing later in
Section 5. Recall that we defined 𝑊:=diam(Θ).
Theorem1 (StationarydistributionofPoissonSGD) .Fixarbitrary 𝛽,𝜀> 0. SupposeAssumption1holds. Wesetthe
hyper-parameter of Poisson SGD as 𝐶𝑃=1/𝜀. Then, for any 𝐾∈N, the distribution 𝜇z,𝐾satisfies
W1(𝜇z,𝐾,𝜇(𝛽,𝜀)
z)≤4√
𝑑𝐾𝜀+𝑊·𝜅(𝛽,𝜀,𝑑)𝐾, (8)
where 0<𝜅(𝛽,𝜀,𝑑)<1is a constant.
Moreover, if 𝜅(𝛽,𝜀,𝑑)satisfies lim𝐾→∞𝜅(𝛽,𝛿/𝐾,𝑑)𝐾=0with some𝛿 >0, there exists a sequence 𝜀=𝜀𝐾↘0as
𝐾→∞such thatW1(𝜇z,𝐾,𝜇(𝛽,𝜀)
z)=𝑜(1)as𝐾→∞holds.
Thistheoremshowsthattheparameterdistribution 𝜇z,𝐾byPoissonSGDconvergestothestationarydistribution 𝜇(𝛽,𝜀)
z
owingtotherandomlearningrate(5). ThisiscontrasttoordinarySGD,whichisnotshowntoconvergetoastationary
distribution. Further, Poisson SGD does not make any assumptions on the gradient noise 𝜉(𝑛,𝑚)
𝑘in Remark 1, unlike
SGLD, which converges to a stationary distribution by introducing Gaussianity in the gradient noise.
The right-hand side in (8) shows an approximation-complexity trade-off of Poisson SGD described as follows. In
preparation, we will introduce a certain stochastic process to achieve the stationary distribution 𝜇(𝛽,𝜀)
z(detail is in
Section5). Thefirsttermof(8)describesanapproximationerrorofPoissonSGDtothestochasticprocess. Thesecond
term of (8) denotes a convergence error of the stochastic process to the stationary distribution 𝜇(𝛽,𝜀)
z, which reflects
the complexity of the stochastic process. 𝜀is a parameter for the stochastic process and controls the balance between
the approximation error and the complexity error.
Wefurtherdiscusstheadditionalassumption lim𝐾→∞𝜅(𝛽,𝛿/𝐾,𝑑)𝐾=0. Thisconditionisrelatedtotheconvergence
rateoftheapproximatedstochasticprocessofPoissonSGD.Althoughtheexplicitformof 𝜅(𝛽,𝛿/𝐾,𝑑)isnotclarified
inourcase,thereisacommonexamplehavingitsexplicitform. OneexampleisSGLD:Raginskyetal.(2017)shows
that a form of 𝜅(𝛽,𝛿/𝐾,𝑑)can be calculated, because SGLD is reduced to the Langevin process.
Remark 4 (Form of𝜅(𝛽,𝜀,𝑑)).We discuss a form of 𝜅(𝛽,𝜀,𝑑)of other related algorithms, although we could not
achievetheexplicitformof 𝜅(𝛽,𝜀,𝑑)ofPoissonSGD.InthecaseofLangevindynamicswiththesettingofRaginsky
et al. (2017), 𝜅(𝛽,𝜀,𝑑)isΩ(𝑐𝐿𝑆𝑘𝜂/𝛽(𝛽+𝑑)), where𝑐𝐿𝑆is the logarithmic Sobolev constant. On the other hand,
explicitly deriving 𝜅(𝛽,𝜀,𝑑)for a class of PDMP is a challenging task as described in Deligiannidis et al. (2019);
Durmus et al. (2020), as well as that of Poisson-SGD.
Remark 5 (Comparison with SGLD) .We discuss the difference between Poisson SGD and SGLD, which is another
method achieving a stationary distribution. First, while SGLD adds a Gaussian noise to the update formula of SGD,
Poisson SGD does not have an additive noise. The second difference is the form of the stationary distribution. A
stationary distribution of SGLD is the Gibbs distribution, and that of Poisson SGD has the different form (7). This
difference is derived from the random learning rate of Poisson SGD.
Remark 6 (Relation to flat minima) .From Theorem 1, we can state the property of Poisson SGD being easier to go
totheflatminimathanthesharpminima. Weconsidertheprobabilityofexistencearoundaflatminimum 𝜃1∈Θand
a sharp minimum 𝜃2∈𝜃, when we find that, due to the shape of the distribution, a measure of an 𝜖-neighborhood of
𝜃1is greater than that within an 𝜖-neighborhood of 𝜃2. Hence, we can claim that Poisson SGD also tends to favor flat
minima.
4.2 Global Convergence
We discuss the global convergence statement, that is, the empirical risk 𝐿z(𝜃𝐾)with Poisson SGD is minimized with
high probability. We consider the additional assumption for the loss function ℓ:
Assumption 2. With some𝑐1>0,sup𝑧∈Z∥∇ℓ(𝑧;𝜃1)−∇ℓ(𝑧;𝜃2)∥≤𝑐1∥𝜃1−𝜃2∥holds for every 𝜃1≠𝜃2∈Θ.
7Under review as submission to TMLR
Then,weobtainthefollowingglobalconvergencetheorem. Wedefine 𝐵:=sup𝑧∈Z∥∇ℓ(𝑧; 0)∥byfollowingAssump-
tion 1.
Theorem 2 (Global convergence of Poisson SGD on empirical risk) .Fix arbitrary 𝛽,𝜀 > 0. Consider Poisson SGD
in which𝐶𝑃=1/𝜀. Let the upper bound of W1(𝜇z,𝐾,𝜇(𝛽,𝜀)
z)obtained in Theorem 1 be 𝑑𝐾(𝛽,𝜀,𝑑). Also, suppose
that Assumption 1 and 2 hold. Then, it holds that
E𝜃𝐾∼𝜇z,𝐾[𝐿z(𝜃𝐾)]−min
𝜃∈Θ𝐿z(𝜃)
≤(𝑐1𝑊+𝐵)√︁
𝑊𝑑𝐾(𝛽,𝜀,𝑑)+1
𝛽𝑑
2log𝑒𝑊2𝑐1𝛽
𝑑+log
1+𝑎𝑑(𝑐1𝑊+𝐵)
𝑀ℓ
. (9)
Theorem2statesthatwecanmake E[𝐿z(𝜃𝐾)]bearbitrarilycloseto min𝜃∈Θ𝐿z(𝜃)byselectinglarge 𝛽,providedthat
we can make 𝑑𝐾(𝛽,𝜀,𝑑)arbitrarily small by the choice of 𝜀and𝐾in spite of𝛽. Intuitively, Poisson SGD achieves
global convergence by appropriately adjusting the learning rate and momentum coefficient based on the shape of the
loss function at the current location. Poisson SGD achieves the global convergence by the similar approach of global
convergence of SGLD by Raginsky et al. (2017).
The right-hand side of (9) is divided into two terms. The first term expresses the distance between the parameter and
its stationary distribution. The second represents the degree of concentration of the stationary distribution 𝜇(𝛽,𝜀)
zon
the global optima. The higher the inverse temperature 𝛽, the more the term decreases.
5 Proof Outline
5.1 Overview
We give an overview of a proof of Theorem 1. In preparation, we present several key concepts: (i) the property of
thepiece-wise deterministic Markov process (PDMP) (Davis, 1984; 1993), and (ii) the ergodicity of bouncy particle
sampler(BPS) (Peters & de With, 2012). The PDMP is a class of Markov processes that behave deterministically for
someperiodandjumpsrandomly,whicheasilyconvergestoastationarydistribution. BPSisastochasticalgorithmin
the class of the PDMP.
We show the statement by the following steps:
(I) We show that the distribution of the parameter by Poisson SGD is sufficiently close to that of a parameter by
BPS. We show this claim by using the approximation theory on PDMP (Theorem 3).
(II) We derive a stationary distribution and the ergodicity of BPS, following previous researches (Theorem 4).
5.2 Design of BPS
We introduce BPS, which is one of the most popular algorithms in PDMPs, and actively studied in terms of MCMC
algorithm(Deligiannidisetal.,2019;Bouchard-Côtéetal.,2018). BPSgeneratesasequenceofparameters {b𝜃𝑘}𝐾
𝑘=1⊂Θ
andvelocityvectors {b𝑣𝑘}𝐾
𝑘=1⊂R𝑑initsrecursivemanner,asshowninAlgorithm2. Let (b𝜃0,b𝑣0)betheinitialization.
For the𝑘-th iteration, BPS generates a learning rate b𝜂𝑘from an exponential distribution whose intensity depends on
the previous pair(b𝜃𝑘−1,b𝑣𝑘−1)and the positive constants Λrefand𝐶𝐵. After obtaining the parameter b𝜃𝑘, we consider
the stochastic update of the velocity vector. That is, with the probability
b𝑝𝑘:=𝛽⟨∇𝐿z(b𝜃𝑘),b𝑣𝑘−1⟩++𝐶𝐵
𝛽⟨∇𝐿z(b𝜃𝑘),b𝑣𝑘−1⟩++Λref+𝐶𝐵, (10)
weupdatethevelocityvectorwiththegradientofthefull-batchloss ∇𝐿z,otherwisewiththesamplefromtheuniform
distribution on S𝑑−1. The former update is called reflection, and the latter is refreshment . We remark that∥b𝑣𝑘∥is
constant for𝑘=1,2,...,𝐾in the same way as Poisson SGD (See Proposition 7 in Appendix).
5.3 Connect Poisson SGD and BPS
WeshowthattheoutputdistributionofPoissonSGDandthatofBPSaresufficientlycloseasinthefollowingstatement:
8Under review as submission to TMLR
Algorithm 2 Bouncy Particle Sampler
1:Initialize(b𝜃0,b𝑣0)as∥b𝑣0∥=1.
2:for𝑘=1,2,...,𝐾do
3:Sample b𝜂𝑘asb𝜂𝑘∼𝑃(b𝜂𝑘≥𝑡)=exp
−∫𝑡
0{𝛽⟨∇𝐿z(b𝜃𝑘−1+𝑟b𝑣𝑘−1),b𝑣𝑘−1⟩++Λref+𝐶𝐵}𝑑𝑟
4:Update b𝜃𝑘asb𝜃𝑘=b𝜃𝑘−1+b𝜂𝑘b𝑣𝑘−1
5:With probability b𝑝𝑘as (10), update b𝑣𝑘as
b𝑣𝑘=b𝑣𝑘−1−2⟨∇𝐿z(b𝜃𝑘),b𝑣𝑘−1⟩
∥∇𝐿z(b𝜃𝑘)∥2∇𝐿z(b𝜃𝑘)
Otherwise, update b𝑣𝑘as
b𝑣𝑘∼Unif(S𝑑−1)
6:end for
7:Return(b𝜃𝐾,b𝑣𝐾)
Theorem3 (DistancebetweenPoissonSGDandBPS) .Fixarbitrary 𝛽,𝜀> 0. AsforPoissonSGD,weset 𝐶𝑃=1/𝜀.
As for BPS, we set Λrefand𝐶𝐵asΛref+𝐶𝐵=𝛽𝑀ℓ+1/𝜀. Let the distribution of the obtained parameter by Poisson
SGD and BPS be 𝜇z,𝐾andb𝜇z,𝐾respectively. We set the same initial value between Poisson SGD and BPS. Then, the
following holds:
W1(𝜇z,𝐾,b𝜇z,𝐾)≤4√
𝑑𝐾𝜀.
For proving this theorem, we calculate the distance between Poisson SGD and BPS by a one-step update. Then, we
simplyaccumulatethiserrorfor 𝐾times. Inthisdiscussion,wemainlyusethepropertythatiflearningrate 𝜂𝑘andb𝜂𝑘
are small, the difference of 𝑣𝑘andb𝑣𝑘is also made to be small. This type of discussion is also used in Raginsky et al.
(2017).
5.4 The Stationary Distribution and Ergodicity of BPS
In this section, we investigate the stationary distribution and ergodicity of BPS. First, we define the term ergodicity .
Definition 2 (Ergodicity) .We consider the discrete-time Markov process. If the process converges to a unique
stationary distribution, we call the process has the ergodicity. Especially, if the ergodic process converges to its
stationary distribution by the exponential rate about the number of iteration 𝑘, the process is called exponentially
ergodic.
Without ergodicity, the stochastic process may converge to more than one stationary distribution, or not converge to
any stationary distribution due to stacking to a saddle point in the parameter space. So we have to prove this property
when we try to analyze the stationary distribution of a stochastic process.
Now, we show our result about BPS.
Theorem 4 (Stationary Distribution of BPS) .Suppose that Assumption 1 holds. Set the parameter of BPS, Λrefand
𝐶𝐵as in Theorem 3. Then, the distribution b𝜇z,𝐾of the obtained parameters b𝜃𝐾by BPS satisfies the following:
∥b𝜇z,𝐾−𝜇(𝛽,𝜀)
z∥TV≤𝜅(𝛽,𝜀,𝑑)𝐾,
where𝜅(𝛽,𝜀,𝑑)is a positive constant less than 1.
In the proof of this theorem, we use the discussion in Deligiannidis et al. (2019) which showed that continuous-time
BPS converges to the unique stationary distribution 𝜋(𝜃)∝exp(−𝑈(𝜃))by the exponential rate in TV distance.
6 Generalization Error Analysis
We define an expected risk of 𝜃∈Θ, also known as the generalization error
𝐿(𝜃):=E𝑧∼𝑃∗[ℓ(𝑧;𝜃)],
9Under review as submission to TMLR
which measures a prediction performance with unseen data. We calculate the generalization error of the parameter
obtained by the Poisson SGD, using the discussion in Raginsky et al. (2017).
Now, we give our results. We define 𝐴:=sup𝑧∈Z|ℓ(𝑧; 0)|by following Assumption 1.
Theorem 5 (Generalization Error of Poisson SGD) .Suppose that Assumption 1 and 2 hold. Let 𝜃𝐾be the parameter
obtained by Poisson SGD with 𝐶𝑃=1/𝜀. Then, we obtain the following bound:
Ez∼𝑃𝑛∗[E𝜃𝐾∼𝜇z,𝐾[𝐿(𝜃𝐾)]]− min
𝜃∈Θ𝐿(𝜃)
≤(𝑐1𝑊+𝐵) √︁
𝑊𝑑𝐾(𝛽,𝜀,𝑑)+2𝑊 𝐶𝑑+𝛽𝐶
𝑛1
2
+𝐶𝑑+𝛽𝐶
𝑛1
4!!
+1
𝛽𝑑
2log𝑒𝑊2𝑐1𝛽
𝑑+log
1+𝑎𝑑(𝑐1𝑊+𝐵)
𝑀ℓ
,
where𝑑𝐾(𝛽,𝜀,𝑑)is the upper bound of the Wasserstein distance in Theorem 1, 𝐶𝑑=4𝑎𝑑(𝑐1𝑊+𝐵)/𝑀ℓ, and
𝐶=𝑐1𝑊2+2𝐵𝑊+2𝐴.
Theorem5statesthattheexpectedvalueofthegeneralizationerrorofPoissonSGDcanbearbitrarilyclosetoitsglobal
optima in𝜃∈Θ, by selecting small 𝜀, large𝐾, large𝛽, and large𝑛, provided that 𝑑𝐾(𝛽,𝜀,𝑑)can be arbitrarily small
only by the choice of 𝜀and𝐾.
We further discuss a way of improve an order of the generalization bound in Theorem 5. While our bound has the
order𝑂((1/𝑛)1/4), we can obtain an order 𝑂(1/𝑛)by using the dissipativity condition of the loss function, which is
used in Raginsky et al. (2017) for SGLD. The dissipativity condition allows us to derive log-Sobolev inequality for
𝐿z(𝜃), which leads the improved sample complexity. We state this fact in the following proposition.
Proposition6. SupposethatthesameconditionandsettingasTheorem5hold. Inaddition,weassumethattheGibbs
distribution𝜈(𝛽)
z∝exp(−𝛽𝐿 z(𝜃))satisfies the log-Sobolev inequality for any dataset z={𝑧1,...,𝑧𝑛}, that is,
E[𝑓(𝜃)2log𝑓(𝜃)2]−E[𝑓(𝜃)2]logE[𝑓(𝜃)2]≤𝑐(𝛽)
LSE[∥∇𝑓(𝜃)∥2]
holds for all smooth functions 𝑓and any data z={𝑧1,...,𝑧𝑛}, where𝜃∼𝜈(𝛽)
zand𝑐(𝛽)
LS<∞is a constant. Then, the
following holds:
Ez∼𝑃𝑛∗[E𝜃𝐾∼𝜇z,𝐾[𝐿(𝜃𝐾)]]− min
𝜃∈Θ𝐿(𝜃)
≤(𝑐1𝑊+𝐵) √︁
𝑊𝑑𝐾(𝛽,𝜀,𝑑)+2𝑐(𝛽)
LS𝛽𝑀ℓ
𝑛!
+𝑊√︃
2𝑐(𝛽)
LSlog(1+𝑎𝑑𝛽𝜀𝑀ℓ)+𝑑
2𝛽log𝑒𝑊2𝑐1𝛽
𝑑
.
7 Experiments
Wegiveseveralexperimentalresulttovalidateourtheoreticalclaim. Specifically,weshowthatPoissonSGDcanlearn
parameters in a practical situation. Note that our aim is not to develop an effective method with high generalisation
performance, but to develop a method that can evaluate the global convergence.
7.1 MNIST Dataset
We conducted experiments with the MNIST dataset (Deng, 2012). We consider a 4-layer fully connected neural
networkwith 200unitsofhiddenlayersareall 200andthesigmoidactivationfunction. Wecomparetheperformance
ofPoissonSGDwithSGD,SGDwithMomentum,SGLD.Wesetthebatchsizeas 256,thelearningrateoftheSGD,
the SGD with momentum and SGLD as 0.01, and the momentum coefficient as 0.9. We choose the hyper-parameter
ofPoissonSGDas 𝐶𝑃=100,𝐶𝛼=100,and𝛽=10000. Wealsouse 𝛽=10000forSGLD.Weuse 60000imagesfor
training and 10000images for validation.
Figure 2 shows the result. The vertical line shows the misclassification ratio (%) and the horizontal line shows the
number of epochs. We can see that Poisson SGD achieves sufficiently low errors, suggesting that it achieves a good
minimum. In addition, Poisson SGD converges faster than other methods.
10Under review as submission to TMLR
Figure2: ThecomparisonofthetrainerrorrateandvaliderrorratewhenoptimizedbyPoissonSGD,SGD,SGDwith
momentum, and SGLD for 4-Layer DNN on MNIST.
7.2 CIFAR-10 Dataset
We conducted experiments with the CIFAR-10 dataset (Krizhevsky et al., 2009). We trained a convolutional neural
network of 3layers with the ReLU activation function, a 3×3kernel, and the dropout rate 0.25. We compare the
performanceofPoissonSGDwithSGD,SGDwithMomentum,SGLD.Wesetthebatchsizeas256,thelearningrateof
SGD,SGDwithmomentumandSGLDas0.01,andthemomentumcoefficientas0.9. Wechoosethehyper-parameter
of Poisson SGD as 𝐶𝑃=100,𝐶𝛼=1, and𝛽=10000. We also use 𝛽=10000for SGLD. We use 45000 data for
training and 5000 data for validation.
Figure 3 shows the result. The vertical line shows the misclassification ratio (%) and the horizontal line shows the
numberofepochs. AlthoughPoissonSGDisnotthebestmethod,itachievessufficientlylowerrors,suggestingthatit
achieves a good minimum. In addition, it also achieves good accuracy comparable to that of SGD.
Figure3: ThecomparisonofthetrainerrorrateandvaliderrorratewhenoptimizedbyPoissonSGD,SGD,SGDwith
momentum, and SGLD for CNN on Cifar-10.
8 Conclusion
We developed a new variant of SGD, Poisson SGD, whose search direction degenerates and derived its stationary
distribution by incorporating a modification on the learning rate. The parameters trained by Poisson SGD are close
enough to the global minima to take advantage of convergence to the stationary distribution. The generalization error
is also evaluated. We believe that our work leads to the analysis of the actual SGD dynamics, not variants of it in the
future.
11Under review as submission to TMLR
Broader Impact Statement
The paper aims to provide a theoretical understanding of machine learning methods, which in itself has no direct
negative impact on society.
References
DominiqueBakry,IvanGentil,andMichelLedoux. AnalysisandGeometryofMarkovDiffusionOperators . Springer,
2014.
Barak Battash, Lior Wolf, and Ofir Lindenbaum. Revisiting the noise model of stochastic gradient descent. In
International Conference on Artificial Intelligence and Statistics , 2024.
Andrea Bertazzi, Joris Bierkens, and Paul Dobson. Approximations of piecewise deterministic markov processes and
their convergence properties. Stochastic Processes and their Applications , 154:91–153, 2022.
Léonard Blier, Pierre Wolinski, and Yann Ollivier. Learning with random learning rates. In ECML PKDD , 2019.
François Bolley and Cédric Villani. Weighted csiszár-kullback-pinsker inequalities and applications to transportation
inequalities. Annales de la Faculté des sciences de Toulouse: Mathématiques , 14(3):331–352, 2005.
SilvereBonnabel. Stochasticgradientdescentonriemannianmanifolds. IEEETransactionsonAutomaticControl ,58:
2217—2229, 2013.
Léon Bottou. Stochastic gradient learning in neural networks. Proceedings of Neuro-Nımes , 91(8):12, 1991.
Alexandre Bouchard-Côté, Sebastian J Vollmer, and Arnaud Doucet. The bouncy particle sampler: A nonreversible
rejection-freemarkovchainmontecarlomethod. JournaloftheAmericanStatisticalAssociation ,113(522):855–867,
2018.
Nicolas Boumal. An Introduction to Optimization on Smooth Manifolds . Cambridge University Press, 2023.
ZaiweiChen,ShancongMou,andSivaThejaMaguluri. Stationarybehaviorofconstantstepsizesgdtypealgorithms:
Anasymptoticcharacterization. ProceedingsoftheACMonMeasurementandAnalysisofComputingSystems ,6(1)
(19):1–24, 2022.
XiangCheng,DongYin,PeterBartlett,andMichaelJordan.Stochasticgradientandlangevinprocesses.In International
Conference on Machine Learning , 2020.
ArnakDalalyan. Furtherandstrongeranalogybetweensamplingandoptimization: Langevinmontecarloandgradient
descent. In Conference on Learning Theory , 2017.
M.H.ADavis. Piecewise-deterministicmarkovprocesses: Ageneralclassofnon-diffusionstochasticmodels. Journal
of the Royal Statistical Society , Series B (Methodological)(46):353–388, 1984.
M.H.A Davis. Markov Models and Optimization . Chapman & Hall/CRC Monographs on Statistics & Applied
Probability. Taylor & Francis, 1993.
George Deligiannidis, Alexandre Bouchard-Côté, and Arnaud Doucet. Exponential ergodicity of the bouncy particle
sampler.The Annals of Statistics , 47:1268–1287, 2019.
Li Deng. The mnist database of handwritten digit images for machine learning research. IEEE Signal Processing
Magazine , 29(6):141–142, 2012.
AymericDieuleveut,AlainDurmus,andFrancisBach. Bridgingthegapbetweenconstantstepsizestochasticgradient
descent and markov chains. The Annals of Statistics , 48(3):1348–1382, 2020.
Alain Durmus and Éric Moulines. Sampling from a strongly log-concave distribution with the unadjusted langevin
algorithm. HAL preprint hal-01304430v1 , 2016.
12Under review as submission to TMLR
Alain Durmus, Arnaud Guillin, and Pierre Monmarché. Geometric ergodicity of the bouncy particle sampler. Annals
of Applied Probability , 30:2069–2098, 2020.
Guillaume Garrigos and Robert M. Gower. Handbook of convergence theorems for (stochastic) gradient methods.
arXiv preprint arXiv:2301.11235 , 2023.
Alison L. Gibbs and Francis Edward Su. On choosing and bounding probability metrics. International Statistical
Review / Revue Internationale de Statistique , 70(3):419–435, 2002.
Jeff Z. HaoChen, Colin Wei, Jason D. Lee, and Tengyu Ma. Shape matters: Understanding the implicit bias of the
noise covariance. In Conference On Learning Theory , 2021.
Fengxiang He, Tongliang Liu, and Dacheng Tao. Control batch size and learning rate to generalize well: Theoretical
and empirical evidence. In Advances in Neural Information Processing Systems , 2019.
Elad Hoffer, Itay Hubara, and Daniel Soudry. Train longer, generalize better: closing the generalization gap in large
batch training of neural networks. Advances in neural information processing systems , 30, 2017.
Stanislaw Jastrzebski, Zachary Kenton, Devansh Arpit, Nicolas Ballas, Asja Fischer, Yoshua Bengio, and Amos
Storkey. Three factors influencing minima in sgd. arXiv preprint arXiv:1711.04623 , 2017.
NitishShirishKeskar,DheevatsaMudigere,JorgeNocedal,MikhailSmelyanskiy,andPingTakPeterTang. Onlarge-
batch training for deep learning: Generalization gap and sharp minima. In International Conference on Learning
Representations , 2016.
Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.
Harold J. Kushner and G. George Yin. Stochastic Approximation and Recursive Algorithms . Springer, 2003.
Guanghui Lan. First-order and Stochastic Optimization Methods for Machine Learning . Springer, 2020.
Jonas Latz. Analysis of stochastic gradient descent in continuous time. Statistics and Computing , 31(39), 2021.
QianxiaoLi,ChengTai,andWeinanE. Stochasticmodifiedequationsandadaptivestochasticgradientalgorithms. In
International Conference on Machine Learning , 2017.
LennartLjung. Analysisofrecursivestochasticalgorithm. IEEEtransactionsonautomaticcontrol ,22:551–575,1977.
Stephan Mandt, Matthew D. Hoffman, and David M. Blei. Stochastic gradient descent as approximate bayesian
inference. Journal of Machine Learning Research , 18:1–35, 2017.
Daniele Musso. Stochastic gradient descent with random learning rate. arXiv preprint arXiv:2003.06926 , 2020.
Thanh Huy Nguyen, Umut Simsekli, Mert Gürbüzbalaban, and Gaël Richard. First exit time analysis of stochastic
gradient descent under heavy-tailed gradient noise. Advances in neural information processing systems , 32, 2019.
E.A.J.F.PetersandG.deWith. Rejection-freemontecarlosamplingforgeneralpotentials. PhysicalReviewE ,2012.
Feng Qi and Qiu-Ming Luo. Bounds for the ratio of two gamma functions: from Wendel’s asymptotic relation to
Elezović-Giordano-Pečarić’s theorem. Journal of Inequalities and Applications , 2013(1):542, November 2013.
Ning Qian. On the momentum term in gradient descent learning algorithms. Neural Networks , pp. 145–151, 1999.
Maxim Raginsky, Alexander Rakhlin, and Matus Telgarsky. Non-convex learning via stochastic gradient langevin
dynamics: a nonasymptotic analysis. In Conference On Learning Theory , 2017.
Chris Sherlock and Alexandre H. Thiery. A discrete bouncy particle sampler. Biometrika , 109:335–349, 2022.
Umut Simsekli, Levent Sagun, and Mert Gurbuzbalaban. A tail-index analysis of stochastic gradient noise in deep
neural networks. In International Conference on Machine Learning , pp. 5827–5837. PMLR, 2019.
13Under review as submission to TMLR
IlyaSutskever,JamesMartens,GeorgeDahl,andGeoffreyHinton. Ontheimportanceofinitializationandmomentum
in deep learning. In International Conference on Machine Learning , 2013.
Nilesh Tripuraneni, Nicolas Flammarion, Francis Bach, and Michael I. Jordan. Averaging stochastic gradient descent
on riemannian manifolds. In Conference On Learning Theory , 2018.
Max Welling and Yee Whye Teh. Bayesian learning via stochastic gradient langevin dynamics. In International
Conference on Machine Learning , 2011.
Jingfeng Wu, Wenqing Hu, Haoyi Xiong, Jun Huan, Vladimir Braverman, and Zhanxing Zhu. On the noisy gradient
descent that generalizes as sgd. In International Conference on Machine Learning , 2020.
Zhanxing Zhu, Jingfeng Wu, Bing Yu, Lei Wu, and Jinwen Ma. The anisotropic noise in stochastic gradient descent:
Its behavior of escaping from sharp minima and regularization effects. In International Conference on Machine
Learning, 2019.
A Supportive Information
WeverifythatthevelocityvectorisnormalizedbythechoiceofthemomentumcoefficientforPoissonSGDandBPS.
Proposition7. Considertheupdate (4)for𝑣𝑘withitsmomentumcoefficient (6). Then,for∀𝑘∈{1,2,...,𝐾},wehave
∥𝑣𝑘∥=1. Further, for b𝑣𝑘defined in Algorithm 2, we obtain ∥b𝑣𝑘∥=1for every𝑘=1,...,𝐾.
Proof.We first consider 𝑣𝑘with the Poisson SGD case. Simply, we have
∥𝑣𝑘∥=𝑣𝑘−1−2⟨∇b𝐿(𝑘)
z(𝜃𝑘),𝑣𝑘−1⟩
∥∇b𝐿(𝑘)
z(𝜃𝑘)∥2∇b𝐿(𝑘)
z(𝜃𝑘)
= 
𝐼𝑑−2∇b𝐿(𝑘)
z(𝜃𝑘)∇b𝐿(𝑘)
z(𝜃𝑘)⊤
∥∇b𝐿(𝑘)
z(𝜃𝑘)∥2!
𝑣𝑘−1
=vut
𝑣⊤
𝑘−1 
𝐼𝑑−2∇b𝐿(𝑘)
z(𝜃𝑘)∇b𝐿(𝑘)
z(𝜃𝑘)⊤
∥∇b𝐿(𝑘)
z(𝜃𝑘)∥2!2
𝑣𝑘−1
=∥𝑣𝑘−1∥.
Since we set∥𝑣0∥=1for initialization, the statement holds.
Forb𝑣𝑘with the BPS case, the reflection does not change the norm of b𝑣𝑘in the same way, and the refreshment also
keeps∥b𝑣𝑘∥=1, which completes the proof. □
B Proof of Theorem 1
Proof.By Theorem 3 and 4, we can bound the approximation error
W1(𝜇z,𝐾,b𝜇z,𝐾)≤4√
𝑑𝐾𝜀,
and the convergence error of BPS as
∥b𝜇z,𝐾−𝜇(𝛽,𝜀)
z∥TV≤𝜅(𝛽,𝜀,𝑑)𝐾.
From Theorem 4 in Gibbs & Su (2002) (explicit form is Theorem 13 in Appendix H), we can bound the Wasserstein
distance by the total variation, then obtain
W1(b𝜇z,𝐾,𝜇(𝛽,𝜀)
z)≤𝑊∥b𝜇z,𝐾−𝜇(𝛽,𝜀)
z∥TV≤𝑊𝜅(𝛽,𝜀,𝑑)𝐾.
The triangle inequality completes the proof. □
14Under review as submission to TMLR
C Proof of Theorem 3
Proof.From the definition of Wasserstein distance,
W1(𝜇z,𝑘,b𝜇z,𝑘)= inf
𝜋∈Π(𝜇z,𝑘,b𝜇z,𝑘)E𝜋[∥𝜃𝑘−b𝜃𝑘∥1]
holds,sowestudythedistancebetween 𝜃𝑘andb𝜃𝑘intermsofthenorm ∥·∥1. Since∥𝑣𝑘∥=∥𝑣𝑘−1∥=∥b𝑣𝑘∥=∥b𝑣𝑘−1∥=1
holds by Proposition 7, we have
E𝜋[∥𝜃𝑘−b𝜃𝑘∥1]=E𝜋[∥𝜃𝑘−1+𝜂𝑘𝑣𝑘−1−(b𝜃𝑘−1+b𝜂𝑘b𝑣𝑘−1)∥1]
≤E𝜋[∥𝜃𝑘−1−b𝜃𝑘−1∥1]+E𝜋[∥(b𝜂𝑘−𝜂𝑘)b𝑣𝑘−1+𝜂𝑘(b𝑣𝑘−1−𝑣𝑘−1)∥1]
≤E𝜋[∥𝜃𝑘−1−b𝜃𝑘−1∥1]+E𝜋[∥(b𝜂𝑘−𝜂𝑘)b𝑣𝑘−1∥1]+E𝜋[∥𝜂𝑘(b𝑣𝑘−1−𝑣𝑘−1)∥1]
≤E𝜋[∥𝜃𝑘−1−b𝜃𝑘−1∥1]+√
𝑑E𝜋[|𝜂𝑘−b𝜂𝑘|]+2√
𝑑E𝜋[𝜂𝑘], (11)
where we use∥·∥ 1≤√
𝑑∥·∥in the last inequality.
We first evaluate the second term of (11). There exists a coupling 𝜋such that
E𝜋[|𝜂𝑘−b𝜂𝑘|]=W1(𝑃𝜂𝑘,𝑃b𝜂𝑘)
holds,where 𝑃𝜂𝑘and𝑃b𝜂𝑘denotethedistributionof 𝜂𝑘andb𝜂𝑘respectively. Weusesuchacouplingas 𝜋. Inevaluating
W1(𝑃𝜂𝑘,𝑃b𝜂𝑘), we consider the following analysis. 𝜂𝑘andb𝜂𝑘are 1-dimensional and their cumulative distribution
function is written as
𝐹1(𝑡)=1−exp
−∫𝑡
0(𝛽⟨∇b𝐿(𝑘)
z(𝜃+𝑟𝑣),𝑣⟩++𝐶𝑃)𝑑𝑟
,
𝐹2(𝑡)=1−exp
−∫𝑡
0(𝛽⟨∇𝐿z(𝜃+𝑟𝑣),𝑣⟩++𝐶𝐵+Λref)𝑑𝑟
,
respectively, and we also have
𝛽⟨∇b𝐿(𝑘)
z(𝜃+𝑟𝑣),𝑣⟩++𝐶𝑃≥𝐶𝑃,
𝛽⟨∇𝐿z(𝜃+𝑟𝑣),𝑣⟩++𝐶𝐵+Λref≥𝐶𝐵+Λref,and
|(𝛽⟨∇b𝐿(𝑘)
z(𝜃+𝑟𝑣),𝑣⟩++𝐶𝑃)−(𝛽⟨∇𝐿z(𝜃+𝑟𝑣),𝑣⟩++𝐶𝐵+Λref)|
≤max{|−𝛽𝑀ℓ+𝐶𝑃−(𝐶𝐵+Λref)|,|𝛽𝑀ℓ+𝐶𝑃−(𝐶𝐵+Λref)|}.
Hence, we can use Lemma 8 and obtain
W1(𝑃𝜂𝑘,𝑃b𝜂𝑘)≤max{|−𝛽𝑀ℓ+𝐶𝑃−(𝐶𝐵+Λref)|,|𝛽𝑀ℓ+𝐶𝑃−(𝐶𝐵+Λref)|}
𝐶𝑃(𝐶𝐵+Λref). (12)
Next, we evaluate the third term of (11). We have
E[𝜂𝑘]=∫∞
0𝑃(𝜂𝑘≥𝑡)𝑑𝑡
=∫∞
0exp
−∫𝑡
0{𝛽⟨∇b𝐿(𝑘)
z(𝜃𝑘−1+𝑟𝑣𝑘−1),𝑣𝑘−1⟩++𝐶𝑃}𝑑𝑟
𝑑𝑡
≤∫∞
0exp(−𝐶𝑃𝑡)𝑑𝑡
=1
𝐶𝑃. (13)
Substituting (12) and (13) into (11), we have
E𝜋[∥𝜃𝑘−b𝜃𝑘∥1]≤E𝜋[∥𝜃𝑘−1−b𝜃𝑘−1∥1] (14)
15Under review as submission to TMLR
+√
𝑑max{|−𝛽𝑀ℓ+𝐶𝑃−(𝐶𝐵+Λref)|,|𝛽𝑀ℓ+𝐶𝑃−(𝐶𝐵+Λref)|}
𝐶𝑃(𝐶𝐵+Λref)+2√
𝑑
𝐶𝑃.
Sincewetake 𝐶𝑃inPoissonSGDas 𝐶𝑃=1/𝜀and𝐶𝐵andΛrefinBPSas𝐶𝐵+Λref=𝛽𝑀ℓ+1/𝜀,(14)canbewritten
as
E𝜋[∥𝜃𝑘−b𝜃𝑘∥1]≤E𝜋[∥𝜃𝑘−1−b𝜃𝑘−1∥1]+4√
𝑑𝜀.
Hence, solving this recursive inequality with 𝜃0=b𝜃0, we have
E𝜋[∥𝜃𝐾−b𝜃𝐾∥1]≤4√
𝑑𝐾𝜀,
which is the desired conclusion. □
Lemma 8. Let𝑎1and𝑎2beR-valued random variables whose cumulative distribution functions are
𝐹1(𝑡)=1−exp
−∫𝑡
0𝑓1(𝑟)𝑑𝑟
,and𝐹2(𝑡)=1−exp
−∫𝑡
0𝑓2(𝑟)𝑑𝑟
,
respectively, where 𝑓1, 𝑓2:R→Rare continuous functions. Let the distributions of 𝑎1and𝑎2be𝑃1and𝑃2
respectively. Supposethat thereexists 𝑀,𝑚 1,𝑚2>0suchthat|𝑓2(𝑡)−𝑓1(𝑡)|≤𝑀,𝑚1≤𝑓1(𝑡), and𝑚2≤𝑓2(𝑡)hold
for∀𝑡∈R. Then, the Wasserstein distance between 𝑃1and𝑃2satisfies
W1(𝑃1,𝑃2)≤𝑀
𝑚1𝑚2.
Proof.Since𝑎1and𝑎2are 1-dimensional, we have
W1(𝑃1,𝑃2)=∫1
0𝐹−1
1(𝑞)−𝐹−1
2(𝑞)𝑑𝑞.
We introduce several notation 𝛿(𝑟)=𝑓2(𝑟)−𝑓1(𝑟),𝑡=𝐹−1
1(𝑞), and𝑡′=𝐹−1
2(𝑞), then
∫𝑡
0𝑓1(𝑟)𝑑𝑟=log1
1−𝑞
∫𝑡′
0(𝑓1(𝑟)+𝛿(𝑟))𝑑𝑟=log1
1−𝑞
holds. So, we obtain
∫𝑡
𝑡′𝑓1(𝑟)𝑑𝑟=∫𝑡′
0𝛿(𝑟)𝑑𝑟.
Hence, we have
∫𝑡
𝑡′𝑓1(𝑟)𝑑𝑟=∫max{𝑡,𝑡′}
min{𝑡,𝑡′}𝑓1(𝑟)𝑑𝑟≤𝑀𝑡′.
In addition,∫max{𝑡,𝑡′}
min{𝑡,𝑡′}𝑓1(𝑟)𝑑𝑟≥𝑚1|𝑡−𝑡′|holds, so we have
|𝑡−𝑡′|≤𝑀𝑡′
𝑚1.
We have the upper bound of 𝑡′as
log1
1−𝑞=∫𝑡′
0𝑓2(𝑟)𝑑𝑟≥𝑚2𝑡′,
16Under review as submission to TMLR
so we have
|𝑡−𝑡′|≤𝑀
𝑚1𝑚2log1
1−𝑞.
Since∫1
0|log(1−𝑞)|𝑑𝑞=1holds, we obtain
∫1
0𝐹−1
1(𝑞)−𝐹−1
2(𝑞)𝑑𝑞≤𝑀
𝑚1𝑚2.
□
D Proof of Theorem 4
We prove this theorem by two steps. First, we prove that BPS has 𝜇(𝛽,𝜀)
zas one of its stationary distributions in
section D.1. At this stage, BPS may have other forms of stationary distribution or may not converge to its stationary
distribution. Second,weprovethatBPShasauniquestationarydistributionandconvergestoitsstationarydistribution
at exponential rate, in other words, it has the exponential ergodicity, in section D.2.
D.1 The form of the stationary distribution
In this section, we check that BPS has 𝜇(𝛽,𝜀)
zas a stationary distribution. In the proof, we define 𝜆(𝜃,𝑣):=
𝛽⟨∇𝐿z(𝜃),𝑣⟩+,¯𝜆(𝜃,𝑣):=𝜆(𝜃,𝑣)+Λref,and𝑅z(𝜃):=𝐼𝑑−2∇𝐿z(𝜃)∇𝐿z(𝜃)⊤
∥∇𝐿z(𝜃)∥2. Weremarkthat 𝑅zisasymmetricmatrix
and satisfies 𝑅z(𝜃)2=𝐼𝑑, so it is also an orthogonal matrix.
From the proof of Lemma 1 in the supplementary material of Deligiannidis et al. (2019), we can write the transition
probability b𝑄of BPS as following for arbitrary measurable sets 𝐴⊂Θand𝐵⊂S𝑑−1:
b𝑄((𝜃,𝑣),𝐴×𝐵)=∫∞
0exp
−∫𝑠
0 ¯𝜆(𝜃+𝑢𝑣,𝑣)+𝐶𝐵𝑑𝑢
× ¯𝜆(𝜃+𝑢𝑣,𝑣)+𝐶𝐵𝐾((𝜃+𝑠𝑣,𝑣),𝐴×𝐵)𝑑𝑠, (15)
where a transition kernel 𝐾is expressed as
𝐾((𝜃,𝑣),𝐴×𝐵)=𝜆(𝜃,𝑣)+𝐶𝐵
¯𝜆(𝜃,𝑣)+𝐶𝐵1[𝜃∈𝐴] 1[𝑅z(𝜃)𝑣∈𝐵] (16)
+Λref
¯𝜆(𝜃,𝑣)+𝐶𝐵1[𝜃∈𝐴]𝜇unif(𝐵),
where𝜇unifis the uniform probability measure on S𝑑−1.
Lemma 9. Under Assumption 1, a probability measure on Θ×S𝑑−1
b𝜇z(𝐴×𝐵)∝∫
𝐴×𝐵 ¯𝜆(𝜃,−𝑣)+𝐶𝐵exp(−𝛽𝐿 z(𝜃))𝑑𝜃𝜇 unif(𝑑𝑣)
is the stationary distribution induced from the transition probability b𝑄as(15).
Proof.Our proof is almost the same as the proof of Lemma 1 in Deligiannidis et al. (2019). Let 𝜋z(𝑑𝜃,𝑑𝑣)=
exp(−𝛽𝐿z(𝜃))𝑑𝜃𝜇 unif(𝑑𝑣).
First, we prove∫
(¯𝜆(𝜃,𝑣)+𝐶𝐵)𝜋z(𝑑𝜃,𝑑𝑣)𝐾((𝜃,𝑣),𝐴×𝐵)∝b𝜇z(𝐴×𝐵). (17)
Substituting (16), the left side of (17) is rewritten as
∫
𝜋z(𝑑𝜃,𝑑𝑣)(𝜆(𝜃,𝑣)+𝐶𝐵) 1[𝜃∈𝐴] 1[𝑅z(𝜃)𝑣∈𝐵]+∫
𝜋z(𝑑𝜃,𝑑𝑣)Λref 1[𝜃∈𝐴]𝜇unif(𝐵).
17Under review as submission to TMLR
Weconsiderchangingthevariableas 𝑣′=𝑅z(𝜃)𝑣. Since𝑅z(𝜃)−1=𝑅z(𝜃)holds,weget 𝜆(𝜃,𝑅z(𝜃)−1𝑣′)=𝜆(𝜃,−𝑣′).
In addition, since|det(𝑅z(𝜃))|=1, and𝜇unif(𝑅z(𝜃)−1𝑑𝑣′)=𝜇unif(𝑑𝑣′)hold due to the rotational invariance of 𝜇unif,
we obtain ∫
𝐴×𝐵𝜋z(𝑑𝜃,𝑑𝑣′)(𝜆(𝜃,−𝑣′)+𝐶𝐵)+∫
𝐴×𝐵𝜋z(𝑑𝜃,𝑑𝑣′)Λref,
which is proportional to the right side of (17) from the definition of b𝜇z.
Second, we prove∫b𝑄((𝜃,𝑣),(𝑑𝑦,𝑑𝑤))b𝜇z(𝑑𝜃,𝑑𝑣)=b𝜇z(𝑑𝑦,𝑑𝑤). We have
∫
b𝑄((𝜃,𝑣),(𝑑𝑦,𝑑𝑤))b𝜇z(𝑑𝜃,𝑑𝑣)
∝∫∞
0exp
−∫𝑠
0{¯𝜆(𝜃+𝑢𝑣,𝑣)+𝐶𝐵}𝑑𝑢
{¯𝜆(𝜃+𝑠𝑣,𝑣)+𝐶𝐵}
×𝐾((𝜃+𝑠𝑣,𝑣),(𝑑𝑦,𝑑𝑤)){¯𝜆(𝜃,−𝑣)+𝐶𝐵}𝜋z(𝑑𝜃,𝑑𝑣)𝑑𝑠.
If we change 𝜃as𝑡=𝜃+𝑠𝑣, then this integral becomes
∫∞
0exp
−∫𝑠
0{¯𝜆(𝑡+(𝑢−𝑠)𝑣,𝑣)+𝐶𝐵}𝑑𝑢
{¯𝜆(𝑡,𝑣)+𝐶𝐵}
×𝐾((𝑡,𝑣),(𝑑𝑦,𝑑𝑤)){¯𝜆(𝑡−𝑠𝑣,−𝑣)+𝐶𝐵}𝜋z(𝑑𝜃,𝑑𝑣)𝑑𝑠.
Since𝐿z(𝜃)is absolutely continuous,
exp(−𝛽𝐿z(𝑡−𝑠𝑣))=exp
−𝛽𝐿z(𝑡)−∫𝑠
0𝜆(𝑡−𝑤𝑣,−𝑣)𝑑𝑤+∫𝑠
0𝜆(𝑡−𝑤𝑣,𝑣)𝑑𝑤
holds in the same way as Deligiannidis et al. (2019). Substituting it into 𝜋z(𝑑𝑥,𝑑𝑣)and changing 𝑢as𝑢−𝑠=−𝑤,
∫∞
0exp
−∫𝑠
0{¯𝜆(𝑡−𝑤𝑣,−𝑣)+𝐶𝐵}𝑑𝑤
{¯𝜆(𝑡−𝑠𝑣,−𝑣)+𝐶𝐵}𝑑𝑠
×{¯𝜆(𝑡,𝑣)+𝐶𝐵}𝐾((𝑡,𝑣),(𝑑𝑦,𝑑𝑤))𝜋z(𝑑𝑡,𝑑𝑣)
holds. The first line can be calculated ash
−exp
−∫𝑠
0{¯𝜆(𝑡−𝑤𝑣,−𝑣)+𝐶𝐵}𝑑𝑤i∞
0=1, so it is equal to
∫
{¯𝜆(𝑡,𝑣)+𝐶𝐵}𝐾((𝑡,𝑣),(𝑑𝑦,𝑑𝑤))𝜋z(𝑑𝑡,𝑑𝑣).
Using (17), it is proportional to b𝜇z(𝑑𝑦,𝑑𝑤), which completes the proof. □
Bythefollowingproposition,weprovethat 𝜇(𝛽,𝜀)
zisoneofthestationarydistributionsofBPS.Recallthatwedefined
𝑎𝑑:=Γ(𝑑/2)/(√𝜋Γ(𝑑/2+1/2)).
Proposition 10. The marginal distribution of the stationary distribution expressed in Lemma 9 is written as
b𝜇z(𝑑𝜃)∝(Λref+𝐶𝐵+𝑎𝑑𝛽∥∇𝐿z(𝜃)∥)exp(−𝛽𝐿 z(𝜃))𝑑𝜃.
Hence, if we put Λrefand𝐶𝐵asΛref+𝐶𝐵=𝛽𝑀ℓ+1/𝜀, it corresponds to 𝜇(𝛽,𝜀)
z.
Proof.We only need to integrate with 𝑣the distribution b𝜇zexpressed in Lemma 9. We have
b𝜇z(𝑑𝜃)∝∫
𝑣∈S𝑑−1(Λref+𝐶𝐵+𝛽⟨∇𝐿z(𝜃),−𝑣⟩+)exp(−𝛽𝐿z(𝜃))𝑑𝜃𝜇 unif(𝑑𝑣)
=(Λref+𝐶𝐵)exp(−𝛽𝐿z(𝜃))𝑑𝜃+exp(−𝛽𝐿z(𝜃))𝑑𝜃𝛽E𝑣∼𝜇unif[⟨∇𝐿z(𝜃),𝑣⟩+].
18Under review as submission to TMLR
We can calculate the expected value in the last term as
E𝑣∼𝜇unif[⟨∇𝐿z(𝜃),𝑣⟩+]=E𝑣∼𝜇unif[∥∇𝐿z(𝜃)∥(cos𝜙)+]=∥∇𝐿z(𝜃)∥E𝑣∼𝜇unif[(cos𝜙)+],
where𝜙∈Ris a random variable dependent on 𝑣which satisfies
cos𝜙=∇𝐿z(𝜃)
∥∇𝐿z(𝜃)∥,𝑣
. (18)
From the symmetry of the uniform distribution, we can calculate E𝑣∼𝜇unif[(cos𝜙)+]by replacing∇𝐿z(𝜃)
∥∇𝐿z(𝜃)∥in (18) by
(1,0,···,0). Hence,
E𝑣∼𝜇unif[(cos𝜙)+]=E𝑣∼𝜇unif[(𝑣1)+]=E©­­
«𝑥1√︃
𝑥2
1+···+𝑥2
𝑑ª®®
¬+
holds, where 𝑣1is the first component of 𝑣and𝑥𝑖(𝑖=1,...,𝑑)isi.i.d.standard Gaussian variables.
For(𝑥1,...,𝑥𝑑)∼N( 0,𝐼𝑑), we have
Evt
𝑥2
1
𝑥2
1+···+𝑥2
𝑑=∫
R𝑑vt
𝑧2
1
𝑧2
1+···+𝑧2
𝑑1
(2𝜋)𝑑/2exp 
−𝑧2
1+···+𝑧2
𝑑
2!
𝑑𝑧1···𝑑𝑧𝑑
=∫
[0,∞)2√︂𝑟
𝑟+𝑠𝑟−1/2exp(−𝑟/2)√
2𝜋𝑠(𝑑−1)/2−1exp(−𝑠/2)
Γ((𝑑−1)/2)2(𝑑−1)/2𝑑𝑟𝑑𝑠
=∫
[0,1]𝑡1/2𝑡1/2−1(1−𝑡)(𝑑−1)/2−1
B(1/2,(𝑑−1)/2)𝑑𝑡
=B(1,(𝑑−1)/2)
B(1/2,(𝑑−1)/2)
=Γ(1)Γ((𝑑−1)/2)Γ(𝑑/2)
Γ(1/2)Γ((𝑑−1)/2)Γ(𝑑/2+1/2)
=Γ(𝑑/2)√𝜋Γ(𝑑/2+1/2).
Note that for all 𝑑≥2,
1√︁
𝑑/2≤Γ(𝑑/2)
Γ(𝑑/2+1/2)≤1√︁
𝑑/2−1/2
holds (e.g., see Qi & Luo (2013)). Therefore, for all 𝑑≥2, we have
E©­­
«𝑥1√︃
𝑥2
1+···+𝑥2
𝑑ª®®
¬+=Γ(𝑑/2)
2√𝜋Γ(𝑑/2+1/2)∈"
1√
2𝜋𝑑,1√︁
2𝜋(𝑑−1)#
.
□
D.2 The exponential ergodicity of BPS
The next proposition is on the minorization condition of the 2-skeletons of BPS on the restricted domains. In short,
minorizationmeansthatthestochasticprocesscangofromanymeasurablesettoanymeasurablesetintheparameter
space,whichisasufficientconditionfortheexponentialergodicityinthecompactparameterspace. 2-Skeletonmeans
2 step of the stochastic process. This proposition completes the proof of Theorem 4.
19Under review as submission to TMLR
Proposition 11. Under Assumption 1, the 2-skeletons of BPS satisfies the minorization condition; that is, for some
𝑐>0, for all(𝜃,𝑣)∈Θ×S𝑑−1and all measurable 𝐸⊂Θ×S𝑑−1, we have
b𝑄2((𝜃,𝑣),𝐸)≥𝑐∫
Θ∫
S𝑑−11[(𝜃,𝑣)∈𝐸]𝑑𝜃𝜇 unif(𝑑𝑣).
Moreover, BPS is exponentially ergodic in total variation distance.
Proof.We partially follow the proof of Lemma 4 in Deligiannidis et al. (2019).
Let𝑓:Θ×S𝑑−1→ [ 0,∞)be a non-negative and bounded function. We also use the notation 𝑀′=
sup(𝜃,𝑣)∈Θ×S𝑑−1(¯𝜆(𝜃,𝑣)+𝐶𝐵)<∞. By considering the event where the first update of 𝑣isrefreshment from
Unif(S𝑑−1), we see that for any (𝜃0,𝑣0)∈Θ×S𝑑−1,
∫
Θ×S𝑑−1𝑓(𝜃,𝑣)b𝑄2((𝜃0,𝑣0),(𝑑𝜃,𝑑𝑣))
=∫
Θ×S𝑑−1∫
Θ×S𝑑−1𝑓(𝜃,𝑣)b𝑄((𝜃1,𝑣1),(𝑑𝜃,𝑑𝑣))b𝑄((𝜃0,𝑣0),(𝑑𝜃1,𝑑𝑣 1))
≥Λref
𝑀′inf
𝜃1∈Θ∫
Θ×S𝑑−1𝑓(𝜃,𝑣)b𝑄((𝜃1,𝑣1),(𝑑𝜃,𝑑𝑣))𝜇unif(𝑑𝑣1)
holds. We also obtain that for 𝑇∼Exp(𝑀′),𝑉1,𝑉2∼i.i.d.Unif(S𝑑−1), we have
inf
𝜃1∈Θ∫
Θ×S𝑑−1𝑓(𝜃,𝑣)b𝑄((𝜃1,𝑣1),𝑑𝜃𝑑𝑣)𝜇unif(𝑑𝑣1)
≥inf
𝜃1∈ΘΛref
𝑀′E[ 1[𝜃1+𝑇𝑉1∈Θ]𝑓(𝜃1+𝑇𝑉1,𝑉2)]
≥inf
𝜃1∈ΘΛ2
ref
𝑀′∫
[0,∞)×S𝑑−11[𝜃1+𝑡𝑣1∈Θ]𝑒−𝑀′𝑡𝑓(𝜃1+𝑡𝑣1,𝑣)𝑑𝑡𝜇 unif(𝑑𝑣1)𝜇unif(𝑑𝑣)
≥inf
𝜃1∈ΘΛ2
ref𝑒−𝑀′diam(Θ)
𝑀′∫
[0,∞)×S𝑑−11[𝜃1+𝑡𝑣1∈Θ]𝑓(𝜃1+𝑡𝑣1,𝑣)𝑑𝑡𝜇 unif(𝑑𝑣1)𝜇unif(𝑑𝑣)
=inf
𝜃1∈ΘΛ2
ref𝑒−𝑀′diam(Θ)
𝑀′∫
Θ×S𝑑−11[𝜃∈Θ]𝑓(𝜃,𝑣)∥𝜃−𝜃1∥1−𝑑𝑑𝜃𝜇 unif(𝑑𝑣)
≥Λ2
ref𝑒−𝑀′diam(Θ)
𝑀′diam(Θ)𝑑−1∫
Θ×S𝑑−1𝑓(𝜃,𝑣)𝑑𝜃𝜇 unif(𝑑𝑣),
where the second last equality uses a change of coordinates. Since 𝑓is generic, the minorization condition holds.
Harris’s theorem thus gives the exponential ergodicity of BPS. □
E Proof of Theorem 5
Proof.We prove in the same way as the proof of Theorem 2.1 in Raginsky et al. (2017). Let 𝜃𝜇be a random variable
satisfying𝜃𝜇∼𝜇(𝛽,𝜀)
z,where𝜇(𝛽,𝜀)
zisdefinedin(7). Wedenote 𝜃𝐾∼𝜇z,𝐾astheoutputofPoissonSGD(Algorithm
1). We have
Ez[E𝜃𝐾[𝐿(𝜃𝐾)]]− inf
𝜃∈Θ𝐿(𝜃)
=Ez[E𝜃𝐾[𝐿(𝜃𝐾)]−E𝜃𝜇[𝐿(𝜃𝜇)]]+{ Ez[E𝜃𝜇[𝐿(𝜃𝜇)]]− inf
𝜃∈Θ𝐿(𝜃)},
and the second term of right-hand side is written as
Ez[E𝜃𝜇[𝐿(𝜃𝜇)]]− inf
𝜃∈Θ𝐿(𝜃)
20Under review as submission to TMLR
=Ez[E𝜃𝜇[𝐿(𝜃𝜇)]]−Ez[E𝜃𝜇[𝐿z(𝜃𝜇)]]+
Ez[E𝜃𝜇[𝐿z(𝜃𝜇)]]− inf
𝜃∈Θ𝐿(𝜃)
.
Letting𝜃◦=argmin𝜃∈Θ𝐿(𝜃), the second part of the right-hand side in the equation above is
Ez[E𝜃𝜇[𝐿z(𝜃𝜇)]]− inf
𝜃∈Θ𝐿(𝜃)=Ez[E𝜃𝜇[𝐿z(𝜃𝜇)]− inf
𝜃∈Θ𝐿z(𝜃)]+
Ez
inf
𝜃∈Θ𝐿z(𝜃)−𝐿z(𝜃◦)
≤Ez[E𝜃𝜇[𝐿z(𝜃𝜇)]− inf
𝜃∈Θ𝐿z(𝜃)].
As a result, we have
Ez[E𝜃𝐾[𝐿(𝜃𝐾)]]− inf
𝜃∈Θ𝐿(𝜃)≤Ez[E𝜃𝐾[𝐿(𝜃𝐾)]−E𝜃𝜇[𝐿(𝜃𝜇)]] (19)
+Ez[E𝜃𝜇[𝐿(𝜃𝜇)]−E𝜃𝜇[𝐿z(𝜃𝜇)]] (20)
+Ez[E𝜃𝜇[𝐿z(𝜃𝜇)]− inf
𝜃∈Θ𝐿z(𝜃)]. (21)
Toevaluatetheterms(19),(20),and(21),wepreparethefollowinglemmatocalculatetheupperboundofthedifference
between two expected value by the Wasserstein distance.
Lemma12. Considerprobabilitymeasures 𝜇and𝜈onΘ. Supposethat sup𝑧∈Z|ℓ(𝑧; 0)|≤𝐴andsup𝑧∈Z∥∇ℓ(𝑧; 0)∥≤
𝐵hold. Then, we obtain
E𝜃1∼𝜇[ℓ(𝑧;𝜃1)]−E𝜃2∼𝜈[ℓ(𝑧;𝜃2)]≤(𝑐1𝑊+𝐵)√︁
𝑊W1(𝜇,𝜈),and (22)
E𝜃1∼𝜇[𝐿(𝜃1)]−E𝜃2∼𝜈[𝐿(𝜃2)]≤(𝑐1𝑊+𝐵)√︁
𝑊W1(𝜇,𝜈). (23)
Proof.Under the assumption, Lemma 3.1 in Raginsky et al. (2017) holds. Hence, we have
∥∇ℓ(𝑧;𝜃)∥≤𝑐1∥𝜃∥+𝐵,∀𝜃∈Θ,∀𝑧∈Z (24)
ℓ(𝑧;𝜃)≤𝑐1
2∥𝜃∥2+𝐵∥𝜃∥+𝐴,∀𝜃∈Θ,∀𝑧∈Z. (25)
Moreover, from Lemma 3.5 in Raginsky et al. (2017), for arbitrary two probability measures 𝜇and𝜈, if we let
𝜎2=max{E𝜃1∼𝜇[∥𝜃1∥2],E𝜃2∼𝜈[∥𝜃2∥2]},
then we haveE𝜃1∼𝜇[ℓ(𝑧;𝜃1)]−E𝜃2∼𝜈[ℓ(𝑧;𝜃2)]≤(𝑐1𝜎+𝐵)W 2(𝜇,𝜈).
Obviously, it also holds that
E𝜃1∼𝜇[𝐿(𝜃1)]−E𝜃2∼𝜈[𝐿(𝜃2)]≤(𝑐1𝜎+𝐵)W 2(𝜇,𝜈).
Since we have 𝜎≤𝑊andW2(𝜇,𝜈)=inf𝜋∈Π(𝜇,𝜈)(∫
Θ∥𝑧−𝑧′∥2𝑑𝜋(𝑧,𝑧′))1/2≤inf𝜋∈Π(𝜇,𝜈)(∫
Θ𝑊∥𝑧−
𝑧′∥1𝑑𝜋(𝑧,𝑧′))1/2=√︁
𝑊W1(𝜇,𝜈), we obtain the statement. □
We start evaluating each of the terms (19), (20), and (21).
First, we study (19). From (23) in Lemma 12, we have
E𝜃𝐾[𝐿(𝜃𝐾)]−E𝜃𝜇[𝐿(𝜃𝜇)]≤(𝑐1𝑊+𝐵)√︃
𝑊W1(𝜇z,𝐾,𝜇(𝛽,𝜀)
z)
≤(𝑐1𝑊+𝐵)√︁
𝑊𝑑𝐾(𝛽,𝜀,𝑑). (26)
Second, we evaluate (20) using the same approach as Raginsky et al. (2017). Here, we need to evaluate
E𝜃𝜇[ℓ(𝑧;𝜃𝜇)]−E𝜃𝜇′[ℓ(𝑧;𝜃𝜇′)],
21Under review as submission to TMLR
where𝑧∈Zis an arbitrary sampled data, 𝜃𝜇′∼𝜇(𝛽,𝜀)
z′and𝜇(𝛽,𝜀)
z′is the stationary distribution of BPS when one
of the data𝑧𝑖is changed to arbitrary ¯𝑧𝑖∈Zandz′is a dataset with replacing 𝑧𝑖to¯𝑧𝑖, and𝐿z′be its corresponding
empirical risk. From (22) in Lemma 12, we have
E𝜃𝜇[ℓ(𝑧;𝜃𝜇)]−E𝜃𝜇′[ℓ(𝑧;𝜃𝜇′)]≤(𝑐1𝑊+𝐵)W 2(𝜇(𝛽,𝜀)
z,𝜇(𝛽,𝜀)
z′)
≤(𝑐1𝑊+𝐵)𝐶𝜇′√︃
𝐷(𝜇(𝛽,𝜀)
z||𝜇(𝛽,𝜀)
z′)+ 
𝐷(𝜇(𝛽,𝜀)
z||𝜇(𝛽,𝜀)
z′)
2!1
4,
where𝐷(·||·)is KL-divergence and
𝐶𝜇′:=2 inf
𝜆>01
𝜆3
2+log∫
Θ𝑒𝜆∥𝜃∥2𝜇(𝛽,𝜀)
z′(𝑑𝜃) 1
2
,
which is from Corollary 2.3 in Bolley & Villani (2005) (explicit form is Theorem 14 in Section H). Also, since we
have∥𝜃∥≤𝑊,𝐶𝜇′≤2𝑊holds. We denote the density functions of 𝜇(𝛽,𝜀)
z,𝜇(𝛽,𝜀)
z′as𝑝z,𝑝z′, and the normalization
constants asΛz,Λz′respectively. Let us calculate 𝐷(𝜇(𝛽,𝜀)
z||𝜇(𝛽,𝜀)
z′). We have
𝑝z(𝜃)
𝑝z′(𝜃)=Λz′
Λz·𝛽𝑀ℓ+1/𝜀+𝑎𝑑𝛽∥∇𝐿z(𝜃)∥
𝛽𝑀ℓ+1/𝜀+𝑎𝑑𝛽∥∇𝐿z′(𝜃)∥exp(−𝛽(𝐿z(𝜃)−𝐿z′(𝜃))), (27)
so inorder toobtain theupper boundof 𝐷(𝜇(𝛽,𝜀)
z||𝜇(𝛽,𝜀)
z′), wesuppress eachof thethree termsof theright-hand side
of (27). First, we suppress the second term.
∥∇𝐿z(𝜃)∥=∇𝐿z′(𝜃)+1
𝑛(∇ℓ(𝑧𝑖;𝜃)−∇ℓ(¯𝑧𝑖;𝜃)
≤∥∇𝐿z′(𝜃)∥+1
𝑛∥∇ℓ(𝑧𝑖;𝜃)−∇ℓ(¯𝑧𝑖;𝜃)∥
≤∥∇𝐿z′(𝜃)∥+2
𝑛(𝑐1∥𝜃∥+𝐵),
where the last inequality is from (24). Hence,
𝛽𝑀ℓ+1/𝜀+𝑎𝑑𝛽∥∇𝐿z(𝜃)∥
𝛽𝑀ℓ+1/𝜀+𝑎𝑑𝛽∥∇𝐿z′(𝜃)∥≤𝛽𝑀ℓ+1/𝜀+𝑎𝑑𝛽
∥∇𝐿z′(𝜃)∥+2
𝑛(𝑐1∥𝜃∥+𝐵)
𝛽𝑀ℓ+1/𝜀+𝑎𝑑𝛽∥∇𝐿z′(𝜃)∥
≤1+2𝑎𝑑𝛽(𝑐1𝑊+𝐵)
𝑛(𝛽𝑀ℓ+1/𝜀)
≤1+2𝑎𝑑(𝑐1𝑊+𝐵)
𝑛𝑀ℓ(28)
holds. Second, we suppress the third term. We have
exp(−𝛽(𝐿z(𝜃)−𝐿z′(𝜃)))=exp
−𝛽1
𝑛(ℓ(𝑧𝑖;𝜃)−ℓ(¯𝑧𝑖;𝜃))
≤exp𝛽
𝑛𝑐1∥𝜃∥2
2+𝐵∥𝜃∥+𝐴
≤exp𝛽
𝑛𝑐1𝑊2
2+𝐵𝑊+𝐴
, (29)
where we use (25). Finally, we suppress the first term. Using (28) and (29), we have
Λz′
Λz=∫
𝜃∈Θ(𝛽𝑀ℓ+1/𝜀+𝑎𝑑𝛽∥∇𝐿z′(𝜃)∥)exp(−𝛽𝐿z′(𝜃))𝑑𝜃
∫
𝜃∈Θ(𝛽𝑀ℓ+1/𝜀+𝑎𝑑𝛽∥∇𝐿𝑧(𝜃)∥)exp(−𝛽𝐿𝑧(𝜃))𝑑𝜃
22Under review as submission to TMLR
≤
1+2𝑎𝑑(𝑐1𝑊+𝐵)
𝑛𝑀ℓ
exp𝛽
𝑛𝑐1𝑊2
2+𝐵𝑊+𝐴
. (30)
Combining (28), (29) and (30), we have
log𝑝z(𝜃)
𝑝z′(𝜃)≤2 log
1+2𝑎𝑑(𝑐1𝑊+𝐵)
𝑛𝑀ℓ
+2𝛽
𝑛𝑐1𝑊2
2+𝐵𝑊+𝐴
≤1
𝑛4𝑎𝑑(𝑐1𝑊+𝐵)
𝑀ℓ+𝛽(𝑐1𝑊2+2𝐵𝑊+2𝐴)
,
so
𝐷(𝜇(𝛽,𝜀)
z||𝜇(𝛽,𝜀)
z′)≤1
𝑛4𝑎𝑑(𝑐1𝑊+𝐵)
𝑀ℓ+𝛽(𝑐1𝑊2+2𝐵𝑊+2𝐴)
holds. We set 𝐶𝑑=4𝑎𝑑(𝑐1𝑊+𝐵)/𝑀ℓand𝐶=𝑐1𝑊2+2𝐵𝑊+2𝐴, then we have
(20)≤2𝑊(𝑐1𝑊+𝐵) 𝐶𝑑+𝛽𝐶
𝑛1
2
+𝐶𝑑+𝛽𝐶
𝑛1
4!
. (31)
Finally, we evaluate (21). Let us denote
Λz(𝜃)=Λ
𝛽𝑀ℓ+1/𝜀+𝑎𝑑𝛽∥∇𝐿z(𝜃)∥
Λ=∫
𝜃∈Θ(𝛽𝑀ℓ+1/𝜀+𝑎𝑑𝛽∥∇𝐿z(𝜃)∥)𝑒−𝛽𝐿z(𝜃)𝑑𝜃.
Since the distribution of 𝜃𝜇is
𝜇(𝛽,𝜀)
z(𝑑𝜃)∝
𝛽𝑀ℓ+1
𝜀+𝑎𝑑𝛽∥∇𝐿z(𝜃)∥
exp(−𝛽𝐿z(𝜃)),
we have
E𝜃𝜇[𝐿z(𝜃𝜇)]=−1
𝛽
E𝜃𝜇
log𝑒−𝛽𝐿z(𝜃𝜇)
Λz(𝜃𝜇)
+E𝜃𝜇[logΛz(𝜃𝜇)]
=1
𝛽
E𝜃𝜇[−log𝑝z(𝜃𝜇)]−E𝜃𝜇[logΛz(𝜃𝜇)]
.
Since we have E𝜃𝜇[∥𝜃𝜇∥2]≤𝑊2, we can calculate the upper bound of E𝜃𝜇[−log𝑝z(𝜃𝜇)]by the differential entropy
of Gaussian distribution in the same way as the discussion of Section 3.5 in Raginsky et al. (2017):
E𝜃𝜇[−log𝑝z(𝜃𝜇)]≤𝑑
2log2𝜋𝑒
𝑑𝑊2
.
Using (24), we have
logΛz(𝜃)≥logΛ
𝛽𝑀ℓ+1/𝜀+𝑎𝑑𝛽(𝑐1𝑊+𝐵)).
In addition,
logΛ=log∫
𝜃∈Θ(𝛽𝑀ℓ+1/𝜀+𝑎𝑑𝛽∥∇𝐿z(𝜃)∥)𝑒−𝛽𝐿z(𝜃)𝑑𝜃
≥log∫
𝜃∈Θ(𝛽𝑀ℓ+1/𝜀)𝑒−𝛽𝐿z(𝜃)𝑑𝜃
=log(𝛽𝑀ℓ+1/𝜀)+log∫
𝜃∈Θ𝑒−𝛽𝐿z(𝜃)𝑑𝜃
23Under review as submission to TMLR
≥log(𝛽𝑀ℓ+1/𝜀)−𝛽𝐿∗
z+𝑑
2log2𝜋
𝑐1𝛽
holds,wherethelastinequalityisfromtheequation(3.21)inRaginskyetal.(2017). Here,wedenote 𝐿∗
z=inf𝜃∈Θ𝐿z(𝜃).
Hence, we have
(21)≤1
𝛽𝑑
2log2𝜋𝑒
𝑑𝑊2
+log𝛽𝑀ℓ+1/𝜀+𝑎𝑑𝛽(𝑐1𝑊+𝐵)
𝛽𝑀ℓ+1/𝜀+𝛽𝐿∗
z−𝑑
2log2𝜋
𝑐1𝛽
−𝐿∗
z
≤1
𝛽𝑑
2log𝑒𝑊2𝑐1𝛽
𝑑+log
1+𝑎𝑑(𝑐1𝑊+𝐵)
𝑀ℓ
. (32)
We combine the result (26), (31), and (32), then obtain the statement. □
F Proof of Proposition 6
Proof.Let𝜃𝜇and𝜃𝜈be the random variable which obey the distributions 𝜇(𝛽,𝜀)
zand𝜈(𝛽)
zrespectively.
In the same way as Theorem 5, we have
E𝑧[E𝜃𝐾[𝐿(𝜃𝐾)]]− inf
𝜃∈Θ𝐿(𝜃)≤E𝑧[E𝜃𝐾[𝐿(𝜃𝐾)]−E𝜃𝜇[𝐿(𝜃𝜇)]] (33)
+E𝑧[E𝜃𝜇[𝐿(𝜃𝜇)]−E𝜃𝜈[𝐿(𝜃𝜈)]] (34)
+E𝑧[E𝜃𝜈[𝐿(𝜃𝜈)]−E𝜃𝜈[𝐿z(𝜃𝜈)]] (35)
+E𝑧[E𝜃𝜈[𝐿z(𝜃𝜈)]− inf
𝜃∈Θ𝐿z(𝜃)]. (36)
(33) can be evaluated in the same as Theorem 5.
First, we evaluate (34). We have
E𝜃𝜇[𝐿(𝜃𝜇)]−E𝜃𝜈[𝐿(𝜃𝜈)]≤𝑊W2(𝜇(𝛽,𝜀)
z,𝜈(𝛽)
z)
from the same discussion in the proof of Theorem 5. Since both 𝜃𝜇and𝜃𝜈satisfy the log-Sobolev inequality, we can
use Otto-Villani theorem (Bakry et al., 2014) (explicit form is Theorem 15 in Section H), and
W2(𝜇(𝛽,𝜀)
z,𝜈(𝛽)
z)≤√︃
2𝑐(𝛽)
LS𝐷(𝜇(𝛽,𝜀)
z||𝜈(𝛽)
z)
holds, where 𝐷denotes the KL-divergence and 𝑐(𝛽)
LSis the log-Sobolev constant of 𝜈(𝛽)
z. We have
𝐷(𝜇(𝛽,𝜀)
z||𝜈(𝛽)
z)=E𝜃∼𝜇
log(𝛽𝑀ℓ+1/𝜀+𝑎𝑑𝛽∥∇𝐿z(𝜃)∥)exp(−𝛽𝐿z(𝜃))/Λ𝜇
exp(−𝛽𝐿z(𝜃))/Λ𝜈
≤E𝜃∼𝜇
log(𝛽𝑀ℓ+1/𝜀+𝑎𝑑𝛽𝑀ℓ)Λ𝜈
Λ𝜇
,
whereΛ𝜇andΛ𝜈are normalizing constants of the density functions of 𝜇(𝛽,𝜀)
zand𝜈(𝛽)
zrespectively. We have
Λ𝜈
Λ𝜇=∫
Θexp(−𝛽𝐿z(𝜃))𝑑𝜃
∫
Θ(𝛽𝑀ℓ+1/𝜀+𝑎𝑑𝛽∥∇𝐿z(𝜃)∥)exp(−𝛽𝐿z(𝜃))𝑑𝜃≤1
𝛽𝑀ℓ+1/𝜀,
hence we have
𝐷(𝜇(𝛽,𝜀)
z||𝜈(𝛽)
z)≤log(1+𝑎𝑑𝛽𝜀𝑀ℓ).
As a result, we obtain
E𝜃𝜇[𝐿(𝜃𝜇)]−E𝜃𝜈[𝐿(𝜃𝜈)]≤𝑊√︃
2𝑐(𝛽)
LSlog(1+𝑎𝑑𝛽𝜀𝑀ℓ). (37)
24Under review as submission to TMLR
Second, we evaluate (35). Let 𝜈(𝛽)
z′be the Gibbs distribution when one of the data 𝑧𝑖is replaced by 𝑧′
𝑖. In the same
way as Section 3.6 in Raginsky et al. (2017), we have
W2(𝜈(𝛽)
z,𝜈(𝛽)
z′)≤2𝑐(𝛽)
LS𝛽𝑀ℓ
𝑛.
Hence, we have
E𝜃𝜈[𝐿(𝜃𝜈)]−E𝜃𝜈[𝐿z(𝜃𝜈)]≤(𝑐1𝑊+𝐵)2𝑐(𝛽)
LS𝛽𝑀ℓ
𝑛. (38)
Finally,weevaluate(36). ThistermcanbeevaluatedonthesamewayasProposition3.4inRaginskyetal.(2017)and
we have
E𝜃𝜈[𝐿z(𝜃𝜈)]− inf
𝜃∈Θ𝐿z(𝜃)≤1
𝛽𝑑
2log2𝜋𝑒𝑊2
𝑑
−𝑑
2log2𝜋
𝑐1𝛽
=𝑑
2𝛽log𝑒𝑊2𝑐1𝛽
𝑑
. (39)
We combine the result (37), (38), and (39), then obtain the statement. □
G Proof of Theorem 2
Proof.Let𝜃𝐾,𝜃𝜇betherandomvariableswhosedistributionis 𝜇(𝛽,𝜀)
z,𝐾and𝜇(𝛽,𝜀)
zrespectively. Let 𝐿∗
z=min𝜃∈Θ𝐿z(𝜃).
We have
E𝜃𝐾[𝐿z(𝜃𝐾)]−𝐿∗
z=(E𝜃𝐾[𝐿z(𝜃𝐾)]−E𝜃𝜇[𝐿z(𝜃𝜇)])+( E𝜃𝜇[𝐿z(𝜃𝜇)]−𝐿∗
z).
As the first term of the right-hand side, we can use the Wasserstein distance in the same way as the proof of Theorem
5 as in (26). Hence, we have
E𝜃𝐾[𝐿z(𝜃𝐾)]−E𝜃𝜇[𝐿z(𝜃𝜇)]≤(𝑐1𝑊+𝐵)√︁
𝑊𝑑𝐾(𝛽,𝜀,𝑑).
Further, using (32) in the Proof of Theorem 5,
E𝜃𝜇[𝐿z(𝜃𝜇)]≤1
𝛽𝑑
2log𝑒𝑊2𝑐1𝛽
𝑑+log
1+𝑎𝑑(𝑐1𝑊+𝐵)
𝑀ℓ
+𝐿∗
z
holds, which completes the proof. □
H Explicit citation of the existing theorems
Theorem13 (Theorem4,Gibbs&Su(2002)) .Onthecompactset Ω,theWassersteinmetric 𝑑𝑊andthetotalvariation
distance𝑑𝑇𝑉satisfy the following relation:
𝑑𝑊≤diam(Ω)·𝑑𝑇𝑉,
where diam(Ω)=sup{𝑑(𝑥,𝑦)|𝑥,𝑦∈Ω}.
Theorem 14 (Corollary 2.3, Bolley & Villani (2005)) .Let𝑋be a measurable space equipped with a measurable
distance𝑑, let𝑝≥1and let𝜈be a probability measure on 𝑋. Assume that there exist 𝑥0∈𝑋and𝛼 > 0such that∫
𝑒𝛼𝑑(𝑥0,𝑥)𝑝𝑑𝜈(𝑥)is finite. Then,∀𝜇∈𝑃(𝑋),
𝑊𝑝(𝜇,𝜈)≤𝐶"
𝐻(𝜇|𝜈)1
𝑝+𝐻(𝜇|𝜈)
21
2𝑝#
,
where
𝐶=2 inf
𝑥0∈𝑋,𝛼> 01
𝛼3
2+log∫
𝑒𝛼𝑑(𝑥0,𝑥)𝑝𝑑𝜈(𝑥) 1
𝑝
<∞.
25Under review as submission to TMLR
Theorem 15 (Theorem 9.6.1, Bakry et al. (2014)) .Let𝜇be a probability measure on 𝑀. If𝜇satisfies a logarithmic
Sobolev inequality 𝐿𝑆(𝐶)for some constant 𝐶 > 0, then it satisfies following for every probability measure 𝜈on𝑀:
W2(𝜇,𝜈)2≤2𝐶·𝐷(𝜈||𝜇),
whereW2denotes the Wasserstein-2 distance and 𝐷denotes the Kullback-Leibler divergence.
26