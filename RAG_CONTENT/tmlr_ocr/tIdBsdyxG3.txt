Under review as submission to TMLR
The Alignment Problem in Curriculum Learning
Anonymous authors
Paper under double-blind review
Abstract
In curriculum learning, teaching involves cooperative selection of sequences of data via plans
to facilitate eﬃcient and eﬀective learning. One-oﬀ cooperative selection of data has been
mathematically formalized as entropy-regularized optimal transport and the limiting be-
havior of myopic sequential interactions has been analyzed, both yielding theoretical and
practical guarantees. We recast sequential cooperation with curriculum planning in a rein-
forcement learning framework and analyze performance mathematically and by simulation.
We prove that inﬁnite length plans are equivalent to not planning under certain assumptions
on the method of planning, and isolate instances where monotonicity and hence convergence
in the limit hold, as well as cases where it does not. We also demonstrate through simu-
lations that argmax data selection is the same across planning horizons and demonstrate
problem-dependent sensitivity of learning to the teacher’s planning horizon. Thus, we ﬁnd
that planning ahead yields eﬃciency at the cost of eﬀectiveness. This failure of alignment
is illustrated in particular with grid world examples in which the teacher must attempt to
steer the learner away from a particular location in order to reach the desired grid square.
We conclude with implications and directions for eﬃcient and eﬀective curricula.
1 Introduction
Advances in AI and machine learning enable the possibility that artiﬁcial systems may autonomously facili-
tate human goals, including human learning. Design of such systems requires addressing a value alignment
problem (Russell, 2019; Christian, 2020), which requires interacting with the system to achieve the desired
goals. Toward this end, formalizing models of cooperation among agents that bridge human and machine
learning is an important direction for research. In this paper, we identify a novel value alignment problem
in the context of agents that facilitate learning, and we identify and test suﬃcient conditions for ensuring
value alignment for curriculum learning.
Learning may be facilitated by the teacher planning ahead, which becomes a problem of reinforcement
learning. Thereexistsanextensiveliteratureoncurriculumlearning(Elman,1993;Khanetal.,2011;Pentina
et al., 2015; Matiisen et al., 2020; Graves et al., 2017); however, this literature focuses on naive learners rather
than those that reason cooperatively about the teacher’s selection of data. Theoretical results are limited
(Milli and Dragan, 2020) and have not systematically considered the possibility of alignment problems or
their solutions. Recent advances in theoretical foundations for cooperative inference admit a more uniﬁed
formal treatment (Wang et al., 2020b), which is necessary to understand whether, when, and why alignment
problems arise.
We formalize the alignment problem in curriculum learning via the mathematical condition of consistency.
Given a teacher and learner cooperatively communicating, the teacher aims to convey a distribution θon the
ﬁnite set of possible hypotheses Hto the learner, over an inﬁnite horizon. That is, if θndenotes the learner’s
distribution at the n-th round of communication, the alignment problem is to have limn→∞/summationtext
h∈H|θ(h)−
θn(h)|= 0. When the teacher is conveying a speciﬁc hypothesis h/primeto the learner, the distribution to be
learned isθ=δh/prime, a Dirac distribution.
We investigate the alignment problem in curriculum learning by recasting sequential cooperative Bayesian
inference(SCBI)asaMarkovdecisionprocess(MDP).Indoingso, weretainthetheoreticalstrengthsofprior
1Under review as submission to TMLR
formalizations which yielded proofs of consistency and rates of convergence, while considering the beneﬁts
and drawbacks of planning by the teacher. Section 2 gives background on one-oﬀ cooperative inference and
sequential cooperative inference, as well as the interpretation of SCBI as a Markov chain. Section 3 recasts
SCBI as a Markov decision process, distinct from the trivial realization of a Markov chain as an MDP,
contrasts SCBI with no planning ahead vs. using a teaching plan which calculates several steps into the
future, and isolates the theoretical basis of misalignment. One main result of this section is that for a class
of reward/cost functions, planning inﬁnitely far ahead is equivalent to not planning at all; this includes using
a reward proportional to the probability of selecting the correct hypothesis. The other main result is to give
a condition for monotonicity in expectation which yields a suﬃcient requirement for alignment. Section 4
gives examples and simulations. Section 5 gives related work, and Section 6 concludes.
Notation. M∈R|D|×|H|
>0is the joint distribution for the teacher and learner between data and hypotheses,
with|D|many rows and|H|many columns, with M(d,h)the entry of the joint distribution corresponding to
datumdand hypothesis h.Mθ,λis the joint distribution Mnormalized using Sinkhorn scaling to have row
sums equal to θand column sums equal to λ. That is, for every h∈H,/summationtext
d∈DMθ,λ
(d,h)=θ(h)and for every
d∈D,/summationtext
h∈HMθ,λ
(d,h)=λ(d).π:P(H)→P(D)is a teaching strategy used by the teacher for a single round
of teaching, while πN
R:P(H)→P (D)is the teaching strategy obtained from πby planning Nteaching
rounds into the future and using the random variable R, representing rewards/costs inherent to the problem.
δMθ,λ
(d,−)/λ(d)is the atomic distribution on P(H)with atom Mθ,λ
(d,−)/λ(d); i.e.δMθ,λ
(d,−)/λ(d)∈P(P(H)), the
space of distributions on the distributions on hypotheses, where the Markov operator Ψπin our formalism
is acting. ΨN
πdenotes Ψπ:P(P(H))→P(P(H))composed with itself Ntimes. Frequently we will shorten
the notation δMθ,λ
(d,−)/λ(d)toδ(d).
2 Background
Curriculum learning involves selecting a sequence of learning problems that lead the learner to a desired
knowledge or capability. We formalize these as a sequence of data that lead the learner to a target hypothesis.
Throughout, we will assume teachers and learners are probabilistic agents reasoning over discrete and ﬁnite
spaces of hypotheses h∈Hand datad∈D. Recall, in standard probabilistic inference, learners will update
their posterior beliefs P(h|d)in proportion to the product of the prior beliefs, P(h)and the likelihood of the
data,P(d|h), as dictated by Bayes rule.
One-oﬀ cooperative inference. Cooperative inference between probabilistic agents diﬀers from standard
Bayesian inference in the second agent, the teacher, who selects the data, and in that the agents reason
about each other’s beliefs. Based on the shared joint distribution between data and hypotheses, the teacher
reasons about the learner’s beliefs, and samples data to pass according to the learner’s current distribution
on hypotheses, the joint distribution, and the desired hypothesis to be conveyed. The learner then reasons
baseduponwhatdatatheyhavebeenpassedbytheteacherandinfers, basedonthesharedjointdistribution,
what hypothesis the teacher is attempting to convey. This process may be represented mathematically by
the following system of equations:
PL(h|d) =PT(d|h)PL0(h)
PL(d), PT(d|h) =PL(h|d)PT0(d)
PT(h)(1)
wherePL(h|d)represents the learner’s posterior probability for hypothesis hgiven datum d;PT(d|h)is the
probability of the teacher selecting datum dto convey hypothesis h;PL0(h)represents the learner’s prior for
hypothesis h;PT0(d)is the teacher’s prior for selecting data d;PL(d)andPT(h)are normalizing constants.
Sinkhorn scaling of matrices (i.e. alternating row-column normalization of the joint distribution) may be
used to solve equation (1), and the result is an optimal entropy-regularized plan for transporting beliefs
(Wang et al., 2019; 2020b).
Sequential cooperative inference. In sequential cooperative Bayesian inference (SCBI), a teacher and
learner participate in rounds of learning. To convey a particular hypothesis (or belief on the space of possible
hypotheses) from the hypothesis-space H, in each round the teacher passes a datum d∈Dto the learner, and
the learner updates their belief distribution accordingly. At the end of each round, the teacher and learner
2Under review as submission to TMLR
both update their posterior distributions to become their prior distributions in the next round (Wang et al.,
2020a). Each round of learning behaves as in cooperative inference, where the system of equations (1) must
be solved. However, at the end of each round, each round diﬀers in having updated the prior, which is one
marginal constraint for Sinkhorn scaling in (1).
The process of teaching-learning-updating works as follows: Beginning with the joint distribution Mnof the
previous round and distribution θn∈P(H), which represents the learner’s beliefs from the previous round
of teaching-learning, where P(H)is the simplex of probability distributions on H, the teacher computes the
Sinkhorn scaling of Mnwith row sums λand column sums θn. Call this Mn+1. Hereλis an underlying
distribution onDreﬂecting inherent biases in selecting particular data points; λis typically taken to be
the uniform distribution. Then the teacher uses the distribution Mn+1ˆθto sample datum dn+1fromDand
passes it to the learner, where ˆθis the desired belief on hypotheses which the teacher wishes to convey,
typically a Dirac distribution, corresponding to a corner of the simplex. The learner then calculates Mn+1
in exactly the same way as the teacher, then multiplies θnby the likelihood of selecting dn+1. Normalizing
gives a distribution θn+1. The process then repeats inductively, with nreplaced everywhere by n+ 1.
SCBI as a Markov chain. The process of SCBI can be realized as a Markov chain on P(H)(Wang et al.,
2020a). With Td:P(H)→P (H)the map bringing the learner’s prior to posterior when data dis chosen
by the teacher; and τ:P(H)→P (D)the map of the teacher’s sample distribution based on the learner’s
prior, andτdthed-th component of this map, the Markov transition operator for a ﬁxed hypothesis h∈H,
Ψ(h) :P(P(H))→P(P(H))is deﬁned as:
(Ψ(h)(µ))(E) :=/integraldisplay
E/summationdisplay
d∈Dτd(T−1
d(θ)d(T∗
d(µ))(θ) (2)
whereT∗
dis the push-forward of Tdon Borel measures1,µis a Borel probability measure on the simplex
P(H), andE⊆P(H)is a Borel measurable subset. Tdandτare computed as above by using the Sinkhorn
scaling of the joint distribution between data and hypotheses.
Wang et al. (2020b) consider the problem of a teacher and learner communicating cooperatively in discrete
rounds of teaching/learning. The teacher and learner reason using Bayesian inference at each round, without
any reference to what future probability distributions on the available hypotheses might be, and without
reference to any costs/rewards the teacher uses in order to determine what distribution to use to sample the
data which they are passing to the learner. Although there is a discussion of SCBI as a Markov chain in
(Wang et al., 2020b), there is no extension of the method to a Markov decision process. Here we extend the
formalism to include planning ahead by the teacher, as well as rewards/costs the teacher uses in planning in
order to bias the learner towards/away from a particular hypothesis.
3 Curriculum planning via Markov decision processes
Curriculum planning involves the teacher planning several teaching moves in advance. We may model this
using a Markov decision process (MDP) as follows: Let the state space of the process be given by S=P(H),
the probability distributions on H. Let the action space of the MDP be given by A=D, the data available
for the teacher to pass to the learner (interpretation: an action d∈Dcorresponds to passing dto the
learner). We ﬁx an underlying distribution λ∈P(D), which represents any inherent bias towards selecting
or not selecting some particular data. In SCBI (Wang et al., 2020a), λis the uniform distribution. We will
allow the reward function Rto be a combination of positive and negative pieces (to include costs, if some
hypotheses are particularly undesirable).
The transition probability T(ω|d,θ)of the MDP between probability distributions ωandθonH, based
upon the teacher selecting datum d, isλ(d)ifω=Td(θ)and is zero otherwise. A teaching strategy then
consists of a plan π:P(H)→P (D), eﬀectively, ‘sample datum dusingπ(θ)when the current distribution
on hypotheses is θ.’ In SCBI, the d−thcomponent of the plan πisπd(θ) =τd(θ), i.e. the adjustment of the
teacher’s distribution according to the learner’s current distribution. The teacher’s strategy can be made
1i.e.T∗
d(µ)(E) =µ(T−1
d(E)).
3Under review as submission to TMLR
Figure 1: Analysis of convergence to the target hypothesis as a function of longer curricula (0-4) for
probabilistic action selection and argmax action selection, given 3 data and 2 hypotheses. We show the
probability of the target hypothesis according to the learner versus # teaching/learning rounds, using
X(θ) =θ([true hypothesis ]).(left)On average, when the teacher selects actions probabilistically (i.e.
Eq. 3), longer curricula yield a marginal diﬀerence. (right)On average, when the teacher selects actions
using argmax, longer curricula is not diﬀerent from not planning ahead. All curves for 0steps ahead to 4
steps ahead are exactly overlaid.
deterministic if πdis an atomic distribution with a single atom for every d∈D. Throughout this paper we
will be focusing on plans πwhich amount to the teacher preparing a curriculum by calculating what might
happen in future rounds of learning based upon the current data selection.
Explicitly, the plan πN
R:P(H)→P (D)corresponding to planning ahead Nmoves based on the teaching
strategyπand a random variable Xis realized by the following procedure:
πN
R(θ)(d) =Norm/parenleftbigg
Mθ,λ
(d,h)EΨNπδMθ,λ
(d,−)/λ(d)[R] +C/parenrightbigg
=Norm/parenleftBig
Mθ,λ
(d,h)EΨNπδ(d)[R] +C/parenrightBig
(3)
Here Norm (·)is normalization to ensure that/summationtext
d(πN
R(θ)(d)) = 1,Mθ,λis the Sinkhorn scaling of Mso that
for everyd∈Dandh∈H,/summationtext
d/prime∈DMθ,λ
(d/prime,h)=θ(h)and/summationtext
h/prime∈HMθ,λ
(d,h/prime)=λ(d). Thehin Equation (3) is the
target hypothesis the teacher desires to convey, R:P(H)→Ris a random variable representing a reward
or cost, and Ψπ:P(P(H))→P (P(H))is the Markov operator induced by the teaching plan π, i.e. for a
Borel measure µ∈P(P(H))andE⊆P(H)a measurable subset,
Ψπ(µ)(E) =/integraldisplay
E/summationdisplay
d∈Dπd(T−1
d(θ))d(T∗
dµ)(θ), (4)
andCis a constant so that what is being normalized to a distribution is non-negative (typically chosen to
be/epsilon1+ min/braceleftBig
Mθ,λ
(d,h)EΨN
πδ(d)[R]|d∈D,h∈H/bracerightBig
, for some small /epsilon1). Frequently we will drop the subscript R, to
simplify notation.
Note that the behavior of the teaching/learning interaction will vary, depending upon the normalization
constantC. On one extreme, as C→0+,πN
R(θ)(d)→Norm/parenleftBig
Mθ,λ
(d,h)EΨNπδ(d)[R]/parenrightBig
. Because of the potential
non-positivity, but the overall normalization, this corresponds to a signed probability measure. At the
other extreme, as C→ ∞,πN
R(θ)(d)→1
|D|, independently of θ; i.e. the teacher’s distribution is the
uniform distribution on |D|, regardless of the learner’s beliefs, hence the teacher’s choice of data is random.
In particular, if Cis much greater than the expectation term for some particular choice of data d, then
πN
R(θ)≈1
|D|. In order to make the distribution positive, we must have:
C >/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsinglemin
ds.t.Mθ,λ
(d,h)Eδ(d)[R]≤0Mθ,λ
(d,h)EΨNπδ(d)[R]/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle(5)
4Under review as submission to TMLR
On the other hand, in order to preserve the teacher’s knowledge and avoid random data selection, we would
also like:
C≤ min
d|Mθ,λ
(d,h)EΨNπδ(d)[R]>0Mθ,λ
(d,h)EΨNπδ(d)[R] (6)
However, for random variables Rwhich place too high a cost on some outcomes, versus too small a reward,
it may be impossible to simultaneously meet these two conditions. That is, ensuring a positive probability
distribution may create a more uniform distribution for the data which already have a positive probability
of selection, prior to the addition of the normalization constant C.
In Equation (3), if R(ω) =ω(h), this corresponds to the teacher calculating Nsteps ahead in order to choose
the data at the current step which will increase the likelihood the learner will be able to infer the target
hypothesis in the future. Furthermore, we can replace the expectation of the random variable ω(h)with an
expectation that accounts for rewards/costs inherent to the problem. For example, if πis the SCBI teaching
strategy, then π(θ)(d) =Mθ,λ
(d,h)
θ(h), wherehis the target hypothesis (Wang et al., 2020b).
The expectation term of (3) may be simpliﬁed as follows, letting ˜θ=Mθ,λ
(d,−), and assuming that the original
teaching plan follows the scheme of SCBI:
EΨNδ˜θ[R(θ)] =/summationdisplay
d1,...,dN∈DτdN(˜θ)τdN−1(TdN(˜θ))···τd1(Td2◦Td3◦···◦TdN(˜θ))R(Td1◦···◦TdN(˜θ)).(7)
This formula will be useful for simulations, as it allows us to replace the integral arising from the expectation
EΨNπδ(d)[R]by a ﬁnite (though growing exponentially in the number of steps planning ahead) sum. The
teacher, when planning a curriculum consisting of ﬁnitely many data points, can only guide the learner to a
ﬁnite number of distributions on hypotheses.
3.1 Optimal policy
We may compute the optimal policy as follows:
E/bracketleftBigg∞/summationdisplay
t=0γtRat(θt,θt+1)/bracketrightBigg
=∞/summationdisplay
t=0γtR(θt)Pat(θt,θt+1) =∞/summationdisplay
t=0γtR(θt)λ(d(t))δθt+1,Td(t)(θt)πd(t)(θt)(8)
Assuming that λis uniform, i.e. the teacher has no underlying bias with regards to the data, and also using
the fact that δθt+1,Td(t)(θt)is only nonzero when Td(t)(θt) =θt+1, this becomes:
=1
|D|∞/summationdisplay
t=0γtR/parenleftbig
Td(t−1)◦Td(t−2)◦···◦Td(0)(θ0)/parenrightbig
πd(t)/parenleftbig
Td(t−1)◦Td(t−2)◦···◦Td(0)(θ0)/parenrightbig
·
·πd(t−1)/parenleftbig
Td(t−2)◦···◦Td(0)(θ0)/parenrightbig
···πd(0)(θ0)
The optimal policy which maximizes the expectation term in Equation 8 is therefore the argmax over all
possible functions d:N→D, where the action at step tisd(t−1), and argmax over functions π:P(H)→
P(D). Note that Equation 8 is the same as Equation 7 with a factor of1
Din front,N→∞, reverse indexing,
and a discount factor γ∈[0,1]. In particular, this implies that taking an argmax over the distribution in
Equation 3 gives a ﬁnite horizon approximation of the optimal policy.
We may state this more formally as follows:
Theorem 1 The optimal policy is given by the following:
lim
γ→1argmaxπargmaxd:N→DE/bracketleftBigg∞/summationdisplay
t=0γtR(θt)/bracketrightBigg
=argmaxπlim
N→∞argmaxd:{0,...,N−1}→DπN(θ0)(9)
5Under review as submission to TMLR
Figure 2: Analysis of convergence as before using reward R(θ) = 10·dL1(θ,δhbad)−dL1(θ,δhtrue). On average,
when the teacher selects actions probabilistically (left)or deterministically (argmax) (right)curricula yield
worse outcomes. Planning ahead leads to misalignment between learners and teachers, when the teacher
attempts to avoid an undesirable hypothesis. Note that further planning ahead reduces misalignment,
consistent with the fact that planning suﬃciently far ahead is equivalent to not planning at all.
Proof:Note that normalization does not aﬀect which entries of πN(θ)are larger than others, so we have:
lim
N→∞argmaxd:{0,...,N−1}→DπN
d(N−1)(θ0) = lim
N→∞argmaxd:{0,...,N−1}→D/parenleftBig
Mθ,λ
(d(N−1),h)EΨNπδ(d)[R] +C/parenrightBig
= lim
N→∞argmaxd:{0,...,N−1}→D/parenleftBig
Mθ,λ
(d(N−1),h)EΨNπδ(d(N−1)[R]/parenrightBig
= lim
N→∞argmaxd:{0,...,N−1}→D/parenleftBig
Td(N−1)(θ)(h)EΨNπδ(d(N−1)[R]/parenrightBig
BecauseTd(N−1)(θ)(h)is multiplied by every term of the expectation sum, we obtain that the above is equal
to:
= lim
N→∞argmaxd:{0,...,N−1}→D/parenleftBig
EΨNπδ(d(N−1)[R]/parenrightBig
= lim
γ→1E/bracketleftBigg∞/summationdisplay
t=0γtR(θt)/bracketrightBigg
Taking the argmax over policies π:P(H)→P(D)then yields the result. 
The policy πd(t)(θ0)which satisﬁes the argmax above at time step tshould be the Dirac distribution δd(t),
whered(t)is the optimal choice of datum at time t. We may therefore obtain an estimate of the optimal
policy by taking the argmax of πNasNbecomes progressively larger.
SCBI vs SCBI with planning. When preparing a curriculum, if the teacher does not plan ahead, they
may base their teaching strategy for each round on Bayesian inference. This amounts to an initial teaching
strategy of SCBI. However, if the same Bayesian teacher plans a curriculum which includes inﬁnitely many
data points, and uses the random variable R(θ) =θ(htarget )in their planning in order to eﬀectively sample
data points leading the learner to a stronger belief in the true hypothesis, the usual process of SCBI is
recovered.
Theorem 2 Suppose that π:D→His a teaching plan and Xis a reward/cost such that for every d∈D,
limN→∞EΨNπδ(d)[R]exists, and is independent of d. Thenπ∞
R:= limN→∞πN
Rexists and is equal to the SCBI
teaching strategy.
Proof:Letθ∈P(H), andd∈D, and suppose that the teacher is teaching hypothesis h∈H. To ease the
readability, we will drop the subscript Ron the teaching plan. Then:
lim
N→∞πN(θ)(d) = lim
N→∞Mθ,λ
(d,h)EΨNπδ(d)[R]
/summationtext
d/primeMθ,λ
(d/prime,h)EΨNπδ(d)[R]=Mθ,λ
(d,h)/summationtext
d/primeMθ,λ
(d/prime,h)=Mθ,λ
(d,h)
θ(h)(10)
This is the formula used in the standard SCBI teaching strategy at each round of learning. 
6Under review as submission to TMLR
Corollary 3 Withπthe standard SCBI teaching strategy and R(θ) =θ(h), wherehis the target hypothesis,
planning inﬁnitely far ahead is identical to not planning ahead at all.
Proof:By the proof of ((Wang et al., 2020a), Theorem 3.5), limN→∞EΨNπµ[θ(h)] = 1, for any Borel measure
µ∈P(P(H)). The claim then follows from Theorem 2 above. 
The corollary above says that the limit of planning inﬁnitely far ahead using SCBI is identical to SCBI with
no planning ahead! Intuitively, this makes sense: in the inﬁnite limit, the teacher is considering all possible
inﬁnite strings of data to pass to the learner; however, most of the strings will be indistinguishable as the
learner approaches a particular belief on the true hypothesis, and so only the short-term behaviour of the
strings is important. Furthermore, because the proof in Wang et al. (2020b) also shows that the convergence
is monotone increasing; this implies that not planning ahead is the optimal teaching strategy when the
reward at each round is equal to the probability that the learner would select the target hypothesis out of
all hypotheses.
Figure 1 compares planning ahead up to four steps for two hypotheses and three data points, using R(θ) =
θ(h), withhthe hypothesis to be taught, assuming actions are selected probabilistically. The vertical axis
is the learner’s probability of choosing has the correct hypothesis if they were to randomly guess, while the
horizontal axis represents the number of rounds of teaching/learning. The plot was created by randomly
generating 1500initial joint distributions, performing 50rounds of teaching-learning with each 30times,
then averaging over the learner’s performance.
An interesting feature of planning ahead with R(θ) =θ(htarget )is that if the teacher uses the deterministic
procedure of argmax instead of sampling from the distribution to choose data to pass to the learner, the
choice is the same for every number of steps planning ahead; i.e., in this case, local maximization is the same
as global maximization (see Figure 1).
Other random variables are available to use in the expectation: for example, if there is one particular
hypothesis which the teacher wishes to avoid, while biasing the learner toward the true hypothesis, the
random variable could be R(θ) = 10·dL1(θ,δhbad)−dL1(θ,δhtrue), wheredL1(·,·)is theL1distance between
θandδhrepresented as points of a simplex. In this case, there is non-monotonic behaviour, as the teacher
overcompensates for trying to move the learner away from the ‘bad’ hypothesis, which subsequently may
lead the learner closer to a neutral hypothesis than to the true hypothesis. See Figure (2), in particular the
trajectories corresponding to planning ahead one step and two steps, where the probability that the learner
selects the true hypothesis decreases.
For a comparison with a naive Bayesian learner, see Section 1 of the supplementary materials.
Guaranteed alignment via monotonicity. One of the key results of (Wang et al., 2020b) is the consis-
tency of SCBI. Consistency here refers to the convergence in expectation of the learner’s belief distribution
to a Dirac distribution on the target hypothesis over the inﬁnite limit of teaching rounds. In particular,
Wang et al. (2020b) shows that if monotonicity holds, i.e. EΨπµ[θ(h)]−Eµ[θ(h)]>0, wherehis the target
hypothesis, then consistency and hence alignment follows. By writing out the equation above for mono-
tonicity with πreplaced by πN
R, for some choice of rewards/costs R, we obtain a condition for monotonicity
and hence for alignment. Writing out the equation for EΨπXµ[θ(h)]andEµ[θ(h)], we may obtain an explicit
condition for when EΨπXµ[θ(h)]≥Eµ[θ(h)]. Hence:
Theorem 4 Monotonicity holds if and only if for any µ∈P(H):
/integraldisplay
∆/summationdisplay
dπd(θ)·(Td(θ)(h)−θ(h))dµ(θ)>0 (11)
Proof:Throughout this proof, we rewrite π(θ)(d)asπd(θ)for ease of readability. Expanding EΨπµ[θ(h)]−
Eµ[θ(h)]using the deﬁnition of Ψπyields:
/integraldisplay
∆/summationdisplay
dπd/parenleftbig
T−1
d(θ)/parenrightbig
θ(h)d(T∗
dµ)(θ)−/integraldisplay
∆θ(h)dµ(θ)
7Under review as submission to TMLR
=/summationdisplay
d/integraldisplay
T−1
d(∆)πd(θ)Td(θ(h))dµ(θ)−/integraldisplay
∆θ(h)dµ(θ)
=/integraldisplay
∆/summationdisplay
dπd(θ)·(Td(θ)(h)−θ(h))dµ(θ)
To get from the penultimate line to the ﬁnal line, we use the fact that Td: ∆→∆is a homeomorphism of
the simplex. 
From this equation, we can see that if the δMθ,λ
(d,−)expectation of Ris overly negative for a hypothesis for
whichTd(θ)(h)> θ(h), whileTd/prime(θ)(h)< θ(h)for other hypotheses, monotonicity will be broken. This
implies that if the teacher places a heavy cost on a hypothesis lying close in belief-space to the target
hypothesis, the curriculum of the teacher may over-emphasize moving away from the heavy cost hypothesis,
at the expense of potentially converging to a more neutral hypothesis. Here two hypotheses h1andh2‘lying
close in belief-space’ means that with respect to the initial joint distribution M, theL1distance on the
simplexP(D)betweenM(−,h1)andM(−,h2)is small. This implies that whatever data the teacher passes to
the learner will aﬀect both rows similarly.
4 Examples and simulations
Figure 3: (top)GridWorld on a cylinder, with the entire top and bottom edges of the rectangle glued
together. The grey region is hypothesis C, which is an undesirable hypothesis, and the target hypothesis
is A.(bottom) Analysis of convergence to the target hypothesis, A, as a function of number of steps of
planning ahead (0-4) for probabilistic action selection. Curricula yield marginal gains.
All simulations were run on a laptop computer with an AMD Ryzen 5 3550H with Radeon Vega Mobile Gfx
2.10 GHz processor or a macOS Mojave with 3.6 GHz Intel Core i7.
8Under review as submission to TMLR
Figure 4: Analysis of convergence of the learner’s beliefs for the random variable R(θ) = 10·dL1(θ,δC)−
dL1(θ,δA)on the cylinder gridworld with a teacher that probabilistically selects data. (left)Probability
that the learner guesses the correct location is A, the target hypothesis, versus # teaching/learning rounds.
(right)Probability that the learner incorrectly guesses the location is B, the neutral hypothesis, versus
# teaching/learning rounds. Avoiding the undesirable hypothesis, C, results in misalignment such that the
teacher’s examples lead the learner to the incorrect hypothesis Bwhen planning ahead.
GridWorld on a cylinder. Suppose we have a grid on a cylinder, as in Figure 3 (left), where the top and
thebottomedgehavebeengluedtogether. Inthisproblem, theteacherisattemptingtodirectanagenttothe
locationA, while the location C, taking up three grid squares, represents a particularly undesirable location.
In this case, the hypotheses correspond to the locations A,B,C, so|H|= 3, and the data correspond to
the four possible directions in the grid (up, down, left, right), so |D|= 4. Starting from location X, the
teachers who plan ahead out-perform the teacher who does not plan ahead on average, as shown in Figure
3 (right). When the teacher attempts to bias the agent away from Cand toward Aby using the random
variableR(θ) = 10·dL1(θ,δC)−dL1(θ,δA)in planning, the agent is more likely to converge to the neutral
hypothesis, as there is more incentive to move away from Cthan to head toward A, so the agent gets closer
toB. This is shown in Figures 4 (left) and (right).
Robustness comparisons. To compare robustness, we perturbed the teacher’s estimation of the learner’s
prior at each round of teaching/learning. As shown in Figure 5, planning ahead is typically less robust
than not. Each step of planning ahead relies upon estimating the learner’s beliefs, which compounds the
overall error at each step of teaching/learning. In particular, Figure 5 shows that as the error increases, the
convergence of the teachers who are planning ahead slows down, and that teachers who plan further ahead
are less robust to perturbations in general.
As another test of robustness, we compare runs with two diﬀerent initial joint distributions. This corresponds
to the teacher and learner not having a shared knowledge of the relationships between data and hypotheses
at the start. Another way to think of this is that one of the agents may have a mis-estimation of the
relationship between data and hypotheses. Figure 6 shows that no tested strategies are robust with respect
to an initial perturbation in joint distribution.
Section 2 of the supplementary materials contains simulations detailing convergence for various sizes of
data/hypothesis spaces.
5 Related work
The mathematical theory of cooperative communication as optimal transport has been explored in (Wang
et al., 2020b), (Yang et al., 2018), and (Wang et al., 2019). Sequential cooperation as utilized in this paper,
including proofs of stability and rates of convergence, may be found in (Wang et al., 2020a).
Cooperative inference and reinforcement learning appear together in several contexts. For example, cooper-
ative inverse reinforcement learning is centered around cooperative inference (Hadﬁeld-Menell et al., 2016;
Fisac et al., 2020), and cooperative inference and reinforcement learning often appear in the coordination of
multiple agents, e.g. (Pesce and Montana, 2020). Milli and Dragan (2020) consider misalignment in terms
of levels of theory of mind recursion. Our work diﬀers in considering the problem of developing curricula to
9Under review as submission to TMLR
Figure5: Analysisoftherobustnessofcurriculumlearningtoerrorsintheteacher’sestimationofthelearner’s
prior, assuming reward R(θ) =θ(h), with three hypotheses and four data points. Probability that the learner
infers the correct hypothesis versus # teaching/learning rounds. (upper left) Average perturbation of size
.001on the teacher’s estimation of the learner’s prior, with the teacher sampling probabilistically. (upper
right)Average perturbation of size .1on the teacher’s estimation of the learner’s prior, with the teacher
sampling probabilistically. (lower left) Average perturbation of size .001on the teacher’s estimation of the
learner’s prior, with the teacher using argmax to select data. (lower right) Average perturbation of size
.1on the teacher’s estimation of the learner’s prior with the teacher using argmax to select data. Longer
curricula lead to greater misalignment due to propagation of error, and the misalignment is more severe with
larger error.
Figure 6: Convergence analysis when teacher accurately (solid lines) or inaccurately (dashed lines) esti-
mates the learner’s joint distribution when curriculum varies in the number steps planning ahead (0-4) with
probabilistic data selection. Accurate estimates result in robust convergence across curricula. In contrast,
inaccurate estimates result in severe alignment problems, which are worse for longer curricula.
foster learning and oﬀers novel theoretical and simulation-based insights into the possibility of misaligned
curricula.
In curriculum learning (Elman, 1993), a learner is presented with carefully chosen examples or tasks, often
increasing in complexity and carefully curated by the teacher. Such strategies are used in human cognition
when teaching-learning complex tasks (Khan et al., 2011), and curriculum learning is a useful method in
machine learning for gradually revealing complex concepts by slowly building up from simpler components
(Bengio et al., 2009). Curriculum learning is used, for example, in neural networks in order to maximize
10Under review as submission to TMLR
learning eﬃciency (Graves et al., 2017), and multitask learning (Pentina et al., 2015). Curriculum learning
has been included into the framework of a partially observable Markov decision process in Matiisen et al.
(2020); Narvekar and Stone (2018). In Matiisen et al. (2020), the student is not directly observable by the
teacher and the actions of the teacher correspond to teaching the learner on a certain task for a speciﬁc
number of iterations. In Narvekar and Stone (2018), curriculum design is viewed as transfer learning, and
the problem of learning a meta-policy is addressed. Our approach is distinctive in focusing on the alignment
problem and oﬀering strong mathematical proofs.
It is attractive to consider curricula that actively avoid common misconceptions. We are not aware of prior
work in curriculum learning that does so, but it is common in reinforcement learning to consider worlds
with negative reward, which is equivalent. In education, dealing with misconceptions most typically occurs
through direct negation; for example, by including incorrect examples for students to correct (Heemsoth and
Heinze, 2014) and through refutation texts (Tippett, 2010). Evidence suggests that such eﬀorts are only
eﬀective when students know enough about the domain and are most eﬀective at avoiding the misconception
rather than inducing the correct belief. Thus, our ﬁndings regarding lack of monotonicity and misalignment
are broadly consistent.
6 Conclusion
We investigate the possibility of alignment problems in curriculum learning by building recent theoretical
advances in modeling cooperation. Recasting sequential cooperative Bayesian inference (SCBI) as a Markov
decision process (MDP) enables the inclusion of curriculum learning to a teacher and learner cooperatively
learning in multiple rounds of interactions. Through theoretical and simulation-based analysis, we show
that curriculum planning introduces brittleness that leads to misalignment when the curriculum introduces
additional costs, for example when avoiding a misconception, or when the teacher has imperfect knowledge
of the learner. SCBI without curriculum planning oﬀers competitive rates of convergence and desirable
theoretical guarantees across our theoretical and simulation analyses. We also show that under simple
assumptions on the reward/cost, e.g. taking the reward to be the probability that the learner will select
the correct hypothesis, myopia appears to be optimal: planning ahead can actually decrease the rate of
convergence, while converging to no planning as the number of steps ahead tends to inﬁnity. Although this
paper has primarily focused on the SCBI framework of cooperative communication, it is feasible that similar
results exist for other MDP models of curriculum learning. Namely, naive choices of rewards/costs which
may at face-value be reasonable, could result in misalignment or slower convergence, and are a potential
pitfall for those working in reinforcement and curriculum learning. Future work should consider whether
other curriculum learning approaches can oﬀer similar theoretical and practical guarantees of alignment.
Broader Impact
Our work is on alignment problems in curriculum learning. We have identiﬁed a new alignment problem
which may be of longer term beneﬁt if methods for addressing it are developed. There is no obvious reason
why people may be put at disadvantage by this research, and it is unclear what the long term consequences
of this work will be.
References
Y. Bengio, J. Laouradour, R. Colloberty, and J. Weston. Curriculum learning. In L. Bottou and M. Littman,
editors,Proceedings of the 26th Annual International Conference on Machine Learning (ICML 2009) ,pages
41–48, New York, NY, 2009. Association for Computing Machinery.
B. Christian. The Alignment Problem: Machine Learning and Human Values . WW Norton & Company,
2020.
J.-L. Elman. Learning and development in neural networks: the importance of starting small. Cognition ,
48:71–99, 1993.
11Under review as submission to TMLR
J.-F. Fisac, M.-A. Gates, J.-B. Hamrick, C. Liu, D. Hadﬁeld-Menell, M. Palaniappan, D. Malik, S. S.
Sastry, T.-L. Griﬃths, and A.-D. Dragan. Pragmatic-pedagogic value alignment. In N. Amato, G. Hager,
S. Thomas, and M. Torres-Torriti, editors, Robotics Research. Springer Proceedings in Advanced Robotics,
vol 10, pages 49–57, Switzerland, 2020. Springer, Cham.
A. Graves, M.-G. Bellemare, J. Menick, R. Munos, and K. Kavukcuoglu. Automated curriculum learning for
neural networks. In D. Precup and Y. W. Teh, editors, Proceedings of the 34th International Conference
on Machine Learning (ICML 2017) , pages 1311–1320, online, 2017. JMLR.org.
D. Hadﬁeld-Menell, S. Russell, P. Abbeel, and A.-D. Dragan. Cooperative inverse reinforcement learning.
In D. Lee, M. Sugiyama, U. Luxburg, I. Guyon, and R. Garnett, editors, Advances in Neural Information
Processing Systems (NeurIPS 2016) , pages 3909–3917, Red Hook, NY, 2016. Curran Associates Inc.
T. Heemsoth and A. Heinze. The impact of incorrect examples on learning fractions: A ﬁeld experiment
with 6th grade students. Instructional Science , 42(4):639–657, 2014.
F. Khan, J. Zhu, and B. Mutlu. How do humans teach: on curriculum learning and teaching dimension. In
J. Shawe-Taylor, R. Zemel, P. Bartlett, F. Pereira, and K. Weinberger, editors, Advances in Neural Infor-
mation Processing Systems 24 (NIPS 2011) , pages 1449–1457, Red Hook, NY, 2011. Curran Associates
Inc.
T. Matiisen, A. Oliver, T. Cohen, and J. Schulman. Teacher-student curriculum learning. IEEE Transactions
on Neural Networks and Learning Systems , 31(9):3732–3740, 2020.
S. Milliand A.-D. Dragan. Literal or pedagogic human? analyzinghuman modelmisspeciﬁcationin objective
learning. In Uncertainty in artiﬁcial intelligence , pages 925–934. PMLR, 2020.
S. Narvekar and P. Stone. Learning curriculum policies for reinforcement learning. arXiv preprint
arXiv:1812.00285 , 2018.
A. Pentina, V. Sharmanska, and C.-H. Lampert. Curriculum learning of multiple tasks. In L. O’Conner,
editor,2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR 2015) , pages 5492–
5500, Piscataway, NJ, 2015. Conference Publishing Services.
E. Pesce and G. Montana. Improving coordination in small-scale multi-agent deep reinforcement learning
through memory-driven communication. Machine Learning , 109:1727–1747, 2020.
S. Russell. Human compatible: Artiﬁcial intelligence and the problem of control . Penguin, 2019.
C.-D. Tippett. Refutation text in science education: A review of two decades of research. International
journal of science and mathematics education , 8(6):951–970, 2010.
J. Wang, P. Wang, and P. Shafto. Sequential cooperative bayesian inference. In H. D. III and A. Singh,
editors,Proceedings of Machine Learning Research (PMLR 2020) , pages 10039–10049, online, 2020a.
PMLR.
P. Wang, P. Paranamana, and P. Shafto. Generalizing the theory of cooperative inference. In K. Chaudhuri
and M. Sugiyama, editors, Proceedings of Machine Learning Research (PMLR 2019) , pages 1841–1850,
online, 2019. PMLR.
P. Wang, J. Wang, P. Paranamana, and P. Shafto. A mathematical theory of cooperative inference. In
H. Larochelle, M. Ranzato, R. Hadsell, M. Balcan, and H. Lin, editors, Advances in Neural Information
Processing Systems Pre-proceedings (NeurIPS 2020) , online, 2020b. PMLR.
S. C.-H. Yang, Y. Yu, A. Givchi, P. Wang, W. K. Vong, and P. Shafto. Optimal cooperative inference.
In A. Storkey and F. Perez-Cruz, editors, Proceedings of the 21st International Conference on Artiﬁcial
Intelligence and Statistics (AISTATS 2018) , pages 376–385, online, 2018. PMLR.
12