Published in Transactions on Machine Learning Research (06/2023)
Empirical Study on Optimizer Selection
for Out-of-Distribution Generalization
Hiroki Naganuma1,2, Kartik Ahuja1,2, Shiro Takagi3, Tetsuya Motokawa4,
{naganuma.hiroki, kartik.ahuja}@mila.quebec, {takagi4646, moto.t.03.td}@gmail.com,
1Mila - Quebec AI Institute,2Université de Montréal,3Independent Researcher,4University of Tsukuba,
Rio Yokota5, Kohta Ishikawa6, Ikuro Sato5,6, Ioannis Mitliagkas1,2,7
rioyokota@gsic.titech.ac.jp, kishikawa@d-itlab.co.jp, isato@c.titech.ac.jp, ioannis@mila.quebec
5Tokyo Institute of Technology,6Denso IT Laboratory Inc.,7Canada CIFAR AI Chair
Reviewed on OpenReview: https: // openreview. net/ forum? id= ipe0IMglFF
Abstract
Modern deep learning systems do not generalize well when the test data distribution is
slightly diﬀerent to the training data distribution. While much promising work has been
accomplished to address this fragility, a systematic study of the role of optimizers and their
out-of-distribution generalization performance has not been undertaken. In this study, we
examine the performance of popular ﬁrst-order optimizers for diﬀerent classes of distributional
shift under empirical risk minimization and invariant risk minimization. We address this
question for image and text classiﬁcation using DomainBed, WILDS, and Backgrounds
Challenge as testbeds for studying diﬀerent types of shifts—namely correlation and diversity
shift. We search over a wide range of hyperparameters and examine classiﬁcation accuracy
(in-distribution and out-of-distribution) for over 20,000 models. We arrive at the following
ﬁndings, which we expect to be helpful for practitioners: i) adaptive optimizers (e.g.,
Adam) perform worse than non-adaptive optimizers (e.g., SGD, momentum SGD) on out-of-
distribution performance. In particular, even though there is no signiﬁcant diﬀerence in in-
distribution performance, we show a measurable diﬀerence in out-of-distribution performance.
ii) in-distribution performance and out-of-distribution performance exhibit three types of
behavior depending on the dataset—linear returns, increasing returns, and diminishing
returns. For example, in the training of natural language data using Adam, ﬁne-tuning
the performance of in-distribution performance does not signiﬁcantly contribute to the
out-of-distribution generalization performance.
1 Introduction
The choice of numerical optimization method can make a big diﬀerence when it comes to successfully
training deep neural networks. In particular, the choice of optimizer inﬂuences training speed, stability,
and generalization performance. Several studies have compared a variety of optimizers and investigated
their inﬂuence on trainability and generalization (Wilson et al., 2017; Schneider et al., 2019; Choi et al.,
2019). Some concluded that non-adaptive optimizers yield better generalization (Wilson et al., 2017; Balles
& Hennig, 2018), while others countered that optimizer selection does not aﬀect generalization performance
(Schneider et al., 2019; Schmidt et al., 2021).
The conﬂicting nature of past reports can be explained by disparities in the methodology used for hyperpa-
rameter search. For Adam, in particular, hyperparameter /epsilon1controls the degree of adaptation. Low values,
which are used by default, lead to a highly adaptive method. Unusually high values lead to less adaptation.
In the limit of large /epsilon1, Adam turns into a non-adaptive momentum method. In other words, when arbitrary
tuning is allowed, methods like a Adam can be thought of as a superset of gradient descent with momentum.
1Published in Transactions on Machine Learning Research (06/2023)
The authors in Choi et al. (2019) take this approach, and also consider the less adaptive and non-adaptive
regimes of Adam. Not surprisingly, they ﬁnd that in this full generality Adam never performs worse than
gradient descent with momentum in terms of in-distribution generalization, and in some cases, it can perform
better.
Although these studies have made substantial progress to improve our understanding of optimizer char-
acteristics, they are based on a common, foundational assumption in learning: training and test data are
drawn from the same distribution. In applications, however, it is often the case that test data obey a
distribution diﬀerent from the one for training data. This distributional shift violates the typical assumption
of independent and identically distributed (i.i.d.) data for learning (Nagarajan et al., 2021). Comparing the
generalization performance of diﬀerent optimizers under this distributional shift, known as out-of-distribution
(OOD) generalization (Shen et al., 2021), is of great interest in theory and practice.
In our large suite of experiments, we focus on this OOD generalization question for Natural Language
Processing (NLP) and image classiﬁcation tasks. We train deep neural networks under the principle of
Empirical Risk Minimization (ERM) or Invariant Risk Minimization (IRM) (Arjovsky et al., 2019) using a
variety of optimizers. Because our objective is to investigate the impact of commonly used optimizers, we
target ﬁve of the most popular optimizers that have been used and studied in recent years (Schmidt et al.,
2021): stochastic gradient descent (SGD), Momentum SGD (Polyak, 1964), and Nesterov accelerated gradient
(NAG, also known as Nesterov’s momentum) (Nesterov, 2003) in the family of non-adaptive methods as well
as RMSProp (Tieleman & Hinton, 2012) and Adam (Kingma & Ba, 2015) in the family of adaptive optimizers.
We evaluate the OOD generalization performance of these optimizers on 10 diﬀerent benchmarks: DomainBed
(which includes seven image datasets) (Gulrajani & Lopez-Paz, 2021), the Backgrounds Challenge dataset
(Xiao et al., 2021), , and CivilComments-WILDS (Koh et al., 2021).
As discussed above, methods like RMSProp and Adam can be tuned not to be adaptive, in which case they
would subsume methods like gradient descent with momentum, and we would trivially get that the more
general method can never underperform. Instead, we focus on the more nuanced question of adaptive vs
non-adaptive methods: we tune RMSProp and Adam using a range of values for the hyperparameter /epsilon1,
which is strictly wider than ranges used in practice but still keeps the optimizers in their adaptive regime.
In this context, we conduct an exhaustive hyperparameter search to select conﬁgurations that give good
in-distribution validation accuracy for each optimizer. We then test the selected models on the above OOD
test sets. Importantly, we demonstrate that our experiments explore more hyperparameter conﬁgurations
than many existing benchmarks, as betrayed by the fact that we ﬁnd better-performing models on said
benchmarks.
In summary, our contributions are as follows:
•We design and perform a comparison of the eﬀect of optimizers on OOD generalization on a number
of OOD benchmarks. To the best of our knowledge, we are the ﬁrst to consider such a wide variety
and scale of datasets. Also, we conduct an empirical study using a wide range of hyperparameter
conﬁgurations, examining over 20,000 models, evaluating their performance, and measuring the
performance gap when moving from the in-distribution test set to the OOD test set.
•We demonstrate on a large number of image classiﬁcation and NLP tasks that diﬀerent optimizer
choices lead to diﬀerences in OOD generalization performance. Furthermore, we show that adaptive
optimizers yield more in-distribution overﬁtting and degrade OOD performance more than non-
adaptive optimizers.
•We show that the observed correlation behaviors between in-distribution performance and OOD
performance can be categorized into typical patterns: linear return, diminishing return, and increasing
return1. This categorization should help practitioners better understand and select optimizers.
The following evidence supports the claim that non-adaptive methods outperform adaptive methods in OOD
settings: i) There is no signiﬁcant diﬀerence in the in-distribution generalization performance between adaptive
1These show how much performance can be expected in the out-of-distribution if we increase the in-distribution performance.
These terms are explained in detail in Section 4.3.
2Published in Transactions on Machine Learning Research (06/2023)
and non-adaptive optimizers, ii) Adaptive optimizers perform worse in 10 out of 12 tasks regarding best
out-of-distribution generalization performance, iii) We match models trained by adaptive optimizers to models
trained by non-adaptive optimizers that yield the same in-distribution performance. Using this matching
scheme for our comparison, models trained by non-adaptive methods achieve better OOD performance on 11
out of 12 tasks. These results are based on a comprehensive hyperparameter search and validated through
soundness checks in the Appendix G.4.
Given the similarity of the results between ERM and IRM training principles, we have opted to focus
our analysis on the former. Therefore, in the main section of this paper, we primarily report the results
of ERM. Our observations that adaptive optimizers perform worse than non-adaptive methods in OOD
performance align with theoretical results (Zou et al., 2022) previously reported in the literature, highlighting
the drawbacks of adaptive optimizers such as Adam under the i.i.d. assumption and their tendency to ﬁt
noise in the data.
The paper is structured as follows. In Section 2, we discuss optimizer selection under the i.i.d. assumption and
outline the problem of OOD generalization in the presence of distributional shifts, which can be encountered
in real-world problems. In Section 3, we outline the optimizers and the OOD datasets that are the subject of
this study, as well as our model selection method. In Section 4, we present the experimental protocol and the
experimental results of 10 diﬀerent datasets and 5 optimizers in each experimental setting.
2 Related Work
2.1 Optimizer Selection
Understanding the characteristics of the many optimizers proposed for deep neural network training (Schmidt
et al., 2021) is of great importance to the machine learning research community. In terms of convergence,
preconditioned optimizers, including Adam, are known to be superior to non-adaptive optimizers (Kingma &
Ba, 2015; Liu et al., 2020; Amari, 1998).
While preconditioned optimization methods seem to be better than non-preconditioned ones in terms of
convergence, Zhang et al. (2019) argued that there is a trade-oﬀ between generalization performance and
convergence rate (Zhang et al., 2019). Wadia et al. (2020) also reported that preconditioned methods,
especially second-order methods, do not provide great generalization performance either empirically or
theoretically. With a simple theoretical and empirical analysis, Wilson et al. (2017) showed that adaptive
optimizers are worse at generalization than simple SGD. Balles & Hennig (2018) also reported that Adam
generalizes worse than Momentum SGD.
Contrary to these studies, Schneider et al. (2019) found that no single optimizer is the “best" in general.
Similarly, Schmidt et al. (2021) claimed that optimizer performance varies from task to task. Choi et al. (2019)
have come to a diﬀerent conclusion from all the studies cited above. As described in Section 1, Choi et al.
(2019) tuned the hyperparameter /epsilon1of adaptive methods that control the degree of adaptivity, which leads
to non-adaptive methods. As a result, they found that well-tuned adaptive optimizers never underperform
simple gradient methods.
These studies provided insights into how optimizer selection inﬂuences generalization. However, they focused
on the classical supervised learning setting, in which the test distribution is assumed to be the same as the
training distribution. Our research diﬀers from theirs in that we investigate optimizer selection’s inﬂuence on
OOD generalization.
2.2 Out-of-Distribution Generalization
Taming the distributional shift is a big challenge in machine learning research (Sugiyama & Kawanabe, 2012;
Ben-David et al., 2010; Pan & Yang, 2009; Szegedy et al., 2014; Arjovsky et al., 2019). Geirhos et al. (2020)
argued that many modern deep neural network models sometimes learn shortcut features instead of intended
features and overﬁt to a speciﬁc dataset.
3Published in Transactions on Machine Learning Research (06/2023)
Some studies have focused on generalization with adversarial noise to evaluate the impact of optimizer
selection on OOD generalization. For instance, the theoretical and empirical analysis by Khoury (2019)
showed that SGD is more robust against adversarial noise than adaptive optimizers. Wang et al. (2019)
argued that adversarial examples to some methods are not necessarily adversarial to others. Abdelzad et al.
(2020) found that the best optimizers for OOD detection vary by experimental setting. Metz et al. (2020)
reported that a learned optimizer somehow unexpectedly outperformed a human-designed optimizer in terms
of the OOD generalization.
These studies provide valuable insights on optimizer selection for the OOD problem. However, we emphasize
that the previous work discussed above explores hyperparameters in a limited range. Khoury (2019) conducted
the most exhaustive hyperparameter search but tuned only the learning rate for adaptive optimizers. As we
brieﬂy explain in Section 1, an exhaustive hyperparameter search is crucial for the empirical investigation of
an optimizer’s eﬀect. Thus, we explored more hyperparameters than previous studies, including searching for
over 20,000 models.
Here we emphasize that only shifts such as adversarial noise have been studied in previous studies of
optimization selection for OOD generalization. Thus, we use a much more diverse set of real OOD datasets,
including image classiﬁcation and NLP tasks where the distributional shift is signiﬁcant, covariate shift,
correlation shift, subpopulation shift, and background shift, not only domain generalization. The set of
datasets we explored is the most exhaustive for evaluating the optimizer’s role in OOD generalization, as far
as we know.
Finally, we mention to some relevant works Kumar et al. (2022a); Wortsman et al. (2022); Chen et al. (2023);
Kumar et al. (2022b). Kumar et al. (2022a); Wortsman et al. (2022); Kumar et al. (2022b) have studied the
intricate balance between ID and OOD performance when ﬁne-tuning pre-trained models. Chen et al. (2023)
theoretically showed that optimizing the ERM with the relaxed OOD penalty is not likely to have a good
performance and proposed a better practical solution.
3 Preliminaries
3.1 Optimizers Subjected to Our Analysis
Similar to previous studies (Wilson et al., 2017; Schneider et al., 2019; Choi et al., 2019), we compare two
types of optimizers. The ﬁrst one is non-adaptive optimizers. The update equation at iteration tof model
parameterθtis as follows:
vt←γvt−1+ηt˜∇θt−1/lscript(θt−1),θt←θt−1−vt (1)
whereηtis the learning rate, /lscript(θ)is the loss, and ˜∇θt−1is the stochastic gradient, in the particular case of
stochastic gradient descent, γ= 0. Optimizers with momentum terms such as Momentum SGD (Polyak,
1964), and Nesterov momentum (Nesterov, 2003) are also classiﬁed as non-adaptive optimizers, and γis
the parameter for controlling the momentum term. For Nesterov momentum, /lscript(θt−1)should be replaced
/lscript(θt−1−γvt−1).
The second type of optimizer is adaptive methods. Adam and RMSprop are adaptive optimizers and they
can be written in the form of the generic adaptive optimization method. The generic adaptive optimization
method can be written as in Algorithm 1. This is based on what (Liu et al., 2020) and (Reddi et al., 2018)
propose.
Our selection of optimizers aligns with prior research on optimizer comparison (Choi et al., 2019) and
recent studies on out-of-distribution (OOD) tasks, which have predominantly focused on Adam rather than
alternatives such as AdamW. Therefore, we concentrate on the ﬁve most commonly employed optimizers
in practice (Schmidt et al., 2021). Given their widespread usage, we chose to focus on studying Adam’s
performance under the adaptive regime, as this would provide more pertinent ﬁndings for present-day practices.
Experimental results for AdamW (Loshchilov & Hutter, 2017) and SAM (Foret et al., 2020), although not for
all data sets, are also discussed in Appendix I.3.
4Published in Transactions on Machine Learning Research (06/2023)
Algorithm 1 Generic adaptive optimization method setup.
Require:{ηt}T
t=1: step size,{φt,ψt}T
t=1function to calculate momentum and adaptive rate, θ0: initial
parameter, /lscript(θ): objective function
1:fort= 1toTdo
2:gt←˜∇θft(θt−1)(Calculate stochastic gradients w.r.t. objective at timestep t)
3:wt←φt(g1,...,gt)(Calculate momentum)
4:lt←ψt(g1,...,gt)(Calculate adaptive learning rate)
5:θt←θt−1−ηtwtlt(Update parameters)
6:end for
3.2 Out-of-Distribution Generalization Datasets
We use the following datasets to evaluate the optimizer inﬂuence on OOD generalization: DomainBed (Gulra-
jani & Lopez-Paz, 2021), Backgrounds Challenge (Xiao et al., 2021), (Koh et al., 2021), and CivilComments-
WILDS (Koh et al., 2021). These datasets include both artiﬁcially created and real-world data, and the
applications include image classiﬁcation and NLP tasks. Although we describe the details of these datasets in
Appendix C, we summarize them below.
Image Classiﬁcation Datasets: DomainBed consists of a set of benchmark datasets for domain general-
ization, which includes PACS (Fang et al., 2013), VLCS (Li et al., 2017), Oﬃce-Home (Venkateswara et al.,
2017), Terra Incognita (Beery et al., 2018) DomainNet (Peng et al., 2019), Rotated MNIST (Ghifary et al.,
2015), and Colored MNIST (Arjovsky et al., 2019). These datasets contain a variety of distributional shifts.
VLCS is a set of diﬀerent image datasets, for example, images of birdsfrom several datasets. Terra Incognita
is a dataset consisting of images taken by cameras at diﬀerent locations. The diﬀerence in the location of the
camera corresponds to the diﬀerence in the domain. PACS, Oﬃce-Home, and DomainNet are image datasets
whose style varies by domain. For example, an image of PACS in one domain is photography, while that in
another domain is a sketch. Rotated MNIST is an artiﬁcially generated dataset of domains that have been
given diﬀerent rotation angles.
Colored MNIST is an anomaly in the DomainBed dataset. P(Y|X)remains the same for all datasets
(covariate shift) except in Colored MNIST. This dataset is designed to make models fail by exploiting spurious
correlations in training environments. In particular, the dataset is such that the model can only exploit the
source of spurious correlation (color) and achieve a very high training accuracy without relying on the true
invariant source of correlation (shape). In contrast, none of the other datasets have such strong spuriousness.
The strength of the spurious correlation ﬂips in the test domain and thus it induces a negative correlation
between the validation and the test accuracy.
The Backgrounds Challenge dataset measures a model’s robustness against background shift (Xiao et al.,
2021). A model is trained on an image and evaluated on the same image with a diﬀerent background. If the
model exploits the background features during training, it will be fooled during evaluation. Therefore, if
the model strongly depends on the background, this dataset is a diﬃcult OOD dataset, while if the model
does not, this dataset is easy to model. To further strengthen our claim, we also performed experiments on
CIFAR10-C and CIFAR10-P which can be casted to image corruption and perturbation shift. The results are
shown in Appendix I.1.
Natural Language Processing (NLP) Datasets: The CivilComments-WILDS dataset is cast as a
subpopulation shift problem. The shift problem tackles a binary classiﬁcation problem that predicts the
toxicity of comments on articles, and domain vectors are assigned according to whether the comment refers
to one of eight demographic identities. In the subpopulation shift, the test distribution is a subpopulation of
the training distribution.
The dataset has the characteristics of a hybrid shift of a subpopulation shift and domain shift. This dataset
is cast as the problem of estimating a rating of 1–5 from the rating comments of each user. In this kind
5Published in Transactions on Machine Learning Research (06/2023)
of dataset, each user corresponds to a domain, and the goal is to produce a high performance for all user
comments.
3.3 Model Selection Method and Evaluation Metrics
We leverage two distinct model selection methodologies. Our ﬁrst approach involves partitioning the data
from the training domains into a training dataset and a validation dataset, subsequently selecting the
model that demonstrates the highest average performance (accuracy) based on the validation data from
the training domain. This approach, referred to as training-domain validation set model selection
strategy , follows the research methodology established by Gulrajani & Lopez-Paz (2021). The second
method employs an Oracle-based model selection strategy using test domain data. The following shows
the model selection methodology when using validation in the training domain.
In the training phase of DomainBed datasets, we do not access the data in the test domain but split data from
the training domains into a training dataset and validation dataset. We choose the model with the highest
average performance (accuracy) on the validation data in the training domain. As a metric for evaluation, we
evaluated the generalization performance in the test domain as the OOD accuracy.
The Backgrounds Challenge uses Imagenet-1k as the training data set and selects models based on their
accuracy on the validation data in the training domain. After that, we measure the in-distribution performance
with IN9L (Xiao et al., 2021), which aggregates the test data into nine classes. As for the OOD performance,
we measure the classiﬁcation performance on the data where the background image of IN9L is replaced with
the background image of other images.
In CivilComments-WILDS, we divide the data into training, validation, and test datasets and maximize
worst-group accuracy in the validation data (and by association, maximize the average accuracy over all
domains). Then, we perform model selection and evaluate the OOD accuracy on the test data. We adopt the
same hyperparameter selection strategy for datasets as that for CivilComments-WILDS. As a metric, we do
not evaluate the worst-group performance but rather the 10th-percentile accuracy for the performance of
each domain by following the standard federated learning literature (Caldas et al., 2018).
4 Experiments
4.1 Experimental Overview
Our study aims to elucidate the inﬂuence of optimizer selection under distributional shifts. To that end,
we perform image classiﬁcation and NLP tasks and evaluate the trained model accuracy on the benchmark
datasets introduced in Section 3.2. By comparing the test accuracy for in-distribution samples with that
for OOD samples, we can observe how the solution found by each optimizer is robust to the distributional
shift. We investigate both ERM and IRMv1 (Arjovsky et al., 2019), a problem set to solve IRM, to clarify
the relationship between the problem formulation and optimizer selection in the OOD problem. For VREx
(Krueger et al., 2021) and CORAL (Sun & Saenko, 2016), small-scale experimental results are also provided
in Appendix I.7.
We compare ﬁve optimizers for all datasets except for the Backgrounds Challenge dataset. For the Backgrounds
Challenge dataset, we compare only Momentum SGD and Adam due to the computational cost. We describe
the conﬁgurations of hyperparameters and protocol for the experiments in further detail in Appendix E and
Appendix D respectively. The remaining experimental settings of environment are explained in Appendix B.
Hyperparameter Tuning: The hyperparameters are tuned using Bayes optimization functionality of
Weights&Biases2by evaluating in-distribution validation accuracy. Bayesian optimization sequentially
explored the potential hyperparameter candidate points, and we evaluated all the trained models in the
search process for comparison. As a conﬁrmation of the soundness of our hyperparameter search, Appendix
H.1 shows the histogram that the explored hyperparameters are drawn from the reasonably wide range. In
addition, we investigated the relationship between the number of trials to explore hyperparameters and the
2https://wandb.ai/site
6Published in Transactions on Machine Learning Research (06/2023)
Table 1: Comparison of the best OOD accuracy (%) of ERM between ﬁve optimizers. We use Oracle(see
deﬁnition at 3.1 in Gulrajani & Lopez-Paz (2021)) as model selection method in this table. The model
selection results in the training-domain validation set are shown in Table 13. Except for a small set of
problems, non-adaptive optimizer outperforms adaptive optimizer in all but 10 out of 12 tasks. As a soundness
check, we conﬁrm that our Adam results outperform all existing benchmark results using Adam. Details are
given in Appendix G.4.
Model OOD DatasetNon-Adaptive Optimizer Adaptive Optimizer
SGD Momentum Netsterov RMSProp Adam
4-Layer CNNColoredMNIST 34.01% 34.23% 40.56% 89.30% 73.92%
RotatedMNIST 90.00% 95.41% 94.06% 96.27% 96.40%
ResNet50VLCS 99.43% 99.43% 99.29% 99.29% 99.29%
PACS 88.67% 89.55% 89.25% 88.81% 89.30%
OﬃceHome 64.64% 65.01% 63.82% 62.91% 63.82%
TerraIncognita 63.21% 62.41% 62.85% 62.31% 61.35%
DomainNet 58.38% 61.91% 62.24% 55.74% 58.48%
BackgroundChallenge - 80.09% - - 77.90%
DistilBERTAmazon-WILDS 52.00% 54.66% 54.66% 53.33% 52.00%
CivilComment-WILDS 51.66% 57.69% 60.07% 45.39% 46.82%
ResNet-20 ColoredMNIST - 33.50% - - 31.47%
ViT PACS - 91.80% - - 91.26%
best OOD performance and show results in Appendix G.2. The impact of initialization strategies during
hyperparameter optimization is also conﬁrmed in Appendix H.3. The data shown in the box plot (Figure 2,
3, and 4) are from the evaluation of several trained models.
The number of epochs (the steps budget) used is in line with previous studies (Gulrajani & Lopez-Paz, 2021;
Koh et al., 2021; Choi et al., 2019) to ensure the soundness of our experimental design. Since we use the ﬁxed
epoch, it might seem to be unfair than when tuning the epoch as well for the optimizer that converges faster.
Thus, we studied the eﬀect of early stopping, which corresponds to the tuned epoch in Appendix G.4, and
the result conﬁrms that employing a ﬁxed epoch does not impair the fairness of our comparison experiments.
We discuss the details in Appendix G.4.
Boxplot: We believe that sharing the whole distribution of tuning outcomes is important because: it gives
an idea of how sensitive methods are to tuning and how much tuning eﬀort is required. We also share the
raw data as scatter plots (Figure 15, 16, and 16) in Appendix F.4.
4.2 Experimental Results
For training-domain validation set model selection, Figure 1 shows the relationship between the in-distribution
and OOD accuracy in the ERM setting. The x-axis of the plot is the in-distribution accuracy, and the y-axis
is the OOD accuracy. To clarify the trend, the in-distribution accuracy corresponding to the x-axis is divided
into ten bins, and the average performance of the OOD accuracy in each bin is shown on the y-axis. We
compared Momentum SGD, the best non-adaptive optimizer, and Adam, the best adaptive optimizer. Our
ﬁndings reveal that, in our ﬁeld of study, where high in-distribution performance is achieved, Momentum
SGD outperforms Adam on 11 out of 12 tasks (Table 13).
With respect to the Oracle model selection strategy, best OOD performance, non-adaptive optimizers
surpassed adaptive optimizer methods on 10 out of 12 tasks (Table 1). These results suggest that non-
adaptiveoptimizersaremoreadvantageousthanadaptiveoptimizersinOOD,despitetheirsimilarperformance
in the ID environment. For a more detailed explanation of the experimental results for each dataset, please
refer to the following section.
DomainBed: Figure 2 shows a box plot of the diﬀerence between the average in-distribution and OOD
accuracy for ERM. In the following discussion, we call this diﬀerence a gapfor convenience. Due to the
limitation of the paper length, only the plots of PACS and Oﬃce-Home are shown here. All results, including
IRM results, are shown in Appendix F.2.1.
7Published in Transactions on Machine Learning Research (06/2023)
0.0 0.5 1.0
in-distribution acc0.00.20.40.60.81.0out-of-distribution accColoredMNIST:ERM
momentum_sgd
adam
0.00 0.25 0.50 0.75 1.00
in-distribution acc0.00.20.40.60.81.0out-of-distribution accRotatedMNIST:ERM
0.00 0.25 0.50 0.75 1.00
in-distribution acc0.00.20.40.60.81.0out-of-distribution accVLCS:ERM
0.00 0.25 0.50 0.75 1.00
in-distribution acc0.00.20.40.60.81.0out-of-distribution accPACS:ERM
0.00 0.25 0.50 0.75 1.00
in-distribution acc0.00.20.40.60.81.0out-of-distribution accOfficeHome:ERM
0.00 0.25 0.50 0.75 1.00
in-distribution acc0.00.20.40.60.81.0out-of-distribution accT erraIncognita:ERM
0.00 0.25 0.50 0.75 1.00
in-distribution acc0.00.20.40.60.81.0out-of-distribution accDomainNet:ERM
0.00 0.25 0.50 0.75 1.00
in-distribution acc0.00.20.40.60.81.0out-of-distribution accBackground Challenge
0.00 0.25 0.50 0.75 1.00
in-distribution acc0.00.20.40.60.81.0out-of-distribution accWILDS_Amazon:ERM
0.00 0.25 0.50 0.75 1.00
in-distribution acc0.00.20.40.60.81.0out-of-distribution accWILDS_civilcomments:ERM
Figure 1. Comparison of the in-distribution accuracy and the OOD accuracy of ERM between Momentum
SGD and Adam. We also show the training results in the exhaustive hyperparameter search range as a scatter
plot in Appendix F.3. In these ten plots, for each dataset, the in-distribution performance is separated by
every ten bins 0.1. The average OOD performance when evaluating the checkpoints in that bin is shown on
the vertical axis. We compare which optimizer shows better OOD performance for each model that achieves
equivalent in-distribution performance. This approach can be characterized as a model selection strategy,
which follows the methodology of utilizing a training-domain validation set . In most cases, momentum
SGD outperforms Adam in OOD performance in the rightmost region of our interest (the region where high
in-distribution performance is achieved).
We found two distinct optimizer eﬀect patterns, depending on the dataset: i) PACS, Oﬃce-Home, VLCS,
Terra Incognita, Rotated MNIST, and DomainNet, and ii) Colored MNIST. Because these patterns appear in
both the results of ERM and IRM, we focus on ERM for the explanation.
For PACS, Oﬃce Home, VLCS, TerraIncognita, RotatedMNIST, and DomainNet, the OOD accuracy is
greater for non-adaptive optimizers. The gap between the mean OOD accuracy and the mean in-distribution
accuracy was smaller for the non-adaptive optimizer except for TerraIncognita. This means the models
trained with the non-adaptive optimizer are more robust on average. In TerraIncognita, the non-adaptive
optimizer signiﬁcantly outperforms the adaptive optimizer on the average in-distribution performance and
OOD performance. We note, however, that except in this problem, the adaptive optimizer achieves a
smaller gap between the two. As shown in Figure 1, when comparing models with the same in-distribution
performance, the non-adaptive optimizer showed higher OOD accuracy than the adaptive optimizer. The
results in Figures 1 and 2 conﬁrm that the non-adaptive method achieves higher OOD accuracy than the
adaptive method, both on average and for models with the same in-distribution performance.
Colored MNIST shows the opposite pattern of these results, where the adaptive optimizer is better than
the non-adaptive optimizers. As outlined in Section 3.2, Colored MNIST diﬀers from the other datasets. To
comprehend why adaptive optimizers are more eﬀective in OOD generalization for this dataset, we have
plotted the average training, validation, and test accuracy over time during training, as illustrated in Figure
33. Our explanation for this outcome can be found in Appendix G.3.
We note our considerations regarding the exceptional behavior of ColoredMNIST. ColoredMNIST is a binary
classiﬁcation task dataset with random labels, where spurious features are positively correlated with invariant
features in in-distribution and negatively correlated with OOD. Non-adaptive optimizers learn the spurious
features, achieve the oracle performance for in-distribution and perform worse than a random guess for OOD
due to inverted correlations. Adaptive optimizers, in contrast, seem to overﬁt the training data, achieving
100% accuracy in the training set (more than oracle3) in this dataset, and failing to learn the spurious
3Since ColoredMNIST includes label ﬂip as noise, even the best model that correctly learns the data rules will only perform
85%.
8Published in Transactions on Machine Learning Research (06/2023)
SGD Momentum Nesterov RMSProp Adam
optimizer_name0.00.20.40.60.81.0out-of-distribution acc
PACS: ERM
SGD Momentum Nesterov RMSProp Adam
optimizer_name0.00.20.40.60.81.0in-distribution acc
PACS: ERM
SGD Momentum Nesterov RMSProp Adam
optimizer_name0.00.20.40.60.81.0gap
PACS: ERM
SGD Momentum Nesterov RMSProp Adam
optimizer_name0.00.20.40.60.81.0out-of-distribution acc
OfficeHome: ERM
SGD Momentum Nesterov RMSProp Adam
optimizer_name0.00.20.40.60.81.0in-distribution acc
OfficeHome: ERM
SGD Momentum Nesterov RMSProp Adam
optimizer_name0.00.20.40.60.81.0gap
OfficeHome: ERM
Figure 2. PACS and OﬃceHome in DomainBed: Comparison of the in-distribution (validation) accuracy and
the out-of-distribution (test) accuracy of ERM across ﬁve optimizers. Non-adaptive optimizers outperform
the adaptive optimizers in terms of OOD generalization, and the gap between in-distribution performance and
OOD performance is small. The details and results of other dataset experiments are described in Appendix
F.2.
Momentum Adam
optimizer_name0.00.20.40.60.81.0out-of-distribution acc
Background Challenge
Momentum Adam
optimizer_name0.00.20.40.60.81.0in-distribution acc
Background Challenge
Momentum Adam
optimizer_name0.00.20.40.60.81.0gap
Background Challenge
Figure 3. Backgrounds Challenge: Comparison of the in-distribution (validation) accuracy and the out-of-
distribution (test) accuracy of ERM across two optimizers. In terms of OOD generalization, Momentum
SGD outperforms Adam in both average and best OOD performance. The highest value of in-distribution
accuracy remains the same, but Momentum SGD shows higher performance on average.
features. Due the aforementioned (synthetic) inverted correlations in the dataset, this overﬁtting behaviour
in-distribution happens to favor adaptive optimizers. This behavior happens exceptionally on ColoredMNIST
due to these synthetic ﬂips in correlation. This exploitation unexpectedly enables Adam to avert being
trapped in the training domain and produces better OOD generalization.
Backgrounds Challenge: Backgrounds Challenge requires training of ImageNet-1k on ResNet50 from
scratch, and it takes 256 GPU hours to obtain a single trained model, so we only compared Momentum SGD
with Adam. Because Momentum SGD achieved competitive performance among non-adaptive optimization
methods in the DomainBed and WILDS experiments, we adopted Momentum SGD to represent non-adaptive
optimization methods. In the same way, Adam outperformed RMSProp in almost all the benchmarks, so we
adopted Adam as a representative of the adaptive optimizers.
Figure 3 compares the accuracy for ORIGINAL (in-distribution) and Mixed-Rand (out-of-distribution). The
best in-distribution performance is the same for each optimizer, but concerning the best OOD performance,
9Published in Transactions on Machine Learning Research (06/2023)
SGD Momentum Nesterov RMSProp Adam
optimizer_name0.00.20.40.60.81.0out-of-distribution acc
WILDS_amazon: ERM
SGD Momentum Nesterov RMSProp Adam
optimizer_name0.00.20.40.60.81.0in-distribution acc
WILDS_amazon: ERM
SGD Momentum Nesterov RMSProp Adam
optimizer_name0.00.20.40.60.81.0gap
WILDS_amazon: ERM
SGD Momentum Nesterov RMSProp Adam
optimizer_name0.00.20.40.60.81.0out-of-distribution acc
WILDS_civilcomments: ERM
SGD Momentum Nesterov RMSProp Adam
optimizer_name0.00.20.40.60.81.0in-distribution acc
WILDS_civilcomments: ERM
SGD Momentum Nesterov RMSProp Adam
optimizer_name0.00.20.40.60.81.0gap
WILDS_civilcomments: ERM
Figure 4. and CivilComments-WILDS: Comparison of the in-distribution (validation) accuracy and the
out-of-distribution (test) accuracy of ERM across ﬁve optimizers. Both non-adaptive and adaptive optimizers
have similar in-distribution accuracy, but the adaptive method signiﬁcantly degrades the performance of the
OOD generalization.
the non-adaptive optimizer outperformed the adaptive optimizer. As can be seen from Figure 1 (second
row, middle column), particularly in the region of high in-distribution performance on the right, the OOD
performance of Momentum SGD exceeds that of Adam.
WILDS: A comparison of in-distribution and OOD averages for WILDS is shown in Figure 4. It can be
clearly seen that the adaptive optimizer is ﬁt too well to the in-distribution in the WILDS problem setting.
For and CivilComment-WILDS, as in DomainBed and Backgrounds Challenge, the non-adaptive optimizer
outperforms the adaptive optimizer in terms of OOD generalization. The gap between in-distribution
accuracy and OOD accuracy is also tiny for non-adaptive optimizers. In particular, the CivilComments-
WILDS experimental result is remarkable, as both non-adaptive and adaptive optimizers show similar high
in-distribution performance, but in the OOD environment, adaptive methods signiﬁcantly fail to make
inferences.
4.3 Correlation Behaviors
Our results show that three typical types of behavior are observed in terms of the correlation between
in-distribution performance and OOD performance for diﬀerent datasets. Detailed results of the experiments
on all datasets are shown in Appendix F.4. We follow Miller et al. (2021), who used a probit transform to
show the relationship. The three types are increasing return, linear return, and diminishing return. These
show how much performance in OOD can be expected if we increase the in-distribution performance.
The increasing return is an example, as shown in the leftmost part of Figure 5. The increasing returns in
large regions of the in-distribution generalization signiﬁcantly aﬀect the OOD generalization. This suggests
that the last tuning is signiﬁcant for the OOD generalization, as seen in all domain generalization datasets
except for DomainNet.
The linear return is as shown in the middle of Figure 5. The OOD accuracy increases linearly with the
in-distribution accuracy. This is generally the same result for in-distribution validation and test.
Conversely, diminishing return behavior, illustrated in the rightmost part of Figure 5, indicates that substantial
in-distribution improvement leads to a saturation of OOD generalization with only a marginal eﬀect. This
10Published in Transactions on Machine Learning Research (06/2023)
observation implies that the eﬀort invested in ﬁne-tuning might not always yield signiﬁcant enhancements
in OOD generalization. We have observed similar trends in settings with subpopulation shifts, such as
CivilComments-WILDS.
In Appendix F.4, we present our experimental results without a probit transform, conﬁrming that diminishing
returns are not necessarily linear before probit transformation. This ﬁnding aligns with recent studies by
Wenzel et al. (2022); Teney et al. (2022), stating that the accuracy of ID and OOD varies across datasets.
Moreover, as Baek et al. (2022) suggests, the occurrence of linear return depends on the problem set. Our
results support these claims, providing a comprehensive analysis of datasets and problem sets that Miller
et al. (2021) did not address and uncovering trends they did not reveal.
For practitioners, our ﬁndings oﬀer valuable insights into adjusting their expectations and strategies based on
the observed behavior. For instance, if they work with a dataset similar to CivilComments-WILDS, they can
anticipate one of the identiﬁed types of behavior. Should their dataset exhibit saturating behavior, they can
adjust their expectations accordingly; conversely, if their dataset demonstrates a regime where every slight
improvement in in-distribution accuracy aids, they should pursue enhanced in-distribution optimization.
5 Discussion and Conclusion
We conduct an exhaustive empirical comparison of the generalization performance of various optimizers under
diﬀerent practical distributional shifts. Notably, ten state-of-the-art OOD datasets were used to study the
environment of broad shifts in correlation and diversity shifts. The investigation elucidates how optimizer
selection aﬀects OOD generalization. As the main claim, the answer to our research objective is that the
non-adaptive optimizer is superior to the adaptive optimizer in terms of OOD generalization.
The following evidence supports this: i) when comparing in-distribution accuracy is the same, OOD accuracy
of non-adaptive optimizers is greater than that of adaptive optimizers (Figure 1); ii) on average and top
performance, the OOD accuracy of the non-adaptive optimizer is higher (Figures 2, 3, and 4). All these
points support our main claim that non-adaptive optimizers are superior in OOD generalization within our
exhaustive experiments. We have tuned Adam to the range that it is used in practice and have updated
Adam’s scores on all OOD datasets, which use Adam optimizer as default for benchmarks. This supports the
soundness of our experiments.
Overall, we can conclude that optimizer selection inﬂuences OOD generalization in the cases we are interested
in. Future research should consider the algorithm or loss function and optimizers in the OOD problem.
The results of IRM show a trend similar to ERM’s, but a more detailed analysis is needed to consider the
diﬀerences in loss landscapes. All these points support our main claim that non-adaptive optimizers are
superior in OOD generalization within our exhaustive experiments.
0.10 0.25 0.50 0.70 0.80 0.90 0.950.100.250.500.700.800.90 SGD
Momentum SGD
Nesterov Momentum SGD
RMSProp
Adam
PACS:ERM (probit scaling)
in-distribution accout-of-distribution acc
0.10 0.25 0.500.100.250.500.70
SGD
Momentum SGD
Nesterov Momentum SGD
RMSProp
Adam
DomainNet:ERM (probit scaling)
in-distribution accout-of-distribution acc
0.10 0.25 0.50 0.70 0.80 0.900.100.250.50
SGD
Momentum SGD
Nesterov Momentum SGD
RMSProp
Adam
WILDS_civilcomments:IRM (probit scaling)
in-distribution accout-of-distribution acc
Figure 5. Three types of correlation behavior: increasing return (PACS), linear return (DomainNet), and
diminishing return (CivilComments-WILDS). The legend circles on the right side of each ﬁgure show, in
order, the SGD, Momentum SGD, Nesterov Momentum, RMProp, and Adam. The details and results of
other dataset experiments are described in Appendixes 15, 16, and 16
.
11Published in Transactions on Machine Learning Research (06/2023)
Finally, we would like to mention the limitations of our work. One limitation is that we did not study recently
proposed and less popular optimizers.
The choice of optimizers we study is in line with previous work (Choi et al., 2019), (Schmidt et al., 2021)
on optimizer comparison and with most recent OOD work; those studies overwhelmingly focused on Adam
rather than e.g., AdamW (Loshchilov & Hutter, 2017). Similarly, other less popular optimizers such as SWA
(Izmailov et al., 2018), SWAD (Cha et al., 2021), SAM (Foret et al., 2020), have been omitted to allow for a
more extensive study of the chosen methods4.
Another limitation is that we employed a total of six models used in the DomainBed (ConvNet, ResNet20,
ResNet50 and Vision Transformer (Dosovitskiy et al., 2020)), Backgrounds Challenge (ResNet50), and WILDS
(DistilBERT (Sanh et al., 2019)) benchmarks.
Acknowledgments
Our deepest gratitude goes out to the anonymous reviewers and the Action Editor whose invaluable insights
substantially enhanced the quality of this manuscript. We express our heartfelt thanks to Kilian Fatras
(Mila, McGill), Divyat Mahajan (Mila, UdeM), and Mehrnaz Mofakhami (Mila, UdeM) for their constructive
feedback that signiﬁcantly contributed to the research process. Our sincere appreciation extends to the
Masason Foundation Fellowship, which generously supported this work and made an award to Hiroki
Naganuma.
The computational resources instrumental to this study were provided under the auspices of the "ABCI
Grand Challenge" Program, National Institute of Advanced Industrial Science and Technology (AIST),
and the TSUBAME Grand Challenge Program, Tokyo Institute of Technology. Special thanks to the AI
Bridging Cloud Infrastructure (ABCI) and the TSUBAME 3.0. Moreover, we acknowledge the generous
allocation of computational resources from the TSUBAME3.0 supercomputer, facilitated by Tokyo Institute
of Technology. This assistance came through the Exploratory Joint Research Project Support Program from
JHPCN (EX23401) and the TSUBAME Encouragement Program for Young / Female Users, whose support
was instrumental to this research.
References
Vahdat Abdelzad, Krzysztof Czarnecki, and Rick Salay. The eﬀect of optimization methods on the robustness
of out-of-distribution detection approaches. arXiv preprint arXiv:2006.14584 , 2020.
Shunichi Amari. Natural gradient works eﬃciently in learning. Neural computation , 10(2):251–276, 1998.
Martin Arjovsky, Léon Bottou, Ishaan Gulrajani, and David Lopez-Paz. Invariant risk minimization. arXiv
preprint arXiv:1907.02893 , 2019.
Christina Baek, Yiding Jiang, Aditi Raghunathan, and Zico Kolter. Agreement-on-the-line: Predicting the
performance of neural networks under distribution shift. arXiv preprint arXiv:2206.13089 , 2022.
Lukas Balles and Philipp Hennig. Dissecting adam: The sign, magnitude and variance of stochastic gradients.
InInternational Conference on Machine Learning , pp. 404–413. PMLR, 2018.
Peter Bandi, Oscar Geessink, Quirine Manson, Marcory Van Dijk, Maschenka Balkenhol, Meyke Hermsen,
Babak Ehteshami Bejnordi, Byungjae Lee, Kyunghyun Paeng, Aoxiao Zhong, et al. From detection of
individual metastases to classiﬁcation of lymph node status at the patient level: the camelyon17 challenge.
IEEE Transactions on Medical Imaging , 2018.
Sara Beery, Grant Van Horn, and Pietro Perona. Recognition in terra incognita. In Proceedings of the
European Conference on Computer Vision (ECCV) , pp. 456–473, 2018.
Sara Beery, Elijah Cole, and Arvi Gjoka. The iwildcam 2020 competition dataset. arXiv preprint
arXiv:2004.10340 , 2020.
4In some of the limited tasks, SAM and AdamW were also evaluated, the results of which are presented in Appendix I.
12Published in Transactions on Machine Learning Research (06/2023)
Shai Ben-David, John Blitzer, Koby Crammer, Alex Kulesza, Fernando Pereira, and Jennifer Wortman
Vaughan. A theory of learning from diﬀerent domains. Machine learning , 79(1):151–175, 2010.
Daniel Borkan, Lucas Dixon, Jeﬀrey Sorensen, Nithum Thain, and Lucy Vasserman. Nuanced metrics for
measuring unintended bias with real data for text classiﬁcation. In Companion Proceedings of The 2019
World Wide Web Conference , 2019.
Sebastian Caldas, Sai Meher Karthik Duddu, Peter Wu, Tian Li, Jakub Konečn` y, H Brendan McMa-
han, Virginia Smith, and Ameet Talwalkar. Leaf: A benchmark for federated settings. arXiv preprint
arXiv:1812.01097 , 2018.
Junbum Cha, Sanghyuk Chun, Kyungjae Lee, Han-Cheol Cho, Seunghyun Park, Yunsung Lee, and Sungrae
Park. Swad: Domain generalization by seeking ﬂat minima. Advances in Neural Information Processing
Systems, 34, 2021.
Yongqiang Chen, Kaiwen Zhou, Yatao Bian, Binghui Xie, Bingzhe Wu, Yonggang Zhang, MA KAILI, Han
Yang, Peilin Zhao, Bo Han, and James Cheng. Pareto invariant risk minimization: Towards mitigating the
optimization dilemma in out-of-distribution generalization. In The Eleventh International Conference on
Learning Representations , 2023. URL https://openreview.net/forum?id=esFxSb_0pSL .
Dami Choi, Christopher J Shallue, Zachary Nado, Jaehoon Lee, Chris J Maddison, and George E Dahl. On
empirical comparisons of optimizers for deep learning. arXiv preprint arXiv:1910.05446 , 2019.
Myung Jin Choi, Joseph J Lim, Antonio Torralba, and Alan S Willsky. Exploiting hierarchical context on a
large database of object categories. In 2010 IEEE computer society conference on computer vision and
pattern recognition , pp. 129–136. IEEE, 2010.
Gordon Christie, Neil Fendley, James Wilson, and Ryan Mukherjee. Functional map of the world. In
Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , 2018.
Etienne David, Simon Madec, Pouria Sadeghi-Tehran, Helge Aasen, Bangyou Zheng, Shouyang Liu, Norbert
Kirchgessner, Goro Ishikawa, Koichi Nagasawa, Minhajul A Badhon, Curtis Pozniak, Benoit de Solan,
Andreas Hund, Scott C. Chapman, Frederic Baret, Ian Stavness, and Wei Guo. Global wheat head detection
(gwhd) dataset: a large and diverse dataset of high-resolution rgb-labelled images to develop and benchmark
wheat head detection methods. Plant Phenomics , 2020, 2020.
Etienne David, Mario Serouart, Daniel Smith, Simon Madec, Kaaviya Velumani, Shouyang Liu, Xu Wang,
Francisco Pinto Espinosa, Shahameh Shaﬁee, Izzat S. A. Tahir, Hisashi Tsujimoto, Shuhei Nasuda, Bangyou
Zheng, Norbert Kichgessner, Helge Aasen, Andreas Hund, Pouria Sadhegi-Tehran, Koichi Nagasawa, Goro
Ishikawa, Sebastien Dandrifosse, Alexis Carlier, Benoit Mercatoris, Ken Kuroki, Haozhou Wang, Masanori
Ishii, Minhajul A. Badhon, Curtis Pozniak, David Shaner LeBauer, Morten Lilimo, Jesse Poland, Scott
Chapman, Benoit de Solan, Frederic Baret, Ian Stavness, and Wei Guo. Global wheat head dataset 2021:
an update to improve the benchmarking wheat head localization with more diversity, 2021.
AlexeyDosovitskiy, LucasBeyer, AlexanderKolesnikov, DirkWeissenborn, XiaohuaZhai, ThomasUnterthiner,
Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words:
Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929 , 2020.
Mark Everingham, Luc Van Gool, Christopher KI Williams, John Winn, and Andrew Zisserman. The pascal
visual object classes (voc) challenge. International journal of computer vision , 88(2):303–338, 2010.
Chen Fang, Ye Xu, and Daniel N Rockmore. Unbiased metric learning: On the utilization of multiple datasets
and web images for softening bias. In Proceedings of the IEEE International Conference on Computer
Vision, pp. 1657–1664, 2013.
Li Fei-Fei, Rob Fergus, and Pietro Perona. Learning generative visual models from few training examples:
An incremental bayesian approach tested on 101 object categories. In 2004 conference on computer vision
and pattern recognition workshop , pp. 178–178. IEEE, 2004.
13Published in Transactions on Machine Learning Research (06/2023)
Pierre Foret, Ariel Kleiner, Hossein Mobahi, and Behnam Neyshabur. Sharpness-aware minimization for
eﬃciently improving generalization. arXiv preprint arXiv:2010.01412 , 2020.
Robert Geirhos, Jörn-Henrik Jacobsen, Claudio Michaelis, Richard Zemel, Wieland Brendel, Matthias Bethge,
and Felix A Wichmann. Shortcut learning in deep neural networks. Nature Machine Intelligence , 2(11):
665–673, 2020.
Muhammad Ghifary, W Bastiaan Kleijn, Mengjie Zhang, and David Balduzzi. Domain generalization for
object recognition with multi-task autoencoders. In Proceedings of the IEEE international conference on
computer vision , pp. 2551–2559, 2015.
Ishaan Gulrajani and David Lopez-Paz. In search of lost domain generalization. In International Conference
on Learning Representations , 2021.
Dan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common corruptions
and perturbations. In International Conference on Learning Representations , 2019.
Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele Catasta, and
Jure Leskovec. Open graph benchmark: Datasets for machine learning on graphs. In Advances in Neural
Information Processing Systems (NeurIPS) , 2020.
Pavel Izmailov, Dmitrii Podoprikhin, Timur Garipov, Dmitry Vetrov, and Andrew Gordon Wilson. Averaging
weights leads to wider optima and better generalization. arXiv preprint arXiv:1803.05407 , 2018.
Marc Khoury. Adaptive versus standard descent methods and robustness against adversarial examples. arXiv
preprint arXiv:1911.03784 , 2019.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International Conference
on Learning Representations , 2015.
Pang Wei Koh, Shiori Sagawa, Henrik Marklund, Sang Michael Xie, Marvin Zhang, Akshay Balsubramani,
Weihua Hu, Michihiro Yasunaga, Richard Lanas Phillips, Irena Gao, et al. Wilds: A benchmark of
in-the-wild distribution shifts. In International Conference on Machine Learning , pp. 5637–5664. PMLR,
2021.
David Krueger, Ethan Caballero, Joern-Henrik Jacobsen, Amy Zhang, Jonathan Binas, Dinghuai Zhang,
Remi Le Priol, and Aaron Courville. Out-of-distribution generalization via risk extrapolation (rex). In
International Conference on Machine Learning , pp. 5815–5826. PMLR, 2021.
Ananya Kumar, Aditi Raghunathan, Robbie Matthew Jones, Tengyu Ma, and Percy Liang. Fine-tuning can
distort pretrained features and underperform out-of-distribution. In International Conference on Learning
Representations , 2022a. URL https://openreview.net/forum?id=UYneFzXSJWh .
Ananya Kumar, Ruoqi Shen, Sébastien Bubeck, and Suriya Gunasekar. How to ﬁne-tune vision models with
sgd.arXiv preprint arXiv:2211.09359 , 2022b.
Da Li, Yongxin Yang, Yi-Zhe Song, and Timothy M Hospedales. Deeper, broader and artier domain
generalization. In Proceedings of the IEEE international conference on computer vision , pp. 5542–5550,
2017.
Liyuan Liu, Haoming Jiang, Pengcheng He, Weizhu Chen, Xiaodong Liu, Jianfeng Gao, and Jiawei Han.
On the variance of the adaptive learning rate and beyond. In International Conference on Learning
Representations , 2020.
Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101 ,
2017.
Shuai Lu, Daya Guo, Shuo Ren, Junjie Huang, Alexey Svyatkovskiy, Ambrosio Blanco, Colin Clement,
Dawn Drain, Daxin Jiang, Duyu Tang, et al. Codexglue: A machine learning benchmark dataset for code
understanding and generation. arXiv preprint arXiv:2102.04664 , 2021.
14Published in Transactions on Machine Learning Research (06/2023)
Luke Metz, Niru Maheswaranathan, C Daniel Freeman, Ben Poole, and Jascha Sohl-Dickstein. Tasks, stability,
architecture, and compute: Training more eﬀective learned optimizers, and using them to train themselves.
arXiv preprint arXiv:2009.11243 , 2020.
John P Miller, Rohan Taori, Aditi Raghunathan, Shiori Sagawa, Pang Wei Koh, Vaishaal Shankar, Percy
Liang, Yair Carmon, and Ludwig Schmidt. Accuracy on the line: on the strong correlation between
out-of-distribution and in-distribution generalization. In International Conference on Machine Learning ,
pp. 7721–7735. PMLR, 2021.
Norman Mu and Justin Gilmer. Mnist-c: A robustness benchmark for computer vision. arXiv preprint
arXiv:1906.02337 , 2019.
Vaishnavh Nagarajan, Anders Andreassen, and Behnam Neyshabur. Understanding the failure modes of
out-of-distribution generalization. In International Conference on Learning Representations , 2021.
Yurii Nesterov. Introductory lectures on convex optimization: A basic course , volume 87. Springer Science &
Business Media, 2003.
Jianmo Ni, Jiacheng Li, and Julian McAuley. Justifying recommendations using distantly-labeled reviews and
ﬁne-grained aspects. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language
Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP) ,
2019.
Sinno Jialin Pan and Qiang Yang. A survey on transfer learning. IEEE Transactions on knowledge and data
engineering , 22(10):1345–1359, 2009.
Xingchao Peng, Qinxun Bai, Xide Xia, Zijun Huang, Kate Saenko, and Bo Wang. Moment matching for
multi-source domain adaptation. In Proceedings of the IEEE/CVF International Conference on Computer
Vision, pp. 1406–1415, 2019.
Boris T Polyak. Some methods of speeding up the convergence of iteration methods. Ussr computational
mathematics and mathematical physics , 4(5):1–17, 1964.
Veselin Raychev, Pavol Bielik, and Martin Vechev. Probabilistic model for code with decision trees. ACM
SIGPLAN Notices , 2016.
Sashank J. Reddi, Satyen Kale, and Sanjiv Kumar. On the convergence of adam and beyond. In International
Conference on Learning Representations , 2018.
Bryan C Russell, Antonio Torralba, Kevin P Murphy, and William T Freeman. Labelme: a database and
web-based tool for image annotation. International journal of computer vision , 77(1-3):157–173, 2008.
Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. Distilbert, a distilled version of bert:
smaller, faster, cheaper and lighter. arXiv preprint arXiv:1910.01108 , 2019.
Robin M Schmidt, Frank Schneider, and Philipp Hennig. Descending through a crowded valley-benchmarking
deep learning optimizers. In International Conference on Machine Learning , pp. 9367–9376. PMLR, 2021.
Frank Schneider, Lukas Balles, and Philipp Hennig. DeepOBS: A deep learning optimizer benchmark suite.
InInternational Conference on Learning Representations , 2019.
Zheyan Shen, Jiashuo Liu, Yue He, Xingxuan Zhang, Renzhe Xu, Han Yu, and Peng Cui. Towards out-of-
distribution generalization: A survey. arXiv preprint arXiv:2108.13624 , 2021.
Masashi Sugiyama and Motoaki Kawanabe. Machine learning in non-stationary environments: Introduction
to covariate shift adaptation . MIT press, 2012.
Baochen Sun and Kate Saenko. Deep coral: Correlation alignment for deep domain adaptation. In Computer
Vision–ECCV 2016 Workshops: Amsterdam, The Netherlands, October 8-10 and 15-16, 2016, Proceedings,
Part III 14 , pp. 443–450. Springer, 2016.
15Published in Transactions on Machine Learning Research (06/2023)
Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob
Fergus. Intriguing properties of neural networks. In International Conference on Learning Representations ,
2014.
J. Taylor, B. Earnshaw, B. Mabey, M. Victors, and J. Yosinski. Rxrx1: An image set for cellular morphological
variation across many experimental batches. In International Conference on Learning Representations
(ICLR), 2019.
Damien Teney, Yong Lin, Seong Joon Oh, and Ehsan Abbasnejad. Id and ood performance are sometimes
inversely correlated on real-world datasets, 2022. URL https://arxiv.org/abs/2209.00613 .
Tijmen Tieleman and Geoﬀrey Hinton. Lecture 6.5-rmsprop: Divide the gradient by a running average of its
recent magnitude. COURSERA: Neural networks for machine learning , 4(2):26–31, 2012.
Hemanth Venkateswara, Jose Eusebio, Shayok Chakraborty, and Sethuraman Panchanathan. Deep hashing
network for unsupervised domain adaptation. In Proceedings of the IEEE conference on computer vision
and pattern recognition , pp. 5018–5027, 2017.
Neha S Wadia, Daniel Duckworth, Samuel S Schoenholz, Ethan Dyer, and Jascha Sohl-Dickstein. Whitening
and second order optimization both destroy information about the dataset, and can make generalization
impossible. arXiv preprint arXiv:2008.07545 , 2020.
Yixiang Wang, Jiqiang Liu, Jelena Mišić, Vojislav B Mišić, Shaohua Lv, and Xiaolin Chang. Assessing
optimizer impact on dnn model sensitivity to adversarial examples. IEEE Access , 7:152766–152776, 2019.
Florian Wenzel, Andrea Dittadi, Peter Vincent Gehler, Carl-Johann Simon-Gabriel, Max Horn, Dominik
Zietlow, David Kernert, Chris Russell, Thomas Brox, Bernt Schiele, et al. Assaying out-of-distribution
generalization in transfer learning. arXiv preprint arXiv:2207.09239 , 2022.
Ashia C Wilson, Rebecca Roelofs, Mitchell Stern, Nati Srebro, and Benjamin Recht. The marginal value
of adaptive gradient methods in machine learning. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach,
R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Advances in Neural Information Processing Systems ,
2017.
Mitchell Wortsman, Gabriel Ilharco, Jong Wook Kim, Mike Li, Simon Kornblith, Rebecca Roelofs,
Raphael Gontijo Lopes, Hannaneh Hajishirzi, Ali Farhadi, Hongseok Namkoong, et al. Robust ﬁne-
tuning of zero-shot models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pp. 7959–7971, 2022.
Kai Yuanqing Xiao, Logan Engstrom, Andrew Ilyas, and Aleksander Madry. Noise or signal: The role of
image backgrounds in object recognition. In International Conference on Learning Representations , 2021.
Christopher Yeh, Anthony Perez, Anne Driscoll, George Azzari, Zhongyi Tang, David Lobell, Stefano Ermon,
and Marshall Burke. Using publicly available satellite imagery and deep learning to understand economic
well-being in africa. Nature Communications , 2020.
Guodong Zhang, Lala Li, Zachary Nado, James Martens, Sushant Sachdeva, George E Dahl, Christopher J
Shallue, and Roger Grosse. Which algorithmic choices matter at which batch sizes? insights from a noisy
quadratic model. arXiv preprint arXiv:1907.04164 , 2019.
Difan Zou, Yuan Cao, Yuanzhi Li, and Quanquan Gu. Understanding the generalization of adam in
learning neural networks with proper regularization, 2022. URL https://openreview.net/forum?id=
G7PfyLimZBp .
16Published in Transactions on Machine Learning Research (06/2023)
Contents (Appendix)
A Optimizers 19
A.1 SGD . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19
A.2 Momentum-SGD . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19
A.3 Nesterov Momentum . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19
A.4 RMSprop . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19
A.5 Adam . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19
B Implementation and Environment for Experiments 19
C Datasets 20
C.1 DomainBed . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20
C.2 Backgrounds Challenge Dataset . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20
C.3 Amazon-WILDS and CivilComments-WILDS Dataset . . . . . . . . . . . . . . . . . . . . . . 21
C.4 CIFAR10-C and CIFAR10-P Dataset . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21
D Experimental Protocol 22
D.1 DomainBed . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22
D.2 Backgrounds Challenge Dataset . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22
D.3 WILDS . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23
D.4 CIFAR10-C and CIFAR10-P . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23
E Hyperparameters and Detailed Conﬁgurations 24
E.1 DomainBed . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24
E.2 Backgrounds Challenge Dataset . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26
E.3 WILDS . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26
E.4 CIFAR10-C and CIFAR10-P . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27
F Full Results of Experiments 28
F.1 Full Results of Table . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28
F.2 Full Results of Boxplot . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29
F.2.1 Full Results of Filtered Boxplot (ERM) . . . . . . . . . . . . . . . . . . . . . . . . . . 29
F.2.2 Full Results of Filtered Boxplot (IRM) . . . . . . . . . . . . . . . . . . . . . . . . . . . 33
F.3 Full Results of Bin-Diagram Plots . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35
F.4 Full Results of Scatter Plots . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36
F.4.1 ERM . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36
F.4.2 IRM . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37
G Ablation Study 39
G.1 Probit Transformed Scatter Plot . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39
G.1.1 ERM . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39
G.1.2 IRM . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42
G.2 Model Performance Transition throught Hyperparameter Search . . . . . . . . . . . . . . . . 44
G.2.1 Hyperparameter Trial Budget vs OOD Accuracy . . . . . . . . . . . . . . . . . . . . . 44
G.2.2 Hyperparameter Trial Budget vs OOD Error . . . . . . . . . . . . . . . . . . . . . . . 46
G.3 Learning Curve of ColoredMNIST . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 50
G.4 Early Stopping . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51
H Soundness Check of Our Experiments 61
H.1 Histgram of Hyperparameters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61
H.2 Hyperparameters and OOD Accuracy Box-Plot . . . . . . . . . . . . . . . . . . . . . . . . . . 61
H.3 Eﬀect of Initial Conﬁguration on Hyperparameter Optimization . . . . . . . . . . . . . . . . . 64
H.4 Best OOD Performance Comparison against with Existing Benckmark . . . . . . . . . . . . . 66
17Published in Transactions on Machine Learning Research (06/2023)
I Additional Study 67
I.1 Corruption and Perturbation Shift . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 67
I.2 Model Architecture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 67
I.2.1 ResNet-20 for ColoredMNIST . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 67
I.2.2 Vision Transformer for PACS . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 67
I.3 State-of-the-Arts Optimizers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 68
I.3.1 Sharpness Aware Minimization (SAM) . . . . . . . . . . . . . . . . . . . . . . . . . . . 68
I.3.2 Adam with Decoupled Weight Decay (AdamW) . . . . . . . . . . . . . . . . . . . . . . 68
I.4 Large /epsilon1for Adam . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 69
I.5 Learning Rate Schedule . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 69
I.6 The Eﬀect of Random Seeds . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 70
I.7 Algorithms (ERM, IRM, VREx and CORAL) . . . . . . . . . . . . . . . . . . . . . . . . . . . 71
18Published in Transactions on Machine Learning Research (06/2023)
A Optimizers
As we explained in Section 4.1, we compared SGD, momentum-SGD, Nesterov momentum, RMSprop, and
Adam. We write the algorithm of these optimizers below.
A.1 SGD
θt←θt−1−ηt˜∇θt−1/lscript(θt−1) (2)
A.2 Momentum-SGD
vt←γvt−1+ηt˜∇θt−1/lscript(θt−1) (3)
θt←θt−1−vt (4)
A.3 Nesterov Momentum
vt←γvt−1+ηt˜∇θt−1/lscript(θt−1−γvt−1) (5)
θt←θt−1−vt (6)
A.4 RMSprop
vt←αvt−1+ (1−α)˜∇θt−1/lscript(θt)2(7)
mt←γmt−1+ηt√vt+/epsilon1˜∇θt−1/lscript(θt) (8)
θt←θt−1−mt (9)
A.5 Adam
mt←β1mt−1+ (1−β1)˜∇θt−1/lscript(θt) (10)
vt←β2vt−1+ (1−β2)˜∇θt−1/lscript(θt)2(11)
bt←/radicalbig
1−βt
2
1−βt
1(12)
θt←θt−1−ηtmt√vt+/epsilon1bt. (13)
B Implementation and Environment for Experiments
We perform our experiment with ABCI (AI Bridging Cloud Infrastructure), a supercomputer owned by the
National Institute of Advanced Industrial Science and Technology, and TSUBAME, a supercomputer owned
by the Tokyo Institute of Technology.
All codes for experiments are modiﬁcations of the codes provided by the authors who introduced the datasets
Gulrajani & Lopez-Paz (2021); Koh et al. (2021); Xiao et al. (2021). Licenses of the codes are MIT license
for DomainBed Gulrajani & Lopez-Paz (2021) and WILDS Koh et al. (2021). The code of Backgrounds
Challenge does not indicate the license. Our code can be found at the link below.
https://github.com/Hiroki11x/Optimizer_Comparison_OOD
19Published in Transactions on Machine Learning Research (06/2023)
C Datasets
C.1 DomainBed
DomainBed consists of sub-datasets shown in Table 2, where we exclude Terra Incognita and Rotated MNIST
as stated in Section 3.2. We summarize the dataset information in the Table by referring to (Gulrajani &
Lopez-Paz, 2021).
Table 2: DomainBed: Dataset Information
domain examples class
Colored MNIST [0.1, 0.2, 0.9] 70000 2
Rotated MNIST [0, 15, 30, 45, 60, 75] 70000 10
PACS [art, cartoons, photos, sketches] 9991 7
VLCS [Caltech101, LabelMe,415SUN09, VOC2007] 10729 5
Oﬃce-Home [art, clipart, product, real] 15588 65
TerraIncognita [L100, L38, L43, L46] 24788 10
DomainNet [clipart, infograph, painting, quickdraw,420real, sketch] 586575 345
Colored MNIST is a dataset for binary classiﬁcation of MNIST dataset (Arjovsky et al., 2019). The digits
from 0 to 4 are labeled 0, and those greater than 5 are labeled 1, and the task is to classify these classes
successfully. However, each digit is also colored by either red or green. This is for having models to confuse
the important feature for classiﬁcation. The domain dindicates the correlation of the label with color.
For example, if the domain is 0.1, the correlation between, say red, with the number smaller than 5 is 0.1.
Furthermore, the label is ﬂipped at a constant rate: in this paper, 15 % label is ﬂipped. Therefore, the
correlation between color and digit is d, while between label and digits is 0.85. That is, what models should
learn is the correlation between label and noise, resulting in classiﬁcation accuracy of 0.85. However, if
the model exploits spurious correlation of the domain, it will learn the correlation between label and color,
resulting in training accuracy being 0.9 but test accuracy being 0.1 in this case.
PACS and Oﬃce-Home are image datasets whose domain determines the style of the image. These are
benchmark datasets for domain generalization.
VLCS is a set of diﬀerent photographic datasets, PASCAL VOC (Everingham et al., 2010), LabelMe (Russell
et al., 2008), Caltech101 (Fei-Fei et al., 2004), and SUN09 (Choi et al., 2010). PASCAL VOC, LabelMe, and
SUN09 are benchmark datasets for object detection. Caltech101 is 101 classes image datasets, where each
class has 40 - 80 samples.
DomainNet is a large dataset proposed for the study of domain generalization. The number of classes,
domains, and dataset size is the largest in the DomainBed dataset.
TerraIncognita is a dataset consisting of images taken by cameras at diﬀerent locations. The diﬀerence in the
camera’s location corresponds to the diﬀerence in the domain.
RotatedMNIST is a dataset that artiﬁcially rotates MNIST and divides the domain according to the rotation
angle. The number in the domain corresponds to the rotation angle.
We leave one domain for the ﬁnal evaluation and use the remaining domains for training. To evaluate the
performance during training, we split the data of each domain into training data and validation data. The
split ratio is 80 % for training and 20 % for validation. We take an average of test accuracies and validation
accuracies across domains, respectively, and use them to evaluate the OOD generalization.
C.2 Backgrounds Challenge Dataset
In Section D.2, we explained that we use the subset of ImageNet (ORIGINAL). ORIGINAL consists of nine
classes displayed in table 3. These classes are synthetically created from ImageNet classes based on WordNet
ID. This table is a copy of a table in the original paper (Xiao et al., 2021).
20Published in Transactions on Machine Learning Research (06/2023)
Table 3: Backgrounds Challenge: Dataset information originally created in (Xiao et al., 2021)
classes WordNet ID num sub-classes
Dog n02084071 116
Bird n01503061 52
Vehicle n04576211 42
Reptile n01661091 36
Carnivore n02075296 35
Insect n02159955 27
Instrument n03800933 26
Primate n02469914 20
Fish n02512053 16
This dataset is ﬁltered to balance samples across classes. We follow the same ﬁltering procedure as the
original paper. For further details, please refer to the original paper (Xiao et al., 2021).
C.3 Amazon-WILDS and CivilComments-WILDS Dataset
WILDS is a set ob benchmark datasets with distributional shift and their variants: iWildCam (Beery et al.,
2020), Camelyon17 (Bandi et al., 2018), RxRx1 (Taylor et al., 2019), OGB-MolPCBA (Hu et al., 2020),
GlobalWheat (David et al., 2020; 2021), CivilComments (Borkan et al., 2019), FMoW (Christie et al., 2018),
PovertyMap (Yeh et al., 2020), Amazon (Ni et al., 2019), and Py150 (Lu et al., 2021; Raychev et al., 2016).
We use CivilComments and Amazon for our experiment as NLP tasks.
C.4 CIFAR10-C and CIFAR10-P Dataset
First, for the corrupted or perturbed data generalization studies, we use CIFAR-10-C, CIFAR-10-P to evaluate
the generalization performance Hendrycks & Dietterich (2019); Mu & Gilmer (2019). CIFAR-10-C consist
of CIFAR image data corrupted by 19 noise patterns. CIFAR-10-P is similar, but with ten perturbations.
Similar corruptions and perturbations frequently occur in real-world imaging applications. Thus, evaluating
the robustness against this noise is essential for improving practical applicability. Because noises changes the
input samples, we can regard the corrupted and perturbed data as being sampled from a diﬀerent distribution
P(x,y)/primethan the original distribution: P(x,y)/prime/negationslash=P(x,y). If a classiﬁer f:X→Yis robust to corruption
c:X →X, orEc∼C[P(f(c(x) =y))]with corruption distribution C, we can say that the classiﬁer can
generalize for the corrupted samples. The same is true for perturbation.
21Published in Transactions on Machine Learning Research (06/2023)
D Experimental Protocol
We employ a Bayesian hyperparameter search strategy in the sweep functionality of Weights&Biases5. In this
strategy, the relationship between parameters and evaluation metrics is modeled as a Gaussian process, and
the parameters are selected in such a way as to optimize the improvement probability. Table 4 shows the
number of hyperparameter optimizations performed for each task. The transition of the best ood accuracy in
these hyperparameter optimizations is shown in Appendix G.2.
In line with previous studies (Gulrajani & Lopez-Paz, 2021; Xiao et al., 2021; Koh et al., 2021) diﬀerent
benchmarks use diﬀerent evaluation metrics, each of which is outlined in the following sections.
Table 4: Number of Experiments for Each Dataset
SGDMomentum Nesterov RMSprop Adam
RotatedMNIST 200 200 200 200 200
ColoredMNIST 200 342 200 200 200
PACS 200 200 200 200 412
VLCS 200 200 200 200 324
OﬃceHome 399 200 200 200 699
TerraIncognita 200 200 200 451 202
DomainNet 490 515 857 1160 1137
Amazon-WILDS 440 438 449 489 466
Amazon-CivilComments 594 543 588 575 554
Background Challenge - 347 - - 567
D.1 DomainBed
We follow the setting that is employed in the original paper (Gulrajani & Lopez-Paz, 2021). We train models
with training domains and evaluate their performance on the test domain, which is the domain not used
for training. We use ResNet-50 for PACS, VLCS, Oﬃce-Home, DomainNet, TerraIncognita, and MNIST
ConvNet (Gulrajani & Lopez-Paz, 2021) for RotatedMNIST and Colored MNIST.
In our experiments, one domain is used as the test domain (OOD: out-of-distribution) and the other domain as
the training domain (ID: in-distribution). More precisely, the test domain is "Art"for PACS, "Caltech101" for
VLSC,"Art"for Oﬃce-Home, "Clipart" for DomainNet, "L100"for TerraIncognita, "30°"for RotatedMNIST,
and"-90%"for ColoredMNIST. We explain the experimental conﬁgurations in Section E.1.
D.2 Backgrounds Challenge Dataset
We follow Xiao et al. (2021) for using Backgrounds Challenge dataset for evaluation. Thus, we use the
following data-generating procedure proposed by Xiao et al. (2021). We train ResNet-50 on ImageNet-1k with
two popular optimizers, Momentum SGD, and Adam. The test datasets to evaluate the trained model are
derivations of ImageNet dataset. First, we construct a subset of the ImageNet which has nine coarse-grained
classes, e.g. insect ( ORIGINAL ). Especially, we refer to ORIGINAL with all images from ImageNet as IN9L.
Then, we create a dataset by changing the background of the images of IN9L. In particular, we change
the background of each image into a random background cropped from another image in ORIGINAL. We
call this dataset Mixed-Random , following Xiao et al. (2021). By comparing the accuracy of Mixed-Same
with that of ORIGINAL, we can measure the dependence of the model on the spurious correlation of
background information. Thus, we investigate the relations between these two accuracies. The search range
for hyperparameters is shown in Section E.2.
5https://wandb.ai/site
22Published in Transactions on Machine Learning Research (06/2023)
D.3 WILDS
We also follow the setting that is employed in the original paper (Koh et al., 2021). First, we divide the
dataset into train, validation, and test and train a model using the train data. For model selection, we use
the performance evaluation of validation data. In the test dataset, considering the subpopulation shift, we
measure the performance of the OOD in the worst group for CivilComments-WILDS and in the domain of
10-percentile for Amazon-WILDS. In both Amazon-WILDS and CivilComments-WILDS, DistilBERT (Sanh
et al., 2019) is used as the deep neural network model architecture. We explain the further details of the
experimental conﬁgurations in Section E.3.
D.4 CIFAR10-C and CIFAR10-P
First, we trained deep neural networks with CIFAR-106. Then we evaluated the trained models with CIFAR-
10-C, CIFAR-10-P7. For the corruption datasets (CIFAR-10-C), we compared the accuracy for the corruption
or perturbation test samples (samples from CIFAR-10-C) with that for the training domain test samples
(samples from CIFAR-10). For the perturbation dataset, we measured the perturbation robustness introduced
by Hendrycks & Dietterich (2019).
6https://www.cs.toronto.edu/~kriz/cifar.html
7https://zenodo.org/record/2535967
23Published in Transactions on Machine Learning Research (06/2023)
E Hyperparameters and Detailed Conﬁgurations
We report the hyperparameter’s search space. For vanilla SGD, we search learning rate η, learning rate decay
rateρand the timing to decay learning rate δ, and regularization coeﬃcient of weight decay λ. Whenδ= 0.7,
it means that the learning rate decays when training passes 70 % of the total iterations. We do not search ρ
andδfor DomainBed because we do not employ a learning rate schedule.
For non-adaptive momentum methods, a parameter to control momentum γis added to the hyperparameters.
For RMSprop, we further add parameters αand/epsilon1, which control the second-order momentum and numerical
stability, respectively. Although /epsilon1is originally introduced for numerical stability, this parameter is found to
play a crucial role in generalization performance (Choi et al., 2019). Thus, we follow Choi et al. and vary
this parameter as well.
For Adam, we add /epsilon1,β1,β2to vanilla SGD’s conﬁguration. The parameter /epsilon1is the same as that for RMSprop
andβ1andβ2control ﬁrst and second-order momentum terms, respectively.
E.1 DomainBed
We conduct Bayesian optimization for hyperparameter search of DomainBed. First, we sampled hyperparam-
eters from uniform distribution whose minimum and maximum values are shown in the table 6 and 7 Then,
we conducted Bayesian optimization on these sampled candidate hyperparameters and selected some of the
hyperparameters among them. For reference, the scatter plot of Adam’s learning rate for the OﬃceHome
dataset is shown in Appendix H.1. Note that the values for batch size Bin the tables do not indicate
minimum and maximum for Bayesian optimization but those for grid search. Unlike other datasets, we
implement IRMv1 in addition to ERM. IRMv1 is a heuristic optimization problem to solve IRM, introduced
by Arjovsky et al. (2019). Thus, we search the hyperparameters of IRMv1 as well.
IRMv1 is the following constrained optimization problem (Arjovsky et al., 2019):
min
Φ:X→Y/summationdisplay
e∈EtrRe(Φ) +λIRM/vextenddouble/vextenddouble∇w|w=1.0Re(w◦Φ)/vextenddouble/vextenddouble2, (14)
where Φis representation function and wis the weight on top of the function of a model f=w◦Φ.Xis the
input domain and Yis the output domain. The character eindicates an environment in training environment
setEtrandReis the risk of the environment. Because this is the optimization with regularization term, we
search coeﬃcient λIRM. In addition, we implement annealing of this coeﬃcient and so we try various penalty
annealing iterations NIRMtoo. The basic workload is summarized in table 5.
We made two changes to the experimental setup in the original paper, as well as to the optimizer. We added
regularization to the training of ColoredMNIST and RotatedMNIST to stabilize the learning. In addition, we
increased the steps budget for RotatedMNIST because we observed that the training loss of RotatedMNIST
was not suﬃciently reduced.
We also experimented with two additional model architectures (ResNet-20 and Vision Transformer) for the
benchmarks proposed in the existing DomainBed.
24Published in Transactions on Machine Learning Research (06/2023)
Table 5: DomainBed: Workloads
Model Dataset Batch size Step Budget
MNIST ConvNet (Gulrajani & Lopez-Paz, 2021) Colored MNIST [128, 512, 2048] 5000
MNIST ConvNet (Gulrajani & Lopez-Paz, 2021) Rotated MNIST [128, 512, 2048] 100K
ResNet-50 VLCS [64, 128] 5000
ResNet-50 PACS [64, 128] 5000
ResNet-50 Oﬃce Home [64, 128] 5000
ResNet-50 DomainNet [64, 128] 5000
ResNet-50 TerraIncognita [64, 128] 5000
Vision Transformer (for Additional Study) PACS [64] 5000
ResNet-20 (for Additional Study) Colored MNIST [64] 5000
Table 6: DomainBed: ResNet-50
B η λ γ α /epsilon1 λ IRMNIRM
SGD [64, 128] [1e-5, 1e-2] [1e-6, 1e-2] - - -[1e-1,
1e+5][1e+0,
1e+4]
Momentum [64, 128] [1e-5, 1e-2] [1e-6, 1e-2] [0, 0.99999] - -[1e-1,
1e+5][1e+0,
1e+4]
Nesterov [64, 128] [1e-5, 1e-2] [1e-6, 1e-2] [0, 0.99999] - -[1e-1,
1e+5][1e+0,
1e+4]
RMSprop [64, 128] [1e-5, 1e-2] [1e-6, 1e-2] [0, 0.99999] [0, 0.999] [1e-8, 1e-3][1e-1,
1e+5][1e+0,
1e+4]
B η λ β 1β2/epsilon1 λ IRMNIRM
Adam[64, 128] [1e-5, 1e-2] [1e-6, 1e-2] [0, 0.999] [0, 0.999] [1e-8, 1e-3][1e-1,
1e+5][1e+0,
1e+4]
B η γ /epsilon1 ρ
SAM[64] [1e-5, 1e-2] [0.9] [1e-4] [5e-2]
Table 7: DomainBed: MNIST ConvNet (Gulrajani & Lopez-Paz, 2021)
B η λ γ α /epsilon1 λ IRMNIRM
SGD [128, 521, 2048] [1e-5, 1e-2] [1e-6, 1e-2] - - -
Momentum [128, 521, 2048] [1e-5, 1e-2] [1e-6, 1e-2] [0, 0.99999] - -[1e-1,
1e+5][1e+0,
1e+4]
Nesterov [128, 521, 2048] [1e-5, 1e-2] [1e-6, 1e-2] [0, 0.99999] - -[1e-1,
1e+5][1e+0,
1e+4]
RMSprop [128, 521, 2048] [1e-5, 1e-2] [1e-6, 1e-2] [0, 0.99999] [0, 0.999] [1e-8, 1e-3][1e-1,
1e+5][1e+0,
1e+4]
B η λ β 1β2/epsilon1 λ IRMNIRM
Adam[128, 521, 2048] [1e-5, 1e-2] -[0, 0.999] [0, 0.999] [1e-8, 1e-3][1e-1,
1e+5][1e+0,
1e+4]
25Published in Transactions on Machine Learning Research (06/2023)
E.2 Backgrounds Challenge Dataset
We conduct Bayesian optimization for the Backgrounds Challenge dataset as well. We use 4096 as the batch
size. For all conﬁgurations other than batch size, as we follow Choi et al. (2019).
We conduct the hyperparameter search and restart training from scratch. Of the trained models, we evaluate
trained model performance in the OOD dataset.
Table 8: Backgrounds Challenge: ResNet-50
η λ γ
Momentum [1e-5, 1e-2] [1e-6, 1e-2] [0, 0.99999]
η λ β 1β2/epsilon1
Adam[1e-5, 1e-2] [1e-6, 1e-2] [0, 0.999] [0, 0.999] [1e-8, 1e-3]
E.3 WILDS
We conduct Bayesian optimization for the hyperparameter search of WILDS. The hyperparameters are
sampled by uniform distribution whose minimum and maximum values are shown in the table 9 and 10. Note
that the values for batch size Bis ﬁxed in these experiments due to computational eﬃciency. We implement
IRMv1 as well as DomainBed experiments.
For training the Amazon-WILDS and CivilComments-WILDS datasets that we used as NLP datasets, we
followed the deep neural network model and training conﬁguration proposed in the original paper. DistilBERT,
a distillation of BERT Base, is used as the deep neural network model.
Table 9: WILDS-Amazon: DistilBERT
B η λ γ α /epsilon1 λ IRMNIRM
SGD [8] [1e-5, 1e-2] [1e-6, 1e-2] - - -[1e-1,
1e+5][1e+0,
1e+4]
Momentum [8] [1e-5, 1e-2] [1e-6, 1e-2] [0, 0.99999] - -[1e-1,
1e+5][1e+0,
1e+4]
Nesterov [8] [1e-5, 1e-2] [1e-6, 1e-2] [0, 0.99999] - -[1e-1,
1e+5][1e+0,
1e+4]
RMSprop [8] [1e-5, 1e-2] [1e-6, 1e-2] [0, 0.99999] [0, 0.999] [1e-8, 1e-3][1e-1,
1e+5][1e+0,
1e+4]
B η λ β 1β2/epsilon1 λ IRMNIRM
Adam[8] [1e-5, 1e-2] [1e-6, 1e-2] [0, 0.999] [0, 0.999] [1e-8, 1e-3][1e-1,
1e+5][1e+0,
1e+4]
B η β 1β2/epsilon1
AdamW [8] [1e-5, 1e-2] [0.9] [0.999] [1e-8]
B η γ /epsilon1 ρ
SAM[8] [1e-5, 1e-2] [0.9] [1e-4] [5e-2]
26Published in Transactions on Machine Learning Research (06/2023)
Table 10: WILDS-CivilComments: DistilBERT
B η λ γ α /epsilon1 λ IRMNIRM
SGD [16] [1e-5, 1e-2] [1e-6, 1e-2] - - -[1e-1,
1e+5][1e+0,
1e+4]
Momentum [16] [1e-5, 1e-2] [1e-6, 1e-2] [0, 0.99999] - -[1e-1,
1e+5][1e+0,
1e+4]
Nesterov [16] [1e-5, 1e-2] [1e-6, 1e-2] [0, 0.99999] - -[1e-1,
1e+5][1e+0,
1e+4]
RMSprop [16] [1e-5, 1e-2] [1e-6, 1e-2] [0, 0.99999] [0, 0.999] [1e-8, 1e-3][1e-1,
1e+5][1e+0,
1e+4]
B η λ β 1β2/epsilon1 λ IRMNIRM
Adam[16] [1e-5, 1e-2] [1e-6, 1e-2] [0, 0.999] [0, 0.999] [1e-8, 1e-3][1e-1,
1e+5][1e+0,
1e+4]
E.4 CIFAR10-C and CIFAR10-P
Corruption: The corruption dataset we consider contains 19 corruption patterns. These are Gaussian Noise ,
Shot Noise ,Impulse Noise ,Defocus Blur ,Frosted Glass Blur ,Motion Blur ,Zoom Blue ,Show,Frost,Fog,
Brightness ,Contrast,Elastic,Pixelate, andJPEG. Sample corrupted images are shown in the original paper
by Hendrycks & Dietterich (2019). Following Hendrycks & Dietterich (2019), we compute the classiﬁcation
errorEs,cfor each corruption cwith a ﬁve-point scale for the noise severity s. Averaging over a ﬁve-point
scale, we measure the classiﬁcation accuracy for each corruption Acccas follows:
Accc= 1−/summationtext5
s=1Es,c
5(15)
This accuracy is the test accuracy (corruption) we deﬁned in the main body of this paper. We evaluate the
generalization performance by comparing this quantity with the accuracy calculated using the original test
dataset Acc = 1−E. Note that Eis the classiﬁcation error for the test (training domain) samples.
Perturbation: The perturbation dataset has six corruption patterns that are the same as in the corruption
dataset and four additional digital transformations: Gaussian Noise ,Shot Noise ,Motion Blur ,Zoom Blur ,
Show,Brightness ,Translate ,Rotate,Tilt, andScale. The perturbation dataset diﬀers from the corruption
dataset in that each corruption generates more than thirty perturbation sequences. Hence, we use not a
simple measure of accuracy but the following metric to determine the perturbation robustness:
d(τ(x),τ(x/prime)) =5/summationdisplay
i=1max{i,σ(i)}/summationdisplay
j=min{i,σ(i)}+11(1≤j−1≤5), (16)
whereτ(x)is the ranked prediction of the classiﬁer on image xandσ(i) =τ(x/prime)/τ(x). This criterion
measures how the top-5 prediction on two diﬀerent images diﬀers. By using this quantity, top-5 robustness
perturbation can be written as follows:
uT5Dp=1
m(n−1)m/summationdisplay
i=1n/summationdisplay
j=2d(τ(xj),τ(xj−1)). (17)
27Published in Transactions on Machine Learning Research (06/2023)
Table 11: Hyperparameter Search Range: ResNet-8 / CIFAR10
B η λ γ
Momentum 256 [1e-6, 1e+1] [1e-5, 1e-4] [1e-4 0.9999]
B η λ β 1 β2 /epsilon1
Adam 256 [1e-4, 1e-1] [1e-5, 1e-4] [0.9, 0.999] [0.99, 0.9999] [1e-4, 1e-2]
F Full Results of Experiments
We conducted an evaluation of optimizers using two distinct model selection strategies. One approach is
Oracle-based, while the other employs a method of training-domain validation set (Gulrajani & Lopez-Paz,
2021). In Section F.1, we compare the OOD accuracy of models obtained through each strategy, as displayed
in a tabular format. Particularly for the latter strategy, we present both the average and variance of the
Top-10 models, as well as the average and variance for models surpassing a certain threshold.
The rationale behind providing the average and variance of the Top-10 models lies in our adoption of Bayesian
optimization for hyperparameters, with the aim of illustrating the error bars of high-performing models
obtained through this method. The results for models exceeding a certain threshold (in the rightmost bin)
are shared to facilitate comparison of average OOD accuracy performance when non-adaptive and adaptive
optimization methods are comparable in terms of ID accuracy.
In Section F.2, we present the results for all hyperparameters we explored (except for the model inferior to
random guess), along with their distributions in a box plot. Finally, in Section F.4, we present the OOD
accuracy and ID accuracy results for all the results from which these results are derived as a scatter plot.
F.1 Full Results of Table
Oracle:
In this section, we provide additional information on Table 1 in the paper. Table 12 shows the mean and
standard deviation for the top-10 models, including those selected by the Oracle model selection method. In
eight of the ten datasets, the non-adaptive method outperforms the adaptive method in out-of-distribution
accuracy.
Table 12: Top-10 Trials: Mean and Standard Deviation for Each Dataset (Model Selection by Oracle). The
variance is relatively suppressed in many tasks because it is the mean and variance of the model that achieves
Top-10 OOD accuracy.
Model OOD DatasetNon-Adaptive Optimizer Adaptive Optimizer
SGD Momentum Netsterov RMSProp Adam
4-Layer CNNRotatedMNIST 89.62±0.18 93.23±0.69 92.89±0.2 95.81±0.16 96.05±0.16
ColoredMNIST 14.64±0.07 29.80±1.84 28.85±2.0 52.82±6.21 52.39±6.89
ResNet50VLCS 98.98±0.1 98.70±0.16 98.49±0.12 96.23±1.39 97.09±1.61
PACS 86.56±0.83 86.90±0.82 86.01±1.36 81.10±2.93 82.59±2.93
OﬃceHome 63.68±0.12 63.52±0.44 62.40±0.47 55.34±3.39 62.12±0.35
TerraIncognita 52.22±1.62 56.05±1.96 52.98±1.61 53.67±2.62 48.78±3.53
DomainNet 55.31±2.38 56.12±3.85 58.19±2.2 52.71±2.74 55.74±1.65
BackgroundChallenge - 78.99±0.48 - - 75.55±1.5
DistilBERTWILDS_amazon 52.00±0.0 54.67±0.0 54.67±0.0 48.67±3.46 52.44±1.5
WILDS_civilcomments 51.81±0.29 56.40±0.61 55.98±0.47 38.42±7.04 37.71±4.0
ResNet-20 ColoredMNIST - 28.93±2.63 - - 29.84±1.64
ViT PACS - 90.28±0.54 - - 90.05±0.29
Training-Domain Validation Set: Table 13 shows the results of top-10 models with training-domain
validation set (Gulrajani & Lopez-Paz, 2021) as the model selection method. It shows that 11 out of 12
tasks showed that the non-adaptive optimization method is superior. As exhibited in Table 14, we present
the out-of-distribution accuracy along with the mean and standard deviation of the subset exhibiting high
28Published in Transactions on Machine Learning Research (06/2023)
validation performance within our targeted training domain, as graphically depicted in Figure 1. This data
speciﬁcally corresponds to the apex of the rightmost bin in the bar chart encapsulated in Figure 1. As
noted, the relatively elevated standard deviations observed can be primarily attributed to our calculation
methodology. This approach calculates the mean and standard deviation values of the out-of-distribution
accuracy, speciﬁcally focusing on the rightmost bins indicative of high validation accuracy as portrayed in
Figure 1. It is crucial to acknowledge that this unique approach could potentially yield a broader range of
variances.
Table 13: Top-10 Trials: Mean and Standard Deviation for Each Dataset (Model Selection by Training-
Domain Validation Set). The variance is relatively suppressed in many tasks because it is the mean and
variance of the model that achieves Top-10 OOD accuracy.
Model OOD DatasetNon-Adaptive Optimizer Adaptive Optimizer
SGD Momentum Netsterov RMSProp Adam
4-Layer CNNRotatedMNIST 89.13±0.56 93.09±0.82 91.36±1.4 95.51±0.48 95.76±0.43
ColoredMNIST 10.4±0.3 10.04±0.07 10.09±0.44 9.84±0.18 10.08±0.4
ResNet50VLCS 98.32±0.34 98.15±0.57 97.93±0.28 94.49±3.9 96.76±2.56
PACS 85.01±1.27 85.60±1.21 84.91±1.53 81.10±2.93 82.36±3.11
OﬃceHome 62.62±0.74 62.65±1.02 60.98±1.46 55.02±3.94 60.09±1.08
TerraIncognita 46.84±4.15 44.67±9.2 45.21±4.88 44.48±7.96 41.82±7.65
DomainNet 55.25±2.51 55.96±4.1 58.19±2.2 52.71±2.74 55.67±1.78
BackgroundChallenge - 78.99±0.48 - - 75.55±1.5
DistilBERTWILDS_amazon 52.00±0.0 54.13±0.69 53.77±0.63 50.93±3.25 52.44±1.5
WILDS_civilcomments 49.17±1.09 50.09±1.91 50.40±1.54 31.65±12.56 33.13±5.89
ResNet-20 ColoredMNIST - 11.00±0.52 - - 10.09±0.15
ViT PACS - 90.28±0.54 - - 90.05±0.29
Table 14: Trials in Rightmost Bin: Mean and Standard Deviation for Each Dataset (Model Selection by
Training-Domain Validation Set). The variance is relatively large because we compute the mean and variance
of models that exceed a speciﬁc threshold (the rightmost bin in Figure 1) in ID accuracy.
Model OOD DatasetNon-Adaptive Optimizer Adaptive Optimizer
SGD Momentum Netsterov RMSProp Adam
4-Layer CNNRotatedMNIST 82.83±6.15 87.84±5.27 87.43±5.58 94.29±1.22 94.43±1.91
ColoredMNIST 10.93±1.61 12.86±4.66 10.44±2.53 22.84±6.45 16.12±7.98
ResNet50VLCS 97.95±1.02 96.81±2.96 96.54±2.38 94.07±3.4 94.49±6.59
PACS 79.67±4.35 78.58±6.07 78.11±5.9 74.98±8.59 70.86±6.96
OﬃceHome 61.11±2.07 60.38±2.35 58.29±2.58 55.98±3.39 57.74±3.1
TerraIncognita 42.56±5.31 42.22±8.33 41.44±6.3 41.96±10.36 40.76±8.06
DomainNet 55.79±1.94 57.40±3.1 57.51±2.55 55.09±0.65 56.05±1.41
BackgroundChallenge - 74.98±4.7 - - 69.14±5.61
DistilBERTAmazon-WILDS 50.58±1.07 52.20±1.49 52.00±1.56 51.73±1.55 51.32±1.83
CivilComments-WILDS 42.24±5.8 44.79±6.57 44.15±6.37 21.79±9.59 23.55±7.43
ResNet-20 ColoredMNIST - 19.62±7.35 - - 21.70±7.66
ViT PACS - 87.07±5.52 - - 85.17±7.67
F.2 Full Results of Boxplot
Since we could not include all the results in the main paper, we report all the OOD accuracy, ID accuracy
and their diﬀerence (we denote as gap) results including ERM and IRM results in a box plot.
F.2.1 Full Results of Filtered Boxplot (ERM)
What we want to ﬁnd out is the OOD generalization performance for models that perform better than
random guess in the ID test set, i.e., models that have been trained. As Filtered Results, we deﬁne
random guess = 1/num of classes in each problem setting, and show the results of eliminating those models
whose ID accuracy does not exceed this threshold.
29Published in Transactions on Machine Learning Research (06/2023)
SGD Momentum Nesterov RMSProp Adam
optimizer_name0.00.20.40.60.81.0out-of-distribution acc
RotatedMNIST: ERM
SGD Momentum Nesterov RMSProp Adam
optimizer_name0.00.20.40.60.81.0in-distribution acc
RotatedMNIST: ERM
SGD Momentum Nesterov RMSProp Adam
optimizer_name0.00.20.40.60.81.0gap
RotatedMNIST: ERM
SGD Momentum Nesterov RMSProp Adam
optimizer_name0.00.20.40.60.81.0out-of-distribution acc
ColoredMNIST: ERM
SGD Momentum Nesterov RMSProp Adam
optimizer_name0.00.20.40.60.81.0in-distribution acc
ColoredMNIST: ERM
SGD Momentum Nesterov RMSProp Adam
optimizer_name0.00.20.40.60.81.0gap
ColoredMNIST: ERM
Figure 6. Filtered Results of RotatedMNIST and ColoredMNIST in DomainBed: Comparison of the
in-distribution (validation) accuracy and the out-of-distribution (test) accuracy of ERM across ﬁve optimizers.
30Published in Transactions on Machine Learning Research (06/2023)
SGD Momentum Nesterov RMSProp Adam
optimizer_name0.00.20.40.60.81.0out-of-distribution acc
PACS: ERM
SGD Momentum Nesterov RMSProp Adam
optimizer_name0.00.20.40.60.81.0in-distribution acc
PACS: ERM
SGD Momentum Nesterov RMSProp Adam
optimizer_name0.00.20.40.60.81.0gap
PACS: ERM
SGD Momentum Nesterov RMSProp Adam
optimizer_name0.00.20.40.60.81.0out-of-distribution acc
VLCS: ERM
SGD Momentum Nesterov RMSProp Adam
optimizer_name0.00.20.40.60.81.0in-distribution acc
VLCS: ERM
SGD Momentum Nesterov RMSProp Adam
optimizer_name0.25
0.000.250.500.751.00gap
VLCS: ERM
SGD Momentum Nesterov RMSProp Adam
optimizer_name0.00.20.40.60.81.0out-of-distribution acc
OfficeHome: ERM
SGD Momentum Nesterov RMSProp Adam
optimizer_name0.00.20.40.60.81.0in-distribution acc
OfficeHome: ERM
SGD Momentum Nesterov RMSProp Adam
optimizer_name0.00.20.40.60.81.0gap
OfficeHome: ERM
SGD Momentum Nesterov RMSProp Adam
optimizer_name0.00.20.40.60.81.0out-of-distribution acc
T erraIncognita: ERM
SGD Momentum Nesterov RMSProp Adam
optimizer_name0.00.20.40.60.81.0in-distribution acc
T erraIncognita: ERM
SGD Momentum Nesterov RMSProp Adam
optimizer_name0.00.20.40.60.81.0gap
T erraIncognita: ERM
SGD Momentum Nesterov RMSProp Adam
optimizer_name0.00.20.40.60.81.0out-of-distribution acc
DomainNet: ERM
SGD Momentum Nesterov RMSProp Adam
optimizer_name0.00.20.40.60.81.0in-distribution acc
DomainNet: ERM
SGD Momentum Nesterov RMSProp Adam
optimizer_name0.00.20.40.60.81.0gap
DomainNet: ERM
Figure 7. Filtered Results of PACS, VLCS, OﬃceHome, TerraIncognita, and DomainNet in DomainBed:
Comparison of the in-distribution (validation) accuracy and the out-of-distribution (test) accuracy of ERM
across ﬁve optimizers.
31Published in Transactions on Machine Learning Research (06/2023)
Momentum Adam
optimizer_name0.00.20.40.60.81.0out-of-distribution acc
Background Challenge
Momentum Adam
optimizer_name0.00.20.40.60.81.0in-distribution acc
Background Challenge
Momentum Adam
optimizer_name0.00.20.40.60.81.0gap
Background Challenge
Figure 8. Filtered Results of Backgrounds Challenge: Comparison of the in-distribution (validation) accuracy
and the out-of-distribution (test) accuracy of ERM across ﬁve optimizers.
SGD Momentum Nesterov RMSProp Adam
optimizer_name0.00.20.40.60.81.0out-of-distribution acc
WILDS_amazon: ERM
SGD Momentum Nesterov RMSProp Adam
optimizer_name0.00.20.40.60.81.0in-distribution acc
WILDS_amazon: ERM
SGD Momentum Nesterov RMSProp Adam
optimizer_name0.00.20.40.60.81.0gap
WILDS_amazon: ERM
SGD Momentum Nesterov RMSProp Adam
optimizer_name0.00.20.40.60.81.0out-of-distribution acc
WILDS_civilcomments: ERM
SGD Momentum Nesterov RMSProp Adam
optimizer_name0.00.20.40.60.81.0in-distribution acc
WILDS_civilcomments: ERM
SGD Momentum Nesterov RMSProp Adam
optimizer_name0.00.20.40.60.81.0gap
WILDS_civilcomments: ERM
Figure 9. Filtered Results of Amazon-WILDS and CivilComments-WILDS: Comparison of the in-distribution
(validation) accuracy and the out-of-distribution (test) accuracy of ERM across ﬁve optimizers.
32Published in Transactions on Machine Learning Research (06/2023)
F.2.2 Full Results of Filtered Boxplot (IRM)
SGD Momentum Nesterov RMSProp Adam
optimizer_name0.00.20.40.60.81.0out-of-distribution acc
PACS: IRM
SGD Momentum Nesterov RMSProp Adam
optimizer_name0.00.20.40.60.81.0in-distribution acc
PACS: IRM
SGD Momentum Nesterov RMSProp Adam
optimizer_name0.00.20.40.60.81.0gap
PACS: IRM
SGD Momentum Nesterov RMSProp Adam
optimizer_name0.00.20.40.60.81.0out-of-distribution acc
VLCS: IRM
SGD Momentum Nesterov RMSProp Adam
optimizer_name0.00.20.40.60.81.0in-distribution acc
VLCS: IRM
SGD Momentum Nesterov RMSProp Adam
optimizer0.00.51.0gap
VLCS: IRM
SGD Momentum Nesterov RMSProp Adam
optimizer_name0.00.20.40.60.81.0out-of-distribution acc
OfficeHome: IRM
SGD Momentum Nesterov RMSProp Adam
optimizer_name0.00.20.40.60.81.0in-distribution acc
OfficeHome: IRM
SGD Momentum Nesterov RMSProp Adam
optimizer_name0.00.20.40.60.81.0gap
OfficeHome: IRM
SGD Momentum Nesterov RMSProp Adam
optimizer_name0.00.20.40.60.81.0out-of-distribution acc
T erraIncognita: IRM
SGD Momentum Nesterov RMSProp Adam
optimizer_name0.00.20.40.60.81.0in-distribution acc
T erraIncognita: IRM
SGD Momentum Nesterov RMSProp Adam
optimizer0.00.20.40.60.81.0gap
T erraIncognita: IRM
SGD Momentum Nesterov RMSProp Adam
optimizer_name0.00.20.40.60.81.0out-of-distribution acc
DomainNet: IRM
SGD Momentum Nesterov RMSProp Adam
optimizer_name0.00.20.40.60.81.0in-distribution acc
DomainNet: IRM
SGD Momentum Nesterov RMSProp Adam
optimizer_name0.00.20.40.60.81.0gap
DomainNet: IRM
Figure 10. Filtered Results of PACS, VLCS, OﬃceHome, TerraIncognita and DomainNet in DomainBed:
Comparison of the in-distribution (validation) accuracy and the out-of-distribution (test) accuracy of IRM
across ﬁve optimizers.
33Published in Transactions on Machine Learning Research (06/2023)
SGD Momentum Nesterov RMSProp Adam
optimizer_name0.00.20.40.60.81.0out-of-distribution acc
RotatedMNIST: IRM
SGD Momentum Nesterov RMSProp Adam
optimizer_name0.00.20.40.60.81.0in-distribution acc
RotatedMNIST: IRM
SGD Momentum Nesterov RMSProp Adam
optimizer_name0.00.20.40.60.81.0gap
RotatedMNIST: IRM
SGD Momentum Nesterov RMSProp Adam
optimizer_name0.00.20.40.60.81.0out-of-distribution acc
ColoredMNIST: IRM
SGD Momentum Nesterov RMSProp Adam
optimizer_name0.00.20.40.60.81.0in-distribution acc
ColoredMNIST: IRM
SGD Momentum Nesterov RMSProp Adam
optimizer_name0.00.20.40.60.81.0gap
ColoredMNIST: IRM
Figure 11. Filtered Results of RotatedMNIST and ColoredMNIST in DomainBed: Comparison of the
in-distribution (validation) accuracy and the out-of-distribution (test) accuracy of IRM across ﬁve optimizers.
SGD Momentum Nesterov RMSProp Adam
optimizer_name0.00.20.40.60.81.0out-of-distribution acc
WILDS_amazon: IRM
SGD Momentum Nesterov RMSProp Adam
optimizer_name0.00.20.40.60.81.0in-distribution acc
WILDS_amazon: IRM
SGD Momentum Nesterov RMSProp Adam
optimizer_name0.00.20.40.60.81.0gap
WILDS_amazon: IRM
SGD Momentum Nesterov RMSProp Adam
optimizer_name0.00.20.40.60.81.0out-of-distribution acc
WILDS_civilcomments: IRM
SGD Momentum Nesterov RMSProp Adam
optimizer_name0.00.20.40.60.81.0in-distribution acc
WILDS_civilcomments: IRM
SGD Momentum Nesterov RMSProp Adam
optimizer_name0.00.20.40.60.81.0gap
WILDS_civilcomments: IRM
Figure 12. Filtered Results of Amazon-WILDS and CivilComments-WILDS: Comparison of the in-distribution
(validation) accuracy and the out-of-distribution (test) accuracy of IRM across ﬁve optimizers.
34Published in Transactions on Machine Learning Research (06/2023)
F.3 Full Results of Bin-Diagram Plots
0.0 0.5 1.0
in-distribution acc0.00.20.40.60.81.0out-of-distribution accColoredMNIST:ERM
momentum_sgd
adam
0.00 0.25 0.50 0.75 1.00
in-distribution acc0.00.20.40.60.81.0out-of-distribution accRotatedMNIST:ERM
0.00 0.25 0.50 0.75 1.00
in-distribution acc0.00.20.40.60.81.0out-of-distribution accPACS:ERM
0.00 0.25 0.50 0.75 1.00
in-distribution acc0.00.20.40.60.81.0out-of-distribution accVLCS:ERM
0.00 0.25 0.50 0.75 1.00
in-distribution acc0.00.20.40.60.81.0out-of-distribution accOfficeHome:ERM
0.00 0.25 0.50 0.75 1.00
in-distribution acc0.00.20.40.60.81.0out-of-distribution accT erraIncognita:ERM
0.00 0.25 0.50 0.75 1.00
in-distribution acc0.00.20.40.60.81.0out-of-distribution accDomainNet:ERM
0.00 0.25 0.50 0.75 1.00
in-distribution acc0.00.20.40.60.81.0out-of-distribution accBackground Challenge
0.00 0.25 0.50 0.75 1.00
in-distribution acc0.00.20.40.60.81.0out-of-distribution accWILDS_Amazon:ERM
0.00 0.25 0.50 0.75 1.00
in-distribution acc0.00.20.40.60.81.0out-of-distribution accWILDS_civilcomments:ERM
Figure 13. DomainBed, Backgrounds Challenge, Amazon-WILDS, and CivilComments-WILDS: Comparison
of the in-distribution accuracy and the out-of-distribution accuracy of ERM between Momentum SGD and
Adam. Since Adam showed better OOD performance than RMSProp, Adam is presented as a representative
of adaptive methods. Momentum SGD shows competitive performance in OOD with Vanilla SGD and
Nesterov Momentum and is a representative of non-adaptive methods.
0.0 0.5 1.0
in-distribution acc0.00.20.40.60.81.0out-of-distribution accColoredMNIST:IRM
momentum_sgd
adam
0.00 0.25 0.50 0.75 1.00
in-distribution acc0.00.20.40.60.81.0out-of-distribution accRotatedMNIST:IRM
0.00 0.25 0.50 0.75 1.00
in-distribution acc0.00.20.40.60.81.0out-of-distribution accPACS:IRM
0.00 0.25 0.50 0.75 1.00
in-distribution acc0.00.20.40.60.81.0out-of-distribution accVLCS:IRM
0.00 0.25 0.50 0.75 1.00
in-distribution acc0.00.20.40.60.81.0out-of-distribution accOfficeHome:IRM
0.00 0.25 0.50 0.75 1.00
in-distribution acc0.00.20.40.60.81.0out-of-distribution accT erraIncognita:IRM
0.00 0.25 0.50 0.75 1.00
in-distribution acc0.00.20.40.60.81.0out-of-distribution accDomainNet:IRM
0.00 0.25 0.50 0.75 1.00
in-distribution acc0.00.20.40.60.81.0out-of-distribution accWILDS_Amazon:IRM
0.00 0.25 0.50 0.75 1.00
in-distribution acc0.00.20.40.60.81.0out-of-distribution accWILDS_civilcomments:IRM
Figure 14. DomainBed, Amazon-WILDS, and CivilComments-WILDS: Comparison of the in-distribution
accuracy and the out-of-distribution accuracy of IRM between Momentum SGD and Adam. Since Adam
showed better OOD performance than RMSProp, Adam is presented as a representative of adaptive methods.
Momentum SGD shows competitive performance in OOD with Vanilla SGD and Nesterov Momentum and is
a representative of non-adaptive methods.
35Published in Transactions on Machine Learning Research (06/2023)
F.4 Full Results of Scatter Plots
The results of the comparison of OOD accuracy and in-distribution accuracy shown in Section 4.3 are shown
below for all benchmarks.
The box plots shown in Section F.2 and the Reliability Diagram Like Plot shown in Section F.3 are based on
the data shown in the Scatter Plot shown below.
F.4.1 ERM
0.0 0.2 0.4 0.6 0.8 1.0
in-distribution acc0.00.20.40.60.81.0out-of-distribution accColoredMNIST:ERM
ERM SGD
ERM Momentum SGD
ERM Nesterov Momentum SGD
ERM RMSProp
ERM Adam
0.0 0.2 0.4 0.6 0.8 1.0
in-distribution acc0.00.20.40.60.81.0out-of-distribution accRotatedMNIST:ERM
ERM SGD
ERM Momentum SGD
ERM Nesterov Momentum SGD
ERM RMSProp
ERM Adam
0.0 0.2 0.4 0.6 0.8 1.0
in-distribution acc0.00.20.40.60.81.0out-of-distribution accPACS:ERM
ERM SGD
ERM Momentum SGD
ERM Nesterov Momentum SGD
ERM RMSProp
ERM Adam
0.0 0.2 0.4 0.6 0.8 1.0
in-distribution acc0.00.20.40.60.81.0out-of-distribution accVLCS:ERM
ERM SGD
ERM Momentum SGD
ERM Nesterov Momentum SGD
ERM RMSProp
ERM Adam
0.0 0.2 0.4 0.6 0.8 1.0
in-distribution acc0.00.20.40.60.81.0out-of-distribution accOfficeHome:ERM
ERM SGD
ERM Momentum SGD
ERM Nesterov Momentum SGD
ERM RMSProp
ERM Adam
0.0 0.2 0.4 0.6 0.8 1.0
in-distribution acc0.00.20.40.60.81.0out-of-distribution accT erraIncognita:ERM
ERM SGD
ERM Momentum SGD
ERM Nesterov Momentum SGD
ERM RMSProp
ERM Adam
Figure 15. DomainBed (ColoredMNIST, RotatedMNIST, PACS, VLCS, OﬃceHome and TerraIncognita):
Comparison of the in-distribution accuracy and the out-of-distribution accuracy of ERM across optimizers.
The legend circles on the right side of each ﬁgure show, in order, VanillaSGD, Momentum SGD, Nesterov
Momentum, RMProp, and Adam. The diﬀerence in each data point indicates the diﬀerence in hyperparameter
conﬁguration.
36Published in Transactions on Machine Learning Research (06/2023)
0.0 0.2 0.4 0.6 0.8 1.0
in-distribution acc0.00.20.40.60.81.0out-of-distribution accDomainNet:ERM
ERM SGD
ERM Momentum SGD
ERM Nesterov Momentum SGD
ERM RMSProp
ERM Adam
0.0 0.2 0.4 0.6 0.8 1.0
in-distribution acc0.00.20.40.60.81.0out-of-distribution accBackground Challenge
0.0 0.2 0.4 0.6 0.8 1.0
in-distribution acc0.00.20.40.60.81.0out-of-distribution accWILDS_amazon:ERM
ERM SGD
ERM Momentum SGD
ERM Nesterov Momentum SGD
ERM RMSProp
ERM Adam
0.0 0.2 0.4 0.6 0.8 1.0
in-distribution acc0.00.20.40.60.81.0out-of-distribution accWILDS_civilcomments:ERM
ERM SGD
ERM Momentum SGD
ERM Nesterov Momentum SGD
ERM RMSProp
ERM Adam
Figure 16. DomainBed (DomainNet), Backgrounds Challenge and WILDS: Comparison of the in-distribution
accuracy and the out-of-distribution accuracy of ERM across optimizers. The legend circles on the right side
of each ﬁgure show, in order, VanillaSGD, Momentum SGD, Nesterov Momentum, RMProp, and Adam. The
diﬀerence in each data point indicates the diﬀerence in hyperparameter conﬁguration.
F.4.2 IRM
0.0 0.2 0.4 0.6 0.8 1.0
in-distribution acc0.00.20.40.60.81.0out-of-distribution accColoredMNIST:IRM
IRM SGD
IRM Momentum SGD
IRM Nesterov Momentum SGD
IRM RMSProp
IRM Adam
0.0 0.2 0.4 0.6 0.8 1.0
in-distribution acc0.00.20.40.60.81.0out-of-distribution accRotatedMNIST:IRM
IRM SGD
IRM Momentum SGD
IRM Nesterov Momentum SGD
IRM RMSProp
IRM Adam
0.0 0.2 0.4 0.6 0.8 1.0
in-distribution acc0.00.20.40.60.81.0out-of-distribution accPACS:IRM
IRM SGD
IRM Momentum SGD
IRM Nesterov Momentum SGD
IRM RMSProp
IRM Adam
0.0 0.2 0.4 0.6 0.8 1.0
in-distribution acc0.00.20.40.60.81.0out-of-distribution accVLCS:IRM
IRM SGD
IRM Momentum SGD
IRM Nesterov Momentum SGD
IRM RMSProp
IRM Adam
Figure 17. DomainBed (ColoredMNIST, RotatedMNIST, PACS and VLCS): Comparison of the in-distribution
accuracy and the out-of-distribution accuracy of IRM across optimizers. The legend circles on the right side
of each ﬁgure show, in order, VanillaSGD, Momentum SGD, Nesterov Momentum, RMProp, and Adam. The
diﬀerence in each data point indicates the diﬀerence in hyperparameter conﬁguration.
37Published in Transactions on Machine Learning Research (06/2023)
0.0 0.2 0.4 0.6 0.8 1.0
in-distribution acc0.00.20.40.60.81.0out-of-distribution accOfficeHome:IRM
IRM SGD
IRM Momentum SGD
IRM Nesterov Momentum SGD
IRM RMSProp
IRM Adam
0.0 0.2 0.4 0.6 0.8 1.0
in-distribution acc0.00.20.40.60.81.0out-of-distribution accT erraIncognita:IRM
IRM SGD
IRM Momentum SGD
IRM Nesterov Momentum SGD
IRM RMSProp
IRM Adam
0.0 0.2 0.4 0.6 0.8 1.0
in-distribution acc0.00.20.40.60.81.0out-of-distribution accDomainNet:IRM
IRM SGD
IRM Momentum SGD
IRM Nesterov Momentum SGD
IRM RMSProp
IRM Adam
0.0 0.2 0.4 0.6 0.8 1.0
in-distribution acc0.00.20.40.60.81.0out-of-distribution accWILDS_amazon:IRM
IRM SGD
IRM Momentum SGD
IRM Nesterov Momentum SGD
IRM RMSProp
IRM Adam
0.0 0.2 0.4 0.6 0.8 1.0
in-distribution acc0.00.20.40.60.81.0out-of-distribution accWILDS_civilcomments:IRM
IRM SGD
IRM Momentum SGD
IRM Nesterov Momentum SGD
IRM RMSProp
IRM Adam
Figure 18. DomainBed (OﬃceHome, TerraIncognita, DomainNet) and WILDS: Comparison of the in-
distribution accuracy and the out-of-distribution accuracy of IRM across optimizers. The legend circles on
the right side of each ﬁgure show, in order, VanillaSGD, Momentum SGD, Nesterov Momentum, RMProp,
and Adam. The diﬀerence in each data point indicates the diﬀerence in hyperparameter conﬁguration.
38Published in Transactions on Machine Learning Research (06/2023)
G Ablation Study
In addition to the experimental results presented in the main paper, we provide a more in-depth analysis,
which is shown in this chapter.
First of all, We show that the pattern of linear correlation of accuracy claimed by (Miller et al., 2021) does
not necessarily occur even when the correlation patterns we observed, such as diminishing returns, are probit
transformed. The results of the probit transform of the scatter plots shown in Appendix F.4 are shown,
following the method of (Miller et al., 2021). This may be due to the fact that our experimental setup uses a
larger number of data sets and takes IRM and other factors into account.
In the second section, we present results comparing the performance improvement of OOD with a larger trial
budget in the search for hyperparameters by Bayesian optimization.
Thirdly, We investigate why Adam shows better OOD performance than SGD in the negatively correlated
case seen in Figure 6 for ColoredMNIST by considering the learning curve.
Finally, we examine the eﬀectiveness of the early stopping for each optimizer to study if adaptive optimizers
overﬁt because of their speed of convergence.
G.1 Probit Transformed Scatter Plot
As shown in Section 4.3, the work of (Miller et al., 2021) compares the accuracy of OOD with the accuracy of
in-distribution by showing a plot on a probit scale. Their comparison shows that the correlation of accuracy
is linear.
In this section, we convert the scatter plots identiﬁed in Section F.4 to probit scale and conﬁrm that they do
not necessarily show a linear correlation (Figure 19, 21, 22, 23, 23, and 24).
G.1.1 ERM
0.50 0.70 0.800.100.250.50SGD
Momentum SGD
Nesterov Momentum SGD
RMSProp
Adam
ColoredMNIST:ERM (probit scaling)
in-distribution accout-of-distribution acc
0.10 0.25 0.50 0.70 0.80 0.90 0.95 0.990.100.250.500.700.800.900.95SGD
Momentum SGD
Nesterov Momentum SGD
RMSProp
Adam
RotatedMNIST:ERM (probit scaling)
in-distribution accout-of-distribution acc
Figure 19. Probit Scale / DomainBed (ColoredMNIST and RotatedMNIST): Comparison of the in-distribution
accuracy and the out-of-distribution accuracy of ERM across optimizers. The diﬀerence in each data point
indicates the diﬀerence in hyperparameter conﬁguration.
39Published in Transactions on Machine Learning Research (06/2023)
0.10 0.25 0.50 0.70 0.80 0.90 0.950.100.250.500.700.800.90 SGD
Momentum SGD
Nesterov Momentum SGD
RMSProp
Adam
PACS:ERM (probit scaling)
in-distribution accout-of-distribution acc
0.10 0.25 0.50 0.70 0.800.100.250.500.700.800.900.950.99SGD
Momentum SGD
Nesterov Momentum SGD
RMSProp
Adam
VLCS:ERM (probit scaling)
in-distribution accout-of-distribution acc
0.10 0.25 0.50 0.70 0.800.100.250.500.70
SGD
Momentum SGD
Nesterov Momentum SGD
RMSProp
Adam
OfficeHome:ERM (probit scaling)
in-distribution accout-of-distribution acc
0.10 0.25 0.500.100.250.500.70
SGD
Momentum SGD
Nesterov Momentum SGD
RMSProp
Adam
DomainNet:ERM (probit scaling)
in-distribution accout-of-distribution acc
0.10 0.25 0.50 0.70 0.80 0.900.100.250.50SGD
Momentum SGD
Nesterov Momentum SGD
RMSProp
Adam
TerraIncognita:ERM (probit scaling)
in-distribution accout-of-distribution acc
Figure 20. Probit Scale / DomainBed (PACS, VLCS, OﬃceHome, DomainNet and TerraIncognita): Com-
parison of the in-distribution accuracy and the out-of-distribution accuracy of ERM across optimizers. The
diﬀerence in each data point indicates the diﬀerence in hyperparameter conﬁguration.
40Published in Transactions on Machine Learning Research (06/2023)
0.50 0.70 0.80 0.90 0.950.250.500.700.80 Momentum SGD
Adam
Background Challenge (probit scaling)
in-distribution accout-of-distribution acc
Figure 21. Probit Scale / Backgrounds Challenge: Comparison of the in-distribution accuracy and the
out-of-distribution accuracy of ERM across optimizers. The diﬀerence in each data point indicates the
diﬀerence in hyperparameter conﬁguration.
0.50 0.700.250.50SGD
Momentum SGD
Nesterov Momentum SGD
RMSProp
Adam
WILDS_amazon:ERM (probit scaling)
in-distribution accout-of-distribution acc
0.50 0.70 0.80 0.900.100.250.50SGD
Momentum SGD
Nesterov Momentum SGD
RMSProp
Adam
WILDS_civilcomments:ERM (probit scaling)
in-distribution accout-of-distribution acc
Figure 22. Probit Scale / WILDS: Comparison of the in-distribution accuracy and the out-of-distribution
accuracyofERMacrossoptimizers. Thediﬀerenceineachdatapointindicatesthediﬀerenceinhyperparameter
conﬁguration.
41Published in Transactions on Machine Learning Research (06/2023)
G.1.2 IRM
0.50 0.70 0.800.100.250.50SGD
Momentum SGD
Nesterov Momentum SGD
RMSProp
Adam
ColoredMNIST:IRM (probit scaling)
in-distribution accout-of-distribution acc
0.10 0.25 0.50 0.70 0.80 0.90 0.95 0.990.100.250.500.700.800.900.95
SGD
Momentum SGD
Nesterov Momentum SGD
RMSProp
Adam
RotatedMNIST:IRM (probit scaling)
in-distribution accout-of-distribution acc
0.10 0.25 0.50 0.70 0.80 0.90 0.950.100.250.500.700.80SGD
Momentum SGD
Nesterov Momentum SGD
RMSProp
Adam
PACS:IRM (probit scaling)
in-distribution accout-of-distribution acc
0.10 0.25 0.50 0.70 0.800.100.250.500.700.800.900.950.99SGD
Momentum SGD
Nesterov Momentum SGD
RMSProp
Adam
VLCS:IRM (probit scaling)
in-distribution accout-of-distribution acc
0.10 0.25 0.50 0.70 0.800.100.250.500.70
SGD
Momentum SGD
Nesterov Momentum SGD
RMSProp
Adam
OfficeHome:IRM (probit scaling)
in-distribution accout-of-distribution acc
0.10 0.25 0.500.100.250.50SGD
Momentum SGD
Nesterov Momentum SGD
RMSProp
Adam
DomainNet:IRM (probit scaling)
in-distribution accout-of-distribution acc
0.10 0.25 0.50 0.70 0.80 0.900.100.250.50SGD
Momentum SGD
Nesterov Momentum SGD
RMSProp
Adam
TerraIncognita:IRM (probit scaling)
in-distribution accout-of-distribution acc
Figure 23. Probit Scale / DomainBed (ColoredMNIST, RotatedMNIST, PACS, VLCS, OﬃceHome, Do-
mainNed and TerraIncognita): Comparison of the in-distribution accuracy and the out-of-distribution accuracy
of IRM across optimizers. The diﬀerence in each data point indicates the diﬀerence in hyperparameter
conﬁguration.
42Published in Transactions on Machine Learning Research (06/2023)
0.10 0.25 0.500.100.25SGD
Momentum SGD
Nesterov Momentum SGD
RMSProp
Adam
WILDS_amazon:IRM (probit scaling)
in-distribution accout-of-distribution acc
0.10 0.25 0.50 0.70 0.80 0.900.100.250.50
SGD
Momentum SGD
Nesterov Momentum SGD
RMSProp
Adam
WILDS_civilcomments:IRM (probit scaling)
in-distribution accout-of-distribution acc
Figure 24. Probit Scale / WILDS: Comparison of the in-distribution accuracy and the out-of-distribution
accuracy of IRM across optimizers. The diﬀerence in each data point indicates the diﬀerence in hyperparameter
conﬁguration.
43Published in Transactions on Machine Learning Research (06/2023)
G.2 Model Performance Transition throught Hyperparameter Search
For practitioners, we show how much the trial budget for hyperparameter optimization aﬀects OOD general-
ization.
G.2.1 Hyperparameter Trial Budget vs OOD Accuracy
0 25 50 75 100 125 150 175 200
trials0.00.20.40.60.81.0out-distribution accDomainNet ERM Momentum
top-1
top-10 average
all average
0 25 50 75 100 125 150 175 200
trials0.00.20.40.60.81.0out-distribution accDomainNet ERM Adam
top-1
top-10 average
all average
0 25 50 75 100 125 150 175 200
trials0.00.20.40.60.81.0out-distribution accOfficeHome ERM Momentum
top-1
top-10 average
all average
0 25 50 75 100 125 150 175 200
trials0.00.20.40.60.81.0out-distribution accOfficeHome ERM Adam
top-1
top-10 average
all average
0 25 50 75 100 125 150 175 200
trials0.00.20.40.60.81.0out-distribution accT erraIncognita ERM Momentum
top-1
top-10 average
all average
0 25 50 75 100 125 150 175 200
trials0.00.20.40.60.81.0out-distribution accT erraIncognita ERM Adam
top-1
top-10 average
all average
Figure 25. ERM DomainNet, OﬃceHome and TerraIncognita / OOD accuracy when horizontal axis is the
trial budget
44Published in Transactions on Machine Learning Research (06/2023)
0 25 50 75 100 125 150 175 200
trials0.00.20.40.60.81.0out-distribution accColoredMNIST ERM Momentum
top-1
top-10 average
all average
0 25 50 75 100 125 150 175 200
trials0.00.20.40.60.81.0out-distribution accColoredMNIST ERM Adam
top-1
top-10 average
all average
0 25 50 75 100 125 150 175 200
trials0.00.20.40.60.81.0out-distribution accRotatedMNIST ERM Momentum
top-1
top-10 average
all average
0 25 50 75 100 125 150 175 200
trials0.00.20.40.60.81.0out-distribution accRotatedMNIST ERM Adam
top-1
top-10 average
all average
0 25 50 75 100 125 150 175 200
trials0.00.20.40.60.81.0out-distribution accVLCS ERM Momentum
top-1
top-10 average
all average
0 25 50 75 100 125 150 175 200
trials0.00.20.40.60.81.0out-distribution accVLCS ERM Adam
top-1
top-10 average
all average
0 25 50 75 100 125 150 175 200
trials0.00.20.40.60.81.0out-distribution accPACS ERM Momentum
top-1
top-10 average
all average
0 25 50 75 100 125 150 175 200
trials0.00.20.40.60.81.0out-distribution accPACS ERM Adam
top-1
top-10 average
all average
Figure 26. ERM ColoredMNIST, RotatedMNIST, VLCS and PACS / OOD accuracy when horizontal axis is
the trial budget
45Published in Transactions on Machine Learning Research (06/2023)
0 25 50 75 100 125 150 175 200
trials0.00.20.40.60.81.0out-distribution accBackground Challenge Momentum
top-1
top-10 average
all average
0 25 50 75 100 125 150 175 200
trials0.00.20.40.60.81.0out-distribution accBackground Challenge Adam
top-1
top-10 average
all average
Figure 27. ERM Background Challenge / OOD accuracy when horizontal axis is the trial budget
0 25 50 75 100 125 150 175 200
trials0.00.20.40.60.81.0out-distribution accWILDS_Amazon ERM Momentum
top-1
top-10 average
all average
0 25 50 75 100 125 150 175 200
trials0.00.20.40.60.81.0out-distribution accWILDS_Amazon ERM Adam
top-1
top-10 average
all average
0 25 50 75 100 125 150 175 200
trials0.00.20.40.60.81.0out-distribution accWILDS_civilcomments ERM Momentum
top-1
top-10 average
all average
0 25 50 75 100 125 150 175 200
trials0.00.20.40.60.81.0out-distribution accWILDS_civilcomments ERM Adam
top-1
top-10 average
all average
Figure 28. ERM WILDS Amazon and WILDS CivilComment / OOD accuracy when horizontal axis is the
trial budget
G.2.2 Hyperparameter Trial Budget vs OOD Error
To visualize the slight increase at the end of OOD accuracy more clearly, we visualized it in log scale as OOD
error.
46Published in Transactions on Machine Learning Research (06/2023)
0 25 50 75 100 125 150 175 200
trials102
101
out-distribution errorDomainNet ERM Momentum
top-1
top-10 average
all average
0 25 50 75 100 125 150 175 200
trials102
101
out-distribution errorDomainNet ERM Adam
top-1
top-10 average
all average
0 25 50 75 100 125 150 175 200
trials102
101
out-distribution errorOfficeHome ERM Momentum
top-1
top-10 average
all average
0 25 50 75 100 125 150 175 200
trials102
101
out-distribution errorOfficeHome ERM Adam
top-1
top-10 average
all average
0 25 50 75 100 125 150 175 200
trials102
101
out-distribution errorT erraIncognita ERM Momentum
top-1
top-10 average
all average
0 25 50 75 100 125 150 175 200
trials102
101
out-distribution errorT erraIncognita ERM Adam
top-1
top-10 average
all average
Figure 29. ERM DomainNet, OﬃceHome and TerraIncognita / OOD error when horizontal axis is the trial
budget
47Published in Transactions on Machine Learning Research (06/2023)
0 25 50 75 100 125 150 175 200
trials102
101
out-distribution errorColoredMNIST ERM Momentum
top-1
top-10 average
all average
0 25 50 75 100 125 150 175 200
trials102
101
out-distribution errorColoredMNIST ERM Adam
top-1
top-10 average
all average
0 25 50 75 100 125 150 175 200
trials102
101
out-distribution errorRotatedMNIST ERM Momentum
top-1
top-10 average
all average
0 25 50 75 100 125 150 175 200
trials102
101
out-distribution errorRotatedMNIST ERM Adam
top-1
top-10 average
all average
0 25 50 75 100 125 150 175 200
trials102
101
out-distribution errorVLCS ERM Momentum
top-1
top-10 average
all average
0 25 50 75 100 125 150 175 200
trials102
101
out-distribution errorVLCS ERM Adam
top-1
top-10 average
all average
0 25 50 75 100 125 150 175 200
trials102
101
out-distribution errorPACS ERM Momentum
top-1
top-10 average
all average
0 25 50 75 100 125 150 175 200
trials102
101
out-distribution errorPACS ERM Adam
top-1
top-10 average
all average
Figure 30. ERM ColoredMNIST, RotatedMNIST, VLCS and PACS / OOD error when horizontal axis is the
trial budget
48Published in Transactions on Machine Learning Research (06/2023)
0 25 50 75 100 125 150 175 200
trials102
101
out-distribution errorBackground Challenge Momentum
top-1
top-10 average
all average
0 25 50 75 100 125 150 175 200
trials102
101
out-distribution errorBackground Challenge Adam
top-1
top-10 average
all average
Figure 31. ERM Background Challenge / OOD accuracy when horizontal axis is the trial budget
0 25 50 75 100 125 150 175 200
trials102
101
out-distribution errorWILDS_Amazon ERM Momentum
top-1
top-10 average
all average
0 25 50 75 100 125 150 175 200
trials102
101
out-distribution errorWILDS_Amazon ERM Adam
top-1
top-10 average
all average
0 25 50 75 100 125 150 175 200
trials102
101
out-distribution errorWILDS_civilcomments ERM Momentum
top-1
top-10 average
all average
0 25 50 75 100 125 150 175 200
trials102
101
out-distribution errorWILDS_civilcomments ERM Adam
top-1
top-10 average
all average
Figure 32. ERM WILDS Amazon and WILDS CivilComment / OOD accuracy when horizontal axis is the
trial budget
49Published in Transactions on Machine Learning Research (06/2023)
G.3 Learning Curve of ColoredMNIST
In Section 4.2, we conjecture that the better performance of Adam in Colored MNIST classiﬁcation may
come from overﬁtting to training data. To conﬁrm this hypothesis, we plot the averaged training accuracy,
the averaged validation accuracy, and the averaged test accuracy throughout the training. We pick the top-14
results in terms of test accuracy and use them for the plot. We show the result in Figure 33.
As is evident from Figure 33 (a) and Figure 33 (b), we observe that training accuracy increases while the
validation accuracy keeps unchanged. This indicates that overﬁtting occurs in the training. However, Figure
33 (c) indicates that test accuracy gradually improves as the model is overﬁtting. On the other hand, SGD
shows no such overﬁtting and OOD generalization improvement, as shown in Figure 34. Thus, we can
empirically support our conjecture that Adam produces the better OOD generalization performance by
overﬁtting training data.
0 1000 2000 3000 4000 5000
steps0.10.20.30.40.50.60.70.80.91.0average train accuracyERM ColoredMNIST Adam
trial-1
trial-2
trial-3
trial-4
trial-5
trial-6
trial-7
trial-8
trial-9
trial-10
trial-11
trial-12
trial-13
trial-14
(a) ERM: Averaged training accuracy of Adam on Colored
MNIST throughout the training.
0 1000 2000 3000 4000 5000
steps0.10.20.30.40.50.60.70.80.91.0average validation accuracyERM ColoredMNIST Adam
trial-1
trial-2
trial-3
trial-4
trial-5
trial-6
trial-7
trial-8
trial-9
trial-10
trial-11
trial-12
trial-13
trial-14(b) ERM: Averaged validation accuracy of Adam on Col-
ored MNIST throughout the training.
0 1000 2000 3000 4000 5000
steps0.10.20.30.40.50.60.70.80.91.0average test accuracyERM ColoredMNIST Adam
trial-1
trial-2
trial-3
trial-4
trial-5
trial-6
trial-7
trial-8
trial-9
trial-10
trial-11
trial-12
trial-13
trial-14
(c) ERM: Averaged test accuracy of Adam on Colored
MNIST throughout the training.
Figure 33. Adam: Comparison of averaged training accuracy, averaged validation accuracy, and averaged test
accuracy throughout the training. We plot these values of check points whose train accuracy is in the top-14.
50Published in Transactions on Machine Learning Research (06/2023)
0 1000 2000 3000 4000 5000
steps0.10.20.30.40.50.60.70.80.91.0average train accuracyERM ColoredMNIST vanilla SGD
trial-1
trial-2
trial-3
trial-4
trial-5
trial-6
trial-7
trial-8
trial-9
trial-10
trial-11
trial-12
trial-13
trial-14
(a) ERM: Averaged training accuracy of SGD on Colored
MNIST throughout the training.
0 1000 2000 3000 4000 5000
steps0.10.20.30.40.50.60.70.80.91.0average validation accuracyERM ColoredMNIST vanilla SGD
trial-1
trial-2
trial-3
trial-4
trial-5
trial-6
trial-7
trial-8
trial-9
trial-10
trial-11
trial-12
trial-13
trial-14(b) ERM: Averaged validation accuracy of SGD on Col-
ored MNIST throughout the training.
0 1000 2000 3000 4000 5000
steps0.10.20.30.40.50.60.70.80.91.0average test accuracyERM ColoredMNIST vanilla SGD
trial-1
trial-2
trial-3
trial-4
trial-5
trial-6
trial-7
trial-8
trial-9
trial-10
trial-11
trial-12
trial-13
trial-14
(c) ERM: Averaged test accuracy of SGD on Colored
MNIST throughout the training.
Figure 34. SGD: Comparison of averaged training accuracy, averaged validation accuracy, and averaged test
accuracy of SGD throughout the training. We plot these values of checkpoints whose train accuracy is in the
top-14.
G.4 Early Stopping
In Section 4.2, ﬁgures 1 shows that adaptive optimizers tend to overﬁt to training domain. A possible reason
is that the training speed of adaptive methods is faster than non-adaptive ones. That is, in the same steps
budget, adaptive optimizers converge faster in eﬀect. To validate if this is the case, we investigate whether
early stopping improves the OOD generalization of adaptive optimizers.
In particular, we compute the diﬀerence between averaged accuracy at early stopping and at last epoch for
test accuracy and validation accuracy, respectively:
Accdiﬀ=1
NesNes/summationdisplay
i=1Acces-i−1
NleNle/summationdisplay
i=1Accle-i (18)
whereAccdiﬀrepresents the diﬀerence in accuracy between early stopping and the last epoch, Nesis the
number of trials at early stopping, Nleis the number of trials at the last epoch, Acces-idenotes the accuracy
at the early stopping for the i−thtrial,Accle-idenotes the accuracy at the last epoch for the i−thtrial.
If the average accuracy at early stopping is larger, the diﬀerence is positive and vice versa.
51Published in Transactions on Machine Learning Research (06/2023)
Figures 35 to 40 are the results of this comparison for each dataset and algorithm. The y-axis is the diﬀerence
of out-of-distribution (test) accuracy and the x-axis is the diﬀerence of in-distribution (validation) accuracy.
The color indicates the epoch of early stopping. The darker color indicates that early stopping is conducted
in relatively earlier epochs.
For PACS, both diﬀerences are positive and lighter colors are concentrated at points of small diﬀerence of the
validation accuracy, as indicated in Figures 35 and 36. Therefore, we can conclude that when the validation
accuracy deteriorates by further training, the test accuracy also gets worse. However, the eﬀect of further
training is less evident for Adam. In other words, early stopping does not inﬂuence Adam so much but keeps
test accuracy from decreasing for SGD.
The result of VLCS shows a similar pattern as PACS (Figures 37 and 38). If anything, Further training after
early stopping makes adaptive optimizes be likely to result in better test accuracy than SGD, though they
have a large variance. With respect to SGD, additional training degrades the test accuracy as much as it
degrades validation accuracy.
Oﬃce-Home shows similar results as PACS, as presented in Figures 39 and 40.
In summary, we ﬁnd that early stopping does not inﬂuence adaptive optimizers. Therefore, we can conclude
that adaptive optimizer overﬁts not because it trains faster than non-adaptive methods.
The fact that early stopping does not aﬀect the adaptive optimizer means that adjusting the number of
epochs instead of a ﬁxed number of epochs will not change the result. We have followed previous studies and
experimented with a ﬁxed number of epochs in the present study, and our results suggest that this does not
have a serious impact on the comparison of adaptive and non-adaptive optimizers. Thus, we can see that
using a ﬁxed epoch number does not undermine the validity of our optimizer comparison experiment in this
sense.
52Published in Transactions on Machine Learning Research (06/2023)
0.0 0.2 0.4 0.6 0.8 1.0
difference of validation accuracy0.2
0.00.20.40.60.81.0difference of test accuracyvanilla_sgd sorted by percentage of early stopping
ERM PACS (Acc @Early Stopping - Acc @Last Epoch)
0.20.40.60.8
0.0 0.2 0.4 0.6 0.8 1.0
difference of validation accuracy0.2
0.00.20.40.60.81.0difference of test accuracymomentum_sgd sorted by percentage of early stopping
ERM PACS (Acc @Early Stopping - Acc @Last Epoch)
0.00.20.40.60.8
0.0 0.2 0.4 0.6 0.8 1.0
difference of validation accuracy0.2
0.00.20.40.60.81.0difference of test accuracynesterov_momentum_sgd sorted by percentage of early stopping
ERM PACS (Acc @Early Stopping - Acc @Last Epoch)
0.00.20.40.60.8
0.0 0.2 0.4 0.6 0.8 1.0
difference of validation accuracy0.2
0.00.20.40.60.81.0difference of test accuracyrmsprop sorted by percentage of early stopping
ERM PACS (Acc @Early Stopping - Acc @Last Epoch)
0.00.20.40.60.8
0.0 0.2 0.4 0.6 0.8 1.0
difference of validation accuracy0.2
0.00.20.40.60.81.0difference of test accuracyadam sorted by percentage of early stopping
ERM PACS (Acc @Early Stopping - Acc @Last Epoch)
0.00.20.40.60.8
Figure 35. ERM/PACS: Diﬀerence between accuracy at early stopping and at last epoch. The y-axis is the
diﬀerence of out-of-distribution (test) accuracy and the x-axis is the diﬀerence of in-distribution (validation)
accuracy. The color indicates the epoch of early stopping. The darker color indicates that early stopping is
conducted in relatively earlier epochs.
53Published in Transactions on Machine Learning Research (06/2023)
0.0 0.2 0.4 0.6 0.8 1.0
difference of validation accuracy0.2
0.00.20.40.60.81.0difference of test accuracyvanilla_sgd sorted by percentage of early stopping
IRM PACS (Acc @Early Stopping - Acc @Last Epoch)
0.00.20.40.60.8
0.0 0.2 0.4 0.6 0.8 1.0
difference of validation accuracy0.2
0.00.20.40.60.81.0difference of test accuracymomentum_sgd sorted by percentage of early stopping
IRM PACS (Acc @Early Stopping - Acc @Last Epoch)
0.00.20.40.60.8
0.0 0.2 0.4 0.6 0.8 1.0
difference of validation accuracy0.2
0.00.20.40.60.81.0difference of test accuracynesterov_momentum_sgd sorted by percentage of early stopping
IRM PACS (Acc @Early Stopping - Acc @Last Epoch)
0.00.20.40.60.8
0.0 0.2 0.4 0.6 0.8 1.0
difference of validation accuracy0.2
0.00.20.40.60.81.0difference of test accuracyrmsprop sorted by percentage of early stopping
IRM PACS (Acc @Early Stopping - Acc @Last Epoch)
0.00.20.40.60.8
0.0 0.2 0.4 0.6 0.8 1.0
difference of validation accuracy0.2
0.00.20.40.60.81.0difference of test accuracyadam sorted by percentage of early stopping
IRM PACS (Acc @Early Stopping - Acc @Last Epoch)
0.00.20.40.60.8
Figure 36. IRM/PACS: Diﬀerence between accuracy at early stopping and at last epoch. The y-axis is the
diﬀerence of out-of-distribution (test) accuracy and the x-axis is the diﬀerence of in-distribution (validation)
accuracy. The color indicates the epoch of early stopping. The darker color indicates that early stopping is
conducted in relatively earlier epochs.
54Published in Transactions on Machine Learning Research (06/2023)
0.0 0.2 0.4 0.6 0.8 1.0
difference of validation accuracy0.2
0.00.20.40.60.81.0difference of test accuracyvanilla_sgd sorted by percentage of early stopping
ERM VLCS (Acc @Early Stopping - Acc @Last Epoch)
0.20.40.60.8
0.0 0.2 0.4 0.6 0.8 1.0
difference of validation accuracy0.2
0.00.20.40.60.81.0difference of test accuracymomentum_sgd sorted by percentage of early stopping
ERM VLCS (Acc @Early Stopping - Acc @Last Epoch)
0.00.20.40.60.8
0.0 0.2 0.4 0.6 0.8 1.0
difference of validation accuracy0.2
0.00.20.40.60.81.0difference of test accuracynesterov_momentum_sgd sorted by percentage of early stopping
ERM VLCS (Acc @Early Stopping - Acc @Last Epoch)
0.00.20.40.60.8
0.0 0.2 0.4 0.6 0.8 1.0
difference of validation accuracy0.2
0.00.20.40.60.81.0difference of test accuracyrmsprop sorted by percentage of early stopping
ERM VLCS (Acc @Early Stopping - Acc @Last Epoch)
0.00.20.40.60.8
0.0 0.2 0.4 0.6 0.8 1.0
difference of validation accuracy0.2
0.00.20.40.60.81.0difference of test accuracyadam sorted by percentage of early stopping
ERM VLCS (Acc @Early Stopping - Acc @Last Epoch)
0.00.20.40.60.8
Figure 37. ERM/VLCS: Diﬀerence between accuracy at early stopping and at last epoch. The y-axis is the
diﬀerence of out-of-distribution (test) accuracy and the x-axis is the diﬀerence of in-distribution (validation)
accuracy. The color indicates the epoch of early stopping. The darker color indicates that early stopping is
conducted in relatively earlier epochs.
55Published in Transactions on Machine Learning Research (06/2023)
0.0 0.2 0.4 0.6 0.8 1.0
difference of validation accuracy0.2
0.00.20.40.60.81.0difference of test accuracyvanilla_sgd sorted by percentage of early stopping
IRM VLCS (Acc @Early Stopping - Acc @Last Epoch)
0.00.20.40.60.8
0.0 0.2 0.4 0.6 0.8 1.0
difference of validation accuracy0.2
0.00.20.40.60.81.0difference of test accuracymomentum_sgd sorted by percentage of early stopping
IRM VLCS (Acc @Early Stopping - Acc @Last Epoch)
0.00.20.40.60.8
0.0 0.2 0.4 0.6 0.8 1.0
difference of validation accuracy0.2
0.00.20.40.60.81.0difference of test accuracynesterov_momentum_sgd sorted by percentage of early stopping
IRM VLCS (Acc @Early Stopping - Acc @Last Epoch)
0.00.20.40.60.8
0.0 0.2 0.4 0.6 0.8 1.0
difference of validation accuracy0.2
0.00.20.40.60.81.0difference of test accuracyrmsprop sorted by percentage of early stopping
IRM VLCS (Acc @Early Stopping - Acc @Last Epoch)
0.00.20.40.60.8
0.0 0.2 0.4 0.6 0.8 1.0
difference of validation accuracy0.2
0.00.20.40.60.81.0difference of test accuracyadam sorted by percentage of early stopping
IRM VLCS (Acc @Early Stopping - Acc @Last Epoch)
0.00.20.40.60.8
Figure 38. IRM/VLCS: Diﬀerence between accuracy at early stopping and at last epoch. The y-axis is the
diﬀerence of out-of-distribution (test) accuracy and the x-axis is the diﬀerence of in-distribution (validation)
accuracy. The color indicates the epoch of early stopping. The darker color indicates that early stopping is
conducted in relatively earlier epochs.
56Published in Transactions on Machine Learning Research (06/2023)
0.0 0.2 0.4 0.6 0.8 1.0
difference of validation accuracy0.2
0.00.20.40.60.81.0difference of test accuracyvanilla_sgd sorted by percentage of early stopping
ERM OfficeHome (Acc @Early Stopping - Acc @Last Epoch)
0.20.40.60.8
0.0 0.2 0.4 0.6 0.8 1.0
difference of validation accuracy0.2
0.00.20.40.60.81.0difference of test accuracymomentum_sgd sorted by percentage of early stopping
ERM OfficeHome (Acc @Early Stopping - Acc @Last Epoch)
0.00.20.40.60.8
0.0 0.2 0.4 0.6 0.8 1.0
difference of validation accuracy0.2
0.00.20.40.60.81.0difference of test accuracynesterov_momentum_sgd sorted by percentage of early stopping
ERM OfficeHome (Acc @Early Stopping - Acc @Last Epoch)
0.00.20.40.60.8
0.0 0.2 0.4 0.6 0.8 1.0
difference of validation accuracy0.2
0.00.20.40.60.81.0difference of test accuracyrmsprop sorted by percentage of early stopping
ERM OfficeHome (Acc @Early Stopping - Acc @Last Epoch)
0.00.20.40.60.8
0.0 0.2 0.4 0.6 0.8 1.0
difference of validation accuracy0.2
0.00.20.40.60.81.0difference of test accuracyadam sorted by percentage of early stopping
ERM OfficeHome (Acc @Early Stopping - Acc @Last Epoch)
0.00.20.40.60.8
Figure 39. ERM/Oﬃce-Home: Diﬀerence between accuracy at early stopping and at last epoch. The y-axis
is the diﬀerence of out-of-distribution (test) accuracy and the x-axis is the diﬀerence of in-distribution
(validation) accuracy. The color indicates the epoch of early stopping. The darker color indicates that early
stopping is conducted in relatively earlier epochs.
57Published in Transactions on Machine Learning Research (06/2023)
0.0 0.2 0.4 0.6 0.8 1.0
difference of validation accuracy0.2
0.00.20.40.60.81.0difference of test accuracyvanilla_sgd sorted by percentage of early stopping
IRM OfficeHome (Acc @Early Stopping - Acc @Last Epoch)
0.00.20.40.60.8
0.0 0.2 0.4 0.6 0.8 1.0
difference of validation accuracy0.2
0.00.20.40.60.81.0difference of test accuracymomentum_sgd sorted by percentage of early stopping
IRM OfficeHome (Acc @Early Stopping - Acc @Last Epoch)
0.00.20.40.60.8
0.0 0.2 0.4 0.6 0.8 1.0
difference of validation accuracy0.2
0.00.20.40.60.81.0difference of test accuracynesterov_momentum_sgd sorted by percentage of early stopping
IRM OfficeHome (Acc @Early Stopping - Acc @Last Epoch)
0.00.20.40.60.8
0.0 0.2 0.4 0.6 0.8 1.0
difference of validation accuracy0.2
0.00.20.40.60.81.0difference of test accuracyrmsprop sorted by percentage of early stopping
IRM OfficeHome (Acc @Early Stopping - Acc @Last Epoch)
0.00.20.40.60.8
0.0 0.2 0.4 0.6 0.8 1.0
difference of validation accuracy0.2
0.00.20.40.60.81.0difference of test accuracyadam sorted by percentage of early stopping
IRM OfficeHome (Acc @Early Stopping - Acc @Last Epoch)
0.00.20.40.60.8
Figure 40. IRM/Oﬃce-Home: Diﬀerence between accuracy at early stopping and at last epoch. The y-axis
is the diﬀerence of out-of-distribution (test) accuracy and the x-axis is the diﬀerence of in-distribution
(validation) accuracy. The color indicates the epoch of early stopping. The darker color indicates that early
stopping is conducted in relatively earlier epochs.
However, we ﬁnd diﬀerent results from Colored MNIST. As shown in Figures 41 and 42, we can observe that
the diﬀerence of validation accuracy is positive and that of test accuracy is negative. That is further training
after early stopping decreases validation accuracy but increases test accuracy on Colored MNIST, while there
is no diﬀerence for SGD. This is consistent with the result of Appendix G.3.
58Published in Transactions on Machine Learning Research (06/2023)
0.0 0.2 0.4 0.6 0.8 1.0
difference of validation accuracy0.2
0.00.20.40.60.81.0difference of test accuracyvanilla_sgd sorted by percentage of early stopping
ERM ColoredMNIST (Acc @Early Stopping - Acc @Last Epoch)
0.20.40.60.8
0.0 0.2 0.4 0.6 0.8 1.0
difference of validation accuracy0.2
0.00.20.40.60.81.0difference of test accuracymomentum_sgd sorted by percentage of early stopping
ERM ColoredMNIST (Acc @Early Stopping - Acc @Last Epoch)
0.20.40.60.8
0.0 0.2 0.4 0.6 0.8 1.0
difference of validation accuracy0.2
0.00.20.40.60.81.0difference of test accuracynesterov_momentum_sgd sorted by percentage of early stopping
ERM ColoredMNIST (Acc @Early Stopping - Acc @Last Epoch)
0.20.40.60.8
0.0 0.2 0.4 0.6 0.8 1.0
difference of validation accuracy0.2
0.00.20.40.60.81.0difference of test accuracyrmsprop sorted by percentage of early stopping
ERM ColoredMNIST (Acc @Early Stopping - Acc @Last Epoch)
0.00.20.40.60.8
0.0 0.2 0.4 0.6 0.8 1.0
difference of validation accuracy0.2
0.00.20.40.60.81.0difference of test accuracyadam sorted by percentage of early stopping
ERM ColoredMNIST (Acc @Early Stopping - Acc @Last Epoch)
0.00.20.40.60.8
Figure 41. ERM/Colored MNIST: Diﬀerence between accuracy at early stopping and at last epoch. The
y-axis is the diﬀerence of out-of-distribution (test) accuracy and the x-axis is the diﬀerence of in-distribution
(validation) accuracy. The color indicates the epoch of early stopping. The darker color indicates that early
stopping is conducted in relatively earlier epochs.
59Published in Transactions on Machine Learning Research (06/2023)
0.0 0.2 0.4 0.6 0.8 1.0
difference of validation accuracy0.2
0.00.20.40.60.81.0difference of test accuracyvanilla_sgd sorted by percentage of early stopping
IRM ColoredMNIST (Acc @Early Stopping - Acc @Last Epoch)
0.00.10.20.30.4
0.0 0.2 0.4 0.6 0.8 1.0
difference of validation accuracy0.2
0.00.20.40.60.81.0difference of test accuracymomentum_sgd sorted by percentage of early stopping
IRM ColoredMNIST (Acc @Early Stopping - Acc @Last Epoch)
0.00.20.40.60.8
0.0 0.2 0.4 0.6 0.8 1.0
difference of validation accuracy0.2
0.00.20.40.60.81.0difference of test accuracynesterov_momentum_sgd sorted by percentage of early stopping
IRM ColoredMNIST (Acc @Early Stopping - Acc @Last Epoch)
0.00.20.40.60.8
0.0 0.2 0.4 0.6 0.8 1.0
difference of validation accuracy0.2
0.00.20.40.60.81.0difference of test accuracyrmsprop sorted by percentage of early stopping
IRM ColoredMNIST (Acc @Early Stopping - Acc @Last Epoch)
0.00.20.40.60.8
0.0 0.2 0.4 0.6 0.8 1.0
difference of validation accuracy0.2
0.00.20.40.60.81.0difference of test accuracyadam sorted by percentage of early stopping
IRM ColoredMNIST (Acc @Early Stopping - Acc @Last Epoch)
0.00.20.40.60.8
Figure 42. IRM/Colored MNIST: Diﬀerence between accuracy at early stopping and at last epoch. The
y-axis is the diﬀerence of out-of-distribution (test) accuracy and x-axis is the diﬀerence of in-distribution
(validation) accuracy. The color indicates the epoch of early stopping. The darker color indicates that early
stopping is conducted in relatively earlier epochs.
60Published in Transactions on Machine Learning Research (06/2023)
H Soundness Check of Our Experiments
H.1 Histgram of Hyperparameters
We have shown the histgram of the hyperparameters to be used for training just for reference. In particular,
we display a result for learning rate of Momentum SGD and Adam for the each dataset. We observe that we
could sample hyperparameters from a reasonably wide range.
105
104
103
102
Learning Rate050100150200250300#TrialsERM_ColoredMNIST
momentum_sgd
adam
105
104
103
102
Learning Rate050100150200250300#TrialsERM_RotatedMNIST
momentum_sgd
adam
105
104
103
102
Learning Rate0200400600800#TrialsERM_PACS
momentum_sgd
adam
105
104
103
102
Learning Rate02004006008001000120014001600#TrialsERM_VLCS
momentum_sgd
adam
105
104
103
102
Learning Rate050010001500200025003000#TrialsERM_OfficeHome
momentum_sgd
adam
105
104
103
102
Learning Rate050100150200250#TrialsERM_T erraIncognita
momentum_sgd
adam
105
104
103
102
Learning Rate050100150200#TrialsERM_DomainNet
momentum_sgd
adam
105
104
103
102
Learning Rate0100200300400500600700#TrialsERM_WILDS_amazon
momentum_sgd
adam
105
104
103
102
Learning Rate050100150200250300#TrialsERM_WILDS_civilcomments
momentum_sgd
adam
Figure 43. Histogram of explored hyperparameters (learning rate) in training of DomainBed, WILDS dataset
in ERM. Momentum SGD and Adam results are included. Although uniform distribution is used as the prior
distribution for hyperparameter optimization, the histogram results do not match the uniform distribution
because Bayesian optimization is used.
H.2 Hyperparameters and OOD Accuracy Box-Plot
The previous section provided information on the hyperparameter search range. In this section, we share
the results of the out-of-distribution performance for a speciﬁc hyperparameter range as a box plot with the
hyperparameters separating the bin as shown in Figure 44, 45 and 46.
From these results, we observe that lr, which achieves high out-of-distribution accuracy, is within the search
range of learning rate and thus has suﬃcient range to perform the search.
61Published in Transactions on Machine Learning Research (06/2023)
(1e-05, 2.15e-05](2.15e-05, 4.64e-05](4.64e-05, 0.0001] (0.0001, 0.000215](0.000215, 0.000464](0.000464, 0.001] (0.001, 0.00215](0.00215, 0.00464](0.00464, 0.01]
lr0.100.150.200.250.30out-of-distribution acc
ERM_ColoredMNIST_momentum_sgd
(1e-05, 2.15e-05](2.15e-05, 4.64e-05](4.64e-05, 0.0001] (0.0001, 0.000215](0.000215, 0.000464](0.000464, 0.001] (0.001, 0.00215](0.00215, 0.00464](0.00464, 0.01]
lr0.10.20.30.40.50.60.7out-of-distribution acc
ERM_ColoredMNIST_adam
(1e-05, 2.15e-05](2.15e-05, 4.64e-05](4.64e-05, 0.0001] (0.0001, 0.000215](0.000215, 0.000464](0.000464, 0.001] (0.001, 0.00215](0.00215, 0.00464](0.00464, 0.01]
lr0.40.50.60.70.80.9out-of-distribution acc
ERM_RotatedMNIST_momentum_sgd
(1e-05, 2.15e-05](2.15e-05, 4.64e-05](4.64e-05, 0.0001] (0.0001, 0.000215](0.000215, 0.000464](0.000464, 0.001] (0.001, 0.00215](0.00215, 0.00464](0.00464, 0.01]
lr0.20.30.40.50.60.70.80.91.0out-of-distribution acc
ERM_RotatedMNIST_adam
(1e-05, 2.15e-05](2.15e-05, 4.64e-05](4.64e-05, 0.0001] (0.0001, 0.000215](0.000215, 0.000464](0.000464, 0.001] (0.001, 0.00215](0.00215, 0.00464](0.00464, 0.01]
lr0.10.20.30.40.50.60.70.80.9out-of-distribution acc
ERM_PACS_momentum_sgd
(1e-05, 2.15e-05](2.15e-05, 4.64e-05](4.64e-05, 0.0001] (0.0001, 0.000215](0.000215, 0.000464](0.000464, 0.001] (0.001, 0.00215](0.00215, 0.00464](0.00464, 0.01]
lr0.10.20.30.40.50.60.70.80.9out-of-distribution acc
ERM_PACS_adam
(1e-05, 2.15e-05](2.15e-05, 4.64e-05](4.64e-05, 0.0001] (0.0001, 0.000215](0.000215, 0.000464](0.000464, 0.001] (0.001, 0.00215](0.00215, 0.00464](0.00464, 0.01]
lr0.20.40.60.81.0out-of-distribution acc
ERM_VLCS_momentum_sgd
(1e-05, 2.15e-05](2.15e-05, 4.64e-05](4.64e-05, 0.0001] (0.0001, 0.000215](0.000215, 0.000464](0.000464, 0.001] (0.001, 0.00215](0.00215, 0.00464](0.00464, 0.01]
lr0.20.40.60.81.0out-of-distribution acc
ERM_VLCS_adam
Figure 44. Box-Plot of Out-of-Distribution Accuracy per Log-Scale of Learning Rate: ColoredMNIST,
RotatedMNIST, PACS and VLCS
62Published in Transactions on Machine Learning Research (06/2023)
(1e-05, 2.15e-05](2.15e-05, 4.64e-05](4.64e-05, 0.0001] (0.0001, 0.000215](0.000215, 0.000464](0.000464, 0.001] (0.001, 0.00215](0.00215, 0.00464](0.00464, 0.01]
lr0.00.10.20.30.40.50.6out-of-distribution acc
ERM_OfficeHome_momentum_sgd
(1e-05, 2.15e-05](2.15e-05, 4.64e-05](4.64e-05, 0.0001] (0.0001, 0.000215](0.000215, 0.000464](0.000464, 0.001] (0.001, 0.00215](0.00215, 0.00464](0.00464, 0.01]
lr0.00.10.20.30.40.50.6out-of-distribution acc
ERM_OfficeHome_adam
(1e-05, 2.15e-05](2.15e-05, 4.64e-05](4.64e-05, 0.0001] (0.0001, 0.000215](0.000215, 0.000464](0.000464, 0.001] (0.001, 0.00215](0.00215, 0.00464](0.00464, 0.01]
lr0.00.10.20.30.40.50.6out-of-distribution acc
ERM_T erraIncognita_momentum_sgd
(1e-05, 2.15e-05](2.15e-05, 4.64e-05](4.64e-05, 0.0001] (0.0001, 0.000215](0.000215, 0.000464](0.000464, 0.001] (0.001, 0.00215](0.00215, 0.00464](0.00464, 0.01]
lr0.00.10.20.30.40.5out-of-distribution acc
ERM_T erraIncognita_adam
(1e-05, 2.15e-05](2.15e-05, 4.64e-05](4.64e-05, 0.0001] (0.0001, 0.000215](0.000215, 0.000464](0.000464, 0.001] (0.001, 0.00215](0.00215, 0.00464](0.00464, 0.01]
lr0.00.10.20.30.40.50.6out-of-distribution acc
ERM_DomainNet_momentum_sgd
(1e-05, 2.15e-05](2.15e-05, 4.64e-05](4.64e-05, 0.0001] (0.0001, 0.000215](0.000215, 0.000464](0.000464, 0.001] (0.001, 0.00215](0.00215, 0.00464](0.00464, 0.01]
lr0.00.10.20.30.40.50.6out-of-distribution acc
ERM_DomainNet_adam
Figure 45. Box-Plot of Out-of-Distribution Accuracy per Log-Scale of Learning Rate: OﬃceHome, Ter-
raIncognita, and DomainNet
63Published in Transactions on Machine Learning Research (06/2023)
(1e-05, 2.15e-05](2.15e-05, 4.64e-05](4.64e-05, 0.0001] (0.0001, 0.000215](0.000215, 0.000464](0.000464, 0.001] (0.001, 0.00215](0.00215, 0.00464](0.00464, 0.01]
lr0.200.250.300.350.400.450.500.55out-of-distribution accERM_WILDS_amazon_momentum_sgd
(1e-05, 2.15e-05](2.15e-05, 4.64e-05](4.64e-05, 0.0001] (0.0001, 0.000215](0.000215, 0.000464](0.000464, 0.001] (0.001, 0.00215](0.00215, 0.00464](0.00464, 0.01]
lr0.200.250.300.350.400.450.500.55out-of-distribution acc
ERM_WILDS_amazon_adam
(1e-05, 2.15e-05](2.15e-05, 4.64e-05](4.64e-05, 0.0001] (0.0001, 0.000215](0.000215, 0.000464](0.000464, 0.001] (0.001, 0.00215](0.00215, 0.00464](0.00464, 0.01]
lr0.00.10.20.30.40.50.6out-of-distribution acc
ERM_WILDS_civilcomments_momentum_sgd
(1e-05, 2.15e-05](2.15e-05, 4.64e-05](4.64e-05, 0.0001] (0.0001, 0.000215](0.000215, 0.000464](0.000464, 0.001] (0.001, 0.00215](0.00215, 0.00464](0.00464, 0.01]
lr0.00.10.20.30.4out-of-distribution acc
ERM_WILDS_civilcomments_adam
Figure 46. Box-Plot of Out-of-Distribution Accuracy per Log-Scale of Learning Rate: Amazon-WILDS, and
CivilComments-WILDS
H.3 Eﬀect of Initial Conﬁguration on Hyperparameter Optimization
In this section, we investigate how the ﬁrst hyperparameter combination aﬀected the search in our Bayesian
optimization of hyperparameters. We compared Momentum SGD to Adam and chose PACS as our dataset.
The experimental results showed that random initialization which we used, given a suﬃcient number of
trials (e.g., more than 200 trials within our experimental protocol), did not diﬀer from the ﬁnal performance
obtained when searching from the default hyperparameters of pytorch.
0 25 50 75 100 125 150 175 200
trials0.00.20.40.60.81.0out-distribution accPACS ERM Momentum
top-1
top-10 average
all average
0 25 50 75 100 125 150 175 200
trials0.00.20.40.60.81.0out-distribution accPACS ERM Momentum
top-1
top-10 average
all average
Figure 47. ERM PACS MomentumSGD / Comparison of Initialization (Left: Random initialization, Right:
Pytorch default hyperparameter initialization)
64Published in Transactions on Machine Learning Research (06/2023)
0 25 50 75 100 125 150 175 200
trials0.00.20.40.60.81.0out-distribution accPACS ERM Adam
top-1
top-10 average
all average
0 25 50 75 100 125 150 175 200
trials0.00.20.40.60.81.0out-distribution accPACS ERM Adam
top-1
top-10 average
all average
Figure 48. ERM PACS Adam / Comparison of Initialization (Left: Random initialization, Right: Pytorch
default hyperparameter initialization)
65Published in Transactions on Machine Learning Research (06/2023)
H.4 Best OOD Performance Comparison against with Existing Benckmark
In order to conﬁrm the soundness of our experiments, we compared our results with existing benchmarks to
see how well they actually performed, in addition to the hyperparameter search ranges in the previous section.
In particular, we compared our experimental results with those of DomainBed (Gulrajani & Lopez-Paz, 2021),
an existing oracle benchmark that uses Adam.
Table 15: OOD accuracy (%) comparison of our experimental results with the benchmark results reported in
DomainBed (Gulrajani & Lopez-Paz, 2021)
Dataset OOD Domain Existing Benchmark Results(Adam) Our Results(Adam)
ColoredMNIST 0.9 30.0±0.3 73.92
RotatedMNIST 0 96.0±0.2 96.40
VLCS C 97.7±0.3 99.36
PACS A 87.8±0.4 89.30
OﬃceHome A 61.2±1.4 63.12
TerraIncognita L100 59.9±1.0 61.35
DomainNet clipart 58.4±0.3 58.48
66Published in Transactions on Machine Learning Research (06/2023)
I Additional Study
I.1 Corruption and Perturbation Shift
In the main body of our paper, we presented experimental results for the seven types of domain shifts included
in DomainBed (Gulrajani & Lopez-Paz, 2021), as well as the Background Challenge (Xiao et al., 2021), which
deals with background shifts, and WILDS datasets (Koh et al., 2021) which deals with the population shift
dataset. In this section, we report the results of our investigation of the corruption and perturbation datasets
to investigate a broader range of out-of-distribution generalization. Details of the experiments are described
in Appendix C.4, D.4 and E.4. CIFAR10-C and CIFAR10-P (Hendrycks & Dietterich, 2019) were used as the
datasets. Momentum SGD and Adam were used as optimization methods for comparison.
The CIFAR10-C results are averaged performance results for 19 diﬀerent noise types of corruption and are
based on Hendrycks & Dietterich (2019) experimental protocol. The higher the performance, the better. The
CIFAR10-P experiment shows the variability of inference for noise perturbations, with lower values indicating
better performance. Experimental results show that Momentum SGD outperforms Adam in both CIFAR10-C
and CIFAR10-P (Table 16).
Table 16: CIFAR-10-C (averaging corruption classiﬁcation accuracy %) and CIFAR-10-P (top-5 robustness
perturbation): Performance comparison between Momentum SGD and Adam with mean and standard
deviation.
Dataset Momentum Adam
CIFAR10-C ( ↑)42.89±6.66 42.06±6.79
CIFAR10-P ( ↓)1.38±0.33 1.62±0.26
I.2 Model Architecture
In DomainBed experiments, we followed Gulrajani & Lopez-Paz (2021) and used only ConvNet for the
MNIST-based dataset and ResNet-50 for the ImageNet-based dataset. In this section, we investigate the
impact of changing the model architecture.
I.2.1 ResNet-20 for ColoredMNIST
Here are the results of ResNet-20 on the ColorMNIST Task (Figure 17). In ConvNet case, Adam outperformed
Momentum SGD, but the results were reversed in ResNet-20.
Table 17: ColoredMNIST: OOD accuracy (%) comparison between Momentum SGD and Adam
Model Architecture Momentum Adam
ConvNet 12.86±4.66 16.12±7.98
ResNet-20 11.00±0.52 10.09±0.15
I.2.2 Vision Transformer for PACS
Vision Transformer (ViT)(Dosovitskiy et al., 2020) is a neural network using the recent attention structure.
We evaluated the out-of-distribution performance when using Vision Transformer as well as ResNet-50 used
in DomainBed. However, due to computational resource constraints, we compared Adam and Momentum
SGD only for the task on the PACS dataset. The experimental results are shown in Table 18.
As a result, the experimental results with Vision Transformer show a signiﬁcant performance improvement
over the ResNet-50 case. Furthermore, the result that Momentum SGD outperforms Adam is consistent with
the ResNet-50 case.
67Published in Transactions on Machine Learning Research (06/2023)
Table 18: PACS: OOD accuracy(%) comparison between Momentum SGD and Adam
Model Architecture Momentum Adam
ResNet-50 87.03±0.65 83.94±0.88
ViT 90.28±0.54 90.05±0.29
I.3 State-of-the-Arts Optimizers
I.3.1 Sharpness Aware Minimization (SAM)
Sharpness-Aware Minimization (SAM) (Foret et al., 2020) prevents convergence to high curvature local
minima. Its convergence towards smaller curvature solutions results in high validation and test performance on
in-distribution (ID) environment. SAM searches for points where the loss is maximized within a neighborhood
ofρand uses the gradient at that point for iterative optimization. The larger the ρ, the higher the eﬀect of
preventing convergence to high curvature local minima.
We carried out experiments with SAM on PACS and Amazon-WILDS datasets tasks, comparing it to both
Momentum SGD and Adam over a range of hyperparameters outlined in Appendices E.3 and E.3. The
experimental results indicated competitive performance by SAM, equaling Momentum SGD. In the case of
the Amazon-WILDS dataset, SAM proved superior for both in-distribution and out-of-distribution accuracy.
Table 19: PACS: Accuracy (%) comparison of SAM with Momentum SGD and Adam
Accuracy Momentum Adam SAM
ID accuracy 96.79±0.9 96.78±0.42 97.54±0.07
OOD accuracy 87.03±0.65 83.94±0.88 86.65±0.9
Table 20: Amazon-WILDS: Accuracy (%) comparison of SAM with Momentum SGD and Adam
Accuracy Momentum Adam SAM
ID accuracy 72.51±0.06 72.02±0.07 73.42±0.07
OOD accuracy 53.33±0.0 52.67±0.77 54.0±0.94
I.3.2 Adam with Decoupled Weight Decay (AdamW)
The AdamW optimizer, proposed by Loshchilov & Hutter (2017) is an extension of the popular Adam
optimization algorithm. AdamW addresses the shortcomings of the original Adam optimizer concerning
weight decay regularization. In the original Adam algorithm, weight decay is directly applied to the adaptive
learning rates, causing a discrepancy between the intended eﬀect of weight decay and the actual eﬀect in
practice. The AdamW optimizer decouples weight decay from the adaptive learning rates, resulting in a more
eﬀective regularization method that is better suited for various deep learning tasks. By incorporating weight
decay separately from the update step, the AdamW optimizer exhibits improved convergence properties and
generalization performance compared to the original Adam algorithm.
We show the results of our AdamW experiment in Table 21. After tuning learning rate and selecting the
model with the best learning rate, we ran the experiment with three diﬀerent seeds and computed the mean
and variance. We found that AdamW performs better than Adam, but not as well as Momentum SGD.
Table 21: Amazon-WILDS: Comparison of AdamW with Momentum SGD and Adam
Accuracy Momentum Adam AdamW
ID accuracy 72.51±0.06 72.02±0.07 72.27±0.21
OOD accuracy 53.33±0.0 52.67±0.77 53.33±0.33
68Published in Transactions on Machine Learning Research (06/2023)
I.4 Large /epsilon1for Adam
The study in Choi et al. (2019) shows that high in-distribution performance similar to Momentum SGD can
be achieved with large /epsilon1. However, /epsilon1is a hyperparameter that has been introduced to prevent zero-percentage,
and is speciﬁed as /epsilon1= 1e-8 in pytorch’s default implementation8. It is known that as /epsilon1increases, Adam
approximate Momentum SGD (Choi et al., 2019). While this section of our paper investigated with /epsilon1to the
extent that it behaves as Adam, in this section we provide experimental results at large /epsilon1, where Adam is
expected to behave more like Momentum SGD.
The results of the experiment are shown in Figure 49. The x marker in the lower left corner shows the results
for the default hyperparameters in Adam. The other circle markers are for diﬀerent /epsilon1in Adam. Red stars
indicate some of the Momentum results. It can be seen that the larger /epsilon1achieved performance closer to
Momentum SGD than the default /epsilon1in Adam.
0.940 0.945 0.950 0.955 0.960 0.965 0.970 0.975 0.980
in-distribution acc0.820.830.840.850.860.870.88out-of-distribution acc
Adam ( =1e-8)
Momentum
Momentum
MomentumMomentum
(Adam)
1e-02
1e-04
0
1e+02
1e+04
1e+06
1e+08
1e+10
1e+14
Figure 49. Demonstrating the varying performance of out-of-distribution accuracy according to the value of
/epsilon1in Adam. As /epsilon1becomes larger, it approaches the performance of Momentum.
I.5 Learning Rate Schedule
Amazon-WILDS task uses linear scheduling without warmups9. However, learning rate scheduling can aﬀect
performance. To consider this impact, we compared and veriﬁed the following four learning rate schedules
and eight patterns with and without warmup.
Table 22 shows the results of the Amazon-WILDS experiments. However, the results for the largest out-of-
distribution accuracy are shown since no signiﬁcant diﬀerences were found for all experiments. No signiﬁcant
diﬀerences were found with or without warmup. It was not clear that introducing warmup is always eﬀective.
The Cosine LR Schedule was found to be the most eﬀective in the problem setting of this study.
8https://pytorch.org/docs/stable/generated/torch.optim.Adam.html
9https://github.com/p-lambda/wilds
69Published in Transactions on Machine Learning Research (06/2023)
Table 22: Amazon-WILDS: OOD Accuracy (%) comparison of LR Scheduler and Warmup
Learning Rate Schedule Momentum Adam AdamW
Constant LR 53.33 53.33 52.00
Constant LR + Warmup 53.33 52.00 53.33
Cosine LR 54.67 54.67 54.67
Cosine LR + Warmup 54.67 54.67 54.67
Linear LR (Default) 54.67 52.00 52.00
Linear LR + Warmup 54.67 54.67 53.33
MultiStep LR 52.00 53.33 52.00
MultiStep LR + Warmup 50.67 53.33 52.00
The CivilComments-WILDS task, akin to Amazon-WILDS, employs linear scheduling without warmups,
as elucidated in the oﬃcial repository10. We proceeded to examine the CivilComments-WILDS dataset to
identify any analogous trends that may emerge. Our analyses from the CivilComments-WILDS dataset
corroborated the ﬁndings from our Amazon-WILDS study, thereby reinforcing our preliminary conclusion that
non-adaptive optimizers typically exhibit superior performance over their adaptive counterparts. Notably,
we observed a signiﬁcant deviation in the OOD performance when modifying the learning rate scheduler
in the CivilComments-WILDS dataset. This observation starkly contrasts with our experiences in the
Amazon-WILDS setting. Moreover, despite these modiﬁcations to the learning rate scheduler, we could
not surpass the performance outcomes achieved with the default learning rate schedule. This reiterates the
eﬃcacy of the default setting, and further validates our overall ﬁndings.
Table 23: CivilComments-WILDS: OOD Accuracy (%) comparison of LR Scheduler and Warmup
Learning Rate Schedule Momentum Adam
Constant LR 47.82 44.29
Constant LR + Warmup 47.54 44.44
Cosine LR 56.98 45.40
Cosine LR + Warmup 56.83 45.48
Linear LR (Default) 57.69 46.82
Linear LR + Warmup 57.14 45.00
MultiStep LR 47.82 44.29
MultiStep LR + Warmup 51.35 44.04
I.6 The Eﬀect of Random Seeds
We conducted experiments on the eﬀect of seed, which controls randomness, such as model initialization, on
learning, using the PACS and Amazon-WILDS datasets.
In the PACS experiment shown in Table 24, a performance diﬀerence of around 6% was observed due to the
eﬀect of seed, especially for Adam. In contrast, in Momentum SGD, the eﬀect of seed was not signiﬁcant. In
the Amazon-WILDS experiment shown in Table 25, seed had no signiﬁcant eﬀect on either Adam or Nesterov
Momentum SGD.
Table 24: PACS: OOD Accuracy (%)
Diﬀerent Seed Comparison
SeedMomentum Adam
202186.47 81.20
202286.47 87.06
202387.74 85.69Table 25: Amazon-WILDS: OOD Accuracy (%)
Diﬀerent Seed Comparison
SeedNesterov Adam
202153.33 52.00
202253.73 53.33
202354.67 53.33
10https://github.com/p-lambda/wilds
70Published in Transactions on Machine Learning Research (06/2023)
I.7 Algorithms (ERM, IRM, VREx and CORAL)
In this study, we focused on ERM and IRM and obtained consistent results that the Non-Adaptive op-
timizer outperforms the Adaptive optimizer in out-of-distribution performance. We also veriﬁed the use
of VREx(Krueger et al., 2021) and CORAL(Sun & Saenko, 2016) as the other algorithms. The results of
these experiments are shown in Table 26. However, due to limited computing resources, experiments were
conducted only for Momentum SGD and Adam for PACS.
Table 26: PACS: OOD Accuracy (%) comparison of algorithms (ERM, IRM, VREx and CORAL)
Algorithm Momentum Adam
ERM 87.03±0.65 83.94±0.88
IRM 83.06±0.32 83.05±0.44
VREx 85.70±0.24 84.85±1.88
CORAL 84.25±1.35 84.10±1.13
71